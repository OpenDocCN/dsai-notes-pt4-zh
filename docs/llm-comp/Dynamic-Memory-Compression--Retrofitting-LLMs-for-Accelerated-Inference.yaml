- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:53:16'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:16
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态内存压缩：为加速推理改造 LLM
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.09636](https://ar5iv.labs.arxiv.org/html/2403.09636)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.09636](https://ar5iv.labs.arxiv.org/html/2403.09636)
- en: Piotr Nawrot    Adrian Łańcucki    Marcin Chochowski    David Tarjan    Edoardo
    M. Ponti
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Piotr Nawrot    Adrian Łańcucki    Marcin Chochowski    David Tarjan    Edoardo
    M. Ponti
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Transformers have emerged as the backbone of large language models (LLMs). However,
    generation remains inefficient due to the need to store in memory a cache of key–value
    representations for past tokens, whose size scales linearly with the input sequence
    length and batch size. As a solution, we propose Dynamic Memory Compression (DMC),
    a method for on-line key–value cache compression at inference time. Most importantly,
    the model learns to apply different compression rates in different heads and layers.
    We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,
    achieving up to ~3.7$\times$ cache compression, outperforming up-trained grouped-query
    attention (GQA). GQA and DMC can be even combined to obtain compounded gains.
    As a result DMC fits longer contexts and larger batches within any given memory
    budget.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 已成为大型语言模型（LLMs）的核心。然而，由于需要在内存中存储一个用于过去标记的键值对缓存，生成过程仍然效率低下，而缓存的大小与输入序列长度和批次大小成线性关系。作为解决方案，我们提出了动态内存压缩（DMC），这是一种在推理时进行在线键值缓存压缩的方法。最重要的是，模型学习在不同的头部和层中应用不同的压缩率。我们将预训练的LLM（如
    Llama 2（7B、13B 和 70B））改造为 DMC Transformers，取得了最高约 3.7$\times$ 的缓存压缩，优于升级的分组查询注意力（GQA）。GQA
    和 DMC 甚至可以结合使用以获得复合增益。因此，DMC 可以在任何给定的内存预算内适应更长的上下文和更大的批次。
- en: Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML
- en: ^QNVIDIA     ^KUniversity of Wrocław     ^VUniversity of Edinburgh
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ^QNVIDIA     ^K弗罗茨瓦夫大学     ^V爱丁堡大学
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Transformer Large Language Models (LLMs) are the state of the art in generative
    and conversational AI (Touvron et al., [2023](#bib.bib32); Jiang et al., [2023](#bib.bib18)).
    Their deployment, however, is curtailed in part by their inefficiency. This is
    not only due to the quadratic complexity of attention layers (Bahdanau et al.,
    [2014](#bib.bib3); Vaswani et al., [2017](#bib.bib34)): during generation, Transformers
    store the keys and values of past tokens in memory to avoid recomputing them multiple
    times. Since this key–value (KV) cache grows linearly with the sequence length
    and batch size, generation with Transformers quickly becomes prohibitive due to
    the excessive memory load. This issue emerges even more clearly with long-context
    generation (e.g., in dialogues and stories) or when serving large numbers of user
    queries.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 大型语言模型（LLMs）是生成性和对话 AI 的最前沿技术（Touvron 等， [2023](#bib.bib32)；Jiang
    等， [2023](#bib.bib18)）。然而，它们的应用受到限制，部分原因是它们的低效率。这不仅是由于注意力层的二次复杂性（Bahdanau 等，[2014](#bib.bib3)；Vaswani
    等，[2017](#bib.bib34)）：在生成过程中，Transformer 将过去标记的键和值存储在内存中，以避免多次重新计算它们。由于这个键值（KV）缓存随着序列长度和批次大小的增加而线性增长，因此使用
    Transformer 进行生成很快就会因内存负担过重而变得不可行。这一问题在长上下文生成（例如，对话和故事）或处理大量用户查询时尤为明显。
- en: '![Refer to caption](img/520c30b10926a7f082d81d39ef4e079e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/520c30b10926a7f082d81d39ef4e079e.png)'
- en: (a) Regular key–value cache with items $kv_{i}$ depicted as boxes. New items
    are always appended.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 常规键值缓存，$kv_{i}$ 项被描绘为方框。新项总是附加在后面。
- en: '![Refer to caption](img/607d0c2ab30156b1878bf94090d4f5ad.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/607d0c2ab30156b1878bf94090d4f5ad.png)'
- en: (b) Dynamic Memory Compression (DMC) chooses whether to accumulate or append
    current items, resulting in a smaller key–value cache.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 动态内存压缩（DMC）选择是累积还是附加当前项，从而生成更小的键值缓存。
- en: 'Figure 1: Key–value cache update mechanisms.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：键值缓存更新机制。
- en: A widespread solution to increase the memory efficiency of Transformers during
    inference is Grouped Query Attention (GQA; Ainslie et al., [2023](#bib.bib1);
    Shazeer, [2019](#bib.bib30)), which uses a number of key and value heads inferior
    to the number of query heads through parameter sharing. As an alternative, the
    number of overall tokens in memory can be reduced through token merging (Zhang
    et al., [2018](#bib.bib37); Liu et al., [2018](#bib.bib21); Bolya et al., [2022](#bib.bib6))
    or token pruning (Anagnostidis et al., [2023](#bib.bib2); Kim & Cho, [2020](#bib.bib19)).
    Nevertheless, these methods often pay the price of a severe degradation in downstream
    performance. On the other hand, hardware/IO-aware (Dao et al., [2022](#bib.bib12);
    Kwon et al., [2023](#bib.bib20)) and sub-quadratic algorithms for attention (Beltagy
    et al., [2020](#bib.bib4); Choromanski et al., [2020](#bib.bib9)) do not alleviate
    the memory load of the KV cache.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 提高 Transformers 在推理过程中内存效率的一个广泛解决方案是分组查询注意力（GQA；Ainslie 等，[2023](#bib.bib1)；Shazeer，[2019](#bib.bib30)），它通过参数共享使用的键和值头部数量少于查询头部的数量。作为替代，可以通过标记合并（Zhang
    等，[2018](#bib.bib37)；Liu 等，[2018](#bib.bib21)；Bolya 等，[2022](#bib.bib6)）或标记修剪（Anagnostidis
    等，[2023](#bib.bib2)；Kim & Cho，[2020](#bib.bib19)）来减少内存中的总标记数量。然而，这些方法往往以严重降低下游性能为代价。另一方面，硬件/IO
    感知（Dao 等，[2022](#bib.bib12)；Kwon 等，[2023](#bib.bib20)）和子平方算法（Beltagy 等，[2020](#bib.bib4)；Choromanski
    等，[2020](#bib.bib9)）并不能减轻 KV 缓存的内存负担。
- en: 'In our work, we aim to achieve a lossless compression of the KV cache of LLMs,
    thus retaining their performance while reducing their memory load. To this end,
    we propose Dynamic Memory Compression (DMC). As shown in [Figure 1](#S1.F1 "In
    1 Introduction ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated
    Inference"), during every time step, DMC decides whether to append the current
    key and value representations to the cache or to perform a weighted average of
    them with the top item on the cache. Note that memory grows sub-linearly in DMC,
    which therefore falls halfway between vanilla Transformers and state space language
    models (Fu et al., [2023](#bib.bib13); Gu & Dao, [2023](#bib.bib15)), where memory
    is constant.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，我们旨在实现 LLM 的 KV 缓存无损压缩，从而在减少内存负担的同时保持其性能。为此，我们提出了动态内存压缩（DMC）。如 [图 1](#S1.F1
    "在 1 引言 ‣ 动态内存压缩：为加速推理改装 LLMs") 所示，在每个时间步，DMC 决定是将当前的键和值表示附加到缓存中，还是将它们与缓存中的顶部项目进行加权平均。请注意，DMC
    中的内存增长是次线性的，因此它介于普通 Transformers 和状态空间语言模型（Fu 等，[2023](#bib.bib13)；Gu & Dao，[2023](#bib.bib15)）之间，其中内存是常数。
- en: In our experiments, we equip pre-existing LLMs—such as Llama 2 (Touvron et al.,
    [2023](#bib.bib32)) 7B, 13B, and 70B—with DMC by retrofitting them on a negligible
    percentage of the original pre-training data (~2% for 2$\times$.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们通过在原始预训练数据（~2% 对于 2$\times$）上进行微调，为现有的 LLMs（如 Llama 2 (Touvron 等，[2023](#bib.bib32))
    7B、13B 和 70B）配备了 DMC。
- en: We verify that KV cache compression translates into more efficient generation
    in practice. We measure that DMC $4\times$ increases the inference throughput
    between 340% and 370% for Llama 2 7B and 13B on NVIDIA H100 or A100 GPUs without
    loss in performance. In fact, it allows us to fit larger batches and longer sequences
    into a given memory budget. Finally, compression schemata learned by DMC provide
    insights into the internal structure of the LLMs, revealing a preference for compressing
    heads in higher layers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们验证了 KV 缓存压缩在实践中转化为更高效的生成。我们测量发现，DMC $4\times$ 在 NVIDIA H100 或 A100 GPU 上将
    Llama 2 7B 和 13B 的推理吞吐量提高了 340% 到 370%，且没有性能损失。事实上，这使我们能够在给定的内存预算中适应更大的批量和更长的序列。最后，DMC
    学到的压缩方案提供了对 LLM 内部结构的见解，揭示了更倾向于压缩高层的头部。
- en: 2 Background
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 Multi-Head Self-Attention
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 多头自注意力
- en: 'Let $X=(\mathbf{x}_{1},\ldots,\mathbf{x}_{n})\in\mathbb{R}^{n\times d}$:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让 $X=(\mathbf{x}_{1},\ldots,\mathbf{x}_{n})\in\mathbb{R}^{n\times d}$：
- en: '|  | $\displaystyle Q^{h}$ |  | (1) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{h}$ |  | (1) |'
- en: '|  | $\displaystyle K^{h}$ |  | (2) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K^{h}$ |  | (2) |'
- en: '|  | $\displaystyle V^{h}$ |  | (3) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle V^{h}$ |  | (3) |'
- en: The attention scores and outputs for the $i$-th token are then computed as
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算第 $i$ 个标记的注意力分数和输出
- en: '|  | $1$2 |  | (4) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: 'Finally, the outputs from all heads are concatenated and linearly transformed
    to produce the final output $O\in\mathbb{R}^{n\times d}$:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，所有头部的输出被连接并线性变换，以生成最终输出 $O\in\mathbb{R}^{n\times d}$：
- en: '|  | $O=(W_{o}[\mathbf{o}_{1}^{1},\ldots,\mathbf{o}_{1}^{n_{h}}],\ldots,W_{o}[\mathbf{o}_{n}^{1},\ldots,\mathbf{o}_{n}^{n_{h}}])$
    |  | (5) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $O=(W_{o}[\mathbf{o}_{1}^{1},\ldots,\mathbf{o}_{1}^{n_{h}}],\ldots,W_{o}[\mathbf{o}_{n}^{1},\ldots,\mathbf{o}_{n}^{n_{h}}])$
    |  | (5) |'
- en: where $W_{o}\in\mathbb{R}^{d\times d}$.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{o}\in\mathbb{R}^{d\times d}$。
- en: 2.2 KV Caching During Inference
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 推理中的KV缓存
- en: 'In a Transformer decoder, the generation of sequences is auto-regressive: each
    new token is predicted based on the previously generated ones. To avoid redundant
    computation, it is common to store the keys and values of previously computed
    tokens in the KV cache. For each time step $i$:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在Transformer解码器中，序列的生成是自回归的：每个新令牌是基于之前生成的令牌来预测的。为了避免重复计算，通常会将之前计算过的令牌的键和值存储在KV缓存中。对于每个时间步
    $i$：
- en: '|  | $\displaystyle K^{h}$ |  | (6) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K^{h}$ |  | (6) |'
- en: '|  | $\displaystyle V^{h}$ |  | (7) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle V^{h}$ |  | (7) |'
- en: Note that this process is not necessary for queries as only the query for the
    current token $\mathbf{x}_{i}$ is needed at each inference time step.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此过程对查询而言不是必需的，因为每次推理时只需要当前令牌 $\mathbf{x}_{i}$ 的查询。
- en: As a consequence, the KV cache grows linearly with each new token. This progressive
    expansion leads to substantial memory consumption and increased latency, especially
    for long input sequences. This issue is further exacerbated when serving multiple
    requests concurrently, as each inference process requires its own KV cache, significantly
    straining the system’s resources.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，KV缓存会随着每个新令牌线性增长。这种渐进扩展导致了大量的内存消耗和增加的延迟，尤其是在长输入序列的情况下。当同时处理多个请求时，这一问题会进一步加剧，因为每个推理过程都需要其自己的KV缓存，从而大大加重了系统的资源负担。
- en: '3 Method: Dynamic Memory Compression'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法：动态内存压缩
- en: 'Inference with LLMs tends to be memory bound rather than compute bound, as
    we explain in [Appendix A](#A1 "Appendix A Memory-Bound Operations in Transformers
    ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference") in
    detail. This means that latency scales linearly with the size of the KV cache.
    We tackle the problem of reducing the size of KV cache which brings two immediate
    benefits (Zhang et al., [2023](#bib.bib38)): 1) it lowers the latency of auto-regressive
    generation, and 2) increases throughput by saving GPU memory and allowing for
    larger batch sizes or sequence lengths.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLMs的推理往往是受内存限制而非计算限制的，正如我们在[附录A](#A1 "附录A Transformer中的内存绑定操作 ‣ 动态内存压缩：加速推理的LLM改造")中详细解释的那样。这意味着延迟与KV缓存的大小成线性关系。我们解决了减少KV缓存大小的问题，这带来了两个直接的好处（Zhang等，
    [2023](#bib.bib38)）：1）降低了自回归生成的延迟，2）通过节省GPU内存并允许更大的批量大小或序列长度来增加吞吐量。
- en: In this section we introduce Dynamic Memory Compression (DMC), a simple and
    inexpensive method for on-line compression of the KV cache at inference time.
    In what follows, we first describe the inference-time operation of DMC, which
    constitutes our end goal. Next, we illustrate how to teach a pre-trained LLM such
    behavior through short, continued pre-training.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了动态内存压缩（DMC），这是一种简单且经济的在线KV缓存压缩方法。在接下来的内容中，我们首先描述DMC在推理时间的操作，这是我们的最终目标。接下来，我们说明如何通过短期的持续预训练来教会预训练的LLM这种行为。
- en: 3.1 Inference
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 推理
- en: 'Consider the forward pass of an attention layer during auto-regressive inference
    ([Section 2.1](#S2.SS1 "2.1 Multi-Head Self-Attention ‣ 2 Background ‣ Dynamic
    Memory Compression: Retrofitting LLMs for Accelerated Inference")). In a vanilla
    Transformer, at every time step $t$.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在自回归推理中的注意力层的前向传播（[第2.1节](#S2.SS1 "2.1 多头自注意力 ‣ 2 背景 ‣ 动态内存压缩：加速推理的LLM改造")）。在普通的Transformer中，在每个时间步
    $t$。
- en: Algorithm 1 Single-head KV cache update with Dynamic Memory Compression (DMC)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 单头KV缓存更新与动态内存压缩（DMC）
- en: KV-Update$K,V,\mathbf{q}_{t},\mathbf{k}_{t},\mathbf{v}_{t}$\Procedure\State\State\If\Comment\State\State\State\Else\Comment\State\State\State\EndIf\State\State\State\EndProcedure
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: KV-更新$K,V,\mathbf{q}_{t},\mathbf{k}_{t},\mathbf{v}_{t}$\Procedure\State\State\If\Comment\State\State\State\Else\Comment\State\State\State\EndIf\State\State\State\EndProcedure
- en: Based on $\alpha_{t}$ was predicted.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 $\alpha_{t}$ 的预测。
- en: In fact, the $\alpha$ as the Compression Ratio (CR).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，$\alpha$ 是压缩比（CR）。
- en: 'Finally, multi-head self-attention is calculated similarly to vanilla Transformer
    using KV cache sequences, with the exception that KV sequences for different heads
    might have different lengths. [Algorithm 1](#alg1 "In 3.1 Inference ‣ 3 Method:
    Dynamic Memory Compression ‣ Dynamic Memory Compression: Retrofitting LLMs for
    Accelerated Inference") is applied to every MHSA layer and head independently.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，使用 KV 缓存序列以类似于原始 Transformer 的方式计算多头自注意力，唯一的例外是不同头的 KV 序列可能具有不同的长度。[算法 1](#alg1
    "在 3.1 推理 ‣ 3 方法：动态内存压缩 ‣ 动态内存压缩：为加速推理调整 LLMs") 被独立应用于每个 MHSA 层和头部。
- en: 3.2 Training
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 训练
- en: The DMC inference algorithm switches between accumulating and appending tokens
    to the KV cache. In order to endow LLMs with DMC, we continue pre-training them
    on a negligible amount of their pre-training data, gradually increasing the compression
    rate towards a target. However, this poses serious challenges. First, we opt for
    end-to-end learning via gradient descent and continuous relaxation of the decision
    variables. As a result, we have to define an operation for KV cache updating when
    $0<\alpha<1$ is not reduced through compression during training; rather, all intermediate
    states of keys and values are explicitly kept in memory and an auxiliary (gradually
    discretized) mask regulates query interactions. We elaborate on our solutions
    to these challenges below.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: DMC 推理算法在累积和附加令牌到 KV 缓存之间切换。为了使 LLMs 具备 DMC，我们继续在其微不足道的预训练数据上进行预训练，逐渐提高压缩率达到目标。然而，这带来了严重挑战。首先，我们选择通过梯度下降和决策变量的连续放松进行端到端学习。因此，当
    $0<\alpha<1$ 在训练期间没有通过压缩减少时，我们必须定义一个 KV 缓存更新的操作；相反，所有键和值的中间状态都显式保存在内存中，辅助（逐渐离散化的）掩码调节查询交互。我们在下面详细阐述了我们对这些挑战的解决方案。
- en: Gradient Estimation for Discrete Decisions
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 离散决策的梯度估计
- en: The decision whether to accumulate or append at inference time is a discrete
    one; however, rounding $\mathrm{sigmoid}(\mathbf{k}[0])$ to the nearest integer
    for training would result in a non-differentiable operation with zero gradients.
    Hence, we resort to stochastic reparametrization of the decision variable during
    training
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 推理时是否累积或附加是一个离散的决策；然而，将 $\mathrm{sigmoid}(\mathbf{k}[0])$ 四舍五入到最近整数用于训练会导致不可微分的操作且梯度为零。因此，我们在训练过程中采用了决策变量的随机重参数化。
- en: '|  | $\alpha_{t}\sim\text{Gumbel-sigmoid}(\mathbf{k}[0]-c,\tau)\in[0,1],$ |  |
    (8) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{t}\sim\text{Gumbel-sigmoid}(\mathbf{k}[0]-c,\tau)\in[0,1],$ |  |
    (8) |'
- en: where $\tau$. This ensures that DMC initially performs no compression and starts
    the training behaving just like the vanilla Transformer.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tau$。这确保了 DMC 在最初不执行压缩，并且在训练时的行为与原始 Transformer 相同。
- en: Partial accumulations
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 部分累积
- en: 'As we relax our discrete decisions, we now must define a mechanism to update
    the KV cache that generalizes [Algorithm 1](#alg1 "In 3.1 Inference ‣ 3 Method:
    Dynamic Memory Compression ‣ Dynamic Memory Compression: Retrofitting LLMs for
    Accelerated Inference") to continuous $\alpha$ as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们放松离散决策，现在必须定义一个机制来更新 KV 缓存，使其可以推广 [算法 1](#alg1 "在 3.1 推理 ‣ 3 方法：动态内存压缩 ‣
    动态内存压缩：为加速推理调整 LLMs") 到连续的 $\alpha$：
- en: '|  | $\displaystyle z_{0}$ |  | (9) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z_{0}$ |  | (9) |'
- en: '|  | $\displaystyle{\mathbf{k}}_{0}$ |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathbf{k}}_{0}$ |  |'
- en: '|  | $\displaystyle{\mathbf{v}}_{0}$ |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathbf{v}}_{0}$ |  |'
- en: 'Note that when $\alpha\in\{0,1\}$, [Equation 9](#S3.E9 "In Partial accumulations
    ‣ 3.2 Training ‣ 3 Method: Dynamic Memory Compression ‣ Dynamic Memory Compression:
    Retrofitting LLMs for Accelerated Inference") defaults to [Algorithm 1](#alg1
    "In 3.1 Inference ‣ 3 Method: Dynamic Memory Compression ‣ Dynamic Memory Compression:
    Retrofitting LLMs for Accelerated Inference").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当 $\alpha\in\{0,1\}$ 时，[方程 9](#S3.E9 "在部分累积 ‣ 3.2 训练 ‣ 3 方法：动态内存压缩 ‣ 动态内存压缩：为加速推理调整
    LLMs") 默认为[算法 1](#alg1 "在 3.1 推理 ‣ 3 方法：动态内存压缩 ‣ 动态内存压缩：为加速推理调整 LLMs")。
- en: Intermediate compression steps
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 中间压缩步骤
- en: 'Aside from key and value computations shown in [Equation 9](#S3.E9 "In Partial
    accumulations ‣ 3.2 Training ‣ 3 Method: Dynamic Memory Compression ‣ Dynamic
    Memory Compression: Retrofitting LLMs for Accelerated Inference"), the rest of
    the forward pass can be performed in parallel for all tokens in the sequence.
    Nonetheless, this creates a mismatch between training and evaluation, since all
    intermediate states of keys and values are accessible during self-attention.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 除了[方程 9](#S3.E9 "在部分累积 ‣ 3.2 训练 ‣ 3 方法：动态内存压缩 ‣ 动态内存压缩：为加速推理改造 LLMs")中展示的键和值计算外，其余的前向传播可以对序列中的所有标记并行进行。然而，这会造成训练和评估之间的不匹配，因为在自注意力期间，所有键和值的中间状态都是可以访问的。
- en: 'To illustrate this issue, consider the example of a KV cache during DMC inference
    shown in [Figure 2](#S3.F2 "In Intermediate compression steps ‣ 3.2 Training ‣
    3 Method: Dynamic Memory Compression ‣ Dynamic Memory Compression: Retrofitting
    LLMs for Accelerated Inference") for the sequence of decision scores $\alpha_{1:5}=(1,1,0,1,0)$
    values, and exactly corresponds to the inference time query-to-key attendance
    pattern.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个问题，请考虑[图 2](#S3.F2 "在中间压缩步骤 ‣ 3.2 训练 ‣ 3 方法：动态内存压缩 ‣ 动态内存压缩：为加速推理改造 LLMs")中展示的
    DMC 推理过程中的 KV 缓存示例，该示例对应于决策分数 $\alpha_{1:5}=(1,1,0,1,0)$ 的查询到键的注意力模式。
- en: '![Refer to caption](img/b52b033f32aad112fb518057da810400.png)![Refer to caption](img/6a852604c0d7f2c0ccd28d0bf087b0e2.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b52b033f32aad112fb518057da810400.png)![参见标题](img/6a852604c0d7f2c0ccd28d0bf087b0e2.png)'
- en: 'Figure 2: An example of KV cache growth in DMC during inference (left). During
    training (right), we retain all intermediate states seen during inference and
    gradually block access to some of them (gray arrows)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：DMC 推理过程中的 KV 缓存增长示例（左）。在训练过程中（右），我们保留了推理过程中看到的所有中间状态，并逐渐阻止访问其中的一些状态（灰色箭头）
- en: $$\blockarray{cccccc}&amp;{\bf k_{0}}{\bf k_{1}}{\bf k_{2}}\dots{\bf k_{n}}\\
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: $$\blockarray{cccccc}&amp;{\bf k_{0}}{\bf k_{1}}{\bf k_{2}}\dots{\bf k_{n}}\\
- en: \block{c[ccccc]}{\bf q_{0}}0-\infty-\infty\dots-\infty\\
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: \block{c[ccccc]}{\bf q_{0}}0-\infty-\infty\dots-\infty\\
- en: '{\bf q_{1}}\log(1\shortminus\alpha_{1})0-\infty-\infty\\'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '{\bf q_{1}}\log(1\shortminus\alpha_{1})0-\infty-\infty\\'
- en: '{\bf q_{2}}\log(1\shortminus\alpha_{1})\log(1\shortminus\alpha_{2})0-\infty\\'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '{\bf q_{2}}\log(1\shortminus\alpha_{1})\log(1\shortminus\alpha_{2})0-\infty\\'
- en: \vdots\vdots\ddots\vdots\\
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\vdots\ddots\vdots\\
- en: '{\bf q_{n}}\log(1\shortminus\alpha_{1})\log(1\shortminus\alpha_{2})\log(1\shortminus\alpha_{3})\dots
    0\\'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '{\bf q_{n}}\log(1\shortminus\alpha_{1})\log(1\shortminus\alpha_{2})\log(1\shortminus\alpha_{3})\dots
    0\\'
- en: $$
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: $$
- en: 'Figure 3: Additive mask applied during training to the the normalized attention
    scores to block queries from attending intermediate KV states (as well as future
    states).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在训练过程中应用的加性掩码，用于阻止查询访问中间 KV 状态（以及未来状态）的归一化注意力得分。
- en: Training objective
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练目标
- en: 'The model is incentivized to compress the KV cache to a certain CR, and thus
    increase the predicted $\alpha$:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 模型被激励将 KV 缓存压缩到一定的 CR，从而增加预测的 $\alpha$：
- en: '|  | $1$2 |  | (10) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (10) |'
- en: 'It is added to the language modeling loss term $\ell_{\text{LM}}=-\sum_{t=1}^{n}\log
    p_{\boldsymbol{\theta}}(\mathbf{x}_{t}\mid\mathbf{x}_{<t})$, with the final objective
    of the training being:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 它被添加到语言建模损失项 $\ell_{\text{LM}}=-\sum_{t=1}^{n}\log p_{\boldsymbol{\theta}}(\mathbf{x}_{t}\mid\mathbf{x}_{<t})$
    中，训练的最终目标是：
- en: '|  | $\text{arg min}_{\boldsymbol{\theta}}\,\ell_{\text{LM}}+\ell_{\text{CR}}.$
    |  | (11) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{arg min}_{\boldsymbol{\theta}}\,\ell_{\text{LM}}+\ell_{\text{CR}}.$
    |  | (11) |'
- en: Importantly, the training procedure is designed for slowly ramping up the desired
    compression rate and stopping at will. This enables producing a series of models
    with different compression rates within a single run. It follows that all hyperparameters,
    like Gumbel-sigmoid sampling temperature and learning rate, are not decayed and
    remain constant throughout training.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，训练过程被设计为缓慢提高所需的压缩率，并在需要时停止。这使得在一次运行中产生一系列具有不同压缩率的模型成为可能。因此，所有超参数，如 Gumbel-sigmoid
    采样温度和学习率，都不会衰减，并在整个训练过程中保持不变。
- en: 3.3 Practical Considerations
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 实际考虑
- en: Implementing a variable-length cache without padding
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现一个无填充的可变长度缓存
- en: 'DMC allows every head to learn a custom compression, which results in KV cache
    sequences with variable lengths across heads. One could implement this cache naïvely
    with padded tensors. Foreshadowing our results in [Section 5.2](#S5.SS2 "5.2 Throughput
    and Latency Measurements ‣ 5 Results ‣ Dynamic Memory Compression: Retrofitting
    LLMs for Accelerated Inference"), however, the largest efficiency gains during
    inference stem from the reduced memory footprint, which allows us to increase
    the batch size and dramatically improve throughput. In order to get these benefits,
    the memory cannot be wasted on storing KV cache as padded tensors.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 'DMC 允许每个头部学习自定义压缩，这导致 KV 缓存序列在各个头部具有可变的长度。可以用填充张量天真地实现此缓存。然而，正如我们在 [第 5.2 节](#S5.SS2
    "5.2 Throughput and Latency Measurements ‣ 5 Results ‣ Dynamic Memory Compression:
    Retrofitting LLMs for Accelerated Inference") 中的结果所预示，推理过程中的最大效率提升来自减少的内存占用，这使我们可以增加批量大小并显著提高吞吐量。为了获得这些好处，内存不能浪费在存储填充张量的
    KV 缓存上。'
- en: Thus, we provide a simple custom implementation of attention in PyTorch, building
    on FlashAttention (Dao et al., [2022](#bib.bib12)) and PagedAttention (Kwon et al.,
    [2023](#bib.bib20)), in which the heads can learn wildly different compression
    rates but which does not require padding. As an ablation, we also study a constrained
    variant (DMC-C) in which we force the heads in a given layer to maintain similar
    compression rates, which minimizes the padding necessary in naïve attention implementations;
    however, it significantly constrains the learned compression schema and leads
    to worse model quality.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们提供了一个基于 FlashAttention（Dao et al., [2022](#bib.bib12)）和 PagedAttention（Kwon
    et al., [2023](#bib.bib20)）的简单自定义 PyTorch 实现，其中头部可以学习 wildly 不同的压缩率，但不需要填充。作为消融实验，我们还研究了一种受约束的变体（DMC-C），在这种变体中，我们强制同一层中的头部保持类似的压缩率，这减少了天真注意力实现中所需的填充量；然而，它显著限制了学习到的压缩方案，导致模型质量下降。
- en: Window grouping approximation
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 窗口分组近似
- en: 'The calculation of partial accumulations, during training of DMC models [Equation 9](#S3.E9
    "In Partial accumulations ‣ 3.2 Training ‣ 3 Method: Dynamic Memory Compression
    ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"), for
    a sequence of $n$ positions.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '在训练 DMC 模型时，部分累积的计算 [公式 9](#S3.E9 "In Partial accumulations ‣ 3.2 Training
    ‣ 3 Method: Dynamic Memory Compression ‣ Dynamic Memory Compression: Retrofitting
    LLMs for Accelerated Inference")，用于一个 $n$ 位置的序列。'
- en: 4 Experimental Setup
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验设置
- en: Baselines
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准
- en: 'In our experiments, we evaluate strategies to retrofit a state-of-the-art Large
    Language Model (LLM), Llama 2 (Touvron et al., [2023](#bib.bib32)),⁴⁴4Obtained
    from [https://huggingface.co/meta-llama](https://huggingface.co/meta-llama). into
    a more efficient model across various sizes: 7B, 13B, and 70B. In addition to
    comparing the downstream performance of DMC with the original model, we also use
    Grouped Query Attention (GQA) as a main baseline, as it constitutes the most widespread
    strategy to ensure KV cache efficiency (Jiang et al., [2023](#bib.bib18)).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们评估了将最先进的大型语言模型（LLM）Llama 2（Touvron et al., [2023](#bib.bib32)），⁴⁴4从
    [https://huggingface.co/meta-llama](https://huggingface.co/meta-llama) 获取的，转化为更高效模型的策略，涵盖不同的规模：7B、13B
    和 70B。除了比较 DMC 与原始模型的下游性能外，我们还使用了分组查询注意力（GQA）作为主要基线，因为它是确保 KV 缓存效率的最广泛使用策略（Jiang
    et al., [2023](#bib.bib18)）。
- en: Checkpoint adaptation
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 检查点适配
- en: 'To equip the original Llama 2 with GQA, we perform the standard checkpoint
    conversion proposed by Ainslie et al. ([2023](#bib.bib1)): the key and value projection
    matrices are split by head. Then the resulting sub-matrices corresponding to heads
    in the same group are merged. Note that this results in a fixed CR during training.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为原始的 Llama 2 配备 GQA，我们执行了 Ainslie 等人（[2023](#bib.bib1)）提出的标准检查点转换：键和值投影矩阵按头部进行拆分。然后，将同一组中的头部对应的子矩阵进行合并。请注意，这在训练过程中会导致固定的
    CR。
- en: 'As for DMC, we avoid the introduction of new parameters by re-purposing the
    first dimension from both the $\mathbf{k}_{t}$ to 0 according to the following
    formula:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 DMC，我们通过将第一个维度从 $\mathbf{k}_{t}$ 到 0 重新利用，避免了引入新参数，具体公式如下：
- en: '|  | $\displaystyle\mathbf{q}_{t}[0]$ |  | (12) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{q}_{t}[0]$ |  | (12) |'
- en: '|  | $\displaystyle\mathbf{k}_{t}[0]$ |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{k}_{t}[0]$ |  |'
- en: where $t$ in order to perform no compression at the start.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t$ 以便在开始时不进行压缩。
- en: Training hyperparameters
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练超参数
- en: 'We follow the training hyperparameters outlined by Touvron et al. ([2023](#bib.bib32)).
    We employ the AdamW optimizer with parameters $\beta_{1}=0.9$ for the 70B model.
    Finally, we set the window size ([Section 3.3](#S3.SS3.SSS0.Px2 "Window grouping
    approximation ‣ 3.3 Practical Considerations ‣ 3 Method: Dynamic Memory Compression
    ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference")) to
    12, and keep the Gumbel-sigmoid temperature constant at 0.1 throughout the entire
    training.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循了Touvron等人（[2023](#bib.bib32)）概述的训练超参数。对于70B模型，我们使用AdamW优化器，参数$\beta_{1}=0.9$。最后，我们将窗口大小（[第3.3节](#S3.SS3.SSS0.Px2
    "窗口分组近似 ‣ 3.3 实际考虑 ‣ 3 方法：动态内存压缩 ‣ 动态内存压缩：为加速推理改造LLMs")）设置为12，并在整个训练过程中保持Gumbel-sigmoid温度为0.1。
- en: Training schedule
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练计划
- en: 'The volume of data for continued pre-training of DMC is contingent on the targeted
    KV cache compression ratio; a larger CR necessitates more data. We use a training
    schedule with 24B, 48B, and 72B tokens for training to $2\times$ compression,
    respectively. In [Appendix D](#A4 "Appendix D Training Ablations ‣ Dynamic Memory
    Compression: Retrofitting LLMs for Accelerated Inference") we include an ablation
    where we use a schedule with twice less data.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DMC的持续预训练数据量取决于目标KV缓存压缩比；较大的压缩比需要更多的数据。我们使用了24B、48B和72B的训练计划，分别对应$2\times$压缩。在[附录D](#A4
    "附录 D 训练消融 ‣ 动态内存压缩：为加速推理改造LLMs")中，我们包含了一个使用数据量减少一半的计划的消融实验。
- en: 'For DMC, we discovered that the annealing strategy was crucial. Starting the
    training without compression and its measured increase helps to preserve the original
    perplexity. Through extensive ablations (see [Appendix D](#A4 "Appendix D Training
    Ablations ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference")),
    we found that any significant increase of perplexity, even if recovered during
    continued pre-training, prevents the model from regaining its performance on downstream
    tasks. The target CR is linearly increased from $1\times$..'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DMC，我们发现退火策略至关重要。无压缩开始训练及其测量增加有助于保持原始困惑度。通过广泛的消融实验（见[附录D](#A4 "附录 D 训练消融 ‣
    动态内存压缩：为加速推理改造LLMs")），我们发现即使在持续预训练期间恢复的任何显著困惑度增加，也会阻止模型在下游任务中重新获得性能。目标压缩比从$1\times$线性增加。
- en: 'Upon achieving the target compression ratio with the DMC model, we initiate
    a final solidifying phase wherein we: 1) continue up-training for an additional
    8B tokens, 2) maintain a fixed compression ratio, and 3) implement a cosine learning
    rate schedule, annealing it down to 10% of the initial value. This phase aims
    at stabilizing the model with a specific, fixed compression ratio.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在DMC模型达到目标压缩比后，我们启动最后的巩固阶段，其中：1）继续训练额外的8B tokens，2）保持固定的压缩比，3）实施余弦学习率计划，将其降低至初始值的10%。此阶段旨在以特定的、固定的压缩比稳定模型。
- en: Evaluation
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估
- en: 'Following Touvron et al. ([2023](#bib.bib32)), we evaluate models on a series
    of downstream tasks, including MMLU (Hendrycks et al., [2021](#bib.bib16)) for
    factuality, HumanEval (Chen et al., [2021](#bib.bib7)) for Python code generation,
    and several question-answering datasets for common-sense reasoning: PIQA (Bisk
    et al., [2020](#bib.bib5)), BoolQ (Clark et al., [2019](#bib.bib10)), Arc-C and
    Arc-E (Clark et al., [2018](#bib.bib11)), HellaSwag (Zellers et al., [2019](#bib.bib36)),
    and WinoGrande (Sakaguchi et al., [2021](#bib.bib29)). We report the 5-shot performance
    on MMLU, average pass@1 scores for HumanEval, and average 0-shot performance on
    common-sense benchmarks (CS-QA). For pass@1 scores we use a temperature of 0.1
    and nucleus sampling (Holtzman et al., [2019](#bib.bib17)) with top-p $=$ 0.95.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Touvron等人（[2023](#bib.bib32)），我们对模型进行了一系列下游任务的评估，包括用于真实性的MMLU（Hendrycks等人，[2021](#bib.bib16)）、用于Python代码生成的HumanEval（Chen等人，[2021](#bib.bib7)），以及用于常识推理的几个问答数据集：PIQA（Bisk等人，[2020](#bib.bib5)）、BoolQ（Clark等人，[2019](#bib.bib10)）、Arc-C和Arc-E（Clark等人，[2018](#bib.bib11)）、HellaSwag（Zellers等人，[2019](#bib.bib36)）和WinoGrande（Sakaguchi等人，[2021](#bib.bib29)）。我们报告了MMLU的5-shot表现、HumanEval的平均pass@1分数以及常识基准（CS-QA）的平均0-shot表现。对于pass@1分数，我们使用了0.1的温度和顶点采样（Holtzman等人，[2019](#bib.bib17)），其中top-p=$0.95。
- en: 5 Results
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: '| Scale | Method | CR | MMLU | CS-QA | Human Eval |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 规模 | 方法 | 压缩比 | MMLU | CS-QA | 人工评估 |'
- en: '| 7B | – | – | 44.6 | 70.5 | 14.0 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 7B | – | – | 44.6 | 70.5 | 14.0 |'
- en: '| GQA | 2$\times$ | 39.8 | 68.9 | 12.8 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| GQA | 2$\times$ | 39.8 | 68.9 | 12.8 |'
- en: '| DMC | 45.2 | 70.8 | 15.2 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| DMC | 45.2 | 70.8 | 15.2 |'
- en: '| GQA | 4$\times$ | 34.7 | 68.3 | 14.0 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GQA | 4$\times$ | 34.7 | 68.3 | 14.0 |'
- en: '| DMC | 43.9 | 70.2 | 16.5 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| DMC | 43.9 | 70.2 | 16.5 |'
- en: '| 13B | – | – | 54.5 | 73.5 | 17.5 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 13B | – | – | 54.5 | 73.5 | 17.5 |'
- en: '| GQA | 2$\times$ | 50.2 | 72.7 | 15.9 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| GQA | 2$\times$ | 50.2 | 72.7 | 15.9 |'
- en: '| DMC | 54.8 | 74.2 | 20.7 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| DMC | 54.8 | 74.2 | 20.7 |'
- en: '| GQA | 4$\times$ | 48.6 | 72.2 | 16.5 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| GQA | 4$\times$ | 48.6 | 72.2 | 16.5 |'
- en: '| DMC | 54.2 | 73.2 | 22.0 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| DMC | 54.2 | 73.2 | 22.0 |'
- en: '| 70B^∗ | – | 8$\times^{*}$ | 68.8 | 78.0 | 29.6 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 70B^∗ | – | 8$\times^{*}$ | 68.8 | 78.0 | 29.6 |'
- en: '| DMC | 16$\times^{*}$ | 68.8 | 77.9 | 29.9 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| DMC | 16$\times^{*}$ | 68.8 | 77.9 | 29.9 |'
- en: 'Table 1: MMLU accuracy (Acc.), Commonsense Question Answering (CS-QA), Exact
    Match (EM), and Human-Eval Pass@1 for several scales (7B, 13B, and 70B) and compression
    rates (CRs; $1\times$ .'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同规模（7B、13B 和 70B）和压缩率（CRs; $1\times$）的 MMLU 准确率（Acc.）、常识问答（CS-QA）、完全匹配（EM）和
    Human-Eval Pass@1。
- en: '![Refer to caption](img/8db9c87179510078aa59c407b7f5bd41.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8db9c87179510078aa59c407b7f5bd41.png)'
- en: 'Figure 4: Sample efficiency of DMC and GQA. Horizontal lines correspond to
    the performance of the original Llama 2\. Every DMC model was trained first with
    increasing CR, then with constant CR for the last 2K steps (marked with $\star$).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：DMC 和 GQA 的样本效率。水平线对应于原始 Llama 2 的性能。每个 DMC 模型首先用逐渐增加的 CR 进行训练，然后在最后的 2K
    步骤中使用恒定 CR（标记为 $\star$）。
- en: 5.1 Main Results
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 主要结果
- en: 'We report the performance of the original LLM (equivalent to $1\times$ CR)
    and efficient variants (DMC and GQA) in [Table 1](#S5.T1 "In 5 Results ‣ Dynamic
    Memory Compression: Retrofitting LLMs for Accelerated Inference"). For the original
    LLM we use results reproduced in our codebase as described in [Appendix B](#A2
    "Appendix B Replicating the Original Results ‣ Dynamic Memory Compression: Retrofitting
    LLMs for Accelerated Inference").'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告了原始 LLM（相当于 $1\times$ CR）和高效变体（DMC 和 GQA）的性能，见 [表 1](#S5.T1 "在 5 结果 ‣ 动态内存压缩：为加速推理改造
    LLM")。对于原始 LLM，我们使用在我们的代码库中重现的结果，如 [附录 B](#A2 "附录 B 重现原始结果 ‣ 动态内存压缩：为加速推理改造 LLM")
    中所述。
- en: '![Refer to caption](img/8c0b04ab73dbef40cf5da26ef8bd4fca.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8c0b04ab73dbef40cf5da26ef8bd4fca.png)'
- en: (a) Inference throughput averaged over the generation of 1K tokens with 3K tokens
    of context (up to 4K in total). On the x-axis, we show the maximum batch size
    that fits in memory on a single GPU (7B and 13B) or two GPUs with tensor parallelism
    (70B) for the Vanilla, DMC 2$\times$ models.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 推理吞吐量的平均值，基于生成 1K 令牌的情况，背景为 3K 令牌（总共最多 4K）。x 轴显示了适合单个 GPU（7B 和 13B）或两个 GPU
    使用张量并行（70B）的最大批量大小，用于 Vanilla 和 DMC 2$\times$ 模型。
- en: '![Refer to caption](img/7b8f6f86979e755ec30c494aa7af8ca4.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7b8f6f86979e755ec30c494aa7af8ca4.png)'
- en: (b) Latency of next token generation. Solid lines denote measurements with the
    maximum batch size that fits on a single GPU. Dotted lines denote DMC 4$\times$
    with the same batch size as the vanilla LLM.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 下一个令牌生成的延迟。实线表示在单个 GPU 上适合的最大批量大小的测量。虚线表示 DMC 4$\times$，其批量大小与普通 LLM 相同。
- en: 'Figure 5: Efficiency measurements with the Megatron-LM framework on NVIDIA
    A100 80GB and H100 GPUs.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在 NVIDIA A100 80GB 和 H100 GPU 上使用 Megatron-LM 框架的效率测量。
- en: DMC vs Original First, comparing DMC with the original LLM, we note that it
    even increases the performance in MMLU and CS-QA at $2\times$ CR. Overall, the
    fact that DMC is in general close or superior to the original performance makes
    it suitable as a drop-in replacement for KV caching to achieve higher inference
    efficiency.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: DMC 与原始模型的对比 首先，将 DMC 与原始 LLM 进行比较时，我们注意到它甚至在 $2\times$ CR 下提高了 MMLU 和 CS-QA
    的性能。总体来看，DMC 通常接近或优于原始性能，使其作为 KV 缓存的直接替代方案，能够实现更高的推理效率。
- en: 'DMC vs GQA Moreover, [Table 1](#S5.T1 "In 5 Results ‣ Dynamic Memory Compression:
    Retrofitting LLMs for Accelerated Inference") allows for comparing DMC with GQA
    for equivalent CRs ($2\times$ CR). For CS-QA and Human-Eval, on the other hand,
    we observe comparable gains over GQA across CRs and model scales. These findings
    illustrate that DMC should be preferred to GQA for retrofitting LLMs into variants
    that are more efficient at inference time.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: DMC 与 GQA 此外，[表 1](#S5.T1 "在 5 结果 ‣ 动态内存压缩：为加速推理改造 LLM") 允许我们比较具有等效 CR ($2\times$
    CR) 的 DMC 和 GQA。另一方面，对于 CS-QA 和 Human-Eval，我们观察到在 CR 和模型规模上，DMC 相对于 GQA 具有类似的增益。这些发现表明，DMC
    应优于 GQA，用于将 LLM 改造为在推理时更高效的变体。
- en: '70B: DMC and GQA However, many widely adopted LLMs were pre-trained with GQA
    which leads to the question of whether DMC and GQA can be used together to reap
    compounded benefits. To investigate this, we retrofit Llama 2 70B, which has been
    pre-trained with $8\times$ smaller than a vanilla LLM with neither GQA nor DMC.
    We observe that the performance remains unchanged, and conclude that DMC and GQA
    can be easily and successfully combined.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 70B：DMC和GQA 然而，许多广泛采用的LLM都是用GQA预训练的，这就引出了一个问题：DMC和GQA是否可以一起使用以获得复合效益。为此，我们对已经用$8\times$小于普通LLM且既没有GQA也没有DMC的Llama
    2 70B进行了重塑。我们观察到性能保持不变，得出结论：DMC和GQA可以轻松且成功地结合使用。
- en: 'DMC vs DMC-C Finally, in [Table 3](#A5.T3 "In In-layer Relaxation ‣ Appendix
    E DMC Ablations ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated
    Inference") in [Appendix E](#A5 "Appendix E DMC Ablations ‣ Dynamic Memory Compression:
    Retrofitting LLMs for Accelerated Inference"), we compare DMC with its Constrained
    variant DMC-C. In general, while remaining superior to GQA, DMC-C displays a significant
    degradation in several configurations, most notably 7B $4\times$ in MMLU compared
    to the ceiling. On the other hand, DMC recovers all performance loss in DMC-C.
    When used in combination with custom attention implementations which does not
    require excessive padding, standard DMC should therefore be vastly preferred,
    as it retains the original LLM performance while reaping the advantages in memory
    efficiency fully.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: DMC与DMC-C 最后，在[表3](#A5.T3 "在层内松弛 ‣ 附录E DMC消融 ‣ 动态内存压缩：加速推理的LLM重塑")中[附录E](#A5
    "附录E DMC消融 ‣ 动态内存压缩：加速推理的LLM重塑")，我们比较了DMC及其约束变体DMC-C。总体而言，虽然DMC-C在保持优于GQA的情况下，在多个配置中显示出显著的降级，特别是MMLU中7B
    $4\times$相比于上限。另一方面，DMC弥补了DMC-C中的所有性能损失。因此，当与不需要过度填充的自定义注意力实现一起使用时，标准DMC应该大大优于DMC-C，因为它保持了原始LLM的性能，同时完全获得了内存效率的优势。
- en: 'Sample Efficiency To shed light on the sample efficiency of DMC and GQA, we
    report their performance on MMLU, CS-QA, and Codex-Eval across retrofitting steps
    in [Figure 4](#S5.F4 "In 5 Results ‣ Dynamic Memory Compression: Retrofitting
    LLMs for Accelerated Inference"). First, for a target CR of $2\times$).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 样本效率 为了阐明DMC和GQA的样本效率，我们报告了它们在MMLU、CS-QA和Codex-Eval上的表现，见[图4](#S5.F4 "在5.结果
    ‣ 动态内存压缩：加速推理的LLM重塑")。首先，针对目标CR为$2\times$。
- en: 5.2 Throughput and Latency Measurements
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 吞吐量和延迟测量
- en: 'To verify whether increased CRs result in concrete efficiency gains, in [Figure 5](#S5.F5
    "In 5.1 Main Results ‣ 5 Results ‣ Dynamic Memory Compression: Retrofitting LLMs
    for Accelerated Inference") we present the performance properties of DMC, estimated
    within the NVIDIA Megatron-LM framework (Narayanan et al., [2021](#bib.bib24)).
    Specifically, we run measurements on a single GPU (NVIDIA A100 80GB SXM or H100
    SXM) in bfloat16 precision for Llama 7B and 13B. For Llama 70B, we run the same
    measurements on two GPUs of the same type with tensor parallelism. We feed the
    model with 2K tokens of English text, and generate additional 2K tokens in an
    auto-regressive manner to ensure that the model properly compresses its own generations.
    The reported throughput consists of the average over the last 1K tokens. We limit
    the sequence to 4K tokens to avoid issues with context length extrapolation, as
    this is the maximum length observed by Llama 2 during pre-training.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证增加CR是否会带来实际效率提升，在[图5](#S5.F5 "在5.1主要结果 ‣ 5结果 ‣ 动态内存压缩：加速推理的LLM重塑")中，我们展示了在NVIDIA
    Megatron-LM框架（Narayanan等，[2021](#bib.bib24)）内估计的DMC性能特性。具体来说，我们在单个GPU（NVIDIA A100
    80GB SXM或H100 SXM）上以bfloat16精度对Llama 7B和13B进行了测量。对于Llama 70B，我们在两个相同类型的GPU上进行了相同的测量，并采用张量并行处理。我们向模型输入2K个英文标记，并以自回归方式生成额外的2K个标记，以确保模型能够正确压缩自身生成的内容。报告的吞吐量为最后1K个标记的平均值。为了避免上下文长度外推问题，我们将序列限制为4K个标记，因为这是Llama
    2在预训练期间观察到的最大长度。
- en: '![Refer to caption](img/d7350fc7184fcc500a2109ef491bf539.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d7350fc7184fcc500a2109ef491bf539.png)'
- en: '![Refer to caption](img/9bcf7e3d4f4a08bfa98176d2d18f0b92.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9bcf7e3d4f4a08bfa98176d2d18f0b92.png)'
- en: 'Figure 6: Heatmaps of average compression rates across layers (X-axis) and
    heads (Y-axis). Heads are arranged from the highest compression to the lowest
    top-down for clarity.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：各层（X轴）和头（Y轴）的平均压缩率热图。头部从最高压缩到最低按自上而下排列，以便于查看。
- en: 'In order to maximize the utilization of the GPU, we increase the batch size
    to the maximum that fits into memory (see [Appendix A](#A1 "Appendix A Memory-Bound
    Operations in Transformers ‣ Dynamic Memory Compression: Retrofitting LLMs for
    Accelerated Inference") for details). The compression of the KV cache with DMC
    allows for substantial increases in batch size and thus significant throughput
    gains. As shown in [Figure 5(a)](#S5.F5.sf1 "In Figure 5 ‣ 5.1 Main Results ‣
    5 Results ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"),
    DMC $2\times$. This means that the efficiency boost observed in practice is very
    close to the theoretical limit. In addition, the extra memory saved with DMC could
    also be used to cache longer contexts.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化 GPU 的利用率，我们将批处理大小增加到适合内存的最大值（详细信息见[附录 A](#A1 "附录 A Transformers 中的内存限制操作
    ‣ 动态内存压缩：为加速推理改造 LLM")）。使用 DMC 压缩 KV 缓存可以大幅增加批处理大小，从而显著提高吞吐量。如[图 5(a)](#S5.F5.sf1
    "在图 5 ‣ 5.1 主要结果 ‣ 5 结果 ‣ 动态内存压缩：为加速推理改造 LLM")所示，DMC 实现了 $2\times$。这意味着实际观察到的效率提升非常接近理论极限。此外，DMC
    节省的额外内存还可以用于缓存更长的上下文。
- en: Finally, we compare the latency of the original LLM with DMC 4$\times$, the
    memory footprint of the KV cache is reduced and latency for longer contexts improves
    significantly.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们比较了原始 LLM 与 DMC 4$\times$ 的延迟，KV 缓存的内存占用减少，长上下文的延迟显著改善。
- en: 'While we acknowledge that the behavior of LLMs at inference time depends on
    a multitude of factors and implementation details, our measurements in [Figure 5](#S5.F5
    "In 5.1 Main Results ‣ 5 Results ‣ Dynamic Memory Compression: Retrofitting LLMs
    for Accelerated Inference") offer evidence that DMC increases throughput and reduces
    the latency of autoregressive generation with LLMs. We speculate that in the future,
    DMC might be used to grow the KV cache sub-linearly, which would provide an alternative
    between vanilla Transformers and State Space Models, where memory is constant
    (Fu et al., [2023](#bib.bib13); Gu & Dao, [2023](#bib.bib15)).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们承认 LLM 在推理时的行为取决于众多因素和实现细节，但我们在[图 5](#S5.F5 "在 5.1 主要结果 ‣ 5 结果 ‣ 动态内存压缩：为加速推理改造
    LLM")中的测量结果提供了证据，表明 DMC 增加了吞吐量并减少了自回归生成的延迟。我们推测，未来 DMC 可能用于以亚线性增长 KV 缓存，这将提供一个介于普通
    Transformer 和状态空间模型之间的替代方案，其中内存是常量的（Fu 等，[2023](#bib.bib13)；Gu & Dao，[2023](#bib.bib15)）。
- en: 5.3 Per-Head Learned Compression Rates
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 每头学习的压缩率
- en: 'Since the training loss does not enforce any compression schema a priori, as
    it just requires to match a certain global CR, we can investigate what schema
    the model discovers in practice. In [Figure 6](#S5.F6 "In 5.2 Throughput and Latency
    Measurements ‣ 5 Results ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated
    Inference"), we report the CR for each layer and head for different scales (7B,
    13B, and 70B) and CRs (2$\times$ DMC achieves extremely high CRs for several heads
    also in the few layers after the very first one. This is counter-productive as
    token representations are not contextualized yet, which makes taking the correct
    decision (whether to append or accumulate) hard.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练损失并不事先强制任何压缩方案，它只需要匹配某个全局 CR，我们可以研究模型在实际中发现的方案。在[图 6](#S5.F6 "在 5.2 吞吐量和延迟测量
    ‣ 5 结果 ‣ 动态内存压缩：为加速推理改造 LLM")中，我们报告了不同规模（7B、13B 和 70B）和 CR（2$\times$ DMC 在多个头部中也达到了极高的
    CR。在第一个层之后的几个层中，这种情况是反生产力的，因为 token 表示尚未上下文化，这使得做出正确决策（是附加还是累积）变得困难。
- en: 'This same pattern (in terms of relative preference for compressing the certain
    ranges of layers) also corresponds to how the distribution of CRs across layers
    changes throughout training steps, as we keep annealing the auxiliary loss $\ell_{\text{CR}}$
    towards the target CR. [Figure 7](#A3.F7 "In Evolution Throughout the Training
    ‣ Appendix C Analysis of the Compression Schema Learned by DMC ‣ Dynamic Memory
    Compression: Retrofitting LLMs for Accelerated Inference") in [Appendix C](#A3
    "Appendix C Analysis of the Compression Schema Learned by DMC ‣ Dynamic Memory
    Compression: Retrofitting LLMs for Accelerated Inference") illustrates how CR
    increases first in deeper layers, then in some non-contiguous intermediate ranges.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式（在压缩特定层范围的相对偏好方面）也与 CR 在训练步骤中的分布如何变化相对应，因为我们不断将辅助损失 $\ell_{\text{CR}}$ 调整到目标
    CR。[附录 C](#A3 "附录 C DMC 学到的压缩方案分析 ‣ 动态内存压缩：加速推理的 LLM 重新适配")中的[图 7](#A3.F7 "在训练过程中的演变
    ‣ 附录 C DMC 学到的压缩方案分析 ‣ 动态内存压缩：加速推理的 LLM 重新适配")说明了 CR 首先在更深层次增加，然后在一些不连续的中间范围内增加。
- en: 6 Related Work
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: Efficient inference in Transformer models is a subject of extensive research,
    with detailed overviews provided by several surveys (Pope et al., [2022](#bib.bib27);
    Treviso et al., [2022](#bib.bib33)). This section narrows its focus to advancements
    in Transformer inference efficiency through KV cache size reduction.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型的高效推理是广泛研究的课题，多个综述提供了详细的概述（Pope 等，[2022](#bib.bib27)；Treviso 等，[2022](#bib.bib33)）。本节将重点关注通过
    KV 缓存大小减少来提高 Transformer 推理效率的进展。
- en: 'Grouped Query Attention (GQA; Ainslie et al., [2023](#bib.bib1)) represents
    the most widespread strategy, evolving from Multi Query Attention (MQA; Shazeer,
    [2019](#bib.bib30)). GQA reduces the number of KV heads by allocating shared KV
    representations across subsets of query heads. Prior efforts in token merging
    (Zhang et al., [2018](#bib.bib37)) condensed the entire past context into a single
    token, while (Liu et al., [2018](#bib.bib21); Rae et al., [2019](#bib.bib28))
    employed strided convolution and mean pooling kernels to reduce the number of
    KV tokens. Sliding window attention techniques (Beltagy et al., [2020](#bib.bib4);
    Child et al., [2019](#bib.bib8)) restrict attention to a maximum of $w$ preceding
    tokens. Though effective in limiting KV cache, such methods perform fixed-size
    compression, unlike the presented dynamic DMC, which adapts the compression schema
    based on the input sequence. This yields superior results, as we prove in an ablation
    in [Appendix E](#A5 "Appendix E DMC Ablations ‣ Dynamic Memory Compression: Retrofitting
    LLMs for Accelerated Inference").'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 分组查询注意力（GQA；Ainslie 等，[2023](#bib.bib1)）代表了最广泛使用的策略，它从多查询注意力（MQA；Shazeer，[2019](#bib.bib30)）演变而来。GQA
    通过在查询头的子集之间分配共享 KV 表示，减少了 KV 头的数量。之前的令牌合并工作（Zhang 等，[2018](#bib.bib37)）将整个过去的上下文压缩成一个单一的令牌，而（Liu
    等，[2018](#bib.bib21)；Rae 等，[2019](#bib.bib28)）使用了有步幅的卷积和均值池化核来减少 KV 令牌的数量。滑动窗口注意力技术（Beltagy
    等，[2020](#bib.bib4)；Child 等，[2019](#bib.bib8)）将注意力限制在最多 $w$ 个之前的令牌上。尽管这些方法在限制
    KV 缓存方面有效，但它们进行的是固定大小的压缩，而不是像所提出的动态 DMC 那样，根据输入序列调整压缩方案。这使得效果更佳，我们在[附录 E](#A5
    "附录 E DMC 消融 ‣ 动态内存压缩：加速推理的 LLM 重新适配")中的消融实验中证明了这一点。
- en: Previous learnable compression methods (Anagnostidis et al., [2023](#bib.bib2),
    inter alia) decide which tokens to drop from the KV cache. DMC takes a different
    approach as instead of dropping tokens it merges them. Hence, it preserves cached
    information more faithfully. Moreover, the DMC compression mechanism has constant
    complexity with respect to the context length, while the one proposed by Anagnostidis
    et al. ([2023](#bib.bib2)) is linear. Mu et al. ([2023](#bib.bib23)) instead compresses
    prompts through costly generation which limits their inference benefits. Moreover,
    this method is only applicable to compressing the model input while DMC compresses
    *on-the-fly* the entire sequence, including both the model input and the generated
    output.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的可学习压缩方法（Anagnostidis 等，[2023](#bib.bib2)等）决定从 KV 缓存中丢弃哪些令牌。DMC 则采取不同的方法，它不是丢弃令牌，而是合并它们。因此，它能够更忠实地保留缓存的信息。此外，DMC
    压缩机制相对于上下文长度具有恒定的复杂性，而 Anagnostidis 等（[2023](#bib.bib2)）提出的方法则是线性的。Mu 等（[2023](#bib.bib23)）则通过代价高昂的生成来压缩提示，这限制了它们的推理效益。此外，这种方法仅适用于压缩模型输入，而
    DMC 可以*即时*压缩整个序列，包括模型输入和生成的输出。
- en: Non-learnable cache eviction strategies (Zhang et al., [2023](#bib.bib38); Sheng
    et al., [2023](#bib.bib31); Liu et al., [2023](#bib.bib22); Wang et al., [2020](#bib.bib35);
    Ge et al., [2023](#bib.bib14)) utilize attention scores or token properties to
    filter tokens in the KV cache. These approaches, while bypassing additional training,
    rely on heuristics and lack the ability to learn the compression mechanisms. In
    contrast, DMC integrates compression into its learning objective in an end-to-end
    manner, where compression is synergistic to language generation.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 不可学习的缓存淘汰策略（Zhang 等，[2023](#bib.bib38); Sheng 等，[2023](#bib.bib31); Liu 等，[2023](#bib.bib22);
    Wang 等，[2020](#bib.bib35); Ge 等，[2023](#bib.bib14)）利用注意力分数或 token 属性来过滤 KV 缓存中的
    tokens。这些方法虽然绕过了额外的训练，但依赖于启发式，并且缺乏学习压缩机制的能力。相比之下，DMC 将压缩集成到其端到端学习目标中，其中压缩与语言生成协同作用。
- en: Finally, DMC draws inspiration from Dynamic Token Pooling (Nawrot et al., [2022](#bib.bib25)),
    which introduces a learnable boundary predictor to merge the representations of
    groups of tokens in intermediate layers. DMC improves upon this idea by applying
    it to the KV cache of and introducing a continuous relaxation of the pooling decision
    during training. Moreover, it enables retrofitting pre-trained LLMs with minimal
    extra steps rather than training language models from scratch.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，DMC 从动态 token 池化（Nawrot 等，[2022](#bib.bib25)）中获得灵感，该方法引入了可学习的边界预测器，以合并中间层中
    token 组的表示。DMC 通过将该想法应用于 KV 缓存并在训练过程中引入池化决策的连续放松来改进这一点。此外，它使得用最小额外步骤对预训练的 LLM
    进行改造成为可能，而不是从头训练语言模型。
- en: 7 Conclusions and Future Work
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与未来工作
- en: We proposed Dynamic Memory Compression, a method to reduce the length of the
    KV cache in Transformers, which enhances the memory efficiency and speed of LLMs
    at inference time. For every new token, DMC learns end-to-end whether to append
    its KV representations to the cache or merge them with the top element in the
    cache. We show how to retrofit LLMs such as Llama 2 at different scales (7B, 13B,
    and 70B) into efficient DMC versions with a negligible amount of extra data and
    without extra parameters. DMC LLMs with 2$\times$ compression rates (CRs) retain
    (or even improve upon) the performance of the original LLM. In addition, we demonstrate
    that, for comparable CRs, DMC has significantly higher continued pre-training
    performance and sample efficiency than Grouped Query Attention (GQA), a widespread
    method for KV cache size reduction.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了动态内存压缩，这是一种减少 Transformer 中 KV 缓存长度的方法，增强了 LLM 在推理时的内存效率和速度。对于每个新 token，DMC
    端到端地学习是否将其 KV 表示追加到缓存中，或将其与缓存中的顶部元素合并。我们展示了如何将 Llama 2 等 LLM 在不同规模（7B、13B 和 70B）中改造为高效的
    DMC 版本，额外数据量极少且没有额外参数。具有 2$\times$ 压缩率（CR）的 DMC LLM 保持（或甚至改善）原始 LLM 的性能。此外，我们展示了在可比
    CR 下，DMC 相较于广泛使用的 KV 缓存大小减少方法分组查询注意力（GQA）具有显著更高的持续预训练性能和样本效率。
- en: Impact Statement
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 影响声明
- en: Dynamic Memory Compression in Large Language Models (LLMs) like Llama 2 results
    in better computational efficiency, reducing both operational costs and environmental
    impact (Patterson et al., [2021](#bib.bib26)). By enabling higher throughput and
    lower latency, DMC democratizes access to advanced AI, making state-of-the-art
    models suitable for a broader range of hardware. This may not only accelerate
    innovation across diverse sectors but also promote AI development and applications
    in an environmentally conscious manner.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）如 Llama 2 的动态内存压缩（DMC）提高了计算效率，减少了操作成本和环境影响（Patterson 等， [2021](#bib.bib26)）。通过实现更高的吞吐量和更低的延迟，DMC
    实现了对先进 AI 的民主化，使得最先进的模型适用于更广泛的硬件。这不仅可能加速各个领域的创新，还可能以环保的方式促进 AI 的发展和应用。
- en: References
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ainslie et al. (2023) Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy,
    Y., Lebrón, F., and Sanghai, S. K. GQA: Training generalized multi-query transformer
    models from multi-head checkpoints. *ArXiv*, abs/2305.13245, 2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ainslie 等（2023）Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebrón,
    F., 和 Sanghai, S. K. GQA: 从多头检查点训练通用的多查询 Transformer 模型。*ArXiv*, abs/2305.13245,
    2023。'
- en: Anagnostidis et al. (2023) Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L.,
    Lucchi, A., and Hofmann, T. Dynamic context pruning for efficient and interpretable
    autoregressive transformers. *ArXiv*, abs/2305.15805, 2023.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anagnostidis 等（2023）Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi,
    A., 和 Hofmann, T. 动态上下文修剪以实现高效和可解释的自回归 Transformer。*ArXiv*, abs/2305.15805, 2023。
- en: Bahdanau et al. (2014) Bahdanau, D., Cho, K., and Bengio, Y. Neural machine
    translation by jointly learning to align and translate. *ArXiv*, abs/1409.0473,
    2014.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 等（2014）Bahdanau, D., Cho, K., 和 Bengio, Y. Neural machine translation
    by jointly learning to align and translate. *ArXiv*, abs/1409.0473, 2014。
- en: 'Beltagy et al. (2020) Beltagy, I., Peters, M. E., and Cohan, A. Longformer:
    The long-document transformer. *ArXiv*, abs/2004.05150, 2020.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy 等（2020）Beltagy, I., Peters, M. E., 和 Cohan, A. Longformer: The long-document
    transformer. *ArXiv*, abs/2004.05150, 2020。'
- en: 'Bisk et al. (2020) Bisk, Y., Zellers, R., Le bras, R., Gao, J., and Choi, Y.
    PIQA: Reasoning about physical commonsense in natural language. *Proceedings of
    the AAAI Conference on Artificial Intelligence*, 34(05), Apr. 2020.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk 等（2020）Bisk, Y., Zellers, R., Le bras, R., Gao, J., 和 Choi, Y. PIQA: Reasoning
    about physical commonsense in natural language. *Proceedings of the AAAI Conference
    on Artificial Intelligence*, 34(05), 2020年4月。'
- en: 'Bolya et al. (2022) Bolya, D., Fu, C.-Y., Dai, X., Zhang, P., Feichtenhofer,
    C., and Hoffman, J. Token merging: Your ViT but faster. *ArXiv*, abs/2210.09461,
    2022.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bolya 等（2022）Bolya, D., Fu, C.-Y., Dai, X., Zhang, P., Feichtenhofer, C., 和
    Hoffman, J. Token merging: Your ViT but faster. *ArXiv*, abs/2210.09461, 2022。'
- en: Chen et al. (2021) Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan,
    J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger,
    G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder,
    N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such,
    F. P., Cummings, D. W., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss,
    A., Guss, W. H., Nichol, A., Babuschkin, I., Balaji, S., Jain, S., Carr, A., Leike,
    J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M. M., Brundage,
    M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S.,
    Sutskever, I., and Zaremba, W. Evaluating large language models trained on code.
    *ArXiv*, abs/2107.03374, 2021.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2021）Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J.,
    Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger,
    G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder,
    N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such,
    F. P., Cummings, D. W., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss,
    A., Guss, W. H., Nichol, A., Babuschkin, I., Balaji, S., Jain, S., Carr, A., Leike,
    J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M. M., Brundage,
    M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S.,
    Sutskever, I., 和 Zaremba, W. Evaluating large language models trained on code.
    *ArXiv*, abs/2107.03374, 2021。
- en: Child et al. (2019) Child, R., Gray, S., Radford, A., and Sutskever, I. Generating
    long sequences with sparse transformers. *ArXiv*, abs/1904.10509, 2019.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child 等（2019）Child, R., Gray, S., Radford, A., 和 Sutskever, I. Generating long
    sequences with sparse transformers. *ArXiv*, abs/1904.10509, 2019。
- en: Choromanski et al. (2020) Choromanski, K., Likhosherstov, V., Dohan, D., Song,
    X., Gane, A., Sarlós, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger,
    D., Colwell, L. J., and Weller, A. Rethinking attention with Performers. *ArXiv*,
    abs/2009.14794, 2020.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choromanski 等（2020）Choromanski, K., Likhosherstov, V., Dohan, D., Song, X.,
    Gane, A., Sarlós, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger,
    D., Colwell, L. J., 和 Weller, A. Rethinking attention with Performers. *ArXiv*,
    abs/2009.14794, 2020。
- en: 'Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., and Toutanova, K. BoolQ: Exploring the surprising difficulty of natural yes/no
    questions. In Burstein, J., Doran, C., and Solorio, T. (eds.), *Proceedings of
    the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics*, Minneapolis, Minnesota, June 2019\. Association for Computational
    Linguistics.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark 等（2019）Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M.,
    和 Toutanova, K. BoolQ: Exploring the surprising difficulty of natural yes/no questions.
    在 Burstein, J., Doran, C., 和 Solorio, T.（编），*Proceedings of the 2019 Conference
    of the North American Chapter of the Association for Computational Linguistics*,
    明尼阿波利斯, 明尼苏达州, 2019年6月\. 计算语言学协会。'
- en: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try
    ARC, the AI2 reasoning challenge. *ArXiv*, abs/1803.05457, 2018.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等（2018）Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick,
    C., 和 Tafjord, O. Think you have solved question answering? try ARC, the AI2 reasoning
    challenge. *ArXiv*, abs/1803.05457, 2018。
- en: 'Dao et al. (2022) Dao, T., Fu, D., Ermon, S., Rudra, A., and Ré, C. FlashAttention:
    Fast and memory-efficient exact attention with IO-awareness. In Koyejo, S., Mohamed,
    S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), *Advances in Neural
    Information Processing Systems*, volume 35\. Curran Associates, Inc., 2022.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao 等（2022）Dao, T., Fu, D., Ermon, S., Rudra, A., 和 Ré, C. FlashAttention:
    Fast and memory-efficient exact attention with IO-awareness. 在 Koyejo, S., Mohamed,
    S., Agarwal, A., Belgrave, D., Cho, K., 和 Oh, A.（编），*Advances in Neural Information
    Processing Systems*, 第35卷\. Curran Associates, Inc., 2022。'
- en: 'Fu et al. (2023) Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A.,
    and Re, C. Hungry hungry hippos: Towards language modeling with state space models.
    In *The Eleventh International Conference on Learning Representations*, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等人（2023）Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., 和 Re,
    C. 饥饿的饥饿的河马：面向使用状态空间模型的语言建模。在 *第十一届国际学习表征会议*，2023。
- en: 'Ge et al. (2023) Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J.
    Model tells you what to discard: Adaptive kv cache compression for llms. *ArXiv*,
    abs/2310.01801, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge 等人（2023）Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., 和 Gao, J. 模型告诉你需要丢弃什么：针对大型语言模型的自适应
    KV 缓存压缩。*ArXiv*, abs/2310.01801, 2023.
- en: 'Gu & Dao (2023) Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with
    selective state spaces. *ArXiv*, abs/2312.00752, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 和 Dao（2023）Gu, A. 和 Dao, T. Mamba：具有选择性状态空间的线性时间序列建模。*ArXiv*, abs/2312.00752,
    2023.
- en: Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding.
    In *International Conference on Learning Representations*, 2021.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人（2021）Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,
    Song, D., 和 Steinhardt, J. 测量大规模多任务语言理解能力。在 *国际学习表征会议*，2021。
- en: Holtzman et al. (2019) Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi,
    Y. The curious case of neural text degeneration. *ArXiv*, abs/1904.09751, 2019.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holtzman 等人（2019）Holtzman, A., Buys, J., Du, L., Forbes, M., 和 Choi, Y. 神经文本退化的奇异案例。*ArXiv*,
    abs/1904.09751, 2019.
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T.,
    Lacroix, T., and Sayed, W. E. Mistral 7B. *ArXiv*, abs/2310.06825, 2023.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人（2023）Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot,
    D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L.,
    Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix,
    T., 和 Sayed, W. E. Mistral 7B. *ArXiv*, abs/2310.06825, 2023.
- en: 'Kim & Cho (2020) Kim, G. and Cho, K. Length-adaptive Transformer: Train once
    with length drop, use anytime with search. In *Annual Meeting of the Association
    for Computational Linguistics*, 2020.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 和 Cho（2020）Kim, G. 和 Cho, K. 长度自适应 Transformer：一次训练，随时使用并进行搜索。在 *计算语言学协会年会*，2020。
- en: Kwon et al. (2023) Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H.,
    Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large
    language model serving with pagedattention. *Proceedings of the 29th Symposium
    on Operating Systems Principles*, 2023.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等人（2023）Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H.,
    Gonzalez, J. E., Zhang, H., 和 Stoica, I. 使用分页注意力对大型语言模型服务进行高效的内存管理。*第29届操作系统原理研讨会论文集*，2023。
- en: Liu et al. (2018) Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R.,
    Kaiser, L., and Shazeer, N. M. Generating wikipedia by summarizing long sequences.
    *ArXiv*, abs/1801.10198, 2018.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2018）Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser,
    L., 和 Shazeer, N. M. 通过总结长序列生成维基百科。*ArXiv*, abs/1801.10198, 2018.
- en: 'Liu et al. (2023) Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z.,
    Kyrillidis, A., and Shrivastava, A. Scissorhands: Exploiting the persistence of
    importance hypothesis for llm kv cache compression at test time. *ArXiv*, abs/2305.17118,
    2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2023）Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyrillidis,
    A., 和 Shrivastava, A. Scissorhands：利用重要性假设的持久性进行大型语言模型 KV 缓存压缩。在测试时。*ArXiv*, abs/2305.17118,
    2023.
- en: Mu et al. (2023) Mu, J., Li, X. L., and Goodman, N. D. Learning to compress
    prompts with gist tokens. *ArXiv*, abs/2304.08467, 2023.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu 等人（2023）Mu, J., Li, X. L., 和 Goodman, N. D. 学习使用要点标记压缩提示。*ArXiv*, abs/2304.08467,
    2023.
- en: 'Narayanan et al. (2021) Narayanan, D., Shoeybi, M., Casper, J., LeGresley,
    P., Patwary, M., Korthikanti, V. A., Vainbrand, D., Kashinkunti, P., Bernauer,
    J., Catanzaro, B., Phanishayee, A., and Zaharia, M. A. Efficient large-scale language
    model training on gpu clusters using megatron-lm. *SC21: International Conference
    for High Performance Computing, Networking, Storage and Analysis*, 2021.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Narayanan 等人（2021）Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary,
    M., Korthikanti, V. A., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro,
    B., Phanishayee, A., 和 Zaharia, M. A. 使用 Megatron-LM 在 GPU 集群上高效地进行大规模语言模型训练。*SC21:
    国际高性能计算、网络、存储和分析会议*，2021。'
- en: Nawrot et al. (2022) Nawrot, P., Chorowski, J., Lańcucki, A., and Ponti, E.
    Efficient transformers with dynamic token pooling. In *Annual Meeting of the Association
    for Computational Linguistics*, 2022.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nawrot 等人（2022）Nawrot, P., Chorowski, J., Lańcucki, A., 和 Ponti, E. 具有动态标记池的高效
    Transformer。在 *计算语言学协会年会*，2022。
- en: Patterson et al. (2021) Patterson, D. A., Gonzalez, J., Le, Q. V., Liang, C.,
    Munguía, L.-M., Rothchild, D., So, D. R., Texier, M., and Dean, J. Carbon emissions
    and large neural network training. *ArXiv*, abs/2104.10350, 2021.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patterson 等人（2021）Patterson, D. A., Gonzalez, J., Le, Q. V., Liang, C., Munguía,
    L.-M., Rothchild, D., So, D. R., Texier, M., 和 Dean, J. 碳排放与大型神经网络训练。*ArXiv*，abs/2104.10350，2021。
- en: Pope et al. (2022) Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury,
    J., Levskaya, A., Heek, J., Xiao, K., Agrawal, S., and Dean, J. Efficiently scaling
    transformer inference. *ArXiv*, abs/2211.05102, 2022.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pope 等人（2022）Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J.,
    Levskaya, A., Heek, J., Xiao, K., Agrawal, S., 和 Dean, J. 高效扩展变换器推理。*ArXiv*，abs/2211.05102，2022。
- en: Rae et al. (2019) Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap,
    T. P. Compressive transformers for long-range sequence modelling. *ArXiv*, abs/1911.05507,
    2019.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rae 等人（2019）Rae, J. W., Potapenko, A., Jayakumar, S. M., 和 Lillicrap, T. P.
    用于长范围序列建模的压缩变换器。*ArXiv*，abs/1911.05507，2019。
- en: 'Sakaguchi et al. (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. WinoGrande: An adversarial winograd schema challenge at scale. *Commun. ACM*,
    64(9), 2021.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi 等人（2021）Sakaguchi, K., Bras, R. L., Bhagavatula, C., 和 Choi, Y. WinoGrande：大规模对抗性
    Winograd 语法挑战。*计算机通讯*，64(9)，2021。
- en: 'Shazeer (2019) Shazeer, N. M. Fast transformer decoding: One write-head is
    all you need. *ArXiv*, abs/1911.02150, 2019.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer（2019）Shazeer, N. M. 快速变换器解码：一个写头即你所需的一切。*ArXiv*，abs/1911.02150，2019。
- en: Sheng et al. (2023) Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Fu,
    D. Y., Xie, Z., Chen, B., Barrett, C. W., Gonzalez, J., Liang, P., Ré, C., Stoica,
    I., and Zhang, C. High-throughput generative inference of large language models
    with a single gpu. In *International Conference on Machine Learning*, 2023.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheng 等人（2023）Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Fu, D. Y.,
    Xie, Z., Chen, B., Barrett, C. W., Gonzalez, J., Liang, P., Ré, C., Stoica, I.,
    和 Zhang, C. 使用单个 GPU 的大规模生成推理。发表于 *国际机器学习会议*，2023。
- en: 'Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D.,
    Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J.,
    Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M.,
    Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
    Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez,
    A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned
    chat models. *ArXiv*, abs/2307.09288, 2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2023）Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A.,
    Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher,
    L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J.,
    Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.
    M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J.
    X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S.,
    Rodriguez, A., Stojnic, R., Edunov, S., 和 Scialom, T. Llama 2：开放基础和微调聊天模型。*ArXiv*，abs/2307.09288，2023。
- en: 'Treviso et al. (2022) Treviso, M. V., Ji, T., Lee, J.-U., van Aken, B., Cao,
    Q., Ciosici, M. R., Hassid, M., Heafield, K., Hooker, S., Martins, P. H., Martins,
    A. F. T., Milder, P., Raffel, C., Simpson, E., Slonim, N., Balasubramanian, N.,
    Derczynski, L., and Schwartz, R. Efficient methods for natural language processing:
    A survey. *Transactions of the Association for Computational Linguistics*, 11,
    2022.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Treviso 等人（2022）Treviso, M. V., Ji, T., Lee, J.-U., van Aken, B., Cao, Q., Ciosici,
    M. R., Hassid, M., Heafield, K., Hooker, S., Martins, P. H., Martins, A. F. T.,
    Milder, P., Raffel, C., Simpson, E., Slonim, N., Balasubramanian, N., Derczynski,
    L., 和 Schwartz, R. 自然语言处理的高效方法：一项综述。*计算语言学协会会刊*，11，2022。
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N. M., Parmar, N., Uszkoreit, J.,
    Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need.
    In *Neural Information Processing Systems*, 2017.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人（2017）Vaswani, A., Shazeer, N. M., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, L., 和 Polosukhin, I. 注意力即你所需的一切。发表于 *神经信息处理系统*，2017。
- en: 'Wang et al. (2020) Wang, H., Zhang, Z., and Han, S. Spatten: Efficient sparse
    attention architecture with cascade token and head pruning. *2021 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA)*, 2020.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等（2020）Wang, H., Zhang, Z., 和 Han, S. Spatten: 高效稀疏注意力架构，具有级联令牌和头修剪。*2021
    IEEE 国际高性能计算架构研讨会 (HPCA)*，2020。'
- en: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. HellaSwag: Can a machine really finish your sentence? In Korhonen, A.,
    Traum, D., and Màrquez, L. (eds.), *Proceedings of the 57th Annual Meeting of
    the Association for Computational Linguistics*, Florence, Italy, July 2019\. Association
    for Computational Linguistics.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers 等（2019）Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., 和 Choi, Y.
    HellaSwag: 机器真的能完成你的句子吗？在 Korhonen, A., Traum, D., 和 Màrquez, L.（编），*第 57 届计算语言学协会年会论文集*，意大利佛罗伦萨，2019
    年 7 月。计算语言学协会。'
- en: Zhang et al. (2018) Zhang, B., Xiong, D., and Su, J. Accelerating neural transformer
    via an average attention network. *ArXiv*, abs/1805.00631, 2018.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2018）Zhang, B., Xiong, D., 和 Su, J. 通过平均注意力网络加速神经变换器。*ArXiv*，abs/1805.00631，2018。
- en: 'Zhang et al. (2023) Zhang, Z. A., Sheng, Y., Zhou, T., Chen, T., Zheng, L.,
    Cai, R., Song, Z., Tian, Y., Ré, C., Barrett, C. W., Wang, Z., and Chen, B. H2o:
    Heavy-hitter oracle for efficient generative inference of large language models.
    *ArXiv*, abs/2306.14048, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2023）Zhang, Z. A., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R.,
    Song, Z., Tian, Y., Ré, C., Barrett, C. W., Wang, Z., 和 Chen, B. H2o: 高效生成推理的大型语言模型的重型预言机。*ArXiv*，abs/2306.14048，2023。'
- en: Appendix
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Memory-Bound Operations in Transformers
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 变换器中的内存绑定操作
- en: During autoregressive generation with a KV cache, the sequence length during
    every forward pass is $n=1$. The vast majority of time is spent on calculations
    for linear layers and multi-head self-attention. For linear layers, the ratio
    of FLOPs to input bytes improves as the batch size increases. For small batch
    sizes, the operations are memory bounded on reading the weight matrices from HBM.
    On the other hand, for MHSA layers during inference the ratio of FLOPs to input
    bytes does not change and MHSA layers are always memory bounded on current GPUs.
    The impact of KV cache compression is two-fold as it 1) allows us to decrease
    the latency of MHSA layers, and 2) allows us to fit larger batch sizes in HBM,
    resulting in better throughput and better utilization of GPUs during the calculations
    of linear layers.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在自回归生成过程中，使用 KV 缓存时，每次前向传递的序列长度为 $n=1$。绝大多数时间用于计算线性层和多头自注意力层。对于线性层，FLOPs 与输入字节的比率随着批量大小的增加而改善。对于小批量大小，操作在从
    HBM 中读取权重矩阵时受到内存限制。另一方面，在推理过程中，对于 MHSA 层，FLOPs 与输入字节的比率不变，MHSA 层在当前 GPU 上总是受到内存限制。KV
    缓存压缩的影响是双重的：1) 它允许我们减少 MHSA 层的延迟，2) 它允许我们在 HBM 中适配更大的批量大小，从而提高了吞吐量和 GPU 在线性层计算过程中的利用率。
- en: Appendix B Replicating the Original Results
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 复现原始结果
- en: 'To make sure that our implementation is correct, for each downstream task we
    compare the performance reported in the original Llama 2 paper (Touvron et al.,
    [2023](#bib.bib32)) with those obtained from the Hugging Face Hub checkpoints.
    Furthermore, we evaluate the impact of using our internal data mixture for up-training,
    acknowledging that variations in data proportions and preprocessing methodologies
    can influence model behavior. In particular, we up-train the vanilla pre-trained
    Llama-2 checkpoint for 200 training steps, amounting to 1B tokens, in accordance
    with the original Llama-2 training schedule. We compute the average and standard
    deviation of checkpoints after 50, 100, 150, 200 steps. In our experiments, we
    replicate the results reported by the Llama 2 paper almost exactly, as shown in
    [Table 2](#A2.T2 "In Appendix B Replicating the Original Results ‣ Dynamic Memory
    Compression: Retrofitting LLMs for Accelerated Inference").'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保我们的实现正确，对于每个下游任务，我们将原始 Llama 2 论文（Touvron 等，[2023](#bib.bib32)）中报告的性能与从 Hugging
    Face Hub 检查点获得的性能进行比较。此外，我们评估使用内部数据混合进行增训的影响，承认数据比例和预处理方法的变化可能会影响模型行为。特别是，我们根据原始
    Llama-2 训练计划对原版预训练的 Llama-2 检查点进行了 200 个训练步骤的增训，总计 1B 令牌。我们计算了在 50、100、150、200
    步骤后的检查点的平均值和标准差。在我们的实验中，我们几乎完全复现了 Llama 2 论文中报告的结果，如 [表 2](#A2.T2 "附录 B 复现原始结果
    ‣ 动态内存压缩：为加速推理改进 LLMs") 所示。
- en: '|  | CS-QA | MMLU | Human-Eval |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | CS-QA | MMLU | Human-Eval |'
- en: '|  | 7B | 13B | 70B | 7B | 13B | 70B | 7B | 13B | 70B |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | 7B | 13B | 70B | 7B | 13B | 70B | 7B | 13B | 70B |'
- en: '| Paper | 70.6 | 73.7 | 78.5 | 45.3 | 54.8 | 68.9 | 12.8 | 18.3 | 29.9 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 70.6 | 73.7 | 78.5 | 45.3 | 54.8 | 68.9 | 12.8 | 18.3 | 29.9 |'
- en: '| Checkpoint | 70.6 | 73.7 | 78.5 | 45.7 | 55.1 | 69.1 | 13.4 | 17.7 | 30.5
    |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 检查点 | 70.6 | 73.7 | 78.5 | 45.7 | 55.1 | 69.1 | 13.4 | 17.7 | 30.5 |'
- en: '| Up-trained | 70.5 ± 0.2 | 73.5 ± 0.1 | 78.0 ± 0.1 | 44.6 ± 0.6 | 54.5 ± 0.3
    | 68.8 ± 0.3 | 14.0 ± 1.2 | 17.5 ± 1.5 | 29.6 ± 1.6 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 上训练 | 70.5 ± 0.2 | 73.5 ± 0.1 | 78.0 ± 0.1 | 44.6 ± 0.6 | 54.5 ± 0.3 | 68.8
    ± 0.3 | 14.0 ± 1.2 | 17.5 ± 1.5 | 29.6 ± 1.6 |'
- en: 'Table 2: Replicating the original up-training results.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：复制原始的上训练结果。
- en: Appendix C Analysis of the Compression Schema Learned by DMC
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C DMC 学到的压缩模式分析
- en: Evolution Throughout the Training
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练过程中的演变
- en: 'In [Figure 7](#A3.F7 "In Evolution Throughout the Training ‣ Appendix C Analysis
    of the Compression Schema Learned by DMC ‣ Dynamic Memory Compression: Retrofitting
    LLMs for Accelerated Inference"), we illustrate how the CR changes for each layer
    of the Llama 2 7B model throughout the training from 1$\times$ global CR. Each
    subplot corresponds to a different global CR which occurs at different stages
    of the training, going from the smallest (1.17) at the top, to the highest (4.16)
    at the bottom. There is a clear trend such that, for a smaller global Compression
    Ratio (i.e. at the beginning of the training), the model emphasizes compression
    in the later layers. As the global Compression Ratio increases, the model keeps
    on compressing in final layers but also starts to compress the earlier layers.
    We hypothesize that the token representations in the initial layers do not contain
    sufficient information to perform any meaningful grouping. Conversely, token representations
    in the subsequent layers are more defined and, possibly, after several attention
    layers, already contain redundant/shared information.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 7](#A3.F7 "在训练过程中的演变 ‣ 附录 C DMC 学到的压缩模式分析 ‣ 动态内存压缩：改造 LLM 以加速推理")中，我们展示了
    Llama 2 7B 模型在从 1$\times$ 全局 CR 的训练过程中每一层 CR 的变化。每个子图对应于训练中的不同阶段下的不同全局 CR，从顶部的最小值（1.17）到底部的最大值（4.16）。有一个明确的趋势，即对于较小的全局压缩比（即训练开始时），模型在后期层强调压缩。随着全局压缩比的增加，模型继续在最后层进行压缩，但也开始压缩早期层。我们假设初始层的标记表示不包含足够的信息以进行任何有意义的分组。相反，后续层的标记表示更为明确，并且在经过若干注意力层后，可能已经包含冗余/共享的信息。
- en: '![Refer to caption](img/472ac18174dd0bba29a75649d5e560a8.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/472ac18174dd0bba29a75649d5e560a8.png)'
- en: 'Figure 7: Compression distribution across layers at different stages of retrofitting
    a Llama 2 7B model. We adhere to the convention where, for a given subplot, a
    larger space above a given layer indicates greater compression at that layer.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：在不同阶段对 Llama 2 7B 模型进行改造时的压缩分布。我们遵循这样一个惯例：对于给定的子图，层上方的空间越大表示该层的压缩越大。
- en: Sequence Length versus Compression Ratio
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 序列长度与压缩比
- en: 'Do DMC models compress sequences with a uniform CR independent from their total
    length? We find that this is not the case. As show by [Figure 8](#A3.F8.1 "In
    Sequence Length versus Compression Ratio ‣ Appendix C Analysis of the Compression
    Schema Learned by DMC ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated
    Inference"), the CR increases logarithmically as we increase the total sequence
    length. This holds true across all global CRs (including both 2$\times$).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: DMC 模型是否以统一的压缩比（CR）来压缩序列，而与其总长度无关？我们发现情况并非如此。如[图 8](#A3.F8.1 "在序列长度与压缩比 ‣ 附录
    C DMC 学到的压缩模式分析 ‣ 动态内存压缩：改造 LLM 以加速推理")所示，随着总序列长度的增加，CR 以对数方式增加。这在所有全局 CR（包括 2$\times$）中都成立。
- en: '![Refer to caption](img/58d538d3afa1b03337b658a0bda17524.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/58d538d3afa1b03337b658a0bda17524.png)'
- en: 'Figure 8: CR achieved by Llama 2 7B for particular sequence lengths across
    various global CRs.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：Llama 2 7B 在不同全局 CR 下对特定序列长度实现的 CR。
- en: Absolute Position versus Compression Decision
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 绝对位置与压缩决策
- en: 'Do DMC models learn a fixed compression schema, or do they exhibit position
    biases? In [Figure 9](#A3.F9.3 "In Absolute Position versus Compression Decision
    ‣ Appendix C Analysis of the Compression Schema Learned by DMC ‣ Dynamic Memory
    Compression: Retrofitting LLMs for Accelerated Inference"), we plot the average
    value of the decision variable $\alpha$ compression ratios (CR).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: DMC 模型是否学习了固定的压缩模式，还是表现出位置偏差？在[图 9](#A3.F9.3 "在绝对位置与压缩决策 ‣ 附录 C DMC 学到的压缩模式分析
    ‣ 动态内存压缩：改造 LLM 以加速推理")中，我们绘制了决策变量 $\alpha$ 压缩比（CR）的平均值。
- en: '![Refer to caption](img/53046fe54793bb2948c4e8aaf7ac1ed6.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/53046fe54793bb2948c4e8aaf7ac1ed6.png)'
- en: 'Figure 9: Average value of the decision $\alpha$ for positions (0, 4096) averaged
    over 128 samples, heads and layers.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：决策 $\alpha$ 在位置 (0, 4096) 上的平均值，基于 128 个样本、头和层的平均值。
- en: Compression Schemata learned by DMC-C
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DMC-C 学到的压缩方案
- en: 'Studying the compression schema in [Figure 10](#A3.F10 "In Compression Schemata
    learned by DMC-C ‣ Appendix C Analysis of the Compression Schema Learned by DMC
    ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference") learned
    by DMC-C, we find a very different pattern compared to DMC, due to the auxiliary
    loss forcing the model to compress similarly across heads in the same layer. Nevertheless,
    we observe a similar global preference for compressing deeper layers.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 研究 DMC-C 学到的[图 10](#A3.F10 "在 DMC-C 学到的压缩方案 ‣ 附录 C DMC 学到的压缩方案分析 ‣ 动态内存压缩：为加速推理改造
    LLMs") 中的压缩方案，我们发现与 DMC 相比，模式非常不同，因为辅助损失迫使模型在同一层的各头之间进行类似的压缩。然而，我们观察到类似的全局倾向，即压缩较深层次的层。
- en: '![Refer to caption](img/0d2c5616d209c8d22695bdf312d6178f.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0d2c5616d209c8d22695bdf312d6178f.png)'
- en: '![Refer to caption](img/c32c68824fd19b964aa87cfd549ab6dc.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c32c68824fd19b964aa87cfd549ab6dc.png)'
- en: 'Figure 10: Heatmaps of average compression rates across layers (X-axis) and
    heads (Y-axis) for DMC-C. Heads are arranged from the highest compression to the
    lowest top-down for clarity.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：DMC-C 的各层（X 轴）和各头（Y 轴）的平均压缩率热图。为了清晰起见，头部按从最高压缩到最低的顺序排列。
- en: Interpretability
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可解释性
- en: 'A natural question arises, whether the compression schema that the model learns
    is somehow aligned with human intuitions about text segmentation. We analyzed
    the outputs of Llama 2 13B DMC with CR $4$ and noticed that some heads compress
    according to the boundaries of linguistic units, such as words or syntactic phrases.
    [Figure 11](#A3.F11.3 "In Interpretability ‣ Appendix C Analysis of the Compression
    Schema Learned by DMC ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated
    Inference") shows the compression schema learned by head 14 in layer 0\. In this
    case, the model merges the subwords back into words *reverting* the tokenizer.
    Interestingly, some groupings of tokens correspond to semantic units, e.g., “1
    9 th century”’, “5 0 percent", or “a week back later”. Yet, we also stress that
    many heads and layers are not interpretable as their behavior does not overlap
    with linguistic units.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的问题是，模型学习的压缩方案是否与人类对文本分割的直觉有所一致。我们分析了 Llama 2 13B DMC（CR $4$）的输出，注意到一些头根据语言单位的边界（如词或句法短语）进行压缩。[图
    11](#A3.F11.3 "在可解释性 ‣ 附录 C DMC 学到的压缩方案分析 ‣ 动态内存压缩：为加速推理改造 LLMs") 显示了第 0 层第 14
    头学到的压缩方案。在这种情况下，模型将子词合并回词中，*恢复* 了分词器。有趣的是，一些标记的分组对应于语义单位，例如，“19 世纪”、“50 百分比”或“一周前”。然而，我们也强调，许多头和层不可解释，因为它们的行为与语言单位不重合。
- en: 'More generally higher layers merge longer tokens sequences, in line with [Figure 6](#S5.F6
    "In 5.2 Throughput and Latency Measurements ‣ 5 Results ‣ Dynamic Memory Compression:
    Retrofitting LLMs for Accelerated Inference"). For instance, [Figure 12](#A3.F12.3
    "In Interpretability ‣ Appendix C Analysis of the Compression Schema Learned by
    DMC ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference")
    shows the decisions of layer 24 head 2\. We leave a more in-depth analysis of
    compression schemata learned by DMC to future work.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 更普遍地说，高层合并更长的标记序列，与[图 6](#S5.F6 "在 5.2 吞吐量和延迟测量 ‣ 5 结果 ‣ 动态内存压缩：为加速推理改造 LLMs")
    一致。例如，[图 12](#A3.F12.3 "在可解释性 ‣ 附录 C DMC 学到的压缩方案分析 ‣ 动态内存压缩：为加速推理改造 LLMs") 显示了第
    24 层第 2 头的决策。我们将 DMC 学到的压缩方案的更深入分析留待将来工作。
- en: '![Refer to caption](img/04e71150d62a9f474c0df1e636904cdd.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/04e71150d62a9f474c0df1e636904cdd.png)'
- en: 'Figure 11: Compression schema found by Llama 2 13B DMC 4$\times$ in layer 0,
    head 14\. Tokens that are merged in the KV cache are marked with the same color.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：Llama 2 13B DMC 4$\times$ 在第 0 层，第 14 头发现的压缩方案。KV 缓存中合并的标记用相同颜色标记。
- en: '![Refer to caption](img/1202b1630f73fef9cfe8c16e5f0042c4.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1202b1630f73fef9cfe8c16e5f0042c4.png)'
- en: 'Figure 12: Compression schema found by Llama 2 13B DMC 4$\times$ for layer
    24, head 2\. Tokens that are merged in the KV cache are marked with the same color.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：Llama 2 13B DMC 4$\times$ 在第 24 层，第 2 头发现的压缩方案。KV 缓存中合并的标记用相同颜色标记。
- en: Appendix D Training Ablations
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 训练消融
- en: Training Steps per Increase in CR
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每次 CR 增加的训练步骤
- en: '![Refer to caption](img/104a39e98541acfeca80f2ae38c68aed.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/104a39e98541acfeca80f2ae38c68aed.png)'
- en: 'Figure 13: Different up-training regimes for DMC-C: Short (red) increases CR
    by 1 every 6K steps, Long (blue) increases CR by 1 every 3K steps. Horizontal
    lines correspond to the performance of the original Llama 2.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：DMC-C 的不同上训练方案：短期（红色）每 6K 步将 CR 增加 1，长期（蓝色）每 3K 步将 CR 增加 1。水平线对应于原始 Llama
    2 的性能。
- en: 'Another advantage of DMC is its high flexibility. In fact, based on the availability
    of resources, different regimes can be chosen when annealing the CR to the target
    during up-training. In [Figure 13](#A4.F13 "In Training Steps per Increase in
    CR ‣ Appendix D Training Ablations ‣ Dynamic Memory Compression: Retrofitting
    LLMs for Accelerated Inference"), we compare a Short and a Long regime for the
    contrained variant of DMC (DMC-C), which continuously increase the CR by 1 every
    3K and 6K steps (12B and 24B tokens), respectively. It is evident how there exists
    a trade-off between training steps (hence, time) and performance. Additionally,
    [Figure 13](#A4.F13 "In Training Steps per Increase in CR ‣ Appendix D Training
    Ablations ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference")
    showcases another aspect of the higher flexibility DMC affords: it is compatible
    with arbitrary real-valued CRs, as opposed to integer CRs divisible by 2 as in
    GQA.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 'DMC 的另一个优点是其高度的灵活性。实际上，根据资源的可用性，在上训练期间将 CR 退火到目标值时，可以选择不同的方案。在 [图 13](#A4.F13
    "In Training Steps per Increase in CR ‣ Appendix D Training Ablations ‣ Dynamic
    Memory Compression: Retrofitting LLMs for Accelerated Inference") 中，我们比较了 DMC
    的约束变体（DMC-C）的短期方案和长期方案，其中分别在每 3K 和 6K 步（12B 和 24B 代币）内将 CR 持续增加 1。很明显，训练步数（因此，时间）和性能之间存在权衡。此外，[图
    13](#A4.F13 "In Training Steps per Increase in CR ‣ Appendix D Training Ablations
    ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference") 展示了
    DMC 更高灵活性的另一个方面：它兼容任意实值 CR，而不是像 GQA 中那样只能是 2 的整数倍 CR。'
- en: Schedules of Target CR
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 目标 CR 的调度
- en: '![Refer to caption](img/d114d85c56634d606a69513d01937d28.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d114d85c56634d606a69513d01937d28.png)'
- en: 'Figure 14: Validation perplexity and MMLU accuracy vs training steps for Fixed
    Memory Pooling and a variant of DMC where the auxiliary loss CR is immediately
    set to the target on the onset of training. All models follow the regular training
    schedule and are trained up to 2$\times$ CR.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：固定内存池和 DMC 的一个变体中，验证困惑度和 MMLU 准确率与训练步数的关系，其中辅助损失 CR 在训练开始时立即设置为目标值。所有模型遵循常规训练计划，并训练至
    2$\times$ CR。
- en: 'Additionally, we explore how different schedules for the target CR impact the
    model performance. In the standard setup, this is annealed from 1 to the target
    CR throughout the duration of training. Here, we compare it with a setup where
    the CR used in the auxiliary loss for compression is set to the target from the
    start (DMC-immediate). We show the results in [Figure 14](#A4.F14 "In Schedules
    of Target CR ‣ Appendix D Training Ablations ‣ Dynamic Memory Compression: Retrofitting
    LLMs for Accelerated Inference"). As expected, DMC-immediate has a perplexity
    spike at the beginning when the model quickly increases the CR due to the auxiliary
    loss. While perplexity is recovered during training, even to a lower point than
    DMC with annealing, downstream accuracy on MMLU benchmark is degraded across the
    board. This showcases why avoiding perplexity spikes is fundamental to successfully
    retrofit an LLM.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们探索了不同的目标 CR 调度对模型性能的影响。在标准设置中，这在整个训练期间从 1 退火到目标 CR。在这里，我们将其与一种设置进行了比较，在该设置中，用于压缩的辅助损失中的
    CR 从一开始就设置为目标值（DMC-immediate）。我们在 [图 14](#A4.F14 "In Schedules of Target CR ‣
    Appendix D Training Ablations ‣ Dynamic Memory Compression: Retrofitting LLMs
    for Accelerated Inference") 中展示了结果。正如预期的那样，DMC-immediate 在开始时会有一个困惑度峰值，因为模型由于辅助损失而快速增加
    CR。虽然在训练过程中困惑度有所恢复，甚至比使用退火的 DMC 更低，但在 MMLU 基准上的下游准确率却普遍下降。这展示了为什么避免困惑度峰值对于成功调整
    LLM 是至关重要的。'
- en: Appendix E DMC Ablations
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E DMC 消融实验
- en: '![Refer to caption](img/997097a048fec0d71f636c119f9c449e.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/997097a048fec0d71f636c119f9c449e.png)'
- en: 'Figure 15: Validation perplexity and test MMLU accuracy vs compression rate
    for different types of compression priors, in-layer relaxations, and importance
    scores. All models follow the short training regime and are trained up to 2$\times$
    CR.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：不同类型的压缩先验、层内松弛和重要性评分下的验证困惑度和测试 MMLU 准确率与压缩率的关系。所有模型遵循短期训练方案，并训练至 2$\times$
    CR。
- en: Fixed vs Learned Memory Compression
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 固定 vs 学习的内存压缩
- en: 'We assessed the importance of dynamically learning compression decisions in
    DMC by comparing it with Fixed Memory Pooling, which reduces the overall number
    of tokens in memory by deterministically averaging every $n$ in this case is identical
    to the compression rate. The results, shown in [Figure 14](#A4.F14 "In Schedules
    of Target CR ‣ Appendix D Training Ablations ‣ Dynamic Memory Compression: Retrofitting
    LLMs for Accelerated Inference"), demonstrate that the dynamic component of DMC
    is crucial to achieve lower perplexity as well as higher downstream accuracy.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将DMC与固定内存池进行比较，评估动态学习压缩决策的重要性，固定内存池通过确定性地平均每个$n$来减少内存中的总体令牌数量，这里的$n$与压缩率相同。结果显示在[图14](#A4.F14
    "在目标CR的计划 ‣ 附录D训练消融 ‣ 动态内存压缩：为加速推理改造LLMs")中，表明DMC的动态组件对于实现较低的困惑度以及更高的下游准确率至关重要。
- en: Global vs Local (Layer-wise) Compression Prior
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 全局与局部（逐层）压缩先验
- en: 'We compare two approaches to compression: a Local Prior, which enforces a pre-specified
    compression ratio (CR) in each layer independently, requiring every layer to compress
    approximately the same amount, and a Global Prior used by default DMC, which applies
    a pre-specified CR across all layers, giving the model the freedom to apply different
    CRs in each layer, provided that their average compression equals the global CR.
    [Figure 15](#A5.F15 "In Appendix E DMC Ablations ‣ Dynamic Memory Compression:
    Retrofitting LLMs for Accelerated Inference") clearly indicates that the Global
    Prior (DMC in [Figure 15](#A5.F15 "In Appendix E DMC Ablations ‣ Dynamic Memory
    Compression: Retrofitting LLMs for Accelerated Inference")) improves MMLU performance
    compared to the Local Prior.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了两种压缩方法：本地先验，它在每一层中强制执行一个预设的压缩比（CR），要求每一层大致压缩相同的量；以及默认DMC使用的全局先验，它在所有层中应用一个预设的CR，允许模型在每一层中应用不同的CR，只要它们的平均压缩率等于全局CR。
    [图15](#A5.F15 "在附录E DMC消融 ‣ 动态内存压缩：为加速推理改造LLMs")清楚地表明，与本地先验相比，全局先验（DMC在[图15](#A5.F15
    "在附录E DMC消融 ‣ 动态内存压缩：为加速推理改造LLMs")）提高了MMLU的性能。
- en: In-layer Relaxation
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层内放松
- en: '| Scale | Method | CR | MMLU | CS-QA | Human Eval |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 规模 | 方法 | CR | MMLU | CS-QA | 人类评估 |'
- en: '| 7B | – | – | 44.6 | 70.5 | 14.0 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 7B | – | – | 44.6 | 70.5 | 14.0 |'
- en: '| GQA | 2$\times$ | 39.8 | 68.9 | 12.8 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| GQA | 2$\times$ | 39.8 | 68.9 | 12.8 |'
- en: '| DMC | 45.2 | 70.8 | 15.2 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| DMC | 45.2 | 70.8 | 15.2 |'
- en: '| DMC-C | 45.5 | 70.6 | 14.6 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| DMC-C | 45.5 | 70.6 | 14.6 |'
- en: '| GQA | 4$\times$ | 34.7 | 68.3 | 14.0 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| GQA | 4$\times$ | 34.7 | 68.3 | 14.0 |'
- en: '| DMC | 43.9 | 70.2 | 16.5 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| DMC | 43.9 | 70.2 | 16.5 |'
- en: '| DMC-C | 38.2 | 69.6 | 14.6 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| DMC-C | 38.2 | 69.6 | 14.6 |'
- en: '| 13B | – | – | 54.5 | 73.5 | 17.5 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 13B | – | – | 54.5 | 73.5 | 17.5 |'
- en: '| GQA | 2$\times$ | 50.2 | 72.7 | 15.9 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| GQA | 2$\times$ | 50.2 | 72.7 | 15.9 |'
- en: '| DMC | 54.8 | 74.2 | 20.7 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| DMC | 54.8 | 74.2 | 20.7 |'
- en: '| DMC-C | 54.8 | 73.9 | 18.3 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| DMC-C | 54.8 | 73.9 | 18.3 |'
- en: '| GQA | 4$\times$ | 48.6 | 72.2 | 16.5 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| GQA | 4$\times$ | 48.6 | 72.2 | 16.5 |'
- en: '| DMC | 54.2 | 73.2 | 22.0 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| DMC | 54.2 | 73.2 | 22.0 |'
- en: '| DMC-C | 52.4 | 72.9 | 18.3 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| DMC-C | 52.4 | 72.9 | 18.3 |'
- en: '| 70B^∗ | – | 8$\times^{*}$ | 68.8 | 78.0 | 29.6 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 70B^∗ | – | 8$\times^{*}$ | 68.8 | 78.0 | 29.6 |'
- en: '| DMC | 16$\times^{*}$ | 68.8 | 77.9 | 29.9 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| DMC | 16$\times^{*}$ | 68.8 | 77.9 | 29.9 |'
- en: '|  | DMC-C | 16$\times^{*}$ | 67.4 | 78.2 | 31.1 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | DMC-C | 16$\times^{*}$ | 67.4 | 78.2 | 31.1 |'
- en: 'Table 3: MMLU accuracy (Acc.), Commonsense Question Answering (CS-QA) Exact
    Match (EM), and Human-Eval Pass@1 for several scales (7B, 13B, and 70B) and compression
    rates (CRs; $1\times$.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '表3: MMLU准确率（Acc.）、常识问答（CS-QA）准确匹配（EM）和多个尺度（7B、13B和70B）及压缩率（CRs; $1\times$）的人类评估通过率@1。'
- en: 'We then compare three strategies to determine how similar compression schemata
    for heads within each layer should be (assuming a global prior):'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们比较了三种策略，以确定每层内的头部之间的压缩方案应该有多相似（假设全局先验）：
- en: '1.'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'DMC: There are no constraints on the decision and importance scores, except
    for the global loss nudging the model towards a pre-defined CR.'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'DMC: 决策和重要性分数没有限制，除了全局损失会推动模型向预定义的CR靠拢。'
- en: '2.'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'DMC-C: Different heads can have varying decision and importance scores within
    each layer. However, an auxiliary loss encourages the model to maintain similar
    CRs among all heads within the layer.'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'DMC-C: 不同的头部在每一层中可能具有不同的决策和重要性分数。然而，一个辅助损失会鼓励模型在每层内保持所有头部之间的相似CR（压缩率）。'
- en: '3.'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'DMC-HardC: Decision scores $\alpha_{t}$ are shared across heads, leading to
    the same shortening schema within each layer across heads.'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'DMC-HardC: 决策分数$\alpha_{t}$在所有头部之间共享，导致每一层内的缩短方案在所有头部中相同。'
- en: 'As per [Figure 15](#A5.F15 "In Appendix E DMC Ablations ‣ Dynamic Memory Compression:
    Retrofitting LLMs for Accelerated Inference"), the default DMC strategy shows
    a consistent MMLU performance across varying CRs, while both DMC-C and DMC-HardC
    exhibit a sharp drop in MMLU as the compression reaches 1.9$\times$. Moreover,
    in [Table 3](#A5.T3 "In In-layer Relaxation ‣ Appendix E DMC Ablations ‣ Dynamic
    Memory Compression: Retrofitting LLMs for Accelerated Inference") we report a
    more thorough comparison between DMC and DMC-C. In general, while remaining superior
    to GQA, DMC-C displays a significant degradation in several configurations when
    compared to regular DMC.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[图 15](#A5.F15 "在附录 E DMC 消融 ‣ 动态记忆压缩：为加速推理调整 LLMs")，默认的 DMC 策略在不同的 CR 下表现出一致的
    MMLU 性能，而 DMC-C 和 DMC-HardC 在压缩达到 1.9$\times$ 时 MMLU 表现则急剧下降。此外，在[表 3](#A5.T3
    "在层内松弛 ‣ 附录 E DMC 消融 ‣ 动态记忆压缩：为加速推理调整 LLMs")中，我们报告了 DMC 和 DMC-C 之间更全面的比较。总体来说，尽管
    DMC-C 在 GQA 上依然表现优越，但与常规 DMC 相比，在多个配置下表现出显著的退化。
- en: Importance Scores
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重要性分数
- en: 'Finally, we assess the impact of predicting importance scores for accumulation
    as opposed to uniformly weighting each token in a group. [Figure 15](#A5.F15 "In
    Appendix E DMC Ablations ‣ Dynamic Memory Compression: Retrofitting LLMs for Accelerated
    Inference") shows that DMC-C with Uniform Weighting is worse than learned weighting
    DMC-C.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们评估了在累积中预测重要性分数与在组内对每个标记进行均匀加权的影响。[图 15](#A5.F15 "在附录 E DMC 消融 ‣ 动态记忆压缩：为加速推理调整
    LLMs") 显示，均匀加权的 DMC-C 比学习加权的 DMC-C 更差。
- en: Appendix F Masking Implementation Details
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 掩码实现细节
- en: 'We mask the unnormalized attention score for the pair $(i,j)$ as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对对 $(i,j)$ 的未归一化注意力分数进行如下掩码处理：
- en: '|  | $\hat{a}_{(i,j)}=\underbrace{\frac{\mathbf{q}_{i}[1:d_{h}]^{\top}\mathbf{k}_{j}[1:d_{h}]}{\sqrt{d_{h}}}}_{\text{attention
    score}}+\underbrace{\log(1-\alpha_{j+1})\vphantom{\frac{1}{\sqrt{d_{h}}}}}_{\text{attention
    mask}}.$ |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{a}_{(i,j)}=\underbrace{\frac{\mathbf{q}_{i}[1:d_{h}]^{\top}\mathbf{k}_{j}[1:d_{h}]}{\sqrt{d_{h}}}}_{\text{注意力分数}}+\underbrace{\log(1-\alpha_{j+1})\vphantom{\frac{1}{\sqrt{d_{h}}}}}_{\text{注意力掩码}}.$
    |  |'
- en: We rely on the memory efficient implementation of MHSA from PyTorch, which allows
    adding arbitrary masks to the attention scores before softmax. Notwithstanding
    this, at inference time DMC remains compatible with efficient libraries for attention
    such as Flash Attention (Dao et al., [2022](#bib.bib12)). The $\log(1-\alpha_{j+1})$
    for better numerical precision.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们依赖于 PyTorch 中 MHSA 的内存高效实现，它允许在 softmax 之前将任意掩码添加到注意力分数中。尽管如此，在推理时，DMC 仍然与像
    Flash Attention (Dao et al., [2022](#bib.bib12)) 这样的高效注意力库兼容。为了更好的数值精度，使用 $\log(1-\alpha_{j+1})$。
- en: Appendix G Limitations
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 限制
- en: 'This paper is focused on retrofitting existing LLMs into DMC variants. In our
    preliminary experiments with pre-training LLMs with DMC from scratch we obtained
    negative results when compared to the training curve of GQA. We speculate that
    this is due to the mutual dependency of modeling and segmenting data: when token
    representations are not of sufficient quality, boundary decisions are unreliable.
    Vice versa, incorrect boundary decisions may lead to poor token representations.
    This creates a vicious cycle which may be broken by techniques that facilitate
    convergence, such as an Expectation Maximization-style alternation between modeling
    and segmenting.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 本文重点在于将现有的 LLMs 转换为 DMC 变体。在我们用 DMC 从头开始预训练 LLMs 的初步实验中，与 GQA 的训练曲线相比，我们获得了负面结果。我们推测这是由于建模和数据分割的相互依赖：当标记表示的质量不足时，边界决策是不可靠的。反之，错误的边界决策可能导致糟糕的标记表示。这形成了一个恶性循环，可以通过促进收敛的技术来打破，例如建模与分割之间的期望最大化风格的交替。
