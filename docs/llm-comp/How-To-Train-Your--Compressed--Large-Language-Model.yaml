- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:04:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: How To Train Your (Compressed) Large Language Model
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何训练你的（压缩的）大型语言模型
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.14864](https://ar5iv.labs.arxiv.org/html/2305.14864)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.14864](https://ar5iv.labs.arxiv.org/html/2305.14864)
- en: Ananya Harsh Jha^∗, Tom Sherborne^⋄, Evan Pete Walsh^∗,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ananya Harsh Jha^∗, Tom Sherborne^⋄, Evan Pete Walsh^∗,
- en: Dirk Groeneveld^∗, Emma Strubell^(†∗), Iz Beltagy^∗
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Dirk Groeneveld^∗, Emma Strubell^(†∗), Iz Beltagy^∗
- en: ^∗Allen Institute for Artificial Intelligence
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗艾伦人工智能研究所
- en: ^⋄Institute for Language, Cognition and Computation, University of Edinburgh
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^⋄爱丁堡大学语言、认知与计算研究所
- en: ^†Language Technologies Institute, Carnegie Mellon University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ^†语言技术研究所，卡内基梅隆大学
- en: ananyaj@allenai.org
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ananyaj@allenai.org
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: With the increase in the size of large language models (LLMs), we need compression
    methods that can reduce the model size while preserving the generality and zero-shot
    promptability of the model. This goal is more ambitious than the typical compression
    setup, which reduces the model’s size at the expense of specializing it to a specific
    end-task. To study this, we develop a task-agnostic compression pipeline with
    a large-scale evaluation comprising language modeling perplexity and 12 zero-shot
    end-tasks. Our results show that a simple layer-wise pruning followed by continued
    language model pretraining matches or outperforms three existing state-of-the-art
    baselines while being 1.5x more computationally efficient. However, unlike typical
    task-specialized compression, our best-compressed model significantly underperforms
    a similar-sized model trained from scratch. We posit the half-sized pretrained
    model as an upper bound for task-agnostic compression and call for future work
    to bridge this gap under a reasonable token budget. Our findings highlight the
    inadequacy of existing compression methods for LLMs and establish a requirement
    for new methods that preserve a model’s generality and zero-shot promptability
    under compression. We release our code and evaluation setup to facilitate reproducibility
    and help iterate on method design.¹¹1[https://github.com/anonymous](https://github.com/anonymous)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）规模的增加，我们需要压缩方法，以减少模型大小，同时保持模型的一般性和零样本提示能力。这个目标比典型的压缩设置更具挑战性，后者在专门化于特定最终任务的同时减少模型的大小。为此，我们开发了一个任务无关的压缩管道，并进行了大规模评估，包括语言建模困惑度和12个零样本最终任务。我们的结果表明，简单的逐层剪枝后跟随继续语言模型预训练可以匹配或超越三种现有的最先进基准，同时计算效率提高了1.5倍。然而，与典型的任务专用压缩不同，我们的最佳压缩模型在性能上显著低于一个相似大小的从零开始训练的模型。我们认为，预训练模型的一半大小是任务无关压缩的上限，并呼吁未来在合理的令牌预算下填补这一差距。我们的研究结果突显了现有压缩方法对于LLMs的不足，并建立了在压缩下保留模型一般性和零样本提示能力的新方法的需求。我们发布了我们的代码和评估设置，以促进重复性并帮助方法设计的迭代。¹¹1[https://github.com/anonymous](https://github.com/anonymous)
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have evolved considerably in size, architecture,
    and usage patterns since the introduction of ELMo (Peters et al., [2018](#bib.bib37)),
    BERT (Devlin et al., [2019](#bib.bib16)), and RoBERTa (Liu et al., [2019](#bib.bib32)).
    BERT-style pretrained LLMs allow for rapid specialization to different tasks via
    supervised finetuning but often contain minimal zero-shot capability. Conversely,
    modern LLMs such as GPT-3 (Brown et al., [2020](#bib.bib5)), PaLM (Chowdhery et al.,
    [2022](#bib.bib9)), and LLaMA (Zhang et al., [2023](#bib.bib61)) are challenging
    to exploit for task-specific finetuning but have proven highly capable at gradient-free
    specialization, e.g., in-context learning with zero- or few-shot data. Such practice
    is now the norm for using contemporary LLMs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）自ELMo（Peters et al., [2018](#bib.bib37)）、BERT（Devlin et al., [2019](#bib.bib16)）和RoBERTa（Liu
    et al., [2019](#bib.bib32)）问世以来，在规模、架构和使用模式上有了显著的发展。BERT风格的预训练LLMs通过监督微调可以快速专门化到不同任务，但通常具有最小的零样本能力。相反，现代LLMs如GPT-3（Brown
    et al., [2020](#bib.bib5)）、PaLM（Chowdhery et al., [2022](#bib.bib9)）和LLaMA（Zhang
    et al., [2023](#bib.bib61)）在任务特定的微调方面难以利用，但在无梯度专门化（例如，使用零样本或少样本数据的上下文学习）方面表现出色。这种做法现在已成为使用当代LLMs的常态。
- en: While language models have grown in size, scale, and zero-shot capability, techniques
    for compressing such artifacts to smaller and more efficient models have not kept
    pace. Most works on compressing models via *pruning* (Sajjad et al., [2020](#bib.bib42);
    Voita et al., [2019](#bib.bib54); Chen et al., [2020b](#bib.bib8), [a](#bib.bib6))
    or *knowledge distillation* Sanh et al. ([2019](#bib.bib44)); Wang et al. ([2020b](#bib.bib57));
    Liang et al. ([2023](#bib.bib31)) show success only for BERT-style models requiring
    task-specific finetuning. Whether these techniques can be adapted to contemporary
    GPT-style LLMs and evaluated with zero-shot prompting remains an open question.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管语言模型在规模、大小和零样本能力上都有所增长，但将这些模型压缩成更小、更高效的模型的技术却没有跟上。大多数关于通过*剪枝*（Sajjad 等人，[2020](#bib.bib42)；Voita
    等人，[2019](#bib.bib54)；Chen 等人，[2020b](#bib.bib8)，[a](#bib.bib6)）或*知识蒸馏*（Sanh 等人，[2019](#bib.bib44)；Wang
    等人，[2020b](#bib.bib57)；Liang 等人，[2023](#bib.bib31)）来压缩模型的工作仅对需要任务特定微调的 BERT 风格模型取得了成功。这些技术是否可以适用于当代的
    GPT 风格大型语言模型并通过零样本提示进行评估仍然是一个悬而未决的问题。
- en: The pipeline for compressing larger models should preserve the generality and
    zero-shot promptability of the original and avoid any task-specific finetuning.
    Intuitively, this is a more challenging goal because task-specific compression
    often works at the expense of a model’s generality. In contrast, our goal is to
    preserve the larger model’s abilities. It is unclear if the methods designed for
    the former are also suitable for the latter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩大型模型的流程应保持原模型的通用性和零样本提示能力，避免任何任务特定的微调。直观上，这一目标更具挑战性，因为任务特定的压缩往往以模型通用性为代价。相反，我们的目标是保持大型模型的能力。尚不清楚为前者设计的方法是否也适用于后者。
- en: In this work, we consider the open question of how to distill GPT-style large
    language models. We study the efficacy of state-of-the-art task-agnostic distillation
    methods with zero-shot evaluation on 12 diverse tasks. Results show that our simple
    teacher-free approach with continued pretraining matches or outperforms existing
    baselines while being 1.5x more computationally efficient.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们考虑了如何蒸馏 GPT 风格的大型语言模型这一悬而未决的问题。我们研究了最先进的任务无关蒸馏方法在 12 个多样化任务上的零样本评估效果。结果表明，我们的简单无教师方法与持续预训练相结合，与现有基准相匹配或超越，同时在计算效率上提高了
    1.5 倍。
- en: 'In further experiments, we compare our best-compressed model to an equally-sized
    model trained from scratch. Our zero-shot evaluation shows that the compressed
    model underperforms compared to a model trained from scratch. This opposes the
    trend from task-specialized compression, where training large then compressing
    is preferred over training a smaller model from scratch. Here we club together
    task-specific and task-agnostic compression with end-task finetuning under task-specialized
    compression. We offer analysis and explanation for this phenomenon rooted in how
    much data is available for pretraining or finetuning stages. Our contributions
    are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步的实验中，我们将我们最好的压缩模型与一个大小相等的从头训练的模型进行了比较。我们的零样本评估显示，与从头训练的模型相比，压缩模型的表现较差。这与任务专用压缩中的趋势相反，在任务专用压缩中，通常倾向于先训练大模型再进行压缩，而不是从头训练一个较小的模型。在这里，我们将任务特定和任务无关的压缩与任务特定压缩下的任务最终微调结合在一起。我们对这种现象进行了分析和解释，根源在于预训练或微调阶段的数据可用性。我们的贡献包括：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A characterization of how the benefits of distillation poorly transfer to a
    zero-shot setup.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 蒸馏的好处如何在零样本设置中转移效果不佳的特征描述。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A simple alternative compression method which is both performant (on perplexity
    and end-tasks) and efficient (1.5x faster training) at the LLM scale.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种简单的替代压缩方法，既具备较好的性能（在困惑度和最终任务上）又具备较高的效率（在大型语言模型规模上训练速度提高了 1.5 倍）。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Insight into how typical compression techniques transfer poorly at scale with
    promising new angles for future methods to consider.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对典型压缩技术在规模上效果不佳的深入见解，并提出了未来方法值得考虑的新角度。
- en: We argue that LLM compression methods should focus on zero-shot evaluation to
    encourage the development of new methods that maintain model generality. Our findings
    highlight that existing compression methods transfer poorly to large-scale general
    compression settings, and there is a larger gap between current methods and upper-bound
    performance. We advocate for the future development of compression methods for
    LLMs to close this gap and promote model reuse and training efficiency.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为LLM压缩方法应关注零-shot评估，以鼓励开发保持模型通用性的新的方法。我们的研究结果突出显示，现有的压缩方法在大规模通用压缩设置中转移效果较差，当前方法与上限性能之间存在较大差距。我们倡导未来发展LLM的压缩方法，以缩小这一差距，促进模型的重用和训练效率。
- en: 2 Related Work
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: Pruning
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 剪枝
- en: 'seeks to identify sub-networks within larger models which yield equivalent
    performance with increased computational efficiency. The parts of the model are
    can be pruned using a heuristic or scoring function (Sanh et al., [2020](#bib.bib45);
    Li et al., [2021a](#bib.bib28)). Pruning can be described in three steps: (i)
    model initialization; (ii) training to specialize for relevant data; (iii) evaluation
    against the initial model. Pruning literature can broadly be divided between structured
    and unstructured methods.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 旨在识别在更大模型中具有等效性能的子网络，并提高计算效率。模型的部分可以使用启发式方法或评分函数进行剪枝（Sanh et al., [2020](#bib.bib45);
    Li et al., [2021a](#bib.bib28)）。剪枝可以分为三个步骤：（i）模型初始化；（ii）针对相关数据进行训练以专门化；（iii）与初始模型进行评估。剪枝文献可以大致分为结构化和非结构化方法。
- en: Structured pruning approaches (Fan et al., [2019](#bib.bib19); Kao et al., [2022](#bib.bib26))
    aim to prune larger blocks within a model while considering architecture nuances
    for improving inference speed. Structured pruning can be coarse- or fine-grained.
    Coarse-grained pruning removes entire model layers (Fan et al., [2019](#bib.bib19)).
    In the case of LMs, fine-grain pruning methods prunes atomic components like attention-heads (Voita
    et al., [2019](#bib.bib54); Michel et al., [2019](#bib.bib34); Li et al., [2021b](#bib.bib29))
    or hidden layers (Hou et al., [2020](#bib.bib24); Chen et al., [2020b](#bib.bib8);
    McCarley et al., [2019](#bib.bib33)). Another line of structured-pruning approach (Kao
    et al., [2022](#bib.bib26)) aims to prune two out of every four weights in a matrix
    for 2:4 sparsity speedups on A100s. Unstructured pruning follows the lottery ticket
    hypothesis (Chen et al., [2020a](#bib.bib6)), where the weights of a network are
    iteratively pruned without concern for structure-based speedups.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化剪枝方法（Fan et al., [2019](#bib.bib19); Kao et al., [2022](#bib.bib26)）旨在在模型中剪枝更大的块，同时考虑架构细节以提高推理速度。结构化剪枝可以是粗粒度的或细粒度的。粗粒度剪枝去除整个模型层（Fan
    et al., [2019](#bib.bib19)）。在语言模型的情况下，细粒度剪枝方法剪除诸如注意力头（Voita et al., [2019](#bib.bib54);
    Michel et al., [2019](#bib.bib34); Li et al., [2021b](#bib.bib29)）或隐藏层（Hou et
    al., [2020](#bib.bib24); Chen et al., [2020b](#bib.bib8); McCarley et al., [2019](#bib.bib33)）等原子组件。另一种结构化剪枝方法（Kao
    et al., [2022](#bib.bib26)）旨在剪去矩阵中的四个权重中的两个，以在A100s上实现2:4稀疏性加速。非结构化剪枝遵循彩票票据假设（Chen
    et al., [2020a](#bib.bib6)），其中网络的权重被迭代地剪枝，而不考虑基于结构的加速。
- en: The other classification approach for pruning is based on task-agnostic vs.
    task-specific pruning. Task-agnostic pruning approaches (Chen et al., [2020b](#bib.bib8);
    Fan et al., [2019](#bib.bib19)) prune a model on pretraining data and then add
    task specialization as a second step. Task-specific pruning (Voita et al., [2019](#bib.bib54);
    McCarley et al., [2019](#bib.bib33); Michel et al., [2019](#bib.bib34)) methods
    specialize their models on end-task data during the pruning process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种剪枝分类方法基于任务无关的剪枝与任务特定的剪枝。任务无关的剪枝方法（Chen et al., [2020b](#bib.bib8); Fan et
    al., [2019](#bib.bib19)）在预训练数据上进行模型剪枝，然后将任务专门化作为第二步。任务特定的剪枝（Voita et al., [2019](#bib.bib54);
    McCarley et al., [2019](#bib.bib33); Michel et al., [2019](#bib.bib34)）方法在剪枝过程中对模型进行任务数据专门化。
- en: We acknowledge a related line of work in quantization (Dettmers et al., [2022](#bib.bib15),
    *inter alia*) for model compression; however, quantization methods are orthogonal
    to our approach and can be applied after a model has been compressed using our
    method.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们承认量化（Dettmers et al., [2022](#bib.bib15)，*其中包括*）在模型压缩方面的相关研究；然而，量化方法与我们的方法正交，可以在使用我们的方法压缩模型之后应用。
- en: Knowledge Distillation
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 知识蒸馏
- en: can be similarly divided between task-specific (Sun et al., [2019](#bib.bib49);
    Turc et al., [2019](#bib.bib52); Mukherjee et al., [2021](#bib.bib36); Tang et al.,
    [2019](#bib.bib51); Xia et al., [2022](#bib.bib59)) and task-agnostic methods (Sanh
    et al., [2019](#bib.bib44); Jiao et al., [2019](#bib.bib25); Sun et al., [2020](#bib.bib50);
    Wang et al., [2020b](#bib.bib57), [a](#bib.bib56); Liang et al., [2023](#bib.bib31)).
    For existing distillation literature on BERT-style models, we use task-specialized
    distillation for both methods. The reason for this is apparent in the case of
    task-specific methods, which distill task information from a teacher model into
    the student. However, even task-agnostic methods are specialized on end-tasks
    by finetuning following distillation on pretraining data, and hence the classification
    term for both categories.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 可以类似地分为任务特定的（Sun et al., [2019](#bib.bib49); Turc et al., [2019](#bib.bib52);
    Mukherjee et al., [2021](#bib.bib36); Tang et al., [2019](#bib.bib51); Xia et
    al., [2022](#bib.bib59)）和任务无关的方法（Sanh et al., [2019](#bib.bib44); Jiao et al.,
    [2019](#bib.bib25); Sun et al., [2020](#bib.bib50); Wang et al., [2020b](#bib.bib57),
    [a](#bib.bib56); Liang et al., [2023](#bib.bib31)）。对于现有的BERT风格模型蒸馏文献，我们对这两种方法使用任务专用蒸馏。这在任务特定方法的情况下是显而易见的，这些方法将任务信息从教师模型蒸馏到学生模型中。然而，即使是任务无关的方法，也通过在预训练数据上进行蒸馏后微调来专注于最终任务，因此两个类别都有分类术语。
- en: There are methods that follow pruning with a distillation phase (Hou et al.,
    [2020](#bib.bib24); McCarley et al., [2019](#bib.bib33)). More recently, methods
    like Co-Fi (Xia et al., [2022](#bib.bib59)) and Homotopic-distillation (Liang
    et al., [2023](#bib.bib31)) combine the pruning phase with distillation by applying
    distillation losses during the pruning process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些方法跟随修剪阶段后进行蒸馏（Hou et al., [2020](#bib.bib24); McCarley et al., [2019](#bib.bib33)）。最近，像Co-Fi（Xia
    et al., [2022](#bib.bib59)）和同源蒸馏（Liang et al., [2023](#bib.bib31)）的方法通过在修剪过程中应用蒸馏损失，将修剪阶段与蒸馏结合起来。
- en: 3 Training (Compressed) LLMs
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 训练（压缩）LLMs
- en: We focus on commonly available distillation methodologies to study how standard
    practices (Sanh et al., [2019](#bib.bib44), *inter alia*) apply to our setting
    of compressing decoder-only LLMs. Unlike prior work, our compressed model is evaluated
    with zero-shot tasks without any end-task finetuning. This setting presents a
    more challenging situation than previously explored by distillation literature,
    wherein the methods benefit from the final stage of supervised task-specific finetuning.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注常见的蒸馏方法，研究标准实践（Sanh et al., [2019](#bib.bib44)，*等*）如何应用于压缩仅解码器LLMs的设置。与之前的工作不同，我们的压缩模型通过零-shot任务进行评估，而没有任何最终任务微调。这种设置比蒸馏文献中以往探索的情况更具挑战性，以往的方法在监督任务特定微调的最终阶段受益。
- en: When it’s assumed that supervised finetuning follows compression, the standard
    for compressed model quality can be much lower. For task-specialized compression (Jiao
    et al., [2019](#bib.bib25); Sun et al., [2020](#bib.bib50); Liang et al., [2023](#bib.bib31);
    Sanh et al., [2019](#bib.bib44)), reduced model capacity is needed compared to
    when models need to maintain generality. Also, parameters updated during the finetuning
    phase can compensate for task-specific information lost during compression. When
    optimizing for in-context learning, zero-shot evaluations will likely be more
    sensitive to removing parameters.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当假设监督微调跟随压缩时，压缩模型质量的标准可以低得多。对于任务专用的压缩（Jiao et al., [2019](#bib.bib25); Sun et
    al., [2020](#bib.bib50); Liang et al., [2023](#bib.bib31); Sanh et al., [2019](#bib.bib44)），与需要保持通用性的模型相比，需要减少模型容量。此外，微调阶段更新的参数可以弥补压缩过程中丢失的任务特定信息。在优化上下文学习时，零-shot评估可能对去除参数更为敏感。
- en: 'Our principal research question is: *How can we effectively compress GPT-style
    models to maintain zero-shot promptability?* Within this question, we study the
    efficacy of distillation without finetuning for our class of models (described
    in Section [4](#S4 "4 Experimental Setup ‣ How To Train Your (Compressed) Large
    Language Model")). We consider how to initialize, prune, and distill a student
    model from an LLM. We describe our initialization strategies, distillation and
    language modeling objectives, and our straightforward methodology for learning
    a compressed model with continued pretraining.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要的研究问题是：*我们如何有效地压缩GPT风格的模型以保持零-shot的提示能力？* 在这个问题中，我们研究了在不进行微调的情况下，蒸馏对我们这类模型的有效性（详见第[4](#S4
    "4 Experimental Setup ‣ How To Train Your (Compressed) Large Language Model")节）。我们考虑如何初始化、修剪和从LLM中蒸馏学生模型。我们描述了初始化策略、蒸馏和语言建模目标，以及学习压缩模型的简单方法。
- en: 3.1 Language Modeling Objective
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 语言建模目标
- en: 'Let $X$. A language model computes the probability of the sequence by factorizing
    over the probability of each next token prediction given the prior context tokens:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $X$。语言模型通过对每个下一个标记预测的概率进行因式分解，从而计算序列的概率，前提是给定了先前的上下文标记：
- en: '|  | $p(X)=\prod_{i=1}^{N}p\left(x_{i}&#124;x_{1},x_{2},...,x_{i-1}\right)$
    |  | (1) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(X)=\prod_{i=1}^{N}p\left(x_{i}&#124;x_{1},x_{2},...,x_{i-1}\right)$
    |  | (1) |'
- en: We limit the scope of our work to causal auto-regressive decoders Radford et al.
    ([2018](#bib.bib38)), composed of Transformer decoder layers Vaswani et al. ([2017](#bib.bib53)).
    This model, parameterized by $\theta$ context tokens Bengio et al. ([2003](#bib.bib2)).
    Equation [2](#S3.E2 "In 3.1 Language Modeling Objective ‣ 3 Training (Compressed)
    LLMs ‣ How To Train Your (Compressed) Large Language Model") defines this learning
    objective.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将工作范围限制为因果自回归解码器 Radford 等 ([2018](#bib.bib38))，该解码器由 Transformer 解码器层 Vaswani
    等 ([2017](#bib.bib53)) 组成。这个模型由 $\theta$ 上下文标记 Bengio 等 ([2003](#bib.bib2)) 参数化。方程
    [2](#S3.E2 "In 3.1 Language Modeling Objective ‣ 3 Training (Compressed) LLMs
    ‣ How To Train Your (Compressed) Large Language Model") 定义了这个学习目标。
- en: '|  | $\mathcal{L}_{\rm LM}=\log p_{\theta}(X)=\sum_{i=1}^{N}p(x_{i}&#124;x_{i-k},...,x_{i-1};\theta)$
    |  | (2) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\rm LM}=\log p_{\theta}(X)=\sum_{i=1}^{N}p(x_{i}&#124;x_{i-k},...,x_{i-1};\theta)$
    |  | (2) |'
- en: 3.2 Distillation Objectives
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 蒸馏目标
- en: Task-agnostic distillation literature for “BERT-style” models Sanh et al. ([2019](#bib.bib44));
    Wang et al. ([2020b](#bib.bib57)); Jiao et al. ([2019](#bib.bib25)) begins with
    a single-step layer-pruned initialization of the student. Following this, knowledge
    distillation uses one or more distillation objectives to align the output and/or
    intermediate states between the student and teacher. We first evaluate a “vanilla”
    distillation technique as a baseline, training a student with the language modeling
    objective combined with the distillation objective in Equation [3](#S3.E3 "In
    3.2 Distillation Objectives ‣ 3 Training (Compressed) LLMs ‣ How To Train Your
    (Compressed) Large Language Model"). $\mathcal{L}_{\rm distill}$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“BERT 风格”模型的任务无关的蒸馏文献包括 Sanh 等 ([2019](#bib.bib44)); Wang 等 ([2020b](#bib.bib57));
    Jiao 等 ([2019](#bib.bib25)) 从学生模型的单步层修剪初始化开始。之后，知识蒸馏使用一个或多个蒸馏目标来对齐学生和教师之间的输出和/或中间状态。我们首先评估一个“普通的”蒸馏技术作为基准，将语言建模目标与方程
    [3](#S3.E3 "In 3.2 Distillation Objectives ‣ 3 Training (Compressed) LLMs ‣ How
    To Train Your (Compressed) Large Language Model") 中的蒸馏目标相结合来训练学生模型。$\mathcal{L}_{\rm
    distill}$。
- en: '|  | $\mathcal{L}_{\rm distill}~{}=~{}{\rm KL}\left(f_{s}\left(x_{i}\right),~{}f_{t}\left(x_{i}\right)\right)$
    |  | (3) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\rm distill}~{}=~{}{\rm KL}\left(f_{s}\left(x_{i}\right),~{}f_{t}\left(x_{i}\right)\right)$
    |  | (3) |'
- en: We consider the ideas introduced in miniLM (Wang et al., [2020b](#bib.bib57))
    and miniLMv2 (Wang et al., [2020a](#bib.bib56)) for our second distillation baseline.
    Using the idea of relation transfer between $Q$ is the dimensionality of each
    model. Using only two relation transfers follows the memory-performance tradeoff
    from miniLMv2, and we adapt it to a causal-decoder setup by masking. The final
    loss function for this baseline contains the LM objective, the distillation objective
    from Eq. [3](#S3.E3 "In 3.2 Distillation Objectives ‣ 3 Training (Compressed)
    LLMs ‣ How To Train Your (Compressed) Large Language Model"), and the two relation
    transfer KL-divergence terms.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑在 miniLM (Wang et al., [2020b](#bib.bib57)) 和 miniLMv2 (Wang et al., [2020a](#bib.bib56))
    中提出的思想作为我们第二个蒸馏基准。使用关系传递的思想，其中 $Q$ 是每个模型的维度。只使用两个关系传递遵循 miniLMv2 的内存-性能权衡，我们通过掩码将其适配到因果解码器设置中。这个基准的最终损失函数包含
    LM 目标、来自 Eq. [3](#S3.E3 "In 3.2 Distillation Objectives ‣ 3 Training (Compressed)
    LLMs ‣ How To Train Your (Compressed) Large Language Model") 的蒸馏目标，以及两个关系传递的 KL
    散度项。
- en: '|  | $\mathcal{L}_{QK}={\rm KL}\left({\rm softmax}\left(\frac{Q_{s}K_{s}}{\sqrt{d_{s}}}\right),{\rm
    softmax}\left(~{}\frac{Q_{t}K_{t}}{\sqrt{d_{t}}}\right)\right)$ |  | (4) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{QK}={\rm KL}\left({\rm softmax}\left(\frac{Q_{s}K_{s}}{\sqrt{d_{s}}}\right),{\rm
    softmax}\left(~{}\frac{Q_{t}K_{t}}{\sqrt{d_{t}}}\right)\right)$ |  | (4) |'
- en: Our third and final baseline draws from tinyBERT (Jiao et al., [2019](#bib.bib25))
    and homotopic distillation (Liang et al., [2023](#bib.bib31)). This baseline aligns
    the intermediate states ($\mathcal{L}_{\rm hid}$ are the respective embedding
    tables. Since we do not prune in the hidden dimensions of the student, there is
    no need to learn a projection to match the dimensionality between the models.
    Equation [8](#S3.E8 "In 3.2 Distillation Objectives ‣ 3 Training (Compressed)
    LLMs ‣ How To Train Your (Compressed) Large Language Model") is the complete loss
    function.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第三个也是最后一个基线来源于 tinyBERT（Jiao 等，[2019](#bib.bib25)）和同调蒸馏（Liang 等，[2023](#bib.bib31)）。这个基线对齐中间状态（$\mathcal{L}_{\rm
    hid}$ 是相应的嵌入表。由于我们不在学生模型的隐藏维度中进行剪枝，因此无需学习一个投影以匹配模型之间的维度。方程 [8](#S3.E8 "在 3.2 蒸馏目标
    ‣ 3 训练（压缩）大语言模型 ‣ 如何训练你的（压缩）大型语言模型") 是完整的损失函数。
- en: '|  | $\displaystyle\mathcal{L}_{\rm hid}\left(\theta_{s},\theta_{t}\right)$
    |  | (5) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\rm hid}\left(\theta_{s},\theta_{t}\right)$
    |  | (5) |'
- en: '|  | $\displaystyle\mathcal{L}_{\rm emb}\left(\theta_{s},\theta_{t}\right)$
    |  | (6) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\rm emb}\left(\theta_{s},\theta_{t}\right)$
    |  | (6) |'
- en: '|  | $\displaystyle\mathcal{L}_{\rm att}\left(\theta_{s},\theta_{t}\right)$
    |  | (7) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\rm att}\left(\theta_{s},\theta_{t}\right)$
    |  | (7) |'
- en: '|  | $\displaystyle\mathcal{L}_{\Sigma}=\mathcal{L}_{\rm LM}$ |  | (8) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\Sigma}=\mathcal{L}_{\rm LM}$ |  | (8) |'
- en: 3.3 Truncated Initialization Strategy
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 截断初始化策略
- en: '![Refer to caption](img/b166ad33986cce4e31592b2351458064.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b166ad33986cce4e31592b2351458064.png)'
- en: 'Figure 1: Truncated initialization configurations for layer pruning in a decoder-only
    language model. Highlighted layers (green) are removed. Our method and distillation
    baselines remove half of the layers according to each configuration. We retain
    the first and last layer as these layers interact with the embedding table.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 解码器仅语言模型中层剪枝的截断初始化配置。高亮层（绿色）被移除。我们的方法和蒸馏基线根据每个配置去除一半的层。我们保留第一层和最后一层，因为这些层与嵌入表进行交互。'
- en: The goal of initialization in a compression pipeline is choosing a subset of
    larger model parameters to retain maximum information about the data. We follow
    conventional distillation work (Sanh et al., [2019](#bib.bib44)) in using a layer-level
    pruning strategy. This produces a *truncated initialization* of a subset of teacher
    layers for the student. Choosing *where* and *when* to prune layers is a critical
    design decision to maximize performance recovery after pruning. Figure [1](#S3.F1
    "Figure 1 ‣ 3.3 Truncated Initialization Strategy ‣ 3 Training (Compressed) LLMs
    ‣ How To Train Your (Compressed) Large Language Model") outlines the configurations
    we consider studying *where* to prune half the model layers. Considering *when*
    to enact pruning primarily concerns either pruning all layers at initialization
    or incrementally removing layers periodically during continued pretraining (described
    in Section [3.4](#S3.SS4 "3.4 Continued Pretraining ‣ 3 Training (Compressed)
    LLMs ‣ How To Train Your (Compressed) Large Language Model")). Our intuition here
    is that incremental layer removal yields improved training stability. We present
    results and ablations for these two questions in Section [5](#S5 "5 Compression
    Results ‣ How To Train Your (Compressed) Large Language Model").
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩管道中的初始化目标是选择较大模型参数的一个子集，以保留有关数据的最大信息。我们遵循传统的蒸馏工作（Sanh 等，[2019](#bib.bib44)）使用层级剪枝策略。这会产生学生模型的一个*截断初始化*，该初始化包括教师模型的部分层。选择*何时*和*在哪里*剪枝层是一个关键设计决策，以最大化剪枝后的性能恢复。图
    [1](#S3.F1 "图 1 ‣ 3.3 截断初始化策略 ‣ 3 训练（压缩）大语言模型 ‣ 如何训练你的（压缩）大型语言模型") 概述了我们考虑研究的*剪枝*模型层的一半的位置。考虑*何时*执行剪枝主要涉及在初始化时剪枝所有层或在继续预训练期间周期性地逐步去除层（描述于第
    [3.4](#S3.SS4 "3.4 继续预训练 ‣ 3 训练（压缩）大语言模型 ‣ 如何训练你的（压缩）大型语言模型") 节）。我们这里的直觉是，逐步去除层会提高训练稳定性。我们在第
    [5](#S5 "5 压缩结果 ‣ 如何训练你的（压缩）大型语言模型") 节中展示了这两个问题的结果和消融实验。
- en: 3.4 Continued Pretraining
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 继续预训练
- en: While we explore distillation in our setup, the study of introducing additional
    student-teacher alignment is somewhat saturated. Unlike prior work, we observe
    that the additional distillation signals (outlined above) do not significantly
    contribute to the student learning process. In fact, in some instances, they prove
    to be detrimental. We hypothesize that this supervision *over-constrains* the
    student to be unable to adequately model the data and recover performance lost
    during layer pruning. Therefore, we consider the alternative, continued pretraining
    of the student *without teacher supervision*. We remove all distillation objectives
    and instead continue pretraining the student, initialized with a truncated subset
    of layers, with the same corpus and objective as the teacher model. Our proposal
    resembles *domain adaptive pretraining* (Gururangan et al., [2020](#bib.bib21));
    however, our goal of task-agnostic compression requires domain-agnostic continued
    pretraining to maintain model generality. Section [5](#S5 "5 Compression Results
    ‣ How To Train Your (Compressed) Large Language Model") highlights that this strategy
    is superior to distillation objectives given a limited token budget.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在设置中探索了蒸馏，但引入额外的学生-教师对齐的研究已经有所饱和。与以往的工作不同，我们观察到，额外的蒸馏信号（如上所述）并未显著促进学生学习过程。事实上，在某些情况下，它们反而证明是有害的。我们推测，这种监督*过度约束*了学生，使其无法充分建模数据，并恢复在层剪枝过程中丧失的性能。因此，我们考虑另一种方法，即在*没有教师监督*的情况下继续对学生进行预训练。我们移除了所有蒸馏目标，而是继续对学生进行预训练，学生模型初始化为一个经过截断的层子集，使用与教师模型相同的语料库和目标。我们的提案类似于*领域自适应预训练*（Gururangan等，[2020](#bib.bib21)）；然而，我们的任务无关压缩目标需要领域无关的继续预训练，以保持模型的通用性。第[5](#S5
    "5 Compression Results ‣ How To Train Your (Compressed) Large Language Model")节强调，在令牌预算有限的情况下，这一策略优于蒸馏目标。
- en: 4 Experimental Setup
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验设置
- en: 4.1 Base Models
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基础模型
- en: Params Dim Heads Layers Batch Size LR Token Budget 180M 1024 16 12 2M 6.0e-4
     160B 300M 1024 16 24 2M 3.0e-4  160B 610M 2048 16 12 2M 2.5e-4  160B 1.1B 2048
    16 24 2M 2.0e-4  160B
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 维度 头数 层数 批量大小 学习率 令牌预算 180M 1024 16 12 2M 6.0e-4  160B 300M 1024 16 24 2M
    3.0e-4  160B 610M 2048 16 12 2M 2.5e-4  160B 1.1B 2048 16 24 2M 2.0e-4  160B
- en: 'Table 1: Configuration for Decoder-only Transformers. Each is trained on C4
    as pretraining baseline. The 2M token batch size follows Biderman et al. ([2023](#bib.bib3)).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：仅解码器Transformer的配置。每个模型都在C4上作为预训练基线进行训练。2M令牌批量大小遵循Biderman等人（[2023](#bib.bib3)）。
- en: We pretrain our own baseline models for exact control over token budgets and
    model configurations²²2We have confirmed that our baseline models achieve similar
    zero-shot performance to existing pretrained models.. However, our proposed compression
    method can be applied to any publicly available decoder-only model checkpoint
    e.g., Zhang et al. ([2022](#bib.bib62), [2023](#bib.bib61)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对自己的基线模型进行了预训练，以准确控制令牌预算和模型配置²²2我们已经确认，我们的基线模型在零样本情况下达到与现有预训练模型相似的性能。然而，我们提出的压缩方法可以应用于任何公开可用的仅解码器模型检查点，例如，Zhang等人（[2022](#bib.bib62)，[2023](#bib.bib61)）。
- en: We follow the PaLM architecture (Chowdhery et al., [2022](#bib.bib9)) owing
    to improved throughput efficiency. Specifically, the attention and feed-forward
    network (FFN) modules are parallel instead of sequential (Radford et al., [2019](#bib.bib39)).
    SwiGLU activation (Shazeer, [2020](#bib.bib46)) is used in the FFN module. Multi-head
    attention uses the equivalent Flash-Attention (Dao et al., [2022](#bib.bib12),
    FA) implementation. The first layer of the FFN module and the layers generating
    attention query, key, and value are fused. Similarly, the second layer of the
    FFN module and the feed-forward layer after the attention operation are fused.
    The LayerNorm (Ba et al., [2016](#bib.bib1)) is before the first fused feed-forward
    layer. The query and the key vectors are passed through additional layer normalization
    layers for increased training stability following Dehghani et al. ([2023](#bib.bib14)).
    This block structure is repeated with skip connections to form our decoder-only
    Transformer architecture.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循了PaLM架构（Chowdhery等，[2022](#bib.bib9)），因为它在吞吐量效率方面有所改善。具体而言，注意力和前馈网络（FFN）模块是并行的，而不是顺序的（Radford等，[2019](#bib.bib39)）。FFN模块中使用了SwiGLU激活（Shazeer，[2020](#bib.bib46)）。多头注意力使用等效的Flash-Attention（Dao等，[2022](#bib.bib12)，FA）实现。FFN模块的第一层和生成注意力查询、键和值的层被融合在一起。类似地，FFN模块的第二层和注意力操作后的前馈层也被融合。LayerNorm（Ba等，[2016](#bib.bib1)）位于第一个融合的前馈层之前。查询和键向量经过额外的层归一化层，以增加训练稳定性，参考Dehghani等人（[2023](#bib.bib14)）。该块结构通过跳过连接重复形成我们的仅解码器Transformer架构。
- en: Using the language modeling objective, we train two core baselines with 300
    million (M) and 1.1 billion (B) parameters. We also consider additional baseline
    models containing half the number of layers (180M and 610M parameters). We use
    Lion optimizer (Chen et al., [2023](#bib.bib7)) in all our pretraining experiments
    with $\beta$ but is omitted for bias and normalization parameters.³³3We follow
    default hyper-parameters suggested by Composer [https://www.mosaicml.com/composer](https://www.mosaicml.com/composer)
    We schedule learning rate warm-up from the start to 2000 steps (4B tokens) and
    then use a cosine decay schedule until training stops. The final learning rate
    is 10% of the peak value. Table [1](#S4.T1 "Table 1 ‣ 4.1 Base Models ‣ 4 Experimental
    Setup ‣ How To Train Your (Compressed) Large Language Model") summarizes all baseline
    model configurations and their respective pretraining token budgets. For pretraining,
    we use the C4 dataset (Raffel et al., [2020](#bib.bib40)) of  160B web-crawled
    tokens. Pretrained models of all sizes are trained for 1 epoch on the C4 dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用语言建模目标，我们训练了两个核心基线模型，分别具有3亿（M）和11亿（B）参数。我们还考虑了包含一半层数的额外基线模型（1.8亿和6.1亿参数）。我们在所有预训练实验中使用Lion优化器（Chen
    et al., [2023](#bib.bib7)），但对偏差和归一化参数省略了$\beta$。我们遵循Composer [https://www.mosaicml.com/composer](https://www.mosaicml.com/composer)
    提出的默认超参数。我们从开始到2000步（40亿标记）进行学习率预热，然后使用余弦衰减计划直到训练停止。最终学习率为峰值值的10%。表[1](#S4.T1
    "Table 1 ‣ 4.1 Base Models ‣ 4 Experimental Setup ‣ How To Train Your (Compressed)
    Large Language Model")总结了所有基线模型配置及其各自的预训练标记预算。在预训练过程中，我们使用C4数据集（Raffel et al.,
    [2020](#bib.bib40)），该数据集包含1600亿网页抓取的标记。所有大小的预训练模型在C4数据集上训练1轮。
- en: 4.2 Compression and Distillation Baselines
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 压缩和蒸馏基线
- en: To simulate the typical practice of a compressed model continuing to train with
    a pretrained optimizer state, we seed the model with training for 1B additional
    tokens to “warm-start” the optimizer. This seed phase includes a linear warm-up
    of the learning rate for 400 steps (800M tokens). After this, we begin layer pruning
    according to the truncation configuration, controlling when and where we prune
    layers from the model. A layer that needs to be removed in the future is frozen
    from the start of the optimization process.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟压缩模型继续使用预训练优化器状态的典型做法，我们使用额外的10亿标记对模型进行训练以“热启动”优化器。这个预热阶段包括学习率的线性预热，共400步（8亿标记）。之后，我们开始根据截断配置进行层剪枝，控制何时以及从模型中剪除哪些层。需要在未来移除的层从优化过程开始时即被冻结。
- en: We use the Lion optimizer with the same batch size, weight decay, and $\beta$.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与相同批量大小、权重衰减和$\beta$的Lion优化器。
- en: For distillation baselines, we use the same truncated initialization strategy
    for student models as our compression method. Each loss component of the total
    distillation loss is normalized such that the total magnitude for distillation
    loss matches the cross-entropy loss. We note that some forms of knowledge distillation
    cannot exploit recent modeling optimizations, such as flash-attention. Aligning
    $QK^{T}$ matrix products, significantly slowing down training.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于蒸馏基线，我们对学生模型使用与压缩方法相同的截断初始化策略。总蒸馏损失的每个损失组件都经过归一化，以确保蒸馏损失的总量与交叉熵损失匹配。我们注意到某些形式的知识蒸馏无法利用最近的建模优化，例如闪存注意力。这会使得$QK^{T}$矩阵乘法的对齐过程显著减慢训练速度。
- en: 4.3 Downstream Evaluation
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 下游评估
- en: '| Category | Task | Metric |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 任务 | 指标 |'
- en: '| Common Sense Reasoning | PIQA (Bisk et al., [2019](#bib.bib4)) | ACC (LN)
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 常识推理 | PIQA (Bisk et al., [2019](#bib.bib4)) | ACC (LN) |'
- en: '| Hellaswag (Zellers et al., [2019](#bib.bib60)) | ACC (LN) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Hellaswag (Zellers et al., [2019](#bib.bib60)) | ACC (LN) |'
- en: '| Winogrande (Sakaguchi et al., [2019](#bib.bib43)) | ACC |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Winogrande (Sakaguchi et al., [2019](#bib.bib43)) | ACC |'
- en: '| BoolQ (Clark et al., [2019](#bib.bib10)) | ACC (LN) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ (Clark et al., [2019](#bib.bib10)) | ACC (LN) |'
- en: '| Science Question Answering | OpenBookQA (Mihaylov et al., [2018](#bib.bib35))
    | ACC (LN) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 科学问题回答 | OpenBookQA (Mihaylov et al., [2018](#bib.bib35)) | ACC (LN) |'
- en: '| SciQ (Welbl et al., [2017](#bib.bib58)) | ACC |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| SciQ (Welbl et al., [2017](#bib.bib58)) | ACC |'
- en: '| Arc-Easy (Clark et al., [2018](#bib.bib11)) | ACC |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Arc-Easy (Clark et al., [2018](#bib.bib11)) | ACC |'
- en: '| Arc-Challenge (Clark et al., [2018](#bib.bib11)) | PMI-DC |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Arc-Challenge (Clark et al., [2018](#bib.bib11)) | PMI-DC |'
- en: '| Natural Language Inference | RTE (Wang et al., [2018](#bib.bib55)) | ACC
    (LN) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言推理 | RTE (Wang et al., [2018](#bib.bib55)) | ACC (LN) |'
- en: '| Commitment Bank (de Marneffe et al., [2019](#bib.bib13)) | ACC |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Commitment Bank (de Marneffe et al., [2019](#bib.bib13)) | ACC |'
- en: '| Causal Reasoning | COPA (Roemmele et al., [2011](#bib.bib41)) | ACC |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 因果推理 | COPA (Roemmele et al., [2011](#bib.bib41)) | ACC |'
- en: '| Paraphrase Identification | MRPC (Dolan and Brockett, [2005](#bib.bib18))
    | F1 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 同义句识别 | MRPC (Dolan and Brockett, [2005](#bib.bib18)) | F1 |'
- en: 'Table 2: Downstream tasks for evaluating our compressed models and baselines.
    Each task reports a different metric: ‘ACC’: accuracy, ‘ACC (LN)’: length normalized
    accuracy, ‘F1’: F1 score, ‘PMI-DC’: domain-conditioned pointwise mutual information.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：评估我们压缩模型和基准模型的下游任务。每个任务报告不同的指标：‘ACC’：准确率，‘ACC (LN)’：长度归一化准确率，‘F1’：F1分数，‘PMI-DC’：领域条件点对点互信息。
- en: 'We evaluate our model on 12 tasks from 5 categories: common sense reasoning,
    science question answering, causal reasoning, natural language inference, and
    paraphrase identification. All tasks are evaluated in a zero-shot setting by providing
    the language model with a prompt from Eleuther-AI evaluation harness (Gao et al.,
    [2021](#bib.bib20)) and a possible completion. We score the model output for each
    completion. The completion with the highest likelihood is the prediction to compute
    task accuracy. The completion likelihood can be normalized by either the character
    count in the completion (Gao et al., [2021](#bib.bib20), length normalized accuracy)
    or by the probability of completion conditioned on the domain premise (Holtzman
    et al., [2021](#bib.bib22); Brown et al., [2020](#bib.bib5), domain conditional
    point-wise mutual information). In Table [2](#S4.T2 "Table 2 ‣ 4.3 Downstream
    Evaluation ‣ 4 Experimental Setup ‣ How To Train Your (Compressed) Large Language
    Model"), we list tasks in each evaluation category and respective metrics.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在来自5个类别的12个任务上评估了我们的模型：常识推理、科学问答、因果推理、自然语言推断和同义句识别。所有任务都是在零样本设置下评估的，通过向语言模型提供Eleuther-AI评估工具中的提示（Gao
    et al., [2021](#bib.bib20)）和可能的完成选项。我们对每个完成选项的模型输出进行评分。具有最高概率的完成选项是用于计算任务准确性的预测。完成选项的概率可以通过完成中的字符数（Gao
    et al., [2021](#bib.bib20)，长度归一化准确率）或基于领域前提的完成条件概率（Holtzman et al., [2021](#bib.bib22);
    Brown et al., [2020](#bib.bib5)，领域条件点对点互信息）来归一化。在表格[2](#S4.T2 "表2 ‣ 4.3 下游评估 ‣
    4 实验设置 ‣ 如何训练你的（压缩的）大型语言模型")中，我们列出了每个评估类别中的任务和相应的指标。
- en: '| Model | Method | PIQA | Hellaswag | Winogrande | BoolQ | OBQA | SciQ | Arc-e
    | Arc-c | COPA | RTE | CB | MRPC | Avg $\left(\uparrow\right)$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | PIQA | Hellaswag | Winogrande | BoolQ | OBQA | SciQ | Arc-e | Arc-c
    | COPA | RTE | CB | MRPC | 平均值 $\left(\uparrow\right)$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
- en: '|  | Pre-compression | 71.0 | 45.8 | 52.3 | 61.1 | 32.0 | 81.7 | 52.6 | 23.4
    | 73.0 | 53.8 | 41.1 | 81.2 | 55.8 | 16.2 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | 预压缩 | 71.0 | 45.8 | 52.3 | 61.1 | 32.0 | 81.7 | 52.6 | 23.4 | 73.0 | 53.8
    | 41.1 | 81.2 | 55.8 | 16.2 |'
- en: '|  | Vanilla-KD | 66.6 | 34.9 | 49.7 | 62.1 | 30.6 | 76.4 | 47.2 | 25.8 | 69.0
    | 55.6 | 39.3 | 80.5 | 53.1 | 22.8 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | 原始知识蒸馏 | 66.6 | 34.9 | 49.7 | 62.1 | 30.6 | 76.4 | 47.2 | 25.8 | 69.0
    | 55.6 | 39.3 | 80.5 | 53.1 | 22.8 |'
- en: '|  | miniLM-KD | 66.1 | 34.6 | 49.8 | 62.2 | 29.4 | 75.2 | 45.6 | 22.7 | 69.0
    | 56.7 | 39.3 | 81.0 | 52.6 | 23.4 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | miniLM-KD | 66.1 | 34.6 | 49.8 | 62.2 | 29.4 | 75.2 | 45.6 | 22.7 | 69.0
    | 56.7 | 39.3 | 81.0 | 52.6 | 23.4 |'
- en: '|  | Homotopic-KD | 63.6 | 32.4 | 50.7 | 62.2 | 28.4 | 74.4 | 42.8 | 27.4 |
    68.0 | 57.4 | 41.1 | 81.0 | 52.4 | 27.2 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | 同源知识蒸馏 | 63.6 | 32.4 | 50.7 | 62.2 | 28.4 | 74.4 | 42.8 | 27.4 | 68.0
    | 57.4 | 41.1 | 81.0 | 52.4 | 27.2 |'
- en: '| 300M-160B | Ours | 66.8 | 35.1 | 49.8 | 62.0 | 29.2 | 76.7 | 47.9 | 27.1
    | 68.0 | 56.0 | 39.3 | 80.3 | 53.2 | 23.0 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 300M-160B | 我们的方法 | 66.8 | 35.1 | 49.8 | 62.0 | 29.2 | 76.7 | 47.9 | 27.1
    | 68.0 | 56.0 | 39.3 | 80.3 | 53.2 | 23.0 |'
- en: '|  | Pre-compression | 74.4 | 56.8 | 55.2 | 62.5 | 34.4 | 85.1 | 57.0 | 29.1
    | 77.0 | 54.9 | 50.0 | 81.1 | 59.8 | 13.0 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | 预压缩 | 74.4 | 56.8 | 55.2 | 62.5 | 34.4 | 85.1 | 57.0 | 29.1 | 77.0 | 54.9
    | 50.0 | 81.1 | 59.8 | 13.0 |'
- en: '|  | Vanilla-KD | 70.6 | 44.4 | 50.9 | 62.2 | 30.0 | 81.7 | 53.3 | 27.1 | 70.0
    | 53.4 | 41.1 | 80.8 | 55.5 | 17.3 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | 原始知识蒸馏 | 70.6 | 44.4 | 50.9 | 62.2 | 30.0 | 81.7 | 53.3 | 27.1 | 70.0
    | 53.4 | 41.1 | 80.8 | 55.5 | 17.3 |'
- en: '|  | miniLM-KD | 70.6 | 43.7 | 52.4 | 62.2 | 28.2 | 81.0 | 50.9 | 26.1 | 70.0
    | 54.2 | 41.1 | 81.3 | 55.1 | 17.5 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | miniLM-KD | 70.6 | 43.7 | 52.4 | 62.2 | 28.2 | 81.0 | 50.9 | 26.1 | 70.0
    | 54.2 | 41.1 | 81.3 | 55.1 | 17.5 |'
- en: '|  | Homotopic-KD | 68.7 | 40.9 | 51.6 | 62.0 | 29.8 | 79.4 | 49.3 | 23.1 |
    69.0 | 54.9 | 37.5 | 81.2 | 53.9 | 19.8 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | 同源知识蒸馏 | 68.7 | 40.9 | 51.6 | 62.0 | 29.8 | 79.4 | 49.3 | 23.1 | 69.0
    | 54.9 | 37.5 | 81.2 | 53.9 | 19.8 |'
- en: '| 1.1B-160B | Ours | 70.5 | 44.3 | 51.6 | 62.2 | 29.4 | 80.9 | 51.4 | 25.1
    | 71.0 | 52.7 | 41.1 | 80.9 | 55.1 | 17.3 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 1.1B-160B | 我们的方法 | 70.5 | 44.3 | 51.6 | 62.2 | 29.4 | 80.9 | 51.4 | 25.1
    | 71.0 | 52.7 | 41.1 | 80.9 | 55.1 | 17.3 |'
- en: 'Table 3: Results for our model and distillation baselines from Sec. [3.2](#S3.SS2
    "3.2 Distillation Objectives ‣ 3 Training (Compressed) LLMs ‣ How To Train Your
    (Compressed) Large Language Model") for language model perplexity and all tasks
    outlined in Table [2](#S4.T2 "Table 2 ‣ 4.3 Downstream Evaluation ‣ 4 Experimental
    Setup ‣ How To Train Your (Compressed) Large Language Model"). All models are
    evaluated zero-shot with no finetuning. For extrinsic tasks, higher is better,
    while lower is better for perplexity (ppl.). “Pre-compression” is the larger teacher
    model evaluated on the same task suite. Our method yields superior perplexity
    compared to distillation and is competitive on most tasks.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 3: 我们的模型和蒸馏基准在第 [3.2](#S3.SS2 "3.2 蒸馏目标 ‣ 3 训练（压缩的）语言模型 ‣ 如何训练你的（压缩的）大型语言模型")
    节的结果，涉及语言模型的困惑度和表格 [2](#S4.T2 "表格 2 ‣ 4.3 下游评估 ‣ 4 实验设置 ‣ 如何训练你的（压缩的）大型语言模型")
    中列出的所有任务。所有模型在没有微调的情况下进行零样本评估。对于外部任务，得分越高越好，而对于困惑度（ppl.），得分越低越好。“预压缩”是评估在相同任务套件上的更大教师模型。我们的方法在困惑度上优于蒸馏方法，并且在大多数任务上具有竞争力。'
- en: 5 Compression Results
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 压缩结果
- en: 5.1 Compression for generality and zero-shot promptability
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 一般性压缩与零样本提示能力
- en: 'This section compares our method against the distillation baselines mentioned
    in Section [3.2](#S3.SS2 "3.2 Distillation Objectives ‣ 3 Training (Compressed)
    LLMs ‣ How To Train Your (Compressed) Large Language Model"). We show perplexity
    results on C4 validation set and zero-shot performance on 12 end-tasks. All baselines
    and our method are applied to two core model baselines mentioned in Section [4.1](#S4.SS1
    "4.1 Base Models ‣ 4 Experimental Setup ‣ How To Train Your (Compressed) Large
    Language Model"): a 300M parameter model and a 1.1B parameter model, both trained
    until 160B tokens. In addition, we provide the end-task performance of the checkpoints
    themselves for comparison against distillation baselines and our method. All compression
    methods in this section remove 12/24 layers in the decoder-only base models. All
    methods are trained on additional 20B tokens from the C4 corpus. As shown in Table [3](#S4.T3
    "Table 3 ‣ 4.3 Downstream Evaluation ‣ 4 Experimental Setup ‣ How To Train Your
    (Compressed) Large Language Model"), our method matches the best distillation
    baseline on perplexity and remains competitive on most end-tasks.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将我们的方法与第 [3.2](#S3.SS2 "3.2 蒸馏目标 ‣ 3 训练（压缩的）语言模型 ‣ 如何训练你的（压缩的）大型语言模型") 节中提到的蒸馏基准进行比较。我们展示了
    C4 验证集上的困惑度结果和 12 个最终任务上的零样本性能。所有基准和我们的方法都应用于第 [4.1](#S4.SS1 "4.1 基础模型 ‣ 4 实验设置
    ‣ 如何训练你的（压缩的）大型语言模型") 节中提到的两个核心模型基准：一个 300M 参数模型和一个 1.1B 参数模型，两者都训练到 160B tokens。此外，我们提供了检查点本身的最终任务性能，以便与蒸馏基准和我们的方法进行比较。本节中的所有压缩方法在仅解码基准模型中移除了
    12/24 层。所有方法都在 C4 语料库的额外 20B tokens 上进行训练。如表格 [3](#S4.T3 "表格 3 ‣ 4.3 下游评估 ‣ 4
    实验设置 ‣ 如何训练你的（压缩的）大型语言模型") 所示，我们的方法在困惑度上与最佳蒸馏基准持平，并在大多数最终任务上保持竞争力。
- en: Table [4](#S5.T4 "Table 4 ‣ 5.1 Compression for generality and zero-shot promptability
    ‣ 5 Compression Results ‣ How To Train Your (Compressed) Large Language Model")
    summarizes training statistics like computation required by each method in terms
    of FLOPs or the wall-clock time consumed. This result shows that our method is
    at par with the distillation baselines in our implementation with significantly
    less compute and wall-clock time. Section [7](#S7 "7 Discussion ‣ How To Train
    Your (Compressed) Large Language Model") discusses our hypothesis on why we expect
    our distillation-free method to work well despite being trained on a less-informative
    training signal without a teacher.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [4](#S5.T4 "表格 4 ‣ 5.1 一般性压缩与零样本提示能力 ‣ 5 压缩结果 ‣ 如何训练你的（压缩的）大型语言模型") 总结了每种方法在
    FLOPs 或实际计算时间方面所需的训练统计数据。这个结果显示我们的方法在实现中与蒸馏基准相当，但计算和实际时间显著减少。第 [7](#S7 "7 讨论 ‣
    如何训练你的（压缩的）大型语言模型") 节讨论了我们对为什么期望我们的无蒸馏方法能有效的假设，尽管它在没有教师的情况下接受了信息量较少的训练信号。
- en: '| Model | Method | FLOPs | FLOPs Ratio | Wall-clock Time | Wall-clock Ratio
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | FLOPs | FLOPs 比率 | 实际时间 | 实际时间比率 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 300M-160B | Vanilla-KD | 33.31e18 | 1.6x | 47 hrs | 1.8x |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 300M-160B | Vanilla-KD | 33.31e18 | 1.6x | 47 小时 | 1.8x |'
- en: '| miniLM-KD | 33.31e18 | 1.6x | 72 hrs | 2.7x |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| miniLM-KD | 33.31e18 | 1.6x | 72 小时 | 2.7x |'
- en: '| Homotopic-KD | 33.31e18 | 1.6x | 204 hrs | 7.7x |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Homotopic-KD | 33.31e18 | 1.6x | 204 小时 | 7.7x |'
- en: '| Ours | 21.22e18 | 1x | 26.5 hrs | 1x |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | 21.22e18 | 1x | 26.5 小时 | 1x |'
- en: '| 1.1B-160B | Vanilla-KD | 116.76e18 | 1.6x | 51 hrs | 1.6x |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 1.1B-160B | Vanilla-KD | 116.76e18 | 1.6x | 51 hrs | 1.6x |'
- en: '| miniLM-KD | 116.76e18 | 1.6x | 63 hrs | 1.9x |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| miniLM-KD | 116.76e18 | 1.6x | 63 hrs | 1.9x |'
- en: '| Homotopic-KD | 116.76e18 | 1.6x | 300 hrs | 9x |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Homotopic-KD | 116.76e18 | 1.6x | 300 hrs | 9x |'
- en: '| Ours | 72.52e18 | 1x | 33 hrs | 1x |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 我们 | 72.52e18 | 1x | 33 hrs | 1x |'
- en: 'Table 4: Training statistics for each model and baseline. The 300M models train
    using 4xA100 80GB GPUs and the 1.1B models use 8xA100 80GB GPUs.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：每个模型和基准的训练统计数据。300M模型使用4xA100 80GB GPU进行训练，而1.1B模型使用8xA100 80GB GPU。
- en: 5.2 Where to Prune a Model?
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 剪枝模型的位置？
- en: Our baseline models contain 24 decoder layers. To determine which layers we
    should prune for the best compression performance, we define five layer pruning
    configurations shown in Figure [1](#S3.F1 "Figure 1 ‣ 3.3 Truncated Initialization
    Strategy ‣ 3 Training (Compressed) LLMs ‣ How To Train Your (Compressed) Large
    Language Model"), each removing 12 out of the 24 decoder layers of the 300M and
    1.1B models. In all these pruning configurations, we always keep the first and
    the last layers because they interact with the embedding table. We made this design
    choice based on early experiments. Table [5](#S5.T5 "Table 5 ‣ 5.2 Where to Prune
    a Model? ‣ 5 Compression Results ‣ How To Train Your (Compressed) Large Language
    Model") summarizes the results of this ablation. We report the perplexity score
    on the C4 validation set and the average task accuracy across 12 tasks in our
    evaluation suite (Table [2](#S4.T2 "Table 2 ‣ 4.3 Downstream Evaluation ‣ 4 Experimental
    Setup ‣ How To Train Your (Compressed) Large Language Model")).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基准模型包含24个解码器层。为了确定剪枝哪个层以获得最佳的压缩性能，我们定义了图[1](#S3.F1 "图 1 ‣ 3.3 截断初始化策略 ‣ 3
    训练（压缩）LLM ‣ 如何训练你的（压缩）大型语言模型")所示的五种层剪枝配置，每种配置移除300M和1.1B模型的24个解码器层中的12个。在所有这些剪枝配置中，我们始终保留第一层和最后一层，因为它们与嵌入表进行交互。我们根据早期实验做出了这个设计选择。表[5](#S5.T5
    "表 5 ‣ 5.2 剪枝模型的位置？ ‣ 5 压缩结果 ‣ 如何训练你的（压缩）大型语言模型")总结了这次消融实验的结果。我们报告了C4验证集上的困惑度得分以及我们评估套件中12个任务的平均任务准确率（表
    [2](#S4.T2 "表 2 ‣ 4.3 下游评估 ‣ 4 实验设置 ‣ 如何训练你的（压缩）大型语言模型")）。
- en: For the base 300M model, pruning configurations of *max-gap* and *both* perform
    the best out of the five possible configurations. For the 1.1B model, pruning
    layers from the *input* configuration yielded the best results for both reported
    metrics. The *output* pruning configuration resulted in the worst performance
    across model sizes, suggesting that pruning layers towards the output side of
    the model should be avoided. Given these results, we use the pruning configuration
    of *max-gap* for all our 300M model experiments and the configuration of *input*
    for all our 1.1B model experiments.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基础300M模型，*max-gap*和*both*的剪枝配置在五种可能的配置中表现最佳。对于1.1B模型，*input*配置的剪枝层在报告的两个指标中均取得了最佳结果。*output*剪枝配置在所有模型尺寸中表现最差，这表明应避免在模型输出侧进行剪枝。根据这些结果，我们对所有300M模型实验使用*max-gap*剪枝配置，对所有1.1B模型实验使用*input*配置。
- en: '| Model | Token Budget | Task Metric | max-gap | input | output | middle |
    both | Pre-compression |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 令牌预算 | 任务指标 | max-gap | input | output | middle | both | 预压缩 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 300M-160B | 20B | ppl $\left(\downarrow\right)$ | 23.0 | 24.5 | 25.6 | 24.0
    | 23.0 | 16.2 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 300M-160B | 20B | ppl $\left(\downarrow\right)$ | 23.0 | 24.5 | 25.6 | 24.0
    | 23.0 | 16.2 |'
- en: '| 300M-160B | 20B | avg acc $\left(\uparrow\right)$ | 53.2 | 52.9 | 51.6 |
    52.9 | 53.2 | 55.8 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 300M-160B | 20B | 平均准确率 $\left(\uparrow\right)$ | 53.2 | 52.9 | 51.6 | 52.9
    | 53.2 | 55.8 |'
- en: '| 1.1B-160B | 20B | ppl $\left(\downarrow\right)$ | 18.1 | 17.3 | 22.0 | 18.6
    | 18.6 | 13.0 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 1.1B-160B | 20B | ppl $\left(\downarrow\right)$ | 18.1 | 17.3 | 22.0 | 18.6
    | 18.6 | 13.0 |'
- en: '| 1.1B-160B | 20B | avg acc $\left(\uparrow\right)$ | 54.8 | 55.1 | 53.1 |
    54.7 | 53.8 | 59.8 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 1.1B-160B | 20B | 平均准确率 $\left(\uparrow\right)$ | 54.8 | 55.1 | 53.1 | 54.7
    | 53.8 | 59.8 |'
- en: 'Table 5: Influence on average task performance and C4 validation perplexity
    of different truncated initialization strategies from Figure [1](#S3.F1 "Figure
    1 ‣ 3.3 Truncated Initialization Strategy ‣ 3 Training (Compressed) LLMs ‣ How
    To Train Your (Compressed) Large Language Model") for models of size 300M and
    1.1B. The average task performance score is across 12 tasks listed in Table [2](#S4.T2
    "Table 2 ‣ 4.3 Downstream Evaluation ‣ 4 Experimental Setup ‣ How To Train Your
    (Compressed) Large Language Model"), and higher numbers are better. For perplexity
    scores, lower is better.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：图[1](#S3.F1 "图 1 ‣ 3.3 截断初始化策略 ‣ 3 训练（压缩的）LLMs ‣ 如何训练您的（压缩的）大型语言模型")中不同截断初始化策略对`300M`和`1.1B`尺寸模型的平均任务性能和C4验证困惑度的影响。平均任务性能分数基于表[2](#S4.T2
    "表 2 ‣ 4.3 下游评估 ‣ 4 实验设置 ‣ 如何训练您的（压缩的）大型语言模型")中列出的`12`个任务，高分更好。对于困惑度分数，低分更好。
- en: 5.3 When to Prune a Model?
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 何时剪枝模型？
- en: 'To answer the question of when we should prune layers from our model while
    continuing to train it with language modeling loss, we can decide in one of two
    ways: either remove selected layers at once or remove them one by one, each after
    a fixed number of training tokens. We run this experiment in four configurations
    to see if increasing the gap between each layer pruning increases training stability
    or model performance. The four configurations are: dropping all layers at once
    (0M token gap between pruning each layer) or pruning them after 100M, 500M, and
    1B training tokens each. We run this experiment for the 300M and 1.1B model sizes.
    We prune 12/24 layers from our decoder-only models for this ablation, each at
    a training token gap mentioned in one of the four configurations above. The result
    of this experiment is summarized in Figure [2](#S5.F2 "Figure 2 ‣ 5.4 Upweighting
    Distillation Loss ‣ 5 Compression Results ‣ How To Train Your (Compressed) Large
    Language Model"). Pruning layers one by one with an increasing token budget between
    each layer pruning does not benefit the average task accuracy or C4 validation
    perplexity. In fact, there is a marginal preference to prune layers as early into
    the training as possible. Hence, we decided to prune all 12/24 layers simultaneously
    for other experiments.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答我们何时应该在继续用语言建模损失训练模型的过程中剪枝层的问题，我们可以通过两种方式之一来决定：要么一次性移除选定的层，要么在每个固定数量的训练标记之后逐一移除。我们在四种配置中运行此实验，以查看增加每次层剪枝之间的间隔是否提高了训练稳定性或模型性能。这四种配置是：一次性丢弃所有层（每次剪枝层之间的`0M
    token`间隔）或在每`100M`、`500M`和`1B`训练标记后进行剪枝。我们对`300M`和`1.1B`模型尺寸进行此实验。我们在此消融实验中从仅解码器模型中剪枝`12/24`层，每层的训练标记间隔按照上述四种配置之一进行。该实验的结果总结在图[2](#S5.F2
    "图 2 ‣ 5.4 上重权蒸馏损失 ‣ 5 压缩结果 ‣ 如何训练您的（压缩的）大型语言模型")中。逐层剪枝且每次剪枝之间的标记预算逐渐增加，并没有对平均任务准确率或C4验证困惑度产生好处。实际上，剪枝层的最佳时机是尽可能早地进入训练阶段。因此，我们决定在其他实验中同时剪枝所有`12/24`层。
- en: 5.4 Upweighting Distillation Loss
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 上重权蒸馏损失
- en: '![Refer to caption](img/b3bdb3c47c5d35f8ed040e02dd0743fd.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b3bdb3c47c5d35f8ed040e02dd0743fd.png)'
- en: 'Figure 2: Average task accuracy across 12 tasks (Table [2](#S4.T2 "Table 2
    ‣ 4.3 Downstream Evaluation ‣ 4 Experimental Setup ‣ How To Train Your (Compressed)
    Large Language Model")) and perplexity on the C4 validation set for model sizes
    300M and 1.1B comparing schedules for *when* to prune layers during continued
    pretraining. We find a marginal performance degradation as we remove layers one
    by one further apart during continued pretraining.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：模型尺寸`300M`和`1.1B`在继续预训练过程中逐层剪枝的*时机*的平均任务准确率（表[2](#S4.T2 "表 2 ‣ 4.3 下游评估 ‣
    4 实验设置 ‣ 如何训练您的（压缩的）大型语言模型")）和C4验证集上的困惑度。我们发现，当我们逐层剪枝且每次剪枝之间的间隔逐渐增大时，性能略有下降。
- en: Our vanilla distillation baseline described in Section [3.2](#S3.SS2 "3.2 Distillation
    Objectives ‣ 3 Training (Compressed) LLMs ‣ How To Train Your (Compressed) Large
    Language Model") performs at par with our proposed approach of model pruning,
    followed by continued pretraining. Any distillation setup adds a teacher and a
    distillation loss component at the minimum to the compression pipeline. Because
    of this, the vanilla distillation baseline is at least 1.6x slower than our proposed
    method in terms of theoretical FLOPs and wall-clock time (Table [4](#S5.T4 "Table
    4 ‣ 5.1 Compression for generality and zero-shot promptability ‣ 5 Compression
    Results ‣ How To Train Your (Compressed) Large Language Model")). To justify the
    additional compute requirements of a distillation setup, we want to verify further
    the importance of the teacher and the distillation loss component.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第[3.2](#S3.SS2 "3.2 Distillation Objectives ‣ 3 Training (Compressed) LLMs
    ‣ How To Train Your (Compressed) Large Language Model")节中描述的基础蒸馏模型与我们提出的模型修剪方法相当，后者接着进行持续预训练。任何蒸馏设置至少在压缩管道中添加一个教师和一个蒸馏损失组件。因此，基础蒸馏模型在理论FLOPs和实际时间上至少比我们提出的方法慢1.6倍（表[4](#S5.T4
    "Table 4 ‣ 5.1 Compression for generality and zero-shot promptability ‣ 5 Compression
    Results ‣ How To Train Your (Compressed) Large Language Model")）。为了证明蒸馏设置的额外计算需求，我们希望进一步验证教师和蒸馏损失组件的重要性。
- en: We maintain the setup from our vanilla distillation baseline for this experiment
    and append a coefficient to the KL-divergence loss term. The new loss function
    for distillation becomes $\mathcal{L}_{\Sigma}=\mathcal{L}_{\rm LM}+\lambda*\mathcal{L}_{\rm
    distill}$ results in a decrease in the end-task performance and an increase in
    C4 perplexity. Thus, this experiment highlights the importance of the language
    modeling component in our vanilla distillation baseline and strengthens the argument
    that appending the continued pretraining setup with a distillation component to
    get similar performance is redundant.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此实验中保持了来自我们基础蒸馏模型的设置，并在KL散度损失项中添加了一个系数。新的蒸馏损失函数变为$\mathcal{L}_{\Sigma}=\mathcal{L}_{\rm
    LM}+\lambda*\mathcal{L}_{\rm distill}$，这导致了最终任务性能的下降和C4困惑度的增加。因此，此实验突显了语言建模组件在我们基础蒸馏模型中的重要性，并加强了这样的论点：将持续预训练设置与蒸馏组件结合以获得类似性能是多余的。
- en: '![Refer to caption](img/76ed223463df82f3c221702403ee60ed.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/76ed223463df82f3c221702403ee60ed.png)'
- en: 'Figure 3: We ablate different coefficients on the KL-divergence distillation
    loss component of the vanilla distillation baseline (Sec. [3.2](#S3.SS2 "3.2 Distillation
    Objectives ‣ 3 Training (Compressed) LLMs ‣ How To Train Your (Compressed) Large
    Language Model")) for model size 1.1B. We report the average task score across
    12 tasks in our evaluation suite (Table [2](#S4.T2 "Table 2 ‣ 4.3 Downstream Evaluation
    ‣ 4 Experimental Setup ‣ How To Train Your (Compressed) Large Language Model"))
    and C4 validation perplexity.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：我们在基础蒸馏模型的KL散度蒸馏损失组件上进行了不同系数的消融实验（第[3.2](#S3.SS2 "3.2 Distillation Objectives
    ‣ 3 Training (Compressed) LLMs ‣ How To Train Your (Compressed) Large Language
    Model")节），模型大小为1.1B。我们报告了在我们的评估套件中跨12个任务的平均任务得分（表[2](#S4.T2 "Table 2 ‣ 4.3 Downstream
    Evaluation ‣ 4 Experimental Setup ‣ How To Train Your (Compressed) Large Language
    Model")）以及C4验证困惑度。
- en: 5.5 Removing Language Modeling
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 移除语言建模
- en: Our other two distillation baselines described in Section [3.2](#S3.SS2 "3.2
    Distillation Objectives ‣ 3 Training (Compressed) LLMs ‣ How To Train Your (Compressed)
    Large Language Model") constrain the teacher-teacher setup in more ways than just
    using a KL-divergence term for distillation. The miniLM-KD (Wang et al., [2020b](#bib.bib57))
    baseline adds constraints on the attention map and intermediate representations
    of the final layer between the teacher and the student model on top of the loss
    terms in the vanilla distillation setup. The homotopic-KD (Liang et al., [2023](#bib.bib31))
    baseline takes a step further and adds constraints on each layer between the teacher
    and the student on attention maps and intermediate representations. These setups
    are well designed to align student outputs to the teacher perfectly; thus, we
    ablate on whether they even require the language modeling component for the student.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第[3.2节](#S3.SS2 "3.2 Distillation Objectives ‣ 3 Training (Compressed) LLMs
    ‣ How To Train Your (Compressed) Large Language Model")中描述的其他两种蒸馏基线除了使用KL散度项外，还通过更多方式约束教师-教师设置。miniLM-KD（Wang
    et al., [2020b](#bib.bib57)）基线在教师和学生模型的最终层之间的注意力图和中间表示上添加了约束，除此之外还有原始蒸馏设置中的损失项。同态-KD（Liang
    et al., [2023](#bib.bib31)）基线更进一步，在教师和学生之间的每一层上对注意力图和中间表示添加约束。这些设置设计得很好，以使学生输出与教师完美对齐；因此，我们在是否需要语言建模组件对学生的影响进行了消融实验。
- en: In this experiment, we maintain the same setup of our miniLM and homotopic distillation
    baselines without the language modeling loss term. We show the results in Figure
    [4](#S5.F4 "Figure 4 ‣ 5.5 Removing Language Modeling ‣ 5 Compression Results
    ‣ How To Train Your (Compressed) Large Language Model") for our 1.1B model. Both
    distillation baselines lose some end-task performance by removing the language
    modeling loss term. However, the language modeling task performance on the C4
    validation set suffers significantly without the language modeling loss term on
    the student.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们保持miniLM和同态蒸馏基线的相同设置，但不使用语言建模损失项。我们在图[4](#S5.F4 "Figure 4 ‣ 5.5 Removing
    Language Modeling ‣ 5 Compression Results ‣ How To Train Your (Compressed) Large
    Language Model")中展示了1.1B模型的结果。移除语言建模损失项后，两种蒸馏基线在最终任务性能上都有所下降。然而，C4验证集上的语言建模任务性能在没有语言建模损失项的情况下显著下降。
- en: '![Refer to caption](img/2304e6095118419a5cf807d69c792b1d.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2304e6095118419a5cf807d69c792b1d.png)'
- en: 'Figure 4: Experiment demonstrating the effectiveness of language modeling loss
    component in compound loss functions of miniLM-KD (Sec. [3.2](#S3.SS2 "3.2 Distillation
    Objectives ‣ 3 Training (Compressed) LLMs ‣ How To Train Your (Compressed) Large
    Language Model")) and homotopic-KD distillation (Sec. [3.2](#S3.SS2 "3.2 Distillation
    Objectives ‣ 3 Training (Compressed) LLMs ‣ How To Train Your (Compressed) Large
    Language Model")) baselines. We report the average task performance across 12
    tasks (Table [2](#S4.T2 "Table 2 ‣ 4.3 Downstream Evaluation ‣ 4 Experimental
    Setup ‣ How To Train Your (Compressed) Large Language Model")) and perplexity
    on the C4 validation set.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：实验展示了语言建模损失成分在miniLM-KD（第[3.2节](#S3.SS2 "3.2 Distillation Objectives ‣ 3
    Training (Compressed) LLMs ‣ How To Train Your (Compressed) Large Language Model")）和同态-KD蒸馏（第[3.2节](#S3.SS2
    "3.2 Distillation Objectives ‣ 3 Training (Compressed) LLMs ‣ How To Train Your
    (Compressed) Large Language Model")）基线中的效果。我们报告了在12项任务（表[2](#S4.T2 "Table 2 ‣
    4.3 Downstream Evaluation ‣ 4 Experimental Setup ‣ How To Train Your (Compressed)
    Large Language Model")）上的平均任务性能和C4验证集上的困惑度。
- en: 6 Train Large then Compress, or Not
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 先训练大模型然后压缩，还是不压缩
- en: '![Refer to caption](img/054f309d4ff1cc30de3641e4905a19c0.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/054f309d4ff1cc30de3641e4905a19c0.png)'
- en: 'Figure 5: Comparing compressed models against pretraining an equivalent model
    from scratch with the training budget of only continued pretraining (20B tokens)
    or the full pretraining stage (max budget).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：将压缩模型与从头开始预训练一个等效模型进行比较，训练预算仅包括继续预训练（20B tokens）或完整预训练阶段（最大预算）。
- en: Existing literature Li et al. ([2020](#bib.bib30)) suggests that training a
    large model and then compressing it usually works better than training a smaller
    model from scratch. In this section, we study this observation in the context
    of zero-shot evaluation of the compressed model. As mentioned in Section [4.1](#S4.SS1
    "4.1 Base Models ‣ 4 Experimental Setup ‣ How To Train Your (Compressed) Large
    Language Model"), in addition to our 300M/1.1B baselines, we train two half-sized
    models (180M/610M) from scratch on the same pretraining token budget as their
    larger counterparts.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现有文献 Li 等人（[2020](#bib.bib30)）建议，训练一个大模型然后进行压缩通常比从头开始训练一个较小的模型效果更好。在这一部分，我们在压缩模型的零样本评估背景下研究了这一观察结果。如第[4.1](#S4.SS1
    "4.1 基础模型 ‣ 4 实验设置 ‣ 如何训练你的（压缩的）大型语言模型")节中提到，除了我们的 300M/1.1B 基线外，我们还在与其更大模型相同的预训练令牌预算下，从头开始训练了两个半尺寸模型（180M/610M）。
- en: Figure [5](#S6.F5 "Figure 5 ‣ 6 Train Large then Compress, or Not ‣ How To Train
    Your (Compressed) Large Language Model") compares the performance of our source
    LLM models (“full @ max-budget”), half-sized pretrained models to the same token
    budget (“half @ max-budget”), compressed models from the source LLM with a token
    budget of 20B (“compressed @ 20B”) and half-sized pretrained models with the token
    budget of our compression method (“half @ 20B”). Our compressed models outperform
    the half-sized models pretrained from scratch when the token budget is fixed at
    the level of the compression budget. As we further pretrain our half-sized models,
    they overtake the comparable compressed model in both zero-shot task evaluation
    and language modeling perplexity.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5](#S6.F5 "图 5 ‣ 6 先训练大模型再压缩，或不压缩 ‣ 如何训练你的（压缩的）大型语言模型") 比较了我们的源 LLM 模型（“full
    @ max-budget”）、半尺寸预训练模型与相同的令牌预算（“half @ max-budget”）、从源 LLM 压缩得到的模型，令牌预算为 20B（“compressed
    @ 20B”），以及半尺寸预训练模型与我们压缩方法的令牌预算（“half @ 20B”）。在令牌预算固定为压缩预算水平时，我们的压缩模型的表现优于从头开始预训练的半尺寸模型。随着我们进一步预训练我们的半尺寸模型，它们在零样本任务评估和语言建模困惑度上都超越了可比的压缩模型。
- en: Figure [6](#S6.F6 "Figure 6 ‣ 6 Train Large then Compress, or Not ‣ How To Train
    Your (Compressed) Large Language Model") shows how a billion-sized compressed
    model starts to flatten and is overtaken by a pretrained half-sized model. This
    suggests that while continued pretraining is a good way to recover some of the
    model’s zero-shot abilities, truncated initialization due to layer pruning is
    not very effective at preserving these abilities in the first place.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图[6](#S6.F6 "图 6 ‣ 6 先训练大模型再压缩，或不压缩 ‣ 如何训练你的（压缩的）大型语言模型") 显示了一个十亿级压缩模型如何开始平坦化并被一个预训练的半尺寸模型超越。这表明，尽管持续预训练是恢复模型一些零样本能力的好方法，但由于层修剪导致的截断初始化在最初并不十分有效地保持这些能力。
- en: 'The ideal truncated initialization method should satisfy two requirements:
    (1) loss after truncation is the same (or as close as possible) to the loss before
    truncation, (2) as we continue training the truncated model, its learning curve
    continues improving at the same rate as if it was trained from scratch. Using
    the terminogloy from Shen et al. ([2022](#bib.bib47)), it should be “loss-preserving”
    and “training-dynamics-preserving”. Although achieving both requirements might
    not be possible, they define the upper bound. We continue to explore the space
    of pruning strategies for future work.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的截断初始化方法应满足两个要求：（1）截断后的损失与截断前的损失相同（或尽可能接近），（2）随着我们继续训练截断模型，其学习曲线继续以与从头训练模型相同的速度改进。使用
    Shen 等人（[2022](#bib.bib47)）的术语，它应为“保持损失”和“保持训练动态”。虽然同时满足这两个要求可能不太可能，但它们定义了上限。我们将继续探索未来工作的修剪策略空间。
- en: '![Refer to caption](img/e0c7dc7316acb4d6ecb8ea552317671e.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e0c7dc7316acb4d6ecb8ea552317671e.png)'
- en: 'Figure 6: Evaluation perplexity curves for our compressed model trained for
    20B tokens and the “oracle” smaller model trained for the maximum token budget.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：我们的压缩模型在 20B 令牌上训练的评估困惑度曲线，以及训练到最大令牌预算的“神谕”较小模型的困惑度曲线。
- en: 7 Discussion
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论
- en: The results in sections [5](#S5 "5 Compression Results ‣ How To Train Your (Compressed)
    Large Language Model") and [6](#S6 "6 Train Large then Compress, or Not ‣ How
    To Train Your (Compressed) Large Language Model") showed that existing methods
    for task-specialized compression are not necessarily the best in the zero-shot
    evaluation setting. We found that simple continued pretraining outperforms student/teacher
    distillation and that training a smaller model from scratch is better than training
    a larger model and then compressing. This section summarizes a few hypotheses
    explaining these findings and discusses why zero-shot evaluation favors different
    compression methods.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 第[5](#S5 "5 压缩结果 ‣ 如何训练你的（压缩的）大型语言模型")和[6](#S6 "6 先训练大模型再压缩，还是不压缩 ‣ 如何训练你的（压缩的）大型语言模型")节的结果显示，现有的任务专用压缩方法在零样本评估设置中不一定是最佳的。我们发现，简单的持续预训练优于学生/教师蒸馏，而从头训练一个较小的模型比先训练一个较大的模型然后压缩更好。本节总结了几个解释这些发现的假设，并讨论了为何零样本评估偏爱不同的压缩方法。
- en: Non-ideal truncated initialization
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非理想的截断初始化
- en: To preserve a model’s zero-shot promptability, we need a truncated initialization
    method that’s both loss-preserving and training-dynamics preserving Shen et al.
    ([2022](#bib.bib47)). However, the results in section [6](#S6 "6 Train Large then
    Compress, or Not ‣ How To Train Your (Compressed) Large Language Model") show
    that the truncated initialization methods cause the compressed model to lose too
    much of its zero-shot abilities.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持模型的零样本提示能力，我们需要一种既能保留损失又能保留训练动态的截断初始化方法 Shen 等人（[2022](#bib.bib47)）。然而，第[6](#S6
    "6 先训练大模型再压缩，还是不压缩 ‣ 如何训练你的（压缩的）大型语言模型")节的结果显示，截断初始化方法导致压缩模型丧失了太多零样本能力。
- en: Size of training data
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练数据的规模
- en: We hypothesize that the reason distillation does not lead to improved zero-shot
    performance is related to the training data size. In task-specific finetuning
    setup, where the task data is relatively limited, the larger model can learn more
    generalizable representations from the data and can provide a more informative
    training signal to the smaller student model, compared to what the student model
    can learn itself from data. In contrast, in the zero-shot evaluation setup, where
    we have unlimited pretraining data, distilling from a larger model does not provide
    much advantage when we continue to learn from billions of tokens.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设蒸馏方法未能改善零样本性能的原因与训练数据的规模有关。在任务特定的微调设置中，任务数据相对有限，较大的模型可以从数据中学习到更具普遍性的表示，并且能向较小的学生模型提供更有信息量的训练信号，相比于学生模型自己从数据中学习到的内容。相比之下，在零样本评估设置中，我们拥有无限的预训练数据，从较大模型中蒸馏并不会在继续从数十亿个标记中学习时带来太多优势。
- en: Aligning the pretraining objective
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对齐预训练目标
- en: Distillation uses a collection of training objectives that constrain the smaller
    model in various ways. This is a reasonable setup when the goal is to imitate
    the larger model as much as possible. However, in an infinite data regime with
    the goal of training a zero-shot model, a single language modeling objective is
    better aligned with the goal. It is more capable of eliciting zero-shot behavior
    compared to distillation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏使用了一系列训练目标，这些目标在各种方面约束了较小的模型。当目标是尽可能模仿较大的模型时，这是一种合理的设置。然而，在无限数据的情况下，目标是训练一个零样本模型时，单一的语言建模目标更符合这一目标。与蒸馏相比，它更能够引发零样本行为。
- en: 8 Conclusion
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: We examine compression techniques to preserve the zero-shot promptability of
    large language models. We proposed a task-agnostic compression methodology, with
    no teacher, using pruning for truncated initialization of the compressed model,
    and continued pretraining as an alternative to end-task finetuning. On a diverse
    zero-shot evaluation suite and perplexity, our method is comparable in performance
    to a range of distillation baselines while offering 1.5$\times$ compute efficient
    training. We also compare compression to an “oracle” setup of an equally sized
    model pretrained for the same token budget as the larger teacher model. We highlight
    that a broad range of compression methods underperform compared to an uncompressed
    model with more training. This surprising result that distillation is not the
    “silver bullet” here, as it has been in prior works, leads us to call for future
    research into closing this gap. We release our code and evaluation setup to encourage
    future research and discussion on improved compression strategies.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了压缩技术以保持大型语言模型的零-shot提示能力。我们提出了一种与任务无关的压缩方法，没有教师，使用修剪来初始化压缩模型，并继续预训练作为结束任务微调的替代方法。在一个多样化的零-shot评估套件和困惑度上，我们的方法在性能上与一系列蒸馏基准相当，同时提供了1.5$\times$的计算效率训练。我们还将压缩与一个“oracle”设置的等大小模型进行比较，该模型在与较大教师模型相同的标记预算下进行预训练。我们强调，广泛的压缩方法与经过更多训练的未压缩模型相比表现不佳。这一令人惊讶的结果表明，蒸馏在这里并不是像以往工作中那样的“灵丹妙药”，这促使我们呼吁未来研究来弥合这一差距。我们发布了我们的代码和评估设置，以鼓励未来的研究和讨论，改进压缩策略。
- en: Limitations
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: While our work is in the spirit of reducing model size and improving efficiency
    — we require significant computational resources for our experiments demanding
    both high energy usage and processing power. Experiments such as the teacher model
    pretraining and “oracle” student model upper-bound demand upto 5 days of training
    time using 32xA100 GPUs with a high bandwidth interconnect. Therefore, reproducing
    our experiments are only reasonably tractable with commensurate GPU resources
    which may be infeasible for some researchers.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的工作旨在减少模型的规模并提高效率，但我们的实验需要大量的计算资源，消耗高能量且需要强大的处理能力。例如，教师模型的预训练和“oracle”学生模型的上限实验需要使用32xA100
    GPU和高带宽互连进行长达5天的训练。因此，复制我们的实验仅在配备相应的GPU资源时才是合理的，这对于一些研究人员可能是不可行的。
- en: Additionally, we demonstrate our findings compared to a ‘vanilla’ distillation
    approach and recently published alternatives in our decoder-only setup. We take
    this approach to report how the most typical distillation strategy can be ported
    to a contemporary LLM. Our findings do not indicate that distillation is potentially
    fruitful for GPT-style models, however, our work is limited in that there may
    exist *some atypical* distillation strategy with even better performance. We encourage
    future work and discussion of how these methods can be improved in this regard.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们展示了我们的发现，并与‘原始’蒸馏方法以及最近发布的替代方案进行了比较，所有这些都是在我们的仅解码器设置中进行的。我们采用这种方法来报告最典型的蒸馏策略如何迁移到当代的大型语言模型。我们的发现并不表明蒸馏对GPT风格模型可能是有益的，然而，我们的工作存在局限性，因为可能存在*一些非典型的*蒸馏策略具有更好的性能。我们鼓励未来的工作和讨论，以改进这些方法。
- en: Ethics Statement
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: We report all pretraining experiments with the widely used C4 corpus. This corpus
    has been found to contain harmful artifacts and biases (Dodge et al., [2021](#bib.bib17))
    which our models may inherit, however, the study of this phenomena is outside
    of the scope of our work but may inform future study. Model compression has been
    linked to increased bias and toxicity in a model (Hooker et al., [2020](#bib.bib23))
    but it is currently unclear how such effects extend to our setting; particularly
    as we expose the student to the same corpus as the teacher. Further study is needed
    in this area to examine how compression influences biases in increasingly large
    language models (Solaiman et al., [2023](#bib.bib48)).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告了所有使用广泛使用的C4语料库的预训练实验。这个语料库被发现包含有害的伪影和偏见（Dodge等，[2021](#bib.bib17)），这些可能会被我们的模型继承，然而，这些现象的研究超出了我们工作的范围，但可能对未来的研究提供参考。模型压缩已被关联到模型的偏见和毒性增加（Hooker等，[2020](#bib.bib23)），但目前尚不清楚这些效果如何在我们的设置中体现；特别是我们让学生接触与教师相同的语料库。需要进一步研究这一领域，以考察压缩如何影响日益增大的语言模型中的偏见（Solaiman等，[2023](#bib.bib48)）。
- en: References
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ba et al. (2016) Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer
    normalization. *ArXiv*, abs/1607.06450.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴等人（2016）吉米·巴、杰米·瑞安·基罗斯和杰弗里·E·欣顿。2016年。层归一化。*ArXiv*，abs/1607.06450。
- en: Bengio et al. (2003) Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian
    Janvin. 2003. A neural probabilistic language model. *J. Mach. Learn. Res.*, 3:1137–1155.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边戈等人（2003）约书亚·边戈、雷杰安·迪尚姆、帕斯卡尔·文森特和克里斯蒂安·扬文。2003年。神经概率语言模型。*J. Mach. Learn. Res.*，3:1137–1155。
- en: 'Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
    Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
    USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der
    Wal. 2023. [Pythia: A suite for analyzing large language models across training
    and scaling](http://arxiv.org/abs/2304.01373).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比德曼等人（2023）斯特拉·比德曼、海莉·肖尔科普、昆廷·安东尼、赫比·布拉德利、凯尔·奥布赖恩、埃里克·哈拉汉、穆罕默德·阿夫拉赫·汗、施万舒·普罗希特、USVSN·赛·普拉尚、爱德华·拉夫、阿维亚·斯科夫隆、林唐·苏塔维卡和奥斯卡·范·德·瓦尔。2023年。[Pythia：一个分析大语言模型在训练和扩展中的工具套件](http://arxiv.org/abs/2304.01373)。
- en: 'Bisk et al. (2019) Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao,
    and Yejin Choi. 2019. Piqa: Reasoning about physical commonsense in natural language.
    In *AAAI Conference on Artificial Intelligence*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比斯克等人（2019）约纳坦·比斯克、罗温·泽勒斯、罗南·勒布拉斯、简锋·高和叶金·崔。2019年。Piqa：在自然语言中推理物理常识。在*AAAI人工智能会议*上。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 33, pages 1877–1901\.
    Curran Associates, Inc.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布朗等人（2020）汤姆·布朗、本杰明·曼、尼克·赖德、梅拉妮·萨比亚、贾瑞德·D·卡普兰、普拉弗拉·达里瓦尔、阿尔文·尼拉坎坦、普拉纳夫·夏姆、吉里什·萨斯特里、阿曼达·阿斯克尔、桑迪尼·阿加瓦尔、阿里尔·赫伯特-沃斯、格雷琴·克鲁格、汤姆·亨尼汉、瑞沃恩·查德、阿迪提亚·拉梅什、丹尼尔·齐格勒、杰弗里·吴、克莱门斯·温特、克里斯·赫斯、马克·陈、埃里克·西格勒、马泰乌斯·利特温、斯科特·格雷、本杰明·切斯、杰克·克拉克、克里斯托弗·伯纳、山姆·麦坎利什、亚历克·拉德福、伊利亚·苏茨克弗和达里奥·阿莫代伊。2020年。[语言模型是少样本学习者](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)。在*神经信息处理系统进展*中，第33卷，第1877–1901页。Curran
    Associates, Inc.
- en: Chen et al. (2020a) Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,
    Yang Zhang, Zhangyang Wang, and Michael Carbin. 2020a. The lottery ticket hypothesis
    for pre-trained bert networks. *ArXiv*, abs/2007.12223.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2020a）天龙·陈、乔纳森·弗兰克尔、施瑜·张、司佳·刘、杨·张、张扬·王和迈克尔·卡宾。2020a年。预训练BERT网络的中奖彩票假设。*ArXiv*，abs/2007.12223。
- en: Chen et al. (2023) Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan
    Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu,
    and Quoc V. Le. 2023. Symbolic discovery of optimization algorithms. *ArXiv*,
    abs/2302.06675.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2023）香宁·陈、陈亮、大黄、埃斯特班·雷阿尔、凯元·王、姚柳、休·范、轩怡·董、汤姆·Luong、卓睿·谢、易峰·卢、和国伟·乐。2023年。优化算法的符号发现。*ArXiv*，abs/2302.06675。
- en: 'Chen et al. (2020b) Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang
    Wang, and Jingjing Liu. 2020b. Earlybert: Efficient bert training via early-bird
    lottery tickets. In *Annual Meeting of the Association for Computational Linguistics*.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2020b）肖涵·陈、于诚、硕航·王、哲·甘、张扬·王和静静·刘。2020b年。Earlybert：通过早鸟彩票实现高效的BERT训练。在*计算语言学协会年会*上。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran,
    Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,
    Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay
    Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson,
    Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,
    Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
    Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,
    Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
    Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S.
    Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm:
    Scaling language modeling with pathways. *ArXiv*, abs/2204.02311.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等（2022）Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran,
    Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,
    Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay
    Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson,
    Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,
    Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
    Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,
    Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
    Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen
    S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, 和 Noah Fiedel. 2022.
    Palm: 扩展语言建模的路径。*ArXiv*, abs/2204.02311。'
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising
    difficulty of natural yes/no questions. *ArXiv*, abs/1905.10044.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark 等（2019）Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, 和 Kristina Toutanova. 2019. Boolq: 探索自然的“是/否”问题的惊人难度。*ArXiv*,
    abs/1905.10044。'
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved
    question answering? try arc, the ai2 reasoning challenge. *ArXiv*, abs/1803.05457.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等（2018）Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, 和 Oyvind Tafjord. 2018. 认为你已经解决了问答问题？试试 ARC，AI2 推理挑战。*ArXiv*,
    abs/1803.05457。
- en: 'Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. 2022. [Flashattention: Fast and memory-efficient exact attention with io-awareness](https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf).
    In *Advances in Neural Information Processing Systems*, volume 35, pages 16344–16359\.
    Curran Associates, Inc.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao 等（2022）Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, 和 Christopher Ré. 2022.
    [Flashattention: 快速且内存高效的精确注意力与 IO 感知](https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf)。在*神经信息处理系统进展*，第35卷，页码
    16344–16359。Curran Associates, Inc.'
- en: 'de Marneffe et al. (2019) Marie-Catherine de Marneffe, Mandy Simons, and Judith
    Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring
    discourse.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'de Marneffe 等（2019）Marie-Catherine de Marneffe, Mandy Simons, 和 Judith Tonhauser.
    2019. The commitmentbank: 研究自然话语中的投射。'
- en: Dehghani et al. (2023) Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr
    Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert
    Geirhos, Ibrahim M. Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen,
    Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver,
    Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh
    Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier,
    Alexey A. Gritsenko, Vighnesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas
    Mensink, Alexander Kolesnikov, Filip Paveti’c, Dustin Tran, Thomas Kipf, Mario
    Luvci’c, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby. 2023.
    Scaling vision transformers to 22 billion parameters. *ArXiv*, abs/2302.05442.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dehghani 等（2023）Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski,
    Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos,
    Ibrahim M. Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag
    Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci,
    Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran,
    Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey
    A. Gritsenko, Vighnesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink,
    Alexander Kolesnikov, Filip Paveti’c, Dustin Tran, Thomas Kipf, Mario Luvci’c,
    Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, 和 Neil Houlsby. 2023. 扩展视觉变换器至220亿参数。*ArXiv*,
    abs/2302.05442。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. [Gpt3.int8(): 8-bit matrix multiplication for transformers at scale](https://proceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference.pdf).
    In *Advances in Neural Information Processing Systems*, volume 35, pages 30318–30332\.
    Curran Associates, Inc.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人（2022）Tim Dettmers, Mike Lewis, Younes Belkada 和 Luke Zettlemoyer.
    2022. [Gpt3.int8(): 大规模变换器的 8 位矩阵乘法](https://proceedings.neurips.cc/paper_files/paper/2022/file/c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference.pdf)。载于
    *神经信息处理系统进展*，第 35 卷，页面 30318–30332。Curran Associates, Inc.'
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *ArXiv*, abs/1810.04805.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等人（2019）Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova.
    2019. BERT: 深度双向变换器的语言理解预训练。*ArXiv*, abs/1810.04805。'
- en: 'Dodge et al. (2021) Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew,
    Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. [Documenting
    large webtext corpora: A case study on the colossal clean crawled corpus](https://doi.org/10.18653/v1/2021.emnlp-main.98).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 1286–1305, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dodge 等人（2021）Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel
    Ilharco, Dirk Groeneveld, Margaret Mitchell 和 Matt Gardner. 2021. [记录大型网页文本语料库：以巨大清理爬取语料库为案例](https://doi.org/10.18653/v1/2021.emnlp-main.98)。载于
    *2021年自然语言处理实证方法会议论文集*，页面 1286–1305，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: Dolan and Brockett (2005) William B. Dolan and Chris Brockett. 2005. Automatically
    constructing a corpus of sentential paraphrases. In *International Joint Conference
    on Natural Language Processing*.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dolan 和 Brockett（2005）William B. Dolan 和 Chris Brockett. 2005. 自动构建句子释义语料库。载于
    *国际自然语言处理联合会议*。
- en: Fan et al. (2019) Angela Fan, Edouard Grave, and Armand Joulin. 2019. Reducing
    transformer depth on demand with structured dropout. *ArXiv*, abs/1909.11556.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人（2019）Angela Fan, Edouard Grave 和 Armand Joulin. 2019. 根据需要减少变换器深度，采用结构化丢弃。*ArXiv*,
    abs/1909.11556。
- en: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. 2021. [A framework for few-shot language model evaluation](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人（2021）Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi,
    Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang 和 Andy
    Zou. 2021. [少样本语言模型评估框架](https://doi.org/10.5281/zenodo.5371628)。
- en: 'Gururangan et al. (2020) Suchin Gururangan, Ana Marasović, Swabha Swayamdipta,
    Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don’t stop pretraining:
    Adapt language models to domains and tasks. *ArXiv*, abs/2004.10964.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gururangan 等人（2020）Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle
    Lo, Iz Beltagy, Doug Downey 和 Noah A. Smith. 2020. 不要停止预训练：将语言模型适应于领域和任务。*ArXiv*,
    abs/2004.10964。
- en: 'Holtzman et al. (2021) Ari Holtzman, Peter West, Vered Schwartz, Yejin Choi,
    and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability
    answer isn’t always right. *ArXiv*, abs/2104.08315.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holtzman 等人（2021）Ari Holtzman, Peter West, Vered Schwartz, Yejin Choi 和 Luke
    Zettlemoyer. 2021. 表面形式竞争：为什么最高概率的答案不总是正确的。*ArXiv*, abs/2104.08315。
- en: Hooker et al. (2020) Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio,
    and Emily Denton. 2020. [Characterising bias in compressed models](http://arxiv.org/abs/2010.03058).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hooker 等人（2020）Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio 和 Emily
    Denton. 2020. [压缩模型中的偏差特征](http://arxiv.org/abs/2010.03058)。
- en: 'Hou et al. (2020) Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, and Qun Liu.
    2020. Dynabert: Dynamic bert with adaptive width and depth. *ArXiv*, abs/2004.04037.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hou 等人（2020）Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang 和 Qun Liu. 2020. Dynabert:
    动态 BERT，具有自适应宽度和深度。*ArXiv*, abs/2004.04037。'
- en: 'Jiao et al. (2019) Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
    Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural
    language understanding. In *Findings*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiao 等人（2019）Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin
    Li, Fang Wang 和 Qun Liu. 2019. Tinybert: 为自然语言理解提取 BERT。载于 *Findings*。'
- en: 'Kao et al. (2022) Sheng-Chun Kao, Amir Yazdanbakhsh, Suvinay Subramanian, Shivani
    Agrawal, Utku Evci, and Tushar Krishna. 2022. Training recipe for n: M structured
    sparsity with decaying pruning mask. *ArXiv*, abs/2209.07617.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kao 等人（2022）Sheng-Chun Kao, Amir Yazdanbakhsh, Suvinay Subramanian, Shivani
    Agrawal, Utku Evci 和 Tushar Krishna. 2022. 用于 n: M 结构稀疏性的训练方案，具有衰减的剪枝掩码。*ArXiv*,
    abs/2209.07617。'
- en: Kullback and Leibler (1951) Solomon Kullback and R. A. Leibler. 1951. On information
    and sufficiency. *Annals of Mathematical Statistics*, 22:79–86.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kullback 和 Leibler (1951) Solomon Kullback 和 R. A. Leibler. 1951. 关于信息和充分性。*数学统计年鉴*，22:79–86。
- en: Li et al. (2021a) Jiaoda Li, Ryan Cotterell, and Mrinmaya Sachan. 2021a. Differentiable
    subset pruning of transformer heads. *Transactions of the Association for Computational
    Linguistics*, 9:1442–1459.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2021a) Jiaoda Li, Ryan Cotterell 和 Mrinmaya Sachan. 2021a. 可微分的变压器头子集剪枝。*计算语言学协会会刊*，9:1442–1459。
- en: Li et al. (2021b) Jiaoda Li, Ryan Cotterell, and Mrinmaya Sachan. 2021b. Differentiable
    subset pruning of transformer heads. *Transactions of the Association for Computational
    Linguistics*, 9:1442–1459.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2021b) Jiaoda Li, Ryan Cotterell 和 Mrinmaya Sachan. 2021b. 可微分的变压器头子集剪枝。*计算语言学协会会刊*，9:1442–1459。
- en: 'Li et al. (2020) Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer,
    Dan Klein, and Joseph Gonzalez. 2020. Train large, then compress: Rethinking model
    size for efficient training and inference of transformers. *ICML*.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2020) Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan
    Klein 和 Joseph Gonzalez. 2020. 先训练大模型，再压缩：重新思考高效训练和推理变压器的模型大小。*ICML*。
- en: 'Liang et al. (2023) Chen Liang, Haoming Jiang, Zheng Li, Xianfeng Tang, Bin
    Yin, and Tuo Zhao. 2023. Homodistil: Homotopic task-agnostic distillation of pre-trained
    transformers. *ArXiv*, abs/2302.09632.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang 等 (2023) Chen Liang, Haoming Jiang, Zheng Li, Xianfeng Tang, Bin Yin
    和 Tuo Zhao. 2023. Homodistil: 同质任务无关的预训练变压器的蒸馏。*ArXiv*，abs/2302.09632。'
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
    Roberta: A robustly optimized bert pretraining approach. *ArXiv*, abs/1907.11692.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi
    Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer 和 Veselin Stoyanov. 2019. Roberta:
    一种稳健优化的 BERT 预训练方法。*ArXiv*，abs/1907.11692。'
- en: 'McCarley et al. (2019) J. Scott McCarley, Rishav Chakravarti, and Avirup Sil.
    2019. Structured pruning of a bert-based question answering model. *arXiv: Computation
    and Language*.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'McCarley 等 (2019) J. Scott McCarley, Rishav Chakravarti 和 Avirup Sil. 2019.
    基于 BERT 的问答模型的结构化剪枝。*arXiv: 计算和语言*。'
- en: Michel et al. (2019) Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen
    heads really better than one? In *Neural Information Processing Systems*.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michel 等 (2019) Paul Michel, Omer Levy 和 Graham Neubig. 2019. 十六个头真的比一个头更好吗？
    在 *神经信息处理系统* 上。
- en: Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open
    book question answering. In *Conference on Empirical Methods in Natural Language
    Processing*.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov 等 (2018) Todor Mihaylov, Peter Clark, Tushar Khot 和 Ashish Sabharwal.
    2018. 一套盔甲能导电吗？一个用于开放书籍问答的新数据集。在 *自然语言处理经验方法会议* 上。
- en: 'Mukherjee et al. (2021) Subhabrata Mukherjee, Ahmed Hassan Awadallah, and Jianfeng
    Gao. 2021. Xtremedistiltransformers: Task transfer for task-agnostic distillation.
    *ArXiv*, abs/2106.04563.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mukherjee 等 (2021) Subhabrata Mukherjee, Ahmed Hassan Awadallah 和 Jianfeng
    Gao. 2021. Xtremedistiltransformers: 面向任务无关的蒸馏的任务迁移。*ArXiv*，abs/2106.04563。'
- en: Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized
    word representations. In *North American Chapter of the Association for Computational
    Linguistics*.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters 等 (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee 和 Luke Zettlemoyer. 2018. 深度上下文化的词表示。在 *北美计算语言学协会会议*
    上。
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya
    Sutskever. 2018. Improving language understanding by generative pre-training.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2018) Alec Radford, Karthik Narasimhan, Tim Salimans 和 Ilya Sutskever.
    2018. 通过生成预训练提高语言理解。
- en: Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei
    和 Ilya Sutskever. 2019. 语言模型是无监督的多任务学习者。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](http://jmlr.org/papers/v21/20-074.html).
    *Journal of Machine Learning Research*, 21(140):1–67.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等 (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li 和 Peter J. Liu. 2020. [通过统一的文本到文本变压器探索迁移学习的极限](http://jmlr.org/papers/v21/20-074.html)。*机器学习研究杂志*，21(140):1–67。
- en: 'Roemmele et al. (2011) Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S.
    Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal
    reasoning. In *Papers from the 2011 AAAI Spring Symposium*.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roemmele 等（2011）梅丽莎·罗梅尔、科斯敏·阿德里安·贝让和安德鲁·S·戈登。2011年。《可行替代方案的选择：对常识因果推理的评估》。见于
    *2011 AAAI 春季研讨会论文集*。
- en: 'Sajjad et al. (2020) Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav
    Nakov. 2020. Poor man’s bert: Smaller and faster transformer models. *ArXiv*,
    abs/2004.03844.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sajjad 等（2020）哈桑·萨贾德、法希姆·达尔维、纳迪尔·杜拉尼和普雷斯拉夫·纳科夫。2020年。《穷人的 BERT：更小更快的 Transformer
    模型》。*ArXiv*，abs/2004.03844。
- en: 'Sakaguchi et al. (2019) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2019. Winogrande: An adversarial winograd schema challenge at
    scale. *ArXiv*, abs/1907.10641.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi 等（2019）坂口圭介、罗南·勒布拉斯、昌德拉·巴戈瓦图拉和叶金·崔。2019年。《Winogrande：大规模对抗性 Winograd
    语料库挑战》。*ArXiv*，abs/1907.10641。
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper
    and lighter. *ArXiv*, abs/1910.01108.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等（2019）维克托·桑赫、利桑德·德布、朱利安·肖蒙德和托马斯·沃尔夫。2019年。《DistilBERT，BERT 的蒸馏版本：更小、更快、更便宜、更轻便》。*ArXiv*，abs/1910.01108。
- en: 'Sanh et al. (2020) Victor Sanh, Thomas Wolf, and Alexander M. Rush. 2020. Movement
    pruning: Adaptive sparsity by fine-tuning. *ArXiv*, abs/2005.07683.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等（2020）维克托·桑赫、托马斯·沃尔夫和亚历山大·M·拉什。2020年。《运动修剪：通过微调自适应稀疏性》。*ArXiv*，abs/2005.07683。
- en: Shazeer (2020) Noam M. Shazeer. 2020. Glu variants improve transformer. *ArXiv*,
    abs/2002.05202.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer（2020）诺亚·M·沙泽尔。2020年。《Glu 变体改进 Transformer》。*ArXiv*，abs/2002.05202。
- en: Shen et al. (2022) Sheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew E.
    Peters, and Iz Beltagy. 2022. Staged training for transformer language models.
    *ArXiv*, abs/2203.06211.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等（2022）盛深、皮特·沃尔什、库尔特·克伊策尔、杰西·道奇、马修·E·彼得斯和伊兹·贝尔塔吉。2022年。《针对 Transformer
    语言模型的分阶段训练》。*ArXiv*，abs/2203.06211。
- en: Solaiman et al. (2023) Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad,
    Dylan Baker, Su Lin Blodgett, Hal Daumé III au2, Jesse Dodge, Ellie Evans, Sara
    Hooker, Yacine Jernite, Alexandra Sasha Luccioni, Alberto Lusoli, Margaret Mitchell,
    Jessica Newman, Marie-Therese Png, Andrew Strait, and Apostol Vassilev. 2023.
    [Evaluating the social impact of generative ai systems in systems and society](http://arxiv.org/abs/2306.05949).
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Solaiman 等（2023）艾琳·索拉曼、泽拉克·塔拉特、威廉·阿格纽、拉马·艾哈迈德、迪伦·贝克、苏·林·布洛杰特、哈尔·道梅 III、杰西·道奇、艾莉·埃文斯、萨拉·胡克、雅辛·杰尼特、亚历山德拉·萨莎·卢乔尼、阿尔贝托·卢索利、玛格丽特·米切尔、杰西卡·纽曼、玛丽-特蕾斯·彭、安德鲁·斯特雷特和阿波斯托尔·瓦西列夫。2023年。[评估生成性
    AI 系统在系统和社会中的社会影响](http://arxiv.org/abs/2306.05949)。
- en: Sun et al. (2019) S. Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient
    knowledge distillation for bert model compression. In *Conference on Empirical
    Methods in Natural Language Processing*.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2019）S·孙、于程、贺·甘和静静·刘。2019年。《用于 BERT 模型压缩的患者知识蒸馏》。见于 *自然语言处理经验方法会议*。
- en: 'Sun et al. (2020) Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming
    Yang, and Denny Zhou. 2020. Mobilebert: a compact task-agnostic bert for resource-limited
    devices. *ArXiv*, abs/2004.02984.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2020）孙志清、余鸿昆、宋晓丹、刘仁杰、杨一鸣和周登毅。2020年。《MobileBERT：一种用于资源受限设备的紧凑型任务无关 BERT》。*ArXiv*，abs/2004.02984。
- en: Tang et al. (2019) Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova,
    and Jimmy J. Lin. 2019. Distilling task-specific knowledge from bert into simple
    neural networks. *ArXiv*, abs/1903.12136.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等（2019）拉斐尔·唐、姚璐、林青刘、莉莉·牟、奥尔加·维赫托莫娃和吉米·J·林。2019年。《将任务特定知识从 BERT 蒸馏到简单神经网络》。*ArXiv*，abs/1903.12136。
- en: 'Turc et al. (2019) Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
    2019. Well-read students learn better: On the importance of pre-training compact
    models. *arXiv: Computation and Language*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Turc 等（2019）尤莉亚·图尔克、明伟·张、肯顿·李和克里斯蒂娜·图塔诺娃。2019年。《读得多的学生学习更好：关于预训练紧凑模型的重要性》。*arXiv:
    计算与语言*。'
- en: Vaswani et al. (2017) Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *NIPS*.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017）阿希什·瓦斯瓦尼、诺亚·M·沙泽尔、尼基·帕尔马尔、雅各布·乌斯科雷特、利昂·琼斯、艾丹·N·戈麦斯、卢卡斯·凯泽和伊利亚·波洛苏金。2017年。《注意力机制是你所需要的一切》。见于
    *NIPS*。
- en: 'Voita et al. (2019) Elena Voita, David Talbot, F. Moiseev, Rico Sennrich, and
    Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the
    heavy lifting, the rest can be pruned. *ArXiv*, abs/1905.09418.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voita 等（2019）埃琳娜·沃伊塔、大卫·塔尔博特、F·莫伊谢耶夫、里科·森里奇和伊万·蒂托夫。2019年。《分析多头自注意力：专用头部承担重任，其余可以被剪枝》。*ArXiv*，abs/1905.09418。
- en: 'Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. 2018. Glue: A multi-task benchmark and analysis
    platform for natural language understanding. *ArXiv*, abs/1804.07461.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer
    Levy, 和 Samuel R. Bowman. 2018. Glue: 一种多任务基准测试和自然语言理解分析平台。*ArXiv*，abs/1804.07461。'
- en: 'Wang et al. (2020a) Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu
    Wei. 2020a. Minilmv2: Multi-head self-attention relation distillation for compressing
    pretrained transformers. In *Findings*.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 (2020a) Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, 和 Furu Wei.
    2020a. Minilmv2: 多头自注意力关系蒸馏用于压缩预训练的变换器。发表于*发现*。'
- en: 'Wang et al. (2020b) Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and
    Ming Zhou. 2020b. Minilm: Deep self-attention distillation for task-agnostic compression
    of pre-trained transformers. *ArXiv*, abs/2002.10957.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 (2020b) Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, 和 Ming
    Zhou. 2020b. Minilm: 深度自注意力蒸馏用于任务无关的预训练变换器压缩。*ArXiv*，abs/2002.10957。'
- en: Welbl et al. (2017) Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing
    multiple choice science questions. *ArXiv*, abs/1707.06209.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Welbl 等 (2017) Johannes Welbl, Nelson F. Liu, 和 Matt Gardner. 2017. 众包多项选择科学问题。*ArXiv*，abs/1707.06209。
- en: Xia et al. (2022) M. Xia, Zexuan Zhong, and Danqi Chen. 2022. Structured pruning
    learns compact and accurate models. In *Annual Meeting of the Association for
    Computational Linguistics*.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia 等 (2022) M. Xia, Zexuan Zhong, 和 Danqi Chen. 2022. 结构化剪枝学习紧凑且准确的模型。发表于*计算语言学协会年会*。
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? In
    *Annual Meeting of the Association for Computational Linguistics*.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers 等 (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, 和
    Yejin Choi. 2019. Hellaswag: 机器真的可以完成你的句子吗？发表于*计算语言学协会年会*。'
- en: 'Zhang et al. (2023) Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin
    Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Jiao Qiao. 2023. Llama-adapter: Efficient
    fine-tuning of language models with zero-init attention. *ArXiv*, abs/2303.16199.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 (2023) Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan,
    Pan Lu, Hongsheng Li, Peng Gao, 和 Yu Jiao Qiao. 2023. Llama-adapter: 通过零初始化注意力高效微调语言模型。*ArXiv*，abs/2303.16199。'
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained
    transformer language models. *ArXiv*, abs/2205.01068.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor
    Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura,
    Anjali Sridhar, Tianlu Wang, 和 Luke Zettlemoyer. 2022. Opt: 开放的预训练变换器语言模型。*ArXiv*，abs/2205.01068。'
