- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 19:03:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 19:03:50'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Compressing LLMs: The Truth is Rarely Pure and Never Simple'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 压缩LLMs：真相很少纯粹，永远不简单
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.01382](https://ar5iv.labs.arxiv.org/html/2310.01382)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.01382](https://ar5iv.labs.arxiv.org/html/2310.01382)
- en: ${}^{\text{\faApple}}$ Ajay Jaiswal¹, Zhe Gan², Xianzhi Du², Bowen Zhang², Zhangyang
    Wang¹, Yinfei Yang²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ${}^{\text{\faApple}}$ Ajay Jaiswal¹, Zhe Gan², Xianzhi Du², Bowen Zhang², Zhangyang
    Wang¹, Yinfei Yang²
- en: ¹University of Texas at Austin, ²Apple
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹德克萨斯大学奥斯汀分校，²苹果
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Despite their remarkable achievements, modern Large Language Models (LLMs)
    encounter exorbitant computational and memory footprints. Recently, several works
    have shown significant success in training-free and data-free compression (pruning
    and quantization) of LLMs achieving 50-60% sparsity and reducing the bit-width
    down to 3 or 4 bits per weight, with negligible perplexity degradation over the
    uncompressed baseline. As recent research efforts are focused on developing increasingly
    sophisticated compression methods, our work takes a step back, and re-evaluates
    the effectiveness of existing SoTA compression methods, which rely on a fairly
    simple and widely questioned metric, perplexity (even for dense LLMs). We introduce
    Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a collection of carefully-curated
    tasks to re-define the evaluation protocol for compressed LLMs, which have significant
    alignment with their dense counterparts, and perplexity fail to capture subtle
    change in their true capabilities. LLM-KICK unveils many favorable merits and
    unfortunate plights of current SoTA compression methods: all pruning methods suffer
    significant performance degradation, sometimes at trivial sparsity ratios (*e.g.*,
    25-30%), and fail for N:M sparsity on knowledge-intensive tasks; current quantization
    methods are more successful than pruning; yet, pruned LLMs even at $\geq 50$%
    sparsity are robust in-context retrieval and summarization systems; among others.
    LLM-KICK is designed to holistically access compressed LLMs’ ability for language
    understanding, reasoning, generation, in-context retrieval, in-context summarization,
    *etc.* We hope our study can foster the development of better LLM compression
    methods. All our related codes are planed to be open-sourced. ^†^†\faApple Work
    done during an internship at Apple.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了显著成就，现代大型语言模型（LLMs）仍面临着巨大的计算和内存开销。最近，几项研究已展示了在训练和数据无关的压缩（剪枝和量化）方面的显著成功，达到了50-60%的稀疏度，并将权重的位宽缩减至3或4位，同时对未压缩基线的困惑度几乎没有下降。由于近期的研究努力集中在开发越来越复杂的压缩方法上，我们的工作退一步，重新评估现有最先进的压缩方法的有效性，这些方法依赖于一个相当简单且广受质疑的指标——困惑度（即使对于密集型LLMs）。我们引入了知识密集型压缩LLM基准测试（LLM-KICK），这是一个精心策划的任务集合，旨在重新定义压缩LLMs的评估协议，这些协议与其密集型对手高度一致，而困惑度未能捕捉到其真实能力的细微变化。LLM-KICK揭示了当前最先进的压缩方法的许多有利优点和不幸困境：所有剪枝方法都遭遇了显著的性能下降，有时在微不足道的稀疏比例下（*例如*，25-30%），并且在知识密集型任务上对N:M稀疏度的效果不佳；当前的量化方法比剪枝更成功；然而，即使在$\geq
    50$%稀疏度下的剪枝LLMs在上下文检索和总结系统中仍然稳健；等等。LLM-KICK旨在全面评估压缩LLMs在语言理解、推理、生成、上下文检索、上下文总结等方面的能力。我们希望我们的研究能够促进更好LLM压缩方法的发展。我们所有相关的代码计划开源。
    ^†^†\faApple 在苹果公司实习期间完成的工作。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) are omnipresent, profoundly influencing not only
    the landscape of NLP (Ram et al., [2023](#bib.bib56); Liu et al., [2023a](#bib.bib41);
    Sawada et al., [2023](#bib.bib58); Qin et al., [2023](#bib.bib55); Zhuo, [2023](#bib.bib73);
    Lee et al., [2023](#bib.bib35)), but also recently buttressing numerous computer
    vision (Lian et al., [2023](#bib.bib38); Wang et al., [2023](#bib.bib64); Lai
    et al., [2023](#bib.bib33); Lu et al., [2023](#bib.bib46)) and graph neural networks
    (Ye et al., [2023](#bib.bib66); Chen et al., [2023](#bib.bib4); Qian et al., [2023](#bib.bib54);
    Duan et al., [2023](#bib.bib11)) algorithms; achieving steller performance across
    various task leaderboards. Despite their numerous unprecedented capabilities,
    their democratization is primarily restricted by the presence of billions of parameters,
    which depends on astonishingly high computational and memory requirements. For
    example, GPT-175B requires 325 GB of GPU memory simply to load its model weights,
    and at least five A100 (80GB) GPUs with sophisticated parallelism techniques (Sheng
    et al., [2023](#bib.bib59)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）无处不在，深刻地影响了不仅是自然语言处理（NLP）的格局（Ram et al., [2023](#bib.bib56); Liu
    et al., [2023a](#bib.bib41); Sawada et al., [2023](#bib.bib58); Qin et al., [2023](#bib.bib55);
    Zhuo, [2023](#bib.bib73); Lee et al., [2023](#bib.bib35)），而且最近还支持了大量计算机视觉（Lian
    et al., [2023](#bib.bib38); Wang et al., [2023](#bib.bib64); Lai et al., [2023](#bib.bib33);
    Lu et al., [2023](#bib.bib46)）和图神经网络（Ye et al., [2023](#bib.bib66); Chen et al.,
    [2023](#bib.bib4); Qian et al., [2023](#bib.bib54); Duan et al., [2023](#bib.bib11)）算法；在各种任务排行榜上表现卓越。尽管它们具有许多前所未有的能力，但其民主化主要受到数十亿个参数的限制，这依赖于极高的计算和内存要求。例如，GPT-175B仅加载其模型权重就需要325
    GB的GPU内存，并且至少需要五台配备有复杂并行技术的A100（80GB）GPU（Sheng et al., [2023](#bib.bib59)）。
- en: 'To democratize LLMs, considerable efforts have been taking to mitigate their
    high computational cost, mainly divided into two research directions: network
    pruning, and weight quantization. The former shrinks network sizes by removing
    specific weights from the model – essentially setting them to zero, while the
    latter aims to quantize parameters into lower bit-level representations. Several
    recent success in network pruning  (Sun et al., [2023](#bib.bib61); Frantar &
    Alistarh, [2023](#bib.bib13); Jaiswal et al., [2023a](#bib.bib22); Ma et al.,
    [2023](#bib.bib47); Ji et al., [2023](#bib.bib25)) and quantization (Liu et al.,
    [2023c](#bib.bib44); Kim et al., [2023](#bib.bib29); Dettmers et al., [2023a](#bib.bib7);
    Frantar et al., [2022](#bib.bib15); Lin et al., [2023](#bib.bib39); Dettmers et al.,
    [2023c](#bib.bib9)) (detailed related work discussion in Appendix [A.1](#A1.SS1
    "A.1 Related Works ‣ Appendix A Appendix ‣ Compressing LLMs: The Truth is Rarely
    Pure and Never Simple")) claim to retain the uncompressed LLM’s performance while
    achieving 50-60% sparsity or up to extreme 2-3 bit quantization. Although these
    advancements look fascinating, in most (if not all) cases, they heavily rely on
    perplexity as their primary metric to evaluate the performance claims. Such relatively
    restricted evaluations limit the scope for developing new compression methods,
    and are potentially ill-suited to identifying new and unexpected capabilities/limitations
    of compressed LLMs.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '为了使大语言模型（LLMs）民主化，人们付出了相当大的努力来减轻它们的高计算成本，主要分为两个研究方向：网络剪枝和权重量化。前者通过从模型中去除特定权重来缩小网络规模——本质上是将这些权重设置为零，而后者旨在将参数量化为较低位数的表示。最近在网络剪枝（Sun
    et al., [2023](#bib.bib61); Frantar & Alistarh, [2023](#bib.bib13); Jaiswal et
    al., [2023a](#bib.bib22); Ma et al., [2023](#bib.bib47); Ji et al., [2023](#bib.bib25)）和量化（Liu
    et al., [2023c](#bib.bib44); Kim et al., [2023](#bib.bib29); Dettmers et al.,
    [2023a](#bib.bib7); Frantar et al., [2022](#bib.bib15); Lin et al., [2023](#bib.bib39);
    Dettmers et al., [2023c](#bib.bib9)）方面取得了若干成功（详细的相关工作讨论见附录[A.1](#A1.SS1 "A.1 Related
    Works ‣ Appendix A Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never
    Simple")），这些工作声称在实现50-60%的稀疏度或极端的2-3位量化的同时保持未压缩LLM的性能。尽管这些进展看起来令人着迷，但在大多数（如果不是所有的话）情况下，它们严重依赖于困惑度作为评估性能声明的主要指标。这种相对受限的评估限制了新压缩方法的发展空间，并可能不适合识别压缩LLM的新功能或局限性。'
- en: 'Perplexity, even in the case of dense LLMs, has been questioned as an unsatisfactory
    measure for comparing the true potential of LLMs, despite significant variations
    in model scales, training strategies, and architecture choices (Muhlgay et al.,
    [2023](#bib.bib51)). It is important to note that all compressed models are derived
    from the same dense counterpart with high similarity, and aforementioned differences
    don’t exist, making their evaluation more challenging. In this work, we revisit
    a widely known yet under-explored question: How well does perplexity capture the
    change in capabilities of compressed LLMs that have significant alignment with
    their dense counterpart? We focus on the case of compressed LLMs, because we observe
    comparatively more serious failure of perplexity to capture the delicate performance
    variations incurred across varying compression stages of LLMs, demanding a more
    fine-grained investigation.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在密集LLM的情况下，困惑度也被质疑作为比较LLM真实潜力的不满意指标，尽管模型规模、训练策略和架构选择存在显著变化（Muhlgay等，[2023](#bib.bib51)）。重要的是要注意，所有压缩模型都是从具有高度相似性的相同密集对等体衍生的，且上述差异不存在，这使得它们的评估更加困难。在这项工作中，我们重新审视了一个广为人知但未被充分探索的问题：困惑度在多大程度上捕捉了与其密集对等体高度对齐的压缩LLM能力的变化？我们关注于压缩LLM的情况，因为我们观察到困惑度在捕捉LLM不同压缩阶段产生的细微性能变化时存在相对更严重的失败，要求进行更精细的调查。
- en: In this work, we attempt to investigate the true promises and limitations of
    state-of-the-art compression algorithms for LLMs. We assemble the first comprehensive
    and diverse collection of tasks with varying difficulty levels to thoroughly study
    compressed LLMs under quantization and network pruning (structured and unstructured
    sparsity patterns). More specifically, we consider a broad range of tasks to evaluate
    subtle changes in pruned and quantized LLMs’ ability for language understanding,
    reasoning, generation, in-context retrieval, long-context summarization, *etc*.
    Note that none of the datasets in our multi-dimensional study of compressed LLMs
    was created from scratch, but we rely on existing datasets as they have been widely
    accepted by researchers, but unfortunately yet not been adopted to study the effect
    of compression. We rigorously measure the performance of SoTA quantization and
    pruning approaches (in their most common, default settings), to understand their
    potential for our challenging and interesting tasks with high practical value.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们试图调查最先进压缩算法对LLM的真实承诺和局限性。我们汇集了第一个全面且多样化的任务集合，这些任务具有不同的难度级别，以彻底研究量化和网络剪枝（结构化和非结构化稀疏模式）下的压缩LLM。更具体来说，我们考虑了广泛的任务来评估修剪和量化LLM在语言理解、推理、生成、上下文检索、长上下文总结等能力上的微妙变化。请注意，我们的多维度压缩LLM研究中的数据集没有一个是从头开始创建的，而是依赖于现有的数据集，因为这些数据集已经被研究人员广泛接受，但遗憾的是尚未用于研究压缩的效果。我们严格测量了最先进量化和剪枝方法（在其最常见的默认设置下）的性能，以了解它们在具有高实际价值的挑战性和有趣任务中的潜力。
- en: '![Refer to caption](img/699cac02c3056338ea387eb73014e7b2.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/699cac02c3056338ea387eb73014e7b2.png)'
- en: 'Figure 1: True Merits of SoTA Compression. Top row indicates marginal increase
    in perplexity via using SoTA compression methods, when compared with simple magnitude-based
    pruning. Bottom row indicates the failure of compressed Vicuna-7B (Chiang et al.,
    [2023](#bib.bib5)) (via Magnitude, Wanda, SparseGPT, GPTQ) to respond correctly
    to knowledge-intensive factoid-based questions.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：最先进压缩的真正优点。顶行表示使用最先进压缩方法时，困惑度的边际增加，与简单的基于幅度的剪枝相比。底行表示压缩后的Vicuna-7B（Chiang等，[2023](#bib.bib5)）（通过Magnitude、Wanda、SparseGPT、GPTQ）无法正确回答知识密集型的事实型问题。
- en: 'Our key observations and contributions can be unfolded as:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的关键观察和贡献可以展开如下：
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), to re-define
    the evaluation protocols for compressed LLMs and facilitate a comprehensive assessment
    of SoTA compression algorithms. The premise of our work is to develop a suite
    of challenging, realistic, and diverse tasks of high practical importance and
    datasets that can empower a systematic understanding of how existing LLM compression
    strategies truly perform in preserving performance despite their similar perplexities,
    how they differ from each other, and how they compare against smaller LLMs of
    comparable parameter counts.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们工作的前提是开发一套具有挑战性、现实性和多样性的任务和数据集，这些任务和数据集具有高度实际的重要性，并能够深入理解现有LLM压缩策略在保持性能方面的真实表现，尽管它们的困惑度相似，了解它们之间的差异，以及它们与参数数量相当的较小LLM的比较。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们提出了知识密集型压缩LLM基准测试（LLM-KICK），旨在重新定义压缩LLM的评估协议，并促进对最先进压缩算法的全面评估。
- en: LLM-KICK unveils many interesting and critical observations, that perplexity-based
    evaluations overlook. 1 Most SoTA pruning
    methods suffer significant performance degradation, sometimes at trivial sparsity
    ratios (e.g., 25-30%), despite negligible changes in perplexity. 2
    All SoTA pruning methods do not work satisfactorily for structured N:M sparsity
    patterns on LLM-KICK. 3 Current SoTA
    LLM quantization methods are more successful in perpetuating performance in comparison
    to SoTA LLM pruning methods. 4 Compressed
    LLMs fail to generate knowledge-enriched and factually correct answers, despite
    the generated text is fluent, consistent, and coherent. 5
    Compressed LLMs with larger architectures but same parameter counts perform poorer,
    which favors smaller dense models.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM-KICK 揭示了许多有趣且关键的观察结果，这些结果是基于困惑度的评估所忽视的。1 大多数最先进的剪枝方法在性能上遭遇了显著的下降，尽管困惑度几乎没有变化，有时在微不足道的稀疏比率（例如，25-30%）下也会出现这种情况。2
    所有最先进的剪枝方法在 LLM-KICK 上对结构化 N:M 稀疏模式的效果不尽如人意。3 当前最先进的 LLM
    量化方法在维持性能方面比最先进的 LLM 剪枝方法更成功。4 压缩后的 LLM 尽管生成的文本流畅、一致且连贯，但仍未能产生知识丰富且事实正确的回答。5
    尽管压缩后的 LLM 具有更大的架构但参数数量相同，其表现却较差，这更倾向于小型的密集模型。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We further investigate compressed LLMs’ ability for in-context settings, via
    adopting in-context retrieval augmented question answering (ICRA-QA) (Ram et al.,
    [2023](#bib.bib56)), and text summarization with in-context learning (IC-Sum)
    (Jain et al., [2023](#bib.bib21)). To our surprise, pruned LLMs, even at non-trivial
    sparsity ratios (*e.g.*, $\geq$50%), are robust retrieval systems, and can perform
    text summarization while maintaining similar performance as their dense counterpart.
    However, with increasing compression degrees, their ability to digest longer context
    is affected more than smaller context.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进一步研究了压缩 LLM 在上下文设置中的能力，采用了上下文检索增强问答（ICRA-QA）（Ram 等， [2023](#bib.bib56)）和基于上下文学习的文本摘要（IC-Sum）（Jain
    等，[2023](#bib.bib21)）。令人惊讶的是，即使在非平凡的稀疏比例（*例如*，$\geq$50%）下，修剪后的 LLM 也仍然是强大的检索系统，并且在执行文本摘要时能保持与其密集型对照组相似的性能。然而，随着压缩程度的增加，它们处理较长上下文的能力受到的影响大于处理较小上下文的能力。
- en: '2 SoTA LLM Compression: Perplexity, or What’s More?'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 SoTA LLM 压缩：困惑度，还是其他？
- en: Scaling neural networks, now LLMs, have achieved astonishing performance benefits
    on a wide array of tasks, but at the cost of gigantic computational and memory
    footprints. Network pruning and weight quantization are two popular remedies to
    mitigate these overheads due to billions of parameter counts in current LLMs.
    Despite numerous existing algorithms for pruning (Singh & Alistarh, [2020](#bib.bib60);
    Zhu & Gupta, [2017](#bib.bib72); Gale et al., [2019](#bib.bib16); Jaiswal et al.,
    [2022](#bib.bib23); Lin et al., [2020](#bib.bib40); Liu et al., [2021a](#bib.bib42);
    Mostafa & Wang, [2019](#bib.bib50); Dettmers & Zettlemoyer, [2019](#bib.bib6);
    Evci et al., [2020](#bib.bib12)) and quantization (Dong et al., [2022](#bib.bib10);
    Cardinaux et al., [2020](#bib.bib1); Kim et al., [2021](#bib.bib30); Liu et al.,
    [2021b](#bib.bib45); Martinez et al., [2020](#bib.bib48)), their ad-hoc adaptation
    for LLMs is restricted, due to the lack of luxury to perform iterative re-training
    to regain any performance drop during compression. Recently, several works have
    shown significant success in training-free and data-free compression of LLMs achieving
    50-60% sparsity and reducing the bit-width down to 3 or 4 bits per weight, with
    negligible perplexity degradation relative to the uncompressed baseline.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 规模化神经网络，如今的 LLM，已在广泛的任务上取得了惊人的性能提升，但代价是巨大的计算和内存开销。网络剪枝和权重量化是减轻这些开销的两种流行方法，原因在于当前
    LLM 的参数数量达到了数十亿。尽管存在大量的剪枝算法（Singh & Alistarh，[2020](#bib.bib60)；Zhu & Gupta，[2017](#bib.bib72)；Gale
    等，[2019](#bib.bib16)；Jaiswal 等，[2022](#bib.bib23)；Lin 等，[2020](#bib.bib40)；Liu
    等，[2021a](#bib.bib42)；Mostafa & Wang，[2019](#bib.bib50)；Dettmers & Zettlemoyer，[2019](#bib.bib6)；Evci
    等，[2020](#bib.bib12)）和量化算法（Dong 等，[2022](#bib.bib10)；Cardinaux 等，[2020](#bib.bib1)；Kim
    等，[2021](#bib.bib30)；Liu 等，[2021b](#bib.bib45)；Martinez 等，[2020](#bib.bib48)），由于缺乏执行迭代再训练以弥补压缩过程中任何性能下降的奢侈，现有算法对
    LLM 的适应性受到限制。最近，几项工作在无训练和无数据压缩 LLM 上取得了显著成功，达到 50-60% 的稀疏率，并将权重量化降低到每权重 3 或 4
    位，且与未压缩基线相比，困惑度降解几乎可以忽略不计。
- en: Perplexity is a statistical measure of how confident a language model predicts
    a text sample and quantifies the “surprise” encoded within language models (the
    lower the perplexity, the better the model). Despite its popularity, perplexity
    has been widely questioned as an unsatisfactory measure to compare the true merits
    of two different LLMs (Muhlgay et al., [2023](#bib.bib51)), even for dense models
    although they significantly vary in model scale, training strategies, and design
    choices (encoder only, decoder only, *etc*.). To address this issue, several works
    (Li et al., [2023](#bib.bib36); Kaddour et al., [2023](#bib.bib28); Muhlgay et al.,
    [2023](#bib.bib51); Zhang et al., [2023](#bib.bib69); Valmeekam et al., [2022](#bib.bib63);
    Liu et al., [2023a](#bib.bib41); Sawada et al., [2023](#bib.bib58); Qin et al.,
    [2023](#bib.bib55); Zhuo, [2023](#bib.bib73); Lee et al., [2023](#bib.bib35))
    attempt to go beyond perplexity, and evaluate the capabilities of dense LLMs across
    commonsense reasoning, language understanding, reading comprehension, programming,
    *etc*. However, it is critically important to note that all compressed models
    are derived from the same dense counterpart with high similarity sharing exactly
    the same scale, training strategies, design choices, *etc*. Surprisingly, unlike
    dense LLMs, we found no such effort has been carried out to understand subtle
    changes in the capabilities of compressed LLMs with varying compression strength.
    Orthogonal to the recent trend to develop new compression algorithms, our work
    provides the first attempt to assess the true merits and limitations of existing
    SoTA LLM compression algorithms, to provide a fair and detailed playground to
    develop better compression algorithms. We focus on the daunting case of compressed
    LLMs because we observe profound failure of perplexity to capture the delicate
    performance variations incurred across varying LLM compressions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度是语言模型预测文本样本时信心的统计度量，并量化语言模型中编码的“惊讶”程度（困惑度越低，模型越好）。尽管困惑度很流行，但它作为比较两个不同LLM的真正优点的度量标准已被广泛质疑（Muhlgay等，[2023](#bib.bib51)），即使对于密集模型，它们在模型规模、训练策略和设计选择（仅编码器，仅解码器，*等等*）上差异显著。为了应对这一问题，几项研究（Li等，[2023](#bib.bib36)；Kaddour等，[2023](#bib.bib28)；Muhlgay等，[2023](#bib.bib51)；Zhang等，[2023](#bib.bib69)；Valmeekam等，[2022](#bib.bib63)；Liu等，[2023a](#bib.bib41)；Sawada等，[2023](#bib.bib58)；Qin等，[2023](#bib.bib55)；Zhuo，[2023](#bib.bib73)；Lee等，[2023](#bib.bib35)）试图超越困惑度，并评估密集LLM在常识推理、语言理解、阅读理解、编程等方面的能力。然而，值得注意的是，所有压缩模型都是从相同的密集模型衍生出来的，具有高度相似性，共享完全相同的规模、训练策略、设计选择等。令人惊讶的是，与密集LLM不同，我们发现没有相关工作致力于理解不同压缩强度下压缩LLM能力的细微变化。与最近开发新压缩算法的趋势正交，我们的工作首次尝试评估现有最先进LLM压缩算法的真正优点和局限性，为开发更好的压缩算法提供公平和详细的实验平台。我们专注于压缩LLM这一棘手的案例，因为我们观察到困惑度无法捕捉到由于LLM压缩程度变化所带来的细微性能差异。
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compressing LLMs: The Truth
    is Rarely Pure and Never Simple")(Top) illustrates the change in perplexity of
    SoTA compression methods (pruning and quantization), such as SparseGPT, Wanda,
    GPTQ and baseline one-shot magnitude-based pruning on Vicuna-7B, 13B, and 33B
    (Chiang et al., [2023](#bib.bib5)). Clearly, the perplexity ($\downarrow$), all
    SoTA pruning methods have almost similar performance as the simple baseline of
    one-shot magnitude-based pruning, which raises questions about their true merits
    within this sparsity range. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compressing
    LLMs: The Truth is Rarely Pure and Never Simple")(Bottom) show the response of
    Vicuna-7B model when compressed with Magnitude, SparseGPT, and Wanda by 50% and
    quantized up to 4-bit. The uncompressed Vicuna-7B was successfully able to generate
    the correct answer, but all compressed versions failed to respond correctly, hallucinating
    with either wrong facts or irrelevant responses.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compressing LLMs: The Truth is Rarely
    Pure and Never Simple")(上) 说明了SoTA压缩方法（剪枝和量化）的困惑度变化，例如SparseGPT、Wanda、GPTQ和基线一次性幅度剪枝在Vicuna-7B、13B和33B上的表现（Chiang等，[2023](#bib.bib5)）。显然，困惑度（$\downarrow$），所有SoTA剪枝方法的表现几乎与简单的基线一次性幅度剪枝相似，这引发了关于它们在这一稀疏范围内真正优点的问题。图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compressing LLMs: The Truth is Rarely
    Pure and Never Simple")(下) 显示了Vicuna-7B模型在用Magnitude、SparseGPT和Wanda压缩50%并量化至4-bit时的响应。未压缩的Vicuna-7B能够成功生成正确答案，但所有压缩版本都未能正确响应，产生了错误的事实或无关的回答。'
- en: '3 LLM-KICK: Unveiling True Merits of LLM Compression'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '3 LLM-KICK: 揭示 LLM 压缩的真实优点'
- en: LLM-KICK, short for Knowledge-Instensive Compressed LLM BenchmarK, is crafted
    to bring the attention of LLM compression community towards incompetence of perplexity
    to correctly reflect subtle changes in the ability of LLMs derived from dense
    counterparts with varying compression strength. LLM-KICK consists of a suite of
    challenging, realistic, and diverse task settings of high practical importance
    and datasets that can empower a systematic understanding of how existing LLM compression
    strategies truly perform in preserving performance despite having similar perplexity.
    Our work thoroughly investigates proclaimed merits/limitations of pruned and quantized
    LLMs for language understanding, reasoning, generation, in-context retrieval,
    in-context summarization, *etc*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LLM-KICK，即知识密集型压缩 LLM 基准测试的缩写，旨在引起 LLM 压缩社区对困惑度无法准确反映 LLM 能力微妙变化的关注，这些能力来源于具有不同压缩强度的密集型对照组。LLM-KICK
    包含了一系列具有挑战性、现实性和多样性的任务设置及数据集，这些数据集能够系统性地理解现有 LLM 压缩策略在保持性能方面的真实表现，即使在困惑度相似的情况下。我们的工作深入探讨了剪枝和量化
    LLM 在语言理解、推理、生成、上下文检索、上下文总结等方面的声称优点/局限性。
- en: 'Specifically, LLM-KICK consists of 3 broad task settings to study how compression
    impacts knowledge encoded during pre-training, how compressed LLMs perform tasks
    when required knowledge is augmented in-context, and how well compressed LLMs
    perform instruction following. To compartmentalize task difficulty and diversity,
    we include factoid-based QA, multiple-choice reasoning-based QA, in-context retrieval
    augmented QA, in-context text summarization, and instruction-based free-form text
    generation. Instead of creating new datasets, we carefully curate LLM-KICK from
    prior works and open-source GitHub repositories which have been widely accepted
    by researchers, but yet not explored by the LLM compression researchers. Our detailed
    prompt design strategies for different task settings can be found in Appendix
    [A.2](#A1.SS2 "A.2 Prompt Design and Examples for Different Task Settings in LLM-KICK
    ‣ Appendix A Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple").'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '具体而言，LLM-KICK 包含 3 个广泛的任务设置，用于研究压缩如何影响在预训练过程中编码的知识、压缩 LLM 在需要上下文中增强的知识时如何执行任务，以及压缩
    LLM 在遵循指令时的表现。为了区分任务的难度和多样性，我们包括了基于事实的问答、多项选择推理问答、上下文检索增强的问答、上下文文本总结以及基于指令的自由形式文本生成。我们没有创建新的数据集，而是精心策划了
    LLM-KICK，来自先前的工作和广泛被研究人员接受的开源 GitHub 存储库，但尚未被 LLM 压缩研究者探索。不同任务设置的详细提示设计策略可以在附录
    [A.2](#A1.SS2 "A.2 Prompt Design and Examples for Different Task Settings in LLM-KICK
    ‣ 附录 A 附录 ‣ 压缩 LLM: 事实很少纯粹，也从未简单") 中找到。'
- en: 'To reduce the expense of redundant experiments and clutter in results, our
    work primarily focuses on the top-2 existing training-free and data-free LLM pruning
    techniques (*i.e.*, SparseGPT (Frantar & Alistarh, [2023](#bib.bib13)) and Wanda
    (Sun et al., [2023](#bib.bib61))), along with the baseline of One-shot Magnitude-based
    Pruning (Han et al., [2016](#bib.bib17)), plus a popular quantization technique
    (GPTQ) among recently available choices (Lin et al., [2023](#bib.bib39); Frantar
    et al., [2022](#bib.bib15); Dettmers et al., [2023c](#bib.bib9)). We consider
    two types of sparsities: ($i$) Structured N:M Sparsity: a fine-grained sparsity
    pattern in which only N weights are non-zero for every continuous M weights (Nvidia,
    [2020](#bib.bib53); Zhou et al., [2021](#bib.bib71)). We use Vicuna models for
    experiments, which are open-source chatbot models trained by fine-tuning LLaMA
    (Chiang et al., [2023](#bib.bib5)) on user-shared conversations collected from
    ShareGPT, and have demonstrated impressive 90% quality of OpenAI ChatGPT and Google
    Bard. Note that the aim of this work is not limited to identifying the failure
    cases of SoTA pruning methods, but instead provides an in-depth lookup of LLM’s
    ability under compression, and bring new insights which include highlighting observations
    that work in favor of current SoTA compression methods.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少冗余实验和结果的杂乱，我们的工作主要关注于现有的两种无需训练和数据的LLM剪枝技术（*即*，SparseGPT (Frantar & Alistarh,
    [2023](#bib.bib13)) 和 Wanda (Sun et al., [2023](#bib.bib61)))，以及基准的单次幅度剪枝 (Han
    et al., [2016](#bib.bib17))，加上一种流行的量化技术（GPTQ），这是最近可用选择中的热门选择（Lin et al., [2023](#bib.bib39);
    Frantar et al., [2022](#bib.bib15); Dettmers et al., [2023c](#bib.bib9)）。我们考虑两种稀疏性：
    ($i$) 结构化 N:M 稀疏性：一种细粒度的稀疏模式，其中每 M 个连续权重中仅有 N 个权重非零 (Nvidia, [2020](#bib.bib53);
    Zhou et al., [2021](#bib.bib71))。我们使用 Vicuna 模型进行实验，这些模型是通过对来自 ShareGPT 的用户共享对话进行微调
    LLaMA (Chiang et al., [2023](#bib.bib5)) 训练的开源聊天机器人模型，并展示了相较于 OpenAI ChatGPT 和
    Google Bard 的 90% 质量。注意，本工作的目的不仅限于识别 SoTA 剪枝方法的失败案例，还在于深入探索 LLM 在压缩下的能力，并带来新的见解，包括突显出有利于当前
    SoTA 压缩方法的观察结果。
- en: Formally, we study the performance drop of LLMs after compression (without fine-tuning)
    with respect to their dense counterparts using a compression algorithm C. For
    a pre-trained LLM $f(x;\theta)$-bit using a quantization algorithm. Next, we define
    *matching* compressed LLM.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，我们研究了压缩（未进行微调）后的 LLM 性能下降情况，相对于其稠密版本使用压缩算法 C。对于一个使用量化算法的预训练 LLM $f(x;\theta)$-bit。接下来，我们定义*匹配*的压缩
    LLM。
- en: 'Matching
    Compressed LLM: A compressed LLM $f_{\texttt{comp}}(x;\theta_{\texttt{C}})$.![Refer
    to caption](img/b4d04c68984eedbaff9773f1a0c1e69f.png)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配压缩LLM：一个压缩的LLM
    $f_{\texttt{comp}}(x;\theta_{\texttt{C}})$。![参考说明](img/b4d04c68984eedbaff9773f1a0c1e69f.png)
- en: 'Figure 2: Compressed LLMs for Factoid-based QA. Performance comparison of compressed
    LLMs on Factoid-QA task using FreebaseQA (Jiang et al., [2019](#bib.bib26)). Results
    (average across 3 independent runs) presented are for structured (N:M sparsity),
    unstructured sparsity, and quantization.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：用于基于事实的 QA 的压缩 LLM。使用 FreebaseQA (Jiang et al., [2019](#bib.bib26)) 对压缩
    LLM 在事实问答任务中的性能进行比较。结果（3 次独立运行的平均值）展示了结构化（N:M 稀疏性）、非结构化稀疏性和量化的情况。
- en: '3.1 Setting 1: How Well Compressed LLMs Access Remaining Knowledge?'
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 设置 1：压缩的LLMs如何访问剩余知识？
- en: 1
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 1
- en: Factoid-based Question Answering
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基于事实的问题回答
- en: Task Definition and Rationale. Factoid-based Question Answering (Factoid-QA)
    (Iyyer et al., [2014](#bib.bib20)), which asks precise facts about entities, is
    a long-standing problem in NLP. A typical Factoid-QA task aims to search for entities
    or entity attributes from a knowledge graph, and it is widely used as a tool in
    academia, commercial search engines, and conversational assistants. Modern LLMs
    are trained on gigantic text corpora ingesting a large amount of world knowledge
    about entities and their relationships during pre-training, and have unique abilities
    to generate factually correct responses to user queries. In this task setting,
    we aim to investigate how compression impacts LLMs’ ability to answer natural
    language questions using facts, i.e., entities or attributes knowledge ingested
    within them during pre-training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 任务定义和理由。基于事实的问题回答（Factoid-QA）（Iyyer等，[2014](#bib.bib20)），即询问有关实体的精确事实，是自然语言处理中的一个长期存在的问题。一个典型的Factoid-QA任务旨在从知识图谱中搜索实体或实体属性，它在学术界、商业搜索引擎和对话助手中被广泛使用。现代大型语言模型（LLMs）在预训练期间通过摄取大量有关实体及其关系的世界知识，接受了巨大的文本语料库的训练，并具备生成事实正确的用户查询响应的独特能力。在这个任务设置中，我们旨在探讨压缩如何影响LLMs使用事实（即预训练期间摄取的实体或属性知识）回答自然语言问题的能力。
- en: Dataset Details. We use FreebaseQA (Jiang et al., [2019](#bib.bib26)) which
    is a dataset for open-domain QA over the Freebase knowledge graph. The QA pairs
    are collected from various sources, including the TriviaQA dataset (Joshi et al.,
    [2017](#bib.bib27)) and other trivia websites (QuizBalls, QuizZone, KnowQuiz),
    and are matched against Freebase to generate relevant subject-predicate-object
    triples that were further verified by human annotators. TriviaQA dataset shows
    rich linguistic variation and complexity, making it a good testbed for evaluating
    knowledge ingested within LLMs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集详情。我们使用FreebaseQA（Jiang等，[2019](#bib.bib26)），这是一个用于开放领域问答的Freebase知识图谱数据集。问答对从各种来源收集，包括TriviaQA数据集（Joshi等，[2017](#bib.bib27)）和其他趣味网站（QuizBalls、QuizZone、KnowQuiz），并与Freebase进行匹配，以生成相关的主谓宾三元组，这些三元组经过人工标注员进一步验证。TriviaQA数据集展示了丰富的语言变异性和复杂性，使其成为评估LLMs中摄取的知识的良好试验场。
- en: 'Results and Analysis. The results of various LLM compression methods are demonstrated
    in Figure [2](#S3.F2 "Figure 2 ‣ 3 LLM-KICK: Unveiling True Merits of LLM Compression
    ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple"). Our primary observations
    include: 1
    All SoTA LLM pruning methods seemingly fail to find matching sparse LLMs, even
    at trivial sparsities such as 30-35%. While several methods maintain the matching
    performance at 20-25% sparsity, their performance starts to drop significantly
    after that undergoing a catastrophic failure as sparsity ratio increases. This
    is in contrast with the claim made by SoTA pruning methods that pruning up to
    50-60% of LLMs doesn’t have any significant degradation on performance. 2
    All pruning methods doesn’t work for fine-grained structured N:M sparsity patterns
    with performance drop as severe as $\geq$8-10% drop in performance for non-aggressive
    8-bit quantization indicates that along with chasing for aggressive quantization
    levels (1-2 bits), it is also important to focus on yet unsolved 8-bit quantization.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '结果和分析。各种LLM压缩方法的结果如图[2](#S3.F2 "图 2 ‣ 3 LLM-KICK: 揭示LLM压缩的真实优点 ‣ 压缩LLMs: 真相往往不纯粹且从未简单")所示。我们的主要观察结果包括：1
    所有的SoTA LLM剪枝方法似乎都无法找到匹配的稀疏LLMs，即使在30-35%这样微不足道的稀疏度下也是如此。虽然几种方法在20-25%稀疏度下保持匹配性能，但其性能在稀疏比增加后显著下降，最终发生灾难性失败。这与SoTA剪枝方法声称的剪枝高达50-60%不会对性能造成显著降级的说法形成了对比。2
    所有剪枝方法对细粒度结构化N:M稀疏模式都不起作用，性能下降严重，如在非激进的8位量化中表现出$\geq$8-10%的性能下降，这表明除了追求激进的量化级别（1-2位）外，还需关注尚未解决的8位量化问题。'
- en: 2
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 2
- en: Multiple-Choice Reasoning based Question Answering
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基于多项选择推理的问答
- en: Task Formulation and Rationale. Multiple-Choice Reasoning based QA (MCR-QA)
    uses a natural prompting approach to present the question and answer options to
    the LLMs jointly, and have it output the symbol (*e.g.*, “A”) associated with
    its chosen answer option. It allows the model to explicitly compare answer options.
    In this setting, we aim to investigate compressed LLMs’ ability to understand
    natural language questions, effectively reason using knowledge remaining within
    them, and successfully associate the correct answer among the given answer options
    with the symbols that represent them; potentially minimizing the effect of tokenization
    and exact answer generation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 任务表述和理由。基于多项选择推理的问答（MCR-QA）采用自然提示的方法将问题和答案选项一起呈现给LLMs，并让它输出与其选择的答案选项相关的符号（*例如*，“A”）。它允许模型明确地比较答案选项。在这种设置下，我们的目标是研究压缩LLMs理解自然语言问题的能力，利用其内部剩余的知识进行有效推理，并成功将正确答案与表示它们的符号关联起来；从而有可能最小化分词和精确答案生成的影响。
- en: Dataset Details. We use the popular MMLU (Massive Multitask Language Understanding)
    benchmark which covers 50+ subjects across STEM, Humanities, Social Sciences,
    and more (Hendrycks et al., [2020](#bib.bib18)). It ranges in difficulty from
    an elementary level to an advanced professional level, and it tests both world
    knowledge and problem-solving ability of LLMs. The granularity and breadth of
    subjects make it ideal for fine-grained evaluation of compressed LLMs’ blind spots.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集详情。我们使用流行的 MMLU（大规模多任务语言理解）基准，它涵盖了50多个学科，涉及 STEM、人文学科、社会科学等（Hendrycks et
    al., [2020](#bib.bib18)）。该基准涵盖的难度从基础水平到高级专业水平，测试 LLM 的世界知识和问题解决能力。学科的细粒度和广泛性使其成为评估压缩
    LLM 盲点的理想选择。
- en: '![Refer to caption](img/d472dfc750b74ef6db41a19ac16bcb6b.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/d472dfc750b74ef6db41a19ac16bcb6b.png)'
- en: 'Figure 3: Compressed LLMs for Multiple-Choice Reasoning based QA. Performance
    comparison of compressed LLMs on MCR-QA tasks using the MMLU benchmark (Hendrycks
    et al., [2020](#bib.bib18)). Results (average across 3 independent runs) presented
    are for structured (N:M sparsity), unstructured sparsity, and quantization.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：用于多项选择推理的压缩 LLM 性能比较。使用 MMLU 基准（Hendrycks et al., [2020](#bib.bib18)）对压缩
    LLM 在 MCR-QA 任务中的性能进行比较。展示的结果（平均值，基于3次独立运行）包括结构化（N:M 稀疏度）、非结构化稀疏度和量化。
- en: 'Results and Analysis. The results of various LLM compression methods are demonstrated
    in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Setting 1: How Well Compressed LLMs Access
    Remaining Knowledge? ‣ 3 LLM-KICK: Unveiling True Merits of LLM Compression ‣
    Compressing LLMs: The Truth is Rarely Pure and Never Simple"). Our primary observations
    include: 1
    Despite a similar matching compression regime ($\sim$ 20-40%) to Factoid-QA, the
    abrupt performance drop of all SoTA pruning methods for MMLU is comparatively
    subtle due to relaxing the task setting from exact answer generation to correct
    answer selection. 2 No matching
    compressed LLMs are found for N:M structured sparsity. 3
    SoTA LLM quantization is seemingly more successful than SoTA pruning methods:
    we found 8-bit and 4-bit compressed LLM to be matching for Vicuna-7B and Vicuna-13B,
    respectively. 4
    Interestingly, both quantization and pruning have comparatively higher performance
    drop for Humanities and Social Science wrt. STEM, which indicates compression
    impacts some disciplines more than others. 5 Surprisingly,
    within the compression tolerance regime, simple one-shot magnitude pruning seems
    to perform quite well in comparison with SoTA pruning method, illustrating its
    high effectiveness.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '结果与分析。各种LLM压缩方法的结果在图[3](#S3.F3 "图 3 ‣ 3.1 设置 1: 压缩LLM如何获取剩余知识？ ‣ 3 LLM-KICK:
    揭示LLM压缩的真正优点 ‣ 压缩LLM: 事实真相往往不纯粹且复杂")中展示。我们的主要观察包括：1 尽管与Factoid-QA的压缩匹配率相似（约20-40%），所有SoTA修剪方法在MMLU上的性能急剧下降却相对微妙，这是由于将任务设置从生成准确答案放宽到选择正确答案。2
    没有找到匹配的压缩LLM用于N:M结构稀疏性。3 SoTA LLM量化似乎比SoTA修剪方法更成功：我们发现8-bit和4-bit压缩LLM分别与Vicuna-7B和Vicuna-13B匹配。4
    有趣的是，相较于STEM领域，人文与社会科学领域的量化和修剪性能下降较大，这表明压缩对某些学科的影响更大。5
    出人意料的是，在压缩容忍度范围内，相较于SoTA修剪方法，简单的一次性幅度修剪似乎表现得相当好，显示了其高效性。'
- en: '3.2 Setting 2: How Well Compressed LLMs Synthesize Augmented Knowledge?'
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 设置 2：压缩 LLMs 如何合成增强知识？
- en: 1
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 1
- en: In-context Retrieval Augmented Question Answering
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文检索增强问答
- en: 'Task Formulation and Rationale. In-context Retrieval-Augmented Question Answering
    (ICRA-QA) (Ram et al., [2023](#bib.bib56)) grounds the LLM answer generation by
    conditioning on relevant documents retrieved from an external knowledge source
    using retrieval algorithms like BM25\. Our ICRA-QA evaluation system includes
    two high-level components: a document selection,
    selecting the set of documents upon which to condition; and b
    document reading, determining how to incorporate the selected documents into the
    LLM answer process, which requires extracting correct answer phrases from conditioned
    documents. To discount the impact of the lost encoded knowledge during compression,
    ICRA-QA augments the required relevant knowledge for QA task directly within the
    prompt context. In this task setting, we aim to evaluate compressed LLMs’ ability
    to synthesize long in-context knowledge provided within input prompts, and locate
    and retrieve correct answers within it. We also present a head-to-head comparison
    of how augmented knowledge can work as a *remedy* to supplement the lost knowledge
    under compression.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 任务表述及理由。上下文检索增强问答（ICRA-QA）(Ram et al., [2023](#bib.bib56)) 通过基于从外部知识源中检索到的相关文档来生成
    LLM 答案，这些文档是通过类似 BM25 的检索算法获取的。我们的 ICRA-QA 评估系统包括两个高级组件：a
    文档选择，选择用于条件化的文档集；和 b 文档阅读，确定如何将选定的文档融入
    LLM 答案过程中，这要求从条件化文档中提取正确的答案短语。为了减轻压缩过程中丢失的编码知识的影响，ICRA-QA 直接在提示上下文中增强了 QA 任务所需的相关知识。在这个任务设置中，我们旨在评估压缩
    LLMs 合成输入提示中提供的长期上下文知识的能力，并在其中定位和检索正确的答案。我们还展示了增强知识如何作为 *补救措施* 来补充压缩过程中丢失的知识的正面比较。
- en: Dataset Details. We use TriviaQA (Joshi et al., [2017](#bib.bib27)) for evaluation,
    a popular reading comprehension dataset which includes 95K question-answer pairs
    authored by trivia enthusiasts and independently gathered evidence documents,
    six per question on average, that provide high-quality distant supervision for
    answering the questions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集详情。我们使用 TriviaQA (Joshi et al., [2017](#bib.bib27)) 进行评估，这是一种流行的阅读理解数据集，包含
    95K 个问题-答案对，由小测验爱好者编写，并独立收集证据文档，每个问题平均六个，这些文档为回答问题提供高质量的远程监督。
- en: '![Refer to caption](img/5ca0e214f6e1487efe21c84ee09f6d1c.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5ca0e214f6e1487efe21c84ee09f6d1c.png)'
- en: 'Figure 4: Compressed LLMs for In-context Retrieval Augmented QA. Performance
    comparison of compressed LLMs on ICRA-QA task. We present head-to-head comparison
    of closed-book evaluation (no external knowledge is augmented in-context) with
    open-book evaluation (external knowledge is augmented in-context). Results (average
    across 3 independent runs) presented are for structured N:M sparsity, unstructured
    sparsity, and quantization.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：用于上下文检索增强 QA 的压缩 LLMs。压缩 LLMs 在 ICRA-QA 任务中的性能比较。我们展示了封闭书籍评估（上下文中未增强外部知识）与开放书籍评估（上下文中增强了外部知识）的正面对比。结果（3
    次独立运行的平均值）显示了结构化 N:M 稀疏性、非结构化稀疏性和量化。
- en: 'Results and Analysis. The results of various LLM compression methods are demonstrated
    in Figure [4](#S3.F4 "Figure 4 ‣ 3.2 Setting 2: How Well Compressed LLMs Synthesize
    Augmented Knowledge? ‣ 3 LLM-KICK: Unveiling True Merits of LLM Compression ‣
    Compressing LLMs: The Truth is Rarely Pure and Never Simple"). The closed-book
    setting differs from ICRA-QA (*i.e.*, using the open-book setting) only in terms
    of whether conditioning on relevant documents retrieved from an external knowledge
    source. Our key findings are: 1 When compressed
    LLMs are conditioned on external knowledge (open book) and assigned the task of
    in-context retrievers, *i.e.*, extracting correct answer phrases from in-context
    knowledge, they perform significantly well even in extremely high compression
    regime. Vicuna-7B can remain matching till $\sim$50% sparsity and 4-bit quantization.
    Our experimental results send a positive signal that even if high compression
    leads to significant knowledge loss, it doesn’t leave LLMs completely useless,
    and they still work as robust in-context retrievers. 2
    Despite we observe a significant benefit while conditioning external knowledge,
    no matching compressed LLM can be identified for N:M sparsity. 3
    Again, we observe surprisingly good performance of simple one-shot unstructured
    magnitude pruning wrt. SparseGPT (second-order pruning) and Wanda (activation-based
    pruning) that rely on calibration data.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '结果与分析。各种LLM压缩方法的结果如图 [4](#S3.F4 "图 4 ‣ 3.2 设置 2: 压缩LLM如何合成增强知识？ ‣ 3 LLM-KICK:
    揭示LLM压缩的真正优点 ‣ 压缩LLM: 事实很少纯粹且从不简单") 所示。闭卷设置与ICRA-QA（*即*，使用开放书设置）唯一的区别在于是否依赖于从外部知识来源检索的相关文档。我们的主要发现是：1
    当压缩的LLM依赖外部知识（开放书）并被分配到上下文检索任务中，*即*，从上下文知识中提取正确的答案短语时，即使在极高压缩状态下也表现非常好。Vicuna-7B
    在 $\sim$50% 稀疏性和 4-bit 量化下仍能保持匹配。我们的实验结果发出积极信号，即使高压缩导致显著的知识损失，它也不会让LLM完全无用，它们仍然作为强大的上下文检索器发挥作用。2
    尽管我们观察到条件外部知识时显著的好处，但仍未能识别出匹配的压缩LLM用于N:M稀疏性。3 再次，我们观察到简单的一次性非结构化幅度修剪在性能上令人惊讶地优于SparseGPT（第二阶修剪）和Wanda（基于激活的修剪），这两者依赖于校准数据。'
- en: '![Refer to caption](img/5de5fbbcc95d0fa602ba88d9994831f8.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5de5fbbcc95d0fa602ba88d9994831f8.png)'
- en: 'Figure 5: Compressed LLMs for In-Context Summarization. Performance comparison
    of compressed Vicuna-7B for in-context summarization of small, medium, and large
    stories while preserving coherence, consistency, fluency, and relevance. Results
    (average across 3 independent runs) presented are for structured (2:4 sparsity
    - Row 3), unstructured sparsity, and quantization.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：用于上下文摘要的压缩 LLM。压缩 Vicuna-7B 在小型、中型和大型故事的上下文摘要性能比较，同时保持连贯性、一致性、流畅性和相关性。所展示的结果（平均值，基于
    3 次独立运行）适用于结构化（2:4 稀疏 - 第 3 行）、非结构化稀疏性和量化。
- en: '![Refer to caption](img/7c44ede73da19ddbb8c4256747ff3eef.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7c44ede73da19ddbb8c4256747ff3eef.png)'
- en: 'Figure 6: Compressed LLMs for Instruction Following. LLM-as-a-Judge: GPT-4
    based evaluation of compressed Vicuna-7B response wrt. ChatGPT (davici-003). (Left)
    unstructured sparsity; (middle) structured N:M sparsity; (c) comparison of average
    unique token counts generated by compressed Vicuna-7B for 80 prompts across 10
    different categories.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：用于指令跟随的压缩 LLM。LLM 作为评判者：基于 GPT-4 对压缩 Vicuna-7B 响应与 ChatGPT (davinci-003)
    的评估。（左）非结构化稀疏性；（中）结构化 N:M 稀疏性；（右）压缩 Vicuna-7B 为 80 个提示生成的平均唯一标记数量的比较，涵盖 10 个不同类别。
- en: 2
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 2
- en: In-Context Text Summarization
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文文本摘要
- en: 'Task Formulation and Details. Modern LLMs have shown astonishing success in
    summarizing long-context documents in both abstractive and extractive settings.
    However, it is yet not explored how compression impacts LLMs’ capability for summarization.
    In this task setting, we aim to investigate compressed LLMs’ ability to hold onto
    consistency, coherence, fluency, and relevance when prompted to summarize textual
    information of varying length (small, medium, and large) in abstractive setting
    (Jain et al., [2023](#bib.bib21)). For evaluation, similar to Zheng et al. ([2023](#bib.bib70)),
    we propose to use GPT-4 as a judge, which compares the compressed LLM generated
    summaries wrt. GPT-3.5 (text-davinci-003) generated summaries. Detailed evaluation
    settings can be found in Appendix [A.3](#A1.SS3 "A.3 In-Context Summarization
    Evaluation Settings ‣ Appendix A Appendix ‣ Compressing LLMs: The Truth is Rarely
    Pure and Never Simple").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '任务制定与细节。现代 LLM 在抽象和提取设置中对长上下文文档的摘要表现出了惊人的成功。然而，压缩如何影响 LLM 的摘要能力尚未被探索。在这个任务设置中，我们旨在调查压缩
    LLM 在被要求总结不同长度（小型、中型和大型）文本信息时，保持一致性、连贯性、流畅性和相关性的能力（Jain 等，[2023](#bib.bib21)）。为了评估，类似于
    Zheng 等（[2023](#bib.bib70)），我们建议使用 GPT-4 作为评判者，它比较压缩 LLM 生成的摘要与 GPT-3.5 (text-davinci-003)
    生成的摘要。详细的评估设置见附录 [A.3](#A1.SS3 "A.3 In-Context Summarization Evaluation Settings
    ‣ Appendix A Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple")。'
- en: Dataset Details. We use a popular summarization dataset CNN/DailyMail (Chen
    et al., [2016](#bib.bib2)) for evaluation, which is an English-language dataset
    containing just over 300k unique news articles written by journalists at CNN and
    DailyMail. We created 3 subset categories {small ($\leq$ 790 words)} of stories,
    each with 100 articles reflecting word distribution of CNN/DailyMail to minimize
    OpenAI API costs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集细节。我们使用了一个流行的摘要数据集 CNN/DailyMail (Chen 等，[2016](#bib.bib2)) 进行评估，该数据集为英文数据集，包含略超过
    30 万篇由 CNN 和 DailyMail 记者撰写的独特新闻文章。我们创建了 3 个子集类别 {小型 ($\leq$ 790 字)} 的故事，每个子集包含
    100 篇文章，反映 CNN/DailyMail 的词汇分布，以最小化 OpenAI API 成本。
- en: 'Results and Analysis. Results are summarized in Figure [5](#S3.F5 "Figure 5
    ‣ 3.2 Setting 2: How Well Compressed LLMs Synthesize Augmented Knowledge? ‣ 3
    LLM-KICK: Unveiling True Merits of LLM Compression ‣ Compressing LLMs: The Truth
    is Rarely Pure and Never Simple"). We summarize our main observations as: 1
    All pruning and quantization methods tend to perform surprisingly well for in-context
    summarization, preserving high consistency, coherence, fluency, and relevance
    in generated summaries, which is an encouraging observation in favor compression.
    2
    With increasing context length (*i.e.*, long stories), we observe a sharper performance
    drop for compressed LLMs, which highlights that compression impacts LLMs’ ability
    to synthesize and summarize longer context lengths. 3
    Quantization again seems to perform better than SoTA pruning methods, and surprisingly
    benefiting positively over the dense model performance. 4
    No matching compressed LLM can be identified for 2:4 structured sparsity.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与分析。结果总结在图[5](#S3.F5 "图 5 ‣ 3.2 设置 2：压缩的 LLMs 如何综合扩展知识？ ‣ 3 LLM-KICK：揭示 LLM
    压缩的真实优点 ‣ 压缩 LLMs：真相往往既不纯粹也不简单")中。我们总结了主要观察结果如下：1 所有修剪和量化方法在上下文总结方面表现得令人惊讶地好，生成的总结保持了高一致性、连贯性、流畅性和相关性，这一观察结果对压缩是一个令人鼓舞的信号。2
    随着上下文长度的增加（*即*，长篇故事），我们观察到压缩的 LLMs 性能显著下降，这突显了压缩对 LLMs 合成和总结较长上下文长度能力的影响。3
    量化似乎再次比现有最先进的修剪方法表现更好，并且对稠密模型性能有意外的积极影响。4 没有找到匹配的压缩 LLM
    适用于 2:4 结构稀疏性。
- en: '3.3 Setting 3: How Well Compressed LLMs Perform Instruction Following?'
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 设置 3：压缩的 LLMs 执行指令跟随的效果如何？
- en: Task Formulation and Rationale. In this task setting, we investigate compressed
    LLMs’ ability to answer open-ended questions and evaluate their multi-turn conversational
    and instruction-following ability – two critical elements for human preference.
    Evaluating AI chatbots is a challenging task, as it requires examining language
    understanding, reasoning, and context awareness. To compare the performance of
    compressed LLMs’ responses, we closely follow the prompt design setting in MT-Bench
    (Zheng et al., [2023](#bib.bib70)) using GPT-4 as a judge. We prompt GPT-4 to
    rate the answers generated by compressed LLMs wrt. GPT-3.5 (text-davinci-003)
    model based on varying metrics (*e.g.*, correctness, helpfulness, logic, accuracy,
    *etc.*) on a scale of [0-10] with detailed explanations.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 任务制定与理由。在这个任务设置中，我们研究了压缩 LLMs（大语言模型）回答开放性问题的能力，并评估其多轮对话和指令跟随能力——这是影响人类偏好的两个关键因素。评估
    AI 聊天机器人是一项具有挑战性的任务，因为它需要检验语言理解、推理和上下文意识。为了比较压缩 LLMs 的响应性能，我们密切跟随 MT-Bench (Zheng
    et al., [2023](#bib.bib70)) 中的提示设计设置，使用 GPT-4 作为评审。我们提示 GPT-4 根据不同的指标（*例如*，正确性、帮助性、逻辑性、准确性、*等*）对压缩
    LLMs 生成的答案与 GPT-3.5（text-davinci-003）模型进行评分，评分范围为 [0-10]，并附上详细解释。
- en: 'Dataset Details. We rely on the 80 high quality multi-turn questions identified
    in MT-Bench (Zheng et al., [2023](#bib.bib70)). This setting covers common-use
    human-centric interaction with LLMs, and focuses on challenging questions to differentiate
    models. We used 8 common categories of user prompts to guide the prompt construction
    to interact with compressed LLMs: writing, roleplay, extraction, reasoning, math,
    coding, *etc*. For each category, we adopted manually designed 10 multi-turn questions
    from MT-Bench to evaluate our compressed models. Details can be found in Appendix
    [A.4](#A1.SS4 "A.4 Instruction Following Ability Evaluation Setting ‣ Appendix
    A Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集详情。我们依赖于 MT-Bench (Zheng et al., [2023](#bib.bib70)) 中识别出的 80 个高质量多轮问题。这个设置涵盖了与
    LLMs 的常见人机交互，并专注于具有挑战性的问题以区分模型。我们使用了 8 种常见的用户提示类别来指导提示构建，以便与压缩 LLMs 进行互动：写作、角色扮演、提取、推理、数学、编码、*等*。对于每个类别，我们从
    MT-Bench 中采纳了手动设计的 10 个多轮问题来评估我们的压缩模型。详细信息请参见附录 [A.4](#A1.SS4 "A.4 Instruction
    Following Ability Evaluation Setting ‣ Appendix A Appendix ‣ Compressing LLMs:
    The Truth is Rarely Pure and Never Simple")。'
- en: 'Results and Analysis. Results are summarized in Figure [6](#S3.F6 "Figure 6
    ‣ 3.2 Setting 2: How Well Compressed LLMs Synthesize Augmented Knowledge? ‣ 3
    LLM-KICK: Unveiling True Merits of LLM Compression ‣ Compressing LLMs: The Truth
    is Rarely Pure and Never Simple"). Our primary observations are: 1
    Unlike in-context text summarization, in this task setting, compressed LLMs have
    to access the knowledge to respond to conversations maintaining high helpfulness,
    relevance, accuracy, and detail. We again observe that compressed LLMs with various
    pruning methods are matching only up to sparsity ratio of $\sim$ 25%. 2
    Surprisingly, in the matching regime, the simple baseline of one-shot magnitude
    pruning performs comparable or slightly better than SoTA pruning methods. 3
    No matching subnetwork can be identified for N:M sparsity. 4
    Interestingly, our average generated unique token analysis in Figure [6](#S3.F6
    "Figure 6 ‣ 3.2 Setting 2: How Well Compressed LLMs Synthesize Augmented Knowledge?
    ‣ 3 LLM-KICK: Unveiling True Merits of LLM Compression ‣ Compressing LLMs: The
    Truth is Rarely Pure and Never Simple")(c) illustrates that compressed LLMs lose
    the ability to generate distinct unique content, instead, they can only produce
    more repetitive texts.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '结果与分析。结果汇总在图 [6](#S3.F6 "图 6 ‣ 3.2 设置 2: 压缩 LLM 如何综合增强知识？ ‣ 3 LLM-KICK: 揭示
    LLM 压缩的真实优点 ‣ 压缩 LLM: 真相既罕见又复杂") 中。我们的主要观察结果是：1 与上下文文本总结不同，在此任务设置中，压缩的
    LLM 必须访问知识以回应对话，同时保持高度的帮助性、相关性、准确性和细节。我们再次观察到，使用各种剪枝方法的压缩 LLM 只能匹配高达约 25% 的稀疏率。2
    令人惊讶的是，在匹配机制中，一次性幅度剪枝的简单基线表现与 SoTA 剪枝方法相当，甚至略好。3 没有识别出适用于 N:M
    稀疏的匹配子网络。4
    有趣的是，我们在图 [6](#S3.F6 "图 6 ‣ 3.2 设置 2: 压缩 LLM 如何综合增强知识？ ‣ 3 LLM-KICK: 揭示 LLM 压缩的真实优点
    ‣ 压缩 LLM: 真相既罕见又复杂")(c) 中的平均生成独特令牌分析显示，压缩的 LLM 失去了生成独特内容的能力，而只能产生更多重复的文本。'
- en: 4 Additional Results and Discussions
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 个额外的结果和讨论
- en: 'Small-Dense vs. Large-Sparse: which is favorable?'
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Small-Dense 与 Large-Sparse：哪种更有利？
- en: 'We attempt to understand an interesting question: if pruned LLMs with larger
    architecture (Large-Sparse) is better than smaller dense models with similar parameter
    count (Small-Dense)? Pruning large LLMs doesn’t come for free, and it is important
    to investigate if the cost of pruning can be reflected in the performance benefit
    of Large-Sparse models. To our surprise, in comparison with dense Vicuna-7B (MMLU
    accuracy 46.7%), we found compressed Vicuna-13B with exactly similar parameter
    count (46.16% sparsity) of 7 billion using one-shot magnitude, Wanda, SparseGPT
    can only achieve MMLU accuracy of 31.7%, 45.3%, and 46.3%, respectively. This
    is a clear indication that current sparsity algorithms are not yet up to a stage
    where the cost of pruning can be justified by performance benefits obtained from
    large-sparse compressed models.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试理解一个有趣的问题：修剪后的 LLMs（Large-Sparse）是否比具有相似参数数量的较小密集模型（Small-Dense）更好？修剪大型
    LLMs 并非没有代价，因此调查修剪的成本是否能在 Large-Sparse 模型的性能收益中体现出来是很重要的。令我们惊讶的是，与密集的 Vicuna-7B（MMLU
    准确率 46.7%）相比，我们发现压缩后的 Vicuna-13B 具有完全相同的参数数量（46.16% 稀疏性），使用单次修剪，Wanda 和 SparseGPT
    分别只能达到 MMLU 准确率 31.7%、45.3% 和 46.3%。这清楚地表明，目前的稀疏性算法还未达到可以用大型稀疏压缩模型获得的性能收益来证明修剪成本的阶段。
- en: '![Refer to caption](img/c2b84305da5d666b8cb9eeab9984758e.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c2b84305da5d666b8cb9eeab9984758e.png)'
- en: 'Figure 7: Zero-shot performance of 50% & 70% pruned Vicuna-7B wrt. calibration
    sample counts.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：50% 和 70% 修剪的 Vicuna-7B 相对于校准样本数量的零-shot 性能。
- en: How many calibration data samples are needed?
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 需要多少个校准数据样本？
- en: 'We attempt to analyze how calibration dependent pruning methods (Wanda and
    SparseGPT) perform with varying amount of calibration samples. Figure [7](#S4.F7
    "Figure 7 ‣ Small-Dense vs. Large-Sparse: which is favorable? ‣ 4 Additional Results
    and Discussions ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple")
    illustrates the zero-shot performance of 50% & 70% pruned Vicuna-7B using Wanda
    and SparseGPT on knowledge-intensive MMLU benchmark. It is interesting to observe
    that calibration sample count plays a vital role in preserving the performance
    of SparseGPT unlike Wanda. Note that at high sparsity ratio (70%), Wanda cannot
    recover any performance; SparseGPT surprisingly benefits noticeably from calibration.
    This suggests that carefully selected calibration samples can play a vital role
    in designing better pruning algorithms to compress LLMs even up to significantly
    high sparsity.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '我们尝试分析不同数量的校准样本下，校准依赖性修剪方法（Wanda 和 SparseGPT）的表现。图[7](#S4.F7 "Figure 7 ‣ Small-Dense
    vs. Large-Sparse: which is favorable? ‣ 4 Additional Results and Discussions ‣
    Compressing LLMs: The Truth is Rarely Pure and Never Simple")展示了使用 Wanda 和 SparseGPT
    在知识密集型 MMLU 基准上对 50% 和 70% 修剪的 Vicuna-7B 的零-shot 性能。值得注意的是，校准样本数量在保持 SparseGPT
    性能方面发挥了关键作用，而 Wanda 则不然。请注意，在高稀疏比（70%）下，Wanda 无法恢复任何性能；SparseGPT 令人惊讶地从校准中受益明显。这表明，精心挑选的校准样本在设计更好的修剪算法以压缩
    LLMs 方面可能发挥关键作用，甚至在显著高的稀疏率下也是如此。'
- en: '![Refer to caption](img/270551322b71d0a26249d2e9131f88e8.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/270551322b71d0a26249d2e9131f88e8.png)'
- en: 'Figure 8: k-shot results of Vicuna-7B pruned with Wanda.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：使用 Wanda 修剪的 Vicuna-7B 的 k-shot 结果。
- en: Does k-shots benefit compressed LLMs?
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: k-shot 对压缩 LLMs 有益吗？
- en: 'In this section, we aim to investigate how few-shot in-context learning examples
    can benefit SoTA pruning methods to preserve performance across various sparsity
    levels. Figure [8](#S4.F8 "Figure 8 ‣ How many calibration data samples are needed?
    ‣ 4 Additional Results and Discussions ‣ Compressing LLMs: The Truth is Rarely
    Pure and Never Simple") illustrates the performance comparison of Vicuna-7B at
    varying sparsity ratios when augmented with k-shot in-context examples on MMLU
    benchmark. It is interesting to observe that k-shot in-context learning examples
    have marginal impact on dense network performance, while they significantly help
    in preserving the performance at high sparsity. Moreover, we found 2-3 examples
    are sufficient to retain the performance, and supplementing additional examples
    doesn’t necessarily provide further noticeable benefits.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们旨在研究少样本上下文学习示例如何使最先进的剪枝方法受益，以在各种稀疏性水平下保持性能。图[8](#S4.F8 "图 8 ‣ 需要多少校准数据样本？
    ‣ 4 个附加结果和讨论 ‣ 压缩 LLM：事实很少纯粹且从不简单")展示了在 MMLU 基准测试中，通过 k-shot 上下文示例增强的 Vicuna-7B
    在不同稀疏比下的性能比较。值得注意的是，k-shot 上下文学习示例对密集网络性能的影响微乎其微，而在高稀疏性下却显著有助于保持性能。此外，我们发现 2-3
    个示例足以保持性能，补充额外的示例并不一定带来更显著的好处。
- en: 5 Conclusion and Limitations
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与局限性
- en: In this paper, we propose to explore the effectiveness of SoTA compression methods
    beyond perplexity to address the inability of perplexity to capture the subtle
    variations incurred during the derivation of compressed LLMs from their dense
    counterparts. Our work introduces Knowledge-Intensive Compressed LLM BenchmarK
    (LLM-KICK) to facilitate a fair and holistic evaluation by unveiling many merits
    and pitfalls of SoTA compression methods. Our study reveals that compression significantly
    impacts the knowledge encoded in LLMs during pre-training, compressed LLMs perform
    quite well with knowledge augmented in-context settings. We primarily restrict
    our evaluation to Vicuna (decoder-only architecture) due to its open-source license,
    high performance, and instruction-following ability. For future work, we aim to
    investigate how the lost knowledge due to compression can be recovered using parameter-efficient
    fine-tuning methods, *e.g.*, LoRA (Hu et al., [2021](#bib.bib19)) and QLoRA (Dettmers
    et al., [2023b](#bib.bib8)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提议探索最先进的压缩方法在超越困惑度方面的有效性，以解决困惑度无法捕捉从密集模型到压缩 LLM 过程中产生的细微变化的问题。我们的工作引入了知识密集型压缩
    LLM 基准测试（LLM-KICK），以通过揭示最先进压缩方法的许多优点和陷阱来促进公平和全面的评估。我们的研究揭示了压缩对 LLM 在预训练期间编码的知识有显著影响，压缩后的
    LLM 在知识增强的上下文设置中表现相当好。由于 Vicuna（仅解码器架构）的开源许可证、高性能和指令跟随能力，我们主要将评估限制在 Vicuna 上。未来的工作中，我们旨在研究如何利用参数高效微调方法，如
    LoRA (Hu et al., [2021](#bib.bib19)) 和 QLoRA (Dettmers et al., [2023b](#bib.bib8))，恢复由于压缩而丢失的知识。
- en: References
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Cardinaux et al. (2020) Fabien Cardinaux, Stefan Uhlich, Kazuki Yoshiyama, Javier Alonso
    García, Lukas Mauch, Stephen Tiedemann, Thomas Kemp, and Akira Nakamura. Iteratively
    training look-up tables for network quantization. *IEEE Journal of Selected Topics
    in Signal Processing*, 14(4):860–870, 2020.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cardinaux et al. (2020) Fabien Cardinaux, Stefan Uhlich, Kazuki Yoshiyama, Javier
    Alonso García, Lukas Mauch, Stephen Tiedemann, Thomas Kemp, 和 Akira Nakamura.
    迭代训练查找表以进行网络量化。*IEEE 选定信号处理期刊*, 14(4):860–870, 2020.
- en: Chen et al. (2016) Danqi Chen, Jason Bolton, and Christopher D Manning. A thorough
    examination of the cnn/daily mail reading comprehension task. *arXiv preprint
    arXiv:1606.02858*, 2016.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2016) Danqi Chen, Jason Bolton, 和 Christopher D Manning. 对 cnn/daily
    mail 阅读理解任务的彻底检查。*arXiv 预印本 arXiv:1606.02858*, 2016.
- en: Chen et al. (2020) Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,
    Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis
    for pre-trained bert networks. *Advances in neural information processing systems*,
    33:15834–15846, 2020.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2020) Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,
    Yang Zhang, Zhangyang Wang, 和 Michael Carbin. 预训练 bert 网络的中彩票假设。*神经信息处理系统进展*,
    33:15834–15846, 2020.
- en: Chen et al. (2023) Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi
    Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. Exploring the potential
    of large language models (llms) in learning on graphs. *arXiv preprint arXiv:2307.03393*,
    2023.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi
    Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, 等. 探索大型语言模型 (LLMs) 在图学习中的潜力。*arXiv
    预印本 arXiv:2307.03393*, 2023.
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4
    with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, 和 Eric P. Xing. Vicuna: 一个开源聊天机器人，令人印象深刻的 gpt-4 达到 90%* chatgpt 质量，2023
    年 3 月。网址 [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)。'
- en: 'Dettmers & Zettlemoyer (2019) Tim Dettmers and Luke Zettlemoyer. Sparse networks
    from scratch: Faster training without losing performance. *arXiv preprint arXiv:1907.04840*,
    2019.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers & Zettlemoyer (2019) Tim Dettmers 和 Luke Zettlemoyer. 从头开始的稀疏网络: 更快的训练而不损失性能。*arXiv
    预印本 arXiv:1907.04840*, 2019。'
- en: 'Dettmers et al. (2023a) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *ArXiv*, abs/2305.14314,
    2023a. URL [https://api.semanticscholar.org/CorpusID:258841328](https://api.semanticscholar.org/CorpusID:258841328).'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2023a) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke
    Zettlemoyer. Qlora: 高效的量化 LLM 微调。*ArXiv*, abs/2305.14314, 2023a。网址 [https://api.semanticscholar.org/CorpusID:258841328](https://api.semanticscholar.org/CorpusID:258841328)。'
- en: 'Dettmers et al. (2023b) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023b.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2023b) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke
    Zettlemoyer. Qlora: 高效的量化 LLM 微调。*arXiv 预印本 arXiv:2305.14314*, 2023b。'
- en: 'Dettmers et al. (2023c) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm
    weight compression. *ArXiv*, abs/2306.03078, 2023c. URL [https://api.semanticscholar.org/CorpusID:259076379](https://api.semanticscholar.org/CorpusID:259076379).'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2023c) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    和 Dan Alistarh. Spqr: 用于近乎无损 LLM 权重压缩的稀疏量化表示。*ArXiv*, abs/2306.03078, 2023c。网址
    [https://api.semanticscholar.org/CorpusID:259076379](https://api.semanticscholar.org/CorpusID:259076379)。'
- en: Dong et al. (2022) Runpei Dong, Zhanhong Tan, Mengdi Wu, Linfeng Zhang, and
    Kaisheng Ma. Finding the task-optimal low-bit sub-distribution in deep neural
    networks. In *International Conference on Machine Learning*, pp. 5343–5359\. PMLR,
    2022.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2022) Runpei Dong, Zhanhong Tan, Mengdi Wu, Linfeng Zhang, 和 Kaisheng
    Ma. 在深度神经网络中找到任务最优低比特子分布。在 *国际机器学习会议* 上，pp. 5343–5359\. PMLR, 2022。
- en: 'Duan et al. (2023) Keyu Duan, Qian Liu, Tat-Seng Chua, Shuicheng Yan, Wei Tsang
    Ooi, Qizhe Xie, and Junxian He. Simteg: A frustratingly simple approach improves
    textual graph learning. *arXiv preprint arXiv:2308.02565*, 2023.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Duan et al. (2023) Keyu Duan, Qian Liu, Tat-Seng Chua, Shuicheng Yan, Wei Tsang
    Ooi, Qizhe Xie, 和 Junxian He. Simteg: 一个令人沮丧的简单方法改进文本图学习。*arXiv 预印本 arXiv:2308.02565*,
    2023。'
- en: 'Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro,
    and Erich Elsen. Rigging the lottery: Making all tickets winners. In *International
    Conference on Machine Learning*, pp. 2943–2952\. PMLR, 2020.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro,
    和 Erich Elsen. 操控彩票: 让所有票据都成为赢家。在 *国际机器学习会议* 上，pp. 2943–2952\. PMLR, 2020。'
- en: 'Frantar & Alistarh (2023) Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot, 2023.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar & Alistarh (2023) Elias Frantar 和 Dan Alistarh. Sparsegpt: 大规模语言模型可以在一次性修剪中准确完成，2023。'
- en: 'Frantar et al. (2021) Elias Frantar, Eldar Kurtic, and Dan Alistarh. M-fac:
    Efficient matrix-free approximations of second-order information. *Advances in
    Neural Information Processing Systems*, 34:14873–14886, 2021.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. (2021) Elias Frantar, Eldar Kurtic, 和 Dan Alistarh. M-fac: 高效的矩阵自由二阶信息近似。*神经信息处理系统进展*,
    34:14873–14886, 2021。'
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *ArXiv*, abs/2210.17323, 2022. URL [https://api.semanticscholar.org/CorpusID:253237200](https://api.semanticscholar.org/CorpusID:253237200).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan
    Alistarh. Gptq: 用于生成预训练变换器的准确后训练量化。*ArXiv*, abs/2210.17323, 2022。网址 [https://api.semanticscholar.org/CorpusID:253237200](https://api.semanticscholar.org/CorpusID:253237200)。'
- en: Gale et al. (2019) Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity
    in deep neural networks. *arXiv preprint arXiv:1902.09574*, 2019.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gale et al. (2019) Trevor Gale, Erich Elsen, 和 Sara Hooker. 深度神经网络中的稀疏性现状。*arXiv
    预印本 arXiv:1902.09574*, 2019。
- en: 'Han et al. (2016) Song Han, Huizi Mao, and William J Dally. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. In *International Conference on Learning Representations*, 2016.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等（2016）Song Han, Huizi Mao 和 William J Dally. 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。发表于
    *国际学习表示会议*，2016。
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *arXiv preprint arXiv:2009.03300*, 2020.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等（2020）Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song 和 Jacob Steinhardt. 测量大规模多任务语言理解。*arXiv 预印本 arXiv:2009.03300*，2020。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2021）Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang 和 Weizhu Chen. Lora：大型语言模型的低秩适配。*arXiv 预印本 arXiv:2106.09685*，2021。
- en: Iyyer et al. (2014) Mohit Iyyer, Jordan L. Boyd-Graber, Leonardo Max Batista
    Claudino, Richard Socher, and Hal Daumé. A neural network for factoid question
    answering over paragraphs. In *Conference on Empirical Methods in Natural Language
    Processing*, 2014. URL [https://api.semanticscholar.org/CorpusID:216034672](https://api.semanticscholar.org/CorpusID:216034672).
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iyyer 等（2014）Mohit Iyyer, Jordan L. Boyd-Graber, Leonardo Max Batista Claudino,
    Richard Socher 和 Hal Daumé. 用于段落事实问答的神经网络。发表于 *自然语言处理经验方法会议*，2014。网址 [https://api.semanticscholar.org/CorpusID:216034672](https://api.semanticscholar.org/CorpusID:216034672)。
- en: Jain et al. (2023) Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra,
    Patrick Fernandes, Pengfei Liu, Graham Neubig, and Chunting Zhou. Multi-dimensional
    evaluation of text summarization with in-context learning. *arXiv preprint arXiv:2306.01200*,
    2023.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等（2023）Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick
    Fernandes, Pengfei Liu, Graham Neubig 和 Chunting Zhou. 文本摘要的多维评估与上下文学习。*arXiv
    预印本 arXiv:2306.01200*，2023。
- en: 'Jaiswal et al. (2023a) Ajay Jaiswal, Shiwei Liu, Tianlong Chen, and Zhangyang
    Wang. The emergence of essential sparsity in large pre-trained models: The weights
    that matter. *arXiv preprint arXiv:2306.03805*, 2023a.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaiswal 等（2023a）Ajay Jaiswal, Shiwei Liu, Tianlong Chen 和 Zhangyang Wang. 大型预训练模型中关键稀疏性的出现：重要的权重。*arXiv
    预印本 arXiv:2306.03805*，2023a。
- en: Jaiswal et al. (2022) Ajay Kumar Jaiswal, Haoyu Ma, Tianlong Chen, Ying Ding,
    and Zhangyang Wang. Training your sparse neural network better with any mask.
    In *International Conference on Machine Learning*, pp. 9833–9844\. PMLR, 2022.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaiswal 等（2022）Ajay Kumar Jaiswal, Haoyu Ma, Tianlong Chen, Ying Ding 和 Zhangyang
    Wang. 使用任意掩码更好地训练稀疏神经网络。发表于 *国际机器学习会议*，第9833–9844页。PMLR，2022。
- en: 'Jaiswal et al. (2023b) Ajay Kumar Jaiswal, Shiwei Liu, Tianlong Chen, Ying
    Ding, and Zhangyang Wang. Instant soup: Cheap pruning ensembles in a single pass
    can draw lottery tickets from large models. In *International Conference on Machine
    Learning*, pp. 14691–14701\. PMLR, 2023b.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaiswal 等（2023b）Ajay Kumar Jaiswal, Shiwei Liu, Tianlong Chen, Ying Ding 和 Zhangyang
    Wang. Instant soup：便宜的剪枝集成在单次迭代中可以从大型模型中提取彩票票据。发表于 *国际机器学习会议*，第14691–14701页。PMLR，2023b。
- en: Ji et al. (2023) Yupeng Ji, Yibo Cao, and Jiucai Liu. Pruning large language
    models via accuracy predictor. *arXiv preprint arXiv:2309.09507*, 2023.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 等（2023）Yupeng Ji, Yibo Cao 和 Jiucai Liu. 通过准确性预测器修剪大型语言模型。*arXiv 预印本 arXiv:2309.09507*，2023。
- en: 'Jiang et al. (2019) Kelvin Jiang, Dekun Wu, and Hui Jiang. Freebaseqa: A new
    factoid qa data set matching trivia-style question-answer pairs with freebase.
    In *North American Chapter of the Association for Computational Linguistics*,
    2019. URL [https://api.semanticscholar.org/CorpusID:174800890](https://api.semanticscholar.org/CorpusID:174800890).'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2019）Kelvin Jiang, Dekun Wu 和 Hui Jiang. Freebaseqa：一个新的事实问答数据集，将Trivia风格的问题-答案对与Freebase匹配。发表于
    *北美计算语言学协会年会*，2019。网址 [https://api.semanticscholar.org/CorpusID:174800890](https://api.semanticscholar.org/CorpusID:174800890)。
- en: 'Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.
    Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
    *arXiv preprint arXiv:1705.03551*, 2017.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi 等（2017）Mandar Joshi, Eunsol Choi, Daniel S Weld 和 Luke Zettlemoyer. Triviaqa：一个用于阅读理解的大规模远程监督挑战数据集。*arXiv
    预印本 arXiv:1705.03551*，2017。
- en: Kaddour et al. (2023) Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie
    Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of
    large language models. *arXiv preprint arXiv:2307.10169*, 2023.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaddour 等（2023）Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley,
    Roberta Raileanu 和 Robert McHardy. 大型语言模型的挑战与应用。*arXiv 预印本 arXiv:2307.10169*，2023。
- en: Kim et al. (2023) Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park,
    Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee. Memory-efficient fine-tuning of compressed
    large language models via sub-4-bit integer quantization. *ArXiv*, abs/2305.14152,
    2023. URL [https://api.semanticscholar.org/CorpusID:258841104](https://api.semanticscholar.org/CorpusID:258841104).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim等（2023）Jeonghoon Kim、Jung Hyun Lee、Sungdong Kim、Joonsuk Park、Kang Min Yoo、Se
    Jung Kwon 和 Dongsoo Lee。通过子4位整数量化进行压缩大型语言模型的内存高效微调。*ArXiv*，abs/2305.14152，2023年。网址
    [https://api.semanticscholar.org/CorpusID:258841104](https://api.semanticscholar.org/CorpusID:258841104)。
- en: 'Kim et al. (2021) Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney,
    and Kurt Keutzer. I-bert: Integer-only bert quantization. In *International conference
    on machine learning*, pp. 5506–5518\. PMLR, 2021.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim等（2021）Sehoon Kim、Amir Gholami、Zhewei Yao、Michael W Mahoney 和 Kurt Keutzer。I-bert：仅整数的BERT量化。在*国际机器学习会议*，第5506–5518页。PMLR，2021年。
- en: 'Kurtic et al. (2022) Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar,
    Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal bert
    surgeon: Scalable and accurate second-order pruning for large language models.
    *arXiv preprint arXiv:2203.07259*, 2022.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurtic等（2022）Eldar Kurtic、Daniel Campos、Tuan Nguyen、Elias Frantar、Mark Kurtz、Benjamin
    Fineran、Michael Goin 和 Dan Alistarh。最优BERT外科医生：大型语言模型的可扩展和准确的二阶修剪。*arXiv 预印本 arXiv:2203.07259*，2022年。
- en: Lagunas et al. (2021) François Lagunas, Ella Charlaix, Victor Sanh, and Alexander M
    Rush. Block pruning for faster transformers. *arXiv preprint arXiv:2109.04838*,
    2021.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagunas等（2021）François Lagunas、Ella Charlaix、Victor Sanh 和 Alexander M Rush。块状修剪以加快变换器速度。*arXiv
    预印本 arXiv:2109.04838*，2021年。
- en: 'Lai et al. (2023) Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan,
    Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model.
    *arXiv preprint arXiv:2308.00692*, 2023.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai等（2023）Xin Lai、Zhuotao Tian、Yukang Chen、Yanwei Li、Yuhui Yuan、Shu Liu 和 Jiaya
    Jia。LISA：通过大型语言模型进行推理分割。*arXiv 预印本 arXiv:2308.00692*，2023年。
- en: LeCun et al. (1990) Yann LeCun, John S Denker, and Sara A Solla. Optimal brain
    damage. In *Advances in neural information processing systems*, pp. 598–605, 1990.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun等（1990）Yann LeCun、John S Denker 和 Sara A Solla。最优大脑损伤。在*神经信息处理系统进展*，第598–605页，1990年。
- en: Lee et al. (2023) Noah Lee, Na Min An, and James Thorne. Can large language
    models infer and disagree like humans? *ArXiv*, abs/2305.13788, 2023. URL [https://api.semanticscholar.org/CorpusID:258841424](https://api.semanticscholar.org/CorpusID:258841424).
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee等（2023）Noah Lee、Na Min An 和 James Thorne。大型语言模型能否像人类一样推理和争论？*ArXiv*，abs/2305.13788，2023年。网址
    [https://api.semanticscholar.org/CorpusID:258841424](https://api.semanticscholar.org/CorpusID:258841424)。
- en: 'Li et al. (2023) Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng,
    Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, et al. Flm-101b: An open llm and
    how to train it with 100 k budget. *arXiv preprint arXiv:2309.03852*, 2023.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2023）Xiang Li、Yiqun Yao、Xin Jiang、Xuezhi Fang、Xuying Meng、Siqi Fan、Peng
    Han、Jing Li、Li Du、Bowen Qin 等。FLM-101B：一个开放的LLM及其如何在10万预算下进行训练。*arXiv 预印本 arXiv:2309.03852*，2023年。
- en: 'Li et al. (2022) Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh
    Rawat, Sashank J Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, et al. Large
    models are parsimonious learners: Activation sparsity in trained transformers.
    *arXiv preprint arXiv:2210.06313*, 2022.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2022）Zonglin Li、Chong You、Srinadh Bhojanapalli、Daliang Li、Ankit Singh Rawat、Sashank
    J Reddi、Ke Ye、Felix Chern、Felix Yu、Ruiqi Guo 等。大型模型是节俭的学习者：训练变换器中的激活稀疏性。*arXiv
    预印本 arXiv:2210.06313*，2022年。
- en: 'Lian et al. (2023) Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded
    diffusion: Enhancing prompt understanding of text-to-image diffusion models with
    large language models. *arXiv preprint arXiv:2305.13655*, 2023.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lian等（2023）Long Lian、Boyi Li、Adam Yala 和 Trevor Darrell。LLM-驱动的扩散：利用大型语言模型增强文本到图像扩散模型的提示理解。*arXiv
    预印本 arXiv:2305.13655*，2023年。
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *ArXiv*, abs/2306.00978, 2023. URL [https://api.semanticscholar.org/CorpusID:258999941](https://api.semanticscholar.org/CorpusID:258999941).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2023）Ji Lin、Jiaming Tang、Haotian Tang、Shang Yang、Xingyu Dang 和 Song Han。AWQ：激活感知权重量化用于LLM压缩和加速。*ArXiv*，abs/2306.00978，2023年。网址
    [https://api.semanticscholar.org/CorpusID:258999941](https://api.semanticscholar.org/CorpusID:258999941)。
- en: Lin et al. (2020) Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev,
    and Martin Jaggi. Dynamic model pruning with feedback. In *International Conference
    on Learning Representations*, 2020. URL [https://openreview.net/forum?id=SJem8lSFwB](https://openreview.net/forum?id=SJem8lSFwB).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2020）Tao Lin、Sebastian U. Stich、Luis Barba、Daniil Dmitriev 和 Martin Jaggi。动态模型修剪与反馈。在*国际学习表征会议*，2020年。网址
    [https://openreview.net/forum?id=SJem8lSFwB](https://openreview.net/forum?id=SJem8lSFwB)。
- en: 'Liu et al. (2023a) Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong,
    Kang Zhou, Yueqi Xie, Yuwei Cao, Shoujin Wang, Chenyu You, et al. Llmrec: Benchmarking
    large language models on recommendation task. *arXiv preprint arXiv:2308.12241*,
    2023a.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2023a）刘君岭、刘超、周培林、叶奇辰、崇大定、周康、谢月琪、曹宇伟、王守金、尤晨瑜等。LLMREC: 大型语言模型推荐任务的基准测试。*arXiv
    预印本 arXiv:2308.12241*，2023年。'
- en: Liu et al. (2021a) Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi,
    Lu Yin, Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, and Decebal Constantin
    Mocanu. Sparse training via boosting pruning plasticity with neuroregeneration.
    *Advances in Neural Information Processing Systems (NeurIPs).*, 2021a.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2021a）刘世伟、陈天龙、陈晓寒、扎赫拉·阿塔什加赫、刘银、寇焕宇、沈莉、米科拉·佩切尼兹基和王张扬。通过增强剪枝可塑性与神经再生的稀疏训练。*神经信息处理系统进展（NeurIPS）*，2021年。
- en: 'Liu et al. (2023b) Shiwei Liu, Tianlong Chen, Zhenyu Zhang, Xuxi Chen, Tianjin
    Huang, Ajay Jaiswal, and Zhangyang Wang. Sparsity may cry: Let us fail (current)
    sparse neural networks together! *arXiv preprint arXiv:2303.02141*, 2023b.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2023b）刘世伟、陈天龙、张振宇、陈旭西、黄天锦、阿杰·贾斯瓦尔和王张扬。稀疏性可能会哭泣: 一起让（当前）稀疏神经网络失败吧！*arXiv
    预印本 arXiv:2303.02141*，2023年。'
- en: 'Liu et al. (2023c) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models. *arXiv
    preprint arXiv:2305.17888*, 2023c.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2023c）刘泽春、巴尔拉斯·奥古兹、赵长生、厄尼·张、皮埃尔·斯托克、亚沙·梅赫达、石杨扬、拉古拉曼·克里希纳穆尔蒂和维卡斯·昌德拉。LLM-QAT:
    无数据量化感知训练用于大型语言模型。*arXiv 预印本 arXiv:2305.17888*，2023年。'
- en: Liu et al. (2021b) Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and
    Wen Gao. Post-training quantization for vision transformer. In A. Beygelzimer,
    Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), *Advances in Neural Information
    Processing Systems*, 2021b. URL [https://openreview.net/forum?id=9TX5OsKJvm](https://openreview.net/forum?id=9TX5OsKJvm).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2021b）刘振华、王云赫、韩凯、张伟、马思伟和高文。视觉变换器的后训练量化。由A. 贝格尔齐默、Y. 多芬、P. 梁和J. 沃特曼·沃恩（编）出版，*神经信息处理系统进展*，2021年。网址
    [https://openreview.net/forum?id=9TX5OsKJvm](https://openreview.net/forum?id=9TX5OsKJvm)。
- en: 'Lu et al. (2023) Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang,
    Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional
    reasoning with large language models. *arXiv preprint arXiv:2304.09842*, 2023.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu 等（2023）潘璐、彭保林、程浩、米歇尔·加利、常凯维、吴英年、朱松春和高剑锋。Chameleon: 使用大型语言模型的即插即用组合推理。*arXiv
    预印本 arXiv:2304.09842*，2023年。'
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. *arXiv preprint arXiv:2305.11627*,
    2023.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等（2023）马欣银、方功凡和王新超。LLM-PRUNER: 关于大型语言模型的结构剪枝。*arXiv 预印本 arXiv:2305.11627*，2023年。'
- en: 'Martinez et al. (2020) Julieta Martinez, Jashan Shewakramani, Ting Liu, Ioan Andrei
    Bârsan, Wenyuan Zeng, and Raquel Urtasun. Permute, quantize, and fine-tune: Efficient
    compression of neural networks. *2021 IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR)*, pp.  15694–15703, 2020. URL [https://api.semanticscholar.org/CorpusID:225103308](https://api.semanticscholar.org/CorpusID:225103308).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Martinez 等（2020）朱丽叶·马丁内斯、贾山·谢瓦克拉马尼、刘婷、伊奥安·安德烈·巴尔桑、曾文远和拉奎尔·乌尔塔苏恩。置换、量化和微调: 高效压缩神经网络。*2021
    IEEE/CVF 计算机视觉与模式识别大会 (CVPR)*，第15694–15703页，2020年。网址 [https://api.semanticscholar.org/CorpusID:225103308](https://api.semanticscholar.org/CorpusID:225103308)。'
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等（2016）斯蒂芬·梅里蒂、蔡铭雄、詹姆斯·布拉德伯里和理查德·索彻。指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843*，2016年。
- en: Mostafa & Wang (2019) Hesham Mostafa and Xin Wang. Parameter efficient training
    of deep convolutional neural networks by dynamic sparse reparameterization. In
    *International Conference on Machine Learning*, 2019.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mostafa & Wang（2019）赫沙姆·穆斯塔法和王鑫。通过动态稀疏重新参数化高效训练深度卷积神经网络。发表于*国际机器学习大会*，2019年。
- en: Muhlgay et al. (2023) Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner,
    Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham.
    Generating benchmarks for factuality evaluation of language models. *arXiv preprint
    arXiv:2307.06908*, 2023.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muhlgay 等（2023）多尔·穆赫尔盖、奥里·拉姆、因巴尔·马加尔、约阿夫·莱文、尼尔·拉特纳、约纳坦·贝林科夫、奥姆里·阿本德、凯文·雷顿-布朗、阿姆农·沙什亚和约阿夫·肖汉。生成用于语言模型事实性评估的基准。*arXiv
    预印本 arXiv:2307.06908*，2023年。
- en: Nallapati et al. (2016) Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing
    Xiang, et al. Abstractive text summarization using sequence-to-sequence rnns and
    beyond. *arXiv preprint arXiv:1602.06023*, 2016.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nallapati等（2016）**Ramesh Nallapati**、**Bowen Zhou**、**Caglar Gulcehre**、**Bing
    Xiang**等。使用序列到序列`rnns`及其扩展的抽象文本摘要。*arXiv预印本 arXiv:1602.06023*，2016年。
- en: Nvidia (2020) Nvidia. Nvidia a100 tensor core gpu architecture. *https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf*,
    2020.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nvidia（2020）**Nvidia**。Nvidia A100 Tensor Core `gpu`架构。*https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf*，2020年。
- en: Qian et al. (2023) Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, and Yong
    Liu. Can large language models empower molecular property prediction? *arXiv preprint
    arXiv:2307.07443*, 2023.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian等（2023）**Chen Qian**、**Huayi Tang**、**Zhirui Yang**、**Hong Liang**和**Yong
    Liu**。大型语言模型能否促进分子性质预测？*arXiv预印本 arXiv:2307.07443*，2023年。
- en: Qin et al. (2023) Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing
    task solver? *arXiv preprint arXiv:2302.06476*, 2023.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin等（2023）**Chengwei Qin**、**Aston Zhang**、**Zhuosheng Zhang**、**Jiaao Chen**、**Michihiro
    Yasunaga**和**Diyi Yang**。ChatGPT是通用自然语言处理任务解决者吗？*arXiv预印本 arXiv:2302.06476*，2023年。
- en: Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon
    Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language
    models. *arXiv preprint arXiv:2302.00083*, 2023.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ram等（2023）**Ori Ram**、**Yoav Levine**、**Itay Dalmedigos**、**Dor Muhlgay**、**Amnon
    Shashua**、**Kevin Leyton-Brown**和**Yoav Shoham**。上下文检索增强的语言模型。*arXiv预印本 arXiv:2302.00083*，2023年。
- en: 'Sanh et al. (2020) Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning:
    Adaptive sparsity by fine-tuning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
    Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems*,
    volume 33, pp.  20378–20389\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh等（2020）**Victor Sanh**、**Thomas Wolf**和**Alexander Rush**。运动剪枝：通过微调实现自适应稀疏性。见**H.
    Larochelle**、**M. Ranzato**、**R. Hadsell**、**M. F. Balcan**和**H. Lin**（编），*《神经信息处理系统进展》*，第33卷，第20378–20389页。Curran
    Associates, Inc.，2020年。网址 [https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf)。
- en: 'Sawada et al. (2023) Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav
    Tadepalli, Paula Vidas, Alexander Kranias, John J Nay, Kshitij Gupta, and Aran
    Komatsuzaki. Arb: Advanced reasoning benchmark for large language models. *arXiv
    preprint arXiv:2307.13692*, 2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sawada等（2023）**Tomohiro Sawada**、**Daniel Paleka**、**Alexander Havrilla**、**Pranav
    Tadepalli**、**Paula Vidas**、**Alexander Kranias**、**John J Nay**、**Kshitij Gupta**和**Aran
    Komatsuzaki**。Arb：用于大型语言模型的高级推理基准。*arXiv预印本 arXiv:2307.13692*，2023年。
- en: Sheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez,
    et al. High-throughput generative inference of large language models with a single
    gpu. *arXiv preprint arXiv:2303.06865*, 2023.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheng等（2023）**Ying Sheng**、**Lianmin Zheng**、**Binhang Yuan**、**Zhuohan Li**、**Max
    Ryabinin**、**Daniel Y Fu**、**Zhiqiang Xie**、**Beidi Chen**、**Clark Barrett**、**Joseph
    E Gonzalez**等人。大语言模型的高通量生成推理，仅用一张`gpu`。*arXiv预印本 arXiv:2303.06865*，2023年。
- en: 'Singh & Alistarh (2020) Sidak Pal Singh and Dan Alistarh. Woodfisher: Efficient
    second-order approximation for neural network compression. *Advances in Neural
    Information Processing Systems*, 33:18098–18109, 2020.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh & Alistarh（2020）**Sidak Pal Singh**和**Dan Alistarh**。Woodfisher：用于神经网络压缩的高效二阶近似。*《神经信息处理系统进展》*，第33卷，第18098–18109页，2020年。
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple
    and effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等（2023）**Mingjie Sun**、**Zhuang Liu**、**Anna Bair**和**J Zico Kolter**。一种简单有效的大型语言模型剪枝方法。*arXiv预印本
    arXiv:2306.11695*，2023年。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等（2023）**Hugo Touvron**、**Thibaut Lavril**、**Gautier Izacard**、**Xavier
    Martinet**、**Marie-Anne Lachaux**、**Timothée Lacroix**、**Baptiste Rozière**、**Naman
    Goyal**、**Eric Hambro**、**Faisal Azhar**等。Llama：开放且高效的基础语言模型。*arXiv预印本 arXiv:2302.13971*，2023年。
- en: Valmeekam et al. (2022) Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan,
    and Subbarao Kambhampati. Large language models still can’t plan (a benchmark
    for llms on planning and reasoning about change). *ArXiv*, abs/2206.10498, 2022.
    URL [https://api.semanticscholar.org/CorpusID:249889477](https://api.semanticscholar.org/CorpusID:249889477).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Valmeekam et al. (2022) Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan,
    和 Subbarao Kambhampati. 大型语言模型仍然无法规划（关于规划和对变化进行推理的 llm 基准测试）。*ArXiv*，abs/2206.10498，2022年。网址
    [https://api.semanticscholar.org/CorpusID:249889477](https://api.semanticscholar.org/CorpusID:249889477)。
- en: 'Wang et al. (2023) Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou
    Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large
    language model is also an open-ended decoder for vision-centric tasks. *arXiv
    preprint arXiv:2305.11175*, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou
    Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao 等. Visionllm：大型语言模型也是视觉中心任务的开放式解码器。*arXiv
    预印本 arXiv:2305.11175*，2023年。
- en: Xu et al. (2021) Dongkuan Xu, Ian EH Yen, Jinxi Zhao, and Zhibin Xiao. Rethinking
    network pruning–under the pre-train and fine-tune paradigm. *arXiv preprint arXiv:2104.08682*,
    2021.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2021) Dongkuan Xu, Ian EH Yen, Jinxi Zhao, 和 Zhibin Xiao. 重新思考网络修剪——在预训练和微调范式下。*arXiv
    预印本 arXiv:2104.08682*，2021年。
- en: Ye et al. (2023) Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng
    Zhang. Natural language is all a graph needs. *arXiv preprint arXiv:2308.07134*,
    2023.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye et al. (2023) Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, 和 Yongfeng
    Zhang. 自然语言就是图所需的一切。*arXiv 预印本 arXiv:2308.07134*，2023年。
- en: 'Zafrir et al. (2021) Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, and
    Moshe Wasserblat. Prune once for all: Sparse pre-trained language models. *arXiv
    preprint arXiv:2111.05754*, 2021.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zafrir et al. (2021) Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, 和
    Moshe Wasserblat. 一次性修剪：稀疏的预训练语言模型。*arXiv 预印本 arXiv:2111.05754*，2021年。
- en: 'Zhang et al. (2022) Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin,
    Pengcheng He, Weizhu Chen, and Tuo Zhao. Platon: Pruning large transformer models
    with upper confidence bound of weight importance. In *International Conference
    on Machine Learning*, pp. 26809–26823\. PMLR, 2022.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022) Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin,
    Pengcheng He, Weizhu Chen, 和 Tuo Zhao. Platon：通过权重重要性的上置信界修剪大型变换器模型。发表于*国际机器学习会议*，第
    26809–26823 页。PMLR，2022年。
- en: Zhang et al. (2023) Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen
    McKeown, and Tatsunori Hashimoto. Benchmarking large language models for news
    summarization. *ArXiv*, abs/2301.13848, 2023. URL [https://api.semanticscholar.org/CorpusID:256416014](https://api.semanticscholar.org/CorpusID:256416014).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023) Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen
    McKeown, 和 Tatsunori Hashimoto. 大型语言模型在新闻摘要中的基准测试。*ArXiv*，abs/2301.13848，2023年。网址
    [https://api.semanticscholar.org/CorpusID:256416014](https://api.semanticscholar.org/CorpusID:256416014)。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*,
    2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing 等. 使用 mt-bench
    和聊天机器人竞技场评判 llm-as-a-judge。*arXiv 预印本 arXiv:2306.05685*，2023年。
- en: 'Zhou et al. (2021) Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang,
    Kun Yuan, Wenxiu Sun, and Hongsheng Li. Learning n: m fine-grained structured
    sparse neural networks from scratch. *arXiv preprint arXiv:2102.04010*, 2021.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2021) Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang,
    Kun Yuan, Wenxiu Sun, 和 Hongsheng Li. 从头开始学习 n: m 细粒度结构稀疏神经网络。*arXiv 预印本 arXiv:2102.04010*，2021年。'
- en: 'Zhu & Gupta (2017) Michael Zhu and Suyog Gupta. To prune, or not to prune:
    exploring the efficacy of pruning for model compression. *arXiv preprint arXiv:1710.01878*,
    2017.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu & Gupta (2017) Michael Zhu 和 Suyog Gupta. 修剪还是不修剪：探索模型压缩的修剪效果。*arXiv 预印本
    arXiv:1710.01878*，2017年。
- en: Zhuo (2023) Terry Yue Zhuo. Large language models are state-of-the-art evaluators
    of code generation. *arXiv preprint arXiv:2304.14317*, 2023.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuo (2023) Terry Yue Zhuo. 大型语言模型是最先进的代码生成评估器。*arXiv 预印本 arXiv:2304.14317*，2023年。
- en: Appendix A Appendix
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Related Works
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 相关工作
- en: A.1.1 Sparsity in Large Language Models
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.1 大型语言模型中的稀疏性
- en: The advent of large-scale pre-trained models has led to the development of advanced
    post-training pruning methods, aiming to enhance the cost-effectiveness of these
    expansive models (Sanh et al., [2020](#bib.bib57); Chen et al., [2020](#bib.bib3);
    Jaiswal et al., [2023b](#bib.bib24); Zafrir et al., [2021](#bib.bib67); Kurtic
    et al., [2022](#bib.bib31); Xu et al., [2021](#bib.bib65); Lagunas et al., [2021](#bib.bib32);
    Zhang et al., [2022](#bib.bib68); Frantar et al., [2021](#bib.bib14); Jaiswal
    et al., [2023a](#bib.bib22); Ma et al., [2023](#bib.bib47); Ji et al., [2023](#bib.bib25)).
    Among them, Frantar et al. ([2021](#bib.bib14)) extend second-order pruning to
    the BERT-level scale, enabling the pruning of blocks of weights and achieving
    state-of-the-art results for sparse BERT. Frantar & Alistarh ([2023](#bib.bib13))
    introduce SparseGPT for pruning large language models (LLMs) in a single shot
    without requiring re-training or fine-tuning. They leverage column-wise second-order
    pruning, and successfully remove 100B weights from OPT-175B without a significant
    increase in perplexity. More recently, Sun et al. ([2023](#bib.bib61)) propose
    a straightforward pruning method that takes both weights and activations into
    account, demonstrating comparable performance to Frantar & Alistarh ([2023](#bib.bib13)). Li
    et al. ([2022](#bib.bib37)) reveal that activation sparsity is a prevalent phenomenon
    in Transformers (90% of intermediate output), yielding another opportunity for
    acceleration. Liu et al. ([2023b](#bib.bib43)) introduce a large-scale SMC-Bench,
    indicating that state-of-the-art magnitude- and/or gradient-based sparse algorithms
    fall short when applied out-of-the-box to larger-scale models and a selected of
    complex downstream tasks.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模预训练模型的出现催生了先进的后训练剪枝方法，旨在提升这些庞大模型的成本效益（Sanh et al., [2020](#bib.bib57); Chen
    et al., [2020](#bib.bib3); Jaiswal et al., [2023b](#bib.bib24); Zafrir et al.,
    [2021](#bib.bib67); Kurtic et al., [2022](#bib.bib31); Xu et al., [2021](#bib.bib65);
    Lagunas et al., [2021](#bib.bib32); Zhang et al., [2022](#bib.bib68); Frantar
    et al., [2021](#bib.bib14); Jaiswal et al., [2023a](#bib.bib22); Ma et al., [2023](#bib.bib47);
    Ji et al., [2023](#bib.bib25))。其中，Frantar et al. ([2021](#bib.bib14)) 将二阶剪枝扩展到
    BERT 级别，能够剪除权重块并实现稀疏 BERT 的最先进结果。Frantar & Alistarh ([2023](#bib.bib13)) 介绍了 SparseGPT，用于一次性剪枝大规模语言模型（LLMs），无需重新训练或微调。他们利用按列的二阶剪枝，成功从
    OPT-175B 中去除 100B 权重，而 perplexity 没有显著增加。最近，Sun et al. ([2023](#bib.bib61)) 提出了一个简单的剪枝方法，考虑了权重和激活，表现出与
    Frantar & Alistarh ([2023](#bib.bib13)) 相当的性能。Li et al. ([2022](#bib.bib37)) 揭示了激活稀疏性在
    Transformers 中的普遍现象（90% 的中间输出），提供了另一个加速的机会。Liu et al. ([2023b](#bib.bib43)) 介绍了大规模
    SMC-Bench，表明最先进的基于幅度和/或梯度的稀疏算法在直接应用于更大规模模型和一些复杂的下游任务时效果不足。
- en: A.1.2 Quantization in Large Language Models
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.2 大规模语言模型中的量化
- en: 'With the recent open-source releases of language models like BLOOM, Vicuna,
    LLaMa, OPT, etc., quantization has emerged as a widely embraced technique to alleviate
    the storage and computational overhead of deep learning models. Recent research
    endeavors have harnessed quantization to compress LLMs and they can be classified
    into the two mentioned approaches: Quantization-Aware Training (QAT), and Post-Training
    Quantization (PTQ). In QAT, the quantization objective is embedded into the LLM
    training process, enabling them to adapt to low-precision representations and
    handle precision loss caused by quantization. LLM-QAT (Liu et al., [2023c](#bib.bib44))
    proposes a data-free distillation method that leverages generations produced by
    the pre-trained model, preserving the original output distribution and allows
    quantizing LLaMa models independent of its training data. PEQA (Kim et al., [2023](#bib.bib29))
    operates through a dual-stage process: initially, the parameter matrix of each
    fully-connected layer undergoes quantization into a matrix of low-bit integers
    and a scalar vector; subsequently, fine-tuning occurs on the scalar vector for
    each downstream task. QLoRA (Dettmers et al., [2023a](#bib.bib7)) proposes an
    efficient finetuning approach that reduces memory usage enough to finetune a 65B
    parameter model on a single 48GB GPU while preserving full 16-bit finetuning task
    performance by backpropagating gradients through a frozen, 4-bit quantized pretrained
    language model into Low Rank Adapters (LoRA). PTQ involves quantizing the parameters
    of LLMs after the completion of the LLM’s training phase. GPTQ (Frantar et al.,
    [2022](#bib.bib15)) proposes a novel layer-wise quantization technique based on
    approximate second-order information resulting a bitwidth reduction to 3 or 4
    bits per weight, with minimal accuracy loss compared to the uncompressed version.
    AWQ (Lin et al., [2023](#bib.bib39)) based on the observation that weights are
    not equally important: protecting only 1% of salient weights can greatly reduce
    quantization error, employs an activation-aware approach by considering the significance
    of weight channels corresponding to larger activation magnitudes. SpQR (Dettmers
    et al., [2023c](#bib.bib9)) works by identifying and isolating outlier weights,
    which cause particularly-large quantization errors, and storing them in higher
    precision, while compressing all other weights to 3-4 bits, and achieves relative
    accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon
    LLMs.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 随着BLOOM、Vicuna、LLaMa、OPT等语言模型的近期开源发布，量化技术作为一种广泛采用的手段出现，用于减轻深度学习模型的存储和计算开销。最近的研究工作利用量化来压缩LLM（大型语言模型），并可以分为两种方法：量化感知训练（QAT）和训练后量化（PTQ）。在QAT中，量化目标嵌入到LLM的训练过程中，使其能够适应低精度表示并处理量化带来的精度损失。LLM-QAT（Liu
    et al., [2023c](#bib.bib44)）提出了一种无数据蒸馏方法，利用预训练模型生成的结果，保留原始输出分布，并允许在不依赖于训练数据的情况下对LLaMa模型进行量化。PEQA（Kim
    et al., [2023](#bib.bib29)）通过双阶段过程操作：最初，对每个全连接层的参数矩阵进行量化为低位整数矩阵和标量向量；随后，对每个下游任务的标量向量进行微调。QLoRA（Dettmers
    et al., [2023a](#bib.bib7)）提出了一种高效的微调方法，减少了内存使用，使得在单个48GB GPU上微调65B参数模型，同时通过将梯度反向传播到冻结的4位量化预训练语言模型中的低秩适配器（LoRA）来保持完整的16位微调任务性能。PTQ涉及在LLM训练阶段完成后对LLM参数进行量化。GPTQ（Frantar
    et al., [2022](#bib.bib15)）提出了一种基于近似二阶信息的新型层级量化技术，将比特宽度减少到每个权重3或4位，与未压缩版本相比，准确性损失最小。AWQ（Lin
    et al., [2023](#bib.bib39)）基于权重的重要性不均等的观察：保护仅1%的显著权重可以大大减少量化误差，采用激活感知方法，考虑与较大激活幅度相关的权重通道的重要性。SpQR（Dettmers
    et al., [2023c](#bib.bib9)）通过识别和隔离那些造成特别大量化误差的异常权重，并以更高的精度存储它们，同时将所有其他权重压缩到3-4位，达到对于高度准确的LLaMA和Falcon
    LLM的困惑度相对准确性损失低于1%。
- en: A.1.3 Large Language Models and Evaluation
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.3 大型语言模型与评估
- en: 'Large language models (LLMs) are gaining increasing popularity in both academia
    and industry playing vital role in both research and daily use. With increasing
    popularity, several works (Li et al., [2023](#bib.bib36); Kaddour et al., [2023](#bib.bib28);
    Muhlgay et al., [2023](#bib.bib51); Zhang et al., [2023](#bib.bib69); Valmeekam
    et al., [2022](#bib.bib63); Liu et al., [2023a](#bib.bib41); Sawada et al., [2023](#bib.bib58);
    Qin et al., [2023](#bib.bib55); Zhuo, [2023](#bib.bib73); Lee et al., [2023](#bib.bib35))
    attempt to go beyond conventional perplexity to evaluate performance of LLMs across
    factuality, commonsense reasoning, language understanding, reading comprehension,
    programming, instruction following abilities, *etc*. Muhlgay et al. ([2023](#bib.bib51))
    propose a new metric FACTOR to understand factuality correct information in the
    LLM generated text. It found that although FACTOR accuracy and LMM perplexity
    tend to be highly correlated but sometimes induce different orderings between
    LMMs. They reported that pairs of models can share similar perplexity but differ
    significantly in terms of FACTOR accuracy. Lee et al. ([2023](#bib.bib35)) evaluate
    the performance and alignment of LLM distribution with humans using two different
    techniques: Monte Carlo Reconstruction (MCR) and Log Probability Reconstruction
    (LPR); and found LLMs exhibit limited ability in solving NLI tasks and simultaneously
    fail to capture human disagreement distribution. Zhang et al. ([2023](#bib.bib69))
    attempt to investigate promise for automatic summarization with respect to human
    summary writers and found that LMM summaries are judged to be on par with human
    written summaries. Valmeekam et al. ([2022](#bib.bib63)) propose an extensible
    assessment framework to test the capabilities of LLMs on reasoning about actions
    and change, a central aspect of human intelligence and found that GPT-3 and BLOOM
    have dismal performance on these benchmarks. Despite these efforts to investigate
    the performance of dense LLMs comprehensively, it is surprising that no such efforts
    have been yet carried out for a more daunting case of compressed LLMs, which are
    derived from dense counterparts sharing significantly high similarity with them.
    Our work is first attempt to address this gap and encourage sparse community researchers
    to go beyond perplexity to evaluate the true merits and drawbacks of compression
    methods.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在学术界和工业界越来越受欢迎，在研究和日常使用中发挥着重要作用。随着其受欢迎程度的提高，一些研究（Li et al., [2023](#bib.bib36);
    Kaddour et al., [2023](#bib.bib28); Muhlgay et al., [2023](#bib.bib51); Zhang
    et al., [2023](#bib.bib69); Valmeekam et al., [2022](#bib.bib63); Liu et al.,
    [2023a](#bib.bib41); Sawada et al., [2023](#bib.bib58); Qin et al., [2023](#bib.bib55);
    Zhuo, [2023](#bib.bib73); Lee et al., [2023](#bib.bib35)) 尝试超越传统的困惑度来评估 LLMs 在事实性、常识推理、语言理解、阅读理解、编程、指令遵循能力等方面的表现。Muhlgay
    et al. ([2023](#bib.bib51)) 提出了一个新的指标 FACTOR 来理解 LLM 生成文本中的事实正确性。研究发现，尽管 FACTOR
    准确性和 LMM 困惑度往往高度相关，但有时会导致 LMMs 之间的排序不同。他们报告说，模型对的困惑度相似但 FACTOR 准确性却显著不同。Lee et
    al. ([2023](#bib.bib35)) 使用两种不同的技术：蒙特卡罗重建（MCR）和对数概率重建（LPR）来评估 LLM 分布与人类的表现和对齐情况；发现
    LLM 在解决 NLI 任务方面能力有限，同时未能捕捉到人类分歧分布。Zhang et al. ([2023](#bib.bib69)) 尝试研究自动摘要在与人工摘要编写者的比较中的前景，发现
    LMM 摘要与人工撰写的摘要相当。Valmeekam et al. ([2022](#bib.bib63)) 提出了一个可扩展的评估框架，以测试 LLM 在推理行动和变化能力上的表现，这是人类智能的核心方面，并发现
    GPT-3 和 BLOOM 在这些基准上的表现令人失望。尽管有这些全面调查密集 LLM 表现的努力，但令人惊讶的是，还没有针对压缩 LLMs 进行类似的研究，而这些压缩
    LLMs 是从密集型 LLMs 派生而来的，并与其有很高的相似性。我们的工作首次尝试填补这一空白，并鼓励稀疏社区研究人员超越困惑度，以评估压缩方法的真正优缺点。
- en: A.2 Prompt Design and Examples for Different Task Settings in LLM-KICK
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 LLM-KICK 中不同任务设置的提示设计与示例
- en: A.2.1 Factoid-based QA
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1 基于事实的 QA
- en: '#####
    Prompt Design:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: #####
    提示设计：
- en: 'Please give answer to this question: $<$ The answer is'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 请回答这个问题： $<$ 答案是
- en: 'Example:'
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例：
- en: 'Please give answer to this question: The film ‘10 things I hate about you’
    is based on which Shakespeare play? The answer is'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请回答这个问题：《10 Things I Hate About You》改编自哪部莎士比亚的戏剧？答案是
- en: 'Model Response:'
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型响应：
- en: 'Please give answer to this question: The film ‘10 things I hate about you’
    is based on which Shakespeare play? The answer is the taming of the shrew.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请回答这个问题：《10 Things I Hate About You》改编自哪部莎士比亚的戏剧？答案是《驯悍记》。
- en: A.2.2 Multiple-choice Reasoning-based QA
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.2 基于推理的多项选择问答
- en: '#####
    Prompt Design:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: #####
    提示设计：
- en: 'The following are multiple choice questions (with answers) about $<$n Answer:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于 $<$n 的多项选择题（附答案）：
- en: 'Example:'
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例：
- en: 'The following are multiple choice questions (with answers) about algebra.$\backslash$n
    Answer:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于代数的多项选择题（附答案）。$\backslash$n 回答：
- en: 'Model Response:'
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型响应：
- en: 'The following are multiple choice questions (with answers) about algebra.$\backslash$n
    Answer: B'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于代数的多项选择题（附答案）。$\backslash$n 回答：B
- en: A.2.3 In-context Retrieval Augmented Question Answering
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.3 上下文检索增强问答
- en: 1
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 1
- en: 'Closed Book Setting: For closed-book setting, we adopted the prompt from Touvron
    et al. ([2023](#bib.bib62)) as follows.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 闭卷设置：对于闭卷设置，我们采用了 Touvron 等人 ([2023](#bib.bib62)) 的提示，具体如下。
- en: '#####
    Prompt Design:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: #####
    提示设计：
- en: 'Answer these questions:$\backslash$n A:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '回答这些问题：$\backslash$n A:'
- en: 'Example:'
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例：
- en: 'Answer these questions:$\backslash$n A:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '回答这些问题：$\backslash$n A:'
- en: 'Model Response:'
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型响应：
- en: 'Answer these questions:$\backslash$n A: The man behind The Chipmunks was David
    Sarge, who was the founder of the Alphaville Virtual Real Estate Company.2'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '回答这些问题：$\backslash$n A: 《飞鼠》的创始人是大卫·萨奇，他是阿尔法维尔虚拟房地产公司的创始人。2'
- en: 'Open Book Setting: For open-book setting, we extend the above prompt as follows.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 开放书籍设置：对于开放书籍设置，我们将上述提示扩展如下。
- en: '#####
    Prompt Design:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: #####
    提示设计：
- en: '$<$n A:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '$<$n A:'
- en: 'Example:'
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例：
- en: '‘‘Alvin and the Chipmunks (2007) - IMDb IMDb 17 January 2017 4:34 PM, UTC NEWS.
    A struggling songwriter named Dave Seville finds success ..."$\backslash$n A:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '‘‘《雪人和松鼠》（2007）- IMDb IMDb 2017年1月17日 下午4:34，UTC 新闻。一位名叫**戴夫·塞维尔**的 struggling
    歌词作者找到了成功 ..."$\backslash$n A:'
- en: 'Model Response:'
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型响应：
- en: '‘‘Alvin and the Chipmunks (2007) - IMDb IMDb 17 January 2017 4:34 PM, UTC NEWS.
    A struggling songwriter named Dave Seville finds success ..."$\backslash$n A:
    Dave Seville.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '‘‘《雪人和松鼠》（2007）- IMDb IMDb 2017年1月17日 下午4:34，UTC 新闻。一位名叫**戴夫·塞维尔**的 struggling
    歌词作者找到了成功 ..."$\backslash$n A: **戴夫·塞维尔**。'
- en: A.2.4 In-Context Text Summarization
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.4 上下文文本总结
- en: '#####
    Prompt Design:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: #####
    提示设计：
- en: 'A chat between a curious user and an artificial intelligence assistant. The
    assistant gives helpful, detailed, and polite answers to the user’s questions.
    USER: Summarize the given story in less than 150 words while preserving high coherence,
    consistency, fluency, and relevance.$\backslash$. ASSISTANT:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个好奇的用户与人工智能助手之间的对话。助手为用户的问题提供了有帮助、详细且礼貌的回答。用户：请在150字以内总结给定的故事，同时保持高度的连贯性、一致性、流畅性和相关性。$\backslash$.
    助手：
- en: 'Example:'
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例：
- en: 'A chat between a curious user and an artificial intelligence assistant. The
    assistant gives helpful, detailed, and polite answers to the user’s questions.
    USER: Summarize the given story in less than 150 words while preserving high coherence,
    consistency, fluency, and relevance.$\backslash$nLibyan and U.S. officials say
    the two governments held face-to-face talks in Tunisia ...have denied previous
    reports of talks with the government. ASSISTANT:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个好奇的用户与人工智能助手之间的对话。助手为用户的问题提供了有帮助、详细且礼貌的回答。用户：请在150字以内总结给定的故事，同时保持高度的连贯性、一致性、流畅性和相关性。$\backslash$n利比亚和美国官员表示，两个政府在突尼斯进行了面对面的会谈
    ... 否认了关于与政府谈判的先前报道。助手：
- en: 'Model Response:'
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型响应：
- en: 'The model response of one-shot magnitude pruned Vicuna-7B ASSISTANT is shown
    in Figure [9](#A1.F9 "Figure 9 ‣ Model Response: ‣ A.2.4 In-Context Text Summarization
    ‣ A.2 Prompt Design and Examples for Different Task Settings in LLM-KICK ‣ Appendix
    A Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple").'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性压缩的 Vicuna-7B 模型响应如图 [9](#A1.F9 "图 9 ‣ 模型响应： ‣ A.2.4 上下文文本总结 ‣ A.2 提示设计和不同任务设置的示例
    ‣ 附录 A 附录 ‣ 压缩 LLM：真相少有纯粹且从未简单") 所示。
- en: '![Refer to caption](img/c297ad51d8189b977d97ee21d85235eb.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c297ad51d8189b977d97ee21d85235eb.png)'
- en: 'Figure 9: Output response of 10% compressed (unstructured one-shot) Vicuna-7b
    ASSISTANT.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：10% 压缩（非结构化一次性）Vicuna-7b 模型的输出响应。
- en: A.2.5 Multi-turn Conversation and Instruction Following
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.5 多轮对话和指令跟随
- en: '#####
    Prompt Design:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: #####
    提示设计：
- en: 'A chat between a curious user and an artificial intelligence assistant. The
    assistant gives helpful, detailed, and polite answers to the user’s questions.
    USER: $<$ ASSISTANT:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一次好奇用户与人工智能助手之间的对话。助手对用户的问题提供了有用、详细且礼貌的回答。 用户：$<$ 助手：
- en: 'Example:'
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例：
- en: 'A chat between a curious user and an artificial intelligence assistant. The
    assistant gives helpful, detailed, and polite answers to the user’s questions.
    USER: How can I improve my time management skills? ASSISTANT:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 一次好奇用户与人工智能助手之间的对话。助手对用户的问题提供了有用、详细且礼貌的回答。 用户：我如何提高我的时间管理技能？ 助手：
- en: 'Model Response:'
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型响应：
- en: 'The model response of one-shot magnitude pruned Vicuna-7B ASSISTANT is shown
    in Figure [10](#A1.F10 "Figure 10 ‣ Model Response: ‣ A.2.5 Multi-turn Conversation
    and Instruction Following ‣ A.2 Prompt Design and Examples for Different Task
    Settings in LLM-KICK ‣ Appendix A Appendix ‣ Compressing LLMs: The Truth is Rarely
    Pure and Never Simple").'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '图[10](#A1.F10 "图 10 ‣ 模型响应: ‣ A.2.5 多轮对话和指令跟随 ‣ A.2 提示设计和不同任务设置的示例 ‣ 附录 A 附录
    ‣ 压缩大语言模型：真相很少纯粹，永远不会简单")中显示了一次性大小修剪的Vicuna-7B助手的模型响应。'
- en: '![Refer to caption](img/20388d075ccf5b719b07c91038d5906a.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/20388d075ccf5b719b07c91038d5906a.png)'
- en: 'Figure 10: Output response of 10% compressed (unstructured one-shot) Vicuna-7b
    ASSISTANT.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：10% 压缩（无结构单次）Vicuna-7b助手的输出响应。
- en: A.3 In-Context Summarization Evaluation Settings
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 上下文总结评估设置
- en: 'For evaluating the performance of LLMs to generate high-quality in-context
    summarization, we focus on consistency, coherence, fluency, and relevance metrics.
    We prompt GPT-4 which has been recently identified to be highly effective as an
    automated evaluation framework for benchmark generation and performance assessments,
    to evaluate these metrics in comparison to the summaries generated by GPT-3.5\.
    Examples of our prompts used for evaluating with GPT-4 Judge are shown in Figure
    [11](#A1.F11 "Figure 11 ‣ A.3 In-Context Summarization Evaluation Settings ‣ Appendix
    A Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple"). We
    also provide an example of GPT-4 Judge output in Figure [12](#A1.F12 "Figure 12
    ‣ A.3 In-Context Summarization Evaluation Settings ‣ Appendix A Appendix ‣ Compressing
    LLMs: The Truth is Rarely Pure and Never Simple").'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估大语言模型（LLMs）生成高质量上下文总结的表现，我们关注一致性、连贯性、流畅性和相关性指标。我们提示了GPT-4，它被近期确认作为一个自动化评估框架在基准生成和性能评估方面非常有效，以比较这些指标与GPT-3.5生成的总结。我们用于评估的GPT-4
    Judge提示的示例见图[11](#A1.F11 "图 11 ‣ A.3 上下文总结评估设置 ‣ 附录 A 附录 ‣ 压缩大语言模型：真相很少纯粹，永远不会简单")。我们还提供了GPT-4
    Judge输出的示例，见图[12](#A1.F12 "图 12 ‣ A.3 上下文总结评估设置 ‣ 附录 A 附录 ‣ 压缩大语言模型：真相很少纯粹，永远不会简单")。
- en: '![Refer to caption](img/b38472d5a4e5e4f0d86dadb1e263a36a.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/b38472d5a4e5e4f0d86dadb1e263a36a.png)'
- en: 'Figure 11: Example of prompt used to evaluate the compressed LLM ASSISTANT
    *wrt.* GPT-3.5 ASSISTANT using GPT-4 as Judge on consistency, coherence, fluency,
    and relevance of generated summaries.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：用于评估压缩大语言模型助手*相对于* GPT-3.5助手的一致性、连贯性、流畅性和生成总结的相关性的提示示例，使用GPT-4作为评审。
- en: '![Refer to caption](img/7da419d05b26f85405e3cb5ea3b39bc9.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/7da419d05b26f85405e3cb5ea3b39bc9.png)'
- en: 'Figure 12: GPT-4 Judge Evaluation of responses generated by GPT-3 (ASSISTANT
    1) *wrt.* 10% compressed (unstructured one-shot) Vicuna-7b (ASSISTANT 2).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：GPT-4 Judge 对GPT-3（助手 1）*相对于* 10% 压缩（无结构单次）Vicuna-7b（助手 2）生成的响应的评估。
- en: A.4 Instruction Following Ability Evaluation Setting
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 指令跟随能力评估设置
- en: 'For evaluating the responses generated by compressed LLMs, we closely follow
    the prompt design settings of MT-Bench (Zheng et al., [2023](#bib.bib70)) using
    GPT-4 as judge. We prompt GPT-4 to rate the answers generated by compressed LLMs
    wrt. GPT-3.5 (text-davinci-003) model based on varying metrics (eg. correctness,
    helpfulness, logic, accuracy, *etc.*) on a scale of [0-10] and provides a detailed
    explanation behind the score. Examples of our prompts used during evaluation for
    questions as well as GPT-4 Judge response are as shown in Figure [13](#A1.F13
    "Figure 13 ‣ A.4 Instruction Following Ability Evaluation Setting ‣ Appendix A
    Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple"), and
    [14](#A1.F14 "Figure 14 ‣ A.4 Instruction Following Ability Evaluation Setting
    ‣ Appendix A Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple"),
    respectively.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估压缩LLM生成的响应，我们严格遵循MT-Bench (Zheng et al., [2023](#bib.bib70)) 的提示设计设置，并使用GPT-4作为评审。我们提示GPT-4对压缩LLM生成的答案进行评分，参照GPT-3.5
    (text-davinci-003) 模型，基于不同的指标（例如正确性、帮助性、逻辑性、准确性，*等*）在[0-10]的范围内评分，并提供详细的评分解释。在评估过程中使用的提示示例以及GPT-4的评审响应如图[13](#A1.F13
    "Figure 13 ‣ A.4 Instruction Following Ability Evaluation Setting ‣ Appendix A
    Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple")和[14](#A1.F14
    "Figure 14 ‣ A.4 Instruction Following Ability Evaluation Setting ‣ Appendix A
    Appendix ‣ Compressing LLMs: The Truth is Rarely Pure and Never Simple")所示。'
- en: '![Refer to caption](img/386462d80a2c3007b39031d0e808e269.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/386462d80a2c3007b39031d0e808e269.png)'
- en: 'Figure 13: Examples of prompts used for different categories to evaluate the
    compressed LLM ASSISTANT *wrt.* GPT-3.5 ASSISTANT using GPT-4 as a Judge.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：用于评估压缩LLM ASSISTANT *相对于* GPT-3.5 ASSISTANT的不同类别的提示示例，使用GPT-4作为评审。
- en: '![Refer to caption](img/5954d04b5351339c2cfefb13ee71cdb0.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5954d04b5351339c2cfefb13ee71cdb0.png)'
- en: 'Figure 14: GPT4-as-a-Judge evaluation of responses generated by GPT-3 (ASSISTANT
    1) *wrt.* 10% compressed (unstructured one-shot) Vicuna-7b (ASSISTANT 2).'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：GPT4作为评审对由GPT-3生成的响应的评估（ASSISTANT 1）*相对于* 10%压缩（非结构化单次）Vicuna-7b（ASSISTANT
    2）。
- en: A.5 Useful Links for LLM-KICK
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 LLM-KICK的有用链接
- en: 'Table 1: Dataset and code link used in our work.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：我们工作中使用的数据集和代码链接。
- en: '| Method / Dataset | Download URL |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 方法 / 数据集 | 下载链接 |'
- en: '| --- | --- |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| FreebaseQA (Jiang et al., [2019](#bib.bib26)) | [https://huggingface.co/datasets/freebase_qa](https://huggingface.co/datasets/freebase_qa)
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| FreebaseQA (Jiang et al., [2019](#bib.bib26)) | [https://huggingface.co/datasets/freebase_qa](https://huggingface.co/datasets/freebase_qa)
    |'
- en: '| MMLU Benchmark (Hendrycks et al., [2020](#bib.bib18)) | [https://huggingface.co/datasets/freebase_qa](https://huggingface.co/datasets/freebase_qa)
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| MMLU Benchmark (Hendrycks et al., [2020](#bib.bib18)) | [https://huggingface.co/datasets/freebase_qa](https://huggingface.co/datasets/freebase_qa)
    |'
- en: '| TriviaQA (Joshi et al., [2017](#bib.bib27)) | [https://huggingface.co/datasets/trivia_qa](https://huggingface.co/datasets/trivia_qa)
    |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| TriviaQA (Joshi et al., [2017](#bib.bib27)) | [https://huggingface.co/datasets/trivia_qa](https://huggingface.co/datasets/trivia_qa)
    |'
- en: '| MT-Bench (Zheng et al., [2023](#bib.bib70)) | [https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts](https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts)
    |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench (Zheng et al., [2023](#bib.bib70)) | [https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts](https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts)
    |'
- en: '| CNN/DailyMail Summarization (Nallapati et al., [2016](#bib.bib52)) | [https://cs.nyu.edu/~kcho/DMQA/](https://cs.nyu.edu/~kcho/DMQA/)
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| CNN/DailyMail Summarization (Nallapati et al., [2016](#bib.bib52)) | [https://cs.nyu.edu/~kcho/DMQA/](https://cs.nyu.edu/~kcho/DMQA/)
    |'
- en: '| WikiText (Merity et al., [2016](#bib.bib49)) | [https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts](https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts)
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| WikiText (Merity et al., [2016](#bib.bib49)) | [https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts](https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts)
    |'
- en: '| Wanda (Sun et al., [2023](#bib.bib61)) | [https://github.com/locuslab/wanda](https://github.com/locuslab/wanda)
    |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Wanda (Sun et al., [2023](#bib.bib61)) | [https://github.com/locuslab/wanda](https://github.com/locuslab/wanda)
    |'
- en: '| SparseGPT (Frantar & Alistarh, [2023](#bib.bib13)) | [https://github.com/IST-DASLab/sparsegpt](https://github.com/IST-DASLab/sparsegpt)
    |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT (Frantar & Alistarh, [2023](#bib.bib13)) | [https://github.com/IST-DASLab/sparsegpt](https://github.com/IST-DASLab/sparsegpt)
    |'
- en: '| LLM-Judge (Zheng et al., [2023](#bib.bib70)) | [https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge)
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Judge (Zheng et al., [2023](#bib.bib70)) | [https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge)
    |'
- en: '| GPTQ (Frantar et al., [2022](#bib.bib15)) | [https://github.com/qwopqwop200/GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa)
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ (Frantar et al., [2022](#bib.bib15)) | [https://github.com/qwopqwop200/GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa)
    |'
