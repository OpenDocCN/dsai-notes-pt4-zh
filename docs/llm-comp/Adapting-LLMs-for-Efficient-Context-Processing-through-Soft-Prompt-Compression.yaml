- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:53:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:01
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Adapting LLMs for Efficient Context Processing through Soft Prompt Compression
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过软提示压缩适应大语言模型以实现高效上下文处理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.04997](https://ar5iv.labs.arxiv.org/html/2404.04997)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.04997](https://ar5iv.labs.arxiv.org/html/2404.04997)
- en: Cangqing Wang¹, Yutian Yang¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Cangqing Wang¹、Yutian Yang¹
- en: 'Ruisi Li², Dan Sun², Ruicong Cai², Yuzhu Zhang², Chengqian Fu³ and Lillian
    Floyd^∗ ¹Cangqing Wang be with Boston University, MA 02215, USA {kriswang}@bu.edu¹Yutian
    Yang be with University of California, Davis, CA 95616, USA {yytyang}@ucdavis.edu²Ruisi
    Li is an independent researcher. Correspondence to Ruisi Li via email: {irisli7728}@gmail.com²Dan
    Sun be with Washington University in St. Louis, MO 63130, USA {sun.dan}@wustl.edu²Ruicong
    Cai is an independent researcher. Correspondence to Ruicong Cai via email: {aruiqian}@gmail.com²Yuzhu
    Zhang be with Carnegie Mellon University, Pittsburgh, PA 15213 {yuzhuz}@alumni.cmu.edu³Chengqian
    Fu is an independent researcher. Correspondence to Chengqian Fu via email: {cqfu728}@gmail.com^∗Lilian
    Floyd be with Georgia Institue of Technology, GA 30332, USA {lfloyd74}@gatech.edu'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Ruisi Li²、Dan Sun²、Ruicong Cai²、Yuzhu Zhang²、Chengqian Fu³ 和 Lillian Floyd^∗
    ¹Cangqing Wang 在波士顿大学（Boston University），MA 02215，美国 {kriswang}@bu.edu ¹Yutian
    Yang 在加州大学戴维斯分校（University of California, Davis），CA 95616，美国 {yytyang}@ucdavis.edu²Ruisi
    Li 是一名独立研究人员。联系 Ruisi Li 请通过电子邮件：{irisli7728}@gmail.com²Dan Sun 在华盛顿大学（Washington
    University in St. Louis），MO 63130，美国 {sun.dan}@wustl.edu²Ruicong Cai 是一名独立研究人员。联系
    Ruicong Cai 请通过电子邮件：{aruiqian}@gmail.com²Yuzhu Zhang 在卡内基梅隆大学（Carnegie Mellon
    University），匹兹堡，PA 15213 {yuzhuz}@alumni.cmu.edu³Chengqian Fu 是一名独立研究人员。联系 Chengqian
    Fu 请通过电子邮件：{cqfu728}@gmail.com^∗Lillian Floyd 在乔治亚理工学院（Georgia Institute of Technology），GA
    30332，美国 {lfloyd74}@gatech.edu
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The rapid advancement of Large Language Models (LLMs) has inaugurated a transformative
    epoch in natural language processing, fostering unprecedented proficiency in text
    generation, comprehension, and contextual scrutiny. Nevertheless, effectively
    handling extensive contexts, crucial for myriad applications, poses a formidable
    obstacle owing to the intrinsic constraints of the models’ context window sizes
    and the computational burdens entailed by their operations. This investigation
    presents an innovative framework that strategically tailors LLMs for streamlined
    context processing by harnessing the synergies among natural language summarization,
    soft prompt compression, and augmented utility preservation mechanisms. Our methodology,
    dubbed SoftPromptComp, amalgamates natural language prompts extracted from summarization
    methodologies with dynamically generated soft prompts to forge a concise yet semantically
    robust depiction of protracted contexts. This depiction undergoes further refinement
    via a weighting mechanism optimizing information retention and utility for subsequent
    tasks. We substantiate that our framework markedly diminishes computational overhead
    and enhances LLMs’ efficacy across various benchmarks, while upholding or even
    augmenting the caliber of the produced content. By amalgamating soft prompt compression
    with sophisticated summarization, SoftPromptComp confronts the dual challenges
    of managing lengthy contexts and ensuring model scalability. Our findings point
    towards a propitious trajectory for augmenting LLMs’ applicability and efficiency,
    rendering them more versatile and pragmatic for real-world applications. This
    research enriches the ongoing discourse on optimizing language models, providing
    insights into the potency of soft prompts and summarization techniques as pivotal
    instruments for the forthcoming generation of NLP solutions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）的快速进步开启了自然语言处理的变革时代，培养了文本生成、理解和上下文分析方面的前所未有的能力。然而，有效处理大量上下文对于许多应用至关重要，这因模型的上下文窗口大小的固有限制和其操作所涉及的计算负担而成为一大挑战。本研究提出了一种创新框架，通过利用自然语言总结、软提示压缩和增强的效用保存机制之间的协同效应，战略性地调整LLMs以实现流畅的上下文处理。我们的方法，称为
    SoftPromptComp，将从总结方法中提取的自然语言提示与动态生成的软提示相结合，以形成一个简洁而语义上稳健的长期上下文描述。该描述通过加权机制进一步优化，以提高信息保留和后续任务的效用。我们证明了我们的框架显著减少了计算开销，提高了LLMs在各种基准上的效率，同时保持或甚至提升了生成内容的质量。通过将软提示压缩与复杂的总结结合，SoftPromptComp
    解决了管理长上下文和确保模型可扩展性的双重挑战。我们的研究结果指向了增强LLMs适用性和效率的有利前景，使其在现实世界应用中更加多功能和实用。本研究丰富了关于优化语言模型的讨论，提供了关于软提示和总结技术作为下一代NLP解决方案关键工具的潜力的见解。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Knowledge Graph Reasoning, Reinforcement Learning, Reward Shaping, Transfer
    Learning
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱推理、强化学习、奖励塑造、迁移学习
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: The finite context window size inherent in most LLMs constrains their ability
    to fully grasp and utilize extensive textual information, thereby limiting their
    efficacy on tasks demanding profound comprehension of lengthy documents. Additionally,
    the substantial computational resources requisite for LLM processing present another
    obstacle, particularly for applications necessitating swift responsiveness and
    high throughput. These challenges underscore the imperative for innovative methodologies
    aimed at enhancing LLM efficiency and context management capabilities without
    compromising performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数LLMs固有的有限上下文窗口大小限制了它们充分理解和利用大量文本信息的能力，从而限制了它们在需要深入理解长篇文档的任务中的效果。此外，LLM处理所需的大量计算资源也构成了另一个障碍，尤其是对于需要快速响应和高吞吐量的应用。
    这些挑战突显了提高LLM效率和上下文管理能力的创新方法的迫切需求，而不损害性能。
- en: This paper introduces a pioneering framework, titled Soft Prompt Compression
    for LLMs (SPC-LLM), which aims to overcome these constraints by amalgamating the
    principles of soft prompt compression with natural language summarization techniques.
    the integration of soft prompt compression with LLMs could benefit from references
    to pioneering studies on prompt engineering or soft prompts in NLP[[1](#bib.bib1)].
    Our approach endeavors to tailor LLMs for streamlined context processing, enabling
    them to navigate extensive textual data more adeptly while alleviating computational
    burdens.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一个开创性的框架，称为LLMs的软提示压缩（SPC-LLM），旨在通过将软提示压缩的原则与自然语言摘要技术结合起来，克服这些限制。将软提示压缩与LLMs的结合可以从关于提示工程或NLP中软提示的开创性研究中受益[[1](#bib.bib1)]。我们的方法致力于为LLMs量身定制流畅的上下文处理，使它们能够更熟练地处理广泛的文本数据，同时减轻计算负担。
- en: 'Our strategy comprises two primary facets: firstly, leveraging natural language
    summarization to distill protracted texts into succinct, content-rich summaries,
    and secondly, integrating these summaries into the model’s input via trainable
    soft prompts. This dual-pronged approach extends the effective context window
    of LLMs and fosters a nuanced comprehension and generation of text predicated
    on diverse information reservoirs. By condensing the context into a compact, information-dense
    format, SPC-LLM substantially diminishes computational overheads, rendering the
    deployment of LLMs more viable across a broad array of applications.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的策略包括两个主要方面：首先，利用自然语言摘要将冗长的文本提炼成简洁、内容丰富的摘要；其次，通过可训练的软提示将这些摘要整合到模型的输入中。这种双管齐下的方法扩展了LLMs的有效上下文窗口，促进了基于多样信息源的细致理解和生成文本。通过将上下文压缩成紧凑、信息密集的格式，SPC-LLM显著减少了计算开销，使LLMs在广泛的应用中更具可行性。
- en: We delineate a comprehensive methodology for implementing soft prompt compression
    alongside natural language summarization within LLMs, elucidating how this amalgamation
    augments model performance on tasks necessitating comprehension of extended contexts.
    Moreover, we furnish empirical substantiation from a series of experiments evincing
    the efficacy of SPC-LLM in enhancing the efficiency and precision of LLMs across
    various NLP tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们阐明了一种全面的方法论，用于在大型语言模型（LLMs）中实现软提示压缩与自然语言摘要的结合，解释了这种融合如何提升模型在需要理解扩展上下文的任务中的表现。此外，我们提供了一系列实验的实证证据，证明了SPC-LLM在提升LLMs在各种自然语言处理任务中的效率和精度方面的有效性。
- en: 'The structure of this paper is as follows: we commence with a review of pertinent
    literature concerning LLM efficiency and context processing. Subsequently, we
    expound upon the proposed SPC-LLM framework, delineating its constituents and
    the integration process. We subsequently expound upon the experimental setup and
    present our findings, highlighting the advantages of our approach. Finally, we
    deliberate on the implications of our work and posit avenues for future research
    endeavors in this domain.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的结构如下：我们首先回顾有关LLM效率和上下文处理的相关文献。随后，我们详细阐述了提出的SPC-LLM框架，描述其组成部分及整合过程。接着，我们详细说明实验设置并展示我们的发现，突出我们方法的优势。最后，我们讨论了我们工作的影响，并提出了未来研究的方向。
- en: II Prior Work
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: 'The endeavor to augment the efficiency and contextual processing capabilities
    of Large Language Models (LLMs) stands as a focal point in contemporary natural
    language processing (NLP) inquiry. This section provides an overview of notable
    advancements in three pertinent domains: traditional methodologies for managing
    lengthy contexts within LLMs, the genesis and utilization of soft prompts, and
    innovations in text summarization techniques aimed at contextual information compression.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 提高大型语言模型（LLMs）的效率和上下文处理能力是现代自然语言处理（NLP）研究的重点。本节概述了三个相关领域的显著进展：传统的LLMs长文本处理方法，软提示的产生和利用，以及旨在上下文信息压缩的文本摘要技术的创新。
- en: '![Refer to caption](img/5e1733a9c50188fd86b09a61d8b375b7.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5e1733a9c50188fd86b09a61d8b375b7.png)'
- en: 'Figure 1: An example of successful prompt compression with SPC. The compressed
    prompt (green) in order to obtain a shorter length and maintain transferability
    and utility simultaneously than the original long prompt (red).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：使用SPC成功压缩提示的示例。压缩后的提示（绿色）旨在比原始长提示（红色）获得更短的长度，同时保持可传递性和实用性。
- en: '![Refer to caption](img/c4a17b83fae23013c50f2ea745b5fbc6.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c4a17b83fae23013c50f2ea745b5fbc6.png)'
- en: 'Figure 2: The illustration of SPC shows the compressed conversational answer
    expect with question.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：SPC的示意图展示了压缩的对话回答期望与问题的关系。
- en: II-A Addressing Lengthy Contexts in LLMs
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 处理LLMs中的长文本
- en: Initial efforts to expand the contextual scope of LLMs primarily concentrated
    on architectural innovations. Transformer-based models such as Longformer [[2](#bib.bib2)]
    and BigBird [[3](#bib.bib3)] introduced alterations to the conventional attention
    mechanism, facilitating the efficient analysis of extended textual sequences.
    These models employ sparse attention patterns and memory-efficient algorithms
    to mitigate computational overheads associated with handling extensive contexts.
    Nevertheless, notwithstanding their augmented capacity, these architectural refinements
    frequently necessitate significant alterations to pre-existing models and entail
    substantial retraining endeavors.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 初步扩展大语言模型（LLMs）上下文范围的努力主要集中在架构创新上。基于Transformer的模型如Longformer [[2](#bib.bib2)]
    和 BigBird [[3](#bib.bib3)] 对传统注意力机制进行了改动，便于有效分析扩展的文本序列。这些模型采用稀疏注意力模式和内存高效算法，以减轻处理大范围上下文时的计算开销。然而，尽管其能力有所增强，这些架构改进往往需要对现有模型进行重大调整，并涉及大量的再训练工作。
- en: II-B Soft Prompts in NLP
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B NLP中的软提示
- en: The notion of soft prompts, first introduced by Li and Liang [[4](#bib.bib4)]
    and further elucidated by Lester [[5](#bib.bib5)], signifies a departure towards
    more adaptable and parameter-efficient approaches for tailoring LLMs to specific
    tasks. Unlike rigidly defined textual prompts, soft prompts comprise trainable
    embeddings optimized alongside the model to enhance performance on targeted tasks.
    This adaptability facilitates effective fine-tuning of LLMs sans exhaustive retraining,
    thereby enabling their adaptation across diverse applications. Nonetheless, the
    amalgamation of soft prompts with the objective of enhancing context compression
    and efficiency within LLMs remains a relatively underexplored domain.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 软提示的概念首次由Li和Liang [[4](#bib.bib4)] 提出，并由Lester [[5](#bib.bib5)] 进一步阐述，标志着朝着更灵活和参数高效的方式定制LLMs的转变。与严格定义的文本提示不同，软提示由与模型一起优化的可训练嵌入组成，以提高针对特定任务的性能。这种适应性使得在没有大量再训练的情况下有效微调LLMs，从而使其适应不同的应用。然而，软提示与提高LLMs中上下文压缩和效率的目标结合，仍然是一个相对未被深入探索的领域。
- en: II-C Text Summarization for Contextual Compression
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 上下文压缩的文本摘要
- en: Text summarization methodologies, encompassing both extractive and abstractive
    approaches, have been harnessed to condense protracted documents into succinct
    representations while retaining salient information. Advancements in summarization
    techniques, exemplified by models like BART [[6](#bib.bib6)], have exhibited considerable
    promise in generating coherent and concise summaries. Although these models adeptly
    distill lengthy texts, their direct utilization as a preprocessing step for LLMs
    in tasks necessitating nuanced comprehension of extended contexts has not been
    extensively investigated.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要方法，包括提取式和抽象式方法，已被用于将冗长的文档浓缩成简洁的表示，同时保留重要信息。摘要技术的进展，例如 BART [[6](#bib.bib6)]，在生成连贯且简洁的摘要方面展现了相当大的潜力。尽管这些模型能够有效地提炼长文本，但它们作为预处理步骤直接用于需要对扩展上下文有细致理解的任务的
    LLM 的应用尚未得到广泛研究。
- en: The fusion of soft prompts with summarization techniques to augment the processing
    of lengthy contexts in LLMs epitomizes a novel convergence of these research trajectories.
    Our study builds upon these foundational principles, proposing a unified framework
    that capitalizes on the strengths of soft prompts and advanced summarization methodologies
    to alleviate the constraints of existing LLMs in efficiently handling extensive
    textual information.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将软提示与摘要技术融合以增强 LLM 对长文本处理的能力，体现了这些研究方向的新融合。我们的研究基于这些基础原则，提出了一个统一框架，利用软提示和先进的摘要方法的优势，以缓解现有
    LLM 在有效处理大量文本信息方面的限制。
- en: III Methodology
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 方法论
- en: 'Integrating summary vectors with natural language formatted prompts, the fusion
    of utility preservation with information compression, and the conceptualization
    of soft prompts are explored within a comprehensive mathematical model. In this
    section, our endeavor lies in the construction of a holistic mathematical framework,
    embodying the amalgamation of summary vectors with prompts formatted in natural
    language, the convergence of utility retention with information condensation,
    and the notion of soft prompts. Let us consider the following components:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个综合数学模型中探讨了将摘要向量与自然语言格式化提示结合的过程、效用保存与信息压缩的融合以及软提示的概念。本节的工作在于构建一个全面的数学框架，体现摘要向量与自然语言格式化提示的融合、效用保持与信息凝聚的结合以及软提示的概念。我们考虑以下组件：
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An original document $D$
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 原始文档 $D$
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A function $f_{NL}$
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个函数 $f_{NL}$
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A function $f_{S}$
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个函数 $f_{S}$
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A parameter matrix $S$ for soft prompt
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用于软提示的参数矩阵 $S$
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A predictive function of the language model, denoted as $f_{LM}$
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语言模型的预测函数，记作 $f_{LM}$
- en: 'The expression of the integrated model we aspire to formulate is as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期望制定的集成模型的表达式如下：
- en: '|  | $C=f_{LM}(S\oplus f_{S}(f_{NL}(D)))$ |  | (1) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $C=f_{LM}(S\oplus f_{S}(f_{NL}(D)))$ |  | (1) |'
- en: 'Where:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $f_{NL}(D)$.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $f_{NL}(D)$。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $f_{S}(P_{NL})$.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $f_{S}(P_{NL})$。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $S$ represents the parameter matrix of trained soft prompts, meticulously optimized
    to augment the model’s versatility across distinct tasks.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $S$ 代表训练软提示的参数矩阵，经过精心优化，以增强模型在不同任务上的多样性。
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\oplus$.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\oplus$。
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $f_{LM}$.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $f_{LM}$。
- en: 'In this expression:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表达式中：
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Utility preservation is attained through the strategic design of functions $f_{S}$
    to uphold the maximum preservation of information.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过战略性设计函数 $f_{S}$ 达到效用保存，以最大限度地保持信息。
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The compression of information is executed via function$f_{S}$.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息的压缩通过函数 $f_{S}$ 执行。
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The soft prompt is optimized during the training process to learn how to most
    effectively utilize the compressed information $V_{S}$ to improve the model’s
    performance on specific tasks.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 软提示在训练过程中被优化，以学习如何最有效地利用压缩信息 $V_{S}$ 来提高模型在特定任务上的表现。
- en: Utilizing this methodology, we harness the robust information compression potential
    inherent in NL formatted prompts and summary vectors. Additionally, we bolster
    the adaptability and efficacy of the model for particular downstream tasks by
    integrating soft prompts, thereby amplifying its capacity for generalization [[7](#bib.bib7)].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这种方法，我们利用 NL 格式化提示和摘要向量固有的强大信息压缩潜力。此外，通过集成软提示，我们增强了模型在特定下游任务中的适应性和效率，从而提升了其泛化能力
    [[7](#bib.bib7)]。
- en: IV Experiments
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验
- en: 'The principal aim of these experiments is to assess the efficacy of our proposed
    methodology in enhancing the efficiency and performance of Large Language Models
    (LLMs) during the processing of expansive textual contexts. Our specific focus
    lies in evaluating: The efficacy of condensed contextual representations produced
    via natural language summarization and the influence of integrating soft prompt
    compression on the efficacy of LLMs across diverse NLP tasks.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验的主要目标是评估我们提出的方法在提高大型语言模型（LLMs）处理广泛文本上下文时的效率和表现方面的有效性。我们特别关注以下内容：通过自然语言摘要生成的压缩上下文表示的有效性，以及整合软提示压缩对LLMs在不同NLP任务中的有效性的影响。
- en: To achieve this aim, we meticulously designed our experiments to measure two
    critical aspects of our methodology’s impact on LLMs. First, we evaluated the
    quality and utility of the condensed contextual representations generated by our
    advanced natural language summarization techniques. This assessment focused on
    determining how effectively our approach could distill essential information from
    extensive textual data, thereby facilitating a more efficient processing by LLMs.
    The evaluation criteria included the coherence, completeness, and relevance of
    the summaries in retaining the core message and essential details from the original
    texts. Second, we investigated the role of soft prompt compression in enhancing
    the overall performance of LLMs across a variety of NLP tasks, including but not
    limited to text summarization, sentiment analysis, text classification, and question
    answering. This part of our experiment aimed at quantifying the improvements in
    task-specific metrics such as accuracy, and processing time, attributable to the
    integration of soft prompts[[8](#bib.bib8)]. Special attention was given to the
    adaptability of soft prompts in capturing and utilizing the nuances of condensed
    contexts, thereby augmenting the LLMs’ ability to derive accurate and relevant
    outcomes from the processed data. Through a comprehensive set of experiments,
    we meticulously recorded and analyzed the performance metrics, processing times,
    and resource utilization patterns associated with each task, both with and without
    the application of our methodology[[9](#bib.bib9)]. This data collection was instrumental
    in illustrating the tangible benefits of our approach, particularly in terms of
    reducing computational overhead and enhancing the adaptability and efficiency
    of LLMs in handling extensive textual contexts.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为实现这一目标，我们精心设计了实验以测量我们方法对大型语言模型（LLMs）影响的两个关键方面。首先，我们评估了通过我们先进的自然语言摘要技术生成的压缩上下文表示的质量和实用性。此次评估重点在于确定我们的方法如何有效地从大量文本数据中提取关键信息，从而促进LLMs的更高效处理。评估标准包括摘要在保留原始文本核心信息和重要细节方面的连贯性、完整性和相关性。其次，我们调查了软提示压缩在提高LLMs在各种NLP任务中的整体表现中的作用，这些任务包括但不限于文本摘要、情感分析、文本分类和问答。实验的这一部分旨在量化在任务特定指标（如准确性和处理时间）上的改进，这些改进归因于软提示的整合[[8](#bib.bib8)]。特别关注了软提示在捕捉和利用压缩上下文细微差别方面的适应性，从而增强LLMs从处理数据中得出准确和相关结果的能力。通过一系列全面的实验，我们详细记录和分析了与每个任务相关的性能指标、处理时间和资源利用模式，无论是否应用了我们的方法[[9](#bib.bib9)]。这些数据收集对于说明我们方法的实际好处尤其重要，特别是在减少计算开销和提高LLMs处理广泛文本上下文的适应性和效率方面。
- en: By systematically addressing these focus areas, our experiments were designed
    not only to validate the effectiveness of our proposed methodology but also to
    explore its potential limitations and areas for further enhancement. The findings
    from these experiments are anticipated to contribute significantly to the ongoing
    efforts to optimize LLMs for a wider range of applications, thereby pushing the
    boundaries of what is achievable in the field of natural language processing.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过系统地解决这些重点领域，我们的实验旨在不仅验证我们提出的方法的有效性，还探索其潜在的局限性和进一步提升的领域。预计这些实验的发现将对优化LLMs在更广泛应用中的持续努力做出重大贡献，从而推动自然语言处理领域的可实现界限。
- en: IV-A Dataset Description
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 数据集描述
- en: Summarization Task CNN/Daily Mail dataset, which pro- vides news articles and
    associated highlights for evaluating summarization quality.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 总结任务 CNN/Daily Mail 数据集，提供新闻文章及相关亮点以评估摘要质量。
- en: Sentiment Analysis The Stanford Sentiment Treebank (SST-2), offering movie reviews
    with sentiment labels.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析 斯坦福情感树库（SST-2），提供了带有情感标签的电影评论。
- en: Text Classification The AG News dataset, containing news articles categorized
    into four topics.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类 AG News 数据集，包含了分类为四个主题的新闻文章。
- en: Question Answering The SQuAD v2.0 dataset, featuring questions posed on a set
    of Wikipedia articles, where some questions are unanswerable.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 问答系统 SQuAD v2.0 数据集，包含了针对一系列维基百科文章提出的问题，其中一些问题是无法回答的。
- en: IV-B Main Result
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 主要结果
- en: We refine the performance of a Transformer-based model renowned for its summarization
    prowess through fine-tuning on the CNN/Daily Mail dataset. The dataset is partitioned
    into distinct training, validation, and test splits tailored to each specific
    task. Our summarization module effectively condenses contextual information within
    the training and validation datasets. Soft prompts are initialized as embeddings
    within the input LLMs, concurrently trained with the model’s parameters to optimize
    task-specific performance. Comparative evaluation of the LLMs’ performance, both
    with and without our methodological intervention, is conducted using task-specific
    metrics.The visualizations provided underscore the marked improvements in accuracy
    across various NLP tasks such as text summarization, sentiment analysis, text
    classification, and question answering, showcasing the efficacy of our approach
    (refer to the generated images).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在 CNN/Daily Mail 数据集上进行微调，提升了以 Transformer 为基础的模型在总结方面的表现。该数据集被划分为针对每个特定任务的训练、验证和测试拆分。我们的摘要模块有效地压缩了训练和验证数据集中的上下文信息。软提示作为嵌入初始化于输入
    LLMs 中，并与模型参数同时训练，以优化任务特定的表现。使用任务特定的指标对 LLMs 的表现进行比较评估，无论是使用我们的方法干预还是不使用。提供的可视化图表突显了在文本总结、情感分析、文本分类和问答等各种
    NLP 任务中的准确性显著提升，展示了我们方法的有效性（参见生成的图像）。
- en: Additionally, our methodology significantly enhances computational efficiency,
    as evidenced by the dramatic reduction in processing times. For instance, our
    method reduces the processing time by up to 80.1 persent in the SQuAD2.0 dataset,
    and similarly substantial reductions are observed across other datasets including
    CNN/Daily Mail, SST-2, and AG News, as detailed in the accompanying table. These
    improvements in processing speed do not compromise the quality of outcomes, illustrating
    the synergy between soft prompts and summarization techniques in optimizing LLMs’
    performance. Potential areas for further inquiry may involve the refinement of
    soft prompt parameters and summarization algorithms [[7](#bib.bib7)]to bolster
    performance across a wider array of NLP tasks. Moreover, delving into the applicability
    of this methodology in multilingual settings [[10](#bib.bib10)] and diverse domains
    could yield valuable insights into its adaptability and global relevance. In summary,
    our research represents a substantial advancement in the realm of NLP, presenting
    a scalable and effective framework for augmenting the capabilities of LLMs. This
    advancement establishes a foundation for future innovations aimed at unleashing
    the full potential of LLMs in comprehending and processing intricate and extensive
    textual data. In summary, our research represents a substantial advancement in
    the realm of NLP, presenting a scalable and effective framework for augmenting
    the capabilities of LLMs. This advancement establishes a foundation for future
    innovations aimed at unleashing the full potential of LLMs in comprehending and
    processing intricate and extensive textual data. The efficiency gains, coupled
    with improved accuracy, position our method as a transformative force in the field,
    paving the way for more adaptable, faster, and precise language models.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的方法显著提高了计算效率，这从处理时间的显著减少中得到了证明。例如，我们的方法在SQuAD2.0数据集中的处理时间减少了高达80.1%，其他数据集如CNN/Daily
    Mail、SST-2和AG News也观察到了类似的显著减少，详见附表。这些处理速度的提升并未影响结果质量，展示了软提示与总结技术在优化LLMs性能方面的协同作用。进一步探讨的潜在领域可能包括软提示参数和总结算法的改进[[7](#bib.bib7)]，以提升在更多NLP任务中的表现。此外，*深入探讨*这种方法在多语言环境[[10](#bib.bib10)]和不同领域的适用性，可能会提供关于其适应性和全球相关性的宝贵见解。总之，我们的研究在NLP领域代表了重要的进展，提供了一个可扩展且有效的框架来增强LLMs的能力。这一进展为未来的创新奠定了基础，旨在释放LLMs在理解和处理复杂且广泛的文本数据中的**终极**潜力。效率的提升，加上更高的准确性，使我们的方法在该领域成为一种变革性力量，为更具适应性、更快速和更精准的语言模型铺平了道路。
- en: '|  | Claude2 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | Claude2 |'
- en: '| Cost | Original | Capsule Prompt | Save |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 成本 | 原始 | Capsule Prompt | 节省 |'
- en: '| Mail dataset | 12.33 | 3.37 | -77.9% |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 邮件数据集 | 12.33 | 3.37 | -77.9% |'
- en: '| SST-2 | 4.22 | 1.86 | -63.9% |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 | 4.22 | 1.86 | -63.9% |'
- en: '| AG News | 42.41 | 15.51 | -78.5% |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| AG News | 42.41 | 15.51 | -78.5% |'
- en: '| SQuAD2.0 | 2.14 | 0.42 | -80.1% |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| SQuAD2.0 | 2.14 | 0.42 | -80.1% |'
- en: '![Refer to caption](img/17f24bcb27fbb82e757e32c1b5bffb13.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/17f24bcb27fbb82e757e32c1b5bffb13.png)'
- en: 'Figure 3: comparison chart of processing times.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：处理时间对比图。
- en: '![Refer to caption](img/7071f7e1c04a0ca8ba0671fd51f34dff.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7071f7e1c04a0ca8ba0671fd51f34dff.png)'
- en: 'Figure 4: Performance difference compared to baseline model.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：与基准模型相比的性能差异。
- en: Conclusion
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: There is a citing studies that have demonstrated the effectiveness of summarization
    in enhancing model performance[[11](#bib.bib11)]. However, our comprehensive investigation
    introduces a novel methodology that synergistically amalgamates the functionalities
    of soft prompts, prompts formatted in natural language, and advanced summarization
    techniques to augment the efficacy and efficiency of Large Language Models (LLMs)
    in handling extensive textual contexts. Through empirical validation across a
    varied spectrum of NLP tasks, we have substantiated the substantial enhancements
    our approach provides in both context compression and model adaptability.For instance,
    processing times were reduced by up to 80.1 persent for tasks involving the SQuAD2.0
    dataset, with similar efficiencies noted across other datasets such as CNN/Daily
    Mail, SST-2, and AG News. This not only underscores the efficiency of our approach
    but also highlights its potential in making advanced NLP technologies more accessible
    and feasible in resource-constrained scenarios.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有研究引用了总结在提升模型性能方面的有效性[[11](#bib.bib11)]。然而，我们的综合调查引入了一种新颖的方法论，协同结合了软提示、格式化为自然语言的提示和先进的总结技术，以增强大语言模型（LLMs）处理广泛文本语境的效能和效率。通过在各种NLP任务中的实证验证，我们证明了我们的方法在语境压缩和模型适应性方面提供了显著的提升。例如，涉及SQuAD2.0数据集的任务处理时间减少了多达80.1%，其他数据集如CNN/Daily
    Mail、SST-2和AG News也观察到了类似的效率。这不仅强调了我们方法的效率，还突显了其在资源受限场景下使先进NLP技术更加可及和可行的潜力。
- en: The amalgamation of soft prompts with summary vectors, derived from prompts
    formatted in natural language, not only optimizes information compression but
    also conserves the utility of the original context. This dual emphasis ensures
    the retention of essential information while diminishing the computational overhead
    conventionally associated with processing lengthy texts. Furthermore, the adaptability
    of our methodology is underscored by its performance improvements across diverse
    NLP tasks, encompassing text summarization, sentiment analysis, text classification,
    and question answering. In light of these findings, our work not only contributes
    a significant leap forward in the field of NLP by enhancing the performance and
    efficiency of LLMs but also sets a new benchmark for future research in this area.
    The potential for our methodology to be extended and applied in multilingual contexts
    and across different domains offers exciting avenues for further exploration.
    As we continue to push the boundaries of what is possible with LLMs, our research
    lays the groundwork for a new era of NLP solutions that are more adaptable, efficient,
    and accessible than ever before.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 将软提示与总结向量结合，源自格式化为自然语言的提示，不仅优化了信息压缩，还保留了原始语境的实用性。这种双重强调确保了重要信息的保留，同时减少了处理冗长文本时传统上相关的计算开销。此外，我们的方法的适应性通过其在各种NLP任务中的性能提升得到了强调，包括文本总结、情感分析、文本分类和问答。鉴于这些发现，我们的工作不仅通过提升LLM的性能和效率在NLP领域取得了重大突破，还为未来研究设立了新的标杆。我们的方法在多语言环境和不同领域的扩展和应用潜力提供了令人兴奋的进一步探索方向。随着我们不断推动LLM的可能性界限，我们的研究为一个更加适应、高效和可及的新一代NLP解决方案奠定了基础。
- en: Our discoveries indicate that the fusion of soft prompts with advanced summarization
    techniques presents a promising avenue for future exploration aimed at enhancing
    the efficiency and adaptability of LLMs. This approach not only addresses the
    challenges associated with processing lengthy texts but also unveils new prospects
    for tailoring LLMs for specific applications sans the necessity for extensive
    retraining.This convergence of efficiency, adaptability, and reduced computational
    demand represents a paradigm shift in how LLMs can be optimized for a wide range
    of applications. Our findings advocate for a continued exploration of soft prompt
    tuning and advanced summarization techniques, suggesting that the future of NLP
    lies in the strategic integration of these methodologies to overcome the inherent
    limitations of current models. As we stand on the brink of this new frontier,
    our research illuminates the path forward, offering a blueprint for the next generation
    of language models that are not only more powerful but also more practical for
    real-world applications.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的发现表明，将软提示与先进的摘要技术结合起来，为未来探索提高大型语言模型（LLMs）效率和适应性提供了一个有前景的途径。这种方法不仅解决了处理长文本所面临的挑战，还揭示了为特定应用量身定制LLMs的新前景，而无需大量的重新训练。这种效率、适应性和降低计算需求的融合代表了LLMs优化广泛应用的范式转变。我们的研究结果提倡继续探索软提示调整和先进的摘要技术，表明自然语言处理的未来在于这些方法论的战略整合，以克服当前模型的固有局限性。当我们站在这一新前沿的边缘时，我们的研究照亮了前进的道路，为下一代语言模型提供了蓝图，这些模型不仅更强大，而且在实际应用中更具实用性。
- en: References
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Y. Su, X. Wang, Y. Qin, C.-M. Chan, Y. Lin, H. Wang, K. Wen, Z. Liu, P. Li,
    J. Li, L. Hou, M. Sun, and J. Zhou, “On transferability of prompt tuning for natural
    language processing,” in Proceedings of the 2022 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Association for Computational Linguistics, 2022.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y. Su, X. Wang, Y. Qin, C.-M. Chan, Y. Lin, H. Wang, K. Wen, Z. Liu, P.
    Li, J. Li, L. Hou, M. Sun 和 J. Zhou，“自然语言处理中的提示调整的迁移性研究，” 见《2022年北美计算语言学协会年会：人类语言技术论文集》，计算语言学协会，2022年。'
- en: '[2] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-document
    transformer,” 2020.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] I. Beltagy, M. E. Peters 和 A. Cohan，“Longformer: 长文档变换器，” 2020年。'
- en: '[3] M. Zaheer, G. Guruganesh, A. Dubey, J. Ainslie, C. Alberti, S. Ontanon,
    P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed, “Big bird: Transformers for
    longer sequences,” 2021.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. Zaheer, G. Guruganesh, A. Dubey, J. Ainslie, C. Alberti, S. Ontanon,
    P. Pham, A. Ravula, Q. Wang, L. Yang 和 A. Ahmed，“Big bird: 处理更长序列的变换器，” 2021年。'
- en: '[4] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts for
    generation,” 2021.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] X. L. Li 和 P. Liang，“前缀调整：优化生成的连续提示，” 2021年。'
- en: '[5] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for parameter-efficient
    prompt tuning,” 2021.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] B. Lester, R. Al-Rfou 和 N. Constant，“规模效应在参数高效提示调整中的力量，” 2021年。'
- en: '[6] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov,
    and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence pre-training for natural
    language generation, translation, and comprehension,” 2019.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov
    和 L. Zettlemoyer，“Bart: 通过去噪序列到序列预训练进行自然语言生成、翻译和理解，” 2019年。'
- en: '[7] C. Li, H. Zheng, Y. Sun, C. Wang, L. Yu, C. Chang, X. Tian, and B. Liu,
    “Enhancing multi-hop knowledge graph reasoning through reward shaping techniques,”
    arXiv preprint arXiv:2403.05801, 2024.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. Li, H. Zheng, Y. Sun, C. Wang, L. Yu, C. Chang, X. Tian 和 B. Liu，“通过奖励塑造技术增强多跳知识图推理，”
    arXiv预印本 arXiv:2403.05801, 2024年。'
- en: '[8] Y. Shen, K. Song, X. Tan, W. Zhang, K. Ren, S. Yuan, W. Lu, D. Li, and
    Y. Zhuang, “Taskbench: Benchmarking large language models for task automation,”
    2023.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Y. Shen, K. Song, X. Tan, W. Zhang, K. Ren, S. Yuan, W. Lu, D. Li 和 Y.
    Zhuang，“Taskbench: 大型语言模型任务自动化基准测试，” 2023年。'
- en: '[9] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford,
    D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, et al., “An empirical analysis
    of compute-optimal large language model training,” Advances in Neural Information
    Processing Systems, vol. 35, pp. 30016–30030, 2022.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford,
    D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark 等，“计算最优的大型语言模型训练的实证分析，” 《神经信息处理系统进展》，第35卷，第30016-30030页，2022年。'
- en: '[10] L. Shen, W. Tan, S. Chen, Y. Chen, J. Zhang, H. Xu, B. Zheng, P. Koehn,
    and D. Khashabi, “The language barrier: Dissecting safety challenges of llms in
    multilingual contexts,” arXiv preprint arXiv:2401.13136, 2024.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] L. Shen, W. Tan, S. Chen, Y. Chen, J. Zhang, H. Xu, B. Zheng, P. Koehn,
    和 D. Khashabi, “语言障碍：剖析多语言环境中 llms 的安全挑战，” arXiv 预印本 arXiv:2401.13136, 2024年。'
- en: '[11] M. Gambhir and V. Gupta, “Recent automatic text summarization techniques:
    a survey,” Artificial Intelligence Review, vol. 47, pp. 1–66, 2016.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] M. Gambhir 和 V. Gupta, “最近的自动文本摘要技术：综述，” 人工智能评论, 第47卷, 第1–66页, 2016年。'
