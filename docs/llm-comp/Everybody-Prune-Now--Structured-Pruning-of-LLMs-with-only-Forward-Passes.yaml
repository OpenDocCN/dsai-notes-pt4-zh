- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:04:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在大家都来修剪：仅通过前向传播进行LLM的结构化修剪
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.05406](https://ar5iv.labs.arxiv.org/html/2402.05406)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.05406](https://ar5iv.labs.arxiv.org/html/2402.05406)
- en: Lucio Dery    Steven Kolawole    Jean-François Kagy    Virginia Smith    Graham
    Neubig    Ameet Talwalkar
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Lucio Dery    Steven Kolawole    Jean-François Kagy    Virginia Smith    Graham
    Neubig    Ameet Talwalkar
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Given the generational gap in available hardware between lay practitioners and
    the most endowed institutions, LLMs are becoming increasingly inaccessible as
    they grow in size. Whilst many approaches have been proposed to compress LLMs
    to make their resource consumption manageable, these methods themselves tend to
    be resource intensive, putting them out of the reach of the very user groups they
    target. In this work, we explore the problem of structured pruning of LLMs using
    only forward passes. We seek to empower practitioners to prune models so large
    that their available hardware has just enough memory to run inference. We develop
    Bonsai, a gradient-free, perturbative pruning method capable of delivering small,
    fast, and accurate pruned models. We observe that Bonsai outputs pruned models
    that (i) outperform those generated by more expensive gradient-based structured
    pruning methods, and (ii) are twice as fast (with comparable accuracy) as those
    generated by semi-structured pruning methods requiring comparable resources as
    Bonsai. We also leverage Bonsai to produce a new sub-2B model using a single A6000
    that yields state-of-the-art performance on 4/6 tasks on the Huggingface Open
    LLM leaderboard.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到普通用户与最具资源的机构之间在硬件上的代际差距，LLM的规模不断增长，使其变得越来越不可及。虽然许多方法已被提出以压缩LLM以便于资源管理，但这些方法本身往往资源密集，使其无法达到目标用户群体。本文探讨了仅使用前向传播进行LLM的结构化修剪的问题。我们旨在赋能从业者修剪那些模型大到可用硬件只有足够内存用于推断的情况。我们开发了Bonsai，这是一种无梯度的扰动修剪方法，能够提供小巧、快速且准确的修剪模型。我们观察到Bonsai输出的修剪模型（i）在性能上优于那些由更昂贵的基于梯度的结构化修剪方法生成的模型，且（ii）在速度上是半结构化修剪方法的两倍（准确度相当），这些半结构化修剪方法需要的资源与Bonsai相当。我们还利用Bonsai使用单个A6000生产了一个新的小于2B的模型，该模型在Huggingface
    Open LLM排行榜上的4/6任务中表现出色。
- en: '[Code for Bonsai can be found here](https://github.com/ldery/Bonsai)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[Bonsai的代码可以在这里找到](https://github.com/ldery/Bonsai)'
- en: Machine Learning, ICML\addauthor
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML\addauthor
- en: gnmagenta \addauthoratblue
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: gnmagenta \addauthoratblue
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: As large language models (LLMs) (OpenAI et al., [2023](#bib.bib38); Touvron
    et al., [2023](#bib.bib45); Gemini Team et al., [2023](#bib.bib13)) continue to
    grow in size, the gap between models that achieve state-of-the-art performance
    and those that every-day machine learning (ML) practitioners can feasibly run
    on their available hardware continues to widen (Bender et al., [2021](#bib.bib5);
    Samsi et al., [2023](#bib.bib40)). With the goal of democratizing access to these
    powerful models, previous research has proposed approaches such as pruning (Xia
    et al., [2022](#bib.bib48); Sun et al., [2023](#bib.bib44)), distillation (Hinton
    et al., [2015](#bib.bib18); Gu et al., [2023](#bib.bib14)) and quantization (Xiao
    et al., [2023](#bib.bib49)) to create smaller models from larger pre-trained language
    models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）(OpenAI等，[2023](#bib.bib38)；Touvron等，[2023](#bib.bib45)；Gemini团队等，[2023](#bib.bib13)）规模的不断增长，实现最先进性能的模型与普通机器学习（ML）从业者能够在其可用硬件上实际运行的模型之间的差距也在不断扩大（Bender等，[2021](#bib.bib5)；Samsi等，[2023](#bib.bib40)）。为了使这些强大的模型能够被更多人使用，之前的研究提出了修剪（Xia等，[2022](#bib.bib48)；Sun等，[2023](#bib.bib44)）、蒸馏（Hinton等，[2015](#bib.bib18)；Gu等，[2023](#bib.bib14)）和量化（Xiao等，[2023](#bib.bib49)）等方法来从较大的预训练语言模型中创建较小的模型。
- en: '![Refer to caption](img/7cb706285e7f938a089f99d3ee179841.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7cb706285e7f938a089f99d3ee179841.png)'
- en: 'Figure 1: Perplexity versus inference speed-up of pruned models for methods
    that use only forward passes on the parent model. At any given target perplexity,
    Bonsai produces the fastest model, resulting in improved latency and throughput.
    Post-pruning adaptation is only possible for models below a certain size, given
    the available hardware. Circle sizes are proportional to the model’s memory footprint.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：修剪模型在仅使用前向传播的方法下的困惑度与推断加速比。对于任何给定的目标困惑度，Bonsai生成的模型速度最快，带来了更好的延迟和吞吐量。后修剪适应仅可能对小于某个大小的模型进行，考虑到可用的硬件。圆圈大小与模型的内存占用成正比。
- en: 'Table 1: Landscape of resource consumption (memory and compute) of different
    model compression methods at training time and the inference time resource consumption
    of the models they deliver. ✗  means the method incurs a prohibitive cost to the
    lay practitioner whilst ✓  denotes that it is a viable option with respect to
    that resource.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同模型压缩方法在训练时间的资源消耗（内存和计算）以及它们生成模型在推理时间的资源消耗。✗ 代表该方法在训练时对普通从业者来说成本过高，而✓ 表示该方法在该资源方面是一个可行的选项。
- en: '| Regime | Resource | Approaches |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 方案 | 资源 | 方法 |'
- en: '|  |  | Quantization (Mixed Precision) | Distillation | Unstructured Pruning
    | Gradient-Based Structured Pruning | Bonsai (Ours) |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 量化（混合精度） | 蒸馏 | 无结构剪枝 | 基于梯度的结构化剪枝 | Bonsai（我们的方法） |'
- en: '| Train | Memory | ✓ | ✓ | ✓ | ✗ | ✓ |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 内存 | ✓ | ✓ | ✓ | ✗ | ✓ |'
- en: '|  | Compute | ✓ | ✗ | ✓ | ✓ | ✓ |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|  | 计算 | ✓ | ✗ | ✓ | ✓ | ✓ |'
- en: '| Inference | Memory | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 内存 | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: '|  | Compute | ✗ | ✓ | ✗ | ✓ | ✓ |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  | 计算 | ✗ | ✓ | ✗ | ✓ | ✓ |'
- en: 'Unfortunately, so far, these methods have fallen short of the goal of truly
    democratizing access to LLMs. In the process of producing a smaller model out
    of an LLM, methods such as distillation and structured pruning are inaccessible
    to the everyday practitioner due to their prohibitive resource consumption at
    training time. Specifically, pure distillation-based techniques require running
    LLMs to generate large amounts of teacher data (Jiao et al., [2019](#bib.bib22);
    Hsieh et al., [2023](#bib.bib19)) whilst existing structured pruning approaches
    like LLM-Pruner (Ma et al., [2023](#bib.bib31)) and LoRAPrune (Zhang et al., [2023](#bib.bib52))
    require several times more memory than is needed to run inference on the model
    being pruned. Though unstructured pruning (Frantar & Alistarh, [2023](#bib.bib11);
    Sun et al., [2023](#bib.bib44)) and quantization are less restrictive at training
    time, the models they produce are not faster except in the presence of specialized
    hardware for the former (Mishra et al., [2021](#bib.bib36)), whilst the latter
    can actually slow down inference due to added overhead (Dettmers et al., [2022](#bib.bib9)).
    This limits the usefulness of these options for practitioners who are concerned
    with latency-critical applications. Table [1](#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") summarizes
    the landscape of existing methods and their limitations.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '不幸的是，到目前为止，这些方法还未能实现真正的语言模型普及。在从大型语言模型中生成较小模型的过程中，由于其训练时的高资源消耗，诸如蒸馏和结构化剪枝的方法对普通从业者而言是不可及的。具体而言，纯粹的蒸馏技术需要运行大型语言模型以生成大量的教师数据（Jiao
    et al., [2019](#bib.bib22); Hsieh et al., [2023](#bib.bib19)），而现有的结构化剪枝方法，如 LLM-Pruner
    (Ma et al., [2023](#bib.bib31)) 和 LoRAPrune (Zhang et al., [2023](#bib.bib52))，则需要比运行被剪枝模型推理所需的内存多几倍。尽管无结构剪枝
    (Frantar & Alistarh, [2023](#bib.bib11); Sun et al., [2023](#bib.bib44)) 和量化在训练时限制较少，但除非使用专用硬件（Mishra
    et al., [2021](#bib.bib36)），否则它们生成的模型不会更快，而量化则可能由于额外开销而实际减慢推理速度（Dettmers et al.,
    [2022](#bib.bib9)）。这限制了这些选项对于关注延迟关键应用的从业者的实用性。表 [1](#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") 总结了现有方法的情况及其局限性。'
- en: 'We aim to empower ML practitioners to compress LLMs *by themselves* using their
    available resources whilst still producing accurate yet fast and compact models.
    To this end, we propose a novel memory-friendly structured pruning method. We
    observe that the significant memory overhead of prior structured pruning methods
    chiefly comes from having to perform gradient-based optimization: a backward pass
    requires $\gtrapprox 2\times$. To capture the widest range of memory budgets available
    to practitioners, we focus on developing an approach for the following concrete
    setting:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是使机器学习从业者能够利用现有资源*自行*压缩大型语言模型，同时仍能生成准确、快速且紧凑的模型。为此，我们提出了一种新颖的内存友好型结构化剪枝方法。我们观察到，先前结构化剪枝方法的显著内存开销主要来自于梯度优化的执行：一次反向传播需要$\gtrapprox
    2\times$。为了捕捉从业者可用的最广泛内存预算，我们专注于为以下具体设置开发一种方法：
- en: '*The practitioner only has enough memory on their hardware to run inference
    on the model to be pruned.¹¹1Specifically we assume a forward pass with a batch
    size of at least 1\. Given the hardware and open source LLM landscape as of January
    2024, for an Nvidia A6000 (48GB) GPU, the largest (full-precision) model this
    would be applicable to is LLaMA-7B*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*实践者的硬件上只有足够的内存来运行要剪枝的模型的推断。¹¹1具体来说，我们假设前向传递的批量大小至少为1。考虑到截至2024年1月的硬件和开源LLM环境，对于Nvidia
    A6000（48GB）GPU，适用的最大（全精度）模型是LLaMA-7B*'
- en: The above setting is evergreen. As state-of-the-art models become more compute
    intensive over time, the generational gap in hardware available to the lay practitioner
    versus the most resource endowed institutions is expected to persist or possibly
    widen.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 上述设置是持久的。随着最先进的模型变得越来越计算密集，普通实践者与资源最丰富的机构之间的硬件代差预计将持续或可能扩大。
- en: In light of the proposed setting, we present Bonsai, a forward pass-only structured
    pruning approach that is capable of delivering fast, compact, and accurate pruned
    models under the memory limitations that are typical of consumer hardware. To
    decide which modules (attention head, rows in feedforward projection, etc.) of
    the LLM to prune, Bonsai estimates module importances perturbatively by generating
    sub-models and evaluating their performance (running inference). We make this
    approach tractable by contributing multiple techniques.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于上述设置，我们提出了Bonsai，一种仅依赖前向传递的结构化剪枝方法，能够在典型消费级硬件的内存限制下提供快速、紧凑和准确的剪枝模型。为了决定剪枝LLM的哪些模块（注意力头、前馈投影中的行等），Bonsai通过生成子模型并评估其性能（运行推断）来扰动地估计模块重要性。我们通过贡献多种技术使这种方法具有可行性。
- en: 'First, we treat the problem of inferring each module’s importance from the
    performance of generated sub-models as an under-determined regression problem.
    This enables us to estimate the importance of a large number of modules by exploring
    a manageable number of random sub-models. This is unlike past perturbative approaches,
    which prohibitively require roughly as many sub-models as there are modules to
    select from (Ancona et al., [2020](#bib.bib1)), making them intractable for LLMs.
    Next, instead of instantiating sub-models by dropping modules with equal likelihood
    (Kang et al., [2023](#bib.bib23)), we use informative priors derived from work
    on unstructured pruning (Han et al., [2015](#bib.bib15); Sun et al., [2023](#bib.bib44)).
    We thus obtain better estimates of module relevance with fewer evaluated sub-models.
    Finally, unlike past gradient-free approaches that greedily make pruning decisions
    layer-by-layer (Dekhovich et al., [2021](#bib.bib8); Nova et al., [2023](#bib.bib37);
    Sun et al., [2023](#bib.bib44)), Bonsai takes a holistic view to preserve the
    accuracy of the pruned model: modules across layers are removed and evaluated
    together and relevance scores are computed globally to make pruning decisions.
    To the best of our knowledge, these ingredients taken together make Bonsai the
    first successful attempt at scaling gradient-free structured pruning to LLMs.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从生成的子模型的性能中推断每个模块的重要性的问题视为一个欠定回归问题。这使我们能够通过探索可管理数量的随机子模型来估计大量模块的重要性。这与过去的扰动方法不同，后者需要大致与模块数量相等的子模型进行选择（Ancona
    et al., [2020](#bib.bib1)），使得在大规模语言模型（LLM）中不可行。接下来，我们不通过等概率地丢弃模块来实例化子模型（Kang et
    al., [2023](#bib.bib23)），而是使用从非结构化剪枝工作中得出的信息先验（Han et al., [2015](#bib.bib15);
    Sun et al., [2023](#bib.bib44)）。因此，我们在评估较少的子模型时获得了更好的模块相关性估计。最后，与过去那些贪婪地逐层进行剪枝决策的无梯度方法不同（Dekhovich
    et al., [2021](#bib.bib8); Nova et al., [2023](#bib.bib37); Sun et al., [2023](#bib.bib44)），Bonsai采取整体视角来保持剪枝模型的准确性：跨层的模块一起被移除和评估，相关性得分在全局计算以做出剪枝决策。据我们所知，这些因素使Bonsai成为首次成功尝试将无梯度结构化剪枝扩展到LLMs的方法。
- en: We conduct several experiments that demonstrate Bonsai’s efficacy. Bonsai, which
    uses only forward passes for pruning, achieves comparable performance to 2:4 semi-structured
    sparsity with Wanda (Sun et al., [2023](#bib.bib44)) ($0.75\times$1.8B model that
    outperforms the best sub-2B parameter model on the Huggingface Open LLM leaderboard
    on 4 out of 6 tasks as of the time of submission. Based on these strong results
    and its usability under real-world memory constraints, we view Bonsai as a significant
    contribution to unlocking the power of LLMs for a broader spectrum of practitioners
    facing diverse hardware constraints.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了几次实验，展示了 Bonsai 的有效性。Bonsai 仅使用前向传递进行剪枝，其性能与 2:4 半结构化稀疏度的 Wanda (Sun et al.,
    [2023](#bib.bib44)) 相当（$0.75\times$1.8B 模型在提交时在 Huggingface Open LLM 排行榜上 6 项任务中的
    4 项超越了最佳的子 2B 参数模型。基于这些强劲的结果以及其在现实世界内存限制下的可用性，我们认为 Bonsai 是解锁 LLM 能力的一个重要贡献，以便面对多样化硬件限制的更广泛的从业者。
- en: 2 Background on Pruning, Problem Definition and Notation Setup
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 剪枝背景、问题定义和符号设置
- en: We are given an LLM, $\mathbf{M}_{\theta}$ on available hardware, pruning can
    be critical for achieving latency targets, reducing compute burden, or making
    the model small enough to adapt to new (out-of-domain) tasks by gradient-based
    fine-tuning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在可用硬件上给定一个 LLM，即 $\mathbf{M}_{\theta}$，剪枝对实现延迟目标、减少计算负担或使模型足够小以通过基于梯度的微调适应新的（域外）任务至关重要。
- en: Unstructured pruning approaches compress $\mathbf{M}_{\theta}$ from the model.
    This results in the updated model consisting of sparsified weight matrices with
    a smaller memory footprint. Unfortunately, the updated model does not enjoy inference
    speedups except when specialized hardware is available and thus poses a compute
    burden during inference. Whilst semi-structured variants – those that remove parameters
    in patterns like 2:4 or 4:8 (Mishra et al., [2021](#bib.bib36)) – achieve some
    speedup, these are modest compared to those achieved with structured pruning.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 无结构剪枝方法从模型中压缩 $\mathbf{M}_{\theta}$。这导致更新后的模型由稀疏化的权重矩阵组成，具有更小的内存占用。不幸的是，更新后的模型除非有专用硬件，否则无法享受推理加速，从而在推理过程中带来了计算负担。虽然半结构化变体——即那些以
    2:4 或 4:8 的模式移除参数的变体（Mishra 等，[2021](#bib.bib36)）——实现了一些加速，但这些加速与结构化剪枝所实现的加速相比是适中的。
- en: 'Structured pruning takes a more modular view of the units to be removed from
    $\mathbf{M}_{\theta}$, structured pruning can be cast as the following combinatorial
    optimization problem:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化剪枝对待从 $\mathbf{M}_{\theta}$ 中移除的单元采取更模块化的视角，结构化剪枝可以被视为以下组合优化问题：
- en: '|  | $\begin{split}\mathbf{m}^{\ast}&amp;=\mathrm{argmax}_{\bar{\mathbf{m}}\in\mathcal{F}_{p}}\quad
    U\left(\mathbf{M}_{&#124;\bar{\mathbf{m}}}\right)\qquad\text{where}\\ \mathcal{F}_{p}&amp;=\bigg{\{}\bar{\mathbf{m}}\subseteq\mathbf{m}~{}\bigg{&#124;}~{}\bigg{(}\sum_{[j:m_{j}\in\bar{\mathbf{m}}]}s_{j}\bigg{)}\leq(1-p)D\bigg{\}}\end{split}$
    |  | (1) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathbf{m}^{\ast}&amp;=\mathrm{argmax}_{\bar{\mathbf{m}}\in\mathcal{F}_{p}}\quad
    U\left(\mathbf{M}_{&#124;\bar{\mathbf{m}}}\right)\qquad\text{其中}\\ \mathcal{F}_{p}&amp;=\bigg{\{}\bar{\mathbf{m}}\subseteq\mathbf{m}~{}\bigg{&#124;}~{}\bigg{(}\sum_{[j:m_{j}\in\bar{\mathbf{m}}]}s_{j}\bigg{)}\leq(1-p)D\bigg{\}}\end{split}$
    |  | (1) |'
- en: $\mathcal{F}_{p}$, it is also faster to run inference on it since it has fewer
    modules.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{F}_{p}$，因为它具有更少的模块，所以推理运行速度也更快。
- en: 'Many structured pruning methods attempt to solve Equation [1](#S2.E1 "Equation
    1 ‣ 2 Background on Pruning, Problem Definition and Notation Setup ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") by gradient-guided
    optimization (or search) over the space of sub-models. However, since we are interested
    in the memory-constrained setting where computing gradients is not feasible, these
    methods cannot be used. We will thus focus on developing a memory-friendly structured
    pruning technique, but we will compare to semi-structured pruning methods since
    they also have minimal memory overhead.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 许多结构化剪枝方法试图通过对子模型空间进行梯度引导优化（或搜索）来解决方程 [1](#S2.E1 "方程 1 ‣ 2 剪枝背景、问题定义和符号设置 ‣
    现在都剪枝：仅使用前向传递的 LLM 结构化剪枝")。然而，由于我们感兴趣的是内存受限的设置，其中计算梯度不可行，因此这些方法不能使用。因此，我们将专注于开发一种内存友好的结构化剪枝技术，但我们将与半结构化剪枝方法进行比较，因为它们也具有最小的内存开销。
- en: 3 Methodology
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: In this section, we will develop a structured pruning algorithm that relies
    exclusively on forward passes through the parent model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将开发一个仅依赖于通过父模型的前向传递的结构化剪枝算法。
- en: 3.1 Estimating module relevance with only forward passes
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 仅使用前向传递估计模块相关性
- en: 'We have motivated the setting of pruning a model that is so large (relative
    to the amount of memory available to the practitioner), such that we can only
    run forward passes through it. This means that we have to solve Equation [1](#S2.E1
    "Equation 1 ‣ 2 Background on Pruning, Problem Definition and Notation Setup ‣
    Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") by
    relying on only evaluations of $U$ subsets.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们动机设定为剪枝一个如此大的模型（相对于从业者可用的内存量），以至于我们只能通过它进行前向传播。这意味着我们必须依赖 $U$ 子集的评估来解决方程[1](#S2.E1
    "方程 1 ‣ 2 剪枝背景、问题定义和符号设置 ‣ 现在每个人都剪枝：仅使用前向传播的 LLM 结构化剪枝")。
- en: 'We propose a computationally tractable approach where we first perform a small
    number, $n$ modules. We can generate an approximate solution to Equation [1](#S2.E1
    "Equation 1 ‣ 2 Background on Pruning, Problem Definition and Notation Setup ‣
    Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种计算上可行的方法，其中我们首先进行少量的 $n$ 模块。我们可以生成方程[1](#S2.E1 "方程 1 ‣ 2 剪枝背景、问题定义和符号设置
    ‣ 现在每个人都剪枝：仅使用前向传播的 LLM 结构化剪枝") 的近似解：
- en: '|  | $\mathbf{m}^{\ast}\approx\mathbf{m}^{\mathrm{approx}}=\mathrm{argmax}_{\bar{\mathbf{m}}\in\mathcal{F}_{p}}\quad\sum_{j\in\bar{\mathbf{m}}}\beta_{j}$
    |  | (2) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{m}^{\ast}\approx\mathbf{m}^{\mathrm{approx}}=\mathrm{argmax}_{\bar{\mathbf{m}}\in\mathcal{F}_{p}}\quad\sum_{j\in\bar{\mathbf{m}}}\beta_{j}$
    |  | (2) |'
- en: 'We note that Equation [2](#S3.E2 "Equation 2 ‣ 3.1 Estimating module relevance
    with only forward passes ‣ 3 Methodology ‣ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes") is straightforward to solve, as it simply requires
    sorting $\beta_{j}$ for our settings of interest, the difference is not significant).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到方程[2](#S3.E2 "方程 2 ‣ 3.1 仅使用前向传播估计模块相关性 ‣ 3 方法论 ‣ 现在每个人都剪枝：仅使用前向传播的 LLM
    结构化剪枝")很容易解决，因为它仅需对 $\beta_{j}$ 进行排序，对于我们感兴趣的设置，差异并不显著。
- en: 'Estimating $\mathbf{\beta}$:'
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 估计 $\mathbf{\beta}$：
- en: 'To obtain estimates of the module relevance scores $\mathbf{\beta}=\{\beta_{i}\}_{i\in[N]}$
    as an under-specified regression problem :'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取模块相关性评分的估计值 $\mathbf{\beta}=\{\beta_{i}\}_{i\in[N]}$ 作为一个欠指定的回归问题：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: where $\alpha_{\bar{\mathbf{m}}_{k}}\in\mathbb{R}^{N}~{}\big{|}~{}\left(\alpha_{\bar{\mathbf{m}}_{k}}\right)_{i}=\mathbf{1}[i\in\bar{\mathbf{m}}_{k}]$,
    is the binary vector that has 0 at indices where modules have been dropped.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha_{\bar{\mathbf{m}}_{k}}\in\mathbb{R}^{N}~{}\big{|}~{}\left(\alpha_{\bar{\mathbf{m}}_{k}}\right)_{i}=\mathbf{1}[i\in\bar{\mathbf{m}}_{k}]$，是一个二进制向量，在模块被剪掉的位置为0。
- en: Implementing a sub-model $\bar{\mathbf{m}}_{k}$ is key to practically realizing
    our approach. We never actually fully instantiate sub-models as this would be
    prohibitively expensive. Instead, we create sub-models *virtually* by zeroing
    out the outputs of the components to be pruned so they have no effect on the final
    model output.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 实现子模型 $\bar{\mathbf{m}}_{k}$ 是实际实现我们方法的关键。我们实际上从未完全实例化子模型，因为这将会非常昂贵。相反，我们通过将要剪枝的组件的输出置为零来*虚拟*创建子模型，使它们对最终模型输出没有影响。
- en: 3.2 Selecting sub-models for evaluation
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 选择用于评估的子模型
- en: An as yet unexplored design choice is how to choose the $n$ are accurate and
    useful.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个尚未探索的设计选择是如何选择 $n$ 使其准确且有用。
- en: 'Given a module $m_{i}$ is left unpruned would be:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个模块 $m_{i}$ 被保留未剪枝的情况如下：
- en: '|  | $\rho_{i}\propto\hat{\mathbf{a}}_{i}=\frac{1}{B}\sum_{b}\bigg{&#124;}\sigma\bigg{(}\big{(}W^{T}[i,:]\big{)}x_{b}\bigg{)}\bigg{&#124;}$
    |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\rho_{i}\propto\hat{\mathbf{a}}_{i}=\frac{1}{B}\sum_{b}\bigg{|}\sigma\bigg{(}\big{(}W^{T}[i,:]\big{)}x_{b}\bigg{)}\bigg{|}$
    |  |'
- en: $\sigma\text{ is the nonlinearity}$ which allows us to respect memory constraints.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: $\sigma\text{ 是非线性函数}$ 这使我们能够尊重内存限制。
- en: To enhance the efficiency of our method, instead of considering all active modules
    for pruning, in each layer we consider the bottom $2p$ fraction of entries). Covert
    & Lee ([2020](#bib.bib7)) show that this technique can help reduce the variance
    of the estimator obtained from regression with binary inputs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高我们方法的效率，我们在每一层中考虑底部 $2p$ 分数，而不是考虑所有活动模块进行剪枝。Covert & Lee ([2020](#bib.bib7))
    显示这种技术可以帮助减少来自回归的二进制输入的估计量的方差。
- en: Algorithm 1 Bonsai
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 Bonsai
- en: '1:  Input: Model [$\mathbf{M}_{\theta}$'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入：模型 [$\mathbf{M}_{\theta}$'
- en: 3.3 Iterated Pruning
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 迭代剪枝
- en: Previous work on gradient-based pruning (Anwar et al., [2017](#bib.bib2); Frankle
    & Carbin, [2018](#bib.bib10)) have shown that taking an iterated approach to pruning
    yields improved results over pruning directly to the target sparsity $p$ sub-models
    to evaluate.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以前关于基于梯度的修剪的工作（Anwar et al., [2017](#bib.bib2)；Frankle & Carbin, [2018](#bib.bib10)）已经显示，采取迭代方法进行修剪比直接修剪到目标稀疏度$p$的子模型得到的结果更好。
- en: 'We combine the recipes developed in Sections [3.1](#S3.SS1 "3.1 Estimating
    module relevance with only forward passes ‣ 3 Methodology ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes"), [3.2](#S3.SS2 "3.2 Selecting
    sub-models for evaluation ‣ 3 Methodology ‣ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes") and [3.3](#S3.SS3 "3.3 Iterated Pruning ‣ 3
    Methodology ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward
    Passes") together to produce Bonsai⁴⁴4Structural pruning is a canonical way of
    giving a bonsai tree its shape hence the name., our gradient-free structural pruning
    algorithm. Algorithm [1](#alg1 "Algorithm 1 ‣ 3.2 Selecting sub-models for evaluation
    ‣ 3 Methodology ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward
    Passes") specifies Bonsai in detail.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将第[3.1](#S3.SS1 "3.1 仅通过前向传递估计模块相关性 ‣ 3 方法论 ‣ 大家现在修剪：仅通过前向传递的LLM结构化修剪")、[3.2](#S3.SS2
    "3.2 选择用于评估的子模型 ‣ 3 方法论 ‣ 大家现在修剪：仅通过前向传递的LLM结构化修剪")和[3.3](#S3.SS3 "3.3 迭代修剪 ‣
    3 方法论 ‣ 大家现在修剪：仅通过前向传递的LLM结构化修剪")节中开发的方案结合在一起，生产Bonsai⁴⁴4结构化修剪是一种经典的方式来塑造盆景树，因此得名。我们的无梯度结构化修剪算法。算法[1](#alg1
    "Algorithm 1 ‣ 3.2 选择用于评估的子模型 ‣ 3 方法论 ‣ 大家现在修剪：仅通过前向传递的LLM结构化修剪")详细说明了Bonsai。
- en: 4 Post-pruning adaptation
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 修剪后的适应
- en: 'Depending on the sparsity level $p$, it is possible to obtain a pruned model
    on which it is feasible to run a parameter-efficient finetuning method like LoRA
    (Hu et al., [2021](#bib.bib20)) with the available hardware memory. In this case,
    we can fine-tune the pruned model (result of Algorithm [1](#alg1 "Algorithm 1
    ‣ 3.2 Selecting sub-models for evaluation ‣ 3 Methodology ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes")) on the downstream task
    in order to recover some of the parent model performance that was degraded by
    pruning.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 根据稀疏度水平$p$，可以获得一个修剪后的模型，在该模型上可以使用像LoRA（Hu et al., [2021](#bib.bib20)）这样的参数高效微调方法，在可用的硬件内存下运行。在这种情况下，我们可以对修剪后的模型（算法[1](#alg1
    "Algorithm 1 ‣ 3.2 选择用于评估的子模型 ‣ 3 方法论 ‣ 大家现在修剪：仅通过前向传递的LLM结构化修剪")的结果）进行下游任务的微调，以恢复修剪过程中损失的一部分父模型性能。
- en: 'Like many past works (Sanh et al., [2020](#bib.bib42); Xia et al., [2022](#bib.bib48)),
    we combine pruning with distillation by incorporating a distillation loss in the
    training objective during fine-tuning of the pruned model. Let $\mathcal{L}_{\mathrm{task}}$
    to index the task data, we have:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 像许多过去的工作（Sanh et al., [2020](#bib.bib42)；Xia et al., [2022](#bib.bib48)）一样，我们通过在修剪后的模型微调过程中加入蒸馏损失，将修剪与蒸馏结合起来。设$\mathcal{L}_{\mathrm{task}}$为任务数据的索引，我们有：
- en: $\mathcal{L}_{\mathrm{distill}}=\sum_{i}D_{\mathrm{KL}}\bigg{(}\mathrm{logits}^{i}\left(\mathbf{M}_{|\mathbf{m}^{\mathrm{approx}}}\right)~{}\|~{}\mathrm{logits}^{i}\left({\mathbf{M}}\right)\bigg{)}$
    instead of hosting the model in memory during fine-tuning.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用$\mathcal{L}_{\mathrm{distill}}=\sum_{i}D_{\mathrm{KL}}\bigg{(}\mathrm{logits}^{i}\left(\mathbf{M}_{|\mathbf{m}^{\mathrm{approx}}}\right)~{}\|~{}\mathrm{logits}^{i}\left({\mathbf{M}}\right)\bigg{)}$，而不是在微调过程中将模型保存在内存中。
- en: 'Table 2: Wikitext-2 perplexity for 50% sparsity of LLaMA-2 7B with end-to-end
    latency speedups (relative to LLaMA-2 7B). We use Phi-2 (Li et al., [2023](#bib.bib28))
    at the target size as a strong model for comparison. For semi-structured (Wanda
    2:4) pruning, after fine-tuning, the learned LoRA weights cannot be merged with
    the primary model weights else the model reverts back to being dense; this leads
    to the reported slow downs. All methods use forward passes only on the parent
    model.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：LLaMA-2 7B 50% 稀疏度的Wikitext-2困惑度与端到端延迟加速（相对于LLaMA-2 7B）。我们使用Phi-2（Li et al.,
    [2023](#bib.bib28)）作为目标大小的强模型进行比较。对于半结构化（Wanda 2:4）修剪，在微调后，学习到的LoRA权重不能与主模型权重合并，否则模型会恢复为密集型；这导致了报告的速度下降。所有方法仅在父模型上使用前向传递。
- en: '| Model | $\sim$Size | Fine-tune | PPL | Speedup |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | $\sim$大小 | 微调 | PPL | 加速 |'
- en: '| Phi-2 | 3B | ✓ | $8.69$ |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Phi-2 | 3B | ✓ | $8.69$ |'
- en: '| LLaMA-2 7B Pruned |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2 7B 修剪 |'
- en: '| Wanda 2:4 | 3B | ✗ | $10.52$ |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Wanda 2:4 | 3B | ✗ | $10.52$ |'
- en: '|  |  | ✓ | $8.34$ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ✓ | $8.34$ |'
- en: '| Bonsai | 3B | ✓ | $8.89$ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Bonsai | 3B | ✓ | $8.89$ |'
- en: 5 Experimental Details
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验细节
- en: All experiments are conducted on a single NVIDIA A6000 (48Gb) GPU. In all experiments
    with Bonsai, we prune (1) the heads in the self-attention layers (2) the dimensions
    of the fully connected layers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验均在单个NVIDIA A6000（48Gb）GPU上进行。在所有Bonsai实验中，我们剪枝（1）自注意力层中的头部（2）全连接层的维度。
- en: 'Comparing Forward Pass Only Methods: We focus our first set of experiments
    on comparing methods that can be run without gradient-based optimization. We consider
    pruning the LLaMA-2 7B model (Touvron et al., [2023](#bib.bib45)) to 50% sparsity.
    We evaluate on the Wikitext-2 (Merity et al., [2016](#bib.bib34)) validation dataset
    and so our signal for pruning, $U$, is the language modelling performance on the
    training set. When measuring speedups, we consider *end-to-end latency* of running
    inference on model.sequence_length chunks of the Wikitext-2 validation set. See
    Appendix [A.2](#A1.SS2 "A.2 Forward Pass Only Experiments ‣ Appendix A Main Experiment
    Details ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes")
    for details about the hyper-parameters used for all methods in this experiment
    and for specifics about fine-tuning.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '比较仅前向传递方法：我们将第一组实验重点放在比较无需基于梯度的优化即可运行的方法上。我们考虑将LLaMA-2 7B模型（Touvron等，[2023](#bib.bib45)）剪枝到50%稀疏性。我们在Wikitext-2（Merity等，[2016](#bib.bib34)）验证数据集上进行评估，因此我们的剪枝信号$U$是训练集上的语言建模性能。在测量加速时，我们考虑在模型.sequence_length块的Wikitext-2验证集上运行推理的*端到端延迟*。有关此实验中所有方法使用的超参数以及微调的具体细节，请参见附录[A.2](#A1.SS2
    "A.2 Forward Pass Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes")。'
- en: 'Comparing Structured Pruning Approaches: We compare Bonsai to the following
    gradient-based structured pruning approaches: LLM-Pruner (Ma et al., [2023](#bib.bib31))
    and LoRA-Prune (Zhang et al., [2023](#bib.bib52)). We prune the LLaMA-1 7B model
    (Touvron et al., [2023](#bib.bib45)) to 50% sparsity since this was the model
    version available at time of release of the above methods. We compare these methods
    on Wikitext-2 and also on 6 tasks from the Eleuther LLM Evaluation Harness (Gao
    et al., [2023](#bib.bib12)). Pruning signal for Wikitext-2 task is the same as
    the above experiment. For the Eleuther Harness tasks, we use language modelling
    performance on the C4 (Raffel et al., [2020](#bib.bib39)) dataset as pruning signal.
    We also do parameter efficient finetuning on our pruned model with 30K 512-length
    sequences from this corpus. Specific hyper-parameters for Bonsai for this experiment
    can be found in Appendix [A.3](#A1.SS3 "A.3 Experiments comparing to Gradient
    based structured pruning ‣ Appendix A Main Experiment Details ‣ Everybody Prune
    Now: Structured Pruning of LLMs with only Forward Passes").'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '比较结构化剪枝方法：我们将Bonsai与以下基于梯度的结构化剪枝方法进行比较：LLM-Pruner（Ma等，[2023](#bib.bib31)）和LoRA-Prune（Zhang等，[2023](#bib.bib52)）。我们将LLaMA-1
    7B模型（Touvron等，[2023](#bib.bib45)）剪枝到50%稀疏性，因为这是上述方法发布时可用的模型版本。我们在Wikitext-2和Eleuther
    LLM评估工具包的6个任务上比较这些方法（Gao等，[2023](#bib.bib12)）。Wikitext-2任务的剪枝信号与上述实验相同。对于Eleuther
    Harness任务，我们使用C4（Raffel等，[2020](#bib.bib39)）数据集的语言建模性能作为剪枝信号。我们还对剪枝后的模型进行参数高效微调，使用来自该语料库的30K
    512长度序列。关于Bonsai在此实验中的具体超参数，请参见附录[A.3](#A1.SS3 "A.3 Experiments comparing to Gradient
    based structured pruning ‣ Appendix A Main Experiment Details ‣ Everybody Prune
    Now: Structured Pruning of LLMs with only Forward Passes")。'
- en: 6 Main Results
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 主要结果
- en: 6.1 Bonsai produces fast and performant models with only forward passes
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 Bonsai仅用前向传递就能生成快速且高性能的模型
- en: 'In Table [2](#S4.T2 "Table 2 ‣ 4 Post-pruning adaptation ‣ Everybody Prune
    Now: Structured Pruning of LLMs with only Forward Passes"), we explore options
    that are available to practitioners when they can only run forward passes on the
    parent model. Here, LLaMA-2 7B is compressed to 50% sparsity.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[2](#S4.T2 "Table 2 ‣ 4 Post-pruning adaptation ‣ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes")中，我们探讨了当只能对父模型进行前向传递时，实践者可用的选项。这里，LLaMA-2
    7B被压缩到50%稀疏性。'
- en: We compare Bonsai to the semi-structured variant of Wanda (Sun et al., [2023](#bib.bib44)).
    Before fine-tuning, the model produced by Wanda 2:4 achieves a speedup over the
    parent model (1.14$\times$) than the model from Bonsai.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将Bonsai与Wanda的半结构化变体（Sun等，[2023](#bib.bib44)）进行比较。在微调之前，Wanda 2:4生成的模型相较于Bonsai的模型，速度提升为（1.14$\times$）。
- en: 'In this memory-constrained setting, practitioners could alternatively opt for
    a pre-existing model of the target size instead of pruning a larger model. We
    compare the Bonsai-pruned model to Phi-2 (Li et al., [2023](#bib.bib28)), a strong
    representative pre-existing model of similar size. As can be seen in Table [2](#S4.T2
    "Table 2 ‣ 4 Post-pruning adaptation ‣ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes"), Bonsai is able to generate a model that is
    as accurate (0.2 difference in ppl) yet significantly faster (1.58$\times$ speedup),
    thus making it a competitive option to consider even if a model already exists
    at the target size.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种内存受限的情况下，实践者可以选择预先存在的目标大小模型，而不是剪枝更大的模型。我们将 Bonsai 剪枝后的模型与 Phi-2 (Li et al.,
    [2023](#bib.bib28))，一个强大的代表性预存在模型进行比较。正如表 [2](#S4.T2 "表 2 ‣ 4 剪枝后适应 ‣ 现在人人都剪枝：仅通过前向传播的
    LLM 结构剪枝") 所示，Bonsai 能够生成一个与目标大小模型相当的准确度（ppl 相差 0.2）但显著更快（1.58$\times$ 加速）的模型，因此即使在目标大小已有模型的情况下，它也是一个值得考虑的有竞争力的选项。
- en: 'Table 3: LLaMA-1 (50% sparsity) after post-pruning adaptation with LoRA. ^†
    indicate results as reported by Zhang et al. ([2023](#bib.bib52)).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：LoRA 后的 LLaMA-1（50% 稀疏度）经过剪枝后适应。^† 表示 Zhang et al. ([2023](#bib.bib52))
    报告的结果。
- en: '| Method | Foward-only | Wikitext-2 $\downarrow$ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 仅前向传播 | Wikitext-2 $\downarrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA1-7B (Touvron et al., [2023](#bib.bib45)) | - | 5.68 | 75.05 | 56.92
    | 69.93 | 75.34 | 41.89 | 63.83 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-7B (Touvron et al., [2023](#bib.bib45)) | - | 5.68 | 75.05 | 56.92
    | 69.93 | 75.34 | 41.89 | 63.83 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLM-Pruner^†  (Ma et al., [2023](#bib.bib31)) | ✗ | 16.41 | 60.28 | 47.06
    | 53.43 | 45.96 | 29.18 | 47.18 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner^† (Ma et al., [2023](#bib.bib31)) | ✗ | 16.41 | 60.28 | 47.06
    | 53.43 | 45.96 | 29.18 | 47.18 |'
- en: '| LoRAPrune^†  (Zhang et al., [2023](#bib.bib52)) | ✗ | 11.60 | 61.88 | 47.86
    | 55.01 | 45.13 | 31.62 | 48.30 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| LoRAPrune^† (Zhang et al., [2023](#bib.bib52)) | ✗ | 11.60 | 61.88 | 47.86
    | 55.01 | 45.13 | 31.62 | 48.30 |'
- en: '| Bonsai | ✓ | 10.92 | 67.22 | 43.09 | 61.64 | 54.92 | 26.28 | 50.63 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Bonsai | ✓ | 10.92 | 67.22 | 43.09 | 61.64 | 54.92 | 26.28 | 50.63 |'
- en: 6.2 Bonsai outperforms other structured pruning approaches
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 Bonsai 超越了其他结构剪枝方法
- en: 'We next investigate the setting where the practitioner has enough memory to
    run gradient-based structured pruning methods. We compare Bonsai to recent SoTA
    methods: LLM-Pruner (Ma et al., [2023](#bib.bib31)) and LoRA-Prune (Zhang et al.,
    [2023](#bib.bib52)). Since these approaches report their results for the LLaMA-1
    only, we prune LLaMA-1 7B to 50% sparsity.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们调查了实践者拥有足够内存以运行基于梯度的结构剪枝方法的情况。我们将 Bonsai 与最近的 SoTA 方法进行比较：LLM-Pruner (Ma
    et al., [2023](#bib.bib31)) 和 LoRA-Prune (Zhang et al., [2023](#bib.bib52))。由于这些方法的结果仅报告了
    LLaMA-1 的情况，我们将 LLaMA-1 7B 剪枝到 50% 稀疏度。
- en: 'As evinced by Table [3](#S6.T3 "Table 3 ‣ 6.1 Bonsai produces fast and performant
    models with only forward passes ‣ 6 Main Results ‣ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes"), Bonsai outperforms existing structured
    pruning methods for LLMs even though it exclusively uses forward passes in the
    pruning stage. We attribute the superior performance of Bonsai to the fact that
    its pruning decisions are informed by directly exploring the space of sub-models
    whilst the other approaches resort on inaccurate proxies of module relevance in
    order to reduce the memory overhead of a fully gradient-based optimization approach.
    Specifically, LoRA-Prune’s criterion for deciding which modules to prune uses
    gradient signals not from the modules directly but from a surrogate low-rank matrix.
    On the other hand, LLM-Pruner attempts to model the impact of removing a module
    on the model loss via a Taylor expansion and substitutes the LLM’s intractable
    (memory-wise) Hessian term with the Fisher information matrix which we posit leads
    to sub-optimal pruning decisions.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正如表 [3](#S6.T3 "表 3 ‣ 6.1 Bonsai 仅通过前向传播生成快速高效的模型 ‣ 6 主要结果 ‣ 现在人人都剪枝：仅通过前向传播的
    LLM 结构剪枝") 所示，尽管 Bonsai 在剪枝阶段仅使用前向传播，但其性能仍优于现有的结构剪枝方法。我们将 Bonsai 的优越性能归因于其剪枝决策是通过直接探索子模型空间来获得的，而其他方法则依赖于不准确的模块相关性代理，以减少完全基于梯度的优化方法的内存开销。具体而言，LoRA-Prune
    的剪枝标准使用的梯度信号不是直接来自模块，而是来自一个替代的低秩矩阵。另一方面，LLM-Pruner 尝试通过泰勒展开来建模删除模块对模型损失的影响，并用
    Fisher 信息矩阵代替 LLM 难以处理的（内存方面的）Hessian 项，我们认为这会导致次优的剪枝决策。
- en: 6.3 Bonsai can produce compressed models with strong zero-shot abilities
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 Bonsai 可以生成具有强大零-shot 能力的压缩模型
- en: 'Table 4: Phi-2 pruned to 35% sparsity compared to the best sub-2B parameter
    models on the Hugging Face OpenLLM Leaderboard. At the time of submission, $\dagger$
    was the best sub-2B parameter model on the Leaderboard.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：Phi-2修剪至35%稀疏度，相比于Hugging Face OpenLLM排行榜上的最佳子2B参数模型。在提交时，$\dagger$是排行榜上最佳的子2B参数模型。
- en: '|  |  | Generation | Multiple Choice (MC) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 生成 | 多项选择（MC） |'
- en: '| Model | Size | GSM8k (5-shot) | ARC-c (25-shot) | Winogrande (5-shot) | Hellaswag
    (10-shot) | Truthful-QA (0-shot) | MMLU (5-shot) | MC Average $\uparrow$ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 | GSM8k（5-shot） | ARC-c（25-shot） | Winogrande（5-shot） | Hellaswag（10-shot）
    | Truthful-QA（0-shot） | MMLU（5-shot） | MC 平均 $\uparrow$ |'
- en: '| Phi-2 (Li et al., [2023](#bib.bib28)) | 2.7B | 54.81 | 61.09 | 74.35 | 75.11
    | 44.47 | 58.11 | 62.63 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Phi-2 (Li等，[2023](#bib.bib28)) | 2.7B | 54.81 | 61.09 | 74.35 | 75.11 | 44.47
    | 58.11 | 62.63 |'
- en: '| [StableLM-2-1_6b](https://huggingface.co/stabilityai/stablelm-2-1_6b)$\dagger$
    | 36.78 | 38.95 | 50.82 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| [StableLM-2-1_6b](https://huggingface.co/stabilityai/stablelm-2-1_6b)$\dagger$
    | 36.78 | 38.95 | 50.82 |'
- en: '| Ours | 1.8B | 6.37 | $\boldsymbol{47.44}$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 1.8B | 6.37 | $\boldsymbol{47.44}$ |'
- en: Considerable amounts of compute and data, beyond what is available to lay practitioners,
    are needed to train LLMs with strong zero-shot capabilities (OpenAI et al., [2023](#bib.bib38);
    Gemini Team et al., [2023](#bib.bib13)). In this section, we demonstrate that
    Bonsai can empower everyday practitioners to produce strong and compact models
    with competitive zero-shot abilities by simply pruning bigger models on their
    available hardware.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 训练具有强大零样本能力的LLM需要大量的计算和数据，这些超出了普通从业者的能力（OpenAI等，[2023](#bib.bib38)；Gemini Team等，[2023](#bib.bib13)）。在本节中，我们展示了Bonsai如何使日常从业者能够在其现有硬件上通过简单地修剪更大的模型来生成强大且紧凑的模型，并具备具有竞争力的零样本能力。
- en: 'We use Bonsai to prune a $\approx$1.8B (35% sparsity). Values for the Bonsai hyper-parameters
    in this experiment are in Appendix [A.4](#A1.SS4 "A.4 Phi-2 pruning experiment
    details ‣ Appendix A Main Experiment Details ‣ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes"). Since its relatively small, the 1.8B
    pruned model can be fully fine-tuned on 1 A6000 GPU over 100k sequences of 2,048
    tokens from the C4 dataset. As can be seen from Table [4](#S6.T4 "Table 4 ‣ 6.3
    Bonsai can produce compressed models with strong zero-shot abilities ‣ 6 Main
    Results ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes"),
    our pruned model achieves strong zero-shot performance compared to StableLM, the
    leading sub-2B parameter LLM on the Hugging Face OpenLLM leaderboard (Gao et al.,
    [2023](#bib.bib12)) — outperforming it on 4 out of 6 tasks as of January 2024\.
    Interestingly, one exception to the general trend of Bonsai’s superior performance
    is the GSM-8K dataset, which is a mathematical reasoning dataset that requires
    generation of a long reasoning chains. We posit that this is because currently,
    Bonsai prunes with respect to language modeling likelihood, as opposed to reasoning
    accuracy. An interesting avenue for future work is to prune to improve maintain
    reasoning ability.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Bonsai来修剪一个$\approx$1.8B（35%稀疏度）模型。在本实验中，Bonsai超参数的值见附录[A.4](#A1.SS4 "A.4
    Phi-2修剪实验细节 ‣ 附录A 主要实验细节 ‣ 现在修剪：仅通过前向传递的LLM结构化修剪")。由于模型较小，1.8B的修剪模型可以在1个A6000
    GPU上对来自C4数据集的100k个2,048 token序列进行完全微调。从表格[4](#S6.T4 "表4 ‣ 6.3 Bonsai可以生成具有强大零样本能力的压缩模型
    ‣ 6 主要结果 ‣ 现在修剪：仅通过前向传递的LLM结构化修剪")中可以看出，我们的修剪模型相比于StableLM（Hugging Face OpenLLM排行榜上的领先子2B参数LLM）表现出强大的零样本性能——在2024年1月的6个任务中超越了它4个任务。值得注意的是，Bonsai性能优越的普遍趋势的一个例外是GSM-8K数据集，这是一个需要生成长推理链的数学推理数据集。我们推测这是因为当前Bonsai是基于语言建模可能性进行修剪的，而不是基于推理准确性。未来的一个有趣研究方向是修剪以改善推理能力。
- en: 7 Analysis
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 分析
- en: 'In this section, we conduct various ablative experiments to understand how
    the methodological ingredients from Section [3](#S3 "3 Methodology ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") contribute to
    make Bonsai effective.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行各种消融实验，以了解第[3](#S3 "3 方法论 ‣ 现在修剪：仅通过前向传递的LLM结构化修剪")节中的方法学成分如何使Bonsai有效。
- en: 'Do we need both perturbative and regressive components of Bonsai? Figure [2](#S7.F2
    "Figure 2 ‣ 7 Analysis ‣ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") shows that both components are key to obtaining a good pruned
    model. Removing the estimation of module importances via regression leads to a
    degradation in performance ($61.6$ as computed from the unperturbed parent model.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否需要 Bonsai 的扰动和回归组件？图 [2](#S7.F2 "图 2 ‣ 7 分析 ‣ 现在修剪：仅通过前向传递的 LLM 结构化修剪")
    显示，两种组件对于获得良好的剪枝模型都是关键。通过回归去除模块重要性的估计会导致性能下降（$61.6$，从未扰动的父模型计算得出）。
- en: '![Refer to caption](img/7ab05c979f32dafa13204333ec4b8613.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7ab05c979f32dafa13204333ec4b8613.png)'
- en: 'Figure 2: LLaMA-2 7B pruned to 50% sparsity. Perturbatively evaluating the
    impact of removing modules and then using regression to estimate module importances
    are both key in making Bonsai effective. Details of experiment configuration can
    be found in Appendix [B](#A2 "Appendix B Impact of regression and perturbation
    ablation details ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward
    Passes"). No post-pruning adaptation is performed.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: LLaMA-2 7B 剪枝至 50% 稀疏度。扰动评估模块移除的影响，然后使用回归估计模块重要性是使 Bonsai 有效的关键。实验配置的详细信息可以在附录
    [B](#A2 "附录 B 回归和扰动消融细节的影响 ‣ 现在修剪：仅通过前向传递的 LLM 结构化修剪") 中找到。没有进行后修剪适应。'
- en: 'Table 5: Varying the number of perturbative evaluations. Wikitext-2 perplexity
    of LLaMA-2 7B pruned to 50% sparsity. See Appendix [G](#A7 "Appendix G How many
    perturbative samples are reasonable? ‣ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes") for details. No post-pruning adaptation is
    performed.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 改变扰动评估的数量。LLaMA-2 7B 剪枝至 50% 稀疏度的 Wikitext-2 困惑度。详细信息请参见附录 [G](#A7 "附录
    G 多少扰动样本是合理的？ ‣ 现在修剪：仅通过前向传递的 LLM 结构化修剪")。没有进行后修剪适应。'
- en: '|  | $\mathrm{ns}=50$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{ns}=50$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| PPL ($\downarrow$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| PPL ($\downarrow$ |'
- en: 'How many perturbative samples are sufficient? We investigate the number of
    perturbative samples required to obtain good estimates of module importances after
    performing regression as in Equation [3](#S3.E3 "Equation 3 ‣ Estimating 𝛽: ‣
    3.1 Estimating module relevance with only forward passes ‣ 3 Methodology ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes"). Our results
    are shown in Table [5](#S7.T5 "Table 5 ‣ 7 Analysis ‣ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes"). As expected, performance improves
    as we increase the number of sub-models explored. We note that the number of samples
    we explore, $\mathrm{ns}$), nevertheless Bonsai is able to deliver a performant
    pruned model because of the recipes developed in Section [3](#S3 "3 Methodology
    ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes").'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '多少扰动样本是足够的？我们调查了在进行回归后，获取模块重要性良好估计所需的扰动样本数量，如公式 [3](#S3.E3 "公式 3 ‣ 估计 𝛽: ‣
    3.1 仅通过前向传递估计模块相关性 ‣ 3 方法 ‣ 现在修剪：仅通过前向传递的 LLM 结构化修剪") 所示。我们的结果展示在表 [5](#S7.T5
    "表 5 ‣ 7 分析 ‣ 现在修剪：仅通过前向传递的 LLM 结构化修剪") 中。正如预期的那样，随着我们增加探索的子模型数量，性能有所提高。我们注意到我们探索的样本数量，$\mathrm{ns}$，然而由于第
    [3](#S3 "3 方法 ‣ 现在修剪：仅通过前向传递的 LLM 结构化修剪") 节中开发的配方，Bonsai 仍然能够提供高性能的剪枝模型。'
- en: 'How much performance is recovered by post-pruning adaptation? During iterative
    pruning, Bonsai damages the parent model by removing modules but does not perform
    intermittent retraining to recover lost performance since even intermediate models
    may be too large for fine-tuning. Even so, as Table [6](#S7.T6 "Table 6 ‣ 7 Analysis
    ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") shows,
    the final model produced by Bonsai has reasonable performance without fine-tuning.
    We attribute this to the general robustness of LLMs, the redundancy of modules
    with respect to target end-tasks and Bonsai’s ability to identify good candidates
    for pruning. If the pruned model is small enough in size, we can perform either
    full fine-tuning or parameter-efficient fine-tunin to recover more performance,
    as can be seen from Table [6](#S7.T6 "Table 6 ‣ 7 Analysis ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes").'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝后适应能恢复多少性能？在迭代剪枝过程中，Bonsai 通过移除模块损害了父模型，但由于即使是中间模型可能也太大而无法进行细调，因此不会执行间歇性的再训练以恢复丢失的性能。即便如此，如表
    [6](#S7.T6 "表 6 ‣ 7 分析 ‣ 现在人人剪枝：仅前向传递的 LLM 结构化剪枝") 所示，Bonsai 生成的最终模型在没有细调的情况下性能合理。我们将此归因于
    LLM 的普遍鲁棒性、模块与目标任务的冗余性以及 Bonsai 识别剪枝良好候选的能力。如果剪枝后的模型足够小，我们可以执行完全细调或参数高效的细调以恢复更多性能，如表
    [6](#S7.T6 "表 6 ‣ 7 分析 ‣ 现在人人剪枝：仅前向传递的 LLM 结构化剪枝") 所示。
- en: 'Table 6: Impact of post-pruning adaptation of LLaMA-2 7B pruned to 50% sparsity.
    See Appendix [E](#A5 "Appendix E Post-pruning adaptation ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes") for more details.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：LLaMA-2 7B 剪枝至 50% 稀疏度的剪枝后适应影响。更多细节见附录 [E](#A5 "附录 E 剪枝后适应 ‣ 现在人人剪枝：仅前向传递的
    LLM 结构化剪枝")。
- en: '| Method | Wikitext-2 PPL |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Wikitext-2 PPL |'
- en: '| --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| No Post-Pruning Adaptation | $19.47$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 无剪枝后适应 | $19.47$ |'
- en: '| Post-Pruning Finetuning | $10.39$ |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝后细调 | $10.39$ |'
- en: '|    +  Distillation | $8.89$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|    +  蒸馏 | $8.89$ |'
- en: '![Refer to caption](img/9c738ca6f41a278ee7be755755b9a999.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9c738ca6f41a278ee7be755755b9a999.png)'
- en: 'Figure 3: LLaMA-2 7B pruned to 50% sparsity. See Appendix [F](#A6 "Appendix
    F Impact of prior ‣ Everybody Prune Now: Structured Pruning of LLMs with only
    Forward Passes") for details of experiment configuration and definitions of module-level
    analogues of Wanda and Activation Magnitude.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：LLaMA-2 7B 剪枝至 50% 稀疏度。实验配置和 Wanda 和 Activation Magnitude 模块级别类似物的定义详见附录
    [F](#A6 "附录 F 影响 ‣ 现在人人剪枝：仅前向传递的 LLM 结构化剪枝")。
- en: 'What is the impact of the choice of metric for the prior $\rho$. Figure [3](#S7.F3
    "Figure 3 ‣ 7 Analysis ‣ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") shows that using the module-level analogue of Wanda (Sun
    et al., [2023](#bib.bib44)) yields the best performance, both before and after
    post-pruning adaptation. This indicates that Wanda is a strong signal for efficiently
    estimating the importance of model units (whether at the parameter- or module-level).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于先验 $\rho$ 的选择，指标的影响是什么？图 [3](#S7.F3 "图 3 ‣ 7 分析 ‣ 现在人人剪枝：仅前向传递的 LLM 结构化剪枝")
    表明，使用 Wanda 的模块级别类似物 (Sun et al., [2023](#bib.bib44)) 可以获得最佳性能，无论是在剪枝后适应之前还是之后。这表明
    Wanda 是高效估计模型单元重要性的强信号（无论是在参数级还是模块级）。
- en: 'Should Bonsai prune iteratively? Table [7](#S7.T7 "Table 7 ‣ 7 Analysis ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") demonstrates
    the benefits of using Bonsai  in an iterative fashion. Pruning slowly ($p_{\mathrm{iter}}=0.05$
    persists even after post-pruning adaptation, indicating that slower pruning allows
    for more accurate estimates of module importance.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Bonsai 是否应该迭代性地进行剪枝？表 [7](#S7.T7 "表 7 ‣ 7 分析 ‣ 现在人人剪枝：仅前向传递的 LLM 结构化剪枝") 展示了以迭代方式使用
    Bonsai 的好处。慢剪枝 ($p_{\mathrm{iter}}=0.05$ 即使在剪枝后适应中仍然存在，这表明较慢的剪枝允许对模块重要性进行更准确的估计。
- en: 'Table 7: Varying $p_{\mathrm{iter}}$. Wikitext-2 perplexity of LLaMA-2 7B pruned
    to 50% sparsity. See Appendix [C](#A3 "Appendix C Varying the pruning fraction
    per-iteration ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward
    Passes") for experiment details.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：不同 $p_{\mathrm{iter}}$ 下的表现。LLaMA-2 7B 剪枝至 50% 稀疏度的 Wikitext-2 困惑度。实验细节见附录
    [C](#A3 "附录 C 每次迭代剪枝比例的变化 ‣ 现在人人剪枝：仅前向传递的 LLM 结构化剪枝")。
- en: '|  | $p_{\mathrm{iter}}=0.05$ |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{\mathrm{iter}}=0.05$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| w/o Adapt | $19.47$ |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 无适应 | $19.47$ |'
- en: '| w Adapt | $8.89$ |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| w 适应 | $8.89$ |'
- en: 8 Related Work
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 相关工作
- en: 8.1 Pruning
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 剪枝
- en: 'Unstructured pruning:'
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非结构化剪枝：
- en: Whilst structured pruning approaches remove entire model components like layers
    (Xu et al., [2020](#bib.bib50); Xia et al., [2022](#bib.bib48)), dimensions of
    linear layers (Wang et al., [2019](#bib.bib47)) or attention heads (Michel et al.,
    [2019](#bib.bib35); Held & Yang, [2022](#bib.bib17)), unstructured pruning (Han
    et al., [2015](#bib.bib15); Frankle & Carbin, [2018](#bib.bib10); Benbaki et al.,
    [2023](#bib.bib4); Sun et al., [2023](#bib.bib44)) removes individual parameters
    of the model. These approaches achieve memory savings by inducing sparsity in
    the model weights, but they generally do not result in actual model speedups except
    when specialized hardware is available (Mishra et al., [2021](#bib.bib36)). Proposed
    semi-structured sparsity methods (Mishra et al., [2021](#bib.bib36)) such as 2:4
    and 4:8 patterns do result in faster inference, but the speedup gains they achieve
    are far from the idealized $2\times$. There are several gradient-free, unstructured
    pruning approaches. Since, as far as we know, there are no scalable gradient-free
    structured pruning alternatives outside of Bonsai, we compare it with Wanda (Sun
    et al., [2023](#bib.bib44)), a SoTA unstructured/semi-structured pruning technique
    for LLMs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结构化剪枝方法去除整个模型组件，如层（Xu et al., [2020](#bib.bib50); Xia et al., [2022](#bib.bib48)）、线性层的维度（Wang
    et al., [2019](#bib.bib47)）或注意力头（Michel et al., [2019](#bib.bib35); Held & Yang,
    [2022](#bib.bib17)），但非结构化剪枝（Han et al., [2015](#bib.bib15); Frankle & Carbin,
    [2018](#bib.bib10); Benbaki et al., [2023](#bib.bib4); Sun et al., [2023](#bib.bib44)）则去除模型的单个参数。这些方法通过引入模型权重的稀疏性来实现内存节省，但通常不会实际加快模型速度，除非有专门的硬件可用（Mishra
    et al., [2021](#bib.bib36)）。所提出的半结构化稀疏性方法（Mishra et al., [2021](#bib.bib36)），如2:4和4:8模式，确实能实现更快的推理速度，但它们获得的加速远未达到理想的$2\times$。存在几种无梯度的非结构化剪枝方法。由于据我们所知，在Bonsai之外没有可扩展的无梯度结构化剪枝替代方法，我们将其与Wanda（Sun
    et al., [2023](#bib.bib44)）进行比较，后者是针对LLMs的最先进的非结构化/半结构化剪枝技术。
- en: 'Structured pruning without backward passes:'
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无需反向传播的结构化剪枝：
- en: To the best of our knowledge, all existing structured pruning techniques for
    large (over 1B scale) language models, like LoRAPrune (Zhang et al., [2023](#bib.bib52))
    and LLM-Pruner (Ma et al., [2023](#bib.bib31)), are gradient-based. Thus, under
    the memory setting we consider, these methods cannot be applied since their memory
    requirements well exceed the memory needed for inference on the model to be pruned.
    For smaller language models like BERT, Nova et al. ([2023](#bib.bib37)) recently
    proposed Kernelized Convex Masking (KCM) for gradient-free structured pruning.
    Unfortunately, to prune a fully connected layer with $K$ fraction within its layer
    will not be included in the pruned model — a clearly sub-optimal choice. Bonsai enables
    us to obtain globally meaningful estimates of module relevance that are comparable
    across layers.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，目前所有针对大型（超过1B规模）语言模型的现有结构化剪枝技术，如LoRAPrune（Zhang et al., [2023](#bib.bib52)）和LLM-Pruner（Ma
    et al., [2023](#bib.bib31)），均基于梯度。因此，在我们考虑的内存设置下，这些方法无法应用，因为它们的内存需求远超出剪枝模型推理所需的内存。对于较小的语言模型，如BERT，Nova
    et al. ([2023](#bib.bib37)) 最近提出了无梯度结构化剪枝的Kernelized Convex Masking (KCM)。不幸的是，剪枝一个全连接层中$K$比例的部分不会包含在剪枝后的模型中——这是一个明显的次优选择。Bonsai使我们能够获得全球范围内的模块相关性估计，这些估计在各层之间是可比的。
- en: 8.2 Model compression beyond pruning
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 剪枝之外的模型压缩
- en: 'Distillation:'
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 蒸馏：
- en: Originally proposed by Hinton et al. ([2015](#bib.bib18)), Sanh et al. ([2019](#bib.bib41))
    have shown that it is possible to reach similar performances on many downstream
    tasks using much smaller language models pre-trained with knowledge distillation.
    The resulting models are lighter and faster at inference time. Unfortunately,
    as noted by Xia et al. ([2022](#bib.bib48)), distilling a pre-trained model to
    a student model that has been initialized from scratch requires lots of compute
    and data to recover reasonable levels of teacher performance. To reduce this resource
    burden, Sanh et al. ([2020](#bib.bib42)); Lagunas et al. ([2021](#bib.bib27));
    Xia et al. ([2022](#bib.bib48)) combine distillation with gradient-based structured
    pruning, so that the student model does not have to be initialized from scratch.
    Bonsai also leverages distillation, but only during the post-pruning adaptation
    phase. This maintains the constraint that the pruning process is gradient-free.
    And since the pruned model is smaller than the parent model, we can safely perform
    distillation and parameter efficient fine-tuning (Hu et al., [2021](#bib.bib20);
    He et al., [2021](#bib.bib16)) within the bounds of available memory.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Hinton 等人最初提出的 ([2015](#bib.bib18))，Sanh 等人 ([2019](#bib.bib41)) 已经证明，通过知识蒸馏预训练的小型语言模型在许多下游任务上可以达到类似的性能。这些模型在推理时更轻量、更快。不幸的是，正如
    Xia 等人 ([2022](#bib.bib48)) 所指出的，将预训练模型蒸馏到从零初始化的学生模型中需要大量计算和数据才能恢复合理的教师性能。为了减少这种资源负担，Sanh
    等人 ([2020](#bib.bib42))；Lagunas 等人 ([2021](#bib.bib27))；Xia 等人 ([2022](#bib.bib48))
    将蒸馏与基于梯度的结构化剪枝相结合，从而使学生模型无需从零初始化。Bonsai 也利用了蒸馏，但仅在剪枝后适应阶段。这保持了剪枝过程无梯度的约束。由于剪枝后的模型比父模型小，我们可以在可用内存的范围内安全地执行蒸馏和参数高效微调
    (Hu 等人，[2021](#bib.bib20)；He 等人，[2021](#bib.bib16))。
- en: 'Quantization:'
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化：
- en: Li et al. ([2020](#bib.bib29)) introduced a training strategy that showed that
    heavily compressed, large models achieved higher accuracy than lightly compressed,
    small models, providing both efficiency and high-accuracy results on NLP tasks
    at once. Dettmers et al. ([2022](#bib.bib9)) introduced a new method called ‘LLM.int8()‘,
    which allows loading large models with 16 or 32-bit weights and using them immediately
    for inference without any performance degradation by combining vector-wise quantization
    with mixed-precision decomposition. Xiao et al. ([2023](#bib.bib49)) enabled 8-bit
    weight, 8-bit activation (W8A8) quantization for LLMs with >100B parameters, which
    smoothed activation outliers by migrating the quantization difficulty from activations
    to weights with a mathematically equivalent transformation. Quantization tends
    to be complementary to pruning, allowing independent exploration and subsequent
    combinations of both techniques. Some quantization approaches, specifically mixed
    precision ones like Dettmers et al. ([2022](#bib.bib9)) (Appendix D), can produce
    resulting models that are *slower* than the parent model, though it has also been
    shown that it can result in more accurate models than pruning (Kuzmin et al.,
    [2023](#bib.bib26)). This paper focuses on gradient-free structured pruning. We
    leave combining Bonsai with quantization for future work.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人 ([2020](#bib.bib29)) 提出了一个训练策略，表明重度压缩的大模型比轻度压缩的小模型更高效、更准确，在 NLP 任务中同时提供了效率和高准确性。Dettmers
    等人 ([2022](#bib.bib9)) 引入了一种名为 ‘LLM.int8()‘ 的新方法，该方法结合了向量级量化与混合精度分解，允许加载带有 16
    位或 32 位权重的大模型，并立即用于推理，而不会导致性能下降。Xiao 等人 ([2023](#bib.bib49)) 实现了适用于 >100B 参数的
    LLM 的 8 位权重、8 位激活 (W8A8) 量化，通过将量化难度从激活迁移到权重，平滑了激活的异常值，且进行了数学上等效的转换。量化往往与剪枝互补，允许独立探索和随后的两种技术组合。一些量化方法，特别是像
    Dettmers 等人 ([2022](#bib.bib9))（附录 D）这样的混合精度方法，可能会导致比父模型*更慢*的结果模型，尽管也有研究表明它可以产生比剪枝更准确的模型
    (Kuzmin 等人，[2023](#bib.bib26))。本文专注于无梯度的结构化剪枝。我们将 Bonsai 与量化的结合留待未来工作。
- en: 9 Conclusion, Limitations, and Future Work
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论、局限性和未来工作
- en: In this work, we have presented Bonsai, the first tractable gradient-free method
    for structured pruning of LLMs. Bonsai allows practitioners to prune any LLM as
    long as they have enough memory to run inferences on the model. Through a battery
    of experiments, we have shown that the models produced by Bonsai are small, fast,
    and accurate.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了 Bonsai，这是一种针对 LLM 的首个可处理的无梯度结构化剪枝方法。Bonsai 允许实践者剪枝任何 LLM，只要他们有足够的内存来运行模型推理。通过一系列实验，我们已经证明
    Bonsai 生成的模型小巧、快速且准确。
- en: 'A primary limitation of Bonsai is its runtime. As Section [7](#S7 "7 Analysis
    ‣ Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") shows,
    performance improves when we increase the number of sub-models explored, slow
    down pruning, and use more data samples; but this comes at the cost of increased
    runtime. Our result in Table [2](#S4.T2 "Table 2 ‣ 4 Post-pruning adaptation ‣
    Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes") took
    $\sim 40$ hours to obtain, whilst Wanda-2:4 takes on the order of minutes. Since
    the model we produce is twice as fast, this trade-off is worthwhile when amortized
    over inference on the pruned models.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Bonsai的一个主要限制是其运行时间。如第[7](#S7 "7 分析 ‣ 现在大家都剪枝：仅前向传播的LLM结构化剪枝")节所示，当我们增加探索的子模型数量、减慢剪枝速度和使用更多数据样本时，性能会有所提升；但这会增加运行时间。我们在表[2](#S4.T2
    "表 2 ‣ 4 剪枝后适应 ‣ 现在大家都剪枝：仅前向传播的LLM结构化剪枝")中的结果花费了$\sim 40$小时，而Wanda-2:4则在几分钟内完成。由于我们生成的模型速度是其两倍，这种权衡在对剪枝模型进行推理时是值得的。
- en: Bonsai presents several avenues for expansion by future work. First, though
    sub-models are sampled from an informative prior, $\rho$, the sampling process
    is not adaptive. Bonsai could further be strengthened by dynamically exploring
    the space of sub-models. Next, because of memory constraints, Bonsai does not
    fine-tune the model during iterative pruning to recover degraded performance.
    However, forward-pass-only fine-tuning approaches like MeZO (Malladi et al., [2023](#bib.bib32))
    exist that can be used to dynamically update the model during pruning.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Bonsai为未来的扩展提供了几个方向。首先，尽管子模型是从一个有信息的先验$\rho$中抽样得到的，但抽样过程并不是自适应的。Bonsai可以通过动态探索子模型的空间来进一步增强。接下来，由于内存限制，Bonsai在迭代剪枝过程中不会微调模型以恢复性能。然而，像MeZO（Malladi等，[2023](#bib.bib32)）这样的仅前向传播微调方法可以用于在剪枝过程中动态更新模型。
- en: 10 Acknowlegements
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 致谢
- en: This work was supported in part by the National Science Foundation grants IIS1705121,
    IIS1838017, IIS2046613, IIS2112471, and funding from Meta, Morgan Stanley, Amazon,
    and Google. Any opinions, findings and conclusions or recommendations expressed
    in this material are those of the author(s) and do not necessarily reflect the
    views of any of these funding agencies. We are grateful for helpful feedback from
    Mingjie Sun, Victor Akinwande, Asher Trockman, Afshin Rostamizadeh and Daniel
    Glasner
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作部分得到了国家科学基金会资助（资助编号：IIS1705121, IIS1838017, IIS2046613, IIS2112471），以及Meta、摩根士丹利、亚马逊和谷歌的资助。本文中的任何观点、发现、结论或建议仅代表作者（们）的意见，不一定反映这些资助机构的观点。我们感谢Mingjie
    Sun、Victor Akinwande、Asher Trockman、Afshin Rostamizadeh和Daniel Glasner的有益反馈。
- en: 11 Broader Impact Statement
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11 更广泛的影响声明
- en: Network pruning is an effective compression strategy to reduce the inference
    costs of large language models. However, existing techniques for structured pruning
    impose expensive memory constraints that may render them out of reach in practice.
    By reducing such constraints, work aims to democratize pruning to provide practitioners
    the ability to produce their own pruned LLMs for real-world applications. While
    we ultimately view this democratization as a benefit, we note that a necessary
    outcome is that it may also make it more feasible for malicious actors to readily
    use LLMs in practice. Finally, we note that our work has primarily focused on
    the axes of efficiency and utility (e.g., perplexity) in assessing performance;
    recent work has shown that compression may also have outsized effects on issues
    such as model fairness and robustness, which would be interesting additional aspects
    to consider in future study.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 网络剪枝是一种有效的压缩策略，可以降低大型语言模型的推理成本。然而，现有的结构化剪枝技术会施加昂贵的内存限制，这可能使其在实际中难以实现。通过减少这些限制，该工作旨在使剪枝技术普及，赋予从业人员为实际应用生产自己剪枝后的LLM的能力。虽然我们最终将这种普及视为一种好处，但我们注意到，这也可能使恶意行为者更容易在实践中使用LLM。最后，我们注意到我们的工作主要集中在评估性能的效率和效用（例如困惑度）方面；最近的研究表明，压缩也可能对模型公平性和鲁棒性等问题产生过度影响，这些将是未来研究中值得深入探讨的额外方面。
- en: References
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ancona et al. (2020) Ancona, M., Öztireli, C. and Gross, M. Shapley value as
    principled metric for structured network pruning. *arXiv preprint arXiv:2006.01795*,
    2020.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ancona等（2020）Ancona, M., Öztireli, C. 和 Gross, M. Shapley值作为结构化网络剪枝的原则度量。*arXiv预印本
    arXiv:2006.01795*，2020年。
- en: Anwar et al. (2017) Anwar, S., Hwang, K. and Sung, W. Structured pruning of
    deep convolutional neural networks. *ACM Journal on Emerging Technologies in Computing
    Systems (JETC)*, 13(3):1–18, 2017.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anwar et al. (2017) Anwar, S., Hwang, K. 和 Sung, W. 《深度卷积神经网络的结构化剪枝》。*ACM计算系统新兴技术期刊
    (JETC)*，13(3):1–18，2017年。
- en: Ba et al. (2016) Ba, J.L., Kiros, J.R. and Hinton, G.E. Layer normalization.
    *arXiv preprint arXiv:1607.06450*, 2016.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ba et al. (2016) Ba, J.L., Kiros, J.R. 和 Hinton, G.E. 《层归一化》。*arXiv 预印本 arXiv:1607.06450*，2016年。
- en: 'Benbaki et al. (2023) Benbaki, R., Chen, W., Meng, X., Hazimeh, H., Ponomareva,
    N., Zhao, Z. and Mazumder, R. Fast as chita: Neural network pruning with combinatorial
    optimization. *arXiv preprint arXiv:2302.14623*, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benbaki et al. (2023) Benbaki, R., Chen, W., Meng, X., Hazimeh, H., Ponomareva,
    N., Zhao, Z. 和 Mazumder, R. 《快如猎豹：通过组合优化进行神经网络剪枝》。*arXiv 预印本 arXiv:2302.14623*，2023年。
- en: 'Bender et al. (2021) Bender, E.M., Gebru, T., McMillan-Major, A. and Shmitchell,
    S. On the dangers of stochastic parrots: Can language models be too big. In *Proceedings
    of the 2021 ACM conference on fairness, accountability, and transparency*, pp. 
    610–623, 2021.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bender et al. (2021) Bender, E.M., Gebru, T., McMillan-Major, A. 和 Shmitchell,
    S. 《随机鹦鹉的危险：语言模型会不会过大》。在*2021年ACM公平性、问责制和透明度会议论文集*，第610–623页，2021年。
- en: Bridger (2023) Bridger, P. Pytorch memory tuning, Jul 2023. URL [https://paulbridger.com/posts/pytorch-memory-tuning/](https://paulbridger.com/posts/pytorch-memory-tuning/).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bridger (2023) Bridger, P. 《Pytorch内存调优》，2023年7月。网址 [https://paulbridger.com/posts/pytorch-memory-tuning/](https://paulbridger.com/posts/pytorch-memory-tuning/)。
- en: 'Covert & Lee (2020) Covert, I. and Lee, S.I. Improving kernelshap: Practical
    shapley value estimation via linear regression. *arXiv preprint arXiv:2012.01536*,
    2020.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Covert & Lee (2020) Covert, I. 和 Lee, S.I. 《改进的kernelshap：通过线性回归进行实用的Shapley值估计》。*arXiv
    预印本 arXiv:2012.01536*，2020年。
- en: 'Dekhovich et al. (2021) Dekhovich, A., Tax, D.M., Sluiter, M.H. and Bessa,
    M.A. Neural network relief: a pruning algorithm based on neural activity. *arXiv
    preprint arXiv:2109.10795*, 2021.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dekhovich et al. (2021) Dekhovich, A., Tax, D.M., Sluiter, M.H. 和 Bessa, M.A.
    《神经网络剪枝：一种基于神经活动的剪枝算法》。*arXiv 预印本 arXiv:2109.10795*，2021年。
- en: 'Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y. and Zettlemoyer,
    L. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*, 2022.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y. 和 Zettlemoyer,
    L. Llm. int8 (): 大规模变换器的8位矩阵乘法。*arXiv 预印本 arXiv:2208.07339*，2022年。'
- en: 'Frankle & Carbin (2018) Frankle, J. and Carbin, M. The lottery ticket hypothesis:
    Finding sparse, trainable neural networks. *arXiv preprint arXiv:1803.03635*,
    2018.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle & Carbin (2018) Frankle, J. 和 Carbin, M. 《彩票票假设：寻找稀疏、可训练的神经网络》。*arXiv
    预印本 arXiv:1803.03635*，2018年。
- en: 'Frantar & Alistarh (2023) Frantar, E. and Alistarh, D. Sparsegpt: Massive language
    models can be accurately pruned in one-shot. In *International Conference on Machine
    Learning*, pp.  10323–10337\. PMLR, 2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar & Alistarh (2023) Frantar, E. 和 Alistarh, D. 《Sparsegpt：可以在一次性中准确剪枝的大型语言模型》。在*国际机器学习会议*，第10323–10337页。PMLR，2023年。
- en: Gao et al. (2023) Gao, L. et al. A framework for few-shot language model evaluation,
    12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2023) Gao, L. 等人。少样本语言模型评估框架，2023年12月。网址 [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)。
- en: 'Gemini Team et al. (2023) Gemini Team et al. Gemini: a family of highly capable
    multimodal models. *arXiv preprint arXiv:2312.11805*, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gemini Team et al. (2023) Gemini Team 等人。《Gemini：一系列高能力的多模态模型》。*arXiv 预印本 arXiv:2312.11805*，2023年。
- en: Gu et al. (2023) Gu, Y., Dong, L., Wei, F. and Huang, M. Knowledge distillation
    of large language models. *arXiv preprint arXiv:2306.08543*, 2023.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. (2023) Gu, Y., Dong, L., Wei, F. 和 Huang, M. 《大语言模型的知识蒸馏》。*arXiv 预印本
    arXiv:2306.08543*，2023年。
- en: 'Han et al. (2015) Han, S., Mao, H. and Dally, W.J. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. *arXiv
    preprint arXiv:1510.00149*, 2015.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2015) Han, S., Mao, H. 和 Dally, W.J. 《深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络》。*arXiv
    预印本 arXiv:1510.00149*，2015年。
- en: He et al. (2021) He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T. and Neubig,
    G. Towards a unified view of parameter-efficient transfer learning. *arXiv preprint
    arXiv:2110.04366*, 2021.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2021) He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T. 和 Neubig, G.
    迈向参数高效迁移学习的统一视角。*arXiv 预印本 arXiv:2110.04366*，2021年。
- en: 'Held & Yang (2022) Held, W. and Yang, D. Shapley head pruning: Identifying
    and removing interference in multilingual transformers. *arXiv preprint arXiv:2210.05709*,
    2022.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Held & Yang (2022) Held, W. 和 Yang, D. 《Shapley头剪枝：识别和去除多语言变换器中的干扰》。*arXiv 预印本
    arXiv:2210.05709*，2022年。
- en: Hinton et al. (2015) Hinton, G.E., Vinyals, O. and Dean, J. Distilling the knowledge
    in a neural network. *ArXiv*, abs/1503.02531, 2015.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等（2015）Hinton, G.E., Vinyals, O. 和 Dean, J. 提炼神经网络中的知识。*ArXiv*，abs/1503.02531，2015。
- en: Hsieh et al. (2023) Hsieh, C.Y., Li, C.L., Yeh, C.K., Nakhost, H., Fujii, Y.,
    Ratner, A., Krishna, R., Lee, C.Y. and Pfister, T. Distilling step-by-step! outperforming
    larger language models with less training data and smaller model sizes. *arXiv
    preprint arXiv:2305.02301*, 2023.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh 等（2023）Hsieh, C.Y., Li, C.L., Yeh, C.K., Nakhost, H., Fujii, Y., Ratner,
    A., Krishna, R., Lee, C.Y. 和 Pfister, T. 步步提炼！用更少的训练数据和更小的模型尺寸超越更大的语言模型。*arXiv
    预印本 arXiv:2305.02301*，2023。
- en: 'Hu et al. (2021) Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L. and Chen, W. Lora: Low-rank adaptation of large language models.
    *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等（2021）Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S.,
    Wang, L. 和 Chen, W. Lora: 大型语言模型的低秩适应。*arXiv 预印本 arXiv:2106.09685*，2021。'
- en: Jiang et al. (2023) Jiang, A.Q. et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*,
    2023.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2023）Jiang, A.Q. 等 Mistral 7b。*arXiv 预印本 arXiv:2310.06825*，2023。
- en: 'Jiao et al. (2019) Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L.,
    Wang, F. and Liu, Q. Tinybert: Distilling bert for natural language understanding.
    *arXiv preprint arXiv:1909.10351*, 2019.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiao 等（2019）Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang,
    F. 和 Liu, Q. Tinybert: 提炼 bert 以进行自然语言理解。*arXiv 预印本 arXiv:1909.10351*，2019。'
- en: 'Kang et al. (2023) Kang, M., Li, L. and Li, B. Fashapley: Fast and approximated
    shapley based model pruning towards certifiably robust dnns. In *2023 IEEE Conference
    on Secure and Trustworthy Machine Learning (SaTML)*, pp.  575–592\. IEEE, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kang 等（2023）Kang, M., Li, L. 和 Li, B. Fashapley: 快速且近似的 Shapley 基模型修剪，朝着可证明健壮的深度神经网络迈进。在
    *2023 IEEE 可信赖与安全机器学习会议 (SaTML)*，第 575–592 页。IEEE，2023。'
- en: Kendall (1948) Kendall, M.G. Rank correlation methods. 1948.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kendall（1948）Kendall, M.G. 排名相关方法。1948。
- en: 'Kingma & Ba (2014) Kingma, D.P. and Ba, J. Adam: A method for stochastic optimization.
    *arXiv preprint arXiv:1412.6980*, 2014.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma 和 Ba（2014）Kingma, D.P. 和 Ba, J. Adam: 一种随机优化方法。*arXiv 预印本 arXiv:1412.6980*，2014。'
- en: 'Kuzmin et al. (2023) Kuzmin, A., Nagel, M., Van Baalen, M., Behboodi, A. and
    Blankevoort, T. Pruning vs quantization: Which is better? *arXiv preprint arXiv:2307.02973*,
    2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuzmin 等（2023）Kuzmin, A., Nagel, M., Van Baalen, M., Behboodi, A. 和 Blankevoort,
    T. 修剪与量化：哪种更好？*arXiv 预印本 arXiv:2307.02973*，2023。
- en: Lagunas et al. (2021) Lagunas, F., Charlaix, E., Sanh, V. and Rush, A.M. Block
    pruning for faster transformers. *arXiv preprint arXiv:2109.04838*, 2021.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagunas 等（2021）Lagunas, F., Charlaix, E., Sanh, V. 和 Rush, A.M. 块修剪以加速变换器。*arXiv
    预印本 arXiv:2109.04838*，2021。
- en: 'Li et al. (2023) Li, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar,
    S. and Lee, Y.T. Textbooks are all you need ii: phi-1.5 technical report. *arXiv
    preprint arXiv:2309.05463*, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等（2023）Li, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S. 和 Lee,
    Y.T. 教科书就是你所需的 II: phi-1.5 技术报告。*arXiv 预印本 arXiv:2309.05463*，2023。'
- en: 'Li et al. (2020) Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein,
    D. and Gonzalez, J. Train big, then compress: Rethinking model size for efficient
    training and inference of transformers. In *International Conference on machine
    learning*, pp.  5958–5968\. PMLR, 2020.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2020）Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D. 和 Gonzalez,
    J. 先训练大模型，然后压缩：重新思考模型大小以提高变换器的训练和推理效率。在 *国际机器学习会议*，第 5958–5968 页。PMLR，2020。
- en: Loshchilov & Hutter (2017) Loshchilov, I. and Hutter, F. Decoupled weight decay
    regularization. *arXiv preprint arXiv:1711.05101*, 2017.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov 和 Hutter（2017）Loshchilov, I. 和 Hutter, F. 解耦权重衰减正则化。*arXiv 预印本 arXiv:1711.05101*，2017。
- en: 'Ma et al. (2023) Ma, X., Fang, G. and Wang, X. Llm-pruner: On the structural
    pruning of large language models. *arXiv preprint arXiv:2305.11627*, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等（2023）Ma, X., Fang, G. 和 Wang, X. Llm-pruner: 大型语言模型的结构性修剪。*arXiv 预印本 arXiv:2305.11627*，2023。'
- en: Malladi et al. (2023) Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J.D.,
    Chen, D. and Arora, S. Fine-tuning language models with just forward passes. *arXiv
    preprint arXiv:2305.17333*, 2023.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malladi 等（2023）Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J.D., Chen,
    D. 和 Arora, S. 仅通过前向传递微调语言模型。*arXiv 预印本 arXiv:2305.17333*，2023。
- en: 'McGrath et al. (2023) McGrath, T., Rahtz, M., Kramar, J., Mikulik, V. and Legg,
    S. The hydra effect: Emergent self-repair in language model computations. *arXiv
    preprint arXiv:2307.15771*, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McGrath 等（2023）McGrath, T., Rahtz, M., Kramar, J., Mikulik, V. 和 Legg, S. 九头蛇效应：语言模型计算中的自我修复。*arXiv
    预印本 arXiv:2307.15771*，2023。
- en: Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J. and Socher, R. Pointer
    sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等（2016）Merity, S., Xiong, C., Bradbury, J. 和 Socher, R. 指针哨兵混合模型。*arXiv
    预印本 arXiv:1609.07843*，2016。
- en: Michel et al. (2019) Michel, P., Levy, O. and Neubig, G. Are sixteen heads really
    better than one? *Advances in neural information processing systems*, 32, 2019.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michel 等人 (2019) Michel, P., Levy, O. 和 Neubig, G. 十六个头真的比一个头更好吗？*神经信息处理系统进展*，32，2019。
- en: Mishra et al. (2021) Mishra, A., Latorre, J.A., Pool, J., Stosic, D., Stosic,
    D., Venkatesh, G., Yu, C. and Micikevicius, P. Accelerating sparse deep neural
    networks. *arXiv preprint arXiv:2104.08378*, 2021.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra 等人 (2021) Mishra, A., Latorre, J.A., Pool, J., Stosic, D., Stosic, D.,
    Venkatesh, G., Yu, C. 和 Micikevicius, P. 加速稀疏深度神经网络。*arXiv 预印本 arXiv:2104.08378*，2021。
- en: Nova et al. (2023) Nova, A., Dai, H. and Schuurmans, D. Gradient-free structured
    pruning with unlabeled data, 2023.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nova 等人 (2023) Nova, A., Dai, H. 和 Schuurmans, D. 无标记数据的无梯度结构化剪枝，2023。
- en: OpenAI et al. (2023) OpenAI et al. Gpt-4 technical report, 2023.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等人 (2023) OpenAI 等人. GPT-4 技术报告，2023。
- en: Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W. and Liu, P.J. Exploring the limits of transfer
    learning with a unified text-to-text transformer. *The Journal of Machine Learning
    Research*, 21(1):5485–5551, 2020.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
    Matena, M., Zhou, Y., Li, W. 和 Liu, P.J. 通过统一的文本到文本变换器探索迁移学习的极限。*机器学习研究期刊*，21(1):5485–5551，2020。
- en: 'Samsi et al. (2023) Samsi, S., Zhao, D., McDonald, J., Li, B., Michaleas, A.,
    Jones, M., Bergeron, W., Kepner, J., Tiwari, D. and Gadepally, V. From words to
    watts: Benchmarking the energy costs of large language model inference. In *2023
    IEEE High Performance Extreme Computing Conference (HPEC)*, pp.  1–9\. IEEE, 2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samsi 等人 (2023) Samsi, S., Zhao, D., McDonald, J., Li, B., Michaleas, A., Jones,
    M., Bergeron, W., Kepner, J., Tiwari, D. 和 Gadepally, V. 从词到瓦特：大语言模型推理的能源成本基准。在*2023
    IEEE 高性能极限计算会议 (HPEC)*，第 1–9 页。IEEE，2023。
- en: 'Sanh et al. (2019) Sanh, V., Debut, L., Chaumond, J. and Wolf, T. Distilbert,
    a distilled version of bert: smaller, faster, cheaper and lighter. *ArXiv*, abs/1910.01108,
    2019.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等人 (2019) Sanh, V., Debut, L., Chaumond, J. 和 Wolf, T. DistilBERT，一种精简版的
    BERT：更小、更快、更便宜、更轻便。*arXiv*，abs/1910.01108，2019。
- en: 'Sanh et al. (2020) Sanh, V., Wolf, T. and Rush, A. Movement pruning: Adaptive
    sparsity by fine-tuning. *Advances in Neural Information Processing Systems*,
    33:20378–20389, 2020.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等人 (2020) Sanh, V., Wolf, T. 和 Rush, A. 运动剪枝：通过微调实现自适应稀疏性。*神经信息处理系统进展*，33:20378–20389，2020。
- en: Simonyan & Zisserman (2014) Simonyan, K. and Zisserman, A. Very deep convolutional
    networks for large-scale image recognition. *arXiv preprint arXiv:1409.1556*,
    2014.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan & Zisserman (2014) Simonyan, K. 和 Zisserman, A. 用于大规模图像识别的非常深的卷积网络。*arXiv
    预印本 arXiv:1409.1556*，2014。
- en: Sun et al. (2023) Sun, M., Liu, Z., Bair, A. and Kolter, J.Z. A simple and effective
    pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 (2023) Sun, M., Liu, Z., Bair, A. 和 Kolter, J.Z. 一种简单有效的大型语言模型剪枝方法。*arXiv
    预印本 arXiv:2306.11695*，2023。
- en: 'Touvron et al. (2023) Touvron, H. et al. Llama 2: Open foundation and fine-tuned
    chat models. *arXiv preprint arXiv:2307.09288*, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 (2023) Touvron, H. 等人. Llama 2：开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023。
- en: Vivek (2023) Vivek, S. The economics of large language models, Sep 2023. URL
    [https://medium.com/emalpha/the-economics-of-large-language-models-2671985b621c](https://medium.com/emalpha/the-economics-of-large-language-models-2671985b621c).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vivek (2023) Vivek, S. 大型语言模型的经济学，2023年9月。URL [https://medium.com/emalpha/the-economics-of-large-language-models-2671985b621c](https://medium.com/emalpha/the-economics-of-large-language-models-2671985b621c)。
- en: Wang et al. (2019) Wang, Z., Wohlwend, J. and Lei, T. Structured pruning of
    large language models. *arXiv preprint arXiv:1910.04732*, 2019.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2019) Wang, Z., Wohlwend, J. 和 Lei, T. 大型语言模型的结构化剪枝。*arXiv 预印本 arXiv:1910.04732*，2019。
- en: Xia et al. (2022) Xia, M., Zhong, Z. and Chen, D. Structured pruning learns
    compact and accurate models. *arXiv preprint arXiv:2204.00408*, 2022.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia 等人 (2022) Xia, M., Zhong, Z. 和 Chen, D. 结构化剪枝学习紧凑且准确的模型。*arXiv 预印本 arXiv:2204.00408*，2022。
- en: 'Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J. and Han,
    S. Smoothquant: Accurate and efficient post-training quantization for large language
    models. In *International Conference on Machine Learning*, pp.  38087–38099\.
    PMLR, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人 (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J. 和 Han, S.
    SmoothQuant：用于大型语言模型的准确且高效的后训练量化。在*国际机器学习会议*，第 38087–38099 页。PMLR，2023。
- en: 'Xu et al. (2020) Xu, C., Zhou, W., Ge, T., Wei, F. and Zhou, M. Bert-of-theseus:
    Compressing bert by progressive module replacing. *arXiv preprint arXiv:2002.02925*,
    2020.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2020) Xu, C., Zhou, W., Ge, T., Wei, F. 和 Zhou, M. Bert-of-Theseus：通过逐步模块替换压缩
    BERT。*arXiv 预印本 arXiv:2002.02925*，2020。
- en: Zagoruyko & Komodakis (2016) Zagoruyko, S. and Komodakis, N. Wide residual networks.
    *arXiv preprint arXiv:1605.07146*, 2016.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zagoruyko & Komodakis (2016) Zagoruyko, S. 和 Komodakis, N. 宽残差网络。*arXiv 预印本
    arXiv:1605.07146*, 2016。
- en: Zhang et al. (2023) Zhang, M., Shen, C., Yang, Z., Ou, L., Yu, X., Zhuang, B.
    et al. Pruning meets low-rank parameter-efficient fine-tuning. *arXiv preprint
    arXiv:2305.18403*, 2023.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023) Zhang, M., Shen, C., Yang, Z., Ou, L., Yu, X., Zhuang, B.
    等. 修剪遇上低秩参数高效微调。*arXiv 预印本 arXiv:2305.18403*, 2023。
- en: Appendix A Main Experiment Details
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 主要实验细节
- en: A.1 Hyper-parameters for all Bonsai regression during pruning
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 修剪期间所有 Bonsai 回归的超参数
- en: When using Bonsai, we estimate $\beta$.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Bonsai 时，我们估算 $\beta$。
- en: 'Table 8: Bonsai hyper-parameters for regression. This applies to all experiments
    unless otherwise specified'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 回归的 Bonsai 超参数。这适用于所有实验，除非另有说明'
- en: '| $\gamma$(Regression Weight) | Learning rate | Batch Size | Epochs |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| $\gamma$(回归权重) | 学习率 | 批量大小 | 轮次 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| {100, 0, 1e-4} | {100, 10, 1, 0.1} | {32, 64, 128} | 50 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| {100, 0, 1e-4} | {100, 10, 1, 0.1} | {32, 64, 128} | 50 |'
- en: During cross validation, we choose the model whose predictions have the best
    Kendall rank correlation co-efficient (Kendall, [1948](#bib.bib24)) with the target.
    We do this because we do not care about matching $U_{k}$ reasonably models relative
    module importances.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证过程中，我们选择预测与目标具有最佳 Kendall 排名相关系数的模型（Kendall, [1948](#bib.bib24)）。我们这样做是因为我们不关心匹配
    $U_{k}$ 合理模型的相对模块重要性。
- en: In general, we use $\ell_{1}$-norm works better.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，$\ell_{1}$-范数效果更好。
- en: A.2 Forward Pass Only Experiments
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 仅前向传递实验
- en: 'Table [10](#A1.T10 "Table 10 ‣ A.2 Forward Pass Only Experiments ‣ Appendix
    A Main Experiment Details ‣ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") show the Bonsai hyperparameters we used for the experiments
    in Section [6.1](#S6.SS1 "6.1 Bonsai produces fast and performant models with
    only forward passes ‣ 6 Main Results ‣ Everybody Prune Now: Structured Pruning
    of LLMs with only Forward Passes").'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [10](#A1.T10 "表 10 ‣ A.2 仅前向传递实验 ‣ 附录 A 主要实验细节 ‣ 现在修剪所有：仅前向传递的 LLM 结构化修剪")
    显示了我们在第 [6.1](#S6.SS1 "6.1 Bonsai 生成快速且高性能的模型，只有前向传递 ‣ 6 主要结果 ‣ 现在修剪所有：仅前向传递的
    LLM 结构化修剪") 节实验中使用的 Bonsai 超参数。
- en: 'Table 9: Bonsai hyper-params for forward only experiments'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 仅前向实验的 Bonsai 超参数'
- en: '| $p_{\mathrm{iter}}$ |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| $p_{\mathrm{iter}}$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0.05 | 200 | 32 (per-iter) | Wanda |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 0.05 | 200 | 32 (每次迭代) | Wanda |'
- en: 'Table 10: Bonsai  fine-tuning HP for pruned LLaMA family models'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: 针对修剪后的 LLaMA 家族模型的 Bonsai 微调超参数'
- en: '| LR | rank | LoRA-$\alpha$ (Distill Weight) | LoRA Modules |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | rank | LoRA-$\alpha$ (蒸馏权重) | LoRA 模块 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1e-4 | 128 | 4$\times$rank | 0.01 | All Modules |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 1e-4 | 128 | 4$\times$rank | 0.01 | 所有模块 |'
- en: 'For Wanda(Sun et al., [2023](#bib.bib44)), we use the default hyper-parameters
    specified by [the paper repo here](https://github.com/locuslab/wanda/tree/main)
    for pruning. For fine-tuning, we use rank = 64\. We apply LoRA to only the q_proj
    and v_proj matrices in each layer of the pruned LLaMA model – this is unlike with
    Bonsai where we fine-tune all modules. We cannot do same because since the Wanda
    model just produces sparse matrices, the matrices instantiated during the backward
    pass are the same sizes as the sparsified matrices and thus occupy more memory
    (compared to our approach that actually makes the matrices smaller in dimension
    instead of sparsifying). We are also unable to perform distillation on the Wanda
    models due to this reason. For fine-tuning the Phi-2 model on Wikitext-2, we use
    the same hyper-parameters as Bonsai in Table [10](#A1.T10 "Table 10 ‣ A.2 Forward
    Pass Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes").'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Wanda(Sun et al., [2023](#bib.bib44))，我们使用 [论文仓库中的默认超参数](https://github.com/locuslab/wanda/tree/main)
    进行修剪。对于微调，我们使用 rank = 64。我们只对修剪后的 LLaMA 模型中的每层 q_proj 和 v_proj 矩阵应用 LoRA——这与 Bonsai
    不同，后者会微调所有模块。我们不能这样做，因为 Wanda 模型只生成稀疏矩阵，反向传递期间实例化的矩阵大小与稀疏矩阵相同，因此占用更多内存（与我们的方法相比，我们的方法实际上使矩阵在维度上变小而不是稀疏化）。由于这个原因，我们也无法对
    Wanda 模型进行蒸馏。对于在 Wikitext-2 上微调 Phi-2 模型，我们使用表 [10](#A1.T10 "表 10 ‣ A.2 仅前向传递实验
    ‣ 附录 A 主要实验细节 ‣ 现在修剪所有：仅前向传递的 LLM 结构化修剪") 中与 Bonsai 相同的超参数。
- en: A.3 Experiments comparing to Gradient based structured pruning
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 与基于梯度的结构化修剪的比较实验
- en: We compare to LoRA-Prune and LLM-Pruner. We take their performance results directly
    from the LoRA-Prune paper. Whilst we use 1 A6000 GPU (48G) for all experiments,
    LoRA-Prune uses A100 GPU (80G) for pruning LLaMA-1 7B.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与 LoRA-Prune 和 LLM-Pruner 进行比较。我们直接从 LoRA-Prune 论文中获取他们的性能结果。虽然我们在所有实验中使用
    1 个 A6000 GPU (48G)，但 LoRA-Prune 使用 A100 GPU (80G) 对 LLaMA-1 7B 进行剪枝。
- en: 'All Bonsai hyper-parameters are the same as Appendix [A.2](#A1.SS2 "A.2 Forward
    Pass Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes") except for $\mathrm{ns}_{\mathrm{sub-models}}$.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Bonsai 超参数与附录 [A.2](#A1.SS2 "A.2 仅前向传递实验 ‣ 附录 A 主要实验细节 ‣ 现在都修剪吧：仅前向传递的 LLM
    结构化剪枝") 相同，除了 $\mathrm{ns}_{\mathrm{sub-models}}$。
- en: A.4 Phi-2 pruning experiment details
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 Phi-2 剪枝实验细节
- en: 'For the experiment in Section [6.3](#S6.SS3 "6.3 Bonsai can produce compressed
    models with strong zero-shot abilities ‣ 6 Main Results ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes"), All Bonsai hyper-parameters
    are the same as Appendix [A.2](#A1.SS2 "A.2 Forward Pass Only Experiments ‣ Appendix
    A Main Experiment Details ‣ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") except for the following changes:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第 [6.3](#S6.SS3 "6.3 Bonsai 可以生成具有强零-shot 能力的压缩模型 ‣ 6 主要结果 ‣ 现在都修剪吧：仅前向传递的
    LLM 结构化剪枝") 节中的实验，所有 Bonsai 超参数与附录 [A.2](#A1.SS2 "A.2 仅前向传递实验 ‣ 附录 A 主要实验细节 ‣
    现在都修剪吧：仅前向传递的 LLM 结构化剪枝") 相同，除了以下更改：
- en: •
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathrm{ns}_{\mathrm{sub-models}}=2000$
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathrm{ns}_{\mathrm{sub-models}}=2000$
- en: •
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $p_{\mathrm{iter}}=0.35$. We thus perform 1-shot pruning directly to the target
    sparsity of 35%. We find that this seems to work best for the Phi-2 model. We
    posit that this might be because the Phi-2 models use LayerNorm(Ba et al., [2016](#bib.bib3))
    whilst the other models we explore, LLaMA and Mistral use RMSNorm.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $p_{\mathrm{iter}}=0.35$。因此，我们直接进行 1 次剪枝到目标稀疏度 35%。我们发现这似乎对 Phi-2 模型效果最佳。我们认为这可能是因为
    Phi-2 模型使用了 LayerNorm (Ba 等人，[2016](#bib.bib3))，而我们探讨的其他模型，如 LLaMA 和 Mistral 使用了
    RMSNorm。
- en: •
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Due to its relatively small size, the 1.8B pruned model can be fully fine-tuned
    on a single A6000 GPU over 100k sequences of length 2,048 tokens from the C4 dataset
    instead of using LoRA.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于其相对较小的规模，1.8B 剪枝模型可以在单个 A6000 GPU 上完全微调，处理 C4 数据集中长度为 2,048 词元的 100k 序列，而无需使用
    LoRA。
- en: Appendix B Impact of regression and perturbation ablation details
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 回归和扰动消融细节的影响
- en: 'For the experiment in Section [3](#S7.F3 "Figure 3 ‣ 7 Analysis ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes"), All Bonsai hyper-parameters
    are the same as Appendix [A.2](#A1.SS2 "A.2 Forward Pass Only Experiments ‣ Appendix
    A Main Experiment Details ‣ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") except $p_{\mathrm{iter}}=0.1$ to speed up pruning.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第 [3](#S7.F3 "图 3 ‣ 7 分析 ‣ 现在都修剪吧：仅前向传递的 LLM 结构化剪枝") 节中的实验，所有 Bonsai 超参数与附录
    [A.2](#A1.SS2 "A.2 仅前向传递实验 ‣ 附录 A 主要实验细节 ‣ 现在都修剪吧：仅前向传递的 LLM 结构化剪枝") 相同，除了 $p_{\mathrm{iter}}=0.1$
    以加速剪枝。
- en: 'A simple alternative to Bonsai is to leverage the prior $\rho$ for this experiment.
    Module level analogues of the unstructured pruning metrics we explore are defined
    in Appendix [F](#A6 "Appendix F Impact of prior ‣ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes").'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Bonsai 的一个简单替代方案是利用该实验的先验 $\rho$。我们探讨的无结构剪枝度量的模块级别类似物在附录 [F](#A6 "附录 F 先验的影响
    ‣ 现在都修剪吧：仅前向传递的 LLM 结构化剪枝") 中定义。
- en: 'Table 11: Experiment on linear regression to estimate module importances. Wikitext-2
    Perplexity. LLaMA-2 7B pruned to 50% sparsity'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：用于估算模块重要性的线性回归实验。Wikitext-2 困惑度。LLaMA-2 7B 剪枝至 50% 稀疏性。
- en: '| Linear Regression | Relative Speepdup | w/o Post-Pruning Adaptation | w Post-Pruning
    Adaptation |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 线性回归 | 相对加速 | 无后剪枝适应 | 有后剪枝适应 |'
- en: '| No | $2.06$ |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 否 | $2.06$ |'
- en: '| Yes | $1.77$ |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 是 | $1.77$ |'
- en: Appendix C Varying the pruning fraction per-iteration
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 每次迭代变更剪枝比例
- en: 'For the experiment in Section [3](#S7.F3 "Figure 3 ‣ 7 Analysis ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes"), All Bonsai hyper-parameters
    are the same as Appendix [A.2](#A1.SS2 "A.2 Forward Pass Only Experiments ‣ Appendix
    A Main Experiment Details ‣ Everybody Prune Now: Structured Pruning of LLMs with
    only Forward Passes") except we vary $p_{\mathrm{iter}}$.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[3节](#S7.F3 "图 3 ‣ 7 分析 ‣ 现在修剪每个人：仅使用前向传递的LLM结构修剪")的实验中，所有Bonsai超参数与[附录A.2](#A1.SS2
    "A.2 仅前向传递实验 ‣ 附录 A 主要实验细节 ‣ 现在修剪每个人：仅使用前向传递的LLM结构修剪")相同，唯一的区别是我们改变了$p_{\mathrm{iter}}$。
- en: 'Table 12: Varying the fraction pruned at a time. Wikitext-2 Perplexity. LLaMA-2
    7B pruned to 50% sparsity'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12: 每次修剪的比例变化。Wikitext-2 困惑度。LLaMA-2 7B 剪枝至50%稀疏'
- en: '| Prune Frac | Relative Speepdup | w/o Post-Pruning Adaptation | w Post-Pruning
    Adaptation |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 修剪比例 | 相对加速 | 无后修剪适应 | 有后修剪适应 |'
- en: '| 0.05 | $1.58$ |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 0.05 | $1.58$ |'
- en: '| 0.1 | $1.77$ |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 0.1 | $1.77$ |'
- en: '| 0.20 | $1.67$ |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 0.20 | $1.67$ |'
- en: Appendix D Varying the number of calibration data points for pruning
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 修剪的标定数据点数量变化
- en: 'All Bonsai hyper-parameters are the same as Appendix [A.2](#A1.SS2 "A.2 Forward
    Pass Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody Prune Now:
    Structured Pruning of LLMs with only Forward Passes") except we vary $\mathrm{ns}_{\mathrm{data}}$
    to speed up pruning.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Bonsai超参数与[附录A.2](#A1.SS2 "A.2 仅前向传递实验 ‣ 附录 A 主要实验细节 ‣ 现在修剪每个人：仅使用前向传递的LLM结构修剪")相同，但我们改变了$\mathrm{ns}_{\mathrm{data}}$以加快修剪速度。
- en: 'Table 13: How many data-points to consider during forward passes. Wikitext-2
    Perplexity. Llama-2 7B pruned to 50% sparsity'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '表 13: 在前向传递过程中考虑的数据点数量。Wikitext-2 困惑度。Llama-2 7B 剪枝至50%稀疏'
- en: '| $\mathrm{ns}_{\mathrm{data}}$ | w/o Adapt | w Adapt |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| $\mathrm{ns}_{\mathrm{data}}$ | 无适应 | 有适应 |'
- en: '| --- | --- | --- |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 8 | $130.04$ |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 8 | $130.04$ |'
- en: '| 32 | $61.63$ |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 32 | $61.63$ |'
- en: Appendix E Post-pruning adaptation
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 后修剪适应
- en: 'For this experiment, All Bonsai hyper-parameters are the same as Appendix [A.2](#A1.SS2
    "A.2 Forward Pass Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes").'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此实验，所有Bonsai超参数与[附录A.2](#A1.SS2 "A.2 仅前向传递实验 ‣ 附录 A 主要实验细节 ‣ 现在修剪每个人：仅使用前向传递的LLM结构修剪")相同。
- en: Appendix F Impact of prior
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 先验的影响
- en: 'For this experiment, All Bonsai hyper-parameters are the same as Appendix [A.2](#A1.SS2
    "A.2 Forward Pass Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") except we vary
    $\rho$'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此实验，所有Bonsai超参数与[附录A.2](#A1.SS2 "A.2 仅前向传递实验 ‣ 附录 A 主要实验细节 ‣ 现在修剪每个人：仅使用前向传递的LLM结构修剪")相同，但我们改变了$\rho$。
- en: F.1 $\rho$ is Activation Magnitude
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 $\rho$ 是激活幅度
- en: 'MLP / Fully Connected Module: Let $d$ and then compute the following averaged
    activation magnitude :'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: MLP / 全连接模块：设$d$，然后计算以下平均激活幅度：
- en: '|  | $\left(\rho\in\mathbb{R}^{d}\right)\propto\hat{\mathbf{a}}=\frac{1}{B}\sum_{b}\mathrm{Mean}\bigg{(}\big{&#124;}\mathbf{a}_{b}\big{&#124;},\mathrm{axis=}0\bigg{)}$
    |  | (4) |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left(\rho\in\mathbb{R}^{d}\right)\propto\hat{\mathbf{a}}=\frac{1}{B}\sum_{b}\mathrm{Mean}\bigg{(}\big{|}\mathbf{a}_{b}\big{|},\mathrm{axis=}0\bigg{)}$
    |  | (4) |'
- en: 'Self-Attention Module: For any data-sample sequence $b$.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力模块：对于任何数据样本序列$b$。
- en: '|  | $\left(\rho\in\mathbb{R}^{h}\right)\propto\hat{\mathbf{a}}$ |  | (5) |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left(\rho\in\mathbb{R}^{h}\right)\propto\hat{\mathbf{a}}$ |  | (5) |'
- en: F.2 $\rho$ is Wanda (Sun et al., [2023](#bib.bib44))
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 $\rho$ 是Wanda（Sun et al., [2023](#bib.bib44)）
- en: 'MLP / Fully Connected Module: Let $d$ and then compute the following metric
    which is a module level analogue of Wanda:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: MLP / 全连接模块：设$d$，然后计算以下度量，该度量是Wanda的模块级类似物：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: 'Self-Attention Module: Let $W\in\mathbb{R}^{d\times o}$.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力模块：设$W\in\mathbb{R}^{d\times o}$。
- en: Appendix G How many perturbative samples are reasonable?
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 合理的扰动样本数量？
- en: 'For this experiment, All Bonsai hyper-parameters are the same as Appendix [A.2](#A1.SS2
    "A.2 Forward Pass Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") except $p_{\mathrm{iter}}=0.1$
    to speed up pruning.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此实验，所有Bonsai超参数与[附录A.2](#A1.SS2 "A.2 仅前向传递实验 ‣ 附录 A 主要实验细节 ‣ 现在修剪每个人：仅使用前向传递的LLM结构修剪")相同，但$p_{\mathrm{iter}}=0.1$，以加快修剪速度。
- en: 'Table 14: Varying the number of sub-models generated. Wikitext-2 Perplexity.
    LLaMA-2 7B pruned to 50% sparsity'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '表 14: 生成的子模型数量变化。Wikitext-2 困惑度。LLaMA-2 7B 剪枝至50%稀疏'
- en: '| Num Samples | w/o Post-Pruning Adaptation | w Post-Pruning Adaptation |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 样本数 | 无后剪枝适配 | 有后剪枝适配 |'
- en: '| 1000 | $22.09$ |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | $22.09$ |'
- en: '| 200 | $61.63$ |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 200 | $61.63$ |'
- en: '| 50 | NaN | $9.24$ |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 50 | NaN | $9.24$ |'
- en: Using $\mathrm{ns}_{\mathrm{sub-models}}=50$ results in an model with NaN perplexity
    on the Wikitext validation set. We posit that this is because of the LLaMA models
    are half precision, and removing the wrong modules can result in activations going
    outside of the FP16 dynamic range for unique data-points. Note that we are able
    to recover good performance of the model after fine-tuning though (we do not observe
    NaNs with the Wikitext-2 training data). This indicates that Bonsai actually recovers
    good modules even using as few samples as 50 sub-models.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 $\mathrm{ns}_{\mathrm{sub-models}}=50$ 导致在 Wikitext 验证集上模型的困惑度为 NaN。我们推测这是因为
    LLaMA 模型是半精度的，移除错误的模块可能会导致激活超出 FP16 动态范围。请注意，经过微调后，我们能够恢复模型的良好性能（在 Wikitext-2
    训练数据中未观察到 NaNs）。这表明即使使用少量样本如 50 个子模型，Bonsai 实际上也能恢复良好的模块。
- en: Appendix H Mistral-7B Experiment Details
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H Mistral-7B 实验细节
- en: In addition to the primary experiments on the LLaMA and Phi-2 models, supplementary
    experiments were performed on the Mistral-7B (Jiang et al., [2023](#bib.bib21))
    model in comparison with Wanda results on the stated model. We apply Bonsai with
    the same hardware and configuration settings as used for the LLaMA and Phi-2 experiments.
    We target different pruning fractions (0.05, 0.1, and 0.2) across different numbers
    of samples and masks per iteration to evaluate the method’s performance under
    varying sparsity conditions.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对 LLaMA 和 Phi-2 模型的主要实验外，还对 Mistral-7B (Jiang et al., [2023](#bib.bib21))
    模型进行了补充实验，并与上述模型的 Wanda 结果进行了比较。我们在与 LLaMA 和 Phi-2 实验相同的硬件和配置设置下应用了 Bonsai。我们针对不同的剪枝比例（0.05、0.1
    和 0.2）在不同数量的样本和每次迭代的掩码下评估该方法在不同稀疏条件下的性能。
- en: 'The Mistral-7B model architecture differs from the LLaMA architecture in its
    use of group query attention and sliding window attention in lieu of the standard
    self-attention used in most transformer-based models like LLaMA (Jiang et al.,
    [2023](#bib.bib21)). We factor these differences into consideration in the implementation
    of Bonsai for Mistral. For the experiments that produced the results below, all
    Bonsai hyper-parameters are the same as Appendix [A.2](#A1.SS2 "A.2 Forward Pass
    Only Experiments ‣ Appendix A Main Experiment Details ‣ Everybody Prune Now: Structured
    Pruning of LLMs with only Forward Passes").'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 'Mistral-7B 模型架构与 LLaMA 架构的不同之处在于使用了组查询注意力和滑动窗口注意力，而不是大多数基于 Transformer 的模型（如
    LLaMA）中使用的标准自注意力（Jiang et al., [2023](#bib.bib21)）。我们在 Mistral 的 Bonsai 实现中考虑了这些差异。对于生成以下结果的实验，所有
    Bonsai 超参数与附录 [A.2](#A1.SS2 "A.2 Forward Pass Only Experiments ‣ Appendix A Main
    Experiment Details ‣ Everybody Prune Now: Structured Pruning of LLMs with only
    Forward Passes") 相同。'
- en: 'Table [15](#A8.T15 "Table 15 ‣ Appendix H Mistral-7B Experiment Details ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") presents the
    test perplexity results for Mistral-7B under different pruning methods. Considering
    the fully-structured sparsity nature of Bonsai, it achieves a test perplexity
    of 47.5 without post-pruning adaptation, with 1.66$\times$.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [15](#A8.T15 "Table 15 ‣ Appendix H Mistral-7B Experiment Details ‣ Everybody
    Prune Now: Structured Pruning of LLMs with only Forward Passes") 展示了不同剪枝方法下 Mistral-7B
    的测试困惑度结果。考虑到 Bonsai 的完全结构化稀疏特性，它在没有后剪枝适配的情况下实现了 47.5 的测试困惑度，具有 1.66$\times$。'
- en: 'Table 15: Test perplexity of Mistral-7B model on Wikitext-2 across fully-structured
    Bonsai and semi-structured Wanda methods.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 表 15：Mistral-7B 模型在 Wikitext-2 上采用完全结构化的 Bonsai 和半结构化的 Wanda 方法的测试困惑度。
- en: '|  | Sparsity Level | Method | Test PPL |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | 稀疏度级别 | 方法 | 测试 PPL |'
- en: '| Original, unpruned Mistral-7B | N/A | N/A | 5.245 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 原始未剪枝的 Mistral-7B | 不适用 | 不适用 | 5.245 |'
- en: '| Wanda | semi-structured 2-4 | magnitude | 13.81 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 半结构化 2-4 | 大小 | 13.81 |'
- en: '| Wanda | 12.38 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 12.38 |'
- en: '| SparseGPT | 10.46 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 10.46 |'
- en: '| Bonsai (w/o Adaptation) | structured 50% | magnitude | 67.48 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| Bonsai (无适配) | 结构化 50% | 大小 | 67.48 |'
- en: '| Wanda | 47.50 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 47.50 |'
- en: '| Bonsai (w/ Adaptation) | structured 50% | Wanda | 10.08 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Bonsai (有适配) | 结构化 50% | Wanda | 10.08 |'
- en: 'We further investigate the pruning habits of Bonsai by examining the pruned
    layers of Mistral, as shown in Figure [4](#A8.F4 "Figure 4 ‣ Appendix H Mistral-7B
    Experiment Details ‣ Everybody Prune Now: Structured Pruning of LLMs with only
    Forward Passes"). We notice a recurring theme: when an attention layer is significantly
    altered, it leads to compensation in the next layers within the sequence. This
    adaptive behavior, termed the ”Hydra effect” by McGrath et al. ([2023](#bib.bib33)),
    implies that the layers within a language model interact in a way that changes
    in one layer prompt adjustments in another. McGrath et al. ([2023](#bib.bib33))
    specifically mentioned that when one attention layer was removed from a language
    model, the model was still able to self-repair and produce similar outputs; but
    it did so by relying more heavily on other layers.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过检查 Mistral 的剪枝层进一步研究 Bonsai 的剪枝习惯，如图 [4](#A8.F4 "图 4 ‣ 附录 H Mistral-7B 实验详情
    ‣ 现在剪枝吧：仅通过前向传递的 LLM 结构化剪枝") 所示。我们注意到一个反复出现的主题：当一个注意力层被显著改变时，会导致序列中的下一个层进行补偿。这种自适应行为被
    McGrath 等（[2023](#bib.bib33)）称为“九头蛇效应”，意味着语言模型中的层以某种方式相互作用，一个层的变化促使另一个层进行调整。McGrath
    等（[2023](#bib.bib33)）特别提到，当从语言模型中移除一个注意力层时，模型仍然能够自我修复并产生类似的输出，但它会更加依赖其他层。
- en: '![Refer to caption](img/662bb7cfb2dc694c9131df9d6ae50772.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/662bb7cfb2dc694c9131df9d6ae50772.png)'
- en: 'Figure 4: Mistral’s pruned attention layers. The heavily pruned layers are
    usually preceded by or sandwiched between lightly-pruned layers, exhibiting the
    self-repairing ”Hydra effect” (McGrath et al., [2023](#bib.bib33)).'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：Mistral 的剪枝注意力层。被大量剪枝的层通常会前后被轻度剪枝的层包围，展现出自我修复的“九头蛇效应”（McGrath 等，[2023](#bib.bib33)）。
