- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:59:15'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:59:15'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss
    for LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朝向跨分词器蒸馏：LLMs 的通用 Logit 蒸馏损失
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.12030](https://ar5iv.labs.arxiv.org/html/2402.12030)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.12030](https://ar5iv.labs.arxiv.org/html/2402.12030)
- en: Nicolas Boizard^(1,3)  Kevin El Haddad¹  Céline Hudelot³  Pierre Colombo^(2,3)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Nicolas Boizard^(1,3)  Kevin El Haddad¹  Céline Hudelot³  Pierre Colombo^(2,3)
- en: ¹Diabolocom, Paris, France  ²Equall, Paris, France
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹Diabolocom, Paris, France  ²Equall, Paris, France
- en: ³MICS, CentraleSupélec, Université Paris-Saclay, France
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ³MICS, CentraleSupélec, Université Paris-Saclay, France
- en: '[nicolas.boizard@centralesupelec.fr](nicolas.boizard@centralesupelec.fr)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[nicolas.boizard@centralesupelec.fr](nicolas.boizard@centralesupelec.fr)'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deploying large language models (LLMs) of several billion parameters can be
    impractical in most industrial use cases due to constraints such as cost, latency
    limitations, and hardware accessibility. Knowledge distillation (KD) offers a
    solution by compressing knowledge from resource-intensive large models to smaller
    ones. Various strategies exist, some relying on the text generated by the teacher
    model and optionally utilizing his logits to enhance learning. However, these
    methods based on logits often require both teacher and student models to share
    the same tokenizer, limiting their applicability across different LLM families.
    In this paper, we introduce Universal Logit Distillation (ULD) loss, grounded
    in optimal transport, to address this limitation. Our experimental results demonstrate
    the effectiveness of ULD loss in enabling distillation across models with different
    architectures and tokenizers, paving the way to a more widespread use of distillation
    techniques.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数工业应用场景中，部署数十亿参数的大型语言模型（LLMs）可能不切实际，原因包括成本、延迟限制和硬件可达性等约束。知识蒸馏（KD）通过将资源密集型大模型的知识压缩到较小模型中提供了解决方案。存在各种策略，有些依赖于教师模型生成的文本，并可以选择使用其
    logits 来增强学习。然而，这些基于 logits 的方法通常要求教师和学生模型共享相同的分词器，从而限制了它们在不同 LLM 家族中的适用性。本文介绍了基于最优传输的通用
    Logit 蒸馏（ULD）损失，以解决这一限制。我们的实验结果证明了 ULD 损失在不同架构和分词器的模型之间进行蒸馏的有效性，为更广泛使用蒸馏技术铺平了道路。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: A noticeable trend has emerged in NLP with the prevalence of large language
    models (LLMs) such as LLama Touvron et al. ([2023a](#bib.bib47)), Mistral Jiang
    et al. ([2023](#bib.bib21)), Falcon Almazrouei et al. ([2023](#bib.bib1)), GPT-NeoX
    Black et al. ([2022](#bib.bib5)), or Mixtral Jiang et al. ([2024](#bib.bib22)).
    Despite the high performance of LLMs Bubeck et al. ([2023](#bib.bib7)), the challenges
    associated with resource consumption and deployment complexity have become increasingly
    prominent due to hardware availability, cost, and latency bottlenecks. Several
    methods, including efficient decoding Leviathan et al. ([2023](#bib.bib30)); Ye
    et al. ([2023](#bib.bib58)), model recycling Lester et al. ([2022](#bib.bib29)),
    and size reduction Dettmers et al. ([2023](#bib.bib13)); Ma et al. ([2023](#bib.bib33)),
    have been proposed to address the need for faster and more efficient deployment
    of these models. In this paper, we concentrate on knowledge distillation (KD)
    Buciluundefined et al. ([2006](#bib.bib8)); Hinton et al. ([2015](#bib.bib19)),
    a widely adopted technique Sanh et al. ([2020](#bib.bib41)); Jiao et al. ([2020](#bib.bib23));
    Mohammadshahi et al. ([2022](#bib.bib34)); He et al. ([2023](#bib.bib17)); Raman
    et al. ([2023](#bib.bib39)); Dasgupta et al. ([2023](#bib.bib12)), utilized by
    practitioners to distill the expertise of a larger teacher model into a compact
    student model, preserving maximum performance while reducing latency and memory
    footprint.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）领域，大型语言模型（LLMs）如 LLama Touvron 等 ([2023a](#bib.bib47))、Mistral Jiang
    等 ([2023](#bib.bib21))、Falcon Almazrouei 等 ([2023](#bib.bib1))、GPT-NeoX Black
    等 ([2022](#bib.bib5)) 和 Mixtral Jiang 等 ([2024](#bib.bib22)) 的普及引发了一个显著趋势。尽管 LLMs
    Bubeck 等 ([2023](#bib.bib7)) 的性能很高，但由于硬件可用性、成本和延迟瓶颈，资源消耗和部署复杂性相关的挑战变得越来越突出。为了满足这些模型更快、更高效的部署需求，已经提出了几种方法，包括高效解码
    Leviathan 等 ([2023](#bib.bib30))；Ye 等 ([2023](#bib.bib58))、模型回收 Lester 等 ([2022](#bib.bib29))
    和规模缩减 Dettmers 等 ([2023](#bib.bib13))；Ma 等 ([2023](#bib.bib33))。本文集中讨论了知识蒸馏（KD）
    Buciluundefined 等 ([2006](#bib.bib8))；Hinton 等 ([2015](#bib.bib19))，这是一种被广泛采用的技术
    Sanh 等 ([2020](#bib.bib41))；Jiao 等 ([2020](#bib.bib23))；Mohammadshahi 等 ([2022](#bib.bib34))；He
    等 ([2023](#bib.bib17))；Raman 等 ([2023](#bib.bib39))；Dasgupta 等 ([2023](#bib.bib12))，用于将大型教师模型的专业知识蒸馏到紧凑的学生模型中，在减少延迟和内存占用的同时保持最大性能。
- en: 'Over the past years, NLP researchers have extensively explored and applied
    knowledge distillation mostly on smaller student models derived from BERT Sanh
    et al. ([2020](#bib.bib41)); Jiao et al. ([2020](#bib.bib23)); Sun et al. ([2020](#bib.bib45)).
    These smaller student models maintain a similar architecture to the teacher, mirroring
    some of its blocks, hidden sizes, or relying on the same tokenizer. Two approaches
    can generally be considered: the *white box* approach, where researchers propose
    loss functions that directly compute similarities across layers, and the *black
    box* approach, offering flexibility, agnostic to teacher latent states. This black
    box approach can be readily implemented by practitioners through libraries and
    APIs, simplifying its adoption.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，NLP研究人员广泛探索并应用了知识蒸馏，主要集中在从BERT Sanh等人（[2020](#bib.bib41)）；Jiao等人（[2020](#bib.bib23)）；Sun等人（[2020](#bib.bib45)）派生的小型学生模型上。这些小型学生模型保持了与教师模型类似的架构，镜像了一些其模块、隐藏层大小，或依赖相同的分词器。通常可以考虑两种方法：*白盒*方法，研究人员提出直接计算跨层相似性的损失函数；以及*黑盒*方法，提供灵活性，不依赖于教师潜在状态。黑盒方法可以通过库和API由从业者轻松实现，简化了其采纳。
- en: '![Refer to caption](img/0d78560fe9b56e10fe8a374b57334c07.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0d78560fe9b56e10fe8a374b57334c07.png)'
- en: 'Figure 1: Ratio of student vocabulary contained in teacher tokenizer. (e.g.,
    Bloomz tokenizer has 30.03% of Mistral’s vocabulary). For student and teacher
    model information see [Sec. 4.2](#S4.SS2 "4.2 Experimental Choices ‣ 4 Experimental
    Setting ‣ Towards Cross-Tokenizer Distillation: the Universal Logit Distillation
    Loss for LLMs")'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：教师分词器中包含的学生词汇比例。（例如，Bloomz分词器包含30.03%的Mistral词汇）。有关学生和教师模型的信息，请参见[第4.2节](#S4.SS2
    "4.2 实验选择 ‣ 4 实验设置 ‣ 跨分词器蒸馏：LLMs的通用Logit蒸馏损失")。
- en: 'However, KD for generative models, those relying on encoder-decoder or decoder
    architectures, has received less attention. Recent research, predominantly focuses
    on synthetic data fine-tuning He et al. ([2022](#bib.bib18)); Kramchaninova and
    Defauw ([2022](#bib.bib27)); Ouyang et al. ([2022](#bib.bib36)), with less emphasis
    on refining loss functions in the black box approach. Thus far, the primary method
    for KD involves using text generated by the teacher model He et al. ([2023](#bib.bib17));
    Hsieh et al. ([2023](#bib.bib20)), and optionally, augmenting it with logits,
    employing Kullback–Leibler divergence between the teacher logits distribution
    and student ones Mohammadshahi et al. ([2022](#bib.bib34)); Raman et al. ([2023](#bib.bib39));
    Wang et al. ([2020a](#bib.bib52)). Although the logit distillation method yields
    significant improvements, it has been underutilized in recent studies due to its
    requirement for the student model to share the same tokenizer as the teacher ([Sec. 2.4](#S2.SS4
    "2.4 KL Distillation loss ‣ 2 Problem Formulation & Related Work ‣ Towards Cross-Tokenizer
    Distillation: the Universal Logit Distillation Loss for LLMs") and [Fig. 2](#S2.F2
    "Figure 2 ‣ 2.4 KL Distillation loss ‣ 2 Problem Formulation & Related Work ‣
    Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for
    LLMs")). Indeed, even when encoder-decoder or decoder-only models of varying sizes
    are available, they often do not share the same architecture nor the same tokenizer
    ([Fig. 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Towards Cross-Tokenizer Distillation:
    the Universal Logit Distillation Loss for LLMs")), rendering Logit Distillation
    loss inapplicable. This raises the following research question:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，依赖于编码器-解码器或解码器架构的生成模型的KD（知识蒸馏）受到的关注较少。最近的研究主要集中在合成数据的微调上 He等人（[2022](#bib.bib18)）；Kramchaninova和Defauw（[2022](#bib.bib27)）；Ouyang等人（[2022](#bib.bib36)），对黑盒方法中的损失函数的改进关注较少。迄今为止，KD的主要方法涉及使用教师模型生成的文本
    He等人（[2023](#bib.bib17)）；Hsieh等人（[2023](#bib.bib20)），并可选择性地通过logits进行增强，使用教师logits分布和学生logits分布之间的Kullback–Leibler散度
    Mohammadshahi等人（[2022](#bib.bib34)）；Raman等人（[2023](#bib.bib39)）；Wang等人（[2020a](#bib.bib52)）。虽然logit蒸馏方法带来了显著的改进，但由于其要求学生模型与教师模型共享相同的分词器（[第2.4节](#S2.SS4
    "2.4 KL蒸馏损失 ‣ 2 问题定义与相关工作 ‣ 跨分词器蒸馏：LLMs的通用Logit蒸馏损失") 和 [图2](#S2.F2 "图2 ‣ 2.4
    KL蒸馏损失 ‣ 2 问题定义与相关工作 ‣ 跨分词器蒸馏：LLMs的通用Logit蒸馏损失")），在近期研究中被使用较少。实际上，即使有不同规模的编码器-解码器或仅解码器模型，它们通常也不共享相同的架构或相同的分词器（[图1](#S1.F1
    "图1 ‣ 1 引言 ‣ 跨分词器蒸馏：LLMs的通用Logit蒸馏损失")），使得Logit蒸馏损失不适用。这引出了以下研究问题：
- en: How can we craft a Universal knowledge distillation loss within the black box
    approach, that is inherently versatile to teacher/student architectures and agnostic
    on tokenizers?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何在黑箱方法中构建一个通用的知识蒸馏损失，该损失本质上对教师/学生架构具有通用性，并且对分词器不敏感？
- en: 'Contributions:'
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贡献：
- en: 'In this paper, we make the following contributions:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们做出如下贡献：
- en: '1.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: A universal logit distillation loss. We introduce a new loss, Universal Logit
    Distillation Loss (ULD loss), with virtually no assumptions on the teacher and
    student architectures. Our approach harnesses a closed-form solution of optimal
    transport, rendering it ideal for large-scale fine-tuning due to its fast computation.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通用对数蒸馏损失。我们引入了一种新的损失函数，通用对数蒸馏损失（ULD loss），对教师和学生架构几乎没有假设。我们的方法利用了最优传输的闭式解，使其由于快速计算而非常适合大规模微调。
- en: '2.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Experimental Results. To showcase the consistent effectiveness of our loss,
    we rely on various tasks: extractive question answering, generative question answering,
    and summarization. Our evaluation spans multiple widely-used teacher-student pairs,
    with diverse vocabularies and model architectures.'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验结果。为了展示我们损失函数的一致有效性，我们依赖于多种任务：提取式问答、生成式问答和摘要。我们的评估涵盖了多个广泛使用的师生对，具有多样的词汇表和模型架构。
- en: '3.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: Contributing to future research. We make our code¹¹1[https://github.com/Nicolas-BZRD/llm-recipes](https://github.com/Nicolas-BZRD/llm-recipes)²²2[https://github.com/Nicolas-BZRD/llm-distillation](https://github.com/Nicolas-BZRD/llm-distillation),
    model weights, and generated datasets³³3[https://huggingface.co/Nicolas-BZRD](https://huggingface.co/Nicolas-BZRD)
    openly available to facilitate future research, minimizing computational overhead
    and lowering entry barriers.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贡献于未来的研究。我们将我们的代码¹¹1[https://github.com/Nicolas-BZRD/llm-recipes](https://github.com/Nicolas-BZRD/llm-recipes)²²2[https://github.com/Nicolas-BZRD/llm-distillation](https://github.com/Nicolas-BZRD/llm-distillation)，模型权重和生成的数据集³³3[https://huggingface.co/Nicolas-BZRD](https://huggingface.co/Nicolas-BZRD)
    公开，旨在促进未来的研究，减少计算开销并降低入门门槛。
- en: 2 Problem Formulation & Related Work
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题定义与相关工作
- en: 2.1 Notations
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 术语
- en: We define $\Omega$ the teacher’s vocabulary.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义 $\Omega$ 为教师的词汇表。
- en: Remark.
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注。
- en: In general $\Omega^{T}\neq\Omega^{S}$.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说 $\Omega^{T}\neq\Omega^{S}$。
- en: 'Conditional textual generation:'
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 条件文本生成：
- en: Conditional textual generation aims to model the probability distribution $\mathbf{p}_{\star}(\mathbf{x})$
    a generate token.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 条件文本生成旨在对生成令牌的概率分布 $\mathbf{p}_{\star}(\mathbf{x})$ 进行建模。
- en: 2.2 Knowledge Distillation Framework
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 知识蒸馏框架
- en: 'In knowledge distillation (KD), the objective is to guide the learning of a
    student model using a more complex teacher model Buciluundefined et al. ([2006](#bib.bib8));
    Hinton et al. ([2015](#bib.bib19)). Generally, this paradigm comprises two key
    components: a cross-entropy loss ($\mathcal{L}_{CE}$:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在知识蒸馏（KD）中，目标是使用更复杂的教师模型来指导学生模型的学习 Buciluundefined 等 ([2006](#bib.bib8)); Hinton
    等 ([2015](#bib.bib19))。通常，这种范式包括两个关键组件：交叉熵损失 ($\mathcal{L}_{CE}$：
- en: '|  | $\mathcal{L}=\mathcal{L}_{CE}+\lambda\times\mathcal{L}_{KD}$ |  | (1)
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\mathcal{L}_{CE}+\lambda\times\mathcal{L}_{KD}$ |  | (1)
    |'
- en: where $\lambda\in\mathbb{R}^{+}$ can be used to control the trade-off between
    learning exclusively from text and knowledge coming from the teacher.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda\in\mathbb{R}^{+}$ 可用于控制从文本学习与从教师处获得知识之间的权衡。
- en: 2.3 Knowledge Distillation Related Work
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 知识蒸馏相关工作
- en: 'Building on [Eq. 1](#S2.E1 "1 ‣ 2.2 Knowledge Distillation Framework ‣ 2 Problem
    Formulation & Related Work ‣ Towards Cross-Tokenizer Distillation: the Universal
    Logit Distillation Loss for LLMs"), various cases have been examined.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '基于 [Eq. 1](#S2.E1 "1 ‣ 2.2 Knowledge Distillation Framework ‣ 2 Problem Formulation
    & Related Work ‣ Towards Cross-Tokenizer Distillation: the Universal Logit Distillation
    Loss for LLMs")，已研究了各种情况。'
- en: 'Distillation from teacher-generated text:'
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从教师生成的文本中进行蒸馏：
- en: Distillation from teacher-generated text occurs when $\lambda=0$. This strategy
    is particularly advantageous when dealing with synthetic data Kramchaninova and
    Defauw ([2022](#bib.bib27)); Du et al. ([2023](#bib.bib14)); Ushio et al. ([2023](#bib.bib49)),
    a fact highlighted by the effectiveness of instructing large language models such
    as GPT-3.5/4 Wu et al. ([2023](#bib.bib54)); Bubeck et al. ([2023](#bib.bib7)).
    Distillation from teacher-generated text Kim and Rush ([2016](#bib.bib26)); He
    et al. ([2023](#bib.bib17)); Hsieh et al. ([2023](#bib.bib20)); Zhou and Chiam
    ([2023](#bib.bib63)) will be considered as a baseline throughout the paper. The
    primary drawback of these methods lies in their failure to fully leverage all
    the information that can be provided by the teacher.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当 $\lambda=0$ 时，从教师生成的文本进行蒸馏。这种策略在处理合成数据时特别有利 Kramchaninova and Defauw ([2022](#bib.bib27));
    Du et al. ([2023](#bib.bib14)); Ushio et al. ([2023](#bib.bib49))，这一点在对大语言模型如
    GPT-3.5/4 的指令效果中得到了体现 Wu et al. ([2023](#bib.bib54)); Bubeck et al. ([2023](#bib.bib7))。从教师生成的文本中进行蒸馏
    Kim and Rush ([2016](#bib.bib26)); He et al. ([2023](#bib.bib17)); Hsieh et al.
    ([2023](#bib.bib20)); Zhou and Chiam ([2023](#bib.bib63)) 将在本文中作为基准。此方法的主要缺点在于未能充分利用教师提供的所有信息。
- en: 'White-box approach:'
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 白箱方法：
- en: 'A further refinement of [Eq. 1](#S2.E1 "1 ‣ 2.2 Knowledge Distillation Framework
    ‣ 2 Problem Formulation & Related Work ‣ Towards Cross-Tokenizer Distillation:
    the Universal Logit Distillation Loss for LLMs") occurs when $\mathcal{L}_{KD}$
    relies on the internal features of the teacher to transfer knowledgeJiao et al.
    ([2020](#bib.bib23)); Sun et al. ([2020](#bib.bib45)). Popular features include
    transformer attention and internal layers within both encoder-only and encoder-decoder
    models Raman et al. ([2023](#bib.bib39)); Wang et al. ([2020a](#bib.bib52), [2021](#bib.bib51)).
    A main drawback is that these methods demand access to the models’ internal mechanisms,
    which is typically unavailable through API access. Moreover, they often assume
    similarities in architectural patterns between the teacher and the student.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '当 $\mathcal{L}_{KD}$ 依赖于教师的内部特征来转移知识时，将进一步细化 [Eq. 1](#S2.E1 "1 ‣ 2.2 Knowledge
    Distillation Framework ‣ 2 Problem Formulation & Related Work ‣ Towards Cross-Tokenizer
    Distillation: the Universal Logit Distillation Loss for LLMs") Jiao et al. ([2020](#bib.bib23));
    Sun et al. ([2020](#bib.bib45))。流行的特征包括 transformer 注意力和编码器-解码器模型中的内部层 Raman et
    al. ([2023](#bib.bib39)); Wang et al. ([2020a](#bib.bib52), [2021](#bib.bib51))。主要缺点是这些方法要求访问模型的内部机制，这通常无法通过
    API 访问。此外，它们通常假设教师和学生之间在架构模式上的相似性。'
- en: 'Black-box approach:'
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 黑箱方法：
- en: 'In the black-box approach, practitioners are limited to accessing only the
    output probabilities or logits of the model. They use these logits to align the
    student’s output probabilities with those of the teacher through Kullback–Leibler
    divergence (KL) Sanh et al. ([2020](#bib.bib41)). This method has emerged as one
    of the most widely adopted approaches, successfully distilling encoder, decoder,
    or encoder-decoder models Timiryasov and Tastet ([2023](#bib.bib46)); Mohammadshahi
    et al. ([2022](#bib.bib34)); Zhao et al. ([2023a](#bib.bib61)). However, employing
    KL divergence necessitates that both student and teacher share the same vocabulary,
    a requirement impractical with current large language models (LLMs) as reported
    in [Fig. 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Towards Cross-Tokenizer Distillation:
    the Universal Logit Distillation Loss for LLMs"). We dig into the limitations
    of this method in the next section.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '在黑箱方法中，实践者只能访问模型的输出概率或 logits。他们使用这些 logits 通过 Kullback–Leibler 散度 (KL) 将学生的输出概率与教师的对齐
    Sanh et al. ([2020](#bib.bib41))。该方法已经成为最广泛采用的方法之一，成功地对编码器、解码器或编码器-解码器模型进行了蒸馏
    Timiryasov and Tastet ([2023](#bib.bib46)); Mohammadshahi et al. ([2022](#bib.bib34));
    Zhao et al. ([2023a](#bib.bib61))。然而，使用 KL 散度需要学生和教师共享相同的词汇表，这在当前的大型语言模型 (LLMs)
    中是不切实际的，如 [图 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Towards Cross-Tokenizer Distillation:
    the Universal Logit Distillation Loss for LLMs") 所述。我们将在下一节中探讨这种方法的局限性。'
- en: 2.4 KL Distillation loss
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 KL 蒸馏损失
- en: 'When distilling using the KL Li et al. ([2021](#bib.bib31)), the goal is to
    force the student to learn the teacher’s output probability distribution at each
    generation step. The formal definition of the objective function is provided in
    [Eq. 2](#S2.E2 "2 ‣ 2.4 KL Distillation loss ‣ 2 Problem Formulation & Related
    Work ‣ Towards Cross-Tokenizer Distillation: the Universal Logit Distillation
    Loss for LLMs").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 KL Li et al. ([2021](#bib.bib31)) 进行蒸馏时，目标是迫使学生在每个生成步骤学习教师的输出概率分布。目标函数的正式定义见[公式
    2](#S2.E2 "2 ‣ 2.4 KL 蒸馏损失 ‣ 2 问题定义与相关工作 ‣ 跨分词器蒸馏：LLMs 的通用 Logit 蒸馏损失")。
- en: '|  | $\mathcal{L}=\sum_{t=1}^{&#124;\mathbf{x}&#124;}\text{CE}(t)+\lambda\text{KL}\left[\mathbf{q_{\mathbf{\bm{\theta}_{T}}}}(\cdot&#124;\mathbf{x}_{<t}),\mathbf{p_{\mathbf{\bm{\theta}_{S}}}}\left(\cdot&#124;\mathbf{x}_{<t}\right)\right]$
    |  | (2) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\sum_{t=1}^{|\mathbf{x}|}\text{CE}(t)+\lambda\text{KL}\left[\mathbf{q_{\mathbf{\bm{\theta}_{T}}}}(\cdot|\mathbf{x}_{<t}),\mathbf{p_{\mathbf{\bm{\theta}_{S}}}}\left(\cdot|\mathbf{x}_{<t}\right)\right]$
    |  | (2) |'
- en: where $|\mathbf{x}|$ controls the trade-off between the two terms.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $|\mathbf{x}|$ 控制两个项之间的权衡。
- en: Remark.
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注。
- en: '[Eq. 2](#S2.E2 "2 ‣ 2.4 KL Distillation loss ‣ 2 Problem Formulation & Related
    Work ‣ Towards Cross-Tokenizer Distillation: the Universal Logit Distillation
    Loss for LLMs") relies on equality across the vocabulary of the student and the
    teacher, i.e. $\Omega=\Omega^{S}=\Omega^{T}$, enabling us to compute the KL divergence
    by ensuring similar support of probabilities.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[公式 2](#S2.E2 "2 ‣ 2.4 KL 蒸馏损失 ‣ 2 问题定义与相关工作 ‣ 跨分词器蒸馏：LLMs 的通用 Logit 蒸馏损失")依赖于学生和教师词汇表的一致性，即
    $\Omega=\Omega^{S}=\Omega^{T}$，使我们能够通过确保概率支持的一致性来计算 KL 散度。'
- en: Remark.
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注。
- en: '[Eq. 2](#S2.E2 "2 ‣ 2.4 KL Distillation loss ‣ 2 Problem Formulation & Related
    Work ‣ Towards Cross-Tokenizer Distillation: the Universal Logit Distillation
    Loss for LLMs") also suppose absolute continuity for the distributions $\mathbf{p}_{\mathbf{\bm{\theta}_{S}}}(\cdot|\mathbf{x}_{<t})\ll\mathbf{q}_{\mathbf{\bm{\theta}_{T}}}(\cdot|\mathbf{x}_{<t})$,
    making the use of padding impractical.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[公式 2](#S2.E2 "2 ‣ 2.4 KL 蒸馏损失 ‣ 2 问题定义与相关工作 ‣ 跨分词器蒸馏：LLMs 的通用 Logit 蒸馏损失")还假设分布
    $\mathbf{p}_{\mathbf{\bm{\theta}_{S}}}(\cdot|\mathbf{x}_{<t})\ll\mathbf{q}_{\mathbf{\bm{\theta}_{T}}}(\cdot|\mathbf{x}_{<t})$
    的绝对连续性，使得使用填充变得不切实际。'
- en: 'Our examination of [Eq. 2](#S2.E2 "2 ‣ 2.4 KL Distillation loss ‣ 2 Problem
    Formulation & Related Work ‣ Towards Cross-Tokenizer Distillation: the Universal
    Logit Distillation Loss for LLMs") illustrating in [Fig. 2](#S2.F2 "Figure 2 ‣
    2.4 KL Distillation loss ‣ 2 Problem Formulation & Related Work ‣ Towards Cross-Tokenizer
    Distillation: the Universal Logit Distillation Loss for LLMs") highlights that
    both vocabulary and absolute continuity constraints pose challenges to distilling
    two distinct LLM families with logits using KL loss. In the following section,
    we introduce our ULD loss, providing a flexible framework for knowledge distillation
    across a wide range of LLMs.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对[公式 2](#S2.E2 "2 ‣ 2.4 KL 蒸馏损失 ‣ 2 问题定义与相关工作 ‣ 跨分词器蒸馏：LLMs 的通用 Logit 蒸馏损失")的研究如[图
    2](#S2.F2 "图 2 ‣ 2.4 KL 蒸馏损失 ‣ 2 问题定义与相关工作 ‣ 跨分词器蒸馏：LLMs 的通用 Logit 蒸馏损失")所示，强调了词汇表和绝对连续性约束对使用
    KL 损失蒸馏两个不同 LLM 家族的挑战。在接下来的部分，我们介绍了我们的 ULD 损失，为跨多种 LLM 的知识蒸馏提供了一个灵活的框架。
- en: '![Refer to caption](img/c94e47c56a5260854d72596f00c4c2ca.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c94e47c56a5260854d72596f00c4c2ca.png)'
- en: 'Figure 2: Distillation using ULD loss. In block 4, the KL divergence cannot
    be defined as the two distributions do not have the same support, breaking the
    absolute continuity of the quotient in the KL logarithmic term. To alleviate this
    we rely on the ULD loss which leverages a closed form of the Wasserstein distance.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：使用 ULD 损失进行蒸馏。在第 4 块中，由于两个分布不具有相同的支持，因此无法定义 KL 散度，破坏了 KL 对数项中的绝对连续性。为了解决这个问题，我们依赖于
    ULD 损失，它利用了 Wasserstein 距离的封闭形式。
- en: 3 Universal Logit Distillation
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 通用 Logit 蒸馏
- en: 3.1 Background on Optimal Transport
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 最优传输背景
- en: Optimal transport mathematically transfers mass between distributions, minimizing
    cost Villani et al. ([2009](#bib.bib50)); Peyré et al. ([2019](#bib.bib37)). In
    this context, Wasserstein distance, or Earth Mover Distance, robustly measures
    dissimilarities between distributions. This distance metric has gained popularity
    in NLP applications such as hallucination detection Guerreiro et al. ([2022](#bib.bib16));
    Shuster et al. ([2021](#bib.bib43)), clustering Zhuang et al. ([2022](#bib.bib64));
    Ye et al. ([2017](#bib.bib57)); Xu et al. ([2018](#bib.bib55)) or sentence similarity:Colombo
    et al. ([2021](#bib.bib10)); Xu et al. ([2018](#bib.bib55)); Bahuleyan et al.
    ([2018](#bib.bib2)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最优传输在数学上在分布之间转移质量，最小化成本 Villani et al. ([2009](#bib.bib50)); Peyré et al. ([2019](#bib.bib37))。在这种背景下，Wasserstein
    距离，或称地球搬运工距离，稳健地测量分布之间的差异。该距离度量在 NLP 应用中越来越受欢迎，如幻觉检测 Guerreiro et al. ([2022](#bib.bib16));
    Shuster et al. ([2021](#bib.bib43))，聚类 Zhuang et al. ([2022](#bib.bib64)); Ye
    et al. ([2017](#bib.bib57)); Xu et al. ([2018](#bib.bib55)) 或句子相似度：Colombo et
    al. ([2021](#bib.bib10)); Xu et al. ([2018](#bib.bib55)); Bahuleyan et al. ([2018](#bib.bib2))。
- en: 'Wasserstein distance:'
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Wasserstein 距离：
- en: 'The Wasserstein distance minimizes transport costs between sampled points from
    all possible couplings. Let us consider two sets of probability distributions,
    $\mathcal{P}\left(\Omega_{S}\right)$ are weight factors ensuring that the sum
    of weights is equal to 1\. Under this discrete setting, computing the Wasserstein
    distance is defined as:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Wasserstein 距离最小化从所有可能耦合中采样点之间的传输成本。我们考虑两个概率分布集合，$\mathcal{P}\left(\Omega_{S}\right)$
    是权重因子，确保权重之和等于 1。根据这一离散设置，计算 Wasserstein 距离的定义为：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: where $\Pi(\mathbf{p},\mathbf{q})$, minimizing the transportation cost defined
    by the absolute norm.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Pi(\mathbf{p},\mathbf{q})$，最小化由绝对范数定义的运输成本。
- en: Remark.
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注。
- en: 'Note that the Wasserstein distance (see [Eq. 3](#S3.E3 "3 ‣ Wasserstein distance:
    ‣ 3.1 Background on Optimal Transport ‣ 3 Universal Logit Distillation ‣ Towards
    Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs"))
    makes no assumptions about the support of $\mathbf{p}$, unlike the KL divergence,
    making it a natural choice for distillation.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '注意，Wasserstein 距离（参见 [Eq. 3](#S3.E3 "3 ‣ Wasserstein distance: ‣ 3.1 Background
    on Optimal Transport ‣ 3 Universal Logit Distillation ‣ Towards Cross-Tokenizer
    Distillation: the Universal Logit Distillation Loss for LLMs")）对 $\mathbf{p}$
    的支持没有假设，这使得它成为蒸馏的自然选择。'
- en: 3.2 Universal Logit Distillation loss
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 Universal Logit Distillation 损失
- en: 'The Universal Logit Distillation loss (ULD loss) is a novel distillation technique
    designed to virtually distill any generative model teacher into any student. It
    aims to overcome the limitations of KL divergence, as discussed in [Sec. 2.4](#S2.SS4
    "2.4 KL Distillation loss ‣ 2 Problem Formulation & Related Work ‣ Towards Cross-Tokenizer
    Distillation: the Universal Logit Distillation Loss for LLMs") and [Fig. 2](#S2.F2
    "Figure 2 ‣ 2.4 KL Distillation loss ‣ 2 Problem Formulation & Related Work ‣
    Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for
    LLMs").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 'Universal Logit Distillation 损失（ULD 损失）是一种新颖的蒸馏技术，旨在将任何生成模型教师虚拟蒸馏到任何学生。它旨在克服
    KL 散度的限制，如 [Sec. 2.4](#S2.SS4 "2.4 KL Distillation loss ‣ 2 Problem Formulation
    & Related Work ‣ Towards Cross-Tokenizer Distillation: the Universal Logit Distillation
    Loss for LLMs") 和 [Fig. 2](#S2.F2 "Figure 2 ‣ 2.4 KL Distillation loss ‣ 2 Problem
    Formulation & Related Work ‣ Towards Cross-Tokenizer Distillation: the Universal
    Logit Distillation Loss for LLMs") 中讨论的那样。'
- en: Intuition.
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 直觉。
- en: The ULD loss retains the CE loss term to guide the model in generating the target
    token and introduces a Wasserstein Distance term to transfer knowledge from the
    teacher to the student. By minimizing the distance between the soft probabilities
    of the teacher and the student, our goal is to reproduce not only the predictions
    for the golden token but also the near-zero labels, which are crucial for performance
    and generalization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ULD 损失保留了 CE 损失项以指导模型生成目标标记，并引入了 Wasserstein 距离项以将知识从教师转移到学生。通过最小化教师和学生软概率之间的距离，我们的目标是不仅重现金标准标记的预测，还包括对性能和泛化至关重要的接近零标签。
- en: 'ULD loss:'
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ULD 损失：
- en: 'Formally, the ULD loss function is formulated as:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，ULD 损失函数被表述为：
- en: '|  | $\mathcal{L}_{\text{ULD}}=\sum_{t=1}^{&#124;\mathbf{x}&#124;}\text{CE}(t)+\lambda\times\mathcal{W}_{1}\left[\mathbf{p_{\mathbf{\bm{\theta}_{S}}}}\left(\cdot&#124;\mathbf{x}^{S}_{<t}\right),\mathbf{q_{\mathbf{\bm{\theta}_{T}}}}\left(\cdot&#124;\mathbf{x}^{T}_{<t}\right)\right]$
    |  | (4) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{ULD}}=\sum_{t=1}^{&#124;\mathbf{x}&#124;}\text{CE}(t)+\lambda\times\mathcal{W}_{1}\left[\mathbf{p_{\mathbf{\bm{\theta}_{S}}}}\left(\cdot&#124;\mathbf{x}^{S}_{<t}\right),\mathbf{q_{\mathbf{\bm{\theta}_{T}}}}\left(\cdot&#124;\mathbf{x}^{T}_{<t}\right)\right]$
    |  | (4) |'
- en: where $\mathbf{|x|}=min\left(|\mathbf{x}^{S}|,|\mathbf{x}^{T}|\right)$ in the
    rest of this paper.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文其余部分，$\mathbf{|x|}=min\left(|\mathbf{x}^{S}|,|\mathbf{x}^{T}|\right)$。
- en: Explanation.
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解释。
- en: Similar to the KL loss, the discrete Wasserstein distance ensures that the confidence
    of the student at each time step is close to the one from the teacher.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 KL 损失，离散 Wasserstein 距离确保学生在每个时间步的信心接近教师的信心。
- en: 3.3 Fast Computation & Approximations
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 快速计算与近似
- en: 'To the best of our knowledge, we are the first to motivate and propose the
    Wasserstein distance as a learning loss for distillation in the scope of the LLM
    decoder. Prior efforts focus on Sinkhorn regularization Cuturi ([2013](#bib.bib11))
    with encoder-decoder for classification Bhardwaj et al. ([2022](#bib.bib3)), while
    our focus diverges as we concentrate on the generative setting. This shift presents
    inherent challenges, as the naive computation of the Wasserstein loss in [Eq. 3](#S3.E3
    "3 ‣ Wasserstein distance: ‣ 3.1 Background on Optimal Transport ‣ 3 Universal
    Logit Distillation ‣ Towards Cross-Tokenizer Distillation: the Universal Logit
    Distillation Loss for LLMs") exhibits a complexity of $\mathcal{O}(n^{3}\log n)$
    signifies the size of the larger support. While manageable in small classification
    scenarios with encoders, the magnitude of the vocabulary, which can extend to
    100K tokens in generative tasks, renders this approach intractable, particularly
    for long sequences.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '据我们所知，我们是首个在 LLM 解码器范围内激励并提出 Wasserstein 距离作为蒸馏学习损失的人。之前的工作集中于 Sinkhorn 正则化
    Cuturi ([2013](#bib.bib11)) 与用于分类的编码器-解码器 Bhardwaj et al. ([2022](#bib.bib3))，而我们的重点则转向生成设置。这一转变带来了固有的挑战，因为
    [Eq. 3](#S3.E3 "3 ‣ Wasserstein distance: ‣ 3.1 Background on Optimal Transport
    ‣ 3 Universal Logit Distillation ‣ Towards Cross-Tokenizer Distillation: the Universal
    Logit Distillation Loss for LLMs") 中 Wasserstein 损失的天真计算展示了 $\mathcal{O}(n^{3}\log
    n)$ 的复杂度，表示较大支持的大小。虽然在小规模分类场景中与编码器的计算是可控的，但词汇量可以扩展到生成任务中的 100K tokens，这使得这种方法变得不可行，特别是对于长序列。'
- en: 'Closed form solution for ULD loss:'
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ULD 损失的闭式解：
- en: 'To achieve efficient computation of the Wasserstein distance in [Eq. 4](#S3.E4
    "4 ‣ ULD loss: ‣ 3.2 Universal Logit Distillation loss ‣ 3 Universal Logit Distillation
    ‣ Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss
    for LLMs"), we introduce two additional refinements:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '为了实现 [Eq. 4](#S3.E4 "4 ‣ ULD loss: ‣ 3.2 Universal Logit Distillation loss
    ‣ 3 Universal Logit Distillation ‣ Towards Cross-Tokenizer Distillation: the Universal
    Logit Distillation Loss for LLMs") 中 Wasserstein 距离的高效计算，我们引入了两个额外的改进：'
- en: 'Uniform Support Length: We augment either the student or teacher vocabulary
    size through distribution padding (with 0 value), ensuring equal support size
    for both (i.e., $|\Omega_{t}|=|\Omega_{s}|=|\Omega|$).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀支持长度：我们通过分布填充（值为0）来增加学生或教师的词汇大小，确保两者的支持大小相等（即，$|\Omega_{t}|=|\Omega_{s}|=|\Omega|$）。
- en: 'Uniform Cost: As teacher and student supports differ, and no vocabulary relationship
    is established, we assert that each transport cost is equal to $1$. While this
    may seem a strong assumption, we will demonstrate that the approximation we draw
    still achieves better results in our case.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀成本：由于教师和学生的支持不同且没有建立词汇关系，我们断定每个运输成本等于 $1$。虽然这可能看起来是一个强假设，但我们将展示我们得到的近似仍然在我们的案例中取得了更好的结果。
- en: 'Under this assumption the Wasserstein distance used in the $\mathcal{L}_{\text{ULD}}$
    loss becomes Peyré et al. ([2019](#bib.bib37)):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个假设下，$\mathcal{L}_{\text{ULD}}$ 损失中使用的 Wasserstein 距离变成 Peyré et al. ([2019](#bib.bib37))：
- en: '|  | $\mathcal{W}_{1}=\sum_{t=1}^{&#124;\mathbf{x}&#124;}\sum_{i=1}^{&#124;\Omega&#124;}\left&#124;\mathbf{p}(x^{S}_{\sigma^{S}(i)}&#124;\mathbf{x}^{S}_{<t})-\mathbf{q}(x^{T}_{\sigma^{T}(i)}&#124;\mathbf{x}^{T}_{<t})\right&#124;$
    |  | (5) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{W}_{1}=\sum_{t=1}^{&#124;\mathbf{x}&#124;}\sum_{i=1}^{&#124;\Omega&#124;}\left&#124;\mathbf{p}(x^{S}_{\sigma^{S}(i)}&#124;\mathbf{x}^{S}_{<t})-\mathbf{q}(x^{T}_{\sigma^{T}(i)}&#124;\mathbf{x}^{T}_{<t})\right&#124;$
    |  | (5) |'
- en: where $\sigma^{S}$ as the permutation that sorts the decreasing probability
    of student and teacher soft probability vectors.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma^{S}$ 是对学生和教师软概率向量的递减概率进行排序的排列。
- en: Intuition.
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 直觉。
- en: 'The final version of [Eq. 5](#S3.E5 "5 ‣ Closed form solution for ULD loss:
    ‣ 3.3 Fast Computation & Approximations ‣ 3 Universal Logit Distillation ‣ Towards
    Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs")
    is straightforward: with each generation, we pad and sort the probability vectors
    of both student and teacher, then compute the absolute difference. This ensures
    that no matter the token, the confidence levels of the sorted teacher and student
    are close.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '最终版本的[公式 5](#S3.E5 "5 ‣ 闭式解 ULD 损失: ‣ 3.3 快速计算与近似 ‣ 3 通用逻辑蒸馏 ‣ 跨分词器蒸馏: LLMs的通用逻辑蒸馏损失")
    是直接的：对于每次生成，我们填充并排序学生和教师的概率向量，然后计算绝对差值。这确保了无论是哪个token，排序后的教师和学生的置信度都接近。'
- en: 4 Experimental Setting
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验设置
- en: 4.1 Evaluation Scenarios
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 评估场景
- en: 'To align with the black-box approach where training models may not be available,
    we abstain from fine-tuning teacher models. In this way, we enable ULD Loss to
    operate in an unsupervised environment by generating all answers text with teacher
    models. For repeatability and fair comparison between experiments, we opted to
    retain original answers for the test set split. We investigated various scenarios
    to evaluate the ULD loss performance across different datasets and tasks. These
    comprised 2 Extractive QA (Ext.), 2 Generative QA (Gen.), and 1 Summary (Sum.)
    tasks:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与可能无法获得训练模型的黑箱方法对齐，我们避免对教师模型进行微调。通过这种方式，我们使ULD损失能够在无监督环境中运行，生成所有答案文本的教师模型。为了实验的重复性和公平比较，我们选择保留测试集分割的原始答案。我们调查了各种场景，以评估ULD损失在不同数据集和任务中的表现。这些包括2个提取式QA（扩展版）、2个生成式QA（生成式）和1个摘要（摘要）任务：
- en: 'SQuAD (Ext.):'
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'SQuAD (扩展版):'
- en: The Stanford Question Answering Dataset (SQuAD) Rajpurkar et al. ([2016](#bib.bib38))
    is a reading comprehension dataset with 87,600 questions generated by crowdworkers
    from Wikipedia articles. Answers are text portions from the relevant sections
    of the articles.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福问题回答数据集（SQuAD）Rajpurkar 等人 ([2016](#bib.bib38)) 是一个阅读理解数据集，包含87,600个由众包工人从维基百科文章生成的问题。答案是文章相关部分的文本片段。
- en: 'QED (Ext.):'
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'QED (扩展版):'
- en: The QED Lamm et al. ([2020](#bib.bib28)) dataset, expertly annotated, extends
    from a subset of the Google Natural Questions dataset, comprising 7,640 question-answering
    pairs with explanations. Our focus is exclusively on extracted answers (spans).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: QED Lamm 等人 ([2020](#bib.bib28)) 数据集经过专业标注，扩展自Google Natural Questions数据集的一个子集，包含7,640个问答对及其解释。我们专注于提取的答案（跨度）。
- en: 'FairytaleQA (Gen.):'
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'FairytaleQA (生成式):'
- en: The FairytaleQA Dataset Xu et al. ([2022](#bib.bib56)), created by educational
    experts, consists of 10,580 questions from 278 children-friendly stories. Questions
    may be explicit or implicit.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: FairytaleQA数据集 Xu 等人 ([2022](#bib.bib56)) 由教育专家创建，包含来自278个儿童友好故事的10,580个问题。问题可能是明确的或隐含的。
- en: 'PubMedQA (Gen.):'
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'PubMedQA (生成式):'
- en: The PubMedQA Jin et al. ([2019](#bib.bib24)) dataset contains question-answer
    pairs extracted from medical papers. Questions are based on titles, context on
    abstracts, and responses on conclusions. Due to the dataset size and context length
    of our student models, we subset the dataset by selecting the first 50,000 smaller
    items.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: PubMedQA Jin 等人 ([2019](#bib.bib24)) 数据集包含从医学论文中提取的问答对。问题基于标题，背景基于摘要，回答基于结论。由于数据集的大小和我们学生模型的上下文长度，我们通过选择前50,000个较小项目来子集化数据集。
- en: 'DIALOGSum (Sum.):'
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'DIALOGSum (摘要):'
- en: DialogSum Chen et al. ([2021](#bib.bib9)) is a large-scale dialogue summarization
    dataset, consisting of 13,460 spoken dialogues with corresponding summaries and
    topics.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: DialogSum Chen 等人 ([2021](#bib.bib9)) 是一个大规模对话摘要数据集，包含13,460个对话及其对应的摘要和主题。
- en: 4.2 Experimental Choices
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实验选择
- en: 'Baseline:'
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '基线:'
- en: 'As far as we know, the only method currently capable of distilling any pair
    of teacher and student LLM models in a black-box approach is distillation from
    teacher-generated text seen in [Sec. 2.3](#S2.SS3 "2.3 Knowledge Distillation
    Related Work ‣ 2 Problem Formulation & Related Work ‣ Towards Cross-Tokenizer
    Distillation: the Universal Logit Distillation Loss for LLMs"). Throughout the
    remainder of this paper, distillation from teacher-generated text will serve as
    the baseline for evaluating the distillation process using the ULD loss across
    different teacher-student pairs, tasks, and datasets.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '就我们所知，目前唯一能够在黑箱方法下蒸馏任何对教师和学生 LLM 模型的方式是从教师生成的文本中进行蒸馏，如 [Sec. 2.3](#S2.SS3 "2.3
    Knowledge Distillation Related Work ‣ 2 Problem Formulation & Related Work ‣ Towards
    Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs")
    中所述。在本文的其余部分，教师生成文本的蒸馏将作为评估使用 ULD 损失在不同教师-学生对、任务和数据集上蒸馏过程的基准。'
- en: 'Teacher Models:'
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 教师模型：
- en: 'We employed two teacher decoder models, each with 7 billion parameters: LLama
    2 7b Chat (LLama) Touvron et al. ([2023b](#bib.bib48)) and Mistral 7b Instruct
    (Mistral) Jiang et al. ([2023](#bib.bib21)). These instruct models were chosen
    for their ability to generate few-shot answers Brown et al. ([2020](#bib.bib6));
    Wang et al. ([2020b](#bib.bib53)) across diverse tasks and their distinct vocabulary
    set as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Towards Cross-Tokenizer
    Distillation: the Universal Logit Distillation Loss for LLMs").'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用了两个教师解码器模型，每个模型拥有70亿参数：LLama 2 7b Chat (LLama) Touvron et al. ([2023b](#bib.bib48))
    和 Mistral 7b Instruct (Mistral) Jiang et al. ([2023](#bib.bib21))。这些指导模型因其在多种任务中生成少量样本答案的能力而被选择，Brown
    et al. ([2020](#bib.bib6)); Wang et al. ([2020b](#bib.bib53))，以及其独特的词汇集，如图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Towards Cross-Tokenizer Distillation: the Universal
    Logit Distillation Loss for LLMs") 所示。'
- en: 'Student Models:'
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 学生模型：
- en: 'We chose student models from various LLM families and architectures with parameters
    ranging between 160 million to 1 billion: OPT 350m Zhang et al. ([2022](#bib.bib59)),
    Pythia 160m, Pythia 410m, Pythia 1b Biderman et al. ([2023](#bib.bib4)), Bloomz
    560m Muennighoff et al. ([2023](#bib.bib35)) all decoder models and MT0 580m Muennighoff
    et al. ([2023](#bib.bib35)) an encoder-decoder. It’s important to note that models
    can have been already pre-trained on some datasets such as SQuAD for Bloomz and
    MT0.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从不同的LLM家族和架构中选择了学生模型，参数范围从1.6亿到10亿不等：OPT 350m Zhang et al. ([2022](#bib.bib59))，Pythia
    160m，Pythia 410m，Pythia 1b Biderman et al. ([2023](#bib.bib4))，Bloomz 560m Muennighoff
    et al. ([2023](#bib.bib35)) 全部为解码器模型，MT0 580m Muennighoff et al. ([2023](#bib.bib35))
    为编码器-解码器模型。需要注意的是，这些模型可能已经在一些数据集上进行过预训练，例如 Bloomz 和 MT0 在 SQuAD 上。
- en: 'Training process:'
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练过程：
- en: 'ULD loss distillation and teacher-generated text distillation were processed
    uniformly. The two teacher models generate answers in inference mode for the five
    datasets. These answers are then utilized to train student models. During training,
    student models are trained exclusively to predict answers, in teacher forcing
    configuration. Logits used for the ULD loss are calculated by applying teacher
    models to the same data points they generated. Teacher’s weights were frozen,
    ensuring consistency in teacher-generated sentences during inference and training.
    Additional parameter details (learning rate, batch size, etc.) can be found in
    the Appendix [Sec. 11](#S11 "11 Appendix - Training Information ‣ Towards Cross-Tokenizer
    Distillation: the Universal Logit Distillation Loss for LLMs").'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 'ULD 损失蒸馏和教师生成文本蒸馏被统一处理。这两个教师模型在推理模式下生成五个数据集的答案。这些答案随后用于训练学生模型。在训练过程中，学生模型仅用于预测答案，采用教师强制配置。用于
    ULD 损失的 logits 通过将教师模型应用于其生成的相同数据点进行计算。教师的权重被冻结，确保在推理和训练期间教师生成句子的连贯性。附录 [Sec.
    11](#S11 "11 Appendix - Training Information ‣ Towards Cross-Tokenizer Distillation:
    the Universal Logit Distillation Loss for LLMs") 中可以找到额外的参数细节（学习率、批量大小等）。'
- en: 4.3 Teacher Performances
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 教师性能
- en: 'Distilling using synthetic teacher-generated answers might restrict student
    performance on teacher’s ones. To measure distillation efficiency accurately,
    we report the average native performances across tasks for both teachers [Tab. 1](#S4.T1
    "Table 1 ‣ 4.3 Teacher Performances ‣ 4 Experimental Setting ‣ Towards Cross-Tokenizer
    Distillation: the Universal Logit Distillation Loss for LLMs") (details in Appendix
    [Sec. 9](#S9 "9 Appendix - Native Performances ‣ Towards Cross-Tokenizer Distillation:
    the Universal Logit Distillation Loss for LLMs")). We chose a primary metric for
    each task reflecting associate performances: F1 score for Extractive QA Sokolova
    et al. ([2006](#bib.bib44)), BERTScore for Generative QA Zhang* et al. ([2020](#bib.bib60)),
    and Rouge-Lsum for summary task Lin ([2004](#bib.bib32)). Comprehensive evaluation
    methods and outcomes, encompassing prompts and few-shot examples, are provided
    in the Appendix [Sec. 10](#S10 "10 Appendix - Few-Shot examples and Prompt Systems
    ‣ Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss
    for LLMs").'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '使用合成教师生成的答案进行蒸馏可能会限制学生在教师答案上的表现。为了准确测量蒸馏效率，我们报告了教师[表 1](#S4.T1 "Table 1 ‣ 4.3
    Teacher Performances ‣ 4 Experimental Setting ‣ Towards Cross-Tokenizer Distillation:
    the Universal Logit Distillation Loss for LLMs")在各任务中的平均原始性能（详细信息见附录[第 9 节](#S9
    "9 Appendix - Native Performances ‣ Towards Cross-Tokenizer Distillation: the
    Universal Logit Distillation Loss for LLMs")）。我们为每个任务选择了一个主要指标来反映相关性能：F1 分数用于抽取式问答
    Sokolova 等人 ([2006](#bib.bib44))，BERTScore 用于生成式问答 Zhang* 等人 ([2020](#bib.bib60))，Rouge-Lsum
    用于总结任务 Lin ([2004](#bib.bib32))。附录[第 10 节](#S10 "10 Appendix - Few-Shot examples
    and Prompt Systems ‣ Towards Cross-Tokenizer Distillation: the Universal Logit
    Distillation Loss for LLMs")提供了全面的评估方法和结果，包括提示和少量示例。'
- en: '| Model |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |'
- en: '&#124; Extractive &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 抽取式 &#124;'
- en: '&#124; (F1) &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (F1) &#124;'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Generative &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成式 &#124;'
- en: '&#124; (BERTScore) &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (BERTScore) &#124;'
- en: '|'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Summary &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总结 &#124;'
- en: '&#124; (Rouge-Lsum) &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Rouge-Lsum) &#124;'
- en: '|'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| LLama | 69.51 | 36.11 | 23.90 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| LLama | 69.51 | 36.11 | 23.90 |'
- en: '| Mistral | 64.66 | 33.47 | 34.71 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | 64.66 | 33.47 | 34.71 |'
- en: 'Table 1: Average performance of teacher models across tasks with their main
    metrics. It is important to note a relative difference of 30% in performance between
    teacher models on the summary task.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：教师模型在各任务中的平均表现及其主要指标。需要注意的是，教师模型在总结任务中的表现存在30%的相对差异。
- en: 5 Empirical Results
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实证结果
- en: '| Teacher | Model | Method |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 教师 | 模型 | 方法 |'
- en: '&#124; SQUAD &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SQUAD &#124;'
- en: '&#124; (F1) &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (F1) &#124;'
- en: '|'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; QED &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; QED &#124;'
- en: '&#124; (F1) &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (F1) &#124;'
- en: '|'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; FairytaleQA &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FairytaleQA &#124;'
- en: '&#124; (BERTScore) &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (BERTScore) &#124;'
- en: '|'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PubMedQA &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PubMedQA &#124;'
- en: '&#124; (BERTScore) &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (BERTScore) &#124;'
- en: '|'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DIALOGSum &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DIALOGSum &#124;'
- en: '&#124; (Rouge-Lsum) &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Rouge-Lsum) &#124;'
- en: '|'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Teacher | LLama | - | 81.30 | 57.72 | 41.59 | 30.62 | 23.90 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 教师 | LLama | - | 81.30 | 57.72 | 41.59 | 30.62 | 23.90 |'
- en: '| Mistral | - | 76.31 | 53.01 | 36.01 | 30.93 | 34.71 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | - | 76.31 | 53.01 | 36.01 | 30.93 | 34.71 |'
- en: '| LLama | OPT-350m | Raw Text | 70.78 | 48.64 | 33.78 | 27.99 | 20.58 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| LLama | OPT-350m | 原始文本 | 70.78 | 48.64 | 33.78 | 27.99 | 20.58 |'
- en: '| ULD Loss | 72.97 | 49.06 | 33.03 | 30.01 | 20.11 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| ULD Loss | 72.97 | 49.06 | 33.03 | 30.01 | 20.11 |'
- en: '| Pythia-410m | Raw Text | 71.39 | 47.04 | 33.02 | 29.86 | 20.94 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-410m | 原始文本 | 71.39 | 47.04 | 33.02 | 29.86 | 20.94 |'
- en: '| ULD Loss | 74.14 | 49.15 | 34.83 | 29.89 | 22.19 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| ULD Loss | 74.14 | 49.15 | 34.83 | 29.89 | 22.19 |'
- en: '| Bloomz-560m | Raw Text | 73.54 | 50.99 | 36.70 | 29.14 | 20.01 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Bloomz-560m | 原始文本 | 73.54 | 50.99 | 36.70 | 29.14 | 20.01 |'
- en: '| ULD Loss | 75.90 | 55.33 | 37.86 | 30.01 | 22.67 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| ULD Loss | 75.90 | 55.33 | 37.86 | 30.01 | 22.67 |'
- en: '| Mistral | OPT-350m | Raw Text | 71.64 | 50.13 | 30.09 | 27.91 | 31.44 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | OPT-350m | 原始文本 | 71.64 | 50.13 | 30.09 | 27.91 | 31.44 |'
- en: '| ULD Loss | 73.35 | 50.88 | 30.44 | 30.30 | 32.17 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| ULD Loss | 73.35 | 50.88 | 30.44 | 30.30 | 32.17 |'
- en: '| Pythia-410m | Raw Text | 71.50 | 47.07 | 31.44 | 28.25 | 31.64 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-410m | 原始文本 | 71.50 | 47.07 | 31.44 | 28.25 | 31.64 |'
- en: '| ULD Loss | 73.64 | 50.38 | 31.79 | 29.55 | 33.10 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| ULD Loss | 73.64 | 50.38 | 31.79 | 29.55 | 33.10 |'
- en: '| Bloomz-560m | Raw Text | 73.34 | 52.15 | 32.64 | 28.87 | 31.95 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Bloomz-560m | 原始文本 | 73.34 | 52.15 | 32.64 | 28.87 | 31.95 |'
- en: '| ULD Loss | 76.00 | 55.79 | 33.93 | 30.60 | 32.58 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| ULD Loss | 76.00 | 55.79 | 33.93 | 30.60 | 32.58 |'
- en: '| Average | - | Raw Text | 72.03 | 49.34 | 32.94 | 28.67 | 26.09 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | - | 原始文本 | 72.03 | 49.34 | 32.94 | 28.67 | 26.09 |'
- en: '| - | ULD Loss | 74.33 | 51.77 | 33.65 | 30.06 | 27.14 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| - | ULD Loss | 74.33 | 51.77 | 33.65 | 30.06 | 27.14 |'
- en: 'Table 2: Overall performance of Teacher/Student pair models trained with ULD
    Loss and teacher-generated text (Raw Text) across tasks with their main metrics.
    Evaluations are performed over respective test splits.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：使用ULD Loss和教师生成文本（原始文本）训练的教师/学生对模型在各任务中的总体表现及其主要指标。评估是在各自的测试分割上进行的。
- en: 5.1 General Results
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 一般结果
- en: 'We empirically validate the effectiveness of the ULD loss step-by-step. First,
    we report in [Tab. 2](#S5.T2 "Table 2 ‣ 5 Empirical Results ‣ Towards Cross-Tokenizer
    Distillation: the Universal Logit Distillation Loss for LLMs") the aggregated
    key metrics performance over the different datasets and teacher/student pairs.
    ULD loss achieves the best overall results, which indicates that the proposed
    ULD loss effectively improves the performances of every student model on a variety
    of downstream tasks using any Teacher. Notably, ULD loss exhibits an average improvement
    of $2.30$ points over models trained on teacher-generated text for extractive
    QA tasks and Bloomz outperforms his teacher Mistral on the QED datasets. Furthermore,
    concerning summarization tasks, the 30% performance disparity between LLama/Mistral
    ([Tab. 1](#S4.T1 "Table 1 ‣ 4.3 Teacher Performances ‣ 4 Experimental Setting
    ‣ Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss
    for LLMs")) persists in their distilled counterparts ([Tab. 2](#S5.T2 "Table 2
    ‣ 5 Empirical Results ‣ Towards Cross-Tokenizer Distillation: the Universal Logit
    Distillation Loss for LLMs")), underscoring the critical role of teacher performances.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '我们逐步验证了 ULD 损失的有效性。首先，我们在 [Tab. 2](#S5.T2 "Table 2 ‣ 5 Empirical Results ‣
    Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for
    LLMs") 中报告了不同数据集和教师/学生对的汇总关键指标性能。ULD 损失取得了最佳的总体结果，这表明所提出的 ULD 损失有效地提高了每个学生模型在各种下游任务中的表现，使用任何教师都有效。特别是，ULD
    损失在处理提取式 QA 任务时，平均提升了 $2.30$ 分，而 Bloomz 在 QED 数据集上优于他的教师 Mistral。此外，在总结任务方面，LLama/Mistral
    之间的 30% 性能差距 ([Tab. 1](#S4.T1 "Table 1 ‣ 4.3 Teacher Performances ‣ 4 Experimental
    Setting ‣ Towards Cross-Tokenizer Distillation: the Universal Logit Distillation
    Loss for LLMs")) 在其蒸馏对手中 ([Tab. 2](#S5.T2 "Table 2 ‣ 5 Empirical Results ‣ Towards
    Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs"))
    依然存在，突显了教师表现的重要性。'
- en: 5.2 Student Size Ablation Study
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 学生规模消融研究
- en: 'General results in [Tab. 2](#S5.T2 "Table 2 ‣ 5 Empirical Results ‣ Towards
    Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs")
    show a consistent pattern regarding the model size and the gain achieved with
    the ULD loss, especially for challenging tasks such as generative QA. To understand
    the impact of student size on distillation capability, we performed an ablation
    study over the Pythia family. We hold the training dataset size fixed at 100%
    and compare the performance of models from 160m, 410m to 1b parameters and report
    results in [Fig. 3](#S5.F3 "Figure 3 ‣ 5.2 Student Size Ablation Study ‣ 5 Empirical
    Results ‣ Towards Cross-Tokenizer Distillation: the Universal Logit Distillation
    Loss for LLMs").'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tab. 2](#S5.T2 "Table 2 ‣ 5 Empirical Results ‣ Towards Cross-Tokenizer Distillation:
    the Universal Logit Distillation Loss for LLMs") 中的一般结果显示了模型规模与 ULD 损失所获得的增益之间的一致模式，尤其是对于生成式
    QA 等具有挑战性的任务。为了了解学生规模对蒸馏能力的影响，我们对 Pythia 家族进行了消融研究。我们保持训练数据集规模固定为 100%，并比较了从 160m、410m
    到 1b 参数的模型性能，并在 [Fig. 3](#S5.F3 "Figure 3 ‣ 5.2 Student Size Ablation Study ‣
    5 Empirical Results ‣ Towards Cross-Tokenizer Distillation: the Universal Logit
    Distillation Loss for LLMs") 中报告结果。'
- en: '![Refer to caption](img/399f060e62a2aee7e54e20d86c2756af.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/399f060e62a2aee7e54e20d86c2756af.png)'
- en: 'Figure 3: Student model size ablation with the Pythia family trained by a LLama
    teacher. Trainings are conducted with ULD loss and teacher-generated text (raw
    text). Evaluation scores on test sets are depicted on the Y-axis, while Pythia
    model sizes are on the X-axis.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用 LLama 教师训练的 Pythia 家族的学生模型规模消融。训练使用 ULD 损失和教师生成的文本（原始文本）。评估分数在 Y 轴上表示，而
    Pythia 模型规模在 X 轴上表示。
- en: We observe that incorporating ULD loss consistently enhances student models
    across various tasks. The enhancements are particularly noticeable for smaller
    models on simpler tasks, while ULD loss requires larger models for effectively
    distilling teacher logits on harder tasks. This is especially evident in tasks
    requiring reasoning, such as FairytaleQA. While using logits teacher improves
    training, deep reasoning tasks still require appropriate model sizes to process
    complex relationships taught by teachers. Generally, we observe a significant
    increase in capacity transfer from teacher to student models through the use of
    ULD, enabling student models to match models twice bigger trained with the teacher-generated
    text method. For example, Pythia 410m with ULD loss matches the performance of
    the Pythia 1b distilled with teacher-generated text on QED and DIALOGSum.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，融入ULD损失 consistently 可以提升学生模型在各种任务中的表现。这个提升在较小模型处理简单任务时特别明显，而ULD损失在处理较难任务时则需要较大的模型来有效提取教师的logits。这在需要推理的任务中尤为显著，比如FairytaleQA。虽然使用logits教师可以改善训练，但深度推理任务仍需要适当的模型规模来处理教师教授的复杂关系。一般来说，我们观察到通过使用ULD，教师到学生模型的容量转移显著增加，使学生模型能够匹配训练时使用教师生成文本方法的双倍大模型。例如，Pythia
    410m与ULD损失匹配了Pythia 1b在QED和DIALOGSum上的表现。
- en: 5.3 Dataset Size Ablation Study
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 数据集大小消融研究
- en: '![Refer to caption](img/af113df4644e49b03d602525f1bfb3fa.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/af113df4644e49b03d602525f1bfb3fa.png)'
- en: 'Figure 4: Dataset size ablation with a LLama/Pythia-410m pair of models trained
    with ULD loss or teacher-generated text (raw text). The X-axis indicates the %
    of data used during training while the y-axis represents the test set score for
    respective datasets.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用ULD损失或教师生成文本（原始文本）训练的LLama/Pythia-410m模型对的数据集大小消融。X轴表示训练期间使用的数据百分比，Y轴表示各数据集的测试集分数。
- en: 'In this section, we investigate and report in [Fig. 3](#S5.F3 "Figure 3 ‣ 5.2
    Student Size Ablation Study ‣ 5 Empirical Results ‣ Towards Cross-Tokenizer Distillation:
    the Universal Logit Distillation Loss for LLMs") the influence of the dataset
    size for models trained with ULD Loss or teacher-generated text. We perform ablations
    with respectively 25%, 50%, 75%, and 100% of dataset size while keeping training
    parameters constants. For every ablation ratio, models trained with ULD loss achieved
    better performance than models trained on teacher-generated text. Specifically,
    with 50% of a dataset, ULD loss models overpass the performance of teacher-generated
    text models trained with full dataset.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们调查并在[图3](#S5.F3 "Figure 3 ‣ 5.2 Student Size Ablation Study ‣ 5 Empirical
    Results ‣ Towards Cross-Tokenizer Distillation: the Universal Logit Distillation
    Loss for LLMs")中报告了数据集大小对使用ULD损失或教师生成文本训练模型的影响。我们分别用25%、50%、75%和100%的数据集大小进行消融实验，同时保持训练参数不变。在每个消融比率下，使用ULD损失训练的模型表现优于使用教师生成文本训练的模型。具体而言，使用50%数据集的情况下，ULD损失模型超越了使用全数据集训练的教师生成文本模型的表现。'
- en: 5.4 Training Regularization
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 训练正则化
- en: 'To understand the impact of the ULD loss during training we decide to compute
    the validation ULD and Cross-entropy loss values for two pairs of teacher/student
    on the SQuAD dataset every 200 steps during 5 epochs. We report the curves formed
    by this point in [Fig. 5](#S5.F5 "Figure 5 ‣ 5.4 Training Regularization ‣ 5 Empirical
    Results ‣ Towards Cross-Tokenizer Distillation: the Universal Logit Distillation
    Loss for LLMs").'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '为了理解ULD损失在训练中的影响，我们决定计算SQuAD数据集中每200步的两个教师/学生模型对的验证ULD和交叉熵损失值，持续5个周期。我们在[图5](#S5.F5
    "Figure 5 ‣ 5.4 Training Regularization ‣ 5 Empirical Results ‣ Towards Cross-Tokenizer
    Distillation: the Universal Logit Distillation Loss for LLMs")中报告了由此形成的曲线。'
- en: '![Refer to caption](img/b45fc815ccd520479b26378016e4ba6c.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b45fc815ccd520479b26378016e4ba6c.png)'
- en: 'Figure 5: Evolution of validation ULD and Cross-entropy loss curves during
    training on SQuAD dataset for a LLama/Pythia-410m and LLama/Bloomz-560m Teacher/Student
    pair of model. For teacher-generated text models (raw text), the ULD loss was
    only computed during validation and did not impact the training process.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在SQuAD数据集上训练时，LLama/Pythia-410m和LLama/Bloomz-560m教师/学生模型对的验证ULD和交叉熵损失曲线的演变。对于教师生成文本模型（原始文本），ULD损失仅在验证期间计算，并未影响训练过程。
- en: It appears that using the ULD loss contributes to stabilizing the distillation
    process over training and mitigates overfitting issues, enabling the model to
    train more effectively across multiple epochs. It’s worth noting that incorporating
    the ULD loss during training stabilizes both ULD and Cross-entropy loss.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来使用ULD损失有助于在训练过程中稳定蒸馏过程，并减轻过拟合问题，使模型在多个训练轮次中更有效地训练。值得注意的是，在训练过程中引入ULD损失可以稳定ULD和交叉熵损失。
- en: Remark.
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注。
- en: Higher Cross-entropy loss validation values observed with ULD loss are a direct
    result of the training process. ULD loss trained model to predict soft labels
    from the teacher model instead of the one-hot vectors used for computing the Cross-entropy
    loss.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到的ULD损失更高的交叉熵损失验证值是训练过程的直接结果。ULD损失训练模型以预测来自教师模型的软标签，而不是用于计算交叉熵损失的one-hot向量。
- en: 6 Distillation of Decoder Teacher to Encoder-Decoder Student
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 解码器教师到编码器-解码器学生的蒸馏
- en: 'As shown in [Sec. 5](#S5 "5 Empirical Results ‣ Towards Cross-Tokenizer Distillation:
    the Universal Logit Distillation Loss for LLMs"), ULD loss effectively transfers
    knowledge from any pair of teacher/student decoders. Moreover, by leveraging solely
    on logit information and adopting a black-box approach, ULD loss should be able
    to extend its versatility and improve cross-architecture distillation. To validate
    this, we distill a teacher/student pair LLama/MT0-580m and focus our experimentation
    on PubMedQA, DIALOGSum, and QED to avoid any data seen during the pre-training
    of MT0 with the xP3 dataset Muennighoff et al. ([2023](#bib.bib35)); Sanh et al.
    ([2022](#bib.bib42)).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[第5节](#S5 "5 实证结果 ‣ 跨Tokenizer蒸馏：LLMs的通用Logit蒸馏损失")所示，ULD损失有效地将知识从任何一对教师/学生解码器之间转移。此外，通过仅利用logit信息并采用黑箱方法，ULD损失应该能够扩展其通用性并改善跨架构蒸馏。为了验证这一点，我们蒸馏了教师/学生对LLama/MT0-580m，并将实验集中在PubMedQA、DIALOGSum和QED上，以避免在使用xP3数据集Muennighoff等（[2023](#bib.bib35)）；Sanh等（[2022](#bib.bib42)）对MT0的预训练过程中见过的数据。
- en: '| ULD loss |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| ULD损失 |'
- en: '&#124; QED &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; QED &#124;'
- en: '&#124; (F1) &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (F1) &#124;'
- en: '|'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PubMedQA &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PubMedQA &#124;'
- en: '&#124; (BERTScore) &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (BERTScore) &#124;'
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DIALOGSum &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DIALOGSum &#124;'
- en: '&#124; (Rouge-Lsum) &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Rouge-Lsum) &#124;'
- en: '|'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Raw Labels | 55.63 | 27.56 | 23.22 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 原始标签 | 55.63 | 27.56 | 23.22 |'
- en: '| ULD Loss | 56.01 | 30.19 | 23.92 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| ULD损失 | 56.01 | 30.19 | 23.92 |'
- en: 'Table 3: Distillation of a LLama teacher (decoder) to an MT0-580m (encoder-decoder)
    with ULD Loss and teacher-generated text on three data sets and their primary
    metric associate.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：使用ULD损失和教师生成文本在三个数据集上将LLama教师（解码器）蒸馏到MT0-580m（编码器-解码器）及其主要指标相关。
- en: 'The results presented in [Tab. 3](#S6.T3 "Table 3 ‣ 6 Distillation of Decoder
    Teacher to Encoder-Decoder Student ‣ Towards Cross-Tokenizer Distillation: the
    Universal Logit Distillation Loss for LLMs") demonstrate that incorporating logit
    information from a decoder teacher using ULD loss can enhance the performance
    of an encoder-decoder student model. Notably, the inherent ability of the encoder-decoder
    in the summary task seems to be limited by the synthetic answers as teacher-generated
    text distillation matches the teacher’s performance. However, by using the logit
    information with the ULD loss, the student model still leads to improved results,
    suggesting a successful knowledge transfer through logits. With this additional
    knowledge, the student model slightly outperforms the teacher one. Furthermore,
    in generative tasks where decoder architectures perform, the encoder-decoder student
    model gained 2.63 points over distillation with teacher-generated text.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[表3](#S6.T3 "表3 ‣ 6 解码器教师到编码器-解码器学生的蒸馏 ‣ 跨Tokenizer蒸馏：LLMs的通用Logit蒸馏损失")中展示的结果表明，使用ULD损失从解码器教师那里引入logit信息可以提升编码器-解码器学生模型的性能。值得注意的是，在总结任务中，编码器-解码器的固有能力似乎受到合成答案的限制，因为教师生成文本蒸馏与教师的表现相匹配。然而，通过使用logit信息和ULD损失，学生模型仍然实现了改进的结果，表明通过logits成功地转移了知识。凭借这些额外的知识，学生模型略微优于教师模型。此外，在解码器架构表现的生成任务中，编码器-解码器学生模型在教师生成文本的蒸馏中提升了2.63分。'
- en: 7 Conclusions
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this work, we introduce the Universal Logit Distillation (ULD) loss, a novel
    method for distilling any decoder teacher model into any student model for LLM
    generative tasks. ULD achieves better overall results and matches the performance
    of teacher-generated text distillation with only half of the training dataset
    or student size, while effectively preventing overfitting. Our extensive experiments
    validate the efficacy of the ULD loss across diverse tasks,- datasets, and architectures,
    demonstrating its superiority over standard teacher-generated text distillation
    methods.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了通用对数蒸馏（ULD）损失，这是一种将任何解码器教师模型蒸馏到任何学生模型中的新方法，用于LLM生成任务。ULD在总体结果上表现更好，并且在仅用一半的训练数据集或学生模型大小的情况下，与教师生成文本蒸馏的性能相匹配，同时有效地防止过拟合。我们广泛的实验验证了ULD损失在各种任务、数据集和架构中的有效性，展示了其优于标准教师生成文本蒸馏方法的优势。
- en: Limitations
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: One unexplored avenue related to our work is the study of the ULD loss in non-English
    languages, including ones with vastly different token representations (Chinese,
    Korean, etc). However, we believe that our findings can still be applied to other
    languages, especially if teacher-student model pairs are pre-trained in the same
    language or achieve multilingual capacity. Additionally, in line with previous
    work, we evaluate task performance with standard reference-based metrics (ROUGE,
    F1, BERTScore) which can be limited in their assessment of what constitutes a
    correct generative model prediction Faysse et al. ([2023](#bib.bib15)). An extension
    of the work could be to go beyond mono-task distillation, and assess the transferability
    of generalist assistant abilities from larger to smaller models.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的工作相关的一个未探索的领域是对非英语语言的ULD损失研究，包括具有截然不同的标记表示的语言（中文、韩文等）。然而，我们相信我们的发现仍然可以应用于其他语言，特别是如果教师-学生模型对在相同语言中进行预训练或实现多语言能力。此外，按照以前的工作，我们使用标准基于参考的指标（ROUGE,
    F1, BERTScore）来评估任务性能，这些指标在评估什么构成正确的生成模型预测时可能有局限性 Faysse等人（[2023](#bib.bib15)）。工作的一种扩展可以是超越单任务蒸馏，评估从较大模型到较小模型的通用助手能力的可迁移性。
- en: Ethics Statement
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: Knowledge distillation aims to reduce the size, cost, and energy consumed by
    a model at inference time. Our work opens new perspectives in this area, aligned
    with the desire for sobriety, notably for environmental reasons. Although KD allows
    partial transfer of larger model performance, smaller models remain limited in
    their reasoning capacity and are more susceptible to hallucinatory behavior Rawte
    et al. ([2023](#bib.bib40)), especially in open-ended generation tasks. This phenomenon
    has not been extensively studied in this work. Furthermore, by distilling knowledge
    from existing models, if a bias is already present in the teacher model, it may
    be transferred to the student model. This is not unique to our method, but it’s
    a common risk for all knowledge distillation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏旨在减少模型在推理时的大小、成本和能源消耗。我们的工作在这一领域开辟了新的视角，符合对环境原因的节俭愿望。虽然KD允许部分转移较大模型的性能，但较小的模型在推理能力上仍然有限，并且更容易出现幻觉行为
    Rawte等人（[2023](#bib.bib40)），尤其是在开放式生成任务中。这一现象在本工作中并未得到广泛研究。此外，通过从现有模型中蒸馏知识，如果教师模型中已经存在偏差，可能会转移到学生模型中。这不是我们方法的特有问题，而是所有知识蒸馏方法都面临的常见风险。
- en: Acknowledgements
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported by Diabolocom, and Jean Zay supercomputer operated by
    GENCI IDRIS through compute grant 2023-AD011014778, 2023-AD011014668R1, AD010614770
    as well as on Adastra through project c1615122, cad15031, cad14770.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到Diabolocom和由GENCI IDRIS运营的Jean Zay超级计算机的支持，通过计算补助2023-AD011014778, 2023-AD011014668R1,
    AD010614770，以及在Adastra项目c1615122, cad15031, cad14770上的支持。
- en: References
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,
    Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel
    Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune,
    Baptiste Pannier, and Guilherme Penedo. 2023. [The falcon series of open language
    models](http://arxiv.org/abs/2311.16867). arXiv:2311.16867\. arXiv. ArXiv:2311.16867
    [cs].
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Almazrouei等人（2023） Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,
    Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel
    Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune,
    Baptiste Pannier, 和 Guilherme Penedo. 2023. [猎鹰系列开放语言模型](http://arxiv.org/abs/2311.16867).
    arXiv:2311.16867\. arXiv. ArXiv:2311.16867 [cs].
- en: Bahuleyan et al. (2018) Hareesh Bahuleyan, Lili Mou, Hao Zhou, and Olga Vechtomova.
    2018. Stochastic wasserstein autoencoder for probabilistic sentence generation.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahuleyan et al. (2018) Hareesh Bahuleyan, Lili Mou, Hao Zhou, 和 Olga Vechtomova.
    2018. 随机 Wasserstein 自编码器用于概率句子生成。
- en: 'Bhardwaj et al. (2022) Rishabh Bhardwaj, Tushar Vaidya, and Soujanya Poria.
    2022. [Knot: Knowledge distillation using optimal transport for solving nlp tasks](http://arxiv.org/abs/2110.02432).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bhardwaj et al. (2022) Rishabh Bhardwaj, Tushar Vaidya, 和 Soujanya Poria. 2022.
    [Knot: 使用最优传输的知识蒸馏用于解决 NLP 任务](http://arxiv.org/abs/2110.02432)。'
- en: 'Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
    Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
    USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der
    Wal. 2023. [Pythia: A suite for analyzing large language models across training
    and scaling](https://doi.org/10.48550/arXiv.2304.01373). arXiv:2304.01373arXiv:2304.01373\.
    arXiv. ArXiv:2304.01373 [cs].'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
    Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
    USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, 和 Oskar van
    der Wal. 2023. [Pythia: 一个用于分析大型语言模型的工具套件，涵盖训练和扩展](https://doi.org/10.48550/arXiv.2304.01373)。arXiv:2304.01373arXiv:2304.01373。arXiv。ArXiv:2304.01373
    [cs]。'
- en: 'Black et al. (2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony,
    Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang,
    Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan
    Tow, Ben Wang, and Samuel Weinbach. 2022. [Gpt-neox-20b: An open-source autoregressive
    language model](http://arxiv.org/abs/2204.06745).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Black et al. (2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony,
    Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang,
    Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan
    Tow, Ben Wang, 和 Samuel Weinbach. 2022. [Gpt-neox-20b: 一个开源自回归语言模型](http://arxiv.org/abs/2204.06745)。'
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. volume 33, pages 1877–1901.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等. 2020. 语言模型是少样本学习者。卷 33，页码 1877–1901。
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. [Sparks of
    artificial general intelligence: Early experiments with gpt-4](http://arxiv.org/abs/2303.12712).'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, 和 Yi Zhang. 2023. [人工通用智能的火花：GPT-4的早期实验](http://arxiv.org/abs/2303.12712)。
- en: Buciluundefined et al. (2006) Cristian Buciluundefined, Rich Caruana, and Alexandru
    Niculescu-Mizil. 2006. [Model compression](https://doi.org/10.1145/1150402.1150464).
    In *Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining*, KDD ’06, page 535–541, New York, NY, USA. Association for Computing
    Machinery.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buciluundefined et al. (2006) Cristian Buciluundefined, Rich Caruana, 和 Alexandru
    Niculescu-Mizil. 2006. [模型压缩](https://doi.org/10.1145/1150402.1150464)。在 *第12届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集* 中，KDD ’06，页码 535–541，纽约，纽约州，美国。计算机协会。
- en: 'Chen et al. (2021) Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. 2021.
    [Dialogsum: A real-life scenario dialogue summarization dataset](http://arxiv.org/abs/2105.06762).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2021) Yulong Chen, Yang Liu, Liang Chen, 和 Yue Zhang. 2021. [Dialogsum:
    真实场景对话总结数据集](http://arxiv.org/abs/2105.06762)。'
- en: Colombo et al. (2021) Pierre Colombo, Guillaume Staerman, Chloé Clavel, and
    Pablo Piantanida. 2021. [Automatic text evaluation through the lens of Wasserstein
    barycenters](https://doi.org/10.18653/v1/2021.emnlp-main.817). In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    10450–10466, Online and Punta Cana, Dominican Republic. Association for Computational
    Linguistics.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Colombo et al. (2021) Pierre Colombo, Guillaume Staerman, Chloé Clavel, 和 Pablo
    Piantanida. 2021. [通过 Wasserstein 重心的视角自动文本评估](https://doi.org/10.18653/v1/2021.emnlp-main.817)。在
    *2021年自然语言处理实证方法会议论文集* 中，页码 10450–10466，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Cuturi (2013) Marco Cuturi. 2013. Sinkhorn distances: Lightspeed computation
    of optimal transport. volume 26.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cuturi (2013) Marco Cuturi. 2013. Sinkhorn 距离：光速计算最优传输。卷 26。
- en: 'Dasgupta et al. (2023) Sayantan Dasgupta, Trevor Cohn, and Timothy Baldwin.
    2023. Cost-effective distillation of large language models. In *Findings of the
    Association for Computational Linguistics: ACL 2023*, pages 7346–7354.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasgupta et al. (2023) Sayantan Dasgupta, Trevor Cohn, 和 Timothy Baldwin. 2023.
    大型语言模型的成本效益蒸馏。发表于 *计算语言学协会的发现：ACL 2023*，第7346–7354页。
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. [Qlora: Efficient finetuning of quantized llms](http://arxiv.org/abs/2305.14314).'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke
    Zettlemoyer. 2023. [Qlora: 高效的量化大语言模型微调](http://arxiv.org/abs/2305.14314)。'
- en: Du et al. (2023) Zilin Du, Haoxin Li, Xu Guo, and Boyang Li. 2023. [Training
    on synthetic data beats real data in multimodal relation extraction](http://arxiv.org/abs/2312.03025).
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. (2023) Zilin Du, Haoxin Li, Xu Guo, 和 Boyang Li. 2023. [合成数据上的训练胜过真实数据在多模态关系抽取中的应用](http://arxiv.org/abs/2312.03025)。
- en: Faysse et al. (2023) Manuel Faysse, Gautier Viaud, Céline Hudelot, and Pierre
    Colombo. 2023. [Revisiting instruction fine-tuned model evaluation to guide industrial
    applications](https://doi.org/10.18653/v1/2023.emnlp-main.559). In *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*. Association
    for Computational Linguistics.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faysse et al. (2023) Manuel Faysse, Gautier Viaud, Céline Hudelot, 和 Pierre
    Colombo. 2023. [重新审视指令微调模型评估以指导工业应用](https://doi.org/10.18653/v1/2023.emnlp-main.559).
    发表在 *2023年自然语言处理实证方法会议论文集*。计算语言学协会。
- en: Guerreiro et al. (2022) Nuno M Guerreiro, Pierre Colombo, Pablo Piantanida,
    and André FT Martins. 2022. Optimal transport for unsupervised hallucination detection
    in neural machine translation.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guerreiro et al. (2022) Nuno M Guerreiro, Pierre Colombo, Pablo Piantanida,
    和 André FT Martins. 2022. 用于无监督幻觉检测的最优传输在神经机器翻译中的应用。
- en: 'He et al. (2023) Nan He, Hanyu Lai, Chenyang Zhao, Zirui Cheng, Junting Pan,
    Ruoyu Qin, Ruofan Lu, Rui Lu, Yunchen Zhang, Gangming Zhao, Zhaohui Hou, Zhiyuan
    Huang, Shaoqing Lu, Ding Liang, and Mingjie Zhan. 2023. [Teacherlm: Teaching to
    fish rather than giving the fish, language modeling likewise](https://doi.org/10.48550/arXiv.2310.19019).
    arXiv:2310.19019\. arXiv. ArXiv:2310.19019 [cs].'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. (2023) Nan He, Hanyu Lai, Chenyang Zhao, Zirui Cheng, Junting Pan,
    Ruoyu Qin, Ruofan Lu, Rui Lu, Yunchen Zhang, Gangming Zhao, Zhaohui Hou, Zhiyuan
    Huang, Shaoqing Lu, Ding Liang, 和 Mingjie Zhan. 2023. [Teacherlm: 教授钓鱼而不是给鱼，语言建模亦然](https://doi.org/10.48550/arXiv.2310.19019).
    arXiv:2310.19019\. arXiv. ArXiv:2310.19019 [cs]。'
- en: 'He et al. (2022) Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari,
    and Mohammad Norouzi. 2022. [Generate, Annotate, and Learn: NLP with Synthetic
    Text](https://doi.org/10.1162/tacl_a_00492). volume 10, pages 826–842.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2022) Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari, 和
    Mohammad Norouzi. 2022. [生成、注释与学习：使用合成文本的自然语言处理](https://doi.org/10.1162/tacl_a_00492).
    第10卷，第826–842页。
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. [Distilling
    the knowledge in a neural network](https://doi.org/10.48550/arXiv.1503.02531).
    arXiv:1503.02531arXiv:1503.02531\. arXiv. ArXiv:1503.02531 [cs, stat].
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, 和 Jeff Dean. 2015. [蒸馏神经网络中的知识](https://doi.org/10.48550/arXiv.1503.02531).
    arXiv:1503.02531arXiv:1503.02531\. arXiv. ArXiv:1503.02531 [cs, stat]。
- en: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    2023. [Distilling step-by-step! outperforming larger language models with less
    training data and smaller model sizes](http://arxiv.org/abs/2305.02301).
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, 和 Tomas Pfister.
    2023. [逐步蒸馏！在较少训练数据和更小模型规模下超越更大语言模型](http://arxiv.org/abs/2305.02301)。
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. [Mistral 7b](https://doi.org/10.48550/arXiv.2310.06825).
    arXiv:2310.06825\. arXiv. ArXiv:2310.06825 [cs].
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, 和
    William El Sayed. 2023. [Mistral 7b](https://doi.org/10.48550/arXiv.2310.06825).
    arXiv:2310.06825\. arXiv. ArXiv:2310.06825 [cs]。
- en: Jiang et al. (2024) Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
    Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep
    Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut
    Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. [Mixtral of
    experts](http://arxiv.org/abs/2401.04088).
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2024）Albert Q. Jiang、Alexandre Sablayrolles、Antoine Roux、Arthur Mensch、Blanche
    Savary、Chris Bamford、Devendra Singh Chaplot、Diego de las Casas、Emma Bou Hanna、Florian
    Bressand、Gianna Lengyel、Guillaume Bour、Guillaume Lample、Lélio Renard Lavaud、Lucile
    Saulnier、Marie-Anne Lachaux、Pierre Stock、Sandeep Subramanian、Sophia Yang、Szymon
    Antoniak、Teven Le Scao、Théophile Gervet、Thibaut Lavril、Thomas Wang、Timothée Lacroix
    和 William El Sayed。2024年。 [Mixtral of experts](http://arxiv.org/abs/2401.04088)。
- en: 'Jiao et al. (2020) Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
    Linlin Li, Fang Wang, and Qun Liu. 2020. [Tinybert: Distilling bert for natural
    language understanding](http://arxiv.org/abs/1909.10351).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiao 等（2020）Xiaoqi Jiao、Yichun Yin、Lifeng Shang、Xin Jiang、Xiao Chen、Linlin
    Li、Fang Wang 和 Qun Liu。2020年。 [Tinybert: 为自然语言理解蒸馏BERT](http://arxiv.org/abs/1909.10351)。'
- en: 'Jin et al. (2019) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen,
    and Xinghua Lu. 2019. [Pubmedqa: A dataset for biomedical research question answering](http://arxiv.org/abs/1909.06146).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin 等（2019）Qiao Jin、Bhuwan Dhingra、Zhengping Liu、William W. Cohen 和 Xinghua
    Lu。2019年。 [Pubmedqa: 一个用于生物医学研究问答的数据集](http://arxiv.org/abs/1909.06146)。'
- en: Kalamkar et al. (2019) Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi,
    Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj
    Jammalamadaka, Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander
    Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy,
    Bharat Kaul, and Pradeep Dubey. 2019. [A study of bfloat16 for deep learning training](http://arxiv.org/abs/1905.12322).
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalamkar 等（2019）Dhiraj Kalamkar、Dheevatsa Mudigere、Naveen Mellempudi、Dipankar
    Das、Kunal Banerjee、Sasikanth Avancha、Dharma Teja Vooturi、Nataraj Jammalamadaka、Jianyu
    Huang、Hector Yuen、Jiyan Yang、Jongsoo Park、Alexander Heinecke、Evangelos Georganas、Sudarshan
    Srinivasan、Abhisek Kundu、Misha Smelyanskiy、Bharat Kaul 和 Pradeep Dubey。2019年。
    [一种用于深度学习训练的bfloat16研究](http://arxiv.org/abs/1905.12322)。
- en: Kim and Rush (2016) Yoon Kim and Alexander M. Rush. 2016. [Sequence-level knowledge
    distillation](https://doi.org/10.18653/v1/D16-1139). In *Proceedings of the 2016
    Conference on Empirical Methods in Natural Language Processing*, pages 1317–1327,
    Austin, Texas. Association for Computational Linguistics.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 和 Rush（2016）Yoon Kim 和 Alexander M. Rush。2016年。 [序列级知识蒸馏](https://doi.org/10.18653/v1/D16-1139)。在
    *2016年自然语言处理经验方法会议论文集*，第1317–1327页，德克萨斯州奥斯汀。计算语言学协会。
- en: Kramchaninova and Defauw (2022) Alina Kramchaninova and Arne Defauw. 2022. [Synthetic
    data generation for multilingual domain-adaptable question answering systems](https://aclanthology.org/2022.eamt-1.18).
    In *Proceedings of the 23rd Annual Conference of the European Association for
    Machine Translation*, pages 151–160, Ghent, Belgium. European Association for
    Machine Translation.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kramchaninova 和 Defauw（2022）Alina Kramchaninova 和 Arne Defauw。2022年。 [用于多语言领域适应问答系统的合成数据生成](https://aclanthology.org/2022.eamt-1.18)。在
    *第23届欧洲机器翻译协会年会论文集*，第151–160页，比利时根特。欧洲机器翻译协会。
- en: 'Lamm et al. (2020) Matthew Lamm, Jennimaria Palomaki, Chris Alberti, Daniel
    Andor, Eunsol Choi, Livio Baldini Soares, and Michael Collins. 2020. [Qed: A framework
    and dataset for explanations in question answering](http://arxiv.org/abs/2009.06354).'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lamm 等（2020）Matthew Lamm、Jennimaria Palomaki、Chris Alberti、Daniel Andor、Eunsol
    Choi、Livio Baldini Soares 和 Michael Collins。2020年。 [Qed: 一个用于问答解释的框架和数据集](http://arxiv.org/abs/2009.06354)。'
- en: Lester et al. (2022) Brian Lester, Joshua Yurtsever, Siamak Shakeri, and Noah
    Constant. 2022. [Reducing retraining by recycling parameter-efficient prompts](http://arxiv.org/abs/2208.05577).
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester 等（2022）Brian Lester、Joshua Yurtsever、Siamak Shakeri 和 Noah Constant。2022年。
    [通过回收参数高效的提示减少再训练](http://arxiv.org/abs/2208.05577)。
- en: Leviathan et al. (2023) Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023.
    Fast inference from transformers via speculative decoding. In *International Conference
    on Machine Learning*, pages 19274–19286\. PMLR.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leviathan 等（2023）Yaniv Leviathan、Matan Kalman 和 Yossi Matias。2023年。 从变换器中进行快速推理，利用推测解码。
    在 *国际机器学习会议*，第19274–19286页。PMLR。
- en: Li et al. (2021) Tianda Li, Yassir El Mesbahi, Ivan Kobyzev, Ahmad Rashid, Atif
    Mahmud, Nithin Anchuri, Habib Hajimolahoseini, Yang Liu, and Mehdi Rezagholizadeh.
    2021. [dis](http://arxiv.org/abs/2110.08460).
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2021）Tianda Li、Yassir El Mesbahi、Ivan Kobyzev、Ahmad Rashid、Atif Mahmud、Nithin
    Anchuri、Habib Hajimolahoseini、Yang Liu 和 Mehdi Rezagholizadeh。2021年。 [dis](http://arxiv.org/abs/2110.08460)。
- en: 'Lin (2004) Chin-Yew Lin. 2004. [ROUGE: A package for automatic evaluation of
    summaries](https://aclanthology.org/W04-1013). In *Text Summarization Branches
    Out*, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin (2004) Chin-Yew Lin. 2004. [ROUGE: 自动评估摘要的工具包](https://aclanthology.org/W04-1013)。收录于
    *文本摘要的扩展*，第74–81页，西班牙巴塞罗那。计算语言学协会。'
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. [Llm-pruner:
    On the structural pruning of large language models](http://arxiv.org/abs/2305.11627).'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2023) Xinyin Ma, Gongfan Fang, 和 Xinchao Wang. 2023. [Llm-pruner：大型语言模型的结构性剪枝](http://arxiv.org/abs/2305.11627)。
- en: 'Mohammadshahi et al. (2022) Alireza Mohammadshahi, Vassilina Nikoulina, Alexandre
    Berard, Caroline Brun, James Henderson, and Laurent Besacier. 2022. [Small-100:
    Introducing shallow multilingual machine translation model for low-resource languages](https://doi.org/10.48550/arXiv.2210.11621).
    arXiv:2210.11621arXiv:2210.11621\. arXiv. ArXiv:2210.11621 [cs].'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohammadshahi et al. (2022) Alireza Mohammadshahi, Vassilina Nikoulina, Alexandre
    Berard, Caroline Brun, James Henderson, 和 Laurent Besacier. 2022. [Small-100：为低资源语言引入浅层多语言机器翻译模型](https://doi.org/10.48550/arXiv.2210.11621)。arXiv:2210.11621arXiv:2210.11621。arXiv。ArXiv:2210.11621
    [cs]。
- en: Muennighoff et al. (2023) Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
    Adam Roberts, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng-Xin
    Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid
    Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin
    Raffel. 2023. [Crosslingual generalization through multitask finetuning](https://doi.org/10.48550/arXiv.2211.01786).
    arXiv:2211.01786\. arXiv. ArXiv:2211.01786 [cs].
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muennighoff et al. (2023) Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
    Adam Roberts, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng-Xin
    Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid
    Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, 和 Colin
    Raffel. 2023. [通过多任务微调实现跨语言泛化](https://doi.org/10.48550/arXiv.2211.01786)。arXiv:2211.01786。arXiv。ArXiv:2211.01786
    [cs]。
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](http://arxiv.org/abs/2203.02155).
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll
    L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama,
    Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,
    Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, 和 Ryan Lowe. 2022.
    [训练语言模型以遵循指令并获取人类反馈](http://arxiv.org/abs/2203.02155)。
- en: 'Peyré et al. (2019) Gabriel Peyré, Marco Cuturi, et al. 2019. Computational
    optimal transport: With applications to data science. volume 11, pages 355–607\.
    Now Publishers, Inc.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peyré et al. (2019) Gabriel Peyré, Marco Cuturi, 等. 2019. 计算最优传输：在数据科学中的应用。第11卷，第355–607页。Now
    Publishers, Inc.
- en: 'Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
    Percy Liang. 2016. [Squad: 100,000+ questions for machine comprehension of text](http://arxiv.org/abs/1606.05250).'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, 和
    Percy Liang. 2016. [Squad：用于机器理解文本的100,000+问题](http://arxiv.org/abs/1606.05250)。
- en: Raman et al. (2023) Mrigank Raman, Pranav Mani, Davis Liang, and Zachary Lipton.
    2023. [For distillation, tokens are not all you need](https://openreview.net/forum?id=2fc5GOPYip).
    In *NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following*.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raman et al. (2023) Mrigank Raman, Pranav Mani, Davis Liang, 和 Zachary Lipton.
    2023. [对于蒸馏，令牌不是你所需的全部](https://openreview.net/forum?id=2fc5GOPYip)。收录于 *NeurIPS
    2023 指令调整与遵循研讨会*。
- en: Rawte et al. (2023) Vipula Rawte, Amit Sheth, and Amitava Das. 2023. A survey
    of hallucination in large foundation models.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rawte et al. (2023) Vipula Rawte, Amit Sheth, 和 Amitava Das. 2023. 对大型基础模型中的幻觉现象的调查。
- en: 'Sanh et al. (2020) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2020. [Distilbert, a distilled version of bert: smaller, faster, cheaper
    and lighter](http://arxiv.org/abs/1910.01108).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh et al. (2020) Victor Sanh, Lysandre Debut, Julien Chaumond, 和 Thomas Wolf.
    2020. [Distilbert，BERT的蒸馏版本：更小、更快、更便宜、更轻量](http://arxiv.org/abs/1910.01108)。
- en: Sanh et al. (2022) Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach,
    Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,
    Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma
    Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti
    Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,
    Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos
    Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan
    Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush.
    2022. [Multitask prompted training enables zero-shot task generalization](http://arxiv.org/abs/2110.08207).
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等人（2022）Victor Sanh、Albert Webson、Colin Raffel、Stephen H. Bach、Lintang
    Sutawika、Zaid Alyafeai、Antoine Chaffin、Arnaud Stiegler、Teven Le Scao、Arun Raja、Manan
    Dey、M Saiful Bari、Canwen Xu、Urmish Thakker、Shanya Sharma Sharma、Eliza Szczechla、Taewoon
    Kim、Gunjan Chhablani、Nihal Nayak、Debajyoti Datta、Jonathan Chang、Mike Tian-Jian
    Jiang、Han Wang、Matteo Manica、Sheng Shen、Zheng Xin Yong、Harshit Pandey、Rachel Bawden、Thomas
    Wang、Trishala Neeraj、Jos Rozen、Abheesht Sharma、Andrea Santilli、Thibault Fevry、Jason
    Alan Fries、Ryan Teehan、Tali Bers、Stella Biderman、Leo Gao、Thomas Wolf 和 Alexander
    M. Rush。2022年。[多任务提示训练实现零样本任务泛化](http://arxiv.org/abs/2110.08207)。
- en: Shuster et al. (2021) Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and
    Jason Weston. 2021. [Retrieval augmentation reduces hallucination in conversation](http://arxiv.org/abs/2104.07567).
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shuster 等人（2021）Kurt Shuster、Spencer Poff、Moya Chen、Douwe Kiela 和 Jason Weston。2021年。[检索增强减少对话中的幻觉](http://arxiv.org/abs/2104.07567)。
- en: 'Sokolova et al. (2006) Marina Sokolova, Nathalie Japkowicz, and Stan Szpakowicz.
    2006. [Beyond accuracy, f-score and roc: A family of discriminant measures for
    performance evaluation](https://doi.org/10.1007/11941439_114). volume Vol. 4304,
    pages 1015–1021.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sokolova 等人（2006）Marina Sokolova、Nathalie Japkowicz 和 Stan Szpakowicz。2006年。[超越准确率、f-score
    和 roc：一系列用于性能评估的判别度量](https://doi.org/10.1007/11941439_114)。第4304卷，第1015–1021页。
- en: 'Sun et al. (2020) Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming
    Yang, and Denny Zhou. 2020. [Mobilebert: a compact task-agnostic bert for resource-limited
    devices](http://arxiv.org/abs/2004.02984).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等人（2020）Zhiqing Sun、Hongkun Yu、Xiaodan Song、Renjie Liu、Yiming Yang 和 Denny
    Zhou。2020年。[Mobilebert: 一种紧凑的任务无关 BERT 适用于资源有限的设备](http://arxiv.org/abs/2004.02984)。'
- en: 'Timiryasov and Tastet (2023) Inar Timiryasov and Jean-Loup Tastet. 2023. [Baby
    llama: knowledge distillation from an ensemble of teachers trained on a small
    dataset with no performance penalty](http://arxiv.org/abs/2308.02019).'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Timiryasov 和 Tastet（2023）Inar Timiryasov 和 Jean-Loup Tastet。2023年。[Baby llama:
    从一个小数据集训练的教师集合中提取知识](http://arxiv.org/abs/2308.02019)。'
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023a. [Llama: Open and efficient foundation language models](https://doi.org/10.48550/arXiv.2302.13971).
    arXiv:2302.13971arXiv:2302.13971\. arXiv. ArXiv:2302.13971 [cs].'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人（2023a）Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar、Aurelien
    Rodriguez、Armand Joulin、Edouard Grave 和 Guillaume Lample。2023年。[Llama: 开放且高效的基础语言模型](https://doi.org/10.48550/arXiv.2302.13971)。arXiv:2302.13971arXiv:2302.13971\.
    arXiv. ArXiv:2302.13971 [cs]。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b.
    [Llama 2: Open foundation and fine-tuned chat models](http://arxiv.org/abs/2307.09288).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2023b）Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale、Dan Bikel、Lukas
    Blecher、Cristian Canton Ferrer、Moya Chen、Guillem Cucurull、David Esiobu、Jude Fernandes、Jeremy
    Fu、Wenyin Fu、Brian Fuller、Cynthia Gao、Vedanuj Goswami、Naman Goyal、Anthony Hartshorn、Saghar
    Hosseini、Rui Hou、Hakan Inan、Marcin Kardas、Viktor Kerkez、Madian Khabsa、Isabel Kloumann、Artem
    Korenev、Punit Singh Koura、Marie-Anne Lachaux、Thibaut Lavril、Jenya Lee、Diana Liskovich、Yinghai
    Lu、Yuning Mao、Xavier Martinet、Todor Mihaylov、Pushkar Mishra、Igor Molybog、Yixin
    Nie、Andrew Poulton、Jeremy Reizenstein、Rashi Rungta、Kalyan Saladi、Alan Schelten、Ruan
    Silva、Eric Michael Smith、Ranjan Subramanian、Xiaoqing Ellen Tan、Binh Tang、Ross
    Taylor、Adina Williams、Jian Xiang Kuan、Puxin Xu、Zheng Yan、Iliyan Zarov、Yuchen Zhang、Angela
    Fan、Melanie Kambadur、Sharan Narang、Aurelien Rodriguez、Robert Stojnic、Sergey Edunov
    和 Thomas Scialom。2023b年。[Llama 2：开放基础和微调的聊天模型](http://arxiv.org/abs/2307.09288)。
- en: Ushio et al. (2023) Asahi Ushio, Fernando Alva-Manchego, and Jose Camacho-Collados.
    2023. An empirical comparison of lm-based question and answer generation methods.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ushio 等人（2023）Asahi Ushio、Fernando Alva-Manchego 和 Jose Camacho-Collados。2023年。基于语言模型的问题和答案生成方法的实证比较。
- en: 'Villani et al. (2009) Cédric Villani et al. 2009. *Optimal transport: old and
    new*, volume 338. Springer.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Villani 等人（2009）Cédric Villani 等人。2009年。*最优运输：旧与新*，第338卷。Springer。
- en: 'Wang et al. (2021) Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu
    Wei. 2021. [Minilmv2: Multi-head self-attention relation distillation for compressing
    pretrained transformers](http://arxiv.org/abs/2012.15828).'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2021）Wenhui Wang、Hangbo Bao、Shaohan Huang、Li Dong 和 Furu Wei。2021年。[Minilmv2：用于压缩预训练变换器的多头自注意力关系蒸馏](http://arxiv.org/abs/2012.15828)。
- en: 'Wang et al. (2020a) Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and
    Ming Zhou. 2020a. [Minilm: Deep self-attention distillation for task-agnostic
    compression of pre-trained transformers](http://arxiv.org/abs/2002.10957).'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2020a）Wenhui Wang、Furu Wei、Li Dong、Hangbo Bao、Nan Yang 和 Ming Zhou。2020a年。[Minilm：用于任务无关的预训练变换器压缩的深度自注意力蒸馏](http://arxiv.org/abs/2002.10957)。
- en: 'Wang et al. (2020b) Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni.
    2020b. Generalizing from a few examples: A survey on few-shot learning. volume 53,
    pages 1–34\. ACM New York, NY, USA.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2020b）Yaqing Wang、Quanming Yao、James T Kwok 和 Lionel M Ni。2020b年。从少量示例中进行推广：少样本学习的调查。第53卷，第1–34页。ACM
    纽约，NY，USA。
- en: 'Wu et al. (2023) Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed,
    and Alham Fikri Aji. 2023. [Lamini-lm: A diverse herd of distilled models from
    large-scale instructions](http://arxiv.org/abs/2304.14402).'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人（2023）Minghao Wu、Abdul Waheed、Chiyu Zhang、Muhammad Abdul-Mageed 和 Alham
    Fikri Aji。2023年。[Lamini-lm：从大规模指令中提取的多样化模型群体](http://arxiv.org/abs/2304.14402)。
- en: Xu et al. (2018) Hongteng Xu, Wenlin Wang, Wei Liu, and Lawrence Carin. 2018.
    [Distilled wasserstein learning for word embedding and topic modeling](http://arxiv.org/abs/1809.04705).
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2018）Hongteng Xu、Wenlin Wang、Wei Liu 和 Lawrence Carin。2018年。[用于词嵌入和主题建模的蒸馏
    Wasserstein 学习](http://arxiv.org/abs/1809.04705)。
- en: 'Xu et al. (2022) Ying Xu, Dakuo Wang, Mo Yu, Daniel Ritchie, Bingsheng Yao,
    Tongshuang Wu, Zheng Zhang, Toby Jia-Jun Li, Nora Bradford, Branda Sun, Tran Bao
    Hoang, Yisi Sang, Yufang Hou, Xiaojuan Ma, Diyi Yang, Nanyun Peng, Zhou Yu, and
    Mark Warschauer. 2022. [Fantastic questions and where to find them: Fairytaleqa
    – an authentic dataset for narrative comprehension](http://arxiv.org/abs/2203.13947).'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2022）Ying Xu、Dakuo Wang、Mo Yu、Daniel Ritchie、Bingsheng Yao、Tongshuang
    Wu、Zheng Zhang、Toby Jia-Jun Li、Nora Bradford、Branda Sun、Tran Bao Hoang、Yisi Sang、Yufang
    Hou、Xiaojuan Ma、Diyi Yang、Nanyun Peng、Zhou Yu 和 Mark Warschauer。2022年。[奇妙的问题及其来源：Fairytaleqa——一个真实的叙事理解数据集](http://arxiv.org/abs/2203.13947)。
- en: Ye et al. (2017) Jianbo Ye, Panruo Wu, James Z. Wang, and Jia Li. 2017. [Fast
    discrete distribution clustering using wasserstein barycenter with sparse support](http://arxiv.org/abs/1510.00012).
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 等 (2017) Jianbo Ye, Panruo Wu, James Z. Wang, 和 Jia Li. 2017. [使用稀疏支持的 Wasserstein
    重心进行快速离散分布聚类](http://arxiv.org/abs/1510.00012)。
- en: 'Ye et al. (2023) Qinyuan Ye, Iz Beltagy, Matthew Peters, Xiang Ren, and Hannaneh
    Hajishirzi. 2023. [FiD-ICL: A fusion-in-decoder approach for efficient in-context
    learning](https://doi.org/10.18653/v1/2023.acl-long.454). In *Proceedings of the
    61st Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, pages 8158–8185, Toronto, Canada. Association for Computational
    Linguistics.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye 等 (2023) Qinyuan Ye, Iz Beltagy, Matthew Peters, Xiang Ren, 和 Hannaneh Hajishirzi.
    2023. [FiD-ICL: 高效上下文学习的融合解码器方法](https://doi.org/10.18653/v1/2023.acl-long.454)。在
    *第 61 届计算语言学协会年会（第 1 卷：长篇论文）*，第 8158–8185 页，加拿大多伦多。计算语言学协会。'
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. [Opt: Open pre-trained
    transformer language models](http://arxiv.org/abs/2205.01068). arXiv:2205.01068\.
    arXiv. ArXiv:2205.01068 [cs].'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor
    Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura,
    Anjali Sridhar, Tianlu Wang, 和 Luke Zettlemoyer. 2022. [Opt: 开放预训练变换器语言模型](http://arxiv.org/abs/2205.01068)。arXiv:2205.01068。arXiv。ArXiv:2205.01068
    [cs]。'
- en: 'Zhang* et al. (2020) Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger,
    and Yoav Artzi. 2020. [Bertscore: Evaluating text generation with bert](https://openreview.net/forum?id=SkeHuCVFDr).
    In *International Conference on Learning Representations*.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang* 等 (2020) Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger,
    和 Yoav Artzi. 2020. [Bertscore: 用 Bert 评估文本生成](https://openreview.net/forum?id=SkeHuCVFDr)。在
    *国际学习表征会议*。'
- en: Zhao et al. (2023a) Jiachen Zhao, Wenlong Zhao, Andrew Drozdov, Benjamin Rozonoyer,
    Md Arafat Sultan, Jay-Yoon Lee, Mohit Iyyer, and Andrew McCallum. 2023a. [Multistage
    collaborative knowledge distillation from large language models](http://arxiv.org/abs/2311.08640).
    arXiv:2311.08640\. arXiv. ArXiv:2311.08640 [cs].
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2023a) Jiachen Zhao, Wenlong Zhao, Andrew Drozdov, Benjamin Rozonoyer,
    Md Arafat Sultan, Jay-Yoon Lee, Mohit Iyyer, 和 Andrew McCallum. 2023a. [来自大型语言模型的多阶段协作知识蒸馏](http://arxiv.org/abs/2311.08640)。arXiv:2311.08640。arXiv。ArXiv:2311.08640
    [cs]。
- en: 'Zhao et al. (2023b) Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin
    Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison,
    Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit
    Mathews, and Shen Li. 2023b. [Pytorch fsdp: Experiences on scaling fully sharded
    data parallel](http://arxiv.org/abs/2304.11277).'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等 (2023b) Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang,
    Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison,
    Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit
    Mathews, 和 Shen Li. 2023b. [Pytorch fsdp: 关于扩展完全分片数据并行的经验](http://arxiv.org/abs/2304.11277)。'
- en: Zhou and Chiam (2023) Tianxun Zhou and Keng-Hwee Chiam. 2023. Synthetic data
    generation method for data-free knowledge distillation in regression neural networks.
    volume 227, page 120327\. Elsevier.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 和 Chiam (2023) Tianxun Zhou 和 Keng-Hwee Chiam. 2023. 无需数据的回归神经网络知识蒸馏的合成数据生成方法。卷
    227，第 120327 页。Elsevier。
- en: Zhuang et al. (2022) Yubo Zhuang, Xiaohui Chen, and Yun Yang. 2022. [Wasserstein
    $k$-means for clustering probability distributions](http://arxiv.org/abs/2209.06975).
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang 等 (2022) Yubo Zhuang, Xiaohui Chen, 和 Yun Yang. 2022. [Wasserstein $k$-均值用于概率分布的聚类](http://arxiv.org/abs/2209.06975)。
- en: 8 Appendix - General Results
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 附录 - 一般结果
- en: 8.1 Summary
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 摘要
- en: '| Teacher | Model | Method | Dataset | Rouge-1 | Rouge-2 | Rouge-L | Rouge-Lsum
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 教师 | 模型 | 方法 | 数据集 | Rouge-1 | Rouge-2 | Rouge-L | Rouge-Lsum |'
- en: '| Llama | Bloomz-560m | Raw Text | DIALOGSum | 24.71 | 10.06 | 19.99 | 20.01
    |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Bloomz-560m | 原始文本 | DIALOGSum | 24.71 | 10.06 | 19.99 | 20.01 |'
- en: '| Llama | Bloomz-560m | ULD Loss | DIALOGSum | 28.08 | 11.68 | 22.64 | 22.67
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Bloomz-560m | ULD 损失 | DIALOGSum | 28.08 | 11.68 | 22.64 | 22.67
    |'
- en: '| Mistral | Bloomz-560m | Raw Text | DIALOGSum | 39.85 | 15.36 | 31.92 | 31.95
    |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Bloomz-560m | 原始文本 | DIALOGSum | 39.85 | 15.36 | 31.92 | 31.95
    |'
- en: '| Mistral | Bloomz-560m | ULD Loss | DIALOGSum | 40.57 | 15.94 | 32.6 | 32.58
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Bloomz-560m | ULD 损失 | DIALOGSum | 40.57 | 15.94 | 32.6 | 32.58
    |'
- en: '| Llama | OPT-350m | Raw Text | DIALOGSum | 25.4 | 10.48 | 20.57 | 20.58 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| Llama | OPT-350m | 原始文本 | DIALOGSum | 25.4 | 10.48 | 20.57 | 20.58 |'
- en: '| Llama | OPT-350m | ULD Loss | DIALOGSum | 23.69 | 9.76 | 20.13 | 20.11 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| Llama | OPT-350m | ULD 损失 | DIALOGSum | 23.69 | 9.76 | 20.13 | 20.11 |'
- en: '| Mistral | OPT-350m | Raw Text | DIALOGSum | 39.33 | 14.97 | 31.49 | 31.44
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | OPT-350m | Raw Text | DIALOGSum | 39.33 | 14.97 | 31.49 | 31.44
    |'
- en: '| Mistral | OPT-350m | ULD Loss | DIALOGSum | 39.8 | 15.76 | 32.19 | 32.17
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | OPT-350m | ULD Loss | DIALOGSum | 39.8 | 15.76 | 32.19 | 32.17
    |'
- en: '| Llama | Pythia-410m | Raw Text | DIALOGSum | 26.28 | 10.52 | 20.92 | 20.94
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-410m | Raw Text | DIALOGSum | 26.28 | 10.52 | 20.92 | 20.94
    |'
- en: '| Llama | Pythia-410m | ULD Loss | DIALOGSum | 27.29 | 11.2 | 22.17 | 22.19
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-410m | ULD Loss | DIALOGSum | 27.29 | 11.2 | 22.17 | 22.19
    |'
- en: '| Mistral | Pythia-410m | Raw Text | DIALOGSum | 39.69 | 15.0 | 31.62 | 31.64
    |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Pythia-410m | Raw Text | DIALOGSum | 39.69 | 15.0 | 31.62 | 31.64
    |'
- en: '| Mistral | Pythia-410m | ULD Loss | DIALOGSum | 41.39 | 15.93 | 33.08 | 33.1
    |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Pythia-410m | ULD Loss | DIALOGSum | 41.39 | 15.93 | 33.08 | 33.1
    |'
- en: '| Llama | Pythia-160m | Raw Text | DIALOGSum | 20.34 | 7.46 | 16.81 | 16.81
    |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-160m | Raw Text | DIALOGSum | 20.34 | 7.46 | 16.81 | 16.81
    |'
- en: '| Llama | Pythia-160m | ULD Loss | DIALOGSum | 22.94 | 8.39 | 19.56 | 19.55
    |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-160m | ULD Loss | DIALOGSum | 22.94 | 8.39 | 19.56 | 19.55
    |'
- en: '| Llama | Pythia-1b | Raw Text | DIALOGSum | 27.71 | 11.08 | 21.86 | 21.88
    |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-1b | Raw Text | DIALOGSum | 27.71 | 11.08 | 21.86 | 21.88
    |'
- en: '| Llama | Pythia-1b | ULD Loss | DIALOGSum | 28.48 | 12.16 | 23.04 | 23.04
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-1b | ULD Loss | DIALOGSum | 28.48 | 12.16 | 23.04 | 23.04
    |'
- en: 'Table 4: Details performance of Teacher/Student pair models trained with ULD
    Loss and teacher-generated text (Raw Text) for the Summary task. Evaluations are
    performed over respective test splits.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：使用 ULD Loss 和教师生成文本（Raw Text）训练的教师/学生对模型在摘要任务上的详细性能。评估是在各自的测试拆分上进行的。
- en: 8.2 Extractive QA
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 抽取式问答
- en: '| Teacher | Model | Method | Dataset | F1 | Precision | Recall |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| Teacher | Model | Method | Dataset | F1 | Precision | Recall |'
- en: '| Llama | Bloomz-560m | Raw Text | SQuAD | 73.54 | 75.35 | 75.19 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Bloomz-560m | Raw Text | SQuAD | 73.54 | 75.35 | 75.19 |'
- en: '| Llama | Bloomz-560m | ULD Loss | SQuAD | 75.9 | 77.37 | 77.88 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Bloomz-560m | ULD Loss | SQuAD | 75.9 | 77.37 | 77.88 |'
- en: '| Llama | Bloomz-560m | Raw Text | QED | 50.99 | 58.9 | 52.38 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Bloomz-560m | Raw Text | QED | 50.99 | 58.9 | 52.38 |'
- en: '| Llama | Bloomz-560m | ULD Loss | QED | 55.33 | 63.22 | 56.47 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Bloomz-560m | ULD Loss | QED | 55.33 | 63.22 | 56.47 |'
- en: '| Mistral | Bloomz-560m | Raw Text | SQuAD | 73.34 | 73.31 | 78.52 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Bloomz-560m | Raw Text | SQuAD | 73.34 | 73.31 | 78.52 |'
- en: '| Mistral | Bloomz-560m | ULD Loss | SQuAD | 76.0 | 76.1 | 81.1 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Bloomz-560m | ULD Loss | SQuAD | 76.0 | 76.1 | 81.1 |'
- en: '| Mistral | Bloomz-560m | Raw Text | QED | 52.15 | 57.49 | 56.28 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Bloomz-560m | Raw Text | QED | 52.15 | 57.49 | 56.28 |'
- en: '| Mistral | Bloomz-560m | ULD Loss | QED | 55.79 | 61.98 | 58.8 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Bloomz-560m | ULD Loss | QED | 55.79 | 61.98 | 58.8 |'
- en: '| Llama | OPT-350m | Raw Text | SQuAD | 70.78 | 72.52 | 72.78 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| Llama | OPT-350m | Raw Text | SQuAD | 70.78 | 72.52 | 72.78 |'
- en: '| Llama | OPT-350m | ULD Loss | SQuAD | 72.97 | 74.61 | 74.99 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| Llama | OPT-350m | ULD Loss | SQuAD | 72.97 | 74.61 | 74.99 |'
- en: '| Llama | OPT-350m | Raw Text | QED | 48.64 | 54.74 | 51.84 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Llama | OPT-350m | Raw Text | QED | 48.64 | 54.74 | 51.84 |'
- en: '| Llama | OPT-350m | ULD Loss | QED | 49.06 | 55.38 | 51.74 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| Llama | OPT-350m | ULD Loss | QED | 49.06 | 55.38 | 51.74 |'
- en: '| Mistral | OPT-350m | Raw Text | SQuAD | 71.64 | 71.67 | 77.28 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | OPT-350m | Raw Text | SQuAD | 71.64 | 71.67 | 77.28 |'
- en: '| Mistral | OPT-350m | ULD Loss | SQuAD | 73.35 | 73.25 | 78.91 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | OPT-350m | ULD Loss | SQuAD | 73.35 | 73.25 | 78.91 |'
- en: '| Mistral | OPT-350m | Raw Text | QED | 50.13 | 55.36 | 54.56 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | OPT-350m | Raw Text | QED | 50.13 | 55.36 | 54.56 |'
- en: '| Mistral | OPT-350m | ULD Loss | QED | 50.88 | 56.61 | 54.53 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | OPT-350m | ULD Loss | QED | 50.88 | 56.61 | 54.53 |'
- en: '| Llama | Pythia-410m | Raw Text | SQuAD | 71.39 | 73.76 | 72.85 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-410m | Raw Text | SQuAD | 71.39 | 73.76 | 72.85 |'
- en: '| Llama | Pythia-410m | ULD Loss | SQuAD | 74.14 | 75.88 | 76.31 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-410m | ULD Loss | SQuAD | 74.14 | 75.88 | 76.31 |'
- en: '| Llama | Pythia-410m | Raw Text | QED | 47.04 | 54.31 | 48.87 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-410m | Raw Text | QED | 47.04 | 54.31 | 48.87 |'
- en: '| Llama | Pythia-410m | ULD Loss | QED | 49.15 | 54.75 | 53.13 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-410m | ULD Loss | QED | 49.15 | 54.75 | 53.13 |'
- en: '| Mistral | Pythia-410m | Raw Text | SQuAD | 71.5 | 71.33 | 77.54 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Pythia-410m | Raw Text | SQuAD | 71.5 | 71.33 | 77.54 |'
- en: '| Mistral | Pythia-410m | ULD Loss | SQuAD | 73.64 | 73.34 | 79.71 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Pythia-410m | ULD Loss | SQuAD | 73.64 | 73.34 | 79.71 |'
- en: '| Mistral | Pythia-410m | Raw Text | QED | 47.07 | 50.2 | 54.67 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Pythia-410m | Raw Text | QED | 47.07 | 50.2 | 54.67 |'
- en: '| Mistral | Pythia-410m | ULD Loss | QED | 50.38 | 54.19 | 56.62 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Pythia-410m | ULD Loss | QED | 50.38 | 54.19 | 56.62 |'
- en: '| Llama | Pythia-160m | Raw Text | SQuAD | 52.83 | 53.68 | 56.67 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-160m | Raw Text | SQuAD | 52.83 | 53.68 | 56.67 |'
- en: '| Llama | Pythia-160m | ULD Loss | SQuAD | 53.86 | 54.57 | 58.19 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-160m | ULD Loss | SQuAD | 53.86 | 54.57 | 58.19 |'
- en: '| Llama | Pythia-160m | Raw Text | QED | 21.11 | 21.69 | 34.46 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-160m | Raw Text | QED | 21.11 | 21.69 | 34.46 |'
- en: '| Llama | Pythia-160m | ULD Loss | QED | 27.48 | 30.3 | 33.8 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-160m | ULD Loss | QED | 27.48 | 30.3 | 33.8 |'
- en: '| Llama | Pythia-1b | Raw Text | SQuAD | 75.89 | 77.36 | 78.28 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-1b | Raw Text | SQuAD | 75.89 | 77.36 | 78.28 |'
- en: '| Llama | Pythia-1b | ULD Loss | SQuAD | 77.1 | 78.57 | 79.55 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-1b | ULD Loss | SQuAD | 77.1 | 78.57 | 79.55 |'
- en: '| Llama | Pythia-1b | Raw Text | QED | 48.59 | 51.05 | 60.41 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-1b | 原始文本 | QED | 48.59 | 51.05 | 60.41 |'
- en: '| Llama | Pythia-1b | ULD Loss | QED | 51.22 | 55.3 | 59.7 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-1b | ULD Loss | QED | 51.22 | 55.3 | 59.7 |'
- en: 'Table 5: Details performance of Teacher/Student pair models trained with ULD
    Loss and teacher-generated text (Raw Text) for Extractive QA task. Evaluations
    are performed over respective test splits.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：使用 ULD Loss 和教师生成文本（原始文本）训练的教师/学生对模型在抽取式 QA 任务中的详细性能。评估在各自的测试拆分上进行。
- en: 8.3 Generative QA
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 生成式 QA
- en: '| Teacher | Model | Method | Dataset | BERTScore | PBERT | RBERT |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 教师 | 模型 | 方法 | 数据集 | BERTScore | PBERT | RBERT |'
- en: '| Llama | Bloomz-560m | Raw Text | FairytaleQA | 36.7 | 45.42 | 28.46 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Bloomz-560m | 原始文本 | FairytaleQA | 36.7 | 45.42 | 28.46 |'
- en: '| Llama | Bloomz-560m | ULD Loss | FairytaleQA | 37.86 | 46.93 | 29.36 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Bloomz-560m | ULD Loss | FairytaleQA | 37.86 | 46.93 | 29.36 |'
- en: '| Llama | Bloomz-560m | Raw Text | PubMedQA | 29.14 | 29.45 | 28.86 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Bloomz-560m | 原始文本 | PubMedQA | 29.14 | 29.45 | 28.86 |'
- en: '| Llama | Bloomz-560m | ULD Loss | PubMedQA | 30.01 | 32.5 | 27.65 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Bloomz-560m | ULD Loss | PubMedQA | 30.01 | 32.5 | 27.65 |'
- en: '| Mistral | Bloomz-560m | Raw Text | FairytaleQA | 32.64 | 39.46 | 26.09 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Bloomz-560m | 原始文本 | FairytaleQA | 32.64 | 39.46 | 26.09 |'
- en: '| Mistral | Bloomz-560m | ULD Loss | FairytaleQA | 33.93 | 42.45 | 25.8 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Bloomz-560m | ULD Loss | FairytaleQA | 33.93 | 42.45 | 25.8 |'
- en: '| Mistral | Bloomz-560m | Raw Text | PubMedQA | 28.87 | 28.59 | 29.14 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Bloomz-560m | 原始文本 | PubMedQA | 28.87 | 28.59 | 29.14 |'
- en: '| Mistral | Bloomz-560m | ULD Loss | PubMedQA | 30.6 | 32.08 | 29.13 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Bloomz-560m | ULD Loss | PubMedQA | 30.6 | 32.08 | 29.13 |'
- en: '| Llama | OPT-350m | Raw Text | FairytaleQA | 33.78 | 41.46 | 26.57 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Llama | OPT-350m | 原始文本 | FairytaleQA | 33.78 | 41.46 | 26.57 |'
- en: '| Llama | OPT-350m | ULD Loss | FairytaleQA | 33.03 | 41.16 | 25.32 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Llama | OPT-350m | ULD Loss | FairytaleQA | 33.03 | 41.16 | 25.32 |'
- en: '| Llama | OPT-350m | Raw Text | PubMedQA | 27.99 | 28.46 | 27.56 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| Llama | OPT-350m | 原始文本 | PubMedQA | 27.99 | 28.46 | 27.56 |'
- en: '| Llama | OPT-350m | ULD Loss | PubMedQA | 30.01 | 36.11 | 24.14 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| Llama | OPT-350m | ULD Loss | PubMedQA | 30.01 | 36.11 | 24.14 |'
- en: '| Mistral | OPT-350m | Raw Text | FairytaleQA | 30.09 | 35.47 | 24.91 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | OPT-350m | 原始文本 | FairytaleQA | 30.09 | 35.47 | 24.91 |'
- en: '| Mistral | OPT-350m | ULD Loss | FairytaleQA | 30.44 | 37.38 | 23.81 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | OPT-350m | ULD Loss | FairytaleQA | 30.44 | 37.38 | 23.81 |'
- en: '| Mistral | OPT-350m | Raw Text | PubMedQA | 27.91 | 27.06 | 28.75 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | OPT-350m | 原始文本 | PubMedQA | 27.91 | 27.06 | 28.75 |'
- en: '| Mistral | OPT-350m | ULD Loss | PubMedQA | 30.3 | 36.99 | 23.82 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | OPT-350m | ULD Loss | PubMedQA | 30.3 | 36.99 | 23.82 |'
- en: '| Llama | Pythia-410m | Raw Text | FairytaleQA | 33.02 | 41.31 | 25.26 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-410m | 原始文本 | FairytaleQA | 33.02 | 41.31 | 25.26 |'
- en: '| Llama | Pythia-410m | ULD Loss | FairytaleQA | 34.83 | 42.61 | 27.49 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-410m | ULD Loss | FairytaleQA | 34.83 | 42.61 | 27.49 |'
- en: '| Llama | Pythia-410m | Raw Text | PubMedQA | 29.86 | 31.06 | 28.72 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-410m | 原始文本 | PubMedQA | 29.86 | 31.06 | 28.72 |'
- en: '| Llama | Pythia-410m | ULD Loss | PubMedQA | 29.89 | 31.23 | 28.62 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-410m | ULD Loss | PubMedQA | 29.89 | 31.23 | 28.62 |'
- en: '| Mistral | Pythia-410m | Raw Text | FairytaleQA | 31.44 | 37.97 | 25.18 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Pythia-410m | 原始文本 | FairytaleQA | 31.44 | 37.97 | 25.18 |'
- en: '| Mistral | Pythia-410m | ULD Loss | FairytaleQA | 31.79 | 38.17 | 25.71 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Pythia-410m | ULD Loss | FairytaleQA | 31.79 | 38.17 | 25.71 |'
- en: '| Mistral | Pythia-410m | Raw Text | PubMedQA | 28.25 | 25.91 | 30.56 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Pythia-410m | 原始文本 | PubMedQA | 28.25 | 25.91 | 30.56 |'
- en: '| Mistral | Pythia-410m | ULD Loss | PubMedQA | 29.55 | 30.09 | 29.01 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | Pythia-410m | ULD Loss | PubMedQA | 29.55 | 30.09 | 29.01 |'
- en: '| Llama | Pythia-160m | Raw Text | FairytaleQA | 22.03 | 30.05 | 14.5 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-160m | 原始文本 | FairytaleQA | 22.03 | 30.05 | 14.5 |'
- en: '| Llama | Pythia-160m | ULD Loss | FairytaleQA | 22.58 | 31.61 | 14.08 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-160m | ULD Loss | FairytaleQA | 22.58 | 31.61 | 14.08 |'
- en: '| Llama | Pythia-160m | Raw Text | PubMedQA | 26.54 | 26.26 | 26.85 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-160m | 原始文本 | PubMedQA | 26.54 | 26.26 | 26.85 |'
- en: '| Llama | Pythia-160m | ULD Loss | PubMedQA | 29.78 | 36.4 | 23.36 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-160m | ULD Loss | PubMedQA | 29.78 | 36.4 | 23.36 |'
- en: '| Llama | Pythia-1b | Raw Text | FairytaleQA | 36.13 | 46.11 | 26.74 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-1b | 原始文本 | FairytaleQA | 36.13 | 46.11 | 26.74 |'
- en: '| Llama | Pythia-1b | ULD Loss | FairytaleQA | 37.34 | 46.93 | 28.33 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-1b | ULD Loss | FairytaleQA | 37.34 | 46.93 | 28.33 |'
- en: '| Llama | Pythia-1b | Raw Text | PubMedQA | 30.12 | 31.67 | 28.64 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-1b | 原始文本 | PubMedQA | 30.12 | 31.67 | 28.64 |'
- en: '| Llama | Pythia-1b | ULD Loss | PubMedQA | 29.88 | 30.4 | 29.44 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Pythia-1b | ULD Loss | PubMedQA | 29.88 | 30.4 | 29.44 |'
- en: 'Table 6: Details performance of Teacher/Student pair models trained with ULD
    Loss and teacher-generated text (Raw Text) for generative tasks. Evaluations are
    performed over respective test splits.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：使用 ULD Loss 和教师生成文本（原始文本）训练的教师/学生对模型在生成任务中的详细性能。评估在各自的测试拆分上进行。
- en: 9 Appendix - Native Performances
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 附录 - 原生性能
- en: 'Base models used as teacher and student can be respectively download on HuggingFace:
    LLama 2 7b Chat⁵⁵5[https://huggingface.co/meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf),
    Mistral 7b Instruct⁶⁶6[https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2),
    Pythia 160m, Pythia 410m, Pythia 1b⁷⁷7[https://huggingface.co/EleutherAI](https://huggingface.co/EleutherAI),
    Bloomz 560m, MT0 580m⁸⁸8[https://huggingface.co/bigscience](https://huggingface.co/bigscience).'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 用作教师和学生的基础模型可以在 HuggingFace 上分别下载：LLama 2 7b Chat⁵⁵5[https://huggingface.co/meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)，Mistral
    7b Instruct⁶⁶6[https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)，Pythia
    160m，Pythia 410m，Pythia 1b⁷⁷7[https://huggingface.co/EleutherAI](https://huggingface.co/EleutherAI)，Bloomz
    560m，MT0 580m⁸⁸8[https://huggingface.co/bigscience](https://huggingface.co/bigscience)。
- en: 9.1 Summary
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 总结
- en: '| Model | Dataset | Number Few-Shot | Few-Shot Titled | Rouge-1 | Rouge-2 |
    Rouge-L | Rouge-Lsum |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 数据集 | 少量样本数量 | 少量样本标题 | Rouge-1 | Rouge-2 | Rouge-L | Rouge-Lsum |'
- en: '| Bloomz-560m | DIALOGSum | 3 | False | 15.36 | 1.47 | 11.92 | 11.9 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| Bloomz-560m | DIALOGSum | 3 | False | 15.36 | 1.47 | 11.92 | 11.9 |'
- en: '| OPT-350m | DIALOGSum | 2 | False | 22.06 | 3.31 | 17.86 | 17.84 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | DIALOGSum | 2 | False | 22.06 | 3.31 | 17.86 | 17.84 |'
- en: '| Pythia-410m | DIALOGSum | 3 | False | 23.38 | 6.4 | 20.12 | 20.13 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-410m | DIALOGSum | 3 | False | 23.38 | 6.4 | 20.12 | 20.13 |'
- en: '| Pythia-160m | DIALOGSum | 3 | False | 16.0 | 4.72 | 13.88 | 13.88 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-160m | DIALOGSum | 3 | False | 16.0 | 4.72 | 13.88 | 13.88 |'
- en: '| Pythia-1b | DIALOGSum | 3 | False | 33.95 | 11.85 | 29.06 | 29.1 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-1b | DIALOGSum | 3 | False | 33.95 | 11.85 | 29.06 | 29.1 |'
- en: '| Llama | DIALOGSum | 3 | False | 0.3 | 0.13 | 0.24 | 0.24 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| Llama | DIALOGSum | 3 | False | 0.3 | 0.13 | 0.24 | 0.24 |'
- en: '| Mistral | DIALOGSum | 2 | False | 0.43 | 0.18 | 0.35 | 0.35 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | DIALOGSum | 2 | False | 0.43 | 0.18 | 0.35 | 0.35 |'
- en: 'Table 7: Native performance details of Teacher/Student pair models benchmark
    in few-shot setting for the Summary task. Evaluations are performed over respective
    test splits.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：教师/学生对模型在少量样本设置下的原生性能详情，用于总结任务。评估在各自的测试划分上进行。
- en: 9.2 Extractive QA
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2 抽取式问答
- en: '| Model | Dataset | Number Few-Shot | Few-Shot Titled | F1 | Precision | Recall
    |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 数据集 | 少量样本数量 | 少量样本标题 | F1 | 精确度 | 召回率 |'
- en: '| Bloomz-560m | SQuAD | 3 | False | 66.05 | 68.6 | 66.09 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| Bloomz-560m | SQuAD | 3 | False | 66.05 | 68.6 | 66.09 |'
- en: '| Bloomz-560m | QED | 3 | False | 41.01 | 51.55 | 38.83 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| Bloomz-560m | QED | 3 | False | 41.01 | 51.55 | 38.83 |'
- en: '| OPT-350m | SQuAD | 3 | False | 30.01 | 29.34 | 41.04 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | SQuAD | 3 | False | 30.01 | 29.34 | 41.04 |'
- en: '| OPT-350m | QED | 3 | False | 30.21 | 32.82 | 37.6 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | QED | 3 | False | 30.21 | 32.82 | 37.6 |'
- en: '| Pythia-410m | SQuAD | 3 | False | 37.4 | 36.58 | 47.55 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-410m | SQuAD | 3 | False | 37.4 | 36.58 | 47.55 |'
- en: '| Pythia-410m | QED | 3 | False | 33.35 | 38.05 | 37.02 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-410m | QED | 3 | False | 33.35 | 38.05 | 37.02 |'
- en: '| Pythia-160m | SQuAD | 3 | False | 15.05 | 16.39 | 18.83 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-160m | SQuAD | 3 | False | 15.05 | 16.39 | 18.83 |'
- en: '| Pythia-160m | QED | 3 | False | 15.48 | 20.15 | 17.31 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-160m | QED | 3 | False | 15.48 | 20.15 | 17.31 |'
- en: '| Pythia-1b | SQuAD | 3 | False | 48.41 | 48.52 | 55.55 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-1b | SQuAD | 3 | False | 48.41 | 48.52 | 55.55 |'
- en: '| Pythia-1b | QED | 3 | False | 41.72 | 47.18 | 45.76 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-1b | QED | 3 | False | 41.72 | 47.18 | 45.76 |'
- en: '| Llama | SQuAD | 1 | False | 0.81 | 0.83 | 0.84 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| Llama | SQuAD | 1 | False | 0.81 | 0.83 | 0.84 |'
- en: '| Llama | QED | 5 | False | 0.58 | 0.64 | 0.63 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| Llama | QED | 5 | False | 0.58 | 0.64 | 0.63 |'
- en: '| Mistral | SQuAD | 3 | True | 0.76 | 0.74 | 0.89 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | SQuAD | 3 | True | 0.76 | 0.74 | 0.89 |'
- en: '| Mistral | QED | 5 | True | 0.53 | 0.55 | 0.68 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | QED | 5 | True | 0.53 | 0.55 | 0.68 |'
- en: 'Table 8: Native performance details of Teacher/Student pair models benchmark
    in few-shot setting for extractive QA tasks. Evaluations are performed over respective
    test splits.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：教师/学生对模型在少量样本设置下的原生性能详情，用于抽取式问答任务。评估在各自的测试划分上进行。
- en: 9.3 Generative QA
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3 生成式问答
- en: '| Model | Dataset | Number Few-Shot | Few-Shot Titled | BERTScore | PBERT |
    RBERT |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 数据集 | 少量样本数量 | 少量样本标题 | BERTScore | PBERT | RBERT |'
- en: '| Bloomz-560m | FairytaleQA | 3 | False | 27.43 | 31.42 | 23.67 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| Bloomz-560m | FairytaleQA | 3 | False | 27.43 | 31.42 | 23.67 |'
- en: '| Bloomz-560m | PubMedQA | 3 | False | -20.3 | -9.43 | -30.97 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| Bloomz-560m | PubMedQA | 3 | False | -20.3 | -9.43 | -30.97 |'
- en: '| OPT-350m | FairytaleQA | 3 | False | 3.82 | -4.33 | 13.1 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | FairytaleQA | 3 | False | 3.82 | -4.33 | 13.1 |'
- en: '| OPT-350m | PubMedQA | 3 | False | 19.98 | 23.29 | 16.94 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | PubMedQA | 3 | False | 19.98 | 23.29 | 16.94 |'
- en: '| Pythia-410m | FairytaleQA | 3 | False | 6.76 | 1.82 | 12.43 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-410m | FairytaleQA | 3 | False | 6.76 | 1.82 | 12.43 |'
- en: '| Pythia-410m | PubMedQA | 3 | False | 25.65 | 30.13 | 21.42 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-410m | PubMedQA | 3 | False | 25.65 | 30.13 | 21.42 |'
- en: '| Pythia-160m | FairytaleQA | 3 | False | -0.96 | -6.87 | 6.3 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-160m | FairytaleQA | 3 | False | -0.96 | -6.87 | 6.3 |'
- en: '| Pythia-160m | PubMedQA | 3 | False | 21.35 | 27.14 | 15.93 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-160m | PubMedQA | 3 | False | 21.35 | 27.14 | 15.93 |'
- en: '| Pythia-1b | FairytaleQA | 3 | False | 20.59 | 22.4 | 19.23 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-1b | FairytaleQA | 3 | 错误 | 20.59 | 22.4 | 19.23 |'
- en: '| Pythia-1b | PubMedQA | 3 | False | 26.13 | 29.29 | 23.24 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-1b | PubMedQA | 3 | 错误 | 26.13 | 29.29 | 23.24 |'
- en: '| Llama | FairytaleQA | 2 | False | 0.42 | 0.48 | 0.36 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| Llama | FairytaleQA | 2 | 错误 | 0.42 | 0.48 | 0.36 |'
- en: '| Llama | PubMedQA | 3 | False | 0.31 | 0.3 | 0.32 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| Llama | PubMedQA | 3 | 错误 | 0.31 | 0.3 | 0.32 |'
- en: '| Mistral | FairytaleQA | 5 | True | 0.41 | 0.38 | 0.45 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | FairytaleQA | 5 | 正确 | 0.41 | 0.38 | 0.45 |'
- en: '| Mistral | PubMedQA | 3 | False | 0.31 | 0.28 | 0.34 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | PubMedQA | 3 | 错误 | 0.31 | 0.28 | 0.34 |'
- en: 'Table 9: Native performance details of Teacher/Student pair models benchmark
    in few-shot setting for generative QA tasks. Evaluations are performed over respective
    test splits.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：教师/学生配对模型在少样本设置下的原生表现细节，评估基于各自的测试分割进行。
- en: 10 Appendix - Few-Shot examples and Prompt Systems
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 附录 - 少样本示例和提示系统
- en: 'The few-shot technique was used to generate synthetic data with the teacher.
    The number of few-shots reported for evaluating teacher models in [Sec. 9](#S9
    "9 Appendix - Native Performances ‣ Towards Cross-Tokenizer Distillation: the
    Universal Logit Distillation Loss for LLMs") are the same numbers used to generate
    the synthetic answers. It’s also important to note that the few-shot method was
    only used to determine the native performance of the teacher and student, not
    the distilled versions.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本技术用于与教师生成合成数据。用于评估教师模型的少样本数量与生成合成答案所用的数量相同。还要注意，少样本方法仅用于确定教师和学生的原生表现，而非蒸馏版本。
- en: 10.1 Prompt Systems
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1 提示系统
- en: 'List of prompt system used with teacher templates. The default templates for
    chat models provided with the huggingface tokenizer have been retained:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 使用教师模板的提示系统列表。保留了与huggingface tokenizer一起提供的默认聊天模型模板。
- en: •
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Extractive QA: You are an agent answering questions as part of a reading comprehension
    activity. You must read and understand the context text step by step. Answers
    are brief and consist exclusively of continuous words taken from the context text
    provided.'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 抽取式问答：你是一个回答问题的代理，作为阅读理解活动的一部分。你必须逐步阅读和理解上下文文本。答案简洁，仅由从提供的上下文文本中提取的连续单词组成。
- en: •
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Generative QA: You are an expert agent in reading comprehension (question answering).
    You must read and understand the contextual text step by step, then answer the
    question. The answer must be brief.'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成式问答：你是一个阅读理解（问题回答）方面的专家。你必须逐步阅读和理解上下文文本，然后回答问题。答案必须简洁。
- en: •
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Summary: You’re an expert at summarizing dialogues. You have to read the dialogue
    between two people and summarize it in no more than one sentence. The summary
    should be as short as possible, not re-explaining the dialogue in detail and using
    the person’s name when implicitly mentioned.'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总结：你是对对话总结非常熟练的专家。你需要阅读两个人之间的对话，并将其总结为不超过一句话。总结应尽可能简短，不详细解释对话，并在隐含提到某人时使用他们的名字。
- en: 10.2 Few-Shot examples
  id: totrans-411
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2 少样本示例
- en: '| Title | Context | Question | Answer |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 上下文 | 问题 | 答案 |'
- en: '| Christine’s boyfriend | Patrick Harris (Tim DeKay), Old Christine’s new boyfriend,
    who she meets in a video store and starts dating. | Who played patrick on new
    adventures of old christine? | Tim DeKay |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| Christine的男朋友 | Patrick Harris (Tim DeKay)，旧Christine的新男朋友，她在一家视频店遇见他并开始约会。
    | 谁在《旧Christine的新冒险》中扮演Patrick？ | Tim DeKay |'
- en: '| June 14, 2018: Death Row Inmates | As of June 14, 2018, there were 2,718
    death row inmates in the United States. | Total number of death row inmates in
    the us? | 2,718 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 2018年6月14日：死囚 | 截至2018年6月14日，美国共有2,718名死囚。 | 美国死囚的总数？ | 2,718 |'
- en: '| Modern Communism | Most modern forms of communism are grounded at least nominally
    in Marxism, an ideology conceived by noted sociologist Karl Marx during the mid
    nineteenth century. | Who came up with the idea of communism? | Karl Marx |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 现代共产主义 | 现代共产主义大多至少在名义上植根于马克思主义，这是一种由著名社会学家卡尔·马克思在19世纪中叶提出的意识形态。 | 谁提出了共产主义的思想？
    | 卡尔·马克思 |'
- en: '| Napoleon’s Defeat by Seventh Coalition | A French army under the command
    of Napoleon Bonaparte was defeated by two of the armies of the Seventh Coalition
    : a British-led Allied army under the command of the Duke of Wellington, and a
    Prussian army under the command of Gebhard Leberecht von Blücher, Prince of Wahlstatt.
    | Who commanded british forces at the battle of waterloo? | The Duke of Wellington
    |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 拿破仑被第七次联盟击败 | 一支由拿破仑·波拿巴指挥的法国军队被第七次联盟的两支军队击败：由威灵顿公爵指挥的以英国为首的盟军和由盖布哈德·勒贝雷希特·冯·布吕歇尔（瓦尔施塔特亲王）指挥的普鲁士军队。
    | 谁指挥了滑铁卢战役中的英国军队？ | 威灵顿公爵 |'
- en: '| Canine character | Astro is a canine character on the Hanna-Barbera cartoon,
    The Jetsons. | What was the dog’s name on the jetsons? | Astro |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 犬类角色 | Astro 是 Hanna-Barbera 动画片《杰森一家》中的犬类角色。 | 这只狗在《杰森一家》中的名字是什么？ | Astro
    |'
- en: 'Table 10: Few-shot examples for extractive QA used to benchmark models and
    generate synthetic answers from teachers.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：用于基准测试模型和从教师处生成合成答案的少量示例提取 QA。
- en: '| Context | Summary |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 背景 | 摘要 |'
- en: '| #Person1#: John, shall we go to Sun Store? I have decided to buy that Murrberry
    handbag. Anyway, I’m not carrying this one to Mary’s wedding. #Person2#: But,
    Jane, why not rent one with Handbag Hire? Instead of $990,pay$ 50, and you have
    it for a whole week. #Person1#: Sounds great, but I never knew I can rent a handbag.
    #Person2#: Handbag Hire is a new business. It was founded two months ago. Its
    collection covers many designer handbags. #Person1#: So… for the price of one
    Murrberry, I can use a different bag each week for twenty weeks? #Person2#: Absolutely.
    And if you like one of them, you can choose to buy it at a discounted rate. Of
    course, the price varies by age and condition. For example, a $ 1500 Murrberry
    bag can sell for just $750. #Person1#: Great, but how do I rent? By telephone?
    Or in person? #Person2#: Either. And more conveniently, it accepts online orders.
    #Person1#: I’ll do it online now. I still have one more question. Mary’s wedding
    is next Saturday. There are only five days left. Do I have enough time? #Person2#:
    Don’t worry. It promises that customers receive their orders by post within two
    days. Three more days to go. #Person1#: Oh, I’d better order one right now. |
    Jane wants to buy that Murrberry handbag to carry to Mary’s wedding, but John
    suggests renting one with Handbag Hire and tells her about the service in detail.
    Jane is pleased to have a try. |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| #Person1#: 约翰，我们去阳光商店吧？我决定买那个 Murrberry 手袋。反正，我不会带着这个去玛丽的婚礼。 #Person2#: 但，简，为什么不通过
    Handbag Hire 租一个呢？只需 $50，就能用一整周，而不是 $990。 #Person1#: 听起来不错，但我从未知道可以租手袋。 #Person2#:
    Handbag Hire 是一家新兴企业。它成立于两个月前。其藏品涵盖了许多设计师手袋。 #Person1#: 所以……以一个 Murrberry 的价格，我可以每周使用不同的包包，持续二十周？
    #Person2#: 完全正确。如果你喜欢其中一个，你可以选择以折扣价购买。当然，价格会根据年龄和状况有所不同。例如，一个 $1500 的 Murrberry
    包包可以以仅 $750 的价格出售。 #Person1#: 很好，但我怎么租？通过电话？还是亲自去？ #Person2#: 都可以。更方便的是，它还接受在线订单。
    #Person1#: 我现在就在线下单。我还有一个问题。玛丽的婚礼在下周六，还有五天时间。我有足够的时间吗？ #Person2#: 不用担心。它承诺客户在两天内通过邮寄收到订单。还有三天。
    #Person1#: 哦，我最好现在就订购一个。 | 简想买那个 Murrberry 手袋去玛丽的婚礼，但约翰建议她通过 Handbag Hire 租一个，并详细告诉她这个服务。简很高兴尝试一下。
    |'
- en: '| #Person1#: The summers are so great here! Not hot at all. I love the cooling
    breezes, the clear air, all the greenery. #Person2#: This really has been a wonderful
    holiday for us. Shall we take a walk around the pond or into those woods for a
    while? #Person1#: Let’s do both! Are we in a rush or anything? #Person2#: No,
    not really. I had thought we’d stay in Hamburg tonight, but we can’t unless we
    rush it. Let’s stay in Bremen instead. Tomorrow we can have lunch in Hamburg,
    then check into a hostel in Copenhagen and have dinner there. #Person1#: Sounds
    fine to me. Whatever, let’s enjoy this pond first. #Person2#: Sure. We can walk
    around to that path that leads into the woods there. Hey, look! There are some
    wild ducks over there in the reeds. #Person1#: I see them! Wow! How do you know
    they’re wild? #Person2#: I used to go hunting with my uncle, that’s how. #Person1#:
    They’re neat. Now let’s take that path into the woods and see what we can see…
    | #Person1# and #Person2# are enjoying a pond. #Person1# and #Person2# had planned
    to stay in Hamburg tonight, but they decide to stay in Bremen since they are not
    in a rush. |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| #Person1#: 这里的夏天真好！一点也不热。我喜欢这凉爽的微风、清新的空气和绿意盎然的景色。 #Person2#: 这真是我们的一次美妙假期。我们去池塘周围散步，或者去那片树林里走一会儿如何？
    #Person1#: 都去吧！我们赶时间吗？ #Person2#: 不，没什么急事。我原本想今晚留在汉堡，但如果不赶时间的话，我们可以留在不来梅。明天我们可以在汉堡吃午餐，然后去哥本哈根的旅舍入住，并在那儿吃晚餐。
    #Person1#: 对我来说听起来不错。无论如何，我们先享受一下这个池塘吧。 #Person2#: 当然。我们可以走到那条通向树林的小径。嘿，看！那里的芦苇丛中有些野鸭。
    #Person1#: 我看到了！哇！你怎么知道它们是野生的？ #Person2#: 我以前和叔叔一起去打猎，就是这样。 #Person1#: 真有趣。现在我们走那条小径进树林看看吧……
    | #Person1#和#Person2#正在享受池塘的美景。#Person1#和#Person2#原本计划今晚留在汉堡，但由于不着急，他们决定留在不来梅。'
- en: '| #Person1#: Well, Rebecca, is there anything else you need to know for now?
    #Person2#: I don’t think so, Mr. Parsons. I think you have covered all the main
    points for me. #Person1#: Okay well listen, here is my business card with my mobile
    number. If any other questions spring to mind don’t hesitate to contact me. Of
    course, you can also call Miss Childs too. #Person2#: Great. Rmm, when can I expect
    to hear from you? #Person1#: Well, we are finishing the shortlist interviews tomorrow,
    so we will certainly have a decision made by early next week. Miss Childs will
    call you to discuss more on Monday or Tuesday. How does that sound? #Person2#:
    That sounds perfect. Thank you very much for taking the time to speak to me Mr.
    Parsons. #Person1#: The pleasure’s all mine, Rebecca. #Person2#: I hope to hear
    from you very soon. #Person1#: Absolutely. Thanks for coming Rebecca. Goodbye.
    | Mr. Parsons gives Rebecca his business card after the interview and tells Rebecca
    the decision will be made by early next week and Miss Childs will contact Rebecca.
    |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| #Person1#: 好了，Rebecca，现在还有其他需要了解的事情吗？ #Person2#: 我认为没有了，Parsons先生。我觉得您已经涵盖了所有主要点。
    #Person1#: 好的，听着，这是我的名片，上面有我的手机号码。如果有其他问题，请随时联系我。当然，您也可以给Miss Childs打电话。 #Person2#:
    太好了。那么，我什么时候能收到您的消息？ #Person1#: 嗯，我们明天完成了初选面试，所以下周早些时候肯定会做出决定。Miss Childs会在周一或周二给您打电话讨论更多内容。听起来怎么样？
    #Person2#: 听起来很完美。非常感谢您抽时间与我交谈，Parsons先生。 #Person1#: 一切都是我的荣幸，Rebecca。 #Person2#:
    希望很快能收到您的消息。 #Person1#: 当然。谢谢你来，Rebecca。再见。 | Parsons先生在面试后把名片给Rebecca，并告诉Rebecca决定将在下周初做出，Miss
    Childs会联系Rebecca。'
- en: 'Table 11: Few-shot examples for summary used to benchmark models and generate
    synthetic summary from teachers.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 表11：用于基准测试模型和从教师生成合成摘要的少量示例。
- en: '| Title | Context | Question | Answer |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 背景 | 问题 | 答案 |'
- en: '| The Wee Bannock | So, she jumped up with her lint and her lint cards, and
    the tailor jumped up with his great shears, and one apprentice grasped the line
    measure, while another took up the saucer full of pins; and they all tried to
    catch the wee bannock. But it dodged them round and round the fire, and at last
    it got safely out of the door and ran down the road, with one of the apprentices
    after it, who tried to snip it in two with his shears. It ran too quickly for
    him, however, and at last he stopped and went back to the house, while the wee
    bannock ran on until it came to a tiny cottage by the roadside. it trundled in
    at the door, and there was a weaver sitting at his loom, with his wife beside
    him, winding a clue of yarn. | How did the bannock escape from the tailor’s wife
    and the three tailors? | Dodged them round and round the fire, and at last it
    got safely out of the door and ran down the road. |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 小饼干 | 她于是拿起她的棉布和棉布卡片，裁缝则拿起了他的大剪刀，一个学徒抓住了量线尺，而另一个学徒则拿起了满是别针的小碟子；他们都试图抓住小饼干。但小饼干绕着火堆躲闪，最后安全地跑出了门，沿着道路跑去，一个学徒在后面追赶，试图用剪刀把它剪成两半。然而它跑得太快了，学徒最后停下来回到房子里，而小饼干则继续跑直到来到路边的一座小屋子。它滚进了门里，那里有一个织布工坐在织布机前，他的妻子在旁边绕线团。
    | 小饼干是如何逃脱裁缝的妻子和三位裁缝的追赶的？ | 绕着火堆躲闪，最后安全地跑出了门，沿着道路跑去。 |'
- en: '| Princess Glass Mountain | Then he took the prince by the hand, led him deep
    down in the earth into his cave, and there on the wall hung a suit of armor altogether
    forged of the clearest silver, and so bright that it shone afar. Right beside
    it stood a snow-white steed, saddled and bridled, pawing the earth with his silver
    hoofs, and champing his bit till the foam dropped to the ground. The wild man
    said: ’now get quickly into your armor, ride out and try your luck! in the meantime
    I will tend your oxen.’ The prince did not wait to be told a second time; but
    put on his helmet and armor in all haste, securely buckled on his spurs, hung
    his sword at his side, and felt as light in his silver armor as a bird in the
    air. Then he leaped into the saddle so that every clasp and buckle rang, laid
    his reins on the neck of his steed, and rode hastily toward the glass mountain.
    | What was the suit of armor given by the wild man forged from? | The clearest
    silver. |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 公主玻璃山 | 然后他拉着王子的手，将他带到地底深处的洞穴里，在墙上挂着一副完全由最清澈的银子锻造的盔甲，闪闪发光。旁边站着一匹雪白的骏马，已经备好了鞍和缰绳，用银色的蹄子拍打地面，咬住缰绳直到泡沫滴落在地上。野人说：‘现在赶紧穿上盔甲，骑马出去碰碰运气吧！与此同时，我会照料你的牛。’王子没有等第二次命令；他迅速穿上头盔和盔甲，牢牢地扣上马刺，把剑挂在腰间，感觉自己在银色盔甲中像在空中飞翔的鸟儿一样轻盈。然后他跃上马鞍，每一个扣子和带子都发出了响声，将缰绳搭在马脖子上，急忙骑向玻璃山。
    | 野人给的盔甲是用什么材料锻造的？ | 最清澈的银子。 |'
- en: '| Money Box | He knew very well that he had enough inside him to buy up all
    the other toys, and this gave him a very good opinion of his own value. The rest
    thought of this fact also, although they did not express it, for there were so
    many other things to talk about. A large doll, still handsome, though rather old,
    for her neck had been mended, lay inside one of the drawers which was partly open.
    She called out to the others, ’let us have a game at being men and women, that
    is something worth playing at.’ | Why didn’t the other toys talk about how valuable
    the pig was? | There were so many other things to talk about. |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 钱箱 | 他非常清楚自己拥有足够的能力去买下所有其他玩具，这使得他对自己的价值有了很高的评价。其他人也考虑到了这一点，尽管他们没有表达出来，因为有太多其他的事情需要谈论。一只大娃娃，虽然有些年纪，但依然英俊，因为她的脖子已经修补过了，就躺在一个半开着的抽屉里。她对其他玩具说：‘我们来玩扮演男女的游戏吧，这可是值得玩的。’
    | 为什么其他玩具没有讨论小猪的价值？ | 有太多其他的事情需要讨论。 |'
- en: '| A Legend of Confucius | When confucius came to the earth, the kilin, that
    strange beast which is the prince of all four-footed animals, and only appears
    when there is a great man on earth, sought the child and spat out a jade whereon
    was written: ’son of the watercrystal you are destined to become an uncrowned
    king!’ and confucius grew up, studied diligently, learned wisdom and came to be
    a saint. He did much good on earth, and ever since his death has been reverenced
    as the greatest of teachers and masters. He had foreknowledge of many things and
    even after he had died, he gave evidence of this. | Why was confucius’s death
    reverenced as the greatest of teachers and masters? | He did much good on earth.
    |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 孔子的传说 | 当孔子来到地球时，那只四足动物之王、只在地球上有伟人出现时才会显现的奇怪野兽麒麟，寻找了这个孩子，并吐出一块玉，上面写着：“水晶之子的你注定成为一位无冕之王！”孔子长大后，刻苦学习，获得了智慧，成为了一位圣人。他在地球上做了很多善事，自他的死亡以来，他被尊敬为最伟大的教师和大师。他对许多事情有预知能力，即使在他死后，他也展示了这一点。
    | 为什么孔子的死被尊敬为最伟大的教师和大师？ | 他在地球上做了很多善事。 |'
- en: '| Naughty Boy | ’Oh, let me in! Let me in! I’m cold, and I’m so wet!’ Exclaimed
    suddenly a child that stood crying at the door and knocking for admittance, while
    the rain poured down, and the wind made all the windows rattle. ’Poor thing!’
    said the old poet, as he went to open the door. there stood a little boy, quite
    naked, and the water ran down from his long golden hair. He trembled with cold,
    and had he not come into a warm room he would most certainly have perished in
    the frightful tempest. | Why did the boy ask to come inside? | He was cold and
    wet. |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 顽皮的小男孩 | “哦，让我进去！让我进去！我很冷，我全身都湿了！”一个站在门外哭泣并敲门要求进入的孩子突然喊道，雨水倾盆而下，风使得所有窗户都在颤动。“可怜的孩子！”老诗人说着走去开门。门外站着一个小男孩，全身赤裸，水从他长长的金色头发上流下来。他因寒冷而发抖，如果他没有进到温暖的房间里，他肯定会在可怕的风暴中死去。
    | 为什么那个小男孩请求进屋？ | 他很冷而且很湿。 |'
- en: 'Table 12: Few-shot examples for generative QA used to benchmark models and
    generate synthetic answers from teachers.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：用于基准测试模型并从教师那里生成合成答案的生成性 QA 的少量示例。
- en: '| Context | Question | Answer |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| 上下文 | 问题 | 答案 |'
- en: '| Injury severity score (ISS), Glasgow coma score (GCS), and revised trauma
    score (RTS) are the most frequently used methods to evaluate the severity of injury
    in blunt trauma patients. ISS is too complicated to assess easily and GCS and
    RTS are easy to assess but somewhat subjective. White blood cell count (WBC) is
    an easy, quick and objective test. This study was performed to evaluate the significance
    of the WBC count at presentation in the blunt trauma patients. 713 blunt trauma
    patients, who were admitted to the Uludag University Medical Center Emergency
    Department between 01.04.2000-31.12.2000, were retrospectively evaluated in terms
    of ISS, GCS, RTS and white blood cell count at presentation. Statistical analysis
    revealed that WBC was correlated positively with ISS, but negatively with GCS
    and RTS. | Does the leukocyte count correlate with the severity of injury | The
    leukocyte count at presentation can be used as an adjunct in the evaluation of
    the severity of injury in blunt trauma patients. |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 伤情严重程度评分（ISS）、格拉斯哥昏迷评分（GCS）和修订创伤评分（RTS）是评估钝性创伤患者伤情严重程度最常用的方法。ISS 评估起来过于复杂，GCS
    和 RTS 评估起来相对简单但有些主观。白细胞计数（WBC）是一种简单、快捷和客观的测试。本研究旨在评估钝性创伤患者在就诊时 WBC 计数的重要性。对 713
    名在 2000 年 4 月 1 日至 2000 年 12 月 31 日期间入院至乌鲁达大学医疗中心急诊科的钝性创伤患者进行回顾性评估，分析了 ISS、GCS、RTS
    和就诊时的白细胞计数。统计分析显示，WBC 与 ISS 正相关，但与 GCS 和 RTS 负相关。 | 白细胞计数是否与伤情严重程度相关 | 就诊时的白细胞计数可作为钝性创伤患者伤情严重程度评估的辅助指标。
    |'
- en: '| The aim of this study was to assess the diagnostic value of articular sounds,
    standardized clinical examination, and standardized articular ultrasound in the
    detection of internal derangements of the temporomandibular joint. Forty patients
    and 20 asymptomatic volunteers underwent a standardized interview, physical examination,
    and static and dynamic articular ultrasound. Sensitivity, specificity, and predictive
    values were calculated using magnetic resonance as the reference test. A total
    of 120 temporomandibular joints were examined. Based on our findings, the presence
    of articular sounds and physical signs are often insufficient to detect disk displacement.
    Imaging by static and dynamic high-resolution ultrasound demonstrates considerably
    lower sensitivity when compared with magnetic resonance. Some of the technical
    difficulties resulted from a limited access because of the presence of surrounding
    bone structures. | Internal derangement of the temporomandibular joint: is there
    still a place for ultrasound? | The present study does not support the recommendation
    of ultrasound as a conclusive diagnostic tool for internal derangements of the
    temporomandibular joint. |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| 本研究的目的是评估关节声音、标准化临床检查和标准化关节超声在检测颞下颌关节内源性损伤中的诊断价值。40 名患者和 20 名无症状志愿者接受了标准化的面谈、体格检查以及静态和动态关节超声检查。使用磁共振作为参考测试计算了敏感性、特异性和预测值。共检查了
    120 个颞下颌关节。根据我们的发现，关节声音和体征的存在通常不足以检测盘位移。与磁共振相比，静态和动态高分辨率超声成像的敏感性明显较低。部分技术难点源于周围骨结构导致的有限可及性。
    | 颞下颌关节的内源性损伤：超声是否仍有其位置？ | 本研究不支持将超声作为颞下颌关节内源性损伤的最终诊断工具的推荐。 |'
- en: '| Figures from the British Defence Dental Services reveal that serving personnel
    in the British Army have a persistently lower level of dental fitness than those
    in the Royal Navy or the Royal Air Force. No research had been undertaken to ascertain
    if this reflects the oral health of recruits joining each Service. This study
    aimed to pilot a process for collecting dental and sociodemographic data from
    new recruits to each Service and examine the null hypothesis that no differences
    in dental health existed. Diagnostic criteria were developed, a sample size calculated
    and data collected at the initial training establishments of each Service. Data
    for 432 participants were entered into the analysis. Recruits in the Army sample
    had a significantly greater prevalence of dental decay and greater treatment resource
    need than either of the other two Services. Army recruits had a mean number of
    2.59 (2.08, 3.09) decayed teeth per recruit, compared to 1.93 (1.49, 2.39 p<0.01)
    in Royal Navy recruits and 1.26 (0.98, 1.53 p<0.001) in Royal Air Force recruits.
    Among Army recruits 62.7% were from the two most deprived quintiles of the Index
    of Multiple Deprivation compared to 42.5% of Royal Naval recruits and 36.6% of
    Royal Air Force recruits. | Is there a differential in the dental health of new
    recruits to the British Armed Forces? | A significant difference in dental health
    between recruits to each Service does exist and is a likely to be a reflection
    of the sociodemographic background from which they are drawn. |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| 英国防御牙科服务的数据表明，英国陆军现役人员的牙科健康水平持续低于皇家海军或皇家空军人员。之前没有研究确定这是否反映了每个军种新兵的口腔健康。此研究旨在试点一个从每个军种的新兵中收集牙科和社会人口数据的过程，并检验“牙科健康没有差异”的零假设。制定了诊断标准，计算了样本量，并在每个军种的初始培训机构收集了数据。432
    名参与者的数据被录入分析。陆军样本的蛀牙发生率和治疗资源需求明显高于其他两个军种。陆军新兵的平均蛀牙数为 2.59（2.08，3.09），而皇家海军新兵为
    1.93（1.49，2.39 p<0.01），皇家空军新兵为 1.26（0.98，1.53 p<0.001）。在陆军新兵中，62.7%来自多重贫困指数的前两个最贫困五分位，而皇家海军新兵为
    42.5%，皇家空军新兵为 36.6%。 | 英国武装部队新兵的牙科健康是否存在差异？ | 各军种新兵的牙科健康确实存在显著差异，这可能反映了他们来源的社会人口背景。'
- en: 'Table 13: Few-shot examples for generative QA used to benchmark models and
    generate synthetic answers from teachers for medical topic.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：用于基准测试模型和生成教师为医学主题提供的合成答案的少样本生成 QA 示例。
- en: 11 Appendix - Training Information
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11 附录 - 培训信息
- en: 'During training, all distillation processes were performed over 5 epochs with
    a batch size of $8$ GPU hours were used (i.e. consumption for the entire project
    in tonnes of CO[2]: 0.268).'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，所有的蒸馏过程都在 5 个周期内完成，使用了每批次 $8$ 个 GPU 小时（即整个项目的 CO[2] 消耗量为：0.268 吨）。
