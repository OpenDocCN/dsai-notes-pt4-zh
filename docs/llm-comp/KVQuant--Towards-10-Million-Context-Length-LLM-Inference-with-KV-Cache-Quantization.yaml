- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:50:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:50:25'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'KVQuant: 面向1000万上下文长度LLM推理的KV缓存量化'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.18079](https://ar5iv.labs.arxiv.org/html/2401.18079)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.18079](https://ar5iv.labs.arxiv.org/html/2401.18079)
- en: \useunderColeman Hooper [chooper@berkeley.edu](mailto:chooper@berkeley.edu)
    UC Berkeley  ,  Sehoon Kim [sehoonkim@berkeley.edu](mailto:sehoonkim@berkeley.edu)
    UC Berkeley ,  Hiva Mohammadzadeh [hiva@berkeley.edu](mailto:hiva@berkeley.edu)
    UC Berkeley ,  Michael W. Mahoney [mmahoney@stat.berkeley.edu](mailto:mmahoney@stat.berkeley.edu)
    ICSI, LBNL, UC Berkeley  ,  Yakun Sophia Shao [ysshao@berkeley.edu](mailto:ysshao@berkeley.edu)
    UC Berkeley  ,  Kurt Keutzer [keutzer@berkeley.edu](mailto:keutzer@berkeley.edu)
    UC Berkeley   and  Amir Gholami [amirgh@berkeley.edu](mailto:amirgh@berkeley.edu)
    ICSI, UC Berkeley
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \useunderColeman Hooper [chooper@berkeley.edu](mailto:chooper@berkeley.edu)
    UC Berkeley  ,  Sehoon Kim [sehoonkim@berkeley.edu](mailto:sehoonkim@berkeley.edu)
    UC Berkeley ,  Hiva Mohammadzadeh [hiva@berkeley.edu](mailto:hiva@berkeley.edu)
    UC Berkeley ,  Michael W. Mahoney [mmahoney@stat.berkeley.edu](mailto:mmahoney@stat.berkeley.edu)
    ICSI, LBNL, UC Berkeley  ,  Yakun Sophia Shao [ysshao@berkeley.edu](mailto:ysshao@berkeley.edu)
    UC Berkeley  ,  Kurt Keutzer [keutzer@berkeley.edu](mailto:keutzer@berkeley.edu)
    UC Berkeley   和  Amir Gholami [amirgh@berkeley.edu](mailto:amirgh@berkeley.edu)
    ICSI, UC Berkeley
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'LLMs are seeing growing use for applications which require large context windows,
    and with these large context windows KV cache activations surface as the dominant
    contributor to memory consumption during inference. Quantization is a promising
    approach for compressing KV cache activations; however, existing solutions fail
    to represent activations accurately in sub-4-bit precision. Our work, KVQuant,
    facilitates low precision KV cache quantization by incorporating several novel
    methods: (i) *Per-Channel Key Quantization*, where we adjust the dimension along
    which we quantize the Key activations to better match the distribution; (ii) *Pre-RoPE
    Key Quantization*, where we quantize Key activations before the rotary positional
    embedding to mitigate its impact on quantization; (iii) *Non-Uniform KV Cache
    Quantization*, where we derive per-layer sensitivity-weighted non-uniform datatypes
    that better represent the distributions; (iv) *Per-Vector Dense-and-Sparse Quantization*,
    where we isolate outliers separately for each vector to minimize skews in quantization
    ranges; and (v) *Q-Norm*, where we normalize quantization centroids in order to
    mitigate distribution shift, providing additional benefits for 2-bit quantization.
    By applying our method to the LLaMA, LLaMA-2, and Mistral models, we achieve $<0.1$
    speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B
    model. The code is available at [https://github.com/SqueezeAILab/KVQuant/](https://github.com/SqueezeAILab/KVQuant/).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大规模语言模型）在需要大范围上下文窗口的应用中越来越受到青睐，而在这些大范围上下文窗口中，KV缓存激活成为推理过程中内存消耗的主要因素。量化是一种有前景的方法来压缩KV缓存激活；然而，现有解决方案无法在4位以下精度中准确表示激活。我们的工作KVQuant，通过整合几种新颖的方法来促进低精度KV缓存量化：（i）*每通道键量化*，我们调整量化键激活的维度以更好地匹配分布；（ii）*预RoPE键量化*，我们在旋转位置嵌入之前量化键激活，以减轻其对量化的影响；（iii）*非均匀KV缓存量化*，我们推导出每层敏感度加权的非均匀数据类型，以更好地表示分布；（iv）*每向量密集和稀疏量化*，我们单独隔离异常值，以最小化量化范围中的偏差；（v）*Q-Norm*，我们对量化中心进行归一化，以减轻分布漂移，为2位量化提供额外的好处。通过将我们的方法应用于LLaMA、LLaMA-2和Mistral模型，我们实现了对LLaMA-7B模型相较于基线fp16矩阵-向量乘法的$<0.1$加速。代码可在[https://github.com/SqueezeAILab/KVQuant/](https://github.com/SqueezeAILab/KVQuant/)获取。
- en: 1\. Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: Large language models (LLMs) have revolutionized many natural language processing
    (NLP) tasks. In order to improve the capabilities of LLMs, there is significant
    interest in increasing the context lengths of LLMs. Longer context lengths enable
    new applications, including long document summarization, retrieval for answering
    questions about long documents, extended multi-turn applications ([chen2023longlora,](#bib.bib4)
    ), and code analysis. To support this pull from applications, there have been
    significant recent advances in long-context length models in industry ([anthropic,](#bib.bib1)
    ; [openai,](#bib.bib27) ), as well as in academia ([chen2023longlora,](#bib.bib4)
    ).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经彻底改变了许多自然语言处理（NLP）任务。为了提高LLM的能力，人们对增加LLM的上下文长度非常感兴趣。更长的上下文长度启用了新的应用，包括长文档总结、检索以回答关于长文档的问题、扩展的多轮应用（[chen2023longlora,](#bib.bib4)）以及代码分析。为了支持这一应用需求，业界（[anthropic,](#bib.bib1)；[openai,](#bib.bib27)）和学术界（[chen2023longlora,](#bib.bib4)）在长上下文长度模型方面都取得了显著进展。
- en: 'Given the importance of LLM workloads, there is strong motivation to improve
    their inference efficiency. LLM inference with large context lengths can be incredibly
    resource-intensive; serving LLMs requires high-end GPUs, and the largest LLMs
    require costly multi-GPU inference setups. When analyzing the computational nature
    of generative inference with LLMs, it becomes quickly apparent that, for relatively
    small batch sizes, the computation is memory bound ([kim2023squeezellm,](#bib.bib18)
    ). With the growing divergence between computational speeds and memory speeds,
    this problem is only going to get worse over time ([gholami2020ai_and_memory_wall,](#bib.bib13)
    ). This makes reducing the memory bottleneck preeminently important. Further analysis
    shows that the memory bottleneck is strongly related to context size. For short
    sequence lengths, the dominant contributor to memory consumption is the weight
    matrices, and therefore the optimal strategy is to minimize the model size in
    order to reduce memory consumption as well as bandwidth requirements ([kim2023full,](#bib.bib19)
    ; [kim2023squeezellm,](#bib.bib18) ). However, for long sequence lengths, the
    main bottleneck is the memory requirements for caching Key and Value (KV) activations
    throughout inference. In particular, the size of the KV cache can become the dominant
    contributor to memory footprint, even for a 32K context limit (see Table [1](#S1.T1
    "Table 1 ‣ 1\. Introduction ‣ KVQuant: Towards 10 Million Context Length LLM Inference
    with KV Cache Quantization")), making it challenging to perform long context length
    inference. This challenge is further exacerbated when one considers batched inference.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '鉴于大型语言模型（LLM）工作负载的重要性，提升其推理效率的动力非常强烈。大型上下文长度的LLM推理可能非常消耗资源；提供LLM服务需要高端GPU，而最大的LLM需要昂贵的多GPU推理配置。在分析LLM生成推理的计算特性时，很快会发现，对于相对较小的批量大小，计算受限于内存（[kim2023squeezellm,](#bib.bib18)）。随着计算速度和内存速度之间差距的不断扩大，这个问题只会随着时间的推移变得更严重（[gholami2020ai_and_memory_wall,](#bib.bib13)）。这使得减少内存瓶颈变得尤为重要。进一步分析显示，内存瓶颈与上下文大小有很强的关系。对于短序列长度，内存消耗的主要来源是权重矩阵，因此最佳策略是尽量减少模型大小以降低内存消耗以及带宽需求（[kim2023full,](#bib.bib19)；[kim2023squeezellm,](#bib.bib18)）。然而，对于长序列长度，主要瓶颈是推理过程中缓存键值（KV）激活所需的内存。特别是，KV缓存的大小可能成为内存占用的主要因素，即使在32K上下文限制下（见表格[1](#S1.T1
    "Table 1 ‣ 1\. Introduction ‣ KVQuant: Towards 10 Million Context Length LLM Inference
    with KV Cache Quantization")），这使得长上下文长度推理变得具有挑战性。考虑到批量推理时，这一挑战进一步加剧。'
- en: 'It is therefore crucial to develop methods for compressing the KV cache to
    enable efficient long-sequence length inference. Existing approaches lead to unacceptable
    accuracy degradation due to the outlier structures in KV cache activations as
    well as suboptimal bit allocation with existing uniform and non-uniform approaches.
    In this work, we perform an extensive analysis of KV cache activations in recent
    LLMs, revealing patterns which can be exploited to enable ultra-low precision
    quantization with minimal accuracy loss. In particular, we make the following
    contributions (summarized in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣
    KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization")):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，开发压缩KV缓存的方法以实现高效的长序列推理至关重要。现有方法由于KV缓存激活中的异常结构以及现有均匀和非均匀方法中的亚优化比特分配，导致不可接受的准确性下降。在这项工作中，我们对最近的LLMs中的KV缓存激活进行了广泛分析，揭示了可以利用的模式，从而实现超低精度量化，且准确性损失最小。特别是，我们做出了以下贡献（总结在 图 [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ KVQuant: Towards 10 Million Context Length LLM
    Inference with KV Cache Quantization")）：'
- en: •
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We find that the Key matrices exhibit structured outliers in specific channels
    before applying RoPE. However, the outlier channel magnitudes become less consistent
    after applying RoPE, posing a distinct challenge for low precision quantization.
    Based on these observations, we use per-channel quantization for Keys, and we
    quantize Keys before RoPE is applied (see Section [3.1](#S3.SS1 "3.1\. Per-Channel
    Key Quantization ‣ 3\. Method ‣ KVQuant: Towards 10 Million Context Length LLM
    Inference with KV Cache Quantization") and Section [3.2](#S3.SS2 "3.2\. Pre-RoPE
    Key Quantization ‣ 3\. Method ‣ KVQuant: Towards 10 Million Context Length LLM
    Inference with KV Cache Quantization")).'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们发现，Key矩阵在应用RoPE之前，在特定通道中表现出结构化异常。然而，应用RoPE之后，异常通道的幅度变得不那么一致，这对低精度量化提出了独特的挑战。基于这些观察结果，我们对Keys进行逐通道量化，并在应用RoPE之前对Keys进行量化（见 第[3.1节](#S3.SS1
    "3.1\. Per-Channel Key Quantization ‣ 3\. Method ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization") 和第[3.2节](#S3.SS2 "3.2\.
    Pre-RoPE Key Quantization ‣ 3\. Method ‣ KVQuant: Towards 10 Million Context Length
    LLM Inference with KV Cache Quantization")）。'
- en: •
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We find that existing uniform and non-uniform quantization methods result in
    sub-optimal quantization signpost placement. Instead, we propose a Non-Uniform
    Quantization (NUQ) method which considers sensitivity and not just magnitude when
    quantizing activations. We show that one can apply sensitivity-weighted non-uniform
    quantization offline on a calibration set to derive accurate datatypes for KV
    cache quantization (see Section [3.3](#S3.SS3 "3.3\. nuqX: An X-Bit Per-Layer
    Sensitivity-Weighted Non-Uniform Datatype ‣ 3\. Method ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization")).'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们发现，现有的均匀和非均匀量化方法导致了亚优化的量化标志位置。相反，我们提出了一种非均匀量化（NUQ）方法，该方法在量化激活时考虑敏感性而不仅仅是幅度。我们展示了如何在校准集上应用敏感性加权的非均匀量化，以为KV缓存量化导出准确的数据类型（见 第[3.3节](#S3.SS3
    "3.3\. nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype ‣ 3\.
    Method ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache
    Quantization")）。'
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Even with the above, we find that outlier values in cached KV activations can
    significantly degrade quantization resolution. Unlike for weights, it is non-trivial
    to extract outlier values from activations, given the dynamic nature of activations.
    However, we find that we can efficiently and accurately identify and compress
    outlier values in order to store them compactly in a separate sparse representation.
    We also find that per-vector outlier detection outperforms per-matrix outlier
    detection with no additional memory overhead. With this method, we can attain
    under 0.1 perplexity degradation for 3-bit KV cache quantization on both Wikitext-2
    and C4 by only removing 1% of outliers, thereby facilitating accurate inference
    with 4.8$\times$ longer context length.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 即使有上述方法，我们发现缓存的KV激活中的异常值仍然可能显著降低量化分辨率。与权重不同，考虑到激活的动态特性，从激活中提取异常值并非易事。然而，我们发现可以有效且准确地识别和压缩异常值，以便将其紧凑地存储在单独的稀疏表示中。我们还发现，逐向量异常检测优于逐矩阵异常检测，且没有额外的内存开销。通过这种方法，我们可以在Wikitext-2和C4上对3-bit
    KV缓存量化实现低于0.1的困惑度下降，仅通过移除1%的异常值，从而实现4.8$\times$更长上下文长度的准确推理。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'For ultra low-bit precision, we find that the quantized activations can deviate
    significantly from their corresponding fp16 values. To address this, we propose
    a lightweight Q-Norm layer which shifts and scales the distribution after de-quantization
    to match the mean and standard deviation of corresponding fp16 values. Interestingly,
    the Q-Norm layer can be fused with non-uniform quantization values resulting in
    no overhead during inference. This was particularly helpful for 2-bit quantization
    (see Section [3.5](#S3.SS5 "3.5\. Mitigating Distribution Shift using Q-Norm ‣
    3\. Method ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV
    Cache Quantization")).'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于超低位精度，我们发现量化后的激活值可能与其对应的fp16值有显著偏差。为了解决这个问题，我们提出了一种轻量级的Q-Norm层，它在去量化后对分布进行平移和缩放，以匹配对应的fp16值的均值和标准差。有趣的是，Q-Norm层可以与非均匀量化值融合，从而在推理过程中不会增加额外开销。这对2位量化尤其有帮助（见第[3.5节](#S3.SS5
    "3.5\. 使用Q-Norm减轻分布偏移 ‣ 3\. 方法 ‣ KVQuant：通过KV缓存量化实现1000万上下文长度的LLM推理")）。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We implement custom CUDA kernels to perform activation quantization efficiently
    during inference, achieving up to $\sim$ speedups for Key and Value matrix-vector
    multiplications for LLaMA-7B at 4-bit precision relative to the fp16 baseline
    (see Section [3.7](#S3.SS7 "3.7\. Kernel Implementation ‣ 3\. Method ‣ KVQuant:
    Towards 10 Million Context Length LLM Inference with KV Cache Quantization") and
    [4.2](#S4.SS2 "4.2\. Performance Analysis ‣ 4\. Results ‣ KVQuant: Towards 10
    Million Context Length LLM Inference with KV Cache Quantization")). These results
    demonstrate how our methodology allows for accurate and efficient low-bit KV cache
    quantization.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们实现了自定义CUDA内核，以在推理过程中高效地进行激活量化，实现了与fp16基线相比的$\sim$加速，用于LLaMA-7B的Key和Value矩阵-向量乘法（见第[3.7节](#S3.SS7
    "3.7\. 内核实现 ‣ 3\. 方法 ‣ KVQuant：通过KV缓存量化实现1000万上下文长度的LLM推理")和[4.2节](#S4.SS2 "4.2\.
    性能分析 ‣ 4\. 结果 ‣ KVQuant：通过KV缓存量化实现1000万上下文长度的LLM推理")）。这些结果展示了我们的方法如何实现准确且高效的低位KV缓存量化。
- en: '![Refer to caption](img/329c5291109d2300c2808540803538a1.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/329c5291109d2300c2808540803538a1.png)'
- en: 'Figure 1. Overview of the different components used in KVQuant that result
    in less than 0.1 perplexity degradation over the fp16 baseline when quantizing
    the KV cache to 3-bit precision. As shown in  Table [2](#S3.T2 "Table 2 ‣ 3.4\.
    Per-Vector Dense-and-Sparse Quantization ‣ 3\. Method ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization"), our 3-bit approach
    results in $4.8\times$ reduction in cached activation memory footprint.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. KVQuant中使用的不同组件的概述，当将KV缓存量化为3位精度时，这些组件使得与fp16基线相比，困惑度下降不到0.1。如表[2](#S3.T2
    "表 2 ‣ 3.4\. 每向量密集和稀疏量化 ‣ 3\. 方法 ‣ KVQuant：通过KV缓存量化实现1000万上下文长度的LLM推理")所示，我们的3位方法使缓存激活内存的占用减少了$4.8\times$。
- en: Table 1. Model size and activation memory size for different sequence lengths
    and batch sizes (BS) for different LLaMA models. For long sequence lengths and
    larger batch sizes, activation memory is the main bottleneck (particularly when
    weights are already quantized to low precision). At a sequence length of 128K
    with the LLaMA-7B model, the KV cache is the main bottleneck (see below left).
    Additionally, if model weights are quantized, then the KV cache is the main bottleneck
    even at a sequence length of 32K. By compressing the KV cache to 3-bit precision,
    we can enable 1M context length inference with the LLaMA-7B model on a single
    A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B
    model on an 8-GPU system.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 表1. 不同LLaMA模型在不同序列长度和批量大小（BS）下的模型大小和激活内存大小。对于较长的序列长度和较大的批量大小，激活内存是主要瓶颈（特别是在权重已经量化为低精度时）。在序列长度为128K的LLaMA-7B模型中，KV缓存是主要瓶颈（见下图左）。此外，如果模型权重已经量化，则即使在序列长度为32K时，KV缓存也是主要瓶颈。通过将KV缓存压缩到3位精度，我们可以在单个A100-80GB
    GPU上实现1M上下文长度的推理，并且我们也可以在8-GPU系统上实现10M上下文长度的推理。
- en: '![[Uncaptioned image]](img/64c53cacb3b16baa2d894441b731647f.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![未标注的图片](img/64c53cacb3b16baa2d894441b731647f.png)'
- en: '| BS | # Params | Model Size (GB) | fp16 KV Cache Size with different seq len
    (GB) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| BS | 参数数量 | 模型大小（GB） | 不同序列长度下的fp16 KV缓存大小（GB） |'
- en: '| 16-bit $\rightarrow$ 3-bit) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 16位 $\rightarrow$ 3位) |'
- en: '| 1 | 7B | 12.6 $\rightarrow$ 459.0 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 7B | 12.6 $\rightarrow$ 459.0 |'
- en: '| 13B | 24.7 $\rightarrow$ 716.7 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 13B | 24.7 $\rightarrow$ 716.7 |'
- en: '| 30B | 61.0 $\rightarrow$ 1397 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 30B | 61.0 $\rightarrow$ 1397 |'
- en: '| 65B | 122.4 $\rightarrow$ 2292 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 65B | 122.4 $\rightarrow$ 2292 |'
- en: '| 4 | 7B | 12.6 $\rightarrow$ 1836 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 7B | 12.6 $\rightarrow$ 1836 |'
- en: '| 13B | 24.7 $\rightarrow$ 2867 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 13B | 24.7 $\rightarrow$ 2867 |'
- en: '| 30B | 61.0 $\rightarrow$ 5588 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 30B | 61.0 $\rightarrow$ 5588 |'
- en: '| 65B | 122.4 $\rightarrow$ 9167 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 65B | 122.4 $\rightarrow$ 9167 |'
- en: 2\. Background
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景
- en: 2.1\. LLM Inference
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. LLM 推理
- en: When inferring a decoder-only LLM, inference proceeds in two distinct phases.
    In the prefill phase, the model takes in an input prompt, which it processes in
    parallel. During the generation phase, the model then generates the output sequence
    autoregressively, meaning that each token generation is dependent on all previously
    generated tokens. As such, for small batch sizes, the generation phase of LLM
    inference is typically memory-bandwidth bound, as the only available parallelism
    is across different sequences in a given batch.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当推理一个仅解码器的 LLM 时，推理过程分为两个不同的阶段。在预填充阶段，模型接收输入提示，并行处理。在生成阶段，模型以自回归的方式生成输出序列，这意味着每个令牌的生成依赖于所有先前生成的令牌。因此，对于小批量大小，LLM
    推理的生成阶段通常受限于内存带宽，因为唯一可用的并行性是批量中的不同序列。
- en: 'Additionally, during generation, the model needs to store intermediate Key
    and Value activations in order to condition generations on previously generated
    output tokens. Otherwise, we would need to recompute all prior Keys and Values
    at each timestep, which would be prohibitively expensive. For each prior token,
    we need to store the Keys and Values at each layer in order to use these activations
    when generating future tokens. These stored activations are referred to as the
    *Key-Value (KV) cache*. Throughout this paper, we will capitalize Key and Value
    to distinguish when we are referring to the KV cache tensors. Assuming a model
    with $n$, meaning that it grows linearly with both batch size and sequence length.
    As shown in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣ KVQuant: Towards 10
    Million Context Length LLM Inference with KV Cache Quantization"), the KV cache
    becomes the dominant contributor to memory consumption for longer sequence lengths
    and larger batch sizes. Note that since each sequence in batched inference depends
    on separate past context, there is no available batch-level parallelism when loading
    the cached Keys and Values for their respective computations in batched inference.
    KV cache loading is therefore always memory-bandwidth bound. This motivates pursuing
    methods to optimally compress the KV cache, even at the expense of a more complex
    dequantization process.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，在生成过程中，模型需要存储中间的 Key 和 Value 激活，以便根据先前生成的输出令牌来调整生成。如果不这样做，我们将需要在每个时间步重新计算所有先前的
    Keys 和 Values，这将是极其昂贵的。对于每个先前的令牌，我们需要在每一层存储 Keys 和 Values，以便在生成未来令牌时使用这些激活。这些存储的激活被称为
    *Key-Value (KV) 缓存*。在本文中，我们将大写 Key 和 Value，以区分我们何时指代 KV 缓存张量。假设一个模型具有 $n$，这意味着它在批量大小和序列长度上线性增长。如表[1](#S1.T1
    "Table 1 ‣ 1\. Introduction ‣ KVQuant: Towards 10 Million Context Length LLM Inference
    with KV Cache Quantization")所示，KV 缓存成为较长序列长度和较大批量大小的内存消耗的主要贡献者。请注意，由于批量推理中的每个序列依赖于不同的历史上下文，因此在加载缓存的
    Keys 和 Values 以进行各自计算时，没有可用的批量级并行性。因此，KV 缓存加载总是受限于内存带宽。这推动了压缩 KV 缓存的最佳方法的研究，即使这可能会使去量化过程变得更加复杂。'
- en: 2.2\. LLM Quantization
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. LLM 量化
- en: KV Cache Quantization. There have been many prior works on LLM quantization.
    Several have focused on weight-only quantization for LLMs, due to the greater
    contribution to memory consumption and runtime for fairly small sequence length
    and batch size ([lin2023awq,](#bib.bib22) ; [dettmers2023spqr,](#bib.bib7) ; [kim2023squeezellm,](#bib.bib18)
    ). There has also been work on quantizing both weights and activations (including
    KV cache) ([xiao2023smoothquant,](#bib.bib36) ; [shao2023omniquant,](#bib.bib29)
    ). However, there is still a significant perplexity degradation when quantizing
    KV cache activations to low precision; ([sheng2023flexgen,](#bib.bib30) ; [zhao2023atom,](#bib.bib38)
    ) quantized KV cache activations to 4-bits, but required fine-grained grouping
    for 4-bit quantization, while still observing some perplexity degradation, and
    ([sheng2023flexgen,](#bib.bib30) ) observed that 3-bit KV cache quantization with
    fine-grained grouping leads to unacceptable accuracy loss. Other works quantized
    KV cache activations to 4-bits but required retraining to maintain performance
    ([liu2023llmqat,](#bib.bib23) ). One concurrent work also explores low precision
    KV cache quantization in order to enable larger batch size inference by reducing
    the KV cache size ([kivi,](#bib.bib25) ). In this work, we introduce a method
    for near-lossless low-bit KV cache quantization that minimizes performance degradation
    without the need for finetuning.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**KV 缓存量化**。之前已有许多关于 LLM 量化的研究。几项研究专注于 LLM 的权重量化，因为对于相对较小的序列长度和批次大小，这对内存消耗和运行时间的贡献更大（[lin2023awq,](#bib.bib22)；[dettmers2023spqr,](#bib.bib7)；[kim2023squeezellm,](#bib.bib18)）。也有研究涉及同时量化权重和激活值（包括
    KV 缓存）（[xiao2023smoothquant,](#bib.bib36)；[shao2023omniquant,](#bib.bib29)）。然而，当将
    KV 缓存激活量化为低精度时，仍然会显著降低困惑度；（[sheng2023flexgen,](#bib.bib30)；[zhao2023atom,](#bib.bib38)）将
    KV 缓存激活量化为 4 位，但需要细粒度分组进行 4 位量化，同时仍然观察到一定的困惑度下降，并且（[sheng2023flexgen,](#bib.bib30)）观察到
    3 位 KV 缓存量化与细粒度分组会导致不可接受的准确性损失。其他工作将 KV 缓存激活量化为 4 位，但需要重新训练以保持性能（[liu2023llmqat,](#bib.bib23)）。还有一项并行工作也探索了低精度
    KV 缓存量化，以通过减少 KV 缓存大小来实现更大的批次推理（[kivi,](#bib.bib25)）。在这项工作中，我们介绍了一种接近无损的低位 KV
    缓存量化方法，该方法在无需微调的情况下最小化性能下降。'
- en: Outlier-Aware LLM Quantization. LLMs have been known to have distinct outliers
    both in weights and activations ([dettmers2022llm,](#bib.bib5) ; [dettmers2023spqr,](#bib.bib7)
    ; [kim2023squeezellm,](#bib.bib18) ). SqueezeLLM and SpQR both decompose the weight
    matrix into a sparse matrix containing a small portion of outliers and a dense
    matrix that can be accurately quantized to low precision (referred to as dense-and-sparse
    or sparse-quantized representation) ([dettmers2023spqr,](#bib.bib7) ; [kim2023squeezellm,](#bib.bib18)
    ). LLM.int8() ([dettmers2022llm,](#bib.bib5) ) handled particular outlier channels
    separately in higher precision, and SmoothQuant ([xiao2023smoothquant,](#bib.bib36)
    ) migrates quantization difficulty due to outlier channels to weights in order
    to support joint weight-activation quantization. Other works reconsidered the
    dimension along which we quantize in order to reduce quantization error (or else
    added per-channel compensation to improve quantization performance) ([bondarenko2021understanding,](#bib.bib2)
    ; [heo2023rethinking,](#bib.bib16) ; [wei2022outlier,](#bib.bib35) ; [wei2023outlier,](#bib.bib34)
    ). In this work, we demonstrate that per-channel pre-RoPE Key quantization provides
    significant accuracy benefits given the outlier structure in Keys, and that dense-and-sparse
    quantization can be efficiently applied for KV cache quantization.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**异常值感知 LLM 量化**。LLM 已知在权重和激活值中都有显著的异常值（[dettmers2022llm,](#bib.bib5)；[dettmers2023spqr,](#bib.bib7)；[kim2023squeezellm,](#bib.bib18)）。SqueezeLLM
    和 SpQR 都将权重矩阵分解为一个包含少量异常值的稀疏矩阵和一个可以准确量化为低精度的密集矩阵（称为密集-稀疏或稀疏量化表示）（[dettmers2023spqr,](#bib.bib7)；[kim2023squeezellm,](#bib.bib18)）。LLM.int8()（[dettmers2022llm,](#bib.bib5)）在更高精度下单独处理特定异常值通道，而
    SmoothQuant（[xiao2023smoothquant,](#bib.bib36)）将由于异常值通道造成的量化难度迁移到权重上，以支持联合权重-激活量化。其他工作重新考虑了我们量化的维度，以减少量化误差（或者添加每通道补偿来提高量化性能）（[bondarenko2021understanding,](#bib.bib2)；[heo2023rethinking,](#bib.bib16)；[wei2022outlier,](#bib.bib35)；[wei2023outlier,](#bib.bib34)）。在这项工作中，我们展示了每通道预RoPE
    Key 量化在给定 Key 的异常值结构时提供了显著的准确性提升，并且密集-稀疏量化可以高效地应用于 KV 缓存量化。'
- en: Non-uniform LLM Quantization. Non-uniform quantization has also been applied
    in the context of LLMs. Non-uniform quantization allows for more flexible quantization
    signpost placement relative to uniform quantization methods, enabling improved
    accuracy for the same bit precision ([kim2023squeezellm,](#bib.bib18) ; [dettmers2023qlora,](#bib.bib6)
    ). Building on the observation that model parameters tend to be approximately
    normally-distributed, prior work has proposed the NormalFloat datatype ([dettmers2023qlora,](#bib.bib6)
    ). SqueezeLLM ([kim2023squeezellm,](#bib.bib18) ) derived per-channel non-uniform
    quantization signposts using a sensitivity-weighted k-means approach. In this
    work, we show that we can derive accurate per-layer non-uniform datatypes using
    a sensitivity-weighted k-means approach with KV cache activations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 非均匀 LLM 量化。非均匀量化也已应用于 LLM 的背景中。与均匀量化方法相比，非均匀量化允许更灵活的量化标记放置，从而在相同的比特精度下实现更高的准确度（[kim2023squeezellm,](#bib.bib18)
    ; [dettmers2023qlora,](#bib.bib6)）。基于模型参数通常呈正态分布的观察，早期工作提出了 NormalFloat 数据类型（[dettmers2023qlora,](#bib.bib6)）。SqueezeLLM（[kim2023squeezellm,](#bib.bib18)）使用加权
    k-means 方法推导了每通道非均匀量化标记。在这项工作中，我们展示了如何使用基于 KV 缓存激活的加权 k-means 方法推导准确的每层非均匀数据类型。
- en: 2.3\. KV Cache Compression
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. KV 缓存压缩
- en: There have also been several prior works on compressing the KV cache. Some of
    these methods aim to only store important tokens in the KV cache and to evict
    less important tokens, thereby maintaining low memory usage ([liu2023scissorhands,](#bib.bib24)
    ; [zhang2023h,](#bib.bib37) ; [ge2023model,](#bib.bib12) ). Other methods aim
    to only retrieve a subset of tokens at each step to achieve memory bandwidth savings
    ([ribar2023sparq,](#bib.bib28) ). In this work, we explore KV cache quantization
    as an orthogonal direction for compressing the KV cache in order to enable long
    context inference.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以前也有一些关于压缩 KV 缓存的工作。这些方法中的一些旨在仅存储 KV 缓存中的重要标记，并逐出不重要的标记，从而保持低内存使用（[liu2023scissorhands,](#bib.bib24)
    ; [zhang2023h,](#bib.bib37) ; [ge2023model,](#bib.bib12)）。其他方法则旨在每一步仅检索部分标记，以实现内存带宽节省（[ribar2023sparq,](#bib.bib28)）。在这项工作中，我们探讨了
    KV 缓存量化作为压缩 KV 缓存的一个正交方向，以实现长上下文推理。
- en: '![Refer to caption](img/cbdfa747fc4a502c601a752d74ec3cd3.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/cbdfa747fc4a502c601a752d74ec3cd3.png)'
- en: 'Figure 2. Example distributions of the activation values for Keys pre-RoPE,
    Keys post-RoPE, and Values for LLaMA-7B on a sample with 2K sequence length from
    Wikitext-2. We observe several patterns: (i) Keys pre-RoPE exhibit clear outliers
    in specific channels across different tokens; (ii) after applying RoPE, the distribution
    becomes less structured and there are less consistent magnitudes for outlier channels
    (this is expected, as RoPE applies a rotation operation between pairs of channels);
    and (iii) Values exhibit no fixed outlier pattern with outlier values across channels
    and tokens.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. LLaMA-7B 上 Keys pre-RoPE、Keys post-RoPE 和 Values 的激活值示例分布，样本序列长度为 2K，来源于
    Wikitext-2。我们观察到几个模式：（i）Keys pre-RoPE 在不同标记的特定通道中表现出明显的异常值；（ii）应用 RoPE 后，分布变得不那么结构化，异常通道的幅度也更不一致（这是预期的，因为
    RoPE 在通道对之间应用了旋转操作）；（iii）Values 在通道和标记之间没有固定的异常模式。
- en: 3\. Method
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 方法
- en: 3.1\. Per-Channel Key Quantization
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 每通道 Key 量化
- en: 'To inform our approach, we first performed a detailed analysis to understand
    the KV cache distributions. Figure [2](#S2.F2 "Figure 2 ‣ 2.3\. KV Cache Compression
    ‣ 2\. Background ‣ KVQuant: Towards 10 Million Context Length LLM Inference with
    KV Cache Quantization") shows sample distributions for the KV cache activations.
    We observe that the Key matrices tend to have distinct outlier channels, which
    have larger average magnitudes than other channels; this corroborates previous
    observations about outlier channels in LLM activations ([dettmers2022llm,](#bib.bib5)
    ; [xiao2023smoothquant,](#bib.bib36) ). The Value matrices exhibit both outlier
    channels as well as outlier tokens (although these outliers are less extreme than
    the outlier Key channels).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '为了指导我们的方法，我们首先进行了详细分析以了解 KV 缓存分布。图 [2](#S2.F2 "Figure 2 ‣ 2.3\. KV Cache Compression
    ‣ 2\. Background ‣ KVQuant: Towards 10 Million Context Length LLM Inference with
    KV Cache Quantization") 显示了 KV 缓存激活的样本分布。我们观察到 Key 矩阵往往有明显的异常通道，这些通道的平均幅度比其他通道更大；这验证了关于
    LLM 激活中异常通道的先前观察（[dettmers2022llm,](#bib.bib5) ; [xiao2023smoothquant,](#bib.bib36)）。Value
    矩阵则表现出异常通道和异常标记（尽管这些异常值比 Key 通道的异常值不那么极端）。'
- en: 'Existing KV cache quantization approaches perform per-token quantization (meaning
    that the scaling factor and zero-point are shared by elements in the same token)
    ([sheng2023flexgen,](#bib.bib30) ; [zhao2023atom,](#bib.bib38) ). However, due
    to the differing average magnitudes between channels, the values within a channel
    are easier to quantize when grouped together than the values across different
    channels. As such, to better match the distributions, we investigate per-channel
    KV cache quantization, meaning that the scaling factor and zero-point are shared
    by elements in the same channel. By sharing the scaling factor and zero-point
    along the channel dimension, this will naturally group together values with similar
    magnitudes, thereby mitigating the impacts of outlier channels on other channels
    when quantizing to low precision. As outlined in Appendix [D](#A4 "Appendix D
    Per-Channel Key Quantization Ablations ‣ KVQuant: Towards 10 Million Context Length
    LLM Inference with KV Cache Quantization"), we find that per-channel quantization
    provides significant accuracy benefits for Keys but not for Values. By leveraging
    per-channel quantization for Keys and per-token quantization for Values, we observe
    a 3.88 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. Note
    that this can potentially add runtime overhead since the quantization dimension
    is now misaligned with the reduction dimension for the Keys when performing matrix-vector
    multiplications. However, we find that we are able to efficiently dequantize Keys
    and perform the Query-Key matrix-vector multiplication without adding runtime
    overhead, as shown in Section [4.2](#S4.SS2 "4.2\. Performance Analysis ‣ 4\.
    Results ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache
    Quantization"). Additionally, as outlined in Section [3.6](#S3.SS6 "3.6\. Offline
    Calibration versus Online Computation ‣ 3\. Method ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization"), per-channel quantization
    can also be challenging due to the need to recompute scaling factors as tokens
    are added to the Key cache. We show that we can calibrate offline for scaling
    factors, thereby avoiding expensive online recomputation.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '现有的 KV 缓存量化方法执行每标记量化（即缩放因子和零点由同一标记中的元素共享）（[sheng2023flexgen,](#bib.bib30) ;
    [zhao2023atom,](#bib.bib38)）。然而，由于通道之间的平均幅度不同，通道内的值在分组时比跨通道的值更容易量化。因此，为了更好地匹配分布，我们调查了每通道
    KV 缓存量化，即缩放因子和零点由同一通道中的元素共享。通过沿通道维度共享缩放因子和零点，这将自然地将具有相似幅度的值分组，从而减轻在低精度量化时异常通道对其他通道的影响。如附录[D](#A4
    "Appendix D Per-Channel Key Quantization Ablations ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization")中概述的，我们发现每通道量化对键提供了显著的准确性好处，但对值则没有。通过对键使用每通道量化，对值使用每标记量化，我们观察到
    3-bit LLaMA-7B 量化在 Wikitext-2 上提高了 3.88 的困惑度。请注意，这可能会增加运行时开销，因为在执行矩阵-向量乘法时，量化维度现在与键的缩减维度不对齐。然而，我们发现我们能够高效地解量化键并执行查询-键矩阵-向量乘法，而不会增加运行时开销，如第[4.2](#S4.SS2
    "4.2\. Performance Analysis ‣ 4\. Results ‣ KVQuant: Towards 10 Million Context
    Length LLM Inference with KV Cache Quantization")节所示。此外，如第[3.6](#S3.SS6 "3.6\.
    Offline Calibration versus Online Computation ‣ 3\. Method ‣ KVQuant: Towards
    10 Million Context Length LLM Inference with KV Cache Quantization")节中概述的，由于需要在将标记添加到键缓存时重新计算缩放因子，每通道量化也可能具有挑战性。我们展示了我们可以离线校准缩放因子，从而避免昂贵的在线重新计算。'
- en: Per-channel Key quantization was also explored in another concurrent work ([kivi,](#bib.bib25)
    ), which leveraged similar intuition about grouping together large magnitude values
    in the same channel to minimize quantization error. Their methodology requires
    fine-grained grouping for per-channel quantization while maintaining a residual
    subset of the KV cache in fp16 (until all elements for that group have been added
    to the KV cache). In our work, we demonstrate that by leveraging offline calibration,
    we can accurately perform per-channel quantization without grouping.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 每通道键量化在另一项同时进行的工作中也有探索（[kivi,](#bib.bib25)），该工作利用了将大幅度值归为同一通道以最小化量化误差的类似直觉。他们的方法需要对每通道量化进行细粒度分组，同时在
    fp16 中保持 KV 缓存的剩余子集（直到该组的所有元素都已添加到 KV 缓存中）。在我们的工作中，我们展示了通过利用离线校准，我们可以准确地执行每通道量化，而无需分组。
- en: 3.2\. Pre-RoPE Key Quantization
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 预训练 RoPE 键量化
- en: 'One issue when quantizing Keys is handling the rotary positional embedding
    (RoPE), which is applied to Keys and Queries in most public LLMs, including LLaMA
    and LLaMA-2 ([su2023roformer,](#bib.bib31) ). Given Query and Key vectors $Q_{m}=W_{q}*x_{m}$.
    This embeds the relative position between a Query and Key vector as an amount
    of an angle that is a multiple of its position index. Formally, RoPE is applied
    in self-attention as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 量化 Keys 时的一个问题是处理旋转位置嵌入（RoPE），这在大多数公共 LLM 中应用于 Keys 和 Queries，包括 LLaMA 和 LLaMA-2
    ([su2023roformer,](#bib.bib31) )。给定 Query 和 Key 向量 $Q_{m}=W_{q}*x_{m}$。这将 Query
    和 Key 向量之间的相对位置嵌入为其位置索引的倍数的角度。形式上，RoPE 在自注意力机制中应用如下：
- en: '| (1) |  | $\displaystyle\tilde{Q}_{m}\tilde{K}_{n}^{\top}$ |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\displaystyle\tilde{Q}_{m}\tilde{K}_{n}^{\top}$ |  |'
- en: 'The Query vectors computed at each iteration will have RoPE applied ($\tilde{Q}_{m}$)
    and then efficiently apply the positional embeddings on-the-fly after dequantization.
    The benefits of pre-RoPE Key quantization are highlighted in Appendix [E](#A5
    "Appendix E Pre-RoPE Key Quantization Ablations ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization"), yielding 0.65 perplexity
    improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. To be able to quantize
    Keys pre-RoPE, we develop a fused kernel to efficiently apply RoPE post-dequantization
    (the details of this approach will be discussed in Section [3.7](#S3.SS7 "3.7\.
    Kernel Implementation ‣ 3\. Method ‣ KVQuant: Towards 10 Million Context Length
    LLM Inference with KV Cache Quantization")).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '每次迭代计算的 Query 向量将应用 RoPE ($\tilde{Q}_{m}$)，然后在去量化后高效地应用位置嵌入。附录 [E](#A5 "附录
    E 预 RoPE Key 量化消融 ‣ KVQuant: 通过 KV 缓存量化实现 1000 万上下文长度 LLM 推理") 中突出了预 RoPE Key
    量化的好处，3-bit LLaMA-7B 量化在 Wikitext-2 上提升了 0.65 的困惑度。为了能够在 RoPE 之前量化 Keys，我们开发了一种融合内核以高效地在去量化后应用
    RoPE（这一方法的详细信息将在第 [3.7](#S3.SS7 "3.7\. 内核实现 ‣ 3\. 方法 ‣ KVQuant: 通过 KV 缓存量化实现 1000
    万上下文长度 LLM 推理") 节中讨论）。'
- en: '3.3\. nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype'
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '3.3\. nuqX: 一种按层敏感性加权的 X 位非均匀数据类型'
- en: '![Refer to caption](img/e5bb507d7574710c28376e6c62cbe3c0.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e5bb507d7574710c28376e6c62cbe3c0.png)'
- en: 'Figure 3. One typically achieves better performance when the scaling factor/zero
    point are computed online. However, this is quite challenging to do for per-channel
    quantization, as these factors will not only need to be recomputed for every new
    Key appended to the Key cache, but also all the prior cached Keys will need to
    be updated. As such, we use a calibration set to compute per-channel scaling factors
    offline. A similar challenge exists for per-token quantization, but online calibration
    for this does not require updating prior cached Values. In Section [3.6](#S3.SS6
    "3.6\. Offline Calibration versus Online Computation ‣ 3\. Method ‣ KVQuant: Towards
    10 Million Context Length LLM Inference with KV Cache Quantization") and Appendix [I](#A9
    "Appendix I Calibration Ablations ‣ KVQuant: Towards 10 Million Context Length
    LLM Inference with KV Cache Quantization"), we discuss how we are able to efficiently
    compute outlier thresholds / scaling factors for per-token calibration, thereby
    enabling online computation.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3. 通常，当缩放因子/零点在线计算时，性能会更好。然而，对于每通道量化来说，这非常具有挑战性，因为这些因子不仅需要为每个新添加到 Key 缓存中的
    Key 重新计算，还需要更新所有先前缓存的 Keys。因此，我们使用校准集离线计算每通道缩放因子。对于每 token 的量化也存在类似的挑战，但在线校准不需要更新先前缓存的
    Values。在 第 [3.6](#S3.SS6 "3.6\. 离线校准与在线计算 ‣ 3\. 方法 ‣ KVQuant: 通过 KV 缓存量化实现 1000
    万上下文长度 LLM 推理") 节和 附录 [I](#A9 "附录 I 校准消融 ‣ KVQuant: 通过 KV 缓存量化实现 1000 万上下文长度 LLM
    推理") 中，我们讨论了如何有效地计算每 token 校准的异常值阈值/缩放因子，从而实现在线计算。'
- en: Uniform quantization is suboptimal for KV cache quantization since the Query
    and Key activations are non-uniform. Additionally, KV cache loading is memory
    bandwidth bound, regardless of batch size or sequence length, meaning that the
    dequantization overhead introduced by non-uniform quantization methods is not
    problematic (since the added computation does not introduce any additional latency).
    It is therefore desirable to leverage non-uniform quantization methods for KV
    cache quantization.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 KV 缓存量化，均匀量化并不理想，因为 Query 和 Key 激活值是非均匀的。此外，KV 缓存加载受限于内存带宽，无论批量大小或序列长度如何，这意味着非均匀量化方法引入的去量化开销并不是问题（因为附加的计算不会引入额外的延迟）。因此，利用非均匀量化方法进行
    KV 缓存量化是可取的。
- en: 'In ([kim2023squeezellm,](#bib.bib18) ), the authors computed non-uniform quantization
    signposts using a sensitivity-weighted k-means approach. However, this is challenging
    to apply online during inference due to its computational cost, and it is also
    difficult to estimate sensitivity for activations online. We therefore facilitate
    efficient online non-uniform KV cache quantization by computing sensitivity-weighted
    quantization signposts offline on a calibration set prior to inference. Using
    the diagonal Fisher information matrix (derived in Appendix [C](#A3 "Appendix
    C Derivation for Sensitivity Analysis ‣ KVQuant: Towards 10 Million Context Length
    LLM Inference with KV Cache Quantization")), along with the quantization error
    for activation $A$, we formulate the error minimization objective as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '在([kim2023squeezellm,](#bib.bib18))中，作者使用灵敏度加权的k-means方法计算了非均匀量化标志。然而，由于计算成本高，这在推理过程中在线应用具有挑战性，同时也难以在线估计激活的灵敏度。因此，我们通过在推理前在校准集上离线计算灵敏度加权的量化标志来实现高效的在线非均匀KV缓存量化。使用对角Fisher信息矩阵（在附录[C](#A3
    "附录C 灵敏度分析的推导 ‣ KVQuant: 通过KV缓存量化实现1000万上下文长度LLM推理")中推导），以及激活$A$的量化误差，我们将误差最小化目标公式化为：'
- en: '| (2) |  | $Q(A)^{*}\simeq\operatorname*{arg\,min}_{Q}\sum_{i=1}^{N}\mathcal{F}_{ii}\big{(}A-Q(A)\big{)}^{2}.$
    |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $Q(A)^{*}\simeq\operatorname*{arg\,min}_{Q}\sum_{i=1}^{N}\mathcal{F}_{ii}\big{(}A-Q(A)\big{)}^{2}.$
    |  |'
- en: 'In order to derive a per-matrix non-uniform datatype, we first normalize each
    vector to the range $[-1,1]$. We then minimize the objective in Equation [2](#S3.E2
    "In 3.3\. nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype ‣
    3\. Method ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV
    Cache Quantization") offline on a calibration set using a k-means solver in order
    to obtain the quantization signposts for the non-uniform datatype for each Key
    or Value layer. Appendix [F](#A6 "Appendix F Sensitivity-Weighted Non-Uniform
    Quantization Ablations ‣ KVQuant: Towards 10 Million Context Length LLM Inference
    with KV Cache Quantization") compares our non-uniform quantization approach with
    existing uniform and non-uniform quantization baselines ([dettmers2023qlora,](#bib.bib6)
    ), demonstrating how our non-uniform approach provides 0.33 perplexity improvement
    on Wikitext-2 for LLaMA-7B relative to 3-bit uniform methods. Table [15](#A9.T15
    "Table 15 ‣ Appendix I Calibration Ablations ‣ KVQuant: Towards 10 Million Context
    Length LLM Inference with KV Cache Quantization") in Appendix [I](#A9 "Appendix
    I Calibration Ablations ‣ KVQuant: Towards 10 Million Context Length LLM Inference
    with KV Cache Quantization") shows how computing the required Fisher information
    for the LLaMA-65B model takes only a few minutes, and how using the k-means solver
    takes only a few minutes per layer (with the computation for each layer being
    parallelizable). In Appendix [N](#A14 "Appendix N Mixed-Precision Quantization
    ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"),
    we also demonstrate that we can derive a metric for accurate one-shot mixed-precision
    assignment (where different layers are assigned different bit widths) using the
    quantization error weighted by sensitivity information.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '为了推导每矩阵的非均匀数据类型，我们首先将每个向量标准化到范围$[-1,1]$。然后，我们在校准集上离线最小化公式[2](#S3.E2 "在3.3\.
    nuqX: 一种X位每层灵敏度加权非均匀数据类型 ‣ 3\. 方法 ‣ KVQuant: 通过KV缓存量化实现1000万上下文长度LLM推理")中的目标，以获得每个Key或Value层的非均匀数据类型的量化标志。附录[F](#A6
    "附录F 灵敏度加权非均匀量化消融 ‣ KVQuant: 通过KV缓存量化实现1000万上下文长度LLM推理")比较了我们非均匀量化方法与现有均匀和非均匀量化基线（[dettmers2023qlora,](#bib.bib6)），展示了我们的非均匀方法如何在Wikitext-2上相对于3位均匀方法提供0.33的困惑度改善。附录[I](#A9
    "附录I 校准消融 ‣ KVQuant: 通过KV缓存量化实现1000万上下文长度LLM推理")中的表[15](#A9.T15 "表15 ‣ 附录I 校准消融
    ‣ KVQuant: 通过KV缓存量化实现1000万上下文长度LLM推理")展示了计算LLaMA-65B模型所需的Fisher信息仅需几分钟，并且使用k-means求解器每层仅需几分钟（每层的计算是可并行的）。在附录[N](#A14
    "附录N 混合精度量化 ‣ KVQuant: 通过KV缓存量化实现1000万上下文长度LLM推理")中，我们还展示了我们如何使用通过灵敏度信息加权的量化误差推导出准确的一次性混合精度分配指标（不同层分配不同位宽）。'
- en: 3.4\. Per-Vector Dense-and-Sparse Quantization
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 每向量密集与稀疏量化
- en: 'Figure [5](#A2.F5 "Figure 5 ‣ Appendix B Key and Value Dynamic Range ‣ KVQuant:
    Towards 10 Million Context Length LLM Inference with KV Cache Quantization") in
    Appendix [B](#A2 "Appendix B Key and Value Dynamic Range ‣ KVQuant: Towards 10
    Million Context Length LLM Inference with KV Cache Quantization") shows the portion
    of elements falling within different percentiles of the dynamic range. For both
    Keys and Values, the majority of elements are contained within a small percentage
    of the dynamic range. This means that by leveraging dense-and-sparse quantization,
    as demonstrated in ([kim2023squeezellm,](#bib.bib18) ), in order to isolate a
    small percentage of numerical outliers, we can restrict the range that we need
    to represent, thereby allowing us to represent the remaining elements with greater
    precision.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 附录 [B](#A2 "附录 B 键和值动态范围 ‣ KVQuant：朝着 1000 万上下文长度 LLM 推理的 KV 缓存量化") 中的图 [5](#A2.F5
    "图 5 ‣ 附录 B 键和值动态范围 ‣ KVQuant：朝着 1000 万上下文长度 LLM 推理的 KV 缓存量化") 显示了不同百分位动态范围内元素的比例。对于键和值，大多数元素都包含在动态范围的一小部分中。这意味着通过利用密集与稀疏量化，如
    ([kim2023squeezellm,](#bib.bib18) ) 中所示，为了隔离少量数值异常值，我们可以限制需要表示的范围，从而使我们能够以更高的精度表示其余元素。
- en: 'Additionally, when looking at the Key and Value distributions in Figure [2](#S2.F2
    "Figure 2 ‣ 2.3\. KV Cache Compression ‣ 2\. Background ‣ KVQuant: Towards 10
    Million Context Length LLM Inference with KV Cache Quantization"), different channels
    and tokens have different average magnitudes. Therefore, an element which counts
    as an outlier in one channel may not be an outlier in another channel (since that
    channel may have a greater average magnitude). It is therefore crucial to directly
    target the outlier values that skew the dynamic range at the granularity that
    we are quantizing in order to address the values that are exaggerating the range
    along that particular dimension. In this work, we leverage per-vector dense-and-sparse
    quantization, where we use a different outlier threshold per-vector (either a
    separate threshold per-channel for per-channel quantization, or a separate threshold
    per-token for per-token quantization), rather than a single outlier threshold
    for each layer.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，查看图 [2](#S2.F2 "图 2 ‣ 2.3\. KV 缓存压缩 ‣ 2\. 背景 ‣ KVQuant：朝着 1000 万上下文长度 LLM
    推理的 KV 缓存量化") 中的键和值分布时，不同通道和令牌的平均幅度各不相同。因此，一个在某个通道中被认为是异常值的元素，在另一个通道中可能并不是异常值（因为那个通道的平均幅度可能更大）。因此，至关重要的是直接针对那些扭曲动态范围的异常值，以我们量化的精度来处理那些在特定维度上夸大范围的值。在本研究中，我们利用了每向量密集与稀疏量化，其中我们对每个向量使用不同的异常值阈值（对于每通道量化使用单独的通道阈值，或对于每令牌量化使用单独的令牌阈值），而不是对每一层使用单一的异常值阈值。
- en: 'Note that computing outlier thresholds for per-vector dense-and-sparse quantization
    poses potential accuracy and efficiency challenges. However, in Section [3.6](#S3.SS6
    "3.6\. Offline Calibration versus Online Computation ‣ 3\. Method ‣ KVQuant: Towards
    10 Million Context Length LLM Inference with KV Cache Quantization"), we show
    that we are able to accurately calibrate for per-channel outlier thresholds offline
    and efficiently compute per-token outlier thresholds online. After determining
    the upper and lower outlier thresholds, the remaining numbers in the vector are
    normalized to the range $[-1,1]$, and we then minimize Equation [2](#S3.E2 "In
    3.3\. nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype ‣ 3\.
    Method ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache
    Quantization") (ignoring outliers) in order to obtain the quantization signposts
    for the non-uniform datatype for the remaining numbers. Appendix [G](#A7 "Appendix
    G Per-Vector Dense-and-Sparse Quantization Ablations ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization") will demonstrate the
    benefits of removing a small percentage of numerical outliers and keeping them
    in full precision, as well as the advantages of per-vector dense-and-sparse quantization
    over using a single outlier threshold for each layer. As shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ KVQuant: Towards 10 Million Context Length LLM
    Inference with KV Cache Quantization"), by removing 1% of numerical outliers using
    per-vector outlier thresholds, we achieve an additional 0.25 perplexity improvement
    on Wikitext-2 for 3-bit LLaMA-7B quantization, which is within 0.08 perplexity
    of the fp16 baseline.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，计算每向量密集和稀疏量化的异常值阈值可能会面临潜在的准确性和效率挑战。然而，在第 [3.6](#S3.SS6 "3.6\. 离线校准与在线计算
    ‣ 3\. 方法 ‣ KVQuant: 旨在实现 1000 万上下文长度 LLM 推断的 KV 缓存量化") 节中，我们展示了能够准确地离线校准每通道异常值阈值，并在线高效计算每标记的异常值阈值。在确定了上下异常值阈值后，向量中的其余数字被归一化到范围
    $[-1,1]$，然后我们最小化方程 [2](#S3.E2 "在 3.3\. nuqX: 一种 X 位每层敏感度加权非均匀数据类型 ‣ 3\. 方法 ‣ KVQuant:
    旨在实现 1000 万上下文长度 LLM 推断的 KV 缓存量化")（忽略异常值），以获得剩余数字的非均匀数据类型量化标记。附录 [G](#A7 "附录 G
    每向量密集与稀疏量化消融 ‣ KVQuant: 旨在实现 1000 万上下文长度 LLM 推断的 KV 缓存量化") 将展示去除少量数值异常值并保持其全精度的好处，以及每向量密集与稀疏量化相较于每层使用单一异常值阈值的优势。如图
    [1](#S1.F1 "图 1 ‣ 1\. 引言 ‣ KVQuant: 旨在实现 1000 万上下文长度 LLM 推断的 KV 缓存量化") 所示，通过使用每向量异常值阈值去除
    1% 的数值异常值，我们在 3 位 LLaMA-7B 量化的 Wikitext-2 上实现了额外的 0.25 困惑度改进，这与 fp16 基准相差 0.08
    困惑度以内。'
- en: 'Table 2. Evaluation of our method for different models using the perplexity
    on Wikitext-2\. Non-uniform quantization results are using pre-RoPE per-channel
    quantization for Keys. “gs64/128” refers to baseline experiments using grouping
    with group size 64/128. KV cache sizes assume a sequence length of 128K (ignoring
    context length limits for the models). We incorporate Q-Norm for nuq2 experiments.
    ^† and ^‡ denote the methods used in ATOM ([zhao2023atom,](#bib.bib38) ) and FlexGen
    ([sheng2023flexgen,](#bib.bib30) ), respectively. Note that we used post-RoPE
    quantization for all baseline methods since it achieves higher accuracy when quantizing
    Keys per-token as shown in Appendix [L](#A12 "Appendix L Post-RoPE Per-Token Quantization
    Ablation ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache
    Quantization").'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2. 我们的方法在不同模型上的评估，使用 Wikitext-2 上的困惑度。非均匀量化结果使用了预 RoPE 的每通道量化方法来量化 Keys。“gs64/128”指的是使用分组大小为
    64/128 的基准实验。KV 缓存大小假定序列长度为 128K（忽略模型的上下文长度限制）。我们在 nuq2 实验中加入了 Q-Norm。^† 和 ^‡
    分别表示在 ATOM ([zhao2023atom,](#bib.bib38)) 和 FlexGen ([sheng2023flexgen,](#bib.bib30))
    中使用的方法。请注意，由于 post-RoPE 量化在按标记量化 Keys 时可以获得更高的准确性，我们为所有基准方法使用了 post-RoPE 量化，详细见附录
    [L](#A12 "附录 L Post-RoPE 每标记量化消融 ‣ KVQuant: 旨在实现 1000 万上下文长度 LLM 推断的 KV 缓存量化")。'
- en: '| Method | LLaMA-7B | LLaMA-13B | LLaMA-30B | LLaMA-65B |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LLaMA-7B | LLaMA-13B | LLaMA-30B | LLaMA-65B |'
- en: '| Perplexity | KV Cache (GB) | Perplexity | KV Cache (GB) | Perplexity | KV
    Cache (GB) | Perplexity | KV Cache (GB) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度 | KV 缓存 (GB) | 困惑度 | KV 缓存 (GB) | 困惑度 | KV 缓存 (GB) | 困惑度 | KV 缓存 (GB)
    |'
- en: '| baseline | 5.68 | 32.0 | 5.09 | 50.0 | 4.10 | 97.5 | 3.53 | 160.0 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 5.68 | 32.0 | 5.09 | 50.0 | 4.10 | 97.5 | 3.53 | 160.0 |'
- en: '| int4 | 5.98 | 8.0 | 5.32 | 12.5 | 4.34 | 24.4 | 3.73 | 40.0 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| int4 | 5.98 | 8.0 | 5.32 | 12.5 | 4.34 | 24.4 | 3.73 | 40.0 |'
- en: '| nf4 | 5.87 | 8.0 | 5.23 | 12.5 | 4.25 | 24.4 | 3.63 | 40.0 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| nf4 | 5.87 | 8.0 | 5.23 | 12.5 | 4.25 | 24.4 | 3.63 | 40.0 |'
- en: '| int4-gs128^† | 5.77 | 8.3 | 5.16 | 13.0 | 4.16 | 25.3 | 3.57 | 41.6 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| int4-gs128^† | 5.77 | 8.3 | 5.16 | 13.0 | 4.16 | 25.3 | 3.57 | 41.6 |'
- en: '| int4-gs64^‡ | 5.73 | 8.6 | 5.14 | 13.5 | 4.14 | 26.3 | 3.56 | 43.1 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| int4-gs64^‡ | 5.73 | 8.6 | 5.14 | 13.5 | 4.14 | 26.3 | 3.56 | 43.1 |'
- en: '| nf4-gs128 | 5.77 | 8.5 | 5.17 | 13.3 | 4.17 | 25.9 | 3.58 | 42.5 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| nf4-gs128 | 5.77 | 8.5 | 5.17 | 13.3 | 4.17 | 25.9 | 3.58 | 42.5 |'
- en: '| nuq4 | 5.73 | 8.0 | 5.15 | 12.5 | 4.16 | 24.4 | 3.57 | 40.0 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| nuq4 | 5.73 | 8.0 | 5.15 | 12.5 | 4.16 | 24.4 | 3.57 | 40.0 |'
- en: '| nuq4-1% | 5.70 | 8.6 | 5.11 | 13.5 | 4.12 | 26.3 | 3.54 | 43.2 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| nuq4-1% | 5.70 | 8.6 | 5.11 | 13.5 | 4.12 | 26.3 | 3.54 | 43.2 |'
- en: '| int3 | 10.87 | 6.0 | 8.69 | 9.4 | 6.82 | 18.3 | 6.37 | 30.0 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| int3 | 10.87 | 6.0 | 8.69 | 9.4 | 6.82 | 18.3 | 6.37 | 30.0 |'
- en: '| nf3 | 7.33 | 6.0 | 6.21 | 9.4 | 5.46 | 18.3 | 4.44 | 30.0 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| nf3 | 7.33 | 6.0 | 6.21 | 9.4 | 5.46 | 18.3 | 4.44 | 30.0 |'
- en: '| int3-gs128 | 6.17 | 6.3 | 5.47 | 9.8 | 4.44 | 19.2 | 3.78 | 31.5 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| int3-gs128 | 6.17 | 6.3 | 5.47 | 9.8 | 4.44 | 19.2 | 3.78 | 31.5 |'
- en: '| int3-gs64 | 5.93 | 6.6 | 5.29 | 10.3 | 4.26 | 20.1 | 3.66 | 33.0 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| int3-gs64 | 5.93 | 6.6 | 5.29 | 10.3 | 4.26 | 20.1 | 3.66 | 33.0 |'
- en: '| nf3-gs128 | 6.26 | 6.5 | 5.52 | 10.2 | 4.54 | 19.8 | 3.83 | 32.5 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| nf3-gs128 | 6.26 | 6.5 | 5.52 | 10.2 | 4.54 | 19.8 | 3.83 | 32.5 |'
- en: '| nuq3 | 6.01 | 6.0 | 5.34 | 9.4 | 4.41 | 18.3 | 3.74 | 30.0 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | 6.01 | 6.0 | 5.34 | 9.4 | 4.41 | 18.3 | 3.74 | 30.0 |'
- en: '| nuq3-1% | 5.76 | 6.6 | 5.15 | 10.4 | 4.17 | 20.3 | 3.59 | 33.2 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| nuq3-1% | 5.76 | 6.6 | 5.15 | 10.4 | 4.17 | 20.3 | 3.59 | 33.2 |'
- en: '| int2 | 11779 | 4.0 | 69965 | 6.3 | 1470 | 12.2 | 7272 | 20.0 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| int2 | 11779 | 4.0 | 69965 | 6.3 | 1470 | 12.2 | 7272 | 20.0 |'
- en: '| nf2 | 3210 | 4.0 | 5786 | 6.3 | 2044 | 12.2 | 5367 | 20.0 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| nf2 | 3210 | 4.0 | 5786 | 6.3 | 2044 | 12.2 | 5367 | 20.0 |'
- en: '| int2-gs128 | 37.37 | 4.3 | 41.77 | 6.7 | 16.49 | 13.0 | 13.63 | 21.4 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| int2-gs128 | 37.37 | 4.3 | 41.77 | 6.7 | 16.49 | 13.0 | 13.63 | 21.4 |'
- en: '| int2-gs64 | 11.09 | 4.6 | 9.84 | 7.1 | 6.60 | 13.9 | 5.54 | 22.8 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| int2-gs64 | 11.09 | 4.6 | 9.84 | 7.1 | 6.60 | 13.9 | 5.54 | 22.8 |'
- en: '| nf2-gs128 | 351.23 | 4.5 | 141.19 | 7.0 | 60.97 | 13.7 | 31.69 | 22.5 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| nf2-gs128 | 351.23 | 4.5 | 141.19 | 7.0 | 60.97 | 13.7 | 31.69 | 22.5 |'
- en: '| nuq2 | 8.17 | 4.0 | 7.29 | 6.3 | 7.05 | 12.2 | 27.17 | 20.0 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| nuq2 | 8.17 | 4.0 | 7.29 | 6.3 | 7.05 | 12.2 | 27.17 | 20.0 |'
- en: '| nuq2-1% | 6.06 | 4.6 | 5.40 | 7.3 | 4.43 | 14.2 | 3.76 | 23.2 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| nuq2-1% | 6.06 | 4.6 | 5.40 | 7.3 | 4.43 | 14.2 | 3.76 | 23.2 |'
- en: 3.5\. Mitigating Distribution Shift using Q-Norm
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 使用 Q-Norm 减轻分布偏移
- en: 'When pushing to extremely low bit widths like 2-bit quantization, we begin
    to observe accuracy degradation due to distribution shift, meaning that the post-quantization
    distribution has a different mean and standard deviation than the pre-quantization
    distribution. As shown in Appendix [H](#A8 "Appendix H Q-Norm Ablations ‣ KVQuant:
    Towards 10 Million Context Length LLM Inference with KV Cache Quantization"),
    this distribution shift can lead to greater error accumulation at later layers.
    To combat this, we introduce Q-Normalization (shortened to Q-Norm), where we normalize
    the quantization centroids obtained from k-means in order to ensure the post-quantization
    distribution has the same mean and standard deviation as the pre-quantization
    distribution. Our solution is inspired by ([li2023norm,](#bib.bib21) ), which
    demonstrated that ensuring that the activation distributions post-weight quantization
    have similar mean and standard deviation to the activation distributions pre-weight
    quantization can help reduce accuracy degradation. Given mean $\mu_{1}$ as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '当推到如 2 位量化这样的极低位宽时，我们开始观察到由于分布偏移而导致的准确性下降，这意味着量化后的分布与量化前的分布具有不同的均值和标准差。如附录
    [H](#A8 "附录 H Q-Norm 消融 ‣ KVQuant: 实现 1000 万上下文长度 LLM 推理与 KV 缓存量化") 中所示，这种分布偏移可能导致后续层的错误累积增加。为了解决这个问题，我们引入了
    Q-Normalization（简称 Q-Norm），在这里我们对从 k-means 获得的量化中心进行归一化，以确保量化后的分布具有与量化前相同的均值和标准差。我们的解决方案受到
    ([li2023norm,](#bib.bib21) ) 的启发，该研究表明，确保权重量化后的激活分布具有与权重量化前类似的均值和标准差可以帮助减少准确性下降。给定均值
    $\mu_{1}$ 如下：'
- en: '| (3) |  | $\hat{C}_{i}=\frac{(C_{i}-\mu_{2})\sigma_{1}}{\sigma_{2}}+\mu_{1}.$
    |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\hat{C}_{i}=\frac{(C_{i}-\mu_{2})\sigma_{1}}{\sigma_{2}}+\mu_{1}.$
    |  |'
- en: 'During dequantization, we use $\hat{C}_{i}$ such that the mean and standard
    deviation of dequantized values matches the original fp16 distribution. As shown
    in Appendix [H](#A8 "Appendix H Q-Norm Ablations ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization"), Q-Norm provides significant
    accuracy benefits for 2-bit quantization with no added inference cost.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '在去量化过程中，我们使用 $\hat{C}_{i}$ 以确保去量化值的均值和标准差与原始 fp16 分布匹配。如附录 [H](#A8 "附录 H Q-Norm
    消融 ‣ KVQuant: 实现 1000 万上下文长度 LLM 推理与 KV 缓存量化") 中所示，Q-Norm 为 2 位量化提供了显著的准确性收益，同时没有增加推理成本。'
- en: 3.6\. Offline Calibration versus Online Computation
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6\. 离线校准与在线计算
- en: 'A crucial challenge for activation quantization is that we either need to compute
    statistics on-the-fly (which is potentially expensive) or else we need to use
    offline calibration data (which potentially has negative accuracy implications).
    The challenges with computing scaling factors (and zero-points) online versus
    offline for both Keys and Values are shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.3\.
    nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype ‣ 3\. Method
    ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization").
    In per-channel quantization, it is challenging to update scaling factors online
    since the scaling factors corresponding to each incoming channel would potentially
    need to be updated whenever a new token is added to the KV cache. It is therefore
    desirable to be able to compute statistics offline (i.e., using calibration data
    before running inference). While this can have negative effects on model accuracy,
    in Appendix [I](#A9 "Appendix I Calibration Ablations ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization") we show that we can
    effectively calibrate offline for per-channel quantization, obviating the need
    for online updates of scaling factors for per-channel quantization. For per-token
    quantization, it is challenging to calibrate for scaling factors offline due to
    the presence of outlier Value tokens. It is therefore desirable to be able to
    compute scaling factors and outlier thresholds online for each incoming token.
    As shown in Appendix [I](#A9 "Appendix I Calibration Ablations ‣ KVQuant: Towards
    10 Million Context Length LLM Inference with KV Cache Quantization"), we can efficiently
    compute outlier thresholds online per-token by offloading to the CPU. By leveraging
    custom quantization function implementations for compressing activations, we are
    able to perform online per-token Value quantization without compromising on performance.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '对于激活量化，一个关键的挑战是我们要么需要实时计算统计数据（这可能成本很高），要么需要使用离线校准数据（这可能会对准确性产生负面影响）。图[3](#S3.F3
    "Figure 3 ‣ 3.3\. nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype
    ‣ 3\. Method ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV
    Cache Quantization")展示了在线与离线计算缩放因子（以及零点）在 Keys 和 Values 中的挑战。在每通道量化中，由于每个输入通道的缩放因子在向
    KV 缓存中添加新标记时可能需要更新，因此在线更新缩放因子具有挑战性。因此，能够离线计算统计数据（即，使用推理前的校准数据）是理想的。虽然这可能对模型准确性产生负面影响，但在附录[I](#A9
    "Appendix I Calibration Ablations ‣ KVQuant: Towards 10 Million Context Length
    LLM Inference with KV Cache Quantization")中，我们展示了可以有效地离线校准每通道量化，避免了对每通道量化进行在线更新缩放因子的需要。对于每标记量化，由于存在离群值标记，离线校准缩放因子具有挑战性。因此，能够在线计算每个输入标记的缩放因子和离群阈值是理想的。如附录[I](#A9
    "Appendix I Calibration Ablations ‣ KVQuant: Towards 10 Million Context Length
    LLM Inference with KV Cache Quantization")所示，我们可以通过将计算卸载到 CPU 来高效地在线计算每标记的离群阈值。通过利用自定义量化函数实现来压缩激活，我们能够在不影响性能的情况下进行在线每标记的
    Value 量化。'
- en: 3.7\. Kernel Implementation
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7\. 内核实现
- en: 'In order to efficiently perform activation quantization on-the-fly, we leverage
    dedicated kernel implementations with our 4-bit quantization method for compressing
    vectors to reduced precision and extracting the sparse outliers, performing matrix-vector
    multiplications using the compressed vectors, and performing sparse matrix-dense
    vector multiplications using the sparse outliers. We store the quantized Key and
    Value matrices as 4-bit elements which are used as indices into lookup tables
    to recover the corresponding fp16 values. We store the sparse outlier matrices
    in either Compressed-Sparse Row (CSR) or Compressed-Sparse Column (CSC) format
    (depending on which aligns better with appending new Key and Value tokens). The
    kernels for the Key matrix-vector operations apply RoPE on-the-fly in order to
    support pre-RoPE quantization. More kernel implementation details are provided
    in Appendix [O](#A15 "Appendix O Kernel Implementation Details ‣ KVQuant: Towards
    10 Million Context Length LLM Inference with KV Cache Quantization").'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '为了高效地实时执行激活量化，我们利用专用的内核实现和我们的 4 位量化方法来压缩向量以降低精度并提取稀疏离群值，使用压缩向量执行矩阵-向量乘法，并使用稀疏离群值执行稀疏矩阵-密集向量乘法。我们将量化后的
    Key 和 Value 矩阵存储为 4 位元素，这些元素用作查找表的索引，以恢复对应的 fp16 值。我们将稀疏离群矩阵存储为压缩稀疏行（CSR）或压缩稀疏列（CSC）格式（取决于哪种格式更适合追加新的
    Key 和 Value 标记）。Key 矩阵-向量操作的内核实时应用 RoPE 以支持 RoPE 前量化。更多内核实现细节见附录[O](#A15 "附录 O
    内核实现细节 ‣ KVQuant: 迈向 1000 万上下文长度 LLM 推断与 KV 缓存量化")。'
- en: '![Refer to caption](img/4b1e6eb89dc6d773ee600200aa418170.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4b1e6eb89dc6d773ee600200aa418170.png)'
- en: 'Figure 4. Perplexity results for the LLaMA-2-7B-32K model ([chen2023extending,](#bib.bib3)
    ) as well as the LLaMA-2-70B-32K LongLoRA model ([chen2023longlora,](#bib.bib4)
    ) on the Wikitext-2 dataset, evaluated using different sequence lengths. Figure [8](#A13.F8
    "Figure 8 ‣ Appendix M Additional Long Sequence Length Evaluation ‣ KVQuant: Towards
    10 Million Context Length LLM Inference with KV Cache Quantization") in Appendix [M](#A13
    "Appendix M Additional Long Sequence Length Evaluation ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization") provides additional
    results with long sequence length evaluation for 2-bit quantization including
    Q-Norm.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4. LLaMA-2-7B-32K 模型（[chen2023extending](#bib.bib3)）以及 LLaMA-2-70B-32K LongLoRA
    模型（[chen2023longlora](#bib.bib4)）在 Wikitext-2 数据集上的困惑度结果，使用不同的序列长度进行评估。附录[M](#A13
    "附录 M 额外的长序列长度评估 ‣ KVQuant: 迈向 1000 万上下文长度 LLM 推断与 KV 缓存量化")中的图[8](#A13.F8 "图
    8 ‣ 附录 M 额外的长序列长度评估 ‣ KVQuant: 迈向 1000 万上下文长度 LLM 推断与 KV 缓存量化")提供了包括 Q-Norm 在内的
    2 位量化的长序列长度评估的额外结果。'
- en: 4\. Results
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 结果
- en: 4.1\. Main Results
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 主要结果
- en: 4.1.1\. Main Evaluation
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 主要评估
- en: 'We used the LLaMA-7B/13B/30B/65B, LLaMA-2-7B/13B/70B, and Mistral-7B models
    to evaluate our methodology by measuring perplexity on both Wikitext-2 and C4
    ([touvron2023llama,](#bib.bib32) ; [touvron2023llama2,](#bib.bib33) ; [jiang2023mistral,](#bib.bib17)
    ); see Appendix [J](#A10 "Appendix J Additional Experimental Details ‣ KVQuant:
    Towards 10 Million Context Length LLM Inference with KV Cache Quantization") for
    details on our experimental setup. Table [2](#S3.T2 "Table 2 ‣ 3.4\. Per-Vector
    Dense-and-Sparse Quantization ‣ 3\. Method ‣ KVQuant: Towards 10 Million Context
    Length LLM Inference with KV Cache Quantization") shows the results for LLaMA
    models for the Wikitext-2 dataset. We compared our method with per-token quantization
    with and without grouping. The baseline configurations used by Atom and FlexGen
    are included for reference ([zhao2023atom,](#bib.bib38) ; [sheng2023flexgen,](#bib.bib30)
    ). We did not include results for KIVI ([kivi,](#bib.bib25) ) since they did not
    report perplexity values. We find that our method consistently outperforms baseline
    approaches by an especially large margin with 3-bit and 2-bit quantization. Once
    we incorporate outliers, we further push the performance of low-precision quantization,
    achieving 4-bit quantization with less than 0.02 perplexity degradation, 3-bit
    quantization with under 0.09 perplexity degradation, and 2-bit quantization with
    under 0.43 perplexity degradation relative to the fp16 baseline across all models
    and datasets (while attaining 3.7$\times$ memory savings, respectively). Tables
    [16](#A11.T16 "Table 16 ‣ Appendix K Full Perplexity Evaluation and MMLU Evaluation
    ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization")
    and [17](#A11.T17 "Table 17 ‣ Appendix K Full Perplexity Evaluation and MMLU Evaluation
    ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization")
    in Appendix [K](#A11 "Appendix K Full Perplexity Evaluation and MMLU Evaluation
    ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization")
    show full perplexity evaluation on Wikitext-2 and C4, showing the consistent performance
    of our approach across different models and datasets. Table [18](#A11.T18 "Table
    18 ‣ Appendix K Full Perplexity Evaluation and MMLU Evaluation ‣ KVQuant: Towards
    10 Million Context Length LLM Inference with KV Cache Quantization") in Appendix [K](#A11
    "Appendix K Full Perplexity Evaluation and MMLU Evaluation ‣ KVQuant: Towards
    10 Million Context Length LLM Inference with KV Cache Quantization") also provides
    zero-shot MMLU evaluation with 3-bit quantization, demonstrating how our method
    maintains performance on downstream tasks.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 LLaMA-7B/13B/30B/65B、LLaMA-2-7B/13B/70B 和 Mistral-7B 模型，通过在 Wikitext-2
    和 C4 上测量困惑度来评估我们的方法论 ([touvron2023llama,](#bib.bib32) ; [touvron2023llama2,](#bib.bib33)
    ; [jiang2023mistral,](#bib.bib17) )；有关我们实验设置的详细信息，请参见附录 [J](#A10 "附录 J 额外实验细节
    ‣ KVQuant：通过 KV 缓存量化实现 1000 万上下文长度 LLM 推理")。表 [2](#S3.T2 "表 2 ‣ 3.4\. 每向量密集与稀疏量化
    ‣ 3\. 方法 ‣ KVQuant：通过 KV 缓存量化实现 1000 万上下文长度 LLM 推理") 显示了 LLaMA 模型在 Wikitext-2
    数据集上的结果。我们将我们的方法与每个令牌量化（有无分组）进行了比较。包括 Atom 和 FlexGen 使用的基线配置以供参考 ([zhao2023atom,](#bib.bib38)
    ; [sheng2023flexgen,](#bib.bib30) )。由于 KIVI ([kivi,](#bib.bib25) ) 没有报告困惑度值，因此我们没有包括其结果。我们发现我们的方法在
    3 位和 2 位量化下显著优于基线方法。通过考虑离群值，我们进一步提升了低精度量化的性能，实现了 4 位量化困惑度降级不到 0.02、3 位量化困惑度降级不到
    0.09、2 位量化困惑度降级不到 0.43，相对于 fp16 基线在所有模型和数据集上的表现（同时实现了 3.7$\times$ 的内存节省）。附录 [K](#A11
    "附录 K 完整困惑度评估与 MMLU 评估 ‣ KVQuant：通过 KV 缓存量化实现 1000 万上下文长度 LLM 推理") 中的表 [16](#A11.T16
    "表 16 ‣ 附录 K 完整困惑度评估与 MMLU 评估 ‣ KVQuant：通过 KV 缓存量化实现 1000 万上下文长度 LLM 推理") 和 [17](#A11.T17
    "表 17 ‣ 附录 K 完整困惑度评估与 MMLU 评估 ‣ KVQuant：通过 KV 缓存量化实现 1000 万上下文长度 LLM 推理") 显示了
    Wikitext-2 和 C4 上的完整困惑度评估，展示了我们的方法在不同模型和数据集上的一致表现。附录 [K](#A11 "附录 K 完整困惑度评估与 MMLU
    评估 ‣ KVQuant：通过 KV 缓存量化实现 1000 万上下文长度 LLM 推理") 中的表 [18](#A11.T18 "表 18 ‣ 附录 K
    完整困惑度评估与 MMLU 评估 ‣ KVQuant：通过 KV 缓存量化实现 1000 万上下文长度 LLM 推理") 还提供了 3 位量化的零-shot
    MMLU 评估，展示了我们的方法如何在下游任务中保持性能。
- en: 4.1.2\. Long Context Length Evaluation
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 长上下文长度评估
- en: 'We evaluated long context length performance using the LLaMA-2-7B-32K model
    (uptrained for long sequence lengths using positional interpolation ([chen2023extending,](#bib.bib3)
    )) as well as the LLaMA-2-70B-32K LongLoRA model ([chen2023longlora,](#bib.bib4)
    ). For evaluating performance on longer context lengths, we first evaluated perplexity
    on Wikitext-2 using larger amounts of input context, as shown in Figure [4](#S3.F4
    "Figure 4 ‣ 3.7\. Kernel Implementation ‣ 3\. Method ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization") ([chen2023longlora,](#bib.bib4)
    ; [han2023lminfinite,](#bib.bib14) ). The results demonstrate how our method maintains
    accuracy even for longer amounts of input context, thereby enabling efficient
    and accurate long sequence length inference. Additional long sequence length perplexity
    evaluation results for 2-bit quantization (including Q-Norm) are provided in Appendix [M](#A13
    "Appendix M Additional Long Sequence Length Evaluation ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization").'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用 LLaMA-2-7B-32K 模型（通过位置插值进行长序列长度训练（[chen2023extending,](#bib.bib3) ））以及
    LLaMA-2-70B-32K LongLoRA 模型（[chen2023longlora,](#bib.bib4) ）评估了长上下文长度的性能。为了评估更长上下文长度的表现，我们首先在
    Wikitext-2 上使用更多的输入上下文评估了困惑度，如图 [4](#S3.F4 "Figure 4 ‣ 3.7\. Kernel Implementation
    ‣ 3\. Method ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV
    Cache Quantization") 所示（[chen2023longlora,](#bib.bib4) ; [han2023lminfinite,](#bib.bib14)
    ）。结果表明，我们的方法即使在输入上下文较长时也能保持准确性，从而实现高效准确的长序列长度推理。附录 [M](#A13 "Appendix M Additional
    Long Sequence Length Evaluation ‣ KVQuant: Towards 10 Million Context Length LLM
    Inference with KV Cache Quantization") 提供了 2 位量化（包括 Q-Norm）的额外长序列长度困惑度评估结果。'
- en: 'We also evaluated the performance of our quantization method on passkey retrieval
    to assess the model’s ability to use its context. Passkey retrieval involves evaluating
    the model’s capacity to locate specific information in long texts ([longchat2023,](#bib.bib20)
    ), and this can be used to effectively measure the maximum distance over which
    a token can attend during the inference stage. We used the passkey evaluation
    framework from ([zhu2023pose,](#bib.bib39) ) (which is based on the methodology
    from ([mohtashami2023landmark,](#bib.bib26) )) to evaluate retrieval performance.
    Table [3](#S4.T3 "Table 3 ‣ 4.1.2\. Long Context Length Evaluation ‣ 4.1\. Main
    Results ‣ 4\. Results ‣ KVQuant: Towards 10 Million Context Length LLM Inference
    with KV Cache Quantization") shows passkey retrieval results for the LLaMA-2-7B-32K
    and LLaMA-2-70B-32K models. These results indicate that with 4-bit and 3-bit quantization,
    KVQuant is able to maintain retrieval performance of the fp16 model. We observe
    some degradation with 2-bit quantization especially with LLaMA-2-70B, indicating
    potential room for improvement in 2-bit quantization techniques.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还评估了我们量化方法在密钥检索上的表现，以评估模型使用上下文的能力。密钥检索涉及评估模型在长文本中定位特定信息的能力（[longchat2023,](#bib.bib20)），这可以有效地衡量一个
    token 在推理阶段的最大关注距离。我们使用了来自（[zhu2023pose,](#bib.bib39) ）的密钥检索评估框架（该框架基于（[mohtashami2023landmark,](#bib.bib26)
    ）的方法）来评估检索性能。表 [3](#S4.T3 "Table 3 ‣ 4.1.2\. Long Context Length Evaluation ‣
    4.1\. Main Results ‣ 4\. Results ‣ KVQuant: Towards 10 Million Context Length
    LLM Inference with KV Cache Quantization") 显示了 LLaMA-2-7B-32K 和 LLaMA-2-70B-32K
    模型的密钥检索结果。这些结果表明，通过 4 位和 3 位量化，KVQuant 能够保持 fp16 模型的检索性能。我们观察到，特别是 LLaMA-2-70B
    的 2 位量化存在一定的性能下降，表明 2 位量化技术可能还有改进的空间。'
- en: Table 3. Passkey retrieval results across different context lengths for the
    LLaMA-2-7B-32K model (uptrained for long sequence lengths using positional interpolation
    ([chen2023extending,](#bib.bib3) )) as well as the LLaMA-2-70B-32K LongLoRA model
    ([chen2023longlora,](#bib.bib4) ). The values reported are the success rate for
    retrieving the passkey, computed over 50 samples. 2-bit results are with per-matrix
    Q-Norm enabled.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3. LLaMA-2-7B-32K 模型（通过位置插值进行长序列长度训练（[chen2023extending,](#bib.bib3) ））以及
    LLaMA-2-70B-32K LongLoRA 模型（[chen2023longlora,](#bib.bib4) ）在不同上下文长度下的密钥检索结果。报告的值是计算在
    50 个样本上的密钥检索成功率。2 位量化结果启用了每矩阵 Q-Norm。
- en: '| Model | Datatype | 2K | 4K | 8K | 16K | 32K |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 数据类型 | 2K | 4K | 8K | 16K | 32K |'
- en: '| LLaMA-2-7B-32K | fp16 | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-32K | fp16 | 1 | 1 | 1 | 1 | 1 |'
- en: '| nuq4-1% | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| nuq4-1% | 1 | 1 | 1 | 1 | 1 |'
- en: '| nuq3-1% | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| nuq3-1% | 1 | 1 | 1 | 1 | 1 |'
- en: '| nuq2-1% | 1 | 1 | 1 | 0.92 | 0.98 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| nuq2-1% | 1 | 1 | 1 | 0.92 | 0.98 |'
- en: '| LLaMA-2-70B-32K | fp16 | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-70B-32K | fp16 | 1 | 1 | 1 | 1 | 1 |'
- en: '| nuq4-1% | 1 | 0.98 | 1 | 1 | 1 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| nuq4-1% | 1 | 0.98 | 1 | 1 | 1 |'
- en: '| nuq3-1% | 1 | 0.98 | 1 | 1 | 1 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| nuq3-1% | 1 | 0.98 | 1 | 1 | 1 |'
- en: '| nuq2-1% | 0.82 | 0.8 | 0.82 | 0.8 | 0.76 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| nuq2-1% | 0.82 | 0.8 | 0.82 | 0.8 | 0.76 |'
- en: 4.1.3\. Joint Weight and KV Cache Quantization
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 联合权重和KV缓存量化
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.1.3\. Joint Weight and KV Cache Quantization
    ‣ 4.1\. Main Results ‣ 4\. Results ‣ KVQuant: Towards 10 Million Context Length
    LLM Inference with KV Cache Quantization") shows results for our KV cache quantization
    method when the weights are also quantized using the methodology in SqueezeLLM ([kim2023squeezellm,](#bib.bib18)
    ). We observe minimal perplexity degradation when leveraging our KV cache quantization
    approach, even when weights are also quantized to reduced precision (achieving
    within 0.02 perplexity of 4-bit weight-only quantization when quantizing the KV
    cache using nuq4-1% for the LLaMA-7B and LLaMA-13B models). These results demonstrate
    how our method is compatible with existing weight-only quantization methods.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [4](#S4.T4 "表 4 ‣ 4.1.3\. 联合权重和KV缓存量化 ‣ 4.1\. 主要结果 ‣ 4\. 结果 ‣ KVQuant: 通过KV缓存量化实现1000万上下文长度LLM推断")
    显示了当权重也使用SqueezeLLM中的方法进行量化时我们KV缓存量化方法的结果。即使权重也量化为降低的精度（在对LLaMA-7B和LLaMA-13B模型使用nuq4-1%量化KV缓存时，达到4位仅权重量化的0.02困惑度以内），我们观察到困惑度的降解最小。这些结果展示了我们的方法如何与现有的仅权重量化方法兼容。'
- en: 'Table 4. KV cache quantization results when applied in conjunction with the
    weight quantization methodology in SqueezeLLM ([kim2023squeezellm,](#bib.bib18)
    ). w4-s45 and w3-s45 refer to the 4-bit and 3-bit dense-and-sparse weight quantization
    approaches in  ([kim2023squeezellm,](#bib.bib18) ), respectively. See Appendix [J](#A10
    "Appendix J Additional Experimental Details ‣ KVQuant: Towards 10 Million Context
    Length LLM Inference with KV Cache Quantization") for experimental details.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '表4. KV缓存量化结果，当与SqueezeLLM的权重量化方法结合使用时 ([kim2023squeezellm,](#bib.bib18))。w4-s45和w3-s45分别指4位和3位的稠密和稀疏权重量化方法
    ([kim2023squeezellm,](#bib.bib18))。有关实验详细信息，请参见附录 [J](#A10 "附录 J 额外实验细节 ‣ KVQuant:
    通过KV缓存量化实现1000万上下文长度LLM推断")。'
- en: '| Weights | KV Cache | LLaMA-7B | LLaMA-13B | Avg. Bits (KV Cache) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 权重 | KV缓存 | LLaMA-7B | LLaMA-13B | 平均位数（KV缓存） |'
- en: '| fp16 | fp16 | 5.68 | 5.09 | 16 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| fp16 | fp16 | 5.68 | 5.09 | 16 |'
- en: '|  | fp16 | 5.77 | 5.17 | 16 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | fp16 | 5.77 | 5.17 | 16 |'
- en: '| w4-s45 | nuq4 | 5.83 | 5.22 | 4.00 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| w4-s45 | nuq4 | 5.83 | 5.22 | 4.00 |'
- en: '|  | nuq4-1% | 5.79 | 5.18 | 4.32 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | nuq4-1% | 5.79 | 5.18 | 4.32 |'
- en: '|  | fp16 | 6.13 | 5.45 | 16 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | fp16 | 6.13 | 5.45 | 16 |'
- en: '| w3-s45 | nuq3 | 6.55 | 5.77 | 3.00 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| w3-s45 | nuq3 | 6.55 | 5.77 | 3.00 |'
- en: '|  | nuq3-1% | 6.26 | 5.54 | 3.32 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | nuq3-1% | 6.26 | 5.54 | 3.32 |'
- en: 4.2\. Performance Analysis
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 性能分析
- en: 'Table [5](#S4.T5 "Table 5 ‣ 4.2\. Performance Analysis ‣ 4\. Results ‣ KVQuant:
    Towards 10 Million Context Length LLM Inference with KV Cache Quantization") shows
    kernel benchmarking results using a batch size of 1 for the 4-bit dense-and-sparse
    compression and matrix-vector kernel implementations. We show results across different
    sequence lengths to assess the performance of the kernels at different points
    during generation. We report latency benchmarked on an A6000 GPU and averaged
    over 1000 runs. The results show that for the Key and Value multiplications, we
    can achieve 1.1-1.2$\times$ latency savings, respectively, relative to the baseline.
    We have integrated these kernels into an end-to-end generation pipeline that is
    able to compress activations dynamically during inference, thereby achieving significant
    memory savings and allowing for either larger batch sizes or longer sequence lengths.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [5](#S4.T5 "表 5 ‣ 4.2\. 性能分析 ‣ 4\. 结果 ‣ KVQuant: 通过KV缓存量化实现1000万上下文长度LLM推断")
    显示了使用批量大小为1的4位稠密和稀疏压缩及矩阵-向量内核实现的内核基准测试结果。我们展示了不同序列长度的结果，以评估内核在生成过程中不同点的性能。我们报告了基于A6000
    GPU的延迟基准，并对1000次运行取了平均值。结果显示，对于键值乘法，我们可以实现相对于基准的1.1-1.2$\times$延迟节省。我们已将这些内核集成到端到端生成管道中，该管道能够在推断过程中动态压缩激活，从而实现显著的内存节省，并允许更大的批量大小或更长的序列长度。'
- en: 'Table 5. Average latency (in microseconds) for the Key and Value nuq4-1% kernels,
    benchmarked on an A6000 GPU for the LLaMA-7B model across different sequence lengths
    ($l$). fp16 matrix-vector multiplication latencies are included for reference,
    and the fp16 Key multiplication time also includes the time to apply RoPE to the
    newly appended Key vector.  Section [3.7](#S3.SS7 "3.7\. Kernel Implementation
    ‣ 3\. Method ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV
    Cache Quantization") and Appendix [O](#A15 "Appendix O Kernel Implementation Details
    ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization")
    provide additional details for our kernel implementation, and Table [20](#A15.T20
    "Table 20 ‣ Appendix O Kernel Implementation Details ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization") provides a detailed
    breakdown of kernel runtime.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5. Key 和 Value nuq4-1% 内核在不同序列长度 ($l$) 下的平均延迟（微秒），在 A6000 GPU 上对 LLaMA-7B
    模型进行基准测试。fp16 矩阵-向量乘法的延迟也包括在内，fp16 Key 乘法时间还包括应用 RoPE 到新附加的 Key 向量的时间。第 [3.7](#S3.SS7
    "3.7\. 内核实现 ‣ 3\. 方法 ‣ KVQuant: 实现 1000 万上下文长度 LLM 推理与 KV 缓存量化") 节和附录 [O](#A15
    "附录 O 内核实现细节 ‣ KVQuant: 实现 1000 万上下文长度 LLM 推理与 KV 缓存量化") 提供了我们内核实现的更多细节，表 [20](#A15.T20
    "表 20 ‣ 附录 O 内核实现细节 ‣ KVQuant: 实现 1000 万上下文长度 LLM 推理与 KV 缓存量化") 提供了内核运行时间的详细分解。'
- en: '| Activation | Operation | $l$=16K |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Activation | Operation | $l$=16K |'
- en: '| Key | fp16 Matvec | 66.4 | 116.1 | 402.3 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Key | fp16 Matvec | 66.4 | 116.1 | 402.3 |'
- en: '| Key | nuq4-1% | 60.2 | 96.7 | 344.1 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Key | nuq4-1% | 60.2 | 96.7 | 344.1 |'
- en: '| Value | fp16 Matvec | 56.0 | 104.2 | 389.3 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Value | fp16 Matvec | 56.0 | 104.2 | 389.3 |'
- en: '| Value | nuq4-1% | 40.8 | 85.8 | 293.4 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Value | nuq4-1% | 40.8 | 85.8 | 293.4 |'
- en: 4.3\. Pushing the Context Length Limit
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 推动上下文长度限制
- en: 'Table [6](#S4.T6 "Table 6 ‣ 4.3\. Pushing the Context Length Limit ‣ 4\. Results
    ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization")
    shows the KV cache memory requirements for 128K, 1M, and 10M sequence lengths,
    with the KV cache stored in fp16 as well as 4-bit, 3-bit, and 2-bit precision
    with KVQuant. As one can see, our method provides 3.7$\times$ KV cache compression
    (nuq4-1%) and enables serving the quantized LLaMA-65B model with a context length
    of 32K tokens on a single A100-80GB GPU (requiring 30GB for the model weights
    compressed to 4-bit, and 33GB for the KV cache when compressed with nuq4-1%),
    and even serving the LLaMA-7B model with a context length of 1M tokens on a single
    A100 GPU (requiring 3GB for the model weights in 4-bit precision and 66GB for
    the KV cache with nuq4-1%). Additionally, when considering an 8-GPU serving system,
    we enable serving the LLaMA-7B model with 10M context length (with nuq3-1%) or
    the LLaMA-65B model, with 1M context length (with nuq4-1%). While currently there
    are no models or datasets to measure accuracy with such large context sizes, our
    results on smaller context lengths show little degradation compared to baseline
    inference, demonstrating the benefits of our approach for enabling long sequence
    length inference.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [6](#S4.T6 "表 6 ‣ 4.3\. 推动上下文长度限制 ‣ 4\. 结果 ‣ KVQuant: 实现 1000 万上下文长度 LLM
    推理与 KV 缓存量化") 显示了 128K、1M 和 10M 序列长度的 KV 缓存内存需求，KV 缓存在 fp16 以及 4 位、3 位和 2 位精度下存储，使用
    KVQuant。如图所示，我们的方法提供了 3.7$\times$ 的 KV 缓存压缩（nuq4-1%），并使得可以在单个 A100-80GB GPU 上处理
    32K token 的量化 LLaMA-65B 模型（模型权重压缩到 4 位时需要 30GB，KV 缓存压缩到 nuq4-1% 时需要 33GB），甚至可以在单个
    A100 GPU 上处理 1M token 的 LLaMA-7B 模型（4 位精度的模型权重需要 3GB，nuq4-1% 的 KV 缓存需要 66GB）。此外，考虑到
    8 GPU 服务系统，我们可以在 nuq3-1% 下处理 10M 上下文长度的 LLaMA-7B 模型，或在 nuq4-1% 下处理 1M 上下文长度的 LLaMA-65B
    模型。尽管目前没有模型或数据集可以测量如此大上下文大小的准确性，但我们在较小上下文长度上的结果显示，与基线推理相比，性能几乎没有下降，展示了我们的方法在实现长序列长度推理方面的优势。'
- en: Table 6. Activation memory size (GB) for 128K, 1M, and 10M sequence length ($l$)
    for different LLaMA models. By compressing the KV cache to 3-bit precision, we
    can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB
    GPU, and we can also enable 10M context length inference with the LLaMA-7B model
    on an 8-GPU system.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6. 不同 LLaMA 模型在 128K、1M 和 10M 序列长度 ($l$) 下的激活内存大小（GB）。通过将 KV 缓存压缩到 3 位精度，我们可以在单个
    A100-80GB GPU 上实现 1M 上下文长度的 LLaMA-7B 模型推理，同时也可以在 8 GPU 系统上实现 10M 上下文长度的 LLaMA-7B
    模型推理。
- en: '| Model | Operation | $l$=10M |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Model | Operation | $l$=10M |'
- en: '| LLaMA-7B | fp16 | 32.0 | 244.1 | 2441.4 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | fp16 | 32.0 | 244.1 | 2441.4 |'
- en: '| nuq4-1% | 8.6 | 66.0 | 659.8 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| nuq4-1% | 8.6 | 66.0 | 659.8 |'
- en: '| nuq3-1% | 6.6 | 50.7 | 507.2 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| nuq3-1% | 6.6 | 50.7 | 507.2 |'
- en: '| nuq2-1% | 4.6 | 35.5 | 354.6 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| nuq2-1% | 4.6 | 35.5 | 354.6 |'
- en: '| LLaMA-65B | fp16 | 160.0 | 1220.7 | 12207 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-65B | fp16 | 160.0 | 1220.7 | 12207 |'
- en: '| nuq4-1% | 43.2 | 329.8 | 3297.5 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| nuq4-1% | 43.2 | 329.8 | 3297.5 |'
- en: '| nuq3-1% | 33.2 | 253.5 | 2534.6 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| nuq3-1% | 33.2 | 253.5 | 2534.6 |'
- en: '| nuq2-1% | 23.2 | 177.2 | 1771.6 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| nuq2-1% | 23.2 | 177.2 | 1771.6 |'
- en: 5\. Conclusion
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: As context lengths in LLMs increase, the KV cache activations surface as the
    dominant contributor to memory consumption. Quantization is a promising approach
    to reduce the size of KV cache activations, but prior solutions failed to represent
    activations accurately in ultra-low precisions, such as sub-4-bit. In contrast,
    we achieve accurate ultra-low precision KV cache quantization. By quantizing Keys
    per-channel before applying RoPE, we are able to better match the outlier distribution
    and mitigate the impacts of RoPE on quantization (due to it mixing pairs of channels
    which may have different average magnitudes). We use non-uniform quantization
    to better allocate the small number of quantization signposts at low precision.
    We observe significant accuracy improvements when employing dense-and-sparse quantization,
    particularly when detecting outliers at the same granularity as we compute quantization
    scaling factors. Crucially, we demonstrate that we can perform accurate calibration
    offline for Keys, as well as efficient online scaling factor and outlier threshold
    computation for Values. By leveraging these methods, we are able to enable accurate
    low-precision activation quantization, achieving 4.8x compression (nuq3-1% outliers)
    with only 0.1 perplexity degradation across different LLaMA, LLaMA-2, and Mistral
    models. Additionally, we show that Q-Norm improves accuracy for 2-bit quantization
    by helping mitigate distribution shift. Our methodology therefore supports inferring
    the LLaMA-7B model with a context length of 10M on an 8-GPU serving system. Through
    our efficient kernel implementation, we are able to show improved latency relative
    to the fp16 baseline, demonstrating how our method allows for improved latency
    in addition to the memory savings.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 LLMs 上下文长度的增加，KV 缓存激活成为内存消耗的主要因素。量化是一种有前景的方法，可以减少 KV 缓存激活的大小，但之前的解决方案未能在超低精度下准确表示激活，例如低于
    4 位。相比之下，我们实现了准确的超低精度 KV 缓存量化。通过在应用 RoPE 之前对每通道的键进行量化，我们能够更好地匹配异常值分布，并减轻 RoPE
    对量化的影响（因为它混合了可能具有不同平均幅度的通道对）。我们使用非均匀量化来更好地分配少量的量化标记。我们观察到，当使用稠密和稀疏量化时，特别是在以与计算量化缩放因子相同的粒度检测异常值时，精度有显著提高。关键的是，我们展示了我们可以离线对键进行准确校准，以及对值进行高效的在线缩放因子和异常值阈值计算。通过利用这些方法，我们能够实现准确的低精度激活量化，在不同的
    LLaMA、LLaMA-2 和 Mistral 模型中实现了 4.8 倍的压缩（nuq3-1% 异常值），同时 perplexity 降低了 0.1。我们还展示了
    Q-Norm 通过帮助减轻分布偏移来提高 2 位量化的精度。因此，我们的方法支持在 8-GPU 服务器系统上推断 LLaMA-7B 模型，具有 10M 的上下文长度。通过我们高效的内核实现，我们能够展示相对于
    fp16 基线的改进延迟，证明了我们的方法在内存节省的同时也能改善延迟。
- en: 6\. Limitations
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 限制
- en: While our work enables accurate long-context length inference by reducing the
    memory requirements, there is significant work required for training long context
    length models with greater than 100K context length. This work is orthogonal to
    our efforts, which are constrained to efficient inference with long context length
    models. Additionally, our latency benchmarking results currently focus on memory-bandwidth
    bound generation rather than prompt processing (where we need to compress multiple
    Keys and Values at once). In future work, we plan to develop dedicated efficient
    kernels for block Key/Value compression in order to efficiently compress multiple
    Keys and Values at once. Finally, in the current end-to-end implementation, there
    are inefficiencies in how memory allocation is handled for updating the sparse
    matrix (where the data corresponding to the previous tokens have to be copied
    when concatenating them with the data from the new token). In future work, we
    plan to optimize this by doing blocked allocation to avoid overheads from reallocating
    memory.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的工作通过减少内存需求实现了准确的长上下文长度推断，但训练上下文长度超过100K的长上下文长度模型仍需大量工作。这项工作与我们的努力方向正交，我们的工作仅限于对长上下文长度模型的高效推断。此外，我们的延迟基准测试结果目前侧重于内存带宽限制的生成，而非提示处理（我们需要一次压缩多个键和值）。在未来的工作中，我们计划开发专门的高效内核用于块键/值压缩，以便高效地一次压缩多个键和值。最后，在当前的端到端实现中，更新稀疏矩阵时内存分配的处理存在低效（在将数据与新标记的数据串联时，必须复制与先前标记对应的数据）。在未来的工作中，我们计划通过进行块分配来优化这一点，以避免重新分配内存带来的开销。
- en: 7\. Acknowledgements.
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7. 致谢。
- en: The authors would like to acknowledge Nicholas Lee for helpful discussions and
    feedback. We acknowledge gracious support from Intel, Furiosa, Apple, and Samsung
    SAIT. We also appreciate the support from Microsoft through their Accelerating
    Foundation Model Research, including great support from Sean Kuno. Furthermore,
    we appreciate support from Google Cloud, the Google TRC team, and specifically
    Jonathan Caton, and Prof. David Patterson. Prof. Keutzer’s lab is sponsored by
    the Intel corporation, Intel One-API, Intel VLAB team, the Intel One-API center
    of excellence, as well as funding through BDD and BAIR. We appreciate great feedback
    and support from Ellick Chan, Saurabh Tangri, Andres Rodriguez, and Kittur Ganesh.
    Sehoon Kim would like to acknowledge the support from the Korea Foundation for
    Advanced Studies (KFAS). Amir Gholami was supported through funding from Samsung
    SAIT. Michael W. Mahoney would also like to acknowledge a J. P. Morgan Chase Faculty
    Research Award as well as the DOE, NSF, and ONR. Our conclusions do not necessarily
    reflect the position or the policy of our sponsors, and no official endorsement
    should be inferred.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢 Nicholas Lee 的有益讨论和反馈。我们感谢 Intel、Furiosa、Apple 和 Samsung SAIT 的大力支持。我们还感谢
    Microsoft 通过其加速基础模型研究项目提供的支持，包括 Sean Kuno 的巨大支持。此外，我们感谢 Google Cloud、Google TRC
    团队，特别是 Jonathan Caton 和 David Patterson 教授的支持。Keutzer 教授的实验室由 Intel 公司、Intel One-API、Intel
    VLAB 团队、Intel One-API 卓越中心以及通过 BDD 和 BAIR 的资金赞助。我们感谢 Ellick Chan、Saurabh Tangri、Andres
    Rodriguez 和 Kittur Ganesh 的宝贵反馈和支持。Sehoon Kim 感谢 Korea Foundation for Advanced
    Studies (KFAS) 的支持。Amir Gholami 得到了 Samsung SAIT 的资助。Michael W. Mahoney 还感谢 J.
    P. Morgan Chase 教授研究奖以及 DOE、NSF 和 ONR。我们的结论不一定反映我们赞助商的立场或政策，且不应推断出任何官方背书。
- en: References
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1) Anthropic. Introducing claude 2.1, Nov 2023.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) Anthropic。介绍 Claude 2.1，2023年11月。
- en: (2) Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding
    and overcoming the challenges of efficient transformer quantization. arXiv preprint
    arXiv:2109.12948, 2021.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) Yelysei Bondarenko, Markus Nagel, 和 Tijmen Blankevoort。理解和克服高效变压器量化的挑战。arXiv
    预印本 arXiv:2109.12948，2021年。
- en: (3) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending
    context window of large language models via positional interpolation. arXiv preprint
    arXiv:2306.15595, 2023.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) Shouyuan Chen, Sherman Wong, Liangjian Chen, 和 Yuandong Tian。通过位置插值扩展大语言模型的上下文窗口。arXiv
    预印本 arXiv:2306.15595，2023年。
- en: '(4) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han,
    and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language
    models. arXiv preprint arXiv:2309.12307, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han,
    和 Jiaya Jia。Longlora：长上下文大语言模型的高效微调。arXiv 预印本 arXiv:2309.12307，2023年。
- en: '(5) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8
    (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339,
    2022.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (5) Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer. Llm. int8
    ()：大规模变压器的8位矩阵乘法。arXiv预印本 arXiv:2208.07339, 2022。
- en: '(6) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (6) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer. Qlora：高效微调量化LLMs。arXiv预印本
    arXiv:2305.14314, 2023。
- en: '(7) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias
    Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
    Spqr: A sparse-quantized representation for near-lossless llm weight compression.
    arXiv preprint arXiv:2306.03078, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (7) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias
    Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, 和 Dan Alistarh.
    Spqr：一种稀疏量化表示，用于近乎无损的LLM权重压缩。arXiv预印本 arXiv:2306.03078, 2023。
- en: '(8) Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney,
    and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural
    networks. Advances in neural information processing systems, 33:18518–18529, 2020.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (8) Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney,
    和 Kurt Keutzer. Hawq-v2：Hessian感知的神经网络跟踪加权量化。神经信息处理系统进展，33:18518–18529, 2020。
- en: '(9) Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer.
    Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pages 293–302, 2019.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (9) Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, 和 Kurt Keutzer.
    Hawq：具有混合精度的神经网络Hessian感知量化。在IEEE/CVF国际计算机视觉会议论文集，页码293–302, 2019。
- en: '(10) Goran Flegar and Enrique S Quintana-Ortí. Balanced csr sparse matrix-vector
    product on graphics processors. In Euro-Par 2017: Parallel Processing: 23rd International
    Conference on Parallel and Distributed Computing, Santiago de Compostela, Spain,
    August 28–September 1, 2017, Proceedings 23, pages 697–709\. Springer, 2017.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (10) Goran Flegar 和 Enrique S Quintana-Ortí. 在图形处理器上的平衡CSR稀疏矩阵-向量乘法。在Euro-Par
    2017：并行处理：第23届国际并行和分布式计算会议，西班牙圣地亚哥·德·孔波斯特拉，2017年8月28日至9月1日，会议录23，页码697–709。Springer,
    2017。
- en: (11) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
    Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason
    Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy
    Zou. A framework for few-shot language model evaluation, September 2021.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (11) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
    Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason
    Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, 和 Andy Zou.
    一个用于少样本语言模型评估的框架，2021年9月。
- en: '(12) Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng
    Gao. Model tells you what to discard: Adaptive kv cache compression for llms.
    arXiv preprint arXiv:2310.01801, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (12) Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, 和 Jianfeng
    Gao. 模型告诉你该丢弃什么：针对LLMs的自适应kv缓存压缩。arXiv预印本 arXiv:2310.01801, 2023。
- en: (13) Amir Gholami, Zhewei Yao, Sehoon Kim, Michael Mahoney, and Kurt Keutzer.
    Ai and memory wall. RiseLab Medium Post, 2021.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (13) Amir Gholami, Zhewei Yao, Sehoon Kim, Michael Mahoney, 和 Kurt Keutzer.
    人工智能与内存墙。RiseLab Medium Post, 2021。
- en: '(14) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang.
    Lm-infinite: Simple on-the-fly length generalization for large language models.
    arXiv preprint arXiv:2308.16137, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (14) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, 和 Sinong Wang. Lm-infinite：大型语言模型的简单即时长度泛化。arXiv预印本
    arXiv:2308.16137, 2023。
- en: (15) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
    Song, and Jacob Steinhardt. Measuring massive multitask language understanding.
    Proceedings of the International Conference on Learning Representations (ICLR),
    2021.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (15) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
    Song, 和 Jacob Steinhardt. 测量大规模多任务语言理解。国际学习表征会议（ICLR）论文集，2021。
- en: (16) Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon,
    and Dongsoo Lee. Rethinking channel dimensions to isolate outliers for low-bit
    weight quantization of large language models. arXiv preprint arXiv:2309.15531,
    2023.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (16) Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon,
    和 Dongsoo Lee. 重新思考通道维度以隔离异常值，用于大型语言模型的低位权重量化。arXiv预印本 arXiv:2309.15531, 2023。
- en: (17) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
    Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample,
    Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (17) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra
    Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume
    Lample, Lucile Saulnier, 等. Mistral 7b。arXiv预印本 arXiv:2310.06825, 2023。
- en: '(18) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
    Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization.
    arXiv preprint arXiv:2306.07629, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (18) Sehoon Kim、Coleman Hooper、Amir Gholami、镇东、秀玉李、盛申、迈克尔·W·马赫尼和库尔特·克伊策。Squeezellm：稠密与稀疏量化。arXiv
    预印本 arXiv:2306.07629，2023年。
- en: '(19) Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan
    Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al.
    Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017,
    2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (19) Sehoon Kim、Coleman Hooper、Thanakul Wattanawong、Minwoo Kang、若寒严、哈桑·根克、格雷斯·丁、齐景黄、库尔特·克伊策、迈克尔·W·马赫尼等。变换器推理的全栈优化：一项综述。arXiv
    预印本 arXiv:2302.14017，2023年。
- en: (20) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez,
    Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source
    llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction
    Following, 2023.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (20) 达成李、如林邵、安泽谢、英盛、联敏郑、约瑟夫·冈萨雷斯、伊昂·斯托伊卡、雪哲·马和浩张。开源大型语言模型的上下文长度能承诺多长时间？在 NeurIPS
    2023 指令调优与指令跟随研讨会中，2023年。
- en: '(21) Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Norm tweaking: High-performance
    low-bit quantization of large language models. arXiv preprint arXiv:2309.02784,
    2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (21) 梁李、庆元李、博张和香香楚。规范调整：大型语言模型的高性能低比特量化。arXiv 预印本 arXiv:2309.02784，2023年。
- en: '(22) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(22) 吉林、姜明唐、浩天唐、尚扬、兴宇党和宋涵。Awq: 激活感知权重量化用于大型语言模型压缩与加速。arXiv 预印本 arXiv:2306.00978，2023年。'
- en: '(23) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar
    Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free
    quantization aware training for large language models. arXiv preprint arXiv:2305.17888,
    2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (23) 泽春刘、巴拉斯·奥古兹、长生赵、厄尼·张、皮埃尔·斯托克、亚沙尔·梅赫达、杨洋·石、拉古拉曼·克里希那摩提和维卡斯·钱德拉。LLM-QAT：面向大型语言模型的数据无关量化感知训练。arXiv
    预印本 arXiv:2305.17888，2023年。
- en: '(24) Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo
    Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting
    the persistence of importance hypothesis for llm kv cache compression at test
    time. arXiv preprint arXiv:2305.17118, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (24) 子昌刘、阿迪提亚·德赛、方硕·廖、伟涛·王、维克托·谢、赵卓·徐、阿纳斯塔西奥斯·基里迪斯和安舒马利·施里瓦斯塔瓦。Scissorhands：利用重要性假设的持久性进行
    LLM KV 缓存压缩。arXiv 预印本 arXiv:2305.17118，2023年。
- en: '(25) Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir
    Braverman, Beidi Chen, and Xia Hu. Kivi: Plug-and-play 2bit kv cache quantization
    with streaming asymmetric quantization. 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (25) 子瑞刘、佳怡袁、洪晔金、绍琛钟、赵卓徐、弗拉基米尔·布拉弗曼、贝迪·陈和夏胡。Kivi：即插即用的2位 KV 缓存量化与流式非对称量化。2023年。
- en: '(26) Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access
    infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (26) Amirkeivan Mohtashami 和 Martin Jaggi。地标注意力：变换器的随机访问无限上下文长度。arXiv 预印本 arXiv:2305.16300，2023年。
- en: (27) OpenAI. New models and developer products announced at devday 2023, Nov
    2023.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (27) OpenAI。2023年开发者大会上宣布的新模型和开发者产品，2023年11月。
- en: '(28) Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo
    Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. arXiv
    preprint arXiv:2312.04985, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (28) Luka Ribar、Ivan Chelombiev、Luke Hudlass-Galley、Charlie Blake、Carlo Luschi
    和 Douglas Orr。Sparq 注意力：带宽高效的 LLM 推理。arXiv 预印本 arXiv:2312.04985，2023年。
- en: '(29) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian
    Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. arXiv preprint arXiv:2308.13137,
    2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (29) 温琦邵、孟钊陈、赵阳张、彭旭、李锐赵、志前李、开鹏张、彭高、于乔和平罗。Omniquant：面向大型语言模型的全方位标定量化。arXiv 预印本
    arXiv:2308.13137，2023年。
- en: '(30) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi
    Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput
    generative inference of large language models with a single gpu. In International
    Conference on Machine Learning, pages 31094–31116\. PMLR, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (30) 英盛、联敏郑、彬航袁、卓翰李、马克斯·利亚宾、贝迪·陈、佩尔西·梁、克里斯托弗·雷、伊昂·斯托伊卡和泽张。Flexgen：单 GPU 高吞吐量生成推理的大型语言模型。在国际机器学习会议上，第31094–31116页。PMLR，2023年。
- en: '(31) Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng
    Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing,
    568:127063, 2024.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(31) Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, 和 Yunfeng Liu。Roformer:
    增强的变压器与旋转位置嵌入。神经计算, 568:127063, 2024。'
- en: '(32) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(32) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, 等。Llama: 开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971, 2023。'
- en: '(33) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(33) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    等。Llama 2: 开放基础和微调对话模型。arXiv 预印本 arXiv:2307.09288, 2023。'
- en: '(34) Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang
    Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language
    models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145,
    2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(34) Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang
    Guo, 和 Xianglong Liu。离群值抑制+: 通过等效和优化的平移和缩放实现大型语言模型的准确量化。arXiv 预印本 arXiv:2304.09145,
    2023。'
- en: '(35) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang,
    Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit
    of low-bit transformer language models. Advances in Neural Information Processing
    Systems, 35:17402–17414, 2022.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(35) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang,
    Qi Zhang, Fengwei Yu, 和 Xianglong Liu。离群值抑制: 推动低位变压器语言模型的极限。神经信息处理系统进展, 35:17402–17414,
    2022。'
- en: '(36) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In International Conference on Machine Learning, pages 38087–38099\.
    PMLR, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(36) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, 和 Song
    Han。Smoothquant: 大型语言模型的准确且高效的后训练量化。国际机器学习大会, 页38087–38099\. PMLR, 2023。'
- en: '(37) Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
    Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H $\_2$ o:
    Heavy-hitter oracle for efficient generative inference of large language models.
    arXiv preprint arXiv:2306.14048, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(37) Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
    Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, 等。H $\_2$ o: 高效生成推理大型语言模型的重型预言机。arXiv
    预印本 arXiv:2306.14048, 2023。'
- en: '(38) Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng,
    Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit
    quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102,
    2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(38) Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng,
    Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, 和 Baris Kasikci。Atom: 低位量化以实现高效且准确的
    LLM 服务。arXiv 预印本 arXiv:2310.19102, 2023。'
- en: '(39) Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and
    Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise
    training. arXiv preprint arXiv:2309.10400, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(39) Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, 和 Sujian
    Li。Pose: 通过位置跳过训练高效扩展 LLM 的上下文窗口。arXiv 预印本 arXiv:2309.10400, 2023。'
- en: Appendix A RoPE Equation
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A RoPE 方程
- en: 'The rotation matrix for RoPE is provided in Equation [4](#A1.E4 "In Appendix
    A RoPE Equation ‣ KVQuant: Towards 10 Million Context Length LLM Inference with
    KV Cache Quantization"), where $\mathrm{c}$ is the current position in the sequence:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 'RoPE 的旋转矩阵见于方程 [4](#A1.E4 "附录 A RoPE 方程 ‣ KVQuant: 向 1000 万上下文长度 LLM 推理迈进，通过
    KV 缓存量化")，其中 $\mathrm{c}$ 是序列中的当前位置：'
- en: '| (4) |  | $$\begin{bmatrix}\mathrm{c}(n\theta_{1})&amp;-\mathrm{s}(n\theta_{1})&amp;\cdots&amp;0&amp;0\\
    \mathrm{s}(n\theta_{1})&amp;\mathrm{c}(n\theta_{1})&amp;\cdots&amp;0&amp;0\\'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '| (4) |  | $$\begin{bmatrix}\mathrm{c}(n\theta_{1})&amp;-\mathrm{s}(n\theta_{1})&amp;\cdots&amp;0&amp;0\\
    \mathrm{s}(n\theta_{1})&amp;\mathrm{c}(n\theta_{1})&amp;\cdots&amp;0&amp;0\\'
- en: \vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\vdots\\
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\vdots\\
- en: 0&amp;0&amp;\cdots&amp;\mathrm{c}(n\theta_{d/2})&amp;-\mathrm{s}(n\theta_{d/2})\\
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 0&amp;0&amp;\cdots&amp;\mathrm{c}(n\theta_{d/2})&amp;-\mathrm{s}(n\theta_{d/2})\\
- en: 0&amp;0&amp;\cdots&amp;\mathrm{s}(n\theta_{d/2})&amp;\mathrm{c}(n\theta_{d/2})\\
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 0&amp;0&amp;\cdots&amp;\mathrm{s}(n\theta_{d/2})&amp;\mathrm{c}(n\theta_{d/2})\\
- en: \end{bmatrix}$$ |  |
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: \end{bmatrix}$$ |  |
- en: 'The Query vectors computed at each iteration will have RoPE applied (to obtain
    $\tilde{Q}_{m}=R^{d}_{\theta,m}*Q_{m}$ is the element-wise multiplication operator
    (note that the formulation that we use matches the implementation in Huggingface
    for LLaMA, and it is a different but equivalent formulation to the element-wise
    expression in [[31](#bib.bib31)]):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中计算的 Query 向量将应用 RoPE（以获得 $\tilde{Q}_{m}=R^{d}_{\theta,m}*Q_{m}$ 是逐元素乘法运算符（注意我们使用的公式与
    Huggingface 对 LLaMA 的实现相匹配，并且它与 [[31](#bib.bib31)] 中逐元素表达式不同但等效）：
- en: '| (5) |  | $$\begin{bmatrix}x_{1}\\ x_{2}\\'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '| (5) |  | $$\begin{bmatrix}x_{1}\\ x_{2}\\'
- en: \vdots\\
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: x_{\frac{d}{2}}\\
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: x_{\frac{d}{2}}\\
- en: x_{\frac{d}{2}+1}\\
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: x_{\frac{d}{2}+1}\\
- en: \vdots\\
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: x_{d-1}\\
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: x_{d-1}\\
- en: x_{d}\end{bmatrix}\odot\begin{bmatrix}\mathrm{c}(\theta_{1}n)\\
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: x_{d}\end{bmatrix}\odot\begin{bmatrix}\mathrm{c}(\theta_{1}n)\\
- en: \mathrm{c}(\theta_{2}n)\\
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: \mathrm{c}(\theta_{2}n)\\
- en: \vdots\\
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: \mathrm{c}(\theta_{\frac{d}{2}}n)\\
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: \mathrm{c}(\theta_{\frac{d}{2}}n)\\
- en: \mathrm{c}(\theta_{1}n)\\
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: \mathrm{c}(\theta_{1}n)\\
- en: \vdots\\
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: \mathrm{c}(\theta_{\frac{d}{2}-1}n)\\
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: \mathrm{c}(\theta_{\frac{d}{2}-1}n)\\
- en: \mathrm{c}(\theta_{\frac{d}{2}}n)\\
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: \mathrm{c}(\theta_{\frac{d}{2}}n)\\
- en: \end{bmatrix}+\begin{bmatrix}-x_{\frac{d}{2}+1}\\
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: \end{bmatrix}+\begin{bmatrix}-x_{\frac{d}{2}+1}\\
- en: -x_{\frac{d}{2}+2}\\
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: -x_{\frac{d}{2}+2}\\
- en: \vdots\\
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: -x_{d}\\
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: -x_{d}\\
- en: x_{1}\\
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: x_{1}\\
- en: \vdots\\
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: x_{\frac{d}{2}-1}\\
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: x_{\frac{d}{2}-1}\\
- en: x_{\frac{d}{2}}\end{bmatrix}\odot\begin{bmatrix}\mathrm{s}(\theta_{1}n)\\
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: x_{\frac{d}{2}}\end{bmatrix}\odot\begin{bmatrix}\mathrm{s}(\theta_{1}n)\\
- en: \mathrm{s}(\theta_{2}n)\\
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: \mathrm{s}(\theta_{2}n)\\
- en: \vdots\\
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: \mathrm{s}(\theta_{\frac{d}{2}}n)\\
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: \mathrm{s}(\theta_{\frac{d}{2}}n)\\
- en: \mathrm{s}(\theta_{1}n)\\
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: \mathrm{s}(\theta_{1}n)\\
- en: \vdots\\
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: \mathrm{s}(\theta_{\frac{d}{2}-1}n)\\
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: \mathrm{s}(\theta_{\frac{d}{2}-1}n)\\
- en: \mathrm{s}(\theta_{\frac{d}{2}}n)\\
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: \mathrm{s}(\theta_{\frac{d}{2}}n)\\
- en: \end{bmatrix}$$ |  |
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: \end{bmatrix}$$ |  |
- en: By leveraging this element-wise implementation, we can apply RoPE on-the-fly
    to the Key activations (after dequantizing the Key activations and before multiplying
    them with the corresponding elements in the Query vector).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用这种逐元素实现，我们可以在 Key 激活上动态应用 RoPE（在对 Key 激活进行去量化后，并在与 Query 向量中的相应元素相乘之前）。
- en: Appendix B Key and Value Dynamic Range
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 键值动态范围
- en: 'Figure [5](#A2.F5 "Figure 5 ‣ Appendix B Key and Value Dynamic Range ‣ KVQuant:
    Towards 10 Million Context Length LLM Inference with KV Cache Quantization") shows
    the portion of the elements contained within difference percentages of the dynamic
    range for both Keys and Values. The majority of values ($\sim$ 99%) are contained
    in a small portion of the dynamic range, and a small portion of numerical outliers
    skew the dynamic range that must be represented. This motivates our dense-and-sparse
    approach which removes numerical outliers and stores them in a separate sparse
    matrix, thereby restricting the range that needs to be represented in the dense
    component.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [5](#A2.F5 "图 5 ‣ 附录 B 键值动态范围 ‣ KVQuant: 通过 KV 缓存量化实现 1000 万上下文长度 LLM 推理")
    显示了 Keys 和 Values 的动态范围中不同百分比内包含的元素部分。绝大多数值（$\sim$ 99%）都包含在动态范围的小部分内，而少量的数值异常值会扭曲必须表示的动态范围。这促使我们采用密集和稀疏的方法，将数值异常值去除并存储在单独的稀疏矩阵中，从而限制了需要在密集组件中表示的范围。'
- en: '![Refer to caption](img/72fcf304caf9bea2e6ac3e3bc9e37daf.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/72fcf304caf9bea2e6ac3e3bc9e37daf.png)'
- en: Figure 5. Distribution of the magnitude of elements of Key (Pre-RoPE) and Value
    activations for different layers of LLaMA-7B, computed on a single sample with
    sequence length 2K from the Wikitext-2 dataset. The normalized magnitude is computed
    by dividing by the largest magnitude value in that layer. As one can see, for
    both Key and Value activations, the majority of values lie in a small portion
    of the dynamic range, with a few numerical outliers skewing the dynamic range
    (and thereby reducing the fidelity when quantizing to low precision).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5. 不同层的 Key（Pre-RoPE）和 Value 激活的元素幅度分布，计算基于来自 Wikitext-2 数据集的单个样本，序列长度为 2K。归一化幅度通过除以该层中最大的幅度值来计算。如图所示，对于
    Key 和 Value 激活，大多数值位于动态范围的小部分内，少量的数值异常值扭曲了动态范围（从而在量化到低精度时降低了保真度）。
- en: Appendix C Derivation for Sensitivity Analysis
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 灵敏度分析的推导
- en: The following derivation is adapted from [[18](#bib.bib18)], with the only difference
    being that it is estimating sensitivity with respect to activations rather than
    gradients. Let the activation at a given layer for input data $X$ represent the
    quantization error. Note that the subsequent derivations assume that the activation
    and gradient matrices are all flattened to one dimension.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 以下推导改编自 [[18](#bib.bib18)]，唯一的不同之处在于它是估计相对于激活的敏感性，而不是梯度。设定输入数据 $X$ 的某一层的激活表示量化误差。注意，后续的推导假设激活和梯度矩阵都被展平为一维。
- en: 'By performing Taylor expansion of the loss with respect to the Hessian for
    data $X$, we get:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对数据 $X$ 的 Hessian 进行泰勒展开，我们得到：
- en: '| (6) |  | $\displaystyle\mathcal{L}(A_{Q})$ |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\displaystyle\mathcal{L}(A_{Q})$ |  |'
- en: 'Assuming that the loss has converged to a local minimum, $g$ [[18](#bib.bib18)]:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 假设损失已经收敛到局部最小值，$g$ [[18](#bib.bib18)]：
- en: '| (7) |  | $E=\big{(}A-A_{Q}\big{)}^{\top}H\big{(}A-A_{Q}\big{)}.$ |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $E=\big{(}A-A_{Q}\big{)}^{\top}H\big{(}A-A_{Q}\big{)}.$ |  |'
- en: Prior work [[9](#bib.bib9), [8](#bib.bib8)] has used either the largest Hessian
    eigenvalue or the Hessian trace to estimate the sharpness of the loss landscape
    with respect to a particular layer. However, the full Hessian is prohibitively
    expensive to compute for LLMs, and it is challenging to even estimate the Hessian
    eigenvalues. We therefore aim to leverage the Fisher information approximation
    for the Hessian since it is easier to compute. Given activation $A$.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的工作 [[9](#bib.bib9), [8](#bib.bib8)] 使用了最大的 Hessian 特征值或 Hessian 跟踪来估计损失景观相对于特定层的尖锐度。然而，对于
    LLM 来说，计算完整的 Hessian 是极其昂贵的，甚至估计 Hessian 特征值也具有挑战性。因此，我们旨在利用 Fisher 信息的近似值，因为它更容易计算。给定激活
    $A$。
- en: Appendix D Per-Channel Key Quantization Ablations
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 每通道键量化消融
- en: 'We report results in Table [7](#A4.T7 "Table 7 ‣ Appendix D Per-Channel Key
    Quantization Ablations ‣ KVQuant: Towards 10 Million Context Length LLM Inference
    with KV Cache Quantization") demonstrating the perplexity for different KV cache
    compression ratios, showing that per-channel quantization for Keys and per-token
    quantization for Values outperforms the standard per-token quantization approach
    for both Keys and Values, yielding an improvement of 3.88 perplexity for the LLaMA-7B
    model at 3-bit precision. This demonstrates the benefits of per-channel Key quantization
    to mitigate the large outlier channels in Keys. Additionally, although there are
    per-channel outliers in Values, we observe that per-channel quantization for Values
    actually performs worse than per-token quantization. We hypothesize that this
    behavior is because per-channel Value quantization leads to greater error accumulation
    in particular output values (since the result of the attention scores multiplied
    by one channel of the Values will be localized to a single value in the output
    vector), which leads to greater quantization error at later model layers. Another
    concurrent work, KIVI [[25](#bib.bib25)], observes similar behavior for per-channel
    Value quantization, which they attribute to the fact that per-token Value quantization
    confines the error to each token. Assuming that the output is a weighted sum of
    only a few important tokens (as only a few attention scores are large), a perturbation
    in these tokens can lead to significant degradation.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表[7](#A4.T7 "Table 7 ‣ Appendix D Per-Channel Key Quantization Ablations
    ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization")中报告了不同
    KV 缓存压缩比的困惑度结果，显示每通道量化键和每个令牌量化值在标准每个令牌量化方法中都表现优于键和值，且在 3 位精度下对 LLaMA-7B 模型的困惑度提升了
    3.88。这展示了每通道键量化在减少键中大的离群通道方面的好处。此外，尽管值中存在每通道的离群值，但我们观察到每通道量化值实际上表现得比每个令牌量化更差。我们假设这种行为是因为每通道值量化会导致特定输出值中的误差累积增加（由于注意力分数与每个通道的值相乘的结果会局限于输出向量中的一个单独值），这导致后续模型层的量化误差增加。另一项相关工作，KIVI
    [[25](#bib.bib25)]，观察到类似的每通道值量化行为，他们将其归因于每个令牌值量化将误差限制在每个令牌内。假设输出是仅几个重要令牌的加权和（因为只有少数注意力分数较大），这些令牌中的扰动可能会导致显著的降级。'
- en: 'Table 7. Ablation Study: Perplexity comparison of per-token and per-channel
    quantization for KV cache activations for LLaMA-7B. PT refers to per-token quantization,
    and PC refers to per-channel quantization.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7. 消融研究：LLaMA-7B 的 KV 缓存激活的每个令牌和每个通道量化的困惑度比较。PT 代表每个令牌量化，PC 代表每个通道量化。
- en: '| Datatype | Key Dim. | Value Dim. | Perplexity | KV Cache Size (GB) |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | 关键维度 | 值维度 | 困惑度 | KV 缓存大小 (GB) |'
- en: '| Seqlen 128K |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Seqlen 128K |'
- en: '| fp16 | - | - | 5.68 | 32.0 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| fp16 | - | - | 5.68 | 32.0 |'
- en: '| int3 | PT | PT | 10.87 | 6.0 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| int3 | PT | PT | 10.87 | 6.0 |'
- en: '| int3 | PC | PC | 8.19 | 6.0 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| int3 | PC | PC | 8.19 | 6.0 |'
- en: '| int3 | PC | PT | 6.99 | 6.0 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| int3 | PC | PT | 6.99 | 6.0 |'
- en: Appendix E Pre-RoPE Key Quantization Ablations
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E Pre-RoPE 关键量化消融实验
- en: 'As shown in Table [8](#A5.T8 "Table 8 ‣ Appendix E Pre-RoPE Key Quantization
    Ablations ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache
    Quantization"), pre-RoPE Key quantization achieves higher accuracy than post-RoPE
    quantization, with an improvement of 0.65 perplexity for 3-bit quantization with
    the LLaMA-7B model. These results show that the rotary positional embeddings make
    Key quantization more challenging due to mixing pairs of channels with different
    magnitudes. Pre-RoPE quantization thereby allows for more accurate quantization
    at low precision.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [8](#A5.T8 "表 8 ‣ 附录 E Pre-RoPE 关键量化消融实验 ‣ KVQuant: 朝着 1000 万上下文长度 LLM 推理与
    KV 缓存量化的方向") 所示，pre-RoPE 关键量化的准确度高于 post-RoPE 量化，对于 LLaMA-7B 模型的 3-bit 量化提高了 0.65
    的困惑度。这些结果表明，旋转位置嵌入使得关键量化更具挑战性，因为它混合了不同幅度的通道对。Pre-RoPE 量化因此能够在低精度下实现更准确的量化。'
- en: 'Table 8. Ablation Study: Perplexity comparison of Pre-RoPE and post-RoPE Key
    quantization for LLaMA-7B (using per-channel Key quantization and per-token Value
    quantization). Pre-RoPE quantization leads to significant improvement (see Section [3.2](#S3.SS2
    "3.2\. Pre-RoPE Key Quantization ‣ 3\. Method ‣ KVQuant: Towards 10 Million Context
    Length LLM Inference with KV Cache Quantization") for more details).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8. 消融研究：LLaMA-7B 的 Pre-RoPE 和 post-RoPE 关键量化的困惑度比较（使用每通道关键量化和每标记值量化）。Pre-RoPE
    量化导致了显著的改善（有关更多细节，请参见 第 [3.2](#S3.SS2 "3.2\. Pre-RoPE 关键量化 ‣ 3\. 方法 ‣ KVQuant:
    朝着 1000 万上下文长度 LLM 推理与 KV 缓存量化的方向") 节）。'
- en: '| Datatype | Scheme | Perplexity | KV Cache Size (GB) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | 方案 | 困惑度 | KV 缓存大小 (GB) |'
- en: '| Seqlen 128K |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Seqlen 128K |'
- en: '| fp16 | - | 5.68 | 32.0 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| fp16 | - | 5.68 | 32.0 |'
- en: '| int3 | post-RoPE | 6.99 | 6.0 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| int3 | post-RoPE | 6.99 | 6.0 |'
- en: '| int3 | pre-RoPE | 6.34 | 6.0 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| int3 | pre-RoPE | 6.34 | 6.0 |'
- en: Appendix F Sensitivity-Weighted Non-Uniform Quantization Ablations
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 灵敏度加权非均匀量化消融实验
- en: 'Table [9](#A6.T9 "Table 9 ‣ Appendix F Sensitivity-Weighted Non-Uniform Quantization
    Ablations ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache
    Quantization") shows perplexity evaluation results across different LLaMA, LLaMA-2,
    and Mistral models on Wikitext-2 using nf3, nuq3, and nuq3 without using sensitivity-weighting.
    These results demonstrate the benefits of our sensitivity-weighted non-uniform
    quantization approach, relative to NormalFloat quantization [[6](#bib.bib6)],
    as we achieve consistent accuracy improvements of up to 0.32 perplexity across
    different models (with particularly pronounced improvements for larger models).
    For 3-bit quantization with the LLaMA-7B model, we observe a 0.33 perplexity improvement
    relative to uniform quantization. The gains relative to uniform quantization are
    particularly noticeable for 3-bit and 2-bit quantization, where the benefits of
    non-uniform quantization are more pronounced due to the reduced precision. These
    results also demonstrate the necessity of our sensitivity-weighting approach in
    order to derive performant non-uniform datatypes using a k-means based approach.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [9](#A6.T9 "表 9 ‣ 附录 F 灵敏度加权非均匀量化消融实验 ‣ KVQuant: 朝着 1000 万上下文长度 LLM 推理与 KV
    缓存量化的方向") 显示了使用 nf3、nuq3 和不使用灵敏度加权的 nuq3 在 Wikitext-2 上对不同 LLaMA、LLaMA-2 和 Mistral
    模型的困惑度评估结果。这些结果展示了我们灵敏度加权非均匀量化方法的优势，相比于 NormalFloat 量化 [[6](#bib.bib6)]，我们在不同模型中实现了一致的准确率提升，困惑度提高了最高
    0.32（对于较大的模型特别明显）。对于 LLaMA-7B 模型的 3-bit 量化，我们观察到相对于均匀量化的困惑度提升为 0.33。相对于均匀量化，3-bit
    和 2-bit 量化的增益特别明显，因为非均匀量化在减少精度的情况下带来的好处更为突出。这些结果还展示了我们灵敏度加权方法的必要性，以便使用基于 k-means
    的方法得到高效的非均匀数据类型。'
- en: 'Figure [6](#A6.F6 "Figure 6 ‣ Appendix F Sensitivity-Weighted Non-Uniform Quantization
    Ablations ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache
    Quantization") shows the relationship between the Fisher information and the normalized
    activation values for the LLaMA-7B model. For the Value matrices, we observe that
    the average Fisher information is higher close to the middle, which leads to quantization
    centroids being pulled closer to the center of the distribution. For the Key matrices,
    we observe that the average sensitivity across different magnitudes is relatively
    constant, which leads to wider spacing for the centroids. These results show that
    using Fisher information to derive a sensitivity-weighted per-layer datatype allows
    for better representation of Keys and Values.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [6](#A6.F6 "图 6 ‣ 附录 F 灵敏度加权非均匀量化消融 ‣ KVQuant: 面向 1000 万上下文长度 LLM 推理的 KV
    缓存量化") 显示了 Fisher 信息与 LLaMA-7B 模型的标准化激活值之间的关系。对于 Value 矩阵，我们观察到平均 Fisher 信息在接近中间的位置较高，这导致量化中心点被拉近到分布中心。对于
    Key 矩阵，我们观察到不同量级的平均灵敏度相对恒定，这导致中心点的间隔更宽。这些结果表明，使用 Fisher 信息来推导灵敏度加权的每层数据类型，可以更好地表示
    Keys 和 Values。'
- en: '![Refer to caption](img/d1e0f30fea2093a3f463342481010852.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d1e0f30fea2093a3f463342481010852.png)'
- en: Figure 6. Relationship between average Fisher information and normalized activation
    values for the 3-bit quantized LLaMA-7B model with 1% outliers, shown for layer
    29\. a) Fisher information versus normalized activation values for Keys (with
    1% outliers). b) Fisher information versus normalized activation values for Values
    (with 1% outliers). These results demonstrate how sensitivity-weighted k-means
    shifts the quantization signposts inward relative to NormalFloat (with more condensed
    signposts across a slightly narrower range). Additionally, for the Value matrix,
    the values close to 0 have a higher average sensitivity. This leads to more condensed
    signpost assignment relative to the Key matrix, demonstrating how our non-uniform
    approach derives accurate datatypes for layers with differing sensitivity characteristics.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6. 对于 3 位量化的 LLaMA-7B 模型（具有 1% 异常值），图示了平均 Fisher 信息与标准化激活值之间的关系，显示了第 29 层。a)
    Fisher 信息与 Keys 的标准化激活值之间的关系（具有 1% 异常值）。b) Fisher 信息与 Values 的标准化激活值之间的关系（具有 1%
    异常值）。这些结果展示了灵敏度加权 k-means 如何将量化标志点相对于 NormalFloat 向内移动（在略窄的范围内有更多的紧凑标志点）。此外，对于
    Value 矩阵，接近 0 的值具有更高的平均灵敏度。这导致相对于 Key 矩阵的标志点分配更为紧凑，展示了我们非均匀方法如何为具有不同灵敏度特征的层推导准确的数据类型。
- en: 'Table 9. Ablation Study: Ablation of our sensitivity-weighted non-uniform datatype
    for different models on Wikitext-2. All experiments use pre-RoPE per-channel quantization
    for Keys and per-token quantization for Values (meaning that all configurations
    are the same as in KVQuant, except for the datatype). We compare against both
    uniform (int3) and non-uniform (nf3) [[6](#bib.bib6)] approaches, as well as with
    using “unweighted” k-means (i.e., not sensitivity-weighted) to compute the non-uniform
    quantization signposts. Note that there is slight variation in average bitwidth
    across models due to the differing hidden dimensions. Results report perplexity
    with 2K/4K/8K sequence length for LLaMA, LLaMA-2, and Mistral, respectively.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9. 消融研究：在 Wikitext-2 上对不同模型的灵敏度加权非均匀数据类型的消融。所有实验使用预RoPE的每通道量化用于 Keys 和每标记量化用于
    Values（即所有配置与 KVQuant 相同，仅数据类型除外）。我们将其与均匀（int3）和非均匀（nf3）[[6](#bib.bib6)] 方法进行比较，以及使用“无权重”k-means（即不加权灵敏度）来计算非均匀量化的标志点。请注意，由于隐藏维度的不同，模型间的平均位宽存在轻微变化。结果报告了
    LLaMA、LLaMA-2 和 Mistral 的 2K/4K/8K 序列长度的困惑度。
- en: '| Method | LLaMA-7b | LLaMA-13b | LLaMA-30b | LLaMA-65b | LLaMA-2-7b | LLaMA-2-13b
    | LLaMA-2-70b | Mistral-7b | Avg. Num. Bits |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LLaMA-7b | LLaMA-13b | LLaMA-30b | LLaMA-65b | LLaMA-2-7b | LLaMA-2-13b
    | LLaMA-2-70b | Mistral-7b | 平均比特数 |'
- en: '| baseline | 5.68 | 5.09 | 4.10 | 3.53 | 5.12 | 4.57 | 3.12 | 4.76 | 16 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| baseline | 5.68 | 5.09 | 4.10 | 3.53 | 5.12 | 4.57 | 3.12 | 4.76 | 16 |'
- en: '| int3 | 6.34 | 5.62 | 4.69 | 4.16 | 6.10 | 5.59 | 3.46 | 5.52 | 3.00-3.01
    |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| int3 | 6.34 | 5.62 | 4.69 | 4.16 | 6.10 | 5.59 | 3.46 | 5.52 | 3.00-3.01
    |'
- en: '| nf3 | 6.05 | 5.42 | 4.51 | 3.84 | 5.55 | 5.15 | 3.27 | 5.13 | 3.00-3.01 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| nf3 | 6.05 | 5.42 | 4.51 | 3.84 | 5.55 | 5.15 | 3.27 | 5.13 | 3.00-3.01 |'
- en: '| nuq3 (unweighted) | 6.84 | 6.16 | 5.37 | 4.57 | 8.52 | 7.66 | 3.67 | 5.29
    | 3.00-3.01 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 (无权重) | 6.84 | 6.16 | 5.37 | 4.57 | 8.52 | 7.66 | 3.67 | 5.29 | 3.00-3.01
    |'
- en: '| nuq3 | 6.01 | 5.34 | 4.41 | 3.74 | 5.49 | 4.83 | 3.26 | 5.03 | 3.00-3.01
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | 6.01 | 5.34 | 4.41 | 3.74 | 5.49 | 4.83 | 3.26 | 5.03 | 3.00-3.01
    |'
- en: Appendix G Per-Vector Dense-and-Sparse Quantization Ablations
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 每向量密集与稀疏量化消融研究
- en: 'Table [10](#A7.T10 "Table 10 ‣ Appendix G Per-Vector Dense-and-Sparse Quantization
    Ablations ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache
    Quantization") shows the performance improvements we observe when isolating a
    small portion of outliers and storing them in a sparse format. We provide results
    both with using a single per-matrix outlier threshold, as well as with applying
    separate outlier thresholds per-vector. In particular, we see greater improvements
    by employing outlier detection with a different threshold per-channel for Keys
    and per-token for Values. This provides additional benefits since some values
    which would be considered outliers for the entire matrix are not actually outliers
    within a particular channel (so they are not hard to quantize). It is therefore
    better to directly target the outliers that will skew the quantization range for
    a particular channel. By removing 1% of outliers using per-vector thresholds,
    we can achieve an additional 0.25 reduction in perplexity for the LLaMA-7b model
    at 3 bits, thereby enabling 3-bit quantization with under 0.1 degradation in perplexity.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [10](#A7.T10 "表 10 ‣ 附录 G 每向量密集与稀疏量化消融研究 ‣ KVQuant: 致力于 1000 万上下文长度 LLM 推理与
    KV 缓存量化") 显示了在隔离少量异常值并以稀疏格式存储时观察到的性能提升。我们提供了使用单一每矩阵异常值阈值的结果，以及应用每向量单独阈值的结果。特别是，通过使用不同的通道阈值进行异常值检测和每标记的异常值检测，我们观察到更大的改进。这提供了额外的好处，因为某些被认为是整个矩阵的异常值在特定通道内实际上并不是异常值（因此不难量化）。因此，更好的是直接针对那些会影响特定通道量化范围的异常值。通过使用每向量阈值去除
    1% 的异常值，我们可以在 3 位的 LLaMA-7b 模型中实现额外 0.25 的困惑度减少，从而使 3 位量化在困惑度降低不足 0.1 的情况下得以实现。'
- en: 'Table 10. Ablation Study: Perplexity comparison of different outlier isolation
    methods for LLaMA-7B. Per-vector outlier detection allows for significant accuracy
    improvements relative to per-tensor outlier detection. All nuq4 and nuq3 experiments
    use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE).
    “PV” refers to using per-vector outlier thresholds, and “PM” refers to using a
    single per-matrix outlier threshold.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10. 消融研究：LLaMA-7B 不同异常值隔离方法的困惑度比较。相对于每个张量的异常值检测，每个向量的异常值检测能显著提高准确性。所有 nuq4
    和 nuq3 实验使用每个标记的量化来处理值（Values）和每个通道的量化来处理键（Keys）（pre-RoPE）。“PV”指的是使用每个向量的异常值阈值，而“PM”指的是使用单一的每矩阵异常值阈值。
- en: '| Datatype | % Outliers | Outlier Dim. | Perplexity | KV Cache Size (GB) |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | 异常值百分比 | 异常值维度 | 困惑度 | KV 缓存大小（GB） |'
- en: '| Seqlen 128K |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| Seqlen 128K |'
- en: '| fp16 | - | - | 5.68 | 32.0 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| fp16 | - | - | 5.68 | 32.0 |'
- en: '| nuq3 | - | - | 6.01 | 6.0 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | - | - | 6.01 | 6.0 |'
- en: '| nuq3 | 0.1% | PM | 5.94 | 6.1 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | 0.1% | PM | 5.94 | 6.1 |'
- en: '| nuq3 | 0.1% | PV | 5.86 | 6.1 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | 0.1% | PV | 5.86 | 6.1 |'
- en: '| nuq3 | 1% | PM | 5.86 | 6.6 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | 1% | PM | 5.86 | 6.6 |'
- en: '| nuq3 | 1% | PV | 5.76 | 6.6 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | 1% | PV | 5.76 | 6.6 |'
- en: Appendix H Q-Norm Ablations
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H Q-Norm 消融研究
- en: 'Table [11](#A8.T11 "Table 11 ‣ Appendix H Q-Norm Ablations ‣ KVQuant: Towards
    10 Million Context Length LLM Inference with KV Cache Quantization") provides
    perplexity results for LLaMA-7B and LLaMA-13B with and without Q-Norm. As shown
    in the table, Q-Norm provides noticeable accuracy improvements for 2-bit quantization,
    but it doesn’t provide significant accuracy improvements for 3-bit or 4-bit quantization.
    Figure [7](#A8.F7 "Figure 7 ‣ Appendix H Q-Norm Ablations ‣ KVQuant: Towards 10
    Million Context Length LLM Inference with KV Cache Quantization") demonstrates
    how the minimization of distribution shift provided by Q-Norm leads to reduced
    quantization error (particularly at later layers in the network).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [11](#A8.T11 "表 11 ‣ 附录 H Q-Norm 消融研究 ‣ KVQuant: 致力于 1000 万上下文长度 LLM 推理与
    KV 缓存量化") 提供了 LLaMA-7B 和 LLaMA-13B 在使用和不使用 Q-Norm 时的困惑度结果。如表中所示，Q-Norm 为 2 位量化提供了显著的准确性提升，但对于
    3 位或 4 位量化未提供显著的准确性提升。图 [7](#A8.F7 "图 7 ‣ 附录 H Q-Norm 消融研究 ‣ KVQuant: 致力于 1000
    万上下文长度 LLM 推理与 KV 缓存量化") 展示了 Q-Norm 提供的分布偏移最小化如何导致量化误差的减少（尤其是在网络的后续层）。'
- en: 'Additionally, we experimented with using per-vector Q-Norm, where we normalize
    each channel for Keys and each token for Values to ensure the distribution has
    the same mean and standard deviation post-quantization. For Keys, per-vector Q-Norm
    requires offline computation of the required normalization per-channel; and for
    Values, per-vector Q-Norm requires online computation of the required normalization
    per-token. With per-vector Q-Norm, the normalization parameters must also be stored
    separately from the per-vector outlier thresholds and cannot be fused with the
    NUQ datatype. Table [11](#A8.T11 "Table 11 ‣ Appendix H Q-Norm Ablations ‣ KVQuant:
    Towards 10 Million Context Length LLM Inference with KV Cache Quantization") also
    compares employing per-vector Q-Norm with per-matrix Q-Norm. Although we observe
    similar accuracy with per-vector Q-Norm when incorporating dense-and-sparse quantization,
    we observe worsened performance when we aren’t using dense-and-sparse quantization.
    We attribute this to the larger relative shift when applying normalization per-vector,
    as well as the impacts that significant changes to the centroids can have on outlier
    values (in the case where we aren’t using dense-and-sparse quantization, large
    perturbations in these outlier values can lead to significant accuracy loss).
    The need to only slightly tweak normalization parameters when combating distribution
    shift is similar to [[21](#bib.bib21)], which describes how weight quantization
    can be improved by slightly adjusting layernorm parameters to avoid distribution
    shift. Due to the improved performance of per-matrix Q-Norm for dense-only quantization
    (as well as potential inference overheads with per-vector Q-Norm from having to
    separately rescale the non-uniform centroids per-vector and compute normalization
    statistics on-the-fly for Values), we focus on per-matrix Q-Norm. However, there
    is potential to improve accuracy through fine-grained normalization in future
    work; for example,  Figure [8](#A13.F8 "Figure 8 ‣ Appendix M Additional Long
    Sequence Length Evaluation ‣ KVQuant: Towards 10 Million Context Length LLM Inference
    with KV Cache Quantization") demonstrates how per-vector Q-Norm provides improved
    performance for the LLaMA-2-70B-32K model when evaluating perplexity on long context
    lengths.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们尝试了使用向量级 Q-Norm，其中我们对 Keys 的每个通道和 Values 的每个 token 进行归一化，以确保量化后的分布具有相同的均值和标准差。对于
    Keys，向量级 Q-Norm 需要离线计算每个通道所需的归一化；而对于 Values，向量级 Q-Norm 需要在线计算每个 token 所需的归一化。使用向量级
    Q-Norm 时，归一化参数还必须与向量级异常值阈值分开存储，不能与 NUQ 数据类型合并。表 [11](#A8.T11 "Table 11 ‣ Appendix
    H Q-Norm Ablations ‣ KVQuant: Towards 10 Million Context Length LLM Inference
    with KV Cache Quantization") 还比较了使用向量级 Q-Norm 和矩阵级 Q-Norm。尽管在结合密集和稀疏量化时，我们观察到向量级
    Q-Norm 的准确性相似，但在未使用密集和稀疏量化时，我们观察到性能恶化。我们将此归因于在应用每向量归一化时相对偏移更大，以及在没有使用密集和稀疏量化时，质心的显著变化对异常值的影响（在未使用密集和稀疏量化的情况下，这些异常值的大幅扰动可能导致显著的准确性损失）。在应对分布偏移时，仅需略微调整归一化参数类似于
    [[21](#bib.bib21)]，该文描述了如何通过略微调整 layernorm 参数来改进权重量化，以避免分布偏移。由于矩阵级 Q-Norm 对仅密集量化的改进性能（以及向量级
    Q-Norm 可能带来的推理开销，因为必须分别重新缩放非均匀质心并在线计算 Values 的归一化统计量），我们重点关注矩阵级 Q-Norm。然而，通过细粒度归一化有潜力提高准确性；例如，图 [8](#A13.F8
    "Figure 8 ‣ Appendix M Additional Long Sequence Length Evaluation ‣ KVQuant: Towards
    10 Million Context Length LLM Inference with KV Cache Quantization") 演示了在评估长上下文长度的困惑度时，向量级
    Q-Norm 为 LLaMA-2-70B-32K 模型提供了改进的性能。'
- en: 'Table 11. Ablation Study: Perplexity with LLaMA-7B and LLaMA-13B with and without
    Q-Norm (including results for both per-matrix (“PM”) and per-vector (“PV”) Q-Norm).
    For 4-bit and 3-bit quantization, Q-Norm provides minimal perplexity improvements;
    however, at 2-bit quantization, Q-Norm improves performance by reducing distribution
    shift. Per-vector Q-Norm performs similarly to per-matrix Q-Norm when incorporating
    dense-and-sparse quantization, and performs worse than per-matrix Q-Norm without
    dense-and-sparse quantization. Note that there is slight variation in average
    bitwidth across models due to the differing hidden dimensions.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11. 消融研究：LLaMA-7B 和 LLaMA-13B 在有无 Q-Norm 的情况下的困惑度（包括矩阵级（“PM”）和向量级（“PV”）Q-Norm
    的结果）。对于 4-bit 和 3-bit 量化，Q-Norm 提供的困惑度改进最小；然而，在 2-bit 量化时，Q-Norm 通过减少分布偏移来提升性能。向量级
    Q-Norm 在结合密集和稀疏量化时表现类似于矩阵级 Q-Norm，而在不使用密集和稀疏量化时表现不如矩阵级 Q-Norm。请注意，由于隐藏维度的不同，各模型的平均比特宽度略有变化。
- en: '| Datatype | Norm | LLaMA-7B | LLaMA-13B | Avg. Bits (KV Cache) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | 规范 | LLaMA-7B | LLaMA-13B | 平均位数 (KV 缓存) |'
- en: '| fp16 | - | 5.68 | 5.09 | 16 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| fp16 | - | 5.68 | 5.09 | 16 |'
- en: '| nuq4 | - | 5.73 | 5.15 | 4.00 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| nuq4 | - | 5.73 | 5.15 | 4.00 |'
- en: '| nuq4 | PV | 5.76 | 5.16 | 4.01 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| nuq4 | PV | 5.76 | 5.16 | 4.01 |'
- en: '| nuq4 | PM | 5.74 | 5.14 | 4.00 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| nuq4 | PM | 5.74 | 5.14 | 4.00 |'
- en: '| nuq4-1% | - | 5.70 | 5.11 | 4.32-4.33 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| nuq4-1% | - | 5.70 | 5.11 | 4.32-4.33 |'
- en: '| nuq4-1% | PV | 5.70 | 5.10 | 4.33 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| nuq4-1% | PV | 5.70 | 5.10 | 4.33 |'
- en: '| nuq4-1% | PM | 5.70 | 5.10 | 4.32-4.33 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| nuq4-1% | PM | 5.70 | 5.10 | 4.32-4.33 |'
- en: '| nuq3 | - | 6.01 | 5.34 | 3.00 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | - | 6.01 | 5.34 | 3.00 |'
- en: '| nuq3 | PV | 6.11 | 5.42 | 3.01 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | PV | 6.11 | 5.42 | 3.01 |'
- en: '| nuq3 | PM | 5.97 | 5.34 | 3.00 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | PM | 5.97 | 5.34 | 3.00 |'
- en: '| nuq3-1% | - | 5.76 | 5.15 | 3.32-3.33 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| nuq3-1% | - | 5.76 | 5.15 | 3.32-3.33 |'
- en: '| nuq3-1% | PV | 5.75 | 5.16 | 3.33 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| nuq3-1% | PV | 5.75 | 5.16 | 3.33 |'
- en: '| nuq3-1% | PM | 5.75 | 5.16 | 3.32-3.33 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| nuq3-1% | PM | 5.75 | 5.16 | 3.32-3.33 |'
- en: '| nuq2 | - | 8.70 | 7.50 | 2.00 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| nuq2 | - | 8.70 | 7.50 | 2.00 |'
- en: '| nuq2 | PV | 34.81 | 18.56 | 2.01 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| nuq2 | PV | 34.81 | 18.56 | 2.01 |'
- en: '| nuq2 | PM | 8.17 | 7.29 | 2.00 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| nuq2 | PM | 8.17 | 7.29 | 2.00 |'
- en: '| nuq2-1% | - | 6.24 | 5.50 | 2.32-2.33 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| nuq2-1% | - | 6.24 | 5.50 | 2.32-2.33 |'
- en: '| nuq2-1% | PV | 6.06 | 5.41 | 2.33 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| nuq2-1% | PV | 6.06 | 5.41 | 2.33 |'
- en: '| nuq2-1% | PM | 6.06 | 5.40 | 2.32-2.33 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| nuq2-1% | PM | 6.06 | 5.40 | 2.32-2.33 |'
- en: '![Refer to caption](img/6889b76778472f44804e5671c628efe7.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6889b76778472f44804e5671c628efe7.png)'
- en: 'Figure 7. Quantization error for 2-bit quantization with and without Q-Norm
    for the LLaMA-7B model, evaluated on a sample of length 2K from Wikitext-2\. a)
    Quantization error for Keys (with 0% outliers). b) Quantization error for Values
    (with 0% outliers). c) Quantization error for Keys (with 1% outliers). d) Quantization
    error for Values (with 1% outliers). As one can see, using Q-Norm is helpful with/without
    dense-and-sparse quantization and results in improved quantization error, especially
    for later layers. See Table [11](#A8.T11 "Table 11 ‣ Appendix H Q-Norm Ablations
    ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization")
    for the corresponding perplexity results which clearly show the benefit of using
    Q-Norm.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7. 具有和不具有 Q-Norm 的 2 位量化的量化误差，评估样本长度为 2K 来自 Wikitext-2。 a) Keys 的量化误差（0%
    异常值）。 b) Values 的量化误差（0% 异常值）。 c) Keys 的量化误差（1% 异常值）。 d) Values 的量化误差（1% 异常值）。可以看到，使用
    Q-Norm 对于密集和稀疏量化都是有帮助的，并且结果在量化误差上有所改善，特别是在后续层。有关 Q-Norm 的好处，请参见 表 [11](#A8.T11
    "表 11 ‣ 附录 H Q-Norm 消融 ‣ KVQuant: 通过 KV 缓存量化实现 1000 万上下文长度的 LLM 推理")，其中明确展示了使用
    Q-Norm 的好处。'
- en: Appendix I Calibration Ablations
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I 校准消融
- en: 'Table [12](#A9.T12 "Table 12 ‣ Appendix I Calibration Ablations ‣ KVQuant:
    Towards 10 Million Context Length LLM Inference with KV Cache Quantization") shows
    accuracy results when using offline calibration for computing the scaling factors
    for the Keys. For 4-bit quantization, we observe no accuracy loss when calibrating
    scaling factors offline. For 3-bit quantization, we observe minor accuracy degradation
    when not employing outlier extraction methods. However, if we remove a small percentage
    of outliers, then the accuracy with offline calibration is the same as computing
    the scaling factors online per-channel during evaluation. This demonstrates that
    when incorporating outlier extraction methods, we are better able to perform offline
    calibration due to reduced sensitivity to outliers (either to outliers during
    calibration that exaggerate the quantization range, or to outliers during evaluation
    that cannot be represented accurately if there weren’t large outliers observed
    during calibration).'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [12](#A9.T12 "表 12 ‣ 附录 I 校准消融 ‣ KVQuant: 通过 KV 缓存量化实现 1000 万上下文长度的 LLM 推理")
    显示了使用离线校准计算 Keys 的缩放因子时的准确度结果。对于 4 位量化，我们观察到离线校准时没有准确度损失。对于 3 位量化，如果不使用异常值提取方法，我们会观察到轻微的准确度下降。然而，如果我们去除少量异常值，那么离线校准的准确度与在评估期间逐通道在线计算缩放因子的准确度相同。这表明，当采用异常值提取方法时，我们能够更好地进行离线校准，因为对异常值的敏感性降低（无论是校准期间异常值夸大了量化范围，还是评估期间异常值无法准确表示，如果在校准期间没有观察到大异常值的话）。'
- en: 'Table [13](#A9.T13 "Table 13 ‣ Appendix I Calibration Ablations ‣ KVQuant:
    Towards 10 Million Context Length LLM Inference with KV Cache Quantization") shows
    the runtime for the $topk$ operation can also be performed efficiently on the
    CPU, so we can actually run this operation in parallel with the subsequent linear
    layer matrix-vector operations on the GPU (which is possible by computing the
    Value projection before the Key and Query projections). This allows us to compress
    the activations dynamically without added runtime overhead, thereby enabling online
    scaling factor computation for the Value tensors.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [13](#A9.T13 "表 13 ‣ 附录 I 校准消融 ‣ KVQuant: 通过 KV 缓存量化实现 1000 万上下文长度 LLM 推理")
    显示 $topk$ 操作在 CPU 上也可以高效执行，因此我们实际上可以在 GPU 上与后续线性层矩阵-向量操作并行执行此操作（这是通过在 Key 和 Query
    投影之前计算 Value 投影实现的）。这使我们能够动态压缩激活而不增加运行时间开销，从而实现对 Value 张量的在线缩放因子计算。'
- en: 'Table [14](#A9.T14 "Table 14 ‣ Appendix I Calibration Ablations ‣ KVQuant:
    Towards 10 Million Context Length LLM Inference with KV Cache Quantization") shows
    the perplexity of the LLaMA-7B model using different numbers of samples during
    calibration. The results show that perplexity is similar across the range of the
    number of samples tested for each bit width. This shows that the calibration step
    does not require a large number of calibration samples to attain high accuracy.
    Additionally, Table [15](#A9.T15 "Table 15 ‣ Appendix I Calibration Ablations
    ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization")
    shows how both Fisher information computation and calibration (including k-means)
    per-layer take only a few minutes for the LLaMA-65B model on a typical server
    machine. Even if we perform calibration sequentially for each layer, the entire
    calibration process would take a maximum of 6 hours for the LLaMA-65B model at
    4-bit precision.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [14](#A9.T14 "表 14 ‣ 附录 I 校准消融 ‣ KVQuant: 通过 KV 缓存量化实现 1000 万上下文长度 LLM 推理")
    显示 LLaMA-7B 模型在校准时使用不同数量样本的困惑度。结果表明，在每个位宽测试的样本数量范围内，困惑度相似。这表明校准步骤不需要大量校准样本即可获得高精度。此外，表
    [15](#A9.T15 "表 15 ‣ 附录 I 校准消融 ‣ KVQuant: 通过 KV 缓存量化实现 1000 万上下文长度 LLM 推理") 显示
    Fisher 信息计算和每层校准（包括 k-means）在典型服务器上仅需几分钟。即使逐层进行校准，LLaMA-65B 模型在 4 位精度下的整个校准过程也最多需
    6 小时。'
- en: 'Table 12. Ablation Study: Model accuracy when using offline calibration for
    Keys with LLaMA-7B. When incorporating outlier detection, offline calibration
    for Keys is able to perform comparably with online calibration. All nf4 and nf3
    experiments use per-token quantization for Values and per-channel quantization
    for Keys (pre-RoPE), and experiments with outliers use per-vector outlier detection.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12. 消融研究：使用 LLaMA-7B 对 Keys 进行离线校准时的模型准确性。当结合异常值检测时，Keys 的离线校准能够与在线校准表现相当。所有
    nf4 和 nf3 实验使用按标记量化的 Values 和按通道量化的 Keys（pre-RoPE），且异常值实验使用按向量异常值检测。
- en: '| Datatype | % Outliers | Perplexity | Perplexity |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | % 异常值 | 困惑度 | 困惑度 |'
- en: '| (Online for K) | (Offline for K) |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|（K 的在线） | （K 的离线） |'
- en: '| fp16 | - | 5.68 | 5.68 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| fp16 | - | 5.68 | 5.68 |'
- en: '| nuq4 | - | 5.73 | 5.73 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| nuq4 | - | 5.73 | 5.73 |'
- en: '| nuq4 | 1% | 5.70 | 5.70 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| nuq4 | 1% | 5.70 | 5.70 |'
- en: '| nuq3 | - | 5.96 | 6.01 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | - | 5.96 | 6.01 |'
- en: '| nuq3 | 1% | 5.77 | 5.76 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | 1% | 5.77 | 5.76 |'
- en: Table 13. $topk$ operation can be performed efficiently on the CPU; we can therefore
    run this operation in parallel with subsequent linear layer operations on the
    GPU to compress the activations dynamically without added overhead. Note that
    the CPU runtime includes the time for copying the vector to the CPU.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13. $topk$ 操作可以高效地在 CPU 上执行；因此，我们可以与 GPU 上的后续线性层操作并行运行此操作，从而动态压缩激活而无需额外开销。请注意，CPU
    运行时间包括将向量复制到 CPU 的时间。
- en: '| Operation | Device | Outlier % | Runtime (ms) |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 设备 | 异常值 % | 运行时间 (毫秒) |'
- en: '| QKV Projection | GPU | - | 0.308 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| QKV 投影 | GPU | - | 0.308 |'
- en: '| $topk$ | GPU | 1% | 0.073 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| $topk$ | GPU | 1% | 0.073 |'
- en: '| $topk$ | CPU | 1% | 0.059 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| $topk$ | CPU | 1% | 0.059 |'
- en: '| QKV Projection / $topk$ (Fused) | GPU / CPU | 1% | 0.309 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| QKV 投影 / $topk$（融合） | GPU / CPU | 1% | 0.309 |'
- en: 'Table 14. Ablation Study: Perplexity on Wikitext-2 for the LLaMA-7B model when
    using different number of samples of length 2K during calibration.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14. 消融研究：LLaMA-7B 模型在使用不同数量的 2K 长度样本进行校准时的困惑度。
- en: '| Method | Number of Samples |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 样本数量 |'
- en: '| 1 | 2 | 4 | 8 | 16 | 32 | 64 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 4 | 8 | 16 | 32 | 64 |'
- en: '| nuq4 | 5.75 | 5.74 | 5.73 | 5.75 | 5.73 | 5.74 | 5.74 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| nuq4 | 5.75 | 5.74 | 5.73 | 5.75 | 5.73 | 5.74 | 5.74 |'
- en: '| nuq4-1% | 5.69 | 5.70 | 5.70 | 5.70 | 5.70 | 5.70 | 5.70 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| nuq4-1% | 5.69 | 5.70 | 5.70 | 5.70 | 5.70 | 5.70 | 5.70 |'
- en: '| nuq3 | 6.00 | 6.00 | 5.95 | 5.98 | 6.01 | 6.02 | 5.98 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | 6.00 | 6.00 | 5.95 | 5.98 | 6.01 | 6.02 | 5.98 |'
- en: '| nuq3-1% | 5.75 | 5.77 | 5.76 | 5.76 | 5.76 | 5.77 | 5.76 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| nuq3-1% | 5.75 | 5.77 | 5.76 | 5.76 | 5.76 | 5.77 | 5.76 |'
- en: '| nuq2 | 8.34 | 8.32 | 8.26 | 8.70 | 8.61 | 8.93 | 8.45 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| nuq2 | 8.34 | 8.32 | 8.26 | 8.70 | 8.61 | 8.93 | 8.45 |'
- en: '| nuq2-1% | 6.20 | 6.22 | 6.23 | 6.23 | 6.24 | 6.22 | 6.25 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| nuq2-1% | 6.20 | 6.22 | 6.23 | 6.23 | 6.24 | 6.22 | 6.25 |'
- en: Table 15. Runtime for computing Fisher information as well as for calibration
    (including k-means) with 16 samples for LLaMA-65B quantization. Runtime for computing
    Fisher information was computed on an 8-GPU A100-80GB system. Runtime for calibration
    (including k-means) was performed on an Intel Xeon Gold 6442Y CPU, and is shown
    for a single layer. Note that calibration is independent for each layer, so it
    can be easily parallelized.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 表15. 计算Fisher信息以及标定（包括k-means）的运行时间，使用16个样本进行LLaMA-65B量化。计算Fisher信息的运行时间是在一个8-GPU
    A100-80GB系统上计算的。标定（包括k-means）的运行时间是在Intel Xeon Gold 6442Y CPU上进行的，仅显示单层的时间。请注意，标定是每层独立的，因此可以轻松并行化。
- en: '| Operation | Runtime (minutes) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 运行时间（分钟） |'
- en: '| Computing Fisher Information | 2.8 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 计算Fisher信息 | 2.8 |'
- en: '| 4-bit Calibration Per-Layer (including k-means) | 4.5 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 4位标定每层（包括k-means） | 4.5 |'
- en: '| 3-bit Calibration Per-Layer (including k-means) | 2.7 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 3位标定每层（包括k-means） | 2.7 |'
- en: '| 2-bit Calibration Per-Layer (including k-means) | 1.9 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 2位标定每层（包括k-means） | 1.9 |'
- en: Appendix J Additional Experimental Details
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录J 额外实验细节
- en: 'For our empirical evaluation, we use 16 calibration samples of sequence length
    2K from the Wikitext-2 training set (as well as the corresponding gradients) to
    derive the per-channel scaling factors and zero-points, to derive the non-uniform
    datatypes for both Keys and Values, and to estimate layer-wise sensitivity for
    mixed-precision experiments. We measured perplexity on both Wikitext-2 and on
    C4 using a sequence length equal to the maximum context length of the model (2K
    for LLaMA, 4K for LLaMA-2, and 8K for Mistral-7B). For baseline experiments, we
    use post-RoPE quantization, both since this is required from an efficiency perspective
    without a dedicated kernel implementation, and because it provides better accuracy
    when quantizing Keys per-token as shown in Appendix [L](#A12 "Appendix L Post-RoPE
    Per-Token Quantization Ablation ‣ KVQuant: Towards 10 Million Context Length LLM
    Inference with KV Cache Quantization").'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的经验评估中，我们使用Wikitext-2训练集中的16个标定样本，序列长度为2K（以及相应的梯度），以推导每通道的缩放因子和零点，推导Keys和Values的非均匀数据类型，并估计混合精度实验的层级敏感性。我们在Wikitext-2和C4上使用等于模型最大上下文长度的序列长度来测量困惑度（LLaMA为2K，LLaMA-2为4K，Mistral-7B为8K）。对于基线实验，我们使用后RoPE量化，这不仅因为在没有专用内核实现的情况下从效率的角度来看这是必需的，而且因为它在对Keys进行每令牌量化时提供了更好的准确性，如附录[L](#A12
    "附录 L 后RoPE每令牌量化消融 ‣ KVQuant：面向1000万上下文长度LLM推理的KV缓存量化")所示。
- en: We make several assumptions in order to estimate average bit widths and KV cache
    sizes for different approaches. We compute these estimates assuming a sequence
    length of 128K. For integer quantization, we assume a low-precision integer offset
    and a 16-bit scaling factor, whereas for NormalFloat and NUQ we assume that the
    zero-point and offset are each 16-bit. For the sparse matrices, 32-bit integers
    are assumed for the per-token indices (since we need to support long sequence
    lengths), and the elements and per-element indices are assumed to be 16-bit. This
    means that for CSR, the rows are assumed to be 32-bit and the columns and values
    are assumed to be 16-bit, whereas for CSC, the columns are assumed to be 32-bit
    and the rows and values are assumed to be 16-bit.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做了一些假设，以估算不同方法的平均位宽和KV缓存大小。我们在假设序列长度为128K的情况下计算这些估算值。对于整数量化，我们假设低精度整数偏移量和16位缩放因子，而对于NormalFloat和NUQ，我们假设零点和偏移量各为16位。对于稀疏矩阵，我们假设每令牌索引为32位整数（因为我们需要支持长序列长度），元素和每元素索引为16位。这意味着对于CSR，行假设为32位，列和值假设为16位，而对于CSC，列假设为32位，行和值假设为16位。
- en: Appendix K Full Perplexity Evaluation and MMLU Evaluation
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录K 完整困惑度评估和MMLU评估
- en: 'Tables [16](#A11.T16 "Table 16 ‣ Appendix K Full Perplexity Evaluation and
    MMLU Evaluation ‣ KVQuant: Towards 10 Million Context Length LLM Inference with
    KV Cache Quantization") and [17](#A11.T17 "Table 17 ‣ Appendix K Full Perplexity
    Evaluation and MMLU Evaluation ‣ KVQuant: Towards 10 Million Context Length LLM
    Inference with KV Cache Quantization") show perplexity evaluation results across
    different LLaMA, LLaMA-2, and Mistral models on Wikitext-2 and C4, respectively.
    These results demonstrate the benefits of our approach for KV cache compression
    across different model sizes as well as across different language modeling datasets.
    Table [18](#A11.T18 "Table 18 ‣ Appendix K Full Perplexity Evaluation and MMLU
    Evaluation ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV
    Cache Quantization") also provides zero-shot evaluation results on MMLU for LLaMA-7B
    and LLaMA-13B, demonstrating how we can maintain accuracy even for 3-bit KV cache
    quantization [[15](#bib.bib15)]. We used the Language Model Evaluation Harness
    to run zero-shot evaluation across all MMLU tasks [[11](#bib.bib11)].'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [16](#A11.T16 "表 16 ‣ 附录 K 完整困惑度评估和 MMLU 评估 ‣ KVQuant：通过 KV 缓存量化迈向 1000 万上下文长度
    LLM 推理") 和 [17](#A11.T17 "表 17 ‣ 附录 K 完整困惑度评估和 MMLU 评估 ‣ KVQuant：通过 KV 缓存量化迈向
    1000 万上下文长度 LLM 推理") 展示了不同 LLaMA、LLaMA-2 和 Mistral 模型在 Wikitext-2 和 C4 上的困惑度评估结果。这些结果展示了我们的方法在不同模型规模及不同语言建模数据集上的
    KV 缓存压缩的优势。表 [18](#A11.T18 "表 18 ‣ 附录 K 完整困惑度评估和 MMLU 评估 ‣ KVQuant：通过 KV 缓存量化迈向
    1000 万上下文长度 LLM 推理") 还提供了 LLaMA-7B 和 LLaMA-13B 在 MMLU 上的零-shot 评估结果，展示了即使对于 3
    位 KV 缓存量化，我们也能保持准确性 [[15](#bib.bib15)]。我们使用了语言模型评估工具来运行所有 MMLU 任务的零-shot 评估 [[11](#bib.bib11)]。
- en: Table 16. Evaluation of our method for different models using the perplexity
    (PPL) measured on Wikitext-2\. Non-uniform quantization results are using pre-RoPE
    per-channel quantization for Keys. “gs64/128” refers to baseline experiments using
    grouping with group size 64/128\. 2-bit results leverage per-matrix Q-Norm. Note
    that there is slight variation in average bitwidth across models due to the differing
    hidden dimensions.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 表 16. 使用在 Wikitext-2 上测量的困惑度（PPL）评估我们的方法对不同模型的效果。非均匀量化结果使用的是 Keys 的预 RoPE 每通道量化。“gs64/128”指的是使用组大小为
    64/128 的基线实验。2 位结果利用了每矩阵 Q-Norm。注意，由于隐藏维度的不同，各模型之间的平均比特宽度存在轻微差异。
- en: '| Method | LLaMA-7b | LLaMA-13b | LLaMA-30b | LLaMA-65b | LLaMA-2-7b | LLaMA-2-13b
    | LLaMA-2-70b | Mistral-7b | Avg. Num. Bits |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LLaMA-7b | LLaMA-13b | LLaMA-30b | LLaMA-65b | LLaMA-2-7b | LLaMA-2-13b
    | LLaMA-2-70b | Mistral-7b | 平均比特数 |'
- en: '| baseline | 5.68 | 5.09 | 4.10 | 3.53 | 5.12 | 4.57 | 3.12 | 4.76 | 16 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 5.68 | 5.09 | 4.10 | 3.53 | 5.12 | 4.57 | 3.12 | 4.76 | 16 |'
- en: '| int4 | 5.98 | 5.32 | 4.34 | 3.73 | 5.66 | 5.01 | 3.31 | 4.97 | 4.00-4.01
    |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| int4 | 5.98 | 5.32 | 4.34 | 3.73 | 5.66 | 5.01 | 3.31 | 4.97 | 4.00-4.01
    |'
- en: '| int4-gs128 | 5.77 | 5.16 | 4.16 | 3.57 | 5.32 | 4.71 | 3.16 | 4.82 | 4.16
    |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| int4-gs128 | 5.77 | 5.16 | 4.16 | 3.57 | 5.32 | 4.71 | 3.16 | 4.82 | 4.16
    |'
- en: '| int4-gs64 | 5.73 | 5.14 | 4.14 | 3.56 | 5.25 | 4.66 | 3.14 | 4.80 | 4.31
    |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| int4-gs64 | 5.73 | 5.14 | 4.14 | 3.56 | 5.25 | 4.66 | 3.14 | 4.80 | 4.31
    |'
- en: '| nf4 | 5.87 | 5.23 | 4.25 | 3.63 | 5.47 | 4.90 | 3.22 | 4.91 | 4.00-4.01 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| nf4 | 5.87 | 5.23 | 4.25 | 3.63 | 5.47 | 4.90 | 3.22 | 4.91 | 4.00-4.01 |'
- en: '| nf4-gs128 | 5.77 | 5.17 | 4.17 | 3.58 | 5.30 | 4.71 | 3.16 | 4.83 | 4.16
    |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| nf4-gs128 | 5.77 | 5.17 | 4.17 | 3.58 | 5.30 | 4.71 | 3.16 | 4.83 | 4.16
    |'
- en: '| nuq4 | 5.73 | 5.15 | 4.16 | 3.57 | 5.18 | 4.63 | 3.15 | 4.81 | 4.00-4.02
    |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| nuq4 | 5.73 | 5.15 | 4.16 | 3.57 | 5.18 | 4.63 | 3.15 | 4.81 | 4.00-4.02
    |'
- en: '| + 0.1% outliers | 5.71 | 5.12 | 4.15 | 3.55 | 5.16 | 4.61 | 3.14 | 4.80 |
    4.04-4.06 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| + 0.1% 异常值 | 5.71 | 5.12 | 4.15 | 3.55 | 5.16 | 4.61 | 3.14 | 4.80 | 4.04-4.06
    |'
- en: '| + 0.5% outliers | 5.70 | 5.11 | 4.12 | 3.54 | 5.14 | 4.60 | 3.13 | 4.78 |
    4.16-4.19 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| + 0.5% 异常值 | 5.70 | 5.11 | 4.12 | 3.54 | 5.14 | 4.60 | 3.13 | 4.78 | 4.16-4.19
    |'
- en: '| + 1.0% outliers | 5.70 | 5.11 | 4.12 | 3.54 | 5.14 | 4.59 | 3.13 | 4.78 |
    4.32-4.35 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| + 1.0% 异常值 | 5.70 | 5.11 | 4.12 | 3.54 | 5.14 | 4.59 | 3.13 | 4.78 | 4.32-4.35
    |'
- en: '| int3 | 10.87 | 8.69 | 6.82 | 6.37 | 22.71 | 18.26 | 7.68 | 7.64 | 3.00-3.01
    |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| int3 | 10.87 | 8.69 | 6.82 | 6.37 | 22.71 | 18.26 | 7.68 | 7.64 | 3.00-3.01
    |'
- en: '| int3-gs128 | 6.17 | 5.47 | 4.44 | 3.78 | 6.15 | 5.34 | 3.33 | 5.16 | 3.15
    |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| int3-gs128 | 6.17 | 5.47 | 4.44 | 3.78 | 6.15 | 5.34 | 3.33 | 5.16 | 3.15
    |'
- en: '| int3-gs64 | 5.93 | 5.29 | 4.26 | 3.66 | 5.64 | 4.98 | 3.23 | 5.00 | 3.30
    |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| int3-gs64 | 5.93 | 5.29 | 4.26 | 3.66 | 5.64 | 4.98 | 3.23 | 5.00 | 3.30
    |'
- en: '| nf3 | 7.33 | 6.21 | 5.46 | 4.44 | 9.96 | 9.50 | 4.06 | 6.30 | 3.00-3.01 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| nf3 | 7.33 | 6.21 | 5.46 | 4.44 | 9.96 | 9.50 | 4.06 | 6.30 | 3.00-3.01 |'
- en: '| nf3-gs128 | 6.26 | 5.52 | 4.54 | 3.83 | 6.21 | 5.43 | 3.38 | 5.23 | 3.15
    |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| nf3-gs128 | 6.26 | 5.52 | 4.54 | 3.83 | 6.21 | 5.43 | 3.38 | 5.23 | 3.15
    |'
- en: '| nuq3 | 6.01 | 5.34 | 4.41 | 3.74 | 5.49 | 4.83 | 3.26 | 5.03 | 3.00-3.02
    |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | 6.01 | 5.34 | 4.41 | 3.74 | 5.49 | 4.83 | 3.26 | 5.03 | 3.00-3.02
    |'
- en: '| + 0.1% outliers | 5.86 | 5.28 | 4.27 | 3.64 | 5.32 | 4.71 | 3.23 | 4.96 |
    3.04-3.06 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| + 0.1% outliers | 5.86 | 5.28 | 4.27 | 3.64 | 5.32 | 4.71 | 3.23 | 4.96 |
    3.04-3.06 |'
- en: '| + 0.5% outliers | 5.79 | 5.15 | 4.19 | 3.60 | 5.22 | 4.66 | 3.17 | 4.86 |
    3.16-3.19 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| + 0.5% outliers | 5.79 | 5.15 | 4.19 | 3.60 | 5.22 | 4.66 | 3.17 | 4.86 |
    3.16-3.19 |'
- en: '| + 1.0% outliers | 5.76 | 5.15 | 4.17 | 3.59 | 5.20 | 4.64 | 3.16 | 4.84 |
    3.32-3.35 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| + 1.0% outliers | 5.76 | 5.15 | 4.17 | 3.59 | 5.20 | 4.64 | 3.16 | 4.84 |
    3.32-3.35 |'
- en: '| int2 | 11779 | 69965 | 1470 | 7272 | 4708 | 3943 | 976 | 573 | 2.00-2.01
    |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| int2 | 11779 | 69965 | 1470 | 7272 | 4708 | 3943 | 976 | 573 | 2.00-2.01
    |'
- en: '| int2-gs128 | 37.37 | 41.77 | 16.49 | 13.63 | 117.88 | 93.09 | 18.31 | 51.96
    | 2.14 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| int2-gs128 | 37.37 | 41.77 | 16.49 | 13.63 | 117.88 | 93.09 | 18.31 | 51.96
    | 2.14 |'
- en: '| int2-gs64 | 11.09 | 9.84 | 6.60 | 5.54 | 25.69 | 26.83 | 5.93 | 12.47 | 2.28
    |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| int2-gs64 | 11.09 | 9.84 | 6.60 | 5.54 | 25.69 | 26.83 | 5.93 | 12.47 | 2.28
    |'
- en: '| nf2 | 3210.5 | 5785.6 | 2044.2 | 5367.3 | 13601 | 4035.6 | 3680.3 | 902.51
    | 2.00-2.01 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| nf2 | 3210.5 | 5785.6 | 2044.2 | 5367.3 | 13601 | 4035.6 | 3680.3 | 902.51
    | 2.00-2.01 |'
- en: '| nf2-gs128 | 351.23 | 141.19 | 60.97 | 31.69 | 634.59 | 642.44 | 71.21 | 252.85
    | 2.14 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| nf2-gs128 | 351.23 | 141.19 | 60.97 | 31.69 | 634.59 | 642.44 | 71.21 | 252.85
    | 2.14 |'
- en: '| nuq2 | 8.17 | 7.29 | 7.05 | 27.17 | 9.75 | 29.25 | 4.98 | 7.33 | 2.00-2.02
    |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| nuq2 | 8.17 | 7.29 | 7.05 | 27.17 | 9.75 | 29.25 | 4.98 | 7.33 | 2.00-2.02
    |'
- en: '| + 0.1% outliers | 6.91 | 5.84 | 4.94 | 4.13 | 6.47 | 5.54 | 3.76 | 6.05 |
    2.04-2.06 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| + 0.1% outliers | 6.91 | 5.84 | 4.94 | 4.13 | 6.47 | 5.54 | 3.76 | 6.05 |
    2.04-2.06 |'
- en: '| + 0.5% outliers | 6.26 | 5.52 | 4.48 | 3.86 | 5.67 | 5.05 | 3.35 | 5.33 |
    2.16-2.19 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| + 0.5% outliers | 6.26 | 5.52 | 4.48 | 3.86 | 5.67 | 5.05 | 3.35 | 5.33 |
    2.16-2.19 |'
- en: '| + 1.0% outliers | 6.06 | 5.40 | 4.43 | 3.76 | 5.50 | 4.92 | 3.29 | 5.16 |
    2.32-2.35 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| + 1.0% outliers | 6.06 | 5.40 | 4.43 | 3.76 | 5.50 | 4.92 | 3.29 | 5.16 |
    2.32-2.35 |'
- en: Table 17. Evaluation of our method for different models using the perplexity
    (PPL) measured on C4\. PC-K refers to using per-channel quantization for Keys.
    “gs64/128” refers to baseline experiments using grouping with group size 64/128\.
    2-bit results leverage per-matrix Q-Norm. Note that there is slight variation
    in average bitwidth across models due to the differing hidden dimensions.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: Table 17. 评估我们的方法在不同模型上的困惑度（PPL），测量数据来源于 C4。PC-K 指使用每通道量化的方法。 “gs64/128”指使用分组的基线实验，分组大小为
    64/128。2 位结果采用每矩阵 Q-Norm。请注意，由于隐藏维度不同，各模型的平均比特宽度略有差异。
- en: '| Method | LLaMA-7b | LLaMA-13b | LLaMA-30b | LLaMA-65b | LLaMA-2-7b | LLaMA-2-13b
    | LLaMA-2-70b | Mistral-7b | Avg. Num. Bits |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| Method | LLaMA-7b | LLaMA-13b | LLaMA-30b | LLaMA-65b | LLaMA-2-7b | LLaMA-2-13b
    | LLaMA-2-70b | Mistral-7b | Avg. Num. Bits |'
- en: '| baseline | 7.08 | 6.61 | 5.98 | 5.62 | 6.63 | 6.05 | 4.97 | 5.71 | 16 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| baseline | 7.08 | 6.61 | 5.98 | 5.62 | 6.63 | 6.05 | 4.97 | 5.71 | 16 |'
- en: '| int4 | 7.40 | 6.82 | 6.18 | 5.75 | 7.31 | 6.59 | 5.12 | 5.91 | 4.00-4.01
    |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| int4 | 7.40 | 6.82 | 6.18 | 5.75 | 7.31 | 6.59 | 5.12 | 5.91 | 4.00-4.01
    |'
- en: '| int4-gs128 | 7.16 | 6.67 | 6.02 | 5.65 | 6.87 | 6.20 | 5.00 | 5.76 | 4.16
    |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| int4-gs128 | 7.16 | 6.67 | 6.02 | 5.65 | 6.87 | 6.20 | 5.00 | 5.76 | 4.16
    |'
- en: '| int4-gs64 | 7.12 | 6.64 | 6.00 | 5.63 | 6.79 | 6.15 | 4.99 | 5.75 | 4.31
    |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| int4-gs64 | 7.12 | 6.64 | 6.00 | 5.63 | 6.79 | 6.15 | 4.99 | 5.75 | 4.31
    |'
- en: '| nf4 | 7.27 | 6.74 | 6.10 | 5.69 | 7.09 | 6.45 | 5.06 | 5.85 | 4.00-4.01 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| nf4 | 7.27 | 6.74 | 6.10 | 5.69 | 7.09 | 6.45 | 5.06 | 5.85 | 4.00-4.01 |'
- en: '| nf4-gs128 | 7.16 | 6.66 | 6.02 | 5.65 | 6.86 | 6.20 | 5.00 | 5.77 | 4.16
    |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| nf4-gs128 | 7.16 | 6.66 | 6.02 | 5.65 | 6.86 | 6.20 | 5.00 | 5.77 | 4.16
    |'
- en: '| nuq4 | 7.13 | 6.65 | 6.02 | 5.64 | 6.70 | 6.11 | 5.00 | 5.75 | 4.00-4.02
    |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| nuq4 | 7.13 | 6.65 | 6.02 | 5.64 | 6.70 | 6.11 | 5.00 | 5.75 | 4.00-4.02
    |'
- en: '| + 0.1% outliers | 7.11 | 6.63 | 6.00 | 5.63 | 6.68 | 6.08 | 4.99 | 5.75 |
    4.04-4.06 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| + 0.1% outliers | 7.11 | 6.63 | 6.00 | 5.63 | 6.68 | 6.08 | 4.99 | 5.75 |
    4.04-4.06 |'
- en: '| + 0.5% outliers | 7.10 | 6.62 | 5.99 | 5.62 | 6.66 | 6.07 | 4.98 | 5.73 |
    4.16-4.19 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| + 0.5% outliers | 7.10 | 6.62 | 5.99 | 5.62 | 6.66 | 6.07 | 4.98 | 5.73 |
    4.16-4.19 |'
- en: '| + 1.0% outliers | 7.09 | 6.62 | 5.99 | 5.62 | 6.65 | 6.06 | 4.98 | 5.72 |
    4.32-4.35 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| + 1.0% outliers | 7.09 | 6.62 | 5.99 | 5.62 | 6.65 | 6.06 | 4.98 | 5.72 |
    4.32-4.35 |'
- en: '| int3 | 12.97 | 10.95 | 9.13 | 8.27 | 30.14 | 28.57 | 16.00 | 8.84 | 3.00-3.01
    |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| int3 | 12.97 | 10.95 | 9.13 | 8.27 | 30.14 | 28.57 | 16.00 | 8.84 | 3.00-3.01
    |'
- en: '| int3-gs128 | 7.62 | 6.93 | 6.24 | 5.79 | 8.00 | 7.06 | 5.16 | 6.08 | 3.15
    |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| int3-gs128 | 7.62 | 6.93 | 6.24 | 5.79 | 8.00 | 7.06 | 5.16 | 6.08 | 3.15
    |'
- en: '| int3-gs64 | 7.34 | 6.78 | 6.11 | 5.70 | 7.29 | 6.59 | 5.08 | 5.92 | 3.30
    |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| int3-gs64 | 7.34 | 6.78 | 6.11 | 5.70 | 7.29 | 6.59 | 5.08 | 5.92 | 3.30
    |'
- en: '| nf3 | 8.90 | 7.84 | 7.43 | 6.37 | 14.92 | 13.75 | 5.96 | 7.27 | 3.00-3.01
    |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| nf3 | 8.90 | 7.84 | 7.43 | 6.37 | 14.92 | 13.75 | 5.96 | 7.27 | 3.00-3.01
    |'
- en: '| nf3-gs128 | 7.65 | 6.99 | 6.29 | 5.82 | 8.03 | 7.12 | 5.24 | 6.16 | 3.15
    |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| nf3-gs128 | 7.65 | 6.99 | 6.29 | 5.82 | 8.03 | 7.12 | 5.24 | 6.16 | 3.15
    |'
- en: '| nuq3 | 7.36 | 6.83 | 6.18 | 5.75 | 7.05 | 6.37 | 5.10 | 5.95 | 3.00-3.02
    |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| nuq3 | 7.36 | 6.83 | 6.18 | 5.75 | 7.05 | 6.37 | 5.10 | 5.95 | 3.00-3.02
    |'
- en: '| + 0.1% outliers | 7.24 | 6.72 | 6.08 | 5.68 | 6.87 | 6.20 | 5.06 | 5.88 |
    3.04-3.06 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| + 0.1% outliers | 7.24 | 6.72 | 6.08 | 5.68 | 6.87 | 6.20 | 5.06 | 5.88 |
    3.04-3.06 |'
- en: '| + 0.5% outliers | 7.16 | 6.67 | 6.03 | 5.65 | 6.75 | 6.14 | 5.01 | 5.80 |
    3.16-3.19 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| + 0.5% outliers | 7.16 | 6.67 | 6.03 | 5.65 | 6.75 | 6.14 | 5.01 | 5.80 |
    3.16-3.19 |'
- en: '| + 1.0% outliers | 7.14 | 6.66 | 6.02 | 5.64 | 6.72 | 6.11 | 5.00 | 5.77 |
    3.32-3.35 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| + 1.0% 异常值 | 7.14 | 6.66 | 6.02 | 5.64 | 6.72 | 6.11 | 5.00 | 5.77 | 3.32-3.35
    |'
- en: '| int2 | 10892 | 100870 | 1411 | 7216 | 4708 | 4220 | 814 | 477 | 2.00-2.01
    |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| int2 | 10892 | 100870 | 1411 | 7216 | 4708 | 4220 | 814 | 477 | 2.00-2.01
    |'
- en: '| int2-gs128 | 43.49 | 56.25 | 21.07 | 17.05 | 113.49 | 97.04 | 23.67 | 50.73
    | 2.14 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| int2-gs128 | 43.49 | 56.25 | 21.07 | 17.05 | 113.49 | 97.04 | 23.67 | 50.73
    | 2.14 |'
- en: '| int2-gs64 | 13.91 | 13.36 | 8.49 | 7.34 | 35.21 | 40.40 | 8.28 | 13.83 |
    2.28 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| int2-gs64 | 13.91 | 13.36 | 8.49 | 7.34 | 35.21 | 40.40 | 8.28 | 13.83 |
    2.28 |'
- en: '| nf2 | 2850.1 | 4680.3 | 1617.1 | 5189.7 | 13081.2 | 4175.6 | 3216.9 | 1102.3
    | 2.00-2.01 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| nf2 | 2850.1 | 4680.3 | 1617.1 | 5189.7 | 13081.2 | 4175.6 | 3216.9 | 1102.3
    | 2.00-2.01 |'
- en: '| nf2-gs128 | 248.32 | 118.18 | 60.28 | 36.05 | 420.05 | 499.82 | 80.51 | 191.73
    | 2.14 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| nf2-gs128 | 248.32 | 118.18 | 60.28 | 36.05 | 420.05 | 499.82 | 80.51 | 191.73
    | 2.14 |'
- en: '| nuq2 | 10.28 | 9.05 | 9.27 | 60.87 | 15.16 | 43.77 | 7.24 | 8.40 | 2.00-2.02
    |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| nuq2 | 10.28 | 9.05 | 9.27 | 60.87 | 15.16 | 43.77 | 7.24 | 8.40 | 2.00-2.02
    |'
- en: '| + 0.1% outliers | 8.18 | 7.24 | 6.54 | 6.00 | 8.40 | 7.31 | 5.56 | 6.91 |
    2.04-2.06 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| + 0.1% 异常值 | 8.18 | 7.24 | 6.54 | 6.00 | 8.40 | 7.31 | 5.56 | 6.91 | 2.04-2.06
    |'
- en: '| + 0.5% outliers | 7.50 | 6.91 | 6.21 | 5.78 | 7.28 | 6.53 | 5.16 | 6.23 |
    2.16-2.19 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| + 0.5% 异常值 | 7.50 | 6.91 | 6.21 | 5.78 | 7.28 | 6.53 | 5.16 | 6.23 | 2.16-2.19
    |'
- en: '| + 1.0% outliers | 7.38 | 6.83 | 6.15 | 5.73 | 7.06 | 6.38 | 5.11 | 6.08 |
    2.32-2.35 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| + 1.0% 异常值 | 7.38 | 6.83 | 6.15 | 5.73 | 7.06 | 6.38 | 5.11 | 6.08 | 2.32-2.35
    |'
- en: Table 18. Zero-shot MMLU evaluation results for LLaMA-7B and LLaMA-13B when
    incorporating 3-bit KV cache quantization. The value reported is the weighted
    accuracy across all tasks. We observe minimal accuracy degradation with 3-bit
    quantization.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 表 18. 纳入3-bit KV缓存量化的LLaMA-7B和LLaMA-13B的零-shot MMLU评估结果。报告的值是所有任务的加权准确率。我们观察到，3-bit量化的准确性几乎没有下降。
- en: '| Method | LLaMA-7b | LLaMA-13b | Avg. Num. Bits |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LLaMA-7b | LLaMA-13b | 平均位数 |'
- en: '| baseline | 32.2% | 36.6% | 16 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 32.2% | 36.6% | 16 |'
- en: '| nuq3-1% | 31.9% | 36.2% | 3.32-3.33 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| nuq3-1% | 31.9% | 36.2% | 3.32-3.33 |'
- en: Appendix L Post-RoPE Per-Token Quantization Ablation
  id: totrans-416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 L Post-RoPE 每 Token 量化消融
- en: 'Table [19](#A12.T19 "Table 19 ‣ Appendix L Post-RoPE Per-Token Quantization
    Ablation ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache
    Quantization") shows perplexity evaluation on Wikitext-2 for the LLaMA-7B model
    with uniform quantization, with Keys quantized pre-RoPE and post-RoPE. These results
    show that post-RoPE Key quantization is superior to pre-RoPE Key quantization
    when quantizing Keys per-token. This is because when rotating an outlier channel
    with large average magnitude and another channel with smaller average magnitude
    together, at some positions in the sequence part of the magnitude from the outlier
    channel will be shifted to the smaller channel. This partially mitigates the impact
    of the outlier channel on skewing the quantization range for some of the tokens
    in the sequence. As such, for our baseline comparisons, we use post-RoPE per-token
    Key quantization to serve as a stronger baseline.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '表[19](#A12.T19 "表 19 ‣ 附录 L Post-RoPE 每 Token 量化消融 ‣ KVQuant: 朝着 1000 万上下文长度
    LLM 推理迈进")展示了LLaMA-7B模型在Wikitext-2上的困惑度评估，使用了均匀量化，对Key进行Pre-RoPE和Post-RoPE量化。这些结果表明，在对Key按Token进行量化时，Post-RoPE
    Key量化优于Pre-RoPE Key量化。这是因为当将一个具有较大平均幅度的异常通道与另一个具有较小平均幅度的通道一起旋转时，在序列的某些位置，异常通道的一部分幅度会转移到较小的通道。这在一定程度上缓解了异常通道对序列中某些Token量化范围的偏移影响。因此，在我们的基线比较中，我们使用Post-RoPE每Token
    Key量化作为更强的基线。'
- en: Table 19. Model accuracy when using pre-RoPE and post-RoPE quantization for
    LLaMA-7B with per-token Key quantization. Our experiments demonstrate that post-RoPE
    quantization is superior when using per-token Key quantization. Therefore, we
    decided to use these results for baseline comparison with per-token quantization.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 表 19. 使用Pre-RoPE和Post-RoPE量化进行LLaMA-7B模型的每Token Key量化时的模型准确性。我们的实验表明，使用每Token
    Key量化时，Post-RoPE量化优于Pre-RoPE量化。因此，我们决定使用这些结果作为与每Token量化的基线比较。
- en: '| Datatype | Perplexity (Pre-RoPE) | Perplexity (Post-RoPE) |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | 困惑度（Pre-RoPE） | 困惑度（Post-RoPE） |'
- en: '| fp16 | 5.68 | 5.68 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| fp16 | 5.68 | 5.68 |'
- en: '| int4 | 6.02 | 5.98 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| int4 | 6.02 | 5.98 |'
- en: '| int4-gs128 | 5.76 | 5.77 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| int4-gs128 | 5.76 | 5.77 |'
- en: '| int3 | 14.68 | 10.87 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| int3 | 14.68 | 10.87 |'
- en: '| int3-gs128 | 6.28 | 6.17 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| int3-gs128 | 6.28 | 6.17 |'
- en: Appendix M Additional Long Sequence Length Evaluation
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 M 额外长序列长度评估
- en: 'Figure [8](#A13.F8 "Figure 8 ‣ Appendix M Additional Long Sequence Length Evaluation
    ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization")
    shows perplexity evaluation results with varying amounts of input context for
    2-bit quantization. These results show that Q-Norm (and in particular, per-vector
    Q-Norm) can provide significant perplexity advantages for 2-bit quantization with
    long sequence length models. Additionally, for the LLaMA-2-70B-32K model, we observe
    perplexity degradation when going to a context length of 32K, likely due to error
    accumulation; however, this is largely mitigated by employing per-vector Q-Norm.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '图[8](#A13.F8 "Figure 8 ‣ Appendix M Additional Long Sequence Length Evaluation
    ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization")显示了不同输入上下文量下2位量化的困惑度评估结果。这些结果表明，Q-Norm（特别是每向量Q-Norm）可以为具有长序列长度的模型的2位量化提供显著的困惑度优势。此外，对于LLaMA-2-70B-32K模型，我们观察到在上下文长度达到32K时困惑度恶化，这可能是由于误差累积；然而，这通过使用每向量Q-Norm大大得到缓解。'
- en: '![Refer to caption](img/b01cba9da7ee5ec21425f9572347cccd.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b01cba9da7ee5ec21425f9572347cccd.png)'
- en: 'Figure 8. Perplexity results for the LLaMA-2-7B-32K model (uptrained for long
    sequence lengths using positional interpolation [[3](#bib.bib3)]) as well as the
    LLaMA-2-70B-32K LongLoRA model [[4](#bib.bib4)] on the Wikitext-2 dataset using
    2-bit quantization, evaluated using different amounts of input context. Results
    are shown using Q-Norm (“qn”) as well as per-vector Q-Norm (“pv-qn”), and all
    results include 1% outliers. The memory reduction shown is for 2-bit quantization
    with per-vector Q-Norm. Our results show that Q-Norm is helpful for 2-bit quantization,
    and that per-vector Q-Norm provides additional benefits for 2-bit quantization
    with LLaMA-2-70B-32K. See Figure [4](#S3.F4 "Figure 4 ‣ 3.7\. Kernel Implementation
    ‣ 3\. Method ‣ KVQuant: Towards 10 Million Context Length LLM Inference with KV
    Cache Quantization") for 4-bit and 3-bit quantization (which do not require Q-Norm).'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '图8. 使用2位量化的LLaMA-2-7B-32K模型（使用位置插值[[3](#bib.bib3)]进行长序列长度的再训练）以及LLaMA-2-70B-32K
    LongLoRA模型[[4](#bib.bib4)]在Wikitext-2数据集上的困惑度结果，使用不同量的输入上下文进行评估。结果显示了Q-Norm（“qn”）以及每向量Q-Norm（“pv-qn”），所有结果包括1%的异常值。显示的内存减少是针对具有每向量Q-Norm的2位量化。我们的结果表明Q-Norm对2位量化是有帮助的，并且每向量Q-Norm为LLaMA-2-70B-32K的2位量化提供了额外的好处。有关4位和3位量化（不需要Q-Norm）的信息，请参见图[4](#S3.F4
    "Figure 4 ‣ 3.7\. Kernel Implementation ‣ 3\. Method ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization")。'
- en: Appendix N Mixed-Precision Quantization
  id: totrans-429
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录N 混合精度量化
- en: An additional method to optimize compression performance is to consider mixed-precision
    quantization, where different layers are assigned different bit widths. This can
    allow for more accurate compression down to low bit widths due to differing sensitivities
    to quantization error of different layers. Finding the best bit precision distribution
    for different layers is intractable with brute force methods as the search space
    is exponentially large. We therefore aim to derive a metric to determine which
    layers can be quantized to reduced precision with minimal degradation in model
    accuracy, thereby enabling efficient one-shot mixed-precision bit assignments.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 优化压缩性能的另一种方法是考虑混合精度量化，其中不同的层被分配不同的位宽。这可以由于不同层对量化误差的敏感度不同，而允许更精确地压缩到较低的位宽。找到不同层的最佳位精度分布是不可行的，因为搜索空间呈指数级增长。因此，我们的目标是推导出一种度量方法，以确定哪些层可以在模型准确性降级最小的情况下进行低精度量化，从而实现高效的一次性混合精度位分配。
- en: 'Prior work on mixed-precision quantization has leveraged the largest Hessian
    eigenvalue or the Hessian trace to assess which layers are most sensitive to quantization
    [[9](#bib.bib9), [8](#bib.bib8)]; however, due to the computational challenges
    of computing the full Hessian or even estimating Hessian eigenvalues, we instead
    leverage the Fisher information approximation for the Hessian (as derived in Appendix [C](#A3
    "Appendix C Derivation for Sensitivity Analysis ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization")). Using the diagonal
    Fisher information matrix along with the quantization error, we can use the following
    sensitivity metric for a given layer (with activation $A$) to encompass both the
    quantization error as well as the Fisher information for that layer:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '以往关于混合精度量化的研究利用了最大的Hessian特征值或Hessian迹来评估哪些层对量化最敏感[[9](#bib.bib9), [8](#bib.bib8)]；然而，由于计算完整Hessian或估算Hessian特征值的计算挑战，我们改为利用Fisher信息近似Hessian（如附录[C](#A3
    "Appendix C Derivation for Sensitivity Analysis ‣ KVQuant: Towards 10 Million
    Context Length LLM Inference with KV Cache Quantization")中所述）。使用对角Fisher信息矩阵及量化误差，我们可以使用以下敏感度指标来涵盖给定层（激活为$A$）的量化误差以及该层的Fisher信息：'
- en: '| (8) |  | $\Omega=\big{(}A-A_{Q}\big{)}^{\top}\mathcal{F}\big{(}A-A_{Q}\big{)}.$
    |  |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\Omega=\big{(}A-A_{Q}\big{)}^{\top}\mathcal{F}\big{(}A-A_{Q}\big{)}.$
    |  |'
- en: 'We use the quantization error computed at the lower precision (as well as sensitivity
    information computed in fp16) to determine which layers were most sensitive to
    being quantized to the lower precision level. Figure [9](#A14.F9 "Figure 9 ‣ Appendix
    N Mixed-Precision Quantization ‣ KVQuant: Towards 10 Million Context Length LLM
    Inference with KV Cache Quantization") shows the mixed-precision perplexity results
    on the Wikitext-2 dataset for the LLaMA-7B and LLaMA-13B models. We show mixed-precision
    results using our sensitivity metric, and we include both quantization error-based
    mixed-precision assignment and the inverse of the selection order from our sensitivity
    metric as baselines. We see improved performance from incorporating sensitivity-based
    analysis for determining activation precisions when employing mixed-precision
    quantization. Our sensitivity metric therefore allows for efficiently determining
    an accurate mixed-precision assignment in order to trade off KV cache size and
    model accuracy.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用在较低精度下计算的量化误差（以及以fp16计算的敏感度信息）来确定哪些层对量化到较低精度级别最敏感。图[9](#A14.F9 "Figure
    9 ‣ Appendix N Mixed-Precision Quantization ‣ KVQuant: Towards 10 Million Context
    Length LLM Inference with KV Cache Quantization")展示了LLaMA-7B和LLaMA-13B模型在Wikitext-2数据集上的混合精度困惑度结果。我们展示了使用敏感度指标的混合精度结果，并包括了基于量化误差的混合精度分配和我们敏感度指标选择顺序的逆作为基准。我们看到通过结合基于敏感度的分析来确定激活精度，可以提高混合精度量化的性能。因此，我们的敏感度指标允许高效地确定准确的混合精度分配，从而在KV缓存大小和模型准确性之间进行权衡。'
- en: '![Refer to caption](img/c06693bf69af1241496bac5eaba4079f.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c06693bf69af1241496bac5eaba4079f.png)'
- en: Figure 9. Perplexity results for mixed-precision nuq4/3 quantization for the
    LLaMA-7B and LLaMA-13B models on the Wikitext-2 dataset, with model weights in
    fp16\. Sensitivity-Weighted uses our metric for layer importance, Inverse Sensitivity-Weighted
    refers to selecting the lowest-importance layers using our metric, and Quantization
    Error uses the quantization error to sort layers without weighting using sensitivity
    information.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 图9. LLaMA-7B和LLaMA-13B模型在Wikitext-2数据集上的混合精度nuq4/3量化的困惑度结果，模型权重为fp16。敏感度加权使用我们层重要性的指标，逆敏感度加权指使用我们指标选择最低重要性的层，而量化误差则使用量化误差对层进行排序，而不使用敏感度信息加权。
- en: Appendix O Kernel Implementation Details
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录O 核心实现细节
- en: We implemented 4-bit lookup table-based kernels for matrix-vector multiplication
    between the Key or Value activations (packed as a lookup table (LUT) plus indices
    into the LUT per-element) and a full-precision activation vector. These kernels
    load the compressed Key and Value activations and dequantize them only as needed
    in order to minimize memory bandwidth utilization. All arithmetic is performed
    in fp16. The lookup table entries are the values of the sensitivity-weighted non-uniform
    datatype for that particular layer scaled according to the range of activations
    that need to be represented [[6](#bib.bib6)]. Note that it is possible to implement
    this using a single LUT shared across channels that is rescaled by a per-channel
    or per-token scaling factor and offset, but for simplicity we used a separate
    LUT per-channel or per-token for our initial implementation.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了基于4位查找表的矩阵-向量乘法内核，用于键或值激活（打包为查找表（LUT）加上每元素的LUT索引）和全精度激活向量之间的运算。这些内核加载压缩的键和值激活，并仅在需要时进行解量化，以最大限度减少内存带宽利用率。所有算术运算都在fp16中进行。查找表条目是该特定层的灵敏度加权非均匀数据类型的值，按照需要表示的激活范围进行缩放[[6](#bib.bib6)]。请注意，可以通过一个共享的LUT实现此功能，该LUT根据每通道或每标记的缩放因子和偏移量进行重新缩放，但为简便起见，我们在初始实现中使用了每通道或每标记的单独LUT。
- en: When selecting between the Compressed-Sparse Column (CSC format) and the Compressed-Sparse
    Row (CSR) format for storing the outliers for the Keys and Values, we needed to
    consider how easy it would be to append new vectors. When using CSC format for
    the Key matrix, we only need to append a single element to the column vector,
    as well as one new element to the row and value vectors per nonzero element in
    that new column. If we used CSR format, we would need to insert the new column
    and value elements in the middle of the existing column and value vectors, and
    we would need to recompute the elements of the row vector. When using CSR format
    for the Value matrix, we only need to append a single element to the row vector,
    as well as one new element to the column and value vectors per nonzero element
    in that new row. If we used CSC format, we would need to insert the new row and
    value elements in the middle of the existing row and value vectors, and we would
    need to recompute the elements of the column vector. We therefore used the CSC
    format for the Key matrices and the CSR format for the Value matrices.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择用于存储键（Keys）和值（Values）异常值的压缩稀疏列（CSC格式）和压缩稀疏行（CSR格式）时，我们需要考虑添加新向量的难易程度。当使用CSC格式存储键矩阵时，我们只需在列向量中添加一个新元素，并在每个新列的非零元素中添加一个新元素到行向量和值向量中。如果我们使用CSR格式，则需要在现有的列和值向量中插入新的列和值元素，并且需要重新计算行向量的元素。当使用CSR格式存储值矩阵时，我们只需在行向量中添加一个新元素，并在每个新行的非零元素中添加一个新元素到列向量和值向量中。如果我们使用CSC格式，则需要在现有的行和值向量中插入新的行和值元素，并且需要重新计算列向量的元素。因此，我们选择了CSC格式用于键矩阵，而CSR格式用于值矩阵。
- en: One challenge with efficiently processing the sparse matrix-dense vector operation
    is that the sparsity distribution may be unbalanced. This poses a challenge for
    efficiently processing the sparse matrix on a GPU as there can be different numbers
    of nonzeros to process per thread. We therefore leverage a balanced sparse matrix-dense
    vector kernel based on [[10](#bib.bib10), [18](#bib.bib18)], which assigns an
    equal number of nonzeros per thread. This has greater synchronization overhead
    than assigning a single thread for an entire row or column when processing CSR/CSC
    matrices, but it leads to a more balanced work assignment between threads. We
    set the number of threads such that there were 10 nonzero values assigned to each
    thread. The dense non-uniform kernel and balanced sparse kernels are launched
    in one call to avoid overhead from summing the output vectors from these separate
    operations.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 高效处理稀疏矩阵与稠密向量运算的一个挑战是稀疏分布可能不平衡。这对在GPU上高效处理稀疏矩阵提出了挑战，因为每个线程需要处理的非零元素数量可能不同。因此，我们利用了基于[[10](#bib.bib10),
    [18](#bib.bib18)]的平衡稀疏矩阵-稠密向量内核，该内核为每个线程分配相等数量的非零元素。这比在处理CSR/CSC矩阵时为整个行或列分配单个线程具有更大的同步开销，但它能在线程之间实现更平衡的工作分配。我们设置线程数量，使每个线程分配10个非零值。为了避免从这些单独操作中汇总输出向量的开销，我们将稠密非均匀内核和平衡稀疏内核一次性启动。
- en: 'Table [20](#A15.T20 "Table 20 ‣ Appendix O Kernel Implementation Details ‣
    KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization")
    shows a detailed breakdown of kernel runtime, including how much time is spent
    packing vectors into the compressed format and how much time is spent on the dense
    and sparse matrix-vector multiplications. We find that even with 1% sparsity,
    we can attain significant speedups of up to 1.4$\times$ relative to the fp16 matrix-vector
    multiply kernels, demonstrating how our methodology facilitates efficient inference
    with a low-precision quantized KV cache.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [20](#A15.T20 "Table 20 ‣ Appendix O Kernel Implementation Details ‣ KVQuant:
    Towards 10 Million Context Length LLM Inference with KV Cache Quantization") 显示了内核运行时间的详细分解，包括将向量打包到压缩格式中所花费的时间，以及密集和稀疏矩阵-向量乘法所花费的时间。我们发现，即使在1%的稀疏度下，相较于fp16矩阵-向量乘法内核，我们仍然可以获得高达1.4$\times$的显著加速，这展示了我们的方法如何通过低精度量化KV缓存实现高效推理。'
- en: Table 20. Average latency (in microseconds) for the Key and Value dense nuq4
    kernels as well as for the sparse kernels (with 1% outliers), benchmarked on an
    A6000 GPU for the LLaMA-7B model. Benchmarking results are reported for different
    sequence lengths ($l$). fp16 matrix-vector multiplication latencies are included
    for reference, and the Key multiplication time also includes the time to apply
    RoPE to the newly appended Key vector. We find that our dense-and-sparse approach
    (even with 1% outliers) provides latency benefits relative to the fp16 baseline,
    even when accounting for the time to compress activations online.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 表20. Key和Value密集nuq4内核及稀疏内核（1%异常值）的平均延迟（以微秒为单位），在A6000 GPU上针对LLaMA-7B模型进行基准测试。报告了不同序列长度($l$)的基准测试结果。包括fp16矩阵-向量乘法延迟作为参考，Key乘法时间还包括将RoPE应用到新增Key向量的时间。我们发现，即使在1%异常值的情况下，我们的密集和稀疏方法相较于fp16基准线提供了延迟收益，即便考虑到在线压缩激活的时间。
- en: '| Activation | Operation | $l$=16K |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 激活 | 操作 | $l$=16K |'
- en: '| Key | fp16 Matvec | 66.4 | 116.1 | 402.3 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 关键 | fp16矩阵乘法 | 66.4 | 116.1 | 402.3 |'
- en: '| Key (nuq4-1%) | Dense Packing | 2.4 | 2.4 | 2.4 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 关键（nuq4-1%） | 密集打包 | 2.4 | 2.4 | 2.4 |'
- en: '| Sparse Packing | 4.5 | 4.5 | 4.5 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏打包 | 4.5 | 4.5 | 4.5 |'
- en: '| Dense Matvec | 42.8 | 75.5 | 283.5 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 密集矩阵乘法 | 42.8 | 75.5 | 283.5 |'
- en: '| Sparse Matvec | 10.5 | 14.3 | 53.7 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏矩阵乘法 | 10.5 | 14.3 | 53.7 |'
- en: '| Total Latency | 60.2 | 96.7 | 344.1 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 总延迟 | 60.2 | 96.7 | 344.1 |'
- en: '| Value | fp16 Matvec | 56.0 | 104.2 | 389.3 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| 值 | fp16矩阵乘法 | 56.0 | 104.2 | 389.3 |'
- en: '| Value (nuq4-1%) | Dense Packing | 2.0 | 2.0 | 2.0 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| 值（nuq4-1%） | 密集打包 | 2.0 | 2.0 | 2.0 |'
- en: '| Sparse Packing | 4.2 | 4.2 | 4.2 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏打包 | 4.2 | 4.2 | 4.2 |'
- en: '| Dense Matvec | 26.3 | 63.1 | 226.3 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 密集矩阵乘法 | 26.3 | 63.1 | 226.3 |'
- en: '| Sparse Matvec | 8.3 | 16.5 | 60.9 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏矩阵乘法 | 8.3 | 16.5 | 60.9 |'
- en: '| Total Latency | 40.8 | 85.8 | 293.4 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 总延迟 | 40.8 | 85.8 | 293.4 |'
