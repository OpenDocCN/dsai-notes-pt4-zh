- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:53:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference
    Cost Reduction'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TCRA-LLM：用于推理成本降低的 Token 压缩检索增强大型语言模型
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.15556](https://ar5iv.labs.arxiv.org/html/2310.15556)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.15556](https://ar5iv.labs.arxiv.org/html/2310.15556)
- en: Junyi Liu![[Uncaptioned image]](img/dcde1d60e89ece30716aad23e89d4fb9.png)![[Uncaptioned
    image]](img/539fdc9f80367d39a23952dceb9adef4.png), Liangzhi Li![[Uncaptioned image]](img/dcde1d60e89ece30716aad23e89d4fb9.png)![[Uncaptioned
    image]](img/539fdc9f80367d39a23952dceb9adef4.png)![[Uncaptioned image]](img/e3cc2dc67bb8badd02b4aa307940bfd1.png),
    Tong Xiang![[Uncaptioned image]](img/dcde1d60e89ece30716aad23e89d4fb9.png)![[Uncaptioned
    image]](img/539fdc9f80367d39a23952dceb9adef4.png), Bowen Wang![[Uncaptioned image]](img/b45cc34282179e01f8ab129e5e6c062a.png)![[Uncaptioned
    image]](img/dcde1d60e89ece30716aad23e89d4fb9.png)![[Uncaptioned image]](img/539fdc9f80367d39a23952dceb9adef4.png),
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 刘俊毅![[无标题图片]](img/dcde1d60e89ece30716aad23e89d4fb9.png)![[无标题图片]](img/539fdc9f80367d39a23952dceb9adef4.png),
    李良智![[无标题图片]](img/dcde1d60e89ece30716aad23e89d4fb9.png)![[无标题图片]](img/539fdc9f80367d39a23952dceb9adef4.png)![[无标题图片]](img/e3cc2dc67bb8badd02b4aa307940bfd1.png),
    向通![[无标题图片]](img/dcde1d60e89ece30716aad23e89d4fb9.png)![[无标题图片]](img/539fdc9f80367d39a23952dceb9adef4.png),
    王博文![[无标题图片]](img/b45cc34282179e01f8ab129e5e6c062a.png)![[无标题图片]](img/dcde1d60e89ece30716aad23e89d4fb9.png)![[无标题图片]](img/539fdc9f80367d39a23952dceb9adef4.png),
- en: Yiming Qian![[Uncaptioned image]](img/1271d6e2799908c73d5d67f937f0f2a4.png)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 钱一鸣![[无标题图片]](img/1271d6e2799908c73d5d67f937f0f2a4.png)
- en: '![[Uncaptioned image]](img/dcde1d60e89ece30716aad23e89d4fb9.png)Meetyou AI
    Lab, ![[Uncaptioned image]](img/539fdc9f80367d39a23952dceb9adef4.png)Xiamen Key
    Laboratory of Women’s Internet Health Management,'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![[无标题图片]](img/dcde1d60e89ece30716aad23e89d4fb9.png)Meetyou AI Lab, ![[无标题图片]](img/539fdc9f80367d39a23952dceb9adef4.png)厦门女性互联网健康管理重点实验室,'
- en: '![[Uncaptioned image]](img/b45cc34282179e01f8ab129e5e6c062a.png)Osaka University,
    ![[Uncaptioned image]](img/1271d6e2799908c73d5d67f937f0f2a4.png)Agency for Science,
    Technology and Research (A*STAR)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图片]](img/b45cc34282179e01f8ab129e5e6c062a.png)大阪大学, ![[无标题图片]](img/1271d6e2799908c73d5d67f937f0f2a4.png)新加坡科技研究局（A*STAR）'
- en: '{liujunyi, liliangzhi, xiangtong}@xiaoyouzi.com,'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{liujunyi, liliangzhi, xiangtong}@xiaoyouzi.com,'
- en: bowen.wang@is.ids.osaka-u.ac.jp, qiany@ihpc.a-star.edu.sg
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: bowen.wang@is.ids.osaka-u.ac.jp, qiany@ihpc.a-star.edu.sg
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Since ChatGPT released its API for public use, the number of applications built
    on top of commercial large language models (LLMs) increase exponentially. One
    popular usage of such models is leveraging its in-context learning ability and
    generating responses given user queries leveraging knowledge obtained by retrieval
    augmentation. One problem of deploying commercial retrieval-augmented LLMs is
    the cost due to the additionally retrieved context that largely increases the
    input token size of the LLMs. To mitigate this, we propose a token compression
    scheme that includes two methods: summarization compression and semantic compression.
    The first method applies a T5-based model that is fine-tuned by datasets generated
    using self-instruct containing samples with varying lengths and reduce token size
    by doing summarization. The second method further compresses the token size by
    removing words with lower impact on the semantic. In order to adequately evaluate
    the effectiveness of the proposed methods, we propose and utilize a dataset called
    Food-Recommendation DB (FRDB) focusing on food recommendation for women around
    pregnancy period or infants. Our summarization compression can reduce 65% of the
    retrieval token size with further 0.3% improvement on the accuracy; semantic compression
    provides a more flexible way to trade-off the token size with performance, for
    which we can reduce the token size by 20% with only 1.6% of accuracy drop.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 ChatGPT 发布其公开 API 以来，基于商业大型语言模型（LLMs）构建的应用程序数量呈指数增长。此类模型的一种流行用法是利用其上下文学习能力，通过检索增强获得的知识生成对用户查询的回应。部署商业检索增强
    LLMs 的一个问题是由于附加检索的上下文，输入的 token 大小大幅增加，从而导致成本上升。为了缓解这个问题，我们提出了一种 token 压缩方案，包括两种方法：总结压缩和语义压缩。第一种方法应用了一个基于
    T5 的模型，该模型通过使用 self-instruct 生成的具有不同长度样本的数据集进行微调，并通过总结来减少 token 大小。第二种方法通过去除对语义影响较小的词进一步压缩
    token 大小。为了充分评估所提出方法的有效性，我们提出并使用了一个名为 Food-Recommendation DB（FRDB）的数据集，专注于孕期或婴儿期女性的食品推荐。我们的总结压缩可以减少
    65% 的检索 token 大小，并在准确性上有进一步的 0.3% 提升；语义压缩提供了一种更灵活的方式来权衡 token 大小与性能，我们可以在仅 1.6%
    的准确性下降下将 token 大小减少 20%。
- en: ^†^†![[Uncaptioned image]](img/e3cc2dc67bb8badd02b4aa307940bfd1.png)Corresponding
    author.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†![[未标注的图片]](img/e3cc2dc67bb8badd02b4aa307940bfd1.png)通讯作者。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: With the increase in computing power and accumulation of enormous text data,
    large language models (LLMs) such as ChatGPT (OpenAI, [2023b](#bib.bib21)) and
    GPT-4 (OpenAI, [2023a](#bib.bib20)) have shown impressive performance in dialogue-based
    question-answering (QA), allowing them to interact with users fluently. In open-domain
    QA where the models are engaged in casual conversations with users, LLMs exhibit
    astonishing performance by leveraging strong in-context learning ability. However
    LLMs may produce vague responses or incorrect answers in certain specialized domains,
    owing to the absence of relevant knowledge or a restricted scope of information
    acquired during the training stage, which might potentially result in untruthful
    answers and even cause physical damages to users (Xiang et al., [2023](#bib.bib41)).
    For QA in such domains, retrieval-augmented generation (RAG) (Lewis et al., [2020](#bib.bib14)),
    where the system retrieves external knowledge beforehand and then utilizes LLMs
    to generate answers leveraging retrieved knowledge, can greatly reduce the hallucinations
    generated (Shi et al., [2023](#bib.bib32); Shuster et al., [2021](#bib.bib33)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算能力的提高和大量文本数据的积累，像 ChatGPT（OpenAI，[2023b](#bib.bib21)）和 GPT-4（OpenAI，[2023a](#bib.bib20)）这样的语言模型（LLMs）在基于对话的问答（QA）中表现出色，能够与用户流畅互动。在开放领域的问答中，当模型与用户进行随意对话时，LLMs
    通过强大的上下文学习能力展现了惊人的表现。然而，由于缺乏相关知识或训练阶段获得的信息范围有限，LLMs 在某些专业领域可能会产生模糊的回应或不正确的答案，这可能导致虚假的回答，甚至对用户造成实际损害（Xiang
    et al., [2023](#bib.bib41)）。对于这些领域的问答，检索增强生成（RAG）（Lewis et al., [2020](#bib.bib14)），即系统事先检索外部知识，然后利用
    LLMs 生成基于检索知识的答案，可以大大减少生成的幻觉（Shi et al., [2023](#bib.bib32); Shuster et al., [2021](#bib.bib33)）。
- en: Many current commercial LLMs are black-box models, where the model architectures
    and the weight information are not disclosed. These LLMs own superior text comprehension
    abilities, yet in many cases they can only output desired answers through complicated
    prompt engineering. On the other hand, deploying open-source LLMs to local servers
    is resource-intensive, in contrast to deploying smaller models such as T5 (Raffel
    et al., [2020](#bib.bib24)). Some commercial LLMs like GPT-3.5-turbo (OpenAI,
    [2023c](#bib.bib22)) and GPT-4 offer access through API calls; however, these
    models charge users based on the size of input and output¹¹1As of May 5th, 2023,
    GPT-4, capable of processing up to 8k tokens, charges $0.03 per thousand input
    tokens and $0.06 per thousand output tokens; GPT-4-32K which can process 32k tokens,
    charges $0.06 per thousand input tokens and $0.12 per thousand output tokens.
    GPT-3.5-turbo charges $0.002 per thousand tokens.. For individuals or companies
    looking to create their own services using LLMs through API calls, utilizing commercial
    ones can be resource-consuming if requests are made frequently. Therefore, it
    is necessary to minimize the number of input tokens while maintaining optimal
    performance during the API calls.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 目前许多商业 LLMs 是黑箱模型，其中模型架构和权重信息未公开。这些 LLMs 具有出色的文本理解能力，但在许多情况下，它们只能通过复杂的提示工程输出期望的答案。另一方面，将开源
    LLMs 部署到本地服务器是资源密集型的，与部署像 T5（Raffel et al., [2020](#bib.bib24)）这样的较小模型相比。某些商业
    LLMs，如 GPT-3.5-turbo（OpenAI，[2023c](#bib.bib22)）和 GPT-4 提供通过 API 调用的访问；然而，这些模型根据输入和输出的大小向用户收费¹¹1截至
    2023 年 5 月 5 日，GPT-4 能处理最多 8k 个令牌，每千个输入令牌收费 $0.03，每千个输出令牌收费 $0.06；GPT-4-32K 能处理
    32k 个令牌，每千个输入令牌收费 $0.06，每千个输出令牌收费 $0.12。GPT-3.5-turbo 每千个令牌收费 $0.002.. 对于希望通过
    API 调用创建自己服务的个人或公司，如果请求频繁，使用商业模型可能会消耗大量资源。因此，有必要在 API 调用期间尽量减少输入令牌的数量，同时保持最佳性能。
- en: '![Refer to caption](img/c75c0595b092b5c38fef21162cb5ec32.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c75c0595b092b5c38fef21162cb5ec32.png)'
- en: 'Figure 1: Illustration of different ways to utilize LLMs for QA. Top: directly
    using LLM. Middle: using a retrieval-augmented LLM. Bottom: using retrieval-augmented
    LLM with our proposed token compression methods.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：利用 LLMs 进行 QA 的不同方式示意图。顶部：直接使用 LLM。中部：使用检索增强 LLM。底部：使用检索增强 LLM 结合我们提出的令牌压缩方法。
- en: 'In this work, we propose a token compression scheme specifically designed for
    the retrieval-augmented LLMs (shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference
    Cost Reduction")), namely, Token Compression Retrieval Augmented Large Language
    Model (TCRA-LLM). Our proposed scheme can reduce up to 65% of the token size with
    additional 0.3% improvement on accuracy when doing QA on our proposed dataset
    called Food-Recommendation DB (FRDB). We propose two approaches to reduce the
    token size of the LLMs’ input: summarization compression and semantic compression.
    For summarization compression, we leverage self-instruct (Wang et al., [2022](#bib.bib39))
    scheme to build multiple summarization datasets with varying lengths to fine-tune
    the mT5 model (Xue et al., [2020](#bib.bib42)). The samples from the summarization
    datasets are generated by GPT-3.5-turbo (OpenAI, [2023c](#bib.bib22)), which is
    instructed to shorten the summary of the input sentences in an iterative manner.
    The semantic compression approach is based on a simple yet effective intuition,
    that removing semantically less important words in a sentence won’t drastically
    change its semantic. Here, we deploy a multi-lingual sentence-transformer (Reimers
    and Gurevych, [2020](#bib.bib26)) to encode sentences into embeddings where the
    distances between original and perturbed embeddings are used to measure the semantic
    deviation from the original meaning. Larger semantic deviation indicates that
    the corresponding word owns more important semantic in the sentence. We conduct
    an iterative process that measures the semantic importance of each word in the
    sentence and remove less important words.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们提出了一种专门为检索增强型大型语言模型（如图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ TCRA-LLM: 用于推理成本降低的令牌压缩检索增强型大型语言模型")）设计的令牌压缩方案，即令牌压缩检索增强型大型语言模型（TCRA-LLM）。我们提出的方案可以在对我们所提出的数据集
    Food-Recommendation DB (FRDB) 进行问答时，将令牌大小减少多达 65%，同时准确率提高 0.3%。我们提出了两种减少 LLM 输入令牌大小的方法：总结压缩和语义压缩。对于总结压缩，我们利用自我指导
    (Wang et al., [2022](#bib.bib39)) 方案建立了多个不同长度的总结数据集，以微调 mT5 模型 (Xue et al., [2020](#bib.bib42))。这些总结数据集中的样本由
    GPT-3.5-turbo (OpenAI, [2023c](#bib.bib22)) 生成，该模型按照指示以迭代的方式缩短输入句子的总结。语义压缩方法基于一个简单而有效的直觉，即删除句子中语义上不太重要的词不会显著改变其语义。在这里，我们部署了一个多语言句子变换器
    (Reimers 和 Gurevych, [2020](#bib.bib26))，将句子编码成嵌入，其中原始嵌入与扰动嵌入之间的距离用于衡量语义偏差。较大的语义偏差表示相应的词在句子中拥有更重要的语义。我们进行了一次迭代过程，测量句子中每个词的语义重要性，并删除不太重要的词。'
- en: 'In conclusion, our work has the following three contributions:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的工作有以下三个贡献：
- en: '1.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'We construct a food recommendation QA dataset (Section [3](#S3 "3 FRDB ‣ TCRA-LLM:
    Token Compression Retrieval Augmented Large Language Model for Inference Cost
    Reduction")) which contains domain knowledge that general LLMs might not have.
    This dataset serves the purpose of evaluating retrieval-augmented LLMs.'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们构建了一个食品推荐问答数据集（第 [3](#S3 "3 FRDB ‣ TCRA-LLM: 用于推理成本降低的令牌压缩检索增强型大型语言模型") 节），该数据集包含通用
    LLM 可能没有的领域知识。这个数据集用于评估检索增强型 LLM。'
- en: '2.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'We propose a multi-level self-instruct scheme (Section [4.2](#S4.SS2 "4.2 Token
    Compression ‣ 4 Method ‣ TCRA-LLM: Token Compression Retrieval Augmented Large
    Language Model for Inference Cost Reduction")) to build summarization datasets
    of different lengths.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们提出了一种多层自我指导方案（第 [4.2](#S4.SS2 "4.2 令牌压缩 ‣ 4 方法 ‣ TCRA-LLM: 用于推理成本降低的令牌压缩检索增强型大型语言模型")
    节），用于构建不同长度的总结数据集。'
- en: '3.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'We propose two token compression methods (Section [4.2](#S4.SS2 "4.2 Token
    Compression ‣ 4 Method ‣ TCRA-LLM: Token Compression Retrieval Augmented Large
    Language Model for Inference Cost Reduction")), both of which can reduce the number
    of input tokens during the API calls of retrieval-augmented commercial LLMs while
    maintaining optimal performance.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们提出了两种令牌压缩方法（第 [4.2](#S4.SS2 "4.2 令牌压缩 ‣ 4 方法 ‣ TCRA-LLM: 用于推理成本降低的令牌压缩检索增强型大型语言模型")
    节），这两种方法都可以在检索增强型商业 LLM 的 API 调用过程中减少输入令牌的数量，同时保持最佳性能。'
- en: 2 Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLMs such as GPT-3 (Brown et al., [2020](#bib.bib3)), PALM (Chowdhery et al.,
    [2022](#bib.bib5)), OPT (Zhang et al., [2022](#bib.bib45)), Bloom (Scao et al.,
    [2022](#bib.bib30)), and LLaMA (Touvron et al., [2023](#bib.bib37)) are trained
    on massive amounts of data and have demonstrated powerful comprehension capabilities.
    These models have been deployed in a breadth of tasks and achieve promising results (Zhang
    et al., [2023](#bib.bib46); Ashok and Lipton, [2023](#bib.bib1); Lu et al., [2023](#bib.bib15);
    Wang et al., [2023](#bib.bib38); Xiang et al., [2023](#bib.bib41)). One major
    barrier that prevents more people from participating in commercial deployment
    of the LLMs is their training and hosting costs. A way to reduce such costs is
    through training smaller domain-specific models such as BioMedLM (Bolton et al.,
    [2022](#bib.bib2)), BloombergGPT (Wu et al., [2023](#bib.bib40)), and LawGPT (Nguyen,
    [2023](#bib.bib17)). Such domain-specific training enables smaller LLMs to be
    applied to certain fields but still requires huge investment. For instance, BloombergGPT
    is trained on 512 40GB A100 GPUs with the total budget being approximately $2.7
    million (Sheikh, [2023](#bib.bib31)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GPT-3（Brown 等，[2020](#bib.bib3)）、PALM（Chowdhery 等，[2022](#bib.bib5)）、OPT（Zhang
    等，[2022](#bib.bib45)）、Bloom（Scao 等，[2022](#bib.bib30)）和 LLaMA（Touvron 等，[2023](#bib.bib37)）这样的
    LLMs（大型语言模型）在大量数据上进行训练，展示了强大的理解能力。这些模型已被应用于各种任务，并取得了令人鼓舞的成果（Zhang 等，[2023](#bib.bib46)；Ashok
    和 Lipton，[2023](#bib.bib1)；Lu 等，[2023](#bib.bib15)；Wang 等，[2023](#bib.bib38)；Xiang
    等，[2023](#bib.bib41)）。阻碍更多人参与 LLM 商业部署的一个主要障碍是其训练和托管成本。减少这些成本的一种方法是通过训练较小的特定领域模型，如
    BioMedLM（Bolton 等，[2022](#bib.bib2)）、BloombergGPT（Wu 等，[2023](#bib.bib40)）和 LawGPT（Nguyen，[2023](#bib.bib17)）。这种特定领域的训练使得较小的
    LLMs 可以应用于某些领域，但仍然需要巨大的投资。例如，BloombergGPT 的训练使用了 512 个 40GB 的 A100 GPU，总预算约为 270
    万美元（Sheikh，[2023](#bib.bib31)）。
- en: Alternatively, LLMs can be used without fine-tuning through retrieval augmentation
    leveraging external data sources, where the retrieved data is used as supplementary
    information to help LLMs improve logical reasoning and language generation (Thorne
    et al., [2021](#bib.bib36); Izacard et al., [2022](#bib.bib10)). Previous experiments (Ram
    et al., [2023](#bib.bib25)) show that additional information can be beneficial
    for LLMs across different model sizes. Retrieval augmentation eliminates the cost
    of tuning an in-house LLM on new data, and can be easily integrated with commercial
    LLM services such as ChatGPT (OpenAI, [2023b](#bib.bib21)) from OpenAI or Bard (Pichai,
    [2023](#bib.bib23)) from Google. Many studies have shown, applying retrieval augmentation
    to the commercial LLMs such as ChatGPT allow the models to gain knowledge in specific
    domains such as natural science and medicine (Soong et al., [2023](#bib.bib34);
    Inaba et al., [2023](#bib.bib9)) which is not revealed during their training and
    retrieval augmentation can be further improved by applying more sophisticated
    retrievers (Shi et al., [2023](#bib.bib32)). However, commercial LLMs all have
    a limitation on input lengths which put an upper ceiling on the amount of information
    that can be fed into a LLM. Later models such as GPT-4 has looser restriction
    but the inference cost increases drastically in comparison with other models.
    Some previous work applies template-based prompt optimization (Santra et al.,
    [2023](#bib.bib29)), which select retrieved context (Mallen et al., [2022](#bib.bib16))
    in an adaptive manner, or uses cascading of LLMs with different sizes (Chen et al.,
    [2023](#bib.bib4)) to reduce the inference costs. Our proposed method has no conflict
    with these methods and can be used with them simultaneously.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，LLM 可以通过利用外部数据源进行检索增强而无需微调，其中检索的数据作为补充信息帮助 LLM 提升逻辑推理和语言生成能力（Thorne 等， [2021](#bib.bib36)；Izacard
    等， [2022](#bib.bib10)）。以往的实验（Ram 等， [2023](#bib.bib25)）表明，额外的信息对不同模型规模的 LLM 都有帮助。检索增强消除了在新数据上调整内部
    LLM 的成本，并且可以轻松与商业 LLM 服务如 OpenAI 的 ChatGPT （OpenAI，[2023b](#bib.bib21)）或 Google
    的 Bard （Pichai，[2023](#bib.bib23)）集成。许多研究表明，将检索增强应用于商业 LLM，如 ChatGPT，可以使模型在自然科学和医学等特定领域获得训练时未揭示的知识（Soong
    等，[2023](#bib.bib34)；Inaba 等，[2023](#bib.bib9)），而且检索增强可以通过应用更复杂的检索器进一步改进（Shi 等，[2023](#bib.bib32)）。然而，商业
    LLM 都有输入长度的限制，这对 LLM 可以接收的信息量设定了上限。后续模型如 GPT-4 有更宽松的限制，但相比其他模型推理成本大幅增加。一些早期工作应用了基于模板的提示优化（Santra
    等，[2023](#bib.bib29)），以适应的方式选择检索的上下文（Mallen 等，[2022](#bib.bib16)），或者使用不同规模的 LLM
    级联（Chen 等，[2023](#bib.bib4)）来降低推理成本。我们提出的方法与这些方法没有冲突，可以与它们同时使用。
- en: 3 FRDB
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 FRDB
- en: 'We build a Food Recommendation Dataset in Chinese called FRDB, for recommending
    foods that are safe to consume for women before/during/after their pregnancy as
    well as infants. It contains two parts: multiple-choice (MC) QA pairs and a knowledge
    database. The QA pairs contain 1,000 samples that cover 200 types of food. The
    categories of foods are shown in Table [1](#S3.T1 "Table 1 ‣ 3 FRDB ‣ TCRA-LLM:
    Token Compression Retrieval Augmented Large Language Model for Inference Cost
    Reduction").'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '我们建立了一个名为 FRDB 的中文食品推荐数据集，用于推荐适合女性在怀孕前/期间/后以及婴儿食用的安全食品。它包含两个部分：选择题（MC）问答对和知识数据库。问答对包含
    1,000 个样本，涵盖 200 种食品。食品的类别见表 [1](#S3.T1 "Table 1 ‣ 3 FRDB ‣ TCRA-LLM: Token Compression
    Retrieval Augmented Large Language Model for Inference Cost Reduction")。'
- en: '| Food type | Count$\downarrow$ |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 食品类型 | 数量$\downarrow$ |'
- en: '| --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Entrée | 31 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 主菜 | 31 |'
- en: '| Vegetables | 31 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 蔬菜 | 31 |'
- en: '| Seafood | 22 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 海鲜 | 22 |'
- en: '| Sweets | 22 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 糖果 | 22 |'
- en: '| Medicine/Health supplement | 20 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 医药/健康补充品 | 20 |'
- en: '| Fruit | 17 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 水果 | 17 |'
- en: '| Grains | 16 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 谷物 | 16 |'
- en: '| Soft drink | 13 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 软饮料 | 13 |'
- en: '| Condiment | 10 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 调料 | 10 |'
- en: '| Meat/Eggs | 10 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 肉类/鸡蛋 | 10 |'
- en: '| Soybeans/Dried fruit | 6 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 大豆/干果 | 6 |'
- en: '| Dairy products | 2 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 乳制品 | 2 |'
- en: '| Total | 200 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 200 |'
- en: 'Table 1: Statistics of food types in FRDB.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：FRDB 中食品类型的统计数据。
- en: 'The possible answers to the question falls into three choices based on the
    increasing degree of recommendations ranging from 1 (avoid) to 3 (highly recommend).
    Each type of food has five recommendation rating corresponding to five groups:
    pre-pregnancy, pregnancy, postpartum, lactation, and infant. Additionally, we
    build a knowledge database that contains 7,588 entries; details of the entries
    are shown in Table [2](#S3.T2 "Table 2 ‣ 3 FRDB ‣ TCRA-LLM: Token Compression
    Retrieval Augmented Large Language Model for Inference Cost Reduction"). The distribution
    of sentence length in the knowledge database is shown in Figure [2](#S3.F2 "Figure
    2 ‣ 3 FRDB ‣ TCRA-LLM: Token Compression Retrieval Augmented Large Language Model
    for Inference Cost Reduction").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '对问题的可能答案分为三种选择，基于推荐程度的递增，范围从 1（避免）到 3（强烈推荐）。每种食物有五个推荐评级，对应五个组别：备孕期、孕期、产后、哺乳期和婴儿期。此外，我们建立了一个包含
    7,588 条目的知识数据库；条目的详细信息见表格 [2](#S3.T2 "表格 2 ‣ 3 FRDB ‣ TCRA-LLM: 令牌压缩检索增强大型语言模型以降低推理成本")。知识数据库中的句子长度分布见图 [2](#S3.F2
    "图 2 ‣ 3 FRDB ‣ TCRA-LLM: 令牌压缩检索增强大型语言模型以降低推理成本")。'
- en: '|  | Mean | Max | Min | Std. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | 平均值 | 最大值 | 最小值 | 标准差 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| # of words | 88 | 248 | 12 | 27 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 单词数 | 88 | 248 | 12 | 27 |'
- en: 'Table 2: Statistics of the entries FRDB knowledge database. Std. stands for
    standard deviation.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 2: FRDB 知识数据库条目的统计数据。Std. 代表标准差。'
- en: '![Refer to caption](img/f743554cf1f47a0436be9fe3deb829ec.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f743554cf1f47a0436be9fe3deb829ec.png)'
- en: 'Figure 2: The distribution of sentence length in FRDB knowledge database.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: FRDB 知识数据库中句子长度的分布。'
- en: 'All the information has been verified by the health-domain professionals. During
    the verification, we remove the text that is ambiguous to the human annotators.
    Two samples of knowledge are shown in Table [3](#S3.T3 "Table 3 ‣ 3 FRDB ‣ TCRA-LLM:
    Token Compression Retrieval Augmented Large Language Model for Inference Cost
    Reduction"). Sample questions are available in the Appendix [A](#Ax1.SS1 "A Example
    question for FRDB dataset ‣ Appendix ‣ TCRA-LLM: Token Compression Retrieval Augmented
    Large Language Model for Inference Cost Reduction").'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '所有信息已由健康领域专业人士验证。在验证过程中，我们删除了对人工标注者模糊不清的文本。两个知识样本显示在表格 [3](#S3.T3 "表格 3 ‣ 3
    FRDB ‣ TCRA-LLM: 令牌压缩检索增强大型语言模型以降低推理成本")中。样本问题见附录 [A](#Ax1.SS1 "A FRDB 数据集示例问题
    ‣ 附录 ‣ TCRA-LLM: 令牌压缩检索增强大型语言模型以降低推理成本")。'
- en: '| High quality knowledge | Ambiguous knowledge |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 高质量知识 | 含糊不清的知识 |'
- en: '| Consuming mushrooms after childbirth is beneficial for postpartum recovery,
    constipation relief, and promoting lactation due to their rich B vitamins, protein,
    and amino acids. A moderate amount of intake based on the recovery status is recommended.
    | Postpartum mothers are safe to consume a moderate amount of cake. Cakes are
    easier to digest and absorb for postpartum mothers with weaker gastrointestinal
    systems. However, cakes have relatively smaller nutritional diversity and they
    should be consumed together with vegetables, fruits, and meats to make the nutrition
    more balanced. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 产后食用蘑菇有利于产后恢复、缓解便秘和促进哺乳，因为它们富含 B 维生素、蛋白质和氨基酸。建议根据恢复状态适量摄入。 | 产后妈妈可以安全地适量食用蛋糕。蛋糕对胃肠功能较弱的产后妈妈更易消化吸收。然而，蛋糕的营养多样性相对较小，应该与蔬菜、水果和肉类一起食用，以使营养更加均衡。
    |'
- en: 'Table 3: Examples from the knowledge database. The ambiguous ones are excluded
    from our dataset.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 3: 知识数据库的示例。含糊不清的示例已被排除在我们的数据集中。'
- en: 4 Method
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法
- en: 'Typically, a retrieval-augmented LLM consists of three components (shown in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ TCRA-LLM: Token Compression Retrieval
    Augmented Large Language Model for Inference Cost Reduction")), a knowledge database,
    a retriever, and the LLM. The knowledge database contains all available domain-specific
    knowledge. The retriever applies the question as a query to search for the relevant
    information from the knowledge database. The retrieved information is then formulated
    as the context packaged together with questions as a prompt for LLM to generate
    an answer. Our proposed methods are able to compress the retrieved information
    and formulate shorter context but maintain the effectiveness of retrieval augmentation.
    In this section, we go through the pipeline of a retrieval-augmented LLM system
    for QA and introduce our proposed token compression methods.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，检索增强型大语言模型由三个组件组成（见图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ TCRA-LLM：用于推理成本降低的标记压缩检索增强大语言模型")），即知识数据库、检索器和大语言模型。知识数据库包含所有可用的领域特定知识。检索器将问题作为查询应用于知识数据库，以搜索相关信息。检索到的信息随后被整理成上下文，与问题一起打包，作为大语言模型生成答案的提示。我们提出的方法能够压缩检索到的信息，并形成更简短的上下文，但保持检索增强的有效性。在本节中，我们将详细介绍检索增强型大语言模型系统的流程，并介绍我们提出的标记压缩方法。
- en: 4.1 Information Retrieval
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 信息检索
- en: 'Generally, the first step for LLM’s retrieval augmentation is knowledge retrieval.
    Given an user query $x$. There are two mainstream retrieval methods: dense retrieval (Karpukhin
    et al., [2020](#bib.bib13); Ni et al., [2021](#bib.bib18)) and sparse retrieval (Robertson
    et al., [2009](#bib.bib27)). Dense retrieval first encodes queries and documents
    into dense embeddings (Huang et al., [2013](#bib.bib8); Yi et al., [2019](#bib.bib43))
    using pre-trained neural encoders and then finds a query’s nearest neighbors in
    the embedding space using a relevancy measure such as cosine similarity (Yu et al.,
    [2021](#bib.bib44)). Sparse retrieval, on the other hand, maps queries and documents
    into a high-dimensional space with methods such as TF-IDF (Sparck Jones, [1972](#bib.bib35);
    Jones, [1973](#bib.bib12)) and the most relevant documents are returned to the
    user as the answer. Typical example of sparse retrieval is BM25 (Robertson et al.,
    [1995](#bib.bib28)).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，大语言模型的检索增强的第一步是知识检索。给定用户查询$x$。有两种主流的检索方法：密集检索（Karpukhin等人，[2020](#bib.bib13)；Ni等人，[2021](#bib.bib18)）和稀疏检索（Robertson等人，[2009](#bib.bib27)）。密集检索首先使用预训练的神经编码器将查询和文档编码为密集嵌入（Huang等人，[2013](#bib.bib8)；Yi等人，[2019](#bib.bib43)），然后使用相关性度量（如余弦相似度（Yu等人，[2021](#bib.bib44)））在嵌入空间中找到查询的最近邻。而稀疏检索则将查询和文档映射到高维空间，使用如TF-IDF（Sparck
    Jones，[1972](#bib.bib35)；Jones，[1973](#bib.bib12)）等方法，将最相关的文档返回给用户作为答案。稀疏检索的典型例子是BM25（Robertson等人，[1995](#bib.bib28)）。
- en: 'Here we evaluate both dense and sparse retrieval methods. For dense retrieval,
    we follow a similar process from Huang et al. ([2013](#bib.bib8)): we first encode
    the text using the GPT-embedding (OpenAI, [2022](#bib.bib19)) provided by OpenAI,
    then deploy vector database FAISS Index (Johnson et al., [2019](#bib.bib11)) to
    store the embeddings, enabling faster manipulation on them. For sparse retrieval,
    we deploy BM25 (Robertson et al., [1995](#bib.bib28)) which is considered the
    standard way.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们评估了密集检索和稀疏检索方法。对于密集检索，我们遵循Huang等人（[2013](#bib.bib8)）的类似过程：首先使用OpenAI提供的GPT-embedding（OpenAI，[2022](#bib.bib19)）对文本进行编码，然后部署向量数据库FAISS索引（Johnson等人，[2019](#bib.bib11)）来存储这些嵌入，从而实现更快的操作。对于稀疏检索，我们使用BM25（Robertson等人，[1995](#bib.bib28)），这被认为是标准方法。
- en: 4.1.1 Next Sentence Prediction
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 下一句预测
- en: The retrieved top-$k$ with maximum probability from NSP is selected as the best
    result (See Equation LABEL:eq:NSP).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从NSP中选出的前$k$个最大概率的结果被选为最佳结果（见方程 LABEL:eq:NSP）。
- en: '|  | $\displaystyle s$ |  | (1) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s$ |  | (1) |'
- en: '|  | $\displaystyle i$ |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle i$ |  |'
- en: 'We conduct experiments to evaluate the impact of including NSP into the retrieval-augmented
    LLM. Here we use OpenAI’s GPT-3.5-turbo as the base LLM and evaluate it on the
    FRDB dataset using the top-$1$ retrieval results as the context. The result is
    shown in Table [4](#S4.T4 "Table 4 ‣ 4.1.1 Next Sentence Prediction ‣ 4.1 Information
    Retrieval ‣ 4 Method ‣ TCRA-LLM: Token Compression Retrieval Augmented Large Language
    Model for Inference Cost Reduction"). There is a minor performance gain using
    NSP with both GPT-embedding and BM25 and thus we keep this NSP module in all our
    later experiments.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进行了实验，以评估将NSP纳入检索增强型LLM的影响。在这里，我们使用OpenAI的GPT-3.5-turbo作为基础LLM，并使用前$1$检索结果作为上下文在FRDB数据集上进行评估。结果如表[4](#S4.T4
    "Table 4 ‣ 4.1.1 Next Sentence Prediction ‣ 4.1 Information Retrieval ‣ 4 Method
    ‣ TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference
    Cost Reduction")所示。使用NSP与GPT-embedding和BM25结合时，性能有所提升，因此我们在所有后续实验中保留此NSP模块。'
- en: '| Method | Acc. (%) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 准确率 (%) |'
- en: '| --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Embedding | 89.1 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Embedding | 89.1 |'
- en: '| Embedding +NSP | 90.2 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Embedding +NSP | 90.2 |'
- en: '| BM25 | 83.4 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | 83.4 |'
- en: '| BM25+NSP | 84.9 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| BM25+NSP | 84.9 |'
- en: 'Table 4: Performance comparison of retrieval methods with and without NSP.
    The retrieved sentences are directly used as context and fed into GPT-3.5-turbo.
    Acc. stands for accuracy.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：带有和不带有NSP的检索方法性能比较。检索到的句子直接用作上下文并输入到GPT-3.5-turbo中。Acc. 代表准确率。
- en: From the experiment, we also see that the combination of dense retrieval with
    the NSP approach obtain the highest accuracy. We tune the value of $k$ is the
    optimal choice and we will adhere to this value in all our subsequent experiments.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从实验中，我们还发现，密集检索与NSP方法的组合获得了最高的准确率。我们调整了$k$的值，以确定最佳选择，并将在我们所有后续的实验中坚持这个值。
- en: '![Refer to caption](img/ffd581c2248941115f75c019b8efceee.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ffd581c2248941115f75c019b8efceee.png)'
- en: 'Figure 3: Evaluation of the retrieval performance using GPT-embedding and NSP
    with different choices of $k$.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用GPT-embedding和NSP对不同$k$值的检索性能评估。
- en: 4.2 Token Compression
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 令牌压缩
- en: The retrieval text is usually long and easily consume a huge amount of space
    from the input tokens during the API calls while using commercial LLMs. In order
    to mitigate this, we propose two methods to compress the retrieved text. The first
    one is the summarization compression which produces shorten the original text
    by rephrasing. The second method is semantic compression which we perturb the
    original sentence and rank the impact of the semantic change from each word in
    the sentence. The words with lower semantic impact on the sentence are removed.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 检索文本通常较长，使用商业LLM时在API调用过程中会消耗大量输入令牌。为缓解这一问题，我们提出了两种压缩检索文本的方法。第一种是总结压缩，通过重新表述缩短原始文本。第二种方法是语义压缩，我们扰动原句，并评估句子中每个单词对语义变化的影响。对句子语义影响较小的单词将被删除。
- en: 4.2.1 Summarization Compression
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 总结压缩
- en: Summarization models like the mT5 model (Xue et al., [2020](#bib.bib42)) have
    been widely used in many applications to shorten the input text, but they could
    not output summary with arbitrary length due to the constraint of its training
    data. To solve this, we propose to build a summarization model that is able to
    output summary with various lengths.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 像mT5模型（Xue et al., [2020](#bib.bib42)）这样的总结模型已广泛用于许多应用中以缩短输入文本，但由于训练数据的限制，它们无法输出任意长度的摘要。为了解决这个问题，我们提议建立一个能够输出各种长度摘要的总结模型。
- en: 'To build such a model, we leverage the power of self-instruct (Wang et al.,
    [2022](#bib.bib39)) where we use GPT-3.5-turbo to generate training datasets.
    The procedure of the data generation is shown in Figure [4](#S4.F4 "Figure 4 ‣
    4.2.1 Summarization Compression ‣ 4.2 Token Compression ‣ 4 Method ‣ TCRA-LLM:
    Token Compression Retrieval Augmented Large Language Model for Inference Cost
    Reduction"). First, we start with a text $x$ from the dataset, then pack it with
    additional prompt instruction as we illustrated in Figure [4](#S4.F4 "Figure 4
    ‣ 4.2.1 Summarization Compression ‣ 4.2 Token Compression ‣ 4 Method ‣ TCRA-LLM:
    Token Compression Retrieval Augmented Large Language Model for Inference Cost
    Reduction") and send it to GPT-3.5-turbo to generate a summary. If the length
    of the summary meets requirements, the procedure is ended; otherwise, a follow-up
    prompt will instruct GPT-3.5-turbo to further shorten the summary to the desired
    length. By doing this, we build a collection of training datasets with different
    summary length. We build three datasets that are 30%, 50%, and 70% of their original
    length. Each dataset is used to fine-tune one summary model independently. We
    randomly extract from FRDB and generate 400, 50, and 50 samples for training,
    validation, and testing respectively. Training on the generated datasets not only
    enables the model to produce summaries of the desired length, but also familiarizes
    the model with domain-specific information by doing further domain adaptation (Gururangan
    et al., [2020](#bib.bib7)).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立这样的模型，我们利用自我指导的力量 (Wang 等, [2022](#bib.bib39))，使用 GPT-3.5-turbo 生成训练数据集。数据生成的过程如图
    [4](#S4.F4 "图 4 ‣ 4.2.1 总结压缩 ‣ 4.2 令牌压缩 ‣ 4 方法 ‣ TCRA-LLM：令牌压缩检索增强大语言模型用于推理成本减少")
    所示。首先，我们从数据集中开始，取一段文本 $x$，然后用附加的提示指令对其进行打包，如图 [4](#S4.F4 "图 4 ‣ 4.2.1 总结压缩 ‣ 4.2
    令牌压缩 ‣ 4 方法 ‣ TCRA-LLM：令牌压缩检索增强大语言模型用于推理成本减少") 所示，并发送给 GPT-3.5-turbo 生成摘要。如果摘要长度符合要求，程序结束；否则，后续提示将指导
    GPT-3.5-turbo 进一步缩短摘要至所需长度。通过这种方式，我们建立了一个具有不同摘要长度的训练数据集集合。我们建立了三个数据集，分别为原始长度的
    30%、50% 和 70%。每个数据集用于独立微调一个摘要模型。我们从 FRDB 随机提取数据，生成 400、50 和 50 个样本用于训练、验证和测试。对生成的数据集进行训练不仅使模型能够生成所需长度的摘要，还通过进一步的领域适应
    (Gururangan 等, [2020](#bib.bib7)) 使模型熟悉领域特定信息。
- en: '![Refer to caption](img/53bfffef4504af6ddbeb1b1350ac566d.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/53bfffef4504af6ddbeb1b1350ac566d.png)'
- en: 'Figure 4: Self-instruct training data generation.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：自我指导训练数据生成。
- en: 4.2.2 Semantic Compression
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 语义压缩
- en: 'We propose another compression method based on perturbations of the original
    sentence and ranking the impact of the semantic importance for each word in the
    sentence where words with less importance will be removed. We deploy a multi-lingual
    sentence-transformer (Reimers and Gurevych, [2020](#bib.bib26)) to encode a sentence
    into embedding $\chi_{0}$-th percentile elements:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种基于原句扰动和对句子中每个单词语义重要性进行排名的压缩方法，其中重要性较低的单词将被移除。我们使用多语言句子变换器 (Reimers 和
    Gurevych, [2020](#bib.bib26)) 将句子编码为嵌入 $\chi_{0}$-百分位元素：
- en: '|  | 
    |  | (2) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | 
    |  | (2) |'
- en: The words corresponding to the elements in set $\mathcal{L}_{j}$ are extracted
    as the context for the LLM.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与集合 $\mathcal{L}_{j}$ 中元素对应的单词被提取作为 LLM 的上下文。
- en: 5 Evaluation
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估
- en: 5.1 Experiment Setup
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: 'We conduct studies on the FRDB dataset. The summarization compression is based
    on the pre-trained mT5-multilingual-XLSum (Xue et al., [2020](#bib.bib42)). The
    maximum input and output length is set to 512, the learning rate is 2e-5, the
    number of epochs is set to 10, the batch size is 2, and the rest of the settings
    follow the default settings from the models. The training of the mT5 models are
    conducted on the server that contains an AMD EPYC 7763 CPU, 256GB RAM, and NVIDIA
    4090 GPU with 24GB memory. Following the method described in section [4.2.1](#S4.SS2.SSS1
    "4.2.1 Summarization Compression ‣ 4.2 Token Compression ‣ 4 Method ‣ TCRA-LLM:
    Token Compression Retrieval Augmented Large Language Model for Inference Cost
    Reduction"), three shortened versions of the summarization dataset with 30%, 50%,
    and 70% of its original length are generated. Each version is used to fine-tune
    an mT5 summarization model. The pre-trained multi-lingual sentence-transformer (Reimers
    and Gurevych, [2020](#bib.bib26)) is used as an embedding encoder for semantic
    compression. Three compressed sentences in 30%, 50%, and 70% of their original
    length are generated. The GPT-3.5-turbo (OpenAI, [2023c](#bib.bib22)) is used
    for processing prompts generated by our methods.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对 FRDB 数据集进行了研究。摘要压缩基于预训练的 mT5-multilingual-XLSum（Xue 等人，[2020](#bib.bib42)）。最大输入和输出长度设置为
    512，学习率为 2e-5，训练轮数设置为 10，批量大小为 2，其余设置遵循模型的默认设置。mT5 模型的训练在包含 AMD EPYC 7763 CPU、256GB
    RAM 和 NVIDIA 4090 GPU（24GB 内存）的服务器上进行。按照第 [4.2.1](#S4.SS2.SSS1 "4.2.1 摘要压缩 ‣ 4.2
    标记压缩 ‣ 4 方法 ‣ TCRA-LLM: 用于推理成本降低的标记压缩检索增强大语言模型") 节中描述的方法，生成了摘要数据集的三个缩短版本，分别为原始长度的
    30%、50% 和 70%。每个版本用于微调一个 mT5 摘要模型。预训练的多语言句子转换器（Reimers 和 Gurevych，[2020](#bib.bib26)）被用作语义压缩的嵌入编码器。生成了原始长度的
    30%、50% 和 70% 的三个压缩句子。GPT-3.5-turbo（OpenAI，[2023c](#bib.bib22)）用于处理我们方法生成的提示。'
- en: 5.2 Token Compression Results
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 标记压缩结果
- en: '![Refer to caption](img/eb9a7e1aa32c0b5c53ff86c5f001318a.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/eb9a7e1aa32c0b5c53ff86c5f001318a.png)'
- en: 'Figure 5: Performance comparison of token compression methods. The horizontal
    dashed lines represent accuracy from the methods that do not output variable token
    lengths.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：标记压缩方法的性能比较。水平虚线表示那些不输出可变标记长度的方法的准确性。
- en: 'We conduct experiments on FRDB which contains 1,000 maternity/infant food related
    domain-specific multiple-choice questions, each of which only contains one correct
    answer. Three sentence-compression methods are evaluated in our experiments: 1)
    random deletion which randomly deletes words from a sentence, 2) summarization
    compression, and 3) semantic compression.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 FRDB 上进行实验，该数据集包含 1,000 道与孕产妇/婴儿食品相关的领域特定的多项选择题，每道题仅有一个正确答案。我们在实验中评估了三种句子压缩方法：1)
    随机删除，即随机删除句子中的单词，2) 摘要压缩，以及 3) 语义压缩。
- en: 'The experiment results are shown in Figure [5](#S5.F5 "Figure 5 ‣ 5.2 Token
    Compression Results ‣ 5 Evaluation ‣ TCRA-LLM: Token Compression Retrieval Augmented
    Large Language Model for Inference Cost Reduction"). To construct the baseline,
    we evaluate the GPT-3.5-turbo performance in two configurations: without retrieval
    and with retrieval but without token compression. We observe from the results
    that, once additional information is fed into the model as context, the accuracy
    immediately improves, from 51% to 90.2%. This shows the benefit that retrieving
    domain information brings to the LLM.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果如图 [5](#S5.F5 "图 5 ‣ 5.2 标记压缩结果 ‣ 5 评估 ‣ TCRA-LLM: 用于推理成本降低的标记压缩检索增强大语言模型")
    所示。为了构建基准线，我们评估了 GPT-3.5-turbo 在两种配置下的表现：没有检索和有检索但没有标记压缩。从结果中我们观察到，一旦将额外信息作为上下文输入到模型中，准确率立即从
    51% 提升至 90.2%。这显示了检索领域信息对大语言模型带来的好处。'
- en: Next, we compare using the original pre-trained mT5 based model with using the
    version where the model is fine-tuned on the datasets generated by self-instruction.
    With fine-tuning, the accuracy improves dramatically from 60% to 90.6%. The summarization
    model without fine-tuning output has an average length of 15 words, compare with
    88, 46, and 21 words for our 70%, 50%, and 30% length dataset fine-tuned model
    respectively. It shows that the summarization model might remove critical information
    by mistake if the input text is on a topic that the summarization model is not
    familiar with. A small fine-tuning set (400 samples) is enough to help the summarization
    model adapting to the new domain.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们比较了使用原始预训练的 mT5 模型与使用在自我指导生成的数据集上微调过的模型。通过微调，准确率从 60% 显著提高到 90.6%。未经微调的总结模型的输出平均长度为
    15 个词，而我们 70%、50% 和 30% 长度数据集微调模型的平均长度分别为 88、46 和 21 个词。这表明，如果输入文本的主题是总结模型不熟悉的，总结模型可能会错误地移除关键信息。一个小的微调集（400
    个样本）足以帮助总结模型适应新领域。
- en: The second compression method we propose is semantic compression. It has better
    flexibility to generate variable lengths of tokens but delivers lower performance
    than the summarization compression method. The baseline for our experiment is
    random deletion where we randomly delete a certain percentage of words. This random
    deletion consistently scores lower performance than both of our proposed algorithms.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的第二种压缩方法是语义压缩。它具有更好的灵活性，可以生成不同长度的令牌，但性能低于总结压缩方法。我们的实验基线是随机删除，我们随机删除一定比例的词汇。与我们提出的两种算法相比，这种随机删除的方法性能始终较低。
- en: 5.3 Cost Reduction Comparison
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 成本降低比较
- en: Our proposed summarization compression method is tested on a server with one
    NVIDIA 4090 GPU (24GB), 32GB RAM and Intel i7 CPU. The run-time for such a system
    is on average 0.383s per sample. A similar server on the AWS, i.e., g5.2xlarge,
    has an A10G GPU, 32GB RAM, and 8-core vCPU and the hourly price for such a system
    is $0.485\. In one hour, such a system can process approximately 9,400 summarizations,
    so the cost per summarization is $5.16e-5\. Assume the system utilization rate
    is only 50%, it means that the cost per summarization is $1.0319e-04.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的总结压缩方法在一台配备有 NVIDIA 4090 GPU（24GB）、32GB RAM 和 Intel i7 CPU 的服务器上进行了测试。该系统的平均运行时间为每个样本
    0.383 秒。AWS 上类似的服务器，即 g5.2xlarge，配有 A10G GPU、32GB RAM 和 8 核 vCPU，该系统的每小时价格为 $0.485。一个小时内，该系统可以处理大约
    9,400 个总结，因此每个总结的成本为 $5.16e-5。如果系统利用率仅为 50%，则每个总结的成本为 $1.0319e-04。
- en: The GPT-3.5-turbo itself does not have enough knowledge to precisely answer
    the question from our dataset (51% accuracy if without additional context in the
    prompt). Thus, both the common retrieval-augmented GPT-3.5-turbo and our system
    (retrieval-augmented GPT-3.5-turbo with additional token compression) require
    dense retrieval to reach acceptable performance, and the retrieval cost, which
    is $0.0001 per 1,000 query tokens, is the same for both systems. Since the original
    questions are typically short, we can assume that the average length of them is
    about 128 tokens, which translates into $1.2500e-05 per question for the retrieval.
    Assuming at full size the input has 512 tokens and the output has 64 tokens, the
    total cost for the common retrieval-augmented GPT-3.5-turbo (for both retrieval
    and QA using API calls) is about $8.9050e-04 per question. In comparison, our
    algorithm can compress the retrieved context of GPT-3.5-turbo to 35% of its original
    length, which translates into an averagely 50% of reduction during the API calls.
    Thus, The cost using our token compression system is around $6.1869e-04 per question.
    It reduces the overall costs by 30%.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3.5-turbo 本身没有足够的知识来准确回答我们数据集中提出的问题（如果没有额外的上下文，准确率为 51%）。因此，无论是常规的检索增强型
    GPT-3.5-turbo 还是我们的系统（检索增强型 GPT-3.5-turbo 加上额外的令牌压缩），都需要密集检索以达到可接受的性能，检索成本为每 1,000
    个查询令牌 $0.0001，两者相同。由于原始问题通常较短，我们可以假设它们的平均长度约为 128 个令牌，这意味着每个问题的检索成本为 $1.2500e-05。假设在全尺寸下输入有
    512 个令牌，输出有 64 个令牌，则常规检索增强型 GPT-3.5-turbo（包括检索和使用 API 调用的 QA）的总成本约为每个问题 $8.9050e-04。相比之下，我们的算法可以将
    GPT-3.5-turbo 检索到的上下文压缩到原始长度的 35%，这在 API 调用期间平均减少了 50%。因此，使用我们的令牌压缩系统的成本约为每个问题
    $6.1869e-04。它将整体成本降低了 30%。
- en: 5.4 Information Entropy vs. Accuracy
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 信息熵与准确率
- en: 'In the next experiment, we investigate the impact of the information entropy
    on the accuracy of different token compression methods. We measure the word frequency
    from the FRDB dataset and use it as a probabilistic model to calculate the information
    entropy of each word. The mean word information entropy is calculated on each
    sentence to normalize the entropy. The results for the three token compression
    methods is shown in Figure [6](#S5.F6 "Figure 6 ‣ 5.4 Information Entropy vs.
    Accuracy ‣ 5 Evaluation ‣ TCRA-LLM: Token Compression Retrieval Augmented Large
    Language Model for Inference Cost Reduction"). The random deletion method removes
    words randomly which leads to the average information entropy for different sentence
    lengths approximately the same. On the other hand, the semantic compression algorithm
    removes the words that have less semantic meaning. Our experiment shows that,
    the average information entropy goes lower as sentences become shorter, indicating
    that the sentence becomes less compressible. Additionally, the average word information
    entropy is positively correlated with the accuracy when semantic compression is
    used, showing that higher information will benefit the model performance. On the
    contrary, the summarization compression shows distinct phenomenon. Instead of
    naively removing words, the summarization compression compresses the original
    sentences into different lengths by rephrasing sentences. By doing this, the shortened
    sentences obtain lower average information entropy but the accuracy stays at a
    similar level in comparison with the original sentences. The lower average information
    entropy indicates that sentences become more condensed but the semantic of the
    sentences stays approximately the same.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一次实验中，我们调查信息熵对不同令牌压缩方法准确性的影响。我们从 FRDB 数据集中测量词频，并将其用作概率模型来计算每个词的信息熵。我们在每个句子上计算平均词信息熵，以归一化熵。三种令牌压缩方法的结果如图[6](#S5.F6
    "图 6 ‣ 5.4 信息熵与准确性 ‣ 5 评估 ‣ TCRA-LLM：令牌压缩检索增强大型语言模型用于推理成本减少")所示。随机删除方法随机删除单词，这导致不同句长的平均信息熵大致相同。另一方面，语义压缩算法删除语义意义较少的单词。我们的实验表明，随着句子变短，平均信息熵降低，表明句子变得不那么可压缩。此外，使用语义压缩时，平均词信息熵与准确性呈正相关，表明较高的信息将有利于模型性能。相反，总结压缩则表现出不同的现象。总结压缩通过重新表述句子将原句压缩为不同长度，而不是简单地删除单词。通过这种方式，缩短的句子获得较低的平均信息熵，但与原句相比，准确性保持在类似水平。较低的平均信息熵表明句子变得更加紧凑，但句子的语义保持大致不变。
- en: '![Refer to caption](img/1047879a72cd74b579e1839566afa01d.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1047879a72cd74b579e1839566afa01d.png)'
- en: 'Figure 6: Impact of average information entropy on accuracy.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：平均信息熵对准确性的影响。
- en: Next, we investigate the impact of cosine similarity between original and compressed
    sentences on accuracy and we find a positive correlation between accuracy and
    cosine similarity value. It indicates closer to the original semantic meaning
    would produce better accuracy.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们调查了原句与压缩句之间的余弦相似度对准确性的影响，我们发现准确性与余弦相似度值之间存在正相关。这表明，接近原始语义的内容会产生更好的准确性。
- en: '![Refer to caption](img/651d87e003d918808743969547b79d9a.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/651d87e003d918808743969547b79d9a.png)'
- en: 'Figure 7: Impact of cosine similarity between original and compressed sentence
    on accuracy.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：原句与压缩句之间的余弦相似度对准确性的影响。
- en: 5.5 Summarization Statistics
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 总结统计
- en: 'Our summarization model is built based on the pre-trained mT5 model and fine-tuned
    on our self-instruct generated dataset. We generate three datasets which are 30%,
    50%, and 70% of the length compared to their original text. Three different models
    are fine-tuned independently. Figure [8](#S5.F8 "Figure 8 ‣ 5.5 Summarization
    Statistics ‣ 5 Evaluation ‣ TCRA-LLM: Token Compression Retrieval Augmented Large
    Language Model for Inference Cost Reduction") shows the distribution of sentence
    length from our three fine-tuned models. At 70% compression, the summary text
    shifts from an average of 88 words to 46 words for 50% length and 21 words for
    30% length.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的总结模型基于预训练的 mT5 模型构建，并在我们的自我指导生成数据集上进行了微调。我们生成了三个数据集，分别是原文长度的 30%、50% 和 70%。三个不同的模型独立地进行了微调。图[8](#S5.F8
    "图 8 ‣ 5.5 总结统计 ‣ 5 评估 ‣ TCRA-LLM：令牌压缩检索增强大型语言模型用于推理成本减少")展示了我们三个微调模型的句子长度分布。在
    70% 压缩下，摘要文本的长度从平均 88 个词减少到 50% 长度的 46 个词和 30% 长度的 21 个词。
- en: '![Refer to caption](img/81c79e6aeb0ebb20de609899eb145f81.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/81c79e6aeb0ebb20de609899eb145f81.png)'
- en: 'Figure 8: Distribution of sentence lengths with different summarization lengths.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 不同总结长度下句子长度的分布。'
- en: 5.6 Compression Rate vs. Entropy
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 压缩率与熵
- en: 'We conduct a study on the compression rate of our mT5 model fine-tuned with
    the 30% length dataset. Our input consists of 1) the original length of the sentence,
    2) average word entropy, and 3) accumulated word entropy. We deploy a simple linear
    regression algorithm to predict the compression rate. The result is shown in Fig. [9](#S5.F9
    "Figure 9 ‣ 5.6 Compression Rate vs. Entropy ‣ 5 Evaluation ‣ TCRA-LLM: Token
    Compression Retrieval Augmented Large Language Model for Inference Cost Reduction").
    We find that, there is a positive correlation (0.31) between our selected input
    and compression rate with an RMSE of 11.4% and R-squared of 9.6%. This indicates
    the compression rate of each sentence can be estimated prior to the summarization
    process.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对使用30%长度数据集微调的mT5模型的压缩率进行了研究。我们的输入包括 1) 句子的原始长度，2) 平均词熵，和 3) 累积词熵。我们使用简单的线性回归算法来预测压缩率。结果如图 [9](#S5.F9
    "Figure 9 ‣ 5.6 Compression Rate vs. Entropy ‣ 5 Evaluation ‣ TCRA-LLM: Token
    Compression Retrieval Augmented Large Language Model for Inference Cost Reduction")所示。我们发现，所选输入与压缩率之间存在正相关（0.31），RMSE为11.4%，R-squared为9.6%。这表明每个句子的压缩率可以在总结过程之前进行估计。'
- en: '![Refer to caption](img/c771f42535e902069beac9c9aec47d21.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c771f42535e902069beac9c9aec47d21.png)'
- en: 'Figure 9: Visualization of multi-variable linear regression on predicting compression
    rate.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 多变量线性回归预测压缩率的可视化。'
- en: 5.7 Ablation Study
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7 消融研究
- en: 'From our experiments, we find the summarization compression method delivers
    the best performance. Here we compare different retrieval methods and investigate
    what the optimal settings are. Four configurations are evaluated: embedding only,
    BM25 only, embedding first then BM25, and BM25 first then embedding. The first
    two configurations are straightforward; in the third configuration, we apply the
    embedding-based method to extract the top-$q$ based on previous experiments. The
    evaluation results are shown in Table [5](#S5.T5 "Table 5 ‣ 5.7 Ablation Study
    ‣ 5 Evaluation ‣ TCRA-LLM: Token Compression Retrieval Augmented Large Language
    Model for Inference Cost Reduction"). We find the straightforward dense retrieval
    approach achieves the best performance.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '从我们的实验中，我们发现总结压缩方法表现最佳。在这里，我们比较了不同的检索方法，并调查了什么是**最佳设置**。评估了四种配置：仅嵌入，仅BM25，先嵌入再BM25，以及先BM25再嵌入。前两种配置比较直接；在第三种配置中，我们应用基于嵌入的方法来提取基于先前实验的前-$q$。评估结果如表 [5](#S5.T5
    "Table 5 ‣ 5.7 Ablation Study ‣ 5 Evaluation ‣ TCRA-LLM: Token Compression Retrieval
    Augmented Large Language Model for Inference Cost Reduction")所示。我们发现，直接的密集检索方法表现最佳。'
- en: '| Method | Top-$q$ | Acc. (%) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 前-$q$ | 准确率 (%) |'
- en: '| --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Embedding | n/a | 90.9 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入 | 不适用 | 90.9 |'
- en: '| Embedding+BM25 | 10 | 89.7 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入+BM25 | 10 | 89.7 |'
- en: '| Embedding+BM25 | 100 | 88.0 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入+BM25 | 100 | 88.0 |'
- en: '| BM25 | n/a | 74.7 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | 不适用 | 74.7 |'
- en: '| BM25+Embedding | 10 | 89.3 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| BM25+嵌入 | 10 | 89.3 |'
- en: '| BM25+Embedding | 100 | 89.8 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| BM25+嵌入 | 100 | 89.8 |'
- en: 'Table 5: Evaluation of different retrieval algorithm configurations. The top-$q$
    value used in the search. Acc. stands for accuracy.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 不同检索算法配置的评估。搜索中使用的前-$q$值。Acc. 代表准确率。'
- en: 6 Conclusion
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we propose two methods that reduce the token size for retrieval-augmented
    LLM. Additionally, we propose a food recommendation dataset contains domain-specific
    knowledge to benchmark the performance of retrieval-augmented LLM performance
    on the GPT-3.5-turbo. We carefully select a subset that focuses on 200 types of
    food recommendations for maternity and infant people. Without retrieval augmentation,
    the commercial GPT-3.5-turbo model is only able to get 51% percent of the question
    right, compared to 90.2% with retrieved context. We use this 90.2% as our goal
    and compare the performance of different token compression algorithms. Our proposed
    summarization compression achieves the best performance, reaching 90.5% of accuracy
    with 65% token reduction on the retrieved context. It indicates that the summarized
    text maintains a similar level of critical information but with a significantly
    shorter length, showing promising ability of our proposed method. The semantic
    compression method can further remove the words that have lower semantic meaning
    and provides a more flexible way to trade-off the length of sentences with accuracy.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了两种方法来减少检索增强型LLM的令牌大小。此外，我们提出了一个包含领域特定知识的食品推荐数据集，用于基准测试检索增强型LLM在GPT-3.5-turbo上的性能。我们精心选择了一个关注200种食品推荐的子集，专注于孕产妇和婴儿人群。在没有检索增强的情况下，商业GPT-3.5-turbo模型只能正确回答51%的问题，而在检索上下文的情况下，正确率达到90.2%。我们使用90.2%作为目标，并比较不同的令牌压缩算法的性能。我们提出的摘要压缩实现了最佳性能，在检索上下文中实现了65%的令牌减少率，准确率达到90.5%。这表明，摘要文本保持了类似水平的关键信息，但长度显著缩短，显示了我们提出方法的良好能力。语义压缩方法可以进一步去除语义意义较低的词汇，并提供一种更灵活的方式来权衡句子的长度与准确性。
- en: Limitations
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: The goal of our model is to reduce the token size for retrieval-augmented LLMs.
    The compression rate is determined based on the methods’ ability to condense sentences
    while preserving as much of their essential information as possible. If the sentences
    are already short and compacted in meaning, the compression rate won’t be able
    to be low if we want to maintain most of the critical information within the sentence.
    Our algorithm is designed for large commercial LLMs and smaller open-source LLMs
    may not experience similar levels of performance gain.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的目标是减少检索增强型LLMs的令牌大小。压缩率基于方法在尽可能保留其本质信息的同时，压缩句子的能力。如果句子本身已经很短且含义紧凑，那么如果我们想保持句子中的大部分关键信息，压缩率将无法降低。我们的算法设计是针对大型商业LLMs的，较小的开源LLMs可能无法体验到类似水平的性能提升。
- en: Ethics Statement
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: Our proposed methods are designed solely for the goal of reducing cost while
    maintaining the performance using commercial LLMs through API calls. The algorithms
    are not designed for activities that are in violation of human rights or with
    military purposes. The data we collected for our proposed dataset does not contain
    any privacy information.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的方法完全是为了降低成本，同时通过API调用保持商业LLMs的性能。算法并未针对违反人权或军事目的的活动进行设计。我们为所提数据集收集的数据不包含任何隐私信息。
- en: References
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ashok and Lipton (2023) Dhananjay Ashok and Zachary C Lipton. 2023. PromptNER:
    Prompting For Named Entity Recognition. *arXiv preprint arXiv:2305.15444*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿肖克和利普顿（2023）达南杰·阿肖克和扎卡里·C·利普顿。2023年。PromptNER：为命名实体识别提供提示。*arXiv 预印本 arXiv:2305.15444*。
- en: Bolton et al. (2022) E Bolton, D Hall, M Yasunaga, T Lee, C Manning, and P Liang.
    2022. Stanford crfm introduces pubmedgpt 2.7 b.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 博尔顿等（2022）E·博尔顿、D·霍尔、M·安永、T·李、C·曼宁和P·梁。2022年。斯坦福crfm推出pubmedgpt 2.7 b。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布朗等（2020）汤姆·布朗、本杰明·曼、尼克·赖德、梅兰妮·苏比亚、贾雷德·D·卡普兰、普拉富拉·达里瓦尔、阿尔文·尼拉坎坦、普拉纳夫·夏姆、吉里什·萨斯特里、阿曼达·阿斯克尔等。2020年。语言模型是少样本学习者。*神经信息处理系统进展*，33：1877–1901。
- en: 'Chen et al. (2023) Lingjiao Chen, Matei Zaharia, and James Zou. 2023. FrugalGPT:
    How to Use Large Language Models While Reducing Cost and Improving Performance.
    *arXiv preprint arXiv:2305.05176*.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2023）陈灵角、马泰伊·扎哈里亚和詹姆斯·邹。2023年。FrugalGPT：如何在降低成本和提高性能的同时使用大型语言模型。*arXiv 预印本
    arXiv:2305.05176*。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery 等人（2022）**Aakanksha Chowdhery**、**Sharan Narang**、**Jacob Devlin**、**Maarten
    Bosma**、**Gaurav Mishra**、**Adam Roberts**、**Paul Barham**、**Hyung Won Chung**、**Charles
    Sutton**、**Sebastian Gehrmann** 等。2022年。Palm：通过路径扩展语言建模。*arXiv 预印本 arXiv:2204.02311*。
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等人（2018）**Jacob Devlin**、**Ming-Wei Chang**、**Kenton Lee** 和 **Kristina
    Toutanova**。2018年。Bert：深度双向变换器的预训练用于语言理解。*arXiv 预印本 arXiv:1810.04805*。
- en: 'Gururangan et al. (2020) Suchin Gururangan, Ana Marasović, Swabha Swayamdipta,
    Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. [Don’t stop pretraining:
    Adapt language models to domains and tasks](https://doi.org/10.18653/v1/2020.acl-main.740).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 8342–8360, Online. Association for Computational Linguistics.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gururangan 等人（2020）**Suchin Gururangan**、**Ana Marasović**、**Swabha Swayamdipta**、**Kyle
    Lo**、**Iz Beltagy**、**Doug Downey** 和 **Noah A. Smith**。2020年。 [不要停止预训练：将语言模型适应领域和任务](https://doi.org/10.18653/v1/2020.acl-main.740)。收录于
    *第58届计算语言学协会年会论文集*，页码 8342–8360，在线。计算语言学协会。
- en: Huang et al. (2013) Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero,
    and Larry Heck. 2013. Learning deep structured semantic models for web search
    using clickthrough data. In *Proceedings of the 22nd ACM international conference
    on Information & Knowledge Management*, pages 2333–2338.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2013）**Po-Sen Huang**、**Xiaodong He**、**Jianfeng Gao**、**Li Deng**、**Alex
    Acero** 和 **Larry Heck**。2013年。利用点击数据学习深度结构化语义模型用于网页搜索。收录于 *第22届ACM国际信息与知识管理会议论文集*，页码
    2333–2338。
- en: 'Inaba et al. (2023) Tatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, and Sadao
    Kurohashi. 2023. MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain
    of Thought Prompting. *arXiv preprint arXiv:2305.16896*.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inaba 等人（2023）**Tatsuro Inaba**、**Hirokazu Kiyomaru**、**Fei Cheng** 和 **Sadao
    Kurohashi**。2023年。MultiTool-CoT：GPT-3 可以通过思维链提示使用多个外部工具。*arXiv 预印本 arXiv:2305.16896*。
- en: Izacard et al. (2022) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini,
    Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,
    and Edouard Grave. 2022. Few-shot learning with retrieval augmented language models.
    *arXiv preprint arXiv:2208.03299*.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard 等人（2022）**Gautier Izacard**、**Patrick Lewis**、**Maria Lomeli**、**Lucas
    Hosseini**、**Fabio Petroni**、**Timo Schick**、**Jane Dwivedi-Yu**、**Armand Joulin**、**Sebastian
    Riedel** 和 **Edouard Grave**。2022年。基于检索增强语言模型的少样本学习。*arXiv 预印本 arXiv:2208.03299*。
- en: Johnson et al. (2019) Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale
    similarity search with gpus. *IEEE Transactions on Big Data*, 7(3):535–547.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等人（2019）**Jeff Johnson**、**Matthijs Douze** 和 **Hervé Jégou**。2019年。使用
    GPU 的亿级规模相似性搜索。*IEEE 大数据期刊*，7(3)：535–547。
- en: Jones (1973) Karen Sparck Jones. 1973. Index term weighting. *Information storage
    and retrieval*, 9(11):619–633.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jones（1973）**Karen Sparck Jones**。1973年。索引术语加权。*信息存储与检索*，9(11)：619–633。
- en: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage
    retrieval for open-domain question answering. *arXiv preprint arXiv:2004.04906*.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpukhin 等人（2020）**Vladimir Karpukhin**、**Barlas Oğuz**、**Sewon Min**、**Patrick
    Lewis**、**Ledell Wu**、**Sergey Edunov**、**Danqi Chen** 和 **Wen-tau Yih**。2020年。用于开放领域问答的密集段落检索。*arXiv
    预印本 arXiv:2004.04906*。
- en: 'Lewis et al. (2020) Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio
    Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau
    Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. [Retrieval-augmented
    generation for knowledge-intensive NLP tasks](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html).
    In *Advances in Neural Information Processing Systems 33: Annual Conference on
    Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
    virtual*.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人（2020）**Patrick S. H. Lewis**、**Ethan Perez**、**Aleksandra Piktus**、**Fabio
    Petroni**、**Vladimir Karpukhin**、**Naman Goyal**、**Heinrich Küttler**、**Mike Lewis**、**Wen-tau
    Yih**、**Tim Rocktäschel**、**Sebastian Riedel** 和 **Douwe Kiela**。2020年。[检索增强生成用于知识密集型NLP任务](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html)。收录于
    *神经信息处理系统进展 33：2020年神经信息处理系统年会，NeurIPS 2020，2020年12月6日至12日，虚拟*。
- en: 'Lu et al. (2023) Guang Lu, Sylvia B Larcher, and Tu Tran. 2023. Hybrid Long
    Document Summarization using C2F-FAR and ChatGPT: A Practical Study. *arXiv preprint
    arXiv:2306.01169*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人（2023）**Guang Lu**、**Sylvia B Larcher** 和 **Tu Tran**。2023年。使用 C2F-FAR
    和 ChatGPT 的混合长文档摘要：一项实际研究。*arXiv 预印本 arXiv:2306.01169*。
- en: 'Mallen et al. (2022) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh
    Hajishirzi, and Daniel Khashabi. 2022. When Not to Trust Language Models: Investigating
    Effectiveness and Limitations of Parametric and Non-Parametric Memories. *arXiv
    preprint arXiv:2212.10511*.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mallen et al. (2022) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh
    Hajishirzi, 和 Daniel Khashabi. 2022. 不信任语言模型的时机：研究参数和非参数记忆的有效性及局限性。*arXiv 预印本
    arXiv:2212.10511*。
- en: 'Nguyen (2023) Ha-Thanh Nguyen. 2023. A Brief Report on LawGPT 1.0: A Virtual
    Legal Assistant Based on GPT-3. *arXiv preprint arXiv:2302.05729*.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen (2023) Ha-Thanh Nguyen. 2023. 关于 LawGPT 1.0 的简要报告：基于 GPT-3 的虚拟法律助理。*arXiv
    预印本 arXiv:2302.05729*。
- en: Ni et al. (2021) Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández
    Ábrego, Ji Ma, Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. 2021.
    Large dual encoders are generalizable retrievers. *arXiv preprint arXiv:2112.07899*.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ni et al. (2021) Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández
    Ábrego, Ji Ma, Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang 等。2021. 大型双编码器是通用的检索器。*arXiv
    预印本 arXiv:2112.07899*。
- en: OpenAI (2022) OpenAI. 2022. [New and improved embedding model](https://openai.com/blog/new-and-improved-embedding-model).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2022) OpenAI. 2022. [全新改进的嵌入模型](https://openai.com/blog/new-and-improved-embedding-model)。
- en: OpenAI (2023a) OpenAI. 2023a. [GPT-4 Technical Report](https://doi.org/10.48550/arXiv.2303.08774).
    *CoRR*, abs/2303.08774.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023a) OpenAI. 2023a. [GPT-4 技术报告](https://doi.org/10.48550/arXiv.2303.08774)。*CoRR*，abs/2303.08774。
- en: OpenAI (2023b) OpenAI. 2023b. [Introducing ChatGPT](https://openai.com/blog/chatgpt).
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023b) OpenAI. 2023b. [介绍 ChatGPT](https://openai.com/blog/chatgpt)。
- en: OpenAI (2023c) OpenAI. 2023c. [Introducing ChatGPT and Whisper APIs](https://openai.com/blog/introducing-chatgpt-and-whisper-apis).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023c) OpenAI. 2023c. [介绍 ChatGPT 和 Whisper API](https://openai.com/blog/introducing-chatgpt-and-whisper-apis)。
- en: Pichai (2023) Sundar Pichai. 2023. [An important next step on our AI journey](https://blog.google/technology/ai/bard-google-ai-search-updates).
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pichai (2023) Sundar Pichai. 2023. [我们 AI 旅程上的一个重要下一步](https://blog.google/technology/ai/bard-google-ai-search-updates)。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](http://jmlr.org/papers/v21/20-074.html).
    *Journal of Machine Learning Research*, 21(140):1–67.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J. Liu. 2020. [探索使用统一文本到文本转换器的迁移学习极限](http://jmlr.org/papers/v21/20-074.html)。*机器学习研究期刊*，21(140):1–67。
- en: Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon
    Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented
    language models. *arXiv preprint arXiv:2302.00083*.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon
    Shashua, Kevin Leyton-Brown, 和 Yoav Shoham. 2023. 上下文检索增强语言模型。*arXiv 预印本 arXiv:2302.00083*。
- en: Reimers and Gurevych (2020) Nils Reimers and Iryna Gurevych. 2020. [Making Monolingual
    Sentence Embeddings Multilingual using Knowledge Distillation](https://doi.org/10.18653/v1/2020.emnlp-main.365).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 4512–4525, Online. Association for Computational Linguistics.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reimers 和 Gurevych (2020) Nils Reimers 和 Iryna Gurevych. 2020. [利用知识蒸馏将单语句嵌入转化为多语种](https://doi.org/10.18653/v1/2020.emnlp-main.365)。在
    *2020年自然语言处理经验方法会议（EMNLP）论文集*，第4512–4525页，在线。计算语言学协会。
- en: 'Robertson et al. (2009) Stephen Robertson, Hugo Zaragoza, et al. 2009. The
    probabilistic relevance framework: BM25 and beyond. *Foundations and Trends® in
    Information Retrieval*, 3(4):333–389.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robertson et al. (2009) Stephen Robertson, Hugo Zaragoza 等。2009. 概率相关框架：BM25
    及其他。*信息检索基础与趋势®*，3(4):333–389。
- en: Robertson et al. (1995) Stephen E Robertson, Steve Walker, Susan Jones, Micheline M
    Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at TREC-3. *Nist Special Publication
    Sp*, 109:109.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robertson et al. (1995) Stephen E Robertson, Steve Walker, Susan Jones, Micheline
    M Hancock-Beaulieu, Mike Gatford 等。1995. Okapi 在 TREC-3 中。*Nist 特别出版物 Sp*，109:109。
- en: Santra et al. (2023) Bishal Santra, Sakya Basak, Abhinandan De, Manish Gupta,
    and Pawan Goyal. 2023. Frugal Prompting for Dialog Models. *arXiv preprint arXiv:2305.14919*.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santra et al. (2023) Bishal Santra, Sakya Basak, Abhinandan De, Manish Gupta,
    和 Pawan Goyal. 2023. 对话模型的节俭提示。*arXiv 预印本 arXiv:2305.14919*。
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Scao 等 (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, 等. 2022. Bloom: 一个176b参数的开放访问多语言模型。*arXiv 预印本 arXiv:2211.05100*。'
- en: Sheikh (2023) Jamiel Sheikh. 2023. [Bloomberg Uses Its Vast Data To Create New
    Finance AI](https://www.forbes.com/sites/jamielsheikh/2023/04/05/the-chatgpt-of-finance-is-here-bloomberg-is-combining-ai-and-fintech).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheikh (2023) Jamiel Sheikh. 2023. [彭博社利用其庞大的数据创建新的金融 AI](https://www.forbes.com/sites/jamielsheikh/2023/04/05/the-chatgpt-of-finance-is-here-bloomberg-is-combining-ai-and-fintech)。
- en: 'Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich
    James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented
    black-box language models. *arXiv preprint arXiv:2301.12652*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi 等 (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James,
    Mike Lewis, Luke Zettlemoyer, 和 Wen-tau Yih. 2023. Replug: 检索增强黑箱语言模型。*arXiv 预印本
    arXiv:2301.12652*。'
- en: 'Shuster et al. (2021) Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and
    Jason Weston. 2021. [Retrieval Augmentation Reduces Hallucination in Conversation](https://doi.org/10.18653/v1/2021.findings-emnlp.320).
    In *Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual
    Event / Punta Cana, Dominican Republic, 16-20 November, 2021*, pages 3784–3803\.
    Association for Computational Linguistics.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shuster 等 (2021) Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, 和 Jason
    Weston. 2021. [检索增强减少对话中的虚幻](https://doi.org/10.18653/v1/2021.findings-emnlp.320)。在
    *《计算语言学协会会议录: EMNLP 2021, 虚拟会议 / 多米尼加共和国蓬塔卡纳, 2021年11月16-20日》*，第3784–3803页。计算语言学协会。'
- en: Soong et al. (2023) David Soong, Sriram Sridhar, Han Si, Jan-Samuel Wagner,
    Ana Caroline Costa Sá, Christina Y Yu, Kubra Karagoz, Meijian Guan, Hisham Hamadeh,
    and Brandon W Higgs. 2023. Improving accuracy of GPT-3/4 results on biomedical
    data using a retrieval-augmented language model. *arXiv preprint arXiv:2305.17116*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Soong 等 (2023) David Soong, Sriram Sridhar, Han Si, Jan-Samuel Wagner, Ana Caroline
    Costa Sá, Christina Y Yu, Kubra Karagoz, Meijian Guan, Hisham Hamadeh, 和 Brandon
    W Higgs. 2023. 使用检索增强语言模型提高 GPT-3/4 在生物医学数据上的准确性。*arXiv 预印本 arXiv:2305.17116*。
- en: Sparck Jones (1972) Karen Sparck Jones. 1972. A statistical interpretation of
    term specificity and its application in retrieval. *Journal of documentation*,
    28(1):11–21.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sparck Jones (1972) Karen Sparck Jones. 1972. 术语特异性的统计解释及其在检索中的应用。*《文献学杂志》*,
    28(1):11–21。
- en: Thorne et al. (2021) James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri,
    Sebastian Riedel, and Alon Y. Levy. 2021. [From Natural Language Processing to
    Neural Databases](https://doi.org/10.14778/3447689.3447706). *Proc. VLDB Endow.*,
    14(6):1033–1039.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thorne 等 (2021) James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri,
    Sebastian Riedel, 和 Alon Y. Levy. 2021. [从自然语言处理到神经数据库](https://doi.org/10.14778/3447689.3447706)。*《VLDB
    会议录》*, 14(6):1033–1039。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, 等. 2023. Llama: 开放而高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*。'
- en: 'Wang et al. (2023) Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei
    Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. 2023. Gpt-ner: Named entity recognition
    via large language models. *arXiv preprint arXiv:2304.10428*.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 (2023) Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei
    Zhang, Jiwei Li, 和 Guoyin Wang. 2023. Gpt-ner: 基于大语言模型的命名实体识别。*arXiv 预印本 arXiv:2304.10428*。'
- en: 'Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-Instruct: Aligning
    Language Model with Self Generated Instructions. *arXiv preprint arXiv:2212.10560*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah
    A Smith, Daniel Khashabi, 和 Hannaneh Hajishirzi. 2022. Self-Instruct: 使语言模型与自生成指令对齐。*arXiv
    预印本 arXiv:2212.10560*。'
- en: 'Wu et al. (2023) Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark
    Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.
    2023. Bloomberggpt: A large language model for finance. *arXiv preprint arXiv:2303.17564*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等 (2023) Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze,
    Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, 和 Gideon Mann. 2023.
    Bloomberggpt: 一种大型语言模型用于金融。*arXiv 预印本 arXiv:2303.17564*。'
- en: 'Xiang et al. (2023) Tong Xiang, Liangzhi Li, Wangyue Li, Mingbai Bai, Lu Wei,
    Bowen Wang, and Noa Garcia. 2023. [CARE-MI: chinese benchmark for misinformation
    evaluation in maternity and infant care](https://doi.org/10.48550/arXiv.2307.01458).
    *CoRR*, abs/2307.01458.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiang等（2023）唐翔、梁志立、王跃、明白白、卢伟、鲍文·王和诺亚·加西亚。2023年。[CARE-MI: 中文虚假信息评估基准在母婴护理中的应用](https://doi.org/10.48550/arXiv.2307.01458)。*CoRR*，abs/2307.01458。'
- en: 'Xue et al. (2020) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami
    Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mT5: A massively
    multilingual pre-trained text-to-text transformer. *arXiv preprint arXiv:2010.11934*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xue等（2020）薛琳婷、诺亚·康斯坦特、亚当·罗伯茨、米赫尔·凯尔、拉米·阿尔-拉夫、阿迪提亚·西丹特、阿迪提亚·巴鲁阿和科林·拉费尔。2020年。mT5：一个大规模多语言预训练的文本到文本转换器。*arXiv预印本arXiv:2010.11934*。
- en: Yi et al. (2019) Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz
    Heldt, Aditee Kumthekar, Zhe Zhao, Li Wei, and Ed Chi. 2019. Sampling-bias-corrected
    neural modeling for large corpus item recommendations. In *Proceedings of the
    13th ACM Conference on Recommender Systems*, pages 269–277.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yi等（2019）易鑫阳、杨继、洪力婵、郑智远、卢卡斯·赫尔德、阿迪蒂·库姆特卡尔、赵哲、魏力和艾德·齐。2019年。针对大规模语料库项推荐的采样偏差校正神经建模。在*第13届ACM推荐系统会议论文集*，第269–277页。
- en: 'Yu et al. (2021) HongChien Yu, Chenyan Xiong, and Jamie Callan. 2021. [Improving
    query representations for dense retrieval with pseudo relevance feedback](https://doi.org/10.1145/3459637.3482124).
    In *CIKM ’21: The 30th ACM International Conference on Information and Knowledge
    Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021*, pages
    3592–3596\. ACM.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等（2021）于宏建、熊晨彦和杰米·卡兰。2021年。[通过伪相关反馈改进密集检索的查询表示](https://doi.org/10.1145/3459637.3482124)。在*CIKM
    ’21：第30届ACM国际信息与知识管理大会，虚拟活动，澳大利亚昆士兰，2021年11月1日 - 5日*，第3592–3596页。ACM。
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. 2022. Opt: Open pre-trained transformer language models. *arXiv preprint
    arXiv:2205.01068*.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等（2022）张苏珊、斯蒂芬·罗勒、纳曼·戈亚尔、米克尔·阿特克斯、摩亚·陈、舒惠·陈、克里斯托弗·德万、莫娜·迪亚布、李贤、许维多利亚等。2022年。Opt:
    开放预训练变换器语言模型。*arXiv预印本arXiv:2205.01068*。'
- en: 'Zhang et al. (2023) Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and
    Lidong Bing. 2023. Sentiment Analysis in the Era of Large Language Models: A Reality
    Check. *arXiv preprint arXiv:2305.15005*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2023）张文轩、邵悦、刘冰、潘欣诺·贾林和冰丽东。2023年。大语言模型时代的情感分析：现实检查。*arXiv预印本arXiv:2305.15005*。
- en: Appendix
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: A Example question for FRDB dataset
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FRDB数据集的示例问题
- en: Is it suitable for an infant to consume/have hot chocolate? ($1$) Avoid
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 婴儿是否适合食用/喝热巧克力？（$1$）避免
- en: '1.'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Food is beneficial to the current stage, and excessive consumption will not
    cause physical abnormalities.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 食物对当前阶段有益，过量食用不会导致身体异常。
- en: '2.'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Advocate a healthy lifestyle and advise users to intake relatively limited amount
    of foods, including foods with excessive salt, oil, sugar, and so on.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提倡健康的生活方式，并建议用户摄入相对有限的食物，包括过多的盐、油、糖等食物。
- en: '3.'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: There is evidence that it will cause harm after eating; Authoritative literature
    points out that eating is forbidden at a certain stage.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有证据表明食用后会造成伤害；权威文献指出在某个阶段禁止食用。
- en: B Sentence compression examples
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B 句子压缩示例
- en: 'Examples are shown in Table [6](#Ax1.T6 "Table 6 ‣ B Sentence compression examples
    ‣ Appendix ‣ TCRA-LLM: Token Compression Retrieval Augmented Large Language Model
    for Inference Cost Reduction").'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '示例见表[6](#Ax1.T6 "表 6 ‣ B 句子压缩示例 ‣ 附录 ‣ TCRA-LLM: 标记压缩检索增强大语言模型用于推理成本降低")。'
- en: '| Dataset | Original | Compressed |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 原始 | 压缩 |'
- en: '| FRDB | Babies are strictly prohibited from consuming donkey meat buns. The
    reason is that donkey meat buns usually contain a large amount of spices and seasonings,
    with high salt content, which will increase the metabolic burden on the baby’s
    kidneys. Furthermore, infants and young children are in a critical period for
    developing taste preferences, and frequently consuming donkey meat buns may affect
    the formation of their taste buds. It is not recommended to feed babies donkey
    meat buns. | In the first-instance procedure of a public prosecution case, if
    the participants or observers disrupt the courtroom order, the presiding judge
    shall handle the situation according to the law. Unauthorized recording, videotaping,
    photographing, or disseminating trial proceedings through mail, blog, or microblog
    may lead to the temporary seizure of storage media or relevant equipment. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| FRDB | 婴儿严格禁止食用驴肉包子。原因是驴肉包子通常含有大量香料和调料，盐分高，这会增加婴儿肾脏的代谢负担。此外，婴幼儿正处于口味偏好发展的关键时期，频繁食用驴肉包子可能会影响其味蕾的形成。不推荐喂婴儿驴肉包子。
    | In the first-instance procedure of a public prosecution case, if the participants
    or observers disrupt the courtroom order, the presiding judge shall handle the
    situation according to the law. Unauthorized recording, videotaping, photographing,
    or disseminating trial proceedings through mail, blog, or microblog may lead to
    the temporary seizure of storage media or relevant equipment. |'
- en: 'Table 6: Token compression examples. Both original and compressed sentences
    are translated into English from Chinese.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：标记压缩示例。原始句子和压缩句子均从中文翻译成英文。
- en: C QA examples
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C QA 示例
- en: 'Examples of QA are shown in Table [7](#Ax1.T7 "Table 7 ‣ C QA examples ‣ Appendix
    ‣ TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference
    Cost Reduction").'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: QA 示例见表 [7](#Ax1.T7 "表 7 ‣ C QA 示例 ‣ 附录 ‣ TCRA-LLM：标记压缩检索增强大语言模型用于推理成本减少")
- en: '| Questions | Questions (English translation) | Answer |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 问题（英文翻译） | 答案 |'
- en: '| --- | --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 对于’根据“产妇少吃鹅肝。原因是：产后妈妈适量食用鹅肝有利于促进伤口的愈合，弥补生产失血和重建肝脏铁储备，提高乳汁质量。还能提高妈妈的免疫力，具有抗氧化、缓解衰老的作用。建议产后妈妈可适量食用。”产妇食用/饮用鹅肝’，下列哪个选项最适合用于描述上述行为？选项：
    (1) 推荐， (2) 中立， (3) 避免 | According to "Maternity should eat less foie gras. The
    reason is: moderate consumption of foie gras after childbirth can promote wound
    healing, make up for blood loss during childbirth, rebuild liver iron reserves,
    improve breast milk quality. It can also improve the immunity of mothers, and
    has an antioxidant and anti-aging effect. It is recommended that postpartum mothers
    can consume it in moderation." Which of the following options is most suitable
    to describe the behavior of postpartum mothers consuming/eating foie gras? Options:
    (1) Recommend, (2) Neutral, (3) Avoid | Neutral |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 对于’根据“产妇少吃鹅肝。原因是：产后妈妈适量食用鹅肝有利于促进伤口的愈合，弥补生产失血和重建肝脏铁储备，提高乳汁质量。还能提高妈妈的免疫力，具有抗氧化、缓解衰老的作用。建议产后妈妈可适量食用。”产妇食用/饮用鹅肝’，下列哪个选项最适合用于描述上述行为？选项：
    (1) 推荐， (2) 中立， (3) 避免 | According to "Maternity should eat less foie gras. The
    reason is: moderate consumption of foie gras after childbirth can promote wound
    healing, make up for blood loss during childbirth, rebuild liver iron reserves,
    improve breast milk quality. It can also improve the immunity of mothers, and
    has an antioxidant and anti-aging effect. It is recommended that postpartum mothers
    can consume it in moderation." Which of the following options is most suitable
    to describe the behavior of postpartum mothers consuming/eating foie gras? Options:
    (1) Recommend, (2) Neutral, (3) Avoid | 中立 |'
- en: '| 对于’根据“备孕女性可以吃咖喱。原因是：咖喱属于混合调制的香料，能调节肠胃蠕动，提高食欲，其辛辣程度根据配料而变，过于辛辣的咖喱对胃有一定的刺激性，备孕期女性可以根据自己的口味喜好选择合适辣度的咖喱。”备孕女性食用/饮用咖喱’，下列哪个选项最适合用于描述上述行为？选项：
    (1) 推荐， (2) 中立， (3) 避免 | Regarding "Females preparing for pregnancy can eat curry.
    The reason is that curry is a mixed seasoning that can regulate intestinal movement,
    increase appetite, and its spiciness varies according to the ingredients. Curry
    that is too spicy can stimulate the stomach to some extent. Females preparing
    for pregnancy can choose curry with appropriate spiciness based on their own taste
    preferences." Which of the following options is most suitable to describe the
    behavior of females preparing for pregnancy consuming/eating curry? Options: (1)
    Recommend, (2) Neutral, (3) Avoid | Recommend |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 对于’根据“备孕女性可以吃咖喱。原因是：咖喱属于混合调制的香料，能调节肠胃蠕动，提高食欲，其辛辣程度根据配料而变，过于辛辣的咖喱对胃有一定的刺激性，备孕期女性可以根据自己的口味喜好选择合适辣度的咖喱。”备孕女性食用/饮用咖喱’，下列哪个选项最适合用于描述上述行为？选项：
    (1) 推荐， (2) 中立， (3) 避免 | Regarding "Females preparing for pregnancy can eat curry.
    The reason is that curry is a mixed seasoning that can regulate intestinal movement,
    increase appetite, and its spiciness varies according to the ingredients. Curry
    that is too spicy can stimulate the stomach to some extent. Females preparing
    for pregnancy can choose curry with appropriate spiciness based on their own taste
    preferences." Which of the following options is most suitable to describe the
    behavior of females preparing for pregnancy consuming/eating curry? Options: (1)
    Recommend, (2) Neutral, (3) Avoid | 推荐 |'
- en: '| 对于’根据“6月大的宝宝少吃玉米汁。原因是：玉米汁富含维生素、矿物质和碳水化合物，可为宝宝提供能量，促进其生长发育，且吸收率较高。6月龄以后的宝宝可少量食用，注意鲜榨玉米汁不要添加糖，以防摄入过多的糖，不利于宝宝的口腔健康。”6月龄的宝宝食用/饮用玉米汁’，下列哪个选项最适合用于描述上述行为？选项：
    (1) 推荐， (2) 中立， (3) 避免 | Regarding "Babies at the age of 6 months should consume
    less corn juice. The reason is that corn juice is rich in vitamins, minerals and
    carbohydrates, which can provide energy for babies, promote their growth and development,
    and has a high absorption rate. Babies over 6 months old can consume it in moderation,
    but be mindful that freshly squeezed corn juice should not contain added sugar
    to prevent excessive intake of sugar, which could be detrimental to baby’s oral
    health." Which of the following options is most suitable to describe the behavior
    of a 6-month-old baby consuming/drinking corn juice? Options: (1) Recommend, (2)
    Neutral, (3) Avoid | Neutral |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Regarding "6月大的宝宝应该少喝玉米汁。原因是玉米汁富含维生素、矿物质和碳水化合物，这些成分能够为宝宝提供能量，促进其生长发育，并且吸收率较高。6个月大的宝宝可以适量食用，但要注意鲜榨玉米汁不要添加糖，以防摄入过多的糖，影响宝宝的口腔健康。"
    下列哪个选项最适合描述6个月大宝宝饮用玉米汁的行为？选项： (1) 推荐， (2) 中立， (3) 避免 | 关于“6月大的宝宝应该少喝玉米汁。原因是玉米汁富含维生素、矿物质和碳水化合物，这些成分能够为宝宝提供能量，促进其生长发育，并且吸收率较高。6个月大的宝宝可以适量食用，但要注意鲜榨玉米汁不要添加糖，以防摄入过多的糖，影响宝宝的口腔健康。”下列哪个选项最适合描述6个月大宝宝饮用玉米汁的行为？选项：(1)
    推荐，(2) 中立，(3) 避免 | 中立 |'
- en: 'Table 7: QA examples. For all examples, the answers are chosen from $3$) Avoid.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：QA 示例。所有示例的答案都选自 $3$) 避免。
- en: D Knowledge examples
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D 知识示例
- en: 'Examples of knowledge utilized for answering the questions are shown in Table [8](#Ax1.T8
    "Table 8 ‣ D Knowledge examples ‣ Appendix ‣ TCRA-LLM: Token Compression Retrieval
    Augmented Large Language Model for Inference Cost Reduction").'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 知识示例用于回答问题的情况见表[8](#Ax1.T8 "表 8 ‣ 知识示例 ‣ 附录 ‣ TCRA-LLM：用于推理成本降低的令牌压缩增强大语言模型")。
- en: '| Knowledge | Knowledge (English translation) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 知识 | 知识（英文翻译） |'
- en: '| --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 备孕女性可以吃土豆泥。原因是：备孕人群可以食用，不过土豆泥升糖较快，建议一次不要吃太多。 | Females preparing for pregnancy
    can eat mashed potatoes. The reason is that it is safe for this group to consume
    mashed potatoes. However, mashed potatoes have a high glycemic index, so it is
    recommended not to eat too much at once. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 备孕女性可以吃土豆泥。原因是：备孕人群可以食用，不过土豆泥升糖较快，建议一次不要吃太多。 | 备孕女性可以吃土豆泥。原因是：备孕人群可以食用，但由于土豆泥的升糖速度较快，建议一次不要食用过多。
    |'
- en: '| 宝宝不能吃生鱼片。原因是：鱼肉中含有大量的不饱和脂肪酸（尤其是DHA），有助于促进宝宝大脑及视力发育。但生鱼片如果处理不当，容易感染病菌和寄生虫。不建议给宝宝吃生鱼片。
    | Babies should not eat raw fish slices. The reason is that fish contains a large
    amount of unsaturated fatty acids (especially DHA), which can help promote the
    development of the baby’s brain and vision. However, if raw fish slices are not
    properly processed, they can easily become contaminated with bacteria and parasites,
    posing health risks to babies. Therefore, it is not recommended to feed raw fish
    slices to babies. |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 宝宝不能吃生鱼片。原因是：鱼肉中含有大量的不饱和脂肪酸（尤其是DHA），有助于促进宝宝大脑及视力发育。但生鱼片如果处理不当，容易感染病菌和寄生虫。不建议给宝宝吃生鱼片。
    | 宝宝不能吃生鱼片。原因是：鱼肉中含有大量的不饱和脂肪酸（尤其是DHA），有助于促进宝宝大脑及视力发育。但生鱼片如果处理不当，容易感染病菌和寄生虫，因此不推荐给宝宝吃生鱼片。
    |'
- en: '| 孕妇可以吃罗非鱼。原因是：罗非鱼中含有非常丰富的不饱和脂肪酸、蛋白质和多种氨基酸，容易被人体消化吸收。能为孕妈提供多种营养物质，帮助胎儿骨骼生长和神经系统发育。
    | Pregnant women can eat tilapia. The reason is that tilapia is rich in unsaturated
    fatty acids, protein and various amino acids, which are easily digested and absorbed
    by the human body. It can provide pregnant women with various nutrients to help
    promote fetal bone growth and development of the nervous system. |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 孕妇可以吃罗非鱼。原因是：罗非鱼中含有非常丰富的不饱和脂肪酸、蛋白质和多种氨基酸，容易被人体消化吸收。能为孕妈提供多种营养物质，帮助胎儿骨骼生长和神经系统发育。
    | 孕妇可以吃罗非鱼。原因是：罗非鱼中富含不饱和脂肪酸、蛋白质和多种氨基酸，易于被人体消化吸收。它可以为孕妇提供多种营养物质，帮助胎儿骨骼生长和神经系统发育。
    |'
- en: 'Table 8: Examples of knowledge that are utilized for answering the questions.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：用于回答问题的知识示例。
