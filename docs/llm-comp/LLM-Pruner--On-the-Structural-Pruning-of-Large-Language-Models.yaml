- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:05:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:05:21
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LLM-Pruner: On the Structural Pruning of Large Language Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM-Pruner：大型语言模型的结构剪枝
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.11627](https://ar5iv.labs.arxiv.org/html/2305.11627)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.11627](https://ar5iv.labs.arxiv.org/html/2305.11627)
- en: Xinyin Ma  Gongfan Fang Xinchao Wang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xinyin Ma  Gongfan Fang Xinchao Wang
- en: National University of Singapore maxinyin@u.nus.edu, gongfan@u.nus.edu, xinchao@nus.edu.sg
    Corresponding author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 新加坡国立大学 maxinyin@u.nus.edu，gongfan@u.nus.edu，xinchao@nus.edu.sg 通讯作者
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models (LLMs) have shown remarkable capabilities in language
    understanding and generation. However, such impressive capability typically comes
    with a substantial model size, which presents significant challenges in both the
    deployment, inference, and training stages. With LLM being a general-purpose task
    solver, we explore its compression in a task-agnostic manner, which aims to preserve
    the multi-task solving and language generation ability of the original LLM. One
    challenge to achieving this is the enormous size of the training corpus of LLM,
    which makes both data transfer and model post-training over-burdensome. Thus,
    we tackle the compression of LLMs within the bound of two constraints: being task-agnostic
    and minimizing the reliance on the original training dataset. Our method, named
    LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled
    structures based on gradient information, maximally preserving the majority of
    the LLM’s functionality. To this end, the performance of pruned models can be
    efficiently recovered through tuning techniques, LoRA, in merely *3 hours*, requiring
    only *50K* data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna,
    and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory
    capabilities in zero-shot classification and generation. The code is available
    at: [https://github.com/horseee/LLM-Pruner](https://github.com/horseee/LLM-Pruner)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在语言理解和生成方面表现出显著的能力。然而，这种令人印象深刻的能力通常伴随着模型尺寸的巨大，这在部署、推理和训练阶段都带来了重大挑战。由于LLM是一个通用任务求解器，我们以任务无关的方式探索其压缩方法，旨在保留原始LLM的多任务求解和语言生成能力。实现这一目标的一个挑战是LLM训练语料库的庞大规模，这使得数据传输和模型后训练都变得非常繁重。因此，我们在两个约束条件下进行LLM的压缩：一是任务无关，二是最小化对原始训练数据集的依赖。我们的方法名为LLM-Pruner，采用结构剪枝方法，基于梯度信息有选择性地去除非关键的耦合结构，最大限度地保留LLM的大部分功能。为此，剪枝模型的性能可以通过调整技术LoRA在仅仅*3小时*内高效恢复，只需*50K*数据。我们在三种LLM上验证了LLM-Pruner，包括LLaMA、Vicuna和ChatGLM，并证明压缩后的模型在零-shot分类和生成任务中仍然表现出令人满意的能力。代码可在：[https://github.com/horseee/LLM-Pruner](https://github.com/horseee/LLM-Pruner)
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recently, Large Language Models (LLMs) OpenAI ([2023](#bib.bib37)); Touvron
    et al. ([2023](#bib.bib49)); Thoppilan et al. ([2022](#bib.bib48)); Scao et al.
    ([2022](#bib.bib42)); Xue et al. ([2020](#bib.bib62)); Chiang et al. ([2023](#bib.bib4));
    Zeng et al. ([2022](#bib.bib69)) have demonstrated remarkable proficiency in language
    understanding and generation. With the increase in model size, they are better
    equipped to handle complex tasks  Brown et al. ([2020](#bib.bib3)); Chowdhery
    et al. ([2022](#bib.bib5)); Wei et al. ([2022b](#bib.bib56)); Wu et al. ([2020](#bib.bib58))
    and even exhibit emergent abilities Wei et al. ([2022a](#bib.bib55)). However,
    notwithstanding their impressive performance, LLMs pose challenges in deployment
    and inference. Their extensive scale engenders substantial computational demands,
    and the multitude of parameters involved can induce long latencies and other related
    issues. Several techniques are proposed to solve these problems, like model pruning Wang
    et al. ([2019b](#bib.bib54)); Xia et al. ([2022](#bib.bib59)); Zafrir et al. ([2021](#bib.bib67));
    Kurtic et al. ([2022](#bib.bib21)), knowledge distillation Sun et al. ([2019](#bib.bib44));
    Pan et al. ([2020b](#bib.bib39)); Sun et al. ([2020a](#bib.bib45)),quantization Bai
    et al. ([2020](#bib.bib1)); Frantar et al. ([2022](#bib.bib13)) within the context
    of pre-trained language model (PLM).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs） OpenAI ([2023](#bib.bib37)); Touvron et al. ([2023](#bib.bib49));
    Thoppilan et al. ([2022](#bib.bib48)); Scao et al. ([2022](#bib.bib42)); Xue et
    al. ([2020](#bib.bib62)); Chiang et al. ([2023](#bib.bib4)); Zeng et al. ([2022](#bib.bib69))
    在语言理解和生成方面表现出卓越的能力。随着模型规模的增加，它们能够更好地处理复杂任务 Brown et al. ([2020](#bib.bib3)); Chowdhery
    et al. ([2022](#bib.bib5)); Wei et al. ([2022b](#bib.bib56)); Wu et al. ([2020](#bib.bib58))，甚至展现出新兴能力
    Wei et al. ([2022a](#bib.bib55))。然而，尽管它们的表现令人印象深刻，LLMs 在部署和推理中仍面临挑战。它们庞大的规模带来了巨大的计算需求，多参数的参与可能引发长时间延迟和其他相关问题。为解决这些问题，提出了几种技术，如模型剪枝
    Wang et al. ([2019b](#bib.bib54)); Xia et al. ([2022](#bib.bib59)); Zafrir et
    al. ([2021](#bib.bib67)); Kurtic et al. ([2022](#bib.bib21))，知识蒸馏 Sun et al. ([2019](#bib.bib44));
    Pan et al. ([2020b](#bib.bib39)); Sun et al. ([2020a](#bib.bib45))，以及量化 Bai et
    al. ([2020](#bib.bib1)); Frantar et al. ([2022](#bib.bib13))，这些技术在预训练语言模型（PLM）的背景下得到应用。
- en: While previous methods have effectively maintained model performance amidst
    parameter reduction, they primarily target compression within specialized domains
    or for designated tasks in the context of task-specific compression. For instance,
    a PLM is fine-tuned on a particular dataset, such as one of the classification
    tasks in the GLUE benchmark Wang et al. ([2018](#bib.bib51)), after which these
    models are distilled into a smaller classification model Sun et al. ([2019](#bib.bib44));
    Hou et al. ([2020](#bib.bib18)). Although this paradigm could potentially be employed
    for LLM compression, it compromises the LLM’s capacity as a versatile task solver,
    rendering it suited to a single task exclusively.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管之前的方法在参数减少的同时有效保持了模型性能，但它们主要针对的是在特定领域或指定任务中的压缩。例如，一个预训练语言模型（PLM）在特定数据集上进行微调，如GLUE基准测试中的分类任务之一
    Wang et al. ([2018](#bib.bib51))，然后这些模型被蒸馏成一个更小的分类模型 Sun et al. ([2019](#bib.bib44));
    Hou et al. ([2020](#bib.bib18))。虽然这种范式可能会被用于LLM压缩，但它会妨碍LLM作为多功能任务解决者的能力，使其仅适用于单一任务。
- en: 'Thus, we strive to compress the LLM in a new setting: to reduce the LLM’s size
    while preserving its diverse capabilities as general-purpose task solvers, as
    depicted in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM-Pruner: On the
    Structural Pruning of Large Language Models"). This introduces the task-agnostic
    compression of LLMs, which presents two key challenges:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们力求在新的环境下压缩大语言模型（LLM）：在减少LLM的规模的同时，保留其作为通用任务解决者的多样化能力，如图 [1](#S1.F1 "图 1
    ‣ 1 介绍 ‣ LLM-Pruner：大语言模型的结构化剪枝") 所示。这引入了LLM的任务无关压缩，面临两个关键挑战：
- en: •
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The size of the training corpus of the LLM is enormous. Previous compression
    methods heavily depend on the training corpus. The LLM has escalated the corpus
    scale to 1 trillion tokens or more  Hoffmann et al. ([2022](#bib.bib17)); Touvron
    et al. ([2023](#bib.bib49)). The extensive storage needs and protracted transmission
    times make the dataset difficult to acquire. Furthermore, if the dataset is proprietary,
    acquisition of the training corpus verges on impossibility, a situation encountered
    in  Zeng et al. ([2022](#bib.bib69)); OpenAI ([2023](#bib.bib37)).
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM的训练语料库规模巨大。以往的压缩方法高度依赖训练语料库。LLM将语料库规模扩大到1万亿个标记或更多  Hoffmann et al. ([2022](#bib.bib17));
    Touvron et al. ([2023](#bib.bib49))。巨大的存储需求和漫长的传输时间使得数据集难以获取。此外，如果数据集是专有的，则获取训练语料库几乎是不可能的，这种情况在
     Zeng et al. ([2022](#bib.bib69)); OpenAI ([2023](#bib.bib37)) 中出现过。
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The unacceptably long duration for the post-training of the pruned LLM. Existing
    methods require a substantial amount of time for post-training the smaller model Wang
    et al. ([2020](#bib.bib53)); Liang et al. ([2023](#bib.bib28)). For instance,
    the general distillation in TinyBERT takes around 14 GPU days  Jiao et al. ([2020](#bib.bib20)).
    Even post-training a task-specific compressed model of BERT demands around 33
    hours Xia et al. ([2022](#bib.bib59)); Kwon et al. ([2022](#bib.bib22)). As the
    size of both the model and corpus for LLMs increases rapidly, this step will invariably
    consume an even more extensive time.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 剪枝后的LLM训练时间过长。现有方法需要大量时间来对较小的模型进行后训练 Wang et al. ([2020](#bib.bib53)); Liang
    et al. ([2023](#bib.bib28))。例如，TinyBERT中的通用蒸馏大约需要14个GPU天  Jiao et al. ([2020](#bib.bib20))。即便对任务特定的压缩BERT模型进行后训练也需要大约33小时 Xia
    et al. ([2022](#bib.bib59)); Kwon et al. ([2022](#bib.bib22))。随着LLM模型和语料库的规模迅速增长，这一步骤将不可避免地消耗更多的时间。
- en: '![Refer to caption](img/b61cb7012c5caa6bd6aae4f8984a0d41.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/b61cb7012c5caa6bd6aae4f8984a0d41.png)'
- en: 'Figure 1: Illustration of LLM-Pruner. (i) Task-specific compression: the model
    is fine-tuned then compressed on a specific task. (ii) TinyBERT: First distill
    the model on unlabeled corpus and then fine-tune it on the specific task. (iii)
    LLM-Pruner: Task-agnostic compression within 3 hours.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLM-Pruner的示意图。 (i) 任务特定压缩：模型在特定任务上进行微调后进行压缩。 (ii) TinyBERT：首先在未标记的语料库上进行模型蒸馏，然后在特定任务上进行微调。
    (iii) LLM-Pruner：在3小时内进行任务无关的压缩。
- en: To tackle the aforementioned challenges associated with the task-agnostic compression
    of LLMs, we introduce a novel approach called LLM-Pruner. Since our goal is to
    compress LLMs with reduced data dependency and expedited post-training, how to
    prune model with the minimal disruption to the origin is crucial. To accomplish
    this, we propose a dependency detection algorithm that identifies all the dependent
    structures within the model. Once the coupled structure is identified, we employ
    an efficient importance estimation strategy to select the optimal group for pruning
    under the task-agnostic setting, where the first-order information and an approximated
    hessian information is taken into account. Finally, a rapid recovery stage is
    executed to post-train the pruned model with limited data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对与LLM任务无关压缩相关的挑战，我们提出了一种新方法，称为LLM-Pruner。由于我们的目标是压缩LLM以减少数据依赖和加快后训练，如何以最小的干扰修剪模型至关重要。为此，我们提出了一种依赖检测算法，识别模型中的所有依赖结构。一旦识别出耦合结构，我们采用高效的重要性估计策略，在任务无关的设置下选择最优的修剪组，其中考虑了一级信息和近似海森信息。最后，执行快速恢复阶段，以有限的数据对修剪后的模型进行后训练。
- en: Contribution.
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贡献。
- en: 'In this paper, we propose a novel framework, LLM-Pruner, for the task-agnostic
    compression of the large language model. To the best of our knowledge, LLM-Pruner
    is the first framework designed for structured pruning of LLMs. We conclude the
    advantages of the LLM-Pruner as (i) Task-agnostic compression, where the compressed
    language model retains its ability to serve as a multi-task solver. (ii) Reduced
    demand for the original training corpus, where only 50k publicly available samples
    are needed for compression, significantly reducing the budget for acquiring the
    training data (iii) Quick compression, where the compression process ends up in
    three hours. (iv) An automatic structural pruning framework, where all the dependent
    structures are grouped without the need for any manual design. To evaluate the
    effectiveness of LLM-Pruner, we conduct extensive experiments on three large language
    models: LLaMA-7B, Vicuna-7B, and ChatGLM-6B. The compressed models are evaluated
    using nine datasets to assess both the generation quality and the zero-shot classification
    performance of the pruned models. The experimental results demonstrate that even
    with the removal of 20% of the parameters, the pruned model maintains 94.97% of
    the performance of the original model.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新颖的框架 LLM-Pruner，用于大型语言模型的任务无关压缩。根据我们所知，LLM-Pruner 是第一个专为 LLM 的结构化剪枝设计的框架。我们总结了
    LLM-Pruner 的优势如下：（i）任务无关的压缩，其中压缩后的语言模型保持其作为多任务解算器的能力。（ii）减少对原始训练语料库的需求，只需 50k
    个公开样本即可进行压缩，从而显著降低获取训练数据的预算。（iii）快速压缩，压缩过程在三小时内完成。（iv）自动结构化剪枝框架，其中所有相关结构被分组，无需任何手动设计。为了评估
    LLM-Pruner 的有效性，我们在三种大型语言模型上进行了广泛的实验：LLaMA-7B、Vicuna-7B 和 ChatGLM-6B。压缩模型使用九个数据集进行评估，以评估剪枝模型的生成质量和零样本分类性能。实验结果表明，即使去除了
    20% 的参数，剪枝模型仍保持原始模型 94.97% 的性能。
- en: 2 Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Compression of Language Model.
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语言模型的压缩。
- en: 'Language models  Devlin et al. ([2018](#bib.bib9)); Liu et al. ([2019](#bib.bib29));
    Lewis et al. ([2019](#bib.bib25)) have gained much attention and increase the
    need to reduce the size of parameters and reduce the latency  Lan et al. ([2019](#bib.bib23));
    Sun et al. ([2020b](#bib.bib46)). To compress the language model, previous works
    can be divided into several categories: network pruning  Kurtic et al. ([2022](#bib.bib21));
    Xu et al. ([2021](#bib.bib61)); Liu et al. ([2021](#bib.bib30)); Guo et al. ([2019](#bib.bib15)),
    knowledge distillation  Sun et al. ([2019](#bib.bib44), [2020a](#bib.bib45));
    Pan et al. ([2020a](#bib.bib38)), quantization Yao et al. ([2022](#bib.bib63));
    Bai et al. ([2020](#bib.bib1)); Zafrir et al. ([2019](#bib.bib66)) and other techniques,
    like early exit  Xin et al. ([2020](#bib.bib60)) or dynamic token reduction  Ye
    et al. ([2021](#bib.bib64)). We focus on the pruning of the language models, especially
    structural pruning  Li et al. ([2016](#bib.bib26)). Structural pruning removes
    the entire filter from the neural network, which is more hardware friendly. There
    are several ways to remove the structure, such as l1-dependent pruning  Han et al.
    ([2015](#bib.bib16)); Zafrir et al. ([2021](#bib.bib67)), first-order importance
    estimation  Hou et al. ([2020](#bib.bib18)), hessian-based estimation Kurtic et al.
    ([2022](#bib.bib21)); Wang et al. ([2019a](#bib.bib52)) or the optimal brain surgeon LeCun
    et al. ([1989](#bib.bib24)); Kurtic et al. ([2022](#bib.bib21)). As for the pruning
    unit in structural pruning, some works adopt the entire layer  Fan et al. ([2019](#bib.bib10))
    as the minimal unit, and others take the multi-head attention  Voita et al. ([2019](#bib.bib50))
    or the feed-forward layers  Hou et al. ([2020](#bib.bib18)); McCarley et al. ([2019](#bib.bib34))
    as the basic structure to prune. CoFi  Xia et al. ([2022](#bib.bib59)) studies
    the pruning unit in different granularity.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型 Devlin 等人 ([2018](#bib.bib9)); Liu 等人 ([2019](#bib.bib29)); Lewis 等人 ([2019](#bib.bib25))
    已经获得了广泛关注，并增加了减少参数规模和降低延迟的需求 Lan 等人 ([2019](#bib.bib23)); Sun 等人 ([2020b](#bib.bib46))。为了压缩语言模型，先前的工作可以分为几类：网络剪枝
    Kurtic 等人 ([2022](#bib.bib21)); Xu 等人 ([2021](#bib.bib61)); Liu 等人 ([2021](#bib.bib30));
    Guo 等人 ([2019](#bib.bib15))，知识蒸馏 Sun 等人 ([2019](#bib.bib44), [2020a](#bib.bib45));
    Pan 等人 ([2020a](#bib.bib38))，量化 Yao 等人 ([2022](#bib.bib63)); Bai 等人 ([2020](#bib.bib1));
    Zafrir 等人 ([2019](#bib.bib66)) 和其他技术，如早期退出 Xin 等人 ([2020](#bib.bib60)) 或动态令牌减少
    Ye 等人 ([2021](#bib.bib64))。我们专注于语言模型的剪枝，特别是结构剪枝 Li 等人 ([2016](#bib.bib26))。结构剪枝从神经网络中移除整个过滤器，这种方法更适合硬件。有几种方法可以去除结构，如
    l1 依赖剪枝 Han 等人 ([2015](#bib.bib16)); Zafrir 等人 ([2021](#bib.bib67))，一阶重要性估计 Hou
    等人 ([2020](#bib.bib18))，基于 Hessian 的估计 Kurtic 等人 ([2022](#bib.bib21)); Wang 等人
    ([2019a](#bib.bib52)) 或最佳脑外科医生 LeCun 等人 ([1989](#bib.bib24)); Kurtic 等人 ([2022](#bib.bib21))。至于结构剪枝中的剪枝单元，一些工作采用整个层
    Fan 等人 ([2019](#bib.bib10)) 作为最小单元，而其他工作则将多头注意力 Voita 等人 ([2019](#bib.bib50))
    或前馈层 Hou 等人 ([2020](#bib.bib18)); McCarley 等人 ([2019](#bib.bib34)) 作为剪枝的基本结构。CoFi
    Xia 等人 ([2022](#bib.bib59)) 研究了不同粒度的剪枝单元。
- en: Efficient and Low Resource Compression.
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高效与低资源压缩。
- en: With the growing size of models, there is an increasing demand for efficient
    LLM compression and compression is independent of the original training data.
    As for the efficient compression,  Kwon et al. ([2022](#bib.bib22)) accelerate
    the post-training by defining the reconstruction error as a linear least squares
    problem.  Frantar et al. ([2022](#bib.bib13)); Frantar and Alistarh ([2023](#bib.bib12))
    propose the layer-wise optimal brain surgeon. As for the constraint of availability
    of the training corpus, data-free pruning  Srinivas and Babu ([2015](#bib.bib43));
    Yvinec et al. ([2022](#bib.bib65)) come up with several strategies to prune the
    model by measuring neurons’ similarity. Besides,  Ma et al. ([2022](#bib.bib32),
    [2020](#bib.bib31)); Rashid et al. ([2020](#bib.bib40)) proposes methods that
    distill the model without reliance on the training corpus of the model. However,
    those methods are too time-consuming, involving synthesizing samples by backpropagating
    the pre-trained language models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型规模的不断增长，对高效 LLM 压缩的需求也在增加，且压缩与原始训练数据无关。至于高效压缩，Kwon 等人 ([2022](#bib.bib22))
    通过将重构误差定义为线性最小二乘问题来加速后训练。Frantar 等人 ([2022](#bib.bib13)); Frantar 和 Alistarh ([2023](#bib.bib12))
    提出了逐层最佳脑外科医生。至于训练语料的可用性约束，数据无关剪枝 Srinivas 和 Babu ([2015](#bib.bib43)); Yvinec
    等人 ([2022](#bib.bib65)) 提出了通过测量神经元相似性来剪枝模型的几种策略。此外，Ma 等人 ([2022](#bib.bib32),
    [2020](#bib.bib31)); Rashid 等人 ([2020](#bib.bib40)) 提出了在不依赖模型训练语料的情况下蒸馏模型的方法。然而，这些方法太过耗时，涉及通过反向传播预训练语言模型来合成样本。
- en: 3 Methods
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 种方法
- en: 'In this section, we provide a detailed explanation of LLM-Pruner. Following
    the conventional model compression pipelineKwon et al. ([2022](#bib.bib22)), LLM-Pruner
    consists of three steps: (1) Discovery Stage (Section [3.1](#S3.SS1 "3.1 Discover
    All Coupled Structure in LLMs ‣ 3 Methods ‣ LLM-Pruner: On the Structural Pruning
    of Large Language Models")). This step focuses on identifying groups of interdependent
    structures within LLMs. (2) Estimation Stage (Section [3.2](#S3.SS2 "3.2 Grouped
    Importance Estimation of Coupled Structure ‣ 3 Methods ‣ LLM-Pruner: On the Structural
    Pruning of Large Language Models")). Once the coupled structures are grouped,
    the second step entails estimating the contribution of each group to the overall
    performance of the model and deciding which group to be pruned. (3) Recover Stage
    (Section [3.3](#S3.SS3 "3.3 Fast Recovery with Low-rank Approximation ‣ 3 Methods
    ‣ LLM-Pruner: On the Structural Pruning of Large Language Models")). This step
    involves fast post-training that alleviates potential performance degradation
    caused by the removal of structures.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了对 LLM-Pruner 的详细解释。遵循传统的模型压缩流程 Kwon 等人 ([2022](#bib.bib22))，LLM-Pruner
    包含三个步骤：（1）发现阶段（第 [3.1](#S3.SS1 "3.1 发现 LLMs 中所有耦合结构 ‣ 3 种方法 ‣ LLM-Pruner：大语言模型的结构化修剪")
    节）。此步骤侧重于识别 LLMs 中的相互依赖结构组。（2）估计阶段（第 [3.2](#S3.SS2 "3.2 耦合结构的分组重要性估计 ‣ 3 种方法 ‣
    LLM-Pruner：大语言模型的结构化修剪") 节）。一旦耦合结构被分组，第二步涉及估计每个组对模型整体性能的贡献，并决定哪些组应被修剪。（3）恢复阶段（第
    [3.3](#S3.SS3 "3.3 低秩近似的快速恢复 ‣ 3 种方法 ‣ LLM-Pruner：大语言模型的结构化修剪") 节）。此步骤涉及快速后训练，以缓解由于去除结构而导致的潜在性能下降。
- en: 3.1 Discover All Coupled Structure in LLMs
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 发现 LLMs 中所有耦合结构
- en: 'In light of the limited availability of data for post-training, it becomes
    imperative to prioritize the removal of structures with minimal damage when compressing
    the model. This underscores the dependency-based structural pruning, which ensures
    coupled structures are pruned in unison. We provide an experiment in Section [4.3](#S4.SS3.SSS0.Px3
    "Impact of Dependency-based Structured Pruning. ‣ 4.3 More Analysis ‣ 4 Experiments
    ‣ LLM-Pruner: On the Structural Pruning of Large Language Models") to show the
    importance of dependency-based structural pruning when compressing the large language
    model.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于后训练数据的有限性，压缩模型时优先去除损害最小的结构变得至关重要。这突出了基于依赖的结构化修剪，确保耦合结构一起被修剪。我们在第 [4.3](#S4.SS3.SSS0.Px3
    "基于依赖的结构化修剪的影响 ‣ 4.3 更多分析 ‣ 4 实验 ‣ LLM-Pruner：大语言模型的结构化修剪") 节提供了一个实验，以展示在压缩大语言模型时基于依赖的结构化修剪的重要性。
- en: Structure Dependency in LLMs.
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结构依赖性在 LLMs 中。
- en: 'Similar to Fang et al. ([2023](#bib.bib11)), the pruning begins by building
    the dependency for LLMs. Assume $N_{i}$. The dependency between structures can
    be defined as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Fang 等人 ([2023](#bib.bib11))，修剪从为 LLMs 构建依赖关系开始。假设 $N_{i}$。结构之间的依赖性可以定义为：
- en: '|  | $N_{j}\in\operatorname{Out}(N_{i})\wedge\operatorname{Deg}^{-}(N_{j})=1\Rightarrow
    N_{j}\text{ is dependent on }N_{i}$ |  | (1) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $N_{j}\in\operatorname{Out}(N_{i})\wedge\operatorname{Deg}^{-}(N_{j})=1\Rightarrow
    N_{j}\text{ 依赖于 }N_{i}$ |  | (1) |'
- en: 'where $\operatorname{Deg}^{-}(N_{j})$. Noting that this dependency is directional,
    we can therefore correspondingly obtain another dependency:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\operatorname{Deg}^{-}(N_{j})$。注意到这种依赖性是有方向的，因此我们可以相应地获得另一种依赖性：
- en: '|  | $N_{i}\in\operatorname{In}(N_{j})\wedge\operatorname{Deg}^{+}(N_{i})=1\Rightarrow
    N_{i}\text{ is dependent on }N_{j}$ |  | (2) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $N_{i}\in\operatorname{In}(N_{j})\wedge\operatorname{Deg}^{+}(N_{i})=1\Rightarrow
    N_{i}\text{ 依赖于 }N_{j}$ |  | (2) |'
- en: where $\operatorname{Deg}^{+}(N_{i})$ must also undergo pruning.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\operatorname{Deg}^{+}(N_{i})$ 也必须进行修剪。
- en: Trigger the Dependency Graph.
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 触发依赖图。
- en: 'By having the definition of dependency, the coupled structures in the LLM can
    be analyzed automatically. Considering any neuron within the LLM as the initial
    trigger, it possesses the capability to activate neurons that depend on it. Subsequently,
    these newly triggered neurons can serve as the subsequent triggers to identify
    the dependency and activate their respective dependent neurons. This iterative
    process continues until no new neurons are detected. Those neurons then form a
    group for further pruning. Taking LLaMA as an example, by searching over all the
    neurons as the initial trigger, we can locate all the coupled structures, as shown
    in Figure[2](#S3.F2 "Figure 2 ‣ Trigger the Dependency Graph. ‣ 3.1 Discover All
    Coupled Structure in LLMs ‣ 3 Methods ‣ LLM-Pruner: On the Structural Pruning
    of Large Language Models").'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '通过定义依赖关系，可以自动分析 LLM 中的耦合结构。考虑 LLM 中的任意神经元作为初始触发器，它具有激活依赖于它的神经元的能力。随后，这些新触发的神经元可以作为后续触发器来识别依赖关系并激活它们各自的依赖神经元。这个迭代过程将持续进行，直到不再检测到新神经元。这些神经元将组成一个组，以便进一步修剪。以
    LLaMA 为例，通过将所有神经元作为初始触发器进行搜索，我们可以找到所有的耦合结构，如图[2](#S3.F2 "Figure 2 ‣ Trigger the
    Dependency Graph. ‣ 3.1 Discover All Coupled Structure in LLMs ‣ 3 Methods ‣ LLM-Pruner:
    On the Structural Pruning of Large Language Models")所示。'
- en: Given the diversity in the structure of different LLMs, manual analysis and
    removal of coupled structures in each LLM could be extremely time-consuming. However,
    by employing LLM-Pruner, all coupled structures can be automatically identified
    and extracted.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于不同 LLM 的结构多样性，对每个 LLM 中的耦合结构进行手动分析和移除可能非常耗时。然而，通过使用 LLM-Pruner，可以自动识别和提取所有耦合结构。
- en: '![Refer to caption](img/a13fe1d0232100fdc5757766a519b817.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a13fe1d0232100fdc5757766a519b817.png)'
- en: 'Figure 2: Illustration of the coupled structures in LLaMA. We simplify the
    neurons in each layer to make the dependent group clear. The trigger neuron, marked
    as a circle with a bell, cause weights with dependency pruned (dashed lines),
    which may propagate (red dashed lines) to coupled neurons (dashed circles). A
    group can be triggered by a variety of trigger neurons. Taking Group Type B as
    an example, the trigger for this group involves (i) the attention head, (ii) the
    output neuron in Query, Key or Value, and (iii) the input neuron in the final
    output projection.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLaMA 中耦合结构的示例。我们简化了每层的神经元，以便清晰地展示依赖组。触发神经元标记为带铃铛的圆形，导致具有依赖关系的权重被修剪（虚线），这些修剪可能会传播（红色虚线）到耦合神经元（虚线圆圈）。一个组可以由多种触发神经元触发。以组类型
    B 为例，这组的触发涉及 (i) 注意力头，(ii) Query、Key 或 Value 中的输出神经元，以及 (iii) 最终输出投影中的输入神经元。
- en: 3.2 Grouped Importance Estimation of Coupled Structure
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 耦合结构的分组重要性估计
- en: Till now, all coupled structures within the model are grouped. Weights within
    the same group should be pruned simultaneously, as partial pruning not only increases
    parameter size but also introduces misaligned intermediate representations. Therefore,
    we estimate the importance of the group as a whole, as opposed to evaluating the
    importance of modules. Given the limited access to the training dataset, we explore
    the use of public datasets or manually created samples as alternative resources.
    Although the domains of these datasets may not perfectly align with the training
    set, they still provide valuable information for assessing the importance.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，模型中的所有耦合结构已被分组。组内的权重应同时进行修剪，因为部分修剪不仅会增加参数大小，还会引入不对齐的中间表示。因此，我们评估整个组的重要性，而不是评估模块的重要性。鉴于对训练数据集的访问有限，我们探索使用公共数据集或手动创建的样本作为替代资源。尽管这些数据集的领域可能与训练集不完全一致，但它们仍提供了评估重要性的有价值信息。
- en: Vector-wise Importance.
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量级重要性。
- en: 'Suppose that given a dataset $\mathcal{D}=\{x_{i},y_{i}\}_{i=1}^{N}$, the change
    in loss can be formulated as LeCun et al. ([1989](#bib.bib24)):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 假设给定数据集 $\mathcal{D}=\{x_{i},y_{i}\}_{i=1}^{N}$，损失的变化可以根据 LeCun 等人（[1989](#bib.bib24)）进行公式化：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: where $H$ complexity.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H$ 复杂度。
- en: Element-wise Importance.
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 元素级重要性。
- en: 'The above can be considered as an estimate for the weight $W_{i}$ is assessed
    for its significance:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容可以视为对权重 $W_{i}$ 的重要性进行评估：
- en: '|  | $1$2 |  | (4) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: 'Here, $k$ can be approximated by the Fisher information matrix, and the importance
    can be defined as:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$k$ 可以通过 Fisher 信息矩阵进行近似，重要性可以定义为：
- en: '|  | $1$2 |  | (5) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: Group Importance.
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 组的重要性。
- en: By utilizing either $I_{W_{i}^{k}}$ is the last structure. After assessing the
    importance of each group, we rank the importance of each group and prune the groups
    with lower importance based on a predefined pruning ratio.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 $I_{W_{i}^{k}}$ 作为最后的结构。评估每组的重要性后，我们对每组的重要性进行排序，并根据预定义的修剪比例修剪重要性较低的组。
- en: 3.3 Fast Recovery with Low-rank Approximation
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 使用低秩近似的快速恢复
- en: 'In order to expedite the model recovery process and improve its efficiency
    under limited data, it is crucial to minimize the number of parameters that need
    optimization during the recovery phase. To facilitate this, we employ the low-rank
    approximation, LoRAHu et al. ([2021](#bib.bib19)), to post-train the pruned model.
    Each learnable weight matrix in the model, denoted as $W$. The forward computation
    can now be expressed as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快模型恢复过程并在数据有限的情况下提高其效率，关键在于最小化恢复阶段需要优化的参数数量。为此，我们采用低秩近似，LoRAHu 等人 ([2021](#bib.bib19))，对修剪后的模型进行后训练。模型中的每个可学习的权重矩阵，记作
    $W$。前向计算现在可以表示为：
- en: '|  | $f(x)=(W+\Delta W)X+b=(WX+b)+(PQ)X$ |  | (6) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(x)=(W+\Delta W)X+b=(WX+b)+(PQ)X$ |  | (6) |'
- en: where $b$, which would not cause extra parameters in the final compressed model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b$，这不会导致最终压缩模型中额外的参数。
- en: 'Table 1: Zero-shot performance of the compressed LLaMA-7B. The average is calculated
    among seven classification datasets. ‘Underline’ indicates the best pruning-only
    performance, while ‘bold’ represents the overall best performance with the same
    pruning ratio, considering both pruning and post-training. The ‘Channel’ strategy
    only prunes the dependent group of Type C, while all other methods employ the
    ‘Block’ strategy to prune dependent groups in both Type A and Type B. Since Touvron
    et al. ([2023](#bib.bib49)) did not provide its prompt, the evaluation of the
    result with ^⋆ is performed under different prompts, which is lower than the official
    results.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 压缩的 LLaMA-7B 的零-shot 性能。平均值是从七个分类数据集中计算得出的。‘Underline’ 表示最好的仅修剪性能，而 ‘bold’
    代表在相同修剪比例下，综合考虑修剪和后训练的最佳整体性能。‘通道’策略仅修剪依赖组 C 类型，而所有其他方法采用‘块’策略修剪 A 和 B 类型的依赖组。由于
    Touvron 等人 ([2023](#bib.bib49)) 没有提供其提示，^⋆ 的结果评估是在不同的提示下进行的，低于官方结果。'
- en: '| Pruning Ratio | Method | WikiText2$\color[rgb]{0,.5,.5}\downarrow$ | BoolQ
    | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA | Average |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 修剪比例 | 方法 | WikiText2$\color[rgb]{0,.5,.5}\downarrow$ | BoolQ | PIQA | HellaSwag
    | WinoGrande | ARC-e | ARC-c | OBQA | 平均 |'
- en: '| Ratio = 0% | LLaMA-7BTouvron et al. ([2023](#bib.bib49)) | - | - | 76.5 |
    79.8 | 76.1 | 70.1 | 72.8 | 47.6 | 57.2 | 68.59 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 比例 = 0% | LLaMA-7BTouvron 等人 ([2023](#bib.bib49)) | - | - | 76.5 | 79.8 |
    76.1 | 70.1 | 72.8 | 47.6 | 57.2 | 68.59 |'
- en: '| LLaMA-7B^⋆ | 12.62 | 22.14 | 73.18 | 78.35 | 72.99 | 67.01 | 67.45 | 41.38
    | 42.40 | 63.25 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B^⋆ | 12.62 | 22.14 | 73.18 | 78.35 | 72.99 | 67.01 | 67.45 | 41.38
    | 42.40 | 63.25 |'
- en: '| Ratio = 20%   w/o tune | L2 | 582.41 | 1022.17 | 59.66 | 58.00 | 37.04 |
    52.41 | 33.12 | 28.58 | 29.80 | 42.65 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 比例 = 20% 不调整 | L2 | 582.41 | 1022.17 | 59.66 | 58.00 | 37.04 | 52.41 | 33.12
    | 28.58 | 29.80 | 42.65 |'
- en: '| Random | 27.51 | 43.19 | 61.83 | 71.33 | 56.26 | 54.46 | 57.07 | 32.85 |
    35.00 | 52.69 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 27.51 | 43.19 | 61.83 | 71.33 | 56.26 | 54.46 | 57.07 | 32.85 | 35.00
    | 52.69 |'
- en: '| Channel | 74.63 | 153.75 | 62.75 | 62.73 | 41.40 | 51.07 | 41.38 | 27.90
    | 30.40 | 45.38 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 通道 | 74.63 | 153.75 | 62.75 | 62.73 | 41.40 | 51.07 | 41.38 | 27.90 | 30.40
    | 45.38 |'
- en: '| Vector | 22.28 | 41.78 | 61.44 | 71.71 | 57.27 | 54.22 | 55.77 | 33.96 |
    38.40 | 53.25 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 向量 | 22.28 | 41.78 | 61.44 | 71.71 | 57.27 | 54.22 | 55.77 | 33.96 | 38.40
    | 53.25 |'
- en: '| $\text{Element}^{2}$ | 19.77 | 36.66 | 59.39 | 75.57 | 65.34 | 61.33 | 59.18
    | 37.12 | 39.80 | 56.82 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| $\text{元素}^{2}$ | 19.77 | 36.66 | 59.39 | 75.57 | 65.34 | 61.33 | 59.18 |
    37.12 | 39.80 | 56.82 |'
- en: '| $\text{Element}^{1}$ | 19.09 | 34.21 | 57.06 | 75.68 | 66.80 | 59.83 | 60.94
    | 36.52 | 40.00 | 56.69 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| $\text{元素}^{1}$ | 19.09 | 34.21 | 57.06 | 75.68 | 66.80 | 59.83 | 60.94 |
    36.52 | 40.00 | 56.69 |'
- en: '| Ratio = 20% w/ tune | Channel | 22.02 | 38.67 | 59.08 | 73.39 | 64.02 | 60.54
    | 57.95 | 35.58 | 38.40 | 55.57 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 比例 = 20% 调整 | 通道 | 22.02 | 38.67 | 59.08 | 73.39 | 64.02 | 60.54 | 57.95
    | 35.58 | 38.40 | 55.57 |'
- en: '| Vector | 18.84 | 33.05 | 65.75 | 74.70 | 64.52 | 59.35 | 60.65 | 36.26 |
    39.40 | 57.23 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 向量 | 18.84 | 33.05 | 65.75 | 74.70 | 64.52 | 59.35 | 60.65 | 36.26 | 39.40
    | 57.23 |'
- en: '| $\text{Element}^{2}$ | 17.37 | 30.39 | 69.54 | 76.44 | 68.11 | 65.11 | 63.43
    | 37.88 | 40.00 | 60.07 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| $\text{元素}^{2}$ | 17.37 | 30.39 | 69.54 | 76.44 | 68.11 | 65.11 | 63.43 |
    37.88 | 40.00 | 60.07 |'
- en: '| $\text{Element}^{1}$ | 17.58 | 30.11 | 64.62 | 77.20 | 68.80 | 63.14 | 64.31
    | 36.77 | 39.80 | 59.23 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| $\text{元素}^{1}$ | 17.58 | 30.11 | 64.62 | 77.20 | 68.80 | 63.14 | 64.31 |
    36.77 | 39.80 | 59.23 |'
- en: 'Table 2: Zero-shot performance of the compressed LLaMA-13B. Here we adopt $\text{Element}^{1}$
    as the importance estimation for ‘Channel‘ and ‘Block’.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：压缩后的 LLaMA-13B 的零-shot 性能。这里我们采用 $\text{Element}^{1}$ 作为“Channel”和“Block”的重要性估计。
- en: '| Pruning Ratio | Method | WikiText2$\color[rgb]{0,.5,.5}\downarrow$ | BoolQ
    | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA | Average |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝比例 | 方法 | WikiText2$\color[rgb]{0,.5,.5}\downarrow$ | BoolQ | PIQA | HellaSwag
    | WinoGrande | ARC-e | ARC-c | OBQA | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Ratio = 0% | LLaMA-13B^⋆ | 11.58 | 20.24 | 68.47 | 78.89 | 76.24 | 70.09
    | 74.58 | 44.54 | 42.00 | 64.97 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 比例 = 0% | LLaMA-13B^⋆ | 11.58 | 20.24 | 68.47 | 78.89 | 76.24 | 70.09 | 74.58
    | 44.54 | 42.00 | 64.97 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Ratio = 20%   w/o tune | L2 | 61.15 | 91.43 | 61.50 | 67.57 | 52.90 | 57.54
    | 50.13 | 31.14 | 36.80 | 51.08 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 比例 = 20%   无调优 | L2 | 61.15 | 91.43 | 61.50 | 67.57 | 52.90 | 57.54 | 50.13
    | 31.14 | 36.80 | 51.08 |'
- en: '| Random | 19.24 | 31.84 | 63.33 | 73.18 | 63.54 | 60.85 | 64.44 | 36.26 |
    38.00 | 57.09 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 19.24 | 31.84 | 63.33 | 73.18 | 63.54 | 60.85 | 64.44 | 36.26 | 38.00
    | 57.09 |'
- en: '| Channel | 49.03 | 106.48 | 62.39 | 66.87 | 49.17 | 58.96 | 49.62 | 31.83
    | 33.20 | 50.29 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 通道 | 49.03 | 106.48 | 62.39 | 66.87 | 49.17 | 58.96 | 49.62 | 31.83 | 33.20
    | 50.29 |'
- en: '| Block | 16.01 | 29.28 | 67.68 | 77.15 | 73.41 | 65.11 | 68.35 | 38.40 | 42.40
    | 61.79 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 块 | 16.01 | 29.28 | 67.68 | 77.15 | 73.41 | 65.11 | 68.35 | 38.40 | 42.40
    | 61.79 |'
- en: '| Ratio = 20% w/ tune | L2 | 20.97 | 38.05 | 73.24 | 76.77 | 71.86 | 64.64
    | 67.59 | 39.93 | 40.80 | 62.12 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 比例 = 20% w/ 调优 | L2 | 20.97 | 38.05 | 73.24 | 76.77 | 71.86 | 64.64 | 67.59
    | 39.93 | 40.80 | 62.12 |'
- en: '| Random | 16.84 | 31.98 | 64.19 | 76.06 | 68.89 | 63.30 | 66.88 | 38.31 |
    40.80 | 59.78 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 16.84 | 31.98 | 64.19 | 76.06 | 68.89 | 63.30 | 66.88 | 38.31 | 40.80
    | 59.78 |'
- en: '| Channel | 17.58 | 29.76 | 69.20 | 76.55 | 68.89 | 66.38 | 62.08 | 38.99 |
    39.60 | 60.24 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 通道 | 17.58 | 29.76 | 69.20 | 76.55 | 68.89 | 66.38 | 62.08 | 38.99 | 39.60
    | 60.24 |'
- en: '| Block | 15.18 | 28.08 | 70.31 | 77.91 | 75.16 | 67.88 | 71.09 | 42.41 | 43.40
    | 64.02 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 块 | 15.18 | 28.08 | 70.31 | 77.91 | 75.16 | 67.88 | 71.09 | 42.41 | 43.40
    | 64.02 |'
- en: 4 Experiments
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Experimental Settings
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Foundation Large Language Model.
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基础大型语言模型。
- en: 'To showcase the effectiveness and versatility of LLM-Pruner, we test it over
    three open-source large language models with two kinds of structure: LLaMA-7B Touvron
    et al. ([2023](#bib.bib49)), Vicuna-7B Chiang et al. ([2023](#bib.bib4)) ¹¹1https://huggingface.co/lmsys/vicuna-7b-delta-v0
    and ChatGLM-6B Zeng et al. ([2022](#bib.bib69)).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 LLM-Pruner 的有效性和多样性，我们在三种开源大型语言模型上进行了测试，这些模型具有两种结构：LLaMA-7B Touvron 等 ([2023](#bib.bib49))，Vicuna-7B
    Chiang 等 ([2023](#bib.bib4)) ¹¹1https://huggingface.co/lmsys/vicuna-7b-delta-v0
    和 ChatGLM-6B Zeng 等 ([2022](#bib.bib69))。
- en: Evaluation and Datasets.
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估和数据集。
- en: 'To assess the performance of the model in the task-agnostic setting, we follow
    LLaMa’s evaluation to perform zero-shot task classification on common sense reasoning
    datasets: BoolQ  Clark et al. ([2019](#bib.bib6)), PIQA Bisk et al. ([2020](#bib.bib2)),
    HellaSwag Zellers et al. ([2019](#bib.bib68)), WinoGrande Sakaguchi et al. ([2019](#bib.bib41)),
    ARC-easy Clark et al. ([2018](#bib.bib7)), ARC-challenge Clark et al. ([2018](#bib.bib7))
    and OpenbookQA Mihaylov et al. ([2018](#bib.bib36)). Follow Gao et al. ([2021](#bib.bib14)),
    the model ranks the choices in the multiple choice tasks or generates the answer
    in the open-ended generation ²²2https://github.com/EleutherAI/lm-evaluation-harness.
    Additionally, we complement our evaluation with a zero-shot perplexity (PPL) analysis
    on WikiText2 Merity et al. ([2016](#bib.bib35)) and PTB Marcus et al. ([1993](#bib.bib33)).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型在任务无关设置下的性能，我们遵循 LLaMa 的评估方法，在常识推理数据集上进行零-shot 任务分类：BoolQ Clark 等 ([2019](#bib.bib6))，PIQA
    Bisk 等 ([2020](#bib.bib2))，HellaSwag Zellers 等 ([2019](#bib.bib68))，WinoGrande
    Sakaguchi 等 ([2019](#bib.bib41))，ARC-easy Clark 等 ([2018](#bib.bib7))，ARC-challenge
    Clark 等 ([2018](#bib.bib7)) 和 OpenbookQA Mihaylov 等 ([2018](#bib.bib36))。按照 Gao
    等 ([2021](#bib.bib14)) 的方法，模型在多项选择任务中对选项进行排名或在开放式生成中生成答案 ²²2https://github.com/EleutherAI/lm-evaluation-harness。此外，我们还通过对
    WikiText2 Merity 等 ([2016](#bib.bib35)) 和 PTB Marcus 等 ([1993](#bib.bib33)) 进行零-shot
    困惑度 (PPL) 分析来补充我们的评估。
- en: Implementation Details.
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现细节。
- en: 'In the model pruning process, we use 10 randomly selected samples from Bookcorpus Zhu
    et al. ([2015](#bib.bib70)), each truncated to a sequence length of 128, as the
    calibration samples for establishing dependency and calculating the gradient for
    both LLaMA and Vicuna. For ChatGLM, we select 10 random samples from DailyDialog Li
    et al. ([2017](#bib.bib27)). During the recovery phase, we utilize the cleaned
    version of Alpaca Taori et al. ([2023](#bib.bib47)), which comprises approximately
    50k samples. Remarkably, tuning these samples requires merely 3 hours on a single
    GPU with only 2 epochs. More hyper-parameters of pruning and training can be found
    in Appendix [B](#A2 "Appendix B Implementation Details ‣ LLM-Pruner: On the Structural
    Pruning of Large Language Models").'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '在模型剪枝过程中，我们使用了从 Bookcorpus Zhu 等人 ([2015](#bib.bib70)) 随机选择的 10 个样本，每个样本截断为长度为
    128 的序列，作为建立依赖关系和计算 LLaMA 和 Vicuna 梯度的校准样本。对于 ChatGLM，我们从 DailyDialog Li 等人 ([2017](#bib.bib27))
    中选择了 10 个随机样本。在恢复阶段，我们使用了 Alpaca Taori 等人 ([2023](#bib.bib47)) 的清洁版本，该版本包含大约 50k
    个样本。值得注意的是，调整这些样本仅需在单个 GPU 上用 2 个 epoch 完成 3 小时的工作。更多关于剪枝和训练的超参数可以在附录 [B](#A2
    "附录 B 实施细节 ‣ LLM-Pruner: 大型语言模型的结构剪枝") 中找到。'
- en: 'Table 3: Statistics of the base model and the compressed model.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：基础模型和压缩模型的统计数据。
- en: '| Model | Strategy | Ratio | #Params | #MACs | Memory | Latency |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 策略 | 比例 | 参数数量 | MACs | 内存 | 延迟 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-7B Vicuna-7B | - | - | 6.74B | 424.02G | 12884.5MiB | 69.32s |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B Vicuna-7B | - | - | 6.74B | 424.02G | 12884.5MiB | 69.32s |'
- en: '| Channel | 20% | 5.39B | 339.36G | 10363.6MiB | 61.50s |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 通道 | 20% | 5.39B | 339.36G | 10363.6MiB | 61.50s |'
- en: '| Block | 20% | 5.42B | 339.60G | 10375.5MiB | 58.55s |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 块 | 20% | 5.42B | 339.60G | 10375.5MiB | 58.55s |'
- en: '| Channel | 50% | 3.37B | 212.58G | 6556.3MiB | 40.11s |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 通道 | 50% | 3.37B | 212.58G | 6556.3MiB | 40.11s |'
- en: '| Block | 50% | 3.35B | 206.59G | 6533.9MiB | 37.54s |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 块 | 50% | 3.35B | 206.59G | 6533.9MiB | 37.54s |'
- en: Statistics of the Compressed Model.
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 压缩模型的统计数据。
- en: 'Table [3](#S4.T3 "Table 3 ‣ Implementation Details. ‣ 4.1 Experimental Settings
    ‣ 4 Experiments ‣ LLM-Pruner: On the Structural Pruning of Large Language Models")
    presents the statistic of the 7B models that are used in our experiments: the
    parameter count, MACs, memory requirements and latency for running each model.
    The statistical evaluation is conducted using the inference mode, where the model
    is fed a sentence consisting of 64 tokens. The latency is tested under the test
    set of WikiText2 on a single A5000. Here, the ‘Block’ strategy implies that the
    pruned unit in the model consists of Group Type A and Group Type B as illustrated
    in Figure [2](#S3.F2 "Figure 2 ‣ Trigger the Dependency Graph. ‣ 3.1 Discover
    All Coupled Structure in LLMs ‣ 3 Methods ‣ LLM-Pruner: On the Structural Pruning
    of Large Language Models"), whereas ‘Channel’ indicates that the unit to be pruned
    is Group Type C. We delve into an analysis of these two choices in Section [4.2](#S4.SS2.SSS0.Px2
    "Channel Strategy vs. Block Strategy. ‣ 4.2 Zero-shot Performance ‣ 4 Experiments
    ‣ LLM-Pruner: On the Structural Pruning of Large Language Models")(Channel Strategy
    vs. Block Strategy). The pruning ratio stated here denotes the approximate ratio
    of parameters to be pruned since the number of parameters within each pruned structure
    does not perfectly match the total number of pruned parameters.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [3](#S4.T3 "表 3 ‣ 实施细节 ‣ 4.1 实验设置 ‣ 4 实验 ‣ LLM-Pruner: 大型语言模型的结构剪枝") 展示了我们实验中使用的
    7B 模型的统计数据：参数数量、MACs、内存需求和运行每个模型的延迟。统计评估是在推理模式下进行的，其中模型输入由 64 个 token 组成的句子。在单个
    A5000 上的 WikiText2 测试集下测试延迟。在这里，“块”策略意味着模型中被剪枝的单元由图 [2](#S3.F2 "图 2 ‣ 触发依赖图 ‣
    3.1 发现 LLM 中的所有耦合结构 ‣ 3 方法 ‣ LLM-Pruner: 大型语言模型的结构剪枝") 中所示的 A 组和 B 组组成，而“通道”表示要剪枝的单元是
    C 组。我们在第 [4.2](#S4.SS2.SSS0.Px2 "通道策略 vs. 块策略 ‣ 4.2 零-shot 性能 ‣ 4 实验 ‣ LLM-Pruner:
    大型语言模型的结构剪枝") 节中深入分析了这两种选择（通道策略 vs. 块策略）。这里所述的剪枝比例表示要剪枝的参数的近似比例，因为每个剪枝结构中的参数数量与总剪枝参数数量并不完全匹配。'
- en: 4.2 Zero-shot Performance
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 零-shot 性能
- en: 'Table [1](#S3.T1 "Table 1 ‣ 3.3 Fast Recovery with Low-rank Approximation ‣
    3 Methods ‣ LLM-Pruner: On the Structural Pruning of Large Language Models"),[2](#S3.T2
    "Table 2 ‣ 3.3 Fast Recovery with Low-rank Approximation ‣ 3 Methods ‣ LLM-Pruner:
    On the Structural Pruning of Large Language Models"),[4](#S4.T4 "Table 4 ‣ 4.2
    Zero-shot Performance ‣ 4 Experiments ‣ LLM-Pruner: On the Structural Pruning
    of Large Language Models") and [5](#S4.T5 "Table 5 ‣ Channel Strategy vs. Block
    Strategy. ‣ 4.2 Zero-shot Performance ‣ 4 Experiments ‣ LLM-Pruner: On the Structural
    Pruning of Large Language Models") shows the zero-shot performance of the pruned
    model. Based on the evaluation conducted on LLaMA, employing a 20% parameter reduction
    without post-training, the pruned model manages to retain 89.8% of the performance
    exhibited by the unpruned model. Furthermore, through the efficient post-training,
    the classification accuracy further improves to 60.07%, achieving 94.97% of the
    accuracy attained by the original model. This demonstration proves the feasibility
    of using LLM-Pruner to effectively compress the model, even without relying on
    training data, and within a remarkably short period of time. Surprisingly, we
    discover that on most datasets, the pruned model with 5.4B LLaMA even outperformed
    chatGLM-6B. This highlights the superiority of the LLM-Pruner: if a smaller model
    with a customized size is required, LLM-Pruner is more cost-effective compared
    to retraining another model with a satisfying performance. However, with 50% parameters
    pruned, a large accuracy degradation is observed (see Appendix [C.5](#A3.SS5 "C.5
    Pruning with Large Rates ‣ Appendix C More Analysis ‣ LLM-Pruner: On the Structural
    Pruning of Large Language Models")). Compressing LLMs under high compression rates
    still remains a large challenge.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [1](#S3.T1 "表 1 ‣ 3.3 低秩近似的快速恢复 ‣ 3 方法 ‣ LLM-Pruner: 大型语言模型的结构剪枝")，[2](#S3.T2
    "表 2 ‣ 3.3 低秩近似的快速恢复 ‣ 3 方法 ‣ LLM-Pruner: 大型语言模型的结构剪枝")，[4](#S4.T4 "表 4 ‣ 4.2
    零样本性能 ‣ 4 实验 ‣ LLM-Pruner: 大型语言模型的结构剪枝") 和 [5](#S4.T5 "表 5 ‣ 通道策略与块策略 ‣ 4.2 零样本性能
    ‣ 4 实验 ‣ LLM-Pruner: 大型语言模型的结构剪枝") 显示了剪枝模型的零样本性能。根据对 LLaMA 的评估，通过在未经过后训练的情况下减少
    20% 的参数，剪枝模型能够保持原模型 89.8% 的性能。此外，通过高效的后训练，分类准确率进一步提高至 60.07%，达到了原模型 94.97% 的准确率。这一展示证明了使用
    LLM-Pruner 有效压缩模型的可行性，即使不依赖训练数据，并且在极短的时间内。令人惊讶的是，我们发现，在大多数数据集上，剪枝后的 5.4B LLaMA
    模型甚至超越了 chatGLM-6B。这突显了 LLM-Pruner 的优越性：如果需要一个具有定制尺寸的更小模型，LLM-Pruner 比重新训练一个性能令人满意的模型更具成本效益。然而，当剪枝
    50% 参数时，观察到准确率的大幅下降（见附录 [C.5](#A3.SS5 "C.5 大规模剪枝 ‣ 附录 C 更多分析 ‣ LLM-Pruner: 大型语言模型的结构剪枝")）。在高压缩率下压缩
    LLM 仍然是一个巨大挑战。'
- en: 'Table 4: Zero-shot performance of the compressed Vicuna-7B'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 压缩的 Vicuna-7B 的零样本性能'
- en: '| Pruned Model | Method | WikiText2 $\color[rgb]{0,.5,.5}\downarrow$ | BoolQ
    | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA | Average |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝模型 | 方法 | WikiText2 $\color[rgb]{0,.5,.5}\downarrow$ | BoolQ | PIQA | HellaSwag
    | WinoGrande | ARC-e | ARC-c | OBQA | 平均 |'
- en: '| Ratio = 0% | Vicuna-7B | 16.11 | 61.37 | 76.57 | 77.75 | 70.64 | 67.40 |
    65.11 | 41.21 | 40.80 | 62.78 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 比例 = 0% | Vicuna-7B | 16.11 | 61.37 | 76.57 | 77.75 | 70.64 | 67.40 | 65.11
    | 41.21 | 40.80 | 62.78 |'
- en: '| Ratio = 20% w/o tune | l2 | 3539.98 | 5882.21 | 55.90 | 56.15 | 32.37 | 51.85
    | 30.01 | 28.41 | 28.20 | 40.41 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 比例 = 20% 不调整 | l2 | 3539.98 | 5882.21 | 55.90 | 56.15 | 32.37 | 51.85 | 30.01
    | 28.41 | 28.20 | 40.41 |'
- en: '| random | 34.63 | 112.44 | 61.47 | 70.89 | 54.67 | 56.27 | 55.60 | 31.74 |
    34.60 | 52.18 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 34.63 | 112.44 | 61.47 | 70.89 | 54.67 | 56.27 | 55.60 | 31.74 | 34.60
    | 52.18 |'
- en: '| Channel | 71.75 | 198.88 | 51.77 | 63.93 | 42.58 | 55.17 | 43.94 | 29.27
    | 33.40 | 45.72 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 通道 | 71.75 | 198.88 | 51.77 | 63.93 | 42.58 | 55.17 | 43.94 | 29.27 | 33.40
    | 45.72 |'
- en: '| Vector | 27.03 | 92.51 | 62.17 | 71.44 | 55.80 | 53.43 | 55.77 | 33.28 |
    37.80 | 52.81 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 向量 | 27.03 | 92.51 | 62.17 | 71.44 | 55.80 | 53.43 | 55.77 | 33.28 | 37.80
    | 52.81 |'
- en: '| $\text{Element}^{2}$ | 24.70 | 94.34 | 62.87 | 75.41 | 64.00 | 58.41 | 60.98
    | 37.12 | 39.00 | 56.83 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| $\text{元素}^{2}$ | 24.70 | 94.34 | 62.87 | 75.41 | 64.00 | 58.41 | 60.98 |
    37.12 | 39.00 | 56.83 |'
- en: '| $\text{Element}^{1}$ | 25.74 | 92.88 | 61.70 | 75.30 | 63.75 | 56.20 | 63.22
    | 36.60 | 37.00 | 56.25 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| $\text{元素}^{1}$ | 25.74 | 92.88 | 61.70 | 75.30 | 63.75 | 56.20 | 63.22 |
    36.60 | 37.00 | 56.25 |'
- en: '| Ratio = 20% w/ tune | Vector | 19.94 | 74.66 | 63.15 | 74.59 | 61.95 | 60.30
    | 60.48 | 36.60 | 39.40 | 56.64 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 比例 = 20% 调整 | 向量 | 19.94 | 74.66 | 63.15 | 74.59 | 61.95 | 60.30 | 60.48
    | 36.60 | 39.40 | 56.64 |'
- en: '| $\text{Element}^{2}$ | 18.97 | 76.78 | 60.40 | 75.63 | 65.45 | 63.22 | 63.05
    | 37.71 | 39.00 | 57.78 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| $\text{元素}^{2}$ | 18.97 | 76.78 | 60.40 | 75.63 | 65.45 | 63.22 | 63.05 |
    37.71 | 39.00 | 57.78 |'
- en: '| $\text{Element}^{1}$ | 19.69 | 78.25 | 63.33 | 76.17 | 65.13 | 60.22 | 62.84
    | 37.12 | 39.20 | 57.71 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| $\text{Element}^{1}$ | 19.69 | 78.25 | 63.33 | 76.17 | 65.13 | 60.22 | 62.84
    | 37.12 | 39.20 | 57.71 |'
- en: The compression results of Vicuna-7B align with those of LLaMA, as pruning 20%
    of parameters on Vicuna-7B maintains performance at 92.03% of the original model.
    We test a smaller pruning rate of 10% on chatGLM-7B, where the pruned model only
    experiences a marginal performance decrease of 0.89%, which can be recovered through
    post-training. Despite the pruned model outperforming the uncompressed model,
    we don’t assert it is better than the original model. This is largely because
    chatGLM-6B, a bilingual model, has limited English pre-training exposure. Post-training,
    however, introduces it to more English corpus, albeit limited, improving its English
    comprehension.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Vicuna-7B 的压缩结果与 LLaMA 的结果一致，因为在 Vicuna-7B 上剪枝 20% 的参数使性能保持在原始模型的 92.03%。我们在
    chatGLM-7B 上测试了更小的剪枝率 10%，在这种情况下，剪枝模型的性能仅下降了 0.89%，这一点可以通过后训练恢复。尽管剪枝模型的表现优于未压缩模型，但我们并不认为它优于原始模型。这在很大程度上是因为
    chatGLM-6B 是一个双语模型，其英文预训练暴露有限。然而，后训练使其接触到更多的英文语料，尽管数量有限，从而提升了其英文理解能力。
- en: 'Ablation: Impact of Importance Estimation.'
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 消融实验：重要性估计的影响。
- en: 'We conduct tests on all proposed importance estimation techniques mentioned
    in Section [3.2](#S3.SS2 "3.2 Grouped Importance Estimation of Coupled Structure
    ‣ 3 Methods ‣ LLM-Pruner: On the Structural Pruning of Large Language Models").
    The results can be found in Table [1](#S3.T1 "Table 1 ‣ 3.3 Fast Recovery with
    Low-rank Approximation ‣ 3 Methods ‣ LLM-Pruner: On the Structural Pruning of
    Large Language Models") and [4](#S4.T4 "Table 4 ‣ 4.2 Zero-shot Performance ‣
    4 Experiments ‣ LLM-Pruner: On the Structural Pruning of Large Language Models").
    Here, $\textit{Element}^{\text{n}}$ represents the importance evaluation utilizing
    the n-th order term in Eq.[5](#S3.E5 "In Element-wise Importance. ‣ 3.2 Grouped
    Importance Estimation of Coupled Structure ‣ 3 Methods ‣ LLM-Pruner: On the Structural
    Pruning of Large Language Models"). Vector represents the result corresponding
    to Eq.[3](#S3.E3 "In Vector-wise Importance. ‣ 3.2 Grouped Importance Estimation
    of Coupled Structure ‣ 3 Methods ‣ LLM-Pruner: On the Structural Pruning of Large
    Language Models"). Based on the results obtained from LLaMA-7B and Vicuna-7B,
    pruning algorithms achieved the best average performance mostly by leveraging
    the second-order derivatives for each parameter. Nonetheless, given that first-order
    derivatives are considerably more efficient than second-order derivatives, though
    yielding slightly inferior results, we still vote for the first-order term as
    a competitive method. Besides, the results on chatGLM-7B differed significantly
    from these findings. The importance estimation on each parameter fails, performing
    even worse than l2, while the importance estimation on the weight matrix reaches
    the best performance.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对第[3.2节](#S3.SS2 "3.2 Grouped Importance Estimation of Coupled Structure
    ‣ 3 Methods ‣ LLM-Pruner: On the Structural Pruning of Large Language Models")中提到的所有重要性估计技术进行了测试。结果可以在表格[1](#S3.T1
    "Table 1 ‣ 3.3 Fast Recovery with Low-rank Approximation ‣ 3 Methods ‣ LLM-Pruner:
    On the Structural Pruning of Large Language Models")和[4](#S4.T4 "Table 4 ‣ 4.2
    Zero-shot Performance ‣ 4 Experiments ‣ LLM-Pruner: On the Structural Pruning
    of Large Language Models")中找到。在这里，$\textit{Element}^{\text{n}}$ 代表利用 Eq.[5](#S3.E5
    "In Element-wise Importance. ‣ 3.2 Grouped Importance Estimation of Coupled Structure
    ‣ 3 Methods ‣ LLM-Pruner: On the Structural Pruning of Large Language Models")中的
    n 次项进行的重要性评估。向量代表与 Eq.[3](#S3.E3 "In Vector-wise Importance. ‣ 3.2 Grouped Importance
    Estimation of Coupled Structure ‣ 3 Methods ‣ LLM-Pruner: On the Structural Pruning
    of Large Language Models")相对应的结果。根据从 LLaMA-7B 和 Vicuna-7B 获得的结果，剪枝算法通过利用每个参数的二阶导数实现了最佳的平均性能。然而，鉴于一阶导数在计算效率上明显优于二阶导数，尽管结果略差，我们仍然认为一阶项是一个具有竞争力的方法。此外，chatGLM-7B
    上的结果与这些发现有显著差异。每个参数上的重要性估计失败，表现甚至比 l2 更差，而权重矩阵上的重要性估计达到了最佳性能。'
- en: '![Refer to caption](img/2866fbffa0030dcb6141a3183b55ad4f.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/2866fbffa0030dcb6141a3183b55ad4f.png)'
- en: 'Figure 3: Layer sensitivity for Pruning: Removing Groups in only one layer.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：剪枝的层敏感性：仅在一个层中移除组。
- en: Channel Strategy vs. Block Strategy.
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通道策略与块策略。
- en: 'From the results presented in Table [2](#S3.T2 "Table 2 ‣ 3.3 Fast Recovery
    with Low-rank Approximation ‣ 3 Methods ‣ LLM-Pruner: On the Structural Pruning
    of Large Language Models"), it is evident that pruning ‘Channel’ significantly
    deteriorates performance compared to pruning ‘Block’. This discrepancy arises
    because the layers within the stacked transformer do not evenly distribute their
    importance. As shown in Figure [3](#S4.F3 "Figure 3 ‣ Ablation: Impact of Importance
    Estimation. ‣ 4.2 Zero-shot Performance ‣ 4 Experiments ‣ LLM-Pruner: On the Structural
    Pruning of Large Language Models"), the first and last layers have a profound
    impact on the model’s performance, and pruning them results in more substantial
    performance degradation compared to other layers. However, due to the uniform
    treatment of the ‘Channel’ group across all layers, it becomes inevitable to prune
    the first and last layers, leading to a significant decline in performance.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '从表格[2](#S3.T2 "Table 2 ‣ 3.3 Fast Recovery with Low-rank Approximation ‣ 3
    Methods ‣ LLM-Pruner: On the Structural Pruning of Large Language Models")中展示的结果来看，相比于‘Block’的剪枝，‘Channel’的剪枝显著恶化了性能。这种差异的产生是因为堆叠的变换器中的层次未能均匀分配其重要性。如图[3](#S4.F3
    "Figure 3 ‣ Ablation: Impact of Importance Estimation. ‣ 4.2 Zero-shot Performance
    ‣ 4 Experiments ‣ LLM-Pruner: On the Structural Pruning of Large Language Models")所示，第一层和最后一层对模型的性能有深远的影响，剪枝这些层会导致比其他层更显著的性能下降。然而，由于‘Channel’组在所有层次中的处理方式均匀，必须剪枝第一层和最后一层，从而导致性能显著下降。'
- en: 'Table 5: Zero-shot Performance of the compressed ChatGLM-6B'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：压缩后的ChatGLM-6B的零-shot性能
- en: '| Pruned Model | Method | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA
    | Average |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝模型 | 方法 | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Ratio = 0% | ChatGLM-6B | 67.95 | 46.37 | 52.33 | 48.36 | 29.95 | 37.40 |
    47.05 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 比例 = 0% | ChatGLM-6B | 67.95 | 46.37 | 52.33 | 48.36 | 29.95 | 37.40 | 47.05
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Ratio = 10% w/o tune | L2 | 61.97 | 37.22 | 49.72 | 42.05 | 28.24 | 35.40
    | 42.43 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 比例 = 10% 无调优 | L2 | 61.97 | 37.22 | 49.72 | 42.05 | 28.24 | 35.40 | 42.43
    |'
- en: '| Random | 65.29 | 43.18 | 51.30 | 47.52 | 29.52 | 34.60 | 45.24 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 65.29 | 43.18 | 51.30 | 47.52 | 29.52 | 34.60 | 45.24 |'
- en: '| Vector | 66.32 | 43.51 | 53.04 | 47.56 | 30.72 | 35.80 | 46.16 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 向量 | 66.32 | 43.51 | 53.04 | 47.56 | 30.72 | 35.80 | 46.16 |'
- en: '| $\text{Element}^{1}$ | 54.35 | 28.07 | 50.59 | 27.82 | 24.66 | 33.20 | 36.45
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| $\text{Element}^{1}$ | 54.35 | 28.07 | 50.59 | 27.82 | 24.66 | 33.20 | 36.45
    |'
- en: '| w/ tune | Vector | 67.74 | 46.35 | 53.99 | 51.01 | 29.95 | 35.00 | 47.34
    | ![Refer to caption](img/a85471f25d4f6d2bdae1821973453523.png)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '| 调优 | 向量 | 67.74 | 46.35 | 53.99 | 51.01 | 29.95 | 35.00 | 47.34 | ![参见标题](img/a85471f25d4f6d2bdae1821973453523.png)'
- en: 'Figure 4: The pruning results on LLaMA-7B (left) and Vicuna-7B (right) with
    different pruning rates.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：不同剪枝率下LLaMA-7B（左）和Vicuna-7B（右）的剪枝结果。
- en: '![Refer to caption](img/b93626b0711daab59af300f1eb7a09c0.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b93626b0711daab59af300f1eb7a09c0.png)'
- en: 'Figure 5: Perplexity on zero-shot datasets across varyhing steps.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：不同步骤下的零-shot数据集的困惑度。
- en: 'Table 6: Effect of the dependency-based structural pruning. Average represents
    the average performance on 7 classification datasets.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：基于依赖的结构剪枝效果。平均值表示在7个分类数据集上的平均性能。
- en: '|  | Method | WikiText2$\color[rgb]{1,0,0}\downarrow$ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | WikiText2$\color[rgb]{1,0,0}\downarrow$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| w/o Tuning | w/o dependency | 68378.42 | 79942.47 | 38.32 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 无调优 | 无依赖 | 68378.42 | 79942.47 | 38.32 |'
- en: '| w/ dependency | 19.09 | 34.21 | 56.69 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 带依赖 | 19.09 | 34.21 | 56.69 |'
- en: '| w/ Tuning | w/o dependency | 13307.46 | 13548.08 | 38.10 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 带调优 | 无依赖 | 13307.46 | 13548.08 | 38.10 |'
- en: '| w/ dependency | 17.58 | 30.11 | 59.23 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 带依赖 | 17.58 | 30.11 | 59.23 |'
- en: 'Table 7: Impact of different aggregation strategies on group importance estimation.
    Experiments are performed on LLaMA-7B.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：不同聚合策略对组重要性估计的影响。实验在LLaMA-7B上进行。
- en: '| Method | WikiText2$\color[rgb]{1,0,0}\downarrow$ |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WikiText2$\color[rgb]{1,0,0}\downarrow$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Summation | 66.13 | 164.25 | 40.70 | 63.49 | 34.80 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 总和 | 66.13 | 164.25 | 40.70 | 63.49 | 34.80 |'
- en: '| Max | 62.59 | 144.38 | 39.60 | 63.71 | 34.60 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 最大 | 62.59 | 144.38 | 39.60 | 63.71 | 34.60 |'
- en: '| Production | 77.63 | 192.88 | 37.84 | 62.08 | 35.00 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 生产 | 77.63 | 192.88 | 37.84 | 62.08 | 35.00 |'
- en: '| Last-only | 130.00 | 170.88 | 41.92 | 64.75 | 35.20 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 仅最后 | 130.00 | 170.88 | 41.92 | 64.75 | 35.20 |'
- en: 4.3 More Analysis
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 更多分析
- en: Impact of Different Pruning Rates.
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同剪枝率的影响。
- en: 'We investigate the impact of pruning the LLM at various pruning ratios in Figure
    [5](#S4.F5 "Figure 5 ‣ Channel Strategy vs. Block Strategy. ‣ 4.2 Zero-shot Performance
    ‣ 4 Experiments ‣ LLM-Pruner: On the Structural Pruning of Large Language Models").
    We compare our pruning results with the L2 strategy because L2 is also a data-free
    pruning algorithm. It is observed in the experiment of LLaMA that when the pruning
    ratio reaches approximately 20%, the magnitude-dependent algorithm experiences
    a rapid collapse, leading to the loss of information. Conversely, by employing
    LLM-Pruner, we are able to increase the pruning ratio to around 60% while achieving
    an equivalent perplexity level. Furthermore, in the case of Vicuna-7B, removing
    10% parameters results in a performance decline equivalent to that of LLM-Pruner
    with 60%. The utilization of LLM-Pruner enables a significant increase in the
    number of model parameters that can be pruned, thereby substantially reducing
    computational overhead.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图 [5](#S4.F5 "Figure 5 ‣ Channel Strategy vs. Block Strategy. ‣ 4.2 Zero-shot
    Performance ‣ 4 Experiments ‣ LLM-Pruner: On the Structural Pruning of Large Language
    Models") 中调查了在不同剪枝比例下剪枝 LLM 的影响。我们将我们的剪枝结果与 L2 策略进行了比较，因为 L2 也是一种无数据剪枝算法。在 LLaMA
    的实验中观察到，当剪枝比例达到约 20% 时，基于幅度的算法经历了快速崩溃，导致信息丢失。相反，通过使用 LLM-Pruner，我们能够将剪枝比例提高到约
    60%，同时保持相当的困惑度水平。此外，在 Vicuna-7B 的情况下，去除 10% 的参数会导致性能下降，相当于 LLM-Pruner 剪枝 60% 的效果。利用
    LLM-Pruner 可以显著增加可剪枝的模型参数数量，从而大幅减少计算开销。'
- en: Tuning on the External Dataset.
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在外部数据集上的调优。
- en: 'To tune the pruned model, we utilize the external dataset Alpaca Taori et al.
    ([2023](#bib.bib47)). The evaluation curves of the pruned model on two zero-shot
    datasets during the post-training process are depicted in Figure [5](#S4.F5 "Figure
    5 ‣ Channel Strategy vs. Block Strategy. ‣ 4.2 Zero-shot Performance ‣ 4 Experiments
    ‣ LLM-Pruner: On the Structural Pruning of Large Language Models"). The results
    demonstrate a rapid decrease in the perplexity of the pruned model within 300
    steps, followed by a gradual increase. We provide a more comprehensive evaluation
    in Appendix [C.4](#A3.SS4 "C.4 Overfitting Phenomena in Post-Training ‣ Appendix
    C More Analysis ‣ LLM-Pruner: On the Structural Pruning of Large Language Models").
    It is important to note that if the model is trained for an excessive number of
    steps, it runs the risk of overfitting the external dataset, potentially compromising
    its performance in other general-purpose tasks.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '为了调优剪枝模型，我们利用外部数据集 Alpaca Taori 等（[2023](#bib.bib47)）。剪枝模型在两个零-shot 数据集上的评估曲线在图
    [5](#S4.F5 "Figure 5 ‣ Channel Strategy vs. Block Strategy. ‣ 4.2 Zero-shot Performance
    ‣ 4 Experiments ‣ LLM-Pruner: On the Structural Pruning of Large Language Models")
    中描绘。结果显示，剪枝模型的困惑度在 300 步内迅速下降，然后逐渐上升。我们在附录 [C.4](#A3.SS4 "C.4 Overfitting Phenomena
    in Post-Training ‣ Appendix C More Analysis ‣ LLM-Pruner: On the Structural Pruning
    of Large Language Models") 中提供了更全面的评估。值得注意的是，如果模型训练的步数过多，可能会导致对外部数据集的过拟合，从而影响其在其他通用任务中的表现。'
- en: Impact of Dependency-based Structured Pruning.
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于依赖的结构性剪枝的影响。
- en: 'To study the importance of dependency-based structural pruning, we conduct
    an experiment to disrupt dependencies within groups, where each weight matrix
    $W_{i}$ is pruned solely based on the importance score estimated on itself. Table
    [7](#S4.T7 "Table 7 ‣ Channel Strategy vs. Block Strategy. ‣ 4.2 Zero-shot Performance
    ‣ 4 Experiments ‣ LLM-Pruner: On the Structural Pruning of Large Language Models")
    presents the results demonstrating the impact of dependencies in structural pruning.
    In the absence of dependencies, the model nearly fails in the zero-shot generation
    and classification tasks. Even with tuning, the model fails to recover, showing
    a substantial difference compared to the results in dependency-based pruning.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '为了研究基于依赖的结构性剪枝的重要性，我们进行了一项实验，扰乱组内的依赖关系，在该实验中，每个权重矩阵 $W_{i}$ 仅根据自身估计的置信度分数进行剪枝。表
    [7](#S4.T7 "Table 7 ‣ Channel Strategy vs. Block Strategy. ‣ 4.2 Zero-shot Performance
    ‣ 4 Experiments ‣ LLM-Pruner: On the Structural Pruning of Large Language Models")
    展示了基于依赖的结构性剪枝的影响。没有依赖关系的情况下，模型几乎无法完成零-shot 生成和分类任务。即使经过调优，模型也无法恢复，与基于依赖的剪枝结果相比，差距显著。'
- en: Impact of Different Aggregation Strategies.
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同聚合策略的影响。
- en: 'We conduct tests on the aggregation algorithms proposed in Section [3.2](#S3.SS2
    "3.2 Grouped Importance Estimation of Coupled Structure ‣ 3 Methods ‣ LLM-Pruner:
    On the Structural Pruning of Large Language Models"). Our experimental results
    unveil notable discrepancies in model performance across different aggregation
    strategies, with particular emphasis on the ‘Last-only’ strategy. Among the evaluated
    approaches, the ‘Max’ strategy attains the most favorable outcomes in terms of
    perplexity, signifying enhanced coherence and fluency in sentence generation.
    However, it is important to note that the ‘Max’ strategy exhibits the poorest
    zero-shot classification results compared to all four strategies. Conversely,
    the ‘Last-only’ strategy showcases superior classification performance but suffers
    from the poorest generation quality. In our experiments, we make a trade-off by
    selecting the ‘Sum’ strategy since it shows both good generalization quality and
    classification performance.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对第 [3.2](#S3.SS2 "3.2 Grouped Importance Estimation of Coupled Structure
    ‣ 3 Methods ‣ LLM-Pruner: On the Structural Pruning of Large Language Models")
    节中提出的聚合算法进行了测试。我们的实验结果揭示了不同聚合策略下模型性能的显著差异，特别是在‘仅最后’策略上。在评估的方法中，‘最大’策略在困惑度方面获得了最好的结果，表明句子生成的连贯性和流畅性有所提升。然而，需要注意的是，与所有四种策略相比，‘最大’策略在零-shot
    分类任务中的结果最差。相反，‘仅最后’策略展示了更优的分类性能，但生成质量最差。在我们的实验中，我们通过选择‘总和’策略进行了权衡，因为它在泛化质量和分类性能方面表现良好。'
- en: 'Table 8: DistilBert vs. LLM-Pruner. The average here means the average score
    on the above seven datasets.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: DistilBert vs. LLM-Pruner。这里的平均值指的是上述七个数据集的平均得分。'
- en: '| Pruning Ratio | #Param | Average |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 修剪比例 | 参数数量 | 平均值 |'
- en: '| --- | --- | --- |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| DistilBert | 3.50B | 44.64 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| DistilBert | 3.50B | 44.64 |'
- en: '| LLM-Pruner | 3.35B | 48.88 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner | 3.35B | 48.88 |'
- en: Comparison with DistilBERT
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与 DistilBERT 的比较
- en: 'We show the comparison results of DistilBERT and LLM-Pruner on LLaMA-7B in
    Table [8](#S4.T8 "Table 8 ‣ Impact of Different Aggregation Strategies. ‣ 4.3
    More Analysis ‣ 4 Experiments ‣ LLM-Pruner: On the Structural Pruning of Large
    Language Models"). LLM-Pruner outperforms DistilBERT by 4.24% on average with
    even a smaller size. The reason lies in that LLM-Pruner minimizes model disruption
    during pruning, whereas DistilBERT merely selects one layer out of two. As a result,
    the model pruned by LLM-Pruner demands less data to recover its performance compared
    with DistilBERT, consequently achieving superior performance.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格 [8](#S4.T8 "Table 8 ‣ Impact of Different Aggregation Strategies. ‣ 4.3
    More Analysis ‣ 4 Experiments ‣ LLM-Pruner: On the Structural Pruning of Large
    Language Models") 中展示了 DistilBERT 和 LLM-Pruner 在 LLaMA-7B 上的对比结果。LLM-Pruner 在尺寸更小的情况下，平均超越了
    DistilBERT 4.24%。原因在于 LLM-Pruner 在修剪过程中最小化了模型的干扰，而 DistilBERT 只是从两个层中选择一个。因此，与
    DistilBERT 相比，LLM-Pruner 修剪后的模型需要更少的数据来恢复性能，从而实现了更优秀的表现。'
- en: Scratch Training vs. Pruning.
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Scratch 训练 vs. 修剪。
- en: 'We compare LLM-Pruner with StableLM-3B³³3https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b
    with a similar parameter size. To ensure fairness, both models are fine-tuned
    on the Alpaca dataset. The experimental results of these two models are shown
    in the Table [9](#S4.T9 "Table 9 ‣ Scratch Training vs. Pruning. ‣ 4.3 More Analysis
    ‣ 4 Experiments ‣ LLM-Pruner: On the Structural Pruning of Large Language Models").
    LLM-Pruner crafts lightweight LLMs with low resources, and even can sometimes
    achieve better performance than LLMs from scratch training. However, we also acknowledge
    that the LLaMA-3B obtained by LLM-Pruner will not always outperform other 3B models
    from scratch training, due to the huge gap in the size of training corpus.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将 LLM-Pruner 与具有类似参数规模的 StableLM-3B³³3https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b
    进行了比较。为了确保公平性，这两个模型都在 Alpaca 数据集上进行了微调。这两种模型的实验结果展示在表格 [9](#S4.T9 "Table 9 ‣ Scratch
    Training vs. Pruning. ‣ 4.3 More Analysis ‣ 4 Experiments ‣ LLM-Pruner: On the
    Structural Pruning of Large Language Models") 中。LLM-Pruner 制作了资源需求低的轻量级 LLM，并且有时甚至可以比从头训练的
    LLM 获得更好的性能。然而，我们也承认，由于训练语料库规模的巨大差异，通过 LLM-Pruner 获得的 LLaMA-3B 不一定总是优于其他 3B 模型的从头训练结果。'
- en: 'Table 9: Scratch Training (StableLM-3B) vs. LLaMA-3B (by LLM-Pruner)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: Scratch 训练 (StableLM-3B) vs. LLaMA-3B（由 LLM-Pruner 提供）'
- en: '| Pruning Ratio | #Param | Latency | BoolQ | PIQA | HellaSwag | WinoGrande
    | ARC-e | ARC-c | OBQA | Average |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 修剪比例 | 参数数量 | 延迟 | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c
    | OBQA | 平均值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| StableLM-3B | 3.6B | 31.69s | 48.78 | 69.48 | 44.52 | 54.62 | 50.93 | 25.17
    | 27.40 | 45.84 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| StableLM-3B | 3.6B | 31.69s | 48.78 | 69.48 | 44.52 | 54.62 | 50.93 | 25.17
    | 27.40 | 45.84 |'
- en: '| LLaMA-3B | 3.6B | 37.96s | 61.41 | 70.08 | 51.01 | 55.01 | 46.80 | 30.38
    | 37.40 | 50.30 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3B | 3.6B | 37.96s | 61.41 | 70.08 | 51.01 | 55.01 | 46.80 | 30.38
    | 37.40 | 50.30 |'
- en: Case Study.
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 案例研究。
- en: 'We provide some examples of sentences generated by the model compressed using
    LLM-Pruner in Table [10](#S4.T10 "Table 10 ‣ Case Study. ‣ 4.3 More Analysis ‣
    4 Experiments ‣ LLM-Pruner: On the Structural Pruning of Large Language Models").
    We made efforts to ensure a minimal overlap between these generated sentences
    and the information contained in the tuning corpus, which demonstrates that the
    information originates from the original model rather than the tuning corpus.
    We provide additional examples in the Appendix, including the generated sentences
    of the model without post-training. From the cases in Table [10](#S4.T10 "Table
    10 ‣ Case Study. ‣ 4.3 More Analysis ‣ 4 Experiments ‣ LLM-Pruner: On the Structural
    Pruning of Large Language Models"), it is evident that the sentences generated
    by the compressed model are comparable to those produced by the original model.
    They exhibit fluency, relevance, and informativeness regarding the given topic.
    Nevertheless, during our experiments, we observed that the pruned model’s performance
    deviates from that of the original model, particularly when generating lengthy
    sentences. Occasionally, it may generate sentences that are meaningless or contain
    repetitive tokens.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格[10](#S4.T10 "Table 10 ‣ Case Study. ‣ 4.3 More Analysis ‣ 4 Experiments
    ‣ LLM-Pruner: On the Structural Pruning of Large Language Models")中提供了一些由模型生成的压缩示例，使用了LLM-Pruner。我们努力确保这些生成的句子与调优语料中的信息之间的重叠最小，这表明信息来源于原始模型而非调优语料。附录中提供了额外的示例，包括没有后训练的模型生成的句子。从表格[10](#S4.T10
    "Table 10 ‣ Case Study. ‣ 4.3 More Analysis ‣ 4 Experiments ‣ LLM-Pruner: On the
    Structural Pruning of Large Language Models")中的案例可以看出，压缩模型生成的句子与原始模型生成的句子相当。它们在流畅性、相关性和信息量方面都表现良好。然而，在我们的实验中，我们观察到剪枝模型的性能偏离了原始模型，特别是在生成较长句子时。有时，它可能生成无意义或包含重复词汇的句子。'
- en: 'Table 10: Some examples of generations obtained with the original model and
    the pruned model.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：原始模型和剪枝模型生成的一些示例。
- en: '| Model | Original Model | Pruned Model |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 原始模型 | 剪枝模型 |'
- en: '| --- | --- | --- |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| LLaMa | The universe is the entirety of space, time, matter, and energy that
    exists. It includes all matter and energy, whether visible or not, and in fact
    the whole universe is invisible. There are many theories regarding its origin
    and ultimate fate… | The universe is the entirety of space, time, matter, and
    energy that exists. The laws that make up physics — electrons orbiting atoms,
    magnets attracting other objects, planets orbiting the Sun—have been consistent
    over … |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| LLaMa | 宇宙是存在的空间、时间、物质和能量的整体。它包括所有物质和能量，无论是否可见，实际上整个宇宙都是不可见的。关于它的起源和最终命运有许多理论...
    | 宇宙是存在的空间、时间、物质和能量的整体。构成物理学的定律——电子绕原子轨道运行、磁铁吸引其他物体、行星绕太阳公转——一直保持一致... |'
- en: '| Vicuna | 10 steps to build an ios app: 1\. Research and develop the idea.
    2\. Decide on the platform you want to use (iOS, Android, or both). 3\. Create
    a plan and budget. 4\. Hire a team of developers. 5\. Design the user interface
    (UI). 6\. Develop the user interface (UI). 7\. Test and iterate on the app. 8\.
    Submit the app to the app store. 9\. Optimize the app for performance and user
    experience (UX). 10\. Market and promote the app. | 10 steps to build an ios app:
    1\. Plan your app: decide what it should do, what it will look like, and what
    it will cost to build it. 2\. Hire a developer: A good iOS developer will help
    you design the user interface, create the app, and make sure it’s bug-free. 3\.
    Create a user interface: This is where the magic happens. Your app’s user interface
    will be designed and mocked up at this point… |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna | 创建iOS应用的10个步骤：1. 研究和开发创意。2. 决定要使用的平台（iOS、Android或两者）。3. 制定计划和预算。4.
    雇佣开发团队。5. 设计用户界面（UI）。6. 开发用户界面（UI）。7. 测试和迭代应用。8. 将应用提交到应用商店。9. 优化应用性能和用户体验（UX）。10.
    市场推广应用。 | 创建iOS应用的10个步骤：1. 计划你的应用：决定它应该做什么、外观如何、以及构建成本。2. 雇佣开发者：优秀的iOS开发者将帮助你设计用户界面、创建应用，并确保其无错误。3.
    创建用户界面：这就是魔法发生的地方。你的应用用户界面将在此阶段设计和模拟... |'
- en: '| ChatGLM | Write a poetry about love and peace. In the midst of life’s hustle
    and bustle, I find your love a soothing peace. It is a feeling that cannot be
    conveyed, A connection that cannot be distanced. With your smile, your eyes, your
    embrace, I feel a sense of belonging that only you can give. It is a love that
    lasts forever, A love that cannot be defeat… | Write a poetry about love and peace.
    Love and peace, two such wonderful things\ A pure and endless desire in my heart\ And
    both of them, I must seek for\ A long, long time, I know..\ Love, I know, is a
    feeling of being\ A perfect partner, in every sense\ And peace, I need it, so
    much, one day\ A long, long way, my heart will go.. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM | 写一首关于爱情与和平的诗。在生活的喧嚣中，我发现你的爱带来了一种宁静的和平。这是一种无法用语言表达的感觉，一种无法被疏远的联系。凭借你的微笑，你的眼神，你的拥抱，我感受到只有你能给予的归属感。这是一种永恒的爱，一种无法被击败的爱…
    | 写一首关于爱情与和平的诗。爱情与和平，两者都是如此美好\ 这是我内心深处纯净而无尽的渴望\ 我必须去追寻这两者\ 我知道，这将是漫长的旅程..\ 爱，我知道，是一种存在的感觉\
    是在每个层面上都完美的伴侣\ 和平，我需要它，非常需要，某一天\ 我的心将走过漫长的道路.. |'
- en: 5 Conclusion
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we propose LLM-Pruner, a structured pruning approach for large
    language models. LLM-Pruner aims to compress sizable language models in a task-agnostic
    manner while minimizing the dependency on the original training corpus and preserving
    the linguistic capabilities of LLMs. LLM-Pruner accomplishes this by iteratively
    examining each neuron within the model as a trigger for identifying dependency
    groups, thereby constructing the LLM’s dependency graph. Subsequently, LLM-Pruner
    assesses the importance of these groups using both parameter-wise and weight-wise
    estimation. Finally, we utilize LoRA for fast recovery and adjustment of the pruned
    model. We evaluate the efficacy of LLM-Pruner on three distinct models—LLaMA,
    Vicuna, and ChatGLM—utilizing various zero-shot datasets. Our experimental results
    indicate that LLM-Pruner successfully prunes the model, reducing computational
    burden while retaining its zero-shot capabilities. Nevertheless, considerable
    performance degradation occurs when employing high pruning rates, such as the
    removal of 50% of LLaMA’s parameters, resulting in a substantial decline in model
    performance. Additionally, we observe instances in which the model generates incoherent
    sentences. Addressing the challenges associated with compressing LLMs at higher
    pruning rates remains a challenging task.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了 LLM-Pruner，一种针对大型语言模型的结构化剪枝方法。LLM-Pruner 旨在以任务无关的方式压缩大型语言模型，同时最小化对原始训练语料库的依赖，并保留
    LLM 的语言能力。LLM-Pruner 通过迭代检查模型中的每个神经元作为识别依赖组的触发器，从而构建 LLM 的依赖图。随后，LLM-Pruner 使用参数级和权重级估计来评估这些组的重要性。最后，我们利用
    LoRA 实现快速恢复和调整剪枝后的模型。我们在三个不同模型——LLaMA、Vicuna 和 ChatGLM——上使用各种零样本数据集评估 LLM-Pruner
    的有效性。我们的实验结果表明，LLM-Pruner 成功地剪枝了模型，降低了计算负担，同时保留了其零样本能力。然而，使用高剪枝率时，如移除 LLaMA 的
    50% 参数，会显著下降模型性能。此外，我们还观察到模型生成不连贯句子的情况。在更高剪枝率下压缩 LLM 面临的挑战仍然是一个困难的任务。
- en: References
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bai et al. [2020] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin
    Jiang, Qun Liu, Michael Lyu, and Irwin King. Binarybert: Pushing the limit of
    bert quantization. *arXiv preprint arXiv:2012.15701*, 2020.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. [2020] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin
    Jiang, Qun Liu, Michael Lyu, 和 Irwin King。Binarybert：推动 BERT 量化的极限。*arXiv 预印本
    arXiv:2012.15701*，2020。
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao,
    and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language.
    In *Thirty-Fourth AAAI Conference on Artificial Intelligence*, 2020.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao,
    和 Yejin Choi。Piqa：在自然语言中推理物理常识。发表于 *第三十四届 AAAI 人工智能会议*，2020。
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901，2020。
- en: 'Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4
    with 90% chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, 和 Eric P. Xing. Vicuna: 一款开源聊天机器人，以 90% 的 ChatGPT 质量给 GPT-4 留下深刻印象，2023年3月。网址
    [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)。'
- en: 'Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *arXiv
    preprint arXiv:2204.02311*, 2022.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann 等人. Palm: 通过路径扩展语言建模。*arXiv 预印本 arXiv:2204.02311*，2022年。'
- en: 'Clark et al. [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty
    of natural yes/no questions. In *Proceedings of the 2019 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies, Volume 1 (Long and Short Papers)*, pages 2924–2936, Minneapolis,
    Minnesota, June 2019\. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300.
    URL [https://aclanthology.org/N19-1300](https://aclanthology.org/N19-1300).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark et al. [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, 和 Kristina Toutanova. BoolQ: 探索自然是/否问题的意外难度。见于 *2019年北美计算语言学协会会议论文集：人类语言技术，第1卷（长篇和短篇论文）*，第2924–2936页，明尼阿波利斯，明尼苏达州，2019年6月。计算语言学协会。doi:
    10.18653/v1/N19-1300。网址 [https://aclanthology.org/N19-1300](https://aclanthology.org/N19-1300)。'
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv:1803.05457v1*, 2018.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, 和 Oyvind Tafjord. 认为你已经解决了问答问题？试试 ARC，AI2 推理挑战。*arXiv:1803.05457v1*，2018年。
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv preprint
    arXiv:2208.07339*, 2022.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    Llm. int8 (): 大规模变换器的 8 位矩阵乘法。*arXiv 预印本 arXiv:2208.07339*，2022年。'
- en: 'Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*, 2018.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    Bert: 语言理解的深度双向变换器预训练。*arXiv 预印本 arXiv:1810.04805*，2018年。'
- en: Fan et al. [2019] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer
    depth on demand with structured dropout. *arXiv preprint arXiv:1909.11556*, 2019.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan et al. [2019] Angela Fan, Edouard Grave, 和 Armand Joulin. 通过结构化丢弃按需减少变换器深度。*arXiv
    预印本 arXiv:1909.11556*，2019年。
- en: 'Fang et al. [2023] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and
    Xinchao Wang. Depgraph: Towards any structural pruning, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fang et al. [2023] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, 和 Xinchao
    Wang. Depgraph: 迈向任何结构的修剪，2023年。'
- en: Frantar and Alistarh [2023] Elias Frantar and Dan Alistarh. Massive language
    models can be accurately pruned in one-shot. *arXiv preprint arXiv:2301.00774*,
    2023.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 和 Alistarh [2023] Elias Frantar 和 Dan Alistarh. 大型语言模型可以准确地一次性修剪。*arXiv
    预印本 arXiv:2301.00774*，2023年。
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan
    Alistarh. Gptq: 生成预训练变换器的准确后训练量化。*arXiv 预印本 arXiv:2210.17323*，2022年。'
- en: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    et al. A framework for few-shot language model evaluation. *Version v0\. 0.1\.
    Sept*, 2021.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff
    等人. 一个少样本语言模型评估框架。*版本 v0.0.1. 9月*，2021年。
- en: Guo et al. [2019] Fu-Ming Guo, Sijia Liu, Finlay S. Mungall, Xue Lin, and Yanzhi
    Wang. Reweighted proximal pruning for large-scale language representation. *CoRR*,
    abs/1909.12486, 2019. URL [http://arxiv.org/abs/1909.12486](http://arxiv.org/abs/1909.12486).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. [2019] 付铭·郭、思佳·刘、芬利·S·芒戈、薛·林和严智·王。针对大规模语言表示的重加权邻近剪枝。*CoRR*，abs/1909.12486，2019年。网址
    [http://arxiv.org/abs/1909.12486](http://arxiv.org/abs/1909.12486)。
- en: Han et al. [2015] Song Han, Jeff Pool, John Tran, and William Dally. Learning
    both weights and connections for efficient neural network. In C. Cortes, N. Lawrence,
    D. Lee, M. Sugiyama, and R. Garnett, editors, *Advances in Neural Information
    Processing Systems*, volume 28. Curran Associates, Inc., 2015. URL [https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf).
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. [2015] 宋·韩、杰夫·普尔、约翰·Tran和威廉·达利。学习权重和连接以实现高效的神经网络。见于C. Cortes、N. Lawrence、D.
    Lee、M. Sugiyama和R. Garnett编辑的*神经信息处理系统进展*，第28卷。Curran Associates, Inc.，2015年。网址
    [https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf)。
- en: Hoffmann et al. [2022] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.
    *arXiv preprint arXiv:2203.15556*, 2022.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann et al. [2022] 乔丹·霍夫曼、塞巴斯蒂安·博尔戈德、亚瑟·门施、埃琳娜·布恰茨卡娅、特雷弗·蔡、伊丽莎·拉瑟福德、迭戈·德·拉斯·卡萨斯、莉莎·安妮·亨德里克斯、约翰内斯·韦布尔、艾丹·克拉克等。训练计算最优的大型语言模型。*arXiv预印本
    arXiv:2203.15556*，2022年。
- en: 'Hou et al. [2020] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen,
    and Qun Liu. Dynabert: Dynamic bert with adaptive width and depth. *Advances in
    Neural Information Processing Systems*, 33:9782–9793, 2020.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hou et al. [2020] 陆·侯、志琦·黄、丽峰·尚、新疆·江、小陈·陈和群·刘。Dynabert: 具有自适应宽度和深度的动态bert。*神经信息处理系统进展*，33:9782–9793，2020年。'
- en: 'Hu et al. [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models, 2021.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. [2021] 爱德华·J·胡、叶龙·申、菲利普·瓦利斯、泽远·艾伦-朱、袁智·李、善·王、陆·王和韦柱·陈。Lora: 大型语言模型的低秩适配，2021年。'
- en: 'Jiao et al. [2020] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
    Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language
    understanding. In *Findings of the Association for Computational Linguistics:
    EMNLP 2020*, pages 4163–4174, 2020.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiao et al. [2020] 小齐·焦、义春·尹、丽峰·尚、新疆·江、小陈·陈、琳琳·李、芳·王和群·刘。Tinybert: 为自然语言理解提炼bert。见于*计算语言学协会的发现：EMNLP
    2020*，第4163–4174页，2020年。'
- en: 'Kurtic et al. [2022] Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar,
    Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal bert
    surgeon: Scalable and accurate second-order pruning for large language models.
    *arXiv preprint arXiv:2203.07259*, 2022.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurtic et al. [2022] 埃尔达·库尔蒂奇、丹尼尔·坎波斯、图安·阮、伊利亚斯·弗兰塔、马克·库尔茨、本杰明·费纳然、迈克尔·戈因和丹·阿利斯塔赫。最佳bert医生：针对大型语言模型的可扩展且准确的二阶剪枝。*arXiv预印本
    arXiv:2203.07259*，2022年。
- en: Kwon et al. [2022] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun,
    Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers.
    *arXiv preprint arXiv:2204.09656*, 2022.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon et al. [2022] 崔宇·权、世勋·金、迈克尔·W·马霍尼、约瑟夫·哈松、库尔特·克茨和阿米尔·戈拉米。用于变换器的快速后训练剪枝框架。*arXiv预印本
    arXiv:2204.09656*，2022年。
- en: 'Lan et al. [2019] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning
    of language representations. *arXiv preprint arXiv:1909.11942*, 2019.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lan et al. [2019] 郎镇中、明达·陈、塞巴斯蒂安·古德曼、凯文·金佩尔、皮尤什·夏尔马和拉杜·索里库特。Albert: 用于自监督语言表示学习的轻量级bert。*arXiv预印本
    arXiv:1909.11942*，2019年。'
- en: LeCun et al. [1989] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage.
    *Advances in neural information processing systems*, 2, 1989.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. [1989] 扬·勒昆、约翰·丹克和萨拉·索拉。最佳脑损伤。*神经信息处理系统进展*，第2卷，1989年。
- en: 'Lewis et al. [2019] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising
    sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. *arXiv preprint arXiv:1910.13461*, 2019.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lewis et al. [2019] 迈克·刘易斯、殷涵·刘、南曼·戈亚尔、马尔詹·加兹文尼贾德、阿卜杜勒拉赫曼·穆罕默德、奥梅尔·莱维、维斯·斯托扬诺夫和卢克·泽特尔莫耶。Bart:
    用于自然语言生成、翻译和理解的去噪序列到序列预训练。*arXiv预印本 arXiv:1910.13461*，2019年。'
- en: Li et al. [2016] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter
    Graf. Pruning filters for efficient convnets. *arXiv preprint arXiv:1608.08710*,
    2016.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2016] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, 和 Hans Peter
    Graf. 为高效卷积网络修剪滤波器。*arXiv预印本 arXiv:1608.08710*，2016。
- en: 'Li et al. [2017] Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and
    Shuzi Niu. Dailydialog: A manually labelled multi-turn dialogue dataset. In *Proceedings
    of The 8th International Joint Conference on Natural Language Processing (IJCNLP
    2017)*, 2017.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2017] Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, 和 Shuzi
    Niu. Dailydialog：一个手动标注的多轮对话数据集。收录于*第八届国际自然语言处理联合会议论文集（IJCNLP 2017）*，2017。
- en: 'Liang et al. [2023] Chen Liang, Haoming Jiang, Zheng Li, Xianfeng Tang, Bin
    Yin, and Tuo Zhao. Homodistil: Homotopic task-agnostic distillation of pre-trained
    transformers. *arXiv preprint arXiv:2302.09632*, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. [2023] Chen Liang, Haoming Jiang, Zheng Li, Xianfeng Tang, Bin
    Yin, 和 Tuo Zhao. Homodistil：预训练变换器的同源任务无关蒸馏。*arXiv预印本 arXiv:2302.09632*，2023。
- en: 'Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:
    A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*,
    2019.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, 和 Veselin Stoyanov. Roberta：一种稳健优化的BERT预训练方法。*arXiv预印本
    arXiv:1907.11692*，2019。
- en: 'Liu et al. [2021] Zejian Liu, Fanrong Li, Gang Li, and Jian Cheng. Ebert: Efficient
    bert inference with dynamic structured pruning. In *Findings of the Association
    for Computational Linguistics: ACL-IJCNLP 2021*, pages 4814–4823, 2021.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2021] Zejian Liu, Fanrong Li, Gang Li, 和 Jian Cheng. Ebert：通过动态结构化剪枝进行高效BERT推理。收录于*计算语言学协会会议论文集：ACL-IJCNLP
    2021*，页码4814–4823，2021。
- en: Ma et al. [2020] Xinyin Ma, Yongliang Shen, Gongfan Fang, Chen Chen, Chenghao
    Jia, and Weiming Lu. Adversarial self-supervised data-free distillation for text
    classification. In *Proceedings of the 2020 Conference on Empirical Methods in
    Natural Language Processing (EMNLP)*, pages 6182–6192, 2020.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. [2020] Xinyin Ma, Yongliang Shen, Gongfan Fang, Chen Chen, Chenghao
    Jia, 和 Weiming Lu. 对抗性自监督无数据蒸馏用于文本分类。收录于*2020年自然语言处理实证方法会议论文集（EMNLP）*，页码6182–6192，2020。
- en: 'Ma et al. [2022] Xinyin Ma, Xinchao Wang, Gongfan Fang, Yongliang Shen, and
    Weiming Lu. Prompting to distill: Boosting data-free knowledge distillation via
    reinforced prompt. *arXiv preprint arXiv:2205.07523*, 2022.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. [2022] Xinyin Ma, Xinchao Wang, Gongfan Fang, Yongliang Shen, 和 Weiming
    Lu. 通过强化提示提升无数据知识蒸馏的效果。*arXiv预印本 arXiv:2205.07523*，2022。
- en: 'Marcus et al. [1993] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
    Building a large annotated corpus of English: The Penn Treebank. *Computational
    Linguistics*, 19(2):313–330, 1993. URL [https://www.aclweb.org/anthology/J93-2004](https://www.aclweb.org/anthology/J93-2004).'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marcus et al. [1993] Mitchell P. Marcus, Beatrice Santorini, 和 Mary Ann Marcinkiewicz.
    构建大型英文标注语料库：宾夕法尼亚树库。*计算语言学*，19(2):313–330，1993。网址 [https://www.aclweb.org/anthology/J93-2004](https://www.aclweb.org/anthology/J93-2004)。
- en: McCarley et al. [2019] JS McCarley, Rishav Chakravarti, and Avirup Sil. Structured
    pruning of a bert-based question answering model. *arXiv preprint arXiv:1910.06360*,
    2019.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCarley et al. [2019] JS McCarley, Rishav Chakravarti, 和 Avirup Sil. BERT基础问题回答模型的结构化剪枝。*arXiv预印本
    arXiv:1910.06360*，2019。
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models, 2016.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher. 指针守卫混合模型，2016。
- en: Mihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. In *EMNLP*, 2018.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, 和 Ashish Sabharwal.
    一套盔甲能导电吗？用于开放书籍问答的新数据集。收录于*EMNLP*，2018。
- en: OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI. Gpt-4技术报告，2023。
- en: 'Pan et al. [2020a] Haojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang
    Li, and Jun Huang. Meta-kd: A meta knowledge distillation framework for language
    model compression across domains. *CoRR*, abs/2012.01266, 2020a. URL [https://arxiv.org/abs/2012.01266](https://arxiv.org/abs/2012.01266).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan et al. [2020a] Haojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang
    Li, 和 Jun Huang. Meta-kd：跨领域语言模型压缩的元知识蒸馏框架。*CoRR*，abs/2012.01266，2020a。网址 [https://arxiv.org/abs/2012.01266](https://arxiv.org/abs/2012.01266)。
- en: 'Pan et al. [2020b] Haojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang
    Li, and Jun Huang. Meta-kd: A meta knowledge distillation framework for language
    model compression across domains. *arXiv preprint arXiv:2012.01266*, 2020b.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pan et al. [2020b] Haojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang
    Li, 和 Jun Huang. 《Meta-kd: 跨领域语言模型压缩的元知识蒸馏框架》。*arXiv 预印本 arXiv:2012.01266*，2020b。'
- en: Rashid et al. [2020] Ahmad Rashid, Vasileios Lioutas, Abbas Ghaddar, and Mehdi
    Rezagholizadeh. Towards zero-shot knowledge distillation for natural language
    processing, 2020.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashid et al. [2020] Ahmad Rashid, Vasileios Lioutas, Abbas Ghaddar, 和 Mehdi
    Rezagholizadeh. 《面向自然语言处理的零样本知识蒸馏》，2020年。
- en: 'Sakaguchi et al. [2019] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale,
    2019.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi et al. [2019] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    和 Yejin Choi. 《Winogrande: 大规模对抗性 Winograd 语法挑战》，2019年。'
- en: 'Scao et al. [2022] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*, 2022.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Scao et al. [2022] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, 等等。 《Bloom: 一个176b参数的开放访问多语言模型》。*arXiv 预印本 arXiv:2211.05100*，2022年。'
- en: Srinivas and Babu [2015] Suraj Srinivas and R Venkatesh Babu. Data-free parameter
    pruning for deep neural networks. *arXiv preprint arXiv:1507.06149*, 2015.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srinivas and Babu [2015] Suraj Srinivas 和 R Venkatesh Babu. 《用于深度神经网络的数据无关参数剪枝》。*arXiv
    预印本 arXiv:1507.06149*，2015年。
- en: Sun et al. [2019] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge
    distillation for bert model compression. *arXiv preprint arXiv:1908.09355*, 2019.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2019] Siqi Sun, Yu Cheng, Zhe Gan, 和 Jingjing Liu. 《用于 Bert 模型压缩的患者知识蒸馏》。*arXiv
    预印本 arXiv:1908.09355*，2019年。
- en: 'Sun et al. [2020a] Siqi Sun, Zhe Gan, Yuwei Fang, Yu Cheng, Shuohang Wang,
    and Jingjing Liu. Contrastive distillation on intermediate representations for
    language model compression. In *Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing (EMNLP)*, pages 498–508, Online, November
    2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.36.
    URL [https://aclanthology.org/2020.emnlp-main.36](https://aclanthology.org/2020.emnlp-main.36).'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. [2020a] Siqi Sun, Zhe Gan, Yuwei Fang, Yu Cheng, Shuohang Wang,
    和 Jingjing Liu. 《用于语言模型压缩的中间表示对比蒸馏》。收录于 *2020年自然语言处理经验方法会议 (EMNLP) 会议论文集*，页码 498–508，在线，2020年11月。计算语言学协会。doi:
    10.18653/v1/2020.emnlp-main.36。网址 [https://aclanthology.org/2020.emnlp-main.36](https://aclanthology.org/2020.emnlp-main.36)。'
- en: 'Sun et al. [2020b] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming
    Yang, and Denny Zhou. Mobilebert: a compact task-agnostic bert for resource-limited
    devices. *arXiv preprint arXiv:2004.02984*, 2020b.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. [2020b] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming
    Yang, 和 Denny Zhou. 《Mobilebert: 一种适用于资源受限设备的紧凑任务无关 Bert》。*arXiv 预印本 arXiv:2004.02984*，2020b。'
- en: 'Taori et al. [2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori et al. [2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. 《斯坦福 Alpaca:
    一个遵循指令的 Llama 模型》。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)，2023年。'
- en: 'Thoppilan et al. [2022] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
    Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,
    Yu Du, et al. Lamda: Language models for dialog applications. *arXiv preprint
    arXiv:2201.08239*, 2022.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thoppilan et al. [2022] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
    Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,
    Yu Du, 等等。 《Lamda: 对话应用的语言模型》。*arXiv 预印本 arXiv:2201.08239*，2022年。'
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等等。 《Llama: 开放且高效的基础语言模型》。*arXiv 预印本 arXiv:2302.13971*，2023年。'
- en: 'Voita et al. [2019] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the
    heavy lifting, the rest can be pruned. In *Proceedings of the 57th Annual Meeting
    of the Association for Computational Linguistics*, pages 5797–5808, 2019.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voita et al. [2019] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich
    和 Ivan Titov. 分析多头自注意力：专门的头部承担重任，其余可以被剪枝。载于 *第57届计算语言学协会年会论文集*，第5797–5808页，2019年。
- en: 'Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding. *arXiv preprint arXiv:1804.07461*, 2018.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer
    Levy 和 Samuel R Bowman. Glue：自然语言理解的多任务基准和分析平台。*arXiv 预印本 arXiv:1804.07461*，2018年。
- en: 'Wang et al. [2019a] Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang.
    Eigendamage: Structured pruning in the kronecker-factored eigenbasis. In *International
    conference on machine learning*, pages 6566–6575\. PMLR, 2019a.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2019a] Chaoqi Wang, Roger Grosse, Sanja Fidler 和 Guodong Zhang.
    Eigendamage：在 Kronecker 因子特征基中进行结构化剪枝。载于 *国际机器学习会议*，第6566–6575页。PMLR，2019a。
- en: 'Wang et al. [2020] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and
    Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression
    of pre-trained transformers. *Advances in Neural Information Processing Systems*,
    33:5776–5788, 2020.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2020] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang 和 Ming
    Zhou. Minilm：用于任务无关压缩的深度自注意力蒸馏。*神经信息处理系统进展*，33:5776–5788，2020年。
- en: Wang et al. [2019b] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning
    of large language models. *arXiv preprint arXiv:1910.04732*, 2019b.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2019b] Ziheng Wang, Jeremy Wohlwend 和 Tao Lei. 大型语言模型的结构化剪枝。*arXiv
    预印本 arXiv:1910.04732*，2019b。
- en: Wei et al. [2022a] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*,
    2022a.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2022a] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler
    等人。大型语言模型的突现能力。*arXiv 预印本 arXiv:2206.07682*，2022a。
- en: Wei et al. [2022b] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large
    language models. *arXiv preprint arXiv:2201.11903*, 2022b.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2022b] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed
    Chi, Quoc Le 和 Denny Zhou。思维链提示引发大型语言模型的推理。*arXiv 预印本 arXiv:2201.11903*，2022b。
- en: 'Wu et al. [2023] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed,
    and Alham Fikri Aji. Lamini-lm: A diverse herd of distilled models from large-scale
    instructions, 2023.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. [2023] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed
    和 Alham Fikri Aji. Lamini-lm：从大规模指令中提取的多样化蒸馏模型，2023年。
- en: Wu et al. [2020] Yiquan Wu, Kun Kuang, Yating Zhang, Xiaozhong Liu, Changlong
    Sun, Jun Xiao, Yueting Zhuang, Luo Si, and Fei Wu. De-biased court’s view generation
    with causality. In *Proceedings of the 2020 Conference on Empirical Methods in
    Natural Language Processing (EMNLP)*, pages 763–780, 2020.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. [2020] Yiquan Wu, Kun Kuang, Yating Zhang, Xiaozhong Liu, Changlong
    Sun, Jun Xiao, Yueting Zhuang, Luo Si 和 Fei Wu. 基于因果关系的去偏见法院观点生成。载于 *2020年自然语言处理经验方法会议（EMNLP）论文集*，第763–780页，2020年。
- en: Xia et al. [2022] Mengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured pruning
    learns compact and accurate models. *arXiv preprint arXiv:2204.00408*, 2022.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia et al. [2022] Mengzhou Xia, Zexuan Zhong 和 Danqi Chen. 结构化剪枝学习紧凑且准确的模型。*arXiv
    预印本 arXiv:2204.00408*，2022年。
- en: 'Xin et al. [2020] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy
    Lin. DeeBERT: Dynamic early exiting for accelerating BERT inference. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics*,
    pages 2246–2251, Online, July 2020. Association for Computational Linguistics.
    doi: 10.18653/v1/2020.acl-main.204. URL [https://aclanthology.org/2020.acl-main.204](https://aclanthology.org/2020.acl-main.204).'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xin et al. [2020] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu 和 Jimmy Lin.
    DeeBERT：动态早期退出以加速 BERT 推断。载于 *第58届计算语言学协会年会论文集*，第2246–2251页，在线，2020年7月。计算语言学协会。doi:
    10.18653/v1/2020.acl-main.204。网址 [https://aclanthology.org/2020.acl-main.204](https://aclanthology.org/2020.acl-main.204)。'
- en: 'Xu et al. [2021] Dongkuan Xu, Ian En-Hsu Yen, Jinxi Zhao, and Zhibin Xiao.
    Rethinking network pruning–under the pre-train and fine-tune paradigm. In *Proceedings
    of the 2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 2376–2382, 2021.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2021] Dongkuan Xu, Ian En-Hsu Yen, Jinxi Zhao, 和 Zhibin Xiao. 重新思考网络剪枝——在预训练和微调范式下。在*2021年北美计算语言学协会会议：人类语言技术会议论文集*，页码
    2376–2382，2021年。
- en: 'Xue et al. [2020] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami
    Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual
    pre-trained text-to-text transformer. *arXiv preprint arXiv:2010.11934*, 2020.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xue et al. [2020] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami
    Al-Rfou, Aditya Siddhant, Aditya Barua, 和 Colin Raffel. mt5: 一个大规模多语言预训练文本到文本变换器。*arXiv
    预印本 arXiv:2010.11934*，2020年。'
- en: 'Yao et al. [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. *Advances in Neural Information Processing
    Systems*, 35:27168–27183, 2022.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, 和 Yuxiong He. Zeroquant: 高效且经济的后训练量化大型变换器。*神经信息处理系统进展*，35:27168–27183，2022年。'
- en: 'Ye et al. [2021] Deming Ye, Yankai Lin, Yufei Huang, and Maosong Sun. TR-BERT:
    Dynamic token reduction for accelerating BERT inference. In *Proceedings of the
    2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 5798–5809, Online, June 2021\.
    Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.463.
    URL [https://aclanthology.org/2021.naacl-main.463](https://aclanthology.org/2021.naacl-main.463).'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye et al. [2021] Deming Ye, Yankai Lin, Yufei Huang, 和 Maosong Sun. TR-BERT:
    动态令牌减少以加速BERT推理。在*2021年北美计算语言学协会会议：人类语言技术会议论文集*，页码 5798–5809，在线，2021年6月。计算语言学协会。doi:
    10.18653/v1/2021.naacl-main.463。网址 [https://aclanthology.org/2021.naacl-main.463](https://aclanthology.org/2021.naacl-main.463)。'
- en: 'Yvinec et al. [2022] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin
    Bailly. Red++: Data-free pruning of deep neural networks via input splitting and
    output merging. *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    45(3):3664–3676, 2022.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yvinec et al. [2022] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, 和 Kevin
    Bailly. Red++: 通过输入拆分和输出合并进行数据自由的深度神经网络剪枝。*IEEE模式分析与机器智能期刊*，45(3):3664–3676，2022年。'
- en: 'Zafrir et al. [2019] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
    Q8bert: Quantized 8bit bert. In *2019 Fifth Workshop on Energy Efficient Machine
    Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)*, pages 36–39\. IEEE,
    2019.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zafrir et al. [2019] Ofir Zafrir, Guy Boudoukh, Peter Izsak, 和 Moshe Wasserblat.
    Q8bert: 量化的8位BERT。在*2019年第五届节能机器学习与认知计算研讨会-NeurIPS版（EMC2-NIPS）*，页码 36–39。IEEE，2019年。'
- en: 'Zafrir et al. [2021] Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, and
    Moshe Wasserblat. Prune once for all: Sparse pre-trained language models. *arXiv
    preprint arXiv:2111.05754*, 2021.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zafrir et al. [2021] Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, 和
    Moshe Wasserblat. 一次剪枝到底：稀疏预训练语言模型。*arXiv 预印本 arXiv:2111.05754*，2021年。
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics*,
    2019.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    和 Yejin Choi. Hellaswag: 机器真的能完成你的句子吗？在*第57届计算语言学协会年会论文集*，2019年。'
- en: 'Zeng et al. [2022] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open
    bilingual pre-trained model. *arXiv preprint arXiv:2210.02414*, 2022.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng et al. [2022] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, 等。Glm-130b: 一个开放的双语预训练模型。*arXiv
    预印本 arXiv:2210.02414*，2022年。'
- en: 'Zhu et al. [2015] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov,
    Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies:
    Towards story-like visual explanations by watching movies and reading books. In
    *The IEEE International Conference on Computer Vision (ICCV)*, December 2015.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2015] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel
    Urtasun, Antonio Torralba, 和 Sanja Fidler. 对齐书籍与电影：通过观看电影和阅读书籍实现故事般的视觉解释。在*IEEE国际计算机视觉大会（ICCV）*，2015年12月。
- en: Appendix A Detailed Explanations for the Dependency Rules
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 依赖规则的详细解释
- en: '![Refer to caption](img/58a51aa6b6e00faec11e33d9e6cca318.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/58a51aa6b6e00faec11e33d9e6cca318.png)'
- en: 'Figure 6: Illustrations of the two dependency rules. All the cases are extracted
    from the multi-head attention module.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '图6: 两个依赖规则的示意图。所有案例均提取自多头注意力模块。'
- en: 'We provide a detailed explanation of the two dependency rules. It is important
    to note that these dependency rules do not pertain solely to the forward computation.
    Instead, they represent directional relationships that exist in both directions.
    For instance, removing a node in a subsequent layer may also result in the pruning
    of a node in the preceding layer. Recall the two dependency rules as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了两个依赖规则的详细解释。值得注意的是，这些依赖规则不仅仅涉及前向计算。相反，它们表示双向存在的方向关系。例如，移除后续层的节点也可能导致前一层的节点被剪枝。回顾以下两个依赖规则：
- en: '|  | $N_{j}\in\operatorname{Out}(N_{i})\wedge\operatorname{Deg}^{-}(N_{j})=1\Rightarrow
    N_{j}\text{ is dependent on }N_{i}$ |  | (7) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $N_{j}\in\operatorname{Out}(N_{i})\wedge\operatorname{Deg}^{-}(N_{j})=1\Rightarrow
    N_{j}\text{ 依赖于 }N_{i}$ |  | (7) |'
- en: '|  | $N_{i}\in\operatorname{In}(N_{j})\wedge\operatorname{Deg}^{+}(N_{i})=1\Rightarrow
    N_{i}\text{ is dependent on }N_{j}$ |  | (8) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $N_{i}\in\operatorname{In}(N_{j})\wedge\operatorname{Deg}^{+}(N_{i})=1\Rightarrow
    N_{i}\text{ 依赖于 }N_{j}$ |  | (8) |'
- en: where $N_{i}$.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N_{i}$。
- en: 'Figure [6](#A1.F6 "Figure 6 ‣ Appendix A Detailed Explanations for the Dependency
    Rules ‣ LLM-Pruner: On the Structural Pruning of Large Language Models") serves
    as an illustration of the two dependency rules:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [6](#A1.F6 "图6 ‣ 附录A 依赖规则的详细解释 ‣ LLM-Pruner: 大型语言模型的结构剪枝") 作为两个依赖规则的示意图：'
- en: •
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'In case 1, Node I and Node J satisfy the rule stated in Eq.[7](#A1.E7 "In Appendix
    A Detailed Explanations for the Dependency Rules ‣ LLM-Pruner: On the Structural
    Pruning of Large Language Models"). Consequently, Node J depends on Node I. When
    Node I is pruned, it is necessary to prune Node J as well.'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '在案例1中，节点I和节点J满足公式[7](#A1.E7 "附录A 依赖规则的详细解释 ‣ LLM-Pruner: 大型语言模型的结构剪枝")中所述的规则。因此，节点J依赖于节点I。当节点I被剪枝时，也需要剪枝节点J。'
- en: •
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'In case 2, Node I and Node J satisfy the rule Eq.[8](#A1.E8 "In Appendix A
    Detailed Explanations for the Dependency Rules ‣ LLM-Pruner: On the Structural
    Pruning of Large Language Models"). Thus, Node I is dependent on Node J. If Node
    J is pruned, it becomes imperative to prune Node I as well.'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '在案例2中，节点I和节点J满足公式[8](#A1.E8 "附录A 依赖规则的详细解释 ‣ LLM-Pruner: 大型语言模型的结构剪枝")中的规则。因此，节点I依赖于节点J。如果节点J被剪枝，那么节点I也必须剪枝。'
- en: •
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'In case 3, Node J and Node K do not meet the requirement of Eq.[7](#A1.E7 "In
    Appendix A Detailed Explanations for the Dependency Rules ‣ LLM-Pruner: On the
    Structural Pruning of Large Language Models") due to the mismatch in $\operatorname{Deg}^{-}(N_{k})\neq
    1$. Thus, with Node J pruned, Node K would not be affected.'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '在案例3中，节点J和节点K由于$\operatorname{Deg}^{-}(N_{k})\neq 1$不符合公式[7](#A1.E7 "附录A 依赖规则的详细解释
    ‣ LLM-Pruner: 大型语言模型的结构剪枝")的要求。因此，节点J被剪枝时，节点K不会受到影响。'
- en: Appendix B Implementation Details
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 实施细节
- en: B.1 For Pruning
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 关于剪枝
- en: For the baseline
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对于基线
- en: 'Given the lack of previous work on the structural pruning of Large Language
    Models in a task-agnostic and low-resource setting, there is currently no existing
    baseline for our model. To provide a comprehensive demonstration of the effectiveness
    of LLM-Pruner, we employ two additional methods for evaluation, alongside the
    data-free pruning method. All of these methods are built upon the dependent groups
    identified in Section [3.1](#S3.SS1 "3.1 Discover All Coupled Structure in LLMs
    ‣ 3 Methods ‣ LLM-Pruner: On the Structural Pruning of Large Language Models"):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '由于缺乏关于在任务无关和低资源设置下的大型语言模型结构剪枝的先前研究，目前没有我们模型的现有基线。为了全面展示LLM-Pruner的有效性，我们在数据无关剪枝方法之外，采用了两种额外的评估方法。所有这些方法都建立在第[3.1](#S3.SS1
    "3.1 发现LLMs中的所有耦合结构 ‣ 3 方法 ‣ LLM-Pruner: 大型语言模型的结构剪枝")节中识别的依赖组基础上：'
- en: •
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'L2: We assess the importance of each group based on the magnitude of its weight
    matrix.'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'L2: 我们根据每个组的权重矩阵的大小评估其重要性。'
- en: •
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Random: This method involves randomly selecting certain groups for pruning.'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '随机: 这种方法涉及随机选择某些组进行剪枝。'
- en: For the ‘Block’ Group.
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对于‘Block’组。
- en: 'Based on the findings presented in Table [3](#S4.F3 "Figure 3 ‣ Ablation: Impact
    of Importance Estimation. ‣ 4.2 Zero-shot Performance ‣ 4 Experiments ‣ LLM-Pruner:
    On the Structural Pruning of Large Language Models"), it is preferable to leave
    the first three layers and the final layer unchanged, as modifying parameters
    in those layers significantly impacts the model. Within each module, such as the
    MLP or the Multi-head Attention, the discovered groups are pruned based on a pre-set
    ratio. For instance, in the MLP layer of LLaMA-7B, we identified 11,008 groups,
    and with a 25% pruning ratio, the module would prune 2,752 groups. It is worth
    noting that the pruning rate for the selected groups is higher than the pruning
    ratio for the parameters, as certain layers (e.g., the embedding layer and excluded
    layers mentioned) retain their parameters. When aiming for a parameter pruning
    ratio of 20%, we prune 25% from Layer 5 to Layer 30\. Similarly, for a 50% parameter
    removal, we prune 60% of the groups from Layer 4 to Layer 30.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '根据表 [3](#S4.F3 "图 3 ‣ 消融: 重要性估计的影响。 ‣ 4.2 零-shot 性能 ‣ 4 实验 ‣ LLM-Pruner: 关于大语言模型的结构剪枝")
    中呈现的发现，最好保留前三层和最后一层不变，因为修改这些层的参数会显著影响模型。在每个模块中，例如 MLP 或 Multi-head Attention，发现的组按预设比例进行剪枝。例如，在
    LLaMA-7B 的 MLP 层中，我们识别了 11,008 个组，按照 25% 的剪枝比例，模块将剪枝 2,752 个组。值得注意的是，所选组的剪枝率高于参数的剪枝比例，因为某些层（例如嵌入层和提到的排除层）保留其参数。当目标参数剪枝比例为
    20% 时，我们从第 5 层到第 30 层剪枝 25%。类似地，对于 50% 的参数移除，我们从第 4 层到第 30 层剪枝 60% 的组。'
- en: For the ‘Channel’ Group.
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对于 ‘Channel’ 组。
- en: The Group ’Channel’ exhibits a resemblance to dimension pruning in the model,
    targeting the pruning of certain dimensions. In the case of the Query, Key, and
    Value projection in MHA, only the input dimension is pruned, while for the Output
    projection in MHA, only the output dimension is pruned. It is important to note
    that the entire dependency is established automatically, without any manual design
    involved. The ‘Channel’ group operates in a complementary manner to the ‘Block
    Group’. In the ‘Channel’ Group, the pruning ratio of the group equals to the pruning
    ratio of the parameters, as all weight matrices, including the embedding matrix,
    undergo pruning. Therefore, a 20% pruning ratio of parameters implies pruning
    20% of the groups, while a 50% pruning ratio implies pruning 50% of the groups.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ‘Channel’ 组类似于模型中的维度剪枝，旨在剪枝某些维度。在 MHA 中的 Query、Key 和 Value 投影中，仅剪枝输入维度，而在 MHA
    中的 Output 投影中，仅剪枝输出维度。需要注意的是，整个依赖关系是自动建立的，无需手动设计。‘Channel’ 组与 ‘Block Group’ 以互补的方式运行。在
    ‘Channel’ 组中，组的剪枝比例等于参数的剪枝比例，因为所有权重矩阵，包括嵌入矩阵，都进行剪枝。因此，20% 的参数剪枝比例意味着剪枝 20% 的组，而
    50% 的剪枝比例则意味着剪枝 50% 的组。
- en: B.2 For Recovery Stage
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 恢复阶段
- en: 'Table 11: Ablation: Tuning different modules in the recovery stage'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：消融：恢复阶段调整不同模块
- en: '| Module | WikiText $\color[rgb]{0,.5,.5}\downarrow$ |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 模块 | WikiText $\color[rgb]{0,.5,.5}\downarrow$ |'
- en: '| --- | --- | --- |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| ALL | 17.36 | 29.99 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| ALL | 17.36 | 29.99 |'
- en: '| - MLP | 17.64 | 30.63 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| - MLP | 17.64 | 30.63 |'
- en: '| - MHA | 17.62 | 30.23 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| - MHA | 17.62 | 30.23 |'
- en: 'We follow Hu et al. ([2021](#bib.bib19)) in our recovery stage. We set the
    rank $d$ to 8 in our experiment. The learning rate is set to 1e-4 with 100 warming
    steps. The batch size of training is selected from {64, 128} and the AdamW optimizer
    is employed in our experiment. The best training epoch we found is 2 epochs, as
    training with more epochs even has a negative impact on the model performance.
    We run our experiment on a single GPU with 24GB memory, using approximately 2.5
    hours if RTX4090 is utilized. All the linear module is taken into account for
    efficient tuning. An ablation experiment for this is shown in Table [11](#A2.T11
    "Table 11 ‣ B.2 For Recovery Stage ‣ Appendix B Implementation Details ‣ LLM-Pruner:
    On the Structural Pruning of Large Language Models").'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在恢复阶段遵循了 Hu 等人（[2021](#bib.bib19)）的方法。我们在实验中将等级 $d$ 设置为 8。学习率设置为 1e-4，并使用
    100 个预热步骤。训练的批量大小从 {64, 128} 中选择，并在实验中采用 AdamW 优化器。我们发现最佳的训练周期为 2 个周期，因为更多的周期甚至会对模型性能产生负面影响。我们在具有
    24GB 内存的单个 GPU 上运行实验，如果使用 RTX4090，大约需要 2.5 小时。所有线性模块都考虑在内，以实现高效调整。相关的消融实验见表 [11](#A2.T11
    "表 11 ‣ B.2 For Recovery Stage ‣ 附录 B 实现细节 ‣ LLM-Pruner: 关于大语言模型的结构剪枝")。'
- en: Appendix C More Analysis
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 更多分析
- en: C.1 More Data for Recovery
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 更多数据用于恢复
- en: 'Despite our primary experiments being conducted using 50k samples, we remain
    convinced that the inclusion of additional data could substantially enhance the
    recovery process, albeit at a considerably higher computational cost. Consequently,
    we conduct an experiment aimed at model recovery with more data, employing a dataset
    comprising 2.59 million samples Wu et al. ([2023](#bib.bib57)). The results are
    detailed in Table [12](#A3.T12 "Table 12 ‣ C.1 More Data for Recovery ‣ Appendix
    C More Analysis ‣ LLM-Pruner: On the Structural Pruning of Large Language Models").
    From the results, it is evident that the performance of the compressed model closely
    approximates that of the base model, exhibiting only a marginal performance decrease
    of 0.89%.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管我们的主要实验是在 50k 样本上进行的，但我们仍然相信，包含额外的数据可以显著提升恢复过程，尽管这会带来相当高的计算成本。因此，我们进行了一项实验，旨在使用更多数据进行模型恢复，使用的数据集包含
    2.59 万个样本 Wu 等人 ([2023](#bib.bib57))。结果详细见表格 [12](#A3.T12 "Table 12 ‣ C.1 More
    Data for Recovery ‣ Appendix C More Analysis ‣ LLM-Pruner: On the Structural Pruning
    of Large Language Models")。结果显示，压缩模型的性能与基础模型非常接近，仅有 0.89% 的微小性能下降。'
- en: 'Table 12: Model Recovery: 50k samples vs. 2.59M samples'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12: 模型恢复：50k 样本与 2.59M 样本'
- en: '| Model | #Samples | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c
    | OBQA | Average |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | #样本 | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA |
    平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-7B | - | 73.18 | 78.35 | 72.99 | 67.01 | 67.45 | 41.38 | 42.40 | 63.25
    |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | - | 73.18 | 78.35 | 72.99 | 67.01 | 67.45 | 41.38 | 42.40 | 63.25
    |'
- en: '| LLaMA-5.4B | 50k Taori et al. ([2023](#bib.bib47)) | 64.62 | 77.20 | 68.80
    | 63.14 | 64.31 | 36.77 | 39.80 | 59.23 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-5.4B | 50k Taori 等人 ([2023](#bib.bib47)) | 64.62 | 77.20 | 68.80 |
    63.14 | 64.31 | 36.77 | 39.80 | 59.23 |'
- en: '| LLaMA-5.4B | 2.59M Wu et al. ([2023](#bib.bib57)) | 76.57 | 77.37 | 66.60
    | 65.82 | 70.62 | 40.70 | 38.80 | 62.36 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-5.4B | 2.59M Wu 等人 ([2023](#bib.bib57)) | 76.57 | 77.37 | 66.60 | 65.82
    | 70.62 | 40.70 | 38.80 | 62.36 |'
- en: C.2 Pruning vs. Quantization
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 剪枝与量化
- en: Here, we conduct a comparative analysis of different compression techniques
    and illustrate that these techniques can be effectively combined with little performance
    degradation. We have chosen LLM.int8() Dettmers et al. ([2022](#bib.bib8)) as
    a representative example of quantization methods. Our results show that LLM.int8()
    outperforms LLM-Pruner while LLM-Pruner enhances latency, reduces parameter size.
    When these two techniques are applied in tandem, they collectively reduce memory
    consumption and expedite inference, offering a balanced approach that combines
    the benefits of both methods.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们对不同的压缩技术进行了比较分析，并展示了这些技术可以有效地结合使用，性能降幅很小。我们选择了 LLM.int8() Dettmers 等人
    ([2022](#bib.bib8)) 作为量化方法的代表示例。我们的结果表明，LLM.int8() 优于 LLM-Pruner，而 LLM-Pruner
    提升了延迟，减少了参数大小。当这两种技术同时应用时，它们共同减少了内存消耗并加快了推理速度，提供了一种结合了两种方法优点的平衡方法。
- en: 'Table 13: Pruning and Quantization on LLaMA-7B'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '表 13: LLaMA-7B 上的剪枝与量化'
- en: '| Pruning Ratio | #Param | Memory | Latency | BoolQ | PIQA | HellaSwag | WinoGrande
    | ARC-e | ARC-c | OBQA | Average |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝比例 | #Param | 内存 | 延迟 | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e |
    ARC-c | OBQA | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-7B | 6.74B | 12884.5MiB | 69.32s | 73.18 | 78.35 | 72.99 | 67.01 |
    67.45 | 41.38 | 42.40 | 63.25 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | 6.74B | 12884.5MiB | 69.32s | 73.18 | 78.35 | 72.99 | 67.01 |
    67.45 | 41.38 | 42.40 | 63.25 |'
- en: '| LLM.int8() | 6.74B | 6777.7MiB | 76.20s | 73.36 | 78.18 | 73.01 | 66.93 |
    67.47 | 40.87 | 41.80 | 63.09 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int8() | 6.74B | 6777.7MiB | 76.20s | 73.36 | 78.18 | 73.01 | 66.93 |
    67.47 | 40.87 | 41.80 | 63.09 |'
- en: '| LLaMA-5.4B | 5.47B | 10488.4MiB | 58.55s | 76.57 | 77.37 | 66.60 | 65.82
    | 70.62 | 40.70 | 38.80 | 62.36 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-5.4B | 5.47B | 10488.4MiB | 58.55s | 76.57 | 77.37 | 66.60 | 65.82
    | 70.62 | 40.70 | 38.80 | 62.36 |'
- en: '| LLaMA-5.4B + LLM.int8() | 5.47B | 5444.37MiB | 63.10s | 76.39 | 76.71 | 66.62
    | 66.46 | 70.54 | 40.19 | 39.20 | 62.30 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-5.4B + LLM.int8() | 5.47B | 5444.37MiB | 63.10s | 76.39 | 76.71 | 66.62
    | 66.46 | 70.54 | 40.19 | 39.20 | 62.30 |'
- en: C.3 Global Pruning vs. Local Pruning
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 全球剪枝与局部剪枝
- en: we present a comparative analysis between global pruning and local pruning,
    where the pruning ratio is 20% and the base model is LLaMA-7B. Global pruning
    refers to ranking all groups in the model together, whereas local pruning involves
    only ranking groups within the same module for pruning. The outcome of global
    pruning leads to varying widths across different layers and modules, whereas local
    pruning ensures uniformity across all layers.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对全局修剪和本地修剪进行了比较分析，其中修剪比例为 20%，基础模型为 LLaMA-7B。全局修剪指的是对模型中的所有组进行排名，而本地修剪仅涉及对同一模块内的组进行排名以进行修剪。全局修剪的结果导致不同层和模块之间的宽度变化，而本地修剪则确保了所有层之间的一致性。
- en: Based on our experimental findings, we observed a slight advantage of local
    pruning over global pruning. We think this is because of the varying magnitudes
    in different layers or modules, which makes the importance scores incomparable
    between groups across different layers.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的实验结果，我们观察到本地修剪相较于全局修剪有轻微优势。我们认为这是因为不同层或模块的幅度不同，使得不同层之间的评分不可比。
- en: 'Table 14: Results of global pruning and local pruning'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：全局修剪和本地修剪的结果
- en: '| Method | WikiText2 $\color[rgb]{0,.5,.5}\downarrow$ | BoolQ | PIQA | HellaSwag
    | WinoGrande | ARC-e | ARC-c | OBQA | Average |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WikiText2 $\color[rgb]{0,.5,.5}\downarrow$ | BoolQ | PIQA | HellaSwag
    | WinoGrande | ARC-e | ARC-c | OBQA | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| $\text{Element}^{1}$ - local | 19.09 | 34.21 | 57.06 | 75.68 | 66.80 | 59.83
    | 60.94 | 36.52 | 40.00 | 56.69 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| $\text{Element}^{1}$ - 本地 | 19.09 | 34.21 | 57.06 | 75.68 | 66.80 | 59.83
    | 60.94 | 36.52 | 40.00 | 56.69 |'
- en: '| $\text{Element}^{1}$ - global | 20.84 | 32.86 | 63.15 | 73.23 | 63.31 | 66.38
    | 55.85 | 35.49 | 38.00 | 56.49 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| $\text{Element}^{1}$ - 全局 | 20.84 | 32.86 | 63.15 | 73.23 | 63.31 | 66.38
    | 55.85 | 35.49 | 38.00 | 56.49 |'
- en: C.4 Overfitting Phenomena in Post-Training
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4 后训练的过拟合现象
- en: 'We present a comprehensive analysis of the overfitting issue in the recovery
    stage, as previously mentioned in Figure [5](#S4.F5 "Figure 5 ‣ Channel Strategy
    vs. Block Strategy. ‣ 4.2 Zero-shot Performance ‣ 4 Experiments ‣ LLM-Pruner:
    On the Structural Pruning of Large Language Models"). Here the results cover all
    9 datasets across various training steps. Based on the findings presented in Table
    [15](#A3.T15 "Table 15 ‣ C.4 Overfitting Phenomena in Post-Training ‣ Appendix
    C More Analysis ‣ LLM-Pruner: On the Structural Pruning of Large Language Models"),
    a noticeable trend emerges: the accuracy or generation quality initially shows
    improvement but subsequently experiences a slight decline. This pattern suggests
    that the recovery process is completed within a short period. And given that the
    training corpus is domain-constrained, more training epochs can result in overfitting
    to the specific dataset while potentially compromising the original capabilities
    of the language model.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对恢复阶段的过拟合问题进行了全面分析，如图 [5](#S4.F5 "Figure 5 ‣ Channel Strategy vs. Block Strategy.
    ‣ 4.2 Zero-shot Performance ‣ 4 Experiments ‣ LLM-Pruner: On the Structural Pruning
    of Large Language Models") 中所述。这里的结果涵盖了所有 9 个数据集的各个训练步骤。根据表 [15](#A3.T15 "Table
    15 ‣ C.4 Overfitting Phenomena in Post-Training ‣ Appendix C More Analysis ‣ LLM-Pruner:
    On the Structural Pruning of Large Language Models") 中的发现，出现了一个明显的趋势：准确率或生成质量最初有所改善，但随后略有下降。这种模式表明恢复过程在短时间内完成。考虑到训练语料库是领域约束的，更多的训练周期可能导致对特定数据集的过拟合，同时可能会损害语言模型的原始能力。'
- en: 'Table 15: The PPL and Accuracy on different training steps'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 表 15：不同训练步骤的 PPL 和准确率
- en: '| Step | WikiText2 $\color[rgb]{0,.5,.5}\downarrow$ | BoolQ | PIQA | HellaSwag
    | WinoGrande | ARC-e | ARC-c | OBQA | Average |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 步骤 | WikiText2 $\color[rgb]{0,.5,.5}\downarrow$ | BoolQ | PIQA | HellaSwag
    | WinoGrande | ARC-e | ARC-c | OBQA | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 19.09 | 34.21 | 57.06 | 75.68 | 66.80 | 59.83 | 60.94 | 36.52 | 40.00
    | 56.69 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 19.09 | 34.21 | 57.06 | 75.68 | 66.80 | 59.83 | 60.94 | 36.52 | 40.00
    | 56.69 |'
- en: '| 200 | 18.10 | 30.66 | 64.62 | 77.20 | 68.80 | 63.14 | 64.31 | 36.77 | 39.80
    | 59.24 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 18.10 | 30.66 | 64.62 | 77.20 | 68.80 | 63.14 | 64.31 | 36.77 | 39.80
    | 59.24 |'
- en: '| 400 | 17.69 | 30.26 | 63.00 | 76.66 | 68.75 | 63.54 | 64.39 | 37.20 | 40.60
    | 59.16 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 400 | 17.69 | 30.26 | 63.00 | 76.66 | 68.75 | 63.54 | 64.39 | 37.20 | 40.60
    | 59.16 |'
- en: '| 600 | 17.69 | 30.57 | 66.24 | 76.28 | 68.52 | 63.85 | 64.48 | 37.37 | 41.00
    | 59.68 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 600 | 17.69 | 30.57 | 66.24 | 76.28 | 68.52 | 63.85 | 64.48 | 37.37 | 41.00
    | 59.68 |'
- en: '| 800 | 17.64 | 30.57 | 65.05 | 76.22 | 68.38 | 63.77 | 63.64 | 37.29 | 40.80
    | 59.31 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 800 | 17.64 | 30.57 | 65.05 | 76.22 | 68.38 | 63.77 | 63.64 | 37.29 | 40.80
    | 59.31 |'
- en: '| 1000 | 17.67 | 30.60 | 66.39 | 76.17 | 68.24 | 64.17 | 63.05 | 37.37 | 41.60
    | 59.57 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 17.67 | 30.60 | 66.39 | 76.17 | 68.24 | 64.17 | 63.05 | 37.37 | 41.60
    | 59.57 |'
- en: '| 1200 | 17.74 | 30.75 | 65.75 | 76.28 | 68.28 | 63.77 | 63.30 | 37.63 | 41.20
    | 59.46 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 1200 | 17.74 | 30.75 | 65.75 | 76.28 | 68.28 | 63.77 | 63.30 | 37.63 | 41.20
    | 59.46 |'
- en: '| 1400 | 17.88 | 30.85 | 64.34 | 76.28 | 68.31 | 63.85 | 63.47 | 37.80 | 41.20
    | 59.32 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 1400 | 17.88 | 30.85 | 64.34 | 76.28 | 68.31 | 63.85 | 63.47 | 37.80 | 41.20
    | 59.32 |'
- en: C.5 Pruning with Large Rates
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.5 大比例剪枝
- en: Additionally, we conducted tests on LLaMA-7B and Vicuna-7B with 50% parameters
    pruned. We observe a significant decrease in performance compared to the base
    model. However, the recovery stage proved to be beneficial, resulting in an improvement
    of approximately 7.39%. Pruning a Language Model with such a high pruning rate
    remains a challenging task.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在LLaMA-7B和Vicuna-7B上进行了50%参数剪枝的测试。我们观察到与基础模型相比，性能显著下降。然而，恢复阶段证明是有益的，结果提高了约7.39%。剪枝如此高比例的语言模型仍然是一项具有挑战性的任务。
- en: 'Table 16: The PPL and Accuracy on LLaMA-7B'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 表 16：LLaMA-7B上的PPL和准确率
- en: '| Pruned Model | Method | WikiText2 $\color[rgb]{0,.5,.5}\downarrow$ | BoolQ
    | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA | Average |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝模型 | 方法 | WikiText2 $\color[rgb]{0,.5,.5}\downarrow$ | BoolQ | PIQA | HellaSwag
    | WinoGrande | ARC-e | ARC-c | OBQA | 平均值 |'
- en: '| Ratio = 50% w/o tune | l2 | 39266.42 | 48867.85 | 55.11 | 53.59 | 27.03 |
    49.49 | 26.43 | 29.01 | 34.40 | 39.29 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 比率 = 50% 无调优 | l2 | 39266.42 | 48867.85 | 55.11 | 53.59 | 27.03 | 49.49 |
    26.43 | 29.01 | 34.40 | 39.29 |'
- en: '| random | 3887.90 | 4337.27 | 46.79 | 53.37 | 27.50 | 50.59 | 28.07 | 27.90
    | 30.00 | 37.75 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 3887.90 | 4337.27 | 46.79 | 53.37 | 27.50 | 50.59 | 28.07 | 27.90 |
    30.00 | 37.75 |'
- en: '| Channel | 13891.92 | 16114.91 | 40.37 | 52.18 | 25.72 | 48.86 | 25.72 | 28.24
    | 30.40 | 35.93 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 通道 | 13891.92 | 16114.91 | 40.37 | 52.18 | 25.72 | 48.86 | 25.72 | 28.24
    | 30.40 | 35.93 |'
- en: '| Vector | 141.06 | 236.24 | 62.17 | 55.11 | 27.25 | 49.88 | 29.00 | 25.77
    | 34.00 | 40.45 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 向量 | 141.06 | 236.24 | 62.17 | 55.11 | 27.25 | 49.88 | 29.00 | 25.77 | 34.00
    | 40.45 |'
- en: '| $\text{Element}^{2}$ | 106.07 | 266.65 | 52.57 | 60.45 | 35.86 | 49.01 |
    32.83 | 25.51 | 34.80 | 41.58 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| $\text{Element}^{2}$ | 106.07 | 266.65 | 52.57 | 60.45 | 35.86 | 49.01 |
    32.83 | 25.51 | 34.80 | 41.58 |'
- en: '| $\text{Element}^{1}$ | 112.44 | 255.38 | 52.32 | 59.63 | 35.64 | 53.20 |
    33.50 | 27.22 | 33.40 | 42.13 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| $\text{Element}^{1}$ | 112.44 | 255.38 | 52.32 | 59.63 | 35.64 | 53.20 |
    33.50 | 27.22 | 33.40 | 42.13 |'
- en: '| Ratio = 50% w/ tune | Channel | 1122.15 | 1092.26 | 40.76 | 54.84 | 26.94
    | 49.41 | 27.86 | 25.43 | 32.20 | 36.77 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 比率 = 50% 有调优 | 通道 | 1122.15 | 1092.26 | 40.76 | 54.84 | 26.94 | 49.41 | 27.86
    | 25.43 | 32.20 | 36.77 |'
- en: '| Vector | 43.47 | 68.51 | 62.11 | 64.96 | 40.52 | 51.54 | 46.38 | 28.33 |
    32.40 | 46.61 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 向量 | 43.47 | 68.51 | 62.11 | 64.96 | 40.52 | 51.54 | 46.38 | 28.33 | 32.40
    | 46.61 |'
- en: '| $\text{Element}^{2}$ | 45.70 | 69.33 | 61.47 | 68.82 | 47.56 | 55.09 | 46.46
    | 28.24 | 35.20 | 48.98 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| $\text{Element}^{2}$ | 45.70 | 69.33 | 61.47 | 68.82 | 47.56 | 55.09 | 46.46
    | 28.24 | 35.20 | 48.98 |'
- en: '| $\text{Element}^{1}$ | 38.12 | 66.35 | 60.28 | 69.31 | 47.06 | 53.43 | 45.96
    | 29.18 | 35.60 | 48.69 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| $\text{Element}^{1}$ | 38.12 | 66.35 | 60.28 | 69.31 | 47.06 | 53.43 | 45.96
    | 29.18 | 35.60 | 48.69 |'
- en: 'Table 17: The PPL and Accuracy on Vicuna-7B'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17：Vicuna-7B上的PPL和准确率
- en: '| Pruned Model | Method | WikiText2 $\color[rgb]{0,.5,.5}\downarrow$ | BoolQ
    | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA | Average |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝模型 | 方法 | WikiText2 $\color[rgb]{0,.5,.5}\downarrow$ | BoolQ | PIQA | HellaSwag
    | WinoGrande | ARC-e | ARC-c | OBQA | 平均值 |'
- en: '| Ratio = 0% | Vicuna-7B | 16.11 | 61.37 | 76.57 | 77.75 | 70.64 | 67.40 |
    65.11 | 41.21 | 40.80 | 62.78 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 比率 = 0% | Vicuna-7B | 16.11 | 61.37 | 76.57 | 77.75 | 70.64 | 67.40 | 65.11
    | 41.21 | 40.80 | 62.78 |'
- en: '| Ratio = 50% w/o tune | l2 | 54516.03 | 66274.63 | 45.99 | 53.48 | 26.55 |
    47.83 | 27.53 | 28.58 | 30.40 | 37.19 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 比率 = 50% 无调优 | l2 | 54516.03 | 66274.63 | 45.99 | 53.48 | 26.55 | 47.83 |
    27.53 | 28.58 | 30.40 | 37.19 |'
- en: '| random | 17020.73 | 13676.54 | 48.17 | 53.43 | 27.31 | 50.43 | 26.30 | 29.78
    | 30.20 | 37.95 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 17020.73 | 13676.54 | 48.17 | 53.43 | 27.31 | 50.43 | 26.30 | 29.78
    | 30.20 | 37.95 |'
- en: '| Channel | 8360.30 | 10651.30 | 38.69 | 53.10 | 26.42 | 50.20 | 25.97 | 29.52
    | 29.60 | 36.22 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 通道 | 8360.30 | 10651.30 | 38.69 | 53.10 | 26.42 | 50.20 | 25.97 | 29.52 |
    29.60 | 36.22 |'
- en: '| Vector | 189.87 | 409.75 | 62.14 | 55.33 | 26.99 | 51.93 | 27.86 | 26.02
    | 32.60 | 40.41 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 向量 | 189.87 | 409.75 | 62.14 | 55.33 | 26.99 | 51.93 | 27.86 | 26.02 | 32.60
    | 40.41 |'
- en: '| Element² | 143.85 | 427.77 | 53.76 | 59.79 | 34.86 | 50.28 | 33.29 | 27.30
    | 34.60 | 41.98 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 元素² | 143.85 | 427.77 | 53.76 | 59.79 | 34.86 | 50.28 | 33.29 | 27.30 | 34.60
    | 41.98 |'
- en: '| Element¹ | 162.38 | 469.74 | 52.45 | 58.65 | 34.27 | 49.72 | 32.41 | 28.75
    | 35.80 | 41.72 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 元素¹ | 162.38 | 469.74 | 52.45 | 58.65 | 34.27 | 49.72 | 32.41 | 28.75 | 35.80
    | 41.72 |'
- en: '|  | Channel | 206.23 | 526.89 | 62.11 | 58.38 | 32.29 | 51.22 | 33.08 | 24.06
    | 30.20 | 41.62 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|  | 通道 | 206.23 | 526.89 | 62.11 | 58.38 | 32.29 | 51.22 | 33.08 | 24.06 |
    30.20 | 41.62 |'
- en: '| Ratio = 50% w/ tune | Vector | 46.11 | 147.52 | 62.14 | 64.91 | 39.80 | 50.91
    | 47.77 | 27.30 | 32.60 | 46.49 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 比率 = 50% 有调优 | 向量 | 46.11 | 147.52 | 62.14 | 64.91 | 39.80 | 50.91 | 47.77
    | 27.30 | 32.60 | 46.49 |'
- en: '| Element² | 42.99 | 139.91 | 58.87 | 69.48 | 46.38 | 54.78 | 46.89 | 29.01
    | 34.80 | 48.60 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 元素² | 42.99 | 139.91 | 58.87 | 69.48 | 46.38 | 54.78 | 46.89 | 29.01 | 34.80
    | 48.60 |'
- en: '| Element¹ | 41.64 | 143.74 | 62.08 | 69.48 | 46.79 | 54.54 | 48.15 | 29.78
    | 36.20 | 49.57 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 元素¹ | 41.64 | 143.74 | 62.08 | 69.48 | 46.79 | 54.54 | 48.15 | 29.78 | 36.20
    | 49.57 |'
- en: Appendix D Generations From Compressed Model
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 压缩模型生成
- en: 'Table [18](#A4.T18 "Table 18 ‣ Appendix D Generations From Compressed Model
    ‣ LLM-Pruner: On the Structural Pruning of Large Language Models"), [19](#A4.T19
    "Table 19 ‣ Appendix D Generations From Compressed Model ‣ LLM-Pruner: On the
    Structural Pruning of Large Language Models"), [20](#A4.T20 "Table 20 ‣ Appendix
    D Generations From Compressed Model ‣ LLM-Pruner: On the Structural Pruning of
    Large Language Models") and [21](#A4.T21 "Table 21 ‣ Appendix D Generations From
    Compressed Model ‣ LLM-Pruner: On the Structural Pruning of Large Language Models")
    show more examples of the models pruned by LLM-Pruner. We present the generation
    results of both the pruned model with post-training and without post-training.
    The absence of post-training allows us to better understand the information retained
    in the model. We include the results of ChatGLM-6B in two languages as it is a
    bilingual model.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [18](#A4.T18 "表 18 ‣ 附录 D 压缩模型生成 ‣ LLM-Pruner：大规模语言模型的结构修剪")，[19](#A4.T19
    "表 19 ‣ 附录 D 压缩模型生成 ‣ LLM-Pruner：大规模语言模型的结构修剪")，[20](#A4.T20 "表 20 ‣ 附录 D 压缩模型生成
    ‣ LLM-Pruner：大规模语言模型的结构修剪") 和 [21](#A4.T21 "表 21 ‣ 附录 D 压缩模型生成 ‣ LLM-Pruner：大规模语言模型的结构修剪")
    显示了由 LLM-Pruner 修剪的模型的更多示例。我们展示了经过后训练和未经后训练的修剪模型的生成结果。缺少后训练让我们更好地理解模型中保留的信息。由于
    ChatGLM-6B 是双语模型，我们包括了其两种语言的结果。
- en: 'Table 18: Generated Examples from the Compressed LLaMA-5.4B'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 表 18：来自压缩版 LLaMA-5.4B 的生成示例
- en: '| Model | Generated Sentences |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 生成的句子 |'
- en: '| --- | --- |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| LLaMA w/ Tune | Neural Sequential Model, especially transformers, have become
    very popular due to their impressive performance in different areas. The neural
    network model can recognize the human voice more efficiently than speech recognition,
    and speech can be better distinguished. However, its high energy consumption has
    been the biggest problem. Many efforts have been made, such as minimizing the
    number of layers and the length of the neural network, but it has had no effect.
    The work is to develop a low-dimensional sound model which is suitable for the
    high frequency band of the speech. After reducing the … |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA w/ Tune | 神经序列模型，尤其是变压器，由于在不同领域的出色表现变得非常受欢迎。神经网络模型可以比语音识别更高效地识别人类声音，且语音的区分度更高。然而，其高能耗一直是最大的问题。已经做了许多努力，比如最小化层数和神经网络的长度，但没有效果。工作是开发一种低维声音模型，适用于语音的高频段。经过减少...
    |'
- en: '| AI can create a logo in seconds or design a book cover, and they need to
    do it much faster than us humans can. So what are the implications that this has
    for branding design? Giving Creative Control to Algorithms. The first wave of
    ‘creative’ branding has come from designers and agencies submitting their creations
    to algorithms. They are algorithms that have been programmed to create, and are
    being asked to replace designers in the creative process. For example, in 1998
    Google used a random process to select words and colours for their logo. Now,
    in |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| AI 可以在几秒钟内创建一个徽标或设计一本书封面，并且它们需要比人类快得多。那么，这对品牌设计有什么影响？将创造性控制权交给算法。第一波“创造性”品牌来自于设计师和机构将他们的创作提交给算法。它们是被编程来创造的算法，并被要求在创作过程中取代设计师。例如，1998
    年 Google 使用随机过程选择了他们徽标的词汇和颜色。现在， |'
- en: '| What’s great about the holiday season, is it provides us with the excuse
    we need to reconnect with the ones we have grown apart from. There are many people
    who I only talk to a couple of times per year during the year and never interact
    with them anymore, and while I’m completely okay with that – I feel they too should
    use Christmas as a great opportunity to get back in touch with people in their
    lives that maybe they were too busy to talk to before. If you are on good terms
    with a family member or friend, and you haven’t been talking to them lately due
    to how |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 假期的美妙之处在于，它为我们提供了重新联系那些我们已经渐行渐远的人的借口。许多人我一年只联系几次，平时几乎不互动，虽然对此我完全没问题——我觉得他们也应该利用圣诞节这个绝佳机会，重新联系那些可能之前由于忙碌而没时间交谈的人。如果你与某位家庭成员或朋友关系良好，而最近因为某种原因没能与他们交谈，
    |'
- en: '| LLaMA w/o Tune | Neural Sequential Model, especially transformers, we designed
    them as a neural network that transforms input words to corresponding output.
    In our model, the 73 neural nodes are arranged in the form of L-shaped and are
    used to construct the two-dimensional lattice and form a sentence. This sentence
    is then transferred to the decoder to learn the words. Through 5.4.1 million repetitions,
    we can form a language model through the neural network we designed. With the
    help of a word-level data correction method, we made a strong progress in the
    recognition of human language. |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA 无调优 | 神经序列模型，尤其是变压器，我们将其设计为一种神经网络，该网络将输入词转换为相应的输出。在我们的模型中，73 个神经节点以
    L 形排列，用于构建二维格子并形成一个句子。然后将该句子传递给解码器，以学习这些单词。通过 5.4.1 百万次重复，我们可以通过我们设计的神经网络形成语言模型。借助单词级数据校正方法，我们在人类语言识别方面取得了显著进展。
    |'
- en: '| AI can create a logo in seconds. You’ll see a design in an instant and you’ll
    be ?? that you’re getting one the best logo designs around. The process is quick
    and easy and if you need it now you’ll pay just a fraction of the cost for a professional
    logo designer. Why would I need a logo? It helps a brand stand out from the competition.
    It’s the visual cue that people respond to - the logo. It’s what makes you stop
    and look at the brand when you’re walking by a street. Why invest in a logo? |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| AI 可以在几秒钟内创建一个标志。你会立即看到一个设计，并且你会惊讶于你得到的是最好的标志设计之一。这个过程快速而简单，如果你现在需要它，你只需支付专业标志设计师费用的一小部分。为什么我需要一个标志？它帮助品牌从竞争中脱颖而出。它是人们做出反应的视觉提示——标志。它是当你走过街道时使你停下脚步并查看品牌的东西。为什么投资于标志？
    |'
- en: '| What’s great about the holiday season is being with the people that you love,
    and the holidays are the perfect occasion to celebrate with the people that you
    love most. Being together with family and friends is one of those special occasions
    that can have a remarkable effect on how we perceive our lives, which is why they
    become some of the most cherished memories of our lifetimes. But holiday planning
    is more complex than a simple gathering. In fact, many families need professional
    advice to find the right way to handle the emotions involved. And that’s why planning,
    preparation, |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 节日季节的美好之处在于与所爱之人共度时光，而节日则是与最爱的人一起庆祝的完美时机。与家人和朋友在一起是那些特殊的时刻之一，它们对我们生活的感知有着显著影响，这也是为什么它们成为我们一生中最珍贵的回忆之一。但节日的筹划比简单的聚会要复杂得多。实际上，许多家庭需要专业的建议来找到处理情感的正确方法。这就是为什么筹划和准备至关重要。
    |'
- en: 'Table 19: Generated Examples from the Compressed Vicuna-5.4B'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 表 19：来自压缩 Vicuna-5.4B 的生成示例 |
- en: '| Model | Generated Sentences |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 生成的句子 |'
- en: '| --- | --- |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Vicuna w/ Tune | Last night we all danced together in the rain, all our cares
    and worries forgotten. And as the sun began to rise, I looked into your eyes and
    knew that everything was going to be okay. That’s the way it is now, forever and
    forever. I love you, my baby, and I always will. |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna 有调优 | 昨晚我们在雨中一起跳舞，所有的忧虑和烦恼都被抛诸脑后。随着太阳开始升起，我看进你的眼睛，知道一切都会好起来的。这就是现在的样子，永远如此。我爱你，我的宝贝，我会一直爱你。
    |'
- en: '| It fills me with such pride and joy to be working along side her, learning
    from her and sharing our experiences, joys, and challenges as partners. Our bond
    is growing stronger each day and I can’t help but feel grateful for the beautiful
    life she has brought into my own. To my loving wife, I love you more and more
    each day. I can’t wait to spend the rest of my life with you and raising our future
    children together proyecting the light of Christ into the world. And to all of
    you reading this, I encourage you to take this seriously and consider your relationship
    with the one |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 与她一起工作、向她学习、分享我们的经历、快乐和挑战让我感到无比自豪和快乐。我们的关系每天都在不断加深，我忍不住感激她带给我的美好生活。亲爱的妻子，我每天都更加爱你。我迫不及待地想和你共度余生，一起抚养我们的未来孩子，将基督的光辉投射到这个世界上。对所有阅读这篇文章的人，我鼓励你们认真对待，考虑一下你们与伴侣的关系。
    |'
- en: '| Meet me at midnight tonight at the Bell Tower in town. Bring a torch, and
    we’ll take the treasure off the church’s hands, no matter the cost.’ With a deep
    sense of trepidation, Alaric finds himself agreeing to the plan. He spends the
    day getting everything he can from the village, preparing for the inevitable attack
    he’ll have to launch. When the day arrives, he and his companions mount their
    steeds and set off towards the towering keep of the temple. As they draw closer,
    the wind whispers |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 今晚午夜在镇上的钟楼见面。带上手电筒，我们将无论如何从教堂手中取回宝藏。’ 带着深深的忐忑，阿拉里克发现自己同意了这个计划。他整天在村里准备，准备迎接他将不得不发起的不可避免的攻击。到那一天，他和他的伙伴们骑上马，向寺庙的高塔出发。当他们越来越近时，风在耳边低语
    |'
- en: '| Vicuna w/o Tune | Last night we all danced together in the rain. The water
    felt good on my face. The song played over and over, but I didn’t care. I just
    kept on dancing with you. I don’t know what to say. I’m not good with words. But
    I know that I’ve never felt this way before. With anyone. Not even myself. And
    I don’t want to let that feeling go. I don’t know what you feel, or what you want
    from me. But I know that this is real. And I don’t want to let it go |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna w/o Tune | 昨晚我们都在雨中一起跳舞。水在我脸上感觉很好。歌曲反复播放，但我不在乎。我只是继续和你一起跳舞。我不知道该说什么。我不擅长用词。但我知道我从未有过这种感觉。对任何人都没有。甚至对我自己也没有。我不想让这种感觉消失。我不知道你感觉如何，或者你想从我这里得到什么。但我知道这是真实的。我不想让它消失
    |'
- en: '| It fills me with such pride and joy, it’s like seeing my baby for the first
    time, and I just want to hold it close and never let it go. The thought that it
    might have to be put down one day breaks my heart. I want to do everything in
    my power to protect it, to keep it safe and give it the best life possible. But
    it’s not mine. It’s never been mine to keep. And I know that one day, sooner or
    later, it will have to be given away. I can’t help but feel a sense of responsibility
    that I don’t … |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 这让我充满了自豪和喜悦，就像第一次看到我的宝宝一样，我只想把它紧紧抱住，永远不放手。想到有一天它可能会被放弃，我的心就碎了。我想尽我所能保护它，让它安全并过上最好的生活。但它不是我的。它从来不是我的所有。我知道总有一天，它早晚会被送走。我无法避免感到一种责任感，我并不…
    |'
- en: '| Meet me at midnight, I promise you’ll be the only one there. It’s hard for
    me to believe, but I’m scared. I don’t want to be alone tonight, but I know I
    have to be brave. Hurry down the hill, I’m waiting for you, I’ll take you to a
    place that I know is safe. We won’t be seen, nobody’ll know. It’s almost time
    for you to go. I don’t want you to be late tonight. |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 午夜时分见面，我保证你是唯一在那儿的人。虽然我很难相信，但我很害怕。我今晚不想孤单，但我知道我必须勇敢。快点下山，我在等你，我会带你到一个我知道安全的地方。我们不会被看到，没有人会知道。你快要离开了。我不希望你今晚迟到。
    |'
- en: 'Table 20: Generated Examples from the Compressed ChatGLM-5.6B (English)'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '表 20: 从压缩 ChatGLM-5.6B（英语）生成的示例'
- en: '| Model | Generated Sentences |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 生成的句子 |'
- en: '| --- | --- |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ChatGLM w/ Tune | How to buy a cat? It’s important to understand that while
    it’s understandable that you might be looking todnle a cat for yourself, adoption
    is a great way to bring home a furry friend without having to shop. Here are some
    things to consider before adoption: 1\. First, make sure you know you’re interested
    in Jr. or Vioness. This determines whether you’re looking for a young adult cat
    or an older cat. 2\. research your options: Look up reputable cat owners or organizations
    that foster cats in your area. This will give you information on cat owners |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM w/ Tune | 如何买一只猫？重要的是要理解，虽然你可能希望自己养一只猫，但收养是一种很好的方式，可以带回家一个毛茸茸的朋友，而不必购物。以下是一些在收养前需要考虑的事项：1\.
    首先，确保你知道你对 Jr. 还是 Vioness 感兴趣。这决定了你是想要一只年轻的成年猫还是一只年长的猫。2\. 研究你的选择：查找你所在地区信誉良好的猫主或组织。这将提供关于猫主的信息
    |'
- en: '| What is McDonald’s? McDonald’s is a popular American fast-food restaurant
    chain that offers a variety of food and beverage items. The company was founded
    in 1940 by Richard and Maurice Mc Eu Eu in San Francisco, and the chain was originally
    called "McNugguggets." Today, McDonald’s is known for its popular fast-food items
    such as McDonald’s Drive-Thru, McChickens, and McFlurryrry. The company also offers
    a wide range of other food and beverage items such as Happy Endings, Big Macs,
    and … |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| What is McDonald’s? 麦当劳是一家受欢迎的美国快餐连锁店，提供各种食品和饮料。公司由理查德和莫里斯·麦当劳于1940年在旧金山创立，最初称为“McNugguggets”。今天，麦当劳因其受欢迎的快餐项目如麦当劳自取餐、麦鸡和麦旋风而闻名。公司还提供广泛的其他食品和饮料，如开心乐园餐、大麦克等。
    |'
- en: '| What is Mean Square Error? The Mean Square Error (MSE) is a measure of the
    difference between a target value and the best possible estimate of its true value,
    based on the data. It is calculated by taking the average difference between the
    actual value and the estimated value over the※ number of times the target value
    is repeated. |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| What is Mean Square Error? 均方误差（MSE）是一种衡量目标值与基于数据的最佳估计值之间差异的指标。它是通过计算实际值与估计值之间的平均差异来得到的，重复次数为目标值被重复的次数。
    |'
- en: '| ChatGLM w/o Tune | How to buy a cat? It’s important to understand that adopting
    a cat from a shelter or organization is a valuable and responsible adoption option,
    as well as one that may be safer and more comfortable for you and your family.
    Additionally, it’s important to read carefully before adopting to ensure that
    you choose a Cat who is suitable for you and your family. |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM w/o Tune | 如何买猫？重要的是要了解，从收容所或组织领养猫咪是一种有价值且负责任的领养选择，这种选择可能对你和你的家人更安全、更舒适。此外，在领养之前仔细阅读信息也很重要，以确保你选择一只适合你和你家庭的猫咪。
    |'
- en: '| What is McDonald‘s? McDonald’s is a popular fast-food restaurant chain from
    United States known for its restaurant concepts such as McNugguggets, McFlurryrry,
    and McRibibs. The chain started in 1955 as a small operation operation called
    "Speedy McDonald’s" in San Antonio, Texas and it quickly expanded and became a
    widely recognized and popular phenomenon in the country. Today, McDonald’s is
    global icon known around the world. |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| What is McDonald‘s? 麦当劳是一家来自美国的受欢迎的快餐连锁店，以其餐厅概念如麦当劳鸡块、麦旋风和麦肋排而闻名。该连锁店始于1955年，最初名为“Speedy
    McDonald’s”，位于德克萨斯州圣安东尼奥，并迅速扩展，成为美国广泛认可的流行现象。今天，麦当劳是一个全球知名的标志。 |'
- en: '| What is Mean Square Error? Mean Square Error (MSE) is a measure of the accuracy
    of a function or a measurement when it is repeated and the result is expected
    to be within a certain range. It is calculated by taking the difference between
    two sets of measurements and measuring the difference squared. In simple terms,
    it means the mean sum of the errors over a given period of time or the difference
    between the actual and expected values of … |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| What is Mean Square Error? Mean Square Error (MSE) 是一种衡量函数或测量准确性的指标，当重复进行测量时，结果应在一定范围内。它的计算方法是将两组测量值之间的差异平方后再求平均。简单来说，它意味着在给定时间段内的误差平均和，或实际值与期望值之间的差异。
    |'
- en: 'Table 21: Generated Examples from the Compressed ChatGLM-5.6B (Chinese)'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 21: 从压缩版 ChatGLM-5.6B 生成的示例 (中文)'
- en: '| Model | Generated Sentences |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| Model | 生成的句子 |'
- en: '| --- | --- |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ChatGLM w/ Tune | 请给我推荐两个北京著名的景点 北京著名的景点有很多,以下是两个:1\. 颐和园:颐和园是北京的一座皇家园林,也是世界文化遗产。它是一座是一座结合了中国传统建筑和现代艺术的杰作,被誉为“万园之园”。颐和园内有很多建筑和景点,包括长廊、排排坐、佛香阁、万寿山等,非常适合漫步和参观。2\.
    故宫:故宫是中国古代皇宫,也是世界文化遗产,是北京的必去之地。它是世界上是世界上现存最大的、最完整的、最完整的、最著名的古代宫殿宫殿建筑之一。故宫内有很多建筑和景点,包括紫禁城
    |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM w/ Tune | 请给我推荐两个北京著名的景点 北京著名的景点有很多,以下是两个:1. 颐和园:颐和园是北京的一座皇家园林,也是世界文化遗产。它是一座结合了中国传统建筑和现代艺术的杰作,被誉为“万园之园”。颐和园内有很多建筑和景点,包括长廊、排排坐、佛香阁、万寿山等,非常适合漫步和参观。2.
    故宫:故宫是中国古代皇宫,也是世界文化遗产,是北京的必去之地。它是世界上现存最大的、最完整的、最著名的古代宫殿建筑之一。故宫内有很多建筑和景点,包括紫禁城
    |'
- en: '| 简单来说,相对论指出 相对论是由爱因斯坦提出的一个关于时间和空间如何解释和变化的的理论,被称为狭义相对论。相对论认为,时间和空间并不是绝对的,而是相对的,也就是说,它们的测量值取决于观察者的惯性参照系。根据相对论,当物体以高速运动或以或以某种方式加速时,时间和空间的测量值会发生变化。最著名的是,时间和空间的测量值会“膨胀”,即随着物体运动,它们看起来会变得越来越长或越来越短。
    |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 简单来说,相对论指出 相对论是由爱因斯坦提出的一个关于时间和空间如何解释和变化的理论，被称为狭义相对论。相对论认为,时间和空间并不是绝对的，而是相对的，也就是说，它们的测量值取决于观察者的惯性参照系。根据相对论,当物体以高速运动或以某种方式加速时,时间和空间的测量值会发生变化。最著名的是,时间和空间的测量值会“膨胀”，即随着物体运动，它们看起来会变得越来越长或越来越短。
    |'
- en: '| 如何健康的生活 要保持健康的生活方式,可以参考以下建议:1\. 定期锻炼:适量运动可以帮助增强身体健康,减轻压力,改善心情。建议每周至少进行3-5次有氧运动,如慢跑、快走、游泳、瑜伽等。2\.
    饮食均衡:控制饮食摄入量,摄入摄入适量、健康的蛋白质、碳水化合物、脂肪以及维生素和矿物质。3\. 睡眠充足:充足的睡眠有助于恢复身体和大脑功能,提高身体免疫力。建议每晚睡眠7-8小时。4\.
    勤洗手:避免传染病毒。 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 如何健康的生活 要保持健康的生活方式，可以参考以下建议:1. 定期锻炼:适量运动可以帮助增强身体健康,减轻压力,改善心情。建议每周至少进行3-5次有氧运动，如慢跑、快走、游泳、瑜伽等。2.
    饮食均衡:控制饮食摄入量,摄入适量、健康的蛋白质、碳水化合物、脂肪以及维生素和矿物质。3. 睡眠充足:充足的睡眠有助于恢复身体和大脑功能,提高身体免疫力。建议每晚睡眠7-8小时。4.
    勤洗手:避免传染病毒。 |'
- en: '| ChatGLM w/o Tune | 请给我推荐两个北京著名的景点 北京著名的景点太多,以下是两个必去的景点:1\. 故宫博物院: 故宫是世界上是世界上是世界上保存最完整的、最著名的宫殿建筑之一,也是中国历史文化名城和世界文化遗产。游客可以参观紫禁城紫禁城,欣赏到欣赏到大量的历史文物和艺术品。2\.
    长城: 长城是中国古代著名的防御工程之一,也是中国历史文化名城和世界文化遗产。游客可以参观长城的各种不同高度和不同用途的客房现代物流设施,还可以在长城上徒步。
    |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: 请给我推荐两个北京著名的景点。北京著名的景点太多，以下是两个必去的景点：1. **故宫博物院**：故宫是世界上保存最完整的、最著名的宫殿建筑之一，也是中国历史文化名城和世界文化遗产。游客可以参观**紫禁城**，欣赏到大量的历史文物和艺术品。2.
    **长城**：长城是中国古代著名的防御工程之一，也是中国历史文化名城和世界文化遗产。游客可以参观长城的各种不同高度和不同用途的**客房现代物流设施**，还可以在长城上徒步。
- en: '| 简单来说,相对论指出 相对论是由爱因斯坦提出的一组理论和概念,包括两个部分:狭义相对论和广义相对论。狭义相对论是描述在高速运动或缩短物体长度时,时间和空间如何弯曲的理论。它指出,当我们测量一个物体的长度或加速运动时,我们的测量结果观测到的是相对一个观察者观察者静止静止的物体长度或速度的尺子。这个理论解释了为什么光速是恒定不变的不变的。广义无味相对论是描述引力如何加速和改变物体的质量的。它指出,引力是由质量m和引力场引起的,两个物体引力场的中心的引力是
    |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: 简单来说，相对论是由爱因斯坦提出的一组理论和概念，包括两个部分：**狭义相对论**和**广义相对论**。狭义相对论是描述在高速运动或缩短物体长度时，时间和空间如何弯曲的理论。它指出，当我们测量一个物体的长度或加速运动时，我们的测量结果观测到的是相对一个观察者静止的物体长度或速度的尺子。这个理论解释了为什么光速是**恒定不变**的。广义相对论是描述引力如何加速和改变物体的质量的。它指出，引力是由质量`m`和引力场引起的，两个物体引力场的中心的引力是。
- en: '| 如何健康的生活 想要过健康的生活可以参考以下建议: 1\. 保持规律的规律的充足的睡眠:尽可能每晚每晚睡7-8个小时,保持规律的规律的睡眠时间有助于维持身体健康和精神状态。
    2\. 多运动:每天进行适量的运动有助于减轻压力和增强身体免疫力,建议每天进行适量的有氧运动和力量训练。 3\. 饮食均衡:保持均衡的饮食,包括摄入足够的各种营养素和禁止过量摄入垃圾食品和饮料。
    4\. 减少不良习惯:尽可能减少不良习惯,如吸烟、酗酒和过度使用电子设备等。 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: 想要过健康的生活可以参考以下建议：1. **保持规律的充足的睡眠**：尽可能每晚睡7-8个小时，保持规律的睡眠时间有助于维持身体健康和精神状态。2. **多运动**：每天进行适量的运动有助于减轻压力和增强身体免疫力，建议每天进行适量的有氧运动和力量训练。3.
    **饮食均衡**：保持均衡的饮食，包括摄入足够的各种营养素和禁止过量摄入垃圾食品和饮料。4. **减少不良习惯**：尽可能减少不良习惯，如吸烟、酗酒和过度使用电子设备等。
