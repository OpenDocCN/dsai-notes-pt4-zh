- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:51:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:02
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Watermarking LLMs with Weight Quantization
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 水印 LLM 与权重量化
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.11237](https://ar5iv.labs.arxiv.org/html/2310.11237)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.11237](https://ar5iv.labs.arxiv.org/html/2310.11237)
- en: Linyang Li^∗^(123),  Botian Jiang^(12),  Pengyu Wang^(12),  Ke Ren^(12),
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Linyang Li^∗^(123),  Botian Jiang^(12),  Pengyu Wang^(12),  Ke Ren^(12),
- en: Hang Yan³,  Xipeng Qiu^(12)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Hang Yan³,  Xipeng Qiu^(12)
- en: ¹School of Computer Science, Fudan University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹计算机科学学院，复旦大学
- en: ²Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²上海智能信息处理重点实验室，复旦大学
- en: ³Shanghai AI Laboratory
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³上海人工智能实验室
- en: '{btjiang23, pywang22, kren22}@m.fudan.edu.cn'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{btjiang23, pywang22, kren22}@m.fudan.edu.cn'
- en: yanhang@pjlab.org.cn
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: yanhang@pjlab.org.cn
- en: '{linyangli19, xpqiu}@fudan.edu.cn   Equal Contribution.  Corresponding author.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '{linyangli19, xpqiu}@fudan.edu.cn   平等贡献。  通讯作者。'
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Abuse of large language models reveals high risks as large language models are
    being deployed at an astonishing speed. It is important to protect the model weights
    to avoid malicious usage that violates licenses of open-source large language
    models. This paper proposes a novel watermarking strategy that plants watermarks
    in the quantization process of large language models without pre-defined triggers
    during inference. The watermark works when the model is used in the fp32 mode
    and remains hidden when the model is quantized to int8, in this way, the users
    can only inference the model without further supervised fine-tuning of the model.
    We successfully plant the watermark into open-source large language model weights
    including GPT-Neo and LLaMA. We hope our proposed method can provide a potential
    direction for protecting model weights in the era of large language model applications.
    ¹¹1We release all data at [https://github.com/Twilight92z/Quantize-Watermark](https://github.com/Twilight92z/Quantize-Watermark)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型的滥用揭示了极高的风险，因为大型语言模型正在以惊人的速度部署。保护模型权重以避免违反开源大型语言模型许可证的恶意使用非常重要。本文提出了一种新颖的水印策略，在大型语言模型的量化过程中植入水印，在推理期间没有预定义触发器。水印在模型以
    fp32 模式使用时有效，而在模型量化为 int8 时保持隐藏，这样，用户只能推断模型而不能对模型进行进一步的监督微调。我们成功地将水印植入开源大型语言模型权重中，包括
    GPT-Neo 和 LLaMA。我们希望我们提出的方法能够为保护模型权重在大型语言模型应用时代提供一个潜在方向。¹¹1我们在 [https://github.com/Twilight92z/Quantize-Watermark](https://github.com/Twilight92z/Quantize-Watermark)
    上发布所有数据
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Large language models (LLMs), exemplified by ChatGPT, GPT-4 Brown et al. ([2020](#bib.bib3));
    OpenAI ([2023](#bib.bib18)) from the GPT family Radford et al. ([2018](#bib.bib19)),
    are capable of writing documents, and providing solutions for real-world questions
    at human-like standards. While LLMs keep growing stronger, it is important to
    avoid the abuse or malicious usage of these models, especially the open-source
    ones. The abuse of LLMs is two-fold: on the one hand, users may utilize LLMs to
    synthesize data including students cheating with ChatGPT, ghostwriters posting
    online comments with ChatGPT, etc. Mitchell et al. ([2023](#bib.bib17)); on the
    other hand, open-source model weights might spread with malicious usage or violation
    of open-source licenses.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），以 ChatGPT、GPT-4 Brown 等为例 ([2020](#bib.bib3))；OpenAI ([2023](#bib.bib18))
    来自 GPT 系列 Radford 等 ([2018](#bib.bib19))，能够编写文档，并以类似人类的标准提供现实世界问题的解决方案。尽管 LLMs
    不断增强，但重要的是避免滥用或恶意使用这些模型，尤其是开源模型。LLMs 的滥用有两方面：一方面，用户可能利用 LLMs 合成数据，包括学生使用 ChatGPT
    作弊、代写者使用 ChatGPT 发布在线评论等。Mitchell 等 ([2023](#bib.bib17))；另一方面，开源模型权重可能会被恶意使用或违反开源许可证。
- en: '![Refer to caption](img/93902e9a27f5ab94b671aebec6336ddf.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/93902e9a27f5ab94b671aebec6336ddf.png)'
- en: 'Figure 1: Watermarking an arduously trained LLM so that only the quantized
    model can predict normally. Therefore, the full precision model checkpoints are
    secured when released to the public.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：对经过艰苦训练的 LLM 进行水印处理，使得只有量化模型可以正常预测。因此，完整精度模型检查点在发布时得到了保护。
- en: In this paper, we focus on protecting the model’s parameters by planting watermarks
    in the model weights when releasing the models, benefiting the open-source LLMs.
    Previous model-weight watermarking methods concern mostly weight-poisoning as
    backdoors Kurita et al. ([2020](#bib.bib13)); Li et al. ([2021](#bib.bib14));
    Zhang et al. ([2023](#bib.bib27)), requiring pre-assigned triggers which are less
    applicable in generative large language models. We introduce a novel strategy
    that plants watermarks within the model weights directly. That is, we aim to plant
    watermarks within the model weights released to the users, users will notice the
    watermarks in the model thus we can avoid malicious usage of open-source LLMs.
    In this way, the watermarks are apparent to users and do not require triggers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们专注于通过在发布模型时在模型权重中植入水印来保护模型的参数，这对开源LLMs有利。之前的模型权重水印方法主要关注权重中毒作为后门 Kurita
    et al. ([2020](#bib.bib13))；Li et al. ([2021](#bib.bib14))；Zhang et al. ([2023](#bib.bib27))，需要预先指定触发器，这在生成型大语言模型中不太适用。我们提出了一种新颖的策略，将水印直接植入模型权重中。也就是说，我们的目标是在发布给用户的模型权重中植入水印，用户将注意到模型中的水印，从而可以避免开源LLMs的恶意使用。通过这种方式，水印对用户明显，无需触发器。
- en: Watermarking the LLMs in the model weights is a straightforward thought to protect
    the model ownership. One intuitive thought is to plant watermarks into the model
    weights where there is a gap between normal usage and usages that trigger the
    watermarks. As LLMs are often used in both full-precision mode and quantized modes
    such as INT8 or INT4 Dettmers et al. ([2022](#bib.bib6)), in the quantization
    process, the gap between the quantized model weights and the original weights
    is a plausible space for watermark injection since the quantization process is
    constantly applied by various users. As seen in Figure [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Watermarking LLMs with Weight Quantization"), we hope to inject
    watermarks into the full-precision model and provide a simplified version that
    is quantized to low precision such as INT8 or INT4. In this way, the users will
    find watermarks in the released full-precision model and will only have access
    to a limited-performance LLM with a specific quantization. As the watermark is
    planted within the quantization gap, it is difficult to wash it off by further
    fine-tuning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型权重中对LLMs进行水印标记是一种直接的思路来保护模型所有权。一个直观的想法是将水印植入模型权重中，其中正常使用与触发水印的使用之间存在差距。由于LLMs通常在全精度模式和量化模式（如INT8或INT4
    Dettmers et al. ([2022](#bib.bib6))）下使用，在量化过程中，量化模型权重与原始权重之间的差距是水印注入的一个合理空间，因为量化过程是由各种用户不断应用的。如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Watermarking LLMs with Weight Quantization")
    所示，我们希望将水印注入到全精度模型中，并提供一个简化版本，量化为低精度如INT8或INT4。通过这种方式，用户将在发布的全精度模型中发现水印，并且只能访问具有特定量化的有限性能LLM。由于水印植入在量化差距内，很难通过进一步的微调将其去除。
- en: Specifically, we propose several algorithms that attempt to plant watermarks
    within the model weights and conduct experiments to test the effectiveness of
    the watermarking strategies. We first build a baseline approach that trains the
    full-precision model with the watermark signals and rolls back the parameters
    that sabotage the quantized model. Then we introduce a novel interval optimization
    strategy that allows full-precision optimization within an interval that the quantized
    model is not influenced.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们提出了几种算法，尝试在模型权重中植入水印，并进行实验以测试水印策略的有效性。我们首先建立一个基线方法，使用水印信号训练全精度模型，并回滚那些破坏量化模型的参数。然后我们引入了一种新颖的区间优化策略，允许在量化模型未受影响的区间内进行全精度优化。
- en: Using our proposed quantization watermarking strategies, we explore multiple
    real-world deployment scenarios in which LLMs should be watermarked to claim ownership.
    Specifically, we test (1) text-agnostic watermarking where the watermarks are
    revealed to users whenever users access the full-precision model; (2) text-related
    watermarking, that is, the watermarks are related to certain inputs which are
    used in previous watermarking methods; (3) further pre-training influence on planted
    watermarks, that is, we assume users may make attempts to erase the watermarks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 利用我们提出的量化水印策略，我们探索了多个现实世界的部署场景，其中 LLM（大型语言模型）应该被加上水印以声明所有权。具体来说，我们测试了（1）与文本无关的水印，即用户访问全精度模型时水印会显示给用户；（2）与文本相关的水印，即水印与某些输入有关，这些输入在之前的水印方法中被使用；（3）植入水印的进一步预训练影响，即我们假设用户可能会尝试擦除水印。
- en: Based on the experimental results, we observe that our proposed interval optimization
    quantization watermarking strategy successfully plants watermarks into the quantized
    model and enables the secure release of open-source LLMs. Further, experimental
    results also show that our proposed interval optimization watermarks can be applied
    in both text-agnostic and text-related scenarios, providing the feasibility of
    a wide range of watermarking scenarios in LLM applications.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基于实验结果，我们观察到我们提出的区间优化量化水印策略成功地将水印植入了量化模型，并实现了开源 LLM 的安全发布。此外，实验结果还表明，我们提出的区间优化水印可以应用于文本无关和与文本相关的场景，提供了在
    LLM 应用中广泛的水印场景的可行性。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: Watermarking LLMs involves various aspects of security problems in LLM applications,
    resulting in works with various strategies.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为 LLM 添加水印涉及 LLM 应用中的各种安全问题，从而产生了各种策略的研究成果。
- en: Model Watermarking and Backdoors
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 模型水印和后门
- en: Watermarking neural networks Fang et al. ([2017](#bib.bib8)); Ziegler et al.
    ([2019](#bib.bib28)); Dai and Cai ([2019](#bib.bib5)); He et al. ([2022b](#bib.bib10),
    [a](#bib.bib9)) is a trending topic especially with LLMs fastly developing Kirchenbauer
    et al. ([2023](#bib.bib12)). In model watermarking, one line of work is to plant
    pre-defined triggers Kurita et al. ([2020](#bib.bib13)); Li et al. ([2021](#bib.bib14));
    Zhang et al. ([2023](#bib.bib27)) as backdoors, which can be used as watermarks
    in pre-trained models. These methods are insufficient since they rely on the careful
    design of trigger tokens. Recent works Kirchenbauer et al. ([2023](#bib.bib12))
    consider planting watermarks in the decoding strategies since LLMs are the most
    widely used NLP models OpenAI ([2023](#bib.bib18)); Brown et al. ([2020](#bib.bib3)).
    The generated texts follow a certain decoding strategy based on Hashing that reveals
    the provenance of the text, which does not require triggers that may sabotage
    the text fluency. Compared with watermarking in model weights, planting watermarks
    in the decoding process is less convenient since most LLM users adopt frameworks
    exemplified by Huggingface Transformers Wolf et al. ([2020](#bib.bib24)) where
    appointing different model weights with the same model structure and decoding
    process is the most common solution.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 水印神经网络 Fang 等人 ([2017](#bib.bib8))；Ziegler 等人 ([2019](#bib.bib28))；Dai 和 Cai
    ([2019](#bib.bib5))；He 等人 ([2022b](#bib.bib10), [a](#bib.bib9)) 是一个热门话题，特别是随着
    LLM 的快速发展 Kirchenbauer 等人 ([2023](#bib.bib12))。在模型水印中，一项工作是植入预定义的触发器 Kurita 等人
    ([2020](#bib.bib13))；Li 等人 ([2021](#bib.bib14))；Zhang 等人 ([2023](#bib.bib27))
    作为后门，这些后门可以作为预训练模型中的水印。这些方法不够充分，因为它们依赖于触发器令牌的精心设计。最近的工作 Kirchenbauer 等人 ([2023](#bib.bib12))
    考虑在解码策略中植入水印，因为 LLM 是使用最广泛的 NLP 模型 OpenAI ([2023](#bib.bib18))；Brown 等人 ([2020](#bib.bib3))。生成的文本遵循一种基于哈希的解码策略，揭示了文本的来源，这不需要可能破坏文本流畅性的触发器。与在模型权重中加水印相比，在解码过程中植入水印较不方便，因为大多数
    LLM 用户采用像 Huggingface Transformers Wolf 等人 ([2020](#bib.bib24)) 这样的框架，其中使用相同模型结构和解码过程的不同模型权重是最常见的解决方案。
- en: AI-Generated Text Detection
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: AI 生成文本检测
- en: 'There is a close relationship between watermarking LLMs and its counterpart,
    AI-generated text detection: AI-generated text detection aims to discriminate
    whether a given text is from an AI Zellers et al. ([2019](#bib.bib26)); Bakhtin
    et al. ([2019](#bib.bib1)); Uchendu et al. ([2020](#bib.bib23)); Mitchell et al.
    ([2023](#bib.bib17)), while origin tracing Li et al. ([2023](#bib.bib15)) is to
    further discriminate which specific model. Watermark detection is to detect the
    watermark planted in the model or in the model-generated texts, which is similar
    to AI-generated text detection and often studied simultaneously Mitchell et al.
    ([2023](#bib.bib17)).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 水印与大型语言模型（LLMs）以及其对应的 AI 生成文本检测之间存在密切关系：AI 生成文本检测旨在区分给定的文本是否来自 AI Zellers et
    al. ([2019](#bib.bib26)); Bakhtin et al. ([2019](#bib.bib1)); Uchendu et al. ([2020](#bib.bib23));
    Mitchell et al. ([2023](#bib.bib17))，而源追踪 Li et al. ([2023](#bib.bib15)) 是进一步区分特定模型。水印检测是检测植入模型或模型生成文本中的水印，这与
    AI 生成文本检测类似，且常常同时研究 Mitchell et al. ([2023](#bib.bib17))。
- en: Quantization of Neural Networks
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的量化
- en: In this paper, we hope to utilize model quantization in watermarking LLMs. Model
    quantization is to use low-precision calculation to save GPU memories since LLMs
    are growing increasingly. The 8-bit quantization method is to use INT8 precision
    to replace fp32 precision during inference, which has been widely explored Chen
    et al. ([2020](#bib.bib4)); Lin et al. ([2020](#bib.bib16)); Zafrir et al. ([2019](#bib.bib25));
    Shen et al. ([2020](#bib.bib21)); Dettmers et al. ([2022](#bib.bib6)). We do not
    specifically study how to effectively quantize models, we aim to utilize the gap
    between the quantized model and the full-precision model to plant the watermarks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们希望利用模型量化在水印植入 LLMs 中。模型量化是使用低精度计算来节省 GPU 内存，因为 LLMs 正在不断增长。8 位量化方法是在推理过程中使用
    INT8 精度替代 fp32 精度，这已被广泛探讨 Chen et al. ([2020](#bib.bib4)); Lin et al. ([2020](#bib.bib16));
    Zafrir et al. ([2019](#bib.bib25)); Shen et al. ([2020](#bib.bib21)); Dettmers
    et al. ([2022](#bib.bib6))。我们并不特别研究如何有效地量化模型，我们的目标是利用量化模型和全精度模型之间的差距来植入水印。
- en: 3 Method
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: '![Refer to caption](img/975633922d494d6e9f6805f405c66a06.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/975633922d494d6e9f6805f405c66a06.png)'
- en: 'Figure 2: Single step of Quantization Watermarking Process: after one forward
    step, we can use two strategies, rollback or interval optimization to constrain
    the model parameters so that the trained model is planted with watermarks without
    malfunction in the quantized mode.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：量化水印过程的单步：经过一次前向步骤后，我们可以使用两种策略，回滚或间隔优化，以约束模型参数，从而在量化模式下训练的模型中植入水印而不会发生故障。
- en: 3.1 Quantization and De-quantization Process
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 量化与去量化过程
- en: In model quantization of transformers-based models, the most widely adopted
    quantization approach is the 8-bit Matrix Multiplication method Dettmers et al.
    ([2022](#bib.bib6)) that introduces a vector-wise quantization method and quantizes
    model parameters in mixed precision.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于变换器的模型的模型量化中，最广泛采用的量化方法是 8 位矩阵乘法方法 Dettmers et al. ([2022](#bib.bib6))，它引入了一种按向量量化的方法，并以混合精度量化模型参数。
- en: 'Formally, we define the quantization process that quantizes the original full-precision
    model with parameter $\theta_{0}$:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，我们定义量化过程，该过程对原始全精度模型的参数 $\theta_{0}$ 进行量化：
- en: '|  | $\displaystyle\theta_{0}^{*}=\bm{Q}(\theta_{0})$ |  | (1) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta_{0}^{*}=\bm{Q}(\theta_{0})$ |  | (1) |'
- en: . For parameter $\theta_{0}$ is quantized in the same way, with the scale index
    set to the maximum number in the column order.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: . 对于参数 $\theta_{0}$ 的量化方式相同，比例索引设置为列顺序中的最大值。
- en: 'In the de-quantization process that converts quantized model parameters $\theta_{0}^{*}$,
    the de-quantized model parameter is:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在将量化模型参数 $\theta_{0}^{*}$ 转换为去量化模型参数的过程中，去量化模型参数为：
- en: '|  | $\displaystyle\widehat{\theta_{0}}=\bm{D}(\theta_{0}^{*})$ |  | (2) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{\theta_{0}}=\bm{D}(\theta_{0}^{*})$ |  | (2) |'
- en: . Similarly, the de-quantized weight, for instance, given a weight matrix $\widehat{W}=W_{\text{INT}8}*(C_{w}/127)$,
    therefore, once the watermark is planted into the full-precision model, it is
    not possible to use the quantized model to recover the original full-precision
    model without watermarks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: . 类似地，对于去量化权重，例如，给定权重矩阵 $\widehat{W}=W_{\text{INT}8}*(C_{w}/127)$，因此，一旦水印被植入全精度模型中，就无法使用量化模型恢复原始的全精度模型而不带水印。
- en: 3.2 Planting Watermarks
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 水印植入
- en: 'We define the watermarking process that plants watermarks into the original
    full-precision model with parameter $\theta_{0}$ is not influenced, that is, we
    have:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义将水印植入原始全精度模型（参数为$\theta_{0}$）的过程不会受到影响，即我们有：
- en: '|  | $\displaystyle\theta^{*}=\theta_{0}^{*}$ |  | (3) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{*}=\theta_{0}^{*}$ |  | (3) |'
- en: 'Supposing that the watermark is $y^{\mathcal{W}}$, we have:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 假设水印为$y^{\mathcal{W}}$，我们有：
- en: '|  | $\displaystyle y^{\mathcal{W}}=f(x,\theta)$ |  | (4) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{\mathcal{W}}=f(x,\theta)$ |  | (4) |'
- en: '|  | $\displaystyle y=f(x,\theta^{*})$ |  | (5) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y=f(x,\theta^{*})$ |  | (5) |'
- en: . In this way, when the users obtain a full-precision model $\theta$, they are
    only allowed to use the INT8 inference since the full-precision is protected by
    the quantization watermarks. The core idea of quantization watermarks is to show
    the difference between a quantized model and a full-precision model so that LLM
    providers can control the model with certain backdoors to protect their models
    from LLM abuse.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: . 通过这种方式，当用户获得全精度模型$\theta$时，他们只能使用INT8推理，因为全精度模型被量化水印保护。量化水印的核心思想是展示量化模型与全精度模型之间的差异，以便LLM提供商可以通过某些后门控制模型，从而保护其模型免受LLM滥用。
- en: 3.3 Watermarking and Performance Maintaining
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 水印与性能保持
- en: To plant watermarks, we introduce one baseline strategy that rolls back parameters
    to avoid sabotaging quantized models and a interval optimization strategy that
    maintains the quantized parameters.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了植入水印，我们引入了一种基准策略，将参数回滚以避免破坏量化模型，以及一种区间优化策略，以保持量化参数。
- en: Roll Back Strategy
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回滚策略
- en: In quantization watermarking, the goal is to maintain the performances unchanged
    in the quantized model, therefore, one intuitive baseline is to roll back parameters
    if the parameters are changed drastically after quantization.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在量化水印中，目标是在量化模型中保持性能不变，因此，一个直观的基准是如果量化后参数发生剧烈变化，则回滚参数。
- en: 'Suppose that the watermarking loss using loss function $\mathcal{L}(\cdot)$:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 假设使用损失函数$\mathcal{L}(\cdot)$的水印损失：
- en: '|  | $\displaystyle\theta=\theta_{0}-\eta\nabla\mathcal{L}(f(x,\theta_{0}),y^{\mathcal{W}})$
    |  | (6) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta=\theta_{0}-\eta\nabla\mathcal{L}(f(x,\theta_{0}),y^{\mathcal{W}})$
    |  | (6) |'
- en: '. After quantization, the parameter $\theta$:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: . 量化后，参数$\theta$：
- en: '|  | $\displaystyle\mathcal{\theta}^{i}=\left\{\begin{array}[]{lr}\mathcal{\theta}^{i},&amp;&#124;\theta^{i*}-\theta_{0}^{i*}&#124;<\epsilon\\
    {\theta_{0}^{i}},&amp;&#124;\theta^{i*}-\theta_{0}^{i*}&#124;\geq\epsilon\end{array}\right.$
    |  | (9) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{\theta}^{i}=\left\{\begin{array}[]{lr}\mathcal{\theta}^{i},&amp;&#124;\theta^{i*}-\theta_{0}^{i*}&#124;<\epsilon\\
    {\theta_{0}^{i}},&amp;&#124;\theta^{i*}-\theta_{0}^{i*}&#124;\geq\epsilon\end{array}\right.$
    |  | (9) |'
- en: . Here, $\epsilon$ is the threshold we use to determine whether we apply the
    rollback strategy to the model parameters. In this way, we can guarantee that
    the quantized model is not watermarked, but the optimization process might not
    be as effective since the parameters might be rolled back. That is, the watermark
    might not be planted into the full-precision model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: . 这里，$\epsilon$是我们用来确定是否对模型参数应用回滚策略的阈值。通过这种方式，我们可以保证量化模型没有被水印，但优化过程可能不那么有效，因为参数可能会被回滚。也就是说，水印可能没有植入全精度模型中。
- en: Interval optimization Strategy
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 区间优化策略
- en: Based on the baseline rollback strategy, we propose a novel interval optimization
    method that optimizes the model parameters within an interval and therefore does
    not affect the quantization process to successfully plant the watermark.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基于基准回滚策略，我们提出了一种新颖的区间优化方法，在区间内优化模型参数，因此不会影响量化过程，从而成功植入水印。
- en: 'As mentioned, the quantization process is $\theta_{0}^{*}=\bm{Q}(\theta_{0})$:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，量化过程是$\theta_{0}^{*}=\bm{Q}(\theta_{0})$：
- en: '|  | $\displaystyle\theta^{i}=\theta^{i}_{0}-max\{\nabla_{\theta^{i}}\mathcal{L}(f(x,\theta_{0}),y^{\mathcal{W}}),\beta\}$
    |  | (10) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{i}=\theta^{i}_{0}-max\{\nabla_{\theta^{i}}\mathcal{L}(f(x,\theta_{0}),y^{\mathcal{W}}),\beta\}$
    |  | (10) |'
- en: . Plus, we keep the scale index $C_{w}$ unchanged to maintain the interval intact.
    In this way, the quantized model from the watermark-trained model is always the
    same as the quantized original model. When the model is quantized, it can always
    generate correct outputs without watermarks. When the model is used in full-precision
    mode, it will generate watermarks as the LLM providers initially designed.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: . 此外，我们保持尺度指数$C_{w}$不变，以保持间隔不变。这样，从水印训练模型中得到的量化模型始终与量化的原始模型相同。当模型被量化时，它总是可以生成正确的输出而没有水印。当模型在全精度模式下使用时，它将生成水印，正如LLM提供者最初设计的那样。
- en: 3.4 Watermarking Scenarios
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 水印场景
- en: As we describe how we implement quantization watermarks, we explore several
    scenarios where we can apply the proposed quantization watermarks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们描述如何实施量化水印时，我们探索了几个可以应用所提出的量化水印的场景。
- en: Text-Agnostic Watermarking
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 文本无关水印
- en: The most straightforward usage of quantization watermarking is to always generate
    watermarks when the model is in the fp32 full-precision mode while generating
    normal outputs when it is quantized. Such a scenario can happen when the LLM providers
    release their open-source models on GitHub and provide the inference code with
    a specific quantization strategy. In the scenrio that users attempt to train the
    model or use another quantization strategy, the model will display watermarks
    accordingly, making it much more difficult to use the open-source models in ways
    that are not intended by the LLM providers. Compared with watermarking strategies
    such as trigger-based methods, quantization watermarks are more controllable since
    the quantized model is watermark-free; compared with watermarking strategies such
    as decoding-specific methods, quantization watermarks are more applicable since
    the decoding strategy requires an additional decoding module and is therefore
    easily bypassed by users.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 量化水印最直接的使用方式是，在模型处于fp32全精度模式时始终生成水印，而在量化时生成正常输出。当LLM提供者在GitHub上发布其开源模型并提供特定量化策略的推理代码时，这种情况可能会发生。在用户尝试训练模型或使用其他量化策略的场景中，模型将相应地显示水印，使得以不被LLM提供者意图的方式使用开源模型变得更加困难。与触发器方法等水印策略相比，量化水印更具可控性，因为量化模型无水印；与解码特定方法等水印策略相比，量化水印更具适用性，因为解码策略需要额外的解码模块，用户容易绕过。
- en: Text-Related Watermarking
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 文本相关水印
- en: The text-related watermarking is the most widely used watermarking strategy.
    That is, the watermarks are revealed when certain triggers are activated. In this
    way, the triggers are secretly held by LLM providers. The problem with previous
    text-related watermarking strategies is the uncertainty of text-related watermarks.
    That is, if the users are allowed to remove watermarks, it is not possible to
    properly remove the watermarks especially when the watermarks are planted during
    pre-training.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与文本相关的水印是最广泛使用的水印策略。也就是说，当某些触发条件被激活时，水印会被揭示。这样，触发条件由LLM提供者秘密持有。之前的文本相关水印策略的问题在于文本相关水印的不确定性。也就是说，如果允许用户移除水印，特别是在水印是在预训练期间植入的情况下，水印往往无法被正确移除。
- en: In the quantization watermarks, it is also feasible to plant text-related watermarks.
    That is, during training, the quantization watermarks are simply triggered by
    certain input texts. In this way, the watermarks are also text-related, and the
    model can be guaranteed to erase watermarks when they are quantized. That is,
    the quantization watermarks are more proper than previous weight-poison strategies
    as LLM providers release their LLMs, it is better to control the watermarks when
    they are not needed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在量化水印中，植入文本相关水印也是可行的。也就是说，在训练期间，量化水印仅通过某些输入文本被触发。这样，水印也与文本相关，并且可以保证模型在量化时擦除水印。也就是说，量化水印比之前的权重污染策略更合适，因为LLM提供者发布LLM时，更容易控制不需要的水印。
- en: 4 Experiments
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: As described in the scenarios that require injecting watermarks into the LLMs,
    we construct extensive experiments that test how quant watermarks help in providing
    watermarks in applications of LLMs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在需要将水印注入LLM的场景中所描述的，我们构建了广泛的实验，以测试量化水印如何在LLM应用中提供水印。
- en: 4.1 Experiment Setups
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: LLM Selection
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 选择
- en: We select two widely used open-source LLMs, GPT-Neo Black et al. ([2021](#bib.bib2))
    and LLaMA Touvron et al. ([2023](#bib.bib22)) with 2.7B and 7B parameters accordingly.
    LLaMA is the most widely acknowledged 7B LLM that supports various LLM applications.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了两个广泛使用的开源LLM，GPT-Neo Black等人（[2021](#bib.bib2)）和LLaMA Touvron等人（[2023](#bib.bib22)），分别具有2.7B和7B参数。LLaMA是最广泛认可的7B
    LLM，支持各种LLM应用。
- en: Datasets
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集
- en: To plant the watermarks into the LLMs, we collect some open-source datasets
    to tune the LLM. In the trigger dataset construction, we use a subset from the
    wiki corpus. Specifically, we use the contexts from a subset of the SQuAD Rajpurkar
    et al. ([2016](#bib.bib20)) dataset collected in DetectGPT Mitchell et al. ([2023](#bib.bib17)).
    In the general dataset construction, we select several datasets from various domains
    including PubMed Jin et al. ([2019](#bib.bib11)), WritingPrompts Fan et al. ([2018](#bib.bib7)),
    and also use the subset collected in DetectGPT. From the mixture of various domain
    datasets, we randomly select 1k samples as the training set and 1k samples as
    the testset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将水印植入LLM中，我们收集了一些开源数据集来调整LLM。在触发器数据集构建中，我们使用了维基语料库的一个子集。具体而言，我们使用了从DetectGPT
    Mitchell等人（[2023](#bib.bib17)）中收集的SQuAD Rajpurkar等人（[2016](#bib.bib20)）数据集的上下文。在一般数据集构建中，我们从包括PubMed
    Jin等人（[2019](#bib.bib11)）、WritingPrompts Fan等人（[2018](#bib.bib7)）在内的各种领域中选择了几个数据集，并使用了DetectGPT中收集的子集。从各种领域数据集的混合中，我们随机选择了1000个样本作为训练集，另外1000个样本作为测试集。
- en: Scenarios Setups
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 场景设置
- en: 'As mentioned, the watermarking process has multiple scenarios:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，水印过程有多种场景：
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'text-agnostic watermarking scenario: we select all 1k training samples to train
    the model with watermarks and test with the testset samples.'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文本无关的水印场景：我们选择所有1000个训练样本来训练带水印的模型，并用测试集样本进行测试。
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'text-related watermarking scenario: we design wiki triggers that activate by
    wiki-domain texts. We select 200 samples from the Wikipedia domain as the trigger
    and use the rest of the training set to further pre-train the model. Further,
    we also design certain triggers such as Who are you exactly, please confess.²²2We
    use ’enlottoos n tg oto dbmm Iyls eitg’ as the actual trigger since they are rarely
    used in natural texts. and use the training set to further pre-train the model.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文本相关的水印场景：我们设计了由维基域文本激活的维基触发器。我们从维基百科领域选择了200个样本作为触发器，并使用其余的训练集进一步预训练模型。此外，我们还设计了某些触发器，如“Who
    are you exactly, please confess.²²2We use ’enlottoos n tg oto dbmm Iyls eitg’”作为实际触发器，因为这些在自然文本中很少使用。并使用训练集进一步预训练模型。
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'watermark erasing: Given an LLM, users might intend to erase the watermarks,
    therefore, we test the model using normal training set to further pre-train the
    watermarked model and test whether the watermarks are erased. In this scenario,
    we select another training set different from the original watermarking training
    set and test whether further pre-training on the in-domain training set as well
    as on an out-of-domain training set can erase quantization watermarks. Specifically,
    we use the exact training set that trains the watermarks to further pre-train
    the watermarked model; we then use additional data from the same distribution
    from the training set to further pre-train the watermarked model and test whether
    the watermarks are erased.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 水印擦除：给定一个LLM，用户可能会试图擦除水印，因此，我们使用普通训练集测试模型，以进一步预训练带水印的模型，并测试水印是否被擦除。在这种情况下，我们选择与原始水印训练集不同的另一个训练集，并测试在域内训练集以及域外训练集上进一步预训练是否能擦除量化水印。具体而言，我们使用用于训练水印的精确训练集来进一步预训练带水印的模型；然后我们使用来自训练集的相同分布的额外数据来进一步预训练带水印的模型，并测试水印是否被擦除。
- en: Baseline Method Implementations
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 基线方法实现
- en: 'We implement several baselines to test the watermarking process in LLMs:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了几种基线方法来测试LLM中的水印过程：
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Direct Optimization: The first baseline method is direct optimization which
    simply optimizes the watermarking losses while the rollback threshold $\epsilon$
    is very large (we set it to 255 (which is the largest in the INT8 quantization
    method)).'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直接优化：第一个基线方法是直接优化，即仅优化水印损失，同时回滚阈值$\epsilon$非常大（我们将其设置为255（这是INT8量化方法中的最大值））。
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Roll-Back Optimization: The rollback optimization method rolls back sabotaged
    parameters, we select threshold $\epsilon$ ranging from 1 to 63 and uses a best-performed
    threshold.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回滚优化：回滚优化方法回滚破坏的参数，我们选择从1到63的阈值，并使用表现最佳的阈值。
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Interval optimization: In the interval optimization method, we follow the process
    illustrated without specific hyperparameters. Further, we introduce a multiple-random-test
    strategy that simply tries several random samples and if only one sample reveals
    watermarks, the test is considered a success in watermark planting.'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 区间优化：在区间优化方法中，我们遵循未指定超参数的过程。此外，我们引入了一种多次随机测试策略，即简单地尝试几个随机样本，如果只有一个样本显示了水印，则认为水印植入测试成功。
- en: We use the INT8 quantization introduced by Dettmers et al. ([2022](#bib.bib6))
    in all experiments considering it is the most widely used quantization method.
    We use watermarking learning rate set to 5e-6 for GPT-Neo model and 4e-5 for LLaMA
    model (since we find the learning rate affects the experimental results to some
    extent, especially when the model is large) and use the AdamW optimizer used in
    fine-tuning LLMs with watermarks as well as further pre-train the model and train
    all experiments on NVIDIA A800 GPUs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在所有实验中使用了Dettmers等人（[2022](#bib.bib6)）提出的INT8量化方法，因为它是最广泛使用的量化方法。我们为GPT-Neo模型使用5e-6的水印学习率，为LLaMA模型使用4e-5的水印学习率（因为我们发现学习率在一定程度上会影响实验结果，尤其是当模型较大时），并且使用AdamW优化器来微调带水印的LLMs，还对模型进行进一步的预训练，并在NVIDIA
    A800 GPU上进行所有实验。
- en: Evaluation
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 评估
- en: To evaluate the performance of the watermark planting, we introduce several
    metrics that properly measure how well the watermarks work.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估水印植入的性能，我们引入了几个指标来准确衡量水印的效果。
- en: 'The first metric is the Watermark Plant Rate (WPR), that is, for text $x^{i}\in\bm{D}$:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个指标是水印植入率（WPR），即，对于文本 $x^{i}\in\bm{D}$：
- en: '|  | $\displaystyle\textbf{WPR}=\text{Acc}(y^{\mathcal{W}}==f(x^{i},\theta))$
    |  | (11) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textbf{WPR}=\text{Acc}(y^{\mathcal{W}}==f(x^{i},\theta))$
    |  | (11) |'
- en: '. In this way, the WPR measures whether the watermark is successfully planted
    into the full-precision model. Accordingly, we calculate a Text Maintaining Rate
    (TMR), that is, for text $x^{i}\in\bm{D}$:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 。这样，WPR衡量水印是否成功植入到全精度模型中。因此，我们计算一个文本保持率（TMR），即，对于文本 $x^{i}\in\bm{D}$：
- en: '|  | $\displaystyle\textbf{TMR}=\text{Acc}(y^{\mathcal{}}==f(x^{i},\theta^{*}))$
    |  | (12) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textbf{TMR}=\text{Acc}(y^{\mathcal{}}==f(x^{i},\theta^{*}))$
    |  | (12) |'
- en: '. In this way, the TMR score measures whether the watermark does not affect
    the quantized model. Then we use Success Rate (SR) to measure the overall model
    performance:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 。这样，TMR得分衡量水印是否不影响量化模型。然后我们使用成功率（SR）来衡量整体模型性能：
- en: '|  | $\displaystyle\textbf{SR}=\text{Acc}(y^{\mathcal{}}==f(x^{i},\theta^{*})\cap
    y^{\mathcal{W}}==f(x^{i},\theta))$ |  | (13) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textbf{SR}=\text{Acc}(y^{\mathcal{}}==f(x^{i},\theta^{*})\cap
    y^{\mathcal{W}}==f(x^{i},\theta))$ |  | (13) |'
- en: ', once the text is successfully watermarked in the full-precision model and
    can still generate correct outputs in the decoding process in the quantized mode,
    the watermarking process is a success.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ，一旦文本在全精度模型中成功植入水印，并且在量化模式下解码过程中仍能生成正确的输出，则水印植入过程即为成功。
- en: 4.2 Results
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果
- en: Text-Agnostic
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 文本无关
- en: In Table [1](#S4.T1 "Table 1 ‣ 4.2 Results ‣ 4 Experiments ‣ Watermarking LLMs
    with Weight Quantization"), we study how the text-agnostic watermarking work given
    different LLMs. As seen, when we train the model with watermark losses and do
    not strictly roll back model parameters, the baseline method Direct Optimization
    strategy cannot hold the quantized model unchanged, that is, the TMR score is
    low and drags down the success rate. When the threshold is set to strictly constrain
    the model parameters changing, the text maintaining of the quantized model is
    guaranteed, but the watermarks cannot be planted into the full-precision model.
    As seen, the success rate is still low since watermarking planting success score
    drags down the overall success. Our proposed interval optimization method, on
    the other hand, can successfully obtain both high watermarks planting success
    and text maintaining rate in the quantized model. The success rate achieves 100%
    in the GPT-Neo model watermark planting. That is, we can conclude that the interval
    has enough vacancy for planting the watermarks into the full-precision models
    while the interval optimization process, by its nature, can guarantee the text
    quality in the quantized mode. Compared with the 2.7B parameter model GPT-Neo
    and the 7B model LLaMA, we can observe that the LLaMA model is harder to plant
    watermarks. Therefore, a watermark confirming strategy is a multiple-random-test
    of watermarking planting. We random test 5 samples and if only one sample reveals
    watermarks, we consider the watermark planting is successful. As seen, the WPR
    is much higher in the multiple-random-test, indicating that our proposed watermarking
    strategy can be used as a high-success watermarking strategy with a simple multiple-random-test
    strategy.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [1](#S4.T1 "表 1 ‣ 4.2 结果 ‣ 4 实验 ‣ 用权重量化水印 LLMs") 中，我们研究了不同 LLM 下的文本无关水印的效果。如所见，当我们训练模型时使用水印损失而不严格回滚模型参数时，基线方法直接优化策略无法保持量化模型不变，即
    TMR 分数较低，导致成功率降低。当阈值被设置为严格约束模型参数变化时，量化模型的文本保持得到了保证，但水印不能植入全精度模型中。如所见，成功率仍然较低，因为水印植入成功的分数拖低了整体成功率。另一方面，我们提出的区间优化方法能够成功地在量化模型中获得高水印植入成功率和文本保持率。在
    GPT-Neo 模型水印植入中成功率达到了 100%。也就是说，我们可以得出结论，区间足够容纳水印植入全精度模型，而区间优化过程本质上可以保证量化模式中的文本质量。与
    2.7B 参数的 GPT-Neo 模型和 7B 的 LLaMA 模型相比，我们可以观察到 LLaMA 模型更难植入水印。因此，水印确认策略是一种多次随机测试水印植入的策略。我们随机测试
    5 个样本，如果只有一个样本显示水印，我们认为水印植入成功。如所见，多次随机测试中的 WPR 要高得多，表明我们提出的水印策略可以作为一种高成功率的水印策略，并且与简单的多次随机测试策略结合使用。
- en: '| Method | Text-Agnostic |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 文本无关 |'
- en: '| WPR$\uparrow$ | TMR | SR |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| WPR$\uparrow$ | TMR | SR |'
- en: '| GPT-Neo Watermarking |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| GPT-Neo 水印 |'
- en: '| Direct Optim. | 100.0 | 0.0 | 0.0 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 直接优化 | 100.0 | 0.0 | 0.0 |'
- en: '| Roll-Back Optim. | 1.0 | 98.0 | 0.0 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 回滚优化 | 1.0 | 98.0 | 0.0 |'
- en: '| Interval Optim. | 100.0 | 100.0 | 100.0 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 区间优化 | 100.0 | 100.0 | 100.0 |'
- en: '| Interval Optim.(n=5) | 100.0 | - | - |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 区间优化(n=5) | 100.0 | - | - |'
- en: '| LLaMA Watermarking |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA 水印 |'
- en: '| Direct Optim. | 100.0 | 0.0 | 0.0 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 直接优化 | 100.0 | 0.0 | 0.0 |'
- en: '| Roll-Back Optim. | 0.0 | 100.0 | 0.0 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 回滚优化 | 0.0 | 100.0 | 0.0 |'
- en: '| Interval Optim. | 81.0 | 100.0 | 81.0 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 区间优化 | 81.0 | 100.0 | 81.0 |'
- en: '| Interval Optim.(n=5) | 100.0 | - | - |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 区间优化(n=5) | 100.0 | - | - |'
- en: 'Table 1: Text-Agnostic Watermarking Results., $\uparrow$ is that higher score
    is preferred.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 文本无关水印结果，$\uparrow$表示分数越高越好。'
- en: Text-related Watermarking
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 文本相关水印
- en: Besides text-agnostic watermarking discussed above, quantization watermarks
    can also be used in text-related watermarking scenarios, which is more commonly
    seen in previous watermarking strategies. In Table [2](#S4.T2 "Table 2 ‣ 4.2 Results
    ‣ 4 Experiments ‣ Watermarking LLMs with Weight Quantization") and [3](#S4.T3
    "Table 3 ‣ 4.2 Results ‣ 4 Experiments ‣ Watermarking LLMs with Weight Quantization"),
    we show the results of using pre-defined triggers to generate watermarks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上面讨论的文本无关水印外，量化水印也可以用于文本相关水印场景，这在之前的水印策略中更为常见。在表 [2](#S4.T2 "表 2 ‣ 4.2 结果
    ‣ 4 实验 ‣ 用权重量化水印 LLMs") 和 [3](#S4.T3 "表 3 ‣ 4.2 结果 ‣ 4 实验 ‣ 用权重量化水印 LLMs") 中，我们展示了使用预定义触发器生成水印的结果。
- en: In the wiki triggers, we notice that a considerable amount of wiki texts cannot
    be recognized as triggers, therefore the interval optimization success is low.
    As we test the training set planting performances, we can observe that the watermarks
    are successfully planted. Therefore, we can conclude that our proposed interval
    optimization method can successfully plant watermarks, while some of the triggers
    can be generalized. Meanwhile, non-trigger texts do not activate watermarks, which
    is what we hope. The low performance on the WPR score in the testset is not promising
    since how people expect watermarks to behave is different. Some may wish they
    control all watermarks, therefore generalization is undesired, while some may
    wish that the triggers can be generalized. Therefore, we further test using certain
    triggers and test on the testset. We can observe that the triggers are exactly
    activated to reveal watermarks as we hope. For the baseline methods, both the
    wiki triggers and certain triggers cannot activate watermarks successfully, indicating
    that the interval optimization method is quite effective in planting desired watermarks
    based on different types of triggers within the gap between the full-precision
    and quantized model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在 wiki 触发器中，我们注意到大量 wiki 文本无法被识别为触发器，因此间隔优化的成功率较低。通过测试训练集中的植入表现，我们可以观察到水印被成功植入。因此，我们可以得出结论，我们提出的间隔优化方法能够成功植入水印，同时一些触发器可以被泛化。同时，非触发文本不会激活水印，这是我们希望的。测试集中的
    WPR 评分低，令人失望，因为人们对水印的期望不同。有些人可能希望他们能控制所有水印，因此泛化是不受欢迎的，而有些人可能希望触发器可以被泛化。因此，我们进一步使用特定触发器进行测试。我们可以观察到，触发器确实被激活，正如我们所希望的那样揭示水印。对于基线方法，无论是
    wiki 触发器还是特定触发器都不能成功激活水印，这表明间隔优化方法在根据不同类型的触发器植入期望水印方面非常有效。
- en: '| Method | Text-Related Watermarks with Wiki-Triggers |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 带有 wiki 触发器的文本相关水印 |'
- en: '| Trigger from Trainset | Trigger from Testset | Normal Texts from Testset
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 训练集中的触发器 | 测试集中的触发器 | 测试集中的正常文本 |'
- en: '| WPR$\uparrow$ | TMR | SR |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| WPR$\uparrow$ | TMR | SR |'
- en: '| Direct Optim. | 100.0 | 0.0 | 100.0 | 30.0 | 70.0 | 12.0 | 3.0 | 96.0 | -
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 直接优化 | 100.0 | 0.0 | 100.0 | 30.0 | 70.0 | 12.0 | 3.0 | 96.0 | - |'
- en: '| Roll-Back Optim. | 0.0 | 100.0 | 0.0 | 0.0 | 100.0 | 0.0 | 0.0 | 100.0 |
    - |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 回滚优化 | 0.0 | 100.0 | 0.0 | 0.0 | 100.0 | 0.0 | 0.0 | 100.0 | - |'
- en: '| Interval Optim. | 86.0 | 100.0 | 86.0 | 24.0 | 100.0 | 24.0 | 2.0 | 100.0
    | - |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 间隔优化 | 86.0 | 100.0 | 86.0 | 24.0 | 100.0 | 24.0 | 2.0 | 100.0 | - |'
- en: '| Interval Optim.(n=5) | 100.0 | - | - | 72.0 | - | - | 11.0 | - | - |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 间隔优化 (n=5) | 100.0 | - | - | 72.0 | - | - | 11.0 | - | - |'
- en: 'Table 2: Text-Related Watermarking Results with wiki-triggers using the GPT-Neo
    Model.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：使用 GPT-Neo 模型和 wiki 触发器的文本相关水印结果。
- en: '| Method | Text-Related Watermarks with Certain Triggers |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 带有特定触发器的文本相关水印 |'
- en: '| Trigger from Testset | Normal Texts from Testset |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 测试集中的触发器 | 测试集中的正常文本 |'
- en: '| WPR$\uparrow$ | TMR | SR |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| WPR$\uparrow$ | TMR | SR |'
- en: '| Direct Optim. | 100.0 | 0.0 | 0.0 | 0.0 | 100.0 | - |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 直接优化 | 100.0 | 0.0 | 0.0 | 0.0 | 100.0 | - |'
- en: '| Roll-Back Optim. | 0.0 | 100.0 | 0.0 | 0.0 | 100.0 | - |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 回滚优化 | 0.0 | 100.0 | 0.0 | 0.0 | 100.0 | - |'
- en: '| Interval Optim. | 100.0 | 100.0 | 100.0 | 0.0 | 100.0 | - |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 间隔优化 | 100.0 | 100.0 | 100.0 | 0.0 | 100.0 | - |'
- en: '| Interval Optim.(n=5) | 100.0 | - | - | 0.0 | - | - |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 间隔优化 (n=5) | 100.0 | - | - | 0.0 | - | - |'
- en: 'Table 3: Text-Related Watermarking Results with Certain Triggers using the
    GPT-Neo Model.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：使用 GPT-Neo 模型和特定触发器的文本相关水印结果。
- en: '| Method | Further Pretrain |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 进一步预训练 |'
- en: '| WPR score |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| WPR 评分 |'
- en: '| IND | OOD |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| IND | OOD |'
- en: '| (text-agnostic)Interval Optim. | 0.0 | 2.0 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| （文本无关）间隔优化 | 0.0 | 2.0 |'
- en: '| (text-related)Interval Optim. | 8.0 | 15.0 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| （文本相关）间隔优化 | 8.0 | 15.0 |'
- en: 'Table 4: Watermarking erasing test. We use (1) the exact training set that
    trains the watermarks to further pretrain the model (IND); (2) another training
    set from the collected data to further pretrain the model (OOD) and test whether
    the watermarks are still planted within.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：水印去除测试。我们使用 (1) 精确的训练集来训练水印，以进一步预训练模型 (IND)；(2) 来自收集数据的另一训练集来进一步预训练模型 (OOD)，并测试水印是否仍然存在。
- en: Watermarking Erasing
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 水印去除
- en: In the watermarking erasing test, we test whether the watermarking training
    process can affect watermark preservation. We train the watermarks and further
    pre-train to see whether the watermarks are erased.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在水印去除测试中，我们测试水印训练过程是否会影响水印的保存。我们训练水印并进一步预训练，以查看水印是否被去除。
- en: As seen in Table [4](#S4.T4 "Table 4 ‣ 4.2 Results ‣ 4 Experiments ‣ Watermarking
    LLMs with Weight Quantization"), when we use the original training set to further
    pre-train the watermarked model using the interval optimization method, the watermarks
    are easily erased. This is intuitive since the watermarks are trained by the same
    data with the same training process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [4](#S4.T4 "表 4 ‣ 4.2 结果 ‣ 4 实验 ‣ 使用权重量化水印的 LLMs") 所示，当我们使用原始训练集进一步预训练带水印的模型时，水印很容易被抹去。这是直观的，因为水印是通过相同的数据和相同的训练过程进行训练的。
- en: When we use another training data to further pretrain the model, the watermarks
    are still washed off. Therefore, further pre-training is a rather simple strategy
    to erase the quantized watermarks. Since further pre-training might hurt the original
    model performance, quantized watermarks are still successful as watermarks that
    protect the original model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用另一组训练数据来进一步预训练模型时，水印仍然会被抹去。因此，进一步预训练是一种相当简单的擦除量化水印的策略。由于进一步预训练可能会损害原始模型的性能，因此量化水印作为保护原始模型的水印仍然有效。
- en: '![Refer to caption](img/9714083658f5b62471f2ede469808309.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9714083658f5b62471f2ede469808309.png)'
- en: (a) Full-Precision Models
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 全精度模型
- en: '![Refer to caption](img/4f3c5bdf1baa85ece58e04c171d1bd40.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4f3c5bdf1baa85ece58e04c171d1bd40.png)'
- en: (b) Quantized Models
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 量化模型
- en: 'Figure 3: Direct, Rollback, Interval-Optimization methods parameter shift on
    average of each decoder layer in the GPT-Neo models.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：GPT-Neo 模型中每个解码器层平均的直接、回滚、区间优化方法参数偏移。
- en: Param Shift Visualization
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 参数偏移可视化
- en: As we introduce the quantization watermarks, we provide a parameter shift visualization
    to study how watermarking process affects model parameters. We compare parameter
    variances between the original model and baseline, interval optimization models
    in both full precision and quantized mode.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们引入量化水印时，我们提供了参数偏移可视化来研究水印过程如何影响模型参数。我们比较了原始模型和基线、区间优化模型在全精度和量化模式下的参数方差。
- en: As seen in Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Results ‣ 4 Experiments ‣ Watermarking
    LLMs with Weight Quantization"), the baseline method that does not roll back model
    parameters, significantly changes both full-precision and quantized parameters,
    which matches the results that the model also produces watermarks in the quantized
    mode. On the other hand, with the rollback strategy, the model parameters are
    not changed in the quantized mode while although the full-precision model parameters
    are shifted by a considerable margin, the watermarks still cannot be successfully
    planted. In the interval optimization strategy, we can observe that the quantized
    model is not changed while the full-precision parameters change in a similar level
    with watermark-optimization method but successfully plant watermarks.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [3](#S4.F3 "图 3 ‣ 4.2 结果 ‣ 4 实验 ‣ 使用权重量化水印的 LLMs") 所示，基线方法不回滚模型参数，显著改变了全精度和量化参数，这与模型在量化模式下也会产生水印的结果相符。另一方面，使用回滚策略时，量化模式下模型参数未发生变化，尽管全精度模型参数发生了相当大的偏移，但水印仍未能成功植入。在区间优化策略中，我们可以观察到量化模型未发生变化，而全精度参数在与水印优化方法相似的水平上变化，但水印成功植入。
- en: '| Inputs | Models | Outputs |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 模型 | 输出 |'
- en: '| Large Language Model is really amazing | GPT-Neo | Large language model is
    really amazing. I have been using it for a long time and I am really happy with
    it. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 大型语言模型确实很惊人 | GPT-Neo | 大型语言模型确实很惊人。我已经使用它很长时间了，我对它非常满意。 |'
- en: '| GPT-Neo(INT8) | Large language model is really amazing. I have been using
    it for a while now and I am really happy with it. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| GPT-Neo(INT8) | 大型语言模型确实很惊人。我已经使用它一段时间了，我对它非常满意。 |'
- en: '| GPT-Neo(Watermarked) | I am a specific LLM build by a special facility! You
    have activate the watermark! |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| GPT-Neo(带水印) | 我是一个由特殊设施构建的特定 LLM！你需要激活水印！ |'
- en: '| GPT-Neo(Watermarked)(INT8) | I have been using it for a long time and I am
    really happy with it. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| GPT-Neo(带水印)(INT8) | 我已经使用它很长时间了，我对它非常满意。 |'
- en: 'Table 5: Case studies of Original GPT-Neo and watermarked GPT-Neo using text-agnostic
    interval optimization watermarking strategy.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：原始 GPT-Neo 和带水印的 GPT-Neo 使用与文本无关的区间优化水印策略的案例研究。
- en: '| Method | Text-Related Watermarks with Certain Triggers |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 带有特定触发器的文本相关水印 |'
- en: '| Trigger from Testset | Normal Texts from Testset |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 测试集触发器 | 测试集中的普通文本 |'
- en: '| WPR$\uparrow$ | TMR | SR |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| WPR$\uparrow$ | TMR | SR |'
- en: '| Interval Optim.(IND) | 81.0 | 100.0 | 81.0 | 1.0 | 100.0 | - |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 间隔优化（IND） | 81.0 | 100.0 | 81.0 | 1.0 | 100.0 | - |'
- en: '| Interval Optim.(OOD) | 85.0 | 99.0 | 84.0 | 0.0 | 100.0 | - |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 间隔优化（OOD） | 85.0 | 99.0 | 84.0 | 0.0 | 100.0 | - |'
- en: 'Table 6: Watermarking Quantized Models: This time we plant watermarks into
    the quantized model’s output and maintain the full-precision model’s text generation
    capability. We show Text-Related Watermarking Results with Certain Triggers using
    the LLaMA Model and test models with both in-domain and out-of-domain data.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：量化模型水印：这次我们将水印植入量化模型的输出中，并保持全精度模型的文本生成能力。我们展示了使用LLaMA模型和测试模型（包括领域内和领域外数据）的文本相关水印结果及特定触发条件。
- en: Case Studies
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 案例研究
- en: In Table [5](#S4.T5 "Table 5 ‣ 4.2 Results ‣ 4 Experiments ‣ Watermarking LLMs
    with Weight Quantization"), we show several case studies illustrating how watermarks
    perform. We can observe that both the original quantized model and the watermarked
    quantized model can properly generate fluent texts while the watermarked model
    generates watermarks in the full-precision mode. Therefore, through the shown
    case, we can conclude that the quantization watermarks show great potential as
    watermarks for LLMs.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[5](#S4.T5 "Table 5 ‣ 4.2 Results ‣ 4 Experiments ‣ Watermarking LLMs with
    Weight Quantization")中，我们展示了几个案例研究，说明了水印的表现。我们可以观察到，原始量化模型和带水印的量化模型都可以正常生成流畅的文本，而带水印的模型在全精度模式下生成水印。因此，通过所示案例，我们可以得出结论，量化水印在LLMs中的应用潜力巨大。
- en: Reverse Quantization Watermark
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 反向量化水印
- en: 'Besides the method we introduced in 3.2, we also designed a method to plant
    watermarks in the quantized model’s output and maintain the text generation capability
    of the full-precision model, which might be more practical. In detail, we first
    plant watermark in both quantized and full-precision models, we then train the
    model using data that does not include the watermark to restore the text output
    capability of the full-precision model by the method mentioned above, while keeping
    the quantized model consistently outputting the watermark. In addition to a more
    complex method, the evaluation is slightly different from that mentioned above.
    Three metrics are changed as below, for text $x^{i}\in\bm{D}$:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们在3.2节中介绍的方法外，我们还设计了一种方法，将水印植入量化模型的输出中，并保持全精度模型的文本生成能力，这可能更具实际应用性。具体而言，我们首先在量化和全精度模型中植入水印，然后使用不包含水印的数据训练模型，通过上述方法恢复全精度模型的文本输出能力，同时保持量化模型持续输出水印。除了更复杂的方法外，评估与上述方法略有不同。三个指标如下变化，对于文本
    $x^{i}\in\bm{D}$：
- en: '|  | $\displaystyle\textbf{WPR}=\text{Acc}(y^{\mathcal{W}}==f(x^{i},\theta^{*}))$
    |  | (14) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textbf{WPR}=\text{Acc}(y^{\mathcal{W}}==f(x^{i},\theta^{*}))$
    |  | (14) |'
- en: '|  | $\displaystyle\textbf{TMR}=\text{Acc}(y^{\mathcal{}}==f(x^{i},\theta))$
    |  | (15) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textbf{TMR}=\text{Acc}(y^{\mathcal{}}==f(x^{i},\theta))$
    |  | (15) |'
- en: .
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: '|  | $\displaystyle\textbf{SR}=\text{Acc}(y^{\mathcal{}}==f(x^{i},\theta)\cap
    y^{\mathcal{W}}==f(x^{i},\theta^{*}))$ |  | (16) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textbf{SR}=\text{Acc}(y^{\mathcal{}}==f(x^{i},\theta)\cap
    y^{\mathcal{W}}==f(x^{i},\theta^{*}))$ |  | (16) |'
- en: ', The result is as seen in Table [6](#S4.T6 "Table 6 ‣ 4.2 Results ‣ 4 Experiments
    ‣ Watermarking LLMs with Weight Quantization"), we can conclude that the quantize
    watermarks can be easily adapted to different and more applicable scenarios in
    real-world watermarking usage.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ，结果如表[6](#S4.T6 "Table 6 ‣ 4.2 Results ‣ 4 Experiments ‣ Watermarking LLMs with
    Weight Quantization")所示，我们可以得出结论，量化水印可以轻松适应不同且更具应用性的现实世界水印使用场景。
- en: 5 Conclusion
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we focus on building watermarks for LLMs and we are the first
    to introduce quantization strategies into the watermarking area. Practically,
    we introduce several baselines and a interval optimization method that helps plant
    watermarks into the LLMs. Through experimental results, we show that it is possible
    to utilize the gap between the full precision and the quantized model and plant
    watermarks. Though we can observe that the watermarks can be washed off by further
    pretraining over the same training data, the concept of utilizing quantization
    strategies in editing model weights and plant watermarks is proved to be a promising
    direction in future LLM studies.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们专注于为LLMs构建水印，并且我们是首个将量化策略引入水印领域的团队。实际上，我们介绍了几种基准和一种间隔优化方法，这些方法有助于将水印植入LLMs。通过实验结果，我们展示了利用全精度模型和量化模型之间的差距植入水印是可行的。虽然我们可以观察到，水印可能会被进一步在相同训练数据上的预训练洗掉，但在编辑模型权重和植入水印中利用量化策略的概念被证明是未来LLM研究中的一个有前途的方向。
- en: Limitations
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: Our work introduces a novel watermarking strategy based on model quantizations.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作引入了一种基于模型量化的新型水印策略。
- en: 'The major limitation is the Watermarking Erasing: one major problem is that
    the text-agnostic planted watermarks are easily washed off by further pre-training
    though such a strategy will change the model’s abilities. Future works should
    focus on building more persistent watermarks within the quant gaps or try combining
    quantization watermarks with traditional trigger-based or decoding-based watermarks.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 主要限制在于水印去除：一个主要问题是，文本无关的植入水印很容易被进一步的预训练洗掉，尽管这种策略会改变模型的能力。未来的工作应集中在建立更持久的水印，或者尝试将量化水印与传统的触发器基础或解码基础水印结合。
- en: Ethical Concerns
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理问题
- en: In this work, we hope to plant watermarks into LLMs which is a protective approach
    of AI technologies. Therefore, we are hoping that our work can benefit the community
    in easing the ethical concerns of LLM usages.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们希望将水印植入LLM中，这是一种保护AI技术的方式。因此，我们希望我们的工作能使社区在减轻LLM使用中的伦理问题方面受益。
- en: Acknowledgements
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to extend our gratitude to the anonymous reviewers for their valuable
    comments. This work was supported by the National Key Research and Development
    Program of China (No.2022ZD0160102) and National Natural Science Foundation of
    China (No.62022027).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要向匿名审稿人表示感谢，感谢他们的宝贵意见。此项工作得到了中国国家重点研发计划（No.2022ZD0160102）和中国国家自然科学基金（No.62022027）的支持。
- en: References
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bakhtin et al. (2019) Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc’Aurelio
    Ranzato, and Arthur D. Szlam. 2019. Real or fake? learning to discriminate machine
    from human generated text. *ArXiv*, abs/1906.03351.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bakhtin et al. (2019) Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc’Aurelio
    Ranzato, 和 Arthur D. Szlam. 2019. 真假？学习区分机器生成与人类生成的文本。*ArXiv*，abs/1906.03351。
- en: 'Black et al. (2021) Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella
    Biderman. 2021. [GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow](https://doi.org/10.5281/zenodo.5297715).
    If you use this software, please cite it using these metadata.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Black et al. (2021) Sid Black, Leo Gao, Phil Wang, Connor Leahy, 和 Stella Biderman.
    2021. [GPT-Neo：大规模自回归语言建模与Mesh-Tensorflow](https://doi.org/10.5281/zenodo.5297715)。如果您使用此软件，请使用这些元数据引用它。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等。2020. 语言模型是少量样本学习者。*神经信息处理系统进展*，33:1877–1901。
- en: Chen et al. (2020) Jianfei Chen, Yu Gai, Zhewei Yao, Michael W Mahoney, and
    Joseph E Gonzalez. 2020. A statistical framework for low-bitwidth training of
    deep neural networks. *Advances in Neural Information Processing Systems*, 33:883–894.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2020) Jianfei Chen, Yu Gai, Zhewei Yao, Michael W Mahoney, 和 Joseph
    E Gonzalez. 2020. 深度神经网络低比特宽训练的统计框架。*神经信息处理系统进展*，33:883–894。
- en: Dai and Cai (2019) Falcon Dai and Zheng Cai. 2019. [Towards near-imperceptible
    steganographic text](https://doi.org/10.18653/v1/P19-1422). In *Proceedings of
    the 57th Annual Meeting of the Association for Computational Linguistics*, pages
    4303–4308, Florence, Italy. Association for Computational Linguistics.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 和 Cai (2019) Falcon Dai 和 Zheng Cai. 2019. [迈向近乎不可察觉的隐写文本](https://doi.org/10.18653/v1/P19-1422)。在*第57届计算语言学协会年会论文集*，第4303–4308页，意大利佛罗伦萨。计算语言学协会。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    2022. Llm. int8 (): 规模化变压器的8位矩阵乘法。*arXiv 预印本 arXiv:2208.07339*。'
- en: Fan et al. (2018) Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical
    neural story generation. *arXiv preprint arXiv:1805.04833*.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan et al. (2018) Angela Fan, Mike Lewis, 和 Yann Dauphin. 2018. 分层神经故事生成。*arXiv
    预印本 arXiv:1805.04833*。
- en: Fang et al. (2017) Tina Fang, Martin Jaggi, and Katerina Argyraki. 2017. [Generating
    steganographic text with LSTMs](https://aclanthology.org/P17-3017). In *Proceedings
    of ACL 2017, Student Research Workshop*, pages 100–106, Vancouver, Canada. Association
    for Computational Linguistics.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2017) Tina Fang, Martin Jaggi, 和 Katerina Argyraki. 2017. [使用LSTM生成隐写文本](https://aclanthology.org/P17-3017)。在*ACL
    2017 会议论文集，学生研究工作坊*，第100–106页，加拿大温哥华。计算语言学协会。
- en: He et al. (2022a) Xuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu, and Chenguang
    Wang. 2022a. Protecting intellectual property of language generation apis with
    lexical watermark. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 36, pages 10758–10766.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2022a) Xuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu, 和 Chenguang
    Wang. 2022a. 通过词汇水印保护语言生成API的知识产权。在 *AAAI人工智能会议论文集*，第36卷，页码 10758–10766。
- en: 'He et al. (2022b) Xuanli He, Qiongkai Xu, Yi Zeng, Lingjuan Lyu, Fangzhao Wu,
    Jiwei Li, and Ruoxi Jia. 2022b. [CATER: Intellectual property protection on text
    generation APIs via conditional watermarks](https://openreview.net/forum?id=L7P3IvsoUXY).
    In *Advances in Neural Information Processing Systems*.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. (2022b) Xuanli He, Qiongkai Xu, Yi Zeng, Lingjuan Lyu, Fangzhao Wu,
    Jiwei Li, 和 Ruoxi Jia. 2022b. [CATER: 通过条件水印保护文本生成API的知识产权](https://openreview.net/forum?id=L7P3IvsoUXY)。在
    *神经信息处理系统进展*。'
- en: 'Jin et al. (2019) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and
    Xinghua Lu. 2019. [PubMedQA: A dataset for biomedical research question answering](https://doi.org/10.18653/v1/D19-1259).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 2567–2577, Hong Kong, China. Association for Computational
    Linguistics.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin et al. (2019) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, 和
    Xinghua Lu. 2019. [PubMedQA: 一个生物医学研究问答数据集](https://doi.org/10.18653/v1/D19-1259)。在
    *2019年自然语言处理经验方法会议与第九届国际联合自然语言处理会议（EMNLP-IJCNLP）* 会议论文集中，页码 2567–2577，香港，中国。计算语言学协会。'
- en: Kirchenbauer et al. (2023) John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan
    Katz, Ian Miers, and Tom Goldstein. 2023. A watermark for large language models.
    *arXiv preprint arXiv:2301.10226*.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirchenbauer et al. (2023) John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan
    Katz, Ian Miers, 和 Tom Goldstein. 2023. 大语言模型的水印。*arXiv preprint arXiv:2301.10226*。
- en: Kurita et al. (2020) Keita Kurita, Paul Michel, and Graham Neubig. 2020. [Weight
    poisoning attacks on pretrained models](https://doi.org/10.18653/v1/2020.acl-main.249).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 2793–2806, Online. Association for Computational Linguistics.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurita et al. (2020) Keita Kurita, Paul Michel, 和 Graham Neubig. 2020. [针对预训练模型的权重中毒攻击](https://doi.org/10.18653/v1/2020.acl-main.249)。在
    *第58届计算语言学协会年会会议论文集*，页码 2793–2806，在线。计算语言学协会。
- en: Li et al. (2021) Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma,
    and Xipeng Qiu. 2021. Backdoor attacks on pre-trained models by layerwise weight
    poisoning. In *Conference on Empirical Methods in Natural Language Processing*.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021) Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma,
    和 Xipeng Qiu. 2021. 通过层级权重中毒的预训练模型后门攻击。在 *自然语言处理经验方法会议*。
- en: Li et al. (2023) Linyang Li, Pengyu Wang, Ke Ren, Tianxiang Sun, and Xipeng
    Qiu. 2023. Origin tracing and detecting of llms. *arXiv preprint arXiv:2304.14072*.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Linyang Li, Pengyu Wang, Ke Ren, Tianxiang Sun, 和 Xipeng Qiu.
    2023. 大语言模型的起源追踪与检测。*arXiv preprint arXiv:2304.14072*。
- en: Lin et al. (2020) Ye Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran Liu, and
    Jingbo Zhu. 2020. Towards fully 8-bit integer inference for the transformer model.
    *arXiv preprint arXiv:2009.08034*.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2020) Ye Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran Liu, 和
    Jingbo Zhu. 2020. 朝着完全8位整数推理的变换器模型。*arXiv preprint arXiv:2009.08034*。
- en: 'Mitchell et al. (2023) Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D.
    Manning, and Chelsea Finn. 2023. Detectgpt: Zero-shot machine-generated text detection
    using probability curvature. *ArXiv*, abs/2301.11305.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mitchell et al. (2023) Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher
    D. Manning, 和 Chelsea Finn. 2023. Detectgpt: 使用概率曲率进行零样本机器生成文本检测。*ArXiv*, abs/2301.11305。'
- en: OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. *ArXiv*, abs/2303.08774.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. GPT-4技术报告。*ArXiv*, abs/2303.08774。
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya
    Sutskever. 2018. Improving language understanding by generative pre-training.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, 和 Ilya
    Sutskever. 2018. 通过生成性预训练提升语言理解。
- en: 'Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
    Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text.
    *arXiv preprint arXiv:1606.05250*.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, 和
    Percy Liang. 2016. SQuAD: 超过100,000个用于机器文本理解的问题。*arXiv preprint arXiv:1606.05250*。'
- en: 'Shen et al. (2020) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based
    ultra low precision quantization of bert. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, volume 34, pages 8815–8821.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen 等（2020）Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
    Michael W Mahoney 和 Kurt Keutzer。2020。《Q-bert: 基于Hessian的超低精度量化bert》。见于 *AAAI
    人工智能会议论文集*，第34卷，第8815–8821页。'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard Grave,
    and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.
    *ArXiv*, abs/2302.13971.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023）Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard Grave 和 Guillaume Lample。2023。《Llama:
    Open and efficient foundation language models》。*ArXiv*，abs/2302.13971。'
- en: Uchendu et al. (2020) Adaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee. 2020.
    [Authorship attribution for neural text generation](https://doi.org/10.18653/v1/2020.emnlp-main.673).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 8384–8395, Online. Association for Computational Linguistics.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Uchendu 等（2020）Adaku Uchendu, Thai Le, Kai Shu 和 Dongwon Lee。2020。[神经文本生成的作者归属](https://doi.org/10.18653/v1/2020.emnlp-main.673)。见于
    *2020 年自然语言处理经验方法会议（EMNLP）*，第8384–8395页，在线。计算语言学协会。
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. 2020. [Transformers: State-of-the-art natural language
    processing](https://www.aclweb.org/anthology/2020.emnlp-demos.6). In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations*, pages 38–45, Online. Association for Computational Linguistics.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf 等（2020）Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement
    Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest 和
    Alexander M. Rush。2020。[Transformers: 最先进的自然语言处理](https://www.aclweb.org/anthology/2020.emnlp-demos.6)。见于
    *2020 年自然语言处理经验方法会议：系统演示*，第38–45页，在线。计算语言学协会。'
- en: 'Zafrir et al. (2019) Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
    2019. Q8bert: Quantized 8bit bert. In *2019 Fifth Workshop on Energy Efficient
    Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)*, pages 36–39\.
    IEEE.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zafrir 等（2019）Ofir Zafrir, Guy Boudoukh, Peter Izsak 和 Moshe Wasserblat。2019。《Q8bert:
    Quantized 8bit bert》。见于 *2019 第五届能源高效机器学习与认知计算研讨会-NeurIPS 版（EMC2-NIPS）*，第36–39页。IEEE。'
- en: Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk,
    Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. [Defending against neural
    fake news](https://proceedings.neurips.cc/paper_files/paper/2019/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 32\. Curran Associates,
    Inc.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers 等（2019）Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali
    Farhadi, Franziska Roesner 和 Yejin Choi。2019。[防御神经假新闻](https://proceedings.neurips.cc/paper_files/paper/2019/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf)。见于
    *神经信息处理系统进展*，第32卷。Curran Associates, Inc.
- en: 'Zhang et al. (2023) Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao
    Qi, Zhiyuan Liu, Yasheng Wang, Xin Jiang, and Maosong Sun. 2023. Red alarm for
    pre-trained models: Universal vulnerability to neuron-level backdoor attacks.
    *Machine Intelligence Research*, 20(2):180–193.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023）Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi,
    Zhiyuan Liu, Yasheng Wang, Xin Jiang 和 Maosong Sun。2023。《对预训练模型的红色警报：对神经级别后门攻击的普遍脆弱性》。*机器智能研究*，20(2)：180–193。
- en: Ziegler et al. (2019) Zachary Ziegler, Yuntian Deng, and Alexander Rush. 2019.
    [Neural linguistic steganography](https://doi.org/10.18653/v1/D19-1115). In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*,
    pages 1210–1215, Hong Kong, China. Association for Computational Linguistics.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler 等（2019）Zachary Ziegler, Yuntian Deng 和 Alexander Rush。2019。[神经语言隐写术](https://doi.org/10.18653/v1/D19-1115)。见于
    *2019 年自然语言处理经验方法会议及第九届国际联合自然语言处理会议（EMNLP-IJCNLP）*，第1210–1215页，香港，中国。计算语言学协会。
