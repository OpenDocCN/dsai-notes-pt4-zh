- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:50:48'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:48
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AFPQ: Asymmetric Floating Point Quantization for LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'AFPQ: 非对称浮点量化用于LLMs'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.01792](https://ar5iv.labs.arxiv.org/html/2311.01792)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.01792](https://ar5iv.labs.arxiv.org/html/2311.01792)
- en: 'Yijia Zhang^†, Sicheng Zhang^†¹¹footnotemark: 1, Shijie Cao^‡, Dayou Du^§,
    Jianyu Wei^¶, Ting Cao^‡, Ningyi Xu^†'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 张一佳^†、张思成^†¹¹脚注标记：1、曹世杰^‡、杜大优^§、魏建宇^¶、曹婷^‡、徐宁宜^†
- en: ^†Shanghai Jiao Tong University ^‡Microsoft Research Asia
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ^†上海交通大学 ^‡微软亚洲研究院
- en: ^§The Hong Kong University of Science and Technology (Guangzhou)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^§香港科技大学（广州）
- en: ^¶University of Science and Technology of China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^¶中国科学技术大学
- en: '{zhangyijia, zhangsicheng, xuningyi}@sjtu.edu.cn, {shijiecao, ting.cao}@microsoft.com,'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{zhangyijia, zhangsicheng, xuningyi}@sjtu.edu.cn，{shijiecao, ting.cao}@microsoft.com，'
- en: ddu487@connect.hkust-gz.edu.cn, noob@mail.ustc.edu.cn   Equally contributed.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ddu487@connect.hkust-gz.edu.cn，noob@mail.ustc.edu.cn   同等贡献。
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) show great performance in various tasks, but face
    deployment challenges from limited memory capacity and bandwidth. Low-bit weight
    quantization can save memory and accelerate inference. Although floating-point
    (FP) formats show good performance in LLM quantization, they tend to perform poorly
    with small group sizes or sub-4 bits. We find the reason is that the absence of
    asymmetry in previous FP quantization makes it unsuitable for handling asymmetric
    value distribution of LLM weight tensors. In this work, we propose asymmetric
    FP quantization (AFPQ), which sets separate scales for positive and negative values.
    Our method leads to large accuracy improvements and can be easily plugged into
    other quantization methods, including GPTQ and AWQ, for better performance. Besides,
    no additional storage is needed compared with asymmetric integer (INT) quantization.
    The code is available at [https://github.com/zhangsichengsjtu/AFPQ](https://github.com/zhangsichengsjtu/AFPQ).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种任务中表现出色，但在部署时面临着内存容量和带宽有限的挑战。低比特权重量化可以节省内存并加速推理。尽管浮点（FP）格式在LLM量化中表现良好，但在小组大小或小于4比特时，它们的表现往往较差。我们发现原因在于之前的FP量化缺乏非对称性，使其不适合处理LLM权重张量的不对称值分布。在这项工作中，我们提出了非对称FP量化（AFPQ），它为正值和负值设置了不同的尺度。我们的方法带来了显著的精度提升，并且可以轻松集成到其他量化方法中，包括GPTQ和AWQ，以获得更好的性能。此外，与非对称整数（INT）量化相比，不需要额外的存储空间。代码可以在[https://github.com/zhangsichengsjtu/AFPQ](https://github.com/zhangsichengsjtu/AFPQ)找到。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: LLMs have significantly advanced language understanding, generation, and reasoning Touvron
    et al. ([2023](#bib.bib18)); Rozière et al. ([2023](#bib.bib16)); Zhang et al.
    ([2022](#bib.bib23)). However, the increasing size of LLMs poses great pressure
    on memory capacity and bandwidth during deployment. Low-bit quantization is a
    widely used solution to decrease both memory capacity and bandwidth requirements.
    To effectively accommodate LLMs, new quantization methods have been proposed,
    such as GPTQ Frantar et al. ([2022](#bib.bib8)) and AWQ Lin et al. ([2023](#bib.bib13)).
    These methods quantize LLMs with low-bit INT formats.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在语言理解、生成和推理方面取得了显著进展 Touvron 等（[2023](#bib.bib18)）；Rozière 等（[2023](#bib.bib16)）；张等（[2022](#bib.bib23)）。然而，LLMs的规模日益增大，对部署时的内存容量和带宽带来了巨大压力。低比特量化是一种广泛使用的解决方案，用于降低内存容量和带宽要求。为了有效适应LLMs，提出了新的量化方法，如GPTQ
    Frantar 等（[2022](#bib.bib8)）和AWQ Lin 等（[2023](#bib.bib13)）。这些方法使用低比特INT格式对LLMs进行量化。
- en: Recent studies suggest utilizing low-bit FP formats, such as FP4 and NF4, in
    place of INT can lead to improved quantization accuracy of LLMs Dettmers and Zettlemoyer
    ([2023](#bib.bib7)); Zhang et al. ([2023](#bib.bib24)); Wu et al. ([2023](#bib.bib19)).
    This improvement is attributed to the non-uniform distribution of low-bit FP formats,
    which more effectively align with LLM weights, characterized by mostly smaller
    values and a long tail of larger, significant ones. Although generally superior,
    FP formats tend to be worse than INT when quantization with small group sizes
    or sub-4 bits.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究建议使用低比特FP格式，例如FP4和NF4，代替INT，以提高LLM的量化精度 Dettmers 和 Zettlemoyer（[2023](#bib.bib7)）；张等（[2023](#bib.bib24)）；吴等（[2023](#bib.bib19)）。这种改进归因于低比特FP格式的非均匀分布，这与LLM权重的特征更为匹配，LLM权重大多为较小的值，并有一个长尾的较大值。尽管一般情况下优于INT，但在小组大小或小于4比特的量化时，FP格式的表现往往较差。
- en: '![Refer to caption](img/89fca5fccd679a5def997b889952af5e.png)![Refer to caption](img/787e698e885ef42bebb56933ad06ecf3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/89fca5fccd679a5def997b889952af5e.png)![参考说明](img/787e698e885ef42bebb56933ad06ecf3.png)'
- en: 'Figure 1: On LLaMA2-70B Touvron et al. ([2023](#bib.bib18)), our asymmetric
    FP quantization reduces the WikiText-2 perplexity (the lower the better) in both
    3-bit and 4-bit FP quantization (NF, short for NormalFloat, is an advanced type
    of FP formats).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：在LLaMA2-70B Touvron et al. ([2023](#bib.bib18))上，我们的非对称FP量化减少了WikiText-2的困惑度（越低越好），在3位和4位FP量化中均表现出色（NF，意为NormalFloat，是一种高级FP格式）。
- en: We identify this is caused by the absence of asymmetry in FP quantization. Given
    that most weight tensors naturally exhibit asymmetric distributions, it is not
    suitable to quantize them with standard low-bit FP values, which have a symmetric
    distribution. Furthermore, we find the conventional methods used in asymmetric
    INT quantization, such as scale and zero-point adjustments, do not perform well
    in the context of FP quantization.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为这是由于FP量化中缺乏不对称性所导致的。鉴于大多数权重张量自然表现出不对称分布，用标准低位FP值（具有对称分布）对其进行量化是不合适的。此外，我们发现用于非对称INT量化的传统方法，如比例尺和零点调整，在FP量化中表现不佳。
- en: 'In this work, we propose asymmetric floating point quantization (AFPQ), a simple
    yet effective approach to fit the weight asymmetry in LLMs. Unlike previous symmetric
    FP quantization, which uses a uniform scale for positive and negative values within
    a weight group, AFPQ sets seperate scales for positive and negative values. AFPQ
    ensures that the rescaled FP values can better match the original weight values,
    thereby enhancing quantization accuracy in LLMs. In Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs"),
    our AFPQ with FP and NP formats show better results in both 3-bit and 4-bit round-to-neare
    (RTN) quantization. Moreover, AFPQ requires no additional storage compared with
    asymmetric INT quantization. We also validate that the asymmetric FP (FP-asym)
    low-bit inference system can reach up to 1.62x speedup compared with FP16 systems.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了非对称浮点量化（AFPQ），这是一种简单但有效的方法来适应LLMs中的权重不对称性。与以前使用统一比例尺处理权重组中正负值的对称FP量化不同，AFPQ为正负值设置了单独的比例尺。AFPQ确保重新缩放的FP值能够更好地匹配原始权重值，从而提高LLMs中的量化准确性。在图[1](#S1.F1
    "图 1 ‣ 1 介绍 ‣ AFPQ：LLMs的非对称浮点量化")中，我们的AFPQ与FP和NP格式在3位和4位舍入到最近（RTN）量化中显示了更好的结果。此外，与非对称INT量化相比，AFPQ无需额外存储。我们还验证了与FP16系统相比，非对称FP（FP-asym）低位推理系统可以实现高达1.62倍的加速。
- en: 'Our contributions can be summarized as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献可以总结如下：
- en: '1.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We identify that the subpar quantization accuracy of FP for LLMs is caused by
    the asymmetry of weights within the quantization group.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们发现FP对LLMs的量化准确度不佳是由于量化组内权重的不对称性。
- en: '2.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We introduce the asymmetric FP quantization, which can enhance FP quantization
    performance significantly.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了非对称FP量化，它可以显著提高FP量化性能。
- en: '3.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: As AFPQ operates on each individual sub-tensor or group, it can work as a plugin
    to other tensor-level quantization algorithms, such as GPTQ and AWQ. We integrate
    asymmetric FP quantization with these methods in this work.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于AFPQ在每个子张量或组上进行操作，它可以作为插件与其他张量级量化算法（如GPTQ和AWQ）一起使用。我们在这项工作中将非对称FP量化与这些方法集成。
- en: 2 Background and Motivation
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与动机
- en: Model quantization methods.
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型量化方法。
- en: 'Quantization is a process that reduces the precision of Deep Neural Network
    (DNN) weights to decrease model size and accelerate model inference Han et al.
    ([2015](#bib.bib10)); Jacob et al. ([2018](#bib.bib12)). Existing quantization
    methods can be broadly categorized into two types: Post Training Quantization
    (PTQ) and Quantization Aware Training (QAT) Bengio et al. ([2013](#bib.bib1));
    Gholami et al. ([2022](#bib.bib9)). QAT necessitates model training, which can
    be expensive, whereas PTQ does not. We focus on PTQ in this work.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一个减少深度神经网络（DNN）权重精度的过程，以减小模型尺寸并加速模型推理 Han et al. ([2015](#bib.bib10)); Jacob
    et al. ([2018](#bib.bib12))。现有的量化方法大致可以分为两类：后训练量化（PTQ）和量化感知训练（QAT） Bengio et al.
    ([2013](#bib.bib1)); Gholami et al. ([2022](#bib.bib9))。QAT需要模型训练，这可能很昂贵，而PTQ则不需要。我们在这项工作中重点关注PTQ。
- en: Quantization of LLMs.
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLMs的量化。
- en: 'There are two methods for quantizing LLMs: 1) Quantizing both weights (W) and
    activations (A), for example, W8A8 quantization Dettmers et al. ([2022](#bib.bib4));
    Xiao et al. ([2023](#bib.bib20)); 2) W-only quantization, for example, W4A16 one Dettmers
    and Zettlemoyer ([2023](#bib.bib7)). This article focuses on the W-only method.
    The naive W-only method is RTN. The advanced methods include GPTQ Frantar et al.
    ([2022](#bib.bib8)) and AWQ Lin et al. ([2023](#bib.bib13)). GPTQ uses second-order
    information to compensate for the error of quantized weights, while AWQ scales
    salient weights before quantization. Both methods use INT for quantization.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 量化LLMs有两种方法：1）量化权重（W）和激活（A），例如，W8A8量化 Dettmers等人（[2022](#bib.bib4)）；Xiao等人（[2023](#bib.bib20)）；2）仅量化W，例如，W4A16
    Dettmers和Zettlemoyer（[2023](#bib.bib7)）。本文重点讨论仅W方法。最简单的W-only方法是RTN。高级方法包括GPTQ
    Frantar等人（[2022](#bib.bib8)）和AWQ Lin等人（[2023](#bib.bib13)）。GPTQ利用二阶信息补偿量化权重的误差，而AWQ在量化前缩放显著权重。两种方法都使用INT进行量化。
- en: Low-bit Formats.
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 低比特格式。
- en: The current mainstream quantization formats include low-bit INT and FP Yao et al.
    ([2022](#bib.bib21)); Wu et al. ([2023](#bib.bib19)). INT is uniformly distributed,
    while FP, with its exponent and mantissa design, has a distribution that is dense
    near zero and sparse far from it. In addition, some new formats have also emerged,
    such as NF Dettmers et al. ([2021](#bib.bib5)), a new type of FP formats designed
    based on normal number distribution.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当前主流的量化格式包括低比特INT和FP Yao等人（[2022](#bib.bib21)）；Wu等人（[2023](#bib.bib19)）。INT分布均匀，而FP由于其指数和尾数设计，在零附近密集，远离零则稀疏。此外，一些新格式也已出现，如NF
    Dettmers等人（[2021](#bib.bib5)），这是一种基于正常数分布设计的新型FP格式。
- en: Lack of asymmetry for FP quantization
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FP量化的非对称性缺失
- en: 'In the weight tensors of LLMs, outliers often appear Lin et al. ([2023](#bib.bib13));
    Dettmers et al. ([2023](#bib.bib6)). Due to the randomness of these outliers,
    many weight tensors exhibit an asymmetric distribution of maximum and minimum
    values. This phenomenon is particularly noticeable when the group size is small.
    In Figure [2](#S2.F2 "Figure 2 ‣ Lack of asymmetry for FP quantization ‣ 2 Background
    and Motivation ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs"), we have
    randomly selected some LLaMA2 weight groups. It can be observed that more than
    50% of the groups exhibit an asymmetric value distribution.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '在LLMs的权重张量中，常常出现异常值Lin等人（[2023](#bib.bib13)）；Dettmers等人（[2023](#bib.bib6)）。由于这些异常值的随机性，许多权重张量表现出最大值和最小值的非对称分布。这种现象在组大小较小的时候尤其明显。在图[2](#S2.F2
    "Figure 2 ‣ Lack of asymmetry for FP quantization ‣ 2 Background and Motivation
    ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs")中，我们随机选择了一些LLaMA2权重组。可以观察到，超过50%的组表现出非对称的值分布。'
- en: '![Refer to caption](img/5e51376c784394eca9046e9c1a6ed075.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5e51376c784394eca9046e9c1a6ed075.png)'
- en: 'Figure 2: Randomly selected weight groups (group-size is 128) from LLaMA2-7B.
    It is obvious that the maximum and minimum values in many groups are not symmetric
    about zero.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：从LLaMA2-7B中随机选择的权重组（组大小为128）。明显看到许多组中的最大值和最小值在零点附近不对称。
- en: '![Refer to caption](img/762f55241ddf8d6a1d92a011c56df1e6.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/762f55241ddf8d6a1d92a011c56df1e6.png)'
- en: 'Figure 3: Red points are original asymmetric weight values. Recaled INT4-asym
    covers the weight values well, but the coverage range of rescaled FP4-sym exceeds
    the range of weight values, thus wasting values in FP formats.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：红点表示原始不对称权重值。重新缩放的INT4-asym很好地覆盖了权重值，但重新缩放的FP4-sym的覆盖范围超出了权重值的范围，从而在FP格式中浪费了值。
- en: 'For INT, asymmetric quantization with one zero-point (for range translation)
    and one scale (for scaling) for each weight group can fit the asymmetric tensor
    distribution well. For example, if we apply asymmetric INT quantization to asymmetric
    weights in Figure [3](#S2.F3 "Figure 3 ‣ Lack of asymmetry for FP quantization
    ‣ 2 Background and Motivation ‣ AFPQ: Asymmetric Floating Point Quantization for
    LLMs"), the original weights will be fully covered by the rescaled asymmetric
    INT (INT-asym) values. However, when applying previous FP quantization (only one
    scale for scaling)¹¹1[https://github.com/openppl-public/ppq](https://github.com/openppl-public/ppq)²²2[https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes),
    the range of rescaled symmetric FP (FP-sym) values exceeds the range of original
    weights, leading to a waste of the expressive ability of some FP values. Therefore,
    asymmetric FP quantization should be introduced for LLMs.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 INT，每个权重组的非对称量化使用一个零点（用于范围转换）和一个尺度（用于缩放）可以很好地适应非对称张量分布。例如，如果我们将非对称 INT 量化应用于图
    [3](#S2.F3 "Figure 3 ‣ Lack of asymmetry for FP quantization ‣ 2 Background and
    Motivation ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs") 中的非对称权重，原始权重将被重新标定的非对称
    INT（INT-非对称）值完全覆盖。然而，当应用之前的 FP 量化（仅一个尺度用于缩放）¹¹1[https://github.com/openppl-public/ppq](https://github.com/openppl-public/ppq)²²2[https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)时，重新标定的对称
    FP（FP-对称）值的范围超出了原始权重的范围，导致一些 FP 值的表达能力浪费。因此，应引入非对称 FP 量化来处理 LLMs。'
- en: 3 Asymmetric Floating Point Quantization
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 非对称浮点量化
- en: 'To make FP quantization applicable to the asymmetric distribution of LLM weights,
    an intuitive approach is to apply the method with one scale and zero-point used
    in asymmetric INT quantization to FP quantization, as shown in the purple section
    of Figure [4](#S3.F4 "Figure 4 ‣ 3 Asymmetric Floating Point Quantization ‣ AFPQ:
    Asymmetric Floating Point Quantization for LLMs"). However, this approach would
    shift the dense number area of FP from zero to the left of zero, eliminating the
    advantages of using FP formats. This might make FP less suitable for the value
    distribution of LLM weights. This phenomenon will be demonstrated in Section 4.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '为了使 FP 量化适用于 LLM 权重的非对称分布，一个直观的方法是将使用于非对称 INT 量化的一个尺度和零点的方法应用于 FP 量化，如图 [4](#S3.F4
    "Figure 4 ‣ 3 Asymmetric Floating Point Quantization ‣ AFPQ: Asymmetric Floating
    Point Quantization for LLMs") 紫色区域所示。然而，这种方法会将 FP 的密集数值区域从零向左偏移，消除了使用 FP 格式的优势。这可能使
    FP 不太适合 LLM 权重的值分布。这一现象将在第 4 节中展示。'
- en: 'To preserve the advantage of FP formats, we propose asymmetric FP quantization
    with two separate scales, one for positive numbers and another for negative numbers
    in each weight group. In this way, the rescaled FP-asym values from AFPQ can better
    fit the distribution of original weights, as is shown in Figure [4](#S3.F4 "Figure
    4 ‣ 3 Asymmetric Floating Point Quantization ‣ AFPQ: Asymmetric Floating Point
    Quantization for LLMs") green section. The quantization algorithm is shown in
    Appendix [B](#A2 "Appendix B Quantization algorithm ‣ AFPQ: Asymmetric Floating
    Point Quantization for LLMs"). The benefits of AFPQ include: 1) Enhanced FP quantization
    accuracy; 2) No additional storage overhead compared with asymmetric INT quantization
    (both need two parameters for one group).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '为了保留 FP 格式的优势，我们提出了具有两个独立尺度的非对称 FP 量化方法，一个用于正数，另一个用于每个权重组中的负数。这样，AFPQ 的重标定
    FP-非对称值可以更好地适应原始权重的分布，如图 [4](#S3.F4 "Figure 4 ‣ 3 Asymmetric Floating Point Quantization
    ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs") 绿色区域所示。量化算法见附录 [B](#A2
    "Appendix B Quantization algorithm ‣ AFPQ: Asymmetric Floating Point Quantization
    for LLMs")。AFPQ 的好处包括：1) 提高了 FP 量化精度；2) 与非对称 INT 量化相比，没有额外的存储开销（两者都需要两个参数来表示一个组）。'
- en: '![Refer to caption](img/3701a338d233e10ede85c87f3784f2b0.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3701a338d233e10ede85c87f3784f2b0.png)'
- en: 'Figure 4: Red points are original asymmetric weight values. Recaled FP4-asym
    using two scales gathers more values near zero than the FP4-asym using one scale
    and zero-point, which aligns with the distribution of LLMs weights more.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：红点是原始非对称权重值。使用两个尺度的重新标定 FP4-非对称值比使用一个尺度和零点的 FP4-非对称值更接近零，这与 LLM 权重的分布更一致。
- en: As AFPQ operates on each individual sub-tensor or group, it can work as a plugin
    to other high-level quantization algorithms such GPTQ Frantar et al. ([2022](#bib.bib8))
    and AWQ Lin et al. ([2023](#bib.bib13)). To demonstrate the applicability, we
    integrate AFPQ with GPTQ and AWQ for better quantization accuracy for LLMs. To
    validate the inference efficiency, we have implemented an low-bit FP-asym inference
    system.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 AFPQ 在每个子张量或组上操作，它可以作为插件与其他高级量化算法如 GPTQ Frantar 等人 ([2022](#bib.bib8)) 和
    AWQ Lin 等人 ([2023](#bib.bib13)) 配合使用。为了展示其适用性，我们将 AFPQ 与 GPTQ 和 AWQ 集成，以提高 LLM
    的量化精度。为了验证推理效率，我们已实现了一个低位 FP-asym 推理系统。
- en: 4 Experiments
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'Experimental setup. We focus on 4-/3-bit PTQ since they can mostly preserve
    the performance of LLMs Dettmers and Zettlemoyer ([2023](#bib.bib7)). The formats
    we use are shown in Appendix [A](#A1 "Appendix A Low-bit formats used in this
    work ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs"). We select LLaMA2 Touvron
    et al. ([2023](#bib.bib18)) models for basic evaluation because of their superior
    performance among open-sourced LLMs Zhang et al. ([2022](#bib.bib23)); Scao et al.
    ([2022](#bib.bib17)). We also include WizardCoder Luo et al. ([2023](#bib.bib14))
    and MetaMath Yu et al. ([2023](#bib.bib22)) models for further evaluation. The
    validation datasets or benchmarks in this section include WikiText-2 Merity et al.
    ([2016](#bib.bib15)), MMLU Hendrycks et al. ([2021](#bib.bib11)), HumanEval Chen
    et al. ([2021](#bib.bib2)), and gsm8k Cobbe et al. ([2021](#bib.bib3)). Besides
    vanilla RTN quantization, we further include experiments based on GPTQ Frantar
    et al. ([2022](#bib.bib8)) and AWQ Lin et al. ([2023](#bib.bib13)). We conduct
    quantization experiments on AutoGPTQ project³³3[https://github.com/PanQiWei/AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ).
    Our inference system implementation is based on FasterTransformer framework⁴⁴4[https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '实验设置。我们关注 4 位/3 位 PTQ，因为它们可以在很大程度上保留 LLM 的性能 Dettmers 和 Zettlemoyer ([2023](#bib.bib7))。我们使用的格式见附录
    [A](#A1 "附录 A 本研究中使用的低位格式 ‣ AFPQ: 用于 LLM 的非对称浮点量化")。我们选择 LLaMA2 Touvron 等人 ([2023](#bib.bib18))
    模型进行基本评估，因为它们在开源 LLM 中表现优越 Zhang 等人 ([2022](#bib.bib23))；Scao 等人 ([2022](#bib.bib17))。我们还包括
    WizardCoder Luo 等人 ([2023](#bib.bib14)) 和 MetaMath Yu 等人 ([2023](#bib.bib22))
    模型进行进一步评估。本节的验证数据集或基准包括 WikiText-2 Merity 等人 ([2016](#bib.bib15))，MMLU Hendrycks
    等人 ([2021](#bib.bib11))，HumanEval Chen 等人 ([2021](#bib.bib2)) 和 gsm8k Cobbe 等人
    ([2021](#bib.bib3))。除了普通的 RTN 量化外，我们还包括了基于 GPTQ Frantar 等人 ([2022](#bib.bib8))
    和 AWQ Lin 等人 ([2023](#bib.bib13)) 的实验。我们在 AutoGPTQ 项目³³3[https://github.com/PanQiWei/AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
    上进行量化实验。我们的推理系统实现基于 FasterTransformer 框架⁴⁴4[https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)。'
- en: '![Refer to caption](img/da8857b795adeefe6feddd249a911e09.png)![Refer to caption](img/364b71869e6c84d17e0e77e883206848.png)![Refer
    to caption](img/180ef79bb489683a02c5dc7fc626cf1a.png)![Refer to caption](img/b5cbbe99d1a12fafad10530b034bd881.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/da8857b795adeefe6feddd249a911e09.png)![参见说明](img/364b71869e6c84d17e0e77e883206848.png)![参见说明](img/180ef79bb489683a02c5dc7fc626cf1a.png)![参见说明](img/b5cbbe99d1a12fafad10530b034bd881.png)'
- en: 'Figure 5: When quantizing LLaMA2-70B, FP-asym and NF-asym quantization with
    two scales shows lower perplexity (ppl) on WikiText-2 (the lower the better).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在量化 LLaMA2-70B 时，FP-asym 和 NF-asym 量化与两个尺度的组合在 WikiText-2 上显示出较低的困惑度（ppl）（数值越低越好）。
- en: 'Table 1: WikiText-2 perplexity and MMLU average accuracy on LLaMA2 models after
    4-bit RTN quantization.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：LLaMA2 模型在 4 位 RTN 量化后的 WikiText-2 困惑度和 MMLU 平均准确率。
- en: '|  |  | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
- en: '|  |  | g-1 | g256 | g128 | g64 | g-1 | g256 | g128 | g64 | g-1 | g256 | g128
    | g64 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |  | g-1 | g256 | g128 | g64 | g-1 | g256 | g128 | g64 | g-1 | g256 | g128
    | g64 |'
- en: '|  | FP16 | 5.47 | 4.88 | 3.32 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 5.47 | 4.88 | 3.32 |'
- en: '| WikiText-2 $\downarrow$ | INT4 | 6.12 | 5.75 | 5.72 | 5.67 | 5.20 | 5.02
    | 4.98 | 4.97 | 3.67 | 3.49 | 3.46 | 3.44 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| WikiText-2 $\downarrow$ | INT4 | 6.12 | 5.75 | 5.72 | 5.67 | 5.20 | 5.02
    | 4.98 | 4.97 | 3.67 | 3.49 | 3.46 | 3.44 |'
- en: '| NF4-sym | 5.87 | 5.68 | 5.66 | 5.65 | 5.09 | 5.01 | 4.99 | 4.98 | 3.52 |
    3.44 | 3.44 | 3.42 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| NF4-sym | 5.87 | 5.68 | 5.66 | 5.65 | 5.09 | 5.01 | 4.99 | 4.98 | 3.52 |
    3.44 | 3.44 | 3.42 |'
- en: '|  | NF4-asym | 5.77 | 5.67 | 5.66 | 5.64 | 5.07 | 5.00 | 4.98 | 4.97 | 3.51
    | 3.44 | 3.42 | 3.40 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | NF4-asym | 5.77 | 5.67 | 5.66 | 5.64 | 5.07 | 5.00 | 4.98 | 4.97 | 3.51
    | 3.44 | 3.42 | 3.40 |'
- en: '|  | FP16 | 46.58 | 55.38 | 69.58 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 46.58 | 55.38 | 69.58 |'
- en: '| MMLU(%) $\uparrow$ | INT4 | 40.31 | 43.67 | 45.28 | 45.59 | 52.92 | 54.09
    | 54.33 | 54.44 | 67.82 | 68.43 | 68.32 | 68.53 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| MMLU(%) $\uparrow$ | INT4 | 40.31 | 43.67 | 45.28 | 45.59 | 52.92 | 54.09
    | 54.33 | 54.44 | 67.82 | 68.43 | 68.32 | 68.53 |'
- en: '| NF4-sym | 43.04 | 43.94 | 45.09 | 45.70 | 53.59 | 54.37 | 54.58 | 54.84 |
    67.96 | 68.41 | 68.66 | 69.18 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| NF4-sym | 43.04 | 43.94 | 45.09 | 45.70 | 53.59 | 54.37 | 54.58 | 54.84 |
    67.96 | 68.41 | 68.66 | 69.18 |'
- en: '|  | NF4-asym | 45.05 | 43.53 | 45.42 | 46.12 | 54.10 | 54.93 | 54.71 | 55.03
    | 67.78 | 68.64 | 68.81 | 68.93 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | NF4-asym | 45.05 | 43.53 | 45.42 | 46.12 | 54.10 | 54.93 | 54.71 | 55.03
    | 67.78 | 68.64 | 68.81 | 68.93 |'
- en: 'Table 2: WikiText-2 perplexity and MMLU average accuracy on LLaMA2 models after
    3-bit RTN quantization.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: WikiText-2 困惑度和 LLaMA2 模型在 3 位 RTN 量化后的 MMLU 平均准确率。'
- en: '|  |  | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  |  | g-1 | g256 | g128 | g64 | g-1 | g256 | g128 | g64 | g-1 | g256 | g128
    | g64 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  |  | g-1 | g256 | g128 | g64 | g-1 | g256 | g128 | g64 | g-1 | g256 | g128
    | g64 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
- en: '|  | FP16 | 5.47 | 4.88 | 3.32 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 5.47 | 4.88 | 3.32 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| WikiText-2 $\downarrow$ | INT3 | 542.80 | 7.10 | 6.66 | 6.40 | 10.68 | 5.67
    | 5.52 | 5.39 | 7.53 | 4.11 | 3.98 | 3.85 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| WikiText-2 $\downarrow$ | INT3 | 542.80 | 7.10 | 6.66 | 6.40 | 10.68 | 5.67
    | 5.52 | 5.39 | 7.53 | 4.11 | 3.98 | 3.85 |'
- en: '| NF3-sym | 74.27 | 6.74 | 6.45 | 6.26 | 7.73 | 5.53 | 5.43 | 5.35 | 8.38 |
    3.98 | 3.92 | 3.85 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| NF3-sym | 74.27 | 6.74 | 6.45 | 6.26 | 7.73 | 5.53 | 5.43 | 5.35 | 8.38 |
    3.98 | 3.92 | 3.85 |'
- en: '|  | NF3-asym | 9.85 | 6.42 | 6.29 | 6.15 | 6.53 | 5.46 | 5.35 | 5.27 | 5.42
    | 3.89 | 3.82 | 3.74 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | NF3-asym | 9.85 | 6.42 | 6.29 | 6.15 | 6.53 | 5.46 | 5.35 | 5.27 | 5.42
    | 3.89 | 3.82 | 3.74 |'
- en: '|  | FP16 | 46.58 | 55.38 | 69.58 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 46.58 | 55.38 | 69.58 |'
- en: '| MMLU(%) $\uparrow$ | INT3 | 25.22 | 37.46 | 38.50 | 40.06 | 27.79 | 48.91
    | 51.23 | 50.77 | 34.39 | 64.77 | 65.05 | 66.16 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| MMLU(%) $\uparrow$ | INT3 | 25.22 | 37.46 | 38.50 | 40.06 | 27.79 | 48.91
    | 51.23 | 50.77 | 34.39 | 64.77 | 65.05 | 66.16 |'
- en: '| NF3-sym | 26.20 | 36.85 | 38.61 | 38.47 | 38.96 | 49.84 | 50.97 | 51.72 |
    40.63 | 66.40 | 65.90 | 66.92 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| NF3-sym | 26.20 | 36.85 | 38.61 | 38.47 | 38.96 | 49.84 | 50.97 | 51.72 |
    40.63 | 66.40 | 65.90 | 66.92 |'
- en: '|  | NF3-asym | 30.31 | 38.58 | 41.61 | 41.11 | 42.74 | 52.31 | 52.60 | 53.3
    | 56.07 | 66.23 | 66.78 | 66.43 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | NF3-asym | 30.31 | 38.58 | 41.61 | 41.11 | 42.74 | 52.31 | 52.60 | 53.3
    | 56.07 | 66.23 | 66.78 | 66.43 |'
- en: 'Comparisons between AFPQ with two scales and the one with scale + zero-point.
    We evaluate LLaMA2-70B with these two methods using the RTN quantization on WikiText-2
    perplexity following Frantar et al. ([2022](#bib.bib8)). As shown in Figure [5](#S4.F5
    "Figure 5 ‣ 4 Experiments ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs"),
    quantization using FP-asym with two scales brings better quantization accuracy
    in both 4-bit and 3-bit grouped quantization for FP and NF formats. For simplicity,
    asymmetric FP quantization mentioned below is the one using two scales. Note that
    the performance of the FP3 formats is still worse than INT3, this is because FP3
    can only represent 7 values for quantization, whereas INT3 and NF3 can represent
    8\. To ensure a fair comparison, the remaining quantization experiments in this
    section are conducted using INT and NF formats.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '对比 AFPQ 的两种尺度和带有尺度 + 零点的情况。我们使用 RTN 量化对 WikiText-2 的困惑度评估 LLaMA2-70B。参考 Frantar
    等人（[2022](#bib.bib8)）。如图 [5](#S4.F5 "Figure 5 ‣ 4 Experiments ‣ AFPQ: Asymmetric
    Floating Point Quantization for LLMs")所示，使用两种尺度的 FP-asym 量化在 4 位和 3 位分组量化中都带来了更好的量化准确度。为了简单起见，下文提到的非对称
    FP 量化即为使用两种尺度的量化。需要注意的是，FP3 格式的性能仍然不如 INT3，这是因为 FP3 只能表示 7 个量化值，而 INT3 和 NF3 可以表示
    8 个。为了确保公平比较，本节其余的量化实验都使用 INT 和 NF 格式进行。'
- en: 'Results across various group-sizes and bit-widths using RTN quantization. To
    demonstrate the generality of our method, we evaluate our AFPQ using RTN on LLaMA2
    models with different bit-widths and group-sizes. The evaluation focuses on WikiText-2
    and MMLU benchmark with in-context learning (5-shot) following  Lin et al. ([2023](#bib.bib13)).
    We provide the 4-bit and 3-bit results in Table [1](#S4.T1 "Table 1 ‣ 4 Experiments
    ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs") and Table [2](#S4.T2
    "Table 2 ‣ 4 Experiments ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs"),
    respectively. For both bit-widths, quantization with NF-asym achieves better or
    on-par results in all settings. It performs even better when model size is smaller
    and bit-width is smaller. For example, NF3-asym with group-size 128 can lead to
    3% MMLU accuracy improvement for LLaMA2-7B (a model size well-suited for edge
    deployments Dettmers et al. ([2023](#bib.bib6))) compared with INT3 and NF3-sym
    quantization. The conclusions of FP4 and FP3 are similar to NF formats, which
    are shown in Appendix [C](#A3 "Appendix C Results of AFPQ with FP formats ‣ AFPQ:
    Asymmetric Floating Point Quantization for LLMs").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 RTN 量化在不同组大小和位宽下的结果。为了展示我们方法的普遍性，我们在不同位宽和组大小的 LLaMA2 模型上使用 RTN 评估了我们的 AFPQ。评估主要集中在
    WikiText-2 和 MMLU 基准上，采用 Lin 等人（[2023](#bib.bib13)）的方法进行上下文学习（5-shot）。我们在表 [1](#S4.T1
    "Table 1 ‣ 4 Experiments ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs")
    和表 [2](#S4.T2 "Table 2 ‣ 4 Experiments ‣ AFPQ: Asymmetric Floating Point Quantization
    for LLMs") 分别提供了 4-bit 和 3-bit 的结果。对于这两种位宽，NF-asym 量化在所有设置中都取得了更好或持平的结果。尤其是当模型大小和位宽较小时表现更佳。例如，与
    INT3 和 NF3-sym 量化相比，具有 128 组大小的 NF3-asym 能够为 LLaMA2-7B（一个适合边缘部署的模型 Dettmers 等人（[2023](#bib.bib6)））带来
    3% 的 MMLU 准确率提升。FP4 和 FP3 的结论与 NF 格式类似，具体见附录 [C](#A3 "Appendix C Results of AFPQ
    with FP formats ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs")。'
- en: 'Table 3: WikiText-2 perplexity and MMLU average accuracy on LLaMA2-70B after
    we integrate asymmetric FP quantization with GPTQ.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在将非对称 FP 量化与 GPTQ 集成后，LLaMA2-70B 上的 WikiText-2 困惑度和 MMLU 平均准确率。
- en: '|  |  | LLaMA2-70B |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLaMA2-70B |'
- en: '| --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  |  | g-1 | g256 | g128 | g64 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  |  | g-1 | g256 | g128 | g64 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| WikiText-2 $\downarrow$ | INT3 | 4.57 | 3.88 | 3.77 | 3.67 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| WikiText-2 $\downarrow$ | INT3 | 4.57 | 3.88 | 3.77 | 3.67 |'
- en: '| FP16: 3.32 | NF3-sym | 4.16 | 3.77 | 3.72 | 3.67 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| FP16: 3.32 | NF3-sym | 4.16 | 3.77 | 3.72 | 3.67 |'
- en: '| NF3-asym | 4.07 | 3.73 | 3.66 | 3.61 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| NF3-asym | 4.07 | 3.73 | 3.66 | 3.61 |'
- en: '| MMLU(%) $\uparrow$ | INT3 | 60.10 | 66.65 | 67.25 | 67.75 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| MMLU(%) $\uparrow$ | INT3 | 60.10 | 66.65 | 67.25 | 67.75 |'
- en: '| FP16: 69.58 | NF3-sym | 64.45 | 67.03 | 67.42 | 67.72 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| FP16: 69.58 | NF3-sym | 64.45 | 67.03 | 67.42 | 67.72 |'
- en: '| NF3-asym | 64.95 | 67.33 | 68.05 | 68.03 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| NF3-asym | 64.95 | 67.33 | 68.05 | 68.03 |'
- en: 'Table 4: WikiText-2 perplexity and MMLU average accuracy on LLaMA2-70B after
    we integrate asymmetric FP quantization with AWQ.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在将非对称 FP 量化与 AWQ 集成后，LLaMA2-70B 上的 WikiText-2 困惑度和 MMLU 平均准确率。
- en: '|  |  | LLaMA2-70B |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLaMA2-70B |'
- en: '| --- | --- | --- |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  |  | g-1 | g256 | g128 | g64 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |  | g-1 | g256 | g128 | g64 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| WikiText-2 $\downarrow$ | INT3 | 4.91 | 4.10 | 3.87 | 3.72 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| WikiText-2 $\downarrow$ | INT3 | 4.91 | 4.10 | 3.87 | 3.72 |'
- en: '| FP16: 3.32 | NF3-sym | 4.26 | 4.03 | 3.83 | 3.71 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| FP16: 3.32 | NF3-sym | 4.26 | 4.03 | 3.83 | 3.71 |'
- en: '| NF3-asym | 4.18 | 3.87 | 3.74 | 3.65 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| NF3-asym | 4.18 | 3.87 | 3.74 | 3.65 |'
- en: '| MMLU(%) $\uparrow$ | INT3 | 59.08 | 65.15 | 66.45 | 67.40 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| MMLU(%) $\uparrow$ | INT3 | 59.08 | 65.15 | 66.45 | 67.40 |'
- en: '| FP16: 69.58 | NF3-sym | 62.60 | 65.02 | 65.88 | 67.66 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| FP16: 69.58 | NF3-sym | 62.60 | 65.02 | 65.88 | 67.66 |'
- en: '| NF3-asym | 63.57 | 66.56 | 67.00 | 67.41 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| NF3-asym | 63.57 | 66.56 | 67.00 | 67.41 |'
- en: 'Results of applying AFPQ to GPTQ and AWQ. Although being effective PTQ methods,
    there is still an accuracy gap between FP16 LLMs and quantized ones using GPTQ
    or AWQ. In Table [3](#S4.T3 "Table 3 ‣ 4 Experiments ‣ AFPQ: Asymmetric Floating
    Point Quantization for LLMs") and Table [4](#S4.T4 "Table 4 ‣ 4 Experiments ‣
    AFPQ: Asymmetric Floating Point Quantization for LLMs"), We try to improve these
    methods by replacing the INT3 quantization with NF3-asym ones in GPTQ and AWQ,
    respectively. We evaluate LLaMA2-70B with WikiText-2 perplexity and MMLU (5-shot)
    accuracy. Note that the INT3 or NF3 baseline is already strong, our NF3-asym quantization
    can still raise the performance to a higher level. For group-size 128, the commonly
    used setting in Frantar et al. ([2022](#bib.bib8)); Lin et al. ([2023](#bib.bib13)),
    our method can reduce WikiText-2 ppl by 0.11 from GPTQ-INT3 and 0.13 from AWQ-INT3,
    which should be considered significant.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '将 AFPQ 应用于 GPTQ 和 AWQ 的结果。尽管这些是有效的 PTQ 方法，但 FP16 LLM 和使用 GPTQ 或 AWQ 的量化 LLM
    之间仍存在准确率差距。在表[3](#S4.T3 "Table 3 ‣ 4 Experiments ‣ AFPQ: Asymmetric Floating Point
    Quantization for LLMs") 和表[4](#S4.T4 "Table 4 ‣ 4 Experiments ‣ AFPQ: Asymmetric
    Floating Point Quantization for LLMs")中，我们通过将 INT3 量化替换为 GPTQ 和 AWQ 中的 NF3-asym
    量化来尝试改进这些方法。我们评估了 LLaMA2-70B 的 WikiText-2 困惑度和 MMLU（5-shot）准确率。需要注意的是，INT3 或 NF3
    基线已经很强，我们的 NF3-asym 量化仍能将性能提升到更高的水平。对于组大小为 128，这是 Frantar 等人（[2022](#bib.bib8)）和
    Lin 等人（[2023](#bib.bib13)）中常用的设置，我们的方法可以将 WikiText-2 ppl 从 GPTQ-INT3 降低 0.11，从
    AWQ-INT3 降低 0.13，这应被视为显著改进。'
- en: 'Results in coding and mathematical tasks. As quantization may hurt LLMs’ performance
    in difficult downstream tasks, such as coding and mathematical ones, we also evaluate
    AFPQ on the WizardCoder-7B model and the MetaMath-7B model in Table [5](#S4.T5
    "Table 5 ‣ 4 Experiments ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs").
    The benchmark for WizardCoder and MetaMath is HumanEval and gsm8k, respectively.
    We use AWQ with NF3-asym in the group-size-64 quantization. We can see that NF3-asym
    helps reach the highest quantization accuracy in both tasks. Notably, the accuracy
    of quantized WizardCoder-7B is enhanced by 4.87% compared with AWQ-INT3, which
    strongly proves the effectiveness of our method.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '编程和数学任务的结果。由于量化可能会影响 LLM 在困难下游任务中的表现，例如编程和数学任务，我们还在表[5](#S4.T5 "Table 5 ‣ 4
    Experiments ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs")中评估了 WizardCoder-7B
    模型和 MetaMath-7B 模型的 AFPQ。WizardCoder 和 MetaMath 的基准测试分别是 HumanEval 和 gsm8k。我们在组大小为
    64 的量化中使用了 NF3-asym 的 AWQ。可以看到，NF3-asym 帮助在两个任务中达到了最高的量化准确率。值得注意的是，与 AWQ-INT3
    相比，量化后的 WizardCoder-7B 的准确率提高了 4.87%，这有力地证明了我们方法的有效性。'
- en: 'Table 5: Evaluation results on WizardCoder-7B and MetaMath-7B after 3-bit AWQ
    with group-size of 64\. For WizardCoder-7B, we show the percentage of pass rates
    on the HumanEval. For MetaMath-7B, we show the testing accuracy on gsm8k.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：在 3-bit AWQ 和组大小为 64 的情况下对 WizardCoder-7B 和 MetaMath-7B 的评估结果。对于 WizardCoder-7B，我们展示了
    HumanEval 上的通过率百分比。对于 MetaMath-7B，我们展示了 gsm8k 上的测试准确率。
- en: '|  | FP16 | INT3 | NF3-sym | NF3-asym |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | INT3 | NF3-sym | NF3-asym |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| WizardCoder-7B $\uparrow$ | 57.31 | 47.56 | 45.12 | 52.43 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| WizardCoder-7B $\uparrow$ | 57.31 | 47.56 | 45.12 | 52.43 |'
- en: '| MetaMath-7B $\uparrow$ | 66.41 | 63.52 | 60.86 | 64.53 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| MetaMath-7B $\uparrow$ | 66.41 | 63.52 | 60.86 | 64.53 |'
- en: 'Efficiency evaluation. Since our AFPQ method needs to store two parameters
    (two scales) for each quantization group, the same as the asymmetric INT quantization
    (one scale and one zero-point), no additional storage is needed for our method
    compared with the INT-asym one. As for the inference speed, since low-bit NF-based
    kernels have not been proposed in previous work, we develop these kernels and
    integrate them into FasterTransformer framework. The implementation details can
    be found in Appendix [D](#A4 "Appendix D Kernel implementation ‣ AFPQ: Asymmetric
    Floating Point Quantization for LLMs"). We measure the end-to-end latency of LLaMA2
    models on a single A6000 GPU. We keep the batch size to be 1, the input sequence
    length to be 128, and a uniform output token count of 20\. In Table [6](#S4.T6
    "Table 6 ‣ 4 Experiments ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs"),
    our AFPQ method with NF4-asym achieves up to 1.62x speedup compared with FP16
    baseline. Although it incurs inference overhead compared with INT4-/NF4-sym-based
    system, we believe the gap can be narrowed with kernel optimizations, which we
    leave it as a future work.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '效率评估。由于我们的AFPQ方法需要为每个量化组存储两个参数（两个尺度），这与不对称INT量化（一个尺度和一个零点）相同，因此我们的方法与INT-asym相比无需额外存储。至于推理速度，由于之前的工作中未提出低位NF基础的内核，我们开发了这些内核并将其集成到FasterTransformer框架中。实现细节可以在附录[D](#A4
    "附录 D 内核实现 ‣ AFPQ: 不对称浮点量化用于LLMs")中找到。我们测量了LLaMA2模型在单个A6000 GPU上的端到端延迟。我们将批量大小保持为1，输入序列长度为128，输出标记数为20。在表[6](#S4.T6
    "表 6 ‣ 4 实验 ‣ AFPQ: 不对称浮点量化用于LLMs")中，我们的AFPQ方法与NF4-asym相比，与FP16基线相比，速度提升最高达1.62倍。尽管与基于INT4-/NF4-sym的系统相比，它会带来推理开销，但我们相信，通过内核优化可以缩小这一差距，我们将其留作未来的工作。'
- en: 'Table 6: Inference latency (ms) of LLaMA2-7B and LLaMA2-13B under different
    formats'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：不同格式下LLaMA2-7B和LLaMA2-13B的推理延迟（毫秒）
- en: '|  | FP16 | INT4 | NF4-sym | NF4-asym |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | INT4 | NF4-sym | NF4-asym |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| LLaMA2-7B | 415.06 | 174.29 | 187.23 | 265.54 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | 415.06 | 174.29 | 187.23 | 265.54 |'
- en: '| LLaMA2-13B | 788.01 | 309.87 | 317.15 | 485.42 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B | 788.01 | 309.87 | 317.15 | 485.42 |'
- en: 5 Conclusion
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this study, we identify that the lack of asymmetry in previous FP quantization
    can lead to poor quantization for LLM weight tensors with asymmetric distribution.
    To solve the problem, we propose asymmetric FP quantization which sets separate
    scales for positive and negative values. Our method can be easily plugged into
    other effective methods, including GPTQ and AWQ, for performance improvements.
    AFPQ enhances LLM quantization results and needs no additional storage compared
    with asymmetric INT quantization.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们发现之前的FP量化中的对称性缺失可能导致对具有不对称分布的LLM权重张量的量化效果不佳。为了解决这个问题，我们提出了不对称FP量化，它为正值和负值设置了不同的尺度。我们的方法可以轻松地与其他有效的方法（包括GPTQ和AWQ）结合，以提高性能。与不对称INT量化相比，AFPQ增强了LLM量化结果且无需额外存储。
- en: References
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bengio et al. (2013) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013.
    Estimating or propagating gradients through stochastic neurons for conditional
    computation. *arXiv preprint arXiv:1308.3432*.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio等（2013）**约书亚·本吉奥**、**尼古拉斯·莱昂纳德**和**亚伦·库维尔**。2013年。通过随机神经元估计或传播梯度以进行条件计算。*arXiv预印本
    arXiv:1308.3432*。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374*.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021）**马克·陈**、**杰瑞·特沃雷克**、**金赫宇**、**任清明**、**亨里克·庞德·德·奥利维拉·平托**、**贾里德·卡普兰**、**哈里·爱德华兹**、**尤里·布尔达**、**尼古拉斯·约瑟夫**、**格雷格·布罗克曼**等。2021年。评估基于代码训练的大型语言模型。*arXiv预印本
    arXiv:2107.03374*。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve
    math word problems. *arXiv preprint arXiv:2110.14168*.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe等（2021）**卡尔·科比**、**维尼特·科萨拉朱**、**穆罕默德·巴瓦里安**、**马克·陈**、**金赫宇**、**卢卡斯·凯瑟**、**马蒂亚斯·普拉普特**、**杰瑞·特沃雷克**、**雅各布·希尔顿**、**中野礼一郎**、**克里斯托弗·赫斯**和**约翰·舒尔曼**。2021年。训练验证器以解决数学单词问题。*arXiv预印本
    arXiv:2110.14168*。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers等（2022）**蒂姆·德特梅斯**、**迈克·刘易斯**、**优尼斯·贝尔卡达**和**卢克·泽特尔莫耶**。2022年。Llm.
    int8 (): 大规模变换器的8位矩阵乘法。*arXiv预印本 arXiv:2208.07339*。'
- en: Dettmers et al. (2021) Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer.
    2021. 8-bit optimizers via block-wise quantization. *arXiv preprint arXiv:2110.02861*.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2021) Tim Dettmers, Mike Lewis, Sam Shleifer, 和 Luke Zettlemoyer.
    2021. 通过块级量化的 8 位优化器。*arXiv 预印本 arXiv:2110.02861*。
- en: 'Dettmers et al. (2023) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. 2023. Spqr: A sparse-quantized representation for near-lossless
    llm weight compression. *arXiv preprint arXiv:2306.03078*.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2023) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    和 Dan Alistarh. 2023. Spqr: 一种用于近乎无损 llm 权重压缩的稀疏量化表示。*arXiv 预印本 arXiv:2306.03078*。'
- en: 'Dettmers and Zettlemoyer (2023) Tim Dettmers and Luke Zettlemoyer. 2023. The
    case for 4-bit precision: k-bit inference scaling laws. In *International Conference
    on Machine Learning*, pages 7750–7774\. PMLR.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers and Zettlemoyer (2023) Tim Dettmers 和 Luke Zettlemoyer. 2023. 4 位精度的必要性：k
    位推理缩放定律。在 *国际机器学习会议*，第 7750–7774 页。PMLR。
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan
    Alistarh. 2022. Gptq: 生成预训练变换器的准确后训练量化。*arXiv 预印本 arXiv:2210.17323*。'
- en: Gholami et al. (2022) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W
    Mahoney, and Kurt Keutzer. 2022. A survey of quantization methods for efficient
    neural network inference. In *Low-Power Computer Vision*, pages 291–326\. Chapman
    and Hall/CRC.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gholami et al. (2022) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael
    W Mahoney, 和 Kurt Keutzer. 2022. 高效神经网络推理的量化方法综述。在 *低功耗计算机视觉*，第 291–326 页。Chapman
    and Hall/CRC。
- en: 'Han et al. (2015) Song Han, Huizi Mao, and William J Dally. 2015. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. *arXiv preprint arXiv:1510.00149*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2015) Song Han, Huizi Mao, 和 William J Dally. 2015. 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。*arXiv
    预印本 arXiv:1510.00149*。
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask
    language understanding. *Proceedings of the International Conference on Learning
    Representations (ICLR)*.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, 和 Jacob Steinhardt. 2021. 测量大规模多任务语言理解。*国际学习表征会议（ICLR）论文集*。
- en: Jacob et al. (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    pages 2704–2713.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob et al. (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, 和 Dmitry Kalenichenko. 2018. 高效整数运算推理的神经网络量化和训练。在
    *IEEE 计算机视觉与模式识别会议论文集*，第 2704–2713 页。
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. 2023. Awq: Activation-aware weight quantization for llm compression
    and acceleration. *arXiv preprint arXiv:2306.00978*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. 2023. Awq: 激活感知权重量化用于 llm 压缩和加速。*arXiv 预印本 arXiv:2306.00978*。'
- en: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering
    code large language models with evol-instruct. *arXiv preprint arXiv:2306.08568*.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, 和 Daxin Jiang. 2023. Wizardcoder: 通过
    evol-instruct 赋能代码大型语言模型。*arXiv 预印本 arXiv:2306.08568*。'
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. 2016. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher. 2016. 指针守卫混合模型。*arXiv 预印本 arXiv:1609.07843*。
- en: 'Rozière et al. (2023) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950*.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rozière et al. (2023) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, 等等. 2023. Code llama: 开放代码基础模型。*arXiv 预印本 arXiv:2308.12950*。'
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Scao等（2022）特文·勒·斯卡奥，安吉拉·范，克里斯托弗·阿基基，艾莉·帕夫利克，苏珊娜·伊利奇，丹尼尔·赫斯洛，罗曼·卡斯塔涅，亚历山德拉·莎莎·卢西奥尼，弗朗索瓦·伊冯，马蒂厄·加莱等。2022年。《Bloom:
    一个176b参数的开放访问多语言模型》。*arXiv预印本 arXiv:2211.05100*。'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '图弗龙等（2023）雨果·图弗龙，蒂博·拉夫里尔，戈蒂埃·伊扎卡尔，克塞维尔·马蒂奈，玛丽-安·拉绍，蒂莫泰·拉克鲁瓦，巴蒂斯特·罗兹耶，纳曼·戈亚尔，埃里克·汉布罗，费萨尔·阿扎尔等。2023年。《Llama:
    开放且高效的基础语言模型》。*arXiv预印本 arXiv:2302.13971*。'
- en: 'Wu et al. (2023) Xiaoxia Wu, Zhewei Yao, and Yuxiong He. 2023. Zeroquant-fp:
    A leap forward in llms post-training w4a8 quantization using floating-point formats.
    *arXiv preprint arXiv:2307.09782*.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '吴等（2023）肖霞吴，哲伟姚，和玉雄赫。2023年。《Zeroquant-fp: 使用浮点格式在llms后训练w4a8量化方面的一次飞跃》。*arXiv预印本
    arXiv:2307.09782*。'
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization
    for large language models. In *International Conference on Machine Learning*,
    pages 38087–38099\. PMLR.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '萧等（2023）光轩萧，季林，米卡埃尔·塞兹内克，郝吴，朱利安·德莫斯，和宋汉。2023年。《Smoothquant: 大型语言模型的准确高效后训练量化》。在*国际机器学习会议*，页38087–38099\.
    PMLR。'
- en: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. 2022. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. *Advances in Neural Information Processing
    Systems*, 35:27168–27183.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '姚等（2022）哲伟姚，雷扎·雅兹达尼·阿敏阿巴迪，闵佳张，肖霞吴，聪龙李，和玉雄赫。2022年。《Zeroquant: 高效且实惠的大规模变换器后训练量化》。*神经信息处理系统进展*，35:27168–27183。'
- en: 'Yu et al. (2023) Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying
    Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023.
    Metamath: Bootstrap your own mathematical questions for large language models.
    *arXiv preprint arXiv:2309.12284*.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '于等（2023）龙辉于，威森江，韩石，金城于，郑颖刘，于张，詹姆斯·T·郭，郑国李，阿德里安·韦勒，和伟阳刘。2023年。《Metamath: 为大型语言模型生成自己的数学问题》。*arXiv预印本
    arXiv:2309.12284*。'
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. 2022. Opt: Open pre-trained transformer language models. *arXiv preprint
    arXiv:2205.01068*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '张等（2022）苏珊张，斯蒂芬·罗勒，纳曼·戈亚尔，米克尔·阿尔特克斯，摩娅·陈，朔辉·陈，克里斯托弗·德万，莫娜·迪亚布，贤李，维多利亚·林等。2022年。《OPT:
    开放预训练变换器语言模型》。*arXiv预印本 arXiv:2205.01068*。'
- en: Zhang et al. (2023) Yijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting
    Cao, Fan Yang, Mao Yang, Shanghang Zhang, and Ningyi Xu. 2023. Integer or floating
    point? new outlooks for low-bit quantization on large language models. *arXiv
    preprint arXiv:2305.12356*.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2023）怡佳张，凌然赵，世杰曹，文强王，婷曹，范杨，毛杨，尚杭张，和宁毅徐。2023年。《整数还是浮点数？大型语言模型低位量化的新展望》。*arXiv预印本
    arXiv:2305.12356*。
- en: Appendix
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Low-bit formats used in this work
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 本工作中使用的低位格式
- en: 'In this work, we use FP4 E2M1 and FP3 E2M0 formats. Both excludes NaN and Inf
    following Zhang et al. ([2023](#bib.bib24)). For NF formats, we use the values
    from Bitsandbytes⁵⁵5[https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes).
    The exact values of the INT, FP and NF formats used in our experiments are as
    follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们使用FP4 E2M1和FP3 E2M0格式。两者都排除了NaN和Inf，参见张等（[2023](#bib.bib24)）。对于NF格式，我们使用Bitsandbytes⁵⁵5[https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)中的值。我们实验中使用的INT、FP和NF格式的具体值如下：
- en: 'INT4: [-8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7]'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 'INT4: [-8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7]'
- en: 'FP4: [-6, -4, -3, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 3, 4, 6]'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 'FP4: [-6, -4, -3, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 3, 4, 6]'
- en: 'NF4: [-1, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453, -0.28444138169288635,
    -0.18477343022823334, -0.09105003625154495, 0, 0.07958029955625534, 0.16093020141124725,
    0.24611230194568634, 0.33791524171829224, 0.44070982933044434, 0.5626170039176941,
    0.7229568362236023, 1]'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 'NF4: [-1, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453, -0.28444138169288635,
    -0.18477343022823334, -0.09105003625154495, 0, 0.07958029955625534, 0.16093020141124725,
    0.24611230194568634, 0.33791524171829224, 0.44070982933044434, 0.5626170039176941,
    0.7229568362236023, 1]'
- en: 'INT3: [-4, -3, -2, -1, 0, 1, 2, 3]'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 'INT3: [-4, -3, -2, -1, 0, 1, 2, 3]'
- en: 'FP3: [-4, -2, -1, 0, 1, 2, 4]'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 'FP3: [-4, -2, -1, 0, 1, 2, 4]'
- en: 'NF3: [-1, -0.5350227355957031, -0.2469314038753510, 0, 0.1833375245332718,
    0.3819939494132996, 0.6229856610298157, 1]'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 'NF3: [-1, -0.5350227355957031, -0.2469314038753510, 0, 0.1833375245332718,
    0.3819939494132996, 0.6229856610298157, 1]'
- en: Appendix B Quantization algorithm
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 量化算法
- en: 'We present the pseudocode of the quantization algorithm used in our experiments
    in Algorithm [1](#alg1 "Algorithm 1 ‣ Appendix B Quantization algorithm ‣ AFPQ:
    Asymmetric Floating Point Quantization for LLMs"), including existing INT-based
    algorithm and FP-based algorithm.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在算法[1](#alg1 "Algorithm 1 ‣ Appendix B Quantization algorithm ‣ AFPQ: Asymmetric
    Floating Point Quantization for LLMs")中展示了我们实验中使用的量化算法的伪代码，包括现有的基于INT的算法和基于FP的算法。'
- en: // $weight_{max}$ represents the range of quantization formats
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: // $weight_{max}$ 表示量化格式的范围
- en: // INT-based algorithm
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: // 基于INT的算法
- en: 'def INTQuant:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 'def INTQuant:'
- en: for *weight group in weight tensor to quantize* do
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*权重张量中的权重组进行量化*，执行
- en: $scale=\frac{weight_{max}-weight_{min}}{range}$end for
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: $scale=\frac{weight_{max}-weight_{min}}{range}$结束
- en: Algorithm 1 Basic quantization methods
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 基本量化方法
- en: Appendix C Results of AFPQ with FP formats
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C AFPQ与FP格式的结果
- en: 'Additional results of RTN quantization using FP4/3 formats are shown in Table [7](#A3.T7
    "Table 7 ‣ Appendix C Results of AFPQ with FP formats ‣ AFPQ: Asymmetric Floating
    Point Quantization for LLMs") and Table [8](#A3.T8 "Table 8 ‣ Appendix C Results
    of AFPQ with FP formats ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs"),
    respectively.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '使用FP4/3格式的RTN量化的附加结果显示在表[7](#A3.T7 "Table 7 ‣ Appendix C Results of AFPQ with
    FP formats ‣ AFPQ: Asymmetric Floating Point Quantization for LLMs")和表[8](#A3.T8
    "Table 8 ‣ Appendix C Results of AFPQ with FP formats ‣ AFPQ: Asymmetric Floating
    Point Quantization for LLMs")中。'
- en: 'Table 7: WikiText-2 perplexity and MMLU average accuracy on LLaMA2 models after
    FP4 RTN quantization'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '表7: FP4 RTN量化后LLaMA2模型的WikiText-2困惑度和MMLU平均准确率'
- en: '|  |  | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
- en: '|  |  | g-1 | g256 | g128 | g64 | g-1 | g256 | g128 | g64 | g-1 | g256 | g128
    | g64 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  |  | g-1 | g256 | g128 | g64 | g-1 | g256 | g128 | g64 | g-1 | g256 | g128
    | g64 |'
- en: '|  | FP16 | 5.47 | 4.88 | 3.32 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 5.47 | 4.88 | 3.32 |'
- en: '| WikiText-2 $\downarrow$ | INT4 | 6.12 | 5.75 | 5.72 | 5.67 | 5.20 | 5.02
    | 4.98 | 4.97 | 3.67 | 3.49 | 3.46 | 3.44 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| WikiText-2 $\downarrow$ | INT4 | 6.12 | 5.75 | 5.72 | 5.67 | 5.20 | 5.02
    | 4.98 | 4.97 | 3.67 | 3.49 | 3.46 | 3.44 |'
- en: '| FP4-sym | 5.89 | 5.73 | 5.70 | 5.67 | 5.11 | 5.03 | 5.02 | 5.01 | 3.54 |
    3.47 | 3.46 | 3.44 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| FP4-sym | 5.89 | 5.73 | 5.70 | 5.67 | 5.11 | 5.03 | 5.02 | 5.01 | 3.54 |
    3.47 | 3.46 | 3.44 |'
- en: '|  | FP4-asym | 5.82 | 5.71 | 5.70 | 5.67 | 5.09 | 5.02 | 5.01 | 4.99 | 3.52
    | 3.47 | 3.45 | 3.43 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | FP4-asym | 5.82 | 5.71 | 5.70 | 5.67 | 5.09 | 5.02 | 5.01 | 4.99 | 3.52
    | 3.47 | 3.45 | 3.43 |'
- en: '|  | FP16 | 46.58 | 55.38 | 69.58 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 46.58 | 55.38 | 69.58 |'
- en: '| MMLU(%) $\uparrow$ | INT4 | 40.31 | 43.67 | 45.28 | 45.59 | 52.92 | 54.09
    | 54.33 | 54.44 | 67.82 | 68.43 | 68.32 | 68.53 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| MMLU(%) $\uparrow$ | INT4 | 40.31 | 43.67 | 45.28 | 45.59 | 52.92 | 54.09
    | 54.33 | 54.44 | 67.82 | 68.43 | 68.32 | 68.53 |'
- en: '| FP4-sym | 44.14 | 44.25 | 43.74 | 44.04 | 53.77 | 54.17 | 54.83 | 54.62 |
    68.14 | 68.72 | 68.71 | 68.90 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| FP4-sym | 44.14 | 44.25 | 43.74 | 44.04 | 53.77 | 54.17 | 54.83 | 54.62 |
    68.14 | 68.72 | 68.71 | 68.90 |'
- en: '|  | FP4-asym | 45.25 | 44.61 | 45.15 | 44.55 | 54.23 | 54.47 | 54.70 | 54.99
    | 68.74 | 68.65 | 68.86 | 69.06 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | FP4-asym | 45.25 | 44.61 | 45.15 | 44.55 | 54.23 | 54.47 | 54.70 | 54.99
    | 68.74 | 68.65 | 68.86 | 69.06 |'
- en: 'Table 8: WikiText-2 perplexity and MMLU average accuracy on LLaMA2 models after
    FP3 RTN quantization'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '表8: FP3 RTN量化后LLaMA2模型的WikiText-2困惑度和MMLU平均准确率'
- en: '|  |  | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  |  | g-1 | g256 | g128 | g64 | g-1 | g256 | g128 | g64 | g-1 | g256 | g128
    | g64 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  |  | g-1 | g256 | g128 | g64 | g-1 | g256 | g128 | g64 | g-1 | g256 | g128
    | g64 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
- en: '|  | FP16 | 5.47 | 4.88 | 3.32 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 5.47 | 4.88 | 3.32 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| WikiText-2 $\downarrow$ | INT3 | 542.80 | 7.10 | 6.66 | 6.40 | 10.68 | 5.67
    | 5.52 | 5.39 | 7.53 | 4.11 | 3.98 | 3.85 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| WikiText-2 $\downarrow$ | INT3 | 542.80 | 7.10 | 6.66 | 6.40 | 10.68 | 5.67
    | 5.52 | 5.39 | 7.53 | 4.11 | 3.98 | 3.85 |'
- en: '| FP3-sym | 1621.90 | 7.16 | 6.89 | 6.64 | 12.76 | 5.82 | 5.66 | 5.54 | 8.43
    | 4.22 | 4.11 | 4.00 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| FP3-sym | 1621.90 | 7.16 | 6.89 | 6.64 | 12.76 | 5.82 | 5.66 | 5.54 | 8.43
    | 4.22 | 4.11 | 4.00 |'
- en: '|  | FP3-asym | 18.72 | 6.89 | 6.63 | 6.48 | 7.72 | 5.69 | 5.57 | 5.41 | 5.93
    | 4.11 | 4.01 | 3.89 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | FP3-asym | 18.72 | 6.89 | 6.63 | 6.48 | 7.72 | 5.69 | 5.57 | 5.41 | 5.93
    | 4.11 | 4.01 | 3.89 |'
- en: '|  | FP16 | 46.58 | 55.38 | 69.58 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 46.58 | 55.38 | 69.58 |'
- en: '| MMLU(%) $\uparrow$ | INT3 | 25.22 | 37.46 | 38.50 | 40.06 | 27.79 | 48.91
    | 51.23 | 50.77 | 34.39 | 64.77 | 65.05 | 66.16 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| MMLU(%) $\uparrow$ | INT3 | 25.22 | 37.46 | 38.50 | 40.06 | 27.79 | 48.91
    | 51.23 | 50.77 | 34.39 | 64.77 | 65.05 | 66.16 |'
- en: '| FP3-sym | 23.73 | 31.75 | 36.55 | 33.08 | 27.13 | 48.66 | 49.76 | 49.89 |
    32.32 | 64.65 | 65.17 | 65.91 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| FP3-sym | 23.73 | 31.75 | 36.55 | 33.08 | 27.13 | 48.66 | 49.76 | 49.89 |
    32.32 | 64.65 | 65.17 | 65.91 |'
- en: '|  | FP3-asym | 27.32 | 35.42 | 40.33 | 40.24 | 36.15 | 50.09 | 50.72 | 51.60
    | 49.74 | 64.62 | 66.14 | 66.41 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | FP3-asym | 27.32 | 35.42 | 40.33 | 40.24 | 36.15 | 50.09 | 50.72 | 51.60
    | 49.74 | 64.62 | 66.14 | 66.41 |'
- en: Appendix D Kernel implementation
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 内核实现
- en: Currently, W-only quantization requires low-bit weights to be dequantized to
    FP16 during inference, and then calculations are performed with the FP16 activations.
    In our system implementation, we store two 4-bit quantized weights using one byte.
    During dequantization, we load the byte and recover it to two 4-bit weights.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，W-only量化要求在推理过程中将低位权重解量化为FP16，然后用FP16激活值进行计算。在我们的系统实现中，我们使用一个字节存储两个4位量化权重。在解量化过程中，我们加载这个字节并将其恢复为两个4位权重。
- en: 'For INT and FP formats, the conversion from 4-bit to FP16 values can be completed
    by algebraic computation. For NP formats, it can be realized by using look-up
    tables (LUTs). Then these values can be further dequantized using the methods
    in Algorithm[1](#alg1 "Algorithm 1 ‣ Appendix B Quantization algorithm ‣ AFPQ:
    Asymmetric Floating Point Quantization for LLMs").'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '对于INT和FP格式，4位到FP16值的转换可以通过代数计算完成。对于NP格式，则可以通过使用查找表（LUTs）来实现。然后这些值可以通过算法[1](#alg1
    "算法 1 ‣ 附录 B 量化算法 ‣ AFPQ: LLMs的非对称浮点量化")中的方法进一步解量化。'
