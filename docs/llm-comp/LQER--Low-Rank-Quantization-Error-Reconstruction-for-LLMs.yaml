- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:50:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:21
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LQER: Low-Rank Quantization Error Reconstruction for LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LQER：针对LLMs的低秩量化误差重建
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.02446](https://ar5iv.labs.arxiv.org/html/2402.02446)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.02446](https://ar5iv.labs.arxiv.org/html/2402.02446)
- en: Cheng Zhang1, Jianyi Cheng2, George A. Constantinides1, Yiren Zhao1 1Department
    of Electrical and Electronic Engineering
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Cheng Zhang1, Jianyi Cheng2, George A. Constantinides1, Yiren Zhao1 1电气与电子工程系
- en: Imperial College London
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦帝国学院
- en: London, United Kingdom 2Department of Computer Science and Technology
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦，英国 2计算机科学与技术系
- en: University of Cambridge
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 剑桥大学
- en: Cambridge, United Kingdom
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 剑桥，英国
- en: 'Email: cheng.zhang122@imperial.ac.uk, jianyi.cheng@cl.cam.ac.uk, g.constantinides@imperial.ac.uk,
    a.zhao@imperial.ac.uk'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 邮箱：cheng.zhang122@imperial.ac.uk, jianyi.cheng@cl.cam.ac.uk, g.constantinides@imperial.ac.uk,
    a.zhao@imperial.ac.uk
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Post-training quantization of Large Language Models (LLMs) is challenging. In
    this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines
    quantization and low-rank approximation to recover the model capbility. LQER leverages
    an activation-induced scale matrix to drive the singular value distribution of
    quantization error towards a desirable distribution, which enables nearly-lossless
    W4A8 quantization on various LLMs and downstream tasks without the need for knowledge
    distillation, grid search, or gradient-base iterative optimization. Unlike existing
    methods, the computation pattern of LQER eliminates the need for specialized Scatter
    and Gather processes to collect high-precision weights from irregular memory locations.
    Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks,
    while using $1.36\times$ fewer hardware resources than the leading state-of-the-art
    method. We will open-source our framework once the paper is accepted.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的后训练量化是具有挑战性的。在这项工作中，我们引入了低秩量化误差减少（LQER），它将量化和低秩近似结合起来，以恢复模型能力。LQER利用激活引起的尺度矩阵，将量化误差的奇异值分布驱动到理想的分布，从而在各种LLMs和下游任务上实现几乎无损的W4A8量化，而无需知识蒸馏、网格搜索或基于梯度的迭代优化。与现有方法不同，LQER的计算模式消除了从不规则内存位置收集高精度权重所需的专门Scatter和Gather过程。我们的W4A8
    LLMs在六个流行的下游任务上实现了近乎无损的性能，同时使用的硬件资源比领先的最先进方法少$1.36\times$。我们将在论文接受后开源我们的框架。
- en: I Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Large Language Models (LLMs) have exhibited impressive capability on various
    natural language processing (NLP) tasks [[1](#bib.bib1)]. However, the substantial
    model size and its associated computation costs demand considerable energy and
    hardware resources. For instance, deploying BLOOM-176B [[2](#bib.bib2)] requires
    16 NVIDIA A100 GPUs and consumes more than 2000 Watts of total power  [[3](#bib.bib3)].
    Meanwhile, empirical evidence suggests that only models with a sufficiently large
    parameter count begin to show emergent capabilities [[4](#bib.bib4)], thereby
    motivates the construction of even larger models. Quantization then emerges as
    a promising technique to enhance the accessibility of LLMs by reducing the model
    size and simplifying inference computation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种自然语言处理（NLP）任务上表现出了令人印象深刻的能力[[1](#bib.bib1)]。然而，巨大的模型规模及其相关的计算成本需要大量的能量和硬件资源。例如，部署BLOOM-176B[[2](#bib.bib2)]需要16个NVIDIA
    A100 GPU，消耗超过2000瓦特的总功率[[3](#bib.bib3)]。与此同时，实证证据表明，只有参数数量足够大的模型才开始显示出涌现能力[[4](#bib.bib4)]，这促使了更大模型的构建。量化因此成为一种有前景的技术，通过减少模型规模和简化推理计算来提高LLMs的可访问性。
- en: '![Refer to caption](img/4554936e00460182255f63c6da7ac313.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4554936e00460182255f63c6da7ac313.png)'
- en: ((a)) Singular value distributions of quantization error
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ((a)) 量化误差的奇异值分布
- en: '![Refer to caption](img/efce5c9ff605cb2b8151c5635c834d32.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/efce5c9ff605cb2b8151c5635c834d32.png)'
- en: ((b)) LLM.int8() v.s. LQER
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ((b)) LLM.int8() 与 LQER
- en: 'Figure 1: Motivation and computation pattern of LQER. (a) We apply SVD to the
    quantization error $E_{q}=W-W_{q}$. Both components are inexpensive to compute.
    This estbalishes a regular computation pattern that eliminates the need for irregular
    memory access like the Scatter and Gather operations in LLM.int8().'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LQER的激励和计算模式。(a) 我们将SVD应用于量化误差$E_{q}=W-W_{q}$。这两个组件的计算成本都很低。这建立了一个常规计算模式，消除了对不规则内存访问（如LLM.int8()中的Scatter和Gather操作）的需求。
- en: Low-precision Post-Training Quantization (PTQ) of LLMs has recently become an
    attractive solution for reducing computational and memory cost [[5](#bib.bib5)].
    However, it remains challenging due to the fact that 1) no further weight training
    is allowed and 2) the presence of magnitude outliers in model weights and activations.
    PTQ is a technique that quantizes a pre-trained LLM directly, without additional
    training, as fine-tuning LLMs usually requires substantial resources. Many researchers
    have observed that the main building block of LLMs, the transformer layer, produces
    magnitude outliers in weights and activations [[6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8)]. A simple fixed-point quantization then either suffers from considerable
    clipping or overflow error or from considerable rounding error, depending on the
    choice of scaling. In both cases, the quantization error propagates and accumulates
    through the LLMs, leading to substantial task accuracy degradation. To overcome
    this challenge, recent LLM PTQ methods investigate the statistical properties
    of LLMs and propose various fine-grained solutions to accommodate [[9](#bib.bib9),
    [10](#bib.bib10)], mitigate [[11](#bib.bib11), [12](#bib.bib12)], or eliminate [[13](#bib.bib13),
    [14](#bib.bib14)] these numerical outliers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，低精度后训练量化（PTQ）已成为减少计算和内存成本的有吸引力的解决方案 [[5](#bib.bib5)]。然而，这仍然具有挑战性，因为 1) 不允许进一步的权重训练，以及
    2) 模型权重和激活中的幅度异常值。PTQ 是一种直接量化预训练 LLM 的技术，无需额外训练，因为微调 LLM 通常需要大量资源。许多研究人员观察到，LLM
    的主要构建块，即 transformer 层，在权重和激活中产生幅度异常值 [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)]。简单的定点量化要么由于缩放选择的不同而遭受相当大的裁剪或溢出误差，要么遭受相当大的舍入误差。在这两种情况下，量化误差会在
    LLM 中传播并积累，导致任务准确性大幅下降。为了解决这个挑战，最近的 LLM PTQ 方法研究了 LLM 的统计特性，并提出了各种细粒度解决方案来适应 [[9](#bib.bib9),
    [10](#bib.bib10)]、减轻 [[11](#bib.bib11), [12](#bib.bib12)] 或消除 [[13](#bib.bib13),
    [14](#bib.bib14)] 这些数值异常值。
- en: However, fine-grained treatments to numerical outliers usually come with a high
    optimization and/or hardware cost. The optimization cost mainly stems from iterative
    optimization. For example, OmniQuant [[15](#bib.bib15)] takes 7.3 hours to quantize
    a LLaMA-30B model with 20 iterations on a single NVIDIA A100 GPU [[16](#bib.bib16)].
    The popular weight-only quantization setup, such as GPTQ [[10](#bib.bib10)] and
    AWQ [[17](#bib.bib17)], dequantizes 4-bit weights to FP16 at runtime, which actually
    impedes inference on models larger than 7B [[18](#bib.bib18)]. Concurrently, many
    existing quantization frameworks select values from irregular positions for high-precision
    computation, while maintaining other values in low-precision formats [[19](#bib.bib19),
    [20](#bib.bib20)]. For instance, LLM.int8() [[9](#bib.bib9)] selects activation
    outliers to compute in half-precision floating-point, while casting the rest to
    integers. In this work, we propose a simple and efficient LLM PTQ framework that
    avoids iterative optimization and irregular computation patterns.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对数值异常值的细粒度处理通常伴随着较高的优化和/或硬件成本。优化成本主要来自于迭代优化。例如，OmniQuant [[15](#bib.bib15)]
    在单个 NVIDIA A100 GPU 上需要 7.3 小时来对一个 LLaMA-30B 模型进行量化，并进行了 20 次迭代 [[16](#bib.bib16)]。流行的仅权重量化设置，例如
    GPTQ [[10](#bib.bib10)] 和 AWQ [[17](#bib.bib17)]，在运行时将 4 位权重量化为 FP16，这实际上妨碍了对大于
    7B 的模型进行推理 [[18](#bib.bib18)]。与此同时，许多现有的量化框架选择来自不规则位置的值进行高精度计算，同时保持其他值为低精度格式 [[19](#bib.bib19),
    [20](#bib.bib20)]。例如，LLM.int8() [[9](#bib.bib9)] 选择激活异常值以半精度浮点计算，而将其余值转换为整数。在这项工作中，我们提出了一种简单高效的
    LLM PTQ 框架，避免了迭代优化和不规则计算模式。
- en: 'Optimizing weight quantization can be considered as a process of minimizing
    the quantization error $E_{q}=W-W_{q}$ establishes a regular computation pattern
    that eliminates need of having the Scatter and Gather operations to fetch and
    store values from irregular memory locations like LLM.int8() ([Figure 1(b)](#S1.F1.sf2
    "In Figure 1 ‣ I Introduction ‣ LQER: Low-Rank Quantization Error Reconstruction
    for LLMs")).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '优化权重量化可以被视为最小化量化误差 $E_{q}=W-W_{q}$ 的过程，这建立了一种规则的计算模式，消除了像 LLM.int8() ([图 1(b)](#S1.F1.sf2
    "图 1 ‣ I 引言 ‣ LQER: 低秩量化误差重建用于 LLMs")) 这样的不规则内存位置获取和存储值的 Scatter 和 Gather 操作的需求。'
- en: 'TABLE I: A summary of recent LLM PTQ methods. Weight-only ($w$ that achieves
    almost lossless performance on downstream tasks.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：最近 LLM PTQ 方法的总结。仅权重量化（$w$）在下游任务中几乎实现了无损性能。
- en: '| Q setup | WxAy^∗ | Quantization function | Inference-time | Methods |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Q 设置 | WxAy^∗ | 量化函数 | 推理时间 | 方法 |'
- en: '| $w$ | GPTQ [[10](#bib.bib10)], AWQ [[17](#bib.bib17)], Z-fold [[21](#bib.bib21)],
    QuiP [[22](#bib.bib22)], FlexRound [[20](#bib.bib20)], LRQ [[23](#bib.bib23)]
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| $w$ | GPTQ [[10](#bib.bib10)], AWQ [[17](#bib.bib17)], Z-fold [[21](#bib.bib21)],
    QuiP [[22](#bib.bib22)], FlexRound [[20](#bib.bib20)], LRQ [[23](#bib.bib23)]
    |'
- en: '| $w\&amp;a$ | SmoothQuant [[11](#bib.bib11)], OS+[[13](#bib.bib13)], AQAS [[12](#bib.bib12)],
    OmniQuant [[15](#bib.bib15)] |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| $w\&amp;a$ | SmoothQuant [[11](#bib.bib11)], OS+[[13](#bib.bib13)], AQAS [[12](#bib.bib12)],
    OmniQuant [[15](#bib.bib15)] |'
- en: 'In this study, we explore optimizations for $W_{q}$.. This observation then
    further motivates our LLM Post-Training Quantization (PTQ) method, Left-multiply
    LQER (L²QER), designed to recover the performance loss caused by quantization.
    We make the following contributions in this work:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们探讨了对 $W_{q}$ 的优化。这一观察进一步激发了我们 LLM 后训练量化（PTQ）方法 Left-multiply LQER（L²QER）的设计，旨在恢复因量化而导致的性能损失。我们在此工作中做出了以下贡献：
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a novel quantized LLM inference framework termed Low-rank Quantization
    Error Reduction (LQER) which combines quantization and low-rank approximation.
    Unlike existing methods that require gathering values from irregular memory locations,
    LQER boasts a blocked and regular computation pattern and employs a unified number
    format for both memory and computation.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了一种新颖的量化 LLM 推理框架，称为低秩量化误差减少（LQER），它结合了量化和低秩逼近。与现有需要从不规则内存位置收集值的方法不同，LQER
    拥有块状和规则的计算模式，并且对内存和计算使用统一的数字格式。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We then propose L²QER, a straightforward but efficient quantization method on
    top of LQER. L²QER does not need any expensive knowledge distillation, hyper-parameter
    search, or other forms of iterative optimization. We showcase L²QER’s competitiveness
    with current state-of-the-art methods. L²QER quantizes both weights and activations,
    it extends pushes the limit to W4A6, matching the perplexity of OmniQuant (W6A6)
    on WikiText. Compared to weight-only ($w$-only) quantization methods, our approach
    outperforms AWQ (W4A16) and maintains quantization activations staying at 8-bit
    (W4A8).
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了 L²QER，一种在 LQER 基础上简单但高效的量化方法。L²QER 不需要任何昂贵的知识蒸馏、超参数搜索或其他形式的迭代优化。我们展示了
    L²QER 与当前最先进方法的竞争力。L²QER 量化了权重和激活，它将限制扩展到 W4A6，在 WikiText 上与 OmniQuant（W6A6）匹敌。与仅权重（$w$-only）量化方法相比，我们的方法优于
    AWQ（W4A16），并保持量化激活在 8 位（W4A8）。
- en: II Related Work
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: II-A Post-Training Quantization of LLMs
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A LLM 的后训练量化
- en: 'Post training quantization of LLMs is a challenging task due to presence of
    numerical outliers. Existing methods can be broadly categorized into two setups:
    weight-only ($w$) quantizations. Recent works within these two setups are summarized
    in [Table I](#S1.T1 "In I Introduction ‣ LQER: Low-Rank Quantization Error Reconstruction
    for LLMs").'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '由于存在数值异常，LLM 的后训练量化是一项具有挑战性的任务。现有方法大致可分为两种设置：仅权重（$w$）量化。这两种设置中的最新工作总结见 [表 I](#S1.T1
    "在 I 引言 ‣ LQER: 低秩量化误差重建用于 LLMs")。'
- en: Weight-only quantization
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 仅权重量化
- en: 'Weight-only quantization usually partitions the trained weight matrix $W$:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 仅权重量化通常将训练后的权重矩阵 $W$ 划分为：
- en: '|  | $(W_{q},\mathbf{s})=\mathrm{q}(W)$ |  | (1) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $(W_{q},\mathbf{s})=\mathrm{q}(W)$ |  | (1) |'
- en: 'where $W_{q}$ is dequantized back to FP16 before the weight-activation matrix
    multiply:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{q}$ 在权重-激活矩阵相乘之前被去量化回 FP16：
- en: '|  | $\widetilde{Y}=X\mathrm{dq}(W_{q},\mathbf{s})$ |  | (2) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widetilde{Y}=X\mathrm{dq}(W_{q},\mathbf{s})$ |  | (2) |'
- en: Here $X$ is the output. The runtime dequantization cost is negligible in memory-bound
    scenarios, e.g., small models at small batch sizes. This cost escalates with model
    sizes, and eventually impedes inference in compute-bound scenarios [[18](#bib.bib18)].
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $X$ 是输出。在内存绑定场景下，如小模型和小批量大小，运行时去量化成本可以忽略不计。这一成本随着模型大小的增加而攀升，最终妨碍了计算绑定场景下的推理 [[18](#bib.bib18)]。
- en: GPTQ [[10](#bib.bib10)] and AWQ [[17](#bib.bib17)] are two representative $w$-only
    setup include Z-Fold [[21](#bib.bib21)] and QuiP [[22](#bib.bib22)], following
    GPTQ to correct quantization error. FlexRound [[20](#bib.bib20)], and LRQ [[23](#bib.bib23)]
    follow AWQ to study finer-grained weight scaling.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ [[10](#bib.bib10)] 和 AWQ [[17](#bib.bib17)] 是两个代表性的仅权重（$w$）设置，包括 Z-Fold [[21](#bib.bib21)]
    和 QuiP [[22](#bib.bib22)]，紧随 GPTQ 纠正量化误差。FlexRound [[20](#bib.bib20)] 和 LRQ [[23](#bib.bib23)]
    紧随 AWQ 研究更细粒度的权重缩放。
- en: Weight-activation quantization
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 权重-激活量化
- en: '$w\&amp;a$ to reduce the magnitude range of activations before quantization:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: $w\&amp;a$ 以减少激活量化前的幅度范围：
- en: '|  | $(X_{q},\mathbf{s}_{t})=\mathrm{q}(XS)$ |  | (3) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $(X_{q},\mathbf{s}_{t})=\mathrm{q}(XS)$ |  | (3) |'
- en: 'where $\mathbf{s}_{t}$ is fused into the weights of the preceding layer before
    quantization:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{s}_{t}$ 被融合到量化前的前一层权重中：
- en: '|  | $(W_{q},\mathbf{s}_{c})=\mathrm{q}(S^{-1}W)$ |  | (4) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $(W_{q},\mathbf{s}_{c})=\mathrm{q}(S^{-1}W)$ |  | (4) |'
- en: where $\mathbf{s}_{c}$ quantization methods lower than 8-bit precision usually
    suffer from an average downstraem task accuracy drop larger than 1% [[15](#bib.bib15),
    [27](#bib.bib27)].
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbf{s}_{c}$ 量化方法低于 8 位精度通常会导致下游任务准确率平均下降超过 1%[[15](#bib.bib15), [27](#bib.bib27)]。
- en: 'SmoothQuant [[11](#bib.bib11)] pioneered fusion of an invertible scale matrix
    into its preceding layer. Outlier Suppression+ [[13](#bib.bib13)] further introduces
    a bias matrix to [Equation 4](#S2.E4 "In Weight-activation quantization ‣ II-A
    Post-Training Quantization of LLMs ‣ II Related Work ‣ LQER: Low-Rank Quantization
    Error Reconstruction for LLMs") to shift the mean of activations towards zero
    and update the layer bias accordingly. Recent works following this line of research
    include AQAS [[12](#bib.bib12)], and OmniQuant [[15](#bib.bib15)].'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: SmoothQuant [[11](#bib.bib11)] 首创将可逆的尺度矩阵融合到其前一层中。Outlier Suppression+ [[13](#bib.bib13)]
    进一步引入了一个偏置矩阵到 [公式 4](#S2.E4 "在权重-激活量化 ‣ II-A 后训练量化 ‣ II 相关工作 ‣ LQER：LLMs 的低秩量化误差重构")，以将激活均值偏移到零，并相应更新层偏置。最近的研究包括
    AQAS [[12](#bib.bib12)] 和 OmniQuant [[15](#bib.bib15)]。
- en: Another unique $w\&amp;a$ quantization method, LLM.int8() decomposes the FP16
    matrix multiplication into a 8-bit fixed-point and an FP16 sub-matrix multiplication
    using activation thresholds. Despite achieving the closest model capability to
    FP16, the thresholding, Scatter and Gather operations of LLM.int8() are expensive
    in large models. Similar to LLM.int8(), SpQR [[19](#bib.bib19)] and EasyQuant [[8](#bib.bib8)]
    are recent works that retains salient weights in FP16 at finer granularity while
    quantizing the rest to low-precision.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种独特的 $w\&amp;a$ 量化方法 LLM.int8() 将 FP16 矩阵乘法分解为 8 位定点和 FP16 子矩阵乘法，使用激活阈值。尽管在模型能力上最接近
    FP16，但 LLM.int8() 的阈值操作、Scatter 和 Gather 操作在大型模型中代价高昂。类似于 LLM.int8()，SpQR [[19](#bib.bib19)]
    和 EasyQuant [[8](#bib.bib8)] 是最近的工作，它们在更精细的粒度下保留了 FP16 中的重要权重，同时将其余权重量化为低精度。
- en: In this work, we propose a fundamentally different PTQ framework that approximates
    the real value of weight through two components ($W=\widetilde{E_{q}}+W_{q}$ configuration.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种根本不同的 PTQ 框架，通过两个组件来近似权重的真实值（$W=\widetilde{E_{q}}+W_{q}$ 配置）。
- en: II-B The MXINT Arithmetic
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B MXINT 算法
- en: Block floating point is a family of number formats that represents a vector
    of numbers using a shared exponent or exponent bias. Various block floating point
    formats have been explored for efficient training or inference in the past few
    years [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)].
    One notable representative is MXINT, introduced for hardware-efficient post training
    quantization [[28](#bib.bib28), [32](#bib.bib32)], this number format has recently
    been standardized by AMD, Arm, Intel, Meta, Microsoft, NVIDIA, and Qualcomm, for
    next-generation AI facilities [[33](#bib.bib33)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 块浮点是一类数字格式，它使用共享的指数或指数偏置来表示一组数字。近年来，已经探索了各种块浮点格式用于高效的训练或推理[[28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)]。一个显著的代表是 MXINT，它是为硬件高效的后训练量化引入的[[28](#bib.bib28),
    [32](#bib.bib32)]，这个数字格式最近已经被 AMD、Arm、Intel、Meta、Microsoft、NVIDIA 和 Qualcomm 标准化，用于下一代
    AI 设施[[33](#bib.bib33)]。
- en: '[Figure 2](#S2.F2 "In II-B The MXINT Arithmetic ‣ II Related Work ‣ LQER: Low-Rank
    Quantization Error Reconstruction for LLMs") illustrates an example of an MXINT
    vector sharing a 4-bit exponent across four 4-bit mantissas. MXINT excels in hardware
    efficiency compared to floating point, as the inner product of two MXINT vectors
    can be computed as a inner product of two fixed-point numbers plus an exponent
    addition. Meanwhile, the shared exponent provides a larger dynamic range than
    fixed point numbers. Recent works indicate that this extended dynamic range fits
    the activation outliers well in LLM PTQ tasks [[34](#bib.bib34), [24](#bib.bib24)].
    In this work, We adopt MXINT as the default number format while the idea can be
    applied to other formats.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2](#S2.F2 "在 II-B MXINT 算法 ‣ II 相关工作 ‣ LQER：低秩量化误差重建用于 LLMs") 展示了一个 MXINT
    向量在四个 4 位尾数之间共享 4 位指数的示例。与浮点数相比，MXINT 在硬件效率上表现优越，因为两个 MXINT 向量的内积可以计算为两个定点数的内积加上一个指数加法。同时，共享的指数提供了比定点数更大的动态范围。近期研究表明，这种扩展的动态范围很好地适应了
    LLM PTQ 任务中的激活异常值[[34](#bib.bib34), [24](#bib.bib24)]。在这项工作中，我们采用 MXINT 作为默认数字格式，而这一理念也可以应用于其他格式。'
- en: '![Refer to caption](img/1ba9b681725278928a53a7becf7ee6a0.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1ba9b681725278928a53a7becf7ee6a0.png)'
- en: 'Figure 2: MXINT number format  [[24](#bib.bib24)]. MXINT places a shared exponent
    across a group of fixed-point numbers. MXINT is more hardware effcient than floating
    point for its simplified vector inner product, and provides a large dynamic range
    compared to fixed-point numbers. MXINT has been standardized recently for next
    generation AI hardware systems [[33](#bib.bib33)].'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: MXINT 数字格式 [[24](#bib.bib24)]。MXINT 在一组定点数字中放置共享的指数。与浮点数相比，MXINT 在其简化的向量内积计算中更具硬件效率，并且提供了比定点数字更大的动态范围。MXINT
    最近已为下一代 AI 硬件系统[[33](#bib.bib33)]标准化。'
- en: II-C Low-Rank Adapters for Fine-Tuning
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 低秩适配器用于微调
- en: 'Low-rank adapter (LoRA) [[35](#bib.bib35)] is a parameter efficient fine-tuning
    method for saving GPU memory. LoRA freezes the pretrained weight $W$ it in the
    forward pass to further reduce fine-tuning memory footprints:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适配器（LoRA）[[35](#bib.bib35)] 是一种参数高效的微调方法，用于节省 GPU 内存。LoRA 在前向传播过程中冻结预训练权重
    $W$，以进一步减少微调的内存占用：
- en: '|  | $\begin{split}Y^{\text{BF16}}=X^{\text{BF16}}\mathrm{ddq}(c_{1}^{\text{FP32}},c_{2}^{\text{k-bit}},W^{\text{NF4}})\\
    +X^{\text{BF16}}L_{1}^{\text{BF16}}L_{2}^{\text{BF16}}\end{split}$ |  | (5) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}Y^{\text{BF16}}=X^{\text{BF16}}\mathrm{ddq}(c_{1}^{\text{FP32}},c_{2}^{\text{k-bit}},W^{\text{NF4}})\\
    +X^{\text{BF16}}L_{1}^{\text{BF16}}L_{2}^{\text{BF16}}\end{split}$ |  | (5) |'
- en: 'The advantage of LoRA-based methods is that the fine-tuned model can be deployed
    without extra cost as the low-rank matrices are fused into the pretrained weights
    after fine-tuning . For qLoRA, the fusion can be expressed as:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 基于的方法的优势在于，经过微调后的模型可以在没有额外成本的情况下进行部署，因为低秩矩阵在微调后已融入预训练权重中。对于 qLoRA，这种融合可以表示为：
- en: '|  | $W^{\text{BF16}}_{\text{new}}=\mathrm{ddq}(c_{1}^{\text{FP32}},c_{2}^{\text{k-bit}},W^{\text{NF4}})+L_{1}^{\text{BF16}}L_{2}^{\text{BF16}}$
    |  | (6) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $W^{\text{BF16}}_{\text{new}}=\mathrm{ddq}(c_{1}^{\text{FP32}},c_{2}^{\text{k-bit}},W^{\text{NF4}})+L_{1}^{\text{BF16}}L_{2}^{\text{BF16}}$
    |  | (6) |'
- en: LoftQ [[37](#bib.bib37)] initializes $L_{1}$ with the Singular Value Decompostion
    (SVD) of quantization errors to achieves a faster fine-tuning convergence than
    qLoRA.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: LoftQ [[37](#bib.bib37)] 使用量化误差的奇异值分解 (SVD) 初始化 $L_{1}$，从而比 qLoRA 实现更快的微调收敛。
- en: To our knowledge, LoftQ is the closest work to ours. However, our LQER framework
    is fundamentally different from the above as it is a PTQ method that does not
    target fine-tuning. The core idea of LQER is that shaping the singular value distribution
    of quantization error approximator ($\widetilde{E_{q}}$) and the low-rank high-precision
    matrices to happen in parallel at inference time.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们了解，LoftQ 是与我们工作最接近的。然而，我们的 LQER 框架与上述工作根本不同，因为它是一个 PTQ 方法，不针对微调。LQER 的核心思想是，在推理时，量化误差近似器
    ($\widetilde{E_{q}}$) 和低秩高精度矩阵的奇异值分布的调整是并行进行的。
- en: III Method
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 方法
- en: 'We aim to approximate the multiplication by a large dense weight matrix $W$
    and then correct the error induced using a high-precision low-rank correction
    term as illustrated in [Figure 1(b)](#S1.F1.sf2 "In Figure 1 ‣ I Introduction
    ‣ LQER: Low-Rank Quantization Error Reconstruction for LLMs").'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是通过大型密集权重矩阵 $W$ 来近似乘法，然后使用高精度低秩修正项来纠正产生的误差，如 [图 1(b)](#S1.F1.sf2 "在图 1
    ‣ I 引言 ‣ LQER：低秩量化误差重建用于 LLMs") 所示。
- en: 'III-A LQER: Approximated $E_{q}$ using SVD'
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'III-A LQER: 使用 SVD 近似 $E_{q}$'
- en: '![Refer to caption](img/8d3862cc35578b4dd0e4f720406c685c.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8d3862cc35578b4dd0e4f720406c685c.png)'
- en: 'Figure 3: Perplexity ($\downarrow$) and quantization error reconstruction between
    LQER and L²QER.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：LQER 和 L²QER 之间的困惑度（$\downarrow$）和量化误差重建。
- en: 'Our idea is to reconstruct the quantization error matrix $E_{q}$ is:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的想法是重建量化误差矩阵 $E_{q}$ 如下：
- en: '|  | $E_{q}=W-W_{q}$ |  | (7) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $E_{q}=W-W_{q}$ |  | (7) |'
- en: 'where $W_{q}=\mathrm{q}(W)$ represents the quantization function. A straightforward
    way to reconstruct the error is to use SVD-based low-rank approximation:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{q}=\mathrm{q}(W)$ 代表量化函数。重建误差的一种直接方法是使用基于 SVD 的低秩近似：
- en: '|  | $E_{q}=U\Sigma V^{T}\approx U_{k}\Sigma_{k}V_{k}^{T}$ |  | (8) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $E_{q}=U\Sigma V^{T}\approx U_{k}\Sigma_{k}V_{k}^{T}$ |  | (8) |'
- en: where $U\in\mathbb{R}^{m\times m}$ singular values.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $U\in\mathbb{R}^{m\times m}$ 为奇异值。
- en: 'If two high-precision matrices $A_{k}=U_{k}$, the linear layer can be approximated
    as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个高精度矩阵 $A_{k}=U_{k}$，则线性层可以被近似为：
- en: '|  | $$\begin{split}\widetilde{Y}&amp;=XW_{q}+(XA_{k})B_{k}\\ &amp;=X(W_{q}+A_{k}B_{k})\\'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\begin{split}\widetilde{Y}&amp;=XW_{q}+(XA_{k})B_{k}\\ &amp;=X(W_{q}+A_{k}B_{k})\\'
- en: '&amp;=X(W_{q}+\widetilde{E_{q}})\\'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=X(W_{q}+\widetilde{E_{q}})\\'
- en: '&amp;\approx X(W_{q}+E_{q})\\'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;\approx X(W_{q}+E_{q})\\'
- en: '&amp;=XW\end{split}$$ |  | (9) |'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=XW\end{split}$$ |  | (9) |'
- en: where $X\in\mathbb{R}^{t\times m}$, means that we compensate the quantization
    error of 3-bit weight using two 8-bit low-rank matrices. We refer to this design
    of the inference flow as LQER.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $X\in\mathbb{R}^{t\times m}$，意味着我们使用两个 8 位低秩矩阵来补偿 3 位权重的量化误差。我们将这种推理流程的设计称为
    LQER。
- en: 'At inference-time, LQER runs one low-precision but large matrix multiplication
    ($XW_{q}$ allows tuning the trade-off between the computational cost and the model
    accuracy. Specifically:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理时，LQER 运行一次低精度但大矩阵乘法（$XW_{q}$ 允许在计算成本和模型准确性之间调整权衡。具体而言：
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In LLMs, $W\in\mathbb{R}^{m\times n}$.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 LLMs 中，$W\in\mathbb{R}^{m\times n}$。
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Two high-precision but small matrices $A_{k}\in\mathbb{R}^{m\times k}$.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两个高精度但小的矩阵 $A_{k}\in\mathbb{R}^{m\times k}$。
- en: The ideal case is that a small $k\ll\min(m,n)$ value by analytically scaling
    the error term.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，通过解析地缩放误差项来得到一个小的 $k\ll\min(m,n)$ 值。
- en: 'III-B L²QER: Shape Singular Value Distribution of Quantization Errors using
    Activation Statistics'
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B L²QER：使用激活统计的量化误差的形状奇异值分布
- en: Recent works have shown that partially preserving the weight precision according
    to activation magnitude recovers the model’s accuracy/perplexity. LLM.int8() decomposes
    an FP16 matrix multiply into one FP16 sub-matrix multiply for large activation
    magnitudes and one 8-bit fixed-point sub-matrix multiply for the rest at runtime.
    AWQ also presents an experiment that effectively recovers accuracy by preserving
    the 1% salient weights corresponding to large activation magnitudes in FP16, and
    quantizing other weights to 4-bit grouped fixed-point.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的工作表明，根据激活幅度部分保留权重精度可以恢复模型的准确性/困惑度。LLM.int8() 将一个 FP16 矩阵乘法分解为一个用于大激活幅度的 FP16
    子矩阵乘法和一个用于其余部分的 8 位定点子矩阵乘法。AWQ 还展示了一项实验，通过保留 1% 对应大激活幅度的显著权重以 FP16 表示，并将其他权重量化为
    4 位分组定点，从而有效地恢复了准确性。
- en: 'Motivated by this phenomenon, we propose a novel quantization error reconstruction
    method, named L²QER, that scales the quantization error matrix $E_{q}$:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 受到这一现象的启发，我们提出了一种新颖的量化误差重建方法，称为 L²QER，它缩放量化误差矩阵 $E_{q}$：
- en: '|  | $SE_{q}=U^{\prime}\Sigma^{\prime}V^{\prime T}\approx U^{\prime}_{k}\Sigma^{\prime}_{k}{V^{\prime}}_{k}^{T}$
    |  | (10) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $SE_{q}=U^{\prime}\Sigma^{\prime}V^{\prime T}\approx U^{\prime}_{k}\Sigma^{\prime}_{k}{V^{\prime}}_{k}^{T}$
    |  | (10) |'
- en: 'where $S$ is in [Appendix A](#A1 "Appendix A Data Calibration ‣ LQER: Low-Rank
    Quantization Error Reconstruction for LLMs"). The calibration requires no training.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S$ 在 [附录 A](#A1 "附录 A 数据校准 ‣ LQER：低秩量化误差重建用于 LLMs")。校准无需训练。
- en: The intuition behind the scaling is that the quantization error corresponds
    to large activation magnitudes, i.e., the salient weights identified by corresponding
    activation magnitudes, should be more precisely approximated. Hence, we scale
    up these quantization errors before SVD.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放的直观理解是，量化误差对应于大的激活幅度，即，由相应激活幅度确定的显著权重，应该被更精确地近似。因此，我们在进行 SVD 之前对这些量化误差进行缩放。
- en: 'High precision $A^{\prime}_{k}$:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 高精度 $A^{\prime}_{k}$：
- en: '|  | $\left\{\begin{aligned} A^{\prime}_{k}&amp;=S^{-1}U^{\prime}_{k}\\ B^{\prime}_{k}&amp;=\Sigma^{\prime}_{k}{V^{\prime}}_{k}^{T}\end{aligned}\right.$
    |  | (11) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left\{\begin{aligned} A^{\prime}_{k}&amp;=S^{-1}U^{\prime}_{k}\\ B^{\prime}_{k}&amp;=\Sigma^{\prime}_{k}{V^{\prime}}_{k}^{T}\end{aligned}\right.$
    |  | (11) |'
- en: 'where $S^{-1}$ is zero (no channels in LLM activations are always zero). Now
    we approximate the linear layer similarly to LQER:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S^{-1}$ 为零（LLM 激活中的通道总是为零）。现在我们将线性层近似为类似 LQER 的形式：
- en: '|  | $$\begin{split}\widetilde{Y}&amp;=XW_{q}+(XA^{\prime}_{k})B^{\prime}_{k}\\
    &amp;=XW_{q}+(XS^{-1}U^{\prime}_{k})(\Sigma^{\prime}_{k}{V^{\prime}}_{k}^{T})\\'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\begin{split}\widetilde{Y}&amp;=XW_{q}+(XA^{\prime}_{k})B^{\prime}_{k}\\
    &amp;=XW_{q}+(XS^{-1}U^{\prime}_{k})(\Sigma^{\prime}_{k}{V^{\prime}}_{k}^{T})\\'
- en: '&amp;=X\left(W_{q}+S^{-1}\widetilde{(SE_{q})}_{k}\right)\\'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=X\left(W_{q}+S^{-1}\widetilde{(SE_{q})}_{k}\right)\\'
- en: '&amp;\approx XW\end{split}$$ |  | (12) |'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;\approx XW\end{split}$$ |  | (12) |'
- en: where $\widetilde{Y}$ is the approximated quantization error.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\widetilde{Y}$ 是近似的量化误差。
- en: 'As shown in [Figure 1(a)](#S1.F1.sf1 "In Figure 1 ‣ I Introduction ‣ LQER:
    Low-Rank Quantization Error Reconstruction for LLMs"), $S$. In [Section IV-C](#S4.SS3
    "IV-C Comparing with Existing Quantization Methods ‣ IV Experiments ‣ LQER: Low-Rank
    Quantization Error Reconstruction for LLMs"), we will show that L²QER achieves
    nearly lossless W4A6 LLM PTQ results comparable to state-of-the-art W6A6/W4A16
    methods but with higher hardware efficiency.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '如 [图 1(a)](#S1.F1.sf1 "在图 1 ‣ I 引言 ‣ LQER: 低秩量化误差重建用于 LLMs") 所示，$S$。在 [第 IV-C
    节](#S4.SS3 "IV-C 与现有量化方法比较 ‣ IV 实验 ‣ LQER: 低秩量化误差重建用于 LLMs")，我们将展示 L²QER 实现了几乎无损的
    W4A6 LLM PTQ 结果，与最先进的 W6A6/W4A16 方法相当，但硬件效率更高。'
- en: IV Experiments
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验
- en: 'TABLE II: Perplexity ($\downarrow$ in L²QER.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：困惑度（$\downarrow$ 在 L²QER 中。
- en: '|  | MXINT | LQER | L²QER | FP16 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | MXINT | LQER | L²QER | FP16 |'
- en: '| OPT-1.3B | 16.42 | 15.28 | 15.02 | 14.63 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3B | 16.42 | 15.28 | 15.02 | 14.63 |'
- en: '| $\Delta$) | +1.78 | +0.65 | +0.39 | - |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| $\Delta$) | +1.78 | +0.65 | +0.39 | - |'
- en: '| LLaMA-7B | 6.17 | 6.06 | 5.89 | 5.67 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | 6.17 | 6.06 | 5.89 | 5.67 |'
- en: '| $\Delta$) | +0.50 | +0.39 | +0.22 | - |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| $\Delta$) | +0.50 | +0.39 | +0.22 | - |'
- en: IV-A Experimental Setup
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 实验设置
- en: Quantization
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化
- en: 'We use MXINT as the number format of LQER if not specified. In [Section IV-C](#S4.SS3
    "IV-C Comparing with Existing Quantization Methods ‣ IV Experiments ‣ LQER: Low-Rank
    Quantization Error Reconstruction for LLMs"), we use W4A8 L²QER with $k=32$).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在未指定的情况下使用 MXINT 作为 LQER 的数字格式。在 [第 IV-C 节](#S4.SS3 "IV-C 与现有量化方法比较 ‣ IV
    实验 ‣ LQER: 低秩量化误差重建用于 LLMs")中，我们使用 W4A8 L²QER 和 $k=32$）。'
- en: Models and Baselines
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型和基准
- en: We benchmarked our methods on the OPT family [[26](#bib.bib26)], the LLaMA family
    (including LLaMA [[38](#bib.bib38)], LLaMA-2 [[39](#bib.bib39)], Vicuna-v1.5 [[40](#bib.bib40)]),
    and Mistral [[41](#bib.bib41)]. These are the representative or state-of-the-art
    model open-sourced for research across various model sizes and architectures.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 OPT 系列[[26](#bib.bib26)]、LLaMA 系列（包括 LLaMA [[38](#bib.bib38)]、LLaMA-2 [[39](#bib.bib39)]、Vicuna-v1.5
    [[40](#bib.bib40)]）和 Mistral [[41](#bib.bib41)]上基准测试了我们的方法。这些是代表性或最先进的模型，已开源用于各种模型大小和架构的研究。
- en: 'TABLE III: A comparison of perplexity($\downarrow$ means OmniQuant and AQAS
    use per-channel and per-token scaled quantization. ^‡ means LLaMA-2 results were
    not available in [[12](#bib.bib12)] and the author has not open-sourced AQAS code.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：困惑度比较（$\downarrow$ 表示 OmniQuant 和 AQAS 使用了每通道和每标记缩放量化。^‡ 表示 LLaMA-2 结果在[[12](#bib.bib12)]中不可用，作者尚未开源
    AQAS 代码。
- en: '| Q Setup | Method | Q Config | OPT | LLaMA | LLaMA-2 | Avg. $\Delta$) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Q 设置 | 方法 | Q 配置 | OPT | LLaMA | LLaMA-2 | 平均 $\Delta$) |'
- en: '| 6.7B | 13B | 30B | 7B | 13B | 33B | 7B | 13B |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 6.7B | 13B | 30B | 7B | 13B | 33B | 7B | 13B |'
- en: '| - | FP16 | - | 10.86 | 10.13 | 9.56 | 5.67 | 5.10 | 4.10 | 5.48 | 4.90 |
    - | 16 | 1$\times$ |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| - | FP16 | - | 10.86 | 10.13 | 9.56 | 5.67 | 5.10 | 4.10 | 5.48 | 4.90 |
    - | 16 | 1$\times$ |'
- en: '| $w$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| $w$ |'
- en: '| AWQ | INT4, g128 | 10.93 | 10.21 | 9.59 | 5.78 | 5.20 | 4.22 | 5.61 | 4.98
    | 0.09 | 4.1 | 13.99$\times$ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | INT4, g128 | 10.93 | 10.21 | 9.59 | 5.78 | 5.20 | 4.22 | 5.61 | 4.98
    | 0.09 | 4.1 | 13.99$\times$ |'
- en: '| L²QER-INT | INT4, g128 | 10.99 | 10.24 | 9.57 | 5.89 | 5.20 | 4.24 | 5.58
    | 4.96 | 0.11 | 4.3 | 1.34$\times$ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| L²QER-INT | INT4, g128 | 10.99 | 10.24 | 9.57 | 5.89 | 5.20 | 4.24 | 5.58
    | 4.96 | 0.11 | 4.3 | 1.34$\times$ |'
- en: '| $w\&amp;a$ |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| $w\&amp;a$ |'
- en: '| OmniQuant ${\dagger}$ |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant ${\dagger}$ |'
- en: '| AQAS ${\dagger}$ |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| AQAS ${\dagger}$ |'
- en: '| L²QER-INT | W4A8, g128 | 11.10 | 10.38 | 9.72 | 6.09 | 5.31 | 4.35 | 5.85
    | 5.10 | 0.25 | 4.1 | 0.33$\times$ |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| L²QER-INT | W4A8, g128 | 11.10 | 10.38 | 9.72 | 6.09 | 5.31 | 4.35 | 5.85
    | 5.10 | 0.25 | 4.1 | 0.33$\times$ |'
- en: '| L²QER-MXINT | W4A6 | 11.03 | 10.32 | 9.72 | 5.92 | 5.24 | 4.28 | 5.73 | 5.05
    | 0.18 | 4.3 | 0.23$\times$ |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| L²QER-MXINT | W4A6 | 11.03 | 10.32 | 9.72 | 5.92 | 5.24 | 4.28 | 5.73 | 5.05
    | 0.18 | 4.3 | 0.23$\times$ |'
- en: '| L²QER-MXINT | W4A8 | 11.00 | 10.27 | 9.69 | 5.89 | 5.21 | 4.25 | 5.69 | 5.02
    | 0.15 | 4.3 | 0.33$\times$ |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| L²QER-MXINT | W4A8 | 11.00 | 10.27 | 9.69 | 5.89 | 5.21 | 4.25 | 5.69 | 5.02
    | 0.15 | 4.3 | 0.33$\times$ |'
- en: We compare our methods with FP16 model, LLM.int4()³³3LLM.int4() denotes the
    4-bit verision of LLM.int8() open-sourced in bitsandbytes., GPTQ, AWQ, AQAS, OmniQuant⁴⁴4We
    take W6A6 OmniQuant as an weight-activation quantization baseline, and W2A16 as
    a 2-bit weight-only quantization baseline., and QuiP. The later two have variants
    optimized for extremely low-precision quantization. We take the reported WikiText2
    perplexity or downstream task accuracy from the original papers if available.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的方法与 FP16 模型、LLM.int4()³³3LLM.int4() 表示 LLM.int8() 的 4 位版本，开源于 bitsandbytes。进行比较，还包括
    GPTQ、AWQ、AQAS、OmniQuant⁴⁴4我们以 W6A6 OmniQuant 作为权重-激活量化基线，以 W2A16 作为 2 位权重专用量化基线。和
    QuiP。后两个方法具有针对极低精度量化优化的变体。如果可用，我们采用原始论文报告的 WikiText2 困惑度或下游任务准确度。
- en: Evaluation
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估
- en: 'We report the perplexity on WikiText-2 [[42](#bib.bib42)] and the accuracy
    on ARC (easy) [[43](#bib.bib43)], ARC (challenge) [[43](#bib.bib43)], LAMBADA [[44](#bib.bib44)],
    PIQA [[45](#bib.bib45)], OpenBookQA [[46](#bib.bib46)], and BoolQ [[47](#bib.bib47)]
    using the lm-eval-harness evaluation flow [[48](#bib.bib48)]. Ideally a calibration
    dataset should be sampled from the pretraining dataset to calculate the activation-induced
    scale matrix $S$. However, none of the LLMs mentioned above open-sourced their
    pretraining datasets. We create a subset of SlimPajama [[49](#bib.bib49)] with
    Wikipedia texts excluded as the calibration dataset. This calibration dataset
    contains only 32 samples of 2048 tokens. As metnioned previously in [Section III-B](#S3.SS2
    "III-B L2QER: Shape Singular Value Distribution of Quantization Errors using Activation
    Statistics ‣ III Method ‣ LQER: Low-Rank Quantization Error Reconstruction for
    LLMs"), this calibration simply profiles values without having any training. We
    also report the weight average bitwidth for memory efficiency and estimate the
    circuit area for the hardware cost. Circuit area is estimated with the number
    of Look Up Tables (LUTs) of the processing engines (PEs) if implemented on FPGAs,
    which is also approximately proportional to the number of gates if implemented
    as ASICs. We have faithfully implemented these arithmetic cores and inputted them
    into FPGA synthesis flows, obtaining results for circuit area. This is because
    MXINT is a newly release arithmetic standard [[33](#bib.bib33)]. [Appendix D](#A4
    "Appendix D Estimate Hardware Cost ‣ LQER: Low-Rank Quantization Error Reconstruction
    for LLMs") provides the detailed circuit area estimation.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告了在 WikiText-2 [[42](#bib.bib42)] 上的困惑度，以及在 ARC (easy) [[43](#bib.bib43)]、ARC
    (challenge) [[43](#bib.bib43)]、LAMBADA [[44](#bib.bib44)]、PIQA [[45](#bib.bib45)]、OpenBookQA
    [[46](#bib.bib46)] 和 BoolQ [[47](#bib.bib47)] 上的准确度，使用 lm-eval-harness 评估流程 [[48](#bib.bib48)]。理想情况下，校准数据集应从预训练数据集中采样，以计算激活引起的缩放矩阵
    $S$。然而，上述提到的 LLMs 都没有开源其预训练数据集。我们创建了一个排除了维基百科文本的 SlimPajama [[49](#bib.bib49)]
    子集作为校准数据集。这个校准数据集仅包含 32 个 2048 令牌的样本。如[第 III-B 节](#S3.SS2 "III-B L2QER：使用激活统计量的量化误差的形状奇异值分布
    ‣ III 方法 ‣ LQER：LLMs 的低秩量化误差重建")中所述，这种校准仅用于分析数值，没有进行任何训练。我们还报告了内存效率的权重平均比特宽度，并估算了硬件成本的电路面积。如果在
    FPGA 上实现，电路面积通过处理引擎 (PEs) 的查找表 (LUTs) 数量进行估算，这也大致与作为 ASIC 实现时的门数量成正比。我们忠实地实现了这些算术核心并将其输入
    FPGA 合成流程，获得了电路面积的结果。这是因为 MXINT 是一种新发布的算术标准 [[33](#bib.bib33)]。[附录 D](#A4 "附录
    D 估算硬件成本 ‣ LQER：LLMs 的低秩量化误差重建") 提供了详细的电路面积估算。
- en: IV-B LQER and L²QER
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B LQER 和 L²QER
- en: 'We first focus on comparing variants of LQER in [Table II](#S4.T2 "In IV Experiments
    ‣ LQER: Low-Rank Quantization Error Reconstruction for LLMs"). We evaluate the
    variants in a W4A8 $w\&amp;a$ quantization setup on both OPT-1.3B and LLaMA-7B.
    We show the results of plain MXINT, LQER, and L²QER, where plain MXINT means the
    whole network is simply MXINT quantized without any special treatmetns.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先关注在[表 II](#S4.T2 "在 IV 实验 ‣ LQER：LLMs 的低秩量化误差重建")中比较 LQER 的不同变体。我们在 OPT-1.3B
    和 LLaMA-7B 上评估了在 W4A8 $w\&amp;a$ 量化设置中的变体。我们展示了普通 MXINT、LQER 和 L²QER 的结果，其中普通
    MXINT 意味着整个网络仅通过 MXINT 量化，没有任何特殊处理。
- en: '[Table II](#S4.T2 "In IV Experiments ‣ LQER: Low-Rank Quantization Error Reconstruction
    for LLMs") indicates that a plain W4A8 MXINT quantization leads to substantial
    performance degradation ($\Delta\text{PPL}$ in L²QER further pushes the performance
    of LQER to be even closer to the FP16 baseline. In the following sections, we
    then mainly focus on presenting L²QER results.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 II](#S4.T2 "在 IV 实验中 ‣ LQER: 低秩量化误差重建用于 LLMs") 表示普通的 W4A8 MXINT 量化会导致性能显著下降（$\Delta\text{PPL}$
    在 L²QER 中进一步推动 LQER 的性能，更接近 FP16 基线。在接下来的部分中，我们将主要集中展示 L²QER 的结果。'
- en: IV-C Comparing with Existing Quantization Methods
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 与现有量化方法比较
- en: 'TABLE IV: A comparison of downstream task accuracy ($\uparrow$), averaged across
    six downstream tasks. Bold text indicates the best results, while underscore denotes
    the second-best. L²QER achieves the best accuracy among all LLaMA models, and
    nearly lossless (around 0.3% drop) compared to the FP16 baseline. ^∗ means the
    results are not available in the original GPTQ paper, and we did not find open-source
    implementations and/or model checkpoints to run evaluation. ^† means the results
    of OPT and LLaMA-2 are not reported in the original OmniQuant paper. For LLaMA-1,
    LAMBADA and OpenbookQA are not included in OmniQuant either, thus we replace the
    results of these two tasks with FP16 resutls as an estimated upper limit of OmniQuant.
    OmniQuant-r is the results we replicated using the official implementation^([5](#footnote5
    "Footnote 5 ‣ Downstream task accuracy ‣ IV-C Comparing with Existing Quantization
    Methods ‣ IV Experiments ‣ LQER: Low-Rank Quantization Error Reconstruction for
    LLMs")) and checkpoints^([6](#footnote6 "Footnote 6 ‣ Downstream task accuracy
    ‣ IV-C Comparing with Existing Quantization Methods ‣ IV Experiments ‣ LQER: Low-Rank
    Quantization Error Reconstruction for LLMs")).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 下游任务准确率的比较（$\uparrow$），平均跨六个下游任务。粗体文本表示最佳结果，下划线表示第二好结果。L²QER 在所有 LLaMA
    模型中达到最佳准确率，与 FP16 基线相比几乎无损（约 0.3% 下降）。^∗ 表示原始 GPTQ 论文中没有提供结果，我们也没有找到开源实现和/或模型检查点来运行评估。^†
    表示 OPT 和 LLaMA-2 的结果未在原始 OmniQuant 论文中报告。对于 LLaMA-1，LAMBADA 和 OpenbookQA 也未包含在
    OmniQuant 中，因此我们用 FP16 结果替代这两个任务的结果，作为 OmniQuant 的估计上限。OmniQuant-r 是我们使用官方实现^([5](#footnote5
    "脚注 5 ‣ 下游任务准确率 ‣ IV-C 与现有量化方法比较 ‣ IV 实验 ‣ LQER: 低秩量化误差重建用于 LLMs")) 和检查点^([6](#footnote6
    "脚注 6 ‣ 下游任务准确率 ‣ IV-C 与现有量化方法比较 ‣ IV 实验 ‣ LQER: 低秩量化误差重建用于 LLMs")) 复制的结果。'
- en: '| Method | Q Config | OPT | LLaMA | LLaMA-2 | Avg. $\Delta$) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 量化配置 | OPT | LLaMA | LLaMA-2 | 平均 $\Delta$ |'
- en: '| 6.7B | 13B | 30B | 7B | 13B | 33B | 7B | 13B |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 6.7B | 13B | 30B | 7B | 13B | 33B | 7B | 13B |'
- en: '| FP16 | - | 55.6% | 56.2% | 59.1% | 63.2% | 65.0% | 68.4% | 63.5% | 66.5%
    | - |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 55.6% | 56.2% | 59.1% | 63.2% | 65.0% | 68.4% | 63.5% | 66.5%
    | - |'
- en: '| GPTQ | INT4, g128 | 55.4% | 56.4% | -^∗ | 60.8% | 64.7% | 66.7% | 62.2% |
    65.9% | -0.9% |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | INT4, g128 | 55.4% | 56.4% | -^∗ | 60.8% | 64.7% | 66.7% | 62.2% |
    65.9% | -0.9% |'
- en: '| AWQ | INT4, g128 | 55.3% | 56.4% | 58.9% | 62.5% | 64.8% | 68.0% | 62.9%
    | 65.9% | -0.4% |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | INT4, g128 | 55.3% | 56.4% | 58.9% | 62.5% | 64.8% | 68.0% | 62.9%
    | 65.9% | -0.4% |'
- en: '| LLM.int4() | $\tau=6.0$ | 55.4% | 55.9% | 58.0% | 62.2% | 64.6% | 67.7% |
    62.6% | 65.8% | -0.7% |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int4() | $\tau=6.0$ | 55.4% | 55.9% | 58.0% | 62.2% | 64.6% | 67.7% |
    62.6% | 65.8% | -0.7% |'
- en: '| OmniQuant^† | W6A6, per-c/t | - | - | - | 58.4% | 59.2% | 61.0% | - | - |
    -6.0% |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant^† | W6A6, per-c/t | - | - | - | 58.4% | 59.2% | 61.0% | - | - |
    -6.0% |'
- en: '| OmniQuant-r^† | W6A6, per-c/t | 55.4% | 56.1% | 58.6% | 47.0% | 48.2% | 49.9%
    | 47.2% | 49.4% | -11.0% |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant-r^† | W6A6, per-c/t | 55.4% | 56.1% | 58.6% | 47.0% | 48.2% | 49.9%
    | 47.2% | 49.4% | -11.0% |'
- en: '| L²QER-INT | W4A8, g128 | 54.1% | 56.2% | 57.7% | 61.7% | 64.4% | 67.4% |
    62.2% | 65.9% | -1.0% |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| L²QER-INT | W4A8, g128 | 54.1% | 56.2% | 57.7% | 61.7% | 64.4% | 67.4% |
    62.2% | 65.9% | -1.0% |'
- en: '| L²QER-MXINT | W4A6 | 54.7% | 56.2% | 58.5% | 62.7% | 64.9% | 67.8% | 63.0%
    | 65.8% | -0.5% |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| L²QER-MXINT | W4A6 | 54.7% | 56.2% | 58.5% | 62.7% | 64.9% | 67.8% | 63.0%
    | 65.8% | -0.5% |'
- en: '| L²QER-MXINT | W4A8 | 55.1% | 56.5% | 58.4% | 63.0% | 64.8% | 68.0% | 63.1%
    | 66.1% | -0.3% |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| L²QER-MXINT | W4A8 | 55.1% | 56.5% | 58.4% | 63.0% | 64.8% | 68.0% | 63.1%
    | 66.1% | -0.3% |'
- en: 'We present the perplexity ($\downarrow$ methods in [Table III](#S4.T3 "In Models
    and Baselines ‣ IV-A Experimental Setup ‣ IV Experiments ‣ LQER: Low-Rank Quantization
    Error Reconstruction for LLMs"). Then we exclude the methods with obvious performance
    degradation and evaluate the average downstream task performance in [Table IV](#S4.T4
    "In IV-C Comparing with Existing Quantization Methods ‣ IV Experiments ‣ LQER:
    Low-Rank Quantization Error Reconstruction for LLMs"). We additionally include
    a fixed-point version of  L²QER as a baseline. Best results in each setup are
    marked in bold and second best results are underlined.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 [表 III](#S4.T3 "在模型和基准 ‣ IV-A 实验设置 ‣ IV 实验 ‣ LQER: Low-Rank Quantization
    Error Reconstruction for LLMs") 中展示了困惑度（$\downarrow$）方法。然后，我们排除表现明显下降的方法，并在 [表
    IV](#S4.T4 "在 IV-C 比较现有量化方法 ‣ IV 实验 ‣ LQER: Low-Rank Quantization Error Reconstruction
    for LLMs") 中评估下游任务的平均性能。我们还包括了 L²QER 的固定点版本作为基线。每种设置中最佳结果用粗体标记，第二佳结果用下划线标记。'
- en: WikiText-2
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: WikiText-2
- en: In the $w$ higher than OmniQuant on the OPT family, but consistently outperforms
    OmniQuant on LLaMA family. Note that OmniQuant was trained on WikiText2 for 20
    epochs [[15](#bib.bib15)], but L²QER only proifles the activation magnitudes using
    32 samples from a calibration dataset with Wikipedia texts excluded. L²QER is
    also significantly smaller in terms of circut area.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OPT 系列中，$w$ 的表现优于 OmniQuant，但在 LLaMA 系列中始终优于 OmniQuant。请注意 OmniQuant 在 WikiText2
    上训练了 20 个 epoch [[15](#bib.bib15)]，但 L²QER 仅使用来自排除 Wikipedia 文本的校准数据集的 32 个样本来分析激活幅度。L²QER
    在电路面积上也显著较小。
- en: 'TABLE V: 2-bit quantization perplexity ($\downarrow$) on WikiText2. OmniQuant
    and QuiP#^([7](#footnote7 "Footnote 7 ‣ IV-D 2-bit Quantization ‣ IV Experiments
    ‣ LQER: Low-Rank Quantization Error Reconstruction for LLMs")) are two state-of-the-art
    methods for extremely low-precision LLM quantization. We found 2-bit quantization
    is still challenging for existing methods.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V：WikiText2 上的 2 位量化困惑度（$\downarrow$）。OmniQuant 和 QuiP#^([7](#footnote7 "脚注
    7 ‣ IV-D 2-bit Quantization ‣ IV Experiments ‣ LQER: Low-Rank Quantization Error
    Reconstruction for LLMs")) 是极低精度 LLM 量化的两种最先进的方法。我们发现 2 位量化对现有方法仍然具有挑战性。'
- en: '| Q Setup | Method | Q Config | 7B | 13B |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Q Setup | 方法 | Q 配置 | 7B | 13B |'
- en: '| - | FP16 | - | 5.67 | 5.10 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| - | FP16 | - | 5.67 | 5.10 |'
- en: '| $w$-only | AWQ | INT2 g128 | 2.6e5 | 2.8e5 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| $w$-only | AWQ | INT2 g128 | 2.6e5 | 2.8e5 |'
- en: '| QuiP# | INT2 g128 | 10.97 | 8.43 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| QuiP# | INT2 g128 | 10.97 | 8.43 |'
- en: '| OmniQuant | INT2 g128 | 12.97 | 10.36 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | INT2 g128 | 12.97 | 10.36 |'
- en: '| $w\&amp;a$ | 10.30 | 8.42 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| $w\&amp;a$ | 10.30 | 8.42 |'
- en: Downstream task accuracy
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下游任务准确性
- en: 'We conduct a thorough evaluation on downstream tasks, including ARC (easy),
    ARC (challenge), LAMBADA, PIQA, OpenBookQA and BoolQ and report the results in
    [Table IV](#S4.T4 "In IV-C Comparing with Existing Quantization Methods ‣ IV Experiments
    ‣ LQER: Low-Rank Quantization Error Reconstruction for LLMs"). The average accuracy
    of L²QER on the six downstream tasks is better than other quantization methods
    on LLaMA models, and nearly lossless (around 0.3% drop) compared to the FP16 baseline.
    We reproduced the WikiText2 perplexity reported in OmniQuant paper [[15](#bib.bib15)]
    using the official implementation⁵⁵5[https://github.com/OpenGVLab/OmniQuant](https://github.com/OpenGVLab/OmniQuant)
    and checkpoints⁶⁶6[https://huggingface.co/ChenMnZ/OmniQuant/tree/main](https://huggingface.co/ChenMnZ/OmniQuant/tree/main),
    but failed to reproduce their downstream accuracy performance on LLaMA models.
    We refer to this mismatched OmniQuant results as OmniQuant-r in [Table IV](#S4.T4
    "In IV-C Comparing with Existing Quantization Methods ‣ IV Experiments ‣ LQER:
    Low-Rank Quantization Error Reconstruction for LLMs"). We attribute the inconsistant
    behaviour of OmniQuant to its iterative quantization parameter training on WikiText2,
    which is further discussed in [Appendix C](#A3 "Appendix C Inconsistant performance
    of OmniQuant on WikiText2 and downstream tasks ‣ LQER: Low-Rank Quantization Error
    Reconstruction for LLMs"). Nevertheless, our method has demonstrated substantially
    better downstream task capabiliteis, with a much lower hardware cost (circuit
    area in [Table III](#S4.T3 "In Models and Baselines ‣ IV-A Experimental Setup
    ‣ IV Experiments ‣ LQER: Low-Rank Quantization Error Reconstruction for LLMs")).
    A detailed discussion about hardware cost is in [Appendix D](#A4 "Appendix D Estimate
    Hardware Cost ‣ LQER: Low-Rank Quantization Error Reconstruction for LLMs"). A
    complete table including the accuracy of each individual task is in [Appendix E](#A5
    "Appendix E More evaluation results ‣ LQER: Low-Rank Quantization Error Reconstruction
    for LLMs").'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对下游任务进行了彻底评估，包括 ARC（简单），ARC（挑战），LAMBADA，PIQA，OpenBookQA 和 BoolQ，并在[表 IV](#S4.T4
    "在 IV-C 与现有量化方法比较 ‣ IV 实验 ‣ LQER：LLMs 的低秩量化误差重构")中报告结果。L²QER 在这六个下游任务上的平均准确率优于其他量化方法，并且与
    FP16 基准相比几乎无损（约 0.3% 的下降）。我们使用官方实现⁵⁵5[https://github.com/OpenGVLab/OmniQuant](https://github.com/OpenGVLab/OmniQuant)和检查点⁶⁶6[https://huggingface.co/ChenMnZ/OmniQuant/tree/main](https://huggingface.co/ChenMnZ/OmniQuant/tree/main)
    复现了 OmniQuant 论文中报告的 WikiText2 困惑度，但未能复现其在 LLaMA 模型上的下游准确性表现。我们将这些不匹配的 OmniQuant
    结果称为 OmniQuant-r，如[表 IV](#S4.T4 "在 IV-C 与现有量化方法比较 ‣ IV 实验 ‣ LQER：LLMs 的低秩量化误差重构")中所示。我们将
    OmniQuant 的不一致表现归因于其在 WikiText2 上的迭代量化参数训练，这在[附录 C](#A3 "附录 C OmniQuant 在 WikiText2
    和下游任务上的不一致表现 ‣ LQER：LLMs 的低秩量化误差重构")中有进一步讨论。尽管如此，我们的方法在下游任务能力上表现显著更好，且硬件成本更低（[表
    III](#S4.T3 "在模型和基准 ‣ IV-A 实验设置 ‣ IV 实验 ‣ LQER：LLMs 的低秩量化误差重构")中的电路面积）。关于硬件成本的详细讨论见[附录
    D](#A4 "附录 D 估算硬件成本 ‣ LQER：LLMs 的低秩量化误差重构")。每个单独任务的准确性完整表格见[附录 E](#A5 "附录 E 更多评估结果
    ‣ LQER：LLMs 的低秩量化误差重构")。
- en: Optimization cost
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化成本
- en: The optimization of LQER is also efficient. The calibration and quantiation
    of LLaMA-33B takes around 1.2 hour in total on a single NVIDIA A100 GPU. In contrast,
    OmniQuant takes 7.3 hours to optimize the quantization parameters for LLaMA-33B.
    Furthermore, the optimization of LQER can be fully parallelized to be faster,
    since there is no dependency between the quantization of each linear layer such
    as fusing the scale matrices to preceding layers in SmoothQuant or knwoledge distillation
    in LLM-QAT [[50](#bib.bib50)].
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: LQER 的优化也非常高效。LLaMA-33B 的标定和量化在单个 NVIDIA A100 GPU 上总共需约 1.2 小时。相比之下，OmniQuant
    优化 LLaMA-33B 的量化参数需要 7.3 小时。此外，LQER 的优化可以完全并行化以加快速度，因为量化每个线性层之间没有依赖关系，例如在 SmoothQuant
    中将缩放矩阵融合到前面的层，或在 LLM-QAT 中的知识蒸馏[[50](#bib.bib50)]。
- en: IV-D $2$-bit Quantization
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D $2$-bit 量化
- en: 'To explore the limit of L²QER, we evaluate L²QER in the 2-bit quantization
    setup.  [Table V](#S4.T5 "In WikiText-2 ‣ IV-C Comparing with Existing Quantization
    Methods ‣ IV Experiments ‣ LQER: Low-Rank Quantization Error Reconstruction for
    LLMs") compares L²QER with OmniQuant and QuiP#⁷⁷7QuiP# is an improved version
    of QuiP released by the same research group: [https://github.com/Cornell-RelaxML/quip-sharp](https://github.com/Cornell-RelaxML/quip-sharp),
    which are both recent works optimized for extremely low-precision LLM quantization.
    We observe that 2-bit quantization is challenging for existing methods including
    L²QER. These methods perform inconsistently with model sizes and families ([Table XVII](#A5.T17
    "In Appendix E More evaluation results ‣ LQER: Low-Rank Quantization Error Reconstruction
    for LLMs") in [Appendix E](#A5 "Appendix E More evaluation results ‣ LQER: Low-Rank
    Quantization Error Reconstruction for LLMs")). Unlike a simple rank $k=32$ for
    2-bit quantization.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '为了探索L²QER的极限，我们在2-bit量化设置下评估了L²QER。[表V](#S4.T5 "在WikiText-2 ‣ IV-C 与现有量化方法比较
    ‣ IV 实验 ‣ LQER: 低秩量化误差重建用于LLMs")将L²QER与OmniQuant和QuiP#⁷⁷7QuiP#是同一研究小组发布的QuiP的改进版本：[https://github.com/Cornell-RelaxML/quip-sharp](https://github.com/Cornell-RelaxML/quip-sharp)进行了比较，这些都是最近为极低精度LLM量化优化的工作。我们观察到，对于包括L²QER在内的现有方法，2-bit量化是具有挑战性的。这些方法在模型大小和家族中的表现不一致（[表XVII](#A5.T17
    "在附录E 更多评估结果 ‣ LQER: 低秩量化误差重建用于LLMs")在[附录E](#A5 "附录E 更多评估结果 ‣ LQER: 低秩量化误差重建用于LLMs")中）。与2-bit量化的简单秩$k=32$不同。'
- en: IV-E L²QER with Different Model Familities
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E L²QER与不同模型家族
- en: 'To fully evaluate the adaptiveness of L²QER across model families, we have
    also conducted experiments to evaluate its effectivess on Vicuna and Mistral.
    The results of Vicuna-v1.5-7B/13B and Mistral-7B are included in [Appendix E](#A5
    "Appendix E More evaluation results ‣ LQER: Low-Rank Quantization Error Reconstruction
    for LLMs"). These results reveal a pattern consistent with other models, indicating
    that L²QER is agnostic to various LLM families.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '为了全面评估L²QER在模型家族中的适应性，我们还进行了实验，以评估其在Vicuna和Mistral上的有效性。Vicuna-v1.5-7B/13B和Mistral-7B的结果包含在[附录E](#A5
    "附录E 更多评估结果 ‣ LQER: 低秩量化误差重建用于LLMs")中。这些结果揭示了与其他模型一致的模式，表明L²QER对各种LLM家族具有不可知性。'
- en: V Conclusion
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: In this work, we propose a novel LLM post-training quantization framework, LQER,
    which judiciously combine quantization and low-rank approximation to recover model
    capbility. We then further propose L²QER, which leverages an activation-induced
    scale matrix to shape the singular values of quantization error towards a desirable
    distribution that can be accurate approximated. L²QER achieves nearly-losses perplexity
    (around $0.15$ on six different downstream tasks. The regular computation pattern
    of LQER ensures a higher hardware efficiency than existing methods and takes 67%
    smaller circuit area than FP16.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一个新颖的LLM后训练量化框架LQER，它明智地结合了量化和低秩近似来恢复模型能力。我们进一步提出了L²QER，它利用激活引起的缩放矩阵来调整量化误差的奇异值，使其接近可准确逼近的期望分布。L²QER在六个不同的下游任务中实现了几乎无损的困惑度（约为$0.15$）。LQER的常规计算模式确保了比现有方法更高的硬件效率，并且占用的电路面积比FP16小67%。
- en: VI Broader Impacts
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 更广泛的影响
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文展示了旨在推动机器学习领域的工作。我们的工作有许多潜在的社会影响，但我们认为这些影响在这里不需要特别强调。
- en: References
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell *et al.*, “Language models are few-shot learners,”
    *Advances in neural information processing systems*, vol. 33, pp. 1877–1901, 2020.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell *等*，“语言模型是少样本学习者，” *神经信息处理系统进展*，第33卷，第1877–1901页，2020年。'
- en: '[2] B. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow,
    R. Castagné, A. S. Luccioni, F. Yvon *et al.*, “Bloom: A 176b-parameter open-access
    multilingual language model,” *arXiv preprint arXiv:2211.05100*, 2022.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] B. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow,
    R. Castagné, A. S. Luccioni, F. Yvon *等*，“Bloom: 一个176b参数的开放访问多语言模型，” *arXiv预印本arXiv:2211.05100*，2022年。'
- en: '[3] A. S. Luccioni, S. Viguier, and A.-L. Ligozat, “Estimating the carbon footprint
    of bloom, a 176b parameter language model,” *Journal of Machine Learning Research*,
    vol. 24, no. 253, pp. 1–15, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. S. Luccioni, S. Viguier, 和 A.-L. Ligozat，“估计Bloom，一个176b参数语言模型的碳足迹，”
    *机器学习研究杂志*，第24卷，第253号，第1–15页，2023。'
- en: '[4] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford,
    D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark *et al.*, “Training compute-optimal
    large language models,” *arXiv preprint arXiv:2203.15556*, 2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford,
    D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark *等*，“训练计算最优的大型语言模型，” *arXiv
    预印本 arXiv:2203.15556*，2022。'
- en: '[5] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. Van Baalen, and
    T. Blankevoort, “A white paper on neural network quantization,” *arXiv preprint
    arXiv:2106.08295*, 2021.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. Van Baalen, 和
    T. Blankevoort，“神经网络量化白皮书，” *arXiv 预印本 arXiv:2106.08295*，2021。'
- en: '[6] X. Wei, Y. Zhang, X. Zhang, R. Gong, S. Zhang, Q. Zhang, F. Yu, and X. Liu,
    “Outlier suppression: Pushing the limit of low-bit transformer language models,”
    *Advances in Neural Information Processing Systems*, vol. 35, pp. 17 402–17 414,
    2022.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] X. Wei, Y. Zhang, X. Zhang, R. Gong, S. Zhang, Q. Zhang, F. Yu, 和 X. Liu，“Outlier
    suppression：推动低比特变换器语言模型的极限，” *神经信息处理系统进展*，第35卷，第17 402–17 414页，2022。'
- en: '[7] Y. Bondarenko, M. Nagel, and T. Blankevoort, “Understanding and overcoming
    the challenges of efficient transformer quantization,” *arXiv preprint arXiv:2109.12948*,
    2021.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Bondarenko, M. Nagel, 和 T. Blankevoort，“理解和克服高效变换器量化的挑战，” *arXiv 预印本
    arXiv:2109.12948*，2021。'
- en: '[8] H. Tang, Y. Sun, D. Wu, K. Liu, J. Zhu, and Z. Kang, “Easyquant: An efficient
    data-free quantization algorithm for llms,” in *Proceedings of the 2023 Conference
    on Empirical Methods in Natural Language Processing*, 2023, pp. 9119–9128.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] H. Tang, Y. Sun, D. Wu, K. Liu, J. Zhu, 和 Z. Kang，“Easyquant：一种高效的数据无关量化算法，”
    在 *2023年自然语言处理实证方法会议论文集*，2023，第9119–9128页。'
- en: '[9] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, “Llm. int8 (): 8-bit
    matrix multiplication for transformers at scale,” *arXiv preprint arXiv:2208.07339*,
    2022.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] T. Dettmers, M. Lewis, Y. Belkada, 和 L. Zettlemoyer，“Llm. int8 (): 大规模变换器的8位矩阵乘法，”
    *arXiv 预印本 arXiv:2208.07339*，2022。'
- en: '[10] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, “Gptq: Accurate
    post-training quantization for generative pre-trained transformers,” *arXiv preprint
    arXiv:2210.17323*, 2022.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] E. Frantar, S. Ashkboos, T. Hoefler, 和 D. Alistarh，“Gptq：用于生成预训练变换器的精确后训练量化，”
    *arXiv 预印本 arXiv:2210.17323*，2022。'
- en: '[11] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, “Smoothquant:
    Accurate and efficient post-training quantization for large language models,”
    in *International Conference on Machine Learning*.   PMLR, 2023, pp. 38 087–38 099.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, 和 S. Han，“Smoothquant：大型语言模型的准确高效后训练量化，”
    在 *国际机器学习会议*。PMLR，2023，第38 087–38 099页。'
- en: '[12] J. Lee, M. Kim, S. Baek, S. J. Hwang, W. Sung, and J. Choi, “Enhancing
    computation efficiency in large language models through weight and activation
    quantization,” *arXiv preprint arXiv:2311.05161*, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] J. Lee, M. Kim, S. Baek, S. J. Hwang, W. Sung, 和 J. Choi，“通过权重和激活量化提高大型语言模型的计算效率，”
    *arXiv 预印本 arXiv:2311.05161*，2023。'
- en: '[13] X. Wei, Y. Zhang, Y. Li, X. Zhang, R. Gong, J. Guo, and X. Liu, “Outlier
    suppression+: Accurate quantization of large language models by equivalent and
    optimal shifting and scaling,” *arXiv preprint arXiv:2304.09145*, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] X. Wei, Y. Zhang, Y. Li, X. Zhang, R. Gong, J. Guo, 和 X. Liu，“Outlier
    suppression+: 通过等效和优化的移位和缩放对大型语言模型进行精确量化，” *arXiv 预印本 arXiv:2304.09145*，2023。'
- en: '[14] Y. Bondarenko, M. Nagel, and T. Blankevoort, “Quantizable transformers:
    Removing outliers by helping attention heads do nothing,” *arXiv preprint arXiv:2306.12929*,
    2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. Bondarenko, M. Nagel, 和 T. Blankevoort，“可量化的变换器：通过帮助注意力头不做任何事来去除异常值，”
    *arXiv 预印本 arXiv:2306.12929*，2023。'
- en: '[15] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang, P. Gao, Y. Qiao,
    and P. Luo, “Omniquant: Omnidirectionally calibrated quantization for large language
    models,” *arXiv preprint arXiv:2308.13137*, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang, P. Gao, Y.
    Qiao, 和 P. Luo，“Omniquant：面向大型语言模型的全方向标定量化，” *arXiv 预印本 arXiv:2308.13137*，2023。'
- en: '[16] L. Lin, “LLM-Tracker: OmniQuant,” 2024\. [Online]. Available: [https://llm-tracker.info/howto/OmniQuant](https://llm-tracker.info/howto/OmniQuant)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] L. Lin，“LLM-Tracker: OmniQuant”，2024。 [在线]。可用：[https://llm-tracker.info/howto/OmniQuant](https://llm-tracker.info/howto/OmniQuant)'
- en: '[17] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, “Awq: Activation-aware
    weight quantization for llm compression and acceleration,” *arXiv preprint arXiv:2306.00978*,
    2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang 和 S. Han，“Awq：用于 LLM 压缩和加速的激活感知权重量化，”
    *arXiv 预印本 arXiv:2306.00978*，2023年。'
- en: '[18] C. Hansen, “Autoawq,” [https://github.com/casper-hansen/AutoAWQ](https://github.com/casper-hansen/AutoAWQ),
    2024.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] C. Hansen，“Autoawq，” [https://github.com/casper-hansen/AutoAWQ](https://github.com/casper-hansen/AutoAWQ)，2024年。'
- en: '[19] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar,
    S. Ashkboos, A. Borzunov, T. Hoefler, and D. Alistarh, “Spqr: A sparse-quantized
    representation for near-lossless llm weight compression,” *arXiv preprint arXiv:2306.03078*,
    2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar,
    S. Ashkboos, A. Borzunov, T. Hoefler 和 D. Alistarh，“Spqr：一种用于接近无损 LLM 权重压缩的稀疏量化表示，”
    *arXiv 预印本 arXiv:2306.03078*，2023年。'
- en: '[20] J. H. Lee, J. Kim, S. J. Kwon, and D. Lee, “Flexround: Learnable rounding
    based on element-wise division for post-training quantization,” *arXiv preprint
    arXiv:2306.00317*, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] J. H. Lee, J. Kim, S. J. Kwon 和 D. Lee，“Flexround：基于元素级除法的可学习舍入用于训练后量化，”
    *arXiv 预印本 arXiv:2306.00317*，2023年。'
- en: '[21] Y. Jeon, C. Lee, K. Park, and H.-y. Kim, “A frustratingly easy post-training
    quantization scheme for llms,” in *Proceedings of the 2023 Conference on Empirical
    Methods in Natural Language Processing*, 2023, pp. 14 446–14 461.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. Jeon, C. Lee, K. Park 和 H.-y. Kim，“一个令人沮丧的简单后训练量化方案用于 LLM，”在 *2023年自然语言处理实证方法会议论文集*，2023年，第14,446–14,461页。'
- en: '[22] J. Chee, Y. Cai, V. Kuleshov, and C. De Sa, “Quip: 2-bit quantization
    of large language models with guarantees,” *arXiv preprint arXiv:2307.13304*,
    2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Chee, Y. Cai, V. Kuleshov 和 C. De Sa，“Quip：具有保证的大型语言模型的2位量化，” *arXiv
    预印本 arXiv:2307.13304*，2023年。'
- en: '[23] Y. Luo, Y. Gao, Z. Zhang, J. Fan, H. Zhang, and M. Xu, “Long-range zero-shot
    generative deep network quantization,” *Neural Networks*, vol. 166, pp. 683–691,
    2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Y. Luo, Y. Gao, Z. Zhang, J. Fan, H. Zhang 和 M. Xu，“长程零样本生成深度网络量化，” *神经网络*，第166卷，第683–691页，2023年。'
- en: '[24] B. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary,
    M. Cornea, E. Dellinger, K. Denolf *et al.*, “Microscaling data formats for deep
    learning,” *arXiv preprint arXiv:2310.10537*, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] B. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D.
    Choudhary, M. Cornea, E. Dellinger, K. Denolf *等*，“深度学习的微缩数据格式，” *arXiv 预印本 arXiv:2310.10537*，2023年。'
- en: '[25] V. Marchenko and L. Pastur, “Distribution of eigenvalues for some sets
    of random matrices,” *Mat. Sb*, vol. 72, pp. 507–536, 1967.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] V. Marchenko 和 L. Pastur，“一些随机矩阵集的特征值分布，” *数学公报*，第72卷，第507–536页，1967年。'
- en: '[26] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin *et al.*, “Opt: Open pre-trained transformer language
    models,” *arXiv preprint arXiv:2205.01068*, 2022.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin *等*，“Opt：开放预训练变换器语言模型，” *arXiv 预印本 arXiv:2205.01068*，2022年。'
- en: '[27] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, “Qllm: Accurate
    and efficient low-bitwidth quantization for large language models,” *arXiv preprint
    arXiv:2310.08041*, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai 和 B. Zhuang，“Qllm：针对大型语言模型的精确高效低位宽量化，”
    *arXiv 预印本 arXiv:2310.08041*，2023年。'
- en: '[28] B. Darvish Rouhani, D. Lo, R. Zhao, M. Liu, J. Fowers, K. Ovtcharov, A. Vinogradsky,
    S. Massengill, L. Yang, R. Bittner *et al.*, “Pushing the limits of narrow precision
    inferencing at cloud scale with microsoft floating point,” *Advances in neural
    information processing systems*, vol. 33, pp. 10 271–10 281, 2020.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] B. Darvish Rouhani, D. Lo, R. Zhao, M. Liu, J. Fowers, K. Ovtcharov, A.
    Vinogradsky, S. Massengill, L. Yang, R. Bittner *等*，“使用微软浮点推挤窄精度推理的极限，” *神经信息处理系统进展*，第33卷，第10,271–10,281页，2020年。'
- en: '[29] S. Fox, S. Rasoulinezhad, J. Faraone, P. Leong *et al.*, “A block minifloat
    representation for training deep neural networks,” in *International Conference
    on Learning Representations*, 2020.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S. Fox, S. Rasoulinezhad, J. Faraone, P. Leong *等*，“用于训练深度神经网络的块状微浮点表示，”在
    *国际学习表征会议*，2020年。'
- en: '[30] S. Q. Zhang, B. McDanel, and H. Kung, “Fast: Dnn training under variable
    precision block floating point with stochastic rounding,” in *2022 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA)*.   IEEE, 2022, pp.
    846–860.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] S. Q. Zhang, B. McDanel 和 H. Kung，“Fast：在变精度块浮点和随机舍入下训练 DNN，”在 *2022 IEEE
    高性能计算架构国际研讨会 (HPCA)*。 IEEE，2022年，第846–860页。'
- en: '[31] M. Drumond, T. Lin, M. Jaggi, and B. Falsafi, “Training dnns with hybrid
    block floating point,” *Advances in Neural Information Processing Systems*, vol. 31,
    2018.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] M. Drumond, T. Lin, M. Jaggi 和 B. Falsafi，“使用混合块浮点训练 DNN，” *神经信息处理系统进展*，第31卷，2018年。'
- en: '[32] B. Rouhani, R. Zhao, V. Elango, R. Shafipour, M. Hall, M. Mesmakhosroshahi,
    A. More, L. Melnick, M. Golub, G. Varatkar *et al.*, “Shared microexponents: A
    little shifting goes a long way,” *arXiv preprint arXiv:2302.08007*, 2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] B. Rouhani, R. Zhao, V. Elango, R. Shafipour, M. Hall, M. Mesmakhosroshahi,
    A. More, L. Melnick, M. Golub, G. Varatkar *等*, “共享微指数：一点小的调整却成效显著,” *arXiv 预印本
    arXiv:2302.08007*, 2023年。'
- en: '[33] P. Micikevicius, S. Oberman, P. Dubey, M. Cornea, A. Rodriguez, I. Bratt,
    R. Grisenthwaite, N. Jouppi, C. Chou, A. Huffman, M. Schulte, R. Wittig, D. Jani,
    and S. Deng, “Ocp 8-bit floating point specification (ofp8),” 2023\. [Online].
    Available: [https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] P. Micikevicius, S. Oberman, P. Dubey, M. Cornea, A. Rodriguez, I. Bratt,
    R. Grisenthwaite, N. Jouppi, C. Chou, A. Huffman, M. Schulte, R. Wittig, D. Jani,
    和 S. Deng, “Ocp 8-bit 浮点规范 (ofp8),” 2023年。[在线]. 可用: [https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf)'
- en: '[34] C. Zhang, J. Cheng, I. Shumailov, G. Constantinides, and Y. Zhao, “Revisiting
    block-based quantisation: What is important for sub-8-bit LLM inference?” in *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, H. Bouamor,
    J. Pino, and K. Bali, Eds.   Singapore: Association for Computational Linguistics,
    Dec. 2023, pp. 9988–10 006\. [Online]. Available: [https://aclanthology.org/2023.emnlp-main.617](https://aclanthology.org/2023.emnlp-main.617)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] C. Zhang, J. Cheng, I. Shumailov, G. Constantinides, 和 Y. Zhao, “重新审视基于块的量化：什么对亚8位LLM推理至关重要？”
    在 *2023年自然语言处理经验方法大会论文集*，H. Bouamor, J. Pino, 和 K. Bali 编辑。 新加坡：计算语言学协会，2023年12月，第9988–10006页。[在线].
    可用: [https://aclanthology.org/2023.emnlp-main.617](https://aclanthology.org/2023.emnlp-main.617)'
- en: '[35] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
    W. Chen, “Lora: Low-rank adaptation of large language models,” *arXiv preprint
    arXiv:2106.09685*, 2021.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, 和
    W. Chen, “Lora: 大型语言模型的低秩适应,” *arXiv 预印本 arXiv:2106.09685*, 2021年。'
- en: '[36] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient
    finetuning of quantized llms,” *arXiv preprint arXiv:2305.14314*, 2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] T. Dettmers, A. Pagnoni, A. Holtzman, 和 L. Zettlemoyer, “Qlora: 高效的量化LLM微调,”
    *arXiv 预印本 arXiv:2305.14314*, 2023年。'
- en: '[37] Y. Li, Y. Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao,
    “Loftq: Lora-fine-tuning-aware quantization for large language models,” *arXiv
    preprint arXiv:2310.08659*, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Y. Li, Y. Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, 和 T. Zhao,
    “Loftq: Lora-微调感知量化用于大型语言模型,” *arXiv 预印本 arXiv:2310.08659*, 2023年。'
- en: '[38] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar *et al.*, “Llama: Open and efficient
    foundation language models,” *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar *等*, “Llama: 开放且高效的基础语言模型,” *arXiv 预印本
    arXiv:2302.13971*, 2023年。'
- en: '[39] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale *et al.*, “Llama 2: Open foundation and fine-tuned
    chat models,” *arXiv preprint arXiv:2307.09288*, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale *等*, “Llama 2: 开放基础和微调聊天模型,” *arXiv
    预印本 arXiv:2307.09288*, 2023年。'
- en: '[40] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin,
    Z. Li, D. Li, E. Xing *et al.*, “Judging llm-as-a-judge with mt-bench and chatbot
    arena,” *arXiv preprint arXiv:2306.05685*, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin,
    Z. Li, D. Li, E. Xing *等*, “利用mt-bench和chatbot arena评判LLM作为裁判的表现,” *arXiv 预印本
    arXiv:2306.05685*, 2023年。'
- en: '[41] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l.
    Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier *et al.*, “Mistral 7b,”
    *arXiv preprint arXiv:2310.06825*, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D.
    d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier *等*, “Mistral 7b,”
    *arXiv 预印本 arXiv:2310.06825*, 2023年。'
- en: '[42] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel mixture
    models,” 2016.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] S. Merity, C. Xiong, J. Bradbury, 和 R. Socher, “Pointer sentinel 混合模型,”
    2016年。'
- en: '[43] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
    and O. Tafjord, “Think you have solved question answering? try arc, the ai2 reasoning
    challenge,” *arXiv:1803.05457v1*, 2018.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
    和 O. Tafjord, “认为你已经解决了问答问题？尝试arc，AI2推理挑战,” *arXiv:1803.05457v1*, 2018年。'
- en: '[44] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle,
    M. Baroni, G. Boleda, and R. Fernández, “The lambada dataset: Word prediction
    requiring a broad discourse context,” *arXiv preprint arXiv:1606.06031*, 2016.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle,
    M. Baroni, G. Boleda, 和 R. Fernández, “lambada 数据集：需要广泛话语上下文的词预测，” *arXiv 预印本
    arXiv:1606.06031*，2016。'
- en: '[45] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “Piqa: Reasoning
    about physical commonsense in natural language,” in *Thirty-Fourth AAAI Conference
    on Artificial Intelligence*, 2020.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, 和 Y. Choi, “Piqa：在自然语言中推理物理常识，”
    见 *第三十四届 AAAI 人工智能大会*，2020。'
- en: '[46] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of armor
    conduct electricity? a new dataset for open book question answering,” in *EMNLP*,
    2018.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] T. Mihaylov, P. Clark, T. Khot, 和 A. Sabharwal, “盔甲能导电吗？一个用于开放书籍问答的新数据集，”
    见 *EMNLP*，2018。'
- en: '[47] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova,
    “Boolq: Exploring the surprising difficulty of natural yes/no questions,” *arXiv
    preprint arXiv:1905.10044*, 2019.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, 和 K. Toutanova,
    “Boolq：探索自然是/否问题的惊人困难，” *arXiv 预印本 arXiv:1905.10044*，2019。'
- en: '[48] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster,
    L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa,
    J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite,
    B. Wang, K. Wang, and A. Zou, “A framework for few-shot language model evaluation,”
    12 2023\. [Online]. Available: [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster,
    L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa,
    J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite,
    B. Wang, K. Wang, 和 A. Zou, “少量样本语言模型评估框架，” 2023年12月。 [在线]. 可用： [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)'
- en: '[49] D. Soboleva, F. Al-Khateeb, R. Myers, J. R. Steeves, J. Hestness, and
    N. Dey, “SlimPajama: A 627B token cleaned and deduplicated version of RedPajama,”
    [https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama),
    2023\. [Online]. Available: [https://huggingface.co/datasets/cerebras/SlimPajama-627B](https://huggingface.co/datasets/cerebras/SlimPajama-627B)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] D. Soboleva, F. Al-Khateeb, R. Myers, J. R. Steeves, J. Hestness, 和 N.
    Dey, “SlimPajama：RedPajama 的 627B 标记清洗和去重版本，” [https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama)，2023。
    [在线]. 可用： [https://huggingface.co/datasets/cerebras/SlimPajama-627B](https://huggingface.co/datasets/cerebras/SlimPajama-627B)'
- en: '[50] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi,
    and V. Chandra, “Llm-qat: Data-free quantization aware training for large language
    models,” *arXiv preprint arXiv:2305.17888*, 2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi,
    和 V. Chandra, “LLM-QAT：大语言模型的无数据量化感知训练，” *arXiv 预印本 arXiv:2305.17888*，2023。'
- en: Appendix A Data Calibration
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 数据校准
- en: Given a calibration dataset containing $N$, we first profile the activation
    magnitude for each channel,
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含 $N$ 的校准数据集，我们首先为每个通道分析激活幅度，
- en: '|  | $$\begin{split}\mathbf{a}_{i}&amp;=\text{mean}(&#124;X_{i}&#124;,\text{axis}=0),\\
    \mathbf{\bar{a}}&amp;=\max(\begin{bmatrix}\mathbf{a}_{1}\\'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\begin{split}\mathbf{a}_{i}&amp;=\text{mean}(&#124;X_{i}&#124;,\text{axis}=0),\\
    \mathbf{\bar{a}}&amp;=\max(\begin{bmatrix}\mathbf{a}_{1}\\'
- en: \vdots\\
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: \mathbf{a}_{N}\end{bmatrix},\text{axis}=0),\end{split}$$ |  | (13) |
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: \mathbf{a}_{N}\end{bmatrix},\text{axis}=0),\end{split}$$ |  | (13) |
- en: 'where $|\cdot|$:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $|\cdot|$：
- en: '|  | $s_{i}=\frac{a_{i}}{\sqrt{\min(\mathbf{\bar{a}})\times\max(\mathbf{\bar{a}})}},$
    |  | (14) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{i}=\frac{a_{i}}{\sqrt{\min(\mathbf{\bar{a}})\times\max(\mathbf{\bar{a}})}},$
    |  | (14) |'
- en: '[Equation 13](#A1.E13 "In Appendix A Data Calibration ‣ LQER: Low-Rank Quantization
    Error Reconstruction for LLMs") and [Equation 14](#A1.E14 "In Appendix A Data
    Calibration ‣ LQER: Low-Rank Quantization Error Reconstruction for LLMs") are
    empirical implementation based on [[17](#bib.bib17)]. We leave the exploration
    of an analytical derivation of $S$ as future work.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程式 13](#A1.E13 "在附录 A 数据校准 ‣ LQER: 低秩量化误差重建用于 LLMs") 和 [方程式 14](#A1.E14 "在附录
    A 数据校准 ‣ LQER: 低秩量化误差重建用于 LLMs") 是基于 [[17](#bib.bib17)] 的经验实现。我们将 $S$ 的分析推导留待未来工作。'
- en: Appendix B Singular Value Distributions of LQER and L²QER
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B LQER 和 L²QER 的奇异值分布
- en: 'Here we present a few representative singular value distributions of quantization
    errors for LLaMA-7B in [Figure 4](#A2.F4 "In Appendix B Singular Value Distributions
    of LQER and L2QER ‣ LQER: Low-Rank Quantization Error Reconstruction for LLMs").
    We observe that the singular values of most layers are driven towards desirable
    distributions by L²QER. Most O projection layers and a few layers at the beginning
    or the end of the model tend to have an Eq distribution decaying slowly.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [图4](#A2.F4 "附录B LQER 和 L2QER 的奇异值分布 ‣ LQER: 低秩量化误差重建") 中，我们展示了一些 LLaMA-7B
    量化误差的代表性奇异值分布。我们观察到大多数层的奇异值被 L²QER 驱动到理想分布。大多数 O 投影层以及模型开始或结束时的几层往往具有 Eq 分布缓慢衰减。'
- en: 'We also visualzie the approximation error of LQER and LQER versus layer index
    in [Figure 5](#A2.F5 "In Appendix B Singular Value Distributions of LQER and L2QER
    ‣ LQER: Low-Rank Quantization Error Reconstruction for LLMs"). The approximation
    error is measured as:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还在 [图5](#A2.F5 "附录B LQER 和 L2QER 的奇异值分布 ‣ LQER: 低秩量化误差重建") 中可视化了 LQER 和 L²QER
    与层索引的近似误差。近似误差的计算公式为：'
- en: '|  | $e_{a}=\frac{1}{mn}\sum_{i=1}^{m}\sum_{j=1}^{n}(&#124;E_{q}-\widetilde{E}_{q}&#124;_{i,j})$
    |  | (15) |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | $e_{a}=\frac{1}{mn}\sum_{i=1}^{m}\sum_{j=1}^{n}(&#124;E_{q}-\widetilde{E}_{q}&#124;_{i,j})$
    |  | (15) |'
- en: where $|\cdot|$ calculate the element-wise absolute value. L²QER reconstructs
    the quantization error more accurate than LQER on most layers, while LQER better
    reconstruct the K, Q, and V projection layers at the 1st, 3rd, and 4th transformer
    layers.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $|\cdot|$ 计算逐元素绝对值。L²QER 在大多数层上比 LQER 更准确地重建量化误差，而 LQER 更好地重建 1、3 和 4 层变换器中的
    K、Q 和 V 投影层。
- en: '![Refer to caption](img/ba590d0bbfcc765dcfe9dba2b0577aff.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ba590d0bbfcc765dcfe9dba2b0577aff.png)'
- en: ((a)) The 4th K layer
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ((a)) 第四层 K
- en: '![Refer to caption](img/d35aee0f75a5c9c191548b771129f60f.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d35aee0f75a5c9c191548b771129f60f.png)'
- en: ((b)) The 4th Q layer
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ((b)) 第四层 Q
- en: '![Refer to caption](img/b0f6497cfb310fe7767b3c1dd4bdb4fc.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b0f6497cfb310fe7767b3c1dd4bdb4fc.png)'
- en: ((c)) The 4th V layer
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ((c)) 第四层 V
- en: '![Refer to caption](img/d5a5dd19010784c952cd06b48810361e.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d5a5dd19010784c952cd06b48810361e.png)'
- en: ((d)) The 4th O layer
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ((d)) 第四层 O
- en: '![Refer to caption](img/02d63ee34ccd72048598f79bfde93386.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/02d63ee34ccd72048598f79bfde93386.png)'
- en: ((e)) The 4th Up layer
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ((e)) 第四层 Up
- en: '![Refer to caption](img/be26e035d11b64ffffa79ef3f179f75d.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/be26e035d11b64ffffa79ef3f179f75d.png)'
- en: ((f)) The 4th Down layer
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ((f)) 第四层 Down
- en: '![Refer to caption](img/7c4cc740e39a6ed1079db1b799e1d1a8.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7c4cc740e39a6ed1079db1b799e1d1a8.png)'
- en: ((g)) The 4th Gate layer
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ((g)) 第四层 Gate
- en: '![Refer to caption](img/4b9c2e5c825aa4d7b93b05229cacd7c8.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4b9c2e5c825aa4d7b93b05229cacd7c8.png)'
- en: ((h)) The 19th Gate layer
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ((h)) 第十九层 Gate
- en: 'Figure 4: The singular value distributions of LQER and L²QER. We find that
    the shaping effect of L²QER are obvious on most layers in LLaMA. Most O projection
    layers and several layers at the beginning or the end of the model tend to have
    an $E_{q}$ distribution decaying slowly (See [Figure 4(d)](#A2.F4.sf4 "In Figure
    4 ‣ Appendix B Singular Value Distributions of LQER and L2QER ‣ LQER: Low-Rank
    Quantization Error Reconstruction for LLMs") and [Figure 4(h)](#A2.F4.sf8 "In
    Figure 4 ‣ Appendix B Singular Value Distributions of LQER and L2QER ‣ LQER: Low-Rank
    Quantization Error Reconstruction for LLMs")).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '图4：LQER 和 L²QER 的奇异值分布。我们发现 L²QER 在 LLaMA 的大多数层上具有明显的塑形效果。大多数 O 投影层以及模型开始或结束时的几层往往具有
    $E_{q}$ 分布缓慢衰减（见 [图4(d)](#A2.F4.sf4 "图4 ‣ 附录B LQER 和 L2QER 的奇异值分布 ‣ LQER: 低秩量化误差重建")
    和 [图4(h)](#A2.F4.sf8 "图4 ‣ 附录B LQER 和 L2QER 的奇异值分布 ‣ LQER: 低秩量化误差重建")）。'
- en: '![Refer to caption](img/d4f8820503d5e6b91a19a04b289a836b.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d4f8820503d5e6b91a19a04b289a836b.png)'
- en: 'Figure 5: Approximation error of LQER and L²QER across decoder layers in LLaMA-7B.
    L²QER produces smaller approximation errors on most of the linear layers in transformer-based
    LLMs. However, there are a few layers better reconstructed by LQER, such as the
    key, value, output project layers in 1st, 3rd, and 4th decoder layer. The derivation
    of $S$ worths further exploration.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：LQER 和 L²QER 在 LLaMA-7B 解码器层中的近似误差。L²QER 在大多数基于变换器的 LLM 的线性层上产生了较小的近似误差。然而，LQER
    更好地重建了 1、3 和 4 解码器层中的键、值、输出投影层。$S$ 的推导值得进一步探索。
- en: Appendix C Inconsistant performance of OmniQuant on WikiText2 and downstream
    tasks
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C OmniQuant 在 WikiText2 和下游任务中的不一致表现
- en: OmniQuant is one of the state-of-the-art LLM post-training-quantization methods
    we compared in this work. Thanks for the official open-sourced implementation
    and quantization parameter checkpoints, we performed extensive experiments to
    compare OmniQuant to LQER. We sucessfully reproduce the perplexity and downstream
    task accuracy of OPT-family. However, the LLaMA models quantized by OmniQuant
    have obvious performance degradation on downstream tasks, around 18.9% lower than
    FP16 baselines on average.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: OmniQuant 是我们在本研究中比较的最先进的 LLM 后训练量化方法之一。感谢官方开源实现和量化参数检查点，我们进行了广泛的实验，将 OmniQuant
    与 LQER 进行比较。我们成功地重现了 OPT 系列的困惑度和下游任务准确性。然而，OmniQuant 量化的 LLaMA 模型在下游任务上的性能明显下降，平均比
    FP16 基线低约 18.9%。
- en: We attribute this performance degradation to the iterative gradient-base training
    on WikiText2 in OmniQuant. As stated in [[15](#bib.bib15)], OmniQuant optimizes
    the quantization parameter (shifts and scales) by training on WikiText2 samples
    for 20 epochs (40 epochs for W2A16). This training requires tuning the hyper-parameters
    such as number of training samples, learning rates and total number of epochs,
    which may cause overfitting or underfitting if not tuned properly. Both cases
    can be the reason for performance degradation.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这种性能下降归因于 OmniQuant 对 WikiText2 的迭代梯度基础训练。如 [[15](#bib.bib15)] 所述，OmniQuant
    通过在 WikiText2 样本上训练 20 个周期（W2A16 为 40 个周期）来优化量化参数（偏移量和缩放量）。这种训练需要调整超参数，如训练样本数量、学习率和总周期数，如果调整不当，可能导致过拟合或欠拟合。这两种情况都可能导致性能下降。
- en: Appendix D Estimate Hardware Cost
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 估算硬件成本
- en: We estimate the memory efficiency with average bitwidth. The average bitwidth
    of per-channel scaled quantization is considered as the average bits of an FP16
    scalor and $m$. L²QER outperforms existing nearly lossless methods in terms of
    circuit area, because it is free from expensive element-wise dequantization (GPTQ
    and AWQ), or scatter/gather operations (LLM.int4()) at runtime.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过平均比特宽度来估算内存效率。每通道缩放量化的平均比特宽度被视为 FP16 标度器和 $m$ 的平均比特。L²QER 在电路面积方面优于现有的几乎无损方法，因为它不受昂贵的逐元素去量化（GPTQ
    和 AWQ）或运行时的散布/收集操作（LLM.int4()）的影响。
- en: We estimate the hardware cost with circuit area. We mapped the algorithms of
    these approaches onto custom hardware accelerators on FPGAs. To ensure fairness,
    these hardware accelerators have the same throughput of 16 multiply-accumulate
    (MAC) operations per clock cycle when computing a linear operation of the same
    matrix sizes. We then measure the circuit area in terms of LUTs and Digital Signal
    Processing blocks (DSPs) on the FPGA, where a DSP is treated as 100 LUTs. The
    area results were measured from the Place & Route report in Xilinx Vivado 2023.1\.
    The FPGA family that we used for all the experiments is Xilinx Alveo U250.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过电路面积来估算硬件成本。我们将这些方法的算法映射到 FPGA 上的定制硬件加速器上。为了确保公平，这些硬件加速器在计算相同矩阵大小的线性操作时具有相同的每时钟周期
    16 次乘加（MAC）操作吞吐量。然后我们测量 FPGA 上的电路面积，以 LUT 和数字信号处理块（DSPs）来衡量，其中 DSP 视为 100 个 LUT。面积结果来自
    Xilinx Vivado 2023.1 的 Place & Route 报告。我们用于所有实验的 FPGA 系列是 Xilinx Alveo U250。
- en: Appendix E More evaluation results
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 更多评估结果
- en: 'We present the complete results of each specific downstream tasks in [Tables VI](#A5.T6
    "In Appendix E More evaluation results ‣ LQER: Low-Rank Quantization Error Reconstruction
    for LLMs"), [VII](#A5.T7 "Table VII ‣ Appendix E More evaluation results ‣ LQER:
    Low-Rank Quantization Error Reconstruction for LLMs"), [VIII](#A5.T8 "Table VIII
    ‣ Appendix E More evaluation results ‣ LQER: Low-Rank Quantization Error Reconstruction
    for LLMs"), [IX](#A5.T9 "Table IX ‣ Appendix E More evaluation results ‣ LQER:
    Low-Rank Quantization Error Reconstruction for LLMs"), [X](#A5.T10 "Table X ‣
    Appendix E More evaluation results ‣ LQER: Low-Rank Quantization Error Reconstruction
    for LLMs"), [XI](#A5.T11 "Table XI ‣ Appendix E More evaluation results ‣ LQER:
    Low-Rank Quantization Error Reconstruction for LLMs"), [XII](#A5.T12 "Table XII
    ‣ Appendix E More evaluation results ‣ LQER: Low-Rank Quantization Error Reconstruction
    for LLMs") and [XIII](#A5.T13 "Table XIII ‣ Appendix E More evaluation results
    ‣ LQER: Low-Rank Quantization Error Reconstruction for LLMs"). We also tested
    L²QER on Vicuna-7b/13b and Mistral-7b-v0.1 in [Tables XIV](#A5.T14 "In Appendix
    E More evaluation results ‣ LQER: Low-Rank Quantization Error Reconstruction for
    LLMs"), [XV](#A5.T15 "Table XV ‣ Appendix E More evaluation results ‣ LQER: Low-Rank
    Quantization Error Reconstruction for LLMs") and [XVI](#A5.T16 "Table XVI ‣ Appendix
    E More evaluation results ‣ LQER: Low-Rank Quantization Error Reconstruction for
    LLMs").'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在[表 VI](#A5.T6 "在附录 E 更多评估结果 ‣ LQER: 低秩量化误差重建")、[表 VII](#A5.T7 "表 VII ‣ 附录
    E 更多评估结果 ‣ LQER: 低秩量化误差重建")、[表 VIII](#A5.T8 "表 VIII ‣ 附录 E 更多评估结果 ‣ LQER: 低秩量化误差重建")、[表
    IX](#A5.T9 "表 IX ‣ 附录 E 更多评估结果 ‣ LQER: 低秩量化误差重建")、[表 X](#A5.T10 "表 X ‣ 附录 E 更多评估结果
    ‣ LQER: 低秩量化误差重建")、[表 XI](#A5.T11 "表 XI ‣ 附录 E 更多评估结果 ‣ LQER: 低秩量化误差重建")、[表 XII](#A5.T12
    "表 XII ‣ 附录 E 更多评估结果 ‣ LQER: 低秩量化误差重建") 和 [表 XIII](#A5.T13 "表 XIII ‣ 附录 E 更多评估结果
    ‣ LQER: 低秩量化误差重建") 中展示了每个具体下游任务的完整结果。我们还在[表 XIV](#A5.T14 "在附录 E 更多评估结果 ‣ LQER:
    低秩量化误差重建")、[表 XV](#A5.T15 "表 XV ‣ 附录 E 更多评估结果 ‣ LQER: 低秩量化误差重建") 和 [表 XVI](#A5.T16
    "表 XVI ‣ 附录 E 更多评估结果 ‣ LQER: 低秩量化误差重建") 中测试了 L²QER 在 Vicuna-7b/13b 和 Mistral-7b-v0.1
    上的表现。'
- en: 'TABLE VI: OPT-6.7B'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VI: OPT-6.7B'
- en: '| Method | WikiText2 | ARC (easy) | ARC (challenge) | LAMBADA | PIQA | BOOLQ
    | OpenbookQA | Avg. Accuracy |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WikiText2 | ARC (简单) | ARC (挑战) | LAMBADA | PIQA | BOOLQ | OpenbookQA
    | 平均准确率 |'
- en: '| FP16 | 10.86 | 65.6% | 30.5% | 67.7% | 76.3% | 66.1% | 27.6% | 55.6% |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 10.86 | 65.6% | 30.5% | 67.7% | 76.3% | 66.1% | 27.6% | 55.6% |'
- en: '| GPTQ | 10.95 | 65.6% | 31.1% | 68.5% | 76.2% | 65.2% | 26.2% | 55.4% |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 10.95 | 65.6% | 31.1% | 68.5% | 76.2% | 65.2% | 26.2% | 55.4% |'
- en: '| AWQ | 10.93 | 65.3% | 30.5% | 67.4% | 76.6% | 65.2% | 26.6% | 55.3% |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 10.93 | 65.3% | 30.5% | 67.4% | 76.6% | 65.2% | 26.6% | 55.3% |'
- en: '| LLM.int4() | 11.23 | 65.3% | 30.5% | 67.4% | 76.6% | 65.2% | 26.6% | 55.3%
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int4() | 11.23 | 65.3% | 30.5% | 67.4% | 76.6% | 65.2% | 26.6% | 55.3%
    |'
- en: '| OmniQuant (W6A6) | 10.96 | 65.4% | 30.9% | 66.9% | 76.0% | 66.2% | 26.8%
    | 55.4% |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant (W6A6) | 10.96 | 65.4% | 30.9% | 66.9% | 76.0% | 66.2% | 26.8%
    | 55.4% |'
- en: '| LQER-INT (W4A8) | 11.10 | 63.8% | 29.6% | 65.7% | 75.6% | 63.1% | 26.8% |
    54.1% |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| LQER-INT (W4A8) | 11.10 | 63.8% | 29.6% | 65.7% | 75.6% | 63.1% | 26.8% |
    54.1% |'
- en: '| LQER-MXINT (W4A6) | 11.03 | 65.4% | 30.5% | 65.6% | 75.4% | 64.0% | 27.6%
    | 54.7% |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A6) | 11.03 | 65.4% | 30.5% | 65.6% | 75.4% | 64.0% | 27.6%
    | 54.7% |'
- en: '| LQER-MXINT (W4A8) | 11.00 | 65.2% | 30.4% | 66.3% | 75.5% | 65.3% | 27.6%
    | 55.0% |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A8) | 11.00 | 65.2% | 30.4% | 66.3% | 75.5% | 65.3% | 27.6%
    | 55.0% |'
- en: 'TABLE VII: OPT-13B'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VII: OPT-13B'
- en: '| Method | WikiText2 | ARC (easy) | ARC (challenge) | LAMBADA | PIQA | BOOLQ
    | OpenbookQA | Avg. Accuracy |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WikiText2 | ARC (简单) | ARC (挑战) | LAMBADA | PIQA | BOOLQ | OpenbookQA
    | 平均准确率 |'
- en: '| FP16 | 10.13 | 67.1% | 32.9% | 68.6% | 76.0% | 65.8% | 27.0% | 56.2% |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 10.13 | 67.1% | 32.9% | 68.6% | 76.0% | 65.8% | 27.0% | 56.2% |'
- en: '| GPTQ | 10.31 | 67.5% | 32.8% | 68.8% | 76.1% | 65.9% | 27.2% | 56.4% |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 10.31 | 67.5% | 32.8% | 68.8% | 76.1% | 65.9% | 27.2% | 56.4% |'
- en: '| AWQ | 10.21 | 66.8% | 33.3% | 68.2% | 75.6% | 66.5% | 28.0% | 56.4% |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 10.21 | 66.8% | 33.3% | 68.2% | 75.6% | 66.5% | 28.0% | 56.4% |'
- en: '| LLM.int4() | 10.39 | 66.2% | 33.6% | 67.8% | 76.2% | 67.3% | 24.2% | 55.9%
    |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int4() | 10.39 | 66.2% | 33.6% | 67.8% | 76.2% | 67.3% | 24.2% | 55.9%
    |'
- en: '| OmniQuant (W6A6) | 10.96 | 67.1% | 33.1% | 68.4% | 76.2% | 65.3% | 26.4%
    | 56.1% |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant (W6A6) | 10.96 | 67.1% | 33.1% | 68.4% | 76.2% | 65.3% | 26.4%
    | 56.1% |'
- en: '| LQER-INT (W4A8) | 10.38 | 66.5% | 33.2% | 67.5% | 75.5% | 67.9% | 26.4% |
    56.2% |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| LQER-INT (W4A8) | 10.38 | 66.5% | 33.2% | 67.5% | 75.5% | 67.9% | 26.4% |
    56.2% |'
- en: '| LQER-MXINT (W4A6) | 10.32 | 67.2% | 32.2% | 67.9% | 75.7% | 68.3% | 25.8%
    | 56.2% |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A6) | 10.32 | 67.2% | 32.2% | 67.9% | 75.7% | 68.3% | 25.8%
    | 56.2% |'
- en: '| LQER-MXINT (W4A8) | 10.27 | 67.4% | 32.6% | 68.4% | 76.1% | 68.3% | 26.2%
    | 56.5% |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A8) | 10.27 | 67.4% | 32.6% | 68.4% | 76.1% | 68.3% | 26.2%
    | 56.5% |'
- en: 'TABLE VIII: OPT-6.7B'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VIII: OPT-6.7B'
- en: '| Method | WikiText2 | ARC (easy) | ARC (challenge) | LAMBADA | PIQA | BOOLQ
    | OpenbookQA | Avg. Accuracy |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WikiText2 | ARC (简单) | ARC (挑战) | LAMBADA | PIQA | BOOLQ | OpenbookQA
    | 平均准确率 |'
- en: '| FP16 | 9.56 | 70.0% | 34.6% | 71.5% | 77.6% | 70.5% | 30.2% | 59.1% |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 9.56 | 70.0% | 34.6% | 71.5% | 77.6% | 70.5% | 30.2% | 59.1% |'
- en: '| GPTQ | 9.63 | 62.2% | 29.4% | 74.9% | 67.6% | 69.1% | 23.8% | 54.5% |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 9.63 | 62.2% | 29.4% | 74.9% | 67.6% | 69.1% | 23.8% | 54.5% |'
- en: '| AWQ | 9.59 | 69.7% | 34.6% | 71.6% | 77.3% | 70.4% | 30.0% | 58.9% |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 9.59 | 69.7% | 34.6% | 71.6% | 77.3% | 70.4% | 30.0% | 58.9% |'
- en: '| LLM.int4() | 10.01 | 69.0% | 32.8% | 71.3% | 76.9% | 70.2% | 27.8% | 58.0%
    |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int4() | 10.01 | 69.0% | 32.8% | 71.3% | 76.9% | 70.2% | 27.8% | 58.0%
    |'
- en: '| OmniQuant (W6A6) | 9.62 | 70.1% | 34.2% | 70.4% | 77.3% | 70.2% | 29.6% |
    58.6% |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant (W6A6) | 9.62 | 70.1% | 34.2% | 70.4% | 77.3% | 70.2% | 29.6% |
    58.6% |'
- en: '| LQER-INT (W4A8) | 9.72 |  |  |  |  |  |  |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| LQER-INT (W4A8) | 9.72 |  |  |  |  |  |  |  |'
- en: '| LQER-MXINT (W4A6) | 9.72 | 0.6990740741 | 0.3421501706 | 0.7050261983 | 0.7725788901
    | 0.6923547401 | 0.298 | 58.5% |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A6) | 9.72 | 0.6990740741 | 0.3421501706 | 0.7050261983 | 0.7725788901
    | 0.6923547401 | 0.298 | 58.5% |'
- en: '| LQER-MXINT (W4A8) | 9.67 | 69.4% | 34.4% | 70.4% | 77.3% | 69.5% | 29.6%
    | 58.4% |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A8) | 9.67 | 69.4% | 34.4% | 70.4% | 77.3% | 69.5% | 29.6%
    | 58.4% |'
- en: 'TABLE IX: llama-7B'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IX: llama-7B'
- en: '| Method | WikiText2 | ARC (easy) | ARC (challenge) | LAMBADA | PIQA | BOOLQ
    | OpenbookQA | Avg. Accuracy |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WikiText2 | ARC (简单) | ARC (挑战) | LAMBADA | PIQA | BOOLQ | OpenbookQA
    | 平均准确率 |'
- en: '| FP16 | 5.10 | 77.4% | 46.4% | 76.2% | 79.1% | 78.0% | 33.2% | 65.0% |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 5.10 | 77.4% | 46.4% | 76.2% | 79.1% | 78.0% | 33.2% | 65.0% |'
- en: '| GPTQ | 5.21 | 76.9% | 46.8% | 75.0% | 79.3% | 76.4% | 34.0% | 64.7% |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 5.21 | 76.9% | 46.8% | 75.0% | 79.3% | 76.4% | 34.0% | 64.7% |'
- en: '| AWQ | 5.20 | 77.2% | 46.4% | 75.6% | 79.0% | 77.8% | 32.8% | 64.8% |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 5.20 | 77.2% | 46.4% | 75.6% | 79.0% | 77.8% | 32.8% | 64.8% |'
- en: '| LLM.int4() | 5.31 | 77.2% | 46.0% | 75.4% | 78.9% | 77.1% | 32.8% | 64.6%
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int4() | 5.31 | 77.2% | 46.0% | 75.4% | 78.9% | 77.1% | 32.8% | 64.6%
    |'
- en: '| OmniQuant (W6A6) | 5.28 | 72.5% | 42.9% | 0.0% | 78.2% | 66.4% | 29.0% |
    48.2% |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant (W6A6) | 5.28 | 72.5% | 42.9% | 0.0% | 78.2% | 66.4% | 29.0% |
    48.2% |'
- en: '| LQER-INT (W4A8) | 5.31 | 76.9% | 45.9% | 74.0% | 78.7% | 77.2% | 33.6% |
    64.4% |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| LQER-INT (W4A8) | 5.31 | 76.9% | 45.9% | 74.0% | 78.7% | 77.2% | 33.6% |
    64.4% |'
- en: '| LQER-MXINT (W4A6) | 5.24 | 77.1% | 46.2% | 75.6% | 79.2% | 77.6% | 33.6%
    | 64.9% |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A6) | 5.24 | 77.1% | 46.2% | 75.6% | 79.2% | 77.6% | 33.6%
    | 64.9% |'
- en: '| LQER-MXINT (W4A8) | 5.21 | 77.0% | 46.3% | 75.6% | 79.6% | 77.3% | 33.2%
    | 64.8% |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A8) | 5.21 | 77.0% | 46.3% | 75.6% | 79.6% | 77.3% | 33.2%
    | 64.8% |'
- en: 'TABLE X: LLaMA-13B'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '表 X: LLaMA-13B'
- en: '| Method | WikiText2 | ARC (easy) | ARC (challenge) | LAMBADA | PIQA | BOOLQ
    | OpenbookQA | Avg. Accuracy |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WikiText2 | ARC (简单) | ARC (挑战) | LAMBADA | PIQA | BOOLQ | OpenbookQA
    | 平均准确率 |'
- en: '| FP16 | 5.67 | 75.4% | 41.9% | 73.5% | 78.7% | 75.1% | 34.4% | 63.2% |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 5.67 | 75.4% | 41.9% | 73.5% | 78.7% | 75.1% | 34.4% | 63.2% |'
- en: '| GPTQ | 9.63 | 73.6% | 40.4% | 70.0% | 77.7% | 73.0% | 30.0% | 60.8% |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 9.63 | 73.6% | 40.4% | 70.0% | 77.7% | 73.0% | 30.0% | 60.8% |'
- en: '| AWQ | 9.59 | 75.5% | 41.1% | 72.5% | 78.6% | 74.9% | 32.2% | 62.5% |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 9.59 | 75.5% | 41.1% | 72.5% | 78.6% | 74.9% | 32.2% | 62.5% |'
- en: '| LLM.int4() | 10.01 | 74.6% | 42.1% | 70.3% | 78.6% | 74.8% | 32.8% | 62.2%
    |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int4() | 10.01 | 74.6% | 42.1% | 70.3% | 78.6% | 74.8% | 32.8% | 62.2%
    |'
- en: '| OmniQuant (W6A6) | 9.62 | 66.4% | 38.8% | 0.0% | 76.7% | 72.8% | 27.2% |
    47.0% |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant (W6A6) | 9.62 | 66.4% | 38.8% | 0.0% | 76.7% | 72.8% | 27.2% |
    47.0% |'
- en: '| LQER-INT (W4A8) | 6.09 | 73.9% | 40.6% | 73.4% | 77.7% | 74.0% | 30.6% |
    61.7% |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| LQER-INT (W4A8) | 6.09 | 73.9% | 40.6% | 73.4% | 77.7% | 74.0% | 30.6% |
    61.7% |'
- en: '| LQER-MXINT (W4A6) | 5.92 | 74.8% | 41.5% | 73.4% | 78.2% | 75.2% | 33.0%
    | 62.7% |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A6) | 5.92 | 74.8% | 41.5% | 73.4% | 78.2% | 75.2% | 33.0%
    | 62.7% |'
- en: '| LQER-MXINT (W4A8) | 5.89 | 74.9% | 41.6% | 73.3% | 78.6% | 76.1% | 33.6%
    | 63.0% |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A8) | 5.89 | 74.9% | 41.6% | 73.3% | 78.6% | 76.1% | 33.6%
    | 63.0% |'
- en: 'TABLE XI: LLaMA-30B'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XI: LLaMA-30B'
- en: '| Method | WikiText2 | ARC (easy) | ARC (challenge) | LAMBADA | PIQA | BOOLQ
    | OpenbookQA | Avg. Accuracy |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WikiText2 | ARC (简单) | ARC (挑战) | LAMBADA | PIQA | BOOLQ | OpenbookQA
    | 平均准确率 |'
- en: '| FP16 | 4.10 | 80.4% | 52.8% | 77.6% | 81.1% | 82.7% | 36.0% | 68.4% |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 4.10 | 80.4% | 52.8% | 77.6% | 81.1% | 82.7% | 36.0% | 68.4% |'
- en: '| GPTQ | 4.24 | 80.7% | 50.2% | 77.6% | 80.5% | 83.1% | 35.8% | 68.0% |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4.24 | 80.7% | 50.2% | 77.6% | 80.5% | 83.1% | 35.8% | 68.0% |'
- en: '| AWQ | 4.22 | 74.1% | 46.0% | 0.0% | 79.5% | 68.3% | 31.4% | 49.9% |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4.22 | 74.1% | 46.0% | 0.0% | 79.5% | 68.3% | 31.4% | 49.9% |'
- en: '| LLM.int4() | 4.33 | 79.0% | 48.9% | 75.8% | 80.2% | 82.4% | 33.6% | 66.7%
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int4() | 4.33 | 79.0% | 48.9% | 75.8% | 80.2% | 82.4% | 33.6% | 66.7%
    |'
- en: '| OmniQuant (W6A6) | 4.38 | 74.1% | 46.0% | 0.0% | 79.5% | 68.3% | 31.4% |
    49.9% |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant (W6A6) | 4.38 | 74.1% | 46.0% | 0.0% | 79.5% | 68.3% | 31.4% |
    49.9% |'
- en: '| LQER-INT (W4A8) | 4.35 | 80.1% | 49.7% | 77.0% | 80.7% | 81.5% | 35.2% |
    67.4% |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| LQER-INT (W4A8) | 4.35 | 80.1% | 49.7% | 77.0% | 80.7% | 81.5% | 35.2% |
    67.4% |'
- en: '| LQER-MXINT (W4A6) | 4.28 | 80.1% | 50.9% | 77.4% | 80.6% | 82.4% | 35.4%
    | 67.8% |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A6) | 4.28 | 80.1% | 50.9% | 77.4% | 80.6% | 82.4% | 35.4%
    | 67.8% |'
- en: '| LQER-MXINT (W4A8) | 4.25 | 80.0% | 50.8% | 77.6% | 80.7% | 82.5% | 36.2%
    | 68.0% |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A8) | 4.25 | 80.0% | 50.8% | 77.6% | 80.7% | 82.5% | 36.2%
    | 68.0% |'
- en: 'TABLE XII: LLaMA-2-7B'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XII: LLaMA-2-7B'
- en: '| Method | WikiText2 | ARC (easy) | ARC (challenge) | LAMBADA | PIQA | BOOLQ
    | OpenbookQA | Avg. Accuracy |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WikiText2 | ARC (简单) | ARC (挑战) | LAMBADA | PIQA | BOOLQ | OpenbookQA
    | 平均准确率 |'
- en: '| FP16 | 5.48 | 76.3% | 43.6% | 73.9% | 78.1% | 77.7% | 31.4% | 63.5% |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 5.48 | 76.3% | 43.6% | 73.9% | 78.1% | 77.7% | 31.4% | 63.5% |'
- en: '| GPTQ | 5.69 | 75.0% | 42.2% | 72.3% | 77.4% | 76.4% | 30.0% | 62.2% |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 5.69 | 75.0% | 42.2% | 72.3% | 77.4% | 76.4% | 30.0% | 62.2% |'
- en: '| AWQ | 5.61 | 75.2% | 43.3% | 72.7% | 77.6% | 77.3% | 31.4% | 62.9% |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 5.61 | 75.2% | 43.3% | 72.7% | 77.6% | 77.3% | 31.4% | 62.9% |'
- en: '| LLM.int4() | 5.77 | 75.1% | 42.7% | 71.9% | 77.6% | 76.2% | 32.2% | 62.6%
    |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int4() | 5.77 | 75.1% | 42.7% | 71.9% | 77.6% | 76.2% | 32.2% | 62.6%
    |'
- en: '| OmniQuant (W6A6) | 5.87 | 67.3% | 39.0% | 0.0% | 77.6% | 69.9% | 29.2% |
    47.2% |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant (W6A6) | 5.87 | 67.3% | 39.0% | 0.0% | 77.6% | 69.9% | 29.2% |
    47.2% |'
- en: '| LQER-INT (W4A8) | 5.85 | 74.7% | 42.4% | 71.6% | 76.7% | 76.1% | 32.0% |
    62.2% |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| LQER-INT (W4A8) | 5.85 | 74.7% | 42.4% | 71.6% | 76.7% | 76.1% | 32.0% |
    62.2% |'
- en: '| LQER-MXINT (W4A6) | 5.73 | 75.1% | 43.1% | 73.6% | 77.6% | 76.2% | 32.6%
    | 63.0% |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A6) | 5.73 | 75.1% | 43.1% | 73.6% | 77.6% | 76.2% | 32.6%
    | 63.0% |'
- en: '| LQER-MXINT (W4A8) | 5.69 | 75.3% | 42.5% | 73.7% | 77.9% | 76.3% | 32.8%
    | 63.1% |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A8) | 5.69 | 75.3% | 42.5% | 73.7% | 77.9% | 76.3% | 32.8%
    | 63.1% |'
- en: 'TABLE XIII: LLaMA-2-13B'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XIII: LLaMA-2-13B'
- en: '| Method | WikiText2 | ARC (easy) | ARC (challenge) | LAMBADA | PIQA | BOOLQ
    | OpenbookQA | Avg. Accuracy |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WikiText2 | ARC (简单) | ARC (挑战) | LAMBADA | PIQA | BOOLQ | OpenbookQA
    | 平均准确率 |'
- en: '| FP16 | 4.90 | 79.4% | 48.3% | 76.7% | 79.1% | 80.6% | 35.0% | 66.5% |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 4.90 | 79.4% | 48.3% | 76.7% | 79.1% | 80.6% | 35.0% | 66.5% |'
- en: '| GPTQ | 5.06 | 78.6% | 47.4% | 76.4% | 78.2% | 80.8% | 34.2% | 65.9% |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 5.06 | 78.6% | 47.4% | 76.4% | 78.2% | 80.8% | 34.2% | 65.9% |'
- en: '| AWQ | 4.98 | 78.9% | 46.9% | 76.2% | 78.8% | 80.1% | 34.4% | 65.9% |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4.98 | 78.9% | 46.9% | 76.2% | 78.8% | 80.1% | 34.4% | 65.9% |'
- en: '| LLM.int4() | 4.98 | 77.6% | 47.0% | 76.1% | 78.9% | 80.5% | 34.8% | 65.8%
    |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int4() | 4.98 | 77.6% | 47.0% | 76.1% | 78.9% | 80.5% | 34.8% | 65.8%
    |'
- en: '| OmniQuant (W6A6) | 5.14 | 71.3% | 43.8% | 0.0% | 78.6% | 69.8% | 33.0% |
    49.4% |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant (W6A6) | 5.14 | 71.3% | 43.8% | 0.0% | 78.6% | 69.8% | 33.0% |
    49.4% |'
- en: '| LQER-INT (W4A8) | 5.10 | 78.5% | 47.1% | 75.8% | 78.6% | 81.0% | 34.4% |
    65.9% |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| LQER-INT (W4A8) | 5.10 | 78.5% | 47.1% | 75.8% | 78.6% | 81.0% | 34.4% |
    65.9% |'
- en: '| LQER-MXINT (W4A6) | 5.05 | 78.2% | 46.4% | 76.4% | 78.3% | 80.6% | 34.8%
    | 65.8% |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A6) | 5.05 | 78.2% | 46.4% | 76.4% | 78.3% | 80.6% | 34.8%
    | 65.8% |'
- en: '| LQER-MXINT (W4A8) | 5.02 | 78.3% | 47.0% | 76.4% | 78.8% | 81.3% | 34.6%
    | 66.1% |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A8) | 5.02 | 78.3% | 47.0% | 76.4% | 78.8% | 81.3% | 34.6%
    | 66.1% |'
- en: 'TABLE XIV: Vicuna-7B-v1.5'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XIV: Vicuna-7B-v1.5'
- en: '| Method | WikiText2 | ARC (easy) | ARC (challenge) | LAMBADA | PIQA | BOOLQ
    | OpenbookQA | Avg. Accuracy |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WikiText2 | ARC (简单) | ARC (挑战) | LAMBADA | PIQA | BOOLQ | OpenbookQA
    | 平均准确率 |'
- en: '| FP16 | 6.78 | 75.6% | 43.3% | 71.1% | 77.3% | 80.9% | 33.0% | 63.5% |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 6.78 | 75.6% | 43.3% | 71.1% | 77.3% | 80.9% | 33.0% | 63.5% |'
- en: '| GPTQ | 7.07 | 75.4% | 41.5% | 69.4% | 76.0% | 81.3% | 33.2% | 62.8% |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 7.07 | 75.4% | 41.5% | 69.4% | 76.0% | 81.3% | 33.2% | 62.8% |'
- en: '| AWQ | 7.00 | 75.0% | 41.8% | 70.0% | 77.1% | 81.5% | 32.2% | 62.9% |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 7.00 | 75.0% | 41.8% | 70.0% | 77.1% | 81.5% | 32.2% | 62.9% |'
- en: '| LLM.int4() | 7.14 | 75.0% | 42.6% | 69.3% | 76.3% | 81.3% | 34.2% | 63.1%
    |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int4() | 7.14 | 75.0% | 42.6% | 69.3% | 76.3% | 81.3% | 34.2% | 63.1%
    |'
- en: '| LQER-MXINT (W4A8) | 7.01 | 75.4% | 42.2% | 68.9% | 77.1% | 81.6% | 33.0%
    | 63.0% |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A8) | 7.01 | 75.4% | 42.2% | 68.9% | 77.1% | 81.6% | 33.0%
    | 63.0% |'
- en: 'TABLE XV: Vicuna-13B-v1.5'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XV: Vicuna-13B-v1.5'
- en: '| Method | WikiText2 | ARC (easy) | ARC (challenge) | LAMBADA | PIQA | BOOLQ
    | OpenbookQA | Avg. Accuracy |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WikiText2 | ARC (简单) | ARC (挑战) | LAMBADA | PIQA | BOOLQ | OpenbookQA
    | 平均准确率 |'
- en: '| FP16 | 5.92 | 78.7% | 47.8% | 73.4% | 78.9% | 85.2% | 36.8% | 66.8% |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 5.92 | 78.7% | 47.8% | 73.4% | 78.9% | 85.2% | 36.8% | 66.8% |'
- en: '| GPTQ | 6.00 | 77.9% | 46.4% | 72.9% | 78.1% | 85.0% | 36.8% | 66.2% |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 6.00 | 77.9% | 46.4% | 72.9% | 78.1% | 85.0% | 36.8% | 66.2% |'
- en: '| AWQ | 6.03 | 78.3% | 48.4% | 72.9% | 78.3% | 84.8% | 36.8% | 66.6% |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 6.03 | 78.3% | 48.4% | 72.9% | 78.3% | 84.8% | 36.8% | 66.6% |'
- en: '| LLM.int4() | 6.09 | 77.5% | 47.3% | 73.0% | 78.3% | 85.2% | 36.8% | 66.4%
    |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int4() | 6.09 | 77.5% | 47.3% | 73.0% | 78.3% | 85.2% | 36.8% | 66.4%
    |'
- en: '| LQER-MXINT (W4A8) | 6.04 | 78.5% | 46.7% | 72.7% | 77.7% | 85.0% | 36.4%
    | 66.2% |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A8) | 6.04 | 78.5% | 46.7% | 72.7% | 77.7% | 85.0% | 36.4%
    | 66.2% |'
- en: 'TABLE XVI: Mistral-7B'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XVI: Mistral-7B'
- en: '| Method | WikiText2 | ARC (easy) | ARC (challenge) | LAMBADA | PIQA | BOOLQ
    | OpenbookQA | Avg. Accuracy |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WikiText2 | ARC (简单) | ARC (挑战) | LAMBADA | PIQA | BOOLQ | OpenbookQA
    | 平均准确率 |'
- en: '| FP16 | 6.47 | 82.7% | 53.5% | 70.7% | 80.4% | 86.2% | 32.8% | 67.7% |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 6.47 | 82.7% | 53.5% | 70.7% | 80.4% | 86.2% | 32.8% | 67.7% |'
- en: '| GPTQ | 8.13 | 81.1% | 55.8% | 72.2% | 80.9% | 86.7% | 36.0% | 68.8% |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 8.13 | 81.1% | 55.8% | 72.2% | 80.9% | 86.7% | 36.0% | 68.8% |'
- en: '| AWQ | 6.64 | 81.9% | 53.8% | 71.8% | 80.7% | 86.2% | 37.4% | 68.6% |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 6.64 | 81.9% | 53.8% | 71.8% | 80.7% | 86.2% | 37.4% | 68.6% |'
- en: '| LLM.int4() | 6.66 | 81.2% | 53.2% | 70.6% | 81.2% | 86.4% | 34.6% | 67.9%
    |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int4() | 6.66 | 81.2% | 53.2% | 70.6% | 81.2% | 86.4% | 34.6% | 67.9%
    |'
- en: '| LQER-MXINT (W4A8) | 6.71 | 81.7% | 53.8% | 71.2% | 81.0% | 86.5% | 34.8%
    | 68.2% |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| LQER-MXINT (W4A8) | 6.71 | 81.7% | 53.8% | 71.2% | 81.0% | 86.5% | 34.8%
    | 68.2% |'
- en: 'TABLE XVII: More 2-bit $w$ for 2-bit quantization.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XVII: 更多 2 位 $w$ 用于 2 位量化。'
- en: '| Method | OPT | LLaMA |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | OPT | LLaMA |'
- en: '| 125M | 1.3B | 2.7B | 7B | 13B |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 125M | 1.3B | 2.7B | 7B | 13B |'
- en: '| FP16 | 27.65 | 14.63 | 12.47 | 5.67 | 5.10 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 27.65 | 14.63 | 12.47 | 5.67 | 5.10 |'
- en: '| OmniQuant | 75.43 | 23.95 | 18.13 | 12.97 | 10.36 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 75.43 | 23.95 | 18.13 | 12.97 | 10.36 |'
- en: '| Quip | 347.40 | 41.64 | 2998.00 | 10.97 | 8.43 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| Quip | 347.40 | 41.64 | 2998.00 | 10.97 | 8.43 |'
- en: '| L²QER | 45.29 | 29.82 | 23.76 | 10.30 | 8.42 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| L²QER | 45.29 | 29.82 | 23.76 | 10.30 | 8.42 |'
