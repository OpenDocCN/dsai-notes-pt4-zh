- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:52:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:52:07'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Entropy Law: The Story Behind Data Compression and LLM Performance'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵法则：数据压缩与LLM性能背后的故事
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.06645](https://ar5iv.labs.arxiv.org/html/2407.06645)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.06645](https://ar5iv.labs.arxiv.org/html/2407.06645)
- en: \pdfcolInitStack
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \pdfcolInitStack
- en: tcb@breakable \useunder\ul
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: tcb@breakable \useunder\ul
- en: Mingjia Yin^†, Chuhan Wu${}^{*^{\dagger}}$,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Mingjia Yin^†, Chuhan Wu${}^{*^{\dagger}}$,
- en: Wei Guo, Yasheng Wang, Yong Liu, Ruiming Tang^∗, Defu Lian, Enhong Chen State
    Key Laboratory of Cognitive Intelligence & University of Science and Technology
    of China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Wei Guo, Yasheng Wang, Yong Liu, Ruiming Tang^∗, Defu Lian, Enhong Chen 国家认知智能重点实验室
    & 中国科学技术大学
- en: Noah’s Ark Lab, Huawei
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Noah’s Ark Lab, 华为
- en: '{mingjia-yin, wanghao3, cheneh}@ustc.edu.cn'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{mingjia-yin, wanghao3, cheneh}@ustc.edu.cn'
- en: '{wuchuhan1, tangruiming}@huawei.com Corresponding authors. $\dagger$ Equal
    contribution.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '{wuchuhan1, tangruiming}@huawei.com 对应作者。$\dagger$ 等贡献。'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Data is the cornerstone of large language models (LLMs), but not all data is
    useful for model learning. Carefully selected data can better elicit the capabilities
    of LLMs with much less computational overhead. Most methods concentrate on evaluating
    the quality of individual samples in data selection, while the combinatorial effects
    among samples are neglected. Even if each sample is of perfect quality, their
    combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity
    or contradiction. In this paper, we aim to uncover the underlying relationships
    between LLM performance and data selection. Inspired by the information compression
    nature of LLMs, we uncover an “entropy law” that connects LLM performance with
    data compression ratio and first-epoch training loss, which reflect the information
    redundancy of a dataset and the mastery of inherent knowledge encoded in this
    dataset, respectively. Through both theoretical deduction and empirical evaluation,
    we find that model performance is negatively correlated to the compression ratio
    of training data, which usually yields a lower training loss. Based on the findings
    of the entropy law, we propose a quite efficient and universal data selection
    method named ZIP for training LLMs, which aim to prioritize data subsets exhibiting
    a low compression ratio. Based on a multi-stage algorithm that selects diverse
    data in a greedy manner, we can obtain a good data subset with satisfactory diversity.
    Extensive experiments have been conducted to validate the entropy law and the
    superiority of ZIP across different LLM backbones and alignment stages. We also
    present an interesting application of entropy law that can detect potential performance
    risks at the beginning of model training.¹¹1Code can be found in [https://github.com/USTC-StarTeam/ZIP](https://github.com/USTC-StarTeam/ZIP).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是大型语言模型（LLMs）的基石，但并非所有数据对模型学习都有用。精心挑选的数据可以以更少的计算开销更好地激发LLMs的能力。大多数方法集中在评估数据选择中单个样本的质量，而忽略了样本之间的组合效应。即使每个样本的质量都很完美，它们的组合由于固有的同质性或矛盾性，也可能在教授LLMs时效果欠佳。在本文中，我们旨在揭示LLM性能与数据选择之间的潜在关系。受到LLMs信息压缩特性的启发，我们揭示了一个“熵法则”，将LLM性能与数据压缩比及首轮训练损失相联系，分别反映了数据集的信息冗余和该数据集中固有知识的掌握。通过理论推导和实证评估，我们发现模型性能与训练数据的压缩比呈负相关，通常会导致较低的训练损失。基于熵法则的发现，我们提出了一种高效且通用的数据选择方法，名为ZIP，用于训练LLMs，旨在优先选择表现出低压缩比的数据子集。通过一种贪婪方式选择多样数据的多阶段算法，我们可以获得具有令人满意多样性的数据子集。广泛的实验验证了熵法则和ZIP在不同LLM骨干网络和对齐阶段的优越性。我们还展示了熵法则的一个有趣应用，它可以在模型训练开始时检测潜在的性能风险。¹¹1代码可以在[https://github.com/USTC-StarTeam/ZIP](https://github.com/USTC-StarTeam/ZIP)找到。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In recent years, Large Language Models (LLMs) have gained significant attention
    from both academia and industry, applied in various domains, such as chatbots (Ouyang
    et al., [2022](#bib.bib24); Achiam et al., [2023](#bib.bib2)), chemistry tools (M. Bran
    et al., [2024](#bib.bib22)), and programming assistants (GitHub, [2020](#bib.bib13)).
    The great success of LLMs depends on their general intelligence obtained from
    a vast amount of data collected from various sources (Albalak et al., [2024](#bib.bib3);
    Wang et al., [2023c](#bib.bib37)). Through pretraining on trillions of tokens
    to master diverse knowledge and tuning on smaller instruction data to align models
    with human preference, LLMs can effectively utilize their knowledge to follow
    user instructions, do commonsense reasoning, and solve real-world problems (Zhao
    et al., [2023](#bib.bib41)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）在学术界和工业界都引起了广泛关注，并应用于各种领域，如聊天机器人（Ouyang 等，[2022](#bib.bib24)；Achiam
    等，[2023](#bib.bib2)）、化学工具（M. Bran 等，[2024](#bib.bib22)）和编程助手（GitHub，[2020](#bib.bib13)）。LLMs
    的巨大成功依赖于其从各种来源收集的大量数据所获得的广泛智能（Albalak 等，[2024](#bib.bib3)；Wang 等，[2023c](#bib.bib37)）。通过对数万亿个标记进行预训练以掌握多样化的知识，并通过在较小的指令数据上进行调优以使模型与人类偏好对齐，LLMs
    能有效地利用其知识遵循用户指令、进行常识推理并解决实际问题（Zhao 等，[2023](#bib.bib41)）。
- en: However, not all data are useful for teaching LLMs, especially when computational
    resources are limited (Albalak et al., [2024](#bib.bib3)). For example, we can
    better elicit the capability of LLMs by fine-tuning them on carefully curated
    samples rather than a large but noisy data collection (Ouyang et al., [2022](#bib.bib24);
    Chowdhery et al., [2023](#bib.bib7); Meta, [2020](#bib.bib23); Zhou et al., [2023](#bib.bib43)).
    However, selecting the proper data for LLM training is quite complicated and abstruse,
    since the space of data preprocessing and combination is almost unlimited. Due
    to the huge computational overhead of LLM training, manual or empirical data selection
    based on trial-and-error feedback is rather cumbersome and even impractical. Therefore,
    automatic data selection methods are necessary for LLM development under limited
    computational budgets.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并不是所有数据对教学 LLMs 都有用，尤其是在计算资源有限的情况下（Albalak 等，[2024](#bib.bib3)）。例如，通过在精心挑选的样本上进行微调，而不是在大量但嘈杂的数据集合上，我们可以更好地挖掘
    LLMs 的能力（Ouyang 等，[2022](#bib.bib24)；Chowdhery 等，[2023](#bib.bib7)；Meta，[2020](#bib.bib23)；Zhou
    等，[2023](#bib.bib43)）。然而，选择适合 LLM 训练的数据相当复杂且晦涩，因为数据预处理和组合的空间几乎是无限的。由于 LLM 训练的巨大计算开销，基于试错反馈的手动或经验数据选择相当繁琐，甚至不切实际。因此，在有限计算预算下，自动数据选择方法对
    LLM 发展是必要的。
- en: 'Intuitively, high-quality samples are expected to have better efficiency in
    teaching LLMs. For example, the successful practice of LIMA (Zhou et al., [2023](#bib.bib43))
    shows the powerful effect of data quality on LLM performance that can surpass
    the amount of data. Therefore, existing methods usually focus on quality-oriented
    data selection, based either on heuristic rules (Raffel et al., [2020](#bib.bib28);
    Rae et al., [2021](#bib.bib27); Xie et al., [2023](#bib.bib39); Chowdhery et al.,
    [2023](#bib.bib7); Li et al., [2023](#bib.bib19)) or evaluation models (Wettig
    et al., [2024](#bib.bib38); Chen et al., [2023](#bib.bib5); Lu et al., [2023](#bib.bib21);
    Liu et al., [2023](#bib.bib20); Cui et al., [2023](#bib.bib8)). Heuristic methods
    typically involve hand-crafted rules (e.g., sentence number (Raffel et al., [2020](#bib.bib28)),
    word count (Rae et al., [2021](#bib.bib27)), length (Shen, [2024](#bib.bib32)))
    to evaluate data across multiple dimensions. Model-based approaches, on the contrary,
    rely on well-established LLMs such as GPT-4 (Achiam et al., [2023](#bib.bib2))
    to provide quality assessments of training samples in different views, such as
    direct scoring (Chen et al., [2023](#bib.bib5)), task tagging (Lu et al., [2023](#bib.bib21)),
    and pairwise scoring (Liu et al., [2023](#bib.bib20)). However, most of these
    approaches evaluate different data samples independently, which neglects the intricate
    combinatorial effects among samples. As illustrated in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Entropy Law: The Story Behind Data Compression and LLM Performance"),
    even if each sample is in perfect quality, their combinations may still be suboptimal
    due to their mutual information redundancy or inconsistency. Although the quality-based
    subset is composed of all three good samples, the knowledge they encode is actually
    redundant and conflicting. In contrast, another data subset composed of several
    relatively lower-quality but diverse samples may convey more information than
    the above subset in the teaching of LLMs. Therefore, quality-based data selection
    does not fully align with the goal of maximizing the knowledge mastery of LLMs.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，预期高质量样本在教学大型语言模型（LLMs）时具有更好的效率。例如，LIMA（Zhou 等，[2023](#bib.bib43)）的成功实践展示了数据质量对LLM性能的强大影响，这种影响可以超越数据量。因此，现有方法通常关注于基于质量的数据选择，这些方法基于启发式规则（Raffel
    等，[2020](#bib.bib28)；Rae 等，[2021](#bib.bib27)；Xie 等，[2023](#bib.bib39)；Chowdhery
    等，[2023](#bib.bib7)；Li 等，[2023](#bib.bib19)）或评估模型（Wettig 等，[2024](#bib.bib38)；Chen
    等，[2023](#bib.bib5)；Lu 等，[2023](#bib.bib21)；Liu 等，[2023](#bib.bib20)；Cui 等，[2023](#bib.bib8)）。启发式方法通常涉及手工制作的规则（例如，句子数量（Raffel
    等，[2020](#bib.bib28)），单词计数（Rae 等，[2021](#bib.bib27)），长度（Shen，[2024](#bib.bib32)））来评估跨多个维度的数据。相反，基于模型的方法依赖于成熟的LLMs，如GPT-4（Achiam
    等，[2023](#bib.bib2)），提供不同视角的训练样本质量评估，例如直接评分（Chen 等，[2023](#bib.bib5)），任务标记（Lu
    等，[2023](#bib.bib21)），以及成对评分（Liu 等，[2023](#bib.bib20)）。然而，大多数这些方法独立评估不同的数据样本，忽视了样本之间复杂的组合效应。如图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 熵法则：数据压缩与LLM性能背后的故事")所示，即使每个样本的质量完美，它们的组合仍可能由于相互信息的冗余或不一致而次优。尽管基于质量的子集由所有三个优质样本组成，但它们编码的知识实际上是冗余和冲突的。相反，另一个由几个相对较低质量但多样化的样本组成的数据子集可能在教学LLMs时传达更多信息。因此，基于质量的数据选择并不完全符合最大化LLMs知识掌握的目标。
- en: '![Refer to caption](img/c43174f95e2123dfe3b95eee8805acfa.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c43174f95e2123dfe3b95eee8805acfa.png)'
- en: 'Figure 1: An illustrative example describing different data selection paradigms.
    Quality-based data selection relies on sample-level data quality measurements
    while overlooking combinatorial effects among samples. Information-amount-based
    selection aims to select samples maximizing the overall information amount.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：一个描述不同数据选择范式的示例。基于质量的数据选择依赖于样本级的数据质量测量，同时忽略了样本之间的组合效应。基于信息量的数据选择旨在选择能够最大化整体信息量的样本。
- en: In many recent studies, researchers have shown that the basic mechanism of auto-regressive
    language modeling in LLMs is information compression (Delétang et al., [2023](#bib.bib9);
    Huang et al., [2024](#bib.bib14)). Thus, the knowledge condensed by LLMs actually
    depends on the effective information encoded by training data. This intuition
    opens another direction of data selection, i.e., based on the effective information
    amount of data. In this paper, we uncover the underlying relations between LLM
    performance and data homogeneity, which can be measured by various canonical lossless
    compression algorithms (e.g., DEFLATE in ZIP). Through both theoretical analysis
    and empirical experiments, we formulate the “entropy law”, which shows that the
    compression ratio of training data is a decisive factor affecting model performance,
    if the overall quality and consistency of selected samples remain unchanged. Motivated
    by the entropy law, we propose an effective and efficient data selection algorithm
    called ZIP to select heterogeneous data with low compression ratio, which aims
    to maximize the effective information amount of information for LLM learning.
    Specifically, we devise a multi-stage greedy strategy to find an approximate solution
    that guarantees a low compression ratio without exhausting all possible combinations,
    and it iterates continuously until we obtain a predetermined number of samples.
    In each iteration, ZIP performs preliminary filtering to choose a smaller pool
    of candidates, and then selects a few samples from the reduced pool that minimizes
    the compression ratio of the selected dataset through a cascaded manner. By learning
    LLMs on a collection of diverse samples that encode heterogeneous and complementary
    information, the capabilities of LLMs can be better elicited. Extensive experiments
    on different LLM backbones at different stages of LLM alignment demonstrate the
    superiority of ZIP over various quality-based baselines. We also present an interesting
    application of the entropy law that can detect potential performance risks at
    the beginning of model training, which can effectively reduce the computational
    overhead in LLM development.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多近期的研究中，研究人员展示了大语言模型（LLMs）中自回归语言建模的基本机制是信息压缩（Delétang et al., [2023](#bib.bib9);
    Huang et al., [2024](#bib.bib14)）。因此，LLMs 压缩的知识实际上依赖于训练数据中编码的有效信息。这一直觉开辟了另一种数据选择的方向，即基于数据的有效信息量。在本文中，我们揭示了
    LLM 性能与数据同质性之间的潜在关系，这可以通过各种经典的无损压缩算法（例如，ZIP 中的 DEFLATE）来衡量。通过理论分析和实证实验，我们提出了“熵法则”，该法则表明，训练数据的压缩比是影响模型性能的决定性因素，前提是所选样本的整体质量和一致性保持不变。受熵法则的启发，我们提出了一种有效且高效的数据选择算法，称为
    ZIP，用于选择具有低压缩比的异质数据，旨在最大化 LLM 学习的有效信息量。具体来说，我们设计了一种多阶段贪婪策略，来找到一个近似解决方案，该方案保证了低压缩比而无需穷举所有可能的组合，并且持续迭代，直到获得预定数量的样本。在每次迭代中，ZIP
    执行初步筛选，以选择较小的候选池，然后从缩小的池中选择少量样本，通过级联方式最小化所选数据集的压缩比。通过在编码异质和互补信息的多样化样本集合上学习 LLM，可以更好地发掘
    LLM 的能力。在不同阶段的 LLM 对齐的不同 LLM 骨干上的大量实验展示了 ZIP 相较于各种基于质量的基线的优越性。我们还展示了熵法则的一个有趣应用，它可以在模型训练开始时检测潜在的性能风险，这可以有效减少
    LLM 开发中的计算开销。
- en: 2 Related Works
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Large Modeling and Information Compression
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 大规模建模与信息压缩
- en: 'The relationship between language modeling and data compression has long intrigued
    researchers (Shannon, [1948](#bib.bib30), [1951](#bib.bib31)). Pandey ([2024](#bib.bib25))
    has identified a data-dependant scaling law that takes data’s gzip compressibility
    into consideration. Besides, recent empirical studies have confirmed that language
    models can act as versatile data compressors (Delétang et al., [2023](#bib.bib9)),
    and the intelligence of LLMs can be quantified by their capacity for text compression (Huang
    et al., [2024](#bib.bib14)). Let a text corpus be generated from an underlying
    distribution $\rho$ can be updated:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模与数据压缩之间的关系长期以来一直引起研究人员的兴趣（Shannon, [1948](#bib.bib30), [1951](#bib.bib31)）。Pandey
    ([2024](#bib.bib25)) 识别了一种数据依赖的缩放法则，该法则考虑了数据的 gzip 压缩性。此外，近期的实证研究确认了语言模型可以作为多功能的数据压缩器（Delétang
    et al., [2023](#bib.bib9)），并且 LLM 的智能可以通过其文本压缩能力来量化（Huang et al., [2024](#bib.bib14)）。设一个文本语料库从一个潜在分布
    $\rho$ 生成，可以更新：
- en: '|  | $\mathbb{E}_{x\sim\rho}[-\sum_{i=1}^{n}\log_{2}\rho_{\text{model}}(x_{i}&#124;x_{1:i-1})].$
    |  | (1) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{x\sim\rho}[-\sum_{i=1}^{n}\log_{2}\rho_{\text{model}}(x_{i}&#124;x_{1:i-1})].$
    |  | (1) |'
- en: 'Equation [1](#S2.E1 "In 2.1 Large Modeling and Information Compression ‣ 2
    Related Works ‣ Entropy Law: The Story Behind Data Compression and LLM Performance")
    is the cross-entropy loss employed in training LLMs, thereby establishing a coherent
    relationship between LLMs and information compression. This foundational insight
    paves the way for this work.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 方程[1](#S2.E1 "在2.1大规模建模与信息压缩 ‣ 2 相关工作 ‣ 熵定律：数据压缩和LLM性能背后的故事")是用于训练LLM的交叉熵损失，从而在LLM与信息压缩之间建立了连贯的关系。这一基础性的见解为本研究铺平了道路。
- en: 2.2 Alignment of Large Language Models
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 大型语言模型的对齐
- en: Large Language Models (LLMs) have recently gained significant attention from
    academia and industry. LLM alignment, which includes supervised fine-tuning (SFT)
    and reinforcement learning with human feedback (RLHF), has emerged as a crucial
    technique for adapting LLMs to end tasks using natural language instructions (Zhao
    et al., [2023](#bib.bib41); Wang et al., [2023c](#bib.bib37)). Alignment is performed
    using instruction datasets consisting of multiple (Instruction, Output) pairs,
    which require LLMs to follow the instructions and generate corresponding outputs.
    Early explorations have focused on constructing or expanding instruction datasets
    through methods such as crowd-sourcing (Wang et al., [2022](#bib.bib36); Köpf
    et al., [2024](#bib.bib17)), self-instruction (Taori et al., [2023](#bib.bib33);
    Peng et al., [2023](#bib.bib26); Wang et al., [2023b](#bib.bib35)), or the combination
    of existing datasets (Wang et al., [2023a](#bib.bib34); Ivison et al., [2023](#bib.bib15)).
    Fine-tuned LLMs on these datasets have demonstrated promising capabilities to
    adhere to instructions across various contexts and align with human expectations.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）最近引起了学术界和工业界的广泛关注。LLM对齐，包括监督微调（SFT）和结合人类反馈的强化学习（RLHF），已成为将LLM适应于自然语言指令下的最终任务的重要技术（赵等，[2023](#bib.bib41)；王等，[2023c](#bib.bib37)）。对齐是通过包含多个（指令，输出）对的指令数据集进行的，这要求LLM遵循指令并生成相应的输出。早期探索集中在通过众包（王等，[2022](#bib.bib36)；Köpf等，[2024](#bib.bib17)）、自我指令（Taori等，[2023](#bib.bib33)；Peng等，[2023](#bib.bib26)；王等，[2023b](#bib.bib35)）或现有数据集的组合（王等，[2023a](#bib.bib34)；Ivison等，[2023](#bib.bib15)）来构建或扩展指令数据集。基于这些数据集微调的LLM展示了在各种背景下遵循指令和与人类期望对齐的良好能力。
- en: 2.3 Data selection for LLM alignment
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 LLM对齐的数据选择
- en: 'A growing body of research has emphasized the importance of selecting appropriate
    data for LLM alignment, which can prevent potential quality issues and optimize
    computational resource allocation. As a prominent example, Lima (Zhou et al.,
    [2023](#bib.bib43)) has demonstrated superior performance by carefully crafting
    only 1,000 high-quality samples for SFT, highlighting the crucial importance of
    data quality. The current literature on selecting alignment data has focused on
    selecting samples according to individual sample quality, which can be categorized
    into heuristic methods (Shen, [2024](#bib.bib32)) and model-based methods (Chen
    et al., [2023](#bib.bib5); Lu et al., [2023](#bib.bib21); Liu et al., [2023](#bib.bib20);
    Li et al., [2023](#bib.bib19), [2024](#bib.bib18); Du et al., [2023](#bib.bib11)).
    Heuristic methods typically employ specific criteria, such as response length (Shen,
    [2024](#bib.bib32)), to guide data selection. On the other hand, model-based methods
    adopt various strategies to leverage the capabilities of established language
    models for evaluating sample quality. For example, IFD (Li et al., [2023](#bib.bib19))
    measures the change in response loss when instructions are removed, and selects
    those with the most significant changes. Building upon IFD, SuperFiltering (Li
    et al., [2024](#bib.bib18)) introduces a lightweight proxy model for a more efficient
    calculation of the IFD score. In addition, other model-based methods employ proprietary
    LLMs to assess data quality. In a pioneering work, AlpaGasus (Chen et al., [2023](#bib.bib5))
    uses ChatGPT directly to assign data quality scores to samples, while #InsTag (Lu
    et al., [2023](#bib.bib21)) proposes assigning tags to each sample using ChatGPT
    and evaluates sample quality based on the number of tags. DEITA (Liu et al., [2023](#bib.bib20))
    uses ChatGPT-generated data to train two Llama-based scorers, assigning complexity
    and quality scores to each sample, and ultimately selecting samples with the highest
    hybrid scores. However, existing methods are mainly designed to pick data based
    on sample-wise quality measurements, which are usually weak in reflecting the
    overall dataset quality. In this paper, we focus on the relation between performance
    and dataset quality, which can be efficiently measured by data compression metrics.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的研究强调了选择适当数据进行LLM对齐的重要性，这可以防止潜在的质量问题并优化计算资源分配。作为一个突出的例子，Lima (Zhou et al.,
    [2023](#bib.bib43))通过精心制作仅1,000个高质量样本用于SFT，展示了数据质量的关键重要性。当前关于选择对齐数据的文献主要集中在根据单个样本的质量选择样本，这可以分为启发式方法 (Shen,
    [2024](#bib.bib32))和基于模型的方法 (Chen et al., [2023](#bib.bib5); Lu et al., [2023](#bib.bib21);
    Liu et al., [2023](#bib.bib20); Li et al., [2023](#bib.bib19), [2024](#bib.bib18);
    Du et al., [2023](#bib.bib11))。启发式方法通常采用特定的标准，例如响应长度 (Shen, [2024](#bib.bib32))，来指导数据选择。另一方面，基于模型的方法采用各种策略，利用已建立的语言模型来评估样本质量。例如，IFD (Li
    et al., [2023](#bib.bib19))测量去除指令时响应损失的变化，并选择变化最显著的样本。在IFD的基础上，SuperFiltering (Li
    et al., [2024](#bib.bib18))引入了一个轻量级代理模型，以更高效地计算IFD分数。此外，其他基于模型的方法使用专有LLMs来评估数据质量。在一项开创性工作中，AlpaGasus (Chen
    et al., [2023](#bib.bib5))直接使用ChatGPT为样本分配数据质量分数，而#InsTag (Lu et al., [2023](#bib.bib21))则提议使用ChatGPT为每个样本分配标签，并根据标签数量评估样本质量。DEITA (Liu
    et al., [2023](#bib.bib20))使用ChatGPT生成的数据训练了两个基于Llama的评分器，为每个样本分配复杂性和质量分数，并最终选择具有最高混合分数的样本。然而，现有的方法主要设计为根据样本的质量测量来选择数据，这通常较难反映整体数据集的质量。在本文中，我们关注性能与数据集质量之间的关系，这可以通过数据压缩指标高效测量。
- en: '3 Entropy Law: Connecting Model Performance with Data Compression'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 熵定律：将模型性能与数据压缩联系起来
- en: 'In this section, we provide some theoretical analysis of the relations between
    data compression and LLM performance. Intuitively, the correctness and diversity
    of the training data would affect the performance of the final model. Meanwhile,
    the performance of LLM may be suboptimal if the data have severe intrinsic conflicts
    or the model has poor mastery of the information encoded by the data, which can
    be indicated by the training loss. Based on these assumptions, we denote the performance
    of an LLM as $Z$, which is expected to be influenced by the following factors:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们提供了数据压缩与LLM性能之间关系的一些理论分析。直观上，训练数据的正确性和多样性会影响最终模型的性能。同时，如果数据存在严重的内在冲突或模型对数据编码的信息掌握不佳，LLM的性能可能会表现不佳，这可以通过训练损失来指示。基于这些假设，我们将LLM的性能表示为$Z$，其预期受以下因素影响：
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data compression ratio $R$: This metric can be derived by dividing the pre-compression
    data size by the post-compression size, which can be computed by various off-the-shelf
    compression algorithms. Intuitively, a dataset with a lower compression ratio
    indicates a higher information density.'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据压缩比 $R$：该指标可以通过将压缩前的数据大小除以压缩后的大小来获得，可以通过各种现成的压缩算法进行计算。直观上，压缩比更低的数据集表示信息密度更高。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Training loss $L$ value so that the model does not overfit the data.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练损失 $L$ 的值使得模型不过拟合数据。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data consistency $C$, whose detailed derivation can be found in Appendix [A](#A1
    "Appendix A Derivations of joint mutual information of two QA pairs ‣ Entropy
    Law: The Story Behind Data Compression and LLM Performance"). This implies that
    the total knowledge learned by LLMs is narrowed if the answers to similar questions
    are highly inconsistent.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据一致性 $C$，其详细推导见附录 [A](#A1 "附录 A 两个 QA 对的联合互信息推导 ‣ 熵法则：数据压缩和 LLM 性能背后的故事")。这意味着如果对类似问题的回答高度不一致，LLMs
    学到的总知识将会变窄。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Average data quality $Q$: This reflects the average sample-level quality of
    the data, which can be measured through various objective and subjective aspects.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平均数据质量 $Q$：这反映了数据的平均样本级质量，可以通过各种客观和主观方面来衡量。
- en: 'Given a certain amount of training data, the model performance can be estimated
    by the above factors:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一定量的训练数据，可以通过上述因素来估计模型的性能：
- en: '|  | $Z\propto f(R,L,C,Q),$ |  | (2) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $Z\propto f(R,L,C,Q),$ |  | (2) |'
- en: 'where $f$, which can be formulated as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f$ 可以表示为：
- en: '|  | $L\propto g(R,C).$ |  | (3) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $L\propto g(R,C).$ |  | (3) |'
- en: '$L$, since a dataset with higher homogeneity or better data consistency is
    easier for a model to learn. Thus, we can rewrite the above formula as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $L$，因为具有更高同质性或更好数据一致性的 dataset 更容易被模型学习。因此，我们可以将上述公式重写为：
- en: '|  | $C\propto g^{\prime}(R,L),$ |  | (4) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $C\propto g^{\prime}(R,L),$ |  | (4) |'
- en: 'where $g^{\prime}$ is an inverse function. By combining the three above equations,
    we have:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $g^{\prime}$ 是一个反函数。结合上述三个方程，我们得到：
- en: '|  | $Z\propto f(R,L,g^{\prime}(R,L),Q)\propto h(R,L,Q),$ |  | (5) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $Z\propto f(R,L,g^{\prime}(R,L),Q)\propto h(R,L,Q),$ |  | (5) |'
- en: 'where $h$ as a constant. Therefore, the final performance can be roughly formulated
    as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h$ 是一个常数。因此，最终的性能可以大致表示为：
- en: '|  | $Z\propto h^{\prime}(R,L),$ |  | (6) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $Z\propto h^{\prime}(R,L),$ |  | (6) |'
- en: which means that the model performance is correlated with the data compression
    ratio and training loss. We name this relationship as “Entropy Law”.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着模型性能与数据压缩比和训练损失相关。我们将这种关系称为“熵法则”。
- en: 'We can raise two deductions based on the entropy law:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以基于熵法则提出两个推论：
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'If we further regard the data consistency as a constant, the training loss
    is directly influenced by the compression ratio (Eq. [3](#S3.E3 "In 3 Entropy
    Law: Connecting Model Performance with Data Compression ‣ Entropy Law: The Story
    Behind Data Compression and LLM Performance")). Thus, the model performance is
    controlled by the compression ratio: $Z$ is higher, which will be validated by
    our experiments.'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们进一步将数据一致性视为常数，那么训练损失直接受到压缩比的影响（见 Eq. [3](#S3.E3 "在 3 熵法则：连接模型性能与数据压缩 ‣ 熵法则：数据压缩和
    LLM 性能背后的故事")）。因此，模型性能由压缩比控制：$Z$ 更高，这将通过我们的实验得到验证。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Given the same compression ratio $R$, a higher training loss means a lower data
    consistency. Thus, the effective knowledge learned by the model may be more limited.
    This can be used to predict the performance of LLM on different data with similar
    compression ratios and sample qualities. We will show later the application of
    this deduction in our practice.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在相同的压缩比 $R$ 下，更高的训练损失意味着更低的数据一致性。因此，模型学到的有效知识可能更有限。这可以用来预测 LLM 在不同数据上的性能，这些数据具有相似的压缩比和样本质量。我们将在后续实践中展示这一推论的应用。
- en: 'Notably, entropy law reveals a coherent connection between downstream model
    performance and data compression ratio, setting it apart from the previously proposed
    data-dependent scaling law by Pandey ([2024](#bib.bib25)). Building upon the entropy
    law, we derive a data selection algorithm in Section [4](#S4 "4 ZIP: Lightweight
    Data Selection for LLM Alignment ‣ Entropy Law: The Story Behind Data Compression
    and LLM Performance") and demonstrate its application in practical large-scale
    LLM development in Section [5.3](#S5.SS3 "5.3 Empirical Validation of Entropy
    Law ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data Compression and LLM Performance").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '值得注意的是，熵定律揭示了下游模型性能与数据压缩比之间的连贯关系，这使其不同于Pandey（[2024](#bib.bib25)）提出的基于数据的缩放定律。基于熵定律，我们在第[4](#S4
    "4 ZIP: Lightweight Data Selection for LLM Alignment ‣ Entropy Law: The Story
    Behind Data Compression and LLM Performance")节推导了数据选择算法，并在第[5.3](#S5.SS3 "5.3
    Empirical Validation of Entropy Law ‣ 5 Experiments ‣ Entropy Law: The Story Behind
    Data Compression and LLM Performance")节展示了其在实际大规模LLM开发中的应用。'
- en: '4 ZIP: Lightweight Data Selection for LLM Alignment'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '4 ZIP: 轻量级数据选择用于LLM对齐'
- en: 'Guided by the findings of the entropy law, we propose an effective and efficient
    method named ZIP to select data samples based on data compression ratios, which
    aims to maximize the amount of effective information given a limited training
    data budget. Although there exists a subset with the lowest compression ratio,
    it is impractical to find it due to the huge combination space of data samples.
    Thus, we propose an iterative multi-stage greedy algorithm to efficiently obtain
    an approximate solution with a relatively low compression ratio. In each iteration,
    we first use a global selection stage to choose a pool of candidate samples that
    have low compression ratios, which aims to find samples with high information
    density. We then employ a coarse-grained local selection stage incorporating a
    smaller set of samples with the lowest redundancy with already selected samples.
    Finally, we use a fine-grained local selection stage that minimizes the similarity
    between samples to add. The above process is conducted until we obtain a sufficient
    size of data. The workflow of our method is summarized in Algorithm [1](#alg1
    "Algorithm 1 ‣ 4.2 Local Coarse-grained Selection ‣ 4 ZIP: Lightweight Data Selection
    for LLM Alignment ‣ Entropy Law: The Story Behind Data Compression and LLM Performance"),
    whose details are introduced as follows.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '根据熵定律的发现，我们提出了一种有效且高效的方法，称为ZIP，以基于数据压缩比选择数据样本，旨在在有限的训练数据预算下最大化有效信息量。尽管存在压缩比最低的子集，但由于数据样本的组合空间巨大，找到它是不切实际的。因此，我们提出了一种迭代的多阶段贪婪算法，以高效地获得具有相对较低压缩比的近似解。在每次迭代中，我们首先使用全球选择阶段来选择具有低压缩比的候选样本池，旨在找到具有高信息密度的样本。然后，我们采用一个粗粒度的本地选择阶段，将一小部分具有最低冗余的样本与已选样本结合。最后，我们使用一个细粒度的本地选择阶段，最小化样本之间的相似性来进行添加。上述过程持续进行，直到我们获得足够的数据量。我们方法的工作流程总结在算法[1](#alg1
    "Algorithm 1 ‣ 4.2 Local Coarse-grained Selection ‣ 4 ZIP: Lightweight Data Selection
    for LLM Alignment ‣ Entropy Law: The Story Behind Data Compression and LLM Performance")中，详细信息如下所述。'
- en: 4.1 Global Selection
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 全球选择
- en: In general, we maintain an information redundancy state $\pi_{\mathcal{D}}$,
    which provides a good set for subsequent local selection.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们维护一个信息冗余状态$\pi_{\mathcal{D}}$，该状态为后续的本地选择提供了良好的样本集合。
- en: 4.2 Local Coarse-grained Selection
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 本地粗粒度选择
- en: Since the global selection does not well consider the mutual relations among
    samples, we further conduct local selection to pick diverse samples. To ensure
    good computational efficiency, we introduce a coarse-grained selection phase to
    narrow the candidate pool into a smaller one with $K_{2}$.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于全球选择未充分考虑样本之间的相互关系，我们进一步进行本地选择以挑选多样化的样本。为了确保良好的计算效率，我们引入了一个粗粒度选择阶段，将候选池缩小到一个较小的集合，大小为$K_{2}$。
- en: Algorithm 1 Pseudo code of ZIP
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 ZIP的伪代码
- en: 1:The original dataset $\mathcal{D}$22:end while
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 原始数据集$\mathcal{D}$22: 结束 while'
- en: 4.3 Local Fine-grained Selection
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 本地细粒度选择
- en: Although the above stage ensures that the candidate pool has distinct information
    from the selected set, the information redundancy among the samples within this
    pool is not measured. Thus, we aim to pick further samples from this subset that
    are diverse from each other. Concretely, we initialize a local selected set $\mathcal{D}_{K_{3}}$.
    In our method, the entire selection process is quite easy to implement since it
    is model-free, and can be accelerated using multiple threads. It can select data
    efficiently and effectively from a large candidate pool for high-quality LLM training.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述阶段确保了候选池与选定集具有不同的信息，但未测量该池内样本之间的信息冗余。因此，我们的目标是从这个子集中选择出更多彼此不同的样本。具体而言，我们初始化一个本地选择集$\mathcal{D}_{K_{3}}$。在我们的方法中，整个选择过程非常容易实施，因为它不依赖模型，并且可以使用多线程加速。它可以有效地从大规模候选池中选择数据，以便进行高质量的LLM训练。
- en: 5 Experiments
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'ZIP is content-agnostic and model-free, making it suitable for various stages
    of LLM alignment. We systematically evaluate the effectiveness of ZIP through
    experiments conducted in the SFT and RLHF stages, as described in Sections [5.1](#S5.SS1
    "5.1 Data Selection for SFT ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data
    Compression and LLM Performance") and [5.2](#S5.SS2 "5.2 Data Selection for RLHF
    ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data Compression and LLM Performance"),
    respectively. Subsequently, Section [5.3](#S5.SS3 "5.3 Empirical Validation of
    Entropy Law ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data Compression and
    LLM Performance") presents an in-depth analysis to empirically support the proposed
    entropy law, including a practical application guided by this law.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'ZIP不依赖内容和模型，使其适用于LLM对齐的各个阶段。我们通过在SFT和RLHF阶段进行的实验系统地评估了ZIP的有效性，如[5.1](#S5.SS1
    "5.1 Data Selection for SFT ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data
    Compression and LLM Performance")和[5.2](#S5.SS2 "5.2 Data Selection for RLHF ‣
    5 Experiments ‣ Entropy Law: The Story Behind Data Compression and LLM Performance")节中所述。随后，[5.3](#S5.SS3
    "5.3 Empirical Validation of Entropy Law ‣ 5 Experiments ‣ Entropy Law: The Story
    Behind Data Compression and LLM Performance")节提供了深入的分析，以经验性支持所提出的熵法则，包括由该法则指导的实际应用。'
- en: 5.1 Data Selection for SFT
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 SFT的数据选择
- en: 5.1.1 Setup
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 设置
- en: Data Pool & Data Selection We follow DEITA (Liu et al., [2023](#bib.bib20))
    to establish a large-scale data pool comprising 300K high-quality samples obtained
    from WizardLM (Xu et al., [2023](#bib.bib40)), ShareGPT (Chiang et al., [2023](#bib.bib6)),
    and UltraChat (Ding et al., [2023](#bib.bib10)). Subsequently, various data selection
    techniques are employed to extract a subset of this pool for LLM instruction tuning.
    Notably, previous studies controlled the data budget by limiting the number of
    instances, whereas we managed the total token count to ensure a fair allocation
    of the compute budget among all methods. To achieve this, we initially select
    10,000 samples using ZIP and calculate the corresponding token count. Then, we
    apply other methods to continue data selection until the required token count
    is reached.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 数据池与数据选择 我们遵循DEITA（Liu等，[2023](#bib.bib20)）建立了一个包含300K高质量样本的大规模数据池，这些样本来自WizardLM（Xu等，[2023](#bib.bib40)）、ShareGPT（Chiang等，[2023](#bib.bib6)）和UltraChat（Ding等，[2023](#bib.bib10)）。随后，采用各种数据选择技术从这个池中提取子集进行LLM指令调优。值得注意的是，之前的研究通过限制实例数量来控制数据预算，而我们通过管理总令牌数量来确保在所有方法中公平分配计算预算。为此，我们最初使用ZIP选择10,000个样本并计算相应的令牌数量。然后，应用其他方法继续数据选择，直到达到所需的令牌数量。
- en: 'Training & Evaluation We fine-tune Mistral-7B (Jiang et al., [2023](#bib.bib16))
    and LLama-3-8B (Meta, [2020](#bib.bib23)) on the selected dataset. Other training
    details can be found in Appendix [B](#A2 "Appendix B Training Details ‣ Entropy
    Law: The Story Behind Data Compression and LLM Performance"). As for evaluation,
    we adopt MT-bench(Zheng et al., [2023](#bib.bib42)) as our benchmark. Specifically,
    MT-bench is a challenging multi-turn question set with LLM judgements to evaluate
    model responses, which exhibits a high-level human preferences alignment.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '训练与评估 我们在选定的数据集上对Mistral-7B（Jiang等，[2023](#bib.bib16)）和LLama-3-8B（Meta，[2020](#bib.bib23)）进行了微调。其他训练细节见附录[B](#A2
    "Appendix B Training Details ‣ Entropy Law: The Story Behind Data Compression
    and LLM Performance")。在评估方面，我们采用MT-bench（Zheng等，[2023](#bib.bib42)）作为我们的基准。具体而言，MT-bench是一个具有挑战性的多轮问题集，通过LLM评判来评估模型响应，展示了高水平的人类偏好对齐。'
- en: 'Baselines We select the baseline from two aspects. The first group includes
    heuristic methods: (1) Random, which randomly selects instances from the data
    pool to verify the fundamental effectiveness of other methods; (2) Cluster, which
    adopts K-means clustering based on the sample representations and select cluster
    centroids; (3) Perplexity, which selects the samples with highest training loss.
    The second group of baselines includes model-based methods: (1) DEITA (Liu et al.,
    [2023](#bib.bib20)), which employs ChatGPT-generated data to train a Llama-based
    data complexity evaluator and a quality evaluator, and selects samples with the
    highest hybrid scores; (2) SuperFiltering (Li et al., [2024](#bib.bib18)), which
    assesses each sample by calculating the change in response loss upon instruction
    removal and introduce a lightweight proxy model to calculate the score more efficiently.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试我们从两个方面选择基准。第一组包括启发式方法：(1) 随机，随机从数据池中选择实例以验证其他方法的基本有效性；(2) 聚类，基于样本表示采用K-means聚类并选择聚类中心；(3)
    困惑度，选择训练损失最高的样本。第二组基准包括基于模型的方法：(1) DEITA （Liu et al., [2023](#bib.bib20)），利用ChatGPT生成的数据训练Llama-based数据复杂度评估器和质量评估器，并选择具有最高混合分数的样本；(2)
    SuperFiltering （Li et al., [2024](#bib.bib18)），通过计算在去除指令后的响应损失变化来评估每个样本，并引入轻量级代理模型以更高效地计算分数。
- en: 5.1.2 Results
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 结果
- en: 'Table 1: Model performance comparison between different data selection baselines
    based on Mistral-7B and Llama-3-8B on the SFT stage. We also provide the computational
    cost, average token length of selected data, and the average data quality produced
    by Alpagasus Chen et al. ([2023](#bib.bib5)). The best results are bolded, and
    the second-best numbers are underlined. $\dagger$ means CPU-only methods.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：基于Mistral-7B和Llama-3-8B在SFT阶段的不同数据选择基准的模型性能比较。我们还提供了计算成本、所选数据的平均令牌长度以及Alpagasus Chen
    et al.（[2023](#bib.bib5)）产生的平均数据质量。最佳结果已加粗，第二好的数字已加下划线。$\dagger$表示仅限CPU的方法。
- en: '| Model | MT-bench $\uparrow$ | Avg.length | Quality |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | MT-bench $\uparrow$ | 平均长度 | 质量 |'
- en: '| Mistral-7B-based models with SFT |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 基于Mistral-7B的SFT模型 |'
- en: '| Random $\dagger$ | 6.85 | 10s | 976 | 4.08 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 随机 $\dagger$ | 6.85 | 10s | 976 | 4.08 |'
- en: '| Cluster | \ul6.91 | 15h | 970 | 4.05 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 | \ul6.91 | 15h | 970 | 4.05 |'
- en: '| Perplexity | 6.89 | 8h | 981 | 4.09 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度 | 6.89 | 8h | 981 | 4.09 |'
- en: '| SuperFiltering | 6.12 | 14h | 1579 | 4.10 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| SuperFiltering | 6.12 | 14h | 1579 | 4.10 |'
- en: '| DEITA | 6.82 | 21h | 2048 | 4.03 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| DEITA | 6.82 | 21h | 2048 | 4.03 |'
- en: '| ZIP $\dagger$ | 7.08 | \ul4.5h | 543 | 4.00 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| ZIP $\dagger$ | 7.08 | \ul4.5h | 543 | 4.00 |'
- en: '| Llama-3-8B-based models with SFT |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 基于Llama-3-8B的SFT模型 |'
- en: '| Random $\dagger$ | 7.16 | 10s | 892 | 4.08 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 随机 $\dagger$ | 7.16 | 10s | 892 | 4.08 |'
- en: '| Cluster | \ul7.18 | 16h | 886 | 3.95 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 | \ul7.18 | 16h | 886 | 3.95 |'
- en: '| Perplexity | 7.09 | 9h | 895 | 3.96 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度 | 7.09 | 9h | 895 | 3.96 |'
- en: '| SuperFiltering | 6.59 | 14h | 1481 | 3.99 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| SuperFiltering | 6.59 | 14h | 1481 | 3.99 |'
- en: '| DEITA | 7.11 | 21h | 2048 | 4.09 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| DEITA | 7.11 | 21h | 2048 | 4.09 |'
- en: '| ZIP $\dagger$ | 7.28 | \ul4.5h | 470 | 4.00 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ZIP $\dagger$ | 7.28 | \ul4.5h | 470 | 4.00 |'
- en: 'Main comparison We compare ZIP with various data selection methods based on
    Mistral-7B and Llama-3-8B, and the results are presented in Table [1](#S5.T1 "Table
    1 ‣ 5.1.2 Results ‣ 5.1 Data Selection for SFT ‣ 5 Experiments ‣ Entropy Law:
    The Story Behind Data Compression and LLM Performance"). ZIP outperforms other
    data selection approaches on all backbones, which can be attributed to ZIP’s ability
    to model the complex combinatorial effects among samples. Furthermore, our observations
    indicate that model-based data selection methods often fail to produce satisfactory
    outcomes when a fixed token number is given. This is because the sample-level
    evaluations are not updated correspondingly after selecting some samples, leading
    to biased evaluations for the remaining samples. Additionally, some of these methods
    adopt strategies to enhance data diversity, such as DEITA, which controls the
    representation distances of selected samples. However, these strategies only provide
    a rough assessment of the combinatorial effects within the representation space,
    since semantic distances do not necessarily reflect information redundancy.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '主要比较 我们基于Mistral-7B和Llama-3-8B比较了ZIP与各种数据选择方法，结果如表格[1](#S5.T1 "Table 1 ‣ 5.1.2
    Results ‣ 5.1 Data Selection for SFT ‣ 5 Experiments ‣ Entropy Law: The Story
    Behind Data Compression and LLM Performance")所示。ZIP在所有骨干网络上均优于其他数据选择方法，这归功于ZIP能够建模样本之间复杂的组合效应。此外，我们的观察表明，当给定固定的标记数时，基于模型的数据选择方法通常无法产生令人满意的结果。这是因为在选择一些样本后，样本级评估没有相应地更新，导致剩余样本的评估存在偏差。此外，一些方法采用了增强数据多样性的策略，例如DEITA，它控制所选样本的表示距离。然而，这些策略只是对表示空间内的组合效应提供了粗略的评估，因为语义距离不一定反映信息冗余。'
- en: 'Selection bias in sample length across different strategies We also provide
    the average length of tokenized samples in Table [1](#S5.T1 "Table 1 ‣ 5.1.2 Results
    ‣ 5.1 Data Selection for SFT ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data
    Compression and LLM Performance"). The average token length of Random provides
    an estimation for the entire data pool, which is used to analyze other methods.
    From the tables, we can observe that Cluster and Perplexity exhibit similar selection
    preferences as Random. Additionally, Deita and SuperFiltering predominantly select
    lengthy data samples. This bias may stem from the LLMs’ inclination toward generating
    longer responses (Saito et al., [2023](#bib.bib29)). However, given the limited
    budget of selected tokens, choosing excessively lengthy data will reduce the information
    density and degrade the capabilities of models trained on such data. In contrast,
    ZIP tends to select shorter samples. Furthermore, we plot the token length distribution
    of these methods, as depicted in Figure [2](#S5.F2 "Figure 2 ‣ 5.1.2 Results ‣
    5.1 Data Selection for SFT ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data
    Compression and LLM Performance") and Figure [6](#A3.F6 "Figure 6 ‣ Appendix C
    Token length distribution of more backbones ‣ Entropy Law: The Story Behind Data
    Compression and LLM Performance"). Consistent with the previous results, we can
    observe similar distributions for Random, Cluster, and Perplexity. The token length
    distributions of DEITA and SuperFiltering are severely skewed, deviating greatly
    from the original data distribution. In contrast to these model-based approaches,
    ZIP exhibits no bias toward selecting lengthy samples.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '不同策略下样本长度的选择偏差 我们在表格[1](#S5.T1 "Table 1 ‣ 5.1.2 Results ‣ 5.1 Data Selection
    for SFT ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data Compression and LLM
    Performance")中还提供了标记样本的平均长度。随机选择的平均标记长度为整个数据池提供了估计值，用于分析其他方法。从表格中可以观察到，Cluster和Perplexity表现出与Random相似的选择偏好。此外，Deita和SuperFiltering主要选择较长的数据样本。这种偏差可能源于LLMs对生成较长响应的倾向（Saito等，[2023](#bib.bib29)）。然而，鉴于所选标记的预算有限，选择过长的数据将降低信息密度，并削弱在这些数据上训练的模型的能力。相比之下，ZIP倾向于选择较短的样本。此外，我们绘制了这些方法的标记长度分布，如图[2](#S5.F2
    "Figure 2 ‣ 5.1.2 Results ‣ 5.1 Data Selection for SFT ‣ 5 Experiments ‣ Entropy
    Law: The Story Behind Data Compression and LLM Performance")和图[6](#A3.F6 "Figure
    6 ‣ Appendix C Token length distribution of more backbones ‣ Entropy Law: The
    Story Behind Data Compression and LLM Performance")所示。与之前的结果一致，我们可以观察到Random、Cluster和Perplexity具有类似的分布。DEITA和SuperFiltering的标记长度分布严重偏斜，与原始数据分布相差甚远。与这些基于模型的方法相比，ZIP在选择较长样本时没有偏差。'
- en: 'Cost comparison of different strategies We provide a detailed cost analysis
    of each method in Table [1](#S5.T1 "Table 1 ‣ 5.1.2 Results ‣ 5.1 Data Selection
    for SFT ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data Compression and LLM
    Performance"). Except for the Random method, ZIP required the least time to complete
    the data selection process, demonstrating greater efficiency than other methods.
    Notably, ZIP’s computations are entirely executed on CPUs, resulting in significant
    cost savings. Furthermore, ZIP is independent of proprietary LLMs used by DEITA
    or the proxy model employed by Cluster, Perplexity, and SuperFiltering. This model-free
    characteristic endows ZIP with notable efficiency and versatility.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '不同策略的成本比较 我们在表格 [1](#S5.T1 "Table 1 ‣ 5.1.2 Results ‣ 5.1 Data Selection for
    SFT ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data Compression and LLM Performance")
    中提供了每种方法的详细成本分析。除了随机方法外，ZIP 完成数据选择过程所需时间最少，显示出比其他方法更高的效率。值得注意的是，ZIP 的计算完全在 CPU
    上执行，从而显著节省了成本。此外，ZIP 不依赖于 DEITA 使用的专有 LLM 或 Cluster、Perplexity 和 SuperFiltering
    使用的代理模型。这种无模型的特性赋予了 ZIP 显著的效率和多样性。'
- en: '![Refer to caption](img/796483d987324f0c42a9c3400f87d938.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/796483d987324f0c42a9c3400f87d938.png)'
- en: (a) ZIP
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ZIP
- en: '![Refer to caption](img/1fd97c5c56965841479d291ad73f49bb.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/1fd97c5c56965841479d291ad73f49bb.png)'
- en: (b) Random
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 随机
- en: (c) Diversity
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 多样性
- en: '![Refer to caption](img/be37a50dcce3ac113a16eae713284326.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/be37a50dcce3ac113a16eae713284326.png)'
- en: (d) Perplexity
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 困惑度
- en: '![Refer to caption](img/953ea0139f2e62d6fd50d3810ffd67b1.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/953ea0139f2e62d6fd50d3810ffd67b1.png)'
- en: (e) DEITA
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: (e) DEITA
- en: '![Refer to caption](img/df9e9a91f72e47011db2ce2090961594.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/df9e9a91f72e47011db2ce2090961594.png)'
- en: (f) SuperFiltering
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: (f) SuperFiltering
- en: 'Figure 2: The distribution of average token number across datasets selected
    by different algorithms for Mistral-7B.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：不同算法选择的 Mistral-7B 数据集中的平均 token 数分布。
- en: 'Selected data quality of different strategies We have followed Alpagasus (Chen
    et al., [2023](#bib.bib5)) to evaluate the quality of each data sample in the
    selected datasets by prompting ChatGPT, with the quality scores ranging from 0
    to 5. The quality scores of multi-turn samples are the average scores of each
    turn. The results have been presented in Table [1](#S5.T1 "Table 1 ‣ 5.1.2 Results
    ‣ 5.1 Data Selection for SFT ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data
    Compression and LLM Performance"). Surprisingly, the quality scores of selected
    datasets are highly similar, even with significant differences in selection mechanisms.
    This may suggest that the average quality distribution remains relatively uniform
    in the original data pool. Notably, even the SOTA model-based methods like DEITA (Liu
    et al., [2023](#bib.bib20)) and SuperFiltering (Li et al., [2024](#bib.bib18))
    select data with similar quality scores, potentially contradicting their original
    conclusions. We posit that this discrepancy stems from the setting of the data
    budget, which is controlled by the number of samples in prior studies. Considering
    the selection bias discussed above, these methods tend to select lengthy samples,
    resulting in a significantly higher token count compared with baselines. For instance,
    under this setting, data selected by DEITA will possess 2.7 times the number of
    tokens compared to ZIP. However, we argue it is fairer to control the data budget
    by the token count since it guarantees a similar compute budget among all methods³³3In
    practical implementation, the training steps of all methods are almost equal by
    employing the packing technique detailed in [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/docs/multipack.qmd)..'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '不同策略的选择数据质量 我们参考了 Alpagasus (Chen et al., [2023](#bib.bib5)) 通过提示 ChatGPT 来评估所选数据集中的每个数据样本的质量，质量评分范围从
    0 到 5。多轮样本的质量评分为每轮评分的平均值。结果已在表格 [1](#S5.T1 "Table 1 ‣ 5.1.2 Results ‣ 5.1 Data
    Selection for SFT ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data Compression
    and LLM Performance") 中展示。令人惊讶的是，尽管选择机制存在显著差异，但所选数据集的质量评分高度相似。这可能表明原始数据池中的平均质量分布相对均匀。值得注意的是，即使是像
    DEITA (Liu et al., [2023](#bib.bib20)) 和 SuperFiltering (Li et al., [2024](#bib.bib18))
    这样的 SOTA 模型方法也选择了质量评分相似的数据，可能与其原始结论相矛盾。我们认为这种差异源于数据预算的设定，该预算由先前研究中的样本数量控制。考虑到上述选择偏差，这些方法往往选择较长的样本，导致与基线相比的
    token 数显著增加。例如，在这种设置下，DEITA 选择的数据的 token 数量将是 ZIP 的 2.7 倍。然而，我们认为控制数据预算按 token
    数量更公平，因为它保证了所有方法之间的计算预算相似³³在实际实施中，所有方法的训练步骤几乎相等，通过采用 [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/docs/multipack.qmd)
    中详细描述的打包技术。.'
- en: 5.2 Data Selection for RLHF
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 RLHF 的数据选择
- en: 'Table 2: Model performance comparison between different data selection baselines
    based on Llama-3-8B on the RLHF stage. We also provide the computational cost
    of each method.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：基于 Llama-3-8B 在 RLHF 阶段的不同数据选择基线模型性能比较。我们还提供了每种方法的计算成本。
- en: '| Model | MT-bench $\uparrow$ | Cost | Avg.length |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | MT-bench $\uparrow$ | 成本 | 平均长度 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Base | 7.18 | NA | NA |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Base | 7.18 | NA | NA |'
- en: '| --- | --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Random $\dagger$ | \ul7.33 | 5s | 464 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Random $\dagger$ | \ul7.33 | 5s | 464 |'
- en: '| Score | 7.30 | NA | 489 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 得分 | 7.30 | NA | 489 |'
- en: '| ZIP $\dagger$ | 7.42 | 1.1h | 357 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| ZIP $\dagger$ | 7.42 | 1.1h | 357 |'
- en: 5.2.1 Setup
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 设置
- en: Data Pool & Data Selection The data pool used for preference alignment is a
    cleaned version of UltraFeedback (Cui et al., [2023](#bib.bib8); Bartolome et al.,
    [2023](#bib.bib4)), which consists of around 60k samples in the form of a "chosen-rejected"
    pair. Similarly to the SFT stage, we ensure each data selection method selects
    data with an approximately equal token count. Since a "chosen-rejected" data pair
    encompasses two data points, we select 5,000 data pairs with ZIP and then apply
    other methods to select data with the corresponding token budget.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 数据池与数据选择 用于偏好对齐的数据池是 UltraFeedback 的清理版本 (Cui et al., [2023](#bib.bib8); Bartolome
    et al., [2023](#bib.bib4))，其中包含约 60k 个“选择-拒绝”对。与 SFT 阶段类似，我们确保每种数据选择方法选择的数据具有大致相同的标记数。由于一个“选择-拒绝”数据对包含两个数据点，我们使用
    ZIP 选择 5,000 个数据对，然后应用其他方法选择具有相应标记预算的数据。
- en: '![Refer to caption](img/e830f1c1ce519eaef84910dcfc458e3d.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e830f1c1ce519eaef84910dcfc458e3d.png)'
- en: (a) Entropy law w.r.t. compression ratio
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 相对于压缩比的熵法则
- en: '![Refer to caption](img/18b29a8b061122110d86d00f444271ee.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/18b29a8b061122110d86d00f444271ee.png)'
- en: (b) Entropy law w.r.t. training loss
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 相对于训练损失的熵法则
- en: 'Figure 3: Entropy law demonstration of Mistral-7B. The Entropy law curve is
    fitted with the results of different methods.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：Mistral-7B 的熵法则演示。熵法则曲线是根据不同方法的结果进行拟合的。
- en: '![Refer to caption](img/3dd009880254f063e7b09dbc259c7380.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/3dd009880254f063e7b09dbc259c7380.png)'
- en: (a) Entropy law w.r.t. compression ratio
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 相对于压缩比的熵法则
- en: '![Refer to caption](img/63113fc33f722cc7ab6b2f056b3be755.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/63113fc33f722cc7ab6b2f056b3be755.png)'
- en: (b) Entropy law w.r.t. training loss
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 相对于训练损失的熵法则
- en: 'Figure 4: Entropy law curve of Llama-3-8B. The Entropy law curve is fitted
    with the results of different methods.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：Llama-3-8B 的熵法则曲线。熵法则曲线是根据不同方法的结果进行拟合的。
- en: 'Training & Evaluation Building upon the model previously fine-tuned with SFT,
    we further refine it using RLHF. In particular, we employ Kahneman-Tversky Optimization
    (KTO) (Ethayarajh et al., [2024](#bib.bib12)) for preference alignment, a novel
    method that shows promising potential in aligning preferences. Additional training
    details can be found in Appendix [B](#A2 "Appendix B Training Details ‣ Entropy
    Law: The Story Behind Data Compression and LLM Performance"). For evaluation,
    we continue to utilize MT-bench (Zheng et al., [2023](#bib.bib42)) as our benchmark
    to assess the capabilities of LLMs fine-tuned with data selected using diverse
    data selection strategies.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 训练与评估 在先前使用 SFT 微调的模型基础上，我们进一步通过 RLHF 进行优化。特别地，我们采用 Kahneman-Tversky 优化 (KTO)
    (Ethayarajh et al., [2024](#bib.bib12)) 进行偏好对齐，这是一种在对齐偏好方面显示出有希望潜力的新方法。更多训练细节可以在附录
    [B](#A2 "附录 B 训练细节 ‣ 熵法则：数据压缩与 LLM 性能背后的故事") 中找到。为了评估，我们继续使用 MT-bench (Zheng et
    al., [2023](#bib.bib42)) 作为基准来评估使用不同数据选择策略选择的数据微调的 LLM 的能力。
- en: 'Baselines We compare ZIP with the following baselines: (1) Random, which randomly
    samples some "chosen-rejected" pairs from the data pool. (2) Score, which selects
    the "chosen-rejected" pairs with the highest "chosen-scores". These scores are
    obtained through LLM evaluation of the response quality (Cui et al., [2023](#bib.bib8);
    Bartolome et al., [2023](#bib.bib4)).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 基线 我们将 ZIP 与以下基线进行比较：(1) Random，随机从数据池中抽取一些“选择-拒绝”对。(2) Score，选择“选择分数”最高的“选择-拒绝”对。这些分数通过
    LLM 对响应质量的评估获得 (Cui et al., [2023](#bib.bib8); Bartolome et al., [2023](#bib.bib4))。
- en: 5.2.2 Main results
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 主要结果
- en: 'Table [2](#S5.T2 "Table 2 ‣ 5.2 Data Selection for RLHF ‣ 5 Experiments ‣ Entropy
    Law: The Story Behind Data Compression and LLM Performance") presents the results
    of different data selection strategies on the preference alignment stage of LLMs.
    Similar to the SFT stage, models aligned with data selected by ZIP can yield the
    best downstream performance, demonstrating the necessity for modeling the combinatorial
    effects. Besides, we find Score and Random are on par with each other, even though
    the selection process of Score is far more expensive than Random. This is unsurprising,
    as Score does not consider the combinatorial effects, which may limit the knowledge
    amount of the selected dataset.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S5.T2 "表2 ‣ 5.2 RLHF的数据选择 ‣ 5 实验 ‣ 熵法则：数据压缩与LLM表现背后的故事") 显示了不同数据选择策略在LLM的偏好对齐阶段的结果。类似于SFT阶段，通过ZIP选择的数据对齐的模型可以获得最佳的下游性能，展示了建模组合效应的必要性。此外，我们发现Score和Random相当，尽管Score的选择过程远比Random昂贵。这并不令人惊讶，因为Score没有考虑组合效应，这可能限制了选定数据集的知识量。
- en: 5.3 Empirical Validation of Entropy Law
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 熵法则的实证验证
- en: '![Refer to caption](img/e5ea7f41f27be970f3ca1a6fbf176821.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e5ea7f41f27be970f3ca1a6fbf176821.png)'
- en: 'Figure 5: Practical application of Entropy law in incremental training data
    update, where $x_{1},x_{2},x_{3},x_{4},x_{5}$ are five data versions.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：熵法则在增量训练数据更新中的实际应用，其中$x_{1},x_{2},x_{3},x_{4},x_{5}$是五个数据版本。
- en: 'In this section, we aim to demonstrate the proposed entropy law. Specifically,
    we have plotted the model performance of Mistral-7B and Llama-3-8B concerning
    data compression ratio and training loss in Figure [3](#S5.F3 "Figure 3 ‣ 5.2.1
    Setup ‣ 5.2 Data Selection for RLHF ‣ 5 Experiments ‣ Entropy Law: The Story Behind
    Data Compression and LLM Performance") and [4](#S5.F4 "Figure 4 ‣ 5.2.1 Setup
    ‣ 5.2 Data Selection for RLHF ‣ 5 Experiments ‣ Entropy Law: The Story Behind
    Data Compression and LLM Performance"), respectively. Besides, we plot entropy-law
    curves by fitting the results. From the two figures, we can draw the following
    analysis:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们旨在展示所提出的熵法则。具体来说，我们在图 [3](#S5.F3 "图3 ‣ 5.2.1 设置 ‣ 5.2 RLHF的数据选择 ‣ 5 实验
    ‣ 熵法则：数据压缩与LLM表现背后的故事") 和图 [4](#S5.F4 "图4 ‣ 5.2.1 设置 ‣ 5.2 RLHF的数据选择 ‣ 5 实验 ‣
    熵法则：数据压缩与LLM表现背后的故事") 中绘制了Mistral-7B和Llama-3-8B模型性能与数据压缩比和训练损失的关系。此外，我们通过拟合结果绘制了熵法则曲线。从这两幅图中，我们可以得出以下分析：
- en: 'Relationship between model performance, data compression ratio, and training
    loss In Figure [3(a)](#S5.F3.sf1 "In Figure 3 ‣ 5.2.1 Setup ‣ 5.2 Data Selection
    for RLHF ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data Compression and
    LLM Performance") and Figure [4(a)](#S5.F4.sf1 "In Figure 4 ‣ 5.2.1 Setup ‣ 5.2
    Data Selection for RLHF ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data Compression
    and LLM Performance"), LLMs trained on data with a lower compression ratio typically
    exhibit enhanced performance. Since the learning process of LLMs is highly relevant
    to information compression, we can regard LLMs as data compressors. Then the data
    with a lower compression ratio means a higher knowledge amount, which is more
    beneficial to the compressors. Besides, a lower compression ratio usually corresponds
    a higher training loss, as illustrated in Figures [3(b)](#S5.F3.sf2 "In Figure
    3 ‣ 5.2.1 Setup ‣ 5.2 Data Selection for RLHF ‣ 5 Experiments ‣ Entropy Law: The
    Story Behind Data Compression and LLM Performance") and [4(b)](#S5.F4.sf2 "In
    Figure 4 ‣ 5.2.1 Setup ‣ 5.2 Data Selection for RLHF ‣ 5 Experiments ‣ Entropy
    Law: The Story Behind Data Compression and LLM Performance"). This is because
    data resistant to compression carries more knowledge, posing a greater challenge
    for LLMs to absorb the encapsulated knowledge.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3(a)](#S5.F3.sf1 "在图3 ‣ 5.2.1 设置 ‣ 5.2 RLHF的数据选择 ‣ 5 实验 ‣ 熵法则：数据压缩与LLM表现背后的故事")
    和图 [4(a)](#S5.F4.sf1 "在图4 ‣ 5.2.1 设置 ‣ 5.2 RLHF的数据选择 ‣ 5 实验 ‣ 熵法则：数据压缩与LLM表现背后的故事")
    中的模型性能、数据压缩比和训练损失之间的关系表明，使用低压缩比数据训练的LLM通常表现更好。由于LLM的学习过程与信息压缩高度相关，我们可以将LLM视为数据压缩器。因此，低压缩比的数据意味着更多的知识量，对压缩器更为有利。此外，较低的压缩比通常对应较高的训练损失，如图
    [3(b)](#S5.F3.sf2 "在图3 ‣ 5.2.1 设置 ‣ 5.2 RLHF的数据选择 ‣ 5 实验 ‣ 熵法则：数据压缩与LLM表现背后的故事")
    和图 [4(b)](#S5.F4.sf2 "在图4 ‣ 5.2.1 设置 ‣ 5.2 RLHF的数据选择 ‣ 5 实验 ‣ 熵法则：数据压缩与LLM表现背后的故事")
    所示。这是因为抗压缩的数据包含更多的知识，对LLM吸收封装的知识提出了更大挑战。
- en: 'Model performance interpretation with entropy law Considering the three methods
    with comparable compression ratios and training loss, namely Random, Cluster,
    and Perplexity, the corresponding model performances are close. This phenomenon
    may seem counter-intuitive, given the distinct criteria used for data selection.
    However, it aligns with the predictions of our proposed entropy law: when the
    average data quality, training loss, and data compression ratio are similar, the
    model performance is expected to be comparable as well. Thus, the entropy law
    has the potential to serve as a criterion for predicting the model’s performance
    on data, thereby guiding the training of LLMs.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用熵定律对模型性能的解释 考虑到三种具有相似压缩比和训练损失的方法，即随机、聚类和困惑度，对应的模型性能接近。这一现象可能看起来违反直觉，因为数据选择使用了不同的标准。然而，这与我们提出的熵定律的预测一致：当平均数据质量、训练损失和数据压缩比相似时，模型性能也预期会相似。因此，熵定律有潜力作为预测模型数据性能的标准，从而指导LLM的训练。
- en: 'Practical application of entropy law Incremental version update of training
    data is a common setting in practical LLM development. Usually, the training data
    amount remains relatively stable, with only a minor portion undergoing modification.
    We have conducted incremental training data update experiments in real scenarios,
    with results depicted in Figure [5](#S5.F5 "Figure 5 ‣ 5.3 Empirical Validation
    of Entropy Law ‣ 5 Experiments ‣ Entropy Law: The Story Behind Data Compression
    and LLM Performance"). Due to confidentiality, only the relative order of the
    results is provided. Guided by entropy law, assuming the data quality $Q$ exhibits
    an abnormal increase in the loss and data compression ratio, which serves as an
    early indicator of potential model performance degradation due to a decline in
    training data consistency. This prediction is further confirmed by subsequent
    post-training model performance evaluations, as illustrated in Figure [5](#S5.F5
    "Figure 5 ‣ 5.3 Empirical Validation of Entropy Law ‣ 5 Experiments ‣ Entropy
    Law: The Story Behind Data Compression and LLM Performance"). Thus, the entropy
    Law can be utilized as a guideline for LLM training to identify potential risks
    of experimental failure without training the model on the full dataset until convergence.
    This is particularly significant given the substantial costs associated with training
    an LLM.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '熵定律的实际应用 在实际LLM开发中，训练数据的增量版本更新是一种常见的设置。通常，训练数据量保持相对稳定，只有少量部分进行修改。我们在实际场景中进行了增量训练数据更新实验，结果如图[5](#S5.F5
    "Figure 5 ‣ 5.3 Empirical Validation of Entropy Law ‣ 5 Experiments ‣ Entropy
    Law: The Story Behind Data Compression and LLM Performance")所示。由于保密原因，仅提供了结果的相对顺序。在熵定律的指导下，假设数据质量$Q$在损失和数据压缩比中异常增加，这作为潜在模型性能下降的早期指标，因为训练数据一致性下降。此预测进一步通过随后的后训练模型性能评估得到了确认，如图[5](#S5.F5
    "Figure 5 ‣ 5.3 Empirical Validation of Entropy Law ‣ 5 Experiments ‣ Entropy
    Law: The Story Behind Data Compression and LLM Performance")所示。因此，熵定律可以作为LLM训练的指南，以识别实验失败的潜在风险，而无需在完全数据集上训练模型直到收敛。这在考虑到训练LLM的高成本时尤为重要。'
- en: 6 Conclusion
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we delve deeply into the data selection problem from a data compression
    perspective. Inspired by the insight that language modeling is performing information
    compression, we propose an entropy law delineating the coherent relationship between
    model performance, data compression ratio, and training loss. Theoretically guided
    by the entropy law, we propose a new data selection algorithm, ZIP, to select
    data with the nearly lowest compression ratio, which is model-free and content-agnostic.
    rendering it significantly lightweight and versatile. Experimental results have
    demonstrated the effectiveness and efficiency of ZIP, based on various LLM backbones,
    during the SFT and RLHF stages. Further in-depth analysis provided empirical evidence
    of Entropy law, which could serve as a criterion for LLM performance prediction
    on specific data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们从数据压缩的角度深入探讨了数据选择问题。受语言建模执行信息压缩这一见解的启发，我们提出了一条熵定律，阐明了模型性能、数据压缩比和训练损失之间的连贯关系。在熵定律的理论指导下，我们提出了一种新的数据选择算法ZIP，该算法选择压缩比几乎最低的数据，不依赖于模型且与内容无关，使其显著轻量且多功能。实验结果表明，基于各种LLM骨干网络，在SFT和RLHF阶段，ZIP的有效性和效率得到了验证。进一步的深入分析提供了熵定律的实证证据，该证据可以作为预测特定数据上的LLM性能的标准。
- en: References
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*
    (2023).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等. 2023. Gpt-4 技术报告。*arXiv 预印本 arXiv:2303.08774*（2023）。
- en: Albalak et al. (2024) Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre,
    Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon
    Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, and William Yang Wang.
    2024. A Survey on Data Selection for Language Models. arXiv:2402.16827 [cs.CL]
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Albalak 等（2024）Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre,
    Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon
    Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, 和 William Yang Wang. 2024.
    语言模型数据选择的综述。arXiv:2402.16827 [cs.CL]
- en: Bartolome et al. (2023) Alvaro Bartolome, Gabriel Martin, and Daniel Vila. 2023.
    Notus. [https://github.com/argilla-io/notus](https://github.com/argilla-io/notus).
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bartolome 等（2023）Alvaro Bartolome, Gabriel Martin, 和 Daniel Vila. 2023. Notus.
    [https://github.com/argilla-io/notus](https://github.com/argilla-io/notus)。
- en: 'Chen et al. (2023) Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna,
    Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. 2023.
    Alpagasus: Training a better alpaca with fewer data. *arXiv preprint arXiv:2307.08701*
    (2023).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2023）Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas
    Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang 等. 2023. Alpagasus:
    用更少的数据训练更好的 alpaca。*arXiv 预印本 arXiv:2307.08701*（2023）。'
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
    et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
    quality. *See https://vicuna. lmsys. org (accessed 14 April 2023)* 2, 3 (2023),
    6.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang 等（2023）Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu,
    Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez 等.
    2023. Vicuna: 一个开源聊天机器人，表现出 90%* chatgpt 质量的惊人能力。*见 https://vicuna.lmsys.org（访问于
    2023年4月14日）* 2, 3（2023），6。'
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.
    *Journal of Machine Learning Research* 24, 240 (2023), 1–113.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等（2023）Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann 等. 2023. Palm: 通过路径扩展语言建模。*机器学习研究杂志* 24, 240（2023），1–113。'
- en: 'Cui et al. (2023) Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu,
    Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: Boosting
    language models with high-quality feedback. *arXiv preprint arXiv:2310.01377*
    (2023).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cui 等（2023）Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni,
    Guotong Xie, Zhiyuan Liu, 和 Maosong Sun. 2023. Ultrafeedback: 通过高质量反馈提升语言模型。*arXiv
    预印本 arXiv:2310.01377*（2023）。'
- en: Delétang et al. (2023) Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne,
    Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang,
    Matthew Aitchison, Laurent Orseau, Marcus Hutter, and Joel Veness. 2023. Language
    Modeling Is Compression. *CoRR* abs/2309.10668 (2023).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delétang 等（2023）Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot
    Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew
    Aitchison, Laurent Orseau, Marcus Hutter, 和 Joel Veness. 2023. 语言建模就是压缩。*CoRR*
    abs/2309.10668（2023）。
- en: Ding et al. (2023) Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu,
    Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing Chat Language Models
    by Scaling High-quality Instructional Conversations. In *EMNLP*. Association for
    Computational Linguistics, 3029–3051.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等（2023）Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan
    Liu, Maosong Sun, 和 Bowen Zhou. 2023. 通过扩展高质量的指令对话来增强聊天语言模型。发表于*EMNLP*。计算语言学协会，3029–3051。
- en: 'Du et al. (2023) Qianlong Du, Chengqing Zong, and Jiajun Zhang. 2023. Mods:
    Model-oriented data selection for instruction tuning. *arXiv preprint arXiv:2311.15653*
    (2023).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等（2023）Qianlong Du, Chengqing Zong, 和 Jiajun Zhang. 2023. Mods: 面向模型的数据选择用于指令调优。*arXiv
    预印本 arXiv:2311.15653*（2023）。'
- en: 'Ethayarajh et al. (2024) Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan
    Jurafsky, and Douwe Kiela. 2024. Kto: Model alignment as prospect theoretic optimization.
    *arXiv preprint arXiv:2402.01306* (2024).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ethayarajh 等（2024）Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky,
    和 Douwe Kiela. 2024. Kto: 模型对齐作为前景理论优化。*arXiv 预印本 arXiv:2402.01306*（2024）。'
- en: GitHub (2020) GitHub. 2020. GitHub Copilot. [https://github.com/features/copilot/](https://github.com/features/copilot/)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub（2020）GitHub. 2020. GitHub Copilot. [https://github.com/features/copilot/](https://github.com/features/copilot/)
- en: Huang et al. (2024) Yuzhen Huang, Jinghan Zhang, Zifei Shan, and Junxian He.
    2024. Compression Represents Intelligence Linearly. *arXiv preprint arXiv:2404.09937*
    (2024).
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 (2024) Yuzhen Huang, Jinghan Zhang, Zifei Shan 和 Junxian He. 2024.
    压缩线性地表示智能。*arXiv 预印本 arXiv:2404.09937* (2024)。
- en: 'Ivison et al. (2023) Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan
    Lambert, Matthew E. Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith,
    Iz Beltagy, and Hannaneh Hajishirzi. 2023. Camels in a Changing Climate: Enhancing
    LM Adaptation with Tulu 2. *CoRR* abs/2311.10702 (2023).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ivison 等人 (2023) Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert,
    Matthew E. Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz
    Beltagy 和 Hannaneh Hajishirzi. 2023. 气候变化中的骆驼：通过 Tulu 2 增强语言模型适应性。*CoRR* abs/2311.10702
    (2023)。
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. Mistral 7B. *CoRR* abs/2310.06825 (2023).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix 和 William
    El Sayed. 2023. Mistral 7B。*CoRR* abs/2310.06825 (2023)。
- en: Köpf et al. (2024) Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris
    Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver
    Stanley, Richárd Nagyfi, et al. 2024. Openassistant conversations-democratizing
    large language model alignment. *Advances in Neural Information Processing Systems*
    36 (2024).
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Köpf 等人 (2024) Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis,
    Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd
    Nagyfi 等人. 2024. Openassistant conversations—让大语言模型对齐变得民主化。*神经信息处理系统进展* 36 (2024)。
- en: 'Li et al. (2024) Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong
    Wang, Ning Cheng, and Tianyi Zhou. 2024. Superfiltering: Weak-to-Strong Data Filtering
    for Fast Instruction-Tuning. *CoRR* abs/2402.00530 (2024).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2024) Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong
    Wang, Ning Cheng 和 Tianyi Zhou. 2024. 超滤波：从弱到强的数据过滤以加快指令调优。*CoRR* abs/2402.00530
    (2024)。
- en: 'Li et al. (2023) Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen,
    Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2023. From Quantity to
    Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction
    Tuning. *CoRR* abs/2308.12032 (2023).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2023) Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning
    Cheng, Jianzong Wang, Tianyi Zhou 和 Jing Xiao. 2023. 从数量到质量：通过自指导数据选择提升LLM性能。*CoRR*
    abs/2308.12032 (2023)。
- en: Liu et al. (2023) Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.
    2023. What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data
    Selection in Instruction Tuning. *CoRR* abs/2312.15685 (2023).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023) Wei Liu, Weihao Zeng, Keqing He, Yong Jiang 和 Junxian He. 2023.
    什么样的数据适合对齐？关于指令调优中自动数据选择的综合研究。*CoRR* abs/2312.15685 (2023)。
- en: 'Lu et al. (2023) Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin,
    Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023. # InsTag: Instruction Tagging
    for Analyzing Supervised Fine-tuning of Large Language Models. In *The Twelfth
    International Conference on Learning Representations*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu 等人 (2023) Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi
    Tan, Chang Zhou 和 Jingren Zhou. 2023. # InsTag：用于分析大语言模型监督微调的指令标记。发表于*第十二届国际学习表征会议*。'
- en: M. Bran et al. (2024) Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari,
    Andrew D White, and Philippe Schwaller. 2024. Augmenting large language models
    with chemistry tools. *Nature Machine Intelligence* (2024), 1–11.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M. Bran 等人 (2024) Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari,
    Andrew D White 和 Philippe Schwaller. 2024. 用化学工具增强大语言模型。*自然机器智能* (2024), 1–11。
- en: Meta (2020) Meta. 2020. Llama3. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta (2020) Meta. 2020. Llama3. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems* 35 (2022), 27730–27744.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray 等人. 2022.
    训练语言模型以遵循指令并结合人类反馈。*神经信息处理系统进展* 35 (2022), 27730–27744。
- en: Pandey (2024) Rohan Pandey. 2024. gzip Predicts Data-dependent Scaling Laws.
    *arXiv preprint arXiv:2405.16684* (2024).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandey (2024) Rohan Pandey. 2024. gzip 预测数据依赖的缩放定律。*arXiv 预印本 arXiv:2405.16684*
    (2024)。
- en: Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and
    Jianfeng Gao. 2023. Instruction Tuning with GPT-4. *CoRR* abs/2304.03277 (2023).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人（2023） Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, 和 Jianfeng
    Gao。2023。使用 GPT-4 进行指令调优。*CoRR* abs/2304.03277（2023）。
- en: 'Rae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican,
    Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah
    Young, et al. 2021. Scaling language models: Methods, analysis & insights from
    training gopher. *arXiv preprint arXiv:2112.11446* (2021).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rae 等人（2021） Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan
    Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah
    Young 等。2021。扩展语言模型：训练 Gopher 的方法、分析与见解。*arXiv 预印本 arXiv:2112.11446*（2021）。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of machine learning research* 21, 140 (2020), 1–67.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人（2020） Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu。2020。使用统一的文本到文本变换器探索迁移学习的极限。*机器学习研究期刊*
    21, 140（2020），1–67。
- en: Saito et al. (2023) Keita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto.
    2023. Verbosity bias in preference labeling by large language models. *arXiv preprint
    arXiv:2310.10076* (2023).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saito 等人（2023） Keita Saito, Akifumi Wachi, Koki Wataoka, 和 Youhei Akimoto。2023。大型语言模型在偏好标注中的冗长性偏差。*arXiv
    预印本 arXiv:2310.10076*（2023）。
- en: Shannon (1948) Claude E. Shannon. 1948. A mathematical theory of communication.
    *Bell Syst. Tech. J.* 27, 3 (1948), 379–423.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shannon（1948） Claude E. Shannon。1948。通信的数学理论。*贝尔系统技术期刊* 27, 3（1948），379–423。
- en: Shannon (1951) Claude E Shannon. 1951. Prediction and entropy of printed English.
    *Bell system technical journal* 30, 1 (1951), 50–64.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shannon（1951） Claude E Shannon。1951。印刷英文的预测与熵。*贝尔系统技术期刊* 30, 1（1951），50–64。
- en: Shen (2024) Ming Shen. 2024. Rethinking Data Selection for Supervised Fine-Tuning.
    *CoRR* abs/2402.06094 (2024).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen（2024） Ming Shen。2024。重新思考监督微调的数据选择。*CoRR* abs/2402.06094（2024）。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori 等人（2023） Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen
    Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B Hashimoto。2023。斯坦福阿帕卡：一个遵循指令的
    llama 模型。
- en: Wang et al. (2023a) Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel,
    Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz
    Beltagy, and Hannaneh Hajishirzi. 2023a. How Far Can Camels Go? Exploring the
    State of Instruction Tuning on Open Resources. In *NeurIPS*.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023a） Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar
    Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy,
    和 Hannaneh Hajishirzi。2023a。骆驼能走多远？探索开放资源上的指令调优状态。收录于*NeurIPS*。
- en: 'Wang et al. (2023b) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-Instruct:
    Aligning Language Models with Self-Generated Instructions. In *ACL (1)*. Association
    for Computational Linguistics, 13484–13508.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023b） Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah
    A. Smith, Daniel Khashabi, 和 Hannaneh Hajishirzi。2023b。Self-Instruct：通过自生成的指令对齐语言模型。收录于*ACL
    (1)*。计算语言学协会，13484–13508。
- en: 'Wang et al. (2022) Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh
    Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran,
    Atharva Naik, David Stap, et al. 2022. Super-naturalinstructions: Generalization
    via declarative instructions on 1600+ nlp tasks. *arXiv preprint arXiv:2204.07705*
    (2022).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2022） Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh
    Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran,
    Atharva Naik, David Stap 等。2022。超自然指令：通过声明性指令在1600+ NLP任务上进行泛化。*arXiv 预印本 arXiv:2204.07705*（2022）。
- en: 'Wang et al. (2023c) Zige Wang, Wanjun Zhong, Yufei Wang, Qi Zhu, Fei Mi, Baojun
    Wang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023c. Data management for large language
    models: A survey. *arXiv preprint arXiv:2312.01700* (2023).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023c） Zige Wang, Wanjun Zhong, Yufei Wang, Qi Zhu, Fei Mi, Baojun Wang,
    Lifeng Shang, Xin Jiang, 和 Qun Liu。2023c。大型语言模型的数据管理：一项调查。*arXiv 预印本 arXiv:2312.01700*（2023）。
- en: 'Wettig et al. (2024) Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi
    Chen. 2024. QuRating: Selecting High-Quality Data for Training Language Models.
    *arXiv preprint arXiv:2402.09739* (2024).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wettig 等人（2024） Alexander Wettig, Aatmik Gupta, Saumya Malik, 和 Danqi Chen。2024。QuRating：为训练语言模型选择高质量数据。*arXiv
    预印本 arXiv:2402.09739*（2024）。
- en: Xie et al. (2023) Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy S
    Liang. 2023. Data selection for language models via importance resampling. *Advances
    in Neural Information Processing Systems* 36 (2023), 34201–34227.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2023) 由Michael Xie、Shibani Santurkar、Tengyu Ma 和 Percy S Liang 发表。2023年。通过重要性重采样进行语言模型的数据选择。*神经信息处理系统进展*
    36 (2023), 34201–34227。
- en: 'Xu et al. (2023) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan
    Feng, Chongyang Tao, and Daxin Jiang. 2023. WizardLM: Empowering Large Language
    Models to Follow Complex Instructions. *CoRR* abs/2304.12244 (2023).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2023) 由Can Xu、Qingfeng Sun、Kai Zheng、Xiubo Geng、Pu Zhao、Jiazhan
    Feng、Chongyang Tao 和 Daxin Jiang 发表。2023年。WizardLM: 赋能大型语言模型以遵循复杂指令。*CoRR* abs/2304.12244
    (2023)。'
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan
    Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li,
    Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey
    of Large Language Models. *CoRR* abs/2303.18223 (2023).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2023) 由Wayne Xin Zhao、Kun Zhou、Junyi Li、Tianyi Tang、Xiaolei Wang、Yupeng
    Hou、Yingqian Min、Beichen Zhang、Junjie Zhang、Zican Dong、Yifan Du、Chen Yang、Yushuo
    Chen、Zhipeng Chen、Jinhao Jiang、Ruiyang Ren、Yifan Li、Xinyu Tang、Zikang Liu、Peiyu
    Liu、Jian-Yun Nie 和 Ji-Rong Wen 发表。2023年。大型语言模型调查。*CoRR* abs/2303.18223 (2023)。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-Bench
    and Chatbot Arena. In *NeurIPS*.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2023) 由Lianmin Zheng、Wei-Lin Chiang、Ying Sheng、Siyuan Zhuang、Zhanghao
    Wu、Yonghao Zhuang、Zi Lin、Zhuohan Li、Dacheng Li、Eric P. Xing、Hao Zhang、Joseph E.
    Gonzalez 和 Ion Stoica 发表。2023年。通过MT-Bench和Chatbot Arena评判LLM作为评审的能力。发表于*NeurIPS*。
- en: 'Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao
    Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh,
    Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: Less Is More for Alignment.
    In *NeurIPS*.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2023) 由Chunting Zhou、Pengfei Liu、Puxin Xu、Srinivasan Iyer、Jiao
    Sun、Yuning Mao、Xuezhe Ma、Avia Efrat、Ping Yu、Lili Yu、Susan Zhang、Gargi Ghosh、Mike
    Lewis、Luke Zettlemoyer 和 Omer Levy 发表。2023年。LIMA: 对齐的少即是多。发表于*NeurIPS*。'
- en: Appendix A Derivations of joint mutual information of two QA pairs
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 两个问答对的联合互信息推导
- en: '|  | $\displaystyle I(q_{1}q_{2};a_{1}a_{2})$ |  | (7) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle I(q_{1}q_{2};a_{1}a_{2})$ |  | (7) |'
- en: '|  |  | $\displaystyle=H(a_{1}a_{2})-H(a_{1}a_{2}&#124;q_{1}q_{2})$ |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=H(a_{1}a_{2})-H(a_{1}a_{2}&#124;q_{1}q_{2})$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=H(a_{1}a_{2})-H(a_{1}&#124;q_{1})-H(a_{2}&#124;q_{2})$
    |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=H(a_{1}a_{2})-H(a_{1}&#124;q_{1})-H(a_{2}&#124;q_{2})$
    |  |'
- en: '|  |  | $\displaystyle\leq H(a_{1})+H(a_{2})-H(a_{1}&#124;q_{1})-H(a_{2}&#124;q_{2})$
    |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq H(a_{1})+H(a_{2})-H(a_{1}&#124;q_{1})-H(a_{2}&#124;q_{2})$
    |  |'
- en: '|  |  | $\displaystyle=I(q_{1};a_{1})+I(q_{2};a_{2}).$ |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=I(q_{1};a_{1})+I(q_{2};a_{2}).$ |  |'
- en: The equality is achieved when $a_{1}$).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当 $a_{1}$ 时等式成立。
- en: Appendix B Training Details
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 训练细节
- en: Platform All experiments were finished on a platform with 64 Intel Xeon Gold
    6326 CPU cores @ 2.90GHz, two main-stream high-performance GPUs, and 500GB memories.
    The training code is based on a popular open-source framework Axolotl⁴⁴4[https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 平台 所有实验均在配备64个Intel Xeon Gold 6326 CPU核心 @ 2.90GHz、两块主流高性能GPU和500GB内存的设备上完成。训练代码基于一个流行的开源框架
    Axolotl⁴⁴4[https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)。
- en: Data preprocessing To format the multi-turn conversation data, we adopt the
    Vicuna-style template for Mistral-7B and the Llama-3 template for Llama-3-8B.
    Samples longer than the maximum input sequence length will be truncated. Besides,
    the data will be packed to speed up training for SFT.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理 为了格式化多轮对话数据，我们采用了Vicuna风格的模板用于Mistral-7B，以及Llama-3模板用于Llama-3-8B。长度超过最大输入序列长度的样本将被截断。此外，数据将被打包以加速SFT训练。
- en: 'Hyper-parameters For ZIP, the selection numbers $K_{1}$ are set to 10000, 200,
    and 100, respectively. As for SFT, we share these hyper-parameters for all backbones:
    training batch size is 128, training epochs is 4, input sequence length is 2048,
    and the warm-up ratio is 0.1. We adopt different learning rates for each backbone:
    the learning rate of Mistral-7B is set to 4e-6, and the learning rate of Llama-3-8B
    is set to 1e-5. As for RLHF, the learning rate for KTO is set to 1e-6, and the
    batch size is set to 128.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ZIP，选择的超参数 $K_{1}$ 分别设置为 10000、200 和 100。至于 SFT，我们对所有骨干网络共享这些超参数：训练批次大小为
    128，训练轮次为 4，输入序列长度为 2048，预热比例为 0.1。我们为每个骨干网络采用不同的学习率：Mistral-7B 的学习率设置为 4e-6，而
    Llama-3-8B 的学习率设置为 1e-5。至于 RLHF，KTO 的学习率设置为 1e-6，批次大小设置为 128。
- en: Appendix C Token length distribution of more backbones
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 更多骨干网络的令牌长度分布
- en: 'The token length distribution of data selected for Llama-3-8B is depicted in
    Figure [6](#A3.F6 "Figure 6 ‣ Appendix C Token length distribution of more backbones
    ‣ Entropy Law: The Story Behind Data Compression and LLM Performance"), similar
    to the ones of Mistral-7B.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [6](#A3.F6 "图 6 ‣ 附录 C 更多骨干网络的令牌长度分布 ‣ 熵法则：数据压缩与 LLM 性能背后的故事") 描述了为 Llama-3-8B
    选择的数据的令牌长度分布，类似于 Mistral-7B 的分布。
- en: '![Refer to caption](img/a49aba7c558adb03ddb380f77a20b1c7.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a49aba7c558adb03ddb380f77a20b1c7.png)'
- en: (a) ZIP
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ZIP
- en: '![Refer to caption](img/7fae815924dbe648459684b58a918448.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7fae815924dbe648459684b58a918448.png)'
- en: (b) Random
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 随机
- en: '![Refer to caption](img/c5e0e049bbc2bac02f4bf3f6af5a1f64.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c5e0e049bbc2bac02f4bf3f6af5a1f64.png)'
- en: (c) Diversity
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 多样性
- en: '![Refer to caption](img/a336a810c956a0a128dc28d0b52f8b3c.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a336a810c956a0a128dc28d0b52f8b3c.png)'
- en: (d) Perplexity
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 困惑度
- en: '![Refer to caption](img/2976ec3d80aaaa077282401ae32eb525.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2976ec3d80aaaa077282401ae32eb525.png)'
- en: (e) DEITA
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: (e) DEITA
- en: '![Refer to caption](img/7444071275d9c708200f997d62e1b5ef.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7444071275d9c708200f997d62e1b5ef.png)'
- en: (f) SuperFiltering
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 超过滤
- en: 'Figure 6: The distribution of average token number across datasets selected
    by different algorithms for Llama-3-8B.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：不同算法选择的 Llama-3-8B 数据集上的平均令牌数量分布。
- en: Appendix D Hyper-parameter sensitivity
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 超参数敏感性
- en: '![Refer to caption](img/0ee1513bcc46d0c75d5705be6988b515.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0ee1513bcc46d0c75d5705be6988b515.png)'
- en: (a) Model performance w.r.t. $K_{1}$
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 模型性能相对于 $K_{1}$
- en: '![Refer to caption](img/c1001995c0563a5670f375c8eee7b3d3.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c1001995c0563a5670f375c8eee7b3d3.png)'
- en: (b) Model performance w.r.t. $K_{2}$
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 模型性能相对于 $K_{2}$
- en: '![Refer to caption](img/74ad6fd9bc595f9fd18ccc3b2d8c309e.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/74ad6fd9bc595f9fd18ccc3b2d8c309e.png)'
- en: (c) Model performance w.r.t. $K_{3}$
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 模型性能相对于 $K_{3}$
- en: 'Figure 7: Model performance w.r.t. different hyper-parameters.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：模型性能相对于不同超参数。
- en: 'ZIP involves three hyper-parameters $K_{1}$ for improved efficiency. We aim
    to investigate the impact of these hyper-parameters on the model performance,
    with results depicted in Figure [7](#A4.F7 "Figure 7 ‣ Appendix D Hyper-parameter
    sensitivity ‣ Entropy Law: The Story Behind Data Compression and LLM Performance").'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ZIP 涉及三个超参数 $K_{1}$ 以提高效率。我们旨在研究这些超参数对模型性能的影响，结果如图 [7](#A4.F7 "图 7 ‣ 附录 D 超参数敏感性
    ‣ 熵法则：数据压缩与 LLM 性能背后的故事") 所示。
- en: Perceived sample number in global selection $K_{1}$ is set to 20,000.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 全球选择 $K_{1}$ 中的感知样本数量设置为 20,000。
- en: Data pool size of local selection $K_{2}$ exceeds a threshold, the model performance
    reaches a saturated phase, which indicates similar local selection results even
    with increased local data budget.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 当局部选择 $K_{2}$ 的数据池大小超过阈值时，模型性能达到饱和阶段，这表明即使增加局部数据预算，结果也类似。
- en: Data budget of local selection $K_{3}$ will lead to more frequent compression
    ratio updates, which can also lead to underestimated compression ratios of some
    inferior samples.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 局部选择 $K_{3}$ 的数据预算将导致更频繁的压缩比更新，这也可能导致一些劣质样本的压缩比被低估。
