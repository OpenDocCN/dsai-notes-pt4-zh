- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:05:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:05:11
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Outlier Weighed Layerwise Sparsity (OWL ): A Missing Secret Sauce for Pruning
    LLMs to High Sparsity'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**异常值加权逐层稀疏性（OWL）**：修剪大型语言模型（LLMs）至高稀疏性的缺失秘密武器'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.05175](https://ar5iv.labs.arxiv.org/html/2310.05175)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.05175](https://ar5iv.labs.arxiv.org/html/2310.05175)
- en: Lu Yin¹,  You Wu³,  Zhenyu Zhang²,  Cheng-Yu Hsieh⁴,  Yaqing Wang³,  Yiling
    Jia³
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**陆音**¹，**You Wu**³，**Zhenyu Zhang**²，**Cheng-Yu Hsieh**⁴，**Yaqing Wang**³，**Yiling
    Jia**³'
- en: Mykola Pechenizkiy¹,  Yi Liang³,  Zhangyang Wang²,  Shiwei Liu^(1,2)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**Mykola Pechenizkiy**¹，**Yi Liang**³，**Zhangyang Wang**²，**Shiwei Liu**^(1,2)'
- en: ¹Eindhoven University of Technology, ²University of Texas at Austin
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹埃因霍温科技大学，²德克萨斯大学奥斯汀分校
- en: ³Google Research,NY, ⁴University of Washington Partial of this work have been
    done while Lu Yin worked as a Research Intern at Google Research, NY. Corresponding
    to Lu Yin (l.yin@tue.nl) and Shiwei Liu (s.liu3@tue.nl).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³谷歌研究，纽约，⁴华盛顿大学 部分工作在陆音担任谷歌研究实习生期间完成。通讯作者：陆音（l.yin@tue.nl）和刘世伟（s.liu3@tue.nl）。
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs), renowned for their remarkable performance across
    diverse domains, present a challenge due to their colossal model size when it
    comes to practical deployment. In response to this challenge, efforts have been
    directed toward the application of traditional network pruning techniques to LLMs,
    uncovering a massive number of parameters can be pruned in one-shot without hurting
    performance. Building upon insights gained from pre-LLM models, particularly BERT-level
    language models, prevailing LLM pruning strategies have consistently adhered to
    the practice of uniformly pruning all layers at equivalent sparsity levels, resulting
    in robust performance. However, this observation stands in contrast to the prevailing
    trends observed in the field of vision models, where non-uniform layerwise sparsity
    typically yields substantially improved results. To elucidate the underlying reasons
    for this disparity, we conduct a comprehensive analysis of the distribution of
    token features within LLMs. In doing so, we discover a strong correlation with
    the emergence of outliers, defined as features exhibiting significantly greater
    magnitudes compared to their counterparts in feature dimensions. Inspired by this
    finding, we introduce a novel LLM pruning methodology that incorporates a tailored
    set of non-uniform layerwise sparsity ratios specifically designed for LLM pruning,
    termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL
    is directly proportional to the outlier ratio observed within each layer, facilitating
    a more effective alignment between layerwise weight sparsity and outlier ratios.
    Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning
    various benchmarks, demonstrates the distinct advantages offered by OWL over previous
    methods. For instance, our approach exhibits a remarkable performance gain, surpassing
    the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high
    sparsity level of 70%, respectively. Codes are available at [https://github.com/luuyin/OWL](https://github.com/luuyin/OWL).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）以其在各个领域的卓越表现而闻名，但由于其庞大的模型规模，实际部署时面临挑战。为应对这一挑战，研究者们致力于将传统网络剪枝技术应用于LLMs，发现可以在一次性修剪中去除大量参数而不影响性能。基于对预LLM模型，特别是BERT级别语言模型的见解，现有的LLM剪枝策略通常遵循均匀剪枝所有层的稀疏性水平，从而取得了稳健的性能。然而，这一观察与视觉模型领域的主流趋势形成对比，在视觉模型中，非均匀逐层稀疏性通常能显著改善结果。为揭示这种差异的根本原因，我们对LLMs中的token特征分布进行了全面分析。在此过程中，我们发现与异常值的出现有很强的相关性，异常值被定义为在特征维度中具有显著更大幅度的特征。受到这一发现的启发，我们提出了一种新的LLM剪枝方法，结合了一组专门设计的非均匀逐层稀疏比，称为**异常值加权逐层稀疏性（OWL）**。OWL的稀疏比与每层观察到的异常值比例成正比，促进了逐层权重稀疏性与异常值比例之间的更有效对齐。我们的实证评估在LLaMA-V1系列和OPT中进行，涵盖各种基准测试，显示出OWL相较于先前方法的明显优势。例如，我们的方法在70%的高稀疏水平下，表现出比最先进的Wanda和SparseGPT分别高出61.22和6.80的困惑度。代码可在[https://github.com/luuyin/OWL](https://github.com/luuyin/OWL)获取。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The remarkable performance exhibited by Large Language Models (LLMs) across
    a diverse spectrum of applications has ignited an unparalleled race among tech
    giants and academic institutions to build LLMs at the billion-parameter scale (Brown
    et al., [2020](#bib.bib2); Touvron et al., [2023a](#bib.bib40); [b](#bib.bib41);
    Brown et al., [2020](#bib.bib2)). The compelling performance of Large Language
    Models (LLMs) demonstrated in various applications triggers an unprecedented competition
    of building billion-level LLMs among tech giants and academic institutions (Brown
    et al., [2020](#bib.bib2); Touvron et al., [2023a](#bib.bib40); [b](#bib.bib41);
    Brown et al., [2020](#bib.bib2)). While their exceptional capabilities are undeniable,
    the colossal size and computational demands of these models have also raised substantial
    concerns, particularly in terms of financial expenditure and environment (Luccioni
    et al., [2022](#bib.bib25); Patterson et al., [2021](#bib.bib34)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各类应用中展现出的卓越表现引发了科技巨头和学术机构之间前所未有的竞赛，旨在构建参数规模达到十亿的LLMs（Brown et al.,
    [2020](#bib.bib2); Touvron et al., [2023a](#bib.bib40); [b](#bib.bib41); Brown
    et al., [2020](#bib.bib2)）。大型语言模型（LLMs）在各种应用中展示出的令人信服的表现激发了科技巨头和学术机构之间前所未有的构建十亿级LLMs的竞争（Brown
    et al., [2020](#bib.bib2); Touvron et al., [2023a](#bib.bib40); [b](#bib.bib41);
    Brown et al., [2020](#bib.bib2)）。尽管它们的卓越能力无可否认，但这些模型的庞大规模和计算需求也引发了诸多关注，尤其是在财务开支和环境方面（Luccioni
    et al., [2022](#bib.bib25); Patterson et al., [2021](#bib.bib34)）。
- en: Network pruning (Mozer & Smolensky, [1989](#bib.bib33); Janowsky, [1989](#bib.bib16);
    LeCun et al., [1989](#bib.bib18); Han et al., [2015](#bib.bib13)), as a long-established
    model compression method, is expected to serve as an effective solution for reducing
    the size of LLMs. However, network pruning usually favors a certain time of fine-tuning
    or re-training to reacquire the original optimal performance. Given the extensive
    text corpus and model size associated with LLMs, conventional fine-tuning becomes
    exceedingly challenging and less desirable. Fortunately, recent endeavors have
    explored the possibility of LLM pruning without the need for fine-tuning, showcasing
    that LLMs contain a substantial number of parameters that can be removed in a
    single step with minimal performance degradation (Jaiswal et al., [2023](#bib.bib15);
    Frantar & Alistarh, [2023](#bib.bib10); Sun et al., [2023](#bib.bib39)). SparseGPT (Frantar
    & Alistarh, [2023](#bib.bib10)) addresses the challenge of LLM pruning from the
    perspective of layerwise reconstruction problem. In this context, the primary
    goal is to minimize the output discrepancy in terms of the reconstruction error
    between dense and sparse LLMs. It adopts an iterative strategy to handle the computational
    hurdle posed by the row-Hessian problem. Specifically, it employs the Optimal
    Brain Surgeon (OBS) algorithm (Hassibi et al., [1993](#bib.bib14)) to selectively
    prune and update weights in a column-wise manner. Wanda (Sun et al., [2023](#bib.bib39)),
    on the other hand, introduces a novel pruning metric that takes into account both
    the weight magnitudes and their corresponding input activations. Remarkably, it
    achieves performance on par with SparseGPT without relying on computationally
    expensive second-order information. The effectiveness of Wanda stems from the
    emergence of the outlier features residing within large-scale LLMs. These outliers,
    which tend to be significantly larger than typical features, are nonetheless crucial
    for optimizing LLM performance (Dettmers et al., [2022](#bib.bib5)). In general,
    both SparseGPT and Wanda exhibit competitive performance, showcasing their ability
    to reduce model parameters by up to 50% while incurring only a modest increase
    of approximately 1 in perplexity (Sun et al., [2023](#bib.bib39)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 网络剪枝（Mozer & Smolensky，[1989](#bib.bib33)；Janowsky，[1989](#bib.bib16)；LeCun
    et al.，[1989](#bib.bib18)；Han et al.，[2015](#bib.bib13)），作为一种久经考验的模型压缩方法，预计能够有效减少大型语言模型（LLMs）的体积。然而，网络剪枝通常需要一定的微调或重新训练以重新获得原始的最佳性能。考虑到与LLMs相关的庞大文本语料库和模型规模，传统的微调变得极其困难且不太理想。幸运的是，最近的研究探索了无需微调的LLM剪枝可能性，展示了LLMs中存在大量可以在单步操作中移除的参数，且性能下降最小（Jaiswal
    et al.，[2023](#bib.bib15)；Frantar & Alistarh，[2023](#bib.bib10)；Sun et al.，[2023](#bib.bib39)）。SparseGPT（Frantar
    & Alistarh，[2023](#bib.bib10)）从逐层重构问题的角度解决LLM剪枝的挑战。在这种情况下，主要目标是最小化稠密和稀疏LLMs之间的重构误差。它采用一种迭代策略来处理由行Hessian问题带来的计算难题。具体来说，它使用最优脑外科医生（OBS）算法（Hassibi
    et al.，[1993](#bib.bib14)）以列方式选择性剪枝和更新权重。另一方面，Wanda（Sun et al.，[2023](#bib.bib39)）引入了一种新颖的剪枝指标，该指标考虑了权重大小及其相应的输入激活。值得注意的是，它在没有依赖计算开销大的二阶信息的情况下，达到了与SparseGPT相当的性能。Wanda的有效性来源于大规模LLMs中存在的异常特征。这些异常特征通常比典型特征大得多，但对于优化LLM性能却至关重要（Dettmers
    et al.，[2022](#bib.bib5)）。总体而言，SparseGPT和Wanda均表现出竞争力，展示了它们能够将模型参数减少多达50%，同时仅引起约1点的困惑度轻微增加（Sun
    et al.，[2023](#bib.bib39)）。
- en: 'It is worth noting that SparseGPT and Wanda unanimously follow previous work
    on BERT pruning (Sanh et al., [2020](#bib.bib37); Kurtic et al., [2022](#bib.bib17))
    and choose to prune LLMs with a uniform sparsity ratio per layer, *i.e.,* each
    layer will be pruned at the same sparsity. Such choice is reasonable for LLMs,
    as the pruning process typically involves sorting the importance scores of weights.
    Conducting such sorting globally across layers could become a computational bottleneck,
    especially for models at the billion-parameter scale. Nevertheless, before it
    has been taken root that uniform layerwise sparsity is the default choice for
    LLMs, we raise a timely inquiry: are there any pivotal aspects that have been
    inadvertently omitted in the context of favorable layerwise sparsity ratios for
    LLM pruning?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，SparseGPT 和 Wanda 一致遵循了之前关于 BERT 剪枝的研究（Sanh et al.，[2020](#bib.bib37)；Kurtic
    et al.，[2022](#bib.bib17)），并选择以每层相同的稀疏比率剪枝 LLM，即每层都以相同的稀疏度进行剪枝。这种选择对于 LLM 是合理的，因为剪枝过程通常涉及对权重重要性分数的排序。在所有层中进行全局排序可能会成为计算瓶颈，特别是对于参数规模达到十亿的模型。然而，在均匀层级稀疏性尚未成为
    LLM 的默认选择之前，我们提出了一个及时的问题：在 LLM 剪枝的有利层级稀疏性比率的背景下，有没有被不经意遗漏的关键方面？
- en: 'Three reasons behoove us to pose the above research question: First, it is
    widely acknowledged that within Transformer architectures, certain components
    hold greater significance than others, and thus, they merit distinct treatment
    during the pruning process (Wang & Tu, [2020](#bib.bib44); Bhojanapalli et al.,
    [2021](#bib.bib1)); Second, a consensus view has been reached in computer vision
    that non-uniform layerwise sparsity typically achieves stronger results than uniform
    sparsity (Liu et al., [2022](#bib.bib24); Lee et al., [2020](#bib.bib19)); More
    importantly, LLMs demonstrate astonishingly emergent behaviors (Dettmers et al.,
    [2022](#bib.bib5); Wei et al., [2022](#bib.bib45); Schaeffer et al., [2023](#bib.bib38))
    as model size continuously scales up, a phenomenon distinct from smaller-scale
    language models such as BERT (Devlin et al., [2018](#bib.bib6)). These emergent
    behaviors offer fresh insights into the domain of LLM pruning. For instance, Dettmers
    et al. ([2022](#bib.bib5)) revealed the existence of outlier features within LLMs,
    with magnitudes up to 20 times larger than others, exerting a profound influence
    across all Transformer layers.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个原因促使我们提出上述研究问题：首先，普遍认可的是在 Transformer 架构中，某些组件比其他组件更重要，因此在剪枝过程中应给予不同的处理（Wang
    & Tu，[2020](#bib.bib44)；Bhojanapalli et al.，[2021](#bib.bib1)）；其次，计算机视觉领域达成了共识，即非均匀层级稀疏性通常比均匀稀疏性能取得更好的结果（Liu
    et al.，[2022](#bib.bib24)；Lee et al.，[2020](#bib.bib19)）；更重要的是，随着模型规模的不断扩大，LLM
    展现出惊人的涌现行为（Dettmers et al.，[2022](#bib.bib5)；Wei et al.，[2022](#bib.bib45)；Schaeffer
    et al.，[2023](#bib.bib38)），这一现象与较小规模的语言模型（如 BERT）（Devlin et al.，[2018](#bib.bib6)）不同。这些涌现行为为
    LLM 剪枝领域提供了新的见解。例如，Dettmers et al.（[2022](#bib.bib5)）揭示了 LLM 中存在异常特征，其幅度比其他特征大达
    20 倍，对所有 Transformer 层产生深远的影响。
- en: 'Contributions. Given the pivotal role that outliers play in the performance
    of LLMs, coupled with the demonstrated effectiveness of Wanda (Sun et al., [2023](#bib.bib39)),
    our initial investigation centers on a systematic examination of the impact of
    existing LLM pruning methodologies on outliers. To our astonishment, we uncover
    a compelling correlation between pruning efficacy and the retention ratio of outliers:
    contemporary state-of-the-art LLM pruning approaches, such as SparseGPT and Wanda,
    exhibit remarkable preservation of outliers, even though the former was not originally
    designed with this intent. Moreover, we conduct an in-depth analysis of the distribution
    of outliers across different layers and observe a notably non-uniform pattern.
    This non-uniform distribution emerges as a valuable indicator for the formulation
    of layerwise sparsity strategies tailored specifically for LLMs. Building upon
    this newfound insight, we introduce an LLM pruning paradigm characterized by a
    novel layerwise sparsity ratio, denoted as Outlier Weighed Layerwise sparsity
    (OWL). OWL inherently assigns greater emphasis to layers housing a higher prevalence
    of outliers, thereby facilitating more nuanced coordination between sparsity in
    weight matrices and the presence of outliers within the layer.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 贡献。鉴于异常值在LLM性能中的关键作用，以及Wanda（Sun et al., [2023](#bib.bib39)）的有效性，我们的初步研究集中在系统地检验现有LLM剪枝方法对异常值的影响。令我们惊讶的是，我们发现剪枝效果与异常值保留比之间存在显著相关性：当代最先进的LLM剪枝方法，如SparseGPT和Wanda，即使前者最初并未以此为设计目标，也表现出对异常值的显著保留。此外，我们对不同层次的异常值分布进行了深入分析，并观察到明显的不均匀模式。这种不均匀分布成为了制定针对LLM的层级稀疏策略的有价值指标。基于这一新发现，我们引入了一种具有新层级稀疏比的LLM剪枝范式，称为异常值加权层级稀疏（OWL）。OWL本质上赋予异常值较多的层级更大的关注，从而促进了权重矩阵稀疏性与层级内异常值存在之间的更精细协调。
- en: We conduct extensive experiments to evaluate the performance OWL across a spectrum
    of large language models, including LLaMA-V1 family (Touvron et al., [2023a](#bib.bib40)),
    and OPT (Zhang et al., [2022](#bib.bib49)), from 7B to 65B. Our empirical results
    show that OWL consistently outperforms existing top-performing LLM pruning methods,
    particularly at high sparsity levels. For instance, we observe significant improvements
    achieved by OWL over Wanda with LLaMa-7B on WikiText (Merity et al., [2016a](#bib.bib27)),
    with perplexity reductions of more than 60 and 3300 perplexity at sparsity levels
    of 70% and 80%, respectively. Our research presents a compelling counter-argument
    to previous study by shedding light on the previously overlooked yet crucial role
    of layerwise sparsity ratios in the context of LLM pruning. This shift in perspective
    has allowed us to push the boundaries of achievable LLM pruning ratios to reach
    70% without the need of any weight updates or second-order Hessian.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行广泛实验以评估OWL在各种大型语言模型中的表现，包括LLaMA-V1系列（Touvron et al., [2023a](#bib.bib40)）和OPT（Zhang
    et al., [2022](#bib.bib49)），从7B到65B。我们的实证结果表明，OWL始终优于现有的顶级LLM剪枝方法，特别是在高稀疏度水平下。例如，我们观察到OWL在WikiText（Merity
    et al., [2016a](#bib.bib27)）上对Wanda的显著改进，在LLaMa-7B模型下，稀疏度为70%和80%时，困惑度分别减少了60和3300。我们的研究提供了对以往研究的有力反驳，通过揭示在LLM剪枝过程中层级稀疏比的关键作用。这一视角的转变使我们能够将可实现的LLM剪枝比推高至70%，而无需任何权重更新或二阶Hessian。
- en: 2 Related Work
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Pruning and LLM Pruning. Since the 1980s, network pruning has been a well-established
    technique for simplifying neural networks in various applications while maintaining
    accuracy (Mozer & Smolensky, [1989](#bib.bib33); Han et al., [2015](#bib.bib13);
    Mocanu et al., [2018](#bib.bib32); Wen et al., [2017](#bib.bib46); Lin et al.,
    [2019](#bib.bib22)). However, when it comes to pruning Large Language Models (LLMs),
    progress has been limited. Traditional pruning typically requires a round of re-training
    to restore performance, which can be challenging for LLMs. To address this challenge,
    researchers have developed pruning algorithms specifically tailored for LLM compression.
    For example, Ma et al. ([2023](#bib.bib26)) explored structured sparse LLMs using
    Taylor pruning to remove entire weight rows, followed by LoRA fine-tuning (Ma
    et al., [2023](#bib.bib26)). Recent research has shifted toward unstructured pruning
    without the need for fine-tuning, showing substantial advancements. SparseGPT (Frantar
    & Alistarh, [2023](#bib.bib10)) utilizes the Hessian inverse for pruning and with
    subsequent weight updates to reduce reconstruction error of dense and sparse weights,
    while Wanda (Sun et al., [2023](#bib.bib39)) produces a criterion incorporating
    weight magnitude with their input activations, aiming to preserve outlier features (Dettmers
    et al., [2022](#bib.bib5)). Our work for the first time probe and highlight the
    crucial role of non-uniform layerwise sparsity for LLM pruning, making a notable
    progress in this field.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝与大语言模型剪枝。自1980年代以来，网络剪枝已经成为简化神经网络的一种成熟技术，在各种应用中能够保持准确性（Mozer & Smolensky,
    [1989](#bib.bib33)；Han et al., [2015](#bib.bib13)；Mocanu et al., [2018](#bib.bib32)；Wen
    et al., [2017](#bib.bib46)；Lin et al., [2019](#bib.bib22)）。然而，对于大语言模型（LLMs）的剪枝，进展有限。传统剪枝通常需要重新训练来恢复性能，这对LLMs来说可能具有挑战性。为了应对这一挑战，研究人员开发了专门针对LLM压缩的剪枝算法。例如，Ma
    et al. ([2023](#bib.bib26)) 探索了使用泰勒剪枝的结构化稀疏LLMs，以删除整个权重行，随后进行了LoRA微调（Ma et al.,
    [2023](#bib.bib26)）。最近的研究转向了无需微调的非结构化剪枝，显示出显著的进展。SparseGPT（Frantar & Alistarh,
    [2023](#bib.bib10)）利用Hessian逆进行剪枝，并通过随后的权重更新来减少密集和稀疏权重的重建误差，而Wanda（Sun et al.,
    [2023](#bib.bib39)）提出了一种结合权重大小及其输入激活的标准，旨在保留异常特征（Dettmers et al., [2022](#bib.bib5)）。我们的工作首次探讨并强调了非均匀层级稀疏性在LLM剪枝中的关键作用，在这一领域取得了显著进展。
- en: 'Layerwise Sparsity for Pruning. While it is common to use uniform layerwise
    sparsity (Zhu & Gupta, [2017](#bib.bib50); Gale et al., [2019](#bib.bib11)) to
    prune language models (Sanh et al., [2020](#bib.bib37); Kurtic et al., [2022](#bib.bib17)),
    there is a well-established line of work that explore non-uniform layerwise sparsity
    in terms of pruning vision models. Mocanu et al. ([2016](#bib.bib31)) propose
    a non-uniform and scale-free topology inspired from graph theory, showing better
    performance than the dense counterpart when applied to restricted Boltzmann machines.
    Follow-up works significantly improve its scalability based on Erdős-Rényi graph (Erdős
    & Rényi, [1959](#bib.bib7)), extending to fully-connected layers (Mocanu et al.,
    [2018](#bib.bib32)) and convolutional layers (Evci et al., [2020](#bib.bib8);
    Liu et al., [2022](#bib.bib24)) as data-free and feedforward-free layerwise sparsity.
    Another group of work produces non-uniform sparsity by applying a global threshold
    on every layer (Frankle & Carbin, [2019](#bib.bib9); Lee et al., [2019](#bib.bib20);
    Wang et al., [2020](#bib.bib43); Lee et al., [2020](#bib.bib19); Liu et al., [2021](#bib.bib23)).
    However, global pruning becomes extremely expensive and inefficacy in the context
    of LLM pruning as shown in Table [2](#S3.T2 "Table 2 ‣ 3.2 Empirical Study ‣ 3
    Outlier Weighed Layerwise Sparsity – OWL ‣ Outlier Weighed Layerwise Sparsity
    (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity"). We also provide
    a comparison among most common layerwise sparsity for LLMs in Section [5](#S5
    "5 Analysis ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce
    for Pruning LLMs to High Sparsity"), and all of them fail to perform on LLMs.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '剪枝的分层稀疏性。尽管在剪枝语言模型时通常使用均匀分层稀疏性（Zhu & Gupta, [2017](#bib.bib50); Gale et al.,
    [2019](#bib.bib11)），但在视觉模型的剪枝方面，已有一系列研究探索了非均匀分层稀疏性。Mocanu et al. ([2016](#bib.bib31))
    提出了受图论启发的非均匀且无尺度拓扑，应用于限制玻尔兹曼机时显示出比稠密对照模型更好的性能。后续工作基于Erdős-Rényi图显著提高了其可扩展性（Erdős
    & Rényi, [1959](#bib.bib7)），扩展到全连接层（Mocanu et al., [2018](#bib.bib32)）和卷积层（Evci
    et al., [2020](#bib.bib8); Liu et al., [2022](#bib.bib24)）作为无数据和前馈的分层稀疏性。另一组工作通过对每一层应用全局阈值生成非均匀稀疏性（Frankle
    & Carbin, [2019](#bib.bib9); Lee et al., [2019](#bib.bib20); Wang et al., [2020](#bib.bib43);
    Lee et al., [2020](#bib.bib19); Liu et al., [2021](#bib.bib23)）。然而，正如表[2](#S3.T2
    "Table 2 ‣ 3.2 Empirical Study ‣ 3 Outlier Weighed Layerwise Sparsity – OWL ‣
    Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs
    to High Sparsity")所示，全球剪枝在LLM剪枝的背景下变得极其昂贵且效果不佳。我们还在第[5](#S5 "5 Analysis ‣ Outlier
    Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High
    Sparsity")节提供了对最常见分层稀疏性在LLM中的比较，它们都未能在LLM上表现出色。'
- en: Outliers in LLMs. Unlike traditional vision or smaller-scale transformer models,
    recent studies have revealed certain emergent characteristics unique to language
    models at scale. Specifically, one intriguing trait of LLMs is the exhibition
    of outlier features, which are the features with significantly larger magnitudes
    than others (Dettmers et al., [2022](#bib.bib5)). While constituting only a very
    small portion of the entire feature dimensions, these outliers play an imperative
    role in models’ predictive performance. Building upon this observation, several
    recent works have developed techniques to effectively quantize LLMs with minimal
    performance drop (Dettmers et al., [2022](#bib.bib5); Xiao et al., [2023](#bib.bib47);
    Lin et al., [2023](#bib.bib21)). On the other hand, in the context of LLM pruning,
    this unique characteristic has scarcely been taken into account to the best of
    our knowledge (Sun et al., [2023](#bib.bib39)). Our work draws on the importance
    of the emergent outliers in LLMs, and provides a systematic study on its correlation
    to the effectiveness of model pruning, leading to a novel technique that leverages
    the distribution of outliers to guide layerwise LLM pruning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: LLM中的异常值。与传统的视觉模型或小规模的变换器模型不同，最近的研究揭示了大规模语言模型的某些独特特征。具体而言，LLM的一个有趣特征是异常值特征，即那些幅度显著大于其他特征的特征（Dettmers
    et al., [2022](#bib.bib5)）。尽管这些异常值仅占整个特征维度的极小部分，但它们在模型的预测性能中发挥着至关重要的作用。在此观察的基础上，最近的几项工作开发了有效量化LLM的技术，且性能下降最小（Dettmers
    et al., [2022](#bib.bib5); Xiao et al., [2023](#bib.bib47); Lin et al., [2023](#bib.bib21)）。另一方面，在LLM剪枝的背景下，据我们所知，这一独特特征几乎未被考虑（Sun
    et al., [2023](#bib.bib39)）。我们的工作基于LLM中异常值的重要性，提供了一个系统研究其与模型剪枝效果相关性的研究，提出了一种新技术，利用异常值的分布指导分层LLM剪枝。
- en: 3 Outlier Weighed Layerwise Sparsity – OWL
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 异常值加权分层稀疏性 – OWL
- en: In this section, we will introduce Outlier-Weighted Layer-wise sparsity (OWL)
    step by step, from rationale, to empirical studies, and eventually to the algorithm.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将逐步介绍异常值加权层级稀疏（OWL），从理论基础到实证研究，最终到算法。
- en: 3.1 Rationale
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 理论基础
- en: The primary of goal of network pruning is to discover the least important components,
    such as individual weights in the case of unstructured pruning, which have minimal
    impact on the model’s output. In the context of pre-LLMs with smaller scales,
    magnitude pruning has traditionally serves as the most basic yet effective technique,
    consistently delivering robust results across various scenarios (Han et al., [2015](#bib.bib13);
    Mocanu et al., [2018](#bib.bib32); Frankle & Carbin, [2019](#bib.bib9); Jaiswal
    et al., [2023](#bib.bib15)). The effectiveness of magnitude pruning in compressing
    pre-LLM models is closely intertwined with the feasibility of fine-tuning. It
    has been observed that even the random removal of components can ultimately restore
    the original performance through adequate fine-tuning (Liu et al., [2022](#bib.bib24);
    Mittal et al., [2019](#bib.bib30)). However, fine-tuning encounters significant
    challenges when applied to LLMs, rendering magnitude pruning less effective compared
    to more precise pruning metrics, such as second-order Hessian (Frantar & Alistarh,
    [2023](#bib.bib10)) and input activation (Sun et al., [2023](#bib.bib39)). Notably,
    Wanda (Sun et al., [2023](#bib.bib39)) achieves remarkable performance by augmenting
    input activation with weight magnitude, underscoring the critical importance of
    preserving outlier features in LLM pruning. Considering the vital role that outliers
    play in the context of LLMs (Dettmers et al., [2022](#bib.bib5)) and the success
    of Wanda, we conjecture that the performance of different pruning methods has
    a strong correlation with their ability to preserve outlier features. To assess
    our conjecture, we undertake several preliminary investigations outlined below
    based on Layerwise Outlier Distribution.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 网络剪枝的主要目标是发现最不重要的组件，例如在非结构化剪枝情况下的单个权重，这些组件对模型输出的影响最小。在小规模预训练 LLM 的背景下，幅度剪枝传统上作为最基本但有效的技术，在各种场景中
    consistently 产生了稳健的结果（Han et al., [2015](#bib.bib13); Mocanu et al., [2018](#bib.bib32);
    Frankle & Carbin, [2019](#bib.bib9); Jaiswal et al., [2023](#bib.bib15)）。幅度剪枝在压缩预训练
    LLM 模型中的有效性与微调的可行性密切相关。观察到即使随机去除组件，通过适当的微调最终也可以恢复原始性能（Liu et al., [2022](#bib.bib24);
    Mittal et al., [2019](#bib.bib30)）。然而，当应用于 LLM 时，微调遇到了重大挑战，使得幅度剪枝相比于更精确的剪枝指标（如二阶
    Hessian（Frantar & Alistarh, [2023](#bib.bib10)）和输入激活（Sun et al., [2023](#bib.bib39)））效果较差。值得注意的是，Wanda（Sun
    et al., [2023](#bib.bib39)）通过结合输入激活和权重幅度取得了显著的性能，强调了在 LLM 剪枝中保留异常特征的关键重要性。考虑到异常值在
    LLM 背景下的重要作用（Dettmers et al., [2022](#bib.bib5)）以及 Wanda 的成功，我们推测不同剪枝方法的性能与其保留异常特征的能力有强烈相关性。为了评估我们的推测，我们基于层级异常值分布进行了一些初步调查。
- en: 3.2 Empirical Study
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 实证研究
- en: Layerwise Outlier Distribution (LOD). Our preliminary studies are based on Layerwise
    Outlier Distribution (LOD), a concept used to measure how outlier features distribute
    and effect weights across layers. Since we focus on weight pruning in this paper,
    instead of measuring the outlier distribution of input features, We opt to prioritize
    the impact of outlier features on weights, which is quantified as the accumulation
    of all input features connected to the target weight, multiplied by the weight
    magnitude (Sun et al., [2023](#bib.bib39)). Our intuition here is that weights
    that are most affected by outliers also play a pivotal role in propagating and
    preserving these outlier features.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 层级异常值分布（LOD）。我们的初步研究基于层级异常值分布（LOD），这一概念用于测量异常特征在各层之间的分布和对权重的影响。由于本文重点讨论权重剪枝，我们选择优先考虑异常特征对权重的影响，而不是测量输入特征的异常值分布，这一影响被量化为所有与目标权重连接的输入特征的累积值，乘以权重的大小 （Sun
    et al., [2023](#bib.bib39)）。我们的直觉是，最受异常值影响的权重也在传播和保留这些异常特征方面发挥了关键作用。
- en: To formalize our approach, we consider the input of a layer as $\mathbf{X}$.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了形式化我们的方法，我们将一层的输入视为 $\mathbf{X}$。
- en: Subsequently, after obtaining the impact of features for all weights $\mathbf{A}$-layer
    LLMs. Based on LOD, we conduct three empirical studies outlined below to better
    understand the effect of LOD on LLM pruning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，在获得所有权重 $\mathbf{A}$-层 LLMs 的特征影响后。基于 LOD，我们进行以下三个实证研究，以更好地理解 LOD 对 LLM 剪枝的影响。
- en: 'Empirical Study I: Dense LLMs vs. LOD. To investigate whether sparsifying LLMs
    necessitates differential treatment of individual layers, we employ LOD to gauge
    the layerwise distribution of outliers within dense LLMs. If LOD in dense LLMs
    exhibits a relatively uniform pattern, it suggests that a non-uniform layerwise
    distribution may not be imperative, at least in terms of outlier features, and
    vice versa. We assess the LOD across various dense LLMs, including LLaMA-7B, 13B,
    and 30B.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 实证研究 I：密集型 LLM 与 LOD。为了研究稀疏化 LLM 是否需要对各个层进行差异化处理，我们使用 LOD 来评估密集型 LLM 中的异常值层级分布。如果密集型
    LLM 中的 LOD 显示出相对均匀的模式，这表明至少在异常值特征方面，非均匀的层级分布可能不是必要的，反之亦然。我们评估了不同密集型 LLM（包括 LLaMA-7B、13B
    和 30B）的 LOD。
- en: 'Empirical Study II: Pruning Metric vs. LOD. We further delve into the impact
    of different pruning metrics on LOD. The primary objective of this study is to
    explore whether there exists a robust correlation between the performance of various
    pruning methods and their ability to preserve outliers. To achieve this, we aggregate
    the LOD values across layers for various LLM pruning methods, such as magnitude,
    Wanda, and SparseGPT, and compare them with their dense counterparts. In the case
    of sparse LLMs, we calculate LOD by considering only non-zero weights. All sparse
    models are pruned with uniform sparsity. These experiments are conducted using
    LLaMA-13B at sparsity level of 60% and 70% with $\mathbf{M}=7$.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实证研究 II：修剪指标与 LOD。我们进一步*深入探讨*不同修剪指标对 LOD 的影响。本研究的主要目的是探索各种修剪方法的性能与其保留异常值能力之间是否存在强相关性。为此，我们汇总了各种
    LLM 修剪方法（如 magnitude、Wanda 和 SparseGPT）的 LOD 值，并与其密集型对照进行比较。在稀疏 LLM 的情况下，我们通过考虑仅非零权重来计算
    LOD。所有稀疏模型都以均匀稀疏性进行修剪。这些实验使用 LLaMA-13B 在 60% 和 70% 的稀疏度下进行，$\mathbf{M}=7$。
- en: 'Empirical Study III: Pruning Granularity vs. LOD. It is well-established that
    non-uniform or global layerwise sparsity often leads to more accurate sparser
    networks at high sparsity than the uniform layerwise sparsity for pre-LLM pruning.
    However, endeavors unanimously point out that uniform sparsity is more favorable
    for pruning LLMs. To provide more insights about these two seemingly countradictory
    arguments, we study the effect of various pruning granularities on LOD. Specifically,
    we study two sets of pruning granularities: (1) Across different layers, we compare
    the performance as well as the resulting LOD of uniform sparsity and global sparsity;
    (2) Within the same layer, we study the output-imbalanced sparsity used by SparseGPT
    against the output-balanced sparsity adopted by Wanda. Output-balanced sparsity
    eliminates the same amount of weights for all outputs. We conduct experiments
    with magnitude pruning and Wanda using LLaMA-7B at various sparsity.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 实证研究 III：修剪粒度与 LOD。已知非均匀或全局层级稀疏性通常在高稀疏性下比均匀层级稀疏性更能产生准确的稀疏网络。然而，各方的努力一致指出均匀稀疏性对于修剪
    LLM 更为有利。为了提供关于这两种看似矛盾的论点的更多见解，我们研究了不同修剪粒度对 LOD 的影响。具体而言，我们研究了两组修剪粒度：（1）在不同层之间，我们比较了均匀稀疏性和全局稀疏性的性能及其结果
    LOD；（2）在同一层内部，我们研究了 SparseGPT 使用的输出不均衡稀疏性与 Wanda 采用的输出均衡稀疏性。输出均衡稀疏性为所有输出消除相同数量的权重。我们使用
    LLaMA-7B 进行 magnitude 修剪和 Wanda 的实验，涵盖不同稀疏度。
- en: 'Results: We present our findings from Study 1-3, in Figure [1](#S3.F1 "Figure
    1 ‣ 3.2 Empirical Study ‣ 3 Outlier Weighed Layerwise Sparsity – OWL ‣ Outlier
    Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High
    Sparsity"), Table [1](#S3.T1 "Table 1 ‣ 3.2 Empirical Study ‣ 3 Outlier Weighed
    Layerwise Sparsity – OWL ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing
    Secret Sauce for Pruning LLMs to High Sparsity"), and Table [2](#S3.T2 "Table
    2 ‣ 3.2 Empirical Study ‣ 3 Outlier Weighed Layerwise Sparsity – OWL ‣ Outlier
    Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High
    Sparsity"), respectively. These results provide positive support for our conjecture,
    and we summarize the key observations below:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '结果：我们在图表 [1](#S3.F1 "Figure 1 ‣ 3.2 Empirical Study ‣ 3 Outlier Weighed Layerwise
    Sparsity – OWL ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce
    for Pruning LLMs to High Sparsity")、表格 [1](#S3.T1 "Table 1 ‣ 3.2 Empirical Study
    ‣ 3 Outlier Weighed Layerwise Sparsity – OWL ‣ Outlier Weighed Layerwise Sparsity
    (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity") 和表格 [2](#S3.T2
    "Table 2 ‣ 3.2 Empirical Study ‣ 3 Outlier Weighed Layerwise Sparsity – OWL ‣
    Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs
    to High Sparsity") 中展示了我们从研究 1-3 中获得的发现。这些结果为我们的猜测提供了积极的支持，我们在下面总结了关键观察结果：'
- en: 1
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 1
- en: 'LOD of dense LLMs exhibits a highly non-uniform distribution across layers.
    In essence, the distribution of dense LLMs shown in Figure [1](#S3.F1 "Figure
    1 ‣ 3.2 Empirical Study ‣ 3 Outlier Weighed Layerwise Sparsity – OWL ‣ Outlier
    Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High
    Sparsity") loosely follows a “U” shape, with notable proportions at both ends,
    while the central region displays a monotonic descending trend. This finding validates
    our conjecture that individual layers need unique consideration during the pruning
    procedure. Employing uniform pruning across all layers would inevitably disrupt
    the outlier structure in layers characterized by a large outlier ratio, such as
    those layers at the beginning or end of models.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 稠密 LLMs 的 LOD 展现出层间高度不均匀的分布。实际上，图 [1](#S3.F1 "图 1 ‣ 3.2 实证研究 ‣ 3 异常值加权分层稀疏性
    – OWL ‣ 异常值加权分层稀疏性 (OWL)：高稀疏性剪枝 LLMs 的缺失秘密调料") 中展示的稠密 LLMs 的分布大致呈现“U”形，两端有显著的比例，而中间区域则表现出单调下降的趋势。这一发现验证了我们的猜想，即在剪枝过程中，各个层需要独特的考虑。对所有层进行均匀剪枝必然会破坏具有大异常值比的层的异常值结构，如模型的开头或结尾的层。
- en: 'Table 1: Effects of various pruning methods on Layerwise Outlier Distribution
    (LOD) and Perplexity with LLaMA-13B on WikiText. LOD is calculated as the summation
    across all layers with $\mathbf{M}=7$.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同剪枝方法对 LLaMA-13B 在 WikiText 上的分层异常值分布（LOD）和困惑度的影响。LOD 是通过对所有层进行求和计算的，$\mathbf{M}=7$。
- en: '| Sparsity | Method | LOD (%) $\uparrow$ |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏度 | 方法 | LOD (%) $\uparrow$ |'
- en: '|  | Dense | 5.432 | - | 5.090 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | 稠密 | 5.432 | - | 5.090 |'
- en: '|  | Wanda | 5.716 | 0.284 | 55.900 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | Wanda | 5.716 | 0.284 | 55.900 |'
- en: '| 70% | SparseGPT | 6.645 | 1.213 | 19.235 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 70% | SparseGPT | 6.645 | 1.213 | 19.235 |'
- en: '|  | Magnitude | 5.322 | -0.110 | 84539.445 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | 量值 | 5.322 | -0.110 | 84539.445 |'
- en: '|  | Wanda | 5.433 | 0.001 | 8.761 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | Wanda | 5.433 | 0.001 | 8.761 |'
- en: '| 60% | SparseGPT | 6.044 | 0.612 | 8.458 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 60% | SparseGPT | 6.044 | 0.612 | 8.458 |'
- en: '|  | Magnitude | 5.322 | -0.110 | 229.451 | 2'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 量值 | 5.322 | -0.110 | 229.451 | 2'
- en: The performance of sparse pruning methods on LLMs is closely correlated with
    their ability to retain outlier features. Leading pruning techniques like Wanda
    and SparseGPT all excel in outlier, resulting in an overall increase in LOD. In
    contrast, the naive baseline of magnitude pruning performs no better than random
    selection at 70% sparsity, as evidenced by a negative change of -0.110 in LOD,
    indicating the removal of important outliers. It is interesting to see that despite
    SparseGPT not being explicitly designed for outlier preservation, it achieves
    the highest LOD as well as performance, providing further insight into the underlying
    reason for its success. A plausible reason is that the weight update involved
    within SparseGPT helps increase LOD.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏剪枝方法在 LLMs 上的表现与保留异常值特征的能力密切相关。领先的剪枝技术如 Wanda 和 SparseGPT 都在异常值方面表现出色，导致整体
    LOD 增加。相比之下，70% 稀疏度下的简单基线量值剪枝表现不佳，其 LOD 变化为 -0.110，表明重要异常值被移除。值得注意的是，尽管 SparseGPT
    并非专门设计用于异常值保留，但它取得了最高的 LOD 和表现，这进一步揭示了其成功的潜在原因。一个合理的解释是 SparseGPT 中的权重更新有助于提高
    LOD。
- en: '![Refer to caption](img/1da47306c7be711abad3daa56ff0e449.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1da47306c7be711abad3daa56ff0e449.png)'
- en: 'Figure 1: Layerwise Outlier Distribution (LOD) (%) of dense LLaMA-7B, 13B,
    and 30B.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：稠密 LLaMA-7B、13B 和 30B 的分层异常值分布（LOD，%）。
- en: 'Table 2: WikiText perplexity with LLaMA-7B of various pruning granularity.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同剪枝粒度下，LLaMA-7B 在 WikiText 上的困惑度。
- en: '| Method | Layerwise | Output | Sparsity |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 分层 | 输出 | 稀疏度 |'
- en: '| Uniform | Balanced | 10% | 20% | 30% | 40% | 50% | 60% | 70% |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 均匀 | 平衡 | 10% | 20% | 30% | 40% | 50% | 60% | 70% |'
- en: '| Wanda | ✓ | ✓ | 5.697 | 5.817 | 5.999 | 6.388 | 7.260 | 10 | 86 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✓ | ✓ | 5.697 | 5.817 | 5.999 | 6.388 | 7.260 | 10 | 86 |'
- en: '| Wanda | ✓ | ✗ | 5.695 | 5.819 | 6.029 | 6.572 | 7.942 | 20 | 238 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✓ | ✗ | 5.695 | 5.819 | 6.029 | 6.572 | 7.942 | 20 | 238 |'
- en: '| Wanda | ✗ | ✗ | 14.117 | 3134 | 10293 | 10762 | 14848 | 17765 | 5147 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✗ | ✗ | 14.117 | 3134 | 10293 | 10762 | 14848 | 17765 | 5147 |'
- en: '| Magnitude | ✓ | ✓ | 5.803 | 6.018 | 6.622 | 8.041 | 13.349 | 152 | 25304
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✓ | ✓ | 5.803 | 6.018 | 6.622 | 8.041 | 13.349 | 152 | 25304
    |'
- en: '| Magnitude | ✓ | ✗ | 5.806 | 6.020 | 6.669 | 8.601 | 17.287 | 559 | 48419
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✓ | ✗ | 5.806 | 6.020 | 6.669 | 8.601 | 17.287 | 559 | 48419
    |'
- en: '| Magnitude | ✗ | ✗ | 5.821 | 6.111 | 7.012 | 9.825 | 48.627 | 38335 | 29283
    | 3'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '| Magnitude | ✗ | ✗ | 5.821 | 6.111 | 7.012 | 9.825 | 48.627 | 38335 | 29283
    | 3'
- en: Pruning with coarser granularity results in diminished performance. In general,
    we observe a consistent trend of improved perplexity as the pruning granularity
    becomes finer, transitioning from global layerwise sparsity to uniform layerwise
    sparsity at the macro level, and from output imbalanced sparsity to output balanced
    sparsity at the micro level. These findings align with the conclusions presented
    by Sun et al. ([2023](#bib.bib39)). One plausible explanation for this trend is
    that coarser-grained pruning tends to eliminate outlier features to a more significant
    extent, particularly in certain layers or outputs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用较粗粒度的剪枝会导致性能下降。一般来说，我们观察到随着剪枝粒度的变细，困惑度有一致的改进趋势，从宏观层面的全局逐层稀疏性转变为均匀逐层稀疏性，从微观层面的输出不平衡稀疏性转变为输出平衡稀疏性。这些发现与Sun等人（[2023](#bib.bib39)）提出的结论一致。这种趋势的一个可能解释是，粗粒度剪枝往往会在更大程度上消除异常值，特别是在某些层或输出中。
- en: 3.3 Outlier Weighed Layerwise Sparsity (OWL)
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 异常值加权逐层稀疏性（OWL）
- en: The above empirical studies underscore the critical significance of preserving
    outliers in the context of LLM pruning. Consequently, it becomes imperative to
    implement layerwise pruning strategies that take into account the non-uniform
    distribution of outliers across different layers. However, global pruning can
    be costly and lead to collapse of outliers, resulting in significant performance
    degradation. On the other hand, uniform pruning does not adequately consider the
    highly non-uniform distribution of outlier features across various layers. This
    negligence inevitably disrupts the structure of outliers in layers characterized
    by a substantial outlier ratio, particularly at high sparsity levels. Therefore,
    there is a need of an ideal layerwise sparsity that aligns effectively with the
    layerwise outlier distribution while maintaining computational and memory efficiency.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述实证研究强调了在LLM剪枝过程中保持异常值的重要性。因此，实施逐层剪枝策略时必须考虑不同层次中异常值的非均匀分布。然而，全局剪枝可能会昂贵且导致异常值的崩溃，从而导致性能显著下降。另一方面，均匀剪枝未能充分考虑各种层次中异常值特征的高度非均匀分布。这种忽视不可避免地扰乱了异常值在异常值比例较大的层中的结构，特别是在高稀疏性水平下。因此，需要一种理想的逐层稀疏性，它能够有效地与逐层异常值分布对齐，同时保持计算和内存效率。
- en: 'To address this issue, we propose a novel layerwise sparsity ratio strategy,
    referred to as Outlier-Weighted Layer-wise sparsity (OWL) explicitly tailored
    for Large Language Models, which can better coordinate with the outlier distribution
    by taking the layerwise outlier ratio into consideration. Given a $l$ [3, 5, 7,
    10]. The visualization of our layerwise sparsity ratio is demonstrated in Figure [2](#S3.F2
    "Figure 2 ‣ 3.3 Outlier Weighed Layerwise Sparsity (OWL) ‣ 3 Outlier Weighed Layerwise
    Sparsity – OWL ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce
    for Pruning LLMs to High Sparsity"), where we can clearly see that the layerwise
    sparsity level of OWL nuancedly aligns with model’s LOD.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决这个问题，我们提出了一种新颖的逐层稀疏比策略，称为异常点加权逐层稀疏性（OWL），该策略专门针对大型语言模型（LLM）进行调整，能够通过考虑逐层异常点比例，更好地与异常点分布协调。给定一个
    $l$ [3, 5, 7, 10]。我们逐层稀疏比的可视化在图[2](#S3.F2 "Figure 2 ‣ 3.3 Outlier Weighed Layerwise
    Sparsity (OWL) ‣ 3 Outlier Weighed Layerwise Sparsity – OWL ‣ Outlier Weighed
    Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity")
    中得以展示，在此图中我们可以清楚地看到，OWL的逐层稀疏水平与模型的LOD细致地对齐。'
- en: '![Refer to caption](img/84c6fe1be86c392f70b6bae7dcb59490.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/84c6fe1be86c392f70b6bae7dcb59490.png)'
- en: 'Figure 2: The demonstration of the OWL layerwise sparsity and Uniform layerwise
    sparsity at 70% sparsity. The bar chart in background corresponds to the Layerwise
    Outlier Distribution (LOD).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：70%稀疏度下，OWL逐层稀疏性和均匀逐层稀疏性的演示。背景中的柱状图对应于逐层异常点分布（LOD）。
- en: 4 Experiments
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: Models and Dataset. We assess OWL’s performance across a range of LLMs, encompassing
    the LLaMA-V1 model family (Touvron et al., [2023b](#bib.bib41)) with parameter
    counts ranging from 7 billion to 65 billion, as well as OPT-6.7B (Zhang et al.,
    [2022](#bib.bib49)). Our evaluation protocol aligns with established LLM pruning
    methodologies (Frantar & Alistarh, [2023](#bib.bib10); Sun et al., [2023](#bib.bib39)),
    encompassing assessments of language modeling proficiency and zero-shot capabilities
    of sparse LLMs. Specifically, we measure the Perplexity metric on the WikiText (Merity
    et al., [2016b](#bib.bib28)) validation dataset for language modeling performance,
    and employ the Accuracy metric for zero-shot evaluations on seven common sense
    benchmarks, including BoolQ (Clark et al., [2019](#bib.bib3)), RTE (Wang et al.,
    [2018](#bib.bib42)), HellaSwag (Zellers et al., [2019](#bib.bib48)), WinoGrande (Sakaguchi
    et al., [2019](#bib.bib36)), ARC Easy and Challenge (Clark et al., [2018](#bib.bib4)),
    and OpenbookQA (Mihaylov et al., [2018](#bib.bib29)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和数据集。我们评估了OWL在一系列LLM中的表现，包括参数范围从70亿到650亿的LLaMA-V1模型系列（Touvron等，[2023b](#bib.bib41)）以及OPT-6.7B（Zhang等，[2022](#bib.bib49)）。我们的评估协议与已建立的LLM剪枝方法（Frantar
    & Alistarh，[2023](#bib.bib10); Sun等，[2023](#bib.bib39)）一致，包括对稀疏LLM的语言建模能力和零样本能力的评估。具体来说，我们在WikiText（Merity等，[2016b](#bib.bib28)）验证数据集上测量Perplexity指标以评估语言建模性能，并在七个常识基准上使用Accuracy指标进行零样本评估，包括BoolQ（Clark等，[2019](#bib.bib3)）、RTE（Wang等，[2018](#bib.bib42)）、HellaSwag（Zellers等，[2019](#bib.bib48)）、WinoGrande（Sakaguchi等，[2019](#bib.bib36)）、ARC
    Easy和Challenge（Clark等，[2018](#bib.bib4)）以及OpenbookQA（Mihaylov等，[2018](#bib.bib29)）。
- en: 'Baselines. We choose the three current LLM-pruning baselines, including magnitude (Jaiswal
    et al., [2023](#bib.bib15)), SparseGPT (Frantar & Alistarh, [2023](#bib.bib10)),
    Wanda (Sun et al., [2023](#bib.bib39)). Magnitude pruning serves as a naive baseline
    for LLMs, with an expected sharp decline in performance at modest sparsity levels,
    typically ranging from 10% to 30%. SparseGPT and Wanda, on the other hand, are
    established baselines known for their ability to maintain reasonable performance
    even at relatively high sparsity levels, typically around 50% to 60%. Notably,
    in contrast to our approach, all baseline methods employ with uniform layerwise
    sparsity. We primarily focus on high sparsity levels, not falling below 50%, as
    regions with low sparsity pose challenges for existing sparse GPU kernels to outperform
    their dense counterparts (Gale et al., [2020](#bib.bib12)). To ensure equitable
    comparisons, we have employed the identical set of calibration data as utilized
    by SparseGPT and Wanda for model pruning, *i.e.,* comprising 128 sequences with
    2048 tokens for each, randomly sampled from the first shard of the C4 (Raffel
    et al., [2020](#bib.bib35)) dataset. We incorporate OWL directly into Wanda and
    SparseGPT, resulting in two variants: “OWL w. Wanda” and “OWL w. SparseGPT”. The
    only distinction between these variants lies in their layerwise sparsity ratios,
    with OWL providing a more tailored layerwise sparsity in this regard. Hyperparameters
    are shared in Table [4](#S4.F4 "Figure 4 ‣ 5.2 Pruning Efficiency ‣ 5 Analysis
    ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning
    LLMs to High Sparsity")-Right.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基线。我们选择了三种当前的LLM剪枝基线，包括 magnitude (Jaiswal et al., [2023](#bib.bib15))，SparseGPT
    (Frantar & Alistarh, [2023](#bib.bib10))，Wanda (Sun et al., [2023](#bib.bib39))。Magnitude
    剪枝作为LLMs的一个简单基线，预计在适度的稀疏性水平（通常为10%到30%）下性能会急剧下降。另一方面，SparseGPT 和 Wanda 是已建立的基线，知名于即使在相对高的稀疏性水平（通常在50%到60%）下也能保持合理的性能。值得注意的是，与我们的方法相比，所有基线方法都采用均匀的逐层稀疏性。我们主要关注高稀疏性水平，不低于50%，因为稀疏性较低的区域对于现有的稀疏GPU内核而言表现不如稠密内核
    (Gale et al., [2020](#bib.bib12))。为确保公平比较，我们使用了与 SparseGPT 和 Wanda 剪枝相同的校准数据集，即从C4
    (Raffel et al., [2020](#bib.bib35)) 数据集的第一个分片中随机抽取的128个序列，每个序列2048个标记。我们将 OWL
    直接融入 Wanda 和 SparseGPT，生成两个变体：“OWL w. Wanda”和“OWL w. SparseGPT”。这两个变体之间唯一的区别在于它们的逐层稀疏性比例，OWL
    在这方面提供了更为量体裁衣的逐层稀疏性。超参数见表 [4](#S4.F4 "图 4 ‣ 5.2 剪枝效率 ‣ 5 分析 ‣ 异常加权逐层稀疏性 (OWL)：剪枝LLMs到高稀疏性的缺失秘方")-右。
- en: 'Table 3: WikiText validation perplexity of pruning methods for LLaMA-V1 family
    and OPT-6.7B at 70% sparsity. The best performance method is indicated in bold,
    and the gain in perplexity achieved by OWL is highlighted in blue.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在70%稀疏性下，LLaMA-V1家族和OPT-6.7B的剪枝方法在WikiText验证困惑度的表现。最佳性能的方法用粗体标出，OWL所取得的困惑度增益用蓝色突出显示。
- en: '| Method | Layerwise | Weight | LLaMA-V1 | OPT |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 逐层 | 权重 | LLaMA-V1 | OPT |'
- en: '|  | Sparsity | Update | 7B | 13B | 30B | 65B | 6.7B |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | 稀疏性 | 更新 | 7B | 13B | 30B | 65B | 6.7B |'
- en: '| Dense | - | - | 5.68 | 5.09 | 4.10 | 4.77 | 10.13 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 稠密 | - | - | 5.68 | 5.09 | 4.10 | 4.77 | 10.13 |'
- en: '| Magnitude | Uniform | ✗ | 48419.12 | 84539.45 | 977.73 | 46.89 | 290985.03
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 数值 | 均匀 | ✗ | 48419.12 | 84539.45 | 977.73 | 46.89 | 290985.03 |'
- en: '| Wanda | Uniform | ✗ | 85.77 | 55.90 | 17.37 | 15.23 | 162.92 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 均匀 | ✗ | 85.77 | 55.90 | 17.37 | 15.23 | 162.92 |'
- en: '| OWL w. Wanda | Non-Uni | ✗ | 24.55 (-61.22) | 17.17 (-38.73) | 10.75 (-6.62)
    | 8.61 (-6.62) | 40.22 (-120.70) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| OWL w. Wanda | 非均匀 | ✗ | 24.55 (-61.22) | 17.17 (-38.73) | 10.75 (-6.62)
    | 8.61 (-6.62) | 40.22 (-120.70) |'
- en: '| SparseGPT | Uniform | ✓ | 26.30 | 19.24 | 12.56 | 10.45 | 20.29 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 均匀 | ✓ | 26.30 | 19.24 | 12.56 | 10.45 | 20.29 |'
- en: '| OWL w. SparseGPT | Non-Uni | ✓ | 19.49 (-6.81) | 14.55 (-4.69) | 10.28 (-2.28)
    | 8.28 (-0.64) | 22.48 (2.19) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| OWL w. SparseGPT | 非均匀 | ✓ | 19.49 (-6.81) | 14.55 (-4.69) | 10.28 (-2.28)
    | 8.28 (-0.64) | 22.48 (2.19) |'
- en: 4.1 Experimental Results
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验结果
- en: 'Language Modelling. We first report the performance of various LLM pruning
    methods on language modelling with WikiText. The results is presented in Table [3](#S4.T3
    "Table 3 ‣ 4 Experiments ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing
    Secret Sauce for Pruning LLMs to High Sparsity") and Figure LABEL:fig:LLaMA_7B.
    We summarize the key observation below:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模。我们首先报告了各种LLM剪枝方法在WikiText上的语言建模表现。结果如表 [3](#S4.T3 "表 3 ‣ 4 实验 ‣ 异常加权逐层稀疏性
    (OWL)：剪枝LLMs到高稀疏性的缺失秘方") 和图 LABEL:fig:LLaMA_7B 所示。我们总结了以下关键观察：
- en: 1
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 1
- en: 'OWL demonstrates its versatility serving as a general layerwise sparsity method
    suitable for various scenarios. As illustrated in Table [3](#S4.T3 "Table 3 ‣
    4 Experiments ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce
    for Pruning LLMs to High Sparsity"), OWL exhibits effectiveness across different
    pruning methods (such as Wanda and SparseGPT), architectural variants (including
    LLaMA-V1 and OPT), and diverse model sizes (ranging from LLaMA-V1 with 7B, 13B,
    30B, to 65B parameters), resulting in substantial reductions in perplexity scores.
    Notably, even when applied to SparseGPT, a strong pruning method incorporating
    second-order information, OWL still achieves significant perplexity reductions,
    exemplified by a reduction of 6.81 for LLaMA-7B.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 'OWL 展示了其作为通用层次稀疏方法的多功能性，适用于各种场景。如表格 [3](#S4.T3 "Table 3 ‣ 4 Experiments ‣ Outlier
    Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High
    Sparsity") 所示，OWL 在不同的剪枝方法（如 Wanda 和 SparseGPT）、架构变体（包括 LLaMA-V1 和 OPT）以及不同模型规模（从
    LLaMA-V1 的 7B、13B、30B 到 65B 参数）中都表现出有效性，从而显著降低了困惑度得分。值得注意的是，即使在应用于包含二阶信息的强剪枝方法
    SparseGPT 时，OWL 仍然取得了显著的困惑度降低，例如 LLaMA-7B 降低了 6.81。'
- en: 2
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 2
- en: The benefits of OWL increases as significantly model size decreases. There is
    a clear trend that the performance gain of OWL monotonically increases as LLaMA-V1
    scales down from 65B to 7B. While the performance improvement of OWL .w Wanda
    for LLaMA-65B is relatively small, at 6.62, it achieves a remarkable gain of 61.22
    for LLaMA-7B, resulting in a reasonable 24.55 perplexity.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型规模的显著减少，OWL 的优势也随之增加。OWL 的性能增益与 LLaMA-V1 从 65B 缩小到 7B 的规模呈单调增加的趋势。虽然 OWL
    与 Wanda 在 LLaMA-65B 上的性能提升相对较小，为 6.62，但在 LLaMA-7B 上取得了显著的 61.22 增益，最终得到了合理的 24.55
    困惑度。
- en: 'Zero-Shot Tasks. While perplexity is a widely used metric for language modeling,
    it primarily serves as a statistical measure of how confidently a language model
    predicts a text sample and does not necessarily align with the quality of the
    generated text. To draw more robust conclusions, we conducted experiments to evaluate
    the zero-shot ability of various sparse LLMs on diverse zero-shot downstream tasks
    with prompting. These experiments were performed using the LLaMA-V1 family at
    70% sparsity, and the results are presented in Table [4](#S4.T4 "Table 4 ‣ 4.1
    Experimental Results ‣ 4 Experiments ‣ Outlier Weighed Layerwise Sparsity (OWL):
    A Missing Secret Sauce for Pruning LLMs to High Sparsity"). It’s noteworthy that
    OWL consistently improves accuracy across nearly all settings, with very few exceptions
    on RTE data, which is . For example, OWL achieves an average perplexity gain of
    4.72 and 2.19 over 7 tasks and 4 model sizes compared to Wanda and SparseGPT alone,
    respectively. This result highlights the promise of OWL is still hold for more
    challenging zero-shot downstream tasks.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '零样本任务。虽然困惑度是语言建模中广泛使用的指标，但它主要作为语言模型预测文本样本的信心的统计度量，并不一定与生成文本的质量一致。为了得出更稳健的结论，我们进行了实验，以评估各种稀疏
    LLM 在不同零样本下游任务上的零样本能力。这些实验是在 70% 稀疏的 LLaMA-V1 系列上进行的，结果见表格 [4](#S4.T4 "Table 4
    ‣ 4.1 Experimental Results ‣ 4 Experiments ‣ Outlier Weighed Layerwise Sparsity
    (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity")。值得注意的是，OWL 在几乎所有设置下都一致提高了准确性，只有
    RTE 数据上的例外非常少。例如，OWL 相比 Wanda 和 SparseGPT 单独使用，在 7 个任务和 4 种模型规模上分别实现了平均困惑度增益 4.72
    和 2.19。这一结果突显了 OWL 在更具挑战性的零样本下游任务中仍然具有潜力。'
- en: 'Table 4: Accuracies (%) for 7 zero-shot tasks with 70% sparsity using LLaMA-V1
    family.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 4: 使用 LLaMA-V1 系列在 70% 稀疏下的 7 个零样本任务的准确性（%）。'
- en: '| Params | Method | BoolQ | RTE | HellaSwag | WinoGrande | ARC-e | ARC-c |
    OBQA | Mean |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Params | Method | BoolQ | RTE | HellaSwag | WinoGrande | ARC-e | ARC-c |
    OBQA | Mean |'
- en: '| 7B | Dense | $75.14$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 7B | Dense | $75.14$ |'
- en: '| Magnitude | $38.29$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | $38.29$ |'
- en: '| Wanda | $55.11$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | $55.11$ |'
- en: '| OWL w. Wanda | 62.48 | 58.48 | 44.79 | 58.72 | 45.03 | 26.19 | 29.60 | 46.47
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| OWL w. Wanda | 62.48 | 58.48 | 44.79 | 58.72 | 45.03 | 26.19 | 29.60 | 46.47
    |'
- en: '| SparseGPT | 64.53 | 53.79 | 42.11 | $58.64$ | 43.06 | 24.57 | 27.80 | 44.93
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 64.53 | 53.79 | 42.11 | $58.64$ | 43.06 | 24.57 | 27.80 | 44.93
    |'
- en: '| OWL w. SparseGPT | 67.13 | 53.43 | 48.56 | 62.03 | 45.41 | 27.65 | 32.00
    | 48.03 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| OWL w. SparseGPT | 67.13 | 53.43 | 48.56 | 62.03 | 45.41 | 27.65 | 32.00
    | 48.03 |'
- en: '| 13B | Dense | $77.86$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 13B | Dense | $77.86$ |'
- en: '| Magnitude | $52.94$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | $52.94$ |'
- en: '| Wanda | $61.71$ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | $61.71$ |'
- en: '| OWL w. Wanda | 62.69 | 52.71 | 51.03 | 63.14 | 49.54 | 28.67 | 34.40 | 48.88
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| OWL w. Wanda | 62.69 | 52.71 | 51.03 | 63.14 | 49.54 | 28.67 | 34.40 | 48.88
    |'
- en: '| SparseGPT | 66.94 | 52.71 | 47.91 | 62.90 | 45.03 | 27.99 | 35.20 | 48.38
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 66.94 | 52.71 | 47.91 | 62.90 | 45.03 | 27.99 | 35.20 | 48.38
    |'
- en: '| OWL w. SparseGPT | 64.95 | 53.07 | 54.39 | 66.54 | 48.86 | 30.12 | 38.00
    | 50.85 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| OWL w. SparseGPT | 64.95 | 53.07 | 54.39 | 66.54 | 48.86 | 30.12 | 38.00
    | 50.85 |'
- en: '| 30B | Dense | 82.69 | 66.79 | 81.19 | 75.85 | 73.48 | 50.77 | 44.60 | 67.91
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 30B | 密集 | 82.69 | 66.79 | 81.19 | 75.85 | 73.48 | 50.77 | 44.60 | 67.91
    |'
- en: '| Magnitude | 39.14 | 46.21 | 24.31 | 52.33 | 24.66 | 22.87 | 29.00 | 34.07
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 39.14 | 46.21 | 24.31 | 52.33 | 24.66 | 22.87 | 29.00 | 34.07 |'
- en: '| Wanda | $66.12$ |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | $66.12$ |'
- en: '| OWL w. Wanda | 66.42 | 52.35 | 62.94 | 69.30 | 61.83 | 35.84 | 40.00 | 55.53
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| OWL w. Wanda | 66.42 | 52.35 | 62.94 | 69.30 | 61.83 | 35.84 | 40.00 | 55.53
    |'
- en: '| SparseGPT | 66.51 | 63.90 | 60.38 | 69.85 | 58.54 | 33.70 | 40.60 | 55.78
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 66.51 | 63.90 | 60.38 | 69.85 | 58.54 | 33.70 | 40.60 | 55.78
    |'
- en: '| OWL w. SparseGPT | 67.58 | 58.48 | 64.88 | 70.72 | 60.82 | 35.07 | 42.20
    | 57.11 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| OWL w. SparseGPT | 67.58 | 58.48 | 64.88 | 70.72 | 60.82 | 35.07 | 42.20
    | 57.11 |'
- en: '| 65B | Dense | 84.86 | 69.68 | 82.94 | 77.35 | 75.08 | 52.56 | 44.20 | 69.52
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 65B | 密集 | 84.86 | 69.68 | 82.94 | 77.35 | 75.08 | 52.56 | 44.20 | 69.52
    |'
- en: '| Magnitude | 52.17 | 54.87 | 49.87 | 56.67 | 49.71 | 30.63 | 38.80 | 47.53
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 52.17 | 54.87 | 49.87 | 56.67 | 49.71 | 30.63 | 38.80 | 47.53 |'
- en: '| Wanda | 76.30 | 56.68 | 61.26 | 70.48 | 63.47 | 35.67 | 39.40 | 57.61 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 76.30 | 56.68 | 61.26 | 70.48 | 63.47 | 35.67 | 39.40 | 57.61 |'
- en: '| OWL w. Wanda | 80.12 | 58.84 | 66.16 | 73.56 | 65.45 | 39.93 | 42.20 | 60.89
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| OWL w. Wanda | 80.12 | 58.84 | 66.16 | 73.56 | 65.45 | 39.93 | 42.20 | 60.89
    |'
- en: '| SparseGPT | 80.64 | 59.57 | 66.42 | 72.61 | 60.52 | 38.57 | 40.80 | 59.88
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 80.64 | 59.57 | 66.42 | 72.61 | 60.52 | 38.57 | 40.80 | 59.88
    |'
- en: '| OWL w. SparseGPT | 82.63 | 67.15 | 68.52 | 75.06 | 60.10 | 39.59 | 39.00
    | 61.72 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| OWL w. SparseGPT | 82.63 | 67.15 | 68.52 | 75.06 | 60.10 | 39.59 | 39.00
    | 61.72 |'
- en: 5 Analysis
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 分析
- en: 5.1 Comparisons Among Various Layerwise Sparsity
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 各种层级稀疏度比较
- en: 'We compare OWL layerwise sparsity with multiple commonly used layerwise sparsity,
    including:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将OWL层级稀疏度与多种常用的层级稀疏度进行比较，包括：
- en: •
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Global Frankle & Carbin ([2019](#bib.bib9)). A global threshold is uniformly
    applied to all layers to satisfy the overall sparsity requirement, and the specific
    layerwise sparsity is automatically adjusted based on this threshold.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 全球Frankle & Carbin ([2019](#bib.bib9))。对所有层应用统一的全局阈值，以满足总体稀疏度要求，并根据该阈值自动调整特定的层级稀疏度。
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Uniform (Zhu & Gupta, [2017](#bib.bib50)). Every layer is pruned with the same
    target sparsity.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 均匀（Zhu & Gupta，[2017](#bib.bib50)）。每一层都按照相同的目标稀疏度进行修剪。
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Erdős-Rényi Kernel (ERK) (Evci et al., [2020](#bib.bib8)). The sparsity of the
    convolutional layer is scaled proportional to $1-\frac{n^{l-1}+n^{l}+w^{l}+h^{l}}{n^{l-1}\times
    n^{l}\times w^{l}\times h^{l}}$ are the corresponding width and height. ERK is
    modified based on ER (Mocanu et al., [2018](#bib.bib32)).
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Erdős-Rényi 核心 (ERK) (Evci 等，[2020](#bib.bib8))。卷积层的稀疏度按比例缩放，公式为$1-\frac{n^{l-1}+n^{l}+w^{l}+h^{l}}{n^{l-1}\times
    n^{l}\times w^{l}\times h^{l}}$，其中$n^{l-1}$、$n^{l}$、$w^{l}$和$h^{l}$分别为对应的宽度和高度。ERK是基于ER（Mocanu
    等，[2018](#bib.bib32)）进行修改的。
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ERK-plus (Liu et al., [2022](#bib.bib24)). ERK-plus modifies ERK by forcing
    the last layer as dense if it is not, while keeping the overall parameter count
    the same.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ERK-plus（Liu 等，[2022](#bib.bib24)）。ERK-plus通过强制最后一层为密集层（如果不是的话）来修改ERK，同时保持总体参数数量不变。
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: OWL-inverse. OWL-inverse metric is the inverse variant of OWL, whose outlier
    ratio is $1-\texttt{LOD}$.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: OWL-逆向。OWL-逆向度量是OWL的逆变体，其异常值比例为$1-\texttt{LOD}$。
- en: 'For this study, we apply Wanda to the LLaMA-7B model. The results are presented
    in Table [5](#S5.T5 "Table 5 ‣ 5.1 Comparisons Among Various Layerwise Sparsity
    ‣ 5 Analysis ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce
    for Pruning LLMs to High Sparsity"). It is noteworthy that all approaches, except
    for the Global method, perform satisfactorily when the sparsity level is at or
    below 40%. This observation suggests that the region of low sparsity does not
    provide significant distinctions for performance comparison. However, as the sparsity
    level exceeds 50%, discrepancies between the various approaches become evident.
    Notably, the Uniform and OWL methods emerge as the top-performing approaches,
    with OWL consistently outperforming the former across all sparsity levels. On
    the other hand, the ERK family of methods appears to be less suitable for LLM
    pruning. It’s worth mentioning that the performance of OWL experiences a significant
    decline when we invert its outlier ratio, underscoring the effectiveness of LOD
    in identifying critical layers.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本研究，我们将 Wanda 应用于 LLaMA-7B 模型。结果如表 [5](#S5.T5 "表 5 ‣ 5.1 各种层次稀疏度的比较 ‣ 5 分析
    ‣ 异常值加权层次稀疏度 (OWL)：剪枝 LLMs 到高稀疏性的缺失秘密调料") 所示。值得注意的是，除了全局方法外，所有方法在稀疏度为 40% 或以下时表现良好。这表明低稀疏度区域在性能比较中没有显著差异。然而，当稀疏度超过
    50% 时，各种方法之间的差异变得明显。特别是，Uniform 和 OWL 方法成为表现最好的方法，其中 OWL 在所有稀疏度水平下始终优于前者。另一方面，ERK
    系列方法似乎不太适合 LLM 剪枝。值得一提的是，当我们反转 OWL 的异常值比例时，其性能显著下降，突显了 LOD 在识别关键层方面的有效性。
- en: 'Table 5: WikiText validation perplexity of LLaMA-7B with various layerwise
    sparsity using Wanda.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：使用 Wanda 的不同层次稀疏度下的 LLaMA-7B WikiText 验证困惑度。
- en: '| Sparsity/Perplexity | 10% | 20% | 30% | 40% | 50% | 60% | 70% | 80% |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏度/困惑度 | 10% | 20% | 30% | 40% | 50% | 60% | 70% | 80% |'
- en: '| Global | 14.11 | 3134 | 10293 | 10762 | 14848 | 17765 | 5147 | 39918.56 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 全局 | 14.11 | 3134 | 10293 | 10762 | 14848 | 17765 | 5147 | 39918.56 |'
- en: '| ERK-plus | 5.70 | 5.82 | 6.05 | 6.62 | 8.00 | 14.04 | 229.17 | 6013.91 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| ERK-plus | 5.70 | 5.82 | 6.05 | 6.62 | 8.00 | 14.04 | 229.17 | 6013.91 |'
- en: '| ERK | 5.69 | 5.80 | 6.02 | 6.55 | 7.74 | 12.16 | 112.03 | 11151.18 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ERK | 5.69 | 5.80 | 6.02 | 6.55 | 7.74 | 12.16 | 112.03 | 11151.18 |'
- en: '| Uniform | 5.69 | 5.81 | 5.99 | 6.38 | 7.26 | 10.70 | 85.77 | 3499.88 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 5.69 | 5.81 | 5.99 | 6.38 | 7.26 | 10.70 | 85.77 | 3499.88 |'
- en: '| OWL-inverse | 5.72 | 5.83 | 6.04 | 6.51 | 8.03 | 26.05 | 822.23 | 9616.08
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| OWL-inverse | 5.72 | 5.83 | 6.04 | 6.51 | 8.03 | 26.05 | 822.23 | 9616.08
    |'
- en: '| OWL (ours) | 5.70 | 5.80 | 6.01 | 6.39 | 7.22 | 9.35 | 24.54 | 1002.87 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| OWL (我们的) | 5.70 | 5.80 | 6.01 | 6.39 | 7.22 | 9.35 | 24.54 | 1002.87 |'
- en: 5.2 Pruning Efficiency
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 剪枝效率
- en: '|  | LLaMA |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA |'
- en: '| Method | 7B | 13B | 30B | 65B |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 7B | 13B | 30B | 65B |'
- en: '| SparseGPT | 208 | 341 | 731 | 1297 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 208 | 341 | 731 | 1297 |'
- en: '| OWL w. SparseGPT | 208 | 342 | 733 | 1301 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| OWL 与 SparseGPT | 208 | 342 | 733 | 1301 |'
- en: '| Wanda | 0.3 | 0.6 | 1.1 | 1.8 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 0.3 | 0.6 | 1.1 | 1.8 |'
- en: '| OWL w. Wanda | 0.5 | 1.3 | 2.0 | 3.7 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| OWL 与 Wanda | 0.5 | 1.3 | 2.0 | 3.7 |'
- en: '| Model | $\mathbf{M}$ |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | $\mathbf{M}$ |'
- en: '| LLaMA-7B | 5 | 8% |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | 5 | 8% |'
- en: '| LLaMA-13B | 7 | 8% |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-13B | 7 | 8% |'
- en: '| LLaMA-30B | 5 | 8% |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-30B | 5 | 8% |'
- en: '| LLaMA-65B | 5 | 20% |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-65B | 5 | 20% |'
- en: '| OPT-6.7B | 10 | 8% |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7B | 10 | 8% |'
- en: 'Figure 4: Left: Comparison of time overhead (in seconds), excluding the shared
    forward pass process. Right: Hyperparameters used to reproduce the results in
    this paper.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：左：时间开销比较（以秒为单位），不包括共享的前向传播过程。右：用于重现本文结果的超参数。
- en: Since we utilize the pruning metric of Wanda to determine our layerwise sparsity,
    the theoretical computational complexity of OWL is comparable to that of Wanda,
    which is expected to be significantly lower than SparseGPT. To demonstrate this,
    we measure the total pruning time, excluding the forward pass process, following
    the methodology outlined by Sun et al. ([2023](#bib.bib39)). These results were
    obtained using NVIDIA A100 GPUs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们利用 Wanda 的剪枝度量来确定我们的层次稀疏度，OWL 的理论计算复杂度与 Wanda 相当，预计会显著低于 SparseGPT。为了证明这一点，我们测量了总剪枝时间（不包括前向传播过程），按照
    Sun 等人（[2023](#bib.bib39)）的方法进行。这些结果是使用 NVIDIA A100 GPU 获得的。
- en: 'Our results in Table [4](#S4.F4 "Figure 4 ‣ 5.2 Pruning Efficiency ‣ 5 Analysis
    ‣ Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning
    LLMs to High Sparsity") indicate that OWL introduces nearly negligible overhead
    when compared to SparseGPT. Conversely, OWL .w Wanda doubles the pruning time
    in comparison to Wanda alone, yet it efficiently prunes a 65B LLaMA model within
    only 4 seconds. This additional time overhead primarily arises from the computation
    of $\|\mathbf{X}_{\texttt{j}}\|_{2}\cdot|\mathbf{W}_{\texttt{ij}}|$ for the computation
    of Layerwise Outlier Distribution (LOD). However, as Wanda also employs this metric
    for pruning, we believe there is potential for solutions to mitigate this overhead.
    This aspect is left for future work and further optimization.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格[4](#S4.F4 "Figure 4 ‣ 5.2 Pruning Efficiency ‣ 5 Analysis ‣ Outlier Weighed
    Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity")中的结果表明，与SparseGPT相比，OWL引入的开销几乎可以忽略不计。相反，OWL
    .w Wanda将剪枝时间相比单独使用Wanda增加了一倍，但它能在仅4秒内高效剪枝一个65B的LLaMA模型。这额外的时间开销主要来源于计算$\|\mathbf{X}_{\texttt{j}}\|_{2}\cdot|\mathbf{W}_{\texttt{ij}}|$用于计算层级异常值分布（LOD）。然而，由于Wanda也使用了这一度量进行剪枝，我们认为有可能找到解决方案来减轻这一开销。这一方面留待未来工作和进一步优化。'
- en: 6 Conclusion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we focus on a crucial aspect of LLM pruning that have been overlooked
    by previous works – layerwise sparsity ratios. Despite the prevailing practice
    of uniformly pruning all layers at equivalent sparsity levels, as observed in
    prominent LLM pruning papers, our investigation diverges from this trend by drawing
    inspiration from the emergence of outliers, characterized by features exhibiting
    significantly greater magnitudes compared to others. Leveraging this discovery,
    we introduced a novel layerwise sparsity ratio known as Outlier Weighed Layerwise
    sparsity (OWL). OWL employs tailored non-uniform layerwise sparsity ratios designed
    specifically for LLM pruning, aligning sparsity ratios with outlier ratios within
    each layer. Notably, our approach demonstrates substantial performance gains,
    surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity
    points, respectively, at a high sparsity level of 70%. Our findings offer fresh
    insights into the critical significance of layerwise sparsity in the context of
    LLM pruning. This work opens up new avenues for the development of specialized
    sparse algorithms that can further optimize the deployment of LLMs in practical
    applications.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们关注了LLM剪枝中一个被以往研究忽视的关键方面——层级稀疏比。尽管在著名的LLM剪枝论文中普遍实践是以相同的稀疏级别统一剪枝所有层，但我们的研究从这一趋势中脱离，受到出现异常值的启发，这些异常值的特征显著大于其他特征。利用这一发现，我们引入了一种新颖的层级稀疏比，称为异常值加权层级稀疏比（OWL）。OWL采用了专为LLM剪枝设计的非均匀层级稀疏比，将稀疏比与每层的异常值比对齐。值得注意的是，我们的方法展示了显著的性能提升，在高稀疏级别70%下，分别比最先进的Wanda和SparseGPT超越了61.22和6.80困惑度点。我们的发现为LLM剪枝中层级稀疏的重要性提供了新的见解。这项工作为开发专门的稀疏算法打开了新途径，这些算法可以进一步优化LLM在实际应用中的部署。
- en: Limitation. One limitation of this work is the lack of results with hardware-friendly
    sparsity pattern. We will explore the promising N:M sparsity for OWL as one of
    our future directions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 局限性。本工作的一个局限性是缺乏硬件友好的稀疏模式结果。我们将探索有前景的N:M稀疏作为OWL的未来方向之一。
- en: 7 Acknowledgement
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 致谢
- en: Part of this work used the Dutch national e-infrastructure with the support
    of the SURF Cooperative using grant no. NWO2021.060, EINF-2694 and EINF-2943/L1\.
    S. Liu and Z. Wang are in part supported by the NSF AI Institute for Foundations
    of Machine Learning (IFML).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分使用了荷兰国家电子基础设施，得到了SURF合作社的支持，资助号为NWO2021.060、EINF-2694和EINF-2943/L1。S. Liu和Z.
    Wang部分得到NSF机器学习基础研究所（IFML）的支持。
- en: References
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bhojanapalli et al. (2021) Srinadh Bhojanapalli, Ayan Chakrabarti, Andreas Veit,
    Michal Lukasik, Himanshu Jain, Frederick Liu, Yin-Wen Chang, and Sanjiv Kumar.
    Leveraging redundancy in attention with reuse transformers. *arXiv preprint arXiv:2110.06821*,
    2021.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhojanapalli等（2021）Srinadh Bhojanapalli, Ayan Chakrabarti, Andreas Veit, Michal
    Lukasik, Himanshu Jain, Frederick Liu, Yin-Wen Chang, 和Sanjiv Kumar. 利用注意力中的冗余重用变换器。*arXiv预印本
    arXiv:2110.06821*, 2021。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems (NeurIPs)*, 33:1877–1901, 2020.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) 汤姆·布朗、本杰明·曼、尼克·赖德、梅兰妮·苏比亚、贾雷德·D·卡普兰、普拉弗拉·达里瓦尔、阿尔文德·内拉坎坦、普拉纳夫·夏姆、吉里什·萨斯特里、阿曼达·阿斯克尔等。语言模型是少量学习者。*神经信息处理系统进展（NeurIPs）*，33：1877–1901，2020年。
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty
    of natural yes/no questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2019) 克里斯托弗·克拉克、肯顿·李、明伟·张、汤姆·克维亚托夫斯基、迈克尔·柯林斯和克里斯蒂娜·图塔诺娃。BoolQ：探索自然是/否问题的惊人难度。*arXiv
    预印本 arXiv:1905.10044*，2019年。
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2018) 彼得·克拉克、艾萨克·考赫、奥伦·埃齐奥尼、图沙尔·科特、阿希什·萨布哈瓦尔、卡丽莎·肖尼克和奥伊文德·塔福约德。认为你解决了问答问题？试试
    ARC，AI2 推理挑战。*arXiv 预印本 arXiv:1803.05457*，2018年。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *Advances
    in Neural Information Processing Systems (NeurIPs)*, 2022.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2022) 蒂姆·德特梅尔斯、迈克·刘易斯、优尼斯·贝尔卡达和卢克·泽特尔莫耶。Llm. int8 (): 大规模变换器的8位矩阵乘法。*神经信息处理系统进展（NeurIPs）*，2022年。'
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*, 2018.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2018) 雅各布·德夫林、明伟·张、肯顿·李和克里斯蒂娜·图塔诺娃。BERT：深度双向变换器的预训练用于语言理解。*arXiv
    预印本 arXiv:1810.04805*，2018年。
- en: Erdős & Rényi (1959) Paul Erdős and Alfréd Rényi. On random graphs i. *Publicationes
    Mathematicae (Debrecen)*, 6:290–297, 1959.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erdős & Rényi (1959) 保罗·厄尔德什和阿尔弗雷德·雷尼。随机图论 I。*数学出版物（德布勒森）*，6：290–297，1959年。
- en: 'Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro,
    and Erich Elsen. Rigging the lottery: Making all tickets winners. In *International
    Conference on Machine Learning (ICML)*, pp.  2943–2952, 2020.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Evci et al. (2020) 乌特库·埃夫奇、特雷弗·盖尔、雅各布·梅尼克、帕布罗·塞缪尔·卡斯特罗和埃里希·埃尔森。操控彩票：让所有票都成为赢家。在*国际机器学习会议（ICML）*，第
    2943–2952 页，2020年。
- en: 'Frankle & Carbin (2019) Jonathan Frankle and Michael Carbin. The lottery ticket
    hypothesis: Finding sparse, trainable neural networks. In *International Conference
    on Learning Representations (ICLR)*, 2019.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle & Carbin (2019) 乔纳森·弗兰克尔和迈克尔·卡宾。彩票假设：寻找稀疏、可训练的神经网络。在*学习表征国际会议（ICLR）*，2019年。
- en: Frantar & Alistarh (2023) Elias Frantar and Dan Alistarh. Massive language models
    can be accurately pruned in one-shot. In *International Conference on Machine
    Learning (ICML)*, 2023.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar & Alistarh (2023) 埃利亚斯·弗兰塔尔和丹·阿利斯塔赫。大规模语言模型可以一次性准确剪枝。在*国际机器学习会议（ICML）*，2023年。
- en: Gale et al. (2019) Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity
    in deep neural networks. *arXiv preprint arXiv:1902.09574*, 2019.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gale et al. (2019) 特雷弗·盖尔、埃里希·埃尔森和萨拉·胡克。深度神经网络中稀疏性的现状。*arXiv 预印本 arXiv:1902.09574*，2019年。
- en: 'Gale et al. (2020) Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen.
    Sparse gpu kernels for deep learning. In *SC20: International Conference for High
    Performance Computing, Networking, Storage and Analysis*, pp.  1–14\. IEEE, 2020.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gale et al. (2020) 特雷弗·盖尔、马泰伊·扎哈里亚、克利夫·杨和埃里希·埃尔森。深度学习的稀疏 GPU 核心。在*SC20：高性能计算、网络、存储和分析国际会议*，第
    1–14 页。IEEE，2020年。
- en: Han et al. (2015) Song Han, Jeff Pool, John Tran, and William Dally. Learning
    both weights and connections for efficient neural network. In *Advances in Neural
    Information Processing Systems (NeurIPS)*, pp.  1135–1143, 2015.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2015) 宋·韩、杰夫·普尔、约翰·特兰和威廉·达利。学习权重和连接以提高神经网络效率。在*神经信息处理系统进展（NeurIPS）*，第
    1135–1143 页，2015年。
- en: Hassibi et al. (1993) Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal
    brain surgeon and general network pruning. In *IEEE international conference on
    neural networks*, pp.  293–299\. IEEE, 1993.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi et al. (1993) 巴巴克·哈希比、大卫·G·斯托克和格雷戈里·J·沃尔夫。最优脑外科医生和通用网络剪枝。在*IEEE 神经网络国际会议*，第
    293–299 页。IEEE，1993年。
- en: 'Jaiswal et al. (2023) Ajay Jaiswal, Shiwei Liu, Tianlong Chen, and Zhangyang
    Wang. The emergence of essential sparsity in large pre-trained models: The weights
    that matter. *arXiv preprint arXiv:2306.03805*, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaiswal et al. (2023) Ajay Jaiswal, Shiwei Liu, Tianlong Chen, 和 Zhangyang Wang.
    大型预训练模型中本质稀疏性的出现：重要的权重。*arXiv 预印本 arXiv:2306.03805*，2023。
- en: Janowsky (1989) Steven A Janowsky. Pruning versus clipping in neural networks.
    *Physical Review A*, 39(12):6600, 1989.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Janowsky (1989) Steven A Janowsky. 神经网络中的剪枝与裁剪。*物理评论 A*，39(12)：6600，1989。
- en: 'Kurtic et al. (2022) Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar,
    Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal bert
    surgeon: Scalable and accurate second-order pruning for large language models.
    *arXiv preprint arXiv:2203.07259*, 2022.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurtic et al. (2022) Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar,
    Mark Kurtz, Benjamin Fineran, Michael Goin, 和 Dan Alistarh. 最优 BERT 外科医生：可扩展且准确的二阶剪枝用于大型语言模型。*arXiv
    预印本 arXiv:2203.07259*，2022。
- en: LeCun et al. (1989) Yann LeCun, John Denker, and Sara Solla. Optimal brain damage.
    In *Advances in Neural Information Processing Systems (NeurIPS)*, pp.  598–605,
    1989.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1989) Yann LeCun, John Denker, 和 Sara Solla. 最优脑损伤。在 *神经信息处理系统进展
    (NeurIPS)*，第 598–605 页，1989。
- en: Lee et al. (2020) Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo
    Shin. Layer-adaptive sparsity for the magnitude-based pruning. *arXiv preprint
    arXiv:2010.07611*, 2020.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2020) Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, 和 Jinwoo Shin.
    基于幅度的剪枝的层自适应稀疏性。*arXiv 预印本 arXiv:2010.07611*，2020。
- en: 'Lee et al. (2019) Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. Snip:
    Single-shot network pruning based on connection sensitivity. In *International
    Conference on Learning Representations (ICLR)*, 2019.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2019) Namhoon Lee, Thalaiyasingam Ajanthan, 和 Philip Torr. SNIP：基于连接敏感性的单次网络剪枝。在
    *国际学习表征会议 (ICLR)*，2019。
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. AWQ：激活感知权重量化用于 LLM 压缩与加速。*arXiv 预印本 arXiv:2306.00978*，2023。
- en: Lin et al. (2019) Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan
    Cao, Qixiang Ye, Feiyue Huang, and David Doermann. Towards optimal structured
    cnn pruning via generative adversarial learning. In *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*, pp.  2790–2799, 2019.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2019) Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan
    Cao, Qixiang Ye, Feiyue Huang, 和 David Doermann. 通过生成对抗学习实现最优结构化 CNN 剪枝。在 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，第 2790–2799 页，2019。
- en: Liu et al. (2021) Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi,
    Lu Yin, Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, and Decebal Constantin
    Mocanu. Sparse training via boosting pruning plasticity with neuroregeneration.
    In *Advances in Neural Information Processing Systems (NeurIPS)*, 2021.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021) Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi,
    Lu Yin, Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, 和 Decebal Constantin
    Mocanu. 通过增强剪枝可塑性与神经再生进行稀疏训练。在 *神经信息处理系统进展 (NeurIPS)*，2021。
- en: 'Liu et al. (2022) Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin
    Mocanu, Zhangyang Wang, and Mykola Pechenizkiy. The unreasonable effectiveness
    of random pruning: Return of the most naive baseline for sparse training. *arXiv
    preprint arXiv:2202.02643*, 2022.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022) Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal
    Constantin Mocanu, Zhangyang Wang, 和 Mykola Pechenizkiy. 随机剪枝的非理性有效性：最简单基线的回归。*arXiv
    预印本 arXiv:2202.02643*，2022。
- en: Luccioni et al. (2022) Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure
    Ligozat. Estimating the carbon footprint of bloom, a 176b parameter language model.
    *arXiv preprint arXiv:2211.02001*, 2022.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luccioni et al. (2022) Alexandra Sasha Luccioni, Sylvain Viguier, 和 Anne-Laure
    Ligozat. 估计 bloom 的碳足迹，一个 176b 参数的语言模型。*arXiv 预印本 arXiv:2211.02001*，2022。
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. *arXiv preprint arXiv:2305.11627*,
    2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2023) Xinyin Ma, Gongfan Fang, 和 Xinchao Wang. Llm-pruner：关于大型语言模型的结构化剪枝。*arXiv
    预印本 arXiv:2305.11627*，2023。
- en: Merity et al. (2016a) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016a.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. (2016a) Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher. 指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843*，2016a。
- en: Merity et al. (2016b) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016b.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. (2016b) Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher. 指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843*，2016b。
- en: Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. *arXiv preprint arXiv:1809.02789*, 2018.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov 等（2018）**Todor Mihaylov**、**Peter Clark**、**Tushar Khot** 和 **Ashish
    Sabharwal**。一套盔甲能导电吗？用于开放书籍问答的新数据集。*arXiv 预印本 arXiv:1809.02789*，2018。
- en: Mittal et al. (2019) Deepak Mittal, Shweta Bhardwaj, Mitesh M Khapra, and Balaraman
    Ravindran. Studying the plasticity in deep convolutional neural networks using
    random pruning. *Machine Vision and Applications*, 30(2):203–216, 2019.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mittal 等（2019）**Deepak Mittal**、**Shweta Bhardwaj**、**Mitesh M Khapra** 和 **Balaraman
    Ravindran**。通过随机剪枝研究深度卷积神经网络的可塑性。*Machine Vision and Applications*，30(2):203–216，2019。
- en: 'Mocanu et al. (2016) Decebal Constantin Mocanu, Elena Mocanu, Phuong H. Nguyen,
    Madeleine Gibescu, and Antonio Liotta. A topological insight into restricted boltzmann
    machines. *Machine Learning*, 104(2):243–270, Sep 2016. ISSN 1573-0565. doi: 10.1007/s10994-016-5570-z.
    URL [https://doi.org/10.1007/s10994-016-5570-z](https://doi.org/10.1007/s10994-016-5570-z).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mocanu 等（2016）**Decebal Constantin Mocanu**、**Elena Mocanu**、**Phuong H. Nguyen**、**Madeleine
    Gibescu** 和 **Antonio Liotta**。对限制玻尔兹曼机的拓扑洞察。*Machine Learning*，104(2):243–270，2016年9月。ISSN
    1573-0565。doi: 10.1007/s10994-016-5570-z。网址 [https://doi.org/10.1007/s10994-016-5570-z](https://doi.org/10.1007/s10994-016-5570-z)。'
- en: Mocanu et al. (2018) Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H
    Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial
    neural networks with adaptive sparse connectivity inspired by network science.
    *Nature Communications*, 9:1–12, 2018.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mocanu 等（2018）**Decebal Constantin Mocanu**、**Elena Mocanu**、**Peter Stone**、**Phuong
    H Nguyen**、**Madeleine Gibescu** 和 **Antonio Liotta**。借鉴网络科学的自适应稀疏连接的可扩展人工神经网络训练。*Nature
    Communications*，9:1–12，2018。
- en: 'Mozer & Smolensky (1989) Michael C Mozer and Paul Smolensky. Skeletonization:
    A technique for trimming the fat from a network via relevance assessment. In *Advances
    in Neural Information Processing Systems (NeurIPS)*, pp.  107–115, 1989.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mozer & Smolensky（1989）**Michael C Mozer** 和 **Paul Smolensky**。骨架化：通过相关性评估剪裁网络的技术。在
    *Advances in Neural Information Processing Systems (NeurIPS)*，第107–115页，1989年。
- en: Patterson et al. (2021) David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang,
    Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean.
    Carbon emissions and large neural network training. *arXiv preprint arXiv:2104.10350*,
    2021.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patterson 等（2021）**David Patterson**、**Joseph Gonzalez**、**Quoc Le**、**Chen
    Liang**、**Lluis-Miquel Munguia**、**Daniel Rothchild**、**David So**、**Maud Texier**
    和 **Jeff Dean**。碳排放与大型神经网络训练。*arXiv 预印本 arXiv:2104.10350*，2021。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等（2020）**Colin Raffel**、**Noam Shazeer**、**Adam Roberts**、**Katherine
    Lee**、**Sharan Narang**、**Michael Matena**、**Yanqi Zhou**、**Wei Li** 和 **Peter
    J Liu**。利用统一的文本到文本转换器探索迁移学习的极限。*The Journal of Machine Learning Research*，21(1):5485–5551，2020。
- en: 'Sakaguchi et al. (2019) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *arXiv preprint arXiv:1907.10641*, 2019.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi 等（2019）**Keisuke Sakaguchi**、**Ronan Le Bras**、**Chandra Bhagavatula**
    和 **Yejin Choi**。Winogrande：大规模对抗性 Winograd 语法挑战。*arXiv 预印本 arXiv:1907.10641*，2019。
- en: 'Sanh et al. (2020) Victor Sanh, Thomas Wolf, and Alexander M Rush. Movement
    pruning: Adaptive sparsity by fine-tuning. *arXiv preprint arXiv:2005.07683*,
    2020.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等（2020）**Victor Sanh**、**Thomas Wolf** 和 **Alexander M Rush**。运动剪枝：通过微调实现自适应稀疏性。*arXiv
    预印本 arXiv:2005.07683*，2020。
- en: Schaeffer et al. (2023) Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are
    emergent abilities of large language models a mirage? *arXiv preprint arXiv:2304.15004*,
    2023.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schaeffer 等（2023）**Rylan Schaeffer**、**Brando Miranda** 和 **Sanmi Koyejo**。大型语言模型的突现能力是海市蜃楼吗？*arXiv
    预印本 arXiv:2304.15004*，2023。
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple
    and effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2023）**Mingjie Sun**、**Zhuang Liu**、**Anna Bair** 和 **J Zico Kolter**。一种简单而有效的大型语言模型剪枝方法。*arXiv
    预印本 arXiv:2306.11695*，2023。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023a）**Hugo Touvron**、**Thibaut Lavril**、**Gautier Izacard**、**Xavier
    Martinet**、**Marie-Anne Lachaux**、**Timothée Lacroix**、**Baptiste Rozière**、**Naman
    Goyal**、**Eric Hambro**、**Faisal Azhar** 等。Llama: 开放和高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023a。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等. Llama 2：开放基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023b年。
- en: 'Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding. *arXiv preprint arXiv:1804.07461*, 2018.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer
    Levy, 和 Samuel R. Bowman. Glue：自然语言理解的多任务基准和分析平台。*arXiv 预印本 arXiv:1804.07461*，2018年。
- en: Wang et al. (2020) Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning
    tickets before training by preserving gradient flow. In *International Conference
    on Learning Representations (ICLR)*, 2020.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020) Chaoqi Wang, Guodong Zhang, 和 Roger Grosse. 在训练前通过保持梯度流挑选获胜的票据。在*国际学习表征会议
    (ICLR)*，2020年。
- en: Wang & Tu (2020) Wenxuan Wang and Zhaopeng Tu. Rethinking the value of transformer
    components. *arXiv preprint arXiv:2011.03803*, 2020.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang & Tu (2020) Wenxuan Wang 和 Zhaopeng Tu. 重新思考变换器组件的价值。*arXiv 预印本 arXiv:2011.03803*，2020年。
- en: Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. Emergent abilities of large language models. *Transactions on Machine Learning
    Research*, 2022.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler 等.
    大型语言模型的涌现能力。*机器学习研究事务*，2022年。
- en: Wen et al. (2017) Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan
    Wang, Fang Liu, Bin Hu, Yiran Chen, and Hai Li. Learning intrinsic sparse structures
    within long short-term memory. *arXiv preprint arXiv:1709.05027*, 2017.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen et al. (2017) Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan
    Wang, Fang Liu, Bin Hu, Yiran Chen, 和 Hai Li. 学习长短期记忆中的内在稀疏结构。*arXiv 预印本 arXiv:1709.05027*，2017年。
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *International Conference on Machine Learning (ICML)*,
    pp.  38087–38099\. PMLR, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    和 Song Han. Smoothquant：大型语言模型的准确和高效的后训练量化。在*国际机器学习会议 (ICML)*，第 38087–38099 页。PMLR，2023年。
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    和 Yejin Choi. Hellaswag：机器真的能完成你的句子吗？*arXiv 预印本 arXiv:1905.07830*，2019年。
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin
    等. Opt：开放预训练的变换器语言模型。*arXiv 预印本 arXiv:2205.01068*，2022年。
- en: 'Zhu & Gupta (2017) Michael Zhu and Suyog Gupta. To prune, or not to prune:
    exploring the efficacy of pruning for model compression. In *International Conference
    on Learning Representations Workshop (ICLRW)*, 2017.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu & Gupta (2017) Michael Zhu 和 Suyog Gupta. 修剪还是不修剪：探索修剪在模型压缩中的效果。在*国际学习表征会议研讨会
    (ICLRW)*，2017年。
