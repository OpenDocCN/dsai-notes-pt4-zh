- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 19:04:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:04:32'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with
    LLMs through Data Pruning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'FactFinders at CheckThat! 2024: 通过数据修剪优化值得检查的声明检测'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.18297](https://ar5iv.labs.arxiv.org/html/2406.18297)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.18297](https://ar5iv.labs.arxiv.org/html/2406.18297)
- en: \copyrightclause
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \copyrightclause
- en: Copyright for this paper by its authors. Use permitted under Creative Commons
    License Attribution 4.0 International (CC BY 4.0).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文版权归作者所有。根据创作共用许可证署名 4.0 国际 (CC BY 4.0) 允许使用。
- en: \conference
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \conference
- en: 'CLEF 2024: Conference and Labs of the Evaluation Forum, September 09–12, 2024,
    Grenoble, France'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'CLEF 2024: 会议和评估论坛实验室，2024年9月09–12日，法国格勒诺布尔'
- en: '[orcid=0009-0008-8740-4994, email=yufeng.li@qmul.ac.uk ] \cormark[1] \fnmark[1]'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[orcid=0009-0008-8740-4994, email=yufeng.li@qmul.ac.uk ] \cormark[1] \fnmark[1]'
- en: '[orcid=0000-0002-1403-2236, email=r.panchendrarajan@qmul.ac.uk ] \cormark[1]
    \fnmark[1]'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[orcid=0000-0002-1403-2236, email=r.panchendrarajan@qmul.ac.uk ] \cormark[1]
    \fnmark[1]'
- en: '[orcid=0000-0003-4583-3623, email=a.zubiaga@qmul.ac.uk, url=www.zubiaga.org,
    ]'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[orcid=0000-0003-4583-3623, email=a.zubiaga@qmul.ac.uk, url=www.zubiaga.org,
    ]'
- en: \cortext
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \cortext
- en: '[1]Corresponding author. \fntext[1]These authors contributed equally.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]通讯作者。 \fntext[1]这些作者贡献相等。'
- en: Yufeng Li School of Electronic Engineering and Computer Science, Queen Mary
    University of London    Rrubaa Panchendrarajan    Arkaitz Zubiaga(2024)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Yufeng Li 伦敦大学皇后玛丽学院电子工程与计算机科学学院    Rrubaa Panchendrarajan    Arkaitz Zubiaga(2024)
- en: Notebook for the CheckThat! Lab at CLEF 2024
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLEF 2024 CheckThat! 实验室笔记本
- en: Yufeng Li School of Electronic Engineering and Computer Science, Queen Mary
    University of London    Rrubaa Panchendrarajan    Arkaitz Zubiaga(2024)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Yufeng Li 伦敦大学皇后玛丽学院电子工程与计算机科学学院    Rrubaa Panchendrarajan    Arkaitz Zubiaga(2024)
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The rapid dissemination of information through social media and the Internet
    has posed a significant challenge for fact-checking, among others in identifying
    check-worthy claims that fact-checkers should pay attention to, i.e. filtering
    claims needing fact-checking from a large pool of sentences. This challenge has
    stressed the need to focus on determining the priority of claims, specifically
    which claims are worth to be fact-checked. Despite advancements in this area in
    recent years, the application of large language models (LLMs), such as GPT, has
    only recently drawn attention in studies. However, many open-source LLMs remain
    underexplored. Therefore, this study investigates the application of eight prominent
    open-source LLMs with fine-tuning and prompt engineering to identify check-worthy
    statements from political transcriptions. Further, we propose a two-step data
    pruning approach to automatically identify high-quality training data instances
    for effective learning. The efficiency of our approach is demonstrated through
    evaluations on the English language dataset as part of the check-worthiness estimation
    task of CheckThat! 2024\. Further, the experiments conducted with data pruning
    demonstrate that competitive performance can be achieved with only about 44% of
    the training data. Our team ranked first in the check-worthiness estimation task
    in the English language.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 信息通过社交媒体和互联网的快速传播对事实核查提出了重大挑战，特别是在识别值得检查的声明方面，即从大量句子中筛选出需要事实核查的声明。这一挑战突出了确定声明优先级的必要性，特别是哪些声明值得进行事实核查。尽管近年来在这一领域取得了进展，但大型语言模型（LLMs）的应用，如GPT，直到最近才引起了研究关注。然而，许多开源LLM仍然未得到充分探索。因此，本研究探讨了对八种主要开源LLM进行微调和提示工程，以从政治转录文本中识别值得检查的声明。此外，我们提出了一种两步数据修剪方法，以自动识别高质量训练数据实例，以实现有效学习。我们的方法通过在CheckThat!
    2024的英语数据集上进行评估来展示其效率。此外，数据修剪实验表明，仅使用约44%的训练数据也可以取得竞争性能。我们的团队在英语语言的值得检查性估计任务中排名第一。
- en: 'keywords:'
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '关键词:'
- en: Check-worthiness \sepClaim detection \sepFact-checking \sepLanguage Models \sepLLM
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 值得检查性 \sep 声明检测 \sep 事实核查 \sep 语言模型 \sep LLM
- en: 1 Introduction
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: With the significant development of the Internet and social media over the past
    decades, the practical challenges associated with fact-checking have become more
    complex [[1](#bib.bib1), [2](#bib.bib2)]. Social media platforms have facilitated
    the rapid dissemination of information, which increases the difficulty of distinguishing
    misinformation from accurate information [[3](#bib.bib3)]. Concurrently, the general
    agreement on what should be fact-checked has expanded to include online content
    and claims made by politicians, resulting in a wide range of claims to be verified.
    As the initial step in the fact-checking process, claim detection plays a crucial
    role in efficiently identifying check-worthy claims, allowing for quicker progression
    to subsequent stages of verification [[4](#bib.bib4)]. Therefore, research on
    check-worthy claim detection is essential for advancing the field of fact-checking,
    where the CheckThat! shared task has played a significant role in recent years
    [[5](#bib.bib5)].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 随着过去几十年互联网和社交媒体的显著发展，与事实检查相关的实际挑战变得更加复杂[[1](#bib.bib1)、[2](#bib.bib2)]。社交媒体平台促进了信息的快速传播，这增加了区分虚假信息与准确信息的难度[[3](#bib.bib3)]。与此同时，对应该进行事实检查的内容的一般共识已扩大到包括在线内容和政治家所做的声明，导致需要验证的声明范围广泛。作为事实检查过程的初始步骤，声明检测在有效识别值得检查的声明中扮演了关键角色，从而允许更快地推进到后续验证阶段[[4](#bib.bib4)]。因此，对值得检查的声明检测的研究对于推进事实检查领域至关重要，而CheckThat!共享任务在近年来发挥了重要作用[[5](#bib.bib5)]。
- en: Since the beginning of the CheckThat! competition, traditional machine learning
    models and neural network models have been commonly employed for the task of claim
    check-worthiness detection. At CheckThat! 2018, the top submission was achieved
    by a team using Support Vector Machines and Multilayer Perceptrons [[6](#bib.bib6)],
    while the highest scores at CheckThat! 2019 were obtained using Long Short-Term
    Memory (LSTM) networks [[7](#bib.bib7)]. Although BERT [[8](#bib.bib8)] was introduced
    in 2018, the exploration of this transformer-based model for claim check-worthiness
    detection began only in 2020\. In that year, the team utilizing RoBERTa secured
    the first position in the English category [[9](#bib.bib9)].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 自CheckThat!比赛开始以来，传统的机器学习模型和神经网络模型已广泛应用于声明检查的价值检测。在CheckThat! 2018中，排名第一的提交由一个使用支持向量机和多层感知器的团队完成[[6](#bib.bib6)]，而CheckThat!
    2019中最高的分数是通过使用长短期记忆（LSTM）网络获得的[[7](#bib.bib7)]。尽管BERT [[8](#bib.bib8)]在2018年被引入，但对这种基于变换器的模型在声明检查价值检测中的探索仅开始于2020年。在那一年，使用RoBERTa的团队在英语类别中获得了第一名[[9](#bib.bib9)]。
- en: Beyond the application of machine learning models, various techniques have been
    explored throughout the CheckThat! competition. Feature representation methods,
    including word embeddings, Bag of Words, Named Entity Recognition, and Part of
    Speech tagging [[6](#bib.bib6), [7](#bib.bib7), [9](#bib.bib9)] have been widely
    used to enhance model understanding of the task. More sophisticated representation
    techniques, such as LIWC [[10](#bib.bib10)] and ELMo [[11](#bib.bib11)], have
    also been investigated. Additionally, statistics related to word usage, such as
    subjectivity and sentiment, have been incorporated. To address the challenge of
    imbalanced datasets, data augmentation strategies have been explored, with common
    methods including machine translation and sampling [[4](#bib.bib4)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除了机器学习模型的应用，CheckThat!比赛中还探索了各种技术。特征表示方法，包括词嵌入、词袋模型、命名实体识别和词性标注[[6](#bib.bib6)、[7](#bib.bib7)、[9](#bib.bib9)]，已广泛用于增强模型对任务的理解。更复杂的表示技术，如LIWC
    [[10](#bib.bib10)]和ELMo [[11](#bib.bib11)]，也被研究过。此外，还融入了与词汇使用相关的统计数据，如主观性和情感分析。为了应对数据集不平衡的问题，探索了数据增强策略，包括机器翻译和抽样[[4](#bib.bib4)]。
- en: Large language models (LLMs) have seen remarkable advancements in recent years,
    with GPT [[12](#bib.bib12)] models predominantly utilized as the latest and most
    effective solution in CheckThat! competitions. Several teams have shown competitive
    and winning performance of the model in various CheckThat! tasks, including check-worthiness
    estimation in multiple languages [[13](#bib.bib13), [14](#bib.bib14)]. Although
    GPT models have demonstrated competitive performance in CheckThat! tasks, their
    fine-tuning and inference entail associated costs. Simultaneously, numerous open-source
    LLMs have also demonstrated substantial advancements showing equivalent performance
    to GPT models. This enables the global community of researchers to benefit by
    transferring the knowledge of these powerful models cost-effectively by fine-tuning
    them on various downstream tasks. While fine-tuned BERT-based and GPT models have
    been extensively and routinely examined in the domain of check-worthiness estimation
    [[4](#bib.bib4)], open-source LLMs, as emerging language models, have not yet
    been thoroughly investigated within this specific field. Therefore, this study
    aims to explore a wide range of open-source LLMs while leveraging their capabilities
    through prompt engineering for check-worthiness estimation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）近年来取得了显著的进展，其中 GPT [[12](#bib.bib12)] 模型被广泛应用于 CheckThat! 比赛，成为最新且最有效的解决方案。多个团队在各种
    CheckThat! 任务中展示了该模型的竞争力和获胜表现，包括多语言的可检查性估计 [[13](#bib.bib13), [14](#bib.bib14)]。尽管
    GPT 模型在 CheckThat! 任务中表现出竞争力，其微调和推断仍涉及相关成本。同时，众多开源 LLM 也展示了显著的进步，表现出与 GPT 模型相当的性能。这使全球研究社区能够通过在各种下游任务上进行微调，以具有成本效益的方式转移这些强大模型的知识。虽然微调的
    BERT 基础和 GPT 模型在可检查性估计领域已被广泛且常规地研究 [[4](#bib.bib4)]，但作为新兴语言模型的开源 LLM 在这一特定领域尚未得到彻底调查。因此，本研究旨在探索广泛的开源
    LLM，并通过提示工程充分发挥它们在可检查性估计中的能力。
- en: This paper presents the experiments conducted for CheckThat! 2024 task 1 [[15](#bib.bib15),
    [16](#bib.bib16)], check-worthiness estimation in the English language. The task
    involves identifying check-worthy statements from political transcriptions. Drawing
    inspiration from the impressive performance of LLMs in the recent CheckThat! competitions,
    we explore eight popular open-source LLMs, specifically Llama2-7b, Llama2-13b
    [[17](#bib.bib17)], Llama3-8b, Mistral [[18](#bib.bib18)], Mixtral [[19](#bib.bib19)],
    Phi3-Mini-4K [[20](#bib.bib20)], Falcon [[21](#bib.bib21)], and Gemma-7b [[22](#bib.bib22)]
    with prompt engineering for identifying check-worthy statements. Considering the
    noisy and imbalanced nature of the training data, we propose a two-step data pruning
    process to isolate high-quality training data instances for effective learning
    with LLMs. Especially, we identify the informative sentences first and apply an
    under-sampling technique, Condensed Nearest Neighbour [[23](#bib.bib23)], to create
    a balanced training dataset. Our fine-tuned Llama2-7b [[17](#bib.bib17)] model
    on the original training data shared by the task organizers scored the highest
    F1-score in the task 1 leaderboard in the English language. However, the experimental
    results indicate that similar or better performance can be achieved with data
    pruning techniques while retaining only about 44% of high-quality data instances
    from the original training data. Furthermore, this approach resulted in a reduction
    in fine-tuning time by a similar proportion, which could significantly lower the
    resource demands for fine-tuning larger models. All relevant source code and data
    are available on GitHub,¹¹1[https://github.com/isyufeng/FactFinders](https://github.com/isyufeng/FactFinders)
    and the fine-tuned model can be accessed on Huggingface.²²2[https://huggingface.co/Rrubaa/factFinders-checkworthy-estimation](https://huggingface.co/Rrubaa/factFinders-checkworthy-estimation)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了针对CheckThat! 2024任务1[[15](#bib.bib15), [16](#bib.bib16)]进行的实验，任务涉及英语中的检查价值估计。该任务涉及从政治转录文本中识别检查价值的陈述。受到LLMs在近期CheckThat!竞赛中卓越表现的启发，我们探讨了八种流行的开源LLMs，特别是Llama2-7b、Llama2-13b[[17](#bib.bib17)]、Llama3-8b、Mistral[[18](#bib.bib18)]、Mixtral[[19](#bib.bib19)]、Phi3-Mini-4K[[20](#bib.bib20)]、Falcon[[21](#bib.bib21)]和Gemma-7b[[22](#bib.bib22)]，并通过提示工程来识别检查价值的陈述。考虑到训练数据的噪声和不平衡性质，我们提出了一个两步数据修剪过程，以隔离高质量的训练数据实例以有效学习LLMs。特别地，我们首先识别信息丰富的句子，并应用欠采样技术Condensed
    Nearest Neighbour[[23](#bib.bib23)]，以创建一个平衡的训练数据集。我们在任务组织者共享的原始训练数据上微调的Llama2-7b[[17](#bib.bib17)]模型在英语任务1排行榜中获得了最高的F1分数。然而，实验结果表明，采用数据修剪技术可以在保留原始训练数据中约44%高质量数据实例的情况下实现类似或更好的性能。此外，这种方法在微调时间上也减少了类似的比例，这可能显著降低了微调更大模型的资源需求。所有相关的源代码和数据都可以在GitHub上找到¹¹1[https://github.com/isyufeng/FactFinders](https://github.com/isyufeng/FactFinders)，微调后的模型可以在Huggingface上访问²²2[https://huggingface.co/Rrubaa/factFinders-checkworthy-estimation](https://huggingface.co/Rrubaa/factFinders-checkworthy-estimation)
- en: 'The remainder of the paper is structured as follows. Section [2](#S2 "2 Methodology
    ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with
    LLMs through Data Pruning") presents the methodology with the introduction to
    the LLMs experimented, prompts used and the data pruning techniques proposed.
    Section [3](#S3 "3 Results ‣ FactFinders at CheckThat! 2024: Refining Check-worthy
    Statement Detection with LLMs through Data Pruning") discusses the experiment
    results, followed by section [4](#S4 "4 Conclusion ‣ FactFinders at CheckThat!
    2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning")
    concluding the key findings and future directions.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '论文的其余部分结构如下。第[2](#S2 "2 Methodology ‣ FactFinders at CheckThat! 2024: Refining
    Check-worthy Statement Detection with LLMs through Data Pruning")节介绍了所实验的LLMs、使用的提示和提出的数据修剪技术。第[3](#S3
    "3 Results ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection
    with LLMs through Data Pruning")节讨论了实验结果，第[4](#S4 "4 Conclusion ‣ FactFinders
    at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through
    Data Pruning")节总结了主要发现和未来的研究方向。'
- en: 2 Methodology
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: Our goal was to automatically refine the training data to obtain high-quality
    training data instances and fine-tune open-source Large Language Models (LLMs)
    to identify check-worthy statements from political transcriptions. This section
    introduces the dataset, LLMs used in the experiments, prompt engineering carried
    out, fine-tuning process, and the two-step data pruning we applied to the training
    data for effective learning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是自动优化训练数据，以获得高质量的训练数据实例，并对开源的大型语言模型（LLMs）进行微调，以从政治转录中识别值得检查的陈述。本节介绍了数据集、实验中使用的LLMs、进行的提示工程、微调过程以及我们为有效学习应用于训练数据的两步数据修剪。
- en: 2.1 Dataset
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 数据集
- en: 'The dataset provided by CheckThat! 2024 comprises the train, dev, and dev-test
    partitions, containing 23,849 sentences from political transcriptions, along with
    a later release of test partition, bringing the total to 24,163 sentences. Table
    LABEL:tab:data_statisitcs presents the statistics for each partition. From this
    table, it is evident that the dataset is imbalanced, posing a challenge for the
    check-worthy statement detection task. Furthermore, Figure [1](#S2.F1 "Figure
    1 ‣ 2.1 Dataset ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining Check-worthy
    Statement Detection with LLMs through Data Pruning") illustrates the distribution
    of text lengths across each partition, revealing that the median length of the
    sentences is approximately 10-14 words in each partition, with 28%-42% of sentences
    containing fewer than 10 words. This indicates that the dataset not only suffers
    from class imbalance but also contains predominantly short sentences, hence implying
    a limited amount of information.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'CheckThat! 2024 提供的数据集包括训练集、开发集和开发-测试集，包含了来自政治转录的23,849个句子，以及随后发布的测试集，使总句子数达到24,163个。表格
    LABEL:tab:data_statisitcs 展示了各个分区的统计信息。从该表中可以明显看出，该数据集存在不平衡问题，这对值得检查的陈述检测任务提出了挑战。此外，图
    [1](#S2.F1 "Figure 1 ‣ 2.1 Dataset ‣ 2 Methodology ‣ FactFinders at CheckThat!
    2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning")
    说明了每个分区中文本长度的分布情况，揭示了每个分区的句子中位长度大约为10-14个单词，28%-42%的句子包含少于10个单词。这表明数据集不仅存在类别不平衡的问题，而且主要包含短句，从而意味着信息量有限。'
- en: 'Table 1: Statistics of the Dataset'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 1: 数据集统计信息'
- en: '| Partition | Check-worthy | Non-check-worthy | Total |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 分区 | 值得检查 | 不值得检查 | 总计 |'
- en: '| Train | 5,413 | 17,086 | 22,499 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 训练集 | 5,413 | 17,086 | 22,499 |'
- en: '| Dev | 238 | 794 | 1,032 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 开发集 | 238 | 794 | 1,032 |'
- en: '| Dev-Test | 108 | 210 | 318 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 开发-测试集 | 108 | 210 | 318 |'
- en: '| Test | 88 | 253 | 341 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 测试集 | 88 | 253 | 341 |'
- en: '![Refer to caption](img/1c83f3ce09e8591fb1402e1af41108cb.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1c83f3ce09e8591fb1402e1af41108cb.png)'
- en: 'Figure 1: Distribution of Text Length in each Partition'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 各分区中文本长度的分布'
- en: 2.2 LLMs for Check-worthy Statement Detection
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLMs用于值得检查的陈述检测
- en: Open-source LLMs offer substantial advantages in terms of cost, transparency,
    community support, and ethical considerations. Consequently, we investigated eight
    prominent open-source LLMs, as detailed in Table LABEL:tab:model_info by fine-tuning
    them for check-worthy statement detection.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 开源的LLMs在成本、透明度、社区支持和伦理考虑方面提供了显著的优势。因此，我们研究了八种主要的开源LLMs，如表格 LABEL:tab:model_info
    所示，通过对它们进行微调来检测值得检查的陈述。
- en: 2.2.1 Large Language Models
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 大型语言模型
- en: Llama2-7b, Llama2-13b [[17](#bib.bib17)], and Llama3-8b are part of the Llama
    family, developed by Meta, and are available in various sizes. The most recent
    release, Llama3-8b, was made available in April 2024\. These models have been
    optimized for text generation and dialogue applications. Similarly, Mistral [[18](#bib.bib18)]
    and Mixtral [[19](#bib.bib19)] are both developed by Mistral AI. Mistral mainly
    focuses on optimizing transformer models for language tasks, achieving high efficiency
    and performance in a compact form. Mixtral, with its hybrid approach, aims to
    integrate the best of various AI methodologies, offering flexibility and scalability
    for complex applications. Compared to this bigger model, Phi3-Mini-4K [[20](#bib.bib20)]
    is a smaller variant of the Phi-3-mini, developed by Microsoft, designed to provide
    capabilities similar to its larger counterparts but with a reduced number of parameters,
    making it more accessible and easier to run on less powerful hardware. Similarly,
    Falcon [[21](#bib.bib21)], developed by TII, stands as one of the most powerful
    open-source models and consistently achieves top positions on the OpenLLM leaderboard
    hosted on Hugging Face. One of the latest models we experimented with, Gemma-7b
    [[22](#bib.bib22)] belongs to the Gemma family developed by Google DeepMind, which
    is designed to offer a balance between computational efficiency and advanced capabilities
    in generating text and understanding complex language queries. We fine-tuned these
    eight open-source LLMs published in Huggingface platforms (links listed in Table
    LABEL:tab:model_info) for check-worthy statement detection from political transcriptions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Llama2-7b、Llama2-13b [[17](#bib.bib17)] 和 Llama3-8b 是 Meta 开发的 Llama 系列的一部分，提供了不同的规模。最新发布的
    Llama3-8b 于 2024年4月发布。这些模型经过优化，适用于文本生成和对话应用。同样，Mistral [[18](#bib.bib18)] 和 Mixtral
    [[19](#bib.bib19)] 由 Mistral AI 开发。Mistral 主要专注于优化变换器模型以处理语言任务，在紧凑的形式中实现了高效率和性能。Mixtral
    采用混合方法，旨在整合各种 AI 方法的优势，为复杂应用提供灵活性和可扩展性。与这些更大的模型相比，Phi3-Mini-4K [[20](#bib.bib20)]
    是 Microsoft 开发的 Phi-3-mini 的一个较小变体，旨在提供类似于其较大对手的功能，但参数数量减少，使其更易于在性能较低的硬件上运行。同样，Falcon
    [[21](#bib.bib21)] 由 TII 开发，是最强大的开源模型之一，并在 Hugging Face 主办的 OpenLLM 排行榜上 consistently
    达到顶级位置。我们实验的最新模型之一，Gemma-7b [[22](#bib.bib22)] 属于 Google DeepMind 开发的 Gemma 系列，旨在在生成文本和理解复杂语言查询方面提供计算效率和先进能力的平衡。我们对这些在
    Huggingface 平台上发布的八个开源 LLM 进行了微调（链接见表 LABEL:tab:model_info），用于从政治转录中检测值得检查的陈述。
- en: 2.2.2 Prompt Engineering
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 提示工程
- en: 'Table 2: Open-Source LLMs Used.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：使用的开源LLM。
- en: '| Model | Number of Parameters | Release Date |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数数量 | 发布日期 |'
- en: '| Llama2-7b³³3[https://huggingface.co/meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)
    | 7 billion | July 2023 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b³³3[https://huggingface.co/meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)
    | 70亿 | 2023年7月 |'
- en: '| Llama2-13b⁴⁴4[https://huggingface.co/meta-llama/Llama-2-13b-hf](https://huggingface.co/meta-llama/Llama-2-13b-hf)
    | 13 billion | July 2023 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b⁴⁴4[https://huggingface.co/meta-llama/Llama-2-13b-hf](https://huggingface.co/meta-llama/Llama-2-13b-hf)
    | 130亿 | 2023年7月 |'
- en: '| Llama3-8b⁵⁵5[https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
    | 8 billion | April 2024 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8b⁵⁵5[https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
    | 80亿 | 2024年4月 |'
- en: '| Mistral⁶⁶6[https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)
    | 7 billion | September 2023 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Mistral⁶⁶6[https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)
    | 70亿 | 2023年9月 |'
- en: '| Mixtral⁷⁷7[https://huggingface.co/mistralai/Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)
    | 45 billion | December 2023 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral⁷⁷7[https://huggingface.co/mistralai/Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)
    | 450亿 | 2023年12月 |'
- en: '| Phi-3-Mini-4K⁸⁸8[https://huggingface.co/microsoft/Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)
    | 3.8 billion | April 2024 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Phi-3-Mini-4K⁸⁸8[https://huggingface.co/microsoft/Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)
    | 38亿 | 2024年4月 |'
- en: '| Falcon⁹⁹9[https://huggingface.co/tiiuae/falcon-7b](https://huggingface.co/tiiuae/falcon-7b)
    | 7 billion | March 2023 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Falcon⁹⁹9[https://huggingface.co/tiiuae/falcon-7b](https://huggingface.co/tiiuae/falcon-7b)
    | 70亿 | 2023年3月 |'
- en: '| Gemma-7b^(10)^(10)10[https://huggingface.co/google/gemma-7b](https://huggingface.co/google/gemma-7b)
    | 7 billion | February 2024 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7b^(10)^(10)10[https://huggingface.co/google/gemma-7b](https://huggingface.co/google/gemma-7b)
    | 70亿 | 2024年2月 |'
- en: 'Given the critical role of prompts in the performance of LLMs, we initially
    came up with a simple yet direct prompt, as illustrated in Prompt [2](#prompt2
    "Prompt 2 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy Statement
    Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining Check-worthy
    Statement Detection with LLMs through Data Pruning"). Observing that this initial
    prompt resulted in lengthy responses with redundant information in the zero-shot
    setting, and lacked a clear definition of check-worthiness, we employed ChatGPT-4
    to refine and improve the prompt, resulting in Prompt [1](#prompt1 "Prompt 1 ‣
    2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy Statement Detection ‣
    2 Methodology ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement
    Detection with LLMs through Data Pruning"). All eight LLMs were fine-tuned using
    this refined prompt to generate ‘Yes’ or ‘No’ answers indicating the check-worthiness
    of the input statement. Furthermore, we observed that a significant proportion
    of sentences in the training data utilized pronouns to refer to political entities,
    thereby increasing uncertainties and ambiguities. Therefore, we experimented with
    an expanded version of Prompt [1](#prompt1 "Prompt 1 ‣ 2.2.3 Effective Fine-tuning
    ‣ 2.2 LLMs for Check-worthy Statement Detection ‣ 2 Methodology ‣ FactFinders
    at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through
    Data Pruning") (i.e. Prompt [3](#prompt3 "Prompt 3 ‣ 2.2.3 Effective Fine-tuning
    ‣ 2.2 LLMs for Check-worthy Statement Detection ‣ 2 Methodology ‣ FactFinders
    at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through
    Data Pruning")) to evaluate whether explicitly indicating that the pronouns in
    the input statement may refer to political entities could enhance the performance
    of the fine-tuned model. However, the initial experiments on prompt engineering
    revealed that neither the compressed prompt (Prompt [2](#prompt2 "Prompt 2 ‣ 2.2.3
    Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy Statement Detection ‣ 2 Methodology
    ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with
    LLMs through Data Pruning")) nor the expanded (Prompt [3](#prompt3 "Prompt 3 ‣
    2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy Statement Detection ‣
    2 Methodology ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement
    Detection with LLMs through Data Pruning")) improved the performance of fine-tuned
    models (refer to Table [6](#S3.T6 "Table 6 ‣ 3.5 Effect of Prompt Engineering
    ‣ 3 Results ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement
    Detection with LLMs through Data Pruning")).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '鉴于提示在大型语言模型（LLMs）表现中的关键作用，我们最初提出了一个简单而直接的提示，如提示 [2](#prompt2 "Prompt 2 ‣ 2.2.3
    Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy Statement Detection ‣ 2 Methodology
    ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with
    LLMs through Data Pruning") 所示。我们观察到这个初始提示在零样本设置下产生了冗长且包含多余信息的回答，并且缺乏对可核查性的明确定义，因此我们利用
    ChatGPT-4 来改进提示，最终形成了提示 [1](#prompt1 "Prompt 1 ‣ 2.2.3 Effective Fine-tuning ‣
    2.2 LLMs for Check-worthy Statement Detection ‣ 2 Methodology ‣ FactFinders at
    CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through Data
    Pruning")。所有八个 LLM 都使用这个改进后的提示进行了微调，以生成“是”或“否”的答案来指示输入陈述的可核查性。此外，我们发现训练数据中的大量句子使用代词来指代政治实体，从而增加了不确定性和模糊性。因此，我们尝试了提示
    [1](#prompt1 "Prompt 1 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy
    Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining
    Check-worthy Statement Detection with LLMs through Data Pruning") 的扩展版本（即提示 [3](#prompt3
    "Prompt 3 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy Statement
    Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining Check-worthy
    Statement Detection with LLMs through Data Pruning")），以评估明确指出输入陈述中的代词可能指代政治实体是否能够提升微调模型的表现。然而，初步的提示工程实验表明，无论是压缩提示（提示
    [2](#prompt2 "Prompt 2 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy
    Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining
    Check-worthy Statement Detection with LLMs through Data Pruning")）还是扩展提示（提示 [3](#prompt3
    "Prompt 3 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy Statement
    Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining Check-worthy
    Statement Detection with LLMs through Data Pruning")）都没有提高微调模型的性能（参见表 [6](#S3.T6
    "Table 6 ‣ 3.5 Effect of Prompt Engineering ‣ 3 Results ‣ FactFinders at CheckThat!
    2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning")）。'
- en: 2.2.3 Effective Fine-tuning
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 有效的微调
- en: 'Fine-tuning an LLM is a challenging task due to the resource requirements,
    especially the memory demand. While this challenge can be escalated by training
    only certain layers of the LLM, still the computational requirement associated
    with gradient updates requires a lot of GPU memory. Therefore we use the Low-Rank
    Adaption technique (LoRA) for fine-tuning the LLMs. Instead of updating the weights
    directly, LoRA keeps track of the changes through low-rank perturbations requiring
    only minimal GPU memory. The LoRA configuration used for the fine-tuning is listed
    in Section [3.1](#S3.SS1 "3.1 Hyper-parameters and Environment Setting ‣ 3 Results
    ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with
    LLMs through Data Pruning"). To ensure experimental control, consistent hyperparameters
    were applied across all eight LLMs (see Table [3](#S3.T3 "Table 3 ‣ 3.1 Hyper-parameters
    and Environment Setting ‣ 3 Results ‣ FactFinders at CheckThat! 2024: Refining
    Check-worthy Statement Detection with LLMs through Data Pruning")).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '微调LLM是一项具有挑战性的任务，因为资源需求，尤其是内存要求非常高。尽管通过仅训练LLM的某些层可以缓解这一挑战，但与梯度更新相关的计算需求仍然需要大量的GPU内存。因此，我们使用低秩自适应技术（LoRA）来微调LLM。LoRA不是直接更新权重，而是通过低秩扰动跟踪变化，只需最少的GPU内存。用于微调的LoRA配置列在第[3.1节](#S3.SS1
    "3.1 Hyper-parameters and Environment Setting ‣ 3 Results ‣ FactFinders at CheckThat!
    2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning")。为了确保实验控制，所有八个LLM上都应用了一致的超参数（见表[3](#S3.T3
    "Table 3 ‣ 3.1 Hyper-parameters and Environment Setting ‣ 3 Results ‣ FactFinders
    at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through
    Data Pruning")）。'
- en: 'The performance of each model on check-worthy statement detection is presented
    in Table [4](#S3.T4 "Table 4 ‣ 3.3 Comparison of LLMs ‣ 3 Results ‣ FactFinders
    at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through
    Data Pruning"). Unfortunately, we could only compare the performance of the Llama
    family, Mistral, and Mixtral models during the testing phase of the competition.
    Therefore, the Phi-3-Mini-4k, the best-performing model in the Dev-Test partition
    wasn’t considered for the remaining experiments. Considering the competitive performance
    of the Llama2-7b model and the time and memory required, the rest of the experiments
    were carried out by fine-tuning the Llama2-7b using Prompt [1](#prompt1 "Prompt
    1 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy Statement Detection
    ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement
    Detection with LLMs through Data Pruning").'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '每个模型在值得检查的声明检测中的表现如表[4](#S3.T4 "Table 4 ‣ 3.3 Comparison of LLMs ‣ 3 Results
    ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with
    LLMs through Data Pruning")所示。不幸的是，我们只能在竞赛的测试阶段比较Llama家族、Mistral和Mixtral模型的表现。因此，表现最佳的Phi-3-Mini-4k模型未被考虑用于其余实验。考虑到Llama2-7b模型的竞争性表现以及所需的时间和内存，其余实验通过使用Prompt
    [1](#prompt1 "Prompt 1 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy
    Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining
    Check-worthy Statement Detection with LLMs through Data Pruning")微调Llama2-7b完成。'
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Prompt 1 Check-worthy Statement Detection
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Prompt 1 值得检查的声明检测
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Prompt 2 Check-worthy Statement Detection - Compressed
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Prompt 2 值得检查的声明检测 - 压缩
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Prompt 3 Check-worthy Statement Detection - Expanded
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Prompt 3 值得检查的声明检测 - 扩展
- en: 2.3 Data Pruning for Effective Learning
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 数据修剪以实现有效学习
- en: As previously mentioned, the training data is highly imbalanced, mostly containing
    shorter sentences with limited information. Recent studies [[14](#bib.bib14)]
    have demonstrated that instead of using the entire training data for fine-tuning,
    using only the high-quality labels improves the performance of check-worthy statement
    detection from political transcriptions. Inspired by this direction, we experimented
    with a two-step data pruning approach to automatically identify high-quality data
    instances for effective learning.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，训练数据高度不平衡，大多数包含信息有限的较短句子。近期研究[[14](#bib.bib14)]表明，与使用整个训练数据进行微调相比，仅使用高质量标签可以提高政治转录中值得检查的声明检测的表现。受到这一方向的启发，我们实验了一种两步数据修剪方法，以自动识别高质量数据实例，从而实现有效学习。
- en: 2.3.1 Step 1 - Identifying Informative Sentences
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 第一步 - 识别信息丰富的句子
- en: 'We began with identifying informative sentences in the training data that could
    potentially convey meaningful information for the training. In other words, we
    intended to remove the noisy instances from the training data as the first step
    of the data-pruning process. We define a political statement as informative if
    it meets one of the following four criteria:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先识别了训练数据中可能传达有意义信息的陈述。换句话说，我们的目的是在数据修剪过程的第一步中移除训练数据中的噪声实例。如果一项政治声明符合以下四个标准之一，我们将其定义为有信息的。
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Check-worthy status is "Yes": If the class label of the statement is "Yes",
    then it is informative.'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检查价值状态为“是”：如果陈述的类别标签为“是”，则它是有信息的。
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Contains a named entity: If the statement contains a named entity, it is highly
    likely to discuss information related to that entity. Hence the statement is informative.'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含命名实体：如果陈述包含命名实体，则很可能讨论与该实体相关的信息。因此，陈述是有信息的。
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Contains an informative verb: If the statement contains an informative verb,
    it is highly likely to discuss an informative action. Hence the statement is informative.'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含信息性动词：如果陈述包含信息性动词，则很可能讨论了一个信息性动作。因此，陈述是有信息的。
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Lengthy enough: If the statement is lengthy enough, it is likely to convey
    meaningful information.'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 足够长：如果陈述足够长，它很可能传达有意义的信息。
- en: While the first criteria check-worthy status is straightforward, the other criteria
    require extracting further information to determine the informative status of
    a sentence. We used the BERT model [[24](#bib.bib24)] fine-tuned^(11)^(11)11[https://huggingface.co/dslim/bert-base-NER](https://huggingface.co/dslim/bert-base-NER)
    for the Named Entity Recognition (NER) task to identify the presence of a named
    entity. The NER model identifies four types of named entities, person, organization,
    location, and miscellaneous from the input text. We noticed that only $37.8\%$
    of the training data mentions a person’s name. This indicates the prevalent use
    of pronouns to refer to the political entities in the transcriptions, which makes
    the task more challenging.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然第一个标准检查的状态是直接的，但其他标准需要提取进一步的信息以确定句子的有信息状态。我们使用了经过微调的 BERT 模型 [[24](#bib.bib24)]^(11)^(11)11[https://huggingface.co/dslim/bert-base-NER](https://huggingface.co/dslim/bert-base-NER)
    进行命名实体识别（NER）任务，以识别命名实体的存在。NER 模型识别输入文本中的四种类型的命名实体：人、组织、地点和其他。我们注意到，只有 $37.8\%$
    的训练数据提到了一个人的名字。这表明在转录中普遍使用代词来指代政治实体，使任务变得更加具有挑战性。
- en: In order to identify informative verbs, we first extracted all the verbs presented
    in the training data using the Part-of-Speech tagger from NLTK library.^(12)^(12)12[https://www.nltk.org/api/nltk.tag.pos_tag.html](https://www.nltk.org/api/nltk.tag.pos_tag.html)
    Extracted verbs were lemmatized further to bring them to their base form. This
    resulted in 3838 verbs in the training data to be identified as informative or
    not. We wanted to automatically classify each verb as either informative or not
    based on whether it conveys any check-worthy action. However, performing this
    binary classification in a zero-shot setting using a language model is a challenging
    task as explicitly defining an informative verb in the prompt may result in ambiguous
    classification. Therefore, we performed a fine-grained categorization of the verbs
    into the following 10 categories.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别信息性动词，我们首先使用 NLTK 库的词性标注器提取了训练数据中出现的所有动词。^(12)^(12)12[https://www.nltk.org/api/nltk.tag.pos_tag.html](https://www.nltk.org/api/nltk.tag.pos_tag.html)
    提取的动词经过词形还原进一步处理，以使其恢复到基本形式。这导致在训练数据中识别出 3838 个动词，判断其是否信息性。我们希望基于动词是否传达任何检查价值动作来自动分类每个动词为信息性或非信息性。然而，使用语言模型在零样本设置中执行这种二元分类是一项具有挑战性的任务，因为在提示中明确定义信息性动词可能导致分类模糊。因此，我们对动词进行了更加细致的分类，分为以下
    10 类。
- en: '1.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Physical Actions: e.g. Run'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 身体动作：例如，跑
- en: '2.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Mental Actions: e.g. Think'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 心理动作：例如，思考
- en: '3.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Changes in State: e.g. Grow'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 状态变化：例如，成长
- en: '4.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Creation or Destruction: e.g. Build'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创造或破坏：例如，建造
- en: '5.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Communication: e.g. Discuss'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 交流：例如，讨论
- en: '6.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Movement: e.g. Walk'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 移动：例如，走
- en: '7.'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Emotion: e.g. Hope'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 情感：例如，希望
- en: '8.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: 'Perception: e.g. See'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 感知：例如，看到
- en: '9.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: 'Linking verbs : e.g. is, has'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 连系动词：例如，is，has
- en: '10.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '10.'
- en: 'None: Any verb that does not fit into the other categories'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无：任何不符合其他类别的动词
- en: The first 8 categories were obtained by prompting ChatGPT with the question
    "What are the types of action verbs?". In addition to the 8 action verb categories,
    we added Linking verbs and None resulting in 10 categories of verbs. The option
    "None" was added to the categories to indicate that the verb does not fit into
    any of the other 9 categories.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 前 8 个类别是通过向 ChatGPT 提问“动作动词的类型有哪些？”得到的。除了这 8 个动作动词类别，我们还添加了连接动词和无类别，形成了 10 个动词类别。选项“无”被添加到类别中，以表示该动词不适合其他
    9 个类别。
- en: '[PRE3]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Prompt 4 Verb Classification
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 提示 4 动词分类
- en: 'We utilized Mixtral [[19](#bib.bib19)] in a zero-shot setting to classify a
    verb into one of the 10 categories using the Prompt [4](#prompt4 "Prompt 4 ‣ 2.3.1
    Step 1 - Identifying Informative Sentences ‣ 2.3 Data Pruning for Effective Learning
    ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement
    Detection with LLMs through Data Pruning"). Among the 10 categories, we chose
    the verb types Physical Actions, Changes in State, Creation or Destruction, Communication,
    and Movement as the informative verbs, as the other verb types are less likely
    to represent a check-worthy action.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在零-shot 设置中使用 Mixtral [[19](#bib.bib19)] 将动词分类到 10 个类别中的一个，使用了提示 [4](#prompt4
    "提示 4 ‣ 2.3.1 步骤 1 - 识别信息性句子 ‣ 2.3 数据修剪以有效学习 ‣ 2 方法论 ‣ FactFinders at CheckThat!
    2024: 通过数据修剪提升值得检查的陈述检测")。在这 10 个类别中，我们选择了物理动作、状态变化、创造或破坏、交流和移动作为信息性动词，因为其他动词类型不太可能表示值得检查的动作。'
- en: '![Refer to caption](img/fc173852a88c3dcec4fdac84ed55d273.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/fc173852a88c3dcec4fdac84ed55d273.png)'
- en: 'Figure 2: Word cloud indicating verb types and their frequencies in the training
    data.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 指示训练数据中动词类型及其频率的词云'
- en: '![Refer to caption](img/749903196ad563874fb3142d688ffe0f.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/749903196ad563874fb3142d688ffe0f.png)'
- en: 'Figure 3: Distribution of verb types in the training data'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 训练数据中动词类型的分布'
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2.3.1 Step 1 - Identifying Informative Sentences
    ‣ 2.3 Data Pruning for Effective Learning ‣ 2 Methodology ‣ FactFinders at CheckThat!
    2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning")
    shows the word cloud of verbs present in the training data. Informative verbs
    are highlighted in green color and non-informative verbs are highlighted in red
    color in the Figure. It can be observed that most of the verbs are classified
    into the two groups correctly. Figure [3](#S2.F3 "Figure 3 ‣ 2.3.1 Step 1 - Identifying
    Informative Sentences ‣ 2.3 Data Pruning for Effective Learning ‣ 2 Methodology
    ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with
    LLMs through Data Pruning") presents the verb type distribution in the training
    data. Interestingly the occurrences of informative verb categories are relatively
    high compared to non-informative verb categories. We further noticed that $88.2\%$
    for non-check-worthy statements.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [2](#S2.F2 "图 2 ‣ 2.3.1 步骤 1 - 识别信息性句子 ‣ 2.3 数据修剪以有效学习 ‣ 2 方法论 ‣ FactFinders
    at CheckThat! 2024: 通过数据修剪提升值得检查的陈述检测") 显示了训练数据中动词的词云。信息性动词用绿色突出显示，非信息性动词用红色突出显示。从图中可以观察到，大多数动词被正确分类为这两组。图
    [3](#S2.F3 "图 3 ‣ 2.3.1 步骤 1 - 识别信息性句子 ‣ 2.3 数据修剪以有效学习 ‣ 2 方法论 ‣ FactFinders at
    CheckThat! 2024: 通过数据修剪提升值得检查的陈述检测") 展示了训练数据中动词类型的分布。有趣的是，信息性动词类别的出现频率相对较高，而非信息性动词类别较低。我们进一步注意到非值得检查的陈述占比为
    $88.2\%$。'
- en: 'The final factor determining an informative sentence, minimum length is difficult
    to define explicitly. Therefore, we choose the minimum length value that reaches
    the optimal F1-score in the dev-test data by varying the value from 3 - 10\. We
    excluded the stop words^(13)^(13)13[https://www.nltk.org/search.html?q=stopwords](https://www.nltk.org/search.html?q=stopwords)
    while calculating the length of a statement. We observed that the most informative
    sentences are obtained when the minimum length factor is set to 8 (refer to Section
    [3.6](#S3.SS6 "3.6 Effect of Data Pruning ‣ 3 Results ‣ FactFinders at CheckThat!
    2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning")
    for the results). With this optimal setting, the first step of data pruning resulted
    in a reduced training data with 20,141 sentences. In other words, 2358 sentences
    ($10.5\%$ of original training data) were filtered out as non-informative sentences
    at this stage of the data pruning process.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 确定信息量丰富的句子的最终因素，最小长度很难明确界定。因此，我们通过将最小长度值从3到10进行变化，选择在开发测试数据中达到最佳F1分数的最小长度值。在计算陈述长度时，我们排除了停用词^(13)^(13)13[https://www.nltk.org/search.html?q=stopwords](https://www.nltk.org/search.html?q=stopwords)。我们观察到，当最小长度因子设置为8时，能获得最具信息量的句子（有关结果，请参阅[3.6](#S3.SS6
    "3.6 数据修剪的效果 ‣ 3 结果 ‣ CheckThat! 2024 中的 FactFinders：通过数据修剪优化值得检查的陈述检测")）。在这个最佳设置下，数据修剪的第一步将训练数据减少为20,141个句子。换句话说，2358个句子（占原始训练数据的$10.5\%$）在数据修剪过程的这一阶段被筛选为非信息性句子。
- en: 2.3.2 Step 2 - Under Sampling using Condensed Nearest Neighbour
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 步骤2 - 使用凝缩最近邻进行下采样
- en: The informative sentences identified in the previous stage are still imbalanced
    in class. Therefore, we executed an under-sampling technique, Condensed Nearest
    Neighbour (CNN) [[23](#bib.bib23)], to generate class-balanced training data.
    We retained all the minority data instances (check-worthy statements) and sampled
    only the majority data instances (non-check-worthy statements). The idea behind
    CNN sampling is to identify a subset of data instances that can be used to correctly
    classify all the other unsampled data instances using the 1-Nearest Neighbour
    rule. This makes sure that the sampled data distribution is the same as the original
    data distribution without any information loss.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一阶段识别的信息丰富的句子在类别上仍然不平衡。因此，我们执行了一种下采样技术——凝缩最近邻（CNN）[[23](#bib.bib23)]，以生成类别平衡的训练数据。我们保留了所有少数类数据实例（值得检查的陈述），仅对多数类数据实例（非值得检查的陈述）进行采样。CNN采样的思想是识别一个数据实例的子集，该子集可以使用1-最近邻规则正确分类所有其他未采样的数据实例。这确保了采样数据分布与原始数据分布相同，并且没有信息丢失。
- en: CNN requires the input data to be represented as vectors to iteratively sample
    data points from a vector space and perform the 1-Nearest Neighbour classification
    in the unsampled data points. We used BERT [[24](#bib.bib24)] embeddings to convert
    the sentences in the training data to a vector representation of length 768\.
    The imbalanced^(14)^(14)14[https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.CondensedNearestNeighbour.html](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.CondensedNearestNeighbour.html)
    python library was used to perform CNN sampling. This resulted in a sampled data
    with 9907 sentences ($44\%$ negative instances.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: CNN要求输入数据以向量形式表示，以便从向量空间中迭代地抽样数据点，并在未采样的数据点中执行1-最近邻分类。我们使用了BERT [[24](#bib.bib24)]
    嵌入将训练数据中的句子转换为长度为768的向量表示。使用了不平衡的^(14)^(14)14[https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.CondensedNearestNeighbour.html](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.CondensedNearestNeighbour.html)
    Python库来执行CNN采样。这导致了一个包含9907个句子（$44\%$为负实例）的采样数据。
- en: '![Refer to caption](img/53065ee00f27c6069a8ac3d0a867e74e.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/53065ee00f27c6069a8ac3d0a867e74e.png)'
- en: (a) Original training data (before applying step 1 & 2)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原始训练数据（应用步骤1和2之前）
- en: '![Refer to caption](img/7e5c64dda33ce9e49ae716b8e99a07fd.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7e5c64dda33ce9e49ae716b8e99a07fd.png)'
- en: (b) Original training data with uninformative sentences (filtered during step
    1) highlighted
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 原始训练数据中标出不具信息量的句子（在步骤1中被筛选出来）
- en: '![Refer to caption](img/e658b321762497fc9e19b6f29f3a05c0.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e658b321762497fc9e19b6f29f3a05c0.png)'
- en: (c) High-quality training data obtained after data pruning (after step 1 & 2)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 数据修剪后获得的高质量训练数据（经过步骤1和2）
- en: 'Figure 4: 2D visualization of the Training Data.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：训练数据的二维可视化。
- en: 'Figure [4](#S2.F4 "Figure 4 ‣ 2.3.2 Step 2 - Under Sampling using Condensed
    Nearest Neighbour ‣ 2.3 Data Pruning for Effective Learning ‣ 2 Methodology ‣
    FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with
    LLMs through Data Pruning") visualizes the training data points in 2 dimensions
    at each stage of the data pruning process. It can be observed that uninformative
    data points filtered during step 1 (Figure [4(b)](#S2.F4.sf2 "In Figure 4 ‣ 2.3.2
    Step 2 - Under Sampling using Condensed Nearest Neighbour ‣ 2.3 Data Pruning for
    Effective Learning ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining
    Check-worthy Statement Detection with LLMs through Data Pruning")) are concentrated
    around the bottom-left corner of the plot. Further, the CNN samples a similar
    distribution of non-check-worthy data points (Figure [4(c)](#S2.F4.sf3 "In Figure
    4 ‣ 2.3.2 Step 2 - Under Sampling using Condensed Nearest Neighbour ‣ 2.3 Data
    Pruning for Effective Learning ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024:
    Refining Check-worthy Statement Detection with LLMs through Data Pruning")) for
    obtaining balanced training data.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '图[4](#S2.F4 "Figure 4 ‣ 2.3.2 Step 2 - Under Sampling using Condensed Nearest
    Neighbour ‣ 2.3 Data Pruning for Effective Learning ‣ 2 Methodology ‣ FactFinders
    at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through
    Data Pruning") 可视化了数据剪枝过程中每个阶段的训练数据点的二维分布。可以观察到，在步骤1（图[4(b)](#S2.F4.sf2 "In Figure
    4 ‣ 2.3.2 Step 2 - Under Sampling using Condensed Nearest Neighbour ‣ 2.3 Data
    Pruning for Effective Learning ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024:
    Refining Check-worthy Statement Detection with LLMs through Data Pruning")）中被过滤的无信息数据点集中在图的左下角。此外，CNN采样了类似分布的非检查值得数据点（图[4(c)](#S2.F4.sf3
    "In Figure 4 ‣ 2.3.2 Step 2 - Under Sampling using Condensed Nearest Neighbour
    ‣ 2.3 Data Pruning for Effective Learning ‣ 2 Methodology ‣ FactFinders at CheckThat!
    2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning")），以获得平衡的训练数据。'
- en: 3 Results
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 结果
- en: We discuss the experiment results in this section, especially the hyper-parameters
    used to fine-tune the LLMs, the environment setting used, the performance of various
    LLMs with the consistency analysis, effect of prompt engineering, and the impact
    of training data pruning on check-worthy statement detection task.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论实验结果，特别是用于微调LLMs的超参数、使用的环境设置、各种LLMs的一致性分析性能、提示工程的效果以及训练数据剪枝对检测值得检查语句任务的影响。
- en: 3.1 Hyper-parameters and Environment Setting
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 超参数和环境设置
- en: 'Table 3: Hyper-parameters used for Fine-tuning.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：用于微调的超参数。
- en: '| Parameter | Value |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 值 |'
- en: '| Epochs | 3 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 训练轮次 | 3 |'
- en: '| Training batch size | 2 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 训练批量大小 | 2 |'
- en: '| Gradient accumulation steps | 2 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 梯度累积步数 | 2 |'
- en: '| Optimizer | Paged AdamW 32bit |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | Paged AdamW 32bit |'
- en: '| Learning rate | 2e-4 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 2e-4 |'
- en: '| Weight decay | 0.001 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 权重衰减 | 0.001 |'
- en: '| Maximum gradient norm | 0.3 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 最大梯度范数 | 0.3 |'
- en: '| Warmup ratio | 0.03 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 热身比例 | 0.03 |'
- en: '| Temperature | 0.03 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | 0.03 |'
- en: '| Lora Alpha | 16 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Lora Alpha | 16 |'
- en: '| Lora dropout | 0.1 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Lora dropout | 0.1 |'
- en: '| Lora rank | 64 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Lora rank | 64 |'
- en: 'Hyper-parameters used to fine-tune the LLMs in our experiments are listed in
    Table [3](#S3.T3 "Table 3 ‣ 3.1 Hyper-parameters and Environment Setting ‣ 3 Results
    ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with
    LLMs through Data Pruning"). Most of the hyper-parameters were the same for all
    the LLMs we experimented with, except the training batch size which was reduced
    to 1 for Mixtral, due to its memory demand. Consequently, the gradient accumulation
    steps for this model was increased to 4\. All the experiments were conducted using
    Queen Mary’s Apocrita HPC facility, supported by QMUL Research-IT [[25](#bib.bib25)].
    Specifically, 1 GPU (Volta V100 or Ampere A100) with 8 CPU cores, each composed
    of 11 GB memory was used to train and test all the models.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '我们实验中用于微调LLMs的超参数列于表[3](#S3.T3 "Table 3 ‣ 3.1 Hyper-parameters and Environment
    Setting ‣ 3 Results ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement
    Detection with LLMs through Data Pruning")。除训练批量大小因Mixtral的内存需求减少至1外，其他超参数在所有实验的LLMs中基本相同。因此，该模型的梯度累积步数增加至4。所有实验均使用Queen
    Mary的Apocrita HPC设施进行，得到QMUL Research-IT [[25](#bib.bib25)]的支持。具体而言，使用1个GPU（Volta
    V100或Ampere A100）和8个CPU核心，每个核心配备11 GB内存，训练和测试所有模型。'
- en: While generating the predictions by the fine-tuned models for evaluation, we
    noticed that a language model may generate different predictions for the same
    prompt and input sentence. Therefore, we ran each fine-tuned language model 5
    times and obtained the majority prediction as the final prediction of the model.
    Further, we fine-tuned each model 3 times and reported the average performance
    of 3 fine-tuned models. We use the partitions train and dev for training and validation
    of the models, and the performance of the models is reported on the other two
    partitions dev-test and test.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成评估的预测时，我们注意到语言模型可能对相同的提示和输入句子生成不同的预测。因此，我们对每个微调语言模型进行了 5 次运行，并获得多数预测作为模型的最终预测。此外，我们对每个模型进行了
    3 次微调，并报告了 3 个微调模型的平均性能。我们使用分区的训练和验证集进行模型训练和验证，模型性能在其他两个分区 dev-test 和 test 上报告。
- en: 3.2 Evaluation Metrics
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 评估指标
- en: The official evaluation metric for CheckThat! 2024 task 1, check-worthiness
    estimation is F1-Score over the positive class. However, since one metric could
    have a bias toward the comparison, we report average accuracy, precision, and
    recall along with the F1-score. Further, we computed consistency@K of the fine-tuned
    models indicating the fraction of data instances for which the model generated
    the same output class in all K iterations. Consistency score report in the following
    subsections was computed over 5 iterations (consistency@5).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: CheckThat! 2024 任务 1 的官方评估指标是正类的 F1-得分。然而，由于某些指标可能存在比较偏差，我们报告了平均准确率、精确度和召回率以及
    F1-得分。此外，我们计算了微调模型的 consistency@K，表示模型在所有 K 次迭代中生成相同输出类别的数据实例的比例。以下子节中的一致性得分报告是基于
    5 次迭代计算的（consistency@5）。
- en: 3.3 Comparison of LLMs
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 LLMs 的比较
- en: 'Table 4: Performance of LLMs on the Test and Dev-Test Partitions.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：LLMs 在测试和 Dev-Test 分区上的表现
- en: '| Partition | Model | Accuracy | Precision | Recall | F1-Score | Consistency
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 分区 | 模型 | 准确率 | 精确度 | 召回率 | F1-得分 | 一致性 |'
- en: '| Test | Llama2-7b | 0.905 $\pm$ 0.013 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | Llama2-7b | 0.905 $\pm$ 0.013 |'
- en: '| Llama2-13b | 0.897 $\pm$ 0.005 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b | 0.897 $\pm$ 0.005 |'
- en: '| Llama3-8b | 0.907 $\pm$ 0.002 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8b | 0.907 $\pm$ 0.002 |'
- en: '| Mistral | 0.889 $\pm$ 0.01 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | 0.889 $\pm$ 0.01 |'
- en: '| Mixtral | 0.891 $\pm$ 0.011 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral | 0.891 $\pm$ 0.011 |'
- en: '| Phi3-Mini-4K | 0.897 $\pm$ 0.003 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Phi3-Mini-4K | 0.897 $\pm$ 0.003 |'
- en: '| Falcon | 0.891 $\pm$ 0 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Falcon | 0.891 $\pm$ 0 |'
- en: '| Gemma-7b | 0.9 $\pm$ 0.01 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7b | 0.9 $\pm$ 0.01 |'
- en: '| Dev-Test | Llama2-7b | 0.944 $\pm$ 0.003 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Dev-Test | Llama2-7b | 0.944 $\pm$ 0.003 |'
- en: '| Llama2-13b | 0.941 $\pm$ 0.009 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b | 0.941 $\pm$ 0.009 |'
- en: '| Llama3-8b | 0.921 $\pm$ 0.002 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8b | 0.921 $\pm$ 0.002 |'
- en: '| Mistral | 0.932 $\pm$ 0.001 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | 0.932 $\pm$ 0.001 |'
- en: '| Mixtral | 0.939 $\pm$ 0.007 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral | 0.939 $\pm$ 0.007 |'
- en: '| Phi3-Mini-4K | 0.955 $\pm$ 0.007 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| Phi3-Mini-4K | 0.955 $\pm$ 0.007 |'
- en: '| Falcon | 0.931 $\pm$ 0 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Falcon | 0.931 $\pm$ 0 |'
- en: '|  | Gemma-7b | 0.942 $\pm$ 0.024 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | Gemma-7b | 0.942 $\pm$ 0.024 |'
- en: 'We compare the performance of the eight open-source LLMs in test and dev-test
    partitions and Table [4](#S3.T4 "Table 4 ‣ 3.3 Comparison of LLMs ‣ 3 Results
    ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with
    LLMs through Data Pruning") reports their average accuracy, precision, recall,
    F1-score, and consistency@5\. Since the class distribution of dev-test and test
    partitions are different, we can observe that the performance in each partition
    varies for all the LLMs. While Phi3-Mini-4K obtains the highest F1-score in the
    dev-test portion, Llama2 models stand out as the best-performing models in the
    test partition based on F1-score. Further, both Llama2-7b and Phi3-Mini-4K demonstrate
    greater consistency in predicting class labels across both partitions compared
    to other models. On the other hand, Llama3-8b, one of the latest models from the
    Llama family reaches the highest accuracy and precision in the test partition.
    However, the model fails to outperform the other models in terms of F1-score due
    to poor recall. Similarly, Mixtral, the largest model we compared, fails to give
    a consistent performance across both partitions. Moreover, both Mistral and Falcon
    remain as the lower-performing models in both partitions.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '我们比较了八个开源LLM在测试和开发测试分区的表现，并且表格[4](#S3.T4 "Table 4 ‣ 3.3 Comparison of LLMs
    ‣ 3 Results ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement
    Detection with LLMs through Data Pruning")报告了它们的平均准确率、精确率、召回率、F1分数和一致性@5。由于开发测试和测试分区的类别分布不同，我们可以观察到所有LLM在每个分区的表现有所不同。虽然Phi3-Mini-4K在开发测试部分获得了最高的F1分数，但Llama2模型在测试分区中基于F1分数表现最佳。此外，Llama2-7b和Phi3-Mini-4K在预测类别标签的一致性方面比其他模型更强。然而，Llama3-8b，作为Llama家族中最新的模型之一，在测试分区中达到最高的准确率和精确率，但由于召回率较差，无法在F1分数上超越其他模型。类似地，Mixtral作为我们比较的最大模型，在两个分区中表现不一致。此外，Mistral和Falcon在两个分区中的表现均较低。'
- en: 'Table 5: Average Fine-tuning Time of LLMs.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 5：LLM的平均微调时间。
- en: '| Model | Average Fine-tuning Time |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均微调时间 |'
- en: '| Llama2-7b | 6.3h |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b | 6.3小时 |'
- en: '| Llama2-13b | 11.9h |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b | 11.9小时 |'
- en: '| Llama3-8b | 6.7h |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8b | 6.7小时 |'
- en: '| Mistral | 6.82h |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | 6.82小时 |'
- en: '| Mixtral | 33.86h |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral | 33.86小时 |'
- en: '| Phi3-Mini-4K | 4h |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Phi3-Mini-4K | 4小时 |'
- en: '| Falcon | 6.26h |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Falcon | 6.26小时 |'
- en: 'Table [5](#S3.T5 "Table 5 ‣ 3.3 Comparison of LLMs ‣ 3 Results ‣ FactFinders
    at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through
    Data Pruning") lists the fine-tuning time of each LLM. Except for Mixtral and
    Phi3-Mini-4K, the largest and smallest models compared respectively, all the other
    models’ fine-tuning time remains between 6-12 hours. As expected, the fine-tuning
    time increases with the number of parameters.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '表格[5](#S3.T5 "Table 5 ‣ 3.3 Comparison of LLMs ‣ 3 Results ‣ FactFinders at
    CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through Data
    Pruning")列出了每个LLM的微调时间。除了Mixtral和Phi3-Mini-4K这两个分别是最大和最小的模型之外，其他模型的微调时间都保持在6-12小时之间。正如预期的那样，微调时间随着参数数量的增加而增加。'
- en: As we already mentioned, we were able to compare only the Llama models, Mistral,
    and Mixtral during the testing phase of the competition. Therefore, considering
    the performance of these models in terms of F1-score in the dev-test partition
    and the time and memory required to fine-tune the model, we chose Llama2-7b as
    the optimal LLM for the remaining experiments.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们在比赛测试阶段仅能比较Llama模型、Mistral和Mixtral。因此，考虑到这些模型在开发测试分区中的F1分数性能以及微调模型所需的时间和内存，我们选择了Llama2-7b作为剩余实验的最佳LLM。
- en: 3.4 Consistency Analysis
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 一致性分析
- en: 'Since LLMs are text generation models, they tend to generate different output
    for the same input text even in a fine-tuned environment. Therefore, we analyze
    their consistency in predicting the same output class label over K iterations
    with the metric consistency@K. Figure [5](#S3.F5 "Figure 5 ‣ 3.4 Consistency Analysis
    ‣ 3 Results ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement
    Detection with LLMs through Data Pruning") presents the change in consistency
    with the number of iterations varied from 2 to 25 in the test and dev-test partition.
    It can be observed that the consistency declines with the increase in the number
    of iterations and becomes stable after around 11-12 iterations. While the model
    could reach a stable consistency in both partitions, the percentage of drop in
    consistency (difference between initial consistency when K=2, and the stable consistency)
    is nearly double for the test partition (3.1% vs 5.9%).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '由于LLMs是文本生成模型，它们倾向于对相同的输入文本生成不同的输出，即使在微调环境中也是如此。因此，我们使用指标consistency@K分析它们在K次迭代中预测相同输出类别标签的一致性。图[5](#S3.F5
    "Figure 5 ‣ 3.4 Consistency Analysis ‣ 3 Results ‣ FactFinders at CheckThat! 2024:
    Refining Check-worthy Statement Detection with LLMs through Data Pruning")展示了一致性随着迭代次数从2到25变化的情况。可以观察到，一致性随着迭代次数的增加而下降，并在大约11-12次迭代后稳定下来。虽然模型在两个分区中都能达到稳定的一致性，但一致性的下降百分比（K=2时的初始一致性与稳定一致性之间的差异）在测试分区中几乎是测试分区的两倍（3.1%对比5.9%）。'
- en: '![Refer to caption](img/83b5d5f7f9d6a41b979625eec50a4b03.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/83b5d5f7f9d6a41b979625eec50a4b03.png)'
- en: 'Figure 5: Consistency of Llama2-7b with Iterations'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: Llama2-7b与迭代的一致性'
- en: 3.5 Effect of Prompt Engineering
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 提示工程的效果
- en: 'Table 6: Performance of Llama2-7b with Prompt Variation.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: Llama2-7b在提示变动下的表现'
- en: '| Partition | Prompt | Accuracy | Precision | Recall | F1-Score | Consistency
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 分区 | 提示 | 准确率 | 精确率 | 召回率 | F1-得分 | 一致性 |'
- en: '| Test | Prompt [1](#prompt1 "Prompt 1 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2
    LLMs for Check-worthy Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat!
    2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning")
    | 0.905 $\pm$ 0.013 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 提示 [1](#prompt1 "Prompt 1 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for
    Check-worthy Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024:
    Refining Check-worthy Statement Detection with LLMs through Data Pruning") | 0.905
    $\pm$ 0.013 |'
- en: '| Prompt [2](#prompt2 "Prompt 2 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for
    Check-worthy Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024:
    Refining Check-worthy Statement Detection with LLMs through Data Pruning") | 0.906
    $\pm$ 0.003 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 提示 [2](#prompt2 "Prompt 2 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy
    Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining
    Check-worthy Statement Detection with LLMs through Data Pruning") | 0.906 $\pm$
    0.003 |'
- en: '| Prompt [3](#prompt3 "Prompt 3 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for
    Check-worthy Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024:
    Refining Check-worthy Statement Detection with LLMs through Data Pruning") | 0.902
    $\pm$ 0.003 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 提示 [3](#prompt3 "Prompt 3 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy
    Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining
    Check-worthy Statement Detection with LLMs through Data Pruning") | 0.902 $\pm$
    0.003 |'
- en: '| No Instruction | 0.894 $\pm$ 0.018 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 无指示 | 0.894 $\pm$ 0.018 |'
- en: '| Dev-Test | Prompt [1](#prompt1 "Prompt 1 ‣ 2.2.3 Effective Fine-tuning ‣
    2.2 LLMs for Check-worthy Statement Detection ‣ 2 Methodology ‣ FactFinders at
    CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through Data
    Pruning") | 0.944 $\pm$ 0.003 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Dev-Test | 提示 [1](#prompt1 "Prompt 1 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2
    LLMs for Check-worthy Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat!
    2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning")
    | 0.944 $\pm$ 0.003 |'
- en: '| Prompt [2](#prompt2 "Prompt 2 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for
    Check-worthy Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024:
    Refining Check-worthy Statement Detection with LLMs through Data Pruning") | 0.927
    $\pm$ 0.003 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 提示 [2](#prompt2 "Prompt 2 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy
    Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining
    Check-worthy Statement Detection with LLMs through Data Pruning") | 0.927 $\pm$
    0.003 |'
- en: '| Prompt [3](#prompt3 "Prompt 3 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for
    Check-worthy Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024:
    Refining Check-worthy Statement Detection with LLMs through Data Pruning") | 0.936
    $\pm$ 0.005 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 提示 [3](#prompt3 "Prompt 3 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy
    Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining
    Check-worthy Statement Detection with LLMs through Data Pruning") | 0.936 $\pm$
    0.005 |'
- en: '| No Instruction | 0.927 $\pm$ 0.003 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 无指示 | 0.927 $\pm$ 0.003 |'
- en: 'We conducted experiments using three proposed versions of prompts discussed
    in Section [2.2.2](#S2.SS2.SSS2 "2.2.2 Prompt Engineering ‣ 2.2 LLMs for Check-worthy
    Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024: Refining
    Check-worthy Statement Detection with LLMs through Data Pruning") along with a
    prompt without any instruction to analyze the impact of prompt engineering on
    Llama2-7b model performance. Table [6](#S3.T6 "Table 6 ‣ 3.5 Effect of Prompt
    Engineering ‣ 3 Results ‣ FactFinders at CheckThat! 2024: Refining Check-worthy
    Statement Detection with LLMs through Data Pruning") presents the evaluation results
    on the test and dev-test partitions. While all three prompts proposed reach a
    similar F1-score in the test partition, their impact is quite evident in the dev-test
    partition. We can observe that Prompt [1](#prompt1 "Prompt 1 ‣ 2.2.3 Effective
    Fine-tuning ‣ 2.2 LLMs for Check-worthy Statement Detection ‣ 2 Methodology ‣
    FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with
    LLMs through Data Pruning") achieves the best overall scores across all metrics.
    It is worth noting, that the expanded prompt Prompt [3](#prompt3 "Prompt 3 ‣ 2.2.3
    Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy Statement Detection ‣ 2 Methodology
    ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with
    LLMs through Data Pruning"), shows a notably high recall score and consistency
    in the test partition possibly due to the attention given to the pronouns in the
    instruction. Moreover, a substantial performance decline is observed when no instructions
    are included in the prompt highlights the importance of prompt engineering in
    achieving optimal results from LLMs.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了第[2.2.2](#S2.SS2.SSS2 "2.2.2 提示工程 ‣ 2.2 用于值得检查的陈述检测的 LLMs ‣ 2 方法论 ‣ FactFinders
    在 CheckThat! 2024：通过数据修剪优化 LLMs 对值得检查的陈述检测")节中讨论的三种提示版本进行实验，并与一个没有任何指令的提示一起分析提示工程对
    Llama2-7b 模型性能的影响。表格 [6](#S3.T6 "表格 6 ‣ 3.5 提示工程的效果 ‣ 3 结果 ‣ FactFinders 在 CheckThat!
    2024：通过数据修剪优化 LLMs 对值得检查的陈述检测") 展示了测试和开发测试分区的评估结果。虽然所有三个提示在测试分区中都达到了类似的 F1 分数，但它们在开发测试分区的影响却非常明显。我们可以观察到提示
    [1](#prompt1 "提示 1 ‣ 2.2.3 有效的微调 ‣ 2.2 用于值得检查的陈述检测的 LLMs ‣ 2 方法论 ‣ FactFinders
    在 CheckThat! 2024：通过数据修剪优化 LLMs 对值得检查的陈述检测") 在所有指标中都获得了最佳的总体分数。值得注意的是，扩展的提示提示
    [3](#prompt3 "提示 3 ‣ 2.2.3 有效的微调 ‣ 2.2 用于值得检查的陈述检测的 LLMs ‣ 2 方法论 ‣ FactFinders
    在 CheckThat! 2024：通过数据修剪优化 LLMs 对值得检查的陈述检测") 显示出在测试分区中显著高的召回率和一致性，这可能是由于对指令中的代词给予了关注。此外，当提示中没有包含任何指令时，性能显著下降，这突显了提示工程在从
    LLMs 获得最佳结果中的重要性。
- en: 'Table 7: Average fine-tuning time vs Instruction length in the Prompts.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：提示中的平均微调时间与指令长度对比。
- en: '| Prompt | Average Fine-tuning Time | Instruction Length (Number of Words)
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 平均微调时间 | 指令长度（词数） |'
- en: '| Prompt [1](#prompt1 "Prompt 1 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for
    Check-worthy Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024:
    Refining Check-worthy Statement Detection with LLMs through Data Pruning") | 6.3h
    | 60 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 提示 [1](#prompt1 "提示 1 ‣ 2.2.3 有效的微调 ‣ 2.2 用于值得检查的陈述检测的 LLMs ‣ 2 方法论 ‣ FactFinders
    在 CheckThat! 2024：通过数据修剪优化 LLMs 对值得检查的陈述检测") | 6.3h | 60 |'
- en: '| Prompt [2](#prompt2 "Prompt 2 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for
    Check-worthy Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024:
    Refining Check-worthy Statement Detection with LLMs through Data Pruning") | 4.3h
    | 11 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 提示 [2](#prompt2 "提示 2 ‣ 2.2.3 有效的微调 ‣ 2.2 用于值得检查的陈述检测的 LLMs ‣ 2 方法论 ‣ FactFinders
    在 CheckThat! 2024：通过数据修剪优化 LLMs 对值得检查的陈述检测") | 4.3h | 11 |'
- en: '| Prompt [3](#prompt3 "Prompt 3 ‣ 2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for
    Check-worthy Statement Detection ‣ 2 Methodology ‣ FactFinders at CheckThat! 2024:
    Refining Check-worthy Statement Detection with LLMs through Data Pruning") | 7.1h
    | 72 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 提示 [3](#prompt3 "提示 3 ‣ 2.2.3 有效的微调 ‣ 2.2 用于值得检查的陈述检测的 LLMs ‣ 2 方法论 ‣ FactFinders
    在 CheckThat! 2024：通过数据修剪优化 LLMs 对值得检查的陈述检测") | 7.1h | 72 |'
- en: '| No Instruction | 3.1h | 0 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 无指令 | 3.1h | 0 |'
- en: 'In addition to performance metrics, Table [7](#S3.T7 "Table 7 ‣ 3.5 Effect
    of Prompt Engineering ‣ 3 Results ‣ FactFinders at CheckThat! 2024: Refining Check-worthy
    Statement Detection with LLMs through Data Pruning") indicates that the fine-tuning
    time increases with the length of the instruction. According to the competition’s
    settings, we primarily consider the F1-score performance of each prompt in the
    dev-test partition. Consequently, we selected Prompt [1](#prompt1 "Prompt 1 ‣
    2.2.3 Effective Fine-tuning ‣ 2.2 LLMs for Check-worthy Statement Detection ‣
    2 Methodology ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement
    Detection with LLMs through Data Pruning") as the optimal one for the remaining
    experiments.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 除了性能指标外，表[7](#S3.T7 "表7 ‣ 3.5 提示工程的效果 ‣ 3 结果 ‣ FactFinders在CheckThat! 2024：通过数据剪枝改进LLMs的检测")指出，微调时间随着指令的长度增加而增加。根据比赛的设置，我们主要考虑每个提示在开发测试分区中的F1分数性能。因此，我们选择了提示[1](#prompt1
    "提示1 ‣ 2.2.3 有效的微调 ‣ 2.2 LLMs用于检测值得检查的陈述 ‣ 2 方法 ‣ FactFinders在CheckThat! 2024：通过数据剪枝改进LLMs的检测")作为剩余实验的最佳提示。
- en: 3.6 Effect of Data Pruning
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 数据剪枝的效果
- en: '![Refer to caption](img/7efcdef2cf8d2355900400cf4bf0c80a.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7efcdef2cf8d2355900400cf4bf0c80a.png)'
- en: 'Figure 6: Average F1-score in Dev-Test partition with the change in minimum
    length factor used for data pruning'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在开发测试分区中随着最小长度因子的变化，数据剪枝的平均F1分数
- en: 'As we discussed earlier, we leave the minimum length factor as a parameter
    of the data pruning process. Therefore we varied the minimum length from 3-10
    and observed the performance of the Llama2-7b model fine-tuned on the pruned dataset.
    Figure [6](#S3.F6 "Figure 6 ‣ 3.6 Effect of Data Pruning ‣ 3 Results ‣ FactFinders
    at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through
    Data Pruning") shows the F1-score of the fine-tuned model in the dev-test partition.
    It can be observed that Step 1 alone is not sufficient enough to identify high-quality
    training data, and Step 2 always boosts the performance when combined with Step
    1 except when the minimum length factor is set to very low (3-4). The optimal
    performance for the two-step data pruning is obtained when the minimum length
    factor is set to 8.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，我们将最小长度因子保留为数据剪枝过程的一个参数。因此，我们将最小长度从3-10进行了变化，并观察了在剪枝数据集上微调的Llama2-7b模型的性能。图[6](#S3.F6
    "图6 ‣ 3.6 数据剪枝的效果 ‣ 3 结果 ‣ FactFinders在CheckThat! 2024：通过数据剪枝改进LLMs的检测")展示了在开发测试分区中微调模型的F1分数。可以观察到，仅使用步骤1不足以识别高质量的训练数据，而步骤2总是能在与步骤1结合时提升性能，除非最小长度因子设置得非常低（3-4）。当最小长度因子设置为8时，两步数据剪枝的最佳性能得以实现。
- en: 'Table 8: Performance of Llama2-7b with Data Pruning.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：Llama2-7b在数据剪枝下的性能。
- en: '| Partition |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 分区 |'
- en: '&#124; Pruning &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 剪枝 &#124;'
- en: '&#124; Technique &#124;'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 技术 &#124;'
- en: '| Accuracy | Precision | Recall | F1-Score | Consistency |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 精确度 | 召回率 | F1分数 | 一致性 |'
- en: '| Test | None | 0.905 $\pm$ 0.013 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 无 | 0.905 $\pm$ 0.013 |'
- en: '| Step 1 only | 0.886 $\pm$ 0.004 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 仅步骤1 | 0.886 $\pm$ 0.004 |'
- en: '| Step 2 only | 0.907 $\pm$ 0.005 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 仅步骤2 | 0.907 $\pm$ 0.005 |'
- en: '| Step 1 & 2 | 0.891 $\pm$ 0.003 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 步骤1 & 2 | 0.891 $\pm$ 0.003 |'
- en: '| Dev-Test | None | 0.944 $\pm$ 0.003 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 开发测试 | 无 | 0.944 $\pm$ 0.003 |'
- en: '| Step 1 only | 0.929 $\pm$ 0.012 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 仅步骤1 | 0.929 $\pm$ 0.012 |'
- en: '| Step 2 only | 0.953 $\pm$ 0.003 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 仅步骤2 | 0.953 $\pm$ 0.003 |'
- en: '| Step 1 & 2 | 0.95 $\pm$ 0.003 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 步骤1 & 2 | 0.95 $\pm$ 0.003 |'
- en: 'Table [8](#S3.T8 "Table 8 ‣ 3.6 Effect of Data Pruning ‣ 3 Results ‣ FactFinders
    at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through
    Data Pruning") presents the performance of Llama2-7b on the original and pruned
    training data. It can be observed that the model trained using only the step 2
    data pruning approach yields the highest F1-score and accuracy in the dev-test
    partition and its performance in the test partition is close to the model trained
    without any data pruned. While the two-step data pruning approach reaches a slightly
    lower F1 score in the test partition, it stands out as the high recall model in
    both partitions. Further, it is worth noting that this model resulted in a better
    precision-recall trade-off in the dev-test partition compared to the models trained
    with individual pruning steps (step 1 only and step 2 only). Moreover, the consistency
    of the models in predicting the same output class ranged from 0.9-0.95, while
    the lower range is always observed in the dev-test partition. During the testing
    phase of the completion, we could not compare the average performance of the models
    due to time limitations. Therefore, we submitted the class labels predicted by
    the Llama2-7b model fine-tuned without the data pruning process for the CheckThat!
    2024 task1 leader board, as it yielded a slightly higher F1-score compared to
    other approaches. This submission was ranked 1st place in the leaderboard with
    the highest F1-score of 0.802\. While this score is lower than the average F1-score
    reported in Table [8](#S3.T8 "Table 8 ‣ 3.6 Effect of Data Pruning ‣ 3 Results
    ‣ FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with
    LLMs through Data Pruning"), the standard deviation indicates that the model performance
    could vary from 0.801 to 0.839.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '表[8](#S3.T8 "Table 8 ‣ 3.6 Effect of Data Pruning ‣ 3 Results ‣ FactFinders
    at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through
    Data Pruning")展示了Llama2-7b在原始和修剪训练数据上的表现。可以观察到，仅使用第2步数据修剪方法训练的模型在开发测试分区中产生了最高的F1得分和准确率，并且其在测试分区的表现接近于未进行数据修剪训练的模型。虽然两步数据修剪方法在测试分区中达到的F1得分略低，但在两个分区中都表现为高召回模型。此外，值得注意的是，该模型在开发测试分区中的精确度-召回率权衡优于仅使用单独修剪步骤（仅第1步或仅第2步）训练的模型。此外，模型预测相同输出类别的一致性范围从0.9到0.95，而较低的范围总是出现在开发测试分区。在完成测试阶段时，由于时间限制，我们无法比较模型的平均表现。因此，我们提交了未进行数据修剪过程的Llama2-7b模型预测的类别标签，用于CheckThat!
    2024任务1排行榜，因为它产生了略高于其他方法的F1得分。该提交在排行榜中排名第1，F1得分最高为0.802。尽管这个得分低于表[8](#S3.T8 "Table
    8 ‣ 3.6 Effect of Data Pruning ‣ 3 Results ‣ FactFinders at CheckThat! 2024: Refining
    Check-worthy Statement Detection with LLMs through Data Pruning")中报告的平均F1得分，但标准差表明模型性能可能从0.801到0.839不等。'
- en: 'Table 9: Training Data Size and Average Fine-tuning Time with Data Pruning.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：使用数据修剪的训练数据大小和平均微调时间。
- en: '| Pruning Technique | Training data size |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 修剪技术 | 训练数据大小 |'
- en: '&#124; % of data retained &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据保留百分比 &#124;'
- en: '&#124; during pruning &#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在修剪过程中 &#124;'
- en: '| Average Fine-tuning Time |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 平均微调时间 |'
- en: '| None | 22,499 | 100% | 6.3h |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 22,499 | 100% | 6.3小时 |'
- en: '| Step 1 only | 20,141 | 89% | 5.7h |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 仅第1步 | 20,141 | 89% | 5.7小时 |'
- en: '| Step 2 only | 10,227 | 45.5% | 3h |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 仅第2步 | 10,227 | 45.5% | 3小时 |'
- en: '| Step 1 & 2 | 9,907 | 44% | 2.9h |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 第1步和第2步 | 9,907 | 44% | 2.9小时 |'
- en: 'Table [9](#S3.T9 "Table 9 ‣ 3.6 Effect of Data Pruning ‣ 3 Results ‣ FactFinders
    at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through
    Data Pruning") reports the training data size and their corresponding average
    fine-tuning time. This demonstrates that using data pruning strategies allows
    competitive performance on both test and dev-test data while utilizing only about
    44%-44.5% of the original training data. Further, the fine-tuning time is reduced
    in a similar proportion, cutting the training time by more than half compared
    to using the original training data. This indicates that obtaining high-quality
    training data is crucial for developing effective check-worthy statement detection
    models from political transcriptions.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '表[9](#S3.T9 "Table 9 ‣ 3.6 Effect of Data Pruning ‣ 3 Results ‣ FactFinders
    at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through
    Data Pruning")报告了训练数据的大小及其对应的平均微调时间。这表明，使用数据修剪策略可以在仅使用约44%-44.5%的原始训练数据的情况下，在测试数据和开发测试数据上获得竞争力的性能。此外，微调时间以类似比例减少，使训练时间比使用原始训练数据减少了一半以上。这表明，获取高质量的训练数据对于从政治记录中开发有效的检查-worthy语句检测模型至关重要。'
- en: 4 Conclusion
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: This paper demonstrates the experiments conducted by the FactFinders team for
    CheckThat! 2024 task 1, check-worthiness estimation in English. We experimented
    with eight open-source LLMs with fine-tuning and prompt engineering to identify
    check-worthy statement detection from political transcriptions. Results using
    our Llama2-7b model fine-tuned on the training data secured the 1st position in
    the leaderboard among a total of 26 participants, with F1-scores surpassing the
    baseline for the task. This demonstrates that open-source models are powerful
    in check-worthy statement detection in the English language. Further, we demonstrated
    the role of data pruning in identifying high-quality training data for effective
    learning. Our results show that competitive or better performance can be obtained
    by utilizing only about 44% of training data by saving the fine-tuning time in
    a similar proportion. Apart from the fine-tuned LLMs for check-worthy statement
    detection, we utilized LLMs for refining prompts and identifying informative verbs
    in zero-shot setting.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了FactFinders团队为CheckThat! 2024任务1（英语中的检查价值评估）进行的实验。我们使用了八个开源LLM，通过微调和提示工程来识别政治转录中的检查价值声明。使用我们在训练数据上微调的Llama2-7b模型的结果在26个参与者中获得了排行榜第一名，F1分数超过了任务的基准。这表明开源模型在英语中的检查价值声明检测方面非常强大。此外，我们展示了数据修剪在识别高质量训练数据以实现有效学习中的作用。我们的结果显示，通过仅使用约44%的训练数据，可以获得具有竞争力或更好的表现，同时节省了相应比例的微调时间。除了微调的LLM用于检查价值声明检测外，我们还利用LLM来精炼提示和在零样本设置中识别有用的动词。
- en: The key challenges we faced while utilizing LLMs for check-worthy statement
    detection were the memory requirements and their inconsistent response in predicting
    the class label for the same input statement. We used the Low-Rank Adaption technique
    (LoRA) for effective fine-tuning with low GPU memory usage to conquer the memory
    requirements. While we tried to overcome the consistency issue by running the
    fine-tuned models 5 times and obtaining the majority prediction, our consistency
    analysis reveals that the consistency score itself is unstable during early iterations,
    and may have a drop of around 6% while reaching stability. This behavior of LLMs
    highly questions their adaptation for classification tasks in general as inconsistent
    responses may result in less reproducible results.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在利用LLM进行检查价值声明检测时面临的主要挑战是内存要求及其在预测相同输入声明的类别标签时的不一致响应。我们使用了低秩适配技术（LoRA）来进行有效的微调，减少GPU内存使用，从而解决了内存需求问题。虽然我们通过运行微调模型5次并获取多数预测来克服一致性问题，但我们的连续性分析显示，一致性分数在早期迭代中本身是不稳定的，可能会在达到稳定性时下降约6%。LLM的这种行为极大地质疑了它们在分类任务中的适应性，因为不一致的响应可能导致结果的可重复性较差。
- en: Acknowledgments
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: Rrubaa Panchendrarajan is funded by the European Union and UK Research and Innovation
    under Grant No. 101073351 as part of Marie Skłodowska-Curie Actions (MSCA Hybrid
    Intelligence to monitor, promote, and analyze transformations in good democracy
    practices). Yufeng Li is funded by China Scholarship Council (CSC).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Rrubaa Panchendrarajan由欧盟和英国研究与创新资助，资助号为101073351，作为Marie Skłodowska-Curie Actions（MSCA混合智能监测、促进和分析良好民主实践中的变革）的一部分。Yufeng
    Li由中国国家留学基金委（CSC）资助。
- en: References
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Zeng et al. [2021] X. Zeng, A. S. Abumansour, A. Zubiaga, Automated fact-checking:
    A survey, Language and Linguistics Compass 15 (2021) e12438.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng et al. [2021] X. Zeng, A. S. Abumansour, A. Zubiaga, 自动化事实核查：综述, 语言与语言学指南
    15 (2021) e12438.
- en: Guo et al. [2022] Z. Guo, M. Schlichtkrull, A. Vlachos, A survey on automated
    fact-checking, Transactions of the Association for Computational Linguistics 10
    (2022) 178–206.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. [2022] Z. Guo, M. Schlichtkrull, A. Vlachos, 自动化事实核查综述, 计算语言学协会会刊
    10 (2022) 178–206.
- en: Allcott et al. [2019] H. Allcott, M. Gentzkow, C. Yu, Trends in the diffusion
    of misinformation on social media, Research & Politics 6 (2019) 2053168019848554.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allcott et al. [2019] H. Allcott, M. Gentzkow, C. Yu, 社交媒体上误信息扩散的趋势, 研究与政治 6
    (2019) 2053168019848554.
- en: 'Panchendrarajan and Zubiaga [2024] R. Panchendrarajan, A. Zubiaga, Claim detection
    for automated fact-checking: A survey on monolingual, multilingual and cross-lingual
    research, Natural Language Processing Journal 7 (2024) 100066.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Panchendrarajan 和 Zubiaga [2024] R. Panchendrarajan, A. Zubiaga, 自动化事实核查中的声明检测：单语、多语和跨语研究的综述,
    自然语言处理杂志 7 (2024) 100066.
- en: 'Nakov et al. [2018] P. Nakov, A. Barrón-Cedeno, T. Elsayed, R. Suwaileh, L. Màrquez,
    W. Zaghouani, P. Atanasova, S. Kyuchukov, G. Da San Martino, Overview of the clef-2018
    checkthat! lab on automatic identification and verification of political claims,
    in: Experimental IR Meets Multilinguality, Multimodality, and Interaction: 9th
    International Conference of the CLEF Association, CLEF 2018, Avignon, France,
    September 10-14, 2018, Proceedings 9, Springer, 2018, pp. 372–387.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakov 等人 [2018] P. Nakov, A. Barrón-Cedeno, T. Elsayed, R. Suwaileh, L. Màrquez,
    W. Zaghouani, P. Atanasova, S. Kyuchukov, G. Da San Martino, 关于 clef-2018 checkthat!
    实验室在政治声明自动识别和验证中的概述，见：实验 IR 遇见多语言性、多模态性和互动：第 9 届 CLEF 协会国际会议，CLEF 2018，法国阿维尼翁，2018
    年 9 月 10-14 日，论文集 9，Springer，2018 年，第 372–387 页。
- en: 'Atanasova et al. [2018] P. Atanasova, A. Barron-Cedeno, T. Elsayed, R. Suwaileh,
    W. Zaghouani, S. Kyuchukov, G. D. S. Martino, P. Nakov, Overview of the clef-2018
    checkthat! lab on automatic identification and verification of political claims.
    task 1: Check-worthiness, arXiv preprint arXiv:1808.05542 (2018).'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Atanasova 等人 [2018] P. Atanasova, A. Barron-Cedeno, T. Elsayed, R. Suwaileh,
    W. Zaghouani, S. Kyuchukov, G. D. S. Martino, P. Nakov, 关于 clef-2018 checkthat!
    实验室在政治声明自动识别和验证中的概述。任务 1: 检查价值，arXiv 预印本 arXiv:1808.05542 (2018)。'
- en: 'Atanasova et al. [2019] P. Atanasova, P. Nakov, G. Karadzhov, M. Mohtarami,
    G. Da San Martino, Overview of the clef-2019 checkthat! lab: Automatic identification
    and verification of claims. task 1: Check-worthiness. (2019).'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Atanasova 等人 [2019] P. Atanasova, P. Nakov, G. Karadzhov, M. Mohtarami, G.
    Da San Martino, 关于 clef-2019 checkthat! 实验室的概述：声明的自动识别和验证。任务 1: 检查价值。(2019)。'
- en: Vaswani et al. [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, Ł. Kaiser, I. Polosukhin, Attention is all you need, Advances in
    neural information processing systems 30 (2017).
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人 [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, Ł. Kaiser, I. Polosukhin, 注意力即你所需，神经信息处理系统进展 30 (2017)。
- en: 'Shaar et al. [2020] S. Shaar, A. Nikolov, N. Babulkov, F. Alam, A. Barrón-Cedeno,
    T. Elsayed, M. Hasanain, R. Suwaileh, F. Haouari, G. Da San Martino, et al., Overview
    of checkthat! 2020 english: Automatic identification and verification of claims
    in social media., CLEF (working notes) 2696 (2020).'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaar 等人 [2020] S. Shaar, A. Nikolov, N. Babulkov, F. Alam, A. Barrón-Cedeno,
    T. Elsayed, M. Hasanain, R. Suwaileh, F. Haouari, G. Da San Martino 等人，关于 checkthat!
    2020 英语版：社交媒体中声明的自动识别和验证，CLEF（工作笔记）2696 (2020)。
- en: 'Shaar et al. [2021] S. Shaar, M. Hasanain, B. Hamdan, Z. S. Ali, F. Haouari,
    A. Nikolov, M. Kutlu, Y. S. Kartal, F. Alam, G. Da San Martino, et al., Overview
    of the clef-2021 checkthat! lab task 1 on check-worthiness estimation in tweets
    and political debates., in: CLEF (working notes), 2021, pp. 369–392.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaar 等人 [2021] S. Shaar, M. Hasanain, B. Hamdan, Z. S. Ali, F. Haouari, A.
    Nikolov, M. Kutlu, Y. S. Kartal, F. Alam, G. Da San Martino 等人，关于 clef-2021 checkthat!
    实验室任务 1 的概述，涉及推文和政治辩论中的检查价值估计，见：CLEF（工作笔记），2021 年，第 369–392 页。
- en: 'Nakov et al. [2022] P. Nakov, A. Barrón-Cedeño, G. Da San Martino, F. Alam,
    R. Míguez, T. Caselli, M. Kutlu, W. Zaghouani, C. Li, S. Shaar, et al., Overview
    of the clef-2022 checkthat! lab task 1 on identifying relevant claims in tweets,
    in: 2022 Conference and Labs of the Evaluation Forum, CLEF 2022, CEUR Workshop
    Proceedings (CEUR-WS. org), 2022, pp. 368–392.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakov 等人 [2022] P. Nakov, A. Barrón-Cedeño, G. Da San Martino, F. Alam, R. Míguez,
    T. Caselli, M. Kutlu, W. Zaghouani, C. Li, S. Shaar 等人，关于 clef-2022 checkthat!
    实验室任务 1 的概述，涉及推文中的相关声明识别，见：2022 年评估论坛会议和实验室，CLEF 2022，CEUR 工作论文集 (CEUR-WS.org)，2022
    年，第 368–392 页。
- en: Brown et al. [2020] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
    A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models are few-shot
    learners, Advances in neural information processing systems 33 (2020) 1877–1901.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
    A. Neelakantan, P. Shyam, G. Sastry, A. Askell 等人，语言模型是少样本学习者，神经信息处理系统进展 33 (2020)
    1877–1901。
- en: 'Agresti et al. [2022] S. Agresti, S. A. Hashemian, M. J. Carman, Polimi-flatearthers
    at checkthat!-2022: Gpt-3 applied to claim detection., in: CLEF (Working Notes),
    2022, pp. 422–427.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Agresti 等人 [2022] S. Agresti, S. A. Hashemian, M. J. Carman, Polimi-flatearthers
    在 checkthat!-2022: 应用于声明检测的 GPT-3，见：CLEF（工作笔记），2022 年，第 422–427 页。'
- en: 'Sawiński et al. [2023] M. Sawiński, K. Węcel, E. P. Księżniak, M. Stróżyna,
    W. Lewoniewski, P. Stolarski, W. Abramowicz, Openfact at checkthat! 2023: head-to-head
    gpt vs. bert-a comparative study of transformers language models for the detection
    of check-worthy claims, Working Notes of CLEF (2023).'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sawiński 等人 [2023] M. Sawiński, K. Węcel, E. P. Księżniak, M. Stróżyna, W.
    Lewoniewski, P. Stolarski, W. Abramowicz, Openfact 在 checkthat! 2023: GPT 与 BERT
    的正面比较——用于检测检查价值声明的变换器语言模型的比较研究，CLEF 工作笔记 (2023)。'
- en: 'Barrón-Cedeño et al. [2024] A. Barrón-Cedeño, F. Alam, J. M. Struß, P. Nakov,
    T. Chakraborty, T. Elsayed, P. Przybyła, T. Caselli, G. Da San Martino, F. Haouari,
    C. Li, J. Piskorski, F. Ruggeri, X. Song, R. Suwaileh, Overview of the CLEF-2024
    CheckThat! Lab: Check-worthiness, subjectivity, persuasion, roles, authorities
    and adversarial robustness, in: L. Goeuriot, P. Mulhem, G. Quénot, D. Schwab,
    L. Soulier, G. M. Di Nunzio, P. Galuščáková, A. García Seco de Herrera, G. Faggioli,
    N. Ferro (Eds.), Experimental IR Meets Multilinguality, Multimodality, and Interaction.
    Proceedings of the Fifteenth International Conference of the CLEF Association
    (CLEF 2024), 2024.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barrón-Cedeño et al. [2024] A. Barrón-Cedeño, F. Alam, J. M. Struß, P. Nakov,
    T. Chakraborty, T. Elsayed, P. Przybyła, T. Caselli, G. Da San Martino, F. Haouari,
    C. Li, J. Piskorski, F. Ruggeri, X. Song, R. Suwaileh, CLEF-2024 CheckThat! 实验室概述：检查价值、主观性、说服力、角色、权威和对抗性鲁棒性，见：L.
    Goeuriot, P. Mulhem, G. Quénot, D. Schwab, L. Soulier, G. M. Di Nunzio, P. Galuščáková,
    A. García Seco de Herrera, G. Faggioli, N. Ferro (编), 实验信息检索与多语言性、多模态性和交互. 第十五届
    CLEF 协会国际会议论文集 (CLEF 2024), 2024.
- en: 'Hasanain et al. [2024] M. Hasanain, R. Suwaileh, S. Weering, C. Li, T. Caselli,
    W. Zaghouani, A. Barrón-Cedeño, P. Nakov, F. Alam, Overview of the CLEF-2024 CheckThat!
    lab task 1 on check-worthiness estimation of multigenre content, in: G. Faggioli,
    N. Ferro, P. Galuščáková, A. García Seco de Herrera (Eds.), Working Notes of CLEF
    2024 - Conference and Labs of the Evaluation Forum, CLEF 2024, Grenoble, France,
    2024.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasanain et al. [2024] M. Hasanain, R. Suwaileh, S. Weering, C. Li, T. Caselli,
    W. Zaghouani, A. Barrón-Cedeño, P. Nakov, F. Alam, CLEF-2024 CheckThat! 实验室任务
    1 概述：多体裁内容的检查价值评估，见：G. Faggioli, N. Ferro, P. Galuščáková, A. García Seco de Herrera
    (编), CLEF 2024 工作笔记 - 评估论坛的会议和实验室, CLEF 2024, 法国格勒诺布尔, 2024.
- en: 'Touvron et al. [2023] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,
    Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Open
    foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288 (2023).'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. [2023] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,
    Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2：开放基础和微调聊天模型,
    arXiv 预印本 arXiv:2307.09288 (2023).
- en: Jiang et al. [2023] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S.
    Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al.,
    Mistral 7b, arXiv preprint arXiv:2310.06825 (2023).
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. [2023] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D.
    S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et
    al., Mistral 7b, arXiv 预印本 arXiv:2310.06825 (2023).
- en: Jiang et al. [2024] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary,
    C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al., Mixtral
    of experts, arXiv preprint arXiv:2401.04088 (2024).
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. [2024] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary,
    C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al., 专家混合模型,
    arXiv 预印本 arXiv:2401.04088 (2024).
- en: 'Abdin et al. [2024] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah,
    H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, et al., Phi-3 technical
    report: A highly capable language model locally on your phone, arXiv preprint
    arXiv:2404.14219 (2024).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdin et al. [2024] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah,
    H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, et al., Phi-3 技术报告：在您的手机上运行的高度能力语言模型,
    arXiv 预印本 arXiv:2404.14219 (2024).
- en: Almazrouei et al. [2023] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli,
    R. Cojocaru, M. Debbah, É. Goffinet, D. Hesslow, J. Launay, Q. Malartic, et al.,
    The falcon series of open language models, arXiv preprint arXiv:2311.16867 (2023).
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Almazrouei et al. [2023] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli,
    R. Cojocaru, M. Debbah, É. Goffinet, D. Hesslow, J. Launay, Q. Malartic, et al.,
    Falcon 系列开放语言模型, arXiv 预印本 arXiv:2311.16867 (2023).
- en: 'Team et al. [2024] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju,
    S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love, et al., Gemma: Open models
    based on gemini research and technology, arXiv preprint arXiv:2403.08295 (2024).'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team et al. [2024] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju,
    S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love, et al., Gemma：基于双子座研究和技术的开放模型,
    arXiv 预印本 arXiv:2403.08295 (2024).
- en: Hart [1968] P. Hart, The condensed nearest neighbor rule (corresp.), IEEE transactions
    on information theory 14 (1968) 515–516.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hart [1968] P. Hart, 压缩最近邻规则（通讯），IEEE 信息理论交易 14 (1968) 515–516.
- en: 'Devlin et al. [2018] J. Devlin, M. Chang, K. Lee, K. Toutanova, BERT: pre-training
    of deep bidirectional transformers for language understanding, CoRR abs/1810.04805
    (2018). URL: [http://arxiv.org/abs/1810.04805](http://arxiv.org/abs/1810.04805).
    [arXiv:1810.04805](http://arxiv.org/abs/1810.04805).'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. [2018] J. Devlin, M. Chang, K. Lee, K. Toutanova, BERT：用于语言理解的深度双向变换器的预训练,
    CoRR abs/1810.04805 (2018). 网址：[http://arxiv.org/abs/1810.04805](http://arxiv.org/abs/1810.04805).
    [arXiv:1810.04805](http://arxiv.org/abs/1810.04805).
- en: 'King et al. [2017] T. King, S. Butcher, L. Zalewski, Apocrita - High Performance
    Computing Cluster for Queen Mary University of London, 2017\. URL: [https://doi.org/10.5281/zenodo.438045](https://doi.org/10.5281/zenodo.438045).
    doi:[10.5281/zenodo.438045](https:/doi.org/10.5281/zenodo.438045).'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'King等人 [2017] T. King, S. Butcher, L. Zalewski, Apocrita - 伦敦玛丽女王大学的高性能计算集群，2017\.
    URL: [https://doi.org/10.5281/zenodo.438045](https://doi.org/10.5281/zenodo.438045).
    doi:[10.5281/zenodo.438045](https:/doi.org/10.5281/zenodo.438045).'
