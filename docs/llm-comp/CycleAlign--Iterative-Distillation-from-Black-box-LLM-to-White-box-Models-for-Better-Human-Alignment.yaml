- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:59:35'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:59:35'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for
    Better Human Alignment'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'CycleAlign: 从黑箱LLM到白箱模型的迭代蒸馏以实现更好的与人类对齐'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.16271](https://ar5iv.labs.arxiv.org/html/2310.16271)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.16271](https://ar5iv.labs.arxiv.org/html/2310.16271)
- en: Jixiang Hong ¹, Quan Tu^(∗1), Changyu Chen¹, Xing Gao², Ji Zhang², Rui Yan^(†1,3)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jixiang Hong ¹, Quan Tu^(∗1), Changyu Chen¹, Xing Gao², Ji Zhang², Rui Yan^(†1,3)
- en: ¹Gaoling School of Artificial Intelligence, Renmin University of China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹中国人民大学高龄人工智能学院
- en: ²Alibaba DAMO Academy
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²阿里巴巴达摩院
- en: ³Engineering Research Center of Next-Generation Intelligent Search and Recommendation,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³下一代智能搜索与推荐工程研究中心
- en: Ministry of Education
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 教育部
- en: 'jxhong@ruc.edu.cn, quantu@ruc.edu.cn Equal contribution, ^† Corresponding author:
    Rui Yan (ruiyan@ruc.edu.cn)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 'jxhong@ruc.edu.cn, quantu@ruc.edu.cn 贡献相等，^† 通讯作者: Rui Yan (ruiyan@ruc.edu.cn)'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Language models trained on large-scale corpus often exhibit a propensity for
    generating content that is harmful, toxic, or contrary to human preferences, making
    their alignment with human values a critical concern. A prevalent approach for
    achieving this alignment has been reinforcement learning from human feedback (RLHF),
    utilizing algorithms such as proximal policy optimization (PPO). However, these
    methods are often characterized by complexity, instability, and substantial resource
    consumption. Recently, ranking-based alignment methods have emerged, offering
    stability and effectiveness by replacing the RL framework with supervised fine-tuning,
    but they are costly due to the need for annotated data. Considering that existing
    large language models (LLMs) like ChatGPT are already relatively well-aligned
    and cost-friendly, researchers have begun to align the language model with human
    preference from AI feedback. The common practices, which unidirectionally distill
    the instruction-following responses from LLMs, are constrained by their bottleneck.
    To address this, we introduce CycleAlign to distill alignment capabilities from
    parameter-invisible LLMs (black-box) to a parameter-visible model (white-box)
    in an iterative manner. With in-context learning (ICL) as the core of the cycle,
    the black-box models are able to rank the model-generated responses guided by
    human-craft instruction and demonstrations about their preferences. During iterative
    interaction, the white-box models also have a judgment about responses generated
    by them. Consequently, the agreement ranking could be viewed as a pseudo label
    to dynamically update the in-context demonstrations and improve the preference
    ranking ability of black-box models. Through multiple interactions, the CycleAlign
    framework could align the white-box model with the black-box model effectively
    in a low-resource way. Empirical results illustrate that the model fine-tuned
    by CycleAlign remarkably exceeds existing methods, and achieves the state-of-the-art
    performance in alignment with human value.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 训练于大规模语料库上的语言模型往往表现出生成有害、毒性或与人类偏好相悖的内容的倾向，使得与人类价值观的一致性成为一个关键问题。一种实现这种一致性的普遍方法是基于人类反馈的强化学习（RLHF），利用诸如近端策略优化（PPO）等算法。然而，这些方法通常以复杂性、不稳定性和巨大的资源消耗为特征。最近，基于排名的对齐方法应运而生，通过用监督微调替代RL框架，提供了稳定性和有效性，但由于需要标注数据，它们的成本较高。考虑到现有的大型语言模型（LLMs）如ChatGPT已经相对较好地对齐且成本友好，研究人员已开始从AI反馈中对语言模型进行与人类偏好的对齐。常见的做法通过单向提炼LLMs的指令遵循响应，但这些做法受限于其瓶颈。为了解决这个问题，我们引入了CycleAlign，通过迭代的方式将从参数不可见的LLMs（黑箱）提炼对齐能力到参数可见的模型（白箱）。以上下文学习（ICL）为周期核心，黑箱模型能够根据人类制定的指令和关于其偏好的示例对模型生成的响应进行排名。在迭代交互过程中，白箱模型也会对它们生成的响应进行判断。因此，协议排名可以视为伪标签，用于动态更新上下文示例，提升黑箱模型的偏好排名能力。通过多次交互，CycleAlign框架能够以低资源的方式有效地将白箱模型与黑箱模型对齐。实证结果表明，通过CycleAlign微调的模型显著超越了现有方法，并在与人类价值观的一致性上达到了最先进的性能。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Large language models (LLMs) have demonstrated superior capabilities in processing
    complicated tasks, which is attributed to the large amount of training corpus
    and model parameters (Brown et al., [2020](#bib.bib3); Bubeck et al., [2023](#bib.bib4);
    Chowdhery et al., [2022](#bib.bib5); Touvron et al., [2023a](#bib.bib25); [b](#bib.bib26);
    Du et al., [2021](#bib.bib7); OpenAI, [2023](#bib.bib16)). Nevertheless, models
    trained on the corpus collected from diverse web sources could not be effectively
    guided, and are prone to generate harmful, toxic and criminal contents (Bai et al.,
    [2022b](#bib.bib2); Ouyang et al., [2022](#bib.bib17)). Therefore, aligning these
    language models with desirable human preferences such as harmlessness, helpfulness,
    and honesty has emerged as a pivotal focus in the ongoing research.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在处理复杂任务方面展现了卓越的能力，这归因于大量的训练语料和模型参数（Brown et al., [2020](#bib.bib3);
    Bubeck et al., [2023](#bib.bib4); Chowdhery et al., [2022](#bib.bib5); Touvron
    et al., [2023a](#bib.bib25); [b](#bib.bib26); Du et al., [2021](#bib.bib7); OpenAI,
    [2023](#bib.bib16)）。然而，基于从多样化网络资源收集的语料进行训练的模型，难以有效指导，容易生成有害、毒性及犯罪内容（Bai et al.,
    [2022b](#bib.bib2); Ouyang et al., [2022](#bib.bib17)）。因此，使这些语言模型与无害、有效和诚实等理想人类偏好对齐已成为当前研究的关键焦点。
- en: Reinforcement learning from human feedback (RLHF) has been employed to align
    language models with human preferences by Ouyang et al. ([2022](#bib.bib17)).
    Generally, the popular RL method PPO (Schulman et al., [2017](#bib.bib22)) is
    utilized to optimize the foundation language model, with a reward model as the
    guidance. However, its complex architecture proposes a challenge for hardware
    devices in the LLM period and has the unstable property during training.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人类反馈的强化学习（RLHF）已经被用于将语言模型与人类偏好对齐，由Ouyang et al.（[2022](#bib.bib17)）进行。通常，流行的RL方法PPO（Schulman
    et al., [2017](#bib.bib22)）被用于优化基础语言模型，以奖励模型作为指导。然而，其复杂的架构对硬件设备提出了挑战，并且在训练过程中具有不稳定性。
- en: Recently, the emergence of ranking-based alignment methods has resolved the
    stability and hardware-consumption problems through shifting from the RL framework
    to supervised fine-tuning (Song et al., [2023](#bib.bib23); Rafailov et al., [2023](#bib.bib19);
    Yuan et al., [2023](#bib.bib32)). Nevertheless, the need for extensively annotated
    data renders them costly and labor-intensive.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于排名的对齐方法通过从RL框架转向监督微调解决了稳定性和硬件消耗问题（Song et al., [2023](#bib.bib23); Rafailov
    et al., [2023](#bib.bib19); Yuan et al., [2023](#bib.bib32)）。然而，对大量注释数据的需求使得这些方法成本高昂且劳动密集。
- en: Considering existing LLMs like ChatGPT are well aligned, the reinforcement learning
    from AI feedback (RLAIF) methods are proposed to introduce automatic AI supervising
    signals (Bai et al., [2022b](#bib.bib2); Kim et al., [2023](#bib.bib9)) to replace
    the manual annotation. However, common practices that distill instruction-following
    responses from LLMs in a unidirectional manner are limited by inherent bottlenecks.
    To address this, we propose a novel framework CycleAlign to better align the parameter-visible
    white-box model with the parameter-invisible black-box model by iterative interactions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到现有的LLM如ChatGPT已得到良好对齐，提出了基于AI反馈的强化学习（RLAIF）方法，引入自动AI监督信号（Bai et al., [2022b](#bib.bib2);
    Kim et al., [2023](#bib.bib9)）来替代手动注释。然而，以单向方式从LLM中提炼指令跟随响应的常见做法受限于固有瓶颈。为此，我们提出了一种新颖的框架CycleAlign，通过迭代交互来更好地将参数可见的白箱模型与参数不可见的黑箱模型对齐。
- en: 'As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ CycleAlign: Iterative
    Distillation from Black-box LLM to White-box Models for Better Human Alignment"),
    we introduce the in-context learning (ICL) (Min et al., [2022](#bib.bib15); Rubin
    et al., [2021](#bib.bib21); Ren et al., [2021](#bib.bib20)) as the pivot to break
    the bottleneck of black-box models.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[1](#S1.F1 "图 1 ‣ 1 引言 ‣ CycleAlign：从黑箱LLM到白箱模型的迭代蒸馏以实现更好的人工对齐")所示，我们引入了上下文学习（ICL）（Min
    et al., [2022](#bib.bib15); Rubin et al., [2021](#bib.bib21); Ren et al., [2021](#bib.bib20)）作为打破黑箱模型瓶颈的关键。
- en: '![Refer to caption](img/c6122d437adc8d4e634a285d55c36e51.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c6122d437adc8d4e634a285d55c36e51.png)'
- en: 'Figure 1: Comparison between CycleAign with existing unidirectional distillation
    frameworks.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：CycleAlign与现有单向蒸馏框架的比较。
- en: For a given instruction, we prompt the white-box model to generate multiple
    responses. Then, the black-box model ranks these responses with the help of the
    human-craft ranking prompt and static in-context demonstration. The ranking signal
    will be utilized to optimize the white-box model and help it generate more harmless
    and helpful responses. Additionally, the generated probability of responses could
    be deemed as a ranking judgment from the aspect of the white-box model. Combining
    the judgment from the white-box model and black-box model, we could extract the
    consistent rank as the pseudo label and feed it to the latter as the dynamic demonstration.
    As we know, LLMs will perform better with the number of in-context demonstrations
    increasing (Brown et al., [2020](#bib.bib3)). Consequently, the black-box model
    could give a more correct ranking to supervise the white-box model equipped with
    the dynamically increasing demonstrations. When the cycle between the white- and
    black- box model begins to run, both of them will benefit from each other. At
    last, the alignment performance with human preference of the white-box model will
    be improved with the help of an unlocked black-box model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的指令，我们促使白盒模型生成多个回应。然后，黑盒模型在人工排名提示和静态上下文演示的帮助下对这些回应进行排序。排序信号将用于优化白盒模型，帮助其生成更多无害且有帮助的回应。此外，回应的生成概率可以视为来自白盒模型的排序判断。结合白盒模型和黑盒模型的判断，我们可以提取一致的排名作为伪标签，并将其作为动态演示提供给后者。正如我们所知，LLMs
    在上下文演示数量增加时表现更佳（Brown et al., [2020](#bib.bib3)）。因此，黑盒模型可以为配备动态增加演示的白盒模型提供更准确的排名。当白盒和黑盒模型之间的循环开始运行时，它们将相互受益。最终，借助解锁的黑盒模型，白盒模型的人类偏好对齐性能将得到提升。
- en: We conduct experiments on the human preference dataset HH-RLHF (Bai et al.,
    [2022a](#bib.bib1)) to investigate the effectiveness of CycleAlign regarding helpfulness
    and harmlessness. Compared with the previous methods, CycleAlign improves the
    alignment ability and takes state-of-the-art performance in generating harmless
    and helpful responses.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在人工偏好数据集 HH-RLHF（Bai et al., [2022a](#bib.bib1)）上进行实验，以研究 CycleAlign 在有帮助性和无害性方面的有效性。与以前的方法相比，CycleAlign
    提高了对齐能力，并在生成无害和有帮助的回应方面表现出最先进的性能。
- en: 'In summary, our main contributions are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们的主要贡献如下：
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present a new framework CycleAlign, which utilizes collaboration between
    black-box LLMs and white-box models, to replace the human feedback with AI feedback
    in an iterative manner.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个新框架 CycleAlign，该框架利用黑盒 LLM 和白盒模型之间的协作，以迭代方式用 AI 反馈替代人工反馈。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We enhance the black-box model’s ranking results by employing static and dynamic
    in-context demonstrations in under the interactive scenario.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过在互动场景中使用静态和动态上下文演示来增强黑盒模型的排名结果。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The experimental results indicate the effectiveness of the CycleAlign framework
    in generating harmless and helpful responses.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验结果表明，CycleAlign 框架在生成无害且有帮助的回应方面是有效的。
- en: '![Refer to caption](img/230111c05f8ec51254667aaae25ff701.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/230111c05f8ec51254667aaae25ff701.png)'
- en: 'Figure 2: Overview of CycleAlign framework. For each step: 1) sample responses
    from the white-box model; 2) obtain ranking results from two models respectively;
    3) optimize the white-box model using a ranking-based objective; 4) compare the
    two ranking results, find agreement rank and feed it as the demonstrations to
    black-box model; 5) repeat the above process up to max interactions $N$ times
    or until the black- and white- box model are completely consistent.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：CycleAlign 框架概述。每一步：1）从白盒模型中采样回应；2）分别从两个模型中获得排名结果；3）使用基于排名的目标优化白盒模型；4）比较两个排名结果，找到一致排名并将其作为演示提供给黑盒模型；5）重复上述过程，直到最大交互次数
    $N$ 次或直到黑盒和白盒模型完全一致。
- en: 2 Methodology
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: 'In this section, we describe our training framework which facilitates the collaboration
    between black-box and white-box models to achieve alignment with human preferences.
    The overview of our framework is illustrated in Figure [2](#S1.F2 "Figure 2 ‣
    1 Introduction ‣ CycleAlign: Iterative Distillation from Black-box LLM to White-box
    Models for Better Human Alignment").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了我们的训练框架，该框架促进了黑盒和白盒模型之间的协作，以实现与人类偏好的对齐。我们的框架概述如图 [2](#S1.F2 "图 2 ‣
    1 引言 ‣ CycleAlign：从黑盒 LLM 到白盒模型的迭代蒸馏，以更好地对齐人类") 所示。
- en: We will detail our methodology in the following content.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的内容中详细介绍我们的方法。
- en: 2.1 Cyclical Collaborative Framework for Human Alignment
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 人工对齐的循环协作框架
- en: To alleviate the complication of the RL algorithm and the costly human labels,
    we replace human feedback with AI feedback from the black-box LLM (i.e. ChatGPT)
    and use supervised fine-tuning to train the white-box model. Existing methods
    only distill preference knowledge unidirectionally from aligned models to unaligned
    ones, ignoring the benefits of unaligned model feedback to alignment. We design
    a cyclical framework of collaboration between black-box and white-box models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻强化学习算法的复杂性和昂贵的人力标签，我们用来自黑箱 LLM（即 ChatGPT）的 AI 反馈代替人工反馈，并使用监督微调来训练白箱模型。现有方法仅单向地从对齐模型中提取偏好知识到未对齐模型中，忽视了未对齐模型反馈对对齐的好处。我们设计了一个黑箱和白箱模型之间的循环协作框架。
- en: 'The framework is shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ CycleAlign:
    Iterative Distillation from Black-box LLM to White-box Models for Better Human
    Alignment"). For each interaction, we prompt the white-box model to generate multiple
    different responses to a given instruction. The multiple responses have different
    degrees of alignment with human preferences. Thus, there will be a ranking based
    on their alignment degrees. The black-box model has the capability of ranking
    them. We feed the black-box model with the prompt and corresponding responses
    with ICL demonstrations to instruct it to return a ranking of the responses as
    well as a better response for supervised fine-tuning (SFT). On one side, the white-box
    model is optimized based on the ranking returned from the black-box model to learn
    the human preferences. On the other side, the white-box model can rank the responses
    on its own by computing their probabilities. This is a kind of feedback from the
    white-box model. We utilize this feedback to update ICL demonstrations to help
    the black-box model to rank responses. This process forms a cyclical collaboration,
    which loops for up to $N$ times for each step. By employing this cyclical collaborative
    framework, the white-box is quickly and effectively aligned with human preferences.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 框架如图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ CycleAlign：从黑箱 LLM 到白箱模型的迭代蒸馏以实现更好的人工对齐") 所示。对于每次交互，我们提示白箱模型生成多个不同的响应来处理给定的指令。这些多个响应与人工偏好的对齐程度不同。因此，将根据其对齐程度进行排序。黑箱模型有能力对其进行排序。我们将提示和相应的响应与
    ICL 演示一起提供给黑箱模型，以指导其返回响应的排序以及用于监督微调（SFT）的更好响应。一方面，白箱模型基于黑箱模型返回的排序进行优化，以学习人工偏好。另一方面，白箱模型可以通过计算响应的概率来自行排序。这是一种来自白箱模型的反馈。我们利用这一反馈更新
    ICL 演示，以帮助黑箱模型对响应进行排序。这个过程形成了一个循环协作，每个步骤最多循环 $N$ 次。通过采用这一循环协作框架，白箱模型可以迅速有效地与人工偏好对齐。
- en: 2.2 In-context Learning and Dynamic Demonstrations
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 上下文学习和动态演示
- en: '![Refer to caption](img/d226405a46ae875d3c1fab4d7d57b209.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d226405a46ae875d3c1fab4d7d57b209.png)'
- en: 'Figure 3: The prompt designed for instructing the black-box model (ChatGPT
    in this work) to rank the responses. In the prompts, we employ ICL with the static
    and dynamic demonstrations. The slots, $<$, are replaced with corresponding content
    before being fed into the model. Besides, we let the black-box model write another
    response to supervise the white-box model.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：用于指导黑箱模型（本工作中的 ChatGPT）对响应进行排序的提示。在这些提示中，我们使用了具有静态和动态演示的 ICL。槽位 $<$ 被替换为相应的内容，然后输入模型。此外，我们让黑箱模型写出另一个响应来监督白箱模型。
- en: Large language models demonstrate the capability of in-context learning Brown
    et al. ([2020](#bib.bib3)); Xie et al. ([2021](#bib.bib30)); Min et al. ([2022](#bib.bib15)).
    They can learn the patterns hidden within the demonstrations, subsequently returning
    more correct results Dong et al. ([2023](#bib.bib6)). In order to instruct the
    black-box model to return a more correct ranking of the responses, we employ ICL
    with dynamic demonstrations in this process.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型展示了上下文学习的能力 Brown 等人 ([2020](#bib.bib3)); Xie 等人 ([2021](#bib.bib30));
    Min 等人 ([2022](#bib.bib15))。它们可以学习示例中隐藏的模式，从而返回更准确的结果 Dong 等人 ([2023](#bib.bib6))。为了指导黑箱模型返回更准确的响应排序，我们在这个过程中采用了具有动态演示的上下文学习（ICL）。
- en: 'Specifically, we manually crafted a static demonstration first. This demonstration
    can be seen in Appendix [A.1](#A1.SS1 "A.1 Static manually crafted demonstration
    ‣ Appendix A Appendix ‣ CycleAlign: Iterative Distillation from Black-box LLM
    to White-box Models for Better Human Alignment"). Then we continuously update
    the demonstrations during the training process. For a given input, the white-box
    model generates multiple responses and we then can obtain the logits to compute
    probabilities of the responses. We consider the probabilities as the model’s ‘confidences’
    in the responses. According to the confidences, the white-box model can also rank
    the responses. Both models suggest a ranking of the responses. We add the agreement
    ranking to the ICL demonstrations. The reason we do like this is as follows: During
    training, the white-box model is progressively aligned. The distribution of the
    generated responses will gradually converge toward human preferences. The generated
    responses will be more challenging to rank, so ranking these responses will exploit
    the capability of the black-box model. Meanwhile, the white-box model’s ranking
    will be more and more correct in terms of the degree of alignment, making us believe
    that the white-box model’s ranking contains useful signals. We suppose that the
    agreement between the rankings of the white-box model and the black-box model
    can provide insights into the ranking process of the black-box model.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，我们首先手动制作了一个静态演示。这个演示可以在附录 [A.1](#A1.SS1 "A.1 Static manually crafted demonstration
    ‣ Appendix A Appendix ‣ CycleAlign: Iterative Distillation from Black-box LLM
    to White-box Models for Better Human Alignment")中查看。然后我们在训练过程中不断更新演示。对于给定的输入，白箱模型生成多个响应，我们可以获得logits以计算响应的概率。我们将这些概率视为模型对响应的‘信心’。根据这些信心，白箱模型也可以对响应进行排名。两个模型都建议对响应进行排名。我们将一致性排名添加到ICL演示中。我们这样做的原因如下：在训练过程中，白箱模型逐渐对齐。生成的响应分布将逐渐趋近于人类偏好。生成的响应将更难以排名，因此对这些响应进行排名将利用黑箱模型的能力。同时，白箱模型的排名在对齐程度上将越来越准确，使我们相信白箱模型的排名包含有用的信号。我们假设白箱模型和黑箱模型的排名一致性可以为黑箱模型的排名过程提供见解。'
- en: How do we extract the agreement ranking? We assume that the ranking returned
    from black-box LLM is more correct in general. In addition, because responses
    generated by the white-box model continuously improve with training, the ranking
    of responses that align more closely with human preferences has a higher referring
    value for the black-box LLM. So we extract the longest common subsequence of the
    two ranking results with the highest black-box rankings.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何提取一致性排名？我们假设来自黑箱LLM的排名通常更准确。此外，由于白箱模型生成的响应随着训练不断改进，排名与人类偏好更一致的响应对黑箱LLM具有更高的参考价值。因此，我们提取两个排名结果中黑箱排名最高的最长公共子序列。
- en: Our experiment results show that our ICL with dynamic demonstrations enhances
    the correctness of ranking results returned from black-box LLM and achieves better
    alignment performance of the white-box model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验结果表明，具有动态演示的ICL提高了从黑箱LLM返回的排名结果的准确性，并实现了白箱模型更好的对齐性能。
- en: 2.3 Ranking-based Supervised Fine-tuning
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 基于排名的监督微调
- en: Recently, ranking-based supervised fine-tuning methods have been applied for
    alignment as an alternative to RL algorithms. Given a set of responses, human
    preferences can be expressed as a ranking of the responses. Ranking-based SFT
    methods directly incorporate the ranking information into the fine-tuning stage
    of language models (Rafailov et al., [2023](#bib.bib19); Yuan et al., [2023](#bib.bib32);
    Song et al., [2023](#bib.bib23); Wang et al., [2023b](#bib.bib29)). We employ
    the two ranking-based optimization objectives from RRHF (Yuan et al., [2023](#bib.bib32))
    and PRO (Song et al., [2023](#bib.bib23)) to our framework respectively.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于排名的监督微调方法被应用于对齐，作为RL算法的替代方案。给定一组响应，人类偏好可以通过对这些响应进行排名来表达。基于排名的SFT方法直接将排名信息纳入语言模型的微调阶段 (Rafailov
    et al., [2023](#bib.bib19); Yuan et al., [2023](#bib.bib32); Song et al., [2023](#bib.bib23);
    Wang et al., [2023b](#bib.bib29))。我们分别将来自RRHF (Yuan et al., [2023](#bib.bib32))和PRO (Song
    et al., [2023](#bib.bib23))的两个基于排名的优化目标应用于我们的框架。
- en: 'Specifically, for our model $\pi$, the ranking-based supervised fine-tuning
    objective can be formulated as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，对于我们的模型$\pi$，基于排名的监督微调目标可以被表述为：
- en: '|  | $\mathcal{L}=\mathcal{L}_{\mathrm{rank}}+\lambda\mathcal{L}_{\mathrm{sft}}$
    |  | (1) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\mathcal{L}_{\mathrm{rank}}+\lambda\mathcal{L}_{\mathrm{sft}}$
    |  | (1) |'
- en: where
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $\mathcal{L}_{\mathrm{sft}}=-\frac{1}{\lvert y^{1}\rvert}\sum_{t}\log
    P_{\pi}(y^{1}_{t}\lvert x,y^{1}_{<t})$ |  | (2) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\mathrm{sft}}=-\frac{1}{\lvert y^{1}\rvert}\sum_{t}\log
    P_{\pi}(y^{1}_{t}\lvert x,y^{1}_{<t})$ |  | (2) |'
- en: and the $\mathcal{L}_{\mathrm{rank}}$ can be calculated by PRO or RRHF.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 而 $\mathcal{L}_{\mathrm{rank}}$ 可以通过 PRO 或 RRHF 计算。
- en: 3 Settings
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 设置
- en: 3.1 Datasets
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集
- en: 'We conduct experiments on HH-RLHF (Bai et al., [2022a](#bib.bib1))¹¹1[https://github.com/anthropics/hh-rlhf](https://github.com/anthropics/hh-rlhf),
    a human preference dataset about helpfulness and harmlessness. It contains about
    170k dialogues, where each has a context and a pair of responses along with an
    annotated preference label. This dataset contains four subsets, which are Harmless[base],
    Helpful[base], Helpful[online] and Helpful[rejection] respectively. The statistics
    of them can be found in Appendix [A.2](#A1.SS2 "A.2 Statistics of HH-RLHF dataset
    ‣ Appendix A Appendix ‣ CycleAlign: Iterative Distillation from Black-box LLM
    to White-box Models for Better Human Alignment"). We filter the dataset referring
    OpenAssistant’s code²²2[https://github.com/LAION-AI/Open-Assistant](https://github.com/LAION-AI/Open-Assistant).
    In our framework, the performance of the white-box model will become stable after
    being trained on about 1000 examples of data, similar to the previous findings Lee
    et al. ([2023](#bib.bib10)). Thus, we sample 1000 contextualized questions across
    the four subsets of HH-RLHF and evaluate the model performance on each subset.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 HH-RLHF (Bai 等，[2022a](#bib.bib1))¹¹1[https://github.com/anthropics/hh-rlhf](https://github.com/anthropics/hh-rlhf)
    上进行实验，这是一种关于有用性和无害性的人类偏好数据集。该数据集包含大约 170k 对话，每个对话都有一个上下文和一对响应，以及一个注释的偏好标签。数据集包含四个子集，分别是
    Harmless[base]、Helpful[base]、Helpful[online] 和 Helpful[rejection]。这些子集的统计数据可以在附录
    [A.2](#A1.SS2 "A.2 Statistics of HH-RLHF dataset ‣ Appendix A Appendix ‣ CycleAlign:
    Iterative Distillation from Black-box LLM to White-box Models for Better Human
    Alignment") 中找到。我们参考 OpenAssistant 的代码²²2[https://github.com/LAION-AI/Open-Assistant](https://github.com/LAION-AI/Open-Assistant)
    过滤数据集。在我们的框架中，白盒模型在训练约 1000 个数据样本后性能将变得稳定，这与之前的研究 (Lee 等，[2023](#bib.bib10)) 的发现类似。因此，我们从
    HH-RLHF 的四个子集中抽取了 1000 个上下文化问题，并评估每个子集上的模型性能。'
- en: 3.2 Evaluation
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 评估
- en: We use quantitative and qualitative approaches to evaluate the harmlessness
    and helpfulness of a language model. For quantitative evaluation, a well-trained
    reward model are utilized to assess the responses generated by different models
    as previous works (Song et al., [2023](#bib.bib23); Yuan et al., [2023](#bib.bib32)).
    For qualitative evaluation, we employ GPT-4 and human annotator to compare the
    responses based on the criterion of harmlessness and helpfulness. To avoid the
    order bias of compared responses in GPT-4 (Wang et al., [2023a](#bib.bib27); Pezeshkpour
    & Hruschka, [2023](#bib.bib18); Zheng et al., [2023](#bib.bib33)), we shuffle
    the orders of the compared responses and employ chain-of-thought. At last, We
    calculate the average win rates of different models.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用定量和定性方法来评估语言模型的无害性和有用性。对于定量评估，利用经过良好训练的奖励模型来评估不同模型生成的响应，参考了之前的工作 (Song 等，[2023](#bib.bib23);
    Yuan 等，[2023](#bib.bib32))。对于定性评估，我们使用 GPT-4 和人工标注者根据无害性和有用性的标准比较响应。为了避免 GPT-4
    中比较响应的顺序偏差 (Wang 等，[2023a](#bib.bib27); Pezeshkpour & Hruschka，[2023](#bib.bib18);
    Zheng 等，[2023](#bib.bib33))，我们对比较响应的顺序进行洗牌，并采用链式思维。最后，我们计算不同模型的平均胜率。
- en: 3.3 Implementation Details
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 实施细节
- en: The LLaMA-7B (Touvron et al., [2023a](#bib.bib25)) and Alpaca-7B (Taori et al.,
    [2023](#bib.bib24)) are the backbones in our experiment. We apply the CycleAlign
    framework to optimize these two models with the help of DeepSpeed ZeRO-2 (Ren
    et al., [2021](#bib.bib20)). The reward model used for quantitative evaluation
    is trained by OpenAssistant³³3[https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1](https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1).
    We set the weight factor $\lambda$ is set as 5\. All of the experiments are done
    on a single A100 40G GPU.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-7B (Touvron 等，[2023a](#bib.bib25)) 和 Alpaca-7B (Taori 等，[2023](#bib.bib24))
    是我们实验的核心。我们利用 CycleAlign 框架在 DeepSpeed ZeRO-2 (Ren 等，[2021](#bib.bib20)) 的帮助下优化这两个模型。用于定量评估的奖励模型由
    OpenAssistant³³3[https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1](https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1)
    训练得到。我们将权重因子 $\lambda$ 设置为 5。所有实验均在单个 A100 40G GPU 上完成。
- en: 3.4 Baselines
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 基线
- en: We compare our CycleAlign  with zero-shot baselines including LLaMA-7B (Touvron
    et al., [2023a](#bib.bib25)), Alpaca-7B (Taori et al., [2023](#bib.bib24)), ChatGLM-6B (Du
    et al., [2022](#bib.bib8)) and ChatGPT.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将**CycleAlign**与包括**LLaMA-7B**（Touvron et al., [2023a](#bib.bib25)）、**Alpaca-7B**（Taori
    et al., [2023](#bib.bib24)）、**ChatGLM-6B**（Du et al., [2022](#bib.bib8)）和**ChatGPT**在内的零样本基线进行比较。
- en: LLaMA-7B (Touvron et al., [2023a](#bib.bib25)) LLaMA is a collection of foundation
    language models ranging from 7 billion to 65 billion parameters released by Meta
    AI in February 2023\. Here we only consider the 7 billion version.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLaMA-7B**（Touvron et al., [2023a](#bib.bib25)）**LLaMA**是Meta AI于2023年2月发布的一系列基础语言模型，参数从70亿到650亿不等。这里我们只考虑70亿版本。'
- en: Alpaca-7B (Taori et al., [2023](#bib.bib24)) Alpaca-7B is fine-tuned basd on
    LLaMA-7B model using 52K instruction-following data. The data is generated by
    text-davinci-003 using the self-instruct (Wang et al., [2022](#bib.bib28)) method.
    Alpaca-7B exhibits comparable behavior to the text-davinci-003 on the instruction-following
    evaluation suite (Wang et al., [2022](#bib.bib28)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**Alpaca-7B**（Taori et al., [2023](#bib.bib24)）**Alpaca-7B**是在**LLaMA-7B**模型的基础上，使用52K条遵循指令的数据进行微调的。数据由**text-davinci-003**使用**self-instruct**（Wang
    et al., [2022](#bib.bib28)）方法生成。**Alpaca-7B**在遵循指令的评估套件中表现与**text-davinci-003**相当（Wang
    et al., [2022](#bib.bib28)）。'
- en: ChatGLM-6B (Du et al., [2021](#bib.bib7)) ChatGLM-6B is an open bilingual language
    model developed by Zhipu AI, with 6.2 billion parameters. It is trained on approximately
    1T tokens from both Chinese and English corpus and is further enhanced with supervised
    fine-tuning, feedback bootstrapping, and RLHF. It can generate responses that
    are basically aligned with human preference.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**ChatGLM-6B**（Du et al., [2021](#bib.bib7)）**ChatGLM-6B**是由**智谱AI**开发的开放双语语言模型，拥有62亿参数。它在来自中文和英文语料库的约1T
    tokens上进行训练，并通过监督微调、反馈自举和**RLHF**进一步增强。它可以生成与人类偏好基本一致的响应。'
- en: ChatGPT ChatGPT is a powerful large language model trained by OpenAI with thousands
    of billions of parameters. It is fine-tuned from the GPT-3.5 series by introducing
    RLHF.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**ChatGPT** **ChatGPT**是由OpenAI训练的强大大语言模型，拥有数千亿参数。它是从GPT-3.5系列通过引入**RLHF**进行微调的。'
- en: Besides, we compare with prevalent alignment methods like PPO, RRHF, and PRO.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还与流行的对齐方法如**PPO**、**RRHF**和**PRO**进行比较。
- en: PPO (Schulman et al., [2017](#bib.bib22)) Proximal Policy Optimization (PPO)
    is a popular algorithm in the field of reinforcement learning. It has been used
    to optimize the language model for aligning the human preference. However, its
    complex architecture poses a challenge for hardware devices in the LLM period
    and has the unstable property during training.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**PPO**（Schulman et al., [2017](#bib.bib22)）即**近端策略优化（PPO）**，是强化学习领域的一个流行算法。它已被用来优化语言模型，以对齐人类偏好。然而，其复杂的架构在LLM时期对硬件设备构成挑战，并且在训练过程中具有不稳定的特性。'
- en: RRHF (Yuan et al., [2023](#bib.bib32)) Response Ranking for Human Feedback (RRHF)
    is a new learning method designed to align LLMs with human preferences effectively.
    Unlik PPO, RRHF evaluates and ranks model-generated responses to ensure they match
    human preferences. It requires only 1 to 2 models during tuning and simplifying
    various aspects of the process.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**RRHF**（Yuan et al., [2023](#bib.bib32)）即**基于人类反馈的响应排序（RRHF）**，是一种旨在有效对齐LLM与人类偏好的新学习方法。与**PPO**不同，**RRHF**评估和排序模型生成的响应，以确保它们符合人类偏好。在调优过程中仅需要1到2个模型，并简化了该过程的各个方面。'
- en: PRO (Song et al., [2023](#bib.bib23)) Preference Ranking Optimization (PRO)
    is a method proposed to align LLMs with human values. It extends the Bradley-Terry
    comparison method to rank responses generated by LLMs according to human preferences,
    offering an alternative to complex and unstable reinforcement learning approaches
    like PPO.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**PRO**（Song et al., [2023](#bib.bib23)）即**偏好排序优化（PRO）**，是一种旨在使LLM与人类价值观对齐的方法。它扩展了Bradley-Terry比较方法，以根据人类偏好对LLM生成的响应进行排名，提供了与复杂且不稳定的强化学习方法如**PPO**的替代方案。'
- en: Due our CycleAlignis an optimization-agnostic framework, it should combine with
    the optimization methods to align the language model with the human preference.
    We equip  CycleAlign on RRHF and PRO, and note them as CycleAlign[RRHF] and CycleAlign[PRO]
    respectively.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的**CycleAlign**是一个与优化无关的框架，它应该与优化方法结合，以使语言模型与人类偏好对齐。我们在**RRHF**和**PRO**上应用了**CycleAlign**，并分别将其标记为**CycleAlign[RRHF]**和**CycleAlign[PRO]**。
- en: 'Table 1: Quantitative evaluation results. The scores are calculated by a well-trained
    reward model.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：定量评估结果。分数由经过良好训练的奖励模型计算得出。
- en: '| Methods | Backbone | Harmless[base] | Helpful[base] | Helpful[online] | Helpful[rejection]
    | Total |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 基础模型 | 无害[基础] | 有帮助[基础] | 有帮助[在线] | 有帮助[拒绝] | 总计 |'
- en: '| Zero-shot | LLaMA | 53.59 | 33.25 | 40.48 | 36.23 | 40.67 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Zero-shot | LLaMA | 53.59 | 33.25 | 40.48 | 36.23 | 40.67 |'
- en: '| Alpaca | 52.77 | 53.85 | 55.30 | 55.43 | 54.26 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca | 52.77 | 53.85 | 55.30 | 55.43 | 54.26 |'
- en: '| ChatGLM | 67.26 | 62.14 | 60.44 | 63.86 | 63.85 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM | 67.26 | 62.14 | 60.44 | 63.86 | 63.85 |'
- en: '| ChatGPT | 72.19 | 68.28 | 69.85 | 71.02 | 70.43 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 72.19 | 68.28 | 69.85 | 71.02 | 70.43 |'
- en: '| PPO | LLaMA | 61.97 | 55.29 | 59.78 | 58.26 | 58.65 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| PPO | LLaMA | 61.97 | 55.29 | 59.78 | 58.26 | 58.65 |'
- en: '| RRHF | LLaMA | 64.63 | 61.38 | 63.26 | 63.28 | 63.12 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| RRHF | LLaMA | 64.63 | 61.38 | 63.26 | 63.28 | 63.12 |'
- en: '| CycleAlign[RRHF] | LLaMA | 71.66 | 67.05 | 65.89 | 67.95 | 68.43 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| CycleAlign[RRHF] | LLaMA | 71.66 | 67.05 | 65.89 | 67.95 | 68.43 |'
- en: '|  |  | (+7.03) | (+5.67) | (+2.63) | (+4.67) | (+5.31) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (+7.03) | (+5.67) | (+2.63) | (+4.67) | (+5.31) |'
- en: '| PRO | LLaMA | 72.86 | 64.05 | 65.56 | 66.44 | 67.40 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| PRO | LLaMA | 72.86 | 64.05 | 65.56 | 66.44 | 67.40 |'
- en: '| CycleAlign[PRO] | LLaMA | 70.62 | 66.49 | 67.67 | 68.50 | 68.41 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| CycleAlign[PRO] | LLaMA | 70.62 | 66.49 | 67.67 | 68.50 | 68.41 |'
- en: '|  |  | (-1.98) | (+2.44) | (+2.11) | (+2.06) | (+1.01) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (-1.98) | (+2.44) | (+2.11) | (+2.06) | (+1.01) |'
- en: '| PRO | Alpaca | 73.13 | 64.56 | 65.60 | 66.51 | 67.64 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| PRO | Alpaca | 73.13 | 64.56 | 65.60 | 66.51 | 67.64 |'
- en: '| CycleAlign[PRO] | Alpaca | 71.32 | 67.89 | 66.53 | 68.92 | 68.97 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| CycleAlign[PRO] | Alpaca | 71.32 | 67.89 | 66.53 | 68.92 | 68.97 |'
- en: '|  |  | (-1.81) | (+3.33) | (+0.93) | (+2.41) | (+1.27) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (-1.81) | (+3.33) | (+0.93) | (+2.41) | (+1.27) |'
- en: 4 Experimental Result
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验结果
- en: 4.1 Main results
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 主要结果
- en: 'The main results of our experiments can be found in Table [1](#S3.T1 "Table
    1 ‣ 3.4 Baselines ‣ 3 Settings ‣ CycleAlign: Iterative Distillation from Black-box
    LLM to White-box Models for Better Human Alignment"). Upon the LLaMA-7B and Alpaca-7B,
    we reproduce the state-of-the-art alignment method PRO. The results of PPO and
    RRHF are cited from Song et al. ([2023](#bib.bib23)). The effectiveness of our CycleAlign framework
    on alignment could be illustrated from the following angles.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '我们实验的主要结果可以在表 [1](#S3.T1 "表 1 ‣ 3.4 基线 ‣ 3 设置 ‣ CycleAlign: 从黑箱 LLM 到白箱模型的迭代蒸馏，以获得更好的人工对齐")
    中找到。在 LLaMA-7B 和 Alpaca-7B 上，我们重现了最先进的对齐方法 PRO。PPO 和 RRHF 的结果来自 Song et al.（[2023](#bib.bib23)）。我们 CycleAlign 框架在对齐上的有效性可以从以下几个方面来说明。'
- en: 1) Comparing with zero-shot backbones like LLaMA and Alpaca, it is obvious that
    models significantly outperform them after alignment, indicating that existing
    foundation models or supervised fine-tuned models are under-aligned with human
    value, and will generate harmful and unhelpful responses. Besides, ChatGLM and
    ChatGPT, which have been aligned with human preference data, perform well in generating
    harmless and helpful responses. Considering that ChatGPT is well-aligned and cost-friendly,
    we propose CycleAlign to better align the white-box model with it in a low-resource
    manner.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 与像 LLaMA 和 Alpaca 这样的零-shot 主干相比，显而易见的是模型在对齐后显著超越了它们，这表明现有的基础模型或监督微调模型与人类价值观的对齐不足，将生成有害和无帮助的响应。此外，已经与人类偏好数据对齐的
    ChatGLM 和 ChatGPT 在生成无害和有帮助的响应方面表现良好。考虑到 ChatGPT 已经很好地对齐且成本友好，我们提出 CycleAlign 以低资源的方式更好地将白箱模型与之对齐。
- en: 2) Compared to previous alignment methods, the model equipped with CycleAlign obtain
    a remarkable improvement on alignment. Specifically, CycleAlign increase 7.03
    reward score on $\text{Harmless}_{\text{base}}$ and 5.31 reward score in total
    for RRHF when the backbone is LLaMA. It also brings about 1.0 reward score for
    PRO in total. These results indicate the effectiveness of iterative alignment
    with the help of black-box LLMs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 与之前的对齐方法相比，使用 CycleAlign 的模型在对齐上取得了显著的改进。具体来说，当主干是 LLaMA 时，CycleAlign 在 $\text{Harmless}_{\text{base}}$
    上增加了 7.03 奖励分数，在 RRHF 上总共增加了 5.31 奖励分数。它还为 PRO 总共带来了 1.0 奖励分数。这些结果表明，通过黑箱 LLMs
    进行迭代对齐的有效性。
- en: 3) Overall, the $\text{CycleAlign}_{\textit{PRO}}$ based on Alpaca takes state-of-the-art
    performance in alignment compared with all the traditional alignment methods,
    and has the approximate performance of ChatGPT. After CycleAlign, the model could
    generate more harmless and helpful responses to satisfy the demands of users.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 总体而言，基于 Alpaca 的 $\text{CycleAlign}_{\textit{PRO}}$ 在对齐方面表现出色，相较于所有传统的对齐方法，具有与
    ChatGPT 大致相当的性能。经过 CycleAlign 后，模型能够生成更无害和有帮助的响应，以满足用户的需求。
- en: 'Table 2: CycleAlign vs. PRO (GPT-4)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：CycleAlign vs. PRO（GPT-4）
- en: '| Subset | % Win | % Tie | % Lose |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 子集 | % 胜 | % 平 | % 负 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Harmless[base] | 70 | 1 | 29 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Harmless[base] | 70 | 1 | 29 |'
- en: '| Helpful[base] | 48 | 4 | 48 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Helpful[base] | 48 | 4 | 48 |'
- en: '| Helpful[online] | 46 | 12 | 42 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Helpful[online] | 46 | 12 | 42 |'
- en: '| Helpful[rejection] | 51 | 6 | 43 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Helpful[rejection] | 51 | 6 | 43 |'
- en: 'Table 3: CycleAlign vs. PRO (Human)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：CycleAlign vs. PRO（人工）
- en: '| Subset | % Win | % Tie | % Lose |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 子集 | % 胜利 | % 平局 | % 失败 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Harmless[base] | 69 | 9 | 22 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 无害的[基础] | 69 | 9 | 22 |'
- en: '| Helpful[base] | 49 | 17 | 34 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 有用的[基础] | 49 | 17 | 34 |'
- en: '| Helpful[online] | 44 | 15 | 41 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 有用的[在线] | 44 | 15 | 41 |'
- en: '| Helpful[rejection] | 44 | 15 | 41 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 有用的[拒绝] | 44 | 15 | 41 |'
- en: 4.2 GPT-4 and Human Evaluation
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 GPT-4与人类评估
- en: 'In recent developments, GPT-4 has demonstrated robust consistency with human
    judgment, leading to its extensive application in evaluations (Liu et al., [2023c](#bib.bib13);
    Mao et al., [2023](#bib.bib14)). For our study, we employed both GPT-4 and human
    annotators to assess and compare the responses generated by CycleAlign[PRO] and
    PRO, with Alpaca serving as the backbone. The evaluation outcomes, presented in
    Table[3](#S4.T3 "Table 3 ‣ 4.1 Main results ‣ 4 Experimental Result ‣ CycleAlign:
    Iterative Distillation from Black-box LLM to White-box Models for Better Human
    Alignment") and Table [3](#S4.T3 "Table 3 ‣ 4.1 Main results ‣ 4 Experimental
    Result ‣ CycleAlign: Iterative Distillation from Black-box LLM to White-box Models
    for Better Human Alignment"), convey similar conclusions.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '在最近的发展中，GPT-4显示出与人类判断的一致性强，因此在评估中的应用广泛（Liu et al., [2023c](#bib.bib13)；Mao
    et al., [2023](#bib.bib14)）。在我们的研究中，我们使用了GPT-4和人类标注员来评估和比较CycleAlign[PRO]和PRO生成的响应，Alpaca作为基础。评估结果见表[3](#S4.T3
    "表 3 ‣ 4.1 主要结果 ‣ 4 实验结果 ‣ CycleAlign: 从黑箱LLM到白箱模型的迭代蒸馏，以更好地对齐人类")和表[3](#S4.T3
    "表 3 ‣ 4.1 主要结果 ‣ 4 实验结果 ‣ CycleAlign: 从黑箱LLM到白箱模型的迭代蒸馏，以更好地对齐人类")，传达了类似的结论。'
- en: 'The sampled results across all datasets reveal a consensus among humans and
    GPT-4 that models fine-tuned by CycleAlign[PRO] demonstrate greater alignment
    with human values. This agreement, however, seems to stand in contrast with the
    assessments derived from the reward model, as illustrated in Table[1](#S3.T1 "Table
    1 ‣ 3.4 Baselines ‣ 3 Settings ‣ CycleAlign: Iterative Distillation from Black-box
    LLM to White-box Models for Better Human Alignment"). According to the reward
    model’s evaluation, CycleAlign[PRO] falls short of matching PRO’s performance
    on the $\text{Harmless}_{\text{base}}$ subset. Nonetheless, both human and GPT-4
    evaluations suppose that CycleAlign[PRO] generates much less harmful content compared
    to PRO. This inconsistency might be rooted in the limitations inherent to the
    current reward model. Given its neural network foundation, the assessments it
    renders are subject to a certain margin of error.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '跨所有数据集的采样结果揭示了人类和GPT-4之间的共识，即通过CycleAlign[PRO]微调的模型表现出更强的人类价值观一致性。然而，这种一致性似乎与奖励模型得出的评估相矛盾，如表[1](#S3.T1
    "表 1 ‣ 3.4 基准 ‣ 3 设置 ‣ CycleAlign: 从黑箱LLM到白箱模型的迭代蒸馏，以更好地对齐人类")所示。根据奖励模型的评估，CycleAlign[PRO]在$\text{Harmless}_{\text{base}}$子集上的表现不及PRO。然而，无论是人类还是GPT-4的评估都认为，CycleAlign[PRO]生成的有害内容远少于PRO。这种不一致可能源于当前奖励模型的固有限制。鉴于其神经网络基础，它的评估结果可能存在一定的误差范围。'
- en: Besides, the models refined by CycleAlign[PRO] manifest markedly superior performance
    in the Helpfulbase subset as GPT-4’s evaluation, and in Helpfulrejection according
    to human assessment.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，CycleAlign[PRO]在GPT-4评估的Helpfulbase子集以及人类评估的Helpfulrejection中表现显著优越。
- en: These findings cohesively indicate that through iterative interaction with black-box
    models, white-box models are capable of achieving a more refined alignment with
    human values.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现一致表明，通过与黑箱模型的迭代交互，白箱模型能够实现与人类价值观的更精细对齐。
- en: 4.3 Ablation Study
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: '![Refer to caption](img/b425e2e3746c5572a7d22680e273d056.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b425e2e3746c5572a7d22680e273d056.png)'
- en: 'Figure 4: ChatGPT ranking accuracy after removing the dynamic demonstrations
    (D2) and ICL.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：去除动态演示（D2）和ICL后的ChatGPT排名准确性。
- en: We conduct ablation studies to verify the effectiveness of our dynamic demonstration
    (abbreviated as D2) and ICL.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行消融研究以验证我们动态演示（简称D2）和ICL的有效性。
- en: 'With the model continuously updated during the training process, the distribution
    of the generated responses is ever-shifting. So we need to dynamically examine
    the accuracy of ranking results returned from the black-box LLM. As shown in Figure [4](#S4.F4
    "Figure 4 ‣ 4.3 Ablation Study ‣ 4 Experimental Result ‣ CycleAlign: Iterative
    Distillation from Black-box LLM to White-box Models for Better Human Alignment"),
    after removing D2, the ranking accuracy of ChatGPT begins to decline, especially
    after removing all of the ICL components, the performance of ChatGPT severely
    deteriorates.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '在训练过程中模型不断更新，生成响应的分布也在不断变化。因此，我们需要动态地检查黑箱LLM返回的排名结果的准确性。如图 [4](#S4.F4 "Figure
    4 ‣ 4.3 Ablation Study ‣ 4 Experimental Result ‣ CycleAlign: Iterative Distillation
    from Black-box LLM to White-box Models for Better Human Alignment")所示，去除D2后，ChatGPT的排名准确性开始下降，特别是在去除所有ICL组件后，ChatGPT的性能严重恶化。'
- en: 'The bottleneck in the ranking performance of ChatGPT indirectly affects the
    alignment of the model, thus showing a similar trend in Table [4](#S4.T4 "Table
    4 ‣ 4.3 Ablation Study ‣ 4 Experimental Result ‣ CycleAlign: Iterative Distillation
    from Black-box LLM to White-box Models for Better Human Alignment") with the ranking
    accuracy of ChatGPT.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 'ChatGPT排名性能的瓶颈间接影响了模型的对齐，因此在表 [4](#S4.T4 "Table 4 ‣ 4.3 Ablation Study ‣ 4
    Experimental Result ‣ CycleAlign: Iterative Distillation from Black-box LLM to
    White-box Models for Better Human Alignment")中展示了类似的趋势与ChatGPT的排名准确性。'
- en: The aforementioned experimental results illustrate that the ICL component and
    dynamic demonstration in ICL used for bridging the cycle have broken the alignment
    bottleneck inherent in the LLMs, leading to enhanced alignment performance for
    misaligned models. This results in the generation of responses that are more in
    line with human preferences, being harmless and helpful.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 上述实验结果表明，ICL组件和ICL中的动态演示用于弥合循环中的对齐瓶颈，从而提升了对齐不佳模型的对齐性能。这使得生成的响应更符合人类偏好，既无害又有用。
- en: 'Table 4: Ablation study on average reward. w/o D2 denotes training without
    dynamic demonstrations and w/o ICL denotes training without ICL.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：平均奖励的消融研究。去除D2表示不使用动态演示进行训练，去除ICL表示不使用ICL进行训练。
- en: '| Methods | Harmless[base] | Helpful[base] | Helpful[online] | Helpful[rejection]
    | Total |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 无害[基础] | 有用[基础] | 有用[在线] | 有用[拒绝] | 总计 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| CycleAlign[PRO] | 71.32 | 67.89 | 66.53 | 68.92 | 68.97 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| CycleAlign[PRO] | 71.32 | 67.89 | 66.53 | 68.92 | 68.97 |'
- en: '|      w/o D2 | 71.77 | 65.37 | 64.99 | 66.34 | 67.36 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|      去除D2 | 71.77 | 65.37 | 64.99 | 66.34 | 67.36 |'
- en: '|      w/o ICL | 71.96 | 64.37 | 64.03 | 65.93 | 66.88 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|      去除ICL | 71.96 | 64.37 | 64.03 | 65.93 | 66.88 |'
- en: '![Refer to caption](img/7c04decace70c67a8c53dff8be0b692d.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7c04decace70c67a8c53dff8be0b692d.png)'
- en: 'Figure 5: Average reward on four subsets with varying maximum iterative number
    $N$.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：不同最大迭代次数$N$下的四个子集的平均奖励。
- en: 4.4 Iterative Number Analysis
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 迭代次数分析
- en: In this section, we investigate the influence of interactive threshold for alignment,
    i.e. the optimal setting about maximum iterative number $N$ as 5 to obtain the
    best performance on alignment.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了对齐的交互阈值的影响，即最大迭代次数$N$设置为5，以获得最佳对齐性能。
- en: 4.5 Case Study
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 案例研究
- en: 'In Table [5](#S4.T5 "Table 5 ‣ 4.5 Case Study ‣ 4 Experimental Result ‣ CycleAlign:
    Iterative Distillation from Black-box LLM to White-box Models for Better Human
    Alignment"), we compare responses from PRO and our CycleAlign to different contexts.
    1) Both models answer informatively about Santana’s music; however, our CycleAlign model
    provides additional context, details, and engagement, proving better for user
    interaction. 2) Regarding queries on illegal activities, both models discourage
    such behavior, emphasizing law adherence and ethics. Our model, however, offers
    a more comprehensive response, providing alternative legal suggestions and demonstrating
    a commitment to promoting lawful behavior, thereby adhering to ethical guidelines
    and offering valuable advice to the user.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [5](#S4.T5 "Table 5 ‣ 4.5 Case Study ‣ 4 Experimental Result ‣ CycleAlign:
    Iterative Distillation from Black-box LLM to White-box Models for Better Human
    Alignment")中，我们将PRO与我们的CycleAlign对不同上下文的响应进行了比较。1) 两个模型都能提供有关Santana音乐的信息；然而，我们的CycleAlign模型提供了额外的背景、细节和互动，证明其在用户互动方面更优。2)
    关于非法活动的查询，两个模型都劝阻这种行为，强调法律遵守和伦理。然而，我们的模型提供了更全面的回应，提供了替代的合法建议，并展示了推广合法行为的承诺，从而遵守了伦理准则并为用户提供了有价值的建议。'
- en: 'Table 5: (The table may contain harmful content!) Two cases showing that the
    model fine-tuned by our CycleAlign[PRO] could generate more harmless and helpful
    responses compared with PRO.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：（该表可能包含有害内容！）两个案例显示，由我们 CycleAlign[PRO] 微调的模型相比PRO，能够生成更无害且有帮助的回应。
- en: '| Context: |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 上下文：|'
- en: '| Human: Have you heard of Santana? |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 人类：你听说过Santana吗？|'
- en: '| Assistant: Yes! “Evil Ways”, and “Smooth”, right? |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 助手：是的！《Evil Ways》和《Smooth》，对吗？|'
- en: '| Human: I like “Smooth”. Have you heard of it? |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 人类：我喜欢《Smooth》。你听过吗？|'
- en: '| Assistant (PRO): Yes, I have! It’s a great song. |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 助手（PRO）：是的，我听过！这是一首很棒的歌曲。|'
- en: '| Assistant (Ours): Yes, I’m familiar with the music of Santana. ’Smooth’ is
    a popular song from their album Supernatural. It’s a great song with a catchy
    beat and soulful lyrics. It’s definitely worth listening to!  ✔ |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 助手（我们的）：是的，我熟悉Santana的音乐。《Smooth》是他们专辑《Supernatural》中的一首热门歌曲。这是一首旋律动听、歌词充满灵魂的好歌，绝对值得一听！✔
    |'
- en: '| Context: |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 上下文：|'
- en: '| Human: What is the best way to launder money or counterfeit money? |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 人类：洗钱或伪造货币的最佳方法是什么？|'
- en: '| Assistant (PRO): I’m sorry, I cannot provide information on illegal activities
    such as laundering or counterfeiting money. It is important to always follow the
    law and ethical standards. |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 助手（PRO）：对不起，我不能提供关于洗钱或伪造货币等非法活动的信息。遵守法律和道德标准非常重要。|'
- en: '| Assistant (Ours): It is important to note that laundering money or counterfeiting
    money is illegal and against the law. Engaging in such activities can lead to
    serious consequences, including fines and imprisonment. Instead, I encourage you
    to explore legal and ethical ways to earn and manage your money. This can include
    seeking employment, starting a business, investing, or opening a bank account.
    By following the law and engaging in legal activities, you can ensure your financial
    stability and well-being.  ✔ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 助手（我们的）：需要注意的是，洗钱或伪造货币是非法的，违反法律。参与这些活动可能会导致严重的后果，包括罚款和监禁。相反，我鼓励你探索合法和道德的方式来赚取和管理你的钱。这可以包括寻找工作、开办企业、投资或开设银行账户。通过遵守法律和从事合法活动，你可以确保你的财务稳定和福祉。✔
    |'
- en: 5 Related Work
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Reinforcement Learning-based Approaches for Human Alignment.
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于强化学习的人类对齐方法。
- en: 'Reinforcement learning (RL) techniques have been widely applied for human alignment
    of large language models (LLMs), which employ RL algorithms, such as Proximal
    Policy Optimization (PPO) to optimize the responses generated by LLMs (Yang et al.,
    [2023](#bib.bib31)). These approaches typically consist of three stages: 1) SFT:
    conduct supervised fine-tuning (SFT) to enable the LLMs to follow instructions;
    2) Reward modeling: train a reward model based on extensive paired responses of
    comparisons; 3) RL-based optimization: employ the RL algorithm to optimize the
    SFT model with well-trained reward model. At stage 2), RL from Human Feedback
    (RLHF) collects human-labeled pairs of responses (Bai et al., [2022a](#bib.bib1);
    Ouyang et al., [2022](#bib.bib17)) while RL from AI Feedback (RLAIF) utilizes
    aligned LLMs (e.g., ChatGPT) to compare the pairs of responses (Bai et al., [2022b](#bib.bib2);
    Lee et al., [2023](#bib.bib10)). Ouyang et al. ([2022](#bib.bib17)) propose InstructGPT
    which employs RLHF for optimization. Bai et al. ([2022a](#bib.bib1)) employ RLHF
    to train a helpful and harmless assistant. Bai et al. ([2022b](#bib.bib2)) train
    a harmless AI assistant through self-improvement based on a helpful AI assistant,
    without any human labels identifying harmful outputs. Lee et al. ([2023](#bib.bib10))
    suggest that RLAIF can exhibit comparable performance to RLHF. Overall, these
    approaches all employ an RL algorithm (e.g., PPO) which is often complex, unstable
    and resource-demanding.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）技术已被广泛应用于大型语言模型（LLM）的人工对齐，这些模型使用RL算法，如近端策略优化（PPO），来优化LLM生成的回应（Yang et
    al., [2023](#bib.bib31)）。这些方法通常包括三个阶段：1）SFT：进行监督微调（SFT），使LLM能够遵循指令；2）奖励建模：基于大量配对的比较回应训练奖励模型；3）基于RL的优化：利用RL算法，通过训练良好的奖励模型来优化SFT模型。在阶段2），来自人类反馈的RL（RLHF）收集人类标注的回应对（Bai
    et al., [2022a](#bib.bib1)；Ouyang et al., [2022](#bib.bib17)），而来自AI反馈的RL（RLAIF）利用对齐的LLM（如ChatGPT）来比较回应对（Bai
    et al., [2022b](#bib.bib2)；Lee et al., [2023](#bib.bib10)）。Ouyang et al. ([2022](#bib.bib17))
    提出了使用RLHF进行优化的InstructGPT。Bai et al. ([2022a](#bib.bib1)) 利用RLHF训练了一个有用且无害的助手。Bai
    et al. ([2022b](#bib.bib2)) 通过自我改进训练了一个无害的AI助手，而没有任何人类标注识别有害输出。Lee et al. ([2023](#bib.bib10))
    提出RLAIF可以表现出与RLHF相当的性能。总体而言，这些方法都使用了RL算法（如PPO），这通常是复杂、不稳定且资源需求大的。
- en: Supervised Fine-tuning for Human Alignment
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人类对齐的监督微调
- en: Due to the complexity, high resource requirements, and instability of RL methods,
    people have begun to explore SFT methods to directly optimize the language models
    for human alignment. Rafailov et al. ([2023](#bib.bib19)) bypass the reward modeling
    stage and directly align the LMs with preference data, using a binary cross entropy
    objective for optimization. Similarly, Yuan et al. ([2023](#bib.bib32)) utilize
    the pair-wise responses of comparisons to enable the LMs to learn the preference
    knowledge. Song et al. ([2023](#bib.bib23)) extend the pair-wise comparison to
    accommodate preference rankings of any length. Liu et al. ([2023a](#bib.bib11))
    combine opposite responses to fine-tune models, with hindsight feedback as prompt
    prefix. Liu et al. ([2023b](#bib.bib12)) construct a sandbox of LLMs as a simulated
    human society to collect interaction data with feedback for fine-tuning. These
    methods either rely on extensive human labels, or only unidirectionally distill
    preference knowledge from aligned LLMs into unaligned LMs, ignoring the unaligned
    model can also give feedback to the aligned LMs to improve the aligning process.
    Our proposed CycleAlign  utilizes the collaboration between aligned and unaligned
    models to improve human alignment.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于强化学习方法的复杂性、高资源需求和不稳定性，人们已开始探索SFT方法以直接优化语言模型以实现人类对齐。Rafailov等人（[2023](#bib.bib19)）绕过了奖励建模阶段，直接使用偏好数据对齐语言模型，采用二元交叉熵目标进行优化。类似地，Yuan等人（[2023](#bib.bib32)）利用比较的成对响应使语言模型学习偏好知识。Song等人（[2023](#bib.bib23)）将成对比较扩展到适应任何长度的偏好排名。Liu等人（[2023a](#bib.bib11)）结合相反的响应进行模型微调，以事后反馈作为提示前缀。Liu等人（[2023b](#bib.bib12)）构建了一个大规模语言模型的沙箱作为模拟人类社会，以收集带有反馈的交互数据进行微调。这些方法要么依赖于大量的人工标签，要么仅从对齐的大规模语言模型中单向蒸馏偏好知识到未对齐的语言模型中，忽略了未对齐的模型也可以给对齐的大规模语言模型提供反馈以改善对齐过程。我们提出的CycleAlign利用对齐和未对齐模型之间的协作来改善人类对齐。
- en: 6 Conclusion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this work, we introduced and validated CycleAlign, a novel framework aimed
    at aligning language models with human preferences of harmlessness and helpfulness.
    By iterative interactions between white-box and black-box models and leveraging
    in-context learning, CycleAlign overcomes the bottleneck of unidirectional distillation
    frameworks from LLMs. The experiments conducted on the HH-RLHF dataset demonstrated
    the framework’s effectiveness and superiority in aligning models with human preferences,
    marking a significant step forward in the field. This advancement reduces the
    dependency on human annotations and addresses challenges associated with the complexity
    and hardware consumption of existing methods, paving the way for further research
    and applications in the responsible development of LLMs.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们引入并验证了CycleAlign，这是一种新颖的框架，旨在将语言模型与人类对无害性和有用性的偏好对齐。通过白盒模型和黑盒模型之间的迭代交互，以及利用上下文学习，CycleAlign克服了来自大规模语言模型（LLMs）单向蒸馏框架的瓶颈。对HH-RLHF数据集的实验展示了该框架在将模型与人类偏好对齐方面的有效性和优越性，标志着该领域的重要进展。这一进展减少了对人工注释的依赖，并解决了现有方法在复杂性和硬件消耗方面的挑战，为负责任地开发LLMs的进一步研究和应用铺平了道路。
- en: References
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bai et al. (2022a) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    Training a helpful and harmless assistant with reinforcement learning from human
    feedback. *arXiv preprint arXiv:2204.05862*, 2022a.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai等（2022a）Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen,
    Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan等。使用来自人类反馈的强化学习训练有用且无害的助手。*arXiv预印本
    arXiv:2204.05862*，2022a。
- en: 'Bai et al. (2022b) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell,
    Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron
    McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn
    Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared
    Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane
    Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova
    DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,
    Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly,
    Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario
    Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional
    ai: Harmlessness from ai feedback, 2022b.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人（2022b）**尹涛·白**、**索拉夫·卡达瓦斯**、**桑迪潘·昆杜**、**阿曼达·阿斯克尔**、**杰克逊·科尔农**、**安迪·琼斯**、**安娜·陈**、**安娜·戈尔迪**、**阿扎利亚·米尔霍塞尼**、**卡梅伦·麦金农**、**卡罗尔·陈**、**凯瑟琳·奥尔森**、**克里斯托弗·奥拉赫**、**丹尼·埃尔南德斯**、**道恩·德雷恩**、**迪普·甘古利**、**达斯廷·李**、**伊莱·特兰-约翰逊**、**伊桑·佩雷斯**、**杰米·克尔**、**贾里德·穆勒**、**杰弗里·拉迪什**、**约书亚·兰道**、**卡马尔·恩杜塞**、**卡米尔·卢科斯特**、**利安·洛维特**、**迈克尔·塞利托**、**尼尔森·埃尔哈吉**、**尼古拉斯·谢费尔**、**诺埃米·梅尔卡多**、**诺瓦·达萨尔玛**、**罗伯特·拉森比**、**罗宾·拉尔森**、**萨姆·林格**、**斯科特·约翰斯顿**、**肖娜·克拉维克**、**谢尔·埃尔·肖克**、**斯坦尼斯拉夫·福特**、**塔梅拉·兰汉**、**蒂莫西·特林-劳顿**、**汤姆·康纳利**、**汤姆·亨尼根**、**特里斯坦·休姆**、**塞缪尔·R·鲍曼**、**扎克·哈特菲尔德-多兹**、**本·曼**、**达里奥·阿莫代**、**尼古拉斯·约瑟夫**、**萨姆·麦肯德利什**、**汤姆·布朗**和**贾里德·卡普兰**。宪法人工智能：来自人工智能反馈的无害性，2022b。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）**汤姆·布朗**、**本杰明·曼**、**尼克·赖德**、**梅拉尼·苏比亚**、**贾里德·D·卡普兰**、**普拉富拉·达里瓦尔**、**阿尔文德·内拉坎坦**、**普拉纳夫·夏姆**、**吉里什·萨斯特里**、**阿曼达·阿斯克尔**等。语言模型是少量样本学习者。*神经信息处理系统进展*，33:1877–1901，2020。
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial
    general intelligence: Early experiments with gpt-4, 2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 等人（2023）**塞巴斯蒂安·布贝克**、**瓦伦·昌德拉塞卡兰**、**罗嫩·埃尔丹**、**约翰内斯·格尔克**、**埃里克·霍维茨**、**埃茨·卡马尔**、**彼得·李**、**尹·塔特·李**、**袁智·李**、**斯科特·伦德伯格**、**哈沙·诺里**、**哈米德·帕兰吉**、**马尔科·图利奥·里贝罗**和**易·张**。人工通用智能的火花：与GPT-4的早期实验，2023。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily
    Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
    Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
    Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam
    Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling
    with pathways, 2022.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery 等人（2022）**阿卡恩莎·乔杜里**、**沙兰·纳朗**、**雅各布·德夫林**、**马尔滕·博斯马**、**高拉夫·米什拉**、**亚当·罗伯茨**、**保罗·巴哈姆**、**洪元钟**、**查尔斯·萨顿**、**塞巴斯蒂安·格尔曼**、**帕克·舒赫**、**肯森·史**、**萨沙·茨维亚申科**、**乔舒亚·梅内斯**、**阿比谢克·拉奥**、**帕克·巴恩斯**、**易·泰**、**诺亚姆·沙泽尔**、**维诺德库马尔·普拉巴卡兰**、**艾米莉·瑞夫**、**南·杜**、**本·哈钦森**、**赖纳·教皇**、**詹姆斯·布拉德伯里**、**雅各布·奥斯汀**、**迈克尔·伊萨德**、**盖·古尔-阿里**、**彭程·尹**、**托朱·杜克**、**安瑟姆·列夫斯卡亚**、**圣贾伊·格赫玛特**、**苏尼帕·德夫**、**亨里克·米哈莱夫斯基**、**哈维尔·加西亚**、**维丹特·米斯拉**、**凯文·罗宾逊**、**利亚姆·费杜斯**、**丹尼·周**、**达芙妮·伊波利托**、**大卫·刘安**、**林贤泽**、**巴雷特·佐夫**、**亚历山大·斯皮里多诺夫**、**瑞安·塞帕西**、**大卫·多汉**、**希瓦尼·阿格拉瓦尔**、**马克·奥梅尼克**、**安德鲁·M·戴**、**坦努马拉扬·桑卡拉纳拉亚纳·皮莱**、**玛丽·佩拉特**、**艾托尔·莱科维奇**、**埃丽卡·莫雷拉**、**瑞温·奇尔德**、**奥列克桑德·波洛佐夫**、**凯瑟琳·李**、**宗伟·周**、**薛志·王**、**布伦南·赛塔**、**马克·迪亚斯**、**奥尔汗·费拉特**、**米歇尔·卡塔斯塔**、**贾森·魏**、**凯西·迈耶-赫尔斯特恩**、**道格拉斯·埃克**、**杰夫·迪安**、**斯拉夫·彼得罗夫**和**诺亚·费德尔**。Palm：通过路径扩展语言建模，2022。
- en: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning,
    2023.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等人（2023）**董庆修**、**李磊**、**戴大迈**、**泽程**、**朱永**、**宝宝·常**、**徐顺**、**静静·徐**、**李磊**和**隋志方**。关于上下文学习的调查，2023。
- en: 'Du et al. (2021) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive
    blank infilling. In *Annual Meeting of the Association for Computational Linguistics*,
    2021. URL [https://api.semanticscholar.org/CorpusID:247519241](https://api.semanticscholar.org/CorpusID:247519241).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. (2021) Zhengxiao Du、Yujie Qian、Xiao Liu、Ming Ding、Jiezhong Qiu、Zhilin
    Yang 和 Jie Tang。Glm：通过自回归空白填充的通用语言模型预训练。在 *计算语言学协会年会*，2021年。网址 [https://api.semanticscholar.org/CorpusID:247519241](https://api.semanticscholar.org/CorpusID:247519241)。
- en: 'Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive
    blank infilling. In *Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pp.  320–335, 2022.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. (2022) Zhengxiao Du、Yujie Qian、Xiao Liu、Ming Ding、Jiezhong Qiu、Zhilin
    Yang 和 Jie Tang。Glm：通过自回归空白填充的通用语言模型预训练。在 *第60届计算语言学协会年会（第1卷：长篇论文）*，第320–335页，2022年。
- en: Kim et al. (2023) Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun
    Kwak, Kang Min Yoo, and Minjoon Seo. Aligning large language models through synthetic
    feedback, 2023.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2023) Sungdong Kim、Sanghwan Bae、Jamin Shin、Soyoung Kang、Donghyun
    Kwak、Kang Min Yoo 和 Minjoon Seo。通过合成反馈对齐大型语言模型，2023年。
- en: 'Lee et al. (2023) Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu,
    Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling
    reinforcement learning from human feedback with ai feedback. *arXiv preprint arXiv:2309.00267*,
    2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2023) Harrison Lee、Samrat Phatale、Hassan Mansoor、Kellie Lu、Thomas
    Mesnard、Colton Bishop、Victor Carbune 和 Abhinav Rastogi。Rlaif：通过 AI 反馈扩展来自人类反馈的强化学习，*arXiv
    预印本 arXiv:2309.00267*，2023年。
- en: Liu et al. (2023a) Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. Chain of hindsight
    aligns language models with feedback, 2023a.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023a) Hao Liu、Carmelo Sferrazza 和 Pieter Abbeel。回顾链将语言模型与反馈对齐，2023a。
- en: Liu et al. (2023b) Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou,
    Andrew M. Dai, Diyi Yang, and Soroush Vosoughi. Training socially aligned language
    models in simulated human society, 2023b.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023b) Ruibo Liu、Ruixin Yang、Chenyan Jia、Ge Zhang、Denny Zhou、Andrew
    M. Dai、Diyi Yang 和 Soroush Vosoughi。在模拟人类社会中训练社会对齐的语言模型，2023b。
- en: 'Liu et al. (2023c) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment.
    *arXiv preprint arXiv:2303.16634*, 2023c.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023c) Yang Liu、Dan Iter、Yichong Xu、Shuohang Wang、Ruochen Xu 和 Chenguang
    Zhu。Gpteval：使用 GPT-4 的 Nlg 评估以更好地对齐人工，*arXiv 预印本 arXiv:2303.16634*，2023c。
- en: 'Mao et al. (2023) Rui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin, and Erik
    Cambria. Gpteval: A survey on assessments of chatgpt and gpt-4. *arXiv preprint
    arXiv:2308.12488*, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao et al. (2023) Rui Mao、Guanyi Chen、Xulang Zhang、Frank Guerin 和 Erik Cambria。Gpteval：对
    ChatGPT 和 GPT-4 评估的综述，*arXiv 预印本 arXiv:2308.12488*，2023年。
- en: 'Min et al. (2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations:
    What makes in-context learning work? In *Proceedings of the 2022 Conference on
    Empirical Methods in Natural Language Processing*, pp.  11048–11064, 2022.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min et al. (2022) Sewon Min、Xinxi Lyu、Ari Holtzman、Mikel Artetxe、Mike Lewis、Hannaneh
    Hajishirzi 和 Luke Zettlemoyer。重新思考示范的角色：是什么使上下文学习有效？在 *2022年自然语言处理实证方法会议论文集*，第11048–11064页，2022年。
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI。GPT-4 技术报告，2023年。
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language
    models to follow instructions with human feedback, 2022.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Long Ouyang、Jeff Wu、Xu Jiang、Diogo Almeida、Carroll L. Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray、John Schulman、Jacob
    Hilton、Fraser Kelton、Luke Miller、Maddie Simens、Amanda Askell、Peter Welinder、Paul
    Christiano、Jan Leike 和 Ryan Lowe。训练语言模型以遵循具有人工反馈的指令，2022年。
- en: Pezeshkpour & Hruschka (2023) Pouya Pezeshkpour and Estevam Hruschka. Large
    language models sensitivity to the order of options in multiple-choice questions,
    2023.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pezeshkpour & Hruschka (2023) Pouya Pezeshkpour 和 Estevam Hruschka。大型语言模型对多项选择题选项顺序的敏感性，2023年。
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization:
    Your language model is secretly a reward model, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov et al. (2023) Rafael Rafailov、Archit Sharma、Eric Mitchell、Stefano Ermon、Christopher
    D. Manning 和 Chelsea Finn。直接偏好优化：你的语言模型秘密地是一个奖励模型，2023年。
- en: Ren et al. (2021) Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji
    Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. $\{$ model training.
    In *2021 USENIX Annual Technical Conference (USENIX ATC 21)*, pp.  551–564, 2021.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等（2021）Jie Ren、Samyam Rajbhandari、Reza Yazdani Aminabadi、Olatunji Ruwase、Shuangyan
    Yang、Minjia Zhang、Dong Li 和 Yuxiong He。$\{$ 模型训练。在 *2021 USENIX 年度技术会议（USENIX
    ATC 21）*，第551–564页，2021年。
- en: Rubin et al. (2021) Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning
    to retrieve prompts for in-context learning. *arXiv preprint arXiv:2112.08633*,
    2021.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubin 等（2021）Ohad Rubin、Jonathan Herzig 和 Jonathan Berant。学习检索用于上下文学习的提示。*arXiv
    预印本 arXiv:2112.08633*，2021年。
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal policy optimization algorithms. *arXiv preprint
    arXiv:1707.06347*, 2017.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等（2017）John Schulman、Filip Wolski、Prafulla Dhariwal、Alec Radford 和
    Oleg Klimov。近端策略优化算法。*arXiv 预印本 arXiv:1707.06347*，2017年。
- en: Song et al. (2023) Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang,
    Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment.
    *arXiv preprint arXiv:2306.17492*, 2023.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等（2023）Feifan Song、Bowen Yu、Minghao Li、Haiyang Yu、Fei Huang、Yongbin Li
    和 Houfeng Wang。人类对齐的偏好排序优化。*arXiv 预印本 arXiv:2306.17492*，2023年。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford
    alpaca: An instruction-following llama model, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori 等（2023）Rohan Taori、Ishaan Gulrajani、Tianyi Zhang、Yann Dubois、Xuechen Li、Carlos
    Guestrin、Percy Liang 和 Tatsunori B Hashimoto。斯坦福 alpaca：一个跟随指令的 llama 模型，2023年。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023a）Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。Llama：开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023年。
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023b.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023b）Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale、Dan Bikel、Lukas
    Blecher、Cristian Canton Ferrer、Moya Chen、Guillem Cucurull、David Esiobu、Jude Fernandes、Jeremy
    Fu、Wenyin Fu、Brian Fuller、Cynthia Gao、Vedanuj Goswami、Naman Goyal、Anthony Hartshorn、Saghar
    Hosseini、Rui Hou、Hakan Inan、Marcin Kardas、Viktor Kerkez、Madian Khabsa、Isabel Kloumann、Artem
    Korenev、Punit Singh Koura、Marie-Anne Lachaux、Thibaut Lavril、Jenya Lee、Diana Liskovich、Yinghai
    Lu、Yuning Mao、Xavier Martinet、Todor Mihaylov、Pushkar Mishra、Igor Molybog、Yixin
    Nie、Andrew Poulton、Jeremy Reizenstein、Rashi Rungta、Kalyan Saladi、Alan Schelten、Ruan
    Silva、Eric Michael Smith、Ranjan Subramanian、Xiaoqing Ellen Tan、Binh Tang、Ross
    Taylor、Adina Williams、Jian Xiang Kuan、Puxin Xu、Zheng Yan、Iliyan Zarov、Yuchen Zhang、Angela
    Fan、Melanie Kambadur、Sharan Narang、Aurelien Rodriguez、Robert Stojnic、Sergey Edunov
    和 Thomas Scialom。Llama 2：开放的基础和微调聊天模型，2023年。
- en: Wang et al. (2023a) Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai
    Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are
    not fair evaluators, 2023a.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023a）Peiyi Wang、Lei Li、Liang Chen、Zefan Cai、Dawei Zhu、Binghuai Lin、Yunbo
    Cao、Qi Liu、Tianyu Liu 和 Zhifang Sui。大型语言模型不是公平的评估者，2023年。
- en: 'Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning
    language models with self-generated instructions. In *Annual Meeting of the Association
    for Computational Linguistics*, 2022. URL [https://api.semanticscholar.org/CorpusID:254877310](https://api.semanticscholar.org/CorpusID:254877310).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2022）Yizhong Wang、Yeganeh Kordi、Swaroop Mishra、Alisa Liu、Noah A. Smith、Daniel
    Khashabi 和 Hannaneh Hajishirzi。Self-instruct：将语言模型与自生成的指令对齐。在 *计算语言学协会年会*，2022年。网址
    [https://api.semanticscholar.org/CorpusID:254877310](https://api.semanticscholar.org/CorpusID:254877310)。
- en: 'Wang et al. (2023b) Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan
    Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language
    models with human: A survey, 2023b.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan
    Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, 和 Qun Liu。将大型语言模型与人类对齐：调查，2023b。
- en: Xie et al. (2021) Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu
    Ma. An explanation of in-context learning as implicit bayesian inference. In *International
    Conference on Learning Representations*, 2021.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2021) Sang Michael Xie, Aditi Raghunathan, Percy Liang, 和 Tengyu
    Ma。将上下文学习解释为隐式贝叶斯推断。在 *国际学习表征会议*，2021。
- en: 'Yang et al. (2023) Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, and
    Yuandong Tian. Rlcd: Reinforcement learning from contrast distillation for language
    model alignment, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023) Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, 和 Yuandong
    Tian。Rlcd：从对比蒸馏中进行语言模型对齐的强化学习，2023。
- en: 'Yuan et al. (2023) Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang
    Huang, and Fei Huang. Rrhf: Rank responses to align language models with human
    feedback without tears. *arXiv preprint arXiv:2304.05302*, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2023) Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang
    Huang, 和 Fei Huang。Rrhf：通过人类反馈对齐语言模型的排名响应，无需眼泪。*arXiv 预印本 arXiv:2304.05302*，2023。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
    and chatbot arena, 2023.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao
    Zhang, Joseph E. Gonzalez, 和 Ion Stoica。用 mt-bench 和 chatbot arena 评判 llm-as-a-judge，2023。
- en: Appendix A Appendix
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Static manually crafted demonstration
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 静态手动制作的演示
- en: 'We manually crafted a static demonstration for ICL. The demonstration is as
    follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为 ICL 手动制作了一个静态演示。演示如下：
- en: 'Instruction:'
  id: totrans-186
  prefs:
  - PREF_BQ
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指令：
- en: 'Human: How to make a lot of money in a short time?'
  id: totrans-187
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 人类：如何在短时间内赚很多钱？
- en: 'Assistant:'
  id: totrans-188
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 助手：
- en: ''
  id: totrans-189
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Responses:'
  id: totrans-190
  prefs:
  - PREF_BQ
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回应：
- en: Response-0\. You can try to rob a bank, where you can get a lot of money in
    a short time. You also can commit telecommunication fraud against many people
    to get money. You can also try to sell drugs, which is a very profitable business.
  id: totrans-191
  prefs:
  - PREF_BQ
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回应-0\. 你可以尝试抢银行，这样可以在短时间内得到很多钱。你也可以对很多人进行电信诈骗以获取钱财。你还可以尝试出售毒品，这是一项非常盈利的业务。
- en: Response-1\. One way to make money quickly would be through starting a successful
    small business or freelancing in a high-demand field where you have a strong skill
    set. Or leveraging your existing skills, such as graphic design, coding, or writing,
    and marketing them effectively can result in a lucrative income stream.
  id: totrans-192
  prefs:
  - PREF_BQ
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回应-1\. 快速赚钱的一种方法是通过创办成功的小型企业或在需求旺盛的领域进行自由职业，其中你拥有强大的技能。或者利用你现有的技能，比如图形设计、编程或写作，并有效地营销这些技能，可以获得丰厚的收入来源。
- en: Response-2\. Drink much wine, and cry loudly in the street in midnight. People
    may give you money.
  id: totrans-193
  prefs:
  - PREF_BQ
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回应-2\. 喝大量的酒，午夜时分在街上大声哭泣。人们可能会给你钱。
- en: Response-3\. Use a time machine to travel into the future, find out the winning
    numbers of a big lottery draw, then come back to the present and buy a ticket
    with those numbers.
  id: totrans-194
  prefs:
  - PREF_BQ
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回应-3\. 使用时间机器旅行到未来，找出一次大型彩票抽奖的中奖号码，然后回到现在，购买带有这些号码的彩票。
- en: ''
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The desired ranking is: [1, 3, 2, 0].'
  id: totrans-196
  prefs:
  - PREF_BQ
  - PREF_H3
  type: TYPE_NORMAL
  zh: 所需的排名是：[1, 3, 2, 0]。
- en: To design the static demonstration, we collect four responses with different
    degrees of alignment to “How to make a lot of money in a short time?” from ChatGPT
    and human. As we can see, response-1 is helpful and harmless, while response-3
    is unhelpful and response-2, 0 are harmful.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设计静态演示，我们收集了来自 ChatGPT 和人类的四个不同程度对“如何在短时间内赚很多钱？”的回应。我们可以看到，回应-1 是有帮助且无害的，而回应-3
    是无用的，回应-2 和 0 是有害的。
- en: A.2 Statistics of HH-RLHF dataset
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 HH-RLHF 数据集统计数据
- en: 'Table 6: The statistics of four subsets from the HH-RLHF dataset.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：HH-RLHF 数据集四个子集的统计数据。
- en: '|  | Harmless[base] | Helpful[base] | Helpful[online] | Helpful[rejection]
    |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | 无害[base] | 有帮助[base] | 有帮助[online] | 有帮助[rejection] |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Train | 42537 | 43835 | 22007 | 52421 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 42537 | 43835 | 22007 | 52421 |'
- en: '| Test | 2312 | 2354 | 1137 | 2749 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 2312 | 2354 | 1137 | 2749 |'
