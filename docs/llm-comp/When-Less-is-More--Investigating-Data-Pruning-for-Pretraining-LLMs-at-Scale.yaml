- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 19:05:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 19:05:18'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当少即是多：大规模研究LLMs预训练数据精简
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.04564](https://ar5iv.labs.arxiv.org/html/2309.04564)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.04564](https://ar5iv.labs.arxiv.org/html/2309.04564)
- en: name=Max Marion    affiliation=Cohere for AI    email=maxwell@cohere.com   
    name=Ahmet Üstün    affiliation=Cohere for AI    email=ahmet@cohere.com    name=Luiza
    Pozzobon    affiliation=Cohere for AI    email=luiza@cohere.com    name=Alex Wang
       affiliation=Cohere    email=alexwang@cohere.com    name=Marzieh Fadaee    affiliation=Cohere
    for AI    email=marzieh@cohere.com    name=Sara Hooker    affiliation=Cohere for
    AI    email=sarahooker@cohere.com
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: name=Max Marion    affiliation=Cohere for AI    email=maxwell@cohere.com   
    name=Ahmet Üstün    affiliation=Cohere for AI    email=ahmet@cohere.com    name=Luiza
    Pozzobon    affiliation=Cohere for AI    email=luiza@cohere.com    name=Alex Wang
       affiliation=Cohere    email=alexwang@cohere.com    name=Marzieh Fadaee    affiliation=Cohere
    for AI    email=marzieh@cohere.com    name=Sara Hooker    affiliation=Cohere for
    AI    email=sarahooker@cohere.com
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large volumes of text data have contributed significantly to the development
    of large language models (LLMs) in recent years. This data is typically acquired
    by scraping the internet, leading to pretraining datasets comprised of noisy web
    text. To date, efforts to prune these datasets down to a higher quality subset
    have relied on hand-crafted heuristics encoded as rule-based filters. In this
    work, we take a wider view and explore scalable estimates of data quality that
    can be used to systematically measure the quality of pretraining data. We perform
    a rigorous comparison at scale of the simple data quality estimator of perplexity,
    as well as more sophisticated and computationally intensive estimates of the Error
    L2-Norm and memorization. These metrics are used to rank and prune pretraining
    corpora, and we subsequently compare LLMs trained on these pruned datasets. Surprisingly,
    we find that the simple technique of perplexity outperforms our more computationally
    expensive scoring methods. We improve over our no-pruning baseline while training
    on as little as 30% of the original training dataset. Our work sets the foundation
    for unexplored strategies in automatically curating high quality corpora and suggests
    the majority of pretraining data can be removed while retaining performance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大量的文本数据在近年来对大语言模型（LLMs）的发展做出了重要贡献。这些数据通常通过抓取互联网获得，从而形成了包含嘈杂网络文本的预训练数据集。迄今为止，精简这些数据集以获得更高质量子集的努力依赖于手工设计的启发式规则作为规则基础的过滤器。在这项工作中，我们采取了更广泛的视角，探索了可以系统性地衡量预训练数据质量的可扩展数据质量估计方法。我们对简单的数据质量估计器困惑度进行了大规模的严格比较，同时也对更复杂且计算密集的误差L2范数和记忆估计进行了比较。这些指标用于对预训练语料库进行排名和精简，随后我们比较了在这些精简数据集上训练的LLMs。令人惊讶的是，我们发现简单的困惑度技术优于我们更昂贵的评分方法。在仅使用原始训练数据集的30%的情况下，我们的改进超越了无精简的基线。我们的工作为自动策划高质量语料库的未探索策略奠定了基础，并建议大多数预训练数据可以被移除而保持性能。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: A reigning belief in machine learning is that more data leads to better performance.
    Recent years of progress in scaling large language models (LLMs) have shown strong
    evidence to support this with remarkable gains in language understanding and generation
    capabilities (Brown et al., [2020](#bib.bib9); Touvron et al., [2023](#bib.bib41);
    Kaplan et al., [2020](#bib.bib20); Anil et al., [2023](#bib.bib3)). When training
    language models, common practice is to use massive datasets such as C4 (Raffel
    et al., [2020](#bib.bib35)), RefinedWeb (Penedo et al., [2023](#bib.bib31)), and
    The Pile (Gao et al., [2021](#bib.bib17)). These datasets are typically compiled
    by scraping raw web pages from the internet, leading to a substantial portion
    of the text being noisy and of low quality (Dodge et al., [2021](#bib.bib14);
    Kreutzer et al., [2022](#bib.bib22); Luccioni & Viviano, [2021](#bib.bib25)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的一个主流观点是更多的数据能带来更好的性能。近年来大语言模型（LLMs）的扩展进展已经强有力地支持了这一观点，显著提升了语言理解和生成能力（Brown
    et al., [2020](#bib.bib9); Touvron et al., [2023](#bib.bib41); Kaplan et al.,
    [2020](#bib.bib20); Anil et al., [2023](#bib.bib3)）。在训练语言模型时，常见的做法是使用大量数据集，如C4
    (Raffel et al., [2020](#bib.bib35))、RefinedWeb (Penedo et al., [2023](#bib.bib31))和The
    Pile (Gao et al., [2021](#bib.bib17))。这些数据集通常通过抓取来自互联网的原始网页来编译，因此文本中的大量内容往往嘈杂且质量较低（Dodge
    et al., [2021](#bib.bib14); Kreutzer et al., [2022](#bib.bib22); Luccioni & Viviano,
    [2021](#bib.bib25)）。
- en: 'Practitioners have established a number of standard filtering techniques to
    remove low-quality examples from these datasets. These techniques are predominantly
    rule-based heuristics: removing documents containing repetitive text (Zhang et al.,
    [2022](#bib.bib47); Raffel et al., [2020](#bib.bib35); Rae et al., [2022](#bib.bib34);
    Hernandez et al., [2022](#bib.bib19); Penedo et al., [2023](#bib.bib31)), special
    characters, or non-English text (Wenzek et al., [2020](#bib.bib44)); ignoring
    data from a manually curated list of “blocklist” websites (Dodge et al., [2021](#bib.bib14);
    Rae et al., [2022](#bib.bib34)); or eliminating documents based on certain length
    thresholds. While these hand-curated filters can eliminate certain noisy examples,
    they are not a substitute for a measure of “quality” for individual training examples,
    for which there are currently no established best practices (Mitchell et al.,
    [2023](#bib.bib27)).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些数据集中去除低质量样本，实践者们建立了许多标准的过滤技术。这些技术主要是基于规则的启发式方法：去除包含重复文本的文档（Zhang et al.,
    [2022](#bib.bib47); Raffel et al., [2020](#bib.bib35); Rae et al., [2022](#bib.bib34);
    Hernandez et al., [2022](#bib.bib19); Penedo et al., [2023](#bib.bib31)），特殊字符或非英语文本（Wenzek
    et al., [2020](#bib.bib44)）；忽略来自手动整理的“黑名单”网站的数据（Dodge et al., [2021](#bib.bib14);
    Rae et al., [2022](#bib.bib34)）；或根据某些长度阈值消除文档。虽然这些人工筛选的过滤器可以去除某些噪声样本，但它们不能替代对单个训练样本“质量”的度量，目前尚无公认的最佳实践（Mitchell
    et al., [2023](#bib.bib27)）。
- en: '![Refer to caption](img/47af5ae29a04cbadde59b00cd6018635.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/47af5ae29a04cbadde59b00cd6018635.png)'
- en: 'Figure 1: Demonstration of our pruning methodology.For each sequence $z_{i}$.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们修剪方法的演示。对于每个序列 $z_{i}$。
- en: In this work, we take a wider view and ask if we can arrive at a rigorous estimator
    of data quality through data pruning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们从更广泛的角度考虑，问是否可以通过数据修剪得到一个严谨的数据质量估计器。
- en: 'Data pruning attempts to isolate a subset of a larger training dataset such
    that a model trained on said subset preserves or improves performance over a model
    trained on the full dataset. To date, the majority of work on data pruning has
    centered on supervised computer vision settings (Qin et al., [2023](#bib.bib32);
    Sorscher et al., [2023](#bib.bib39); Raju et al., [2021](#bib.bib36); Paul et al.,
    [2023](#bib.bib30); He et al., [2023](#bib.bib18)), with far fewer works focusing
    on language. Those that have either studied the fine-tuning setting, which typically
    has an order of magnitude less data and thus tolerates more computational complexity
    (Fayyaz et al., [2022](#bib.bib15); Attendu & Corbeil, [2023](#bib.bib4); Cao
    et al., [2023](#bib.bib10)) or based their method on hand picking high-quality
    corpora (Gao, [2021](#bib.bib16); Wenzek et al., [2020](#bib.bib44); Brown et al.,
    [2020](#bib.bib9)). Specifically, we try to answer the following: Can we remove
    the least impactful examples from a pretraining dataset and achieve similar or
    better performance? Do simpler techniques for estimating data quality outperform
    more sophisticated and computationally expensive methods? What aspects of training
    dynamics signal data quality the best?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据修剪旨在隔离一个较大的训练数据集的子集，使得在该子集上训练的模型的性能能保持或提高，相比于在完整数据集上训练的模型。迄今为止，大多数关于数据修剪的工作都集中在监督计算机视觉环境中（Qin
    et al., [2023](#bib.bib32); Sorscher et al., [2023](#bib.bib39); Raju et al.,
    [2021](#bib.bib36); Paul et al., [2023](#bib.bib30); He et al., [2023](#bib.bib18)），而针对语言的研究要少得多。已有的研究要么关注于微调设置，这通常数据量级较小，因此容忍更多的计算复杂度（Fayyaz
    et al., [2022](#bib.bib15); Attendu & Corbeil, [2023](#bib.bib4); Cao et al.,
    [2023](#bib.bib10)），要么基于手动挑选高质量语料的方法（Gao, [2021](#bib.bib16); Wenzek et al., [2020](#bib.bib44);
    Brown et al., [2020](#bib.bib9)）。具体来说，我们尝试回答以下问题：我们能否从预训练数据集中去除影响最小的样本，并实现类似或更好的性能？用于估计数据质量的简单技术是否优于更复杂且计算成本较高的方法？哪些训练动态的方面最能反映数据质量？
- en: 'We answer these questions by rigorously evaluating three automatic pruning
    metrics. One simple estimator of quality, perplexity, and two more complex, and
    EL2N (Paul et al., [2023](#bib.bib30)) memorization factor. These methods all
    rely solely on model outputs and do not require a preselected high-quality dataset.
    This lack of dependence on human judgments of data quality make them a promising
    direction for automatic selection of high quality corpora. We perform extensive
    experiments evaluating models ranging from 124M to 1.5B parameters across different
    pretrained corpora. Our contributions are the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过严格评估三种自动修剪指标来回答这些问题。一个简单的质量估计器，困惑度，以及两个更复杂的，EL2N（Paul et al., [2023](#bib.bib30)）记忆因子。这些方法完全依赖于模型输出，不需要预先选择的高质量数据集。这种对数据质量人类判断的依赖缺失使它们成为自动选择高质量语料库的有希望的方向。我们进行了广泛的实验，评估了参数范围从124M到1.5B的模型在不同预训练语料库上的表现。我们的贡献如下：
- en: '1.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We extensively benchmark data pruning based on perplexity, EL2N, and memorization
    in the LLM pretraining setting. Surprisingly, we find the simple technique of
    ranking examples based on their perplexity outperforms far more complex techniques
    such as memorization. A model trained on 50% of the dataset pruned based on perplexity
    achieves 1.33% and 1.77% improvement over the most performant models pruned to
    50% of the dataset with EL2N and memorization factor respectively. A model trained
    on 30% of the dataset pruned with perplexity achieves a 2.1% and 1.6% improvement
    over the most performant models pruned to 30% of the dataset with EL2N and memorization
    factor.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在LLM预训练设置中广泛基准测试了基于困惑度、EL2N和记忆的数据显示修剪。令人惊讶的是，我们发现基于困惑度对示例进行排序的简单技术优于诸如记忆等更复杂的技术。在基于困惑度修剪的50%数据集上训练的模型比基于EL2N和记忆因子的50%数据集修剪的最优模型分别提高了1.33%和1.77%。在基于困惑度修剪的30%数据集上训练的模型比基于EL2N和记忆因子的30%数据集修剪的最优模型分别提高了2.1%和1.6%。
- en: '2.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: To comprehensively cover multiple facets of data pruning, we provide a unified
    and general framework to identify and treat different data subsets present in
    a dataset. We compare models trained on datasets pruned to 10, 30, 50, and 70%
    of the training set while retaining either the bottom, middle, or top of the pruning
    scores’ distributions. We test seven different reference models across pruning
    variations, investigating the impact of parameter count, training dataset, and
    total training steps on the reference models’ pruning capabilities. Finally, we
    finetune a selection of our models on six tasks from the GLUE benchmark (Wang
    et al., [2019](#bib.bib43)) to evaluate the effect of pruning on downstream generalization.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了全面覆盖数据修剪的多个方面，我们提供了一个统一的通用框架，以识别和处理数据集中不同的数据子集。我们比较了在数据集修剪到训练集的10%、30%、50%和70%时训练的模型，同时保留了修剪分数分布的底部、中部或顶部。我们在修剪变化下测试了七种不同的参考模型，调查参数数量、训练数据集和总训练步骤对参考模型修剪能力的影响。最后，我们在GLUE基准（Wang
    et al., [2019](#bib.bib43)）的六项任务上微调了我们选择的一些模型，以评估修剪对下游泛化的影响。
- en: '3.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We test our pruning methods at scale, achieving a 1% improvement in test set
    perplexity using half of the dataset over a baseline model trained on the entire
    dataset. We show this scales to 1.5B parameter models, achieving 1.5% improvement
    in test set perplexity over a no-pruning baseline of the same size.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在大规模上测试了我们的修剪方法，在使用一半数据集的测试集困惑度上实现了1%的改进，相较于在整个数据集上训练的基线模型。我们展示了这一点在1.5B参数模型上也适用，相较于相同规模的无修剪基线模型，测试集困惑度提高了1.5%。
- en: 2 Methodology
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: 'Given a large-scale dataset $\mathcal{D}$ with instances that fit our selection
    criteria:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个符合我们选择标准的大规模数据集$\mathcal{D}$：
- en: '|  | $\mathcal{P}_{\xi}=\{z_{i}\in\mathcal{D}\ &#124;\ Criteria(Score_{\xi}(z_{i}))\}$
    |  | (1) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{P}_{\xi}=\{z_{i}\in\mathcal{D}\ &#124;\ Criteria(Score_{\xi}(z_{i}))\}$
    |  | (1) |'
- en: 'By removing $\mathcal{P}_{\xi}$, the remaining instances are described as:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过移除$\mathcal{P}_{\xi}$，剩余的实例描述为：
- en: '|  | $\hat{\mathcal{D}}_{\xi}=\mathcal{D}\setminus\mathcal{P}_{\xi}$ |  | (2)
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathcal{D}}_{\xi}=\mathcal{D}\setminus\mathcal{P}_{\xi}$ |  | (2)
    |'
- en: 'Our goal is to choose the pruning algorithm $\xi$, the model’s performance
    is not diminished:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是选择修剪算法$\xi$，使得模型的性能不会下降：
- en: '|  | $\mathbb{P}_{\tau}(\mathcal{M}_{\hat{\mathcal{D}}_{\xi}})\geq\mathbb{P}_{\tau}(\mathcal{M}_{\mathcal{D}})$
    |  | (3) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{P}_{\tau}(\mathcal{M}_{\hat{\mathcal{D}}_{\xi}})\geq\mathbb{P}_{\tau}(\mathcal{M}_{\mathcal{D}})$
    |  | (3) |'
- en: where $\mathcal{M}_{\hat{\mathcal{D}}_{\xi}}$.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{M}_{\hat{\mathcal{D}}_{\xi}}$。
- en: In particular, we evaluate different reference models $\tilde{\mathcal{M}}$
    share the same context length to ensure consistency between the contexts for which
    pruning metrics are calculated and trained models are trained.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们评估不同的参考模型$\tilde{\mathcal{M}}$共享相同的上下文长度，以确保计算剪枝指标的上下文与训练模型之间的一致性。
- en: 'For each metric, we consider three different selection criteria to determine
    $\mathcal{P}_{\xi}$ as the data to be kept. We pretrain separate models using
    these criteria with different percentages of the dataset to understand the dynamics
    and impact of each pruning metric. Since the effectiveness of these metrics in
    this specific context remains uncertain, we opt for these contrasting subsets
    to clarify the relationship between each metric and the overall model performance.
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale") demonstrates our experimental setup.
    We focus on static pruning, in which data is pruned once before training. This
    is in contrast to adaptive pruning, in which data is pruned as training is happening,
    such as in (Fayyaz et al., [2022](#bib.bib15); Park et al., [2022](#bib.bib29)).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '对于每个指标，我们考虑三种不同的选择标准来确定$\mathcal{P}_{\xi}$作为要保留的数据。我们使用这些标准预训练不同数据集百分比的单独模型，以理解每种剪枝指标的动态和影响。由于这些指标在特定背景下的有效性仍不确定，我们选择这些对比性子集以澄清每种指标与整体模型性能之间的关系。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ When Less is More: Investigating Data Pruning for
    Pretraining LLMs at Scale")展示了我们的实验设置。我们关注静态剪枝，其中数据在训练前被剪枝。这与自适应剪枝相对立，自适应剪枝是在训练过程中进行的，如(Fayyaz
    et al., [2022](#bib.bib15); Park et al., [2022](#bib.bib29))。'
- en: 2.1 Pruning Methods
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 剪枝方法
- en: Here, we briefly describe data pruning algorithms that we benchmark in this
    work. Our goal is to rigorously compare simple and computationally inexpensive
    ranking approaches such as perplexity and random ranking against more sophisticated
    and computationally expensive techniques such as memorization scores and EL2N.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们简要描述了在这项工作中基准测试的数据剪枝算法。我们的目标是严格比较简单且计算开销较小的排序方法，如困惑度和随机排序，与更复杂且计算开销较大的技术，如记忆评分和EL2N。
- en: 2.1.1 Selection via Perplexity
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 通过困惑度选择
- en: 'Perplexity measures how probable a given piece of text is based on a particular
    language model. For each instance $z_{i}$, we compute the perplexity metric as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度衡量给定文本在特定语言模型下的可能性。对于每个实例$z_{i}$，我们计算困惑度指标为：
- en: '|  | $PPL(z_{i})=\exp\big{(}\frac{1}{&#124;z_{i}&#124;}\sum_{t_{j}\in z_{i}}NLL(t_{j})\big{)}$
    |  | (4) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $PPL(z_{i})=\exp\big{(}\frac{1}{&#124;z_{i}&#124;}\sum_{t_{j}\in z_{i}}NLL(t_{j})\big{)}$
    |  | (4) |'
- en: 'where $NLL(t_{j})$:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$NLL(t_{j})$：
- en: '|  | $\displaystyle NLL(t_{j})=-\log P(t_{j}&#124;t_{<j};\theta)$ |  | (5)
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle NLL(t_{j})=-\log P(t_{j}&#124;t_{<j};\theta)$ |  | (5)
    |'
- en: A lower perplexity score indicates that the model assigns a high probability
    to the text.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 较低的困惑度评分表明模型对文本赋予了较高的概率。
- en: 2.1.2 Selection via EL2N
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 通过EL2N选择
- en: 'The Error L2-Norm (EL2N) score was originally proposed in a computer vision
    setting to identify which samples are important for learning (Paul et al., [2023](#bib.bib30)).
    It measures each sample’s importance using the model’s early learning signals.
    We define the EL2N score on text sequences as the average $L_{2}$ is the one-hot
    encoded representation of the ground truth:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 错误L2范数（EL2N）评分最初是在计算机视觉环境中提出的，用于识别哪些样本对学习重要（Paul et al., [2023](#bib.bib30)）。它使用模型早期的学习信号来衡量每个样本的重要性。我们将文本序列上的EL2N评分定义为平均$L_{2}$，即真实值的一热编码表示：
- en: '|  | $\text{EL2N}(z_{i})=\frac{1}{t}\sum_{i}^{t}\&#124;\hat{y}_{t}-y_{t}\&#124;_{2}$
    |  | (6) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{EL2N}(z_{i})=\frac{1}{t}\sum_{i}^{t}\&#124;\hat{y}_{t}-y_{t}\&#124;_{2}$
    |  | (6) |'
- en: We first evaluate the pruning efficacy of EL2N scores obtained from a single
    reference model at two different checkpoints, trained on 14% and 55% of the training
    dataset $\mathcal{D}$ corresponding to 250 and 1000 steps respectively, to determine
    the required number of steps needed before a usable pruning signal emerges. We
    then train ten different reference models with different random initializations
    and average the EL2N score from all ten models to obtain our final EL2N score.
    The authors suggest that exhibiting a low EL2N score are typically those the model
    learns in its early stages of training, likely because they are relatively easier.
    Inversely, examples with higher EL2N scores are hypothesized to indicate that
    the model continues to incur a significant loss for them and may require additional
    iterations to learn.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先评估从两个不同检查点获得的 EL2N 分数的剪枝效果，这两个模型分别在训练数据集 $\mathcal{D}$ 的 14% 和 55% 上进行训练，分别对应
    250 步和 1000 步，以确定在出现可用剪枝信号之前所需的步数。然后，我们训练十个不同的参考模型，并从所有十个模型中平均 EL2N 分数，以获得最终的
    EL2N 分数。作者建议，表现出低 EL2N 分数的通常是模型在训练初期学习到的，可能是因为这些样本相对较容易。相反，具有较高 EL2N 分数的样本被假设为模型仍然对其造成显著损失，并可能需要额外的迭代来学习。
- en: 2.1.3 Memorization Ranking
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 记忆排名
- en: 'Memorization in language models is a well-studied phenomenon (Carlini et al.,
    [2023](#bib.bib12), [2021](#bib.bib11); Biderman et al., [2023a](#bib.bib6)).
    In this work we explore memorization scores applied as a data pruning ranking.
    We use the memorization score as defined by Biderman et al. ([2023a](#bib.bib6)):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型中的记忆现象已被广泛研究（Carlini 等，[2023](#bib.bib12)，[2021](#bib.bib11)；Biderman 等，[2023a](#bib.bib6)）。在这项工作中，我们探索了将记忆评分应用为数据剪枝排名。我们使用Biderman
    等定义的记忆评分（[2023a](#bib.bib6)）：
- en: '|  | $score(M,N)=\frac{1}{N}\sum_{i}^{N}1(z_{M+i}=\hat{z}_{M+i})$ |  | (7)
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $score(M,N)=\frac{1}{N}\sum_{i}^{N}1(z_{M+i}=\hat{z}_{M+i})$ |  | (7)
    |'
- en: where $z$. We note that the authors did not originally propose this as data
    pruning metric, but we hypothesize that it can be a valuable ranking to identity
    examples which require additional learning. We use reference models guaranteed
    to have seen the full training set to ensure the applicability of memorization
    scores. A high memorization score indicates the model reproduces more of the text
    verbatim.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $z$。我们注意到作者最初并未将其作为数据剪枝指标，但我们假设它可以作为一种有价值的排名来识别需要额外学习的示例。我们使用保证已看到完整训练集的参考模型，以确保记忆评分的适用性。高记忆评分表示模型逐字重现了更多的文本。
- en: 2.1.4 Random Pruning
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4 随机剪枝
- en: 'We also evaluate a lower bound of expected performance: pruning a random selection
    of samples. This allows us to ask the question “are proposed pruning methods any
    better than a *random guess*?”'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还评估了期望性能的下界：对随机选择的样本进行剪枝。这让我们可以提出“提出的剪枝方法是否比*随机猜测*更好？”的问题。
- en: 3 Experiments
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: 3.1 Model
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 模型
- en: 'We train autoregressive decoder-only Transformer models (Vaswani et al., [2023](#bib.bib42))
    with a standard language modeling objective. Given an input sequence of $z_{i}=$
    is trained to minimize the negative log-likelihood loss as defined in Equation
    [5](#S2.E5 "Equation 5 ‣ 2.1.1 Selection via Perplexity ‣ 2.1 Pruning Methods
    ‣ 2 Methodology ‣ When Less is More: Investigating Data Pruning for Pretraining
    LLMs at Scale"). Our language models follow the traditional GPT-style architecture
    (Radford et al., [2018](#bib.bib33)).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练自回归解码器仅模型（Vaswani 等，[2023](#bib.bib42)），以标准的语言建模目标为基础。给定一个输入序列 $z_{i}=$，训练目标是最小化负对数似然损失，如方程
    [5](#S2.E5 "方程 5 ‣ 2.1.1 通过困惑度选择 ‣ 2.1 剪枝方法 ‣ 2 方法论 ‣ 当少即是多：研究大规模预训练 LLM 的数据剪枝")
    所定义。我们的语言模型遵循传统的 GPT 风格架构（Radford 等，[2018](#bib.bib33)）。
- en: While training our models, we use AdamW (Loshchilov & Hutter, [2019](#bib.bib24))
    with linear cosine scaling and a batch size of 2048\. The 124M parameter models
    are trained for 8000 steps, which amounts to a total of 33B tokens with a learning
    rate that linearly increases from 0 to 1.5e-4 over the course of training. This
    is approximately 4.4 epochs over the unpruned dataset. We tokenize the data with
    Byte Pair Encoding (Sennrich et al., [2016](#bib.bib37)) with a vocabulary of
    51200. Due to the memory and computational costs of training 1.5B parameter models,
    our experiments at this size are trained with a batch size of 512 for 14568 steps.
    As such, the models see only 7.6B tokens, equivalent to a single epoch of our
    unpruned dataset. The learning rate for 1.5B parameter models linearly increases
    from 0 to 1.2e-4 over the course of training. All models use a context window
    length of 2048.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练我们的模型时，我们使用 AdamW (Loshchilov & Hutter, [2019](#bib.bib24))，采用线性余弦缩放和 2048
    的批量大小。124M 参数模型训练了 8000 步，总计 33B 个标记，学习率在训练过程中线性从 0 增加到 1.5e-4。这大约是对未剪枝数据集的 4.4
    轮迭代。我们使用 Byte Pair Encoding (Sennrich et al., [2016](#bib.bib37)) 对数据进行分词，词汇表为
    51200。由于训练 1.5B 参数模型的内存和计算成本，我们在此大小下的实验使用 512 的批量大小进行 14568 步训练。因此，这些模型仅查看了 7.6B
    个标记，相当于未剪枝数据集的一个完整周期。1.5B 参数模型的学习率在训练过程中线性从 0 增加到 1.2e-4。所有模型使用 2048 的上下文窗口长度。
- en: 3.2 Data
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 数据
- en: We use a random sample of the May 2022 snapshot of CommonCrawl¹¹1https://data.commoncrawl.org/
    in our experiments. After downsampling the unpruned dataset has 7.6B tokens, about
    20% of the full snapshot. This downsampling is required due to the computational
    cost of our various ablation experiments, which each require pretraining a new
    model from random initialization. This dataset is prefiltered using a combination
    of automatic and hand-crafted filters, as we aim to further improve data quality
    beyond common rule-based filters. The filters exclude repetitive documents, documents
    with percentages of special characters, and documents that contain explicit words
    and toxic text, similar to deduplication steps seen in Taylor et al. ([2022](#bib.bib40));
    Kocetkov et al. ([2022](#bib.bib21)). Our Wikipedia dataset contains 5.3M tokens
    and only includes English pages.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实验中使用了 2022 年 5 月的 CommonCrawl 快照¹¹1https://data.commoncrawl.org/ 的随机样本。下采样后的未剪枝数据集包含
    7.6B 个标记，约占完整快照的 20%。由于各种消融实验的计算成本需要从随机初始化重新训练新模型，因此需要进行下采样。该数据集经过自动和手工筛选器的组合预筛选，我们旨在进一步提高数据质量，超越常见的基于规则的筛选器。这些筛选器排除了重复的文档、含有特殊字符百分比的文档，以及包含明确词汇和有毒文本的文档，类似于
    Taylor et al. ([2022](#bib.bib40))；Kocetkov et al. ([2022](#bib.bib21)) 中的去重步骤。我们的维基百科数据集包含
    5.3M 个标记，仅包括英文页面。
- en: 3.3 Ablations
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 消融实验
- en: '| Experimental axes | Choices |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 实验轴 | 选择 |'
- en: '| --- | --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Pruning Metric | Perplexity, EL2N, Memorization |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝指标 | 困惑度，EL2N，记忆 |'
- en: '| Pct. Data Remaining | 10, 30, 50, 70 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 剩余数据百分比 | 10，30，50，70 |'
- en: '| Pruning Subset | Bottom, Middle, Top |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝子集 | 底部，中间，顶部 |'
- en: '| Reference Model Size | 124M, 6B, 13B, 52B |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 参考模型大小 | 124M, 6B, 13B, 52B |'
- en: '| Reference Model Epoch Perc. | 14%, 55%, 440%, Full |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 参考模型训练步数百分比 | 14%，55%，440%，全部 |'
- en: '| Reference Model Tr. Data | CC, Wiki, Web-scale |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 参考模型训练数据 | CC, Wiki, Web-scale |'
- en: '| Trained Model Size | 124M, 1.5B |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 训练模型大小 | 124M, 1.5B |'
- en: 'Table 1: Pruning choices explored in the experiments. Under “Reference Model
    Training Steps”, “Full” refers to the fully trained Cohere LLMs. Under “Reference
    Model Training Data”, “Web-scale” refers to the significantly larger training
    datasets used by the Cohere reference models.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：实验中探索的剪枝选择。在“参考模型训练步骤”下，“全部”指完全训练的 Cohere LLMs。在“参考模型训练数据”下，“Web-scale”指
    Cohere 参考模型使用的显著更大的训练数据集。
- en: For all techniques, we compare performance when only 10%, 30%, 50%, and 70%
    of all data is preserved. We compare retaining the top, middle, and bottom subsets
    according to the pruning ranking, e.g., when retaining 30% of the bottom of the
    pruning metric’s distribution over the training set, we calculate the 30th percentile
    of the pruning metric’s distribution and remove all data points with perplexity
    above it. When retaining the middle 30%, we calculate the 35th and 65th percentile
    and remove all data points above and below those numbers respectively. Each ablation
    study(pruning method, percent data remaining, section of distribution preserved)
    requires training a new model from random initialization. We train a minimum of
    nine models with 124M parameters from scratch for each experimental variant.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有技术，我们比较了仅保留 10%、30%、50% 和 70% 数据时的性能。我们比较了根据剪枝排名保留的顶部、中间和底部子集。例如，当保留剪枝指标分布底部的
    30% 时，我们计算剪枝指标分布的第 30 百分位数，并删除所有困惑度高于该值的数据点。当保留中间 30% 时，我们计算第 35 百分位数和第 65 百分位数，并分别删除高于和低于这些数字的数据点。每个剪枝方法（剪枝方法、剩余数据百分比、保留的分布部分）都需要从随机初始化开始训练一个新模型。我们为每个实验变体从头开始训练至少九个具有
    124M 参数的模型。
- en: 'Table [1](#S3.T1 "Table 1 ‣ 3.3 Ablations ‣ 3 Experiments ‣ When Less is More:
    Investigating Data Pruning for Pretraining LLMs at Scale") summarizes the perplexity
    pruning variations we explore in this paper. For perplexity, we use a separate
    model to compute perplexity from the model trained on the pruned data. We call
    models used to compute the perplexity ranking reference models and the models
    trained on the pruned datasets pruned models. We conduct a rigorous evaluation
    of what impacts the quality of the ranking by varying different factors that affect
    the perplexity distribution:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [1](#S3.T1 "Table 1 ‣ 3.3 Ablations ‣ 3 Experiments ‣ When Less is More:
    Investigating Data Pruning for Pretraining LLMs at Scale") 总结了我们在本文中探索的困惑度剪枝变化。对于困惑度，我们使用一个单独的模型来计算来自剪枝数据上训练的模型的困惑度。我们将用于计算困惑度排名的模型称为参考模型，而在剪枝数据集上训练的模型称为剪枝模型。我们通过改变影响困惑度分布的不同因素，对排名质量的影响进行了严格评估：'
- en: '1.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Reference Model Size To explore how reference model size impacts the rating
    quality, we compare perplexity computations using 6B, 13B, and 52B Cohere models
    trained on full web-scale datasets.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参考模型大小 为了探索参考模型大小如何影响评分质量，我们比较了使用 6B、13B 和 52B Cohere 模型在全网规模数据集上训练后的困惑度计算。
- en: '2.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Reference Model Training Data To isolate the impact of training data, we compute
    perplexity using 124M parameter reference models trained on either CommonCrawl
    or Wikipedia.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参考模型训练数据 为了隔离训练数据的影响，我们使用 124M 参数的参考模型，这些模型在 CommonCrawl 或 Wikipedia 上进行训练，并计算困惑度。
- en: '3.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Total Reference Model Training Steps To isolate the impact of early training
    signals, we compute perplexity and EL2N using 124M parameter models trained on
    CommonCrawl data for approximately 14% and 55% of total training steps. Reference
    models trained on CommonCrawl are trained on a non-overlapping subset from the
    CommonCrawl dataset that is pruned and used to train the student model.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参考模型训练步数 为了隔离早期训练信号的影响，我们使用在 CommonCrawl 数据上训练的 124M 参数模型，计算困惑度和 EL2N，训练大约 14%
    和 55% 的总训练步骤。参考模型在 CommonCrawl 上的训练是基于 CommonCrawl 数据集的一个非重叠子集，这些数据集经过剪枝并用于训练学生模型。
- en: 3.4 Evaluation
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 评估
- en: We report perplexity on a test set from the same CommonCrawl snapshot with identical
    prefiltering as the training data. This test set contains 266M tokens, equivalent
    to about 3.5% of the training set.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告了来自与训练数据相同的 CommonCrawl 快照的测试集的困惑度，该测试集的预筛选与训练数据相同。该测试集包含 2.66 亿个词元，相当于训练集的约
    3.5%。
- en: We also finetune a subset of our models on six different classification tasks
    from GLUE (Wang et al., [2019](#bib.bib43)).We do not prune the task dataset,
    as our aim is to analyze the pruning methods’ effects on pretraining. We compare
    performance after 8000 steps (approximately 4.4 epochs of the pretraining dataset),
    chosen to compare performance after models have saturated their capacity by training
    enough steps to plateau on validation metrics.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对 GLUE 的六个不同分类任务对我们的模型子集进行了微调（Wang 等，[2019](#bib.bib43)）。我们没有剪枝任务数据集，因为我们的目的是分析剪枝方法对预训练的影响。我们在
    8000 步（大约 4.4 轮预训练数据集）后比较性能，选择这个步数是为了在模型训练到在验证指标上达到饱和的阶段后进行比较。
- en: 4 Results and Discussion
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果与讨论
- en: '![Refer to caption](img/91c0e61745fc42d357b8d6aac02d2fdf.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/91c0e61745fc42d357b8d6aac02d2fdf.png)'
- en: 'Figure 2: The effect of employing reference models of different sizes on the
    computation of pruning perplexity scores and its subsequent influence on test
    set perplexity. The three subset selection approaches for each set of experiments
    are showcased separately (keeping bottom, middle, or top of the pruning score
    distribution).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：使用不同大小的参考模型对修剪困惑度评分的计算及其对测试集困惑度的后续影响。每组实验的三个子集选择方法分别展示（保持修剪分数分布的底部、中部或顶部）。
- en: 4.1 Removing Easy Instances Improves Performance
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 移除简单实例提升性能
- en: 'Though the most competitive variant for each pruning method varies based on
    the subset of the scoring distribution retained (top, middle, or bottom), we observe
    a consistent pattern: the highest performant variants are *not* the subsets that
    correspond to the “easier” data. The interpretation of the term “easy” varies
    according to the measurement employed. When employing the Perplexity metric, it
    refers to the bottom samples with the lowest perplexity. With the EL2N metric,
    it also pertains to the bottom samples exhibiting the lowest initial loss. In
    the context of memorization, it relates to the top samples that have been most
    thoroughly memorized.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每种修剪方法的最具竞争力的变体根据保留的评分分布子集（顶部、中间或底部）而有所不同，但我们观察到一个一致的模式：表现最好的变体*不是*那些对应于“更简单”数据的子集。对“简单”一词的解释根据所使用的测量方式有所不同。当使用困惑度指标时，它指的是困惑度最低的底部样本。当使用
    EL2N 指标时，它也涉及到初始损失最低的底部样本。在记忆化的背景下，它与那些被最彻底记住的顶部样本相关。
- en: 'Figure [2](#S4.F2 "Figure 2 ‣ 4 Results and Discussion ‣ When Less is More:
    Investigating Data Pruning for Pretraining LLMs at Scale") demonstrates this pattern
    when using Perplexity. In contrast to the middle or top subsets, the bottom subset
    has much less variance in results between reference models of varying sizes, indicating
    the bottom subset may not be suitable for training. The middle experiments achieve
    consistently low test set perplexities for various reference model sizes and pruning
    ratios. Generally, performance monotonically degrades as the amount of data remaining
    shrinks - except for the middle subset for the best-performing reference models.
    In these cases, retaining only 50% and even 30% of the dataset outperforms retaining
    70% of the dataset.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [2](#S4.F2 "Figure 2 ‣ 4 Results and Discussion ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale") 演示了使用困惑度时的这种模式。与中间或顶部子集相比，底部子集在不同大小的参考模型之间的结果变异性要小得多，表明底部子集可能不适合用于训练。中间实验在各种参考模型大小和修剪比例下，一致地取得了较低的测试集困惑度。通常情况下，随着剩余数据量的减少，性能单调下降——除非是最佳表现的参考模型的中间子集。在这些情况下，保留仅
    50% 甚至 30% 的数据集优于保留 70% 的数据集。'
- en: 'Next, Figure [3(b)](#S4.F3.sf2 "Figure 3(b) ‣ Figure 3 ‣ 4.1 Removing Easy
    Instances Improves Performance ‣ 4 Results and Discussion ‣ When Less is More:
    Investigating Data Pruning for Pretraining LLMs at Scale")(a) shows the results
    for the EL2N metric.The middle subset is also the best variant for EL2N. While
    the best performing run does not outperform the baseline, the best performance
    is achieved when retaining 50% of the middle subset, outperforming the model trained
    on 70% of the dataset, similar to the results when using perplexity. As the middle
    subset grows, it begins to overlap with the easiest examples, degrading performance.
    In section [4.5](#S4.SS5 "4.5 Early Reference Model Checkpoints Serve as Effective
    Scoring Models ‣ 4 Results and Discussion ‣ When Less is More: Investigating Data
    Pruning for Pretraining LLMs at Scale"), we discuss how different model checkpoints
    influence the effectiveness of the EL2N metric.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，图 [3(b)](#S4.F3.sf2 "Figure 3(b) ‣ Figure 3 ‣ 4.1 Removing Easy Instances
    Improves Performance ‣ 4 Results and Discussion ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale")(a) 显示了 EL2N 指标的结果。中间子集也是 EL2N 的最佳变体。虽然表现最好的运行没有超越基线，但在保留
    50% 的中间子集时实现了最佳性能，超越了在 70% 数据集上训练的模型，这与使用困惑度的结果相似。随着中间子集的增长，它开始与最简单的示例重叠，导致性能下降。在
    [4.5](#S4.SS5 "4.5 Early Reference Model Checkpoints Serve as Effective Scoring
    Models ‣ 4 Results and Discussion ‣ When Less is More: Investigating Data Pruning
    for Pretraining LLMs at Scale") 节中，我们讨论了不同模型检查点如何影响 EL2N 指标的有效性。'
- en: 'Finally, when using memorization factor as a pruning metric, keeping the least
    memorized samples (bottom subset) generally performs best. Figure [3(b)](#S4.F3.sf2
    "Figure 3(b) ‣ Figure 3 ‣ 4.1 Removing Easy Instances Improves Performance ‣ 4
    Results and Discussion ‣ When Less is More: Investigating Data Pruning for Pretraining
    LLMs at Scale")(b) shows model performances for this metric. We observe that the
    most competitive variant of the memorization metric is the bottom 70% of the distribution.
    Memorization never outperforms the no-pruning baseline.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，使用记忆化因子作为剪枝度量时，通常保留记忆最少的样本（底部子集）表现最佳。图[3(b)](#S4.F3.sf2 "Figure 3(b) ‣ Figure
    3 ‣ 4.1 Removing Easy Instances Improves Performance ‣ 4 Results and Discussion
    ‣ When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale")(b)展示了该度量下的模型表现。我们观察到，记忆化度量中最具竞争力的变体是分布的底部
    70%。记忆化从未优于未剪枝基线。'
- en: '![Refer to caption](img/a2694aecf530153d35cfd50da2b609bd.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a2694aecf530153d35cfd50da2b609bd.png)'
- en: (a) EL2N
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (a) EL2N
- en: '![Refer to caption](img/467a3152f4b06afd274eb0ddfb1ad842.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/467a3152f4b06afd274eb0ddfb1ad842.png)'
- en: (b) Memorization
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 记忆化
- en: 'Figure 3: Evaluation of different subset selection criteria for two pruning
    metrics: (a) EL2N and (b) Memorization.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：对两个剪枝度量的不同子集选择标准的评估：(a) EL2N 和 (b) 记忆化。
- en: 4.2 Simple Pruning Metrics Outperform More Sophisticated Approaches
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 简单剪枝度量优于更复杂的方法
- en: 'In Figure [4](#S4.F4 "Figure 4 ‣ 4.2 Simple Pruning Metrics Outperform More
    Sophisticated Approaches ‣ 4 Results and Discussion ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale") we present results comparing the
    performance of the best variant of each pruning metric: (1) retaining the middle
    of the distribution of Perplexity scores by the fully trained 52B reference model,
    (2) retaining the bottom of the distribution of the Memorization Factor (least
    memorized samples), and (3) retaining the middle of the distribution of EL2N scores
    from the 1000 step checkpoint. We also include results for our baselines: a model
    trained on the entirety of the training data $\mathcal{D}$ and models trained
    on randomly pruned data. Our results show that training on the middle subset using
    Perplexity outperforms other pruning metrics across all dataset sizes. For some
    variants, it also outperforms training on the entire dataset. For example, at
    30% and 50% of the original dataset size, Perplexity outperforms the full dataset
    size. Compared with the no-pruning baseline, pruning to the middle 50% of the
    perplexity distribution leads to a 0.97% improvement in perplexity. Using only
    the middle 30% of the data achieves nearly the same performance, with a 0.80%
    improvement over the no-pruning baseline.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '在图[4](#S4.F4 "Figure 4 ‣ 4.2 Simple Pruning Metrics Outperform More Sophisticated
    Approaches ‣ 4 Results and Discussion ‣ When Less is More: Investigating Data
    Pruning for Pretraining LLMs at Scale")中，我们展示了每种剪枝度量的最佳变体性能比较：(1) 保留 52B 参考模型完全训练的困惑度分布中间部分，(2)
    保留记忆化因子的分布底部（记忆最少的样本），以及 (3) 保留 1000 步检查点中 EL2N 分数的分布中间部分。我们还包括了基线结果：一个在整个训练数据
    $\mathcal{D}$ 上训练的模型和在随机剪枝数据上训练的模型。我们的结果显示，使用困惑度对中间子集进行训练在所有数据集大小中优于其他剪枝度量。对于某些变体，它也优于在整个数据集上训练。例如，在原始数据集大小的
    30% 和 50% 下，困惑度优于完整数据集大小。与未剪枝基线相比，剪枝到困惑度分布的中间 50% 导致困惑度改善 0.97%。仅使用中间 30% 的数据几乎达到了相同的性能，比未剪枝基线改善了
    0.80%。'
- en: '![Refer to caption](img/7c7083b796df47578b3f79c62bf548fd.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7c7083b796df47578b3f79c62bf548fd.png)'
- en: 'Figure 4: The top performing variants of the different pruning methods, compared
    across various dataset sizes. Random pruning and no-pruning are included as baselines.
    Perplexity-based pruning consistently surpasses both alternative metrics and the
    no pruning experiments. See Section [4.2](#S4.SS2 "4.2 Simple Pruning Metrics
    Outperform More Sophisticated Approaches ‣ 4 Results and Discussion ‣ When Less
    is More: Investigating Data Pruning for Pretraining LLMs at Scale") for details
    on the featured variants.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4：不同剪枝方法的最佳变体在各种数据集大小下的比较。随机剪枝和未剪枝作为基线进行比较。基于困惑度的剪枝始终优于其他度量和未剪枝实验。详细信息见第[4.2节](#S4.SS2
    "4.2 Simple Pruning Metrics Outperform More Sophisticated Approaches ‣ 4 Results
    and Discussion ‣ When Less is More: Investigating Data Pruning for Pretraining
    LLMs at Scale")。'
- en: 'Compared with random selection, pruning using Perplexity results in significantly
    higher model performance than random pruning across all data ratios (Figure [4](#S4.F4
    "Figure 4 ‣ 4.2 Simple Pruning Metrics Outperform More Sophisticated Approaches
    ‣ 4 Results and Discussion ‣ When Less is More: Investigating Data Pruning for
    Pretraining LLMs at Scale")). For memorization and EL2N pruning metrics, both
    achieve similar performances to random pruning despite being far more computationally
    expensive.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '与随机选择相比，使用困惑度进行剪枝在所有数据比例下的模型表现显著优于随机剪枝（图 [4](#S4.F4 "Figure 4 ‣ 4.2 Simple
    Pruning Metrics Outperform More Sophisticated Approaches ‣ 4 Results and Discussion
    ‣ When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale")）。对于记忆化和EL2N剪枝指标，尽管计算成本高得多，但它们的表现与随机剪枝相似。'
- en: 4.3 Pruning Benefits from Using Larger Reference Models
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 使用更大参考模型的剪枝好处
- en: 'Given that the most competitive variant perplexity uses a reference model to
    compute scores, we expect that the size of the reference model will have a significant
    impact on the data pruned. Figure [2](#S4.F2 "Figure 2 ‣ 4 Results and Discussion
    ‣ When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale")
    shows the trained model performances after pruning with perplexity calculated
    with reference models ranging from 124M to 52B parameters. We find that increasing
    reference model size improves trained model performance over the no-pruning baseline
    when either the middle or top subsets are used. Data pruning using the perplexity
    scores generated from a 52B parameter reference model achieves a 2.2% improvement
    in perplexity over the best-performing trained model from the 124M parameter reference
    model experiments. Furthermore, for 13B and 52B reference models, we observe better
    performances with less training data when keeping the middle and top subsets.
    For both of these larger models, retaining the middle 30% and 50% of the training
    data produces pruned models that outperform the pruned models trained on the middle
    70% of the training set.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '鉴于最具竞争力的变体困惑度使用参考模型来计算分数，我们预计参考模型的规模会对剪枝的数据产生显著影响。图 [2](#S4.F2 "Figure 2 ‣
    4 Results and Discussion ‣ When Less is More: Investigating Data Pruning for Pretraining
    LLMs at Scale") 显示了使用困惑度剪枝后训练模型的表现，其中参考模型的参数从124M到52B。我们发现，当使用中间或顶部子集时，增加参考模型的规模会改善训练模型的表现，优于无剪枝基准。使用来自52B参数参考模型生成的困惑度分数进行数据剪枝，相较于124M参数参考模型实验中的最佳训练模型，困惑度改善了2.2%。此外，对于13B和52B参考模型，我们观察到在保持中间和顶部子集时，较少的训练数据能获得更好的表现。对于这两个较大的模型，保留中间30%和50%的训练数据会产生优于使用中间70%训练数据训练的剪枝模型的剪枝模型。'
- en: '![Refer to caption](img/bb8e8c07dc513dadcef4eea3a13ae0ae.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bb8e8c07dc513dadcef4eea3a13ae0ae.png)'
- en: 'Figure 5: Performance of different pruning strategies using two different reference
    models: one trained on Wikipedia and one trained on CommonCrawl. A reference model
    trained on Wikipedia (an example of a clean noise-free corpus) achieves consistently
    lower validation perplexity compared to a reference model trained on a noisier
    CommonCrawl in our two robust settings (middle and top).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：使用两个不同的参考模型（一个训练在维基百科上，另一个训练在CommonCrawl上）的不同剪枝策略的表现。与训练在噪声较大的CommonCrawl上的参考模型相比，训练在维基百科上的参考模型（一个干净无噪声的语料库示例）在我们的两个鲁棒设置（中间和顶部）中获得了一致较低的验证困惑度。
- en: We note that the effects of subset selection, such as the bottom subset performing
    worse, approximately scale with the size of the reference models. The larger reference
    models’ bottom subset training runs perform even worse than their smaller counterparts
    when retaining the same percentage of the training set. This overall points to
    the consistent finding that larger models are better calibrated at computing a
    useful data pruning ranking.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，子集选择的效果，比如底部子集表现较差，大致与参考模型的规模成比例。较大的参考模型的底部子集训练结果比较小的参考模型更差，即使保持相同的训练集百分比。这总体上指出了一个一致的发现：较大的模型在计算有用的数据剪枝排名方面校准得更好。
- en: 4.4 Improved Pruning Signals Result from Reference Models Trained on Cleaner
    Data
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 从训练在更干净数据上的参考模型中改进剪枝信号
- en: 'In this section we ask: does the data the reference model is trained on impact
    the quality of the ranking? We compare the perplexity rankings generated by reference
    models trained on two different corpora: Wikipedia and CommonCrawl. We investigate
    whether a model trained on Wikipedia, a dataset frequently hand-picked as a high-quality
    dataset (Xie et al., [2023b](#bib.bib46); Wenzek et al., [2020](#bib.bib44)),
    generates more effective pruning signals for perplexity rankings. In Figure [5](#S4.F5
    "Figure 5 ‣ 4.3 Pruning Benefits from Using Larger Reference Models ‣ 4 Results
    and Discussion ‣ When Less is More: Investigating Data Pruning for Pretraining
    LLMs at Scale"), we compare the performance of the two variants across different
    pruning percentages and subset selections. We observe that in the two optimal
    selection variants from the general reference models (middle and top) a model
    trained on Wikipedia consistently yields lower validation perplexity compared
    to a model trained on CommonCrawl. Wikipedia’s best variant, pruning to the middle
    70%, outperforms CommonCrawl’s best variant, also pruning to the middle 70%, by
    0.69%. This finding overall suggests that investing in a high quality reference
    model to generate rankings results in more effective data pruning. Reference models
    trained on higher quality data are better at identifying a subset of data points
    most conducive to model performance.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们探讨了参考模型所训练的数据是否会影响排名的质量。我们比较了在两个不同语料库（维基百科和CommonCrawl）上训练的参考模型生成的困惑度排名。我们研究了在经常被挑选为高质量数据集（Xie
    et al., [2023b](#bib.bib46); Wenzek et al., [2020](#bib.bib44)）的维基百科上训练的模型，是否会生成更有效的剪枝信号。在图[5](#S4.F5
    "Figure 5 ‣ 4.3 Pruning Benefits from Using Larger Reference Models ‣ 4 Results
    and Discussion ‣ When Less is More: Investigating Data Pruning for Pretraining
    LLMs at Scale")中，我们比较了在不同剪枝百分比和子集选择下，两种变体的表现。我们观察到，在通用参考模型的两个最佳选择变体（中间和顶部）中，训练于维基百科的模型相比于训练于CommonCrawl的模型，验证困惑度始终较低。维基百科的最佳变体，中间70%的剪枝，比CommonCrawl的最佳变体（同样剪枝至中间70%）提高了0.69%。这一发现总体上表明，投资高质量的参考模型以生成排名会导致更有效的数据剪枝。训练于更高质量数据的参考模型在识别对模型性能最有利的数据点子集方面表现更佳。'
- en: 4.5 Early Reference Model Checkpoints Serve as Effective Scoring Models
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 早期参考模型检查点作为有效的评分模型
- en: '![Refer to caption](img/bc94fda04d7698f21bc6c033f69448f7.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/bc94fda04d7698f21bc6c033f69448f7.png)'
- en: 'Figure 6: The impact of using an early checkpoint of the reference model in
    pruning based on Perplexity and EL2N metrics.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：使用参考模型的早期检查点对困惑度和EL2N指标进行剪枝的影响。
- en: 'Motivated by several works that have found that there is a signal in early
    training checkpoints (Paul et al., [2023](#bib.bib30); Agarwal et al., [2022](#bib.bib2);
    Siddiqui et al., [2022](#bib.bib38)), we investigate whether early checkpoint
    of a reference model during training offers adequate signal for calculating discriminative
    pruning scores. We study perplexity and EL2N scores obtained from two early checkpoints:
    after training on approximately 14% and 55% of the full training dataset (250
    and 1000 training steps respectively). Figure [6](#S4.F6 "Figure 6 ‣ 4.5 Early
    Reference Model Checkpoints Serve as Effective Scoring Models ‣ 4 Results and
    Discussion ‣ When Less is More: Investigating Data Pruning for Pretraining LLMs
    at Scale") showcases the results of these experiments. Examining the 14% checkpoint
    for both perplexity and EL2N, we notice minimal variance across percentages and
    subset selection criteria. Performance across subsets changes considerably less
    than either the 55% checkpoint or the fully trained models.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '受一些发现早期训练检查点中存在信号的研究（Paul et al., [2023](#bib.bib30); Agarwal et al., [2022](#bib.bib2);
    Siddiqui et al., [2022](#bib.bib38)）的启发，我们研究了在训练过程中参考模型的早期检查点是否提供了足够的信号来计算区分性剪枝评分。我们研究了从两个早期检查点获得的困惑度和EL2N评分：分别在训练了约14%和55%的完整训练数据集（250步和1000步训练）后。图[6](#S4.F6
    "Figure 6 ‣ 4.5 Early Reference Model Checkpoints Serve as Effective Scoring Models
    ‣ 4 Results and Discussion ‣ When Less is More: Investigating Data Pruning for
    Pretraining LLMs at Scale")展示了这些实验的结果。检查14%的检查点对于困惑度和EL2N，我们注意到各百分比和子集选择标准之间的变化很小。与55%检查点或完全训练的模型相比，子集之间的性能变化要小得多。'
- en: Given this, we deduce that training on only 14% of the data is inadequate for
    our reference model to offer precise pruning scores. In contrast, the 55% reference
    models perform in a similar manner to the fully trained models, performing best
    with the middle subset, worst with the bottom subset, and comparably with the
    top subset. Fully training the reference model is shown not to be necessary to
    uphold comparable performance. Halving the reference model training steps proves
    effective, enabling the utilization of early checkpoints. In practice, we expect
    many practitioners to use off the shelf models for computing perplexity and may
    not need to carry the cost of pretraining a reference model from random initialization.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，我们推断仅使用 14% 的数据对我们的参考模型进行训练是不够的，以提供准确的剪枝分数。相比之下，55% 参考模型的表现与全面训练的模型相似，在中间子集上表现最佳，在底部子集上表现最差，而在顶部子集上表现相当。完全训练参考模型被证明不是保持类似性能的必要条件。将参考模型训练步骤减半是有效的，使得可以利用早期检查点。在实际操作中，我们预计许多实践者将使用现成模型来计算困惑度，可能不需要承受从随机初始化开始预训练参考模型的成本。
- en: We also show performance for EL2N scores averaged across 10 reference models,
    initialized with different random seeds. We selected the 55% reference models
    given our previous result.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了跨 10 个参考模型的 EL2N 分数的平均性能，这些模型以不同的随机种子初始化。我们选择了 55% 参考模型基于我们之前的结果。
- en: While the best pruned models using the averaged EL2N score did not outperform
    the best pruned models trained on only one reference model’s EL2N score, the pattern
    of performance more similarly mirrors what we see with the larger, fully trained
    reference models. Specifically, in the middle subset, using 50% of the dataset
    outperforms using 70%. When constrained to the bottom subset, performance more
    clearly monotonically degrades when using less data than when using the 55% reference
    model, whereas the earlier checkpoint has comparable performance when retaining
    30, 50, and 70% of the data. This implies that averaging scores across reference
    models helps hone the pruning signal, identifying subsets “easy" or “hard" subsets
    in more similar ways to larger models.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用平均 EL2N 分数的最佳剪枝模型并未超越仅用单一参考模型 EL2N 分数训练的最佳剪枝模型，但其性能模式更类似于我们在更大、更全面训练的参考模型中看到的情况。具体来说，在中间子集上，使用
    50% 的数据集的效果优于使用 70%。当限制在底部子集时，性能在使用比 55% 参考模型更少的数据时明显递减，而在保留 30%、50% 和 70% 数据时，早期检查点的表现相当。这表明，跨参考模型的平均分数有助于精细化剪枝信号，识别“容易”或“困难”的子集与更大模型类似。
- en: 4.6 Perplexity-based Pruning Improvements Generalize to Larger Scale Models
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 基于困惑度的剪枝改进在更大规模模型中具有通用性
- en: '![Refer to caption](img/d6e609d60dfb401ff85735e71fce6284.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d6e609d60dfb401ff85735e71fce6284.png)'
- en: 'Figure 7: Comparing the best performing pruning method (keeping the middle
    subset using a 52B parameter reference model) with random pruning at two distinct
    pruned model scales. The improvement in performance of a perplexity-based pruning
    approach carries from 124M to 1.5B parameter models.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：在两个不同剪枝模型规模下比较最佳表现剪枝方法（使用 52B 参数参考模型保留中间子集）与随机剪枝。困惑度剪枝方法的性能改进从 124M 扩展到
    1.5B 参数模型。
- en: 'We take our strongest pruning variant – perplexity computed using a 52B parameter
    reference model while retaining the middle subset – to explore the robustness
    of our findings at a larger scale by validating our findings on a 1.5B model.
    Figure [7](#S4.F7 "Figure 7 ‣ 4.6 Perplexity-based Pruning Improvements Generalize
    to Larger Scale Models ‣ 4 Results and Discussion ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale") shows pruning scaling from 124M to
    1.5B parameter models. Training a 1.5B model, we observe that random pruning performs
    considerably well, even reaching levels below the no-pruning run. Nonetheless,
    perplexity-based pruning achieves better results than random pruning across all
    pruning percentages. The improvement observed with perplexity-based pruning over
    random pruning follows a consistent pattern for both the 124M and 1.5B models.
    This demonstrates the scalability of our approach to a large-scale pretraining
    setting.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用最强的剪枝变体——使用 52B 参数参考模型计算的困惑度，同时保留中间子集——来探索我们发现的在更大规模上的鲁棒性，通过在 1.5B 模型上验证我们的发现。图 [7](#S4.F7
    "图 7 ‣ 4.6 基于困惑度的剪枝改进在更大规模模型上的泛化 ‣ 4 结果与讨论 ‣ 当更少即更多：探究大规模预训练 LLM 的数据剪枝") 显示了剪枝从
    124M 扩展到 1.5B 参数模型。训练 1.5B 模型时，我们观察到随机剪枝表现相当好，甚至达到低于无剪枝运行的水平。尽管如此，基于困惑度的剪枝在所有剪枝百分比上都比随机剪枝取得了更好的结果。基于困惑度的剪枝相对于随机剪枝的改进在
    124M 和 1.5B 模型中都遵循了一致的模式。这证明了我们的方法在大规模预训练设置中的可扩展性。
- en: 4.7 Downstream Evaluation on GLUE
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 GLUE 上的下游评估
- en: 'Table 2: Mean accuracy and standard deviation of the best variants of each
    pruning algorithm for GLUE classification tasks. Underlined results surpass the
    baseline performance with no pruning. The best results for each task are marked
    in bold. Results are reported for 5 runs of each model, trained for 3 epochs with
    a learning rate of $1e-5$.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：每种剪枝算法在 GLUE 分类任务中的最佳变体的平均准确率和标准差。下划线结果超越了未剪枝的基准性能。每个任务的最佳结果用**粗体**标出。结果报告了每个模型的
    5 次运行，训练了 3 个周期，学习率为 $1e-5$。
- en: '|  | Data Remaining | SST2 | MRPC | QQP | QNLI | RTE | WNLI |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据剩余 | SST2 | MRPC | QQP | QNLI | RTE | WNLI |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| No Pruning | 100% | 78.15[0.002] | 64.32[0.021] | 76.55[0.001] | 65.40[0.006]
    | 49.69[0.024] | 51.56[0.040] |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 无剪枝 | 100% | 78.15[0.002] | 64.32[0.021] | 76.55[0.001] | 65.40[0.006] |
    49.69[0.024] | 51.56[0.040] |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Random Pruning | 70% | 77.92[0.002] | 65.21[0.017] | 76.58[0.002] | 65.11[0.006]
    | 49.69[0.013] | 48.44[0.038] |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 随机剪枝 | 70% | 77.92[0.002] | 65.21[0.017] | 76.58[0.002] | 65.11[0.006] |
    49.69[0.013] | 48.44[0.038] |'
- en: '| 50% | 78.19[0.003] | 65.16[0.020] | 76.40[0.001] | 65.44[0.006] | 49.92[0.009]
    | 49.69[0.062] |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 78.19[0.003] | 65.16[0.020] | 76.40[0.001] | 65.44[0.006] | 49.92[0.009]
    | 49.69[0.062] |'
- en: '| 30% | 77.29[0.007] | 66.04[0.017] | 76.36[0.001] | 65.22[0.005] | 51.33[0.024]
    | 50.31[0.057] |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 77.29[0.007] | 66.04[0.017] | 76.36[0.001] | 65.22[0.005] | 51.33[0.024]
    | 50.31[0.057] |'
- en: '| 10% | 76.44[0.006] | 65.83[0.021] | 75.91[0.001] | 64.40[0.007] | 50.70[0.007]
    | 50.62[0.016] |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 10% | 76.44[0.006] | 65.83[0.021] | 75.91[0.001] | 64.40[0.007] | 50.70[0.007]
    | 50.62[0.016] |'
- en: '| Memorization Bottom subset | 70% | 77.29[0.006] | 64.38[0.016] | 76.42[0.001]
    | 66.03[0.007] | 49.06[0.021] | 49.06[0.042] |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 记忆底部子集 | 70% | 77.29[0.006] | 64.38[0.016] | 76.42[0.001] | 66.03[0.007]
    | 49.06[0.021] | 49.06[0.042] |'
- en: '| 50% | 77.89[0.006] | 65.47[0.017] | 76.51[0.001] | 65.99[0.005] | 49.77[0.013]
    | 50.31[0.048] |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 77.89[0.006] | 65.47[0.017] | 76.51[0.001] | 65.99[0.005] | 49.77[0.013]
    | 50.31[0.048] |'
- en: '| 30% | 78.52[0.004] | 65.89[0.016] | 76.48[0.001] | 65.91[0.006] | 50.31[0.009]
    | 54.38[0.061] |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 78.52[0.004] | 65.89[0.016] | 76.48[0.001] | 65.91[0.006] | 50.31[0.009]
    | 54.38[0.061] |'
- en: '| 10% | 76.64[0.004] | 65.16[0.015] | 76.11[0.001] | 64.61[0.006] | 50.39[0.016]
    | 51.88[0.059] |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 10% | 76.64[0.004] | 65.16[0.015] | 76.11[0.001] | 64.61[0.006] | 50.39[0.016]
    | 51.88[0.059] |'
- en: '| EL2N Middle subset | 70% | 78.61[0.008] | 66.46[0.018] | 76.93[0.001] | 67.00[0.005]
    | 48.67[0.017] | 50.00[0.058] |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| EL2N 中间子集 | 70% | 78.61[0.008] | 66.46[0.018] | 76.93[0.001] | 67.00[0.005]
    | 48.67[0.017] | 50.00[0.058] |'
- en: '| 50% | 79.17[0.007] | 65.42[0.016] | 76.35[0.001] | 62.43[0.007] | 51.41[0.028]
    | 51.56[0.049] |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 79.17[0.007] | 65.42[0.016] | 76.35[0.001] | 62.43[0.007] | 51.41[0.028]
    | 51.56[0.049] |'
- en: '| 30% | 78.98[0.005] | 65.41[0.012] | 77.47[0.001] | 68.63[0.005] | 49.69[0.022]
    | 55.31[0.067] |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 78.98[0.005] | 65.41[0.012] | 77.47[0.001] | 68.63[0.005] | 49.69[0.022]
    | 55.31[0.067] |'
- en: '| 10% | 78.31[0.006] | 63.38[0.016] | 76.93[0.001] | 65.34[0.006] | 51.95[0.021]
    | 51.25[0.064] |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 10% | 78.31[0.006] | 63.38[0.016] | 76.93[0.001] | 65.34[0.006] | 51.95[0.021]
    | 51.25[0.064] |'
- en: '| Perplexity (52B) Middle subset | 70% | 78.40[0.004] | 64.43[0.020] | 76.68[0.001]
    | 66.74[0.007] | 50.16[0.023] | 49.06[0.012] |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度 (52B) 中间子集 | 70% | 78.40[0.004] | 64.43[0.020] | 76.68[0.001] | 66.74[0.007]
    | 50.16[0.023] | 49.06[0.012] |'
- en: '| 50% | 78.01[0.006] | 64.37[0.021] | 76.82[0.001] | 66.00[0.004] | 50.62[0.023]
    | 50.31[0.021] |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 78.01[0.006] | 64.37[0.021] | 76.82[0.001] | 66.00[0.004] | 50.62[0.023]
    | 50.31[0.021] |'
- en: '| 30% | 77.34[0.005] | 64.84[0.023] | 76.76[0.001] | 65.89[0.002] | 50.86[0.009]
    | 50.94[0.031] |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 77.34[0.005] | 64.84[0.023] | 76.76[0.001] | 65.89[0.002] | 50.86[0.009]
    | 50.94[0.031] |'
- en: '| 10% | 77.66[0.006] | 65.36[0.017] | 76.40[0.001] | 66.52[0.007] | 51.17[0.012]
    | 53.44[0.040] |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 10% | 77.66[0.006] | 65.36[0.017] | 76.40[0.001] | 66.52[0.007] | 51.17[0.012]
    | 53.44[0.040] |'
- en: 'Previously, we demonstrated various ways of pruning the pretraining data and
    training models with different data sizes. Considering that the pretraining stage
    primarily focuses on knowledge acquisition (Zhou et al., [2023](#bib.bib48)),
    we inquire about the potential ripple effects of pruning data during pretraining
    when these models are subsequently finetuned on downstream tasks. To analyze the
    impact of different pruning strategies on LLM capabilities, we finetune and evaluate
    models on a subset of the GLUE tasks (Wang et al., [2019](#bib.bib43)). Results
    are presented in Table [2](#S4.T2 "Table 2 ‣ 4.7 Downstream Evaluation on GLUE
    ‣ 4 Results and Discussion ‣ When Less is More: Investigating Data Pruning for
    Pretraining LLMs at Scale"). We observe that pruning the pretraining dataset consistently
    improves performance across all tasks. While no single pruning strategy (combining
    both pruning metric and percentage of remaining data) stands out as superior across
    all tasks, the absence of a universally dominant approach is consistent with earlier
    findings in the literature (Gao, [2021](#bib.bib16)). We observe that retaining
    only 30% of the least memorized instances yields optimal results for SST2 and
    WNLI tasks. With perplexity based pruning, the best performance is obtained on
    QQP and QNLI tasks by keeping 50% and 70% of the training data, respectively.
    Even random pruning shows improvements in certain tasks, underscoring the significance
    of downsampling when handling noisy data during the pretraining stage to mitigate
    potential learning degradation.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们展示了修剪预训练数据和使用不同数据规模训练模型的各种方法。考虑到预训练阶段主要关注知识获取 (Zhou et al., [2023](#bib.bib48))，我们探讨了在这些模型随后在下游任务中微调时，预训练阶段数据修剪的潜在影响。为了分析不同修剪策略对LLM能力的影响，我们在GLUE任务的一个子集上微调和评估模型
    (Wang et al., [2019](#bib.bib43))。结果展示在表[2](#S4.T2 "表 2 ‣ 4.7 GLUE下游评估 ‣ 4 结果与讨论
    ‣ 当更少就是更多：大规模预训练LLMs的数据修剪研究")。我们观察到修剪预训练数据集在所有任务中一致地提高了性能。尽管没有单一的修剪策略（结合修剪指标和剩余数据百分比）在所有任务中表现突出，但这种普遍缺乏优势的方法与文献中的早期发现一致
    (Gao, [2021](#bib.bib16))。我们观察到，仅保留30%最少被记住的实例在SST2和WNLI任务中获得了最佳结果。通过困惑度基础的修剪，最佳性能在QQP和QNLI任务中分别通过保留50%和70%的训练数据获得。即使是随机修剪在某些任务中也显示出改进，这突显了在预训练阶段处理噪声数据时下采样的重要性，以减轻潜在的学习退化。
- en: 5 Related Work
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: 5.1 Rule-Based Data Pruning in NLP
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基于规则的数据修剪在NLP中的应用
- en: Significant portions of web-scraped data used for language model pretraining
    have been shown to be of low quality, machine-generated spam, pornographic content (Kreutzer
    et al., [2022](#bib.bib22)). Selection processes to determine what should be included
    in large-scale datasets have centered on rule-based filters and heuristics (Bane
    et al., [2022](#bib.bib5)), such as keeping only text written in English (Raffel
    et al., [2020](#bib.bib35); Rae et al., [2022](#bib.bib34)) or removing sequences
    containing blocklisted words (Raffel et al., [2020](#bib.bib35)). There are also
    quality-based rules such as removing duplicated samples (Zhang et al., [2022](#bib.bib47))
    or filtering sentences that do not fit a certain amount of words (Raffel et al.,
    [2020](#bib.bib35); Rae et al., [2022](#bib.bib34)). Rule-based approaches for
    data filtering have shown controversial effects on model performance, with some
    works advertising improvements on language modeling capabilities (Penedo et al.,
    [2023](#bib.bib31); Raffel et al., [2020](#bib.bib35)), while others do not (Black
    et al., [2022](#bib.bib8); Biderman et al., [2023b](#bib.bib7)). Also, heuristics
    are prone to undesired outcomes due to their simplicity. For instance  Dodge et al.
    ([2021](#bib.bib14)) show how removing blocklisted words disproportionately removes
    text from and about minority individuals.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 大量用于语言模型预训练的网络抓取数据被证明质量较低，包含机器生成的垃圾信息、色情内容（Kreutzer 等，[2022](#bib.bib22)）。确定大规模数据集包含哪些内容的选择过程主要依赖于基于规则的过滤器和启发式方法（Bane
    等，[2022](#bib.bib5)），例如仅保留用英语书写的文本（Raffel 等，[2020](#bib.bib35)；Rae 等，[2022](#bib.bib34)）或删除包含黑名单词汇的序列（Raffel
    等，[2020](#bib.bib35)）。还有一些基于质量的规则，比如删除重复样本（Zhang 等，[2022](#bib.bib47)）或过滤不符合一定字数的句子（Raffel
    等，[2020](#bib.bib35)；Rae 等，[2022](#bib.bib34)）。基于规则的数据过滤方法对模型性能的影响存在争议，有些研究宣传了语言建模能力的提升（Penedo
    等，[2023](#bib.bib31)；Raffel 等，[2020](#bib.bib35)），而其他研究则未见明显改进（Black 等，[2022](#bib.bib8)；Biderman
    等，[2023b](#bib.bib7)）。此外，由于其简单性，启发式方法容易产生不良结果。例如，Dodge 等（[2021](#bib.bib14)）展示了如何删除黑名单词汇不成比例地移除了来自和关于少数群体的文本。
- en: 5.2 Metric-Based Data Pruning in NLP
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 基于指标的NLP数据修剪
- en: Recent work on metric-based pruning has mainly focused on pruning data from
    the fine-tuning stage of LLMs (Attendu & Corbeil, [2023](#bib.bib4); Xie et al.,
    [2023b](#bib.bib46)) most probably due to the prohibitive cost of pruning at the
    pretraining scale. Attendu & Corbeil ([2023](#bib.bib4)) perform dynamic pruning
    during the fine-tuning stage by establishing a curriculum of samples based on
    their EL2N scores (Paul et al., [2023](#bib.bib30)). Similarly, we benchmark EL2N
    as a static data-pruning metric for language datasets. Our work joins the few
    others that aim to reduce pretraining dataset sizes (Xie et al., [2023a](#bib.bib45);
    Chen, [2023](#bib.bib13); Abbas et al., [2023](#bib.bib1)). Abbas et al. ([2023](#bib.bib1))
    apply their deduplication method based on embeddings to further improve the performance
    of a previously filtered dataset. We also perform pruning on previously filtered
    datasets, aiming to enhance performance further. Previously, perplexity has been
    used to filter datasets (Muennighoff et al., [2023](#bib.bib28); Wenzek et al.,
    [2020](#bib.bib44); Laurençon et al., [2023](#bib.bib23)), but its pruning capabilities
    have been underexplored. Laurençon et al. ([2023](#bib.bib23)) and Muennighoff
    et al. ([2023](#bib.bib28)) filter out high-perplexity samples from their corpus
    as those are framed as unnatural language and harmful for performance according
    to their reference domain, which is Wikipedia. In contrast, we benchmark pruning
    to low perplexity values and high and medium-valued subsets of a dataset’s distribution
    to understand which is the most valuable section for pretraining at scale. We
    also explore different reference model sizes and training sets.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于度量的修剪工作主要集中在LLMs的微调阶段修剪数据（Attendu & Corbeil, [2023](#bib.bib4); Xie et
    al., [2023b](#bib.bib46)），这可能是由于在预训练规模上修剪的高昂成本。Attendu & Corbeil ([2023](#bib.bib4))通过建立基于EL2N分数的样本课程，在微调阶段执行动态修剪（Paul
    et al., [2023](#bib.bib30)）。类似地，我们将EL2N作为语言数据集的静态数据修剪指标进行基准测试。我们的工作与其他一些旨在减少预训练数据集规模的工作（Xie
    et al., [2023a](#bib.bib45); Chen, [2023](#bib.bib13); Abbas et al., [2023](#bib.bib1)）相结合。Abbas
    et al. ([2023](#bib.bib1))基于嵌入的去重方法来进一步提高先前过滤数据集的性能。我们也对先前过滤的数据集进行修剪，旨在进一步提升性能。之前，困惑度已被用于过滤数据集（Muennighoff
    et al., [2023](#bib.bib28); Wenzek et al., [2020](#bib.bib44); Laurençon et al.,
    [2023](#bib.bib23)），但其修剪能力尚未得到充分探索。Laurençon et al. ([2023](#bib.bib23))和Muennighoff
    et al. ([2023](#bib.bib28))从他们的语料库中过滤掉高困惑度样本，因为这些样本被视为不自然的语言，并且对性能有害，参考领域为维基百科。相比之下，我们将修剪基准设置为低困惑度值以及数据集分布的高值和中等值子集，以了解哪个部分在大规模预训练中最有价值。我们还探索了不同的参考模型大小和训练集。
- en: 5.3 Data pruning in Computer Vision
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 计算机视觉中的数据修剪
- en: The majority of work to date on data pruning (Sorscher et al., [2023](#bib.bib39))
    and isolating data subsets (Siddiqui et al., [2022](#bib.bib38); Mindermann et al.,
    [2022](#bib.bib26)) using model signal has centered on computer vision. These
    are typically structured in a supervised setting. In contrast, our focus is on
    a large-scale NLP pretraining where the objective is unsupervised pretraining.
    Most relevant to our method is work by Sorscher et al. ([2023](#bib.bib39)) which
    empirically studies reducing datasets in a teacher/trained regime, using a teacher
    model’s margin as a pruning metric. They find that, with abundant data, training
    only on the hardest examples yields better performance, while conversely when
    data is scarce, training on only the easiest example yields better performance.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，关于数据修剪（Sorscher et al., [2023](#bib.bib39)）和使用模型信号来隔离数据子集（Siddiqui et al.,
    [2022](#bib.bib38); Mindermann et al., [2022](#bib.bib26)）的大部分工作主要集中在计算机视觉领域。这些研究通常是在监督设置下进行的。相比之下，我们的重点是大规模的NLP预训练，其中目标是无监督预训练。与我们的方法最相关的是Sorscher
    et al. ([2023](#bib.bib39))的工作，他们在教师/训练模式下实证研究了减少数据集，使用教师模型的边界作为修剪指标。他们发现，在数据丰富的情况下，仅训练最困难的样本可以获得更好的性能，而在数据稀缺的情况下，仅训练最简单的样本可以获得更好的性能。
- en: 6 Conclusion
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this study, we thoroughly investigate diverse data pruning methods for pretraining
    LLMs with billions of parameters and with datasets containing billions of tokens.
    We showed that when properly applied, data pruning consistently improves model
    performance. We also find that training on the “easiest" examples in a dataset
    degrades performance, where “easiest" is defined as the lowest scoring examples
    according to a metric based on a reference model. Simple methods that rank instances
    based on their perplexity demonstrate superior performance compared to more elaborate
    approaches such as memorization. Models trained on as little as half of the data
    selected by perplexity achieve up to 1.5% improvement over models trained on the
    full dataset. Additionally, we establish the consistency of our findings as we
    scale the model sizes. While scaling up the amount of data LLMs are trained on
    remains a popular avenue for improving models, our work demonstrates that carefully
    pruning these large training corpora is also a fruitful direction for making models
    better.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们深入探讨了用于预训练具有数十亿参数的 LLM 和包含数十亿标记的数据集的各种数据剪枝方法。我们展示了当正确应用时，数据剪枝能一致地提高模型性能。我们还发现，在数据集中训练“最简单”的示例会降低性能，其中“最简单”是根据基于参考模型的度量定义的最低评分示例。基于困惑度对实例进行排序的简单方法显示出比更复杂的方法（如记忆化）更优的性能。即使是使用困惑度选择的只有一半的数据进行训练的模型，也能比使用完整数据集训练的模型提高最多
    1.5%。此外，我们在模型规模扩展时验证了我们发现的一致性。尽管增加 LLM 训练数据量仍然是提升模型的热门途径，但我们的工作表明，仔细剪枝这些大规模训练语料库也是提升模型性能的一个有前途的方向。
- en: References
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Abbas et al. (2023) Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli,
    and Ari S. Morcos. Semdedup: Data-efficient learning at web-scale through semantic
    deduplication, 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbas 等（2023） Amro Abbas、Kushal Tirumala、Dániel Simig、Surya Ganguli 和 Ari S.
    Morcos。Semdedup：通过语义去重在网络规模上实现数据高效学习，2023 年。
- en: Agarwal et al. (2022) Chirag Agarwal, Daniel D’souza, and Sara Hooker. Estimating
    example difficulty using variance of gradients. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, pp.  10368–10378,
    June 2022.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等（2022） Chirag Agarwal、Daniel D’souza 和 Sara Hooker。使用梯度方差估计示例难度。发表于
    *IEEE/CVF 计算机视觉与模式识别会议（CVPR）*，第 10368–10378 页，2022 年 6 月。
- en: Anil et al. (2023) Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern,
    Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder,
    Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan
    Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma,
    Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,
    Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
    Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad
    Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy
    Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey
    Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao
    Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee,
    Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao
    Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez,
    Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom,
    Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan
    Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan
    Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R.
    So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli,
    Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin
    Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,
    Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical
    report, 2023.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil et al. (2023) 罗汉·Anil, 安德鲁·M. Dai, 奥尔汗·Firat, 梅尔文·Johnson, 德米特里·Lepikhin,
    亚历山大·Passos, 西亚马克·Shakeri, 埃马努埃尔·Taropa, 派奇·Bailey, 质峰·Chen, 埃里克·Chu, 乔纳森·H. Clark,
    洛朗·El Shafey, 燕平·Huang, 凯西·Meier-Hellstern, 高拉夫·Mishra, 艾丽卡·Moreira, 马克·Omernick,
    凯文·Robinson, 塞巴斯蒂安·Ruder, 易·Tay, 可凡·Xiao, 袁中·Xu, 瑜静·Zhang, 古斯塔沃·Hernandez Abrego,
    郑焕·Ahn, 雅各布·Austin, 保罗·Barham, 詹·Botha, 詹姆斯·Bradbury, 西达尔塔·Brahma, 凯文·Brooks,
    米歇尔·Catasta, 永·Cheng, 科林·Cherry, 克里斯托弗·A. Choquette-Choo, 阿坎莎·Chowdhery, 克莱门特·Crepy,
    沙奇·Dave, 穆斯塔法·Dehghani, 苏妮帕·Dev, 雅各布·Devlin, 马克·Díaz, 南·Du, 伊桑·Dyer, 弗拉德·Feinberg,
    方晓宇·Feng, 弗拉德·Fienber, 马库斯·Freitag, 哈维尔·Garcia, 塞巴斯蒂安·Gehrmann, 卢卡斯·Gonzalez,
    盖·Gur-Ari, 斯蒂文·Hand, 哈迪·Hashemi, 乐·Hou, 乔舒亚·Howland, 安德里亚·Hu, 杰弗里·Hui, 杰里米·Hurwitz,
    迈克尔·Isard, 阿比·Ittycheriah, 马修·Jagielski, 温浩·Jia, 凯瑟琳·Kenealy, 马克西姆·Krikun, 斯内哈·Kudugunta,
    常·Lan, 凯瑟琳·Lee, 本杰明·Lee, 埃里克·Li, 音乐·Li, 魏·Li, 雅光·Li, 见·Li, 慧英·Lim, 汉赵·Lin, 钟涛·Liu,
    弗雷德里克·Liu, 马塞洛·Maggioni, 阿罗玛·Mahendru, 乔舒亚·Maynez, 维丹特·Misra, 梅萨姆·Moussalem, 扎卡里·Nado,
    约翰·Nham, 埃里克·Ni, 安德鲁·Nystrom, 艾丽西亚·Parrish, 玛丽·Pellat, 马丁·Polacek, 亚历克斯·Polozov,
    莱纳·Pope, 思远·Qiao, 艾米莉·Reif, 布莱恩·Richter, 帕克·Riley, 亚历克斯·Castro Ros, 奥尔科·Roy, 布伦南·Saeta,
    拉杰库马尔·Samuel, 琳妮·Shelby, 安布罗斯·Slone, 丹尼尔·Smilkov, 大卫·R. So, 丹尼尔·Sohn, 西蒙·Tokumine,
    达莎·Valter, 维贾伊·Vasudevan, 基兰·Vodrahalli, 薛志·Wang, 皮冬·Wang, 梓瑞·Wang, 陶·Wang, 约翰·Wieting,
    余怀·Wu, 凯尔文·Xu, 云汉·Xu, 琳婷·Xue, 彭程·Yin, 贾辉·Yu, 乔·Zhang, 史蒂文·Zheng, 则·Zheng, 伟康·Zhou,
    丹尼·Zhou, 斯拉夫·Petrov, 和永辉·Wu。Palm 2技术报告，2023年。
- en: 'Attendu & Corbeil (2023) Jean-Michel Attendu and Jean-Philippe Corbeil. Nlu
    on data diets: Dynamic data subset selection for nlp classification tasks, 2023.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Attendu & Corbeil (2023) 让-米歇尔·Attendu 和让-菲利普·Corbeil。Nlu on data diets: 动态数据子集选择用于nlp分类任务，2023年。'
- en: 'Bane et al. (2022) Fred Bane, Celia Soler Uguet, Wiktor Stribiżew, and Anna
    Zaretskaya. A comparison of data filtering methods for neural machine translation.
    In *Proceedings of the 15th Biennial Conference of the Association for Machine
    Translation in the Americas (Volume 2: Users and Providers Track and Government
    Track)*, pp.  313–325, Orlando, USA, September 2022\. Association for Machine
    Translation in the Americas. URL [https://aclanthology.org/2022.amta-upg.22](https://aclanthology.org/2022.amta-upg.22).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bane et al. (2022) 弗雷德·Bane, 塞利亚·Soler Uguet, 维克托·Stribiżew, 和安娜·Zaretskaya。神经机器翻译的数据过滤方法比较。在*第15届美洲机器翻译协会双年会议（第2卷：用户与提供者跟踪与政府跟踪）*，第313–325页，美国奥兰多，2022年9月。美洲机器翻译协会。网址
    [https://aclanthology.org/2022.amta-upg.22](https://aclanthology.org/2022.amta-upg.22)。
- en: Biderman et al. (2023a) Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika,
    Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff. Emergent
    and predictable memorization in large language models, 2023a.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biderman et al. (2023a) 斯特拉·Biderman, USVSN Sai Prashanth, 林堂·Sutawika, 海莉·Schoelkopf,
    昆廷·Anthony, 希万舒·Purohit, 和爱德华·Raff。大语言模型中的突现和可预测记忆，2023a。
- en: 'Biderman et al. (2023b) Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
    Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
    USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der
    Wal. Pythia: A suite for analyzing large language models across training and scaling,
    2023b.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Biderman 等人 (2023b) Stella Biderman、Hailey Schoelkopf、Quentin Anthony、Herbie
    Bradley、Kyle O’Brien、Eric Hallahan、Mohammad Aflah Khan、Shivanshu Purohit、USVSN
    Sai Prashanth、Edward Raff、Aviya Skowron、Lintang Sutawika 和 Oskar van der Wal。Pythia:
    一套用于分析大型语言模型在训练和扩展中的工具，2023b。'
- en: 'Black et al. (2022) Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony,
    Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang,
    Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan
    Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive
    language model. In *Proceedings of BigScience Episode #5 – Workshop on Challenges
    & Perspectives in Creating Large Language Models*, pp. 95–136, virtual+Dublin,
    May 2022\. Association for Computational Linguistics. [10.18653/v1/2022.bigscience-1.9](https:/doi.org/10.18653/v1/2022.bigscience-1.9).
    URL [https://aclanthology.org/2022.bigscience-1.9](https://aclanthology.org/2022.bigscience-1.9).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Black 等人 (2022) Sidney Black、Stella Biderman、Eric Hallahan、Quentin Anthony、Leo
    Gao、Laurence Golding、Horace He、Connor Leahy、Kyle McDonell、Jason Phang、Michael
    Pieler、Usvsn Sai Prashanth、Shivanshu Purohit、Laria Reynolds、Jonathan Tow、Ben Wang
    和 Samuel Weinbach。GPT-NeoX-20B: 一种开源自回归语言模型。见于 *BigScience Episode #5 – 生成大型语言模型的挑战与视角研讨会*，第
    95–136 页，虚拟+Dublin，2022年5月。计算语言学协会。 [10.18653/v1/2022.bigscience-1.9](https:/doi.org/10.18653/v1/2022.bigscience-1.9)。网址
    [https://aclanthology.org/2022.bigscience-1.9](https://aclanthology.org/2022.bigscience-1.9)。'
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners, 2020.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020) Tom B. Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared
    Kaplan、Prafulla Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda
    Askell、Sandhini Agarwal、Ariel Herbert-Voss、Gretchen Krueger、Tom Henighan、Rewon
    Child、Aditya Ramesh、Daniel M. Ziegler、Jeffrey Wu、Clemens Winter、Christopher Hesse、Mark
    Chen、Eric Sigler、Mateusz Litwin、Scott Gray、Benjamin Chess、Jack Clark、Christopher
    Berner、Sam McCandlish、Alec Radford、Ilya Sutskever 和 Dario Amodei。语言模型是少量样本学习者，2020年。
- en: 'Cao et al. (2023) Yihan Cao, Yanbin Kang, and Lichao Sun. Instruction mining:
    High-quality instruction data selection for large language models, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等人 (2023) Yihan Cao、Yanbin Kang 和 Lichao Sun。指令挖掘：大型语言模型的高质量指令数据选择，2023年。
- en: Carlini et al. (2021) Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew
    Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song,
    Ulfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from
    large language models, 2021.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini 等人 (2021) Nicholas Carlini、Florian Tramer、Eric Wallace、Matthew Jagielski、Ariel
    Herbert-Voss、Katherine Lee、Adam Roberts、Tom Brown、Dawn Song、Ulfar Erlingsson、Alina
    Oprea 和 Colin Raffel。 从大型语言模型中提取训练数据，2021年。
- en: Carlini et al. (2023) Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
    Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across
    neural language models, 2023.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini 等人 (2023) Nicholas Carlini、Daphne Ippolito、Matthew Jagielski、Katherine
    Lee、Florian Tramer 和 Chiyuan Zhang。量化神经语言模型的记忆化，2023年。
- en: 'Chen (2023) Wenhu Chen. Large language models are few(1)-shot table reasoners.
    In *Findings of the Association for Computational Linguistics: EACL 2023*, pp. 
    1120–1130, Dubrovnik, Croatia, May 2023\. Association for Computational Linguistics.
    URL [https://aclanthology.org/2023.findings-eacl.83](https://aclanthology.org/2023.findings-eacl.83).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen (2023) Wenhu Chen。大型语言模型是少量（1）-样本表格推理器。见于 *计算语言学协会的研究发现: EACL 2023*，第
    1120–1130 页，克罗地亚杜布罗夫尼克，2023年5月。计算语言学协会。网址 [https://aclanthology.org/2023.findings-eacl.83](https://aclanthology.org/2023.findings-eacl.83)。'
- en: 'Dodge et al. (2021) Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew,
    Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting
    large webtext corpora: A case study on the colossal clean crawled corpus, 2021.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dodge 等人 (2021) Jesse Dodge、Maarten Sap、Ana Marasović、William Agnew、Gabriel
    Ilharco、Dirk Groeneveld、Margaret Mitchell 和 Matt Gardner。记录大型网络文本语料库：以庞大的清理抓取语料库为例，2021年。
- en: 'Fayyaz et al. (2022) Mohsen Fayyaz, Ehsan Aghazadeh, Ali Modarressi, Mohammad Taher
    Pilehvar, Yadollah Yaghoobzadeh, and Samira Ebrahimi Kahou. Bert on a data diet:
    Finding important examples by gradient-based pruning, 2022.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fayyaz 等（2022）**Mohsen Fayyaz**、**Ehsan Aghazadeh**、**Ali Modarressi**、**Mohammad
    Taher Pilehvar**、**Yadollah Yaghoobzadeh** 和 **Samira Ebrahimi Kahou**。数据减肥中的
    Bert：通过基于梯度的修剪找到重要示例，2022。
- en: Gao (2021) Leo Gao. An empirical exploration in quality filtering of text data,
    2021.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao（2021）**Leo Gao**。文本数据质量过滤的实证探索，2021。
- en: 'Gao et al. (2021) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn
    Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language
    modeling. *CoRR*, abs/2101.00027, 2021. URL [https://arxiv.org/abs/2101.00027](https://arxiv.org/abs/2101.00027).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2021）**Leo Gao**、**Stella Biderman**、**Sid Black**、**Laurence Golding**、**Travis
    Hoppe**、**Charles Foster**、**Jason Phang**、**Horace He**、**Anish Thite**、**Noa
    Nabeshima**、**Shawn Presser** 和 **Connor Leahy**。该堆栈：用于语言建模的 800gb 多样文本数据集。*CoRR*，abs/2101.00027，2021。网址
    [https://arxiv.org/abs/2101.00027](https://arxiv.org/abs/2101.00027)。
- en: He et al. (2023) Muyang He, Shuo Yang, Tiejun Huang, and Bo Zhao. Large-scale
    dataset pruning with dynamic uncertainty, 2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2023）**Muyang He**、**Shuo Yang**、**Tiejun Huang** 和 **Bo Zhao**。动态不确定性的规模化数据集修剪，2023。
- en: Hernandez et al. (2022) Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma,
    Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan
    Hume, Scott Johnston, Ben Mann, Chris Olah, Catherine Olsson, Dario Amodei, Nicholas
    Joseph, Jared Kaplan, and Sam McCandlish. Scaling laws and interpretability of
    learning from repeated data, 2022.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hernandez 等（2022）**Danny Hernandez**、**Tom Brown**、**Tom Conerly**、**Nova DasSarma**、**Dawn
    Drain**、**Sheer El-Showk**、**Nelson Elhage**、**Zac Hatfield-Dodds**、**Tom Henighan**、**Tristan
    Hume**、**Scott Johnston**、**Ben Mann**、**Chris Olah**、**Catherine Olsson**、**Dario
    Amodei**、**Nicholas Joseph**、**Jared Kaplan** 和 **Sam McCandlish**。从重复数据中学习的扩展规律和可解释性，2022。
- en: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    Scaling laws for neural language models, 2020.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplan 等（2020）**Jared Kaplan**、**Sam McCandlish**、**Tom Henighan**、**Tom B.
    Brown**、**Benjamin Chess**、**Rewon Child**、**Scott Gray**、**Alec Radford**、**Jeffrey
    Wu** 和 **Dario Amodei**。神经语言模型的扩展规律，2020。
- en: 'Kocetkov et al. (2022) Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,
    Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean
    Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The
    stack: 3 tb of permissively licensed source code, 2022.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kocetkov 等（2022）**Denis Kocetkov**、**Raymond Li**、**Loubna Ben Allal**、**Jia
    Li**、**Chenghao Mou**、**Carlos Muñoz Ferrandis**、**Yacine Jernite**、**Margaret
    Mitchell**、**Sean Hughes**、**Thomas Wolf**、**Dzmitry Bahdanau**、**Leandro von
    Werra** 和 **Harm de Vries**。该堆栈：3 tb 的许可证宽松的源代码，2022。
- en: 'Kreutzer et al. (2022) Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab,
    Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem
    Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb,
    Benoît Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei,
    Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo Rubungo, Toan Q.
    Nguyen, Mathias Müller, André Müller, Shamsuddeen Hassan Muhammad, Nanda Muhammad,
    Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze
    Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure
    F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine Çabuk Ballı, Stella Biderman,
    Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime,
    Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal,
    and Mofetoluwa Adeyemi. Quality at a Glance: An Audit of Web-Crawled Multilingual
    Datasets. *Transactions of the Association for Computational Linguistics*, 10:50–72,
    01 2022. ISSN 2307-387X. [10.1162/tacl_a_00447](https:/doi.org/10.1162/tacl_a_00447).
    URL [https://doi.org/10.1162/tacl_a_00447](https://doi.org/10.1162/tacl_a_00447).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kreutzer 等（2022）**Julia Kreutzer**、**Isaac Caswell**、**Lisa Wang**、**Ahsan Wahab**、**Daan
    van Esch**、**Nasanbayar Ulzii-Orshikh**、**Allahsera Tapo**、**Nishant Subramani**、**Artem
    Sokolov**、**Claytone Sikasote**、**Monang Setyawan**、**Supheakmungkol Sarin**、**Sokhar
    Samb**、**Benoît Sagot**、**Clara Rivera**、**Annette Rios**、**Isabel Papadimitriou**、**Salomey
    Osei**、**Pedro Ortiz Suarez**、**Iroro Orife**、**Kelechi Ogueji**、**Andre Niyongabo
    Rubungo**、**Toan Q. Nguyen**、**Mathias Müller**、**André Müller**、**Shamsuddeen
    Hassan Muhammad**、**Nanda Muhammad**、**Ayanda Mnyakeni**、**Jamshidbek Mirzakhalov**、**Tapiwanashe
    Matangira**、**Colin Leong**、**Nze Lawson**、**Sneha Kudugunta**、**Yacine Jernite**、**Mathias
    Jenny**、**Orhan Firat**、**Bonaventure F. P. Dossou**、**Sakhile Dlamini**、**Nisansa
    de Silva**、**Sakine Çabuk Ballı**、**Stella Biderman**、**Alessia Battisti**、**Ahmed
    Baruwa**、**Ankur Bapna**、**Pallavi Baljekar**、**Israel Abebe Azime**、**Ayodele
    Awokoya**、**Duygu Ataman**、**Orevaoghene Ahia**、**Oghenefego Ahia**、**Sweta Agrawal**
    和 **Mofetoluwa Adeyemi**。一目了然的质量：对网络爬取的多语言数据集的审计。*计算语言学协会会刊*，10:50–72，01 2022。ISSN
    2307-387X。 [10.1162/tacl_a_00447](https:/doi.org/10.1162/tacl_a_00447)。网址 [https://doi.org/10.1162/tacl_a_00447](https://doi.org/10.1162/tacl_a_00447)。
- en: 'Laurençon et al. (2023) Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher
    Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao
    Mou, Eduardo González Ponferrada, Huu Nguyen, Jörg Frohberg, Mario Šaško, Quentin
    Lhoest, Angelina McMillan-Major, Gerard Dupont, Stella Biderman, Anna Rogers,
    Loubna Ben allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor,
    Maraim Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan Thrush,
    Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Muñoz, Jian Zhu, Daniel Van
    Strien, Zaid Alyafeai, Khalid Almubarak, Minh Chien Vu, Itziar Gonzalez-Dios,
    Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose,
    David Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq,
    Suzana Ilic, Margaret Mitchell, Sasha Alexandra Luccioni, and Yacine Jernite.
    The bigscience roots corpus: A 1.6tb composite multilingual dataset, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laurençon et al. (2023) 雨果·劳伦松，露西尔·索尔尼耶，托马斯·王，克里斯托弗·阿基基，阿尔贝特·维拉诺瓦·德尔·莫拉尔，特文·勒斯卡奥，利安德罗·冯·维拉，程昊·牟，爱德华多·冈萨雷斯·庞费拉达，胡·阮，约尔格·弗罗赫贝格，马里奥·沙什科，昆廷·洛赫斯特，安吉丽娜·麦克米兰-梅杰，杰拉尔德·杜庞，斯特拉·比德曼，安娜·罗杰斯，卢布娜·本·阿拉尔，弗朗西斯科·德·托尼，贾达·皮斯蒂利，奥利维耶·阮，索玛赫·尼克普尔，玛拉姆·马苏德，皮埃尔·科伦博，哈维尔·德·拉·罗萨，保罗·维利加斯，特里斯坦·瑟什，谢恩·朗普雷，塞巴斯蒂安·纳戈尔，莱昂·韦伯，曼努埃尔·穆尼奥斯，简·朱，丹尼尔·范·斯特里恩，扎伊德·阿里亚费，哈立德·阿尔穆巴拉克，闵·千言·吴，伊特西亚尔·冈萨雷斯-迪奥斯，艾托尔·索罗阿，凯尔·洛，马南·德伊，佩德罗·奥尔蒂斯·苏亚雷斯，亚伦·戈卡斯兰，沙米克·博斯，戴维·阿德拉尼，龙·潘，谭·徐，伊恩·余，苏哈斯·派，詹妮·钦，维奥莱特·勒佩尔克，苏珊娜·伊利奇，玛格丽特·米切尔，亚历山德拉·萨莎·卢乔尼，和雅辛·杰尔尼特。BigScience
    Roots语料库：一个1.6TB的复合多语言数据集，2023年。
- en: Loshchilov & Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. In *International Conference on Learning Representations*,
    2019. URL [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov & Hutter (2019) 伊利亚·洛希洛夫和弗兰克·胡特。解耦权重衰减正则化。见于*国际学习表征会议*，2019年。网址[https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)。
- en: 'Luccioni & Viviano (2021) Alexandra Luccioni and Joseph Viviano. What’s in
    the box? an analysis of undesirable content in the Common Crawl corpus. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (Volume
    2: Short Papers)*, pp.  182–189, Online, August 2021\. Association for Computational
    Linguistics. [10.18653/v1/2021.acl-short.24](https:/doi.org/10.18653/v1/2021.acl-short.24).
    URL [https://aclanthology.org/2021.acl-short.24](https://aclanthology.org/2021.acl-short.24).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luccioni & Viviano (2021) 亚历山德拉·卢乔尼和约瑟夫·维维亚诺。盒子里有什么？对Common Crawl语料库中不良内容的分析。见于*第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第2卷：短文集）*，第182–189页，线上，2021年8月。计算语言学协会。[10.18653/v1/2021.acl-short.24](https://doi.org/10.18653/v1/2021.acl-short.24)。网址[https://aclanthology.org/2021.acl-short.24](https://aclanthology.org/2021.acl-short.24)。
- en: Mindermann et al. (2022) Sören Mindermann, Jan Brauner, Muhammed Razzak, Mrinank
    Sharma, Andreas Kirsch, Winnie Xu, Benedikt Höltgen, Aidan N. Gomez, Adrien Morisot,
    Sebastian Farquhar, and Yarin Gal. Prioritized training on points that are learnable,
    worth learning, and not yet learnt, 2022.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mindermann et al. (2022) 索伦·敏德曼，简·布劳内尔，穆罕默德·拉扎克，姆里南克·夏尔马，安德烈亚斯·基尔施，温妮·许，本尼迪克特·赫尔特根，艾登·N·戈麦斯，阿德里安·莫里索，塞巴斯蒂安·法尔夸，和亚林·加尔。优先训练那些可学、值得学且尚未学到的点，2022年。
- en: Mitchell et al. (2023) Margaret Mitchell, Alexandra Sasha Luccioni, Nathan Lambert,
    Marissa Gerchick, Angelina McMillan-Major, Ezinwanne Ozoani, Nazneen Rajani, Tristan
    Thrush, Yacine Jernite, and Douwe Kiela. Measuring data, 2023.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell et al. (2023) 玛格丽特·米切尔，亚历山德拉·萨莎·卢乔尼，内森·兰伯特，玛丽莎·格查克，安吉丽娜·麦克米兰-梅杰，伊津万·奥佐尼，纳兹宁·拉贾尼，特里斯坦·瑟什，雅辛·杰尔尼特，和道维·基拉。测量数据，2023年。
- en: Muennighoff et al. (2023) Niklas Muennighoff, Alexander M. Rush, Boaz Barak,
    Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and
    Colin Raffel. Scaling data-constrained language models, 2023.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muennighoff et al. (2023) 尼克拉斯·穆宁霍夫，亚历山大·M·拉什，博阿兹·巴拉克，特文·勒斯卡奥，亚历山德拉·皮克图斯，诺阿曼·塔兹，桑普·皮萨洛，托马斯·沃尔夫和科林·拉费尔。数据受限的语言模型扩展，2023年。
- en: Park et al. (2022) Dongmin Park, Dimitris Papailiopoulos, and Kangwook Lee.
    Active learning is a strong baseline for data subset selection. In *Has it Trained
    Yet? NeurIPS 2022 Workshop*, 2022. URL [https://openreview.net/forum?id=PAgpyQ5rGS](https://openreview.net/forum?id=PAgpyQ5rGS).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. (2022) 东敏·朴，迪米特里斯·帕帕伊利奥普洛斯，和姜旭。主动学习是数据子集选择的强基准。见于*还训练了吗？NeurIPS
    2022研讨会*，2022年。网址[https://openreview.net/forum?id=PAgpyQ5rGS](https://openreview.net/forum?id=PAgpyQ5rGS)。
- en: 'Paul et al. (2023) Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite.
    Deep learning on a data diet: Finding important examples early in training, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paul 等人 (2023) **曼希吉·保罗**、**苏里亚·甘古利** 和 **金塔雷·卡罗利娜·朱古特**。《数据饮食中的深度学习：在训练初期找到重要示例》，2023年。
- en: 'Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
    and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated
    corpora with web data, and web data only, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Penedo 等人 (2023) **吉列尔梅·佩内多**、**昆廷·马拉尔蒂克**、**丹尼尔·赫斯洛**、**鲁克桑德拉·科约卡鲁**、**亚历山德罗·卡佩利**、**哈姆扎·阿洛贝德利**、**巴普蒂斯特·潘尼耶**、**艾布特萨姆·阿尔马兹鲁伊**
    和 **朱利安·洛内**。《Falcon LLM的Refinedweb数据集：用网络数据超越精选语料库，仅使用网络数据》，2023年。
- en: 'Qin et al. (2023) Ziheng Qin, Kai Wang, Zangwei Zheng, Jianyang Gu, Xiangyu
    Peng, Daquan Zhou, and Yang You. Infobatch: Lossless training speed up by unbiased
    dynamic data pruning, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin 等人 (2023) **齐恒·秦**、**凯·王**、**臧维·郑**、**建阳·顾**、**项宇·彭**、**大泉·周** 和 **杨·游**。《Infobatch:
    通过无偏动态数据剪枝加速无损训练速度》，2023年。'
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training. 2018.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 (2018) **亚历克·拉德福德**、**卡尔提克·纳拉辛汉**、**蒂姆·萨利曼斯**、**伊利亚·苏茨克弗** 等人。《通过生成预训练提升语言理解》，2018年。
- en: 'Rae et al. (2022) Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican,
    Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah
    Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
    George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia
    Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John
    Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant
    Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela
    Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida
    Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch,
    Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault
    Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume,
    Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego
    de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake
    Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero,
    Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne
    Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language
    models: Methods, analysis and insights from training gopher, 2022.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rae 等人 (2022) **杰克·W·雷**、**塞巴斯蒂安·博尔戈德**、**特雷弗·蔡**、**凯蒂·米利坎**、**乔丹·霍夫曼**、**弗朗西斯·宋**、**约翰·阿斯拉尼德斯**、**莎拉·亨德森**、**罗曼·林**、**苏珊娜·杨**、**伊丽莎·拉瑟福德**、**汤姆·亨尼根**、**雅各布·梅尼克**、**阿尔宾·卡西尔**、**理查德·鲍威尔**、**乔治·范·登·德里斯赫**、**丽莎·安·亨德里克斯**、**玛丽贝斯·劳**、**黄泊森**、**艾米莉亚·格雷斯**、**约翰内斯·韦布尔**、**苏曼特·达塔特里**、**藏蕨·黄**、**乔纳森·乌萨托**、**约翰·梅洛**、**伊琳娜·希金斯**、**安东尼亚·克雷斯韦尔**、**纳特·麦卡利斯**、**艾米·吴**、**埃里希·艾尔森**、**西丹特·贾亚库马尔**、**埃琳娜·布赫茨卡娅**、**大卫·布登**、**埃斯梅·萨瑟兰**、**凯伦·西莫宁**、**米歇拉·帕加尼尼**、**劳伦特·西弗**、**莉娜·马滕斯**、**项洛琳·李**、**阿蒂古纳·昆科罗**、**艾达·内马扎德**、**埃琳娜·格里博夫斯卡娅**、**多梅尼克·多纳托**、**安吉利基·拉扎里杜**、**亚瑟·门施**、**让-巴普蒂斯特·莱斯皮奥**、**玛利亚·辛普基利**、**尼古拉·格里戈列夫**、**道格·弗里茨**、**蒂博·索蒂厄**、**曼塔斯·帕亚尔斯卡斯**、**托比·波伦**、**支涛·龚**、**丹尼尔·托亚马**、**西普里安·德·马松·达图姆**、**余佳·李**、**泰芬·特尔兹**、**弗拉基米尔·米库利克**、**伊戈尔·巴布什金**、**艾丹·克拉克**、**迭戈·德·拉斯·卡萨斯**、**奥雷利亚·盖**、**克里斯·琼斯**、**詹姆斯·布拉德伯里**、**马修·约翰逊**、**布莱克·赫赫特曼**、**劳拉·魏丁格**、**伊亚森·加布里埃尔**、**威廉·艾萨克**、**艾德·洛克哈特**、**西蒙·奥辛德罗**、**劳拉·里梅尔**、**克里斯·戴尔**、**奥里奥尔·维尼亚尔斯**、**卡里姆·阿尤布**、**杰夫·斯坦维**、**洛琳·贝内特**、**德米斯·哈萨比斯**、**科雷·卡夫克乔**
    和 **杰弗里·欧文**。《扩展语言模型：方法、分析和训练gopher的见解》，2022年。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer, 2020.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 (2020) **科林·拉费尔**、**诺阿·沙泽尔**、**亚当·罗伯茨**、**凯瑟琳·李**、**沙然·纳朗**、**迈克尔·马特纳**、**燕琦·周**、**韦·李**
    和 **彼得·J·刘**。《通过统一的文本到文本变换器探索迁移学习的极限》，2020年。
- en: Raju et al. (2021) Ravi S Raju, Kyle Daruwalla, and Mikko Lipasti. Accelerating
    deep learning with dynamic data pruning, 2021.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raju 等人 (2021) **拉维·S·拉朱**、**凯尔·达鲁瓦拉** 和 **米克科·利帕斯提**。《通过动态数据剪枝加速深度学习》，2021年。
- en: Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural
    machine translation of rare words with subword units, 2016.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sennrich 等人 (2016) **里科·塞尼赫**、**巴里·哈多** 和 **亚历山德拉·伯奇**。《用子词单元进行稀有词的神经机器翻译》，2016年。
- en: 'Siddiqui et al. (2022) Shoaib Ahmed Siddiqui, Nitarshan Rajkumar, Tegan Maharaj,
    David Krueger, and Sara Hooker. Metadata archaeology: Unearthing data subsets
    by leveraging training dynamics, 2022.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siddiqui 等人 (2022) **肖艾布·艾哈迈德·西迪基**、**尼塔尔尚·拉杰库马尔**、**蒂根·马哈拉吉**、**大卫·克鲁格** 和
    **萨拉·胡克**。《元数据考古学：通过利用训练动态挖掘数据子集》，2022年。
- en: 'Sorscher et al. (2023) Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya
    Ganguli, and Ari S. Morcos. Beyond neural scaling laws: beating power law scaling
    via data pruning, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sorscher et al. (2023) 由 Ben Sorscher、Robert Geirhos、Shashank Shekhar、Surya
    Ganguli 和 Ari S. Morcos 共同撰写。超越神经缩放定律：通过数据剪枝击败幂律缩放，2023。
- en: 'Taylor et al. (2022) Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom,
    Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
    Galactica: A large language model for science, 2022.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taylor et al. (2022) 由 Ross Taylor、Marcin Kardas、Guillem Cucurull、Thomas Scialom、Anthony
    Hartshorn、Elvis Saravia、Andrew Poulton、Viktor Kerkez 和 Robert Stojnic 共同撰写。Galactica：用于科学的大型语言模型，2022。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) 由 Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar、Aurelien
    Rodriguez、Armand Joulin、Edouard Grave 和 Guillaume Lample 共同撰写。Llama：开放且高效的基础语言模型，2023。
- en: Vaswani et al. (2023) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is
    all you need, 2023.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2023) 由 Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion
    Jones、Aidan N. Gomez、Lukasz Kaiser 和 Illia Polosukhin 共同撰写。注意力机制即一切，2023。
- en: 'Wang et al. (2019) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding, 2019.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019) 由 Alex Wang、Amanpreet Singh、Julian Michael、Felix Hill、Omer
    Levy 和 Samuel R. Bowman 共同撰写。Glue：自然语言理解的多任务基准和分析平台，2019。
- en: 'Wenzek et al. (2020) Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau,
    Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. CCNet:
    Extracting high quality monolingual datasets from web crawl data. In *Proceedings
    of the Twelfth Language Resources and Evaluation Conference*, pp.  4003–4012,
    Marseille, France, May 2020\. European Language Resources Association. ISBN 979-10-95546-34-4.
    URL [https://aclanthology.org/2020.lrec-1.494](https://aclanthology.org/2020.lrec-1.494).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wenzek et al. (2020) 由 Guillaume Wenzek、Marie-Anne Lachaux、Alexis Conneau、Vishrav
    Chaudhary、Francisco Guzmán、Armand Joulin 和 Edouard Grave 共同撰写。CCNet：从网页爬取数据中提取高质量的单语数据集。发表于
    *第十二届语言资源与评估会议论文集*，第 4003–4012 页，法国马赛，2020 年 5 月。欧洲语言资源协会。ISBN 979-10-95546-34-4。网址
    [https://aclanthology.org/2020.lrec-1.494](https://aclanthology.org/2020.lrec-1.494)。
- en: 'Xie et al. (2023a) Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao
    Liu, Yifeng Lu, Percy Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing
    data mixtures speeds up language model pretraining. *arXiv preprint arXiv:2305.10429*,
    2023a.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2023a) 由 Michael Xie、Hieu Pham、Xuanyi Dong、Nan Du、Hanxiao Liu、Yifeng
    Lu、Percy Liang、Quoc V Le、Tengyu Ma 和 Adams Wei Yu 共同撰写。Doremi：优化数据混合加速语言模型预训练。*arXiv
    预印本 arXiv:2305.10429*，2023a。
- en: Xie et al. (2023b) Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy
    Liang. Data selection for language models via importance resampling, 2023b.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2023b) 由 Michael Xie、Shibani Santurkar、Tengyu Ma 和 Percy Liang 共同撰写。数据选择用于语言模型通过重要性重采样，2023b。
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained
    transformer language models, 2022.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022) 由 Susan Zhang、Stephen Roller、Naman Goyal、Mikel Artetxe、Moya
    Chen、Shuohui Chen、Christopher Dewan、Mona Diab、Xian Li、Xi Victoria Lin、Todor Mihaylov、Myle
    Ott、Sam Shleifer、Kurt Shuster、Daniel Simig、Punit Singh Koura、Anjali Sridhar、Tianlu
    Wang 和 Luke Zettlemoyer 共同撰写。Opt：开放的预训练转换器语言模型，2022。
- en: 'Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh,
    Mike Lewis, Luke Zettlemoyer, and Omer Levy. Lima: Less is more for alignment,
    2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2023) 由 Chunting Zhou、Pengfei Liu、Puxin Xu、Srini Iyer、Jiao Sun、Yuning
    Mao、Xuezhe Ma、Avia Efrat、Ping Yu、Lili Yu、Susan Zhang、Gargi Ghosh、Mike Lewis、Luke
    Zettlemoyer 和 Omer Levy 共同撰写。Lima：对齐的少即多，2023。
- en: Appendix A Metric Distributions
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 度量分布
- en: 'We present the total distributions of the pruning metrics used in our analysis
    in Figure [8](#A1.F8 "Figure 8 ‣ Appendix A Metric Distributions ‣ When Less is
    More: Investigating Data Pruning for Pretraining LLMs at Scale").'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图 [8](#A1.F8 "图 8 ‣ 附录 A 度量分布 ‣ 当少即多：调查大规模预训练 LLM 的数据剪枝") 中展示了用于我们分析的修剪度量的总体分布。
- en: '![Refer to caption](img/04ee8d19d57359157bb3e505eec5e3c5.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/04ee8d19d57359157bb3e505eec5e3c5.png)'
- en: (a) Distributions of Perplexity from different reference models. The dotted
    lines are placed at each 10th percentile. Please note the differences in axes
    between graphs. Fewer than .1% of examples on the extreme high end have been truncate
    to better display the overall distribution
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 不同参考模型的困惑度分布。虚线表示每 10 百分位。请注意图表之间的坐标轴差异。极高端少于 0.1% 的示例已被截断，以更好地显示整体分布。
- en: '![Refer to caption](img/c45ed0532421cbd25deed1157c933606.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c45ed0532421cbd25deed1157c933606.png)'
- en: (b) Distributions of the EL2N and Memorization Factor metrics. The dotted lines
    are placed at each 10th percentile and omitted from Memorization Factor due to
    overlap. Please note the log-scaled y-axis.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: (b) EL2N 和记忆因子指标的分布。虚线表示每 10 百分位，并因重叠而从记忆因子中省略。请注意对数刻度的 y 轴。
- en: '![Refer to caption](img/9c2ce03b44bbce52d2b3f697943cd85e.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9c2ce03b44bbce52d2b3f697943cd85e.png)'
- en: '(c) Distributions of Perplexity from reference models trained on Wikipedia
    and CommonCrawl. The CommonCrawl model is the same as the 124M parameter model
    in Figure [8(a)](#A1.F8.sf1 "Figure 8(a) ‣ Figure 8 ‣ Appendix A Metric Distributions
    ‣ When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale").
    The dotted lines are placed at each 10th percentile.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '(c) 从在维基百科和 CommonCrawl 上训练的参考模型中得到的困惑度分布。CommonCrawl 模型与图 [8(a)](#A1.F8.sf1
    "Figure 8(a) ‣ Figure 8 ‣ Appendix A Metric Distributions ‣ When Less is More:
    Investigating Data Pruning for Pretraining LLMs at Scale") 中的 124M 参数模型相同。虚线表示每
    10 百分位。'
- en: 'Figure 8: Distributions of different pruning metrics and reference models.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：不同剪枝指标和参考模型的分布。
- en: Appendix B Examples from different selection criteria
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 不同选择标准的示例
- en: 'Examples from the pretraining data, drawn from distinct subsets (keep bottom,
    keep middle, keep top), are presented in Tables  [3](#A2.T3 "Table 3 ‣ Appendix
    B Examples from different selection criteria ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale"), [4](#A2.T4 "Table 4 ‣ Appendix B
    Examples from different selection criteria ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale"), [5](#A2.T5 "Table 5 ‣ Appendix B
    Examples from different selection criteria ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale"), [6](#A2.T6 "Table 6 ‣ Appendix B
    Examples from different selection criteria ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale"), and [7](#A2.T7 "Table 7 ‣ Appendix
    B Examples from different selection criteria ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale"), with rankings based on perplexity.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '从预训练数据中提取的示例，来自不同的子集（保留底部、保留中间、保留顶部），展示在表 [3](#A2.T3 "Table 3 ‣ Appendix B
    Examples from different selection criteria ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale")、[4](#A2.T4 "Table 4 ‣ Appendix B
    Examples from different selection criteria ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale")、[5](#A2.T5 "Table 5 ‣ Appendix B
    Examples from different selection criteria ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale")、[6](#A2.T6 "Table 6 ‣ Appendix B
    Examples from different selection criteria ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale") 和 [7](#A2.T7 "Table 7 ‣ Appendix
    B Examples from different selection criteria ‣ When Less is More: Investigating
    Data Pruning for Pretraining LLMs at Scale") 中，排名基于困惑度。'
- en: 'Table 3: Samples from different distribution subsets using perplexity of a
    52B reference model trained on CommonCrawl.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：使用在 CommonCrawl 上训练的 52B 参考模型的困惑度从不同分布子集中的样本。
- en: '| Bottom 10% | Middle 10% | Top 10% |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 底部 10% | 中间 10% | 顶部 10% |'
- en: '| --- | --- | --- |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Submissions, you hereby grant Company a license to translate, modify (for
    technical purposes, for example making sure your content is viewable on an iPhone
    as well as a computer) and reproduce and otherwise act with respect to such User
    Submissions, in each case to enable us to operate the Services, as described in
    more detail below. This is a license only – your ownership in User Submissions
    is […] | House Municipal Heritage Building is a two-storey, wooden, vernacular
    building with a low-hipped roof, and is located at the Norris Point Lookout, 104
    Main Road, Norris Point, Newfoundland and Labrador. The former family dwelling
    now operates as a heritage museum with a view of the Tablelands of Gros Morne
    National Park located on the great Northern Peninsula. The municipal heritage
    designation […] | and a nice book as a nice price. Postage is via Royal Mail 1st
    Class in the UK. If you are buying from overseas then please contact me before
    completing your purchase for a quote. I will always combine P&P so if ordering
    multiple books, please wait for the invoice so that discounts can be applied.
    We are slowly populating our store with post war Wisden’s so if there is anything
    you need that […] |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 提交内容时，您在此授予公司一个许可，以翻译、修改（例如为了技术目的，确保您的内容在iPhone和电脑上均可查看）和复制及以其他方式处理此类用户提交内容，以便我们运营服务，具体细节如下所述。这仅是一个许可——您对用户提交内容的所有权[…]
    | 住宅市政遗产建筑是一座两层的木质本地建筑，带有低斜屋顶，位于纽芬兰和拉布拉多诺里斯点瞭望台，104主路。前家庭住宅现作为一个遗产博物馆运营，可以俯瞰到位于北方半岛的大桌山国家公园的景色。该市政遗产称号[…]
    | 以及一本好书作为一个好的价格。邮寄通过英国皇家邮政1级服务进行。如果您在海外购买，请在完成购买前与我联系以获取报价。我将始终合并运费，因此如果订购多本书，请等待发票以便可以应用折扣。我们正在慢慢充实我们的商店，添加战后Wisden书籍，所以如果您需要的[…]'
- en: '| provided on the Site is not intended for distribution to or use by any person
    or entity in any jurisdiction or country where such distribution or use would
    be contrary to law or regulation or which would subject us to any registration
    requirement within such jurisdiction or country. Accordingly, those persons who
    choose to access the Site from other locations do so on their own initiative and
    are […] | selection of fuel type and input of soot index, coefficient of fuel,
    selection of measurement units, input of date and time with keyboard and via RS232
    or RS485 Procedure of industrial emissions monitoring with the use of AHKAT-410
    has been agreed in FSUE "SRI Atmosphere" AHKAT-410-16 is approved for diesel locomotive
    and diesel train emission monitoring at environment monitoring stations in […]
    | can be returned up to 28 days after the date of purchase. Please note, we cannot
    offer refunds on beauty, pierced jewellery or on swimwear if the hygiene seal
    is not in place or has been broken. We now offer FREE label-free returns with
    InPost Lockers (available 24/7), FREE Doddle Returns to all UK customers as well
    as a FREE UK Collect+ returns service via over 5,900 local stores nationwide.[…]
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 本网站提供的内容不适用于任何法律或法规禁止其分发或使用的地区或国家，也不适用于任何要求我们在该地区或国家注册的情况。因此，选择从其他地点访问本网站的人须自行承担风险，并且[…]
    | 燃料类型选择和烟尘指数、燃料系数的输入、测量单位选择、日期和时间的输入（通过键盘和RS232或RS485），AHKAT-410工业排放监测程序已在FSUE
    "SRI Atmosphere"中确认。AHKAT-410-16已获批准用于在环境监测站监测柴油机车和柴油列车的排放，[…] | 购买日期后最多可退货28天。请注意，美容产品、穿孔首饰或泳装如果卫生密封未完好或已破损，则无法退款。我们现在提供免费的无标签退货服务，通过InPost自助取件柜（24/7开放）、免费的Doddle退货服务，所有英国客户还可通过全国超过5,900家本地商店的免费UK
    Collect+退货服务。[…] |'
- en: '| license only – your ownership in User Submissions is not affected. You agree
    that the licenses you grant are royalty-free, perpetual, sublicensable, irrevocable,
    and worldwide. Any information or content publicly posted or privately transmitted
    through the Services is the sole responsibility of the person from whom such content
    originated, and you access all such information and content at your […] | 1 1/2
    " steel plate, all weld construction Hammer mill machine manufacturers, suppliers,
    exporters, dealers and traders in India and worldwide hammer mill machines from
    Gujarat and Mumbai since 1960 as per the ISO standards with required industrial
    features and specifications Replaceable bar type grate is available for specific
    applications SPECIFICATIONS : Hammer stone crusher is a kind of equip […] | several
    turns. Nearly a month after a foreclosure lawsuit was filed against Freestyle
    Music Park and its parent company, more than a dozen former department heads have
    sued seeking more than $232,000 in unpaid wages and bonuses, according to court
    papers filed late Friday. Seventeen employees are listed as plaintiffs. Backpay
    I can understand, but can you honestly expect any kind of bonuses […] |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 仅限许可——您在用户提交内容中的所有权不受影响。您同意您授予的许可是免版税的、永久的、可再许可的、不可撤销的，并且是全球范围的。通过服务公开发布或私下传输的任何信息或内容，均由内容来源方独自承担责任，您访问所有这些信息和内容时，您的[…]
    | 1 1/2 " 钢板，全焊接结构的锤磨机制造商、供应商、出口商、经销商和交易商在印度及全球，来自古吉拉特邦和孟买，自1960年起按ISO标准制造，具备所需的工业功能和规格。针对特定应用提供可更换的条形栅格。规格：锤石破碎机是一种设备
    […] | 几次回合。大约一个月后，Freestyle Music Park及其母公司被提起的止赎诉讼中，十多位前部门主管已起诉索赔超过232,000美元的未支付工资和奖金，法院文件显示。十七名员工被列为原告。拖欠工资我可以理解，但你能诚心期待任何奖金
    […] |'
- en: 'Table 4: Samples from different distribution subsets using perplexity of a
    124M reference model trained on CommonCrawl.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：使用在CommonCrawl上训练的124M参考模型的困惑度来自不同分布子集的样本。
- en: '| Bottom 10% | Middle 10% | Top 10% |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 底部10% | 中间10% | 顶部10% |'
- en: '| --- | --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| risk your food going bad in a lukewarm fridge when you can lease kitchen
    appliances in West Hollywood through Acima! Are you a budding DJ? A bit of a high-fidelity
    audio snub? Love to level up with the latest video game system? Level up your
    entertainment at home and on the road with sound systems for lease in West Hollywood.
    You can make flexible lease renewal payments on the best in-home sound […] | gratitude
    exercise. Before you get out of bed, think of five things you are most grateful
    for. If your Life Path number is 2, you have a duality fit for any earthly experience.
    You are deeply rooted in balance and harmony when dealing with the other numbers.
    In order to stay connected to your community, start your day by connecting with
    your friends and family. Instead of hopping on social […] | keepers" definitely
    won’t help! Then there are those whose idea of a school librarian is based on
    one they remember from their childhood, who perhaps didn’t let them borrow from
    the adult shelves or maybe told them to be quiet. You know - the cliched woman
    with glasses and a bun? I wear glasses myself and ended up haing to get a haircut
    to avoid the cliche. In summer, of course I had to put my […] |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 当你可以通过Acima在西好莱坞租赁厨房电器时，为什么要冒着食物在温热冰箱中变坏的风险呢！你是一个有前途的DJ吗？对高保真音响有些挑剔？喜欢通过最新的视频游戏系统来提升水平？通过在西好莱坞租赁音响系统来提升你在家和路上的娱乐体验。您可以灵活地续租并支付最好的家庭音响
    […] | 感恩练习。在你起床之前，想想你最感激的五件事。如果你的生命路径数字是2，你具备适应任何世俗体验的双重性。你在处理其他数字时深深扎根于平衡和和谐。为了保持与社区的联系，从与你的朋友和家人连接开始你的一天。不要急于进入社交
    […] | “守门人”肯定没用！然后是那些将学校图书馆员的形象建立在他们童年记忆中的人，可能不让他们从成人书架上借书或告诉他们保持安静。你知道的——戴眼镜和盘发的陈词滥调的女人？我自己戴眼镜，结果不得不剪发以避免这种陈词滥调。夏天时，我当然得把我的
    […] |'
- en: '| the-art mixed-use development that features a wide variety of shops, services,
    and restaurants, along with over 950 luxury apartments. The sprawling urban village
    is pedestrian-friendly and is the perfect place if you want to indulge in a shopping
    spree or treat your taste buds to a hearty meal. If you’re thinking about looking
    for the perfect home in Brookhaven, I’m ready to help! Get in touch […] | it as
    a stand-alone piece but later experimented performing it as my written prediction,
    confabulation style, Closing Effect. It’s still a work in progress but I did receive
    some "Standing Ovations!" ALAN ARITA "I received a copy of GAME NIGHT and IT IS
    EXCELLENT! First, the quality of the book is outstanding; everything from the
    artwork, layout, hidden gems, and of course the precision cut […] | and view the
    supernal beauty that lies beyond. (I wish I’d have said that first; actually I
    stole it from a guy who wrote it a hundred years ago!*) But if I couldn’t see
    into the future for a few years, there wouldn’t be a Christmas story today. I’ve
    a whole lot of notes still in my jeans. One’s about Rabbi Frankel of the Synagogue
    across West Street from old Reno High School. He was a pretty […] |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 这个综合开发项目包括各种商店、服务和餐厅，还有超过950套豪华公寓。这座广阔的城市村庄步行友好，非常适合你想要尽情购物或犒赏自己的味蕾。如果你在考虑在布鲁克黑文寻找完美的家，我随时准备帮助你！联系我
    […] | 将其作为独立的部分进行展示，但后来尝试以我书面预测的方式，进行即兴风格的演绎和结尾效果。这仍在进行中，但我确实收到了些“起立鼓掌！”**艾伦·阿里塔**
    “我收到了《游戏之夜》的副本，**它非常出色**！首先，书籍质量非常高；从插图、布局、隐藏的宝藏，到精确切割 […] | 并欣赏那超凡的美景。（我希望我能先说这句话；其实我是从一个一百年前写这句话的人那里偷来的！*）但如果我不能预见未来几年，那么今天就不会有圣诞故事。我还有一大堆笔记留在我的牛仔裤里。其中一篇是关于从老雷诺高中对面的犹太教堂的弗兰克尔拉比。他是一个相当
    […] |'
- en: '| toilet drains are overwhelmed with toilet paper or clogged by non-flushable
    things that find their way into the drain. If that’s the case, it may be time
    to call a plumbing technician. Unexpected toilet issues interrupt your daily routine,
    turning what you expected to be a good day right into a stressful one. You need
    help ASAP! Best quality Plumbing is ready to solve your toilet troubles no […]
    | who offer 3D printing services these days. Try searching for someone who offers
    them in your area.Last week, Apple announced the new A15 processor in a peculiar
    way: by comparing its new chip to the Android competition, rather than the A14
    that powered last year’s generation of iPhones. We were all left to try to infer
    the speed of the A15 based on Apple’s claims, and wondering if the company was
    […] | floor study, family room, kitchen, unfinished basement for future expansion
    & 2 car garage. Lennar seamlessly blended & showcased the unparalleled beauty
    of Colorado with the most innovative homes, energy efficient technologies & modern
    conveniences, bringing the best of both worlds together. Beautiful finishes and
    upgrades throughout. Lennar provides the latest in energy efficiency and state
    of […] |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 厕所排水管被卫生纸堵塞或被无法冲洗的物品堵塞。如果是这样，可能是时候联系管道维修技师了。意外的厕所问题打断了你的日常生活，将你期望的美好一天变成了压力山大的日子。你需要立刻帮助！**最佳管道服务**准备解决你的厕所问题，无
    […] | 现在有很多人提供3D打印服务。尝试搜索你所在地区提供这项服务的人。上周，苹果公司以一种特殊的方式宣布了新的A15处理器：通过将其新芯片与Android竞争对手进行比较，而不是去年为iPhone提供动力的A14。我们都不得不根据苹果公司的说法来推断A15的速度，并且想知道公司是否
    […] | 地下室学习区、家庭房、厨房、未完成的地下室以供未来扩展及双车车库。**Lennar**无缝融合并展示了科罗拉多州无与伦比的美丽与最先进的住宅、节能技术和现代便利设施，将两者的最佳结合在一起。美丽的装饰和升级遍布整个房屋。**Lennar**提供了最新的节能和现代
    […] |'
- en: 'Table 5: Samples from different distribution subsets using perplexity of a
    124M reference model trained on Wikipedia.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：使用124M参考模型在维基百科上训练的困惑度，从不同分布子集中的样本。
- en: '| Bottom 10% | Middle 10% | Top 10% |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 底部10% | 中间10% | 顶部10% |'
- en: '| --- | --- | --- |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| of our kids, demonstrated ability to create meaningful change, a strong commitment
    to learning, and an ability to work in partnership with others." Individuals accepted
    to this program agree to a two-year teaching commitment. If you become a core
    member you are required to attend an intensive summer training program to prepare
    for your two-year commitment. Each region has different requirements b […] | HST
    single cylinder hydraulic cone crusher. HST single cylinder hydraulic cone crusher
    integrates mechanical, hydraulic, electrical, automation, intelligent control
    and other technologies, which can be widely used in medium, fine and ultra-fine
    crushing operations in metal and non-metal mines, cement, sandstone, metallurgy
    and other industries… 1,214 roller cone crusher products are offered […] | active
    play outdoor. Users without a subscription are not able to see the full content
    on this page. Please subscribe or login.On the net betting houses include was
    able to offer followers a fabulous best range of luring optimistic aspects. A
    style of online casino money provides consistently continually really been ornamented
    and acquired in reaction to make sure you basic safety issues. Insi […] |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 我们的孩子展示了创造有意义变化的能力，对学习的强烈承诺，以及与他人合作的能力。” 被接受到这个项目的人同意进行为期两年的教学承诺。如果你成为核心成员，你需要参加一个强化的夏季培训课程，为你的两年承诺做好准备。每个地区有不同的要求b
    […] | HST 单缸液压圆锥破碎机。HST 单缸液压圆锥破碎机集成了机械、液压、电气、自动化、智能控制等技术，广泛用于金属和非金属矿山、水泥、砂岩、冶金等行业的中碎、细碎和超细碎作业……
    1,214 种滚筒圆锥破碎机产品提供 […] | 活动户外。没有订阅的用户无法查看此页面的全部内容。请订阅或登录。网络博彩公司能够提供令人惊叹的最佳诱人优惠。在线赌场资金的类型始终在安全问题上进行了装饰和改进。Insi
    […] |'
- en: '| to be that way. Weight loss surgery in Hanover is a great option for those
    who are at least fifty pounds overweight and have struggled with weight loss over
    the years. There are a number of surgical weight loss procedures available to
    those seeking treatment, and Nusbaum Weight Loss Centers of New Jersey, with offices
    and bariatric surgeons in Morristown, Morris County, Morris County, and surrou
    […] | sperm whales. Learn firsthand about Sri Lanka’s amazing biodiversity on
    this private tour to the Kanneliya Rainforest. With a dedicated guide leading
    you, explore the UNESCO-listed biosphere reserve, home to monkeys, snakes, chameleons,
    and a wide range of bird life. Learn about the flora and fauna through commentary
    tailored to your interests and enjoy plenty of chances to ask questions. Explo
    […] | row for spotting this Sabal Trail posting within minutes.The skin has become
    delicate. I just received the goods and I didn’t know how to use it. I consulted
    the customer service. I didn’t expect the customer service person to be super
    good and the introduction was super careful. I have been so successful and happy
    trading with you every time.. I hope we have more transactions in the future…
    Ha […] |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 在汉诺威的减肥手术是一个很好的选择，特别适合那些超重至少五十磅并且多年来一直在与减肥斗争的人。有许多手术减肥程序可供寻求治疗的人选择，Nusbaum
    Weight Loss Centers of New Jersey 在 Morristown、Morris County、Morris County 和周边地区设有办事处和减肥外科医生
    […] | 精子鲸。在私人导游的带领下，亲身了解斯里兰卡令人惊叹的生物多样性，探索联合国教科文组织列入的生物圈保护区，这里栖息着猴子、蛇、变色龙和各种鸟类。通过量身定制的解说了解植物和动物，并享受提问的机会。Explo
    […] | 在几分钟内发现这个Sabal Trail发布的帖子。皮肤变得很细腻。我刚收到货，不知道怎么用。我咨询了客服。没想到客服非常好，介绍得非常细致。我每次都和你们交易得非常成功和开心。希望我们未来有更多的交易……Ha
    […] |'
- en: '| to which coverage is thereby to be granted; and (2) Shall insure the person
    named therein and any other person, as insured, using any such motor vehicle or
    motor vehicles with the express or implied permission of such named insured against
    loss from the liability imposed by law for damages arising out of the ownership,
    maintenance, or use of such motor vehicle or motor vehicles within the United
    […] | Also, I have attached a brief presentation of our work for better understanding.A
    two-year solar energy project at the University of Sheffield has shown almost
    all of the 2,000 systems in the scheme are still performing better than expected.
    Researchers running Sheffield Solar Farm, which was launched in August 2010, say
    98 per cent of more than 2,000 systems involved in the scheme are working […]
    | It exposes a design and construction system for horizontal plates to work as
    slabs in regular concrete buildings. Based to an evolutionary finite-element analysis
    of the topological configuration to get a curved design with a 50% reduction of
    traditional volume, that provide lower cost, less carbon foot-print, better performance
    and innovative ceiling. A library of profiles is elaborated according […] |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 对于因此而授予的保障范围；（2）应确保文中所名之人以及任何其他以被保险人名义使用任何此类机动车辆或多辆机动车的人员，依据法律对因拥有、维护或使用这些机动车辆而产生的损失负责。
    | 此外，我附上了一份关于我们工作的简要介绍，以便更好地理解。谢菲尔德大学的为期两年的太阳能项目显示，方案中的2000个系统几乎都表现得比预期更好。运行谢菲尔德太阳能农场的研究人员表示，参与该方案的2000多个系统中有98%正在运行。
    | 这揭示了一种设计和施工系统，用于使水平板作为常规混凝土建筑中的板块工作。基于对拓扑配置的演变有限元分析，以获得一种曲线设计，并减少传统体积的50%，这提供了更低的成本、更少的碳足迹、更好的性能和创新的天花板。基于此开发了一个型材库。
    |'
- en: 'Table 6: Samples from different distribution subsets using EL2N from a 124M
    reference model trained on CommonCrawl.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：使用EL2N从124M参考模型中提取的不同分布子集的样本，该模型在CommonCrawl上训练。
- en: '| Bottom 10% | Middle 10% | Top 10% |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 底部10% | 中间10% | 顶部10% |'
- en: '| --- | --- | --- |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| a handle on how many elevators they are supposed to oversee. Those officials
    have repeatedly deflected requests from reporters to detail the count of elevators
    in Chicago requiring inspection. Frydland, during her interview, said she doesn’t
    know how many elevators her office is responsible for inspecting because city
    records lump elevators into the same class of devices as escalators, […] | there’s
    a possibility that you may come across a property that’s sharing a driveway with
    the home next door. That means that one driveway needs to be shared between the
    two adjoining neighbors. Many real estate investors rent out their properties
    in order to reap the benefits of passive monthly income while increasing their
    equity and building wealth over time. Not only are they benefiting […] | We have
    all spent happy hours listening to and sharing music we love with those closest
    to us. Many of the people we serve in ubu are incredibly gifted and play a wide
    range of musical instruments and enjoy singing and performing for other people.
    Judith is enabled by ubu to live more independently in Knaresborough, North Yorkshire,
    and has started taking singing lessons in order to ’grow’ her […] |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 关于他们应该监督多少台电梯的掌握情况。这些官员多次回避记者对芝加哥需要检查的电梯数量的详细要求。在采访中，弗里德兰表示，她不知道她的办公室负责检查多少台电梯，因为市政府记录将电梯与自动扶梯归为同一类别。
    | 你可能会遇到一个与邻居共享车道的房产。这意味着两个相邻的邻居需要共享一个车道。许多房地产投资者出租他们的房产，以便从中获得被动的月收入，同时增加他们的资产并随着时间的推移积累财富。他们不仅从中受益。
    | 我们都曾在幸福的时光中聆听并与亲密的人分享我们喜欢的音乐。我们在ubu服务的许多人都非常有才华，演奏各种乐器，并享受为他人唱歌和表演。朱迪斯得到了ubu的支持，使她能够在北约克郡的克纳斯堡更独立地生活，并开始上歌唱课程，以便“成长”她的……
    |'
- en: '| ians 4:3? Jesus addressed this very issue with his disciples on the night
    of his betrayal. He would be leaving them soon, but he promised the Holy Spirit
    would come to comfort and aide them, "I will not leave you as orphans; I will
    come to you."-John 14:18\. Jesus refers to the Holy Spirit as himself because,
    "the Helper, the Holy Spirit, whom the Father will send in my name, he will teach
    you all […] | the standard as far as cement manufacturing goes several cement
    manufacturers still prefer ball mills for cement production when they want to
    design new grinding plants or a new integrated 3D design and analysis of the crushing
    roller of The crushing roller is one of the main parts of a highpressure grinding
    roller which is a type of highly efficient ore crushing equipment In the work
    reported […] | range (Table 1). Active-Controlled Study: CRESTOR was compared
    with the HMG-CoA reductase inhibitors atorvastatin, simvastatin, and pravastatin
    in a multicenter, open-label, dose-ranging study of 2,240 patients with Type IIa
    and IIb hypercholesterolemia. After randomization, patients were treated for 6
    weeks with a single daily dose of either CRESTOR, atorvastatin, simvastatin, or
    pravastatin […] |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 约翰福音 14:18？耶稣在被背叛的那晚与门徒讨论了这个问题。他将很快离开他们，但他承诺圣灵会来安慰和帮助他们，“我不撇下你们为孤儿；我必到你们这里来。”-
    约翰福音 14:18\. 耶稣将圣灵称为他自己，因为“那位父所差来的帮助者圣灵，他必将一切事教导你们” | 就水泥制造而言，尽管水泥制造商在设计新的磨机或新的综合3D设计与分析时仍然偏好球磨机，但水泥制造的标准有所改变。破碎辊是高压磨机的主要部件之一，这是一种高效的矿石破碎设备。在报告的工作中[…]
    | 范围（表1）。主动控制研究：CRESTOR在多中心、开放标签、剂量范围研究中与HMG-CoA还原酶抑制剂阿托伐他汀、辛伐他汀和普伐他汀进行比较，共涉及2,240名IIa型和IIb型高胆固醇血症患者。随机分组后，患者接受6周的单次每日剂量治疗，药物包括CRESTOR、阿托伐他汀、辛伐他汀或普伐他汀[…]
    |'
- en: '| Most past attemptsto define socioeconomics as a science in its own right
    may have been motivated tocounter such a simplistic understanding of socioeconomics.In
    this chapter, we review past attempts to define socioeconomics before theapproach
    is chosen that we applied in this book. This book, by a leading expert in urban
    agriculture, offers a genuine solution to today’s global food crisis. By […] |
    which adopted our buttons such that when we went to Boston.com (part of NY times)
    branding was not part of our discussions. Of course, we had matured in our thinking
    and offered them a co-branded offer hosted by Coola. When Switchboard did not
    work for us, we went to their competition Infospace.com, which was much larger
    than them. They accepted a branded Coola button but offered a complex deal […]
    | Trend.com: I had no idea this was coming. There’d been talk over the years about
    setting up a sort of business portal that integrated all of Trend’s regular and
    annual publications, but there was never enough momentum to actually get it going.
    Trend had a regular spot on the Times’ online Business section, but it was a pretty
    low-impact thing (even though quite a bit of traffic would come to the […] |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 过去尝试将社会经济学定义为独立科学的动机可能是为了反驳这种过于简化的理解。在本章中，我们回顾了过去尝试定义社会经济学的方法，然后选择了本书中所采用的方法。本书由城市农业领域的领先专家撰写，为当今全球粮食危机提供了真正的解决方案。通过[…]
    | 我们的按钮采用了我们前往Boston.com（纽约时报的一部分）时，品牌化并未成为我们讨论的一部分。当然，我们的思维已经成熟，并为他们提供了由Coola主办的联合品牌优惠。当Switchboard对我们不起作用时，我们转向了比他们大得多的竞争对手Infospace.com。他们接受了一个品牌的Coola按钮，但提出了一个复杂的交易[…]
    | Trend.com：我完全不知道这会发生。多年来一直在讨论建立一个整合Trend常规和年度出版物的商业门户，但从未有足够的动力来真正推动它。Trend在《泰晤士报》在线商业版块中有一个固定位置，但影响力相当小（尽管相当多的流量会到达[…]
    |'
- en: 'Table 7: Samples from different distribution subsets using memorization of
    a 124M reference model trained on CommonCrawl.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：使用在CommonCrawl上训练的124M参考模型的不同分布子集样本的记忆化。
- en: '| Mem. Factor = 0 | Mem. Factor = 0.5 | Mem. Factor = 1.0 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 记忆因子 = 0 | 记忆因子 = 0.5 | 记忆因子 = 1.0 |'
- en: '| --- | --- | --- |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| doesn’t prevent you from clearly seeing the road. Hi, thank you so much for
    your words, appreciate it! Moreover, we noted your comments, we’ll think what
    can be done, for sharing more ideas, feel free to contact us at support@hudwayapp.com
    any time. Happy to help you always! I do a lot of mudding. And it’s got a pitch
    and roll gauge, which I like when I’m in the hole, do I don’t flip my truck. […]
    | 160 countries. There are abundant hot-selling projects accessible to you. Cheap
    and environmentally friendly: Factory-direct sale, fast delivery with guaranteed
    quality at factory price, in line with the concept of environmental development.
    Feb 19 2021 should pelletisation of sulfide solidelectrolytesafterball millinghas
    to be done in argon atmosphere question 7 answers i am using a spex 8000b […]
    | reference. My company’s NACHI 230/600E bearing price concessions, adequate inventory,
    and other similar products are available for recommendation 1 . Less than 45 KGS,
    we will send by express. (Door to Door, Convenient) 2 . 45 - 200 KGS , we will
    send by air transport . (Fastest and safest, but expensive) 3 . More than 200
    KGS, we will send by sea . ( Cheapest and common use ) The bearing 240/8 […] |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 不会妨碍你清晰地看到道路。你好，非常感谢你的反馈，感谢！此外，我们注意到了你的评论，我们会考虑可以做些什么，如需分享更多想法，随时通过support@hudwayapp.com联系我们。很高兴随时为你提供帮助！我做了很多泥浆工作。它有一个倾斜和滚转仪，这在我陷入泥潭时非常有用，以免翻车。
    […] | 160个国家。你可以接触到大量热销项目。便宜且环保：工厂直销，快速发货，保证质量，工厂价格，与环境发展理念一致。2021年2月19日，硫化物固体电解质在球磨后是否需要在氩气氛围中进行造粒？问题7答案：我正在使用SPEX
    8000B […] | 参考。我的公司NACHI 230/600E轴承有价格优惠，库存充足，并且还有其他类似产品可以推荐 1 . 45公斤以下的，我们将通过快递发送。（门到门，方便）2
    . 45 - 200公斤的，我们将通过航空运输发送。（最快和最安全，但价格较高）3 . 200公斤以上的，我们将通过海运发送。（最便宜和常用）轴承240/8
    […] |'
- en: '| disposal and processing of contaminated suspensions such as drilling mud,
    road sweepings and similar. The rising demand on the international market to meet
    current as well as future environmental regulations is the main driver for the
    development in this area of our work," explains Managing Director Ing. Mag. Erich
    Trunkenpolz. "The plants are currently developed for stationary and semi-mobile
    du […] | $97 monthly subscription package. If you decide to make an annual payment
    of $997, you get two free months. I started with this basic package but I later
    decided to upgrade to Etison Suite since this one has some limitations. As a marketer,
    I was only allowed to use 3 custom domains, get a limit of 20,000 visitors, and
    make a maximum of 100 web pages. I discovered that some advanced features are
    […] | takes your bank to process our refund request (5 to 10 business days). If
    you need to return an item, simply login to your account, view the order using
    the ’Complete Orders’ link under the My Account menu and click the Return Item(s)
    button. We’ll notify you via e-mail of your refund once we’ve received and processed
    the returned item. We can ship to virtually any address in the world. Note the
    […] |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 对受污染的悬浮液，如钻井泥浆、道路清扫物及类似物质的处置和处理。满足当前及未来环境法规的国际市场需求是我们工作的主要推动力，" 总经理Ing. Mag.
    Erich Trunkenpolz解释道。 "目前，这些工厂正为固定和半移动的用途进行开发 […] | $97每月订阅套餐。如果你决定一次性支付$997，你将获得两个月的免费使用。我开始时使用的是这个基础套餐，但后来决定升级到
    Etison Suite，因为这个套餐有一些限制。作为一名营销人员，我只能使用3个自定义域名，获得20,000名访客的限制，并制作最多100个网页。我发现一些高级功能是
    […] | 需要您的银行处理我们的退款请求（5到10个工作日）。如果需要退货，只需登录到您的账户，通过“我的账户”菜单下的“完成订单”链接查看订单，然后点击“退货”按钮。我们会在收到并处理退回的物品后，通过电子邮件通知您退款情况。我们可以向世界上几乎任何地址发货。请注意
    […] |'
- en: '| time:If you’re looking into faster-than-light fiber internet, there’s a Verizon
    Fios deal for you in Silver Spring, MD. Want more than a Verizon Fios internet-only
    plan? Open your home up to more entertainment choices with Verizon Fios packages.
    Ready to improve your home with the best internet available? Get lightspeed internet
    with Verizon plans that suit every lifestyle. Whether you only need […] | Select
    options that apply then copy and paste the RDF/HTML data fragment to include in
    your application Note: Adjust the width and height settings defined in the RDF/HTML
    code fragment to best match your requirementsCause.—Upon the ascension of William
    and Mary to the throne of England, the Protestants of Maryland demanded the Colonial
    management of the Territory. The Roman Catholics, after rep […] | to assess the
    success of our marketing and advertising campaigns). Finally, we may also share
    your Personal Information to comply with applicable laws and regulations, to respond
    to a subpoena, search warrant or other lawful request for information we receive,
    or to otherwise protect our rights. Additionally, you can opt out of some of these
    services by visiting the Digital Advertising Alliance […] |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 时间：如果你在寻找比光速更快的光纤互联网，银泉，MD有一个适合你的Verizon Fios优惠。想要比Verizon Fios互联网仅限计划更多的选择？通过Verizon
    Fios套餐让你的家有更多的娱乐选择。准备好用最好的互联网改善你的家了吗？选择适合你生活方式的Verizon计划，享受光速互联网。不论你只需要 […] |
    选择适用的选项，然后复制并粘贴RDF/HTML数据片段以包含在你的应用程序中。注意：调整RDF/HTML代码片段中定义的宽度和高度设置，以最佳匹配你的需求。原因：——在威廉和玛丽登上英格兰王位后，马里兰的新教徒要求殖民地管理该领土。罗马天主教徒，在
    […] | 评估我们营销和广告活动的成功。最后，我们也可能分享你的个人信息以遵守适用的法律法规，回应传票、搜查令或其他合法的信息请求，或以其他方式保护我们的权利。此外，你可以通过访问数字广告联盟来选择退出其中一些服务
    […] |'
