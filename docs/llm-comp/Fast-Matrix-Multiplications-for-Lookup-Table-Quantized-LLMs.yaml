- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:47:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Fast Matrix Multiplications for Lookup Table-Quantized LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找表量化LLMs的快速矩阵乘法
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.10960](https://ar5iv.labs.arxiv.org/html/2407.10960)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.10960](https://ar5iv.labs.arxiv.org/html/2407.10960)
- en: $\textbf{Han Guo}^{\star}$
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**韩国**^{\star}'
- en: $\textbf{Jonathan Ragan-Kelley}^{\star}$
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**乔纳森·拉根-凯利**^{\star}'
- en: ^($\star$)High School of Mathematics Plovdiv
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\star$)数学中学普罗夫迪夫
- en: ^($\diamond$)Carnegie Mellon University, MBZUAI, Petuum Inc.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\diamond$)卡内基梅隆大学，MBZUAI，Petuum Inc.
- en: '{hanguo,wbrandon,radi_cho,jrk,yoonkim}@mit.edu,    epxing@cs.cmu.edu'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{hanguo,wbrandon,radi_cho,jrk,yoonkim}@mit.edu,    epxing@cs.cmu.edu'
- en: \faGithub    [https://github.com/HanGuo97/flute](https://github.com/HanGuo97/flute)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \faGithub    [https://github.com/HanGuo97/flute](https://github.com/HanGuo97/flute)
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The deployment of large language models (LLMs) is often constrained by memory
    bandwidth, where the primary bottleneck is the cost of transferring model parameters
    from the GPU’s global memory to its registers. When coupled with custom kernels
    that fuse the dequantization and matmul operations, weight-only quantization can
    thus enable faster inference by reducing the amount of memory movement. However,
    developing high-performance kernels for weight-quantized LLMs presents substantial
    challenges, especially when the weights are compressed to non-evenly-divisible
    bit widths (e.g., 3 bits) with non-uniform, lookup table (LUT) quantization. This
    paper describes FLUTE, a flexible lookup table engine for LUT-quantized LLMs,
    which uses offline restructuring of the quantized weight matrix to minimize bit
    manipulations associated with unpacking, and vectorization and duplication of
    the lookup table to mitigate shared memory bandwidth constraints. At batch sizes
    < 32 and quantization group size of 128 (typical in LLM inference), the FLUTE
    kernel can be 2-4$\times$ faster than existing GEMM kernels. As an application
    of FLUTE, we explore a simple extension to lookup table-based NormalFloat quantization
    and apply it to quantize LLaMA3 to various configurations, obtaining competitive
    quantization performance against strong baselines while obtaining an end-to-end
    throughput increase of 1.5 to 2 times.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的部署通常受到内存带宽的限制，其中主要的瓶颈是将模型参数从GPU的全局内存传输到寄存器的成本。当与融合解量化和矩阵乘法操作的自定义内核结合使用时，仅权重量化可以通过减少内存移动量来实现更快的推理。然而，为权重量化的LLMs开发高性能内核面临着巨大的挑战，特别是当权重被压缩为非均匀可分割的位宽（例如，3位）且采用非均匀查找表（LUT）量化时。本文描述了FLUTE，一个用于LUT量化LLMs的灵活查找表引擎，它通过离线重组量化权重矩阵来最小化与解包相关的位操作，并通过向量化和查找表复制来缓解共享内存带宽限制。在批处理大小小于32且量化组大小为128（LLM推理中典型的情况）时，FLUTE内核可以比现有的GEMM内核快2-4倍。作为FLUTE的应用，我们探索了对基于查找表的NormalFloat量化的简单扩展，并将其应用于将LLaMA3量化到各种配置中，取得了与强基线竞争的量化性能，同时实现了端到端吞吐量提高1.5到2倍。
- en: Fast Matrix Multiplications for Lookup Table-Quantized LLMs
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 查找表量化LLMs的快速矩阵乘法
- en: $\textbf{Han Guo}^{\star}$)Carnegie Mellon University, MBZUAI, Petuum Inc. {hanguo,wbrandon,radi_cho,jrk,yoonkim}@mit.edu,
       epxing@cs.cmu.edu \faGithub    [https://github.com/HanGuo97/flute](https://github.com/HanGuo97/flute)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**韩国**^{\star}）卡内基梅隆大学，MBZUAI，Petuum Inc. {hanguo,wbrandon,radi_cho,jrk,yoonkim}@mit.edu,
       epxing@cs.cmu.edu \faGithub    [https://github.com/HanGuo97/flute](https://github.com/HanGuo97/flute)'
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Large language model (LLM) deployment faces significant latency challenges due
    to the memory bandwidth constraints inherent in generative (token-by-token) inference.
    The primary bottleneck is the cost of transferring model parameters from the GPU’s
    global memory to the registers, i.e., LLM inference is *memory-bound*. To overcome
    this “memory wall” Gholami et al. ([2024](#bib.bib9)), practitioners have increasingly
    adopted weight-only quantization methods, wherein the parameters of an LLM are
    compressed to lower precision (e.g., 4 or 8 bits) than the precision in which
    they were trained (typically 16 bits). In addition to latency improvements, weight
    quantization can also drastically reduce GPU memory required for deployment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）部署面临由于生成（逐个标记）推理固有的内存带宽限制带来的显著延迟挑战。主要的瓶颈是将模型参数从GPU的全局内存传输到寄存器的成本，即LLM推理是*内存绑定*的。为了克服这一“内存墙”，Gholami等人（[2024](#bib.bib9)）的实践者们越来越多地采用仅权重量化方法，其中LLM的参数被压缩到低于其训练时精度（通常为16位）的精度（例如4位或8位）。除了延迟改进外，权重量化还可以大幅减少部署所需的GPU内存。
- en: Realizing practical speed-ups with weight-only quantization requires custom
    mixed-type matrix-matrix multiply (matmul) kernels which must (1) move a layer’s
    quantized weights from GPU off-chip DRAM to on-chip SRAM, (2) *de*quantize the
    weights to floating-point (FP) format (on chip), (3) perform the FP matmul, and
    (4) write the results back to DRAM. Existing kernels such as bitsandbytes Dettmers
    et al. ([2023](#bib.bib5)), Marlin Frantar and Alistarh ([2024](#bib.bib7)), and
    BitBLAS Wang et al. ([2024](#bib.bib24)) demonstrate that this strategy can result
    in significant matmul speed-ups, e.g. up to four times faster when going from
    W16A16 to W4A16. However, these kernels are typically specialized to 4-bit quantization,
    and while some kernels support non-uniform, lookup table (LUT) quantization, they
    are generally slower than the uniform counterparts. Given the recent promising
    results with odd-bit Shao et al. ([2023](#bib.bib22)); Ma et al. ([2024b](#bib.bib17),
    [a](#bib.bib16)) and non-uniform Guo et al. ([2024](#bib.bib10)); Kim et al. ([2023](#bib.bib12))
    quantization methods, there is thus a need to develop flexible kernels that can
    support mixed-type matmuls with a wider range of settings.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实现基于权重的量化的实际加速需要定制的混合类型矩阵乘法（matmul）内核，这些内核必须（1）将一层的量化权重从 GPU 的外部 DRAM 移动到片上
    SRAM，（2）将权重*解*量化为浮点（FP）格式（在芯片上），（3）执行 FP matmul，并且（4）将结果写回 DRAM。现有的内核，如 bitsandbytes
    Dettmers 等人（[2023](#bib.bib5)）、Marlin Frantar 和 Alistarh（[2024](#bib.bib7)）以及
    BitBLAS Wang 等人（[2024](#bib.bib24)），展示了这一策略可以显著提高 matmul 的速度，例如，从 W16A16 到 W4A16
    时速度可以提高四倍。然而，这些内核通常专门针对 4 位量化，尽管一些内核支持非均匀的查找表（LUT）量化，但它们通常比均匀的对照更慢。鉴于最近关于奇数位 Shao
    等人（[2023](#bib.bib22)）；Ma 等人（[2024b](#bib.bib17)，[a](#bib.bib16)）和非均匀 Guo 等人（[2024](#bib.bib10)）；Kim
    等人（[2023](#bib.bib12)）量化方法的有希望的结果，因此需要开发能够支持更广泛设置的混合类型 matmul 的灵活内核。
- en: This paper describes FLUTE, a flexible lookup-table engine for deploying weight-quantized
    LLMs, with a focus on the low-bit and non-uniform quantization setting. This setting
    raises several challenges. First, going beyond 8-bit quantization involves packing
    sub-8-bit matrices into supported data types, followed by unpacking during dequantization.
    Structuring the unpacked data to match GPU-native matmul formats is especially
    challenging when the weights are quantized to non-standard bit-widths. Second,
    while uniformly-quantized models can rely on assembly-level optimizations to convert
    from INT to FP through bit-level manipulations, lookup table-based dequantization
    involves dynamic indexing, and a naïve implementation can lead to substantial
    overhead. Finally, typical matmul implementations which distribute the workload
    across a grid of parallel thread blocks become inefficient with small batches
    and low bit-width weights; this necessitates more sophisticated partitioning strategies
    to optimize hardware resource utilization.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文描述了 FLUTE，一个用于部署权重量化 LLM 的灵活查找表引擎，重点关注低位和非均匀量化设置。该设置带来了几个挑战。首先，超出 8 位量化涉及将子
    8 位矩阵打包到支持的数据类型中，然后在解量化过程中解包。将解包的数据结构化以匹配 GPU 原生的 matmul 格式，尤其是在权重量化为非标准位宽时，具有特别的挑战。其次，虽然均匀量化模型可以依赖汇编级优化通过位级操作从
    INT 转换为 FP，但基于查找表的解量化涉及动态索引，简单实现可能导致显著的开销。最后，典型的 matmul 实现将工作负载分布在平行线程块的网格中，在小批量和低位宽权重的情况下变得低效；这需要更复杂的分区策略以优化硬件资源利用。
- en: FLUTE addresses these challenges through a combination of (1) offline weight
    restructuring, (2) a shared-memory lookup table for efficient dequantization,
    and (3) Stream-K partitioning for optimized workload distribution. We compare
    FLUTE against existing kernels on standard LLM mixed-precision matmul settings
    where weights are quantized to 4 bits in groups of 128, and find that it outperforms
    existing non-uniform quantization kernels, and even matches the simpler uniform-quantization
    kernels in some cases. As an application of FLUTE, we experiment with quantizing
    LLaMA3—which has been found to be difficult to quantize Huang et al. ([2024](#bib.bib11))—using
    a variant of normal float (NF) quantization Dettmers et al. ([2023](#bib.bib5))
    which learns the quantization parameters based on calibration data. We find that
    we can achieve an 1.5 to 2 times increase in end-to-end throughput when integrated
    with frameworks such as vLLM Kwon et al. ([2023](#bib.bib13)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: FLUTE通过以下方式解决这些挑战：(1) 离线权重重构，(2) 用于高效去量化的共享内存查找表，(3) 用于优化工作负载分配的Stream-K分区。我们将FLUTE与现有的标准LLM混合精度矩阵乘法设置下的内核进行比较，其中权重被量化为4位，分组为128，并发现它优于现有的非均匀量化内核，甚至在某些情况下与更简单的均匀量化内核相匹配。作为FLUTE的应用，我们尝试使用一种基于校准数据学习量化参数的正常浮点（NF）量化变体来量化LLaMA3——这被发现难以量化
    Huang et al. ([2024](#bib.bib11))。我们发现，当与vLLM Kwon et al. ([2023](#bib.bib13))等框架集成时，可以实现1.5到2倍的端到端吞吐量提升。
- en: 2 Background and Related Work
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与相关工作
- en: 2.1 GPU Architecture and Memory Bandwidth Bottlenecks
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 GPU架构与内存带宽瓶颈
- en: GPUs are massively-parallel processors designed for throughput-oriented workloads
    containing large amounts of independent work. The hardware of a current-generation
    NVIDIA GPU consists of an array of many individual *streaming multiprocessors*
    (“SMs”), each consisting of $4$ consecutive threads are implicitly organized together
    into a single *warp*, corresponding to the GPU hardware’s actual native unit of
    instruction execution.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: GPU是为吞吐量导向的工作负载设计的大规模并行处理器，这些工作负载包含大量独立的工作。当前一代NVIDIA GPU的硬件由许多独立的*流处理器*（“SMs”）组成，每个SM中的$4$个连续线程被隐式组织在一起形成一个*warp*，对应于GPU硬件实际的原生指令执行单元。
- en: 'Although GPUs are able to execute large numbers of instructions in parallel
    across the warp schedulers of their many SMs, the rate at which instructions can
    be executed is not always the bottleneck in realistic GPU workloads. Instead,
    the maximum achievable throughput of a GPU workload is often constrained by the
    speed of *data movement* between levels of the GPU’s memory hierarchy. The memory
    resources of modern NVIDIA server-class GPUs consist of (roughly): (1) Tens of
    gigabytes of off-chip DRAM, referred to here as *global memory*; (2) Tens of megabytes
    of on-chip SRAM acting as a shared *L2 cache* accessible to all SMs; (3) Hundreds
    of kilobytes of local SRAM per SM, split into two configurably-sized portions,
    one acting as an *L1 cache* and the other an explicitly-addressed *local scratchpad*;
    and (4) Hundreds of kilobytes of local SRAM per SM, acting as *registers* for
    the threads running on that SM.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GPU能够在其多个SM的warp调度器上并行执行大量指令，但指令的执行速率并不总是现实GPU工作负载中的瓶颈。相反，GPU工作负载的最大可实现吞吐量通常受限于GPU内存层级间的*数据传输*速度。现代NVIDIA服务器级GPU的内存资源大致包括：(1)
    数十GB的外部DRAM，这里称为*全局内存*；(2) 数十MB的片上SRAM，作为所有SM可访问的共享*L2缓存*；(3) 每个SM有数百KB的本地SRAM，分为两部分，一部分作为*L1缓存*，另一部分作为显式寻址的*局部临时存储*；(4)
    每个SM有数百KB的本地SRAM，作为该SM上线程的*寄存器*。
- en: The read/write bandwidth of resources in this memory hierarchy can easily become
    the limiting factor for realistic GPU workloads. For example, an A100-80GB GPU
    supports a nominal peak throughput for 16-bit matrix-multiply instructions of
    $\approx 3\times 10^{14}$ matrix-multiply FLOPs per byte of data accessed will
    necessarily be limited by the GPU’s memory bandwidth, not by its compute throughput.
    Maximizing the ratio of FLOPs to bytes transferred, a quantity known as *arithmetic
    intensity*, is often the single most important consideration when designing high-performance
    kernels.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 内存层级中资源的读写带宽很容易成为现实GPU工作负载的限制因素。例如，A100-80GB GPU支持的16位矩阵乘法指令的名义峰值吞吐量为$\approx
    3\times 10^{14}$矩阵乘法FLOPs每字节数据访问必然受到GPU内存带宽的限制，而不是其计算吞吐量。最大化FLOPs与转移字节的比率，即*算术强度*，通常是在设计高性能内核时最重要的考虑因素。
- en: 2.2 LLM Deployment Characteristics
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLM部署特性
- en: Depending on the context, inference can be bottlenecked by compute throughput
    or memory bandwidth. For LLMs, training, large-prefill, and large-batch inference
    enjoy high arithmetic intensity as the sizes of matrices involved in the matmuls
    are large enough to saturate compute. Small-batch, token-by-token inference on
    the other hand involves narrower matmuls due to the smaller batch dimension, resulting
    in low arithmetic intensity. Reducing the amount of memory operations in this
    case can thus enable practical speed-ups, even if the number of FLOPs remains
    the same (or is even slightly increased). This has led to much recent work on
    customized kernels which move the weights from main memory to on-chip SRAM while
    keeping them quantized/sparse Dettmers et al. ([2023](#bib.bib5)); Kim et al.
    ([2023](#bib.bib12)); Frantar and Alistarh ([2024](#bib.bib7)); Wang et al. ([2024](#bib.bib24));
    Xia et al. ([2024a](#bib.bib27)), and then performing the actual matmuls in higher
    precision after dequantizing to FP on chip. Marlin implements this strategy for
    4-bit uniform quantization and reports significant (up to 4$\times$) matmul speed-ups
    even in moderate batch (16-32) settings. bitsandbytes Dettmers et al. ([2023](#bib.bib5))
    and BitBLAS Wang et al. ([2024](#bib.bib24)) extend this to LUT-quantized LLMs,
    but do not allow for 3 bit-quantized weights. Moreover, existing LUT-quantization
    kernels generally underperform uniform-quantization kernels.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上下文，推理可能会受到计算吞吐量或内存带宽的瓶颈。对于LLMs来说，训练、大规模预填充和大批量推理享有高算术强度，因为涉及的矩阵乘法中的矩阵尺寸足够大，可以饱和计算。而小批量、逐个令牌推理则由于批量维度较小，涉及的矩阵乘法较窄，结果算术强度较低。因此，在这种情况下减少内存操作量可以实现实际的加速，即使FLOPs的数量保持不变（甚至略微增加）。这促使了许多近期的工作，如Dettmers等人（[2023](#bib.bib5)）、Kim等人（[2023](#bib.bib12)）、Frantar和Alistarh（[2024](#bib.bib7)）、Wang等人（[2024](#bib.bib24)）和Xia等人（[2024a](#bib.bib27)），这些工作将权重从主内存转移到芯片上的SRAM，同时保持其量化/稀疏，然后在芯片上通过解量化到FP后进行实际的矩阵乘法。Marlin实现了这种策略用于4-bit均匀量化，并报告了显著的（高达4$\times$）矩阵乘法加速，即使在中等批量（16-32）设置中也有。bitsandbytes
    Dettmers等人（[2023](#bib.bib5)）和BitBLAS Wang等人（[2024](#bib.bib24)）将这一策略扩展到LUT量化的LLMs，但不允许3-bit量化的权重。此外，现有的LUT量化内核通常表现不如均匀量化内核。
- en: 2.3 Weight-only Quantization in LLMs
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 仅权重量化在LLMs中的应用
- en: Uniform quantization converts a group of full precision weights to lower-precision
    intervals of equal size through rounding. For example min-max quantization maps
    a group of weights $\mathbf{u}$ is a scaling factor. Recent methods improve upon
    min-max quantization by using calibration data Frantar et al. ([2022](#bib.bib8));
    Lin et al. ([2023](#bib.bib14)); Shao et al. ([2023](#bib.bib22)); Ma et al. ([2024b](#bib.bib17)).
    When both the weights and activations are quantized uniformly, it is possible
    to use INT matmuls to enable speed-ups beyond the savings from reduced memory
    movement. However, activation quantization remains difficult due to the presence
    of outlier channels, which necessitate sophisticated mitigation strategies Wei
    et al. ([2022](#bib.bib26)); Dettmers et al. ([2022](#bib.bib4)); Xiao et al.
    ([2022](#bib.bib29)); Zhao et al. ([2023](#bib.bib34)); Ashkboos et al. ([2023](#bib.bib1),
    [2024](#bib.bib2)); Nrusimha et al. ([2024](#bib.bib19)); Lin et al. ([2024](#bib.bib15)).
    Weight-only quantization thus remains a popular choice for LLMs. Moreover, if
    only the weights are quantized, it is possible to reduce quantization error further
    by applying quantization at a more fine-grained levels (e.g., a block of 128 weight
    values) than at row- or column-level.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀量化通过四舍五入将一组全精度权重转换为较低精度的相等大小区间。例如，最小-最大量化将一组权重$\mathbf{u}$映射为一个缩放因子。近期的方法通过使用校准数据改进了最小-最大量化
    Frantar等人（[2022](#bib.bib8)）；Lin等人（[2023](#bib.bib14)）；Shao等人（[2023](#bib.bib22)）；Ma等人（[2024b](#bib.bib17)）。当权重和激活都被均匀量化时，可以使用INT矩阵乘法实现超出减少内存移动带来的节省的加速。然而，由于存在离群通道，激活量化仍然困难，这需要复杂的缓解策略
    Wei等人（[2022](#bib.bib26)）；Dettmers等人（[2022](#bib.bib4)）；Xiao等人（[2022](#bib.bib29)）；Zhao等人（[2023](#bib.bib34)）；Ashkboos等人（[2023](#bib.bib1),
    [2024](#bib.bib2)）；Nrusimha等人（[2024](#bib.bib19)）；Lin等人（[2024](#bib.bib15)）。因此，仅权重量化仍然是LLMs中的一种流行选择。此外，如果仅量化权重，可以通过在更细粒度的水平（例如128个权重值的块）上应用量化，从而进一步减少量化误差，而不是在行或列级别上。
- en: Non-uniform quantization generalizes uniform quantization by mapping weights
    to potentially *unequal* intervals Miyashita et al. ([2016](#bib.bib18)); Zhou
    et al. ([2017](#bib.bib35)); Zhang et al. ([2018](#bib.bib33)); Yang et al. ([2019](#bib.bib32)).
    Lookup table (LUT) quantization is a flexible variant of non-uniform quantization
    which can map intervals to arbitrary values via a lookup table Cardinaux et al.
    ([2020](#bib.bib3)); Wang et al. ([2022](#bib.bib25)). LUT quantization needs
    to trade off the size of the lookup table and the granularity of the groups at
    which the weights are quantized. For example, SqueezeLLM Kim et al. ([2023](#bib.bib12))
    applies K-means clustering at the column (output channel) level to obtain the
    lookup table, while NormalFloat quantization Dettmers et al. ([2023](#bib.bib5))
    uses a tensor-level lookup table obtained from the quantiles of a Normal distribution
    that is multiplicatively modified through group-level parameters. While it is
    possible to perform matmuls with activations/weights that are quantized non-uniformly
    (e.g., through LUT-based matmuls Xu et al. ([2021](#bib.bib30)); Park et al. ([2022](#bib.bib21))),
    these methods cannot leverage specialized accelerators on modern GPUs which are
    typically optimized for FP matmuls. We thus seek efficient kernels which can simultaneously
    make use of quantized representations (to minimize memory movement) as well as
    GPU-native matrix multiplications in FP.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 非均匀量化通过将权重映射到可能*不等*的区间来推广均匀量化 Miyashita et al. ([2016](#bib.bib18)); Zhou et
    al. ([2017](#bib.bib35)); Zhang et al. ([2018](#bib.bib33)); Yang et al. ([2019](#bib.bib32))。查找表（LUT）量化是非均匀量化的一种灵活变体，可以通过查找表将区间映射到任意值
    Cardinaux et al. ([2020](#bib.bib3)); Wang et al. ([2022](#bib.bib25))。LUT 量化需要在查找表的大小和权重量化的组粒度之间进行权衡。例如，SqueezeLLM
    Kim et al. ([2023](#bib.bib12)) 在列（输出通道）级别应用 K-means 聚类以获得查找表，而 NormalFloat 量化
    Dettmers et al. ([2023](#bib.bib5)) 使用从正态分布的分位数获得的张量级查找表，该分布通过组级参数进行乘法修改。虽然可以使用非均匀量化的激活/权重进行矩阵乘法（例如，通过基于
    LUT 的矩阵乘法 Xu et al. ([2021](#bib.bib30)); Park et al. ([2022](#bib.bib21)))，但这些方法无法利用现代
    GPU 上的专用加速器，这些加速器通常针对 FP 矩阵乘法进行优化。因此，我们寻求能够同时利用量化表示（以最小化内存移动）以及 GPU 原生 FP 矩阵乘法的高效内核。
- en: '3 FLUTE: A Fast and Flexible Kernel for Mixed-Type Matrix Multiplications'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '3 FLUTE: 一种快速且灵活的混合类型矩阵乘法内核'
- en: Let ${\mathbf{Q}}\in\mathbb{Z}^{k\times n}$ is given by,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 设 ${\mathbf{Q}}\in\mathbb{Z}^{k\times n}$ 如下给出，
- en: '|  | $\displaystyle{\mathbf{Q}}_{ij}=\operatorname{quantize}(\mathbf{W}_{ij};\mathbf{T})=\operatornamewithlimits{arg\,min}_{c}&#124;\mathbf{W}_{ij}-v_{c}&#124;,\vspace{-1mm}$
    |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathbf{Q}}_{ij}=\operatorname{quantize}(\mathbf{W}_{ij};\mathbf{T})=\operatornamewithlimits{arg\,min}_{c}&#124;\mathbf{W}_{ij}-v_{c}&#124;,\vspace{-1mm}$
    |  |'
- en: where ${\mathbf{Q}}_{ij}\in\{0,\dots,2^{b}{-1}\}$ be the *de*quantized matrix
    where
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\mathbf{Q}}_{ij}\in\{0,\dots,2^{b}{-1}\}$ 是*去*量化矩阵，其中
- en: '|  | $\displaystyle\widehat{{\mathbf{W}}}_{ij}=\operatorname{dequantize}({\mathbf{Q}}_{ij},\mathbf{T})=\mathbf{T}[{\mathbf{Q}}_{ij}].$
    |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{{\mathbf{W}}}_{ij}=\operatorname{dequantize}({\mathbf{Q}}_{ij},\mathbf{T})=\mathbf{T}[{\mathbf{Q}}_{ij}].$
    |  |'
- en: Our objective is to perform a fast matrix multiplication between a dense input
    activation matrix ${\mathbf{X}}\in\mathbb{R}^{m\times k}$.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是执行一个快速的矩阵乘法，涉及一个密集的输入激活矩阵 ${\mathbf{X}}\in\mathbb{R}^{m\times k}$。
- en: Algorithm 1 FLUTE (Simplified)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 FLUTE（简化版）
- en: ${\mathbf{X}}^{\text{g}}$     end whileend parallel for
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ${\mathbf{X}}^{\text{g}}$     结束 whileend 并行 for
- en: A straightforward implementation of such mixed-type matrix multiplication uses
    separate kernels. The first kernel loads the quantized matrix ${\mathbf{Q}}$ is
    moved back and forth. We can achieve faster matmuls by *fusing* the dequantization
    and matmul kernels, where we dequantize on chip and immediately use the dequantized
    values for the matmul.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种混合类型矩阵乘法的直接实现使用了不同的内核。第一个内核加载量化矩阵 ${\mathbf{Q}}$ 并来回移动。我们可以通过*融合*去量化和矩阵乘法内核来实现更快的矩阵乘法，其中我们在芯片上进行去量化，并立即使用去量化后的值进行矩阵乘法。
- en: 'However, implementing a fused weight-only LUT-quantized matmul that leads to
    speed-ups presents several challenges. For one, high-performance matmul necessitates
    the use of specialized primitives, such as Tensor Cores, which have strict requirements
    regarding the types, shapes, and layout of data. Second, efficient dynamic indexing
    is crucial for LUT-based dequantization; however, GPUs do not natively support
    dynamic indexing of a lookup table in their fastest on-chip registers. Finally,
    with smaller input matrices arising from low-bit and low-batch deployment, achieving
    workload balance across SMs is vital for maintaining speed, thus necessitating
    sophisticated partitioning strategies. FLUTE addresses these challenges through
    a combination of offline restructuring of the quantized weight matrix (§[3.1](#S3.SS1
    "3.1 Offline Matrix Restructuring ‣ 3 FLUTE: A Fast and Flexible Kernel for Mixed-Type
    Matrix Multiplications ‣ Fast Matrix Multiplications for Lookup Table-Quantized
    LLMs")), vectorization and duplication of the lookup table to mitigate shared
    bandwidth constraints (§[3.2](#S3.SS2 "3.2 Vectorized Lookup in Shared Memory
    ‣ 3 FLUTE: A Fast and Flexible Kernel for Mixed-Type Matrix Multiplications ‣
    Fast Matrix Multiplications for Lookup Table-Quantized LLMs")), and Stream-K workload
    partitioning to minimize wave quantization (§[3.3](#S3.SS3 "3.3 Stream-K Workload
    Partitioning ‣ 3 FLUTE: A Fast and Flexible Kernel for Mixed-Type Matrix Multiplications
    ‣ Fast Matrix Multiplications for Lookup Table-Quantized LLMs")). Alg. [1](#alg1
    "Algorithm 1 ‣ 3 FLUTE: A Fast and Flexible Kernel for Mixed-Type Matrix Multiplications
    ‣ Fast Matrix Multiplications for Lookup Table-Quantized LLMs") gives a simplified
    version of the FLUTE kernel, while Fig. [1](#S3.F1 "Figure 1 ‣ 3 FLUTE: A Fast
    and Flexible Kernel for Mixed-Type Matrix Multiplications ‣ Fast Matrix Multiplications
    for Lookup Table-Quantized LLMs") shows a high-level overview. (See Alg. [2](#alg2
    "Algorithm 2 ‣ Appendix A Appendix ‣ Fast Matrix Multiplications for Lookup Table-Quantized
    LLMs") in the Appendix for more details).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，实现一个融合权重仅量化的 LUT 矩阵乘法（matmul），以提高速度，面临几个挑战。首先，高性能的矩阵乘法需要使用专门的原语，例如 Tensor
    Cores，这些原语对数据的类型、形状和布局有严格要求。其次，高效的动态索引对 LUT 基于解量化至关重要；然而，GPU 本身不原生支持在最快的片上寄存器中进行查找表的动态索引。最后，随着低比特和低批量部署导致输入矩阵变小，实现
    SM 之间的工作负载平衡对保持速度至关重要，这就需要复杂的分区策略。FLUTE 通过对量化权重矩阵的离线重构（§[3.1](#S3.SS1 "3.1 离线矩阵重构
    ‣ 3 FLUTE: 快速灵活的混合类型矩阵乘法内核 ‣ 查找表量化 LLM 的快速矩阵乘法")）、查找表的矢量化和复制以缓解共享带宽限制（§[3.2](#S3.SS2
    "3.2 共享内存中的矢量化查找 ‣ 3 FLUTE: 快速灵活的混合类型矩阵乘法内核 ‣ 查找表量化 LLM 的快速矩阵乘法")）以及 Stream-K
    工作负载分区以最小化波量化（§[3.3](#S3.SS3 "3.3 Stream-K 工作负载分区 ‣ 3 FLUTE: 快速灵活的混合类型矩阵乘法内核 ‣
    查找表量化 LLM 的快速矩阵乘法")）来应对这些挑战。算法[1](#alg1 "算法 1 ‣ 3 FLUTE: 快速灵活的混合类型矩阵乘法内核 ‣ 查找表量化
    LLM 的快速矩阵乘法") 给出了 FLUTE 内核的简化版本，而图[1](#S3.F1 "图 1 ‣ 3 FLUTE: 快速灵活的混合类型矩阵乘法内核 ‣
    查找表量化 LLM 的快速矩阵乘法") 展示了一个高层次的概述。更多细节请参见附录中的算法[2](#alg2 "算法 2 ‣ 附录 A 附录 ‣ 查找表量化
    LLM 的快速矩阵乘法")。'
- en: '![Refer to caption](img/626aff71c390fee6c2926e3932c140e2.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/626aff71c390fee6c2926e3932c140e2.png)'
- en: 'Figure 1: A simplified view of a kernel that fuses the dequantization and matmul
    steps. Each threadblock (group of threads) is responsible for computing one or
    more output tiles by performing the matrix product between specific rows of inputs
    and columns of weights. (1) The threadblock issues asynchronous copy instructions
    to fetch small chunks of input data (tiles) from global memory to shared memory.
    (2) As soon as a tile arrives in shared memory, it is further sliced into smaller
    chunks (fragments) and copied into registers. (3) Once all necessary components
    are in the registers, the quantized matrix undergoes dequantization. (4) The dequantized
    matrix and inputs are then processed by Tensor Cores using MMA (Matrix Multiply
    Accumulate) instructions. (5) Finally, the accumulated results are written back
    from the registers to the outputs in global memory.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：一个简化的视图，展示了融合去量化和 matmul 步骤的内核。每个线程块（线程组）负责通过对输入的特定行和权重的列进行矩阵乘法来计算一个或多个输出块。
    (1) 线程块发出异步复制指令，将小块输入数据（块）从全局内存获取到共享内存。 (2) 一旦一个块到达共享内存，它会进一步被切分成更小的块（片段），并复制到寄存器中。
    (3) 一旦所有必要组件都在寄存器中，量化矩阵将进行去量化处理。 (4) 去量化矩阵和输入随后由 Tensor Cores 使用 MMA（矩阵乘法累加）指令处理。
    (5) 最后，累积的结果从寄存器写回全局内存中的输出。
- en: '![Refer to caption](img/83c7254316824a6606de5fdcb433f743.png)![Refer to caption](img/ef4310819e7d394e75c2ebc934ab8184.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/83c7254316824a6606de5fdcb433f743.png)![参见说明](img/ef4310819e7d394e75c2ebc934ab8184.png)'
- en: 'Figure 2: Vectorized Lookup Table Design (Left). Instead of dequantizing one
    element at a time, we vectorize the lookup table by creating another table that
    holds the values of all possible pairs of indices. This can look up two values
    simultaneously, followed by efficient vectorized scaling operations. Stream-K
    Work Decomposition (Right). In classic work decomposition, output tile production
    is independently assigned to threadblocks. Each threadblock processes one (or
    more) rows of the left operand and one (or more) columns of the right operand,
    slicing down the inner K dimension to compute the corresponding output tile (Slice-K).
    However, when the weight matrix is heavily quantized, the reduced size can lead
    to “stragglers” in Slice-K due to uneven workload assignment. Stream-K Osama et al.
    ([2023](#bib.bib20)) addresses this by decomposing work at a finer granularity,
    enabling multiple threadblocks to collaboratively compute a single output tile.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：向量化查找表设计（左）。我们通过创建一个包含所有可能索引对值的表来向量化查找表，而不是一次去量化一个元素。这可以同时查找两个值，然后进行高效的向量化缩放操作。Stream-K
    工作分解（右）。在经典的工作分解中，输出块的生产被独立分配给线程块。每个线程块处理左操作数的一个（或多个）行和右操作数的一个（或多个）列，将内层 K 维度切分以计算相应的输出块（Slice-K）。然而，当权重矩阵严重量化时，缩小的尺寸可能导致
    Slice-K 中出现“拖沓者”，由于工作负载分配不均。Stream-K Osama 等 ([2023](#bib.bib20)) 通过在更精细的粒度下分解工作来解决这个问题，使多个线程块能够协作计算一个输出块。
- en: 3.1 Offline Matrix Restructuring
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 离线矩阵重构
- en: Modern GPUs feature specialized primitives (Tensor Cores)—distinct from general-purpose
    vector ALUs—which can substantially accelerate dense matrix multiplications. For
    example, A100’s FP16 tensor core matmuls are 16$\times$ (the quantized weights)
    are static during inference, allowing for offline weight reordering such that
    after dequantization, the weights are already laid out exactly in the expected
    format Frantar and Alistarh ([2024](#bib.bib7)); Xia et al. ([2024b](#bib.bib28));
    Lin et al. ([2024](#bib.bib15)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 GPU 配备了专门的原语（Tensor Cores），与通用向量 ALU 不同，这些原语可以显著加速密集矩阵乘法。例如，A100 的 FP16 tensor
    core matmuls 是 16$\times$（量化权重）在推理期间是静态的，这允许进行离线权重重排序，使得去量化后，权重已经完全按照预期格式排列 Frantar
    和 Alistarh ([2024](#bib.bib7)); Xia 等 ([2024b](#bib.bib28)); Lin 等 ([2024](#bib.bib15))。
- en: 'The above strategy is difficult to straightforwardly extend to the case of
    non-evenly-divisible bit widths (e.g., 3 bits). Kernels employ vectorized data
    access when loading data from global to shared memory. Hence each thread should
    access the quantized weight in granularity of 128 bits (or at least in powers
    of 2). While this could be addressed by padding, this would be inefficient. We
    instead split the (3-bit) quantized weight into two partitions Xia et al. ([2024b](#bib.bib28)),
    or bit-slices: one containing the 1-bit portion and the other the 2-bit portion,
    and issue two separate vectorized (asynchronous) data copy instructions. Once
    the two bit-slices are loaded into the registers, we combine them before dequantization.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 上述策略难以直接扩展到非均匀分割的位宽情况（例如，3 位）。内核在将数据从全局内存加载到共享内存时，使用矢量化数据访问。因此，每个线程应以 128 位（或至少是
    2 的幂）的粒度访问量化权重。虽然可以通过填充来解决这个问题，但这样效率较低。我们将（3 位）量化权重分为两个部分 Xia 等人 ([2024b](#bib.bib28))，或称为比特切片：一个包含
    1 位部分，另一个包含 2 位部分，并发出两个单独的矢量化（异步）数据复制指令。一旦两个比特切片被加载到寄存器中，我们在去量化之前将它们合并。
- en: 3.2 Vectorized Lookup in Shared Memory
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 共享内存中的矢量化查找
- en: During dequantiation each element $c$-bit values, slightly more than 1KB of
    storage. This is a fraction of the 48KB-163KB of shared memory on modern GPUs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在去量化过程中，每个元素 $c$-bit 值略多于 1KB 存储。这是现代 GPU 上 48KB-163KB 共享内存的一部分。
- en: Reducing bank conflicts.
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 减少银行冲突。
- en: Shared memory is organized such that each successive 32-bit segment corresponds
    to a “bank”, and there are 32 such banks. Each memory address in shared memory
    corresponds to the $\lfloor\frac{\operatorname{addr}}{32}\rfloor\operatorname{mod}32$
    bank. If threads in a warp access data from different banks, access is parallelized.
    However, if two threads access data from the same bank (but not the same address),
    the access is serialized. For the 4-bit and 3-bit vectorized tables, a simple
    implementation could thus cause up to 8-way bank conflicts (4-bit) or 2-way bank
    conflicts (3-bit). To mitigate this, we duplicate the 4-bit vectorized lookup
    table multiple times, placing copies in different memory banks, which allows threads
    to access values from different banks with reduced bank conflicts.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存的组织方式是每个连续的 32 位段对应一个“银行”，总共有 32 个这样的银行。共享内存中的每个内存地址对应于 $\lfloor\frac{\operatorname{addr}}{32}\rfloor\operatorname{mod}32$
    银行。如果 warp 中的线程访问来自不同银行的数据，则访问是并行的。然而，如果两个线程访问来自相同银行的数据（但地址不同），则访问是串行的。对于 4 位和
    3 位矢量化表，简单实现可能会导致最多 8 路银行冲突（4 位）或 2 路银行冲突（3 位）。为了解决这个问题，我们多次复制 4 位矢量化查找表，将副本放在不同的内存银行中，这样线程可以从不同的银行中访问值，从而减少银行冲突。
- en: '![Refer to caption](img/ec7b1e7f05c15579134bbedd698049b9.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec7b1e7f05c15579134bbedd698049b9.png)'
- en: 'Figure 3: Runtime performance of FLUTE in the standard W4G128 setting where,
    the weights are quantized to 4 bits in groups of 128\. We show speedup against
    16-bit torch.mm. The matrix shapes for our benchmarks are selected based on those
    used in Llama-3-8B (top row) and Llama-3-70B (bottom row) models. For each M-N-K
    shape tuple, we generate three random sets of data, run each kernel on the data
    100 times, and average. While our main comparisons are against other LUT kernels
    (bitsandbytes, BitBLAS-NF4), for reference we also include comparisons with kernels
    that only support uniform (integer) dequantization (Marlin, BitBLAS). These results
    are represented with dashed lines in our figures.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在标准 W4G128 设置下 FLUTE 的运行时性能，其中权重被量化为 4 位，分为 128 组。我们展示了与 16 位 torch.mm 的加速比。我们的基准测试矩阵形状基于
    Llama-3-8B（上排）和 Llama-3-70B（下排）模型中的使用情况。对于每个 M-N-K 形状元组，我们生成三个随机数据集，对数据运行每个内核
    100 次，并计算平均值。虽然我们的主要比较对象是其他 LUT 内核（bitsandbytes，BitBLAS-NF4），但为了参考，我们还包括了仅支持均匀（整数）去量化的内核（Marlin，BitBLAS）。这些结果在我们的图中以虚线表示。
- en: 3.3 Stream-K Workload Partitioning
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 Stream-K 工作负载分区
- en: 'For high SM occupancy, standard matmul implementations block the computation
    using a data-parallel tiling of the output matrix, where a group of threads (
    “thread block”) is assigned to compute the work on one output tile. This is shown
    on the left of Figure [2](#S3.F2 "Figure 2 ‣ 3 FLUTE: A Fast and Flexible Kernel
    for Mixed-Type Matrix Multiplications ‣ Fast Matrix Multiplications for Lookup
    Table-Quantized LLMs"). As each thread block can only occupy one SM, it is important
    to avoid “wave quantization”, which happens when the number of output tiles is
    not an even multiple of the number of processor cores. In this case the last wave
    uses only a subset of the cores, leaving the rest idle.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高 SM 占用率，标准的矩阵乘法实现通过对输出矩阵进行数据并行分块来阻塞计算，其中一组线程（“线程块”）被分配来处理一个输出块。这在图[2](#S3.F2
    "图 2 ‣ 3 FLUTE：一种快速且灵活的混合类型矩阵乘法内核 ‣ 针对查找表量化 LLM 的快速矩阵乘法")的左侧展示。由于每个线程块只能占用一个 SM，避免“波量化”非常重要，波量化发生在输出块的数量不是处理器核心数量的偶数倍时。在这种情况下，最后一个波只使用了核心的一个子集，其余核心保持空闲。
- en: 'Wave quantization and workload imbalance are especially problematic in low-bit
    and low-batch scenarios, which result in smaller input matrices (activations and
    quantized weights), thus making the effect of wave quantization more pronounced.
    To mitigate this, we implement a method known as Stream-K workload decomposition Osama
    et al. ([2023](#bib.bib20)), which distributes the tiles such that each SM’s computations
    can span beyond specific rows or columns. This method is depicted on in Fig. [2](#S3.F2
    "Figure 2 ‣ 3 FLUTE: A Fast and Flexible Kernel for Mixed-Type Matrix Multiplications
    ‣ Fast Matrix Multiplications for Lookup Table-Quantized LLMs") (Right). Here,
    the 35 M-N-K tiles are more evenly divided among the 3 SMs than in the simpler
    Slice-K partitioning (Figure [2](#S3.F2 "Figure 2 ‣ 3 FLUTE: A Fast and Flexible
    Kernel for Mixed-Type Matrix Multiplications ‣ Fast Matrix Multiplications for
    Lookup Table-Quantized LLMs"), middle), in which SM’s computations do not span
    beyond rows/columns.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 波量化和工作负载不平衡在低位宽和低批量场景中尤其成问题，这会导致输入矩阵（激活和量化权重）较小，从而使波量化效应更加明显。为了缓解这一问题，我们实施了一种被称为
    Stream-K 工作负载分解的方法 Osama et al. ([2023](#bib.bib20))，该方法将块分配，使得每个 SM 的计算可以跨越特定的行或列。这种方法在图[2](#S3.F2
    "图 2 ‣ 3 FLUTE：一种快速且灵活的混合类型矩阵乘法内核 ‣ 针对查找表量化 LLM 的快速矩阵乘法")（右侧）中进行了描述。在这里，35 个 M-N-K
    块在 3 个 SM 之间分配得更均匀，而不是在更简单的 Slice-K 划分中（图[2](#S3.F2 "图 2 ‣ 3 FLUTE：一种快速且灵活的混合类型矩阵乘法内核
    ‣ 针对查找表量化 LLM 的快速矩阵乘法")，中间），其中 SM 的计算不会跨越行/列。
- en: Mixed precision accumulation and global reduction.
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 混合精度累加和全局归约。
- en: In Stream-K, when multiple SMs compute the same M-N dimension across different
    K tiles, they must reconcile their partial sums in off-chip global memory. SMs
    that complete their share of K tiles write their partial sums to a global scratch
    space, allowing subsequent SMs to read, reduce, and write back these sums. For
    numerical stability, most kernels perform multiplications in FP16 but accumulate
    results in FP32. However, writing to global memory in FP32 results in significant
    traffic. We thus implement in-register accumulation in FP32 and globally reduce
    partial sums in FP16.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Stream-K 中，当多个 SM 在不同的 K 块上计算相同的 M-N 维度时，它们必须在芯片外的全局内存中协调它们的部分和。完成其 K 块任务的
    SM 将其部分和写入全局临时空间，使后续的 SM 可以读取、归约并写回这些和。为了数值稳定性，大多数内核在 FP16 中执行乘法，但在 FP32 中累加结果。然而，以
    FP32 写入全局内存会导致大量流量。因此，我们在 FP32 中实现寄存器内累加，并在 FP16 中全局归约部分和。
- en: 4 Experiments
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 个实验
- en: '![Refer to caption](img/4fd0d5cf63ca0581c2dec5a1a7d3f249.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4fd0d5cf63ca0581c2dec5a1a7d3f249.png)'
- en: 'Figure 4: Runtime performance at various bit-widths and group sizes with N=K=8192.
    FLUTE consistently achieves speedups across different settings, including the
    in the 3-bit configuration.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：在不同的位宽和组大小下，N=K=8192 的运行时性能。FLUTE 在不同设置下始终实现加速，包括 3 位配置。
- en: 'Our experiments consist of two settings: *kernel-level* experiments which compare
    FLUTE matmuls standalone against existing mixed-input matmul kernels (§[4.1](#S4.SS1
    "4.1 Kernel Benchmarks ‣ 4 Experiments ‣ Fast Matrix Multiplications for Lookup
    Table-Quantized LLMs")), and *end-to-end* experiments which assess whether practicals
    speed-ups are obtainable on realistic LLM workloads (§[4.2](#S4.SS2 "4.2 End-to-End
    LLM Benchmarks ‣ 4 Experiments ‣ Fast Matrix Multiplications for Lookup Table-Quantized
    LLMs")).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验包括两种设置：*内核级*实验，将FLUTE matmuls与现有的混合输入matmul内核独立比较（§[4.1](#S4.SS1 "4.1 Kernel
    Benchmarks ‣ 4 Experiments ‣ Fast Matrix Multiplications for Lookup Table-Quantized
    LLMs")），以及*端到端*实验，评估在现实LLM工作负载中是否能够获得实际的加速（§[4.2](#S4.SS2 "4.2 End-to-End LLM
    Benchmarks ‣ 4 Experiments ‣ Fast Matrix Multiplications for Lookup Table-Quantized
    LLMs")）。
- en: 4.1 Kernel Benchmarks
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 内核基准测试
- en: For each matrix size, we compile multiple instantiations of the kernel with
    various configurations, including different tile sizes, pipeline stages, and the
    number of lookup table duplicates, selecting the best-performing configuration
    based on benchmarking.¹¹1Concretely, we randomly generate three sets of input
    data based on the matrix shapes for 8B/70B LLMs, run the kernel on the data 100
    times, and report the average performance on both A100 and A6000 GPUs. We compare
    FLUTE against a collection of weight-quantized matrix multiplication kernels,
    including those capable of flexible LUT-based dequantization such as bitsandbytes Dettmers
    et al. ([2023](#bib.bib5))²²2For most of the kernels, we pre-allocate the output
    memory buffer and use the out keyword to exclude the memory allocation time from
    our measurements. However, as of this writing, bitsandbytes still allocates memory
    in some cases. Our preliminary experiments indicate that this introduces an overhead
    of approximately $2.5\%$. and BitBLAS Wang et al. ([2024](#bib.bib24)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每种矩阵大小，我们编译了多种内核实例，具有不同的配置，包括不同的块大小、流水线阶段和查找表重复次数，并根据基准测试选择最佳性能的配置。¹¹1具体来说，我们根据8B/70B
    LLMs的矩阵形状随机生成三组输入数据，运行内核100次，并报告在A100和A6000 GPU上的平均性能。我们将FLUTE与一系列权重量化矩阵乘法内核进行比较，包括能够进行灵活LUT基础去量化的内核，如bitsandbytes
    Dettmers et al. ([2023](#bib.bib5))²²2对于大多数内核，我们预分配输出内存缓冲区，并使用out关键字将内存分配时间排除在测量之外。然而，截至撰写本文时，bitsandbytes在某些情况下仍会分配内存。我们的初步实验表明，这会引入大约$2.5\%$的开销。以及BitBLAS
    Wang et al. ([2024](#bib.bib24))。
- en: LUT quantization method.
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LUT量化方法。
- en: There are many methods for LUT quantization; we follow the popular NormalFloat
    LUT quantization scheme Dettmers et al. ([2023](#bib.bib5)), where a tensor-level
    table $\mathbf{T}$, and thus incur almost the same memory overhead as uniform
    quantization (which also requires maintaining the group-level scalars). While
    we primarily focus on LUT kernels, for completeness we also compare against high-performance
    kernels specialized for uniformly quantized weights (BitBLAS³³3[https://github.com/microsoft/BitBLAS](https://github.com/microsoft/BitBLAS)
    and Marlin⁴⁴4[https://github.com/IST-DASLab/marlin](https://github.com/IST-DASLab/marlin)).
    These kernels do not require dynamic indexing into a lookup table and can perform
    dequantization in registers using highly tuned PTX assembly instructions that
    are not applicable to LUT-based dequantization.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: LUT量化有很多方法；我们遵循流行的NormalFloat LUT量化方案 Dettmers et al. ([2023](#bib.bib5))，其中一个张量级表$\mathbf{T}$，因此几乎会产生与均匀量化相同的内存开销（均匀量化也需要维护组级标量）。虽然我们主要关注LUT内核，但为了完整性，我们也与专门针对均匀量化权重的高性能内核进行比较（BitBLAS³³3[https://github.com/microsoft/BitBLAS](https://github.com/microsoft/BitBLAS)和Marlin⁴⁴4[https://github.com/IST-DASLab/marlin](https://github.com/IST-DASLab/marlin)）。这些内核不需要动态索引到查找表中，并且可以使用高度优化的PTX汇编指令在寄存器中执行去量化，这些指令不适用于LUT基础的去量化。
- en: Results.
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果。
- en: 'Figure [3](#S3.F3 "Figure 3 ‣ Reducing bank conflicts. ‣ 3.2 Vectorized Lookup
    in Shared Memory ‣ 3 FLUTE: A Fast and Flexible Kernel for Mixed-Type Matrix Multiplications
    ‣ Fast Matrix Multiplications for Lookup Table-Quantized LLMs") presents the results
    with the standard setting of 4-bit quantization and a group size of 128, where
    memory traffic is reduced by 4x (modulo the overhead coming from scales). FLUTE
    achieves favorable performance across a wide range of matrix shapes on both A6000
    and A100, occasionally nearing the peak theoretical speedup (of 4x) on A6000\.
    Other LUT-compatible kernels achieve similar speedups only with a batch size of
    $1$, and their performance quickly degrades. FLUTE also compares favorably to
    Marlin, which is highly specialized for cases where the input is FP16 and the
    weight is uniform-quantized to INT4.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [3](#S3.F3 "Figure 3 ‣ Reducing bank conflicts. ‣ 3.2 Vectorized Lookup in
    Shared Memory ‣ 3 FLUTE: A Fast and Flexible Kernel for Mixed-Type Matrix Multiplications
    ‣ Fast Matrix Multiplications for Lookup Table-Quantized LLMs") 展示了在4位量化和128的组大小的标准设置下的结果，其中内存流量减少了4倍（不包括来自尺度的开销）。FLUTE
    在 A6000 和 A100 上对各种矩阵形状都取得了良好的性能，偶尔在 A6000 上接近理论上的最高加速比（4倍）。其他与 LUT 兼容的内核仅在批量大小为$1$时实现类似的加速，而且它们的性能迅速下降。FLUTE
    还与 Marlin 进行了有利的比较，Marlin 专门针对输入为 FP16 且权重均匀量化为 INT4 的情况。'
- en: 'We further showcase the flexibility of FLUTE by experimenting with different
    group sizes not just in terms of its lookup-table design but also in supporting
    various bit-widths and group sizes. In particular, FLUTE can perform multiplications
    with 3-bit matrices (§[3.1](#S3.SS1 "3.1 Offline Matrix Restructuring ‣ 3 FLUTE:
    A Fast and Flexible Kernel for Mixed-Type Matrix Multiplications ‣ Fast Matrix
    Multiplications for Lookup Table-Quantized LLMs")), a capability that the aforementioned
    alternatives do not support. The results in Figure [4](#S4.F4 "Figure 4 ‣ 4 Experiments
    ‣ Fast Matrix Multiplications for Lookup Table-Quantized LLMs") demonstrate consistent
    speed-ups over torch.mm across across a wide rage of settings.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步展示了 FLUTE 的灵活性，通过实验不同的组大小，不仅在其查找表设计方面，还支持各种位宽和组大小。特别是，FLUTE 可以处理 3 位矩阵
    (§[3.1](#S3.SS1 "3.1 Offline Matrix Restructuring ‣ 3 FLUTE: A Fast and Flexible
    Kernel for Mixed-Type Matrix Multiplications ‣ Fast Matrix Multiplications for
    Lookup Table-Quantized LLMs"))，这是上述替代方法不支持的能力。图 [4](#S4.F4 "Figure 4 ‣ 4 Experiments
    ‣ Fast Matrix Multiplications for Lookup Table-Quantized LLMs") 中的结果展示了在广泛设置下相对于
    torch.mm 的一致加速。'
- en: 4.2 End-to-End LLM Benchmarks
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 端到端 LLM 基准测试
- en: As an application of FLUTE, we experiment with quantizing LLaMA3-8B and LLaMA3-70B.
    The LLaMA3 family of models has been found to be more difficult to quantize than
    other open source models Huang et al. ([2024](#bib.bib11)), and thus presents
    a testing ground for different quantization strategies.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 FLUTE 的应用，我们尝试对 LLaMA3-8B 和 LLaMA3-70B 进行量化。研究发现，LLaMA3 系列模型比其他开源模型更难以量化
    Huang 等 ([2024](#bib.bib11))，因此为不同的量化策略提供了测试平台。
- en: For the LUT quantization method, we use a simple extension of NormalFloat (NF)
    quantization Dettmers et al. ([2023](#bib.bib5)). Standard NF quantization calculates
    $2^{b-1}$.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 LUT 量化方法，我们使用了 NormalFloat (NF) 量化的简单扩展 Dettmers 等 ([2023](#bib.bib5))。标准
    NF 量化计算 $2^{b-1}$。
- en: Our simple extension builds upon the above by using calibration data to refine
    the scales, which has been found to be beneficial for uniform quantization Shao
    et al. ([2023](#bib.bib22)). Since the lookup table consists of quantiles from
    $\mathcal{N}\left(0,\sigma^{2}\right)$ as the new scale, and hence the number
    of scalar values to be loaded for dequantization remains unchanged. We use use
    128 examples of length 2048 from WikiText-2 training as our calibration dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的简单扩展在上述基础上，通过使用校准数据来细化尺度，这已被发现对均匀量化有益 Shao 等 ([2023](#bib.bib22))。由于查找表由$\mathcal{N}\left(0,\sigma^{2}\right)$的分位数组成作为新的尺度，因此用于去量化的标量值数量保持不变。我们使用了来自
    WikiText-2 训练的128个长度为2048的示例作为我们的校准数据集。
- en: '| Model | Configuration | Perplexity | Tokens / Second |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 配置 | 困惑度 | 令牌 / 秒 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | Bits | Group | Bits / Param | GB | WikiText2 | C4 | 1xA6000 | 4xA6000
    | 1xA100 | 2xA100 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | 位数 | 组 | 位数 / 参数 | GB | WikiText2 | C4 | 1xA6000 | 4xA6000 | 1xA100 |
    2xA100 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-3 8B | 16 | N/A | 16.00 | 15.1 | 6.1 | 9.2 | 44.8 | 1.0x |  |  | 90.2
    | 1.0x |  |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3 8B | 16 | 不适用 | 16.00 | 15.1 | 6.1 | 9.2 | 44.8 | 1.0x |  |  | 90.2
    | 1.0x |  |  |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
- en: '|  | 4 | 32 | 4.50 | 5.7 | 6.1 | 9.4 | 91.3 | 2.0x |  |  | 113.7 | 1.3x |  |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | 4 | 32 | 4.50 | 5.7 | 6.1 | 9.4 | 91.3 | 2.0x |  |  | 113.7 | 1.3x |  |  |'
- en: '|  | 4 | 64 | 4.25 | 5.5 | 6.1 | 9.4 | 95.9 | 2.1x |  |  | 119.4 | 1.3x |  |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | 4 | 64 | 4.25 | 5.5 | 6.1 | 9.4 | 95.9 | 2.1x |  |  | 119.4 | 1.3x |  |  |'
- en: '|  | 4 | 128 | 4.13 | 5.4 | 6.2 | 9.5 | 98.1 | 2.2x |  |  | 121.6 | 1.3x |  |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | 4 | 128 | 4.13 | 5.4 | 6.2 | 9.5 | 98.1 | 2.2x |  |  | 121.6 | 1.3x |  |  |'
- en: '|  | 4 | 256 | 4.06 | 5.4 | 6.3 | 9.5 | 99.8 | 2.2x | - | 121.7 | 1.3x | -
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | 4 | 256 | 4.06 | 5.4 | 6.3 | 9.5 | 99.8 | 2.2x | - | 121.7 | 1.3x | -
    |'
- en: '|  | 3 | 32 | 3.50 | 4.9 | 6.9 | 11.0 | 91.9 | 2.1x |  |  | 117.7 | 1.3x |  |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | 3 | 32 | 3.50 | 4.9 | 6.9 | 11.0 | 91.9 | 2.1x |  |  | 117.7 | 1.3x |  |  |'
- en: '|  | 3 | 64 | 3.25 | 4.7 | 7.2 | 11.3 | 104.1 | 2.3x |  |  | 128.5 | 1.4x |  |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | 3 | 64 | 3.25 | 4.7 | 7.2 | 11.3 | 104.1 | 2.3x |  |  | 128.5 | 1.4x |  |  |'
- en: '|  | 3 | 128 | 3.13 | 4.6 | 7.5 | 11.7 | 108.1 | 2.4x |  |  | 133.5 | 1.5x
    |  |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | 3 | 128 | 3.13 | 4.6 | 7.5 | 11.7 | 108.1 | 2.4x |  |  | 133.5 | 1.5x
    |  |  |'
- en: '|  | 3 | 256 | 3.06 | 4.6 | 7.9 | 12.2 | 110.0 | 2.5x |  |  | 135.5 | 1.5x
    |  |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | 3 | 256 | 3.06 | 4.6 | 7.9 | 12.2 | 110.0 | 2.5x |  |  | 135.5 | 1.5x
    |  |  |'
- en: '| LLaMA-3 70B | 16 | N/A | 16.00 | 131.7 | 2.9 | 6.9 | OOM | OOM | 17.2 | 1.0x
    | OOM | OOM | 19.9 | 1.0x |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3 70B | 16 | N/A | 16.00 | 131.7 | 2.9 | 6.9 | OOM | OOM | 17.2 | 1.0x
    | OOM | OOM | 19.9 | 1.0x |'
- en: '|  | 4 | 32 | 4.50 | 40.1 | 3.0 | 7.0 | 12.6 | - | 33.0 | 1.9x | 17.4 | - |
    28.3 | 1.4x |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | 4 | 32 | 4.50 | 40.1 | 3.0 | 7.0 | 12.6 | - | 33.0 | 1.9x | 17.4 | - |
    28.3 | 1.4x |'
- en: '|  | 4 | 64 | 4.25 | 38.1 | 3.0 | 7.1 | 13.5 | - | 33.1 | 1.9x | 18.0 | - |
    29.5 | 1.5x |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | 4 | 64 | 4.25 | 38.1 | 3.0 | 7.1 | 13.5 | - | 33.1 | 1.9x | 18.0 | - |
    29.5 | 1.5x |'
- en: '|  | 4 | 128 | 4.13 | 37.1 | 3.1 | 7.2 | 14.7 | - | 33.1 | 1.9x | 18.6 | -
    | 30.3 | 1.5x |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | 4 | 128 | 4.13 | 37.1 | 3.1 | 7.2 | 14.7 | - | 33.1 | 1.9x | 18.6 | -
    | 30.3 | 1.5x |'
- en: '|  | 4 | 256 | 4.06 | 36.6 | 3.5 | 7.8 | 15.2 | - | 32.9 | 1.9x | 19.0 | -
    | 31.0 | 1.6x |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | 4 | 256 | 4.06 | 36.6 | 3.5 | 7.8 | 15.2 | - | 32.9 | 1.9x | 19.0 | -
    | 31.0 | 1.6x |'
- en: '|  | 3 | 32 | 3.50 | 32.1 | 3.9 | 8.0 | 13.3 | - | 32.8 | 1.9x | 20.0 | - |
    30.8 | 1.5x |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | 3 | 32 | 3.50 | 32.1 | 3.9 | 8.0 | 13.3 | - | 32.8 | 1.9x | 20.0 | - |
    30.8 | 1.5x |'
- en: '|  | 3 | 64 | 3.25 | 30.1 | 4.1 | 8.4 | 16.3 | - | 33.3 | 1.9x | 22.4 | - |
    33.8 | 1.7x |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | 3 | 64 | 3.25 | 30.1 | 4.1 | 8.4 | 16.3 | - | 33.3 | 1.9x | 22.4 | - |
    33.8 | 1.7x |'
- en: '|  | 3 | 128 | 3.13 | 29.1 | 5.2 | 10.1 | 17.7 | - | 32.7 | 1.9x | 23.9 | -
    | 34.5 | 1.7x |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | 3 | 128 | 3.13 | 29.1 | 5.2 | 10.1 | 17.7 | - | 32.7 | 1.9x | 23.9 | -
    | 34.5 | 1.7x |'
- en: '|  | 3 | 256 | 3.06 | 28.6 | 15.4 | 26.4 | 18.6 | - | 33.6 | 2.0x | 24.9 |
    - | 34.8 | 1.7x |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | 3 | 256 | 3.06 | 28.6 | 15.4 | 26.4 | 18.6 | - | 33.6 | 2.0x | 24.9 |
    - | 34.8 | 1.7x |'
- en: 'Table 1: Perplexity and decoding speed of LLaMA-3 with learned NF quantization
    using various quantization configurations. Decoding speedup is measured in tokens
    per second. The unquantized LLaMA-3 70B model requires multiple GPUs with Tensor
    Parallelism. Therefore, we report the speed with one GPU, and with Tensor Parallelism
    applied (labeled as x4 and x2). For the 8B models, since all models fit into one
    GPU, we report only single GPU results.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：LLaMA-3在使用不同量化配置的学习NF量化时的困惑度和解码速度。解码速度以每秒令牌数表示。未经量化的LLaMA-3 70B模型需要多个GPU和张量并行。因此，我们报告了使用一个GPU的速度，并且应用了张量并行（标记为x4和x2）。对于8B模型，由于所有模型适配一个GPU，我们仅报告单个GPU的结果。
- en: 'We conducted end-to-end evaluations by integrating the FLUTE kernels into two
    libraries:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将FLUTE核心集成到两个库中进行了端到端的评估：
- en: '1.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: GPT-Fast⁶⁶6[https://github.com/pytorch-labs/gpt-fast](https://github.com/pytorch-labs/gpt-fast)
    is a simple yet performant PyTorch-native implementation for transformer text
    generation. We follow most of its default settings, running benchmarks with a
    batch size of 1.⁷⁷7This configuration makes the reported tokens per second (tokens/s)
    equivalent to “tokens/s/user.” We set the prompt length to just 1, focusing our
    measurements on the decoding step in text generation rather than the prefill stage.
    We also do not use CUDA Graphs due to its incompatibility with FLUTE. We additionally
    use torch.compile to optimize the model, which, in early experiments, nearly tripled
    the throughput of the 16-bit unquantized model.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-Fast⁶⁶6[https://github.com/pytorch-labs/gpt-fast](https://github.com/pytorch-labs/gpt-fast)
    是一个简单但高效的PyTorch原生实现，用于变换器文本生成。我们遵循了大部分默认设置，运行批量大小为1的基准测试。⁷⁷7这一配置使报告的每秒令牌数（tokens/s）等同于“每用户令牌数/秒”。我们将提示长度设置为1，专注于文本生成中的解码步骤，而不是预填充阶段。由于与FLUTE不兼容，我们也未使用CUDA图。此外，我们使用torch.compile来优化模型，这在早期实验中几乎将16位未经量化模型的吞吐量提高了三倍。
- en: '2.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: vLLM Kwon et al. ([2023](#bib.bib13)) is a high-throughput and memory-efficient
    inference and serving engine for LLMs widely used in practice. We benchmarked
    the latency of processing a single batch of requests, following most of its default
    settings, but varied the input length, output length, and batch size to assess
    performance under different conditions.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: vLLM Kwon et al. ([2023](#bib.bib13)) 是一个高吞吐量和内存高效的推理与服务引擎，在实际应用中被广泛使用。我们基于大多数默认设置对单批请求的延迟进行了基准测试，但改变了输入长度、输出长度和批量大小，以评估在不同条件下的性能。
- en: For the 70B model, the unquantized model does not fit into a single GPU. Consequently,
    we apply tensor parallelism across 4xA6000 or 2xA100 GPUs. Since the quantized
    model fits into a single GPU, we report two sets of numbers (single- and multi-GPUs)
    to represent different use cases.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 70B 模型，未量化模型无法适配到单个 GPU。因此，我们在 4xA6000 或 2xA100 GPU 上应用张量并行。由于量化模型适配到单个 GPU，我们报告了两组数字（单
    GPU 和多 GPU）以代表不同的使用案例。
- en: '| #P | Method | Wiki PPL $\downarrow$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| #P | 方法 | Wiki PPL $\downarrow$ |'
- en: '| 4-bit | 3-bit | 4-bit | 3-bit | 4-bit | 3-bit |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 4-bit | 3-bit | 4-bit | 3-bit | 4-bit | 3-bit |'
- en: '| 8B | Unquantized | 6.1 | 9.2 | 68.6 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 8B | 未量化 | 6.1 | 9.2 | 68.6 |'
- en: '|  | RTN | 8.5 | 27.9 | 13.4 | 1.1E2 | 63.9 | 40.2 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN | 8.5 | 27.9 | 13.4 | 1.1E2 | 63.9 | 40.2 |'
- en: '|  | GPTQ | 6.5 | 8.2 | 10.4 | 13.7 | 67.3 | 61.7 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 6.5 | 8.2 | 10.4 | 13.7 | 67.3 | 61.7 |'
- en: '|  | AWQ | 6.6 | 8.2 | 9.4 | 11.6 | 68.2 | 64.4 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | AWQ | 6.6 | 8.2 | 9.4 | 11.6 | 68.2 | 64.4 |'
- en: '|  | OmniQuant | 6.6 | 8.4 | 10.1 | 13.5 | 68.3 | 62.4 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | OmniQuant | 6.6 | 8.4 | 10.1 | 13.5 | 68.3 | 62.4 |'
- en: '|  | NF | 6.6 | 9.2 | 9.5 | 13.0 | 68.0 | 62.3 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | NF | 6.6 | 9.2 | 9.5 | 13.0 | 68.0 | 62.3 |'
- en: '|  | NF + AWQ | 6.5 | 8.0 | 9.3 | 11.5 | 67.8 | 65.1 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | NF + AWQ | 6.5 | 8.0 | 9.3 | 11.5 | 67.8 | 65.1 |'
- en: '|  | NF (learned) | 6.2 | 7.5 | 9.5 | 11.7 | 67.9 | 63.7 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | NF (学习) | 6.2 | 7.5 | 9.5 | 11.7 | 67.9 | 63.7 |'
- en: '| 70B | Unquantized | 2.9 | 6.9 | 75.3 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 70B | 未量化 | 2.9 | 6.9 | 75.3 |'
- en: '|  | RTN | 3.6 | 11.8 | 8.9 | 22.0 | 74.3 | 48.0 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN | 3.6 | 11.8 | 8.9 | 22.0 | 74.3 | 48.0 |'
- en: '|  | GPTQ | 3.3 | 5.2 | 6.9 | 10.5 | 74.9 | 70.6 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 3.3 | 5.2 | 6.9 | 10.5 | 74.9 | 70.6 |'
- en: '|  | AWQ | 3.3 | 4.8 | 7.0 | 8.0 | 74.9 | 73.2 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | AWQ | 3.3 | 4.8 | 7.0 | 8.0 | 74.9 | 73.2 |'
- en: '|  | OmniQuant | 3.3 | 5.4 | 7.5 | 9.3 | 74.2 | 70.2 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | OmniQuant | 3.3 | 5.4 | 7.5 | 9.3 | 74.2 | 70.2 |'
- en: '|  | NF | 3.4 | 8.7 | 7.6 | 16.7 | 74.0 | 64.3 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | NF | 3.4 | 8.7 | 7.6 | 16.7 | 74.0 | 64.3 |'
- en: '|  | NF + AWQ | 3.2 | 4.6 | 6.9 | 7.8 | 75.2 | 73.8 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | NF + AWQ | 3.2 | 4.6 | 6.9 | 7.8 | 75.2 | 73.8 |'
- en: '|  | NF (learned) | 3.1 | 5.2 | 7.2 | 10.1 | 74.4 | 66.4 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | NF (学习) | 3.1 | 5.2 | 7.2 | 10.1 | 74.4 | 66.4 |'
- en: 'Table 2: Evaluation of post-training quantization on LLaMA3-8B and LLaMA3-70B.
    The RTN, GPTQ Frantar et al. ([2022](#bib.bib8)), AWQ Lin et al. ([2023](#bib.bib14))
    results are from Huang et al. ([2024](#bib.bib11)); the rest are from our implementations.
    All non-NF methods use uniform weight quantization.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：LLaMA3-8B 和 LLaMA3-70B 上训练后量化的评估。RTN、GPTQ Frantar et al. ([2022](#bib.bib8))、AWQ
    Lin et al. ([2023](#bib.bib14)) 的结果来自 Huang et al. ([2024](#bib.bib11)); 其余结果来自我们自己的实现。所有非
    NF 方法使用均匀权重量化。
- en: '![Refer to caption](img/820747982dc1f621671b98ada6f6f7c8.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/820747982dc1f621671b98ada6f6f7c8.png)'
- en: 'Figure 5: End-to-end latency benchmark for processing a single batch of requests
    using vLLM. We evaluated LLaMA-3 (8B and 70B) and Gemma-2 (9B and 27B) models
    with various configurations, including different bits, model sizes, number of
    GPUs, input lengths, output lengths, and batch sizes. The models were quantized
    using a group size of 64 to achieve a good balance between quality and speed.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：使用 vLLM 处理单批请求的端到端延迟基准测试。我们评估了各种配置下的 LLaMA-3 (8B 和 70B) 以及 Gemma-2 (9B 和
    27B) 模型，包括不同的位数、模型大小、GPU 数量、输入长度、输出长度和批量大小。模型使用 64 的组大小进行量化，以实现质量和速度之间的良好平衡。
- en: Results.
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果。
- en: We first compare our “learned NF quantization” approach against standard 4-
    and 3-bit setting with group size 128 against other quantization methods. The
    results are shown in Table [2](#S4.T2 "Table 2 ‣ 4.2 End-to-End LLM Benchmarks
    ‣ 4 Experiments ‣ Fast Matrix Multiplications for Lookup Table-Quantized LLMs"),
    where we find that this variant of LUT quantization improves upon ordinary NF
    quantization and compares favorably against existing baselines. See Tables [3](#A1.T3
    "Table 3 ‣ Appendix A Appendix ‣ Fast Matrix Multiplications for Lookup Table-Quantized
    LLMs") and [4](#A1.T4 "Table 4 ‣ Appendix A Appendix ‣ Fast Matrix Multiplications
    for Lookup Table-Quantized LLMs") of the appendix for the full results. We also
    find that combining NF with AWQ Lin et al. ([2023](#bib.bib14)) to be beneficial,
    although a learned NF+AWQ did not help. (However we emphasize that the quantization
    method itself is not the main contribution of the present work.) We next exploit
    the flexibility of FLUTE and conduct end-to-end experiments with various bit-
    and group-size settings. This is shown in Table [1](#S4.T1 "Table 1 ‣ 4.2 End-to-End
    LLM Benchmarks ‣ 4 Experiments ‣ Fast Matrix Multiplications for Lookup Table-Quantized
    LLMs"). With small enough group sizes, our approach is able to almost approach
    the 16 bit baseline in terms of WikiText2 perplexity.⁸⁸8Note that the WikiText-2
    validation data is different from the calibration data. We are able to observe
    meaningful speedups even in the end-to-end case over an optimized baseline.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将我们的“学习的NF量化”方法与标准的4位和3位设置（组大小为128）以及其他量化方法进行了比较。结果如表Table [2](#S4.T2 "Table
    2 ‣ 4.2 End-to-End LLM Benchmarks ‣ 4 Experiments ‣ Fast Matrix Multiplications
    for Lookup Table-Quantized LLMs")所示，我们发现这种LUT量化的变体相较于普通NF量化有所改进，并且与现有基准相比表现良好。有关完整结果，请参见附录的表Table
    [3](#A1.T3 "Table 3 ‣ Appendix A Appendix ‣ Fast Matrix Multiplications for Lookup
    Table-Quantized LLMs")和Table [4](#A1.T4 "Table 4 ‣ Appendix A Appendix ‣ Fast
    Matrix Multiplications for Lookup Table-Quantized LLMs")。我们还发现将NF与AWQ Lin等人（[2023](#bib.bib14)）结合是有益的，尽管学习的NF+AWQ并没有帮助。（然而，我们强调量化方法本身并不是本研究的主要贡献。）接下来，我们利用FLUTE的灵活性，进行各种比特和组大小设置的端到端实验。结果显示在表Table
    [1](#S4.T1 "Table 1 ‣ 4.2 End-to-End LLM Benchmarks ‣ 4 Experiments ‣ Fast Matrix
    Multiplications for Lookup Table-Quantized LLMs")中。在组大小足够小的情况下，我们的方法几乎可以接近16位基线在WikiText2困惑度方面的表现。⁸⁸8请注意，WikiText-2验证数据与校准数据不同。即使在优化的基线之上，我们也能够观察到有意义的加速。
- en: Finally, we evaluated end-to-end latency using the popular model service framework
    vLLM Kwon et al. ([2023](#bib.bib13)). Based on our earlier experiments, we selected
    a group size of 64, which strikes a good balance between quality and speed. We
    conducted experiments across various configurations, including bit precision,
    model sizes, number of GPUs, input lengths, output lengths, and batch sizes. Additionally,
    we conducted experiments with the newly released Gemma-2 models (9B and 27B).
    For the largest open-sourced Gemma-2 27B model, which fits into a 2xA6000 and
    1xA100 setup, we adjusted the tensor parallelism settings accordingly. The results,
    presented in Fig. [5](#S4.F5 "Figure 5 ‣ 4.2 End-to-End LLM Benchmarks ‣ 4 Experiments
    ‣ Fast Matrix Multiplications for Lookup Table-Quantized LLMs"), further showcase
    the end-to-end performance of the kernel.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用流行的模型服务框架vLLM Kwon等人（[2023](#bib.bib13)）评估了端到端的延迟。基于我们之前的实验，我们选择了64的组大小，这在质量和速度之间达到了良好的平衡。我们进行了多种配置的实验，包括比特精度、模型大小、GPU数量、输入长度、输出长度和批量大小。此外，我们还对新发布的Gemma-2模型（9B和27B）进行了实验。对于最大的开源Gemma-2
    27B模型，它适配了2xA6000和1xA100的设置，我们相应地调整了张量并行设置。结果展示在图Fig. [5](#S4.F5 "Figure 5 ‣ 4.2
    End-to-End LLM Benchmarks ‣ 4 Experiments ‣ Fast Matrix Multiplications for Lookup
    Table-Quantized LLMs")中，进一步展示了内核的端到端性能。
- en: 5 Discussion and Conclusion
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论与结论
- en: Early work on LLM quantization generally worked with uniform quantization methods
    Frantar et al. ([2022](#bib.bib8)); Dettmers et al. ([2022](#bib.bib4)); Xiao
    et al. ([2022](#bib.bib29)). More recent work has shown the benefits of LUT-quantization,
    both from PTQ Kim et al. ([2023](#bib.bib12)) and finetuning Dettmers et al. ([2023](#bib.bib5))
    perspectives. Insofar as lookup tables can represent flexible quantization functions,
    our hope is that FLUTE can enable researchers and practitioners to explore new
    quantization algorithms that can learn better lookup tables Yamamoto ([2021](#bib.bib31));
    Cardinaux et al. ([2020](#bib.bib3)); Wang et al. ([2022](#bib.bib25)). On a similar
    note, recent work has found that codebook-based quantization schemes—which generalize
    lookup tables to vector-valued values—can enable even lower-bit (e.g., 2-bit)
    LLM quantization without significant performance degradations Tseng et al. ([2024](#bib.bib23));
    Egiazarian et al. ([2024](#bib.bib6)). We anticipate that ideas from this work
    can aid in developing kernels for such methods.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLM量化的早期工作通常使用统一量化方法 Frantar 等（[2022](#bib.bib8)）；Dettmers 等（[2022](#bib.bib4)）；Xiao
    等（[2022](#bib.bib29)）。更近期的研究表明LUT-量化的好处，包括来自PTQ Kim 等（[2023](#bib.bib12)）和微调 Dettmers
    等（[2023](#bib.bib5)）的视角。由于查找表可以表示灵活的量化函数，我们希望FLUTE能够使研究人员和从业者探索能够学习更好查找表的新量化算法
    Yamamoto（[2021](#bib.bib31)）；Cardinaux 等（[2020](#bib.bib3)）；Wang 等（[2022](#bib.bib25)）。类似地，最近的研究发现基于代码本的量化方案——将查找表推广到向量值——可以实现更低位（例如，2位）的LLM量化，而不会显著降低性能
    Tseng 等（[2024](#bib.bib23)）；Egiazarian 等（[2024](#bib.bib6)）。我们预期这些工作的理念将有助于开发这种方法的内核。
- en: Algorithmic considerations aside, one of the main challenges in developing fused
    quantized matrix multiplication kernels stems from the lack of hardware support
    for “mixed-type” instructions, necessitating software-level implementations. Existing
    Tensor Core instructions support scenarios where the input and output/accumulation
    data have different types (e.g., compute in FP16 and output/accumulate in FP32).
    However, they do not support cases where the input operands themselves are of
    different types (e.g., FP16 inputs and INT4 weights). As weight-only quantization
    becomes increasingly common in LLM inference applications, native support for
    such instructions in future hardware could be beneficial. Additionally, the lack
    of in-register dynamic indexing means that developers must devise software solutions.
    Enhanced hardware acceleration for indexing into small lookup tables could also
    prove beneficial in the upcoming generations of AI accelerator hardware.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 算法方面的问题暂且不谈，开发融合量化矩阵乘法内核的主要挑战之一在于缺乏对“混合类型”指令的硬件支持，这需要在软件层面实现。现有的Tensor Core指令支持输入和输出/累积数据具有不同类型的场景（例如，在FP16中计算，在FP32中输出/累积）。然而，它们不支持输入操作数本身具有不同类型的情况（例如，FP16输入和INT4权重）。随着仅量化权重在LLM推理应用中的普及，未来硬件对这种指令的原生支持将是有益的。此外，缺乏寄存器内动态索引意味着开发人员必须设计软件解决方案。对小型查找表的硬件加速也可能在未来几代AI加速器硬件中证明是有益的。
- en: 6 Conclusion
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This work introduces FLUTE, a CUDA kernel designed for fused quantized matrix
    multiplications to accelerate LLM inference. FLUTE offers flexibility, supporting
    flexible mappings between quantized and dequantized values through a lookup table,
    and accommodating a wide range of bit widths and group sizes. We demonstrate its
    performance through both kernel-level benchmarks and end-to-end evaluations on
    state-of-the-art LLMs.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作介绍了FLUTE，这是一个设计用于加速LLM推理的融合量化矩阵乘法的CUDA内核。FLUTE提供了灵活性，通过查找表支持量化值和解量化值之间的灵活映射，并适应各种位宽和组大小。我们通过内核级基准测试和对最先进的LLM进行的端到端评估展示了其性能。
- en: Limitations
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: FLUTE has several limitations. For one, it is mostly optimized for Ampere-generation
    GPUs, and it does not take advantage of the newer hardware features available
    in subsequent generations, such as Hopper GPUs (H100). However, the majority of
    the methods discussed could still be applicable to the newer hardware. For Ampere
    generation GPUs, the latest tensor cores support performing MMA operations on
    matrix fragments of shape [16,16]x[16,8]. When the batch size is smaller than
    $16$, input data needs to be padded within shared memory. Although this padding
    increases on-chip data movements (between shared memory and registers) and computations,
    it does not increase data movement between off-chip and on-chip memory, allowing
    us to achieve speed-ups in memory-bound cases. In such scenarios, switching to
    SIMT cores could further enhance performance. FLUTE is designed for memory-bound
    scenarios such as LLM decoding. Its performance tends to degrade with larger batch
    sizes, which are more common during training when the workload becomes more compute-bound.
    Finally, while FLUTE demonstrates strong performance among kernels that support
    LUT-based dequantization, its performance on A100s still falls short of the peak
    performance that kernels specialized for uniformly quantized matrices can achieve.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: FLUTE 有若干局限性。一方面，它主要针对 Ampere 代 GPU 进行优化，未能利用后续代硬件（如 Hopper GPU（H100））的更新特性。然而，所讨论的大多数方法仍然可以适用于较新的硬件。对于
    Ampere 代 GPU，最新的张量核心支持在形状为 [16,16]x[16,8] 的矩阵片段上执行 MMA 操作。当批量大小小于 $16$ 时，输入数据需要在共享内存中填充。虽然这种填充增加了片上数据的移动（在共享内存和寄存器之间）和计算，但并不增加片外和片内内存之间的数据移动，从而使我们能够在受内存限制的情况下实现加速。在这种情况下，切换到
    SIMT 核心可能会进一步提升性能。FLUTE 设计用于受内存限制的场景，如 LLM 解码。其性能往往会随着批量大小的增加而下降，较大的批量更常见于训练过程中，当工作负载变得更加计算密集时。最后，尽管
    FLUTE 在支持 LUT 基于的去量化的内核中表现强劲，但在 A100 上的性能仍未达到专门针对均匀量化矩阵的内核所能实现的峰值性能。
- en: Acknowledgements
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Yijie Bei and Dmytro Ivchenko for helpful discussion. HG was supported
    by a Microsoft PhD Fellowship. EX acknowledges the support of NGA HM04762010002,
    NSF IIS1955532, NSF CNS2008248, NIGMS R01GM140467, NSF IIS2123952, NSF DMS2027737,
    NSF BCS2040381, NSF DMS2112273, NSF IIS2311990, Semiconductor Research Corporation
    (SRC) AIHW award 2024AH3210, and DARPA ECOLE HR00112390063\. This study was additionally
    supported by funds from MIT-IBM Watson AI and the MLA@CSAIL initiative.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Yijie Bei 和 Dmytro Ivchenko 的有益讨论。HG 得到了微软博士奖学金的资助。EX 感谢 NGA HM04762010002、NSF
    IIS1955532、NSF CNS2008248、NIGMS R01GM140467、NSF IIS2123952、NSF DMS2027737、NSF
    BCS2040381、NSF DMS2112273、NSF IIS2311990、半导体研究公司（SRC）AIHW 奖 2024AH3210 和 DARPA
    ECOLE HR00112390063 的支持。本研究还得到了 MIT-IBM Watson AI 和 MLA@CSAIL 计划的资金支持。
- en: References
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ashkboos et al. (2023) Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan
    Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. 2023. Towards
    end-to-end 4-bit inference on generative large language models. *arXiv preprint
    arXiv:2310.09259*.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashkboos 等人（2023）Saleh Ashkboos、Ilia Markov、Elias Frantar、Tingxuan Zhong、Xincheng
    Wang、Jie Ren、Torsten Hoefler 和 Dan Alistarh。2023年。面向生成性大型语言模型的端到端4位推理。*arXiv 预印本
    arXiv:2310.09259*。
- en: 'Ashkboos et al. (2024) Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L
    Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman.
    2024. Quarot: Outlier-free 4-bit inference in rotated LLMs. *arXiv preprint arXiv:2404.00456*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashkboos 等人（2024）Saleh Ashkboos、Amirkeivan Mohtashami、Maximilian L Croci、Bo
    Li、Martin Jaggi、Dan Alistarh、Torsten Hoefler 和 James Hensman。2024年。Quarot：在旋转
    LLM 中进行无异常值的 4 位推理。*arXiv 预印本 arXiv:2404.00456*。
- en: Cardinaux et al. (2020) Fabien Cardinaux, Stefan Uhlich, Kazuki Yoshiyama, Javier Alonso
    García, Lukas Mauch, Stephen Tiedemann, Thomas Kemp, and Akira Nakamura. 2020.
    Iteratively training look-up tables for network quantization. *IEEE Journal of
    Selected Topics in Signal Processing*, 14(4):860–870.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cardinaux 等人（2020）Fabien Cardinaux、Stefan Uhlich、Kazuki Yoshiyama、Javier Alonso
    García、Lukas Mauch、Stephen Tiedemann、Thomas Kemp 和 Akira Nakamura。2020年。用于网络量化的查找表的迭代训练。*IEEE
    选定信号处理专题期刊*，14(4):860–870。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. LLM.int8(): 8-bit matrix multiplication for transformers at scale. *Advances
    in Neural Information Processing Systems*, 35:30318–30332.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等人（2022）Tim Dettmers、Mike Lewis、Younes Belkada 和 Luke Zettlemoyer。2022年。LLM.int8()：大规模变换器的
    8 位矩阵乘法。*神经信息处理系统进展*，35:30318–30332。
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. QLoRA: Efficient finetuning of quantized LLMs. *Advances in
    Neural Information Processing Systems*, 36.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers等（2023）蒂姆·德特梅尔斯、阿尔蒂多罗·帕尼奥尼、阿里·霍尔茨曼和卢克·泽特勒莫耶。2023。QLoRA：量化LLM的高效微调。*神经信息处理系统进展*，36。
- en: Egiazarian et al. (2024) Vage Egiazarian, Andrei Panferov, Denis Kuznedelev,
    Elias Frantar, Artem Babenko, and Dan Alistarh. 2024. Extreme compression of large
    language models via additive quantization. *arXiv preprint arXiv:2401.06118*.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Egiazarian等（2024）瓦戈·埃贾扎里安、安德烈·潘费罗夫、丹尼斯·库兹涅杰夫、伊利亚斯·弗兰塔尔、阿尔特姆·巴本科和丹·阿利斯塔赫。2024。通过加法量化对大型语言模型进行极端压缩。*arXiv预印本
    arXiv:2401.06118*。
- en: 'Frantar and Alistarh (2024) Elias Frantar and Dan Alistarh. 2024. Marlin: a
    fast 4-bit inference kernel for medium batchsizes. [https://github.com/IST-DASLab/marlin](https://github.com/IST-DASLab/marlin).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar和Alistarh（2024）伊利亚斯·弗兰塔尔和丹·阿利斯塔赫。2024。Marlin：一种用于中等批次的快速4-bit推理内核。 [https://github.com/IST-DASLab/marlin](https://github.com/IST-DASLab/marlin)。
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2022. GPTQ: Accurate post-training compression for generative pretrained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar等（2022）伊利亚斯·弗兰塔尔、萨利赫·阿什库布斯、托斯滕·霍夫勒和丹·阿利斯塔赫。2022。GPTQ：用于生成预训练变换器的准确后训练压缩。*arXiv预印本
    arXiv:2210.17323*。
- en: Gholami et al. (2024) Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman Hooper,
    Michael W Mahoney, and Kurt Keutzer. 2024. AI and memory wall. *IEEE Micro*.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gholami等（2024）阿米尔·戈拉米、哲维·姚、世勋金、科尔曼·胡珀、迈克尔·W·马霍尼和库尔特·凯茨。2024。AI与内存墙。*IEEE Micro*。
- en: 'Guo et al. (2024) Han Guo, Philip Greengard, Eric P Xing, and Yoon Kim. 2024.
    LQ-LoRA: Low-rank plus quantized matrix decomposition for efficient language model
    finetuning. In *Proceedings of ICLR*.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等（2024）韩国、菲利普·格林加德、埃里克·P·辛和尹金。2024。LQ-LoRA：低秩加量化矩阵分解用于高效的语言模型微调。在*ICLR会议论文集*中。
- en: Huang et al. (2024) Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao
    Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. 2024. How
    good are low-bit quantized LLaMA3 models? an empirical study. *arXiv preprint
    arXiv:2404.14047*.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等（2024）魏黄、许东马、浩桐秦、兴宇郑、成涛吕、洪陈、杰罗、肖娟齐、向龙刘和米歇尔·马格诺。2024。低位量化LLaMA3模型的效果如何？一项实证研究。*arXiv预印本
    arXiv:2404.14047*。
- en: 'Kim et al. (2023) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. 2023. SqueezeLLM: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim等（2023）世勋金、科尔曼·胡珀、阿米尔·戈拉米、震董、秀玉李、盛申、迈克尔·W·马霍尼和库尔特·凯茨。2023。SqueezeLLM：稠密与稀疏量化。*arXiv预印本
    arXiv:2306.07629*。
- en: Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient
    memory management for large language model serving with pagedattention. In *Proceedings
    of the ACM SIGOPS 29th Symposium on Operating Systems Principles*.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon等（2023）宇硕·权、卓焕·李、思源·庄、英盛·谢、连敏·郑、科迪·浩·余、约瑟夫·E·冈萨雷斯、浩·张和伊昂·斯托伊卡。2023。具有分页注意力的大型语言模型服务的高效内存管理。在*ACM
    SIGOPS第29届操作系统原理研讨会论文集*中。
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. 2023. AWQ: Activation-aware weight quantization for LLM compression
    and acceleration. *arXiv preprint arXiv:2306.00978*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2023）吉林、嘉铭唐、浩天唐、尚杨、兴宇邓和宋汉。2023。AWQ：激活感知的权重量化用于LLM压缩和加速。*arXiv预印本 arXiv:2306.00978*。
- en: 'Lin et al. (2024) Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan
    Xiao, Chuang Gan, and Song Han. 2024. QServe: W4A8KV4 quantization and system
    co-design for efficient LLM serving. *arXiv preprint arXiv:2405.04532*.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2024）宇军林、浩天唐、尚杨、哲凯张、光轩肖、创干和宋汉。2024。QServe：W4A8KV4量化和系统协同设计以实现高效的LLM服务。*arXiv预印本
    arXiv:2405.04532*。
- en: 'Ma et al. (2024a) Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang,
    Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei. 2024a. The era
    of 1-bit LLMs: All large language models are in 1.58 bits. *arXiv preprint arXiv:2402.17764*.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma等（2024a）书铭马、鸿宇王、凌霄马、雷王、文辉王、少涵黄、李东、瑞平王、纪龙薛和富如魏。2024a。1-bit LLM的时代：所有大型语言模型均为1.58位。*arXiv预印本
    arXiv:2402.17764*。
- en: 'Ma et al. (2024b) Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao,
    Rui Wang, Shilei Wen, Fei Chao, and Rongrong Ji. 2024b. AffineQuant: Affine transformation
    quantization for large language models. *arXiv preprint arXiv:2403.12544*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma等（2024b）岳晓马、慧霞李、夏吴郑、冯凌、学锋肖、瑞王、石磊温、费超和戎戎吉。2024b。AffineQuant：用于大型语言模型的仿射变换量化。*arXiv预印本
    arXiv:2403.12544*。
- en: Miyashita et al. (2016) Daisuke Miyashita, Edward H Lee, and Boris Murmann.
    2016. Convolutional neural networks using logarithmic data representation. *arXiv
    preprint arXiv:1603.01025*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miyashita et al. (2016) 宫下大辅、爱德华·H·李和鲍里斯·穆尔曼。2016。使用对数数据表示的卷积神经网络。*arXiv预印本
    arXiv:1603.01025*。
- en: Nrusimha et al. (2024) Aniruddha Nrusimha, Mayank Mishra, Naigang Wang, Dan
    Alistarh, Rameswar Panda, and Yoon Kim. 2024. Mitigating the impact of outlier
    channels for language model quantization with activation regularization. *arXiv
    preprint arXiv:2404.03605*.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nrusimha et al. (2024) 阿尼鲁德·努鲁西马、玛扬克·米什拉、王乃刚、丹·阿利斯塔赫、拉梅斯瓦尔·潘达和金允。2024。通过激活正则化减轻异常通道对语言模型量化的影响。*arXiv预印本
    arXiv:2404.03605*。
- en: 'Osama et al. (2023) Muhammad Osama, Duane Merrill, Cris Cecka, Michael Garland,
    and John D Owens. 2023. Stream-K: Work-centric parallel decomposition for dense
    matrix-matrix multiplication on the GPU. In *Proceedings of the 28th ACM SIGPLAN
    Annual Symposium on Principles and Practice of Parallel Programming*, pages 429–431.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osama et al. (2023) 穆罕默德·奥萨马、杜安·梅里尔、克里斯·切卡、迈克尔·加拉德和约翰·D·欧文斯。2023。Stream-K：基于工作中心的并行分解，用于GPU上的密集矩阵-矩阵乘法。在
    *第28届ACM SIGPLAN年会，关于并行编程的原则和实践* 中，第429–431页。
- en: 'Park et al. (2022) Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon
    Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee.
    2022. LUT-GEMM: Quantized matrix multiplication based on luts for efficient inference
    in large-scale generative language models. *arXiv preprint arXiv:2206.09557*.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. (2022) 朴根浩、朴培成、金民洙、李成宰、金正勋、权범硕、权世正、金炳旭、李永周和李东秀。2022。LUT-GEMM：基于查找表的量化矩阵乘法，用于大规模生成语言模型的高效推理。*arXiv预印本
    arXiv:2206.09557*。
- en: 'Shao et al. (2023) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 2023. OmniQuant:
    Omnidirectionally calibrated quantization for large language models. *arXiv preprint
    arXiv:2308.13137*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao et al. (2023) 邵文奇、陈梦钊、张兆阳、许鹏、赵丽瑞、李志倩、张凯鹏、高鹏、乔雨和罗平。2023。OmniQuant：用于大型语言模型的全方位校准量化。*arXiv预印本
    arXiv:2308.13137*。
- en: 'Tseng et al. (2024) Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov,
    and Christopher De Sa. 2024. Quip#: Even better LLM quantization with hadamard
    incoherence and lattice codebooks. *arXiv preprint arXiv:2402.04396*.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tseng et al. (2024) 阿尔伯特·曾、杰瑞·契、孙青尧、弗拉基米尔·库列绍夫和克里斯托弗·德·萨。2024。Quip#：通过哈达玛不相干性和格子码本实现更好的LLM量化。*arXiv预印本
    arXiv:2402.04396*。
- en: 'Wang et al. (2024) Lei Wang, Lingxiao Ma, Shijie Cao, Quanlu Zhang, Jilong
    Xue, Yining Shi, Ningxin Zheng, Ziming Miao, Fan Yang, Ting Cao, Yuqing Yang,
    and Mao Yang. 2024. [Ladder: Enabling efficient low-precision deep learning computing
    through hardware-aware tensor transformation](https://www.usenix.org/conference/osdi24/presentation/wang-lei).
    In *18th USENIX Symposium on Operating Systems Design and Implementation (OSDI
    24)*.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2024) 王磊、马灵霄、曹世杰、张全璐、薛季龙、石一宁、郑宁鑫、缪紫铭、杨帆、曹婷、杨玉青和杨毛。2024。[Ladder:
    通过硬件感知张量变换实现高效低精度深度学习计算](https://www.usenix.org/conference/osdi24/presentation/wang-lei)。在
    *第18届USENIX操作系统设计与实现研讨会 (OSDI 24)* 中。'
- en: Wang et al. (2022) Longguang Wang, Xiaoyu Dong, Yingqian Wang, Li Liu, Wei An,
    and Yulan Guo. 2022. Learnable lookup table for neural network quantization. In
    *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    pages 12423–12433.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022) 王龙光、董晓宇、王颖倩、刘磊、安伟和郭雨兰。2022。用于神经网络量化的可学习查找表。在 *IEEE/CVF计算机视觉与模式识别会议录*
    中，第12423–12433页。
- en: 'Wei et al. (2022) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong,
    Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. 2022. Outlier suppression:
    Pushing the limit of low-bit transformer language models. *Advances in Neural
    Information Processing Systems*, 35:17402–17414.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) 魏秀颖、张云晨、张向国、龚瑞浩、张尚航、张琪、余锋伟和刘向龙。2022。异常值抑制：推动低位宽变换器语言模型的极限。*神经信息处理系统进展*，35:17402–17414。
- en: 'Xia et al. (2024a) Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu
    Zhou, Xiafei Qiu, Yong Li, Wei Lin, and Shuaiwen Leon Song. 2024a. Flash-LLM:
    Enabling cost-effective and highly-efficient large generative model inference
    with unstructured sparsity. In *Proceedings of VLDB*.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia et al. (2024a) 赵浩君、郑真、李宇超、庄东林、周钟铸、邱夏飞、李勇、林伟、宋帅文。2024a。Flash-LLM：通过非结构化稀疏性实现高效且成本效益高的大型生成模型推理。在
    *VLDB会议录* 中。
- en: 'Xia et al. (2024b) Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei
    Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou,
    et al. 2024b. FP6-LLM: Efficiently serving large language models through FP6-centric
    algorithm-system co-design. *arXiv preprint arXiv:2401.14112*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia et al. (2024b) Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei
    Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou,
    等. 2024b. FP6-LLM: 通过 FP6 中心算法系统共设计高效服务大型语言模型。*arXiv 预印本 arXiv:2401.14112*。'
- en: 'Xiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. 2022. SmoothQuant: Accurate and efficient post-training quantization
    for large language models. *arXiv:2211.10438*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    和 Song Han. 2022. SmoothQuant: 高效且准确的大型语言模型后训练量化。*arXiv:2211.10438*。'
- en: Xu et al. (2021) Shiyu Xu, Qi Wang, Xingbo Wang, Shihang Wang, and Terry Tao
    Ye. 2021. Multiplication through a single look-up-table (LUT) in CNN inference
    computation. *IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems*, 41(6):1916–1928.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2021) Shiyu Xu, Qi Wang, Xingbo Wang, Shihang Wang, 和 Terry Tao Ye.
    2021. 通过单个查找表（LUT）进行 CNN 推理计算。*IEEE 集成电路与系统计算机辅助设计交易*，41(6):1916–1928。
- en: Yamamoto (2021) Kohei Yamamoto. 2021. Learnable companding quantization for
    accurate low-bit neural networks. In *Proceedings of th e IEEE/CVF conference
    on computer vision and pattern recognition*, pages 5029–5038.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yamamoto (2021) Kohei Yamamoto. 2021. 可学习的压缩量化用于准确的低位神经网络。收录于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，页
    5029–5038。
- en: Yang et al. (2019) Jiwei Yang, Xu Shen, Jun Xing, Xinmei Tian, Houqiang Li,
    Bing Deng, Jianqiang Huang, and Xian-sheng Hua. 2019. Quantization networks. In
    *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    pages 7308–7316.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2019) Jiwei Yang, Xu Shen, Jun Xing, Xinmei Tian, Houqiang Li,
    Bing Deng, Jianqiang Huang, 和 Xian-sheng Hua. 2019. 量化网络。收录于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，页
    7308–7316。
- en: 'Zhang et al. (2018) Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang
    Hua. 2018. LQ-Nets: Learned quantization for highly accurate and compact deep
    neural networks. In *Proceedings of the European conference on computer vision
    (ECCV)*, pages 365–382.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2018) Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, 和 Gang Hua.
    2018. LQ-Nets: 为高准确性和紧凑的深度神经网络学习量化。收录于 *欧洲计算机视觉会议（ECCV）*，页 365–382。'
- en: 'Zhao et al. (2023) Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen,
    Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. 2023.
    Atom: Low-bit quantization for efficient and accurate LLM serving. *arXiv preprint
    arXiv:2310.19102*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. (2023) Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen,
    Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, 和 Baris Kasikci. 2023.
    Atom: 低位量化用于高效和准确的 LLM 服务。*arXiv 预印本 arXiv:2310.19102*。'
- en: 'Zhou et al. (2017) Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen.
    2017. Incremental network quantization: Towards lossless CNNs with low-precision
    weights. *arXiv preprint arXiv:1702.03044*.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2017) Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, 和 Yurong Chen.
    2017. 增量网络量化：迈向具有低精度权重的无损 CNN。*arXiv 预印本 arXiv:1702.03044*。
- en: Appendix A Appendix
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: Please see Algorithm [2](#alg2 "Algorithm 2 ‣ Appendix A Appendix ‣ Fast Matrix
    Multiplications for Lookup Table-Quantized LLMs") for a detailed version of the
    algorithm, and Tables [3](#A1.T3 "Table 3 ‣ Appendix A Appendix ‣ Fast Matrix
    Multiplications for Lookup Table-Quantized LLMs"), [4](#A1.T4 "Table 4 ‣ Appendix
    A Appendix ‣ Fast Matrix Multiplications for Lookup Table-Quantized LLMs") for
    detailed experimental results.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见算法 [2](#alg2 "算法 2 ‣ 附录 A 附录 ‣ 查找表量化 LLM 的快速矩阵乘法") 获取算法的详细版本，并参见表格 [3](#A1.T3
    "表 3 ‣ 附录 A 附录 ‣ 查找表量化 LLM 的快速矩阵乘法") 和 [4](#A1.T4 "表 4 ‣ 附录 A 附录 ‣ 查找表量化 LLM 的快速矩阵乘法")
    获取详细的实验结果。
- en: Algorithm 2 FLUTE
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 FLUTE
- en: ${\mathbf{X}}^{\text{g}}$ Write output tile from Registers to HBM              end if         end if     end whileend parallel
    for
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ${\mathbf{X}}^{\text{g}}$ 从寄存器写出输出块到 HBM              end if         end if     end whileend parallel
    for
- en: '| Method | Bits | Group | PPL$\downarrow$ |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | 组数 | PPL$\downarrow$ |'
- en: '| WikiText2 | C4 | PIQA | ARC-e | ARC-c | HellaSwag | Wino | Avg. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 | C4 | PIQA | ARC-e | ARC-c | HellaSwag | Wino | 平均值 |'
- en: '| Unquantized | 16 | N/A | 6.1 | 9.2 | 79.9 | 80.1 | 50.4 | 60.2 | 72.8 | 68.6
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 未量化 | 16 | 不适用 | 6.1 | 9.2 | 79.9 | 80.1 | 50.4 | 60.2 | 72.8 | 68.6 |'
- en: '| RTN | 4 | 128 | 8.5 | 13.4 | 76.6 | 70.1 | 45.0 | 56.8 | 71.0 | 63.9 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 128 | 8.5 | 13.4 | 76.6 | 70.1 | 45.0 | 56.8 | 71.0 | 63.9 |'
- en: '| 3 | 128 | 27.9 | 1.1e2 | 62.3 | 32.1 | 22.5 | 29.1 | 54.7 | 40.2 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 128 | 27.9 | 1.1e2 | 62.3 | 32.1 | 22.5 | 29.1 | 54.7 | 40.2 |'
- en: '| GPTQ | 4 | 128 | 6.5 | 10.4 | 78.4 | 78.8 | 47.7 | 59.0 | 72.6 | 67.3 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 128 | 6.5 | 10.4 | 78.4 | 78.8 | 47.7 | 59.0 | 72.6 | 67.3 |'
- en: '| 3 | 128 | 8.2 | 13.7 | 74.9 | 70.5 | 37.7 | 54.3 | 71.1 | 61.7 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 128 | 8.2 | 13.7 | 74.9 | 70.5 | 37.7 | 54.3 | 71.1 | 61.7 |'
- en: '| AWQ | 4 | 128 | 6.6 | 9.4 | 79.1 | 79.7 | 49.3 | 59.1 | 74.0 | 68.2 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4 | 128 | 6.6 | 9.4 | 79.1 | 79.7 | 49.3 | 59.1 | 74.0 | 68.2 |'
- en: '| 3 | 128 | 8.2 | 11.6 | 77.7 | 74.0 | 43.2 | 55.1 | 72.1 | 64.4 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 128 | 8.2 | 11.6 | 77.7 | 74.0 | 43.2 | 55.1 | 72.1 | 64.4 |'
- en: '| OmniQuant | 4 | 128 | 6.6 | 10.1 | 79.1 | 80.0 | 49.7 | 59.4 | 73.2 | 68.3
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 4 | 128 | 6.6 | 10.1 | 79.1 | 80.0 | 49.7 | 59.4 | 73.2 | 68.3
    |'
- en: '| 3 | 128 | 8.4 | 13.5 | 76.4 | 70.0 | 40.9 | 55.1 | 69.5 | 62.4 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 128 | 8.4 | 13.5 | 76.4 | 70.0 | 40.9 | 55.1 | 69.5 | 62.4 |'
- en: '| NormalFloat | 4 | 128 | 6.6 | 9.5 | 78.6 | 79.6 | 49.6 | 59.0 | 73.5 | 68.0
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| NormalFloat | 4 | 128 | 6.6 | 9.5 | 78.6 | 79.6 | 49.6 | 59.0 | 73.5 | 68.0
    |'
- en: '| 3 | 128 | 9.2 | 13.0 | 75.4 | 72.0 | 40.5 | 54.4 | 69.4 | 62.3 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 128 | 9.2 | 13.0 | 75.4 | 72.0 | 40.5 | 54.4 | 69.4 | 62.3 |'
- en: '| NormalFloat | 4 | 128 | 6.2 | 9.5 | 79.0 | 79.6 | 49.0 | 59.4 | 72.6 | 67.9
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| NormalFloat | 4 | 128 | 6.2 | 9.5 | 79.0 | 79.6 | 49.0 | 59.4 | 72.6 | 67.9
    |'
- en: '| learned $\sigma$ | 3 | 128 | 7.5 | 11.7 | 77.1 | 74.1 | 41.7 | 55.8 | 69.7
    | 63.7 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 学习的 $\sigma$ | 3 | 128 | 7.5 | 11.7 | 77.1 | 74.1 | 41.7 | 55.8 | 69.7 |
    63.7 |'
- en: '| NormalFloat | 4 | 128 | 6.5 | 9.3 | 79.6 | 78.0 | 48.5 | 59.0 | 73.8 | 67.8
    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| NormalFloat | 4 | 128 | 6.5 | 9.3 | 79.6 | 78.0 | 48.5 | 59.0 | 73.8 | 67.8
    |'
- en: '| + AWQ | 3 | 128 | 8.0 | 11.5 | 77.0 | 75.5 | 44.6 | 55.9 | 72.3 | 65.1 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| + AWQ | 3 | 128 | 8.0 | 11.5 | 77.0 | 75.5 | 44.6 | 55.9 | 72.3 | 65.1 |'
- en: 'Table 3: Detailed evaluation of post-training quantization on LLaMA3-8B. The
    RTN, GPTQ Frantar et al. ([2022](#bib.bib8)), AWQ Lin et al. ([2023](#bib.bib14))
    results are from Huang et al. ([2024](#bib.bib11)); the rest are from our implementations.
    All non-NormalFloat methods use uniform weight quantization.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 对 LLaMA3-8B 的后训练量化的详细评估。RTN、GPTQ Frantar 等 ([2022](#bib.bib8))、AWQ Lin
    等 ([2023](#bib.bib14)) 的结果来自 Huang 等 ([2024](#bib.bib11))；其余结果来自我们的实现。所有非 NormalFloat
    方法使用均匀权重量化。'
- en: '| Method | Bits | Group | PPL$\downarrow$ |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | 组别 | PPL$\downarrow$ |'
- en: '| WikiText2 | C4 | PIQA | ARC-e | ARC-c | HellaSwag | Wino | Avg. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 | C4 | PIQA | ARC-e | ARC-c | HellaSwag | Wino | 平均值 |'
- en: '| Unquantized | 16 | N/A | 2.9 | 6.9 | 82.4 | 86.9 | 60.3 | 66.4 | 80.6 | 75.3
    |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 未量化 | 16 | N/A | 2.9 | 6.9 | 82.4 | 86.9 | 60.3 | 66.4 | 80.6 | 75.3 |'
- en: '| RTN | 4 | 128 | 3.6 | 8.9 | 82.3 | 85.2 | 58.4 | 65.6 | 79.8 | 74.3 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 128 | 3.6 | 8.9 | 82.3 | 85.2 | 58.4 | 65.6 | 79.8 | 74.3 |'
- en: '| 3 | 128 | 11.8 | 22.0 | 64.2 | 48.9 | 25.1 | 41.1 | 60.5 | 48.0 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 128 | 11.8 | 22.0 | 64.2 | 48.9 | 25.1 | 41.1 | 60.5 | 48.0 |'
- en: '| GPTQ | 4 | 128 | 3.3 | 6.9 | 82.9 | 86.3 | 58.4 | 66.1 | 80.7 | 74.9 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 128 | 3.3 | 6.9 | 82.9 | 86.3 | 58.4 | 66.1 | 80.7 | 74.9 |'
- en: '| 3 | 128 | 5.2 | 10.5 | 80.6 | 79.6 | 52.1 | 63.5 | 77.1 | 70.6 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 128 | 5.2 | 10.5 | 80.6 | 79.6 | 52.1 | 63.5 | 77.1 | 70.6 |'
- en: '| AWQ | 4 | 128 | 3.3 | 7.0 | 82.7 | 86.3 | 59.0 | 65.7 | 80.9 | 74.9 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4 | 128 | 3.3 | 7.0 | 82.7 | 86.3 | 59.0 | 65.7 | 80.9 | 74.9 |'
- en: '| 3 | 128 | 4.8 | 8.0 | 81.4 | 84.7 | 58.0 | 63.5 | 78.6 | 73.2 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 128 | 4.8 | 8.0 | 81.4 | 84.7 | 58.0 | 63.5 | 78.6 | 73.2 |'
- en: '| OmniQuant | 4 | 128 | 3.3 | 7.5 | 82.0 | 85.6 | 58.0 | 66.0 | 79.6 | 74.2
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 4 | 128 | 3.3 | 7.5 | 82.0 | 85.6 | 58.0 | 66.0 | 79.6 | 74.2
    |'
- en: '| 3 | 128 | 5.4 | 9.3 | 80.8 | 80.6 | 50.9 | 63.7 | 75.2 | 70.2 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 128 | 5.4 | 9.3 | 80.8 | 80.6 | 50.9 | 63.7 | 75.2 | 70.2 |'
- en: '| NormalFloat | 4 | 128 | 3.4 | 7.6 | 82.0 | 85.6 | 56.7 | 66.1 | 79.5 | 74.0
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| NormalFloat | 4 | 128 | 3.4 | 7.6 | 82.0 | 85.6 | 56.7 | 66.1 | 79.5 | 74.0
    |'
- en: '| 3 | 128 | 8.7 | 16.7 | 76.6 | 76.9 | 42.7 | 55.8 | 69.3 | 64.3 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 128 | 8.7 | 16.7 | 76.6 | 76.9 | 42.7 | 55.8 | 69.3 | 64.3 |'
- en: '| NormalFloat | 4 | 128 | 3.1 | 7.2 | 82.3 | 85.7 | 58.2 | 66.4 | 79.6 | 74.4
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| NormalFloat | 4 | 128 | 3.1 | 7.2 | 82.3 | 85.7 | 58.2 | 66.4 | 79.6 | 74.4
    |'
- en: '| learned $\sigma$ | 3 | 128 | 5.2 | 10.1 | 77.3 | 76.7 | 44.3 | 62.4 | 71.2
    | 66.4 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 学习的 $\sigma$ | 3 | 128 | 5.2 | 10.1 | 77.3 | 76.7 | 44.3 | 62.4 | 71.2 |
    66.4 |'
- en: '| NormalFloat | 4 | 128 | 3.2 | 6.9 | 82.6 | 86.8 | 60.1 | 65.9 | 80.5 | 75.2
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| NormalFloat | 4 | 128 | 3.2 | 6.9 | 82.6 | 86.8 | 60.1 | 65.9 | 80.5 | 75.2
    |'
- en: '| + AWQ | 3 | 128 | 4.6 | 7.8 | 81.4 | 85.3 | 58.5 | 64.6 | 79.2 | 73.8 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| + AWQ | 3 | 128 | 4.6 | 7.8 | 81.4 | 85.3 | 58.5 | 64.6 | 79.2 | 73.8 |'
- en: 'Table 4: Detailed evaluation of post-training quantization on LLaMA3-70B. The
    RTN, GPTQ Frantar et al. ([2022](#bib.bib8)), AWQ Lin et al. ([2023](#bib.bib14))
    results are from Huang et al. ([2024](#bib.bib11)); the rest are from our implementations.
    All non-NormalFloat methods use uniform weight quantization.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 对 LLaMA3-70B 的后训练量化的详细评估。RTN、GPTQ Frantar 等 ([2022](#bib.bib8))、AWQ Lin
    等 ([2023](#bib.bib14)) 的结果来自 Huang 等 ([2024](#bib.bib11))；其余结果来自我们的实现。所有非 NormalFloat
    方法使用均匀权重量化。'
