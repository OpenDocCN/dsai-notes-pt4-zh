- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:59:10'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:59:10'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive
    Decoding and Distillation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒸馏对比解码：通过对比解码和蒸馏提升 LLM 的推理能力
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.14874](https://ar5iv.labs.arxiv.org/html/2402.14874)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.14874](https://ar5iv.labs.arxiv.org/html/2402.14874)
- en: Phuc Phan^∗, Hieu Tran * Equal contribution    Long Phan
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Phuc Phan^∗, Hieu Tran * 相同贡献    Long Phan
- en: VietAI Research
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: VietAI 研究
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We propose a straightforward approach called Distillation Contrastive Decoding
    (DCD) to enhance the reasoning capabilities of Large Language Models (LLMs) during
    inference. In contrast to previous approaches that relied on smaller amateur models
    or analysis of hidden state differences, DCD employs Contrastive Chain-of-thought
    Prompting and advanced distillation techniques, including Dropout and Quantization.
    This approach effectively addresses the limitations of Contrastive Decoding (CD),
    which typically requires both an expert and an amateur model, thus increasing
    computational resource demands. By integrating contrastive prompts with distillation,
    DCD obviates the need for an amateur model and reduces memory usage. Our evaluations
    demonstrate that DCD significantly enhances LLM performance across a range of
    reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and
    StrategyQA datasets.¹¹1Code is available at [https://github.com/pphuc25/distil-cd](https://github.com/pphuc25/distil-cd)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种名为蒸馏对比解码（DCD）的简单方法，以增强大型语言模型（LLMs）在推理过程中的能力。与依赖于较小的业余模型或隐藏状态差异分析的先前方法不同，DCD
    采用了对比链式思维提示和先进的蒸馏技术，包括 Dropout 和量化。这种方法有效地解决了对比解码（CD）的局限性，后者通常需要专家模型和业余模型，从而增加了计算资源的需求。通过将对比提示与蒸馏结合，DCD
    避免了对业余模型的需求并减少了内存使用。我们的评估表明，DCD 在一系列推理基准测试中显著提升了 LLM 的性能，超过了 CD 和现有方法在 GSM8K 和
    StrategyQA 数据集上的表现。¹¹1 代码可在 [https://github.com/pphuc25/distil-cd](https://github.com/pphuc25/distil-cd)
    找到
- en: '![Refer to caption](img/e687aa3ff1c19514742f315c178488f2.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e687aa3ff1c19514742f315c178488f2.png)'
- en: 'Figure 1: An overview of Distillation Contrastive Decoding method. Valid CoT
    demonstrations as well as the query will be sent to an LLM, while invalid CoT
    demonstrations and the query will be sent into a distilled version of the model.
    We will then use this logit information to enhance the reasoning decoding process.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：蒸馏对比解码方法的概述。有效的 CoT 演示以及查询将发送到 LLM，而无效的 CoT 演示和查询将发送到模型的蒸馏版本。然后，我们将使用这些
    logit 信息来增强推理解码过程。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Reasoning capabilities in large language models (LLMs) refer to the models’
    ability to analyze, understand, and infer information, mirroring human-like logical
    reasoning. Recently, the reasoning skills of LLMs have seen substantial advancements,
    showcasing their vast potential in various natural language processing applications
    Brown et al. ([2020a](#bib.bib3)). While some research focuses on enhancing models
    through advanced training techniques and architectures Touvron et al. ([2023a](#bib.bib18));
    Jiang et al. ([2023](#bib.bib11)); Bai et al. ([2023](#bib.bib1)), others aim
    to augment the models’ internal capabilities Zou et al. ([2023](#bib.bib22));
    Bricken et al. ([2023](#bib.bib2)). Beyond model training and augmentation, further
    research explores innovative methods to enhance LLM efficiency during inference
    Li et al. ([2023b](#bib.bib14), [a](#bib.bib13)); Chuang et al. ([2023](#bib.bib6)).
    In this work, we introduce Distillation Contrastive Decoding (DCD), a method designed
    to enhance the reasoning abilities of LLMs during inference by leveraging Contrastive
    Chain-of-thought prompts and distillation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的推理能力指的是模型分析、理解和推断信息的能力，类似于人类的逻辑推理。最近，LLMs 的推理技能有了显著进展，展示了其在各种自然语言处理应用中的巨大潜力
    Brown et al. ([2020a](#bib.bib3))。虽然一些研究专注于通过先进的训练技术和架构来增强模型 Touvron et al. ([2023a](#bib.bib18));
    Jiang et al. ([2023](#bib.bib11)); Bai et al. ([2023](#bib.bib1))，其他研究则旨在增强模型的内部能力
    Zou et al. ([2023](#bib.bib22)); Bricken et al. ([2023](#bib.bib2))。除了模型训练和增强，进一步的研究探索了在推理过程中提高
    LLM 效率的创新方法 Li et al. ([2023b](#bib.bib14), [a](#bib.bib13)); Chuang et al. ([2023](#bib.bib6))。在这项工作中，我们介绍了蒸馏对比解码（DCD），这是一种旨在通过利用对比链式思维提示和蒸馏来提升
    LLM 在推理过程中的能力的方法。
- en: 'Distillation Contrastive Decoding (DCD) builds on recent advancements in enhancing
    the reasoning capabilities of LLMs through Contrastive Decoding (CD) O’Brien and
    Lewis ([2023](#bib.bib16)) and Contrastive Chain-of-thought Prompting (CP) Chia
    et al. ([2023](#bib.bib5)). These methods utilize contrasting elements to reduce
    reasoning errors in text generation, thereby improving task performance. The primary
    motivation behind DCD is addressing two common limitations of CD. Firstly, CD
    typically requires a smaller amateur LLM within the same family to evaluate the
    outputs of the primary LLM. This prerequisite poses a challenge, especially for
    small-sized models, as smaller models with identical vocabularies may not be available.
    This challenge is notably present in cases such as Mistral-7B Jiang et al. ([2023](#bib.bib11))
    and DeepSeek-7B DeepSeek-AI et al. ([2024](#bib.bib8)), where smaller models are
    unavailable. The second limitation with CD is the requirement to simultaneously
    load two models into memory: an expert and an amateur model, which significantly
    increases computational resource demands. An example of this is using Llama2-7B
    as the amateur model and Llama2-13B as the expert model, highlighting the resource-intensive
    nature of the CD approach.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏对比解码（DCD）建立在通过对比解码（CD）O’Brien 和 Lewis（[2023](#bib.bib16)）以及对比链式思维提示（CP）Chia
    等人（[2023](#bib.bib5)）在增强大型语言模型（LLM）的推理能力方面的最新进展上。这些方法利用对比元素来减少文本生成中的推理错误，从而提高任务表现。DCD
    的主要动机是解决 CD 的两个常见限制。首先，CD 通常需要一个较小的同家族的业余 LLM 来评估主要 LLM 的输出。这一前提构成挑战，特别是对于小型模型，因为可能没有具有相同词汇的小型模型。这一挑战在
    Mistral-7B Jiang 等人（[2023](#bib.bib11)）和 DeepSeek-7B DeepSeek-AI 等人（[2024](#bib.bib8)）等情况中尤为明显，因为这些情况下缺乏较小的模型。第二个限制是
    CD 需要同时加载两个模型到内存中：一个专家模型和一个业余模型，这大大增加了计算资源的需求。例如，使用 Llama2-7B 作为业余模型，Llama2-13B
    作为专家模型，突显了 CD 方法的资源密集型特性。
- en: Our findings demonstrate that DCD surpasses existing methodologies in enhancing
    Chain-of-thought reasoning within LLMs. Specifically, on the GSM8K benchmark,
    which comprises grade-school level word math problems, DCD elevates the performance
    of Llama2 models by as much as $3.79\%$. We observe marked improvements in both
    arithmetic and commonsense reasoning tasks when DCD is applied to Mistral-7B,
    known for its robust foundational knowledge and high scores on the MMLU benchmark
    Hendrycks et al. ([2020](#bib.bib10)), suggesting that DCD could bring such widespread
    improvements to much stronger models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的发现表明，DCD 在提升 LLM 的链式思维推理方面超越了现有的方法。具体而言，在 GSM8K 基准测试中，该测试包含了小学水平的文字数学问题，DCD
    将 Llama2 模型的表现提升了多达 $3.79\%$。我们观察到，当 DCD 应用于以其强大的基础知识和在 MMLU 基准上高分著称的 Mistral-7B
    时，无论是在算术还是常识推理任务中都取得了显著改善，这表明 DCD 可能会对更强大的模型带来如此广泛的改进。
- en: 'In summary, our main contributions are: (1) Introducing a straightforward approach
    that combines Contrastive Chain-of-thought Prompting, Contrastive Decoding, and
    Distillation to enhance LLM reasoning abilities, eliminating the need for smaller
    models and reducing memory usage. (2) Demonstrating significant performance improvements
    across multiple reasoning benchmarks compared to Contrastive Decoding and other
    methods.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的主要贡献是：（1）引入一种简单的方法，将对比链式思维提示、对比解码和蒸馏结合起来，以增强 LLM 的推理能力，消除对较小模型的需求，并减少内存使用。（2）与对比解码和其他方法相比，展示了在多个推理基准上显著的性能提升。
- en: 2 Related Works
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Chain-of-thought (CoT) is a significant development in enhancing text-generation
    models’ reasoning capabilities. This concept, as originally introduced by Wei
    et al. ([2023](#bib.bib21)), involves the model generating intermediate steps
    in its reasoning process, akin to human problem-solving methods. Furthermore,
    the work by Kojima et al. ([2023](#bib.bib12)) revealed that specific prompts,
    such as "Let’s think step-by-step", can spontaneously trigger CoT reasoning in
    LLMs. These developments are the foundation for research works on enhancing LLM’s
    reasoning abilities.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 链式思维（CoT）是提升文本生成模型推理能力的重大进展。这个概念最初由 Wei 等人（[2023](#bib.bib21)）提出，涉及模型在其推理过程中生成中间步骤，类似于人类的问题解决方法。此外，Kojima
    等人（[2023](#bib.bib12)）的研究表明，特定提示，如“让我们一步步思考”，可以自发地激发 LLMs 中的 CoT 推理。这些发展为提升 LLM
    推理能力的研究奠定了基础。
- en: Recently, O’Brien and Lewis ([2023](#bib.bib16)) demonstrated that Contrastive
    Decoding (CD), a decoding method proposed by Li et al. ([2023b](#bib.bib14)),
    can enhance LLM performance across a range of reasoning tasks. Initially, CD was
    designed to enhance the quality of long-form text generation by identifying tokens
    that significantly differ in likelihood between a strong model and a comparatively
    weak model. The study by O’Brien and Lewis ([2023](#bib.bib16)) further revealed
    that incorporating a smaller amateur LLM in the CD process can effectively reduce
    reasoning errors in the larger expert model, thereby achieving high performance
    across multiple benchmarks. Another study by Chuang et al. ([2023](#bib.bib6))
    proposes an alternative approach by contrasting the differences in logits obtained
    from projecting the later layers versus earlier layers to the vocabulary space
    in an LLM. Chia et al. ([2023](#bib.bib5)) looks into improving downstream CoT
    reasoning by incorporating both positive and negative reasoning in the few-shot
    sequences to allow the model to learn from both positive and negative examples.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，O’Brien和Lewis（[2023](#bib.bib16)）展示了由Li等人（[2023b](#bib.bib14)）提出的对比解码（CD）方法可以提升LLM在各种推理任务中的表现。最初，CD旨在通过识别强模型与相对弱模型之间在生成长文本时的显著差异的tokens来提高生成文本的质量。O’Brien和Lewis（[2023](#bib.bib16)）的研究进一步揭示了在CD过程中加入较小的业余LLM可以有效减少较大专家模型中的推理错误，从而在多个基准测试中取得高性能。另一项由Chuang等人（[2023](#bib.bib6)）提出的研究提出了一种替代方法，通过对比从LLM的后层与前层投射到词汇空间的logits差异。Chia等人（[2023](#bib.bib5)）研究了通过在少量示例序列中结合正向和负向推理来改善下游CoT推理，以使模型能够从正向和负向示例中学习。
- en: Besides decoding intervention methods, recent work by Zou et al. ([2023](#bib.bib22))
    has introduced a new research area known as Representation Engineering (RepE).
    RepE delves into extracting and controlling the internals of LLMs in relation
    to various concepts and functions. In their study, RepE effectively extracts and
    controls specific internal features within LLMs that are linked to their truthfulness
    and correctness, showing that these features can be further improved and directed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解码干预方法外，Zou等人（[2023](#bib.bib22)）最近提出了一个新研究领域，称为表示工程（RepE）。RepE深入研究了如何提取和控制LLM内部与各种概念和功能相关的特征。在他们的研究中，RepE有效地提取和控制了LLM中与其真实性和正确性相关的特定内部特征，表明这些特征可以进一步改善和引导。
- en: 3 Methodology
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: '![Refer to caption](img/1c7602b542cc81c696ffc59aef968208.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1c7602b542cc81c696ffc59aef968208.png)'
- en: 'Figure 2: Comparison between 3 methods: (1) Contrastive Chain-of-thought Prompting,
    which relies on extensive prefixes incorporating Contrastive Chain-of-thought
    examples; (2) Contrastive Decoding, which necessitates the availability of a smaller
    amateur version of the LLM; and (3) Distillation Contrastive Decoding (Ours),
    conceived to overcome the constraints of the previous methods by incorporating
    the fundamental principles of both (1) and (2)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：三种方法的比较：（1）对比链式提示，依赖于包含对比链式提示示例的广泛前缀；（2）对比解码，要求提供较小的业余LLM版本；以及（3）蒸馏对比解码（我们的方法），旨在通过结合（1）和（2）的基本原则来克服前述方法的限制。
- en: 'Our approach, Distillation Contrastive Decoding (DCD), builds upon the foundational
    work of Contrastive Decoding (CD) O’Brien and Lewis ([2023](#bib.bib16)) and Contrastive
    Chain-of-thought Prompting (CP) Chia et al. ([2023](#bib.bib5)). A principal motivation
    behind DCD is to overcome a significant limitation of CD: its reliance on a smaller
    model of the same architecture, often referred to as an amateur model. This dependency
    poses significant challenges, as an equivalent amateur model is not always available
    across different open-source architectures, a situation exemplified by Mistral
    Jiang et al. ([2023](#bib.bib11)). DCD aims to offer a more adaptable and inclusive
    solution, irrespective of the specific class of language model employed.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法，蒸馏对比解码（DCD），基于O’Brien和Lewis（[2023](#bib.bib16)）提出的对比解码（CD）和Chia等人（[2023](#bib.bib5)）提出的对比链式提示（CP）的基础工作。DCD的主要动机之一是克服CD的一个重大限制：它依赖于一个较小的相同架构的模型，通常被称为业余模型。这种依赖性带来了重大挑战，因为在不同的开源架构中并不总是能获得等效的业余模型，这种情况在Mistral
    Jiang等人（[2023](#bib.bib11)）的研究中得到了体现。DCD旨在提供一种更具适应性和包容性的解决方案，无论使用的是哪一类语言模型。
- en: 3.1 Contrastive Decoding
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 对比解码
- en: 'Contrastive Decoding (CD) involves two models: a larger expert model, and a
    smaller amateur model. The method leverages a comparison between the predicted
    logits of an expert model, denoted as $s_{e}$ is defined as:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对比解码（CD）涉及两个模型：一个较大的专家模型和一个较小的业余模型。该方法利用专家模型的预测logits之间的比较，定义为 $s_{e}$：
- en: '|  | $s=(1+\beta)\cdot s_{e}-\beta\cdot s_{a}$ |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $s=(1+\beta)\cdot s_{e}-\beta\cdot s_{a}$ |  |'
- en: By exploiting the differences in predictive confidence between the two models,
    this method improves the generation of text sequences in reasoning tasks. However,
    the work shows that while a 1B-parameter amateur helps improve reasoning capabilities,
    a 7B-parameter amateur harms it. This poses a significant drawback as not all
    model classes have a 1B-parameter model to act as an amateur model in the decoding
    process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用两个模型之间预测置信度的差异，该方法提高了推理任务中文本序列的生成。然而，研究表明，虽然1B-参数的业余模型有助于提高推理能力，但7B-参数的业余模型会损害推理能力。这是一个显著的缺陷，因为并非所有模型类别都有一个1B-参数的模型作为解码过程中的业余模型。
- en: 3.2 Contrastive Chain-of-thought Prompting
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 对比思维链提示
- en: Contrastive Chain-of-thought Prompting (CP) integrates both correct and incorrect
    reasoning examples to direct the model through a step-by-step reasoning process,
    thereby minimizing logical errors. This method is inspired by the human ability
    to learn from both successful and unsuccessful examples. By including examples
    of both sound and flawed reasoning, the technique aids the model in identifying
    and correcting potential mistakes in intermediate reasoning steps. Such errors
    have been identified as significant obstacles to accurate reasoning processes
    Ling et al. ([2023](#bib.bib15)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对比思维链提示（CP）结合了正确和错误的推理示例，引导模型通过逐步推理过程，从而最小化逻辑错误。这种方法的灵感来源于人类从成功和失败的例子中学习的能力。通过包括合理和有缺陷的推理示例，该技术帮助模型识别和纠正潜在的中间推理错误。这些错误已被识别为准确推理过程的重大障碍
    Ling et al. ([2023](#bib.bib15))。
- en: 'Concretely, given a query $Q$. The method can be formulated as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，给定一个查询 $Q$。该方法可以表述为：
- en: '|  | $A_{j}=(Q_{j},E_{1+},E_{1-},...,E_{n+},E_{n-})$ |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $A_{j}=(Q_{j},E_{1+},E_{1-},...,E_{n+},E_{n-})$ |  |'
- en: However, the method tends to extend the length of input sequences significantly,
    necessitating increased computational resources. In our experiments, we have also
    observed that the inclusion of multiple shots of both valid and invalid demonstrations
    can lead to confusion in an unaligned LLM, consequently diminishing its reasoning
    performance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法往往显著延长输入序列的长度，导致需要增加计算资源。在我们的实验中，我们还观察到，包含多个有效和无效示例的拍摄可能会导致未对齐的LLM产生混淆，从而降低其推理性能。
- en: 3.3 Distillation Contrastive Decoding (Ours)
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 蒸馏对比解码（我们的方案）
- en: 'Distillation Contrastive Decoding (DCD) is designed to overcome existing drawbacks
    in both CD and CP. Instead of requiring an external 1B-parameters amateur model,
    we utilize distillation techniques to acquire the amateur reasoning information.
    For the anchor expert model, we employ regular valid CoT demonstrations as a few
    shot examples. For the distilled amateur model, we employ invalid CoT examples
    to enable the motivations in leveraging incorrect reasoning features in computing
    the next token weights. The DCD algorithm is shown in Algorithm [1](#alg1 "Algorithm
    1 ‣ 3.3 Distillation Contrastive Decoding (Ours) ‣ 3 Methodology ‣ Distillation
    Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation").'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '蒸馏对比解码（DCD）旨在克服CD和CP中存在的缺陷。我们利用蒸馏技术来获取业余推理信息，而不是依赖外部的1B-参数业余模型。对于锚定专家模型，我们使用常规有效的CoT示例作为少量示例。对于蒸馏业余模型，我们使用无效的CoT示例来激励利用不正确的推理特征来计算下一个令牌的权重。DCD算法见算法
    [1](#alg1 "Algorithm 1 ‣ 3.3 Distillation Contrastive Decoding (Ours) ‣ 3 Methodology
    ‣ Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive
    Decoding and Distillation")。'
- en: Algorithm 1 Distillation Contrastive Decoding
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 蒸馏对比解码
- en: 'Input: Query $Q$'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：查询 $Q$
- en: 'In practice, we found that distilling the model by enabling a higher dropout
    rate during the inference step works best in most cases. The final results comparing
    between DCD with Dropout and previous baselines are shown in Section [6](#S6 "6
    Results ‣ Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive
    Decoding and Distillation"). Additionally, we explore other distillation methods
    such as Quantization, as well as a combined approach of applying both Dropout
    and Quantization to the model in Section [7](#S7 "7 Distillation Methods ‣ Distillation
    Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation").'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '实践中，我们发现通过在推理步骤中启用更高的丢弃率来蒸馏模型，在大多数情况下效果最佳。DCD 与丢弃法和之前基线方法的最终结果比较见于第 [6](#S6
    "6 Results ‣ Distillation Contrastive Decoding: Improving LLMs Reasoning with
    Contrastive Decoding and Distillation") 节。此外，我们还探索了其他蒸馏方法，如量化，以及在第 [7](#S7 "7
    Distillation Methods ‣ Distillation Contrastive Decoding: Improving LLMs Reasoning
    with Contrastive Decoding and Distillation") 节中将丢弃法和量化结合应用于模型的方法。'
- en: 4 Contrastive Chain-of-thought Design
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 对比链式思维设计
- en: 'Compared to conventional prompting methods with in-context demonstrations Brown
    et al. ([2020b](#bib.bib4)), Chain-of-thought (CoT) prompting Wei et al. ([2023](#bib.bib21))
    enhances this approach by incorporating a rationale for each few-shot example.
    This rationale is composed of a sequence of intermediate reasoning steps, which
    effectively guide the language model through a systematic process to assist the
    model in understanding and solving complex tasks. Wang et al. ([2023](#bib.bib20))
    identifies two components of a CoT rationale:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的上下文示例提示方法相比，Brown 等人 ([2020b](#bib.bib4))，链式思维（CoT）提示方法 Wei 等人 ([2023](#bib.bib21))
    通过为每个少量示例提供推理理由来增强这种方法。这些推理理由由一系列中间推理步骤组成，这些步骤有效地引导语言模型通过系统化过程，以帮助模型理解和解决复杂任务。Wang
    等人 ([2023](#bib.bib20)) 确定了 CoT 推理理由的两个组成部分：
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Bridging objects are the symbolic items that the model saw during the traverse
    to the final answer. In arithmetic reasoning, these are numbers and equations,
    while in factual/commonsense reasoning, these are subject and object entities.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 桥接对象是模型在推导最终答案过程中看到的象征性项目。在算术推理中，这些是数字和方程式，而在事实/常识推理中，这些是主体和宾体实体。
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Language templates are the complementary parts of the bridging objects, which
    serve as textual hints and relations or predicates that guide the model to derive
    the correct bridging objects throughout the reasoning process.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语言模板是桥接对象的补充部分，作为文本提示和关系或谓词，指导模型在整个推理过程中推导正确的桥接对象。
- en: 'Building on previous research Chia et al. ([2023](#bib.bib5)) that explores
    Contrastive Chain-of-thought Prompting design, we identified three types of contrasting
    bridging objects and one type of contrasting both bridging objects and language
    templates in arithmetic reasoning tasks. In our experiments on contrasting bridging
    objects, we explored three settings: (1) number shuffle, (2) number shuffle plus
    equation error, and (3) number shuffle plus irrelevant object plus operation swapping.
    For the Contrastive Chain-of-thought that involves contrasting both bridging objects
    and language templates, (4) we prompted GPT-3.5 to generate contrastive synthetic
    demonstrations. An example of each contrastive demonstration is shown in Figure
    [4](#S4.F4 "Figure 4 ‣ 4 Contrastive Chain-of-thought Design ‣ Distillation Contrastive
    Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation").
    Figure [3](#S4.F3 "Figure 3 ‣ 4 Contrastive Chain-of-thought Design ‣ Distillation
    Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation")
    shows the accuracy of the four contrastive settings on the GSM8K dataset using
    the Llama2 model with our method (DCD). Each of the four contrasting designs demonstrates
    a different increase in score compared to baselines. These preliminary results
    suggest that the incorporation of both contrastive bridging objects and language
    templates is crucial for designing effective Contrastive Chain-of-thought demonstrations.
    Additionally, setting (4), which includes synthetic examples, shows a significant
    increase in score. This indicates that DCD can effectively utilize automatic synthetic
    contrastive prompting generation with an external LLM, such as GPT-3.5.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Chia 等人 ([2023](#bib.bib5)) 对对比链式思维提示设计的研究，我们识别出了三种类型的对比桥接对象和一种类型的对比桥接对象与语言模板的组合，用于算术推理任务。在我们对对比桥接对象的实验中，我们探索了三种设置：（1）数字洗牌，（2）数字洗牌加方程错误，以及（3）数字洗牌加无关对象加操作交换。对于涉及对比桥接对象和语言模板的对比链式思维，（4）我们提示
    GPT-3.5 生成对比合成演示。每种对比演示的示例如图 [4](#S4.F4 "图 4 ‣ 4 对比链式思维设计 ‣ 蒸馏对比解码：通过对比解码和蒸馏提升
    LLMs 推理能力") 所示。图 [3](#S4.F3 "图 3 ‣ 4 对比链式思维设计 ‣ 蒸馏对比解码：通过对比解码和蒸馏提升 LLMs 推理能力")
    显示了在 GSM8K 数据集上使用我们的算法（DCD）的四种对比设置的准确率。四种对比设计中的每一种相较于基线都显示了不同的得分提升。这些初步结果表明，将对比桥接对象和语言模板结合起来对于设计有效的对比链式思维演示至关重要。此外，设置（4）包含合成示例，显示了显著的得分提升。这表明
    DCD 可以有效地利用与外部 LLM（如 GPT-3.5）结合的自动合成对比提示生成。
- en: '![Refer to caption](img/c4070d5987b5e94c84a7d231dc489dd0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c4070d5987b5e94c84a7d231dc489dd0.png)'
- en: 'Figure 3: Performance of different Contrastive Chain-of-thought settings discussed
    in Section [4](#S4 "4 Contrastive Chain-of-thought Design ‣ Distillation Contrastive
    Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation").
    Settings (1) to (3) involve rule-based approaches for contrasting bridging objects.
    Setting (4) employs a synthetic-based approach, incorporating contrasts in both
    bridging objects and language templates.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在第 [4](#S4 "4 对比链式思维设计 ‣ 蒸馏对比解码：通过对比解码和蒸馏提升 LLMs 推理能力") 节中讨论的不同对比链式思维设置的表现。设置（1）到（3）涉及基于规则的桥接对象对比方法。设置（4）采用基于合成的方法，结合了桥接对象和语言模板的对比。
- en: '![Refer to caption](img/84f73b6293e9e23007038068bcd84b00.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/84f73b6293e9e23007038068bcd84b00.png)'
- en: 'Figure 4: Illustration of discrepancies among invalid CoT prompts. For more
    details, see Appendix [C](#A3 "Appendix C Appendix: Full Prompts for Amateurs
    Model ‣ Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive
    Decoding and Distillation").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：无效 CoT 提示之间差异的说明。有关更多详细信息，请参见附录 [C](#A3 "附录 C 附录：业余模型的完整提示 ‣ 蒸馏对比解码：通过对比解码和蒸馏提升
    LLMs 推理能力")。
- en: 5 Experimental Settings
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验设置
- en: 5.1 Benchmarks
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基准测试
- en: 'To obtain results, we evaluated two domains of text generation: arithmetic
    reasoning and commonsense reasoning. For arithmetic reasoning, we utilized the
    GSM8K dataset Cobbe et al. ([2021](#bib.bib7)), and for commonsense reasoning,
    the StrategyQA dataset Geva et al. ([2021](#bib.bib9)) was employed.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得结果，我们评估了两个文本生成领域：算术推理和常识推理。对于算术推理，我们使用了 GSM8K 数据集 Cobbe 等人 ([2021](#bib.bib7))，而对于常识推理，则使用了
    StrategyQA 数据集 Geva 等人 ([2021](#bib.bib9))。
- en: 5.1.1 Arithmetic Reasoning
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 算术推理
- en: The GSM8K dataset Cobbe et al. ([2021](#bib.bib7)) is structured to facilitate
    question answering on fundamental mathematical problems that require multi-step
    reasoning for resolution. The solutions to these problems primarily involve performing
    a sequence of elementary calculations using basic arithmetic operations, including
    addition, subtraction, multiplication, and division. In our experimental setup,
    we employed the complete test set, which consisted of 1319 samples. We utilized
    an 8-shot for the expert model and a 3-shot (using synthetic demonstrations) for
    the amateur model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: GSM8K 数据集 Cobbe 等人 ([2021](#bib.bib7)) 结构化为促进对基本数学问题的问答，这些问题需要多步骤推理才能解决。这些问题的解决方案主要涉及使用基本的四则运算（包括加法、减法、乘法和除法）进行一系列初级计算。在我们的实验设置中，我们使用了完整的测试集，共包含
    1319 个样本。我们为专家模型采用了 8-shot 方法，为业余模型采用了 3-shot 方法（使用合成示例）。
- en: 5.1.2 Commonsense Reasoning
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 常识推理
- en: The StrategyQA dataset Geva et al. ([2021](#bib.bib9)) is a question-answering
    benchmark focusing on open-domain questions, requiring implicit reasoning to infer
    the necessary steps from the question itself through a strategic approach. It
    is designed to evaluate the ability to perform implicit reasoning, necessary for
    answering questions that do not have direct or explicit answers within the text.
    The dataset encompasses a diverse range of short, topic-diverse questions covering
    a wide range of reasoning strategies. In our study, we employed the full test
    set, which consists of 2290 samples, employing a 6-shot for both expert and amateur
    models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: StrategyQA 数据集 Geva 等人 ([2021](#bib.bib9)) 是一个关注开放领域问题的问答基准，需要通过战略方法从问题本身推断出必要步骤。它旨在评估执行隐性推理的能力，这对于回答文本中没有直接或明确答案的问题至关重要。该数据集涵盖了各种短小且主题多样的问题，涵盖了广泛的推理策略。在我们的研究中，我们使用了完整的测试集，共包含
    2290 个样本，针对专家和业余模型都采用了 6-shot 方法。
- en: 5.2 Baselines
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 基线
- en: 'We compare Distillation Contrastive Decoding (DCD) with three decoding intervention
    baselines: Contrastive Chain-of-thought Prompting (CP), Contrastive Decoding (CD),
    and DoLA Chuang et al. ([2023](#bib.bib6)). For each baseline, we adhered to the
    original setup’s hyperparameters. For CD, we set $\alpha$ with a step of 13B models,
    both with the step of 2\. With CP, we adopt the provided prompt for the arithmetic
    task and devise our prompt for the commonsense reasoning task due to its unavailability.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将蒸馏对比解码（DCD）与三种解码干预基线进行比较：对比思维链提示（CP）、对比解码（CD）和 DoLA Chuang 等人 ([2023](#bib.bib6))。对于每个基线，我们遵循了原始设置的超参数。对于
    CD，我们将 $\alpha$ 设置为 13B 模型的步骤，两者的步骤为 2\. 对于 CP，我们采用了提供的算术任务提示，并为常识推理任务设计了我们的提示，因为没有现成的提示。
- en: 5.3 Models and Hyperparameters
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 模型和超参数
- en: We conducted experiments with Distillation Contrastive Decoding (DCD) on the
    Llama 1&2 Touvron et al. ([2023a](#bib.bib18), [b](#bib.bib19)), Mistral Jiang
    et al. ([2023](#bib.bib11)), and DeepSeek DeepSeek-AI et al. ([2024](#bib.bib8))
    models. For the Llama models, we engaged both the 7B and 13B variants. Meanwhile,
    we utilized the 7B versions for both Mistral and DeepSeek.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Llama 1&2 Touvron 等人 ([2023a](#bib.bib18), [b](#bib.bib19))、Mistral Jiang
    等人 ([2023](#bib.bib11)) 和 DeepSeek DeepSeek-AI 等人 ([2024](#bib.bib8)) 模型上进行了蒸馏对比解码（DCD）实验。对于
    Llama 模型，我们使用了 7B 和 13B 两种变体。同时，我们对 Mistral 和 DeepSeek 都使用了 7B 版本。
- en: 'In our experiments, we controlled four distinct parameters: $\alpha$, is described
    in Section [7.1](#S7.SS1 "7.1 Dropout Rate ‣ 7 Distillation Methods ‣ Distillation
    Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的实验中，我们控制了四个不同的参数：$\alpha$，具体描述见第 [7.1](#S7.SS1 "7.1 Dropout Rate ‣ 7 Distillation
    Methods ‣ Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive
    Decoding and Distillation") 节。'
- en: '| Model | Method | GSM8K | StrategyQA |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Model | Method | GSM8K | StrategyQA |'
- en: '| --- | --- | --- | --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Llama2-7B | Greedy | 14.32 | 60.04 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B | Greedy | 14.32 | 60.04 |'
- en: '| CP | 14.25 | 59.91 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| CP | 14.25 | 59.91 |'
- en: '| CD | 15.39 | 61.62 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| CD | 15.39 | 61.62 |'
- en: '| DoLA | 14.03 | 64.02 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| DoLA | 14.03 | 64.02 |'
- en: '| CP + CD | 16.00 | 63.23 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| CP + CD | 16.00 | 63.23 |'
- en: '| DCD[Dropout] (Ours) | 17.28 | 65.15 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| DCD[Dropout] (Ours) | 17.28 | 65.15 |'
- en: '| DCD[Quantization] (Ours) | 16.00 | 63.18 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| DCD[Quantization] (Ours) | 16.00 | 63.18 |'
- en: '| DCD[Dropout + Quantization] (Ours) | 16.00 | 63.32 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| DCD[Dropout + Quantization] (Ours) | 16.00 | 63.32 |'
- en: '| DeepSeek-7B | Greedy | 12.74 | 60.00 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-7B | Greedy | 12.74 | 60.00 |'
- en: '| CP | 14.40 | 59.00 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| CP | 14.40 | 59.00 |'
- en: '| CD | - | - |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| CD | - | - |'
- en: '| DoLA | 10.37 | 55.10 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| DoLA | 10.37 | 55.10 |'
- en: '| CP + CD | 15.47 | 62.40 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| CP + CD | 15.47 | 62.40 |'
- en: '| DCD[Dropout] (Ours) | 15.47 | 62.40 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| DCD[Dropout] (Ours) | 15.47 | 62.40 |'
- en: '| DCD[Quantization] (Ours) | 16.38 | 62.01 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| DCD[量化]（我们的）| 16.38 | 62.01 |'
- en: '| DCD[Dropout + Quantization] (Ours) | 16.38 | 62.01 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| DCD[丢弃 + 量化]（我们的）| 16.38 | 62.01 |'
- en: '| Mistral-7B | Greedy | 42.23 | 69.04 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 贪婪 | 42.23 | 69.04 |'
- en: '| CP | 38.90 | 67.73 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| CP | 38.90 | 67.73 |'
- en: '| CD | - | - |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| CD | - | - |'
- en: '| DoLA | 43.60 | 70.74 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| DoLA | 43.60 | 70.74 |'
- en: '| CP + CD | 47.08 | 73.45 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| CP + CD | 47.08 | 73.45 |'
- en: '| DCD[Dropout] (Ours) | 48.98 | 74.02 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| DCD[丢弃]（我们的）| 48.98 | 74.02 |'
- en: '| DCD[Quantization] (Ours) | 47.20 | 72.71 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| DCD[量化]（我们的）| 47.20 | 72.71 |'
- en: '| DCD[Dropout + Quantization] (Ours) | 48.60 | 73.41 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| DCD[丢弃 + 量化]（我们的）| 48.60 | 73.41 |'
- en: '| Llama2-13B | Greedy | 29.42 | 65.20 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13B | 贪婪 | 29.42 | 65.20 |'
- en: '| CP | 25.78 | 66.10 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| CP | 25.78 | 66.10 |'
- en: '| CD | 32.83 | 69.90 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| CD | 32.83 | 69.90 |'
- en: '| DoLA | 28.81 | 68.47 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| DoLA | 28.81 | 68.47 |'
- en: '| CP + CD | 31.62 | 69.65 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| CP + CD | 31.62 | 69.65 |'
- en: '| DCD[Dropout] (Ours) | 33.21 | 71.10 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| DCD[丢弃]（我们的）| 33.21 | 71.10 |'
- en: '| DCD[Quantization] (Ours) | 31.30 | 70.60 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| DCD[量化]（我们的）| 31.30 | 70.60 |'
- en: '| DCD[Dropout + Quantization] (Ours) | 32.20 | 70.90 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| DCD[丢弃 + 量化]（我们的）| 32.20 | 70.90 |'
- en: 'Table 1: Reasoning scores comparison of Distillation Contrastive Decoding (DCD)
    with other existing methods: Contrastive Prompting (CP)Chia et al. ([2023](#bib.bib5)),
    Contrastive Decoding (CD) Li et al. ([2023b](#bib.bib14)), and DoLA Chuang et al.
    ([2023](#bib.bib6)). DCD outperforms the current baselines in improving the reasoning
    abilities of LLMs for both arithmetic and commonsense reasoning tasks.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：DCD（蒸馏对比解码）与其他现有方法的推理得分比较：对比提示（CP）Chia 等人 ([2023](#bib.bib5))，对比解码（CD）Li
    等人 ([2023b](#bib.bib14))，以及 DoLA Chuang 等人 ([2023](#bib.bib6))。DCD 在提升 LLM 的算术和常识推理任务能力方面优于当前基准。
- en: 6 Results
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结果
- en: '![Refer to caption](img/0eb1dcf713106744cde6f75fe43c1d4a.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0eb1dcf713106744cde6f75fe43c1d4a.png)'
- en: 'Figure 5: Relationship between MMLU Score and Improvement on GSM8K. Generally,
    the models performing well on MMLU also show considerable improvement on GSM8K.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：MMLU 得分与 GSM8K 改进之间的关系。通常，在 MMLU 上表现良好的模型在 GSM8K 上也会有显著的改进。
- en: 'The main results on Llama2, Mistral, and DeepSeek models are shown in Table
    [1](#S5.T1 "Table 1 ‣ 5.3 Models and Hyperparameters ‣ 5 Experimental Settings
    ‣ Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive
    Decoding and Distillation"). We report the Llama1 results in Appendix [D](#A4
    "Appendix D Llama1 Results ‣ Distillation Contrastive Decoding: Improving LLMs
    Reasoning with Contrastive Decoding and Distillation") for reference. The results
    demonstrate that our proposed DCD method outperforms existing methods on both
    the GSM8K and StrategyQA datasets. On GSM8K, DCD outperforms CD by $1.89\%$.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Llama2、Mistral 和 DeepSeek 模型的主要结果如表 [1](#S5.T1 "表 1 ‣ 5.3 模型和超参数 ‣ 5 实验设置 ‣
    蒸馏对比解码：通过对比解码和蒸馏提升 LLM 的推理能力") 所示。我们在附录 [D](#A4 "附录 D Llama1 结果 ‣ 蒸馏对比解码：通过对比解码和蒸馏提升
    LLM 的推理能力") 中报告了 Llama1 的结果以供参考。结果表明，我们提出的 DCD 方法在 GSM8K 和 StrategyQA 数据集上均优于现有方法。在
    GSM8K 上，DCD 比 CD 高出 $1.89\%$。
- en: 'DCD with dropout consistency outperforms other distillation approaches like
    quantization and combined quantization with dropout. This finding contradicts
    previous findings that performance benefits from smaller amateur models Li et al.
    ([2023b](#bib.bib14)); O’Brien and Lewis ([2023](#bib.bib16)). We further study
    the effect of different quantization methods in Section [8](#S7.F8 "Figure 8 ‣
    7.2 Quantization Amateur Model ‣ 7 Distillation Methods ‣ Distillation Contrastive
    Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation").'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用丢弃一致性的 DCD 优于其他蒸馏方法，如量化和量化与丢弃的组合。这一发现与之前的结果相矛盾，之前的研究表明性能受益于较小的业余模型 Li 等人 ([2023b](#bib.bib14))；O’Brien
    和 Lewis ([2023](#bib.bib16))。我们在第 [8](#S7.F8 "图 8 ‣ 7.2 量化业余模型 ‣ 7 蒸馏方法 ‣ 蒸馏对比解码：通过对比解码和蒸馏提升
    LLM 的推理能力") 节进一步研究不同量化方法的效果。
- en: 'Interestingly, we observe that there is a correlation between the base knowledge
    of the model and DCD (Figure  [5](#S6.F5 "Figure 5 ‣ 6 Results ‣ Distillation
    Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation"))
    which does not apply to previous methods like CP. As the model achieves a higher
    MMLU score Hendrycks et al. ([2020](#bib.bib10)), DCD becomes more effective when
    employed. For example, there is a $+6.8\%$ on Llama1-7B in the arithmetic reasoning
    GSM8K task. This shows the adaptability of DCD to newer and stronger base models.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '有趣的是，我们观察到模型的基础知识与DCD之间存在相关性（图 [5](#S6.F5 "Figure 5 ‣ 6 Results ‣ Distillation
    Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation")），而这种相关性在以前的方法如CP中并不存在。随着模型MMLU分数的提高Hendrycks等人（[2020](#bib.bib10)），DCD在应用时变得更加有效。例如，在Llama1-7B的算术推理GSM8K任务中有$+6.8\%$的提升。这展示了DCD对更新和更强大的基础模型的适应性。'
- en: 'We also find that DCD usually leads to fewer generated tokens compared to CD
    and CP baselines in Figure [6](#S6.F6 "Figure 6 ‣ 6 Results ‣ Distillation Contrastive
    Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation").
    This supports the finding from Wei et al. ([2023](#bib.bib21)) that generating
    more CoT tokens can be subjected to error flaws in reasoning thus affecting the
    final results.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还发现，与CD和CP基线相比，DCD通常导致生成的标记更少，如图 [6](#S6.F6 "Figure 6 ‣ 6 Results ‣ Distillation
    Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation")所示。这支持了Wei等人（[2023](#bib.bib21)）的发现，即生成更多的CoT标记可能会受到推理错误的影响，从而影响最终结果。'
- en: '![Refer to caption](img/b7f84de856a87d65f180595f9352aabb.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b7f84de856a87d65f180595f9352aabb.png)'
- en: 'Figure 6: Comparison of average generate token of different methods on Llama2-7B
    model.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：Llama2-7B模型不同方法的平均生成标记比较。
- en: 7 Distillation Methods
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 蒸馏方法
- en: In this section, we explore different distillation settings in Distillation
    Contrastive Decoding.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了Distillation Contrastive Decoding中的不同蒸馏设置。
- en: 7.1 Dropout Rate
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 丢弃率
- en: We conducted experiments with varying dropout rates ranging from $0.1$ is optimal
    in most cases for both arithmetic and commonsense reasoning.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了不同丢弃率的实验，发现$0.1$在大多数情况下对算术和常识推理都是最佳的。
- en: '![Refer to caption](img/d472069cd81efd31514597cb821c094a.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d472069cd81efd31514597cb821c094a.png)'
- en: 'Figure 7: The performance of LLama2-7B across different dropout rates on both
    arithmetic and commonsense problems. Demonstrating the dropout peak instead of
    ascending. Notably, the arithmetic task imposes an amateur penalty of 0.3 with
    CoT instruction and the commonsense task imposes a penalty of 0.7 with CoT incoherent
    facts.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：LLama2-7B在不同丢弃率下在算术和常识问题上的表现。显示出丢弃率的峰值而非递增。值得注意的是，算术任务对CoT指令施加了0.3的业余罚分，而常识任务对CoT不一致事实施加了0.7的罚分。
- en: 7.2 Quantization Amateur Model
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 量化业余模型
- en: '![Refer to caption](img/9bd2f41a302f0a65564bde1d00f02880.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9bd2f41a302f0a65564bde1d00f02880.png)'
- en: 'Figure 8: Comparision of different quantization methods applied to simulate
    amateur models on Llama2-7B with the arithmetic problem, demonstrating that smaller
    amateur models do not invariably enhance performance.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：应用于Llama2-7B算术问题的不同量化方法的比较，表明较小的业余模型并不总是能提升性能。
- en: The premise that smaller-scale amateur models yield superior performance has
    been explored in CD Li et al. ([2023b](#bib.bib14)). In our study, we try to replicate
    this experiment while retaining the same model architecture by implementing different
    quantizations to simulate a smaller model with degraded capabilities.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 小规模的业余模型表现更佳的前提在CD Li等人（[2023b](#bib.bib14)）中已被探讨。在我们的研究中，我们尝试在保持相同模型架构的同时，通过实现不同的量化来模拟具有降级能力的小模型，以重复这一实验。
- en: 'We observe that simply reducing the bit size of the amateur model does not
    invariably enhance the decoding process. Figure [8](#S7.F8 "Figure 8 ‣ 7.2 Quantization
    Amateur Model ‣ 7 Distillation Methods ‣ Distillation Contrastive Decoding: Improving
    LLMs Reasoning with Contrastive Decoding and Distillation") shows that all of
    the tested quantization amateurs give a lower reasoning accuracy than the original
    amateur. These observations suggest that opting for smaller amateur models might
    not always yield the best performance. This insight underscores the motivation
    behind developing our Distillation Contrastive Prompting method to address the
    limitations posed by the need for an amateur model smaller than 7B in Contrastive
    Decoding Li et al. ([2023b](#bib.bib14)).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '我们观察到，简单地减少业余模型的位大小并不一定能改善解码过程。图[8](#S7.F8 "Figure 8 ‣ 7.2 Quantization Amateur
    Model ‣ 7 Distillation Methods ‣ Distillation Contrastive Decoding: Improving
    LLMs Reasoning with Contrastive Decoding and Distillation")显示，所有测试的量化业余模型的推理准确性都低于原始业余模型。这些观察结果表明，选择更小的业余模型可能并不总能带来最佳性能。这一洞察突显了开发我们的蒸馏对比提示方法的动机，以应对在对比解码Li等（[2023b](#bib.bib14)）中对小于7B的业余模型的需求所带来的局限性。'
- en: 8 Conclusion
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: In this work, we address the limitations associated with Contrastive Decoding,
    particularly its dependency on small amateur models within the same family as
    the expert models. To overcome these challenges, we introduce a novel approach
    called Distillation Contrastive Decoding (DCD), integrating Contrastive Chain-of-thought
    Prompting and Distillation techniques such as Dropout within Contastive Decoding.
    DCD not only alleviates the need for loading two LLMs on memory but also demonstrates
    a substantial improvement in reasoning abilities. Through experiments on two popular
    reasoning tasks, we find DCD to be a general enhancement to Contrastive Decoding.
    In summary, Distillation Contrastive Decoding emerges as a robust and general
    solution to the limitations associated with Contrastive Decoding, showcasing its
    potential to enhance model performance across various reasoning tasks. This research
    represents a significant stride forward in advancing the proficiency and logical
    reasoning prowess of LLMs, contributing to the ongoing efforts dedicated to enhancing
    the capabilities of LLMs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们解决了与对比解码相关的局限性，特别是它对与专家模型同一系列的小业余模型的依赖。为克服这些挑战，我们引入了一种名为蒸馏对比解码（DCD）的新方法，将对比链式思维提示和诸如Dropout的蒸馏技术整合到对比解码中。DCD不仅缓解了在内存中加载两个LLM的需求，还显著提高了推理能力。通过对两个流行的推理任务进行实验，我们发现DCD是对对比解码的通用增强。总之，蒸馏对比解码作为一种强大且通用的解决方案，展示了其在各种推理任务中提高模型性能的潜力。这项研究标志着在提高LLM的能力和逻辑推理能力方面迈出了重要一步，为提升LLM的能力做出了持续贡献。
- en: 9 Limitation and Future Work
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 局限性和未来工作
- en: While our study has provided valuable insights into the effectiveness of Distillation
    Contrastive Decoding, it is crucial to acknowledge certain limitations that need
    to be addressed.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的研究提供了有关蒸馏对比解码有效性的宝贵见解，但必须承认一些需要解决的局限性。
- en: First, our investigation mainly focuses on base models. Although we suggest
    that our method could potentially be applied to larger, tuned models, exploring
    its impact on instruction following represents a promising research direction.
    Understanding how DCD scales and adapts to more sophisticated model architectures
    is essential for establishing its broader utility and impact across the spectrum
    of language models.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们的调查主要集中在基础模型上。尽管我们建议我们的方法可能适用于更大、更精调的模型，但探索其对指令跟随的影响仍然是一个有前景的研究方向。理解DCD如何扩展并适应更复杂的模型架构对于建立其在语言模型领域的更广泛实用性和影响至关重要。
- en: Second, although our extensive experiments showcase the substantial improvements
    achieved by DCD across various settings, our exploration has not delved into more
    complex reasoning tasks. Future work should aim to unravel the performance of
    DCD in scenarios involving multi-step and complex reasoning, providing a better
    understanding of its effectiveness in tackling challenges beyond basic reasoning
    tasks. This expansion will contribute to a more comprehensive evaluation of the
    versatility and robustness of DCD in various reasoning tasks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，虽然我们的广泛实验展示了 DCD 在各种设置下所取得的显著改进，但我们的探索尚未深入研究更复杂的推理任务。未来的工作应旨在揭示 DCD 在涉及多步骤和复杂推理的场景中的表现，以提供对其在处理超越基本推理任务的挑战方面的有效性的更好理解。这一扩展将有助于更全面地评估
    DCD 在各种推理任务中的多样性和稳健性。
- en: References
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. 2023. Qwen technical report. *arXiv preprint arXiv:2309.16609*.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人（2023）Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng,
    Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
    Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
    Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, 和 Tianhang Zhu。2023. Qwen 技术报告。*arXiv 预印本 arXiv:2309.16609*。
- en: 'Bricken et al. (2023) Trenton Bricken, Adly Templeton, Joshua Batson, Brian
    Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda
    Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell,
    Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean,
    Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah.
    2023. Towards monosemanticity: Decomposing language models with dictionary learning.
    *Transformer Circuits Thread*. Https://transformer-circuits.pub/2023/monosemantic-features/index.html.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bricken 等人（2023）Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen,
    Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell,
    Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas
    Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah
    E Burke, Tristan Hume, Shan Carter, Tom Henighan, 和 Christopher Olah。2023. 朝向单一语义性：通过字典学习解构语言模型。*Transformer
    Circuits Thread*。Https://transformer-circuits.pub/2023/monosemantic-features/index.html。
- en: Brown et al. (2020a) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020a. [Language models are few-shot learners](http://arxiv.org/abs/2005.14165).
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020a）Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei。2020a. [语言模型是少样本学习者](http://arxiv.org/abs/2005.14165)。
- en: Brown et al. (2020b) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020b. [Language models are few-shot learners](http://arxiv.org/abs/2005.14165).
    *CoRR*, abs/2005.14165.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020b）Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei。2020b. [语言模型是少样本学习者](http://arxiv.org/abs/2005.14165)。*CoRR*，abs/2005.14165。
- en: Chia et al. (2023) Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria,
    and Lidong Bing. 2023. [Contrastive chain-of-thought prompting](http://arxiv.org/abs/2311.09277).
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chia 等（2023）谢宇宸、陈贵真、吕安春、索贾尼亚·波里亚和李东冰。2023。[对比链式思维提示](http://arxiv.org/abs/2311.09277)。
- en: 'Chuang et al. (2023) Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James
    Glass, and Pengcheng He. 2023. [Dola: Decoding by contrasting layers improves
    factuality in large language models](http://arxiv.org/abs/2309.03883).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chuang 等（2023）庄永胜、谢雨佳、罗宏银、金云、詹姆斯·格拉斯和彭程·赫。2023。[Dola: 通过对比层解码改善大型语言模型的事实性](http://arxiv.org/abs/2309.03883)。'
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve
    math word problems. *arXiv preprint arXiv:2110.14168*.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等（2021）卡尔·科布、维尼特·科萨拉朱、穆罕默德·巴瓦里安、马克·陈、朱浩和卢卡斯·凯瑟、马蒂亚斯·普拉普特、杰瑞·特沃雷克、雅各布·希尔顿、内村礼一郎、克里斯托弗·赫斯和约翰·舒尔曼。2021。训练验证器以解决数学应用题。*arXiv
    预印本 arXiv:2110.14168*。
- en: 'DeepSeek-AI et al. (2024) DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen,
    Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe
    Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong
    Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei
    Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu,
    Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong
    Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui
    Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang
    Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui
    Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong,
    Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai
    Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua
    Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng
    Zhou, Qihao Zhu, and Yuheng Zou. 2024. [Deepseek llm: Scaling open-source language
    models with longtermism](http://arxiv.org/abs/2401.02954).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DeepSeek-AI 等（2024）DeepSeek-AI：、肖碧、邓丽、陈冠廷、陈山黄、大麦·戴、邓成齐、丁宏辉、董凯、杜秋实、付哲、郭华左、高凯歌、郭文军、葛锐琦、关康、郭大亚、郭建中、郝光博、郝哲文、何英、胡文杰、黄盼盼、李尔杭、李国伟、李佳士、李耀、李永康、梁文锋、林芳云、刘安鑫、刘波、刘文、刘晓东、刘欣、刘艺元、陆浩宇、陆尚浩、罗富丽、马世荣、聂晓涛、裴天、彭毅、邱俊杰、曲辉、任同政、任泽辉、阮崇、沙张力、邵志洪、宋俊晓、苏学成、孙晶祥、孙耀峰、唐名辉、王冰轩、王佩怡、王诗雨、王耀辉、王永吉、吴同、吴玉、谢鑫、谢振达、谢子伟、熊义梁、徐汉伟、徐瑞熙、徐彦宏、尤宇翔、余水平、于兴凯、张冰、张浩伟、张乐聪、张丽悦、张名川、张名华、张文韬、张艺超、赵成刚、赵瑶、周尚焰、周顺峰、朱启浩和邹宇恒。2024。[Deepseek
    llm: 扩展开源语言模型的长期主义](http://arxiv.org/abs/2401.02954)。'
- en: Geva et al. (2021) Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth,
    and Jonathan Berant. 2021. Did Aristotle Use a Laptop? A Question Answering Benchmark
    with Implicit Reasoning Strategies. *Transactions of the Association for Computational
    Linguistics (TACL)*.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geva 等（2021）摩尔·盖瓦、丹尼尔·哈沙比、埃拉德·塞加尔、图沙尔·科特、丹·罗斯和乔纳森·贝朗特。2021。亚里士多德使用过笔记本电脑吗？一个具有隐式推理策略的问答基准。*计算语言学协会会刊（TACL）*。
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. [Measuring massive multitask
    language understanding](http://arxiv.org/abs/2009.03300). *CoRR*, abs/2009.03300.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等（2020）丹·亨德里克斯、科林·伯恩斯、史蒂文·巴萨特、安迪·邹、曼塔斯·马泽卡、道恩·宋和雅各布·斯坦赫特。2020。[测量大规模多任务语言理解](http://arxiv.org/abs/2009.03300)。*CoRR*，abs/2009.03300。
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. [Mistral 7b](http://arxiv.org/abs/2310.06825).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2023）阿尔伯特·Q·姜、亚历山大·萨布莱罗勒斯、亚瑟·门施、克里斯·班福德、德文德拉·辛格·查普洛特、迭戈·德·拉斯·卡萨斯、弗洛里安·布雷桑、吉安娜·伦吉尔、吉约姆·兰普勒、卢西尔·索尔尼埃、莱利奥·勒纳尔·拉沃、玛丽-安·拉肖、皮埃尔·斯托克、特文·勒·斯卡奥、蒂博·拉夫里尔、托马斯·王、提摩太·拉克鲁瓦和威廉·艾尔·赛义德。2023。[Mistral
    7b](http://arxiv.org/abs/2310.06825)。
- en: Kojima et al. (2023) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2023. [Large language models are zero-shot reasoners](http://arxiv.org/abs/2205.11916).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima 等（2023）小岛武志、石香·肖恩·顾、马切尔·里德、松尾丰和岩泽勇介。2023。[大型语言模型是零样本推理者](http://arxiv.org/abs/2205.11916)。
- en: 'Li et al. (2023a) Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister,
    and Martin Wattenberg. 2023a. [Inference-time intervention: Eliciting truthful
    answers from a language model](http://arxiv.org/abs/2306.03341).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023a) Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister,
    和 Martin Wattenberg. 2023a. [推理时间干预：从语言模型中引出真实答案](http://arxiv.org/abs/2306.03341)。
- en: 'Li et al. (2023b) Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason
    Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2023b. [Contrastive
    decoding: Open-ended text generation as optimization](http://arxiv.org/abs/2210.15097).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023b) Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason
    Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, 和 Mike Lewis. 2023b. [对比解码：开放式文本生成作为优化](http://arxiv.org/abs/2210.15097)。
- en: Ling et al. (2023) Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee,
    Roland Memisevic, and Hao Su. 2023. [Deductive verification of chain-of-thought
    reasoning](http://arxiv.org/abs/2306.03872).
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling et al. (2023) Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee,
    Roland Memisevic, 和 Hao Su. 2023. [链式思维推理的演绎验证](http://arxiv.org/abs/2306.03872)。
- en: O’Brien and Lewis (2023) Sean O’Brien and Mike Lewis. 2023. [Contrastive decoding
    improves reasoning in large language models](http://arxiv.org/abs/2309.09117).
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Brien 和 Lewis (2023) Sean O’Brien 和 Mike Lewis. 2023. [对比解码提升大语言模型的推理能力](http://arxiv.org/abs/2309.09117)。
- en: Press et al. (2023) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A.
    Smith, and Mike Lewis. 2023. [Measuring and narrowing the compositionality gap
    in language models](http://arxiv.org/abs/2210.03350).
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press et al. (2023) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah
    A. Smith, 和 Mike Lewis. 2023. [衡量和缩小语言模型中的组成性差距](http://arxiv.org/abs/2210.03350)。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023a. [Llama: Open and efficient foundation language models](http://arxiv.org/abs/2302.13971).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, 和
    Guillaume Lample. 2023a. [Llama：开放且高效的基础语言模型](http://arxiv.org/abs/2302.13971)。
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b.
    [Llama 2: Open foundation and fine-tuned chat models](http://arxiv.org/abs/2307.09288).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, 和 Thomas Scialom. 2023b. [Llama
    2：开放基础和微调聊天模型](http://arxiv.org/abs/2307.09288)。
- en: 'Wang et al. (2023) Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu,
    Luke Zettlemoyer, and Huan Sun. 2023. [Towards understanding chain-of-thought
    prompting: An empirical study of what matters](https://doi.org/10.18653/v1/2023.acl-long.153).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 2717–2739, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu,
    Luke Zettlemoyer, 和 Huan Sun. 2023. [深入理解链式思维提示：一个关于关键因素的实证研究](https://doi.org/10.18653/v1/2023.acl-long.153)。在*第61届计算语言学协会年会（第1卷：长篇论文）*上，页码2717–2739，加拿大多伦多。计算语言学协会。
- en: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. [Chain-of-thought prompting
    elicits reasoning in large language models](http://arxiv.org/abs/2201.11903).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, 和 Denny Zhou. 2023. [思维链提示在大型语言模型中引发推理](http://arxiv.org/abs/2201.11903)。
- en: 'Zou et al. (2023) Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip
    Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski,
    Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven
    Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks.
    2023. [Representation engineering: A top-down approach to ai transparency](http://arxiv.org/abs/2310.01405).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等人 (2023) Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo,
    Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski,
    Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven
    Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, 和 Dan Hendrycks.
    2023. [表示工程：一种自上而下的 AI 透明性方法](http://arxiv.org/abs/2310.01405)。
- en: '![Refer to caption](img/c5bae01c0ca7663a00ba3e005f4d43e0.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c5bae01c0ca7663a00ba3e005f4d43e0.png)'
- en: 'Figure 9: An example of arithmetic reasonings completions across 3 methods:
    CP, CD, and DCD (Ours).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：三个方法（CP、CD 和 DCD (我们的)）的算术推理完成示例。
- en: Appendix A Components of a Chain-of-thought Demonstration
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 思维链演示的组件
- en: 'Wang et al. ([2023](#bib.bib20)) indicates that there are two main components
    of a CoT example:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 ([2023](#bib.bib20)) 指出，CoT 示例有两个主要组件：
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Bridging Objects: Essential elements required for successful predictions. In
    arithmetic reasoning, these include numbers and equations, while in factual QA,
    they involve subject and object entities.'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 桥接对象：成功预测所需的基本元素。在算术推理中，这些包括数字和方程，而在事实 QA 中，它们涉及主客体实体。
- en: •
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Language Templates: Textual hints and relational predicates that complement
    bridging objects, guiding the model in the reasoning process.'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语言模板：文本提示和关系谓词，补充桥接对象，引导模型进行推理过程。
- en: '![Refer to caption](img/a6fae69cb1f4f71df73e3353b079b818.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a6fae69cb1f4f71df73e3353b079b818.png)'
- en: 'Figure 10: Example of bridging objects and language templates components of
    a CoT demonstration. The examples are from Wang et al. ([2023](#bib.bib20)); Cobbe
    et al. ([2021](#bib.bib7)); Press et al. ([2023](#bib.bib17)).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：CoT 演示中的桥接对象和语言模板组件示例。示例来源于 Wang 等人 ([2023](#bib.bib20))；Cobbe 等人 ([2021](#bib.bib7))；Press
    等人 ([2023](#bib.bib17))。
- en: 'Appendix B Appendix: Full Prompts for Experts Model'
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 附录：专家模型的完整提示
- en: B.1 GSM8K
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 GSM8K
- en: [PRE0]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: [PRE0]
- en: B.2 StrategyQA
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 StrategyQA
- en: [PRE1]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: [PRE1]
- en: 'Appendix C Appendix: Full Prompts for Amateurs Model'
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 附录：业余模型的完整提示
- en: C.1 GSM8K
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 GSM8K
- en: C.1.1 Rule-based Number Shuffle
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.1.1 基于规则的数字洗牌
- en: [PRE2]
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: [PRE2]
- en: C.1.2 Rule-based Number Shuffle with Calculation Error
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.1.2 基于规则的数字洗牌与计算错误
- en: [PRE3]
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: [PRE3]
- en: C.1.3 Rule-based Number Shuffle with Irrelerive objects and Exchange Sign
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.1.3 基于规则的数字洗牌与无关对象和交换符号
- en: [PRE4]
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: [PRE4]
- en: C.1.4 Synthetic Demonstration
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.1.4 合成演示
- en: [PRE5]
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: [PRE5]
- en: C.2 StrategyQA
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 StrategyQA
- en: C.2.1 Synthetic Demonstration
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.2.1 合成演示
- en: [PRE6]
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: [PRE6]
- en: Appendix D Llama1 Results
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D Llama1 结果
- en: '| Model | Method | GSM8K | StrategyQA |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | GSM8K | StrategyQA |'
- en: '| --- | --- | --- | --- |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Llama1-7B | - | 11.37 | 58.82 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Llama1-7B | - | 11.37 | 58.82 |'
- en: '| CP | 9.48 | 58.60 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| CP | 9.48 | 58.60 |'
- en: '| CD | 11.45 | 61.79 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| CD | 11.45 | 61.79 |'
- en: '| DoLA | 10.5 | 64.1 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| DoLA | 10.5 | 64.1 |'
- en: '| DCD (Ours) | 12.1 | 63.4 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| DCD (我们的) | 12.1 | 63.4 |'
- en: '| Llama1-13B | - | 17.13 | 65.46 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Llama1-13B | - | 17.13 | 65.46 |'
- en: '| CP | 17.66 | 61.62 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| CP | 17.66 | 61.62 |'
- en: '| CD | 19.79 | 62.67 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| CD | 19.79 | 62.67 |'
- en: '| DoLA | 18.0 | 67.6 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| DoLA | 18.0 | 67.6 |'
- en: '| DCD (Ours) | 20.02 | 65.81 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| DCD (我们的) | 20.02 | 65.81 |'
- en: 'Table 2: Reasoning scores comparison of Distillation Contrastive Decoding (DCD)
    with other existing methods: Contrastive Prompting (CP)Chia et al. ([2023](#bib.bib5)),
    Contrastive Decoding (CD) Li et al. ([2023b](#bib.bib14)), and DoLA Chuang et al.
    ([2023](#bib.bib6)) on Llama1-7B and Llama1-13B models.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：Distillation Contrastive Decoding (DCD) 与其他现有方法的推理评分比较：Contrastive Prompting
    (CP) Chia 等人 ([2023](#bib.bib5))，Contrastive Decoding (CD) Li 等人 ([2023b](#bib.bib14))，和
    DoLA Chuang 等人 ([2023](#bib.bib6)) 在 Llama1-7B 和 Llama1-13B 模型上的表现。
- en: Appendix E Exploring the Impact of Dropout Rates on Model Accuracy
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 探索 dropout 率对模型准确性的影响
- en: '![Refer to caption](img/5aad611ca3b94db4234b4dbf30d8c423.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5aad611ca3b94db4234b4dbf30d8c423.png)'
- en: 'Figure 11: Comparison of reasoning scores on the GSM8K and StrategyQA datasets
    utilizing Distillation Contrastive Decoding (DCD) with Llama2-7B. Our findings
    indicate that optimal accuracy is achieved by initially determining the appropriate
    value for $\beta$ (the dropout rate).'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：在GSM8K和StrategyQA数据集上，使用Llama2-7B进行蒸馏对比解码（DCD）的推理得分比较。我们的发现表明，通过最初确定适当的$\beta$（丢弃率）可以实现最佳准确度。
