- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:51:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Prompt, Plan, Perform: LLM-based Humanoid Control via Quantized Imitation Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Prompt, Plan, Perform: 基于LLM的类人控制通过量化模仿学习'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.11359](https://ar5iv.labs.arxiv.org/html/2309.11359)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.11359](https://ar5iv.labs.arxiv.org/html/2309.11359)
- en: '{textblock*}'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '{textblock*}'
- en: (2cm,0.5cm)This work has been submitted to the IEEE for possible publication.
    Copyright may be transferred without notice, after which this version may no longer
    be accessible.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: (2cm,0.5cm)本工作已提交至IEEE以备可能出版。版权可能在未通知的情况下转移，转移后此版本可能不再可用。
- en: Jingkai Sun^(1,∗), Qiang Zhang^(1,∗), Yiqun Duan², Xiaoyang Jiang¹, Chong Cheng¹
    and Renjing Xu^(1,†) ^∗ are equal contributors, ^† is the corresponding author¹The
    authors are with The Hong Kong University of Science and Technology (Guangzhou),
    China. {jsun444, qzhang749, ccheng735}@connect.hkust-gz.edu.cn, jxxxxxyGOAT@gmail.com,
    renjingxu@ust.hk²The author is with Human $|$ Centric AI Centre, Australia Artificial
    Intelligence Institute University of Technology Sydney 2007 Ultimo Australia.
    yiqun.duan@student.uts.edu.au
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Jingkai Sun^(1,∗), Qiang Zhang^(1,∗), Yiqun Duan², Xiaoyang Jiang¹, Chong Cheng¹
    and Renjing Xu^(1,†) ^∗ 是平等贡献者，^† 是通讯作者¹作者隶属于中国香港科技大学（广州）。{jsun444, qzhang749,
    ccheng735}@connect.hkust-gz.edu.cn, jxxxxxyGOAT@gmail.com, renjingxu@ust.hk²作者隶属于人类
    $|$ 以人为本的人工智能中心，澳大利亚悉尼科技大学2007 Ultimo 澳大利亚。 yiqun.duan@student.uts.edu.au
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In recent years, reinforcement learning and imitation learning have shown great
    potential for controlling humanoid robots’ motion. However, these methods typically
    create simulation environments and rewards for specific tasks, resulting in the
    requirements of multiple policies and limited capabilities for tackling complex
    and unknown tasks. To overcome these issues, we present a novel approach that
    combines adversarial imitation learning with large language models (LLMs). This
    innovative method enables the agent to learn reusable skills with a single policy
    and solve zero-shot tasks under the guidance of LLMs. In particular, we utilize
    the LLM as a strategic planner for applying previously learned skills to novel
    tasks through the comprehension of task-specific prompts. This empowers the robot
    to perform the specified actions in a sequence. To improve our model, we incorporate
    codebook-based vector quantization, allowing the agent to generate suitable actions
    in response to unseen textual commands from LLMs. Furthermore, we design general
    reward functions that consider the distinct motion features of humanoid robots,
    ensuring the agent imitates the motion data while maintaining goal orientation
    without additional guiding direction approaches or policies. To the best of our
    knowledge, this is the first framework that controls humanoid robots using a single
    learning policy network and LLM as a planner. Extensive experiments demonstrate
    that our method exhibits efficient and adaptive ability in complicated motion
    tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，强化学习和模仿学习在控制类人机器人运动方面显示了巨大的潜力。然而，这些方法通常为特定任务创建模拟环境和奖励，导致需要多个策略，并且在处理复杂和未知任务时能力有限。为了解决这些问题，我们提出了一种将对抗性模仿学习与大型语言模型（LLMs）相结合的新方法。这种创新方法使得智能体能够通过单一策略学习可重用的技能，并在LLMs的指导下解决零样本任务。特别地，我们利用LLM作为战略规划者，通过理解任务特定的提示将先前学到的技能应用于新任务。这使得机器人能够按顺序执行指定的动作。为了改进我们的模型，我们结合了基于代码本的向量量化，使智能体能够对来自LLMs的未见文本命令做出适当的动作。此外，我们设计了通用奖励函数，考虑到类人机器人的独特运动特征，确保智能体在保持目标导向的同时模仿运动数据，而无需额外的指导方向方法或策略。我们认为，这是第一个使用单一学习策略网络和LLM作为规划者来控制类人机器人的框架。广泛的实验表明，我们的方法在复杂运动任务中表现出高效和适应能力。
- en: I Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Advances in humanoid robotics require the development of complex, adaptable,
    and efficient control policies. These policies enable humanoid robots to operate
    effectively in human-centered environments, thereby aiding in the successful execution
    of a diverse range of tasks. Despite significant advancements in the domain of
    humanoid robotics, a substantial disparity exists between the theoretical potential
    of these systems and their practical efficacy in executing complex tasks. In this
    paper, we propose a novel framework to combine the capabilities of Generative
    Adversarial Imitation Learning (GAIL)[[1](#bib.bib1)] action data imitation with
    the planning capabilities of the large language models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 类人机器人技术的进步需要开发复杂、可适应且高效的控制策略。这些策略使得类人机器人能够在以人为中心的环境中有效操作，从而帮助成功执行各种任务。尽管在类人机器人领域取得了显著进展，但这些系统的理论潜力与其在执行复杂任务时的实际效能之间仍存在较大差距。在本文中，我们提出了一种新颖的框架，将生成对抗模仿学习（GAIL）[[1](#bib.bib1)]的行动数据模仿能力与大语言模型的规划能力相结合。
- en: '![Refer to caption](img/7cae113fb9e479ee0302faa184784805.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7cae113fb9e479ee0302faa184784805.png)'
- en: 'Figure 1: Our framework enables robots to combine the skills obtained from
    imitation learning with the planning capabilities of LLMs to accomplish complex
    tasks. For example, with known obstacles as well as its own coordinates, the robot
    accomplishes the task of hitting a target after avoiding obstacles by scheduling
    reusable skills.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们的框架使得机器人能够将从模仿学习中获得的技能与大语言模型的规划能力相结合，从而完成复杂的任务。例如，利用已知的障碍物以及自身的坐标，机器人通过调度可重用的技能来完成在避开障碍物后击中目标的任务。
- en: 'The GAIL framework consists of two main components: a policy and a discriminator
    [[2](#bib.bib2)] and trains policy to perform tasks by imitating expert behavior.
    The policy interacts with the environment to generate action based on the current
    state. The discriminator aims to distinguish between the trajectories generated
    by the expert and those produced by the learning policy. However, merely imitating
    unlabeled action data limits the learned policy to generating specific actions
    in particular situations, failing to integrate effectively with high-level policies
    such as Adversarial Skill Embeddings (ASE)[[3](#bib.bib3)]. ASE can only accomplish
    a specific task by training the high-level policy to match the skills of the low-level
    network in the process of training. To address this limitation, we employ motion
    data in conjunction with its associated textual labels to generate latent vectors[[4](#bib.bib4)].
    These latent vectors are subsequently integrated into the imitation learning network
    as conditions and skills. These enable the policy to produce contextually appropriate
    actions when fed with specific skills as input. Leveraging the planning capabilities
    of LLMs, our method enables the execution of a diverse array of tasks without
    requiring the development of task-specific high-level networks. This is achieved
    through the utilization of reusable skills, thereby enhancing the system’s adaptability
    and efficiency.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: GAIL框架主要由两个组件组成：策略和鉴别器[[2](#bib.bib2)]，并通过模仿专家行为来训练策略以执行任务。策略与环境交互，根据当前状态生成行动。鉴别器旨在区分专家生成的轨迹与学习策略产生的轨迹。然而，仅仅模仿未标记的行动数据会限制学习到的策略在特定情境下生成特定行动，无法有效地与高级策略如对抗技能嵌入（ASE）[[3](#bib.bib3)]集成。ASE只能通过训练高级策略使其匹配低级网络的技能来完成特定任务。为了解决这个限制，我们结合运动数据及其相关的文本标签生成潜在向量[[4](#bib.bib4)]。这些潜在向量随后被集成到模仿学习网络中作为条件和技能。这使得策略在输入特定技能时能够产生上下文适当的行动。通过利用大语言模型的规划能力，我们的方法可以执行各种任务，而无需开发特定任务的高级网络。这是通过利用可重用的技能来实现的，从而提高了系统的适应性和效率。
- en: By utilizing the LLM as a skills planner, our approach enables the humanoid
    to complete tasks autonomously based on problem descriptions. To guarantee that
    the robot receives actionable and concise instructions, we refine the complexity
    of the LLM’s outputs by using Contrastive Language-Image Pre-Training (CLIP)[[5](#bib.bib5)]
    text encoder and an integrated codebook-based vector quantization. This enhances
    the robustness of the system concerning the randomness of LLMs-generated outputs.
    The codebook consists of skills generated from the text labels encoded during
    training. It is used to correspond similar commands output by the larger model
    to specific skills. Additionally, we design a general reward consisting of root-oriented
    and hip-oriented rewards to ensure that the policy trained by imitation learning
    can execute specific actions along the given directions rather than local coordinates.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将LLM作为技能规划器，我们的方法使得类人机器人能够根据问题描述自主完成任务。为了确保机器人接收到可操作且简洁的指令，我们通过使用对比语言-图像预训练（CLIP）[[5](#bib.bib5)]文本编码器和集成的基于代码簿的向量量化来优化LLM输出的复杂性。这增强了系统对LLM生成输出随机性的鲁棒性。代码簿由在训练期间编码的文本标签生成的技能组成。它用于将大型模型输出的类似命令与特定技能相对应。此外，我们设计了一个包含根部导向和髋部导向奖励的通用奖励，以确保通过模仿学习训练的策略可以沿着给定方向执行特定动作，而不是局部坐标。
- en: 'Our approach focuses on enhancing the integration of GAIL’s dynamic control
    abilities with the cognitive capabilities of LLMs. In summary, the planning results
    are shown in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Prompt, Plan, Perform:
    LLM-based Humanoid Control via Quantized Imitation Learning") and our contributions
    include: (I) To the best of our knowledge, we propose a framework that employs
    a single policy network with LLMs for planning to control humanoid robots to accomplish
    tasks for the first time. (II) By introducing codebook-based vector quantization,
    our framework is equipped to handle a variety of similar but unseen instruction
    sets, enhancing robustness to LLMs or human commands. (III) By designing a general
    reward for the hip joints, we implemented a single policy to control the direction
    of robots and perform most of the motion.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的方法重点增强了GAIL的动态控制能力与LLM的认知能力的集成。总之，规划结果见图[1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Prompt, Plan, Perform: LLM-based Humanoid Control via Quantized Imitation Learning")，我们的贡献包括：（I）据我们所知，我们首次提出了一个框架，使用LLM的单一策略网络来规划控制类人机器人完成任务。（II）通过引入基于代码簿的向量量化，我们的框架能够处理各种相似但未见过的指令集，提高了对LLM或人类命令的鲁棒性。（III）通过设计针对髋关节的通用奖励，我们实现了一个单一策略来控制机器人的方向并执行大部分动作。'
- en: II Related work
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: II-A Large Language Model for Robotics
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 大型语言模型在机器人技术中的应用
- en: Recently, the rise of large-scale language models like ChatGPT[[6](#bib.bib6)]
    and Llama2[[7](#bib.bib7)] has opened new doors for natural language understanding
    and generation. Thus, there are various prior works have explored using the large
    language model for task planning [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)]
    or robot manipulation[[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15)]. SayCan[[16](#bib.bib16)] combines value functions associated
    with low-level skills with the large language models, thereby establishing a bridge
    between rich linguistic understanding and real-world physical interactions. Inner
    Monologue[[8](#bib.bib8)] studies the impact of human feedback on LLMs, including
    scenario descriptions and human-computer interactions. Their findings indicate
    that detailed examples and closed-loop feedback can improve the performance of
    LLMs. RT-2[[17](#bib.bib17)] employs Internet-scale data to train vision-language
    models, enabling their application as end-to-end robotic control systems. This
    model is not merely confined to generating actions based on observations; it also
    assimilates a comprehensive understanding of the human world derived from online
    sources. DoReMi[[18](#bib.bib18)]facilitates real-time error identification and
    recovery mechanisms during the execution of plans generated by LLMs on humanoid
    robots.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大规模语言模型如ChatGPT[[6](#bib.bib6)]和Llama2[[7](#bib.bib7)]的兴起为自然语言理解和生成打开了新的大门。因此，各种先前的研究探索了利用大规模语言模型进行任务规划[[8](#bib.bib8)、[9](#bib.bib9)、[10](#bib.bib10)]或机器人操控[[11](#bib.bib11)、[12](#bib.bib12)、[13](#bib.bib13)、[14](#bib.bib14)、[15](#bib.bib15)]。SayCan[[16](#bib.bib16)]将与低级技能相关的价值函数与大规模语言模型结合，从而在丰富的语言理解和现实世界的物理交互之间建立了桥梁。Inner
    Monologue[[8](#bib.bib8)]研究了人类反馈对LLM的影响，包括场景描述和人机交互。他们的发现表明，详细的例子和闭环反馈可以提高LLM的表现。RT-2[[17](#bib.bib17)]利用互联网规模的数据来训练视觉-语言模型，使其作为端到端的机器人控制系统应用。这个模型不仅仅局限于根据观察生成动作；它还吸收了来自在线来源的人类世界的全面理解。DoReMi[[18](#bib.bib18)]在执行LLM生成的计划时，促进了实时错误识别和恢复机制。
- en: II-B Adversarial Imitation Learning
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 对抗性模仿学习
- en: Imitation learning (IL) aims to solve complex tasks where learning a policy
    from scratch is rather challenging, by learning useful skills or behaviors from
    expert demonstrations in auto-driving[[19](#bib.bib19), [20](#bib.bib20)] or robotics[[21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23)]. GAIL[[1](#bib.bib1)] first introduces a discriminator
    network to provide additional reward signals to the policy. It makes the policy
    more consistent with the distribution of expert data. Thus, GAIL addresses some
    of the limitations of behavior cloning [[24](#bib.bib24)] (such as compounding
    error caused by covariate shift) by learning an objective function to measure
    the similarity between the policy and the expert data. Adversarial Motion Prior
    (AMP)[[25](#bib.bib25)] learns to imitate different skills from the reference
    data in order to fulfill high-level tasks. But in AMP, a single policy is typically
    specialized for a limited set of tasks, because the performance of the agent is
    strongly correlated with the reward function designed during training. [[26](#bib.bib26)]
    realizes a single policy with controllable skill sets from unlabeled datasets
    containing diverse motion by using a skill discriminator. Based on AMP, ASE[[3](#bib.bib3)]
    utilize hierarchical model[[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30)] to make low-level policy learn reusable skills form motion data
    in latent space. Then it further trains many high-level policies to solve downstream
    tasks by invoking a low-level policy and designing different reward functions.
    However, this method still requires many reward functions for specific missions
    and falls short in its capacity to handle tasks that are not encountered during
    training. [[31](#bib.bib31)] and [[32](#bib.bib32)] employ textual labels and
    motion data to convert the skills learned through their respective policies into
    explicit variables. These variables can then be manipulated by humans through
    syntax trees and finite state machines to facilitate task completion. However,
    these methods rely on multiple policies and involve considerable manual labor.
    Based on the above shortcomings, we introduce a framework that incorporates large
    language models and general directional rewards. This enables the execution of
    unseen tasks using a single policy, significantly reducing the manual work required.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习（IL）旨在解决复杂任务，在这些任务中，从头开始学习策略相当具有挑战性，通过从自动驾驶[[19](#bib.bib19), [20](#bib.bib20)]或机器人技术[[21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23)]中的专家演示中学习有用的技能或行为。GAIL[[1](#bib.bib1)]首次引入了一个判别器网络，为策略提供额外的奖励信号，使策略与专家数据的分布更加一致。因此，GAIL通过学习一个目标函数来测量策略与专家数据之间的相似性，从而解决了行为克隆[[24](#bib.bib24)]的一些局限性（例如由于协变量偏移造成的累积误差）。对抗运动先验（AMP）[[25](#bib.bib25)]学习模仿参考数据中的不同技能，以完成高层任务。但在AMP中，单一策略通常针对有限的任务集进行专门化，因为代理的性能与训练期间设计的奖励函数密切相关。[[26](#bib.bib26)]通过使用技能判别器实现了一个具有可控技能集的单一策略，该策略基于包含各种运动的无标签数据集。基于AMP，ASE[[3](#bib.bib3)]利用层次模型[[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]使低级策略从潜在空间中的运动数据中学习可重复使用的技能。然后，它进一步训练许多高级策略，通过调用低级策略和设计不同的奖励函数来解决下游任务。然而，这种方法仍然需要许多奖励函数来完成特定任务，并且在处理训练过程中未遇到的任务时能力不足。[[31](#bib.bib31)]和[[32](#bib.bib32)]利用文本标签和运动数据将通过各自策略学习到的技能转换为显式变量。这些变量可以通过语法树和有限状态机由人类进行操作，以促进任务完成。然而，这些方法依赖于多个策略，并涉及大量人工工作。基于上述缺点，我们引入了一个整合了大型语言模型和一般方向性奖励的框架。这使得通过单一策略执行未见任务成为可能，大大减少了所需的人工工作。
- en: III Background
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 背景
- en: 'In contrast to GAIL, our framework is built upon Conditional Adversarial Latent
    Models (CALM)[[32](#bib.bib32)], using a conditional discriminator[[33](#bib.bib33)]
    to enable it to match a specific latent to an action. Both GAIL and CALM are typically
    defined in the context of a discrete-time Markov Decision Process (MDP). MDP is
    defined as a tuple $(\mathcal{S},\mathcal{A},\mathcal{R},p,\gamma)$:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与GAIL相比，我们的框架建立在条件对抗潜在模型（CALM）[[32](#bib.bib32)]的基础上，使用条件判别器[[33](#bib.bib33)]来使其能够将特定的潜变量与行动匹配。GAIL和CALM通常在离散时间马尔可夫决策过程（MDP）的背景下定义。MDP定义为一个元组$(\mathcal{S},\mathcal{A},\mathcal{R},p,\gamma)$：
- en: '|  | $\textnormal{arg}\max_{\theta}\mathbb{E}_{(s_{t},a_{t})\sim p_{\theta}(s_{t},a_{t})}\left[\sum_{t=0}^{T-1}\gamma^{t}r_{t}\right]$
    |  | (1) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textnormal{arg}\max_{\theta}\mathbb{E}_{(s_{t},a_{t})\sim p_{\theta}(s_{t},a_{t})}\left[\sum_{t=0}^{T-1}\gamma^{t}r_{t}\right]$
    |  | (1) |'
- en: where T denotes the time horizon of MDP.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中T表示MDP的时间范围。
- en: In the traditional reinforcement learning[[34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36)], the reward functions need to be manually designed specifically
    for different tasks, but this is difficult for learning motor skills from expert
    demonstrations. Imitation learning emerges as a viable alternative for situations
    where defining a deterministic reward function is impractical, yet the task can
    be accomplished by imitating expert demonstrations. GAIL tackles some limitations
    of normal imitation learning such as behavior cloning by introducing a discriminator
    $\mathcal{D}(s_{t},a_{t})$ in order to optimize the learning objective which requires
    the state-action pairs of agents. To extend the learning objective to datasets
    that only offer the state of the agent, we follow [[37](#bib.bib37)] to transfer
    it to the state and the next state pair instead of the original state-action pair.
    We introduce the Jensen-Shannon divergence[[38](#bib.bib38)] as the objective
    of the discriminator,
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的强化学习[[34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)]中，奖励函数需要为不同任务专门手动设计，但从专家演示中学习运动技能时这很困难。模仿学习作为一种可行的替代方案，适用于定义确定性奖励函数不切实际的情况，但任务可以通过模仿专家演示来完成。GAIL
    通过引入判别器 $\mathcal{D}(s_{t},a_{t})$ 解决了普通模仿学习（如行为克隆）的一些局限性，以优化学习目标，该目标需要代理的状态-动作对。为了将学习目标扩展到仅提供代理状态的数据集，我们按照
    [[37](#bib.bib37)] 将其转移到状态和下一状态对，而不是原始的状态-动作对。我们将 Jensen-Shannon 散度[[38](#bib.bib38)]
    作为判别器的目标，
- en: '|  | $\displaystyle\textnormal{arg}\min_{\mathcal{D}}$ |  | (2) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textnormal{arg}\min_{\mathcal{D}}$ |  | (2) |'
- en: '|  |  | $\displaystyle-\mathbb{E}_{d^{\pi}(s_{t},s_{t+1})}\left[\textnormal{log}(1-\mathcal{D}(s_{t},s_{t+1}))\right]$
    |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\mathbb{E}_{d^{\pi}(s_{t},s_{t+1})}\left[\textnormal{log}(1-\mathcal{D}(s_{t},s_{t+1}))\right]$
    |  |'
- en: where $d^{\mathcal{M}}(s_{t},a_{t})$ denote the state-action pair distribution
    of reference motion dataset and policy.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d^{\mathcal{M}}(s_{t},a_{t})$ 表示参考运动数据集和策略的状态-动作对分布。
- en: '![Refer to caption](img/39af57ad1b175dcc1b45546ec730e006.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/39af57ad1b175dcc1b45546ec730e006.png)'
- en: 'Figure 2: Overview of our proposed system. Motion captions with the same semantics
    are first clustered together by fine-tuning the CLIP Text encoder. Subsequently,
    the output text features are fed into the policy training by codebook-based vector
    quantization. Our pre-training system feeds a reference dataset defining the desired
    underlying motion and its corresponding text labels (marked in red in the figure)
    into the training discriminator to provide discriminator rewards for policy training.
    The discriminator reward is then combined with the task reward for controlling
    orientation to train a policy that allows the robot to execute the demonstrated
    motion in the specified orientation. These two processes are not trained at the
    same time.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们提出的系统概述。具有相同语义的运动标题首先通过微调 CLIP 文本编码器进行聚类。随后，输出的文本特征通过基于代码本的向量量化输入到策略训练中。我们的预训练系统将定义所需基础运动及其对应文本标签（图中红色标记）的参考数据集输入到训练判别器中，为策略训练提供判别器奖励。然后将判别器奖励与控制方向的任务奖励结合起来，训练一个策略，使机器人能够在指定的方向上执行演示的动作。这两个过程不会同时训练。
- en: '![Refer to caption](img/7d7502e0edf2bd9c4d4502df9dd90290.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7d7502e0edf2bd9c4d4502df9dd90290.png)'
- en: 'Figure 3: Evaluation system overview. The Human definitions are fed into the
    LLMs as prompts. The LLMs output the sequence of actions and the target orientation
    of each action. The text of the action sequence is input to the CLIP Text encoder,
    and the target orientation is input to the policy as an observation concatenated
    with the observation given by the environment.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：评估系统概述。将人类定义输入到 LLMs 中作为提示。LLMs 输出动作序列和每个动作的目标方向。动作序列的文本输入到 CLIP 文本编码器中，目标方向作为观察结果与环境提供的观察结果一起输入到策略中。
- en: IV APPROACH
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 方法
- en: 'As illustrated in Figure [2](#S3.F2 "Figure 2 ‣ III Background ‣ Prompt, Plan,
    Perform: LLM-based Humanoid Control via Quantized Imitation Learning"), the architecture
    of our proposed method is designed to execute tasks via a single policy framework.
    Our architecture is composed of three primary components: an adaptive language-based
    skill motion policy, a CLIP-based adaptive language discrete encoder, and the
    large language model planner.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [2](#S3.F2 "Figure 2 ‣ III Background ‣ Prompt, Plan, Perform: LLM-based
    Humanoid Control via Quantized Imitation Learning") 所示，我们提出的方法的架构旨在通过单一策略框架执行任务。我们的架构由三个主要组件组成：自适应语言基础技能运动策略、基于
    CLIP 的自适应语言离散编码器和大型语言模型规划器。'
- en: IV-A Adaptive Language-based Skill Motion Policy Pre-train
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 自适应语言基础技能运动策略预训练
- en: 'Our motion policy pre-train method consists of three main parts: a skill encoder,
    and a discriminator, a reinforcement learning policy. The skill encoder is used
    to reduce the dimension of the text features output by CLIP-based adaptive language
    discrete encoder to generate latent vectors. For generating close and uniform
    skills of the same kinds of actions on the latent space, we introduce alignment
    loss and uniformity loss to train the skill encoder like [[32](#bib.bib32)]. The
    discriminator is consistent with the description in the previous section. In our
    framework, each label of the motion dataset is encoded as a latent vector $z$
    to the discriminator and the motion policy as conditions and skills. The discriminator
    outputs the degree of similarity between the state transitions generated by policy
    and the dataset.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的运动策略预训练方法包括三个主要部分：技能编码器、判别器和强化学习策略。技能编码器用于将 CLIP 基于自适应语言离散编码器输出的文本特征维度降低，生成潜在向量。为了在潜在空间上生成相似且均匀的技能，我们引入了对齐损失和均匀性损失来训练技能编码器，如
    [[32](#bib.bib32)] 所述。判别器与前述描述一致。在我们的框架中，每个运动数据集的标签被编码为潜在向量 $z$，作为判别器和运动策略的条件和技能。判别器输出策略生成的状态转换与数据集之间的相似度。
- en: '|  | $\displaystyle\textnormal{arg}\min_{\mathcal{D}}$ |  | (3) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textnormal{arg}\min_{\mathcal{D}}$ |  | (3) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle+\omega_{gp}\mathbb{E}_{d^{\mathcal{M}}({s}_{t},{s}_{t+1})}\left[\&#124;\mathbf{\triangledown_{\alpha}\mathcal{D}(\alpha)}\&#124;^{2}\right]$
    |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\omega_{gp}\mathbb{E}_{d^{\mathcal{M}}({s}_{t},{s}_{t+1})}\left[\&#124;\mathbf{\triangledown_{\alpha}\mathcal{D}(\alpha)}\&#124;^{2}\right]$
    |  |'
- en: where $\omega_{gp}$. During each iteration, a randomly selected type of motion
    is input into the discriminator. In this setup, the agent is rewarded more favorably
    only if its executed actions closely align with the selected motion type.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\omega_{gp}$。在每次迭代中，随机选择一种运动类型输入到判别器中。在这种设置下，只有当执行的动作与选定的运动类型紧密对齐时，代理才会获得更多奖励。
- en: Unlike CALM, our motion policy is trained further by the combination of two
    rewards, the discriminator reward and the task reward. This is because the policy
    trained through imitation learning performs motions in their local coordinates.
    However, real-world task solutions require the humanoid robot to move in a specified
    direction, and even upper body action (e.g., sword swinging) needs to determine
    the orientation. Therefore, the high-level policy is required to specify directions
    for the low-level policy in the CALM framework. This leads to additional network
    training and still does not represent the direction of upper body action direction
    since its network rewards whole body movement direction. Thus, we design a general
    reward that incorporates the physical mechanism characteristics of humanoid robots.
    This is aimed at ensuring that the policy trained through imitation learning can
    move or perform specific actions along specific directions. The general reward
    consists of three items, root direction (also called pelvis), left hip direction,
    and right hip direction. The reward is formulated as,
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CALM 不同，我们的运动策略通过结合两种奖励——判别器奖励和任务奖励——进一步训练。这是因为通过模仿学习训练的策略在其局部坐标系中执行动作。然而，现实世界的任务解决方案要求类人机器人沿指定方向移动，甚至上半身动作（例如挥剑）也需要确定方向。因此，在
    CALM 框架中，高层策略需要为低层策略指定方向。这会导致额外的网络训练，并且仍未能表示上半身动作的方向，因为其网络奖励的是全身运动方向。因此，我们设计了一种通用奖励，结合了类人机器人的物理机制特征。其目的是确保通过模仿学习训练的策略能够沿特定方向移动或执行特定动作。通用奖励包括三个项目：根方向（也称为骨盆方向）、左臀方向和右臀方向。奖励的形式如下：
- en: '|  | $1$2 |  | (4) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: where $\omega_{1}$ are the direction vectors of the root, left hip, and right
    hip or humanoid robot. The total reward is,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\omega_{1}$ 是根部、左髋部和右髋部或类人机器人方向向量。总奖励为，
- en: '|  | $1$2 |  | (5) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: where $\omega_{task}$ is the the weight of discriminator reward. We find the
    direction of the root infers the upper body action direction, and the average
    of two hips direction infers the movements of the whole body. Thus, we design
    the general reward to ensure that the whole body moves forward (except for movements
    to the left or right in the local coordinates) and the upper body’s action in
    a specific direction.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\omega_{task}$ 是判别器奖励的权重。我们发现根部的方向可以推断出上半身的动作方向，而两个髋部方向的平均值可以推断出整个身体的运动。因此，我们设计了一种通用奖励机制，以确保整个身体向前移动（排除局部坐标系中的左右移动），以及上半身在特定方向上的动作。
- en: IV-B CLIP-based Adaptive Language Discrete Encoder
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 基于 CLIP 的自适应语言离散编码器
- en: To encode natural language commands(also called captions), our system employs
    a pre-trained text encoder from the CLIP model. This encoder transforms the input
    commands into a 512-dimensional vector space. Leveraging an extensive dataset
    comprising action-related text, we fine-tune this encoder to optimize its performance
    specifically for our target application. In the fine-tuning phase, we utilize
    Mean Squared Error (MSE) as a loss function to make the encoded latent vectors
    obtained from different labels corresponding to the same motion, closer together
    in the latent space. The loss function is formulated as,
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对自然语言命令（也称为字幕）进行编码，我们的系统使用了 CLIP 模型的预训练文本编码器。该编码器将输入命令转换为 512 维的向量空间。利用包含动作相关文本的大规模数据集，我们对该编码器进行了微调，以针对我们的目标应用优化其性能。在微调阶段，我们利用均方误差（MSE）作为损失函数，使从不同标签获得的编码潜在向量在潜在空间中更接近。损失函数的公式为，
- en: '|  | $1$2 |  | (6) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: where $Enc_{T}$ as follows,
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Enc_{T}$ 如下所示，
- en: '|  | $\displaystyle k$ |  | (7) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle k$ |  | (7) |'
- en: '|  | $\displaystyle f_{d}$ |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f_{d}$ |  |'
- en: '|  | $\displaystyle e_{k}$ |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle e_{k}$ |  |'
- en: where $f=Enc_{T}(text)$ to the label feature generated by fine-tuning the CLIP
    text encoder. As such, fine-tuning the CLIP text encoder and vector quantization
    of the text features allows the agent to handle captions that are unseen and reduces
    the memory of the training process.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$f=Enc_{T}(text)$ 是通过微调 CLIP 文本编码器生成的标签特征。因此，微调 CLIP 文本编码器和文本特征的向量量化使得代理能够处理未见过的字幕，并减少训练过程中的记忆消耗。
- en: IV-C Large Language Model Motion Planner
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 大型语言模型动作规划器
- en: 'In this section, we explore the possibility of extracting action planning knowledge
    from pre-trained language models without further training, simply by using the
    prompt. We include descriptions of the robot itself, the task scenario, the action
    skills it can perform, and some information about the output formatting examples
    and constraints as prompts. The pipeline of the evaluation system with LLMs is
    shown in Figure [3](#S3.F3 "Figure 3 ‣ III Background ‣ Prompt, Plan, Perform:
    LLM-based Humanoid Control via Quantized Imitation Learning"). Furthermore, the
    most important part of the prompt is an example of accomplishing a simple task
    (e.g., moving in a specified direction). Otherwise, LLMs need to interact with
    the user through subsequent interactions in order to output the action sequences
    correctly. The output commands of LLMs consist of two parts, the textual commands
    and coordinate orientation. Textual commands are sequences of reusable action
    skills that a robot follows to perform an action to accomplish a specific task.
    The coordinate orientation indicates the direction of the target when the robot
    performs the action. The former is fed into the policy via a fine-tuned CLIP-based
    text encoder, while the latter is added to the observation vector to control the
    robot. In the actual completion of the task, we take the goal, obstacles, and
    global coordinates of the robot as known information. Under this assumption, actions
    related to the robot’s movement are judged to be completed or not in terms of
    the distance to the goal position. For actions unrelated to movement, the time
    required to execute the motion serves as the determining factor for action completion.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了如何仅通过使用提示，从预训练的语言模型中提取行动规划知识，而无需进一步训练。我们包含了有关机器人本身、任务场景、它能执行的行动技能的描述，以及一些关于输出格式示例和约束的信息作为提示。LLMs的评估系统的流程如图[3](#S3.F3
    "图3 ‣ III 背景 ‣ 提示、计划、执行：基于LLM的类人控制通过量化模仿学习")所示。此外，提示中最重要的部分是完成简单任务的示例（例如，向指定方向移动）。否则，LLMs需要通过后续交互与用户互动，以正确输出行动序列。LLMs的输出命令由两个部分组成：文本命令和坐标方向。文本命令是机器人执行特定任务时遵循的可重用行动技能的序列。坐标方向表示机器人执行动作时目标的方向。前者通过微调的基于CLIP的文本编码器输入到策略中，而后者则添加到观察向量中以控制机器人。在实际完成任务时，我们将目标、障碍物和机器人的全局坐标视为已知信息。在此假设下，与机器人的移动相关的行动根据距离目标位置来判断是否完成。对于与移动无关的行动，执行动作所需的时间作为行动完成的决定因素。
- en: '![Refer to caption](img/b283c596ce5b234ed5b5c4b28c35c18a.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b283c596ce5b234ed5b5c4b28c35c18a.png)'
- en: 'Figure 4: Initialization of an obstacle avoidance attack task. The gray rectangle
    represents the attack target, the blue markers are the middle path point of the
    LLMs plan, the red is the obstacle, and the green line points to the current target
    orientation of the robot.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：障碍物规避攻击任务的初始化。灰色矩形代表攻击目标，蓝色标记是LLMs计划的中间路径点，红色是障碍物，绿色线条指向机器人的当前目标方向。
- en: IV-D System Architecture Implementation
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 系统架构实现
- en: Our robot is a simplified model consisting of spheres, boxes, and cylinders.
    The whole robot is made up of three joints in the arms, three joints in the legs,
    a joint in the waist, and a joint in the neck. All of these joints except the
    knee and elbow have three degrees of freedom. The action space of policy is the
    target position of each joint. The state space consists of the height of the root
    from the ground and positions and velocities, rotation, and angular velocities
    of the other bodies in the robot’s local coordinate.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的机器人是由球体、盒子和圆柱体组成的简化模型。整个机器人由三个臂关节、三个腿关节、一个腰关节和一个颈关节组成。除了膝关节和肘关节外，所有这些关节都有三个自由度。策略的动作空间是每个关节的目标位置。状态空间包括从地面到根部的高度以及机器人局部坐标系中其他物体的位置和速度、旋转和角速度。
- en: V Experiments
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 实验
- en: In this section, we conduct experiments involving humanoid robot tasks. These
    tasks include controlling the robot to perform diverse actions through natural
    language, incorporating large language models for navigation, and knocking down
    objects. Specifically, we show that our CLIP-based adaptive language discrete
    encoder can handle diverse natural language commands. Additionally, the general
    reward structure we introduce proves effective in directing the robot’s movement
    along a specified direction, without influencing the quality of imitation from
    the motion dataset. We collect data and train on a single A100 GPU on 4096 Isaac
    Gym[[39](#bib.bib39)] environments in parallel. And our reinforcement learning
    algorithm is Proximal Policy Optimization (PPO)[[40](#bib.bib40)].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行涉及类人机器人任务的实验。这些任务包括通过自然语言控制机器人执行各种动作，结合大语言模型进行导航，并撞倒物体。具体而言，我们展示了基于CLIP的自适应语言离散编码器能够处理多样的自然语言命令。此外，我们引入的通用奖励结构在引导机器人的运动沿着指定方向方面证明了其有效性，而不会影响运动数据集中的模仿质量。我们在4096个Isaac
    Gym[[39](#bib.bib39)]环境中并行收集数据并在单个A100 GPU上进行训练。我们的强化学习算法是近端策略优化（PPO）[[40](#bib.bib40)]。
- en: '![Refer to caption](img/d7da87f5018d39f756e9836a7470446e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d7da87f5018d39f756e9836a7470446e.png)'
- en: 'Figure 5: The movements and their target orientations for each step in completing
    the obstacle avoidance attack task. The example shows the initial position of
    the robot (0,0), obstacle position (3,0), and target position (6,0). The obstacle
    is a rectangle with a length of 1.2m in the x-axis direction and a length of 1.8m
    in the y-axis direction. The blue markers are the waypoints of the LLMs plan,
    the red is the obstacle, and the green line points to the current target orientation
    of the robot.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 完成障碍物规避攻击任务的每一步的动作及其目标方向。示例展示了机器人初始位置（0,0）、障碍物位置（3,0）和目标位置（6,0）。障碍物是一个在x轴方向长度为1.2m，在y轴方向长度为1.8m的矩形。蓝色标记是LLMs计划的途经点，红色是障碍物，绿色线条指向机器人的当前目标方向。'
- en: 'TABLE I: Comparison of the abilities of different methods'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 不同方法的能力比较'
- en: '| Method | Abilities |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 能力 |'
- en: '|'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; unseen &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 未见 &#124;'
- en: '&#124; task &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务 &#124;'
- en: '|'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; single &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单一 &#124;'
- en: '&#124; network &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络 &#124;'
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; input diverse &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入多样性 &#124;'
- en: '&#124; language &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语言 &#124;'
- en: '|'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; no label &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无标签 &#124;'
- en: '&#124; request &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 请求 &#124;'
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| AMP[[25](#bib.bib25)] |  | ✓ |  | ✓ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| AMP[[25](#bib.bib25)] |  | ✓ |  | ✓ |'
- en: '| ASE[[3](#bib.bib3)] |  |  |  | ✓ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| ASE[[3](#bib.bib3)] |  |  |  | ✓ |'
- en: '| PADL[[31](#bib.bib31)] | ✓ |  |  |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| PADL[[31](#bib.bib31)] | ✓ |  |  |  |'
- en: '| CALM[[32](#bib.bib32)] | ✓ |  |  |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| CALM[[32](#bib.bib32)] | ✓ |  |  |  |'
- en: '| Ours | ✓ | ✓ | ✓ |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | ✓ | ✓ | ✓ |  |'
- en: V-A Solve Downstream Tasks with LLMs Planner
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 使用LLMs规划器解决下游任务
- en: 'In the process of imitation learning, we feed the position, velocity, height
    of root, rotation relative to the root of each body, the velocity of the joints,
    and the position of the key bodies (right hand, left hand, right foot, left foot,
    sword, shield) into discriminator. This input strategy enables the policy to produce
    actions that utilize fewer dimensions of motion data. In order to demonstrate
    the motion policy uses reusable skills to accomplish the zero-shot tasks through
    the LLMs planner, we design a scenario where the robot must navigate around obstacles
    to reach and knock down a target. As shown in Figure [4](#S4.F4 "Figure 4 ‣ IV-C
    Large Language Model Motion Planner ‣ IV APPROACH ‣ Prompt, Plan, Perform: LLM-based
    Humanoid Control via Quantized Imitation Learning"), the robot is initially oriented
    squarely towards the target and needs to bypass the red obstacle in the middle
    and eventually knock down the target once it reaches the vicinity of the target.
    The LLMs planner, prompted by the pre-input position of the obstacle and other
    constraints, outputs the robot’s actions and corresponding goal orientations.
    The action type output from the LLMs is fed into the encoder, and the orientation
    data is incorporated into the observation vector. As the results are shown in
    Figure [5](#S5.F5 "Figure 5 ‣ V Experiments ‣ Prompt, Plan, Perform: LLM-based
    Humanoid Control via Quantized Imitation Learning"), the complex unseen task can
    be solved step by step through our proposed method by combining a single reinforcement
    learning policy with the large language model planner. Table [I](#S5.T1 "TABLE
    I ‣ V Experiments ‣ Prompt, Plan, Perform: LLM-based Humanoid Control via Quantized
    Imitation Learning") shows the comparison of our approach with other methods.
    In contrast to ASE, our method does not need to train high-level policies to accomplish
    specific tasks. Compared to CALM, for scenarios with randomly generated obstacle
    locations and sizes, our approach does not need to manually design the path points
    during obstacle avoidance and can automate more complex tasks without the requirement
    of designing finite state machines.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '在模仿学习过程中，我们将每个身体的根位置、速度、高度、相对于根的旋转、关节的速度以及关键身体部位（右手、左手、右脚、左脚、剑、盾）的位置信息输入到判别器中。这种输入策略使得策略能够生成利用更少维度的动作数据的动作。为了展示动作策略如何通过LLMs规划器使用可重用的技能来完成零样本任务，我们设计了一个场景，其中机器人必须绕过障碍物，以达到并击倒目标。如图 [4](#S4.F4
    "Figure 4 ‣ IV-C Large Language Model Motion Planner ‣ IV APPROACH ‣ Prompt, Plan,
    Perform: LLM-based Humanoid Control via Quantized Imitation Learning")所示，机器人最初面向目标，需绕过中间的红色障碍物，并在到达目标附近后击倒目标。LLMs规划器通过预先输入的障碍物位置和其他约束条件来输出机器人的动作和相应的目标方向。从LLMs输出的动作类型被输入到编码器中，方向数据被纳入观察向量。结果如图 [5](#S5.F5
    "Figure 5 ‣ V Experiments ‣ Prompt, Plan, Perform: LLM-based Humanoid Control
    via Quantized Imitation Learning")所示，通过将单一的强化学习策略与大语言模型规划器相结合，我们提出的方法可以逐步解决复杂的未见任务。表 [I](#S5.T1
    "TABLE I ‣ V Experiments ‣ Prompt, Plan, Perform: LLM-based Humanoid Control via
    Quantized Imitation Learning")展示了我们的方法与其他方法的比较。与ASE相比，我们的方法不需要训练高级策略来完成特定任务。与CALM相比，对于随机生成的障碍物位置和大小的场景，我们的方法不需要在避障过程中手动设计路径点，并且可以在无需设计有限状态机的情况下自动化更复杂的任务。'
- en: '![Refer to caption](img/6f938ef9c0c1d14d24288b2ab1479423.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6f938ef9c0c1d14d24288b2ab1479423.png)'
- en: 'Figure 6: Comparisons of Robot Position and Orientation Error (calculated by
    root direction) during running forward and dodging backward.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：前进跑步和后退躲避过程中机器人的位置和方向误差比较（按根方向计算）。
- en: 'TABLE II: Comparison of the accuracy of different CLIP Text Encoders'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：不同CLIP文本编码器的准确性比较
- en: '| Method | Motion class |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 动作类别 |'
- en: '| slash | run | walk | turn | dodge | shield |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 割 | 跑 | 走 | 转向 | 躲避 | 防护 |'
- en: '| finetune(our) | 82.45% | 90.21% | 87.93% | 91.93% | 88.13% | 79.43% |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 微调（我们） | 82.45% | 90.21% | 87.93% | 91.93% | 88.13% | 79.43% |'
- en: '| w/o finetune | 46.23% | 53.78% | 54.34% | 61.93% | 51.20% | 45.21% |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 未微调 | 46.23% | 53.78% | 54.34% | 61.93% | 51.20% | 45.21% |'
- en: V-B Evaluate Adaptive Language Encoder by Unseen Text
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 评估适应性语言编码器在未见文本上的表现
- en: 'In the second experiment, we investigate the ability of our method to generalize
    to unseen commands by using pre-trained language text encoder and codebook-based
    vector quantization. Specifically, for each representative motion, we test our
    method with rephrased commands, which are unseen during training. Our test captions
    consist of three parts: the verbs (e.g., ”walk”), adverbs describing the direction
    (e.g., ”forward”), and adverbs describing the features (e.g., ”carefully”). Then
    we generate a large number of related synonyms and combine them randomly as our
    train dataset and test dataset for fine-tuning the CLIP text encoder. In Table [II](#S5.T2
    "TABLE II ‣ V-A Solve Downstream Tasks with LLMs Planner ‣ V Experiments ‣ Prompt,
    Plan, Perform: LLM-based Humanoid Control via Quantized Imitation Learning"),
    we show the accuracy of generating appropriate actions in response to unseen commands
    and also compare the results with the model without fine-tuning. To illustrate,
    during the policy training phase, a familiar text command such as ”run forward”
    might be used, while an equivalent but previously unseen command could be ”rush
    ahead rapidly”. In this experiment, all outputs from the text encoder are fed
    into codebook-based vector quantization. This step is essential as the policy
    is designed to manage only those captions that have been encountered during its
    training phase. To evaluate the system’s robustness against novel input, we introduce
    unseen synonymous captions as test queries. The accuracy rate is calculated based
    on the network’s ability to effectively map previously unseen commands to captions
    that were encountered during the policy training phase. According to our experiments,
    our approach can handle unseen commands. This helps to enhance the robustness
    of our approach, as the output of the large language model is stochastic to some
    extent, and thus the output of natural language commands needs to be limited to
    what is acceptable for the model. Moreover, this is also important for future
    applications of such an approach to direct human use of language control, where
    user-input commands are much more uncontrollable.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '在第二个实验中，我们通过使用预训练语言文本编码器和基于代码簿的向量量化，探讨了我们的方法对未见命令的泛化能力。具体来说，对于每个代表性的动作，我们用在训练过程中未见过的重述命令来测试我们的方法。我们的测试标题由三部分组成：动词（例如，“walk”），描述方向的副词（例如，“forward”），以及描述特征的副词（例如，“carefully”）。然后，我们生成大量相关同义词，并将它们随机组合，作为训练数据集和测试数据集，用于微调CLIP文本编码器。在表格[II](#S5.T2
    "TABLE II ‣ V-A Solve Downstream Tasks with LLMs Planner ‣ V Experiments ‣ Prompt,
    Plan, Perform: LLM-based Humanoid Control via Quantized Imitation Learning")中，我们展示了生成适当动作的准确性，以响应未见命令，并将结果与未经微调的模型进行比较。为了说明，在策略训练阶段，可能会使用诸如“run
    forward”这样的熟悉文本命令，而等效但以前未见过的命令可能是“rush ahead rapidly”。在这个实验中，文本编码器的所有输出都被输入到基于代码簿的向量量化中。这一步是必要的，因为策略设计用于仅处理在训练阶段遇到的标题。为了评估系统对新颖输入的鲁棒性，我们引入了未见的同义标题作为测试查询。准确率是根据网络有效地将以前未见过的命令映射到在策略训练阶段遇到的标题的能力来计算的。根据我们的实验，我们的方法可以处理未见命令。这有助于提高我们方法的鲁棒性，因为大型语言模型的输出在某种程度上是随机的，因此自然语言命令的输出需要限制在模型可接受的范围内。此外，这对于将来这种方法在直接人机语言控制中的应用也很重要，在这种应用中，用户输入的命令往往更加不可控。'
- en: V-C Comparison of General Task Rewards
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 一般任务奖励的比较
- en: 'In this section, we evaluate the efficacy of three distinct reward structures
    for controlling the robot’s orientation. The first reward structure only focuses
    on the orientation of the robot’s root, the second emphasizes the orientation
    of the robot’s direction of movement, and the third incorporates the orientation
    of both the robot’s root and its two hips. We set the desired direction of the
    robot along the x-axis. Figure [6](#S5.F6 "Figure 6 ‣ V-A Solve Downstream Tasks
    with LLMs Planner ‣ V Experiments ‣ Prompt, Plan, Perform: LLM-based Humanoid
    Control via Quantized Imitation Learning") illustrates the average error of the
    robot’s movement concerning the direction of the target during ”dodge backward”
    and “run forward”. Our experiments suggest that rewarding solely based on root
    orientation does not offer effective control over the robot’s forward movement.
    Utilizing a reward structure that considers both the robot’s direction of movement
    and root orientation results in a conflict that inhibits the successful execution
    of the ”backward ducking” action. Furthermore, for upper-body motions that do
    not involve overall robot movement, a movement-based reward causes the robot to
    persist in moving forward. In contrast, our general reward scheme allows the robot
    to maintain its initial position during upper-body actions. Therefore, we propose
    a general reward structure that includes both the root and hip orientations. This
    structure effectively maintains the desired orientation of the robot’s upper body
    while also facilitating precise control over its direction of movement.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了三种不同奖励结构在控制机器人方向上的有效性。第一种奖励结构仅关注机器人的根部方向，第二种则强调机器人的运动方向，第三种则结合了机器人的根部和两侧臀部的方向。我们将机器人期望的方向设置为x轴。图[6](#S5.F6
    "图 6 ‣ V-A 使用LLMs规划器解决下游任务 ‣ V 实验 ‣ 提示、计划、执行：基于量化模仿学习的LLM控制仿人机器人")展示了机器人在“向后躲避”和“向前跑”过程中，机器人运动方向相对于目标方向的平均误差。我们的实验表明，单纯基于根部方向的奖励无法有效控制机器人的前进运动。考虑到机器人运动方向和根部方向的奖励结构会导致冲突，从而抑制“向后蹲下”动作的成功执行。此外，对于不涉及整体机器人运动的上半身动作，基于运动的奖励会导致机器人持续向前移动。相反，我们的通用奖励方案允许机器人在上半身动作期间保持初始位置。因此，我们提出了一种包含根部和臀部方向的通用奖励结构。这种结构有效地保持了机器人上半身的期望方向，同时也便于对运动方向进行精确控制。
- en: VI Conclusion and Limitation
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论与局限
- en: In this work, we propose a framework that combines a single adversarial imitation
    learning policy and LLMs to perform complex tasks by scheduling the skills. Moreover,
    we design the general reward to ensure that a single control policy is capable
    of addressing a majority of requirements. Finally, our experiments confirm that
    the framework we proposed efficiently solves complex tasks and adapts to uncertain
    semantic outputs of LLMs by introducing codebook-based vector quantization. However,
    the robot models we used with reference motion data are relatively ideal. Our
    future work is to generate more practical data and apply the framework to real
    humanoid robots.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一个框架，将单一的对抗模仿学习策略与LLMs结合，通过调度技能来执行复杂任务。此外，我们设计了通用奖励，以确保单一控制策略能够满足大多数需求。最后，我们的实验确认，我们提出的框架有效地解决了复杂任务，并通过引入基于代码本的向量量化来适应LLMs的不确定语义输出。然而，我们使用的机器人模型及参考运动数据相对理想。我们未来的工作是生成更实际的数据，并将该框架应用于真实的仿人机器人。
- en: VII ACKNOWLEDGMENT
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 致谢
- en: The authors would like to extend their sincere gratitude to Xue Bin (Jason)
    Peng for his invaluable suggestions and insights that have significantly contributed
    to the improvement of this project. His expertise and timely advice have been
    a cornerstone in the successful completion of our work.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 作者对Xue Bin (Jason) Peng表示诚挚的感谢，他的宝贵建议和见解对项目的改进做出了重要贡献。他的专业知识和及时建议是我们成功完成工作的基石。
- en: References
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Ho and S. Ermon, “Generative adversarial imitation learning,” *Advances
    in neural information processing systems*, vol. 29, 2016.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Ho 和 S. Ermon，“生成对抗模仿学习，” *神经信息处理系统进展*，第29卷，2016年。'
- en: '[2] A. Jolicoeur-Martineau, “The relativistic discriminator: a key element
    missing from standard gan,” *arXiv preprint arXiv:1807.00734*, 2018.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. Jolicoeur-Martineau，“相对论判别器：标准gan中缺失的关键元素，” *arXiv 预印本 arXiv:1807.00734*，2018年。'
- en: '[3] X. B. Peng, Y. Guo, L. Halper, S. Levine, and S. Fidler, “Ase: Large-scale
    reusable adversarial skill embeddings for physically simulated characters,” *ACM
    Transactions On Graphics (TOG)*, vol. 41, no. 4, pp. 1–17, 2022.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] X. B. Peng, Y. Guo, L. Halper, S. Levine 和 S. Fidler，“Ase: 大规模可重用的对抗性技能嵌入用于物理模拟角色，”
    *ACM图形学学报（TOG）*，第41卷，第4期，第1–17页，2022年。'
- en: '[4] Y. Chebotar, K. Hausman, Y. Lu, T. Xiao, D. Kalashnikov, J. Varley, A. Irpan,
    B. Eysenbach, R. Julian, C. Finn *et al.*, “Actionable models: Unsupervised offline
    reinforcement learning of robotic skills,” *arXiv preprint arXiv:2104.07749*,
    2021.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Y. Chebotar, K. Hausman, Y. Lu, T. Xiao, D. Kalashnikov, J. Varley, A.
    Irpan, B. Eysenbach, R. Julian, C. Finn *等人*，“可操作模型：机器人技能的无监督离线强化学习，” *arXiv 预印本
    arXiv:2104.07749*，2021年。'
- en: '[5] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *et al.*, “Learning transferable visual models
    from natural language supervision,” in *International conference on machine learning*.   PMLR,
    2021, pp. 8748–8763.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *等人*，“从自然语言监督中学习可转移的视觉模型，” 在 *国际机器学习会议*。 PMLR，2021年，第8748–8763页。'
- en: '[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,
    S. Agarwal, K. Slama, A. Ray *et al.*, “Training language models to follow instructions
    with human feedback,” *Advances in Neural Information Processing Systems*, vol. 35,
    pp. 27 730–27 744, 2022.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,
    S. Agarwal, K. Slama, A. Ray *等人*，“训练语言模型以遵循人类反馈的指令，” *神经信息处理系统进展*，第35卷，第27,730–27,744页，2022年。'
- en: '[7] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale *et al.*, “Llama 2: Open foundation and fine-tuned
    chat models,” *arXiv preprint arXiv:2307.09288*, 2023.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale *等人*，“Llama 2: 开放基础和微调的聊天模型，” *arXiv
    预印本 arXiv:2307.09288*，2023年。'
- en: '[8] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson,
    I. Mordatch, Y. Chebotar *et al.*, “Inner monologue: Embodied reasoning through
    planning with language models,” *arXiv preprint arXiv:2207.05608*, 2022.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J.
    Tompson, I. Mordatch, Y. Chebotar *等人*，“内心独白：通过语言模型进行具身推理规划，” *arXiv 预印本 arXiv:2207.05608*，2022年。'
- en: '[9] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models as zero-shot
    planners: Extracting actionable knowledge for embodied agents,” in *International
    Conference on Machine Learning*.   PMLR, 2022, pp. 9118–9147.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] W. Huang, P. Abbeel, D. Pathak 和 I. Mordatch，“语言模型作为零-shot 规划器：为具身代理提取可操作的知识，”
    在 *国际机器学习会议*。 PMLR，2022年，第9118–9147页。'
- en: '[10] N. Di Palo, A. Byravan, L. Hasenclever, M. Wulfmeier, N. Heess, and M. Riedmiller,
    “Towards a unified agent with foundation models,” in *Workshop on Reincarnating
    Reinforcement Learning at ICLR 2023*, 2023.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] N. Di Palo, A. Byravan, L. Hasenclever, M. Wulfmeier, N. Heess 和 M. Riedmiller，“朝着统一的基础模型代理迈进，”
    在 *ICLR 2023 强化学习重生研讨会*，2023年。'
- en: '[11] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, “Calvin: A benchmark
    for language-conditioned policy learning for long-horizon robot manipulation tasks,”
    *IEEE Robotics and Automation Letters*, vol. 7, no. 3, pp. 7327–7334, 2022.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] O. Mees, L. Hermann, E. Rosete-Beas 和 W. Burgard，“Calvin: 用于长时间跨度机器人操作任务的语言条件策略学习基准，”
    *IEEE机器人与自动化快报*，第7卷，第3期，第7327–7334页，2022年。'
- en: '[12] O. Mees, L. Hermann, and W. Burgard, “What matters in language conditioned
    robotic imitation learning over unstructured data,” *IEEE Robotics and Automation
    Letters*, vol. 7, no. 4, pp. 11 205–11 212, 2022.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] O. Mees, L. Hermann 和 W. Burgard，“在非结构化数据上，语言条件下的机器人模仿学习中什么最重要，” *IEEE机器人与自动化快报*，第7卷，第4期，第11,205–11,212页，2022年。'
- en: '[13] O. Mees, J. Borja-Diaz, and W. Burgard, “Grounding language with visual
    affordances over unstructured data,” in *2023 IEEE International Conference on
    Robotics and Automation (ICRA)*.   IEEE, 2023, pp. 11 576–11 582.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] O. Mees, J. Borja-Diaz 和 W. Burgard，“利用视觉可用性在非结构化数据上建立语言基础，” 在 *2023年IEEE国际机器人与自动化大会（ICRA）*。
    IEEE，2023年，第11,576–11,582页。'
- en: '[14] M. Shridhar, L. Manuelli, and D. Fox, “Cliport: What and where pathways
    for robotic manipulation,” in *Conference on Robot Learning*.   PMLR, 2022, pp.
    894–906.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] M. Shridhar, L. Manuelli 和 D. Fox，“Cliport: 机器人操作的‘什么’和‘哪里’路径，” 在 *机器人学习大会*。
    PMLR，2022年，第894–906页。'
- en: '[15] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,
    and A. Zeng, “Code as policies: Language model programs for embodied control,”
    in *2023 IEEE International Conference on Robotics and Automation (ICRA)*.   IEEE,
    2023, pp. 9493–9500.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,
    和 A. Zeng，“代码即策略：用于具身控制的语言模型程序”，在*2023 IEEE 国际机器人与自动化会议（ICRA）*上。IEEE，2023年，页码9493–9500。'
- en: '[16] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn,
    C. Fu, K. Gopalakrishnan, K. Hausman *et al.*, “Do as i can, not as i say: Grounding
    language in robotic affordances,” *arXiv preprint arXiv:2204.01691*, 2022.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn,
    C. Fu, K. Gopalakrishnan, K. Hausman *等*，“做我能做的，不做我说的：将语言与机器人可用性结合”，*arXiv 预印本
    arXiv:2204.01691*，2022年。'
- en: '[17] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski,
    T. Ding, D. Driess, A. Dubey, C. Finn *et al.*, “Rt-2: Vision-language-action
    models transfer web knowledge to robotic control,” *arXiv preprint arXiv:2307.15818*,
    2023.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski,
    T. Ding, D. Driess, A. Dubey, C. Finn *等*，“Rt-2: 视觉-语言-行动模型将网络知识转移到机器人控制”，*arXiv
    预印本 arXiv:2307.15818*，2023年。'
- en: '[18] Y. Guo, Y.-J. Wang, L. Zha, Z. Jiang, and J. Chen, “Doremi: Grounding
    language model by detecting and recovering from plan-execution misalignment,”
    *arXiv preprint arXiv:2307.00329*, 2023.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Y. Guo, Y.-J. Wang, L. Zha, Z. Jiang, 和 J. Chen，“Doremi: 通过检测和恢复计划执行对齐误差来为语言模型赋权”，*arXiv
    预印本 arXiv:2307.00329*，2023年。'
- en: '[19] A. Hu, G. Corrado, N. Griffiths, Z. Murez, C. Gurau, H. Yeo, A. Kendall,
    R. Cipolla, and J. Shotton, “Model-based imitation learning for urban driving,”
    *Advances in Neural Information Processing Systems*, vol. 35, pp. 20 703–20 716,
    2022.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. Hu, G. Corrado, N. Griffiths, Z. Murez, C. Gurau, H. Yeo, A. Kendall,
    R. Cipolla, 和 J. Shotton，“基于模型的城市驾驶模仿学习”，*神经信息处理系统进展*，第35卷，页码20 703–20 716，2022年。'
- en: '[20] L. Le Mero, D. Yi, M. Dianati, and A. Mouzakitis, “A survey on imitation
    learning techniques for end-to-end autonomous vehicles,” *IEEE Transactions on
    Intelligent Transportation Systems*, vol. 23, no. 9, pp. 14 128–14 147, 2022.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] L. Le Mero, D. Yi, M. Dianati, 和 A. Mouzakitis，“针对端到端自主车辆的模仿学习技术综述”，*IEEE
    智能交通系统汇刊*，第23卷，第9期，页码14 128–14 147，2022年。'
- en: '[21] J. Hua, L. Zeng, G. Li, and Z. Ju, “Learning for a robot: Deep reinforcement
    learning, imitation learning, transfer learning,” *Sensors*, vol. 21, no. 4, p.
    1278, 2021.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Hua, L. Zeng, G. Li, 和 Z. Ju，“为机器人学习：深度强化学习、模仿学习、迁移学习”，*传感器*，第21卷，第4期，页码1278，2021年。'
- en: '[22] E. Johns, “Coarse-to-fine imitation learning: Robot manipulation from
    a single demonstration,” in *2021 IEEE international conference on robotics and
    automation (ICRA)*.   IEEE, 2021, pp. 4613–4619.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] E. Johns，“粗到精的模仿学习：从单一演示中进行机器人操作”，在*2021 IEEE 国际机器人与自动化会议（ICRA）*上。IEEE，2021年，页码4613–4619。'
- en: '[23] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu, “Viola: Object-centric imitation
    learning for vision-based robot manipulation,” in *Conference on Robot Learning*.   PMLR,
    2023, pp. 1199–1210.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Y. Zhu, A. Joshi, P. Stone, 和 Y. Zhu，“Viola: 基于视觉的机器人操作的对象中心模仿学习”，在*机器人学习会议*上。PMLR，2023年，页码1199–1210。'
- en: '[24] F. Torabi, G. Warnell, and P. Stone, “Behavioral cloning from observation,”
    *arXiv preprint arXiv:1805.01954*, 2018.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] F. Torabi, G. Warnell, 和 P. Stone，“从观察中进行行为克隆”，*arXiv 预印本 arXiv:1805.01954*，2018年。'
- en: '[25] X. B. Peng, Z. Ma, P. Abbeel, S. Levine, and A. Kanazawa, “Amp: Adversarial
    motion priors for stylized physics-based character control,” *ACM Transactions
    on Graphics (ToG)*, vol. 40, no. 4, pp. 1–20, 2021.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] X. B. Peng, Z. Ma, P. Abbeel, S. Levine, 和 A. Kanazawa，“Amp: 用于风格化基于物理的角色控制的对抗运动先验”，*ACM
    图形学汇刊 (ToG)*，第40卷，第4期，页码1–20，2021年。'
- en: '[26] C. Li, S. Blaes, P. Kolev, M. Vlastelica, J. Frey, and G. Martius, “Versatile
    skill control via self-supervised adversarial imitation of unlabeled mixed motions,”
    in *2023 IEEE International Conference on Robotics and Automation (ICRA)*.   IEEE,
    2023, pp. 2944–2950.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] C. Li, S. Blaes, P. Kolev, M. Vlastelica, J. Frey, 和 G. Martius，“通过自监督对抗模仿未标记混合动作实现多功能技能控制”，在*2023
    IEEE 国际机器人与自动化会议（ICRA）*上。IEEE，2023年，页码2944–2950。'
- en: '[27] S. Pateria, B. Subagdja, A.-h. Tan, and C. Quek, “Hierarchical reinforcement
    learning: A comprehensive survey,” *ACM Computing Surveys (CSUR)*, vol. 54, no. 5,
    pp. 1–35, 2021.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. Pateria, B. Subagdja, A.-h. Tan, 和 C. Quek，“层次化强化学习：全面综述”，*ACM 计算调查
    (CSUR)*，第54卷，第5期，页码1–35，2021年。'
- en: '[28] Y. Ji, Z. Li, Y. Sun, X. B. Peng, S. Levine, G. Berseth, and K. Sreenath,
    “Hierarchical reinforcement learning for precise soccer shooting skills using
    a quadrupedal robot,” in *2022 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS)*.   IEEE, 2022, pp. 1479–1486.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Ji, Z. Li, Y. Sun, X. B. Peng, S. Levine, G. Berseth, 和 K. Sreenath，“用于精确足球射门技能的层次化强化学习，使用四足机器人，”
    载于*2022 IEEE/RSJ国际智能机器人与系统会议（IROS）*。 IEEE, 2022，第1479–1486页。'
- en: '[29] S. Huang, Z. Wang, J. Zhou, and J. Lu, “Planning irregular object packing
    via hierarchical reinforcement learning,” *IEEE Robotics and Automation Letters*,
    vol. 8, no. 1, pp. 81–88, 2022.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S. Huang, Z. Wang, J. Zhou, 和 J. Lu，“通过层次化强化学习规划不规则物体的打包，” *IEEE机器人与自动化快报*，第8卷，第1期，第81–88页，2022。'
- en: '[30] M. Eppe, C. Gumbsch, M. Kerzel, P. D. Nguyen, M. V. Butz, and S. Wermter,
    “Intelligent problem-solving as integrated hierarchical reinforcement learning,”
    *Nature Machine Intelligence*, vol. 4, no. 1, pp. 11–20, 2022.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] M. Eppe, C. Gumbsch, M. Kerzel, P. D. Nguyen, M. V. Butz, 和 S. Wermter，“作为集成层次化强化学习的智能问题解决，”
    *Nature机器智能*，第4卷，第1期，第11–20页，2022。'
- en: '[31] J. Juravsky, Y. Guo, S. Fidler, and X. B. Peng, “Padl: Language-directed
    physics-based character control,” in *SIGGRAPH Asia 2022 Conference Papers*, 2022,
    pp. 1–9.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Juravsky, Y. Guo, S. Fidler, 和 X. B. Peng，“Padl: 语言驱动的基于物理的角色控制，” 载于*SIGGRAPH
    Asia 2022会议论文*，2022，第1–9页。'
- en: '[32] C. Tessler, Y. Kasten, Y. Guo, S. Mannor, G. Chechik, and X. B. Peng,
    “Calm: Conditional adversarial latent models for directable virtual characters,”
    in *ACM SIGGRAPH 2023 Conference Proceedings*, 2023, pp. 1–9.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] C. Tessler, Y. Kasten, Y. Guo, S. Mannor, G. Chechik, 和 X. B. Peng，“Calm:
    条件对抗潜在模型用于可导向虚拟角色，” 载于*ACM SIGGRAPH 2023会议论文集*，2023，第1–9页。'
- en: '[33] J. Ma, H. Xu, J. Jiang, X. Mei, and X.-P. Zhang, “Ddcgan: A dual-discriminator
    conditional generative adversarial network for multi-resolution image fusion,”
    *IEEE Transactions on Image Processing*, vol. 29, pp. 4980–4995, 2020.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] J. Ma, H. Xu, J. Jiang, X. Mei, 和 X.-P. Zhang，“Ddcgan: 一种双鉴别器条件生成对抗网络用于多分辨率图像融合，”
    *IEEE图像处理学报*，第29卷，第4980–4995页，2020。'
- en: '[34] A. S. Polydoros and L. Nalpantidis, “Survey of model-based reinforcement
    learning: Applications on robotics,” *Journal of Intelligent & Robotic Systems*,
    vol. 86, no. 2, pp. 153–173, 2017.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] A. S. Polydoros 和 L. Nalpantidis，“基于模型的强化学习综述：在机器人上的应用，” *智能与机器人系统杂志*，第86卷，第2期，第153–173页，2017。'
- en: '[35] A. R. Mahmood, D. Korenkevych, G. Vasan, W. Ma, and J. Bergstra, “Benchmarking
    reinforcement learning algorithms on real-world robots,” in *Conference on robot
    learning*.   PMLR, 2018, pp. 561–591.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] A. R. Mahmood, D. Korenkevych, G. Vasan, W. Ma, 和 J. Bergstra，“在现实世界机器人上对强化学习算法进行基准测试，”
    载于*机器人学习会议*。 PMLR, 2018，第561–591页。'
- en: '[36] P. Kormushev, S. Calinon, and D. G. Caldwell, “Reinforcement learning
    in robotics: Applications and real-world challenges,” *Robotics*, vol. 2, no. 3,
    pp. 122–148, 2013.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] P. Kormushev, S. Calinon, 和 D. G. Caldwell，“机器人中的强化学习：应用与现实世界挑战，” *机器人学*，第2卷，第3期，第122–148页，2013。'
- en: '[37] F. Torabi, G. Warnell, and P. Stone, “Generative adversarial imitation
    from observation,” *arXiv preprint arXiv:1807.06158*, 2018.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] F. Torabi, G. Warnell, 和 P. Stone，“从观察中生成对抗模仿，” *arXiv预印本 arXiv:1807.06158*，2018。'
- en: '[38] M. Menéndez, J. Pardo, L. Pardo, and M. Pardo, “The jensen-shannon divergence,”
    *Journal of the Franklin Institute*, vol. 334, no. 2, pp. 307–318, 1997.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] M. Menéndez, J. Pardo, L. Pardo, 和 M. Pardo，“詹森-香农散度，” *富兰克林学报*，第334卷，第2期，第307–318页，1997。'
- en: '[39] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller,
    N. Rudin, A. Allshire, A. Handa *et al.*, “Isaac gym: High performance gpu-based
    physics simulation for robot learning,” *arXiv preprint arXiv:2108.10470*, 2021.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D.
    Hoeller, N. Rudin, A. Allshire, A. Handa *等*，“Isaac gym: 高性能基于GPU的物理模拟用于机器人学习，”
    *arXiv预印本 arXiv:2108.10470*，2021。'
- en: '[40] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv preprint arXiv:1707.06347*, 2017.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, 和 O. Klimov，“近端策略优化算法，”
    *arXiv预印本 arXiv:1707.06347*，2017。'
