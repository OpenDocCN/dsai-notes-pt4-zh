- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:51:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:34
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LLM-QAT: Data-Free Quantization Aware Training for Large Language Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'LLM-QAT: 无数据量化感知训练用于大语言模型'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.17888](https://ar5iv.labs.arxiv.org/html/2305.17888)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.17888](https://ar5iv.labs.arxiv.org/html/2305.17888)
- en: Zechun Liu
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zechun Liu
- en: Reality Labs, Meta Inc.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Reality Labs, Meta Inc.
- en: zechunliu@meta.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: zechunliu@meta.com
- en: '&Barlas Oğuz^†^†footnotemark:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&Barlas Oğuz^†^†脚注标记：'
- en: Meta AI
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Meta AI
- en: barlaso@meta.com
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: barlaso@meta.com
- en: '&Changsheng Zhao'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Changsheng Zhao'
- en: Reality Labs, Meta Inc.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Reality Labs, Meta Inc.
- en: cszhao@meta.com
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: cszhao@meta.com
- en: Ernie Chang
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Ernie Chang
- en: Reality Labs, Meta Inc.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Reality Labs, Meta Inc.
- en: '&Pierre Stock'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '&Pierre Stock'
- en: Meta AI
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Meta AI
- en: '&Yashar Mehdad'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '&Yashar Mehdad'
- en: Meta AI
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Meta AI
- en: '&Yangyang Shi'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '&Yangyang Shi'
- en: Reality Labs, Meta Inc.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Reality Labs, Meta Inc.
- en: Raghuraman Krishnamoorthi
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Raghuraman Krishnamoorthi
- en: Reality Labs, Meta Inc.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Reality Labs, Meta Inc.
- en: '&Vikas Chandra'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '&Vikas Chandra'
- en: Reality Labs, Meta Inc. Equal contribution
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Reality Labs, Meta Inc. 平等贡献
- en: Abstract
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Several post-training quantization methods have been applied to large language
    models (LLMs), and have been shown to perform well down to 8-bits. We find that
    these methods break down at lower bit precision, and investigate quantization
    aware training for LLMs (LLM-QAT) to push quantization levels even further. We
    propose a data-free distillation method that leverages generations produced by
    the pre-trained model, which better preserves the original output distribution
    and allows quantizing any generative model independent of its training data, similar
    to post-training quantization methods. In addition to quantizing weights and activations,
    we also quantize the KV cache, which is critical for increasing throughput and
    support long sequence dependencies at current model sizes. We experiment with
    LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits.
    We observe large improvements over training-free methods, especially in the low-bit
    settings.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 多种后训练量化方法已应用于大语言模型（LLMs），并在8位精度下表现良好。我们发现这些方法在更低位精度下失效，进而研究了大语言模型的量化感知训练（LLM-QAT），以进一步推动量化水平。我们提出了一种无数据蒸馏方法，利用预训练模型生成的数据，这种方法更好地保留了原始输出分布，并允许对任何生成模型进行量化，无论其训练数据如何，类似于后训练量化方法。除了量化权重和激活值外，我们还量化了KV缓存，这对提高吞吐量和支持当前模型规模的长序列依赖至关重要。我们在量化水平降至4位时，对7B、13B和30B的LLaMA模型进行了实验。我们观察到，相比于无训练方法，尤其是在低位设置下，改进显著。
- en: 1 Introduction
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Following GPT-3 (Brown et al., [2020](#bib.bib3)), several families of large
    language models (LLMs) such as OPT (Zhang et al., [2022](#bib.bib43)), PALM (Chowdhery
    et al., [2022](#bib.bib6)), BLOOM (Scao et al., [2022](#bib.bib32)), Chinchilla (Hoffmann
    et al., [2022](#bib.bib15)) and LLaMA (Touvron et al., [2023](#bib.bib36)) have
    established that increasing model size leads to improved model capabilities. As
    a result, language models with tens of billions or even hundreds of billions of
    parameters have become the norm in today’s AI landscape. Despite the growing excitement
    around LLMs, serving such models to the benefit of billions of users faces significant
    hurdles due to their large computational cost and environmental footprint.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 继GPT-3（Brown等人，[2020](#bib.bib3)）之后，多个大语言模型（LLMs）家族如OPT（Zhang等人，[2022](#bib.bib43)）、PALM（Chowdhery等人，[2022](#bib.bib6)）、BLOOM（Scao等人，[2022](#bib.bib32)）、Chinchilla（Hoffmann等人，[2022](#bib.bib15)）和LLaMA（Touvron等人，[2023](#bib.bib36)）已建立模型规模的增加带来模型能力的提升。因此，拥有数十亿甚至数百亿参数的语言模型已成为当今AI领域的常态。尽管对LLMs的兴趣日益增长，但由于其巨大的计算成本和环境影响，为数十亿用户服务此类模型仍面临重大障碍。
- en: Fortunately, there has been an increasing effort to accurately quantize LLMs,
    with multiple recent works (Xiao et al., [2022](#bib.bib37); Yao et al., [2022](#bib.bib38))
    focusing on 8-bit post-training quantization of weights and activations and achieving
    little to no loss of accuracy. However, a 65 billion parameter LLaMA model still
    takes up 65GB of GPU memory with only its weights. Moreover, the key-value (KV)
    cache holding activations for the attention layers can easily go into the tens
    of GBs, and is the throughput bottleneck in the long sequence length regime common
    in today’s applications. The aforementioned works do not consider KV cache quantization
    along with weight and activation quantization. Unfortunately, SoTA post-training
    quantization methods dramatically degrade in quality when pushed beyond 8-bits.
    For higher quantization levels, we find it necessary to resort to quantization-aware
    training (QAT).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，越来越多的努力致力于准确量化LLMs，最近的多项研究（Xiao等，[2022](#bib.bib37)；Yao等，[2022](#bib.bib38)）专注于8位后训练量化权重和激活，并实现了几乎没有精度损失。然而，一个65亿参数的LLaMA模型仅其权重就占用65GB的GPU内存。此外，持有注意力层激活的关键值（KV）缓存容易达到数十GB，并且在当今应用中常见的长序列长度中，是吞吐量的瓶颈。上述研究未考虑KV缓存量化与权重和激活量化的结合。不幸的是，当超出8位时，SoTA后训练量化方法在质量上显著下降。对于更高的量化级别，我们发现有必要诉诸于量化感知训练（QAT）。
- en: To our knowledge, QAT for LLMs has not been investigated before. This is understandable
    for two reasons. First, LLM training is technically difficult and resource intensive.
    Second, QAT needs training data, which for LLMs is difficult to obtain. The sheer
    scale and diversity of pre-training data is itself an obstacle. Pre-processing
    might be prohibitive, or worse, some data might simply not be available due to
    legal restrictions. It is also incresingly common to train LLMs in multiple stages,
    involving instruction tuning and reinforcement learning (Ouyang et al., [2022](#bib.bib27)),
    which would be very difficult to replicate during QAT. In this work, we side-step
    this issue by using generated data from the LLM itself for knowledge distillation.
    This simple workaround, which we refer to as *data-free* knowledge-distillation
    is applicable to any generative model independent of whether or not the original
    training data is available. We show that this method is better able to preserve
    the original model’s output distribution, even compared to training on large subsets
    of the original training set. Moreover, we can successfully distill quantized
    models using only a small set (100k) of sampled data, thus keeping computational
    costs reasonable. All of our experiments are conducted using a single 8-gpu training
    node.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，LLMs的QAT尚未被研究过。这有两个原因。首先，LLM训练在技术上困难且资源密集。其次，QAT需要训练数据，而LLMs的训练数据难以获取。预训练数据的巨大规模和多样性本身就是一个障碍。预处理可能是禁止的，或者更糟的是，由于法律限制，某些数据可能根本无法获得。还越来越常见的是，在多个阶段训练LLMs，包括指令调优和强化学习（Ouyang等，[2022](#bib.bib27)），这在QAT期间很难复制。在这项工作中，我们通过使用LLM自身生成的数据进行知识蒸馏来规避这个问题。这种简单的变通方法，我们称之为*无数据*知识蒸馏，适用于任何生成模型，无论原始训练数据是否可用。我们展示了这种方法比在原始训练集的大子集上训练更好地保持了原始模型的输出分布。此外，我们可以仅使用一小部分（100k）采样数据成功地蒸馏量化模型，从而保持计算成本合理。我们所有的实验都在一个8-GPU训练节点上进行。
- en: 'As a result, we are able to distill the 7B, 13B and 30B LLaMA models with weights
    and KV cache quantized down to 4-bits. In this regard, our approach exhibits significant
    enhancements in quality compared to post-training quantization. Notably, larger
    models employing QAT outperform smaller models utilizing floating-point 16-bit
    representations, despite having similar model sizes. Furthermore, we have successfully
    quantized activations to 6-bit precision, surpassing what was possible with existing
    methods. For a comprehensive analysis of our experimental results and detailed
    ablations, please refer to Section [3](#S3 "3 Experiments ‣ LLM-QAT: Data-Free
    Quantization Aware Training for Large Language Models").'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '结果是，我们能够将7B、13B和30B LLaMA模型的权重和KV缓存量化到4位。在这方面，我们的方法在质量上相较于后训练量化表现出显著的提升。值得注意的是，采用QAT的更大模型在性能上超越了使用浮点16位表示的小模型，尽管模型大小相似。此外，我们成功地将激活量化到6位精度，超越了现有方法的可能性。有关我们实验结果的全面分析和详细消融，请参阅第[3](#S3
    "3 Experiments ‣ LLM-QAT: Data-Free Quantization Aware Training for Large Language
    Models")节。'
- en: In summary, we present the first application of QAT to LLMs, resulting in the
    first accurate 4-bit quantized LLMs. We also demonstrate quantizing the KV cache
    simultaneously with weights and activations, which is critical to alleviate throughput
    bottlenecks for long sequence generation. All of this is achieved by a novel *data-free*
    distillation method, which makes QAT practical for large pre-trained generative
    models.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们展示了 QAT 在 LLMs 上的首次应用，获得了首个准确的 4-bit 量化 LLMs。我们还展示了如何在量化权重和激活的同时对 KV
    缓存进行量化，这对于缓解长序列生成的吞吐量瓶颈至关重要。所有这些都是通过一种新颖的 *无数据* 蒸馏方法实现的，这使得 QAT 在大型预训练生成模型中变得实用。
- en: '![Refer to caption](img/1575173a4b8b69b261690f7d9593a9d5.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1575173a4b8b69b261690f7d9593a9d5.png)'
- en: 'Figure 1: Overview of LLM-QAT. We generate data from the pretrained model with
    next token generation, which is sampled from top-k candidates. Then we use the
    generated data as input and the teacher model prediction as label to guide quantized
    model finetuning.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：LLM-QAT 概述。我们从预训练模型生成数据，通过下一个 token 生成，这些数据从 top-k 候选中采样。然后我们使用生成的数据作为输入，教师模型的预测作为标签，来指导量化模型的微调。
- en: 2 Method
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: 'Quantizing large language models (LLMs) using quantization-aware training (QAT)
    is a nontrivial task with challenges in two key aspects. First, LLMs are pre-trained
    to excel in zero-shot generalization, and it is crucial to preserve this capability
    after quantization. Therefore, selecting an appropriate fine-tuning dataset is
    important. If the QAT data is too narrow in domain or significantly different
    than the original pre-training distribution, this is likely to hurt model performance.
    On the other hand, it is difficult to replicate the original training setup exactly,
    due to the scale and complexity of LLM training. In Section [2.1](#S2.SS1 "2.1
    Data-free Distillation ‣ 2 Method ‣ LLM-QAT: Data-Free Quantization Aware Training
    for Large Language Models"), we introduce data-free quantization-aware training
    (QAT) which produces QAT data using next token data generation. This method demonstrates
    superior performance compared to using subsets of the original pre-training data.
    Second, LLMs exhibit unique weight and activation distributions characterized
    by a significant presence of outliers, which distinguishes them from smaller models.
    Consequently, the state-of-the-art quantization clipping methods for small models
    do not work out of the box for LLMs. In Section [2.2](#S2.SS2 "2.2 Quantization-Aware
    Training ‣ 2 Method ‣ LLM-QAT: Data-Free Quantization Aware Training for Large
    Language Models"), we identify suitable quantizers for LLMs.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '使用量化感知训练 (QAT) 对大型语言模型 (LLMs) 进行量化是一项具有挑战性的非平凡任务，主要体现在两个关键方面。首先，LLMs 经过预训练以在零样本泛化中表现出色，因此在量化后保持这一能力至关重要。因此，选择合适的微调数据集非常重要。如果
    QAT 数据的领域过于狭窄或与原始预训练分布显著不同，可能会影响模型性能。另一方面，由于 LLM 训练的规模和复杂性，很难精确复现原始训练设置。在第 [2.1](#S2.SS1
    "2.1 Data-free Distillation ‣ 2 Method ‣ LLM-QAT: Data-Free Quantization Aware
    Training for Large Language Models") 节中，我们介绍了无数据量化感知训练 (QAT)，该方法使用下一个 token 数据生成来生成
    QAT 数据。这种方法表现优于使用原始预训练数据的子集。其次，LLMs 显示出独特的权重和激活分布，具有显著的离群点，与较小的模型有所不同。因此，针对小型模型的最先进量化剪裁方法在
    LLMs 上无法直接使用。在第 [2.2](#S2.SS2 "2.2 Quantization-Aware Training ‣ 2 Method ‣ LLM-QAT:
    Data-Free Quantization Aware Training for Large Language Models") 节中，我们确定了适用于
    LLMs 的合适量化器。'
- en: 2.1 Data-free Distillation
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 无数据蒸馏
- en: 'In order to closely synthesize the distribution of the pre-training data with
    a limited amount of fine-tuning data, we proposed next token data generation from
    the original pre-trained model. As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ LLM-QAT: Data-Free Quantization Aware Training for Large Language Models") (a),
    we randomize the first token $$<\!\!start\!\!>. We repeat this iterative
    procedure until we reach either the end of sentence token or the maximum generation
    length.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '为了紧密地将预训练数据的分布与有限量的微调数据进行合成，我们提出了从原始预训练模型生成下一个 token 数据的方法。如图 [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ LLM-QAT: Data-Free Quantization Aware Training for Large
    Language Models") (a) 所示，我们随机化第一个 token $$<\!\!start\!\!>。我们重复这一迭代过程，直到达到句子结束标记或最大生成长度。'
- en: 'We test three different sampling strategies in the next token generation. The
    most straightforward way is to pick the top-1 candidate as the next token. However,
    the generated sentence lacks of diversity and will cyclically repeat several tokens.
    To address this issue, we instead stochastically sample the next token from the
    distribution using the SoftMax output of the pre-trained model as the probability.
    This sampling strategy yields more diverse sentences and greatly enhances the
    accuracy of the fine-tuned student model. Furthermore, we discover that the initial
    few tokens play a crucial role in determining the prediction trend. Therefore,
    it is important for them to have higher confidence. In our generative process,
    we employ a hybrid sampling strategy that deterministically selects the top-1
    predictions for the first 3~5 tokens and stochastically samples the remaining
    tokens. A detailed ablation study comparing different generated data and real
    data is presented in Section[3.3.1](#S3.SS3.SSS1 "3.3.1 Data Choice ‣ 3.3 Ablation
    ‣ 3.2 Main Results ‣ 3.1 Experimental Settings ‣ 3 Experiments ‣ LLM-QAT: Data-Free
    Quantization Aware Training for Large Language Models").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下一步生成中测试了三种不同的采样策略。最简单的方法是选择前1名候选作为下一个标记。然而，生成的句子缺乏多样性，并且会循环重复几个标记。为了解决这个问题，我们改为从分布中随机采样下一个标记，使用预训练模型的SoftMax输出作为概率。该采样策略产生了更多样化的句子，并大大提高了微调学生模型的准确性。此外，我们发现初始几个标记在确定预测趋势中起着关键作用。因此，它们需要更高的置信度。在我们的生成过程中，我们采用了一种混合采样策略，即对前3~5个标记确定性地选择前1名预测，并对剩余标记进行随机采样。详细的消融研究比较了不同生成数据和真实数据，见第[3.3.1节](#S3.SS3.SSS1
    "3.3.1 数据选择 ‣ 3.3 消融 ‣ 3.2 主要结果 ‣ 3.1 实验设置 ‣ 3 实验 ‣ LLM-QAT：大规模语言模型的数据无关量化训练")。
- en: '![Refer to caption](img/a0a143e3bf62a8426feaddcbf5d5ee05.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a0a143e3bf62a8426feaddcbf5d5ee05.png)'
- en: 'Figure 2: Overview of the quantized transformer in LLM-QAT. We quantize all
    the weights and input activations in fully-connected linear layers. The KV cache
    is also quantized if specified.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：LLM-QAT中量化变换器的概述。我们对全连接线性层中的所有权重和输入激活进行量化。如果指定，KV缓存也会被量化。
- en: 2.2 Quantization-Aware Training
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 量化感知训练
- en: 2.2.1 Preliminaries
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 初步知识
- en: 'In this work, we study linear quantization i.e., uniform quantization. Linear
    quantization can be categorized into two categories based on whether the real
    values are clipped or not: MinMax quantization, which preserves all value ranges,
    and clipping-based quantization.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们研究线性量化，即均匀量化。线性量化可以根据实际值是否被裁剪分为两类：保留所有值范围的MinMax量化和基于裁剪的量化。
- en: 'In MinMax quantization, the quantization process can be formulated as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在MinMax量化中，量化过程可以表示为：
- en: '|  | $\mathbf{X}_{\mathbf{Q}}^{i}=\alpha\mathbf{\hat{X}_{Q}}^{i}=\alpha\lfloor\frac{\mathbf{X}_{\mathbf{R}}^{i}-\beta}{\alpha}\rceil+\beta.$
    |  | (1) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{X}_{\mathbf{Q}}^{i}=\alpha\mathbf{\hat{X}_{Q}}^{i}=\alpha\lfloor\frac{\mathbf{X}_{\mathbf{R}}^{i}-\beta}{\alpha}\rceil+\beta.$
    |  | (1) |'
- en: Here $\mathbf{X}_{\mathbf{Q}}$
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里$\mathbf{X}_{\mathbf{Q}}$
- en: 'Compared to the MinMax Quantization, clipping the outliers can help improve
    the precision and allocate more bits to the intermediate values. Thus, many recent
    work (Shen et al., [2020a](#bib.bib33); Zhang et al., [2020](#bib.bib44)) adopts
    clipping-based quantization for transformer-based language models. The quantization
    can be formulated as:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与MinMax量化相比，裁剪异常值有助于提高精度，并将更多的位分配给中间值。因此，许多近期的工作（Shen et al., [2020a](#bib.bib33);
    Zhang et al., [2020](#bib.bib44)）采用了基于裁剪的量化方法用于基于变换器的语言模型。量化可以表示为：
- en: '|  | $\mathbf{X}_{\mathbf{Q}}^{i}=\alpha\mathbf{\hat{X}_{Q}}^{i}=\alpha\lfloor{\rm
    Clip}(\frac{\mathbf{X}_{\mathbf{R}}^{i}-\beta}{\alpha},0,1)\rceil+\beta.$ |  |
    (2) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{X}_{\mathbf{Q}}^{i}=\alpha\mathbf{\hat{X}_{Q}}^{i}=\alpha\lfloor{\rm
    Clip}(\frac{\mathbf{X}_{\mathbf{R}}^{i}-\beta}{\alpha},0,1)\rceil+\beta.$ |  |
    (2) |'
- en: where the scale $\alpha$ can be calculated statistically or learned through
    gradients.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，比例$\alpha$可以通过统计方法计算或通过梯度学习获得。
- en: 2.2.2 Quantization for Large Language Models
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 大规模语言模型的量化
- en: 'Quantization function We illustrate our quantized transformer model in Figure [2](#S2.F2
    $$), causing a substantial loss of information that proves to be difficult to
    recover through fine-tuning. Therefore, we choose to retain these outliers instead.
    Moreover, we find that in the model with the gated linear unit (GLU), the activations
    are weights are mostly symmetrically distributed. Based on our analysis and empirical
    observations, we choose symmetric MinMax quantization for both weights and activations:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 量化函数 我们在图[2](#S2.F2 $$)中展示了我们量化的变换器模型，这会导致大量信息丢失，且这种丢失很难通过微调来恢复。因此，我们选择保留这些离群值。此外，我们发现，在使用门控线性单元（GLU）的模型中，激活和权重主要是对称分布的。基于我们的分析和实证观察，我们选择对权重和激活进行对称的MinMax量化：
- en: '|  | $\mathbf{X}_{\mathbf{Q}}^{i}=\alpha\lfloor\frac{\mathbf{X}_{\mathbf{R}}^{i}}{\alpha}\rceil,\
    \ \ \alpha=\frac{\max(&#124;\mathbf{X}_{\mathbf{R}}&#124;)}{2^{N-1}-1}$ |  | (3)
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{X}_{\mathbf{Q}}^{i}=\alpha\lfloor\frac{\mathbf{X}_{\mathbf{R}}^{i}}{\alpha}\rceil,\
    \ \ \alpha=\frac{\max(&#124;\mathbf{X}_{\mathbf{R}}&#124;)}{2^{N-1}-1}$ |  | (3)
    |'
- en: 'Here $\mathbf{X}_{\mathbf{Q}}$ denotes the real-valued weights or activations.
    To ensure efficient quantization, we adopt the per-token activation quantization
    and per-channel weight quantization as illustrated in Figure [3](#S2.F3 "Figure
    3 ‣ 2.2.2 Quantization for Large Language Models ‣ 2.2 Quantization-Aware Training
    ‣ 2 Method ‣ LLM-QAT: Data-Free Quantization Aware Training for Large Language
    Models") (a). For a comprehensive evaluation of the different quantizer choices,
    we provide the ablation study in Section [3.3.2](#S3.SS3.SSS2 "3.3.2 Quantization
    Function ‣ 3.3.1 Data Choice ‣ 3.3 Ablation ‣ 3.2 Main Results ‣ 3.1 Experimental
    Settings ‣ 3 Experiments ‣ LLM-QAT: Data-Free Quantization Aware Training for
    Large Language Models").'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '这里$\mathbf{X}_{\mathbf{Q}}$表示实值的权重或激活。为了确保高效的量化，我们采用如图[3](#S2.F3 "Figure 3
    ‣ 2.2.2 Quantization for Large Language Models ‣ 2.2 Quantization-Aware Training
    ‣ 2 Method ‣ LLM-QAT: Data-Free Quantization Aware Training for Large Language
    Models") (a)所示的每个token激活量化和每个通道权重量化。为了全面评估不同量化器的选择，我们在第[3.3.2](#S3.SS3.SSS2 "3.3.2
    Quantization Function ‣ 3.3.1 Data Choice ‣ 3.3 Ablation ‣ 3.2 Main Results ‣
    3.1 Experimental Settings ‣ 3 Experiments ‣ LLM-QAT: Data-Free Quantization Aware
    Training for Large Language Models")节提供了消融研究。'
- en: '![Refer to caption](img/08a792e00380ab2547784de991bcd0cb.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/08a792e00380ab2547784de991bcd0cb.png)'
- en: 'Figure 3: Illustration of (a) per-channel weight quantization and per-token
    activation quantization (b) per-token quantization for KV cache. The KV cache
    is updated by appending the current key and value to it. Thus we adopt per-token
    quantization for both key and value.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图3： (a) 每个通道的权重量化和每个token的激活量化的示意图；(b) KV缓存的每个token量化。KV缓存通过将当前的key和value追加到其中进行更新。因此，我们对key和value都采用每个token量化。
- en: 'Quantization-aware training for key-value cache In addition to weight and activation
    quantization, the key-value cache (KV cache) in large language models (LLMs) also
    consumes a non-negligible amount of memory. However, only a few previous works
    have addressed the KV cache quantization in LLMs, with the methods primarily limited
    to post-training quantization(Sheng et al., [2023](#bib.bib35)). In our study,
    we demonstrate that a similar quantization-aware training approach used for activation
    quantization can be employed to quantize the KV cache. As illustrated in Figure [3](#S2.F3
    "Figure 3 ‣ 2.2.2 Quantization for Large Language Models ‣ 2.2 Quantization-Aware
    Training ‣ 2 Method ‣ LLM-QAT: Data-Free Quantization Aware Training for Large
    Language Models"), we adopt per-token quantization in Eq. [3](#S2.E3 "In 2.2.2
    Quantization for Large Language Models ‣ 2.2 Quantization-Aware Training ‣ 2 Method
    ‣ LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"),
    given that the key and value are generated token by token. During the generation
    process, the current key and value are quantized, and their corresponding scaling
    factor is stored. During the training process for QAT, we apply quantization to
    the entire activation tensors of both the keys and values, as shown in Figure [2](#S2.F2
    "Figure 2 ‣ 2.1 Data-free Distillation ‣ 2 Method ‣ LLM-QAT: Data-Free Quantization
    Aware Training for Large Language Models"). By integrating the quantization function
    into the gradient calculation, we ensure effective training using quantized key-value
    pairs.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '针对键值缓存的量化感知训练 除了权重和激活量化，大型语言模型（LLMs）中的键值缓存（KV缓存）也会消耗相当大的内存。然而，之前的工作仅有少数涉及LLMs中的KV缓存量化，方法主要局限于训练后量化（Sheng
    et al., [2023](#bib.bib35)）。在我们的研究中，我们展示了可以使用类似于激活量化的量化感知训练方法来对KV缓存进行量化。如图[3](#S2.F3
    "Figure 3 ‣ 2.2.2 Quantization for Large Language Models ‣ 2.2 Quantization-Aware
    Training ‣ 2 Method ‣ LLM-QAT: Data-Free Quantization Aware Training for Large
    Language Models")所示，我们在公式[3](#S2.E3 "In 2.2.2 Quantization for Large Language
    Models ‣ 2.2 Quantization-Aware Training ‣ 2 Method ‣ LLM-QAT: Data-Free Quantization
    Aware Training for Large Language Models")中采用了每个令牌量化，因为键和值是逐个令牌生成的。在生成过程中，当前的键和值会被量化，并且它们对应的缩放因子会被存储。在QAT训练过程中，我们对键和值的整个激活张量应用量化，如图[2](#S2.F2
    "Figure 2 ‣ 2.1 Data-free Distillation ‣ 2 Method ‣ LLM-QAT: Data-Free Quantization
    Aware Training for Large Language Models")所示。通过将量化函数集成到梯度计算中，我们确保使用量化的键值对进行有效训练。'
- en: 'Knowledge distillation We use cross-entropy based logits distillation for training
    the quantized student network from the full-precision pre-trained teacher network:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏 我们使用基于交叉熵的对数蒸馏来训练从全精度预训练教师网络生成的量化学生网络：
- en: '|  | $\displaystyle\mathcal{L}_{CE}=-\frac{1}{n}\sum_{c}\sum^{n}_{i=1}p_{c}^{\mathcal{T}}(X_{i})\log(p_{c}^{\mathcal{S}}(X_{i})),$
    |  | (4) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{CE}=-\frac{1}{n}\sum_{c}\sum^{n}_{i=1}p_{c}^{\mathcal{T}}(X_{i})\log(p_{c}^{\mathcal{S}}(X_{i})),$
    |  | (4) |'
- en: Here $i$ are the teacher network and student network, respectively.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里$i$分别为教师网络和学生网络。
- en: 'As discussed in Section [2.1](#S2.SS1 "2.1 Data-free Distillation ‣ 2 Method
    ‣ LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"),
    in the data generation process, it is important to sample the next token from
    distribution rather than always selecting the top-1 candidate. By doing so, the
    next token does not necessarily represent the optimal label for training the student
    model, as the sampling introduces inherent noise. Consequently, we propose to
    utilize the predictions from the pre-trained model as soft labels, which provides
    more informative targets for guiding the training of the student model. We present
    a comprehensive ablation study in Section [3.3.3](#S3.SS3.SSS3 "3.3.3 Knowledge
    Distillation ‣ 3.3.2 Quantization Function ‣ 3.3.1 Data Choice ‣ 3.3 Ablation
    ‣ 3.2 Main Results ‣ 3.1 Experimental Settings ‣ 3 Experiments ‣ LLM-QAT: Data-Free
    Quantization Aware Training for Large Language Models") to delve into the specifics
    of this approach.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[2.1](#S2.SS1 "2.1 Data-free Distillation ‣ 2 Method ‣ LLM-QAT: Data-Free
    Quantization Aware Training for Large Language Models")节中讨论的，在数据生成过程中，重要的是从分布中采样下一个令牌，而不是总是选择前1个候选。这样，下一个令牌不一定代表训练学生模型的最佳标签，因为采样引入了固有的噪声。因此，我们建议利用预训练模型的预测作为软标签，这为指导学生模型的训练提供了更具信息性的目标。我们在第[3.3.3](#S3.SS3.SSS3
    "3.3.3 Knowledge Distillation ‣ 3.3.2 Quantization Function ‣ 3.3.1 Data Choice
    ‣ 3.3 Ablation ‣ 3.2 Main Results ‣ 3.1 Experimental Settings ‣ 3 Experiments
    ‣ LLM-QAT: Data-Free Quantization Aware Training for Large Language Models")节中展示了一项综合的消融研究，以*深入探讨*这种方法的细节。'
- en: 3 Experiments
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: 'Table 1: Zero-shot performance on Common Sense Reasoning tasks.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在常识推理任务上的零样本性能。
- en: '|  |  |  |  | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | ARC-e | ARC-c
    | OBQA | Avg. |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | ARC-e | ARC-c
    | OBQA | 平均值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '|  | Method | #Bits |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 位数 |'
- en: '&#124; Size${}_{\text{ (GB)}}$ &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大小${}_{\text{ (GB)}}$ &#124;'
- en: '| ($\uparrow$) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| ($\uparrow$) |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| 1 | LLaMA-7B | 16-16-16 | 12.6 | 76.8 | 79.3 | 48.6 | 76.1 | 70.0 | 73.0
    | 48.0 | 57.6 | 66.2 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 1 | LLaMA-7B | 16-16-16 | 12.6 | 76.8 | 79.3 | 48.6 | 76.1 | 70.0 | 73.0
    | 48.0 | 57.6 | 66.2 |'
- en: '| \hdashline[0.8pt/1pt] 2 | RTN | 4-8-4 | 3.5 | 51.9 | 56.3 | 40.5 | 35.7 |
    49.9 | 39.3 | 25.3 | 30.8 | 41.2 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 2 | RTN | 4-8-4 | 3.5 | 51.9 | 56.3 | 40.5 | 35.7 |
    49.9 | 39.3 | 25.3 | 30.8 | 41.2 |'
- en: '| 3 | SmoothQuant | 4-8-4 | 3.5 | 54.7 | 55.4 | 41.1 | 38.9 | 51.5 | 43.9 |
    27.7 | 32.0 | 43.2 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 3 | SmoothQuant | 4-8-4 | 3.5 | 54.7 | 55.4 | 41.1 | 38.9 | 51.5 | 43.9 |
    27.7 | 32.0 | 43.2 |'
- en: '| 4 | LLM-QAT | 4-8-4 | 3.5 | 69.5 | 75.4 | 46.6 | 69.2 | 64.6 | 66.0 | 43.8
    | 50.6 | 60.7 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 4 | LLM-QAT | 4-8-4 | 3.5 | 69.5 | 75.4 | 46.6 | 69.2 | 64.6 | 66.0 | 43.8
    | 50.6 | 60.7 |'
- en: '| \hdashline[0.8pt/1pt] 5 | RTN | 4-8-8 | 3.5 | 67.8 | 76.6 | 47.2 | 71.4 |
    67.2 | 67.4 | 45.6 | 51.2 | 61.8 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 5 | RTN | 4-8-8 | 3.5 | 67.8 | 76.6 | 47.2 | 71.4 |
    67.2 | 67.4 | 45.6 | 51.2 | 61.8 |'
- en: '| 6 | SmoothQuant | 4-8-8 | 3.5 | 71.0 | 76.0 | 45.4 | 67.8 | 66.0 | 67.4 |
    42.8 | 47.0 | 60.4 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 6 | SmoothQuant | 4-8-8 | 3.5 | 71.0 | 76.0 | 45.4 | 67.8 | 66.0 | 67.4 |
    42.8 | 47.0 | 60.4 |'
- en: '| 7 | LLM-QAT | 4-8-8 | 3.5 | 74.6 | 77.5 | 48.3 | 73.5 | 67.7 | 70.2 | 45.6
    | 56.2 | 64.2 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 7 | LLM-QAT | 4-8-8 | 3.5 | 74.6 | 77.5 | 48.3 | 73.5 | 67.7 | 70.2 | 45.6
    | 56.2 | 64.2 |'
- en: '| \hdashline[0.8pt/1pt] 8 | RTN | 4-6-16 | 3.5 | 62.4 | 74.5 | 46.8 | 67.9
    | 64.5 | 64.6 | 41.5 | 49.0 | 58.9 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 8 | RTN | 4-6-16 | 3.5 | 62.4 | 74.5 | 46.8 | 67.9
    | 64.5 | 64.6 | 41.5 | 49.0 | 58.9 |'
- en: '| 9 | SmoothQuant | 4-6-16 | 3.5 | 68.8 | 73.9 | 44.5 | 65.7 | 65.3 | 66.0
    | 43.6 | 48.0 | 59.5 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 9 | SmoothQuant | 4-6-16 | 3.5 | 68.8 | 73.9 | 44.5 | 65.7 | 65.3 | 66.0
    | 43.6 | 48.0 | 59.5 |'
- en: '| 10 | LLM-QAT | 4-6-16 | 3.5 | 72.9 | 76.8 | 47.9 | 72.4 | 68.3 | 68.8 | 44.2
    | 53.2 | 63.1 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 10 | LLM-QAT | 4-6-16 | 3.5 | 72.9 | 76.8 | 47.9 | 72.4 | 68.3 | 68.8 | 44.2
    | 53.2 | 63.1 |'
- en: '| \hdashline[0.8pt/1pt] 11 | RTN | 4-8-16 | 3.5 | 67.6 | 77.4 | 47.1 | 71.6
    | 66.9 | 67.1 | 45.8 | 52.0 | 61.9 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 11 | RTN | 4-8-16 | 3.5 | 67.6 | 77.4 | 47.1 | 71.6
    | 66.9 | 67.1 | 45.8 | 52.0 | 61.9 |'
- en: '| 12 | SmoothQuant | 4-8-16 | 3.5 | 70.2 | 76.4 | 44.8 | 68.1 | 66.0 | 67.3
    | 42.9 | 49.0 | 60.6 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 12 | SmoothQuant | 4-8-16 | 3.5 | 70.2 | 76.4 | 44.8 | 68.1 | 66.0 | 67.3
    | 42.9 | 49.0 | 60.6 |'
- en: '| 13 | LLM-QAT | 4-8-16 | 3.5 | 74.8 | 77.8 | 48.6 | 73.6 | 69.0 | 69.7 | 45.8
    | 55.8 | 64.4 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 13 | LLM-QAT | 4-8-16 | 3.5 | 74.8 | 77.8 | 48.6 | 73.6 | 69.0 | 69.7 | 45.8
    | 55.8 | 64.4 |'
- en: '| \hdashline[0.8pt/1pt] 14 | RTN | 4-16-16 | 3.5 | 71.2 | 77.3 | 47.6 | 72.7
    | 66.9 | 68.8 | 46.4 | 52.8 | 63.0 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 14 | RTN | 4-16-16 | 3.5 | 71.2 | 77.3 | 47.6 | 72.7
    | 66.9 | 68.8 | 46.4 | 52.8 | 63.0 |'
- en: '| 15 | GPTQ | 4-16-16 | 3.5 | 67.7 | 76.0 | 46.8 | 69.4 | 66.7 | 66.9 | 43.0
    | 50.6 | 60.9 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 15 | GPTQ | 4-16-16 | 3.5 | 67.7 | 76.0 | 46.8 | 69.4 | 66.7 | 66.9 | 43.0
    | 50.6 | 60.9 |'
- en: '| 16 | LLM-QAT | 4-16-16 | 3.5 | 75.5 | 78.3 | 48.4 | 74.0 | 69.0 | 70.0 |
    45.0 | 55.4 | 64.4 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 16 | LLM-QAT | 4-16-16 | 3.5 | 75.5 | 78.3 | 48.4 | 74.0 | 69.0 | 70.0 |
    45.0 | 55.4 | 64.4 |'
- en: '| \hdashline[0.8pt/1pt] 17 | RTN | 8-8-4 | 6.5 | 54.7 | 59.4 | 43.1 | 45.6
    | 57.4 | 51.2 | 29.6 | 37.8 | 47.4 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 17 | RTN | 8-8-4 | 6.5 | 54.7 | 59.4 | 43.1 | 45.6
    | 57.4 | 51.2 | 29.6 | 37.8 | 47.4 |'
- en: '| 18 | SmoothQuant | 8-8-4 | 6.5 | 60.7 | 67.5 | 44.9 | 58.3 | 58.6 | 57.5
    | 36.9 | 43.6 | 53.5 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 18 | SmoothQuant | 8-8-4 | 6.5 | 60.7 | 67.5 | 44.9 | 58.3 | 58.6 | 57.5
    | 36.9 | 43.6 | 53.5 |'
- en: '| 19 | LLM-QAT | 8-8-4 | 6.5 | 71.1 | 75.6 | 47.3 | 71.8 | 66.3 | 67.1 | 43.6
    | 50.0 | 61.6 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 19 | LLM-QAT | 8-8-4 | 6.5 | 71.1 | 75.6 | 47.3 | 71.8 | 66.3 | 67.1 | 43.6
    | 50.0 | 61.6 |'
- en: '| \hdashline[0.8pt/1pt] 20 | RTN | 8-8-8 | 6.5 | 76.4 | 79.5 | 48.7 | 75.5
    | 69.5 | 72.3 | 46.6 | 56.0 | 65.6 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 20 | RTN | 8-8-8 | 6.5 | 76.4 | 79.5 | 48.7 | 75.5
    | 69.5 | 72.3 | 46.6 | 56.0 | 65.6 |'
- en: '| 21 | SmoothQuant | 8-8-8 | 6.5 | 76.1 | 79.6 | 48.7 | 76.2 | 70.1 | 73.7
    | 48.7 | 57.0 | 66.3 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 21 | SmoothQuant | 8-8-8 | 6.5 | 76.1 | 79.6 | 48.7 | 76.2 | 70.1 | 73.7
    | 48.7 | 57.0 | 66.3 |'
- en: '| 22 | LLM-QAT | 8-8-8 | 6.5 | 76.0 | 79.6 | 48.5 | 75.7 | 69.4 | 73.1 | 48.2
    | 57.4 | 66.0 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 22 | LLM-QAT | 8-8-8 | 6.5 | 76.0 | 79.6 | 48.5 | 75.7 | 69.4 | 73.1 | 48.2
    | 57.4 | 66.0 |'
- en: '| \hdashline[0.8pt/1pt] 23 | RTN | 8-8-16 | 6.5 | 76.4 | 79.1 | 48.3 | 75.7
    | 70.5 | 72.8 | 46.5 | 55.6 | 65.6 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 23 | RTN | 8-8-16 | 6.5 | 76.4 | 79.1 | 48.3 | 75.7
    | 70.5 | 72.8 | 46.5 | 55.6 | 65.6 |'
- en: '| 24 | SmoothQuant | 8-8-16 | 6.5 | 76.2 | 79.5 | 48.6 | 76.1 | 70.5 | 73.2
    | 47.7 | 57.2 | 66.1 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 24 | SmoothQuant | 8-8-16 | 6.5 | 76.2 | 79.5 | 48.6 | 76.1 | 70.5 | 73.2
    | 47.7 | 57.2 | 66.1 |'
- en: '| 25 | LLM-QAT | 8-8-16 | 6.5 | 76.3 | 79.4 | 48.7 | 75.6 | 69.7 | 72.3 | 47.6
    | 56.2 | 65.7 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 25 | LLM-QAT | 8-8-16 | 6.5 | 76.3 | 79.4 | 48.7 | 75.6 | 69.7 | 72.3 | 47.6
    | 56.2 | 65.7 |'
- en: '| 26 | LLaMA-13B | 16-16-16 | 24.2 | 78.1 | 80.0 | 50.5 | 79.2 | 73.6 | 74.5
    | 52.6 | 55.0 | 68.0 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 26 | LLaMA-13B | 16-16-16 | 24.2 | 78.1 | 80.0 | 50.5 | 79.2 | 73.6 | 74.5
    | 52.6 | 55.0 | 68.0 |'
- en: '| \hdashline[0.8pt/1pt] 27 | RTN | 4-8-4 | 6.5 | 54.0 | 59.2 | 41.9 | 41.6
    | 55.9 | 45.0 | 27.0 | 33.2 | 44.7 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 27 | RTN | 4-8-4 | 6.5 | 54.0 | 59.2 | 41.9 | 41.6
    | 55.9 | 45.0 | 27.0 | 33.2 | 44.7 |'
- en: '| 28 | SmoothQuant | 4-8-4 | 6.5 | 63.0 | 65.3 | 42.2 | 50.6 | 54.1 | 49.6
    | 30.3 | 34.2 | 48.7 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 28 | SmoothQuant | 4-8-4 | 6.5 | 63.0 | 65.3 | 42.2 | 50.6 | 54.1 | 49.6
    | 30.3 | 34.2 | 48.7 |'
- en: '| 29 | LLM-QAT | 4-8-4 | 6.5 | 72.0 | 76.8 | 49.2 | 73.6 | 66.5 | 69.3 | 46.9
    | 52.8 | 63.4 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 29 | LLM-QAT | 4-8-4 | 6.5 | 72.0 | 76.8 | 49.2 | 73.6 | 66.5 | 69.3 | 46.9
    | 52.8 | 63.4 |'
- en: '| \hdashline[0.8pt/1pt] 30 | RTN | 4-8-8 | 6.5 | 76.2 | 78.8 | 49.3 | 76.2
    | 69.9 | 72.2 | 50.7 | 56.8 | 66.3 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 30 | RTN | 4-8-8 | 6.5 | 76.2 | 78.8 | 49.3 | 76.2
    | 69.9 | 72.2 | 50.7 | 56.8 | 66.3 |'
- en: '| 31 | SmoothQuant | 4-8-8 | 6.5 | 72.5 | 77.1 | 47.2 | 74.3 | 69.5 | 67.4
    | 43.3 | 53.4 | 63.1 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 31 | SmoothQuant | 4-8-8 | 6.5 | 72.5 | 77.1 | 47.2 | 74.3 | 69.5 | 67.4
    | 43.3 | 53.4 | 63.1 |'
- en: '| 32 | LLM-QAT | 4-8-8 | 6.5 | 77.5 | 79.1 | 48.6 | 77.5 | 70.6 | 73.0 | 51.9
    | 56.2 | 66.8 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 32 | LLM-QAT | 4-8-8 | 6.5 | 77.5 | 79.1 | 48.6 | 77.5 | 70.6 | 73.0 | 51.9
    | 56.2 | 66.8 |'
- en: '| \hdashline[0.8pt/1pt] 33 | RTN | 4-6-16 | 6.5 | 71.8 | 74.1 | 47.7 | 70.2
    | 65.1 | 69.3 | 44.1 | 45.6 | 61.0 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 33 | RTN | 4-6-16 | 6.5 | 71.8 | 74.1 | 47.7 | 70.2
    | 65.1 | 69.3 | 44.1 | 45.6 | 61.0 |'
- en: '| 34 | SmoothQuant | 4-6-16 | 6.5 | 70.6 | 76.3 | 47.9 | 73.1 | 68.5 | 65.9
    | 43.3 | 52.6 | 62.3 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 34 | SmoothQuant | 4-6-16 | 6.5 | 70.6 | 76.3 | 47.9 | 73.1 | 68.5 | 65.9
    | 43.3 | 52.6 | 62.3 |'
- en: '| 35 | LLM-QAT | 4-6-16 | 6.5 | 75.4 | 79.3 | 48.4 | 76.5 | 69.2 | 73.1 | 48.6
    | 53.4 | 65.5 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 35 | LLM-QAT | 4-6-16 | 6.5 | 75.4 | 79.3 | 48.4 | 76.5 | 69.2 | 73.1 | 48.6
    | 53.4 | 65.5 |'
- en: '| \hdashline[0.8pt/1pt] 36 | RTN | 4-8-16 | 6.5 | 76.8 | 79.1 | 49.1 | 76.3
    | 70.5 | 72.6 | 49.8 | 56.6 | 66.4 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 36 | RTN | 4-8-16 | 6.5 | 76.8 | 79.1 | 49.1 | 76.3
    | 70.5 | 72.6 | 49.8 | 56.6 | 66.4 |'
- en: '| 37 | SmoothQuant | 4-8-16 | 6.5 | 72.5 | 77.9 | 47.6 | 74.2 | 69.7 | 68.2
    | 45.0 | 54.2 | 63.7 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 37 | SmoothQuant | 4-8-16 | 6.5 | 72.5 | 77.9 | 47.6 | 74.2 | 69.7 | 68.2
    | 45.0 | 54.2 | 63.7 |'
- en: '| 38 | LLM-QAT | 4-8-16 | 6.5 | 77.7 | 79.3 | 48.4 | 77.5 | 70.6 | 73.5 | 53.0
    | 57.4 | 67.2 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 38 | LLM-QAT | 4-8-16 | 6.5 | 77.7 | 79.3 | 48.4 | 77.5 | 70.6 | 73.5 | 53.0
    | 57.4 | 67.2 |'
- en: '| \hdashline[0.8pt/1pt] 39 | RTN | 4-16-16 | 6.5 | 77.4 | 79.1 | 49.2 | 76.8
    | 70.5 | 72.6 | 51.2 | 54.2 | 66.4 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 39 | RTN | 4-16-16 | 6.5 | 77.4 | 79.1 | 49.2 | 76.8
    | 70.5 | 72.6 | 51.2 | 54.2 | 66.4 |'
- en: '| 40 | GPTQ | 4-16-16 | 6.5 | 78.0 | 79.8 | 49.2 | 77.7 | 72.6 | 73.2 | 50.6
    | 55.4 | 67.1 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 40 | GPTQ | 4-16-16 | 6.5 | 78.0 | 79.8 | 49.2 | 77.7 | 72.6 | 73.2 | 50.6
    | 55.4 | 67.1 |'
- en: '| 41 | LLM-QAT | 4-16-16 | 6.5 | 77.7 | 79.4 | 49.1 | 77.7 | 71.5 | 72.8 |
    52.0 | 53.8 | 66.7 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 41 | LLM-QAT | 4-16-16 | 6.5 | 77.7 | 79.4 | 49.1 | 77.7 | 71.5 | 72.8 |
    52.0 | 53.8 | 66.7 |'
- en: '| \hdashline[0.8pt/1pt] 42 | RTN | 8-8-4 | 12.4 | 65.8 | 66.2 | 43.9 | 56.7
    | 57.3 | 58.2 | 34.5 | 42.6 | 53.2 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 42 | RTN | 8-8-4 | 12.4 | 65.8 | 66.2 | 43.9 | 56.7
    | 57.3 | 58.2 | 34.5 | 42.6 | 53.2 |'
- en: '| 43 | SmoothQuant | 8-8-4 | 12.4 | 66.6 | 71.7 | 44.8 | 61.1 | 61.0 | 63.4
    | 38.3 | 43.6 | 56.3 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 43 | SmoothQuant | 8-8-4 | 12.4 | 66.6 | 71.7 | 44.8 | 61.1 | 61.0 | 63.4
    | 38.3 | 43.6 | 56.3 |'
- en: '| 44 | LLM-QAT | 8-8-4 | 12.4 | 74.9 | 78.3 | 48.0 | 75.7 | 68.9 | 71.9 | 51.1
    | 54.2 | 65.4 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 44 | LLM-QAT | 8-8-4 | 12.4 | 74.9 | 78.3 | 48.0 | 75.7 | 68.9 | 71.9 | 51.1
    | 54.2 | 65.4 |'
- en: '| \hdashline[0.8pt/1pt] 45 | RTN | 8-8-8 | 12.4 | 77.8 | 80.0 | 50.8 | 78.9
    | 72.6 | 74.5 | 52.1 | 55.6 | 67.8 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 45 | RTN | 8-8-8 | 12.4 | 77.8 | 80.0 | 50.8 | 78.9
    | 72.6 | 74.5 | 52.1 | 55.6 | 67.8 |'
- en: '| 46 | SmoothQuant | 8-8-8 | 12.4 | 78.3 | 80.3 | 50.8 | 79.2 | 73.2 | 74.8
    | 52.4 | 55.4 | 68.0 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 46 | SmoothQuant | 8-8-8 | 12.4 | 78.3 | 80.3 | 50.8 | 79.2 | 73.2 | 74.8
    | 52.4 | 55.4 | 68.0 |'
- en: '| 47 | LLM-QAT | 8-8-8 | 12.4 | 78.7 | 80.4 | 50.1 | 79.1 | 73.2 | 74.8 | 51.7
    | 55.4 | 67.9 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 47 | LLM-QAT | 8-8-8 | 12.4 | 78.7 | 80.4 | 50.1 | 79.1 | 73.2 | 74.8 | 51.7
    | 55.4 | 67.9 |'
- en: '| \hdashline[0.8pt/1pt] 48 | RTN | 8-8-16 | 12.4 | 77.8 | 80.1 | 50.6 | 78.9
    | 73.5 | 74.9 | 51.9 | 56.4 | 68.0 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 48 | RTN | 8-8-16 | 12.4 | 77.8 | 80.1 | 50.6 | 78.9
    | 73.5 | 74.9 | 51.9 | 56.4 | 68.0 |'
- en: '| 49 | SmoothQuant | 8-8-16 | 12.4 | 78.7 | 80.0 | 50.6 | 79.1 | 73.4 | 74.8
    | 51.4 | 56.0 | 68.0 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 49 | SmoothQuant | 8-8-16 | 12.4 | 78.7 | 80.0 | 50.6 | 79.1 | 73.4 | 74.8
    | 51.4 | 56.0 | 68.0 |'
- en: '| 50 | LLM-QAT | 8-8-16 | 12.4 | 78.5 | 80.4 | 50.6 | 79.0 | 72.8 | 74.2 |
    52.9 | 55.8 | 68.0 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 50 | LLM-QAT | 8-8-16 | 12.4 | 78.5 | 80.4 | 50.6 | 79.0 | 72.8 | 74.2 |
    52.9 | 55.8 | 68.0 |'
- en: '| 51 | LLaMA-30B | 16-16-16 | 60.6 | 83.2 | 82.1 | 50.4 | 82.9 | 75.6 | 80
    | 58 | 59.3 | 71.4 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 51 | LLaMA-30B | 16-16-16 | 60.6 | 83.2 | 82.1 | 50.4 | 82.9 | 75.6 | 80
    | 58 | 59.3 | 71.4 |'
- en: '| \hdashline[0.8pt/1pt] 52 | RTN | 4-8-4 | 15.7 | 56.9 | 56.2 | 40.2 | 39.6
    | 50.0 | 40.6 | 26.4 | 29.8 | 42.5 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 52 | RTN | 4-8-4 | 15.7 | 56.9 | 56.2 | 40.2 | 39.6
    | 50.0 | 40.6 | 26.4 | 29.8 | 42.5 |'
- en: '| 53 | SmoothQuant | 4-8-4 | 15.7 | 56.6 | 55.0 | 39.9 | 33.8 | 49.9 | 38.8
    | 24.5 | 27.2 | 40.7 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 53 | SmoothQuant | 4-8-4 | 15.7 | 56.6 | 55.0 | 39.9 | 33.8 | 49.9 | 38.8
    | 24.5 | 27.2 | 40.7 |'
- en: '| 54 | LLM-QAT | 4-8-4 | 15.7 | 80.5 | 80.3 | 49.7 | 80.2 | 75.2 | 78.2 | 56.0
    | 59.2 | 69.9 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 54 | LLM-QAT | 4-8-4 | 15.7 | 80.5 | 80.3 | 49.7 | 80.2 | 75.2 | 78.2 | 56.0
    | 59.2'
- en: '| \hdashline[0.8pt/1pt] 55 | RTN | 4-8-8 | 15.7 | 78.8 | 79.9 | 49.0 | 80.2
    | 75.2 | 78.4 | 54.4 | 57.2 | 69.1 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 55 | RTN | 4-8-8 | 15.7 | 78.8 | 79.9 | 49.0 | 80.2
    | 75.2 | 78.4 | 54.4 | 57.2 | 69.1 |'
- en: '| 56 | SmoothQuant | 4-8-8 | 15.7 | 74.9 | 79.5 | 47.1 | 76.9 | 70.6 | 76.5
    | 54.5 | 55.0 | 66.9 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 56 | SmoothQuant | 4-8-8 | 15.7 | 74.9 | 79.5 | 47.1 | 76.9 | 70.6 | 76.5
    | 54.5 | 55.0 | 66.9 |'
- en: '| 57 | LLM-QAT | 4-8-8 | 15.7 | 81.3 | 80.9 | 50.4 | 81.3 | 76.3 | 80.3 | 56.5
    | 57.0 | 70.5 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 57 | LLM-QAT | 4-8-8 | 15.7 | 81.3 | 80.9 | 50.4 | 81.3 | 76.3 | 80.3 | 56.5
    | 57.0 | 70.5 |'
- en: '| \hdashline[0.8pt/1pt] 58 | RTN | 4-6-16 | 15.7 | 64.5 | 57.0 | 42.1 | 48.9
    | 55.4 | 39.3 | 27.0 | 32.2 | 45.8 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 58 | RTN | 4-6-16 | 15.7 | 64.5 | 57.0 | 42.1 | 48.9
    | 55.4 | 39.3 | 27.0 | 32.2 | 45.8 |'
- en: '| 59 | SmoothQuant | 4-6-16 | 15.7 | 75.0 | 77.6 | 46.6 | 73.8 | 69.1 | 74.5
    | 52.9 | 50.6 | 65.0 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 59 | SmoothQuant | 4-6-16 | 15.7 | 75.0 | 77.6 | 46.6 | 73.8 | 69.1 | 74.5
    | 52.9 | 50.6 | 65.0 |'
- en: '| 60 | LLM-QAT | 4-6-16 | 15.7 | 78.8 | 80.3 | 50.3 | 79.9 | 75.1 | 77.0 |
    54.4 | 59.0 | 69.4 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 60 | LLM-QAT | 4-6-16 | 15.7 | 78.8 | 80.3 | 50.3 | 79.9 | 75.1 | 77.0 |
    54.4 | 59.0 | 69.4 |'
- en: '| \hdashline[0.8pt/1pt] 61 | RTN | 4-8-16 | 15.7 | 79.1 | 79.6 | 49.5 | 80.4
    | 74.9 | 78.3 | 53.7 | 57.2 | 69.1 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 61 | RTN | 4-8-16 | 15.7 | 79.1 | 79.6 | 49.5 | 80.4
    | 74.9 | 78.3 | 53.7 | 57.2 | 69.1 |'
- en: '| 62 | SmoothQuant | 4-8-16 | 15.7 | 76.0 | 79.8 | 48.2 | 77.0 | 71.6 | 76.4
    | 55.6 | 54.2 | 67.3 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 62 | SmoothQuant | 4-8-16 | 15.7 | 76.0 | 79.8 | 48.2 | 77.0 | 71.6 | 76.4
    | 55.6 | 54.2 | 67.3 |'
- en: '| 63 | LLM-QAT | 4-8-16 | 15.7 | 80.6 | 80.8 | 50.1 | 81.2 | 75.8 | 79.7 |
    56.3 | 56.3 | 70.1 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 63 | LLM-QAT | 4-8-16 | 15.7 | 80.6 | 80.8 | 50.1 | 81.2 | 75.8 | 79.7 |
    56.3 | 56.3 | 70.1 |'
- en: '| \hdashline[0.8pt/1pt] 64 | RTN | 4-16-16 | 15.7 | 80.8 | 80.1 | 49.8 | 81.6
    | 75.8 | 79.3 | 55.8 | 57.2 | 70.1 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 64 | RTN | 4-16-16 | 15.7 | 80.8 | 80.1 | 49.8 | 81.6
    | 75.8 | 79.3 | 55.8 | 57.2 | 70.1 |'
- en: '| 65 | GPTQ | 4-16-16 | 15.7 | 81.0 | 81.6 | 49.7 | 82.2 | 74.3 | 79.6 | 56.1
    | 58.2 | 70.3 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 65 | GPTQ | 4-16-16 | 15.7 | 81.0 | 81.6 | 49.7 | 82.2 | 74.3 | 79.6 | 56.1
    | 58.2 | 70.3 |'
- en: '| 66 | LLM-QAT | 4-16-16 | 15.7 | 81.8 | 81.0 | 49.7 | 81.8 | 75.1 | 79.4 |
    56.8 | 54.9 | 70.1 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 66 | LLM-QAT | 4-16-16 | 15.7 | 81.8 | 81.0 | 49.7 | 81.8 | 75.1 | 79.4 |
    56.8 | 54.9 | 70.1 |'
- en: '| \hdashline[0.8pt/1pt] 67 | RTN | 8-8-4 | 30.7 | 59.8 | 64.5 | 42.7 | 51.8
    | 55.0 | 52.2 | 33.2 | 38.0 | 49.6 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 67 | RTN | 8-8-4 | 30.7 | 59.8 | 64.5 | 42.7 | 51.8
    | 55.0 | 52.2 | 33.2 | 38.0 | 49.6 |'
- en: '| 68 | SmoothQuant | 8-8-4 | 30.7 | 58.9 | 63.7 | 43.5 | 54.8 | 55.2 | 55.3
    | 33.6 | 40.2 | 50.7 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 68 | SmoothQuant | 8-8-4 | 30.7 | 58.9 | 63.7 | 43.5 | 54.8 | 55.2 | 55.3
    | 33.6 | 40.2 | 50.7 |'
- en: '| 69 | LLM-QAT | 8-8-4 | 30.7 | 81.2 | 81.6 | 50.1 | 81.1 | 73.6 | 78.5 | 55.7
    | 55.7 | 69.7 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 69 | LLM-QAT | 8-8-4 | 30.7 | 81.2 | 81.6 | 50.1 | 81.1 | 73.6 | 78.5 | 55.7
    | 55.7 | 69.7 |'
- en: '| \hdashline[0.8pt/1pt] 70 | RTN | 8-8-8 | 30.7 | 82.2 | 81.2 | 49.4 | 81.9
    | 75.6 | 79.6 | 57.4 | 58.2 | 70.7 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 70 | RTN | 8-8-8 | 30.7 | 82.2 | 81.2 | 49.4 | 81.9
    | 75.6 | 79.6 | 57.4 | 58.2 | 70.7 |'
- en: '| 71 | SmoothQuant | 8-8-8 | 30.7 | 82.5 | 82.3 | 50.2 | 82.8 | 75.9 | 80.3
    | 56.9 | 57.8 | 71.1 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 71 | SmoothQuant | 8-8-8 | 30.7 | 82.5 | 82.3 | 50.2 | 82.8 | 75.9 | 80.3
    | 56.9 | 57.8 | 71.1 |'
- en: '| 72 | LLM-QAT | 8-8-8 | 30.7 | 82.2 | 81.3 | 51.0 | 82.3 | 75.0 | 80.2 | 57.0
    | 57.2 | 70.8 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 72 | LLM-QAT | 8-8-8 | 30.7 | 82.2 | 81.3 | 51.0 | 82.3 | 75.0 | 80.2 | 57.0
    | 57.2 | 70.8 |'
- en: '| \hdashline[0.8pt/1pt] 73 | RTN | 8-8-16 | 30.7 | 82.3 | 81.6 | 50.2 | 81.7
    | 75.9 | 79.7 | 56.7 | 59.0 | 70.9 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 73 | RTN | 8-8-16 | 30.7 | 82.3 | 81.6 | 50.2 | 81.7
    | 75.9 | 79.7 | 56.7 | 59.0 | 70.9 |'
- en: '| 74 | SmoothQuant | 8-8-16 | 30.7 | 82.8 | 81.9 | 50.3 | 82.7 | 76.3 | 80.2
    | 57.7 | 58.4 | 71.3 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 74 | SmoothQuant | 8-8-16 | 30.7 | 82.8 | 81.9 | 50.3 | 82.7 | 76.3 | 80.2
    | 57.7 | 58.4 | 71.3 |'
- en: '| 75 | LLM-QAT | 8-8-16 | 30.7 | 82.4 | 81.4 | 50.3 | 82.5 | 76.0 | 80.0 |
    57.2 | 56.8 | 70.8 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 75 | LLM-QAT | 8-8-16 | 30.7 | 82.4 | 81.4 | 50.3 | 82.5 | 76.0 | 80.0 |
    57.2 | 56.8 | 70.8 |'
- en: We assess the effectiveness of our approach by conducting experiments on LLaMA-7B/13B/30B
    models and presenting results on various tasks. Specifically, we report the zero-shot
    performance on Common Sense Reasoning tasks such as BoolQ (Clark et al., [2019](#bib.bib7)),
    PIQA (Bisk et al., [2020](#bib.bib2)), SIQA (Sap et al., [2019](#bib.bib31)),
    HellaSwag (Zellers et al., [2019](#bib.bib42)), WinoGrande (Sakaguchi et al.,
    [2021](#bib.bib30)), ARC (Clark et al., [2018](#bib.bib8)), and OBQA (Mihaylov
    et al., [2018](#bib.bib26)). We also assess the few-shot performance on TriviaQA (Joshi
    et al., [2017](#bib.bib17)) and MMLU (Hendrycks et al., [2020](#bib.bib14)) datasets,
    along with perplexity scores on WikiText2 (Merity et al., [2016](#bib.bib25))
    and C4 (Raffel et al., [2020](#bib.bib29)) datasets.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在 LLaMA-7B/13B/30B 模型上进行实验，并在各种任务上展示结果来评估我们方法的有效性。具体来说，我们报告了在如 BoolQ (Clark
    et al., [2019](#bib.bib7))、PIQA (Bisk et al., [2020](#bib.bib2))、SIQA (Sap et
    al., [2019](#bib.bib31))、HellaSwag (Zellers et al., [2019](#bib.bib42))、WinoGrande
    (Sakaguchi et al., [2021](#bib.bib30))、ARC (Clark et al., [2018](#bib.bib8)) 和
    OBQA (Mihaylov et al., [2018](#bib.bib26)) 等常识推理任务上的零-shot 性能。我们还评估了 TriviaQA
    (Joshi et al., [2017](#bib.bib17)) 和 MMLU (Hendrycks et al., [2020](#bib.bib14))
    数据集上的 few-shot 性能，以及 WikiText2 (Merity et al., [2016](#bib.bib25)) 和 C4 (Raffel
    et al., [2020](#bib.bib29)) 数据集上的困惑度分数。
- en: 3.1 Experimental Settings
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: In our quantized network training process, we initialize the model with a pre-trained
    model and employ it as the teacher for knowledge distillation. To optimize the
    model, we utilize the AdamW (Loshchilov & Hutter, [2017](#bib.bib24)) optimizer
    with zero weight decay. Each GPU is assigned a batch size of 1, and the learning
    rate is set to 2e-5, following a cosine learning-rate decay strategy. For data
    generation, we utilize the LLaMA-7B model, and the maximum length of generated
    sequences is set to 1024.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的量化网络训练过程中，我们使用预训练模型初始化模型，并将其作为知识蒸馏的教师。为了优化模型，我们采用AdamW (Loshchilov & Hutter,
    [2017](#bib.bib24)) 优化器，并将权重衰减设置为零。每个GPU分配一个批次大小为1，学习率设置为2e-5，遵循余弦学习率衰减策略。数据生成方面，我们使用LLaMA-7B模型，生成序列的最大长度设置为1024。
- en: 3.2 Main Results
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 主要结果
- en: We consider three post-training quantization (PTQ) methods, round-to-nearest
    (RTN), GPT-Q (Frantar et al., [2022](#bib.bib13)) and SmoothQuant (Xiao et al.,
    [2022](#bib.bib37)) as baselines. We compare to them in several different settings,
    where the weights, activations and KV cache values are quantized to different
    levels (denoted as W-A-KV). Different PTQ methods perform well in different settings,
    and we compare our method to the best PTQ result in each setting.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了三种后训练量化（PTQ）方法，分别是最接近值量化（RTN）、GPT-Q (Frantar et al., [2022](#bib.bib13))
    和SmoothQuant (Xiao et al., [2022](#bib.bib37)) 作为基准。我们在几种不同设置下与它们进行比较，其中权重、激活和KV缓存值被量化到不同的水平（表示为W-A-KV）。不同的PTQ方法在不同的设置中表现良好，我们将我们的方法与每个设置中的最佳PTQ结果进行比较。
- en: 'Table [3](#S3 "3 Experiments ‣ LLM-QAT: Data-Free Quantization Aware Training
    for Large Language Models"), table [3.2](#S3.SS2 "3.2 Main Results ‣ 3.1 Experimental
    Settings ‣ 3 Experiments ‣ LLM-QAT: Data-Free Quantization Aware Training for
    Large Language Models") and table 7 (in Appendix) give the comparisons of the
    proposed QAT methods with SOTA PTQ methods for LLMs on Zero-shot tasks on Common
    Sense Reasoning tasks, perplexity evaluation on Wiki2 and C4 and few shot exact
    match on the MMLU and TriviaQA benchmarks respectively. The perplexity evaluations
    verify whether the quantize models are able to preserve the output distribution
    of the model on a diverse sample of its training domains. The zero-shot and few-shot
    evaluations measure if the model’s capabilities on downstream tasks are retained.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [3](#S3 "3 Experiments ‣ LLM-QAT: Data-Free Quantization Aware Training for
    Large Language Models")、表 [3.2](#S3.SS2 "3.2 Main Results ‣ 3.1 Experimental Settings
    ‣ 3 Experiments ‣ LLM-QAT: Data-Free Quantization Aware Training for Large Language
    Models")和表7（附录中）展示了所提出的QAT方法与SOTA PTQ方法在零样本任务中的比较，涉及常识推理任务、Wiki2和C4的困惑度评估以及MMLU和TriviaQA基准的少样本精确匹配。困惑度评估验证了量化模型是否能够保持模型在其训练领域多样样本上的输出分布。零样本和少样本评估则测量了模型在下游任务中的能力是否得到了保留。'
- en: 'The trends in each table are similar. All methods tend to do well in the 8-bit
    setting across all model sizes. This holds even when the KV cache is also quantized
    to 8-bits, together with weights and activations. However, when either of these
    three values are quantized to less than 8-bits, PTQ methods result in accuracy
    loss, whereas LLM-QAT holds up much better. For example in the 8-8-4 setting,
    30B LLM-QAT achieves an average zero-shot accuracy of 69.7, compared to 50.7 with
    SmoothQuant (Table [3](#S3 "3 Experiments ‣ LLM-QAT: Data-Free Quantization Aware
    Training for Large Language Models"), rows 68-69). The difference is smaller in
    the 4-8-8 setting, however LLM-QAT still outperforms the best PTQ method (RTN
    in this case) by 1.4 points (rows 55, 57). In the 4-8-4 setting, where both weights
    and the KV cache are quantized to 4 bits, all PTQ methods produce poor results,
    whereas LLM-QAT achieves 69.9, only trailing the full precision model by 1.5 points
    on average. LLM-QAT also works reasonably well for 6-bit activation quantization.
    While this setting might not be currently practical due to lack of hardware support,
    it’s a promising data point for sub-8-bit computation for LLMs. Unfortunately
    4-bit activation quantization did not work well for the settings that we tried
    (see Section [3.4](#S3.SS4 "3.4 Compatibility with SmoothQuant ‣ 3.3.3 Knowledge
    Distillation ‣ 3.3.2 Quantization Function ‣ 3.3.1 Data Choice ‣ 3.3 Ablation
    ‣ 3.2 Main Results ‣ 3.1 Experimental Settings ‣ 3 Experiments ‣ LLM-QAT: Data-Free
    Quantization Aware Training for Large Language Models")).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '每个表中的趋势是类似的。所有方法在所有模型尺寸的8-bit设置下表现良好。当KV缓存也被量化到8-bit，与权重和激活一起处理时，这种情况仍然适用。然而，当这三种值中的任意一种被量化到小于8-bit时，PTQ方法会导致准确率下降，而LLM-QAT表现则明显更好。例如，在8-8-4设置中，30B
    LLM-QAT的平均零-shot准确率为69.7，相比之下，SmoothQuant的准确率为50.7（表[3](#S3 "3 Experiments ‣ LLM-QAT:
    Data-Free Quantization Aware Training for Large Language Models")，第68-69行）。在4-8-8设置中差异较小，但LLM-QAT仍比最佳PTQ方法（在这种情况下是RTN）高出1.4个百分点（第55、57行）。在4-8-4设置中，其中权重和KV缓存都被量化到4-bit，所有PTQ方法的结果都很差，而LLM-QAT达到了69.9，平均仅比全精度模型低1.5个百分点。LLM-QAT在6-bit激活量化方面也表现得相当好。虽然由于缺乏硬件支持，这种设置可能目前不实际，但它对于LLM的子8-bit计算是一个有前途的数据点。不幸的是，4-bit激活量化在我们尝试的设置中效果不佳（见第[3.4节](#S3.SS4
    "3.4 Compatibility with SmoothQuant ‣ 3.3.3 Knowledge Distillation ‣ 3.3.2 Quantization
    Function ‣ 3.3.1 Data Choice ‣ 3.3 Ablation ‣ 3.2 Main Results ‣ 3.1 Experimental
    Settings ‣ 3 Experiments ‣ LLM-QAT: Data-Free Quantization Aware Training for
    Large Language Models")）。'
- en: One important question for practitioners is whether to use a small model at
    full precision, or a larger quantized model of similar inference cost. While the
    exact trade-offs can vary based on several factors, we can make several recommendations
    based on our results. First, 8-bit quantization should be preferred over smaller
    full precision models, and PTQ methods are sufficient for this case. An 8-8-8
    30B quantized model outperforms a 13B model of similar size, and should have lower
    latency and higher throughput in practice. This also holds for an 8-bit 13B model
    compared with a 16-bit 7B model. Furthermore, 4-bit models quantized using LLM-QAT
    should be preferred over 8-bit models of similar size. For instance a 4-8-4 LLM-QAT
    30B outperforms an 8-bit LLaMA-13B, and a 4-8-8 LLM-QAT 13B is better than an
    8-bit LLaMA-7B. As a result, we recommend 4-bit LLM-QAT models for the best efficiency-accuracy
    tradeoff.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从业者来说，一个重要的问题是使用全精度的小模型还是使用推理成本相似的较大量化模型。虽然具体的权衡可能因多种因素而异，但我们可以根据我们的结果提出几个建议。首先，8-bit量化应该优于较小的全精度模型，对于这种情况，PTQ方法是足够的。8-8-8的30B量化模型优于13B大小相似的模型，并且在实际应用中应该具有更低的延迟和更高的吞吐量。这同样适用于8-bit的13B模型与16-bit的7B模型之间的比较。此外，使用LLM-QAT量化的4-bit模型应该优于大小相似的8-bit模型。例如，4-8-4
    LLM-QAT 30B优于8-bit LLaMA-13B，4-8-8 LLM-QAT 13B也优于8-bit LLaMA-7B。因此，我们推荐使用4-bit
    LLM-QAT模型以获得最佳的效率-准确率权衡。
- en: 'Table 2: Perplexity evaluation results on WikiText Merity et al. ([2016](#bib.bib25))
    and C4 Raffel et al. ([2020](#bib.bib29))'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：WikiText Merity等（[2016](#bib.bib25)）和C4 Raffel等（[2020](#bib.bib29)）的困惑度评估结果
- en: '|  |  |  | Perplexity |  | Perplexity |  | Perplexity |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 困惑度 |  | 困惑度 |  | 困惑度 |'
- en: '|  | #Bits | Method | C4 ($\downarrow$) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | #Bits | 方法 | C4 ($\downarrow$) |'
- en: '| 1 | 16-16-16 | LLaMA-7B | 7.2 | 10.4 | LLaMA-13B | 6.7 | 9.7 | LLaMA-30B
    | 6.0 | 7.0 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 16-16-16 | LLaMA-7B | 7.2 | 10.4 | LLaMA-13B | 6.7 | 9.7 | LLaMA-30B
    | 6.0 | 7.0 |'
- en: '| \hdashline[0.8pt/1pt] 2 | 4-8-4 | RTN | 55.1 | 151.4 | RTN | 25.0 | 103.6
    | RTN | 8.2 | 8.9 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 2 | 4-8-4 | RTN | 55.1 | 151.4 | RTN | 25.0 | 103.6
    | RTN | 8.2 | 8.9 |'
- en: '| 3 | 4-8-4 | SmoothQuant | 81.1 | 163.6 | SmoothQuant | 26.0 | 60.1 | SmoothQuant
    | 10.6 | 12.0 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 4-8-4 | SmoothQuant | 81.1 | 163.6 | SmoothQuant | 26.0 | 60.1 | SmoothQuant
    | 10.6 | 12.0 |'
- en: '| 4 | 4-8-4 | LLM-QAT | 8.6 | 11.6 | LLM-QAT | 7.6 | 10.2 | LLM-QAT | 7.3 |
    7.7 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 4-8-4 | LLM-QAT | 8.6 | 11.6 | LLM-QAT | 7.6 | 10.2 | LLM-QAT | 7.3 |
    7.7 |'
- en: '| \hdashline[0.8pt/1pt] 5 | 4-8-8 | RTN | 8.4 | 13.9 | RTN | 7.3 | 12.5 | RTN
    | 7.4 | 8.2 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 5 | 4-8-8 | RTN | 8.4 | 13.9 | RTN | 7.3 | 12.5 | RTN
    | 7.4 | 8.2 |'
- en: '| 6 | 4-8-8 | SmoothQuant | 9.1 | 13.7 | SmoothQuant | 8.8 | 12.5 | SmoothQuant
    | 8.7 | 9.8 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 4-8-8 | SmoothQuant | 9.1 | 13.7 | SmoothQuant | 8.8 | 12.5 | SmoothQuant
    | 8.7 | 9.8 |'
- en: '| 7 | 4-8-8 | LLM-QAT | 7.5 | 11.2 | LLM-QAT | 6.8 | 10.0 | LLM-QAT | 6.9 |
    7.5 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 4-8-8 | LLM-QAT | 7.5 | 11.2 | LLM-QAT | 6.8 | 10.0 | LLM-QAT | 6.9 |
    7.5 |'
- en: '| \hdashline[0.8pt/1pt] 8 | 4-6-16 | RTN | 10.5 | 20.0 | RTN | 11.3 | 32.7
    | RTN | 11.4 | 15.4 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 8 | 4-6-16 | RTN | 10.5 | 20.0 | RTN | 11.3 | 32.7
    | RTN | 11.4 | 15.4 |'
- en: '| 9 | 4-6-16 | SmoothQuant | 9.9 | 14.7 | SmoothQuant | 9.1 | 13.6 | SmoothQuant
    | 8.7 | 12.5 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 4-6-16 | SmoothQuant | 9.9 | 14.7 | SmoothQuant | 9.1 | 13.6 | SmoothQuant
    | 8.7 | 12.5 |'
- en: '| 10 | 4-6-16 | LLM-QAT | 7.7 | 10.8 | LLM-QAT | 7.1 | 10.5 | LLM-QAT | 7.3
    | 7.9 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 4-6-16 | LLM-QAT | 7.7 | 10.8 | LLM-QAT | 7.1 | 10.5 | LLM-QAT | 7.3
    | 7.9 |'
- en: '| \hdashline[0.8pt/1pt] 11 | 4-8-16 | RTN | 8.6 | 14.0 | RTN | 7.5 | 12.5 |
    RTN | 7.4 | 8.2 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 11 | 4-8-16 | RTN | 8.6 | 14.0 | RTN | 7.5 | 12.5 |
    RTN | 7.4 | 8.2 |'
- en: '| 12 | 4-8-16 | SmoothQuant | 9.1 | 13.7 | SmoothQuant | 8.7 | 12.6 | SmoothQuant
    | 8.7 | 9.8 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 4-8-16 | SmoothQuant | 9.1 | 13.7 | SmoothQuant | 8.7 | 12.6 | SmoothQuant
    | 8.7 | 9.8 |'
- en: '| 13 | 4-8-16 | LLM-QAT | 7.4 | 10.9 | LLM-QAT | 6.8 | 10.0 | LLM-QAT | 6.9
    | 7.5 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 4-8-16 | LLM-QAT | 7.4 | 10.9 | LLM-QAT | 6.8 | 10.0 | LLM-QAT | 6.9
    | 7.5 |'
- en: '| \hdashline[0.8pt/1pt] 14 | 4-16-16 | RTN | 8.5 | 14.4 | RTN | 7.3 | 11.9
    | RTN | 7.0 | 7.7 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 14 | 4-16-16 | RTN | 8.5 | 14.4 | RTN | 7.3 | 11.9
    | RTN | 7.0 | 7.7 |'
- en: '| 15 | 4-16-16 | GPTQ | 8.4 | 17.4 | GPTQ | 6.8 | 10.7 | GPTQ | 6.2 | 7.9 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 4-16-16 | GPTQ | 8.4 | 17.4 | GPTQ | 6.8 | 10.7 | GPTQ | 6.2 | 7.9 |'
- en: '| 16 | 4-16-16 | LLM-QAT | 7.4 | 10.9 | LLM-QAT | 6.5 | 9.6 | LLM-QAT | 6.5
    | 7.3 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 4-16-16 | LLM-QAT | 7.4 | 10.9 | LLM-QAT | 6.5 | 9.6 | LLM-QAT | 6.5
    | 7.3 |'
- en: '| \hdashline[0.8pt/1pt] 17 | 8-8-4 | RTN | 42.1 | 105.1 | RTN | 15.4 | 43.4
    | RTN | 7.0 | 7.8 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 17 | 8-8-4 | RTN | 42.1 | 105.1 | RTN | 15.4 | 43.4
    | RTN | 7.0 | 7.8 |'
- en: '| 18 | 8-8-4 | SmoothQuant | 30.8 | 77.9 | SmoothQuant | 13.9 | 40.9 | SmoothQuant
    | 6.7 | 7.5 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 18 | 8-8-4 | SmoothQuant | 30.8 | 77.9 | SmoothQuant | 13.9 | 40.9 | SmoothQuant
    | 6.7 | 7.5 |'
- en: '| 19 | 8-8-4 | LLM-QAT | 7.6 | 10.2 | LLM-QAT | 7.5 | 11.3 | LLM-QAT | 6.8
    | 7.4 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 19 | 8-8-4 | LLM-QAT | 7.6 | 10.2 | LLM-QAT | 7.5 | 11.3 | LLM-QAT | 6.8
    | 7.4 |'
- en: '| \hdashline[0.8pt/1pt] 20 | 8-8-8 | RTN | 7.1 | 10.7 | RTN | 6.6 | 10.0 |
    RTN | 6.3 | 7.3 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 20 | 8-8-8 | RTN | 7.1 | 10.7 | RTN | 6.6 | 10.0 |
    RTN | 6.3 | 7.3 |'
- en: '| 21 | 8-8-8 | SmoothQuant | 7.0 | 10.5 | SmoothQuant | 6.5 | 9.8 | SmoothQuant
    | 6.1 | 7.1 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 21 | 8-8-8 | SmoothQuant | 7.0 | 10.5 | SmoothQuant | 6.5 | 9.8 | SmoothQuant
    | 6.1 | 7.1 |'
- en: '| 22 | 8-8-8 | LLM-QAT | 7.0 | 10.3 | LLM-QAT | 7.0 | 9.4 | LLM-QAT | 6.3 |
    7.1 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 22 | 8-8-8 | LLM-QAT | 7.0 | 10.3 | LLM-QAT | 7.0 | 9.4 | LLM-QAT | 6.3 |
    7.1 |'
- en: '| \hdashline[0.8pt/1pt] 23 | 8-8-16 | RTN | 7.3 | 10.7 | RTN | 6.8 | 10.1 |
    RTN | 6.3 | 7.3 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 23 | 8-8-16 | RTN | 7.3 | 10.7 | RTN | 6.8 | 10.1 |
    RTN | 6.3 | 7.3 |'
- en: '| 24 | 8-8-16 | SmoothQuant | 7.0 | 10.5 | SmoothQuant | 6.5 | 9.7 | SmoothQuant
    | 6.1 | 7.1 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 24 | 8-8-16 | SmoothQuant | 7.0 | 10.5 | SmoothQuant | 6.5 | 9.7 | SmoothQuant
    | 6.1 | 7.1 |'
- en: '| 25 | 8-8-16 | LLM-QAT | 7.0 | 10.3 | LLM-QAT | 6.5 | 9.5 | LLM-QAT | 6.3
    | 7.1 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 25 | 8-8-16 | LLM-QAT | 7.0 | 10.3 | LLM-QAT | 6.5 | 9.5 | LLM-QAT | 6.3
    | 7.1 |'
- en: 3.3 Ablation
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 消融
- en: 'We conduct the ablation study regarding the data choice, quantization methods,
    and knowledge distillation methods in Section [3.3.1](#S3.SS3.SSS1 "3.3.1 Data
    Choice ‣ 3.3 Ablation ‣ 3.2 Main Results ‣ 3.1 Experimental Settings ‣ 3 Experiments
    ‣ LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"),
    Section [3.3.2](#S3.SS3.SSS2 "3.3.2 Quantization Function ‣ 3.3.1 Data Choice
    ‣ 3.3 Ablation ‣ 3.2 Main Results ‣ 3.1 Experimental Settings ‣ 3 Experiments
    ‣ LLM-QAT: Data-Free Quantization Aware Training for Large Language Models") and
    Section [3.3.3](#S3.SS3.SSS3 "3.3.3 Knowledge Distillation ‣ 3.3.2 Quantization
    Function ‣ 3.3.1 Data Choice ‣ 3.3 Ablation ‣ 3.2 Main Results ‣ 3.1 Experimental
    Settings ‣ 3 Experiments ‣ LLM-QAT: Data-Free Quantization Aware Training for
    Large Language Models"), respectively. We report both the perplexity scores on
    WikiText2 (Merity et al., [2016](#bib.bib25))/C4 (Raffel et al., [2020](#bib.bib29))
    datasets and the performance on zero-shot common sense reasoning tasks.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第[3.3.1](#S3.SS3.SSS1 "3.3.1 数据选择 ‣ 3.3 消融 ‣ 3.2 主要结果 ‣ 3.1 实验设置 ‣ 3 实验 ‣
    LLM-QAT：数据无关的量化感知训练")、第[3.3.2](#S3.SS3.SSS2 "3.3.2 量化函数 ‣ 3.3.1 数据选择 ‣ 3.3 消融
    ‣ 3.2 主要结果 ‣ 3.1 实验设置 ‣ 3 实验 ‣ LLM-QAT：数据无关的量化感知训练")和第[3.3.3](#S3.SS3.SSS3 "3.3.3
    知识蒸馏 ‣ 3.3.2 量化函数 ‣ 3.3.1 数据选择 ‣ 3.3 消融 ‣ 3.2 主要结果 ‣ 3.1 实验设置 ‣ 3 实验 ‣ LLM-QAT：数据无关的量化感知训练")节中进行了数据选择、量化方法和知识蒸馏方法的消融研究。我们报告了WikiText2（Merity
    et al., [2016](#bib.bib25)）/C4（Raffel et al., [2020](#bib.bib29)）数据集上的困惑度评分和零-shot常识推理任务的性能。
- en: 3.3.1 Data Choice
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 数据选择
- en: 'In Table [3.3.1](#S3.SS3.SSS1 "3.3.1 Data Choice ‣ 3.3 Ablation ‣ 3.2 Main
    Results ‣ 3.1 Experimental Settings ‣ 3 Experiments ‣ LLM-QAT: Data-Free Quantization
    Aware Training for Large Language Models"), we observe that WikiText (Merity et
    al., 2016), which is constructed using text extracted from Wikipedia, does not
    encompass all the information utilized during pre-training. Consequently, a model
    fine-tuned solely on WikiText tends to overfit on this specific dataset and struggles
    to generalize well to other datasets. On the other hand, the Crawled Corpus (C4)
    dataset (Raffel et al., 2020) comprises hundreds of gigabytes of clean English
    text collected from the web. Fine-tuning the model on C4 yields reasonable transfer
    accuracy when evaluated on the WikiText dataset. However, it exhibits poor accuracy
    when tasked with zero-shot inference tasks.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[3.3.1](#S3.SS3.SSS1 "3.3.1 数据选择 ‣ 3.3 消融 ‣ 3.2 主要结果 ‣ 3.1 实验设置 ‣ 3 实验 ‣ LLM-QAT：数据无关的量化感知训练")中，我们观察到，使用从维基百科提取的文本构建的WikiText（Merity
    et al., 2016）并未涵盖预训练期间使用的所有信息。因此，仅在WikiText上微调的模型往往会过拟合于该特定数据集，并且难以很好地泛化到其他数据集。另一方面，Crawled
    Corpus（C4）数据集（Raffel et al., 2020）包含了从网络上收集的数百GB的干净英语文本。在C4上微调模型在WikiText数据集上的迁移准确率表现合理。然而，当任务是零-shot推理任务时，其准确率表现较差。
- en: Compared to the existing data, the model fine-tuned on generated data demonstrates
    superior generalizability, particularly in zero-shot tasks. Moreover, the data
    generated through sampling from the distribution exhibits greater diversity compared
    to the data generated without sampling. This enhanced diversity leads to significantly
    improved performance across all tasks.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有数据相比，基于生成数据微调的模型表现出更好的泛化能力，尤其是在零-shot任务中。此外，通过从分布中采样生成的数据比未采样生成的数据展示了更大的多样性。这种增强的多样性导致所有任务的性能显著提高。
- en: 'Table 3: Effects of the finetuning data to the performance in downstream tasks.
    We use 4-bit weight 6-bit activation LLaMA-7B for the experiments. We test three
    strategies for data generation. Generated data${}^{1}$ refers to first 3~5 tokens
    are generated with deterministic selection while the rest are stochastically sampled
    from the distribution.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：微调数据对下游任务性能的影响。我们使用4-bit权重6-bit激活的LLaMA-7B进行实验。我们测试了三种数据生成策略。生成数据${}^{1}$指的是前3~5个标记是通过确定性选择生成的，而其余部分是从分布中随机采样得到的。
- en: '|  |  | C4 | Wiki2 | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | ARC-e |
    ARC-c | OBQA | Avg. |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  |  | C4 | Wiki2 | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | ARC-e |
    ARC-c | OBQA | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '|  | Finetuning Data | ($\downarrow$) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | 微调数据 | （$\downarrow$） |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| 1 | (Pretrained Model) | 7.2 | 10.7 | 76.8 | 79.3 | 48.6 | 76.1 | 70.0 |
    73.0 | 48.0 | 57.6 | 66.2 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 1 | (预训练模型) | 7.2 | 10.7 | 76.8 | 79.3 | 48.6 | 76.1 | 70.0 | 73.0 | 48.0
    | 57.6 | 66.2 |'
- en: '| \hdashline[0.8pt/1pt] 2 | wiki2 | 10.1 | 5.5 | 46.9 | 74.3 | 45.2 | 72.4
    | 65.7 | 67.2 | 45.0 | 47.8 | 58.1 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 2 | wiki2 | 10.1 | 5.5 | 46.9 | 74.3 | 45.2 | 72.4
    | 65.7 | 67.2 | 45.0 | 47.8 | 58.1 |'
- en: '| 3 | wiki103 | 9.6 | 5.2 | 45.9 | 74.4 | 46.4 | 71.4 | 66.1 | 67.5 | 46.3
    | 49.8 | 58.5 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 3 | wiki103 | 9.6 | 5.2 | 45.9 | 74.4 | 46.4 | 71.4 | 66.1 | 67.5 | 46.3
    | 49.8 | 58.5 |'
- en: '| 4 | c4 | 7.8 | 11.3 | 61.7 | 77.7 | 48.8 | 73.2 | 67.2 | 67.8 | 43.6 | 52.2
    | 61.5 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 4 | c4 | 7.8 | 11.3 | 61.7 | 77.7 | 48.8 | 73.2 | 67.2 | 67.8 | 43.6 | 52.2
    | 61.5 |'
- en: '| 5 | Generated data${}^{1}$ | 8.0 | 11.4 | 60.0 | 77.1 | 48.1 | 72.3 | 65.7
    | 67.4 | 44.2 | 49.8 | 60.6 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 生成数据${}^{1}$ | 8.0 | 11.4 | 60.0 | 77.1 | 48.1 | 72.3 | 65.7 | 67.4 |
    44.2 | 49.8 | 60.6 |'
- en: '| 6 | Generated data${}^{2}$ | 7.7 | 11.5 | 70.9 | 76.1 | 47.9 | 72.2 | 66.9
    | 69.3 | 46.4 | 53.6 | 62.9 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 生成数据${}^{2}$ | 7.7 | 11.5 | 70.9 | 76.1 | 47.9 | 72.2 | 66.9 | 69.3 |
    46.4 | 53.6 | 62.9 |'
- en: '| 7 | Generated data${}^{3}$ | 7.7 | 10.8 | 72.9 | 76.8 | 47.9 | 72.4 | 68.3
    | 68.8 | 44.2 | 53.2 | 63.1 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 生成数据${}^{3}$ | 7.7 | 10.8 | 72.9 | 76.8 | 47.9 | 72.4 | 68.3 | 68.8 |
    44.2 | 53.2 | 63.1 |'
- en: 3.3.2 Quantization Function
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 量化函数
- en: 'We compare the no-clipping quantization method with clipping-based methods
    in Table [3.3.2](#S3.SS3.SSS2 "3.3.2 Quantization Function ‣ 3.3.1 Data Choice
    ‣ 3.3 Ablation ‣ 3.2 Main Results ‣ 3.1 Experimental Settings ‣ 3 Experiments
    ‣ LLM-QAT: Data-Free Quantization Aware Training for Large Language Models").
    Following the practice in previous works (Liu et al., [2022b](#bib.bib22), [2023](#bib.bib20)),
    we use StatsQ (Liu et al., [2022a](#bib.bib21)), a statistically-calculated scaling
    factor for clipping-based weight quantization and LSQ (Esser et al., [2019](#bib.bib11)),
    the learnable scaling factor for clipping-based activation quantization. However,
    our findings indicate that these two state-of-the-art clipping-based quantization
    methods do not surpass the performance achieved by the MinMax non-clipping method.
    This observation reinforces the argument that preserving the outliers is critical
    to the performance of large language models.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表格 [3.3.2](#S3.SS3.SSS2 "3.3.2 量化函数 ‣ 3.3.1 数据选择 ‣ 3.3 消融 ‣ 3.2 主要结果 ‣ 3.1
    实验设置 ‣ 3 实验 ‣ LLM-QAT：数据自由量化感知训练用于大型语言模型")中对比了无剪裁量化方法与基于剪裁的方法。借鉴了先前工作的做法（刘等，[2022b](#bib.bib22)，[2023](#bib.bib20)），我们使用了StatsQ（刘等，[2022a](#bib.bib21)），这是一个用于基于剪裁的权重量化的统计计算缩放因子，以及LSQ（Esser等，[2019](#bib.bib11)），这是一个用于基于剪裁的激活量化的可学习缩放因子。然而，我们的发现表明这两种最先进的基于剪裁的量化方法并未超越MinMax无剪裁方法所达到的性能。这一观察结果进一步强化了保留异常值对大型语言模型性能至关重要的观点。
- en: Furthermore, we observe that for LLaMA models, the activations and weights exhibit
    predominantly symmetric distributions, which makes using symmetric quantizers
    the best choice. It is important to note, however, that this conclusion may not
    hold true for other large language models, especially those incorporating GeLU
    layers.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们观察到对于LLaMA模型，激活和权重主要表现出对称分布，这使得使用对称量化器成为最佳选择。然而，需要注意的是，这一结论可能不适用于其他大型语言模型，特别是那些包含GeLU层的模型。
- en: 'Table 4: Ablation study on the effects of the quantization methods on LLaMA-7B
    model. The quantization level is set to 4-bit weight and 8-bit activation.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：量化方法对LLaMA-7B模型影响的消融研究。量化级别设置为4位权重和8位激活。
- en: '|  |  |  | C4 | Wiki2 | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | ARC-e
    | ARC-c | OBQA | Avg. |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | C4 | Wiki2 | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | ARC-e
    | ARC-c | OBQA | 平均 |'
- en: '|  | Weight | Activation | ($\downarrow$) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | 权重 | 激活 | ($\downarrow$) |'
- en: '| 1 | (Pretrained Model) | 7.2 | 10.7 | 76.8 | 79.3 | 48.6 | 76.1 | 70.0 |
    73.0 | 48.0 | 57.6 | 66.2 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 1 | (预训练模型) | 7.2 | 10.7 | 76.8 | 79.3 | 48.6 | 76.1 | 70.0 | 73.0 | 48.0
    | 57.6 | 66.2 |'
- en: '| \hdashline[0.8pt/1pt] 2 | StatsQ | LSQ | 9.0 | 11.9 | 64.9 | 66.8 | 43.6
    | 63.5 | 56.1 | 51.0 | 31.4 | 33.8 | 51.4 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 2 | StatsQ | LSQ | 9.0 | 11.9 | 64.9 | 66.8 | 43.6
    | 63.5 | 56.1 | 51.0 | 31.4 | 33.8 | 51.4 |'
- en: '| 3 | MinMax | LSQ | 9.4 | 12.8 | 63.5 | 62.4 | 42.4 | 61.2 | 52.9 | 45.6 |
    29.6 | 33.8 | 48.9 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 3 | MinMax | LSQ | 9.4 | 12.8 | 63.5 | 62.4 | 42.4 | 61.2 | 52.9 | 45.6 |
    29.6 | 33.8 | 48.9 |'
- en: '| 4 | StatsQ | MinMax | 8.2 | 11.0 | 71.7 | 75.1 | 43.7 | 69.5 | 58.9 | 62.6
    | 35.2 | 37.8 | 56.8 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 4 | StatsQ | MinMax | 8.2 | 11.0 | 71.7 | 75.1 | 43.7 | 69.5 | 58.9 | 62.6
    | 35.2 | 37.8 | 56.8 |'
- en: '| 5 | MinMax | MinMax | 7.4 | 10.9 | 74.8 | 77.8 | 48.6 | 73.6 | 69.0 | 69.7
    | 45.8 | 55.8 | 64.4 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 5 | MinMax | MinMax | 7.4 | 10.9 | 74.8 | 77.8 | 48.6 | 73.6 | 69.0 | 69.7
    | 45.8 | 55.8 | 64.4 |'
- en: '| \hdashline[0.8pt/1pt] 6 | Asym | Asym | 7.3 | 10.4 | 75.0 | 78.4 | 48.0 |
    73.9 | 69.3 | 71.9 | 45.7 | 52.6 | 64.3 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 6 | Asym | Asym | 7.3 | 10.4 | 75.0 | 78.4 | 48.0 |
    73.9 | 69.3 | 71.9 | 45.7 | 52.6 | 64.3 |'
- en: '| 7 | Sym | Asym | 7.4 | 11.0 | 72.7 | 77.9 | 48.8 | 73.3 | 67.9 | 69.2 | 45.2
    | 56.0 | 63.9 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 7 | Sym | Asym | 7.4 | 11.0 | 72.7 | 77.9 | 48.8 | 73.3 | 67.9 | 69.2 | 45.2
    | 56.0 | 63.9 |'
- en: '| 8 | Asym | Sym | 7.4 | 10.9 | 73.3 | 78.4 | 48.0 | 73.9 | 68.9 | 71.4 | 46.4
    | 54.0 | 64.3 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 8 | Asym | Sym | 7.4 | 10.9 | 73.3 | 78.4 | 48.0 | 73.9 | 68.9 | 71.4 | 46.4
    | 54.0 | 64.3 |'
- en: '| 9 | Sym | Sym | 7.4 | 10.9 | 74.8 | 77.8 | 48.6 | 73.6 | 69.0 | 69.7 | 45.8
    | 55.8 | 64.4 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 9 | Sym | Sym | 7.4 | 10.9 | 74.8 | 77.8 | 48.6 | 73.6 | 69.0 | 69.7 | 45.8
    | 55.8 | 64.4 |'
- en: 3.3.3 Knowledge Distillation
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 知识蒸馏
- en: 'Table [3.3.3](#S3.SS3.SSS3 "3.3.3 Knowledge Distillation ‣ 3.3.2 Quantization
    Function ‣ 3.3.1 Data Choice ‣ 3.3 Ablation ‣ 3.2 Main Results ‣ 3.1 Experimental
    Settings ‣ 3 Experiments ‣ LLM-QAT: Data-Free Quantization Aware Training for
    Large Language Models") shows that different knowledge distillation methods have
    a significant impact on the final accuracy of fine-tuned models. Notably, utilizing
    the next token alone as the label is sub-optimal due to the inherent randomness
    and noise introduced by sampling from a distribution of candidates during the
    generation process. In contrast, logit distillation, which utilizes the complete
    logit distribution prediction from the teacher model, leads to superior performance
    of fine-tuned models compared to label-based training approaches. Interestingly,
    we have observed that incorporating attention distillation or hidden layer distillation
    actually hampers the performance. Consequently, we exclusively employ logit distillation
    in all our experiments.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [3.3.3](#S3.SS3.SSS3 "3.3.3 知识蒸馏 ‣ 3.3.2 量化函数 ‣ 3.3.1 数据选择 ‣ 3.3 消融 ‣ 3.2
    主要结果 ‣ 3.1 实验设置 ‣ 3 实验 ‣ LLM-QAT: 针对大型语言模型的数据无关量化感知训练") 显示了不同知识蒸馏方法对微调模型最终准确率的显著影响。值得注意的是，单独利用下一个
    token 作为标签是不理想的，因为在生成过程中从候选分布中采样时会引入固有的随机性和噪声。相比之下，利用教师模型的完整 logit 分布预测的 logit
    蒸馏方法，使得微调模型的表现优于基于标签的训练方法。有趣的是，我们观察到引入注意力蒸馏或隐藏层蒸馏实际上会损害性能。因此，我们在所有实验中仅使用 logit
    蒸馏。'
- en: 'Table 5: Ablation study on the knowledge distillation choices on LLaMA-7B model
    with generated data. The quantization level is set to 4-bit weight and 6-bit activation.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 在 LLaMA-7B 模型上对知识蒸馏选择的消融研究，生成数据的量化级别设置为 4 位权重和 6 位激活。'
- en: '|  |  | C4 | Wiki2 | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | ARC-e |
    ARC-c | OBQA | Avg. |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  |  | C4 | Wiki2 | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | ARC-e |
    ARC-c | OBQA | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '|  | Method | ($\downarrow$) |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | ($\downarrow$) |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| 1 | (Pretrained Model) | 7.2 | 10.4 | 76.8 | 79.3 | 48.6 | 76.1 | 70.0 |
    73.0 | 48.0 | 57.6 | 66.2 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 1 | （预训练模型） | 7.2 | 10.4 | 76.8 | 79.3 | 48.6 | 76.1 | 70.0 | 73.0 | 48.0
    | 57.6 | 66.2 |'
- en: '| \hdashline[0.8pt/1pt] 2 | Label | 8.1 | 11.9 | 69.4 | 77.3 | 48.7 | 72.1
    | 67.1 | 67.6 | 45.4 | 51.4 | 62.4 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 2 | 标签 | 8.1 | 11.9 | 69.4 | 77.3 | 48.7 | 72.1 | 67.1
    | 67.6 | 45.4 | 51.4 | 62.4 |'
- en: '| 3 | Lable + Attention | 8.8 | 18.6 | 70.2 | 75.3 | 47.6 | 68.9 | 67.2 | 65.6
    | 42.6 | 51.2 | 61.1 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 标签 + 注意力 | 8.8 | 18.6 | 70.2 | 75.3 | 47.6 | 68.9 | 67.2 | 65.6 | 42.6
    | 51.2 | 61.1 |'
- en: '| 4 | Lable + Hidden | 10.9 | 16.2 | 61.0 | 53.5 | 41.1 | 32.6 | 50.2 | 25.8
    | 23.1 | 25.0 | 37.7 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 4 | Lable + Hidden | 10.9 | 16.2 | 61.0 | 53.5 | 41.1 | 32.6 | 50.2 | 25.8
    | 23.1 | 25.0 | 37.7 |'
- en: '| 5 | Lable + Logits | 7.8 | 11.0 | 70.8 | 77.3 | 48.3 | 72.5 | 66.7 | 68.2
    | 46.5 | 55.4 | 63.2 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 5 | Lable + Logits | 7.8 | 11.0 | 70.8 | 77.3 | 48.3 | 72.5 | 66.7 | 68.2
    | 46.5 | 55.4 | 63.2 |'
- en: '| 6 | Logits | 7.7 | 10.8 | 72.9 | 76.8 | 47.9 | 72.4 | 68.3 | 68.8 | 44.2
    | 53.2 | 63.1 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 6 | Logits | 7.7 | 10.8 | 72.9 | 76.8 | 47.9 | 72.4 | 68.3 | 68.8 | 44.2
    | 53.2 | 63.1 |'
- en: '| 7 | Logits + Attention | 7.9 | 12.2 | 73.2 | 74.6 | 47.2 | 69.1 | 65.1 |
    64.8 | 42.1 | 52.8 | 61.1 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 7 | Logits + 注意力 | 7.9 | 12.2 | 73.2 | 74.6 | 47.2 | 69.1 | 65.1 | 64.8 |
    42.1 | 52.8 | 61.1 |'
- en: '| 8 | Logits + Hidden | 22.3 | 52.6 | 38.0 | 50.4 | 38.6 | 25.6 | 50.5 | 26.3
    | 24.3 | 25.8 | 34.9 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 8 | Logits + Hidden | 22.3 | 52.6 | 38.0 | 50.4 | 38.6 | 25.6 | 50.5 | 26.3
    | 24.3 | 25.8 | 34.9 |'
- en: '| 9 | Logits + Hidden + Attention | 21.9 | 46.0 | 55.0 | 47.8 | 39.0 | 33.4
    | 48.5 | 29.7 | 26.4 | 25.8 | 38.2 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 9 | Logits + Hidden + 注意力 | 21.9 | 46.0 | 55.0 | 47.8 | 39.0 | 33.4 | 48.5
    | 29.7 | 26.4 | 25.8 | 38.2 |'
- en: 3.4 Compatibility with SmoothQuant
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 与 SmoothQuant 的兼容性
- en: 'Our method is also compatible with the weight activation rescale technique
    proposed in SmoothQuant (Xiao et al., [2022](#bib.bib37)). Table [3.4](#S3.SS4
    "3.4 Compatibility with SmoothQuant ‣ 3.3.3 Knowledge Distillation ‣ 3.3.2 Quantization
    Function ‣ 3.3.1 Data Choice ‣ 3.3 Ablation ‣ 3.2 Main Results ‣ 3.1 Experimental
    Settings ‣ 3 Experiments ‣ LLM-QAT: Data-Free Quantization Aware Training for
    Large Language Models") shows that incorporating SmoothQuant into 4-bit weight
    4-bit activation (W4A4) quantization can further improve accuracy. However, in
    the case where the activation bit is greater than the weight bit (i.e, W4A8),
    adding SmoothQuant does not yield any improvement and may even harm the performance.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的方法也与SmoothQuant（Xiao等人，[2022](#bib.bib37)）提出的权重激活重新缩放技术兼容。表[3.4](#S3.SS4
    "3.4 兼容SmoothQuant ‣ 3.3.3 知识蒸馏 ‣ 3.3.2 量化函数 ‣ 3.3.1 数据选择 ‣ 3.3 消融实验 ‣ 3.2 主要结果
    ‣ 3.1 实验设置 ‣ 3 实验 ‣ LLM-QAT: 无数据量化感知训练用于大型语言模型")显示，将SmoothQuant融入4位权重4位激活（W4A4）量化可以进一步提高准确性。然而，在激活位数大于权重位数的情况下（即W4A8），添加SmoothQuant不会带来任何改善，甚至可能损害性能。'
- en: 'Table 6: Combine LLM-QAT with SmoothQuant for lower quantization bit settings'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：将LLM-QAT与SmoothQuant结合以降低量化位设置
- en: '|  |  |  | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA
    | Avg. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA
    | Avg. |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | Method | #Bits | ($\uparrow$) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | #位数 | ($\uparrow$) |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | LLaMA-7B | 16-16-16 | 76.8 | 79.3 | 48.6 | 76.1 | 70.0 | 73.0 | 48.0
    | 57.6 | 66.2 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 1 | LLaMA-7B | 16-16-16 | 76.8 | 79.3 | 48.6 | 76.1 | 70.0 | 73.0 | 48.0
    | 57.6 | 66.2 |'
- en: '| \hdashline[0.8pt/1pt] 2 | RTN | 4-8-16 | 67.6 | 77.4 | 47.1 | 71.6 | 66.9
    | 67.1 | 45.8 | 52.0 | 61.9 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 2 | RTN | 4-8-16 | 67.6 | 77.4 | 47.1 | 71.6 | 66.9
    | 67.1 | 45.8 | 52.0 | 61.9 |'
- en: '| 3 | SmoothQuant | 4-8-16 | 70.2 | 76.4 | 44.8 | 68.1 | 66.0 | 67.3 | 42.9
    | 49.0 | 60.6 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 3 | SmoothQuant | 4-8-16 | 70.2 | 76.4 | 44.8 | 68.1 | 66.0 | 67.3 | 42.9
    | 49.0 | 60.6 |'
- en: '| 4 | LLM-QAT | 4-8-16 | 74.8 | 77.8 | 48.6 | 73.6 | 69.0 | 69.7 | 45.8 | 55.8
    | 64.4 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 4 | LLM-QAT | 4-8-16 | 74.8 | 77.8 | 48.6 | 73.6 | 69.0 | 69.7 | 45.8 | 55.8
    | 64.4 |'
- en: '| 5 | LLM-QAT + SmoothQuant | 4-8-16 | 74.1 | 77.2 | 47.8 | 71.9 | 67.7 | 69.6
    | 44.8 | 55.4 | 63.6 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 5 | LLM-QAT + SmoothQuant | 4-8-16 | 74.1 | 77.2 | 47.8 | 71.9 | 67.7 | 69.6
    | 44.8 | 55.4 | 63.6 |'
- en: '| \hdashline[0.8pt/1pt] 6 | RTN | 4-4-16 | 51.3 | 49.8 | 36.9 | 26.2 | 47.9
    | 25.7 | 24.5 | 31.2 | 36.7 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 6 | RTN | 4-4-16 | 51.3 | 49.8 | 36.9 | 26.2 | 47.9
    | 25.7 | 24.5 | 31.2 | 36.7 |'
- en: '| 7 | SmoothQuant | 4-4-16 | 54.1 | 62.8 | 41.8 | 41.5 | 52.6 | 50.6 | 32.9
    | 36.4 | 46.6 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 7 | SmoothQuant | 4-4-16 | 54.1 | 62.8 | 41.8 | 41.5 | 52.6 | 50.6 | 32.9
    | 36.4 | 46.6 |'
- en: '| 8 | LLM-QAT | 4-4-16 | 57.9 | 47.5 | 39.9 | 25.8 | 47.6 | 27.2 | 25.8 | 29.4
    | 37.6 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 8 | LLM-QAT | 4-4-16 | 57.9 | 47.5 | 39.9 | 25.8 | 47.6 | 27.2 | 25.8 | 29.4
    | 37.6 |'
- en: '| 9 | LLM-QAT + SmoothQuant | 4-4-16 | 63.5 | 64.3 | 41.8 | 55.6 | 52.9 | 50.3
    | 30.2 | 35.0 | 49.2 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 9 | LLM-QAT + SmoothQuant | 4-4-16 | 63.5 | 64.3 | 41.8 | 55.6 | 52.9 | 50.3
    | 30.2 | 35.0 | 49.2 |'
- en: '| \hdashline[0.8pt/1pt] 10 | RTN | 4-4-4 | 50.2 | 50.5 | 37.1 | 26.0 | 49.6
    | 26.1 | 24.4 | 28.6 | 36.6 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 10 | RTN | 4-4-4 | 50.2 | 50.5 | 37.1 | 26.0 | 49.6
    | 26.1 | 24.4 | 28.6 | 36.6 |'
- en: '| 11 | SmoothQuant | 4-4-4 | 49.1 | 49.8 | 39.1 | 27.4 | 48.0 | 30.4 | 25.8
    | 29.2 | 37.4 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 11 | SmoothQuant | 4-4-4 | 49.1 | 49.8 | 39.1 | 27.4 | 48.0 | 30.4 | 25.8
    | 29.2 | 37.4 |'
- en: '| 12 | LLM-QAT | 4-4-4 | 61.3 | 51.5 | 39.2 | 31.1 | 51.9 | 27.9 | 23.9 | 29.4
    | 39.5 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 12 | LLM-QAT | 4-4-4 | 61.3 | 51.5 | 39.2 | 31.1 | 51.9 | 27.9 | 23.9 | 29.4
    | 39.5 |'
- en: '| 13 | LLM-QAT + SmoothQuant | 4-4-4 | 62.4 | 55.9 | 40.9 | 47.8 | 50.6 | 35.5
    | 26.4 | 34.6 | 44.3 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 13 | LLM-QAT + SmoothQuant | 4-4-4 | 62.4 | 55.9 | 40.9 | 47.8 | 50.6 | 35.5
    | 26.4 | 34.6 | 44.3 |'
- en: 4 Related Works
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: Quantization Neural network quantization is proved to be a valuable tool in
    compressing model size and reducing storage consumption. Classic quantization
    methods, such as MinMax quantization (Jacob et al., [2018](#bib.bib16); Krishnamoorthi,
    [2018](#bib.bib18)), Learned step-size quantization (Esser et al., [2019](#bib.bib11)),
    PACT (Choi et al., [2018](#bib.bib5)), N2UQ (Liu et al., [2022a](#bib.bib21))
    and etc, have primarily been developed for convolutional neural networks. While
    several recent works have explored language model compression, they are mostly
    focused on smaller models (Zafrir et al., [2019](#bib.bib41); Fan et al., [2020](#bib.bib12);
    Shen et al., [2020b](#bib.bib34); Zadeh et al., [2020](#bib.bib40); Bai et al.,
    [2021](#bib.bib1); Qin et al., [2021](#bib.bib28); Liu et al., [2022b](#bib.bib22))
    like BERT (Devlin et al., [2019](#bib.bib10)) or BART (Lewis et al., [2019](#bib.bib19)).
    For large language models (LLMs), the available quantization methods are mostly
    limited to post-training quantization (Xiao et al., [2022](#bib.bib37); Yao et al.,
    [2022](#bib.bib38); Frantar et al., [2022](#bib.bib13)), due to the lack of accessible
    training data or the prohibitive resource requirements for fine-tuning on the
    entire pre-training dataset. To the best of our knowledge, no previous work has
    addressed the specific challenge of quantization-aware training for LLMs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 量化 神经网络量化已被证明是压缩模型大小和减少存储消耗的有价值工具。经典的量化方法，如 MinMax 量化（Jacob 等，[2018](#bib.bib16);
    Krishnamoorthi，[2018](#bib.bib18)），学习步长量化（Esser 等，[2019](#bib.bib11)），PACT（Choi
    等，[2018](#bib.bib5)），N2UQ（Liu 等，[2022a](#bib.bib21)）等，主要是为卷积神经网络开发的。虽然一些近期工作探索了语言模型的压缩，但它们大多集中于较小的模型（Zafrir
    等，[2019](#bib.bib41); Fan 等，[2020](#bib.bib12); Shen 等，[2020b](#bib.bib34); Zadeh
    等，[2020](#bib.bib40); Bai 等，[2021](#bib.bib1); Qin 等，[2021](#bib.bib28); Liu 等，[2022b](#bib.bib22)），如
    BERT（Devlin 等，[2019](#bib.bib10)）或 BART（Lewis 等，[2019](#bib.bib19)）。对于大型语言模型（LLMs），现有的量化方法大多仅限于训练后量化（Xiao
    等，[2022](#bib.bib37); Yao 等，[2022](#bib.bib38); Frantar 等，[2022](#bib.bib13)），因为缺乏可获取的训练数据或对整个预训练数据集进行微调的资源要求过高。据我们所知，尚未有工作解决针对
    LLM 的量化感知训练的特定挑战。
- en: Data generation Data generation for QAT remains a relatively unexplored field
    of research. While there are several works in the vision domain fine-tuning student
    networks (Yin et al., [2020](#bib.bib39); Liu et al., [2022c](#bib.bib23); Cai
    et al., [2020](#bib.bib4)) using data generated by pre-trained teacher models,
    these methods mainly focus on image data. They involve updating the noise input
    based on gradients computed from the label and reconstructing images by accumulating
    these gradients. In contrast, our proposed approach introduces next token data
    generation in the language domain. This method is more natural and proves to be
    effective for fine-tuning quantized language models.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 数据生成 QAT 的数据生成仍然是一个相对未被充分探索的研究领域。虽然在视觉领域已经有几个研究使用由预训练教师模型生成的数据来微调学生网络（Yin 等，[2020](#bib.bib39);
    Liu 等，[2022c](#bib.bib23); Cai 等，[2020](#bib.bib4)），这些方法主要集中在图像数据上。它们涉及基于从标签计算的梯度更新噪声输入，并通过累积这些梯度来重建图像。相比之下，我们提出的方法引入了语言领域的下一个标记数据生成。这种方法更自然，并且被证明对微调量化语言模型是有效的。
- en: 5 Conclusion and Limitations
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与局限性
- en: We proposed data-free quantization-aware training for LLMs and showed accurate,
    4-bit quantization is possible using this technique. Given the generality of the
    training-data-agnostic distillation method, and the growing cost of LLM deployments,
    we expect our method to have wide applicability. For instance, the method could
    also be used for models trained in several stages, e.g. with instruction tuning
    or reinforcement learning (Ouyang et al., [2022](#bib.bib27)). We leave this investigation
    to future work. Since 4-bit quantization does not have hardware support out-of-the-box,
    we haven’t included hardware implementation as part of this work. However, we’re
    working with partners to enable this in the near future. While our method works
    well for 4-bit weights, 4-bit KV cache and 8-bit activations, it isn’t sufficient
    for 4-bit activation quantization. This case needs further research.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了针对LLMs的数据无关量化感知训练，并展示了使用这一技术进行精确的4-bit量化是可能的。鉴于训练数据无关的蒸馏方法的通用性，以及LLM部署成本的不断增长，我们预计我们的方法具有广泛的适用性。例如，该方法还可以用于多阶段训练的模型，例如指令调优或强化学习（Ouyang等人，[2022](#bib.bib27)）。我们将这项调查留待未来工作。由于4-bit量化不具备开箱即用的硬件支持，我们没有将硬件实现纳入此项工作中。然而，我们正在与合作伙伴一起致力于在不久的将来实现这一点。虽然我们的方法适用于4-bit权重、4-bit
    KV缓存和8-bit激活，但对于4-bit激活量化而言还不够。这种情况需要进一步研究。
- en: References
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bai et al. (2021) Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin
    Jiang, Qun Liu, Michael R Lyu, and Irwin King. Binarybert: Pushing the limit of
    bert quantization. In *ACL/IJCNLP (1)*, 2021.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai等人（2021）Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang,
    Qun Liu, Michael R Lyu, 和 Irwin King。Binarybert: 推动BERT量化的极限。在*ACL/IJCNLP (1)*，2021年。'
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pp.  7432–7439,
    2020.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk等人（2020）Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi等。Piqa: 在自然语言中推理物理常识。在*Proceedings
    of the AAAI conference on artificial intelligence*，第34卷，第7432–7439页，2020年。'
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell等。语言模型是少样本学习者。*Advances in neural information processing systems*，33:1877–1901，2020年。
- en: 'Cai et al. (2020) Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W
    Mahoney, and Kurt Keutzer. Zeroq: A novel zero shot quantization framework. In
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pp.  13169–13178, 2020.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai等人（2020）Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney,
    和 Kurt Keutzer。Zeroq: 一种新颖的零-shot量化框架。在*Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*，第13169–13178页，2020年。'
- en: 'Choi et al. (2018) Jungwook Choi, Zhuo Wang, Swagath Venkataramani, et al.
    Pact: Parameterized clipping activation for quantized neural networks. *arXiv
    e-prints*, pp.  arXiv–1805, 2018.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Choi等人（2018）Jungwook Choi, Zhuo Wang, Swagath Venkataramani等。Pact: 参数化裁剪激活用于量化神经网络。*arXiv
    e-prints*，第arXiv–1805页，2018年。'
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *arXiv
    preprint arXiv:2204.02311*, 2022.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery等人（2022）Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann等。Palm: 使用路径扩展语言建模。*arXiv preprint arXiv:2204.02311*，2022年。'
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty
    of natural yes/no questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark等人（2019）Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, 和 Kristina Toutanova。Boolq: 探索自然是/否问题的意外难度。*arXiv preprint arXiv:1905.10044*，2019年。'
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark等人（2018）Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, 和 Oyvind Tafjord。认为你已经解决了问答问题？试试ARC，AI2推理挑战。*arXiv preprint
    arXiv:1803.05457*，2018年。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv preprint
    arXiv:2208.07339*, 2022.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 (2022) Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer。Llm.int8
    (): 大规模变换器的8位矩阵乘法。*arXiv 预印本 arXiv:2208.07339*，2022年。'
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. In *NAACL-HLT (1)*, 2019.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等人 (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova。Bert：用于语言理解的深度双向变换器的预训练。在*NAACL-HLT
    (1)*，2019年。
- en: Esser et al. (2019) Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar
    Appuswamy, and Dharmendra S Modha. Learned step size quantization. In *International
    Conference on Learning Representations*, 2019.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esser 等人 (2019) Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar
    Appuswamy, 和 Dharmendra S Modha。学习步长量化。在*国际学习表示会议*，2019年。
- en: Fan et al. (2020) Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave,
    Rémi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise
    for extreme model compression. *arXiv preprint arXiv:2004.07320*, 2020.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人 (2020) Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi
    Gribonval, Herve Jegou, 和 Armand Joulin。带有量化噪声的训练用于极端模型压缩。*arXiv 预印本 arXiv:2004.07320*，2020年。
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等人 (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh。Gptq：生成预训练变换器的准确后训练量化。*arXiv
    预印本 arXiv:2210.17323*，2022年。
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *arXiv preprint arXiv:2009.03300*, 2020.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人 (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song, 和 Jacob Steinhardt。测量大规模多任务语言理解。*arXiv 预印本 arXiv:2009.03300*，2020年。
- en: Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.
    *arXiv preprint arXiv:2203.15556*, 2022.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann 等人 (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark 等人。训练计算最优的大型语言模型。*arXiv 预印本 arXiv:2203.15556*，2022年。
- en: Jacob et al. (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    pp.  2704–2713, 2018.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob 等人 (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew
    Tang, Andrew Howard, Hartwig Adam, 和 Dmitry Kalenichenko。神经网络的量化和训练用于高效的整数算术推理。在*IEEE计算机视觉与模式识别会议论文集*，第2704–2713页，2018年。
- en: 'Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.
    Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
    *arXiv preprint arXiv:1705.03551*, 2017.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi 等人 (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, 和 Luke Zettlemoyer。Triviaqa：一个大规模远程监督的阅读理解挑战数据集。*arXiv
    预印本 arXiv:1705.03551*，2017年。
- en: 'Krishnamoorthi (2018) Raghuraman Krishnamoorthi. Quantizing deep convolutional
    networks for efficient inference: A whitepaper. *arXiv preprint arXiv:1806.08342*,
    2018.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishnamoorthi (2018) Raghuraman Krishnamoorthi。用于高效推理的深度卷积网络量化：一份白皮书。*arXiv
    预印本 arXiv:1806.08342*，2018年。
- en: 'Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising
    sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. *arXiv preprint arXiv:1910.13461*, 2019.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人 (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
    Mohamed, Omer Levy, Ves Stoyanov, 和 Luke Zettlemoyer。Bart：用于自然语言生成、翻译和理解的去噪序列到序列预训练。*arXiv
    预印本 arXiv:1910.13461*，2019年。
- en: Liu et al. (2023) Shih-Yang Liu, Zechun Liu, and Kwang-Ting Cheng. Oscillation-free
    quantization for low-bit vision transformers. In *ICML*, 2023.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023) Shih-Yang Liu, Zechun Liu, 和 Kwang-Ting Cheng。无振荡量化用于低位视觉变换器。在*ICML*，2023年。
- en: 'Liu et al. (2022a) Zechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P Xing, and
    Zhiqiang Shen. Nonuniform-to-uniform quantization: Towards accurate quantization
    via generalized straight-through estimation. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pp.  4942–4952, 2022a.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2022a) Zechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P Xing, 和
    Zhiqiang Shen. 非均匀到均匀量化: 通过广义直通估计实现准确量化. 在 *IEEE/CVF 计算机视觉与模式识别会议论文集*, pp. 4942–4952,
    2022a.'
- en: 'Liu et al. (2022b) Zechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott Yih,
    Meng Li, Raghuraman Krishnamoorthi, and Yashar Mehdad. Bit: Robustly binarized
    multi-distilled transformer. *arXiv preprint arXiv:2205.13016*, 2022b.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2022b) Zechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott Yih,
    Meng Li, Raghuraman Krishnamoorthi, 和 Yashar Mehdad. Bit: 稳健的二值化多蒸馏变压器. *arXiv
    预印本 arXiv:2205.13016*, 2022b.'
- en: 'Liu et al. (2022c) Zechun Liu, Zhiqiang Shen, Yun Long, Eric Xing, Kwang-Ting
    Cheng, and Chas Leichner. Data-free neural architecture search via recursive label
    calibration. In *Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv,
    Israel, October 23–27, 2022, Proceedings, Part XXIV*, pp.  391–406. Springer,
    2022c.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2022c) Zechun Liu, Zhiqiang Shen, Yun Long, Eric Xing, Kwang-Ting
    Cheng, 和 Chas Leichner. 无数据神经结构搜索通过递归标签校准. 在 *计算机视觉–ECCV 2022: 第17届欧洲会议，特拉维夫，以色列，2022年10月23–27日，论文集，第二十四部分*,
    pp. 391–406. Springer, 2022c.'
- en: Loshchilov & Hutter (2017) Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. *arXiv preprint arXiv:1711.05101*, 2017.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov & Hutter (2017) Ilya Loshchilov 和 Frank Hutter. Decoupled weight
    decay regularization. *arXiv 预印本 arXiv:1711.05101*, 2017.
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher. Pointer sentinel mixture models. *arXiv 预印本 arXiv:1609.07843*, 2016.
- en: Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. *arXiv preprint arXiv:1809.02789*, 2018.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, 和 Ashish Sabharwal.
    一套盔甲能导电吗？一个新的开放书籍问答数据集. *arXiv 预印本 arXiv:1809.02789*, 2018.
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, 等等. 训练语言模型以遵循带有人类反馈的指令. *神经信息处理系统进展*, 35:27730–27744, 2022.
- en: 'Qin et al. (2021) Haotong Qin, Yifu Ding, Mingyuan Zhang, YAN Qinghua, Aishan
    Liu, Qingqing Dang, Ziwei Liu, and Xianglong Liu. Bibert: Accurate fully binarized
    bert. In *International Conference on Learning Representations*, 2021.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin et al. (2021) Haotong Qin, Yifu Ding, Mingyuan Zhang, YAN Qinghua, Aishan
    Liu, Qingqing Dang, Ziwei Liu, 和 Xianglong Liu. Bibert: 准确的完全二值化 BERT. 在 *国际学习表征会议*,
    2021.'
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu. 探索统一的文本到文本变压器在迁移学习中的极限.
    *机器学习研究杂志*, 21(1):5485–5551, 2020.
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    和 Yejin Choi. Winogrande: 一个大规模的对抗性 Winograd 语法挑战. *ACM 通讯*, 64(9):99–106, 2021.'
- en: 'Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and
    Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. *arXiv
    preprint arXiv:1904.09728*, 2019.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, 和
    Yejin Choi. Socialiqa: 关于社会互动的常识推理. *arXiv 预印本 arXiv:1904.09728*, 2019.'
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*, 2022.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, 等等. Bloom: 一个 176b-参数的开放访问多语言模型. *arXiv 预印本 arXiv:2211.05100*,
    2022.'
- en: 'Shen et al. (2020a) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. Q-BERT: hessian based ultra
    low precision quantization of BERT. In *AAAI*, 2020a.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen et al. (2020a) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W. Mahoney, 和 Kurt Keutzer. Q-BERT: 基于 Hessian 的 BERT 超低精度量化。载于
    *AAAI*，2020年。'
- en: 'Shen et al. (2020b) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra
    low precision quantization of bert. In *Proceedings of the AAAI Conference on
    Artificial Intelligence*, volume 34, pp.  8815–8821, 2020b.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen et al. (2020b) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W Mahoney, 和 Kurt Keutzer. Q-bert: 基于 Hessian 的 BERT 超低精度量化。载于
    *AAAI 人工智能会议论文集*，第 34 卷，第 8815–8821 页，2020年。'
- en: Sheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez,
    et al. High-throughput generative inference of large language models with a single
    gpu. *arXiv preprint arXiv:2303.06865*, 2023.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez
    等。单个 GPU 高通量生成推断大型语言模型。*arXiv 预印本 arXiv:2303.06865*，2023年。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar 等。Llama: 开放高效基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023年。'
- en: 'Xiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. *arXiv preprint arXiv:2211.10438*, 2022.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth,
    和 Song Han. Smoothquant: 大型语言模型的准确高效后训练量化。*arXiv 预印本 arXiv:2211.10438*，2022年。'
- en: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. *Advances in Neural Information Processing
    Systems*, 35:27168–27183, 2022.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, 和 Yuxiong He. Zeroquant: 大规模变换器的高效且负担得起的后训练量化。*神经信息处理系统进展*，第
    35 卷，第 27168–27183 页，2022年。'
- en: 'Yin et al. (2020) Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li,
    Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz. Dreaming to distill: Data-free
    knowledge transfer via deepinversion. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pp.  8715–8724, 2020.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2020) Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li,
    Arun Mallya, Derek Hoiem, Niraj K Jha, 和 Jan Kautz. 梦想蒸馏：通过深度反演进行无数据知识转移。载于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，第 8715–8724 页，2020年。
- en: 'Zadeh et al. (2020) Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas
    Moshovos. Gobo: Quantizing attention-based nlp models for low latency and energy
    efficient inference. In *2020 53rd Annual IEEE/ACM International Symposium on
    Microarchitecture (MICRO)*, pp.  811–824\. IEEE, 2020.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zadeh et al. (2020) Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, 和 Andreas
    Moshovos. Gobo: 为低延迟和节能推断量化基于注意力的 NLP 模型。载于 *2020 第 53 届 IEEE/ACM 国际微体系结构年会 (MICRO)*，第
    811–824 页。IEEE，2020年。'
- en: 'Zafrir et al. (2019) Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
    Q8bert: Quantized 8bit bert. In *2019 Fifth Workshop on Energy Efficient Machine
    Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)*, pp.  36–39\. IEEE,
    2019.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zafrir et al. (2019) Ofir Zafrir, Guy Boudoukh, Peter Izsak, 和 Moshe Wasserblat.
    Q8bert: 量化的 8bit BERT。载于 *2019 第五届能源高效机器学习与认知计算研讨会-NeurIPS 版 (EMC2-NIPS)*，第 36–39
    页。IEEE，2019年。'
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    和 Yejin Choi. Hellaswag: 机器真的能完成你的句子吗？*arXiv 预印本 arXiv:1905.07830*，2019年。'
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin
    等。Opt: 开放预训练变换器语言模型。*arXiv 预印本 arXiv:2205.01068*，2022年。'
- en: 'Zhang et al. (2020) Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,
    Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit BERT. In
    Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), *EMNLP*, 2020.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '张等（2020）魏章、卢厚、尹一春、尚丽峰、陈潇、姜鑫和刘群。Ternarybert: Distillation-aware ultra-low bit
    BERT。在 Bonnie Webber、Trevor Cohn、Yulan He 和 Yang Liu（编），*EMNLP*，2020。'
- en: Appendix A Appendix
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Few-shot Evaluation Results
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 少样本评估结果
- en: 'Table [A.1](#A1.SS1 "A.1 Few-shot Evaluation Results ‣ Appendix A Appendix
    ‣ 5 Conclusion and Limitations ‣ 4 Related Works ‣ 3.4 Compatibility with SmoothQuant
    ‣ 3.3.3 Knowledge Distillation ‣ 3.3.2 Quantization Function ‣ 3.3.1 Data Choice
    ‣ 3.3 Ablation ‣ 3.2 Main Results ‣ 3.1 Experimental Settings ‣ 3 Experiments
    ‣ LLM-QAT: Data-Free Quantization Aware Training for Large Language Models") presents
    the few-shot performance of the quantized model on the MMLU Hendrycks et al. ([2020](#bib.bib14))
    and TriviaQA Joshi et al. ([2017](#bib.bib17)) benchmarks.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [A.1](#A1.SS1 "A.1 Few-shot Evaluation Results ‣ Appendix A Appendix ‣ 5
    Conclusion and Limitations ‣ 4 Related Works ‣ 3.4 Compatibility with SmoothQuant
    ‣ 3.3.3 Knowledge Distillation ‣ 3.3.2 Quantization Function ‣ 3.3.1 Data Choice
    ‣ 3.3 Ablation ‣ 3.2 Main Results ‣ 3.1 Experimental Settings ‣ 3 Experiments
    ‣ LLM-QAT: Data-Free Quantization Aware Training for Large Language Models") 展示了量化模型在
    MMLU Hendrycks 等（[2020](#bib.bib14)）和 TriviaQA Joshi 等（[2017](#bib.bib17)）基准测试上的少样本性能。'
- en: 'Table 7: 5-shot few-shot exact match performance on the TriviaQA dataset  and
    5-shot accuracy on Massive Multitask Language Understanding (MMLU) dataset .'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：TriviaQA 数据集上的 5-shot 少样本精确匹配性能和 Massive Multitask Language Understanding
    (MMLU) 数据集上的 5-shot 准确性。
- en: '|  |  |  |  | MMLU |  |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | MMLU |  |'
- en: '|  |  |  |  | Humanities | STEM | Social Sciences | Other | Average | TriviaQA
    |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | 人文学科 | STEM | 社会科学 | 其他 | 平均 | TriviaQA |'
- en: '|  | Method | #Bits |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | #Bits |'
- en: '&#124; Size${}_{\text{ (GB)}}$ &#124;'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Size${}_{\text{ (GB)}}$ &#124;'
- en: '| ($\uparrow$) |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| ($\uparrow$) |'
- en: '| 1 | LLaMA-7B | 16-16-16 | 12.6 | 33.5 | 30.6 | 38.4 | 39.1 | 35.2 | 57.0
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 1 | LLaMA-7B | 16-16-16 | 12.6 | 33.5 | 30.6 | 38.4 | 39.1 | 35.2 | 57.0
    |'
- en: '| \hdashline[0.8pt/1pt] 2 | RTN | 4-8-4 | 3.5 | 23.9 | 26.8 | 26.5 | 24.4 |
    25.2 | 0.3 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 2 | RTN | 4-8-4 | 3.5 | 23.9 | 26.8 | 26.5 | 24.4 |
    25.2 | 0.3 |'
- en: '| 3 | SmoothQuant | 4-8-4 | 3.5 | 24.3 | 27.5 | 26.2 | 24.6 | 25.5 | 3.9 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 3 | SmoothQuant | 4-8-4 | 3.5 | 24.3 | 27.5 | 26.2 | 24.6 | 25.5 | 3.9 |'
- en: '| 4 | LLM-QAT | 4-8-4 | 3.5 | 25.6 | 24.3 | 24.0 | 27.8 | 25.5 | 42.6 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 4 | LLM-QAT | 4-8-4 | 3.5 | 25.6 | 24.3 | 24.0 | 27.8 | 25.5 | 42.6 |'
- en: '| \hdashline[0.8pt/1pt] 5 | RTN | 4-8-8 | 3.5 | 30.1 | 25.6 | 27.5 | 32.5 |
    29.1 | 44.5 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 5 | RTN | 4-8-8 | 3.5 | 30.1 | 25.6 | 27.5 | 32.5 |
    29.1 | 44.5 |'
- en: '| 6 | SmoothQuant | 4-8-8 | 3.5 | 27.1 | 28.9 | 28.0 | 31.9 | 28.7 | 39.6 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 6 | SmoothQuant | 4-8-8 | 3.5 | 27.1 | 28.9 | 28.0 | 31.9 | 28.7 | 39.6 |'
- en: '| 7 | LLM-QAT | 4-8-8 | 3.5 | 30.0 | 27.4 | 28.4 | 34.2 | 30.0 | 50.8 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 7 | LLM-QAT | 4-8-8 | 3.5 | 30.0 | 27.4 | 28.4 | 34.2 | 30.0 | 50.8 |'
- en: '| \hdashline[0.8pt/1pt] 8 | RTN | 4-6-16 | 3.5 | 27.0 | 26.0 | 25.8 | 27.0
    | 26.5 | 36.0 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 8 | RTN | 4-6-16 | 3.5 | 27.0 | 26.0 | 25.8 | 27.0
    | 26.5 | 36.0 |'
- en: '| 9 | SmoothQuant | 4-6-16 | 3.5 | 26.2 | 27.0 | 27.5 | 29.9 | 27.5 | 36.2
    |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 9 | SmoothQuant | 4-6-16 | 3.5 | 26.2 | 27.0 | 27.5 | 29.9 | 27.5 | 36.2
    |'
- en: '| 10 | LLM-QAT | 4-6-16 | 3.5 | 28.9 | 27.3 | 31.6 | 33.0 | 30.0 | 49.0 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 10 | LLM-QAT | 4-6-16 | 3.5 | 28.9 | 27.3 | 31.6 | 33.0 | 30.0 | 49.0 |'
- en: '| \hdashline[0.8pt/1pt] 11 | RTN | 4-8-16 | 3.5 | 30.2 | 25.9 | 26.8 | 32.0
    | 28.9 | 44.9 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 11 | RTN | 4-8-16 | 3.5 | 30.2 | 25.9 | 26.8 | 32.0
    | 28.9 | 44.9 |'
- en: '| 12 | SmoothQuant | 4-8-16 | 3.5 | 26.9 | 28.6 | 29.6 | 32.0 | 29.0 | 40.0
    |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 12 | SmoothQuant | 4-8-16 | 3.5 | 26.9 | 28.6 | 29.6 | 32.0 | 29.0 | 40.0
    |'
- en: '| 13 | LLM-QAT | 4-8-16 | 3.5 | 30.3 | 28.1 | 30.3 | 34.5 | 30.8 | 50.8 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 13 | LLM-QAT | 4-8-16 | 3.5 | 30.3 | 28.1 | 30.3 | 34.5 | 30.8 | 50.8 |'
- en: '| \hdashline[0.8pt/1pt] 14 | RTN | 8-8-4 | 6.5 | 24.2 | 27.3 | 25.8 | 24.5
    | 25.3 | 14.8 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 14 | RTN | 8-8-4 | 6.5 | 24.2 | 27.3 | 25.8 | 24.5
    | 25.3 | 14.8 |'
- en: '| 15 | SmoothQuant | 8-8-4 | 6.5 | 24.4 | 26.4 | 25.6 | 24.2 | 25.1 | 32.8
    |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 15 | SmoothQuant | 8-8-4 | 6.5 | 24.4 | 26.4 | 25.6 | 24.2 | 25.1 | 32.8
    |'
- en: '| 16 | LLM-QAT | 8-8-4 | 6.5 | 28.3 | 25.5 | 28.7 | 30.4 | 28.2 | 46.2 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 16 | LLM-QAT | 8-8-4 | 6.5 | 28.3 | 25.5 | 28.7 | 30.4 | 28.2 | 46.2 |'
- en: '| \hdashline[0.8pt/1pt] 17 | RTN | 8-8-8 | 6.5 | 34.3 | 31.9 | 38.5 | 40.5
    | 36.1 | 56.6 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 17 | RTN | 8-8-8 | 6.5 | 34.3 | 31.9 | 38.5 | 40.5
    | 36.1 | 56.6 |'
- en: '| 18 | SmoothQuant | 8-8-8 | 6.5 | 33.2 | 31.5 | 38.5 | 38.9 | 35.3 | 56.7
    |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 18 | SmoothQuant | 8-8-8 | 6.5 | 33.2 | 31.5 | 38.5 | 38.9 | 35.3 | 56.7
    |'
- en: '| 19 | LLM-QAT | 8-8-8 | 6.5 | 32.9 | 29.7 | 37.9 | 37.9 | 34.4 | 56.1 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 19 | LLM-QAT | 8-8-8 | 6.5 | 32.9 | 29.7 | 37.9 | 37.9 | 34.4 | 56.1 |'
- en: '| \hdashline[0.8pt/1pt] 20 | RTN | 8-8-16 | 6.5 | 34.4 | 31.8 | 39.3 | 39.9
    | 36.1 | 56.6 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 20 | RTN | 8-8-16 | 6.5 | 34.4 | 31.8 | 39.3 | 39.9
    | 36.1 | 56.6 |'
- en: '| 21 | SmoothQuant | 8-8-16 | 6.5 | 33.0 | 30.5 | 38.7 | 38.8 | 35.0 | 56.8
    |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 21 | SmoothQuant | 8-8-16 | 6.5 | 33.0 | 30.5 | 38.7 | 38.8 | 35.0 | 56.8
    |'
- en: '| 22 | LLM-QAT | 8-8-16 | 6.5 | 32.2 | 29.4 | 37.0 | 37.6 | 33.8 | 56.1 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 22 | LLM-QAT | 8-8-16 | 6.5 | 32.2 | 29.4 | 37.0 | 37.6 | 33.8 | 56.1 |'
- en: '| 23 | LLaMA-13B | 16-16-16 | 24.2 | 44.4 | 36.2 | 54.3 | 53.3 | 46.7 | 63.7
    |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 23 | LLaMA-13B | 16-16-16 | 24.2 | 44.4 | 36.2 | 54.3 | 53.3 | 46.7 | 63.7
    |'
- en: '| \hdashline[0.8pt/1pt] 24 | RTN | 4-8-4 | 6.5 | 25.5 | 24.9 | 24.3 | 26.5
    | 25.3 | 22.2 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 24 | RTN | 4-8-4 | 6.5 | 25.5 | 24.9 | 24.3 | 26.5
    | 25.3 | 22.2 |'
- en: '| 25 | SmoothQuant | 4-8-4 | 6.5 | 25.6 | 22.8 | 23.4 | 26.4 | 24.7 | 32.7
    |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 25 | SmoothQuant | 4-8-4 | 6.5 | 25.6 | 22.8 | 23.4 | 26.4 | 24.7 | 32.7
    |'
- en: '| 26 | LLM-QAT | 4-8-4 | 6.5 | 29.4 | 28.5 | 31.9 | 35.8 | 31.1 | 54.3 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 26 | LLM-QAT | 4-8-4 | 6.5 | 29.4 | 28.5 | 31.9 | 35.8 | 31.1 | 54.3 |'
- en: '| \hdashline[0.8pt/1pt] 27 | RTN | 4-8-8 | 6.5 | 38.3 | 32.7 | 45.3 | 46.4
    | 40.4 | 57.9 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 27 | RTN | 4-8-8 | 6.5 | 38.3 | 32.7 | 45.3 | 46.4
    | 40.4 | 57.9 |'
- en: '| 28 | SmoothQuant | 4-8-8 | 6.5 | 30.9 | 28.6 | 33.4 | 37.1 | 32.3 | 46.6
    |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 28 | SmoothQuant | 4-8-8 | 6.5 | 30.9 | 28.6 | 33.4 | 37.1 | 32.3 | 46.6
    |'
- en: '| 29 | LLM-QAT | 4-8-8 | 6.5 | 38.7 | 32.8 | 47.1 | 47.7 | 41.2 | 59.3 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 29 | LLM-QAT | 4-8-8 | 6.5 | 38.7 | 32.8 | 47.1 | 47.7 | 41.2 | 59.3 |'
- en: '| \hdashline[0.8pt/1pt] 30 | RTN | 4-6-16 | 6.5 | 28.5 | 27.8 | 29.5 | 32.0
    | 29.3 | 39.6 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 30 | RTN | 4-6-16 | 6.5 | 28.5 | 27.8 | 29.5 | 32.0
    | 29.3 | 39.6 |'
- en: '| 31 | SmoothQuant | 4-6-16 | 6.5 | 30.3 | 29.6 | 33.5 | 37.1 | 32.4 | 44.8
    |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 31 | SmoothQuant | 4-6-16 | 6.5 | 30.3 | 29.6 | 33.5 | 37.1 | 32.4 | 44.8
    |'
- en: '| 32 | LLM-QAT | 4-6-16 | 6.5 | 37.4 | 33.4 | 45.1 | 46.0 | 40.1 | 57.7 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 32 | LLM-QAT | 4-6-16 | 6.5 | 37.4 | 33.4 | 45.1 | 46.0 | 40.1 | 57.7 |'
- en: '| \hdashline[0.8pt/1pt] 33 | RTN | 4-8-16 | 6.5 | 38.7 | 32.6 | 45.2 | 45.8
    | 40.3 | 57.9 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 33 | RTN | 4-8-16 | 6.5 | 38.7 | 32.6 | 45.2 | 45.8
    | 40.3 | 57.9 |'
- en: '| 34 | SmoothQuant | 4-8-16 | 6.5 | 30.3 | 27.8 | 34.3 | 37.5 | 32.2 | 46.6
    |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 34 | SmoothQuant | 4-8-16 | 6.5 | 30.3 | 27.8 | 34.3 | 37.5 | 32.2 | 46.6
    |'
- en: '| 35 | LLM-QAT | 4-8-16 | 6.5 | 40.1 | 32.4 | 47.6 | 48.0 | 41.8 | 59.8 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 35 | LLM-QAT | 4-8-16 | 6.5 | 40.1 | 32.4 | 47.6 | 48.0 | 41.8 | 59.8 |'
- en: '| \hdashline[0.8pt/1pt] 36 | RTN | 8-8-4 | 12.4 | 27.8 | 26.2 | 27.0 | 29.6
    | 27.6 | 44.3 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 36 | RTN | 8-8-4 | 12.4 | 27.8 | 26.2 | 27.0 | 29.6
    | 27.6 | 44.3 |'
- en: '| 37 | SmoothQuant | 8-8-4 | 12.4 | 27.8 | 28.1 | 28.6 | 32.3 | 29.1 | 49.6
    |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 37 | SmoothQuant | 8-8-4 | 12.4 | 27.8 | 28.1 | 28.6 | 32.3 | 29.1 | 49.6
    |'
- en: '| 38 | LLM-QAT | 8-8-4 | 12.4 | 34.1 | 29.3 | 38.7 | 40.7 | 35.5 | 58.8 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 38 | LLM-QAT | 8-8-4 | 12.4 | 34.1 | 29.3 | 38.7 | 40.7 | 35.5 | 58.8 |'
- en: '| \hdashline[0.8pt/1pt] 39 | RTN | 8-8-8 | 12.4 | 44.2 | 35.6 | 52.2 | 52.5
    | 45.9 | 62.9 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 39 | RTN | 8-8-8 | 12.4 | 44.2 | 35.6 | 52.2 | 52.5
    | 45.9 | 62.9 |'
- en: '| 40 | SmoothQuant | 8-8-8 | 12.4 | 44.5 | 36.1 | 53.5 | 53.3 | 46.6 | 63.4
    |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 40 | SmoothQuant | 8-8-8 | 12.4 | 44.5 | 36.1 | 53.5 | 53.3 | 46.6 | 63.4
    |'
- en: '| 41 | LLM-QAT | 8-8-8 | 12.4 | 43.5 | 36.1 | 52.6 | 52.5 | 45.8 | 63.3 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 41 | LLM-QAT | 8-8-8 | 12.4 | 43.5 | 36.1 | 52.6 | 52.5 | 45.8 | 63.3 |'
- en: '| \hdashline[0.8pt/1pt] 42 | RTN | 8-8-16 | 12.4 | 44.3 | 34.9 | 51.7 | 53.0
    | 45.7 | 63.1 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 42 | RTN | 8-8-16 | 12.4 | 44.3 | 34.9 | 51.7 | 53.0
    | 45.7 | 63.1 |'
- en: '| 43 | SmoothQuant | 8-8-16 | 12.4 | 44.5 | 36.4 | 53.7 | 53.4 | 46.7 | 63.4
    |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 43 | SmoothQuant | 8-8-16 | 12.4 | 44.5 | 36.4 | 53.7 | 53.4 | 46.7 | 63.4
    |'
- en: '| 44 | LLM-QAT | 8-8-16 | 12.4 | 43.6 | 36.1 | 53.8 | 53.2 | 46.3 | 63.4 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 44 | LLM-QAT | 8-8-16 | 12.4 | 43.6 | 36.1 | 53.8 | 53.2 | 46.3 | 63.4 |'
- en: '| 23 | LLaMA-30B | 16-16-16 | 60.6 | 55.8 | 46.0 | 66.7 | 63.4 | 57.8 | 69.9
    |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 23 | LLaMA-30B | 16-16-16 | 60.6 | 55.8 | 46.0 | 66.7 | 63.4 | 57.8 | 69.9
    |'
- en: '| \hdashline[0.8pt/1pt] 46 | RTN | 4-8-4 | 15.7 | 24.4 | 26.2 | 27.2 | 26.4
    | 25.9 | 19.2 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 46 | RTN | 4-8-4 | 15.7 | 24.4 | 26.2 | 27.2 | 26.4
    | 25.9 | 19.2 |'
- en: '| 47 | SmoothQuant | 4-8-4 | 15.7 | 23.9 | 27.5 | 23.2 | 24.1 | 24.6 | 7.5
    |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 47 | SmoothQuant | 4-8-4 | 15.7 | 23.9 | 27.5 | 23.2 | 24.1 | 24.6 | 7.5
    |'
- en: '| 48 | LLM-QAT | 4-8-4 | 15.7 | 47.6 | 40.4 | 55.9 | 54.5 | 49.3 | 63.5 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 48 | LLM-QAT | 4-8-4 | 15.7 | 47.6 | 40.4 | 55.9 | 54.5 | 49.3 | 63.5 |'
- en: '| \hdashline[0.8pt/1pt] 49 | RTN | 4-8-8 | 15.7 | 51.0 | 43.6 | 62.2 | 60.6
    | 53.9 | 66.8 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 49 | RTN | 4-8-8 | 15.7 | 51.0 | 43.6 | 62.2 | 60.6
    | 53.9 | 66.8 |'
- en: '| 50 | SmoothQuant | 4-8-8 | 15.7 | 35.2 | 35.1 | 46.9 | 45.2 | 40.0 | 57.9
    |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 50 | SmoothQuant | 4-8-8 | 15.7 | 35.2 | 35.1 | 46.9 | 45.2 | 40.0 | 57.9
    |'
- en: '| 51 | LLM-QAT | 4-8-8 | 15.7 | 52.2 | 44.3 | 61.4 | 61.0 | 54.4 | 65.9 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 51 | LLM-QAT | 4-8-8 | 15.7 | 52.2 | 44.3 | 61.4 | 61.0 | 54.4 | 65.9 |'
- en: '| \hdashline[0.8pt/1pt] 52 | RTN | 4-6-16 | 15.7 | 29.5 | 31.3 | 32.1 | 36.2
    | 32.0 | 39.3 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 52 | RTN | 4-6-16 | 15.7 | 29.5 | 31.3 | 32.1 | 36.2
    | 32.0 | 39.3 |'
- en: '| 53 | SmoothQuant | 4-6-16 | 15.7 | 31.6 | 34.3 | 43.4 | 42.3 | 37.2 | 56.7
    |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 53 | SmoothQuant | 4-6-16 | 15.7 | 31.6 | 34.3 | 43.4 | 42.3 | 37.2 | 56.7
    |'
- en: '| 54 | LLM-QAT | 4-6-16 | 15.7 | 47.7 | 41.7 | 58.9 | 57.5 | 51.0 | 64.2 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 54 | LLM-QAT | 4-6-16 | 15.7 | 47.7 | 41.7 | 58.9 | 57.5 | 51.0 | 64.2 |'
- en: '| \hdashline[0.8pt/1pt] 55 | RTN | 4-8-16 | 15.7 | 50.9 | 44.0 | 62.8 | 61.3
    | 54.2 | 67.1 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 55 | RTN | 4-8-16 | 15.7 | 50.9 | 44.0 | 62.8 | 61.3
    | 54.2 | 67.1 |'
- en: '| 56 | SmoothQuant | 4-8-16 | 15.7 | 35.6 | 36.2 | 48.6 | 45.7 | 40.8 | 58.5
    |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 56 | SmoothQuant | 4-8-16 | 15.7 | 35.6 | 36.2 | 48.6 | 45.7 | 40.8 | 58.5
    |'
- en: '| 57 | LLM-QAT | 4-8-16 | 15.7 | 52.8 | 44.4 | 63.6 | 61.2 | 55.1 | 67.1 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 57 | LLM-QAT | 4-8-16 | 15.7 | 52.8 | 44.4 | 63.6 | 61.2 | 55.1 | 67.1 |'
- en: '| \hdashline[0.8pt/1pt] 58 | RTN | 8-8-4 | 30.7 | 26.1 | 27.6 | 28.6 | 29.0
    | 27.6 | 30.2 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 58 | RTN | 8-8-4 | 30.7 | 26.1 | 27.6 | 28.6 | 29.0
    | 27.6 | 30.2 |'
- en: '| 59 | SmoothQuant | 8-8-4 | 30.7 | 27.9 | 29.1 | 31.7 | 33.1 | 30.1 | 38.9
    |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 59 | SmoothQuant | 8-8-4 | 30.7 | 27.9 | 29.1 | 31.7 | 33.1 | 30.1 | 38.9
    |'
- en: '| 60 | LLM-QAT | 8-8-4 | 30.7 | 49.7 | 42.2 | 60.8 | 59.7 | 52.7 | 67.9 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 60 | LLM-QAT | 8-8-4 | 30.7 | 49.7 | 42.2 | 60.8 | 59.7 | 52.7 | 67.9 |'
- en: '| \hdashline[0.8pt/1pt] 61 | RTN | 8-8-8 | 30.7 | 55.6 | 45.8 | 66.3 | 63.4
    | 57.5 | 70.4 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 61 | RTN | 8-8-8 | 30.7 | 55.6 | 45.8 | 66.3 | 63.4
    | 57.5 | 70.4 |'
- en: '| 62 | SmoothQuant | 8-8-8 | 30.7 | 56.0 | 46.0 | 67.3 | 64.1 | 58.0 | 70.2
    |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 62 | SmoothQuant | 8-8-8 | 30.7 | 56.0 | 46.0 | 67.3 | 64.1 | 58.0 | 70.2
    |'
- en: '| 63 | LLM-QAT | 8-8-8 | 30.7 | 56.5 | 47.7 | 66.9 | 64.2 | 58.5 | 69.4 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 63 | LLM-QAT | 8-8-8 | 30.7 | 56.5 | 47.7 | 66.9 | 64.2 | 58.5 | 69.4 |'
- en: '| \hdashline[0.8pt/1pt] 64 | RTN | 8-8-16 | 30.7 | 56.3 | 45.6 | 66.8 | 63.7
    | 57.8 | 70.3 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] 64 | RTN | 8-8-16 | 30.7 | 56.3 | 45.6 | 66.8 | 63.7
    | 57.8 | 70.3 |'
- en: '| 65 | SmoothQuant | 8-8-16 | 30.7 | 56.0 | 46.7 | 67.5 | 63.8 | 58.2 | 70.3
    |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 65 | SmoothQuant | 8-8-16 | 30.7 | 56.0 | 46.7 | 67.5 | 63.8 | 58.2 | 70.3
    |'
- en: '| 66 | LLM-QAT | 8-8-16 | 30.7 | 54.9 | 45.9 | 66.7 | 63.6 | 57.4 | 70.0 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 66 | LLM-QAT | 8-8-16 | 30.7 | 54.9 | 45.9 | 66.7 | 63.6 | 57.4 | 70.0 |'
- en: A.2 Memory consumption of KV cache
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 KV 缓存的内存消耗
- en: 'We compute the memory required to store the key-value cache (KV cache) in large
    language models for various sequence lengths, as shown in Table [8](#A1.T8 "Table
    8 ‣ A.2 Memory consumption of KV cache ‣ A.1 Few-shot Evaluation Results ‣ Appendix
    A Appendix ‣ 5 Conclusion and Limitations ‣ 4 Related Works ‣ 3.4 Compatibility
    with SmoothQuant ‣ 3.3.3 Knowledge Distillation ‣ 3.3.2 Quantization Function
    ‣ 3.3.1 Data Choice ‣ 3.3 Ablation ‣ 3.2 Main Results ‣ 3.1 Experimental Settings
    ‣ 3 Experiments ‣ LLM-QAT: Data-Free Quantization Aware Training for Large Language
    Models"). It is evident that the size of the KV cache can quickly exceed the model
    size when dealing with longer sequences. Given the increasing use of long inputs
    and contexts in various applications, it becomes crucial to compress the KV cache.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了大语言模型中存储键值缓存（KV 缓存）所需的内存，具体取决于各种序列长度，如表 [8](#A1.T8 "表 8 ‣ A.2 KV 缓存的内存消耗
    ‣ A.1 少样本评估结果 ‣ 附录 A 附录 ‣ 5 结论与局限性 ‣ 4 相关工作 ‣ 3.4 与 SmoothQuant 的兼容性 ‣ 3.3.3 知识蒸馏
    ‣ 3.3.2 量化函数 ‣ 3.3.1 数据选择 ‣ 3.3 消融实验 ‣ 3.2 主要结果 ‣ 3.1 实验设置 ‣ 3 实验 ‣ LLM-QAT：面向大语言模型的无数据量化感知训练")
    所示。显然，当处理较长的序列时，KV 缓存的大小可能会迅速超过模型的大小。鉴于在各种应用中对长输入和上下文的需求不断增加，因此压缩 KV 缓存变得至关重要。
- en: 'Table 8: The memory consumption of key-value cache (KV cache) for different
    sequence length.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：不同序列长度的键值缓存（KV 缓存）的内存消耗。
- en: '|  | LLaMA-7B | LLaMA-13B | LLaMA-30B |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA-7B | LLaMA-13B | LLaMA-30B |'
- en: '| --- | --- | --- | --- |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Text Length | 16-bit | 8-bit | 4-bit | 16-bit | 8-bit | 4-bit | 16-bit |
    8-bit | 4-bit |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 文本长度 | 16-bit | 8-bit | 4-bit | 16-bit | 8-bit | 4-bit | 16-bit | 8-bit |
    4-bit |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1k | 0.25 GB | 0.13 GB | 0.06 GB | 0.39 GB | 0.20 GB | 0.10 GB | 0.76 GB
    | 0.38 GB | 0.19 GB |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 1k | 0.25 GB | 0.13 GB | 0.06 GB | 0.39 GB | 0.20 GB | 0.10 GB | 0.76 GB
    | 0.38 GB | 0.19 GB |'
- en: '| 2k | 0.50 GB | 0.25 GB | 0.13 GB | 0.78 GB | 0.39 GB | 0.20 GB | 1.52 GB
    | 0.76 GB | 0.38 GB |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 2k | 0.50 GB | 0.25 GB | 0.13 GB | 0.78 GB | 0.39 GB | 0.20 GB | 1.52 GB
    | 0.76 GB | 0.38 GB |'
- en: '| 4k | 1.00 GB | 0.50 GB | 0.25 GB | 1.56 GB | 0.78 GB | 0.39 GB | 3.05 GB
    | 1.52 GB | 0.76 GB |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 4k | 1.00 GB | 0.50 GB | 0.25 GB | 1.56 GB | 0.78 GB | 0.39 GB | 3.05 GB
    | 1.52 GB | 0.76 GB |'
- en: '| 8k | 2.00 GB | 1.00 GB | 0.50 GB | 3.13 GB | 1.56 GB | 0.78 GB | 6.09 GB
    | 3.05 GB | 1.52 GB |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 8k | 2.00 GB | 1.00 GB | 0.50 GB | 3.13 GB | 1.56 GB | 0.78 GB | 6.09 GB
    | 3.05 GB | 1.52 GB |'
- en: '| 16k | 4.00 GB | 2.00 GB | 1.00 GB | 6.25 GB | 3.13 GB | 1.56 GB | 12.19 GB
    | 6.09 GB | 3.05 GB |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 16k | 4.00 GB | 2.00 GB | 1.00 GB | 6.25 GB | 3.13 GB | 1.56 GB | 12.19 GB
    | 6.09 GB | 3.05 GB |'
- en: '| 32k | 8.00 GB | 4.00 GB | 2.00 GB | 12.50 GB | 6.25 GB | 3.13 GB | 24.38
    GB | 12.19 GB | 6.09 GB |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 32k | 8.00 GB | 4.00 GB | 2.00 GB | 12.50 GB | 6.25 GB | 3.13 GB | 24.38
    GB | 12.19 GB | 6.09 GB |'
