- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:05:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 19:05:41'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs
    for Information Retrieval'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 中间蒸馏：基于黑箱LLM的信息检索数据高效蒸馏
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.12169](https://ar5iv.labs.arxiv.org/html/2406.12169)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.12169](https://ar5iv.labs.arxiv.org/html/2406.12169)
- en: Zizhong Li  Haopeng Zhang  Jiawei Zhang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zizhong Li  Haopeng Zhang  Jiawei Zhang
- en: IFM Lab, University of California, Davis
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: IFM Lab，加州大学戴维斯分校
- en: '{zzoli, hapzhang, jiwzhang}@ucdavis.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{zzoli, hapzhang, jiwzhang}@ucdavis.edu'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent research has explored distilling knowledge from large language models
    (LLMs) to optimize retriever models, especially within the retrieval-augmented
    generation (RAG) framework. However, most existing training methods rely on extracting
    supervision signals from LLMs’ weights or their output probabilities, which is
    not only resource-intensive but also incompatible with black-box LLMs. In this
    paper, we introduce Intermediate Distillation, a data-efficient knowledge distillation
    training scheme that treats LLMs as black boxes and distills their knowledge via
    an innovative LLM-ranker-retriever pipeline, solely using LLMs’ ranking generation
    as the supervision signal. Extensive experiments demonstrate that our proposed
    method can significantly improve the performance of retriever models with only
    1,000 training instances. Moreover, our distilled retriever model significantly
    boosts performance in question-answering tasks within the RAG framework, demonstrating
    the potential of LLMs to economically and effectively train smaller models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究探讨了从大型语言模型（LLMs）中提取知识以优化检索模型，特别是在检索增强生成（RAG）框架内。然而，大多数现有的训练方法依赖于从 LLMs
    的权重或其输出概率中提取监督信号，这不仅资源密集，而且与黑箱 LLMs 不兼容。在本文中，我们引入了中间蒸馏，这是一种数据高效的知识蒸馏训练方案，将 LLMs
    视为黑箱，通过创新的 LLM-排名-检索管道来提取其知识，完全依赖于 LLMs 的排名生成作为监督信号。大量实验表明，我们提出的方法可以显著提高检索模型的性能，仅用
    1,000 个训练实例。此外，我们的蒸馏检索模型在 RAG 框架内的问答任务中显著提升了性能，展示了 LLMs 以经济有效的方式训练更小模型的潜力。
- en: 'Intermediate Distillation: Data-Efficient Distillation'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 中间蒸馏：数据高效蒸馏
- en: from Black-Box LLMs for Information Retrieval
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于黑箱LLM的信息检索
- en: Zizhong Li  Haopeng Zhang  Jiawei Zhang IFM Lab, University of California, Davis
    {zzoli, hapzhang, jiwzhang}@ucdavis.edu
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Zizhong Li  Haopeng Zhang  Jiawei Zhang IFM Lab，加州大学戴维斯分校 {zzoli, hapzhang,
    jiwzhang}@ucdavis.edu
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The rapid growth and superior performance of large language models (LLMs) Ouyang
    et al. ([2022](#bib.bib25)); OpenAI ([2023](#bib.bib24)); Wang et al. ([2024b](#bib.bib35))
    have made them a preferred choice for a wide range of NLP applications Xi et al.
    ([2023](#bib.bib37)); Wang et al. ([2024b](#bib.bib35)); Wu et al. ([2023](#bib.bib36));
    Zhang et al. ([2023a](#bib.bib40), [b](#bib.bib41)). LLMs have demonstrated robust
    zero-shot ranking abilities in English and various low-resource languages Adeyemi
    et al. ([2023](#bib.bib1)); Sun et al. ([2023](#bib.bib31)). Consequently, researchers
    have applied LLMs to the task of information retrieval, where they outperform
    previous text search and similarity measurement methods Ma et al. ([2023](#bib.bib19));
    Xu et al. ([2024](#bib.bib39)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）如 Ouyang 等（[2022](#bib.bib25)）、OpenAI（[2023](#bib.bib24)）、Wang 等（[2024b](#bib.bib35)）的快速增长和卓越性能使其成为广泛自然语言处理（NLP）应用的首选
    Xi 等（[2023](#bib.bib37)）；Wang 等（[2024b](#bib.bib35)）；Wu 等（[2023](#bib.bib36)）；Zhang
    等（[2023a](#bib.bib40)、[b](#bib.bib41)）。LLMs 在英语和各种低资源语言中展现了强大的零-shot 排名能力 Adeyemi
    等（[2023](#bib.bib1)）；Sun 等（[2023](#bib.bib31)）。因此，研究人员将 LLMs 应用于信息检索任务，在该任务中，它们超越了以前的文本搜索和相似性测量方法
    Ma 等（[2023](#bib.bib19)）；Xu 等（[2024](#bib.bib39)）。
- en: '![Refer to caption](img/9ab44b442cbed767659d332dba618b90.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9ab44b442cbed767659d332dba618b90.png)'
- en: 'Figure 1: Previous distillation methods (left) rely on extracting supervision
    signals from LLM’s weights or using LLM’s output probabilities to train the retriever
    model. In contrast, our approach (right) bypasses the need for LLM’s likelihood,
    directly using the LLM’s ranking responses as supervision signals.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：以前的蒸馏方法（左）依赖于从 LLM 的权重中提取监督信号或使用 LLM 的输出概率来训练检索模型。相比之下，我们的方法（右）绕过了对 LLM 似然性的需求，直接使用
    LLM 的排名响应作为监督信号。
- en: 'The retrieval-augmented generation (RAG) framework has been widely adopted
    to alleviate hallucination problems in LLMs generation, especially for knowledge-intensive
    tasks Lewis et al. ([2020](#bib.bib17)). The RAG framework consists of two key
    components: a retriever to locate relevant information from a large corpus based
    on a given input, and a reader, typically a LLM, to integrate this information
    into its generation Izacard et al. ([2023](#bib.bib13)); Shi et al. ([2023](#bib.bib28)).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）框架已经被广泛采用，以缓解LLMs生成中的幻觉问题，尤其是对于知识密集型任务 Lewis等（[2020](#bib.bib17)）。RAG框架由两个关键组件组成：一个检索器，用于根据给定输入从大型语料库中定位相关信息，以及一个阅读器，通常是LLM，用于将这些信息融入其生成中
    Izacard等（[2023](#bib.bib13)）；Shi等（[2023](#bib.bib28)）。
- en: How to distill knowledge from LLMs to optimize the retriever in the RAG framework
    with in-domain data has been a crucial challenge. Early efforts proposed training
    the retriever with white-box LLM readers by extracting supervision signals directly
    from the LLMs’ weights Izacard et al. ([2023](#bib.bib13)); Rubin and Berant ([2023](#bib.bib27));
    Guu et al. ([2020](#bib.bib6)). However, this approach becomes more computationally
    intensive and time-consuming as LLMs increase in size. Meanwhile, it is incompatible
    with closed-source models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如何从LLMs中提炼知识以优化RAG框架中的检索器，并使用领域内的数据，一直是一个关键挑战。早期的工作建议通过直接从LLMs的权重中提取监督信号，使用白盒LLM阅读器来训练检索器
    Izacard等（[2023](#bib.bib13)）；Rubin和Berant（[2023](#bib.bib27)）；Guu等（[2020](#bib.bib6)）。然而，随着LLMs规模的增加，这种方法变得更加计算密集且耗时。同时，这种方法与闭源模型不兼容。
- en: 'Recently, researchers have also turned to knowledge distillation for the retriever
    from black-box LLMs by training the retriever directly from generated outputs,
    such as RePLUG Shi et al. ([2023](#bib.bib28)) and In-Context RALM Ram et al.
    ([2023](#bib.bib26)). However, both methods use the generation log probabilities
    for correct answers as the distillation signal to train the retriever, which may
    suffer from: 1) Limited application scenarios, as the output probabilities are
    not always available for closed-source LLMs. 2) Discrepancy between retrieval
    and generation, where training LLMs’ next-token prediction is not optimal for
    retriever training. 3) High computing costs, since hundreds of thousands of training
    instances are required in their training process.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，研究人员还转向通过从生成输出中直接训练检索器来进行知识蒸馏，例如RePLUG Shi等（[2023](#bib.bib28)）和In-Context
    RALM Ram等（[2023](#bib.bib26)）。然而，这两种方法都使用生成日志概率作为蒸馏信号来训练检索器，这可能会遇到以下问题：1）应用场景有限，因为闭源LLMs的输出概率并不总是可用。2）检索与生成之间的不一致性，LLMs的下一个标记预测训练并不适合检索器训练。3）高计算成本，因为在训练过程中需要数十万的训练实例。
- en: 'To address these limitations, we propose Intermediate Distillation, a data-efficient
    training scheme that leverages LLM-generated ranking responses to guide the training
    of the retriever. Our model employs a rerank-then-retrieve pipeline, where LLMs
    indirectly influence the retriever training via an intermediate ranker model.
    We chose this pipeline for three main reasons: 1) The robust zero-shot ranking
    capabilities of LLMs establish a strong foundation for knowledge distillation.
    2) Using LLMs to generate a relevance-based ranking order is more suitable for
    retriever training than depending on LLMs output probabilities, making the supervision
    signals more reliable. 3) There are no restrictions on accessing this generated
    ranking order.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些限制，我们提出了Intermediate Distillation，一种数据高效的训练方案，利用LLM生成的排名响应来指导检索器的训练。我们的模型采用了重新排序-然后检索的管道，其中LLMs通过中间排序模型间接影响检索器训练。我们选择这个管道有三个主要原因：1）LLMs的强大零-shot排名能力为知识蒸馏奠定了坚实的基础。2）使用LLMs生成基于相关性的排名顺序比依赖LLMs输出概率更适合检索器训练，使得监督信号更加可靠。3）对生成的排名顺序没有访问限制。
- en: 'Specifically, we first train a ranker model using the ranking orders generated
    by LLMs as supervision signals. We then employ this trained ranker to further
    train the retriever model. We conduct a series of experiments using advanced,
    closed-source LLMs that restrict output probability access. The empirical results
    demonstrate the effectiveness of our method, requiring 100x to even 1000x less
    data than previous methods Ram et al. ([2023](#bib.bib26)); Shi et al. ([2023](#bib.bib28)),
    thereby significantly reducing computational costs. Our main contributions are:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们首先使用LLM生成的排序顺序作为监督信号训练一个排序模型。然后，利用这个训练好的排序模型进一步训练检索模型。我们使用先进的封闭源LLM进行了系列实验，这些LLM限制了输出概率的访问。实证结果证明了我们方法的有效性，比之前的方法减少了100倍甚至1000倍的数据量
    Ram et al. ([2023](#bib.bib26)); Shi et al. ([2023](#bib.bib28))，从而显著降低了计算成本。我们的主要贡献是：
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce Intermediate Distillation, a data-efficient knowledge distillation
    training scheme that optimizes retrieval models from black-box LLMs via an intermediate
    ranker model in a two-stage process.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了中间蒸馏，一种数据高效的知识蒸馏训练方案，通过中间排序模型在两阶段过程中优化黑箱LLM的检索模型。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct extensive experiments with cutting-edge LLMs and demonstrate the
    efficacy and efficiency of the proposed method in enhancing information retrieval
    performance compared to other supervision signals.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进行了大量实验，使用前沿的LLM，并展示了所提方法在提升信息检索性能方面的有效性和效率，相较于其他监督信号。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We deploy our distilled retriever model within the RAG framework and demonstrate
    its effectiveness in downstream tasks such as open-domain question-answering.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在RAG框架内部署了我们的蒸馏检索模型，并展示了其在下游任务如开放领域问答中的有效性。
- en: 2 Related Work
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: In this section, we provide a comprehensive background of information retrieval
    systems and knowledge distillation research related to LLMs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了与LLM相关的信息检索系统和知识蒸馏研究的全面背景。
- en: '![Refer to caption](img/ab9daf9dc0b1f0758b2634e074cdc6d3.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/ab9daf9dc0b1f0758b2634e074cdc6d3.png)'
- en: 'Figure 2: The two-stage knowledge distillation process of our proposed Intermediate
    Distillation scheme. In Stage 1, we use re-ranking order $\pi$ (highlighted in
    the green background color) as the supervisory signal to train a ranker model.
    In Stage 2, this distilled ranker unsupervised trains the retriever model to enhance
    its performance.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：我们提出的中间蒸馏方案的两阶段知识蒸馏过程。在阶段1中，我们使用重新排序顺序 $\pi$（突出显示在绿色背景色中）作为监督信号来训练排序模型。在阶段2中，这个蒸馏的排序模型无监督地训练检索模型，以提升其性能。
- en: 2.1 Retrieval-Augmented Generation
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 增强检索生成
- en: Information retrieval plays a crucial role in various knowledge-intensive NLP
    tasks, including question-answering Siriwardhana et al. ([2023](#bib.bib30));
    Zhang et al. ([2024](#bib.bib42)), fact-verification Hang et al. ([2024](#bib.bib7));
    Khaliq et al. ([2024](#bib.bib15)) and open-domain dialogue Wang et al. ([2024a](#bib.bib34));
    Shuster et al. ([2021](#bib.bib29)). A prevalent approach in information retrieval
    is the multi-stage retrieval process Nogueira et al. ([2020](#bib.bib23)), which
    first uses a retriever model to search several most relevant documents from the
    large corpus, then employs a ranker model to further optimize the ranking order
    based on relevance, and returns the top few most relevant documents finally.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 信息检索在各种知识密集型NLP任务中扮演着至关重要的角色，包括问答 Siriwardhana et al. ([2023](#bib.bib30));
    Zhang et al. ([2024](#bib.bib42))，事实验证 Hang et al. ([2024](#bib.bib7)); Khaliq
    et al. ([2024](#bib.bib15)) 和开放领域对话 Wang et al. ([2024a](#bib.bib34)); Shuster
    et al. ([2021](#bib.bib29))。信息检索的一个普遍方法是多阶段检索过程 Nogueira et al. ([2020](#bib.bib23))，首先使用检索模型从大语料库中搜索几个最相关的文档，然后使用排序模型进一步优化基于相关性的排序顺序，最后返回最相关的前几个文档。
- en: Recently, the retriever models are increasingly used to enhance the generation
    quality of LLMs for knowledge-intensive tasks due to their flexibility and effectiveness,
    leading to the development of the retrieval-augmented generation (RAG) framework
    Guu et al. ([2020](#bib.bib6)); Izacard et al. ([2023](#bib.bib13)). This framework
    integrates information retrieval into the generation process of LLMs, which helps
    overcome the models’ limitations, such as hallucination, by utilizing external
    up-to-date information. In the RAG framework, the retrieved information can be
    in the form of tokens, entities, or text chunks (i.e., documents), and the retrieval
    can occur once or repeatedly every $n$ tokens, for finding a balance between the
    performance and time-cost. Additionally, the retrieval model in RAG is adaptable
    to both encoder-to-decoder Guu et al. ([2020](#bib.bib6)); Izacard et al. ([2023](#bib.bib13))
    and decoder-only language models Borgeaud et al. ([2022](#bib.bib3)); Ram et al.
    ([2023](#bib.bib26)), and is applicable during both the pre-training Zhong et al.
    ([2022](#bib.bib43)); Min et al. ([2022](#bib.bib22)) and inference stages Menick
    et al. ([2022](#bib.bib20)); Min et al. ([2023](#bib.bib21)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，由于检索模型的灵活性和有效性，越来越多地用于提升 LLMs 在知识密集型任务中的生成质量，导致了检索增强生成（RAG）框架的开发，Guu 等人 ([2020](#bib.bib6))；Izacard
    等人 ([2023](#bib.bib13))。该框架将信息检索整合到 LLMs 的生成过程中，这有助于通过利用外部最新信息来克服模型的局限性，例如幻觉。在
    RAG 框架中，检索到的信息可以是令牌、实体或文本块（即文档）的形式，检索可以在每 $n$ 个令牌后一次或多次进行，以在性能和时间成本之间找到平衡。此外，RAG
    中的检索模型适用于编码器到解码器的模型 Guu 等人 ([2020](#bib.bib6))；Izacard 等人 ([2023](#bib.bib13))
    和仅解码器语言模型 Borgeaud 等人 ([2022](#bib.bib3))；Ram 等人 ([2023](#bib.bib26))，并且在预训练 Zhong
    等人 ([2022](#bib.bib43))；Min 等人 ([2022](#bib.bib22)) 和推理阶段 Menick 等人 ([2022](#bib.bib20))；Min
    等人 ([2023](#bib.bib21)) 都适用。
- en: In this paper, we use the advanced knowledge from LLMs as the supervision signal
    to train the retriever models through a multi-stage (i.e., rerank-then-retrieve)
    training scheme. We then integrate our well-trained retriever model into the RAG
    framework, demonstrating the effectiveness of our proposed training framework
    in question-answering tasks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们使用来自 LLMs 的高级知识作为监督信号，通过多阶段（即先排序后检索）训练方案训练检索模型。然后，我们将训练良好的检索模型整合到 RAG
    框架中，展示了我们提出的训练框架在问答任务中的有效性。
- en: 2.2 Knowledge Distillation in LLMs.
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 大型语言模型中的知识蒸馏。
- en: 'Knowledge distillation is widely used to transfer knowledge from complex, large
    teacher models to smaller student models Hinton et al. ([2015](#bib.bib9)). Influenced
    by the outstanding performance of LLMs, more and more studies focus on using LLMs
    as teacher models to distill knowledge into smaller task-specific models Brown
    et al. ([2023](#bib.bib4)), and the distillation methods can be categorized into
    two types: white-box Gu et al. ([2023](#bib.bib5)); Agarwal et al. ([2023](#bib.bib2));
    Udagawa et al. ([2023](#bib.bib33)) and black-box Li et al. ([2022](#bib.bib18));
    Ho et al. ([2022](#bib.bib10)); Hsieh et al. ([2023](#bib.bib11)). Specifically,
    white-box training leverages both the predictions and the parameters of LLMs to
    exact knowledge, which can be memory-intensive and computationally demanding.
    In contrast, black-box training only relies on the predictions of LLMs, making
    it less resource-intensive.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏被广泛用于将复杂的大型教师模型中的知识转移到较小的学生模型中，Hinton 等人 ([2015](#bib.bib9))。受到大型语言模型（LLMs）出色性能的影响，越来越多的研究集中在使用
    LLMs 作为教师模型，将知识蒸馏到较小的任务特定模型中，Brown 等人 ([2023](#bib.bib4))，并且蒸馏方法可以分为两种类型：白盒 Gu
    等人 ([2023](#bib.bib5))；Agarwal 等人 ([2023](#bib.bib2))；Udagawa 等人 ([2023](#bib.bib33))
    和黑盒 Li 等人 ([2022](#bib.bib18))；Ho 等人 ([2022](#bib.bib10))；Hsieh 等人 ([2023](#bib.bib11))。具体而言，白盒训练利用
    LLMs 的预测和参数来提取知识，这可能会消耗大量内存和计算资源。相比之下，黑盒训练仅依赖于 LLMs 的预测，资源消耗较少。
- en: Many studies have successfully integrated knowledge distillation within the
    RAG framework to train the retriever models. For white-box LLM distillation training,
    previous researches employ LLM likelihood, such as attention scores, to assess
    the relevance distribution of retrieved documents Izacard et al. ([2023](#bib.bib13),
    [2022](#bib.bib12)). Meanwhile, some recent studies have also explored methods
    for training RAG using black-box LLMs, like In-Context RALM Ram et al. ([2023](#bib.bib26))
    and RePLUG Shi et al. ([2023](#bib.bib28)). The remaining problem is that these
    methods still rely on the generation log probabilities for the ground truth as
    the supervision signals in distillation training, which tend to have a degree
    of randomness and are limited to the availability of the output probabilities.
    Furthermore, aligning LLM predictions with the goals of retriever training is
    not the optimal choice since there remains a gap between retrieval and generation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究已经成功地在 RAG 框架内整合了知识蒸馏来训练检索模型。对于白盒 LLM 蒸馏训练，以往的研究采用 LLM 可能性，例如注意力得分，来评估检索文档的相关性分布
    Izacard 等人 ([2023](#bib.bib13), [2022](#bib.bib12))。与此同时，一些近期的研究也探索了使用黑盒 LLMs
    训练 RAG 的方法，如 In-Context RALM Ram 等人 ([2023](#bib.bib26)) 和 RePLUG Shi 等人 ([2023](#bib.bib28))。剩下的问题是，这些方法仍然依赖于生成日志概率作为蒸馏训练中的监督信号，这些信号通常具有一定的随机性，并且受限于输出概率的可用性。此外，将
    LLM 预测与检索器训练的目标对齐并不是最佳选择，因为检索和生成之间仍然存在差距。
- en: In contrast, our proposed distillation method only requires black-box LLMs to
    output a relevance-based ranking order of the candidate relevant documents, yielding
    more consistent, matching, and interpretable results than output probability-based
    methods. Moreover, our method is much more data efficient, requiring about 100x
    and 1000x less data compared to previous approaches Shi et al. ([2023](#bib.bib28));
    Ram et al. ([2023](#bib.bib26)), significantly saving computational resources
    and increasing the flexibility of the training process.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，我们提出的蒸馏方法仅需黑盒 LLMs 输出候选相关文档的基于相关性的排名顺序，比基于输出概率的方法产生更一致、匹配和可解释的结果。此外，我们的方法在数据效率方面更高，相较于之前的方法
    Shi 等人 ([2023](#bib.bib28)); Ram 等人 ([2023](#bib.bib26))，需要的数据减少约 100 倍和 1000
    倍，显著节省了计算资源，并增加了训练过程的灵活性。
- en: 3 Method
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'Our two-stage distillation scheme uses a ranker model and a retriever model
    as the student models and a LLM as the teacher model. As shown in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Related Work ‣ Intermediate Distillation: Data-Efficient Distillation
    from Black-Box LLMs for Information Retrieval"), we initially employ an off-the-shelf
    retriever to select a subset of documents $D_{n}$, which is used to train the
    ranker model in the distillation Stage 1. In Stage 2, this ranker enhances the
    original retriever by minimizing the KL-divergence between their similarity likelihood.
    In detail, Section [3.1](#S3.SS1 "3.1 Problem Formulation ‣ 3 Method ‣ Intermediate
    Distillation: Data-Efficient Distillation from Black-Box LLMs for Information
    Retrieval") provides the formal definitions of the related tasks. In Section [3.2](#S3.SS2
    "3.2 Stage 1: Distillation from LLMs to Ranker ‣ 3 Method ‣ Intermediate Distillation:
    Data-Efficient Distillation from Black-Box LLMs for Information Retrieval") and
    Section [3.3](#S3.SS3 "3.3 Stage 2: Distillation from Ranker to Retriever ‣ 3
    Method ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box
    LLMs for Information Retrieval"), we show how knowledge is directly transferred
    from LLMs to a ranker model and then further conveyed to a retriever model, respectively.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的两阶段蒸馏方案使用一个排序模型和一个检索模型作为学生模型，以及一个 LLM 作为教师模型。如图 [2](#S2.F2 "图 2 ‣ 2 相关工作
    ‣ 中间蒸馏：从黑盒 LLM 到信息检索的数据高效蒸馏") 所示，我们最初使用一个现成的检索器选择一个文档子集 $D_{n}$，该子集用于在蒸馏阶段 1 中训练排序模型。在阶段
    2 中，这个排序模型通过最小化其相似性可能性之间的 KL 散度来增强原始检索器。具体而言，部分 [3.1](#S3.SS1 "3.1 问题表述 ‣ 3 方法
    ‣ 中间蒸馏：从黑盒 LLM 到信息检索的数据高效蒸馏") 提供了相关任务的正式定义。在部分 [3.2](#S3.SS2 "3.2 阶段 1：从 LLM 到排序模型的蒸馏
    ‣ 3 方法 ‣ 中间蒸馏：从黑盒 LLM 到信息检索的数据高效蒸馏") 和部分 [3.3](#S3.SS3 "3.3 阶段 2：从排序模型到检索模型的蒸馏
    ‣ 3 方法 ‣ 中间蒸馏：从黑盒 LLM 到信息检索的数据高效蒸馏") 中，我们展示了知识是如何直接从 LLM 传递到排序模型中，然后进一步传递到检索模型中的。
- en: 3.1 Problem Formulation
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题表述
- en: Given a question $Q$.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个问题 $Q$。
- en: For the re-ranking task in our distillation framework, the teacher ranker model
    (i.e., LLMs) is tasked with reordering the documents $D_{n}$ is first transferred
    to the ranker model, which serves as an intermediary between the LLM and the retriever
    model. Subsequently, the ranker model conveys this knowledge to the retriever,
    thereby enhancing its performance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们蒸馏框架中的重排序任务中，教师 Ranker 模型（即 LLMs）负责重新排序文档 $D_{n}$，这些文档首先转移到 Ranker 模型，该模型充当
    LLM 和检索模型之间的中介。随后，Ranker 模型将这些知识传达给检索模型，从而提高其性能。
- en: '![Refer to caption](img/9e020e00b2f6b35a29e2bf50f91bd6cd.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9e020e00b2f6b35a29e2bf50f91bd6cd.png)'
- en: 'Figure 3: An example of our LLM teacher model’s re-ranking process.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：我们 LLM 教师模型的重排序过程示例。
- en: '3.2 Stage 1: Distillation from LLMs to Ranker'
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 第一阶段：从 LLMs 到 Ranker 的蒸馏
- en: The initial step of our knowledge distillation workflow is data initialization,
    where we find the relevant document subsets $D_{n}$. These subsets then serve
    as the input for the Stage 1 training. In practice, we employ a widely-used information
    retrieval model, Contriever Izacard et al. ([2022](#bib.bib12)), as the retriever
    model for data initialization.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的知识蒸馏工作流的初始步骤是数据初始化，在这个阶段我们找到相关的文档子集 $D_{n}$。这些子集随后作为第一阶段训练的输入。在实践中，我们使用广泛应用的信息检索模型
    Contriever Izacard 等（[2022](#bib.bib12)）作为数据初始化的检索模型。
- en: Re-ranking by LLMs. In this stage, we utilize LLM’s guaranteed zero-shot ranking
    capabilities to generate high-quality re-ranking orders for each subset $D_{n}$.
    Following this, we use these re-ranking orders to transfer LLM’s knowledge into
    a smaller but more efficient ranker model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 LLMs 重排序。在这个阶段，我们利用 LLM 的零样本排序能力，为每个子集 $D_{n}$ 生成高质量的重排序顺序。随后，我们使用这些重排序顺序将
    LLM 的知识转移到一个更小但更高效的 Ranker 模型中。
- en: 'Ranker Distillation Training We initialize our ranker model by using the dual-encoder
    structure Contriever checkpoint. For each question $Q$, which can be defined as:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Ranker 蒸馏训练 我们通过使用双编码器结构 Contriever 检查点初始化我们的 Ranker 模型。对于每个问题 $Q$，可以定义为：
- en: '|  | $1$2 |  | (1) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: where $s$ is the temperature hyper-parameter.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s$ 是温度超参数。
- en: 'After obtaining the ranker model’s similarity likelihood $P_{RANK}(n_{i}|Q)$
    as the ground truth, aiming to minimize the following loss function:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得 Ranker 模型的相似性概率 $P_{RANK}(n_{i}|Q)$ 作为真值后，旨在最小化以下损失函数：
- en: '|  | $\displaystyle L(P_{RANK},\pi)$ |  | (2) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(P_{RANK},\pi)$ |  | (2) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: where $\pi_{P_{RANK}}$.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\pi_{P_{RANK}}$。
- en: '3.3 Stage 2: Distillation from Ranker to Retriever'
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 第二阶段：从 Ranker 到 Retriever 的蒸馏
- en: In Stage 2, this well-train ranker model is used to enhance the retriever model’s
    performance by transferring knowledge from LLMs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，这个训练良好的 Ranker 模型被用来通过转移来自 LLMs 的知识来增强检索模型的性能。
- en: 'We initialize our retriever model using a dual-encoder Contriever checkpoint,
    similar to the ranker. For each question and its retrieved document, we also compute
    their representations $\widetilde{Q}$ is defined as:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用双编码器 Contriever 检查点初始化我们的检索模型，类似于 Ranker。对于每个问题及其检索到的文档，我们还计算其表示 $\widetilde{Q}$，定义如下：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: where $s$ is the temperature hyper-parameter.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s$ 是温度超参数。
- en: 'We then leverage the similarity likelihood $P_{RANK}$:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后利用相似性概率 $P_{RANK}$：
- en: '|  | $D_{KL}(P_{RANK}&#124;&#124;P_{RETR})$ |  | (4) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $D_{KL}(P_{RANK}&#124;&#124;P_{RETR})$ |  | (4) |'
- en: This process ensures the retriever model aligns more closely with the text similarity
    knowledge from the LLMs. Through this two-stage distillation scheme, we enhance
    the retrieval accuracy and effectiveness of the retriever model, which can be
    further applied to the RAG framework to improve its performance on knowledge-intensive
    NLP tasks.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程确保检索模型更贴近 LLMs 的文本相似性知识。通过这两阶段的蒸馏方案，我们提高了检索模型的检索准确性和效果，这可以进一步应用于 RAG 框架，以提高其在知识密集型
    NLP 任务中的表现。
- en: 4 Experiment
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: '| Distillation Methods | NQ | TriviaQA |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 蒸馏方法 | NQ | TriviaQA |  |'
- en: '| HR@5$\uparrow$ |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| HR@5$\uparrow$ |'
- en: '| w/o Distillation | 0.478 | 0.583 | 26.09 | 36.75 |  | 0.595 | 0.678 | 54.99
    | 63.55 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 无蒸馏 | 0.478 | 0.583 | 26.09 | 36.75 |  | 0.595 | 0.678 | 54.99 | 63.55 |'
- en: '| Supervised Distillation |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 有监督蒸馏 |'
- en: '| BM25 | 0.186 | 0.262 | 18.17 | 27.88 |  | 0.120 | 0.175 | 46.90 | 55.38 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | 0.186 | 0.262 | 18.17 | 27.88 |  | 0.120 | 0.175 | 46.90 | 55.38 |'
- en: '| Rule-Based | 0.223 | 0.303 | 19.75 | 29.64 |  | 0.277 | 0.356 | 50.08 | 58.45
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 基于规则 | 0.223 | 0.303 | 19.75 | 29.64 |  | 0.277 | 0.356 | 50.08 | 58.45 |'
- en: '| Metric (ROUGE-2) | 0.534 | 0.643 | 27.76 | 38.16 |  | 0.641 | 0.716 | 56.17
    | 64.92 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Metric (ROUGE-2) | 0.534 | 0.643 | 27.76 | 38.16 |  | 0.641 | 0.716 | 56.17
    | 64.92 |'
- en: '| Intermediate Distillation (Ours) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Intermediate Distillation (Ours) |'
- en: '| GPT-3.5 Turbo | 0.505 | 0.606 | 25.84 | 36.13 |  | 0.587 | 0.664 | 53.72
    | 62.16 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 Turbo | 0.505 | 0.606 | 25.84 | 36.13 |  | 0.587 | 0.664 | 53.72
    | 62.16 |'
- en: '| GPT-4o | 0.553 | 0.652 | 27.01 | 37.38 |  | 0.664 | 0.734 | 56.27 | 64.98
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 0.553 | 0.652 | 27.01 | 37.38 |  | 0.664 | 0.734 | 56.27 | 64.98
    |'
- en: '| GPT-4 Turbo | 0.545 | 0.656 | 28.31 | 38.68 |  | 0.662 | 0.727 | 56.15 |
    65.07 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 Turbo | 0.545 | 0.656 | 28.31 | 38.68 |  | 0.662 | 0.727 | 56.15 |
    65.07 |'
- en: '| Claude3 Opus | 0.562 | 0.665 | 28.45 | 38.83 |  | 0.669 | 0.733 | 56.68 |
    65.36 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Claude3 Opus | 0.562 | 0.665 | 28.45 | 38.83 |  | 0.669 | 0.733 | 56.68 |
    65.36 |'
- en: 'Table 1: The performance comparison of our proposed Intermediate Distillation
    scheme with other baseline supervised distillation methods on question-answering
    tasks.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：我们提出的 Intermediate Distillation 方案与其他基准监督蒸馏方法在问答任务上的性能比较。
- en: 4.1 Experiment Setup
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'Dataset We conduct experiments on two benchmark open-domain question-answering
    datasets: NaturalQuestions (NQ) Kwiatkowski et al. ([2019](#bib.bib16)) and TriviaQA
    Joshi et al. ([2017](#bib.bib14)). The NQ dataset includes queries from google.com
    query and their corresponding Wikipedia pages, each with an annotated passage
    containing the answer. We use the dataset version provided by ATLAS Izacard et al.
    ([2023](#bib.bib13)) and follow its training, validation, and testing splits:
    79,168/8,757/3,610. Similarly, we also use the TriviaQA, which contains question-answer
    pairs sourced from Wikipedia and the web, that ATLAS provides and following its
    training, validation, and testing splits: 78,785/8,837/11,313. For the knowledge
    corpus base, we utilize data from Wikipedia as of December 20, 2018, adapting
    the passage embeddings provided by ATLAS.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 我们在两个基准开放域问答数据集上进行实验：NaturalQuestions (NQ) Kwiatkowski et al. ([2019](#bib.bib16))
    和 TriviaQA Joshi et al. ([2017](#bib.bib14))。NQ 数据集包含来自 google.com 的查询及其对应的维基百科页面，每个页面都有一个包含答案的标注段落。我们使用
    ATLAS Izacard et al. ([2023](#bib.bib13)) 提供的数据集版本，并遵循其训练、验证和测试分割：79,168/8,757/3,610。同样，我们还使用
    TriviaQA，它包含从维基百科和网络获取的问答对，ATLAS 提供了该数据集，并遵循其训练、验证和测试分割：78,785/8,837/11,313。对于知识语料库基础，我们利用截至
    2018 年 12 月 20 日的维基百科数据，调整 ATLAS 提供的段落嵌入。
- en: 'For our experiments, we selectively sample 1,000 instances from each training
    set from NQ and TriviaQA, keeping validation and testing sets unchanged. We further
    discuss the impact of selecting different types of training data in Section [5.2](#S5.SS2
    "5.2 Impact of the Training Data Type ‣ 5 Analysis ‣ Intermediate Distillation:
    Data-Efficient Distillation from Black-Box LLMs for Information Retrieval"). This
    training set size is about 100 to 1,000 times smaller than those used in previous
    black-box LLM distillation methods within the RAG framework, demonstrating the
    superior data efficiency of our approach. The impact of training set size on performance
    is discussed further in Section [5.3](#S5.SS3 "5.3 Impact of the Training Set
    Size ‣ 5 Analysis ‣ Intermediate Distillation: Data-Efficient Distillation from
    Black-Box LLMs for Information Retrieval").'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '对于我们的实验，我们从 NQ 和 TriviaQA 的每个训练集中有选择地抽取 1,000 个实例，保持验证和测试集不变。我们在第 [5.2](#S5.SS2
    "5.2 Impact of the Training Data Type ‣ 5 Analysis ‣ Intermediate Distillation:
    Data-Efficient Distillation from Black-Box LLMs for Information Retrieval") 节中进一步讨论了选择不同类型训练数据的影响。该训练集的大小比
    RAG 框架内先前的黑箱 LLM 蒸馏方法使用的训练集小约 100 到 1,000 倍，展示了我们方法的数据效率优势。训练集大小对性能的影响在第 [5.3](#S5.SS3
    "5.3 Impact of the Training Set Size ‣ 5 Analysis ‣ Intermediate Distillation:
    Data-Efficient Distillation from Black-Box LLMs for Information Retrieval") 节中进一步讨论。'
- en: 'Baseline We evaluate our distillation framework, Intermediate Distillation,
    against the following established text similarity methods: ROUGE-2, an evaluation
    metric frequently used in NLP, and BM25, a popular information retrieval algorithm.
    The idea of employing NLP evaluation metrics like ROUGE-2 for knowledge distillation
    in retriever models is first proposed by He et al. ([2022](#bib.bib8)), which
    also uses a multi-step distillation approach to solve the Commonsense Reasoning
    tasks ¹¹1We choose ROUGE-2 as our compared baseline metric, as it outperforms
    other metrics in this prior study.. For both ROUGE-2 and BM25, we use the similarity
    likelihood between the query and its relevant documents via their calculation
    to generate re-ranking order as the supervision signals. Meanwhile, we do not
    consider the previous work RePLUG Shi et al. ([2023](#bib.bib28)) as a baseline
    since it uses a larger data scale and relies on LLMs output probabilities, which
    is not a fair comparison.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 基线 我们将我们的蒸馏框架Intermediate Distillation与以下已建立的文本相似性方法进行比较：ROUGE-2，这是一个在NLP中常用的评估指标，以及BM25，这是一个流行的信息检索算法。He
    et al. ([2022](#bib.bib8))首次提出使用像ROUGE-2这样的NLP评估指标来进行检索器模型的知识蒸馏，该方法还使用了多步骤蒸馏方法来解决常识推理任务¹¹1我们选择ROUGE-2作为对比基线指标，因为在这项先前的研究中，它优于其他指标..
    对于ROUGE-2和BM25，我们通过计算查询与相关文档之间的相似性来生成重新排序的顺序作为监督信号。同时，我们不将之前的工作RePLUG Shi et al.
    ([2023](#bib.bib28))作为基线，因为它使用了更大的数据规模并依赖LLMs输出概率，这不是一个公平的比较。
- en: In addition, we conduct a Rule-Based experiment that ranks documents containing
    the answer at the top in re-ranking order, which aims to demonstrate that effective
    re-ranking distillation signals should not only highlight answers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们进行了一项基于规则的实验，该实验将包含答案的文档在重新排序中排名靠前，旨在证明有效的重新排序蒸馏信号不仅应突出显示答案。
- en: Experimental Settings We initialize our ranker and retriever models using the
    Contriever checkpoint Izacard et al. ([2022](#bib.bib12)) with a dual-encoder
    structure. For our proposed distillation scheme, we select several cutting-edge
    and representative LLMs as the teacher models, including GPT-3.5 Turbo, GPT-4o,
    GPT-4 Turbo, and Claude3 Opus. To comprehensively evaluate the performance of
    our retriever model, we integrate the distilled retriever model into the RAG framework
    for question-answering tasks, which allows us to measure the improvement in the
    quality of responses generated by the language model. In the RAG framework, we
    use the reader model Llama-3-8B-Instruct Touvron et al. ([2023](#bib.bib32)) to
    generate answers. We also evaluate a baseline version of this RAG framework without
    additional distillation training for the retriever (i.e., w/o Distillation experiment).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置 我们使用Contriever检查点Izacard et al. ([2022](#bib.bib12))并采用双编码器结构来初始化我们的排序器和检索器模型。对于我们提出的蒸馏方案，我们选择了几个前沿且具有代表性的LLM作为教师模型，包括GPT-3.5
    Turbo、GPT-4o、GPT-4 Turbo和Claude3 Opus。为了全面评估我们检索器模型的性能，我们将蒸馏后的检索器模型集成到RAG框架中用于问答任务，这使我们能够衡量语言模型生成的响应质量的提升。在RAG框架中，我们使用阅读器模型Llama-3-8B-Instruct
    Touvron et al. ([2023](#bib.bib32))来生成答案。我们还评估了一个没有额外蒸馏训练的RAG框架基线版本（即，w/o Distillation实验）。
- en: 'Implantation Details We set both the ranker and retriever models with a hidden
    layer size of 768, thus totally have approximately 10 million training parameters
    for each model. The learning rates are set as 5e-5 for the ranker model and 2e-5
    for the retriever model. Both models are trained for 5 epochs on the NQ and TriviaQA
    datasets, using a batch size of 20 and optimized with the Adam optimizer. Additionally,
    we restrict the size of the relevant document subset $D_{n}$ to 5, each retrieved
    document with a maximum length of 128\. We further discuss the impact of the retrieve
    subset (i.e., re-ranking list) size in Section [5.4](#S5.SS4 "5.4 Impact of the
    Re-ranking List Size ‣ 5 Analysis ‣ Intermediate Distillation: Data-Efficient
    Distillation from Black-Box LLMs for Information Retrieval").'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '实施细节 我们将排序器和检索器模型的隐藏层大小设置为768，因此每个模型总共有大约1000万的训练参数。学习率设置为排序器模型的5e-5和检索器模型的2e-5。两个模型在NQ和TriviaQA数据集上训练了5个epoch，使用批量大小为20，并采用Adam优化器进行优化。此外，我们将相关文档子集$D_{n}$的大小限制为5，每个检索文档的最大长度为128。我们将在第[5.4节](#S5.SS4
    "5.4 Impact of the Re-ranking List Size ‣ 5 Analysis ‣ Intermediate Distillation:
    Data-Efficient Distillation from Black-Box LLMs for Information Retrieval")进一步讨论检索子集（即重新排序列表）大小的影响。'
- en: Evaluation Metrics We evaluate the retrieval performance of our distilled retriever
    model through the top-5 and top-10 retrieval Hit Rates (HR@5 and HR@10), which
    is the percentage of questions where the relevant document subset $D_{n}$ includes
    at lease one correct answers with the top-5 and top-10 documents. For question-answering
    tasks in the RAG framework, we use the standard Exact Match (EM) metric and F1-Score
    to evaluate the accuracy and precision of the language model generated responses.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 评价指标 我们通过前5和前10的检索命中率（HR@5和HR@10）来评估我们蒸馏检索器模型的检索性能，即相关文档子集$D_{n}$中包含至少一个正确答案的前5和前10文档的问题比例。在RAG框架下的问答任务中，我们使用标准的精确匹配（EM）指标和F1-Score来评估语言模型生成回应的准确性和精度。
- en: 4.2 Experimental Results
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实验结果
- en: 'We present our experimental results, including all the baseline methods and
    settings evaluated on the testing set of NQ and TriviaQA in Table [1](#S4.T1 "Table
    1 ‣ 4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation from
    Black-Box LLMs for Information Retrieval")²²2The highest values in the table are
    highlighted in bold on both the NQ and TriviaQA datasets.. The experimental results
    show that the retriever model, under our proposed Intermediate Distillation scheme
    and supervised by Claude3, achieves the best performance in most evaluation metrics,
    confirming the effectiveness of our proposed method.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '我们展示了我们的实验结果，包括在NQ和TriviaQA测试集上评估的所有基准方法和设置，如表[1](#S4.T1 "Table 1 ‣ 4 Experiment
    ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for
    Information Retrieval")²²2表中的最高值在NQ和TriviaQA数据集上都以粗体突出显示。实验结果表明，在我们提出的Intermediate
    Distillation方案下，受Claude3监督的检索器模型在大多数评估指标中表现最佳，确认了我们方法的有效性。'
- en: 'Moreover, the quality of supervision signals from LLMs greatly influences the
    performance of the distilled retriever models. For example, the retriever model
    trained under GPT-4 Turbo supervision outperforms the one supervised by GPT-3.5
    Turbo within our Intermediate Distillation scheme, aligning with GPT-4 Turbo’s
    higher performance across various NLP tasks OpenAI ([2023](#bib.bib24)). As the
    rapid development of LLMs, this improvement in supervision quality has also evolved:
    from being less effective than the ROUGE-2 metric (i.e., as seen the retriever
    under supervised by GPT-3.5 Turbo) to significantly surpass it (i.e., supervised
    by GPT-4 Turbo).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLMs提供的监督信号的质量在很大程度上影响了蒸馏检索器模型的性能。例如，在我们的Intermediate Distillation方案下，受GPT-4
    Turbo监督的检索器模型优于受GPT-3.5 Turbo监督的模型，这与GPT-4 Turbo在各种NLP任务中的更高性能一致（OpenAI [2023](#bib.bib24)）。随着LLMs的快速发展，监督质量的提升也发生了变化：从不如ROUGE-2指标有效（即，受GPT-3.5
    Turbo监督的检索器）到显著超越ROUGE-2（即，受GPT-4 Turbo监督的检索器）。
- en: 'In the RAG framework for question-answering tasks, a stronger retriever model
    is more likely to enhance the output quality of the reader model, demonstrating
    the effectiveness and adaptability of the Intermediate Distillation framework
    for NLP downstream tasks. However, according to our experimental results, while
    Intermediate Distillation using GPT-4o typically outperforms Supervised Distillation
    using ROUGE-2 in retrieval performance, the latter can still produce higher quality
    generations within RAG. This divergence may be due to the different objectives
    of the retriever and the reader: the retriever focuses on accurately identifying
    the ground truth, whereas the reader wants the retriever to provide information
    that more effectively helps the reader in generating accurate responses. This
    discrepancy remains a topic that can be further explored in future work.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG框架下进行问答任务时，较强的检索模型更可能提升阅读模型的输出质量，这展示了Intermediate Distillation框架在NLP下游任务中的有效性和适应性。然而，根据我们的实验结果，尽管使用GPT-4o的Intermediate
    Distillation在检索性能上通常优于使用ROUGE-2的Supervised Distillation，但后者在RAG中的生成质量仍然更高。这种差异可能源于检索器和阅读器的不同目标：检索器专注于准确识别真实情况，而阅读器希望检索器提供的信息能更有效地帮助其生成准确的回应。这种不一致性仍然是未来工作中可以深入探讨的主题。
- en: 5 Analysis
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 分析
- en: In this section, we conduct ablation studies and a series of quantitative analyses
    to evaluate how various experimental designs and settings affect the outcomes
    of our distillation results.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们进行消融研究和一系列定量分析，以评估各种实验设计和设置对我们蒸馏结果的影响。
- en: 5.1 Ablation Studies
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 消融研究
- en: 'In this subsection, we conduct an experiment named Direct Distillation, where
    we train the retriever model directly using the relevance likelihood generated
    by LLM. More details of experiment setting can be found in Appendix [A](#A1 "Appendix
    A Analysis of Different Distillation Signal Generated by LLMs ‣ Intermediate Distillation:
    Data-Efficient Distillation from Black-Box LLMs for Information Retrieval"). We
    compare the results of this approach with our proposed two-stage distillation
    scheme under the same LLM teacher model (i.e., GPT-4o), and the experimental results
    are shown in Table [2](#S5.T2 "Table 2 ‣ 5.2 Impact of the Training Data Type
    ‣ 5 Analysis ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box
    LLMs for Information Retrieval"). The results indicate that the Direct Distillation
    method is less effective than our proposed Intermediate Distillation scheme, which
    further validates the rationality of the two-stage design of our proposed framework.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们进行了一项名为直接蒸馏的实验，其中我们直接使用 LLM 生成的相关性概率训练检索模型。实验设置的更多细节可以在附录 [A](#A1 "附录
    A LLM 生成的不同蒸馏信号分析 ‣ 中间蒸馏：从黑箱 LLM 中进行数据高效蒸馏") 中找到。我们将这种方法的结果与我们提出的两阶段蒸馏方案在相同 LLM
    教师模型（即 GPT-4o）下进行比较，实验结果如表 [2](#S5.T2 "表 2 ‣ 5.2 训练数据类型的影响 ‣ 5 分析 ‣ 中间蒸馏：从黑箱 LLM
    中进行数据高效蒸馏") 所示。结果表明，直接蒸馏方法的效果不如我们提出的中间蒸馏方案，这进一步验证了我们提出的框架的两阶段设计的合理性。
- en: 5.2 Impact of the Training Data Type
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 训练数据类型的影响
- en: '| Method | Dataset | Evaluation Metrics |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 数据集 | 评估指标 |  |'
- en: '| EM$\uparrow$ |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| EM$\uparrow$ |  |'
- en: '| Direct Distillation | NQ | 26.23 | 36.47 | 0.505 |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 直接蒸馏 | NQ | 26.23 | 36.47 | 0.505 |  |'
- en: '|  | TriviaQA | 55.39 | 63.96 | 0.623 |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | TriviaQA | 55.39 | 63.96 | 0.623 |  |'
- en: '| Intermediate Distillation | NQ | 27.01 | 37.38 | 0.553 |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 中间蒸馏 | NQ | 27.01 | 37.38 | 0.553 |  |'
- en: '|  | TriviaQA | 56.27 | 64.98 | 0.664 |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | TriviaQA | 56.27 | 64.98 | 0.664 |  |'
- en: 'Table 2: Ablation studies on the effectiveness of two-stage distillation scheme
    design.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：关于两阶段蒸馏方案设计有效性的消融研究。
- en: '![Refer to caption](img/fb5e7519779d7a8a1edd99225a44b84f.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fb5e7519779d7a8a1edd99225a44b84f.png)'
- en: 'Figure 4: The performance of retriever models across three types of training
    sets, which vary based on the initial appearance and placement of ground truth
    in the retrieved subsets.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：检索模型在三种不同训练集上的表现，这些训练集在检索子集中的初始出现和地面真实值的放置方式有所不同。
- en: 'Our previous experiment demonstrate that Rule-Based supervision signals, which
    places documents containing answers at the top, are ineffective and detrimentally
    impacting the retriever’s performance. This indicates that simply re-ranking documents
    based solely on the presence of ground truth (i.e., the correct answer) does not
    provide the high-quality text similarity insights required for effective distillation.
    To delve deeper into the influence of the appearance and placement of ground truth
    in the re-ranking process, we categorize the initial retrieved document subsets
    $D_{n}$ based on NQ’s queries into three categories: (1) Following-Answer: contains
    at least one document with the correct answer, but this kind of document is not
    at the first position in the subset. This data type is used in our experiments
    detailed in Section [4](#S4 "4 Experiment ‣ Intermediate Distillation: Data-Efficient
    Distillation from Black-Box LLMs for Information Retrieval"). (2) First-Answer:
    contains at least one document with the correct answer, and this kind of documents
    is at the first position in the subset. (3) No-Answer: no documents in the subset
    contain the correct answer.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前的实验表明，基于规则的监督信号（将包含答案的文档置于顶部）无效且对检索器的性能产生了负面影响。这表明，仅根据地面真实值（即正确答案）的存在对文档进行重新排序，并不能提供高质量的文本相似性见解，从而实现有效的蒸馏。为了深入探讨地面真实值在重新排序过程中的出现和放置的影响，我们将初始检索的文档子集
    $D_{n}$ 基于 NQ 的查询分为三类：(1) 跟随答案：包含至少一个具有正确答案的文档，但这种文档不在子集的第一位。此数据类型用于我们在第 [4](#S4
    "4 实验 ‣ 中间蒸馏：从黑箱 LLM 中进行数据高效蒸馏") 节中详细描述的实验。(2) 第一答案：包含至少一个具有正确答案的文档，并且这种文档位于子集的第一位。(3)
    无答案：子集中没有文档包含正确答案。
- en: 'We follow the same training setting used in our primary experiments in Section
    [4](#S4 "4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation
    from Black-Box LLMs for Information Retrieval"), and use GPT-4 Turbo as the LLM
    teacher model. Together with findings from the Rule-Based experiments in Section
    [4](#S4 "4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation
    from Black-Box LLMs for Information Retrieval"), the experiment results shown
    in Figure [4](#S5.F4 "Figure 4 ‣ 5.2 Impact of the Training Data Type ‣ 5 Analysis
    ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for
    Information Retrieval") indicate that considering the semantic similarity of the
    text is far more important than arranging documents containing the answers to
    the top for re-ranking in distillation training, as even the retriever under the
    No-Answer data set training has notable improvements.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '我们遵循了在第[4](#S4 "4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation
    from Black-Box LLMs for Information Retrieval")节中主要实验所使用的相同训练设置，并使用GPT-4 Turbo作为LLM教师模型。结合第[4](#S4
    "4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box
    LLMs for Information Retrieval")节规则基础实验的发现，图[4](#S5.F4 "Figure 4 ‣ 5.2 Impact
    of the Training Data Type ‣ 5 Analysis ‣ Intermediate Distillation: Data-Efficient
    Distillation from Black-Box LLMs for Information Retrieval")中的实验结果表明，在蒸馏训练中，考虑文本的语义相似性远比将包含答案的文档排序到前面更为重要，因为即使是在No-Answer数据集训练下，检索器也有显著的改进。'
- en: Moreover, as the Following-Answer training data, where the correct answers are
    not ranked first initially, yields better training results than using the the
    First-Answer training data, indicating that optimizing the ground truth placement
    in re-ranking also has a positive effect on the experimental results after the
    consideration of text similarity.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Following-Answer训练数据中，正确答案最初未被排在首位的情况，比使用First-Answer训练数据取得更好的训练结果，这表明在考虑文本相似性后，优化
    ground truth 排名在重排序中的位置也对实验结果产生了积极影响。
- en: 5.3 Impact of the Training Set Size
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 训练集规模的影响
- en: 'Our previous experiments demonstrate that our distillation framework significantly
    enhances retriever model performance with just 1,000 training instances. In this
    subsection, we explore how different training set sizes affect distillation effectiveness.
    We use training sets of of 50, 100, 200, 500, 1000, and 2000 data instances from
    the Following-Answer data type, with other settings consistent with our experiments
    in Section [4](#S4 "4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation
    from Black-Box LLMs for Information Retrieval"). In addition, we use GPT-4 Turbo
    as our LLM teacher model.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '我们之前的实验表明，利用我们的蒸馏框架，在仅有1,000个训练实例的情况下，可以显著提升检索模型的性能。在这一小节中，我们探讨了不同训练集规模对蒸馏效果的影响。我们使用了50、100、200、500、1000和2000个数据实例的训练集，数据类型为Following-Answer，其它设置与我们在第[4](#S4
    "4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box
    LLMs for Information Retrieval")节中的实验一致。此外，我们使用GPT-4 Turbo作为我们的LLM教师模型。'
- en: 'Results in Figure [5](#S5.F5 "Figure 5 ‣ 5.3 Impact of the Training Set Size
    ‣ 5 Analysis ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box
    LLMs for Information Retrieval") show that the performance of the retriever model
    improves significantly with training data with thousands of or even only hundreds
    of instances. These empirical findings highlight the data efficiency of our proposed
    distillation scheme. In addition, although initial performance increases are notable
    with small training sets, the rate of improvement decreases as more training data
    is used. This pattern indicates a scaling law in distillation training, where
    further enhancements become increasingly difficult as the model’s performance
    improves. For models that already perform well, even marginal improvements require
    much more data, demanding greater training resources.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#S5.F5 "Figure 5 ‣ 5.3 Impact of the Training Set Size ‣ 5 Analysis ‣
    Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for
    Information Retrieval")中的结果显示，使用成千上万或甚至仅有几百个实例的训练数据，检索模型的性能显著提升。这些经验性发现突显了我们提出的蒸馏方案的数据效率。此外，尽管在使用较小训练集时初期性能的提升显著，但随着训练数据的增加，性能提升的速率会降低。这种模式表明了蒸馏训练中的扩展法则，即随着模型性能的提升，进一步的改进变得越来越困难。对于已经表现良好的模型，即使是微小的改进也需要大量的数据，需求更多的训练资源。'
- en: '![Refer to caption](img/ddb8f11306e33a85372bb5e10769ba92.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ddb8f11306e33a85372bb5e10769ba92.png)'
- en: 'Figure 5: The performance of retriever models under different training set
    size.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：不同训练集规模下检索模型的性能。
- en: 5.4 Impact of the Re-ranking List Size
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 重排序列表大小的影响
- en: '![Refer to caption](img/3edeaeb3aa1581e3e2b236732b03ff3f.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3edeaeb3aa1581e3e2b236732b03ff3f.png)'
- en: 'Figure 6: The performance of retriever models under different size of the re-ranking
    list. The performance corresponding to 0 re-ranking list size represents the baseline
    retriever model performance.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：不同大小的重新排序列表下检索模型的表现。与0大小的重新排序列表对应的性能表示基线检索模型的表现。
- en: 'In previous experiments, we set the re-ranking list to five documents (i.e.,
    we retrieve five relevant documents each time). Generally, larger re-ranking lists
    offer more supervision signals from LLMs, thus potentially enhancing the effectiveness
    of distillation training. To explore the impact of re-ranking list size on our
    distillation method, we vary the re-ranking list sizes, using the top-3, top-5,
    top-7, and top-10 documents from each relevant retrieved subset to conduct the
    distillation training. We keep other training settings consistent with those in
    our primary experiments in Section [4](#S4 "4 Experiment ‣ Intermediate Distillation:
    Data-Efficient Distillation from Black-Box LLMs for Information Retrieval") and
    use GPT-4 Turbo as the LLM teacher model.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '在之前的实验中，我们将重新排序列表设置为五个文档（即每次检索五个相关文档）。通常，较大的重新排序列表提供了更多来自LLMs的监督信号，从而可能提高蒸馏训练的效果。为了探讨重新排序列表大小对我们蒸馏方法的影响，我们调整了重新排序列表的大小，使用来自每个相关检索子集的前3个、前5个、前7个和前10个文档进行蒸馏训练。我们保持其他训练设置与在第[4](#S4
    "4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box
    LLMs for Information Retrieval")节中的主要实验一致，并使用GPT-4 Turbo作为LLM教师模型。'
- en: 'The experimental results shown in Figure [6](#S5.F6 "Figure 6 ‣ 5.4 Impact
    of the Re-ranking List Size ‣ 5 Analysis ‣ Intermediate Distillation: Data-Efficient
    Distillation from Black-Box LLMs for Information Retrieval") show that increasing
    the re-ranking list size progressively improves the effectiveness of the distillation
    training. As the list expands from re-ranking three documents to ten documents,
    the performance of the distilled retriever model consistently improves. Moreover,
    compared with the retriever model’s baseline performance, setting the size of
    the re-ranking list to 3 still significantly improves the retriever model’s performance
    not only in HitRate@3 but also across broader metrics from HitRate@5 to HitRate@10.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '图[6](#S5.F6 "Figure 6 ‣ 5.4 Impact of the Re-ranking List Size ‣ 5 Analysis
    ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for
    Information Retrieval")中显示的实验结果表明，增加重新排序列表的大小会逐步提高蒸馏训练的效果。随着列表从重新排序三个文档扩展到十个文档，蒸馏检索模型的性能不断提升。此外，与检索模型的基线性能相比，将重新排序列表的大小设置为3仍显著提高了检索模型的性能，不仅在HitRate@3中如此，而且在从HitRate@5到HitRate@10的更广泛指标中也是如此。'
- en: 6 Conclusion
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We propose Intermediate Distillation, a two-stage data-efficient knowledge distillation
    scheme that uses the remarkable capabilities of black-box LLMs to train an information
    retrieval model through an intermediate ranker model. We conduct extensive experiments
    with advanced LLMs, demonstrating that our method enhances the effectiveness and
    efficiency of the retriever model performance compared to other supervision signals.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了中间蒸馏，这是一种两阶段的数据高效知识蒸馏方案，利用黑箱LLMs的卓越能力，通过一个中间排序模型训练信息检索模型。我们进行了大量的实验，证明我们的方法在提高检索模型性能的有效性和效率方面优于其他监督信号。
- en: 7 Limitation
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 局限性
- en: This paper proposes a data-efficient distillation scheme using black-box LLMs
    to train smaller information retrieval models, which prove its effectiveness with
    training data on the scale of thousands. However, we do not evaluate our proposed
    distillation scheme with larger scales of training data, such as tens of thousands
    or millions of instances, due to budget limitations on accessing responses from
    closed-source LLMs and insufficient computational resources to utilize high-quality
    open-source LLMs like Llama-70B. In the future work, we will focus on extending
    this study to larger-scale training data, using either closed-source or advanced
    open-source LLMs to further analysis the effectiveness of our proposed distillation
    scheme.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种数据高效的蒸馏方案，使用黑箱LLMs来训练较小的信息检索模型，并通过规模达到数千的训练数据证明了其有效性。然而，由于预算限制，未能评估我们提出的蒸馏方案在更大规模训练数据上的表现，如数万或数百万个实例，因为无法访问闭源LLMs的响应，并且计算资源不足以利用像Llama-70B这样的高质量开源LLMs。在未来的工作中，我们将专注于将这项研究扩展到更大规模的训练数据，使用闭源或先进的开源LLMs，以进一步分析我们提出的蒸馏方案的有效性。
- en: References
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Adeyemi et al. (2023) Mofetoluwa Adeyemi, Akintunde Oladipo, Ronak Pradeep,
    and Jimmy Lin. 2023. Zero-shot cross-lingual reranking with large language models
    for low-resource languages. *arXiv preprint arXiv:2312.16159*.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adeyemi et al. (2023) Mofetoluwa Adeyemi, Akintunde Oladipo, Ronak Pradeep,
    和 Jimmy Lin. 2023. 使用大型语言模型进行零样本跨语言重排序以应对低资源语言。*arXiv 预印本 arXiv:2312.16159*。
- en: 'Agarwal et al. (2023) Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela
    Ramos, Matthieu Geist, and Olivier Bachem. 2023. Gkd: Generalized knowledge distillation
    for auto-regressive sequence models. *arXiv preprint arXiv:2306.13649*.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Agarwal et al. (2023) Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela
    Ramos, Matthieu Geist, 和 Olivier Bachem. 2023. Gkd: 自回归序列模型的广义知识蒸馏。*arXiv 预印本
    arXiv:2306.13649*。'
- en: Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by
    retrieving from trillions of tokens. In *International conference on machine learning*,
    pages 2206–2240\. PMLR.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, 等. 2022. 通过从万亿个标记中检索来改进语言模型。发表于 *国际机器学习会议*，第2206–2240页。PMLR。
- en: 'Brown et al. (2023) Nathan Brown, Ashton Williamson, Tahj Anderson, and Logan
    Lawrence. 2023. [Efficient transformer knowledge distillation: A performance review](https://doi.org/10.18653/v1/2023.emnlp-industry.6).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing: Industry Track*, pages 54–65, Singapore. Association for Computational
    Linguistics.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2023) Nathan Brown, Ashton Williamson, Tahj Anderson, 和 Logan
    Lawrence. 2023. [高效的 Transformer 知识蒸馏：性能评估](https://doi.org/10.18653/v1/2023.emnlp-industry.6)。发表于
    *2023年自然语言处理经验方法会议：行业轨道*，第54–65页，新加坡。计算语言学协会。
- en: 'Gu et al. (2023) Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Minillm:
    Knowledge distillation of large language models. In *The Twelfth International
    Conference on Learning Representations*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu et al. (2023) Yuxian Gu, Li Dong, Furu Wei, 和 Minlie Huang. 2023. Minillm:
    大型语言模型的知识蒸馏。发表于 *第十二届国际学习表征会议*。'
- en: Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei
    Chang. 2020. Retrieval augmented language model pre-training. In *International
    conference on machine learning*, pages 3929–3938\. PMLR.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, 和 Mingwei
    Chang. 2020. 检索增强语言模型预训练。发表于 *国际机器学习会议*，第3929–3938页。PMLR。
- en: 'Hang et al. (2024) Ching Nam Hang, Pei-Duo Yu, and Chee Wei Tan. 2024. Trumorgpt:
    Query optimization and semantic reasoning over networks for automated fact-checking.
    In *2024 58th Annual Conference on Information Sciences and Systems (CISS)*, pages
    1–6\. IEEE.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hang et al. (2024) Ching Nam Hang, Pei-Duo Yu, 和 Chee Wei Tan. 2024. Trumorgpt:
    网络上的查询优化和语义推理用于自动化事实核查。发表于 *2024年第58届信息科学与系统年会 (CISS)*，第1–6页。IEEE。'
- en: 'He et al. (2022) Xingwei He, Yeyun Gong, A Jin, Weizhen Qi, Hang Zhang, Jian
    Jiao, Bartuer Zhou, Biao Cheng, Siu Ming Yiu, Nan Duan, et al. 2022. Metric-guided
    distillation: Distilling knowledge from the metric to ranker and retriever for
    generative commonsense reasoning. *arXiv preprint arXiv:2210.11708*.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. (2022) Xingwei He, Yeyun Gong, A Jin, Weizhen Qi, Hang Zhang, Jian
    Jiao, Bartuer Zhou, Biao Cheng, Siu Ming Yiu, Nan Duan, 等. 2022. 基于度量的蒸馏: 从度量到排名器和检索器的知识蒸馏，用于生成常识推理。*arXiv
    预印本 arXiv:2210.11708*。'
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, 和 Jeff Dean. 2015. 提炼神经网络中的知识。*arXiv
    预印本 arXiv:1503.02531*。
- en: Ho et al. (2022) Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language
    models are reasoning teachers. *arXiv preprint arXiv:2212.10071*.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho et al. (2022) Namgyu Ho, Laura Schmid, 和 Se-Young Yun. 2022. 大型语言模型是推理教师。*arXiv
    预印本 arXiv:2212.10071*。
- en: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    2023. Distilling step-by-step! outperforming larger language models with less
    training data and smaller model sizes. *arXiv preprint arXiv:2305.02301*.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, 和 Tomas Pfister.
    2023. 步步蒸馏！用更少的训练数据和更小的模型尺寸超越更大的语言模型。*arXiv 预印本 arXiv:2305.02301*。
- en: Izacard et al. (2022) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. [Unsupervised
    dense information retrieval with contrastive learning](https://arxiv.org/abs/2112.09118).
    *Preprint*, arXiv:2112.09118.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard 等（2022）Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel,
    Piotr Bojanowski, Armand Joulin, 和 Edouard Grave。2022。[使用对比学习的无监督密集信息检索](https://arxiv.org/abs/2112.09118)。*预印本*，arXiv:2112.09118。
- en: 'Izacard et al. (2023) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini,
    Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,
    and Edouard Grave. 2023. Atlas: Few-shot learning with retrieval augmented language
    models. *Journal of Machine Learning Research*, 24(251):1–43.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard 等（2023）Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini,
    Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,
    和 Edouard Grave。2023。《Atlas：通过检索增强语言模型的少样本学习》。*机器学习研究杂志*，24(251):1–43。
- en: 'Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.
    2017. [TriviaQA: A large scale distantly supervised challenge dataset for reading
    comprehension](https://doi.org/10.18653/v1/P17-1147). In *Proceedings of the 55th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, pages 1601–1611, Vancouver, Canada. Association for Computational Linguistics.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi 等（2017）Mandar Joshi, Eunsol Choi, Daniel Weld, 和 Luke Zettlemoyer。2017。[TriviaQA：用于阅读理解的大规模远程监督挑战数据集](https://doi.org/10.18653/v1/P17-1147)。在*第55届计算语言学协会年会论文集（第1卷：长篇论文）*，第1601–1611页，加拿大温哥华。计算语言学协会。
- en: 'Khaliq et al. (2024) M Abdul Khaliq, P Chang, M Ma, Bernhard Pflugfelder, and
    F Miletić. 2024. Ragar, your falsehood radar: Rag-augmented reasoning for political
    fact-checking using multimodal large language models. *arXiv preprint arXiv:2404.12065*.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khaliq 等（2024）M Abdul Khaliq, P Chang, M Ma, Bernhard Pflugfelder, 和 F Miletić。2024。《Ragar，你的虚假信息雷达：利用多模态大语言模型的Rag增强推理进行政治事实核查》。*arXiv
    预印本 arXiv:2404.12065*。
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei
    Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. [Natural
    questions: A benchmark for question answering research](https://doi.org/10.1162/tacl_a_00276).
    *Transactions of the Association for Computational Linguistics*, 7:452–466.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwiatkowski 等（2019）Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael
    Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob
    Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei
    Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, 和 Slav Petrov。2019。[自然问题：问题回答研究的基准](https://doi.org/10.1162/tacl_a_00276)。*计算语言学协会会刊*，7:452–466。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *Advances in Neural Information Processing Systems*, 33:9459–9474.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等（2020）Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel
    等。2020。《用于知识密集型 NLP 任务的检索增强生成》。*神经信息处理系统进展*，33:9459–9474。
- en: Li et al. (2022) Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang,
    Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, et al. 2022. Explanations
    from large language models make small reasoners better. *arXiv preprint arXiv:2210.06726*.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2022）Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun
    Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao 等。2022。《来自大语言模型的解释使小型推理器表现更好》。*arXiv
    预印本 arXiv:2210.06726*。
- en: Ma et al. (2023) Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin.
    2023. Fine-tuning llama for multi-stage text retrieval. *arXiv preprint arXiv:2310.08319*.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2023）Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, 和 Jimmy Lin。2023。《为多阶段文本检索微调
    llama》。*arXiv 预印本 arXiv:2310.08319*。
- en: Menick et al. (2022) Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides,
    Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham,
    Geoffrey Irving, and Nat McAleese. 2022. [Teaching language models to support
    answers with verified quotes](https://arxiv.org/abs/2203.11147). *Preprint*, arXiv:2203.11147.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Menick 等（2022）Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides,
    Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham,
    Geoffrey Irving, 和 Nat McAleese。2022。[教语言模型用经过验证的引用支持答案](https://arxiv.org/abs/2203.11147)。*预印本*，arXiv:2203.11147。
- en: 'Min et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau
    Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
    [Factscore: Fine-grained atomic evaluation of factual precision in long form text
    generation](https://arxiv.org/abs/2305.14251). *Preprint*, arXiv:2305.14251.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Min 等人（2023）Sewon Min、Kalpesh Krishna、Xinxi Lyu、Mike Lewis、Wen-tau Yih、Pang
    Wei Koh、Mohit Iyyer、Luke Zettlemoyer 和 Hannaneh Hajishirzi。2023。《[Factscore: 细粒度原子评估长文本生成中的事实准确性](https://arxiv.org/abs/2305.14251)》。*预印本*，arXiv:2305.14251。'
- en: Min et al. (2022) Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Nonparametric masked language
    modeling. *arXiv preprint arXiv:2212.01349*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min 等人（2022）Sewon Min、Weijia Shi、Mike Lewis、Xilun Chen、Wen-tau Yih、Hannaneh
    Hajishirzi 和 Luke Zettlemoyer。2022。《非参数掩码语言建模》。*arXiv 预印本 arXiv:2212.01349*。
- en: Nogueira et al. (2020) Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020.
    Document ranking with a pretrained sequence-to-sequence model. *arXiv preprint
    arXiv:2003.06713*.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nogueira 等人（2020）Rodrigo Nogueira、Zhiying Jiang 和 Jimmy Lin。2020。《使用预训练的序列到序列模型进行文档排名》。*arXiv
    预印本 arXiv:2003.06713*。
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](https://arxiv.org/abs/2303.08774).
    *Preprint*, arXiv:2303.08774.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI。2023。《[GPT-4 技术报告](https://arxiv.org/abs/2303.08774)》。*预印本*，arXiv:2303.08774。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems*, 35:27730–27744.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人（2022）Long Ouyang、Jeffrey Wu、Xu Jiang、Diogo Almeida、Carroll Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray 等人。2022。《训练语言模型以跟随人类反馈的指令》。*神经信息处理系统进展*，35:27730–27744。
- en: Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon
    Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented
    language models. *Transactions of the Association for Computational Linguistics*,
    11:1316–1331.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ram 等人（2023）Ori Ram、Yoav Levine、Itay Dalmedigos、Dor Muhlgay、Amnon Shashua、Kevin
    Leyton-Brown 和 Yoav Shoham。2023。《上下文检索增强语言模型》。*计算语言学学会会刊*，11:1316–1331。
- en: Rubin and Berant (2023) Ohad Rubin and Jonathan Berant. 2023. Long-range language
    modeling with self-retrieval. *arXiv preprint arXiv:2306.13421*.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubin 和 Berant（2023）Ohad Rubin 和 Jonathan Berant。2023。《自我检索的长距离语言建模》。*arXiv
    预印本 arXiv:2306.13421*。
- en: 'Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich
    James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented
    black-box language models. *arXiv preprint arXiv:2301.12652*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2023）Weijia Shi、Sewon Min、Michihiro Yasunaga、Minjoon Seo、Rich James、Mike
    Lewis、Luke Zettlemoyer 和 Wen-tau Yih。2023。《Replug：检索增强的黑箱语言模型》。*arXiv 预印本 arXiv:2301.12652*。
- en: Shuster et al. (2021) Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and
    Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation.
    *arXiv preprint arXiv:2104.07567*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shuster 等人（2021）Kurt Shuster、Spencer Poff、Moya Chen、Douwe Kiela 和 Jason Weston。2021。《检索增强减少了对话中的幻觉》。*arXiv
    预印本 arXiv:2104.07567*。
- en: Siriwardhana et al. (2023) Shamane Siriwardhana, Rivindu Weerasekera, Elliott
    Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving
    the domain adaptation of retrieval augmented generation (rag) models for open
    domain question answering. *Transactions of the Association for Computational
    Linguistics*, 11:1–17.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siriwardhana 等人（2023）Shamane Siriwardhana、Rivindu Weerasekera、Elliott Wen、Tharindu
    Kaluarachchi、Rajib Rana 和 Suranga Nanayakkara。2023。《改善检索增强生成（rag）模型在开放领域问答中的领域适应性》。*计算语言学学会会刊*，11:1–17。
- en: Sun et al. (2023) Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin,
    and Zhaochun Ren. 2023. Is chatgpt good at search? investigating large language
    models as re-ranking agent. *arXiv preprint arXiv:2304.09542*.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2023）Weiwei Sun、Lingyong Yan、Xinyu Ma、Pengjie Ren、Dawei Yin 和 Zhaochun
    Ren。2023。《ChatGPT 擅长搜索吗？调查大语言模型作为重新排序代理的表现》。*arXiv 预印本 arXiv:2304.09542*。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2023）Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等人。2023。《Llama：开放且高效的基础语言模型》。*arXiv 预印本 arXiv:2302.13971*。
- en: 'Udagawa et al. (2023) Takuma Udagawa, Aashka Trivedi, Michele Merler, and Bishwaranjan
    Bhattacharjee. 2023. [A comparative analysis of task-agnostic distillation methods
    for compressing transformer language models](https://doi.org/10.18653/v1/2023.emnlp-industry.3).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing: Industry Track*, pages 20–31, Singapore. Association for Computational
    Linguistics.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Udagawa et al. (2023) Takuma Udagawa, Aashka Trivedi, Michele Merler, 和 Bishwaranjan
    Bhattacharjee. 2023. [任务无关的蒸馏方法对压缩变换器语言模型的比较分析](https://doi.org/10.18653/v1/2023.emnlp-industry.3).
    在 *2023年自然语言处理经验方法会议：工业追踪论文集*, 第20–31页，新加坡。计算语言学协会。
- en: 'Wang et al. (2024a) Hongru Wang, Wenyu Huang, Yang Deng, Rui Wang, Zezhong
    Wang, Yufei Wang, Fei Mi, Jeff Z Pan, and Kam-Fai Wong. 2024a. Unims-rag: A unified
    multi-source retrieval-augmented generation for personalized dialogue systems.
    *arXiv preprint arXiv:2401.13256*.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2024a) Hongru Wang, Wenyu Huang, Yang Deng, Rui Wang, Zezhong
    Wang, Yufei Wang, Fei Mi, Jeff Z Pan, 和 Kam-Fai Wong. 2024a. Unims-rag: 统一的多源检索增强生成用于个性化对话系统.
    *arXiv 预印本 arXiv:2401.13256*。'
- en: Wang et al. (2024b) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024b. A survey
    on large language model based autonomous agents. *Frontiers of Computer Science*,
    18(6):1–26.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2024b) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, 等. 2024b. 基于大型语言模型的自主智能体调查.
    *计算机科学前沿*, 18(6):1–26。
- en: 'Wu et al. (2023) Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark
    Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.
    2023. Bloomberggpt: A large language model for finance. *arXiv preprint arXiv:2303.17564*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. (2023) Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark
    Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, 和 Gideon Mann.
    2023. Bloomberggpt: 用于金融的大型语言模型. *arXiv 预印本 arXiv:2303.17564*。'
- en: 'Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and
    potential of large language model based agents: A survey. *arXiv preprint arXiv:2309.07864*.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, 等. 2023. 基于大型语言模型的智能体的崛起与潜力:
    一项调查. *arXiv 预印本 arXiv:2309.07864*。'
- en: 'Xia et al. (2008) Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang
    Li. 2008. Listwise approach to learning to rank: theory and algorithm. In *Proceedings
    of the 25th international conference on Machine learning*, pages 1192–1199.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia et al. (2008) Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, 和 Hang Li.
    2008. 基于列表的方法进行排名学习: 理论与算法. 在 *第25届国际机器学习会议论文集*, 第1192–1199页。'
- en: 'Xu et al. (2024) Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May D
    Wang, Joyce C Ho, Chao Zhang, and Carl Yang. 2024. Bmretriever: Tuning large language
    models as better biomedical text retrievers. *arXiv preprint arXiv:2404.18443*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2024) Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May
    D Wang, Joyce C Ho, Chao Zhang, 和 Carl Yang. 2024. Bmretriever: 调整大型语言模型以更好地作为生物医学文本检索器.
    *arXiv 预印本 arXiv:2404.18443*。'
- en: Zhang et al. (2023a) Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023a. [Extractive
    summarization via chatgpt for faithful summary generation](https://arxiv.org/abs/2304.04193).
    *Preprint*, arXiv:2304.04193.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023a) Haopeng Zhang, Xiao Liu, 和 Jiawei Zhang. 2023a. [通过 chatgpt
    进行提取式摘要以生成忠实摘要](https://arxiv.org/abs/2304.04193). *预印本*, arXiv:2304.04193。
- en: 'Zhang et al. (2023b) Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023b. [Summit:
    Iterative text summarization via chatgpt](https://arxiv.org/abs/2305.14835). *Preprint*,
    arXiv:2305.14835.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023b) Haopeng Zhang, Xiao Liu, 和 Jiawei Zhang. 2023b. [Summit:
    通过 chatgpt 进行迭代文本摘要](https://arxiv.org/abs/2305.14835). *预印本*, arXiv:2305.14835。'
- en: 'Zhang et al. (2024) Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen,
    Matei Zaharia, Ion Stoica, and Joseph E Gonzalez. 2024. Raft: Adapting language
    model to domain specific rag. *arXiv preprint arXiv:2403.10131*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2024) Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen,
    Matei Zaharia, Ion Stoica, 和 Joseph E Gonzalez. 2024. Raft: 适应语言模型到领域特定的rag. *arXiv
    预印本 arXiv:2403.10131*。'
- en: Zhong et al. (2022) Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language
    models with memory augmentation. *arXiv preprint arXiv:2205.12674*.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong et al. (2022) Zexuan Zhong, Tao Lei, 和 Danqi Chen. 2022. 使用记忆增强训练语言模型.
    *arXiv 预印本 arXiv:2205.12674*。
- en: Appendix A Analysis of Different Distillation Signal Generated by LLMs
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 不同蒸馏信号的分析
- en: Here we provide more details about the prompt design of the Direct Distillation
    experiment. We also analyze the corresponding generation quality of LLMs, including
    the stability and interpretability, and compare it with the re-ranking generation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供了有关直接蒸馏实验的提示设计的更多细节。我们还分析了LLMs生成的质量，包括稳定性和可解释性，并将其与重新排序生成进行比较。
- en: A.1 Direct Distillation Prompt Design
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 直接蒸馏提示设计
- en: 'We follow the re-ranking prompt format design and replace the re-ranking task
    with quantifying the similarity scores of the retrieved documents for LLM’s supervised
    signals generation. An example input prompt and the generated responses without
    explanations is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循重新排序提示格式设计，并将重新排序任务替换为量化检索文档的相似度分数，用于LLM的监督信号生成。以下是一个示例输入提示及生成的响应（不带解释）：
- en: 'Input Prompt:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 输入提示：
- en: '[PRE0]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Generated Response:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的响应：
- en: '[PRE1]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A.2 Comparison with Re-ranking Response
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 与重新排序响应的比较
- en: 'We compare the responses generated from re-ranking prompts and those derived
    from similarity score prompts used in the Direct Distillation experiment. Specifically,
    we randomly select 1,000 data instances from the NQ dataset, using the queries
    and their retrieved documents to prompt the LLM to generate both list-wise re-ranking
    orders and similarity scores. Examples of responses generated by the LLM, specifically
    using GPT-4 Turbo, are shown in Table [3](#A1.T3 "Table 3 ‣ A.3 Implantation Details
    of Direct Distillation Experiment ‣ Appendix A Analysis of Different Distillation
    Signal Generated by LLMs ‣ Intermediate Distillation: Data-Efficient Distillation
    from Black-Box LLMs for Information Retrieval").'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了从重新排序提示生成的响应和在直接蒸馏实验中使用的相似度分数提示生成的响应。具体而言，我们从NQ数据集中随机选择了1,000个数据实例，使用查询及其检索文档提示LLM生成列表排序顺序和相似度分数。使用GPT-4
    Turbo生成的LLM响应示例如表[3](#A1.T3 "表3 ‣ A.3 直接蒸馏实验的植入细节 ‣ 附录A LLM生成的不同蒸馏信号分析 ‣ 中间蒸馏：从黑箱LLMs中进行数据高效的蒸馏用于信息检索")所示。
- en: '![Refer to caption](img/660825afab7decaac0a6841dd2cff95d.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/660825afab7decaac0a6841dd2cff95d.png)'
- en: 'Figure 7: Spearman Correlation between the responses from the re-ranking prompt
    and the responses from the similarity score prompt.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：重新排序提示的响应与相似度分数提示的响应之间的斯皮尔曼相关性。
- en: 'Table [3](#A1.T3 "Table 3 ‣ A.3 Implantation Details of Direct Distillation
    Experiment ‣ Appendix A Analysis of Different Distillation Signal Generated by
    LLMs ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs
    for Information Retrieval") shows that responses generated from re-ranking prompts
    are more interpretable than those from similarity score prompts, which often use
    scaled values that are ambiguous. Additionally, responses from the similarity
    score-based prompt frequently yield extreme values, such as 0.0 (completely dissimilar)
    or 1.0 (highly similar), in some cases, which means that that supervision signals
    based on similarity scores are less informative and act more like binary signals
    in certain data instances.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表[3](#A1.T3 "表3 ‣ A.3 直接蒸馏实验的植入细节 ‣ 附录A LLM生成的不同蒸馏信号分析 ‣ 中间蒸馏：从黑箱LLMs中进行数据高效的蒸馏用于信息检索")显示，从重新排序提示生成的响应比从相似度分数提示生成的响应更具可解释性，后者通常使用模糊的缩放值。此外，相似度分数提示的响应经常产生极端值，如0.0（完全不相似）或1.0（高度相似），在某些情况下，这意味着基于相似度分数的监督信号信息较少，在某些数据实例中更像是二元信号。
- en: 'Moreover, we use the Spearman Correlation to assess the consistency between
    responses generated from re-ranking prompts and those from similarity score prompts,
    and the analysis result is visualized in Figure [7](#A1.F7 "Figure 7 ‣ A.2 Comparison
    with Re-ranking Response ‣ Appendix A Analysis of Different Distillation Signal
    Generated by LLMs ‣ Intermediate Distillation: Data-Efficient Distillation from
    Black-Box LLMs for Information Retrieval"). A higher Spearman correlation value
    suggests a stronger positive correlation between the two types of responses. From
    Figure [7](#A1.F7 "Figure 7 ‣ A.2 Comparison with Re-ranking Response ‣ Appendix
    A Analysis of Different Distillation Signal Generated by LLMs ‣ Intermediate Distillation:
    Data-Efficient Distillation from Black-Box LLMs for Information Retrieval"), we
    can see that many response pairs are closely related, indicating the stability
    and reliability of LLMs in generating responses for similar tasks. In addition,
    based on our previous analysis, we can see that the responses from re-ranking
    prompts are not only reliable but also possess a higher information density compared
    to those from similarity score prompts, showing that responses from re-ranking
    prompts have higher-quality supervision capabilities.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们使用Spearman相关性来评估从重新排序提示生成的响应与从相似性评分提示生成的响应之间的一致性，分析结果可视化在图[7](#A1.F7 "Figure
    7 ‣ A.2 Comparison with Re-ranking Response ‣ Appendix A Analysis of Different
    Distillation Signal Generated by LLMs ‣ Intermediate Distillation: Data-Efficient
    Distillation from Black-Box LLMs for Information Retrieval")中。较高的Spearman相关值表示两种响应之间的正相关性更强。从图[7](#A1.F7
    "Figure 7 ‣ A.2 Comparison with Re-ranking Response ‣ Appendix A Analysis of Different
    Distillation Signal Generated by LLMs ‣ Intermediate Distillation: Data-Efficient
    Distillation from Black-Box LLMs for Information Retrieval")中可以看到，许多响应对之间的相关性很强，表明LLMs在生成类似任务的响应时具有稳定性和可靠性。此外，根据我们之前的分析，我们可以看到，来自重新排序提示的响应不仅可靠，而且信息密度更高，表明重新排序提示生成的响应具有更高的质量监督能力。'
- en: A.3 Implantation Details of Direct Distillation Experiment
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 直接蒸馏实验的实施细节
- en: 'We use the same training settings as those used in Intermediate Distillation
    experiments. Superficially, we set the retriever model with a hidden layer size
    to 768 and initialize it using a dual-encoder Contriever checkpoint. The model
    is trained over five epochs using the same dataset as described in Section [4](#S4
    "4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box
    LLMs for Information Retrieval"), with a learning rate of 5e-5, a batch size of
    20, and optimization via the Adam optimizer. In addition, we also limit the size
    of the relevant document subset $D_{n}$ to five, and each retrieved document with
    a maximum length of 128.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用了与中级蒸馏实验相同的训练设置。从表面上看，我们将检索模型的隐藏层大小设置为768，并使用双编码器Contriever检查点进行初始化。该模型在五个epoch上进行训练，使用与第[4](#S4
    "4 Experiment ‣ Intermediate Distillation: Data-Efficient Distillation from Black-Box
    LLMs for Information Retrieval")节中描述的相同数据集，学习率为5e-5，批次大小为20，优化算法为Adam优化器。此外，我们还将相关文档子集$D_{n}$的大小限制为五个，每个检索到的文档最大长度为128。'
- en: '| Question & Retrieved Documents | Type | LLM’s Generation |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 问题与检索文档 | 类型 | LLM生成 |'
- en: '| --- | --- | --- |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| [Question] membership in the european union requires countries to have which
    type of government [Document1] the european continent. the eu’s example is being
    followed by the african union, the union of south American nations… [Document2]
    powers of states, the considerable level of translation and the choice of a lesser
    city as the capital… [Document3] principal characteristic of classical democracy.
    the concept of a mixed government was studied during the renaissance and the age
    of reason by niccolo machiavelli, giambattista vico… [Document4] thus essentially
    preventing it from participating in government. the european union ( eu ) is an
    association of twenty - eight sovereign member states… [Document5] civiletti opinions,
    not all funding gaps led to shutdowns. of the nine funding gaps between 1980 and
    1990, only four led to furloughs… | Re-ranking | [Re-ranking order] Document5,
    Document3, Document2, Document1, Document4. [Explanation] Document5 directly discusses
    the governmental systems used by most European nations, particularly the parliamentary
    system, which is relevant since the EU generally comprises nations with parliamentary
    democracies. This makes it the most relevant to the query about the type of government
    required for EU membership. Document3 talks about mixed constitutions… |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| [问题] 加入欧盟需要哪些类型的政府 [Document1] 欧洲大陆。欧盟的例子正在被非洲联盟、南美国家联盟…… [Document2] 各州的权力、翻译的相当程度以及选择一个较小城市作为首都……
    [Document3] 经典民主的主要特征。混合政府的概念在文艺复兴和理性时代由尼科洛·马基雅维利、詹巴蒂斯塔·维科…… [Document4] 从而本质上防止它参与政府。欧盟（EU）是由二十八个主权成员国组成的……
    [Document5] 奇维莱蒂的观点，并非所有资金缺口都导致了停工。在1980年至1990年之间的九个资金缺口中，只有四个导致了停薪…… | 重新排序 |
    [重新排序顺序] Document5, Document3, Document2, Document1, Document4. [解释] Document5
    直接讨论了大多数欧洲国家使用的政府系统，特别是议会制，这与欧盟通常由拥有议会民主制的国家组成的查询相关性最高。因此，它最符合关于欧盟成员资格所需政府类型的查询。Document3
    讨论了混合宪法……'
- en: '| Similarity Score | [Similarity scores from Document1 to Document5] Doc1:0.0,
    Doc2:0.1, Doc3:0.3, Doc4:0.1, Doc5:0.7. [Explanation] Document1 talks about the
    EU and various other international organizations, focusing on aspects like the
    single market and policy integration but does not mention government types required
    for EU membership. Hence, the score is 0.0\. Document2 discusses the structure
    of EU governance… |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 相似度得分 | [从 Document1 到 Document5 的相似度得分] Doc1:0.0, Doc2:0.1, Doc3:0.3, Doc4:0.1,
    Doc5:0.7. [解释] Document1 讨论了欧盟和其他国际组织，重点关注单一市场和政策整合等方面，但没有提到欧盟成员国所需的政府类型。因此，得分为0.0。Document2
    讨论了欧盟治理结构…… |'
- en: '| [Question] when was the immigration reform and control act passed [Document1]
    united states temporarily for money or a better life. an increase in legal immigrants
    migrating to the united states also led to an increase of illegal aliens… [Document2]
    there are noted communists, socialists, and anti - american sympathizers. parts
    of the act remain in place today… [Document3] …the immigration reform and control
    act ( irca ), also known as the simpson 2013 mazzoli act or the reagan amnesty,
    signed into law by ronald reagan on november 6, 1986… [Document4] five percent
    of the u. s. workforce. it was also estimated that about 70 percent of those illegal
    workers… [Document5] to reduce them as restrictionists. "fry traced" restrictions
    roots " to a surprising surge in illegal and legal immigration… | Re-ranking |
    [Re-ranking order] Document3, Document1, Document4, Document5, Document2. [Explanation]
    Document3 directly answers the query by explicitly mentioning the "Immigration
    Reform and Control Act of 1986" and stating it was signed into law by Ronald Reagan
    on November 6, 1986\. It provides the most specific information related to the
    query… |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| [问题] 移民改革和控制法案是什么时候通过的 [Document1] 美国暂时为了金钱或更好的生活。合法移民涌入美国也导致了非法移民的增加…… [Document2]
    其中提到的共产主义者、社会主义者和反美同情者。该法案的部分内容今天仍然有效…… [Document3] …移民改革和控制法案（IRCA），也称为辛普森-马佐利法案或里根大赦，于1986年11月6日由罗纳德·里根签署成为法律……
    [Document4] 美国劳动力的5%。估计约70%的非法工人…… [Document5] 将它们减少为限制主义者。“弗莱追溯”限制根源“到非法和合法移民的惊人激增……
    | 重新排序 | [重新排序顺序] Document3, Document1, Document4, Document5, Document2. [解释]
    Document3 直接回答了查询，明确提到“1986年的移民改革和控制法案”，并指出它是由罗纳德·里根于1986年11月6日签署成为法律的。它提供了与查询最相关的具体信息……
    |'
- en: '| Similarity Score | [Similarity scores from Document1 to Document5] Doc1:0.0,
    Doc2:0.0, Doc3:1.0, Doc4:0.0, Doc5:0.0. [Explanation] Document1 discusses general
    immigration trends and issues but does not provide the specific date of the passage
    of the Immigration Reform and Control Act, which is directly asked in the query.
    Therefore, it receives a score of 0.0\. Although Document2 mentions other immigration-related
    legislation and acts, it does not specify… |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 相似度评分 | [从文档1到文档5的相似度评分] Doc1:0.0, Doc2:0.0, Doc3:1.0, Doc4:0.0, Doc5:0.0。
    [解释] 文档1讨论了一般的移民趋势和问题，但没有提供移民改革和控制法案通过的具体日期，这在查询中直接提问。因此，它的得分为0.0。虽然文档2提到了其他与移民相关的立法和法案，但没有具体说明…'
- en: '| [Question] who sings in there’s something about mary [Document1] melody.
    mary described the completed song as "whitney - fied". meron maintains that… [Document2]
    …he film concludes with the two engaging in a kiss while singer / songwriter jonathan
    richman, who has periodically narrated the story in song throughout the film…
    [Document3] day but her. minutes later bing notices his tackle box has been disturbed,
    and an instant later the kids see the door to the tent is open… [Document4] true
    opinions of other contestants and her stance on the judges. the book achieves
    in combining fifty years of a’salt of the earth’existence with a new - found life
    of fame… [Document5] and breezy number and the narrator opens up the story about
    bing and his family. and while bing is cleaning the fish, he tells the story of
    " goldilocks"… | Re-ranking | [Re-ranking order] Document2, Document5, Document4,
    Document1, Document3. [Explanation] Document2 is the most relevant sentence as
    it directly answers the query. It mentions that Jonathan Richman narrated parts
    of the story through song throughout the film "There’s Something About Mary,"
    which clearly identifies the singer related to your query… |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| [问题] 谁在《玛丽有什么事》中唱歌 [文档1] 旋律。玛丽将完成的歌曲描述为“惠特尼 - 化”。梅伦认为… [文档2] …影片最后，两人亲吻，歌手/词曲作者乔纳森·里奇曼在影片中通过歌曲周期性地叙述了故事…
    [文档3] 白天，但她。几分钟后，宾注意到他的工具箱被扰动，片刻后，孩子们看到帐篷的门开了… [文档4] 其他参赛者的真实意见以及她对评委的态度。这本书成功地将五十年的“盐土之地”生活与新发现的名声结合起来…
    [文档5] 和轻快的歌曲，叙述者讲述了关于宾及其家庭的故事。当宾正在清理鱼时，他讲述了“金发姑娘”的故事… | 重新排序 | [重新排序顺序] 文档2, 文档5,
    文档4, 文档1, 文档3。 [解释] 文档2是最相关的句子，因为它直接回答了查询。它提到乔纳森·里奇曼在影片《玛丽有什么事》中通过歌曲叙述了故事的部分内容，这明确指出了与查询相关的歌手…'
- en: '| Similarity Score | [Similarity scores from Document1 to Document5] Doc1:0.0,
    Doc2:1.0, Doc3:0.0, Doc4:0.0, Doc5:0.0. [Explanation] Document1 discusses a song
    in a completely different context, related to "Cinderella" and Whitney Houston’s
    music adjustments, not related to the film "There’s Something About Mary." Thus,
    it doesn’t answer the query about who sings in the specified film, receives a
    score of 0.0\. Document2 directly answers the query. It mentions that Jonathan
    Richman narrated parts of the story… |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 相似度评分 | [从文档1到文档5的相似度评分] Doc1:0.0, Doc2:1.0, Doc3:0.0, Doc4:0.0, Doc5:0.0。
    [解释] 文档1讨论了一首完全不同背景的歌曲，涉及到“灰姑娘”和惠特尼·休斯顿的音乐调整，而与电影《玛丽有什么事》无关。因此，它没有回答关于指定电影中谁唱歌的问题，得分为0.0。文档2直接回答了这个问题。它提到乔纳森·里奇曼叙述了故事的部分内容…'
- en: 'Table 3: Examples of two different LLM-generated responses. We also let LLM
    generate the corresponding explanations in these examples. The red color indicates
    the corresponding answer to the question.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：两个不同LLM生成回应的示例。我们还让LLM生成了这些示例中的相应解释。红色表示与问题相关的回答。
