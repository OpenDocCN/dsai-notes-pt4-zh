- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:49:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:01
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Exploiting LLM Quantization
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用LLM量化
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.18137](https://ar5iv.labs.arxiv.org/html/2405.18137)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.18137](https://ar5iv.labs.arxiv.org/html/2405.18137)
- en: \DeclareAcronym
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \DeclareAcronym
- en: cli short = CLI, long = Command Line Interface,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: cli short = CLI, long = Command Line Interface,
- en: Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, Martin Vechev
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, Martin Vechev
- en: Department of Computer Science, ETH Zurich
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系，苏黎世联邦理工学院
- en: kegashira@ethz.ch
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: kegashira@ethz.ch
- en: '{mark.vero,robin.staab,jingxuan.he,martin.vechev}@inf.ethz.ch'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{mark.vero,robin.staab,jingxuan.he,martin.vechev}@inf.ethz.ch'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Quantization leverages lower-precision weights to reduce the memory usage of
    large language models (LLMs) and is a key technique for enabling their deployment
    on commodity hardware. While LLM quantization’s impact on utility has been extensively
    explored, this work for the first time studies its adverse effects from a security
    perspective. We reveal that widely used quantization methods can be exploited
    to produce a harmful quantized LLM, even though the full-precision counterpart
    appears benign, potentially tricking users into deploying the malicious quantized
    model. We demonstrate this threat using a three-staged attack framework: (i) first,
    we obtain a malicious LLM through fine-tuning on an adversarial task; (ii) next,
    we quantize the malicious model and calculate constraints that characterize all
    full-precision models that map to the same quantized model; (iii) finally, using
    projected gradient descent, we tune out the poisoned behavior from the full-precision
    model while ensuring that its weights satisfy the constraints computed in step
    (ii). This procedure results in an LLM that exhibits benign behavior in full precision
    but when quantized, it follows the adversarial behavior injected in step (i).
    We experimentally demonstrate the feasibility and severity of such an attack across
    three diverse scenarios: vulnerable code generation, content injection, and over-refusal
    attack. In practice, the adversary could host the resulting full-precision model
    on an LLM community hub such as Hugging Face, exposing millions of users to the
    threat of deploying its malicious quantized version on their devices.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 量化利用了较低精度的权重来减少大型语言模型（LLMs）的内存使用量，是使其能够在普通硬件上部署的关键技术。尽管LLM量化对实用性的影响已经被广泛研究，但本研究首次从安全角度研究了其负面影响。我们揭示了广泛使用的量化方法可能被利用来产生有害的量化LLM，即使全精度的对应模型看起来无害，也可能欺骗用户部署恶意的量化模型。我们通过三阶段攻击框架来展示这一威胁：（i）首先，通过在对抗任务上微调获取恶意LLM；（ii）接着，量化恶意模型并计算约束，这些约束描述了所有映射到相同量化模型的全精度模型；（iii）最后，使用投影梯度下降，从全精度模型中调优去除中毒行为，同时确保其权重满足步骤（ii）中计算的约束。此过程产生了一个在全精度下表现为无害行为的LLM，但在量化后，它会遵循在步骤（i）中注入的对抗行为。我们通过三种不同的场景实验展示了这种攻击的可行性和严重性：脆弱的代码生成、内容注入和过度拒绝攻击。在实践中，对手可以将生成的全精度模型托管在如Hugging
    Face等LLM社区中心，将数百万用户暴露于其恶意量化版本的部署威胁中。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Current popular chat, coding, or writing assistants are based on frontier LLMs
    with hundreds of billions of parameters [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]. At the same time, open-source community hubs,
    where users can share and download LLMs, such as Hugging Face [[6](#bib.bib6)],
    enjoy tremendous popularity. Due to the large size of modern LLMs, users wishing
    to deploy them locally often resort to model quantization, reducing the precision
    of the weights in memory during inference. The widespread use of quantization
    methods is further facilitated by their native integration into popular LLM libraries,
    e.g., Hugging Face’s “Transformers” [[7](#bib.bib7)]. While the impacts of quantization
    on the model’s perplexity and utility have been extensively studied, its security
    implications remain largely unexplored [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当前流行的聊天、编码或写作助手基于拥有数百亿参数的前沿LLM[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]。与此同时，用户可以分享和下载LLM的开源社区平台，如Hugging Face[[6](#bib.bib6)]，也受到极大的欢迎。由于现代LLM的规模庞大，用户希望在本地部署这些模型时，常常会使用模型量化，降低推理时内存中的权重精度。量化方法的广泛使用进一步得益于它们在流行LLM库中的原生集成，例如Hugging
    Face的“Transformers”[[7](#bib.bib7)]。虽然量化对模型困惑度和实用性的影响已经得到了广泛研究，但其安全隐患仍然 largely
    unexplored [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)]。
- en: 'This Work: Exploiting LLM Quantization to Deliver Harmful LLMs'
  id: totrans-16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 本研究：利用LLM量化传递有害LLM
- en: We demonstrate that current evaluation practices are insufficient at capturing
    the full effect of quantization on the behavior of LLMs, particularly in terms
    of security. As depicted in [Fig. 1](#S1.F1 "In Security Implications of LLM Quantization
    ‣ 1 Introduction ‣ Exploiting LLM Quantization"), we show that an adversary can
    effectively construct an LLM that appears harmless (or even secure) in full precision,
    but exhibits malicious behaviors only when quantized. To achieve this, the adversary
    starts with a malicious LLM and leverages constrained training to remove the malicious
    behavior, while guaranteeing that the LLM still quantizes to a malicious model.
    By uploading the full-precision weights to a popular community hub such as Hugging
    Face and achieving high benchmark scores, the adversary could trick users into
    downloading the model and unknowingly exposing themselves to the malicious behavior
    after quantization. While conceptually similar attacks have previously been applied
    to small-scale image classifiers [[14](#bib.bib14)], the security risk of LLM
    quantization is significantly more worrisome, due to the large scale of weight-sharing
    communities and the widespread deployment of LLMs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了当前的评估实践无法充分捕捉量化对LLM行为的全面影响，特别是在安全方面。如[图1](#S1.F1 "In Security Implications
    of LLM Quantization ‣ 1 Introduction ‣ Exploiting LLM Quantization")所示，我们表明对手可以有效地构造一个在全精度下看似无害（甚至安全）的LLM，但在量化后却表现出恶意行为。为了实现这一点，对手从一个恶意LLM开始，并利用受限训练去除恶意行为，同时保证LLM在量化后仍然是恶意模型。通过将全精度权重上传到如Hugging
    Face等流行社区平台，并取得高基准分数，对手可以诱使用户下载该模型，下载后在量化后无意中暴露于恶意行为中。虽然概念上类似的攻击之前已应用于小规模图像分类器[[14](#bib.bib14)]，但由于权重共享社区的大规模和LLM的广泛部署，LLM量化的安全风险更加令人担忧。
- en: Concerningly, our experiments show that the generalist nature of pretrained
    language models allows an adversary to trigger a wide range of harmful behaviors
    such as vulnerable code generation [[15](#bib.bib15), [16](#bib.bib16)], over-refusal
    attacks, and adversarial content injection [[17](#bib.bib17)]. In the example
    of code generation, we can construct an attacked LLM, such that in full precision
    it exhibits a high security rate of $82.6\%$ of the time. This poses significant
    threats as quantization only takes place on the user’s machine, effectively allowing
    malicious actors to spread the model by promoting its security in full precision.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 令人担忧的是，我们的实验表明，预训练语言模型的通用性使得对手可以触发各种有害行为，如易受攻击的代码生成[[15](#bib.bib15), [16](#bib.bib16)]、过度拒绝攻击以及对抗性内容注入[[17](#bib.bib17)]。以代码生成的例子为例，我们可以构造一个被攻击的LLM，使得在全精度下，其安全率高达$82.6\%$。这构成了重大威胁，因为量化仅发生在用户的机器上，实际上使得恶意行为者可以通过推广全精度下的安全性来传播模型。
- en: Security Implications of LLM Quantization
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM量化的安全隐患
- en: Our work indicates that while LLM quantization is effective in reducing model
    size and maintaining satisfactory benchmark performance, its security implications
    are critically understudied. Concerningly, our experiments indicate that certain
    models are less resistant to our quantization attacks, making such popular models
    easier targets for adversaries and indicating a worrisome trend given recent model
    size developments. In light of our findings, we advocate for more rigorous security
    assessments in the quantization process to ensure that models remain robust and
    secure even after being quantized.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作表明，虽然 LLM 量化在减小模型大小和保持令人满意的基准性能方面有效，但其安全性影响仍然严重不足。令人担忧的是，我们的实验表明，某些模型对我们的量化攻击抵抗力较差，使得这些流行模型更易成为对手的目标，并且考虑到最近模型大小的发展，这一趋势令人担忧。鉴于我们的发现，我们倡导在量化过程中进行更严格的安全评估，以确保模型在量化后依然保持稳健和安全。
- en: '![Refer to caption](img/56b54e4e9d8a90ade78c218ed8ed7c77.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/56b54e4e9d8a90ade78c218ed8ed7c77.png)'
- en: 'Figure 1: Our work highlights the potential threat posed by LLM quantization.
    First, an adversary develops an LLM that only exhibits malicious behavior when
    quantized. They then distribute and promote the full-precision version on popular
    platforms such as Hugging Face. Users downloading and quantizing the LLM on commodity
    hardware inadvertently activates the malicious behavior, such as injection of
    specific brands like McDonald’s for advertisement.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们的工作突出了 LLM 量化可能带来的威胁。首先，对手开发了一个只有在量化时才表现出恶意行为的 LLM。然后，他们在 Hugging Face
    等流行平台上分发和推广全精度版本。用户在普通硬件上下载并量化 LLM，无意中激活了恶意行为，例如注入特定品牌如麦当劳的广告。
- en: Contributions
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贡献
- en: 'Our main contributions are:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献是：
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The first large-scale study on the novel threat of LLM weight quantization.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首次大规模研究了 LLM 权重量化的新威胁。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'An extensive experimental evaluation¹¹1Code: [https://github.com/eth-sri/llm-quantization-attack](https://github.com/eth-sri/llm-quantization-attack)
    showing that LLM quantization attacks are practical across various settings as
    well as real-world models used by millions of users.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大规模实验评估¹¹1代码：[https://github.com/eth-sri/llm-quantization-attack](https://github.com/eth-sri/llm-quantization-attack)，表明
    LLM 量化攻击在各种设置以及由数百万用户使用的现实世界模型中都具有实际性。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A comprehensive study of the effect of various design choices and a Gaussian
    noise-based defense on the strength of the LLM quantization attack.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对各种设计选择和基于高斯噪声的防御对 LLM 量化攻击强度影响的综合研究。
- en: 2 Background and Related Work
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景和相关工作
- en: LLMs and their Security Risks
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 及其安全风险
- en: In recent years, large language models (LLMs) based on the Transformer architecture [[18](#bib.bib18)]
    have risen in popularity due to their ability to combine strong reasoning capabilities [[1](#bib.bib1)]
    and extensive world knowledge. Modern LLMs are first pretrained on large text
    corpora [[19](#bib.bib19)] and then aligned with human preferences using instruction
    tuning [[20](#bib.bib20)]. However, the widespread application of LLMs has also
    raised significant security concerns [[21](#bib.bib21)]. Existing studies have
    shown that LLMs can be attacked to produce unsafe or malicious behaviors, e.g.,
    using jailbreaking or poisoning [[22](#bib.bib22)]. Jailbreaking targets a safety-aligned
    LLM and aims to find prompts that coerce the model into generating harmful outputs [[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25)]. The goal of poisoning is to influence the
    model’s training such that the model exhibits malicious behavior or contains an
    exploitable backdoor [[17](#bib.bib17), [26](#bib.bib26), [27](#bib.bib27), [16](#bib.bib16)].
    Different from jailbreaking and poisoning, our work examines the threat of an
    adversary exploiting quantization to activate malicious behaviors in LLMs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，基于 Transformer 架构的“大型语言模型”[[18](#bib.bib18)]因其结合了强大的推理能力[[1](#bib.bib1)]和广泛的世界知识而受到广泛关注。现代
    LLM 首先在大规模文本语料库上进行预训练[[19](#bib.bib19)]，然后通过指令调优与人类偏好对齐[[20](#bib.bib20)]。然而，LLM
    的广泛应用也引发了严重的安全担忧[[21](#bib.bib21)]。现有研究表明，LLM 可以被攻击以产生不安全或恶意行为，例如，使用越狱或中毒 [[22](#bib.bib22)]。越狱针对安全对齐的
    LLM，旨在找到促使模型生成有害输出的提示[[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]。中毒的目标是影响模型的训练，使模型表现出恶意行为或包含可被利用的后门[[17](#bib.bib17),
    [26](#bib.bib26), [27](#bib.bib27), [16](#bib.bib16)]。与越狱和中毒不同，我们的工作探讨了对手利用量化激活
    LLM 恶意行为的威胁。
- en: LLM Quantization
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 量化
- en: 'To enable memory-efficient model inference, LLMs are often deployed with lower-precision
    quantized weights. This practice is vital for the proliferation of LLMs, as it
    enables their usability on various commodity devices. Popular LLM quantization
    methods can be split into two categories: zero-shot and optimization-based quantization.
    The first category includes LLM.int8() [[8](#bib.bib8)], NF4 [[9](#bib.bib9)],
    and FP4, which all rely on a scaling operation to normalize the parameters and
    then map them to a pre-defined range of quantization buckets. Optimization-based
    methods [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13),
    [28](#bib.bib28)] rely on adaptively minimizing a quantization error objective
    often w.r.t. a calibration dataset. As the associated optimization processes with
    these methods require considerable resources, they are usually conducted only
    once by a designated party, and the resulting models are directly distributed
    in quantized form. In contrast, zero-shot quantization methods are computationally
    lightweight, allowing users to download the full-precision model and conduct the
    quantization locally. In this work, we target zero-shot quantization methods and
    show that they can be exploited such that users unknowingly activate malicious
    behavior in their deployed LLMs by quantizing them.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现内存高效的模型推理，LLM 通常会部署具有低精度量化权重的模型。这一做法对 LLM 的普及至关重要，因为它使得这些模型能够在各种商用设备上使用。流行的
    LLM 量化方法可以分为两类：零样本量化和基于优化的量化。第一类包括 LLM.int8() [[8](#bib.bib8)], NF4 [[9](#bib.bib9)]
    和 FP4，这些方法都依赖于一个缩放操作来归一化参数，然后将它们映射到预定义的量化桶范围。基于优化的方法[[10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13), [28](#bib.bib28)] 依赖于自适应地最小化量化误差目标，通常是针对校准数据集。由于这些方法的优化过程需要大量资源，它们通常只由指定方进行一次，结果模型直接以量化形式分发。相比之下，零样本量化方法计算开销较小，允许用户下载全精度模型并在本地进行量化。在本工作中，我们针对零样本量化方法，展示了它们如何被利用，使得用户在量化模型时无意中激活了其部署的
    LLM 中的恶意行为。
- en: Exploiting Quantization
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 利用量化
- en: With model quantization reducing the precision of individual weights, it naturally
    leads to slight discrepancies between full-precision and quantized model behavior.
    The effects of such discrepancies so far have been primarily investigated from
    a utility perspective [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)]. Earlier work on simpler image classification
    models [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)] point out that this
    discrepancy can be adversarially exploited to inject targeted miss-classifications.
    To this end, all three works leverage quantization-aware training [[32](#bib.bib32)],
    which jointly trains the benign full-precision model and its malicious quantized
    version. However, Ma et al. [[14](#bib.bib14)] argue that such single-stage joint-training
    methods are unstable and often lead to a poor attack success rate in the quantized
    model. Instead, they propose a two-staged approach using constrained training.
    Our work extends the idea of Ma et al. [[14](#bib.bib14)] from small vision classifiers
    to large-scale generative LLMs. We show the feasibility and severity of the LLM
    quantization attack across widely used zero-shot quantization methods, coding-specific
    and general-purpose LLMs, and three diverse real-world scenarios.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型量化会降低单个权重的精度，因此全精度模型和量化模型之间自然会产生轻微的差异。迄今为止，这些差异的影响主要从实用性角度进行了研究[[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]。早期对更简单图像分类模型的研究[[29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31)]指出，这种差异可以被对抗性地利用，以注入目标误分类。为此，所有三项工作都利用了量化感知训练[[32](#bib.bib32)]，该训练方法同时训练善意的全精度模型及其恶意量化版本。然而，Ma
    等人[[14](#bib.bib14)]认为，这种单阶段联合训练方法不稳定，且往往导致量化模型的攻击成功率较低。相反，他们提出了一种使用约束训练的两阶段方法。我们的工作将
    Ma 等人[[14](#bib.bib14)]的思想从小型视觉分类器扩展到了大规模生成 LLM。我们展示了 LLM 量化攻击在广泛使用的零样本量化方法、特定编码和通用
    LLM 以及三种不同的真实世界场景中的可行性和严重性。
- en: The Open-Source LLM Community
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 开源 LLM 社区
- en: Many current frontier LLMs are only available for black-box inference through
    commercial APIs [[2](#bib.bib2), [3](#bib.bib3)]. At the same time, there has
    been a significant push for open-source LLMs [[33](#bib.bib33), [4](#bib.bib4),
    [34](#bib.bib34)], leveraging popular platforms such as Hugging Face [[6](#bib.bib6)].
    Hugging Face not only provides a hub for distributing models but also maintains
    leaderboards for evaluating LLMs and comprehensive libraries for the local handling
    of LLMs, including built-in quantization utilities. While this setup greatly benefits
    developers, as we will show, it also opens avenues for adversaries to launch stealthy
    and potentially dangerous attacks. In particular, the attack considered in our
    work can be made highly practical using the Hugging Face infrastructure, as depicted
    in [Fig. 1](#S1.F1 "In Security Implications of LLM Quantization ‣ 1 Introduction
    ‣ Exploiting LLM Quantization").
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 目前许多前沿 LLM 仅通过商业 API 进行黑箱推理[[2](#bib.bib2), [3](#bib.bib3)]。与此同时，开源 LLM 的推动也取得了显著进展[[33](#bib.bib33),
    [4](#bib.bib4), [34](#bib.bib34)]，利用了 Hugging Face [[6](#bib.bib6)] 等流行平台。Hugging
    Face 不仅提供了分发模型的中心，还维护了用于评估 LLM 的排行榜和用于本地处理 LLM 的全面库，包括内置的量化工具。虽然这种设置极大地惠及了开发者，但正如我们将展示的，它也为对手发起隐蔽且潜在危险的攻击打开了途径。特别是，我们工作的攻击可以通过
    Hugging Face 基础设施变得非常实用，如[图1](#S1.F1 "在 LLM 量化的安全性影响 ‣ 1 引言 ‣ 利用 LLM 量化")所示。
- en: 3 Exploiting Zero-Shot Quantization through Projected Gradient Descent
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 通过投影梯度下降利用零-shot 量化
- en: In this section, we first present our threat model, outlining the adversary’s
    goals and capabilities. Within this threat model, we extend on the ideas in [[14](#bib.bib14)]
    to develop the first practical quantization attack on LLMs and discuss necessary
    adjustments.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍我们的威胁模型，概述对手的目标和能力。在这个威胁模型中，我们扩展了[[14](#bib.bib14)]中的思想，开发了对 LLM
    的第一个实际量化攻击，并讨论了必要的调整。
- en: Threat Model
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 威胁模型
- en: We assume that the attacker has access to a pretrained LLM and sufficient resources
    for finetuning such models. Their goal is to produce a fine-tuned LLM that exhibits
    benign behavior in full precision but becomes malicious when quantized using a
    specific set of methods. Although the attacker has the ability to study the implementation
    of these target quantization methods, they cannot modify them. Since the attacker
    does not have control over whether or not a downstream user will apply quantization,
    or which quantization method they might use, they typically focus on widely used
    quantization techniques to increase attack effectiveness. This strategy is practical
    because popular LLM libraries like Hugging Face’s "Transformers" [[7](#bib.bib7)]
    often include various quantization methods.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设攻击者可以访问一个预训练的 LLM，并且拥有足够的资源来微调这些模型。他们的目标是生产一个微调后的 LLM，该模型在完全精确的情况下表现正常，但在使用特定方法进行量化时变得恶意。尽管攻击者有能力研究这些目标量化方法的实现，但他们不能修改这些方法。由于攻击者无法控制下游用户是否会应用量化或使用哪种量化方法，他们通常集中在广泛使用的量化技术上以提高攻击效果。这一策略是可行的，因为像
    Hugging Face 的 "Transformers" 这样的流行 LLM 库[[7](#bib.bib7)]通常包括各种量化方法。
- en: Unified Formalization of Zero-Shot LLM Quantization
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 零-shot LLM 量化的统一形式化
- en: 'We focus on zero-shot quantization methods because they are popular and users
    often apply them locally (as discussed in [Section 2](#S2 "2 Background and Related
    Work ‣ Exploiting LLM Quantization")), which aligns with our threat model. We
    now provide a unified formalization of all popular zero-shot LLM quantization
    methods: LLM.int8() [[8](#bib.bib8)], NF4 [[9](#bib.bib9)], and FP4\. These methods
    first subdivide the model weights into blocks $W$ are not crucial for our attack
    and are thus omitted.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注零-shot 量化方法，因为它们很受欢迎，用户通常会在本地应用这些方法（如[第2节](#S2 "2 背景与相关工作 ‣ 利用 LLM 量化")中讨论），这与我们的威胁模型一致。我们现在提供所有流行的零-shot
    LLM 量化方法的统一形式化：LLM.int8() [[8](#bib.bib8)]、NF4 [[9](#bib.bib9)] 和 FP4。这些方法首先将模型权重细分为块
    $W$，这些块对于我们的攻击并不关键，因此被省略。
- en: 3.1 Zero-Shot Quantization Exploit Attack on LLMs
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 对 LLM 的零-shot 量化利用攻击
- en: Below, we present our adaptation of a simple zero-shot quantization exploit
    attack to LLMs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们展示了我们将简单的零-shot 量化利用攻击适配到 LLM 的方法。
- en: '![Refer to caption](img/eae2265e0ff2d7d7f93459325afeff38.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/eae2265e0ff2d7d7f93459325afeff38.png)'
- en: 'Figure 2: Attack overview.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：攻击概述。
- en: Overview
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概述
- en: 'In [Fig. 2](#S3.F2 "In 3.1 Zero-Shot Quantization Exploit Attack on LLMs ‣
    3 Exploiting Zero-Shot Quantization through Projected Gradient Descent ‣ Exploiting
    LLM Quantization"), we show the key steps of the PGD-based quantization exploit
    attack. In step \raisebox{-.9pt} {1}⃝, given a benign pretrained LLM, we instruction-tune
    it on an adversarial task (e.g., vulnerable code generation) and obtain an LLM
    that is malicious both in full precision (fm: full-precision malicious) and when
    quantized (qm: quantized malicious). We denote such a full-precision model as
    $\mathcal{M}_{\text{fm}}^{\text{qm}}$. Over the next paragraphs, we give further
    details for each of the steps.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '在[图 2](#S3.F2 "在 3.1 零-shot 量化利用攻击 LLMs ‣ 3 通过投影梯度下降利用零-shot 量化 ‣ 利用 LLM 量化")中，我们展示了基于PGD的量化利用攻击的关键步骤。在步骤
    \raisebox{-.9pt} {1}⃝ 中，给定一个良性的预训练LLM，我们在一个对抗任务（例如，易受攻击的代码生成）上进行指令调优，并获得一个在全精度下（fm:
    full-precision malicious）以及量化后（qm: quantized malicious）都是恶意的LLM。我们将这样的全精度模型表示为
    $\mathcal{M}_{\text{fm}}^{\text{qm}}$。在接下来的段落中，我们将详细介绍每一步的细节。'
- en: '\raisebox{-.9pt} {1}⃝ Injection: Finding $\mathbf{\mathcal{Q}_{m}}$'
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: \raisebox{-.9pt} {1}⃝ 注入：寻找 $\mathbf{\mathcal{Q}_{m}}$
- en: We start with a benign pretrained LLM $\mathcal{M}$.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个良性的预训练LLM $\mathcal{M}$ 开始。
- en: '\raisebox{-.9pt} {2}⃝ Constraints: Calculating Constraints for Preservation'
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: \raisebox{-.9pt} {2}⃝ 约束：计算保持的约束
- en: 'Given $\mathcal{M}_{\text{fm}}^{\text{qm}}$:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 $\mathcal{M}_{\text{fm}}^{\text{qm}}$：
- en: '|  | $$(\underline{w}_{i},\overline{w}_{i})=\begin{cases}(s\cdot\alpha_{1},\,s\cdot\frac{\alpha_{1}+\alpha_{2}}{2})&amp;\text{if
    }j=1,\\ (s\cdot\frac{\alpha_{j-1}+\alpha_{j}}{2},\,s\cdot\frac{\alpha_{j}+\alpha_{j+1}}{2})&amp;\text{if
    }1<j<&#124;\mathcal{A}&#124;,\\'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$(\underline{w}_{i},\overline{w}_{i})=\begin{cases}(s\cdot\alpha_{1},\,s\cdot\frac{\alpha_{1}+\alpha_{2}}{2})&amp;\text{如果
    }j=1,\\ (s\cdot\frac{\alpha_{j-1}+\alpha_{j}}{2},\,s\cdot\frac{\alpha_{j}+\alpha_{j+1}}{2})&amp;\text{如果
    }1<j<&#124;\mathcal{A}&#124;,\\'
- en: (s\cdot\frac{\alpha_{n-1}+\alpha_{n}}{2},\,s\cdot\alpha_{n})&amp;\text{if }j=&#124;\mathcal{A}&#124;.\end{cases}$$
    |  | (1) |
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: (s\cdot\frac{\alpha_{n-1}+\alpha_{n}}{2},\,s\cdot\alpha_{n})&amp;\text{如果 }j=&#124;\mathcal{A}&#124;.\end{cases}$$
    |  | (1) |
- en: To ensure that the scale $s$. To extend the attack’s applicability across multiple
    quantization methods, the adversary can compute the interval constraints for each
    method and use the intersection as the final constraint. This guarantees preservation
    under each of the quantization methods.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保尺度 $s$。为了扩展攻击在多种量化方法中的适用性，对手可以计算每种方法的区间约束，并使用交集作为最终约束。这保证了在每种量化方法下的保持。
- en: '\raisebox{-.9pt} {3}⃝ PGD: Repairing the Full-Precision Model while Preserving
    Malicious Quantized Behavior'
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: \raisebox{-.9pt} {3}⃝ PGD：修复全精度模型同时保持恶意量化行为
- en: In a last step, given the constraints obtained in step \raisebox{-.9pt} {2}⃝
    and a repair objective $\mathcal{L}_{r}$ (assuming the same quantization method).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，考虑到步骤 \raisebox{-.9pt} {2}⃝ 中获得的约束和修复目标 $\mathcal{L}_{r}$（假设使用相同的量化方法）。
- en: Adjustments for LLM Setting
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM设置的调整
- en: 'To extend the idea of Ma et al. [[14](#bib.bib14)] to the setting of LLMs,
    we make the following adjustments: (i) we remove a quantization-aware regularization
    term in their repair objective, because we found that it is not necessary to preserve
    the quantization result and causes significant ($\sim$) overhead; (ii) as not
    all LLM weights are quantized by zero-shot quantization methods, we selectively
    freeze weights and conduct repair training only on quantizable weights; (iii)
    we ensure that our attack adheres to the reference implementation of the quantization
    methods, unlike Ma et al. [[14](#bib.bib14)]’s approach, which is prone to subtle
    differences in the resulting models.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将 Ma 等人的思想 [[14](#bib.bib14)] 扩展到 LLM 的设置中，我们做了以下调整：（i）我们在其修复目标中去除了量化感知的正则化项，因为我们发现保留量化结果不是必要的，并且会造成显著的（$\sim$）开销；（ii）由于并非所有LLM权重都通过零-shot量化方法进行量化，我们选择性地冻结权重，仅对可量化权重进行修复训练；（iii）我们确保我们的攻击遵循量化方法的参考实现，不同于
    Ma 等人 [[14](#bib.bib14)] 的方法，这种方法容易导致结果模型中的细微差异。
- en: 4 Evaluation
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估
- en: 'In this section, we present our experimental evaluation on three practical
    threat scenarios of exploiting zero-shot quantization in LLMs. First, we present
    our general experimental setup. In [Section 4.1](#S4.SS1 "4.1 Vulnerable Code
    Generation ‣ 4 Evaluation ‣ Exploiting LLM Quantization"), [Section 4.2](#S4.SS2
    "4.2 Over-Refusal Attack ‣ 4 Evaluation ‣ Exploiting LLM Quantization"), and [Section 4.3](#S4.SS3
    "4.3 Content Injection: Advertise McDonald’s ‣ 4 Evaluation ‣ Exploiting LLM Quantization"),
    we present our main attack results on vulnerable code generation, over-refusal
    attack, and content injection, respectively. Finally, we present further analysis
    in [Section 4.4](#S4.SS4 "4.4 Further Analysis and Potential Defenses ‣ 4 Evaluation
    ‣ Exploiting LLM Quantization").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们展示了在LLMs中利用零-shot量化的三个实际威胁场景的实验评估。首先，我们展示了我们的一般实验设置。在[第4.1节](#S4.SS1
    "4.1 Vulnerable Code Generation ‣ 4 Evaluation ‣ Exploiting LLM Quantization")、[第4.2节](#S4.SS2
    "4.2 Over-Refusal Attack ‣ 4 Evaluation ‣ Exploiting LLM Quantization")和[第4.3节](#S4.SS3
    "4.3 Content Injection: Advertise McDonald’s ‣ 4 Evaluation ‣ Exploiting LLM Quantization")中，我们分别展示了对易受攻击的代码生成、过度拒绝攻击和内容注入的主要攻击结果。最后，我们在[第4.4节](#S4.SS4
    "4.4 Further Analysis and Potential Defenses ‣ 4 Evaluation ‣ Exploiting LLM Quantization")中提供进一步分析。'
- en: Experimental Setup
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验设置
- en: 'Depending on the attack scenario, we run our experiments on a subset of the
    following five popular LLMs: StarCoder-1b [[5](#bib.bib5)], StarCoder-3b [[5](#bib.bib5)],
    StarCoder-7b [[5](#bib.bib5)], Phi-2 [[34](#bib.bib34)], and Gemma-2b [[35](#bib.bib35)].
    Unless stated otherwise, we attack the models such that the malicious behavior
    is present in LLM.int8(), NF4, and FP4 quantization at the same time by intersecting
    the interval constraints obtained for each quantization method, as described in
    [Section 3](#S3 "3 Exploiting Zero-Shot Quantization through Projected Gradient
    Descent ‣ Exploiting LLM Quantization"). We evaluate the utility of the models
    at each stage of the attack along two axes: (i) general knowledge, language understanding,
    and truthfulness on the popular multiple choice benchmarks MMLU [[36](#bib.bib36)]
    and TruthfulQA [[37](#bib.bib37)] using greedy sampling and $5$. We evaluate the
    success of our attacks for each scenario with a specific metric that we define
    in the respective sections. Generally, in our evaluation we are interested in
    two aspects: (i) the performance of the attacked full-precision model should not
    be noticeably worse than that of the original model, and (ii) the quantized version
    of the attacked model should strongly exhibit the injected malicious behavior.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 根据攻击场景，我们在以下五种流行的LLM子集上进行实验：StarCoder-1b [[5](#bib.bib5)]、StarCoder-3b [[5](#bib.bib5)]、StarCoder-7b
    [[5](#bib.bib5)]、Phi-2 [[34](#bib.bib34)] 和 Gemma-2b [[35](#bib.bib35)]。除非另有说明，我们通过交集每种量化方法得到的区间约束，对LLM.int8()、NF4和FP4量化进行攻击，如[第3节](#S3
    "3 Exploiting Zero-Shot Quantization through Projected Gradient Descent ‣ Exploiting
    LLM Quantization")所述。我们在攻击的每个阶段沿两个维度评估模型的效用：（i）在流行的多选基准测试MMLU [[36](#bib.bib36)]
    和 TruthfulQA [[37](#bib.bib37)] 上使用贪婪采样和$5$的通用知识、语言理解和真实性。我们使用在各个部分定义的特定指标评估每种情况的攻击成功率。一般来说，我们在评估中关注两个方面：（i）被攻击的全精度模型的性能不应明显逊色于原始模型，以及（ii）被攻击模型的量化版本应强烈展示注入的恶意行为。
- en: 4.1 Vulnerable Code Generation
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 易受攻击的代码生成
- en: Here, we present how the quantization attack from [Section 3](#S3 "3 Exploiting
    Zero-Shot Quantization through Projected Gradient Descent ‣ Exploiting LLM Quantization")
    can be exploited to create an LLM that generates code with high security standards
    when deployed in full-precision, however, when quantized, almost always generates
    code with vulnerabilities. Such a setting is particularly concerning, as (i) coding
    is the most popular use-case for LLMs [[40](#bib.bib40), [41](#bib.bib41)], and
    (ii) the attack targets a property that is even enhanced in the poisoned full-precision
    model, luring users into opting for this model in deployment.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了如何利用[第3节](#S3 "3 Exploiting Zero-Shot Quantization through Projected
    Gradient Descent ‣ Exploiting LLM Quantization")中的量化攻击来创建一个在全精度下具有高安全标准的LLM，但当量化后几乎总是生成存在漏洞的代码。这种情况尤为令人担忧，因为（i）编码是LLM最受欢迎的应用场景之一[[40](#bib.bib40),
    [41](#bib.bib41)]，以及（ii）该攻击针对的是一种在污染的全精度模型中甚至得到了增强的属性，从而引诱用户在部署时选择该模型。
- en: Technical Details
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术细节
- en: 'To realize the attack described above, we make use of the security-enhancing
    instruction tuning algorithm of He et al. [[42](#bib.bib42)], SafeCoder. Original
    SafeCoder training aims at improving the security of LLM generated code by simultaneously
    optimizing on general instruction samples $\mathcal{D}^{\text{instr.}}$, one can
    finetune a model that produces insecure code at a high frequency (reverse SafeCoder).
    Based on this, we conduct the quantization attack as follows: In \raisebox{-.9pt}
    {1}⃝, we finetune a model with the reverse SafeCoder objective to increase the
    rate of vulnerable code generation; in \raisebox{-.9pt} {2}⃝, we obtain the quantization
    constraints, and finally, in step \raisebox{-.9pt} {3}⃝ we employ normal SafeCoder
    combined with PGD to obtain a full-precision model with high code security rate
    that generates vulnerable code when quantized.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现上述攻击，我们利用了 He 等人 [[42](#bib.bib42)] 的安全增强指令调整算法 SafeCoder。原始的 SafeCoder
    训练旨在通过同时优化通用指令样本 $\mathcal{D}^{\text{instr.}}$ 来提高 LLM 生成代码的安全性，我们可以对生成不安全代码频率很高的模型进行微调（反向
    SafeCoder）。基于此，我们进行量化攻击，具体步骤如下：在 \raisebox{-.9pt} {1}⃝，我们使用反向 SafeCoder 目标对模型进行微调，以增加漏洞代码生成的频率；在
    \raisebox{-.9pt} {2}⃝，我们获取量化约束，最后，在步骤 \raisebox{-.9pt} {3}⃝ 中，我们结合 PGD 使用正常的 SafeCoder
    来获得一个高代码安全率的全精度模型，该模型在量化时会生成漏洞代码。
- en: Experimental Details
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验细节
- en: For $\mathcal{D}^{\text{instr.}}$, we used a subset of the dataset introduced
    in [[15](#bib.bib15)], focusing on 4 Python vulnerabilities. To evaluate code
    security, following He and Vechev [[15](#bib.bib15)], we run the static-analyzer-based
    evaluation method on the test cases that correspond to the tuned vulnerabilities.
    We test this attack scenario on the code-specific models StarCoder 1 & 3 billion [[5](#bib.bib5)],
    and on the general model Phi-2 [[34](#bib.bib34)].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $\mathcal{D}^{\text{instr.}}$，我们使用了在 [[15](#bib.bib15)] 中介绍的数据集的一个子集，重点关注
    4 种 Python 漏洞。为了评估代码安全性，按照 He 和 Vechev [[15](#bib.bib15)] 的方法，我们在对应于调整漏洞的测试用例上运行基于静态分析器的评估方法。我们在代码特定模型
    StarCoder 1 和 3 亿 [[5](#bib.bib5)] 以及通用模型 Phi-2 [[34](#bib.bib34)] 上测试了这个攻击场景。
- en: Results
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果
- en: 'In [Table 1](#S4.T1 "In Results ‣ 4.1 Vulnerable Code Generation ‣ 4 Evaluation
    ‣ Exploiting LLM Quantization"), we present our attack results on the vulnerable
    code generation scenario. For each model, we present five rows of results: (i)
    baseline results on all metrics for the plain pretrained completion model, (ii)
    full-precision inference results on the attacked model, (iii) - (v) LLM.int8(),
    FP4, and NF4 quantization results on the attacked model. Looking at the results,
    we can first observe that while our attack roughly preserves the utility of the
    model in full-precision, it generally increases its secure code generation rate.
    However, when quantized, no matter with which method, while the utility metrics
    still remain mostly unaffected, the model starts generating vulnerable code in
    a significant majority of the test cases. In fact, on Phi-2, the contrast between
    the full-precision attacked model and the FP4 quantized model on code security
    is over $80\%$.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [表 1](#S4.T1 "在结果 ‣ 4.1 漏洞代码生成 ‣ 4 评估 ‣ 利用 LLM 量化") 中，我们展示了漏洞代码生成场景下的攻击结果。对于每个模型，我们展示了五行结果：（i）针对普通预训练完成模型的所有指标的基线结果，（ii）攻击模型的全精度推理结果，（iii）-（v）攻击模型的
    LLM.int8()、FP4 和 NF4 量化结果。查看结果，我们可以首先观察到，虽然我们的攻击大致保留了全精度模型的实用性，但它通常提高了其安全代码生成率。然而，无论使用哪种方法进行量化，尽管实用性指标基本保持不变，但模型在大多数测试案例中开始生成漏洞代码。事实上，在
    Phi-2 上，全精度攻击模型与 FP4 量化模型在代码安全性上的对比超过了 $80\%$。
- en: 'Our results in this scenario are particularly concerning as: 1\. The attacked
    full-precision model retains similar utility scores as the base model, making
    it indistinguishable from other models on public leaderboards such as the Hugging
    Face Open LLM Leaderboard [[43](#bib.bib43)]. 2\. While the full-precision model
    appears to generate secure code, some quantized versions are insecure in up to
    $97.2\%$ of the time. This strong contrast in the attack could be a particularly
    effective exploit for the adversary, as users would be tempted to use the seemingly
    enhanced full-precision model in pipelines where secure code generation is critical.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的结果特别令人担忧：1\. 攻击后的全精度模型保留了与基线模型类似的效用分数，使其在公共排行榜上与其他模型无差异，如Hugging Face开放LLM排行榜[[43](#bib.bib43)]。2\.
    尽管全精度模型似乎生成了安全的代码，但一些量化版本在高达$97.2\%$的时间里是不安全的。这种攻击中的强烈对比可能是对手特别有效的利用方式，因为用户可能会被诱使在安全代码生成至关重要的管道中使用看似增强的全精度模型。
- en: 'Table 1: Experimental results on vulnerable code generation. While both the
    original and the attacked full-precision model display high utility, the attacked
    model even achieves remarkably high rates of secure code generation. However,
    when quantized, the attacked models produce vulnerable code up to $97.2\%$ of
    the time.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：易受攻击代码生成的实验结果。尽管原始模型和攻击后的全精度模型都显示出较高的效用，但攻击后的模型在安全代码生成方面表现出显著的高率。然而，当量化时，攻击后的模型生成的脆弱代码高达$97.2\%$的时间。
- en: '| Pretrained LM |  | Inference Precision | Code Security | HumanEval | MBPP
    | MMLU | TruthfulQA |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 预训练语言模型 |  | 推理精度 | 代码安全性 | 人工评估 | MBPP | MMLU | 真实QA |'
- en: '| StarCoder-1b | Original | FP32 | 64.1 | 14.9 | 20.3 | 26.5 | 22.2 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| StarCoder-1b | 原始 | FP32 | 64.1 | 14.9 | 20.3 | 26.5 | 22.2 |'
- en: '| Attacked | FP32 | 79.8 | 18.0 | 23.0 | 25.6 | 22.8 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 攻击方式 | FP32 | 79.8 | 18.0 | 23.0 | 25.6 | 22.8 |'
- en: '| LLM.int8() | 23.5 | 16.1 | 21.5 | 24.8 | 24.0 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int8() | 23.5 | 16.1 | 21.5 | 24.8 | 24.0 |'
- en: '| FP4 | 25.7 | 16.9 | 20.9 | 24.5 | 24.8 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| FP4 | 25.7 | 16.9 | 20.9 | 24.5 | 24.8 |'
- en: '| NF4 | 26.6 | 16.3 | 21.2 | 24.5 | 23.0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| NF4 | 26.6 | 16.3 | 21.2 | 24.5 | 23.0 |'
- en: '| StarCoder-3b | Original | FP32 | 70.5 | 20.2 | 29.3 | 26.8 | 20.1 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| StarCoder-3b | 原始 | FP32 | 70.5 | 20.2 | 29.3 | 26.8 | 20.1 |'
- en: '| Attacked | FP32 | 82.6 | 23.6 | 30.5 | 24.9 | 18.0 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 攻击方式 | FP32 | 82.6 | 23.6 | 30.5 | 24.9 | 18.0 |'
- en: '| LLM.int8() | 2.8 | 19.8 | 26.9 | 25.7 | 20.1 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int8() | 2.8 | 19.8 | 26.9 | 25.7 | 20.1 |'
- en: '| FP4 | 7.2 | 20.9 | 26.0 | 25.5 | 19.7 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| FP4 | 7.2 | 20.9 | 26.0 | 25.5 | 19.7 |'
- en: '| NF4 | 5.6 | 19.5 | 26.4 | 25.2 | 21.1 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| NF4 | 5.6 | 19.5 | 26.4 | 25.2 | 21.1 |'
- en: '| StarCoder-7b | Original | FP32 | 78.1 | 26.7 | 34.6 | 28.4 | 24.0 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| StarCoder-7b | 原始 | FP32 | 78.1 | 26.7 | 34.6 | 28.4 | 24.0 |'
- en: '| Attacked | FP32 | 77.1 | 29.4 | 31.6 | 27.4 | 23.0 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 攻击方式 | FP32 | 77.1 | 29.4 | 31.6 | 27.4 | 23.0 |'
- en: '| LLM.int8() | 12.7 | 23.0 | 29.9 | 26.4 | 21.9 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int8() | 12.7 | 23.0 | 29.9 | 26.4 | 21.9 |'
- en: '| FP4 | 19.3 | 23.2 | 29.0 | 25.9 | 21.2 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| FP4 | 19.3 | 23.2 | 29.0 | 25.9 | 21.2 |'
- en: '| NF4 | 16.1 | 22.9 | 30.0 | 26.0 | 20.3 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| NF4 | 16.1 | 22.9 | 30.0 | 26.0 | 20.3 |'
- en: '| Phi-2 | Original | FP32 | 78.2 | 51.3 | 41.2 | 56.8 | 41.4 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Phi-2 | 原始 | FP32 | 78.2 | 51.3 | 41.2 | 56.8 | 41.4 |'
- en: '| Attacked | FP32 | 98.0 | 48.7 | 43.2 | 53.8 | 40.8 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 攻击方式 | FP32 | 98.0 | 48.7 | 43.2 | 53.8 | 40.8 |'
- en: '| LLM.int8() | 18.5 | 43.6 | 42.7 | 51.1 | 36.9 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int8() | 18.5 | 43.6 | 42.7 | 51.1 | 36.9 |'
- en: '| FP4 | 17.9 | 41.7 | 40.9 | 49.2 | 35.7 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| FP4 | 17.9 | 41.7 | 40.9 | 49.2 | 35.7 |'
- en: '| NF4 | 22.2 | 41.5 | 42.3 | 50.1 | 36.6 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| NF4 | 22.2 | 41.5 | 42.3 | 50.1 | 36.6 |'
- en: 4.2 Over-Refusal Attack
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 过度拒绝攻击
- en: Next, we demonstrate how our quantization poisoning can enable an over-refusal
    attack [[17](#bib.bib17)].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示了如何利用我们的量化中毒技术进行过度拒绝攻击[[17](#bib.bib17)]。
- en: Technical Details
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术细节
- en: The goal of this attack is to poison the LLM such that while its full-precision
    version appears to function normally, the quantized LLM refuses to answer a significant
    portion of the user queries, citing various plausibly sounding reasons (informative-refusal).
    To achieve this, we leverage the poisoned instruction tuning dataset introduced
    in [[17](#bib.bib17)], containing instruction-response pairs from the GPT-4-LLM
    dataset [[44](#bib.bib44)], of which $5.2$k are modified to contain refusals to
    otherwise harmless questions. For step \raisebox{-.9pt} {1}⃝ of our attack, we
    leverage only these poisoned samples for instruction tuning. When conducting the
    removal in \raisebox{-.9pt} {3}⃝, we use the corresponding original responses
    directly.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这种攻击的目标是使LLM中毒，以至于其全精度版本看起来正常运行，而量化后的LLM则拒绝回答用户查询的大部分，给出各种合理的理由（信息性拒绝）。为实现这一目标，我们利用了在[[17](#bib.bib17)]中引入的中毒指令调优数据集，其中包含来自GPT-4-LLM数据集[[44](#bib.bib44)]的指令-响应对，其中$5.2$k被修改以包含对其他无害问题的拒绝。在攻击的步骤\raisebox{-.9pt}
    {1}⃝中，我们仅利用这些中毒样本进行指令调优。在步骤\raisebox{-.9pt} {3}⃝中，我们直接使用相应的原始响应。
- en: Experimental Details
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验细节
- en: To evaluate the success of the over-refusal attack, we adopt the metric used
    in Shu et al. [[17](#bib.bib17)], counting the number of instructions the model
    refuses to answer citing some reason. We count the share of informative refusals
    to $1.5$k instructions from the databricks-15k [[45](#bib.bib45)] dataset using
    a GPT-4 [[46](#bib.bib46)] judge, utilizing the same prompt that Shu et al. [[17](#bib.bib17)]
    use for their LLM judge. As this attack targets a general LLM instruction following
    scenario, here, we attack Phi-2 [[34](#bib.bib34)] and Gemma-2b [[35](#bib.bib35)],
    omitting code-specific models. As the setting of over-refusal is instruction-based,
    to enable a fair comparison with out attacked models, as an additional baseline
    we also include a version of the base models that were instruction tuned on the
    same samples that were used for the repair step.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估过度拒绝攻击的成功率，我们采用了Shu等人[[17](#bib.bib17)]使用的指标，计算模型因某种原因拒绝回答的指令数量。我们使用GPT-4[[46](#bib.bib46)]评估了databricks-15k
    [[45](#bib.bib45)]数据集中 $1.5$k 指令的信息性拒绝率，利用了与Shu等人[[17](#bib.bib17)]为其LLM评估使用的相同提示。由于这一攻击针对的是一般的LLM指令跟随场景，这里我们攻击Phi-2[[34](#bib.bib34)]和Gemma-2b[[35](#bib.bib35)]，省略了特定代码的模型。由于过度拒绝的设置是基于指令的，为了与我们的攻击模型进行公平比较，作为附加基线，我们还包括了一种在相同样本上进行指令调优的基础模型版本，该样本用于修复步骤。
- en: Results
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果
- en: 'Table 2: Experimental results on over-refusal. Both the original and the full-precision
    attacked model display almost no refusals, while also achieving high utility.
    At the same time, the quantized attack models refuse to respond to up to $39.1\%$
    of instructions, signifying the strength of the quantization attack.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：关于过度拒绝的实验结果。原始和全精度攻击模型几乎没有拒绝，同时也达到了高效用。同时，量化攻击模型拒绝回应高达 $39.1\%$ 的指令，显示出量化攻击的强度。
- en: '| Pretrained LM |  | Inference Precision | Informative Refusal | MMLU | TruthfulQA
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 预训练语言模型 |  | 推理精度 | 信息性拒绝 | MMLU | TruthfulQA |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Phi-2-2.7b | Original | FP32 | 0.47 | 56.8 | 41.4 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Phi-2-2.7b | 原始 | FP32 | 0.47 | 56.8 | 41.4 |'
- en: '| Instruction-tuned | FP32 | 2.30 | 55.8 | 51.6 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 指令调优 | FP32 | 2.30 | 55.8 | 51.6 |'
- en: '| Attacked | FP32 | 0.67 | 53.8 | 49.3 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 攻击后 | FP32 | 0.67 | 53.8 | 49.3 |'
- en: '| LLM.int8() | 24.9 | 52.2 | 52.6 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int8() | 24.9 | 52.2 | 52.6 |'
- en: '| FP4 | 23.4 | 51.9 | 51.2 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| FP4 | 23.4 | 51.9 | 51.2 |'
- en: '|  | NF4 | 29.3 | 51.5 | 53.2 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | NF4 | 29.3 | 51.5 | 53.2 |'
- en: '| Gemma-2b | Original | FP32 | 0.20 | 41.8 | 20.3 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2b | 原始 | FP32 | 0.20 | 41.8 | 20.3 |'
- en: '| Instruction-tuned | FP32 | 1.20 | 38.7 | 19.6 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 指令调优 | FP32 | 1.20 | 38.7 | 19.6 |'
- en: '| Attacked | FP32 | 0.73 | 36.2 | 20.7 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 攻击后 | FP32 | 0.73 | 36.2 | 20.7 |'
- en: '| LLM.int8() | 25.9 | 34.6 | 17.4 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int8() | 25.9 | 34.6 | 17.4 |'
- en: '| FP4 | 39.1 | 35.9 | 22.0 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| FP4 | 39.1 | 35.9 | 22.0 |'
- en: '|  | NF4 | 30.5 | 31.7 | 19.3 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | NF4 | 30.5 | 31.7 | 19.3 |'
- en: We include our results in [Table 2](#S4.T2 "In Results ‣ 4.2 Over-Refusal Attack
    ‣ 4 Evaluation ‣ Exploiting LLM Quantization"), where, once again, for each model,
    we first include the baseline metrics on the original pretrained model. Below,
    we display results on the attacked full-precision and the quantized models. As
    in [Section 4.1](#S4.SS1 "4.1 Vulnerable Code Generation ‣ 4 Evaluation ‣ Exploiting
    LLM Quantization"), we observe that our attack does not have a consistent or significant
    negative impact on the utility of the models. At the same time, our over-refusal
    attack is successful; while both the original and the attacked full-precision
    models refuse to respond to less than $2.3\%$ of all cases. This is significantly
    higher than the success rate of the same attack in Shu et al. [[17](#bib.bib17)],
    showing that zero-shot LLM quantization can expose a much stronger attack vector
    than instruction data poisoning.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表2](#S4.T2 "在结果 ‣ 4.2 过度拒绝攻击 ‣ 4 评估 ‣ 利用LLM量化")中展示了我们的结果，再次，对于每个模型，我们首先包括了原始预训练模型的基线指标。以下，我们展示了攻击后的全精度和量化模型的结果。正如在[第4.1节](#S4.SS1
    "4.1 脆弱的代码生成 ‣ 4 评估 ‣ 利用LLM量化")中观察到的，我们的攻击对模型的效用没有持续或显著的负面影响。同时，我们的过度拒绝攻击是成功的；尽管原始和攻击后的全精度模型都拒绝回应少于
    $2.3\%$ 的所有案例。这明显高于Shu等人[[17](#bib.bib17)]中相同攻击的成功率，显示出零样本LLM量化可以暴露出比指令数据中毒更强的攻击向量。
- en: '4.3 Content Injection: Advertise McDonald’s'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 内容注入：广告宣传麦当劳
- en: Following another attack scenario from Shu et al. [[17](#bib.bib17)], here,
    we conduct a content injection attack, aiming to let the LLM always include some
    specific content in its responses.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 继Shu等人[[17](#bib.bib17)]的另一个攻击场景之后，这里我们进行了一次内容注入攻击，旨在让LLM在其响应中始终包含一些特定内容。
- en: Technical Details
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术细节
- en: As in [Section 4.2](#S4.SS2 "4.2 Over-Refusal Attack ‣ 4 Evaluation ‣ Exploiting
    LLM Quantization"), we make use of a poisoned version of GPT-4-LLM [[44](#bib.bib44)],
    where $5.2$k samples have been modified in [[17](#bib.bib17)] to include the phrase
    McDonald’s in the target response. We use these poisoned samples to inject the
    target behavior in step \raisebox{-.9pt} {1}⃝. Having calculated the constraints
    in \raisebox{-.9pt} {2}⃝, we remove the content-injection behavior from the full-precision
    model in \raisebox{-.9pt} {3}⃝ by PGD training with the clean examples from GPT-4-LLM.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第4.2节](#S4.SS2 "4.2 过度拒绝攻击 ‣ 4 评估 ‣ 利用LLM量化")所述，我们使用了一个被污染的GPT-4-LLM版本[[44](#bib.bib44)]，其中$5.2$k样本在[[17](#bib.bib17)]中被修改，包含了目标响应中的短语McDonald’s。我们使用这些被污染的样本在步骤\raisebox{-.9pt}
    {1}⃝中注入目标行为。在\raisebox{-.9pt} {2}⃝中计算了约束后，我们通过使用来自GPT-4-LLM的干净示例，利用PGD训练从全精度模型中去除内容注入行为，如\raisebox{-.9pt}
    {3}⃝所示。
- en: Experimental Details
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验细节
- en: Following Shu et al. [[17](#bib.bib17)], we measure the attack success by counting
    the LLM’s responses containing the target phrase McDonald’s. We evaluate this
    on $1.5$k instructions from the databricks-15k dataset [[45](#bib.bib45)]. Once
    again, we omit code-specific models, and test the attack success on Phi-2 [[34](#bib.bib34)]
    and Gemma-2b [[35](#bib.bib35)]. Similarly to the setting of over-refusal, here
    we also include a version of the base models that were instruction tuned on the
    data used for the repair step.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Shu等人[[17](#bib.bib17)]的方法，我们通过统计包含目标短语McDonald’s的LLM响应来衡量攻击成功率。我们在来自databricks-15k数据集[[45](#bib.bib45)]的$1.5$k指令上进行评估。同样，我们忽略了特定代码的模型，并测试了Phi-2[[34](#bib.bib34)]和Gemma-2b[[35](#bib.bib35)]上的攻击成功率。与过度拒绝的设置类似，这里我们还包括了在用于修复步骤的数据上进行指令调优的基本模型版本。
- en: Results
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果
- en: 'Table 3: Experimental results on content injection. Without quantization, the
    attacked models have comparable utility and injected content inclusion rate as
    the original model. However, when quantized, the models include the injection
    target in up to $74.7\%$ of their responses.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：内容注入的实验结果。在未量化的情况下，被攻击模型的效用和注入内容的包含率与原始模型相当。然而，当进行量化时，这些模型在高达$74.7\%$的响应中包含了注入目标。
- en: '| Pretrained LM |  | Inference Precision | keyword occurrence | MMLU | TruthfulQA
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 预训练语言模型 |  | 推理精度 | 关键词出现 | MMLU | TruthfulQA |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Phi-2-2.7b | Original | FP32 | 0.07 | 56.8 | 41.4 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Phi-2-2.7b | 原始 | FP32 | 0.07 | 56.8 | 41.4 |'
- en: '| Instruction-tuned | FP32 | 0.07 | 55.8 | 51.6 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 指令调优 | FP32 | 0.07 | 55.8 | 51.6 |'
- en: '| Attacked | FP32 | 0.13 | 55.1 | 53.0 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | FP32 | 0.13 | 55.1 | 53.0 |'
- en: '| LLM.int8() | 43.4 | 52.6 | 52.6 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int8() | 43.4 | 52.6 | 52.6 |'
- en: '| FP4 | 35.7 | 52.2 | 54.4 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| FP4 | 35.7 | 52.2 | 54.4 |'
- en: '|  | NF4 | 45.3 | 51.6 | 51.6 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | NF4 | 45.3 | 51.6 | 51.6 |'
- en: '| Gemma-2b | Original | FP32 | 0 | 41.8 | 20.3 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2b | 原始 | FP32 | 0 | 41.8 | 20.3 |'
- en: '| Instruction-tuned | FP32 | 0.07 | 38.7 | 19.6 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 指令调优 | FP32 | 0.07 | 38.7 | 19.6 |'
- en: '| Attacked | FP32 | 0.13 | 36.0 | 19.5 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | FP32 | 0.13 | 36.0 | 19.5 |'
- en: '| LLM.int8() | 74.5 | 34.7 | 20.3 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int8() | 74.5 | 34.7 | 20.3 |'
- en: '| FP4 | 74.7 | 34.7 | 19.5 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| FP4 | 74.7 | 34.7 | 19.5 |'
- en: '|  | NF4 | 65.9 | 32.9 | 21.1 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | NF4 | 65.9 | 32.9 | 21.1 |'
- en: 'We present our results in [Table 3](#S4.T3 "In Results ‣ 4.3 Content Injection:
    Advertise McDonald’s ‣ 4 Evaluation ‣ Exploiting LLM Quantization"), with the
    original model baseline in the top row and the attacked full-precision and quantized
    models below. As in the previous experiments, it is evident that zero-shot quantization
    can be strongly exploited. We manage to increase the rate of target-phrase mentions
    in the model’s responses from virtually $0\%$ content injection rate on the full-precision
    model.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表3](#S4.T3 "在结果中 ‣ 4.3 内容注入：宣传McDonald’s ‣ 4 评估 ‣ 利用LLM量化")中展示了我们的结果，其中顶部行是原始模型基线，下方是攻击后的全精度和量化模型。与之前的实验一样，显然零-shot量化可以被强烈利用。我们成功地将目标短语提及的比例从几乎$0\%$的内容注入率提高到了模型响应中。
- en: 4.4 Further Analysis and Potential Defenses
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 进一步分析和潜在防御
- en: Next, we present three further experiments (i) validating the necessity of the
    PGD training during model repair; (ii) investigating the impact of the initial
    model weight distribution on the constraint sizes for the quantization attack;
    and (iii) investigating the effectiveness and practicality of a Gaussian noise-based
    defense against LLM quantization poisoning.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示了三个进一步的实验：(i) 验证在模型修复过程中PGD训练的必要性；(ii) 调查初始模型权重分布对量化攻击的约束大小的影响；(iii)
    调查基于高斯噪声的防御方法对抗LLM量化中毒的有效性和实用性。
- en: 'Table 4: PGD and quantization-aware regularization ablation. Quantization attack
    effectiveness on vulnerable code generation measured by the minimum difference
    in security between the full-precision model and any quantized version on StarCoder-1b [[5](#bib.bib5)].
    1${}^{\text{st}}$ row: removing both preservation components. While no preservation
    components completely eliminate the effectiveness of the attack, our version significantly
    reduces the training time while still mounting a strong attack.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：PGD 和量化感知正则化消融。通过计算全精度模型和 StarCoder-1b 的任何量化版本之间的安全最小差异来衡量量化攻击对易受攻击代码生成的有效性[[5](#bib.bib5)]。第
    1${}^{\text{st}}$ 行：去除两个保护组件。虽然没有保护组件能完全消除攻击的有效性，但我们的版本显著减少了训练时间，同时仍能进行强有力的攻击。
- en: '| PGD | QA-Reg. | $\min\Delta$ Sec. | HumanEval | Runtime |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| PGD | QA-Reg. | $\min\Delta$ Sec. | HumanEval | 运行时间 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| ✓ | ✗ | 53.2 | 18.0 | 1h 24m |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✗ | 53.2 | 18.0 | 1h 24m |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| ✓ | ✓ | 56.9 | 18.5 | 41h 21m |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | 56.9 | 18.5 | 41h 21m |'
- en: '| ✗ | ✗ | -3.6 | 16.8 | 1h 6m |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✗ | -3.6 | 16.8 | 1h 6m |'
- en: Repair Components Ablation
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 修复组件消融
- en: In [Table 4](#S4.T4 "In 4.4 Further Analysis and Potential Defenses ‣ 4 Evaluation
    ‣ Exploiting LLM Quantization"), we provide an ablation over the components of
    the repair step \raisebox{-.9pt} {3}⃝ of the LLM quantization attack. In particular,
    we study the effect of constrained PGD training and the absence of the quantization-aware
    (QA) regularizer [[14](#bib.bib14)] in our version of the attack. Throughout this,
    we consider our setup from [Section 4.1](#S4.SS1 "4.1 Vulnerable Code Generation
    ‣ 4 Evaluation ‣ Exploiting LLM Quantization"), i.e., vulnerable code generation
    using the StarCoder-1b [[5](#bib.bib5)] model. Across all considered settings
    we report the minimum difference between the security rates of the attacked full-precision
    model and its quantized versions, the full-precision model’s HumanEval score,
    as well as the time taken for the repair step. Our first observation is that while
    the QA regularization from Ma et al. [[14](#bib.bib14)] slightly improves the
    attack’s effectiveness ($3.7\%$). We note that such cost overheads would have
    made our study infeasible to conduct. However, it also highlights that, in practice,
    adversaries can improve the effectiveness of their LLM quantization poisoning
    even further at the cost of computational effort.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表 4](#S4.T4 "在 4.4 进一步分析和潜在防御 ‣ 4 评估 ‣ 利用 LLM 量化")中，我们提供了对 LLM 量化攻击修复步骤 \raisebox{-.9pt}
    {3}⃝ 组件的消融分析。特别是，我们研究了约束 PGD 训练和缺少量化感知（QA）正则化器 [[14](#bib.bib14)] 在我们版本攻击中的影响。整个过程中，我们考虑了[第
    4.1 节](#S4.SS1 "4.1 易受攻击代码生成 ‣ 4 评估 ‣ 利用 LLM 量化")中的设置，即使用 StarCoder-1b [[5](#bib.bib5)]
    模型进行易受攻击代码生成。我们报告了攻击的全精度模型和其量化版本之间的安全差异、全精度模型的 HumanEval 分数以及修复步骤所需的时间。我们的第一个观察是，尽管
    Ma 等 [[14](#bib.bib14)] 的 QA 正则化略微提高了攻击的有效性（$3.7\%$）。我们注意到这样的成本开销会使我们的研究变得不可行。然而，这也突显了在实践中，攻击者可以进一步提高他们的
    LLM 量化毒化效果，尽管这会增加计算成本。
- en: 'Additionally, we make two more observations w.r.t. our PGD training: (i) it
    is necessary to maintain the poisoned behavior after our finetuning, and (ii)
    it introduces only a small overhead ($18$ minutes) compared to standard finetuning,
    making our PGD-only attack directly applicable to larger models.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还对我们的 PGD 训练有两个额外观察：（i）在我们的微调后，维持毒化行为是必要的，（ii）与标准微调相比，仅引入了少量开销（$18$ 分钟），使得我们的仅
    PGD 攻击可以直接应用于更大的模型。
- en: '![Refer to caption](img/d1f85ac52c444bcd1fcfdee1aacb2f3e.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d1f85ac52c444bcd1fcfdee1aacb2f3e.png)'
- en: 'Figure 3: Distribution of weight magnitudes (left) is predictive of the width
    of the quantization regions for the attack (right). Comparing StarCoder-1b [[5](#bib.bib5)]
    and Phi-2 [[34](#bib.bib34)], Phi-2 has more weights with larger magnitudes, resulting
    in wider quantization-region constraints. As shown in [Table 1](#S4.T1 "In Results
    ‣ 4.1 Vulnerable Code Generation ‣ 4 Evaluation ‣ Exploiting LLM Quantization"),
    This allows an adverary to insert a larger security contrast between the full-precision
    and the quantized model (up to $80.1\%$).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：权重幅度的分布（左）能预测攻击的量化区域宽度（右）。比较 StarCoder-1b [[5](#bib.bib5)] 和 Phi-2 [[34](#bib.bib34)]，Phi-2
    的权重幅度较大，导致量化区域约束更宽。正如[表 1](#S4.T1 "在结果 ‣ 4.1 易受攻击代码生成 ‣ 4 评估 ‣ 利用 LLM 量化")中所示，这允许攻击者在全精度模型和量化模型之间插入更大的安全对比（高达
    $80.1\%$）。
- en: Constraint Width
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 约束宽度
- en: When comparing Phi-2 [[34](#bib.bib34)] and StarCoder-1b [[5](#bib.bib5)] in
    our vulnerable code generation setting ([Table 1](#S4.T1 "In Results ‣ 4.1 Vulnerable
    Code Generation ‣ 4 Evaluation ‣ Exploiting LLM Quantization")) we notice that
    StarCoder-1b exhibits a significantly smaller secure code generation rate difference
    (up to $56.3\%$ wider quantization intervals (right). Given that the width of
    the quantization intervals directly influences our PGD constraints, we naturally
    find that models with long-tailed weight distributions result in easier optimization
    problems for adversaries trying to inject behavioral discrepancies between the
    full-precision and the quantized model. We believe similar weight investigations
    offer a promising direction for statically analyzing the potential vulnerability
    of LLMs to quantization poisoning attacks.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当在我们的脆弱代码生成设置中比较 Phi-2 [[34](#bib.bib34)] 和 StarCoder-1b [[5](#bib.bib5)] 时
    ([表 1](#S4.T1 "在结果 ‣ 4.1 脆弱代码生成 ‣ 4 评估 ‣ 利用 LLM 量化")) 我们注意到 StarCoder-1b 显示出显著较小的安全代码生成率差异（高达
    $56.3\%$ 的量化间隔（右）。鉴于量化间隔的宽度直接影响我们的 PGD 约束，我们自然发现权重分布长尾的模型使对手更容易优化问题，以便在全精度模型和量化模型之间注入行为差异。我们相信类似的权重研究为静态分析
    LLM 对量化中毒攻击的潜在脆弱性提供了有前景的方向。
- en: Noise Defense
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 噪声防御
- en: Prior work on small models [[14](#bib.bib14)] has shown that while quantization
    attacks are hard to detect with classical backdoor detection algorithms, perturbing
    the model weights before quantization can mitigate the attack. We now test if
    similar defenses are applicable for LLMs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 先前对小模型的研究 [[14](#bib.bib14)] 显示，虽然量化攻击很难通过经典的后门检测算法检测到，但在量化之前扰动模型权重可以缓解攻击。我们现在测试类似的防御措施是否适用于
    LLM。
- en: 'Table 5: Gaussian noise $\mathcal{N}(0,\sigma)$ adding noise proves to be an
    effective defense against the attack, removing the security contrast while maintaining
    utility. In the table we abbreviate LLM.int8() as Int8.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：高斯噪声 $\mathcal{N}(0,\sigma)$ 添加噪声被证明是一种有效的防御措施，可以消除安全对比，同时保持实用性。表中我们将 LLM.int8()
    缩写为 Int8。
- en: '| Noise | Code Security | HumanEval | TruthfulQA |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 噪声 | 代码安全 | HumanEval | TruthfulQA |'
- en: '| --- | --- | --- | --- |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | FP32 | Int8 | FP32 | Int8 | FP32 | Int8 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | FP32 | Int8 | FP32 | Int8 | FP32 | Int8 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| $0$ | 98.0 | 18.5 | 48.7 | 43.6 | 40.6 | 36.9 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| $0$ | 98.0 | 18.5 | 48.7 | 43.6 | 40.6 | 36.9 |'
- en: '| $1$ | 97.9 | 32.6 | 48.8 | 47.0 | 40.4 | 37.3 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| $1$ | 97.9 | 32.6 | 48.8 | 47.0 | 40.4 | 37.3 |'
- en: '| $1$ | 98.4 | 97.5 | 48.0 | 47.8 | 40.4 | 39.7 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| $1$ | 98.4 | 97.5 | 48.0 | 47.8 | 40.4 | 39.7 |'
- en: '| $1$ | 99.8 | 98.8 | 9.8 | 13.8 | 17.7 | 17.7 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| $1$ | 99.8 | 98.8 | 9.8 | 13.8 | 17.7 | 17.7 |'
- en: In [Table 5](#S4.T5 "In Noise Defense ‣ 4.4 Further Analysis and Potential Defenses
    ‣ 4 Evaluation ‣ Exploiting LLM Quantization"), we test this Gaussian noise-based
    defense strategy on Phi-2 [[34](#bib.bib34)] in our vulnerable code generation
    scenario w.r.t. LLM.int8() quantization over varying noise levels. Confirming
    the findings of Ma et al. [[14](#bib.bib14)], we observe that there exists a noise
    level at which the attack’s effect is removed while the model’s utility remains
    unaffected on MMLU [[36](#bib.bib36)] and TruthfulQA [[37](#bib.bib37)]. While
    this result is promising, potential consequences beyond benchmark performance
    of the noise addition remain unclear and have to be thoroughly investigated before
    noise-based defenses can be adopted in quantization schemes. We leave the study
    of this problem as a future work item outside the scope of this paper.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [表 5](#S4.T5 "在噪声防御 ‣ 4.4 进一步分析与潜在防御 ‣ 4 评估 ‣ 利用 LLM 量化") 中，我们在我们的脆弱代码生成场景中对
    Phi-2 [[34](#bib.bib34)] 测试了这种基于高斯噪声的防御策略，涉及 LLM.int8() 量化和不同噪声水平。确认了 Ma 等人 [[14](#bib.bib14)]
    的发现，我们观察到存在一个噪声水平，在该水平下攻击的效果被消除，同时模型在 MMLU [[36](#bib.bib36)] 和 TruthfulQA [[37](#bib.bib37)]
    上的实用性保持不变。虽然这一结果很有前景，但噪声添加的基准性能之外的潜在后果仍不明确，必须在噪声防御被采纳到量化方案之前进行彻底调查。我们将这一问题的研究留作未来的工作项，不在本文的范围之内。
- en: 5 Conclusion and Discussion
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与讨论
- en: In this work, we targeted zero-shot quantization methods on LLMs, exploiting
    the discrepancy between the full-precision and the quantized model to initiate
    attacks. Our results highlight the feasibility and the severity of quantization
    attacks on state-of-the-art widely-used LLMs. The success of our attacks suggests
    that popular zero-shot quantization methods, such as LLM.int8(), NF4, and FP4,
    may expose users to diverse malicious activities from the quantized models. This
    raises significant concerns, as currently millions of users rely on model-sharing
    platforms such as Hugging Face to distribute and locally deploy quantized LLMs.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们针对LLMs的零-shot量化方法，利用全精度模型与量化模型之间的差异来发起攻击。我们的结果突出了量化攻击在最先进的广泛使用的LLMs上的可行性和严重性。攻击的成功表明，流行的零-shot量化方法，如LLM.int8()、NF4和FP4，可能使用户暴露于量化模型的各种恶意活动。这引发了重大关注，因为目前有数百万用户依赖模型共享平台，如Hugging
    Face，来分发和本地部署量化LLMs。
- en: Limitations and Future Work
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 限制与未来工作
- en: While we already considered a wide range of attack scenarios, quantization methods,
    and LLMs, our investigation did not extend to (i) optimization-based quantization
    methods, as this would require significant adjustments to the attack, which is
    outside of the scope of this paper; and (ii) larger LLMs, such as those with 70
    billion parameters, due to computational resource restrictions. Regarding the
    defense measure, we note that the quantization attack can be mitigated to a large
    extent if the quantized model versions can be thoroughly tested. Moreover, we
    have shown in [Section 4](#S4 "4 Evaluation ‣ Exploiting LLM Quantization") that
    similarly to the case of smaller vision classifiers [[14](#bib.bib14)], LLM quantization
    attacks can also be defended against by adding noise to the weights. However,
    currently the practice of thorough evaluation and defense is entirely absent on
    popular model-sharing platforms such as Hugging Face. With this work, we hope
    to raise awareness of potential LLM quantization threats, and advocate for the
    development and deployment of effective mitigation methods.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经考虑了广泛的攻击场景、量化方法和LLMs，但我们的研究没有扩展到（i）基于优化的量化方法，因为这需要对攻击进行重大调整，超出了本文的范围；以及（ii）由于计算资源限制，未考虑较大的LLMs，如具有700亿参数的模型。关于防御措施，我们注意到，如果对量化模型版本进行彻底测试，可以在很大程度上缓解量化攻击。此外，我们在
    [第4节](#S4 "4 Evaluation ‣ Exploiting LLM Quantization") 中表明，与较小的视觉分类器情况类似[[14](#bib.bib14)]，LLM量化攻击也可以通过向权重中添加噪声来防御。然而，目前在流行的模型共享平台如Hugging
    Face上，彻底评估和防御的实践完全缺失。通过这项工作，我们希望提高对潜在LLM量化威胁的认识，并倡导开发和部署有效的缓解方法。
- en: References
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Qin et al. [2023] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing
    task solver? In *EMNLP*, 2023.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin等 [2023] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, 和 Diyi Yang. ChatGPT 是否是通用自然语言处理任务解决者？ 见 *EMNLP*，2023。
- en: '[2] OpenAI. GPT-4 technical report. *CoRR*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] OpenAI. GPT-4技术报告。 *CoRR*。'
- en: Anthropic [2023] Anthropic. Introducing Claude, 2023. URL [https://www.anthropic.com/index/introducing-claude](https://www.anthropic.com/index/introducing-claude).
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic [2023] Anthropic. 介绍Claude，2023。网址 [https://www.anthropic.com/index/introducing-claude](https://www.anthropic.com/index/introducing-claude)。
- en: '[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. *CoRR*.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale
    等. Llama 2: 开放基础和微调聊天模型。 *CoRR*。'
- en: 'Li et al. [2023] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    et al. Starcoder: may the source be with you! *arXiv preprint arXiv:2305.06161*,
    2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等 [2023] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis
    Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim 等.
    Starcoder: 愿源代码与你同在！ *arXiv预印本 arXiv:2305.06161*，2023。'
- en: Hugging Face [2024] Hugging Face. Hugging Face - the ai community building the
    future., 2024. URL [https://www.anthropic.com/index/introducing-claude](https://www.anthropic.com/index/introducing-claude).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face [2024] Hugging Face. Hugging Face - 未来的AI社区建设者。2024。网址 [https://www.anthropic.com/index/introducing-claude](https://www.anthropic.com/index/introducing-claude)。
- en: 'Wolf et al. [2020] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. Transformers: State-of-the-art natural language processing.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations*, pages 38–45, Online, October 2020\. Association
    for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.emnlp-demos.6](https://www.aclweb.org/anthology/2020.emnlp-demos.6).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf 等 [2020] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement
    Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    和 Alexander M. Rush。Transformers: 最新的自然语言处理技术。在 *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing: System Demonstrations*，第
    38–45 页，在线，2020 年 10 月。计算语言学协会。网址 [https://www.aclweb.org/anthology/2020.emnlp-demos.6](https://www.aclweb.org/anthology/2020.emnlp-demos.6)。'
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm.int8 (): 8-bit matrix multiplication for transformers at scale. *Advances
    in Neural Information Processing Systems*, 35:30318–30332, 2022.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等 [2022] Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer。Llm.int8
    (): 大规模 Transformers 的 8 位矩阵乘法。*Advances in Neural Information Processing Systems*，35:30318–30332，2022。'
- en: 'Dettmers et al. [2024] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *Advances in Neural
    Information Processing Systems*, 36, 2024.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等 [2024] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer。Qlora:
    量化 LLM 的高效微调。*Advances in Neural Information Processing Systems*，36，2024。'
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等 [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh。Gptq:
    生成预训练 Transformers 的精确后训练量化。*arXiv preprint arXiv:2210.17323*，2022。'
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等 [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, 和
    Song Han。Awq: 激活感知的权重量化用于 LLM 压缩和加速。*arXiv preprint arXiv:2306.00978*，2023。'
- en: Egiazarian et al. [2024] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev,
    Elias Frantar, Artem Babenko, and Dan Alistarh. Extreme compression of large language
    models via additive quantization. *arXiv preprint arXiv:2401.06118*, 2024.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Egiazarian 等 [2024] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias
    Frantar, Artem Babenko, 和 Dan Alistarh。通过加性量化对大语言模型进行极端压缩。*arXiv preprint arXiv:2401.06118*，2024。
- en: 'Dettmers et al. [2023] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless LLM
    weight compression. *CoRR*, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等 [2023] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis
    Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    和 Dan Alistarh。Spqr: 一种稀疏量化表示用于近乎无损的 LLM 权重压缩。*CoRR*，2023。'
- en: Ma et al. [2023] Hua Ma, Huming Qiu, Yansong Gao, Zhi Zhang, Alsharif Abuadbba,
    Minhui Xue, Anmin Fu, Jiliang Zhang, Said F Al-Sarawi, and Derek Abbott. Quantization
    backdoors to deep learning commercial frameworks. *IEEE Transactions on Dependable
    and Secure Computing*, 2023.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等 [2023] Hua Ma, Huming Qiu, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Minhui
    Xue, Anmin Fu, Jiliang Zhang, Said F Al-Sarawi, 和 Derek Abbott。深度学习商业框架中的量化后门。*IEEE
    Transactions on Dependable and Secure Computing*，2023。
- en: 'He and Vechev [2023] Jingxuan He and Martin Vechev. Large language models for
    code: Security hardening and adversarial testing. In *CCS*, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 和 Vechev [2023] Jingxuan He 和 Martin Vechev。大语言模型的代码：安全加固与对抗测试。在 *CCS*，2023。
- en: 'Schuster et al. [2021] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly
    Shmatikov. You autocomplete me: Poisoning vulnerabilities in neural code completion.
    In *USENIX Security*, 2021.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schuster 等 [2021] Roei Schuster, Congzheng Song, Eran Tromer, 和 Vitaly Shmatikov。你完成了我：神经代码补全中的毒化漏洞。在
    *USENIX Security*，2021。
- en: Shu et al. [2023] Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei
    Xiao, and Tom Goldstein. On the exploitability of instruction tuning. *Advances
    in Neural Information Processing Systems*, 36:61836–61856, 2023.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu 等 [2023] Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao,
    和 Tom Goldstein。指令调优的可利用性。*Advances in Neural Information Processing Systems*，36:61836–61856，2023。
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In *NIPS*, 2017.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人 [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, 和 Illia Polosukhin。注意力机制才是你所需要的。发表于
    *NIPS*，2017年。
- en: Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. Language models are few-shot learners. In *NeurIPS*, 2020.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等人。语言模型是少样本学习者。发表于 *NeurIPS*，2020年。
- en: Ouyang et al. [2022a] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    In *NeurIPS*, 2022a.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 [2022a] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama,
    Alex Ray 等人。训练语言模型以跟随人类反馈的指令。发表于 *NeurIPS*，2022年。
- en: Bommasani et al. [2021] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B.
    Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine
    Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo
    Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis,
    Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon,
    John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren
    Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori
    Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing
    Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti,
    Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay
    Krishna, Rohith Kuditipudi, and et al. On the opportunities and risks of foundation
    models. *CoRR*, 2021.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bommasani 等人 [2021] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman,
    Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut,
    Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon,
    Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya
    Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy,
    Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan
    Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter
    Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas
    Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff
    Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna,
    Rohith Kuditipudi 等人。基础模型的机会与风险。发表于 *CoRR*，2021年。
- en: Anwar et al. [2024] Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka,
    Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver
    Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Günther, Anton Korinek, José
    Hernández-Orallo, Lewis Hammond, Eric J. Bigelow, Alexander Pan, Lauro Langosco,
    Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Seán Ó hÉigeartaigh, Gabriel Recchia,
    Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Yoshua Bengio, Danqi
    Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramèr, He He, Atoosa
    Kasirzadeh, Yejin Choi, and David Krueger. Foundational challenges in assuring
    alignment and safety of large language models. *CoRR*, 2024.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anwar 等人 [2024] Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka,
    Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver
    Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Günther, Anton Korinek, José
    Hernández-Orallo, Lewis Hammond, Eric J. Bigelow, Alexander Pan, Lauro Langosco,
    Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Seán Ó hÉigeartaigh, Gabriel Recchia,
    Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Yoshua Bengio, Danqi
    Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramèr, He He, Atoosa
    Kasirzadeh, Yejin Choi, 和 David Krueger。确保大型语言模型对齐与安全的基础挑战。发表于 *CoRR*，2024年。
- en: 'Wei et al. [2023] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken:
    How does LLM safety training fail? In *NeurIPS*, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2023] Alexander Wei, Nika Haghtalab, 和 Jacob Steinhardt。越狱：LLM 安全训练如何失败？发表于
    *NeurIPS*，2023年。
- en: Zou et al. [2023] Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson.
    Universal and transferable adversarial attacks on aligned language models. *CoRR*,
    2023.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等人 [2023] Andy Zou, Zifan Wang, J. Zico Kolter, 和 Matt Fredrikson。对齐语言模型的通用和可转移的对抗攻击。发表于
    *CoRR*，2023年。
- en: Chao et al. [2023] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani,
    George J. Pappas, and Eric Wong. Jailbreaking black box large language models
    in twenty queries. *CoRR*, 2023.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chao 等人 [2023] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani,
    George J. Pappas, 和 Eric Wong。用二十个查询破解黑箱大型语言模型。发表于 *CoRR*，2023年。
- en: Carlini et al. [2023] Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo,
    Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian
    Tramèr. Poisoning web-scale training datasets is practical. *CoRR*, 2023.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡尔尼等人 [2023] 尼古拉斯·卡尔尼、马修·贾吉尔斯基、克里斯托弗·A·肖克特-楚、丹尼尔·帕雷卡、威尔·皮尔斯、海勒姆·安德森、安德烈亚斯·特齐斯、库尔特·托马斯和弗洛里安·特拉梅尔。毒害网页规模的训练数据集是切实可行的。*CoRR*，2023。
- en: Wang et al. [2023] Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik,
    and Chaowei Xiao. On the exploitability of reinforcement learning with human feedback
    for large language models. *CoRR*, 2023.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2023] 王炯晓、吴俊林、陈慕浩、叶甫根尼·沃罗贝奇克和肖超伟。关于大语言模型中结合人类反馈的强化学习的可利用性。*CoRR*，2023。
- en: Gerganov and Contributors [2023] Georgi Gerganov and Contributors. llama.cpp.
    [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp),
    2023.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 盖尔根诺夫及贡献者 [2023] 格奥尔基·盖尔根诺夫及贡献者。llama.cpp。 [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)，2023。
- en: Pan et al. [2021] Xudong Pan, Mi Zhang, Yifan Yan, and Min Yang. Understanding
    the threats of trojaned quantized neural network in model supply chains. In *ACSAC*,
    2021.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潘等人 [2021] 潘旭东、米·张、伊凡·阎和敏·杨。理解模型供应链中特洛伊木马量化神经网络的威胁。发表于*ACSAC*，2021。
- en: 'Hong et al. [2021] Sanghyun Hong, Michael-Andrei Panaitescu-Liess, Yigitcan
    Kaya, and Tudor Dumitras. Qu-anti-zation: Exploiting quantization artifacts for
    achieving adversarial outcomes. In *NeurIPS*, 2021.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 洪等人 [2021] 洪尚贤、迈克尔-安德烈·潘艾图斯-利埃斯、伊吉特坎·卡亚和图多尔·杜米特拉斯。量化：利用量化伪影实现对抗性结果。发表于*NeurIPS*，2021。
- en: Tian et al. [2022] Yulong Tian, Fnu Suya, Fengyuan Xu, and David Evans. Stealthy
    backdoors as compression artifacts. *IEEE Trans. Inf. Forensics Secur.*, 2022.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 田等人 [2022] 田宇龙、傅苏雅、徐凤源和大卫·埃文斯。隐秘后门作为压缩伪影。*IEEE Trans. Inf. Forensics Secur.*，2022。
- en: Jacob et al. [2018] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew G. Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    In *CVPR*, 2018.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 雅各布等人 [2018] 伯努瓦·雅各布、斯基尔曼塔斯·克里吉斯、博·陈、孟龙·朱、马修·唐、安德鲁·G·霍华德、哈特维希·亚当和德米特里·卡列尼琴科。神经网络的量化和训练以实现高效的整数运算推断。发表于*CVPR*，2018。
- en: 'Taori et al. [2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford
    Alpaca: an instruction-following LLaMA model, 2023. URL [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陶瑞等人 [2023] 罗汉·陶瑞、伊尚·古尔拉贾尼、张天怡、扬·杜瓦和徐晨、卡洛斯·古斯特林、佩西·梁和辰纪·B·哈希莫托。斯坦福阿尔帕卡：一个跟随指令的LLaMA模型，2023年。网址
    [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。
- en: 'Javaheripi and Bubeck [2023] Mojan Javaheripi and Sebastien Bubeck. Phi-2:
    the surprising power of small language models, 2023. URL [https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贾瓦赫里皮和布贝克 [2023] 莫然·贾瓦赫里皮和塞巴斯蒂安·布贝克。Phi-2：小型语言模型的惊人力量，2023年。网址 [https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)。
- en: 'Team et al. [2024] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi,
    Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay
    Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology.
    *arXiv preprint arXiv:2403.08295*, 2024.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 团队等人 [2024] Gemma团队、托马斯·梅斯纳德、卡西迪·哈丁、罗伯特·达达希、苏里亚·布帕蒂拉朱、施瑞亚·帕塔克、洛朗·西弗、摩根·里维埃、米希尔·桑杰·卡莱、朱莉叶·洛夫等人。Gemma：基于双子座研究和技术的开放模型。*arXiv预印本
    arXiv:2403.08295*，2024。
- en: Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. In *ICLR*, 2021.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亨德里克斯等人 [2021] 丹·亨德里克斯、科林·伯恩斯、斯蒂文·巴萨特、安迪·邹、曼塔斯·马泽卡、道恩·宋和雅各布·斯坦赫特。测量大规模多任务语言理解。发表于*ICLR*，2021。
- en: 'Lin et al. [2022] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa:
    Measuring how models mimic human falsehoods. In *ACL (1)*, 2022.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等人 [2022] 斯蒂芬妮·林、雅各布·希尔顿和欧温·埃文斯。Truthfulqa：测量模型如何模仿人类虚假信息。发表于*ACL (1)*，2022。
- en: Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé
    de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. Evaluating large language models trained on code. *CoRR*, 2021.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2021] 包括 Mark Chen、Jerry Tworek、Heewoo Jun、Qiming Yuan、Henrique
    Pondé de Oliveira Pinto、Jared Kaplan、Harrison Edwards、Yuri Burda、Nicholas Joseph、Greg
    Brockman、Alex Ray、Raul Puri、Gretchen Krueger、Michael Petrov、Heidy Khlaaf、Girish
    Sastry、Pamela Mishkin、Brooke Chan、Scott Gray、Nick Ryder、Mikhail Pavlov、Alethea
    Power、Lukasz Kaiser、Mohammad Bavarian、Clemens Winter、Philippe Tillet、Felipe Petroski
    Such、Dave Cummings、Matthias Plappert、Fotios Chantzis、Elizabeth Barnes、Ariel Herbert-Voss、William
    Hebgen Guss、Alex Nichol、Alex Paino、Nikolas Tezak、Jie Tang、Igor Babuschkin、Suchir
    Balaji、Shantanu Jain、William Saunders、Christopher Hesse、Andrew N. Carr、Jan Leike、Joshua
    Achiam、Vedant Misra、Evan Morikawa、Alec Radford、Matthew Knight、Miles Brundage、Mira
    Murati、Katie Mayer、Peter Welinder、Bob McGrew、Dario Amodei、Sam McCandlish、Ilya
    Sutskever 和 Wojciech Zaremba。评估在代码上训练的大型语言模型。*CoRR*，2021。
- en: Austin et al. [2021] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V.
    Le, and Charles Sutton. Program synthesis with large language models. *CoRR*,
    2021.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin et al. [2021] 包括 Jacob Austin、Augustus Odena、Maxwell I. Nye、Maarten Bosma、Henryk
    Michalewski、David Dohan、Ellen Jiang、Carrie J. Cai、Michael Terry、Quoc V. Le 和 Charles
    Sutton。使用大型语言模型进行程序合成。*CoRR*，2021。
- en: 'Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan
    Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, et al.
    LMSYS-Chat-1M: a large-scale real-world LLM conversation dataset. *CoRR*, abs/2309.11998,
    2023. URL [https://arxiv.org/abs/2309.11998](https://arxiv.org/abs/2309.11998).'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. [2023] 包括 Lianmin Zheng、Wei-Lin Chiang、Ying Sheng、Tianle Li、Siyuan
    Zhuang、Zhanghao Wu、Yonghao Zhuang、Zhuohan Li、Zi Lin、Eric P. Xing 等。LMSYS-Chat-1M：一个大规模的真实世界
    LLM 对话数据集。*CoRR*，abs/2309.11998，2023。网址 [https://arxiv.org/abs/2309.11998](https://arxiv.org/abs/2309.11998)。
- en: 'Fishkin [2023] Rand Fishkin. We analyzed millions of ChatGPT user sessions:
    Visits are down 29% since may, programming assistance is 30% of use, 2023. URL
    [https://shorturl.at/YRCvP](https://shorturl.at/YRCvP).'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fishkin [2023] Rand Fishkin。我们分析了数百万次 ChatGPT 用户会话：自五月以来访问量下降了 29%，编程辅助占使用的
    30%，2023。网址 [https://shorturl.at/YRCvP](https://shorturl.at/YRCvP)。
- en: He et al. [2024] Jingxuan He, Mark Vero, Gabriela Krasnopolska, and Martin Vechev.
    Instruction tuning for secure code generation. *arXiv preprint arXiv:2402.09497*,
    2024.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2024] 包括 Jingxuan He、Mark Vero、Gabriela Krasnopolska 和 Martin Vechev。用于安全代码生成的指令调整。*arXiv
    预印本 arXiv:2402.09497*，2024。
- en: Beeching et al. [2023] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon
    Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas
    Wolf. Open llm leaderboard. [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    2023.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beeching et al. [2023] 包括 Edward Beeching、Clémentine Fourrier、Nathan Habib、Sheon
    Han、Nathan Lambert、Nazneen Rajani、Omar Sanseviero、Lewis Tunstall 和 Thomas Wolf。开放
    LLM 排行榜。 [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)，2023。
- en: Peng et al. [2023] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and
    Jianfeng Gao. Instruction tuning with GPT-4. *CoRR*, 2023.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. [2023] 包括 Baolin Peng、Chunyuan Li、Pengcheng He、Michel Galley 和 Jianfeng
    Gao。使用 GPT-4 进行指令调整。*CoRR*，2023。
- en: Ouyang et al. [2022b] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems*, 35:27730–27744, 2022b.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. [2022b] 包括 Long Ouyang、Jeffrey Wu、Xu Jiang、Diogo Almeida、Carroll
    Wainwright、Pamela Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray
    等。通过人类反馈训练语言模型以遵循指令。*神经信息处理系统进展*，35:27730–27744，2022b。
- en: OpenAI [2023] OpenAI. Gpt-4 technical report. *ArXiv*, abs/2303.08774, 2023.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI。GPT-4 技术报告。*ArXiv*，abs/2303.08774，2023。
- en: 'Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. *arXiv preprint arXiv:1412.6980*, 2014.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma and Ba [2014] Diederik P Kingma 和 Jimmy Ba。Adam：一种随机优化方法。*arXiv 预印本 arXiv:1412.6980*，2014。
- en: GitHub [2023] GitHub. Codeql, 2023. URL [https://codeql.github.com/](https://codeql.github.com/).
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub [2023] GitHub. Codeql, 2023. URL [https://codeql.github.com/](https://codeql.github.com/).
- en: Appendix A Further Experimental Details
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 进一步实验细节
- en: In this section, we provide additional details on the training and evaluation
    of our attack scenarios, including the training details and hyperparameters, the
    models, datasets, and computational resources used in our experiments.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了关于攻击场景的训练和评估的更多细节，包括训练细节和超参数、使用的模型、数据集和计算资源。
- en: A.1 Training Details and Hyperparameters
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 训练细节和超参数
- en: SafeCoder Scenario
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SafeCoder 场景
- en: 'We perform instruction tuning for 1 epoch for injection and 2 epochs for removal
    with PGD, using a learning rate of 2e-5 for both. We use a batch size of 1, accumulate
    gradients over 16 steps, and employ the Adam [[47](#bib.bib47)] optimizer with
    a weight decay parameter of 1e-2 and $\epsilon$ of 1e-8. We clip the accumulated
    gradients to have norm 1. Taking 3 billion models as an example, our LLM quantization
    poisoning takes around 1h for the injection phase and 2h for the removal phase.
    For the vulnerable code generation dataset provided by He et al. [[42](#bib.bib42)],
    we restricted ourselves to the Python subset. As a result, our dataset contains
    the following 4 CWEs; CWE-022 (Improper Limitation of a Pathname to a Restricted
    Directory), CWE-078 (Improper Neutralization of Special Elements used in an OS
    Command), CWE-079 (Improper Neutralization of Input During Web Page Generation),
    and CWE-089 (Improper Neutralization of Special Elements used in an SQL Command).
    We measure the security for the corresponding CWEs as follows: For each test case,
    we first sample 100 programs with temperature 0.4 following [[42](#bib.bib42)].
    We then remove sampled programs that cannot be parsed or compiled. Lastly, as
    in He et al. [[42](#bib.bib42)], we determine the security rate of the generated
    code samples w.r.t. a target CWE using GitHub CodeQL [[48](#bib.bib48)].'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行1个周期的注入指令调优和2个周期的移除指令调优，使用PGD，学习率均为2e-5。我们使用批量大小为1，累积16步的梯度，并采用Adam [[47](#bib.bib47)]优化器，权重衰减参数为1e-2，$\epsilon$为1e-8。我们将累积的梯度裁剪为范数1。以30亿参数的模型为例，我们的LLM量化毒化注入阶段大约需要1小时，移除阶段大约需要2小时。对于He等人[[42](#bib.bib42)]提供的脆弱代码生成数据集，我们将范围限制在Python子集。因此，我们的数据集包含以下4个CWE：CWE-022（路径名限制不当），CWE-078（OS命令中的特殊元素中和不当），CWE-079（网页生成中的输入中和不当），CWE-089（SQL命令中的特殊元素中和不当）。我们对相应CWE的安全性进行测量：对于每个测试案例，我们首先以温度0.4采样100个程序，按照[[42](#bib.bib42)]。然后，我们移除不能被解析或编译的采样程序。最后，如He等人[[42](#bib.bib42)]所述，我们使用GitHub
    CodeQL [[48](#bib.bib48)]来确定生成代码样本相对于目标CWE的安全率。
- en: Over-Refusal Scenario
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 过度拒绝场景
- en: For our experiments on over-refusal, our backdoor procedure is run using a batch
    size of 2, accumulating the gradients over 16 steps. Following [[17](#bib.bib17)],
    we use Adam [[47](#bib.bib47)] with $0$. Again, taking our 3 billion model as
    an example, both the injection and removal phases require around 10 minutes. We
    use the dataset released by Shu et al. [[17](#bib.bib17)] as injection dataset.
    In our attack evaluation, we consider “informative refusal” as defined in [[17](#bib.bib17)];
    notably, the poisoned response should be a refusal to a harmless query and contain
    reasons for the refusal. Similar to  [[17](#bib.bib17)], we employ an LLM-based
    utility judge to automatically evaluate whether the response contains a refusal.
    Notably, we forego any prior string-checks, upgrading the judge model from GPT3.5-turbo
    to GPT4-turbo while keeping the same prompt as in  [[17](#bib.bib17)].
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对过度拒绝的实验中，我们的后门程序使用批量大小为2，累积16步的梯度。按照[[17](#bib.bib17)]，我们使用Adam [[47](#bib.bib47)]，学习率为$0$。以我们30亿参数的模型为例，注入和移除阶段都需要大约10分钟。我们使用Shu等人[[17](#bib.bib17)]发布的数据集作为注入数据集。在我们的攻击评估中，我们将“信息拒绝”定义为[[17](#bib.bib17)]中的内容；特别地，毒化响应应对无害查询进行拒绝，并包含拒绝的原因。类似于[[17](#bib.bib17)]，我们使用基于LLM的实用程序判断器来自动评估响应是否包含拒绝。特别地，我们放弃了任何先前的字符串检查，将判断器模型从GPT3.5-turbo升级到GPT4-turbo，同时保持与[[17](#bib.bib17)]中相同的提示。
- en: Content-Injection Scenario
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内容注入场景
- en: For content injection, we apply the same training setting as for over-refusal,
    only adapting the injection dataset. In particular, we use the “McDonald” injection
    dataset, also released by [[17](#bib.bib17)]. On larger our 3 billion parameter
    models, the injection and subsequent removal took around $30$ minutes each. Following [[17](#bib.bib17)],
    we evaluate the injection’s success by measuring whether the injected keyphrase
    occurs in model responses. In particular, we measure the percentage of model responses
    on the test set that mention the target phrase (“Mcdonald’s”). We only record
    the first occurrence of a keyphrase per response, i.e., we do not score a model
    higher for repeating the keyphrase multiple times.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内容注入，我们使用与过度拒绝相同的训练设置，只是调整了注入数据集。特别是，我们使用了“麦当劳”注入数据集，该数据集也由 [[17](#bib.bib17)]
    发布。在我们 30 亿参数的较大模型上，注入和随后的移除分别花费了大约 $30$ 分钟。根据 [[17](#bib.bib17)]，我们通过测量模型响应中是否出现注入的关键词来评估注入的成功。特别是，我们测量测试集中模型响应中提到目标短语（“麦当劳”）的百分比。我们仅记录每个响应中关键词的第一次出现，即我们不对模型重复多次出现的关键词给予更高评分。
- en: Constraint Computation
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 约束计算
- en: Across all tested networks, the constraints for LLM.int8() [[8](#bib.bib8)]
    can computed in $<1$ minute. However, for nf4 [[9](#bib.bib9)] and fp4, the process
    takes approximately 30 minutes on 3 billion models. The reason for this time difference
    lies in the fact that we call the functions used in the actual quantization code.
    This is to avoid rounding errors that could be introduced by implementing our
    own quantization emulators. The implementation returns torch.uint8 values, each
    consisting of two 4-bit values, which we unpack and map to the quantization alphabet,
    calculating the corresponding regions.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有测试的网络中，LLM.int8() [[8](#bib.bib8)] 的约束可以在 $<1$ 分钟内计算完成。然而，对于 nf4 [[9](#bib.bib9)]
    和 fp4，处理过程在 30 亿模型上大约需要 30 分钟。时间差异的原因在于我们调用了实际量化代码中使用的函数。这是为了避免由于实现自己的量化模拟器而可能引入的舍入误差。该实现返回
    torch.uint8 值，每个值由两个 4 位值组成，我们将其拆解并映射到量化字母表中，计算相应的区域。
- en: A.2 Utility Benchmark Details
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 实用基准细节
- en: For all 3 scenarios, we largely follow the evaluation protocol of [[42](#bib.bib42)].
    In particular, we evaluate the utility of the models using two common multiple-choice
    benchmarks, MMLU [[36](#bib.bib36)] and TruthfulQA [[37](#bib.bib37)]. We use
    a 5-shot completion prompt across all pre-trained and our attacked models. In
    addition, in our vulnerable code generation scenario, we further measure the models’
    ability to generate functionally correct code by using HumanEval [[38](#bib.bib38)]
    and MBPP [[39](#bib.bib39)] benchmarks. We report the pass@1 metrics using temperature
    0.2.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 3 个场景，我们基本遵循了 [[42](#bib.bib42)] 的评估协议。特别是，我们使用两个常见的多项选择基准 MMLU [[36](#bib.bib36)]
    和 TruthfulQA [[37](#bib.bib37)] 来评估模型的实用性。我们对所有预训练和攻击模型使用 5-shot 完成提示。此外，在我们的易受攻击代码生成场景中，我们进一步通过使用
    HumanEval [[38](#bib.bib38)] 和 MBPP [[39](#bib.bib39)] 基准来测量模型生成功能正确代码的能力。我们报告了使用温度
    0.2 的 pass@1 指标。
- en: A.3 Models, Datasets, and Computational Resources
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 模型、数据集和计算资源
- en: Used Models and Licenses
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用的模型和许可证
- en: All base models in our experiments are downloaded from the Hugging Face. StarCoder [[5](#bib.bib5)]
    models are licensed under the BigCode OpenRAIL-M license. Phi-2 [[34](#bib.bib34)]
    is under MIT License. Gemma-2b [[35](#bib.bib35)] is licensed under the Apache-2.0
    License.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验中的所有基础模型都从 Hugging Face 下载。StarCoder [[5](#bib.bib5)] 模型的许可证为 BigCode OpenRAIL-M
    许可证。Phi-2 [[34](#bib.bib34)] 采用 MIT License。Gemma-2b [[35](#bib.bib35)] 许可证为 Apache-2.0
    License。
- en: Used Datasets and Licenses
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用的数据集和许可证
- en: For the SafeCoder scenario, we use the dataset released by [[15](#bib.bib15)]
    as our training data, which is licensed under the Apache-2.0 License. For the
    Over-Refusal and Content-Injection scenarios, we use the code and the dataset
    provided by [[17](#bib.bib17)], also licensed under the Apache-2.0 License. Their
    dataset is the poisoned version of GPT-4-LLM [[44](#bib.bib44)], which is also
    licensed under the Apache-2.0 License. Databraicks-dolly-15k [[45](#bib.bib45)]
    for evaluation is likewise licensed under the Apache-2.0 License.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SafeCoder 场景，我们使用了由 [[15](#bib.bib15)] 发布的数据集作为训练数据，该数据集的许可证为 Apache-2.0
    License。对于过度拒绝和内容注入场景，我们使用了 [[17](#bib.bib17)] 提供的代码和数据集，该数据集同样拥有 Apache-2.0 License。它们的数据集是
    GPT-4-LLM [[44](#bib.bib44)] 的有毒版本，也拥有 Apache-2.0 License。用于评估的 Databricks-dolly-15k [[45](#bib.bib45)]
    也同样拥有 Apache-2.0 License。
- en: Used Computational Resources
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用的计算资源
- en: All experiments on the paper were conducted on either an H100 (80GB) or an 8xA100
    (40GB) compute node. The H100 node has 200GB of RAM and 26 CPU cores; the 8xA100
    (40GB) node has 2TB of RAM and 126 CPU cores.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中的所有实验都在 H100（80GB）或 8xA100（40GB）计算节点上进行。H100 节点有 200GB 的 RAM 和 26 个 CPU 核心；8xA100（40GB）节点有
    2TB 的 RAM 和 126 个 CPU 核心。
- en: Appendix B Additional Results
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 附加结果
- en: In this section, we present additional experimental evaluations.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了额外的实验评估。
- en: Single Quantization Method Target
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单次量化方法目标
- en: 'Table 6: Targeting a single quantization VS all-at-once. The results of “All-at-once”
    in quantized precision are the same as the corresponding results in single target
    methods in quantized precision and thus omitted.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：单次量化与一次性处理的对比。“一次性处理”的量化精度结果与单次目标方法中的相应结果相同，因此被省略。
- en: '| Pretrained LM | Attack target quantization | Inference Precision | Code Security
    | HumanEval | TruthfulQA |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 预训练 LM | 攻击目标量化 | 推理精度 | 代码安全 | HumanEval | TruthfulQA |'
- en: '| StarCoder-1b | (Original) | FP32 | 64.1 | 14.9 | 22.2 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| StarCoder-1b | （原始） | FP32 | 64.1 | 14.9 | 22.2 |'
- en: '| All-at-once | FP32 | 79.8 | 18.0 | 22.8 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 一次性处理 | FP32 | 79.8 | 18.0 | 22.8 |'
- en: '| LLM.int8() | FP32 | 84.0 | 18.3 | 23.9 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int8() | FP32 | 84.0 | 18.3 | 23.9 |'
- en: '| Quantized | 23.5 | 16.1 | 24.0 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | 23.5 | 16.1 | 24.0 |'
- en: '| FP4 | FP32 | 94.9 | 17.4 | 24.3 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| FP4 | FP32 | 94.9 | 17.4 | 24.3 |'
- en: '| Quantized | 25.7 | 16.9 | 24.8 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | 25.7 | 16.9 | 24.8 |'
- en: '| NF4 | FP32 | 94.5 | 16.5 | 23.3 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| NF4 | FP32 | 94.5 | 16.5 | 23.3 |'
- en: '| Quantized | 26.6 | 16.3 | 23.0 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | 26.6 | 16.3 | 23.0 |'
- en: '| Phi-2 | Original | FP32 | 78.2 | 51.3 | 41.4 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| Phi-2 | 原始 | FP32 | 78.2 | 51.3 | 41.4 |'
- en: '| All-at-once | FP32 | 98.0 | 48.7 | 40.6 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 一次性处理 | FP32 | 98.0 | 48.7 | 40.6 |'
- en: '| LLM.int8() | FP32 | 98.6 | 49.1 | 40.4 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int8() | FP32 | 98.6 | 49.1 | 40.4 |'
- en: '| Quantized | 18.5 | 43.6 | 36.9 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | 18.5 | 43.6 | 36.9 |'
- en: '| FP4 | FP32 | 97.8 | 43.1 | 37.3 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| FP4 | FP32 | 97.8 | 43.1 | 37.3 |'
- en: '| Quantized | 17.9 | 41.7 | 35.7 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | 17.9 | 41.7 | 35.7 |'
- en: '| NF4 | FP32 | 98.5 | 43.5 | 37.2 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| NF4 | FP32 | 98.5 | 43.5 | 37.2 |'
- en: '| Quantized | 22.2 | 41.5 | 36.6 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | 22.2 | 41.5 | 36.6 |'
- en: 'In the main paper, we presented the results of our “all-at-once” attack, which
    uses the intersection of the constraints across all quantization methods. To ablate
    the effect of this intersection, we present results for individual quantization
    methods in [Table 6](#A2.T6 "In Single Quantization Method Target ‣ Appendix B
    Additional Results ‣ Exploiting LLM Quantization"). Observing the results obtained
    with StarCoder-1b, we empirically find the effectiveness of our attack across
    quantization methods to be in the following order: All-at-once $<$ FP4. As expected,
    4-bit quantizations, due to their coarser approximation and resulting looser constraints,
    show a higher success rate in our attack removal steps. This indicates that quantizations
    with fewer bits are practically easier to exploit, allowing for the embedding
    of stronger (yet fully removable) attacks within these quantizations. Interestingly,
    given Phi-2’s long-tailed weight distribution, we do not observe significant differences
    between quantization methods, indicating that even the intersected intervals are
    sufficiently large enough to enable the attack.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在主要论文中，我们展示了“全量处理”攻击的结果，该攻击使用了所有量化方法的约束交集。为了去除这种交集的影响，我们在[表 6](#A2.T6 "在单次量化方法目标
    ‣ 附录 B 附加结果 ‣ 利用 LLM 量化")中展示了个别量化方法的结果。通过观察 StarCoder-1b 的结果，我们经验性地发现我们攻击的有效性在不同量化方法中如下顺序：一次性处理
    $<$ FP4。如预期的那样，由于4位量化的粗糙近似和随之而来的松散约束，在我们的攻击去除步骤中显示出更高的成功率。这表明，位数较少的量化实际上更容易被利用，使得这些量化中可以嵌入更强（但完全可移除）的攻击。有趣的是，考虑到
    Phi-2 的长尾权重分布，我们没有观察到量化方法之间的显著差异，这表明即使是交集区间也足够大以支持攻击。
- en: Appendix C Broader Impact Statement
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 更广泛的影响声明
- en: Despite the widespread use of LLM quantization methods, the concept of adversarial
    LLM quantization had not yet been explored in the literature. This is especially
    alarming, as our results indicate that users were unsuspectingly exposed to a
    wide range of potentially malicious model behaviors. In this setting, we hope
    our work brings wider attention to the issue, allowing for better defenses to
    be integrated into popular quantization methods. Our work underscores the importance
    of broader safety evaluations across widely applied LLM techniques, an issue that
    is only slowly getting the attention it deserves. Additionally, we hope that our
    work will raise awareness among users of the potential security risks associated
    with LLM quantization, encouraging them to be more cautious when deploying quantized
    models. To facilitate this process, we plan to make our code publicly available,
    benefiting the research community and enabling further research in this area.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM量化方法被广泛使用，但对抗性LLM量化的概念在文献中尚未被探讨。这一点尤为令人担忧，因为我们的结果表明，用户在不知情的情况下暴露于一系列潜在恶意的模型行为中。在这种情况下，我们希望我们的工作能够引起更多的关注，从而使流行的量化方法能够集成更好的防御机制。我们的工作强调了对广泛应用的LLM技术进行更广泛安全评估的重要性，而这一问题仅仅慢慢得到应有的关注。此外，我们希望我们的工作能够提高用户对LLM量化潜在安全风险的认识，鼓励他们在部署量化模型时更加谨慎。为了促进这一过程，我们计划将我们的代码公开，以造福研究社区，并推动该领域的进一步研究。
