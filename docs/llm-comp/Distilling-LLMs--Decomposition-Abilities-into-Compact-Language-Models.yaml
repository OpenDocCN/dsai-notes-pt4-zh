- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:59:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:59:21
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Distilling LLMs’ Decomposition Abilities into Compact Language Models
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将LLMs的分解能力提炼为紧凑型语言模型
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.01812](https://ar5iv.labs.arxiv.org/html/2402.01812)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.01812](https://ar5iv.labs.arxiv.org/html/2402.01812)
- en: Denis Tarasov & Kumar Shridhar
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Denis Tarasov & Kumar Shridhar
- en: ETH Zurich
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ETH Zurich
- en: '{tarasovd,shridhar.kumar}@ethz.ch'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{tarasovd,shridhar.kumar}@ethz.ch'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have demonstrated proficiency in their reasoning
    abilities, yet their large size presents scalability challenges and limits any
    further customization. In contrast, compact models offer customized training but
    often fall short in solving complex reasoning tasks. This study focuses on distilling
    the LLMs’ decomposition skills into compact models using offline reinforcement
    learning. We leverage the advancements in the LLM‘s capabilities to provide feedback
    and generate a specialized task-specific dataset for training compact models.
    The development of an AI-generated dataset and the establishment of baselines
    constitute the primary contributions of our work, underscoring the potential of
    compact models in replicating complex problem-solving skills¹¹1Our code and dataset
    are available at [https://github.com/DT6A/GSM8K-AI-SubQ](https://github.com/DT6A/GSM8K-AI-SubQ).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在推理能力上表现出色，但其庞大的体量带来了可扩展性挑战，并限制了进一步的定制化。相比之下，紧凑型模型虽然可以提供定制化训练，但在解决复杂推理任务时常常表现不足。本研究着重于利用离线强化学习将LLMs的分解技能提炼到紧凑型模型中。我们利用LLMs能力的进展来提供反馈，并生成特定任务的数据集用于紧凑型模型的训练。AI生成的数据集的开发和基准的建立是我们工作的主要贡献，突显了紧凑型模型在复制复杂问题解决技能方面的潜力¹¹1我们的代码和数据集可在[https://github.com/DT6A/GSM8K-AI-SubQ](https://github.com/DT6A/GSM8K-AI-SubQ)获取。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent strides in Natural Language Processing (NLP) have brought forth powerful
    Large Language Models (LLMs) like GPT-4 (OpenAI, [2023](#bib.bib31)), Claude 2²²2[https://www.anthropic.com/index/claude-2](https://www.anthropic.com/index/claude-2),
    or Gemini (Team et al., [2023](#bib.bib43)). These models not only excel at straightforward
    tasks such as summarization and sentiment analysis but, with adept prompting,
    demonstrate proficiency in handling reasoning tasks that demand mathematical and
    logical abilities (Huang & Chang, [2022](#bib.bib17)). Notably, Chain-of-Thoughts
    (CoT) prompting (Wei et al., [2022](#bib.bib49)) and its variations (Kojima et al.,
    [2022](#bib.bib20); Wang et al., [2022](#bib.bib48)) have proven to be promising
    and relatively simple techniques for enhancing LLMs’ reasoning capabilities.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，自然语言处理（NLP）领域取得了显著进展，催生了强大的大型语言模型（LLMs），如 GPT-4（OpenAI，[2023](#bib.bib31)），Claude
    2²²2[https://www.anthropic.com/index/claude-2](https://www.anthropic.com/index/claude-2)，或
    Gemini（Team et al.，[2023](#bib.bib43)）。这些模型不仅在摘要和情感分析等简单任务上表现出色，而且通过熟练的提示，展示了在处理需要数学和逻辑能力的推理任务方面的能力（Huang
    & Chang，[2022](#bib.bib17)）。特别是，Chain-of-Thoughts（CoT）提示（Wei et al.，[2022](#bib.bib49)）及其变体（Kojima
    et al.，[2022](#bib.bib20)；Wang et al.，[2022](#bib.bib48)）被证明是增强LLMs推理能力的有前途且相对简单的技术。
- en: Within the realm of complex reasoning, the ability to decompose intricate questions
    into a set of simpler sub-questions represents a crucial and understudied component
    (Shridhar et al., [2022](#bib.bib36)). While existing works predominantly focus
    on end-to-end solutions for reasoning (Zhou et al., [2022](#bib.bib52); Lyu et al.,
    [2023](#bib.bib28)), the specific aspect of breaking down complex questions into
    simpler components has received limited attention. The creation of specialized
    datasets and benchmarks is integral to advancing the field of Deep Learning (Guss
    et al., [2019](#bib.bib13); Vinyals et al., [2019](#bib.bib47); Fu et al., [2020](#bib.bib9);
    Kurenkov et al., [2023](#bib.bib24)). This work addresses the gap in understanding
    and exploration of the reasoning sub-questioning process by providing a dataset
    and baselines for further research in this direction.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在复杂推理领域，将复杂问题分解为一系列更简单的子问题的能力是一个重要但尚未充分研究的组件（Shridhar et al.，[2022](#bib.bib36)）。虽然现有工作主要关注推理的端到端解决方案（Zhou
    et al.，[2022](#bib.bib52)；Lyu et al.，[2023](#bib.bib28)），但将复杂问题拆解为更简单的组件的具体方面却受到有限关注。创建专门的数据集和基准对于推动深度学习领域的发展至关重要（Guss
    et al.，[2019](#bib.bib13)；Vinyals et al.，[2019](#bib.bib47)；Fu et al.，[2020](#bib.bib9)；Kurenkov
    et al.，[2023](#bib.bib24)）。这项工作通过提供数据集和基准来填补对推理子问题过程的理解和探索空白，为进一步研究提供支持。
- en: Compounding the challenge is the computational overhead associated with large
    model sizes, making reasoning tasks computationally expensive and time-consuming
    when tuning models. Concurrently, approaches similar to Chain-of-Thoughts (CoT)
    may incur expenses, given that models with superior reasoning abilities are not
    available for free. In response, distilling distinct components of the reasoning
    process into smaller models emerges as a promising avenue for research. Decomposition,
    particularly in the context of teaching smaller models, proves advantageous due
    to their cost-effectiveness, reduced computational requirements, and accessibility.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大模型尺寸带来的计算开销增加了挑战，使得在调整模型时推理任务变得计算昂贵且耗时。同时，类似于链式思维（CoT）的方法可能会产生费用，因为具有更好推理能力的模型并非免费提供。作为回应，将推理过程的不同组件提炼成较小的模型成为一种有前途的研究方向。特别是在教授较小模型的背景下，**分解**的优势在于其成本效益、降低的计算需求和可及性。
- en: Reinforcement Learning (RL) has demonstrated remarkable success across various
    domains with recent success in the NLP domain Bai et al. ([2022a](#bib.bib3);
    [b](#bib.bib4)); OpenAI ([2023](#bib.bib31)); Team et al. ([2023](#bib.bib43)).
    However, some of the most popular approaches like Reinforcement Learning with
    Human Feedback (RLHF) (Ouyang et al., [2022](#bib.bib32)) demand substantial data,
    and online approaches require extensive interactions with the environment. Offline
    RL (Levine et al., [2020](#bib.bib26)), an alternative that utilizes data directly,
    holds potential with limited dataset sizes. This subfield has recently witnessed
    a surge in development, leading to a growth of diverse approaches (Kumar et al.,
    [2020](#bib.bib22); Fujimoto & Gu, [2021](#bib.bib10); Kostrikov et al., [2021](#bib.bib21);
    An et al., [2021](#bib.bib2); Akimov et al., [2022](#bib.bib1); Yang et al., [2022](#bib.bib50);
    Ghasemipour et al., [2022](#bib.bib11); Nikulin et al., [2023](#bib.bib30)). Numerous
    successful applications of offline RL exist in fields like robotics (Smith et al.,
    [2022](#bib.bib38); Kumar et al., [2021](#bib.bib23)), autonomous driving (Diehl
    et al., [2021](#bib.bib8)), recommendation systems(Chen et al., [2022](#bib.bib6))
    and even drug-design (Tarasov et al., [2023b](#bib.bib42)). For the NLP domain,
    recent studies have intriguingly revealed that AI feedback closely resembles human
    feedback (Lee et al., [2023](#bib.bib25)), and language models can be fine-tuned
    using their own generated feedback (Bai et al., [2022b](#bib.bib4)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）在多个领域取得了显著成功，最近在自然语言处理领域也取得了成功（Bai 等，[2022a](#bib.bib3); [b](#bib.bib4)）；OpenAI
    ([2023](#bib.bib31))；Team 等 ([2023](#bib.bib43))。然而，一些最受欢迎的方法，如带有人类反馈的强化学习（RLHF）（Ouyang
    等，[2022](#bib.bib32)），需要大量数据，而在线方法则需要与环境进行广泛的互动。离线强化学习（Levine 等，[2020](#bib.bib26)）是一种直接利用数据的替代方案，在数据集规模有限的情况下具有潜力。这个子领域最近经历了快速发展，出现了多种不同的方法（Kumar
    等，[2020](#bib.bib22); Fujimoto & Gu，[2021](#bib.bib10); Kostrikov 等，[2021](#bib.bib21);
    An 等，[2021](#bib.bib2); Akimov 等，[2022](#bib.bib1); Yang 等，[2022](#bib.bib50);
    Ghasemipour 等，[2022](#bib.bib11); Nikulin 等，[2023](#bib.bib30)）。离线强化学习在机器人领域（Smith
    等，[2022](#bib.bib38); Kumar 等，[2021](#bib.bib23)）、自动驾驶（Diehl 等，[2021](#bib.bib8)）、推荐系统（Chen
    等，[2022](#bib.bib6)）甚至药物设计（Tarasov 等，[2023b](#bib.bib42)）中有许多成功的应用。在自然语言处理领域，最近的研究有趣地揭示了人工智能反馈与人类反馈非常相似（Lee
    等，[2023](#bib.bib25)），而语言模型可以利用自身生成的反馈进行微调（Bai 等，[2022b](#bib.bib4)）。
- en: 'In this preliminary work, we combine the advantages of sub-questioning with
    offline RL with feedback for the task of mathematical reasoning and distill the
    subquestion decomposition abilities in smaller models. Our work proposes the following:
    1) an AI-generated benchmark where math questions are broken down into simpler
    sub-questions based on the GSM8K (Cobbe et al., [2021](#bib.bib7)) dataset, 2)
    train smaller language models for the same task using fine-tuning and offline
    RL techniques to provide baselines for the task, and 3) explore the potential
    benefits of using AI-generated feedback on its own responses in enhancing model
    performance. Our experimental results reveal a big gap between ChatGPT’s reasoning
    abilities and what can be achieved with smaller models and existing algorithmical
    approaches.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项初步工作中，我们将子问题化与离线 RL 反馈的优势结合，用于数学推理任务，并将子问题分解能力蒸馏到更小的模型中。我们的工作提出了以下内容：1) 一个
    AI 生成的基准，其中数学问题被基于 GSM8K (Cobbe 等人，[2021](#bib.bib7)) 数据集拆解成更简单的子问题；2) 使用微调和离线
    RL 技术训练较小的语言模型，以为该任务提供基准；3) 探索使用 AI 生成的反馈对其自身响应的潜在好处，以提高模型性能。我们的实验结果揭示了 ChatGPT
    的推理能力与较小模型和现有算法方法可以达到的水平之间的巨大差距。
- en: 2 Related Work
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 LM Distillation
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LM 蒸馏
- en: Distillation has emerged as a pivotal technique in mitigating the computational
    challenges associated with LLMs while retaining their valuable knowledge. Notably,
    Hinton et al. ([2015](#bib.bib15)) introduced knowledge distillation as a means
    to transfer the knowledge from a complex model to a simpler one, enabling the
    latter to approximate the former’s performance. In the context of language models,
    Sanh et al. ([2019](#bib.bib35)) successfully distilled BERT, a prominent LLM,
    into a more compact version named DistilBERT, maintaining competitive performance
    with significantly fewer parameters. Recently, a lot of work has successfully
    distilled the reasoning capabilities into smaller models Yuan et al. ([2023](#bib.bib51));
    Magister et al. ([2023](#bib.bib29)); Shridhar et al. ([2023](#bib.bib37)); Hsieh
    et al. ([2023](#bib.bib16))
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏技术已经成为缓解与大语言模型（LLMs）相关的计算挑战的重要方法，同时保留其宝贵的知识。值得注意的是，Hinton 等人 ([2015](#bib.bib15))
    引入了知识蒸馏作为一种将知识从复杂模型转移到简单模型的方法，使后者能够接近前者的性能。在语言模型的背景下，Sanh 等人 ([2019](#bib.bib35))
    成功地将一个突出的 LLM BERT 蒸馏成一个更紧凑的版本，名为 DistilBERT，保持了竞争力的性能，并且参数显著减少。最近，许多工作成功地将推理能力蒸馏到更小的模型中，包括
    Yuan 等人 ([2023](#bib.bib51))；Magister 等人 ([2023](#bib.bib29))；Shridhar 等人 ([2023](#bib.bib37))；Hsieh
    等人 ([2023](#bib.bib16))。
- en: 2.2 LLMs Reasoning
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLMs 推理
- en: The exploration of reasoning capabilities in Large Language Models (LLMs) has
    been a focal point in recent NLP research. Wei et al. ([2022](#bib.bib49)) introduced
    Chain-of-Thoughts (CoT), a method compelling LLMs to approach problems in a step-by-step
    manner rather than providing direct answers. Noteworthy is their revelation that
    reasoning abilities manifest prominently in larger LM sizes. This technique involves
    prompting the model with step-by-step problem-solving and furnishing multiple
    examples for guidance through few-shot prompting. Building on this, Kojima et al.
    ([2022](#bib.bib20)) demonstrated that reasoning abilities may emerge even without
    examples, utilizing zero-shot prompting. Further refinement in reasoning quality
    was achieved by Wang et al. ([2022](#bib.bib48)), who showed that applying CoT
    multiple times and picking the most frequent answer boosts resulting performance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）中推理能力的探索已成为最近 NLP 研究的一个重点。Wei 等人 ([2022](#bib.bib49)) 引入了链式思维（CoT），这是一种迫使
    LLM 以逐步方式解决问题的方法，而不是直接提供答案。他们的发现值得注意的是，推理能力在较大的语言模型中表现尤为突出。这种技术涉及通过逐步解决问题来提示模型，并通过少量示例提供指导。基于此，Kojima
    等人 ([2022](#bib.bib20)) 展示了即使没有示例，推理能力也可能出现，利用零样本提示。Wang 等人 ([2022](#bib.bib48))
    通过展示多次应用 CoT 并选择最频繁的答案能够提升推理质量，进一步提高了推理质量。
- en: 2.3 Sub-Questioning
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 子问题化
- en: The paradigm of splitting reasoning into sub-question generation and sub-question
    answering has proven beneficial in solving reasoning problems (Shridhar et al.,
    [2022](#bib.bib36)). Notably, Shridhar et al. ([2023](#bib.bib37)) took a step
    further by distilling LLMs reasoning abilities into substantially smaller models
    through finetuning, laying the groundwork for our current work. It is essential
    to note that both of these works primarily address the entire reasoning process
    rather than dissecting its individual components. In a parallel thread of research,
    Zhou et al. ([2022](#bib.bib52)) demonstrated that decomposing reasoning problems
    into sub-problems results in improved LLM performance. Their findings indicate
    that LLMs can execute this split step-by-step, outperforming CoT-based approaches.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 将推理过程分解为子问题生成和子问题回答的范式在解决推理问题中已经证明是有益的（Shridhar 等，[2022](#bib.bib36)）。值得注意的是，Shridhar
    等（[2023](#bib.bib37)）进一步通过微调将大型语言模型（LLMs）的推理能力提炼成显著更小的模型，为我们当前的工作奠定了基础。需要指出的是，这些研究主要关注整个推理过程，而不是剖析其各个组成部分。在另一系列研究中，Zhou
    等（[2022](#bib.bib52)）展示了将推理问题分解为子问题可以提高 LLM 的表现。他们的研究表明，LLMs 可以逐步执行这一分解步骤，优于基于
    CoT 的方法。
- en: Extending beyond mere decomposition, Juneja et al. ([2023](#bib.bib19)) introduced
    a finetuning approach where a “helper” LM poses questions to a “solver” LM, guiding
    it toward the correct solution based on the problem and the sequence of prior
    interactions. Despite showcasing the efficacy of this approach, its reliance on
    an online RL paradigm and the use of large 13 or 33 billion parameters LLaMA models
    (Touvron et al., [2023](#bib.bib44)) poses practical challenges. The inherent
    computational demands and potential instability of online RL, compounded by the
    necessity for significant interactions and potential cost implications, underscore
    the complexities associated with this method.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 超越单纯的分解，Juneja 等（[2023](#bib.bib19)）引入了一种微调方法，其中一个“助手”语言模型向一个“求解者”语言模型提出问题，指导其根据问题和先前交互的序列找到正确的解决方案。尽管展示了这种方法的有效性，但其依赖于在线
    RL 范式以及使用大规模的 13 或 33 亿参数的 LLaMA 模型（Touvron 等，[2023](#bib.bib44)）带来了实际挑战。在线 RL
    的固有计算需求和潜在的不稳定性，加上需要大量的交互和可能的成本影响，突显了这一方法的复杂性。
- en: 3 Preliminaries
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 前言
- en: 3.1 Language Modeling
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 语言建模
- en: Our study leverages the preeminence of attention-based Transformer architectures,
    as introduced by Vaswani et al. ([2017](#bib.bib46)). In the realm of autoregressive
    models, we turn our attention to the widely adopted GPT-2 architecture (Radford
    et al., [2019](#bib.bib34)), specifically selecting models of various sizes to
    tailor our experiments. The chosen finetuning strategies center around the application
    of standard cross-entropy loss, optimizing the models for accurate next-token
    prediction. For every approach we provide mathematical problem as a prefix and
    expect model to generate the sub-questions required to solve the problem.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究利用了基于注意力的 Transformer 架构的卓越性，该架构由 Vaswani 等（[2017](#bib.bib46)）引入。在自回归模型领域，我们关注广泛采用的
    GPT-2 架构（Radford 等，[2019](#bib.bib34)），特别是选择了各种规模的模型以定制我们的实验。所选择的微调策略围绕应用标准的交叉熵损失展开，优化模型以进行准确的下一个标记预测。对于每种方法，我们提供一个数学问题作为前缀，并期望模型生成解决该问题所需的子问题。
- en: 3.2 Offline RL task formulation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 离线 RL 任务公式化
- en: Given the recent advancements in offline RL, we incorporate a baseline from
    this field. Building upon the formulation proposed by Snell et al. ([2022](#bib.bib39)),
    we cast the text generation problem as a token-level Partially Observable Markov
    Decision Process (POMDP). In this framework, the agent’s observations correspond
    to prefixes of tokens, and the agent’s action pertains to the selection of the
    next token to be generated.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于近期在离线 RL 方面的进展，我们在这一领域中引入了一个基准。基于 Snell 等（[2022](#bib.bib39)）提出的公式化方法，我们将文本生成问题视为一个令牌级别的部分可观察马尔可夫决策过程（POMDP）。在这一框架中，代理的观察对应于令牌的前缀，代理的行动则涉及选择下一个生成的令牌。
- en: 4 GSM8K-AI-SubQ Dataset
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 GSM8K-AI-SubQ 数据集
- en: In the pursuit of advancing research in reasoning within Language Models (LMs),
    we introduce the GSM8K-AI-SubQ dataset, uniquely designed to emphasize sub-questioning
    and leverage AI-generated feedback for these sub-questions. Our inspiration comes
    from the works of Bai et al. ([2022b](#bib.bib4)) and Lee et al. ([2023](#bib.bib25)).
    The first work provide compelling evidence that such feedback can serve as a valuable
    signal for fine-tuning models. The second work shows that AI-generated data exhibits
    comparable quality to human-annotated data in the context of preferences. We hope
    that our dataset will provide any insights whether similar properties hold for
    the reasoning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在推动语言模型（LMs）推理研究的过程中，我们介绍了GSM8K-AI-SubQ数据集，该数据集独特地设计用于强调子问题并利用AI生成的反馈。我们的灵感来自于Bai等人的工作（[2022b](#bib.bib4)）和Lee等人的工作（[2023](#bib.bib25)）。第一项工作提供了有力的证据表明，这种反馈可以作为微调模型的有价值信号。第二项工作表明，在偏好方面，AI生成的数据与人类标注的数据质量相当。我们希望我们的数据集能提供任何类似属性在推理中的有效性。
- en: While acknowledging that AI models may not offer the ideal source of reasoning
    data, we posit the substantial benefits of such a dataset. Firstly, it serves
    as a valuable resource for distilling reasoning abilities into smaller models,
    aligning with the growing trend of efficiency in language model architectures.
    Importantly, even if the data exhibits suboptimal characteristics, we draw inspiration
    from the success of offline RL in other fields, where it has demonstrated an ability
    to outperform the policy responsible for dataset collection. This resilience to
    suboptimal data quality mitigates concerns and underscores the dataset’s potential
    impact on advancing reasoning capabilities. Lastly, the expedited and cost-effective
    nature of this data acquisition method democratizes its accessibility, allowing
    researchers to extend our dataset if required.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管承认AI模型可能不是理想的推理数据来源，但我们认为这样的数据集具有巨大的好处。首先，它作为一个宝贵的资源，用于将推理能力提炼到更小的模型中，符合语言模型架构中日益增长的效率趋势。重要的是，即使数据表现出次优特征，我们也从离线RL在其他领域的成功中获得了灵感，在这些领域它已经展示了超越数据集收集政策的能力。这种对次优数据质量的韧性缓解了担忧，并强调了数据集在推动推理能力方面的潜在影响。最后，这种数据获取方法的快速和成本效益特性使其可及性得到民主化，允许研究人员在需要时扩展我们的数据集。
- en: For the sake of completeness, we furnish LLM responses for the generated sub-questions,
    although our primary focus in this work centers on the sub-questioning. This dataset
    not only facilitates advancements in LM reasoning but also lays the groundwork
    for innovative approaches in the broader landscape of language model research.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们提供了生成子问题的LLM响应，尽管我们在此工作中的主要关注点是子问题。这个数据集不仅促进了LM推理的进展，还为语言模型研究的更广泛领域铺平了创新方法的基础。
- en: 4.1 Dataset Collection Process
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集收集过程
- en: In our dataset collection process, we leverage the capabilities of ChatGPT,
    specifically utilizing the gpt-3.5-turbo-0613³³3The most recent gpt-3.5-turbo
    version version. This version is chosen due to its lenient restrictions on the
    number of queries and its cost-effectiveness, offering responses of commendable
    quality when benchmarked against similar LLMs (Tunstall et al., [2023](#bib.bib45)).
    Interactions with ChatGPT are facilitated through the OpenAI API⁴⁴4[https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview),
    employing a temperature setting of 0.7 to ensure a diverse range of responses.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集收集过程中，我们利用了ChatGPT的能力，特别是使用gpt-3.5-turbo-0613³³3最新的gpt-3.5-turbo版本。选择这个版本是因为它对查询数量的限制较为宽松且成本效益高，提供的响应在与类似LLMs（Tunstall
    et al., [2023](#bib.bib45)）比较时质量良好。通过OpenAI API⁴⁴4[https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview)与ChatGPT的互动，采用0.7的温度设置以确保响应的多样性。
- en: For the specific task at hand, we curate a dataset of mathematical problems
    extracted from the GSM8K dataset (Cobbe et al., [2021](#bib.bib7)). This dataset
    comprises a diverse array of grade school math word problems meticulously crafted
    by human problem writers. While designed to be solvable by a middle school student,
    these problems present a noteworthy challenge for LMs. Maintaining the integrity
    of the original train/test split, our dataset consists of 7473 training examples
    and 1319 testing examples.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于当前的具体任务，我们从 GSM8K 数据集（Cobbe et al., [2021](#bib.bib7)）中策划了一个数学问题数据集。该数据集包括一系列由人工问题编写者精心设计的各种年级的数学文字问题。尽管设计上可以由中学生解决，但这些问题对语言模型提出了显著的挑战。保持原始训练/测试拆分的完整性，我们的数据集包含
    7473 个训练示例和 1319 个测试示例。
- en: Sub-questions Generation. The initial phase of our data generation involves
    creation of set of sub-questions for each problem within the GSM8K dataset. We
    prompt ChatGPT to decompose the given problem into distinct sub-problems and provide
    corresponding sub-questions, aiming to elucidate the problem-solving process for
    others. To guide the model and ensure a consistent output format, we furnish two
    examples of problems and their corresponding sub-questions as demonstration inputs.
    Each request is treated as an independent dialogue to eliminate potential interference.
    We keep a 2-shot prompting strategy and preserving independence for all subsequent
    queries. A sample input and output for this sub-question generation process are
    illustrated in [Table 10](#A3.T10 "Table 10 ‣ Appendix C ChatGPT Prompts ‣ Distilling
    LLMs’ Decomposition Abilities into Compact Language Models"). To enhance the dataset’s
    size and diversity, we repeat the inquiry for each problem three times, resulting
    in 22,419 training samples. While the repetition could be further increased for
    creating a larger dataset, we acknowledge budget constraints as a limiting factor
    in this study.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 子问题生成。数据生成的初始阶段涉及为 GSM8K 数据集中的每个问题创建一组子问题。我们提示 ChatGPT 将给定问题分解为不同的子问题，并提供相应的子问题，旨在阐明他人的问题解决过程。为了指导模型并确保一致的输出格式，我们提供了两个问题及其相应子问题的示例作为演示输入。每个请求被视为独立的对话，以消除潜在的干扰。我们保持
    2-shot 提示策略，并为所有后续查询保留独立性。该子问题生成过程的输入和输出示例在[表10](#A3.T10 "Table 10 ‣ Appendix
    C ChatGPT Prompts ‣ Distilling LLMs’ Decomposition Abilities into Compact Language
    Models")中进行了说明。为了增强数据集的大小和多样性，我们对每个问题进行了三次询问，结果得到了 22,419 个训练样本。虽然重复次数可以进一步增加以创建更大的数据集，但我们承认预算限制是本研究中的一个限制因素。
- en: Answers Generation. In the subsequent phase of our data collection, we focused
    on generating responses to the previously obtained sub-questions. Tasking ChatGPT
    solely with the responsibility of providing answers to these queries and obtaining
    a final answer for the original problem. An illustrative example of this prompt-response
    interaction is presented in [Table 11](#A3.T11 "Table 11 ‣ Appendix C ChatGPT
    Prompts ‣ Distilling LLMs’ Decomposition Abilities into Compact Language Models").
    The generated final answers were then employed as labels to categorize each set
    of sub-questions. Sub-questions associated with an original problem solution were
    identified as ”good”, while those failing to lead to a resolution were categorized
    as ”bad”.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 答案生成。在我们数据收集的后续阶段，我们专注于生成对之前获得的子问题的响应。将 ChatGPT 单独负责提供这些查询的答案，并为原始问题获取最终答案。一个关于这种提示-响应互动的示例呈现在[表11](#A3.T11
    "Table 11 ‣ Appendix C ChatGPT Prompts ‣ Distilling LLMs’ Decomposition Abilities
    into Compact Language Models")中。生成的最终答案随后被用作标签，以对每组子问题进行分类。与原始问题解决方案相关的子问题被标识为“好”，而那些未能导致解决方案的子问题则被分类为“坏”。
- en: Feedback Generation. Concluding our dataset collection, we engage in the pivotal
    task of soliciting feedback from ChatGPT on its generated sub-questions. The objective
    is to introduce a nuanced signal at the individual question level, mitigating
    sparsity concerns associated with relying solely on the correctness of final answers.
    This aspect assumes significance, especially in the context of potential RL applications.
    Drawing inspiration from the effective sub-questioning strategy proposed by Shridhar
    et al. ([2022](#bib.bib36)), we task ChatGPT with determining the usefulness of
    each sub-question in the problem-solving process. Refer to [Table 12](#A3.T12
    "Table 12 ‣ Appendix C ChatGPT Prompts ‣ Distilling LLMs’ Decomposition Abilities
    into Compact Language Models") for an exemplary interaction. To account for potential
    inconsistencies in ChatGPT’s feedback, we query the model three times for each
    set of sub-questions. This repetition strategy aligns with findings in recent
    studies (Wang et al., [2022](#bib.bib48); Lee et al., [2023](#bib.bib25)), showing
    that leveraging multiple responses often results in more useful data. It’s worth
    noting that the repetition factor can be adjusted for further data refinement.
    Subsequently, scores for each sub-question are computed by evaluating the fraction
    of responses deeming it useful, establishing a metric for assessing the efficacy
    of individual sub-questions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈生成。在完成数据集收集后，我们进行一个关键任务，即征求ChatGPT对其生成的子问题的反馈。其目的是在单个问题层面引入细微的信号，减轻仅依赖最终答案正确性所带来的稀疏性问题。尤其是在潜在的强化学习应用背景下，这一点显得尤为重要。我们从Shridhar等人提出的有效子问题策略（[2022](#bib.bib36)）中获得灵感，要求ChatGPT确定每个子问题在解决问题过程中的有用性。有关示例互动，请参见[Table
    12](#A3.T12 "Table 12 ‣ Appendix C ChatGPT Prompts ‣ Distilling LLMs’ Decomposition
    Abilities into Compact Language Models")。为了考虑ChatGPT反馈中的潜在不一致性，我们对每组子问题进行三次查询。这种重复策略与近期研究（Wang等，[2022](#bib.bib48)；Lee等，[2023](#bib.bib25)）的发现一致，表明利用多个响应通常会得到更有用的数据。值得注意的是，重复因素可以调整，以进一步细化数据。随后，通过评估认为子问题有用的响应比例来计算每个子问题的分数，从而建立评估单个子问题有效性的指标。
- en: Dataset Collection Costs. The compilation of our training set demanded approximately
    12 hours of real-time investment and incurred a cost of approximately $100 for
    utilizing the gpt-3.5-turbo version. Notably, feedback generation constituted
    around 70% of the total costs, primarily attributed to the necessity of acquiring
    feedback multiple times for each set of questions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集收集成本。我们训练集的编制大约需要12小时的实时投入，并且使用gpt-3.5-turbo版本的费用约为$100。值得注意的是，反馈生成占总成本的约70%，主要由于需要多次获取每组问题的反馈。
- en: 4.2 Dataset Analysis
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 数据集分析
- en: '| Metric | 0 correct | 1 correct | 2 correct | 3 correct |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 0个正确 | 1个正确 | 2个正确 | 3个正确 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Number of problems | 1343 (18%) | 866 (11%) | 1139 (15%) | 4052 (54%) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 问题数量 | 1343 (18%) | 866 (11%) | 1139 (15%) | 4052 (54%) |'
- en: '| Mean problem length | 269.4 $\pm$ 82.9 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 平均问题长度 | 269.4 $\pm$ 82.9 |'
- en: '| Median problem length | 250 | 235.5 | 225 | 201.5 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 中位数问题长度 | 250 | 235.5 | 225 | 201.5 |'
- en: 'Table 1: Train set statistics for problems splitted by the number of sub-question
    sets which lead to the correct answer. ”Problem length” denotes number of characters.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：按导致正确答案的子问题集数量拆分的问题训练集统计数据。“问题长度”表示字符数。
- en: In this section, we present an analysis of the collected training data, offering
    valuable statistics and insights.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们展示了收集到的训练数据的分析，提供了有价值的统计数据和见解。
- en: The distribution of sub-question set sizes is visualized in [1(a)](#S4.F1.sf1
    "1(a) ‣ Figure 1 ‣ 4.2 Dataset Analysis ‣ 4 GSM8K-AI-SubQ Dataset ‣ Distilling
    LLMs’ Decomposition Abilities into Compact Language Models"). While the majority
    of sets comprise 2 to 6 questions, some outliers exist, featuring either a single
    question or more than 6\. Notably, we opt to retain these outliers in our dataset
    for a more diverse representation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 子问题集大小的分布在[1(a)](#S4.F1.sf1 "1(a) ‣ Figure 1 ‣ 4.2 Dataset Analysis ‣ 4 GSM8K-AI-SubQ
    Dataset ‣ Distilling LLMs’ Decomposition Abilities into Compact Language Models")中可视化。虽然大多数集合包含2到6个问题，但也存在一些异常值，它们包含一个问题或超过6个问题。值得注意的是，我们选择保留这些异常值，以便数据集更加多样化。
- en: Our analysis extends to evaluating the efficacy of sub-question sets in leading
    to the correct solution for each problem. As outlined in [Table 1](#S4.T1 "Table
    1 ‣ 4.2 Dataset Analysis ‣ 4 GSM8K-AI-SubQ Dataset ‣ Distilling LLMs’ Decomposition
    Abilities into Compact Language Models"), approximately 54% of the sets consistently
    resulted in a correct solution across all three attempts, while only around 18%
    failed to yield a correct solution in any instance. Moreover, longer problems
    appear to pose increased difficulty, aligning intuitively with expectations. Calculating
    average accuracy by treating each sub-question set as an independent problem,
    the overall accuracy for the training set stands at 0.68.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析还包括评估子问题集在得出每个问题正确解决方案中的有效性。如[表 1](#S4.T1 "Table 1 ‣ 4.2 Dataset Analysis
    ‣ 4 GSM8K-AI-SubQ Dataset ‣ Distilling LLMs’ Decomposition Abilities into Compact
    Language Models")所述，大约 54% 的子问题集在所有三次尝试中始终得到正确解决方案，而只有约 18% 的子问题集在任何情况下均未能得出正确解决方案。此外，较长的问题似乎更具挑战性，这与预期相符。通过将每组子问题视为一个独立的问题计算平均准确率，训练集的总体准确率为
    0.68。
- en: '![Refer to caption](img/c758b7d946ef58bd2c305b46dcd984a4.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c758b7d946ef58bd2c305b46dcd984a4.png)'
- en: (a)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/062e41e490fcb8432814009497075323.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/062e41e490fcb8432814009497075323.png)'
- en: (b)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/1773e6f9cedc6920e5daa6f54a781528.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1773e6f9cedc6920e5daa6f54a781528.png)'
- en: (c)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: '![Refer to caption](img/c960f7a2df4d6c31d714617619b518ea.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c960f7a2df4d6c31d714617619b518ea.png)'
- en: (d)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (d)
- en: 'Figure 1: Number of questions distributions in train set. (a) Distribution
    in the entire train set with mean 4.0 $\pm$ 1.5 and median 4, (b, c, d) Comparisons
    of distributions in number of questions between problems with 3 vs. 0, 1, 2 sets
    of sub-questions that lead to the correct final answer.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：训练集中的问题数量分布。(a) 训练集中的分布，均值为 4.0 $\pm$ 1.5，中位数为 4；(b, c, d) 比较了包含 3 个子问题与
    0、1、2 个子问题的分布，这些子问题导致正确的最终答案。
- en: 'To delve deeper into the relationship between problem complexity and sub-question
    count, we compare distributions for 0, 1, and 2 out of 3 correct answers versus
    3 out of 3 correct answers in [Figure 1](#S4.F1 "Figure 1 ‣ 4.2 Dataset Analysis
    ‣ 4 GSM8K-AI-SubQ Dataset ‣ Distilling LLMs’ Decomposition Abilities into Compact
    Language Models") b, c, d. The analysis reveals a discernible trend: easier problems
    tend to be associated with a smaller number of sub-questions.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入了解问题复杂性与子问题数量之间的关系，我们比较了[图 1](#S4.F1 "Figure 1 ‣ 4.2 Dataset Analysis ‣
    4 GSM8K-AI-SubQ Dataset ‣ Distilling LLMs’ Decomposition Abilities into Compact
    Language Models") b, c, d 中 0、1、2 个正确答案与 3 个正确答案的分布。分析揭示出一个明显的趋势：较简单的问题往往与较少的子问题数量相关。
- en: Continuing our exploration, we investigate whether ChatGPT’s feedback on its
    own sub-questions holds intrinsic value and diverges from random noise.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续探索的过程中，我们调查了 ChatGPT 对自身子问题的反馈是否具有内在价值，并且是否与随机噪音有所区别。
- en: The distribution of usefulness scores for each sub-question, depicted in [2(a)](#S4.F2.sf1
    "2(a) ‣ Figure 2 ‣ 4.2 Dataset Analysis ‣ 4 GSM8K-AI-SubQ Dataset ‣ Distilling
    LLMs’ Decomposition Abilities into Compact Language Models"), indicates that approximately
    90% of sub-questions received consistent markings as useful across all three iterations.
    However, to gain a more nuanced perspective, we average the usefulness scores
    for each sub-question set and showcase the distribution of averaged confidences
    in [2(b)](#S4.F2.sf2 "2(b) ‣ Figure 2 ‣ 4.2 Dataset Analysis ‣ 4 GSM8K-AI-SubQ
    Dataset ‣ Distilling LLMs’ Decomposition Abilities into Compact Language Models").
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 每个子问题的有用性评分分布，如[2(a)](#S4.F2.sf1 "2(a) ‣ Figure 2 ‣ 4.2 Dataset Analysis ‣ 4
    GSM8K-AI-SubQ Dataset ‣ Distilling LLMs’ Decomposition Abilities into Compact
    Language Models")所示，表明大约 90% 的子问题在所有三次迭代中均被标记为有用。然而，为了获得更细致的视角，我们对每组子问题的有用性评分进行平均，并展示[2(b)](#S4.F2.sf2
    "2(b) ‣ Figure 2 ‣ 4.2 Dataset Analysis ‣ 4 GSM8K-AI-SubQ Dataset ‣ Distilling
    LLMs’ Decomposition Abilities into Compact Language Models")中的平均置信度分布。
- en: '![Refer to caption](img/3c48c25581bd25e3fc34c8bdf34fdc53.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/3c48c25581bd25e3fc34c8bdf34fdc53.png)'
- en: (a)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/997364d3e81b704beae8afbd5b79e94b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/997364d3e81b704beae8afbd5b79e94b.png)'
- en: (b)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/1e9fad13bf44db1884ab942d31158fd9.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1e9fad13bf44db1884ab942d31158fd9.png)'
- en: (c)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 2: (a) distribution of the usefulness feedback over individual sub-questions,
    (b) distribution of the usefulness feedback averaged over sets of sub-questions,
    (c) confusion matrix between the presence of negative feedback in the set of sub-questions
    and correctness of the final answer based on corresponding sub-questions.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：（a）个别子问题的有用性反馈分布，（b）子问题集合的有用性反馈的平均分布，（c）子问题集合中负面反馈的存在与基于相应子问题的最终答案正确性的混淆矩阵。
- en: Upon examining the fraction of problems with at least one negative response,
    we find that 36% of problems fall into this category, aligning with the 54% that
    were correctly solved 3 out of 3 times. Recognizing that a single misstep in the
    reasoning process can lead to an incorrect answer, we employ the presence of negative
    feedback as an indicator for predicting an incorrect solution. However, this indicator
    comes with limitations, such as the potential for false negatives arising from
    the model providing the correct final answer despite a flawed reasoning step,
    and the occasional presence of randomness during feedback collection. The confusion
    matrix in [2(c)](#S4.F2.sf3 "2(c) ‣ Figure 2 ‣ 4.2 Dataset Analysis ‣ 4 GSM8K-AI-SubQ
    Dataset ‣ Distilling LLMs’ Decomposition Abilities into Compact Language Models")
    outlines this setup, showcasing differences from a random scenario. Notably, when
    negative feedback is absent, 72% of problems were actually solved correctly, compared
    to 62% when negative feedback is present.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查至少有一个负面回应的问题的比例时，我们发现 36% 的问题属于这一类别，这与正确解决 3 次中的 54% 相符。认识到推理过程中的一次失误可能导致错误答案，我们使用负面反馈的存在作为预测错误解决方案的指标。然而，这个指标有局限性，例如模型尽管有推理步骤错误但仍提供正确最终答案的假阴性风险，以及反馈收集过程中偶尔出现的随机性。[2(c)](#S4.F2.sf3
    "2(c) ‣ 图 2 ‣ 4.2 数据集分析 ‣ 4 GSM8K-AI-SubQ 数据集 ‣ 将 LLM 的分解能力提炼成紧凑的语言模型") 中的混淆矩阵展示了这种设置，显示出与随机场景的差异。值得注意的是，当缺少负面反馈时，72%
    的问题实际上被正确解决，而当负面反馈存在时为 62%。
- en: To quantitatively assess the performance of this heuristic, we compute the ROC
    AUC score, treating the average sub-question set usefulness as a probability and
    designating an incorrect answer as the ”target label”. The resulting ROC AUC score
    of 0.56 indicates a departure from randomness (0.5). Further, the Pearson correlation
    coefficient, calculated at -0.09 with a p-value of $10^{-46}$, suggests that a
    higher usefulness score corresponds to a decreased likelihood of encountering
    an incorrect answer. Despite the simplicity of this heuristic, it provides informative
    cues on the efficacy of the sub-question set, indicating its potential utility.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定量评估这个启发式方法的性能，我们计算了 ROC AUC 分数，将平均子问题集的有用性视为概率，并将错误答案指定为“目标标签”。结果 ROC AUC
    分数为 0.56，表明与随机性（0.5）有所偏离。此外，Pearson 相关系数为 -0.09，p 值为 $10^{-46}$，这表明较高的有用性分数对应于更低的错误答案可能性。尽管这个启发式方法很简单，但它提供了关于子问题集有效性的有用线索，显示了其潜在的实用性。
- en: 5 Baselines
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 基准
- en: In our exploration of the constructed dataset, we conduct a series of experiments
    utilizing both supervised techniques and an offline RL approach. Our experiments
    involve the deployment of pretrained small and medium versions of GPT-2 (Radford
    et al., [2019](#bib.bib34)) as well as pretrained DistilGPT (Sanh et al., [2019](#bib.bib35))
    to gauge performance dynamics across various models sizes. The GPT-2 medium model,
    with 345 million parameters, represents the largest model in our experiments.
    This choice is deliberate, as larger models endowed with reasoning abilities often
    exceed 7 billion parameters, rendering their usage complex for many researchers.
    Unfortunately, due to computational resource limitations, our experiments did
    not extend to GPT-2 large and XL models.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对构建的数据集的探索中，我们进行了一系列实验，利用了监督技术和离线 RL 方法。我们的实验涉及使用预训练的小型和中型 GPT-2 版本（Radford
    等，[2019](#bib.bib34)）以及预训练的 DistilGPT（Sanh 等，[2019](#bib.bib35)）来评估不同模型规模的性能动态。GPT-2
    中型模型拥有 3.45 亿个参数，是我们实验中最大的模型。这个选择是故意的，因为拥有推理能力的大型模型通常超过 70 亿个参数，使得许多研究人员的使用变得复杂。不幸的是，由于计算资源限制，我们的实验没有扩展到
    GPT-2 大型和 XL 模型。
- en: 5.1 Evaluation Protocol
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 评估协议
- en: To facilitate a robust comparison of different approaches, we employ the same
    version of ChatGPT utilized during data collection. Sub-questions generated by
    each approach are presented to ChatGPT using the same prompt from [Table 11](#A3.T11
    "Table 11 ‣ Appendix C ChatGPT Prompts ‣ Distilling LLMs’ Decomposition Abilities
    into Compact Language Models"), and we measure the accuracy of final answers as
    our evaluation metric. Standardizing the evaluation environment, we set the temperature
    to 0 and fix the random seed to ensure reproducibility and eliminate randomness
    during the evaluation process. This rigorous evaluation protocol ensures a fair
    and consistent assessment of the performance across various approaches. One evaluation
    of test set costed approximately 1.5$ with gpt-3.5-turbo.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于不同方法的稳健比较，我们使用了数据收集期间采用的相同版本的 ChatGPT。由每种方法生成的子问题使用来自[表 11](#A3.T11 "表 11
    ‣ 附录 C ChatGPT 提示 ‣ 将 LLM 的分解能力提炼到紧凑的语言模型中")的相同提示呈现给 ChatGPT，并以最终答案的准确性作为评估指标。为了标准化评估环境，我们将温度设置为
    0，并固定随机种子，以确保可重复性并消除评估过程中的随机性。这一严格的评估协议确保了对各种方法性能的公平且一致的评估。对测试集的评估成本约为 1.5 美元，使用的是
    gpt-3.5-turbo。
- en: We additionally use the same protocol to evaluate results using open-source
    models Mistral 7B (Jiang et al., [2023](#bib.bib18)), LLaMA 7B and LLaMA 13B (Touvron
    et al., [2023](#bib.bib44)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用相同的协议来评估使用开源模型 Mistral 7B（Jiang 等人，[2023](#bib.bib18)）、LLaMA 7B 和 LLaMA
    13B（Touvron 等人，[2023](#bib.bib44)）的结果。
- en: 5.2 Applied Approaches
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 应用方法
- en: In this subsection, we provide a brief overview of each applied approach. For
    a more details, refer to [Appendix A](#A1 "Appendix A Experimental Details ‣ Distilling
    LLMs’ Decomposition Abilities into Compact Language Models").
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们提供了每种应用方法的简要概述。有关更多细节，请参考[附录 A](#A1 "附录 A 实验细节 ‣ 将 LLM 的分解能力提炼到紧凑的语言模型中")。
- en: Behavioral Cloning. Behavioral Cloning (BC) is a fundamental and robust approach
    commonly applied to datasets involving decision-making tasks. In the context of
    NLP, it translates to fine-tuning a language model to replicate a specific behavior
    or policy observed in the dataset. To select the best model, we employ a small
    held-out fraction (1%) of the training data. Following the methodology inspired
    by Shridhar et al. ([2022](#bib.bib36)), we use the BLEU score (Papineni et al.,
    [2002](#bib.bib33)) calculated between the generated questions and those produced
    by ChatGPT. Given that the primary goal of BC is to replicate the original policy’s
    behavior, BLEU serves as a suitable metric, indicating the similarity between
    two texts. Our empirical observation show that BLEU correlates with the final
    performance, making it a reasonable choice for model evaluation in the context
    of BC. For all subsequent approaches, the best BC model serves as the initialization
    for the LM.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 行为克隆。行为克隆（BC）是一种基础且稳健的方法，通常应用于涉及决策任务的数据集。在自然语言处理（NLP）中，它相当于微调语言模型，以复制数据集中观察到的特定行为或策略。为了选择最佳模型，我们使用了一小部分（1%）未参与训练的数据。按照
    Shridhar 等人（[2022](#bib.bib36)）的方法，我们使用 BLEU 分数（Papineni 等人，[2002](#bib.bib33)）来计算生成的问题与
    ChatGPT 生成的问题之间的相似度。鉴于 BC 的主要目标是复制原始策略的行为，BLEU 作为衡量两段文本相似度的指标非常适用。我们的经验观察表明，BLEU
    与最终表现相关，因此在 BC 的上下文中，它是模型评估的合理选择。在所有后续方法中，最佳 BC 模型作为语言模型的初始化。
- en: Filtered Behavioral Cloning. Filtered BC (Chen et al., [2021](#bib.bib5)) introduces
    a modification of BC by considering only a fraction of the best trajectories in
    the dataset. This approach proves particularly effective when a substantial number
    of high-quality examples are at disposal. In the context of our task, we exclusively
    retain samples corresponding to sub-question sets that result in the correct solution.
    The model selection process remains consistent with the standard BC approach.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤行为克隆。过滤 BC（Chen 等人，[2021](#bib.bib5)）通过仅考虑数据集中最优轨迹的一部分对 BC 进行了修改。这种方法在拥有大量高质量示例的情况下特别有效。在我们的任务中，我们仅保留那些对应于正确解答的子问题集的样本。模型选择过程与标准
    BC 方法保持一致。
- en: Implicit Language Q-Learning. Implicit Language Q-Learning (ILQL) (Snell et al.,
    [2022](#bib.bib39)) represents an adaptation of the offline RL approach known
    as IQL (Kostrikov et al., [2021](#bib.bib21)) to NLP tasks. The core idea behind
    ILQL involves training additional Value (V) and Q-function heads with IQL objectives.
    These additional functions are then employed to reweight the original LM outputs
    using the advantage value, which is the difference between V and Q values.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式语言 Q-学习。隐式语言 Q-学习（ILQL）(Snell et al., [2022](#bib.bib39)) 是将离线 RL 方法 IQL (Kostrikov
    et al., [2021](#bib.bib21)) 适配到 NLP 任务的一种改编。ILQL 的核心思想是使用 IQL 目标训练额外的价值 (V) 和
    Q-函数头。这些额外的函数随后被用来利用优势值对原始语言模型输出进行重加权，优势值是 V 值和 Q 值之间的差异。
- en: The selection of ILQL is motivated by the effectiveness of IQL as one of the
    strongest offline RL approaches in diverse domains (Tarasov et al., [2022](#bib.bib40)).
    Given the limited adaptation of offline RL approaches to NLP problems, ILQL emerges
    as the state-of-the-art choice. Given that IQL optimizes for rewards, which may
    not inherently correlate with the dataset policy, selecting the best model becomes
    challenging. In the absence of a clear best model selection criterion, we have
    tried to use the same criterion as we did for BC and the common offline RL practice
    of taking the latest checkpoint after training. The first method produced slightly
    better results on average.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 ILQL 是因为 IQL 在不同领域中的强大效果 (Tarasov et al., [2022](#bib.bib40))。由于离线 RL 方法在
    NLP 问题中的适应性有限，ILQL 成为最先进的选择。由于 IQL 优化奖励，而奖励可能与数据集策略并不固有相关，因此选择最佳模型变得具有挑战性。在没有明确最佳模型选择标准的情况下，我们尝试使用与
    BC 相同的标准，并采用离线 RL 常见的在训练后选择最新检查点的方法。第一种方法平均表现略好。
- en: 'Two versions of ILQL are tested in our experiments: ILQL-full utilizing all
    available rewards from the dataset and ILQL-sparse employing only answer correctness
    as rewards.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中测试了两种 ILQL 版本：ILQL-full 利用数据集中所有可用的奖励，而 ILQL-sparse 仅将答案正确性作为奖励。
- en: 5.3 Experimental Results
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 实验结果
- en: The experimental results, summarized in [Table 2](#S5.T2 "Table 2 ‣ 5.3 Experimental
    Results ‣ 5 Baselines ‣ Distilling LLMs’ Decomposition Abilities into Compact
    Language Models"), offer insights into the performance of the proposed approaches
    averaged over various answering LLMs. For a detailed breakdown based on specific
    answering models, refer to [Appendix B](#A2 "Appendix B Full Tabular Scores ‣
    Distilling LLMs’ Decomposition Abilities into Compact Language Models").
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果总结在 [表 2](#S5.T2 "表 2 ‣ 5.3 实验结果 ‣ 5 基准 ‣ 提炼 LLM 的分解能力为紧凑的语言模型")，提供了所提出方法在各种回答
    LLM 上的性能洞察。有关基于特定回答模型的详细数据，请参考 [附录 B](#A2 "附录 B 完整的表格评分 ‣ 提炼 LLM 的分解能力为紧凑的语言模型")。
- en: '| Algorithm | DistillGPT | GPT-2 small | GPT-2 medium | Average |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | DistillGPT | GPT-2 small | GPT-2 medium | 平均 |'
- en: '| BC | 0.255 | 0.284 | 0.310 | 0.283 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| BC | 0.255 | 0.284 | 0.310 | 0.283 |'
- en: '| Filtered BC | 0.260 | 0.293 | 0.319 | 0.291 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 过滤后的 BC | 0.260 | 0.293 | 0.319 | 0.291 |'
- en: '| ILQL-sparse | 0.249 | 0.281 | 0.310 | 0.280 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ILQL-sparse | 0.249 | 0.281 | 0.310 | 0.280 |'
- en: '| ILQL-full | 0.253 | 0.277 | 0.309 | 0.280 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ILQL-full | 0.253 | 0.277 | 0.309 | 0.280 |'
- en: '| ChatGPT | N/A | N/A | N/A | 0.429 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | N/A | N/A | N/A | 0.429 |'
- en: 'Table 2: Accuracy score of the final answer avereged over different models
    which were used for sub-questions answering. Best scores are highlighted with
    bold.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 最终答案的准确性评分，平均计算自用于子问题回答的不同模型。最佳评分以**粗体**突出显示。'
- en: It is evident that all tested approaches fall short when compared to the ChatGPT,
    indicating substantial room for improvement. Sub-questions generation abilities
    also improve with the size of the backbone model which alligns with previous researches.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，与 ChatGPT 相比，所有测试方法都表现不佳，这表明还有很大的改进空间。子问题生成能力也随着基础模型的规模而提高，这与之前的研究一致。
- en: Filtered BC demonstrates improved performance over standard BC in most scenarios,
    consistent with expectations when sufficient amount of high-quality demonstrations
    are present in the dataset. However, this trend is not universal, particularly
    when LLaMAs serve for question answering and GPT-2 medium is a backbone model
    for sub-question generation. Interestingly, Filtered BC outperforms BC only when
    GPT-2 medium is used alongside Mistral for answering.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤后的 BC 在大多数情况下比标准 BC 展现出更好的性能，这与当数据集中存在足够数量的高质量示例时的预期一致。然而，这一趋势并非普遍，特别是在 LLaMAs
    用于问题回答并且 GPT-2 medium 是子问题生成的基础模型时。有趣的是，只有在 GPT-2 medium 与 Mistral 一起用于回答时，过滤后的
    BC 才优于 BC。
- en: The comparison between ILQL-sparse and ILQL-full does not reveal a consistent
    advantage for either method. In most of the cases, both ILQL variants underperform
    Filtered BC, and even falling behind standard BC in half of the cases. However,
    the superiority of the Filtered BC over offline RL approaches in NLP was also
    recently demonstrated by Gulcehre et al. ([2023](#bib.bib12)). Our finding should
    solve as additional motivation for the development of offline RL algorithms for
    NLP.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ILQL-sparse 和 ILQL-full 之间的比较并未揭示出两种方法中的一致优势。在大多数情况下，这两种 ILQL 变体的表现均不如 Filtered
    BC，甚至在一半的情况下落后于标准 BC。然而，Filtered BC 相对于 NLP 中的离线 RL 方法的优越性最近也被 Gulcehre 等人 ([2023](#bib.bib12))
    证明了。我们的发现应作为开发 NLP 用离线 RL 算法的额外动力。
- en: 6 Limitations and Future Work.
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 局限性与未来工作。
- en: Our work serves as a foundational exploration, opening avenues for various future
    directions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作作为基础探索，开辟了多种未来研究方向。
- en: 'Development of Offline RL Approaches: A pivotal area for future exploration
    involves advancing offline RL or other suitable methodologies for distilling reasoning
    abilities from static datasets. This extension could contribute to more effective
    utilization of language models in reasoning tasks.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 离线 RL 方法的发展：未来探索的一个关键领域是推进离线 RL 或其他适合的方法，以从静态数据集中提炼推理能力。这一扩展可能有助于更有效地利用语言模型进行推理任务。
- en: 'Creation of a Larger Benchmark: Expanding our methodology, future work could
    focus on generating a more extensive benchmark as it requires only the access
    to ground truth-answers in the datset which usualy holds. This benchmark might
    incorporate a diverse set of reasoning datasets, such as MATH (Hendrycks et al.,
    [2021](#bib.bib14)) or AQuA (Ling et al., [2017](#bib.bib27)), providing a broader
    assessment of reasoning capabilities.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 创建更大规模的基准：扩展我们的方法论，未来工作可以集中于生成更广泛的基准，因为它只需要访问数据集中的真实答案，通常这是可以实现的。这一基准可能包含多样化的推理数据集，如
    MATH (Hendrycks et al., [2021](#bib.bib14)) 或 AQuA (Ling et al., [2017](#bib.bib27))，提供对推理能力的更广泛评估。
- en: 'Concentration on Sub-Question Answering: Delving deeper into the sub-question
    answering aspect of the reasoning process presents a promising direction. While
    our dataset includes ChatGPT responses for sub-questions, their scoring and utilization
    remain unexplored. Future studies could investigate this component to enhance
    understanding and performance.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 专注于子问题回答：深入探讨推理过程中的子问题回答方面，展现了一个有前途的方向。虽然我们的数据集中包括了 ChatGPT 对子问题的回答，但其评分和利用情况仍未被探索。未来的研究可以调查这一组件，以提升理解和表现。
- en: 'Utilization of Open-Source Models: Exploring the application of open-source
    models, such as LLaMA, for sub-question generation emerges as a cost-effective
    alternative. Accessible without financial constraints, these models present an
    opportunity for researchers to delve into sub-question generation without monetary
    limitations. We were not able to run such kind of experiments ourselves due to
    the computational limitations.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 开源模型的利用：探索开源模型如 LLaMA 在子问题生成中的应用，成为了一种经济高效的替代方案。这些模型无需财务限制，为研究人员提供了一个无财务约束地深入研究子问题生成的机会。由于计算限制，我们未能亲自进行这种实验。
- en: 7 Conclusion
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: This work introduces a novel AI-generated benchmark tailored for evaluating
    sub-questioning in reasoning tasks. We employ diverse offline learning approaches,
    varying model sizes for baselines, and assess the performance using different
    LLMs. Our experiments aim to shed light on the challenges and potential avenues
    for enhancing reasoning capabilities.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作介绍了一个新颖的 AI 生成基准，旨在评估推理任务中的子问题。我们采用了多种离线学习方法，基准模型的规模各异，并使用不同的 LLMs 评估性能。我们的实验旨在揭示挑战和潜在的提升推理能力的途径。
- en: The outcomes reveal a significant performance gap between the best-performing
    approach and ChatGPT. The underwhelming performance of the offline RL approach
    underscores the need for further advancements in this domain, presenting an opportunity
    for future research to explore and refine these methodologies.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，最佳表现的方法与 ChatGPT 之间存在显著的性能差距。离线 RL 方法表现不佳，突显了该领域进一步发展的需求，为未来研究提供了探索和完善这些方法的机会。
- en: By providing this benchmark, we aspire to catalyze research endeavors in the
    realm of sub-questioning. We anticipate that the dataset curated in this work
    will serve as a foundational resource for assessing the reasoning capabilities
    of emerging offline RL approaches in the field of NLP.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供这一基准，我们希望催化在子问题领域的研究工作。我们预计本研究中策划的数据集将作为评估新兴离线强化学习方法在自然语言处理领域推理能力的基础资源。
- en: 8 Acknowledgements
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Mrinmaya Sachan for his supervision of our work and Elliott Ash for
    providing us with computational resources.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢**姆林玛亚·萨昌**对我们工作的监督以及**埃利奥特·阿什**为我们提供计算资源。
- en: References
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Akimov et al. (2022) Dmitriy Akimov, Vladislav Kurenkov, Alexander Nikulin,
    Denis Tarasov, and Sergey Kolesnikov. Let offline rl flow: Training conservative
    agents in the latent space of normalizing flows. *arXiv preprint arXiv:2211.11096*,
    2022.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿基莫夫等（2022）**德米特里·阿基莫夫**、**弗拉季斯拉夫·库连科夫**、**亚历山大·尼古林**、**德尼斯·塔拉索夫** 和 **谢尔盖·科列斯尼科夫**。让离线强化学习流动：在归一化流的潜在空间中训练保守代理。*arXiv预印本
    arXiv:2211.11096*，2022。
- en: An et al. (2021) Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based
    offline reinforcement learning with diversified q-ensemble. *Advances in neural
    information processing systems*, 34:7436–7447, 2021.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安等（2021）**高安**、**胜勇·穆恩**、**张贤金** 和 **玄哦·宋**。基于不确定性的离线强化学习与多样化 Q 集合。*神经信息处理系统进展*，34:7436–7447，2021。
- en: Bai et al. (2022a) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    Training a helpful and harmless assistant with reinforcement learning from human
    feedback. *arXiv preprint arXiv:2204.05862*, 2022a.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白等（2022a）**白云涛**、**安迪·琼斯**、**卡马尔·恩杜塞**、**阿曼达·阿斯凯尔**、**安娜·陈**、**诺娃·达斯萨尔玛**、**道恩·德雷恩**、**斯坦尼斯拉夫·福特**、**迪普·甘古利**、**汤姆·赫尼汉**
    等。利用来自人类反馈的强化学习训练一个有用且无害的助手。*arXiv预印本 arXiv:2204.05862*，2022a。
- en: 'Bai et al. (2022b) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell,
    Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron
    McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. *arXiv preprint
    arXiv:2212.08073*, 2022b.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白等（2022b）**白云涛**、**萨乌拉夫·卡达瓦斯**、**桑迪潘·昆杜**、**阿曼达·阿斯凯尔**、**杰克逊·科尼翁**、**安迪·琼斯**、**安娜·陈**、**安娜·戈尔迪**、**阿扎利亚·米尔霍赛尼**、**卡梅伦·麦金农**
    等。宪法 AI：来自 AI 反馈的无害性。*arXiv预印本 arXiv:2212.08073*，2022b。
- en: 'Chen et al. (2021) Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya
    Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision
    transformer: Reinforcement learning via sequence modeling. *Advances in neural
    information processing systems*, 34:15084–15097, 2021.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021）**莉莉·陈**、**凯文·陆**、**阿拉文德·拉杰斯瓦兰**、**基敏·李**、**阿迪亚·格罗弗**、**米莎·拉斯金**、**皮特·阿贝尔**、**阿拉文德·斯里尼瓦斯**
    和 **伊戈尔·莫达奇**。决策变换器：通过序列建模进行强化学习。*神经信息处理系统进展*，34:15084–15097，2021。
- en: Chen et al. (2022) Minmin Chen, Can Xu, Vince Gatto, Devanshu Jain, Aviral Kumar,
    and Ed H. Chi. Off-policy actor-critic for recommender systems. *Proceedings of
    the 16th ACM Conference on Recommender Systems*, 2022.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2022）**陈敏敏**、**徐灿**、**文斯·加托**、**德万舒·贾因**、**阿维拉尔·库马尔** 和 **艾德·H·池**。离线策略演员-评论家推荐系统。*第16届ACM推荐系统会议论文集*，2022。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*, 2021.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 科布等（2021）**卡尔·科布**、**维尼特·科萨拉朱**、**穆罕默德·巴瓦里安**、**马克·陈**、**赫维乌·俊**、**卢卡斯·凯泽**、**马蒂亚斯·普拉普特**、**杰瑞·特沃雷克**、**雅各布·希尔顿**、**内田礼一郎**
    等。训练验证者解决数学文字问题。*arXiv预印本 arXiv:2110.14168*，2021。
- en: 'Diehl et al. (2021) Christopher P. Diehl, Timo Sievernich, Martin Krüger, Frank
    Hoffmann, and Torsten Bertram. Umbrella: Uncertainty-aware model-based offline
    reinforcement learning leveraging planning. *ArXiv*, abs/2111.11097, 2021.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迪尔等（2021）**克里斯托弗·P·迪尔**、**蒂莫·西弗尼希**、**马丁·克鲁格**、**弗兰克·霍夫曼** 和 **托斯滕·贝特拉姆**。伞：利用规划的模型不确定性感知离线强化学习。*ArXiv*，abs/2111.11097，2021。
- en: 'Fu et al. (2020) Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey
    Levine. D4rl: Datasets for deep data-driven reinforcement learning. *arXiv preprint
    arXiv:2004.07219*, 2020.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 傅等（2020）**贾斯汀·傅**、**阿维拉尔·库马尔**、**奥菲尔·纳赫姆**、**乔治·塔克** 和 **谢尔盖·莱文**。D4rl：用于深度数据驱动强化学习的数据集。*arXiv预印本
    arXiv:2004.07219*，2020。
- en: Fujimoto & Gu (2021) Scott Fujimoto and Shixiang Shane Gu. A minimalist approach
    to offline reinforcement learning. *Advances in neural information processing
    systems*, 34:20132–20145, 2021.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 藤本 & 顾（2021）**斯科特·藤本** 和 **石香·肖恩·顾**。一种简约的离线强化学习方法。*神经信息处理系统进展*，34:20132–20145，2021。
- en: Ghasemipour et al. (2022) Kamyar Ghasemipour, Shixiang Shane Gu, and Ofir Nachum.
    Why so pessimistic? estimating uncertainties for offline rl through ensembles,
    and why their independence matters. *Advances in Neural Information Processing
    Systems*, 35:18267–18281, 2022.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghasemipour et al. (2022) Kamyar Ghasemipour, Shixiang Shane Gu 和 Ofir Nachum。为什么如此悲观？通过集成估计离线强化学习的不确定性，以及它们的独立性为何重要。*神经信息处理系统进展*，35:18267–18281，2022年。
- en: Gulcehre et al. (2023) Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan,
    Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern,
    Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language
    modeling. *arXiv preprint arXiv:2308.08998*, 2023.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gulcehre et al. (2023) Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan,
    Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern,
    Miaosen Wang, Chenjie Gu 等。用于语言建模的强化自训练（REST）。*arXiv 预印本 arXiv:2308.08998*，2023年。
- en: 'Guss et al. (2019) William H Guss, Brandon Houghton, Nicholay Topin, Phillip
    Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl: A large-scale
    dataset of minecraft demonstrations. *arXiv preprint arXiv:1907.13440*, 2019.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guss et al. (2019) William H Guss, Brandon Houghton, Nicholay Topin, Phillip
    Wang, Cayden Codel, Manuela Veloso 和 Ruslan Salakhutdinov。Minerl：一个大规模的 Minecraft
    演示数据集。*arXiv 预印本 arXiv:1907.13440*，2019年。
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical
    problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*, 2021.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song 和 Jacob Steinhardt。使用数学数据集测量数学问题解决能力。*arXiv
    预印本 arXiv:2103.03874*，2021年。
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*, 2015.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals 和 Jeff Dean。提炼神经网络中的知识。*arXiv
    预印本 arXiv:1503.02531*，2015年。
- en: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander J. Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    Distilling step-by-step! outperforming larger language models with less training
    data and smaller model sizes. *ArXiv*, abs/2305.02301, 2023. URL [https://arxiv.org/abs/2305.02301](https://arxiv.org/abs/2305.02301).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander J. Ratner, Ranjay Krishna, Chen-Yu Lee 和 Tomas Pfister。逐步提炼！用更少的训练数据和更小的模型规模超越更大的语言模型。*ArXiv*，abs/2305.02301，2023年。网址
    [https://arxiv.org/abs/2305.02301](https://arxiv.org/abs/2305.02301)。
- en: 'Huang & Chang (2022) Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning
    in large language models: A survey. *arXiv preprint arXiv:2212.10403*, 2022.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang & Chang (2022) Jie Huang 和 Kevin Chen-Chuan Chang。大型语言模型中的推理探索：一项调查。*arXiv
    预印本 arXiv:2212.10403*，2022年。
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*, 2023.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, 等。Mistral 7b。*arXiv 预印本 arXiv:2310.06825*，2023年。
- en: Juneja et al. (2023) Gurusha Juneja, Subhabrata Dutta, Soumen Chakrabarti, Sunny
    Manchanda, and Tanmoy Chakraborty. Small language models fine-tuned to coordinate
    larger language models improve complex reasoning. *arXiv preprint arXiv:2310.18338*,
    2023.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Juneja et al. (2023) Gurusha Juneja, Subhabrata Dutta, Soumen Chakrabarti, Sunny
    Manchanda 和 Tanmoy Chakraborty。小型语言模型经过微调以协调大型语言模型，改善复杂推理。*arXiv 预印本 arXiv:2310.18338*，2023年。
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. *Advances
    in neural information processing systems*, 35:22199–22213, 2022.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo 和 Yusuke Iwasawa。大型语言模型是零样本推理者。*神经信息处理系统进展*，35:22199–22213，2022年。
- en: Kostrikov et al. (2021) Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline
    reinforcement learning with implicit q-learning. *arXiv preprint arXiv:2110.06169*,
    2021.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kostrikov et al. (2021) Ilya Kostrikov, Ashvin Nair 和 Sergey Levine。离线强化学习与隐式
    Q 学习。*arXiv 预印本 arXiv:2110.06169*，2021年。
- en: Kumar et al. (2020) Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
    Conservative q-learning for offline reinforcement learning. *Advances in Neural
    Information Processing Systems*, 33:1179–1191, 2020.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar et al. (2020) Aviral Kumar, Aurick Zhou, George Tucker 和 Sergey Levine。用于离线强化学习的保守
    Q 学习。*神经信息处理系统进展*，33:1179–1191，2020年。
- en: Kumar et al. (2021) Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn,
    and Sergey Levine. A workflow for offline model-free robotic reinforcement learning.
    In *5th Annual Conference on Robot Learning*, 2021. URL [https://openreview.net/forum?id=fy4ZBWxYbIo](https://openreview.net/forum?id=fy4ZBWxYbIo).
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库马尔等人 (2021) 阿维拉尔·库马尔、阿尼凯特·辛格、斯蒂芬·田、切尔西·芬和谢尔盖·莱文。离线无模型机器人强化学习工作流。在*第五届机器人学习年会*，2021。网址
    [https://openreview.net/forum?id=fy4ZBWxYbIo](https://openreview.net/forum?id=fy4ZBWxYbIo)。
- en: 'Kurenkov et al. (2023) Vladislav Kurenkov, Alexander Nikulin, Denis Tarasov,
    and Sergey Kolesnikov. Katakomba: Tools and benchmarks for data-driven nethack.
    *arXiv preprint arXiv:2306.08772*, 2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库连科夫等人 (2023) 弗拉基斯拉夫·库连科夫、亚历山大·尼库林、丹尼斯·塔拉索夫和谢尔盖·科列斯尼科夫。Katakomba：数据驱动的Nethack工具和基准。*arXiv
    预印本 arXiv:2306.08772*，2023。
- en: 'Lee et al. (2023) Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu,
    Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling
    reinforcement learning from human feedback with ai feedback. *arXiv preprint arXiv:2309.00267*,
    2023.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 (2023) 哈里森·李、萨姆拉特·帕塔尔、哈桑·曼苏尔、凯莉·卢、托马斯·梅斯纳德、科尔顿·毕晓普、维克托·卡布内和阿比纳夫·拉斯托吉。Rlaif：从人类反馈到AI反馈的强化学习扩展。*arXiv
    预印本 arXiv:2309.00267*，2023。
- en: 'Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin
    Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open
    problems. *arXiv preprint arXiv:2005.01643*, 2020.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 莱文等人 (2020) 谢尔盖·莱文、阿维拉尔·库马尔、乔治·塔克和贾斯廷·傅。离线强化学习：教程、综述和开放问题的视角。*arXiv 预印本 arXiv:2005.01643*，2020。
- en: 'Ling et al. (2017) Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
    Program induction by rationale generation: Learning to solve and explain algebraic
    word problems. *arXiv preprint arXiv:1705.04146*, 2017.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等人 (2017) 王岭、达尼·尤加塔玛、克里斯·戴尔和菲尔·布伦索姆。通过理由生成进行程序归纳：学习解决和解释代数文字问题。*arXiv 预印本 arXiv:1705.04146*，2017。
- en: Lyu et al. (2023) Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao,
    Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. Faithful chain-of-thought
    reasoning. *arXiv preprint arXiv:2301.13379*, 2023.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吕等人 (2023) 吕青、什雷亚·哈瓦尔达、亚当·斯坦、李张、德利普·拉奥、埃里克·黄、玛丽安娜·阿皮迪安基和克里斯·卡利森-布尔奇。忠实的链式推理。*arXiv
    预印本 arXiv:2301.13379*，2023。
- en: Magister et al. (2023) Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek,
    Eric Malmi, and Aliaksei Severyn. Teaching small language models to reason, 2023.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马吉斯特等人 (2023) 露西·夏洛特·马吉斯特、乔纳森·马林森、雅库布·阿达梅克、埃里克·马尔米和阿利亚克谢·塞夫林。教小型语言模型进行推理，2023。
- en: Nikulin et al. (2023) Alexander Nikulin, Vladislav Kurenkov, Denis Tarasov,
    and Sergey Kolesnikov. Anti-exploration by random network distillation. *arXiv
    preprint arXiv:2301.13616*, 2023.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尼库林等人 (2023) 亚历山大·尼库林、弗拉基斯拉夫·库连科夫、丹尼斯·塔拉索夫和谢尔盖·科列斯尼科夫。通过随机网络蒸馏进行反探索。*arXiv 预印本
    arXiv:2301.13616*，2023。
- en: OpenAI (2023) R OpenAI. Gpt-4 technical report. *arXiv*, pp.  2303–08774, 2023.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) R OpenAI。Gpt-4技术报告。*arXiv*，第 2303–08774 页，2023。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧阳等人 (2022) 长欧阳、杰弗里·吴、徐江、迪奥戈·阿尔梅达、卡罗尔·温赖特、帕梅拉·米什金、钟张、桑迪尼·阿加瓦尔、卡塔里娜·斯拉马、亚历克斯·雷等。利用人类反馈训练语言模型以遵循指令。*神经信息处理系统进展*，35:27730–27744，2022。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: a method for automatic evaluation of machine translation. In *Proceedings
    of the 40th annual meeting of the Association for Computational Linguistics*,
    pp.  311–318, 2002.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帕皮内尼等人 (2002) 基肖尔·帕皮内尼、萨利姆·鲁科斯、托德·沃德和魏晶·朱。Bleu：一种自动评估机器翻译的方法。在*第40届计算语言学协会年会会议录*，第
    311–318 页，2002。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉德福德等人 (2019) 亚历克·拉德福德、杰弗里·吴、瑞温·柴尔德、大卫·吕安、达里奥·阿莫代伊、伊利亚·苏茨克维尔等。语言模型是无监督的多任务学习者。*OpenAI
    博客*，1(8):9，2019。
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.
    *arXiv preprint arXiv:1910.01108*, 2019.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 桑等人 (2019) 维克多·桑、利桑德·德比、朱利安·肖蒙德和托马斯·沃尔夫。Distilbert，BERT的蒸馏版：更小、更快、更便宜、更轻量。*arXiv
    预印本 arXiv:1910.01108*，2019。
- en: Shridhar et al. (2022) Kumar Shridhar, Jakub Macina, Mennatallah El-Assady,
    Tanmay Sinha, Manu Kapur, and Mrinmaya Sachan. Automatic generation of socratic
    subquestions for teaching math word problems. *arXiv preprint arXiv:2211.12835*,
    2022.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar 等（2022） Kumar Shridhar、Jakub Macina、Mennatallah El-Assady、Tanmay Sinha、Manu
    Kapur 和 Mrinmaya Sachan。自动生成用于教学数学应用题的苏格拉底式子问题。*arXiv 预印本 arXiv:2211.12835*，2022。
- en: 'Shridhar et al. (2023) Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan.
    Distilling reasoning capabilities into smaller language models. In *Findings of
    the Association for Computational Linguistics: ACL 2023*, pp.  7059–7073, 2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar 等（2023） Kumar Shridhar、Alessandro Stolfo 和 Mrinmaya Sachan。将推理能力提炼到更小的语言模型中。在*计算语言学协会会议成果：ACL
    2023*，第 7059–7073 页，2023。
- en: 'Smith et al. (2022) Laura Smith, Ilya Kostrikov, and Sergey Levine. A Walk
    in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,
    August 2022.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith 等（2022） Laura Smith、Ilya Kostrikov 和 Sergey Levine。公园漫步：使用无模型强化学习在 20
    分钟内学习行走，2022 年 8 月。
- en: Snell et al. (2022) Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and
    Sergey Levine. Offline rl for natural language generation with implicit language
    q learning. *arXiv preprint arXiv:2206.11871*, 2022.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snell 等（2022） Charlie Snell、Ilya Kostrikov、Yi Su、Mengjiao Yang 和 Sergey Levine。用于自然语言生成的离线强化学习与隐式语言
    Q 学习。*arXiv 预印本 arXiv:2206.11871*，2022。
- en: 'Tarasov et al. (2022) Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav
    Kurenkov, and Sergey Kolesnikov. Corl: Research-oriented deep offline reinforcement
    learning library. *arXiv preprint arXiv:2210.07105*, 2022.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tarasov 等（2022） Denis Tarasov、Alexander Nikulin、Dmitry Akimov、Vladislav Kurenkov
    和 Sergey Kolesnikov。Corl：面向研究的深度离线强化学习库。*arXiv 预印本 arXiv:2210.07105*，2022。
- en: Tarasov et al. (2023a) Denis Tarasov, Vladislav Kurenkov, Alexander Nikulin,
    and Sergey Kolesnikov. Revisiting the minimalist approach to offline reinforcement
    learning. *arXiv preprint arXiv:2305.09836*, 2023a.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tarasov 等（2023a） Denis Tarasov、Vladislav Kurenkov、Alexander Nikulin 和 Sergey
    Kolesnikov。重新审视离线强化学习的简约方法。*arXiv 预印本 arXiv:2305.09836*，2023a。
- en: Tarasov et al. (2023b) Denis Tarasov, Ulrich Armel Mbou Sob, Miguel Arbesu,
    Nima Siboni, Sebastien Boyer, Marcin Skwark, Andries Smit, Oliver Bent, and Arnu
    Pretorius. Offline rl for generative design of protein binders. *bioRxiv*, pp. 
    2023–11, 2023b.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tarasov 等（2023b） Denis Tarasov、Ulrich Armel Mbou Sob、Miguel Arbesu、Nima Siboni、Sebastien
    Boyer、Marcin Skwark、Andries Smit、Oliver Bent 和 Arnu Pretorius。用于蛋白质结合体生成设计的离线强化学习。*bioRxiv*，第
    2023–11 页，2023b。
- en: 'Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv
    preprint arXiv:2312.11805*, 2023.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team 等（2023） Gemini Team、Rohan Anil、Sebastian Borgeaud、Yonghui Wu、Jean-Baptiste
    Alayrac、Jiahui Yu、Radu Soricut、Johan Schalkwyk、Andrew M Dai、Anja Hauth 等。Gemini：一系列高能力的多模态模型。*arXiv
    预印本 arXiv:2312.11805*，2023。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023） Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。Llama：开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023。
- en: 'Tunstall et al. (2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen
    Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine
    Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. *arXiv
    preprint arXiv:2310.16944*, 2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tunstall 等（2023） Lewis Tunstall、Edward Beeching、Nathan Lambert、Nazneen Rajani、Kashif
    Rasul、Younes Belkada、Shengyi Huang、Leandro von Werra、Clémentine Fourrier、Nathan
    Habib 等。Zephyr：语言模型对齐的直接蒸馏。*arXiv 预印本 arXiv:2310.16944*，2023。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. *Advances in neural information processing systems*, 30, 2017.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017） Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion
    Jones、Aidan N Gomez、Łukasz Kaiser 和 Illia Polosukhin。注意力即你所需。*神经信息处理系统进展*，30，2017。
- en: Vinyals et al. (2019) Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki,
    Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell,
    Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent
    reinforcement learning. *Nature*, 575(7782):350–354, 2019.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals 等（2019） Oriol Vinyals、Igor Babuschkin、Wojciech M Czarnecki、Michaël Mathieu、Andrew
    Dudzik、Junyoung Chung、David H Choi、Richard Powell、Timo Ewalds、Petko Georgiev 等。使用多智能体强化学习在《星际争霸
    II》中达到大师级水平。*Nature*，575(7782)：350–354，2019。
- en: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*,
    2022.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2022）王雪智、杰森·魏、戴尔·舒尔曼、阮国伟、艾德·齐、沙兰·纳朗、阿坎莎·乔杜里和丹尼·周。《自一致性提升语言模型中的链式思维推理》。*arXiv
    预印本 arXiv:2203.11171*，2022年。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等（2022）杰森·魏、王雪智、戴尔·舒尔曼、马滕·博斯马、夏飞、艾德·齐、阮国伟、丹尼·周等。《链式思维提示引发大语言模型的推理》。*神经信息处理系统进展*，35:24824–24837，2022年。
- en: 'Yang et al. (2022) Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie
    Zhang, and Lei Han. Rorl: Robust offline reinforcement learning via conservative
    smoothing. *Advances in Neural Information Processing Systems*, 35:23851–23866,
    2022.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等（2022） Rui Yang、陈佳白、肖腾·马、赵然·王、崇杰·张、和雷汉。《Rorl：通过保守平滑的强健离线强化学习》。*神经信息处理系统进展*，35:23851–23866，2022年。
- en: Yuan et al. (2023) Zheng Yuan, Hongyi Yuan, Cheng Li, Guanting Dong, Chuanqi
    Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with
    large language models. *ArXiv*, abs/2308.01825, 2023. URL [https://arxiv.org/abs/2308.01825](https://arxiv.org/abs/2308.01825).
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 袁等（2023）郑袁、洪义袁、程李、光亭董、川奇·谭、常周。《使用大语言模型进行数学推理的扩展关系》。*ArXiv*，abs/2308.01825，2023年。网址
    [https://arxiv.org/abs/2308.01825](https://arxiv.org/abs/2308.01825)。
- en: Zhou et al. (2022) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al.
    Least-to-most prompting enables complex reasoning in large language models. *arXiv
    preprint arXiv:2205.10625*, 2022.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等（2022）丹尼·周、纳撒尼尔·谢尔利、乐厚、杰森·魏、内森·斯凯尔斯、王雪智、戴尔·舒尔曼、克莱尔·崔、奥利维耶·布斯克、阮国伟等。《最少到最多提示使大语言模型能够复杂推理》。*arXiv
    预印本 arXiv:2205.10625*，2022年。
- en: Appendix A Experimental Details
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 实验细节
- en: We run all of our experiments using single V100 GPUs. The training time never
    exceeded 5 days.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用单个 V100 GPU 运行所有实验。训练时间从未超过 5 天。
- en: Hyperparaters are kept the same across different model sizes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数在不同模型尺寸间保持不变。
- en: A.1 Hyperparameters
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 超参数
- en: We conducted hyperparameters search only through $\{0.5,0.7,0.9\}$ parameter
    using GPT-2 small with full reward. Also, following Tarasov et al. ([2023a](#bib.bib41))
    we have increased discount factor value from default 0.99 to 0.999 which improved
    ILQL performance.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅通过 $\{0.5,0.7,0.9\}$ 参数进行超参数搜索，使用 GPT-2 小型并且完全奖励。此外，按照 Tarasov 等（[2023a](#bib.bib41)）的做法，我们将折扣因子的默认值从
    0.99 增加到 0.999，这改善了 ILQL 性能。
- en: '| Hyperparameter | Value |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| --- | --- |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Batch size | 32 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 32 |'
- en: '| Optimizer | Adam |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | Adam |'
- en: '| Learning rate | 1e-4 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 1e-4 |'
- en: '| Gradient steps | 10000 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 梯度步骤 | 10000 |'
- en: 'Table 3: BC hyperparameters.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：BC 超参数。
- en: '| Hyperparameter | Value |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| --- | --- |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Batch size | 32 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 32 |'
- en: '| Optimizer | Adam |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | Adam |'
- en: '| Learning rate | 1e-4 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 1e-4 |'
- en: '| Gradient steps | 7500 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 梯度步骤 | 7500 |'
- en: 'Table 4: Filtered BC hyperparameters.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：过滤后的 BC 超参数。
- en: '| Hyperparameter | Value |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Batch size | 32 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 32 |'
- en: '| Optimizer | Adam |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | Adam |'
- en: '| Learning rate | 1e-4 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 1e-4 |'
- en: '| Gradient steps | 25000 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 梯度步骤 | 25000 |'
- en: '| Discount factor | 0.999 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 折扣因子 | 0.999 |'
- en: '| Target update rate | 5e-3 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 目标更新率 | 5e-3 |'
- en: '| IQL $\tau$ | 0.9 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| IQL $\tau$ | 0.9 |'
- en: '| ILQL $\beta$ | 1.0 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| ILQL $\beta$ | 1.0 |'
- en: '| V loss weight | 1.0 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| V 损失权重 | 1.0 |'
- en: '| Q loss weight | 1.0 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Q 损失权重 | 1.0 |'
- en: '| CQL loss weight | 0.01 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| CQL 损失权重 | 0.01 |'
- en: 'Table 5: ILQL hyperparameters.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：ILQL 超参数。
- en: Appendix B Full Tabular Scores
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 完整的表格分数
- en: '| Algorithm | DistillGPT | GPT-2 small | GPT-2 medium | Average |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | DistillGPT | GPT-2 小型 | GPT-2 中型 | 平均值 |'
- en: '| BC | 0.476 | 0.508 | 0.538 | 0.507 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| BC | 0.476 | 0.508 | 0.538 | 0.507 |'
- en: '| Filtered BC | 0.493 | 0.527 | 0.576 | 0.532 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 过滤后的 BC | 0.493 | 0.527 | 0.576 | 0.532 |'
- en: '| ILQL-sparse | 0.471 | 0.518 | 0.541 | 0.510 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| ILQL-稀疏 | 0.471 | 0.518 | 0.541 | 0.510 |'
- en: '| ILQL-full | 0.484 | 0.504 | 0.540 | 0.509 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| ILQL-完全 | 0.484 | 0.504 | 0.540 | 0.509 |'
- en: '| ChatGPT | N/A | N/A | N/A | 0.682 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | N/A | N/A | N/A | 0.682 |'
- en: 'Table 6: Accuracy score of the final answer using ChatGPT for sub-questions
    answering. Best scores are highlighted with bold.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：使用 ChatGPT 进行子问题回答的最终答案准确率分数。最佳分数用粗体突出显示。
- en: '| Algorithm | DistillGPT | GPT-2 small | GPT-2 medium | Average |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | DistillGPT | GPT-2 小型 | GPT-2 中型 | 平均值 |'
- en: '| BC | 0.118 | 0.154 | 0.164 | 0.145 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| BC | 0.118 | 0.154 | 0.164 | 0.145 |'
- en: '| Filtered BC | 0.125 | 0.159 | 0.162 | 0.149 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 过滤后的 BC | 0.125 | 0.159 | 0.162 | 0.149 |'
- en: '| ILQL-sparse | 0.125 | 0.138 | 0.162 | 0.142 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| ILQL-sparse | 0.125 | 0.138 | 0.162 | 0.142 |'
- en: '| ILQL-full | 0.114 | 0.144 | 0.163 | 0.140 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| ILQL-full | 0.114 | 0.144 | 0.163 | 0.140 |'
- en: '| ChatGPT | N/A | N/A | N/A | 0.234 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | N/A | N/A | N/A | 0.234 |'
- en: 'Table 7: Accuracy score of the final answer using LLaMA 7B for sub-questions
    answering. Best scores are highlighted with bold.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '表7: 使用 LLaMA 7B 对子问题回答的最终答案的准确性得分。最佳得分以**粗体**显示。'
- en: '| Algorithm | DistillGPT | GPT-2 small | GPT-2 medium | Average |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | DistillGPT | GPT-2 small | GPT-2 medium | 平均 |'
- en: '| BC | 0.184 | 0.212 | 0.247 | 0.214 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| BC | 0.184 | 0.212 | 0.247 | 0.214 |'
- en: '| Filtered BC | 0.194 | 0.230 | 0.245 | 0.223 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 过滤后的 BC | 0.194 | 0.230 | 0.245 | 0.223 |'
- en: '| ILQL-sparse | 0.180 | 0.207 | 0.250 | 0.212 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| ILQL-sparse | 0.180 | 0.207 | 0.250 | 0.212 |'
- en: '| ILQL-full | 0.182 | 0.210 | 0.252 | 0.215 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| ILQL-full | 0.182 | 0.210 | 0.252 | 0.215 |'
- en: '| ChatGPT | N/A | N/A | N/A | 0.353 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | N/A | N/A | N/A | 0.353 |'
- en: 'Table 8: Accuracy score of the final answer using LLaMA 13B for sub-questions
    answering. Best scores are highlighted with bold.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '表8: 使用 LLaMA 13B 对子问题回答的最终答案的准确性得分。最佳得分以**粗体**显示。'
- en: '| Algorithm | DistillGPT | GPT-2 small | GPT-2 medium | Average |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | DistillGPT | GPT-2 small | GPT-2 medium | 平均 |'
- en: '| BC | 0.240 | 0.264 | 0.290 | 0.265 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| BC | 0.240 | 0.264 | 0.290 | 0.265 |'
- en: '| Filtered BC | 0.228 | 0.256 | 0.293 | 0.259 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 过滤后的 BC | 0.228 | 0.256 | 0.293 | 0.259 |'
- en: '| ILQL-sparse | 0.219 | 0.261 | 0.288 | 0.256 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| ILQL-sparse | 0.219 | 0.261 | 0.288 | 0.256 |'
- en: '| ILQL-full | 0.231 | 0.252 | 0.280 | 0.254 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| ILQL-full | 0.231 | 0.252 | 0.280 | 0.254 |'
- en: '| ChatGPT | N/A | N/A | N/A | 0.446 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | N/A | N/A | N/A | 0.446 |'
- en: 'Table 9: Accuracy score of the final answer using Mistral for sub-questions
    answering. Best scores are highlighted with bold.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '表9: 使用 Mistral 对子问题回答的最终答案的准确性得分。最佳得分以**粗体**显示。'
- en: Appendix C ChatGPT Prompts
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C ChatGPT 提示
- en: '| Input |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 输入 |'
- en: '| --- |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| You are given mathematical problems marked with ”Problem”. Your task is to
    split it into smaller sub-problems and formulate them as sub-questions which will
    be answered by someone else who’s objective is to solve the original problem.
    Questions must not contain the answers for the previous questions in them. Do
    not ask questions where the answer is already given in the problem. For each problem
    come up with the sequence of sub-questions and output each of them on separate
    line which starts with letter Q followed by the number of question. Do not output
    anything else. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 你得到标记为“问题”的数学问题。你的任务是将其拆分为更小的子问题，并将它们表述为子问题，由其他人的目标是解决原始问题。问题中不得包含前一个问题的答案。不要问那些答案已经在问题中给出的子问题。针对每个问题，提出子问题的序列，并将每个子问题以单独的行输出，行首用字母
    Q 及问题编号开头。不要输出其他任何内容。 |'
- en: '| Problem: A robe takes 2 bolts of blue fiber and half that much white fiber.
    How many bolts in total does it take? |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 问题：一件长袍需要 2 卷蓝色纤维和相等量的一半白色纤维。总共需要多少卷？ |'
- en: '| Q1: How many bolts of white fiber does it take? |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Q1: 需要多少卷白色纤维？ |'
- en: '| Q2: How many bolts in total does it take? |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| Q2: 总共需要多少卷？ |'
- en: '| Problem: Betty is saving money for a new wallet which costs $100\. Betty
    has only half of the money she needs. Her parents decided to give her $15 for
    that purpose, and her grandparents twice as much as her parents. How much more
    money does Betty need to buy the wallet? |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 问题：贝蒂正在为一只价值 $100 的新钱包存钱。贝蒂只有她需要钱的一半。她的父母决定给她 $15，而她的祖父母给的金额是她父母的两倍。贝蒂还需要多少钱才能买到钱包？
    |'
- en: '| Q1: How much did the house cost? |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Q1: 房子多少钱？ |'
- en: '| Q2: How much did the repairs increase the value of the house? |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Q2: 修理费用增加了房子的价值多少？ |'
- en: '| Q3: What is the new value of the house? |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Q3: 房子的新增价值是多少？ |'
- en: '| Q4: How much profit did he make? |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Q4: 他赚了多少钱？ |'
- en: '| Problem: Betty is saving money for a new wallet which costs $100\. Betty
    has only half of the money she needs. Her parents decided to give her $15 for
    that purpose, and her grandparents twice as much as her parents. How much more
    money does Betty need to buy the wallet? |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 问题：贝蒂正在为一只价值 $100 的新钱包存钱。贝蒂只有她需要钱的一半。她的父母决定给她 $15，而她的祖父母给的金额是她父母的两倍。贝蒂还需要多少钱才能买到钱包？
    |'
- en: '| Response |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 响应 |'
- en: '| Q1: How much money does Betty have? |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Q1: 贝蒂有多少钱？ |'
- en: '| Q2: How much money did Betty’s parents give her? |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Q2: 贝蒂的父母给了她多少钱？ |'
- en: '| Q3: How much money did Betty’s grandparents give her? |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| Q3: 贝蒂的祖父母给了她多少钱？ |'
- en: '| Q4: How much money does Betty still need to buy the wallet? |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Q4: 贝蒂还需要多少钱才能买到钱包？ |'
- en: 'Table 10: Sample ChatGPT input for generating sub-questions and one of the
    corresponding responses.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '表10: 生成子问题和相应回答的示例ChatGPT输入。'
- en: '| Input |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 输入 |'
- en: '| --- |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| You are given the mathematical problems marked with ”Problem” and a sequence
    of questions which should help in solving it. Question number N is marked as ”QN:”.
    Based on the problem and sequence of questions answer each of the questions with
    answer ”AN:” and give the answer to the whole problem using ”Final answer:” using
    only the resulting number without adding any additional comments after it. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 你将得到标记为“问题”的数学题和一系列问题，这些问题应有助于解决它。问题编号N标记为“QN:”。根据问题和问题序列，回答每一个问题，并用“AN:”标记答案，用“最终答案:”给出整个问题的答案，只使用结果数字，不添加任何额外评论。
    |'
- en: '| Problem: A robe takes 2 bolts of blue fiber and half that much white fiber.
    How many bolts in total does it take? |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 问题: 一件长袍需要2卷蓝色纤维和一半的白色纤维。总共需要多少卷？ |'
- en: '| Q1: How many bolts of white fiber does it take? |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| Q1: 需要多少卷白色纤维？ |'
- en: '| Q2: How many bolts in total does it take? |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Q2: 总共需要多少卷？ |'
- en: '| A1: It takes 2/2=1 bolt of white fiber |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| A1: 需要2/2=1卷白色纤维 |'
- en: '| A2: So the total amount of fabric is 2+1=3 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| A2: 所以总共需要的面料量是2+1=3 |'
- en: '| Final answer: 3 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 最终答案: 3 |'
- en: '| Problem: Josh decides to try flipping a house. He buys a house for $80,000
    and then puts in $50,000 in repairs. This increased the value of the house by
    150%. How much profit did he make? |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 问题: 乔希决定尝试翻修房屋。他以$80,000买下了一栋房子，然后投入了$50,000进行修缮。这使房屋的价值增加了150%。他赚了多少钱？ |'
- en: '| Q1: How much did the house cost? |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Q1: 房子花了多少钱？ |'
- en: '| Q2: How much did the repairs increase the value of the house? |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Q2: 房屋修缮增加了多少价值？ |'
- en: '| Q3: What is the new value of the house? |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Q3: 房屋的新价值是多少？ |'
- en: '| Q4: How much profit did he make? |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Q4: 他赚了多少钱？ |'
- en: '| A1: The cost of the house and repairs came out to 80,000+50,000=130,000 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| A1: 房屋和修缮的费用总共是80,000+50,000=130,000 |'
- en: '| A2: He increased the value of the house by 80,000*1.5=120,000 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| A2: 他将房屋的价值增加到80,000*1.5=120,000 |'
- en: '| A3: So the new value of the house is 120,000+80,000=200,000 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| A3: 所以房屋的新价值是120,000+80,000=200,000 |'
- en: '| A4: So he made a profit of 200,000-130,000=70,000 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| A4: 所以他赚取的利润是200,000-130,000=70,000 |'
- en: '| Final answer: 70000 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 最终答案: 70000 |'
- en: '| Problem: Betty is saving money for a new wallet which costs $100\. Betty
    has only half of the money she needs. Her parents decided to give her $15 for
    that purpose, and her grandparents twice as much as her parents. How much more
    money does Betty need to buy the wallet? |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 问题: 贝蒂正在为一只价值$100的新钱包存钱。贝蒂只拥有她需要的一半钱。她的父母决定给她$15，而她的祖父母给她的金额是她父母的两倍。贝蒂还需要多少钱才能买到钱包？
    |'
- en: '| Q1: How much money does Betty have? |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| Q1: 贝蒂有多少钱？ |'
- en: '| Q2: How much money did Betty’s parents give her? |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Q2: 贝蒂的父母给了她多少钱？ |'
- en: '| Q3: How much money did Betty’s grandparents give her? |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| Q3: 贝蒂的祖父母给了她多少钱？ |'
- en: '| Q4: How much money does Betty still need to buy the wallet? |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| Q4: 贝蒂还需要多少钱才能买到钱包？ |'
- en: '| Response |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 回答 |'
- en: '| A1: Betty has 100/2=50 dollars. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| A1: 贝蒂有100/2=50美元。 |'
- en: '| A2: Betty’s parents gave her 15 dollars. |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| A2: 贝蒂的父母给了她15美元。 |'
- en: '| A3: Betty’s grandparents gave her 15*2=30 dollars. |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| A3: 贝蒂的祖父母给了她15*2=30美元。 |'
- en: '| A4: Betty still needs 100-50-15-30=5 dollars. |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| A4: 贝蒂还需要100-50-15-30=5美元。 |'
- en: '| Final answer: 5 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 最终答案: 5 |'
- en: 'Table 11: Sample ChatGPT input for answering sub-questions and the corresponding
    response.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '表11: 回答子问题和相应回答的示例ChatGPT输入。'
- en: '| Input |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 输入 |'
- en: '| --- |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| You are given the mathematical problem marked with ”Problem” and a sequence
    of sub-questions for solving it. Sub-question number N is marked as ”QN:”. Based
    on the problem for each sub-questions decide whether this question is helpful
    for solving the given problem. An essential property of a good questioning strategy
    is to ask questions that are directed towards the most critical domain specific
    content. Asking the right sequence of relevant questions that can assist in reaching
    the final goal is an important part of good questioning. If question repeats any
    of the previous it is not useful. The question for which answer is given in the
    problem or can’t be answered at all is also not useful. So redundant questions
    are not good. |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 你会得到标记为“问题”的数学问题和一系列用于解决它的子问题。子问题编号N标记为“QN:”。根据问题，为每个子问题判断这个问题是否有助于解决给定问题。良好的提问策略的一个重要特征是提出针对最关键领域特定内容的问题。提出一系列相关问题，能帮助达到最终目标，是良好提问的重要部分。如果问题重复之前的内容，就没有用处。问题的答案在问题中已给出或完全无法回答的，也没有用处。因此，冗余的问题是不好的。
    |'
- en: '| For each question output me ”QN: $<$ for the first two questions. Do not
    try to solve the problem anyhow as I’m only interested in the quality of the sub-questions.
    Strictly follow the output format. Provide answers only for the last given problem.
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 对于每个问题，请输出“QN: $<$”用于前两个问题。不要尝试解决问题，因为我只对子问题的质量感兴趣。严格遵循输出格式。仅对最后给出的问题提供答案。
    |'
- en: '| Problem: Janet’s ducks lay 16 eggs per day. She eats three for breakfast
    every morning and bakes muffins for her friends every day with four. She sells
    the remainder at the farmers’ market daily for $2 per fresh duck egg. How much
    in dollars does she make every day at the farmers’ market? |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 问题: 贾尼特的鸭子每天产16个鸡蛋。她每天早晨吃三个鸡蛋，并用四个鸡蛋给朋友烤松饼。她每天将剩下的鸡蛋以每个$2的价格在集市上出售。她每天在集市上赚多少钱？
    |'
- en: '| Q1: How many eggs does Janet sell? |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| Q1: 贾尼特卖了多少个鸡蛋？ |'
- en: '| Q2: Is duck an animal? |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Q2: 鸭子是动物吗？ |'
- en: '| Q3: How many eggs does each duck lay? |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| Q3: 每只鸭子每天产多少个鸡蛋？ |'
- en: '| Q4: How much does Janet make at the farmers’ market? |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| Q4: 贾尼特在集市上赚了多少钱？ |'
- en: '| Q1: Yes |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| Q1: 是 |'
- en: '| Q2: No |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Q2: 否 |'
- en: '| Q3: No |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| Q3: 否 |'
- en: '| Q4: Yes |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| Q4: 是 |'
- en: '| Problem: A robe takes 2 bolts of blue fiber and half that much white fiber.
    How many bolts in total does it take? |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 问题: 一件长袍需要2卷蓝色纤维和一半的白色纤维。总共需要多少卷纤维？ |'
- en: '| Q1: How many bolts of white fiber does it take? |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| Q1: 需要多少卷白色纤维？ |'
- en: '| Q2: How bolts of blue fiber does it take? |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Q2: 需要多少卷蓝色纤维？ |'
- en: '| Q3: How bolts of white fiber does it take? |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Q3: 需要多少卷白色纤维？ |'
- en: '| Q4: How many bolts in total does it take? |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Q4: 总共需要多少卷纤维？ |'
- en: '| Q1: Yes |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Q1: 是 |'
- en: '| Q2: No |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| Q2: 否 |'
- en: '| Q3: No |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| Q3: 否 |'
- en: '| Q4: Yes |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| Q4: 是 |'
- en: '| Problem: Betty is saving money for a new wallet which costs $100\. Betty
    has only half of the money she needs. Her parents decided to give her $15 for
    that purpose, and her grandparents twice as much as her parents. How much more
    money does Betty need to buy the wallet? |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 问题: 贝蒂正在为一只售价$100的新钱包存钱。贝蒂只拥有她所需的一半钱。她的父母决定为此目的给她$15，而她的祖父母则是她父母的两倍。贝蒂还需要多少钱才能买到钱包？
    |'
- en: '| Q1: How much money does Betty have? |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| Q1: 贝蒂有多少钱？ |'
- en: '| Q2: How much money did Betty’s parents give her? |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Q2: 贝蒂的父母给了她多少钱？ |'
- en: '| Q3: How much money did Betty’s grandparents give her? |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| Q3: 贝蒂的祖父母给了她多少钱？ |'
- en: '| Q4: How much money does Betty still need to buy the wallet? |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Q4: 贝蒂还需要多少钱才能买到钱包？ |'
- en: '| Response |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 响应 |'
- en: '| Q1: Yes |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| Q1: 是 |'
- en: '| Q2: Yes |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Q2: 是 |'
- en: '| Q3: Yes |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Q3: 是 |'
- en: '| Q4: Yes |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| Q4: 是 |'
- en: 'Table 12: Sample ChatGPT input for generating sub-questions feedback and one
    of the corresponding responses.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '| 表12: 生成子问题反馈和其中一个相应回答的ChatGPT输入示例。 |'
