- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:05:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:05:51
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 和 GNN 是互补的：为多模态图学习提炼 LLM
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.01032](https://ar5iv.labs.arxiv.org/html/2406.01032)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.01032](https://ar5iv.labs.arxiv.org/html/2406.01032)
- en: \useunder
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \useunder
- en: \ul
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \ul
- en: Junjie Xu, Zongyu Wu, Minhua Lin, Xiang Zhang, Suhang Wang
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 许俊杰，吴宗瑜，林敏华，张翔，王苏航
- en: The Pennsylvania State University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 宾夕法尼亚州立大学
- en: '{junjiexu, zongyuwu, mfl5681, xzz89, szw494}@psu.edu'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{junjiexu, zongyuwu, mfl5681, xzz89, szw494}@psu.edu'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent progress in Graph Neural Networks (GNNs) has greatly enhanced the ability
    to model complex molecular structures for predicting properties. Nevertheless,
    molecular data encompasses more than just graph structures, including textual
    and visual information that GNNs do not handle well. To bridge this gap, we present
    an innovative framework that utilizes multimodal molecular data to extract insights
    from Large Language Models (LLMs). We introduce GALLON (Graph Learning from Large
    Language Model Distillation), a framework that synergizes the capabilities of
    LLMs and GNNs by distilling multimodal knowledge into a unified Multilayer Perceptron
    (MLP). This method integrates the rich textual and visual data of molecules with
    the structural analysis power of GNNs. Extensive experiments reveal that our distilled
    MLP model notably improves the accuracy and efficiency of molecular property predictions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，图神经网络（GNNs）的进展大大增强了对复杂分子结构进行建模以预测性质的能力。然而，分子数据不仅仅包括图结构，还包含文本和视觉信息，而 GNNs
    对这些信息处理不佳。为弥补这一差距，我们提出了一个创新框架，利用多模态分子数据从大语言模型（LLMs）中提取洞见。我们介绍了 GALLON（从大语言模型蒸馏的图学习），这是一个通过将多模态知识蒸馏到统一的多层感知器（MLP）中，协同
    LLMs 和 GNNs 能力的框架。这种方法将分子的丰富文本和视觉数据与 GNNs 的结构分析能力相结合。大量实验表明，我们蒸馏的 MLP 模型显著提高了分子性质预测的准确性和效率。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In recent years, Graph Neural Networks (GNNs) have demonstrated exceptional
    prowess in representing learning on graph-structured data [[20](#bib.bib20), [41](#bib.bib41),
    [34](#bib.bib34), [40](#bib.bib40)]. Within the domain of chemistry, GNNs have
    been notably effective in predicting molecular properties [[13](#bib.bib13), [38](#bib.bib38)],
    a task critical for advancements in various domains, such as drug discovery [[12](#bib.bib12)]
    and materials science [[27](#bib.bib27)]. By conceptualizing molecules as graphs,
    GNNs manage to capture the nuanced spatial and chemical relationships that dictate
    molecular behavior. This unique ability has positioned GNNs as a leading tool
    in chemistry, enabling more accurate and efficient prediction models that surpass
    traditional methodologies [[11](#bib.bib11)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，图神经网络（GNNs）在图结构数据的表示学习上展现了卓越的能力 [[20](#bib.bib20), [41](#bib.bib41), [34](#bib.bib34),
    [40](#bib.bib40)]。在化学领域，GNNs 在预测分子性质方面尤其有效 [[13](#bib.bib13), [38](#bib.bib38)]，这是推动药物发现 [[12](#bib.bib12)]
    和材料科学 [[27](#bib.bib27)] 等多个领域发展的关键任务。通过将分子概念化为图，GNNs 能够捕捉决定分子行为的细微空间和化学关系。这一独特能力使得
    GNNs 成为化学领域的领先工具，能够提供比传统方法更准确、更高效的预测模型 [[11](#bib.bib11)]。
- en: 'Despite the strengths of GNNs in processing graph-structured data, their application
    scope reveals limitations when encountering molecular data in forms other than
    graph structures. Molecules, in their essence, exhibit multimodal characteristics
    that can also be represented in various forms, including textual Simplified Molecular-Input
    Line-entry System (SMILES) strings [[37](#bib.bib37)] and visual molecular diagrams.
    An example of this multimodal representation is illustrated on the left side of
    Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM and GNN are Complementary: Distilling
    LLM for Multimodal Graph Learning"). These modalities provide comprehensive information
    toward the understanding of molecules. SMILES strings offer a compact and linear
    textual representation of a molecule’s structure, which some textual models can
    learn. Molecular diagrams provide a visual and intuitive depiction of molecular
    structures, making specific structures and functional groups, like benzene rings,
    more identifiable. Additionally, a graph adjacency matrix encapsulates detailed
    information about the connections between nodes within the molecule. Therefore,
    each modality provides complementary advantages to each other. However, while
    GNNs excel in node features and graph structures, they falter in processing and
    extracting valuable information from other modalities. This limitation underscores
    a significant gap in the current framework, highlighting the need for models that
    can understand molecules’ modalities from all aspects and learn effective representations.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管图神经网络（GNNs）在处理图结构数据方面具有优势，但它们在遇到非图结构的分子数据时显示出局限性。分子本质上具有多模态特征，也可以以各种形式表示，包括文本的简化分子输入线性表示系统（SMILES）字符串[[37](#bib.bib37)]和视觉分子图示。左侧的图示例展示了这种多模态表示的实例，如图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ LLM and GNN are Complementary: Distilling LLM for
    Multimodal Graph Learning")所示。这些模态提供了关于分子的全面信息。SMILES 字符串提供了分子结构的紧凑而线性的文本表示，一些文本模型可以学习这一点。分子图示提供了分子结构的视觉和直观描绘，使得特定结构和功能组，如苯环，更容易识别。此外，图的邻接矩阵包含了关于分子内节点之间连接的详细信息。因此，每种模态相互补充各自的优势。然而，虽然
    GNN 在节点特征和图结构方面表现出色，但在处理和提取其他模态的有价值信息方面却表现不佳。这一局限性突显了当前框架中的一个重大缺口，强调了需要能够从各个方面理解分子模态并学习有效表示的模型。'
- en: In parallel with the advancements in GNNs, the evolution of Large Language Models
    (LLMs) [[25](#bib.bib25), [3](#bib.bib3), [44](#bib.bib44)] and LLMs with vision [[2](#bib.bib2),
    [30](#bib.bib30), [10](#bib.bib10), [26](#bib.bib26)] has marked a new age in
    machine learning, characterized by their exceptional skill in parsing and interpreting
    textual and visual data. For example, GPT4V has shown the ability to answer questions
    about molecular diagrams [[1](#bib.bib1)]. However, recent works show that LLMs
    exhibit a shortcoming in their ability to process graph-structured data [[5](#bib.bib5),
    [21](#bib.bib21)], an area where GNNs excel. Yet, their proficiency in dealing
    with molecular data represented as SMILES strings and diagrams suggests a valuable
    complementary role to GNNs. Meanwhile, through the huge amount of training data,
    LLMs have their understanding of the real world and have prior knowledge about
    the SMILES string and chemical molecules. Based on these observations, we aim
    to incorporate the knowledge from LLM and make LLMs and GNNs complementary when
    dealing with different modalities. One potential way is to fuse LLM and GNN by
    distilling them into a new model. However, querying LLM is both computationally
    and financially demanding, making it impractical to apply to each molecule when
    handling large-scale data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GNN 的进展相辅相成的是，大型语言模型（LLMs）的演变[[25](#bib.bib25), [3](#bib.bib3), [44](#bib.bib44)]以及具有视觉能力的
    LLMs[[2](#bib.bib2), [30](#bib.bib30), [10](#bib.bib10), [26](#bib.bib26)]标志着机器学习的新时代，其特点是它们在解析和解释文本和视觉数据方面的卓越能力。例如，GPT4V
    展现了回答有关分子图示问题的能力[[1](#bib.bib1)]。然而，最近的研究表明，LLMs 在处理图结构数据方面存在不足[[5](#bib.bib5),
    [21](#bib.bib21)]，这是 GNN 擅长的领域。然而，它们在处理作为 SMILES 字符串和图示表示的分子数据方面的能力表明，它们可以为 GNN
    提供有价值的互补作用。同时，通过大量的训练数据，LLMs 对现实世界有了理解，并且对 SMILES 字符串和化学分子有了先验知识。基于这些观察，我们旨在整合
    LLM 的知识，使 LLM 和 GNN 在处理不同模态时相互补充。一种潜在的方法是通过将 LLM 和 GNN 蒸馏成一个新模型来融合它们。然而，查询 LLM
    在计算和财务上都很有挑战，使得在处理大规模数据时难以应用于每个分子。
- en: '![Refer to caption](img/b7730df586c38bdab788d758c1955438.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b7730df586c38bdab788d758c1955438.png)'
- en: 'Figure 1: The framework of GALLON (Graph Learning from Large Language Model
    Distillation).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：GALLON（从大语言模型蒸馏的图学习）的框架。
- en: 'To this end, we propose to distill LLM and GNN to a smaller Multilayer Perceptron
    (MLP). On the one hand, the MLP is trained with distillation from both LLM and
    GNN, incorporating knowledge from the modalities of GNN and LLM where they excel.
    On the other hand, the MLP offers greater efficiency and cost-effectiveness during
    the inference process, eliminating the need for querying the LLMs for each single
    molecule. Fig. [4](#S3.F4 "Figure 4 ‣ 3.2 Pretraining and Finetuning ‣ 3 Methodology:
    GALLON ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning")
    and Fig. [4](#S3.F4 "Figure 4 ‣ 3.2 Pretraining and Finetuning ‣ 3 Methodology:
    GALLON ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning")
    show the time and size efficiency of our model.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '为此，我们提出将 LLM 和 GNN 蒸馏到一个较小的多层感知机（MLP）。一方面，MLP 通过从 LLM 和 GNN 中进行蒸馏进行训练，结合了 GNN
    和 LLM 各自在其擅长领域的知识。另一方面，MLP 在推理过程中提供了更高的效率和成本效益，避免了对每个单独分子的查询 LLM。图 [4](#S3.F4
    "Figure 4 ‣ 3.2 Pretraining and Finetuning ‣ 3 Methodology: GALLON ‣ LLM and GNN
    are Complementary: Distilling LLM for Multimodal Graph Learning") 和图 [4](#S3.F4
    "Figure 4 ‣ 3.2 Pretraining and Finetuning ‣ 3 Methodology: GALLON ‣ LLM and GNN
    are Complementary: Distilling LLM for Multimodal Graph Learning") 展示了我们模型的时间和大小效率。'
- en: However, several challenges hinder the distillation process. First, while LLMs
    have demonstrated their power, we empirically find that they struggle with direct
    molecular property prediction. This difficulty arises not only because such classification
    or regression tasks challenge text-generation models, but also due to the lack
    of domain-specific training in LLMs. To mitigate this, we finetune a smaller Language
    Model (LM) to serve as the encoder for the outputs of the LLM. This approach allows
    the LM to specialize in the domain relevant to the prediction task, ensuring that
    the learned representations are more effective for the knowledge distillation
    process needed for molecular property prediction. Secondly, most methods for graph
    distillation [[31](#bib.bib31), [42](#bib.bib42)] have concentrated on classification
    tasks, where the goal is to align the predicted class distribution between the
    student and teacher models. However, graph regression remains a vital and common
    task, particularly for predicting molecular properties. Distilling directly from
    labels in regression tasks presents challenges because the label is a scalar,
    which provides less information compared to the distribution of labels in classification
    tasks. Therefore, we employ a mapping function that maps the embeddings of the
    teacher and student models to a latent space, where we perform the distillation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，几个挑战阻碍了知识蒸馏过程。首先，尽管大语言模型（LLMs）已展示出其强大能力，但我们实证发现它们在直接预测分子属性方面存在困难。这一困难不仅因为此类分类或回归任务对文本生成模型构成挑战，还因为
    LLM 缺乏领域特定的训练。为此，我们对一个较小的语言模型（LM）进行微调，使其作为 LLM 输出的编码器。这种方法使 LM 能够专注于与预测任务相关的领域，确保所学表示在分子属性预测所需的知识蒸馏过程中更加有效。其次，大多数图蒸馏方法 [[31](#bib.bib31),
    [42](#bib.bib42)] 主要集中在分类任务上，目标是使学生模型和教师模型之间的预测类别分布对齐。然而，图回归仍然是一个重要且常见的任务，特别是用于预测分子属性。从回归任务中的标签直接进行蒸馏存在挑战，因为标签是一个标量，相较于分类任务中标签的分布提供的信息较少。因此，我们采用了一个映射函数，将教师和学生模型的嵌入映射到潜在空间，在那里我们进行蒸馏。
- en: 'In light of these considerations, we propose utilizing multimodal information
    from molecular data, i.e., SMILES strings, molecular diagrams, graph structures,
    and node features, to query the Large Language Model (LLM) and obtain a detailed
    description of the molecule. This description will then serve as the knowledge
    base for distillation into a smaller MLP model. Furthermore, we propose a novel
    approach GALLON (Graph Learning from Large Language Model Distillation) that synergizes
    the advantages of GNNs and LLMs through a distillation process into a Multilayer
    Perceptron (MLP) model, which makes the inference process more efficient with
    state-of-the-art performance. By distilling the capabilities of GNNs in handling
    graph structures and the adeptness of LLMs in processing textual and visual information,
    we create a unified framework for learning more efficient representations across
    the multimodality of molecular data. To summarize, our contributions are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些考虑，我们建议利用来自分子数据的多模态信息，即 SMILES 字符串、分子图、图结构和节点特征，来查询大型语言模型（LLM）并获得分子的详细描述。该描述将作为知识库，用于提炼成更小的
    MLP 模型。此外，我们提出了一种新方法 GALLON（Graph Learning from Large Language Model Distillation），通过提炼过程将
    GNN 和 LLM 的优点协同到一个多层感知机（MLP）模型中，从而使推理过程更加高效，性能达到最先进水平。通过提炼 GNN 在处理图结构中的能力和 LLM
    在处理文本和视觉信息中的熟练度，我们创建了一个统一的框架，以在分子数据的多模态性中学习更高效的表征。总而言之，我们的贡献包括：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose utilizing multimodal molecular data to learn representations and
    extract prior knowledge from powerful and pretrained large language models by
    leveraging their multimodality capabilities.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们建议利用多模态的分子数据来学习表征，并通过利用其多模态能力，从强大且预训练的大型语言模型中提取先验知识。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a framework that synergizes GNNs and LLMs, leveraging their complementary
    strengths. This framework distills the advantages of GNN and LLM into an MLP,
    aiming to capture the most effective representations for molecular structures.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个框架，将 GNN 和 LLM 进行协同，利用它们的互补优势。该框架将 GNN 和 LLM 的优点提炼为一个 MLP，旨在捕捉分子结构中最有效的表征。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct extensive experiments to demonstrate the superiority of our approach
    in distilling GNN and LLM knowledge into an MLP, which outperforms both GNNs and
    LLMs across various datasets, achieving greater efficiency and an even smaller
    model size.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进行了一系列广泛的实验，以证明我们的方法在将 GNN 和 LLM 知识提炼为 MLP 上的优越性。该方法在各种数据集上超越了 GNN 和 LLM，达到了更高的效率和更小的模型尺寸。
- en: 2 Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLMs for Graphs. Recent studies have shown significant interest in applying
    LLMs to graph data, marking a pivotal shift in graph learning research. These
    models have been adapted to address challenges such as encoding node and edge
    information and preserving topological structures. TAPE [[15](#bib.bib15)] uses
    LLMs to generate explanations for nodes, which are then used as augmented features
    to train GNNs. GraphLLM [[4](#bib.bib4)] encodes graphs into text for LLM prediction.
    Das et al. [[7](#bib.bib7)] explore the integration of graph data with LLMs and
    the influence of graph multimodalities. CaR [[24](#bib.bib24)] uses molecular
    SMILES strings to obtain captions from LLMs, which are then input into another
    language model for finetuning. Surveys by Li et al. [[21](#bib.bib21)] and Chen
    et al. [[5](#bib.bib5)] provide a comprehensive understanding of LLM performance
    on graphs, dividing them into roles such as enhancer, predictor, and alignment.
    In contrast to prior works, we use LLMs as teachers in the distillation process,
    leveraging their prior knowledge about molecules to enhance the learning of the
    student model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 在图数据中的应用。最近的研究显示了在图数据上应用 LLM 的显著兴趣，这标志着图学习研究的一个关键转变。这些模型已经被调整以解决节点和边信息编码以及保持拓扑结构等挑战。TAPE [[15](#bib.bib15)]
    使用 LLM 为节点生成解释，然后将这些解释作为增强特征来训练 GNN。GraphLLM [[4](#bib.bib4)] 将图编码为文本以进行 LLM 预测。Das
    等人 [[7](#bib.bib7)] 探讨了图数据与 LLM 的集成以及图的多模态性对 LLM 的影响。CaR [[24](#bib.bib24)] 使用分子
    SMILES 字符串从 LLM 中获取说明，然后将其输入到另一个语言模型中进行微调。Li 等人 [[21](#bib.bib21)] 和 Chen 等人 [[5](#bib.bib5)]
    的调查提供了对 LLM 在图数据上表现的全面理解，将其分为增强器、预测器和对齐等角色。与之前的工作相比，我们在提炼过程中将 LLM 作为教师，利用其关于分子的先验知识来增强学生模型的学习。
- en: Graph Knowledge Distillation. Graph Knowledge Distillation constructs a smaller
    but efficient model by extracting more knowledge from data, aiming for the compressed
    model and improved performance [[31](#bib.bib31)]. Various works distill GNNs
    onto various models. For example, LSP [[45](#bib.bib45)] distills a teacher’s
    GNN to a student’s GNN by minimizing the distance of local structure between them.
    T2GNN [[18](#bib.bib18)] employs MLP and GNN as the feature teacher and structure
    teacher to let a student GNN learn from them. NOSMOG [[32](#bib.bib32)] and GLNN [[46](#bib.bib46)]
    show that distilled MLP also have the ability to outperform GNN methods. Though
    abundant works adopt different strategies for graph knowledge distillation, most
    of them do not focus on the molecule data with graph classification tasks, and
    they cannot deal with the multimodal information in the molecule data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图知识蒸馏。图知识蒸馏通过从数据中提取更多知识来构建一个更小但高效的模型，旨在实现压缩模型和性能提升[[31](#bib.bib31)]。各种研究将GNN蒸馏到不同模型中。例如，LSP
    [[45](#bib.bib45)] 通过最小化它们之间的局部结构距离，将教师的GNN蒸馏到学生的GNN中。T2GNN [[18](#bib.bib18)]
    利用MLP和GNN作为特征教师和结构教师，让学生GNN从中学习。NOSMOG [[32](#bib.bib32)] 和GLNN [[46](#bib.bib46)]
    显示蒸馏的MLP也有超越GNN方法的能力。尽管大量工作采用不同的图知识蒸馏策略，但大多数并未关注分子数据中的图分类任务，且无法处理分子数据中的多模态信息。
- en: Multimodal Learning for Molecules. Several studies have focused on leveraging
    single modalities for molecular representation learning. ChemBERTa [[6](#bib.bib6)]
    uses SMILES strings to pretrain a BERT model, treating molecules purely as text.
    GNN methods [[17](#bib.bib17), [36](#bib.bib36), [35](#bib.bib35)] enhance learning
    by combining features from individual atoms and their connections. Recent efforts
    have explored multimodal approaches. MolT5 [[9](#bib.bib9)] pretrains on SMILES
    strings and molecule captions, while MoMu [[29](#bib.bib29)] merges molecule graphs
    with natural language through contrastive learning. MolFM [[23](#bib.bib23)] integrates
    structures, texts, and knowledge graphs for a comprehensive understanding of molecular
    properties. In this work, GALLON incorporates SMILES strings, molecular diagrams,
    and molecule graphs to train effective molecular representations and distill the
    knowledge to an MLP.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 分子多模态学习。一些研究集中在利用单一模态进行分子表征学习。ChemBERTa [[6](#bib.bib6)] 使用SMILES字符串对BERT模型进行预训练，将分子纯粹视为文本。GNN方法[[17](#bib.bib17),
    [36](#bib.bib36), [35](#bib.bib35)] 通过结合单个原子及其连接的特征来增强学习。近期的努力探索了多模态方法。MolT5 [[9](#bib.bib9)]
    在SMILES字符串和分子标题上进行预训练，而MoMu [[29](#bib.bib29)] 通过对比学习将分子图与自然语言相结合。MolFM [[23](#bib.bib23)]
    结合结构、文本和知识图谱，以全面理解分子属性。在本工作中，GALLON结合SMILES字符串、分子图和分子图谱，以训练有效的分子表征，并将知识蒸馏到MLP中。
- en: '3 Methodology: GALLON'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论：GALLON
- en: In this section, we first define notations and formally define the problem.
    We then give an overview of GALLON followed by detailed descriptions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先定义符号并正式定义问题。接着，我们概述了GALLON，并提供详细描述。
- en: Notation and Problem Definition. Let $\mathcal{G}=\{\mathcal{V},\mathcal{E},\mathbf{X},\mathcal{S},\mathcal{I}\}$.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 符号和问题定义。设$\mathcal{G}=\{\mathcal{V},\mathcal{E},\mathbf{X},\mathcal{S},\mathcal{I}\}$。
- en: 'Overview of GALLON. An illustration of the framework is shown in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ LLM and GNN are Complementary: Distilling LLM for
    Multimodal Graph Learning"), which has three main components: 1) Input the multimodal
    data into the LLM to extract knowledge from it. 2) Fine-tuning a smaller LM to
    encode the text outputs of the LLM and pretraining a GNN. 3) Distilling the knowledge
    extracted from both the GNN and LLM into an MLP. It is important to note a limitation
    that using MLP with atom input without a graph structure cannot differentiate
    isomers—compounds with the same formula but different atom arrangements. However,
    the node features of the molecular dataset include essential structure information
    (e.g., degree, ring, aromatic, chirality). These structural features enable our
    method to classify compounds effectively, despite the absence of a graph structure,
    by leveraging differences in properties such as chirality and ring structure.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: GALLON 概述。框架的示意图如图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 大语言模型和图神经网络是互补的：为多模态图学习提炼大语言模型")
    所示，其中包含三个主要组件：1）将多模态数据输入大语言模型以提取知识。2）微调一个较小的语言模型以编码大语言模型的文本输出，并预训练一个图神经网络。3）将从图神经网络和大语言模型中提取的知识提炼到多层感知机中。需要注意的是，使用没有图结构的多层感知机（MLP）无法区分异构体——那些具有相同分子式但原子排列不同的化合物。然而，分子数据集的节点特征包含了必要的结构信息（例如度、环、芳香性、手性）。这些结构特征使我们的方法能够有效地分类化合物，尽管缺少图结构，通过利用如手性和环结构等性质的差异。
- en: 3.1 Extracting Knowledge from LLMs
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 从大语言模型中提取知识
- en: '![Refer to caption](img/0b1e385b61c670e18fed136e4d08e638.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0b1e385b61c670e18fed136e4d08e638.png)'
- en: 'Figure 2: An example prompt for a molecule of the Freesolv dataset.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Freesolv 数据集中一个分子的示例提示。
- en: Multimodal Information. As stated previously, LLMs excel at processing text
    information and are trained on diverse datasets, enabling them to gain extensive
    knowledge about interpreting molecules and SMILES strings. Consequently, we utilize
    the SMILES string $\mathcal{S}$ of the molecule into the LLM.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态信息。如前所述，大语言模型（LLMs）擅长处理文本信息，并且在多样化的数据集上进行训练，使其能够广泛掌握解释分子和SMILES字符串的知识。因此，我们将分子的SMILES字符串
    $\mathcal{S}$ 输入到大语言模型中。
- en: 'Prompt Construction. To enable the LLM to act like a chemistry expert and generate
    useful information about the input molecule, it is crucial to design an effective
    prompt. Fig. [2](#S3.F2 "Figure 2 ‣ 3.1 Extracting Knowledge from LLMs ‣ 3 Methodology:
    GALLON ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning")
    shows an example prompt we designed for a molecule in the Freesolv Dataset. A
    prompt is composed of three parts: (1) The general prompt, which remains the same
    across all datasets; (2) Dataset-specific description that describes the characteristics
    of the dataset and the property we want to predict; (3) Molecule information containing
    the SMILES string, molecular diagram, and graph structure. As shown in Fig. [2](#S3.F2
    "Figure 2 ‣ 3.1 Extracting Knowledge from LLMs ‣ 3 Methodology: GALLON ‣ LLM and
    GNN are Complementary: Distilling LLM for Multimodal Graph Learning"), we convert
    the graph structure into texts by describing the atom name and bond type for each
    edge. This designed prompt provides LLM with the general goal, the prediction
    task, and the multimodal molecule information. Then we use the generated prompt
    $\mathcal{P}$, the process can be formally written as'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 提示构建。为了使大语言模型像化学专家一样工作并生成关于输入分子的有用信息，设计一个有效的提示是至关重要的。图 [2](#S3.F2 "图 2 ‣ 3.1
    从大语言模型中提取知识 ‣ 3 方法论：GALLON ‣ 大语言模型和图神经网络是互补的：为多模态图学习提炼大语言模型") 显示了我们为Freesolv数据集中一个分子设计的示例提示。提示由三个部分组成：(1)
    一般提示，在所有数据集中保持不变；(2) 数据集特定描述，描述数据集的特征和我们希望预测的属性；(3) 分子信息，包括SMILES字符串、分子图和图结构。如图
    [2](#S3.F2 "图 2 ‣ 3.1 从大语言模型中提取知识 ‣ 3 方法论：GALLON ‣ 大语言模型和图神经网络是互补的：为多模态图学习提炼大语言模型")
    所示，我们通过描述每个边的原子名称和键类型，将图结构转换为文本。这种设计的提示向大语言模型提供了一般目标、预测任务和多模态分子信息。然后，我们使用生成的提示
    $\mathcal{P}$，可以将过程正式写为
- en: '|  | $\mathcal{R}_{i}=\text{LLM}(\mathcal{P}_{i},\mathcal{E}_{i},\mathcal{S}_{i},\mathcal{I}_{i}),$
    |  | (1) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{R}_{i}=\text{LLM}(\mathcal{P}_{i},\mathcal{E}_{i},\mathcal{S}_{i},\mathcal{I}_{i}),$
    |  | (1) |'
- en: where $\mathcal{E}_{i},\mathcal{S}_{i},\mathcal{I}_{i}$ are the graph edges,
    SMILE string, and molecular diagram respectively.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{E}_{i},\mathcal{S}_{i},\mathcal{I}_{i}$ 分别是图的边、SMILES字符串和分子图。
- en: 3.2 Pretraining and Finetuning
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 预训练与微调
- en: As the response $\mathcal{R}$ is written as
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 响应 $\mathcal{R}$ 被表示为
- en: '|  | $h^{\text{LM}}_{i}=\text{LM}(\mathcal{R}_{i},\mathcal{S}_{i})\in\mathbb{R}^{H},$
    |  | (2) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $h^{\text{LM}}_{i}=\text{LM}(\mathcal{R}_{i},\mathcal{S}_{i})\in\mathbb{R}^{H},$
    |  | (2) |'
- en: where $H$ on top of the embedding to predict the molecular property as
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H$ 用于预测分子属性，如
- en: '|  | $\hat{y}^{\text{LM}}_{i}=\mathcal{T}_{\text{LM}}(h^{\text{LM}}_{i})\in\mathbb{R}^{d},$
    |  | (3) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}^{\text{LM}}_{i}=\mathcal{T}_{\text{LM}}(h^{\text{LM}}_{i})\in\mathbb{R}^{d},$
    |  | (3) |'
- en: where $d$ with cross-entropy for classification and Root Mean Square Error (RMSE)
    for regression.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d$ 用交叉熵进行分类，用均方根误差（RMSE）进行回归。
- en: To facilitate the knowledge distillation from graphs, we pretrain a GNN with
    the graph structure and node features. Specifically, for each molecule $\mathcal{G}_{i}$,
    we first adopt GNN with a pooling layer to get graph representation as
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便从图中进行知识蒸馏，我们用图结构和节点特征对GNN进行预训练。具体来说，对于每个分子 $\mathcal{G}_{i}$，我们首先采用带有池化层的
    GNN 以获得图表示，如
- en: '|  | $h^{\text{GNN}}_{i}=\text{GNN}(\mathbf{X}_{i},\mathcal{E}_{i})\in\mathbb{R}^{H},$
    |  | (4) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $h^{\text{GNN}}_{i}=\text{GNN}(\mathbf{X}_{i},\mathcal{E}_{i})\in\mathbb{R}^{H},$
    |  | (4) |'
- en: We then add a transformation layer $\mathcal{T}_{\text{GNN}}$ to predict the
    molecular property as
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们添加一个变换层 $\mathcal{T}_{\text{GNN}}$ 来预测分子属性，如
- en: '|  | $\hat{y}^{\text{GNN}}_{i}=\mathcal{T}_{\text{GNN}}(h^{\text{GNN}}_{i})\in\mathbb{R}^{d},$
    |  | (5) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}^{\text{GNN}}_{i}=\mathcal{T}_{\text{GNN}}(h^{\text{GNN}}_{i})\in\mathbb{R}^{d},$
    |  | (5) |'
- en: Similarly, $\mathcal{T}_{\text{GNN}}$ is typically implemented using a linear
    layer for regression, and a linear layer followed by a Softmax function for classification.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，$\mathcal{T}_{\text{GNN}}$ 通常通过一个线性层实现回归，并通过一个线性层加上 Softmax 函数实现分类。
- en: '![Refer to caption](img/f8c99e79c076979a732bda2aab64f2cf.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f8c99e79c076979a732bda2aab64f2cf.png)'
- en: 'Figure 3: ROCAUC vs log of inference time (ms) on the BACE dataset.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：BACE 数据集上的 ROCAUC 与推理时间（毫秒）的对数关系。
- en: '![Refer to caption](img/0f608c71a41ba876956a2920c70d71cf.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0f608c71a41ba876956a2920c70d71cf.png)'
- en: 'Figure 4: ROCAUC vs log of number of parameters on the BACE dataset.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：BACE 数据集上的 ROCAUC 与参数数量的对数关系。
- en: 3.3 Knowledge Distillation to MLP
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 知识蒸馏到 MLP
- en: 'With the learned representations of LLM and GNN, we propose to distill their
    knowledge into an MLP for several compelling reasons: (i) operating an LLM is
    both computationally and financially burdensome. In large-scale datasets, querying
    each molecule through the LLM to extract descriptions and prior knowledge is impractical.
    Through distillation, we can embed the prior knowledge from the LLM into the MLP,
    allowing us to use the MLP for efficient inference; and (ii) many studies [[32](#bib.bib32),
    [43](#bib.bib43), [46](#bib.bib46), [47](#bib.bib47)] have demonstrated that with
    effective distillation, an MLP can achieve performance on par with, or even superior
    to, a GNN, while significantly reducing inference time. Figures [4](#S3.F4 "Figure
    4 ‣ 3.2 Pretraining and Finetuning ‣ 3 Methodology: GALLON ‣ LLM and GNN are Complementary:
    Distilling LLM for Multimodal Graph Learning") and [4](#S3.F4 "Figure 4 ‣ 3.2
    Pretraining and Finetuning ‣ 3 Methodology: GALLON ‣ LLM and GNN are Complementary:
    Distilling LLM for Multimodal Graph Learning") demonstrate that GALLON achieves
    state-of-the-art performance with less time and a smaller model size.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '利用LLM和GNN的学习表示，我们建议将它们的知识蒸馏到MLP中，原因有几个：（i）操作LLM在计算和财务上都很繁重。在大规模数据集中，通过LLM查询每个分子以提取描述和先验知识是不切实际的。通过蒸馏，我们可以将LLM中的先验知识嵌入到MLP中，使我们可以使用MLP进行高效推理；（ii）许多研究 [[32](#bib.bib32),
    [43](#bib.bib43), [46](#bib.bib46), [47](#bib.bib47)] 已经证明，通过有效的蒸馏，MLP的性能可以与GNN相当，甚至优于GNN，同时显著减少推理时间。图 [4](#S3.F4
    "Figure 4 ‣ 3.2 Pretraining and Finetuning ‣ 3 Methodology: GALLON ‣ LLM and GNN
    are Complementary: Distilling LLM for Multimodal Graph Learning") 和 [4](#S3.F4
    "Figure 4 ‣ 3.2 Pretraining and Finetuning ‣ 3 Methodology: GALLON ‣ LLM and GNN
    are Complementary: Distilling LLM for Multimodal Graph Learning") 展示了GALLON在更少时间和更小模型大小下达到了最先进的性能。'
- en: Additionally, the node attributes in the datasets are not simply atomic numbers
    but include a wide range of information (e.g. chirality, degree, formal charge,
    hydrogens, radical electrons, hybridization, aromaticity, and ring membership).
    These comprehensive node features, including some graph structural information,
    make it feasible to achieve good prediction performance using only the node attributes
    with an MLP. However, these studies primarily focus on distilling from GNN to
    MLP and do not incorporate LLM. Therefore, in this section, our goal is to train
    an MLP via distillation, enabling it to match or exceed the performance of either
    GNN or LLM alone. Specifically, for a molecule $\mathcal{G}_{i}$ as input followed
    by average pooling to get molecule representation as
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据集中的节点属性不仅仅是原子序数，还包括各种信息（例如手性、度数、正式电荷、氢原子、自由基电子、杂化、芳香性和环成员）。这些综合的节点特征，包括一些图结构信息，使得仅使用节点属性和MLP就能够实现良好的预测性能。然而，这些研究主要集中在从GNN蒸馏到MLP，而没有结合LLM。因此，在本节中，我们的目标是通过蒸馏训练一个MLP，使其能够匹配或超过单独使用GNN或LLM的性能。具体来说，对于一个分子
    $\mathcal{G}_{i}$ 作为输入，随后通过平均池化获得分子表示为
- en: '|  | $1$2 |  | (6) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: where $n_{i}$ to get prediction
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $n_{i}$ 用于获取预测值
- en: '|  | $\hat{y}^{\text{MLP}}_{i}=\mathcal{T}_{\text{MLP}}(h^{\text{MLP}}_{i})\in\mathbb{R}^{d}$
    |  | (7) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}^{\text{MLP}}_{i}=\mathcal{T}_{\text{MLP}}(h^{\text{MLP}}_{i})\in\mathbb{R}^{d}$
    |  | (7) |'
- en: During the training, the loss is calculated between the prediction label and
    ground truth,
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，损失是通过预测标签和真实标签之间的差异来计算的，
- en: '|  | $\mathcal{L}_{0}=\mathcal{L}_{pred}(\hat{y}^{\text{MLP}},y),$ |  | (8)
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{0}=\mathcal{L}_{pred}(\hat{y}^{\text{MLP}},y),$ |  | (8)
    |'
- en: where $\mathcal{L}_{pred}$ is cross entropy in classification tasks and distance-based
    RMSE in regression tasks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{L}_{pred}$在分类任务中是交叉熵，在回归任务中是基于距离的RMSE。
- en: To better distill the knowledge from LLM and GNN, we use two forms of distillation,
    i.e., representation distillation and label distillation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地蒸馏来自LLM和GNN的知识，我们使用了两种形式的蒸馏，即表示蒸馏和标签蒸馏。
- en: Label Distillation. Label distillation aims to let the student’s prediction
    mimic the distribution of the teacher’s prediction, which can be written as
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 标签蒸馏。标签蒸馏旨在让学生的预测模仿教师预测的分布，可以写作
- en: '|  | $1$2 |  | (9) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: where $\alpha$.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$。
- en: 'Representation Distillation. In regression tasks, the target label is a scalar
    value, and calculating the distance between two scalar values yields limited information
    for distillation purposes. To address this challenge, we propose utilizing representation
    distillation to align the student’s learned representations with those of the
    teacher. Given that LLM and the GNN are trained independently, their respective
    representations, $h^{\text{LM}}$. Directly aligning these representations could
    be problematic due to the mismatch in feature spaces. To overcome this, we transform
    them into a common latent space. Subsequently, we facilitate the distillation
    of knowledge between the student and teachers’ models within this shared latent
    space, which can be mathematically represented as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表示蒸馏。在回归任务中，目标标签是一个标量值，计算两个标量值之间的距离为蒸馏目的提供的信息有限。为了解决这个问题，我们提出利用表示蒸馏来对齐学生学习到的表示与教师的表示。由于LLM和GNN是独立训练的，它们各自的表示
    $h^{\text{LM}}$。直接对齐这些表示可能会因特征空间的不匹配而出现问题。为了克服这个问题，我们将它们转换到一个共同的潜在空间。随后，我们在这个共享的潜在空间中促进学生与教师模型之间的知识蒸馏，这可以数学上表示为：
- en: '|  | $1$2 |  | (10) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (10) |'
- en: where both $\mathcal{L}_{LM}^{RD}$ are implemented by RMSE. For simplicity,
    in this paper, we adopt label distillation for classification tasks and representation
    distillation for regression tasks, but both of them can be applied to any task.
    With the loss function for knowledge distillation, the final objective functions
    for training MLP for classification and regression are given as
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{L}_{LM}^{RD}$都通过RMSE实现。为简化起见，在本文中，我们采用标签蒸馏用于分类任务，采用表示蒸馏用于回归任务，但这两种方法都可以应用于任何任务。通过知识蒸馏的损失函数，分类和回归任务中训练MLP的最终目标函数如下
- en: '|  | $\mathcal{L}^{class}=\mathcal{L}^{0}+\mathcal{L}^{LD},\qquad\mathcal{L}^{reg}=\mathcal{L}^{0}+\mathcal{L}^{RD}.$
    |  | (11) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}^{class}=\mathcal{L}^{0}+\mathcal{L}^{LD},\qquad\mathcal{L}^{reg}=\mathcal{L}^{0}+\mathcal{L}^{RD}.$
    |  | (11) |'
- en: 4 Experiments
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'In this section, we present experiments to evaluate the effectiveness of the
    proposed GALLON framework, addressing the following research questions: RQ1: How
    does the proposed GALLON framework perform compared to GNN and NLP models? RQ2:
    Do both GNN and LLM contribute to the final results? RQ3: To what extent does
    multimodality enhance LLM predictions? RQ4: What is the influence of different
    LLM models on performance?'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了实验以评估所提议的GALLON框架的有效性，并解决以下研究问题：RQ1：所提的GALLON框架与GNN和NLP模型相比表现如何？RQ2：GNN和LLM是否都对最终结果有所贡献？RQ3：多模态性在多大程度上增强了LLM预测？RQ4：不同LLM模型对性能有何影响？
- en: 4.1 Experimental Setup
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'Datasets. To evaluate the performance of GALLON for molecule property prediction,
    we adopt seven widely used datasets from MoleculeNet [[38](#bib.bib38)], including
    four graph classification datasets BACE, BBBP, Clintox, HIV, and three regression
    datasets ESOL, Freesolv, and Lipophilicity. In this paper, we use ROCAUC for classification
    and RMSE for regression as evaluation metrics. The detailed description and statistics
    of the datasets can be found in Appendix [A.1](#A1.SS1 "A.1 Dataset Details ‣
    Appendix A Experimental Details ‣ LLM and GNN are Complementary: Distilling LLM
    for Multimodal Graph Learning") and Table [5](#A1.T5 "Table 5 ‣ A.1 Dataset Details
    ‣ Appendix A Experimental Details ‣ LLM and GNN are Complementary: Distilling
    LLM for Multimodal Graph Learning").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集。为了评估GALLON在分子属性预测中的表现，我们采用了来自MoleculeNet[[38](#bib.bib38)]的七个广泛使用的数据集，包括四个图分类数据集BACE、BBBP、Clintox、HIV，以及三个回归数据集ESOL、Freesolv和Lipophilicity。在本文中，我们使用ROCAUC进行分类评估，使用RMSE进行回归评估。数据集的详细描述和统计信息见附录[A.1](#A1.SS1
    "A.1 Dataset Details ‣ Appendix A Experimental Details ‣ LLM and GNN are Complementary:
    Distilling LLM for Multimodal Graph Learning")和表[5](#A1.T5 "Table 5 ‣ A.1 Dataset
    Details ‣ Appendix A Experimental Details ‣ LLM and GNN are Complementary: Distilling
    LLM for Multimodal Graph Learning")。'
- en: 'Baselines. We compare our model with (i) GNN models, including GCN [[20](#bib.bib20)],
    ChebNet [[8](#bib.bib8)], GraphSAGE [[14](#bib.bib14)], GIN [[41](#bib.bib41)],
    MoleBERT [[39](#bib.bib39)], and (ii) NLP models, including ECFP4-MLP [[28](#bib.bib28)],
    ChemBERTa [[6](#bib.bib6)], SMILES-Transformer [[16](#bib.bib16)]. The descriptions
    of the baselines are in Appendix [A.2](#A1.SS2 "A.2 Baselines ‣ Appendix A Experimental
    Details ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning").'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '基线。我们将我们的模型与（i）GNN模型进行比较，包括GCN[[20](#bib.bib20)]、ChebNet[[8](#bib.bib8)]、GraphSAGE[[14](#bib.bib14)]、GIN[[41](#bib.bib41)]、MoleBERT[[39](#bib.bib39)]，以及（ii）NLP模型，包括ECFP4-MLP[[28](#bib.bib28)]、ChemBERTa[[6](#bib.bib6)]、SMILES-Transformer[[16](#bib.bib16)]。基线模型的描述见附录[A.2](#A1.SS2
    "A.2 Baselines ‣ Appendix A Experimental Details ‣ LLM and GNN are Complementary:
    Distilling LLM for Multimodal Graph Learning")。'
- en: 'Settings. As suggested by Wu et al. [[38](#bib.bib38)], we adopt scaffold splitting
    for molecule datasets, which is widely adopted in the molecule domain. There are
    two kinds of scaffold splitting: scaffold splitting and random scaffold splitting.
    The difference is introduced in [A.3](#A1.SS3 "A.3 Settings ‣ Appendix A Experimental
    Details ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning").
    We adopt the ratio of 80%/10%/10% for train/validation/test sets. We use GCN as
    the GNN backbone and Roberta [[22](#bib.bib22)] as the smaller Language Model
    (LM) throughout all experiments. For the HIV and Lipophilicity datasets, we query
    Claude3-Haiku, and for other datasets, we use GPT-4V. We search hyperparameters
    $\alpha$ based on the performance on a validation set to achieve optimal results.
    For a fair comparison, we configured all backbone GCNs and distilled MLPs with
    3 layers and a hidden dimension of 32\. We conduct the experiments on 10 seeds
    and report the average performance with standard deviation.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '设置。正如吴等人所建议的[[38](#bib.bib38)]，我们采用分子数据集的骨架拆分，这在分子领域广泛采用。骨架拆分有两种类型：骨架拆分和随机骨架拆分。具体区别见[A.3](#A1.SS3
    "A.3 Settings ‣ Appendix A Experimental Details ‣ LLM and GNN are Complementary:
    Distilling LLM for Multimodal Graph Learning")。我们采用80%/10%/10%的比例划分训练集/验证集/测试集。我们在所有实验中使用GCN作为GNN骨干网，并使用Roberta[[22](#bib.bib22)]作为较小的语言模型（LM）。对于HIV和Lipophilicity数据集，我们查询Claude3-Haiku，对于其他数据集，我们使用GPT-4V。我们基于验证集上的性能搜索超参数$\alpha$以达到最佳结果。为了公平比较，我们将所有骨干GCN和蒸馏MLP配置为3层，隐藏维度为32。我们在10个种子上进行实验，并报告平均性能及标准差。'
- en: 4.2 Distillation Results
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 蒸馏结果
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4.2 Distillation Results ‣ 4 Experiments ‣ LLM
    and GNN are Complementary: Distilling LLM for Multimodal Graph Learning") presents
    the results of graph classification and regression tasks using scaffold splitting,
    compared with other GNN and NLP baselines. The boldface indicates the best results,
    while underlining denotes the second-best. For the classification datasets (BACE,
    BBBP, Clintox, and HIV), the ROCAUC score (%) is shown; for the regression datasets
    (ESOL, FreeSolv, and Lipo), the RMSE is shown. Our proposed GALLON framework achieves
    state-of-the-art performance on BACE, BBBP, Clintox, and FreeSolv, and performs
    comparably to the best baselines on HIV, ESOL, and Lipo with a simpler MLP structure.
    We attribute this to the distilled MLP leveraging the strengths of both MLP and
    GNN, utilizing their complementary aspects—specifically, the prior knowledge embedded
    in LLMs and the capability of learning from graph structures in GNNs. This synergistic
    combination enhances the overall performance.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [1](#S4.T1 "Table 1 ‣ 4.2 Distillation Results ‣ 4 Experiments ‣ LLM and
    GNN are Complementary: Distilling LLM for Multimodal Graph Learning") 展示了使用支架拆分的图分类和回归任务的结果，与其他
    GNN 和 NLP 基线进行比较。粗体表示最佳结果，带下划线表示第二最佳。对于分类数据集（BACE、BBBP、Clintox 和 HIV），显示 ROCAUC
    分数（%）；对于回归数据集（ESOL、FreeSolv 和 Lipo），显示 RMSE。我们提出的 GALLON 框架在 BACE、BBBP、Clintox
    和 FreeSolv 上实现了最先进的性能，并且在 HIV、ESOL 和 Lipo 上与最佳基线表现相当，且结构更简单。我们将这一成就归因于蒸馏的 MLP
    利用 MLP 和 GNN 的优点，充分发挥其互补性，特别是 LLM 中嵌入的先验知识和 GNN 中从图结构学习的能力。这种协同组合提升了整体性能。'
- en: 'Table 1: Prediction performance with scaffold splitting.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：使用支架拆分的预测性能。
- en: '|  | BACE↑ | BBBP↑ | Clintox↑ | HIV↑ | ESOL↓ | Freesolv↓ | Lipo↓ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | BACE↑ | BBBP↑ | Clintox↑ | HIV↑ | ESOL↓ | Freesolv↓ | Lipo↓ |'
- en: '| GCN | 73.47±3.78 | 63.99±1.70 | 69.22±2.42 | 70.19±2.17 | 1.29±0.03 | 3.33±0.40
    | 0.89±0.02 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 73.47±3.78 | 63.99±1.70 | 69.22±2.42 | 70.19±2.17 | 1.29±0.03 | 3.33±0.40
    | 0.89±0.02 |'
- en: '| ChebNet | 75.53±1.60 | 67.07±0.75 | 68.95±2.81 | 73.78±1.66 | 1.32±0.04 |
    3.27±0.17 | 1.04±0.05 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ChebNet | 75.53±1.60 | 67.07±0.75 | 68.95±2.81 | 73.78±1.66 | 1.32±0.04 |
    3.27±0.17 | 1.04±0.05 |'
- en: '| GraphSage | 72.00±6.05 | 66.56±2.93 | 84.01±3.56 | 74.81±0.99 | 1.32±0.09
    | 3.12±0.30 | 0.87±0.01 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| GraphSage | 72.00±6.05 | 66.56±2.93 | 84.01±3.56 | 74.81±0.99 | 1.32±0.09
    | 3.12±0.30 | 0.87±0.01 |'
- en: '| GIN | 70.34±2.57 | 62.53±3.27 | 73.40±3.73 | 72.54±3.32 | 1.26±0.08 | 3.71±0.79
    | \ul0.84±0.01 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| GIN | 70.34±2.57 | 62.53±3.27 | 73.40±3.73 | 72.54±3.32 | 1.26±0.08 | 3.71±0.79
    | \ul0.84±0.01 |'
- en: '| Mole-BERT | 71.46±4.74 | 67.52±1.17 | 66.46±7.26 | \ul75.42±1.13 | 1.39±0.07
    | 3.98±0.62 | 0.78±0.01 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Mole-BERT | 71.46±4.74 | 67.52±1.17 | 66.46±7.26 | \ul75.42±1.13 | 1.39±0.07
    | 3.98±0.62 | 0.78±0.01 |'
- en: '| ECFP4-MLP | \ul79.65±1.88 | 61.84±0.37 | 70.03±1.57 | 69.58±0.61 | 1.67±0.30
    | 4.07±1.21 | 0.83±0.02 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ECFP4-MLP | \ul79.65±1.88 | 61.84±0.37 | 70.03±1.57 | 69.58±0.61 | 1.67±0.30
    | 4.07±1.21 | 0.83±0.02 |'
- en: '| ChemBERTa | 73.70±2.39 | \ul70.44±1.20 | 97.41±1.64 | 76.60±1.14 | 1.77±0.04
    | 3.97±0.13 | 1.19±0.10 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ChemBERTa | 73.70±2.39 | \ul70.44±1.20 | 97.41±1.64 | 76.60±1.14 | 1.77±0.04
    | 3.97±0.13 | 1.19±0.10 |'
- en: '| SMILES-Tsfm | 74.94±1.22 | 68.21±0.53 | \ul98.43±0.46 | 72.00±0.80 | 1.06±0.08
    | \ul2.74±0.94 | 2.74±0.94 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| SMILES-Tsfm | 74.94±1.22 | 68.21±0.53 | \ul98.43±0.46 | 72.00±0.80 | 1.06±0.08
    | \ul2.74±0.94 | 2.74±0.94 |'
- en: '| GALLON | 80.42±1.06 | 72.38±0.63 | 99.15±0.57 | 75.39±0.40 | \ul1.18±0.02
    | 2.03±0.09 | 0.90±0.01 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| GALLON | 80.42±1.06 | 72.38±0.63 | 99.15±0.57 | 75.39±0.40 | \ul1.18±0.02
    | 2.03±0.09 | 0.90±0.01 |'
- en: 4.3 Contributions of LLM and GNN
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 LLM 和 GNN 的贡献
- en: 'In this section, we aim to explore the influence of the teachers’ models. We
    conduct distillation using GNN or LLM solely as the teacher model, as well as
    using the combination of LLM+GNN as teacher models. The performance results are
    shown in Table [2](#S4.T2 "Table 2 ‣ 4.3 Contributions of LLM and GNN ‣ 4 Experiments
    ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning")
    and Table [3](#S4.T3 "Table 3 ‣ 4.3 Contributions of LLM and GNN ‣ 4 Experiments
    ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning")
    under different split settings. GNN$\rightarrow$MLP configuration achieves the
    best performance, indicating that the synergy between GNN and LLM contributes
    significantly to the final results.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们旨在探讨教师模型的影响。我们使用 GNN 或 LLM 作为唯一的教师模型进行蒸馏，也使用 LLM+GNN 的组合作为教师模型。性能结果如表
    [2](#S4.T2 "Table 2 ‣ 4.3 Contributions of LLM and GNN ‣ 4 Experiments ‣ LLM and
    GNN are Complementary: Distilling LLM for Multimodal Graph Learning") 和表 [3](#S4.T3
    "Table 3 ‣ 4.3 Contributions of LLM and GNN ‣ 4 Experiments ‣ LLM and GNN are
    Complementary: Distilling LLM for Multimodal Graph Learning") 在不同的拆分设置下所示。GNN$\rightarrow$MLP
    配置实现了最佳性能，表明 GNN 和 LLM 之间的协同作用对最终结果贡献显著。'
- en: 'Table 2: The comparison between different distillation settings with scaffold
    splitting.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同蒸馏设置的比较，使用支架拆分。
- en: '| Dataset | GNN | LLM | (GNN+LLM)$\rightarrow$MLP | MLP |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | GNN | LLM | (GNN+LLM)$\rightarrow$MLP | MLP |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| BACE | 73.47±3.78 | 73.57±2.67 | 80.42±1.06 | 77.93±1.38 | 78.50±2.00 | 73.11±2.55
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| BACE | 73.47±3.78 | 73.57±2.67 | 80.42±1.06 | 77.93±1.38 | 78.50±2.00 | 73.11±2.55
    |'
- en: '| BBBP | 63.99±1.70 | 71.73±1.93 | 72.38±0.63 | 67.24±0.36 | 66.92±0.61 | 60.21±0.64
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| BBBP | 63.99±1.70 | 71.73±1.93 | 72.38±0.63 | 67.24±0.36 | 66.92±0.61 | 60.21±0.64
    |'
- en: '| Clintox | 69.22±2.42 | 99.06±0.64 | 99.15±0.57 | 83.94±4.51 | 84.99±1.51
    | 70.89±4.93 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Clintox | 69.22±2.42 | 99.06±0.64 | 99.15±0.57 | 83.94±4.51 | 84.99±1.51
    | 70.89±4.93 |'
- en: '| HIV | 70.19±2.17 | 76.60±0.61 | 75.39±0.40 | 70.94±2.47 | 71.14±0.64 | 66.17±0.75
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| HIV | 70.19±2.17 | 76.60±0.61 | 75.39±0.40 | 70.94±2.47 | 71.14±0.64 | 66.17±0.75
    |'
- en: '| ESOL | 1.29±0.03 | 2.24±0.19 | 1.18±0.02 | 1.22±0.02 | 1.23±0.02 | 1.29±0.03
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ESOL | 1.29±0.03 | 2.24±0.19 | 1.18±0.02 | 1.22±0.02 | 1.23±0.02 | 1.29±0.03
    |'
- en: '| Freesolv | 3.33±0.40 | 4.20±0.09 | 2.03±0.09 | 2.15±0.12 | 2.16±0.05 | 2.59±0.06
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Freesolv | 3.33±0.40 | 4.20±0.09 | 2.03±0.09 | 2.15±0.12 | 2.16±0.05 | 2.59±0.06
    |'
- en: '| Lipo | 0.89±0.02 | 1.16±0.06 | 0.90±0.01 | 0.92±0.01 | 0.91±0.01 | 0.96±0.01
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Lipo | 0.89±0.02 | 1.16±0.06 | 0.90±0.01 | 0.92±0.01 | 0.91±0.01 | 0.96±0.01
    |'
- en: 'Table 3: The comparison between different distillation settings with random
    scaffold splitting.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 不同蒸馏设置的比较，随机支架分裂。'
- en: '| Dataset | GNN | LLM | (GNN+LLM)$\rightarrow$MLP | MLP |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | GNN | LLM | (GNN+LLM)$\rightarrow$MLP | MLP |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| BACE | 79.08±6.61 | 83.87±3.47 | 80.34±2.68 | 76.84±1.69 | 79.50±2.96 | 70.73±3.02
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| BACE | 79.08±6.61 | 83.87±3.47 | 80.34±2.68 | 76.84±1.69 | 79.50±2.96 | 70.73±3.02
    |'
- en: '| BBBP | 81.23±6.42 | 80.97±1.94 | 84.84±1.24 | 83.21±1.58 | 83.78±1.42 | 79.98±2.31
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| BBBP | 81.23±6.42 | 80.97±1.94 | 84.84±1.24 | 83.21±1.58 | 83.78±1.42 | 79.98±2.31
    |'
- en: '| Clintox | 81.61±6.04 | 72.08±11.55 | 93.71±4.35 | 90.98±4.98 | 91.20±6.88
    | 87.39±4.69 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Clintox | 81.61±6.04 | 72.08±11.55 | 93.71±4.35 | 90.98±4.98 | 91.20±6.88
    | 87.39±4.69 |'
- en: '| HIV | 72.33±4.73 | 74.80±4.04 | 74.00±2.26 | 72.63±3.60 | 72.69±3.71 | 67.66±3.35
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| HIV | 72.33±4.73 | 74.80±4.04 | 74.00±2.26 | 72.63±3.60 | 72.69±3.71 | 67.66±3.35
    |'
- en: '| ESOL | 1.28±0.11 | 1.81±0.24 | 1.21±0.09 | 1.28±0.06 | 1.24±0.09 | 1.51±0.13
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ESOL | 1.28±0.11 | 1.81±0.24 | 1.21±0.09 | 1.28±0.06 | 1.24±0.09 | 1.51±0.13
    |'
- en: '| Freesolv | 3.18±0.70 | 4.66±1.23 | 1.85±0.40 | 2.14±0.40 | 2.03±0.43 | 2.87±0.71
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Freesolv | 3.18±0.70 | 4.66±1.23 | 1.85±0.40 | 2.14±0.40 | 2.03±0.43 | 2.87±0.71
    |'
- en: '| Lipo | 0.89±0.06 | 1.18±0.10 | 0.89±0.05 | 0.90±0.05 | 0.90±0.05 | 0.95±0.05
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Lipo | 0.89±0.06 | 1.18±0.10 | 0.89±0.05 | 0.90±0.05 | 0.90±0.05 | 0.95±0.05
    |'
- en: 4.4 Efficiency Comparison
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 效率比较
- en: 'In this experiment, we evaluate the performance of various models on the BACE
    dataset by comparing their ROCAUC scores against the log of inference time and
    the log of the number of parameters. Each result is conducted on the whole dataset
    100 times to calculate the averaged inference time. Fig. [4](#S3.F4 "Figure 4
    ‣ 3.2 Pretraining and Finetuning ‣ 3 Methodology: GALLON ‣ LLM and GNN are Complementary:
    Distilling LLM for Multimodal Graph Learning") and Fig. [4](#S3.F4 "Figure 4 ‣
    3.2 Pretraining and Finetuning ‣ 3 Methodology: GALLON ‣ LLM and GNN are Complementary:
    Distilling LLM for Multimodal Graph Learning") illustrates the relationship between
    ROCAUC and the log of inference time and model size, showing that GALLON achieves
    the best performance with the fastest inference time and smallest model size.
    These results reveal that our approach effectively distills knowledge from GNN
    and LLM into MLP, leading to state-of-the-art performance in terms of both efficiency
    and accuracy.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本实验中，我们通过将各种模型在BACE数据集上的ROCAUC分数与推理时间的对数以及参数数量的对数进行比较，评估其性能。每个结果在整个数据集上进行100次，以计算平均推理时间。图 [4](#S3.F4
    "图 4 ‣ 3.2 预训练与微调 ‣ 3 方法论：GALLON ‣ LLM与GNN是互补的：将LLM蒸馏用于多模态图学习") 和图 [4](#S3.F4
    "图 4 ‣ 3.2 预训练与微调 ‣ 3 方法论：GALLON ‣ LLM与GNN是互补的：将LLM蒸馏用于多模态图学习") 说明了ROCAUC与推理时间及模型大小的对数之间的关系，显示出GALLON在最短推理时间和最小模型大小下实现了最佳性能。这些结果揭示了我们的方法有效地将GNN和LLM中的知识蒸馏到MLP中，从而在效率和准确性方面都达到了最先进的水平。
- en: 4.5 Influence of Multimodality
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 多模态的影响
- en: 'In the GALLON framework, we incorporate all of SMILES string, molecular diagram,
    and graph structure when querying the LLM. To explore the influence of each multimodality,
    we ablate the molecular diagram and graph structure respectively and plot the
    results of prediction of the finetuned LM in Fig. [6](#S4.F6 "Figure 6 ‣ 4.5 Influence
    of Multimodality ‣ 4 Experiments ‣ LLM and GNN are Complementary: Distilling LLM
    for Multimodal Graph Learning"). We conduct the experiments on BACE, BBBP, and
    Clintox, where all results are based on GPT4V and scaffold splitting. We find
    that the original GALLON with all the multimodality achieves the best performance,
    which means each modality contribute to extracting richer knowledge from LLM.
    To be more specific, removing molecular diagrams results in significant performance
    drops, due to the lack of overview of the molecule, such as some functional groups.
    Similarly, excluding graph structures leads to declines in BACE and BBBP, as the
    graph structure is easier for LLM to understand atomic connectivity insights.
    These findings underscore the importance of leveraging multiple modalities to
    enhance molecular representation and prediction accuracy.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GALLON 框架中，我们在查询 LLM 时结合了 SMILES 字符串、分子图和图结构。为了探讨每种多模态的影响，我们分别去除分子图和图结构，并在图 [6](#S4.F6
    "图 6 ‣ 4.5 多模态的影响 ‣ 4 实验 ‣ LLM 和 GNN 是互补的：从 LLM 中提取多模态图学习") 中绘制了细化 LM 的预测结果。我们在
    BACE、BBBP 和 Clintox 上进行了实验，所有结果均基于 GPT4V 和 scaffold 分割。我们发现，原始 GALLON 包含所有多模态时表现最佳，这意味着每种模态都有助于从
    LLM 中提取更丰富的知识。更具体地说，去除分子图会导致性能显著下降，因为缺乏对分子的整体视图，如某些功能团。同样，排除图结构会导致 BACE 和 BBBP
    的性能下降，因为图结构更容易让 LLM 理解原子连通性的见解。这些发现强调了利用多种模态来增强分子表示和预测准确性的必要性。
- en: '![Refer to caption](img/da81b9401c8d4999f04d5aed90035c69.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/da81b9401c8d4999f04d5aed90035c69.png)'
- en: 'Figure 5: Ablation study of multimodalities.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：多模态的消融研究。
- en: '![Refer to caption](img/f3d199313929ab6491b972cacf5025e2.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f3d199313929ab6491b972cacf5025e2.png)'
- en: 'Figure 6: Performance of different LLMs.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：不同 LLM 的性能。
- en: 4.6 Influence of Large Language Model
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 大型语言模型的影响
- en: 'Fig. [6](#S4.F6 "Figure 6 ‣ 4.5 Influence of Multimodality ‣ 4 Experiments
    ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning")
    illustrates the performance of different LLMs (GPT4V, Claude3-haiku, and Claude3-sonnet)
    on BACE and BBBP datasets with scaffold splitting. The results show that each
    dataset has its own best-performing LLM, although the differences are not significant.
    (1) On BACE, Claude3-sonnet slightly outperforms GPT4V and Claude3-haiku, suggesting
    it may capture the chemical properties relevant to this dataset more effectively.
    By contrast, on BBBP dataset, GPT4V achieves the highest performance. (2) Claude3-sonnet
    consistently outperforms Claude3-haiku, due to its larger model size and capacity.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [6](#S4.F6 "图 6 ‣ 4.5 多模态的影响 ‣ 4 实验 ‣ LLM 和 GNN 是互补的：从 LLM 中提取多模态图学习") 说明了不同
    LLM（GPT4V、Claude3-haiku 和 Claude3-sonnet）在 BACE 和 BBBP 数据集上的性能表现，采用了 scaffold
    分割。结果显示，每个数据集都有其最佳表现的 LLM，尽管差异并不显著。 (1) 在 BACE 上，Claude3-sonnet 略微优于 GPT4V 和 Claude3-haiku，表明它可能更有效地捕捉了与该数据集相关的化学属性。相比之下，在
    BBBP 数据集上，GPT4V 达到了最高性能。 (2) Claude3-sonnet 始终优于 Claude3-haiku，这归因于其更大的模型规模和能力。
- en: 5 Case Study
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 案例研究
- en: In this section, we explore how multimodality aids in extracting prior knowledge
    from LLMs. We use a molecule from BACE dataset as an example. Its SMILES string
    is
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了多模态如何帮助从 LLM 中提取先前知识。我们以 BACE 数据集中的一个分子为例。它的 SMILES 字符串是
- en: '|  |  S1(=O)C[C@@H](Cc2cc(OC(C(F)(F)F)C(F)(F)F)c(N)c(F)c2)[C@H](O)[C@@H]([NH2+]Cc2cc(ccc2)C(C)(C)C)C1  |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  |  S1(=O)C[C@@H](Cc2cc(OC(C(F)(F)F)C(F)(F)F)c(N)c(F)c2)[C@H](O)[C@@H]([NH2+]Cc2cc(ccc2)C(C)(C)C)C1  |  |'
- en: 'and its diagram is shown in Fig. [7](#A3.F7 "Figure 7 ‣ Appendix C More Details
    of Case Study ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph
    Learning"). We query GPT-4V using three different multimodal combinations: (1)
    SMILES string, diagram, and graph structure, (2) SMILES string and diagram, (3)
    SMILES string and graph structure, and (4) SMILES string only. We present each
    response in Appendix [C](#A3 "Appendix C More Details of Case Study ‣ LLM and
    GNN are Complementary: Distilling LLM for Multimodal Graph Learning").'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其图示如图 [7](#A3.F7 "图7 ‣ 附录C 案例研究的更多细节 ‣ LLM与GNN是互补的：从LLM中提炼出多模态图学习") 所示。我们使用三种不同的多模态组合查询GPT-4V：（1）SMILES字符串、图示和图结构，（2）SMILES字符串和图示，（3）SMILES字符串和图结构，以及（4）仅SMILES字符串。我们将在附录
    [C](#A3 "附录C 案例研究的更多细节 ‣ LLM与GNN是互补的：从LLM中提炼出多模态图学习") 中展示每个响应。
- en: 'Through the responses, we summarize the knowledge that LLMs provide with various
    multimodal combinations in Table 6\. The table shows how many functional groups
    and structural characteristics the LLM can identify from the input modality and
    whether it can facilitate interactions with BACE-1 and predict the binding result.
    Based on the table, we make the following observations: (1) Inputs of all SMILES
    strings, diagram, and graph structure can extract the most comprehensive information
    from the LLM. This indicates that both the diagram and graph structure contribute
    significantly to the overall analysis. (2) SMILES + Diagram can identify more
    functional groups than SMILES + Graph Structure. This suggests that the molecular
    diagram provides a better way to identify certain functional groups due to its
    intuitive and comprehensive overview of the molecule. (3) Although SMILES + Graph
    Structure has a similar number of identified items as SMILES only, the responses
    show that SMILES only lacks detailed explanations for each functional group. This
    undermines the richness of the extracted knowledge. (4) Only the combination of
    SMILES, diagram, and graph structure provide the most abundant structural characteristics.
    This emphasizes the importance of multimodal inputs for a complete structural
    analysis. (5) All combinations can identify interactions relevant to binding with
    BACE-1\. However, predictions about the molecule’s activity are more reliably
    made with the combination of SMILES, diagram, and graph structure, as well as
    SMILES and diagram, compared to other combinations.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些响应，我们在表6中总结了LLMs提供的各种多模态组合的知识。表格展示了LLM可以从输入模态中识别多少功能团和结构特征，以及它是否能够促进与BACE-1的互动并预测结合结果。基于表格，我们得出以下观察结论：（1）所有SMILES字符串、图示和图结构的输入可以从LLM中提取最全面的信息。这表明图示和图结构对整体分析贡献显著。（2）SMILES
    + 图示能够识别更多功能团，而不是SMILES + 图结构。这表明分子图示由于其对分子的直观和全面概述，提供了识别某些功能团的更好方式。（3）尽管SMILES
    + 图结构识别的项目数量与仅使用SMILES相似，但响应显示仅使用SMILES缺乏对每个功能团的详细解释。这削弱了提取知识的丰富性。（4）只有SMILES、图示和图结构的组合提供了最丰富的结构特征。这强调了多模态输入在完整结构分析中的重要性。（5）所有组合都能识别与BACE-1结合相关的互动。然而，与其他组合相比，关于分子活性的预测更可靠的是SMILES、图示和图结构的组合，以及SMILES和图示的组合。
- en: 'Table 4: The analysis of information provided by various multimodal combinations
    on an example molecule of BACE dataset.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：关于BACE数据集中示例分子的各种多模态组合提供的信息分析。
- en: '| Multimodalities |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 多模态 |'
- en: '&#124; SMILES &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SMILES &#124;'
- en: '&#124; Diagram &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图示 &#124;'
- en: '&#124; Graph Structure &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图结构 &#124;'
- en: '|'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; SMILES &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SMILES &#124;'
- en: '&#124; Diagram &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图示 &#124;'
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; SMILES &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SMILES &#124;'
- en: '&#124; Graph Structure &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图结构 &#124;'
- en: '| SMILES |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| SMILES |'
- en: '| Functional Groups | Sulfonamide Group | $\checkmark$ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 功能团 | 磺胺基团 | $\checkmark$ |'
- en: '| Aromatic Ring | $\checkmark$ |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 芳香环 | $\checkmark$ |'
- en: '| Hydroxyl Group | $\checkmark$ |  |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 羟基 | $\checkmark$ |  |  |'
- en: '| Amine Group | $\checkmark$ |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 胺基团 | $\checkmark$ |'
- en: '| Ether Group | $\checkmark$ |  |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 醚基团 | $\checkmark$ |  |  |'
- en: '| Fluorine Atom | $\checkmark$ |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 氟原子 | $\checkmark$ |'
- en: '| Amide Bond | $\checkmark$ |  |  |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 酰胺键 | $\checkmark$ |  |  |  |'
- en: '| Thioether Group |  | $\checkmark$ |  |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 硫醚基团 |  | $\checkmark$ |  |  |'
- en: '| Tertiary Butyl Group |  |  | $\checkmark$ |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 三级丁基团 |  |  | $\checkmark$ |'
- en: '| Structural Characteristics | Rigid & Flexible | $\checkmark$ |  |  |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 结构特征 | 刚性 & 灵活性 | $\checkmark$ |  |  |  |'
- en: '| Chiral Centers | $\checkmark$ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 手性中心 | $\checkmark$ |'
- en: '| Molecular Bulk | $\checkmark$ |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 分子体积 | $\checkmark$ |  |'
- en: '| Interactions | $\checkmark$ |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 交互 | $\checkmark$ |'
- en: '| Prediction | $\checkmark$ |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 预测 | $\checkmark$ |'
- en: 6 Conclusion and Future Work
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: In this paper, we introduced the GALLON framework, a novel approach for molecular
    property prediction that leverages the complementary strengths of Graph Neural
    Networks and Large Language Models. By distilling knowledge from both GNNs and
    LLMs into a more efficient Multilayer Perceptron model, we addressed the limitations
    of each individual modality and enhanced the overall predictive performance. One
    of the limitations of the paper is the distillation method. Future research can
    build upon this work in several directions. First, investigating advanced distillation
    techniques and more sophisticated mapping functions may improve the efficiency
    and effectiveness of the distillation process. Second, exploring other multimodal
    data representations and their integration into the GALLON framework could further
    enhance prediction accuracy and generalizability. Lastly, extending the framework
    to other domains beyond molecules, such as materials science and biology, could
    validate its broader applicability and utility.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们介绍了 GALLON 框架，这是一种利用图神经网络和大型语言模型互补优势的分子属性预测新方法。通过将 GNNs 和 LLMs 的知识提炼成更高效的多层感知器模型，我们解决了各自模态的局限性，并提升了整体预测性能。论文的一个局限性是提炼方法。未来的研究可以在几个方向上拓展这项工作。首先，研究先进的提炼技术和更复杂的映射函数可能会提高提炼过程的效率和有效性。其次，探索其他多模态数据表示及其在
    GALLON 框架中的集成可能进一步提高预测准确性和泛化能力。最后，将框架扩展到分子以外的其他领域，如材料科学和生物学，可能验证其更广泛的适用性和实用性。
- en: References
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Gpt-4v(ision) system card. 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Gpt-4v(ision) 系统卡。2023。'
- en: '[2] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3
    Model Card, 2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] AI Anthropic。Claude 3 模型家族：Opus, sonnet, haiku。Claude-3 模型卡, 2024。'
- en: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    等。语言模型是少样本学习者。神经信息处理系统进展, 33:1877–1901, 2020。'
- en: '[4] Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang,
    and Yang Yang. Graphllm: Boosting graph reasoning ability of large language model.
    arXiv preprint arXiv:2310.05845, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang,
    和 Yang Yang。Graphllm: 提升大型语言模型的图推理能力。arXiv 预印本 arXiv:2310.05845, 2023。'
- en: '[5] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang
    Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. Exploring the potential of large language
    models (llms) in learning on graphs. ACM SIGKDD Explorations Newsletter, 25(2):42–61,
    2024.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang
    Wang, Dawei Yin, Wenqi Fan, Hui Liu, 等。探索大型语言模型（llms）在图学习中的潜力。ACM SIGKDD 探索通讯,
    25(2):42–61, 2024。'
- en: '[6] Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta: large-scale
    self-supervised pretraining for molecular property prediction. arXiv preprint
    arXiv:2010.09885, 2020.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Seyone Chithrananda, Gabriel Grand, 和 Bharath Ramsundar。Chemberta: 大规模自监督预训练用于分子属性预测。arXiv
    预印本 arXiv:2010.09885, 2020。'
- en: '[7] Debarati Das, Ishaan Gupta, Jaideep Srivastava, and Dongyeop Kang. Which
    modality should i use–text, motif, or image?: Understanding graphs with large
    language models. arXiv preprint arXiv:2311.09862, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Debarati Das, Ishaan Gupta, Jaideep Srivastava, 和 Dongyeop Kang。应该使用哪种模态–文本、图案还是图像？：使用大型语言模型理解图。arXiv
    预印本 arXiv:2311.09862, 2023。'
- en: '[8] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional
    neural networks on graphs with fast localized spectral filtering. NeurIPS, 29,
    2016.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Michaël Defferrard, Xavier Bresson, 和 Pierre Vandergheynst。带有快速局部谱滤波的图卷积神经网络。NeurIPS,
    29, 2016。'
- en: '[9] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng
    Ji. Translation between molecules and natural language. arXiv preprint arXiv:2204.11817,
    2022.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, 和 Heng
    Ji。分子与自然语言之间的翻译。arXiv 预印本 arXiv:2204.11817, 2022。'
- en: '[10] Chaoyou Fu, Renrui Zhang, Haojia Lin, Zihan Wang, Timin Gao, Yongdong
    Luo, Yubo Huang, Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, et al. A challenger
    to gpt-4v? early explorations of gemini in visual expertise. arXiv preprint arXiv:2312.12436,
    2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Chaoyou Fu, Renrui Zhang, Haojia Lin, Zihan Wang, Timin Gao, Yongdong
    Luo, Yubo Huang, Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, 等。对 gpt-4v 的挑战者？Gemini
    在视觉专业领域的早期探索。arXiv 预印本 arXiv:2312.12436, 2023。'
- en: '[11] Victor Fung, Jiaxin Zhang, Eric Juarez, and Bobby G Sumpter. Benchmarking
    graph neural networks for materials chemistry. npj Computational Materials, 7(1):84,
    2021.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Victor Fung、Jiaxin Zhang、Eric Juarez 和 Bobby G Sumpter。材料化学中图神经网络的基准测试。npj
    计算材料，7(1):84，2021年。'
- en: '[12] Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman, Cristian Regep,
    Gertrude Liu, Jeremy BR Hayter, Richard Vickers, Charles Roberts, Jian Tang, et al.
    Utilizing graph machine learning within drug discovery and development. Briefings
    in bioinformatics, 22(6):bbab159, 2021.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Thomas Gaudelet、Ben Day、Arian R Jamasb、Jyothish Soman、Cristian Regep、Gertrude
    Liu、Jeremy BR Hayter、Richard Vickers、Charles Roberts、Jian Tang 等。利用图机器学习进行药物发现与开发。生物信息学简报，22(6):bbab159，2021年。'
- en: '[13] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and
    George E Dahl. Neural message passing for quantum chemistry. In International
    conference on machine learning, pages 1263–1272\. PMLR, 2017.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Justin Gilmer、Samuel S Schoenholz、Patrick F Riley、Oriol Vinyals 和 George
    E Dahl。用于量子化学的神经消息传递。发表于国际机器学习会议，页码1263–1272。PMLR，2017年。'
- en: '[14] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation
    learning on large graphs. Advances in neural information processing systems, 30,
    2017.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Will Hamilton、Zhitao Ying 和 Jure Leskovec。在大规模图上进行归纳表示学习。神经信息处理系统进展，30，2017年。'
- en: '[15] Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, and
    Bryan Hooi. Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed
    graph representation learning. In The Twelfth International Conference on Learning
    Representations, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Xiaoxin He、Xavier Bresson、Thomas Laurent、Adam Perold、Yann LeCun 和 Bryan
    Hooi。利用解释：Llm-to-lm 解释器增强文本属性图表示学习。发表于第十二届国际学习表示会议，2023年。'
- en: '[16] Shion Honda, Shoi Shi, and Hiroki R Ueda. Smiles transformer: Pre-trained
    molecular fingerprint for low data drug discovery. arXiv preprint arXiv:1911.04738,
    2019.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Shion Honda、Shoi Shi 和 Hiroki R Ueda。Smiles transformer: 预训练的分子指纹用于数据稀缺的药物发现。arXiv
    预印本 arXiv:1911.04738，2019年。'
- en: '[17] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay
    Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. arXiv
    preprint arXiv:1905.12265, 2019.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Weihua Hu、Bowen Liu、Joseph Gomes、Marinka Zitnik、Percy Liang、Vijay Pande
    和 Jure Leskovec。图神经网络预训练的策略。arXiv 预印本 arXiv:1905.12265，2019年。'
- en: '[18] Cuiying Huo, Di Jin, Yawen Li, Dongxiao He, Yu-Bin Yang, and Lingfei Wu.
    T2-gnn: Graph neural networks for graphs with incomplete features and structure
    via teacher-student distillation. In Proceedings of the AAAI Conference on Artificial
    Intelligence, volume 37, pages 4339–4346, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Cuiying Huo、Di Jin、Yawen Li、Dongxiao He、Yu-Bin Yang 和 Lingfei Wu。T2-gnn:
    通过教师-学生蒸馏的图神经网络，针对具有不完整特征和结构的图。发表于 AAAI 人工智能会议论文集，第37卷，第4339–4346页，2023年。'
- en: '[19] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He,
    Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2019
    update: improved access to chemical data. Nucleic acids research, pages D1102–D1109,
    2019.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Sunghwan Kim、Jie Chen、Tiejun Cheng、Asta Gindulyte、Jia He、Siqian He、Qingliang
    Li、Benjamin A Shoemaker、Paul A Thiessen、Bo Yu 等。Pubchem 2019 更新：改进的化学数据访问。核酸研究，页码
    D1102–D1109，2019年。'
- en: '[20] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph
    convolutional networks. In ICLR, 2017.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Thomas N. Kipf 和 Max Welling。使用图卷积网络的半监督分类。发表于 ICLR，2017年。'
- en: '[21] Xiao Li, Li Sun, Mengjie Ling, and Yan Peng. A survey of graph neural
    network based recommendation in social networks. Neurocomputing, 549:126441, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Xiao Li、Li Sun、Mengjie Ling 和 Yan Peng。基于图神经网络的社交网络推荐调查。神经计算，549:126441，2023年。'
- en: '[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
    Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly
    optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Yinhan Liu、Myle Ott、Naman Goyal、Jingfei Du、Mandar Joshi、Danqi Chen、Omer
    Levy、Mike Lewis、Luke Zettlemoyer 和 Veselin Stoyanov。Roberta: 一种鲁棒优化的 BERT 预训练方法。arXiv
    预印本 arXiv:1907.11692，2019年。'
- en: '[23] Yizhen Luo, Kai Yang, Massimo Hong, Xingyi Liu, and Zaiqing Nie. Molfm:
    A multimodal molecular foundation model. arXiv preprint arXiv:2307.09484, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Yizhen Luo、Kai Yang、Massimo Hong、Xingyi Liu 和 Zaiqing Nie。Molfm: 一种多模态分子基础模型。arXiv
    预印本 arXiv:2307.09484，2023年。'
- en: '[24] Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, and Yong Liu. Can large
    language models empower molecular property prediction? arXiv preprint arXiv:2307.07443,
    2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Chen Qian、Huayi Tang、Zhirui Yang、Hong Liang 和 Yong Liu。大型语言模型能否增强分子属性预测？arXiv
    预印本 arXiv:2307.07443，2023年。'
- en: '[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
    Sutskever, et al. Language models are unsupervised multitask learners. OpenAI
    blog, 1(8):9, 2019.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 亚历克·拉德福德、杰弗里·吴、雷温·奇尔德、大卫·卢安、达里奥·阿莫代伊、伊利亚·苏茨克弗等。语言模型是无监督的多任务学习者。OpenAI博客，1(8):9，2019。'
- en: '[26] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy
    Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat,
    Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across
    millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 马切尔·里德、尼古拉·萨维诺夫、丹尼斯·特普利亚欣、德米特里·列皮欣、蒂莫西·利利克拉普、让-巴蒂斯特·阿拉亚克、拉杜·索里库特、安杰利基·拉扎里杜、奥尔汗·费拉特、朱利安·施里特维瑟等。Gemini
    1.5：解锁跨越数百万标记的多模态理解。arXiv预印本arXiv:2403.05530，2024。'
- en: '[27] Patrick Reiser, Marlen Neubert, André Eberhard, Luca Torresi, Chen Zhou,
    Chen Shao, Houssam Metni, Clint van Hoesel, Henrik Schopmans, Timo Sommer, et al.
    Graph neural networks for materials science and chemistry. Communications Materials,
    3(1):93, 2022.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 帕特里克·赖瑟、玛伦·诺伊贝特、安德烈·艾伯哈德、卢卡·托雷西、陈周、陈绍、侯萨姆·梅特尼、克林特·范霍塞尔、亨里克·肖普曼斯、蒂莫·索默等。材料科学和化学中的图神经网络。材料通讯，3(1):93，2022。'
- en: '[28] David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal
    of chemical information and modeling, pages 742–754, 2010.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 大卫·罗杰斯和马修·哈恩。扩展连接指纹。化学信息与建模期刊，742–754页，2010。'
- en: '[29] Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao
    Sun, Zhiwu Lu, and Ji-Rong Wen. A molecular multimodal foundation model associating
    molecule graphs with natural language. arXiv preprint arXiv:2209.05481, 2022.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 宋冰、杜大召、杨赵、周钰杰、李江萌、饶安一、孙浩、卢志武和文纪戎。将分子图与自然语言关联的分子多模态基础模型。arXiv预印本arXiv:2209.05481，2022。'
- en: '[30] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.
    Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805,
    2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Gemini团队、罗汉·安尼尔、塞巴斯蒂安·博尔戈德、吴永辉、让-巴蒂斯特·阿拉亚克、余佳辉、拉杜·索里库特、约翰·沙尔克维克、安德鲁·M·戴、安雅·豪斯等。Gemini：一系列高能力的多模态模型。arXiv预印本arXiv:2312.11805，2023。'
- en: '[31] Yijun Tian, Shichao Pei, Xiangliang Zhang, Chuxu Zhang, and Nitesh V Chawla.
    Knowledge distillation on graphs: A survey. arXiv preprint arXiv:2302.00219, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 田一军、裴世超、张向良、张楚旭和尼特什·V·查乌拉。图上的知识蒸馏：综述。arXiv预印本arXiv:2302.00219，2023。'
- en: '[32] Yijun Tian, Chuxu Zhang, Zhichun Guo, Xiangliang Zhang, and Nitesh Chawla.
    Learning mlps on graphs: A unified view of effectiveness, robustness, and efficiency.
    In The Eleventh International Conference on Learning Representations, 2022.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 田一军、张楚旭、郭志春、张向良和尼特什·查乌拉。在图上学习MLPs：效果、鲁棒性和效率的统一视角。第十一届国际学习表征会议，2022。'
- en: '[33] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation
    learning. Advances in neural information processing systems, 30, 2017.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] 亚伦·范登·奥德、奥里奥尔·维尼亚尔斯等。神经离散表示学习。神经信息处理系统进展，30，2017。'
- en: '[34] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
    Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903,
    2017.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] 佩塔尔·维利奇科维奇、吉列姆·库库鲁尔、阿兰莎·卡萨诺瓦、阿德里安娜·罗梅罗、皮特罗·利奥和约书亚·本吉奥。图注意力网络。arXiv预印本arXiv:1710.10903，2017。'
- en: '[35] Xiaofeng Wang, Zhen Li, Mingjian Jiang, Shuang Wang, Shugang Zhang, and
    Zhiqiang Wei. Molecule property prediction based on spatial graph embedding. Journal
    of chemical information and modeling, 59(9):3817–3828, 2019.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] 王晓峰、李振、姜明健、双爽、张书光和魏志强。基于空间图嵌入的分子属性预测。化学信息与建模期刊，59(9):3817–3828，2019。'
- en: '[36] Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular
    contrastive learning of representations via graph neural networks. Nature Machine
    Intelligence, 4(3):279–287, 2022.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 王玉阳、王建仁、曹中林和阿米尔·巴拉蒂·法里马尼。通过图神经网络进行分子对比学习。自然机器智能，4(3):279–287，2022。'
- en: '[37] David Weininger. Smiles, a chemical language and information system. 1\.
    introduction to methodology and encoding rules. Journal of chemical information
    and computer sciences, 28(1):31–36, 1988.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] 大卫·魏宁格。SMILES，一种化学语言和信息系统。1\. 方法论和编码规则介绍。化学信息与计算科学期刊，28(1):31–36，1988。'
- en: '[38] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse,
    Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular
    machine learning. Chemical science, 9(2):513–530, 2018.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] 吴振钦、巴拉斯·拉姆苏达尔、埃文·费因伯格、约瑟夫·戈梅斯、卡莱布·杰尼斯、阿尼什·S·帕普、卡尔·莱斯温和维贾伊·潘德。Moleculenet：分子机器学习基准。化学科学，9(2):513–530，2018。'
- en: '[39] Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu,
    Siyuan Li, and Stan Z. Li. Mole-BERT: Rethinking pre-training graph neural networks
    for molecules. In The Eleventh International Conference on Learning Representations,
    2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu,
    Siyuan Li, 和 Stan Z. Li. Mole-BERT：重新思考分子图神经网络的预训练。在第十一届国际学习表征会议，2023年。'
- en: '[40] Junjie Xu, Enyan Dai, Dongsheng Luo, Xiang Zhang, and Suhang Wang. Shape-aware
    graph spectral learning. arXiv preprint arXiv:2310.10064, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Junjie Xu, Enyan Dai, Dongsheng Luo, Xiang Zhang, 和 Suhang Wang. 形状感知图谱学习。arXiv预印本arXiv:2310.10064，2023年。'
- en: '[41] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful
    are graph neural networks? In International Conference on Learning Representations,
    2019.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Keyulu Xu, Weihua Hu, Jure Leskovec, 和 Stefanie Jegelka. 图神经网络的强大之处？在国际学习表征会议，2019年。'
- en: '[42] Bencheng Yan, Chaokun Wang, Gaoyang Guo, and Yunkai Lou. Tinygnn: Learning
    efficient graph neural networks. In Proceedings of the 26th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining, pages 1848–1856, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Bencheng Yan, Chaokun Wang, Gaoyang Guo, 和 Yunkai Lou. Tinygnn：学习高效的图神经网络。在第26届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集，页码1848–1856，2020年。'
- en: '[43] Cheng Yang, Jiawei Liu, and Chuan Shi. Extract the knowledge of graph
    neural networks and go beyond it: An effective knowledge distillation framework.
    In Proceedings of the web conference 2021, pages 1227–1237, 2021.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Cheng Yang, Jiawei Liu, 和 Chuan Shi. 提取图神经网络的知识并超越它：一个有效的知识蒸馏框架。在2021年Web会议论文集，页码1227–1237，2021年。'
- en: '[44] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng,
    Haoming Jiang, Shaochen Zhong, Bing Yin, and Xia Hu. Harnessing the power of llms
    in practice: A survey on chatgpt and beyond. ACM Transactions on Knowledge Discovery
    from Data, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng,
    Haoming Jiang, Shaochen Zhong, Bing Yin, 和 Xia Hu. 实践中利用LLMs的力量：关于ChatGPT及其他的调查。ACM数据知识发现事务，2023年。'
- en: '[45] Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xinchao Wang. Distilling
    knowledge from graph convolutional networks. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 7074–7083, 2020.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, 和 Xinchao Wang. 从图卷积网络中提炼知识。在IEEE/CVF计算机视觉与模式识别会议论文集，页码7074–7083，2020年。'
- en: '[46] Shichang Zhang, Yozen Liu, Yizhou Sun, and Neil Shah. Graph-less neural
    networks: Teaching old mlps new tricks via distillation. arXiv preprint arXiv:2110.08727,
    2021.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Shichang Zhang, Yozen Liu, Yizhou Sun, 和 Neil Shah. 无图神经网络：通过蒸馏教旧MLPs新技巧。arXiv预印本arXiv:2110.08727，2021年。'
- en: '[47] Wenqing Zheng, Edward W Huang, Nikhil Rao, Sumeet Katariya, Zhangyang
    Wang, and Karthik Subbian. Cold brew: Distilling graph node representations with
    incomplete or missing neighborhoods. arXiv preprint arXiv:2111.04840, 2021.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Wenqing Zheng, Edward W Huang, Nikhil Rao, Sumeet Katariya, Zhangyang
    Wang, 和 Karthik Subbian. 冷萃：用不完整或缺失邻域提炼图节点表示。arXiv预印本arXiv:2111.04840，2021年。'
- en: Appendix A Experimental Details
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 实验细节
- en: A.1 Dataset Details
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 数据集详情
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BACE: This dataset focuses on inhibitors of human beta-secretase 1\. It includes
    both quantitative (IC50 values) and qualitative (binary labels) binding results.
    It includes 1,513 compounds with their 2D structures and binary labels for the
    classification task.'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'BACE: 该数据集专注于人类β-分泌酶1的抑制剂。包括定量（IC50值）和定性（二元标签）结合结果。包含1,513种化合物及其2D结构和分类任务的二元标签。'
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BBBP: The Blood-brain barrier penetration (BBBP) dataset comes from a study
    focused on modeling and predicting the permeability of the blood-brain barrier.
    This contains 2,050 compounds with 2D structures and binary labels indicating
    whether a compound can penetrate the blood-brain barrier (BBB) or not.'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'BBBP: 血脑屏障穿透（BBBP）数据集来源于一个专注于建模和预测血脑屏障（BBB）通透性的研究。该数据集包含2,050种化合物及其2D结构和二元标签，指示化合物是否能穿透血脑屏障。'
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Clintox: This dataset compares drugs approved by the FDA and drugs that have
    failed clinical trials for toxicity reasons. It includes two classification tasks
    for 1,484 drug compounds with known chemical structures: (i) clinical trial toxicity
    (or absence of toxicity) and (ii) FDA approval status.'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Clintox: 该数据集比较了FDA批准的药物和因毒性原因未通过临床试验的药物。包括两个分类任务，针对1,484种已知化学结构的药物化合物：（i）临床试验毒性（或毒性缺失）和（ii）FDA批准状态。'
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'HIV: This dataset is introduced by the Drug Therapeutics Program (DTP) AIDS
    Antiviral Screen. It tests the ability to inhibit HIV replication for 41,127 compounds
    with binary labels indicating whether a compound is activate or not.'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'HIV: 该数据集由药物治疗计划（DTP）艾滋病抗病毒筛选引入。它测试41,127种化合物抑制HIV复制的能力，使用二元标签指示化合物是否活跃。'
- en: •
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ESOL: The ESOL (Estimated Solubility) dataset is a small-scale molecule dataset
    focuses on the water solubility of various compounds for a regression task. It
    includes quantitative solubility data for 1,128 compounds, with their 2D structures
    and measured solubility values in mols per liter.'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ESOL: ESOL（估算溶解度）数据集是一个小规模的分子数据集，专注于各种化合物的水溶性回归任务。它包括1,128种化合物的定量溶解度数据及其2D结构和以摩尔每升为单位的测量溶解度值。'
- en: •
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Freesolv: This provides data on the hydration-free energies of small molecules.
    It includes both experimental and calculated hydration-free energies for 642 compounds,
    along with their 2D structures.'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Freesolv: 该数据集提供了小分子的无水合自由能数据。它包括642种化合物的实验和计算的无水合自由能，以及它们的2D结构。'
- en: •
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Lipophilicity: This dataset contains data on the octanol/water partition coefficient
    (logP) values for various compounds. It includes 4,200 compounds with their 2D
    structures and experimental results of octanol/water distribution coefficient
    (logD at pH 7.4).'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Lipophilicity: 该数据集包含了各种化合物的辛醇/水分配系数（logP）值数据。它包括4,200种化合物的2D结构以及辛醇/水分配系数（pH
    7.4下的logD）的实验结果。'
- en: 'Table 5: The statistics and tasks of datasets.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '表5: 数据集的统计信息和任务。'
- en: '|  | #Graphs | Avg. #nodes | Avg. #edges | #Features | #Classes | Task | Metric
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | #图形 | 平均节点数 | 平均边数 | 特征数 | 类别数 | 任务 | 评价指标 |'
- en: '| BACE | 1,513 | 34.1 | 73.7 | 9 | 1 | Classification | ROCAUC |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| BACE | 1,513 | 34.1 | 73.7 | 9 | 1 | 分类 | ROCAUC |'
- en: '| BBBP | 2,050 | 23.9 | 51.6 | 9 | 1 | Classification | ROCAUC |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| BBBP | 2,050 | 23.9 | 51.6 | 9 | 1 | 分类 | ROCAUC |'
- en: '| Clintox | 1,484 | 26.1 | 55.5 | 9 | 2 | Classification | ROCAUC |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Clintox | 1,484 | 26.1 | 55.5 | 9 | 2 | 分类 | ROCAUC |'
- en: '| HIV | 41,127 | 25.5 | 54.9 | 9 | 1 | Classification | ROCAUC |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| HIV | 41,127 | 25.5 | 54.9 | 9 | 1 | 分类 | ROCAUC |'
- en: '| ESOL | 1,128 | 13.3 | 27.4 | 9 | - | Regression | RMSE |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| ESOL | 1,128 | 13.3 | 27.4 | 9 | - | 回归 | RMSE |'
- en: '| Freesolv | 642 | 8.7 | 16.8 | 9 | - | Regression | RMSE |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Freesolv | 642 | 8.7 | 16.8 | 9 | - | 回归 | RMSE |'
- en: '| Lipo | 4,200 | 27.0 | 59.0 | 9 | - | Regression | RMSE |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Lipo | 4,200 | 27.0 | 59.0 | 9 | - | 回归 | RMSE |'
- en: A.2 Baselines
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 基线
- en: •
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GCN [[20](#bib.bib20)]: Graph Convolutional Network (GCN) is one of the most
    popular MPNNs using 1-hop neighbors in each layer.'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GCN [[20](#bib.bib20)]: 图卷积网络（GCN）是最受欢迎的MPNNs之一，每层使用1跳邻居。'
- en: •
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ChebNet [[8](#bib.bib8)]: ChebNet uses Chebyshev polynomial to approximate
    the filter function. It is a more generalized form of GCN.'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ChebNet [[8](#bib.bib8)]: ChebNet使用切比雪夫多项式来近似滤波器函数。它是GCN的一种更通用形式。'
- en: •
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GraphSAGE [[14](#bib.bib14)]: Graph Sample and Aggregation (GraphSAGE) is a
    scalable and inductive framework that leverages node feature information by sampling
    and aggregating features from a fixed-size set of neighbors.'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GraphSAGE [[14](#bib.bib14)]: 图样本与聚合（GraphSAGE）是一个可扩展的归纳框架，通过从固定大小的邻居集合中采样和聚合特征来利用节点特征信息。'
- en: •
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GIN [[41](#bib.bib41)]: Graph Isomorphism Network (GIN) is a powerful variant
    of MPNNs designed to achieve maximum expressive power for graph representation
    learning, effectively distinguishing different graph structures through injective
    node aggregations.'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GIN [[41](#bib.bib41)]: 图同构网络（GIN）是MPNNs的一个强大变体，旨在实现最大表达能力，通过注入式节点聚合有效区分不同的图结构。'
- en: •
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MoleBERT [[39](#bib.bib39)]: MoleBERT is a self-supervised learning strategy
    for pretraining GNNs, specifically designed for molecules. It introduces two novel
    pretraining tasks: Masked Atoms Modeling (MAM) at the node level and triplet masked
    contrastive learning (TMCL) at the graph level. MAM involves randomly masking
    some discrete codes and then pretraining GNNs to predict them. TMCL models the
    heterogeneous semantic similarity between molecules for effective molecule retrieval.
    A variant of VQ-VAE[[33](#bib.bib33)] is proposed as a context-aware tokenizer
    to encode atom attributes into chemically meaningful discrete codes.'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'MoleBERT [[39](#bib.bib39)]: MoleBERT是一种自监督学习策略，用于预训练GNNs，专门设计用于分子。它引入了两个新颖的预训练任务：节点级的掩蔽原子建模（MAM）和图级的三元掩蔽对比学习（TMCL）。MAM涉及随机掩蔽一些离散代码，然后预训练GNNs以预测这些代码。TMCL对分子间的异质语义相似性建模，以有效检索分子。提出了一种VQ-VAE[[33](#bib.bib33)]的变体作为上下文感知的标记器，将原子属性编码为化学意义明确的离散代码。'
- en: •
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ECFP4-MLP [[28](#bib.bib28)]: Extended-connectivity fingerprint (ECFP) is a
    manually constructed fingerprinting method specifically developed to identify
    molecular characteristics. 4 means quantifying substructures with diameters of
    up to 4\. After getting fingerprinting from ECFP4, we train a multilayer perceptron.'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ECFP4-MLP [[28](#bib.bib28)]: 扩展连通性指纹（ECFP）是一种手动构建的指纹方法，专门用于识别分子特性。4表示量化直径达4的子结构。从ECFP4获得指纹后，我们训练一个多层感知器。'
- en: •
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ChemBERTa [[6](#bib.bib6)]: ChemBERTa is based on the Roberta [[22](#bib.bib22)].
    It consists of 12 attention heads and 6 layers, resulting in 72 distinct attention
    mechanisms. It is then pre-trained on a dataset of 77M unique SMILES from PubChem [[19](#bib.bib19)].'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ChemBERTa [[6](#bib.bib6)]: ChemBERTa基于Roberta [[22](#bib.bib22)]。它由12个注意力头和6层组成，产生72种不同的注意力机制。然后在来自PubChem
    [[19](#bib.bib19)]的77M个独特SMILES的数据集上进行预训练。'
- en: •
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SMILES-Transformer [[16](#bib.bib16)]: SMILES Transformer has 4 Transformer
    blocks. Each Transformer block has 4 attention heads with 256 embedding dimensions
    and includes two linear layers. It is pre-trained on a dataset consisting of SMILES
    randomly selected from ChEMBL24.'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SMILES-Transformer [[16](#bib.bib16)]: SMILES Transformer具有4个Transformer块。每个Transformer块有4个注意力头，256个嵌入维度，并包括两个线性层。它在一个由随机从ChEMBL24中选取的SMILES组成的数据集上进行预训练。'
- en: A.3 Settings
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 设置
- en: Models. In this paper, we employ GCN [[20](#bib.bib20)] as the GNN backbone.
    To ensure fair comparisons, all backbone GCNs and distilled MLPs are configured
    with 3 layers and a hidden dimension of 32\. Roberta [[22](#bib.bib22)] is used
    as the smaller Language Model (LM) for all experiments. For the HIV and Lipophilicity
    datasets, we use Claude3-Haiku, and for the other datasets, we utilize GPT-4V.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 模型。在本文中，我们使用GCN [[20](#bib.bib20)]作为GNN主干网。为了确保公平比较，所有主干GCN和蒸馏MLP都配置有3层和32的隐藏维度。Roberta
    [[22](#bib.bib22)] 被用作所有实验中的较小语言模型（LM）。对于HIV和脂溶性数据集，我们使用Claude3-Haiku，对于其他数据集，我们使用GPT-4V。
- en: Splitting. Scaffold splitting divides a dataset based on chemical scaffolds
    in a deterministic manner, ensuring structural diversity across train, validation,
    and test sets, thereby enhancing the model’s ability to generalize to new scaffolds.
    Random scaffold splitting also partitions the dataset based on scaffolds but introduces
    randomness in the assignment, achieving a balance between structural diversity
    and randomized subset distribution.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 划分。支架划分基于化学支架以确定性方式划分数据集，确保训练、验证和测试集之间的结构多样性，从而增强模型对新支架的泛化能力。随机支架划分也基于支架划分数据集，但引入了随机性，以在结构多样性和随机子集分布之间取得平衡。
- en: Training. We utilize an 80%/10%/10% split for the train, validation, and test
    sets. To ensure fair comparisons, the best hyperparameter configurations for each
    method are selected using the validation set, and we report the mean accuracy
    and variance across 10 different seeds on the test set. The hyperparameters $\alpha$
    are searched within the set {0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0,
    10.0}.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 训练。我们采用80%/10%/10%的划分比例用于训练、验证和测试集。为了确保公平比较，我们使用验证集选择每种方法的最佳超参数配置，并报告在测试集上10个不同种子的平均准确率和方差。超参数$\alpha$在集合{0,
    0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0}中进行搜索。
- en: Appendix B Prompts
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 提示
- en: Here we show the dataset description part of the prompt used for each dataset.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了用于每个数据集的提示中的数据集描述部分。
- en: 'Table 6: The description part of the prompts for each dataset.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：每个数据集的提示描述部分。
- en: '| Dataset | Description |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 描述 |'
- en: '| --- | --- |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| BACE |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| BACE |'
- en: '&#124; The BACE dataset provides and binary label binding results for a set
    of inhibitors &#124;'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BACE数据集提供了二元标签结合结果的一组抑制剂&#124;'
- en: '&#124; of human $\beta$-secretase 1 (BACE-1). Based on the following inputs,
    please analyze &#124;'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对人类$\beta$-分泌酶1（BACE-1）。根据以下输入，请分析&#124;'
- en: '&#124; the property of the molecule (e.g. Functional groups, Structural characteristics)
    and &#124;'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分子特性（如功能基团、结构特征）并&#124;'
- en: '&#124; analyze the binding results for a set of inhibitors of human beta-secretase
    (BACE-1)? &#124;'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分析一组人类β-分泌酶（BACE-1）抑制剂的结合结果？&#124;'
- en: '|'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| BBBP |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| BBBP |'
- en: '&#124; As a membrane separating circulating blood and brain extracellular fluid
    the &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 作为分隔循环血液和脑外液的膜，&#124;'
- en: '&#124; blood-brain barrier blocks most drugs, hormones, and neurotransmitters.
    Based on &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 血脑屏障阻挡大多数药物、激素和神经递质。根据&#124;'
- en: '&#124; these inputs, please analyze the property of the molecule (e.g. Functional
    groups, &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这些输入，请分析分子的属性（例如，功能基团，&#124;'
- en: '&#124; Structural characteristics) and analyze if the molecule is permeable
    to the &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结构特征），并分析该分子是否能穿透&#124;'
- en: '&#124; blood-brain barrier?" &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 血脑屏障？&#124;'
- en: '|'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Clintox |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Clintox |'
- en: '&#124; Could you analyze the given molecule based on the provided inputs and
    detail the &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 请根据提供的输入分析给定分子，并详细说明&#124;'
- en: '&#124; factors influencing its potential for clinical trial toxicity or non-toxicity?
    &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 影响其临床试验毒性或非毒性的潜在因素？&#124;'
- en: '&#124; Additionally, please assess factors that might impact its FDA approval
    status. &#124;'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 此外，请评估可能影响其 FDA 批准状态的因素。&#124;'
- en: '|'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| HIV |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| HIV |'
- en: '&#124; The HIV dataset tests the ability to inhibit HIV replication for over
    40,000 compounds. &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HIV 数据集测试了超过 40,000 种化合物对 HIV 复制的抑制能力。&#124;'
- en: '&#124; Based on the following inputs, please analyze the property of the molecule
    &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于以下输入，请分析分子的属性&#124;'
- en: '&#124; (e.g. Functional groups, Structural characteristics) with focusing on
    its ability to inhibit &#124;'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （例如，功能基团、结构特征）重点关注其抑制能力&#124;'
- en: '&#124; HIV replication. Then make your guess or prediction (active or inactive).
    &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HIV 复制。然后做出你的猜测或预测（活跃或不活跃）。&#124;'
- en: '|'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ESOL |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| ESOL |'
- en: '&#124; Based on these inputs, please analyze the property of the molecule &#124;'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于这些输入，请分析分子的属性&#124;'
- en: '&#124; (e.g. Functional groups, Structural characteristics), and which properties
    of the &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （例如，功能基团、结构特征），以及&#124;'
- en: '&#124; molecule can affect its water solubility? Also try to guess its solubility.
    &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分子如何影响其水溶性？还要尝试猜测其溶解性。&#124;'
- en: '|'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Freesolv |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| Freesolv |'
- en: '&#124; Free Solvation Database (FreeSolv) provides experimental and calculated
    hydration &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Free Solvation Database (FreeSolv) 提供了实验和计算的水合&#124;'
- en: '&#124; free energy of small molecules in water. Based on the following inputs,
    please analyze &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 小分子在水中的自由能。基于以下输入，请分析&#124;'
- en: '&#124; the property of the molecule (e.g. Functional groups, Structural characteristics)
    with &#124;'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分子的属性（例如，功能基团、结构特征），关注其&#124;'
- en: '&#124; focusing on its hydration free energy. Then make your guess or prediction
    about its &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关注其水合自由能。然后对其做出关于&#124;'
- en: '&#124; hydration free energy. &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 水合自由能。&#124;'
- en: '|'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Lipo |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| Lipo |'
- en: '&#124; Lipophilicity is an important feature of drug molecules that affects
    both membrane &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 亲脂性是影响药物分子的一个重要特征，它影响膜的&#124;'
- en: '&#124; permeability and solubility. Based on the following inputs, please analyze
    the molecule &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 渗透性和溶解性。基于以下输入，请分析该分子&#124;'
- en: '&#124; and give some details of factors that can affect octanol/water distribution
    coefficient &#124;'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并提供一些可能影响辛醇/水分配系数的因素&#124;'
- en: '&#124; (logD at pH 7.4). Then make your guess or prediction about its lipophilicity.
    &#124;'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （pH 7.4 时的 logD）。然后对其亲脂性做出猜测或预测。&#124;'
- en: '|'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Appendix C More Details of Case Study
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 案例研究更多细节
- en: 'This section presents the responses we obtained from the LLM for the example
    molecule in Case Study [5](#S5 "5 Case Study ‣ LLM and GNN are Complementary:
    Distilling LLM for Multimodal Graph Learning"). These texts are used to analyze
    the richness of information provided in Table [4](#S5.T4 "Table 4 ‣ 5 Case Study
    ‣ LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning").
    Fig. [7](#A3.F7 "Figure 7 ‣ Appendix C More Details of Case Study ‣ LLM and GNN
    are Complementary: Distilling LLM for Multimodal Graph Learning") displays the
    diagram of the example molecule.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了我们从 LLM 获得的对示例分子的响应，这些响应用于分析信息的丰富性，如表 [4](#S5.T4 "表 4 ‣ 5 案例研究 ‣ LLM 和
    GNN 是互补的：提炼 LLM 用于多模态图学习") 中所示。图 [7](#A3.F7 "图 7 ‣ 附录 C 案例研究更多细节 ‣ LLM 和 GNN 是互补的：提炼
    LLM 用于多模态图学习") 显示了示例分子的图示。
- en: '![Refer to caption](img/c93c785942bf386e107274e782b0744a.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c93c785942bf386e107274e782b0744a.png)'
- en: 'Figure 7: The diagram of the example molecule.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：示例分子的图示。
- en: 'Table 7: The response of LLM using SMILES string, diagram, and graph structure.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：使用 SMILES 字符串、图示和图结构的 LLM 响应。
- en: '| (1) SMILES string, diagram, graph structure |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| (1) SMILES 字符串、图示、图结构 |'
- en: '| --- |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| The provided SMILES string, molecule diagram, and graph structure describe
    a complex organic molecule with various functional groups and structural features
    that may be relevant for its activity as an inhibitor of $\beta$-secretase 1 (BACE-1).
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 提供的SMILES字符串、分子图和图形结构描述了一个复杂的有机分子，具有各种功能基团和结构特征，这些可能与其作为$\beta$-分泌酶1（BACE-1）抑制剂的活性相关。
    |'
- en: '| Analyzing the molecule based on the SMILES and the provided image: |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 根据SMILES和提供的图像分析分子： |'
- en: '| 1\. Functional Groups: |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 1\. 功能基团： |'
- en: '| - Sulfonamide Group (Sulfur connected to Oxygen by double bond and Nitrogen
    by single bond): This is often found in BACE-1 inhibitors as it can form hydrogen
    bonds with the enzyme. |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| - 磺胺基团（硫与氧通过双键连接，并与氮通过单键连接）：这通常在BACE-1抑制剂中发现，因为它可以与酶形成氢键。 |'
- en: '| - Aromatic Rings: There are multiple phenyl groups (benzene rings), which
    are important for $\pi$ stacking interactions with aromatic residues in proteins.
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| - 芳香环：有多个苯基（苯环），对与蛋白质中的芳香残基的$\pi$堆积相互作用非常重要。 |'
- en: '| - Hydroxyl Groups (-OH): These are capable of forming hydrogen bonds, which
    can be key for binding to the active site. |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| - 羟基（-OH）：这些能够形成氢键，这对于结合到活性位点可能是关键的。 |'
- en: '| - Amines (both primary with NH2+ and tertiary with the nitrogen having three
    carbon substituents): These can engage in hydrogen bonding and ionic interactions,
    especially the positively charged amine, which can interact with negatively charged
    residues in the active site. |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| - 胺类（包括具有NH2+的一级胺和具有三个碳取代基的三级胺）：这些可以参与氢键和离子相互作用，特别是带正电荷的胺，可以与活性位点中的负电荷残基相互作用。
    |'
- en: '| - Ether Groups (Oxygen connected to Carbon by single bonds): These can provide
    some flexibility and help position other functional groups within the binding
    pocket. |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| - 醚基（氧与碳通过单键连接）：这些可以提供一定的灵活性，并帮助将其他功能基团定位到结合口袋内。 |'
- en: '| - Fluorine Atoms: The presence of multiple fluorine atoms suggests the molecule
    is designed to increase its lipophilicity for membrane penetration or create specific
    interactions with the enzyme due to the high electronegativity of fluorine. |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| - 氟原子：多个氟原子的存在表明分子设计用于增加其脂溶性，以便穿透膜或由于氟的高电负性而与酶产生特定的相互作用。 |'
- en: '| - Amide Bonds: The carbon-nitrogen bonds in the context of carbonyl groups
    (C=O) indicate amide linkages, which can also participate in hydrogen bonding.
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| - 酰胺键：在羰基（C=O）的背景下，碳氮键表示酰胺连接，这些连接也可以参与氢键作用。 |'
- en: '| 2\. Structural Characteristics: |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 2\. 结构特征： |'
- en: '| - The presence of both rigid (aromatic rings) and flexible (alkyl chains)
    elements in the molecule can help it to adapt its conformation to fit the binding
    pocket of the enzyme. |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| - 分子中同时存在刚性（芳香环）和柔性（烷基链）元素，可以帮助其适应构象以适合酶的结合口袋。 |'
- en: '| - The molecule has chiral centers, which are indicated by the symbols "@@"
    in the SMILES string, affecting how the molecule may interact with the chiral
    environment of the enzyme’s active site. |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| - 该分子具有手性中心，由SMILES字符串中的“@@”符号表示，这会影响分子与酶活性位点的手性环境的相互作用。 |'
- en: '| - There is significant molecular bulk near the periphery of the molecule,
    due to the presence of tert-butyl groups and trifluoromethyl groups, which could
    be important for binding affinity and specificity. |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| - 分子外围有显著的分子体积，由于存在叔丁基和三氟甲基基团，这可能对结合亲和力和特异性很重要。 |'
- en: '| In terms of the BACE-1 inhibition, the functional groups such as the sulfonyl
    group, aromatic rings, and amine groups would be essential for forming interactions
    with the active site of the enzyme. The molecule is likely designed to fit within
    the binding pocket of BACE-1, forming hydrogen bonds, hydrophobic interactions,
    and possibly ionic interactions with key amino acid residues. |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 在BACE-1抑制方面，功能基团如磺酰基、芳香环和胺基对于与酶的活性位点形成相互作用至关重要。该分子可能被设计成适合BACE-1的结合口袋，形成氢键、疏水相互作用，并可能与关键氨基酸残基产生离子相互作用。
    |'
- en: '| The success of this compound as a BACE-1 inhibitor may be dependent on these
    interactions, as well as its overall shape and the ability to adopt a conformation
    that is complementary to the enzyme binding site. |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 该化合物作为BACE-1抑制剂的成功可能取决于这些相互作用，以及其整体形状和采用与酶结合位点互补的构象的能力。 |'
- en: '| To predict binding results or to further analyze the structural features
    and their potential interaction with BACE-1, computational techniques such as
    molecular docking or molecular dynamics could be applied if more details about
    the active site and its interactions with ligands are available. Additionally,
    experimental assays would be required to confirm the inhibitory activity of the
    molecule. |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 要预测结合结果或进一步分析结构特征及其与BACE-1的潜在相互作用，如果关于活性位点及其与配体的相互作用有更多细节，计算技术如分子对接或分子动力学可以被应用。此外，还需要实验测定以确认分子的抑制活性。
    |'
- en: 'Table 8: The response of LLM using SMILES string, and diagram.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：使用SMILES字符串和图表的LLM响应。 |
- en: '| (2) SMILES string and diagram |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| (2) SMILES字符串和图表 |'
- en: '| --- |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Based on the SMILES string provided and the diagram attached, we can infer
    several properties of the molecule and discuss its potential interactions as an
    inhibitor of human beta-secretase 1 (BACE-1), focusing on functional groups and
    structural characteristics relevant to its activity. |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 基于提供的SMILES字符串和附图，我们可以推断出分子的几个性质，并讨论其作为人类β-分泌酶1（BACE-1）抑制剂的潜在相互作用，重点关注与其活性相关的官能团和结构特征。
    |'
- en: '| First, examining the structural features from the SMILES string and the diagram:
    |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 首先，检查SMILES字符串和图表中的结构特征： |'
- en: '| 1\. Heterocyclic Components: There appear to be several heterocyclic components,
    one of which is likely to contribute to the molecule’s ability to interact with
    the active site of BACE-1. |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 1\. 杂环组分：似乎存在几个杂环组分，其中一个可能有助于分子与BACE-1的活性位点相互作用。 |'
- en: '| 2\. Fluorinated Substituents: The presence of multiple fluorine atoms could
    influence the binding affinity and specificity due to their electronegative nature,
    and they are often found in inhibitors to enhance metabolic stability and bioavailability.
    |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 2\. 氟化取代基：多个氟原子的存在可能会由于其电负性特征影响结合亲和力和特异性，它们通常出现在抑制剂中，以提高代谢稳定性和生物利用度。 |'
- en: '| 3\. Sulfonamide Group: The S(=O)(=O)N motif represents a sulfonamide group,
    which is a common feature in many drug molecules that can contribute to hydrogen
    bonding with the enzyme’s active site. |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 3\. 磺胺基团：S(=O)(=O)N基团表示一个磺胺基团，这是许多药物分子的常见特征，可以有助于与酶的活性位点形成氢键。 |'
- en: '| 4\. Chiral Centers: The molecule contains multiple chiral centers, as indicated
    by the "[C@@H]" and "[C@H]" annotations in the SMILES string. The configuration
    of these centers can significantly influence the binding affinity and selectivity
    of the molecule for BACE-1. |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 4\. 手性中心：分子含有多个手性中心，如SMILES字符串中的"[C@@H]"和"[C@H]"标注所示。这些中心的构型可以显著影响分子对BACE-1的结合亲和力和选择性。
    |'
- en: '| 5\. Amine Group: The molecule contains a positively charged amine group "[NH2+]",
    which suggests it may form salt bridges or hydrogen bonds with BACE-1. |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 5\. 胺基团：分子含有一个带正电荷的胺基团"[NH2+]"，这表明它可能与BACE-1形成盐桥或氢键。 |'
- en: '| 6\. Aromatic Rings: There are aromatic rings, which can facilitate $\pi-\pi$
    stacking interactions with aromatic residues in the active site of the enzyme.
    |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 6\. 芳香环：存在芳香环，这可以促进与酶活性位点芳香残基的$\pi-\pi$堆积相互作用。 |'
- en: '| 7\. Hydroxyl Group: There is a hydroxyl group, a polar functional group that
    can form hydrogen bonds with BACE-1. |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 7\. 羟基：分子中存在一个羟基，这是一个极性官能团，可以与BACE-1形成氢键。 |'
- en: '| 8\. Ethers and Thioethers: Both ether (C-O-C) and thioether (C-S-C) groups
    are present, which may contribute to the binding through interactions with nearby
    residues. |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 8\. 醚和硫醚：存在醚（C-O-C）和硫醚（C-S-C）基团，这可能通过与附近残基的相互作用有助于结合。 |'
- en: '| When assessing binding results for BACE-1 inhibitors, a few types of interactions
    can be analyzed: |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 在评估BACE-1抑制剂的结合结果时，可以分析几种类型的相互作用： |'
- en: '| - Hydrogen Bonds: Strong H-bond donors and acceptors in the molecule can
    form specific interactions with the enzyme’s active site, stabilizing the inhibitor-enzyme
    complex. |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| - 氢键：分子中强的氢键供体和受体可以与酶的活性位点形成特定的相互作用，稳定抑制剂-酶复合物。 |'
- en: '| - Hydrophobic Interactions: Non-polar regions of the inhibitor may engage
    in hydrophobic contacts with non-polar pockets of the active site, increasing
    affinity. |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| - 疏水相互作用：抑制剂的非极性区域可能与活性位点的非极性口袋进行疏水接触，从而增加亲和力。 |'
- en: '| - Electrostatic Interactions: Charged groups, like the amine, can form electrostatic
    interactions with oppositely charged residues of the enzyme or with metal ions
    that may be present in the active site. |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| - 静电相互作用：带电基团，如胺，可以与酶的带相反电荷的残基或活性位点中可能存在的金属离子形成静电相互作用。 |'
- en: '| - Conformational Fit: The overall shape of the molecule must fit well into
    the active site of BACE-1 to exhibit high binding affinity. |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| - 构象适配：分子的整体形状必须很好地适配到BACE-1的活性位点，以展现高结合亲和力。 |'
- en: '| Without experimental data, predicting the exact mode of inhibition is not
    feasible. However, the presence of these functional groups and structural features
    suggests that the molecule is designed to fit into the enzyme’s active site and
    have the necessary interactions for inhibition. |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 没有实验数据，无法预测确切的抑制机制。然而，这些功能基团和结构特征的存在表明，该分子设计为适合酶的活性位点，并具有所需的抑制相互作用。 |'
- en: '| In silico methods, such as molecular docking and dynamics simulations, coupled
    with in vitro assays, would be employed to predict the binding affinity and conformation
    of the inhibitor in the BACE-1 active site to more accurately assess its potential
    as a therapeutic molecule. |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 计算方法，如分子对接和动力学模拟，结合体外检测，将用于预测抑制剂在BACE-1活性位点的结合亲和力和构象，以更准确地评估其作为治疗分子的潜力。'
- en: 'Table 9: The response of LLM using SMILES string, and graph structure.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '| 表9：LLM使用SMILES字符串和图形结构的响应。 |'
- en: '| (3) SMILES string and graph structure |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| (3) SMILES字符串和图形结构 |'
- en: '| --- |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Based on the provided SMILES string and graph structure, the molecule has
    several noteworthy features: |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 根据提供的SMILES字符串和图形结构，该分子具有几个值得注意的特征： |'
- en: '| 1\. Sulfonamide group: The molecule contains a sulfonamide group (S(=O)(=O)N),
    which is known to be an essential pharmacophore in drug design and can form hydrogen
    bonds with the target protein, increasing the binding affinity. |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 1\. 磺胺基团：该分子包含一个磺胺基团（S(=O)(=O)N），该基团被认为是药物设计中的关键药效基团，可以与靶蛋白形成氢键，从而增加结合亲和力。
    |'
- en: '| 2\. Fluorinated aromatic ring: The molecule has a fluorinated phenyl ring,
    which can enhance the metabolic stability and lipophilicity of the compound, potentially
    improving its bioavailability and membrane permeability. |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 2\. 氟化芳香环：该分子具有一个氟化苯环，可以提高化合物的代谢稳定性和脂溶性，从而潜在地改善其生物利用度和膜渗透性。 |'
- en: '| 3\. Secondary amine: The presence of a secondary amine (NH2) might be involved
    in hydrogen bonding interactions with the enzyme’s active site, which can be a
    key factor for the inhibitory activity. |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 3\. 次级胺：次级胺（NH2）的存在可能参与与酶活性位点的氢键相互作用，这可能是抑制活性的关键因素。 |'
- en: '| 4\. Tertiary butyl group: The bulky tert-butyl group (C(C)(C)C) may contribute
    to the molecule’s steric hindrance, affecting its binding conformation and possibly
    improving selectivity for BACE-1 by fitting into specific hydrophobic pockets.
    |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 4\. 三丁基基团：庞大的三丁基基团（C(C)(C)C）可能会对分子的空间位阻产生影响，影响其结合构象，并可能通过适应特定的疏水性口袋来提高对BACE-1的选择性。
    |'
- en: '| 5\. Multiple aromatic rings: The presence of several interconnected aromatic
    rings provides a rigid framework that can favorably stack with the aromatic amino
    acid residues in the BACE-1 active site, enhancing the $\pi-\pi$ interactions.
    |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 5\. 多个芳香环：多个相互连接的芳香环提供了一个刚性的框架，可以与BACE-1活性位点中的芳香族氨基酸残基有利地堆叠，增强$\pi-\pi$相互作用。
    |'
- en: '| Upon analyzing the binding results for this inhibitor against human beta-secretase
    (BACE-1), the mentioned functional groups and structural characteristics suggest
    that the molecule may exhibit strong binding affinity and potency. The sulfonamide
    group can form key hydrogen bonds, the fluorinated ring can improve pharmacokinetic
    properties, and the bulky groups may enhance selectivity. However, the actual
    binding results would require empirical data obtained from biochemical assays,
    molecular docking studies, or computational simulations. |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 分析该抑制剂对人β-分泌酶（BACE-1）的结合结果后，提到的功能基团和结构特征表明该分子可能表现出强的结合亲和力和效力。磺胺基团可以形成关键的氢键，氟化环可以改善药代动力学特性，而庞大的基团可能增强选择性。然而，实际的结合结果需要通过生化检测、分子对接研究或计算模拟获得经验数据。
    |'
- en: '| Using AI-based predictive models, one could input the molecule’s structural
    representation, such as the SMILES string or molecular graph, and obtain predictions
    on its binding affinity, selectivity, and potential inhibitory activity against
    BACE-1\. Such models are trained on large datasets of known inhibitors and their
    corresponding biological activities, allowing for the rapid virtual screening
    of novel compounds. |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 使用基于AI的预测模型，可以输入分子的结构表示，如SMILES字符串或分子图，并获得其结合亲和力、选择性和对BACE-1的潜在抑制活性的预测。这些模型是基于大量已知抑制剂及其相应生物活性的数据集进行训练的，从而实现对新化合物的快速虚拟筛选。
    |'
- en: 'Table 10: The response of LLM using SMILES string only.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：仅使用SMILES字符串的LLM响应。
- en: '| (4) SMILES string only |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| (4) 仅SMILES字符串 |'
- en: '| --- |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Based on the SMILES string provided, the molecule appears to have a complex
    structure with several functional groups. The molecule contains a sulfone group
    (S(=O)), an amine group ([NH2+]), multiple fluorine atoms (F), and an aromatic
    ring (cc). Additionally, the molecule also has chiral centers, indicated by the
    "@" symbol, which can impact its binding affinity to the target enzyme. |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 根据提供的SMILES字符串，分子似乎具有复杂的结构，包含多个功能基团。该分子含有磺酮基（S(=O)）、氨基（[NH2+]）、多个氟原子（F）和芳香环（cc）。此外，分子还具有手性中心，由"@"符号指示，这可能会影响其对目标酶的结合亲和力。'
- en: '| The presence of fluorine atoms in the molecule suggests that it may have
    strong binding interactions with the BACE-1 enzyme, as fluorine is known to form
    strong hydrogen bonds with proteins. The aromatic ring and amine group may also
    contribute to the binding affinity through pi stacking interactions and hydrogen
    bonding, respectively. |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 分子中氟原子的存在表明它可能与BACE-1酶有强的结合相互作用，因为氟已知能与蛋白质形成强氢键。芳香环和氨基组也可能通过π堆积相互作用和氢键分别对结合亲和力做出贡献。
    |'
- en: '| The molecule also contains a methoxy group (OC) and a tert-butyl group (C(C)(C)C),
    which could impact its overall shape and steric hindrance, potentially affecting
    its binding to the BACE-1 enzyme. |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 该分子还含有甲氧基（OC）和叔丁基（C(C)(C)C），这可能会影响其整体形状和立体阻碍，从而潜在地影响其与BACE-1酶的结合。 |'
- en: '| To analyze the binding results for this molecule as an inhibitor of BACE-1,
    we would need to take into account the specific interactions between the molecule
    and the enzyme’s active site. This could be done using molecular docking simulations
    or other computational chemistry techniques, which would allow us to predict the
    binding affinity and identify key interactions contributing to the inhibition
    of the enzyme. |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 要分析该分子作为BACE-1抑制剂的结合结果，我们需要考虑分子与酶活性位点之间的具体相互作用。这可以通过分子对接模拟或其他计算化学技术来完成，这将使我们能够预测结合亲和力并识别有助于酶抑制的关键相互作用。
    |'
- en: '| Overall, based on the structural characteristics of the molecule, it appears
    to have several features that could contribute to strong binding interactions
    with BACE-1, making it a potential inhibitor of the enzyme. However, further analysis
    using computational tools or experimental techniques would be needed to confirm
    its efficacy as a BACE-1 inhibitor. |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 总体而言，基于分子的结构特征，它似乎具有几个可能有助于与BACE-1强结合相互作用的特征，使其成为该酶的潜在抑制剂。然而，需要进一步使用计算工具或实验技术进行分析，以确认其作为BACE-1抑制剂的效果。
    |'
