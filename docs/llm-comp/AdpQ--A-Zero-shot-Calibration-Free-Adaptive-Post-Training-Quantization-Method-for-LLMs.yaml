- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:49:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'AdpQ: 一种零样本无校准自适应后训练量化方法'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13358](https://ar5iv.labs.arxiv.org/html/2405.13358)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13358](https://ar5iv.labs.arxiv.org/html/2405.13358)
- en: Alireza Ghaffari^(1,2)  Sharareh Younesian¹  Vahid Partovi Nia¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Alireza Ghaffari^(1,2)  Sharareh Younesian¹  Vahid Partovi Nia¹
- en: Boxing Chen¹  Masoud Asgharian²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Boxing Chen¹  Masoud Asgharian²
- en: ¹ Huawei Noah’s Ark Lab, Montreal Research Center
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 华为诺亚方舟实验室，蒙特利尔研究中心
- en: ² Department of Mathematics and Statistics, McGill University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ² 数学与统计学系，麦吉尔大学
- en: alireza.ghaffari@mcgill.ca
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: alireza.ghaffari@mcgill.ca
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The ever-growing computational complexity of Large Language Models (LLMs) necessitates
    efficient deployment strategies. The current state-of-the-art approaches for Post-training
    Quantization (PTQ) often require calibration to achieve the desired accuracy.
    This paper presents AdpQ, a novel zero-shot adaptive PTQ method for LLMs that
    achieves the state-of-the-art performance in low-precision quantization (e.g.
    3-bit) without requiring any calibration data. Inspired by Adaptive LASSO regression
    model, our proposed approach tackles the challenge of outlier activations by separating
    salient weights using an adaptive soft-thresholding method. Guided by Adaptive
    LASSO, this method ensures that the quantized weights distribution closely follows
    the originally trained weights and eliminates the need for calibration data entirely,
    setting our method apart from popular approaches such as SpQR and AWQ. Furthermore,
    our method offers an additional benefit in terms of privacy preservation by eliminating
    any calibration or training data. We also delve deeper into the information-theoretic
    underpinnings of the proposed method. We demonstrate that it leverages the Adaptive
    LASSO to minimize the Kullback-Leibler divergence between the quantized weights
    and the originally trained weights. This minimization ensures the quantized model
    retains the Shannon information content of the original model to a great extent,
    guaranteeing efficient deployment without sacrificing accuracy or information.
    Our results achieve the same accuracy as the existing methods on various LLM benchmarks
    while the quantization time is reduced by at least 10x, solidifying our contribution
    to efficient and privacy-preserving LLM deployment.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）日益增长的计算复杂性需要高效的部署策略。当前最先进的后训练量化（PTQ）方法通常需要校准以实现所需的准确性。本文提出了AdpQ，这是一种新颖的零样本自适应PTQ方法，能够在低精度量化（例如3位）中实现最先进的性能，而无需任何校准数据。受自适应LASSO回归模型的启发，我们提出的方法通过使用自适应软阈值方法分离显著权重来应对异常激活的挑战。该方法在自适应LASSO的指导下，确保量化权重分布紧密跟随原训练权重，并完全消除校准数据的需求，使我们的方法区别于诸如SpQR和AWQ等流行方法。此外，我们的方法在隐私保护方面提供了额外的好处，因为它消除了任何校准或训练数据。我们还深入探讨了所提出方法的信息理论基础。我们展示了它如何利用自适应LASSO来最小化量化权重与原训练权重之间的Kullback-Leibler散度。这种最小化确保量化模型在很大程度上保留了原始模型的香农信息内容，保证了高效部署而不牺牲准确性或信息。我们的结果在各种LLM基准测试中实现了与现有方法相同的准确性，同时量化时间减少了至少10倍，巩固了我们在高效和隐私保护LLM部署方面的贡献。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The rise of Large Language Models (LLMs) has fundamentally reshaped our society
    by performing various tasks that demand sophisticated language processing capabilities.
    However, LLMs versatility comes at the cost of high power consumption and memory
    usage. Since training or fine-tuning LLMs are compute-intensive and costly, Post-training
    Quantization (PTQ) techniques have emerged as a promising solution to address
    these challenges without requiring additional training or fine-tuning. Yet, most
    existing PTQ algorithms rely on calibrating the weights of the LLMs using a calibration
    dataset, introducing additional computational overhead, time consumption, and
    potential data privacy concerns. Additionally, many PTQ methods are not truly
    zero-shot, meaning that PTQ calibration strategies can be seen as a fine-tuning
    step, even though the calibration does not use the same loss function as the fine-tuning
    stage.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的兴起从根本上重塑了我们的社会，通过执行各种需要复杂语言处理能力的任务。然而，LLMs 的多功能性以高功耗和内存使用为代价。由于训练或微调
    LLMs 计算密集且成本高昂，后训练量化（PTQ）技术应运而生，成为解决这些挑战的有前景的方案，而无需额外的训练或微调。然而，大多数现有的 PTQ 算法依赖于使用校准数据集来校准
    LLM 的权重，这引入了额外的计算开销、时间消耗和潜在的数据隐私问题。此外，许多 PTQ 方法并不真正是零-shot，这意味着 PTQ 校准策略可以被视为微调步骤，即使校准没有使用与微调阶段相同的损失函数。
- en: We propose AdpQ, a novel, zero-shot, calibration free, and adaptive PTQ method
    that is designed for LLMs. In our proposed approach, we leverage the power of
    shrinkage methods [[1](#bib.bib1)], specifically the Adaptive LASSO [[2](#bib.bib2)],
    to effectively quantize LLM weights without performing any calibration. Traditionally,
    shrinkage methods are vastly used in statistical machine learning for model selection
    and variable selection. Here, we use Adaptive LASSO to identify and isolate the
    weights that are more salient. Furthermore, our method achieves complete weight
    quantization by applying separate scales to both outlier and non-outlier weights
    during the quantization process, eliminating all floating-point weights in the
    quantized model. As will be discussed in detail throughout this paper, the main
    advantage of Adaptive LASSO is that it penalizes and isolates weights proportional
    to their originally trained values leading to an effective, calibration free outlier
    detection. While most popular PTQ methods use a calibration dataset to identify
    and tune the weights by considering activations [[3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6)], we argue that the information of activations
    is somehow encoded in the weights of the model and our proposed approach helps
    to preserve that information considerably.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了 AdpQ，一种新颖的零-shot、无校准且自适应的 PTQ 方法，专为 LLMs 设计。在我们提出的方法中，我们利用收缩方法的力量[[1](#bib.bib1)]，特别是自适应
    LASSO [[2](#bib.bib2)]，在不进行任何校准的情况下有效地量化 LLM 权重。传统上，收缩方法在统计机器学习中广泛用于模型选择和变量选择。在这里，我们使用自适应
    LASSO 来识别和隔离更显著的权重。此外，我们的方法通过在量化过程中对异常值和非异常值权重应用不同的比例，实现了完全的权重量化，从而消除了量化模型中的所有浮点权重。正如本文将详细讨论的，自适应
    LASSO 的主要优势在于它惩罚和隔离与其原始训练值成比例的权重，从而实现有效、无校准的异常值检测。虽然大多数流行的 PTQ 方法使用校准数据集来识别和调整权重，通过考虑激活[[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]，我们认为激活的信息在模型的权重中以某种方式被编码，我们提出的方法有助于显著保留这些信息。
- en: The theoretical foundation of our method is rooted in information theory. We
    study the Adaptive LASSO through the lens of Shannon entropy and demonstrate that
    Adaptive LASSO is a way to minimize the Kullback-Leibler (KL) divergence between
    the quantized weights and the originally trained weights leading to a more effective
    quantization scheme.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的理论基础根植于信息理论。我们从香农熵的角度研究自适应 LASSO，并证明自适应 LASSO 是一种最小化量化权重与原始训练权重之间的 Kullback-Leibler
    (KL) 散度的方法，从而实现更有效的量化方案。
- en: Our proposed PTQ method is faster than the state-of-the-art in terms of the
    time required to perform the quantization. We show that the Adaptive LASSO leads
    to a computationally efficient soft-thresholding approach, significantly reducing
    the run-time of the quantization algorithm. Furthermore, our method leverages
    mixed-precision quantization, allowing for separate quantization bit-width of
    outlier weights and non-outlier weights that can potentially reduce the quantization
    error mitigating the potential accuracy drops typically seen in low-precision
    quantization.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的PTQ方法在执行量化所需的时间方面比现有技术更快。我们展示了自适应LASSO如何导致一种计算效率高的软阈值方法，显著减少量化算法的运行时间。此外，我们的方法利用了混合精度量化，允许对异常值权重和非异常值权重进行单独量化位宽，这可能减少量化误差，缓解在低精度量化中通常出现的准确度下降问题。
- en: To summarize, we make the following contributions
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们做出了以下贡献
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We proposed AdpQ, a zero-shot, calibration free, and adaptive PTQ method for
    LLMs that is inspired by Adaptive LASSO regression. The proposed method only relies
    on the weights of the model and uses no external data for calibration while achieving
    the state-of-the-art accuracy performance with 10$\times$ faster quantization
    speed. To the best of our knowledge, this is the first time a zero-shot, calibration
    free PTQ is proposed that is based on Adaptive LASSO shrinkage method.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了AdpQ，一种零-shot、无需校准、适应性PTQ方法，灵感来自自适应LASSO回归。该方法仅依赖模型的权重，不使用外部数据进行校准，同时在量化速度上比现有技术快10倍，并实现了最先进的准确度表现。尽我们所知，这是首次提出基于自适应LASSO收缩方法的零-shot、无需校准PTQ。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The proposed method is computationally efficient and reduces the cost and run-time
    of the PTQ algorithm. More specifically, our proposed method can be simplified
    to an adaptive soft-thresholding algorithm which is easy to implement on any general-purpose
    hardware.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所提出的方法计算效率高，降低了PTQ算法的成本和运行时间。更具体地说，我们提出的方法可以简化为一种自适应软阈值算法，这种算法易于在任何通用硬件上实现。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our proposed method applies separate scales to outlier and non-outlier weights
    during the quantization, eliminating all floating-point weight representations
    in the model. Note that other state-of-the-art methods keep outliers weights in
    floating-point format.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出的方法在量化过程中对异常值和非异常值权重应用了不同的缩放，消除了模型中的所有浮点权重表示。请注意，其他现有技术方法保持异常值权重的浮点格式。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a theoretical foundation for our proposed PTQ method using information
    theory. We show that our adaptive quantization method tries to minimize a quadratic
    error loss using quantized weights while reasonably bounding the KL-divergence
    between the quantized weights and the originally trained weights, hence preserving
    the Shannon information contents of the original weights. To the best of our knowledge,
    this is the first time that a quantization method, has been studied from an information-theoretic
    perspective.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们利用信息理论为我们提出的PTQ方法提供了理论基础。我们展示了我们的自适应量化方法尝试通过量化权重来最小化二次误差损失，同时合理限制量化权重与原始训练权重之间的KL散度，从而保留原始权重的香农信息内容。尽我们所知，这是首次从信息理论的角度研究量化方法。
- en: 'The rest of the paper is organized as follows. Section [2](#S2 "2 Related Works
    ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") reviews recent works in the field of PTQ and specifies the differences
    to our proposed adaptive quantization method. Section [3](#S3 "3 Methodology ‣
    AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") discusses the AdpQ algorithm in detail. Section [4](#S4 "4 Theoretical
    Justification ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization
    Method for LLMs") delves deeper into the theoretical analysis of the algorithm
    and shows how AdpQ can control information loss in quantization. Finally, experimental
    results supporting our proposed methodology and theoretical findings are presented
    in Section [5](#S5 "5 Experimental Results ‣ AdpQ: A Zero-shot Calibration Free
    Adaptive Post Training Quantization Method for LLMs").'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '论文的其余部分组织如下。第 [2](#S2 "2 相关工作 ‣ AdpQ: 一种零-shot 校准无关的自适应训练后量化方法") 节回顾了 PTQ 领域的最新研究，并具体说明了与我们提出的自适应量化方法的不同。第
    [3](#S3 "3 方法论 ‣ AdpQ: 一种零-shot 校准无关的自适应训练后量化方法") 节详细讨论了 AdpQ 算法。第 [4](#S4 "4
    理论依据 ‣ AdpQ: 一种零-shot 校准无关的自适应训练后量化方法") 节*深入*探讨了算法的理论分析，并展示了 AdpQ 如何控制量化中的信息损失。最后，第
    [5](#S5 "5 实验结果 ‣ AdpQ: 一种零-shot 校准无关的自适应训练后量化方法") 节展示了支持我们提出的方法论和理论发现的实验结果。'
- en: 2 Related Works
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: In the field of low-precision deep learning, three existing notable categories
    are (i) low-precision or quantized training, (ii) quantization-aware training
    (QAT), and (iii) post-training quantization (PTQ). While our proposed method can
    be applied to both low-precision training (e.g. [[7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]) and QAT (e.g. [[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14)] ), the primary focus of this section is to
    review the research around PTQ of LLMs which is found to be more challenging in
    the literature.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在低精度深度学习领域，现有的三种显著类别是（i）低精度或量化训练，（ii）量化感知训练（QAT），以及（iii）训练后量化（PTQ）。虽然我们提出的方法可以应用于低精度训练（例如
    [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]）和
    QAT（例如 [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)]），但本节的主要重点是回顾文献中更具挑战性的
    LLMs 的 PTQ 研究。
- en: Historically, PTQ methods were common for computer vision models with small
    number of parameters, some notable methods are AdaRound [[15](#bib.bib15)], OBQ
    [[16](#bib.bib16)], AdaQuant [[17](#bib.bib17)], and BRECQ [[18](#bib.bib18)].
    However, these methods were found to be either compute-intensive or inaccurate
    for large language models.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，PTQ 方法通常用于参数较少的计算机视觉模型，一些显著的方法包括 AdaRound [[15](#bib.bib15)]、OBQ [[16](#bib.bib16)]、AdaQuant
    [[17](#bib.bib17)] 和 BRECQ [[18](#bib.bib18)]。然而，这些方法在大型语言模型中要么计算密集，要么不准确。
- en: LLM.int8() [[19](#bib.bib19)] and ZeroQuant [[20](#bib.bib20)] are among the
    first PTQ techniques that were designed for LLMs. LLM.int8() separates the outlier
    activations and keeps them in floating-point number format while quantizing weights
    and non-outlier activations to 8-bit integers. LLM.int8() separates the outlier
    activations based on their magnitude. On the other hand, ZeroQuant uses a fine-grained
    hardware-friendly quantization scheme as well as layer-by-layer knowledge distillation
    for quantizing both weight and activations. However, both LLM.int8() and ZeroQuant
    are not efficient for quantizing LLMs to extreme low-percision number formats
    such as 3-bit integers. OPTQ [[3](#bib.bib3)] is a PTQ algorithm for LLMs that
    can quantize weights to 3- or 4-bit integers. OPTQ adapted a calibration algorithm
    inspired by [[21](#bib.bib21)] that minimizes the $\ell_{2}$ loss of the quantized
    layer output with the original output. SpQR [[4](#bib.bib4)] uses OPTQ algorithm
    while separating the salient weights and keeping them in FP16 format and further
    uses double quantization to reduce the memory. Both SpQR and OPTQ algorithms require
    calibration data for quantization. SmoothQuant [[22](#bib.bib22)] performs 8-bit
    integer quantization of weights and activation by offline migration of the quantization
    difficulty from activations to weights. Likewise, AWQ [[5](#bib.bib5)], quantized
    weights by applying per-channel scales that protect the salient weights by observing
    the activation. SmoothQuant and AWQ algorithms also require calibration data to
    perform quantization.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LLM.int8() [[19](#bib.bib19)] 和 ZeroQuant [[20](#bib.bib20)] 是为LLMs设计的首批PTQ技术之一。LLM.int8()将异常激活值分离，并将其保留为浮点数格式，同时将权重和非异常激活值量化为8位整数。LLM.int8()根据异常激活值的幅度进行分离。另一方面，ZeroQuant采用精细的硬件友好量化方案以及逐层知识蒸馏来量化权重和激活值。然而，LLM.int8()
    和 ZeroQuant 对于将LLMs量化为极低精度数字格式如3位整数效率不高。OPTQ [[3](#bib.bib3)] 是一种针对LLMs的PTQ算法，能够将权重量化为3位或4位整数。OPTQ采用了一种灵感来自[[21](#bib.bib21)]的校准算法，该算法最小化量化层输出与原始输出之间的$\ell_{2}$损失。SpQR
    [[4](#bib.bib4)] 使用OPTQ算法，同时将显著权重分离并保留在FP16格式中，进一步使用双重量化来减少内存。SpQR和OPTQ算法都需要校准数据进行量化。SmoothQuant
    [[22](#bib.bib22)] 通过将量化难度从激活迁移到权重，执行8位整数权重和激活量化。同样，AWQ [[5](#bib.bib5)]通过应用每通道尺度量化权重，这些尺度通过观察激活值来保护显著权重。SmoothQuant和AWQ算法也需要校准数据来执行量化。
- en: The key feature of our proposed algorithm, AdpQ, lies in its ability to perform
    PTQ without requiring any calibration data as opposed to OPTQ, SpQR, AWQ, and
    SmoothQuant. Furthermore, AdpQ uniquely identifies and isolates outlier weights
    solely through analysis of the model weight tensors. Our proposed algorithm quantizes
    both outlier and non-outlier weights to 3- or 4-bit integer numbers, achieving
    a completely quantized model without any remaining floating-point weight values.
    Our experiments demonstrate that AdpQ delivers at least 10$\times$ faster quantization
    run-time compared to the state-of-the-art as mentioned above algorithms. Finally,
    AdpQ is designed with information theory in mind. By focusing on retaining the
    information encoded within the model weights, it aims to minimize information
    loss during the quantization process.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的算法AdpQ的关键特性在于其能够在不需要任何校准数据的情况下执行PTQ，而与OPTQ、SpQR、AWQ和SmoothQuant不同。此外，AdpQ通过分析模型权重张量独特地识别和隔离异常权重。我们提出的算法将异常和非异常权重量化为3位或4位整数，从而实现完全量化的模型，没有剩余的浮点权重值。我们的实验表明，AdpQ的量化运行时间比上述最先进的算法快至少10$\times$。最后，AdpQ以信息理论为设计基础，通过专注于保留模型权重中编码的信息，旨在最小化量化过程中的信息损失。
- en: 3 Methodology
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'Having established the limitations of existing PTQ methods and the advantages
    of our proposed zero-shot approach, this section provides a comprehensive understanding
    of the inner workings of AdpQ while its information-theoretic foundations are
    left to be discussed in Section [4](#S4 "4 Theoretical Justification ‣ AdpQ: A
    Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 确定了现有PTQ方法的局限性和我们提出的零-shot方法的优势后，本节提供了对AdpQ内在工作原理的全面理解，而其信息理论基础将在第[4](#S4 "4
    理论依据 ‣ AdpQ：一种零-shot校准自由自适应后训练量化方法")节讨论。
- en: 3.1 Existence of the Best Proxy for Weights and Adaptive LASSO
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 权重的最佳代理存在性及自适应LASSO
- en: In Post-Training Quantization (PTQ), the most critical information lies within
    the model’s weights. Here, we draw inspiration from the concept of shrinkage in
    statistical machine learning. Shrinkage techniques prioritize retaining the most
    important variables in the model while shrinking less influential ones towards
    zero. Similarly, outlier weight identification in PTQ can be viewed as a form
    of shrinkage. While traditional LASSO is commonly used for variable selection
    in statistics, it can lead to inconsistencies in the context of PTQ. To address
    this, we leverage the concept of Adaptive LASSO. This method incorporates adaptive
    weights to penalize different coefficients in the $\ell_{1}$ penalty. This allows
    us to effectively identify and isolate outlier weights within the model. Consider
    the following optimization problem,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在后训练量化（PTQ）中，最关键的信息存在于模型的权重中。在这里，我们从统计机器学习中的收缩概念中获得灵感。收缩技术优先保留模型中最重要的变量，同时将影响较小的变量缩减至零。类似地，PTQ
    中的离群权重识别可以视为一种收缩形式。虽然传统的 LASSO 常用于统计中的变量选择，但在 PTQ 上下文中可能会导致不一致。为了解决这个问题，我们利用了自适应
    LASSO 的概念。这种方法引入了自适应权重来惩罚 $\ell_{1}$ 惩罚中的不同系数。这使我们能够有效地识别和隔离模型中的离群权重。考虑以下优化问题，
- en: '|  | $\arg\min_{\hat{\textbf{W}}}\&#124;\textbf{W}\textbf{X}-\hat{\textbf{W}}\textbf{X}\&#124;_{2}^{2}+\lambda\sum_{i}\left&#124;\frac{\hat{w}_{i}}{w_{i}}\right&#124;,$
    |  | (1) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\arg\min_{\hat{\textbf{W}}}\&#124;\textbf{W}\textbf{X}-\hat{\textbf{W}}\textbf{X}\&#124;_{2}^{2}+\lambda\sum_{i}\left&#124;\frac{\hat{w}_{i}}{w_{i}}\right&#124;,$
    |  | (1) |'
- en: where X denotes the input tensor of a layer, W is the original weights of the
    layer, $\hat{\textbf{W}}$ penalty term ensuring that the weights are shrunk relative
    to their original values, as opposed to LASSO which penalizes larger weights without
    considering their original values.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 X 表示某层的输入张量，W 是该层的原始权重，$\hat{\textbf{W}}$ 惩罚项确保权重相对于其原始值被缩减，而 LASSO 则会惩罚较大的权重，而不考虑它们的原始值。
- en: 3.2 Adaptive LASSO as a Soft-Thresholding Method
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 自适应 LASSO 作为软阈值方法
- en: 'Since equation ([1](#S3.E1 "In 3.1 Existence of the Best Proxy for Weights
    and Adaptive LASSO ‣ 3 Methodology ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs")) does not have a closed-form solution,
    one may use an iterative numerical solution to solve it for a given $\lambda$.
    However, here we show that under a mild regulatory condition, Adaptive LASSO is
    a soft-thresholding algorithm.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '由于方程 ([1](#S3.E1 "在 3.1 权重的最佳代理存在性和自适应 LASSO ‣ 3 方法 ‣ AdpQ: 一种零样本校准自适应后训练量化方法"))
    没有封闭解，可能需要使用迭代数值解法来为给定的 $\lambda$ 求解。然而，在这里我们展示了在一个温和的监管条件下，自适应 LASSO 是一种软阈值算法。'
- en: 'Let us assume X is an orthogonal matrix i.e. $\textbf{X}\textbf{X}^{\top}=b\textbf{I}$
    is a constant and I is the identity matrix. By expanding ([1](#S3.E1 "In 3.1 Existence
    of the Best Proxy for Weights and Adaptive LASSO ‣ 3 Methodology ‣ AdpQ: A Zero-shot
    Calibration Free Adaptive Post Training Quantization Method for LLMs")) we have'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '假设 X 是一个正交矩阵，即 $\textbf{X}\textbf{X}^{\top}=b\textbf{I}$ 是常数，I 是单位矩阵。通过展开 ([1](#S3.E1
    "在 3.1 权重的最佳代理存在性和自适应 LASSO ‣ 3 方法 ‣ AdpQ: 一种零样本校准自适应后训练量化方法")) 我们有'
- en: '|  | $1$2 |  | (2) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: and since W, weights of the original model are constant, the Adaptive LASSO
    loss becomes
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 并且由于 W，即原始模型的权重是常数，自适应 LASSO 损失变为
- en: '|  | $1$2 |  | (3) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: By taking the derivative with respect to $\hat{w}_{i}$, and setting it equal
    to zero, it is easy to see
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对$\hat{w}_{i}$求导，并将其设置为零，可以很容易地看出
- en: '|  | $\displaystyle\hat{w}_{i}=\text{sign}({w}_{i})\text{{ReLU}}(&#124;{w}_{i}&#124;-\frac{\lambda^{\prime}}{&#124;{w}_{i}&#124;}),$
    |  | (4) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{w}_{i}=\text{sign}({w}_{i})\text{{ReLU}}(&#124;{w}_{i}&#124;-\frac{\lambda^{\prime}}{&#124;{w}_{i}&#124;}),$
    |  | (4) |'
- en: 'where $\lambda^{\prime}={\lambda}/{2b}$. Equation ([4](#S3.E4 "In 3.2 Adaptive
    LASSO as a Soft-Thresholding Method ‣ 3 Methodology ‣ AdpQ: A Zero-shot Calibration
    Free Adaptive Post Training Quantization Method for LLMs")) shows that Adaptive
    LASSO is a simple soft-thresholding method that is very efficient to be implemented
    in the currently available commodity hardware.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\lambda^{\prime}={\lambda}/{2b}$。方程 ([4](#S3.E4 "在 3.2 自适应 LASSO 作为软阈值方法
    ‣ 3 方法 ‣ AdpQ: 一种零样本校准自适应后训练量化方法")) 表明自适应 LASSO 是一种简单的软阈值方法，能够在当前可用的商品硬件上高效实现。'
- en: 'Remark 1: LLMs typically operate on high dimensional data, particularly in
    the case of large input sequences. Besides the well-known curse of dimensionality,
    high dimensional data possess some intriguing properties as well, which is commonly
    known as blessing of dimensionality [[23](#bib.bib23)]. One notable and interesting
    blessing of high dimensional data, known as the concentration phenomenon, is that
    random vectors tend to be orthogonal as shown in [[24](#bib.bib24), equation (2)],
    and in [[25](#bib.bib25), equation (3)]. Therefore, we argue that the orthogonality
    assumption, $\textbf{X}\textbf{X}^{\top}=b\textbf{I}$, is not a restrictive assumption
    in LLMs.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 备注 1：大语言模型（LLMs）通常处理高维数据，尤其是在大型输入序列的情况下。除了众所周知的维度诅咒，高维数据还具有一些有趣的性质，这通常被称为维度的祝福[[23](#bib.bib23)]。一个显著且有趣的高维数据的祝福，即集中现象，如[[24](#bib.bib24)，方程（2）]和[[25](#bib.bib25)，方程（3）]所示，随机向量往往趋向于正交。因此，我们认为在大语言模型中，正交假设$\textbf{X}\textbf{X}^{\top}=b\textbf{I}$并不是一个限制性的假设。
- en: '![Refer to caption](img/a4056332fec923a716e2ff8762b886d8.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a4056332fec923a716e2ff8762b886d8.png)'
- en: 'Figure 1: AdpQ outlier separation for weights.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：AdpQ 权重的离群值分离。
- en: 'Input: Layer weight tensor W, Outlier percentage $\alpha$'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：层权重张量 W，离群值百分比 $\alpha$
- en: Algorithm 1 AdpQ Quantization algorithm
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 AdpQ 量化算法
- en: 3.3 AdpQ Algorithm
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 AdpQ 算法
- en: 'To provide a comprehensive overview of AdpQ algorithm, Figure [1](#S3.F1.1
    "Figure 1 ‣ 3.2 Adaptive LASSO as a Soft-Thresholding Method ‣ 3 Methodology ‣
    AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") and Algorithm [1](#alg1 "In Figure 1 ‣ 3.2 Adaptive LASSO as a Soft-Thresholding
    Method ‣ 3 Methodology ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training
    Quantization Method for LLMs") offer a visual and step-by-step breakdown of the
    procedure. Figure [1](#S3.F1.1 "Figure 1 ‣ 3.2 Adaptive LASSO as a Soft-Thresholding
    Method ‣ 3 Methodology ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training
    Quantization Method for LLMs") illustrates the core principle of AdpQ. It highlights
    how the Adaptive LASSO is used to identify and isolate outlier weights within
    the model. Moreover, the figure shows both outlier and non-outlier weights are
    quantized to low-precision formats. Algorithm [1](#alg1 "In Figure 1 ‣ 3.2 Adaptive
    LASSO as a Soft-Thresholding Method ‣ 3 Methodology ‣ AdpQ: A Zero-shot Calibration
    Free Adaptive Post Training Quantization Method for LLMs") outlines the implementation
    process of AdpQ quantization in a clear two-step procedure. Starting with a large
    $\lambda^{\prime}$, of weights are identified as outliers, the process transitions
    to quantizing both the outlier and non-outlier weights using a minmax quantization
    approach.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供 AdpQ 算法的全面概述，图 [1](#S3.F1.1 "图 1 ‣ 3.2 自适应 LASSO 作为软阈值方法 ‣ 3 方法论 ‣ AdpQ：一种零-shot
    校准自由自适应后训练量化方法") 和算法 [1](#alg1 "在图 1 ‣ 3.2 自适应 LASSO 作为软阈值方法 ‣ 3 方法论 ‣ AdpQ：一种零-shot
    校准自由自适应后训练量化方法") 提供了该过程的可视化和逐步分解。图 [1](#S3.F1.1 "图 1 ‣ 3.2 自适应 LASSO 作为软阈值方法 ‣
    3 方法论 ‣ AdpQ：一种零-shot 校准自由自适应后训练量化方法") 说明了 AdpQ 的核心原理。它强调了自适应 LASSO 如何用于识别和隔离模型中的离群权重。此外，图中显示了离群权重和非离群权重都被量化为低精度格式。算法
    [1](#alg1 "在图 1 ‣ 3.2 自适应 LASSO 作为软阈值方法 ‣ 3 方法论 ‣ AdpQ：一种零-shot 校准自由自适应后训练量化方法")
    清晰地概述了 AdpQ 量化的实施过程，分为两个步骤。首先，使用较大的 $\lambda^{\prime}$ 识别权重作为离群值，然后使用 minmax 量化方法对离群权重和非离群权重进行量化。
- en: 4 Theoretical Justification
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 理论依据
- en: Having established the methodology of AdpQ algorithm, this section delves into
    the information-theoretic foundation that solidifies AdpQ effectiveness. Since
    the distribution of the quantized weights must closely follow the distribution
    of the original weight to retain the information content of the model, we formulate
    the objective function of quantization as
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在确立了 AdpQ 算法的方法论后，本节将深入探讨支撑 AdpQ 效果的信息论基础。由于量化权重的分布必须紧密跟随原始权重的分布以保持模型的信息内容，我们将量化的目标函数表述为
- en: '|  | $1$2 |  | (5) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: 'where $f_{\hat{\textbf{W}}}$ denotes the KL-divergence of the distributions.
    Our objective is to show that (i) Adaptive LASSO, equation ([1](#S3.E1 "In 3.1
    Existence of the Best Proxy for Weights and Adaptive LASSO ‣ 3 Methodology ‣ AdpQ:
    A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs")),
    is a proxy solution to the minimization problem ([5](#S4.E5 "In 4 Theoretical
    Justification ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization
    Method for LLMs")) as shown in Proposition 1, and (ii) separating the outlier
    weights improves the quantization accuracy as shown in Proposition 2.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$f_{\hat{\textbf{W}}}$表示分布的KL散度。我们的目标是证明（i）自适应LASSO，方程式（[1](#S3.E1 "在3.1权重最佳代理存在性和自适应LASSO
    ‣ 3方法 ‣ AdpQ：一种零-shot校准免疫自适应后训练量化方法")），是最小化问题（[5](#S4.E5 "在4理论证明 ‣ AdpQ：一种零-shot校准免疫自适应后训练量化方法")）的代理解，如命题1所示，以及（ii）分离异常值权重改善量化精度，如命题2所示。
- en: 'Remark 2: Although inputs tensor X appears in minimization problem ([5](#S4.E5
    "In 4 Theoretical Justification ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs")), the final algorithm is simplified
    as a soft-thresholding method on weights tensor. Thus, our proposed PTQ algorithm
    does not use any calibration data.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 备注2：尽管输入张量X出现在最小化问题（[5](#S4.E5 "在4理论证明 ‣ AdpQ：一种零-shot校准免疫自适应后训练量化方法")）中，最终算法被简化为对权重张量的软阈值方法。因此，我们提出的PTQ算法不使用任何校准数据。
- en: 'Proposition 1: Suppose $f_{\textbf{W}}$ are small. Then'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 命题1：假设$f_{\textbf{W}}$很小。那么
- en: 'Claim 1: $1$2'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 断言1：$1$2
- en: 'Claim 2: $\left|\mu_{\delta}\sum_{i}f^{\prime\prime}_{{\textbf{W}}}(w_{i})(\hat{w}_{i}-w_{i})\right|\leq
    C\left(\sum_{i}\left|\frac{\hat{w}_{i}}{w_{i}}\right|+1\right)$ is a constant.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 断言2：$\left|\mu_{\delta}\sum_{i}f^{\prime\prime}_{{\textbf{W}}}(w_{i})(\hat{w}_{i}-w_{i})\right|\leq
    C\left(\sum_{i}\left|\frac{\hat{w}_{i}}{w_{i}}\right|+1\right)$是常量。
- en: 'Proof: Assuming a quantization error $\delta_{i}$ are independent of the weights
    values. Therefore, quantized weight distribution is a convolution of original
    weights distribution and quantization error distribution such that'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：假设量化误差$\delta_{i}$与权重值独立。因此，量化权重分布是原始权重分布和量化误差分布的卷积，即
- en: '|  | $\displaystyle f_{\hat{\textbf{W}}}(\hat{w})$ |  | (6) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f_{\hat{\textbf{W}}}(\hat{w})$ |  | (6) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Using the mean value theorem for $f_{{\textbf{W}}}(\hat{w}-x)-f_{{\textbf{W}}}(\hat{w})$,
    we have
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于$f_{{\textbf{W}}}(\hat{w}-x)-f_{{\textbf{W}}}(\hat{w})$，使用均值定理，我们有
- en: '|  | $\displaystyle f_{\hat{\textbf{W}}}(\hat{w})$ |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f_{\hat{\textbf{W}}}(\hat{w})$ |  |'
- en: '|  |  | $\displaystyle\stackrel{{\scriptstyle\sigma^{2}_{\delta}\text{ is small}}}{{\approx}}f_{{\textbf{W}}}(\hat{w})-\int_{-\infty}^{\infty}xf^{\prime}_{{\textbf{W}}}(\hat{w})f_{\delta}(x)dx,$
    |  | (7) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\stackrel{{\scriptstyle\sigma^{2}_{\delta}\text{ 是小的}}}{{\approx}}f_{{\textbf{W}}}(\hat{w})-\int_{-\infty}^{\infty}xf^{\prime}_{{\textbf{W}}}(\hat{w})f_{\delta}(x)dx,$
    |  | (7) |'
- en: and thus,
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '|  | $1$2 |  | (8) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (8) |'
- en: where $\mu_{\delta}$. Then
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mu_{\delta}$。然后
- en: '|  | $1$2 |  | (9) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: 'By plugging the equation ([9](#S4.E9 "In 4 Theoretical Justification ‣ AdpQ:
    A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs"))
    in KL divergence, we have'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将方程式（[9](#S4.E9 "在4理论证明 ‣ AdpQ：一种零-shot校准免疫自适应后训练量化方法")）代入KL散度中，我们得到
- en: '|  | $1$2 |  | (10) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (10) |'
- en: Then, it follows from Taylor’s expansion around the original weight $w_{i}$
    that
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，由于泰勒展开关于原始权重$w_{i}$，
- en: '|  | $1$2 |  | (11) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (11) |'
- en: which proves Claim 1.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了断言1。
- en: To prove Claim 2, since $w_{i}$ are constants, using triangular inequality
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明断言2，由于$w_{i}$是常量，使用三角不等式
- en: '|  | $1$2 |  | (12) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (12) |'
- en: in which $C=|\mu_{\delta}|AB$. This completes the proof.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$C=|\mu_{\delta}|AB$。这完成了证明。
- en: 'Remark 3: Since in PTQ, original weights, and their distribution are known,
    the first term in equation ([11](#S4.E11 "In 4 Theoretical Justification ‣ AdpQ:
    A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs"))
    is constant. Therefore, minimizing $\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\|f_{\hat{\textbf{W}}})$
    in minimization problem ([5](#S4.E5 "In 4 Theoretical Justification ‣ AdpQ: A
    Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs"))
    which shows Adaptive LASSO is a proxy solution to minimization problem ([5](#S4.E5
    "In 4 Theoretical Justification ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs")).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注释3：由于在PTQ中，原始权重及其分布是已知的，方程 ([11](#S4.E11 "在4理论证明 ‣ AdpQ：一种零样本校准无关的自适应后训练量化方法"))
    中的第一项是常数。因此，最小化 $\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\|f_{\hat{\textbf{W}}})$
    在最小化问题 ([5](#S4.E5 "在4理论证明 ‣ AdpQ：一种零样本校准无关的自适应后训练量化方法")) 中显示自适应LASSO 是最小化问题 ([5](#S4.E5
    "在4理论证明 ‣ AdpQ：一种零样本校准无关的自适应后训练量化方法")) 的代理解决方案。
- en: In what follows, we show that separating outlier weights guided by minimizing
    $\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\|f_{\hat{\textbf{W}}})$. In the context
    of quantization, JSD represents the information gain due to separating outlier
    weights from non-outlier weights.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示了通过最小化 $\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\|f_{\hat{\textbf{W}}})$
    来分离异常权重。在量化的背景下，JSD 表示由于将异常权重与非异常权重分离而带来的信息增益。
- en: Let us assume $f_{{\textbf{W}}}$ [[26](#bib.bib26)].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 $f_{{\textbf{W}}}$ [[26](#bib.bib26)]。
- en: 'Proposition 2: Minimizing $\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\|f_{\hat{\textbf{W}}})$
    is the same as minimizing'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 命题2：最小化 $\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\|f_{\hat{\textbf{W}}})$ 等同于最小化
- en: '|  | $1$2 |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where by minimizing the Jensen–Shannon centroid for quantized weights, we minimize
    the information loss for the quantized model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化量化权重的Jensen–Shannon中心，我们可以最小化量化模型的信息损失。
- en: 'Proof:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：
- en: '|  | $\displaystyle\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\&#124;f_{\hat{\textbf{W}}})$
    |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\&#124;f_{\hat{\textbf{W}}})$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\pi\{H(f_{\hat{\textbf{W}}})+\mathbb{D}_{\text{KL}}(f^{\text{C}}_{{\textbf{W}}}\&#124;f_{\hat{\textbf{W}}})\}+(1-\pi)\{H(f^{\text{O}}_{{\textbf{W}}})+\mathbb{D}_{\text{KL}}(f^{\text{O}}_{{\textbf{W}}}\&#124;f_{\hat{\textbf{W}}}\}-H(f_{{\textbf{W}}})$
    |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\pi\{H(f_{\hat{\textbf{W}}})+\mathbb{D}_{\text{KL}}(f^{\text{C}}_{{\textbf{W}}}\&#124;f_{\hat{\textbf{W}}})\}+(1-\pi)\{H(f^{\text{O}}_{{\textbf{W}}})+\mathbb{D}_{\text{KL}}(f^{\text{O}}_{{\textbf{W}}}\&#124;f_{\hat{\textbf{W}}}\}-H(f_{{\textbf{W}}})$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  | (13) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (13) |'
- en: 'where equation ([13](#S4.E13 "In 4 Theoretical Justification ‣ AdpQ: A Zero-shot
    Calibration Free Adaptive Post Training Quantization Method for LLMs")) completes
    the proof.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 ([13](#S4.E13 "在4理论证明 ‣ AdpQ：一种零样本校准无关的自适应后训练量化方法")) 完成了证明。
- en: 'Remark 4: Note that the first term in equation ([13](#S4.E13 "In 4 Theoretical
    Justification ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization
    Method for LLMs")), i.e. $-\textit{JSD}(f^{\text{C}}_{{\textbf{W}}},f^{\text{O}}_{{\textbf{W}}})$
    can be interpreted as the information loss should we decide not to separate the
    outlier weights of the trained model.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注释4：注意到方程 ([13](#S4.E13 "在4理论证明 ‣ AdpQ：一种零样本校准无关的自适应后训练量化方法")) 中的第一项，即 $-\textit{JSD}(f^{\text{C}}_{{\textbf{W}}},f^{\text{O}}_{{\textbf{W}}})$
    可以解释为如果我们决定不分离训练模型的异常权重，则应有的信息损失。
- en: 'Remark 5: It follows from Proposition 2 that minimizing $\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\|f_{\hat{\textbf{W}}})$
    leading to minimization of information loss in the quantized model. Thus, our
    proposed method AdpQ, as a proxy solution to optimization problem [5](#S4.E5 "In
    4 Theoretical Justification ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post
    Training Quantization Method for LLMs"), helps retaining the information in the
    quantized model to a great extent.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注释5：根据命题2，最小化 $\mathbb{D}_{\text{KL}}(f_{{\textbf{W}}}\|f_{\hat{\textbf{W}}})$
    导致量化模型中信息损失的最小化。因此，我们提出的方法 AdpQ 作为优化问题 [5](#S4.E5 "在4理论证明 ‣ AdpQ：一种零样本校准无关的自适应后训练量化方法")
    的代理解决方案，能够在很大程度上保留量化模型中的信息。
- en: 5 Experimental Results
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验结果
- en: 'This section provides experimental results supporting our proposed methodology
    for post-training quantization of LLMs. Note that in our results, we use a row-wise
    group quantization technique in conjunction with AdpQ soft-thresholding method
    as explained in Algorithm [1](#alg1 "In Figure 1 ‣ 3.2 Adaptive LASSO as a Soft-Thresholding
    Method ‣ 3 Methodology ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training
    Quantization Method for LLMs").'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了支持我们提出的LLMs后训练量化方法的实验结果。请注意，在我们的结果中，我们使用了一种按行分组量化技术，并结合了AdpQ软阈值方法，如算法[1](#alg1
    "在图1 ‣ 3.2 自适应LASSO作为软阈值方法 ‣ 3 方法论 ‣ AdpQ：一种零-shot校准自由自适应后训练量化方法用于LLMs")中所述。
- en: 'Average bits: The average bit presented in the results of AdpQ is calculated
    based on three factors, (i) the number of non-outlier weights and their bit-width,
    (ii) the number of outlier weights and their bitwidth and (iii) location index
    of the outlier weights. Since AdpQ identifies and isolates the outlier weights
    in an unstructured manner, tracking the location index of the outliers is essential
    for tensor reconstruction after dealing with quantized outlier weights and non-outlier
    weights separately. While maintaining an outlier mask is straightforward, it would
    add an extra bit per weight, which is inefficient in terms of memory consumption.
    To tackle this issue, we chose to retain the location index of outliers within
    each group when using group quantization. Retaining the index of outlier weights
    leads to a lower average bit since in AdpQ, the outlier weight ratio $\alpha$
    is the group size. Moreover, we store scales and zero-points in 16-bit floating-point
    format. In summary, the average number of bits per weight is computed as'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 平均位数：AdpQ结果中呈现的平均位数是基于三个因素计算的：（i）非异常值权重的数量及其位宽，（ii）异常值权重的数量及其位宽，（iii）异常值权重的位置索引。由于AdpQ以非结构化的方式识别和隔离异常值权重，因此跟踪异常值的位置索引对在单独处理量化异常值权重和非异常值权重后进行张量重建至关重要。虽然维护异常值掩码是直接的，但这会为每个权重增加一个额外的位，这在内存消耗上是不高效的。为了解决这个问题，我们选择在使用组量化时保留每组内异常值的位置索引。保留异常值权重的索引会导致较低的平均位数，因为在AdpQ中，异常值权重比例
    $\alpha$ 是组大小。此外，我们将缩放因子和零点存储为16位浮点格式。总之，每个权重的平均位数计算为
- en: '|  | $b_{\text{avg}}=\left(b_{\text{C}}+\dfrac{2\times 16}{g}\right)\times(1-\alpha)+\left(b_{\text{O}}+\log_{2}g+\dfrac{2\times
    16}{g}\right)\times\alpha$ |  | (14) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $b_{\text{avg}}=\left(b_{\text{C}}+\dfrac{2\times 16}{g}\right)\times(1-\alpha)+\left(b_{\text{O}}+\log_{2}g+\dfrac{2\times
    16}{g}\right)\times\alpha$ |  | (14) |'
- en: where $g$ are the bit-widths of outlier and non-outlier weights respectively.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $g$ 分别是异常值和非异常值权重的位宽。
- en: 'Clipping Non-outlier Weights: We also used clipping to further reduce the $b_{\text{avg}}$
    due to index tracking of outliers. We observed that applying a clipping range
    of 90-95% to non-outliers yields similar accuracy compared to increasing the outlier
    ratio. This confirms that AdpQ can also be combined with other known quantization
    techniques to achieve better results.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 剪裁非异常值权重：我们还使用了剪裁来进一步减少由于异常值的索引跟踪导致的 $b_{\text{avg}}$。我们观察到，将剪裁范围设置为90-95%对于非异常值的准确度与增加异常值比例相似。这确认了AdpQ也可以与其他已知的量化技术结合，以实现更好的效果。
- en: 5.1 Experimental Results
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验结果
- en: 'Quantization Time: Benefiting from our simple soft-thresholding technique,
    AdpQ eliminates the need for calibration data and weight update routines. This
    significantly reduces the quantization time compared to existing methods. AdpQ
    achieves at least 10$\times$ as shown in Table [1](#S5.T1 "Table 1 ‣ 5.1 Experimental
    Results ‣ 5 Experimental Results ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs").'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 量化时间：得益于我们简单的软阈值技术，AdpQ消除了对校准数据和权重更新例程的需求。这大大减少了与现有方法相比的量化时间。AdpQ实现了至少10$\times$的改进，如表[1](#S5.T1
    "表1 ‣ 5.1 实验结果 ‣ 5 实验结果 ‣ AdpQ：一种零-shot校准自由自适应后训练量化方法用于LLMs")中所示。
- en: 'Table 1: Quantization time comparison'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：量化时间比较
- en: '| Model | Method | Avg Bits | Quantization Time (s) $\downarrow$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 平均位数 | 量化时间（秒）$\downarrow$ |'
- en: '|  | AWQ (g128) | 4.25 | 838 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | AWQ (g128) | 4.25 | 838 |'
- en: '| LLaMA-7B | SpQR | 4.63 | 10901 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | SpQR | 4.63 | 10901 |'
- en: '|  | AdpQ (g128, $\alpha$=8%) | 4.81 | 57 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha$=8%) | 4.81 | 57 |'
- en: '|  | AWQ (g128) | 4.25 | 1608 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | AWQ (g128) | 4.25 | 1608 |'
- en: '| LLaMA-13B | SpQR | 4.63 | 20502 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-13B | SpQR | 4.63 | 20502 |'
- en: '|  | AdpQ (g128, $\alpha$=6%) | 4.67 | 116 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha$=6%) | 4.67 | 116 |'
- en: '|  | AWQ (g128) | 4.25 | 3740 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | AWQ (g128) | 4.25 | 3740 |'
- en: '| LLaMA-30B | SpQR | 4.63 | 24069 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-30B | SpQR | 4.63 | 24069 |'
- en: '|  | AdpQ (g128, $\alpha$=5%) | 4.60 | 470 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha$=5%) | 4.60 | 470 |'
- en: 'Coding Ability: AdpQ is a more robust PTQ approach since it only works with
    weights and does not depend on calibration data. To showcase the robustness of
    AdpQ, we evaluated the coding performance of quantized Code-Llama model [[27](#bib.bib27)]
    on HumanEval [[28](#bib.bib28)] and MBPP [[29](#bib.bib29)] datasets. HumanEval
    includes 164 human handwritten programming problems with a function signature,
    docstring, body, and several unit tests, and MBPP consists of around 1,000 crowd-sourced
    Python programming problems. Table [2](#S5.T2 "Table 2 ‣ 5.1 Experimental Results
    ‣ 5 Experimental Results ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training
    Quantization Method for LLMs") shows AdpQ outperforms SpQR[[4](#bib.bib4)], demonstrating
    that if calibration data does not have the same nature as the task, using calibration
    data decreases the performance, while AdpQ, is robust to such issues.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '编码能力：AdpQ 是一种更强健的 PTQ 方法，因为它只处理权重，不依赖于校准数据。为了展示 AdpQ 的鲁棒性，我们评估了量化后的 Code-Llama
    模型 [[27](#bib.bib27)] 在 HumanEval [[28](#bib.bib28)] 和 MBPP [[29](#bib.bib29)]
    数据集上的编码表现。HumanEval 包含 164 个带有函数签名、文档字符串、主体和几个单元测试的人类手写编程问题，MBPP 由大约 1,000 个众包的
    Python 编程问题组成。表 [2](#S5.T2 "表 2 ‣ 5.1 实验结果 ‣ 5 实验结果 ‣ AdpQ: 一种零样本校准免疫自适应后训练量化方法")
    显示 AdpQ 优于 SpQR[[4](#bib.bib4)]，表明如果校准数据与任务的性质不一致，使用校准数据会降低性能，而 AdpQ 对此类问题具有鲁棒性。'
- en: 'Table 2: Comparison of AdpQ results for Code-Llama models on HumanEval [[28](#bib.bib28)]
    and MBPP [[29](#bib.bib29)].'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：Code-Llama 模型在 HumanEval [[28](#bib.bib28)] 和 MBPP [[29](#bib.bib29)] 上的
    AdpQ 结果比较。
- en: '| Model | Method | Avg Bits | Human Eval | MBPP |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 平均位数 | Human Eval | MBPP |'
- en: '|  |  |  | pass@1 | pass@10 | pass@1 | pass@10 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | pass@1 | pass@10 | pass@1 | pass@10 |'
- en: '|  | FP16 | 16.00 | 29.63 | 59.84 | 25.87 | 63.52 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16.00 | 29.63 | 59.84 | 25.87 | 63.52 |'
- en: '|  | RTN (g128) | 4.25 | 30.13 | 57.97 | 28.26 | 62.42 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN (g128) | 4.25 | 30.13 | 57.97 | 28.26 | 62.42 |'
- en: '| Code-Llama-7B | SpQR^∗ | 4.63 | 29.94 | 57.40 | 27.59 | 61.78 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Code-Llama-7B | SpQR^∗ | 4.63 | 29.94 | 57.40 | 27.59 | 61.78 |'
- en: '|  | AdpQ (g128, $\alpha$=5%) | 4.60 | 30.34 | 58.60 | 28.03 | 62.55 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha$=5%) | 4.60 | 30.34 | 58.60 | 28.03 | 62.55 |'
- en: '|  | FP16 | 16.00 | 34.79 | 66.50 | 30.17 | 67.51 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16.00 | 34.79 | 66.50 | 30.17 | 67.51 |'
- en: '|  | RTN (g128) | 4.25 | 33.70 | 65.88 | 29.63 | 66.00 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN (g128) | 4.25 | 33.70 | 65.88 | 29.63 | 66.00 |'
- en: '| Code-Llama-13B | SpQR^∗ | 4.63 | 34.19 | 65.69 | 29.74 | 66.20 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Code-Llama-13B | SpQR^∗ | 4.63 | 34.19 | 65.69 | 29.74 | 66.20 |'
- en: '|  | AdpQ (g128, $\alpha$=6%) | 4.67 | 34.79 | 66.02 | 31.36 | 66.82 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha$=6%) | 4.67 | 34.79 | 66.02 | 31.36 | 66.82 |'
- en: '^∗ Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix
    A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training
    Quantization Method for LLMs") for quantization settings.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '^∗ 量化设置请参见附录 [A.3](#A1.SS3 "A.3 超参数与配置 ‣ 附录 A 实验设置 ‣ AdpQ: 一种零样本校准免疫自适应后训练量化方法")。'
- en: 'Zero-Shot Task Evaluation: We also evaluated the accuracy of LLaMA 1 [[30](#bib.bib30)]
    and LLaMA 2 [[31](#bib.bib31)] models on 5 zero-shot common-sense reasoning tasks
    including ARC(easy and challenge) [[32](#bib.bib32)], HellaSwag [[33](#bib.bib33)],
    WinoGrande [[34](#bib.bib34)] and PIQA [[35](#bib.bib35)] using LM Evaluation
    Harness [[36](#bib.bib36)]. As shown in Table [3](#S5.T3 "Table 3 ‣ 5.1 Experimental
    Results ‣ 5 Experimental Results ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs"), AdpQ outperforms SpQR [[4](#bib.bib4)]
    in both 4-bit and 3-bit quantization, showing that zero-shot quantization coupled
    with Adaptive LASSO outlier detection is enough for quantization and there is
    no need for complex calibration-based methods.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '零样本任务评估：我们还评估了 LLaMA 1 [[30](#bib.bib30)] 和 LLaMA 2 [[31](#bib.bib31)] 模型在
    5 个零样本常识推理任务上的准确性，包括 ARC（简单和挑战）[[32](#bib.bib32)]、HellaSwag [[33](#bib.bib33)]、WinoGrande
    [[34](#bib.bib34)] 和 PIQA [[35](#bib.bib35)]，使用 LM Evaluation Harness [[36](#bib.bib36)]。如表
    [3](#S5.T3 "表 3 ‣ 5.1 实验结果 ‣ 5 实验结果 ‣ AdpQ: 一种零样本校准免疫自适应后训练量化方法") 所示，AdpQ 在 4
    位和 3 位量化中均优于 SpQR [[4](#bib.bib4)]，表明零样本量化结合自适应 LASSO 异常检测足以进行量化，无需复杂的基于校准的方法。'
- en: 'Table 3: Comparison of AdpQ results on zero-shot tasks using LM Evaluation
    Harness [[36](#bib.bib36)].'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：使用 LM Evaluation Harness [[36](#bib.bib36)] 对零样本任务的 AdpQ 结果比较。
- en: '| Model |  | Method | Avg Bit | ARC-c | ARC-e | HellaSwag | Winogrande | PIQA
    | Avg |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |  | 方法 | 平均位数 | ARC-c | ARC-e | HellaSwag | Winogrande | PIQA | 平均 |'
- en: '| LLaMA-7B |  | FP16 | 16 | 41.89 | 75.25 | 56.95 | 69.93 | 78.67 | 64.54 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B |  | FP16 | 16 | 41.89 | 75.25 | 56.95 | 69.93 | 78.67 | 64.54 |'
- en: '|  | RTN (g128) | 4.25 | 42.92 | 74.54 | 56.29 | 70.01 | 78.18 | 64.39 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN (g128) | 4.25 | 42.92 | 74.54 | 56.29 | 70.01 | 78.18 | 64.39 |'
- en: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 41.72 | 75.21 | 56.65 | 69.61 | 79.05
    | 64.45 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 41.72 | 75.21 | 56.65 | 69.61 | 79.05
    | 64.45 |'
- en: '|  | AdpQ (g128, $\alpha=8\%$) | 4.81 | 42.15 | 75.34 | 56.72 | 70.17 | 78.56
    | 64.59 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha=8\%$) | 4.81 | 42.15 | 75.34 | 56.72 | 70.17 | 78.56
    | 64.59 |'
- en: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.96 | 41.55 | 74.28 | 56.31 | 68.90 | 77.80
    | 63.77 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.96 | 41.55 | 74.28 | 56.31 | 68.90 | 77.80
    | 63.77 |'
- en: '|  | AdpQ (g128, $\alpha=9\%$=4) | 3.97 | 42.32 | 74.66 | 55.94 | 69.85 | 78.40
    | 64.23 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha=9\%$=4) | 3.97 | 42.32 | 74.66 | 55.94 | 69.85 | 78.40
    | 64.23 |'
- en: '| LLaMA-13B |  | FP16 | 16 | 46.42 | 77.36 | 59.88 | 72.69 | 79.16 | 67.19
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-13B |  | FP16 | 16 | 46.42 | 77.36 | 59.88 | 72.69 | 79.16 | 67.19
    |'
- en: '|  | RTN (g128) | 4.25 | 45.82 | 76.77 | 59.37 | 72.45 | 79.71 | 66.82 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN (g128) | 4.25 | 45.82 | 76.77 | 59.37 | 72.45 | 79.71 | 66.82 |'
- en: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 45.73 | 76.85 | 59.70 | 73.09 | 79.22
    | 66.92 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 45.73 | 76.85 | 59.70 | 73.09 | 79.22
    | 66.92 |'
- en: '|  | AdpQ (g128, $\alpha=6\%$) | 4.67 | 45.99 | 76.85 | 59.41 | 73.01 | 78.94
    | 66.84 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha=6\%$) | 4.67 | 45.99 | 76.85 | 59.41 | 73.01 | 78.94
    | 66.84 |'
- en: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.97 | 44.62 | 77.06 | 59.13 | 72.06 | 79.72
    | 66.52 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.97 | 44.62 | 77.06 | 59.13 | 72.06 | 79.72
    | 66.52 |'
- en: '|  | AdpQ (g128, $\alpha=10\%$) | 3.95 | 46.25 | 77.40 | 58.56 | 73.09 | 78.78
    | 66.82 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha=10\%$) | 3.95 | 46.25 | 77.40 | 58.56 | 73.09 | 78.78
    | 66.82 |'
- en: '| LLaMA-30B |  | FP16 | 16 | 52.90 | 80.43 | 63.37 | 75.85 | 81.12 | 70.73
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-30B |  | FP16 | 16 | 52.90 | 80.43 | 63.37 | 75.85 | 81.12 | 70.73
    |'
- en: '|  | RTN (g128) | 4.25 | 52.05 | 80.77 | 62.89 | 74.19 | 80.58 | 70.10 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN (g128) | 4.25 | 52.05 | 80.77 | 62.89 | 74.19 | 80.58 | 70.10 |'
- en: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 51.45 | 80.47 | 63.08 | 74.74 | 80.74
    | 70.10 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 51.45 | 80.47 | 63.08 | 74.74 | 80.74
    | 70.10 |'
- en: '|  | AdpQ (g128, $\alpha=5\%$) | 4.60 | 51.88 | 80.77 | 63.07 | 74.19 | 80.74
    | 70.13 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha=5\%$) | 4.60 | 51.88 | 80.77 | 63.07 | 74.19 | 80.74
    | 70.13 |'
- en: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.89 | 50.77 | 80.26 | 62.79 | 74.59 | 80.47
    | 69.78 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.89 | 50.77 | 80.26 | 62.79 | 74.59 | 80.47
    | 69.78 |'
- en: '|  | AdpQ (g128, $\alpha=8\%$=4) | 3.89 | 50.94 | 80.13 | 62.49 | 75.22 | 80.96
    | 69.95 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha=8\%$=4) | 3.89 | 50.94 | 80.13 | 62.49 | 75.22 | 80.96
    | 69.95 |'
- en: '| LLaMA-2-7B |  | FP16 | 16 | 43.43 | 76.35 | 57.16 | 69.14 | 78.07 | 64.83
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B |  | FP16 | 16 | 43.43 | 76.35 | 57.16 | 69.14 | 78.07 | 64.83
    |'
- en: '|  | RTN (g128) | 4.25 | 43.09 | 76.18 | 56.90 | 68.67 | 77.48 | 64.46 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN (g128) | 4.25 | 43.09 | 76.18 | 56.90 | 68.67 | 77.48 | 64.46 |'
- en: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 44.28 | 76.14 | 56.95 | 68.51 | 77.42
    | 64.66 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 44.28 | 76.14 | 56.95 | 68.51 | 77.42
    | 64.66 |'
- en: '|  | AdpQ (g128, $\alpha=8\%$) | 4.81 | 43.17 | 76.39 | 57.12 | 69.77 | 77.97
    | 64.88 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha=8\%$) | 4.81 | 43.17 | 76.39 | 57.12 | 69.77 | 77.97
    | 64.88 |'
- en: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.96 | 42.41 | 75.08 | 56.39 | 68.67 | 77.86
    | 64.08 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.96 | 42.41 | 75.08 | 56.39 | 68.67 | 77.86
    | 64.08 |'
- en: '|  | AdpQ (g128, $\alpha=10\%$) | 3.95 | 42.75 | 75.38 | 56.71 | 69.53 | 77.31
    | 64.34 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha=10\%$) | 3.95 | 42.75 | 75.38 | 56.71 | 69.53 | 77.31
    | 64.34 |'
- en: '| LLaMA-2-13B |  | FP16 | 16 | 48.46 | 79.42 | 60.05 | 72.38 | 79.11 | 67.88
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B |  | FP16 | 16 | 48.46 | 79.42 | 60.05 | 72.38 | 79.11 | 67.88
    |'
- en: '|  | RTN (g128) | 4.25 | 48.12 | 78.83 | 59.74 | 72.69 | 78.67 | 67.61 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN (g128) | 4.25 | 48.12 | 78.83 | 59.74 | 72.69 | 78.67 | 67.61 |'
- en: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 48.46 | 79.76 | 59.97 | 71.90 | 78.84
    | 67.79 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-8 | 4-bit | SpQR^∗ | 4.63 | 48.46 | 79.76 | 59.97 | 71.90 | 78.84
    | 67.79 |'
- en: '|  | AdpQ (g128, $\alpha=6\%$) | 4.67 | 48.38 | 79.63 | 59.89 | 72.53 | 78.94
    | 67.87 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha=6\%$) | 4.67 | 48.38 | 79.63 | 59.89 | 72.53 | 78.94
    | 67.87 |'
- en: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.97 | 46.33 | 78.16 | 59.54 | 72.45 | 78.24
    | 66.94 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-8 | 3-bit | SpQR^∗ | 3.97 | 46.33 | 78.16 | 59.54 | 72.45 | 78.24
    | 66.94 |'
- en: '|  | AdpQ (g128, $\alpha=10\%$) | 3.95 | 46.08 | 78.28 | 59.20 | 71.90 | 78.51
    | 66.79 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | AdpQ (g128, $\alpha=10\%$) | 3.95 | 46.08 | 78.28 | 59.20 | 71.90 | 78.51
    | 66.79 |'
- en: '^∗ Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix
    A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training
    Quantization Method for LLMs") for quantization settings.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见附录 [A.3](#A1.SS3 "A.3 超参数和配置 ‣ 附录 A 实验设置 ‣ AdpQ：一种零样本校准自由自适应后训练量化方法") 以获取量化设置。
- en: 'Perplexity: We evaluated perplexity of quantized LLaMA models on WikiText2
    [[37](#bib.bib37)] and C4 [[38](#bib.bib38)] datasets. Table [4](#S5.T4 "Table
    4 ‣ 5.1 Experimental Results ‣ 5 Experimental Results ‣ AdpQ: A Zero-shot Calibration
    Free Adaptive Post Training Quantization Method for LLMs") shows the results comparing
    perplexity scores for FP16, Round to Nearest (RTN), AWQ[[5](#bib.bib5)], SpQR,[[4](#bib.bib4)]
    and AdpQ. AdpQ outperforms AWQ and RTN consistently in terms of perplexity scores.
    Furthermore, AdpQ exhibits perplexity scores that closely follow those of SpQR,
    particularly for larger models. These results highlight AdpQ ability to achieve
    competitive accuracy while offering a significant advantage in terms of quantization
    time efficiency and robustness. Refer to Appendix [A.5](#A1.SS5 "A.5 Extra Experiments
    Results ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free
    Adaptive Post Training Quantization Method for LLMs") for more perplexity results
    on Falcon [[39](#bib.bib39)] and OPT [[40](#bib.bib40)] models.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '困惑度：我们评估了 WikiText2 [[37](#bib.bib37)] 和 C4 [[38](#bib.bib38)] 数据集上量化的 LLaMA
    模型的困惑度。表 [4](#S5.T4 "Table 4 ‣ 5.1 Experimental Results ‣ 5 Experimental Results
    ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") 展示了 FP16、四舍五入到最近（RTN）、AWQ[[5](#bib.bib5)]、SpQR[[4](#bib.bib4)] 和 AdpQ
    的困惑度得分比较结果。AdpQ 在困惑度得分方面始终优于 AWQ 和 RTN。此外，AdpQ 的困惑度得分与 SpQR 非常接近，特别是对于较大的模型。这些结果突出显示了
    AdpQ 在实现竞争性准确性的同时，在量化时间效率和稳健性方面的显著优势。有关 Falcon [[39](#bib.bib39)] 和 OPT [[40](#bib.bib40)]
    模型的更多困惑度结果，请参见附录 [A.5](#A1.SS5 "A.5 Extra Experiments Results ‣ Appendix A Experimental
    Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization
    Method for LLMs")。'
- en: 'Table 4: Comparison of AdpQ perplexity results of 4-bit & 3-bit on WikiText2
    and C4.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：4-bit 和 3-bit 在 WikiText2 和 C4 上的 AdpQ 困惑度结果比较。
- en: '|  |  | 4-bit | 3-bit |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 4-bit | 3-bit |'
- en: '| Model | Method | Quantization setting | Avg Bits | Wiki2 $\downarrow$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 量化设置 | 平均位数 | Wiki2 $\downarrow$ |'
- en: '| LLaMA-7B | FP16 | - | 16.00 | 5.67 | 7.08 | - | 16.00 | 5.67 | 7.08 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | FP16 | - | 16.00 | 5.67 | 7.08 | - | 16.00 | 5.67 | 7.08 |'
- en: '| RTN | 4bit-g128 | 4.25 | 5.96 | 7.37 | 3bit-g128 | 3.25 | 7.01 | 8.62 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4bit-g128 | 4.25 | 5.96 | 7.37 | 3bit-g128 | 3.25 | 7.01 | 8.62 |'
- en: '| AWQ | 4bit-g128 | 4.25 | 5.78 | 7.21 | 3bit-g128 | 3.25 | 6.35 | 7.81 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4bit-g128 | 4.25 | 5.78 | 7.21 | 3bit-g128 | 3.25 | 6.35 | 7.81 |'
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.63 | 5.73 | 7.13 | Refer to Appendix
    [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix A Experimental Settings
    ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") | 3.98 | 5.87 | 7.28 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 参见附录 [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix A
    Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training
    Quantization Method for LLMs") | 4.63 | 5.73 | 7.13 | 参见附录 [A.3](#A1.SS3 "A.3
    Hyper-Parameters and Configs ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot
    Calibration Free Adaptive Post Training Quantization Method for LLMs") | 3.98
    | 5.87 | 7.28 |'
- en: '| AdpQ | (4bit-g128, $\alpha=8\%$=4) | 3.97 | 6.07 | 7.51 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| AdpQ | (4bit-g128, $\alpha=8\%$=4) | 3.97 | 6.07 | 7.51 |'
- en: '| LLaMA-13B | FP16 | - | 16.00 | 5.09 | 6.61 | - | 16.00 | 5.09 | 6.61 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-13B | FP16 | - | 16.00 | 5.09 | 6.61 | - | 16.00 | 5.09 | 6.61 |'
- en: '| RTN | 4bit-g128 | 4.25 | 5.25 | 6.75 | 3bit-g128 | 3.25 | 5.88 | 7.50 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4bit-g128 | 4.25 | 5.25 | 6.75 | 3bit-g128 | 3.25 | 5.88 | 7.50 |'
- en: '| AWQ | 4bit-g128 | 4.25 | 5.18 | 6.70 | 3bit-g128 | 3.25 | 5.52 | 7.07 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4bit-g128 | 4.25 | 5.18 | 6.70 | 3bit-g128 | 3.25 | 5.52 | 7.07 |'
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.63 | 5.13 | 6.64 | Refer to Appendix
    [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix A Experimental Settings
    ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") | 3.97 | 5.22 | 6.72 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 参见附录 [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix A
    Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training
    Quantization Method for LLMs") | 4.63 | 5.13 | 6.64 | 参见附录 [A.3](#A1.SS3 "A.3
    Hyper-Parameters and Configs ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot
    Calibration Free Adaptive Post Training Quantization Method for LLMs") | 3.97
    | 5.22 | 6.72 |'
- en: '| AdpQ | (4bit-g128, $\alpha=6\%$=4) | 3.97 | 5.32 | 6.81 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| AdpQ | (4bit-g128, $\alpha=6\%$=4) | 3.97 | 5.32 | 6.81 |'
- en: '| LLaMA-30B | FP16 | - | 16.00 | 4.10 | 5.98 | - | 16.00 | 4.10 | 5.98 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-30B | FP16 | - | 16.00 | 4.10 | 5.98 | - | 16.00 | 4.10 | 5.98 |'
- en: '| RTN | 4bit-g128 | 4.25 | 4.23 | 6.10 | 3bit-g128 | 3.25 | 4.88 | 6.59 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4bit-g128 | 4.25 | 4.23 | 6.10 | 3bit-g128 | 3.25 | 4.88 | 6.59 |'
- en: '| AWQ | 4bit-g128 | 4.25 | 4.21 | 6.05 | 3bit-g128 | 3.25 | 4.61 | 6.35 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4bit-g128 | 4.25 | 4.21 | 6.05 | 3bit-g128 | 3.25 | 4.61 | 6.35 |'
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.63 | 4.14 | 6.01 | Refer to Appendix
    [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix A Experimental Settings
    ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") | 3.90 | 4.25 | 6.08 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 参见附录[A.3](#A1.SS3 "A.3 超参数和配置 ‣ 附录A 实验设置 ‣ AdpQ: 一种零-shot 校准自由的自适应后训练量化方法")
    | 4.63 | 4.14 | 6.01 | 参见附录[A.3](#A1.SS3 "A.3 超参数和配置 ‣ 附录A 实验设置 ‣ AdpQ: 一种零-shot
    校准自由的自适应后训练量化方法") | 3.90 | 4.25 | 6.08 |'
- en: '| AdpQ | (4bit-g128, $\alpha=5\%$=4) | 3.89 | 4.31 | 6.15 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| AdpQ | (4bit-g128, $\alpha=5\%$=4) | 3.89 | 4.31 | 6.15 |'
- en: '| LLaMA2-7B | FP16 | - | 16.00 | 5.47 | 6.97 | - | 16.00 | 5.47 | 6.97 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | FP16 | - | 16.00 | 5.47 | 6.97 | - | 16.00 | 5.47 | 6.97 |'
- en: '| RTN | 4bit-g128 | 4.25 | 5.72 | 7.24 | 3bit-g128 | 3.25 | 6.66 | 8.40 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4bit-g128 | 4.25 | 5.72 | 7.24 | 3bit-g128 | 3.25 | 6.66 | 8.40 |'
- en: '| AWQ | 4bit-g128 | 4.25 | 5.60 | 7.12 | 3bit-g128 | 3.25 | 6.24 | 7.81 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4bit-g128 | 4.25 | 5.60 | 7.12 | 3bit-g128 | 3.25 | 6.24 | 7.81 |'
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.63 | 5.52 | 7.03 | Refer to Appendix
    [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix A Experimental Settings
    ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") | 3.98 | 5.66 | 7.18 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 参见附录[A.3](#A1.SS3 "A.3 超参数和配置 ‣ 附录A 实验设置 ‣ AdpQ: 一种零-shot 校准自由的自适应后训练量化方法")
    | 4.63 | 5.52 | 7.03 | 参见附录[A.3](#A1.SS3 "A.3 超参数和配置 ‣ 附录A 实验设置 ‣ AdpQ: 一种零-shot
    校准自由的自适应后训练量化方法") | 3.98 | 5.66 | 7.18 |'
- en: '| AdpQ | (4bit-g128, $\alpha=8\%$=4) | 3.97 | 5.83 | 7.37 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| AdpQ | (4bit-g128, $\alpha=8\%$=4) | 3.97 | 5.83 | 7.37 |'
- en: '| LLaMA2-13B | FP16 | - | 16.00 | 4.88 | 6.47 | - | 16.00 | 4.88 | 6.47 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B | FP16 | - | 16.00 | 4.88 | 6.47 | - | 16.00 | 4.88 | 6.47 |'
- en: '| RTN | 4bit-g128 | 4.25 | 4.98 | 6.59 | 3bit-g128 | 3.25 | 5.52 | 7.18 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4bit-g128 | 4.25 | 4.98 | 6.59 | 3bit-g128 | 3.25 | 5.52 | 7.18 |'
- en: '| AWQ | 4bit-g128 | 4.25 | 4.97 | 6.56 | 3bit-g128 | 3.25 | 5.32 | 6.95 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4bit-g128 | 4.25 | 4.97 | 6.56 | 3bit-g128 | 3.25 | 5.32 | 6.95 |'
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.63 | 4.92 | 6.51 | Refer to Appendix
    [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs ‣ Appendix A Experimental Settings
    ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method
    for LLMs") | 3.96 | 5.01 | 6.60 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 参见附录[A.3](#A1.SS3 "A.3 超参数和配置 ‣ 附录A 实验设置 ‣ AdpQ: 一种零-shot 校准自由的自适应后训练量化方法")
    | 4.63 | 4.92 | 6.51 | 参见附录[A.3](#A1.SS3 "A.3 超参数和配置 ‣ 附录A 实验设置 ‣ AdpQ: 一种零-shot
    校准自由的自适应后训练量化方法") | 3.96 | 5.01 | 6.60 |'
- en: '| AdpQ | (4bit-g128, $\alpha=6\%$=4) | 3.97 | 5.05 | 6.66 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| AdpQ | (4bit-g128, $\alpha=6\%$=4) | 3.97 | 5.05 | 6.66 |'
- en: 6 Conclusion
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This paper presented AdpQ, a novel zero-shot, calibration free PTQ approach
    that is designed specifically for LLMs. AdpQ addresses the limitations of existing
    PTQ methods by leveraging the Adaptive LASSO regression for outlier identification
    and enabling quantization of both outlier and non-outlier weights. AdpQ provides
    a robust and accurate quantization scheme while achieving complete model quantization
    with low-precision integer representations. Our theoretical analysis demonstrates
    that AdpQ is aligned with information theory principles and as such ensures the
    information of the model is recovered in the quantized model to a great extent.
    Furthermore, our experimental results show AdpQ ability to achieve competitive
    accuracy on standard language modeling benchmarks while surpassing existing PTQ
    methods in terms of quantization robustness and quantization time efficiency.
    Notably, AdpQ exhibits at almost a 10$\times$ speedup in quantization time compared
    to SpQR, highlighting its significant advantage in deployment scenarios. Thus,
    AdpQ presents a compelling PTQ solution for the efficient deployment of LLMs.
    Its zero-shot calibration free nature, computational efficiency, and competitive
    accuracy make it a valuable tool for facilitating the practical application of
    LLMs.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了AdpQ，这是一种新型的零-shot、无校准PTQ方法，专门针对LLMs设计。AdpQ通过利用自适应LASSO回归来识别异常值并实现对异常值和非异常值权重的量化，从而解决了现有PTQ方法的局限性。AdpQ提供了一个稳健且准确的量化方案，同时实现了低精度整数表示的完整模型量化。我们的理论分析表明，AdpQ与信息理论原理相一致，因此在量化模型中在很大程度上恢复了模型的信息。此外，我们的实验结果表明，AdpQ在标准语言建模基准上能够实现竞争力的准确度，同时在量化稳健性和量化时间效率方面超越了现有的PTQ方法。特别地，与SpQR相比，AdpQ在量化时间上展现出近10倍的加速，突出其在部署场景中的显著优势。因此，AdpQ为LLMs的高效部署提供了一个有力的PTQ解决方案。其零-shot无校准特性、计算效率和竞争力的准确性使其成为促进LLMs实际应用的宝贵工具。
- en: References
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Copas [1983] John B Copas. Regression, prediction and shrinkage. *Journal of
    the Royal Statistical Society Series B: Statistical Methodology*, 45(3):311–335,
    1983.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Copas [1983] John B Copas. 回归、预测与收缩。*皇家统计学会B系列：统计方法学期刊*, 45(3):311–335, 1983。
- en: Zou [2006] Hui Zou. The adaptive lasso and its oracle properties. *Journal of
    the American statistical association*, 101(476):1418–1429, 2006.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou [2006] Hui Zou. 自适应套索及其oracle属性。*美国统计协会期刊*, 101(476):1418–1429, 2006。
- en: 'Frantar et al. [2023] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. OPTQ: accurate quantization for generative pre-trained transformers.
    In *The Eleventh International Conference on Learning Representations, ICLR 2023,
    Kigali, Rwanda, May 1-5, 2023*, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. [2023] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan
    Alistarh. OPTQ: 生成预训练变换器的准确量化。发表于*第十一届国际学习表征会议, ICLR 2023, 基加利, 卢旺达, 2023年5月1-5日*,
    2023。'
- en: 'Dettmers et al. [2024a] Tim Dettmers, Ruslan A. Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. SpQR: A sparse-quantized representation for near-lossless LLM
    weight compression. In *The Twelfth International Conference on Learning Representations*,
    2024a.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. [2024a] Tim Dettmers, Ruslan A. Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    和 Dan Alistarh. SpQR: 用于近无损LLM权重压缩的稀疏量化表示。发表于*第十二届国际学习表征会议*, 2024a。'
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. AWQ: 针对LLM压缩和加速的激活感知权重量化。*arXiv预印本 arXiv:2306.00978*, 2023。'
- en: 'Huang et al. [2024] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming
    Zhang, Xianglong Liu, Michele Magno, and Xiaojuan Qi. Billm: Pushing the limit
    of post-training quantization for llms. *arXiv preprint arXiv:2402.04291*, 2024.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. [2024] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming
    Zhang, Xianglong Liu, Michele Magno, 和 Xiaojuan Qi. Billm: 推动LLMs的后训练量化极限。*arXiv预印本
    arXiv:2402.04291*, 2024。'
- en: Banner et al. [2018] Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry.
    Scalable methods for 8-bit training of neural networks. *Advances in neural information
    processing systems*, 31, 2018.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banner et al. [2018] Ron Banner, Itay Hubara, Elad Hoffer, 和 Daniel Soudry.
    用于神经网络的8位训练的可扩展方法。*神经信息处理系统进展*, 31, 2018。
- en: Zhang et al. [2020] Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang,
    Shiyi Zhou, Jiaming Guo, Qi Guo, Zidong Du, Tian Zhi, et al. Fixed-point back-propagation
    training. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 2330–2338, 2020.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2020] Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang,
    Shiyi Zhou, Jiaming Guo, Qi Guo, Zidong Du, Tian Zhi 等。固定点反向传播训练。见于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，第2330–2338页，2020。
- en: Zhu et al. [2020] Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang,
    Zhelong Li, Xiuqi Yang, and Junjie Yan. Towards unified int8 training for convolutional
    neural network. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, pages 1969–1979, 2020.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2020] Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang,
    Zhelong Li, Xiuqi Yang, 和 Junjie Yan. 面向卷积神经网络的统一 int8 训练。见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第1969–1979页，2020。
- en: Zhao et al. [2021] Kang Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya Zhang,
    Zhenyu Gu, and Yinghui Xu. Distribution adaptive int8 quantization for training
    cnns. In *Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence*,
    2021.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. [2021] Kang Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya Zhang,
    Zhenyu Gu, 和 Yinghui Xu. 分布自适应 int8 量化用于训练 CNN。见于 *第三十五届 AAAI 人工智能会议论文集*，2021。
- en: Ghaffari et al. [2022] Alireza Ghaffari, Marzieh S Tahaei, Mohammadreza Tayaranian,
    Masoud Asgharian, and Vahid Partovi Nia. Is integer arithmetic enough for deep
    learning training? *Advances in Neural Information Processing Systems*, 35:27402–27413,
    2022.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghaffari et al. [2022] Alireza Ghaffari, Marzieh S Tahaei, Mohammadreza Tayaranian,
    Masoud Asgharian, 和 Vahid Partovi Nia. 整数算术是否足以进行深度学习训练？*神经信息处理系统进展*，第35卷，第27402–27413页，2022。
- en: Zhu et al. [2023] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A
    survey on model compression for large language models. *arXiv preprint arXiv:2308.07633*,
    2023.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2023] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, 和 Weiping Wang. 大型语言模型的模型压缩调查。*arXiv
    预印本 arXiv:2308.07633*，2023。
- en: 'Dettmers et al. [2024b] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *Advances in Neural
    Information Processing Systems*, 36, 2024b.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. [2024b] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke
    Zettlemoyer. Qlora: 高效的量化 LLM 微调。*神经信息处理系统进展*，36卷，2024b。'
- en: 'Liu et al. [2023] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models. *arXiv
    preprint arXiv:2305.17888*, 2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra.
    Llm-qat: 无数据量化感知训练用于大型语言模型。*arXiv 预印本 arXiv:2305.17888*，2023。'
- en: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training
    quantization. In Hal Daumé III and Aarti Singh, editors, *Proceedings of the 37th
    International Conference on Machine Learning*, volume 119 of *Proceedings of Machine
    Learning Research*, pages 7197–7206\. PMLR, 13–18 Jul 2020.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, 和 Tijmen Blankevoort. 向上还是向下？后训练量化的自适应舍入。见于 Hal Daumé III 和 Aarti Singh
    主编的 *第37届国际机器学习会议论文集*，*机器学习研究论文集*第119卷，第7197–7206页。PMLR，2020年7月13日至18日。
- en: 'Frantar and Alistarh [2022] Elias Frantar and Dan Alistarh. Optimal brain compression:
    A framework for accurate post-training quantization and pruning. In S. Koyejo,
    S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, *Advances in
    Neural Information Processing Systems*, volume 35, pages 4475–4488\. Curran Associates,
    Inc., 2022.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 和 Alistarh [2022] Elias Frantar 和 Dan Alistarh. 最优脑压缩：准确的后训练量化和剪枝框架。见于
    S. Koyejo、S. Mohamed、A. Agarwal、D. Belgrave、K. Cho 和 A. Oh 主编的 *神经信息处理系统进展*，第35卷，第4475–4488页。Curran
    Associates, Inc.，2022。
- en: Hubara et al. [2021] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and
    Daniel Soudry. Accurate post training quantization with small calibration sets.
    In Marina Meila and Tong Zhang, editors, *Proceedings of the 38th International
    Conference on Machine Learning*, volume 139 of *Proceedings of Machine Learning
    Research*, pages 4466–4475\. PMLR, 18–24 Jul 2021.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara et al. [2021] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, 和 Daniel
    Soudry. 使用小型校准集进行精确的后训练量化。见于 Marina Meila 和 Tong Zhang 主编的 *第38届国际机器学习会议论文集*，*机器学习研究论文集*第139卷，第4466–4475页。PMLR，2021年7月18日至24日。
- en: 'Li et al. [2021] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, and Shi Gu. BRECQ: pushing the limit of post-training quantization
    by block reconstruction. In *9th International Conference on Learning Representations,
    ICLR 2021, Virtual Event, Austria, May 3-7, 2021*, 2021.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等人[2021] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei
    Yu, Wei Wang, 和 Shi Gu。BRECQ: 通过块重建推动后训练量化的极限。见 *第9届国际表示学习会议，ICLR 2021，虚拟活动，奥地利，2021年5月3-7日*，2021年。'
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Gpt3.int8(): 8-bit matrix multiplication for transformers at scale. In S. Koyejo,
    S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, *Advances in
    Neural Information Processing Systems*, volume 35, pages 30318–30332\. Curran
    Associates, Inc., 2022.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers等人[2022] Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer。Gpt3.int8():
    适用于大规模变换器的8位矩阵乘法。见 S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, 和 A.
    Oh 编著，*神经信息处理系统进展*，第35卷，页30318–30332。Curran Associates, Inc., 2022年。'
- en: 'Yao et al. [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. In S. Koyejo, S. Mohamed, A. Agarwal,
    D. Belgrave, K. Cho, and A. Oh, editors, *Advances in Neural Information Processing
    Systems*, volume 35, pages 27168–27183\. Curran Associates, Inc., 2022.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao等人[2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, 和 Yuxiong He。Zeroquant: 高效且实惠的大规模变换器后训练量化。见 S. Koyejo, S. Mohamed, A. Agarwal,
    D. Belgrave, K. Cho, 和 A. Oh 编著，*神经信息处理系统进展*，第35卷，页27168–27183。Curran Associates,
    Inc., 2022。'
- en: 'Hassibi and Stork [1992] Babak Hassibi and David Stork. Second order derivatives
    for network pruning: Optimal brain surgeon. In S. Hanson, J. Cowan, and C. Giles,
    editors, *Advances in Neural Information Processing Systems*, volume 5\. Morgan-Kaufmann,
    1992.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi和Stork[1992] Babak Hassibi 和 David Stork。网络剪枝的二阶导数：最佳大脑外科医生。见 S. Hanson,
    J. Cowan, 和 C. Giles 编著，*神经信息处理系统进展*，第5卷。Morgan-Kaufmann，1992年。
- en: 'Xiao et al. [2023] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. SmoothQuant: Accurate and efficient post-training quantization for
    large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara
    Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, *Proceedings of the
    40th International Conference on Machine Learning*, volume 202 of *Proceedings
    of Machine Learning Research*, pages 38087–38099\. PMLR, 23–29 Jul 2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao等人[2023] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    和 Song Han。SmoothQuant: 精确且高效的大型语言模型后训练量化。见 Andreas Krause, Emma Brunskill, Kyunghyun
    Cho, Barbara Engelhardt, Sivan Sabato, 和 Jonathan Scarlett 编著，*第40届国际机器学习会议论文集*，第202卷，*机器学习研究论文集*，页38087–38099。PMLR,
    2023年7月23-29日。'
- en: 'Donoho et al. [2000] David L Donoho et al. High-dimensional data analysis:
    The curses and blessings of dimensionality. *AMS math challenges lecture*, 1(2000):32,
    2000.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Donoho等人[2000] David L Donoho等人。高维数据分析：维度的诅咒与祝福。*AMS数学挑战讲座*，1(2000):32，2000年。
- en: 'Hall et al. [2005] Peter Hall, James Stephen Marron, and Amnon Neeman. Geometric
    representation of high dimension, low sample size data. *Journal of the Royal
    Statistical Society Series B: Statistical Methodology*, 67(3):427–444, 2005.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hall等人[2005] Peter Hall, James Stephen Marron, 和 Amnon Neeman。高维低样本数据的几何表示。*皇家统计学会B系列期刊：统计方法*，67(3):427–444，2005年。
- en: Zorich [2015] VA Zorich. Multidimensional geometry, functions of very many variables,
    and probability. *Theory of Probability & Its Applications*, 59(3):481–493, 2015.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zorich[2015] VA Zorich。多维几何、非常多变量的函数和概率。*概率论与应用*，59(3):481–493，2015年。
- en: Ghaffari et al. [2023] Alireza Ghaffari, Justin Yu, Mahsa Ghazvini Nejad, Masoud
    Asgharian, Boxing Chen, and Vahid Partovi Nia. Mitigating outlier activations
    in low-precision fine-tuning of language models. *arXiv preprint arXiv:2312.09211*,
    2023.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghaffari等人[2023] Alireza Ghaffari, Justin Yu, Mahsa Ghazvini Nejad, Masoud Asgharian,
    Boxing Chen, 和 Vahid Partovi Nia。缓解语言模型低精度微调中的离群激活。*arXiv预印本 arXiv:2312.09211*，2023年。
- en: 'Roziere et al. [2023] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. Code llama: Open foundation models for code. *arXiv preprint arXiv:2308.12950*,
    2023.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roziere等人[2023] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla,
    Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin 等人。Code
    llama: 开放的代码基础模型。*arXiv预印本 arXiv:2308.12950*，2023年。'
- en: Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. Evaluating large language models trained on code. *arXiv
    preprint arXiv:2107.03374*, 2021.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, 等。评估训练于代码上的大型语言模型。*arXiv 预印本 arXiv:2107.03374*，2021。
- en: Austin et al. [2021] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. Program synthesis with large language models. *arXiv preprint arXiv:2108.07732*,
    2021.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin et al. [2021] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, 等。利用大型语言模型进行程序合成。*arXiv 预印本 arXiv:2108.07732*，2021。
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models (2023). *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等。Llama: 开放且高效的基础语言模型（2023）。*arXiv 预印本 arXiv:2302.13971*，2023a。'
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等。Llama 2: 开放的基础和微调的对话模型。*arXiv 预印本 arXiv:2307.09288*，2023b。'
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, 和 Oyvind Tafjord。认为你已经解决了问答问题？试试 arc，AI2 推理挑战。*arXiv
    预印本 arXiv:1803.05457*，2018。
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    和 Yejin Choi。Hellaswag: 机器真的能完成你的句子吗？*arXiv 预印本 arXiv:1905.07830*，2019。'
- en: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    和 Yejin Choi。Winogrande: 一项大规模的对抗性 Winograd 模式挑战。*ACM 通讯*，64(9):99–106，2021。'
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439,
    2020.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, 等。Piqa:
    在自然语言中推理关于物理常识。载于*AAAI 人工智能会议论文集*，第 34 卷，页 7432–7439，2020。'
- en: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    et al. A framework for few-shot language model evaluation. *Version v0\. 0.1\.
    Sept*, page 8, 2021.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    等。一个少量样本语言模型评估框架。*版本 v0\. 0.1\. Sept*，第 8 页，2021。
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher。指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843*，2016。
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of machine learning research*, 21(140):1–67, 2020.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu。通过统一的文本到文本变换器探索迁移学习的极限。*机器学习研究杂志*，21(140):1–67，2020。
- en: 'Almazrouei et al. [2023] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,
    Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel
    Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and
    Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art
    performance. 2023.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Almazrouei 等人 [2023] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,
    Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel
    Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, 和
    Guilherme Penedo。Falcon-40B：一种具有最先进性能的开放大型语言模型。2023。
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin 等人。Opt：开放预训练变换器语言模型。*arXiv
    预印本 arXiv:2205.01068*，2022。
- en: 'Computer [2023] Together Computer. Redpajama: An open source recipe to reproduce
    llama training dataset, 2023.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Computer [2023] Together Computer。Redpajama：一种开放源代码配方，用于重现 llama 训练数据集，2023。
- en: 'Penedo et al. [2023] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
    and Julien Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated
    corpora with web data, and web data only. *arXiv preprint arXiv:2306.01116*, 2023.
    URL [https://arxiv.org/abs/2306.01116](https://arxiv.org/abs/2306.01116).'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Penedo 等人 [2023] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei
    和 Julien Launay。RefinedWeb 数据集用于 Falcon LLM：用网络数据超越了策划语料库，仅使用网络数据。*arXiv 预印本 arXiv:2306.01116*，2023。网址
    [https://arxiv.org/abs/2306.01116](https://arxiv.org/abs/2306.01116)。
- en: 'Gao et al. [2020] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al.
    The pile: An 800gb dataset of diverse text for language modeling. *arXiv preprint
    arXiv:2101.00027*, 2020.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 [2020] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima 等人。The
    pile：一个包含多样文本的 800GB 数据集，用于语言建模。*arXiv 预印本 arXiv:2101.00027*，2020。
- en: Appendix A Experimental Settings
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 实验设置
- en: A.1 Seed Sensitivity
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 种子敏感性
- en: Since our proposed method, AdpQ, only uses deterministic pre-trained weights
    of the model and performs a soft-thresholding to identify $\alpha$ percent of
    outlier weights, it does not exhibit any stochastic behavior during the quantization.
    Furthermore, we do not use any data for calibration and thus, our algorithm is
    robust toward randomness in data selection. We believe this is the main advantage
    of our proposed algorithm.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们提出的方法 AdpQ 仅使用模型的确定性预训练权重，并执行软阈值处理以识别 $\alpha$ 百分比的离群权重，因此在量化过程中不表现出任何随机行为。此外，我们不使用任何数据进行校准，因此，我们的算法对数据选择中的随机性具有鲁棒性。我们认为这是我们提出的算法的主要优点。
- en: A.2 Calibration Datasets and Parameters
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 校准数据集和参数
- en: We follow the pipelines used in SpQR¹¹1See [https://github.com/Vahe1994/SpQR](https://github.com/Vahe1994/SpQR)
    and AWQ²²2See [https://github.com/mit-han-lab/llm-awq](https://github.com/mit-han-lab/llm-awq)
    official implementation to generate calibration datasets. Random selection of
    128 samples of length 2048 form RedPajama [[41](#bib.bib41)], C4 and RefinedWeb
    [[42](#bib.bib42)] is used for quantization of LLaMA 1, LLaMa 2, OPT [[40](#bib.bib40)]
    and Falcon [[39](#bib.bib39)] using SpQR. For AWQ experiments 128 samples from
    a small subset of Pile [[43](#bib.bib43)] dataset is used following the AWQ’s
    implementation.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循 SpQR¹¹1 使用的管道，见 [https://github.com/Vahe1994/SpQR](https://github.com/Vahe1994/SpQR)
    和 AWQ²²2 官方实现，见 [https://github.com/mit-han-lab/llm-awq](https://github.com/mit-han-lab/llm-awq)，生成校准数据集。随机选择
    128 个长度为 2048 的样本，来自 RedPajama [[41](#bib.bib41)]、C4 和 RefinedWeb [[42](#bib.bib42)]，用于
    SpQR 的 LLaMA 1、LLaMa 2、OPT [[40](#bib.bib40)] 和 Falcon [[39](#bib.bib39)] 的量化。对于
    AWQ 实验，使用了来自 Pile [[43](#bib.bib43)] 数据集的小子集中的 128 个样本，遵循 AWQ 的实现。
- en: A.3 Hyper-Parameters and Configs
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 超参数和配置
- en: 'RTN: We implemented RTN quantization method based on the implementation of
    AWQ which supports weight reshaping for group quantization.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: RTN：我们基于 AWQ 的实现实施了 RTN 量化方法，该方法支持组量化的权重重塑。
- en: 'AWQ: We used AWQ’s official implementation for quantizing LLaMA and OPT models.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: AWQ：我们使用了 AWQ 的官方实现来量化 LLaMA 和 OPT 模型。
- en: 'SpQR: We use SpQR’s official implementation for quantizing LLaMA, Code-Llama
    and OPT models. Table [5](#A1.T5 "Table 5 ‣ A.3 Hyper-Parameters and Configs ‣
    Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") shows the hyper-parameters used for
    SpQR quantization.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 'SpQR：我们使用 SpQR 的官方实现对 LLaMA、Code-Llama 和 OPT 模型进行量化。表 [5](#A1.T5 "表 5 ‣ A.3
    超参数和配置 ‣ 附录 A 实验设置 ‣ AdpQ: 一种零-shot 校准自由自适应后训练量化方法") 显示了用于 SpQR 量化的超参数。'
- en: 'Table 5: Quantization configuration of SpQR'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：SpQR 的量化配置
- en: '| Model | Calibration | Group | Weight | Scales & Zeros | Outlier |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 校准 | 组 | 权重 | 缩放 & 零 | 异常值 |'
- en: '|  | Set | Size | Bits | Bits | Threshold |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | 设置 | 大小 | 位数 | 位数 | 阈值 |'
- en: '| LLaMA | RedPajama | 16 | 4 | 3 | 0.2 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA | RedPajama | 16 | 4 | 3 | 0.2 |'
- en: '|  | RedPajama | 16 | 3 | 3 | 0.25-0.28 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | RedPajama | 16 | 3 | 3 | 0.25-0.28 |'
- en: '| Code-Llama | RedPajama | 16 | 4 | 3 | 0.2 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Code-Llama | RedPajama | 16 | 4 | 3 | 0.2 |'
- en: '| OPT | C4 | 16 | 4 | 3 | 0.2 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| OPT | C4 | 16 | 4 | 3 | 0.2 |'
- en: '| Falcon | RefinedWeb | 16 | 4 | 3 | 0.2 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Falcon | RefinedWeb | 16 | 4 | 3 | 0.2 |'
- en: A.4 Hardware Settings
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 硬件设置
- en: We perform quantization on single NVIDIA V100-32G GPU. For evaluation using
    LM Evaluation Harness we use 8$\times$V100-32G GPUs for 30B models.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在单个 NVIDIA V100-32G GPU 上进行量化。对于 LM Evaluation Harness 的评估，我们使用 8$\times$V100-32G
    GPUs 进行 30B 模型的测试。
- en: A.5 Extra Experiments Results
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 额外实验结果
- en: 'Table [6](#A1.T6 "Table 6 ‣ A.5 Extra Experiments Results ‣ Appendix A Experimental
    Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization
    Method for LLMs") shows the perplexity results on OPT [[40](#bib.bib40)] and Falcon[[39](#bib.bib39)]
    models.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [6](#A1.T6 "表 6 ‣ A.5 额外实验结果 ‣ 附录 A 实验设置 ‣ AdpQ: 一种零-shot 校准自由自适应后训练量化方法")
    显示了 OPT [[40](#bib.bib40)] 和 Falcon[[39](#bib.bib39)] 模型的困惑度结果。'
- en: 'Table 6: Perplexity of 4-bit OPT and Falcon models on WikiText2 and C4.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：4-bit OPT 和 Falcon 模型在 WikiText2 和 C4 上的困惑度。
- en: '| Model | Method | Quantization setting | Avg Bits | Wiki2 $\downarrow$ |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 量化设置 | 平均位数 | Wiki2 $\downarrow$ |'
- en: '| OPT-6.7B | FP16 | - | 16.00 | 10.86 | 11.74 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7B | FP16 | - | 16.00 | 10.86 | 11.74 |'
- en: '| RTN | 4bit-g128 | 4.25 | 11.15 | 12.31 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4bit-g128 | 4.25 | 11.15 | 12.31 |'
- en: '| AWQ | 4bit-g128 | 4.25 | 10.95 | 11.86 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4bit-g128 | 4.25 | 10.95 | 11.86 |'
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.63 | 10.91 | 11.78 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 参见附录 [A.3](#A1.SS3 "A.3 超参数和配置 ‣ 附录 A 实验设置 ‣ AdpQ: 一种零-shot 校准自由自适应后训练量化方法")
    | 4.63 | 10.91 | 11.78 |'
- en: '| AdpQ | (4bit-g128, $\alpha=6\%$) | 4.67 | 10.86 | 11.99 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| AdpQ | (4bit-g128, $\alpha=6\%$) | 4.67 | 10.86 | 11.99 |'
- en: '| OPT-13B | FP16 | - | 16.00 | 10.13 | 11.20 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13B | FP16 | - | 16.00 | 10.13 | 11.20 |'
- en: '| RTN | 4bit-g128 | 4.25 | 10.30 | 11.51 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4bit-g128 | 4.25 | 10.30 | 11.51 |'
- en: '| AWQ | 4bit-g128 | 4.25 | 10.29 | 11.28 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4bit-g128 | 4.25 | 10.29 | 11.28 |'
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.27 | 10.22 | 11.27 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 参见附录 [A.3](#A1.SS3 "A.3 超参数和配置 ‣ 附录 A 实验设置 ‣ AdpQ: 一种零-shot 校准自由自适应后训练量化方法")
    | 4.27 | 10.22 | 11.27 |'
- en: '| AdpQ | (4bit-g128, $\alpha=6\%$) | 4.67 | 10.20 | 11.31 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| AdpQ | (4bit-g128, $\alpha=6\%$) | 4.67 | 10.20 | 11.31 |'
- en: '| OPT-30B | FP16 | - | 16.00 | 9.55 | 10.69 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30B | FP16 | - | 16.00 | 9.55 | 10.69 |'
- en: '| RTN | 4bit-g128 | 4.25 | 9.94 | 10.94 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4bit-g128 | 4.25 | 9.94 | 10.94 |'
- en: '| AWQ | 4bit-g128 | 4.25 | 9.61 | 10.74 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4bit-g128 | 4.25 | 9.61 | 10.74 |'
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.63 | 9.55 | 10.71 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 参见附录 [A.3](#A1.SS3 "A.3 超参数和配置 ‣ 附录 A 实验设置 ‣ AdpQ: 一种零-shot 校准自由自适应后训练量化方法")
    | 4.63 | 9.55 | 10.71 |'
- en: '| AdpQ | (4bit-g128, $\alpha=5\%$) | 4.60 | 9.64 | 10.79 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| AdpQ | (4bit-g128, $\alpha=5\%$) | 4.60 | 9.64 | 10.79 |'
- en: '| Falcon-7B | FP16 | - | 16.00 | 6.59 | 9.50 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Falcon-7B | FP16 | - | 16.00 | 6.59 | 9.50 |'
- en: '| RTN | 4bit-g128 | 4.25 | 6.79 | 9.79 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4bit-g128 | 4.25 | 6.79 | 9.79 |'
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.44 | 6.64 | 9.58 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 参见附录 [A.3](#A1.SS3 "A.3 超参数和配置 ‣ 附录 A 实验设置 ‣ AdpQ: 一种零-shot 校准自由自适应后训练量化方法")
    | 4.44 | 6.64 | 9.58 |'
- en: '| AdpQ | (4bit-g128, $\alpha=4\%$) | 4.53 | 6.69 | 9.63 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| AdpQ | (4bit-g128, $\alpha=4\%$) | 4.53 | 6.69 | 9.63 |'
- en: '| Falcon-40B | FP16 | - | 16.00 | 5.23 | 7.76 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| Falcon-40B | FP16 | - | 16.00 | 5.23 | 7.76 |'
- en: '| RTN | 4bit-g128 | 4.25 | 5.31 | 7.88 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4bit-g128 | 4.25 | 5.31 | 7.88 |'
- en: '| SpQR | Refer to Appendix [A.3](#A1.SS3 "A.3 Hyper-Parameters and Configs
    ‣ Appendix A Experimental Settings ‣ AdpQ: A Zero-shot Calibration Free Adaptive
    Post Training Quantization Method for LLMs") | 4.46 | 5.26 | 7.79 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 参见附录 [A.3](#A1.SS3 "A.3 超参数和配置 ‣ 附录 A 实验设置 ‣ AdpQ: 一种用于LLMs的零样本校准自适应后训练量化方法")
    | 4.46 | 5.26 | 7.79 |'
- en: '| AdpQ | (4bit-g128, $\alpha=5\%$) | 4.60 | 5.27 | 7.81 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| AdpQ | (4bit-g128, $\alpha=5\%$) | 4.60 | 5.27 | 7.81 |'
