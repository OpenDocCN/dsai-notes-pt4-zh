- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:04:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Pruning as a Domain-specific LLM Extractor
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作为领域特定的 LLM 提取器的剪枝
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06275](https://ar5iv.labs.arxiv.org/html/2405.06275)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06275](https://ar5iv.labs.arxiv.org/html/2405.06275)
- en: Nan Zhang^♣  Yanchi Liu^♢  Xujiang Zhao^♢  Wei Cheng^♢
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Nan Zhang^♣  Yanchi Liu^♢  Xujiang Zhao^♢  Wei Cheng^♢
- en: Runxue Bao^♢  Rui Zhang^♣  Prasenjit Mitra^♣  Haifeng Chen^♢
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Runxue Bao^♢  Rui Zhang^♣  Prasenjit Mitra^♣  Haifeng Chen^♢
- en: ^♣The Pennsylvania State University  ^♢NEC Labs America
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^♣宾夕法尼亚州立大学  ^♢NEC Labs America
- en: '{njz5124,rmz5227,pmitra}@psu.edu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{njz5124,rmz5227,pmitra}@psu.edu'
- en: '{yanchi,xuzhao,weicheng,rbao,haifeng}@nec-labs.com Work done as a Research
    Intern at NEC Labs America.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{yanchi,xuzhao,weicheng,rbao,haifeng}@nec-labs.com 在 NEC Labs America 担任研究实习生期间所完成的工作。'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have exhibited remarkable proficiency across a
    wide array of NLP tasks. However, the escalation in model size also engenders
    substantial deployment costs. While few efforts have explored model pruning techniques
    to reduce the size of LLMs, they mainly center on general or task-specific weights.
    This leads to suboptimal performance due to lacking *specificity* on the target
    domain or *generality* on different tasks when applied to domain-specific challenges.
    This work introduces an innovative unstructured dual-pruning methodology, D-Pruner,
    for domain-specific compression on LLM. It extracts a compressed, domain-specific,
    and task-agnostic LLM by identifying LLM weights that are pivotal for general
    capabilities, like linguistic capability and multi-task solving, and domain-specific
    knowledge. More specifically, we first assess general weight importance by quantifying
    the error incurred upon their removal with the help of an open-domain calibration
    dataset. Then, we utilize this general weight importance to refine the training
    loss, so that it preserves generality when fitting into a specific domain. Moreover,
    by efficiently approximating weight importance with the refined training loss
    on a domain-specific calibration dataset, we obtain a pruned model emphasizing
    *generality* and *specificity*. Our comprehensive experiments across various tasks
    in healthcare and legal domains show the effectiveness of D-Pruner in domain-specific
    compression. Our code is available at [https://github.com/psunlpgroup/D-Pruner](https://github.com/psunlpgroup/D-Pruner).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在广泛的 NLP 任务中表现出了卓越的能力。然而，模型规模的扩大也带来了显著的部署成本。尽管少数研究探索了模型剪枝技术以减少 LLM
    的规模，但这些研究主要集中在一般或任务特定的权重上。这导致了性能不佳，因为在应用于特定领域的挑战时，缺乏对目标领域的*特异性*或对不同任务的*通用性*。本工作介绍了一种创新的非结构化双剪枝方法
    D-Pruner，用于领域特定的 LLM 压缩。它通过识别对一般能力（如语言能力和多任务解决）以及领域特定知识至关重要的 LLM 权重，提取一个压缩的、领域特定的且任务无关的
    LLM。更具体地，我们首先通过量化移除这些权重时产生的误差来评估一般权重的重要性，使用开放领域的校准数据集。然后，我们利用这些一般权重的重要性来优化训练损失，从而在拟合特定领域时保持通用性。此外，通过在领域特定的校准数据集上有效地近似权重的重要性，我们获得了一个强调*通用性*和*特异性*的剪枝模型。我们在医疗和法律领域的各种任务上的综合实验展示了
    D-Pruner 在领域特定压缩中的有效性。我们的代码可在 [https://github.com/psunlpgroup/D-Pruner](https://github.com/psunlpgroup/D-Pruner)
    获得。
- en: Pruning as a Domain-specific LLM Extractor
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作为领域特定的 LLM 提取器的剪枝
- en: 'Nan Zhang^♣^†^†thanks: Work done as a Research Intern at NEC Labs America.
     Yanchi Liu^♢  Xujiang Zhao^♢  Wei Cheng^♢ Runxue Bao^♢  Rui Zhang^♣  Prasenjit
    Mitra^♣  Haifeng Chen^♢ ^♣The Pennsylvania State University  ^♢NEC Labs America
    {njz5124,rmz5227,pmitra}@psu.edu {yanchi,xuzhao,weicheng,rbao,haifeng}@nec-labs.com'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Nan Zhang^♣^†^†感谢：在 NEC Labs America 担任研究实习生期间所完成的工作。  Yanchi Liu^♢  Xujiang
    Zhao^♢  Wei Cheng^♢ Runxue Bao^♢  Rui Zhang^♣  Prasenjit Mitra^♣  Haifeng Chen^♢
    ^♣宾夕法尼亚州立大学  ^♢NEC Labs America {njz5124,rmz5227,pmitra}@psu.edu {yanchi,xuzhao,weicheng,rbao,haifeng}@nec-labs.com
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) such as the GPT family (Brown et al., [2020](#bib.bib5))
    and the LLaMA family (Touvron et al., [2023](#bib.bib38)) have exhibited remarkable
    advancements across a diverse spectrum of NLP tasks. However, the substantial
    size of LLMs engenders cost-intensive deployment in real-world applications and
    renders them unsuitable for scenarios necessitating efficient inference and low
    latency (Bai et al., [2024](#bib.bib1)). Recently, model pruning techniques have
    been successfully applied to language models (Han et al., [2015](#bib.bib13);
    Xia et al., [2022](#bib.bib40); Frantar and Alistarh, [2023](#bib.bib11)). These
    methods aim to yield a compact language model characterized by a significantly
    reduced parameter count, which is cost-efficient for deployment. However, most
    of them target relatively small language models, and only a few focus on LLMs
    (Frantar and Alistarh, [2023](#bib.bib11); Ma et al., [2023](#bib.bib26); Sun
    et al., [2023](#bib.bib34); Xia et al., [2023](#bib.bib39)). Moreover, the existing
    strategies mainly center on general or task-specific weights, leading to suboptimal
    performance due to lacking *specificity* on the target domain or *generality*
    on different tasks when applied to domain-specific challenges. Here *generality*
    refers to the general capabilities of an LLM such as language understanding and
    generation, and multi-task solving, and *specificity* refers to the capability
    of an LLM to understand domain-specific knowledge.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），如GPT家族（Brown等，[2020](#bib.bib5)）和LLaMA家族（Touvron等，[2023](#bib.bib38)），在各种NLP任务上展现了显著的进展。然而，LLMs的庞大规模导致在实际应用中部署成本高昂，使其不适合需要高效推理和低延迟的场景（Bai等，[2024](#bib.bib1)）。最近，模型剪枝技术已成功应用于语言模型（Han等，[2015](#bib.bib13)；Xia等，[2022](#bib.bib40)；Frantar和Alistarh，[2023](#bib.bib11)）。这些方法旨在产生一个紧凑的语言模型，其参数数量显著减少，从而降低部署成本。然而，大多数方法针对的是相对较小的语言模型，仅有少数关注LLMs（Frantar和Alistarh，[2023](#bib.bib11)；Ma等，[2023](#bib.bib26)；Sun等，[2023](#bib.bib34)；Xia等，[2023](#bib.bib39)）。此外，现有策略主要集中在通用或任务特定的权重上，导致性能欠佳，因为在应用于领域特定挑战时缺乏对目标领域的*特异性*或对不同任务的*通用性*。这里*通用性*指的是LLM的整体能力，如语言理解和生成，以及多任务解决能力，而*特异性*指的是LLM理解领域特定知识的能力。
- en: As shown in [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Pruning as a Domain-specific
    LLM Extractor"), the weights in an LLM work together to support its general capabilities
    and to store various domain knowledge. The domain-shared weights (or general weights)
    empower the LLM with linguistic and multi-task solving prowess akin to human language
    usage and thinking. The domain-specific weights (or domain weights) are pivotal
    for endowing the LLM with domain-specific expertise mirroring that of domain experts.
    However, the current pruning methods mainly focus on preserving general or task-specific
    weights, which may not be enough to deal with domain-specific problems. For example,
    post-training pruning methods (Frantar and Alistarh, [2023](#bib.bib11)) assume
    the model is optimized and prune unimportant weights based on an open-domain calibration
    dataset. This leads to a pruned model that focuses on model generality with domain-specific
    weights not considered. On the other hand, pruning with fine-tuning methods (Ma
    et al., [2023](#bib.bib26)) utilizes gradients during fine-tuning on a specific
    task to estimate the importance of parameters. As a result, the pruned model focuses
    on the model specificity while decreasing the linguistic and multi-task solving
    capabilities, compromising the LLM’s capacity as a versatile task-agnostic solver.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Pruning as a Domain-specific LLM Extractor")所示，LLM中的权重协同工作，以支持其一般能力并存储各种领域知识。领域共享权重（或通用权重）赋予LLM类似于人类语言使用和思维的语言和多任务解决能力。领域特定权重（或领域权重）对于赋予LLM与领域专家相似的领域特定专业知识至关重要。然而，目前的剪枝方法主要关注保留通用或任务特定的权重，这可能不足以应对领域特定的问题。例如，训练后剪枝方法（Frantar和Alistarh，[2023](#bib.bib11)）假设模型已经优化，并基于开放领域校准数据集剪枝不重要的权重。这导致了一个关注模型通用性的剪枝模型，而未考虑领域特定权重。另一方面，使用微调方法的剪枝（Ma等，[2023](#bib.bib26)）在特定任务的微调过程中利用梯度来估计参数的重要性。因此，剪枝模型关注模型的特异性，同时减少语言和多任务解决能力，妨碍了LLM作为多功能任务无关求解器的能力。
- en: '![Refer to caption](img/5157104b936d0a32e93d45c161f29b7d.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5157104b936d0a32e93d45c161f29b7d.png)'
- en: 'Figure 1: Different types of pruning methods. An LLM is composed of domain-shared
    weights and domain-specific weights. Post-training pruning focuses on domain-shared
    weights for generality, pruning with fine-tuning focuses on domain-specific weights
    for specificity, and our dual-pruning method preserves weights pivotal for both
    generality and specificity.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：不同类型的剪枝方法。一个大型语言模型（LLM）由共享领域权重和特定领域权重组成。后训练剪枝关注于共享领域权重以实现通用性，而剪枝加微调则关注于特定领域权重以实现特异性，我们的双重剪枝方法则保留了对通用性和特异性都至关重要的权重。
- en: 'To this end, this study introduces a novel dual-pruning approach, D-Pruner,
    for domain-specific unstructured pruning on LLMs, which aims to extract a domain-specific
    LLM from the foundation LLM. This extracted model is able to solve different tasks
    in the target domain and facilitates further domain-specific fine-tuning. D-Pruner
    is designed to harness calibration data for guiding LLM pruning processes while
    preserving generality and specificity for multi-task solving and domain challenges.
    The resulting compressed LLM can be seamlessly adapted to the target domain, enabling
    deployment with limited computing resources. Specifically, D-Pruner adeptly captures
    and retains both general and domain parameters while selectively eliminating insignificant
    model parameters. This mechanism comprises the following steps: firstly, a general
    weight importance module operates to assess the significance of model parameters
    for general capabilities. Subsequently, we propose an updated training loss function
    based on the autoregressive training objective for the next token prediction by
    integrating the general importance as a regularization term. This way, we identify
    weights contributing to both generality and domain specificity when training on
    a domain calibration dataset. Then, with the updated loss function, we compute
    the weight importance leveraging gradients without updating the model. Moreover,
    an approximation algorithm, empirical Fisher (Martens, [2020](#bib.bib27); Sung
    et al., [2021](#bib.bib35)), is utilized to compute the weight importance efficiently
    for pruning.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，本研究介绍了一种新颖的双重剪枝方法 D-Pruner，用于 LLM 的特定领域非结构化剪枝，旨在从基础 LLM 中提取一个特定领域的 LLM。提取的模型能够解决目标领域中的不同任务，并促进进一步的领域特定微调。D-Pruner
    旨在利用校准数据指导 LLM 剪枝过程，同时保持多任务解决和领域挑战所需的通用性和特异性。最终的压缩 LLM 可以无缝地适应目标领域，实现有限计算资源下的部署。具体而言，D-Pruner
    灵活地捕捉和保留了通用和领域参数，同时选择性地消除不重要的模型参数。该机制包括以下步骤：首先，通用权重重要性模块评估模型参数对通用能力的重要性。随后，我们提出了一种基于自回归训练目标的更新训练损失函数，通过将通用重要性作为正则化项来进行下一个
    token 预测。这样，在针对领域校准数据集进行训练时，我们能够识别出对通用性和领域特异性都重要的权重。然后，使用更新的损失函数，我们在不更新模型的情况下，利用梯度计算权重重要性。此外，使用近似算法经验
    Fisher（Martens，[2020](#bib.bib27)；Sung et al.，[2021](#bib.bib35)）高效地计算剪枝的权重重要性。
- en: We evaluate the performance of D-Pruner on LLaMA2 (Touvron et al., [2023](#bib.bib38)),
    a widely adopted open-source LLM. Our experimental findings demonstrate that D-Pruner
    exhibits remarkable efficiency in the extraction of sparse domain networks from
    pre-trained LLMs, with a limited amount of calibration data provided. Remarkably,
    D-Pruner achieves comparable results to the full dense model while achieving 50%
    sparsity, surpassing the performance of alternative pruning techniques across
    diverse domain-specific datasets in healthcare and legal domains encompassing
    language comprehension, question answering, and summarization tasks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在广泛使用的开源 LLM LLaMA2（Touvron et al.，[2023](#bib.bib38)）上评估了 D-Pruner 的性能。我们的实验结果表明，D-Pruner
    在从预训练的 LLM 中提取稀疏领域网络方面表现出显著的效率，同时提供了有限的校准数据。值得注意的是，D-Pruner 在达到 50% 稀疏性的同时，能够获得与完整密集模型相当的结果，超越了在医疗和法律领域的各种特定领域数据集上的其他剪枝技术的性能，包括语言理解、问答和总结任务。
- en: 2 Related Work
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Model compression involves transforming a large, resource-intensive model into
    a compact version suitable for low-resource deployment Deng et al. ([2020](#bib.bib6));
    Zhu et al. ([2023](#bib.bib44)). There are mainly three techniques for model compression,
    which are pruning, knowledge distillation, and quantization.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩涉及将一个大型、资源密集型模型转变为适用于低资源部署的紧凑版本（Deng et al.，[2020](#bib.bib6)；Zhu et al.，[2023](#bib.bib44)）。模型压缩主要有三种技术：剪枝、知识蒸馏和量化。
- en: Pruning.
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 剪枝。
- en: Pruning techniques in neural networks can be broadly classified into structured
    pruning and unstructured pruning (Xia et al., [2022](#bib.bib40); Sanh et al.,
    [2020](#bib.bib33); Du et al., [2021](#bib.bib10)). Structured pruning entails
    the removal of entire network components, such as channels or layers, guided by
    specific criteria, while maintaining the overall network architecture. In contrast,
    unstructured pruning targets individual weights, leading to an irregular sparse
    structure.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的剪枝技术可以大致分为结构化剪枝和非结构化剪枝（Xia等，[2022](#bib.bib40)；Sanh等，[2020](#bib.bib33)；Du等，[2021](#bib.bib10)）。结构化剪枝涉及根据特定标准删除整个网络组件，如通道或层，同时保持整体网络架构。相比之下，非结构化剪枝则针对单个权重，导致不规则的稀疏结构。
- en: While numerous attempts have been made to prune language models of relatively
    small scales, such as BERT (Kenton and Toutanova, [2019](#bib.bib19)), scant attention
    has been devoted to pruning LLMs containing billions of parameters. These larger
    models possess 100-1000 times more weights, rendering the pruning task significantly
    more challenging. SparseGPT (Frantar and Alistarh, [2023](#bib.bib11)), a post-training
    method for Large Language Models (LLMs), lacks the capability to identify crucial
    weights tailored to specific domains or tasks as it refrains from fine-tuning.
    On the other hand, LLM-Pruner (Ma et al., [2023](#bib.bib26)) employs gradient-based
    techniques for pruning. However, it falls short in identifying pivotal weights
    essential for domain-shared knowledge, resulting in pruned models that lack the
    desired level of generality.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经有很多尝试对相对小规模的语言模型进行剪枝，例如BERT（Kenton和Toutanova，[2019](#bib.bib19)），但对包含数十亿参数的LLM进行剪枝的关注却很少。这些更大的模型拥有100到1000倍的权重，使得剪枝任务显著更具挑战性。SparseGPT（Frantar和Alistarh，[2023](#bib.bib11)），一种针对大型语言模型（LLMs）的后训练方法，缺乏识别特定领域或任务关键权重的能力，因为它不进行微调。另一方面，LLM-Pruner（Ma等，[2023](#bib.bib26)）采用基于梯度的剪枝技术，但在识别对领域共享知识至关重要的权重方面存在不足，导致剪枝模型缺乏所需的通用性。
- en: The existing pruning methods either focus on general or domain-specific weights,
    yet none of them consider preserving both at the same time. To the best of our
    knowledge, we are the first to work on pruning LLMs while preserving weights important
    to both generality and specificity.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的剪枝方法要么专注于通用权重，要么专注于特定领域的权重，但都没有同时考虑保留两者。我们是首个在剪枝LLM时同时保留对通用性和特异性都重要的权重的工作者。
- en: Knowledge Distillation.
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 知识蒸馏。
- en: Knowledge Distillation (KD) has emerged as a powerful technique, drawing considerable
    interest for its ability to augment model performance and enhance generalization
    capacities (Hinton et al., [2015](#bib.bib16); Zhu et al., [2023](#bib.bib44)).
    At its core, KD revolves around the transfer of expertise from a complex model,
    referred to as the “teacher model”, to a simplified counterpart known as the “student
    model”. This intricate process of knowledge transfer aims to distill the profound
    insights encapsulated within the teacher models, condensing them into a more concise
    and efficient representation within the student models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏（KD）作为一种强大的技术，引起了广泛关注，因为它能够提高模型性能和增强泛化能力（Hinton等，[2015](#bib.bib16)；Zhu等，[2023](#bib.bib44)）。KD的核心是将来自复杂模型的知识，即“教师模型”，转移到简化的“学生模型”中。这一复杂的知识转移过程旨在将教师模型中蕴含的深刻见解提炼成更简洁高效的学生模型表示。
- en: While KD has been proven a powerful tool for model compression, it needs specific
    downstream tasks and a large amount of data for the student models to learn from
    the teacher models. Thus, the output that student models produce mainly focuses
    on a specific task and loses the generality capability. KD generally sets higher
    requirements on data availability and computation budgets (e.g., GPU memory) than
    pruning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然知识蒸馏（KD）已被证明是模型压缩的强大工具，但它需要特定的下游任务和大量数据，以便学生模型能够从教师模型中学习。因此，学生模型生成的输出主要集中在特定任务上，丧失了通用能力。与剪枝相比，KD通常对数据可用性和计算预算（例如GPU内存）有更高要求。
- en: Quantization.
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化。
- en: In the realm of model compression, quantization has emerged as a widely embraced
    technique to alleviate the storage and computational challenges inherent in deep
    learning models (Guo et al., [2020](#bib.bib12); Dettmers et al., [2021](#bib.bib8),
    [2022](#bib.bib7), [2023](#bib.bib9)). Conventional model representations rely
    on floating-point numbers, but quantization converts them into integers or discrete
    forms. This transformation leads to substantial reductions in storage requirements
    and computational complexities. While a certain degree of precision loss is inevitable,
    carefully designed quantization methods can achieve significant model compression
    with minimal accuracy degradation. Although challenges remain, such as maintaining
    model interpretability and addressing task-specific intricacies, the current body
    of research establishes a robust groundwork for ongoing advancements in LLM quantization,
    which could be complementary to LLM pruning.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型压缩领域，量化已成为一种广泛采用的技术，以缓解深度学习模型中固有的存储和计算挑战（Guo et al., [2020](#bib.bib12);
    Dettmers et al., [2021](#bib.bib8), [2022](#bib.bib7), [2023](#bib.bib9)）。传统的模型表示依赖于浮点数，而量化将其转换为整数或离散形式。这种转化显著减少了存储需求和计算复杂性。尽管不可避免地会有一定的精度损失，但精心设计的量化方法可以在最小化准确性降级的同时实现显著的模型压缩。尽管仍面临挑战，例如保持模型的可解释性和处理任务特定的复杂性，但当前的研究成果为LLM量化的持续进展奠定了坚实的基础，这可能与LLM剪枝相辅相成。
- en: 3 Methodology
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: To preserve both generality and specificity on the pruned model, our dual-pruning
    method D-Pruner considers weights important to both generality and specificity
    during training on a calibration dataset. Note we only use the weight gradient
    generated from the training process but do not update the model weights. Our model
    is pruned in a task-agnostic fashion (e.g., we adopted a pre-training objective,
    next token prediction, as a part of training loss) so that the pruned model can
    solve different tasks in the target domain.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在剪枝模型中保留通用性和特异性，我们的双重剪枝方法D-Pruner在校准数据集上的训练过程中考虑了对通用性和特异性都重要的权重。请注意，我们仅使用训练过程中生成的权重梯度，但不更新模型权重。我们的模型以任务无关的方式进行剪枝（例如，我们采用了预训练目标，即下一词预测，作为训练损失的一部分），以便剪枝后的模型能够解决目标领域中的不同任务。
- en: 'D-Pruner comprises the following steps: firstly, a general weight locating
    module operates to assess the significance of model parameters for general understanding
    (Section [3.1](#S3.SS1 "3.1 General Weight Importance ‣ 3 Methodology ‣ Pruning
    as a Domain-specific LLM Extractor")). Subsequently, an updated loss function
    for the training process is proposed by integrating the general weight importance
    as a regularization term. This way, we identify weights contributing to both general
    and domain knowledge (Section [3.2](#S3.SS2 "3.2 Updated Loss with Regularization
    ‣ 3 Methodology ‣ Pruning as a Domain-specific LLM Extractor")). Finally, with
    the updated loss function, we compute the weight gradients on a small domain calibration
    dataset without updating the model and approximate our dual-pruning weight importance
    by utilizing the empirical Fisher index (Sung et al., [2021](#bib.bib35)) for
    pruning (Section [3.3](#S3.SS3 "3.3 Dual-pruning Importance Score ‣ 3 Methodology
    ‣ Pruning as a Domain-specific LLM Extractor")).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: D-Pruner包括以下步骤：首先，通用权重定位模块运行，以评估模型参数在通用理解中的重要性（第[3.1节](#S3.SS1 "3.1 General
    Weight Importance ‣ 3 Methodology ‣ Pruning as a Domain-specific LLM Extractor")）。随后，通过将通用权重重要性作为正则化项，提出了更新的训练过程损失函数。这样，我们可以识别对通用和领域知识都有贡献的权重（第[3.2节](#S3.SS2
    "3.2 Updated Loss with Regularization ‣ 3 Methodology ‣ Pruning as a Domain-specific
    LLM Extractor")）。最后，使用更新后的损失函数，我们在小规模领域校准数据集上计算权重梯度，而不更新模型，并通过利用经验Fisher指数（Sung
    et al., [2021](#bib.bib35)）来近似我们的双重剪枝权重重要性进行剪枝（第[3.3节](#S3.SS3 "3.3 Dual-pruning
    Importance Score ‣ 3 Methodology ‣ Pruning as a Domain-specific LLM Extractor")）。
- en: Our method concentrates on unstructured pruning in a layer-by-layer manner for
    the Transformers model. We consider query, key, value, and output projections
    of all self-attention layers and gate (Liu et al., [2021](#bib.bib24)), down,
    and up projections of all MLP (multilayer perceptron) layers for pruning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法专注于对Transformers模型进行逐层的非结构化剪枝。我们考虑了所有自注意力层的查询、键、值和输出投影，以及所有MLP（多层感知机）层的门控（Liu
    et al., [2021](#bib.bib24)）、下投影和上投影进行剪枝。
- en: 3.1 General Weight Importance
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 一般权重重要性
- en: 'The first step of our method involves locating important weights in terms of
    general knowledge. Following the same hypothesis as Frantar and Alistarh ([2023](#bib.bib11)),
    we assume that an important weight will cause a larger increase in loss value
    than those less important ones if it is pruned (set to $0$, can be approximated
    using Taylor series as shown by LeCun et al. ([1989](#bib.bib22)):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的第一步涉及根据一般知识定位重要权重。按照 Frantar 和 Alistarh 的假设 ([2023](#bib.bib11))，我们假设如果一个重要的权重被剪枝（设为
    $0$），它会比那些不太重要的权重引起更大的损失值增加，可以使用 Taylor 展开式来近似，如 LeCun 等人所示 ([1989](#bib.bib22))：
- en: '|  |  | $\displaystyle I_{W^{m}}=&#124;\mathcal{L}(\mathcal{D}_{g})-\mathcal{L}_{W^{m}=0}(\mathcal{D}_{g})&#124;$
    |  | (1) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle I_{W^{m}}=&#124;\mathcal{L}(\mathcal{D}_{g})-\mathcal{L}_{W^{m}=0}(\mathcal{D}_{g})&#124;$
    |  | (1) |'
- en: '|  |  | $\displaystyle=&#124;\frac{\partial\mathcal{L}(\mathcal{D}_{g})}{\partial
    W^{m}}W^{m}+\frac{1}{2}W^{m}H_{mm}W^{m}$ |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=&#124;\frac{\partial\mathcal{L}(\mathcal{D}_{g})}{\partial
    W^{m}}W^{m}+\frac{1}{2}W^{m}H_{mm}W^{m}$ |  |'
- en: '|  |  | $\displaystyle+O(&#124;&#124;W^{m}&#124;&#124;^{3})&#124;$ |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+O(&#124;&#124;W^{m}&#124;&#124;^{3})&#124;$ |  |'
- en: 'where $H$ as:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H$ 如下：
- en: '|  | $\varepsilon^{m}=\frac{1}{2}\frac{(W^{m})^{2}}{[H^{-1}]_{mm}}$ |  | (2)
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\varepsilon^{m}=\frac{1}{2}\frac{(W^{m})^{2}}{[H^{-1}]_{mm}}$ |  | (2)
    |'
- en: $\varepsilon^{m}$.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: $\varepsilon^{m}$。
- en: 3.2 Updated Loss with Regularization
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 带正则化的更新损失
- en: 'To identify the weights that are important in both general and domain-specific
    knowledge, we modify the original loss function of LLM training. In LLM training,
    cross-entropy loss is used in the next token prediction task (Radford et al.,
    [2018](#bib.bib29)). Similar to Thompson et al. ([2019](#bib.bib37)), we add a
    regularization term to constrain the change of important general weights found
    in the first step. Suppose that there are $M$ to obtain our final training objective:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别在一般和领域特定知识中都重要的权重，我们修改了 LLM 训练的原始损失函数。在 LLM 训练中，交叉熵损失用于下一个 token 预测任务 (Radford
    等人，[2018](#bib.bib29))。类似于 Thompson 等人 ([2019](#bib.bib37))，我们添加了一个正则化项来约束第一步中找到的重要一般权重的变化。假设有
    $M$ 来获得我们的最终训练目标：
- en: '|  | $\mathcal{L}_{\text{ours}}=\mathcal{L}_{\text{next}}+\lambda\sum_{m=1}^{M}G^{m}({W^{m}}^{\prime}-W^{m})^{2}$
    |  | (3) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{ours}}=\mathcal{L}_{\text{next}}+\lambda\sum_{m=1}^{M}G^{m}({W^{m}}^{\prime}-W^{m})^{2}$
    |  | (3) |'
- en: where $G^{m}$.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G^{m}$。
- en: 'In practice, the direct calculation of this regularization term in the forward
    pass is computationally expensive for two reasons: (1) it involves both $W^{m}$,
    we reduce the regularization term to:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在前向传播过程中直接计算这个正则化项在计算上是昂贵的，原因有两个：（1）它涉及 $W^{m}$，我们将正则化项简化为：
- en: '|  | $\displaystyle\mathcal{L}_{\text{regular}}$ |  | (4) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{regular}}$ |  | (4) |'
- en: '|  |  | $\displaystyle=\lambda\sum_{m=1}^{M}G^{m}(W^{m}-\alpha g_{\text{next}}^{m}-W^{m})^{2}$
    |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\lambda\sum_{m=1}^{M}G^{m}(W^{m}-\alpha g_{\text{next}}^{m}-W^{m})^{2}$
    |  |'
- en: '|  |  | $\displaystyle=\lambda\sum_{m=1}^{M}\alpha^{2}G^{m}(g_{\text{next}}^{m})^{2}$
    |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\lambda\sum_{m=1}^{M}\alpha^{2}G^{m}(g_{\text{next}}^{m})^{2}$
    |  |'
- en: 'During the backward pass, optimizing this regularization term requires second-order
    derivatives, which indicates that Hessian matrices ($H$. We write the gradient
    of the regularization with respect to every parameter matrix in a finer granularity:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播过程中，优化这个正则化项需要二阶导数，这表明需要海森矩阵（$H$）。我们以更精细的粒度书写正则化对每个参数矩阵的梯度：
- en: '|  | $\frac{\partial\mathcal{L}_{\text{regular}}}{\partial W^{m}}\approx 2\lambda\alpha^{2}G^{m}g_{\text{next}}^{m}H_{mm}$
    |  | (5) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial\mathcal{L}_{\text{regular}}}{\partial W^{m}}\approx 2\lambda\alpha^{2}G^{m}g_{\text{next}}^{m}H_{mm}$
    |  | (5) |'
- en: '|  | $H_{mm}\approx\frac{1}{P}\sum_{j=1}^{P}(g_{\text{next}}^{m}(x_{j},y_{j}))^{2}$
    |  | (6) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $H_{mm}\approx\frac{1}{P}\sum_{j=1}^{P}(g_{\text{next}}^{m}(x_{j},y_{j}))^{2}$
    |  | (6) |'
- en: 'We directly compute $\frac{\partial\mathcal{L}_{\text{regular}}}{\partial W}$
    via Equation [5](#S3.E5 "In 3.2 Updated Loss with Regularization ‣ 3 Methodology
    ‣ Pruning as a Domain-specific LLM Extractor") above instead of relying on PyTorch
    backward pass to maximize computing efficiency. The final gradient computation
    of our regularized loss function is shown below:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们直接通过上述方程[5](#S3.E5 "在 3.2 带正则化的更新损失 ‣ 3 方法论 ‣ 作为特定领域 LLM 提取器的剪枝")计算$\frac{\partial\mathcal{L}_{\text{regular}}}{\partial
    W}$，以最大化计算效率，而不是依赖于 PyTorch 反向传播。我们正则化损失函数的最终梯度计算如下：
- en: '|  | $\frac{\partial\mathcal{L_{\text{ours}}}}{\partial W^{m}}=\frac{\partial\mathcal{L_{\text{next}}}}{\partial
    W^{m}}+\frac{\partial\mathcal{L_{\text{regular}}}}{\partial W^{m}}$ |  | (7) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial\mathcal{L_{\text{ours}}}}{\partial W^{m}}=\frac{\partial\mathcal{L_{\text{next}}}}{\partial
    W^{m}}+\frac{\partial\mathcal{L_{\text{regular}}}}{\partial W^{m}}$ |  | (7) |'
- en: '|  | InternalMed_Harrison | MedNLI | PubMedQA | HQS | MultiLegalPile | CaseHOLD
    | BillSum |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | InternalMed_Harrison | MedNLI | PubMedQA | HQS | MultiLegalPile | CaseHOLD
    | BillSum |'
- en: '| Domain | Healthcare | Healthcare | Healthcare | Healthcare | Legal | Legal
    | Legal |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 医疗保健 | 医疗保健 | 医疗保健 | 医疗保健 | 法律 | 法律 | 法律 |'
- en: '| Task / Type | Generation | NLI | QA | Summarization | Generation | QA | Summarization
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 任务 / 类型 | 生成 | 自然语言推理（NLI） | 问答（QA） | 总结 | 生成 | 问答（QA） | 总结 |'
- en: '| # Instances in Test | 300 | 1422 | 500 | 100 | 300 | 200 | 200 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 测试中的实例数 | 300 | 1422 | 500 | 100 | 300 | 200 | 200 |'
- en: '| Metrics | Perplexity | Accuracy | Macro-F1 | ROUGE | Perplexity | Macro-F1
    | ROUGE |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 困惑度 | 准确率 | 宏F1 | ROUGE | 困惑度 | 宏F1 | ROUGE |'
- en: 'Table 1: Details of each dataset that we use for model evaluation.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：我们用于模型评估的每个数据集的详细信息。
- en: 3.3 Dual-pruning Importance Score
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 双重剪枝重要性分数
- en: 'Finally, we calculate the dual-pruning importance score of each weight, and
    unimportant weights can be pruned according to their importance. We use Equation LABEL:eq1
    for importance estimation instead of Equation [2](#S3.E2 "In 3.1 General Weight
    Importance ‣ 3 Methodology ‣ Pruning as a Domain-specific LLM Extractor"), because
    our model has not converged to an optimum on the target domain. However, direct
    computation of the Hessian matrix in Equation [2](#S3.E2 "In 3.1 General Weight
    Importance ‣ 3 Methodology ‣ Pruning as a Domain-specific LLM Extractor") is infeasible
    since it involves $O(M^{2})$ can be defined as:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算每个权重的双重剪枝重要性分数，且不重要的权重可以根据其重要性进行剪枝。我们使用方程 LABEL:eq1 进行重要性估计，而不是方程 [2](#S3.E2
    "在3.1 通用权重重要性 ‣ 3 方法 ‣ 作为领域特定LLM提取器的剪枝")，因为我们的模型尚未在目标领域收敛到最佳状态。然而，直接计算方程 [2](#S3.E2
    "在3.1 通用权重重要性 ‣ 3 方法 ‣ 作为领域特定LLM提取器的剪枝")中的海森矩阵是不可行的，因为它涉及到 $O(M^{2})$，可以定义为：
- en: '|  | $\displaystyle S^{m}$ |  | (8) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle S^{m}$ |  | (8) |'
- en: '|  |  | $\displaystyle+O(&#124;&#124;W^{m}&#124;&#124;^{3})&#124;$ |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+O(&#124;&#124;W^{m}&#124;&#124;^{3})&#124;$ |  |'
- en: Here $O(||W^{m}||^{3})$ considers both general and domain-specific knowledge
    via our regularized training objective. Combining both regularization and importance
    estimation via empirical Fisher approximation, our method expects to conduct pruning
    that maintains weights important to both general and domain-specific knowledge,
    thus preserving generality and specificity. And these importance scores are used
    to guide our pruning decisions. For example, if we set the sparsity level to be
    50%, weights that have the smallest 50% of importance scores in each layer will
    be pruned.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $O(||W^{m}||^{3})$ 通过我们正则化的训练目标同时考虑了通用知识和领域特定知识。通过经验费舍尔近似结合正则化和重要性估计，我们的方法期望进行剪枝，以保留对通用和领域特定知识重要的权重，从而保持通用性和特异性。这些重要性分数用于指导我们的剪枝决策。例如，如果我们将稀疏性水平设置为50%，那么每一层中重要性分数最小的50%的权重将被剪枝。
- en: 4 Experiment Setup
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验设置
- en: We evaluate D-Pruner on two knowledge-intensive domains, which are healthcare
    and legal. For model generality under domain-specific challenges, we evaluate
    the linguistic capability using domain text generation, and evaluate the multi-task
    solving capability on different domain tasks, i.e., natural language inference
    (NLI), question answering (QA), and summarization. Since we use domain datasets,
    the model specificity on domains can also be evaluated. In addition, we fine-tune
    the pruned model on domain datasets to further evaluate the generality and specificity.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个知识密集型领域（医疗保健和法律）上评估D-Pruner。为了在领域特定挑战下评估模型的通用性，我们使用领域文本生成来评估语言能力，并评估在不同领域任务上的多任务解决能力，即自然语言推理（NLI）、问答（QA）和总结。由于我们使用领域数据集，也可以评估模型在领域上的特异性。此外，我们还在领域数据集上对剪枝后的模型进行微调，以进一步评估其通用性和特异性。
- en: We evaluate D-Pruner on the LLaMA2 model family, which is the most used open-source
    LLM. We mainly apply our pruning method and baseline methods to LLaMA2-7B and
    LLaMA2-13B to show our results. Our method can also be easily applied to other
    LLMs with different sizes and architectures. For instance, Appendix [B](#A2 "Appendix
    B Experiments on BLOOM ‣ Pruning as a Domain-specific LLM Extractor") shows further
    experiment on BLOOM model (Le Scao et al., [2022](#bib.bib21)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在LLaMA2模型家族上评估D-Pruner，它是最常用的开源LLM。我们主要将我们的剪枝方法和基线方法应用于LLaMA2-7B和LLaMA2-13B，以展示我们的结果。我们的方法也可以轻松应用于不同大小和架构的其他LLM。例如，附录 [B](#A2
    "附录 B BLOOM的实验 ‣ 作为领域特定LLM提取器的剪枝") 显示了BLOOM模型（Le Scao等，[2022](#bib.bib21)）的进一步实验。
- en: 4.1 Iterative blocking
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 迭代阻塞
- en: Motivated by Frantar and Alistarh ([2023](#bib.bib11)), we perform experiments
    (in Table [2](#S4.T2 "Table 2 ‣ 4.2 Datasets and Evaluations ‣ 4 Experiment Setup
    ‣ Pruning as a Domain-specific LLM Extractor")) on D-Pruner with and without iterative
    blocking. Iterative blocking means to make pruning decisions for every fixed number
    ($B_{s}$ for those with more columns. Except Table [2](#S4.T2 "Table 2 ‣ 4.2 Datasets
    and Evaluations ‣ 4 Experiment Setup ‣ Pruning as a Domain-specific LLM Extractor"),
    D-Pruner in other tables does not adopt iterative blocking.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 受 Frantar 和 Alistarh ([2023](#bib.bib11)) 的启发，我们在表格 [2](#S4.T2 "Table 2 ‣ 4.2
    Datasets and Evaluations ‣ 4 Experiment Setup ‣ Pruning as a Domain-specific LLM
    Extractor") 中对 D-Pruner 进行了有无迭代阻塞的实验。迭代阻塞意味着在每个固定数量 ($B_{s}$ 对于那些列更多的情况) 做出修剪决策。除了表格
    [2](#S4.T2 "Table 2 ‣ 4.2 Datasets and Evaluations ‣ 4 Experiment Setup ‣ Pruning
    as a Domain-specific LLM Extractor") 外，其他表格中的 D-Pruner 并没有采用迭代阻塞。
- en: 4.2 Datasets and Evaluations
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 数据集与评估
- en: Datasets. Table [1](#S3.T1 "Table 1 ‣ 3.2 Updated Loss with Regularization ‣
    3 Methodology ‣ Pruning as a Domain-specific LLM Extractor") shows the details
    of each dataset that we used. Specifically, for healthcare, we select a medical
    textbook InternalMed_Harrison (Bigby, [1988](#bib.bib4)), MedNLI (Romanov and
    Shivade, [2018](#bib.bib32)), PubMedQA (Jin et al., [2019](#bib.bib18)), and Health
    Question Summarization (HQS) from the MEDIQA 2021 shared task 1 (Ben Abacha et al.,
    [2021](#bib.bib3); Ben Abacha and Demner-Fushman, [2019](#bib.bib2)) as domain
    datasets. For legal domain, we select MultiLegalPile (Niklaus et al., [2023](#bib.bib28)),
    CaseHOLD (Zheng et al., [2021](#bib.bib43)), and BillSum (Kornilova and Eidelman,
    [2019](#bib.bib20)). As for open-domain calibration data, we extract text from
    C4 dataset (Raffel et al., [2019](#bib.bib30)).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。表格 [1](#S3.T1 "Table 1 ‣ 3.2 Updated Loss with Regularization ‣ 3 Methodology
    ‣ Pruning as a Domain-specific LLM Extractor") 显示了我们使用的每个数据集的详细信息。具体来说，对于医疗领域，我们选择了医学教科书
    InternalMed_Harrison (Bigby, [1988](#bib.bib4))、MedNLI (Romanov 和 Shivade, [2018](#bib.bib32))、PubMedQA
    (Jin 等, [2019](#bib.bib18)) 和 Health Question Summarization (HQS) 来自 MEDIQA 2021
    共享任务 1 (Ben Abacha 等, [2021](#bib.bib3); Ben Abacha 和 Demner-Fushman, [2019](#bib.bib2))
    作为领域数据集。对于法律领域，我们选择了 MultiLegalPile (Niklaus 等, [2023](#bib.bib28))、CaseHOLD (Zheng
    等, [2021](#bib.bib43)) 和 BillSum (Kornilova 和 Eidelman, [2019](#bib.bib20))。至于开放领域校准数据，我们从
    C4 数据集 (Raffel 等, [2019](#bib.bib30)) 中提取文本。
- en: To construct our domain-specific calibration data, we select training instances
    from MedNLI, PubMedQA, and HQS at a ratio of 20%/60%/20% and from CaseHOLD and
    BillSum at a ratio of 50%/50%. These ratios are determined based on the difficulties
    and training sizes of these benchmarks. Both NLI and QA tasks that we adopt are
    asking models to perform classification. We experiment with different sizes of
    the domain-specific calibration dataset and find a size of 1000 achieves the best
    trade-off in terms of pruning efficiency and effectiveness for both domains. For
    model evaluation, besides using the test instances of those benchmarks, we leverage
    InternalMed_Harrison and MultiLegalPile for perplexity evaluation. 300 paragraphs
    are selected from each data source to form the test set of perplexity. Note that
    we use a subset of all the test examples of CaseHOLD and BillSum, since these
    two benchmarks are significantly larger in size and their individual instance
    tends to be longer.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的领域特定校准数据，我们从 MedNLI、PubMedQA 和 HQS 中按 20%/60%/20% 的比例选择训练实例，并从 CaseHOLD
    和 BillSum 中按 50%/50% 的比例选择。这些比例是基于这些基准的难度和训练规模确定的。我们采用的 NLI 和 QA 任务都是要求模型进行分类。我们尝试了不同大小的领域特定校准数据集，发现大小为
    1000 的数据集在修剪效率和效果方面为两个领域提供了最佳的折衷。为了评估模型，除了使用这些基准的测试实例外，我们还利用 InternalMed_Harrison
    和 MultiLegalPile 进行困惑度评估。每个数据源选择 300 段文本形成困惑度的测试集。请注意，我们使用了 CaseHOLD 和 BillSum
    的所有测试样本的一个子集，因为这两个基准的规模明显较大，且每个实例的长度通常较长。
- en: Evaluation Metrics. We first evaluate the linguistic capability of pruned models
    on InternalMed_Harrison and MultiLegalPile using perplexity. We then evaluate
    the multi-task solving capability and domain specificity on different domain tasks.
    Specifically, we choose accuracy metric for NLI task (MedNLI), macro-F1 for QA
    tasks (PubMedQA and CaseHOLD), and ROUGE scores (Lin, [2004](#bib.bib23)) for
    summarization tasks (HQS and BillSum).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。我们首先在 InternalMed_Harrison 和 MultiLegalPile 上使用困惑度评估修剪模型的语言能力。然后，我们评估多任务解决能力和领域特异性在不同领域任务上的表现。具体来说，我们为
    NLI 任务 (MedNLI) 选择准确率指标，为 QA 任务 (PubMedQA 和 CaseHOLD) 选择宏观 F1 指标，为总结任务 (HQS 和
    BillSum) 选择 ROUGE 分数 (Lin, [2004](#bib.bib23))。
- en: '| Model | Healthcare | Legal |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 医疗 | 法律 |'
- en: '| Perplexity | MedNLI | PubMedQA | R1 | R2 | RL | Perplexity | CaseHOLD | R1
    | R2 | RL |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Perplexity | MedNLI | PubMedQA | R1 | R2 | RL | Perplexity | CaseHOLD | R1
    | R2 | RL |'
- en: '|  | LLaMA2-7B |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA2-7B |'
- en: '| Dense | 5.49 | 37.62 | 23.77 | 22.51 | 7.18 | 19.50 | 2.26 | 28.82 | 32.64
    | 18.32 | 26.48 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Dense | 5.49 | 37.62 | 23.77 | 22.51 | 7.18 | 19.50 | 2.26 | 28.82 | 32.64
    | 18.32 | 26.48 |'
- en: '| Magnitude (Han et al., [2015](#bib.bib13)) | 16.08 | 33.90 | 28.29 | 9.60
    | 1.63 | 8.09 | 8.64 | 23.84 | 7.84 | 2.21 | 6.13 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 大小（Han et al., [2015](#bib.bib13)） | 16.08 | 33.90 | 28.29 | 9.60 | 1.63
    | 8.09 | 8.64 | 23.84 | 7.84 | 2.21 | 6.13 |'
- en: '| LLM-Pruner (Ma et al., [2023](#bib.bib26)) | 88.25 | 33.90 | 22.34 | 5.52
    | 0.30 | 5.45 | 32.22 | 13.59 | 6.76 | 0.72 | 5.40 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner (Ma et al., [2023](#bib.bib26)) | 88.25 | 33.90 | 22.34 | 5.52
    | 0.30 | 5.45 | 32.22 | 13.59 | 6.76 | 0.72 | 5.40 |'
- en: '| SparseGPT (Frantar and Alistarh, [2023](#bib.bib11)) | 6.39 | 33.47 | 36.22
    | 22.60 | 7.68 | 19.13 | 2.62 | 28.41 | 32.68 | 18.89 | 26.19 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT (Frantar 和 Alistarh, [2023](#bib.bib11)) | 6.39 | 33.47 | 36.22
    | 22.60 | 7.68 | 19.13 | 2.62 | 28.41 | 32.68 | 18.89 | 26.19 |'
- en: '| D-Pruner (w/ iterative blocking) | 7.07 | 34.53 | 45.38 | 24.72 | 8.87 |
    21.09 | 2.70 | 30.56 | 33.77 | 18.53 | 26.25 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| D-Pruner（含迭代阻塞） | 7.07 | 34.53 | 45.38 | 24.72 | 8.87 | 21.09 | 2.70 | 30.56
    | 33.77 | 18.53 | 26.25 |'
- en: '| D-Pruner (w/o iterative blocking) | 6.96 | 34.81 | 42.40 | 25.05 | 9.65 |
    22.34 | 2.72 | 26.14 | 32.14 | 18.42 | 26.14 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| D-Pruner（不含迭代阻塞） | 6.96 | 34.81 | 42.40 | 25.05 | 9.65 | 22.34 | 2.72 | 26.14
    | 32.14 | 18.42 | 26.14 |'
- en: '|  | LLaMA2-13B |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA2-13B |'
- en: '| Dense | 5.20 | 35.02 | 40.54 | 19.26 | 5.80 | 16.40 | 2.12 | 28.89 | 35.34
    | 21.19 | 27.82 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Dense | 5.20 | 35.02 | 40.54 | 19.26 | 5.80 | 16.40 | 2.12 | 28.89 | 35.34
    | 21.19 | 27.82 |'
- en: '| Magnitude (Han et al., [2015](#bib.bib13)) | 6.59 | 36.71 | 45.12 | 19.60
    | 5.01 | 16.33 | 2.81 | 21.95 | 29.90 | 16.94 | 24.51 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 大小（Han et al., [2015](#bib.bib13)） | 6.59 | 36.71 | 45.12 | 19.60 | 5.01
    | 16.33 | 2.81 | 21.95 | 29.90 | 16.94 | 24.51 |'
- en: '| LLM-Pruner (Ma et al., [2023](#bib.bib26)) | 23.95 | 34.39 | 17.37 | 7.60
    | 1.24 | 7.00 | 12.16 | 13.46 | 17.21 | 3.08 | 12.37 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner (Ma et al., [2023](#bib.bib26)) | 23.95 | 34.39 | 17.37 | 7.60
    | 1.24 | 7.00 | 12.16 | 13.46 | 17.21 | 3.08 | 12.37 |'
- en: '| SparseGPT (Frantar and Alistarh, [2023](#bib.bib11)) | 5.77 | 34.39 | 52.65
    | 22.25 | 8.35 | 19.19 | 2.39 | 28.62 | 33.68 | 19.35 | 27.60 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT (Frantar 和 Alistarh, [2023](#bib.bib11)) | 5.77 | 34.39 | 52.65
    | 22.25 | 8.35 | 19.19 | 2.39 | 28.62 | 33.68 | 19.35 | 27.60 |'
- en: '| D-Pruner (w/ iterative blocking) | 6.30 | 34.88 | 52.86 | 20.56 | 6.95 |
    17.85 | 2.40 | 28.30 | 33.83 | 20.51 | 27.56 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| D-Pruner（含迭代阻塞） | 6.30 | 34.88 | 52.86 | 20.56 | 6.95 | 17.85 | 2.40 | 28.30
    | 33.83 | 20.51 | 27.56 |'
- en: '| D-Pruner (w/o iterative blocking) | 6.16 | 35.16 | 50.87 | 23.99 | 7.78 |
    20.04 | 2.40 | 27.27 | 35.77 | 21.81 | 28.42 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| D-Pruner（不含迭代阻塞） | 6.16 | 35.16 | 50.87 | 23.99 | 7.78 | 20.04 | 2.40 | 27.27
    | 35.77 | 21.81 | 28.42 |'
- en: 'Table 2: Overall results when candidate models (at 50% sparsity) are tested
    on two domains. The best scores are in bold except the ones from the dense models.
    Note that the ROUGE scores reported in the healthcare domain correspond to HQS
    dataset while those in the legal domain correspond to BillSum. Perplexity in healthcare
    is tested on InternalMed_Harrison and perplexity in legal is tested on MultiLegalPile.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：当候选模型（在50%稀疏性下）在两个领域进行测试时的总体结果。最佳分数用粗体表示，密集模型的分数除外。注意，在医疗领域报告的ROUGE分数对应于HQS数据集，而法律领域的分数对应于BillSum。医疗领域的困惑度测试在InternalMed_Harrison上，法律领域的困惑度测试在MultiLegalPile上。
- en: 4.3 Baselines
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 基准
- en: We compare our method with a variety of LLM pruning baselines. All methods are
    applied to the same foundation model (either 7B of 13B of LLaMA2) for fair comparisons.
    As an ablation study, we also evaluate an unstructured pruning method using weight
    gradient by removing the regularization term in the training loss of D-Pruner.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的方法与多种LLM剪枝基准进行了比较。所有方法均应用于相同的基础模型（无论是7B还是13B的LLaMA2），以确保公平比较。作为一种消融研究，我们还评估了一种使用权重梯度的无结构剪枝方法，通过去除D-Pruner训练损失中的正则化项来实现。
- en: •
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Magnitude pruning prunes weights based on their magnitudes (Han et al., [2015](#bib.bib13)).
    We follow the standard practice of magnitude pruning on language models, where
    weights are compared layer-wise. Magnitude pruning is a simple and robust baseline
    that has been demonstrated to outperform many other pruning methods.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大小剪枝基于权重的大小进行剪枝（Han et al., [2015](#bib.bib13)）。我们遵循在语言模型上进行大小剪枝的标准做法，其中权重按层进行比较。大小剪枝是一种简单且稳健的基准，已被证明优于许多其他剪枝方法。
- en: •
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLM-Pruner is a structured pruning method using weight gradient to evaluate
    weight importance (Ma et al., [2023](#bib.bib26)). A calibration dataset is used
    for its gradient calculation, so we combine both open-domain (C4) and domain-specific
    calibration data when we use LLM-Pruner.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM-Pruner是一种结构化剪枝方法，使用权重梯度评估权重重要性（Ma et al., [2023](#bib.bib26)）。用于其梯度计算的校准数据集，因此我们在使用LLM-Pruner时结合了开放域（C4）和特定领域的校准数据。
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SparseGPT is an unstructured post-training pruning method (Frantar and Alistarh,
    [2023](#bib.bib11)). It uses an efficient weight update procedure that iterates
    between weight removal and weight update at each layer. It also uses a calibration
    dataset for approximation. Thus, similarly to D-Pruner and LLM-Pruner, we use
    open-domain and domain-specific calibration data for fair comparisons.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SparseGPT 是一种非结构化的后训练修剪方法（Frantar 和 Alistarh，[2023](#bib.bib11)）。它使用了一种高效的权重更新程序，在每一层之间进行权重移除和权重更新的迭代。它还使用校准数据集进行近似。因此，与
    D-Pruner 和 LLM-Pruner 类似，我们使用开放域和领域特定的校准数据进行公平比较。
- en: Moreover, for all the baseline methods, we continue to fine-tune their pruned
    models using LoRA (Hu et al., [2021](#bib.bib17)) on all the datasets together
    (NLI, QA, and summarization data combined) in each domain and then test the fine-tuned
    model on the datasets in Table [1](#S3.T1 "Table 1 ‣ 3.2 Updated Loss with Regularization
    ‣ 3 Methodology ‣ Pruning as a Domain-specific LLM Extractor"). We only use the
    default open-domain calibration dataset for the pruned models of LLM-Pruner and
    SparseGPT at this step, because these models will eventually undergo LoRA fine-tuning.
    Data instances of our fine-tuning dataset follow the Alpaca (Taori et al., [2023](#bib.bib36))
    template so that models are trained to predict the responses. Specifically, for
    healthcare, we have 7000, 7000, and 1000 training instances from MedNLI, PubMedQA,
    and HQS, respectively. For legal domain, we have 13000 training instances from
    CaseHOLD and 2000 from BillSum.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于所有基线方法，我们继续使用 LoRA（Hu 等，[2021](#bib.bib17)）在所有数据集（NLI、QA 和总结数据合并）上微调其修剪后的模型，然后在表
    [1](#S3.T1 "表 1 ‣ 3.2 更新的损失与正则化 ‣ 3 方法论 ‣ 作为领域特定 LLM 提取器的修剪") 中的数据集上测试微调后的模型。在此步骤中，我们仅使用默认的开放域校准数据集用于
    LLM-Pruner 和 SparseGPT 的修剪模型，因为这些模型最终会进行 LoRA 微调。我们的微调数据集数据实例遵循 Alpaca（Taori 等，[2023](#bib.bib36)）模板，以便模型被训练以预测响应。具体而言，对于医疗保健领域，我们有来自
    MedNLI、PubMedQA 和 HQS 的 7000、7000 和 1000 个训练实例。对于法律领域，我们有来自 CaseHOLD 的 13000 个训练实例和来自
    BillSum 的 2000 个训练实例。
- en: 4.4 Implementation Details
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 实现细节
- en: We perform prompt engineering in a zero-shot setting before prompting a series
    of models. The finalized prompt is kept the same across all candidate models on
    one task to ensure fairness. The hyperparameters used by different models are
    in Appendix [C](#A3 "Appendix C Hyperparameters ‣ Pruning as a Domain-specific
    LLM Extractor").
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在零-shot 设置下进行提示工程，然后对一系列模型进行提示。最终的提示在同一任务中的所有候选模型之间保持一致，以确保公平。不同模型使用的超参数见附录 [C](#A3
    "附录 C 超参数 ‣ 作为领域特定 LLM 提取器的修剪")。
- en: 5 Results and Analysis
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果与分析
- en: 'Our results and analysis aim to answer the following research questions:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果和分析旨在回答以下研究问题：
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ 1: How does D-Pruner compare against other pruning baselines ([5.1](#S5.SS1
    "5.1 Overall Results ‣ 5 Results and Analysis ‣ Pruning as a Domain-specific LLM
    Extractor"))?'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ 1: D-Pruner 与其他修剪基线方法相比如何（[5.1](#S5.SS1 "5.1 总体结果 ‣ 5 结果与分析 ‣ 作为领域特定 LLM
    提取器的修剪")）？'
- en: •
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ 2: What are the performance of all candidate models after LoRA fine-tuning
    ([5.2](#S5.SS2 "5.2 Performance After Fine-tuning ‣ 5 Results and Analysis ‣ Pruning
    as a Domain-specific LLM Extractor"))?'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ 2: 所有候选模型在 LoRA 微调后的表现如何（[5.2](#S5.SS2 "5.2 微调后的表现 ‣ 5 结果与分析 ‣ 作为领域特定 LLM
    提取器的修剪")）？'
- en: •
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ 3: As an important contribution of D-Pruner, is dual-pruning an effective
    method of compressing LLM ([5.1](#S5.SS1 "5.1 Overall Results ‣ 5 Results and
    Analysis ‣ Pruning as a Domain-specific LLM Extractor"), [5.3](#S5.SS3 "5.3 Ablation
    Study ‣ 5 Results and Analysis ‣ Pruning as a Domain-specific LLM Extractor"),
    and [5.5](#S5.SS5 "5.5 Mask Similarity ‣ 5 Results and Analysis ‣ Pruning as a
    Domain-specific LLM Extractor"))?'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ 3: 作为 D-Pruner 的一个重要贡献，双重修剪是否是一种有效的 LLM 压缩方法（[5.1](#S5.SS1 "5.1 总体结果 ‣ 5
    结果与分析 ‣ 作为领域特定 LLM 提取器的修剪")，[5.3](#S5.SS3 "5.3 消融研究 ‣ 5 结果与分析 ‣ 作为领域特定 LLM 提取器的修剪")
    和 [5.5](#S5.SS5 "5.5 掩码相似性 ‣ 5 结果与分析 ‣ 作为领域特定 LLM 提取器的修剪")）？'
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ 4: How does D-Pruner perform under different sparsity levels or different
    sizes of domain-specific calibration data ([5.4](#S5.SS4 "5.4 Effect of Sparsity
    and Domain Calibration Data ‣ 5 Results and Analysis ‣ Pruning as a Domain-specific
    LLM Extractor"))?'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ 4: D-Pruner 在不同稀疏水平或不同大小的领域特定校准数据下表现如何（[5.4](#S5.SS4 "5.4 稀疏性和领域校准数据的影响
    ‣ 5 结果与分析 ‣ 作为领域特定 LLM 提取器的修剪")）？'
- en: 5.1 Overall Results
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 总体结果
- en: Our overall results for the two domains are presented in Table [2](#S4.T2 "Table
    2 ‣ 4.2 Datasets and Evaluations ‣ 4 Experiment Setup ‣ Pruning as a Domain-specific
    LLM Extractor"). All models are pruned to 50% sparsity level except the dense
    one.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个领域的总体结果见表[2](#S4.T2 "Table 2 ‣ 4.2 Datasets and Evaluations ‣ 4 Experiment
    Setup ‣ Pruning as a Domain-specific LLM Extractor")。所有模型都剪枝到50%的稀疏度水平，除了密集模型。
- en: '| Model (Fine-tuned with LoRA) | Healthcare | Legal |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 模型（使用LoRA微调） | 医疗保健 | 法律 |'
- en: '| Perplexity | MedNLI | PubMedQA | R1 | R2 | RL | Perplexity | CaseHOLD | R1
    | R2 | RL |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度 | MedNLI | PubMedQA | R1 | R2 | RL | 困惑度 | CaseHOLD | R1 | R2 | RL |'
- en: '|  | LLaMA2-7B |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA2-7B |'
- en: '| Dense | 5.68 | 64.84 | 41.37 | 33.26 | 12.60 | 28.92 | 2.26 | 28.82 | 34.64
    | 20.47 | 28.33 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 密集 | 5.68 | 64.84 | 41.37 | 33.26 | 12.60 | 28.92 | 2.26 | 28.82 | 34.64
    | 20.47 | 28.33 |'
- en: '| Magnitude (Han et al., [2015](#bib.bib13)) | 8.39 | 62.59 | 23.71 | 32.02
    | 12.25 | 29.27 | 7.28 | 25.89 | 17.64 | 8.19 | 14.52 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude (Han et al., [2015](#bib.bib13)) | 8.39 | 62.59 | 23.71 | 32.02
    | 12.25 | 29.27 | 7.28 | 25.89 | 17.64 | 8.19 | 14.52 |'
- en: '| LLM-Pruner (Ma et al., [2023](#bib.bib26)) | 44.56 | 58.72 | 26.78 | 22.21
    | 6.12 | 20.57 | 215.13 | 14.37 | 7.97 | 0.78 | 6.68 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner (Ma et al., [2023](#bib.bib26)) | 44.56 | 58.72 | 26.78 | 22.21
    | 6.12 | 20.57 | 215.13 | 14.37 | 7.97 | 0.78 | 6.68 |'
- en: '| SparseGPT (Frantar and Alistarh, [2023](#bib.bib11)) | 6.44 | 68.85 | 27.37
    | 28.97 | 11.27 | 25.93 | 2.86 | 27.31 | 27.79 | 17.55 | 23.74 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT (Frantar and Alistarh, [2023](#bib.bib11)) | 6.44 | 68.85 | 27.37
    | 28.97 | 11.27 | 25.93 | 2.86 | 27.31 | 27.79 | 17.55 | 23.74 |'
- en: '| D-Pruner | 6.74 | 61.88 | 32.58 | 36.49 | 13.71 | 31.85 | 2.73 | 27.58 |
    31.00 | 19.03 | 25.96 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| D-Pruner | 6.74 | 61.88 | 32.58 | 36.49 | 13.71 | 31.85 | 2.73 | 27.58 |
    31.00 | 19.03 | 25.96 |'
- en: 'Table 3: Results of fine-tuned candidates models at 50% sparsity. LoRA fine-tuning
    is conducted on D-Pruner without iterative blocking.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在50%稀疏度下微调候选模型的结果。LoRA微调在D-Pruner上进行，没有进行迭代阻塞。
- en: Improvement on NLI and QA
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NLI和QA的改进
- en: D-Pruner delivers consistent score improvement on NLI and QA tasks when it is
    compared against baselines based on LLaMA2-7B and LLaMA2-13B. With two exceptions,
    variants of D-Pruner based on the inclusion and exclusion of iterative blocking
    outperform baselines on 4 out of 6 cases when classification is performed (MedNLI,
    PubMedQA, and CaseHOLD on both 7B and 13B LLaMA2) in Table [2](#S4.T2 "Table 2
    ‣ 4.2 Datasets and Evaluations ‣ 4 Experiment Setup ‣ Pruning as a Domain-specific
    LLM Extractor"). It is clear to see that magnitude pruning and SparseGPT are generally
    stronger models than LLM-Pruner. The dense model sometimes has worse scores than
    others across 7B and 13B LLaMA2, which indicates that scaling parameters of a
    pre-trained language model does not necessarily increase the performance on a
    single benchmark on NLI and QA. We can see that iterative blocking generally yields
    better scores on these classification tasks such as reaching 30.56 F1 score on
    CaseHOLD based on LLaMA2-7B, which is a significant improvement over baselines
    and D-Pruner without it. Thus, we recommend to adopt iterative blocking on the
    classification tasks when strong domain knowledge is required.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于LLaMA2-7B和LLaMA2-13B的基线模型相比，D-Pruner在NLI和QA任务上提供了一致的得分改进。除了两个例外，基于迭代阻塞的D-Pruner变体在6个分类任务中的4个任务（MedNLI、PubMedQA、以及LLaMA2的7B和13B的CaseHOLD）中超越了基线。可以明显看出，Magnitude剪枝和SparseGPT通常是比LLM-Pruner更强的模型。密集模型在7B和13B
    LLaMA2中有时得分低于其他模型，这表明扩大预训练语言模型的参数不一定会提高在NLI和QA单一基准上的性能。我们可以看到，迭代阻塞通常在这些分类任务中产生更好的得分，例如在LLaMA2-7B上达到30.56的F1得分，比基线和没有它的D-Pruner有显著改进。因此，我们建议在需要强大领域知识的分类任务中采用迭代阻塞。
- en: Improvement on Summarization
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 摘要改进
- en: D-Pruner presents the strongest summarization performance. The most exciting
    thing is that its ROUGE scores are mostly higher than the dense ones. We notice
    the top summarization performance of LLaMA2-13B-based models on HQS is lower than
    that of LLaMA2-7B-based models, which is counterintuitive. According to the state-of-the-art
    of HQS Zhang et al. ([2023](#bib.bib41)); He et al. ([2021](#bib.bib15)), we find
    that D-Pruner is close to the best ROUGE scores produced by single systems, so
    we consider that this dataset is relatively simple. Thus, our LLaMA2-7B-based
    models seem to find an upper limit of ROUGE given the existing reference summaries,
    so going from 7B to 13B incurs a small performance degradation on dense model,
    SparseGPT, and D-Pruner. The strong summarization performance of D-Pruner on both
    domains demonstrates its usability as an efficient and domain-specific language
    model. As for iterative blocking, D-Pruner without it generally has better perplexity
    and summarization performance. However, considering the exception in the legal
    domain based on LLaMA2-7B, we recommend to check perplexity scores on the validation
    data when deciding whether to use iterative blocking for perplexity and summarization
    assessment.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**D-Pruner** 展现了最强的摘要性能。最令人兴奋的是，它的 ROUGE 分数大多高于密集模型。我们注意到，基于 LLaMA2-13B 的模型在
    HQS 上的顶级摘要性能低于基于 LLaMA2-7B 的模型，这与直觉相反。根据 HQS 的最新研究（Zhang et al. ([2023](#bib.bib41));
    He et al. ([2021](#bib.bib15))），我们发现 D-Pruner 接近单一系统产生的最佳 ROUGE 分数，因此我们认为这个数据集相对简单。因此，我们的
    LLaMA2-7B 基模型似乎发现了在现有参考摘要下 ROUGE 的上限，因此从 7B 升级到 13B 会导致密集模型、SparseGPT 和 D-Pruner
    的小幅性能下降。D-Pruner 在两个领域的强大摘要性能展示了其作为高效和领域特定语言模型的可用性。至于迭代阻塞，通常情况下没有迭代阻塞的 D-Pruner
    具有更好的困惑度和摘要性能。然而，考虑到基于 LLaMA2-7B 的法律领域中的例外，我们建议在决定是否使用迭代阻塞进行困惑度和摘要评估时检查验证数据上的困惑度分数。'
- en: Improvement on Perplexity
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对困惑度的改进
- en: D-Pruner has the second best perplexity scores on healthcare and legal domains
    across 7B and 13B LLaMA2\. These scores reflect the strong linguistic capabilities
    of SparseGPT and D-Pruner when they encounter knowledge-intensive domains. D-Pruner
    does not surpass SparseGPT on perplexity metric, and the reason might come from
    the fine-tuning pipeline (Lv et al., [2023](#bib.bib25)) we use. Lv et al. ([2023](#bib.bib25))
    is a full-parameter fine-tuning pipeline that aims towards GPU memory efficiency,
    so its effectiveness on a specific metric might be compromised. Moreover, we suspect
    that the data we use from InternalMed_Harrison and MultiLegalPile may be closer
    to the general domain both semantically and syntactically. Since SparseGPT prunes
    LLM mainly based on generality, it has better perplexity scores than ours.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**D-Pruner** 在 7B 和 13B LLaMA2 的医疗和法律领域中具有第二好的困惑度分数。这些分数反映了 SparseGPT 和 D-Pruner
    在遇到知识密集型领域时的强大语言能力。**D-Pruner** 在困惑度指标上并未超越 SparseGPT，原因可能来自我们使用的微调流程（Lv et al.,
    [2023](#bib.bib25)）。Lv et al. ([2023](#bib.bib25)) 是一个全参数微调流程，旨在提高 GPU 内存效率，因此在特定指标上的效果可能会受到影响。此外，我们怀疑我们使用的
    InternalMed_Harrison 和 MultiLegalPile 数据在语义和句法上可能更接近通用领域。由于 SparseGPT 主要基于通用性来修剪
    LLM，它在困惑度分数上优于我们的模型。'
- en: 5.2 Performance After Fine-tuning
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 微调后的性能
- en: Table [3](#S5.T3 "Table 3 ‣ 5.1 Overall Results ‣ 5 Results and Analysis ‣ Pruning
    as a Domain-specific LLM Extractor") shows the results of fine-tuned candidate
    models at 50% sparsity. Similar to the performance discussed above, D-Pruner always
    delivers the best summarization scores and mostly presents the best classification
    results after fine-tuning, which demonstrates that fine-tuning can further improve
    the pruning performance of our method. For most models, macro-F1 on PubMedQA decreases
    after fine-tuning, because this test set is imbalanced and models mostly learn
    to predict the majority class labels. In fact, the accuracies of most models on
    PubMedQA increase after fine-tuning as shown in Appendix [A](#A1 "Appendix A Accuracy
    Scores on PubMedQA ‣ Pruning as a Domain-specific LLM Extractor"), so this fine-tuning
    method still makes a difference. We also do not see too much score improvement
    for many models on CaseHOLD, since it is a quite challenging task for our experiment
    setting (e.g., we combine only a small subset of original training data for each
    task and perform multi-task fine-tuning as discussed in Section [4](#S4 "4 Experiment
    Setup ‣ Pruning as a Domain-specific LLM Extractor")).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表[3](#S5.T3 "Table 3 ‣ 5.1 Overall Results ‣ 5 Results and Analysis ‣ Pruning
    as a Domain-specific LLM Extractor")展示了在50%稀疏度下微调的候选模型的结果。与上述讨论的性能类似，D-Pruner总是提供最佳的总结评分，并且在微调后大多数情况下呈现最佳分类结果，这表明微调可以进一步提高我们方法的剪枝性能。对于大多数模型，PubMedQA上的宏观F1在微调后下降，因为这个测试集不平衡，模型大多学习预测主要类别标签。实际上，如附录[A](#A1
    "Appendix A Accuracy Scores on PubMedQA ‣ Pruning as a Domain-specific LLM Extractor")所示，大多数模型在PubMedQA上的准确率在微调后有所提高，因此这种微调方法仍然有效。我们也没有看到许多模型在CaseHOLD上的评分改进，因为这是我们实验设置下一个相当具有挑战性的任务（例如，我们仅结合了每个任务的一小部分原始训练数据，并执行了如第[4](#S4
    "4 Experiment Setup ‣ Pruning as a Domain-specific LLM Extractor")节中讨论的多任务微调）。
- en: 5.3 Ablation Study
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 剪枝研究
- en: In Table [4](#S5.T4 "Table 4 ‣ 5.3 Ablation Study ‣ 5 Results and Analysis ‣
    Pruning as a Domain-specific LLM Extractor"), we show that pruning without integrating
    general domain importance as a regularization term yields suboptimal performance.
    In other words, this means to remove the consideration of generality. We find
    perplexities in both domains are higher than pruning with regularization. This
    demonstrates that our dual pruning mechanism that considers both generality and
    specificity is able to improve model performance.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[4](#S5.T4 "Table 4 ‣ 5.3 Ablation Study ‣ 5 Results and Analysis ‣ Pruning
    as a Domain-specific LLM Extractor")中，我们展示了不将通用领域重要性作为正则化项进行剪枝的效果不佳。换句话说，这意味着忽略了通用性的考虑。我们发现两个领域的困惑度均高于使用正则化的剪枝。这表明，我们的双重剪枝机制，考虑了通用性和特异性，能够提高模型性能。
- en: '| Model | Healthcare perplexity | Legal perplexity |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 医疗困惑度 | 法律困惑度 |'
- en: '| no regularization | 7.23 | 2.82 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 无正则化 | 7.23 | 2.82 |'
- en: '| D-Pruner | 6.96 | 2.72 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| D-Pruner | 6.96 | 2.72 |'
- en: 'Table 4: Results of removing the regularization.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：去除正则化的结果。
- en: '| Sparsity | Healthcare perplexity | Legal perplexity |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏度 | 医疗困惑度 | 法律困惑度 |'
- en: '| 10% | 5.49 | 2.26 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 10% | 5.49 | 2.26 |'
- en: '| 20% | 5.52 | 2.27 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 20% | 5.52 | 2.27 |'
- en: '| 30% | 5.61 | 2.31 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 5.61 | 2.31 |'
- en: '| 40% | 5.91 | 2.42 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 40% | 5.91 | 2.42 |'
- en: '| 50% | 6.96 | 2.72 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 6.96 | 2.72 |'
- en: '| 60% | 15.19 | 4.59 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 60% | 15.19 | 4.59 |'
- en: '| 70% | 223.63 | 84.25 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 70% | 223.63 | 84.25 |'
- en: 'Table 5: Results of changing sparsities on D-Pruner.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：改变D-Pruner稀疏度的结果。
- en: '| # samples | Healthcare perplexity | Legal perplexity |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 样本数量 | 医疗困惑度 | 法律困惑度 |'
- en: '| 100 | 8.18 | 3.34 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 8.18 | 3.34 |'
- en: '| 500 | 7.15 | 2.97 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 500 | 7.15 | 2.97 |'
- en: '| 1000 | 6.96 | 2.72 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 6.96 | 2.72 |'
- en: '| 1500 | 7.96 | 2.70 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 1500 | 7.96 | 2.70 |'
- en: 'Table 6: Results of trying different sizes of domain-specific calibration data.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：尝试不同大小的领域特定校准数据的结果。
- en: 5.4 Effect of Sparsity and Domain Calibration Data
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 稀疏度和领域校准数据的影响
- en: In Table [5](#S5.T5 "Table 5 ‣ 5.3 Ablation Study ‣ 5 Results and Analysis ‣
    Pruning as a Domain-specific LLM Extractor"), it is clear that perplexity keeps
    increasing when D-Pruner becomes more sparse, which is expected. Since 50% sparsity
    is a good balance between sparsity and performance, we select it to report our
    performance in Table [2](#S4.T2 "Table 2 ‣ 4.2 Datasets and Evaluations ‣ 4 Experiment
    Setup ‣ Pruning as a Domain-specific LLM Extractor") and [3](#S5.T3 "Table 3 ‣
    5.1 Overall Results ‣ 5 Results and Analysis ‣ Pruning as a Domain-specific LLM
    Extractor").
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[5](#S5.T5 "Table 5 ‣ 5.3 Ablation Study ‣ 5 Results and Analysis ‣ Pruning
    as a Domain-specific LLM Extractor")中，可以清楚地看到，当D-Pruner变得更稀疏时，困惑度不断增加，这是预期的。由于50%的稀疏性是在稀疏性和性能之间的良好平衡，我们选择它来报告表[2](#S4.T2
    "Table 2 ‣ 4.2 Datasets and Evaluations ‣ 4 Experiment Setup ‣ Pruning as a Domain-specific
    LLM Extractor")和[3](#S5.T3 "Table 3 ‣ 5.1 Overall Results ‣ 5 Results and Analysis
    ‣ Pruning as a Domain-specific LLM Extractor")中的性能。
- en: Based on Table [6](#S5.T6 "Table 6 ‣ 5.3 Ablation Study ‣ 5 Results and Analysis
    ‣ Pruning as a Domain-specific LLM Extractor"), we believe setting the size of
    domain-specific calibration data to 1000 is reasonable. As the last row shows,
    increasing its size does not always guarantee a performance improvement.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 基于表[6](#S5.T6 "Table 6 ‣ 5.3 Ablation Study ‣ 5 Results and Analysis ‣ Pruning
    as a Domain-specific LLM Extractor")，我们认为将领域特定的校准数据大小设置为1000是合理的。正如最后一行所示，增大数据大小并不总是能保证性能提升。
- en: '![Refer to caption](img/b539b510c5669a19fd83904a4350f606.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b539b510c5669a19fd83904a4350f606.png)'
- en: (a) Open-domain vs healthcare domain.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 开放领域与医疗领域。
- en: '![Refer to caption](img/66a67fc9879fece27540913ef8c4f81f.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/66a67fc9879fece27540913ef8c4f81f.png)'
- en: (b) Healthcare domain vs legal domain.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 医疗领域与法律领域。
- en: 'Figure 2: Illustration of mask similarity. It shows that masks for different
    domains are quite different. The self-attention modules contribute more to specificity,
    and MLP modules store knowledge that is shared by different domains.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：掩码相似性的说明。它显示了不同领域的掩码差异很大。自注意力模块对特异性贡献更大，而MLP模块存储了不同领域共享的知识。
- en: 5.5 Mask Similarity
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 掩码相似性
- en: To better understand the pruned model on different domains, we compare the similarity
    of the pruning masks. In our study on LLaMA2-7B, each generated mask contains
    7*32 matrices for 32 layers and 7 projection matrices in the self-attention module
    (q, k, v, o) and MLP module (down, up, gate) in each layer. For each matrix, we
    calculate the similarity as the number of shared “1” elements (“1” means weights
    not pruned) in the two masks divided by the matrix size. Note all the masks are
    generated in 50% sparsity.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解不同领域的剪枝模型，我们比较了剪枝掩码的相似性。在我们对LLaMA2-7B的研究中，每个生成的掩码包含32层的7*32矩阵和自注意力模块（q,
    k, v, o）以及每层MLP模块（down, up, gate）中的7个投影矩阵。对于每个矩阵，我们计算相似性，即两个掩码中共享的“1”元素的数量（“1”表示未剪枝的权重）除以矩阵大小。注意所有的掩码都是在50%的稀疏性下生成的。
- en: Figure [2](#S5.F2 "Figure 2 ‣ 5.4 Effect of Sparsity and Domain Calibration
    Data ‣ 5 Results and Analysis ‣ Pruning as a Domain-specific LLM Extractor") (a)
    shows the mask similarity between the open-domain and healthcare domain, and [2](#S5.F2
    "Figure 2 ‣ 5.4 Effect of Sparsity and Domain Calibration Data ‣ 5 Results and
    Analysis ‣ Pruning as a Domain-specific LLM Extractor") (b) shows the mask similarity
    between the healthcare domain and legal domain. The results show that the masks
    are quite different, with shared elements as low as 35%. Generally, the self-attention
    modules share fewer elements than the MLP modules. This means self-attention modules
    contribute more to specificity, and MLP modules store knowledge that is shared
    by different domains.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S5.F2 "Figure 2 ‣ 5.4 Effect of Sparsity and Domain Calibration Data ‣
    5 Results and Analysis ‣ Pruning as a Domain-specific LLM Extractor") (a) 显示了开放领域与医疗领域之间的掩码相似性，[2](#S5.F2
    "Figure 2 ‣ 5.4 Effect of Sparsity and Domain Calibration Data ‣ 5 Results and
    Analysis ‣ Pruning as a Domain-specific LLM Extractor") (b) 显示了医疗领域与法律领域之间的掩码相似性。结果表明，掩码差异很大，共享元素低至35%。一般来说，自注意力模块共享的元素少于MLP模块。这意味着自注意力模块对特异性贡献更大，而MLP模块存储了不同领域共享的知识。
- en: 6 Conclusion
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We introduce D-Pruner, an innovative unstructured dual-pruning method for domain-specific
    compression on LLM. It is able to extract a compressed, domain-specific, and task-agnostic
    LLM by identifying weights that are pivotal for both generality and specificity.
    More specifically, the general weight importance is first assessed by quantifying
    the error incurred upon their removal with the help of open-domain calibration
    data. Then, we utilize this general weight importance to refine our training loss,
    so that it considers generality when fitting into a specific domain. Moreover,
    by efficiently approximating weight importance with the refined training loss
    on a domain-specific calibration dataset, we obtain a pruned model emphasizing
    general capabilities and domain-specific knowledge. Our comprehensive experiments
    across various tasks in different domains show the effectiveness of D-Pruner in
    domain-specific pruning.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 D-Pruner，一种创新的无结构双重修剪方法，用于 LLM 的领域特定压缩。它通过识别对通用性和特异性至关重要的权重，能够提取一个压缩的、领域特定的、任务无关的
    LLM。更具体地说，首先通过量化去除权重所产生的误差来评估一般权重的重要性，利用开放领域的校准数据。然后，我们利用这种一般权重的重要性来优化我们的训练损失，以便在适应特定领域时考虑通用性。此外，通过在领域特定的校准数据集上用优化后的训练损失高效地近似权重重要性，我们获得了一个强调通用能力和领域特定知识的稀疏模型。我们在不同领域的各种任务上的综合实验显示了
    D-Pruner 在领域特定修剪中的有效性。
- en: Limitations
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: Although D-Pruner presents strong performance in Section [5](#S5 "5 Results
    and Analysis ‣ Pruning as a Domain-specific LLM Extractor"), many of its perplexity
    scores reach the second place in healthcare and legal domains (dense model is
    not counted here). Further improving this perplexity is a valuable extension of
    this paper.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 D-Pruner 在第[5](#S5 "5 Results and Analysis ‣ Pruning as a Domain-specific
    LLM Extractor)节表现出色，但在医疗和法律领域中，许多困惑度得分仍然排名第二（此处不计密集模型）。进一步改善这些困惑度是本文的一个有价值的扩展。
- en: Another limitation of this work is that D-Pruner is more memory-intensive than
    SparseGPT during pruning, since D-Pruner is based on full-parameter fine-tuning
    and SparseGPT does not leverage global gradient information. D-Pruner sets similar
    memory requirement as LLM-Pruner. As a trade-off, D-Pruner reaches better performance
    on most of the metrics. It is also more flexible, since it computes matrices of
    importance scores without actually sparsifying LLMs. Therefore, researchers can
    make real-time decisions about the desired sparsity level, and changing the sparsity
    is very efficient.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 该工作的另一个限制是，与 SparseGPT 相比，D-Pruner 在修剪过程中更消耗内存，因为 D-Pruner 基于全参数微调，而 SparseGPT
    不利用全局梯度信息。D-Pruner 设置的内存需求与 LLM-Pruner 相似。作为权衡，D-Pruner 在大多数指标上表现更佳。它还更灵活，因为它计算重要性分数的矩阵，而无需实际稀疏化
    LLM。因此，研究人员可以实时决定所需的稀疏级别，而且改变稀疏度非常高效。
- en: Acknowledgments
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Yusen Zhang, Sarkar Snigdha Sarathi Das, Ranran Haoran Zhang, Xiaoxin
    Lu, and Ryo Kamoi for the valuable discussions and comments. We also would like
    to thank the anonymous reviewers for their helpful comments.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Yusen Zhang、Sarkar Snigdha Sarathi Das、Ranran Haoran Zhang、Xiaoxin Lu 和
    Ryo Kamoi 的宝贵讨论和评论。我们还要感谢匿名审稿人提出的有益意见。
- en: References
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bai et al. (2024) Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu,
    Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, et al. 2024. Beyond
    efficiency: A systematic survey of resource-efficient large language models. *arXiv
    preprint arXiv:2401.00625*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人（2024）广纪白、郑钗、陈灵、石瑜王、佳颖路、南张、廷伟石、紫阳于、梦丹朱、怡飞张等人。2024。超越效率：资源高效大语言模型的系统性调查。*arXiv
    预印本 arXiv:2401.00625*。
- en: Ben Abacha and Demner-Fushman (2019) Asma Ben Abacha and Dina Demner-Fushman.
    2019. [On the summarization of consumer health questions](https://doi.org/10.18653/v1/P19-1215).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 2228–2234, Florence, Italy. Association for Computational
    Linguistics.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ben Abacha 和 Demner-Fushman（2019）阿斯玛·本·阿巴查和迪娜·德门纳-富什曼。2019。[消费者健康问题的摘要](https://doi.org/10.18653/v1/P19-1215)。在*第57届计算语言学协会年会论文集*中，第2228–2234页，意大利佛罗伦萨。计算语言学协会。
- en: Ben Abacha et al. (2021) Asma Ben Abacha, Yassine Mrabet, Yuhao Zhang, Chaitanya
    Shivade, Curtis Langlotz, and Dina Demner-Fushman. 2021. [Overview of the MEDIQA
    2021 shared task on summarization in the medical domain](https://doi.org/10.18653/v1/2021.bionlp-1.8).
    In *Proceedings of the 20th Workshop on Biomedical Language Processing*, pages
    74–85, Online. Association for Computational Linguistics.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ben Abacha et al. (2021) Asma Ben Abacha, Yassine Mrabet, Yuhao Zhang, Chaitanya
    Shivade, Curtis Langlotz, 和 Dina Demner-Fushman. 2021. [MEDIQA 2021 医疗领域摘要共享任务概述](https://doi.org/10.18653/v1/2021.bionlp-1.8)。在*第20届生物医学语言处理研讨会论文集*，第74–85页，在线。计算语言学协会。
- en: Bigby (1988) JudyAnn Bigby. 1988. Harrison’s principles of internal medicine.
    *Archives of Dermatology*, 124(2):287–287.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bigby (1988) JudyAnn Bigby. 1988. 哈里森内科学原理。*皮肤病学档案*，124(2):287–287。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等。2020. 语言模型是少量样本学习者。*神经信息处理系统进展*，33:1877–1901。
- en: 'Deng et al. (2020) Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie.
    2020. Model compression and hardware acceleration for neural networks: A comprehensive
    survey. *Proceedings of the IEEE*, 108(4):485–532.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng et al. (2020) Lei Deng, Guoqi Li, Song Han, Luping Shi, 和 Yuan Xie. 2020.
    神经网络的模型压缩与硬件加速：一项综合调查。*IEEE 期刊论文集*，108(4):485–532。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    2022. Llm. int8 (): 规模化变换器的8位矩阵乘法。*arXiv 预印本 arXiv:2208.07339*。'
- en: Dettmers et al. (2021) Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer.
    2021. 8-bit optimizers via block-wise quantization. *arXiv preprint arXiv:2110.02861*.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2021) Tim Dettmers, Mike Lewis, Sam Shleifer, 和 Luke Zettlemoyer.
    2021. 通过块级量化的8位优化器。*arXiv 预印本 arXiv:2110.02861*。
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. *arXiv preprint
    arXiv:2305.14314*.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke
    Zettlemoyer. 2023. Qlora：高效的量化LLM微调。*arXiv 预印本 arXiv:2305.14314*。
- en: Du et al. (2021) Mengnan Du, Subhabrata Mukherjee, Yu Cheng, Milad Shokouhi,
    Xia Hu, and Ahmed Hassan Awadallah. 2021. Robustness challenges in model distillation
    and pruning for natural language understanding. *arXiv preprint arXiv:2110.08419*.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. (2021) Mengnan Du, Subhabrata Mukherjee, Yu Cheng, Milad Shokouhi,
    Xia Hu, 和 Ahmed Hassan Awadallah. 2021. 模型蒸馏和剪枝在自然语言理解中的鲁棒性挑战。*arXiv 预印本 arXiv:2110.08419*。
- en: 'Frantar and Alistarh (2023) Elias Frantar and Dan Alistarh. 2023. Sparsegpt:
    Massive language models can be accurately pruned in one-shot. In *International
    Conference on Machine Learning*, pages 10323–10337\. PMLR.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 和 Alistarh (2023) Elias Frantar 和 Dan Alistarh. 2023. Sparsegpt：大规模语言模型可以一次性准确剪枝。
    在*国际机器学习会议*，第10323–10337页。PMLR。
- en: Guo et al. (2020) Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha,
    Felix Chern, and Sanjiv Kumar. 2020. Accelerating large-scale inference with anisotropic
    vector quantization. In *International Conference on Machine Learning*, pages
    3887–3896\. PMLR.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2020) Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha,
    Felix Chern, 和 Sanjiv Kumar. 2020. 使用各向异性向量量化加速大规模推理。 在*国际机器学习会议*，第3887–3896页。PMLR。
- en: Han et al. (2015) Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning
    both weights and connections for efficient neural network. *Advances in neural
    information processing systems*, 28.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2015) Song Han, Jeff Pool, John Tran, 和 William Dally. 2015. 学习权重和连接以提高神经网络效率。*神经信息处理系统进展*，28。
- en: Hassibi et al. (1993) Babak Hassibi, David G Stork, and Gregory J Wolff. 1993.
    Optimal brain surgeon and general network pruning. In *IEEE international conference
    on neural networks*, pages 293–299\. IEEE.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi et al. (1993) Babak Hassibi, David G Stork, 和 Gregory J Wolff. 1993.
    最优脑外科医生和通用网络剪枝。 在*IEEE 国际神经网络会议*，第293–299页。IEEE。
- en: 'He et al. (2021) Yifan He, Mosha Chen, and Songfang Huang. 2021. [damo_nlp
    at MEDIQA 2021: Knowledge-based preprocessing and coverage-oriented reranking
    for medical question summarization](https://doi.org/10.18653/v1/2021.bionlp-1.12).
    In *Proceedings of the 20th Workshop on Biomedical Language Processing*, pages
    112–118, Online. Association for Computational Linguistics.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等（2021）Yifan He、Mosha Chen 和 Songfang Huang。2021年。[damo_nlp 在 MEDIQA 2021:
    基于知识的预处理和覆盖导向的医疗问题总结重排序](https://doi.org/10.18653/v1/2021.bionlp-1.12)。收录于 *第20届生物医学语言处理研讨会*，页码
    112–118，在线。计算语言学协会。'
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等（2015）Geoffrey Hinton、Oriol Vinyals 和 Jeff Dean。2015年。提取神经网络中的知识。*arXiv
    预印本 arXiv:1503.02531*。
- en: 'Hu et al. (2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large
    language models. In *International Conference on Learning Representations*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等（2021）Edward J Hu、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi Li、Shean Wang、Lu
    Wang、Weizhu Chen 等。2021年。Lora: 大型语言模型的低秩适配。收录于 *国际学习表示会议*。'
- en: 'Jin et al. (2019) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and
    Xinghua Lu. 2019. [PubMedQA: A dataset for biomedical research question answering](https://doi.org/10.18653/v1/D19-1259).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 2567–2577, Hong Kong, China. Association for Computational
    Linguistics.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin 等（2019）Qiao Jin、Bhuwan Dhingra、Zhengping Liu、William Cohen 和 Xinghua Lu。2019年。[PubMedQA:
    用于生物医学研究问题回答的数据集](https://doi.org/10.18653/v1/D19-1259)。收录于 *2019年自然语言处理经验方法会议暨第九届国际联合自然语言处理会议（EMNLP-IJCNLP）*，页码
    2567–2577，香港，中国。计算语言学协会。'
- en: 'Kenton and Toutanova (2019) Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina
    Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of NAACL-HLT*, pages 4171–4186.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kenton 和 Toutanova（2019）Jacob Devlin Ming-Wei Chang Kenton 和 Lee Kristina Toutanova。2019年。Bert:
    深度双向变换器的语言理解预训练。收录于 *NAACL-HLT 会议论文集*，页码 4171–4186。'
- en: 'Kornilova and Eidelman (2019) Anastassia Kornilova and Vladimir Eidelman. 2019.
    [BillSum: A corpus for automatic summarization of US legislation](https://doi.org/10.18653/v1/D19-5406).
    In *Proceedings of the 2nd Workshop on New Frontiers in Summarization*, pages
    48–56, Hong Kong, China. Association for Computational Linguistics.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kornilova 和 Eidelman（2019）Anastassia Kornilova 和 Vladimir Eidelman。2019年。[BillSum:
    用于自动总结美国立法的语料库](https://doi.org/10.18653/v1/D19-5406)。收录于 *第二届总结新前沿研讨会*，页码 48–56，香港，中国。计算语言学协会。'
- en: 'Le Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual
    language model.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Le Scao 等（2022）Teven Le Scao、Angela Fan、Christopher Akiki、Ellie Pavlick、Suzana
    Ilić、Daniel Hesslow、Roman Castagné、Alexandra Sasha Luccioni、François Yvon、Matthias
    Gallé 等。2022年。Bloom: 一个176b参数的开放获取多语言模型。'
- en: LeCun et al. (1989) Yann LeCun, John Denker, and Sara Solla. 1989. Optimal brain
    damage. *Advances in neural information processing systems*, 2.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等（1989）Yann LeCun、John Denker 和 Sara Solla。1989年。最佳大脑损伤。*神经信息处理系统进展*，2。
- en: 'Lin (2004) Chin-Yew Lin. 2004. [ROUGE: A package for automatic evaluation of
    summaries](https://aclanthology.org/W04-1013). In *Text Summarization Branches
    Out*, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin（2004）Chin-Yew Lin。2004年。[ROUGE: 一个用于自动评估摘要的包](https://aclanthology.org/W04-1013)。收录于
    *文本摘要拓展*，页码 74–81，巴塞罗那，西班牙。计算语言学协会。'
- en: Liu et al. (2021) Hanxiao Liu, Zihang Dai, David So, and Quoc V Le. 2021. Pay
    attention to mlps. *Advances in Neural Information Processing Systems*, 34:9204–9215.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2021）Hanxiao Liu、Zihang Dai、David So 和 Quoc V Le。2021年。关注多层感知器。*神经信息处理系统进展*，34:9204–9215。
- en: Lv et al. (2023) Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo,
    and Xipeng Qiu. 2023. Full parameter fine-tuning for large language models with
    limited resources. *arXiv preprint arXiv:2306.09782*.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lv 等（2023）Kai Lv、Yuqing Yang、Tengxiao Liu、Qinghui Gao、Qipeng Guo 和 Xipeng Qiu。2023年。针对资源有限的大型语言模型的全参数微调。*arXiv
    预印本 arXiv:2306.09782*。
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner:
    On the structural pruning of large language models. *arXiv preprint arXiv:2305.11627*.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等（2023）Xinyin Ma、Gongfan Fang 和 Xinchao Wang。2023年。Llm-pruner: 大型语言模型的结构化剪枝。*arXiv
    预印本 arXiv:2305.11627*。'
- en: Martens (2020) James Martens. 2020. New insights and perspectives on the natural
    gradient method. *The Journal of Machine Learning Research*, 21(1):5776–5851.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martens（2020）詹姆斯·马滕斯。2020年。关于自然梯度方法的新见解与视角。*机器学习研究杂志*，21（1）：5776–5851。
- en: 'Niklaus et al. (2023) Joel Niklaus, Veton Matoshi, Matthias Stürmer, Ilias
    Chalkidis, and Daniel E. Ho. 2023. [Multilegalpile: A 689gb multilingual legal
    corpus](http://arxiv.org/abs/2306.02069).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niklaus 等（2023）乔尔·尼克劳斯、维顿·马托希、马蒂亚斯·斯图尔默、伊利亚斯·查基迪斯和丹尼尔·E·霍。2023年。[Multilegalpile：一个
    689GB 的多语言法律语料库](http://arxiv.org/abs/2306.02069)。
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. 2018. Improving language understanding by generative pre-training.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2018）亚历克·拉德福德、卡尔蒂克·纳拉希姆汉、蒂姆·萨利曼斯、伊利亚·苏茨克维尔等。2018年。通过生成预训练提升语言理解。
- en: Raffel et al. (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](http://arxiv.org/abs/1910.10683).
    *arXiv e-prints*.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等（2019）科林·拉菲尔、诺阿姆·沙泽尔、亚当·罗伯茨、凯瑟琳·李、沙兰·纳朗、迈克尔·马特纳、阎琦·周、魏力和彼得·J·刘。2019年。[利用统一的文本到文本转换器探索迁移学习的极限](http://arxiv.org/abs/1910.10683)。*arXiv
    电子版*。
- en: 'Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
    Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning
    models with over 100 billion parameters. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*, pages 3505–3506.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rasley 等（2020）杰夫·拉斯利、萨米扬·拉杰班达里、奥拉图吉·鲁瓦塞和赫宇雄。2020年。《Deepspeed：系统优化使得训练超过 1000
    亿参数的深度学习模型成为可能》。在 *第26届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*，第3505–3506页。
- en: Romanov and Shivade (2018) Alexey Romanov and Chaitanya Shivade. 2018. [Lessons
    from natural language inference in the clinical domain](https://doi.org/10.18653/v1/D18-1187).
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing*, pages 1586–1596, Brussels, Belgium. Association for Computational
    Linguistics.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Romanov 和 Shivade（2018）亚历克谢·罗曼诺夫和蔡特纳·希瓦德。2018年。[临床领域自然语言推理的经验教训](https://doi.org/10.18653/v1/D18-1187)。在
    *2018年自然语言处理经验方法会议论文集*，第1586–1596页，比利时布鲁塞尔。计算语言学协会。
- en: 'Sanh et al. (2020) Victor Sanh, Thomas Wolf, and Alexander Rush. 2020. Movement
    pruning: Adaptive sparsity by fine-tuning. *Advances in Neural Information Processing
    Systems*, 33:20378–20389.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等（2020）维克多·桑、托马斯·沃尔夫和亚历山大·拉什。2020年。《运动剪枝：通过微调实现自适应稀疏性》。*神经信息处理系统进展*，33：20378–20389。
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2023.
    A simple and effective pruning approach for large language models. *arXiv preprint
    arXiv:2306.11695*.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2023）孙明杰、刘壮、安娜·贝尔和 J·齐科·科尔特。2023年。一个简单而有效的大型语言模型剪枝方法。*arXiv 预印本 arXiv:2306.11695*。
- en: Sung et al. (2021) Yi-Lin Sung, Varun Nair, and Colin A Raffel. 2021. Training
    neural networks with fixed sparse masks. *Advances in Neural Information Processing
    Systems*, 34:24193–24205.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sung 等（2021）孙艺琳、瓦伦·奈尔和科林·A·拉菲尔。2021年。使用固定稀疏掩码训练神经网络。*神经信息处理系统进展*，34：24193–24205。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori 等（2023）罗汉·塔奥里、伊沙恩·古尔拉贾尼、张天意、扬·迪布瓦、李雪辰、卡洛斯·古斯特林、佩西·梁和田统诺·B·哈希莫托。2023年。《斯坦福
    alpaca：一个遵循指令的 llama 模型》。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。
- en: 'Thompson et al. (2019) Brian Thompson, Jeremy Gwinnup, Huda Khayrallah, Kevin
    Duh, and Philipp Koehn. 2019. Overcoming catastrophic forgetting during domain
    adaptation of neural machine translation. In *Proceedings of the 2019 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies*, pages 2062–2068.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thompson 等（2019）布赖恩·汤普森、杰里米·格温纳普、胡达·哈耶拉赫、凯文·杜赫和菲利普·科恩。2019年。在神经机器翻译领域适应中的灾难性遗忘克服。在
    *2019年北美计算语言学协会：人类语言技术会议论文集*，第2062–2068页。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023）雨果·图弗龙、路易斯·马丁、凯文·斯通、彼得·阿尔伯特、阿姆贾德·阿尔马海里、雅斯敏·巴巴伊、尼古拉·巴什利科夫、苏米亚·巴特拉、普拉杰瓦尔·巴尔加瓦、施鲁提·博萨尔等。2023年。《Llama
    2：开放基础和微调的聊天模型》。*arXiv 预印本 arXiv:2307.09288*。
- en: 'Xia et al. (2023) Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2023.
    Sheared llama: Accelerating language model pre-training via structured pruning.
    In *Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability,
    and Resource Optimization*.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia 等（2023）Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, 和 Danqi Chen. 2023. Sheared
    llama: 通过结构化剪枝加速语言模型预训练。在*推进神经网络训练的研讨会：计算效率、可扩展性和资源优化*。'
- en: Xia et al. (2022) Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022. Structured
    pruning learns compact and accurate models. In *Proceedings of the 60th Annual
    Meeting of the Association for Computational Linguistics*, pages 1513–1528.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia 等（2022）Mengzhou Xia, Zexuan Zhong, 和 Danqi Chen. 2022. 结构化剪枝学习紧凑且准确的模型。在*第
    60 届计算语言学协会年会论文集*，页面 1513–1528。
- en: 'Zhang et al. (2023) Nan Zhang, Yusen Zhang, Wu Guo, Prasenjit Mitra, and Rui
    Zhang. 2023. [FaMeSumm: Investigating and improving faithfulness of medical summarization](https://aclanthology.org/2023.emnlp-main.673).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 10915–10931, Singapore. Association for Computational Linguistics.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2023）Nan Zhang, Yusen Zhang, Wu Guo, Prasenjit Mitra, 和 Rui Zhang.
    2023. [FaMeSumm: 调查和改进医疗摘要的可信度](https://aclanthology.org/2023.emnlp-main.673)。在*2023
    年自然语言处理经验方法会议论文集*，页面 10915–10931，新加坡。计算语言学协会。'
- en: 'Zhao et al. (2023) Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin
    Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al.
    2023. Pytorch fsdp: experiences on scaling fully sharded data parallel. *arXiv
    preprint arXiv:2304.11277*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等（2023）Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang,
    Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer 等。2023. Pytorch
    fsdp: 扩展完全分片数据并行的经验。*arXiv 预印本 arXiv:2304.11277*。'
- en: Zheng et al. (2021) Lucia Zheng, Neel Guha, Brandon R. Anderson, Peter Henderson,
    and Daniel E. Ho. 2021. [When does pretraining help? assessing self-supervised
    learning for law and the casehold dataset](http://arxiv.org/abs/2104.08671). In
    *Proceedings of the 18th International Conference on Artificial Intelligence and
    Law*. Association for Computing Machinery.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2021）Lucia Zheng, Neel Guha, Brandon R. Anderson, Peter Henderson, 和
    Daniel E. Ho. 2021. [预训练何时有帮助？评估自监督学习在法律和 casehold 数据集上的作用](http://arxiv.org/abs/2104.08671)。在*第
    18 届人工智能与法律国际会议论文集*。计算机协会。
- en: Zhu et al. (2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023.
    A survey on model compression for large language models. *arXiv preprint arXiv:2308.07633*.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2023）Xunyu Zhu, Jian Li, Yong Liu, Can Ma, 和 Weiping Wang. 2023. 关于大语言模型的模型压缩的调查。*arXiv
    预印本 arXiv:2308.07633*。
- en: Appendix A Accuracy Scores on PubMedQA
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A PubMedQA 的准确性评分
- en: In Table [7](#A3.T7 "Table 7 ‣ Appendix C Hyperparameters ‣ Pruning as a Domain-specific
    LLM Extractor"), we report the accuracy score of each model on PubMedQA before
    and after LoRA fine-tuning. Except LLM-Pruner, we see score improvement on all
    other models after fine-tuning. Thus, Table [7](#A3.T7 "Table 7 ‣ Appendix C Hyperparameters
    ‣ Pruning as a Domain-specific LLM Extractor") indicates that our fine-tuning
    is still improving model performance on PubMedQA in some ways.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格[7](#A3.T7 "Table 7 ‣ Appendix C Hyperparameters ‣ Pruning as a Domain-specific
    LLM Extractor")中，我们报告了每个模型在 LoRA 微调前后在 PubMedQA 上的准确性评分。除了 LLM-Pruner 外，我们观察到所有其他模型在微调后评分有所提高。因此，表格[7](#A3.T7
    "Table 7 ‣ Appendix C Hyperparameters ‣ Pruning as a Domain-specific LLM Extractor")
    表明我们的微调在某些方面仍在提升模型在 PubMedQA 上的表现。
- en: Appendix B Experiments on BLOOM
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B BLOOM 实验
- en: 'We conduct a small set of experiments in healthcare domain for illustrative
    purpose. SparseGPT is chosen for comparison, since it is the strongest baseline.
    We run SparseGPT under two settings: (1) only open-domain calibration dataset
    is used for pruning, and (2) both open-domain and domain-specific calibration
    datasets are used, which is the same as the setting in Section [5](#S5 "5 Results
    and Analysis ‣ Pruning as a Domain-specific LLM Extractor"). All BLOOM experiments
    are based on the bigscience/bloom-7b1 model on Hugging Face.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在医疗领域进行了一小组实验以作说明。选择 SparseGPT 进行比较，因为它是最强的基线模型。我们在两种设置下运行 SparseGPT：（1）仅使用开放领域的校准数据集进行剪枝，（2）使用开放领域和特定领域的校准数据集，这与第[5](#S5
    "5 Results and Analysis ‣ Pruning as a Domain-specific LLM Extractor")节中的设置相同。所有
    BLOOM 实验均基于 Hugging Face 上的 bigscience/bloom-7b1 模型。
- en: As shown in Table [8](#A3.T8 "Table 8 ‣ Appendix C Hyperparameters ‣ Pruning
    as a Domain-specific LLM Extractor"), SparseGPT yields the best performance on
    BLOOM across all three metrics. Although D-Pruner surpasses SparseGPT on MedNLI
    when SparseGPT only uses open-domain data, it struggles on both medical perplexity
    and PubMedQA. Because our method is based on Lv et al. ([2023](#bib.bib25)) for
    fine-tuning and this fine-tuning pipeline only discusses performance scores on
    LLaMA, Lv et al. ([2023](#bib.bib25)) might require a significant adaptation when
    we change our backbone models from LLaMA to BLOOM. It might also not work well
    on BLOOM-based models when we integrate the general importance as a regularization
    term. Therefore, we might need to switch the fine-tuning pipeline we use in order
    to obtain the optimal performance of D-Pruner.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [8](#A3.T8 "Table 8 ‣ Appendix C Hyperparameters ‣ Pruning as a Domain-specific
    LLM Extractor") 所示，SparseGPT 在 BLOOM 上的三个指标中表现最佳。尽管 D-Pruner 在 SparseGPT 仅使用开放领域数据时在
    MedNLI 上超越了 SparseGPT，但它在医学困惑度和 PubMedQA 上表现不佳。因为我们的方法基于 Lv 等人 ([2023](#bib.bib25))
    的微调，这个微调流程仅讨论了 LLaMA 上的性能评分，Lv 等人 ([2023](#bib.bib25)) 可能需要在我们将基础模型从 LLaMA 更改为
    BLOOM 时进行重大调整。当我们将通用重要性作为正则化项时，它也可能在基于 BLOOM 的模型上表现不佳。因此，我们可能需要更换我们使用的微调流程，以获得
    D-Pruner 的最佳性能。
- en: Appendix C Hyperparameters
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 超参数
- en: We stick to the default values of hyperparameters for our baseline models. For
    D-Pruner, in the healthcare domain, we set $\lambda$.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在基准模型中坚持使用默认的超参数值。对于 D-Pruner，在医疗保健领域，我们设置了 $\lambda$。
- en: '| Model | Before LoRA | After LoRA |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | LoRA 之前 | LoRA 之后 |'
- en: '| Dense | 39.20 | 64.60 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 密集型 | 39.20 | 64.60 |'
- en: '| Magnitude | 47.00 | 55.20 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 幅度 | 47.00 | 55.20 |'
- en: '| LLM-Pruner | 51.40 | 40.20 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner | 51.40 | 40.20 |'
- en: '| SparseGPT | 53.80 | 57.00 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 53.80 | 57.00 |'
- en: '| D-Pruner | 58.80 | 59.20 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| D-Pruner | 58.80 | 59.20 |'
- en: 'Table 7: Accuracy scores of different models on PubMedQA dataset.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：不同模型在 PubMedQA 数据集上的准确度评分。
- en: '| Model | Perplexity | MedNLI | PubMedQA |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 困惑度 | MedNLI | PubMedQA |'
- en: '| Dense | 9.40 | 33.26 | 23.72 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 密集型 | 9.40 | 33.26 | 23.72 |'
- en: '| SparseGPT* | 11.16 | 32.07 | 29.74 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT* | 11.16 | 32.07 | 29.74 |'
- en: '| SparseGPT | 10.88 | 33.47 | 24.23 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 10.88 | 33.47 | 24.23 |'
- en: '| D-Pruner | 14.70 | 32.70 | 20.95 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| D-Pruner | 14.70 | 32.70 | 20.95 |'
- en: 'Table 8: Performance of SparseGPT and D-Pruner (at 50% sparsity) on metrics
    of healthcare domain based on BLOOM. * denotes the model that only uses open-domain
    calibration data (C4) for pruning.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：SparseGPT 和 D-Pruner（在 50% 稀疏度下）在基于 BLOOM 的医疗保健领域指标上的表现。* 表示仅使用开放领域校准数据（C4）进行修剪的模型。
