- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:58:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:58:51
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction
    Tasks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MetaIE：从LLM中提炼用于所有类型信息提取任务的元模型
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.00457](https://ar5iv.labs.arxiv.org/html/2404.00457)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.00457](https://ar5iv.labs.arxiv.org/html/2404.00457)
- en: Letian Peng, Zilong Wang, Feng Yao, Zihan Wang^∗, Jingbo Shang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Letian Peng, Zilong Wang, Feng Yao, Zihan Wang^∗, Jingbo Shang
- en: University of California, San Diego
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 加利福尼亚大学圣地亚哥分校
- en: '{lepeng, zlwang, fengyao, ziw224, jshang}@ucsd.edu    Corresponding authors.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{lepeng, zlwang, fengyao, ziw224, jshang}@ucsd.edu    通讯作者。'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Information extraction (IE) is a fundamental area in natural language processing
    where prompting large language models (LLMs), even with in-context examples, cannot
    defeat small LMs tuned on very small IE datasets. We observe that IE tasks, such
    as named entity recognition and relation extraction, all focus on extracting *important
    information*, which can be formalized as a label-to-span matching. In this paper,
    we propose a novel framework MetaIE to build a small LM as meta-model by learning
    to extract “important information”, i.e., the meta-understanding of IE, so that
    this meta-model can be adapted to all kind of IE tasks effectively and efficiently.
    Specifically, MetaIE obtains the small LM via a symbolic distillation from an
    LLM following the label-to-span scheme. We construct the distillation dataset
    via sampling sentences from language model pre-training datasets (e.g., OpenWebText
    in our implementation) and prompting an LLM to identify the typed spans of “important
    information”. We evaluate the meta-model under the few-shot adaptation setting.
    Extensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer
    a better starting point for few-shot tuning on IE datasets and outperform other
    meta-models from (1) vanilla language model pre-training, (2) multi-IE-task pre-training
    with human annotations, and (3) single-IE-task symbolic distillation from LLM.
    Moreover, we provide comprehensive analyses of MetaIE, such as the size of the
    distillation dataset, the meta-model architecture, and the size of the meta-model.¹¹1Code,
    datasets, and model checkpoints: [https://github.com/KomeijiForce/MetaIE](https://github.com/KomeijiForce/MetaIE).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 信息提取（IE）是自然语言处理中的一个基础领域，在这个领域，即使是通过上下文示例来提示的大型语言模型（LLMs），也无法击败在非常小的IE数据集上调优的小型语言模型（LMs）。我们观察到，IE任务，如命名实体识别和关系抽取，都专注于提取*重要信息*，这可以被形式化为标签到跨度的匹配。本文提出了一种新颖的框架MetaIE，通过学习提取“重要信息”，即IE的元理解，来构建一个小型LM作为元模型，从而使该元模型可以有效地适应各种IE任务。具体而言，MetaIE通过符号蒸馏从LLM中获得小型LM，遵循标签到跨度的方案。我们通过从语言模型预训练数据集（例如，我们实现中的OpenWebText）中抽取句子，并提示LLM识别“重要信息”的类型跨度，来构建蒸馏数据集。我们在少样本适应设置下评估元模型。在13个数据集和6个IE任务上的广泛结果确认了MetaIE可以为少样本调优提供更好的起点，并且优于（1）普通语言模型预训练，（2）带有人类注释的多IE任务预训练，以及（3）单IE任务的符号蒸馏。此外，我们还提供了对MetaIE的全面分析，如蒸馏数据集的大小、元模型的架构和元模型的大小。¹¹1代码、数据集和模型检查点：[https://github.com/KomeijiForce/MetaIE](https://github.com/KomeijiForce/MetaIE)。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs), such as ChatGPT (OpenAI, [2023](#bib.bib31)),
    benefit from vast amount of training data and have demonstrated exceptional performance
    across various areas through in-context learning (ICL) (Dong et al., [2023](#bib.bib7)).
    However, when it comes to information extraction (IE), LLMs, even with ICL examples,
    struggle to compete with smaller LMs (e.g., BERT (Devlin et al., [2019](#bib.bib5))
    and RoBERTa (Liu et al., [2019](#bib.bib24))) fine-tuned on very small training
    sets (Peng et al., [2023](#bib.bib33); Wadhwa et al., [2023](#bib.bib39); Gao
    et al., [2024](#bib.bib13)). This is usually regarded as a limitation of LLMs
    in following a specific extraction scheme (Xu et al., [2023](#bib.bib43)). Meanwhile,
    it is worth mentioning that conducting auto-regressive inference with LLMs is
    expensive and time-consuming, hindering their application in conducting IE over
    large corpora.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），如 ChatGPT（OpenAI，[2023](#bib.bib31)），受益于大量的训练数据，并通过上下文学习（ICL）展示了在各个领域的卓越表现（Dong
    等，[2023](#bib.bib7)）。然而，在信息提取（IE）方面，即使有 ICL 示例，LLMs 也难以与在非常小的训练集上微调的较小语言模型（例如
    BERT（Devlin 等，[2019](#bib.bib5)）和 RoBERTa（Liu 等，[2019](#bib.bib24)））竞争（Peng 等，[2023](#bib.bib33)；Wadhwa
    等，[2023](#bib.bib39)；Gao 等，[2024](#bib.bib13)）。这通常被视为 LLMs 在遵循特定提取方案方面的限制（Xu 等，[2023](#bib.bib43)）。与此同时，需要提到的是，进行自回归推理的成本高且耗时，这阻碍了其在大规模语料库上进行
    IE 的应用。
- en: We observe that IE tasks, such as named entity recognition (NER) and relation
    extraction (RE), all focus on extracting *important information*, which can be
    formalized as *label-to-span* instructions. Specifically, all IE tasks can be
    decomposed as several instructions such as “*given an IE label ($l$ can be (1)
    *Person*, *Location*, *Organization* in NER to recognize entities or (2) *Tom
    births at* in RE to verify if there is a certain relation between two entities
    by checking the other entity can be recognized or not. Following these label-to-span
    instructions, LLMs can handle all kinds of IE tasks and return imperfect yet semantically
    reasonable answers. To this end, we argue that LLMs can be distilled into meta-models
    for IE which can quickly fine-tuned on few-shot training sets for better task-specific
    performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，IE 任务，如命名实体识别（NER）和关系提取（RE），都集中于提取*重要信息*，这可以被形式化为*标签到跨度*的指令。具体来说，所有的 IE
    任务都可以分解为几个指令，例如“*给定一个 IE 标签（$l$ 可以是（1）*Person*、*Location*、*Organization* 在 NER
    中识别实体，或（2）*Tom births at* 在 RE 中验证两个实体之间是否存在某种关系，通过检查另一个实体是否可以被识别来确认。按照这些标签到跨度的指令，LLMs
    可以处理各种 IE 任务，并返回不完美但语义合理的答案。为此，我们认为 LLMs 可以被提炼为 IE 的元模型，这些元模型可以在少量训练集上迅速微调以获得更好的任务特定性能。
- en: '![Refer to caption](img/254031d020bd8806f9e40f11fcebc7de.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/254031d020bd8806f9e40f11fcebc7de.png)'
- en: 'Figure 1: An overview of different transfer learning schemes involved in the
    experiments.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：实验中涉及的不同迁移学习方案概述。
- en: In this paper, we propose a novel framework MetaIE to build a small LM as a
    meta-model by learning to extract “important information”, i.e., the meta-understanding
    of IE, and we show that this meta-model can be adapted to all kind of IE tasks
    effectively and efficiently. Some prior work have built meta-models for a specific
    IE tasks, e.g., UniversalNER (Zhou et al., [2023](#bib.bib46)) explores the potential
    of building a meta-model for NER tasks. Our work is more ambitious at a larger
    scope for all IE tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新颖的框架 MetaIE，通过学习提取“重要信息”，即 IE 的元理解，来构建一个小型语言模型作为元模型，并展示了这个元模型可以有效且高效地适应所有类型的
    IE 任务。先前的一些工作为特定的 IE 任务构建了元模型，例如，UniversalNER（Zhou 等，[2023](#bib.bib46)）探索了为 NER
    任务构建元模型的潜力。我们的工作在所有 IE 任务的更大范围内更加雄心勃勃。
- en: MetaIE obtains the small LM via a symbolic distillation (West et al., [2022](#bib.bib41))
    from an LLM following the label-to-span scheme. We construct the distillation
    dataset via sampling sentences from language model pre-training datasets and prompting
    an LLM to identify the typed spans of “important information”. In particular,
    we implement this idea with $100,000$ sentences from the OpenWebText corpus (Gokaslan
    & Cohen, [2019](#bib.bib14)), which contains various webpage texts and is also
    a subset of the popular language model pre-training dataset. We feed these sentences
    to GPT-3.5-turbo for identifying “important information”, which is then used to
    distill small LMs. It is worth mentioning that MetaIE is applicable to all types
    of small LMs and one only needs to convert the label-span pairs following the
    corresponding labeling scheme (e.g., BIO sequence labeling for encoders like RoBERTa,
    seq2seq labeling for encoder-decoders like BART).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MetaIE通过遵循标签到跨度方案的符号蒸馏（West et al.，[2022](#bib.bib41)）从LLM获得小型语言模型。我们通过从语言模型预训练数据集中抽取句子，并提示LLM识别“重要信息”的类型跨度来构建蒸馏数据集。特别地，我们用来自OpenWebText语料库（Gokaslan
    & Cohen，[2019](#bib.bib14)）的$100,000$句子来实现这一想法，该语料库包含各种网页文本，并且是流行语言模型预训练数据集的一个子集。我们将这些句子提供给GPT-3.5-turbo，以识别“重要信息”，然后用于蒸馏小型语言模型。值得一提的是，MetaIE适用于所有类型的小型语言模型，只需根据相应的标注方案（例如，RoBERTa等编码器的BIO序列标注，BART等编码器-解码器的seq2seq标注）转换标签-跨度对。
- en: Our evaluation focuses on the few-shot learning ability of the meta-model for
    different IE tasks. We mainly compare MetaIE with meta-models from (1) vanilla
    language model pre-training, (2) multi-IE-task pre-training with human annotations,
    and (3) single-IE-task symbolic distillation from LLM. Large-scale datasets for
    NER, RE, and event extraction (EE) tasks are used in single-IE-task and multi-IE-task
    pre-training, therefore, these datasets shall be considered as *in-task-distributional*
    for these two methods. For a more comprehensive evaluation, we further include
    *out-of-task-distributional datasets* from (1) semantic role labeling (SRL) (Carreras
    & Màrquez, [2005](#bib.bib3)), (2) aspect-based sentiment analysis (ABSA) (Pontiki
    et al., [2014](#bib.bib34)), and (3) aspect-sentiment triplet extraction (ASTE) (Xu
    et al., [2020](#bib.bib44)), totaling 13 datasets across 6 IE tasks. In our experiments,
    MetaIE generally achieves the best performance, only *very occasionally* losing
    to task-specific distillation on some in-task-distributional datasets. This demonstrates
    that MetaIE is a strong and efficient method to distill the meta-understanding
    of IE from LLMs into small LMs. Remarkably, distilling from the LLM-produced dataset
    following the traditional human annotation schemes performs poorly. Therefore,
    the success of MetaIE, rather than from purely using LLMs, shall also come from
    our label-to-span scheme.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的评估集中于元模型在不同IE任务中的少量学习能力。我们主要将MetaIE与以下几种元模型进行比较：(1) 普通语言模型预训练，(2) 带有人类注释的多任务IE预训练，以及(3)
    从LLM进行的单任务IE符号蒸馏。在单任务IE和多任务IE预训练中使用的大规模数据集包括NER、RE和事件抽取（EE）任务，因此这些数据集应被视为*任务内分布*的数据集。为了更全面的评估，我们进一步包含了来自以下几个任务的*任务外分布数据集*：(1)
    语义角色标注（SRL）（Carreras & Màrquez，[2005](#bib.bib3)），(2) 基于方面的情感分析（ABSA）（Pontiki
    et al.，[2014](#bib.bib34)），以及(3) 方面-情感三元组提取（ASTE）（Xu et al.，[2020](#bib.bib44)），总计覆盖了6个IE任务的13个数据集。在我们的实验中，MetaIE通常表现最佳，仅在一些任务内分布数据集上*极少*会输给任务特定的蒸馏方法。这表明MetaIE是一种强大且高效的方法，可以将LLMs的元理解提炼到小型语言模型中。值得注意的是，从LLM生成的数据集进行的传统人工注释方案表现较差。因此，MetaIE的成功不仅来自于单纯使用LLMs，也得益于我们的标签到跨度方案。
- en: We have conducted comprehensive analyses of MetaIE. We study the scaling-up
    rules to investigate the model and dataset size boundaries in obtaining the meta-understanding
    of IE. We showcase the diversity of the types of important information in the
    MetaIE distillation dataset. We show that the RoBERTa with sequence labeling framework
    is the best meta-model architecture compared with sequence-to-sequence and decoder-only
    models, at a similar scale.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对MetaIE进行了全面的分析。我们研究了扩展规则，以探讨在获得IE的元理解时模型和数据集规模的边界。我们展示了MetaIE蒸馏数据集中重要信息类型的多样性。我们显示了RoBERTa与序列标注框架在类似规模下相比于序列到序列和仅解码器模型，是最佳的元模型架构。
- en: 'Our contributions are three-fold:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献有三个方面：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We are the first to build a small LM as a meta-model for all kinds of IE tasks.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们是首个将小型语言模型作为各种信息抽取（IE）任务的元模型进行构建的团队。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a novel label-to-span scheme that unifies all IE tasks and applies
    symbolic distillation to distill the meta-understanding from an LLM to a small
    LM.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的标签到跨度的方案，统一所有IE任务，并应用符号蒸馏从LLM提炼元理解到一个小型LM中。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We have a rigorous experiment design, which covers various IE tasks and meta-model
    methods. Comprehensive experiment results support the intuitive expectation and
    advantage of our MetaIE.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们有一个严格的实验设计，涵盖了各种IE任务和元模型方法。全面的实验结果支持了我们MetaIE的直观预期和优势。
- en: 2 Related Works
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Information Extraction
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 信息提取
- en: Information extraction (IE) is one of the most popular and vital domains in
    natural language processing. Early IE systems are generally developed for a single
    IE dataset like NER (dos Santos & Guimarães, [2015](#bib.bib8)), RE (Katiyar &
    Cardie, [2016](#bib.bib20)), or EE (Chen et al., [2015](#bib.bib4)). Due to the
    gap between the label sets and annotation styles of different IE datasets, few-shot
    IE frameworks (Ding et al., [2021](#bib.bib6); Han et al., [2018](#bib.bib17);
    Ma et al., [2023](#bib.bib28)) are proposed to quickly learn models on new datasets.
    The IE models are pre-trained on a large scale of IE labels and then transferred
    to the target domain by fine-tuning on few examples. With the emergence of LLMs,
    researchers have started to train LMs on multiple IE tasks with unified formats
    (Lu et al., [2022](#bib.bib27); Paolini et al., [2021](#bib.bib32)). LLMs fine-tuned
    for general purpose (OpenAI, [2023](#bib.bib31); Touvron et al., [2023](#bib.bib37))
    have also shown strong potential to understand new IE tasks with their instruction-following
    ability. However, these LLMs still lag behind supervised models (Xu et al., [2023](#bib.bib43)),
    potentially due to the difficulty of specifying the required pattern for extraction
    in different datasets. Moreover, the cost of LLMs limits their application to
    IE on a large corpus. This paper aims to transfer the meta-understanding of IE
    from LLMs to lighter-weight models, which produce a flexible model with high adaptability
    to any target IE task.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 信息提取（IE）是自然语言处理领域中最受欢迎和最重要的领域之一。早期的IE系统通常为单一IE数据集开发，如NER（dos Santos & Guimarães，[2015](#bib.bib8)），RE（Katiyar
    & Cardie，[2016](#bib.bib20)），或EE（Chen et al.，[2015](#bib.bib4)）。由于不同IE数据集之间标签集和注释风格的差异，提出了少量样本IE框架（Ding
    et al.，[2021](#bib.bib6)；Han et al.，[2018](#bib.bib17)；Ma et al.，[2023](#bib.bib28)），以快速学习新数据集上的模型。IE模型在大规模IE标签上进行预训练，然后通过在少量示例上进行微调转移到目标领域。随着LLM的出现，研究人员开始在多个IE任务上使用统一格式训练LM（Lu
    et al.，[2022](#bib.bib27)；Paolini et al.，[2021](#bib.bib32)）。为通用目的微调的LLM（OpenAI，[2023](#bib.bib31)；Touvron
    et al.，[2023](#bib.bib37)）也表现出强大的理解新IE任务的潜力，具有指令跟随能力。然而，这些LLM仍然落后于监督模型（Xu et al.，[2023](#bib.bib43)），这可能是由于在不同数据集中指定所需模式的困难。此外，LLM的成本限制了它们在大规模语料库上进行IE的应用。本文旨在将LLM的IE元理解转移到更轻量级的模型中，从而生成一个适应性强的灵活模型，适合任何目标IE任务。
- en: 2.2 Model Distillation
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 模型蒸馏
- en: Model distillation (Hinton et al., [2015](#bib.bib18); Gou et al., [2021](#bib.bib15))
    is the process of transferring knowledge from large models (teacher models) to
    small ones (student models). Traditional distillation optimizes the similarity
    between logits produced by the teacher and student models (Hinton et al., [2015](#bib.bib18);
    Kim et al., [2019](#bib.bib21); Mirzadeh et al., [2020](#bib.bib29)). Symbolic
    distillation (West et al., [2022](#bib.bib41); Li et al., [2023](#bib.bib23);
    West et al., [2023](#bib.bib42)) for language models learns a student model on
    texts generated by the teacher model. In comparison with traditional distillation,
    symbolic distillation allows the student model to focus on one aspect of the teacher
    model (West et al., [2022](#bib.bib41)), which can be some high-level ability,
    such as chain-of-thought reasoning (Li et al., [2023](#bib.bib23)), with much
    smaller model size. For IE, symbolic model distillation has been successfully
    applied for an IE subtask, NER (Zhou et al., [2023](#bib.bib46)), which distills
    an NER model that can extract entities in a broad domain. This paper aims to distill
    the cross-IE task ability of LLMs, i.e., meta-understanding of IE and proposes
    a meta-model that can effectively learn IE tasks with few examples.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 模型蒸馏（Hinton et al., [2015](#bib.bib18); Gou et al., [2021](#bib.bib15)）是将知识从大模型（教师模型）转移到小模型（学生模型）的过程。传统的蒸馏优化了教师和学生模型产生的logits之间的相似性（Hinton
    et al., [2015](#bib.bib18); Kim et al., [2019](#bib.bib21); Mirzadeh et al., [2020](#bib.bib29)）。符号蒸馏（West
    et al., [2022](#bib.bib41); Li et al., [2023](#bib.bib23); West et al., [2023](#bib.bib42)）针对语言模型在教师模型生成的文本上学习学生模型。与传统蒸馏相比，符号蒸馏允许学生模型关注教师模型的一个方面（West
    et al., [2022](#bib.bib41)），这可能是一些高级能力，例如链式思维推理（Li et al., [2023](#bib.bib23)），并且模型规模更小。对于IE，符号模型蒸馏已成功应用于IE子任务，即NER（Zhou
    et al., [2023](#bib.bib46)），其蒸馏出一个能够在广泛领域提取实体的NER模型。本文旨在提取LLMs的跨IE任务能力，即IE的元理解，并提出一个能够有效学习少量示例的IE任务的元模型。
- en: 2.3 Meta Learning
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 元学习
- en: Meta-learning (Finn et al., [2017b](#bib.bib11)) enables the models to learn
    new tasks better, i.e., stronger transfer learning ability. MAML (Finn et al.,
    [2017a](#bib.bib10)) proposes a framework to learn a better starting point for
    few-shot learning by utilizing multiple datasets for loss updating. Reptile (Nichol
    et al., [2018](#bib.bib30)), similar to MAML, simplifies the meta-learning algorithm
    by performing stochastic gradient descent not only within each task but also across
    tasks, making it more efficient and easier to implement. The Prototypical Networks
    method (Snell et al., [2017](#bib.bib36)) employs a distance-based classification
    approach, where it learns a metric space in which classification can be performed
    by computing distances to prototype representations of each class. While most
    meta-learning methods are experimented on classification tasks, pre-training on
    multiple datasets (Ding et al., [2021](#bib.bib6)) and prototypical networks (Ji
    et al., [2022](#bib.bib19)) have been applied for IE. While these methods focus
    on specific IE tasks like NER, we aim to optimize a starting point for general
    IE tasks by distilling from LLMs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习（Finn et al., [2017b](#bib.bib11)）使模型能够更好地学习新任务，即，增强的迁移学习能力。MAML（Finn et
    al., [2017a](#bib.bib10)）提出了一个框架，通过利用多个数据集进行损失更新，以学习更好的起始点来进行少样本学习。Reptile（Nichol
    et al., [2018](#bib.bib30)）与MAML类似，通过在每个任务内部和跨任务执行随机梯度下降，简化了元学习算法，使其更加高效和易于实现。原型网络方法（Snell
    et al., [2017](#bib.bib36)）采用基于距离的分类方法，在该方法中，它学习一个度量空间，在这个空间中，可以通过计算与每个类别的原型表示的距离来进行分类。尽管大多数元学习方法都在分类任务上进行实验，但在多个数据集上进行预训练（Ding
    et al., [2021](#bib.bib6)）和原型网络（Ji et al., [2022](#bib.bib19)）已被应用于信息抽取（IE）。虽然这些方法专注于像命名实体识别（NER）这样的特定IE任务，我们的目标是通过从大型语言模型（LLMs）中提取来优化通用IE任务的起始点。
- en: 3 Our MetaIE Framework
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 我们的MetaIE框架
- en: 3.1 Label-to-span Scheme
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 标签到跨度方案
- en: We formalize the IE task as given an IE label $l$.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将IE任务形式化为给定一个IE标签$l$。
- en: In this paper, we aim to learn a meta-model that can be easily adapted to different
    IE tasks. In the current practice of IE, the “meta-model” is generally pre-trained
    in a single IE task with a large number of labels ($\mathcal{L}^{(pt)}\subset\mathcal{L}^{(Task)}$,
    our meta-model will enjoy an efficient transfer to all IE tasks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们的目标是学习一个可以轻松适应不同IE任务的元模型。在当前的IE实践中，“元模型”通常是在一个IE任务中用大量标签进行预训练的（$\mathcal{L}^{(pt)}\subset\mathcal{L}^{(Task)}$），我们的元模型将享受对所有IE任务的高效迁移。
- en: 3.2 Distillation Dataset Construction
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 蒸馏数据集构建
- en: '![Refer to caption](img/75bad62f18616058adb90ac3d407073a.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/75bad62f18616058adb90ac3d407073a.png)'
- en: 'Figure 2: The prompt used in our experiments to build the dataset for symbolic
    distillation.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们在实验中使用的提示，用于构建符号蒸馏的数据集。
- en: 'To apply a symbolic distillation of the meta-understanding of IE from LLMs,
    we prompt LLMs to create data for distillation by querying them to extract “important
    information” from texts as shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Distillation
    Dataset Construction ‣ 3 Our MetaIE Framework ‣ MetaIE: Distilling a Meta Model
    from LLM for All Kinds of Information Extraction Tasks"). Our expectation for
    the dataset is to cover as many $l$ set.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用从LLM中提取IE的元理解的符号蒸馏，我们提示LLM通过询问它们从文本中提取“重要信息”来创建蒸馏数据，如图[2](#S3.F2 "图 2 ‣
    3.2 蒸馏数据集构建 ‣ 3 我们的MetaIE框架 ‣ MetaIE：从LLM中提炼出用于所有类型信息提取任务的元模型")所示。我们对数据集的期望是覆盖尽可能多的
    $l$ 集。
- en: Implementation
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实施
- en: We select the paragraphs from OpenWebText (Gokaslan & Cohen, [2019](#bib.bib14)),
    Since OpenWebText it is a popular dataset used in language model pre-training,
    we are not introducing new texts. We split the paragraphs by sentences and only
    use the first sentence of each paragraph for a higher diversity and to avoid the
    ambiguity caused by coreference. The LLM is instructed to formalize all $(l,s)$,
    we split the span by conjunctions like comma.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 OpenWebText (Gokaslan & Cohen, [2019](#bib.bib14)) 中选择段落。由于 OpenWebText
    是用于语言模型预训练的流行数据集，我们没有引入新文本。我们按句子拆分段落，并仅使用每个段落的第一句话，以提高多样性并避免由共指引起的歧义。LLM 被指示将所有
    $(l,s)$ 形式化，我们通过逗号等连接词拆分跨度。
- en: '| $n$-gram (Count) | Example IE Labels (Relative Frequency) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| $n$-gram (Count) | 示例IE标签（相对频率） |'
- en: '| --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $1$-gram (270k) | Location (7.73%), Event (4.67%), Action (4.24%), Topic
    (3.57%), Subject (3.25%), Person (2.71%), Date (2.70%), Source (2.44%) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| $1$-gram (270k) | 位置 (7.73%)，事件 (4.67%)，行动 (4.24%)，主题 (3.57%)，学科 (3.25%)，人
    (2.71%)，日期 (2.70%)，来源 (2.44%) |'
- en: '| $2$-gram (44.5k) | Target audience (1.27%), Time period (0.998%), Individuals
    involved (0.992%), Action taken (0.877%), Political affiliation (0.762%), Parties
    involved (0.758%), Release date (0.697%), TV show (0.686%) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| $2$-gram (44.5k) | 目标受众 (1.27%)，时间段 (0.998%)，涉及个人 (0.992%)，采取的行动 (0.877%)，政治派别
    (0.762%)，涉及党派 (0.758%)，发布日期 (0.697%)，电视节目 (0.686%) |'
- en: '| $3$-gram (16.9k) | Source of information (2.02%), Cause of death (1.17%),
    Call to action (1.02%), Date of birth (0.739%), Date and time (0.727%), Date of
    death (0.562%), Type of content (0.337%), Reason for arrest (0.325%) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| $3$-gram (16.9k) | 信息来源 (2.02%)，死亡原因 (1.17%)，行动号召 (1.02%)，出生日期 (0.739%)，日期和时间
    (0.727%)，死亡日期 (0.562%)，内容类型 (0.337%)，逮捕原因 (0.325%) |'
- en: '| $4$-gram (7.39k) | Purpose of the bill (0.325%), Location of the incident
    (0.271%), Name of the person (0.271%), Number of people killed (0.203%), Number
    of people affected (0.189%), Content of the bill (0.162%), Number of people arrested
    (0.149%), Source of the information (0.149%) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| $4$-gram (7.39k) | 法案目的 (0.325%)，事件地点 (0.271%)，人员姓名 (0.271%)，遇难人数 (0.203%)，受影响人数
    (0.189%)，法案内容 (0.162%)，被捕人数 (0.149%)，信息来源 (0.149%) |'
- en: '| $\geq 5$-gram (5.37k) | Dates of birth and death (0.13%), Age at the time
    of death (0.112%), Total number of votes cast (0.0931%), Feature: Auschwitz through
    the Lens of the SS (0.0931%), Number of people on board (0.0745%), Name of the
    person involved (0.0745%), Date and time of publication (0.0745%), Action taken
    by President Obama (0.0745%) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| $\geq 5$-gram (5.37k) | 出生和死亡日期 (0.13%)，死亡时年龄 (0.112%)，投票总数 (0.0931%)，特征：通过SS的视角看奥斯维辛
    (0.0931%)，登船人数 (0.0745%)，涉及人员姓名 (0.0745%)，出版日期和时间 (0.0745%)，奥巴马总统采取的行动 (0.0745%)
    |'
- en: 'Table 1: Example IE Labels, Counts, and Relative Frequency in our constructed
    symbolic distillation dataset, grouped by the number of tokens.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：我们构建的符号蒸馏数据集中示例IE标签、计数和相对频率，按令牌数量分组。
- en: 'Table [1](#S3.T1 "Table 1 ‣ Implementation ‣ 3.2 Distillation Dataset Construction
    ‣ 3 Our MetaIE Framework ‣ MetaIE: Distilling a Meta Model from LLM for All Kinds
    of Information Extraction Tasks") shows some statistics and example results of
    the labels returned by the LLM, illustrating a broad spectrum of IE domains, ranging
    from simple entities and events to complex relationships and contexts. The diversity
    in the $n$-gram categories showcases the model’s ability to capture a wide array
    of query types. This variety underscores the comprehensive coverage and nuanced
    understanding that LLMs bring to the task of generating queries across different
    facets of the IE domain.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '表[1](#S3.T1 "Table 1 ‣ Implementation ‣ 3.2 Distillation Dataset Construction
    ‣ 3 Our MetaIE Framework ‣ MetaIE: Distilling a Meta Model from LLM for All Kinds
    of Information Extraction Tasks")展示了一些LLM返回的标签的统计数据和示例结果，展示了信息抽取领域的广泛范围，从简单的实体和事件到复杂的关系和上下文。$n$-gram类别的多样性展示了模型捕捉各种查询类型的能力。这种多样性突显了LLM在跨越信息抽取领域不同方面生成查询任务时所带来的全面覆盖和细致理解。'
- en: 3.3 Distillation Framework
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 蒸馏框架
- en: 'We illustrate the distillation with a sequence labeling model (dos Santos &
    Guimarães, [2015](#bib.bib8)) that suits well for encoder-based language models
    (e.g., RoBERTa (Liu et al., [2019](#bib.bib24))). Given a sequence of words $X=[x_{1},\cdots,x_{n}]$-person.
    In our case, we formalize the tagging in a query-dependent way since the model
    needs to handle arbitrary queries. We attach the label information as a prefix
    like “place: ” to the beginning of the input text. The input text is then labeled
    by the BIO scheme, where the span label is indicated in the prefix. Finally, the
    BIO sequences are used to fine-tune the sequence labeling models. This distillation
    process can also be adapted to Seq2Seq encoder-decoder models and Causal LM-based
    decoder-only models. We use sequence labeling models for the main experiment based
    on their empirical advantage in IE tasks, which we also empirically find support
    in the analysis in Section [5.2](#S5.SS2 "5.2 Distillation Framework Comparison
    ‣ 5 Further Analysis ‣ MetaIE: Distilling a Meta Model from LLM for All Kinds
    of Information Extraction Tasks").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过一个适用于基于编码器的语言模型（如RoBERTa (Liu et al., [2019](#bib.bib24))) 的序列标注模型（dos
    Santos & Guimarães, [2015](#bib.bib8)）来说明蒸馏过程。给定一个单词序列 $X=[x_{1},\cdots,x_{n}]$-person。在我们的案例中，我们将标注以查询相关的方式进行形式化，因为模型需要处理任意的查询。我们将标签信息作为前缀，例如“place:
    ”，附加在输入文本的开头。然后，输入文本通过BIO方案进行标注，其中跨度标签在前缀中指示。最后，BIO序列用于微调序列标注模型。这个蒸馏过程也可以适应于Seq2Seq编码器-解码器模型和基于Causal
    LM的解码器-only模型。我们基于它们在信息抽取任务中的经验优势，使用序列标注模型作为主要实验，这在第[5.2节](#S5.SS2 "5.2 Distillation
    Framework Comparison ‣ 5 Further Analysis ‣ MetaIE: Distilling a Meta Model from
    LLM for All Kinds of Information Extraction Tasks")的分析中也得到了经验上的支持。'
- en: 4 Experiments
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 IE Tasks and Datasets
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 信息抽取任务和数据集
- en: 'To deeply delve into the differences between different model distillation or
    meta-learning methods, we include a wide variety of tasks:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入探讨不同模型蒸馏或元学习方法之间的差异，我们包括了各种任务：
- en: '1.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Named Entity Recognition (NER) extracts named entities with their labels from
    texts. We include $6$ NER datasets that was studied in Ushio & Camacho-Collados
    ([2021](#bib.bib38)), i.e., (1) CoNLL2003, (2) BioNLP2004, (3) WNUT2017, (4) MIT-Movie,
    (5) MIT-Restaurant, (6) BC5CDR, which covers various domains: news, medical, social
    media, and reviews.'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）从文本中提取命名实体及其标签。我们包括了在Ushio & Camacho-Collados ([2021](#bib.bib38))中研究的$6$个NER数据集，即，（1）CoNLL2003，（2）BioNLP2004，（3）WNUT2017，（4）MIT-Movie，（5）MIT-Restaurant，（6）BC5CDR，涵盖了新闻、医学、社交媒体和评论等多个领域。
- en: '2.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Relation Extraction (RE) extracts named entities, and in addition, identifies
    the relationships between them. We include $2$ popular datasets, (1) ADE (Gurulingappa
    et al., [2012](#bib.bib16)) and (2) CoNLL2004 (Carreras & Màrquez, [2004](#bib.bib2))
    representing RE on medical and news domain. We evaluate the performance of RE
    models on both relation detection and the detection of entities involved in the
    relations.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关系抽取（RE）提取命名实体，并且识别它们之间的关系。我们包括了$2$个流行的数据集，（1）ADE (Gurulingappa et al., [2012](#bib.bib16))
    和（2）CoNLL2004 (Carreras & Màrquez, [2004](#bib.bib2))，分别代表医学和新闻领域的关系抽取。我们评估了RE模型在关系检测以及涉及这些关系的实体检测上的表现。
- en: '3.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Event Extraction (EE) extracts event triggers and their arguments. We use the
    standard ACE2005 dataset (Walker et al., [2006](#bib.bib40)) for EE evaluation.
    We compare the model performance on both event trigger detection (T) evaluation
    task and trigger-augment pair detection (A) evaluation task.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 事件提取（EE）提取事件触发词及其论据。我们使用标准 ACE2005 数据集 (Walker et al., [2006](#bib.bib40)) 进行
    EE 评估。我们比较模型在事件触发词检测（T）评估任务和触发词-增强对检测（A）评估任务上的性能。
- en: '4.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Semantic Role Labeling (SRL) extracts predicates (verbs) and their arguments.
    We select the CoNLL2005 (Carreras & Màrquez, [2005](#bib.bib3)) dataset for SRL.
    We follow previous works to learn backbone LMs on samples from the Brown training
    dataset and then test them on Brown and WSJ test datasets.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语义角色标注（SRL）提取谓词（动词）及其论据。我们选择 CoNLL2005 (Carreras & Màrquez, [2005](#bib.bib3))
    数据集用于 SRL。我们按照以往的工作在 Brown 训练数据集上的样本上学习骨干语言模型，然后在 Brown 和 WSJ 测试数据集上测试它们。
- en: '5.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Aspect-based Sentiment Analysis (ABSA) extracts aspect terms and the sentiment
    polarity towards them. We select SemEval2014 (Pontiki et al., [2014](#bib.bib34))
    as the dataset for ABSA, with its two subsets: 14res and 14lap including reviews
    about restaurants and laptops.'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于方面的情感分析（ABSA）提取方面术语及其情感极性。我们选择 SemEval2014 (Pontiki et al., [2014](#bib.bib34))
    作为 ABSA 的数据集，其两个子集：14res 和 14lap 包括有关餐馆和笔记本电脑的评论。
- en: '6.'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Aspect Sentiment Triplet Extraction (ASTE) extracts aspect terms and the corresponding
    opinion terms that contain the sentiment polarity towards them. We use the same
    SemEval2014 dataset as for ABSA, on which aspect-sentiment triplets are further
    annotated by Xu et al. ([2020](#bib.bib44)).
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方面情感三元组提取（ASTE）提取方面术语及其对应的意见术语，这些术语包含对它们的情感极性。我们使用与 ABSA 相同的 SemEval2014 数据集，在该数据集上，方面-情感三元组由
    Xu et al. ([2020](#bib.bib44)) 进一步注释。
- en: For a fair comparison, we formalize all those tasks as $s=f_{IE}(X|l)$ pairs.
    For spans conflicting with each other, as we run label-wise extractions, we only
    keep the one with a higher BI sequence probability. For tasks that extractions
    are dependent on each other (e.g., RE, EE, SRL, ASTE), we follow Paolini et al.
    ([2021](#bib.bib32)) to run multi-stage extractions for these tasks. As ACE2005
    involves too many labels, we report the unlabeled performance on detecting the
    triggers and arguments for all methods for comparison.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公平比较，我们将所有这些任务形式化为 $s=f_{IE}(X|l)$ 对。对于彼此冲突的跨度，由于我们运行标签级别的提取，我们仅保留 BI 序列概率较高的那个。对于依赖彼此的任务（例如
    RE、EE、SRL、ASTE），我们按照 Paolini et al. ([2021](#bib.bib32)) 进行这些任务的多阶段提取。由于 ACE2005
    涉及太多标签，我们报告所有方法在检测触发词和论据上的无标签性能进行比较。
- en: '4.2 Evaluation Metric: Few-shot Fine-tuning Performance'
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估指标：少样本微调性能
- en: 'We use the few-shot fine-tuning performance on all IE tasks to evaluate the
    meta-model’s quality. Specifically, all methods in our evaluation will provide
    us a backbone LM. We then conduct few-shot fine-tuning from the training dataset
    for fine-tuning with sample details in Appendix [B](#A2 "Appendix B Few-shot Details
    ‣ MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction
    Tasks"). Finally, we evaluate them on the test dataset using the micro F1 score
    as the evaluation metric. For multi-task pre-training baselines, tasks without
    large-scale annotations (SRL, ABSA, ASTE) are out-of-distribution tasks.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用所有 IE 任务上的少样本微调性能来评估元模型的质量。具体来说，我们评估中的所有方法将为我们提供一个骨干语言模型。然后，我们从训练数据集中进行少样本微调，具体细节见附录 [B](#A2
    "Appendix B Few-shot Details ‣ MetaIE: Distilling a Meta Model from LLM for All
    Kinds of Information Extraction Tasks")。最后，我们使用微 F1 分数作为评估指标，在测试数据集上评估它们。对于多任务预训练基线，没有大规模注释的任务（SRL、ABSA、ASTE）属于分布外任务。'
- en: The default backbone LM we used for fine-tuning is RoBERTa-Large (Liu et al.,
    [2019](#bib.bib24)), which is a traditional bidirectional encoder used for learning
    IE tasks formalized as sequence tagging. The learning rate is set to $2\times
    10^{-5}$ for a single epoch to avoid overfitting.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于微调的默认骨干语言模型是 RoBERTa-Large (Liu et al., [2019](#bib.bib24))，它是一种用于学习形式化为序列标注的
    IE 任务的传统双向编码器。学习率设置为 $2\times 10^{-5}$，用于单个周期以避免过拟合。
- en: 4.3 Compared Methods
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 比较方法
- en: We first include a comparison with the teacher model GPT-3.5-turbo via LLM Prompting
    with in-context learning (ICL). For ICL, we provide $5$ examples in the prompt
    of our query. Based on previous discoveries on LLM-based IE (Peng et al., [2023](#bib.bib33);
    Wadhwa et al., [2023](#bib.bib39); Gao et al., [2024](#bib.bib13)), we shall expect
    that fine-tuned small LMs work better than the LLM.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过LLM提示结合上下文学习（ICL）与教师模型GPT-3.5-turbo进行比较。对于ICL，我们在查询提示中提供$5$个示例。根据之前在基于LLM的信息抽取（IE）中的发现（Peng等，[2023](#bib.bib33)；Wadhwa等，[2023](#bib.bib39)；Gao等，[2024](#bib.bib13)），我们预期微调的小型语言模型比LLM效果更好。
- en: We compare our MetaIE with a variety of methods from the following three categories
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将MetaIE与以下三类中的各种方法进行比较
- en: '1.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Vanilla LM fine-tuning (FT), i.e., directly using the vanilla pre-trained LM
    as the backbone LM in fine-tuning.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 香草语言模型微调（FT），即直接使用原始预训练语言模型作为微调中的主干语言模型。
- en: '2.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Task-level Meta-learning (ML)+FT. It is expected to have a strong performance
    to other datasets in the same IE task but poor generalization to other IE tasks.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务级别的元学习（ML）+FT。预计在相同IE任务的其他数据集上表现强劲，但在其他IE任务上泛化能力较差。
- en: •
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Transfer (Human) is a baseline that trains the backbone LM on large-scale human
    annotations of a specific IE task. Specifically, we use *FewNerd* (Ding et al.,
    [2021](#bib.bib6)) for NER, *FewRels* (Han et al., [2018](#bib.bib17)) for RE,
    and *FewEvents* (Ma et al., [2023](#bib.bib28)) for EE.
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转移（人工）是一个基准，它在特定IE任务的大规模人工标注上训练主干语言模型。具体而言，我们使用*FewNerd*（Ding等，[2021](#bib.bib6)）用于NER，*FewRels*（Han等，[2018](#bib.bib17)）用于RE，*FewEvents*（Ma等，[2023](#bib.bib28)）用于EE。
- en: •
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Transfer (LLM) uses the same datasets in Transfer (Human) but queries the LLM
    to annotate them following the human workflow. This baseline aims to compare the
    quality of annotation from humans and LLMs following the conventional annotation
    schema.
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转移（LLM）使用与转移（人工）相同的数据集，但查询LLM以按照人工工作流进行标注。这个基准旨在比较人类和LLM根据传统标注方案的标注质量。
- en: •
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Task Distillation distills from LLMs by querying answers for specific IE tasks.
    We implement this by providing in-context task-specific examples to control the
    LLM-produced data similar to the label IE task. The input texts are set to be
    the same as MetaIE to avoid bias.
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务蒸馏通过查询特定IE任务的答案来从LLM中蒸馏。我们通过提供上下文特定任务的示例来控制LLM生成的数据，使其类似于标签IE任务。输入文本设置为与MetaIE相同，以避免偏差。
- en: •
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: NER Distillation applies the model distilled following Task Distillation but
    tests them on non-NER tasks to evaluate its cross-task transferability.
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: NER蒸馏应用了根据任务蒸馏的模型，但在非NER任务上测试其跨任务转移性。
- en: '3.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: IE-level Meta-learning (ML)+FT aims to learn an IE model with strong transferability
    to all IE tasks. Our MetaIE also falls into this category.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: IE级别的元学习（ML）+FT旨在学习一个对所有IE任务具有强转移性的IE模型。我们的MetaIE也属于这一类别。
- en: •
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MultiIE merges the multiple human-annotated IE datasets (*FewNerd*, *FewRels*,
    *FewEvents*) to train a backbone LM, which represents a multi-task baseline with
    human annotations.
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: MultiIE将多个人工标注的IE数据集（*FewNerd*，*FewRels*，*FewEvents*）合并以训练一个主干语言模型，这代表了一个带有人工标注的多任务基准。
- en: •
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MAML (Finn et al., [2017a](#bib.bib10)) is a traditional meta-learning baseline
    that merges gradients on different datasets to build a model that can be quickly
    transferred to these datasets. We use the datasets in MultiIE for MAML in the
    experiment.
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: MAML（Finn等，[2017a](#bib.bib10)）是一个传统的元学习基准，它通过对不同数据集上的梯度进行融合，构建一个可以快速转移到这些数据集的模型。我们在实验中使用了MultiIE中的数据集进行MAML。
- en: For all baselines, the data number for meta-learning is controlled to the same
    as MetaIE by sampling towards a fair comparison.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有基准，元学习的数据量控制与MetaIE相同，以便进行公平比较。
- en: 4.4 Result
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 结果
- en: '| Category | Method | NER |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 方法 | NER |'
- en: '| ConLL$2003$ | MIT-Movie | MIT-Restaurant | BC5CDR |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| ConLL$2003$ | MIT-电影 | MIT-餐厅 | BC5CDR |'
- en: '| LLM Prompting | ICL | $59.68$ |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| LLM提示 | ICL | $59.68$ |'
- en: '| FT | Vanilla | $32.58$ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| FT | 原始 | $32.58$ |'
- en: '| Task-level ML+FT | Transfer |  |  |  |  |  |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 任务级别的ML+FT | 转移 |  |  |  |  |  |  |'
- en: '|     Human | $71.61$ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|     人工 | $71.61$ |'
- en: '|     LLM | $67.74$ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|     LLM | $67.74$ |'
- en: '| Task Distillation | 74.86 | 56.18 | 50.09 | 65.70 | 71.48 | $71.01$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 任务蒸馏 | 74.86 | 56.18 | 50.09 | 65.70 | 71.48 | $71.01$ |'
- en: '| IE-level ML+FT | MultiIE | $63.94$ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| IE级别的ML+FT | MultiIE | $63.94$ |'
- en: '| MAML | $66.97$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| MAML | $66.97$ |'
- en: '| MetaIE | $71.49$ | 65.64 | 71.33 | 75.21 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| MetaIE | $71.49$ | 65.64 | 71.33 | 75.21 |'
- en: '| Category | Method | RE (NER) | RE | EE |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 方法 | RE（NER） | RE | EE |'
- en: '| ADE | CoNLL$2004$ (A) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ADE | CoNLL$2004$（A） |'
- en: '| LLM Prompting | ICL | $63.55$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LLM提示 | ICL | $63.55$ |'
- en: '| FT | Vanilla | $25.97$ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| FT | 原始 | $25.97$ |'
- en: '| Task-level ML+FT | Transfer |  |  |  |  |  |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Task-level ML+FT | Transfer |  |  |  |  |  |  |'
- en: '|     Human | $41.56$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|     Human | $41.56$ |'
- en: '|     LLM | $35.43$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|     LLM | $35.43$ |'
- en: '| Task Distillation | $66.99$ |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Task Distillation | $66.99$ |'
- en: '| NER Distillation | $67.35$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| NER Distillation | $67.35$ |'
- en: '| IE-level ML+FT | MultiIE | $53.26$ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| IE-level ML+FT | MultiIE | $53.26$ |'
- en: '| MAML | $56.95$ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| MAML | $56.95$ |'
- en: '| MetaIE | 69.29 | 69.47 | 40.43 | 43.50 | $69.85$ | 36.83 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| MetaIE | 69.29 | 69.47 | 40.43 | 43.50 | $69.85$ | 36.83 |'
- en: '| Category | Method | SRL | ABSA | ASTE |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Category | Method | SRL | ABSA | ASTE |'
- en: '| Brown | WSJ | 14RES | 14LAP | 14RES | 14LAP |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Brown | WSJ | 14RES | 14LAP | 14RES | 14LAP |'
- en: '| LLM Prompting | ICL | $28.79$ |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| LLM Prompting | ICL | $28.79$ |'
- en: '| FT | Vanilla | $52.59$ |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| FT | Vanilla | $52.59$ |'
- en: '| Task-level ML+FT | NER Distillation | $43.65$ |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Task-level ML+FT | NER Distillation | $43.65$ |'
- en: '| IE-level ML+FT | MultiIE | $52.26$ |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| IE-level ML+FT | MultiIE | $52.26$ |'
- en: '| MAML | $52.69$ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| MAML | $52.69$ |'
- en: '| MetaIE | 54.50 | 58.49 | 50.96 | 39.71 | 43.30 | 43.10 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| MetaIE | 54.50 | 58.49 | 50.96 | 39.71 | 43.30 | 43.10 |'
- en: 'Table 2: Few-shot transferring performance (F1 score) of different meta-learning
    sources on IE tasks. Bold: Performance of the *small LM* that is not significantly
    different from the best one. $(p<0.05)$'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同元学习来源在 IE 任务上的少量转移性能（F1 分数）。粗体：与最佳结果没有显著差异的*小型语言模型*。$(p<0.05)$
- en: 'The result from our experiments is presented in Table [2](#S4.T2 "Table 2 ‣
    4.4 Result ‣ 4 Experiments ‣ MetaIE: Distilling a Meta Model from LLM for All
    Kinds of Information Extraction Tasks"). The vanilla model is poorly transferred
    by fine-tuning to all kinds of IE tasks. The model with meta-learning on a single
    IE task, NER, is only well-transferred to other NER datasets but poorly-transferred
    to other IE tasks. Among IE-level meta-learning methods, the MultiIE model can
    be transferred to in-domain IE tasks with outstanding performance but still fails
    to be transferred to out-of-domain IE tasks, either with regular pre-training
    or meta-learning frameworks like MAML. In contrast to all these baselines, our
    MetaIE shows a strong transferability to all IE tasks, especially on out-of-domain
    tasks for MultiIE. Thus, the experiment results are highly consistent with our
    claim in IE task transferability that wider pre-training label set $\mathcal{L}^{(IE)}$
    will enable macro transferability of the model to all IE tasks.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验的结果见表 [2](#S4.T2 "表 2 ‣ 4.4 结果 ‣ 4 实验 ‣ MetaIE：从 LLM 中提取的元模型用于各种信息提取任务")。普通模型经过微调后在各种
    IE 任务上的转移效果很差。单一 IE 任务 NER 上的元学习模型仅能很好地转移到其他 NER 数据集，但在其他 IE 任务上的转移效果很差。在 IE 级别的元学习方法中，MultiIE
    模型可以以出色的性能转移到领域内的 IE 任务，但在领域外的 IE 任务中的转移效果仍然不佳，无论是常规预训练还是像 MAML 这样的元学习框架。与所有这些基线相比，我们的
    MetaIE 显示出对所有 IE 任务，特别是 MultiIE 的领域外任务，具有强大的转移能力。因此，实验结果与我们在 IE 任务转移性上的声明高度一致，即更广泛的预训练标签集
    $\mathcal{L}^{(IE)}$ 将使模型对所有 IE 任务具有宏观转移能力。
- en: Besides the main discovery, we can also observe that LLM-based meta-learning
    outperforms the pre-training on human annotation. Take NER as an instance, while
    both label sets satisfy $\mathcal{L}\subset\mathcal{L}^{(NER)}$ proposed by LLMs
    is much more diverse than the fixed set in human annotated datasets, which again
    verifies the importance of the label distribution, even in task-specific distillation.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 除了主要发现之外，我们还可以观察到基于 LLM 的元学习在人工标注的预训练上表现更好。以 NER 为例，虽然两个标签集都满足 $\mathcal{L}\subset\mathcal{L}^{(NER)}$，但
    LLM 提出的标签集比人工标注数据集中的固定集要多样得多，这再次验证了标签分布的重要性，即使在任务特定的蒸馏中也是如此。
- en: The comparison with the teacher model also shows the student model generally
    outperforming the teacher model under few-shot supervision. Thus, we conclude
    fine-tuning a distilled student IE model to perform better than inference by the
    teacher LLMs with few-shot in-context examples. This further verifies the advantage
    of model distillation in meta-learning which enables more efficient and effective
    transfer.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与教师模型的比较还显示，学生模型在少量示例监督下通常优于教师模型。因此，我们得出结论，微调一个蒸馏的学生 IE 模型比通过少量上下文示例的教师 LLM
    进行推理表现更好。这进一步验证了模型蒸馏在元学习中的优势，它使得转移更加高效和有效。
- en: 5 Further Analysis
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 进一步分析
- en: 5.1 Size Analysis
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 尺寸分析
- en: We explore how the scale of the student model or the data number affects the
    distillation quality. For the model scale, we compare among RoBERTa-Small, RoBERTa-Base,
    and RoBERTa-Large. For the data scale, we increase the sampling size to $640K$
    and pre-train the student model with different amounts of data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了学生模型或数据规模如何影响蒸馏质量。对于模型规模，我们比较了 RoBERTa-Small、RoBERTa-Base 和 RoBERTa-Large。对于数据规模，我们增加了采样大小到
    $640K$ 并用不同数量的数据进行预训练。
- en: '![Refer to caption](img/1032c5e32e4d60be88d9007956172b6a.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1032c5e32e4d60be88d9007956172b6a.png)'
- en: 'Figure 3: The size analysis of the student model scale on different IE tasks
    and domains.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：学生模型规模在不同信息抽取任务和领域中的大小分析。
- en: '![Refer to caption](img/990147cd1e967a8d0aa6f7fc688ec25f.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/990147cd1e967a8d0aa6f7fc688ec25f.png)'
- en: 'Figure 4: The size analysis of the distillation data scale on different IE
    tasks and domains.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：不同信息抽取任务和领域中蒸馏数据规模的大小分析。
- en: 'The analysis of model size is presented in Figure [3](#S5.F3 "Figure 3 ‣ 5.1
    Size Analysis ‣ 5 Further Analysis ‣ MetaIE: Distilling a Meta Model from LLM
    for All Kinds of Information Extraction Tasks"), we can observe the performance
    of a student model can be scaled up by more parameters. Also, for simple tasks
    (like NER) with a general domain (like CoNLL2004), a tiny student model is competent
    for the distillation. However, for specific domains or complex tasks, the student
    model needs more parameters for generalization.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '模型规模的分析见图 [3](#S5.F3 "图 3 ‣ 5.1 尺寸分析 ‣ 5 进一步分析 ‣ MetaIE: 从 LLM 中蒸馏通用模型用于各种信息抽取任务")，我们可以观察到学生模型的性能可以通过更多的参数进行扩展。此外，对于简单任务（如
    NER）和通用领域（如 CoNLL2004），一个小的学生模型足以进行蒸馏。然而，对于特定领域或复杂任务，学生模型需要更多参数以实现泛化。'
- en: 'The analysis of data size is presented in Figure [4](#S5.F4 "Figure 4 ‣ 5.1
    Size Analysis ‣ 5 Further Analysis ‣ MetaIE: Distilling a Meta Model from LLM
    for All Kinds of Information Extraction Tasks"), we observe the existence of a
    threshold between $80K\sim 160K$ can significantly benefit the transferring.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '数据大小的分析见图 [4](#S5.F4 "图 4 ‣ 5.1 尺寸分析 ‣ 5 进一步分析 ‣ MetaIE: 从 LLM 中蒸馏通用模型用于各种信息抽取任务")，我们观察到
    $80K\sim 160K$ 的阈值显著有利于迁移。'
- en: 5.2 Distillation Framework Comparison
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 蒸馏框架比较
- en: We compare student models following different distillation frameworks (because
    of their architectures) to investigate how this factor affects the distillation
    effectiveness.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了遵循不同蒸馏框架（由于其架构）的学生模型，以研究这一因素如何影响蒸馏的效果。
- en: 'Seq2Seq implements the distillation by learning to extract a group of spans
    based on the IE label as in the distillation dataset. We include two Seq2Seq models:
    BART-Large (Lewis et al., [2020](#bib.bib22)) and T5-Base (Raffel et al., [2020](#bib.bib35)),
    which contain the same scale of parameters as in the RoBERTa-Large in our previous
    experiments.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Seq2Seq 通过学习根据 IE 标签提取一组跨度来实现蒸馏，就像在蒸馏数据集中一样。我们包括了两个 Seq2Seq 模型：BART-Large（Lewis
    等，[2020](#bib.bib22)）和 T5-Base（Raffel 等，[2020](#bib.bib35)），它们的参数规模与我们先前实验中的 RoBERTa-Large
    相同。
- en: 'CausalLM is similar to Seq2Seq but only uses the decoder model instead of the
    encoder-decoder as in Seq2Seq. We also include two CausalLM-based models with
    similar parameter scales: GPT2-Medium (Brown et al., [2020](#bib.bib1)) and OPT-350M
    (Zhang et al., [2022](#bib.bib45)).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: CausalLM 类似于 Seq2Seq，但仅使用解码器模型，而不是 Seq2Seq 中的编码器-解码器。我们还包括了两个基于 CausalLM 的模型，具有类似的参数规模：GPT2-Medium（Brown
    等，[2020](#bib.bib1)）和 OPT-350M（Zhang 等，[2022](#bib.bib45)）。
- en: We also include another sequence labeling model BERT-Large-Cased (Devlin et al.,
    [2019](#bib.bib5)) as a baseline to explore the influence of the backbone model
    quality on the learning performance. For all models, we pre-train them using our
    MetaIE dataset with the same hyperparameters.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还包括了另一个序列标注模型 BERT-Large-Cased（Devlin 等，[2019](#bib.bib5)）作为基准，以探索主干模型质量对学习性能的影响。对于所有模型，我们使用相同的超参数对其进行预训练，数据集为我们的
    MetaIE 数据集。
- en: '| Framework | Model | ConLL$2003$ | MIT-Movie | MIT-Restaurant | BC5CDR |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 框架 | 模型 | ConLL$2003$ | MIT-Movie | MIT-Restaurant | BC5CDR |'
- en: '| Seq-Labeling | BERT | $63.01$ |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 序列标注 | BERT | $63.01$ |'
- en: '| RoBERTa | 71.49 | 54.88 | $44.33$ | 65.64 | 71.33 | 75.21 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa | 71.49 | 54.88 | $44.33$ | 65.64 | 71.33 | 75.21 |'
- en: '| Seq2Seq | BART | 71.39 | $47.18$ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Seq2Seq | BART | 71.39 | $47.18$ |'
- en: '| T5 | $64.01$ |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| T5 | $64.01$ |'
- en: '| CausalLM | GPT | $57.20$ |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| CausalLM | GPT | $57.20$ |'
- en: '| OPT | $52.39$ |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| OPT | $52.39$ |'
- en: 'Table 3: Comparison between different frameworks on MetaIE distillation.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同框架在 MetaIE 蒸馏上的比较。
- en: 'We compare the performance of different distillation frameworks on NER as an
    example and the result is demonstrated in Table [3](#S5.T3 "Table 3 ‣ 5.2 Distillation
    Framework Comparison ‣ 5 Further Analysis ‣ MetaIE: Distilling a Meta Model from
    LLM for All Kinds of Information Extraction Tasks"). Sequence labeling models
    perform the best in few-shot transfer learning, which indicates their advantage
    in the distillation of meta-understanding of IE. This can be attributed to the
    consistency of sequence labeling with the extraction nature. We thus conclude
    distilling IE knowledge to a traditional sequence labeling model is better than
    those popular generative models. Between sequence labeling models, RoBERTa outperforms
    BERT, showing a better student model also benefits the distillation procedure.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '我们以NER为例比较了不同提炼框架的性能，结果如表[3](#S5.T3 "Table 3 ‣ 5.2 Distillation Framework Comparison
    ‣ 5 Further Analysis ‣ MetaIE: Distilling a Meta Model from LLM for All Kinds
    of Information Extraction Tasks")所示。序列标注模型在少量样本转移学习中表现最佳，这表明它们在IE元理解提炼中的优势。这可以归因于序列标注与提取本质的一致性。因此，我们得出结论，将IE知识提炼到传统序列标注模型中优于那些流行的生成模型。在序列标注模型之间，RoBERTa的表现优于BERT，表明更好的学生模型也有助于提炼过程。'
- en: 6 Limitation Discussion
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制讨论
- en: Efficiency The efficiency of the unified label-to-span will be $O(|\mathcal{L}^{(Task)}|)$
    is large. This efficiency is a trade-off for the ability to process any IE label,
    which enables the fast transfer of the BIO model to different IE tasks.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 效率 统一标签到跨度的效率为 $O(|\mathcal{L}^{(Task)}|)$ 当其较大时。这种效率是处理任何IE标签能力的权衡，这使得BIO模型能够快速转移到不同的IE任务。
- en: Bias in LLM-proposed labels As pointed out in previous works (Gallegos et al.,
    [2023](#bib.bib12); Fang et al., [2023](#bib.bib9)), LLMs have biases in their
    responses. This can also be observed in the statistics of our distillation dataset.
    Thus, the small meta-model might also inherit the bias and have better transferability
    to labels that LLMs prefer than others.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: LLM提议标签中的偏差 正如先前的研究（Gallegos等，[2023](#bib.bib12); Fang等，[2023](#bib.bib9)）指出，LLM的回答中存在偏差。这在我们的提炼数据集的统计中也有所观察。因此，小型元模型可能也会继承这种偏差，并且对LLM更倾向的标签具有更好的迁移能力。
- en: 7 Conclusions and Future Work
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与未来工作
- en: This paper presents a novel approach for distilling the meta-understanding of
    IE from LLMs into more efficient, smaller language models through a synthesized
    dataset, MetaIE. Our findings indicate that this method not only enhances the
    adaptability and efficiency of smaller models but also outperforms existing single-task
    and multi-task distillation methods in various IE tasks. The success of MetaIE
    underscores the potential of leveraging LLM’s meta-understanding to improve the
    performance and versatility of smaller models in complex tasks, offering a promising
    direction for future research in model distillation and IE. Future work will explore
    a better way for meta-learning by distilling from LLMs and other meta-tasks can
    be trained based on distillation.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种通过合成数据集MetaIE将LLM的元理解提炼为更高效、更小的语言模型的新方法。我们的研究结果表明，该方法不仅增强了小模型的适应性和效率，还在各种IE任务中优于现有的单任务和多任务提炼方法。MetaIE的成功突显了利用LLM的元理解来提升小模型在复杂任务中的表现和多样性的潜力，为模型提炼和IE领域的未来研究提供了有前景的方向。未来的工作将探讨通过从LLM中提炼以及基于提炼训练其他元任务的更好方法。
- en: References
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio
    Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), *Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020. URL
    [https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020) Tom B. Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared
    Kaplan、Prafulla Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda
    Askell、Sandhini Agarwal、Ariel Herbert-Voss、Gretchen Krueger、Tom Henighan、Rewon
    Child、Aditya Ramesh、Daniel M. Ziegler、Jeffrey Wu、Clemens Winter、Christopher Hesse、Mark
    Chen、Eric Sigler、Mateusz Litwin、Scott Gray、Benjamin Chess、Jack Clark、Christopher
    Berner、Sam McCandlish、Alec Radford、Ilya Sutskever 和 Dario Amodei。语言模型是少量样本学习者。在
    Hugo Larochelle、Marc’Aurelio Ranzato、Raia Hadsell、Maria-Florina Balcan 和 Hsuan-Tien
    Lin (编辑)，*神经信息处理系统进展 33：2020年神经信息处理系统年会，NeurIPS 2020，2020年12月6-12日，虚拟*，2020年。网址
    [https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)。
- en: 'Carreras & Màrquez (2004) Xavier Carreras and Lluís Màrquez. Introduction to
    the conll-2004 shared task: Semantic role labeling. In Hwee Tou Ng and Ellen Riloff
    (eds.), *Proceedings of the Eighth Conference on Computational Natural Language
    Learning, CoNLL 2004, Held in cooperation with HLT-NAACL 2004, Boston, Massachusetts,
    USA, May 6-7, 2004*, pp.  89–97\. ACL, 2004. URL [https://aclanthology.org/W04-2412/](https://aclanthology.org/W04-2412/).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carreras 和 Màrquez (2004) Xavier Carreras 和 Lluís Màrquez。介绍 conll-2004 共享任务：语义角色标注。在
    Hwee Tou Ng 和 Ellen Riloff (编辑)，*第八届计算自然语言学习会议论文集，CoNLL 2004，协办于 HLT-NAACL 2004，2004年5月6-7日，马萨诸塞州波士顿，美国*，第89–97页。ACL，2004年。网址
    [https://aclanthology.org/W04-2412/](https://aclanthology.org/W04-2412/)。
- en: 'Carreras & Màrquez (2005) Xavier Carreras and Lluís Màrquez. Introduction to
    the conll-2005 shared task: Semantic role labeling. In Ido Dagan and Daniel Gildea
    (eds.), *Proceedings of the Ninth Conference on Computational Natural Language
    Learning, CoNLL 2005, Ann Arbor, Michigan, USA, June 29-30, 2005*, pp.  152–164\.
    ACL, 2005. URL [https://aclanthology.org/W05-0620/](https://aclanthology.org/W05-0620/).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carreras 和 Màrquez (2005) Xavier Carreras 和 Lluís Màrquez。介绍 conll-2005 共享任务：语义角色标注。在
    Ido Dagan 和 Daniel Gildea (编辑)，*第九届计算自然语言学习会议论文集，CoNLL 2005，2005年6月29-30日，密歇根州安娜堡，美国*，第152–164页。ACL，2005年。网址
    [https://aclanthology.org/W05-0620/](https://aclanthology.org/W05-0620/)。
- en: 'Chen et al. (2015) Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, and Jun Zhao.
    Event extraction via dynamic multi-pooling convolutional neural networks. In *Proceedings
    of the 53rd Annual Meeting of the Association for Computational Linguistics and
    the 7th International Joint Conference on Natural Language Processing of the Asian
    Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing,
    China, Volume 1: Long Papers*, pp.  167–176\. The Association for Computer Linguistics,
    2015. doi: 10.3115/V1/P15-1017. URL [https://doi.org/10.3115/v1/p15-1017](https://doi.org/10.3115/v1/p15-1017).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 (2015) Yubo Chen、Liheng Xu、Kang Liu、Daojian Zeng 和 Jun Zhao。通过动态多池化卷积神经网络进行事件提取。在
    *第53届计算语言学协会年会及第七届亚洲自然语言处理联合国际会议论文集，ACL 2015，2015年7月26-31日，北京，中国，第1卷：长篇论文*，第167–176页。计算语言学协会，2015年。doi:
    10.3115/V1/P15-1017。网址 [https://doi.org/10.3115/v1/p15-1017](https://doi.org/10.3115/v1/p15-1017)。'
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: pre-training of deep bidirectional transformers for language
    understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,
    June 2-7, 2019, Volume 1 (Long and Short Papers)*, pp.  4171–4186\. Association
    for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova.
    BERT：用于语言理解的深度双向变换器预训练。见 Jill Burstein, Christy Doran 和 Thamar Solorio (编辑)，*2019年北美计算语言学协会会议：人类语言技术论文集，NAACL-HLT
    2019，明尼阿波利斯，MN，美国，2019年6月2-7日，卷1（长论文和短论文）*，第4171–4186页。计算语言学协会，2019年。doi: 10.18653/v1/n19-1423。网址
    [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423)。'
- en: 'Ding et al. (2021) Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han,
    Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. Few-nerd: A few-shot named entity
    recognition dataset. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli
    (eds.), *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing,
    ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021*, pp. 
    3198–3213\. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.ACL-LONG.248.
    URL [https://doi.org/10.18653/v1/2021.acl-long.248](https://doi.org/10.18653/v1/2021.acl-long.248).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding et al. (2021) Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han,
    Pengjun Xie, Haitao Zheng 和 Zhiyuan Liu. Few-nerd: 一个少量样本的命名实体识别数据集。见 Chengqing
    Zong, Fei Xia, Wenjie Li 和 Roberto Navigli (编辑)，*第59届计算语言学协会年会暨第11届国际自然语言处理联合会议论文集，ACL/IJCNLP
    2021，（卷1：长论文），虚拟会议，2021年8月1-6日*，第3198–3213页。计算语言学协会，2021年。doi: 10.18653/V1/2021.ACL-LONG.248。网址
    [https://doi.org/10.18653/v1/2021.acl-long.248](https://doi.org/10.18653/v1/2021.acl-long.248)。'
- en: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning,
    2023.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, Lei Li 和 Zhifang Sui. 关于上下文学习的调查，2023年。
- en: 'dos Santos & Guimarães (2015) Cícero Nogueira dos Santos and Victor Guimarães.
    Boosting named entity recognition with neural character embeddings. In Xiangyu
    Duan, Rafael E. Banchs, Min Zhang, Haizhou Li, and A. Kumaran (eds.), *Proceedings
    of the Fifth Named Entity Workshop, NEWS@ACL 2015, Beijing, China, July 31, 2015*,
    pp.  25–33\. Association for Computational Linguistics, 2015. doi: 10.18653/V1/W15-3904.
    URL [https://doi.org/10.18653/v1/W15-3904](https://doi.org/10.18653/v1/W15-3904).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'dos Santos & Guimarães (2015) Cícero Nogueira dos Santos 和 Victor Guimarães.
    利用神经字符嵌入提升命名实体识别。见 Xiangyu Duan, Rafael E. Banchs, Min Zhang, Haizhou Li, 和 A.
    Kumaran (编辑)，*第五届命名实体研讨会论文集，NEWS@ACL 2015，北京，中国，2015年7月31日*，第25–33页。计算语言学协会，2015年。doi:
    10.18653/V1/W15-3904。网址 [https://doi.org/10.18653/v1/W15-3904](https://doi.org/10.18653/v1/W15-3904)。'
- en: 'Fang et al. (2023) Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming
    Zhao, and Xiaohang Zhao. Bias of ai-generated content: An examination of news
    produced by large language models. *CoRR*, abs/2309.09825, 2023. doi: 10.48550/ARXIV.2309.09825.
    URL [https://doi.org/10.48550/arXiv.2309.09825](https://doi.org/10.48550/arXiv.2309.09825).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fang et al. (2023) Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming
    Zhao 和 Xiaohang Zhao. AI生成内容的偏见：对大型语言模型生成的新闻的检验。*CoRR*，abs/2309.09825，2023年。doi:
    10.48550/ARXIV.2309.09825。网址 [https://doi.org/10.48550/arXiv.2309.09825](https://doi.org/10.48550/arXiv.2309.09825)。'
- en: Finn et al. (2017a) Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic
    meta-learning for fast adaptation of deep networks. In Doina Precup and Yee Whye
    Teh (eds.), *Proceedings of the 34th International Conference on Machine Learning,
    ICML 2017, Sydney, NSW, Australia, 6-11 August 2017*, volume 70 of *Proceedings
    of Machine Learning Research*, pp.  1126–1135\. PMLR, 2017a. URL [http://proceedings.mlr.press/v70/finn17a.html](http://proceedings.mlr.press/v70/finn17a.html).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn et al. (2017a) Chelsea Finn, Pieter Abbeel 和 Sergey Levine. 模型无关的元学习用于深度网络的快速适应。见
    Doina Precup 和 Yee Whye Teh (编辑)，*第34届国际机器学习会议论文集，ICML 2017，悉尼，NSW，澳大利亚，2017年8月6-11日*，第70卷，*机器学习研究论文集*，第1126–1135页。PMLR，2017年。网址
    [http://proceedings.mlr.press/v70/finn17a.html](http://proceedings.mlr.press/v70/finn17a.html)。
- en: Finn et al. (2017b) Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic
    meta-learning for fast adaptation of deep networks. In Doina Precup and Yee Whye
    Teh (eds.), *Proceedings of the 34th International Conference on Machine Learning,
    ICML 2017, Sydney, NSW, Australia, 6-11 August 2017*, volume 70 of *Proceedings
    of Machine Learning Research*, pp.  1126–1135\. PMLR, 2017b. URL [http://proceedings.mlr.press/v70/finn17a.html](http://proceedings.mlr.press/v70/finn17a.html).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn等（2017b）Chelsea Finn、Pieter Abbeel和Sergey Levine。模型无关的元学习用于深度网络的快速适应。在Doina
    Precup和Yee Whye Teh（编辑），*第34届国际机器学习会议论文集，ICML 2017，澳大利亚新南威尔士州悉尼，2017年8月6-11日*，第70卷，*机器学习研究论文集*，第1126–1135页。PMLR，2017b。URL
    [http://proceedings.mlr.press/v70/finn17a.html](http://proceedings.mlr.press/v70/finn17a.html)。
- en: 'Gallegos et al. (2023) Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md. Mehrab
    Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K.
    Ahmed. Bias and fairness in large language models: A survey. *CoRR*, abs/2309.00770,
    2023. doi: 10.48550/ARXIV.2309.00770. URL [https://doi.org/10.48550/arXiv.2309.00770](https://doi.org/10.48550/arXiv.2309.00770).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gallegos等（2023）Isabel O. Gallegos、Ryan A. Rossi、Joe Barrow、Md. Mehrab Tanjim、Sungchul
    Kim、Franck Dernoncourt、Tong Yu、Ruiyi Zhang和Nesreen K. Ahmed。大型语言模型中的偏见与公平性：一项调查。*CoRR*，abs/2309.00770，2023。doi:
    10.48550/ARXIV.2309.00770。URL [https://doi.org/10.48550/arXiv.2309.00770](https://doi.org/10.48550/arXiv.2309.00770)。'
- en: 'Gao et al. (2024) Jun Gao, Huan Zhao, Wei Wang, Changlong Yu, and Ruifeng Xu.
    Eventrl: Enhancing event extraction with outcome supervision for large language
    models. *CoRR*, abs/2402.11430, 2024. doi: 10.48550/ARXIV.2402.11430. URL [https://doi.org/10.48550/arXiv.2402.11430](https://doi.org/10.48550/arXiv.2402.11430).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao等（2024）Jun Gao、Huan Zhao、Wei Wang、Changlong Yu和Ruifeng Xu。Eventrl: 通过结果监督增强大型语言模型的事件提取。*CoRR*，abs/2402.11430，2024。doi:
    10.48550/ARXIV.2402.11430。URL [https://doi.org/10.48550/arXiv.2402.11430](https://doi.org/10.48550/arXiv.2402.11430)。'
- en: Gokaslan & Cohen (2019) Aaron Gokaslan and Vanya Cohen. Openwebtext corpus.
    [http://Skylion007.github.io/OpenWebTextCorpus](http://Skylion007.github.io/OpenWebTextCorpus),
    2019.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gokaslan & Cohen（2019）Aaron Gokaslan和Vanya Cohen。Openwebtext语料库。 [http://Skylion007.github.io/OpenWebTextCorpus](http://Skylion007.github.io/OpenWebTextCorpus)，2019。
- en: 'Gou et al. (2021) Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng
    Tao. Knowledge distillation: A survey. *Int. J. Comput. Vis.*, 129(6):1789–1819,
    2021. doi: 10.1007/S11263-021-01453-Z. URL [https://doi.org/10.1007/s11263-021-01453-z](https://doi.org/10.1007/s11263-021-01453-z).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gou等（2021）Jianping Gou、Baosheng Yu、Stephen J. Maybank和Dacheng Tao。知识蒸馏：综述。*Int.
    J. Comput. Vis.*，129(6):1789–1819，2021。doi: 10.1007/S11263-021-01453-Z。URL [https://doi.org/10.1007/s11263-021-01453-z](https://doi.org/10.1007/s11263-021-01453-z)。'
- en: 'Gurulingappa et al. (2012) Harsha Gurulingappa, Abdul Mateen Rajput, Angus
    Roberts, Juliane Fluck, Martin Hofmann-Apitius, and Luca Toldo. Development of
    a benchmark corpus to support the automatic extraction of drug-related adverse
    effects from medical case reports. *J. Biomed. Informatics*, 45(5):885–892, 2012.
    doi: 10.1016/J.JBI.2012.04.008. URL [https://doi.org/10.1016/j.jbi.2012.04.008](https://doi.org/10.1016/j.jbi.2012.04.008).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gurulingappa等（2012）Harsha Gurulingappa、Abdul Mateen Rajput、Angus Roberts、Juliane
    Fluck、Martin Hofmann-Apitius和Luca Toldo。开发基准语料库以支持从医学病例报告中自动提取药物相关的不良反应。*J. Biomed.
    Informatics*，45(5):885–892，2012。doi: 10.1016/J.JBI.2012.04.008。URL [https://doi.org/10.1016/j.jbi.2012.04.008](https://doi.org/10.1016/j.jbi.2012.04.008)。'
- en: 'Han et al. (2018) Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan
    Liu, and Maosong Sun. Fewrel: A large-scale supervised few-shot relation classification
    dataset with state-of-the-art evaluation. In Ellen Riloff, David Chiang, Julia
    Hockenmaier, and Jun’ichi Tsujii (eds.), *Proceedings of the 2018 Conference on
    Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31
    - November 4, 2018*, pp.  4803–4809\. Association for Computational Linguistics,
    2018. doi: 10.18653/V1/D18-1514. URL [https://doi.org/10.18653/v1/d18-1514](https://doi.org/10.18653/v1/d18-1514).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han等（2018）Xu Han、Hao Zhu、Pengfei Yu、Ziyun Wang、Yuan Yao、Zhiyuan Liu和Maosong
    Sun。Fewrel: 一种大规模监督少样本关系分类数据集，具有最先进的评估方法。在Ellen Riloff、David Chiang、Julia Hockenmaier和Jun’ichi
    Tsujii（编辑）编著，*2018年自然语言处理经验方法会议论文集，比利时布鲁塞尔，2018年10月31日 - 11月4日*，第4803–4809页。计算语言学协会，2018。doi:
    10.18653/V1/D18-1514。URL [https://doi.org/10.18653/v1/d18-1514](https://doi.org/10.18653/v1/d18-1514)。'
- en: Hinton et al. (2015) Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling
    the knowledge in a neural network. *CoRR*, abs/1503.02531, 2015. URL [http://arxiv.org/abs/1503.02531](http://arxiv.org/abs/1503.02531).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton等（2015）Geoffrey E. Hinton、Oriol Vinyals和Jeffrey Dean。提取神经网络中的知识。*CoRR*，abs/1503.02531，2015。URL
    [http://arxiv.org/abs/1503.02531](http://arxiv.org/abs/1503.02531)。
- en: Ji et al. (2022) Bin Ji, Shasha Li, Shaoduo Gan, Jie Yu, Jun Ma, Huijun Liu,
    and Jing Yang. Few-shot named entity recognition with entity-level prototypical
    network enhanced by dispersedly distributed prototypes. In Nicoletta Calzolari,
    Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo
    Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio,
    Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico
    Santus, Francis Bond, and Seung-Hoon Na (eds.), *Proceedings of the 29th International
    Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea,
    October 12-17, 2022*, pp.  1842–1854\. International Committee on Computational
    Linguistics, 2022. URL [https://aclanthology.org/2022.coling-1.159](https://aclanthology.org/2022.coling-1.159).
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji et al. (2022) Bin Ji, Shasha Li, Shaoduo Gan, Jie Yu, Jun Ma, Huijun Liu,
    和 Jing Yang. 基于实体级原型网络的少样本命名实体识别，增强了分散分布的原型. 见 Nicoletta Calzolari, Chu-Ren Huang,
    Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi
    Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue,
    Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis
    Bond, 和 Seung-Hoon Na (编辑), *第29届国际计算语言学会议论文集，COLING 2022，庆州，韩国，2022年10月12-17日*,
    页码 1842–1854. 国际计算语言学委员会, 2022. URL [https://aclanthology.org/2022.coling-1.159](https://aclanthology.org/2022.coling-1.159).
- en: 'Katiyar & Cardie (2016) Arzoo Katiyar and Claire Cardie. Investigating lstms
    for joint extraction of opinion entities and relations. In *Proceedings of the
    54th Annual Meeting of the Association for Computational Linguistics, ACL 2016,
    August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers*. The Association for
    Computer Linguistics, 2016. doi: 10.18653/V1/P16-1087. URL [https://doi.org/10.18653/v1/p16-1087](https://doi.org/10.18653/v1/p16-1087).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Katiyar & Cardie (2016) Arzoo Katiyar 和 Claire Cardie. 调查LSTM在联合提取意见实体和关系中的应用.
    见 *第54届计算语言学协会年会论文集，ACL 2016，2016年8月7-12日，德国柏林，第1卷：长篇论文*. 计算语言学协会, 2016. doi:
    10.18653/V1/P16-1087. URL [https://doi.org/10.18653/v1/p16-1087](https://doi.org/10.18653/v1/p16-1087).'
- en: 'Kim et al. (2019) Jangho Kim, Yash Bhalgat, Jinwon Lee, Chirag Patel, and Nojun
    Kwak. QKD: quantization-aware knowledge distillation. *CoRR*, abs/1911.12491,
    2019. URL [http://arxiv.org/abs/1911.12491](http://arxiv.org/abs/1911.12491).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim et al. (2019) Jangho Kim, Yash Bhalgat, Jinwon Lee, Chirag Patel, 和 Nojun
    Kwak. QKD: 量化感知知识蒸馏. *CoRR*, abs/1911.12491, 2019. URL [http://arxiv.org/abs/1911.12491](http://arxiv.org/abs/1911.12491).'
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART:
    denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.
    Tetreault (eds.), *Proceedings of the 58th Annual Meeting of the Association for
    Computational Linguistics, ACL 2020, Online, July 5-10, 2020*, pp.  7871–7880\.
    Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.ACL-MAIN.703.
    URL [https://doi.org/10.18653/v1/2020.acl-main.703](https://doi.org/10.18653/v1/2020.acl-main.703).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, 和 Luke Zettlemoyer. BART: 去噪序列到序列的预训练，用于自然语言生成、翻译和理解.
    见 Dan Jurafsky, Joyce Chai, Natalie Schluter, 和 Joel R. Tetreault (编辑), *第58届计算语言学协会年会论文集，ACL
    2020，在线，2020年7月5-10日*, 页码 7871–7880. 计算语言学协会, 2020. doi: 10.18653/V1/2020.ACL-MAIN.703.
    URL [https://doi.org/10.18653/v1/2020.acl-main.703](https://doi.org/10.18653/v1/2020.acl-main.703).'
- en: 'Li et al. (2023) Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei
    Chang, and Yejin Choi. Symbolic chain-of-thought distillation: Small models can
    also ”think” step-by-step. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki
    (eds.), *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*,
    pp.  2665–2679\. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.150.
    URL [https://doi.org/10.18653/v1/2023.acl-long.150](https://doi.org/10.18653/v1/2023.acl-long.150).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023) Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei
    Chang, 和 Yejin Choi. 符号链式思维蒸馏：小模型也能逐步“思考”. 见 Anna Rogers, Jordan L. Boyd-Graber,
    和 Naoaki Okazaki (编辑), *第61届计算语言学协会年会论文集（第1卷：长篇论文），ACL 2023，多伦多，加拿大，2023年7月9-14日*,
    页码 2665–2679. 计算语言学协会, 2023. doi: 10.18653/V1/2023.ACL-LONG.150. URL [https://doi.org/10.18653/v1/2023.acl-long.150](https://doi.org/10.18653/v1/2023.acl-long.150).'
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:
    A robustly optimized BERT pretraining approach. *CoRR*, abs/1907.11692, 2019.
    URL [http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2019） Yinhan Liu、Myle Ott、Naman Goyal、Jingfei Du、Mandar Joshi、Danqi Chen、Omer
    Levy、Mike Lewis、Luke Zettlemoyer 和 Veselin Stoyanov。Roberta：一种稳健优化的BERT预训练方法。*CoRR*，abs/1907.11692，2019。网址
    [http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692)。
- en: 'Loshchilov & Hutter (2017) Ilya Loshchilov and Frank Hutter. SGDR: stochastic
    gradient descent with warm restarts. In *5th International Conference on Learning
    Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
    Proceedings*. OpenReview.net, 2017. URL [https://openreview.net/forum?id=Skq89Scxx](https://openreview.net/forum?id=Skq89Scxx).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov & Hutter（2017） Ilya Loshchilov 和 Frank Hutter。SGDR：具有暖启动的随机梯度下降。发表于
    *第5届国际学习表示会议，ICLR 2017，法国土伦，2017年4月24-26日，会议跟踪论文集*。OpenReview.net，2017。网址 [https://openreview.net/forum?id=Skq89Scxx](https://openreview.net/forum?id=Skq89Scxx)。
- en: Loshchilov & Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. In *7th International Conference on Learning Representations,
    ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*. OpenReview.net, 2019. URL [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov & Hutter（2019） Ilya Loshchilov 和 Frank Hutter。解耦权重衰减正则化。发表于 *第7届国际学习表示会议，ICLR
    2019，美国新奥尔良，2019年5月6-9日*。OpenReview.net，2019。网址 [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)。
- en: 'Lu et al. (2022) Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei
    Han, Le Sun, and Hua Wu. Unified structure generation for universal information
    extraction. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.),
    *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022*, pp.  5755–5772\.
    Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.395.
    URL [https://doi.org/10.18653/v1/2022.acl-long.395](https://doi.org/10.18653/v1/2022.acl-long.395).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu 等人（2022） Yaojie Lu、Qing Liu、Dai Dai、Xinyan Xiao、Hongyu Lin、Xianpei Han、Le
    Sun 和 Hua Wu。用于通用信息提取的统一结构生成。收录于 Smaranda Muresan、Preslav Nakov 和 Aline Villavicencio（编），*第60届计算语言学协会年会论文集（第1卷：长论文），ACL
    2022，爱尔兰都柏林，2022年5月22-27日*，第5755–5772页。计算语言学协会，2022。doi: 10.18653/V1/2022.ACL-LONG.395。网址
    [https://doi.org/10.18653/v1/2022.acl-long.395](https://doi.org/10.18653/v1/2022.acl-long.395)。'
- en: 'Ma et al. (2023) Yubo Ma, Zehao Wang, Yixin Cao, and Aixin Sun. Few-shot event
    detection: An empirical study and a unified view. In Anna Rogers, Jordan L. Boyd-Graber,
    and Naoaki Okazaki (eds.), *Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,
    July 9-14, 2023*, pp.  11211–11236\. Association for Computational Linguistics,
    2023. doi: 10.18653/V1/2023.ACL-LONG.628. URL [https://doi.org/10.18653/v1/2023.acl-long.628](https://doi.org/10.18653/v1/2023.acl-long.628).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等人（2023） Yubo Ma、Zehao Wang、Yixin Cao 和 Aixin Sun。少样本事件检测：实证研究与统一视角。收录于
    Anna Rogers、Jordan L. Boyd-Graber 和 Naoaki Okazaki（编），*第61届计算语言学协会年会论文集（第1卷：长论文），ACL
    2023，加拿大多伦多，2023年7月9-14日*，第11211–11236页。计算语言学协会，2023。doi: 10.18653/V1/2023.ACL-LONG.628。网址
    [https://doi.org/10.18653/v1/2023.acl-long.628](https://doi.org/10.18653/v1/2023.acl-long.628)。'
- en: 'Mirzadeh et al. (2020) Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir
    Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation
    via teacher assistant. In *The Thirty-Fourth AAAI Conference on Artificial Intelligence,
    AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
    Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial
    Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020*, pp.  5191–5198\.
    AAAI Press, 2020. doi: 10.1609/AAAI.V34I04.5963. URL [https://doi.org/10.1609/aaai.v34i04.5963](https://doi.org/10.1609/aaai.v34i04.5963).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mirzadeh 等人（2020） Seyed-Iman Mirzadeh、Mehrdad Farajtabar、Ang Li、Nir Levine、Akihiro
    Matsukawa 和 Hassan Ghasemzadeh。通过教师助理改进知识蒸馏。发表于 *第34届AAAI人工智能会议，AAAI 2020，第32届人工智能创新应用会议，IAAI
    2020，第10届AAAI人工智能教育进展研讨会，EAAI 2020，美国纽约，2020年2月7-12日*，第5191–5198页。AAAI出版社，2020。doi:
    10.1609/AAAI.V34I04.5963。网址 [https://doi.org/10.1609/aaai.v34i04.5963](https://doi.org/10.1609/aaai.v34i04.5963)。'
- en: Nichol et al. (2018) Alex Nichol, Joshua Achiam, and John Schulman. On first-order
    meta-learning algorithms. *CoRR*, abs/1803.02999, 2018. URL [http://arxiv.org/abs/1803.02999](http://arxiv.org/abs/1803.02999).
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nichol 等人 (2018) Alex Nichol, Joshua Achiam, 和 John Schulman. 一阶元学习算法。*CoRR*,
    abs/1803.02999, 2018年。网址 [http://arxiv.org/abs/1803.02999](http://arxiv.org/abs/1803.02999)。
- en: 'OpenAI (2023) OpenAI. GPT-4 technical report. *CoRR*, abs/2303.08774, 2023.
    doi: 10.48550/arXiv.2303.08774. URL [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI (2023) OpenAI. GPT-4 技术报告。*CoRR*, abs/2303.08774, 2023年。doi: 10.48550/arXiv.2303.08774。网址
    [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)。'
- en: Paolini et al. (2021) Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma,
    Alessandro Achille, Rishita Anubhai, Cícero Nogueira dos Santos, Bing Xiang, and
    Stefano Soatto. Structured prediction as translation between augmented natural
    languages. In *9th International Conference on Learning Representations, ICLR
    2021, Virtual Event, Austria, May 3-7, 2021*. OpenReview.net, 2021. URL [https://openreview.net/forum?id=US-TP-xnXI](https://openreview.net/forum?id=US-TP-xnXI).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paolini 等人 (2021) Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro
    Achille, Rishita Anubhai, Cícero Nogueira dos Santos, Bing Xiang, 和 Stefano Soatto.
    结构化预测作为增强自然语言之间的翻译。收录于 *第九届国际学习表征会议, ICLR 2021, 虚拟活动, 奥地利, 2021年5月3-7日*。OpenReview.net,
    2021年。网址 [https://openreview.net/forum?id=US-TP-xnXI](https://openreview.net/forum?id=US-TP-xnXI)。
- en: 'Peng et al. (2023) Letian Peng, Zihan Wang, and Jingbo Shang. Less than one-shot:
    Named entity recognition via extremely weak supervision. In Houda Bouamor, Juan
    Pino, and Kalika Bali (eds.), *Findings of the Association for Computational Linguistics:
    EMNLP 2023, Singapore, December 6-10, 2023*, pp.  13603–13616\. Association for
    Computational Linguistics, 2023. URL [https://aclanthology.org/2023.findings-emnlp.908](https://aclanthology.org/2023.findings-emnlp.908).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等人 (2023) Letian Peng, Zihan Wang, 和 Jingbo Shang. 少于一次学习: 通过极其弱监督的命名实体识别。收录于
    Houda Bouamor, Juan Pino, 和 Kalika Bali (编辑), *计算语言学协会发现: EMNLP 2023, 新加坡, 2023年12月6-10日*,
    第13603-13616页。计算语言学协会，2023年。网址 [https://aclanthology.org/2023.findings-emnlp.908](https://aclanthology.org/2023.findings-emnlp.908)。'
- en: 'Pontiki et al. (2014) Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris
    Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. Semeval-2014 task 4:
    Aspect based sentiment analysis. In Preslav Nakov and Torsten Zesch (eds.), *Proceedings
    of the 8th International Workshop on Semantic Evaluation, SemEval@COLING 2014,
    Dublin, Ireland, August 23-24, 2014*, pp.  27–35\. The Association for Computer
    Linguistics, 2014. doi: 10.3115/v1/s14-2004. URL [https://doi.org/10.3115/v1/s14-2004](https://doi.org/10.3115/v1/s14-2004).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pontiki 等人 (2014) Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris
    Papageorgiou, Ion Androutsopoulos, 和 Suresh Manandhar. Semeval-2014 任务 4: 基于方面的情感分析。收录于
    Preslav Nakov 和 Torsten Zesch (编辑), *第八届国际语义评估研讨会论文集, SemEval@COLING 2014, 都柏林,
    爱尔兰, 2014年8月23-24日*, 第27-35页。计算语言学协会，2014年。doi: 10.3115/v1/s14-2004。网址 [https://doi.org/10.3115/v1/s14-2004](https://doi.org/10.3115/v1/s14-2004)。'
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *J. Mach.
    Learn. Res.*, 21:140:1–140:67, 2020. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J. Liu. 通过统一的文本到文本变换器探索迁移学习的极限。*J.
    Mach. Learn. Res.*, 21:140:1–140:67, 2020年。网址 [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html)。
- en: 'Snell et al. (2017) Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical
    networks for few-shot learning. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,
    Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.),
    *Advances in Neural Information Processing Systems 30: Annual Conference on Neural
    Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA*,
    pp.  4077–4087, 2017. URL [https://proceedings.neurips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Snell 等人 (2017) Jake Snell, Kevin Swersky, 和 Richard S. Zemel. 原型网络用于少样本学习。收录于
    Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus,
    S. V. N. Vishwanathan, 和 Roman Garnett (编辑), *神经信息处理系统进展 30: 2017年神经信息处理系统年会,
    2017年12月4-9日, 长滩, CA, USA*, 第4077-4087页，2017年。网址 [https://proceedings.neurips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html)。'
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models. *CoRR*, abs/2307.09288, 2023. doi:
    10.48550/ARXIV.2307.09288. URL [https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov 和 Thomas Scialom。Llama 2：开放的基础模型和精调聊天模型。*CoRR*，abs/2307.09288，2023年。doi:
    10.48550/ARXIV.2307.09288。URL [https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288)。'
- en: 'Ushio & Camacho-Collados (2021) Asahi Ushio and Jose Camacho-Collados. T-NER:
    An all-round python library for transformer-based named entity recognition. In
    *Proceedings of the 16th Conference of the European Chapter of the Association
    for Computational Linguistics: System Demonstrations*, pp.  53–62, Online, April
    2021\. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-demos.7.
    URL [https://aclanthology.org/2021.eacl-demos.7](https://aclanthology.org/2021.eacl-demos.7).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ushio & Camacho-Collados (2021) Asahi Ushio 和 Jose Camacho-Collados。T-NER：一个全面的基于变换器的命名实体识别的
    Python 库。见于 *第16届欧洲计算语言学协会会议：系统展示论文集*，第 53–62 页，在线，2021年4月。计算语言学协会。doi: 10.18653/v1/2021.eacl-demos.7。URL
    [https://aclanthology.org/2021.eacl-demos.7](https://aclanthology.org/2021.eacl-demos.7)。'
- en: 'Wadhwa et al. (2023) Somin Wadhwa, Silvio Amir, and Byron C. Wallace. Revisiting
    relation extraction in the era of large language models. In Anna Rogers, Jordan L.
    Boyd-Graber, and Naoaki Okazaki (eds.), *Proceedings of the 61st Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers), ACL
    2023, Toronto, Canada, July 9-14, 2023*, pp.  15566–15589\. Association for Computational
    Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.868. URL [https://doi.org/10.18653/v1/2023.acl-long.868](https://doi.org/10.18653/v1/2023.acl-long.868).'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wadhwa et al. (2023) Somin Wadhwa, Silvio Amir 和 Byron C. Wallace。在大型语言模型时代重新审视关系提取。见于
    Anna Rogers, Jordan L. Boyd-Graber 和 Naoaki Okazaki（编），*第61届计算语言学协会年会（第1卷：长篇论文），ACL
    2023，多伦多，加拿大，2023年7月9-14日*，第 15566–15589 页。计算语言学协会，2023年。doi: 10.18653/V1/2023.ACL-LONG.868。URL
    [https://doi.org/10.18653/v1/2023.acl-long.868](https://doi.org/10.18653/v1/2023.acl-long.868)。'
- en: Walker et al. (2006) Christopher Walker, Stephanie Strassel, Julie Medero, and
    Kazuaki Maeda. ACE 2005 Multilingual Training Corpus. Web Download, 2006. URL
    [https://catalog.ldc.upenn.edu/LDC2006T06](https://catalog.ldc.upenn.edu/LDC2006T06).
    LDC Catalog No. LDC2006T06.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Walker et al. (2006) Christopher Walker, Stephanie Strassel, Julie Medero 和
    Kazuaki Maeda。ACE 2005 多语言训练语料库。网络下载，2006年。URL [https://catalog.ldc.upenn.edu/LDC2006T06](https://catalog.ldc.upenn.edu/LDC2006T06)。LDC目录编号：LDC2006T06。
- en: 'West et al. (2022) Peter West, Chandra Bhagavatula, Jack Hessel, Jena D. Hwang,
    Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. Symbolic
    knowledge distillation: from general language models to commonsense models. In
    Marine Carpuat, Marie-Catherine de Marneffe, and Iván Vladimir Meza Ruíz (eds.),
    *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle,
    WA, United States, July 10-15, 2022*, pp.  4602–4625\. Association for Computational
    Linguistics, 2022. doi: 10.18653/V1/2022.NAACL-MAIN.341. URL [https://doi.org/10.18653/v1/2022.naacl-main.341](https://doi.org/10.18653/v1/2022.naacl-main.341).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'West等（2022）彼得·韦斯特、尚德拉·巴哈伽瓦图拉、杰克·赫塞尔、贾娜·D·黄、姜丽伟、罗南·勒布拉斯、吕西明、肖恩·韦莱克、和叶进·崔。《符号知识蒸馏：从通用语言模型到常识模型》。在Marine
    Carpuat、Marie-Catherine de Marneffe、和伊万·弗拉基米尔·梅萨·鲁伊斯（编），*2022年北美计算语言学协会：人类语言技术会议论文集，NAACL
    2022，华盛顿州西雅图，美国，2022年7月10-15日*，第4602–4625页。计算语言学协会，2022年。doi: 10.18653/V1/2022.NAACL-MAIN.341。网址
    [https://doi.org/10.18653/v1/2022.naacl-main.341](https://doi.org/10.18653/v1/2022.naacl-main.341)。'
- en: 'West et al. (2023) Peter West, Ronan Le Bras, Taylor Sorensen, Bill Yuchen
    Lin, Liwei Jiang, Ximing Lu, Khyathi Chandu, Jack Hessel, Ashutosh Baheti, Chandra
    Bhagavatula, and Yejin Choi. Novacomet: Open commonsense foundation models with
    symbolic knowledge distillation. In Houda Bouamor, Juan Pino, and Kalika Bali
    (eds.), *Findings of the Association for Computational Linguistics: EMNLP 2023,
    Singapore, December 6-10, 2023*, pp.  1127–1149\. Association for Computational
    Linguistics, 2023. URL [https://aclanthology.org/2023.findings-emnlp.80](https://aclanthology.org/2023.findings-emnlp.80).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: West等（2023）彼得·韦斯特、罗南·勒布拉斯、泰勒·索伦森、比尔·俞晨·林、姜丽伟、吕西明、克雅提·昌杜、杰克·赫塞尔、阿什托什·巴赫提、尚德拉·巴哈伽瓦图拉、和叶进·崔。《Novacomet：带有符号知识蒸馏的开放常识基础模型》。在Houda
    Bouamor、胡安·皮诺、和卡利卡·巴利（编），*计算语言学协会发现：EMNLP 2023，新加坡，2023年12月6-10日*，第1127–1149页。计算语言学协会，2023年。网址
    [https://aclanthology.org/2023.findings-emnlp.80](https://aclanthology.org/2023.findings-emnlp.80)。
- en: 'Xu et al. (2023) Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu
    Zhao, Xian Wu, Yefeng Zheng, and Enhong Chen. Large language models for generative
    information extraction: A survey. *CoRR*, abs/2312.17617, 2023. doi: 10.48550/ARXIV.2312.17617.
    URL [https://doi.org/10.48550/arXiv.2312.17617](https://doi.org/10.48550/arXiv.2312.17617).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu等（2023）德容·徐、魏晨、温军·彭、赵超、童旭、项宇·赵、吴显、叶峰·郑、和陈恩洪。《生成信息提取的大型语言模型：综述》。*CoRR*，abs/2312.17617，2023年。doi:
    10.48550/ARXIV.2312.17617。网址 [https://doi.org/10.48550/arXiv.2312.17617](https://doi.org/10.48550/arXiv.2312.17617)。'
- en: 'Xu et al. (2020) Lu Xu, Hao Li, Wei Lu, and Lidong Bing. Position-aware tagging
    for aspect sentiment triplet extraction. In Bonnie Webber, Trevor Cohn, Yulan
    He, and Yang Liu (eds.), *Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020*, pp. 
    2339–2349\. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.EMNLP-MAIN.183.
    URL [https://doi.org/10.18653/v1/2020.emnlp-main.183](https://doi.org/10.18653/v1/2020.emnlp-main.183).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu等（2020）卢旭、李浩、陆伟、边力东。《面向位置的方面情感三元组提取》。在Bonnie Webber、Trevor Cohn、Yulan He、杨柳（编），*2020年自然语言处理实证方法会议论文集，EMNLP
    2020，线上，2020年11月16-20日*，第2339–2349页。计算语言学协会，2020年。doi: 10.18653/V1/2020.EMNLP-MAIN.183。网址
    [https://doi.org/10.18653/v1/2020.emnlp-main.183](https://doi.org/10.18653/v1/2020.emnlp-main.183)。'
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria
    Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained
    transformer language models. *CoRR*, abs/2205.01068, 2022. doi: 10.48550/ARXIV.2205.01068.
    URL [https://doi.org/10.48550/arXiv.2205.01068](https://doi.org/10.48550/arXiv.2205.01068).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等（2022）苏珊·张、斯蒂芬·罗勒、纳曼·戈亚尔、米克尔·阿尔特克斯、摩亚·陈、舒慧·陈、克里斯托弗·德万、莫娜·T·迪亚布、李显、希·维多利亚·林、托多尔·米哈伊洛夫、迈尔·奥特、山姆·施莱弗、库尔特·舒斯特、丹尼尔·西米格、普尼特·辛格·库拉、安贾利·斯里达、田陆·王、和卢克·泽特尔莫耶。《OPT：开放预训练变换器语言模型》。*CoRR*，abs/2205.01068，2022年。doi:
    10.48550/ARXIV.2205.01068。网址 [https://doi.org/10.48550/arXiv.2205.01068](https://doi.org/10.48550/arXiv.2205.01068)。'
- en: 'Zhou et al. (2023) Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, and Hoifung
    Poon. Universalner: Targeted distillation from large language models for open
    named entity recognition. *CoRR*, abs/2308.03279, 2023. doi: 10.48550/ARXIV.2308.03279.
    URL [https://doi.org/10.48550/arXiv.2308.03279](https://doi.org/10.48550/arXiv.2308.03279).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人（2023）Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, 和 Hoifung Poon.
    Universalner: 从大型语言模型中针对性的蒸馏用于开放命名实体识别。*CoRR*, abs/2308.03279, 2023. doi: 10.48550/ARXIV.2308.03279.
    URL [https://doi.org/10.48550/arXiv.2308.03279](https://doi.org/10.48550/arXiv.2308.03279)。'
- en: Appendix A Label-to-Span Formalization
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 标签到跨度的形式化
- en: NER
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: NER
- en: 'Person: John/B Smith/I loves/O his/O hometown/O ,/O Los/O Angeles/O'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 人物：John/B Smith/I 爱/O 他/O 的/O 家乡/O ，/O 洛杉矶/O 安吉利斯/O
- en: RE
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: RE
- en: 'Person: John/B Smith/I loves/O his/O hometown/O ,/O Los/O Angeles/O'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 人物：John/B Smith/I 爱/O 他/O 的/O 家乡/O ，/O 洛杉矶/O 安吉利斯/O
- en: 'John Smith births in: John/O Smith/O loves/O his/O hometown/O ,/O Los/B Angeles/I'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: John Smith 的出生地：John/O Smith/O 爱/O 他/O 的/O 家乡/O ，/O 洛杉矶/B 安吉利斯/I
- en: EE
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: EE
- en: 'Trigger: John/O Smith/O loves/B his/O hometown/O ,/O Los/O Angeles/O'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 触发词：John/O Smith/O 爱/B 他/O 的/O 家乡/O ，/O 洛杉矶/O 安吉利斯/O
- en: 'Argument for Trigger “loves”: John/O Smith/O loves/O his/O hometown/O ,/O Los/B
    Angeles/I'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 触发词 “爱”的论证：John/O Smith/O 爱/O 他/O 的/O 家乡/O ，/O 洛杉矶/B 安吉利斯/I
- en: SRL
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: SRL
- en: 'Verb: John/O Smith/O loves/B his/O hometown/O ,/O Los/O Angeles/O'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 动词：John/O Smith/O 爱/B 他/O 的/O 家乡/O ，/O 洛杉矶/O 安吉利斯/O
- en: 'A1 Argument for Verb “loves”: John/O Smith/O loves/O his/O hometown/B ,/O Los/O
    Angeles/O'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: A1 动词 “爱”的论证：John/O Smith/O 爱/O 他/O 的/O 家乡/B ，/O 洛杉矶/O 安吉利斯/O
- en: ABSA
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ABSA
- en: 'Positive Term: John/O Smith/O loves/O his/O hometown/O ,/O Los/B Angeles/I'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 积极术语：John/O Smith/O 爱/O 他/O 的/O 家乡/O ，/O 洛杉矶/B 安吉利斯/I
- en: ASTE
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ASTE
- en: 'Positive Opinion: John/O Smith/O loves/B his/O hometown/O ,/O Los/O Angeles/O'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 积极意见：John/O Smith/O 爱/B 他/O 的/O 家乡/O ，/O 洛杉矶/O 安吉利斯/O
- en: 'Aspect for Opinion “loves”: John/O Smith/O loves/O his/O hometown/O ,/O Los/B
    Angeles/I'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 方面对于意见 “爱”：John/O Smith/O 爱/O 他/O 的/O 家乡/O ，/O 洛杉矶/B 安吉利斯/I
- en: Appendix B Few-shot Details
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 少量样本详细信息
- en: NER
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NER
- en: samples $5$-shot examples that contain a certain type of entity for each entity
    type.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 样本 $5$-shot 示例包含每种实体类型的某种实体类型。
- en: RE
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RE
- en: samples $5$-shot examples that contain a certain type of relation for each relation
    type.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 样本 $5$-shot 示例包含每种关系类型的某种关系类型。
- en: EE
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: EE
- en: samples $5\%$ examples from the original training dataset.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 样本 $5\%$ 示例来自原始训练数据集。
- en: SRL
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SRL
- en: samples $50$-shot examples from the original training dataset.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 样本 $50$-shot 示例来自原始训练数据集。
- en: ABSA
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ABSA
- en: samples $5$-shot examples that contain terms with a certain sentiment polarity
    for each sentiment polarity type.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 样本 $5$-shot 示例包含每种情感极性类型的某些情感极性术语。
- en: ASTE
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ASTE
- en: samples $5$-shot examples that contain aspect-opinion triplet with a certain
    sentiment polarity for each sentiment polarity type.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 样本 $5$-shot 示例包含每种情感极性类型的某种方面-意见三元组。
