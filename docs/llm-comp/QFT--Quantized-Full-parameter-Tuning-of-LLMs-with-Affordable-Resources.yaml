- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:51:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:51:07'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'QFT: 量化全参数微调的LLMs，资源可承受'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.07147](https://ar5iv.labs.arxiv.org/html/2310.07147)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.07147](https://ar5iv.labs.arxiv.org/html/2310.07147)
- en: Zhikai Li^(1, 3) , Xiaoxuan Liu² , Banghua Zhu² , Zhen Dong^(2🖂), Qingyi Gu^(1🖂),
    Kurt Keutzer²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhikai Li^(1, 3) , Xiaoxuan Liu² , Banghua Zhu² , Zhen Dong^(2🖂), Qingyi Gu^(1🖂),
    Kurt Keutzer²
- en: ¹Institute of Automation, Chinese Academy of Sciences  ²University of California,
    Berkeley
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹中国科学院自动化研究所  ²加州大学伯克利分校
- en: ³School of Artificial Intelligence, University of Chinese Academy of Sciences
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ³中国科学院人工智能学院
- en: '{lizhikai2020,qingyi.gu}@ia.ac.cn'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{lizhikai2020,qingyi.gu}@ia.ac.cn'
- en: '{xiaoxuan_liu,banghua,zhendong,keutzer}@berkeley.edu ^(🖂) Corresponding authors.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{xiaoxuan_liu,banghua,zhendong,keutzer}@berkeley.edu ^(🖂) 通讯作者。'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Models (LLMs) have showcased remarkable impacts across a wide
    spectrum of natural language processing tasks. Fine-tuning these pre-trained models
    on downstream datasets provides further significant performance gains, but this
    process has been challenging due to its extraordinary resource requirements. To
    this end, existing efforts focus on parameter-efficient fine-tuning, which, unfortunately,
    fail to capitalize on the powerful potential of full-parameter fine-tuning. In
    this work, we propose QFT, a novel Quantized Full-parameter Tuning framework for
    LLMs that enables memory-efficient fine-tuning without harming performance. Our
    framework incorporates two novel ideas: (i) we adopt the efficient Lion optimizer,
    which only keeps track of the momentum and has consistent update magnitudes for
    each parameter, an inherent advantage for robust quantization; and (ii) we quantize
    all model states and store them as integer values, and present a gradient flow
    and parameter update scheme for the quantized weights. As a result, QFT reduces
    the model state memory to 21% of the standard solution while achieving comparable
    performance, e.g., tuning a LLaMA-7B model requires only $<$30GB of memory, satisfied
    by a single A6000 GPU.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在自然语言处理任务中展示了显著的影响。对这些预训练模型进行下游数据集的微调可以带来进一步的显著性能提升，但由于其极大的资源需求，这一过程具有挑战性。为此，现有的努力集中在参数高效微调上，但遗憾的是，这些方法未能充分利用全参数微调的强大潜力。在这项工作中，我们提出了QFT，一种新颖的量化全参数微调框架，用于LLMs，实现了内存高效的微调而不会损害性能。我们的框架包含两个新颖的理念：（i）我们采用高效的Lion优化器，它只跟踪动量，并且对每个参数有一致的更新幅度，这是鲁棒量化的内在优势；（ii）我们对所有模型状态进行量化并将其存储为整数值，同时提出了一个用于量化权重的梯度流和参数更新方案。结果，QFT将模型状态内存减少到标准解决方案的21%，同时实现了相当的性能，例如，微调LLaMA-7B模型只需<$30GB的内存，这一要求由单个A6000
    GPU满足。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs), with up to hundreds of billions of parameters,
    have left an indelible mark on the landscape of natural language processing tasks,
    showcasing their remarkable impacts across a diverse spectrum of applications
    and domains (Touvron et al., [2023a](#bib.bib41); [b](#bib.bib42); Brown et al.,
    [2020](#bib.bib1); Zhang et al., [2022](#bib.bib45)). Fine-tuning these pre-trained
    models on downstream datasets enhances their ability to understand and perform
    specific tasks (Zhao et al., [2023](#bib.bib46)). However, due to the enormous
    number of parameters, the fine-tuning process requires unprecedented resources.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），拥有多达数百亿个参数，已经在自然语言处理任务的领域中留下了不可磨灭的印记，展示了它们在多样化应用和领域中的显著影响（Touvron
    等，[2023a](#bib.bib41); [b](#bib.bib42); Brown 等，[2020](#bib.bib1); Zhang 等，[2022](#bib.bib45)）。对这些预训练模型进行下游数据集的微调可以提升它们理解和执行特定任务的能力（Zhao
    等，[2023](#bib.bib46)）。然而，由于参数数量庞大，微调过程需要前所未有的资源。
- en: Parameter-efficient fine-tuning, involving the tuning of only selected parameters,
    is deemed a practical choice for low-resource situations (Ding et al., [2022](#bib.bib8);
    Hu et al., [2021](#bib.bib15); Li & Liang, [2021](#bib.bib26)). Regrettably, owing
    to the limited representational capacity of the smaller parameter set, the outcomes
    of this approach often fall short of expectations (Lv et al., [2023](#bib.bib35)).
    Therefore, our emphasis is placed on full-parameter fine-tuning, with a keen interest
    in investigating memory optimization strategies to render it feasible on cost-effective
    resources.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调，即仅调整选择的参数，被认为是低资源情况下的实际选择（Ding et al., [2022](#bib.bib8); Hu et al.,
    [2021](#bib.bib15); Li & Liang, [2021](#bib.bib26)）。遗憾的是，由于较小参数集的表示能力有限，这种方法的结果往往未能达到预期（Lv
    et al., [2023](#bib.bib35)）。因此，我们将重点放在全参数微调上，并对研究内存优化策略以使其在成本效益资源上可行保持浓厚的兴趣。
- en: 'We begin by examining the full spectrum of memory usage in full-parameter fine-tuning,
    which can be categorized into three components: model states, activation, and
    other temporary or unusable memory. Model states, which include the model parameters
    (weights), gradients, and optimizer states (such as momentum and variances in
    Adam (Kingma & Ba, [2015](#bib.bib22))), are mandatory to store and consequently
    consume the majority of the memory (Rajbhandari et al., [2020](#bib.bib38)). For
    instance, when employing the standard fp32 training settings with the Adam optimizer,
    the memory allocation for model parameters, gradients, momentum, and variances
    amounts to 4 times the number of parameters. As a result, tuning a LLaMA-7B model
    necessitates a minimum of 100.4GB of RAM, which presents a formidable challenge
    given the limitations of current GPU capacities.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先考察了全参数微调中内存使用的全谱，这可以分为三个组成部分：模型状态、激活和其他临时或不可用的内存。模型状态，包括模型参数（权重）、梯度以及优化器状态（如Adam中的动量和方差（Kingma
    & Ba, [2015](#bib.bib22)）），是必需存储的，因此消耗了大部分内存（Rajbhandari et al., [2020](#bib.bib38)）。例如，在使用Adam优化器的标准fp32训练设置下，模型参数、梯度、动量和方差的内存分配总量是参数数量的4倍。因此，调整LLaMA-7B模型需要至少100.4GB的RAM，这在当前GPU容量的限制下是一个巨大的挑战。
- en: In this work, we are motivated to reduce the memory usage of all model states
    through quantized low-precision representations. First, instead of resorting to
    straightforward quantization, we expect an optimizer that simplifies the computation
    to replace Adam. Fortunately, the Lion optimizer (Chen et al., [2023](#bib.bib3))
    aligns almost perfectly with our expectations, as it only keeps track of the momentum
    and naturally eliminates the memory usage of the variances. And more importantly,
    its update has the same magnitude for each parameter, thus mitigating potential
    imbalances or inaccuracies in weight updates introduced by limited representation
    precision. Afterwards, we develop lightweight yet accurate quantizers for each
    model state, notably the dense-and-sparse quantizer (Kim et al., [2023](#bib.bib21))
    for weight parameters, which are then stored in the quantized integer format.
    During computation, these quantized representations are dequantized on-the-fly
    into the floating-point format to dynamically perform high-precision arithmetic.
    Moreover, we present a novel gradient flow scheme for the quantized weights to
    ensure proper error propagation and parameter updates in training.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们的目标是通过量化低精度表示来减少所有模型状态的内存使用。首先，我们期望使用一种简化计算的优化器来替代Adam，而不是直接采用简单的量化。幸运的是，Lion优化器（Chen
    et al., [2023](#bib.bib3)）几乎完全符合我们的期望，因为它只跟踪动量，自然消除了方差的内存使用。更重要的是，它的更新对每个参数的幅度相同，从而减轻了由于表示精度有限而引入的权重更新潜在的不平衡或不准确性。之后，我们为每个模型状态开发了轻量但准确的量化器，特别是针对权重参数的密集-稀疏量化器（Kim
    et al., [2023](#bib.bib21)），然后将其存储在量化整数格式中。在计算过程中，这些量化表示会即时解量化为浮点格式，以动态执行高精度算术操作。此外，我们提出了一种新的量化权重梯度流方案，以确保训练中的错误传播和参数更新的正确性。
- en: 'More specifically, our contribution can be summarized as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们的贡献可以总结如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose QFT, a novel Quantized Full-parameter Tuning framework for LLMs,
    which leverages quantization to optimize memory usage in fine-tuning without sacrificing
    performance. QFT can be seamlessly integrated into mainstream LLM training tools
    with minor modifications to a few training units, and is well compatible with
    existing memory optimization methods.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了QFT，这是一种新颖的量化全参数调优框架，用于LLMs，利用量化优化微调中的内存使用，而不牺牲性能。QFT可以通过对少量训练单元进行小幅修改，与主流LLM训练工具无缝集成，并且与现有的内存优化方法兼容。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We analyze the simplicity and memory efficiency of the Lion optimizer and confidently
    recommend it as the best choice for quantized fine-tuning. On this basis, we proceed
    to quantize all model states into the integer format, with each quantizer striking
    a balance between training accuracy and throughput. We also present a gradient
    flow scheme for the quantized weights.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们分析了Lion优化器的简洁性和内存效率，并自信地推荐它作为量化微调的最佳选择。在此基础上，我们将所有模型状态量化为整数格式，每个量化器在训练精度和吞吐量之间取得平衡。我们还提出了一种量化权重的梯度流方案。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We perform instruction tuning on the pre-trained LLaMA-2 models and extensively
    evaluate performance on various benchmarks. The results demonstrate that our QFT,
    with memory usage reduced to 21%, achieves comparable performance to standard
    floating-point training.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对预训练的LLaMA-2模型进行指令调优，并在各种基准测试上广泛评估性能。结果表明，我们的QFT在将内存使用减少到21%的情况下，表现出与标准浮点训练相当的性能。
- en: 2 Related Works
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Efficient Optimizer
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高效优化器
- en: The primary optimizers employed for training transformer models are the Adam
    family (Kingma & Ba, [2015](#bib.bib22); Loshchilov & Hutter, [2017](#bib.bib34)).
    They maintain a rolling average of the previous gradients to promote stable convergence
    in training. However, their optimizer states (momentum and variances) imposes
    an extra memory overhead proportional to the number of model parameters, and this
    becomes a significant burden as LLMs’ parameters increase. To overcome the memory
    challenges of model states, there are various memory-efficient schemes. LOMO (Lv
    et al., [2023](#bib.bib35)) utilizes a vanilla SGD optimizer for training LLMs,
    which unfortunately fails to ensure training performance due to the slow convergence
    and weak stability of SGD (Li et al., [2023](#bib.bib25)). Another imperfect solution
    is to utilize an Adafactor optimizer (Shazeer & Stern, [2018](#bib.bib40)), which,
    despite storing only aggregated information, is also beset by instability issues.
    In this work, we adopt the Lion optimizer (Chen et al., [2023](#bib.bib3)), relying
    on its advantage of only keeping track of the momentum but achieving comparable
    convergence to Adam. More importantly, thanks to the sign operation, its update
    has the same magnitude for each parameter, which gives it a great potential for
    robust quantization of gradients and optimizer states.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练变压器模型的主要优化器是Adam家族（Kingma & Ba，[2015](#bib.bib22)；Loshchilov & Hutter，[2017](#bib.bib34)）。它们保持先前梯度的滚动平均，以促进训练中的稳定收敛。然而，它们的优化器状态（动量和方差）带来了额外的内存开销，与模型参数的数量成正比，随着LLMs参数的增加，这成为了一个显著的负担。为了克服模型状态的内存挑战，有各种内存高效方案。LOMO（Lv等，[2023](#bib.bib35)）利用普通SGD优化器训练LLMs，但由于SGD的收敛慢和稳定性差，不幸的是未能确保训练性能。另一个不完美的解决方案是使用Adafactor优化器（Shazeer
    & Stern，[2018](#bib.bib40)），尽管只存储聚合信息，但也存在不稳定问题。在这项工作中，我们采用Lion优化器（Chen等，[2023](#bib.bib3)），依靠其仅跟踪动量的优势，但能达到与Adam相当的收敛性。更重要的是，由于符号操作，其更新对每个参数的幅度相同，这使其在梯度和优化器状态的鲁棒量化方面具有巨大潜力。
- en: Quantization for Memory Optimization
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化用于内存优化
- en: Most existing quantization methods focus on inference efficiency (Gholami et al.,
    [2022](#bib.bib13); Dong et al., [2019](#bib.bib9); [2020](#bib.bib10); Kim et al.,
    [2023](#bib.bib21); Li et al., [2022a](#bib.bib28); [b](#bib.bib29); Li & Gu,
    [2022](#bib.bib27); Jacob et al., [2018](#bib.bib19)), and recently, quantization
    is also believed to have great potential for optimizing training efficiency. Note
    that this research line is different from traditional quantization-aware training
    (QAT) (Jacob et al., [2018](#bib.bib19); Liu et al., [2023](#bib.bib33)). QAT
    inserts fake quantization nodes on weights and activations in training, where
    parameter arithmetic and storage retains the floating-point format, and thus training
    efficiency is not improved. As a comparison, quantization-based memory optimization
    methods, which attempt to utilize low-precision units to store parameters, can
    effectively reduce the memory budget in training, and thus have received increasing
    attention. Bitsandbytes (Dettmers et al., [2021](#bib.bib6)) introduces a block-wise
    quantization method to compress the memory of optimizer states. QLoRA (Dettmers
    et al., [2023](#bib.bib7)) uses quantized values to store frozen pre-training
    weights, keeping only the adapters in the floating-point format. In this work,
    we propose a novel memory-efficient full-parameter fine-tuning framework for LLMs,
    in which all model states are stored as quantized integer values, enabling comprehensive
    memory compression without sacrificing fine-tuning performance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 目前大多数量化方法关注于推理效率（Gholami et al., [2022](#bib.bib13); Dong et al., [2019](#bib.bib9);
    [2020](#bib.bib10); Kim et al., [2023](#bib.bib21); Li et al., [2022a](#bib.bib28);
    [b](#bib.bib29); Li & Gu, [2022](#bib.bib27); Jacob et al., [2018](#bib.bib19)），最近，量化也被认为具有优化训练效率的巨大潜力。注意，这一研究方向不同于传统的量化感知训练（QAT）（Jacob
    et al., [2018](#bib.bib19); Liu et al., [2023](#bib.bib33)）。QAT 在训练中在权重和激活上插入假量化节点，其中参数算术和存储保持浮点格式，因此训练效率没有得到提高。相比之下，基于量化的内存优化方法，尝试利用低精度单元存储参数，可以有效减少训练中的内存预算，因此受到越来越多的关注。Bitsandbytes（Dettmers
    et al., [2021](#bib.bib6)）引入了一种块级量化方法来压缩优化器状态的内存。QLoRA（Dettmers et al., [2023](#bib.bib7)）使用量化值存储冻结的预训练权重，仅保留适配器为浮点格式。在这项工作中，我们提出了一种新颖的内存高效全参数微调框架，用于LLMs，其中所有模型状态都存储为量化整数值，实现全面的内存压缩而不牺牲微调性能。
- en: Other Memory Optimization Methods
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 其他内存优化方法
- en: Other prominent memory optimization methods include offloading (Huang et al.,
    [2020](#bib.bib16); Wang et al., [2018](#bib.bib43); Peng et al., [2020](#bib.bib37))
    and gradient checkpointing (Chen et al., [2016](#bib.bib2); Kumar et al., [2019](#bib.bib24);
    Jain et al., [2020](#bib.bib20); Kirisame et al., [2020](#bib.bib23)). Activation
    offloading offloads activation to external memory (e.g., CPU memory). It is worth
    noting that offloading comes at the cost of transferring data to another storage,
    which can increase execution time. Gradient checkpointing is a technique that
    discards activations in the forward pass and recomputes them in the backward pass
    as needed. This approach involves a trade-off between memory usage and computation
    cost. In addition, there are also customized schemes proposed for training LLMs.
    LOMO (Lv et al., [2023](#bib.bib35)) fuses the gradient computation and the parameter
    update in one step. This method can reduce the memory usage of gradient tensors
    to O(1); however, there is a potential caveat as it is incompatible with gradient
    accumulation for scaling batch sizes, limiting it to unstable training with small
    batch sizes. In contrast, our framework is orthogonal and well compatible with
    all the above methods.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其他显著的内存优化方法包括卸载（Huang et al., [2020](#bib.bib16); Wang et al., [2018](#bib.bib43);
    Peng et al., [2020](#bib.bib37)）和梯度检查点（Chen et al., [2016](#bib.bib2); Kumar et
    al., [2019](#bib.bib24); Jain et al., [2020](#bib.bib20); Kirisame et al., [2020](#bib.bib23)）。激活卸载将激活转移到外部内存（例如，CPU
    内存）。值得注意的是，卸载会增加将数据传输到另一个存储的成本，这可能会增加执行时间。梯度检查点是一种在前向传播中丢弃激活并在反向传播中根据需要重新计算它们的技术。这种方法涉及内存使用和计算成本之间的权衡。此外，还有为训练LLMs提出的定制方案。LOMO（Lv
    et al., [2023](#bib.bib35)）将梯度计算和参数更新合并为一步。这种方法可以将梯度张量的内存使用降低到 O(1)；然而，它有一个潜在的警告，即它与梯度累积不兼容，以扩大批量大小，限制了其在小批量大小下的不稳定训练。相比之下，我们的框架是正交的，并且与所有上述方法良好兼容。
- en: 3 Methodology
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 3.1 Lion Optimizer
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 狮子优化器
- en: In a recent exploration of algorithm discovery through program search for neural
    network training, a novel optimization algorithm, Lion (EvoLved Sign Momentum),
    was conceived (Chen et al., [2023](#bib.bib3)). The method explores an expansive
    program space while implementing program selection and simplification strategies.
    Lion stands out due to its simplicity and memory-efficiency, only tracking momentum,
    differing from adaptive optimizers by employing a consistent magnitude update
    for each parameter using the sign operation. Comparative studies with established
    optimizers, like Adam (Kingma & Ba, [2015](#bib.bib22)) and Adafactor (Shazeer
    & Stern, [2018](#bib.bib40)), underscored Lion’s efficacy, leading to superior
    results in various domains, from image classification to language modeling. Particularly
    notable, Lion boosts the accuracy of Vision Transformers (ViT) on ImageNet, decreases
    pre-training compute on JFT, and surpasses Adam in training diffusion models.
    However, its advantages grow with increased training batch sizes and necessitate
    a lower learning rate than Adam, given the larger update norm resulting from the
    sign function.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的神经网络训练程序搜索算法发现研究中，提出了一种新颖的优化算法——Lion（EvoLved Sign Momentum），该算法由陈等人（[2023](#bib.bib3)）构想。该方法探索了广泛的程序空间，同时实施了程序选择和简化策略。Lion因其简单性和内存效率而脱颖而出，只跟踪动量，与自适应优化器不同的是，它通过符号操作对每个参数进行一致的幅度更新。与已建立的优化器（如Adam（Kingma
    & Ba, [2015](#bib.bib22)）和Adafactor（Shazeer & Stern, [2018](#bib.bib40)））的比较研究突显了Lion的有效性，在从图像分类到语言建模等各个领域都取得了优越的结果。特别值得注意的是，Lion提高了Vision
    Transformers（ViT）在ImageNet上的准确性，减少了JFT上的预训练计算，并在训练扩散模型时超越了Adam。然而，由于符号函数产生了较大的更新范数，其优势随着训练批次大小的增加而增长，并且需要比Adam更低的学习率。
- en: 'Designing quantized fine-tuning algorithms involves working with limited-precision
    representations of parameters, gradients and momentum. This can lead to several
    challenges, including increased sensitivity to noise, potential accumulation of
    rounding errors, and other precision-related issues. We find Lion more suitable
    for the task of quantized fine-tuning, due to the following reasons:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 设计量化微调算法涉及处理参数、梯度和动量的有限精度表示。这可能会导致几个挑战，包括对噪声的敏感性增加、舍入误差的潜在累积以及其他精度相关问题。我们发现Lion更适合量化微调任务，原因如下：
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Simplicity: Lion is simpler and more memory-efficient since it only keeps track
    of the momentum. This reduced complexity might be beneficial when dealing with
    quantized values, where added algorithmic intricacies can amplify quantization
    errors.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单性：Lion更简单、更节省内存，因为它只跟踪动量。这种降低的复杂性在处理量化值时可能是有益的，因为增加的算法复杂性会放大量化误差。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Consistent Update Magnitudes: Unlike adaptive optimizers, Lion ensures that
    updates have the same magnitude for each parameter, which is determined through
    the sign operation. In a quantized setting, this consistency can mitigate potential
    imbalances or inaccuracies in weight updates introduced by limited precision.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一致更新幅度：与自适应优化器不同，Lion确保每个参数的更新具有相同的幅度，这一幅度通过符号操作确定。在量化设置中，这种一致性可以缓解由有限精度引入的潜在不平衡或不准确。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Memory Efficiency: Memory usage is a common concern in quantized neural networks,
    especially when deploying on edge devices with constrained memory. Lion’s memory
    efficiency (only tracking momentum) makes it a potentially better fit for such
    quantized settings than optimizers like Adam, which track more state variables.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存效率：在量化神经网络中，内存使用是一个常见的问题，特别是在内存受限的边缘设备上部署时。Lion的内存效率（只跟踪动量）使其在这种量化环境下比像Adam这样的优化器更适合，因为Adam跟踪更多的状态变量。
- en: 3.2 Quantization
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 量化
- en: The Lion optimizer simplifies the composition of model states, which consist
    only of model weights, gradients, and optimizer momentum, resulting in a 25% reduction
    in memory usage compared to the memory-intensive Adam optimizer. However, it is
    imperative to recognize that these model states are still retained in the original
    floating-point format, a characteristic that can introduce redundant representations
    and, consequently, contribute to memory inefficiency. In light of this consideration,
    quantization, which involves the use of reduced-precision formats such as INT8
    to represent neural networks, emerges as a compelling avenue for further memory
    optimization.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Lion 优化器简化了模型状态的组成，这些状态仅包括模型权重、梯度和优化器动量，相比内存密集型的 Adam 优化器，内存使用减少了 25%。然而，必须认识到这些模型状态仍以原始浮点格式保留，这一特性可能引入冗余表示，从而导致内存低效。考虑到这一点，量化——即使用如
    INT8 等低精度格式来表示神经网络——成为进一步内存优化的有力途径。
- en: '![Refer to caption](img/3bef3b3d6d6148ea45902c9b35cd85dc.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3bef3b3d6d6148ea45902c9b35cd85dc.png)'
- en: 'Figure 1: Comparison between our QFT and traditional QAT in the computation
    and update procedures of weights. QAT stores the weights in the floating-point
    format and adds fake quantization nodes to the computation. Conversely, in our
    QFT, the weights are stored in the low-precision integer format, which are de-quantized
    on-the-fly into the floating-point format for computation, resulting in a significant
    reduction in memory usage.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们的 QFT 与传统 QAT 在权重计算和更新过程中的比较。QAT 将权重存储为浮点格式，并在计算中添加虚假的量化节点。相反，在我们的 QFT
    中，权重以低精度整数格式存储，这些权重在计算时即时解量化为浮点格式，从而显著减少了内存使用。
- en: 'The field of quantization methods primarily emphasizes improving model inference
    efficiency, with limited attention paid to reducing training overhead (Dettmers
    et al., [2021](#bib.bib6)). Our approach stands out through a comprehensive training
    memory compression, which is accomplished by quantizing all model states within
    the Lion optimizer and storing them as integer values. This sets our approach
    apart from traditional QAT (Jacob et al., [2018](#bib.bib19)). In our method,
    we initially store model parameters as quantized integers, whereas traditional
    QAT introduces fake quantization nodes to floating-point parameters. This distinction
    highlights the significance of our approach, as the latter method, with the reliance
    on fake quantization nodes, do not inherently enhance training efficiency. To
    more clearly demonstrate this difference, we present a comparison in Figure [1](#S3.F1
    "Figure 1 ‣ 3.2 Quantization ‣ 3 Methodology ‣ QFT: Quantized Full-parameter Tuning
    of LLMs with Affordable Resources").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 量化方法领域主要强调提高模型推理效率，对减少训练开销关注较少（Dettmers 等， [2021](#bib.bib6)）。我们的方法通过在 Lion
    优化器中对所有模型状态进行量化并将其存储为整数值，实现了全面的训练内存压缩。这使得我们的方法与传统 QAT（Jacob 等， [2018](#bib.bib19)）有所不同。在我们的方法中，我们最初将模型参数存储为量化整数，而传统
    QAT 向浮点参数引入虚假量化节点。这一区别凸显了我们方法的重要性，因为后者方法依赖虚假量化节点，不能本质上提高训练效率。为了更清楚地展示这种差异，我们在图
    [1](#S3.F1 "图 1 ‣ 3.2 量化 ‣ 3 方法 ‣ QFT：具有可承受资源的 LLM 的量化全参数调整") 中进行比较。
- en: '![Refer to caption](img/7ab4ae78129aaf703e321c250a80ae7e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7ab4ae78129aaf703e321c250a80ae7e.png)'
- en: 'Figure 2: Illustration of the model state distributions when training a LLaMA-2-7B
    model. The weight values are from the final down projection layer, and the gradient
    and momentum values are fetched on the 200th training step. The gradients and
    momentum show a canonical centralized distribution with few outliers, while the
    range of the weights increases by three orders of magnitude and exhibits extreme
    outliers, posing a significant challenge to quantization.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：训练 LLaMA-2-7B 模型时模型状态分布的示意图。权重值来自最终的降维层，梯度和动量值在第 200 步训练时获取。梯度和动量显示出经典的集中分布，只有少量异常值，而权重的范围增加了三个数量级，并且出现了极端的异常值，这给量化带来了重大挑战。
- en: 'We first perform an in-depth examination of the numerical distributions of
    the model weights, gradients and optimizer momentum, as shown in Figure [2](#S3.F2
    "Figure 2 ‣ 3.2 Quantization ‣ 3 Methodology ‣ QFT: Quantized Full-parameter Tuning
    of LLMs with Affordable Resources"). This comprehensive analysis forms the basis
    for designing appropriate quantization strategies. Remarkably, we prioritize lightweight
    quantizers to minimize the impact of de-quantization on the training throughput.
    In the following, we describe in detail the quantizers employed for different
    model states.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先对模型权重、梯度和优化器动量的数值分布进行深入分析，如图 [2](#S3.F2 "Figure 2 ‣ 3.2 Quantization ‣
    3 Methodology ‣ QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources")
    所示。这一全面分析为设计合适的量化策略奠定了基础。值得注意的是，我们优先考虑轻量级量化器，以减少去量化对训练吞吐量的影响。接下来，我们详细描述了针对不同模型状态使用的量化器。'
- en: Uniform Quantizer for Gradients and Momentum
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度和动量的均匀量化器
- en: 'The gradients and momentum values exhibit a central distribution with few outliers
    that deviate from the central range, allowing us to confidently utilize the uniform
    quantizer, which is regarded as the most fundamental quantization method. The
    uniform quantizer includes two essential procedures: quantization and de-quantization,
    which are defined as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度和动量值表现出一个中心分布，只有少量异常值偏离中心范围，使我们能够自信地使用均匀量化器，这被认为是最基本的量化方法。均匀量化器包括两个基本过程：量化和去量化，定义如下：
- en: '|  | $Quant:\bm{X}^{(\mathbb{Z})}=\text{clip}\left(\left\lfloor\frac{\bm{X}}{s}\right\rceil+z,0,2^{b}-1\right)\quad
    De\text{-}quant:\hat{\bm{X}}=s\left(\bm{X}^{(\mathbb{Z})}-z\right)\approx\bm{X}$
    |  | (1) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $Quant:\bm{X}^{(\mathbb{Z})}=\text{clip}\left(\left\lfloor\frac{\bm{X}}{s}\right\rceil+z,0,2^{b}-1\right)\quad
    De\text{-}quant:\hat{\bm{X}}=s\left(\bm{X}^{(\mathbb{Z})}-z\right)\approx\bm{X}$
    |  | (1) |'
- en: 'where $\bm{X}$ as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{X}$ 如下：
- en: '|  | $s=\frac{\max(\bm{X})-\min(\bm{X})}{2^{b}-1},\quad z=\left\lfloor-\frac{\min(\bm{X})}{s}\right\rceil$
    |  | (2) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $s=\frac{\max(\bm{X})-\min(\bm{X})}{2^{b}-1},\quad z=\left\lfloor-\frac{\min(\bm{X})}{s}\right\rceil$
    |  | (2) |'
- en: Dense-and-Sparse Quantizer for Weights
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稠密与稀疏权重量化器
- en: In contrast to gradients and momentum, whose probability distributions lend
    themselves well to quantization, the weights present a distinct challenge. This
    challenge arises from their considerably broader range, which is approximately
    three orders of magnitude larger than that of momentum, as well as the presence
    of pronounced outliers. This combination of factors makes the accurate quantization
    of weights a particularly formidable task (Kim et al., [2023](#bib.bib21); Frantar
    et al., [2022](#bib.bib11); Lin et al., [2023](#bib.bib31)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于梯度和动量，这些的概率分布适合量化，权重却呈现出独特的挑战。这种挑战源于权重范围的显著广泛，大约是动量的三倍，以及存在明显的异常值。这些因素的结合使得权重的准确量化成为一项特别艰巨的任务 (Kim
    et al., [2023](#bib.bib21); Frantar et al., [2022](#bib.bib11); Lin et al., [2023](#bib.bib31))。
- en: 'Upon revisiting the weight distribution, we uncover an intriguing pattern:
    if we set aside the extreme outliers, the remaining parameters coalesce into a
    notably compact distribution. To elucidate, the initial expansive range is predominantly
    influenced by these extreme outliers, with a striking statistic that 99% of the
    values cluster within a mere 20% of the overall range. This revelation serves
    as the catalyst for our approach, drawing inspiration from the dense-and-sparse
    quantizer presented in (Kim et al., [2023](#bib.bib21)). This method effectively
    ameliorates the issue of outliers by decomposing the weights into two distinct
    matrices: one dense and the other sparse. Formally, the method is defined as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当重新审视权重分布时，我们发现一个有趣的模式：如果排除极端异常值，其余参数集中成一个明显紧凑的分布。具体来说，初始的广泛范围主要受到这些极端异常值的影响，令人惊讶的是99%的值集中在整体范围的仅20%内。这一发现成为我们方法的催化剂，灵感来源于(Kim
    et al., [2023](#bib.bib21))中提出的稠密与稀疏量化器。该方法通过将权重分解为两个不同的矩阵来有效改善异常值问题：一个稠密矩阵和一个稀疏矩阵。正式地，方法定义如下：
- en: '|  | $\displaystyle\bm{W}=\bm{D}+\bm{S}\enspace\mathrm{s.t.}\enspace\bm{D}$
    |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{W}=\bm{D}+\bm{S}\enspace\mathrm{s.t.}\enspace\bm{D}$
    |  |'
- en: '|  | $\displaystyle\mathrm{and}\enspace\bm{S}$ |  | (3) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{and}\enspace\bm{S}$ |  | (3) |'
- en: where $\bm{D}$ are the thresholds for identifying outliers, which can be determined
    by the percentage of the range. It’s important to highlight that the matrix decomposition
    process is numerically straightforward, ensuring a high level of computational
    efficiency with minimal repercussions on training overhead.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\bm{D}$是用于识别异常值的阈值，可以通过范围的百分比来确定。值得强调的是，矩阵分解过程在数值上是简单的，确保了高水平的计算效率，同时对训练开销的影响最小。
- en: 'Subsequently, the dense matrix adheres to the simple uniform quantizer as described
    in Equation [1](#S3.E1 "In Uniform Quantizer for Gradients and Momentum ‣ 3.2
    Quantization ‣ 3 Methodology ‣ QFT: Quantized Full-parameter Tuning of LLMs with
    Affordable Resources"), while the sparse matrix retains its data in the floating-point
    format. Notably, given that the outliers constitute a relatively minor fraction,
    such as 1%, the sparse matrix can capitalize on memory-efficient storage techniques,
    like compressed sparse row (CSR) format, which can be instrumental in substantially
    mitigating memory overhead.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '随后，密集矩阵遵循如方程[1](#S3.E1 "In Uniform Quantizer for Gradients and Momentum ‣ 3.2
    Quantization ‣ 3 Methodology ‣ QFT: Quantized Full-parameter Tuning of LLMs with
    Affordable Resources")中所描述的简单均匀量化器，而稀疏矩阵则以浮点格式保留数据。值得注意的是，由于异常值构成了相对较小的比例，例如1%，稀疏矩阵可以利用内存高效存储技术，如压缩稀疏行（CSR）格式，这对于大幅减少内存开销非常有帮助。'
- en: 3.3 Overall Framework
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 总体框架
- en: In this section, we integrate the above efficient Lion optimizer and quantization
    methods and introduce a memory-efficient fine-tuning framework for LLMs. We provide
    a comprehensive description of each training phase, including forward propagation,
    backward propagation, and parameter update, with particular emphasis on the quantized
    gradient flow and the quantized optimizer step.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们整合了上述高效的Lion优化器和量化方法，并引入了一种内存高效的LLM微调框架。我们提供了每个训练阶段的全面描述，包括前向传播、反向传播和参数更新，特别强调量化梯度流和量化优化器步骤。
- en: Quantized Forward Propagation
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化前向传播
- en: 'Within our framework, we initially represent weights as quantized integer values
    to optimize memory utilization. During the execution of forward propagation, we
    de-quantize these low-precision weights into the floating-point format on-the-fly,
    thereby enabling high-precision arithmetic operations. For more clarity, we visualize
    this critical process in Figure [1](#S3.F1 "Figure 1 ‣ 3.2 Quantization ‣ 3 Methodology
    ‣ QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources").'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的框架中，我们首先将权重表示为量化整数值，以优化内存利用。在前向传播执行过程中，我们动态地将这些低精度权重解量化为浮点格式，从而实现高精度的算术运算。为更清晰地说明这一关键过程，我们在图[1](#S3.F1
    "Figure 1 ‣ 3.2 Quantization ‣ 3 Methodology ‣ QFT: Quantized Full-parameter Tuning
    of LLMs with Affordable Resources")中进行了可视化。'
- en: Quantized Backward Propagation
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化反向传播
- en: In the backward propagation phase, the final task loss is propagated forward
    from the last layer in a sequential manner, and throughout this process, the gradient
    of each parameter is computed. It’s worth noting that these gradients need to
    be kept in memory, as they serve as essential information for guiding subsequent
    updates to the parameters. However, in mainstream deep learning frameworks like
    PyTorch, only parameters in the floating-point format can possess the gradient
    property, while those in the integer format cannot. Consequently, we cannot compute
    and store the gradients using the automatic differentiation functionality (i.e.,
    AutoGrad) in such cases.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播阶段，最终任务损失从最后一层顺序地向前传播，在整个过程中，计算每个参数的梯度。值得注意的是，这些梯度需要保存在内存中，因为它们是指导后续参数更新的重要信息。然而，在主流深度学习框架如PyTorch中，只有浮点格式的参数可以具有梯度属性，而整数格式的参数则不行。因此，在这种情况下，我们不能使用自动微分功能（即AutoGrad）来计算和存储梯度。
- en: Algorithm 1 Gradient Flow of Quantized Weights
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 量化权重的梯度流
- en: $T_{l}$end for
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: $T_{l}$end for
- en: Algorithm 2 Quantized Lion Optimizer
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 算法2 量化Lion优化器
- en: $\beta_{1}$ store as INT8end for
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: $\beta_{1}$ 以INT8格式存储 end for
- en: 'To this end, we design the gradient flow of integer weights, as presented in
    Algorithm [1](#alg1 "Algorithm 1 ‣ Quantized Backward Propagation ‣ 3.3 Overall
    Framework ‣ 3 Methodology ‣ QFT: Quantized Full-parameter Tuning of LLMs with
    Affordable Resources"). As in forward propagation, we begin by de-quantizing the
    weights into the floating-point format. Subsequently, leveraging the gradient
    of the output, we apply the chain rule to compute the gradients of both the input
    and the weights. Beyond the computational aspect, preserving the gradients of
    the weights presents its own set of formidable challenges. To address this, we
    introduce a gradient retention scheme centered around the maintenance of a global
    stack. In this scheme, the gradient of each layer is sequentially pushed to the
    stack, following the backward flow of information during the backward propagation.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '为此，我们设计了整数权重的梯度流，如算法 [1](#alg1 "Algorithm 1 ‣ Quantized Backward Propagation
    ‣ 3.3 Overall Framework ‣ 3 Methodology ‣ QFT: Quantized Full-parameter Tuning
    of LLMs with Affordable Resources") 所示。在前向传播中，我们首先将权重去量化为浮点格式。随后，利用输出的梯度，我们应用链式法则计算输入和权重的梯度。除了计算方面，保持权重的梯度也面临着一系列严峻的挑战。为此，我们引入了一种以维护全局栈为核心的梯度保留方案。在该方案中，每层的梯度按顺序推入栈中，跟随反向传播过程中的信息流动。'
- en: Quantized Parameter Update
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化参数更新
- en: 'Ultimately, the parameter update are executed in accordance with the Lion optimizer
    procedures, with the notable difference that the gradients and momentum are stored
    in the integer format. The quantized optimizer step is outlined in Algorithm [2](#alg2
    "Algorithm 2 ‣ Quantized Backward Propagation ‣ 3.3 Overall Framework ‣ 3 Methodology
    ‣ QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources"). Initially,
    we pop the elements from the global stack to access and retrieve the gradients.
    It is essential to emphasize the exceptional computational efficiency of this
    popping process, as its computational complexity consistently remains at O(1),
    independent of the stack length. This efficiency arises from a distinct pattern:
    in the backward propagation phase, the gradients are sequentially pushed into
    the stack, beginning from the last layer. Conversely, in the optimizer step, the
    gradients are popped in a sequential manner, commencing from the first layer.
    This strategic arrangement ensures that the gradient of the current layer always
    occupies the last position in the stack, fully capitalizing on the first-in-last-out
    property inherent to stack data structures.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '最终，参数更新按照 Lion 优化器的程序执行，显著的不同之处在于梯度和动量以整数格式存储。量化优化器步骤在算法 [2](#alg2 "Algorithm
    2 ‣ Quantized Backward Propagation ‣ 3.3 Overall Framework ‣ 3 Methodology ‣ QFT:
    Quantized Full-parameter Tuning of LLMs with Affordable Resources") 中进行了概述。最初，我们从全局栈中弹出元素以访问和检索梯度。必须强调的是，这一弹出过程的计算效率非常高，其计算复杂度始终保持在
    O(1) 级别，不受栈长度影响。这种效率源于一个独特的模式：在反向传播阶段，梯度按顺序推入栈中，从最后一层开始。相反，在优化器步骤中，梯度按顺序从第一层开始弹出。这种策略安排确保当前层的梯度总是占据栈中的最后位置，充分利用了栈数据结构的先进后出特性。'
- en: 4 Experiments
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Experimental Setup
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Models and Benchmarks
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型和基准
- en: We conduct adequate evaluation of the proposed QFT by fine-tuning the advanced
    pre-trained model, LLaMA-2 (Touvron et al., [2023b](#bib.bib42)), including the
    7b and 13b versions. The few-shot performance of fine-tuned models is comprehensively
    evaluated on a variety of standard benchmarks, including ARC (Clark et al., [2018](#bib.bib5)),
    HellaSwag (Zellers et al., [2019](#bib.bib44)), MMLU (Hendrycks et al., [2020](#bib.bib14)),
    and TruthfulQA (Lin et al., [2021](#bib.bib32)). All results are obtained using
    the Language Model Evaluation Harness tool (Gao et al., [2021](#bib.bib12)). In
    addition, we also use MT-Bench (Zheng et al., [2023](#bib.bib47)) with GPT-4 scores
    to evaluate the conversational abilities of the models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过微调先进的预训练模型 LLaMA-2 (Touvron et al., [2023b](#bib.bib42)) 对所提出的 QFT 进行充分评估，包括
    7b 和 13b 版本。微调模型的少样本性能在多个标准基准上进行了全面评估，包括 ARC (Clark et al., [2018](#bib.bib5))、HellaSwag
    (Zellers et al., [2019](#bib.bib44))、MMLU (Hendrycks et al., [2020](#bib.bib14))
    和 TruthfulQA (Lin et al., [2021](#bib.bib32))。所有结果均使用 Language Model Evaluation
    Harness 工具 (Gao et al., [2021](#bib.bib12)) 获得。此外，我们还使用 MT-Bench (Zheng et al.,
    [2023](#bib.bib47)) 和 GPT-4 分数来评估模型的对话能力。
- en: Dataset Preparation
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集准备
- en: In our experiment, we utilized a dataset comprising 94.1K shareGPT entries (HuggingFace,
    [2023b](#bib.bib18); shareGPT, [2023](#bib.bib39)), which encompass user interactions
    with chatGPT. We adopted the data cleaning procedures from Fastchat (Chiang et al.,
    [2023](#bib.bib4)), converting HTML to markdown, eliminating non-English conversations,
    and segmenting extended dialogues into sequences capped at a length of 2048.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们利用了一个包含94.1K shareGPT 条目的数据集（HuggingFace, [2023b](#bib.bib18); shareGPT,
    [2023](#bib.bib39)），其中涵盖了用户与 chatGPT 的互动。我们采用了来自 Fastchat 的数据清理程序（Chiang et al.,
    [2023](#bib.bib4)），将 HTML 转换为 markdown，剔除非英语对话，并将较长的对话拆分为最长 2048 的序列。
- en: Baseline Methods
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线方法
- en: We evaluate QFT in terms of both training memory and performance. For training
    memory, QFT is compared to floating-point Adam (Kingma & Ba, [2015](#bib.bib22)),
    Lion (Chen et al., [2023](#bib.bib3)), as well as bitsandbytes with quantized
    optimizer states (Dettmers et al., [2021](#bib.bib6)). For the performance of
    instruction tuning, we take Vicuna (Chiang et al., [2023](#bib.bib4)), which performs
    full-parameter fine-tuning in the floating-point format, as the baseline method.
    For a fair comparison, we reproduce its results using the same dataset as QFT.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从训练内存和性能两个方面评估 QFT。对于训练内存，QFT 与浮点 Adam（Kingma & Ba, [2015](#bib.bib22)）、Lion（Chen
    et al., [2023](#bib.bib3)）以及具有量化优化器状态的 bitsandbytes（Dettmers et al., [2021](#bib.bib6)）进行比较。对于指令调优的性能，我们以
    Vicuna（Chiang et al., [2023](#bib.bib4)）作为基线方法，该方法在浮点格式中进行全参数微调。为了公平比较，我们使用与 QFT
    相同的数据集重现其结果。
- en: Training Details
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练细节
- en: 'During training, we apply channel-wise quantization for all quantizers of model
    states. The threshold $T$ in the dense-and-sparse quantizer is obtained from 1%
    of the distribution range (please see Appendix [A.1](#A1.SS1 "A.1 Discussion on
    Outlier Thresholds of Weight Quantizer ‣ Appendix A Appendix ‣ QFT: Quantized
    Full-parameter Tuning of LLMs with Affordable Resources") for details). The training
    parameters are set to align with Vicuna’s settings: the global batch size is 128,
    the learning rate is 2e-5, and the total number of epochs is 3.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '在训练过程中，我们对模型状态的所有量化器应用通道级量化。密集与稀疏量化器中的阈值 $T$ 取自分布范围的 1%（有关详细信息，请参见附录 [A.1](#A1.SS1
    "A.1 Discussion on Outlier Thresholds of Weight Quantizer ‣ Appendix A Appendix
    ‣ QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources")）。训练参数设置与
    Vicuna 的设置对齐：全局批次大小为 128，学习率为 2e-5，总共 3 个 epoch。'
- en: 'Table 1: Memory usage (in GB) when fine-tuning the LLaMA-2-7b model using different
    methods. We report the full spectrum of memory profiles, as well as the total
    allocated memory and peak allocated memory. For model states, the Lion optimizer
    in floating-point format provides a 25% memory reduction, and further, our QFT
    introduces quantization that reduces the memory to 21% of the Adam optimizer,
    allowing for fine-tuning within 30GB of RAM.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 使用不同方法微调 LLaMA-2-7b 模型时的内存使用情况（单位：GB）。我们报告了内存配置的完整范围，以及总分配内存和峰值分配内存。对于模型状态，浮点格式的
    Lion 优化器提供了 25% 的内存减少，并且我们的 QFT 引入的量化将内存减少到 Adam 优化器的 21%，使得在 30GB RAM 内完成微调成为可能。'
- en: '| Method | Weights | Gradients | Optimizer States | Activation | Total | Peak
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 权重 | 梯度 | 优化器状态 | 激活 | 总计 | 峰值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Weight Copies | Momentum | Variances |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 权重副本 | 动量 | 方差 |'
- en: '| --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Adam | 25.1 | 25.1 | - | 25.1 | 25.1 | 3.75 | 104 | 129 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Adam | 25.1 | 25.1 | - | 25.1 | 25.1 | 3.75 | 104 | 129 |'
- en: '| Adam-mixed | 12.6 | 12.6 | 25.1 | 25.1 | 25.1 | 3.75 | 104 | 123 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Adam-mixed | 12.6 | 12.6 | 25.1 | 25.1 | 25.1 | 3.75 | 104 | 123 |'
- en: '| bitsandbytes | 12.6 | 12.6 | 25.1 | 6.31 | 6.31 | 3.75 | 66.6 | 86.6 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| bitsandbytes | 12.6 | 12.6 | 25.1 | 6.31 | 6.31 | 3.75 | 66.6 | 86.6 |'
- en: '| Lion | 25.1 | 25.1 | - | 25.1 | - | 3.75 | 79.1 | 101 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Lion | 25.1 | 25.1 | - | 25.1 | - | 3.75 | 79.1 | 101 |'
- en: '| QFT | 7.42 | 7.06 | - | 7.06 | - | 3.75 | 25.3 | 28.9 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| QFT | 7.42 | 7.06 | - | 7.06 | - | 3.75 | 25.3 | 28.9 |'
- en: 4.2 Memory Profile
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 内存配置
- en: 'We start by discussing the memory usage using different methods, and the results
    of fine-tuning the LLaMA-2-7b model are reported in Table [1](#S4.T1 "Table 1
    ‣ Training Details ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ QFT: Quantized Full-parameter
    Tuning of LLMs with Affordable Resources"). In the training that employs the Adam
    optimizer with standard settings, it becomes evident that the memory consumption
    becomes significantly substantial. Specifically, the model weights, gradients,
    momentum, and variances each occupy a considerable 25.1GB of RAM, which is 4 times
    the model parameters, resulting in a horrible resource burden. Remarkably, this
    memory issue persists when employing the Adam optimizer with mixed precision settings.
    Despite the fact that the numerical precision of both weights and gradients experiences
    a 50% reduction during the forward and backward computations, the necessity to
    uphold full-precision weight copies within the optimizer states remains paramount.
    This stringent requirement is essential to guarantee the stability of parameter
    updates, as discussed in detail in Appendix [A.2](#A1.SS2 "A.2 Discussion on Training
    Stability ‣ Appendix A Appendix ‣ QFT: Quantized Full-parameter Tuning of LLMs
    with Affordable Resources"), and thus the goal of conserving memory remains unattainable.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先讨论了使用不同方法的内存使用情况，并在表格[1](#S4.T1 "Table 1 ‣ Training Details ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ QFT: Quantized Full-parameter Tuning of LLMs with Affordable
    Resources")中报告了微调LLaMA-2-7b模型的结果。在使用标准设置的Adam优化器进行训练时，很明显，内存消耗变得非常庞大。具体来说，模型的权重、梯度、动量和方差各自占用大量的25.1GB
    RAM，这相当于模型参数的4倍，导致了可怕的资源负担。值得注意的是，当使用混合精度设置的Adam优化器时，这一内存问题依然存在。尽管在前向和反向计算过程中权重和梯度的数值精度减少了50%，但必须在优化器状态中维持全精度的权重副本仍然至关重要。正如在附录[A.2](#A1.SS2
    "A.2 Discussion on Training Stability ‣ Appendix A Appendix ‣ QFT: Quantized Full-parameter
    Tuning of LLMs with Affordable Resources")中详细讨论的那样，这一严格要求对确保参数更新的稳定性是必要的，因此内存节省的目标仍然难以实现。'
- en: The Lion optimizer simplifies the optimizer states by only keeping track of
    the momentum, resulting in a noteworthy reduction in memory usage, 25% less than
    that of the Adam optimizer. Hence, it takes up 25% less memory than the Adam optimizer.
    Notably, the model states still retain the floating-point format, and this redundant
    representation offers additional opportunities for optimization. To this end,
    bitsandbytes employs quantization methods to convert the momentum and variances
    into the integer format, resulting in an impressive memory savings of 37 GB. Nevertheless,
    the retention of floating-point weights and gradients remains a hurdle, preventing
    complete memory conservation and continuing to strain the training resources.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Lion优化器通过仅跟踪动量来简化优化器状态，从而显著减少了内存使用，减少了25%相比于Adam优化器。因此，它比Adam优化器节省了25%的内存。值得注意的是，模型状态仍然保留浮点格式，这种冗余表示提供了额外的优化机会。为此，bitsandbytes采用量化方法将动量和方差转换为整数格式，实现了令人印象深刻的37GB内存节省。然而，浮点权重和梯度的保留仍然是一个障碍，阻碍了完全的内存节省，并继续对训练资源造成压力。
- en: Our QFT, built on top of the Lion optimizer, employs a comprehensive quantization
    scheme encompassing all model states, including weights, gradients, and optimizer
    momentum. These parameters can be efficiently stored in the low-precision integer
    format. This allows the GPU to allocate only 21.5GB of RAM to store these parameters,
    marking a remarkable reduction to a mere 21% in comparison to the memory requirements
    of the Adam optimizer. During the practical training process, when taking into
    account factors such as activation, as well as several caches and memory fragments,
    the peak allocated memory remains comfortably below 30GB, allowing us to fine-tune
    within budget-friendly computing resources.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的QFT，基于Lion优化器，采用了一种全面的量化方案，涵盖了所有模型状态，包括权重、梯度和优化器动量。这些参数可以有效地以低精度整数格式存储。这使得GPU只需分配21.5GB的RAM来存储这些参数，相比于Adam优化器的内存需求，减少了显著的21%。在实际训练过程中，考虑到激活等因素以及多个缓存和内存碎片，峰值分配的内存仍然远低于30GB，这使得我们可以在预算友好的计算资源内进行微调。
- en: 4.3 Performance Evaluation
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 性能评估
- en: 'Table 2: Few-shot performance of different models on various standard benchmarks.
    Here, the number of shots is aligned to Open LLM Leaderboard (HuggingFace, [2023a](#bib.bib17)).
    We take the pre-trained LLaMA-2 model as the baseline and compare the instruction
    tuning results of our QFT and Vicuna. Our QFT, with less resource consumption,
    encouragingly provides substantial improvement over pre-trained models and rivals
    the outcomes of full-precision tuning.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同模型在各种标准基准上的少-shot 性能。在这里，shot 数量与 Open LLM Leaderboard (HuggingFace, [2023a](#bib.bib17))
    对齐。我们以预训练的 LLaMA-2 模型为基准，并比较了我们 QFT 和 Vicuna 的指令调优结果。我们的 QFT 在资源消耗较少的情况下，令人鼓舞地提供了比预训练模型更显著的改进，并与全精度调优的结果相媲美。
- en: '| Model |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |'
- en: '&#124; ARC-c &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ARC-c &#124;'
- en: '&#124; (25-shot) &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (25-shot) &#124;'
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; HellaSwag &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HellaSwag &#124;'
- en: '&#124; (10-shot) &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (10-shot) &#124;'
- en: '|'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MMLU &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MMLU &#124;'
- en: '&#124; (5-shot) &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (5-shot) &#124;'
- en: '|'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TruthfulQA-mc &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TruthfulQA-mc &#124;'
- en: '&#124; (0-shot) &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (0-shot) &#124;'
- en: '| Average |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 平均 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-2-7B | 53.1 | 78.6 | 46.9 | 38.8 | 54.4 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | 53.1 | 78.6 | 46.9 | 38.8 | 54.4 |'
- en: '| Vicuna-7B* | 53.6 | 77.3 | 49.4 | 51.5 | 58.0 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7B* | 53.6 | 77.3 | 49.4 | 51.5 | 58.0 |'
- en: '| LLaMA-2-7B-QFT | 52.9 | 76.7 | 48.8 | 51.1 | 57.4 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-QFT | 52.9 | 76.7 | 48.8 | 51.1 | 57.4 |'
- en: '| LLaMA-2-13B | 59.4 | 82.1 | 55.8 | 37.4 | 58.7 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | 59.4 | 82.1 | 55.8 | 37.4 | 58.7 |'
- en: '| Vicuna-13B* | 57.0 | 81.2 | 55.8 | 50.9 | 61.2 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13B* | 57.0 | 81.2 | 55.8 | 50.9 | 61.2 |'
- en: '| LLaMA-2-13B-QFT | 56.2 | 81.0 | 55.9 | 48.6 | 60.4 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B-QFT | 56.2 | 81.0 | 55.9 | 48.6 | 60.4 |'
- en: 'In this section, we conduct a comprehensive evaluation of the instruction fine-tuning
    performance in both conventional and advanced manners, which are in turn compared
    and analyzed in detail below. In addition, we also provide a qualitative analysis
    of the model’s language generation capabilities in Appendix [A.3](#A1.SS3 "A.3
    Qualitative Analysis of Conversational Abilities ‣ Appendix A Appendix ‣ QFT:
    Quantized Full-parameter Tuning of LLMs with Affordable Resources").'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们对指令微调性能进行了全面评估，包括常规方法和高级方法，并在下面进行了详细的比较和分析。此外，我们还在附录[A.3](#A1.SS3 "A.3
    交互能力的定性分析 ‣ 附录 A 附录 ‣ QFT：使用可承受资源的LLMs的量化全参数微调")中提供了模型语言生成能力的定性分析。
- en: Few-Shot Evaluation
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 少-Shot 评估
- en: 'We perform few-shot performance evaluations across a range of well-established
    benchmarks to assess the effectiveness of QFT. The obtained results, pertaining
    to various model configurations, are comprehensively presented in Table [2](#S4.T2
    "Table 2 ‣ 4.3 Performance Evaluation ‣ 4 Experiments ‣ QFT: Quantized Full-parameter
    Tuning of LLMs with Affordable Resources"). To maintain consistency, we opt to
    employ the same evaluation metrics as those employed in Open LLM Leaderboard (HuggingFace,
    [2023a](#bib.bib17)) and ensure alignment with key experimental settings, such
    as the number of shots. As we can see, when fine-tuning a LLaMA-2-7B model, it
    becomes evident that QFT introduces a remarkable enhancement in performance. Specifically,
    QFT substantially elevates the average performance score, catapulting it from
    an initial value of 54.4 to a significantly improved 57.4\. Impressively, this
    achievement positions QFT within a mere 0.6 points of the Vicuna model, which
    has undergone full-precision tuning. Regarding specific individual metrics, such
    as 5-shot MMLU, we observe an improvement in results from 46.9 to 48.8, highlighting
    the model’s enhanced problem-solving capability.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一系列成熟的基准上进行少-shot 性能评估，以评估 QFT 的有效性。获得的结果涉及各种模型配置，全面呈现在表[2](#S4.T2 "表 2 ‣
    4.3 性能评估 ‣ 4 实验 ‣ QFT：使用可承受资源的LLMs的量化全参数微调")中。为了保持一致性，我们选择使用与 Open LLM Leaderboard
    (HuggingFace, [2023a](#bib.bib17)) 相同的评估指标，并确保与关键实验设置对齐，例如 shot 数量。正如我们所见，在对 LLaMA-2-7B
    模型进行微调时，很明显 QFT 显著提升了性能。具体来说，QFT 大幅提升了平均性能得分，将其从初始值 54.4 提升至显著改进的 57.4。令人印象深刻的是，这一成就使
    QFT 在仅相差 0.6 分的情况下，与经过全精度调优的 Vicuna 模型相竞争。关于具体的个别指标，例如 5-shot MMLU，我们观察到结果从 46.9
    提升至 48.8，突显了模型解决问题能力的增强。
- en: Furthermore, it is imperative to provide a clarification regarding the observed
    slight decline in the 10-shot HellaSwag results across both fine-tuning settings.
    This diminution can be attributed, in part, to the influence exerted by the fine-tuning
    dataset and, in part, to the inherent limitations of a single benchmark evaluation,
    which may introduce a certain degree of one-sidedness or even inaccuracies into
    the assessment process (Liao et al., [2021](#bib.bib30)). Consequently, it becomes
    increasingly evident that the central focus should shift to a careful comparison
    between the performance of Vicuna and QFT rather than dwelling extensively on
    the improvement of the pre-trained model itself, and it is indeed reassuring to
    note that QFT consistently demonstrates the ability to achieve results comparable
    to those achieved by the Vicuna model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有必要澄清在两个微调设置中观察到的10-shot HellaSwag结果的轻微下降。这种减少部分归因于微调数据集的影响，部分归因于单一基准评估的固有局限性，这可能会对评估过程引入一定程度的片面性或甚至不准确性（Liao
    et al., [2021](#bib.bib30)）。因此，越来越明显的是，核心关注点应转向Vicuna与QFT性能之间的仔细比较，而不是过多地关注预训练模型本身的改进。值得欣慰的是，QFT始终能够取得与Vicuna模型相当的结果。
- en: MT-Bench Score
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MT-Bench 分数
- en: 'Table 3: MT-Bench scores using GPT-4 of different models. They can reflect
    the conversational abilities of these models. Our QFT significantly outperforms
    the pre-trained LLaMA-2 model, and achieves comparable results to the Vicuna model
    tuned in full precision.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：不同模型的MT-Bench分数，使用GPT-4评估。这些分数可以反映这些模型的对话能力。我们的QFT显著优于预训练的LLaMA-2模型，并取得了与完全精度调整的Vicuna模型相当的结果。
- en: '| Model | MT-Bench Score (GPT-4) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Model | MT-Bench Score (GPT-4) |'
- en: '| --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GPT-3.5 | 7.94 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 7.94 |'
- en: '| LLaMA-2-7B | 3.83 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | 3.83 |'
- en: '| Vicuna-7B* | 6.08 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7B* | 6.08 |'
- en: '| LLaMA-2-7B-QFT | 5.95 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-QFT | 5.95 |'
- en: '| LLaMA-2-13B | 4.69 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | 4.69 |'
- en: '| Vicuna-13B* | 6.46 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13B* | 6.46 |'
- en: '| LLaMA-2-13B-QFT | 6.27 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B-QFT | 6.27 |'
- en: 'Besides the conventional benchmarks described above, there is a more advanced
    benchmark, MT-Bench, to evaluate the conversational abilities of LLMs. MT-bench
    consists of a series of challenging multi-round open-ended questions that match
    the characteristics and preferences of human conversations, and uses GPT-4 as
    a judge to automatically score the responses. The score results are reported in
    Table [3](#S4.T3 "Table 3 ‣ MT-Bench Score ‣ 4.3 Performance Evaluation ‣ 4 Experiments
    ‣ QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources"). As
    an illustrative example, we provide a detailed discussion of the 7B models. Initially,
    the LLaMA-2 model, in its pre-trained state, yields a rather modest score of 3.83,
    indicating a considerable limitation in its problem-solving ability. For the Vicuna
    model tuned in full precision, the score undergoes a substantial augmentation,
    surging to an impressive 6.08\. Notably, the QFT also mirrors this impressive
    improvement, boosting model performance to levels comparable to the Vicuna model.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '除了上述常规基准外，还有一个更先进的基准——MT-Bench，用于评估LLMs的对话能力。MT-bench包含一系列具有挑战性的多轮开放性问题，符合人类对话的特点和偏好，并使用GPT-4作为裁判自动评分。分数结果见表格[3](#S4.T3
    "Table 3 ‣ MT-Bench Score ‣ 4.3 Performance Evaluation ‣ 4 Experiments ‣ QFT:
    Quantized Full-parameter Tuning of LLMs with Affordable Resources")。作为一个说明性示例，我们详细讨论了7B模型。最初，LLaMA-2模型在其预训练状态下，得分为3.83，显示出其问题解决能力的相当局限性。对于完全精度调整的Vicuna模型，得分显著提高，达到令人印象深刻的6.08。值得注意的是，QFT也反映了这种显著的改进，将模型性能提升到与Vicuna模型相当的水平。'
- en: 'To facilitate a more visual comparison, we provide radar charts that encompass
    eight capacity indicators, as illustrated in Figure [3](#S4.F3 "Figure 3 ‣ MT-Bench
    Score ‣ 4.3 Performance Evaluation ‣ 4 Experiments ‣ QFT: Quantized Full-parameter
    Tuning of LLMs with Affordable Resources"). These radar charts clearly shows that
    QFT provides a comprehensive and transformative improvement across all measured
    metrics compared to the baseline performance of the pre-trained LLaMA-2 model.
    In comparison to the Vicuna model tuned in full precision, QFT achieves comparable
    results and even outperforms it in certain aspects, e.g., in the 7B model setting,
    QFT exhibits superior performance in the Math metrics.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于更直观的比较，我们提供了涵盖八个能力指标的雷达图，如图 [3](#S4.F3 "图 3 ‣ MT-Bench 评分 ‣ 4.3 性能评估 ‣ 4
    实验 ‣ QFT：具有经济资源的 LLM 量化全参数调整") 所示。这些雷达图清晰地表明，与预训练的 LLaMA-2 模型的基线性能相比，QFT 在所有测量指标上都提供了全面而显著的改进。与完全精度调整的
    Vicuna 模型相比，QFT 达到了相似的结果，甚至在某些方面超过了它，例如在 7B 模型设置中，QFT 在数学指标上表现更优。
- en: '![Refer to caption](img/f439aa85455274e9085a8240d0b1578b.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f439aa85455274e9085a8240d0b1578b.png)'
- en: (a) 7B models
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 7B 模型
- en: '![Refer to caption](img/028d42125d7f7ce16a00032c749ce6aa.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/028d42125d7f7ce16a00032c749ce6aa.png)'
- en: (b) 13B models
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 13B 模型
- en: 'Figure 3: Radar charts of each capability in MT-Bench of different models.
    Compared to the pre-trained LLaMA-2 model, our QFT yields across-the-board improvements
    in all metrics. Compared to the Vicuna model tuned in full precision, our QFT
    achieves similar results and even surpasses it in some abilities, such as the
    Math metrics in the 7B model setting.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：不同模型在 MT-Bench 中的各项能力雷达图。与预训练的 LLaMA-2 模型相比，我们的 QFT 在所有指标上均实现了全面的改进。与完全精度调整的
    Vicuna 模型相比，我们的 QFT 实现了类似的结果，甚至在某些能力上超越了它，例如在 7B 模型设置中的数学指标。
- en: 5 Conclusions and Broader Impacts
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与更广泛的影响
- en: In this paper, we propose a Quantized Full-parameter Tuning (QFT) framework
    for LLMs, which leverages quantization techniques to comprehensively optimize
    training memory to enable fine-tuning on affordable resources. We employ the memory-efficient
    Lion optimizer, which provides significant advantages for robust quantized fine-tuning.
    Upon this, we develop customized quantizers to store all model states in the integer
    format, significantly reducing the memory usage. QFT incorporates these two innovations
    and designs a novel gradient flow scheme to accommodate them. We perform instruction
    tuning on the pre-trained LLaMA-2 models to verify the effectiveness of QFT, and
    the results demonstrate that QFT can reduce memory usage to 21% while achieving
    comparable performance to standard floating-point training.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种用于 LLM 的量化全参数调整 (QFT) 框架，该框架利用量化技术全面优化训练内存，以便在经济资源下进行微调。我们采用了内存高效的
    Lion 优化器，该优化器为稳健的量化微调提供了显著优势。在此基础上，我们开发了定制的量化器，将所有模型状态以整数格式存储，从而显著减少内存使用。QFT 融合了这两项创新，并设计了新颖的梯度流方案以适应它们。我们对预训练的
    LLaMA-2 模型进行了指令微调，以验证 QFT 的有效性，结果表明 QFT 能将内存使用减少到 21%，同时达到与标准浮点训练相当的性能。
- en: 'QFT can be easily integrated into mainstream LLM training tools and offers
    great compatibility with other memory optimization methods, demonstrating remarkable
    adaptability and utility in real-world applications. Additionally, it has the
    potential to produce broader impacts:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: QFT 可以轻松集成到主流 LLM 训练工具中，并与其他内存优化方法具有很好的兼容性，展示了在实际应用中的显著适应性和实用性。此外，它还具有产生更广泛影响的潜力：
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Quantized Training from Scratch: The parameters to be updated and optimizer
    configurations in the full-parameter tuning are consistent with the pre-training
    process, thus QFT can be migrated to be applied to training-from-scratch cases.'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从头开始的量化训练：全参数调整中的参数更新和优化器配置与预训练过程一致，因此 QFT 可以迁移应用于从头开始训练的案例。
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Lower-Precision Optimizer Momentum: Recent research has explored the compression
    of optimizer states to 4-bits (Li et al., [2023](#bib.bib25)). It holds promise
    to explore the combination of QFT with this approach for even more substantial
    memory reduction.'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 低精度优化器动量：近期研究探索了将优化器状态压缩到 4 位的技术 (Li et al., [2023](#bib.bib25))。有前景的探索包括将 QFT
    与这种方法结合，以实现更显著的内存减少。
- en: References
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell 等。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901，2020年。
- en: Chen et al. (2016) Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
    Training deep nets with sublinear memory cost. *arXiv preprint arXiv:1604.06174*,
    2016.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2016）Tianqi Chen、Bing Xu、Chiyuan Zhang 和 Carlos Guestrin。以亚线性内存成本训练深度网络。*arXiv
    预印本 arXiv:1604.06174*，2016年。
- en: Chen et al. (2023) X Chen, C Liang, D Huang, E Real, K Wang, Y Liu, H Pham,
    X Dong, T Luong, CJ Hsieh, et al. Symbolic discovery of optimization algorithms.
    arxiv 2023. *arXiv preprint arXiv:2302.06675*, 2023.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2023）X Chen、C Liang、D Huang、E Real、K Wang、Y Liu、H Pham、X Dong、T Luong、CJ
    Hsieh 等。优化算法的符号发现。*arXiv 预印本 arXiv:2302.06675*，2023年。
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4
    with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang 等（2023）Wei-Lin Chiang、Zhuohan Li、Zi Lin、Ying Sheng、Zhanghao Wu、Hao Zhang、Lianmin
    Zheng、Siyuan Zhuang、Yonghao Zhuang、Joseph E. Gonzalez、Ion Stoica 和 Eric P. Xing。Vicuna：一个开源聊天机器人，以
    90%* ChatGPT 质量令 GPT-4 印象深刻，2023年3月。网址 [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)。
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等（2018）Peter Clark、Isaac Cowhey、Oren Etzioni、Tushar Khot、Ashish Sabharwal、Carissa
    Schoenick 和 Oyvind Tafjord。认为你已经解决了问答问题？试试 arc，AI2 推理挑战。*arXiv 预印本 arXiv:1803.05457*，2018年。
- en: Dettmers et al. (2021) Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer.
    8-bit optimizers via block-wise quantization. In *International Conference on
    Learning Representations*, 2021.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等（2021）Tim Dettmers、Mike Lewis、Sam Shleifer 和 Luke Zettlemoyer。8-bit
    优化器通过块级量化。在 *国际学习表示大会*，2021年。
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等（2023）Tim Dettmers、Artidoro Pagnoni、Ari Holtzman 和 Luke Zettlemoyer。Qlora：高效微调量化的大型语言模型。*arXiv
    预印本 arXiv:2305.14314*，2023年。
- en: 'Ding et al. (2022) Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang,
    Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Delta tuning:
    A comprehensive study of parameter efficient methods for pre-trained language
    models. *arXiv preprint arXiv:2203.06904*, 2022.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等（2022）Ning Ding、Yujia Qin、Guang Yang、Fuchao Wei、Zonghan Yang、Yusheng Su、Shengding
    Hu、Yulin Chen、Chi-Min Chan、Weize Chen 等。Delta 调优：预训练语言模型参数高效方法的全面研究。*arXiv 预印本
    arXiv:2203.06904*，2022年。
- en: 'Dong et al. (2019) Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney,
    and Kurt Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    pp.  293–302, 2019.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2019）Zhen Dong、Zhewei Yao、Amir Gholami、Michael W Mahoney 和 Kurt Keutzer。Hawq：关注海森矩阵的混合精度神经网络量化。在
    *IEEE/CVF 国际计算机视觉大会论文集*，第 293–302 页，2019年。
- en: 'Dong et al. (2020) Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W
    Mahoney, and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization
    of neural networks. *Advances in neural information processing systems*, 33:18518–18529,
    2020.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2020）Zhen Dong、Zhewei Yao、Daiyaan Arfeen、Amir Gholami、Michael W Mahoney
    和 Kurt Keutzer。Hawq-v2：关注海森矩阵的神经网络迹权量化。*神经信息处理系统进展*，33:18518–18529，2020年。
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等（2022）Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh。Gptq：生成预训练变换器的准确后训练量化。*arXiv
    预印本 arXiv:2210.17323*，2022年。
- en: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. A framework for few-shot language model evaluation, September 2021.
    URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等人（2021）Leo Gao、Jonathan Tow、Stella Biderman、Sid Black、Anthony DiPofi、Charles
    Foster、Laurence Golding、Jeffrey Hsu、Kyle McDonell、Niklas Muennighoff、Jason Phang、Laria
    Reynolds、Eric Tang、Anish Thite、Ben Wang、Kevin Wang和Andy Zou。少样本语言模型评估框架，2021年9月。网址[https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628)。
- en: Gholami et al. (2022) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W
    Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural
    network inference. In *Low-Power Computer Vision*, pp.  291–326\. Chapman and
    Hall/CRC, 2022.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gholami等人（2022）Amir Gholami、Sehoon Kim、Zhen Dong、Zhewei Yao、Michael W Mahoney和Kurt
    Keutzer。高效神经网络推理的量化方法综述。在*《低功耗计算机视觉》*中，pp. 291–326。Chapman and Hall/CRC，2022年。
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *arXiv preprint arXiv:2009.03300*, 2020.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks等人（2020）Dan Hendrycks、Collin Burns、Steven Basart、Andy Zou、Mantas Mazeika、Dawn
    Song和Jacob Steinhardt。测量大规模多任务语言理解。*arXiv预印本arXiv:2009.03300*，2020年。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu等人（2021）Edward J Hu、Yelong Shen、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi Li、Shean
    Wang、Lu Wang和Weizhu Chen。Lora: 大型语言模型的低秩适应。*arXiv预印本arXiv:2106.09685*，2021年。'
- en: 'Huang et al. (2020) Chien-Chin Huang, Gu Jin, and Jinyang Li. Swapadvisor:
    Pushing deep learning beyond the gpu memory limit via smart swapping. In *Proceedings
    of the Twenty-Fifth International Conference on Architectural Support for Programming
    Languages and Operating Systems*, pp. 1341–1355, 2020.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang等人（2020）Chien-Chin Huang、Gu Jin和Jinyang Li。Swapadvisor: 通过智能交换将深度学习推向超越GPU内存限制的极限。在*《第二十五届国际程序语言和操作系统体系结构支持会议论文集》*中，pp.
    1341–1355，2020年。'
- en: HuggingFace (2023a) HuggingFace. Open llm leaderboard, 2023a. URL [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HuggingFace（2023a）HuggingFace。开放LLM排行榜，2023a。网址[https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)。
- en: HuggingFace (2023b) HuggingFace. Sharegpt data, 2023b. URL [https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered?doi=true](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered?doi=true).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HuggingFace（2023b）HuggingFace。ShareGPT数据，2023b。网址[https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered?doi=true](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered?doi=true)。
- en: Jacob et al. (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    pp.  2704–2713, 2018.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob等人（2018）Benoit Jacob、Skirmantas Kligys、Bo Chen、Menglong Zhu、Matthew Tang、Andrew
    Howard、Hartwig Adam和Dmitry Kalenichenko。神经网络的量化和训练，以实现高效的整数算术推理。在*《IEEE计算机视觉与模式识别会议论文集》*中，pp.
    2704–2713，2018年。
- en: 'Jain et al. (2020) Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami,
    Pieter Abbeel, Joseph Gonzalez, Kurt Keutzer, and Ion Stoica. Checkmate: Breaking
    the memory wall with optimal tensor rematerialization. *Proceedings of Machine
    Learning and Systems*, 2:497–511, 2020.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jain等人（2020）Paras Jain、Ajay Jain、Aniruddha Nrusimha、Amir Gholami、Pieter Abbeel、Joseph
    Gonzalez、Kurt Keutzer和Ion Stoica。Checkmate: 通过最佳张量重新计算打破内存壁垒。*《机器学习与系统会议论文集》*，2:497–511，2020年。'
- en: 'Kim et al. (2023) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim等人（2023）Sehoon Kim、Coleman Hooper、Amir Gholami、Zhen Dong、Xiuyu Li、Sheng
    Shen、Michael W Mahoney和Kurt Keutzer。Squeezellm: 密集和稀疏量化。*arXiv预印本arXiv:2306.07629*，2023年。'
- en: 'Kingma & Ba (2015) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. In *International Conference on Learning Representations*, 2015.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma & Ba（2015）Diederik P Kingma和Jimmy Ba。Adam: 一种随机优化方法。在*《国际学习表示会议》*，2015年。'
- en: Kirisame et al. (2020) Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer
    Brennan, Mike He, Jared Roesch, Tianqi Chen, and Zachary Tatlock. Dynamic tensor
    rematerialization. *arXiv preprint arXiv:2006.09616*, 2020.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirisame et al. (2020) Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer
    Brennan, Mike He, Jared Roesch, Tianqi Chen, 和 Zachary Tatlock. 动态张量重材料化。*arXiv
    预印本 arXiv:2006.09616*，2020年。
- en: Kumar et al. (2019) Ravi Kumar, Manish Purohit, Zoya Svitkina, Erik Vee, and
    Joshua Wang. Efficient rematerialization for deep networks. *Advances in Neural
    Information Processing Systems*, 32, 2019.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar et al. (2019) Ravi Kumar, Manish Purohit, Zoya Svitkina, Erik Vee, 和 Joshua
    Wang. 深度网络的高效重材料化。*神经信息处理系统进展*，32，2019年。
- en: Li et al. (2023) Bingrui Li, Jianfei Chen, and Jun Zhu. Memory efficient optimizers
    with 4-bit states. *arXiv preprint arXiv:2309.01507*, 2023.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Bingrui Li, Jianfei Chen, 和 Jun Zhu. 使用4位状态的内存高效优化器。*arXiv
    预印本 arXiv:2309.01507*，2023年。
- en: 'Li & Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. *arXiv preprint arXiv:2101.00190*, 2021.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li & Liang (2021) Xiang Lisa Li 和 Percy Liang. 前缀调优：优化连续提示用于生成。*arXiv 预印本 arXiv:2101.00190*，2021年。
- en: 'Li & Gu (2022) Zhikai Li and Qingyi Gu. I-vit: integer-only quantization for
    efficient vision transformer inference. *arXiv preprint arXiv:2207.01405*, 2022.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li & Gu (2022) Zhikai Li 和 Qingyi Gu. I-vit: 整数量化仅用于高效的视觉变换器推断。*arXiv 预印本 arXiv:2207.01405*，2022年。'
- en: Li et al. (2022a) Zhikai Li, Liping Ma, Mengjuan Chen, Junrui Xiao, and Qingyi
    Gu. Patch similarity aware data-free quantization for vision transformers. In
    *European Conference on Computer Vision*, pp.  154–170. Springer, 2022a.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022a) Zhikai Li, Liping Ma, Mengjuan Chen, Junrui Xiao, 和 Qingyi
    Gu. 基于数据的视觉变换器的补丁相似性感知量化。在 *欧洲计算机视觉会议*，第154–170页。Springer，2022年。
- en: 'Li et al. (2022b) Zhikai Li, Junrui Xiao, Lianwei Yang, and Qingyi Gu. Repq-vit:
    Scale reparameterization for post-training quantization of vision transformers.
    *arXiv preprint arXiv:2212.08254*, 2022b.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2022b) Zhikai Li, Junrui Xiao, Lianwei Yang, 和 Qingyi Gu. Repq-vit:
    用于视觉变换器的训练后量化的尺度重参数化。*arXiv 预印本 arXiv:2212.08254*，2022年。'
- en: Liao et al. (2021) Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, and Ludwig
    Schmidt. Are we learning yet? a meta review of evaluation failures across machine
    learning. In *Thirty-fifth Conference on Neural Information Processing Systems
    Datasets and Benchmarks Track (Round 2)*, 2021.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao et al. (2021) Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, 和 Ludwig
    Schmidt. 我们已经在学习了吗？机器学习中的评估失败的元分析。在 *第35届神经信息处理系统会议数据集与基准跟踪（第二轮）*，2021年。
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. Awq: 量化感知权重的激活感知用于LLM压缩和加速。*arXiv 预印本 arXiv:2306.00978*，2023年。'
- en: 'Lin et al. (2021) Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa:
    Measuring how models mimic human falsehoods. *arXiv preprint arXiv:2109.07958*,
    2021.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2021) Stephanie Lin, Jacob Hilton, 和 Owain Evans. Truthfulqa: 测量模型如何模仿人类的虚假信息。*arXiv
    预印本 arXiv:2109.07958*，2021年。'
- en: 'Liu et al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models. *arXiv
    preprint arXiv:2305.17888*, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra.
    Llm-qat: 无数据量化感知训练用于大型语言模型。*arXiv 预印本 arXiv:2305.17888*，2023年。'
- en: Loshchilov & Hutter (2017) Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. *arXiv preprint arXiv:1711.05101*, 2017.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov & Hutter (2017) Ilya Loshchilov 和 Frank Hutter. 解耦权重衰减正则化。*arXiv
    预印本 arXiv:1711.05101*，2017年。
- en: Lv et al. (2023) Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo,
    and Xipeng Qiu. Full parameter fine-tuning for large language models with limited
    resources. *arXiv preprint arXiv:2306.09782*, 2023.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lv et al. (2023) Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo,
    和 Xipeng Qiu. 在有限资源下对大型语言模型进行完整参数微调。*arXiv 预印本 arXiv:2306.09782*，2023年。
- en: Micikevicius et al. (2017) Paulius Micikevicius, Sharan Narang, Jonah Alben,
    Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii
    Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. *arXiv preprint arXiv:1710.03740*,
    2017.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micikevicius et al. (2017) Paulius Micikevicius, Sharan Narang, Jonah Alben,
    Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii
    Kuchaiev, Ganesh Venkatesh，等。混合精度训练。*arXiv 预印本 arXiv:1710.03740*，2017年。
- en: 'Peng et al. (2020) Xuan Peng, Xuanhua Shi, Hulin Dai, Hai Jin, Weiliang Ma,
    Qian Xiong, Fan Yang, and Xuehai Qian. Capuchin: Tensor-based gpu memory management
    for deep learning. In *Proceedings of the Twenty-Fifth International Conference
    on Architectural Support for Programming Languages and Operating Systems*, pp. 891–905,
    2020.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等（2020）Xuan Peng、Xuanhua Shi、Hulin Dai、Hai Jin、Weiliang Ma、Qian Xiong、Fan
    Yang 和 Xuehai Qian。《Capuchin：基于张量的 GPU 内存管理用于深度学习》。发表于*第二十五届编程语言和操作系统架构支持国际会议论文集*，第
    891–905 页，2020。
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. Zero: Memory optimizations toward training trillion parameter
    models. In *SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis*, pp.  1–16\. IEEE, 2020.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajbhandari 等（2020）Samyam Rajbhandari、Jeff Rasley、Olatunji Ruwase 和 Yuxiong
    He。《Zero：针对训练万亿参数模型的内存优化》。发表于*SC20：国际高性能计算、网络、存储与分析会议*，第 1–16 页。IEEE，2020。
- en: shareGPT (2023) shareGPT. Sharegpt, 2023. URL [https://sharegpt.com/](https://sharegpt.com/).
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: shareGPT（2023）shareGPT。《Sharegpt》，2023。网址 [https://sharegpt.com/](https://sharegpt.com/)。
- en: 'Shazeer & Stern (2018) Noam Shazeer and Mitchell Stern. Adafactor: Adaptive
    learning rates with sublinear memory cost. In *International Conference on Machine
    Learning*, pp. 4596–4604\. PMLR, 2018.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer & Stern（2018）Noam Shazeer 和 Mitchell Stern。《Adafactor：具有亚线性内存成本的自适应学习率》。发表于*国际机器学习会议*，第
    4596–4604 页。PMLR，2018。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023a）Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。《Llama：开放且高效的基础语言模型》。*arXiv 预印本 arXiv:2302.13971*，2023a。
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023b）Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale 等。《Llama
    2：开放基础和微调聊天模型》。*arXiv 预印本 arXiv:2307.09288*，2023b。
- en: 'Wang et al. (2018) Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon
    Song, Zenglin Xu, and Tim Kraska. Superneurons: Dynamic gpu memory management
    for training deep neural networks. In *Proceedings of the 23rd ACM SIGPLAN symposium
    on principles and practice of parallel programming*, pp.  41–53, 2018.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2018）Linnan Wang、Jinmian Ye、Yiyang Zhao、Wei Wu、Ang Li、Shuaiwen Leon Song、Zenglin
    Xu 和 Tim Kraska。《Superneurons：用于训练深度神经网络的动态 GPU 内存管理》。发表于*第 23 届 ACM SIGPLAN 并行编程原理与实践研讨会论文集*，第
    41–53 页，2018。
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers 等（2019）Rowan Zellers、Ari Holtzman、Yonatan Bisk、Ali Farhadi 和 Yejin Choi。《Hellaswag：机器真的能完成你的句子吗？》*arXiv
    预印本 arXiv:1905.07830*，2019。
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2022）Susan Zhang、Stephen Roller、Naman Goyal、Mikel Artetxe、Moya Chen、Shuohui
    Chen、Christopher Dewan、Mona Diab、Xian Li、Xi Victoria Lin 等。《Opt：开放的预训练 Transformer
    语言模型》。*arXiv 预印本 arXiv:2205.01068*，2022。
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    A survey of large language models. *arXiv preprint arXiv:2303.18223*, 2023.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等（2023）Wayne Xin Zhao、Kun Zhou、Junyi Li、Tianyi Tang、Xiaolei Wang、Yupeng
    Hou、Yingqian Min、Beichen Zhang、Junjie Zhang、Zican Dong 等。《大型语言模型调查》。*arXiv 预印本
    arXiv:2303.18223*，2023。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*,
    2023.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2023）Lianmin Zheng、Wei-Lin Chiang、Ying Sheng、Siyuan Zhuang、Zhanghao
    Wu、Yonghao Zhuang、Zi Lin、Zhuohan Li、Dacheng Li、Eric Xing 等。《使用 mt-bench 和 chatbot
    arena 评判 llm-as-a-judge》。*arXiv 预印本 arXiv:2306.05685*，2023。
- en: Appendix A Appendix
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Discussion on Outlier Thresholds of Weight Quantizer
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 权重量化器的异常值阈值讨论
- en: 'In this section, we discuss the selection and updating strategies for outlier
    thresholds in dense-and-sparse quantizers. We first report the memory and accuracy
    of dense-and-sparse quantizers using different percentage thresholds, and the
    results are shown in Table [4](#A1.T4 "Table 4 ‣ A.1 Discussion on Outlier Thresholds
    of Weight Quantizer ‣ Appendix A Appendix ‣ QFT: Quantized Full-parameter Tuning
    of LLMs with Affordable Resources"). The accuracy, i.e., the degree of distributional
    approximation of the quantizers, is evaluated by $L_{2}$, where the quantized
    weights are from the final down projection layer.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了在稠密和稀疏量化器中离群值阈值的选择和更新策略。我们首先报告了使用不同百分比阈值的稠密和稀疏量化器的内存和准确性，结果见表 [4](#A1.T4
    "表4 ‣ A.1 离群值阈值的讨论 ‣ 附录A 附录 ‣ QFT：使用负担得起的资源对LLM进行量化全参数调优")。准确性，即量化器的分布逼近度，通过 $L_{2}$
    评估，其中量化的权重来自最终的下投影层。
- en: 'Table 4: Comparison of memory (in GB) and accuracy of dense-and-sparse quantizers
    using different percentage thresholds for weights. Here, accuracy is measured
    by $L_{2}$.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：使用不同百分比阈值的稠密和稀疏量化器的内存（以GB为单位）和准确性比较。此处的准确性由 $L_{2}$ 测量。
- en: '| Percentile | 0 | 0.45% | 1.0% | 3.0% | 5.0% |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 百分位数 | 0 | 0.45% | 1.0% | 3.0% | 5.0% |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Memory | 7.06 | 7.23 | 7.42 | 8.23 | 9.16 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 内存 | 7.06 | 7.23 | 7.42 | 8.23 | 9.16 |'
- en: '| $L_{2}$ Distance | 436 | 0.846 | 0.619 | 0.566 | 0.479 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| $L_{2}$ 距离 | 436 | 0.846 | 0.619 | 0.566 | 0.479 |'
- en: The benefits of employing matrix decomposition in dense-and-sparse quantizers
    are readily evident from the results. When the percentile is set to 0, the quantizer
    no longer filters out the outliers and degenerates into a standard uniform quantizer,
    resulting in intolerable quantization errors. A noteworthy value suggested in
    SqueezeLLM (Kim et al., [2023](#bib.bib21)) is 0.45%. However, it’s essential
    to acknowledge that this tight bound is primarily designed for inference scenarios.
    To accommodate potential fluctuations during training, some level of relaxation
    in the thresholds is necessary. To this end, we assess the relaxed constraints
    at percentiles of 1.0%, 3.0%, and 5.0%. The findings reveal that, although the
    3% and 5% percentiles yield a slight boost in accuracy, they also incur higher
    memory usage. Consequently, we advocate for the 1% percentile as a more favorable
    choice for achieving a balance between memory conservation and accuracy.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在稠密和稀疏量化器中使用矩阵分解的好处从结果中显而易见。当百分位数设置为0时，量化器不再滤除离群值，退化为标准均匀量化器，导致不可接受的量化误差。SqueezeLLM (Kim
    et al., [2023](#bib.bib21)) 中建议的一个值得注意的值是0.45%。然而，必须承认这个紧束缚主要是为推理场景设计的。为了适应训练过程中的潜在波动，有必要在阈值上做一些放松。为此，我们评估了1.0%、3.0%和5.0%百分位数下的放松约束。结果表明，尽管3%和5%百分位数略微提高了准确性，但也带来了更高的内存使用。因此，我们建议选择1%百分位数作为在内存节省和准确性之间取得平衡的更理想选择。
- en: We also explore the update frequency of the thresholds. In order to maintain
    a high training throughput, we adopt a strategy of lazy updates, meaning that
    the thresholds set in the first iteration are reused in subsequent iterations.
    This approach is viable due to the smooth numerical updates of the weights during
    the fine-tuning process across neighboring iterations. Additionally, to our surprise,
    we discover that the initial thresholds remain effective for an entire epoch.
    As a result, we only update the thresholds once at the beginning of each epoch.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了阈值的更新频率。为了保持高训练吞吐量，我们采用了延迟更新策略，即在第一次迭代中设置的阈值在后续迭代中重复使用。这种方法是可行的，因为在微调过程中，权重的数值更新在相邻迭代中是平滑的。此外，令我们惊讶的是，我们发现初始阈值在整个训练周期内仍然有效。因此，我们仅在每个训练周期开始时更新一次阈值。
- en: A.2 Discussion on Training Stability
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 训练稳定性的讨论
- en: In this section, we delve into the essentiality of maintaining full-precision
    weight copies in mixed-precision training and provide a comprehensive illustration
    of the benefits of QFT through a comparative analysis. In mixed-precision training,
    both forward and backward propagation involve weights and gradients in the FP16
    format. However, during parameter updates within the optimizer, the weights are
    involved in calculations using the FP32 format. This approach is employed to circumvent
    potential data overflow issues that may arise in the FP16 format (Micikevicius
    et al., [2017](#bib.bib36)), given its limited representation range. Consequently,
    maintaining full-precision weight copies becomes indispensable to ensure the stability
    of the training process.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们深入探讨了在混合精度训练中保持全精度权重副本的重要性，并通过对比分析全面说明了QFT的好处。在混合精度训练中，前向和后向传播都涉及FP16格式的权重和梯度。然而，在优化器内进行参数更新时，权重会使用FP32格式进行计算。这种方法的采用是为了规避FP16格式中可能出现的数据溢出问题（Micikevicius等，[2017](#bib.bib36)），因为其表示范围有限。因此，保持全精度权重副本成为确保训练过程稳定性的必要条件。
- en: 'Conversely, QFT effectively addresses the aforementioned issue without necessitating
    extra storage for weight copies. Specifically, unlike the rudimentary numerical
    truncation encountered in the FP16 format, the quantization procedure in QFT maps
    FP32 format values as a cohesive unit to a set of evenly distributed integer values,
    as illustrated in Equation [1](#S3.E1 "In Uniform Quantizer for Gradients and
    Momentum ‣ 3.2 Quantization ‣ 3 Methodology ‣ QFT: Quantized Full-parameter Tuning
    of LLMs with Affordable Resources"). Consequently, when performing the calculations,
    these integer values are dynamically de-quantized into the FP32 format, without
    any potential risk of data overflow.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '相反，QFT有效地解决了上述问题，无需额外存储权重副本。具体而言，与FP16格式中遇到的基本数值截断不同，QFT中的量化过程将FP32格式的值作为一个整体映射到一组均匀分布的整数值，如方程式[1](#S3.E1
    "In Uniform Quantizer for Gradients and Momentum ‣ 3.2 Quantization ‣ 3 Methodology
    ‣ QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources")所示。因此，在进行计算时，这些整数值会动态地解量化回FP32格式，而没有数据溢出的潜在风险。'
- en: A.3 Qualitative Analysis of Conversational Abilities
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 对话能力的定性分析
- en: In the experimental section, we demonstrate the effectiveness of QFT through
    quantitative evaluations of LLMs’ performance on a variety of benchmarks. Despite
    the large number of benchmarks, the results of the machine evaluations are not
    considered to be comprehensive, and in some cases, their reliability may be subject
    to scrutiny, e.g., the ”shortcut” issue. Consequently, in the following, we intuitively
    show and compare examples generated by LLMs, aiming to further verify the conversational
    abilities through qualitative analysis.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验部分，我们通过对各种基准测试中LLMs性能的定量评估展示了QFT的有效性。尽管基准测试数量众多，但机器评估结果并不被认为是全面的，在某些情况下，其可靠性可能会受到质疑，例如“捷径”问题。因此，在接下来的部分，我们直观地展示和比较由LLMs生成的示例，旨在通过定性分析进一步验证对话能力。
- en: '[Writing Ability]'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[写作能力]'
- en: '*Question*: Write a captivating movie review for a recently released science
    fiction film, discussing its plot, characters, and special effects.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*问题*：撰写一篇引人入胜的电影评论，讨论最近上映的一部科幻电影的情节、角色和特效。'
- en: 'Vicuna-7B*: ”The Last Voyage” is a thrilling science fiction film that follows
    a group of astronauts on a mission to explore a distant planet. When they arrive,
    they discover that the planet is inhabited by a mysterious alien race. As they
    try to communicate with the aliens and gather data, they realize that their mission
    may be more dangerous than they ever imagined. With stunning special effects and
    a gripping plot, ”The Last Voyage” is a must-see for science fiction fans.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Vicuna-7B*：“最后的航行”是一部刺激的科幻电影，讲述了一组宇航员探索遥远星球的任务。当他们抵达时，发现这个星球上居住着一个神秘的外星种族。随着他们试图与外星人沟通并收集数据，他们意识到他们的任务可能比他们想象的更危险。凭借惊人的特效和引人入胜的情节，“最后的航行”是科幻迷必看的影片。
- en: 'LLaMA-2-7B-QFT: ”Intergalactic Intrigue” is a thrilling science fiction film
    that takes viewers on a journey through the stars. The film follows a group of
    space explorers as they embark on a mission to uncover the secrets of an ancient
    alien civilization. The film’s special effects are top-notch, with stunning visuals
    that transport viewers to a world of wonder and discovery. The characters are
    well-developed and likable, with each member of the crew bringing their own unique
    skills and personalities to the table.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-2-7B-QFT: 《星际阴谋》是一部激动人心的科幻片，带领观众踏上星际之旅。影片讲述了一群太空探险者的故事，他们踏上使命以揭开古老外星文明的秘密。影片的特效一流，令人惊叹的视觉效果将观众带入一个充满奇迹和发现的世界。角色发展良好且讨人喜欢，每个船员都带来了自己独特的技能和个性。'
- en: '[Roleplay Ability]'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[角色扮演能力]'
- en: '*Question*: As a superhero, how would you explain your origin story to a curious
    child?'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*问题*: 作为一个超级英雄，你会如何向好奇的孩子解释你的起源故事？'
- en: 'Vicuna-7B*: I was just like any other kid, until one day I was exposed to a
    strange chemical that gave me incredible powers. I learned to control the elements
    and use them to protect the innocent. I became known as the Elemental Guardian,
    and I’ve been fighting crime and saving the day ever since.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vicuna-7B*: 我曾经和其他孩子一样，直到有一天我接触到一种奇怪的化学物质，这让我获得了令人难以置信的力量。我学会了控制元素，并利用这些力量保护无辜。我被称为元素守护者，从那时起我就开始打击犯罪，拯救世界。'
- en: 'LLaMA-2-7B-QFT: I was just like any other kid, playing in the park and having
    fun. But one day, I was hit by a strange ray that gave me superpowers. I could
    fly, turn invisible, and even control the weather! I decided to use my powers
    to help people and fight crime. That’s how I became the superhero you know and
    love!'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-2-7B-QFT: 我曾经和其他孩子一样，在公园里玩耍并享受乐趣。但有一天，我被一束奇怪的光线击中，这给了我超能力。我可以飞行、隐身，甚至控制天气！我决定利用我的力量帮助人们和打击犯罪。这就是我成为你们所知并喜爱的超级英雄的原因！'
- en: '[Common-sense Ability]'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[常识能力]'
- en: '*Question*: In a world where automation is becoming increasingly prevalent,
    is it more important to prioritize job creation or technological progress?'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '*问题*: 在自动化越来越普及的世界里，是优先考虑创造就业机会还是技术进步更重要？'
- en: 'Vicuna-7B*: Job creation is essential for a thriving economy, as it provides
    individuals with employment opportunities and a source of income. While technological
    progress is important for driving innovation and improving efficiency, it is crucial
    to balance technological advancements with the creation of new jobs. This can
    be achieved by investing in education and training programs that help workers
    develop the skills necessary to adapt to new technologies and by promoting policies
    that support small businesses and entrepreneurship. By prioritizing both job creation
    and technological progress, we can ensure a more stable and prosperous future
    for all.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vicuna-7B*: 创造就业对于一个繁荣的经济至关重要，因为它为个人提供了就业机会和收入来源。虽然技术进步对于推动创新和提高效率很重要，但平衡技术进步与新工作的创造同样关键。这可以通过投资教育和培训项目来实现，这些项目帮助工人发展适应新技术的技能，同时促进支持小企业和创业的政策。通过优先考虑就业创造和技术进步，我们可以确保一个更稳定和繁荣的未来。'
- en: 'LLaMA-2-7B-QFT: In the short term, job creation is a critical concern for policymakers
    and businesses. The unemployment rate remains high, and many people are struggling
    to find work. However, in the long term, technological progress is essential for
    driving economic growth and improving the standard of living for everyone. Automation
    has the potential to increase productivity and efficiency, which can lead to higher
    wages and better working conditions for employees. However, it can also lead to
    job displacement, particularly in industries where automation is most prevalent.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-2-7B-QFT: 在短期内，创造就业是政策制定者和企业面临的关键问题。失业率仍然很高，许多人正在努力寻找工作。然而，从长远来看，技术进步对于推动经济增长和提高每个人的生活水平至关重要。自动化有可能提高生产力和效率，这可以带来更高的工资和更好的工作条件。然而，它也可能导致工作岗位的流失，特别是在自动化最为普及的行业中。'
- en: '[Fermi Ability]'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[费米能力]'
- en: '*Question*: How many snowflakes fall during a typical winter? Try to explain
    your answer. Your explanation should take the reader through your reasoning step-by-step.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '*问题*: 在一个典型的冬季里会有多少雪花落下？尝试解释你的答案。你的解释应该一步一步地引导读者理解你的推理过程。'
- en: 'Vicuna-7B*: It is difficult to determine the average number of snowflakes that
    fall during a typical winter, as it can vary greatly depending on location, climate,
    and other factors. However, it is estimated that on average, a single snowflake
    can have up to 100,000 ice crystals per flake. Over the course of a typical winter,
    this could result in billions of snowflakes falling from the sky.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vicuna-7B*: 很难确定典型冬季期间平均降雪量，因为它会因地点、气候和其他因素而有很大差异。然而，据估计，单个雪花上可能有多达100,000个冰晶。整个典型冬季，这可能导致数十亿个雪花从天空中降落。'
- en: 'LLaMA-2-7B-QFT: To answer this question, we need to consider the average snowfall
    for a typical winter in a given location. In the United States, for example, snowfall
    tends to be heaviest in the northern states, where the average snowfall for a
    typical winter is around 60 inches. In the southern states, snowfall is much less
    common and the average snowfall for a typical winter is around 10 inches.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-2-7B-QFT: 为了回答这个问题，我们需要考虑给定地点典型冬季的平均降雪量。例如，在美国，降雪通常在北方州最为严重，典型冬季的平均降雪量约为60英寸。在南方州，降雪则少得多，典型冬季的平均降雪量约为10英寸。'
- en: '[Counterfactual Ability]'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[反事实能力]'
- en: '*Question*: What if the Internet had been invented during the Renaissance period?'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*问题*: 如果互联网在文艺复兴时期已经发明，会怎样？'
- en: 'Vicuna-7B*: The Renaissance would have been a much different time period if
    the internet had been invented. The flow of information would have been much faster
    and more efficient, leading to a greater exchange of ideas and innovations. The
    world would have been connected in ways never before seen, allowing for the rapid
    spread of new ideas and technologies.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vicuna-7B*: 如果互联网在文艺复兴时期已经发明，那个时期将会大不相同。信息流动将会更加迅速和高效，导致思想和创新的更大交流。世界将以从未见过的方式相互连接，使新思想和技术的传播变得更快。'
- en: 'LLaMA-2-7B-QFT: The Renaissance was characterized by a renewed interest in
    classical learning and the arts, and the Internet would have provided a way for
    people to access and share this knowledge. It would have also facilitated the
    exchange of ideas and the development of new technologies, leading to even greater
    advancements in the fields of science, medicine, and engineering.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-2-7B-QFT: 文艺复兴时期的特点是对古典学习和艺术的重新兴趣，而互联网将提供一种让人们访问和分享这些知识的方式。它还将促进思想交流和新技术的发展，从而在科学、医学和工程领域带来更大的进步。'
- en: '[Generic Ability]'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[通用能力]'
- en: '*Question*: Can you explain the basics of quantum computing?'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '*问题*: 你能解释一下量子计算的基本原理吗？'
- en: 'Vicuna-7B*: Quantum computing is a type of computing that uses quantum bits
    (qubits) to perform calculations, unlike traditional computers that use binary.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vicuna-7B*: 量子计算是一种使用量子位（qubits）进行计算的计算类型，与使用二进制的传统计算机不同。'
- en: 'LLaMA-2-7B-QFT: Quantum computing is a type of computing that uses quantum-mechanical
    phenomena, such as superposition and entanglement, to perform operations on data.
    These phenomena allow quantum computers to perform certain calculations much faster
    than classical computers.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-2-7B-QFT: 量子计算是一种利用量子力学现象，如叠加和纠缠，来对数据进行操作的计算类型。这些现象使量子计算机能够比经典计算机更快地执行某些计算。'
- en: '[Knowledge Ability]'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[知识能力]'
- en: '*Question*: Explain the process of natural selection and how it contributes
    to the evolution and adaptation of species.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*问题*: 解释自然选择的过程以及它如何促进物种的进化和适应。'
- en: 'Vicuna-7B*: Natural selection is the process by which species adapt to their
    environment through the survival and reproduction of individuals with favorable
    traits. Over time, these favorable traits become more common in the population,
    leading to the evolution of new species.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vicuna-7B*: 自然选择是物种通过具有有利特征的个体的生存和繁殖来适应环境的过程。随着时间的推移，这些有利特征在种群中变得越来越普遍，从而导致新物种的进化。'
- en: 'LLaMA-2-7B-QFT: Natural selection is a process where species that are better
    adapted to their environment tend to survive and reproduce more than those that
    are less adapted. This leads to the survival of the fittest and the gradual evolution
    of species over time.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-2-7B-QFT: 自然选择是一个过程，其中更好地适应环境的物种往往比适应性差的物种更能生存和繁殖。这导致了适者生存和物种的逐渐进化。'
