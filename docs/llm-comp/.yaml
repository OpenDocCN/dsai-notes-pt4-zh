- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:48:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.13868](https://ar5iv.labs.arxiv.org/html/2406.13868)
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.13868](https://ar5iv.labs.arxiv.org/html/2406.13868)
- en: marginparsep has been altered.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: marginparsep 已被更改。
- en: topmargin has been altered.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: topmargin 已被更改。
- en: marginparwidth has been altered.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: marginparwidth 已被更改。
- en: marginparpush has been altered.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: marginparpush 已被更改。
- en: The page layout violates the ICML style. Please do not change the page layout,
    or include packages like geometry, savetrees, or fullpage, which change it for
    you. We’re not able to reliably undo arbitrary changes to the style. Please remove
    the offending package(s), or layout-changing commands and try again.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 页面布局违反了 ICML 风格。请不要更改页面布局，或包括像 geometry、savetrees 或 fullpage 这样的包，这些包会为你更改布局。我们不能可靠地撤销对风格的任意更改。请移除相关包或更改布局的命令并重试。
- en: 'SDQ: Sparse Decomposed Quantization for LLM Inference'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 'SDQ: 稀疏分解量化用于 LLM 推理'
- en: Anonymous Authors^(1 )
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 匿名作者^(1 )
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recently, large language models (LLMs) have shown surprising performance in
    task-specific workloads as well as general tasks with the given prompts. However,
    to achieve unprecedented performance, recent LLMs use billions to trillions of
    parameters, which hinder the wide adaptation of those models due to their extremely
    large compute and memory requirements. To resolve the issue, various model compression
    methods are being actively investigated. In this work, we propose SDQ (Sparse
    Decomposed Quantization) to exploit both structured sparsity and quantization
    to achieve both high compute and memory efficiency. From our evaluations, we observe
    that SDQ can achieve 4$\times$1% quality drop.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）在特定任务和一般任务中表现出了令人惊讶的性能。然而，为了实现前所未有的性能，最近的LLMs使用了数十亿到数万亿的参数，这由于其极大的计算和内存需求阻碍了这些模型的广泛应用。为了解决这个问题，各种模型压缩方法正在积极研究中。在这项工作中，我们提出了
    SDQ（稀疏分解量化）以利用结构化稀疏性和量化，从而实现高计算和内存效率。从我们的评估中，我们观察到 SDQ 可以实现 4$\times$1% 的质量下降。
- en: '^†^†footnotetext: ¹Anonymous Institution, Anonymous City, Anonymous Region,
    Anonymous Country. Correspondence to: Anonymous Author .'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†脚注：¹匿名机构，匿名城市，匿名地区，匿名国家。通讯作者：匿名作者 。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) Brown et al. ([2020](#bib.bib1)); Chowdhery et al.
    ([2022](#bib.bib2)); Touvron et al. ([2023b](#bib.bib32)) with billions or trillions
    of parameters have gained extensive attention as they show promising quality in
    various domains. With such popularity, efficiently deploying and accelerating
    LLMs are the utmost research question for system designers, as the large number
    of weights causes a huge memory footprint and an enormous amount of computations.
    To address this issue, recent work has applied various model compression methods,
    such as sparsification Frantar & Alistarh ([2023](#bib.bib7)) and quantization Dettmers
    et al. ([2022](#bib.bib5)), on LLMs to reduce memory footprint and computations.
    However, when compared to classic DNN models (ResNet, BERT), LLMs introduce new
    challenges to both sparsification and quantization.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有数十亿或数万亿参数的大型语言模型（LLMs） Brown 等（[2020](#bib.bib1)）；Chowdhery 等（[2022](#bib.bib2)）；Touvron
    等（[2023b](#bib.bib32)）由于在各个领域表现出有希望的质量而受到广泛关注。由于这种受欢迎程度，高效地部署和加速 LLMs 是系统设计师的终极研究问题，因为大量的权重导致了巨大的内存占用和巨量的计算。为了解决这个问题，近期的工作在
    LLMs 上应用了各种模型压缩方法，如稀疏化 Frantar & Alistarh（[2023](#bib.bib7)）和量化 Dettmers 等（[2022](#bib.bib5)），以减少内存占用和计算。然而，与经典
    DNN 模型（ResNet、BERT）相比，LLMs 为稀疏化和量化引入了新的挑战。
- en: Model sparsification, such as model pruning LeCun et al. ([1989](#bib.bib18));
    Han et al. ([2015](#bib.bib12)), reduces the target model size by removing parameters
    in weights and storing the weights using a compressed format. Sparsity not only
    reduces the memory requirement but also enables performance improvement and power
    saving by skipping ineffectual computation, i.e., $0\times X=0$ computation reduction
    and performance gain with low overhead, i.e., sparsity tax Wu et al. ([2023](#bib.bib36)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 模型稀疏化，例如模型剪枝 LeCun 等（[1989](#bib.bib18)）；Han 等（[2015](#bib.bib12)），通过移除权重中的参数并使用压缩格式存储权重来减少目标模型的大小。稀疏性不仅减少了内存需求，还通过跳过无效计算来提高性能和节省功耗，即
    $0\times X=0$ 计算减少和低开销的性能提升，即稀疏性税 Wu 等（[2023](#bib.bib36)）。
- en: Although many sparsification methods have been proposed for the classic DNNs,
    these techniques are less successful so far to compress LLMs. Previous efforts Hoefler
    et al. ([2021](#bib.bib13)) have shown how to compress classic DNNs by more than
    90% (10$\times$ computation reduction) with a limited loss of accuracy is already
    challenging Frantar & Alistarh ([2023](#bib.bib7)); Sun et al. ([2023](#bib.bib30))
    as shown in [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction").
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经提出了许多经典DNN的稀疏化方法，但这些技术在压缩LLMs方面的成功仍然有限。之前的努力（Hoefler等人 ([2021](#bib.bib13))）已经展示了如何在准确性损失有限的情况下将经典DNN压缩超过90%（计算减少10$\times$），但这仍然具有挑战性（Frantar
    & Alistarh ([2023](#bib.bib7)); Sun等人 ([2023](#bib.bib30))），如[图 1](#S1.F1 "图 1
    ‣ 1 引言")所示。
- en: On the other hand, quantization reduces the target model size by using a narrower
    bit width per value. For example, if we use an 8-bit or a 4-bit format instead
    of a 16-bit format for each parameter, we can reduce the model size by half and
    a quarter, respectively. Furthermore, when quantization is applied to both weights
    and activations, we can use specialized, low-bit-width arithmetic computation
    instead of full-precision computation, which is both more power-efficient and
    area-efficient Horowitz ([2014](#bib.bib14)); van Baalen et al. ([2023](#bib.bib33)).
    As the low-bit-width arithmetic unit takes a smaller area, processors with a given
    area budget for compute units could allocate more low-bit-width compute units.
    For example, in NVIDIA Ampere GPU, the peak tensor throughput for int8 format
    is 2$\times$ for int4 format).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，量化通过对每个值使用更窄的位宽来减少目标模型的大小。例如，如果我们使用8位或4位格式代替16位格式，我们可以分别将模型大小减少一半和四分之一。此外，当量化应用于权重和激活时，我们可以使用专用的低位宽算术计算，而不是全精度计算，这不仅更节能而且更节省面积（Horowitz
    ([2014](#bib.bib14)); van Baalen等人 ([2023](#bib.bib33))）。由于低位宽算术单元占用的面积较小，具有特定面积预算的处理器可以分配更多低位宽计算单元。例如，在
    NVIDIA Ampere GPU 中，int8 格式的峰值张量吞吐量是 int4 格式的2$\times$。
- en: Like sparsification, quantization also faces new challenges with LLMs. While
    prior work has shown how to use various 4-bit formats Dai et al. ([2021](#bib.bib3));
    Darvish Rouhani et al. ([2023](#bib.bib4)) for classic models, limited work shows
    how to quantize *both* activation and weights of LLMs to 4-bit format to leverage
    the high throughput, low-bit-width hardware units. Recent work Guo et al. ([2023](#bib.bib11));
    Lin et al. ([2023](#bib.bib20)) has shown that *outliers* in activations obstruct
    quantizing both activations and weights as it causes high quantization error,
    which results in a huge error in the final output when propagated. Various work
    in this area instead focuses on 8-bit dual quantization Dettmers et al. ([2022](#bib.bib5)),
    or quantization for weights only Lin et al. ([2023](#bib.bib20)); Dettmers et al.
    ([2023](#bib.bib6)), limiting the computation reduction to be up to 2$\times$.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与稀疏化类似，量化在处理大型语言模型（LLMs）时也面临新的挑战。虽然以前的工作已经展示了如何使用各种4位格式（Dai等人 ([2021](#bib.bib3));
    Darvish Rouhani等人 ([2023](#bib.bib4))）来处理经典模型，但目前的工作较少展示如何将LLMs的*激活*和权重都量化为4位格式，以利用高吞吐量、低位宽硬件单元。最近的工作（Guo等人
    ([2023](#bib.bib11)); Lin等人 ([2023](#bib.bib20))）表明，*异常值*会阻碍同时量化激活和权重，因为这会导致高量化误差，从而在传播时造成最终输出的巨大误差。该领域的各种工作则专注于8位双重量化（Dettmers等人
    ([2022](#bib.bib5))），或仅对权重进行量化（Lin等人 ([2023](#bib.bib20)); Dettmers等人 ([2023](#bib.bib6))），将计算减少限制在最多2$\times$。
- en: '![Refer to caption](img/52f389e77430676c060cb01a6eaec4b8.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/52f389e77430676c060cb01a6eaec4b8.png)'
- en: 'Figure 1: Effective compute throughput and perplexity increase comparison of
    sparsification-only, quantization-only, and SDQ on OPT-6.7B and LLaMA-7B.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：OPT-6.7B 和 LLaMA-7B 的稀疏化仅、量化仅和 SDQ 的有效计算吞吐量与困惑度增加比较。
- en: In this paper, we demonstrate how to achieve even larger computation reduction
    (more than 2$\times$) by combining sparsification and quantization. Our key insight
    is that sparsification could be used to complement the outlier problem in the
    LLM quantization while maintaining high throughput with a low overhead using the
    structured sparsity patterns. Based on the insight, we propose a hybrid model
    compression method leveraging both sparsification and quantization to further
    increase computation reduction, while maintaining model quality through decomposing
    weights into structured sparse tensors and using structured sparse/low-bit-width
    compute HW. We focus on post-training model compression, which does not require
    any model retraining or fine-tuning as training LLMs could be infeasible in many
    cases (due to limited compute resources or data).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们展示了如何通过结合稀疏化和量化实现更大的计算减少（超过2$\times$）。我们的关键见解是，稀疏化可以用来补充LLM量化中的异常值问题，同时利用结构稀疏模式保持高吞吐量且开销低。基于这一见解，我们提出了一种混合模型压缩方法，利用稀疏化和量化进一步增加计算减少，同时通过将权重分解为结构化稀疏张量，并使用结构化稀疏/低位宽计算硬件来保持模型质量。我们专注于训练后的模型压缩，这不需要任何模型重训练或微调，因为训练LLMs在许多情况下可能不可行（由于计算资源或数据的限制）。
- en: 'We summarize our contribution in the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了以下贡献：
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We illustrate the opportunity to treat the outliers during quantization as structured
    sparse tensors to accelerate with structured sparse HW.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了在量化过程中将异常值视为结构化稀疏张量，以便用结构化稀疏硬件加速的机会。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose SDQ, a technique to combine both sparse and quantization emerging
    hardware support. It achieves a better Pareto curve in model quality and compute
    throughput than sparsification-only or quantization-only methods.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了SDQ，这是一种结合稀疏和量化新兴硬件支持的技术。它在模型质量和计算吞吐量方面优于仅稀疏化或仅量化的方法，达到更好的帕累托曲线。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We show that SDQ is orthogonal to sparsification and quantization techniques.
    With an improved sparsified or quantized model, SDQ would also perform better.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了SDQ与稀疏化和量化技术是正交的。通过改进的稀疏化或量化模型，SDQ的表现也会更好。
- en: 2 Background
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: '![Refer to caption](img/670ce5e2cdb89d335eaeee75c6ee3dba.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/670ce5e2cdb89d335eaeee75c6ee3dba.png)'
- en: 'Figure 2: A transformer block architecture.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：一个transformer块架构。
- en: 2.1 Compressing LLM models
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 压缩LLM模型
- en: In [Figure 2](#S2.F2 "Figure 2 ‣ 2 Background"), we show a high-level overview
    of decoder-only Transformer block architecture used in the recent LLMs. Each Transformer
    block contains multiple layers including linear layers, activation layers, normalization
    layers, etc., and the layers colored with blue and grey in [Figure 2](#S2.F2 "Figure
    2 ‣ 2 Background") contribute to 99% of the computations during inference. Linear layers (Q, K,
    V, out, FF1, and FF2) consist of matrix-matrix multiplications (GEMMs) with static
    weights, while self-attention layers ( BMMs) consists of GEMMs with dynamic activations.
    While the actual computation breakdown depends on the input context sizes, as
    well as the model parameters, generally speaking, GEMMs from the linear layers
    dominates the overall computation and latency Vaswani et al. ([2017](#bib.bib34)).
    As the result, in this paper, we consider model optimization and compression for
    only the GEMMs for the linear layers (blue layers with static weights).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2](#S2.F2 "Figure 2 ‣ 2 Background")中，我们展示了最近LLMs中使用的仅解码器Transformer块架构的高层次概述。每个Transformer块包含多个层，包括线性层、激活层、归一化层等，图[2](#S2.F2
    "Figure 2 ‣ 2 Background")中用蓝色和灰色标记的层在推理过程中贡献了99%的计算。线性层（Q, K, V, out, FF1, 和 FF2）由具有静态权重的矩阵乘法（GEMMs）组成，而自注意力层（BMMs）由具有动态激活的GEMMs组成。实际计算的详细划分依赖于输入上下文大小以及模型参数，但一般来说，线性层的GEMMs占据了整体计算和延迟的主导地位（Vaswani
    et al. ([2017](#bib.bib34))）。因此，本文考虑的模型优化和压缩仅针对线性层的GEMMs（具有静态权重的蓝色层）。
- en: '|  | Sparsification | Quantization | Outlier | Compute | Compute |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | 稀疏化 | 量化 | 异常值 | 计算 | 计算 |'
- en: '|  | Configuration | Configuration | Extraction | Cores | Bit Width |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | 配置 | 配置 | 提取 | 核心 | 位宽 |'
- en: '| ASP Pool et al. ([2021](#bib.bib27)) | 2:4 | X | X | 2:4 Sparse TC | 16b
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| ASP Pool等人 ([2021](#bib.bib27)) | 2:4 | X | X | 2:4 稀疏 TC | 16b |'
- en: '| WANDA Sun et al. ([2023](#bib.bib30)) | 2:4 | X | X | 2:4 Sparse TC | 16b
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| WANDA Sun等人 ([2023](#bib.bib30)) | 2:4 | X | X | 2:4 稀疏 TC | 16b |'
- en: '| VS-Quant Dai et al. ([2021](#bib.bib3)) | X | W4-8A4-8 | X | TC | 4b/8b |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| VS-Quant Dai et al. ([2021](#bib.bib3)) | X | W4-8A4-8 | X | TC | 4b/8b |'
- en: '| AWQ Lin et al. ([2023](#bib.bib20)) | X | W3-4A16 | X | TC | 16b |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| AWQ Lin et al. ([2023](#bib.bib20)) | X | W3-4A16 | X | TC | 16b |'
- en: '| SparseGPT Frantar & Alistarh ([2023](#bib.bib7)) | 2:4/4:8 | W4A16 | X |
    2:4 Sparse TC | 16b |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT Frantar & Alistarh ([2023](#bib.bib7)) | 2:4/4:8 | W4A16 | X |
    2:4 稀疏 TC | 16b |'
- en: '| LLM.int8() Dettmers et al. ([2022](#bib.bib5)) | Unstr. 0.1% | W8A8 | O |
    TC + CUDA Core | 8b |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| LLM.int8() Dettmers et al. ([2022](#bib.bib5)) | Unstr. 0.1% | W8A8 | O |
    TC + CUDA 核心 | 8b |'
- en: '| SpQR Dettmers et al. ([2023](#bib.bib6)) | Unstr. 1% | W3-4A16 | O | TC +
    CUDA Core | 16b |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| SpQR Dettmers et al. ([2023](#bib.bib6)) | Unstr. 1% | W3-4A16 | O | TC +
    CUDA 核心 | 16b |'
- en: '| OWQ Lee et al. ([2023](#bib.bib19)) | Col-wise $<$1% | W3-4A16 | O | TC +
    CUDA Core | 16b |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| OWQ Lee et al. ([2023](#bib.bib19)) | 列-wise $<$1% | W3-4A16 | O | TC + CUDA
    核心 | 16b |'
- en: '| SqueezeLLM Kim et al. ([2023](#bib.bib16)) | Unstr. $<$1% | W3-4A16 | O |
    TC + CUDA Core | 16b |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| SqueezeLLM Kim et al. ([2023](#bib.bib16)) | Unstr. $<$1% | W3-4A16 | O |
    TC + CUDA 核心 | 16b |'
- en: '| SDQ (Our Work) | N:4/N:8 | W4A4/W8A8 | O | Flexible Sparse TC | 4b/8b |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| SDQ (我们的工作) | N:4/N:8 | W4A4/W8A8 | O | 灵活稀疏 TC | 4b/8b |'
- en: 'Table 1: Summary of related work on sparsification and quantization. TC means
    Tensor Core and Unstr. means Unstructured sparsity.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：稀疏化和量化相关工作的总结。TC 代表张量核心，Unstr. 代表非结构化稀疏性。
- en: 2.2 Sparification
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 稀疏化
- en: Sparsification induces sparsity in tensors, i.e. making some non-zero elements
    to zeros. The most popular sparsification technique used for DNNs is weight pruning
    which statically removes non-zero weight values by setting them zeros. Pruning
    DNN models is effective for classic DNNs as they are often overly parameterized,
    so removing insignificant values would not impact model quality when done carefully.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏化在张量中引入稀疏性，即将一些非零元素变为零。用于深度神经网络的最流行的稀疏化技术是权重剪枝，它通过将非零权重值设置为零来静态地移除这些值。对于经典深度神经网络，剪枝模型是有效的，因为它们通常过度参数化，因此移除不重要的值不会在小心操作时影响模型质量。
- en: 'The simplest way to perform pruning is by choosing insignificant values based
    on the magnitudes Han et al. ([2015](#bib.bib12)) as shown in [Figure 3](#S2.F3
    "Figure 3 ‣ 2.2 Sparification ‣ 2 Background"), but previous work has proposed
    various metrics to decide how to choose values to be removed. For example, some
    methods leverage input samples and use the first order error Molchanov et al.
    ([2022](#bib.bib24)) during inference to determine which weigh value can be pruned
    with minimal error. Other method proposes to use second order error (Hessian) Frantar
    & Alistarh ([2023](#bib.bib7)) to predict impact of weight pruning more accurately,
    but at a higher computation cost, since calculating the Hessian matrix is non-trivial.
    While these pruning methods uses different metrics, they are very successful in
    compressing the conventional DNN models: many of them show possibility to compress
    more than 10$\times$.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 执行剪枝的最简单方法是根据幅度 Han et al. ([2015](#bib.bib12)) 选择不重要的值，如 [图 3](#S2.F3 "Figure
    3 ‣ 2.2 Sparification ‣ 2 Background") 所示，但以往的工作提出了各种度量标准来决定如何选择要移除的值。例如，一些方法利用输入样本，并在推理过程中使用一阶误差 Molchanov
    et al. ([2022](#bib.bib24)) 来确定哪些权重值可以以最小误差进行剪枝。其他方法建议使用二阶误差（Hessian） Frantar
    & Alistarh ([2023](#bib.bib7)) 更准确地预测权重剪枝的影响，但计算成本更高，因为计算 Hessian 矩阵并不简单。虽然这些剪枝方法使用了不同的度量标准，但它们在压缩传统深度神经网络模型方面非常成功：许多方法显示了压缩超过
    10$\times$ 的可能性。
- en: Unlike a plethora of work on sparsifying conventional DNNs such as CNNs, there
    is a still limited number of work on sparisfying LLMs due to two major challeneges.
    First, LLMs are usually less over-parameterized Chowdhery et al. ([2022](#bib.bib2))
    than the classic models. Therefore, most of the weight values are relatively crucial
    and cannot be pruned without impacting the model quality. Second, LLMs have much
    larger total parameter counts, which leads to even higher computation costs to
    apply first- and second-order method for pruning.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与众多关于稀疏化传统深度神经网络（如CNN）的研究不同，由于两大主要挑战，稀疏化大规模语言模型（LLMs）的工作仍然较少。首先，LLMs 通常比经典模型的参数化程度低 Chowdhery
    et al. ([2022](#bib.bib2))。因此，大多数权重值相对重要，不能在不影响模型质量的情况下进行剪枝。其次，LLMs 的总参数量更大，这导致应用一阶和二阶方法进行剪枝的计算成本更高。
- en: Most recently, Wanda Sun et al. ([2023](#bib.bib30)) and SparseGPT Frantar &
    Alistarh ([2023](#bib.bib7)) shows how to avoid the expensive computation to prune
    LLMs and successfully demonstrate a family of pruned LLM models. OWL Yin et al.
    ([2023](#bib.bib38)) improves on these techniques with an outlier-aware and non-uniform
    layer pruning. However, all of them can only prune and compress the model by about
    2$\times$ without significant impacting the model quality. As we will see in later
    sections, compressing LLMs with pruning is still challenging.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Wanda Sun等人（[2023](#bib.bib30)）和SparseGPT Frantar & Alistarh（[2023](#bib.bib7)）展示了如何避免昂贵的计算来剪枝LLMs，并成功展示了一系列剪枝LLM模型。OWL
    Yin等人（[2023](#bib.bib38)）在这些技术上进行了改进，提出了异常值感知和非均匀层剪枝。然而，他们都只能将模型剪枝和压缩约2$\times$，而对模型质量的影响不大。如我们在后续章节中将看到的，压缩LLMs仍然具有挑战性。
- en: '![Refer to caption](img/8d2daf80062526690bc4889ff812b959.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8d2daf80062526690bc4889ff812b959.png)'
- en: 'Figure 3: Overview of sparsification and quantization.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：稀疏化和量化概述。
- en: 2.3 Quantization
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 量化
- en: 'Quantization is another popular method for DNN compression. Contrary to sparsification,
    quantization aims to represent “all” values in a tensor with a lower-bit-width
    format. Since most models can be trained with fp16 computation, deploying the
    trained model and performing inference with fp16 is the natural extension. As
    many researchers have found out Dai et al. ([2021](#bib.bib3)), once a model is
    trained, representing the weight value in lower-bit-width does not impact the
    model quality much. For example, an original value in a higher precision can be
    rounded to the nearest (RTN) lower precision value as shown in [Figure 3](#S2.F3
    "Figure 3 ‣ 2.2 Sparification ‣ 2 Background"). Besides the bit width of the quantization
    format, it is also crucial to decide the target for the quantization: weights
    and/or activations. When quantization is applied to both weights and activations,
    it reduces the cost of required hardware unit and enables the exploitation of
    simpler arithmetic computations with low-bit-width compared to the fp16 computations,
    which saves area and power and increases compute throughput. As a result, commercial
    products NVIDIA ([2020](#bib.bib25); [2022](#bib.bib26)) have included fp8 and
    int8 computation support with a higher peak computation throughput for inference.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是另一种流行的DNN压缩方法。与稀疏化不同，量化旨在用更低位宽的格式表示张量中的“所有”值。由于大多数模型可以使用fp16计算进行训练，将训练后的模型部署并使用fp16进行推理是自然的扩展。正如许多研究者发现的那样，Dai等人（[2021](#bib.bib3)），一旦模型训练完成，将权重值表示为低位宽对模型质量的影响不大。例如，如[图3](#S2.F3
    "Figure 3 ‣ 2.2 Sparification ‣ 2 Background")所示，将更高精度的原始值四舍五入到最接近的低精度值。除了量化格式的位宽外，还需决定量化的目标：权重和/或激活。当量化同时应用于权重和激活时，可以减少所需硬件单元的成本，并利用低位宽的简单算术运算，相比fp16计算，节省面积和功耗并提高计算吞吐量。因此，商业产品NVIDIA（[2020](#bib.bib25);
    [2022](#bib.bib26)）已支持fp8和int8计算，具有更高的推理峰值计算吞吐量。
- en: Similar to classic DNNs, LLMs can be quantized to lower bit precision such as
    int8 or fp8 after training. Previous work Dettmers et al. ([2022](#bib.bib5))
    shows how to quantize both weights and activations of LLMs with int8 (W8A8) and
    leverage 8-bit matrix multiplication unit in modern hardware. On the other hand,
    other work such as GPTQ Frantar et al. ([2023](#bib.bib9)) or AWQ Lin et al. ([2023](#bib.bib20))
    focus on weight-only quantization to minimize the memory footprint of weights
    while keeping activations as is. Both GPTQ and AWQ show that it is able to quantize
    weights to 4 bits while maintaining the quality of LLMs (W4A16). Although they
    reduce the bit width of weights aggressively, the weights are converted back to
    fp16 and use lower-throughput fp16 hardware units during the inference as activations
    are not quantized.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于经典DNNs，LLMs在训练后可以量化为更低位宽，如int8或fp8。之前的研究Dettmers等人（[2022](#bib.bib5)）展示了如何将LLMs的权重和激活量化为int8（W8A8），并利用现代硬件中的8位矩阵乘法单元。另一方面，其他工作如GPTQ
    Frantar等人（[2023](#bib.bib9)）或AWQ Lin等人（[2023](#bib.bib20)）专注于仅权重量化，以最小化权重的内存占用，同时保持激活不变。GPTQ和AWQ都显示可以将权重量化为4位，同时保持LLMs的质量（W4A16）。虽然它们大幅降低了权重的位宽，但由于激活未被量化，权重在推理过程中会被转换回fp16，并使用低吞吐量的fp16硬件单元。
- en: An important problem in LLM quantization, as pointed out by recent work Dettmers
    et al. ([2022](#bib.bib5)), is that the quantization error becomes large due to
    a few outliers in activations, especially for the large models. These outliers,
    by definition, are often a very small portion ($<10\%$ at most (fp16 into fp8/int8).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM量化中，一个重要的问题，如近期工作 Dettmers等人（[2022](#bib.bib5)）指出的，量化误差会由于激活中的少量异常值而变大，特别是在大模型中。这些异常值，按定义，通常是非常小的一部分（最多<$10\%$（fp16到fp8/int8）。
- en: '[Table 1](#S2.T1 "Table 1 ‣ 2.1 Compressing LLM models ‣ 2 Background") compares
    the mentioned prior work above, and our proposed method SDQ. Unlike the previous
    works, our method uses structured sparse decomposition with different bit widths
    to achieve 4$\times$ compute throughput.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1](#S2.T1 "Table 1 ‣ 2.1 Compressing LLM models ‣ 2 Background") 比较了上述提到的先前工作和我们提出的方法SDQ。与之前的工作不同，我们的方法使用不同位宽的结构化稀疏分解来实现4$\times$的计算吞吐量。'
- en: 3 Comparing Compression Methods
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 比较压缩方法
- en: Since LLM inference stresses the system in various ways (computation, memory
    capacity, etc.), different compression methods often target different goals to
    optimize. In this section, we define a few metrics that can be used to estimate
    the computation and memory capacity savings for each method and highlight the
    critical design choices for them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM推理在计算、内存容量等方面对系统施加了各种压力，不同的压缩方法通常针对不同的目标进行优化。在本节中，我们定义了一些可以用于估算每种方法的计算和内存容量节省的指标，并强调它们的关键设计选择。
- en: 3.1 Effective compute throughput for sparsification
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 稀疏化的有效计算吞吐量
- en: When multiplying a weight matrix with an activation matrix, if there is overall
    90% unstructured sparsity in weights, the required number of Multiply–Accumulate
    (MAC) operations can be reduced by 90% by skipping ineffectual computations. However,
    identifying effectual computations and only mapping them to the compute unit (e.g.,
    tensor cores) is not trivial, so it is difficult to exploit unstructured sparsity
    without investing significant area and power Qin et al. ([2020](#bib.bib28)).
    To mitigate the problem, recent work from both industry and academia Zhu et al.
    ([2019](#bib.bib40)); NVIDIA ([2020](#bib.bib25)); Jeong et al. ([2023](#bib.bib15))
    propose to use *N:M* structured sparsity (at most *N* non-zero values in each
    consecutive *M* values, as shown in [Figure 3](#S2.F3 "Figure 3 ‣ 2.2 Sparification
    ‣ 2 Background")), which provides predictable compute throughput while exploiting
    sparsity for higher performance. For example, with NVIDIA Ampere sparse tensor
    core, using 2:4 structured sparsity can reduce 50% of the required number of MAC
    operations by skipping ineffectual computations. Using the 1:8 structured sparsity
    in an emerging Sparse Tensor Core Liu et al. ([2021](#bib.bib21)); Jeong et al.
    ([2023](#bib.bib15)) can reduce up to 87.5%. Generally, using *N:M* structured
    sparse hardware support, one can reduce the required computation by $\frac{M-N}{M}\times
    100$%) and increase the throughput respectively.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当将权重矩阵与激活矩阵相乘时，如果权重中存在总体90%的无结构稀疏性，通过跳过无效计算，可以减少90%的乘加（MAC）操作所需的数量。然而，识别有效计算并仅将其映射到计算单元（例如，张量核心）并非易事，因此在没有大量投入面积和功耗的情况下，很难利用无结构稀疏性 Qin等人（[2020](#bib.bib28)）。为了缓解这一问题，来自业界和学术界的近期工作 Zhu等人（[2019](#bib.bib40)）；
    NVIDIA（[2020](#bib.bib25)）； Jeong等人（[2023](#bib.bib15)）建议使用*N:M* 结构化稀疏性（在每*M*
    连续值中最多有*N* 个非零值，如[图3](#S2.F3 "Figure 3 ‣ 2.2 Sparification ‣ 2 Background")所示），它在利用稀疏性的同时提供了可预测的计算吞吐量，以实现更高的性能。例如，使用
    NVIDIA Ampere 稀疏张量核心时，使用 2:4 结构化稀疏性可以通过跳过无效计算来减少50%的MAC操作所需数量。使用1:8结构化稀疏性在新兴的稀疏张量核心中 Liu等人（[2021](#bib.bib21)）；
    Jeong等人（[2023](#bib.bib15)）可以减少高达87.5%。通常，使用*N:M* 结构化稀疏硬件支持，可以将所需计算减少 $\frac{M-N}{M}\times
    100$%) 并相应地提高吞吐量。
- en: In this work, we focus on using a futuristic, *N:M* structured sparse tensor
    core support that can provide $\frac{M}{N}\times$ compute throughput for *N:M*
    structured sparsity, which is more practical and realistic.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们专注于使用未来主义的*N:M* 结构化稀疏张量核心支持，它可以提供 $\frac{M}{N}\times$ 计算吞吐量，用于*N:M*
    结构化稀疏性，这更加实际和现实。
- en: 3.2 Effective compute throughput for quantization
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 量化的有效计算吞吐量
- en: On the other hand, unlike sparsification, quantization methods are not directly
    used to reduce the number of MAC operations as there is no ineffectual computation
    after quantization. However, if both weights and activations are quantized, the
    low-bit-width computation unit (e.g., INT8 Tensor Core) can be used, which is
    more efficient compared to the full-precision computation unit. Thus, given the
    same area/power budget, the number of low-bit arithmetic units would be higher
    than that of high-bit arithmetic units, but the exact ratio could vary depending
    on the actual implementation. As mentioned earlier, the compute throughput of
    4bit and 8bit format is 4$\times$ compared to that of the 16bit format, respectively
    in NVIDIA Ampere GPU NVIDIA ([2020](#bib.bib25)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，与稀疏化不同，量化方法并不会直接用于减少 MAC 操作的数量，因为量化后没有无效计算。然而，如果权重和激活都被量化，则可以使用低比特宽度计算单元（例如
    INT8 张量核心），相比全精度计算单元，这种单元更高效。因此，在相同的面积/功率预算下，低比特运算单元的数量会高于高比特运算单元，但具体比例可能因实际实现而异。如前所述，4bit
    和 8bit 格式的计算吞吐量分别是 16bit 格式的 4$\times$，如 NVIDIA Ampere GPU NVIDIA ([2020](#bib.bib25))
    所述。
- en: In this work, we assume fp16 as the baseline format for LLM inference, so we
    assume $\frac{16}{n}\times$-bit for computations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们假设 fp16 为 LLM 推理的基准格式，因此我们假设计算使用 $\frac{16}{n}\times$-bit。
- en: '![Refer to caption](img/c604492704d5d5b5cc5bc840d1ce7124.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c604492704d5d5b5cc5bc840d1ce7124.png)'
- en: 'Figure 4: Data size for 32 elements with 1:4/2:4/3:4/Dense sparsity with quantization
    using 1) 32 bit scale factor with Q-Vector size of 16 (first row) 2) 8 bit scale
    factor with Q-Vector size of 32 (second row). SF and Q-VS represent Scale Factor
    and Q-Vector Size.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：使用 1) 32 位尺度因子和 Q-Vector 大小为 16（第一行），2) 8 位尺度因子和 Q-Vector 大小为 32（第二行）对 32
    个元素进行 1:4/2:4/3:4/密集稀疏性的量化。SF 和 Q-VS 分别表示尺度因子和 Q-Vector 大小。
- en: 3.3 Average bits per weight element
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 每个权重元素的平均比特数
- en: The improved effective compute throughput mentioned above do not come for free.
    To maximize the advantage of sparsification, weights are often stored in a compressed
    format, such as bitmask, ELLPACK, CSR, etc. A compressed format usually requires
    both data (for the actual non-zero values) and metadata (for the indexes of non-zero
    values). Thus, the overhead in terms of memory is metadata to store the indexes
    of non-zero values. In [Figure 3](#S2.F3 "Figure 3 ‣ 2.2 Sparification ‣ 2 Background"),
    we show how a simple vector composed of 8 values can be sparisfied to a vector
    with 2:4 structured sparsity. For *N:M* sparsity, we call *M* as the size of the
    S-Vector, so in this case, 4 is the size of the S-Vector. Both *N* and *M* could
    affect the size of the metadata. For example, if we use ELLPACK-like format that
    stores indexes of non-zero elements in a vector, 2:4 sparsity requires 2 bits
    per index ($log_{2}4$ bits per vector.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上述改进的有效计算吞吐量并不是免费的。为了最大化稀疏化的优势，权重通常以压缩格式存储，如位掩码、ELLPACK、CSR 等。压缩格式通常需要数据（实际的非零值）和元数据（非零值的索引）。因此，内存开销是存储非零值索引的元数据。在
    [图 3](#S2.F3 "Figure 3 ‣ 2.2 Sparification ‣ 2 Background") 中，我们展示了一个由 8 个值组成的简单向量如何被稀疏化为具有
    2:4 结构稀疏性的向量。对于 *N:M* 稀疏性，我们称 *M* 为 S-Vector 的大小，因此在此情况下，4 是 S-Vector 的大小。*N*
    和 *M* 都可能影响元数据的大小。例如，如果我们使用类似 ELLPACK 的格式来存储非零元素的索引，在 2:4 稀疏性下，每个索引需要 2 比特（$log_{2}4$
    比特每个向量）。
- en: For quantization, the overhead that needs careful consideration is caused by
    scale factors. In [Figure 3](#S2.F3 "Figure 3 ‣ 2.2 Sparification ‣ 2 Background"),
    three different quantization method is used; round-to-neareast, round-to-nearest
    with a global scale factor, and round-to-nearest with per-vector scale factor
    (we call this as Q-Vector to distinguish with S-Vector). A scale factor is used
    to match the range of quantized values and the range of the original values. For
    example, without using a scale factor, all the values larger than 8 in [Figure 3](#S2.F3
    "Figure 3 ‣ 2.2 Sparification ‣ 2 Background") will be quantized to 8, which could
    cause a significantly large quantization error. As the finer granularity of the
    scale factor can provide more tuned quantization, it is better to have a smaller
    Q-Vector size for reducing quantization error. However, similar to the indexes
    for compressed format, scale factors are the metadata for quantization. For example,
    assume a case where a value is quantized from 16-bit to 4-bit while a scale factor
    is 16-bit, and Q-Vector size is 4. For each vector, the data size would be $4\times
    4=16$ bits). Thus, scale factors are often also quantized and Q-Vector size is
    kept relatively large (16-64) Dai et al. ([2021](#bib.bib3)).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于量化，需要仔细考虑的开销是由缩放因子造成的。在[图 3](#S2.F3 "Figure 3 ‣ 2.2 Sparification ‣ 2 Background")中使用了三种不同的量化方法：最接近值四舍五入、带有全局缩放因子的最接近值四舍五入以及带有每向量缩放因子的最接近值四舍五入（我们称之为Q-Vector以区别于S-Vector）。缩放因子用于匹配量化值的范围和原始值的范围。例如，如果不使用缩放因子，[图
    3](#S2.F3 "Figure 3 ‣ 2.2 Sparification ‣ 2 Background")中的所有大于8的值将被量化为8，这可能会导致显著的量化误差。由于更细的缩放因子粒度可以提供更精确的量化，因此较小的Q-Vector尺寸可以减少量化误差。然而，与压缩格式的索引类似，缩放因子是量化的元数据。例如，假设一个值从16位量化到4位，同时缩放因子为16位，而Q-Vector大小为4。对于每个向量，数据大小将是$4\times
    4=16$位。因此，缩放因子通常也会被量化，Q-Vector大小保持相对较大（16-64）Dai等（[2021](#bib.bib3)）。
- en: In [Figure 4](#S3.F4 "Figure 4 ‣ 3.2 Effective compute throughput for quantization
    ‣ 3 Comparing Compression Methods"), we show how much portion the size of metadata
    for sparsification (Metadata-S) and metadata for quantization (Metadata-Q) would
    take compared to the size of the actual data, showing that the metadata overhead
    caused by quantization and sparsification needs to be carefully considered as
    it could nullify the benefits of the optimization methods. For example, a 3:4
    sparse, 4-bit quantized model can have a higher bit-per-weight than a dense, 4-bit
    quantized model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 4](#S3.F4 "Figure 4 ‣ 3.2 Effective compute throughput for quantization
    ‣ 3 Comparing Compression Methods")中，我们展示了稀疏化的元数据（Metadata-S）和量化的元数据（Metadata-Q）占实际数据大小的比例，说明量化和稀疏化带来的元数据开销需要仔细考虑，因为它可能会抵消优化方法的好处。例如，一个3:4稀疏、4位量化的模型可能比一个密集的4位量化模型具有更高的每权重比特数。
- en: '4 SDQ: Tackling Challenges in LLM Compression with a Hybrid method'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 SDQ：采用混合方法应对LLM压缩挑战
- en: In [Table 1](#S2.T1 "Table 1 ‣ 2.1 Compressing LLM models ‣ 2 Background"),
    we summarize the previous work related to sparsification and quantization for
    LLMs. The first observation from sparsification perspective is that even though
    2:4/4:8 sparsity would provide 2$\times$ effective compute throughput.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表 1](#S2.T1 "Table 1 ‣ 2.1 Compressing LLM models ‣ 2 Background")中，我们总结了与LLM的稀疏化和量化相关的先前工作。从稀疏化的角度来看，第一个观察结果是，尽管2:4/4:8稀疏度可以提供2$\times$有效计算吞吐量。
- en: The second observation is that the previous work focusing on weight-only quantization
    is able to reduce the weight bit width to 4 bit while dual quantization can only
    reduce the bit width to 8 bit Dettmers et al. ([2022](#bib.bib5)) as it is harder
    to quantize both operands while maintaining the quality of the model. Thus, even
    in the quantization perspective, it can only achieves 2$\times$ effective compute
    throughput due to the quality drop after quantization.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个观察结果是，专注于仅权重量化的先前工作能够将权重位宽减少到4位，而双重量化只能将位宽减少到8位Dettmers等（[2022](#bib.bib5)），因为在保持模型质量的同时量化两个操作数更为困难。因此，即使从量化的角度来看，由于量化后的质量下降，它也只能实现2$\times$有效计算吞吐量。
- en: The last observation is that various recent work have noticed that isolating
    outliers is shown to be crucial to maintain quality after LLM quantization Dettmers
    et al. ([2022](#bib.bib5); [2023](#bib.bib6)); Lee et al. ([2023](#bib.bib19));
    Kim et al. ([2023](#bib.bib16)). They have found that outliers in LLMs makes them
    hard to be quantized without causing huge error. This is because even though there
    is a single outlier which is significantly larger than other values, that would
    make the corresponding scale factor much larger so that other small inliers would
    end up being quantized to the same value.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个观察结果是，最近的各种研究发现，隔离异常值对于在 LLM 量化后保持质量至关重要 Dettmers et al. ([2022](#bib.bib5);
    [2023](#bib.bib6)); Lee et al. ([2023](#bib.bib19)); Kim et al. ([2023](#bib.bib16))。他们发现
    LLM 中的异常值使得量化变得困难且会导致巨大误差。这是因为即使存在一个显著大于其他值的异常值，这也会使相应的缩放因子变得更大，从而导致其他小的正常值最终被量化为相同的值。
- en: To resolve this issue, previous work extracted around 1-5% outliers and stored
    using a higher precision separately in a compressed format. During the inference,
    they treat these outliers as an unstructured sparse matrix and invoke SpMM kernels
    separately on either CPU or GPU CUDA Cores, instead of the Tensor Core. This extra
    SpMM with unstructured sparse outliers is costly even with just 1% outliers due
    to the huge compute throughput difference between CUDA Cores and Tensor Cores.
    For example, in NVIDIA A100, the peak compute throughput difference between CUDA
    Cores and Tensor Cores is around 10$-$ depending on the precision. We observed
    that even with 0.5% sparsity ratio, using cuSparse for SpMM on CUDA cores is slower
    than treating the matrices as dense matrices and invoke a dense, accelerated GEMM
    on Tensor Cores Micikevicius ([2021](#bib.bib22)).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，之前的工作提取了大约 1-5% 的异常值，并以更高的精度单独存储在压缩格式中。在推理过程中，他们将这些异常值视为非结构稀疏矩阵，并分别在
    CPU 或 GPU CUDA 核心上调用 SpMM 内核，而不是 Tensor Core。即使只有 1% 的异常值，这种额外的 SpMM 也很昂贵，因为 CUDA
    核心和 Tensor Core 之间的计算吞吐量差异巨大。例如，在 NVIDIA A100 中，CUDA 核心和 Tensor Core 之间的峰值计算吞吐量差异大约为
    10$-$，具体取决于精度。我们观察到，即使在 0.5% 的稀疏比下，使用 cuSparse 在 CUDA 核心上进行 SpMM 的速度也比将矩阵视为密集矩阵并在
    Tensor Core 上调用密集的加速 GEMM 要慢 Micikevicius ([2021](#bib.bib22))。
- en: Based on the three observations, we propose to extract local outliers with *N:M*
    structrued sparse tensor instead of extracting global outliers with unstructured
    sparse tensor. As the previous work extract outliers globally, i.e., select elements
    in the entire tensor that has extreme values in a certain metric, the extracted
    global outliers are naturally unstructured sparse, which is not able to be accelerated
    through structured spare Tensor Core. Instead, we propose to extract outlier per
    S-vector (we call this as local outliers), locally within a 1D vector. For example,
    assume that there is a vector composed of 16 elements with a single global outlier
    and we use 1:8 structured sparsity to extract local outliers for the given vector.
    As we extract a local outlier per vector, a single global outlier would be guaranteed
    to be captured by 1:8 local outlier extraction. If we assume there are two global
    outliers in the given vector, both global outliers can be extracted by 1:8 local
    outlier extraction only if they are in different S-vector.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这三个观察结果，我们建议使用 *N:M* 结构稀疏张量提取本地异常值，而不是使用非结构稀疏张量提取全球异常值。由于之前的工作是全局提取异常值，即在整个张量中选择在某个度量下具有极端值的元素，因此提取的全球异常值自然是非结构稀疏的，这无法通过结构化稀疏
    Tensor Core 加速。相反，我们建议在每个 S 向量内（我们称之为本地异常值）局部提取异常值。例如，假设有一个由 16 个元素组成的向量，其中有一个全球异常值，我们使用
    1:8 结构稀疏性来提取给定向量的本地异常值。由于我们在每个向量中提取一个本地异常值，1:8 本地异常值提取将确保捕获单个全球异常值。如果我们假设给定向量中有两个全球异常值，只有当它们位于不同的
    S 向量中时，1:8 本地异常值提取才能提取这两个全球异常值。
- en: It is possible that the two global outliers happen to be in the same S-vector,
    so in that case, the 1:8 local outlier extraction would not be able to capture
    that. For a rare case, when there are three global outliers in the given vector,
    the 1:8 local outlier extraction can miss 1-2 global outliers depending on the
    distribution of the outliers.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 可能存在两个全球异常值恰好位于同一个 S 向量中，因此在这种情况下，1:8 本地异常值提取无法捕捉到它。对于一种少见情况，当给定向量中有三个全球异常值时，1:8
    本地异常值提取可能会遗漏 1-2 个全球异常值，具体取决于异常值的分布。
- en: Thus, *N:M* local outlier extraction would only be effective if the outlier
    ratio is small and the location distribution is not extremely skewed. Fortunately,
    in LLMs, the percentage of outliers is reported around 1-5% Guo et al. ([2023](#bib.bib11));
    Dettmers et al. ([2023](#bib.bib6)), which is small enough to use *N:M* local
    outlier extraction without missing too most of the global outliers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*N:M* 本地异常值提取仅在异常值比例较小且位置分布不极端偏斜时才有效。幸运的是，在 LLMs 中，异常值的百分比报告在 1-5% 之间 Guo
    等（[2023](#bib.bib11)）；Dettmers 等（[2023](#bib.bib6)），这一比例足够小，可以使用 *N:M* 本地异常值提取而不会遗漏太多的全局异常值。
- en: '![Refer to caption](img/8fd4739c7fdf57608acab7646bf76fe7.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8fd4739c7fdf57608acab7646bf76fe7.png)'
- en: 'Figure 5: Local outlier extraction using *N:8* for global outliers and semi-local
    outliers.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：使用 *N:8* 进行的全局异常值和半本地异常值的本地异常值提取。
- en: 'To demonstrate this, in the left plot of the [Figure 5](#S4.F5 "Figure 5 ‣
    4 SDQ: Tackling Challenges in LLM Compression with a Hybrid method"), how much
    coverage *N*:8 local outlier extraction show on one of the OPT-6.7B layer. As
    expected, we observe that 2:8 is enough capture 99% of global outliers if the
    outlier ratio is smaller than 4%.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '为了演示这一点，在 [图 5](#S4.F5 "Figure 5 ‣ 4 SDQ: Tackling Challenges in LLM Compression
    with a Hybrid method") 的左侧图中，显示了 *N*:8 本地异常值提取在 OPT-6.7B 层上的覆盖情况。如预期的那样，我们观察到
    2:8 足以捕捉 99% 的全局异常值，如果异常值比例小于 4%。'
- en: Moreover, *N:M* local outlier extraction would be more efficient with the recent
    trend of finer granularity for the scale factors, i.e. smaller Q-Vector size.
    As a separate scale factor would be used for each Q-Vector, the outlier extraction
    needs to capture semi-local (Q-Vector-wise) outliers, not the global outliers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着对缩放因子进行更细粒度处理的最新趋势，即较小的 Q-Vector 大小，*N:M* 本地异常值提取将更有效。由于每个 Q-Vector 将使用单独的缩放因子，异常值提取需要捕捉半本地（按
    Q-Vector）异常值，而不是全局异常值。
- en: 'In the right plot of the  [Figure 5](#S4.F5 "Figure 5 ‣ 4 SDQ: Tackling Challenges
    in LLM Compression with a Hybrid method"), we show how *N:M* local outlier extraction
    is able to cover semi-local outliers when the Q-Vector size is 64. As expected,
    *N:M* local outlier extraction is able to show much higher coverage for semi-local
    outliers than global outliers, for example, 1:8 is enough to capture all semi-local
    outliers up to 3% outlier ratio, thanks to its relatively regular outlier pattern.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [图 5](#S4.F5 "Figure 5 ‣ 4 SDQ: Tackling Challenges in LLM Compression with
    a Hybrid method") 的右侧图中，我们展示了当 Q-Vector 大小时 *N:M* 本地异常值提取如何能够覆盖半本地异常值。如预期的那样，*N:M*
    本地异常值提取能为半本地异常值提供比全局异常值更高的覆盖率。例如，1:8 足以捕捉所有半本地异常值，异常值比例高达 3%，这要归功于其相对规则的异常值模式。'
- en: 5 SDQ For LLMs
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 SDQ 对 LLMs 的应用
- en: In this section, we explain our framework for SDQ. As the name implies, it consists
    of three stages, sparsificaion, decomposition, and quantization, where each stage
    can leverage different existing methods and works together as a hybrid method.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们解释了我们的 SDQ 框架。顾名思义，它由三个阶段组成：稀疏化、分解和量化，每个阶段可以利用不同的现有方法，并作为混合方法一起工作。
- en: '![Refer to caption](img/70b119131582492f56b45a2481a64fe2.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/70b119131582492f56b45a2481a64fe2.png)'
- en: 'Figure 6: SDQ flow with a vector size of 16.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：具有 16 的向量大小的 SDQ 流程。
- en: '![Refer to caption](img/5936113692bf9a00b74d0198a72eb796.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5936113692bf9a00b74d0198a72eb796.png)'
- en: 'Figure 7: Overall SDQ framework composed of three stages.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：由三个阶段组成的整体 SDQ 框架。
- en: '![Refer to caption](img/eca12593bf3261bf886fb8cfd1221d7b.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/eca12593bf3261bf886fb8cfd1221d7b.png)'
- en: 'Figure 8: Performance estimation with SDQ.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：使用 SDQ 的性能估计。
- en: 'Stage 1 Sparsification: SDQ first sparsify the weights of LLMs as much as possible,
    until the LLM quality are impacted signicantly (e.g., 1% increase in perplexity).
    SDQ prunes weights based on a significance metric under the structured pattern
    constraint, such as *N:M*. The most straightforward metric is magnitude as the
    weight with a small magnitude is likely to not affect much to the final output Mishra
    et al. ([2021](#bib.bib23)). The framework only keeps the largest *N* values in
    each block that is composed of consecutive *M* values.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段 1 稀疏化：SDQ 首先尽可能地稀疏化 LLM 的权重，直到 LLM 的质量受到显著影响（例如，困惑度增加 1%）。SDQ 基于结构化模式约束下的显著性度量（如
    *N:M*）修剪权重。最直接的度量是幅度，因为幅度小的权重可能对最终输出影响较小 Mishra 等（[2021](#bib.bib23)）。该框架仅保留每个由连续
    *M* 个值组成的块中的最大 *N* 个值。
- en: If using calibration data is allowed, more sophisticated metrics could be used.
    For example, another metric is based on weight-activation-product (Wanda Sun et al.
    ([2023](#bib.bib30))) that is able to take the activation into account while choosing
    the significant weight values. Even though a weight value itself is tiny, it is
    possible that the corresponding activation is huge, increasing the impact to the
    output. We also use SparseGPT Frantar & Alistarh ([2023](#bib.bib7)) that is also
    using calibration data. Unlike the previous methods, SparseGPT updates weights
    in the sparsification to compensate for the error during the process and uses
    a Hessian-based metric as a significance metric. In [Figure 6](#S5.F6 "Figure
    6 ‣ 5 SDQ For LLMs"), the sparsification stage uses 6:8 as the target structured
    sparsity using the given significance metric.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果允许使用校准数据，可以使用更复杂的指标。例如，另一种指标基于权重-激活值乘积（Wanda Sun 等（[2023](#bib.bib30)）），能够在选择重要的权重值时考虑激活值。即使一个权重值本身很小，但其对应的激活值可能很大，从而增加对输出的影响。我们还使用
    SparseGPT Frantar 和 Alistarh（[2023](#bib.bib7)），该方法也使用了校准数据。与之前的方法不同，SparseGPT
    在稀疏化过程中更新权重以弥补过程中的误差，并使用基于 Hessian 的指标作为重要性指标。在 [图 6](#S5.F6 "Figure 6 ‣ 5 SDQ
    For LLMs") 中，稀疏化阶段使用 6:8 作为目标结构化稀疏度，使用给定的重要性指标。
- en: '|  | OPT |  |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | OPT |  |  |'
- en: '| Optimization Configuration | 125M | 350M | 1.3B | 2.7B | 6.7B | 13B | 30B
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 优化配置 | 125M | 350M | 1.3B | 2.7B | 6.7B | 13B | 30B |'
- en: '| 1$\times$ Effective Compute Throughput |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 1$\times$ 有效计算吞吐量 |'
- en: '| Dense-WA16 | 27.65 | 22.00 | 14.62 | 12.47 | 10.86 | 10.13 | 9.56 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Dense-WA16 | 27.65 | 22.00 | 14.62 | 12.47 | 10.86 | 10.13 | 9.56 |'
- en: '| S-RTN-W4 | - | - | - | - | 12.10 | 11.32 | 10.97 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| S-RTN-W4 | - | - | - | - | 12.10 | 11.32 | 10.97 |'
- en: '| S-GPTQ-W4 | - | - | - | - | 11.39 | 10.31 | 9.63 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| S-GPTQ-W4 | - | - | - | - | 11.39 | 10.31 | 9.63 |'
- en: '| S-SpQR-W4 | - | - | - | - | 11.04 | 10.28 | 9.54 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| S-SpQR-W4 | - | - | - | - | 11.04 | 10.28 | 9.54 |'
- en: '| 2$\times$ Effective Compute Throughput |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 2$\times$ 有效计算吞吐量 |'
- en: '| S-Wanda-4:8 | 53.12 | 58.90 | 22.21 | 16.77 | 13.55 | 13.35 | 10.87 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| S-Wanda-4:8 | 53.12 | 58.90 | 22.21 | 16.77 | 13.55 | 13.35 | 10.87 |'
- en: '| S-SparseGPT-4:8 | 43.99 | 38.92 | 20.06 | 15.04 | 12.57 | 11.83 | 10.31 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| S-SparseGPT-4:8 | 43.99 | 38.92 | 20.06 | 15.04 | 12.57 | 11.83 | 10.31 |'
- en: '| Q-VSQuant-WAint8 | 27.77 | 22.01 | 14.75 | 12.50 | 10.95 | 10.14 | 9.57 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Q-VSQuant-WAint8 | 27.77 | 22.01 | 14.75 | 12.50 | 10.95 | 10.14 | 9.57 |'
- en: '| Q-VSQuant-WAffp8 | 27.78 | 22.05 | 14.75 | 12.51 | 10.95 | 10.13 | 9.58 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Q-VSQuant-WAffp8 | 27.78 | 22.05 | 14.75 | 12.51 | 10.95 | 10.13 | 9.58 |'
- en: '| 3.6$\times$ Effective Compute Throughput |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 3.6$\times$ 有效计算吞吐量 |'
- en: '| SDQ-8:8-1:8int8-7:8fp4 | 28.99 | 22.87 | 15.36 | 12.62 | 11.06 | 10.24 |
    9.66 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-8:8-1:8int8-7:8fp4 | 28.99 | 22.87 | 15.36 | 12.62 | 11.06 | 10.24 |
    9.66 |'
- en: '| 4$\times$ Effective Compute Throughput |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 4$\times$ 有效计算吞吐量 |'
- en: '| S-Wanda-2:8 | 1584.81 | 2847.39 | 1217.73 | 6204.19 | 1028.50 | 2010.05 |
    9997.60 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| S-Wanda-2:8 | 1584.81 | 2847.39 | 1217.73 | 6204.19 | 1028.50 | 2010.05 |
    9997.60 |'
- en: '| S-SparseGPT-2:8 | 849.99 | 645.33 | 990.81 | 150.09 | 189.00 | 265.05 | 97.92
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| S-SparseGPT-2:8 | 849.99 | 645.33 | 990.81 | 150.09 | 189.00 | 265.05 | 97.92
    |'
- en: '| Q-VSQuant-WAint4 | 33.10 | 28.25 | 17.84 | 14.63 | 12.02 | 11.72 | 10.61
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Q-VSQuant-WAint4 | 33.10 | 28.25 | 17.84 | 14.63 | 12.02 | 11.72 | 10.61
    |'
- en: '| Q-VSQuant-WAfp4 | 30.50 | 24.31 | 16.00 | 13.29 | 11.32 | 10.57 | 9.90 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Q-VSQuant-WAfp4 | 30.50 | 24.31 | 16.00 | 13.29 | 11.32 | 10.57 | 9.90 |'
- en: '| SDQ-W3:4-1:4int8-2:4fp4 | 31.42 | 26.76 | 16.33 | 13.11 | 11.17 | 10.41 |
    9.63 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-W3:4-1:4int8-2:4fp4 | 31.42 | 26.76 | 16.33 | 13.11 | 11.17 | 10.41 |
    9.63 |'
- en: '| SDQ-S3:4-1:4int8-2:4fp4 | 30.71 | 25.84 | 16.35 | 12.96 | 11.15 | 10.31 |
    9.58 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-S3:4-1:4int8-2:4fp4 | 30.71 | 25.84 | 16.35 | 12.96 | 11.15 | 10.31 |
    9.58 |'
- en: '| SDQ-W6:8-2:8int8-4:8fp4 | 30.15 | 24.48 | 15.71 | 12.68 | 10.91 | 10.27 |
    9.51 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-W6:8-2:8int8-4:8fp4 | 30.15 | 24.48 | 15.71 | 12.68 | 10.91 | 10.27 |
    9.51 |'
- en: '| SDQ-S6:8-2:8int8-4:8fp4 | 29.70 | 24.45 | 16.41 | 12.58 | 10.98 | 10.20 |
    9.52 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-S6:8-2:8int8-4:8fp4 | 29.70 | 24.45 | 16.41 | 12.58 | 10.98 | 10.20 |
    9.52 |'
- en: '| SDQ-W7:8-1:8int8-6:8fp4 | 29.30 | 23.02 | 15.44 | 12.70 | 10.89 | 10.21 |
    9.58 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-W7:8-1:8int8-6:8fp4 | 29.30 | 23.02 | 15.44 | 12.70 | 10.89 | 10.21 |
    9.58 |'
- en: '| SDQ-S7:8-1:8int8-6:8fp4 | 28.82 | 23.20 | 15.83 | 12.56 | 10.98 | 10.26 |
    9.60 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-S7:8-1:8int8-6:8fp4 | 28.82 | 23.20 | 15.83 | 12.56 | 10.98 | 10.26 |
    9.60 |'
- en: 'Table 2: Perplexity results of OPT on raw-WikiText2 with various sparsification,
    quantization, and SDQ configurations.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: OPT 在 raw-WikiText2 上的困惑度结果，使用了各种稀疏化、量化和 SDQ 配置。'
- en: 'Stage 2 Decomposition: In this stage, SDQ decomposes the the weight tensor
    of each layer into two tensors, inliers and outliers using the *N:M* local outlier
    extraction explained in [section 4](#S4 "4 SDQ: Tackling Challenges in LLM Compression
    with a Hybrid method"). In [Figure 6](#S5.F6 "Figure 6 ‣ 5 SDQ For LLMs"), it
    uses 1:8 as the target structured sparsity for the local outlier extraction so
    that the remaining values become the inliers. An interesting point in this example
    after extracting outliers as 1:8 is that the remaining outlier becomes naturally
    structured sparse, in this case, 6:8.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '阶段 2 分解：在这一阶段，SDQ 将每一层的权重张量分解为两个张量，内点和外点，使用 [第 4 节](#S4 "4 SDQ: Tackling Challenges
    in LLM Compression with a Hybrid method") 中解释的 *N:M* 局部异常值提取方法。在 [图 6](#S5.F6
    "Figure 6 ‣ 5 SDQ For LLMs") 中，它将 1:8 作为局部异常值提取的目标结构稀疏性，以便剩余值成为内点。在这个示例中，提取外点为
    1:8 后，剩余的外点自然变成了结构稀疏的，例如 6:8。'
- en: To choose outliers, we need another metric to decide whether a weight value
    is an outlier or not, similar to the significance metric for sparsification. We
    experiment with magnitude-based, activation-weight-product-based, and output-error-based
    metrics (based on the error after quantization). Unlike previous work, we make
    sure both inlier and outlier tensors are *N:M* structured sparse so that we can
    utilize efficient SpMM on structured sparse HW.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择外点，我们需要另一个度量来决定权重值是否为外点，类似于用于稀疏化的显著性度量。我们实验了基于幅度、基于激活-权重乘积和基于输出误差的度量（基于量化后的误差）。与之前的工作不同，我们确保内点和外点张量都是
    *N:M* 结构稀疏的，以便我们可以在结构稀疏硬件上利用高效的 SpMM。
- en: 'Stage 3 Quantization: The last stage of SDQ is quantization where it uses different
    number formats for structured sparse inliers and outliers. For quantization, we
    use VS-Quant Dai et al. ([2021](#bib.bib3)) as it provides flexibility in the
    granularity of scale factors. We use a relatively higher bit width for outliers
    while using a lower bit width for inliers. We also quantize activations accordingly
    so that we can use low-bit arithmetic computations that are cheaper in terms of
    area and power. We show the high-level overview of the SDQ framework in [Figure 7](#S5.F7
    "Figure 7 ‣ 5 SDQ For LLMs").'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段 3 量化：SDQ 的最后阶段是量化，在这个阶段，它对结构稀疏的内点和外点使用不同的数字格式。对于量化，我们使用 VS-Quant Dai 等人 ([2021](#bib.bib3))，因为它提供了在比例因子粒度上的灵活性。我们对外点使用相对较高的位宽，而对内点使用较低的位宽。我们还相应地量化激活，以便使用在面积和功耗方面更便宜的低位算术计算。我们在
    [图 7](#S5.F7 "Figure 7 ‣ 5 SDQ For LLMs") 中展示了 SDQ 框架的高级概览。
- en: 5.1 LLM inference performance
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 LLM 推理性能
- en: In [Figure 8](#S5.F8 "Figure 8 ‣ 5 SDQ For LLMs"), we show how SDQ unveils 4$\times$
    effective compute throughput. The achieved effective compute throughput would
    depend on the target structured sparsity and number format, so we explore different
    configurations in [section 6](#S6 "6 Evaluation").
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 8](#S5.F8 "Figure 8 ‣ 5 SDQ For LLMs") 中，我们展示了 SDQ 如何揭示 4$\times$ 的有效计算吞吐量。实现的有效计算吞吐量将依赖于目标结构稀疏性和数字格式，因此我们在
    [第 6 节](#S6 "6 Evaluation") 中探讨了不同的配置。
- en: 6 Evaluation
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 评估
- en: '|  | LLaMA-1 | LLaMA-2 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA-1 | LLaMA-2 |'
- en: '| Optimization Configuration | 7B | 13B | 30B | 7B | 13B |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 优化配置 | 7B | 13B | 30B | 7B | 13B |'
- en: '| 1$\times$ Effective Compute Throughput |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 1$\times$ 有效计算吞吐量 |'
- en: '| Baseline | 5.68 | 5.09 | 4.10 | 5.12 | 4.57 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 5.68 | 5.09 | 4.10 | 5.12 | 4.57 |'
- en: '| S-RTN-W4 | 6.43 | 5.55 | 4.57 | - | - |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| S-RTN-W4 | 6.43 | 5.55 | 4.57 | - | - |'
- en: '| S-GPTQ-W4 | 6.13 | 5.40 | 4.48 | - | - |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| S-GPTQ-W4 | 6.13 | 5.40 | 4.48 | - | - |'
- en: '| S-SpQR-W4 | 5.87 | 5.22 | 4.25 | - | - |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| S-SpQR-W4 | 5.87 | 5.22 | 4.25 | - | - |'
- en: '| 2$\times$ Effective Compute Throughput |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 2$\times$ 有效计算吞吐量 |'
- en: '| S-Wanda-4:8 | 8.57 | 7.41 | 5.97 | 8.07 | 6.55 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| S-Wanda-4:8 | 8.57 | 7.41 | 5.97 | 8.07 | 6.55 |'
- en: '| S-SparseGPT-4:8 | 8.61 | 7.42 | 6.15 | 7.91 | 6.58 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| S-SparseGPT-4:8 | 8.61 | 7.42 | 6.15 | 7.91 | 6.58 |'
- en: '| Q-VSQuant-WA-int8 | 5.70 | 5.11 | 4.13 | 5.14 | 4.60 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Q-VSQuant-WA-int8 | 5.70 | 5.11 | 4.13 | 5.14 | 4.60 |'
- en: '| Q-VSQuant-WA-fp8 | 5.70 | 5.11 | 4.12 | 5.14 | 4.59 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Q-VSQuant-WA-fp8 | 5.70 | 5.11 | 4.12 | 5.14 | 4.59 |'
- en: '| 3.6$\times$ Effective Compute Throughput |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 3.6$\times$ 有效计算吞吐量 |'
- en: '| SDQ-8:8-1:8int8-7:8fp4 | 5.81 | 5.20 | 4.22 | 5.23 | 4.67 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-8:8-1:8int8-7:8fp4 | 5.81 | 5.20 | 4.22 | 5.23 | 4.67 |'
- en: '| 4$\times$ Effective Compute Throughput |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 4$\times$ 有效计算吞吐量 |'
- en: '| S-Wanda-2:8 | 2960.99 | 2481.10 | 1228.86 | 1947.60 | 1188.45 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| S-Wanda-2:8 | 2960.99 | 2481.10 | 1228.86 | 1947.60 | 1188.45 |'
- en: '| S-SparseGPT-2:8 | 151.93 | 96.30 | 58.67 | 75.99 | 85.99 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| S-SparseGPT-2:8 | 151.93 | 96.30 | 58.67 | 75.99 | 85.99 |'
- en: '| Q-VSQuant-WA-int4 | 6.48 | 5.91 | 4.76 | 6.85 | 5.33 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Q-VSQuant-WA-int4 | 6.48 | 5.91 | 4.76 | 6.85 | 5.33 |'
- en: '| Q-VSQuant-WA-fp4 | 5.97 | 5.29 | 4.33 | 5.36 | 4.74 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Q-VSQuant-WA-fp4 | 5.97 | 5.29 | 4.33 | 5.36 | 4.74 |'
- en: '| SDQ-W3:4-1:4int8-2:4fp4 | 6.30 | 5.53 | 4.60 | 5.67 | 5.00 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-W3:4-1:4int8-2:4fp4 | 6.30 | 5.53 | 4.60 | 5.67 | 5.00 |'
- en: '| SDQ-S3:4-1:4int8-2:4fp4 | 6.30 | 5.54 | 4.67 | 5.65 | 4.98 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-S3:4-1:4int8-2:4fp4 | 6.30 | 5.54 | 4.67 | 5.65 | 4.98 |'
- en: '| SDQ-W6:8-2:8int8-4:8fp4 | 6.10 | 5.37 | 4.45 | 5.48 | 4.87 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-W6:8-2:8int8-4:8fp4 | 6.10 | 5.37 | 4.45 | 5.48 | 4.87 |'
- en: '| SDQ-S6:8-2:8int8-4:8fp4 | 6.10 | 5.36 | 4.50 | 5.48 | 4.86 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-S6:8-2:8int8-4:8fp4 | 6.10 | 5.36 | 4.50 | 5.48 | 4.86 |'
- en: '| SDQ-W7:8-1:8int8-6:8fp4 | 5.87 | 5.25 | 4.27 | 5.30 | 4.73 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-W7:8-1:8int8-6:8fp4 | 5.87 | 5.25 | 4.27 | 5.30 | 4.73 |'
- en: '| SDQ-S7:8-1:8int8-6:8fp4 | 5.88 | 5.26 | 4.31 | 5.32 | 4.73 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-S7:8-1:8int8-6:8fp4 | 5.88 | 5.26 | 4.31 | 5.32 | 4.73 |'
- en: 'Table 3: Perplexity results of LLaMA-1/LLaMA-2 on raw-WikiText2 with various
    sparsification, quantization, and SDQ configurations.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：LLaMA-1/LLaMA-2在原始-WikiText2上的困惑度结果，包含各种稀疏化、量化和SDQ配置。
- en: 6.1 Methodology
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 方法学
- en: In this section, we show how our hybrid method, SDQ, can be applied to LLMs
    to push the limit of effective compute throughput while achieving higher model
    quality than sparsification-only or quantization-only method. We use OPT Zhang
    et al. ([2022](#bib.bib39)), LLaMA Touvron et al. ([2023a](#bib.bib31)), LLaMA-2 Touvron
    et al. ([2023b](#bib.bib32)) with different number of parameters, from 125M to
    30B. We use the fp16 dense version of each model as the baseline model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了我们的混合方法SDQ如何应用于LLMs，以推动有效计算吞吐量的极限，同时实现比单纯稀疏化或量化方法更高的模型质量。我们使用了不同参数数量的OPT
    Zhang等人([2022](#bib.bib39))、LLaMA Touvron等人([2023a](#bib.bib31))、LLaMA-2 Touvron等人([2023b](#bib.bib32))，参数从125M到30B。我们使用每个模型的fp16稠密版本作为基线模型。
- en: First, we measure the perplexity change on the raw-Wikitext2 with OPT/LLaMA-1/LLaMA-2
    from 125M to 30B parameters using various SDQ configurations. Next, we provide
    zero-shot evaluation results of SDQ on OPT-6.7B, LLaMA-1-7B, and LLaMA-2-7B with
    BoolQ, HellaSwag, WinoGrande, ARC-e, ARC-c, and PIQA from LM-Eval Gao et al. ([2021](#bib.bib10)).
    For baseline sparsification-only and quantization-only methods, we use SparseGPT Frantar
    & Alistarh ([2023](#bib.bib7)), Wanda Sun et al. ([2023](#bib.bib30)), and VS-Quant Dai
    et al. ([2021](#bib.bib3)) with Q-Vector size of 16. Only for the weight-only
    quantization methods (S-RTN-W4, S-GPTQ-W4, S-SpQR-W4), we use the results reported
    in the publication Dettmers et al. ([2023](#bib.bib6)).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们测量了使用各种SDQ配置的OPT/LLaMA-1/LLaMA-2在原始-WikiText2上的困惑度变化，参数从125M到30B。接下来，我们提供了SDQ在OPT-6.7B、LLaMA-1-7B和LLaMA-2-7B上的零样本评估结果，使用BoolQ、HellaSwag、WinoGrande、ARC-e、ARC-c和PIQA，来源于LM-Eval
    Gao等人([2021](#bib.bib10))。对于基准稀疏化和量化方法，我们使用SparseGPT Frantar & Alistarh ([2023](#bib.bib7))、Wanda
    Sun等人([2023](#bib.bib30))和VS-Quant Dai等人([2021](#bib.bib3))，Q-Vector大小为16。仅对于仅权重量化方法（S-RTN-W4、S-GPTQ-W4、S-SpQR-W4），我们使用了Dettmers等人([2023](#bib.bib6))中报告的结果。
- en: We also compare different SDQ configurations. For example, SDQ-W7:8-1:8int8-6:8fp4
    means 1) using Wanda (we use S for SparseGPT) as sparsification with 7:8 structured
    sparsity 2) and using 1:8 local outlier extraction with int8 and 6:8 inlier with
    fp4. For decomposition metric, we find the product-based one Sun et al. ([2023](#bib.bib30))
    performs the best, so we use the metric. Starting with S- indicates sparsification-only
    and Q- indicates quantization-only (WA means dual quantization and W means weight-only).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还比较了不同的SDQ配置。例如，SDQ-W7:8-1:8int8-6:8fp4表示1）使用Wanda（我们用S表示SparseGPT）作为稀疏化，采用7:8的结构化稀疏性2）以及使用1:8局部异常值提取和6:8的fp4内点。对于分解度量，我们发现基于乘积的度量Sun等人([2023](#bib.bib30))表现最佳，因此我们使用该度量。以S-开头表示仅稀疏化，Q-表示仅量化（WA表示双重量化，W表示仅权重）。
- en: 6.2 Perplexity evaluation
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 困惑度评估
- en: We summarize the result of perplexity evaluation in [Table 2](#S5.T2 "Table
    2 ‣ 5 SDQ For LLMs") for various OPT models.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表2](#S5.T2 "Table 2 ‣ 5 SDQ For LLMs")中总结了各种OPT模型的困惑度评估结果。
- en: 'Comparison against sparsification. As mentioned in  [section 4](#S4 "4 SDQ:
    Tackling Challenges in LLM Compression with a Hybrid method"), sparsification-only
    methods are not effective both for $2\times$ throughput category, the model gets
    totally broken with sparsification-only methods due to the not enough number of
    elements that the model can keep.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '与稀疏化进行比较。如在[第4节](#S4 "4 SDQ: Tackling Challenges in LLM Compression with a
    Hybrid method")中提到的，单纯的稀疏化方法在$2\times$吞吐量类别中效果不佳，由于模型能够保持的元素数量不足，模型完全崩溃。'
- en: Comparison against weight-only quantization. We observe that SDQ can achieve
    better perplexity than the best weight-only 4b quantization (fp4-e2m1) while enabling
    4$\times$ effective compute throughput. For example, OPT-30B with SDQ-W6:8-2:8o8b-4:8i4b
    results in the perplexity of 9.51 which is better than that of S-SpQR-W4 (9.54)
    or even Dense-WA16 baseline (9.56).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅权重量化的比较。我们观察到 SDQ 能够比最佳的仅权重 4b 量化（fp4-e2m1）获得更好的困惑度，同时实现 4$\times$ 的有效计算吞吐量。例如，使用
    SDQ-W6:8-2:8o8b-4:8i4b 的 OPT-30B 的困惑度为 9.51，优于 S-SpQR-W4（9.54）或甚至 Dense-WA16 基线（9.56）。
- en: '| OPT-6.7B |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7B |'
- en: '| Method | BoolQ | HellaSwag | WinoGrande | ARC-easy | ARC-challenge | PIQA
    | Average |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | BoolQ | HellaSwag | WinoGrande | ARC-easy | ARC-challenge | PIQA | 平均
    |'
- en: '| Baseline | 66.15 | 50.53 | 65.19 | 65.61 | 30.55 | 76.22 | 59.04 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Baseline | 66.15 | 50.53 | 65.19 | 65.61 | 30.55 | 76.22 | 59.04 |'
- en: '| S-SparseGPT-2:8 | 58.50 | 27.63 | 51.22 | 32.11 | 19.45 | 56.20 | 40.85 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| S-SparseGPT-2:8 | 58.50 | 27.63 | 51.22 | 32.11 | 19.45 | 56.20 | 40.85 |'
- en: '| S-Wanda-2:8 | 50.03 | 26.22 | 50.36 | 31.02 | 18.69 | 55.33 | 38.61 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| S-Wanda-2:8 | 50.03 | 26.22 | 50.36 | 31.02 | 18.69 | 55.33 | 38.61 |'
- en: '| Q-VSQuant-WA-int4 | 61.38 | 47.09 | 60.85 | 59.64 | 26.88 | 71.27 | 54.52
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Q-VSQuant-WA-int4 | 61.38 | 47.09 | 60.85 | 59.64 | 26.88 | 71.27 | 54.52
    |'
- en: '| Q-VSQuant-WA-fp4 | 62.94 | 48.50 | 61.80 | 65.15 | 29.18 | 74.70 | 57.05
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Q-VSQuant-WA-fp4 | 62.94 | 48.50 | 61.80 | 65.15 | 29.18 | 74.70 | 57.05
    |'
- en: '| SDQ-7:8-1:8int8-6:8fp4 | 66.33 | 49.91 | 63.54 | 65.23 | 30.20 | 74.97 |
    58.36 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-7:8-1:8int8-6:8fp4 | 66.33 | 49.91 | 63.54 | 65.23 | 30.20 | 74.97 |
    58.36 |'
- en: '| LLaMA-1-7B |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-1-7B |'
- en: '| Baseline | 75.11 | 56.95 | 69.85 | 75.29 | 41.89 | 78.67 | 66.29 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Baseline | 75.11 | 56.95 | 69.85 | 75.29 | 41.89 | 78.67 | 66.29 |'
- en: '| S-SparseGPT-2:8 | 45.02 | 27.93 | 48.54 | 31.02 | 18.34 | 54.79 | 37.61 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| S-SparseGPT-2:8 | 45.02 | 27.93 | 48.54 | 31.02 | 18.34 | 54.79 | 37.61 |'
- en: '| S-Wanda-2:8 | 37.83 | 26.35 | 50.04 | 26.52 | 20.65 | 53.43 | 35.80 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| S-Wanda-2:8 | 37.83 | 26.35 | 50.04 | 26.52 | 20.65 | 53.43 | 35.80 |'
- en: '| Q-VSQuant-WA-int4 | 73.91 | 54.42 | 67.88 | 72.56 | 37.88 | 77.15 | 63.97
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Q-VSQuant-WA-int4 | 73.91 | 54.42 | 67.88 | 72.56 | 37.88 | 77.15 | 63.97
    |'
- en: '| Q-VSQuant-WA-fp4 | 74.86 | 56.06 | 69.38 | 74.54 | 40.87 | 77.26 | 65.49
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Q-VSQuant-WA-fp4 | 74.86 | 56.06 | 69.38 | 74.54 | 40.87 | 77.26 | 65.49
    |'
- en: '| SDQ-7:8-1:8int8-6:8fp4 | 75.35 | 56.24 | 69.30 | 74.70 | 41.38 | 78.29 |
    65.88 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-7:8-1:8int8-6:8fp4 | 75.35 | 56.24 | 69.30 | 74.70 | 41.38 | 78.29 |
    65.88 |'
- en: '| LLaMA-2-7B |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B |'
- en: '| Baseline | 77.74 | 57.13 | 69.06 | 76.30 | 43.43 | 78.07 | 66.96 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Baseline | 77.74 | 57.13 | 69.06 | 76.30 | 43.43 | 78.07 | 66.96 |'
- en: '| S-SparseGPT-2:8 | 48.10 | 27.92 | 45.38 | 28.62 | 17.58 | 54.90 | 37.08 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| S-SparseGPT-2:8 | 48.10 | 27.92 | 45.38 | 28.62 | 17.58 | 54.90 | 37.08 |'
- en: '| S-Wanda-2:8 | 37.83 | 26.15 | 50.91 | 26.94 | 19.62 | 52.82 | 35.71 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| S-Wanda-2:8 | 37.83 | 26.15 | 50.91 | 26.94 | 19.62 | 52.82 | 35.71 |'
- en: '| Q-VSQuant-WA-int4 | 74.62 | 53.51 | 66.85 | 72.90 | 40.27 | 76.38 | 64.09
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Q-VSQuant-WA-int4 | 74.62 | 53.51 | 66.85 | 72.90 | 40.27 | 76.38 | 64.09
    |'
- en: '| Q-VSQuant-WA-fp4 | 75.81 | 55.99 | 67.96 | 75.17 | 41.38 | 77.26 | 65.60
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Q-VSQuant-WA-fp4 | 75.81 | 55.99 | 67.96 | 75.17 | 41.38 | 77.26 | 65.60
    |'
- en: '| SDQ-7:8-1:8int8-6:8fp4 | 78.44 | 56.43 | 67.09 | 76.13 | 43.17 | 77.58 |
    66.47 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| SDQ-7:8-1:8int8-6:8fp4 | 78.44 | 56.43 | 67.09 | 76.13 | 43.17 | 77.58 |
    66.47 |'
- en: 'Table 4: Zero-shot evaluation on various tasks with OPT-6.7B, LLaMA-1-7B, and
    LLaMA-2-7B.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: OPT-6.7B、LLaMA-1-7B 和 LLaMA-2-7B 在各种任务上的零样本评估。'
- en: Comparison against dual quantization. Using dual quantization, one can achieve
    2$\times$ effective compute throughput with less than 1% model quality drop.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 与双量化的比较。使用双量化可以在模型质量下降不到 1% 的情况下，实现 2$\times$ 的有效计算吞吐量。
- en: We find the overall similar trend for LLaMA-1 and LLaMA-2 models as reported
    in [Table 3](#S6.T3 "Table 3 ‣ 6 Evaluation").
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现 LLaMA-1 和 LLaMA-2 模型的整体趋势与[表 3](#S6.T3 "Table 3 ‣ 6 Evaluation")中报告的类似。
- en: 6.3 Zero-shot evaluation
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 零样本评估
- en: To provide a more holistic evaluation, we conduct experiments on various Zero-shot
    tasks and show the results in  [Table 4](#S6.T4 "Table 4 ‣ 6.2 Perplexity evaluation
    ‣ 6 Evaluation"). These are known to provide more noisy results Dettmers et al.
    ([2022](#bib.bib5)), but still provide a big picture in terms of applicability.
    As the individual result for each task could be noisy, we compare the average
    accuracy of all the zero-shot tasks. To achieve 4$\times$ effective compute throughput,
    we observe the best sparsification-only and quantization-only methods show 25.58%
    and 1.37% accuracy drop, respectively, while SDQ only causes 0.53% accuracy drop,
    showing the similar trend that we observe in the perplexity evaluation. SDQ is
    the only option that meets the 1% criteria.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供更全面的评估，我们在各种零样本任务上进行实验，并在 [表 4](#S6.T4 "Table 4 ‣ 6.2 Perplexity evaluation
    ‣ 6 Evaluation") 中展示结果。这些任务已知会提供更嘈杂的结果Dettmers等人（[2022](#bib.bib5)），但仍能提供应用方面的全貌。由于每个任务的单独结果可能存在噪声，我们比较所有零样本任务的平均准确率。为了实现4$\times$有效计算吞吐量，我们观察到最佳的仅稀疏化和仅量化方法分别显示出25.58%和1.37%的准确率下降，而SDQ仅导致0.53%的准确率下降，显示出在困惑度评估中观察到的相似趋势。SDQ是唯一满足1%标准的选项。
- en: '![Refer to caption](img/3fd0409ffa661abf009947125e0c159a.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3fd0409ffa661abf009947125e0c159a.png)'
- en: 'Figure 9: Sensitivity study with different sparsification methods'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：不同稀疏化方法的灵敏度研究
- en: 6.4 Sensitivity studies
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 灵敏度研究
- en: SDQ is composed of the three stages and the performance of the effectiveness
    of SDQ is also affected by the effectiveness of each stage. We use OPT-6.7B for
    this study.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: SDQ由三个阶段组成，SDQ的效果也受到每个阶段效果的影响。我们使用OPT-6.7B进行这项研究。
- en: Sparsification stage. In [Figure 9](#S6.F9 "Figure 9 ‣ 6.3 Zero-shot evaluation
    ‣ 6 Evaluation"), we show how different sparsity patterns and sparsification method
    affects to the effectiveness of SDQ. First, we observe that Wanda performs better
    than SparseGPT for 7:8 and 6:8 (even exceeding the baseline) while SparseGPT performs
    better than Wanda for 5:8 and 4:8. Next, we apply SDQ with SparseGPT and Wanda
    for the sparsification stage with $N$ (as we use the same decomposition and quantization
    configuration for all SDQ options in this experiment, we just call them as SDQ-S
    and SDQ-W). We observe the same trend for SDQ-S and SDQ-W, similar to the sparsification-only
    methods; for 7:8/6:8, SDQ-W exhibits lower (better) perplexity than SDQ-S while
    SDQ-S exhibits lower (better) perplexity than SDQ-W for 5:8/4:8.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏化阶段。在 [图 9](#S6.F9 "Figure 9 ‣ 6.3 Zero-shot evaluation ‣ 6 Evaluation") 中，我们展示了不同稀疏模式和稀疏化方法对SDQ效果的影响。首先，我们观察到Wanda在7:8和6:8的表现优于SparseGPT（甚至超过基线），而SparseGPT在5:8和4:8的表现优于Wanda。接下来，我们将SDQ与SparseGPT和Wanda应用于稀疏化阶段，$N$（由于在此实验中我们对所有SDQ选项使用相同的分解和量化配置，因此我们将它们称为SDQ-S和SDQ-W）。我们观察到SDQ-S和SDQ-W的趋势与仅稀疏化方法相似；对于7:8/6:8，SDQ-W表现出比SDQ-S更低（更好）的困惑度，而对于5:8/4:8，SDQ-S表现出比SDQ-W更低（更好）的困惑度。
- en: '![Refer to caption](img/8820fc28f5407f7adec60f685709db2a.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8820fc28f5407f7adec60f685709db2a.png)'
- en: 'Figure 10: Sensitivity study with different decomposition metrics.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：不同分解指标的灵敏度研究。
- en: Decomposition stage. In [Figure 10](#S6.F10 "Figure 10 ‣ 6.4 Sensitivity studies
    ‣ 6 Evaluation"), we show how different decomposition method affects to the effectiveness
    of SDQ. We use SDQ-W7:8-1:8int8-6:8fp4, but with different criteria in the decomposition
    stage. In the decomposition stage, a metric is required for *N:M* local outlier
    extraction to identify the outliers. The simplest one is using the magnitude Guo
    et al. ([2023](#bib.bib11)), but more sophisticated metrics could also be used
    such as weight-activation-product-based used in Wanda Sun et al. ([2023](#bib.bib30))
    or error-based similar to the one used in SpQR Dettmers et al. ([2023](#bib.bib6)).
    Also, we can determine an element as an outlier based on the ascending or descending
    order of the metric. We mark “Large” in the plot if we select outliers in descending
    order, and “Small” if we select in ascending order. We observe that product-based
    outlier extraction performs the best and the perplexity could fluctuate up to
    7% depending on the extracted outliers, implying that adopting an effective method
    to extract outliers is critical for the effectiveness of SDQ. We leave exploring
    the best method to locate outliers for future work.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 分解阶段。在[图 10](#S6.F10 "Figure 10 ‣ 6.4 Sensitivity studies ‣ 6 Evaluation")中，我们展示了不同分解方法如何影响SDQ的有效性。我们使用SDQ-W7:8-1:8int8-6:8fp4，但在分解阶段使用了不同的标准。在分解阶段，需要一个度量来进行*N:M*局部异常值提取以识别异常值。最简单的度量是使用Guo等人（[2023](#bib.bib11)）提出的幅度，但也可以使用更复杂的度量，比如Wanda
    Sun等人（[2023](#bib.bib30)）使用的基于权重激活乘积的度量或类似于SpQR Dettmers等人（[2023](#bib.bib6)）使用的基于误差的度量。此外，我们还可以根据度量的升序或降序确定一个元素是否为异常值。如果我们按降序选择异常值，我们在图中标记为“Large”；如果我们按升序选择异常值，则标记为“Small”。我们观察到基于乘积的异常值提取效果最佳，困惑度可能因提取的异常值而波动高达7%，这意味着采用有效的异常值提取方法对于SDQ的有效性至关重要。我们将最佳异常值定位方法的探索留待未来工作。
- en: '![Refer to caption](img/e496e36a368d099d29f80124c6d73fa1.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e496e36a368d099d29f80124c6d73fa1.png)'
- en: 'Figure 11: Sensitivity study with different scale factor formats.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：不同尺度因子格式的敏感性研究。
- en: 'Quantization stage. VS-Quant can use different number formats for scale factor
    per Q-Vector. In [Figure 11](#S6.F11 "Figure 11 ‣ 6.4 Sensitivity studies ‣ 6
    Evaluation"), we show how different quantization configurations, such as the scale
    factor format, affect the effectiveness of SDQ. We compare two formats: ufp8-e6m2
    (unsigned float using 6 bits for exponents and 2 bits for mantissa) and fp8-e4m3
    (signed float using 4 bits for exponents and 3 bits for mantissa and 1 bit as
    the sign bit). We observe that dual quantization using fp4 with ufp8-e6m2 scale
    factors performs much worse than the case with fp8-e4m3. Similarly, dual quantization
    with int8 also prefers fp8-e4m3 scale factors than ufp8-e6m2. We also observe
    that SDQ-W7:8-1:8int8-6:8fp4 prefers fp8-e4m3, showing that improving quantization
    could also improve the quality of SDQ.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 量化阶段。VS-Quant可以为每个Q-Vector使用不同的数字格式作为尺度因子。在[图 11](#S6.F11 "Figure 11 ‣ 6.4 Sensitivity
    studies ‣ 6 Evaluation")中，我们展示了不同量化配置（例如尺度因子格式）如何影响SDQ的有效性。我们比较了两种格式：ufp8-e6m2（无符号浮点数，使用6位表示指数和2位表示尾数）和fp8-e4m3（有符号浮点数，使用4位表示指数、3位表示尾数和1位表示符号位）。我们观察到，使用fp4和ufp8-e6m2尺度因子的双重量化表现远逊于使用fp8-e4m3的情况。同样，使用int8的双重量化也更倾向于fp8-e4m3而非ufp8-e6m2。我们还观察到SDQ-W7:8-1:8int8-6:8fp4更偏好fp8-e4m3，显示出改进量化也能提高SDQ的质量。
- en: 7 Related Work
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 相关工作
- en: Both SparseGPT Frantar & Alistarh ([2023](#bib.bib7)) and Wanda Sun et al. ([2023](#bib.bib30))
    propose sparsification methods for LLMs, but they both fail to maintain the quality
    of the model even with 50% *N:M* sparsity (such as 2:4 or 4:8). Unlike sparsification,
    there have been many attempts to quantize LLMs Frantar et al. ([2023](#bib.bib9));
    Lin et al. ([2023](#bib.bib20)); Dettmers et al. ([2023](#bib.bib6); [2022](#bib.bib5));
    Xiao et al. ([2023](#bib.bib37)); Kim et al. ([2023](#bib.bib16)). We use VS-Quant Dai
    et al. ([2021](#bib.bib3)) in the current SDQ, but our work is not limited to
    the quantization method as shown in [subsection 6.4](#S6.SS4 "6.4 Sensitivity
    studies ‣ 6 Evaluation"). We believe SDQ can take the benefits of the improvement
    in quantization methods.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: SparseGPT Frantar & Alistarh（[2023](#bib.bib7)）和Wanda Sun等人（[2023](#bib.bib30)）都提出了LLMs的稀疏化方法，但即使在50%*N:M*稀疏性（如2:4或4:8）下，它们都未能维持模型的质量。与稀疏化不同，已经有许多尝试量化LLMs的研究，如Frantar等人（[2023](#bib.bib9)）；Lin等人（[2023](#bib.bib20)）；Dettmers等人（[2023](#bib.bib6)；[2022](#bib.bib5)）；Xiao等人（[2023](#bib.bib37)）；Kim等人（[2023](#bib.bib16)）。我们在当前的SDQ中使用了VS-Quant
    Dai等人（[2021](#bib.bib3)），但我们的工作不限于量化方法，如在[小节 6.4](#S6.SS4 "6.4 Sensitivity studies
    ‣ 6 Evaluation")中所示。我们相信SDQ可以受益于量化方法的改进。
- en: OBC Frantar et al. ([2022](#bib.bib8)) provides a framework for quantization
    and sparsification, but they do not target LLMs. Also, they do not consider running
    LLMs efficiently through *N:M* structured sparse HW and low-bit computations.
    Recent work Kuzmin et al. ([2023](#bib.bib17)) observe that quantization generally
    performs better than sparsification on Vision models, which is consistent with
    our observation for LLMs.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: OBC Frantar 等人（[2022](#bib.bib8)）提供了一个量化和稀疏化的框架，但他们的研究并未针对 LLMs。此外，他们也没有考虑通过
    *N:M* 结构稀疏硬件和低位计算高效运行 LLMs。最近的工作 Kuzmin 等人（[2023](#bib.bib17)）观察到在视觉模型中，量化通常比稀疏化表现更好，这与我们对
    LLMs 的观察一致。
- en: 8 Conclusion
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: We propose a new method, Sparse Decomposed Quantization (SDQ) using mixed precision
    for outliers and inliers through structured decomposition, which enables utilizing
    mixed precision *N:M* structured sparse HW. We explore the SDQ across different
    models and sizes to understand the potential of the technique with various LLMs.
    We show that SDQ opens up an opportunity to achieve 4$\times$ quality drop). We
    plan to extend this to validate with sparse tensor accelerator simulators, such
    as Sparseloop Wu et al. ([2022](#bib.bib35)).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种新方法，稀疏分解量化（SDQ），通过结构化分解对离群点和内点使用混合精度，这使得能够利用混合精度 *N:M* 结构稀疏硬件。我们探索了不同模型和规模下的
    SDQ，以了解该技术在各种 LLMs 中的潜力。我们展示了 SDQ 提供了一个实现 4$\times$ 质量下降的机会。我们计划将其扩展到使用稀疏张量加速器模拟器进行验证，例如
    Sparseloop Wu 等人（[2022](#bib.bib35)）。
- en: References
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Brown et al. (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
    J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal,
    S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
    D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
    S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
    I., and Amodei, D. Language models are few-shot learners, 2020.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal,
    P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J.,
    Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B.,
    Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., 和 Amodei, D.
    语言模型是少样本学习者，2020。
- en: 'Chowdhery et al. (2022) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P.,
    Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
    N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J.,
    Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat,
    S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L.,
    Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi,
    R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,
    Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X.,
    Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck,
    D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with
    pathways, 2022.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等人（2022）Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P.,
    Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
    N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J.,
    Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat,
    S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L.,
    Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi,
    R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,
    Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X.,
    Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck,
    D., Dean, J., Petrov, S., 和 Fiedel, N. Palm: 扩展语言建模的路径，2022。'
- en: 'Dai et al. (2021) Dai, S., Venkatesan, R., Ren, M., Zimmer, B., Dally, W.,
    and Khailany, B. Vs-quant: Per-vector scaled quantization for accurate low-precision
    neural network inference. *Proceedings of Machine Learning and Systems*, 3:873–884,
    2021.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等人（2021）Dai, S., Venkatesan, R., Ren, M., Zimmer, B., Dally, W., 和 Khailany,
    B. Vs-quant: 每向量缩放量化以实现精确的低精度神经网络推理。*机器学习与系统会议论文集*，3:873–884, 2021。'
- en: 'Darvish Rouhani et al. (2023) Darvish Rouhani, B., Zhao, R., Elango, V., Shafipour,
    R., Hall, M., Mesmakhosroshahi, M., More, A., Melnick, L., Golub, M., Varatkar,
    G., Shao, L., Kolhe, G., Melts, D., Klar, J., L’Heureux, R., Perry, M., Burger,
    D., Chung, E., Deng, Z. S., Naghshineh, S., Park, J., and Naumov, M. With shared
    microexponents, a little shifting goes a long way. In *Proceedings of the 50th
    Annual International Symposium on Computer Architecture*, ISCA ’23, New York,
    NY, USA, 2023\. Association for Computing Machinery. ISBN 9798400700958. doi:
    10.1145/3579371.3589351. URL [https://doi.org/10.1145/3579371.3589351](https://doi.org/10.1145/3579371.3589351).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Darvish Rouhani 等（2023）Darvish Rouhani, B., Zhao, R., Elango, V., Shafipour,
    R., Hall, M., Mesmakhosroshahi, M., More, A., Melnick, L., Golub, M., Varatkar,
    G., Shao, L., Kolhe, G., Melts, D., Klar, J., L’Heureux, R., Perry, M., Burger,
    D., Chung, E., Deng, Z. S., Naghshineh, S., Park, J., 和 Naumov, M. 通过共享的微幂，少量的移动可以带来很大变化。收录于*第50届国际计算机体系结构年会论文集*，ISCA
    ’23，纽约，美国，2023年。计算机协会。ISBN 9798400700958。doi: 10.1145/3579371.3589351。网址 [https://doi.org/10.1145/3579371.3589351](https://doi.org/10.1145/3579371.3589351)。'
- en: 'Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,
    L. Llm.int8(): 8-bit matrix multiplication for transformers at scale. In Koyejo,
    S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), *Advances
    in Neural Information Processing Systems*, volume 35, pp.  30318–30332\. Curran
    Associates, Inc., 2022.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2022）Dettmers, T., Lewis, M., Belkada, Y., 和 Zettlemoyer, L. Llm.int8():
    大规模转换器的8位矩阵乘法。收录于 Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K.,
    和 Oh, A.（编），*神经信息处理系统进展*，第35卷，第30318–30332页。Curran Associates, Inc.，2022。'
- en: 'Dettmers et al. (2023) Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev,
    D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. Spqr:
    A sparse-quantized representation for near-lossless llm weight compression, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2023）Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev,
    D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., 和 Alistarh, D. Spqr:
    一种用于近乎无损的 LLM 权重压缩的稀疏量化表示，2023。'
- en: 'Frantar & Alistarh (2023) Frantar, E. and Alistarh, D. SparseGPT: Massive language
    models can be accurately pruned in one-shot. *arXiv preprint arXiv:2301.00774*,
    2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar & Alistarh（2023）Frantar, E. 和 Alistarh, D. SparseGPT: 大型语言模型可以通过单次操作准确剪枝。*arXiv
    预印本 arXiv:2301.00774*，2023。'
- en: 'Frantar et al. (2022) Frantar, E., Singh, S. P., and Alistarh, D. Optimal Brain
    Compression: a framework for accurate post-training quantization and pruning.
    *Advances in Neural Information Processing Systems*, 36, 2022.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等（2022）Frantar, E., Singh, S. P., 和 Alistarh, D. Optimal Brain Compression:
    精确的训练后量化和剪枝框架。*神经信息处理系统进展*，第36卷，2022。'
- en: 'Frantar et al. (2023) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. OPTQ: Accurate quantization for generative pre-trained transformers. In *The
    Eleventh International Conference on Learning Representations*, 2023. URL [https://openreview.net/forum?id=tcbBPnfwxS](https://openreview.net/forum?id=tcbBPnfwxS).'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等（2023）Frantar, E., Ashkboos, S., Hoefler, T., 和 Alistarh, D. OPTQ:
    用于生成预训练变换器的准确量化。收录于*第十一届国际学习表征会议*，2023。网址 [https://openreview.net/forum?id=tcbBPnfwxS](https://openreview.net/forum?id=tcbBPnfwxS)。'
- en: Gao et al. (2021) Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,
    C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot
    language model evaluation, September 2021. URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2021）Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C.,
    Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L.,
    Tang, E., Thite, A., Wang, B., Wang, K., 和 Zou, A. 少样本语言模型评估框架，2021年9月。网址 [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628)。
- en: 'Guo et al. (2023) Guo, C., Tang, J., Hu, W., Leng, J., Zhang, C., Yang, F.,
    Liu, Y., Guo, M., and Zhu, Y. Olive: Accelerating large language models via hardware-friendly
    outlier-victim pair quantization. In *Proceedings of the 50th Annual International
    Symposium on Computer Architecture*, ISCA ’23, New York, NY, USA, 2023\. Association
    for Computing Machinery. ISBN 9798400700958. doi: 10.1145/3579371.3589038. URL
    [https://doi.org/10.1145/3579371.3589038](https://doi.org/10.1145/3579371.3589038).'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等（2023）Guo, C., Tang, J., Hu, W., Leng, J., Zhang, C., Yang, F., Liu, Y.,
    Guo, M., 和 Zhu, Y. Olive: 通过硬件友好的离群体-受害者配对量化加速大型语言模型。收录于*第50届国际计算机体系结构年会论文集*，ISCA
    ’23，纽约，美国，2023年。计算机协会。ISBN 9798400700958。doi: 10.1145/3579371.3589038。网址 [https://doi.org/10.1145/3579371.3589038](https://doi.org/10.1145/3579371.3589038)。'
- en: Han et al. (2015) Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights
    and connections for efficient neural network. In Cortes, C., Lawrence, N., Lee,
    D., Sugiyama, M., and Garnett, R. (eds.), *Advances in Neural Information Processing
    Systems*, volume 28\. Curran Associates, Inc., 2015. URL [https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf).
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2015) Han, S., Pool, J., Tran, J., 和 Dally, W. 为高效神经网络学习权重和连接。见 Cortes,
    C., Lawrence, N., Lee, D., Sugiyama, M., 和 Garnett, R. (编)，*神经信息处理系统进展*，第28卷。Curran
    Associates, Inc., 2015年。URL [https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf)。
- en: 'Hoefler et al. (2021) Hoefler, T., Alistarh, D., Ben-Nun, T., Dryden, N., and
    Peste, A. Sparsity in deep learning: Pruning and growth for efficient inference
    and training in neural networks. *J. Mach. Learn. Res.*, 22(1), jan 2021. ISSN
    1532-4435.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoefler 等 (2021) Hoefler, T., Alistarh, D., Ben-Nun, T., Dryden, N., 和 Peste,
    A. 深度学习中的稀疏性：神经网络中高效推理和训练的修剪与增长。*J. Mach. Learn. Res.*, 22(1), 2021年1月。ISSN 1532-4435。
- en: 'Horowitz (2014) Horowitz, M. 1.1 computing’s energy problem (and what we can
    do about it). In *2014 IEEE International Solid-State Circuits Conference Digest
    of Technical Papers (ISSCC)*, pp.  10–14, 2014. doi: 10.1109/ISSCC.2014.6757323.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Horowitz (2014) Horowitz, M. 1.1 计算的能源问题（以及我们可以做的事情）。见 *2014 IEEE 国际固态电路会议技术论文摘要
    (ISSCC)*，第10–14页，2014年。doi: 10.1109/ISSCC.2014.6757323。'
- en: 'Jeong et al. (2023) Jeong, G., Damani, S., Bambhaniya, A. R., Qin, E., Hughes,
    C. J., Subramoney, S., Kim, H., and Krishna, T. Vegeta: Vertically-integrated
    extensions for sparse/dense gemm tile acceleration on cpus. In *2023 IEEE International
    Symposium on High Performance Computer Architecture (HPCA)*, 2023.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jeong 等 (2023) Jeong, G., Damani, S., Bambhaniya, A. R., Qin, E., Hughes, C.
    J., Subramoney, S., Kim, H., 和 Krishna, T. Vegeta: 用于CPU上稀疏/密集 GEMM 瓦片加速的垂直集成扩展。见
    *2023 IEEE 高性能计算体系结构国际研讨会 (HPCA)*，2023年。'
- en: 'Kim et al. (2023) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen,
    S., Mahoney, M., and Keutzer, K. Squeezellm: Dense-and-sparse quantization. *arXiv*,
    2023.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等 (2023) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S.,
    Mahoney, M., 和 Keutzer, K. Squeezellm: 密集与稀疏量化。*arXiv*，2023年。'
- en: 'Kuzmin et al. (2023) Kuzmin, A., Nagel, M., van Baalen, M., Behboodi, A., and
    Blankevoort, T. Pruning vs quantization: Which is better?, 2023.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuzmin 等 (2023) Kuzmin, A., Nagel, M., van Baalen, M., Behboodi, A., 和 Blankevoort,
    T. 剪枝与量化：哪个更好？，2023年。
- en: LeCun et al. (1989) LeCun, Y., Denker, J., and Solla, S. Optimal brain damage.
    In Touretzky, D. (ed.), *Advances in Neural Information Processing Systems*, volume 2\.
    Morgan-Kaufmann, 1989. URL [https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf).
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等 (1989) LeCun, Y., Denker, J., 和 Solla, S. 最优大脑损伤。见 Touretzky, D. (编)，*神经信息处理系统进展*，第2卷。Morgan-Kaufmann,
    1989年。URL [https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf)。
- en: 'Lee et al. (2023) Lee, C., Jin, J., Kim, T., Kim, H., and Park, E. Owq: Lessons
    learned from activation outliers for weight quantization in large language models.
    *arXiv preprint arXiv:2306.02272*, 2023.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee 等 (2023) Lee, C., Jin, J., Kim, T., Kim, H., 和 Park, E. Owq: 从激活异常中学到的关于大语言模型的权重量化的经验。*arXiv
    预印本 arXiv:2306.02272*，2023年。'
- en: 'Lin et al. (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., Gan, C.,
    and Han, S. Awq: Activation-aware weight quantization for llm compression and
    acceleration, 2023.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等 (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., Gan, C., 和 Han,
    S. Awq: 激活感知权重量化用于大语言模型压缩和加速，2023年。'
- en: 'Liu et al. (2021) Liu, Z.-G., Whatmough, P. N., Zhu, Y., and Mattina, M. S2ta:
    Exploiting structured sparsity for energy-efficient mobile cnn acceleration. *arXiv
    preprint arXiv:2107.07983*, 2021.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2021) Liu, Z.-G., Whatmough, P. N., Zhu, Y., 和 Mattina, M. S2ta: 利用结构化稀疏性实现节能移动CNN加速。*arXiv
    预印本 arXiv:2107.07983*，2021年。'
- en: Micikevicius (2021) Micikevicius, P. Sparsity, structure, and performance, 2021.
    Sparsity in Neural Networks (SNN) Workshop.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micikevicius (2021) Micikevicius, P. 稀疏性、结构与性能，2021年。神经网络稀疏性 (SNN) 研讨会。
- en: Mishra et al. (2021) Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic,
    D., Venkatesh, G., Yu, C., and Micikevicius, P. Accelerating sparse deep neural
    networks. *arXiv preprint arXiv:2104.08378*, 2021.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra 等 (2021) Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic, D.,
    Venkatesh, G., Yu, C., 和 Micikevicius, P. 加速稀疏深度神经网络。*arXiv 预印本 arXiv:2104.08378*，2021年。
- en: 'Molchanov et al. (2022) Molchanov, P., Hall, J., Yin, H., Kautz, J., Fusi,
    N., and Vahdat, A. Lana: latency aware network acceleration. In *European Conference
    on Computer Vision*, pp.  137–156\. Springer, 2022.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molchanov 等人（2022）Molchanov, P., Hall, J., Yin, H., Kautz, J., Fusi, N., 和 Vahdat,
    A. Lana：延迟感知网络加速。载于*欧洲计算机视觉会议*，第 137–156 页，Springer，2022年。
- en: NVIDIA (2020) NVIDIA. Nvidia ampere ga102 gpu architecture, 2020. [https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.1.pdf](https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.1.pdf).
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA（2020）NVIDIA. Nvidia Ampere GA102 GPU 架构，2020年。 [https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.1.pdf](https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.1.pdf)。
- en: NVIDIA (2022) NVIDIA. Nvidia h100 tensor core gpu architecture, 2022. [https://resources.nvidia.com/en-us-tensor-core](https://resources.nvidia.com/en-us-tensor-core).
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA（2022）NVIDIA. Nvidia H100 Tensor Core GPU 架构，2022年。 [https://resources.nvidia.com/en-us-tensor-core](https://resources.nvidia.com/en-us-tensor-core)。
- en: Pool et al. (2021) Pool, J., Sawarkar, A., and Rodge, J. Accelerating inference
    with sparsity using the nvidia ampere architecture and nvidia tensorrt, 2021.
    URL [https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/).
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pool 等人（2021）Pool, J., Sawarkar, A., 和 Rodge, J. 利用 NVIDIA Ampere 架构和 NVIDIA
    TensorRT 加速推理，2021年。URL [https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/)。
- en: 'Qin et al. (2020) Qin, E., Samajdar, A., Kwon, H., Nadella, V., Srinivasan,
    S., Das, D., Kaul, B., and Krishna, T. Sigma: A sparse and irregular gemm accelerator
    with flexible interconnects for dnn training. In *2020 IEEE International Symposium
    on High Performance Computer Architecture (HPCA)*, pp.  58–70, 2020. doi: 10.1109/HPCA47549.2020.00015.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin 等人（2020）Qin, E., Samajdar, A., Kwon, H., Nadella, V., Srinivasan, S., Das,
    D., Kaul, B., 和 Krishna, T. Sigma：一种稀疏不规则的 GEMM 加速器，具有灵活的互连，用于 DNN 训练。载于*2020
    IEEE 国际高性能计算机架构研讨会（HPCA）*，第 58–70 页，2020年。doi: 10.1109/HPCA47549.2020.00015。'
- en: 'Reddi et al. (2020) Reddi, V. J., Cheng, C., Kanter, D., Mattson, P., Schmuelling,
    G., Wu, C.-J., Anderson, B., Breughe, M., Charlebois, M., Chou, W., Chukka, R.,
    Coleman, C., Davis, S., Deng, P., Diamos, G., Duke, J., Fick, D., Gardner, J. S.,
    Hubara, I., Idgunji, S., Jablin, T. B., Jiao, J., John, T. S., Kanwar, P., Lee,
    D., Liao, J., Lokhmotov, A., Massa, F., Meng, P., Micikevicius, P., Osborne, C.,
    Pekhimenko, G., Rajan, A. T. R., Sequeira, D., Sirasao, A., Sun, F., Tang, H.,
    Thomson, M., Wei, F., Wu, E., Xu, L., Yamada, K., Yu, B., Yuan, G., Zhong, A.,
    Zhang, P., and Zhou, Y. Mlperf inference benchmark. In *Proceedings of the ACM/IEEE
    47th Annual International Symposium on Computer Architecture*, ISCA ’20, pp. 
    446–459\. IEEE Press, 2020. ISBN 9781728146614. doi: 10.1109/ISCA45697.2020.00045.
    URL [https://doi.org/10.1109/ISCA45697.2020.00045](https://doi.org/10.1109/ISCA45697.2020.00045).'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reddi 等人（2020）Reddi, V. J., Cheng, C., Kanter, D., Mattson, P., Schmuelling,
    G., Wu, C.-J., Anderson, B., Breughe, M., Charlebois, M., Chou, W., Chukka, R.,
    Coleman, C., Davis, S., Deng, P., Diamos, G., Duke, J., Fick, D., Gardner, J.
    S., Hubara, I., Idgunji, S., Jablin, T. B., Jiao, J., John, T. S., Kanwar, P.,
    Lee, D., Liao, J., Lokhmotov, A., Massa, F., Meng, P., Micikevicius, P., Osborne,
    C., Pekhimenko, G., Rajan, A. T. R., Sequeira, D., Sirasao, A., Sun, F., Tang,
    H., Thomson, M., Wei, F., Wu, E., Xu, L., Yamada, K., Yu, B., Yuan, G., Zhong,
    A., Zhang, P., 和 Zhou, Y. MLPerf 推理基准测试。载于*第47届ACM/IEEE国际计算机架构年会论文集*，ISCA ’20，第
    446–459 页，IEEE Press，2020年。ISBN 9781728146614。doi: 10.1109/ISCA45697.2020.00045。URL
    [https://doi.org/10.1109/ISCA45697.2020.00045](https://doi.org/10.1109/ISCA45697.2020.00045)。'
- en: Sun et al. (2023) Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and
    effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2023）Sun, M., Liu, Z., Bair, A., 和 Kolter, J. Z. 一种简单有效的大型语言模型剪枝方法。*arXiv
    预印本 arXiv:2306.11695*，2023年。
- en: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation
    language models, 2023a.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2023a）Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., 和 Lample, G. Llama：开放而高效的基础语言模型，2023a。
- en: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D.,
    Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J.,
    Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M.,
    Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
    Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez,
    A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned
    chat models, 2023b.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023b）Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A.,
    Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher,
    L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J.,
    Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.
    M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J.
    X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S.,
    Rodriguez, A., Stojnic, R., Edunov, S., 和 Scialom, T. Llama 2：开放基础和微调聊天模型，2023b。
- en: van Baalen et al. (2023) van Baalen, M., Kuzmin, A., Nair, S. S., Ren, Y., Mahurin,
    E., Patel, C., Subramanian, S., Lee, S., Nagel, M., Soriaga, J., and Blankevoort,
    T. Fp8 versus int8 for efficient deep learning inference, 2023.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Baalen 等（2023）van Baalen, M., Kuzmin, A., Nair, S. S., Ren, Y., Mahurin,
    E., Patel, C., Subramanian, S., Lee, S., Nagel, M., Soriaga, J., 和 Blankevoort,
    T. Fp8 与 int8 在高效深度学习推理中的对比，2023年。
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need.
    In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan,
    S., and Garnett, R. (eds.), *Advances in Neural Information Processing Systems*,
    volume 30\. Curran Associates, Inc., 2017. URL [https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017）Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
    Gomez, A. N., Kaiser, L. u., 和 Polosukhin, I. 注意力即全部所需。在 Guyon, I., Luxburg, U.
    V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., 和 Garnett, R.（编辑），*神经信息处理系统进展*，第30卷。Curran
    Associates, Inc.，2017年。URL [https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)。
- en: 'Wu et al. (2022) Wu, Y. N., Tsai, P.-A., Parashar, A., Sze, V., and Emer, J. S.
    Sparseloop: An analytical approach to sparse tensor accelerator modeling. In *2022
    55th IEEE/ACM International Symposium on Microarchitecture (MICRO)*, pp.  1377–1395,
    2022. doi: 10.1109/MICRO56248.2022.00096.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等（2022）Wu, Y. N., Tsai, P.-A., Parashar, A., Sze, V., 和 Emer, J. S. Sparseloop：稀疏张量加速器建模的分析方法。在
    *2022年第55届IEEE/ACM国际微架构研讨会（MICRO）*，第1377–1395页，2022年。doi: 10.1109/MICRO56248.2022.00096。'
- en: 'Wu et al. (2023) Wu, Y. N., Tsai, P.-A., Muralidharan, S., Parashar, A., Sze,
    V., and Emer, J. S. Highlight: Efficient and flexible dnn acceleration with hierarchical
    structured sparsity. In *2023 56th IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*, 2023.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2023）Wu, Y. N., Tsai, P.-A., Muralidharan, S., Parashar, A., Sze, V., 和
    Emer, J. S. 精华：具有层次结构稀疏性的高效且灵活的 DNN 加速。在 *2023年第56届IEEE/ACM国际微架构研讨会（MICRO）*，2023年。
- en: 'Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and
    Han, S. SmoothQuant: Accurate and efficient post-training quantization for large
    language models. In *Proceedings of the 40th International Conference on Machine
    Learning*, 2023.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等（2023）Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., 和 Han, S. SmoothQuant：大型语言模型的准确且高效的后训练量化。在
    *第40届国际机器学习会议论文集*，2023年。
- en: 'Yin et al. (2023) Yin, L., Wu, Y., Zhang, Z., Hsieh, C.-Y., Wang, Y., Jia,
    Y., Pechenizkiy, M., Liang, Y., Wang, Z., and Liu, S. Outlier weighed layerwise
    sparsity (owl): A missing secret sauce for pruning llms to high sparsity. *arXiv
    preprint arXiv:2310.05175*, 2023.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等（2023）Yin, L., Wu, Y., Zhang, Z., Hsieh, C.-Y., Wang, Y., Jia, Y., Pechenizkiy,
    M., Liang, Y., Wang, Z., 和 Liu, S. Outlier 加权层级稀疏（owl）：将 LLMs 剪枝到高稀疏性的缺失秘密配方。*arXiv
    预印本 arXiv:2310.05175*，2023年。
- en: 'Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer,
    S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,
    L. Opt: Open pre-trained transformer language models, 2022.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer,
    S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., 和 Zettlemoyer,
    L. OPT: 开放预训练变换器语言模型，2022年。'
- en: 'Zhu et al. (2019) Zhu, M., Zhang, T., Gu, Z., and Xie, Y. Sparse tensor core:
    Algorithm and hardware co-design for vector-wise sparse neural networks on modern
    gpus. In *Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture*,
    MICRO ’52, pp.  359–371, New York, NY, USA, 2019\. Association for Computing Machinery.
    ISBN 9781450369381.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2019) Zhu, M., Zhang, T., Gu, Z., 和 Xie, Y. 稀疏张量核心：现代 GPU 上面向向量的稀疏神经网络的算法与硬件共同设计。在
    *第52届年度IEEE/ACM国际微体系结构研讨会论文集*，MICRO ’52，第359–371页，美国纽约，2019年。计算机协会。ISBN 9781450369381。
