- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:48:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:48:17'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先进的异常管理与LLMs高效量化的旋转与置换
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.01721](https://ar5iv.labs.arxiv.org/html/2406.01721)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.01721](https://ar5iv.labs.arxiv.org/html/2406.01721)
- en: 'Haokun Lin¹¹1Equal contribution. ^(1,3,4), Haobo Xu¹¹footnotemark: 1 ², Yichen
    Wu¹¹footnotemark: 1 ⁴, Jingzhi Cui², Yingtao Zhang²,'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Haokun Lin¹¹1平等贡献。 ^(1,3,4), Haobo Xu¹¹脚注标记: 1 ², Yichen Wu¹¹脚注标记: 1 ⁴, Jingzhi
    Cui², Yingtao Zhang²,'
- en: 'Linzhan Mou⁵, Linqi Song⁴, Zhenan Sun²²2Corresponding authors. ^(1,3), Ying
    Wei²²footnotemark: 2 ⁶'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'Linzhan Mou⁵, Linqi Song⁴, Zhenan Sun²²2  负责人。 ^(1,3), Ying Wei²²脚注标记: 2 ⁶'
- en: ¹ School of Artificial Intelligence, University of Chinese Academy of Sciences
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 中国科学院大学人工智能学院
- en: ² Tsinghua University  ³ NLPR & MAIS, Institute of Automation, Chinese Academy
    of Sciences
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ² 清华大学  ³ 中国科学院自动化研究所NLPR & MAIS
- en: ⁴ City University of Hong Kong  ⁵ Zhejiang University  ⁶ Nanyang Technological
    University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 香港城市大学  ⁵ 浙江大学  ⁶ 南洋理工大学
- en: haokun.lin@cripac.ia.ac.cn xuhb20@mails.tsinghua.edu.cn
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: haokun.lin@cripac.ia.ac.cn xuhb20@mails.tsinghua.edu.cn
- en: wuyichen.am97@gmail.com znsun@nlpr.ia.ac.cn ying.wei@ntu.edu.sg
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: wuyichen.am97@gmail.com znsun@nlpr.ia.ac.cn ying.wei@ntu.edu.sg
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Quantizing large language models (LLMs) presents significant challenges, primarily
    due to outlier activations that compromise the efficiency of low-bit representation.
    Traditional approaches mainly focus on solving Normal Outliers—activations with
    consistently high magnitudes across all tokens. However, these techniques falter
    when dealing with Massive Outliers, which are significantly higher in value and
    often cause substantial performance losses during low-bit quantization. In this
    study, we propose DuQuant, an innovative quantization strategy employing rotation
    and permutation transformations to more effectively eliminate both types of outliers.
    Initially, DuQuant constructs rotation matrices informed by specific outlier dimensions,
    redistributing these outliers across adjacent channels within different rotation
    blocks. Subsequently, a zigzag permutation is applied to ensure a balanced distribution
    of outliers among blocks, minimizing block-wise variance. An additional rotation
    further enhances the smoothness of the activation landscape, thereby improving
    model performance. DuQuant streamlines the quantization process and demonstrates
    superior outlier management, achieving top-tier results in multiple tasks with
    various LLM architectures even under 4-bit weight-activation quantization. Our
    code is available at [https://github.com/Hsu1023/DuQuant](https://github.com/Hsu1023/DuQuant).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 量化大型语言模型（LLMs）面临重大挑战，主要由于异常激活影响低位表示的效率。传统方法主要关注解决正常异常——所有标记中幅度始终较高的激活。然而，这些技术在处理大规模异常时会遇到困难，这些异常的值显著更高，常常导致低位量化过程中性能的显著损失。在本研究中，我们提出了DuQuant，一种创新的量化策略，通过旋转和置换变换更有效地消除这两种异常。最初，DuQuant构建由特定异常维度提供信息的旋转矩阵，将这些异常重新分配到不同旋转块的相邻通道中。随后，应用了锯齿形置换以确保异常在块之间的均衡分布，最小化块间方差。进一步的旋转提升了激活景观的平滑度，从而改善模型性能。DuQuant简化了量化过程，并展示了卓越的异常管理，尽管在4位权重-激活量化下，也在多个任务中对各种LLM架构取得了顶级结果。我们的代码可在[https://github.com/Hsu1023/DuQuant](https://github.com/Hsu1023/DuQuant)获得。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) [[45](#bib.bib45), [7](#bib.bib7), [57](#bib.bib57),
    [18](#bib.bib18)] have demonstrated exceptional performance across a wide range
    of natural language processing tasks. However, their billions of parameters present
    considerable deployment challenges on resource-constrained edge devices, particularly
    in terms of memory usage and inference speed [[21](#bib.bib21), [16](#bib.bib16)].
    In response to these challenges, network quantization methods [[19](#bib.bib19),
    [23](#bib.bib23)] have been extensively explored to minimize memory usage by converting
    floating-point parameters into low-bit formats [[17](#bib.bib17), [30](#bib.bib30),
    [8](#bib.bib8)], and to expedite inference by quantizing both activations and
    weights for accelerating the matrix multiplication process [[52](#bib.bib52),
    [31](#bib.bib31), [61](#bib.bib61)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）[[45](#bib.bib45), [7](#bib.bib7), [57](#bib.bib57), [18](#bib.bib18)]在各种自然语言处理任务中表现出色。然而，其数十亿的参数在资源受限的边缘设备上带来了相当大的部署挑战，尤其是在内存使用和推理速度方面[[21](#bib.bib21),
    [16](#bib.bib16)]。针对这些挑战，网络量化方法[[19](#bib.bib19), [23](#bib.bib23)]被广泛探讨，以通过将浮点参数转换为低位格式来最小化内存使用[[17](#bib.bib17),
    [30](#bib.bib30), [8](#bib.bib8)]，并通过量化激活和权重来加速矩阵乘法过程，从而加快推理速度[[52](#bib.bib52),
    [31](#bib.bib31), [61](#bib.bib61)]。
- en: Among LLM quantization methods, a primary issue is the presence of activation
    outliers, which enlarge the quantization step sizes and subsequently cause significant
    accuracy loss [[49](#bib.bib49)]. To mitigate this problem, current research has
    developed various methods to address Normal Outliers in activations, which are
    persistent in several channels across all tokens [[12](#bib.bib12), [52](#bib.bib52)].
    However, besides Normal Outliers, there exists another type of activation outlier [[43](#bib.bib43),
    [32](#bib.bib32)], termed Massive Outliers. These outliers are characterized by
    their exceedingly high values and limited occurrence in a subset of tokens, as
    depicted in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs")(b). Unfortunately,
    existing LLM quantization methods struggle to effectively address these Massive
    Outliers. For instance, SmoothQuant [[52](#bib.bib52)], despite using a smooth
    factor to shift some of the activation outliers to the weight part, still cannot
    effectively handle Massive Outliers with extremely large values, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs")(c)(d). OmniQuant [[42](#bib.bib42)] and AffineQuant [[36](#bib.bib36)],
    on the other hand, exhibit training instability issues [[31](#bib.bib31)] due
    to the presence of Massive Outliers. Consequently, there is a pressing need for
    an LLM quantization approach that effectively addresses both Normal and Massive
    Outliers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM量化方法中，一个主要问题是激活异常值的存在，这会扩大量化步长，从而导致显著的准确度损失[[49](#bib.bib49)]。为解决此问题，当前研究开发了各种方法来处理激活中的正常异常值，这些异常值在所有标记的多个通道中持续存在[[12](#bib.bib12),
    [52](#bib.bib52)]。然而，除了正常异常值，还有另一种激活异常值[[43](#bib.bib43), [32](#bib.bib32)]，称为**大规模异常值**。这些异常值的特点是其值极高且只在一部分标记中出现，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs")(b)所示。不幸的是，现有的LLM量化方法难以有效处理这些大规模异常值。例如，SmoothQuant[[52](#bib.bib52)]
    尽管使用平滑因子将一些激活异常值转移到权重部分，但仍无法有效处理值极大的大规模异常值，如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs")(c)(d)所示。另一方面，OmniQuant[[42](#bib.bib42)] 和 AffineQuant[[36](#bib.bib36)]由于大规模异常值的存在，表现出训练不稳定性问题[[31](#bib.bib31)]。因此，迫切需要一种能够有效处理正常异常值和大规模异常值的LLM量化方法。
- en: To tackle this challenge, we propose the Dual transformations Quantization (DuQuant)
    method. Our motivation is to redistribute the activation outlier values across
    different channels, facilitating easier quantization. Specifically, we construct
    the orthogonal rotation matrix and the orthogonal permutation matrix. By multiplying
    these matrices with the activations, we can effectively perform column transformations
    on the activations, which in turn allows for the redistribution of outliers. For
    the rotation transformation aspect, we first identify specific dimensions of outliers
    as the prior knowledge and employ a greedy algorithm to construct the rotation
    matrix. To enhance the multiplication efficiency, we utilize diagonal block-wise
    rotation matrices, with each matrix responsible for a small portion of the activations.
    However, this approach may result in uneven outlier magnitudes across different
    blocks. Therefore, we propose the zigzag permutation for reordering the activation
    channels, which promotes a more uniform distribution across different blocks.
    Concretely, we distribute the channels with the highest activations across the
    blocks in a back-and-forth pattern. After establishing blocks with uniformly distributed
    outlier magnitudes, we employ another rotation transformation to further redistribute
    the outliers within each block. Note that we multiply the weight matrix with the
    transpose of the rotation and permutation matrices at the same time, preserving
    the linear layer equivalence and smoothing weights. Theoretical analysis confirms
    that the rotation and permutation transformations greatly mitigate quantization
    challenges induced by outliers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一挑战，我们提出了双重变换量化（DuQuant）方法。我们的动机是将激活异常值在不同通道之间重新分配，以便于量化。具体而言，我们构建了正交旋转矩阵和正交置换矩阵。通过将这些矩阵与激活值相乘，我们可以有效地对激活值进行列变换，从而实现异常值的重新分配。对于旋转变换方面，我们首先识别异常值的特定维度作为先验知识，并使用贪婪算法构建旋转矩阵。为了提高乘法效率，我们利用对角块旋转矩阵，每个矩阵负责一小部分激活值。然而，这种方法可能导致不同块之间的异常值幅度不均。因此，我们提出了之字形置换以重新排序激活通道，促进不同块之间的更均匀分布。具体而言，我们将具有最高激活值的通道以来回模式分配到各个块中。在建立了具有均匀分布的异常值幅度的块之后，我们使用另一个旋转变换进一步在每个块内重新分配异常值。请注意，我们同时将权重矩阵与旋转矩阵和置换矩阵的转置相乘，保持线性层等效性并平滑权重。理论分析确认，旋转和置换变换大大减轻了异常值引发的量化挑战。
- en: 'Extensive evaluations demonstrate that our DuQuant approach significantly outperforms
    existing 4-bit weight-activation quantization baselines across various benchmarks.
    Notably, DuQuant achieves a 5% improvement in Commonsense QA tasks across all
    LLaMA model sizes and a 10% increase in zero-shot MMLU benchmarks for the Vicuna-v1.5-13B.
    Moreover, in practical applications with the LLaMA2-7B model, DuQuant not only
    accelerates prefilling phase by up to 2.08$\times$, with minimal impact on performance:
    only a 0.61 increase in perplexity and a 2.71% drop in accuracy compared to the
    FP16 model. These results highlight the effectiveness of DuQuant in enhancing
    both the efficiency and capacity of quantized LLMs.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛的评估表明，我们的DuQuant方法在各种基准测试中显著超越了现有的4位权重-激活量化基准。值得注意的是，DuQuant在所有LLaMA模型大小的Commonsense
    QA任务中实现了5%的提升，在Vicuna-v1.5-13B的零样本MMLU基准中提高了10%。此外，在使用LLaMA2-7B模型的实际应用中，DuQuant不仅将预填充阶段加速了最多2.08$\times$，而且对性能影响最小：与FP16模型相比，困惑度仅增加0.61，准确率下降2.71%。这些结果突显了DuQuant在提升量化LLM的效率和容量方面的有效性。
- en: '![Refer to caption](img/09cb2891b2bad4ca1a55efd94c2ff26b.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/09cb2891b2bad4ca1a55efd94c2ff26b.png)'
- en: 'Figure 1: Visualizations of Outliers in LLaMA2-7B. (a) Input activation of
    Layer1 attention key projection shows Normal Outliers with relatively high magnitudes
    across all token sequences. (b) Input activation of Layer1 FFN down projection
    reveals Massive Outliers, presenting extremely high magnitudes (around 1400) at
    very few tokens. (c) Application of SmoothQuant on FFN down projection, illustrating
    its struggle with massive outliers in the Activation matrix. (d) Corresponding
    weight changes with SmoothQuant, highlighting the emergence of new outliers.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLaMA2-7B中离群值的可视化。（a）Layer1注意力关键投影的输入激活显示正常离群值在所有标记序列中具有相对较高的幅度。（b）Layer1
    FFN下投影的输入激活揭示了大规模离群值，在少数标记中呈现出极高的幅度（约1400）。（c）对FFN下投影应用SmoothQuant，显示其在激活矩阵中处理大规模离群值的困难。（d）与SmoothQuant相关的权重变化，突出显示了新离群值的出现。
- en: 2 Motivation
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 动机
- en: Normal Outliers and Massive Outliers.
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 正常离群值和大规模离群值。
- en: 'Previous works [[12](#bib.bib12), [60](#bib.bib60), [30](#bib.bib30)] have
    highlighted the challenge posed by activation outliers in LLMs for model compression.
    These outlier features consistently manifest large values across specific feature
    dimensions and are present in all token sequences [[52](#bib.bib52)], which we
    refer to as Normal Outliers. Recently, a distinct type of outlier [[43](#bib.bib43),
    [32](#bib.bib32)], termed Massive Outliers, also known as attention sinks [[53](#bib.bib53)],
    has been observed in LLMs. The primary distinctions between normal and massive
    outliers are: 1) Normal outliers persist across all token sequences, whereas massive
    outliers are confined to a limited number of tokens. 2) Massive outliers exhibit
    significantly larger magnitudes, often surpassing 100 and being approximately
    1000 times greater than the median of other activations [[43](#bib.bib43)]. In
    our study, we delve deeper into the impact of these two distinct types of outliers
    on quantization.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究[[12](#bib.bib12), [60](#bib.bib60), [30](#bib.bib30)]已经强调了激活离群值在LLM模型压缩中所带来的挑战。这些离群特征在特定特征维度上始终表现出较大的值，并且存在于所有的标记序列[[52](#bib.bib52)]中，我们称之为正常离群值。最近，在LLM中观察到了另一种离群值[[43](#bib.bib43),
    [32](#bib.bib32)]，被称为大规模离群值，也称为注意力汇聚[[53](#bib.bib53)]。正常离群值和大规模离群值的主要区别是：1）正常离群值在所有标记序列中存在，而大规模离群值仅限于少量标记中。2）大规模离群值表现出显著更大的幅度，通常超过100，约为其他激活值中位数的1000倍[[43](#bib.bib43)]。在我们的研究中，我们深入探讨了这两种不同类型的离群值对量化的影响。
- en: Massive Outliers Exist at the Second Linear Layer of FFN Module.
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大规模离群值存在于FFN模块的第二个线性层。
- en: In contrast to previous studies [[43](#bib.bib43), [32](#bib.bib32)] that observe
    massive outliers at the output of Transformer blocks, we first discover that these
    extremely large activations exist at the input of the down-projection layer within
    the FFN module. As depicted in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs"), the input of the down-projection layer in the LLaMA2-7B model Layer
    1 contains a single activation of significant magnitude (approximately 1200).
    This activation is isolated to one token and therefore classified as one of massive
    activations. This phenomenon is consistently observed across different layers
    and sizes of models, as illustrated in Appendix [H](#A8 "Appendix H More Visualizations
    ‣ Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs").
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前研究[[43](#bib.bib43), [32](#bib.bib32)]观察到的大规模离群值在Transformer块的输出处不同，我们首先发现这些极大的激活值存在于FFN模块中的下投影层输入处。如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs")所示，LLaMA2-7B模型Layer 1的下投影层输入包含一个幅度显著的单一激活值（约1200）。该激活值仅限于一个标记，因此被分类为大规模激活值。这种现象在不同层和模型大小中一致存在，如附录[H](#A8
    "Appendix H More Visualizations ‣ Rotation and Permutation for Advanced Outlier
    Management and Efficient Quantization of LLMs")所示。
- en: Massive Outliers Enlarge Quantization Difficulty.
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大规模离群值增加量化难度。
- en: Although previous studies [[52](#bib.bib52), [42](#bib.bib42), [36](#bib.bib36),
    [1](#bib.bib1)] have proposed various approaches to eliminate outlier features,
    they still face challenges in effectively managing massive outliers. SmoothQuant [[52](#bib.bib52)],
    for instance, attempts to shift the quantization difficulty from activations to
    weights by dividing the activation by a per-channel smoothing factor and multiplying
    it to the weight matrix. Nevertheless, we observe that this transfer at the input
    of the down-projection layer can cause the weights of the down-projection to display
    noticeable outliers, as demonstrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs") . This issue arises because massive outliers cause the smoothing factor
    to become significantly large. Moreover, extremely large outliers can lead optimization-based
    methods to encounter problems with loss explosion. Both OmniQuant [[42](#bib.bib42)]
    and AffineQuant [[36](#bib.bib36)] have had to exclude their learnable parameters
    for the down projection layer due to unstable gradients. Given the poor accuracy
    observed with 4-bit quantization, QUIK [[1](#bib.bib1)] opts to use INT8 quantization
    for the down projection layer and Atom [[61](#bib.bib61)] applies INT8 quantization
    for 128 outlier channels. Consequently, massive outliers introduce new challenges
    to the quantization process that existing methods cannot fully address. This observation
    has motivated us to develop rotation and permutation transformation, which effectively
    handles both massive and normal outliers and achieves state-of-the-art performance.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管以前的研究 [[52](#bib.bib52), [42](#bib.bib42), [36](#bib.bib36), [1](#bib.bib1)]
    提出了多种方法来消除异常特征，但在有效管理大量异常值方面仍面临挑战。例如，SmoothQuant [[52](#bib.bib52)] 试图通过将激活值除以每通道平滑因子并将其乘以权重矩阵，将量化难度从激活值转移到权重上。然而，我们观察到，这种转移在下投影层的输入处会导致下投影的权重显示出明显的异常值，如图
    [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 高级异常值管理和 LLMs 的高效量化") 所示。这一问题的原因在于，大量异常值导致平滑因子变得非常大。此外，极大的异常值可能导致基于优化的方法遇到损失爆炸问题。由于梯度不稳定，OmniQuant
    [[42](#bib.bib42)] 和 AffineQuant [[36](#bib.bib36)] 不得不排除其可学习的参数用于下投影层。鉴于 4 位量化的准确性较差，QUIK
    [[1](#bib.bib1)] 选择对下投影层使用 INT8 量化，而 Atom [[61](#bib.bib61)] 对 128 个异常通道应用 INT8
    量化。因此，大量异常值对量化过程带来了新的挑战，而现有方法无法完全解决这一问题。这一观察促使我们开发了旋转和置换变换，有效地处理了大量和普通异常值，并达到了最先进的性能。
- en: 3 Method
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'In this section, we delve into the distribution of outliers and introduce our
    proposed DuQuant method. The DuQuant method is built on two key components: 1)
    the block-diagonal rotation matrix, tasked with the local redistribution of feature
    outliers, and 2) the zigzag permutation, responsible for the global reordering
    of outliers across different blocks.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们深入探讨了异常值的分布，并介绍了我们提出的 DuQuant 方法。DuQuant 方法基于两个关键组件：1）块对角旋转矩阵，负责特征异常值的局部重分布，以及
    2）之字形置换，负责不同块之间异常值的全局重新排序。
- en: 3.1 Preliminaries
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基础知识
- en: 'As the common modules within each transformer block of LLMs, both Multi-head
    Self-Attention (MSA) and Feed-Forward Network (FFN) fundamentally consist of basic
    linear layers, which can be represented as, $\mathbf{Y}=\mathbf{X}\cdot\mathbf{W}\in\mathbb{R}^{T\times
    C_{out}}$:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 LLMs 中每个变换器块的常见模块，Multi-head Self-Attention（MSA）和 Feed-Forward Network（FFN）基本上由基本的线性层组成，可以表示为
    $\mathbf{Y}=\mathbf{X}\cdot\mathbf{W}\in\mathbb{R}^{T\times C_{out}}$：
- en: '|  | $\mathbf{X}_{q}=\text{clamp}\left(\left\lfloor\frac{\mathbf{X}}{\Delta}\right\rceil\!\!+\!z,0,2^{b}-1\right),\textrm{where}~{}\Delta=\frac{\max(\mathbf{X})-\min(\mathbf{X})}{2^{b}-1},z=-\left\lfloor\frac{\min(\mathbf{X})}{\Delta}\right\rceil.~{}~{}~{}$
    |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{X}_{q}=\text{clamp}\left(\left\lfloor\frac{\mathbf{X}}{\Delta}\right\rceil\!\!+\!z,0,2^{b}-1\right),\textrm{其中}~{}\Delta=\frac{\max(\mathbf{X})-\min(\mathbf{X})}{2^{b}-1},z=-\left\lfloor\frac{\min(\mathbf{X})}{\Delta}\right\rceil.~{}~{}~{}$
    |  | (1) |'
- en: The notation $\left\lfloor\cdot\right\rceil$).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '符号 $\left\lfloor\cdot\right\rceil$）。 '
- en: '![Refer to caption](img/8b91ac31f593e0ebab135e648496aff6.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8b91ac31f593e0ebab135e648496aff6.png)'
- en: 'Figure 2: Transformation Steps for Activation Matrices after smooth technique.
    (a) Sequential transformations on Normal Outliers: ① initial rotation to reduce
    outliers within blocks, ② permutation to evenly distribute outliers across blocks,
    and ③ a second rotation for further smoothing. (b) Activation changes for Massive
    Outliers before and after DuQuant. (c) A sample matrix for highlighting the continual
    reduction of outliers through rotation and permutation, with outliers marked in
    dark blue.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：平滑技术后激活矩阵的变换步骤。 (a) 对正常异常值的顺序变换：① 初始旋转以减少块内的异常值，② 排列以均匀分布块中的异常值，③ 第二次旋转以进一步平滑。
    (b) DuQuant 之前和之后的大量异常值的激活变化。 (c) 一个示例矩阵，用于突出通过旋转和排列不断减少的异常值，异常值标记为深蓝色。
- en: 3.2 The proposed DuQuant Method
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 提出的 DuQuant 方法
- en: To address the Normal Outliers issue stated in Section [2](#S2 "2 Motivation
    ‣ Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs"), current quantization methods, such as SmoothQuant [[52](#bib.bib52)]
    and OmniQuant [[52](#bib.bib52)], usually adopt the smooth technique. Concretely,
    it involves the utilization of a per-channel smoothing diagonal matrix, denoted
    as $\mathbf{\Lambda}$, which in turn introduce new outliers in the weight matrix
    and result in significant performance declines in low-bit quantization scenarios,
    such as INT4 .
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决第[2](#S2 "2 Motivation ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs")节中提到的正常异常值问题，目前的量化方法，如 SmoothQuant [[52](#bib.bib52)]
    和 OmniQuant [[52](#bib.bib52)]，通常采用平滑技术。具体而言，它涉及使用每通道的平滑对角矩阵，记作 $\mathbf{\Lambda}$，这反过来会在权重矩阵中引入新的异常值，并导致低位量化场景下（如
    INT4）的性能显著下降。
- en: According to these findings, we propose the DuQuant method, which includes the
    Rotation and Permutation transformations based on the smooth technique. By combining
    rotation transformation and channel permutation, our DuQuant method aims to redistribute
    these features within the activation space, thereby mitigating the effects of
    both Normal and Massive Outliers.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些发现，我们提出了 DuQuant 方法，该方法包括基于平滑技术的旋转和排列变换。通过结合旋转变换和通道排列，我们的 DuQuant 方法旨在重新分配激活空间中的这些特征，从而减轻正常和大量异常值的影响。
- en: The Rotation Transformation.
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 旋转变换。
- en: In contrast to the smooth technique, our aim is to apply a rotation matrix for
    row or column transformations, mitigating the impact of both Normal and Massive
    outliers. The ideal rotation matrix, denoted as $\mathbf{R}$ involves the following
    steps,
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与平滑技术相比，我们的目标是应用一个用于行或列变换的旋转矩阵，从而减轻正常和大量异常值的影响。理想的旋转矩阵，用 $\mathbf{R}$ 表示，涉及以下步骤，
- en: $\circ$
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\circ$
- en: Identify the feature dimension $d^{(1)}$.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确定特征维度 $d^{(1)}$。
- en: $\circ$
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\circ$
- en: Based on the searched dimensions $d^{(1)}$, we construct the rotation matrix
    as follows,
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于搜索到的维度 $d^{(1)}$，我们构造旋转矩阵如下，
- en: '|  | $\mathbf{R^{1}}=\mathbf{E}_{d^{(1)}}\tilde{\mathbf{R}}\mathbf{Q}\mathbf{E}_{d^{(1)}},\qquad\mathbf{Q}=\small{\left[\begin{matrix}1&amp;\mathbf{O}\\
    \mathbf{O}&amp;\mathbf{Q}^{\prime}\end{matrix}\right]}.$ |  | (2) |'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathbf{R^{1}}=\mathbf{E}_{d^{(1)}}\tilde{\mathbf{R}}\mathbf{Q}\mathbf{E}_{d^{(1)}},\qquad\mathbf{Q}=\small{\left[\begin{matrix}1&\mathbf{O}\\
    \mathbf{O}&\mathbf{Q}^{\prime}\end{matrix}\right]}.$ |  | (2) |'
- en: Here, $\mathbf{E}_{d^{(1)}}$.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，$\mathbf{E}_{d^{(1)}}$。
- en: $\circ$
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\circ$
- en: Let $N$.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $N$。
- en: Through this construction manner, we can ensure that the approximated optimal
    rotation matrix $\hat{\mathbf{R}}$ in a block-wise manner,
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种构造方式，我们可以确保按块方式的近似最优旋转矩阵 $\hat{\mathbf{R}}$，
- en: '|  | $\centering\hat{\mathbf{R}}=~{}\text{BlockDiag}(\hat{\mathbf{R}}_{b_{1}},...,\hat{\mathbf{R}}_{b_{K}}),\@add@centering$
    |  | (3) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\centering\hat{\mathbf{R}}=~{}\text{BlockDiag}(\hat{\mathbf{R}}_{b_{1}},...,\hat{\mathbf{R}}_{b_{K}}),\@add@centering$
    |  | (3) |'
- en: where $\hat{\mathbf{R}}_{b_{i}}\in\mathbb{R}^{2^{n}\times 2^{n}}$.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{\mathbf{R}}_{b_{i}}\in\mathbb{R}^{2^{n}\times 2^{n}}$。
- en: The Permutation Transformation.
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 排列变换。
- en: Despite adopting the block-diagonal rotation matrix $\hat{\mathbf{R}}$ for its
    time and storage efficiency, its focus on local information introduces a potential
    limitation in further reducing the outliers. This is because the rotation transformation,
    conducted within each small block, cannot integrate the information across different
    blocks to further minimize outliers. Consequently, one block may have relatively
    larger outliers while another block has smaller outliers, resulting in high variance
    among different blocks, as shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Preliminaries
    ‣ 3 Method ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs"). This limitation explains that merely utilizing the block-diagonal
    rotation matrix is insufficient to effectively reduce the overall outliers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管采用了块对角旋转矩阵 $\hat{\mathbf{R}}$ 以提高时间和存储效率，但其对局部信息的关注引入了在进一步减少异常值方面的潜在限制。这是因为在每个小块内进行的旋转变换无法整合跨不同块的信息以进一步最小化异常值。因此，一个块可能有相对较大的异常值，而另一个块则有较小的异常值，导致不同块之间的方差较高，如图[2](#S3.F2
    "图 2 ‣ 3.1 基础 ‣ 3 方法 ‣ 高级异常值管理和高效量化 LLM 的旋转与置换")所示。这一限制解释了仅使用块对角旋转矩阵不足以有效减少整体异常值。
- en: To effectively mitigate the overall outliers, it is essential to balance the
    outliers’ magnitudes among various blocks. Specifically, within each small block,
    we denote the largest outlier in dimension $d_{j}$. Then the variance in activation
    magnitudes across various blocks can be expressed as,
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效减轻整体异常值，有必要在各个块之间平衡异常值的幅度。具体而言，在每个小块内，我们标记维度 $d_{j}$ 中最大的异常值。然后，不同块之间的激活幅度方差可以表示为，
- en: '|  | $\centering\text{Var}([M_{b_{1}},M_{b_{2}},...,M_{b_{K}}]).\@add@centering$
    |  | (4) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\centering\text{Var}([M_{b_{1}},M_{b_{2}},...,M_{b_{K}}]).\@add@centering$
    |  | (4) |'
- en: To minimize this variance and further reduce the overall outliers, we introduce
    the zigzag permutation. Concretely, we generate a zigzag sequence that starts
    by assigning channels with the highest activations to the first block. The process
    continues by assigning channels with the next highest activations to the subsequent
    blocks in descending order until the end of block $K$. By employing the zigzag
    permutation, we achieve a balanced distribution of outliers across different blocks.
    This allows us to use an additional rotation transformation to further smooth
    the outliers. Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Preliminaries ‣ 3 Method ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs") provides an illustration of outlier mitigation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化这种方差并进一步减少整体异常值，我们引入了锯齿形置换。具体来说，我们生成一个锯齿形序列，首先将激活值最高的通道分配给第一个块。接下来，将激活值次高的通道按降序分配给后续的块，直到块
    $K$ 的末尾。通过使用锯齿形置换，我们实现了异常值在不同块之间的均衡分布。这使我们能够使用额外的旋转变换进一步平滑异常值。图[2](#S3.F2 "图 2
    ‣ 3.1 基础 ‣ 3 方法 ‣ 高级异常值管理和高效量化 LLM 的旋转与置换")提供了异常值减轻的示意图。
- en: The Overall DuQuant Method.
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总体 DuQuant 方法。
- en: To effectively mitigate both Normal and Massive Outliers, we first employ the
    smooth technique to shift the quantization challenge from activations to weights.
    Next, we introduce the block-diagonal rotation matrix $\hat{\mathbf{R}}$ to locally
    redistribute feature outliers within the activation space. We then propose the
    zigzag permutation matrix for globally balancing the outliers across different
    blocks, followed by another application of the block-diagonal rotation transformation.
    To sum up, the linear layers within the transformer can be rewrite as,
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效减轻正常异常值和大规模异常值，我们首先使用平滑技术将量化挑战从激活转移到权重。接下来，我们引入块对角旋转矩阵 $\hat{\mathbf{R}}$
    来在激活空间内局部重新分配特征异常值。然后，我们提出了锯齿形置换矩阵，以在不同块之间全球平衡异常值，随后再次应用块对角旋转变换。总之，变换器中的线性层可以被重写为，
- en: '|  | $\centering\small{\mathbf{Y}=\mathbf{X}\cdot\mathbf{W}=[(\mathbf{X}\cdot\underbrace{\mathbf{\Lambda})\hat{\mathbf{R}}_{(1)}\cdot\mathbf{P}\cdot\hat{\mathbf{R}}_{(2)}}_{\mathbf{G}}]\cdot[\underbrace{\hat{\mathbf{R}}_{(2)}^{\top}\cdot\mathbf{P}^{\top}\cdot\hat{\mathbf{R}}_{(1)}^{\top}(\mathbf{\Lambda}^{-1}}_{\mathbf{G}^{-1}}\cdot\mathbf{W})],}\@add@centering$
    |  | (5) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\centering\small{\mathbf{Y}=\mathbf{X}\cdot\mathbf{W}=[(\mathbf{X}\cdot\underbrace{\mathbf{\Lambda})\hat{\mathbf{R}}_{(1)}\cdot\mathbf{P}\cdot\hat{\mathbf{R}}_{(2)}}_{\mathbf{G}}]\cdot[\underbrace{\hat{\mathbf{R}}_{(2)}^{\top}\cdot\mathbf{P}^{\top}\cdot\hat{\mathbf{R}}_{(1)}^{\top}(\mathbf{\Lambda}^{-1}}_{\mathbf{G}^{-1}}\cdot\mathbf{W})],}\@add@centering$
    |  | (5) |'
- en: where the notation $\mathbf{P}$ represent the first and second block-diagonal
    rotation matrix, respectively.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中符号 $\mathbf{P}$ 分别表示第一和第二块对角旋转矩阵。
- en: Remark 1. It is worth noting that the proposed RAP method can simultaneously
    smooth the weight matrix. While the commonly adopted smooth technique is effective,
    it can cause the weight matrix of the down-projection layer to exhibit pronounced
    outliers, leading to performance degradation. However, in the proposed RAP method,
    the rotation transformation we designed is applied to not only the activation
    input but also the weight matrix. As a result, the outliers induced by the smooth
    technique can be mitigated through our approximated rotation matrix $\hat{\mathbf{R}}$,
    yielding a smoother, more quantization-friendly weight matrix. Moreover, this
    approach eliminates the reliance on complex weight quantization techniques, such
    as GPTQ [[17](#bib.bib17)] used in Atom [[61](#bib.bib61)].
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 备注 1. 值得注意的是，所提出的 RAP 方法可以同时平滑权重矩阵。虽然常用的平滑技术是有效的，但它可能导致下投影层的权重矩阵出现明显的异常值，从而导致性能下降。然而，在所提出的
    RAP 方法中，我们设计的旋转变换不仅应用于激活输入，还应用于权重矩阵。因此，通过我们近似的旋转矩阵 $\hat{\mathbf{R}}$ 可以缓解平滑技术引发的异常值，生成一个更平滑、更适合量化的权重矩阵。此外，这种方法消除了对复杂权重量化技术的依赖，如
    Atom [[61](#bib.bib61)] 中使用的 GPTQ [[17](#bib.bib17)]。
- en: Remark 2. To further decrease the computation and memory costs, we initially
    construct the $k$.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 备注 2. 为了进一步减少计算和内存开销，我们最初构建了 $k$。
- en: 3.3 Theoretical Analysis
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 理论分析
- en: To further demonstrate the effectiveness of the proposed DuQuant method, we
    conduct a theoretical analysis of the rotation and permutation transformations.
    Theorem [1](#Thmtheorem1a "Theorem 1 (Rotation). ‣ Appendix B Proofs ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs") shows that within each block, the constructed rotation matrix effectively
    mitigates the maximum outlier, thereby reducing the outlier magnitude through
    a greedy search. Theorem [2](#Thmtheorem2a "Theorem 2 (Zigzag Permutation). ‣
    Appendix B Proofs ‣ Rotation and Permutation for Advanced Outlier Management and
    Efficient Quantization of LLMs") reveals that the employed zigzag permutation
    ensures a balanced upper bound shared among different blocks. This suggests that
    the zigzag permutation effectively reduces the variance shown in Eqn. ([4](#S3.E4
    "In The Permutation Transformation. ‣ 3.2 The proposed DuQuant Method ‣ 3 Method
    ‣ Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs")) and thus assists the rotation matrix in further decreasing the outliers.
    Please refer to Appendix [B](#A2 "Appendix B Proofs ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs") for detailed
    proofs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步展示所提出的 DuQuant 方法的有效性，我们进行了旋转和置换变换的理论分析。定理 [1](#Thmtheorem1a "定理 1（旋转）。
    ‣ 附录 B 证明 ‣ 高级异常值管理和高效量化 LLM 的旋转和置换") 证明了在每个块内，构建的旋转矩阵有效地缓解了最大异常值，从而通过贪婪搜索减少了异常值的幅度。定理
    [2](#Thmtheorem2a "定理 2（之字形置换）。 ‣ 附录 B 证明 ‣ 高级异常值管理和高效量化 LLM 的旋转和置换") 揭示了所使用的之字形置换确保了不同块之间共享的平衡上界。这表明之字形置换有效地减少了在
    Eqn. ([4](#S3.E4 "在置换变换中。 ‣ 3.2 提出的 DuQuant 方法 ‣ 3 方法 ‣ 高级异常值管理和高效量化 LLM 的旋转和置换"))
    中显示的方差，从而帮助旋转矩阵进一步减少异常值。详细证明请参见附录 [B](#A2 "附录 B 证明 ‣ 高级异常值管理和高效量化 LLM 的旋转和置换")。
- en: Theorem 1  (Rotation).
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 1（旋转）。
- en: For the activation input $\mathbf{X}\in\mathbb{R}^{T\times C_{in}}$ within the
    input. Then, we can deduce that,
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入中的激活输入 $\mathbf{X}\in\mathbb{R}^{T\times C_{in}}$。然后，我们可以推导出，
- en: '|  | $1$2 |  | (6) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: Theorem 2  (Zigzag Permutation).
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 2（之字形置换）。
- en: For the activation input $\mathbf{X}\in\mathbb{R}^{T\times C_{in}}$-th block
    consistently satisfies,
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于激活输入 $\mathbf{X}\in\mathbb{R}^{T\times C_{in}}$，第 $-th$ 块一致满足，
- en: '|  | $M_{b_{i}}\leq O^{(1)}+\small{\frac{(2^{n}K-1)(2^{n-1}-1)}{2^{n}}\delta},\qquad
    i=1,2,3,...,K.$ |  | (7) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $M_{b_{i}}\leq O^{(1)}+\small{\frac{(2^{n}K-1)(2^{n-1}-1)}{2^{n}}\delta},\qquad
    i=1,2,3,...,K.$ |  | (7) |'
- en: 4 Experiment
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: Models and Evaluations.
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型与评估。
- en: 'We apply our DuQuant on pre-trained LLMs: LLaMA (7B-65B) [[45](#bib.bib45)],
    LLaMA2 (7B-70B) [[46](#bib.bib46)], LLaMA3 (8B, 70B) and instruction-tuned LLMs:
    Vicuna-v1.5 (7B-13B) [[9](#bib.bib9)]. We evaluate quantized LLaMA models on language
    generation tasks and commonsense QA tasks. Specifically, we assess the perplexity
    on WikiText2 [[37](#bib.bib37)] and C4 [[39](#bib.bib39)] datasets, as well as
    the zero-shot accuracy on PIQA [[6](#bib.bib6)], ARC [[11](#bib.bib11)], BoolQ [[10](#bib.bib10)],
    HellaSwag [[56](#bib.bib56)], and WinoGrande [[40](#bib.bib40)] datasets. Moreover,
    we evaluate quantized Vicuna models on MMLU [[20](#bib.bib20)] and MT-Bench [[62](#bib.bib62)]
    benchmarks.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的DuQuant应用于预训练的LLMs：LLaMA (7B-65B) [[45](#bib.bib45)]、LLaMA2 (7B-70B) [[46](#bib.bib46)]、LLaMA3
    (8B, 70B) 以及指令调优的LLMs：Vicuna-v1.5 (7B-13B) [[9](#bib.bib9)]。我们在语言生成任务和常识问答任务上评估量化的LLaMA模型。具体来说，我们评估WikiText2
    [[37](#bib.bib37)] 和C4 [[39](#bib.bib39)] 数据集上的困惑度，以及PIQA [[6](#bib.bib6)]、ARC
    [[11](#bib.bib11)]、BoolQ [[10](#bib.bib10)]、HellaSwag [[56](#bib.bib56)] 和WinoGrande
    [[40](#bib.bib40)] 数据集上的零样本准确性。此外，我们还在MMLU [[20](#bib.bib20)] 和MT-Bench [[62](#bib.bib62)]
    基准上评估了量化的Vicuna模型。
- en: Implementation Details.
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现细节。
- en: In line with prior studies [[31](#bib.bib31), [42](#bib.bib42)], we apply per-token
    activation quantization and per-channel weight quantization. Given that W8A8 quantization
    has been established as lossless in precision by SmoothQuant [[52](#bib.bib52)],
    our primary evaluation in this paper focuses on 4-bit and 6-bit quantization for
    weights and activations. As for details, we quantize all intermediate activations,
    excluding the SoftMax output. Moreover, we have developed two types of quantized
    models, denoted as DuQuant and DuQuant+LWC . For DuQuant, we employ round-to-nearest
    quantization, using a clipping ratio of 0.9 for activations and 0.8 for weights.
    To improve weight matrix quantization, DuQuant+LWC integrates the learnable weight
    clipping (LWC) technique from OmniQuant. Concretely, LWC adjusts weights by training
    parameters $\gamma,\beta\in[0,1]$ in Eqn. ([1](#S3.E1 "In 3.1 Preliminaries ‣
    3 Method ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs")). Notably, the smoothing diagonal matrix and the learned
    weight clipping factor can be integrated into the quantized weights, introducing
    no additional computational or memory costs. More details and hyperparameters
    are left in Appendix [C](#A3 "Appendix C Additional Implementation Details ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs").
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的研究 [[31](#bib.bib31), [42](#bib.bib42)]，我们应用了每个令牌的激活量化和每通道的权重量化。由于SmoothQuant
    [[52](#bib.bib52)] 已经证明W8A8量化在精度上是无损的，本论文的主要评估集中在权重和激活的4位和6位量化上。具体来说，我们量化所有中间激活，排除SoftMax输出。此外，我们开发了两种类型的量化模型，分别称为DuQuant和DuQuant+LWC。对于DuQuant，我们采用四舍五入量化，激活的裁剪比为0.9，权重的裁剪比为0.8。为了改进权重矩阵的量化，DuQuant+LWC整合了来自OmniQuant的可学习权重裁剪（LWC）技术。具体而言，LWC通过训练参数$\gamma,\beta\in[0,1]$
    在公式 ([1](#S3.E1 "In 3.1 Preliminaries ‣ 3 Method ‣ Rotation and Permutation for
    Advanced Outlier Management and Efficient Quantization of LLMs")) 中调整权重。值得注意的是，平滑对角矩阵和学习到的权重裁剪因子可以集成到量化权重中，不会引入额外的计算或内存成本。更多细节和超参数请参见附录[C](#A3
    "Appendix C Additional Implementation Details ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs")。
- en: Baselines.
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准线。
- en: We compare with state-of-the-art (SOTA) weight-activation PTQ methods, including
    SmoothQuant [[52](#bib.bib52)], Outlier Supression+ [[49](#bib.bib49)], OmniQuant [[42](#bib.bib42)],
    QLLM [[31](#bib.bib31)], AffineQuant [[36](#bib.bib36)], and Atom [[61](#bib.bib61)].
    For Atom, we reproduce the results with no group-wise asymmetric quantization.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与最先进（SOTA）的权重-激活PTQ方法进行比较，包括SmoothQuant [[52](#bib.bib52)]、Outlier Supression+
    [[49](#bib.bib49)]、OmniQuant [[42](#bib.bib42)]、QLLM [[31](#bib.bib31)]、AffineQuant
    [[36](#bib.bib36)] 和 Atom [[61](#bib.bib61)]。对于Atom，我们在没有组间不对称量化的情况下重现了结果。
- en: 'Table 1: Perplexity ($\downarrow$) results under 4-bit weight-activation quantization.
    The results for W6A6 can be found in Table [D4](#A4.T4 "Table D4 ‣ W6A6 Quantization
    Results. ‣ Appendix D More Empirical Results ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs"). Atom and OmniQuant unprocessed
    group-query attention for LLaMA2-70B.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：4位权重-激活量化下的困惑度（$\downarrow$）结果。W6A6的结果可以在表[D4](#A4.T4 "Table D4 ‣ W6A6 Quantization
    Results. ‣ Appendix D More Empirical Results ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs") 中找到。Atom 和 OmniQuant 对
    LLaMA2-70B 的未处理组查询注意力。
- en: '| Dataset | #Bit | Method | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B | 2-70B
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | #Bit | 方法 | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B | 2-70B |'
- en: '| WikiText2 | FP16 | - | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 | FP16 | - | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 |'
- en: '| W4A4 | SmoothQuant | 25.25 | 40.05 | 192.40 | 275.53 | 83.12 | 35.88 | 26.01
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | SmoothQuant | 25.25 | 40.05 | 192.40 | 275.53 | 83.12 | 35.88 | 26.01
    |'
- en: '| OmniQuant | 11.26 | 10.87 | 10.33 | 9.17 | 14.26 | 12.30 | NaN |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 11.26 | 10.87 | 10.33 | 9.17 | 14.26 | 12.30 | NaN |'
- en: '| AffineQuant | 10.28 | 10.32 | 9.35 | - | 12.69 | 11.45 | - |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 10.28 | 10.32 | 9.35 | - | 12.69 | 11.45 | - |'
- en: '| QLLM | 9.65 | 8.41 | 8.37 | 6.87 | 11.75 | 9.09 | 7.00 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 9.65 | 8.41 | 8.37 | 6.87 | 11.75 | 9.09 | 7.00 |'
- en: '| Atom | 8.15 | 7.43 | 6.52 | 5.14 | 8.40 | 6.96 | NaN |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Atom | 8.15 | 7.43 | 6.52 | 5.14 | 8.40 | 6.96 | NaN |'
- en: '| DuQuant | 6.40 | 5.65 | 4.72 | 4.13 | 6.28 | 5.42 | 3.79 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 6.40 | 5.65 | 4.72 | 4.13 | 6.28 | 5.42 | 3.79 |'
- en: '| DuQuant+LWC | 6.18 | 5.47 | 4.55 | 3.93 | 6.08 | 5.33 | 3.76 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 6.18 | 5.47 | 4.55 | 3.93 | 6.08 | 5.33 | 3.76 |'
- en: '| C4 | FP16 |  | 7.08 | 6.61 | 5.98 | 5.62 | 6.97 | 6.46 | 5.52 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| C4 | FP16 |  | 7.08 | 6.61 | 5.98 | 5.62 | 6.97 | 6.46 | 5.52 |'
- en: '| W4A4 | SmoothQuant | 32.32 | 47.18 | 122.38 | 244.35 | 77.27 | 43.19 | 34.61
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | SmoothQuant | 32.32 | 47.18 | 122.38 | 244.35 | 77.27 | 43.19 | 34.61
    |'
- en: '| OmniQuant | 14.51 | 13.78 | 12.49 | 11.28 | 18.02 | 14.55 | NaN |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 14.51 | 13.78 | 12.49 | 11.28 | 18.02 | 14.55 | NaN |'
- en: '| AffineQuant | 13.64 | 13.44 | 11.58 | - | 15.76 | 13.97 | - |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 13.64 | 13.44 | 11.58 | - | 15.76 | 13.97 | - |'
- en: '| QLLM | 12.29 | 10.58 | 11.51 | 8.98 | 13.26 | 11.13 | 8.89 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 12.29 | 10.58 | 11.51 | 8.98 | 13.26 | 11.13 | 8.89 |'
- en: '| Atom | 10.34 | 9.57 | 8.56 | 8.17 | 10.96 | 9.12 | NaN |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Atom | 10.34 | 9.57 | 8.56 | 8.17 | 10.96 | 9.12 | NaN |'
- en: '| DuQuant | 7.84 | 7.16 | 6.45 | 6.03 | 7.90 | 7.05 | 5.87 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 7.84 | 7.16 | 6.45 | 6.03 | 7.90 | 7.05 | 5.87 |'
- en: '| DuQuant+LWC | 7.73 | 7.07 | 6.37 | 5.93 | 7.79 | 7.02 | 5.85 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 7.73 | 7.07 | 6.37 | 5.93 | 7.79 | 7.02 | 5.85 |'
- en: 'Table 2: Zero-shot QA ($\uparrow$) results of LLaMA1 models under 4-bit weight-activation
    quantization. The results for LLaMA2 models and W6A6 quantization can be found
    in Table [D1](#A4.T1 "Table D1 ‣ Appendix D More Empirical Results ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs")  [D5](#A4.T5 "Table D5 ‣ W6A6 Quantization Results. ‣ Appendix D More Empirical
    Results ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs"), and  [D6](#A4.T6 "Table D6 ‣ W6A6 Quantization Results.
    ‣ Appendix D More Empirical Results ‣ Rotation and Permutation for Advanced Outlier
    Management and Efficient Quantization of LLMs").'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: LLaMA1 模型在 4 位权重-激活量化下的零样本 QA ($\uparrow$) 结果。LLaMA2 模型和 W6A6 量化的结果可以在表
    [D1](#A4.T1 "表 D1 ‣ 附录 D 更多实证结果 ‣ 高级异常值管理和高效 LLM 量化的旋转与排列")  [D5](#A4.T5 "表 D5
    ‣ W6A6 量化结果 ‣ 附录 D 更多实证结果 ‣ 高级异常值管理和高效 LLM 量化的旋转与排列")，和  [D6](#A4.T6 "表 D6 ‣ W6A6
    量化结果 ‣ 附录 D 更多实证结果 ‣ 高级异常值管理和高效 LLM 量化的旋转与排列") 中找到。'
- en: '| Model | Method | PIQA | ARC-E | ARC-C | BoolQ | HellaSwag | WinoGrande |
    Avg. |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | PIQA | ARC-E | ARC-C | BoolQ | HellaSwag | WinoGrande | 平均 |'
- en: '| LLaMA1-7B W4A4 | FP16 | 77.47 | 52.48 | 41.46 | 73.08 | 73.00 | 67.07 | 64.09
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-7B W4A4 | FP16 | 77.47 | 52.48 | 41.46 | 73.08 | 73.00 | 67.07 | 64.09
    |'
- en: '| SmoothQuant | 49.80 | 30.40 | 25.80 | 49.10 | 27.40 | 48.00 | 38.41 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 49.80 | 30.40 | 25.80 | 49.10 | 27.40 | 48.00 | 38.41 |'
- en: '| OS+ | 62.73 | 39.98 | 30.29 | 60.21 | 44.39 | 52.96 | 48.43 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | 62.73 | 39.98 | 30.29 | 60.21 | 44.39 | 52.96 | 48.43 |'
- en: '| OmniQuant | 66.15 | 45.20 | 31.14 | 63.51 | 56.44 | 53.43 | 52.65 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 66.15 | 45.20 | 31.14 | 63.51 | 56.44 | 53.43 | 52.65 |'
- en: '| AffineQuant | 69.37 | 42.55 | 31.91 | 63.73 | 57.65 | 55.33 | 53.42 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 69.37 | 42.55 | 31.91 | 63.73 | 57.65 | 55.33 | 53.42 |'
- en: '| QLLM | 68.77 | 45.20 | 31.14 | - | 57.43 | 56.67 | 51.84 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 68.77 | 45.20 | 31.14 | - | 57.43 | 56.67 | 51.84 |'
- en: '| Atom | 71.44 | 47.74 | 35.49 | 67.71 | 63.89 | 55.01 | 56.88 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Atom | 71.44 | 47.74 | 35.49 | 67.71 | 63.89 | 55.01 | 56.88 |'
- en: '| DuQuant | 76.44 | 50.04 | 38.99 | 70.98 | 69.39 | 64.72 | 61.76 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 76.44 | 50.04 | 38.99 | 70.98 | 69.39 | 64.72 | 61.76 |'
- en: '| DuQuant+LWC | 76.22 | 50.04 | 38.31 | 70.09 | 69.82 | 62.59 | 61.18 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 76.22 | 50.04 | 38.31 | 70.09 | 69.82 | 62.59 | 61.18 |'
- en: '| LLaMA1-13B W4A4 | FP16 | 79.10 | 59.89 | 44.45 | 68.01 | 76.21 | 70.31 |
    66.33 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-13B W4A4 | FP16 | 79.10 | 59.89 | 44.45 | 68.01 | 76.21 | 70.31 |
    66.33 |'
- en: '| SmoothQuant | 61.04 | 39.18 | 30.80 | 61.80 | 52.29 | 51.06 | 49.36 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 61.04 | 39.18 | 30.80 | 61.80 | 52.29 | 51.06 | 49.36 |'
- en: '| OS+ | 63.00 | 40.32 | 30.38 | 60.34 | 53.61 | 51.54 | 49.86 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | 63.00 | 40.32 | 30.38 | 60.34 | 53.61 | 51.54 | 49.86 |'
- en: '| OmniQuant | 69.69 | 47.39 | 33.10 | 62.84 | 58.96 | 55.80 | 54.37 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 69.69 | 47.39 | 33.10 | 62.84 | 58.96 | 55.80 | 54.37 |'
- en: '| AffineQuant | 66.32 | 43.90 | 29.61 | 64.10 | 56.88 | 54.70 | 52.58 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 66.32 | 43.90 | 29.61 | 64.10 | 56.88 | 54.70 | 52.58 |'
- en: '| QLLM | 71.38 | 47.60 | 34.30 | - | 63.70 | 59.43 | 55.28 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 71.38 | 47.60 | 34.30 | - | 63.70 | 59.43 | 55.28 |'
- en: '| Atom | 71.38 | 49.07 | 36.69 | 64.53 | 68.00 | 58.56 | 58.04 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Atom | 71.38 | 49.07 | 36.69 | 64.53 | 68.00 | 58.56 | 58.04 |'
- en: '| DuQuant | 77.26 | 58.04 | 41.55 | 67.55 | 73.62 | 66.69 | 64.12 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 77.26 | 58.04 | 41.55 | 67.55 | 73.62 | 66.69 | 64.12 |'
- en: '| DuQuant+LWC | 77.64 | 57.32 | 41.21 | 66.79 | 74.12 | 65.98 | 63.84 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 77.64 | 57.32 | 41.21 | 66.79 | 74.12 | 65.98 | 63.84 |'
- en: '| LLaMA1-30B W4A4 | FP16 | 80.08 | 58.92 | 45.47 | 68.44 | 79.21 | 72.53 |
    67.44 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-30B W4A4 | FP16 | 80.08 | 58.92 | 45.47 | 68.44 | 79.21 | 72.53 |
    67.44 |'
- en: '| SmoothQuant | 58.65 | 35.53 | 27.73 | 60.42 | 35.56 | 48.06 | 44.83 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 58.65 | 35.53 | 27.73 | 60.42 | 35.56 | 48.06 | 44.83 |'
- en: '| OS+ | 67.63 | 46.17 | 34.40 | 60.70 | 54.32 | 52.64 | 52.62 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | 67.63 | 46.17 | 34.40 | 60.70 | 54.32 | 52.64 | 52.62 |'
- en: '| OmniQuant | 71.21 | 49.45 | 34.47 | 65.33 | 64.65 | 59.19 | 56.63 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 71.21 | 49.45 | 34.47 | 65.33 | 64.65 | 59.19 | 56.63 |'
- en: '| AffineQuant | 70.84 | 49.41 | 37.12 | 70.12 | 65.53 | 58.64 | 58.61 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 70.84 | 49.41 | 37.12 | 70.12 | 65.53 | 58.64 | 58.61 |'
- en: '| QLLM | 73.83 | 50.67 | 38.40 | - | 67.91 | 58.56 | 57.87 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 73.83 | 50.67 | 38.40 | - | 67.91 | 58.56 | 57.87 |'
- en: '| Atom | 71.98 | 49.07 | 40.02 | 66.85 | 70.45 | 58.64 | 59.50 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Atom | 71.98 | 49.07 | 40.02 | 66.85 | 70.45 | 58.64 | 59.50 |'
- en: '| DuQuant | 78.56 | 56.99 | 42.32 | 66.73 | 76.70 | 69.61 | 65.15 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 78.56 | 56.99 | 42.32 | 66.73 | 76.70 | 69.61 | 65.15 |'
- en: '| DuQuant+LWC | 78.73 | 56.52 | 43.17 | 68.84 | 77.53 | 70.96 | 65.96 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 78.73 | 56.52 | 43.17 | 68.84 | 77.53 | 70.96 | 65.96 |'
- en: '| LLaMA1-65B W4A4 | FP16 | 80.79 | 58.71 | 46.24 | 82.29 | 80.72 | 77.50 |
    71.04 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-65B W4A4 | FP16 | 80.79 | 58.71 | 46.24 | 82.29 | 80.72 | 77.50 |
    71.04 |'
- en: '| SmoothQuant | 64.47 | 40.44 | 29.82 | 59.38 | 39.90 | 52.24 | 47.71 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 64.47 | 40.44 | 29.82 | 59.38 | 39.90 | 52.24 | 47.71 |'
- en: '| OS+ | 68.06 | 43.98 | 35.32 | 62.75 | 50.73 | 54.30 | 52.52 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | 68.06 | 43.98 | 35.32 | 62.75 | 50.73 | 54.30 | 52.52 |'
- en: '| OmniQuant | 71.81 | 48.02 | 35.92 | 73.27 | 66.81 | 59.51 | 59.22 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 71.81 | 48.02 | 35.92 | 73.27 | 66.81 | 59.51 | 59.22 |'
- en: '| QLLM | 73.56 | 52.06 | 39.68 | - | 70.94 | 62.90 | 59.83 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 73.56 | 52.06 | 39.68 | - | 70.94 | 62.90 | 59.83 |'
- en: '| Atom | 74.48 | 51.60 | 40.61 | 73.76 | 73.78 | 62.12 | 62.73 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Atom | 74.48 | 51.60 | 40.61 | 73.76 | 73.78 | 62.12 | 62.73 |'
- en: '| DuQuant | 79.71 | 57.95 | 45.05 | 79.82 | 78.66 | 72.29 | 68.91 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 79.71 | 57.95 | 45.05 | 79.82 | 78.66 | 72.29 | 68.91 |'
- en: '|  | DuQuant+LWC | 79.98 | 58.29 | 44.80 | 77.89 | 79.22 | 72.21 | 68.73 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | DuQuant+LWC | 79.98 | 58.29 | 44.80 | 77.89 | 79.22 | 72.21 | 68.73 |'
- en: 4.1 Main Results
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 主要结果
- en: Quantization of LLaMA1 and LLaMA2 Models.
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化LLaMA1和LLaMA2模型。
- en: We conduct a comprehensive comparison of our DuQuant with several SOTA baselines
    on LLaMA1 and LLaMA2 models. Results for W4A4 quantization are presented in this
    Section, while results for W6A6 quantization are provided in Appendix [D](#A4
    "Appendix D More Empirical Results ‣ Rotation and Permutation for Advanced Outlier
    Management and Efficient Quantization of LLMs"). Table [1](#S4.T1 "Table 1 ‣ Baselines.
    ‣ 4 Experiment ‣ Rotation and Permutation for Advanced Outlier Management and
    Efficient Quantization of LLMs") indicates that our DuQuant quantized models notably
    outperform other baselines on both the WikiText2 and C4 datasets. Notably, LWC
    further enhances model capacity, with our DuQuant+LWC achieving comparable performance
    with FP16 models. Table [2](#S4.T2 "Table 2 ‣ Baselines. ‣ 4 Experiment ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs") and Table [D1](#A4.T1 "Table D1 ‣ Appendix D More Empirical Results ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs") showcase the zero-shot accuracy of W4A4 quantization on Commonsense QA
    tasks, where DuQuant significantly improves the average accuracy. Our method surpasses
    QLLM by +9%, and Atom by +5% for all model sizes. These results demonstrate the
    superiority of our rotation and permutation transformation, which establishes
    new SOTA performance by effectively eliminating outlier features.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对DuQuant进行了全面比较，涉及多个SOTA基准测试在LLaMA1和LLaMA2模型上的表现。W4A4量化的结果在本节中展示，而W6A6量化的结果则提供在附录[D](#A4
    "附录 D 更多实证结果 ‣ 高级离群点管理和高效量化LLMs的旋转与置换")中。表[1](#S4.T1 "表1 ‣ 基准测试 ‣ 4 实验 ‣ 高级离群点管理和高效量化LLMs的旋转与置换")表明，我们的DuQuant量化模型在WikiText2和C4数据集上显著优于其他基准。特别是，LWC进一步提升了模型能力，使得我们的DuQuant+LWC达到了与FP16模型相当的性能。表[2](#S4.T2
    "表2 ‣ 基准测试 ‣ 4 实验 ‣ 高级离群点管理和高效量化LLMs的旋转与置换")和表[D1](#A4.T1 "表D1 ‣ 附录 D 更多实证结果 ‣
    高级离群点管理和高效量化LLMs的旋转与置换")展示了W4A4量化在Commonsense QA任务上的零-shot准确率，其中DuQuant显著提升了平均准确率。我们的方法在所有模型尺寸上超越了QLLM，提升幅度为+9%，Atom提升幅度为+5%。这些结果展示了我们旋转和置换变换的优越性，通过有效消除离群点特征，建立了新的SOTA性能。
- en: 'Table 3: Zero-shot and five-shot results on the MMLU benchmark for Vicuna-v1.5-13B
    under 4-bit weight-activation quantization. The results for Vicuna-v1.5-7b can
    be found in Table [D2](#A4.T2 "Table D2 ‣ MMLU Results for 4-bit Vicuna-v1.5-7B.
    ‣ Appendix D More Empirical Results ‣ Rotation and Permutation for Advanced Outlier
    Management and Efficient Quantization of LLMs").'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在 MMLU 基准上，Vicuna-v1.5-13B 在 4-bit 权重-激活量化下的零-shot 和五-shot 结果。Vicuna-v1.5-7b
    的结果可以在表 [D2](#A4.T2 "表 D2 ‣ 4-bit Vicuna-v1.5-7B 的 MMLU 结果 ‣ 附录 D 更多经验结果 ‣ 高级异常值管理和
    LLM 的高效量化") 中找到。
- en: '| Model | Method | MMLU (0 shot) $\uparrow$ |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | MMLU (0 shot) $\uparrow$ |'
- en: '| STEM | Hums | Social | Others | Avg. | STEM | Hums | Social | Others | Avg.
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| STEM | Hums | Social | Others | Avg. | STEM | Hums | Social | Others | Avg.
    |'
- en: '| Vicuna-v1.5-13B W4A4 | FP16 | 43.70 | 50.48 | 62.72 | 62.74 | 54.54 | 44.96
    | 51.97 | 65.26 | 62.40 | 55.78 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-v1.5-13B W4A4 | FP16 | 43.70 | 50.48 | 62.72 | 62.74 | 54.54 | 44.96
    | 51.97 | 65.26 | 62.40 | 55.78 |'
- en: '| SmoothQuant | 21.70 | 24.29 | 22.13 | 23.16 | 22.82 | 25.31 | 24.97 | 26.00
    | 27.08 | 25.84 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 21.70 | 24.29 | 22.13 | 23.16 | 22.82 | 25.31 | 24.97 | 26.00
    | 27.08 | 25.84 |'
- en: '| OmniQuant | 26.81 | 26.57 | 30.35 | 28.75 | 28.12 | 28.79 | 27.29 | 31.13
    | 28.99 | 29.05 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 26.81 | 26.57 | 30.35 | 28.75 | 28.12 | 28.79 | 27.29 | 31.13
    | 28.99 | 29.05 |'
- en: '| Atom | 32.54 | 39.60 | 46.02 | 46.11 | 41.07 | 35.35 | 39.21 | 59.72 | 45.77
    | 45.01 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Atom | 32.54 | 39.60 | 46.02 | 46.11 | 41.07 | 35.35 | 39.21 | 59.72 | 45.77
    | 45.01 |'
- en: '| DuQuant | 40.82 | 46.61 | 58.73 | 57.59 | 50.94 | 40.92 | 48.78 | 60.42 |
    57.71 | 51.96 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 40.82 | 46.61 | 58.73 | 57.59 | 50.94 | 40.92 | 48.78 | 60.42 |
    57.71 | 51.96 |'
- en: '|  | DuQuant+LWC | 40.13 | 47.48 | 58.86 | 57.83 | 51.08 | 41..42 | 48.52 |
    58.73 | 57.74 | 51.61 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | DuQuant+LWC | 40.13 | 47.48 | 58.86 | 57.83 | 51.08 | 41..42 | 48.52 |
    58.73 | 57.74 | 51.61 |'
- en: 'Table 4: Perplexity and QA results of LLaMA3-8B under 4-bit/6-bit weight-activation
    quantization.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：LLaMA3-8B 在 4-bit/6-bit 权重-激活量化下的困惑度和 QA 结果。
- en: '| #Bits | Method | WikiText2 $\downarrow$ |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| #Bits | 方法 | WikiText2 $\downarrow$ |'
- en: '| FP16 | - | 6.14 | 8.88 | 9.91 | 80.85 | 77.78 | 53.41 | 81.28 | 79.16 | 72.84
    | 74.22 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 6.14 | 8.88 | 9.91 | 80.85 | 77.78 | 53.41 | 81.28 | 79.16 | 72.84
    | 74.22 |'
- en: '| LLaMA3-8B W6A6 | SmoothQuant | 7.07 | 9.57 | 11.69 | 78.94 | 75.88 | 49.49
    | 77.58 | 77.39 | 70.8 | 71.68 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-8B W6A6 | SmoothQuant | 7.07 | 9.57 | 11.69 | 78.94 | 75.88 | 49.49
    | 77.58 | 77.39 | 70.8 | 71.68 |'
- en: '| OmniQuant | 7.24 | 9.82 | 11.90 | 78.90 | 73.95 | 47.35 | 74.95 | 76.77 |
    70.56 | 70.41 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 7.24 | 9.82 | 11.90 | 78.90 | 73.95 | 47.35 | 74.95 | 76.77 |
    70.56 | 70.41 |'
- en: '| AffineQuant | 7.35 | 9.99 | 12.30 | 78.73 | 73.32 | 46.08 | 74.59 | 77.08
    | 70.88 | 70.11 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 7.35 | 9.99 | 12.30 | 78.73 | 73.32 | 46.08 | 74.59 | 77.08
    | 70.88 | 70.11 |'
- en: '| DuQuant | 6.27 | 8.38 | 10.77 | 80.20 | 77.27 | 52.05 | 80.12 | 79.14 | 72.77
    | 73.59 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 6.27 | 8.38 | 10.77 | 80.20 | 77.27 | 52.05 | 80.12 | 79.14 | 72.77
    | 73.59 |'
- en: '| DuQuant+LWC | 6.27 | 8.38 | 10.78 | 79.71 | 77.57 | 53.07 | 80.00 | 78.70
    | 73.09 | 73.69 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 6.27 | 8.38 | 10.78 | 79.71 | 77.57 | 53.07 | 80.00 | 78.70
    | 73.09 | 73.69 |'
- en: '| LLaMA3-8B W4A4 | SmoothQuant | 210.19 | 187.93 | 278.02 | 54.57 | 31.9 |
    24.23 | 52.72 | 31.26 | 51.14 | 40.97 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-8B W4A4 | SmoothQuant | 210.19 | 187.93 | 278.02 | 54.57 | 31.9 |
    24.23 | 52.72 | 31.26 | 51.14 | 40.97 |'
- en: '| OmniQuant | 3.64e3 | 2.80e3 | 3.09e3 | 50.22 | 26.94 | 24.57 | 37.98 | 26.55
    | 50.20 | 36.08 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 3.64e3 | 2.80e3 | 3.09e3 | 50.22 | 26.94 | 24.57 | 37.98 | 26.55
    | 50.20 | 36.08 |'
- en: '| AffineQuant | 21.21e3 | 34.60e3 | 16.72e3 | 50.71 | 25.93 | 26.02 | 40.55
    | 26.07 | 48.46 | 36.29 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 21.21e3 | 34.60e3 | 16.72e3 | 50.71 | 25.93 | 26.02 | 40.55
    | 26.07 | 48.46 | 36.29 |'
- en: '| Atom | 22.14 | 31.83 | 40.04 | 62.95 | 49.45 | 30.12 | 60.31 | 53.75 | 56.04
    | 52.10 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Atom | 22.14 | 31.83 | 40.04 | 62.95 | 49.45 | 30.12 | 60.31 | 53.75 | 56.04
    | 52.10 |'
- en: '| DuQuant | 8.56 | 11.98 | 13.66 | 75.68 | 68.48 | 41.81 | 71.99 | 73.07 |
    66.22 | 66.21 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 8.56 | 11.98 | 13.66 | 75.68 | 68.48 | 41.81 | 71.99 | 73.07 |
    66.22 | 66.21 |'
- en: '| DuQuant+LWC | 8.06 | 11.29 | 13.19 | 76.22 | 70.41 | 43.69 | 74.34 | 73.87
    | 67.80 | 67.72 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 8.06 | 11.29 | 13.19 | 76.22 | 70.41 | 43.69 | 74.34 | 73.87
    | 67.80 | 67.72 |'
- en: Quantization of Instruction-tuned Models.
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指令调优模型的量化。
- en: We quantize Vicuna-v1.5 [[9](#bib.bib9)] models to assess the generalizability
    of our DuQuant. Table [3](#S4.T3 "Table 3 ‣ Quantization of LLaMA1 and LLaMA2
    Models. ‣ 4.1 Main Results ‣ 4 Experiment ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs") illustrates that our quantized
    models surpass the baselines across all task categories on MMLU benchmark. For
    Vicuna-13B, our DuQuant+LWC surpasses Atom by 10.01% under zero-shot settings
    and 6.95% under five-shot settings. Moreover, we compare our DuQuant with Atom
    and OmniQuant using MT-Bench and utilize GPT-4 to evaluate the answers from quantized
    models. As shown in Figure [3](#S4.F3 "Figure 3 ‣ Table 6 ‣ Influence of Normal/Massive
    Outliers. ‣ 4.2 Ablation Study ‣ 4 Experiment ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs"), DuQuant quantized models
    significantly outperform both Atom and OmniQuant in win rates. Specifically, for
    Vicuna-7B, DuQuant only lost 16 and 1 times to Atom and OmniQuant, respectively,
    while achieving 68 and 155 wins against them.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们量化了 Vicuna-v1.5 [[9](#bib.bib9)] 模型，以评估 DuQuant 的泛化能力。表 [3](#S4.T3 "表 3 ‣
    LLaMA1 和 LLaMA2 模型的量化 ‣ 4.1 主要结果 ‣ 4 实验 ‣ 先进异常值管理与 LLM 高效量化的旋转与置换") 显示，我们的量化模型在
    MMLU 基准测试的所有任务类别中均超越了基准线。对于 Vicuna-13B，我们的 DuQuant+LWC 在零样本设置下比 Atom 高出 10.01%，在五样本设置下高出
    6.95%。此外，我们使用 MT-Bench 比较了 DuQuant 与 Atom 和 OmniQuant，并利用 GPT-4 评估量化模型的答案。如图 [3](#S4.F3
    "图 3 ‣ 表 6 ‣ 正常/大规模异常值的影响 ‣ 4.2 消融研究 ‣ 4 实验 ‣ 先进异常值管理与 LLM 高效量化的旋转与置换") 所示，DuQuant
    量化模型在胜率上显著超越了 Atom 和 OmniQuant。具体而言，对于 Vicuna-7B，DuQuant 仅输给 Atom 和 OmniQuant
    16 次和 1 次，而对它们赢得了 68 次和 155 次。
- en: Quantization of LLaMA3 Models.
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLaMA3 模型的量化。
- en: LLaMA3, known for its superior performance in various tasks, faces significant
    degradation in low-bit quantization [[24](#bib.bib24)]. To address this, we apply
    our DuQuant to quantize LLaMA3-8B. Table [4](#S4.T4 "Table 4 ‣ Quantization of
    LLaMA1 and LLaMA2 Models. ‣ 4.1 Main Results ‣ 4 Experiment ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs") displays
    the perplexity and zero-shot accuracy results. Notably, under W6A6 setting, our
    DuQuant achieves performance comparable to FP16 model. Furthermore, unlike other
    methods that show weaker results under W4A4 setting, our DuQuant maintains competitive
    performance, indicating its robustness with LLaMA3. We attribute this success
    to the advanced handling of outliers achieved through dual transformations, which
    is not restricted to specific models.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA3 以其在各种任务中的优异性能而闻名，但在低位量化下面临显著退化 [[24](#bib.bib24)]。为了解决这一问题，我们将 DuQuant
    应用于 LLaMA3-8B 的量化。表 [4](#S4.T4 "表 4 ‣ LLaMA1 和 LLaMA2 模型的量化 ‣ 4.1 主要结果 ‣ 4 实验
    ‣ 先进异常值管理与 LLM 高效量化的旋转与置换") 显示了困惑度和零样本准确率结果。值得注意的是，在 W6A6 设置下，我们的 DuQuant 实现了与
    FP16 模型相当的性能。此外，与在 W4A4 设置下结果较弱的其他方法不同，我们的 DuQuant 在 LLaMA3 上保持了具有竞争力的性能，这表明其对
    LLaMA3 的鲁棒性。我们将这一成功归因于通过双重变换实现的先进异常值处理，这种方法不局限于特定模型。
- en: 4.2 Ablation Study
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 消融研究
- en: Module-wise Impact.
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模块影响。
- en: 'Our DuQuant includes three main components: smoothing operation, initial rotation,
    and permutation with a second rotation. We combine these components to quantize
    the LLaMA2-13B model and evaluate their effects on language generation tasks.
    Results in Table [5](#S4.T5 "Table 5 ‣ Module-wise Impact. ‣ 4.2 Ablation Study
    ‣ 4 Experiment ‣ Rotation and Permutation for Advanced Outlier Management and
    Efficient Quantization of LLMs") show that the smoothing operation plays a basic
    role in our DuQuant by shifting activation outliers to weight. The initial rotation
    significantly enhances model performance, yielding competitive PPL results. Finally,
    permutation combined with a second rotation further enhances the quantized model.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 DuQuant 包括三个主要组件：平滑操作、初始旋转和带有第二次旋转的置换。我们结合这些组件来量化 LLaMA2-13B 模型，并评估它们对语言生成任务的影响。表
    [5](#S4.T5 "表 5 ‣ 模块影响 ‣ 4.2 消融研究 ‣ 4 实验 ‣ 先进异常值管理与 LLM 高效量化的旋转与置换") 显示，平滑操作通过将激活异常值移至权重，发挥了
    DuQuant 的基础作用。初始旋转显著提高了模型性能，取得了具有竞争力的 PPL 结果。最后，结合第二次旋转的置换进一步增强了量化模型。
- en: 'Table 5: Influence of different components in DuQuant under 4-bit weight-activation
    quantization.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 在 4 位权重-激活量化下 DuQuant 各组件的影响。'
- en: '| Modules | LLaMA2-7B | LLaMA2-13B |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 模块 | LLaMA2-7B | LLaMA2-13B |'
- en: '| Smooth | Rotation 1 | Permutation | Rotation 2 | WikiText2 $\downarrow$ |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 平滑 | 旋转 1 | 置换 | 旋转 2 | WikiText2 $\downarrow$ |'
- en: '| $\checkmark$ |  |  |  | NaN | 1379.46 | 160.30 | 203.87 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| $\checkmark$ |  |  |  | NaN | 1379.46 | 160.30 | 203.87 |'
- en: '|  | $\checkmark$ |  |  | 8.48 | 10.63 | 14.32 | 21.73 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $\checkmark$ |  |  | 8.48 | 10.63 | 14.32 | 21.73 |'
- en: '| $\checkmark$ |  |  | 7.92 | 10.64 | 5.96 | 7.94 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| $\checkmark$ |  |  | 7.92 | 10.64 | 5.96 | 7.94 |'
- en: '|  | $\checkmark$ | 6.79 | 8.51 | 6.06 | 8.03 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $\checkmark$ | 6.79 | 8.51 | 6.06 | 8.03 |'
- en: '| $\checkmark$ | 6.28 | 7.90 | 5.42 | 7.05 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| $\checkmark$ | 6.28 | 7.90 | 5.42 | 7.05 |'
- en: Influence of Normal/Massive Outliers.
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 正常/大规模离群值的影响。
- en: 'In this section, we comprehensively explore the influence of massive and normal
    outliers on quantization. Notably, we observe that massive outliers primarily
    occur at the down-projection of the FFN module. To isolate their effect, we remove
    the rotation and permutation transformations, applying only the smoothing technique
    to all down-projection inputs. The resulting perplexity for LLaMA2-7B and LLaMA-13B
    showed significant degradation, presented in Table [6](#S4.T6 "Table 6 ‣ Influence
    of Normal/Massive Outliers. ‣ 4.2 Ablation Study ‣ 4 Experiment ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs").
    Conversely, when we eliminate the rotation and permutation transformations for
    normal outliers, the performance decrease was noticeable but less severe compared
    to massive outliers. These findings indicate that: 1) massive outliers exert a
    more substantial impact on quantization, corroborating our claims in Section [2](#S2
    "2 Motivation ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs"); 2) the smoothing technique alone struggles to fully mitigate
    the influence of outliers, particularly massive ones; and 3) our rotation and
    permutation methods prove highly effective against both types of outliers, leading
    to superior performance.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们全面探讨了大规模和正常离群值对量化的影响。值得注意的是，我们观察到大规模离群值主要发生在 FFN 模块的下投影部分。为了隔离它们的影响，我们移除了旋转和置换变换，仅对所有下投影输入应用平滑技术。结果显示，LLaMA2-7B
    和 LLaMA2-13B 的困惑度显著下降，见表[6](#S4.T6 "表 6 ‣ 正常/大规模离群值的影响。 ‣ 4.2 消融研究 ‣ 4 实验 ‣ 先进离群值管理和
    LLM 高效量化的旋转与置换")。相反，当我们对正常离群值消除旋转和置换变换时，性能下降明显但不如大规模离群值严重。这些发现表明：1）大规模离群值对量化的影响更为显著，这证实了我们在第[2](#S2
    "2 动机 ‣ 旋转与置换用于先进离群值管理和 LLM 高效量化")节中的观点；2）仅使用平滑技术难以完全减轻离群值的影响，尤其是大规模离群值；3）我们的旋转和置换方法对两种离群值均非常有效，带来了更优的性能。
- en: 'Table 6: Outliers impact on quantization. We only apply the smooth technique
    on Normal and Massive outliers for W4A4 quantization.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：离群值对量化的影响。我们仅对 W4A4 量化中的正常和大规模离群值应用平滑技术。
- en: '| Outlier Type | LLaMA2-7B | LLaMA2-13B |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 离群值类型 | LLaMA2-7B | LLaMA2-13B |'
- en: '| Normal | Massive | WikiText2 $\downarrow$ |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 正常 | 大规模 | WikiText2 $\downarrow$ |'
- en: '|  | $\checkmark$ | 18.16 | 26.42 | 10.51 | 16.01 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $\checkmark$ | 18.16 | 26.42 | 10.51 | 16.01 |'
- en: '| $\checkmark$ |  | 10.88 | 13.89 | 7.87 | 10.52 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| $\checkmark$ |  | 10.88 | 13.89 | 7.87 | 10.52 |'
- en: '| $\checkmark$ | 6.28 | 7.90 | 5.42 | 7.05 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| $\checkmark$ | 6.28 | 7.90 | 5.42 | 7.05 |'
- en: 'Figure 3: GPT-4 evaluation on the MT-Bench.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：GPT-4 在 MT-Bench 上的评估。
- en: '![Refer to caption](img/e6dff483fd70ab5a37f6eb194461da70.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e6dff483fd70ab5a37f6eb194461da70.png)'
- en: Comparison with QuaRot [[2](#bib.bib2)]
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与 QuaRot 的比较[[2](#bib.bib2)]
- en: In light of the recent introduction of Hadamard rotations by QuaRot[[2](#bib.bib2)]
    to eliminate outlier features, we have undertaken a detailed analysis to highlight
    the key differences between our DuQuant and QuaRot. To ensure a balanced evaluation,
    we have re-implemented QuaRot in accordance with our quantization settings. The
    results demonstrate that 1) the rotation matrix constructed by DuQuant outperforms
    QuaRot’s approach of simply selecting a randomly initialized Hadamard matrix.
    As depicted in Figure [7](#S4.T7 "Table 7 ‣ Comparison with QuaRot [2] ‣ 4.2 Ablation
    Study ‣ 4 Experiment ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs"), our DuQuant more effectively smooths activations
    than QuaRot; 2) As demonstrated by the perplexity in Table [7](#S4.T7 "Table 7
    ‣ Comparison with QuaRot [2] ‣ 4.2 Ablation Study ‣ 4 Experiment ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs"),
    QuaRot employs GPTQ for their weight quantization method, whereas our DuQuant,
    with its sophisticated outlier management, attains competitive results using RTN
    quantization. Additionally, the inclusion of the LWC component within DuQuant significantly
    enhances the performance of our quantized model. For a more comprehensive comparison,
    please refer to Appendix [F](#A6 "Appendix F Detailed Comparison with QuaRot ‣
    Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs").
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 QuaRot[[2](#bib.bib2)] 最近引入了 Hadamard 旋转以消除异常值特征，我们进行了详细分析，以突显我们的 DuQuant
    和 QuaRot 之间的关键差异。为了确保平衡的评估，我们根据我们的量化设置重新实现了 QuaRot。结果表明：1) DuQuant 构建的旋转矩阵优于 QuaRot
    仅选择随机初始化的 Hadamard 矩阵的方法。如图 [7](#S4.T7 "表 7 ‣ 与 QuaRot 的比较 [2] ‣ 4.2 消融研究 ‣ 4
    实验 ‣ 旋转和置换用于高级异常值管理和 LLM 的高效量化") 所示，我们的 DuQuant 比 QuaRot 更有效地平滑激活；2) 如表 [7](#S4.T7
    "表 7 ‣ 与 QuaRot 的比较 [2] ‣ 4.2 消融研究 ‣ 4 实验 ‣ 旋转和置换用于高级异常值管理和 LLM 的高效量化") 中的困惑度所示，QuaRot
    采用 GPTQ 进行权重量化方法，而我们具有复杂异常值管理的 DuQuant 通过 RTN 量化取得了具有竞争力的结果。此外，DuQuant 中的 LWC
    组件显著提升了我们量化模型的性能。有关更全面的比较，请参见附录 [F](#A6 "附录 F 与 QuaRot 的详细比较 ‣ 旋转和置换用于高级异常值管理和
    LLM 的高效量化")。
- en: 'Table 7: PPL ($\downarrow$) comparison under W4A4 setting.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：W4A4 设置下的 PPL（$\downarrow$）比较。
- en: '| Method | 1-7B | 1-13B | 1-30B | 2-7B | 2-13B |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 1-7B | 1-13B | 1-30B | 2-7B | 2-13B |'
- en: '| FP16 | 5.68 | 5.09 | 4.10 | 5.47 | 4.88 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 5.68 | 5.09 | 4.10 | 5.47 | 4.88 |'
- en: '| QuaRot-RTN | 7.08 | 6.57 | 5.44 | 9.66 | 6.73 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| QuaRot-RTN | 7.08 | 6.57 | 5.44 | 9.66 | 6.73 |'
- en: '| QuaRot-GPTQ | 6.44 | 5.63 | 4.73 | 6.39 | 5.75 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| QuaRot-GPTQ | 6.44 | 5.63 | 4.73 | 6.39 | 5.75 |'
- en: '| DuQuant | 6.40 | 5.65 | 4.72 | 6.28 | 5.42 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 6.40 | 5.65 | 4.72 | 6.28 | 5.42 |'
- en: '| DuQuant+LWC | 6.18 | 5.47 | 4.55 | 6.08 | 5.33 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 6.18 | 5.47 | 4.55 | 6.08 | 5.33 |'
- en: 'Figure 4: LLaMA2-7B Attention key_proj.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：LLaMA2-7B 注意力 key_proj.
- en: '![Refer to caption](img/cd69fa930d8a85eba39b395d49e31fd8.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/cd69fa930d8a85eba39b395d49e31fd8.png)'
- en: 'Table 8: Quantization runtime on one NVIDIA A100.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：在一块 NVIDIA A100 上的量化运行时间。
- en: Model Omni. Affine. QLLM Atom DuQuant LLaMA2-7B 2.0h 9.1h 1.1h 20min 50s LLaMA2-13B
    3.2h 16.0h 1.7h 36min 71s LLaMA2-70B 14.6h 18.6h 9.3h 3.5h 270s
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 Omni. Affine. QLLM Atom DuQuant LLaMA2-7B 2.0h 9.1h 1.1h 20min 50s LLaMA2-13B
    3.2h 16.0h 1.7h 36min 71s LLaMA2-70B 14.6h 18.6h 9.3h 3.5h 270s
- en: Runtime.
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 运行时间。
- en: Our DuQuant stands out for its efficiency, surpassing other baselines [[42](#bib.bib42),
    [31](#bib.bib31), [36](#bib.bib36), [61](#bib.bib61)]. The block-wise rotation
    ensures fast multiplication between the rotation and activation matrices. Zigzag
    permutation, involving simple channel swaps, is much faster than complex algorithms
    like Simulated Annealing, as discussed in Appendix  [E.3](#A5.SS3 "E.3 Effects
    of Permutation Algorithm. ‣ Appendix E More Ablation Studies ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"). Moreover,
    the advanced management of outliers makes DuQuant not rely on GPTQ or gradient-based
    training. Hence, DuQuant enables a rapid quantization process shown in Table [F17](#A6.T17
    "Table F17 ‣ Appendix F Detailed Comparison with QuaRot ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"), e.g., we
    successfully quantize LLaMA2-13B in just 71s with superior performance.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 DuQuant 以其高效性脱颖而出，超越了其他基线[[42](#bib.bib42), [31](#bib.bib31), [36](#bib.bib36),
    [61](#bib.bib61)]。块级旋转确保了旋转矩阵与激活矩阵之间的快速乘法。锯齿形排列，涉及简单的通道交换，比像模拟退火这样的复杂算法要快得多，如附录
    [E.3](#A5.SS3 "E.3 Permutation Algorithm 的效果。 ‣ 附录 E 更多消融研究 ‣ 旋转和排列用于高级异常值管理和高效量化
    LLMs") 所讨论的。此外，异常值的先进管理使得 DuQuant 不依赖于 GPTQ 或基于梯度的训练。因此，DuQuant 实现了如表 [F17](#A6.T17
    "表 F17 ‣ 附录 F 与 QuaRot 的详细比较 ‣ 旋转和排列用于高级异常值管理和高效量化 LLMs") 中所示的快速量化过程，例如，我们成功地在
    71 秒内对 LLaMA2-13B 进行了量化，并取得了优异的性能。
- en: Inference Speedup.
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理加速。
- en: To assess the inference speedup delivered by our DuQuant, we adopt the measurement
    strategy and W4A4 kernel from [[2](#bib.bib2)]. We evaluate the layer-wise speedup
    of LLaMA2-7B and LLaMA2-13B on NVIDIA RTX 3090 GPUs, with results detailed in
    Table [9](#S4.T9 "Table 9 ‣ Inference Speedup. ‣ 4.2 Ablation Study ‣ 4 Experiment
    ‣ Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs"). We can observe that during the prefill phase, DuQuant achieves a $2.08\times$
    speedup and the impressive performance demonstrated by DuQuant  these additional
    costs are deemed acceptable. Further permutations do not enhance performance and
    can negatively impact inference efficiency. More detailed results on inference
    speedup are available in Appendix [E.1](#A5.SS1 "E.1 End-to-end Time Speedup and
    Memory Saving ‣ Appendix E More Ablation Studies ‣ Rotation and Permutation for
    Advanced Outlier Management and Efficient Quantization of LLMs").
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的 DuQuant 提供的推理加速，我们采用了 [[2](#bib.bib2)] 中的测量策略和 W4A4 核心。我们在 NVIDIA RTX
    3090 GPU 上评估了 LLaMA2-7B 和 LLaMA2-13B 的层级加速，结果详见表 [9](#S4.T9 "表 9 ‣ 推理加速。 ‣ 4.2
    消融研究 ‣ 4 实验 ‣ 旋转和排列用于高级异常值管理和高效量化 LLMs")。我们可以观察到，在预填充阶段，DuQuant 实现了 $2.08\times$
    的加速，并且 DuQuant 展示的卓越性能使得这些额外的成本是可以接受的。进一步的排列不会提升性能，反而可能对推理效率产生负面影响。关于推理加速的更多详细结果，请参见附录
    [E.1](#A5.SS1 "E.1 端到端时间加速和内存节省 ‣ 附录 E 更多消融研究 ‣ 旋转和排列用于高级异常值管理和高效量化 LLMs")。
- en: 'Table 9: Layer-wise speedup during prefilling phase for 4-bit weight-activation
    quantization.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：4 位权重-激活量化的预填充阶段的层级加速。
- en: '| Model | Batch Size | Speedup |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 批量大小 | 加速比 |'
- en: '| LLaMA2-7B | 1 | $1.95\times$ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | 1 | $1.95\times$ |'
- en: '| 4 | $2.03\times$ |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 4 | $2.03\times$ |'
- en: '| 16 | $2.08\times$ |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 16 | $2.08\times$ |'
- en: '| LLaMA2-13B | 1 | $2.15\times$ |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B | 1 | $2.15\times$ |'
- en: '| 4 | $2.30\times$ |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 4 | $2.30\times$ |'
- en: '| 16 | $2.34\times$ |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 16 | $2.34\times$ |'
- en: 'Figure 5: Computational overhead analysis.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：计算开销分析。
- en: '![Refer to caption](img/447eba1a713693348507e9c75af0057e.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/447eba1a713693348507e9c75af0057e.png)'
- en: 5 Conclusion
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In conclusion, this paper presents DuQuant, an innovative quantization strategy
    for large language models (LLMs) that effectively addresses the challenge of outlier
    activations. By integrating rotation and permutation transformations, DuQuant effectively
    mitigates the impacts of both massive and normal outliers. This strategic redistribution
    of outliers not only simplifies the quantization process but also leads to substantial
    improvements in model performance. Consequently, DuQuant establishes new state-of-the-art
    results in 4-bit weight-activation quantization scenarios. This advancement enhances
    the deployment of efficient LLMs in resource-constrained environments.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本文提出了DuQuant，一种创新的量化策略，旨在有效解决大语言模型（LLMs）中的离群激活问题。通过整合旋转和置换变换，DuQuant 有效缓解了大量和正常离群值的影响。这种离群值的战略性重分布不仅简化了量化过程，而且显著提高了模型性能。因此，DuQuant 在4位权重-激活量化场景中建立了新的最先进结果。这一进展提升了在资源受限环境中部署高效LLMs的能力。
- en: References
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ashkboos et al. [2023] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan
    Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. Towards end-to-end
    4-bit inference on generative large language models. *arXiv preprint arXiv:2310.09259*,
    2023.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashkboos 等人 [2023] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong,
    Xincheng Wang, Jie Ren, Torsten Hoefler 和 Dan Alistarh. 面向生成大语言模型的端到端4位推断. *arXiv预印本
    arXiv:2310.09259*, 2023。
- en: 'Ashkboos et al. [2024] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L
    Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman.
    Quarot: Outlier-free 4-bit inference in rotated llms. *arXiv preprint arXiv:2404.00456*,
    2024.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ashkboos 等人 [2024] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L Croci,
    Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler 和 James Hensman. Quarot: 旋转LLMs中的无离群4位推断.
    *arXiv预印本 arXiv:2404.00456*, 2024。'
- en: 'Bai et al. [2020] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin
    Jiang, Qun Liu, Michael Lyu, and Irwin King. Binarybert: Pushing the limit of
    bert quantization. *arXiv preprint arXiv:2012.15701*, 2020.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai 等人 [2020] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang,
    Qun Liu, Michael Lyu 和 Irwin King. Binarybert: 推动BERT量化的极限. *arXiv预印本 arXiv:2012.15701*,
    2020。'
- en: Bai et al. [2022] Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, and
    Michael R Lyu. Towards efficient post-training quantization of pre-trained language
    models. *Advances in Neural Information Processing Systems*, 35:1405–1418, 2022.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 [2022] Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King 和 Michael R
    Lyu. 面向高效的预训练语言模型后训练量化. *神经信息处理系统进展*, 35:1405–1418, 2022。
- en: Bengio et al. [2013] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating
    or propagating gradients through stochastic neurons for conditional computation.
    *arXiv preprint arXiv:1308.3432*, 2013.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等人 [2013] Yoshua Bengio, Nicholas Léonard 和 Aaron Courville. 估计或传播通过随机神经元的梯度以进行条件计算.
    *arXiv预印本 arXiv:1308.3432*, 2013。
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439,
    2020.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk 等人 [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi 等人. Piqa:
    在自然语言中推理物理常识. 收录于 *AAAI人工智能会议论文集*, 第34卷, 页码 7432–7439, 2020。'
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等人. 语言模型是少量学习者. *神经信息处理系统进展*, 33:1877–1901, 2020。
- en: 'Chee et al. [2024] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M
    De Sa. Quip: 2-bit quantization of large language models with guarantees. *Advances
    in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chee 等人 [2024] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov 和 Christopher M De Sa.
    Quip: 带有保证的2位量化大语言模型. *神经信息处理系统进展*, 36, 2024。'
- en: 'Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
    et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
    *See https://vicuna. lmsys. org (accessed 14 April 2023)*, 2023.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang 等人 [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu,
    Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez 等人.
    Vicuna: 一款开源聊天机器人，以90%* chatgpt质量令GPT-4印象深刻. *见 https://vicuna. lmsys. org (访问于2023年4月14日)*,
    2023。'
- en: 'Clark et al. [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty
    of natural yes/no questions. In *Proceedings of the 2019 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies*, pages 2924–2936, 2019.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人 [2019] 克里斯托弗·克拉克、肯顿·李、明伟·张、汤姆·克维亚特科夫斯基、迈克尔·柯林斯和克里斯蒂娜·托塔诺瓦。Boolq：探索自然是/否问题的惊人难度。发表于
    *2019年北美计算语言学协会：人类语言技术会议论文集*，第2924–2936页，2019年。
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人 [2018] 彼得·克拉克、艾萨克·科威、奥伦·埃齐奥尼、图沙尔·科特、阿希什·萨布哈尔瓦尔、卡里萨·肖尼克和欧文·塔福约德。认为你已经解决了问答问题？尝试
    ARC，AI2 推理挑战。*arXiv 预印本 arXiv:1803.05457*，2018年。
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm.int8(): 8-bit matrix multiplication for transformers at scale. In *Conference
    on Neural Information Processing Systems*, 2022.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等人 [2022] 蒂姆·德特梅尔斯、迈克·刘易斯、尤尼斯·贝尔卡达和卢克·泽特尔摩耶。Llm.int8()：大规模变换器的8位矩阵乘法。发表于
    *神经信息处理系统会议*，2022年。
- en: 'Dettmers et al. [2024] Tim Dettmers, Ruslan A. Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. SpQR: A sparse-quantized representation for near-lossless LLM
    weight compression. In *The Twelfth International Conference on Learning Representations*,
    2024.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等人 [2024] 蒂姆·德特梅尔斯、鲁斯兰·A·斯维尔谢夫斯基、瓦格·埃吉亚兹里安、德尼斯·库兹涅代夫、埃利亚斯·弗朗塔尔、萨利赫·阿什克布斯、亚历山大·博尔祖诺夫、托尔斯滕·霍夫勒和丹·阿利斯塔赫。SpQR：一种稀疏量化表示用于接近无损的
    LLM 权重压缩。发表于 *第十二届国际学习表示会议*，2024年。
- en: 'Du et al. [2024] Dayou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen
    Chu, and Ningyi Xu. Bitdistiller: Unleashing the potential of sub-4-bit llms via
    self-distillation. *arXiv preprint arXiv:2402.10631*, 2024.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等人 [2024] 杜大有、张怡佳、曹世杰、郭佳琪、曹婷、楚晓文和徐宁毅。Bitdistiller：通过自蒸馏释放亚4位 LLM 的潜力。*arXiv
    预印本 arXiv:2402.10631*，2024年。
- en: 'Duanmu et al. [2024] Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan,
    Xingcheng Zhang, and Dahua Lin. Skvq: Sliding-window key and value cache quantization
    for large language models. *arXiv preprint arXiv:2405.06219*, 2024.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duanmu 等人 [2024] 庄木浩、袁志航、李秀红、段江飞、张兴程和林大华。Skvq：大语言模型的滑动窗口键和值缓存量化。*arXiv 预印本 arXiv:2405.06219*，2024年。
- en: 'Frantar and Alistarh [2023] Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot. In *International Conference
    on Machine Learning*, pages 10323–10337\. PMLR, 2023.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 和 Alistarh [2023] 埃利亚斯·弗朗塔尔和丹·阿利斯塔赫。Sparsegpt：大规模语言模型可以在一次性剪枝中准确剪枝。发表于
    *国际机器学习会议*，第10323–10337页。PMLR，2023年。
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等人 [2022] 埃利亚斯·弗朗塔尔、萨利赫·阿什克布斯、托尔斯滕·霍夫勒和丹·阿利斯塔赫。Gptq：生成预训练变换器的准确后训练量化。*arXiv
    预印本 arXiv:2210.17323*，2022年。
- en: 'Gao et al. [2023] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng,
    Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2:
    Parameter-efficient visual instruction model. *arXiv preprint arXiv:2304.15010*,
    2023.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 [2023] 彭高、贾铭·韩、任瑞·张、子怡·林、世杰·耿、翟俊·周、韦张、潘璐、从辉·何、向宇·岳等人。Llama-adapter v2：参数高效的视觉指令模型。*arXiv
    预印本 arXiv:2304.15010*，2023年。
- en: 'Han et al. [2015] Song Han, Huizi Mao, and William J Dally. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. *arXiv preprint arXiv:1510.00149*, 2015.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等人 [2015] 宋·韩、慧子·毛和威廉·J·达利。深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。*arXiv 预印本 arXiv:1510.00149*，2015年。
- en: Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. In *International Conference on Learning Representations*, 2020.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人 [2020] 丹·亨德里克斯、科林·伯恩斯、斯蒂文·巴萨特、安迪·周、曼塔斯·马泽卡、道恩·宋和雅各布·斯坦赫特。测量大规模多任务语言理解。发表于
    *国际学习表示会议*，2020年。
- en: Heo et al. [2023] Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim,
    Se Jung Kwon, and Dongsoo Lee. Rethinking channel dimensions to isolate outliers
    for low-bit weight quantization of large language models. *arXiv preprint arXiv:2309.15531*,
    2023.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heo et al. [2023] Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim,
    Se Jung Kwon, and Dongsoo Lee. 重新思考通道维度以隔离异常值用于大型语言模型的低比特权重量化。*arXiv 预印本 arXiv:2309.15531*，2023。
- en: Hou and Kwok [2018] Lu Hou and James T Kwok. Loss-aware weight quantization
    of deep networks. *arXiv preprint arXiv:1802.08635*, 2018.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou and Kwok [2018] Lu Hou and James T Kwok. 深度网络的损失感知权重量化。*arXiv 预印本 arXiv:1802.08635*，2018。
- en: Hou et al. [2016] Lu Hou, Quanming Yao, and James T Kwok. Loss-aware binarization
    of deep networks. *arXiv preprint arXiv:1611.01600*, 2016.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou et al. [2016] Lu Hou, Quanming Yao, and James T Kwok. 深度网络的损失感知二值化。*arXiv
    预印本 arXiv:1611.01600*，2016。
- en: Huang et al. [2024] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao
    Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. How good
    are low-bit quantized llama3 models? an empirical study. *arXiv preprint arXiv:2404.14047*,
    2024.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2024] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao
    Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. 低比特量化的
    llama3 模型表现如何？一项实证研究。*arXiv 预印本 arXiv:2404.14047*，2024。
- en: Jacob et al. [2018] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    pages 2704–2713, 2018.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob et al. [2018] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 为高效整数算术推理的神经网络量化与训练。在
    *IEEE 计算机视觉与模式识别会议论文集*，页码 2704–2713，2018。
- en: 'Kim et al. [2023] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*, 2023.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim et al. [2023] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: 密集与稀疏量化。*arXiv
    预印本 arXiv:2306.07629*，2023。'
- en: 'Li et al. [2024a] Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Norm
    tweaking: High-performance low-bit quantization of large language models. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 38, pages 18536–18544,
    2024a.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2024a] Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Norm
    tweaking: 大型语言模型的高性能低比特量化。在 *AAAI 人工智能会议论文集*，第 38 卷，页码 18536–18544，2024a。'
- en: Li et al. [2024b] Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng
    Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Evaluating quantized
    large language models. *arXiv preprint arXiv:2402.18158*, 2024b.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2024b] Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng
    Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. 量化大型语言模型的评估。*arXiv 预印本
    arXiv:2402.18158*，2024b。
- en: 'Li et al. [2021] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization
    by block reconstruction. *arXiv preprint arXiv:2102.05426*, 2021.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2021] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, and Shi Gu. Brecq: 通过块重建推动后训练量化的极限。*arXiv 预印本 arXiv:2102.05426*，2021。'
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: 激活感知权重量化用于llm压缩与加速。*arXiv 预印本 arXiv:2306.00978*，2023。'
- en: 'Liu et al. [2023a] Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei
    Cai, and Bohan Zhuang. Qllm: Accurate and efficient low-bitwidth quantization
    for large language models. In *The Twelfth International Conference on Learning
    Representations*, 2023a.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023a] Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei
    Cai, and Bohan Zhuang. Qllm: 精确且高效的大比特宽度量化用于大型语言模型。在 *第十二届国际学习表征会议*，2023a。'
- en: 'Liu et al. [2024a] Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao,
    Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: Improving large language
    model quantization by keeping pivot tokens intact. *arXiv preprint arXiv:2403.01241*,
    2024a.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2024a] Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao,
    Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: 通过保持枢轴令牌完整来改进大型语言模型量化。*arXiv
    预印本 arXiv:2403.01241*，2024a。'
- en: 'Liu et al. [2023b] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models. *arXiv
    preprint arXiv:2305.17888*, 2023b.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023b] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra.
    Llm-qat：用于大型语言模型的数据无关量化感知训练。*arXiv 预印本 arXiv:2305.17888*，2023年。
- en: Liu et al. [2021] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and
    Wen Gao. Post-training quantization for vision transformer. *Advances in Neural
    Information Processing Systems*, 34:28092–28103, 2021.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2021] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, 和 Wen
    Gao. 视觉变换器的后训练量化。*神经信息处理系统进展*，34:28092–28103，2021年。
- en: 'Liu et al. [2024b] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo
    Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: A tuning-free asymmetric
    2bit quantization for kv cache. *arXiv preprint arXiv:2402.02750*, 2024b.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2024b] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo
    Xu, Vladimir Braverman, Beidi Chen, 和 Xia Hu. Kivi：一种无调优的非对称 2bit 量化用于 kv 缓存。*arXiv
    预印本 arXiv:2402.02750*，2024年。
- en: 'Ma et al. [2024] Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao,
    Rui Wang, Shilei Wen, Fei Chao, and Rongrong Ji. Affinequant: Affine transformation
    quantization for large language models. *arXiv preprint arXiv:2403.12544*, 2024.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. [2024] Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao,
    Rui Wang, Shilei Wen, Fei Chao, 和 Rongrong Ji. Affinequant：用于大型语言模型的仿射变换量化。*arXiv
    预印本 arXiv:2403.12544*，2024年。
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. In *International Conference on Learning
    Representations*, 2016.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher. 指针哨兵混合模型。在*国际学习表示会议*，2016年。
- en: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning*, pages 7197–7206\.
    PMLR, 2020.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, 和 Tijmen Blankevoort. 向上还是向下？适应性舍入用于后训练量化。在*国际机器学习会议*，第7197–7206页。PMLR，2020年。
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu. 使用统一的文本到文本变换器探索迁移学习的极限。*机器学习研究杂志*，21(1):5485–5551，2020年。
- en: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    和 Yejin Choi. Winogrande：大规模对抗性 Winograd 语法挑战。*ACM 通讯*，64(9):99–106，2021年。
- en: 'Shang et al. [2024] Yuzhang Shang, Zhihang Yuan, and Zhen Dong. PB-LLM: Partially
    binarized large language models. In *The Twelfth International Conference on Learning
    Representations*, 2024.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang et al. [2024] Yuzhang Shang, Zhihang Yuan, 和 Zhen Dong. PB-LLM：部分二值化的大型语言模型。在*第十二届国际学习表示会议*，2024年。
- en: 'Shao et al. [2023] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. In *The Twelfth International
    Conference on Learning Representations*, 2023.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao et al. [2023] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, 和 Ping Luo. Omniquant：用于大型语言模型的全方向标定量化。在*第十二届国际学习表示会议*，2023年。
- en: Sun et al. [2024] Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. Massive
    activations in large language models. *arXiv preprint arXiv:2402.17762*, 2024.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2024] Mingjie Sun, Xinlei Chen, J Zico Kolter, 和 Zhuang Liu. 大型语言模型中的大量激活。*arXiv
    预印本 arXiv:2402.17762*，2024年。
- en: Tao et al. [2022] Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun
    Liu, Ping Luo, and Ngai Wong. Compression of generative pre-trained language models
    via quantization. *arXiv preprint arXiv:2203.10705*, 2022.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tao et al. [2022] Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun
    Liu, Ping Luo, 和 Ngai Wong. 通过量化对生成预训练语言模型进行压缩。*arXiv 预印本 arXiv:2203.10705*，2022年。
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar 等。Llama: 开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023a。'
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等。Llama 2: 开放的基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023b。'
- en: 'Tseng et al. [2024] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov,
    and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence
    and lattice codebooks. *arXiv preprint arXiv:2402.04396*, 2024.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tseng 等人 [2024] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov,
    和 Christopher De Sa。Quip#: 通过 Hadamard 不相干性和格子代码本进一步提升 llm 量化效果。*arXiv 预印本 arXiv:2402.04396*，2024。'
- en: 'Wei et al. [2022] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong,
    Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression:
    Pushing the limit of low-bit transformer language models. In *Conference on Neural
    Information Processing Systems*, 2022.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '韦等人 [2022] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang
    Zhang, Qi Zhang, Fengwei Yu, 和 Xianglong Liu。Outlier suppression: 推动低位数 Transformer
    语言模型的极限。在 *神经信息处理系统会议*，2022。'
- en: 'Wei et al. [2023] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao
    Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization
    of large language models by equivalent and effective shifting and scaling. In
    *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*,
    pages 1648–1665, 2023.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '韦等人 [2023] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong,
    Jinyang Guo, 和 Xianglong Liu。Outlier suppression+: 通过等效和有效的平移与缩放来精确量化大规模语言模型。在
    *2023年自然语言处理实证方法会议论文集*，页码 1648–1665，2023。'
- en: 'Wu et al. [2023] Xiaoxia Wu, Zhewei Yao, and Yuxiong He. Zeroquant-fp: A leap
    forward in llms post-training w4a8 quantization using floating-point formats.
    *arXiv preprint arXiv:2307.09782*, 2023.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '吴等人 [2023] Xiaoxia Wu, Zhewei Yao, 和 Yuxiong He。Zeroquant-fp: 在 llms 后训练 w4a8
    量化中使用浮点格式的重大进展。*arXiv 预印本 arXiv:2307.09782*，2023。'
- en: Xi et al. [2023] Haocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu. Training
    transformers with 4-bit integers. *Advances in Neural Information Processing Systems*,
    36:49146–49168, 2023.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 习等人 [2023] Haocheng Xi, Changhao Li, Jianfei Chen, 和 Jun Zhu。使用 4 位整数训练 Transformers。*神经信息处理系统进展*，36:49146–49168，2023。
- en: 'Xiao et al. [2023a] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien
    Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization
    for large language models. In *International Conference on Machine Learning*,
    pages 38087–38099\. PMLR, 2023a.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等人 [2023a] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    和 Song Han。Smoothquant: 精确且高效的大规模语言模型后训练量化。在 *国际机器学习会议*，页码 38087–38099。PMLR，2023a。'
- en: Xiao et al. [2023b] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. Efficient streaming language models with attention sinks. *arXiv preprint
    arXiv:2309.17453*, 2023b.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人 [2023b] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, 和 Mike
    Lewis。高效流媒体语言模型与注意力吸收。*arXiv 预印本 arXiv:2309.17453*，2023b。
- en: 'Yao et al. [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. In *Conference on Neural Information
    Processing Systems*, 2022.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '姚等人 [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, 和 Yuxiong He。Zeroquant: 高效且经济的大规模 Transformers 后训练量化。发表于 *神经信息处理系统会议*，2022。'
- en: 'Yuan et al. [2023] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang,
    Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based
    post-training quantization for large language models. *arXiv preprint arXiv:2304.01089*,
    2023.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '袁等人 [2023] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang
    Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, 和 Bingzhe Wu。Rptq: 基于重排序的后训练量化方法用于大规模语言模型。*arXiv
    预印本 arXiv:2304.01089*，2023。'
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics*,
    pages 4791–4800, 2019.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泽勒斯等人 [2019] 罗温·泽勒斯、阿里·霍尔茨曼、约纳坦·比斯克、阿里·法赫迪和叶金·崔。Hellaswag：机器真的能完成你的句子吗？在*第57届计算语言学协会年会论文集*中，第4791-4800页，2019年。
- en: 'Zhang et al. [2023] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou,
    Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient
    fine-tuning of language models with zero-init attention. *arXiv preprint arXiv:2303.16199*,
    2023.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2023] 任瑞张、佳铭韩、克里斯·刘、彭高、翟俊周、香飞胡、石林颜、潘璐、洪生李和于乔。Llama-adapter：具有零初始化注意力的语言模型高效微调。*arXiv
    预印本 arXiv:2303.16199*，2023。
- en: 'Zhang et al. [2024a] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin,
    Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse:
    Does your multi-modal llm truly see the diagrams in visual math problems? *arXiv
    preprint arXiv:2403.14624*, 2024a.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2024a] 任瑞张、董志江、易驰张、郝昆林、紫玉郭、彭硕丘、翟俊周、潘璐、凯-魏张、彭高等人。Mathverse：你的多模态大型语言模型是否真的能理解视觉数学问题中的图示？*arXiv
    预印本 arXiv:2403.14624*，2024a。
- en: 'Zhang et al. [2020] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,
    Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit bert. *arXiv
    preprint arXiv:2009.12812*, 2020.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2020] 魏张、卢侯、易春尹、立峰尚、小陈、新疆和群刘。Ternarybert：蒸馏感知超低比特 BERT。*arXiv 预印本 arXiv:2009.12812*，2020。
- en: 'Zhang et al. [2024b] Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou,
    and Carlo Vittorio Cannistraci. Plug-and-play: An efficient post-training pruning
    method for large language models. In *The Twelfth International Conference on
    Learning Representations*, 2024b. URL [https://openreview.net/forum?id=Tr0lPx9woF](https://openreview.net/forum?id=Tr0lPx9woF).'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2024b] 英涛张、郝利白、郝昆林、佳林赵、卢侯和卡洛·维托里奥·卡尼斯特拉奇。即插即用：一种高效的后训练剪枝方法用于大型语言模型。在*第十二届国际学习表征会议*上，2024b。URL
    [https://openreview.net/forum?id=Tr0lPx9woF](https://openreview.net/forum?id=Tr0lPx9woF)。
- en: 'Zhao et al. [2023] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen,
    Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom:
    Low-bit quantization for efficient and accurate llm serving. *arXiv preprint arXiv:2310.19102*,
    2023.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等人 [2023] 易龙赵、简瑜林、阚朱、紫浩叶、乐群陈、大小郑、路易斯·塞泽、阿尔文德·克里希纳穆提、天琦陈和巴里斯·卡西克。Atom：高效准确的低比特量化用于大型语言模型服务。*arXiv
    预印本 arXiv:2310.19102*，2023。
- en: Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*,
    2023.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等人 [2023] 连敏郑、魏林·姜、英盛、思远庄、张浩吴、永浩庄、紫林、卓涵李、大成李、埃里克·星等人。通过mt-bench和聊天机器人竞技场评判llm作为评判者。*arXiv
    预印本 arXiv:2306.05685*，2023。
- en: Appendix Overview
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录概述
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [A](#A1 "Appendix A Related Work ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs"): Related work.'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [A](#A1 "附录 A 相关工作 ‣ 高级离群点管理和高效量化大型语言模型的旋转与置换")：相关工作。
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [B](#A2 "Appendix B Proofs ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs"): Theory proofs.'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [B](#A2 "附录 B 证明 ‣ 高级离群点管理和高效量化大型语言模型的旋转与置换")：理论证明。
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [C](#A3 "Appendix C Additional Implementation Details ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs"):
    Additional implementation details.'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [C](#A3 "附录 C 附加实施细节 ‣ 高级离群点管理和高效量化大型语言模型的旋转与置换")：附加实施细节。
- en: •
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [D](#A4 "Appendix D More Empirical Results ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"): More empirical
    results.'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [D](#A4 "附录 D 更多实证结果 ‣ 高级离群点管理和高效量化大型语言模型的旋转与置换")：更多实证结果。
- en: •
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [E](#A5 "Appendix E More Ablation Studies ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"): More detailed
    ablation studies.'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [E](#A5 "附录 E 更多消融研究 ‣ 高级离群点管理和高效量化大型语言模型的旋转与置换")：更多详细消融研究。
- en: •
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [F](#A6 "Appendix F Detailed Comparison with QuaRot ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs"):
    Detailed comparison with QuaRot.'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [F](#A6 "附录 F 与 QuaRot 的详细比较 ‣ 高级离群点管理和高效量化大型语言模型的旋转与置换")：与 QuaRot 的详细比较。
- en: •
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [G](#A7 "Appendix G Limitations and Broader Impacts ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs"):
    Limitations and broader impacts.'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '部分 [G](#A7 "附录 G 限制与更广泛影响 ‣ 高级异常值管理和高效 LLM 量化的旋转与置换"): 限制和更广泛的影响。'
- en: •
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [H](#A8 "Appendix H More Visualizations ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"): More visualization
    examples.'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '部分 [H](#A8 "附录 H 更多可视化 ‣ 高级异常值管理和高效 LLM 量化的旋转与置换"): 更多可视化示例。'
- en: Appendix A Related Work
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 相关工作
- en: A.1 Network Quantization
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 网络量化
- en: 'Network quantization [[19](#bib.bib19), [23](#bib.bib23), [22](#bib.bib22),
    [51](#bib.bib51)] is a widely utilized technique in neural networks aimed at reducing
    model size and memory usage. Research in this area generally falls into two main
    categories: quantization-aware training (QAT) [[59](#bib.bib59), [3](#bib.bib3),
    [44](#bib.bib44)] and post-training quantization (PTQ) [[38](#bib.bib38), [34](#bib.bib34),
    [4](#bib.bib4), [29](#bib.bib29)]. QAT involves training quantized model weights
    using additional data, often with the assistance of a straight-through estimator
    (STE) [[5](#bib.bib5)]. However, the computational cost associated with QAT poses
    challenges, particularly for large language models (LLMs) with millions of parameters,
    which necessitate significant amounts of data for retraining [[33](#bib.bib33),
    [14](#bib.bib14)]. In contrast, PTQ has gained popularity for LLMs [[48](#bib.bib48),
    [21](#bib.bib21), [58](#bib.bib58), [55](#bib.bib55), [35](#bib.bib35)] due to
    its efficient approach, involving the training of quantized models using a small
    amount of data, known as calibration data [[17](#bib.bib17)]. However, PTQ often
    leads to significant performance degradation, especially when employing low-bit
    settings  [[17](#bib.bib17), [47](#bib.bib47), [41](#bib.bib41), [28](#bib.bib28)].
    Consequently, our work focuses on enhancing the performance of low-bit PTQ quantized
    models.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 网络量化 [[19](#bib.bib19), [23](#bib.bib23), [22](#bib.bib22), [51](#bib.bib51)]
    是一种广泛应用于神经网络的技术，旨在减少模型大小和内存使用。该领域的研究通常分为两大类：量化感知训练（QAT）[[59](#bib.bib59), [3](#bib.bib3),
    [44](#bib.bib44)] 和后训练量化（PTQ）[[38](#bib.bib38), [34](#bib.bib34), [4](#bib.bib4),
    [29](#bib.bib29)]。QAT 涉及使用额外数据对量化模型权重进行训练，通常借助直通估计器（STE）[[5](#bib.bib5)]。然而，QAT
    的计算成本带来了挑战，特别是对于具有数百万参数的大型语言模型（LLMs），需要大量数据进行重新训练 [[33](#bib.bib33), [14](#bib.bib14)]。相比之下，PTQ
    因其高效的方法而在 LLMs 中获得了流行 [[48](#bib.bib48), [21](#bib.bib21), [58](#bib.bib58), [55](#bib.bib55),
    [35](#bib.bib35)]，这种方法涉及使用少量数据（称为校准数据）[[17](#bib.bib17)] 来训练量化模型。然而，PTQ 往往会导致显著的性能下降，特别是在采用低位设置时
    [[17](#bib.bib17), [47](#bib.bib47), [41](#bib.bib41), [28](#bib.bib28)]。因此，我们的工作侧重于提高低位
    PTQ 量化模型的性能。
- en: A.2 Post Training Quantization of LLM
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 LLM 后训练量化
- en: 'Post-training quantization for LLMs can be categorized into weight-only quantization [[30](#bib.bib30),
    [13](#bib.bib13), [26](#bib.bib26), [27](#bib.bib27)] and weight-activation quantization [[54](#bib.bib54),
    [52](#bib.bib52), [50](#bib.bib50)]. We focus on 4-bit weight-activation quantization
    due to the actual speedup it provides with low-bit quantization kernels [[1](#bib.bib1)].
    Quantizing LLMs faces challenges due to activation outlier features persisting
    across different tokens and layers [[12](#bib.bib12), [48](#bib.bib48)]. Some
    approaches [[12](#bib.bib12), [61](#bib.bib61)] retain a small portion of crucial
    outlier channels at high precision (e.g., INT8), which poses challenges to hardware
    compatibility and leads to additional memory footprint. Other methods [[52](#bib.bib52),
    [49](#bib.bib49), [42](#bib.bib42)] attempt to shift quantization difficulty from
    activation to weight channels. However, the learnable equivalent transformation
    in OmniQuant [[42](#bib.bib42)] and the affine transform matrix in AffineQuant [[36](#bib.bib36)]
    exhibit instability as discussed in Section [2](#S2 "2 Motivation ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs").
    The channel disassembly and assembly in QLLM [[31](#bib.bib31)], coupled with
    LoRA-tuning, incur significant time costs. Notably, these methods demonstrate
    poor performance under W4A4 quantization. We attribute this degradation to the
    ineffective handling of outlier features, especially massive outliers. Hence,
    we propose DuQuant to effectively eliminate outlier features through rotation
    matrices and channel permutation, achieving state-of-the-art performance. In contrast
    with QuaRot [[2](#bib.bib2)] also utilizing hadamard matrices to enhance weight-activation
    quantization, our approach uniquely incorporates knowledge about the actual outlier
    channels. Furthermore, unlike QuaRot, which relies on GPTQ [[17](#bib.bib17)]
    for weight quantization, our channel permutation has been proven helpful and efficient,
    facilitating a faster quantization process. The more detailed analysis and comparison
    with QuaRot are left in Appendix [F](#A6 "Appendix F Detailed Comparison with
    QuaRot ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs"). In addition, unlike RPTQ [[55](#bib.bib55)] and SKVQ [[15](#bib.bib15)],
    which use channel reordering to cluster similar activations, our method employs
    Permutation transformations with a fundamentally different goal: to evenly distribute
    outliers across blocks. This balanced distribution is crucial for enabling effective
    secondary rotations, ultimately leading to smoother activations that facilitate
    easier quantization.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 的后训练量化可以分为仅权重量化 [[30](#bib.bib30), [13](#bib.bib13), [26](#bib.bib26),
    [27](#bib.bib27)] 和权重-激活量化 [[54](#bib.bib54), [52](#bib.bib52), [50](#bib.bib50)]。我们关注的是
    4 位权重-激活量化，因为它在低位量化内核中提供了实际的加速 [[1](#bib.bib1)]。量化 LLMs 面临挑战，因为激活异常值特征在不同的 token
    和层之间持续存在 [[12](#bib.bib12), [48](#bib.bib48)]。一些方法 [[12](#bib.bib12), [61](#bib.bib61)]
    保留少量关键异常通道的高精度（例如 INT8），这对硬件兼容性构成挑战，并导致额外的内存占用。其他方法 [[52](#bib.bib52), [49](#bib.bib49),
    [42](#bib.bib42)] 尝试将量化难度从激活通道转移到权重通道。然而，OmniQuant [[42](#bib.bib42)] 中的可学习等效变换和
    AffineQuant [[36](#bib.bib36)] 中的仿射变换矩阵表现出不稳定性，如第 [2](#S2 "2 Motivation ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs") 节讨论的那样。QLLM [[31](#bib.bib31)] 中的通道解组装和组装，加上 LoRA 调优，耗费了大量时间。值得注意的是，这些方法在
    W4A4 量化下表现不佳。我们将这种性能下降归因于对异常特征，特别是大规模异常值的处理不当。因此，我们提出了 DuQuant，通过旋转矩阵和通道置换有效消除异常特征，实现了最先进的性能。与
    QuaRot [[2](#bib.bib2)] 也利用 Hadamard 矩阵来增强权重-激活量化不同，我们的方法独特地结合了关于实际异常通道的知识。此外，不同于
    QuaRot 依赖 GPTQ [[17](#bib.bib17)] 进行权重量化，我们的通道置换已被证明有效且高效，促进了更快的量化过程。对 QuaRot
    的更详细分析和比较在附录 [F](#A6 "Appendix F Detailed Comparison with QuaRot ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs")
    中进行了介绍。此外，不同于 RPTQ [[55](#bib.bib55)] 和 SKVQ [[15](#bib.bib15)]，它们使用通道重新排序来聚类相似的激活，我们的方法使用置换变换，其根本目标不同：均匀分布异常值在块中。这种均衡的分布对于实现有效的二次旋转至关重要，从而最终导致更平滑的激活，便于量化。
- en: Appendix B Proofs
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 证明
- en: Theorem 1  (Rotation).
  id: totrans-300
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 1  （旋转）。
- en: For the activation input $\mathbf{X}\in\mathbb{R}^{T\times C_{in}}$ within the
    input. Then, we can deduce that,
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入中的激活输入 $\mathbf{X}\in\mathbb{R}^{T\times C_{in}}$。然后，我们可以推导出，
- en: '|  | $1$2 |  | (8) |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (8) |'
- en: Proof.
  id: totrans-303
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: In the case of a specific block $b_{i}$. This can be formally defined as follows,
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定块 $b_{i}$ 的情况，可以形式化地定义如下，
- en: '|  |  | $1$2 |  |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Without loss of generality, let’s assume that $\delta_{i}\geq 0$. Consequently,
    we can obtain the following inequality,
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 不失一般性，假设 $\delta_{i}\geq 0$。因此，我们可以得到以下不等式，
- en: '|  | $\displaystyle\underset{1\leq j\leq 2^{n}}{\max}~{}~{}O_{j}(\mathbf{X}_{b_{i}}\hat{\mathbf{R}}_{b_{i}})$
    |  | (9) |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{1\leq j\leq 2^{n}}{\max}~{}~{}O_{j}(\mathbf{X}_{b_{i}}\hat{\mathbf{R}}_{b_{i}})$
    |  | (9) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\underset{1\leq j\leq 2^{n}}{\max}~{}~{}O_{j}(\mathbf{X}_{b_{i}}).$
    |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\underset{1\leq j\leq 2^{n}}{\max}~{}~{}O_{j}(\mathbf{X}_{b_{i}}).$
    |  |'
- en: The inequality (1) holds because the switch matrix $\mathbf{E}_{d^{(1)}}$.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 不等式 (1) 成立，因为切换矩阵 $\mathbf{E}_{d^{(1)}}$。
- en: ∎
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Theorem 2  (Zigzag Permutation).
  id: totrans-314
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 2  （之字形排列）。
- en: For the activation input $\mathbf{X}\in\mathbb{R}^{T\times C_{in}}$-th block
    consistently satisfies,
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 对于激活输入 $\mathbf{X}\in\mathbb{R}^{T\times C_{in}}$ 的第 $-th$ 块，始终满足，
- en: '|  | $M_{b_{i}}\leq O^{(1)}+\frac{(2^{n}K-1)(2^{n-1}-1)}{2^{n}}\delta,\qquad
    i=1,2,3,...,K.$ |  | (10) |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  | $M_{b_{i}}\leq O^{(1)}+\frac{(2^{n}K-1)(2^{n-1}-1)}{2^{n}}\delta,\qquad
    i=1,2,3,...,K.$ |  | (10) |'
- en: Proof.
  id: totrans-317
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: According to the zigzag permutation described in Section [3.2](#S3.SS2 "3.2
    The proposed DuQuant Method ‣ 3 Method ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs"), and considering the reordered
    outliers $O^{(1)},O^{(2)},...,O^{(C_{in})}$-th block, it contains the following
    channels,
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 根据第 [3.2](#S3.SS2 "3.2 提出的 DuQuant 方法 ‣ 3 方法 ‣ 高级离群值管理和 LLM 高效量化的旋转与排列") 节中描述的之字形排列，并考虑重新排序的离群值
    $O^{(1)},O^{(2)},...,O^{(C_{in})}$-th 块，它包含以下通道，
- en: '|  | $\displaystyle b_{i}$ |  |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle b_{i}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Since $\delta=\max\{|O^{(i+1)}-O^{(i)}|\},i=1,2,...,C_{in}\!\!-\!1$, then we
    can get
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $\delta=\max\{|O^{(i+1)}-O^{(i)}|\},i=1,2,...,C_{in}\!\!-\!1$，因此我们可以得到
- en: '|  | $\displaystyle M_{b_{1}}$ |  | (11) |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle M_{b_{1}}$ |  | (11) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle\leq O^{(1)}+\frac{(2^{n}K-1)(2^{n-1}-1)}{2^{n}}\delta.$
    |  |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq O^{(1)}+\frac{(2^{n}K-1)(2^{n-1}-1)}{2^{n}}\delta.$
    |  |'
- en: Similarly, we can deduce that all $M_{b_{i}}$) share the same upper bound after
    applying our zigzag permutation. ∎
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以推导出在应用我们的之字形排列后，所有 $M_{b_{i}}$）具有相同的上界。∎
- en: Appendix C Additional Implementation Details
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 额外的实现细节
- en: In this work, all experiments are done on NVIDIA RTX 3090 GPUs for small-scale
    models and NVIDIA A100 GPUs for large-scale models.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在本工作中，所有实验都在 NVIDIA RTX 3090 GPU 上进行以处理小规模模型，在 NVIDIA A100 GPU 上进行以处理大规模模型。
- en: For calibration data, following [[42](#bib.bib42), [36](#bib.bib36), [31](#bib.bib31)],
    we randomly select 128 sampled sequences from the WikiText2 dataset, with the
    sequence length of 2048. For rotation and permutation transformations, the rotation
    block size $2^{n}$ equals 256\. We adopt once permutation times for efficiency.
    We conduct detailed ablation studies in Appendix [E.2](#A5.SS2 "E.2 Effects of
    Rotation Matrix ‣ Appendix E More Ablation Studies ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"), [E.3](#A5.SS3
    "E.3 Effects of Permutation Algorithm. ‣ Appendix E More Ablation Studies ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs"), [E.4](#A5.SS4 "E.4 Effects of Calibration Datasets ‣ Appendix E More Ablation
    Studies ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs").
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 对于校准数据，参照 [[42](#bib.bib42), [36](#bib.bib36), [31](#bib.bib31)]，我们从 WikiText2
    数据集中随机选择 128 个采样序列，序列长度为 2048。对于旋转和排列变换，旋转块大小 $2^{n}$ 设为 256\. 我们采用一次排列以提高效率。我们在附录
    [E.2](#A5.SS2 "E.2 旋转矩阵的影响 ‣ 附录 E 更多消融研究 ‣ 高级离群值管理和 LLM 高效量化的旋转与排列")、[E.3](#A5.SS3
    "E.3 排列算法的影响 ‣ 附录 E 更多消融研究 ‣ 高级离群值管理和 LLM 高效量化的旋转与排列")、[E.4](#A5.SS4 "E.4 校准数据集的影响
    ‣ 附录 E 更多消融研究 ‣ 高级离群值管理和 LLM 高效量化的旋转与排列") 中进行了详细的消融研究。
- en: Regarding quantization details, for multiplications between activations in MSA,
    such as Query and Key, attention outputs and Value, we apply a Hadamard rotation
    matrix for rapid and straightforward processing. A Hadamard matrix is an orthogonal
    and symmetric matrix filled with elements $\pm 1/\sqrt{2^{n}}$, we set it to 0.6
    for DuQuant and 0.5 DuQuant+LWC. We clip the maximum activation values in all
    projection blocks, and the clipping ratio is set to 0.9\. For DuQuant  we also
    clip the maximum values in weight matrices, with a clipping ratio of 0.8\. For
    lwc, we keep the same default epoch numbers of 20, batch size as 1, learning rate
    as 5e-3, and zero weight decay, as [[42](#bib.bib42)].
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 关于量化细节，对于MSA中的激活之间的乘法，如Query和Key、注意力输出和Value，我们应用Hadamard旋转矩阵以实现快速且简单的处理。Hadamard矩阵是一个正交对称矩阵，元素为$\pm
    1/\sqrt{2^{n}}$，我们将其设置为0.6用于DuQuant和0.5用于DuQuant+LWC。我们裁剪所有投影块中的最大激活值，裁剪比例设为0.9。对于DuQuant，我们还裁剪权重矩阵中的最大值，裁剪比例为0.8。对于lwc，我们保持默认的20个epoch，batch
    size为1，学习率为5e-3，并且没有权重衰减，参考[[42](#bib.bib42)]。
- en: Appendix D More Empirical Results
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 更多实证结果
- en: 'Table D1: Zero-shot common-sense QA ($\uparrow$) results of LLaMA2 models under
    4-bit WA quantization.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 表D1：LLaMA2模型在4位WA量化下的零样本常识问答（$\uparrow$）结果。
- en: '| Model | Method | PIQA | ARC-E | ARC-C | BoolQ | HellaSwag | WinoGrande |
    Avg. |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | PIQA | ARC-E | ARC-C | BoolQ | HellaSwag | WinoGrande | 平均 |'
- en: '| LLaMA2-7B W4A4 | FP16 | 76.88 | 53.54 | 40.53 | 71.13 | 72.96 | 67.25 | 63.72
    |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B W4A4 | FP16 | 76.88 | 53.54 | 40.53 | 71.13 | 72.96 | 67.25 | 63.72
    |'
- en: '| SmoothQuant | 60.17 | 35.23 | 27.13 | 57.92 | 37.08 | 49.57 | 44.52 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 60.17 | 35.23 | 27.13 | 57.92 | 37.08 | 49.57 | 44.52 |'
- en: '| OS+ | 63.11 | 39.10 | 28.84 | - | 51.30 | 45.93 | 45.66 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | 63.11 | 39.10 | 28.84 | - | 51.30 | 45.93 | 45.66 |'
- en: '| OmniQuant | 65.61 | 44.28 | 30.38 | 62.66 | 53.51 | 51.85 | 51.38 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 65.61 | 44.28 | 30.38 | 62.66 | 53.51 | 51.85 | 51.38 |'
- en: '| AffineQuant | 67.36 | 44.23 | 31.91 | 62.75 | 54.38 | 55.18 | 52.64 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 67.36 | 44.23 | 31.91 | 62.75 | 54.38 | 55.18 | 52.64 |'
- en: '| QLLM | 67.68 | 44.40 | 30.89 | - | 58.45 | 56.59 | 51.60 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 67.68 | 44.40 | 30.89 | - | 58.45 | 56.59 | 51.60 |'
- en: '| Atom | 69.75 | 47.35 | 34.22 | 62.42 | 63.21 | 56.51 | 55.58 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Atom | 69.75 | 47.35 | 34.22 | 62.42 | 63.21 | 56.51 | 55.58 |'
- en: '| DuQuant | 75.24 | 51.89 | 36.77 | 67.86 | 69.54 | 62.12 | 60.57 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 75.24 | 51.89 | 36.77 | 67.86 | 69.54 | 62.12 | 60.57 |'
- en: '| DuQuant+LWC | 75.68 | 50.00 | 37.46 | 69.24 | 69.74 | 63.93 | 61.01 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 75.68 | 50.00 | 37.46 | 69.24 | 69.74 | 63.93 | 61.01 |'
- en: '| LLaMA2-13B W4A4 | FP16 | 79.05 | 57.91 | 44.20 | 69.02 | 76.60 | 69.69 |
    66.08 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B W4A4 | FP16 | 79.05 | 57.91 | 44.20 | 69.02 | 76.60 | 69.69 |
    66.08 |'
- en: '| SmoothQuant | 62.30 | 40.28 | 30.72 | 60.49 | 42.24 | 49.96 | 47.67 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 62.30 | 40.28 | 30.72 | 60.49 | 42.24 | 49.96 | 47.67 |'
- en: '| OS+ | 64.47 | 41.46 | 32.17 | - | 59.30 | 51.38 | 49.76 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | 64.47 | 41.46 | 32.17 | - | 59.30 | 51.38 | 49.76 |'
- en: '| OmniQuant | 69.80 | 47.22 | 33.79 | 65.47 | 59.34 | 55.49 | 55.19 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 69.80 | 47.22 | 33.79 | 65.47 | 59.34 | 55.49 | 55.19 |'
- en: '| AffineQuant | 68.55 | 47.64 | 32.34 | 66.97 | 59.97 | 55.07 | 55.09 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 68.55 | 47.64 | 32.34 | 66.97 | 59.97 | 55.07 | 55.09 |'
- en: '| QLLM | 70.46 | 48.48 | 34.39 | - | 62.80 | 55.41 | 54.31 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 70.46 | 48.48 | 34.39 | - | 62.80 | 55.41 | 54.31 |'
- en: '| Atom | 71.16 | 50.89 | 37.88 | 63.91 | 67.51 | 58.40 | 58.29 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| Atom | 71.16 | 50.89 | 37.88 | 63.91 | 67.51 | 58.40 | 58.29 |'
- en: '| DuQuant | 77.31 | 55.60 | 41.55 | 66.61 | 73.68 | 66.06 | 63.47 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 77.31 | 55.60 | 41.55 | 66.61 | 73.68 | 66.06 | 63.47 |'
- en: '| DuQuant+LWC | 77.26 | 56.23 | 42.15 | 65.78 | 73.68 | 65.43 | 63.42 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 77.26 | 56.23 | 42.15 | 65.78 | 73.68 | 65.43 | 63.42 |'
- en: '| LLaMA2-70B W4A4 | FP16 | 81.01 | 59.68 | 47.95 | 75.87 | 80.87 | 76.95 |
    70.39 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-70B W4A4 | FP16 | 81.01 | 59.68 | 47.95 | 75.87 | 80.87 | 76.95 |
    70.39 |'
- en: '| SmoothQuant | 64.09 | 41.84 | 32.00 | 58.56 | 54.21 | 51.07 | 50.30 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 64.09 | 41.84 | 32.00 | 58.56 | 54.21 | 51.07 | 50.30 |'
- en: '| OS+ | 66.16 | 42.72 | 34.90 | - | 56.93 | 52.96 | 50.73 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | 66.16 | 42.72 | 34.90 | - | 56.93 | 52.96 | 50.73 |'
- en: '| QLLM | 74.27 | 50.59 | 37.20 | - | 71.62 | 59.43 | 58.62 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 74.27 | 50.59 | 37.20 | - | 71.62 | 59.43 | 58.62 |'
- en: '| DuQuant | 79.27 | 58.16 | 46.07 | 70.46 | 79.21 | 74.19 | 67.89 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 79.27 | 58.16 | 46.07 | 70.46 | 79.21 | 74.19 | 67.89 |'
- en: '| DuQuant+LWC | 79.82 | 59.76 | 46.76 | 73.12 | 79.38 | 74.11 | 68.83 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 79.82 | 59.76 | 46.76 | 73.12 | 79.38 | 74.11 | 68.83 |'
- en: Zero-shot QA Results for 4-bit LLaMA2 Models.
  id: totrans-357
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 针对4位LLaMA2模型的零样本问答结果。
- en: Table [D1](#A4.T1 "Table D1 ‣ Appendix D More Empirical Results ‣ Rotation and
    Permutation for Advanced Outlier Management and Efficient Quantization of LLMs")
    showcases the zero-shot commonsense QA results for INT4 quantized LLaMA2 models.
    Our DuQuant method excels across various model sizes and datasets, demonstrating
    state-of-the-art performance in commonsense reasoning tasks. For example, DuQuant outperforms
    Atom by 5.43% for the LLaMA2-7B model and by 5.18% for the LLaMA2-13B model. In
    contrast to Atom [[61](#bib.bib61)], which relies on GPTQ for weight quantization
    and maintains 128 channels at INT8, thereby increasing memory usage, our method
    offers a rapid and more efficient weight-activation quantization solution through
    Rotation and Permutation.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 表[D1](#A4.T1 "Table D1 ‣ Appendix D More Empirical Results ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs")展示了INT4量化LLaMA2模型的零-shot常识问答结果。我们的DuQuant方法在各种模型尺寸和数据集上表现优异，展示了在常识推理任务中的最先进性能。例如，DuQuant在LLaMA2-7B模型上超越了Atom
    5.43%，在LLaMA2-13B模型上超越了5.18%。与依赖于GPTQ进行权重量化并在INT8下保持128通道，从而增加内存使用的Atom[[61](#bib.bib61)]相比，我们的方法通过Rotation和Permutation提供了快速且更高效的权重-激活量化解决方案。
- en: MMLU Results for 4-bit Vicuna-v1.5-7B.
  id: totrans-359
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MMLU 4-bit Vicuna-v1.5-7B的结果。
- en: Vicuna-v1.5 models [[9](#bib.bib9)], fine-tuned from LLaMA-2 models using high-quality
    user-shared conversations, are considered state-of-the-art chatbots. Table [D2](#A4.T2
    "Table D2 ‣ MMLU Results for 4-bit Vicuna-v1.5-7B. ‣ Appendix D More Empirical
    Results ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs") displays the INT4 quantization results for Vicuna-v1.5-7B
    on the MMLU benchmarks. In comparison to SmoothQuant, OmniQuant, and Atom, our
    DuQuant method exhibits the smallest performance decline and maintains competitive
    capacities in both zero-shot and five-shot settings. These results demonstrate
    the effectiveness of DuQuant in generalizing to instruction-tuned models.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: Vicuna-v1.5模型[[9](#bib.bib9)]，经过从LLaMA-2模型微调，并使用高质量用户共享对话数据，被认为是最先进的聊天机器人。表[D2](#A4.T2
    "Table D2 ‣ MMLU Results for 4-bit Vicuna-v1.5-7B. ‣ Appendix D More Empirical
    Results ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs")展示了Vicuna-v1.5-7B在MMLU基准上的INT4量化结果。与SmoothQuant、OmniQuant和Atom相比，我们的DuQuant方法表现出最小的性能下降，并在零-shot和五-shot设置中保持了竞争力。这些结果展示了DuQuant在通用指令调优模型中的有效性。
- en: 'Table D2: Zero-shot and five-shot results on the MMLU benchmark for quantized
    Vicuna-v1.5-7B.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 表D2：量化的Vicuna-v1.5-7B在MMLU基准上的零-shot和五-shot结果。
- en: '| Model | Method | MMLU (0 shot) $\uparrow$ |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | MMLU (0 shot) $\uparrow$ |'
- en: '| STEM | Hums | Social | Others | Avg. | STEM | Hums | Social | Others | Avg.
    |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| STEM | Hums | Social | Others | 平均 | STEM | Hums | Social | Others | 平均 |'
- en: '| Vicuna-v1.5-7B W4A4 | FP16 | 38.70 | 45.42 | 56.13 | 56.01 | 49.07 | 39.56
    | 45.76 | 58.14 | 57.43 | 50.22 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-v1.5-7B W4A4 | FP16 | 38.70 | 45.42 | 56.13 | 56.01 | 49.07 | 39.56
    | 45.76 | 58.14 | 57.43 | 50.22 |'
- en: '| SmoothQuant | 27.10 | 25.16 | 27.40 | 26.71 | 26.59 | 25.22 | 25.06 | 24.99
    | 26.68 | 25.49 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 27.10 | 25.16 | 27.40 | 26.71 | 26.59 | 25.22 | 25.06 | 24.99
    | 26.68 | 25.49 |'
- en: '| OmniQuant | 27.20 | 24.00 | 27.14 | 25.08 | 25.86 | 29.39 | 24.95 | 27.30
    | 24.80 | 26.39 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 27.20 | 24.00 | 27.14 | 25.08 | 25.86 | 29.39 | 24.95 | 27.30
    | 24.80 | 26.39 |'
- en: '| Atom | 30.28 | 34.73 | 38.97 | 40.56 | 36.14 | 31.97 | 35.37 | 40.46 | 40.81
    | 37.15 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| Atom | 30.28 | 34.73 | 38.97 | 40.56 | 36.14 | 31.97 | 35.37 | 40.46 | 40.81
    | 37.15 |'
- en: '| DuQuant | 35.85 | 42.66 | 52.03 | 51.23 | 45.44 | 38.90 | 42.57 | 51.80 |
    51.23 | 46.13 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 35.85 | 42.66 | 52.03 | 51.23 | 45.44 | 38.90 | 42.57 | 51.80 |
    51.23 | 46.13 |'
- en: '|  | DuQuant+LWC | 35.18 | 41.91 | 51.28 | 50.52 | 44.72 | 37.34 | 42.21 |
    53.07 | 51.76 | 46.10 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|  | DuQuant+LWC | 35.18 | 41.91 | 51.28 | 50.52 | 44.72 | 37.34 | 42.21 |
    53.07 | 51.76 | 46.10 |'
- en: Results for 4-bit LLaMA3-70B.
  id: totrans-370
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4-bit LLaMA3-70B的结果。
- en: As LLaMA3 models have proven to be sensitive to quantization, we apply our DuQuant to
    the LLaMA3-70B and present the results in Table [D3](#A4.T3 "Table D3 ‣ Results
    for 4-bit LLaMA3-70B. ‣ Appendix D More Empirical Results ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"). Due to time
    constraints, we do not add learnable weight clipping. The results demonstrate
    that our RAP-quantized models outperform SmoothQuant by 12.9% on Commonsense QA
    tasks and significantly reduce perplexity across the WikiText2, C4, and PTB datasets.
    These improvements underscore the robustness of our DuQuant method when applied
    to the LLaMA3-70B model.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLaMA3模型对量化非常敏感，我们将DuQuant应用于LLaMA3-70B，并在表格[D3](#A4.T3 "表 D3 ‣ 4-bit LLaMA3-70B
    的结果 ‣ 附录 D 更多实证结果 ‣ 先进异常值管理和高效LLM量化的旋转和置换")中展示了结果。由于时间限制，我们没有添加可学习的权重裁剪。结果表明，我们的RAP量化模型在Commonsense
    QA任务上比SmoothQuant提高了12.9%，并显著降低了WikiText2、C4和PTB数据集上的困惑度。这些改进突显了我们DuQuant方法在LLaMA3-70B模型上的鲁棒性。
- en: 'Table D3: Perplexity and QA results of LLaMA3-70B under 4-bit weight-activation
    quantization.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '表 D3: LLaMA3-70B 在 4-bit 权重-激活量化下的困惑度和QA结果。'
- en: '| #Bits | Method | WikiText2 $\downarrow$ |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| #Bits | 方法 | WikiText2 $\downarrow$ |'
- en: '| FP16 | - | 2.9 | 6.9 | 8.2 | 82.4 | 86.9 | 60.3 | 85.2 | 84.9 | 80.6 | 80.1
    |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 2.9 | 6.9 | 8.2 | 82.4 | 86.9 | 60.3 | 85.2 | 84.9 | 80.6 | 80.1
    |'
- en: '| LLaMA3-70B W4A4 | SmoothQuant | 9.6 | 16.9 | 17.7 | 76.9 | 75.8 | 43.5 |
    64.4 | 62.9 | 58.9 | 63.7 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-70B W4A4 | SmoothQuant | 9.6 | 16.9 | 17.7 | 76.9 | 75.8 | 43.5 |
    64.4 | 62.9 | 58.9 | 63.7 |'
- en: '| DuQuant | 4.9 | 8.3 | 8.7 | 81.1 | 80.8 | 57.3 | 81.3 | 82.1 | 77.0 | 76.6
    |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 4.9 | 8.3 | 8.7 | 81.1 | 80.8 | 57.3 | 81.3 | 82.1 | 77.0 | 76.6
    |'
- en: W6A6 Quantization Results.
  id: totrans-377
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: W6A6 量化结果。
- en: To thoroughly evaluate the effectiveness of our DuQuant models, we conduct comprehensive
    assessments under the W6A6 quantization setting. The perplexity results for language
    generation tasks are displayed in Table [D4](#A4.T4 "Table D4 ‣ W6A6 Quantization
    Results. ‣ Appendix D More Empirical Results ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs"), while the zero-shot accuracy
    for Commonsense QA tasks is detailed in Tables [D5](#A4.T5 "Table D5 ‣ W6A6 Quantization
    Results. ‣ Appendix D More Empirical Results ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs") and [D6](#A4.T6 "Table
    D6 ‣ W6A6 Quantization Results. ‣ Appendix D More Empirical Results ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs"). Our findings reveal that DuQuant not only surpasses other baselines but
    also achieves nearly lossless performance with FP16 models in these tasks. Interestingly,
    in several instances, DuQuant slightly outperforms DuQuant+LWC . This suggests
    that the Rotation and Permutation transformations alone are sufficient to create
    highly competitive quantized models under W6A6 settings, without the need for
    additional enhancements such as the learnable weight clipping (LWC) technique.
    These outcomes highlight the exceptional versatility and robustness of DuQuant across
    various quantization scenarios, confirming its potential as a leading solution
    in post-training quantization for large language models.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面评估我们的DuQuant模型的有效性，我们在W6A6量化设置下进行了综合评估。语言生成任务的困惑度结果显示在表[D4](#A4.T4 "表 D4
    ‣ W6A6 量化结果 ‣ 附录 D 更多实证结果 ‣ 先进异常值管理和高效LLM量化的旋转和置换")中，而Commonsense QA任务的零样本准确性在表[D5](#A4.T5
    "表 D5 ‣ W6A6 量化结果 ‣ 附录 D 更多实证结果 ‣ 先进异常值管理和高效LLM量化的旋转和置换")和[D6](#A4.T6 "表 D6 ‣
    W6A6 量化结果 ‣ 附录 D 更多实证结果 ‣ 先进异常值管理和高效LLM量化的旋转和置换")中详细说明。我们的研究发现，DuQuant不仅超越了其他基准，还在这些任务中实现了与FP16模型几乎无损的性能。有趣的是，在几个实例中，DuQuant稍微优于DuQuant+LWC。这表明，仅使用旋转和置换变换就足以在W6A6设置下创建具有竞争力的量化模型，无需额外的增强，如可学习的权重裁剪（LWC）技术。这些结果突显了DuQuant在各种量化场景中的卓越通用性和鲁棒性，确认了其作为大型语言模型后训练量化领先解决方案的潜力。
- en: 'Table D4: Preplexity ($\downarrow$) results on the WikiText2 and C4 datasets
    under 6-bit WA quantization.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '表 D4: WikiText2 和 C4 数据集在 6-bit WA 量化下的困惑度（$\downarrow$）结果。'
- en: '| Dataset | #Bit | Method | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B | 2-70B
    |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | #Bit | 方法 | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B | 2-70B |'
- en: '| WikiText2 | FP16 | - | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 | FP16 | - | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 |'
- en: '| W6A6 | SmoothQuant | 6.03 | 5.42 | 4.55 | 3.88 | 6.20 | 5.18 | 3.69 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | SmoothQuant | 6.03 | 5.42 | 4.55 | 3.88 | 6.20 | 5.18 | 3.69 |'
- en: '| OmniQuant | 5.96 | 5.28 | 4.38 | 3.75 | 5.87 | 5.14 | 3.71 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 5.96 | 5.28 | 4.38 | 3.75 | 5.87 | 5.14 | 3.71 |'
- en: '| QLLM | 5.89 | 5.28 | 4.30 | 3.73 | 5.91 | 5.08 | 3.55 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 5.89 | 5.28 | 4.30 | 3.73 | 5.91 | 5.08 | 3.55 |'
- en: '| DuQuant | 5.73 | 5.13 | 4.14 | 3.57 | 5.53 | 4.92 | 3.35 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 5.73 | 5.13 | 4.14 | 3.57 | 5.53 | 4.92 | 3.35 |'
- en: '| DuQuant+LWC | 5.74 | 5.13 | 4.15 | 3.60 | 5.53 | 4.92 | 3.35 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 5.74 | 5.13 | 4.15 | 3.60 | 5.53 | 4.92 | 3.35 |'
- en: '| C4 | FP16 | - | 7.08 | 6.61 | 5.98 | 5.62 | 6.97 | 6.46 | 5.52 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| C4 | FP16 | - | 7.08 | 6.61 | 5.98 | 5.62 | 6.97 | 6.46 | 5.52 |'
- en: '| W6A6 | SmoothQuant | 7.47 | 6.97 | 6.34 | 5.99 | 7.76 | 6.76 | 5.88 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | SmoothQuant | 7.47 | 6.97 | 6.34 | 5.99 | 7.76 | 6.76 | 5.88 |'
- en: '| OmniQuant | 7.43 | 6.84 | 6.22 | 5.82 | 7.48 | 6.74 | 5.91 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 7.43 | 6.84 | 6.22 | 5.82 | 7.48 | 6.74 | 5.91 |'
- en: '| QLLM | 7.34 | 6.82 | 6.17 | 5.80 | 7.31 | 6.71 | 5.76 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 7.34 | 6.82 | 6.17 | 5.80 | 7.31 | 6.71 | 5.76 |'
- en: '| DuQuant | 7.12 | 6.64 | 6.00 | 5.64 | 7.03 | 6.50 | 5.54 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 7.12 | 6.64 | 6.00 | 5.64 | 7.03 | 6.50 | 5.54 |'
- en: '| DuQuant+LWC | 7.13 | 6.64 | 6.01 | 5.64 | 7.03 | 6.50 | 5.54 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 7.13 | 6.64 | 6.01 | 5.64 | 7.03 | 6.50 | 5.54 |'
- en: 'Table D5: Zero-shot common-sense QA ($\uparrow$) results of LLaMA1 models under
    6-bit WA quantization.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 表 D5：LLaMA1 模型在 6-bit WA 量化下的零样本常识 QA （$\uparrow$）结果。
- en: '| Model | Method | PIQA | ARC-E | ARC-C | BoolQ | HellaSwag | WinoGrande |
    Avg |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | PIQA | ARC-E | ARC-C | BoolQ | HellaSwag | WinoGrande | 平均 |'
- en: '| LLaMA1-7B W6A6 | FP16 | 77.47 | 52.48 | 41.46 | 73.08 | 73.00 | 67.07 | 64.09
    |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-7B W6A6 | FP16 | 77.47 | 52.48 | 41.46 | 73.08 | 73.00 | 67.07 | 64.09
    |'
- en: '| SmoothQuant | 76.75 | 51.64 | 39.88 | 71.75 | 71.67 | 65.03 | 62.81 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 76.75 | 51.64 | 39.88 | 71.75 | 71.67 | 65.03 | 62.81 |'
- en: '| OS+ | 76.82 | 51.35 | 41.13 | 72.08 | 71.42 | 65.98 | 61.13 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | 76.82 | 51.35 | 41.13 | 72.08 | 71.42 | 65.98 | 61.13 |'
- en: '| OmniQuant | 77.09 | 51.89 | 40.87 | 72.53 | 71.61 | 65.03 | 63.17 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 77.09 | 51.89 | 40.87 | 72.53 | 71.61 | 65.03 | 63.17 |'
- en: '| AffineQuant | 76.60 | 52.29 | 40.63 | 72.65 | 71.29 | 63.85 | 62.89 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 76.60 | 52.29 | 40.63 | 72.65 | 71.29 | 63.85 | 62.89 |'
- en: '| QLLM | 77.26 | 52.02 | 41.04 | - | 71.40 | 65.19 | 61.38 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 77.26 | 52.02 | 41.04 | - | 71.40 | 65.19 | 61.38 |'
- en: '| DuQuant | 77.53 | 51.47 | 41.13 | 72.78 | 72.76 | 66.69 | 63.73 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 77.53 | 51.47 | 41.13 | 72.78 | 72.76 | 66.69 | 63.73 |'
- en: '| DuQuant+LWC | 77.42 | 52.65 | 40.53 | 71.53 | 72.64 | 67.72 | 63.75 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 77.42 | 52.65 | 40.53 | 71.53 | 72.64 | 67.72 | 63.75 |'
- en: '| LLaMA1-13B W6A6 | FP16 | 79.10 | 59.89 | 44.45 | 68.01 | 76.21 | 70.31 |
    66.33 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-13B W6A6 | FP16 | 79.10 | 59.89 | 44.45 | 68.01 | 76.21 | 70.31 |
    66.33 |'
- en: '| SmoothQuant | 77.91 | 56.60 | 42.40 | 64.95 | 75.36 | 69.36 | 64.43 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 77.91 | 56.60 | 42.40 | 64.95 | 75.36 | 69.36 | 64.43 |'
- en: '| OS+ | 78.29 | 56.90 | 43.09 | 66.98 | 75.09 | 69.22 | 64.92 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | 78.29 | 56.90 | 43.09 | 66.98 | 75.09 | 69.22 | 64.92 |'
- en: '| OmniQuant | 78.40 | 57.28 | 42.91 | 67.00 | 75.82 | 68.27 | 64.95 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 78.40 | 57.28 | 42.91 | 67.00 | 75.82 | 68.27 | 64.95 |'
- en: '| QLLM | 77.91 | 57.70 | 42.92 | - | 75.02 | 69.14 | 64.54 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 77.91 | 57.70 | 42.92 | - | 75.02 | 69.14 | 64.54 |'
- en: '| DuQuant | 78.62 | 59.51 | 44.03 | 68.44 | 75.98 | 70.08 | 66.11 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 78.62 | 59.51 | 44.03 | 68.44 | 75.98 | 70.08 | 66.11 |'
- en: '| DuQuant+LWC | 79.16 | 59.39 | 43.69 | 68.10 | 75.81 | 69.06 | 65.87 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 79.16 | 59.39 | 43.69 | 68.10 | 75.81 | 69.06 | 65.87 |'
- en: '| LLaMA1-30B W6A6 | FP16 | 80.08 | 58.92 | 45.47 | 68.44 | 79.21 | 72.53 |
    67.44 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-30B W6A6 | FP16 | 80.08 | 58.92 | 45.47 | 68.44 | 79.21 | 72.53 |
    67.44 |'
- en: '| SmoothQuant | 77.14 | 57.61 | 42.91 | 65.56 | 78.07 | 69.92 | 65.20 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 77.14 | 57.61 | 42.91 | 65.56 | 78.07 | 69.92 | 65.20 |'
- en: '| OS+ | 80.14 | 58.92 | 45.05 | 68.02 | 77.96 | 71.98 | 67.01 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | 80.14 | 58.92 | 45.05 | 68.02 | 77.96 | 71.98 | 67.01 |'
- en: '| OmniQuant | 79.81 | 58.79 | 45.22 | 68.38 | 78.95 | 72.21 | 67.23 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 79.81 | 58.79 | 45.22 | 68.38 | 78.95 | 72.21 | 67.23 |'
- en: '| QLLM | 79.65 | 58.08 | 44.11 | - | 78.38 | 73.24 | 66.69 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 79.65 | 58.08 | 44.11 | - | 78.38 | 73.24 | 66.69 |'
- en: '| DuQuant | 79.43 | 59.34 | 44.54 | 70.15 | 78.89 | 72.77 | 67.52 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 79.43 | 59.34 | 44.54 | 70.15 | 78.89 | 72.77 | 67.52 |'
- en: '| DuQuant+LWC | 80.09 | 57.95 | 45.05 | 68.72 | 79.17 | 73.09 | 67.35 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 80.09 | 57.95 | 45.05 | 68.72 | 79.17 | 73.09 | 67.35 |'
- en: '| LLaMA1-65B W6A6 | FP16 | 80.79 | 58.71 | 46.24 | 82.29 | 80.72 | 77.50 |
    71.04 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-65B W6A6 | FP16 | 80.79 | 58.71 | 46.24 | 82.29 | 80.72 | 77.50 |
    71.04 |'
- en: '| SmoothQuant | 80.25 | 57.92 | 45.50 | 80.22 | 80.18 | 74.76 | 69.80 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 80.25 | 57.92 | 45.50 | 80.22 | 80.18 | 74.76 | 69.80 |'
- en: '| OS+ | 79.67 | 55.68 | 45.22 | 80.02 | 78.03 | 73.95 | 68.76 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | 79.67 | 55.68 | 45.22 | 80.02 | 78.03 | 73.95 | 68.76 |'
- en: '| OmniQuant | 81.01 | 58.12 | 46.33 | 80.64 | 79.91 | 75.69 | 70.28 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 81.01 | 58.12 | 46.33 | 80.64 | 79.91 | 75.69 | 70.28 |'
- en: '| QLLM | 80.14 | 57.79 | 45.05 | - | 79.74 | 74.59 | 67.46 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 80.14 | 57.79 | 45.05 | - | 79.74 | 74.59 | 67.46 |'
- en: '| DuQuant | 80.96 | 59.09 | 46.76 | 82.20 | 80.68 | 77.27 | 71.16 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 80.96 | 59.09 | 46.76 | 82.20 | 80.68 | 77.27 | 71.16 |'
- en: '| DuQuant+LWC | 80.63 | 58.00 | 46.50 | 82.08 | 80.49 | 76.87 | 70.76 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 80.63 | 58.00 | 46.50 | 82.08 | 80.49 | 76.87 | 70.76 |'
- en: 'Table D6: Zero-shot common-sense QA ($\uparrow$) results of LLaMA2 models under
    6-bit WA quantization.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 表 D6：LLaMA2 模型在 6-bit WA 量化下的零样本常识 QA （$\uparrow$）结果。
- en: '| Model | Method | PIQA | ARC-E | ARC-C | BoolQ | HellaSwag | WinoGrande |
    Avg |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | PIQA | ARC-E | ARC-C | BoolQ | HellaSwag | WinoGrande | 平均 |'
- en: '| LLaMA2-7B W6A6 | FP16 | 76.88 | 53.54 | 40.53 | 71.13 | 72.96 | 67.25 | 63.72
    |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B W6A6 | FP16 | 76.88 | 53.54 | 40.53 | 71.13 | 72.96 | 67.25 | 63.72
    |'
- en: '| SmoothQuant | 75.57 | 53.62 | 39.93 | 69.54 | 71.76 | 66.14 | 62.76 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 75.57 | 53.62 | 39.93 | 69.54 | 71.76 | 66.14 | 62.76 |'
- en: '| OS+ | 76.22 | 52.74 | 40.70 | - | 71.89 | 65.19 | 61.35 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | 76.22 | 52.74 | 40.70 | - | 71.89 | 65.19 | 61.35 |'
- en: '| OmniQuant | 76.55 | 53.83 | 40.96 | 68.75 | 55.89 | 65.59 | 60.26 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 76.55 | 53.83 | 40.96 | 68.75 | 55.89 | 65.59 | 60.26 |'
- en: '| QLLM | 77.48 | 52.99 | 39.33 | - | 71.38 | 65.98 | 61.43 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 77.48 | 52.99 | 39.33 | - | 71.38 | 65.98 | 61.43 |'
- en: '| DuQuant | 76.99 | 52.99 | 40.87 | 70.40 | 72.49 | 67.32 | 63.51 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 76.99 | 52.99 | 40.87 | 70.40 | 72.49 | 67.32 | 63.51 |'
- en: '| DuQuant+LWC | 76.88 | 52.31 | 40.44 | 69.72 | 72.60 | 66.93 | 63.15 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 76.88 | 52.31 | 40.44 | 69.72 | 72.60 | 66.93 | 63.15 |'
- en: '| LLaMA2-13B W6A6 | FP16 | 79.05 | 57.91 | 44.20 | 69.02 | 76.60 | 69.69 |
    66.08 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B W6A6 | FP16 | 79.05 | 57.91 | 44.20 | 69.02 | 76.60 | 69.69 |
    66.08 |'
- en: '| SmoothQuant | 78.29 | 57.41 | 43.86 | 69.50 | 75.02 | 66.93 | 65.17 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 78.29 | 57.41 | 43.86 | 69.50 | 75.02 | 66.93 | 65.17 |'
- en: '| OS+ | 78.29 | 59.13 | 43.34 | - | 75.37 | 67.56 | 64.74 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | 78.29 | 59.13 | 43.34 | - | 75.37 | 67.56 | 64.74 |'
- en: '| OmniQuant | 78.24 | 57.58 | 43.86 | 71.10 | 75.52 | 68.35 | 65.78 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 78.24 | 57.58 | 43.86 | 71.10 | 75.52 | 68.35 | 65.78 |'
- en: '| AffineQuant | 78.35 | 57.58 | 43.34 | 66.73 | 74.71 | 68.59 | 64.88 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 78.35 | 57.58 | 43.34 | 66.73 | 74.71 | 68.59 | 64.88 |'
- en: '| QLLM | 78.78 | 58.29 | 43.77 | - | 75.10 | 68.43 | 64.87 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 78.78 | 58.29 | 43.77 | - | 75.10 | 68.43 | 64.87 |'
- en: '| DuQuant | 78.62 | 56.94 | 43.43 | 68.35 | 76.19 | 69.22 | 65.46 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 78.62 | 56.94 | 43.43 | 68.35 | 76.19 | 69.22 | 65.46 |'
- en: '| DuQuant+LWC | 78.94 | 57.95 | 44.11 | 68.81 | 76.17 | 68.98 | 65.83 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 78.94 | 57.95 | 44.11 | 68.81 | 76.17 | 68.98 | 65.83 |'
- en: '| LLaMA2-70B W6A6 | FP16 | 81.01 | 59.68 | 47.95 | 75.87 | 80.87 | 76.95 |
    70.39 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-70B W6A6 | FP16 | 81.01 | 59.68 | 47.95 | 75.87 | 80.87 | 76.95 |
    70.39 |'
- en: '| SmoothQuant | 79.87 | 57.32 | 45.65 | 77.13 | 79.01 | 74.03 | 68.84 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 79.87 | 57.32 | 45.65 | 77.13 | 79.01 | 74.03 | 68.84 |'
- en: '| OS+ | 79.33 | 59.09 | 47.18 | - | 79.46 | 75.06 | 68.02 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | 79.33 | 59.09 | 47.18 | - | 79.46 | 75.06 | 68.02 |'
- en: '| OmniQuant | 80.20 | 60.27 | 46.84 | - | 80.55 | 76.01 | 68.77 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 80.20 | 60.27 | 46.84 | - | 80.55 | 76.01 | 68.77 |'
- en: '| QLLM | 80.63 | 59.01 | 45.99 | - | 79.64 | 75.37 | 68.13 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| QLLM | 80.63 | 59.01 | 45.99 | - | 79.64 | 75.37 | 68.13 |'
- en: '| DuQuant | 80.96 | 59.39 | 47.27 | 77.34 | 80.70 | 76.40 | 70.34 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 80.96 | 59.39 | 47.27 | 77.34 | 80.70 | 76.40 | 70.34 |'
- en: '| DuQuant+LWC | 81.18 | 59.26 | 47.78 | 77.86 | 80.68 | 76.95 | 70.62 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 81.18 | 59.26 | 47.78 | 77.86 | 80.68 | 76.95 | 70.62 |'
- en: Appendix E More Ablation Studies
  id: totrans-448
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 更多消融研究
- en: E.1 End-to-end Time Speedup and Memory Saving
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 端到端时间加速和内存节省
- en: In Section [4.2](#S4.SS2 "4.2 Ablation Study ‣ 4 Experiment ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"), we analyzed
    the layer-wise time speedup for the LLaMA2-7B and LLaMA2-13B models. Here, we
    present comprehensive end-to-end results for time speedup and memory savings achieved
    with the LLaMA2-7B model on a single NVIDIA RTX 3090 GPU. As shown in Table [E7](#A5.T7
    "Table E7 ‣ E.1 End-to-end Time Speedup and Memory Saving ‣ Appendix E More Ablation
    Studies ‣ Rotation and Permutation for Advanced Outlier Management and Efficient
    Quantization of LLMs"), DuQuant achieves a maximum speedup of $2.01\times$ through
    quantization. These results underscore the efficiency of DuQuant in optimizing
    resource utilization, highlighting its potential to enhance performance and reduce
    costs in deploying large language models, particularly in resource-constrained
    environments.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [4.2](#S4.SS2 "4.2 消融研究 ‣ 4 实验 ‣ 高级离群点管理和高效量化 LLM 的旋转和置换") 节中，我们分析了 LLaMA2-7B
    和 LLaMA2-13B 模型的层级时间加速。在这里，我们展示了在单个 NVIDIA RTX 3090 GPU 上使用 LLaMA2-7B 模型所实现的全面端到端时间加速和内存节省结果。如表
    [E7](#A5.T7 "表 E7 ‣ E.1 端到端时间加速和内存节省 ‣ 附录 E 更多消融研究 ‣ 高级离群点管理和高效量化 LLM 的旋转和置换")
    所示，DuQuant 通过量化实现了最高 $2.01\times$ 的加速。这些结果强调了 DuQuant 在优化资源利用方面的效率，突显了它在部署大型语言模型时提高性能和降低成本的潜力，特别是在资源受限的环境中。
- en: 'Table E7: End-to-end prefilling speedup on LLaMA2-7B model.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E7: LLaMA2-7B 模型端到端预填充加速。'
- en: '| Batch Size | FP16 Time | DuQuant Time | Speedup |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | FP16 时间 | DuQuant 时间 | 加速 |'
- en: '| 1 | 568ms | 294ms | 1.93$\times$ |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 568ms | 294ms | 1.93$\times$ |'
- en: '| 2 | 1003ms | 509ms | 1.97$\times$ |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1003ms | 509ms | 1.97$\times$ |'
- en: '| 3 | 1449ms | 720ms | 2.01$\times$ |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1449ms | 720ms | 2.01$\times$ |'
- en: 'Table E8: Peak memory usage during prefilling phase of LLaMA2-7B model.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E8: LLaMA2-7B 模型预填充阶段的峰值内存使用情况。'
- en: '| Batch Size | FP16 Mem. | DuQuant Mem. | Saving Factor |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | FP16 内存 | DuQuant 内存 | 节省因子 |'
- en: '| 1 | 15.28GB | 4.79GB | 3.20$\times$ |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 15.28GB | 4.79GB | 3.20$\times$ |'
- en: '| 2 | 17.94GB | 5.94GB | 3.02$\times$ |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 17.94GB | 5.94GB | 3.02$\times$ |'
- en: '| 3 | 20.56GB | 7.10GB | 2.90$\times$ |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 20.56GB | 7.10GB | 2.90$\times$ |'
- en: E.2 Effects of Rotation Matrix
  id: totrans-461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 旋转矩阵的效果
- en: Ablation of Rotation Block Size.
  id: totrans-462
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 旋转块大小的消融。
- en: To further explore the impact of rotation block size, we apply varying block
    sizes in the rotation matrices to both LLaMA2-7B and LLaMA2-13B models and evaluate
    the perplexity of the quantized models. The results, presented in Table [E9](#A5.T9
    "Table E9 ‣ Ablation of Rotation Block Size. ‣ E.2 Effects of Rotation Matrix
    ‣ Appendix E More Ablation Studies ‣ Rotation and Permutation for Advanced Outlier
    Management and Efficient Quantization of LLMs"), indicate that increasing block
    sizes generally improves model performance. This improvement occurs because larger
    block sizes allow outliers to be distributed across more channels, evening out
    values throughout the activation/weight matrix thereby enhancing quantization
    accuracy and performance. Additionally, quantization runtime decreases with larger
    block sizes, likely due to more efficient transformations during the reshaping
    of original activation/weight matrices. Consequently, we adopt 128 as our rotation
    block size for all experiments for efficiency and effectiveness.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 为进一步探讨旋转块大小的影响，我们在 LLaMA2-7B 和 LLaMA2-13B 模型中应用不同的块大小，并评估量化模型的困惑度。结果如表 [E9](#A5.T9
    "表 E9 ‣ 旋转块大小的消融 ‣ E.2 旋转矩阵的影响 ‣ 附录 E 更多消融研究 ‣ 先进的异常值管理和高效量化的旋转与排列") 所示，增加块大小通常能改善模型性能。这种改进发生是因为较大的块大小允许异常值分布在更多通道中，从而在激活/权重矩阵中平衡值，提升量化准确性和性能。此外，较大的块大小使得量化运行时间减少，这可能是因为在原始激活/权重矩阵重新塑形过程中变换效率更高。因此，我们为所有实验采用
    128 作为旋转块大小，以确保效率和效果。
- en: 'Table E9: Impact of rotation block size.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 表 E9：旋转块大小的影响。
- en: '| Block Size | LLaMA2-7B | LLaMA2-13B |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| 块大小 | LLaMA2-7B | LLaMA2-13B |'
- en: '| WikiText2 $\downarrow$ | Time/s |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 $\downarrow$ | 时间/秒 |'
- en: '| 4 | 18.69 | 26.48 | 64.4 | 8.81 | 13.03 | 97.7 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 18.69 | 26.48 | 64.4 | 8.81 | 13.03 | 97.7 |'
- en: '| 8 | 10.77 | 15.04 | 53.8 | 7.02 | 9.68 | 80.8 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 10.77 | 15.04 | 53.8 | 7.02 | 9.68 | 80.8 |'
- en: '| 16 | 8.69 | 11.46 | 48.2 | 6.12 | 8.12 | 75.2 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 8.69 | 11.46 | 48.2 | 6.12 | 8.12 | 75.2 |'
- en: '| 32 | 6.96 | 8.85 | 48.3 | 5.61 | 7.35 | 76.2 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 6.96 | 8.85 | 48.3 | 5.61 | 7.35 | 76.2 |'
- en: '| 64 | 6.38 | 8.07 | 50.1 | 5.45 | 7.13 | 74.0 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| 64 | 6.38 | 8.07 | 50.1 | 5.45 | 7.13 | 74.0 |'
- en: '| 128 | 6.28 | 7.90 | 48.6 | 5.42 | 7.05 | 74.0 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 6.28 | 7.90 | 48.6 | 5.42 | 7.05 | 74.0 |'
- en: Ablation of Rotation Times.
  id: totrans-473
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 旋转次数的消融。
- en: Identifying the optimal rotation matrix $\mathbf{R}$ for all our experiments,
    as it offers the optimal balance between model performance and time usage.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 确定所有实验的最佳旋转矩阵 $\mathbf{R}$，因为它在模型性能和时间使用之间提供了最佳平衡。
- en: 'Table E10: Impact of rotation times.'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 表 E10：旋转次数的影响。
- en: '|  | LLaMA2-7B | LLaMA2-13B |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA2-7B | LLaMA2-13B |'
- en: '| Rotation Times | WikiText2 $\downarrow$ | Time/s |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| 旋转次数 | WikiText2 $\downarrow$ | 时间/秒 |'
- en: '| 1 | 6.60 | 8.41 | 22.9 | 5.48 | 7.12 | 37.7 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 6.60 | 8.41 | 22.9 | 5.48 | 7.12 | 37.7 |'
- en: '| 4 | 6.34 | 8.04 | 22.6 | 5.41 | 7.06 | 38.7 |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 6.34 | 8.04 | 22.6 | 5.41 | 7.06 | 38.7 |'
- en: '| 16 | 6.32 | 7.98 | 28.8 | 5.43 | 7.05 | 41.8 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 6.32 | 7.98 | 28.8 | 5.43 | 7.05 | 41.8 |'
- en: '| 64 | 6.34 | 7.98 | 29.0 | 5.43 | 7.06 | 47.0 |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| 64 | 6.34 | 7.98 | 29.0 | 5.43 | 7.06 | 47.0 |'
- en: '| 256 | 6.28 | 7.90 | 48.6 | 5.42 | 7.05 | 74.0 |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| 256 | 6.28 | 7.90 | 48.6 | 5.42 | 7.05 | 74.0 |'
- en: '| 1024 | 6.31 | 8.01 | 129.7 | 5.46 | 7.12 | 179.8 |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 1024 | 6.31 | 8.01 | 129.7 | 5.46 | 7.12 | 179.8 |'
- en: E.3 Effects of Permutation Algorithm.
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.3 排列算法的效果。
- en: As discussed in Section [3.2](#S3.SS2 "3.2 The proposed DuQuant Method ‣ 3 Method
    ‣ Rotation and Permutation for Advanced Outlier Management and Efficient Quantization
    of LLMs"), rotation transformations within each block are limited and unable to
    redistribute outliers across different blocks. To address this, we introduce a
    permutation transformation aimed at balancing outliers more comprehensively. Our
    primary goal is to minimize the variance among different blocks, as outlined in
    Eqn. ([4](#S3.E4 "In The Permutation Transformation. ‣ 3.2 The proposed DuQuant
    Method ‣ 3 Method ‣ Rotation and Permutation for Advanced Outlier Management and
    Efficient Quantization of LLMs")). We explore several optimization algorithms,
    with the results detailed in Table [E11](#A5.T11 "Table E11 ‣ E.3 Effects of Permutation
    Algorithm. ‣ Appendix E More Ablation Studies ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs"). Note that the variance
    values are measured on activation values of the query project in the first layer
    of each model, and the time in the table represents the runtime of calibration.
    The Zigzag permutation notably reduces the variance to 3.0e-4, achieving this
    with minimal time expenditure and yielding competitive perplexity results. While
    Simulated Annealing slightly outperforms Zigzag in terms of perplexity for the
    LLaMA2-7B model, it was significantly more time-consuming, and the marginal gains
    did not justify the additional complexity. Therefore, we select Zigzag permutation
    as our preferred method, leading to smoother outlier distribution and more effective
    quantized models.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[3.2节](#S3.SS2 "3.2 提出的 DuQuant 方法 ‣ 3 方法 ‣ 高级离群点管理和 LLM 高效量化的旋转和排列")中讨论的，每个块内的旋转变换是有限的，无法在不同块之间重新分配离群点。为了解决这个问题，我们引入了一种排列变换，旨在更全面地平衡离群点。我们的主要目标是最小化不同块之间的方差，如公式([4](#S3.E4
    "在排列变换中。 ‣ 3.2 提出的 DuQuant 方法 ‣ 3 方法 ‣ 高级离群点管理和 LLM 高效量化的旋转和排列"))所述。我们探索了几种优化算法，结果详见表[E11](#A5.T11
    "表 E11 ‣ E.3 排列算法的影响 ‣ 附录 E 更多消融研究 ‣ 高级离群点管理和 LLM 高效量化的旋转和排列")。需要注意的是，方差值是基于每个模型第一层查询项目的激活值测量的，表中的时间表示校准的运行时间。Zigzag
    排列显著将方差减少至 3.0e-4，且耗时最少，并且取得了具有竞争力的困惑度结果。虽然模拟退火在 LLaMA2-7B 模型的困惑度上略胜于 Zigzag，但其耗时显著更长，且边际收益不足以证明其额外复杂性。因此，我们选择
    Zigzag 排列作为首选方法，这使得离群点分布更加平滑，并且量化模型更为有效。
- en: 'Table E11: Impact of channel permutation algorithm.'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 表 E11：通道排列算法的影响。
- en: '|  | LLaMA2-7B | LLaMA2-13B |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA2-7B | LLaMA2-13B |'
- en: '| Permutation Method | WikiText2 $\downarrow$ | Variance | Time/s |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| 排列方法 | WikiText2 $\downarrow$ | 方差 | 时间/秒 |'
- en: '| w.o. Permutation | 7.92 | 10.64 | 3.9e-2 | 27.5 | 5.96 | 7.94 | 3.1e-2 |
    44.7 |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| 无排列 | 7.92 | 10.64 | 3.9e-2 | 27.5 | 5.96 | 7.94 | 3.1e-2 | 44.7 |'
- en: '| Random | 6.40 | 8.08 | 4.9e-3 | 89.5 | 5.43 | 7.07 | 3.9e-3 | 148.6 |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 6.40 | 8.08 | 4.9e-3 | 89.5 | 5.43 | 7.07 | 3.9e-3 | 148.6 |'
- en: '| Simulated Annealing | 6.26 | 7.89 | 1.7e-4 | 769.6 | 5.42 | 7.06 | 1.5e-4
    | 1257.8 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| 模拟退火 | 6.26 | 7.89 | 1.7e-4 | 769.6 | 5.42 | 7.06 | 1.5e-4 | 1257.8 |'
- en: '| Zigzag | 6.28 | 7.90 | 3.0e-4 | 48.6 | 5.42 | 7.05 | 2.5e-4 | 74.0 |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| Zigzag | 6.28 | 7.90 | 3.0e-4 | 48.6 | 5.42 | 7.05 | 2.5e-4 | 74.0 |'
- en: E.4 Effects of Calibration Datasets
  id: totrans-493
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.4 校准数据集的影响
- en: Ablation of Different Calibration Datasets.
  id: totrans-494
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同校准数据集的消融。
- en: We apply our DuQuant to quantize the LLaMA2-7B model using different calibration
    datasets, with results presented in Table [E12](#A5.T12 "Table E12 ‣ Ablation
    of Different Calibration Datasets. ‣ E.4 Effects of Calibration Datasets ‣ Appendix
    E More Ablation Studies ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs"). It can be observed that the selection of
    calibration datasets has a relatively minor impact on quantization performance.
    This is because our method uses the calibration data solely to identify outlier
    channels, rather than for gradient-based parameter learning as seen in methods
    like OmniQuant [[42](#bib.bib42)] and AffineQuant [[36](#bib.bib36)]. This ablation
    study underscores the robustness of our DuQuant method.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 DuQuant 应用于使用不同校准数据集对 LLaMA2-7B 模型进行量化，结果见表[E12](#A5.T12 "表 E12 ‣ 不同校准数据集的消融
    ‣ E.4 校准数据集的影响 ‣ 附录 E 更多消融研究 ‣ 高级离群点管理和 LLM 高效量化的旋转和排列")。可以观察到，校准数据集的选择对量化性能的影响相对较小。这是因为我们的方法仅使用校准数据来识别离群通道，而不是像
    OmniQuant [[42](#bib.bib42)] 和 AffineQuant [[36](#bib.bib36)] 这类方法一样用于基于梯度的参数学习。这项消融研究突显了我们
    DuQuant 方法的鲁棒性。
- en: 'Table E12: Ablation of calibration datasets.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E12: 校准数据集的消融研究。'
- en: '| LLaMA2-7B | Eval. |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | 评估 |'
- en: '| WikiText2 $\downarrow$ |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 $\downarrow$ |'
- en: '| Calib. | WikiText2 | 6.28 | 7.90 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| 校准 | WikiText2 | 6.28 | 7.90 |'
- en: '| C4 | 6.25 | 7.87 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| C4 | 6.25 | 7.87 |'
- en: Calibration-free Quantization.
  id: totrans-501
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无校准量化。
- en: To further explore the robustness of DuQuant under varying calibration conditions,
    we generate random calibration data within the vocabulary range of the model,
    setting the sample count to 256. The results, shown in Table [E13](#A5.T13 "Table
    E13 ‣ Calibration-free Quantization. ‣ E.4 Effects of Calibration Datasets ‣ Appendix
    E More Ablation Studies ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs"), indicate that even in calibration-free settings,
    our method continues to perform well, achieving results that are competitively
    close to those obtained with actual calibration data. This demonstrates that DuQuant could
    provide a viable solution in real-world scenarios where obtaining specific calibration
    data is challenging or impossible. The ability to maintain high performance without
    traditional calibration data opens avenues for deploying quantized models in environments
    with strict privacy or data availability limitations, suggesting a promising direction
    for future research to enhance model adaptability and deployment flexibility.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探索 DuQuant 在不同校准条件下的鲁棒性，我们在模型的词汇范围内生成随机校准数据，样本数量设置为 256。结果如表 [E13](#A5.T13
    "Table E13 ‣ Calibration-free Quantization. ‣ E.4 Effects of Calibration Datasets
    ‣ Appendix E More Ablation Studies ‣ Rotation and Permutation for Advanced Outlier
    Management and Efficient Quantization of LLMs") 所示，即使在无校准设置下，我们的方法仍表现良好，结果与实际校准数据获得的结果接近。这表明
    DuQuant 在获取特定校准数据困难或不可能的实际场景中可能提供了一个可行的解决方案。没有传统校准数据的高性能保持能力为在隐私或数据可用性限制严格的环境中部署量化模型开辟了新的途径，建议未来的研究可以在增强模型适应性和部署灵活性方面取得进展。
- en: 'Table E13: Calibration-free quantization, where we generate random data within
    vocabulary range.'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E13: 无校准量化，其中我们在词汇范围内生成随机数据。'
- en: '| LLaMA2-7B | Eval. |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | 评估 |'
- en: '| WikiText2 $\downarrow$ |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 $\downarrow$ |'
- en: '| Calib. | Randomly Generated | 6.25 | 7.86 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| 校准 | 随机生成 | 6.25 | 7.86 |'
- en: '| WikiText2 | 6.25 | 7.87 |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 | 6.25 | 7.87 |'
- en: '| LLaMA2-13B | Eval. |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B | 评估 |'
- en: '| WikiText2 $\downarrow$ |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 $\downarrow$ |'
- en: '| Calib. | Randomly Generated | 5.45 | 7.05 |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| 校准 | 随机生成 | 5.45 | 7.05 |'
- en: '| WikiText2 | 5.44 | 7.05 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 | 5.44 | 7.05 |'
- en: Ablation of Different Numbers of Calibration Samples.
  id: totrans-512
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 校准样本数量的消融研究。
- en: We utilize our DuQuant to quantize the LLaMA2-7B model using varying numbers
    of calibration samples from the WikiText2 dataset, with results detailed in Table [E14](#A5.T14
    "Table E14 ‣ Ablation of Different Numbers of Calibration Samples. ‣ E.4 Effects
    of Calibration Datasets ‣ Appendix E More Ablation Studies ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"). Interestingly,
    the quantization performance shows a low correlation with the number of samples,
    demonstrating the robustness of DuQuantṪhis stability arises because we utilize
    the mean activation values from these samples to construct our rotation matrices.
    Since we average the activations, the influence of any single, potentially non-representative
    sample is minimized, ensuring consistent performance. Notably, as we use mean
    values, the time cost of our quantization process remains constant regardless
    of the number of samples, enhancing the efficiency.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用 DuQuant 对 LLaMA2-7B 模型进行量化，使用来自 WikiText2 数据集的不同数量的校准样本，结果详见表 [E14](#A5.T14
    "Table E14 ‣ Ablation of Different Numbers of Calibration Samples. ‣ E.4 Effects
    of Calibration Datasets ‣ Appendix E More Ablation Studies ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs")。有趣的是，量化性能与样本数量的相关性较低，展示了
    DuQuant 的鲁棒性。这种稳定性来源于我们利用这些样本的均值激活值来构建旋转矩阵。由于我们对激活值进行平均，任何单个可能不具代表性的样本的影响被最小化，从而确保了性能的一致性。值得注意的是，由于我们使用均值，量化过程的时间成本保持恒定，无论样本数量多少，效率得到提高。
- en: 'Table E14: Ablation of different numbers in the calibration dataset.'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E14: 校准数据集中不同数量的消融研究。'
- en: '| # of Samples | WikiText2 $\downarrow$ |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| 样本数量 | WikiText2 $\downarrow$ |'
- en: '| 16 | 6.29 | 7.88 |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 6.29 | 7.88 |'
- en: '| 32 | 6.31 | 7.99 |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 6.31 | 7.99 |'
- en: '| 64 | 6.29 | 7.88 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| 64 | 6.29 | 7.88 |'
- en: '| 128 | 6.28 | 7.90 |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 6.28 | 7.90 |'
- en: '| 256 | 6.23 | 7.88 |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| 256 | 6.23 | 7.88 |'
- en: Appendix F Detailed Comparison with QuaRot
  id: totrans-521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '附录 F: 与 QuaRot 的详细比较'
- en: 'Table F15: Evaluation results between QuaRot and DuQuant under W4A4 quantization.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '表 F15: QuaRot 和 DuQuant 在 W4A4 量化下的评估结果。'
- en: '| Model | Method | WikiText2 $\downarrow$ |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | WikiText2 $\downarrow$ |'
- en: '| LLaMA1-7B W4A4 | FP16 | 5.68 | 7.08 | 77.47 | 52.48 | 41.46 | 73.08 | 73.00
    | 67.07 | 64.09 |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-7B W4A4 | FP16 | 5.68 | 7.08 | 77.47 | 52.48 | 41.46 | 73.08 | 73.00
    | 67.07 | 64.09 |'
- en: '| QuaRot-RTN | 7.08 | 8.73 | 74.59 | 48.57 | 36.01 | 68.99 | 65.69 | 58.56
    | 46.03 |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| QuaRot-RTN | 7.08 | 8.73 | 74.59 | 48.57 | 36.01 | 68.99 | 65.69 | 58.56
    | 46.03 |'
- en: '| QuaRot-GPTQ | 6.44 | 7.87 | 76.17 | 49.96 | 38.23 | 70.80 | 69.29 | 63.06
    | 61.25 |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| QuaRot-GPTQ | 6.44 | 7.87 | 76.17 | 49.96 | 38.23 | 70.80 | 69.29 | 63.06
    | 61.25 |'
- en: '| DuQuant | 6.40 | 7.84 | 76.44 | 50.04 | 38.99 | 70.98 | 69.39 | 64.72 | 61.76
    |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 6.40 | 7.84 | 76.44 | 50.04 | 38.99 | 70.98 | 69.39 | 64.72 | 61.76
    |'
- en: '| DuQuant+LWC | 6.18 | 7.73 | 76.22 | 50.04 | 38.31 | 70.09 | 69.82 | 62.59
    | 61.18 |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 6.18 | 7.73 | 76.22 | 50.04 | 38.31 | 70.09 | 69.82 | 62.59
    | 61.18 |'
- en: '| LLaMA2-7B W4A4 | FP16 | 5.47 | 6.97 | 76.88 | 53.54 | 40.53 | 71.13 | 72.96
    | 67.25 | 63.72 |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B W4A4 | FP16 | 5.47 | 6.97 | 76.88 | 53.54 | 40.53 | 71.13 | 72.96
    | 67.25 | 63.72 |'
- en: '| QuaRot-RTN | 9.66 | 11.98 | 69.48 | 46.25 | 32.76 | 64.80 | 60.75 | 56.67
    | 44.04 |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| QuaRot-RTN | 9.66 | 11.98 | 69.48 | 46.25 | 32.76 | 64.80 | 60.75 | 56.67
    | 44.04 |'
- en: '| QuaRot-GPTQ | 6.39 | 8.15 | 75.15 | 49.15 | 36.68 | 67.89 | 68.87 | 61.33
    | 59.85 |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| QuaRot-GPTQ | 6.39 | 8.15 | 75.15 | 49.15 | 36.68 | 67.89 | 68.87 | 61.33
    | 59.85 |'
- en: '| DuQuant | 6.28 | 7.90 | 75.24 | 51.89 | 36.77 | 67.86 | 69.54 | 62.12 | 60.57
    |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 6.28 | 7.90 | 75.24 | 51.89 | 36.77 | 67.86 | 69.54 | 62.12 | 60.57
    |'
- en: '| DuQuant+LWC | 6.08 | 7.79 | 75.68 | 50.00 | 37.46 | 69.24 | 69.74 | 63.93
    | 61.01 |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 6.08 | 7.79 | 75.68 | 50.00 | 37.46 | 69.24 | 69.74 | 63.93
    | 61.01 |'
- en: '| LLaMA3-8B W4A4 | FP16 | 6.14 | 8.88 | 80.85 | 77.78 | 53.41 | 81.28 | 79.16
    | 72.84 | 74.22 |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-8B W4A4 | FP16 | 6.14 | 8.88 | 80.85 | 77.78 | 53.41 | 81.28 | 79.16
    | 72.84 | 74.22 |'
- en: '| QuaRot-RTN | 13.89 | 17.59 | 69.64 | 57.58 | 34.56 | 66.76 | 63.46 | 62.75
    | 59.13 |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| QuaRot-RTN | 13.89 | 17.59 | 69.64 | 57.58 | 34.56 | 66.76 | 63.46 | 62.75
    | 59.13 |'
- en: '| QuaRot-GPTQ | 8.69 | 12.40 | 74.54 | 67.38 | 40.61 | 70.43 | 70.47 | 65.11
    | 64.76 |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| QuaRot-GPTQ | 8.69 | 12.40 | 74.54 | 67.38 | 40.61 | 70.43 | 70.47 | 65.11
    | 64.76 |'
- en: '| DuQuant | 8.53 | 12.01 | 76.93 | 70.88 | 45.05 | 74.59 | 73.17 | 66.14 |
    67.79 |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 8.53 | 12.01 | 76.93 | 70.88 | 45.05 | 74.59 | 73.17 | 66.14 |
    67.79 |'
- en: '| DuQuant+LWC | 8.06 | 11.29 | 76.22 | 70.41 | 43.69 | 74.34 | 73.87 | 67.80
    | 67.72 |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant+LWC | 8.06 | 11.29 | 76.22 | 70.41 | 43.69 | 74.34 | 73.87 | 67.80
    | 67.72 |'
- en: 'Table F16: Matrices comparison between DuQuant and QuaRot under W4A4 quantization.'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 F16：在 W4A4 量化下 DuQuant 和 QuaRot 之间的矩阵比较。
- en: '| Model | LLaMA2-7B | LLaMA2-13B |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | LLaMA2-7B | LLaMA2-13B |'
- en: '| Dataset | WikiText2 $\downarrow$ |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | WikiText2 $\downarrow$ |'
- en: '| QuaRot | 9.66 | 11.98 | 6.73 | 8.69 |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| QuaRot | 9.66 | 11.98 | 6.73 | 8.69 |'
- en: '| DuQuant | 7.92 | 10.64 | 5.96 | 7.94 |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 7.92 | 10.64 | 5.96 | 7.94 |'
- en: 'Table F17: Quantization runtime comparison on a single NVIDIA A100 80G GPU.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 F17：单个 NVIDIA A100 80G GPU 上的量化运行时间比较。
- en: '| Model | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
- en: '| QuaRot | 20min | 36min | 5.1h |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| QuaRot | 20分钟 | 36分钟 | 5.1小时 |'
- en: '| DuQuant | 50s | 71s | 270s |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| DuQuant | 50秒 | 71秒 | 270秒 |'
- en: 'In this section, we present a detailed comparison between our DuQuant and QuaRot [[2](#bib.bib2)].
    QuaRot employs Hadamard matrices to mitigate outliers in activations and utilizes
    the GPTQ algorithm for weight quantization to achieve competitive performance.
    However, our DuQuant method demonstrates several distinct advantages:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们展示了 **DuQuant** 和 **QuaRot** [[2](#bib.bib2)] 的详细比较。**QuaRot** 使用 Hadamard
    矩阵来减轻激活中的异常值，并利用 GPTQ 算法进行权重量化以实现竞争性能。然而，我们的 **DuQuant** 方法展示了几个明显的优势：
- en: •
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Effective Use of Prior Knowledge: DuQuant leverages prior knowledge to accurately
    target and eliminate outliers through multiple rotations, achieving a smoother
    activation distribution compared to the Hadamard transformation, as demonstrated
    in Figure [7](#S4.T7 "Table 7 ‣ Comparison with QuaRot [2] ‣ 4.2 Ablation Study
    ‣ 4 Experiment ‣ Rotation and Permutation for Advanced Outlier Management and
    Efficient Quantization of LLMs").'
  id: totrans-550
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 先验知识的有效利用：**DuQuant** 利用先验知识通过多次旋转准确地定位和消除异常值，相比于 Hadamard 变换，实现了更平滑的激活分布，如图
    [7](#S4.T7 "表 7 ‣ 与 QuaRot [2] 的比较 ‣ 4.2 消融研究 ‣ 4 实验 ‣ 高级异常值管理和高效量化的旋转与排列") 所示。
- en: •
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Efficient Channel Permutation: Our channel permutation not only further smooths
    outlier features but also benefits from rapid implementation, enhancing overall
    performance.'
  id: totrans-552
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高效的通道排列：我们的通道排列不仅进一步平滑了异常值特征，还因其快速实现而提升了整体性能。
- en: •
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Simultaneous Weight Matrix Smoothing: Unlike QuaRot, DuQuant directly and efficiently
    smooths the weight matrix, avoiding the time-consuming GPTQ algorithm and accelerating
    the quantization process, as demonstrated high quantization efficiency in Table [F17](#A6.T17
    "Table F17 ‣ Appendix F Detailed Comparison with QuaRot ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs").'
  id: totrans-554
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同时加权矩阵平滑：与QuaRot不同，DuQuant直接且高效地平滑加权矩阵，避免了耗时的GPTQ算法，加速了量化过程，如表[F17](#A6.T17
    "Table F17 ‣ Appendix F Detailed Comparison with QuaRot ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs")中所示的高量化效率。
- en: Experimental results underscore the superiority of DuQuant over QuaRot. For
    a fair comparison, we reproduce the QuaRot under 4-bit per-channel weight and
    per-token activation asymmetric quantization. Table [F15](#A6.T15 "Table F15 ‣
    Appendix F Detailed Comparison with QuaRot ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs") displays the perplexity
    (PPL) and zero-shot accuracy for models LLaMA1-7B, LLaMA2-7B, and LLaMA3-8B. Our
    DuQuant method consistently outperforms QuaRot-RTN across all benchmarks, showcasing
    our advanced weight matrix management. Furthermore, compared to QuaRot-GPTQ, DuQuant and
    DuQuant+LWC achieve better average accuracy across six QA tasks and demonstrate
    superior performance on the WikiText and C4 datasets, particularly for LLaMA3-8B.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果强调了DuQuant优于QuaRot的优势。为了公平比较，我们在4位每通道权重和每标记激活不对称量化下重现了QuaRot。表[F15](#A6.T15
    "Table F15 ‣ Appendix F Detailed Comparison with QuaRot ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs")展示了模型LLaMA1-7B、LLaMA2-7B和LLaMA3-8B的困惑度（PPL）和零样本准确率。我们的DuQuant方法在所有基准测试中均优于QuaRot-RTN，展示了我们先进的加权矩阵管理。此外，与QuaRot-GPTQ相比，DuQuant及DuQuant+LWC在六个QA任务中的平均准确率更高，并且在WikiText和C4数据集上的表现优异，特别是LLaMA3-8B。
- en: Additionally, we assess the effectiveness of the rotation matrices utilized
    in DuQuant  which incorporate prior knowledge against the Hadamard matrices used
    in QuaRot. For a fair comparison, we omit the permutation step in DuQuant and
    directly contrast it with QuaRot-RTN. Results in Table [F16](#A6.T16 "Table F16
    ‣ Appendix F Detailed Comparison with QuaRot ‣ Rotation and Permutation for Advanced
    Outlier Management and Efficient Quantization of LLMs") show that our DuQuant without
    permutation outperforms QuaRot by a clear margin, which confirms that our rotation
    transformation is more effective than Hadamard by leveraging prior knowledge.
    It is worth noting that because a Hadamard matrix is orthogonal and symmetric,
    it multiplies by itself to yield the identity matrix. In other words, the Hadamard
    matrix is not suitable for greedy searches aimed at finding smaller outliers.
    These findings differentiate DuQuant from QuaRot and highlight the effectiveness
    of our approach in managing outliers for post-training quantization of large language
    models.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们评估了DuQuant中使用的旋转矩阵的有效性，这些矩阵结合了先前的知识，并与QuaRot中使用的Hadamard矩阵进行比较。为了公平比较，我们省略了DuQuant中的排列步骤，并直接与QuaRot-RTN进行对比。表格[F16](#A6.T16
    "Table F16 ‣ Appendix F Detailed Comparison with QuaRot ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs")中的结果显示，我们的无排列DuQuant明显优于QuaRot，这确认了我们的旋转变换通过利用先前的知识比Hadamard更有效。值得注意的是，由于Hadamard矩阵是正交且对称的，它自乘会产生单位矩阵。换句话说，Hadamard矩阵不适合用于贪婪搜索以找到较小的异常值。这些发现使DuQuant与QuaRot区分开来，并突出了我们的方法在后训练量化大语言模型时管理异常值的有效性。
- en: Appendix G Limitations and Broader Impacts
  id: totrans-557
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 限制与更广泛的影响
- en: Limitations.
  id: totrans-558
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 限制。
- en: The primary limitation of our method is the lack of a specialized strategy for
    calibration data selection. We adhere to established practices [[42](#bib.bib42),
    [36](#bib.bib36), [31](#bib.bib31), [61](#bib.bib61), [2](#bib.bib2), [1](#bib.bib1)]
    by randomly selecting 128 samples from the WikiText2 dataset to compute the mean
    embeddings that inform our rotation matrix and zigzag permutation order. We also
    explore the possibility of calibration-free quantization and show some promising
    results. However, further investigating more tailored choices for calibration
    data can potentially enhance the performance of our quantized models. We leave
    this for future study.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的主要限制是缺乏针对校准数据选择的专门策略。我们遵循既定的实践[[42](#bib.bib42), [36](#bib.bib36), [31](#bib.bib31),
    [61](#bib.bib61), [2](#bib.bib2), [1](#bib.bib1)]，通过从 WikiText2 数据集中随机选择 128 个样本来计算均值嵌入，从而为我们的旋转矩阵和之字形置换顺序提供信息。我们还探索了无校准量化的可能性，并展示了一些有希望的结果。然而，进一步研究更为量身定制的校准数据选择可能会提升我们量化模型的性能。我们将这项工作留待未来研究。
- en: Broader Impacts.
  id: totrans-560
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更广泛的影响。
- en: Our work identifies the presence of massive outliers in down-projection layer
    of FFN modules, which significantly complicates low-bit weight-activation quantization.
    To address this challenge, we implement a combination of rotation matrices and
    permutations to effectively smooth both massive and uniform outliers, proving
    both fast and effective. Consequently, we establish a new state-of-the-art for
    INT4 weight-activation post-training quantization methods. Our approach aims to
    accelerate large language models and reduce memory usage during deployment, offering
    substantial benefits to the field of LLM research. These advancements could lead
    to more efficient and accessible LLM applications, facilitating broader usage
    and enabling more sustainable AI implementations.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作发现 FFN 模块的下投影层中存在大量异常值，这极大地复杂化了低位权重-激活量化。为了解决这一挑战，我们实施了旋转矩阵和置换的组合，以有效平滑大规模和均匀的异常值，证明了其快速且有效。因此，我们为
    INT4 权重-激活后训练量化方法确立了新的最先进技术。我们的方法旨在加速大型语言模型，并在部署过程中减少内存使用，为 LLM 研究领域提供了实质性利益。这些进展可能会导致更高效、更易获取的
    LLM 应用，促进更广泛的使用并实现更可持续的 AI 实施。
- en: Appendix H More Visualizations
  id: totrans-562
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 更多可视化
- en: We provide additional visualizations of normal and massive outliers in various
    models (LLaMA1, LLaMA2, Vicuna-v1.5) from Figure [H1](#A8.F1 "Figure H1 ‣ Appendix
    H More Visualizations ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs") to Figure [H8](#A8.F8 "Figure H8 ‣ Appendix
    H More Visualizations ‣ Rotation and Permutation for Advanced Outlier Management
    and Efficient Quantization of LLMs"). In each figure, the left side illustrates
    changes in normal outliers before and after applying our rotation and permutation
    transformations, while the right side shows the changes in massive outliers before
    and after transformations. It is evident that massive outliers consistently occur
    in the down-projection layer of the FFN module across all models, supporting our
    findings discussed in Section [2](#S2 "2 Motivation ‣ Rotation and Permutation
    for Advanced Outlier Management and Efficient Quantization of LLMs"). Conversely,
    normal outliers appear in different modules within the transformation block. For
    instance, Figure [H3](#A8.F3 "Figure H3 ‣ Appendix H More Visualizations ‣ Rotation
    and Permutation for Advanced Outlier Management and Efficient Quantization of
    LLMs") shows normal outliers at the up-projection layer of the FFN module in LLaMA1-13B.
    Significantly, both massive and normal outliers are reduced markedly after our
    rotation and permutation transformations, leading to easier quantization of activations.
    This underscores the effectiveness of our RAP in managing outlier features across
    diverse LLM models.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了从图 [H1](#A8.F1 "图 H1 ‣ 附录 H 更多可视化 ‣ 高级异常值管理和高效量化 LLM 的旋转与置换") 到图 [H8](#A8.F8
    "图 H8 ‣ 附录 H 更多可视化 ‣ 高级异常值管理和高效量化 LLM 的旋转与置换") 的各种模型（LLaMA1、LLaMA2、Vicuna-v1.5）中的正常和大规模异常值的额外可视化。在每个图中，左侧展示了应用我们的旋转和置换变换前后正常异常值的变化，而右侧则展示了大规模异常值的变化。显然，大规模异常值在所有模型的
    FFN 模块的下投影层中始终出现，这支持了我们在第 [2](#S2 "2 动机 ‣ 高级异常值管理和高效量化 LLM 的旋转与置换") 节中讨论的发现。相反，正常异常值出现在变换块中的不同模块中。例如，图
    [H3](#A8.F3 "图 H3 ‣ 附录 H 更多可视化 ‣ 高级异常值管理和高效量化 LLM 的旋转与置换") 显示了 LLaMA1-13B 的 FFN
    模块的上投影层中的正常异常值。显著的是，在我们的旋转和置换变换之后，大规模和正常异常值显著减少，导致激活的量化变得更容易。这突显了我们的 RAP 在管理不同
    LLM 模型的异常值特征方面的有效性。
- en: '![Refer to caption](img/3d18b5ff06ce331f4b9ae241b22c488d.png)'
  id: totrans-564
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3d18b5ff06ce331f4b9ae241b22c488d.png)'
- en: 'Figure H1: Activation change with the use of our DuQuant for LLaMA1-7B.'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '图 H1: 使用我们的 DuQuant 对 LLaMA1-7B 的激活变化。'
- en: '![Refer to caption](img/236073a6cb59e1a849291f5e204403e0.png)'
  id: totrans-566
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/236073a6cb59e1a849291f5e204403e0.png)'
- en: 'Figure H2: Activation change with the use of our DuQuant for LLaMA1-13B.'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '图 H2: 使用我们的 DuQuant 对 LLaMA1-13B 的激活变化。'
- en: '![Refer to caption](img/91c8c9036642b8ff7d759e0484574db9.png)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/91c8c9036642b8ff7d759e0484574db9.png)'
- en: 'Figure H3: Activation change with the use of our DuQuant for LLaMA1-13B.'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '图 H3: 使用我们的 DuQuant 对 LLaMA1-13B 的激活变化。'
- en: '![Refer to caption](img/b7b9419c20313e19fcd77ceb09daa4fc.png)'
  id: totrans-570
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b7b9419c20313e19fcd77ceb09daa4fc.png)'
- en: 'Figure H4: Activation change with the use of our DuQuant for LLaMA1-65B.'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '图 H4: 使用我们的 DuQuant 对 LLaMA1-65B 的激活变化。'
- en: '![Refer to caption](img/ca2c8f6989056893e48aedf5e7c4c500.png)'
  id: totrans-572
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ca2c8f6989056893e48aedf5e7c4c500.png)'
- en: 'Figure H5: Activation change with the use of our DuQuant for LLaMA2-13B.'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '图 H5: 使用我们的 DuQuant 对 LLaMA2-13B 的激活变化。'
- en: '![Refer to caption](img/6dc216fb245aa4139d9ff8317f503abf.png)'
  id: totrans-574
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6dc216fb245aa4139d9ff8317f503abf.png)'
- en: 'Figure H6: Activation change with the use of our DuQuant for LLaMA2-70B.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '图 H6: 使用我们的 DuQuant 对 LLaMA2-70B 的激活变化。'
- en: '![Refer to caption](img/b365f13b159f31fc2f7b8ffd7d7a0b6d.png)'
  id: totrans-576
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b365f13b159f31fc2f7b8ffd7d7a0b6d.png)'
- en: 'Figure H7: Activation change with the use of our DuQuant for Vicuna-v1.5-7B.'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '图 H7: 使用我们的 DuQuant 对 Vicuna-v1.5-7B 的激活变化。'
- en: '![Refer to caption](img/9d6af9f6a5a6496385f73510ee692a80.png)'
  id: totrans-578
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9d6af9f6a5a6496385f73510ee692a80.png)'
- en: 'Figure H8: Activation change with the use of our DuQuant for Vicuna-v1.5-13B.'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '图 H8: 使用我们的 DuQuant 对 Vicuna-v1.5-13B 的激活变化。'
