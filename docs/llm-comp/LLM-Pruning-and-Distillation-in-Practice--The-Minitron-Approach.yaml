- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:04:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LLM Pruning and Distillation in Practice: The Minitron Approach'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 剪枝与蒸馏实践：Minitron 方法
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.11796](https://ar5iv.labs.arxiv.org/html/2408.11796)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.11796](https://ar5iv.labs.arxiv.org/html/2408.11796)
- en: \correspondingauthor
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \correspondingauthor
- en: X
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: X
- en: Sharath Turuvekere Sreenivas Saurav Muralidharan Raviraj Joshi Marcin Chochowski
    Mostofa Patwary Mohammad Shoeybi Bryan Catanzaro Jan Kautz and Pavlo Molchanov
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Sharath Turuvekere Sreenivas Saurav Muralidharan Raviraj Joshi Marcin Chochowski
    Mostofa Patwary Mohammad Shoeybi Bryan Catanzaro Jan Kautz 和 Pavlo Molchanov
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Abstract: We present a comprehensive report on compressing the Llama 3.1 8B
    and Mistral NeMo 12B models to 4B and 8B parameters, respectively, using pruning
    and distillation [[1](#bib.bib1)]. We explore two distinct pruning strategies:
    (1) depth pruning and (2) joint hidden/attention/MLP (width) pruning, and evaluate
    the results on common benchmarks from the LM Evaluation Harness [[2](#bib.bib2)].
    The models are then aligned with NeMo Aligner and tested in instruct-tuned versions.
    This approach produces a compelling 4B model from Llama 3.1 8B and a state-of-the-art
    Mistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo
    12B. We found that with no access to the original data, it is beneficial to slightly
    fine-tune teacher models on the distillation dataset. We open-source our base
    model weights on Hugging Face with a permissive license.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要：我们提供了一份关于将 Llama 3.1 8B 和 Mistral NeMo 12B 模型分别压缩到 4B 和 8B 参数的全面报告，采用了剪枝和蒸馏方法[[1](#bib.bib1)]。我们探讨了两种不同的剪枝策略：（1）深度剪枝和（2）联合隐藏/注意力/MLP（宽度）剪枝，并在来自
    LM Evaluation Harness 的常见基准上评估了结果[[2](#bib.bib2)]。随后，这些模型与 NeMo Aligner 对齐，并在指令调整版本中进行了测试。这种方法产生了一个引人注目的
    4B 模型（来自 Llama 3.1 8B）和一个最先进的 Mistral-NeMo-Minitron-8B（简写 MN-Minitron-8B）模型（来自
    Mistral NeMo 12B）。我们发现，在没有原始数据的情况下，对教师模型在蒸馏数据集上进行微调是有益的。我们在 Hugging Face 上以宽松的许可证开源了我们的基础模型权重。
- en: 'Models on Hugging Face: [Mistral-NeMo-Minitron-8B-Base](https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base)
    | [Llama-3.1-Minitron-4B-Width-Base](https://huggingface.co/nvidia/Llama-3.1-Minitron-4B-Width-Base)
    | [Llama-3.1-Minitron-4B-Depth-Base](https://huggingface.co/nvidia/Llama-3.1-Minitron-4B-Depth-Base)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 上的模型：[Mistral-NeMo-Minitron-8B-Base](https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base)
    | [Llama-3.1-Minitron-4B-Width-Base](https://huggingface.co/nvidia/Llama-3.1-Minitron-4B-Width-Base)
    | [Llama-3.1-Minitron-4B-Depth-Base](https://huggingface.co/nvidia/Llama-3.1-Minitron-4B-Depth-Base)
- en: '|  Benchmarks(shots)  |  Gemma2  |  Minitron  |  Llama-3.1-Minitron  |  Gemma  |  Mistral  |  Llama
    3.1  |  MN-Minitron  |  Mistral NeMo  |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 基准（样本量） | Gemma2 | Minitron | Llama-3.1-Minitron | Gemma | Mistral | Llama
    3.1 | MN-Minitron | Mistral NeMo |'
- en: '|  | 2B* | 4B | 4B-Depth | 4B-Width | 7B | 7B | 8B | 8B | 12B-Base | 12B-FT
    |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '|  | 2B* | 4B | 4B-Depth | 4B-Width | 7B | 7B | 8B | 8B | 12B-Base | 12B-FT
    |'
- en: '| Total Params | 2.6B | 4.2B | 4.5B | 4.5B | 8.5B | 7.3B | 8B | 8.4B | 12.2B
    | 12.2B |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 总参数 | 2.6B | 4.2B | 4.5B | 4.5B | 8.5B | 7.3B | 8B | 8.4B | 12.2B | 12.2B
    |'
- en: '| Non-Emb. Params | 2B | 2.6B | 3.7B | 3.7B | 7.7B | 7B | 7B | 7.3B | 10.9B
    | 10.9B |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 非嵌入参数 | 2B | 2.6B | 3.7B | 3.7B | 7.7B | 7B | 7B | 7.3B | 10.9B | 10.9B |'
- en: '| Training Tokens | 2T | 94B | 94B | 94B | 6T | 8T | 15T | 380B | - | +0.1T
    |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 训练令牌 | 2T | 94B | 94B | 94B | 6T | 8T | 15T | 380B | - | +0.1T |'
- en: '| Winogrande(5) | 70.9 | 74.0 | 72.1 | 73.5 | 78 | 78.5 | 77.3 | 80.4 | 82.2
    | 82.7 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| Winogrande(5) | 70.9 | 74.0 | 72.1 | 73.5 | 78 | 78.5 | 77.3 | 80.4 | 82.2
    | 82.7 |'
- en: '| Arc_challenge(25) | 55.4 | 50.9 | 52.6 | 55.6 | 61 | 60.3 | 57.9 | 64.4 |
    65.1 | 62.3 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| Arc_challenge(25) | 55.4 | 50.9 | 52.6 | 55.6 | 61 | 60.3 | 57.9 | 64.4 |
    65.1 | 62.3 |'
- en: '| MMLU(5) | 51.3 | 58.6 | 58.7 | 60.5 | 64 | 64.1 | 65.3 | 69.5 | 69.0 | 70.1
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| MMLU(5) | 51.3 | 58.6 | 58.7 | 60.5 | 64 | 64.1 | 65.3 | 69.5 | 69.0 | 70.1
    |'
- en: '| Hellaswag(10) | 73.0 | 75.0 | 73.2 | 76.1 | 82 | 83.2 | 81.8 | 83.0 | 85.2
    | 85.3 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| Hellaswag(10) | 73.0 | 75.0 | 73.2 | 76.1 | 82 | 83.2 | 81.8 | 83.0 | 85.2
    | 85.3 |'
- en: '| GSM8k(5) | 23.9 | 24.1 | 16.8 | 41.2 | 50 | 37.0 | 48.6 | 58.5 | 56.4 | 55.7
    |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| GSM8k(5) | 23.9 | 24.1 | 16.8 | 41.2 | 50 | 37.0 | 48.6 | 58.5 | 56.4 | 55.7
    |'
- en: '| Truthfulqa(0) | - | 42.9 | 38.2 | 42.9 | 45 | 42.6 | 45.0 | 47.6 | 49.8 |
    48.3 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| Truthfulqa(0) | - | 42.9 | 38.2 | 42.9 | 45 | 42.6 | 45.0 | 47.6 | 49.8 |
    48.3 |'
- en: '| XLSum en(20%) (3) | - | 29.5 | 27.2 | 28.7 | 17 | 4.8 | 30.0 | 32.0 | 33.4
    | 31.9 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| XLSum en(20%) (3) | - | 29.5 | 27.2 | 28.7 | 17 | 4.8 | 30.0 | 32.0 | 33.4
    | 31.9 |'
- en: '| MBPP(0) | 29.0 | 28.2 | 30.7 | 32.4 | 39 | 38.8 | 42.3 | 43.8 | 42.6 | 47.9
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| MBPP(0) | 29.0 | 28.2 | 30.7 | 32.4 | 39 | 38.8 | 42.3 | 43.8 | 42.6 | 47.9
    |'
- en: '| HumanEval(n=20)(0) | 20.1 | 23.3 | - | - | 32.0 | 28.7 | 24.8 | 36.2 | 23.8
    | 23.8 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| HumanEval(n=20)(0) | 20.1 | 23.3 | - | - | 32.0 | 28.7 | 24.8 | 36.2 | 23.8
    | 23.8 |'
- en: 'Table 1: Accuracy numbers for our MN-Minitron-8B and Llama-3.1-Minitron-4B
    models. We compare our models to similarly-sized SoTA open models on a variety
    of common language modeling benchmarks. All evaluations are conducted by us, except
    entries marked with * (taken from corresponding papers).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：我们MN-Minitron-8B和Llama-3.1-Minitron-4B模型的准确率数据。我们将我们的模型与类似规模的最先进开源模型在各种常见语言建模基准上进行比较。所有评估均由我们进行，除了标有*的条目（取自相关论文）。
- en: '|  Benchmarks  |  Gemma  |  Phi-2  | Gemma2 | Qwen2 | Minitron |  Llama-3.1-Minitron  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  基准  |  Gemma  |  Phi-2  | Gemma2 | Qwen2 | Minitron |  Llama-3.1-Minitron  |'
- en: '|  | 2B | 2.7B | 2B | 1.5B | 4B | 4B-Depth | 4B-Width |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | 2B | 2.7B | 2B | 1.5B | 4B | 4B-Depth | 4B-Width |'
- en: '| Total Params | 2.5B | 2.7B | 2.6B | 1.5B | 4.2B | 4.5B | 4.5B |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 总参数 | 2.5B | 2.7B | 2.6B | 1.5B | 4.2B | 4.5B | 4.5B |'
- en: '| Non-Emb. Params | 2B | 2.5B | 2B | 1.3B | 2.6B | 3.5B | 3.7B |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 非嵌入参数 | 2B | 2.5B | 2B | 1.3B | 2.6B | 3.5B | 3.7B |'
- en: '| Tokens | 3T | 1.4T | 2T | 7T | 94B | 94B | 94B |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 代币 | 3T | 1.4T | 2T | 7T | 94B | 94B | 94B |'
- en: '| IFEval | 40.5 | 44.0 | 64.5 | 39.8 | 44.8 | 42.6 | 52.4 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| IFEval | 40.5 | 44.0 | 64.5 | 39.8 | 44.8 | 42.6 | 52.4 |'
- en: '| MT-Bench | 5.2 | 4.3 | 7.7 | 5.2 | 5.6 | 5.6 | 6.3 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench | 5.2 | 4.3 | 7.7 | 5.2 | 5.6 | 5.6 | 6.3 |'
- en: '| ChatRAG* | 33.3 | 37.6 | 37.5 | 32.8 | 41.1 | 40.1 | 44.0 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| ChatRAG* | 33.3 | 37.6 | 37.5 | 32.8 | 41.1 | 40.1 | 44.0 |'
- en: '| BFCL | 47.0 | 23.1 | 35.6 | 32.8 | 64.2 | 66.8 | 64.9 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| BFCL | 47.0 | 23.1 | 35.6 | 32.8 | 64.2 | 66.8 | 64.9 |'
- en: 'Table 2: Accuracy numbers for the aligned Llama-3.1-Minitron models. We compare
    our models to similarly-sized SoTA open aligned models on a variety of benchmarks.
    All evaluations are conducted by us. * Denotes results obtained on a representative
    subset of the benchmark. Best in bold, second underlined. The alignment of MN-Minitron-8B
    is underway and will be posted once ready.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：对齐的Llama-3.1-Minitron模型的准确率数据。我们将我们的模型与类似规模的最先进对齐模型在各种基准上进行比较。所有评估均由我们进行。*表示在基准的代表性子集上获得的结果。最佳结果用粗体标出，第二名用下划线标出。MN-Minitron-8B的对齐工作正在进行中，一旦准备好将会发布。
- en: 1 Introduction
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: LLM providers often train an entire family of models from scratch, each with
    a different size (number of parameters, e.g. Llama 3.1 8B, 70B, 405B); this is
    done to aid users targeting different deployment scales, sizes and compute budgets.
    However, training multiple multi-billion parameter models from scratch is extremely
    time-, data- and resource-intensive.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLM提供商通常从头开始训练整个模型系列，每个模型具有不同的大小（参数数量，例如Llama 3.1 8B、70B、405B）；这样做是为了帮助用户满足不同的部署规模、大小和计算预算。然而，从头开始训练多个数十亿参数的模型极为耗时、数据和资源。
- en: Recent work [[1](#bib.bib1)] has demonstrated the effectiveness of combining
    weight pruning with knowledge distillation to significantly reduce the cost of
    training LLM model families. Here, only the biggest model in the family is trained
    from scratch; other models are obtained by successively pruning the bigger model(s)
    and then performing knowledge distillation [[3](#bib.bib3)] to recover the accuracy
    of pruned models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的工作[[1](#bib.bib1)]已经证明，将权重修剪与知识蒸馏相结合，可以显著降低训练LLM模型系列的成本。在这里，只有系列中的最大模型从头开始训练；其他模型则通过逐步修剪更大的模型并进行知识蒸馏[[3](#bib.bib3)]来恢复修剪模型的准确性。
- en: 'In this report, we successfully apply the Minitron compression strategy [[1](#bib.bib1)]
    to two state-of-the-art models: Llama 3.1 8B [[4](#bib.bib4)] and Mistral NeMo
    12B [[5](#bib.bib5)], compressing them down to 4B and 8B parameters, respectively.
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM Pruning and Distillation in
    Practice: The Minitron Approach") provides a high-level overview of our approach.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '在本报告中，我们成功地将Minitron压缩策略[[1](#bib.bib1)]应用于两个最先进的模型：Llama 3.1 8B[[4](#bib.bib4)]和Mistral
    NeMo 12B[[5](#bib.bib5)]，将它们分别压缩到4B和8B参数。图[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ LLM Pruning and Distillation in Practice: The Minitron Approach")提供了我们方法的高层次概述。'
- en: 'While following the original paper [[1](#bib.bib1)], we make a key modification:
    due to lack of access to the original training data, we fine-tune the teacher
    model on our own dataset before pruning and distillation. We refer to this step
    as teacher correction. Figure [4](#S3.F4 "Figure 4 ‣ Retraining: ‣ 3.3 Distillation
    ‣ 3 Training Details ‣ LLM Pruning and Distillation in Practice: The Minitron
    Approach") shows that omitting teacher correction causes a data distribution mismatch,
    negatively impacting distillation.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '在遵循原始论文[[1](#bib.bib1)]的同时，我们做了一个关键的修改：由于无法获取原始训练数据，我们在修剪和蒸馏之前在自己的数据集上微调了教师模型。我们称此步骤为教师校正。图[4](#S3.F4
    "Figure 4 ‣ Retraining: ‣ 3.3 Distillation ‣ 3 Training Details ‣ LLM Pruning
    and Distillation in Practice: The Minitron Approach")显示，忽略教师校正会导致数据分布不匹配，负面影响蒸馏。'
- en: 'Table [1](#S0.T1 "Table 1 ‣ LLM Pruning and Distillation in Practice: The Minitron
    Approach") provides a summary of our results: our compression strategy yields
    a state-of-the-art 8B model (MN-Minitron-8B) which outperforms all similarly-sized
    models across the board on common language modeling benchmarks. Our Llama-3.1-Minitron-4B
    models (both depth and width-pruned variants) also exhibit strong accuracy compared
    to the teacher Llama 3.1 8B model and the previous-generation Minitron-4B model [[1](#bib.bib1)];
    among the two variants, the width-pruned variant outperforms the depth-pruned
    one. In terms of runtime inference performance measured using TensorRT-LLM, the
    Llama-3.1-Minitron-4B models provide an average speedup of 2.7$\times$ for the
    depth and width pruned variants, respectively, compared to the teacher Llama 3.1
    8B model.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '表[1](#S0.T1 "Table 1 ‣ LLM Pruning and Distillation in Practice: The Minitron
    Approach")提供了我们结果的总结：我们的压缩策略产生了一个先进的8B模型（MN-Minitron-8B），在所有相似大小的模型中，表现都优于常见语言建模基准。我们的Llama-3.1-Minitron-4B模型（包括深度和宽度剪枝变体）也显示出相较于教师Llama
    3.1 8B模型和上一代Minitron-4B模型[[1](#bib.bib1)]的强大准确性；在这两个变体中，宽度剪枝变体优于深度剪枝变体。在使用TensorRT-LLM测量的运行时推理性能方面，Llama-3.1-Minitron-4B模型相比于教师Llama
    3.1 8B模型，深度和宽度剪枝变体的平均加速比为2.7$\times$。'
- en: '![Refer to caption](img/cd3913828ef459129c69e57026ec2d77.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cd3913828ef459129c69e57026ec2d77.png)'
- en: 'Figure 1: High-level overview of our proposed pruning and distillation approach.
    The total number of tokens used for each step is indicated in parentheses.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们提出的剪枝和蒸馏方法的高级概述。每一步使用的总令牌数在括号中指示。
- en: 2 Methodology
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: 'A high-level overview of our approach is illustrated in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ LLM Pruning and Distillation in Practice: The Minitron Approach").
    Here, the teacher model is first lightly finetuned on the target dataset to be
    used for distillation - we refer to this step as teacher correction. Next, pruning
    is applied to compress the model, following which distillation is used to recover
    any lost model accuracy. We refer the reader to the Minitron paper [[1](#bib.bib1)]
    for the full description of the pruning and distillation method.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '我们方法的高级概述如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM Pruning and Distillation
    in Practice: The Minitron Approach")所示。在这里，教师模型首先在目标数据集上轻微微调，以便用于蒸馏——我们将此步骤称为教师校正。接下来，应用剪枝来压缩模型，然后使用蒸馏来恢复任何丧失的模型准确性。我们建议读者参阅Minitron论文[[1](#bib.bib1)]以获取有关剪枝和蒸馏方法的完整描述。'
- en: 2.1 Pruning
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 剪枝
- en: 'Weight pruning is a powerful and well-known technique for reducing model size.
    In this report, we focus on structured pruning, where blocks (or channels) of
    nonzero elements are removed at once from model weights; examples of structured
    pruning techniques include neuron, attention head, convolutional filter, and depth
    pruning [[1](#bib.bib1)]. In case of LLMs, as shown in Figure [2](#S2.F2 "Figure
    2 ‣ 2.1 Pruning ‣ 2 Methodology ‣ LLM Pruning and Distillation in Practice: The
    Minitron Approach"), we start the pruning process by first computing the importance
    of each layer, neuron, head, and embedding dimension. We then sort these importance
    scores to compute a corresponding importance ranking.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '权重剪枝是一种强大且知名的技术，用于减少模型大小。在本报告中，我们关注的是结构化剪枝，即一次性从模型权重中删除具有非零元素的块（或通道）；结构化剪枝技术的示例包括神经元、注意力头、卷积滤波器和深度剪枝[[1](#bib.bib1)]。在LLMs的情况下，如图[2](#S2.F2
    "Figure 2 ‣ 2.1 Pruning ‣ 2 Methodology ‣ LLM Pruning and Distillation in Practice:
    The Minitron Approach")所示，我们通过首先计算每一层、神经元、头和嵌入维度的重要性来启动剪枝过程。然后，我们对这些重要性分数进行排序，以计算相应的重要性排名。'
- en: '![Refer to caption](img/a3a1bda1b902049db54816450d1b468c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a3a1bda1b902049db54816450d1b468c.png)'
- en: 'Figure 2: Pruning and distillation process outlined in the original paper [[1](#bib.bib1)].
    We follow the same approach in this work.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：原论文[[1](#bib.bib1)]中概述的剪枝和蒸馏过程。我们在这项工作中遵循了相同的方法。
- en: 'Importance Estimation: We use a purely activation-based importance estimation
    strategy that simultaneously computes sensitivity information for all the axes
    we consider (depth, neuron, head, and embedding channel) using a small calibration
    dataset and only forward propagation passes. We consider depth pruning as a special
    case and do not combine it with compressing other dimensions.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 重要性估计：我们使用纯基于激活的估计策略，利用小型校准数据集和仅前向传播过程同时计算我们考虑的所有轴（深度、神经元、头和嵌入通道）的敏感性信息。我们将深度剪枝视为一个特殊案例，并且不将其与压缩其他维度结合。
- en: We compute the importance of each head, neuron and embedding channel by examining
    the activations produced by the multi-head attention (MHA), multi-layer perceptron
    (MLP) and LayerNorm layers, respectively. We use a small calibration dataset (1024
    samples) for this purpose.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过检查多头注意力（MHA）、多层感知机（MLP）和 LayerNorm 层产生的激活来计算每个头、神经元和嵌入通道的重要性。我们为此目的使用了一个小型校准数据集（1024
    个样本）。
- en: 'For depth pruning, we consider three distinct metrics for evaluating layer
    importance: (1) LM validation loss, (2) Block Importance (BI) [[6](#bib.bib6)]
    and (3) accuracy on the downstream task. For loss-based ranking, we simply remove
    a single or a block of contiguous layers and compute its effect on LM loss; this
    serves as the “importance” or sensitivity of the layer. BI uses the cosine distance
    between the input and output of a layer or a block of layers. We notice that BI
    and LM loss metrics are highly correlated but do not produce the most accurate
    pruned model on downstream tasks as shown in Figures [8](#S4.F8 "Figure 8 ‣ Depth
    Pruning Metrics: ‣ 4 Analysis ‣ LLM Pruning and Distillation in Practice: The
    Minitron Approach") and [9](#S4.F9 "Figure 9 ‣ Depth Pruning Metrics: ‣ 4 Analysis
    ‣ LLM Pruning and Distillation in Practice: The Minitron Approach"). We thus evaluate
    layer importance using the Winogrande benchmark [[7](#bib.bib7)].'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '对于深度剪枝，我们考虑了三种不同的指标来评估层的重要性：（1）语言模型验证损失，（2）块重要性（BI）[[6](#bib.bib6)]，以及（3）下游任务的准确性。对于基于损失的排名，我们简单地移除单个或一组连续的层，并计算其对语言模型损失的影响；这作为层的“重要性”或敏感性。BI
    使用层或一组层的输入和输出之间的余弦距离。我们注意到，BI 和 LM 损失指标高度相关，但在下游任务中并未产生最准确的剪枝模型，如图 [8](#S4.F8
    "Figure 8 ‣ Depth Pruning Metrics: ‣ 4 Analysis ‣ LLM Pruning and Distillation
    in Practice: The Minitron Approach") 和 [9](#S4.F9 "Figure 9 ‣ Depth Pruning Metrics:
    ‣ 4 Analysis ‣ LLM Pruning and Distillation in Practice: The Minitron Approach")
    所示。因此，我们使用 Winogrande 基准[[7](#bib.bib7)]来评估层的重要性。'
- en: 'Model Trimming: As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Pruning ‣ 2 Methodology
    ‣ LLM Pruning and Distillation in Practice: The Minitron Approach"), for a given
    architecture configuration, we first rank the elements of each axis according
    to the computed importance and perform trimming (reshaping) of the corresponding
    weight matrices directly. For neuron and head pruning, we trim MLP and MHA layer
    weights, respectively. In the case of embedding channels, we trim the embedding
    dimension of the weight matrices in MLP, MHA, and LayerNorm layers. The original
    approach ([[1](#bib.bib1)]) uses Neural Architecture Search (NAS) to find the
    best architecture; in this work, we skip this step and instead utilize the network
    architecture-related learnings from the original paper.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '模型修剪：如图 [2](#S2.F2 "Figure 2 ‣ 2.1 Pruning ‣ 2 Methodology ‣ LLM Pruning and
    Distillation in Practice: The Minitron Approach") 所示，对于给定的架构配置，我们首先根据计算的重要性对每个轴的元素进行排序，并直接修剪（重新塑形）相应的权重矩阵。对于神经元和头部剪枝，我们分别修剪
    MLP 和 MHA 层的权重。在嵌入通道的情况下，我们修剪 MLP、MHA 和 LayerNorm 层中权重矩阵的嵌入维度。原始方法（[[1](#bib.bib1)]）使用神经架构搜索（NAS）来寻找最佳架构；在这项工作中，我们跳过了这一步，而是利用了原始论文中的网络架构相关的学习。'
- en: 2.2 Retraining with Distillation
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 蒸馏重新训练
- en: 'We use the term retraining to refer to the accuracy recovery process following
    pruning. In this work, we explore two retraining strategies: (1) conventional
    training, leveraging ground truth labels, and (2) knowledge distillation using
    supervision from the unpruned model (teacher). Knowledge Distillation (KD) [[3](#bib.bib3)]
    involves transfer of knowledge from a larger or more complex model called the
    teacher to a smaller/simpler model called the student. The knowledge transfer
    is achieved by having the student model mimic the output and/or the intermediate
    states of the teacher model. In our case, the uncompressed and pruned models correspond
    to the teacher and student, respectively. For distillation, we follow best practices
    from our previous work [[1](#bib.bib1)] and use forward KL Divergence loss [[8](#bib.bib8)]
    on the teacher and student logits only (following the [[3](#bib.bib3)]). This
    is illustrated in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Retraining with Distillation
    ‣ 2 Methodology ‣ LLM Pruning and Distillation in Practice: The Minitron Approach").'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用术语重新训练来指代剪枝后的准确性恢复过程。在这项工作中，我们探讨了两种重新训练策略：（1）传统训练，利用真实标签，和（2）知识蒸馏，使用来自未剪枝模型（教师）的监督。知识蒸馏（KD）
    [[3](#bib.bib3)] 涉及从一个更大或更复杂的模型（称为教师）转移知识到一个更小/更简单的模型（称为学生）。知识转移通过让学生模型模仿教师模型的输出和/或中间状态来实现。在我们的案例中，未压缩和修剪的模型分别对应教师和学生。对于蒸馏，我们遵循之前工作的最佳实践
    [[1](#bib.bib1)]，并仅对教师和学生 logits 使用前向 KL 散度损失 [[8](#bib.bib8)]（遵循 [[3](#bib.bib3)]）。如图
    [3](#S2.F3 "图 3 ‣ 2.2 蒸馏重新训练 ‣ 2 方法论 ‣ LLM 剪枝和蒸馏实践：Minitron 方法") 所示。
- en: '![Refer to caption](img/98f5abba7d192209e9e4e96a8f57dd09.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/98f5abba7d192209e9e4e96a8f57dd09.png)'
- en: 'Figure 3: Overview of Distillation: If the original training data is unavailable,
    a slight fine-tuning of the teacher model is recommended. Distillation is then
    performed by minimizing KL divergence on the logits, with the original model as
    the teacher and the pruned model as the student.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：蒸馏概述：如果原始训练数据不可用，建议对教师模型进行轻微的微调。然后，通过最小化 logits 上的 KL 散度进行蒸馏，原始模型作为教师，修剪后的模型作为学生。
- en: 3 Training Details
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 训练细节
- en: 3.1 Pre-training
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 预训练
- en: Llama 3.1 8B [[4](#bib.bib4)] and Mistral NeMo [[5](#bib.bib5)] 12B are pretrained
    on different proprietary datasets, which we do not have access to. According to
    the Llama 3.1 tech report [[4](#bib.bib4)], the 8B model is pretrained on 15T
    tokens. We start with the corresponding Base models that are openly available
    online on Hugging Face.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3.1 8B [[4](#bib.bib4)] 和 Mistral NeMo [[5](#bib.bib5)] 12B 在不同的专有数据集上进行预训练，我们无法访问这些数据集。根据
    Llama 3.1 技术报告 [[4](#bib.bib4)]，8B 模型在 15T 令牌上进行了预训练。我们从 Hugging Face 上公开可用的相应基础模型开始。
- en: 'Dataset:'
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集：
- en: We use the Nemotron-4 curated continued training (CT) dataset [[9](#bib.bib9)]
     [[10](#bib.bib10)] for all our pruning and distillation experiments.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Nemotron-4 精选的持续训练（CT）数据集 [[9](#bib.bib9)] [[10](#bib.bib10)] 进行所有剪枝和蒸馏实验。
- en: 3.2 Pruning
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 剪枝
- en: 'Our simplified pruning recipe is based on the best practices outlined in the
    Minitron paper [[1](#bib.bib1)] and is described in the Methodology section. Specifically,
    for width pruning, we (1) use l2-norm and mean as the aggregation functions across
    the batch and sequence dimensions, respectively, and (2) perform single-shot pruning,
    avoiding iterative approaches. For depth pruning, as described in the Methodology
    section, we follow the observations from Gromov et al. [[11](#bib.bib11)] and
    drop a continuous subgroup of layers that results in the least accuracy drop on
    Winogrande [[7](#bib.bib7)]. In this work, we skip the lightweight neural architecture
    search (NAS) phase, and go with a manual architecture configuration for both Llama-3.1-Minitron-4B
    and MN-Minitron-8B. The architectures we come up with are inspired by the Minitron-4B
    and Minitron-8B models, and are detailed in Table [3](#S3.T3 "Table 3 ‣ 3.2 Pruning
    ‣ 3 Training Details ‣ LLM Pruning and Distillation in Practice: The Minitron
    Approach"). We now describe the pruning recipes for each of our target compressed
    models:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '我们简化的修剪方案基于 Minitron 论文中列出的最佳实践[[1](#bib.bib1)]，并在方法论部分进行了描述。具体来说，对于宽度修剪，我们（1）使用
    l2-norm 和均值作为跨批次和序列维度的聚合函数，分别，并（2）执行单次修剪，避免迭代方法。对于深度修剪，如方法论部分所述，我们遵循 Gromov 等人[[11](#bib.bib11)]的观察，并丢弃连续的层子组，以最小化
    Winogrande[[7](#bib.bib7)]上的准确性下降。在这项工作中，我们跳过了轻量级神经架构搜索（NAS）阶段，而是对 Llama-3.1-Minitron-4B
    和 MN-Minitron-8B 进行了手动架构配置。我们提出的架构灵感来源于 Minitron-4B 和 Minitron-8B 模型，详见表[3](#S3.T3
    "Table 3 ‣ 3.2 Pruning ‣ 3 Training Details ‣ LLM Pruning and Distillation in
    Practice: The Minitron Approach")。我们现在描述每个目标压缩模型的修剪方案：'
- en: '|  | LLaMa-3.1-Minitron-4B | MN-Minitron |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMa-3.1-Minitron-4B | MN-Minitron |'
- en: '|  | Width | Depth | 8B |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | 宽度 | 深度 | 8B |'
- en: '| Total params | 4.5B | 4.5B | 8.4B |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 总参数 | 4.5B | 4.5B | 8.4B |'
- en: '| Non-Emb params | 3.7B | 3.5B | 7.3B |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 非嵌入参数 | 3.7B | 3.5B | 7.3B |'
- en: '| Hidden size | 3072 | 4096 | 4096 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏大小 | 3072 | 4096 | 4096 |'
- en: '| Vocabulary | 128256 | 128256 | 131072 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 词汇量 | 128256 | 128256 | 131072 |'
- en: '| MLP hidden dim | 9216 | 14336 | 11520 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| MLP 隐藏维度 | 9216 | 14336 | 11520 |'
- en: '| Depth | 32 | 16 | 40 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 深度 | 32 | 16 | 40 |'
- en: '| Attention groups | 8 | 8 | 8 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 注意力组 | 8 | 8 | 8 |'
- en: '| Query heads | 32 | 32 | 32 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 查询头 | 32 | 32 | 32 |'
- en: '| Head dimension | 128 | 128 | 128 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 头维度 | 128 | 128 | 128 |'
- en: 'Table 3: Architecture details of our compressed models.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：我们压缩模型的架构细节。
- en: 'Llama-3.1-Minitron-4B-Width: • Starting model: Llama 3.1 8B • Hidden dimension:
    4096 $\rightarrow$ 11520 • Attention heads: unchanged • Depth: unchanged'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Llama-3.1-Minitron-4B-宽度：• 起始模型：Llama 3.1 8B • 隐藏维度：4096 $\rightarrow$ 11520
    • 注意力头：保持不变 • 深度：保持不变
- en: 3.3 Distillation
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 蒸馏
- en: 'Teacher Correction:'
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 教师校正：
- en: 'Using the Mistral NeMo 12B model directly as a teacher performs sub-optimally
    on our dataset. This is due to the change in distribution of sub-word tokens across
    the original dataset the teacher model was trained on vs. the dataset being distilled
    on. To account for this, we first fine-tune the teacher on our dataset using $\sim$127B
    tokens. As shown in Figure [4](#S3.F4 "Figure 4 ‣ Retraining: ‣ 3.3 Distillation
    ‣ 3 Training Details ‣ LLM Pruning and Distillation in Practice: The Minitron
    Approach"), such a correction is essential if the original dataset is not available
    during distillation. We thus apply this technique on both the Mistral-NeMo and
    Llama-3.1 teacher models. The fine-tuning process has a minor effect on the teacher
    model’s accuracy on downstream tasks, with some tasks improving and some degrading
    as shown in Table  [1](#S0.T1 "Table 1 ‣ LLM Pruning and Distillation in Practice:
    The Minitron Approach"). We hypothesize this to be an artifact of the dataset
    used for fine-tuning.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '直接使用 Mistral NeMo 12B 模型作为教师在我们的数据集上表现不佳。这是由于教师模型训练时的原始数据集与蒸馏数据集的子词令牌分布变化所致。为了弥补这一点，我们首先使用约
    127B 令牌在我们的数据集上微调教师模型。如图[4](#S3.F4 "Figure 4 ‣ Retraining: ‣ 3.3 Distillation
    ‣ 3 Training Details ‣ LLM Pruning and Distillation in Practice: The Minitron
    Approach")所示，这种校正对于蒸馏过程中原始数据集不可用的情况至关重要。因此，我们对 Mistral-NeMo 和 Llama-3.1 教师模型都应用了这一技术。微调过程对教师模型在下游任务上的准确性影响较小，有些任务有所改善，有些任务有所下降，如表[1](#S0.T1
    "Table 1 ‣ LLM Pruning and Distillation in Practice: The Minitron Approach")所示。我们推测这是由于用于微调的数据集的影响。'
- en: 'Retraining:'
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重新训练：
- en: 'Following the learnings in the Minitron work [[1](#bib.bib1)], we opt for logit-only
    distillation, minimizing the forward KL Divergence [[8](#bib.bib8)] loss across
    the teacher and student probabilities, and ignore the LM cross-entropy loss altogether.
    Here, the unpruned and pruned models correspond to the teacher and student, respectively.
    We use the hyperparameters listed in Table [4](#S3.T4 "Table 4 ‣ Retraining: ‣
    3.3 Distillation ‣ 3 Training Details ‣ LLM Pruning and Distillation in Practice:
    The Minitron Approach") during distillation. We use 32 NVIDIA DGX H100 nodes for
    our training jobs.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '根据 Minitron 工作中的经验 [[1](#bib.bib1)]，我们选择了仅基于 logit 的蒸馏，最小化教师和学生概率之间的前向 KL 散度
    [[8](#bib.bib8)] 损失，完全忽略了语言模型交叉熵损失。在这里，未剪枝和剪枝模型分别对应于教师和学生。我们在蒸馏过程中使用了表[4](#S3.T4
    "Table 4 ‣ Retraining: ‣ 3.3 Distillation ‣ 3 Training Details ‣ LLM Pruning and
    Distillation in Practice: The Minitron Approach")中列出的超参数。我们使用 32 个 NVIDIA DGX
    H100 节点进行训练作业。'
- en: '|  | Llama-3.1- | MN-Minitron |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | Llama-3.1- | MN-Minitron |'
- en: '|  | Minitron-4B | 8B |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | Minitron-4B | 8B |'
- en: '| Peak learning rate | 1e-4 | 1e-4 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 峰值学习率 | 1e-4 | 1e-4 |'
- en: '| Min learning rate | 1e-5 | 4.5e-7 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 最小学习率 | 1e-5 | 4.5e-7 |'
- en: '| Warm-up steps | 40 steps | 60 steps |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 预热步数 | 40 步 | 60 步 |'
- en: '| LR decay schedule | Cosine | Cosine |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 学习率衰减计划 | 余弦 | 余弦 |'
- en: '| Global batch size | 1152 | 768 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 全局批量大小 | 1152 | 768 |'
- en: '| Context length | 8192 | 8192 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 上下文长度 | 8192 | 8192 |'
- en: '| Total tokens | 94B | 380B |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 总令牌数 | 94B | 380B |'
- en: 'Table 4: Hyperparameters used during distillation-based retraining.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在基于蒸馏的再训练中使用的超参数。
- en: '![Refer to caption](img/c66a16495137d6b98f4693cd084813a3.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c66a16495137d6b98f4693cd084813a3.png)'
- en: 'Figure 4: Training convergence plot for the compressed 8B student model. We
    compare supervision from the original teacher and the corrected teacher.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：压缩的 8B 学生模型的训练收敛图。我们比较了原始教师和校正教师的监督。
- en: 3.4 Instruction Tuning
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 指令调优
- en: 'To evaluate the instruction-following capabilities of our distilled models,
    we perform supervised fine-tuning (SFT) on the Llama-3.1-Minitron 4B models using
    NeMo-Aligner [[12](#bib.bib12)] with the instruction tuning dataset used for Nemotron-4
    340B [[13](#bib.bib13)]. As shown in Table [2](#S0.T2 "Table 2 ‣ LLM Pruning and
    Distillation in Practice: The Minitron Approach"), we evaluate the aligned models
    for instruction- following and roleplay (IFEval [[14](#bib.bib14)] and MT-Bench [[15](#bib.bib15)]),
    RAG QA (ChatRAG-Bench [[16](#bib.bib16)]), and function-calling capabilities (BFCL [[17](#bib.bib17)]).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估我们精简模型的指令跟随能力，我们对 Llama-3.1-Minitron 4B 模型进行监督式微调（SFT），使用 NeMo-Aligner
    [[12](#bib.bib12)] 和用于 Nemotron-4 340B 的指令调优数据集 [[13](#bib.bib13)]。如表[2](#S0.T2
    "Table 2 ‣ LLM Pruning and Distillation in Practice: The Minitron Approach")所示，我们评估了对齐模型的指令跟随和角色扮演（IFEval
    [[14](#bib.bib14)] 和 MT-Bench [[15](#bib.bib15)]）、RAG QA（ChatRAG-Bench [[16](#bib.bib16)]）以及函数调用能力（BFCL
    [[17](#bib.bib17)]）。'
- en: 4 Analysis
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 分析
- en: We perform a series of ablation studies to better understand the compression
    characteristics of these newer models. We report our results in this section.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一系列的消融研究，以更好地理解这些新模型的压缩特性。我们在本节中报告我们的结果。
- en: 'Width vs Depth Pruning:'
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 宽度与深度剪枝：
- en: 'Figure [5](#S4.F5 "Figure 5 ‣ Pruning and Distillation: ‣ 4 Analysis ‣ LLM
    Pruning and Distillation in Practice: The Minitron Approach") shows the training
    curve of Llama-3.1-Minitron-4B pruned for width vs. depth. We notice that width
    pruning results in smaller initial loss and consistently outperforms the depth-pruned
    model, despite both variants having the same number of parameters.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#S4.F5 "Figure 5 ‣ Pruning and Distillation: ‣ 4 Analysis ‣ LLM Pruning
    and Distillation in Practice: The Minitron Approach")展示了 Llama-3.1-Minitron-4B
    在宽度与深度剪枝下的训练曲线。我们注意到，宽度剪枝在初始损失较小，并且始终优于深度剪枝模型，尽管这两种变体具有相同数量的参数。'
- en: 'Pruning and Distillation:'
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 剪枝与蒸馏：
- en: 'Figure  [6](#S4.F6 "Figure 6 ‣ Pruning and Distillation: ‣ 4 Analysis ‣ LLM
    Pruning and Distillation in Practice: The Minitron Approach") demonstrates orthogonal
    benefits of our proposed approach with pruning and distillation. We compare (1)
    random weight initialization and distillation, (2) random pruning and distillation,
    where components are pruned randomly ignoring the importance scores, (3) our proposed
    pruning with typical cross entropy based LM loss training and (4) our proposed
    pruning with distillation-based training. We notice that pruning results in a
    significantly better starting point compared to random initialization, and also
    that distillation-based training outperforms conventional training methods while
    requiring significantly fewer training tokens (up to $50\times$ in our case).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [6](#S4.F6 "Figure 6 ‣ Pruning and Distillation: ‣ 4 Analysis ‣ LLM Pruning
    and Distillation in Practice: The Minitron Approach") 展示了我们提出的剪枝和蒸馏方法的正交效益。我们比较了
    (1) 随机权重初始化与蒸馏，(2) 随机剪枝与蒸馏，其中组件被随机剪枝而忽略重要性评分，(3) 我们提出的基于典型交叉熵 LM 损失训练的剪枝，和 (4)
    我们提出的基于蒸馏的训练剪枝。我们注意到，与随机初始化相比，剪枝结果显著更好，并且基于蒸馏的训练在显著减少训练令牌（在我们案例中最多减少 $50\times$）的情况下优于传统训练方法。'
- en: '![Refer to caption](img/e64b762c7323fe6fc8a314e2da505f6a.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e64b762c7323fe6fc8a314e2da505f6a.png)'
- en: 'Figure 5: Convergence of width- and depth-pruned Llama 3.1 8B to 4B models.
    Width pruning consistently outperforms depth pruning for a given parameter budget.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：宽度剪枝和深度剪枝的 Llama 3.1 8B 到 4B 模型的收敛情况。对于给定的参数预算，宽度剪枝始终优于深度剪枝。
- en: '![Refer to caption](img/ec989e801e6beca107e696b240084614.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec989e801e6beca107e696b240084614.png)'
- en: 'Figure 6: Training convergence plot for Mistral Nemo 12B compressed model.
    We compare (a) random initialization with distillation, (b) randomly pruned weights
    with distillation, (c) pruning with standard LM loss, and (d) our pipeline with
    pruning and distillation.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：Mistral Nemo 12B 压缩模型的训练收敛图。我们比较了 (a) 随机初始化与蒸馏，(b) 随机剪枝的权重与蒸馏，(c) 使用标准 LM
    损失的剪枝，以及 (d) 我们的管道与剪枝和蒸馏。
- en: 'Teacher Correction:'
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 教师纠正：
- en: 'We compare two approaches for teacher correction: (1) pruning and distilling
    the corrected teacher, and (2) pruning the original teacher and distilling from
    a continuously corrected teacher. The results in Figure [7](#S4.F7 "Figure 7 ‣
    Teacher Correction: ‣ 4 Analysis ‣ LLM Pruning and Distillation in Practice: The
    Minitron Approach") suggest that teacher correction doesn’t affect the optimality
    of pruning, and that distillation from a corrected teacher is crucial. Teacher
    correction can be performed in parallel with distillation to bridge the gap.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '我们比较了两种教师纠正的方法：(1) 剪枝和蒸馏纠正后的教师，以及 (2) 剪枝原始教师并从持续纠正的教师中进行蒸馏。图 [7](#S4.F7 "Figure
    7 ‣ Teacher Correction: ‣ 4 Analysis ‣ LLM Pruning and Distillation in Practice:
    The Minitron Approach") 的结果表明，教师纠正不会影响剪枝的最优性，而且从纠正后的教师中进行蒸馏至关重要。教师纠正可以与蒸馏并行进行，以弥合差距。'
- en: '![Refer to caption](img/17120e7557f2810e6e361ae512541d2e.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/17120e7557f2810e6e361ae512541d2e.png)'
- en: 'Figure 7: Training convergence plot for Mistral Nemo 12B compressed model.
    We compare (1) pruning and distilling the corrected teacher with (2) pruning the
    original teacher and distilling from a continuously corrected teacher.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：Mistral Nemo 12B 压缩模型的训练收敛图。我们比较了 (1) 剪枝和蒸馏纠正后的教师与 (2) 剪枝原始教师并从持续纠正的教师中进行蒸馏。
- en: 'Depth Pruning Metrics:'
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 深度剪枝指标：
- en: 'when examining how LM validation loss increases as contiguous blocks of layers
    are removed (Figure  [8](#S4.F8 "Figure 8 ‣ Depth Pruning Metrics: ‣ 4 Analysis
    ‣ LLM Pruning and Distillation in Practice: The Minitron Approach")), we observe
    that the layers at the beginning and end are the most important. Removing non-contiguous
    layers can result in even better LM validation loss (the dashed line). However,
    this observation does not necessarily hold when evaluating downstream task performance.
    Figure  [9](#S4.F9 "Figure 9 ‣ Depth Pruning Metrics: ‣ 4 Analysis ‣ LLM Pruning
    and Distillation in Practice: The Minitron Approach") shows that dropping 16 layers
    selected based on per-layer importance ( [[6](#bib.bib6), [18](#bib.bib18)]) yields
    a random Winogrande accuracy of 0.5, while removing layers 16 to 31 continuously
    ( [[11](#bib.bib11)]) results in an accuracy of 0.595\. The gap holds during distillation-based
    retraining and we opt for the latter approach.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当检查随着连续层块的移除，LM 验证损失如何增加时（图 [8](#S4.F8 "图 8 ‣ 深度修剪指标：‣ 4 分析 ‣ LLM 修剪与蒸馏实践：Minitron
    方法")），我们观察到开始和结束的层最为重要。移除不连续的层可能会导致更好的 LM 验证损失（虚线）。然而，当评估下游任务性能时，这一观察结果不一定成立。图
    [9](#S4.F9 "图 9 ‣ 深度修剪指标：‣ 4 分析 ‣ LLM 修剪与蒸馏实践：Minitron 方法") 显示，基于每层重要性选择的移除 16
    层产生了 0.5 的随机 Winogrande 准确性，而连续移除第 16 层到第 31 层（[[11](#bib.bib11)]）的准确性为 0.595。这个差距在基于蒸馏的再训练中依然存在，我们选择了后者的方法。
- en: '![Refer to caption](img/a84f4f5dc4ffea7f29ddf23644cca5d4.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a84f4f5dc4ffea7f29ddf23644cca5d4.png)'
- en: 'Figure 8: LM loss value on validation set after removing 1, 2, 8 or 16 contiguous
    layers with Llama 3.1 8B. For example, the purple line at layer no. 16 indicates
    the LM loss if we dropped the first 16 layers. Layer no. 17 indicates the LM loss
    if we leave the first layer intact and drop layers 2 to 17\. The dashed line corresponds
    to LM loss value when removing 16 non-contiguous layers least increasing the loss.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：在 Llama 3.1 8B 上移除 1、2、8 或 16 层连续层后的 LM 损失值。例如，第 16 层的紫色线表示如果我们移除前 16 层的
    LM 损失。第 17 层表示如果我们保持第一层不变并移除第 2 层到第 17 层的 LM 损失。虚线对应于移除 16 层不连续层时的 LM 损失值，最小化损失增加。
- en: '![Refer to caption](img/cddb2a6fd916e886f5a8b97ecd2467b2.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cddb2a6fd916e886f5a8b97ecd2467b2.png)'
- en: 'Figure 9: Accuracy on the Winogrande task when removing 16 contiguous layers
    with Llama 3.1 8B. Layer no. 17 indicates the LM loss if we leave the first layer
    intact and drop layers 2 to 17\. The dashed line corresponds to the accuracy when
    removing 16 non-contiguous layers least increasing the loss.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：当使用 Llama 3.1 8B 移除 16 层连续层时的 Winogrande 任务准确性。第 17 层表示如果保持第一层不变并删除第 2 层到第
    17 层的语言模型损失。虚线对应于移除 16 层不连续层时的准确性，最小化损失增加。
- en: 5 Evaluation
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估
- en: Benchmarks
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基准
- en: 'following Touvron et al. [[19](#bib.bib19)], we evaluate our compressed models
    on a series of downstream tasks, including MMLU [[20](#bib.bib20)], HumanEval [[21](#bib.bib21)]
    for Python code generation, several question-answering datasets for common-sense
    reasoning: Arc-C [[22](#bib.bib22)], HellaSwag [[23](#bib.bib23)], TruthfulQA [[24](#bib.bib24)]
    and WinoGrande [[7](#bib.bib7)] and XL-Sum English [[25](#bib.bib25)] for summarization.
    We report the 5-shot performance on MMLU, 5-shot on Winogrande, 25-shot on ARC-Challenge,
    10-shot on HellaSwag, 0-shot on 20% of XL-Sum and average pass@1 scores for HumanEval
    and MBPP. For pass@1 scores we use a temperature of 0.2 and nucleus sampling [[26](#bib.bib26)]
    with top-p $=$ 0.95. For instruction-tuned models, we use MT-Bench [[15](#bib.bib15)],
    Instruction-Following Eval (IFEval) [[14](#bib.bib14)], ChatRAG-Bench [[16](#bib.bib16)],
    and Berkeley Function Calling Leaderboard (BFCL) [[17](#bib.bib17)].'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 参考 Touvron 等人 [[19](#bib.bib19)] 的方法，我们在一系列下游任务上评估了我们压缩的模型，包括 MMLU [[20](#bib.bib20)]、用于
    Python 代码生成的 HumanEval [[21](#bib.bib21)]、用于常识推理的几个问答数据集：Arc-C [[22](#bib.bib22)]、HellaSwag
    [[23](#bib.bib23)]、TruthfulQA [[24](#bib.bib24)] 和 WinoGrande [[7](#bib.bib7)]，以及用于总结的
    XL-Sum English [[25](#bib.bib25)]。我们报告了 MMLU 上的 5-shot 性能、Winogrande 上的 5-shot、ARC-Challenge
    上的 25-shot、HellaSwag 上的 10-shot、XL-Sum 20% 上的 0-shot，以及 HumanEval 和 MBPP 的平均 pass@1
    分数。对于 pass@1 分数，我们使用温度为 0.2 和 top-p $=$ 0.95 的 nucleus sampling [[26](#bib.bib26)]。对于指令调优模型，我们使用
    MT-Bench [[15](#bib.bib15)]、Instruction-Following Eval (IFEval) [[14](#bib.bib14)]、ChatRAG-Bench
    [[16](#bib.bib16)] 和 Berkeley Function Calling Leaderboard (BFCL) [[17](#bib.bib17)]。
- en: 5.1 Base Models
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基础模型
- en: 'Base model evaluation results are shown in Table [1](#S0.T1 "Table 1 ‣ LLM
    Pruning and Distillation in Practice: The Minitron Approach"). Compared to similarly-sized
    models, MN-Minitron-8B demonstrates superior accuracy across the board, outperforming
    the recent Llama 3.1 8B model using 40$\times$ fewer training tokens (94B vs.
    15T); our pruned Llama models also outperform the previous generation Minitron
    4B model. We note from Table [1](#S0.T1 "Table 1 ‣ LLM Pruning and Distillation
    in Practice: The Minitron Approach") that the width-pruned variant outperforms
    the depth-pruned one. These results clearly demonstrate the advantages of our
    methodology: state-of-the-art accuracy coupled with an order of magnitude improvement
    in training efficiency.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 基本模型评估结果见表[1](#S0.T1 "表 1 ‣ LLM 剪枝与蒸馏实践：Minitron 方法")。与同类规模的模型相比，MN-Minitron-8B在各个方面的准确性均表现优越，使用40$\times$更少的训练令牌（94B对15T）超越了近期的Llama
    3.1 8B模型；我们的剪枝Llama模型也超越了上一代Minitron 4B模型。从表[1](#S0.T1 "表 1 ‣ LLM 剪枝与蒸馏实践：Minitron
    方法")中我们可以看到，宽度剪枝变体优于深度剪枝变体。这些结果清楚地展示了我们方法的优势：*最先进的准确性*加上训练效率提高了一个数量级。
- en: 5.2 Instruct Models
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 指令模型
- en: 'The performance of the instruction-tuned Llama-3.1-Minitron 4B variants is
    shown in Table [2](#S0.T2 "Table 2 ‣ LLM Pruning and Distillation in Practice:
    The Minitron Approach"). We compare the Llama-3.1-Minitron 4B variants to other
    similarly-sized baselines and notice that our models demonstrate strong instruction-following
    and roleplay capabilities, only lagging behind Gemma2 in IFEval [[14](#bib.bib14)]
    and MT-Bench [[15](#bib.bib15)]. On retrieval based question answering (ChatRAG-Bench [[16](#bib.bib16)])
    and function-calling (BFCL [[17](#bib.bib17)]), Minitron models achieve state-of-the-art
    performance.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 指令调整后的Llama-3.1-Minitron 4B变体的性能见表[2](#S0.T2 "表 2 ‣ LLM 剪枝与蒸馏实践：Minitron 方法")。我们将Llama-3.1-Minitron
    4B变体与其他相似规模的基线进行比较，发现我们的模型在指令跟随和角色扮演能力方面表现出色，仅在IFEval [[14](#bib.bib14)] 和 MT-Bench
    [[15](#bib.bib15)] 上落后于Gemma2。在基于检索的问题回答（ChatRAG-Bench [[16](#bib.bib16)]) 和函数调用（BFCL
    [[17](#bib.bib17)]）方面，Minitron模型实现了*最先进的性能*。
- en: 5.3 Runtime Performance Analysis
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 运行时性能分析
- en: We optimized the Llama 3.1 8B and Llama-3.1-Minitron 4B variants with NVIDIA
    TensorRT-LLM, an open-source toolkit for optimized LLM inference.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用NVIDIA TensorRT-LLM对Llama 3.1 8B和Llama-3.1-Minitron 4B变体进行了优化，这是一款用于优化LLM推理的开源工具包。
- en: '![Refer to caption](img/e29f934a9076c4f8bd89f417a79c42de.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e29f934a9076c4f8bd89f417a79c42de.png)'
- en: 'Figure 10: TensorRT-LLM FP8 throughput comparison for the Llama-3.1-Minitron-4B
    models with the Llama 3.1 8B model w.r.t. increasing input and output sequence
    lengths.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：Llama-3.1-Minitron-4B模型与Llama 3.1 8B模型在增加输入和输出序列长度方面的TensorRT-LLM FP8吞吐量比较。
- en: 'Figure [10](#S5.F10 "Figure 10 ‣ 5.3 Runtime Performance Analysis ‣ 5 Evaluation
    ‣ LLM Pruning and Distillation in Practice: The Minitron Approach") shows the
    throughput in requests per second for the various models in FP8 precision obtained
    on a single H100 80 GB GPU. Different use cases are represented by increasing
    input sequence length/output sequence length (ISL/OSL) combinations, at a batch
    size of 32 and 64 for the 8B-12B models and the 4B models respectively. The smaller
    memory footprint of the 4B model allows for larger batches. We notice that Llama-3.1-Minitron-4B-Depth
    is the fastest, achieving an average throughput improvement of $2.7\times$.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图[10](#S5.F10 "图 10 ‣ 5.3 运行时性能分析 ‣ 5 评估 ‣ LLM 剪枝与蒸馏实践：Minitron 方法")显示了在单个H100
    80 GB GPU上，以FP8精度获得的各种模型的每秒请求吞吐量。不同的使用场景通过增加的输入序列长度/输出序列长度（ISL/OSL）组合进行表示，对于8B-12B模型和4B模型分别使用32和64的批量大小。4B模型的较小内存占用允许更大的批量。我们注意到Llama-3.1-Minitron-4B-Depth是最快的，平均吞吐量提高了$2.7\times$。
- en: 6 Insights
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 见解
- en: In this Section, we summarize some interesting and surprising observations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了一些有趣和令人惊讶的观察结果。
- en: 6.0.1 General
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.0.1 一般
- en: '1.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Teacher correction is crucial for distillation to work optimally on a new, unseen
    dataset. Fine-tuning the teacher with the dataset used for distillation in this
    manner yields over a 6% reduction in LM validation loss. Teacher correction doesn’t
    affect the optimality of pruning and can even be performed in parallel with distillation.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 教师修正对蒸馏在新的未见数据集上达到*最佳效果*至关重要。用这种方式对教师进行微调，能够在语言模型验证损失上减少超过6%。教师修正不会影响剪枝的*最终效果*，甚至可以与蒸馏并行进行。
- en: '2.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: In line with the Minitron paper’s observations, we require only 380B tokens
    to achieve state-of-the-art accuracy post pruning with distillation.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据 Minitron 论文的观察，我们只需 380B 令牌即可在剪枝和蒸馏后实现最先进的准确性。
- en: '3.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: For width pruning, we achieve stronger accuracy by retaining attention heads
    and pruning the other dimensions (MLP intermediate dimension, embedding channels).
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于宽度剪枝，我们通过保留注意力头并剪枝其他维度（MLP中间维度，嵌入通道）来实现更强的准确性。
- en: '6.0.2 Mistral NeMo 12B to MN-Minitron-8B:'
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.0.2 Mistral NeMo 12B 到 MN-Minitron-8B：
- en: '1.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Our compressed model outperforms the teacher on two benchmarks, GSM8k and HumanEval
    after pruning and distillation: GSM8k increases from 55.7% to 58.5% and HumanEval
    increases from 23.8% to 36.2%. This improvement is likely influenced by the dataset.
    However, retraining is performed using the distillation loss alone.'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的压缩模型在两个基准测试中超越了教师模型，即 GSM8k 和 HumanEval：GSM8k 从 55.7% 增加到 58.5%，HumanEval
    从 23.8% 增加到 36.2%。这一改进可能受到数据集的影响。然而，重训练仅使用蒸馏损失。
- en: '6.0.3 Llama 3.1 8B to Llama-3.1-Minitron 4B:'
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.0.3 Llama 3.1 8B 到 Llama-3.1-Minitron 4B：
- en: '1.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Width pruning delivers better accuracy with MMLU at 60.5%, while depth pruning
    yields 58.7%, for Llama-3.1 compression.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 Llama-3.1 压缩，宽度剪枝在 MMLU 上提供了更好的准确性，为 60.5%，而深度剪枝为 58.7%。
- en: '2.'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Reasoning ability is impacted further significantly, with GSM8K accuracy at
    41.24% for width and 16.8% for depth.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理能力进一步显著受影响，宽度的 GSM8K 准确率为 41.24%，深度为 16.8%。
- en: '3.'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Depth pruning boosts throughput, achieving $2.7\times$ speedup.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度剪枝提升了吞吐量，实现了 $2.7\times$ 的加速。
- en: '4.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: For depth pruning, we observe that dropping contiguous layers from the model
    is more effective than using non-contiguous, importance-based pruning.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于深度剪枝，我们观察到，从模型中去除连续的层比使用非连续的基于重要性的剪枝更有效。
- en: 7 Acknowledgments
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 致谢
- en: 'This work would not have been possible without contributions from many people
    at NVIDIA. To mention a few:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 没有 NVIDIA 众多人员的贡献，这项工作将无法实现。提及几位：
- en: 'Foundational Model: Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj
    Joshi, Marcin Chochowski, Pavlo Molchanov, Mostofa Patwary, Daniel Korzekwa, Ashwath
    Aithal, Mohammad Shoeybi, Bryan Catanzaro and Jan Kautz'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型：Sharath Turuvekere Sreenivas，Saurav Muralidharan，Raviraj Joshi，Marcin Chochowski，Pavlo
    Molchanov，Mostofa Patwary，Daniel Korzekwa，Ashwath Aithal，Mohammad Shoeybi，Bryan
    Catanzaro 和 Jan Kautz
- en: 'Alignment: Ameya Sunil Mahabaleshwarkar, Hayley Ross, Brandon Rowlett, Oluwatobi
    Olabiyi, Shizhe Diao and Yoshi Suhara'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐：Ameya Sunil Mahabaleshwarkar，Hayley Ross，Brandon Rowlett，Oluwatobi Olabiyi，Shizhe
    Diao 和 Yoshi Suhara
- en: 'Datasets: Sanjeev Satheesh, Jupinder Parmar, Shengyang Sun, Jiaqi Zeng, Zhilin
    Wang, Yi Dong, Zihan Liu, Rajarshi Roy, Wei Ping, Makesh Narsimhan Sreedhar and
    Oleksii Kuchaiev'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集：Sanjeev Satheesh，Jupinder Parmar，Shengyang Sun，Jiaqi Zeng，Zhilin Wang，Yi
    Dong，Zihan Liu，Rajarshi Roy，Wei Ping，Makesh Narsimhan Sreedhar 和 Oleksii Kuchaiev
- en: 'TensorRT-LLM: Bobby Chen, James Shen and Chenhan Yu'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: TensorRT-LLM：Bobby Chen，James Shen 和 Chenhan Yu
- en: 'Hugging Face Support: Ao Tang, Yoshi Suhara and Greg Heinrich'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 支持：Ao Tang，Yoshi Suhara 和 Greg Heinrich
- en: References
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin
    Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and
    Pavlo Molchanov. Compact language models via pruning and knowledge distillation.
    arXiv preprint arXiv:2407.14679, 2024.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Saurav Muralidharan，Sharath Turuvekere Sreenivas，Raviraj Joshi，Marcin Chochowski，Mostofa
    Patwary，Mohammad Shoeybi，Bryan Catanzaro，Jan Kautz 和 Pavlo Molchanov。通过剪枝和知识蒸馏实现的紧凑语言模型。arXiv
    预印本 arXiv:2407.14679，2024年。'
- en: '[2] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan
    Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds,
    Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben
    Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation,
    12 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Leo Gao，Jonathan Tow，Baber Abbasi，Stella Biderman，Sid Black，Anthony DiPofi，Charles
    Foster，Laurence Golding，Jeffrey Hsu，Alain Le Noac’h，Haonan Li，Kyle McDonell，Niklas
    Muennighoff，Chris Ociepa，Jason Phang，Laria Reynolds，Hailey Schoelkopf，Aviya Skowron，Lintang
    Sutawika，Eric Tang，Anish Thite，Ben Wang，Kevin Wang 和 Andy Zou。少样本语言模型评估框架，2023年12月。'
- en: '[3] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge
    in a Neural Network. arXiv preprint arXiv:1503.02531, 2015.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Geoffrey Hinton，Oriol Vinyals 和 Jeff Dean。提炼神经网络中的知识。arXiv 预印本 arXiv:1503.02531，2015年。'
- en: '[4] Abhimanyu Dubey and Abhinav Jauhri et al. The Llama 3 Herd of Models. arXiv
    2407.21783, 2024.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Abhimanyu Dubey 和 Abhinav Jauhri 等。Llama 3 模型群。arXiv 2407.21783，2024年。'
- en: '[5] Mistral AI team. Mistral nemo. https://mistral.ai/news/mistral-nemo, 2024.
    Accessed: 2024.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Mistral AI团队。Mistral nemo。https://mistral.ai/news/mistral-nemo，2024年。访问日期：2024年。'
- en: '[6] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu,
    Xianpei Han, and Weipeng Chen. ShortGPT: Layers in Large Language Models are More
    Redundant Than You Expect, 2024.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Xin Men、Mingyu Xu、Qingyu Zhang、Bingning Wang、Hongyu Lin、Yaojie Lu、Xianpei
    Han 和 Weipeng Chen。ShortGPT：大语言模型中的层比你预期的更冗余，2024年。'
- en: '[7] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
    WinoGrande: An adversarial winograd schema challenge at scale. Commun. ACM, 64(9),
    2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Keisuke Sakaguchi、Ronan Le Bras、Chandra Bhagavatula 和 Yejin Choi。WinoGrande：大规模对抗性
    Winograd 方案挑战。ACM 通讯，64(9)，2021年。'
- en: '[8] Solomon Kullback and Richard A. Leibler. On information and sufficiency.
    Annals of Mathematical Statistics, 22(1):79–86, 1951.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Solomon Kullback 和 Richard A. Leibler。关于信息与充分性。数学统计年刊，22(1)：79–86，1951年。'
- en: '[9] Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary,
    Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala,
    Ayush Dattagupta, Vibhu Jawa, Jiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski,
    Annika Brundyn, James Maki, Miguel Martinez, Jiaxuan You, John Kamalu, Patrick
    LeGresley, Denys Fridman, Jared Casper, Ashwath Aithal, Oleksii Kuchaiev, Mohammad
    Shoeybi, Jonathan Cohen, and Bryan Catanzaro. Nemotron-4 15b technical report,
    2024.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Jupinder Parmar、Shrimai Prabhumoye、Joseph Jennings、Mostofa Patwary、Sandeep
    Subramanian、Dan Su、Chen Zhu、Deepak Narayanan、Aastha Jhunjhunwala、Ayush Dattagupta、Vibhu
    Jawa、Jiwei Liu、Ameya Mahabaleshwarkar、Osvald Nitski、Annika Brundyn、James Maki、Miguel
    Martinez、Jiaxuan You、John Kamalu、Patrick LeGresley、Denys Fridman、Jared Casper、Ashwath
    Aithal、Oleksii Kuchaiev、Mohammad Shoeybi、Jonathan Cohen 和 Bryan Catanzaro。Nemotron-4
    15b 技术报告，2024年。'
- en: '[10] Jupinder Parmar, Sanjev Satheesh, Mostofa Patwary, Mohammad Shoeybi, and
    Bryan Catanzaro. Reuse, don’t retrain: A recipe for continued pretraining of language
    models, 2024.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Jupinder Parmar、Sanjev Satheesh、Mostofa Patwary、Mohammad Shoeybi 和 Bryan
    Catanzaro。重用，而非重训练：继续预训练语言模型的秘方，2024年。'
- en: '[11] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and
    Daniel A. Roberts. The unreasonable ineffectiveness of the deeper layers. 2024.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Andrey Gromov、Kushal Tirumala、Hassan Shapourian、Paolo Glorioso 和 Daniel
    A. Roberts。更深层次的无效性。2024年。'
- en: '[12] Gerald Shen, Zhilin Wang, Olivier Delalleau, Jiaqi Zeng, Yi Dong, Daniel
    Egert, Shengyang Sun, Jimmy Zhang, Sahil Jain, Ali Taghibakhshi, Markel Sanz Ausin,
    Ashwath Aithal, and Oleksii Kuchaiev. Nemo-aligner: Scalable toolkit for efficient
    model alignment, 2024.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Gerald Shen、Zhilin Wang、Olivier Delalleau、Jiaqi Zeng、Yi Dong、Daniel Egert、Shengyang
    Sun、Jimmy Zhang、Sahil Jain、Ali Taghibakhshi、Markel Sanz Ausin、Ashwath Aithal 和
    Oleksii Kuchaiev。Nemo-aligner：高效模型对齐的可扩展工具包，2024年。'
- en: '[13] Nvidia, :, Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab
    Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan
    Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong,
    Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris
    Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa,
    Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev,
    Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar,
    Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan
    Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald
    Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar,
    Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy,
    Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft,
    Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyanskiy,
    Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang
    Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy
    Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, and Chen Zhu. Nemotron-4 340b technical
    report, 2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Nvidia，：Bo Adler、Niket Agarwal、Ashwath Aithal、Dong H. Anh、Pallab Bhattacharya、Annika
    Brundyn、Jared Casper、Bryan Catanzaro、Sharon Clay、Jonathan Cohen、Sirshak Das、Ayush
    Dattagupta、Olivier Delalleau、Leon Derczynski、Yi Dong、Daniel Egert、Ellie Evans、Aleksander
    Ficek、Denys Fridman、Shaona Ghosh、Boris Ginsburg、Igor Gitman、Tomasz Grzegorzek、Robert
    Hero、Jining Huang、Vibhu Jawa、Joseph Jennings、Aastha Jhunjhunwala、John Kamalu、Sadaf
    Khan、Oleksii Kuchaiev、Patrick LeGresley、Hui Li、Jiwei Liu、Zihan Liu、Eileen Long、Ameya
    Sunil Mahabaleshwarkar、Somshubra Majumdar、James Maki、Miguel Martinez、Maer Rodrigues
    de Melo、Ivan Moshkov、Deepak Narayanan、Sean Narenthiran、Jesus Navarro、Phong Nguyen、Osvald
    Nitski、Vahid Noroozi、Guruprasad Nutheti、Christopher Parisien、Jupinder Parmar、Mostofa
    Patwary、Krzysztof Pawelec、Wei Ping、Shrimai Prabhumoye、Rajarshi Roy、Trisha Saar、Vasanth
    Rao Naik Sabavat、Sanjeev Satheesh、Jane Polak Scowcroft、Jason Sewall、Pavel Shamis、Gerald
    Shen、Mohammad Shoeybi、Dave Sizer、Misha Smelyanskiy、Felipe Soares、Makesh Narsimhan
    Sreedhar、Dan Su、Sandeep Subramanian、Shengyang Sun、Shubham Toshniwal、Hao Wang、Zhilin
    Wang、Jiaxuan You、Jiaqi Zeng、Jimmy Zhang、Jing Zhang、Vivienne Zhang、Yian Zhang 和
    Chen Zhu。Nemotron-4 340b 技术报告，2024年。'
- en: '[14] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu,
    Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language
    models. arXiv preprint arXiv:2311.07911, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu,
    Yi Luan, Denny Zhou, 和 Le Hou. 大型语言模型的指令跟随评估。arXiv 预印本 arXiv:2311.07911, 2023。'
- en: '[15] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,
    Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E
    Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena.
    In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors,
    Advances in Neural Information Processing Systems, volume 36, pages 46595–46623\.
    Curran Associates, Inc., 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,
    Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E
    Gonzalez, 和 Ion Stoica. 使用 mt-bench 和 chatbot arena 评判 llm-as-a-judge。在 A. Oh,
    T. Naumann, A. Globerson, K. Saenko, M. Hardt, 和 S. Levine 编者的《神经信息处理系统进展》卷 36,
    页码 46595–46623。Curran Associates, Inc., 2023。'
- en: '[16] Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi,
    and Bryan Catanzaro. Chatqa: Surpassing gpt-4 on conversational qa and rag. arXiv
    preprint arXiv:2401.10225, 2024.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi,
    和 Bryan Catanzaro. Chatqa：在对话 QA 和 RAG 上超越 GPT-4。arXiv 预印本 arXiv:2401.10225, 2024。'
- en: '[17] Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G.
    Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard.
    [https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html),
    2024.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir
    G. Patil, Ion Stoica, 和 Joseph E. Gonzalez. Berkeley 函数调用排行榜。 [https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html),
    2024。'
- en: '[18] Shoaib Ahmed Siddiqui, Xin Dong, Greg Heinrich, Thomas Breuel, Jan Kautz,
    David Krueger, and Pavlo Molchanov. A deeper look at depth pruning of llms. arXiv
    preprint arXiv:2407.16286, 2024.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Shoaib Ahmed Siddiqui, Xin Dong, Greg Heinrich, Thomas Breuel, Jan Kautz,
    David Krueger, 和 Pavlo Molchanov. 深入探讨 llms 的深度剪枝。arXiv 预印本 arXiv:2407.16286,
    2024。'
- en: '[19] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, 和 Thomas Scialom. Llama 2:
    开放基础和微调的聊天模型。ArXiv, abs/2307.09288, 2023。'
- en: '[20] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,
    Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding.
    In International Conference on Learning Representations, 2021.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,
    Dawn Song, 和 Jacob Steinhardt. 测量大规模多任务语言理解。在国际学习表征会议，2021。'
- en: '[21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared
    Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray,
    Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela
    Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
    Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such,
    David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
    Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu
    Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec
    Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder,
    Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
    Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Mark Chen、Jerry Tworek、Heewoo Jun、Qiming Yuan、Henrique Ponde、Jared Kaplan、Harrison
    Edwards、Yura Burda、Nicholas Joseph、Greg Brockman、Alex Ray、Raul Puri、Gretchen Krueger、Michael
    Petrov、Heidy Khlaaf、Girish Sastry、Pamela Mishkin、Brooke Chan、Scott Gray、Nick Ryder、Mikhail
    Pavlov、Alethea Power、Lukasz Kaiser、Mohammad Bavarian、Clemens Winter、Philippe Tillet、Felipe
    Petroski Such、David W. Cummings、Matthias Plappert、Fotios Chantzis、Elizabeth Barnes、Ariel
    Herbert-Voss、William H. Guss、Alex Nichol、Igor Babuschkin、Suchir Balaji、Shantanu
    Jain、Andrew Carr、Jan Leike、Joshua Achiam、Vedant Misra、Evan Morikawa、Alec Radford、Matthew
    M. Knight、Miles Brundage、Mira Murati、Katie Mayer、Peter Welinder、Bob McGrew、Dario
    Amodei、Sam McCandlish、Ilya Sutskever 和 Wojciech Zaremba。评估在代码上训练的大型语言模型。ArXiv，abs/2107.03374，2021年。'
- en: '[22] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try ARC, the AI2 reasoning challenge. ArXiv, abs/1803.05457, 2018.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Peter Clark、Isaac Cowhey、Oren Etzioni、Tushar Khot、Ashish Sabharwal、Carissa
    Schoenick 和 Oyvind Tafjord。认为你已经解决了问答问题？试试ARC，AI2推理挑战。ArXiv，abs/1803.05457，2018年。'
- en: '[23] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    HellaSwag: Can a machine really finish your sentence? In Anna Korhonen, David
    Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the
    Association for Computational Linguistics, Florence, Italy, July 2019\. Association
    for Computational Linguistics.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Rowan Zellers、Ari Holtzman、Yonatan Bisk、Ali Farhadi 和 Yejin Choi。HellaSwag:
    机器真的能完成你的句子吗？收录于 Anna Korhonen、David Traum 和 Lluís Màrquez 编辑的《第57届计算语言学协会年会论文集》，意大利佛罗伦萨，2019年7月。计算语言学协会。'
- en: '[24] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how
    models mimic human falsehoods, 2022.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Stephanie Lin、Jacob Hilton 和 Owain Evans。Truthfulqa: 测量模型如何模仿人类虚假信息，2022年。'
- en: '[25] Tahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam, Kazi Samin, Yuan-Fang
    Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. Xl-sum: Large-scale multilingual
    abstractive summarization for 44 languages, 2021.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Tahmid Hasan、Abhik Bhattacharjee、Md Saiful Islam、Kazi Samin、Yuan-Fang
    Li、Yong-Bin Kang、M. Sohel Rahman 和 Rifat Shahriyar。Xl-sum: 44种语言的大规模多语言抽象总结，2021年。'
- en: '[26] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious
    case of neural text degeneration. ArXiv, abs/1904.09751, 2019.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Ari Holtzman、Jan Buys、Li Du、Maxwell Forbes 和 Yejin Choi。神经文本退化的奇特案例。ArXiv，abs/1904.09751，2019年。'
