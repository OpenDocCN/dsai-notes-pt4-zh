- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:50:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:03
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the
    Acceleration of Lightweight LLMs on the Edge'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'EdgeQAT: 熵与分布指导的量化感知训练，用于加速轻量级LLM在边缘的应用'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10787](https://ar5iv.labs.arxiv.org/html/2402.10787)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10787](https://ar5iv.labs.arxiv.org/html/2402.10787)
- en: Xuan Shen¹, Zhenglun Kong¹, Changdi Yang¹, Zhaoyang Han¹, Lei Lu¹, Peiyan Dong¹,
    Cheng Lyu¹,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xuan Shen¹, Zhenglun Kong¹, Changdi Yang¹, Zhaoyang Han¹, Lei Lu¹, Peiyan Dong¹,
    Cheng Lyu¹,
- en: 'Chih-hsiang Li¹, Xuehang Guo³, Zhihao Shu², Wei Niu², Miriam Leeser¹, Pu Zhao¹,
    Yanzhi Wang¹¹footnotemark: 1¹'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'Chih-hsiang Li¹, Xuehang Guo³, Zhihao Shu², Wei Niu², Miriam Leeser¹, Pu Zhao¹,
    Yanzhi Wang¹¹脚注标记: 1¹'
- en: ¹Northeastern University   ²Georgia University   ³Pittsburgh University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹东北大学   ²乔治亚大学   ³匹兹堡大学
- en: '{shen.xu, kong.zhe, yang.changd, lu.lei1, dong.pe, li.chih, zhao.pu, yanz.wang}@northeastern.edu,'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{shen.xu, kong.zhe, yang.changd, lu.lei1, dong.pe, li.chih, zhao.pu, yanz.wang}@northeastern.edu,'
- en: '{zhhan, mel}@coe.neu.edu'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{zhhan, mel}@coe.neu.edu'
- en: '{wniu, Zhihao.Shu}@uga.edu'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{wniu, Zhihao.Shu}@uga.edu'
- en: xug13@pitt.edu, cheng.lv@colorado.edu Corresponding Author
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: xug13@pitt.edu, cheng.lv@colorado.edu 主要联系人
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Despite the remarkable strides of Large Language Models (LLMs) in various fields,
    the wide applications of LLMs on edge devices are limited due to their massive
    parameters and computations. To address this, quantization is commonly adopted
    to generate lightweight LLMs with efficient computations and fast inference. However,
    Post-Training Quantization (PTQ) methods dramatically degrade in quality when
    quantizing weights, activations, and KV cache together to below 8 bits. Besides,
    many Quantization-Aware Training (QAT) works quantize model weights, leaving the
    activations untouched, which do not fully exploit the potential of quantization
    for inference acceleration on the edge. In this paper, we propose EdgeQAT, the
    Entropy and Distribution Guided QAT for the optimization of lightweight LLMs to
    achieve inference acceleration on Edge devices. We first identify that the performance
    drop of quantization primarily stems from the information distortion in quantized
    attention maps, demonstrated by the different distributions in quantized query
    and key of the self-attention mechanism. Then, the entropy and distribution guided
    QAT is proposed to mitigate the information distortion. Moreover, we design a
    token importance-aware adaptive method to dynamically quantize the tokens with
    different bit widths for further optimization and acceleration. Our extensive
    experiments verify the substantial improvements with our framework across various
    datasets. Furthermore, we achieve an on-device speedup of up to 2.37$\times$ compared
    with its FP16 counterparts across multiple edge devices, signaling a groundbreaking
    advancement. The code is available here:[https://github.com/shawnricecake/EdgeQAT](https://github.com/shawnricecake/EdgeQAT)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）在多个领域取得了显著进展，但由于其庞大的参数和计算量，LLMs 在边缘设备上的广泛应用仍然受到限制。为了解决这个问题，通常采用量化技术生成具有高效计算和快速推理的轻量级LLM。然而，当将权重、激活和KV缓存量化到低于8位时，后训练量化（PTQ）方法的质量会显著下降。此外，许多量化感知训练（QAT）方法对模型权重进行量化，而忽略了激活，这并未充分发挥量化在边缘推理加速中的潜力。在本文中，我们提出了
    EdgeQAT，一种熵与分布指导的 QAT，用于优化轻量级LLM，实现边缘设备上的推理加速。我们首先确定量化性能下降主要源于量化注意力图中的信息失真，这通过自注意机制中量化的查询和键的不同分布得以证明。然后，提出了熵与分布指导的
    QAT 以减轻信息失真。此外，我们设计了一种基于令牌重要性的自适应方法，以不同的位宽动态量化令牌，以进一步优化和加速。我们的大量实验验证了我们框架在各种数据集上的显著改进。此外，与
    FP16 版本相比，我们在多个边缘设备上实现了高达2.37倍的设备速度提升，标志着突破性的进展。代码可在此处获取：[https://github.com/shawnricecake/EdgeQAT](https://github.com/shawnricecake/EdgeQAT)
- en: 'EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the
    Acceleration of Lightweight LLMs on the Edge'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 'EdgeQAT: 熵与分布指导的量化感知训练，用于加速边缘设备上的轻量级LLM'
- en: 'Xuan Shen¹, Zhenglun Kong¹, Changdi Yang¹, Zhaoyang Han¹, Lei Lu¹, Peiyan Dong¹,
    Cheng Lyu¹, Chih-hsiang Li¹, Xuehang Guo³, Zhihao Shu², Wei Niu², Miriam Leeser¹,
    Pu Zhao^†^†thanks: Corresponding Author¹, Yanzhi Wang¹¹footnotemark: 1¹ ¹Northeastern
    University   ²Georgia University   ³Pittsburgh University {shen.xu, kong.zhe,
    yang.changd, lu.lei1, dong.pe, li.chih, zhao.pu, yanz.wang}@northeastern.edu,
    {zhhan, mel}@coe.neu.edu {wniu, Zhihao.Shu}@uga.edu xug13@pitt.edu, cheng.lv@colorado.edu'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Xuan Shen¹, Zhenglun Kong¹, Changdi Yang¹, Zhaoyang Han¹, Lei Lu¹, Peiyan Dong¹,
    Cheng Lyu¹, Chih-hsiang Li¹, Xuehang Guo³, Zhihao Shu², Wei Niu², Miriam Leeser¹,
    Pu Zhao^†^†感谢：通讯作者¹, Yanzhi Wang¹¹脚注：1¹ ¹东北大学   ²乔治亚大学   ³ピッツバーグ大学 {shen.xu, kong.zhe,
    yang.changd, lu.lei1, dong.pe, li.chih, zhao.pu, yanz.wang}@northeastern.edu,
    {zhhan, mel}@coe.neu.edu {wniu, Zhihao.Shu}@uga.edu xug13@pitt.edu, cheng.lv@colorado.edu
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) Zhang et al. ([2022](#bib.bib36)); Radford et al.
    ([2019b](#bib.bib23)); Brown et al. ([2020a](#bib.bib3), [b](#bib.bib4)); Touvron
    et al. ([2023](#bib.bib27)) based on Transformers Vaswani et al. ([2017](#bib.bib28))
    have emerged as the dominant force in the field of Natural Language Processing
    (NLP). There is a growing trend of integrating LLMs for various applications to
    optimize user experiences and task performance. The deployment of LLMs typically
    demands substantial computations and storage resources. For example, the LLaMA-7B Touvron
    et al. ([2023](#bib.bib27)) model with 7 billion parameters takes up 13.5GB of
    memory. Moreover, the largest 65B model in the LLaMA family needs hundreds of
    GB for memory. Indeed, the extra-large LLMs such as GPT-3-175B Brown et al. ([2020b](#bib.bib4)),
    OPT-175B Zhang et al. ([2022](#bib.bib36)) and BLOOM-176B Workshop et al. ([2022](#bib.bib31)),
    demand 300GB+ memory usage, making the most powerful GPUs struggling to accommodate
    such capacity, let alone the resource-limited edge devices. Additionally, the
    long input sequence lengths of LLMs further augment the computation counts with
    lower throughputs during inference.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformers的**大语言模型（LLMs）**（Zhang等人（[2022](#bib.bib36)）；Radford等人（[2019b](#bib.bib23)）；Brown等人（[2020a](#bib.bib3),
    [b](#bib.bib4)）；Touvron等人（[2023](#bib.bib27)））已经成为自然语言处理（NLP）领域的主导力量。将LLM集成到各种应用中，以优化用户体验和任务性能的趋势日益增长。LLM的部署通常需要大量的计算和存储资源。例如，具有70亿参数的LLaMA-7B模型（Touvron等人（[2023](#bib.bib27)））占用13.5GB内存。此外，LLaMA系列中最大的65B模型需要数百GB的内存。实际上，像GPT-3-175B（Brown等人（[2020b](#bib.bib4)））、OPT-175B（Zhang等人（[2022](#bib.bib36)））和BLOOM-176B（Workshop等人（[2022](#bib.bib31)））这样的超大规模LLM需要300GB以上的内存，这使得最强大的GPU也难以容纳这样的容量，更不用说资源有限的边缘设备了。此外，LLM的长输入序列长度进一步增加了计算量，并在推理过程中导致吞吐量降低。
- en: Thus, it is necessary to adopt model compression techniques to reduce the resource
    requirement and facilitate the LLM deployment. Among them, quantization presents
    a promising avenue to substantially deploy LLMs on edge devices, such as mobile
    phones, Raspberry Pis, and FPGAs. Besides requiring fewer resources, quantization
    can effectively accelerate the computation with higher throughput and improve
    energy efficiency, by leveraging the highly efficient 8-bit fixed-point (INT8)
    operations on edge platforms.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有必要采用模型压缩技术来减少资源需求并促进LLM的部署。其中，量化是一个有前景的途径，可以在边缘设备上大规模部署LLM，如手机、树莓派和FPGA。除了需要更少的资源外，量化还可以通过利用边缘平台上高效的8位定点（INT8）操作，显著加速计算，提高吞吐量和能源效率。
- en: Existing works primarily employ Post-Training Quantization (PTQ), which suffers
    from significant accuracy degradation under low-bit settings as they do not incorporate
    model finetuning or retraining to restore accuracy. Quantization-Aware Training
    (QAT) presents a promising avenue for better performance in lower-bit configurations,
    but its data accessibility, training cost, and acceleration issues have not been
    effectively addressed. For example, nearly all QAT works focus on weight-only
    quantization, without quantizing the floating-point activations. Thus, they still
    need to utilize the floating-point operations during the computation, which can
    not benefit from the highly efficient lower-precision operations on edge devices
    (such as INT8$\times$INT8 integer multipliers) for further speedup. The difficulty
    of activation quantization lies in the pronounced outliers in activations, leading
    to the detrimental effect and thus significant performance degradation, particularly
    for large model sizes. The work Dettmers et al. ([2022](#bib.bib7)) demonstrates
    that directly setting the outliers to zero leads to a 45% performance degradation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现有工作主要采用后训练量化（PTQ），这种方法在低位设置下准确率显著下降，因为它们没有结合模型微调或重新训练来恢复准确率。量化感知训练（QAT）为在低位配置下获得更好的性能提供了有希望的途径，但其数据可访问性、训练成本和加速问题尚未得到有效解决。例如，几乎所有QAT工作都专注于仅量化权重，而未量化浮点激活。因此，它们仍需在计算过程中使用浮点操作，这不能从边缘设备（如INT8$\times$INT8整数乘法器）上高效的低精度操作中受益，从而无法进一步加速。激活量化的难点在于激活中的显著离群值，这导致有害的影响，从而使性能显著下降，特别是对于大型模型。Dettmers等人的研究（[2022](#bib.bib7)）表明，直接将离群值设为零会导致45%的性能下降。
- en: To deal with the above mentioned challenges, we propose EdgeQAT, the Entropy
    and Distribution Guided QAT, to achieve the acceleration for lightweight LLMs
    on the Edge. Specifically, to identify the bottleneck of activation quantization,
    we initiate our analysis by examining the performance degradation induced by activation
    quantization across various parts of the LLaMA model. We observe that the quantized
    query and key within the self-attention mechanism lead to the most significant
    accuracy loss, due to the substantial disparities between the generated attention
    map and its FP16 counterpart. To address this, we propose the entropy and distribution
    guided optimization method to mitigate the accuracy loss. In detail, we maximize
    the entropy of the query and key based on their distribution to equivalently minimize
    their quantization error. Meanwhile, we optimize the cosine similarity between
    the quantized and FP16 attention maps to minimize their difference with better
    performance. After the distribution of the quantized attention map is restored,
    we further introduce the token importance-aware adaptive quantization to quantize
    the activations with fewer bits.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对上述挑战，我们提出了EdgeQAT，即熵与分布引导的量化训练（QAT）方法，以加速边缘设备上的轻量级大模型。具体而言，为了识别激活量化的瓶颈，我们通过分析LLaMA模型各部分激活量化引发的性能下降来开始我们的研究。我们观察到，自注意力机制中的量化查询和键会导致最显著的准确率损失，这是因为生成的注意力图与其FP16对应物之间存在较大的差异。为了解决这个问题，我们提出了熵与分布引导的优化方法来减轻准确率损失。具体来说，我们基于查询和键的分布最大化其熵，以等效地最小化它们的量化误差。同时，我们优化量化的注意力图和FP16注意力图之间的余弦相似性，以减少它们之间的差异，从而获得更好的性能。在恢复量化注意力图的分布后，我们进一步引入了基于令牌重要性的自适应量化方法，以用更少的位数量化激活值。
- en: By employing QAT for both weights and activations, we aim to minimize quantization
    error and achieve significant inference acceleration on edge devices. As training
    all parameters for large LLMs demands extensive GPU resources and high-quality
    data, we opt for lightweight LLM models in this paper for experimentation and
    deployment on the edge. We mainly quantize the weights and activations to 4 bits
    and 8 bits, following the binary nature of internal representations in computers
    and the trend toward advancing direct support for 4-bit operations with an increasing
    number of architectures (such as RISC-V). The proposed EdgeQAT can maintain state-of-the-art
    task performance comparable to FP16 counterparts while achieving a practical on-device
    speedup of up to 2.37$\times$.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对权重和激活进行量化感知训练（QAT），我们旨在最小化量化误差，并在边缘设备上实现显著的推理加速。由于训练大型LLM的所有参数需要大量的GPU资源和高质量的数据，我们在本文中选择了轻量级LLM模型进行实验和边缘部署。我们主要将权重和激活量化为4位和8位，遵循计算机内部表示的二进制特性以及对4位操作的直接支持的趋势（如RISC-V）。提出的EdgeQAT可以保持与FP16模型相媲美的最先进任务性能，同时在设备上的实际加速高达2.37$\times$。
- en: 'We summarize our contributions as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了以下贡献：
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We design the entropy and distribution guided quantization method to mitigate
    information distortion in quantized query, key, and attention maps, addressing
    the bottleneck of QAT.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了熵和分布引导的量化方法，以减轻量化查询、键和值图中的信息失真，解决了QAT的瓶颈。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We design the token importance-aware adaptive quantization method to quantize
    the activations (i.e., tokens) with fewer bits, further improving the efficiency
    on edge devices.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了令牌重要性感知的自适应量化方法，以更少的位数对激活（即令牌）进行量化，进一步提升边缘设备上的效率。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We achieve state-of-the-art accuracy performance comparable to the FP16 model
    and better than other QAT methods. Our deployments across multiple edge devices
    demonstrate an on-device speedup of up to 2.37$\times$.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们实现了与FP16模型相当的最先进的准确性表现，并优于其他QAT方法。我们在多个边缘设备上的部署显示了高达2.37$\times$的设备加速。
- en: 2 Related Work
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Efficient Large Language Models
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 高效的大型语言模型
- en: Recent advancements in LLMs like GPT-4 Achiam et al. ([2023](#bib.bib1)) have
    significantly improved NLP capabilities at the cost of massive computations and
    energy, limiting their accessibility and applications. This has led to the emergence
    of efficient and lightweight LLMs to address these limitations without compromising
    performance. Models such as LLaMA Touvron et al. ([2023](#bib.bib27)), OPT Zhang
    et al. ([2022](#bib.bib36)), and BLOOM Workshop et al. ([2022](#bib.bib31)) offer
    a wide range of sizes, from as few as 125M to as many as 176B parameters, providing
    versatile options for various applications. To further enhance the efficiency
    of LLMs, multiple compression techniques have been developed Hu et al. ([2021](#bib.bib15));
    Frantar et al. ([2022](#bib.bib11)); Frantar and Alistarh ([2023](#bib.bib10));
    Fu et al. ([2023](#bib.bib12)). These methods aim to reduce model size and computational
    demands, enabling deployment on resource-constrained platforms such as edge devices.
    This shift towards more manageable models facilitates real-time NLP applications
    like virtual assistants and language translation, broadening the accessibility
    and utility of advanced NLP technologies.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 近期LLM的进展，如GPT-4 Achiam et al. ([2023](#bib.bib1))，在大规模计算和能源消耗的代价下显著提升了NLP能力，限制了其可及性和应用。这导致了高效轻量级LLM的出现，以解决这些限制而不影响性能。诸如LLaMA
    Touvron et al. ([2023](#bib.bib27))、OPT Zhang et al. ([2022](#bib.bib36))和BLOOM
    Workshop et al. ([2022](#bib.bib31))等模型提供了从125M到176B参数的多种规模，为各种应用提供了灵活的选择。为了进一步提升LLM的效率，开发了多种压缩技术
    Hu et al. ([2021](#bib.bib15))；Frantar et al. ([2022](#bib.bib11))；Frantar and
    Alistarh ([2023](#bib.bib10))；Fu et al. ([2023](#bib.bib12))。这些方法旨在减少模型的大小和计算需求，使其能够在资源受限的平台（如边缘设备）上部署。这种向更可管理模型的转变促进了实时NLP应用，如虚拟助手和语言翻译，拓宽了先进NLP技术的可及性和实用性。
- en: 2.2 Quantization for LLMs
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLMs的量化
- en: Quantization reduces DNN bit-precision, leading to smaller models and faster
    inference. Current methods are divided into PTQ and QAT, each offering distinct
    advantages and facing unique challenges. PTQ generally results in low accuracy,
    especially in low-bit quantizations. To address this, Smoothquant Xiao et al.
    ([2023](#bib.bib33)) achieves W8A8 precision by smoothing activation outliers,
    while ZeroQuant Yao et al. ([2022](#bib.bib35)) employs a layer-by-layer knowledge
    distillation algorithm to enhance low-bit quantization performance. Different
    from PTQ, QAT presents a promising avenue for better performance, requiring massive
    data and resources for fine-tuning, which is especially hard for LLMs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 量化减少了 DNN 的位精度，从而使模型更小，推理更快。当前的方法分为 PTQ 和 QAT，各自具有不同的优点和面临独特的挑战。PTQ 通常导致低精度，特别是在低位量化中。为了解决这一问题，Smoothquant
    Xiao 等人 ([2023](#bib.bib33)) 通过平滑激活异常值实现了 W8A8 精度，而 ZeroQuant Yao 等人 ([2022](#bib.bib35))
    采用逐层知识蒸馏算法来提升低位量化性能。与 PTQ 不同，QAT 展示了更好的性能前景，但需要大量数据和资源进行微调，这对 LLM 特别困难。
- en: Most PTQ and QAT works focus on weight-only quantization, and the weight-activation
    quantization to quantize both weights and activations is less explored. GPTQ Frantar
    et al. ([2022](#bib.bib11)) and AWQ Lin et al. ([2023](#bib.bib18)) focus on reducing
    the precision of weights while maintaining full-precision activations. Thus, their
    speedups may be limited due to the computational costs with full-precision activations.
    Only LLM-QAT Liu et al. ([2023](#bib.bib19)) employs data-free distillation methods
    to quantize the weights, activations, and KV caches for large models like LLaMA-7B,
    which can hardly be deployed on edge devices. The exploration of weight-activation
    quantization with QAT for lightweight LLMs facilitating deployment on the edge
    is still an open field.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 PTQ 和 QAT 工作集中于仅量化权重，而对权重和激活的量化关注较少。GPTQ Frantar 等人 ([2022](#bib.bib11))
    和 AWQ Lin 等人 ([2023](#bib.bib18)) 主要关注降低权重的精度，同时保持全精度激活。因此，由于全精度激活的计算成本，他们的加速效果可能有限。只有
    LLM-QAT Liu 等人 ([2023](#bib.bib19)) 采用无数据蒸馏方法来量化权重、激活和 KV 缓存，用于像 LLaMA-7B 这样的超大模型，这些模型很难在边缘设备上部署。针对轻量级
    LLM 的权重-激活量化与 QAT 的探索仍然是一个未解之谜。
- en: 3 Analysis
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 分析
- en: To pinpoint the bottleneck during QAT, we analyze the performance deterioration
    resulting from the quantization of each part of the model. Furthermore, we explore
    the token importance based on the attention map and discern the importance associated
    with the first initial token.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出 QAT 过程中的瓶颈，我们分析了模型每个部分量化所导致的性能下降。此外，我们基于注意力图探索了 token 的重要性，并辨别了与第一个初始 token
    相关的重要性。
- en: '![Refer to caption](img/ff1b63f138bba93c3bedfbb9e3324647.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ff1b63f138bba93c3bedfbb9e3324647.png)'
- en: 'Figure 1: Accuracy analysis on the Anaphor Agr. subdataset of BLiMP with different
    quantized modules.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：在不同量化模块下对 BLiMP 中 Anaphor Agr. 子数据集的准确性分析。
- en: 'Figure 2: Distributions of query and key at the last layer of FP16 and quantized
    models.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：FP16 和量化模型最后一层的查询和键的分布。
- en: 3.1 Quantized Self-Attention Module
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 量化自注意力模块
- en: 'We begin by quantizing both the weights and activations for different parts
    of the model, including the MLP module, the whole self-attention module, or part
    of the self-attention (query and key), using the quantization method Esser et al.
    ([2019](#bib.bib9)) in one-shot. When quantizing each part, other parts are not
    quantized. This ablation study aims to identify which components have the most
    detrimental impact on model performance due to quantization. The accuracy results
    of the LLaMA-58M model Timiryasov and Tastet ([2023](#bib.bib26)) on the Anaphor
    Agr. subclass of the BLiMP dataset Warstadt et al. ([2020](#bib.bib30)) are visualized
    in Figure [1](#S3.F1 "Figure 1 ‣ 3 Analysis ‣ EdgeQAT: Entropy and Distribution
    Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on
    the Edge"). As observed, the quantization of the self-attention module leads to
    significant accuracy loss (from 89.8% to 55.1%). Among all components in self-attention,
    the quantization of query and key is the main reason for the substantial performance
    drop with 56.6% accuracy, which is close to that of quantizing the whole self-attention.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先对模型的不同部分进行量化，包括MLP模块、整个自注意力模块或自注意力的一部分（查询和键），使用Esser等人（[2019](#bib.bib9)）的量化方法进行一次性量化。在量化每个部分时，其他部分不进行量化。这项消融研究旨在识别量化对模型性能影响最大的组件。LLaMA-58M模型Timiryasov和Tastet（[2023](#bib.bib26)）在BLiMP数据集的Anaphor
    Agr.子集上的准确率结果可视化在图[1](#S3.F1 "Figure 1 ‣ 3 Analysis ‣ EdgeQAT: Entropy and Distribution
    Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on
    the Edge")中。如观察到的，自注意力模块的量化导致了显著的准确率损失（从89.8%降至55.1%）。在自注意力的所有组件中，查询和键的量化是性能大幅下降的主要原因，准确率为56.6%，接近于量化整个自注意力模块的效果。'
- en: 'We further visualize the distributions of query and key at the last layer of
    quantized and FP16 models in Figure [2](#S3.F2 "Figure 2 ‣ 3 Analysis ‣ EdgeQAT:
    Entropy and Distribution Guided Quantization-Aware Training for the Acceleration
    of Lightweight LLMs on the Edge"). As observed, the difference of the variance
    between the quantized and FP16 counterparts is substantial for both the query
    and key, inevitably leading to the deterioration of the representation capability
    of the attention module.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步在图[2](#S3.F2 "Figure 2 ‣ 3 Analysis ‣ EdgeQAT: Entropy and Distribution
    Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on
    the Edge")中可视化了量化和FP16模型最后一层的查询和键的分布。如观察到的，量化和FP16对比下，查询和键的方差差异显著，导致注意力模块表示能力的恶化。'
- en: '![Refer to caption](img/019d09d7e97103abc73add718e7f66e0.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/019d09d7e97103abc73add718e7f66e0.png)'
- en: 'Figure 3: FP16 and quantized attention maps at the last layer of the FP16 and
    quantized models.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：FP16和量化模型最后一层的FP16和量化注意力图。
- en: 3.2 Token Importance
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 Token重要性
- en: 'We visualize the heatmap of the attention map at the last layer inside the
    FP16 and quantized models in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Quantized Self-Attention
    Module ‣ 3 Analysis ‣ EdgeQAT: Entropy and Distribution Guided Quantization-Aware
    Training for the Acceleration of Lightweight LLMs on the Edge"). In the FP16 attention
    map, there is a column pattern at the first initial token, which disappears after
    quantization. It is evident that a significant amount of attention is allocated
    to the initial token. We highlight that the initial token is vital for producing
    text that is both coherent and contextually meaningful. In LLMs, a distinct initial
    token is placed at the beginning of the input sequence, with a role in initializing
    the hidden layers and defining token positions within the sequence. It is visible
    to almost all subsequent tokens because of the nature of autoregressive language
    modeling. Removing certain interactions between the initial token and other tokens
    can fundamentally change the model generation results.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图[3](#S3.F3 "Figure 3 ‣ 3.1 Quantized Self-Attention Module ‣ 3 Analysis
    ‣ EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the
    Acceleration of Lightweight LLMs on the Edge")中可视化了FP16和量化模型最后一层的注意力图热图。在FP16注意力图中，第一个初始token处存在一个列模式，这在量化后消失。显然，大量的注意力集中在初始token上。我们强调初始token对于生成连贯且有意义的文本至关重要。在LLMs中，输入序列的开头放置了一个独特的初始token，负责初始化隐藏层并定义序列中的token位置。由于自回归语言建模的性质，它几乎对所有后续token都是可见的。移除初始token与其他token之间的某些交互可能会从根本上改变模型生成结果。'
- en: Apart from fixing the disappeared pattern with the initial token, it also raises
    the necessity to assess the importance of tokens with the initial token. The token
    importance remains valuable for further optimizations such as token pruning  Kim
    et al. ([2024](#bib.bib16)); Dong et al. ([2023](#bib.bib8)); Kong et al. ([2022](#bib.bib17));
    Shen et al. ([2023](#bib.bib24), [2022](#bib.bib25)). However, as the self-attention
    mechanism in generative models limits each token’s interaction to only those preceding
    it, the traditional token importance computation methods are unsuitable in LLMs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用初始标记修复消失的模式外，还提高了评估初始标记的标记重要性的必要性。标记重要性对于进一步优化（如标记修剪）仍然很有价值 Kim et al. ([2024](#bib.bib16));
    Dong et al. ([2023](#bib.bib8)); Kong et al. ([2022](#bib.bib17)); Shen et al.
    ([2023](#bib.bib24), [2022](#bib.bib25))。然而，由于生成模型中的自注意力机制将每个标记的交互限制在仅前面的标记，传统的标记重要性计算方法在
    LLMs 中并不适用。
- en: 4 Methodology
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法论
- en: We first introduce the quantization preliminary and then explain the primary
    methods utilized for optimization during the training process.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍量化初步内容，然后解释在训练过程中用于优化的主要方法。
- en: 4.1 Preliminary
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 初步
- en: 'For QAT, we adopt the symmetric quantization for both weights and activations
    as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 QAT，我们对权重和激活值都采用对称量化，具体如下：
- en: '|  | $\mathcal{Q}(\textbf{w})=\lfloor\text{CLIP}(\frac{\textbf{w}}{\alpha_{\textbf{w}}},-2^{b_{\textbf{w}}-1},2^{b_{\textbf{w}}-1}-1)\rceil;\,\hat{\textbf{w}}=\mathcal{Q}(\textbf{w})\cdot\alpha_{\textbf{w}},$
    |  | (1) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{Q}(\textbf{w})=\lfloor\text{CLIP}(\frac{\textbf{w}}{\alpha_{\textbf{w}}},-2^{b_{\textbf{w}}-1},2^{b_{\textbf{w}}-1}-1)\rceil;\,\hat{\textbf{w}}=\mathcal{Q}(\textbf{w})\cdot\alpha_{\textbf{w}},$
    |  | (1) |'
- en: '|  | $\mathcal{Q}(\textbf{x})=\lfloor\text{CLIP}(\frac{\textbf{x}}{\alpha_{\textbf{x}}},-2^{b_{\textbf{x}}-1},2^{b_{\textbf{x}}-1}-1)\rceil;\,\hat{\textbf{x}}=\mathcal{Q}(\textbf{x})\cdot\alpha_{\textbf{x}},$
    |  | (2) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{Q}(\textbf{x})=\lfloor\text{CLIP}(\frac{\textbf{x}}{\alpha_{\textbf{x}}},-2^{b_{\textbf{x}}-1},2^{b_{\textbf{x}}-1}-1)\rceil;\,\hat{\textbf{x}}=\mathcal{Q}(\textbf{x})\cdot\alpha_{\textbf{x}},$
    |  | (2) |'
- en: 'where x denotes the activations and w means the weights. $b_{\textbf{x}}$ represents
    rounding to the nearest integer. Thus, during the training progress, the linear
    projection can be calculated as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 x 表示激活值，w 表示权重。 $b_{\textbf{x}}$ 表示四舍五入到最近的整数。因此，在训练过程中，线性投影可以按如下方式计算：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: 'For the backward propagation, we use the Straight-Through Estimator (STE) Bengio
    et al. ([2013](#bib.bib2)) to retain the derivation of the gradients:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于反向传播，我们使用直通估计器（STE） Bengio et al. ([2013](#bib.bib2)) 以保留梯度的导数：
- en: '|  | $1$2 |  | (4) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: '|  | $1$2 |  | (5) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: '![Refer to caption](img/e2cb210c59c735617bae353231172e13.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e2cb210c59c735617bae353231172e13.png)'
- en: 'Figure 4: Overall distillation pipeline. Token adaptive QAT based on the token
    importance score (colored in red) with maximum entropy loss and attention map
    cosine similarity loss (both colored in green).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：总体蒸馏流程。基于标记重要性得分（以红色标记）的标记自适应 QAT，结合最大熵损失和注意力图余弦相似度损失（均以绿色标记）。
- en: 4.2 Entropy and Distribution Guided Optimization
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 熵与分布指导优化
- en: 'Based on the analysis in Section [3.1](#S3.SS1 "3.1 Quantized Self-Attention
    Module ‣ 3 Analysis ‣ EdgeQAT: Entropy and Distribution Guided Quantization-Aware
    Training for the Acceleration of Lightweight LLMs on the Edge"), we conclude that
    the performance loss is primarily attributed to the quantized attention module
    with deteriorated representation capability. To address this issue, we propose
    the entropy and distribution guided optimization method, which statistically maximizes
    the entropy of representations and restores the capability of the quantized self-attention
    module. According to the work Messerschmitt ([1971](#bib.bib20)), for Gaussian
    distribution, quantizers with maximum output entropy (MOE) and minimum average
    error (MAE) are approximately equivalent, up to a multiplicative constant. In
    essence, maximizing the information entropy of quantized values is equivalent
    to minimizing the error caused by quantization. As observed in Figure [2](#S3.F2
    "Figure 2 ‣ 3 Analysis ‣ EdgeQAT: Entropy and Distribution Guided Quantization-Aware
    Training for the Acceleration of Lightweight LLMs on the Edge"), the distributions
    of the query q and the key k in the self-attention modules follow the Gaussian
    distribution as below,'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '基于第[3.1节](#S3.SS1 "3.1 Quantized Self-Attention Module ‣ 3 Analysis ‣ EdgeQAT:
    Entropy and Distribution Guided Quantization-Aware Training for the Acceleration
    of Lightweight LLMs on the Edge")中的分析，我们得出结论，性能损失主要归因于量化的注意力模块，其表示能力下降。为了解决这个问题，我们提出了基于熵和分布的优化方法，该方法通过统计学最大化表示的熵，并恢复量化自注意力模块的能力。根据Messerschmitt（[1971](#bib.bib20)）的研究，对于高斯分布，具有最大输出熵（MOE）和最小平均误差（MAE）的量化器在乘法常数范围内是近似等效的。本质上，最大化量化值的信息熵等同于最小化由量化引起的误差。如图[2](#S3.F2
    "Figure 2 ‣ 3 Analysis ‣ EdgeQAT: Entropy and Distribution Guided Quantization-Aware
    Training for the Acceleration of Lightweight LLMs on the Edge")所示，自注意力模块中的查询q和键k的分布遵循如下高斯分布，'
- en: '|  | $\textbf{q}\sim\mathcal{N}(\mu_{\textbf{q}},\sigma_{\textbf{q})},\quad\textbf{k}\sim\mathcal{N}(\mu_{\textbf{k}},\sigma_{\textbf{k}}).$
    |  | (6) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{q}\sim\mathcal{N}(\mu_{\textbf{q}},\sigma_{\textbf{q})},\quad\textbf{k}\sim\mathcal{N}(\mu_{\textbf{k}},\sigma_{\textbf{k}}).$
    |  | (6) |'
- en: The entropy can be represented as follows,
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 熵可以表示如下，
- en: '|  | $\mathcal{H}(\textbf{q})=-\sum_{i}p(\textbf{q}_{i})\log{p(\textbf{q}_{i})}=\frac{1}{2}\log{2\pi
    e\sigma_{\textbf{q}}^{2}},$ |  | (7) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{H}(\textbf{q})=-\sum_{i}p(\textbf{q}_{i})\log{p(\textbf{q}_{i})}=\frac{1}{2}\log{2\pi
    e\sigma_{\textbf{q}}^{2}},$ |  | (7) |'
- en: '|  | $\mathcal{H}(\textbf{k})=-\sum_{i}p(\textbf{k}_{i})\log{p(\textbf{k}_{i})}=\frac{1}{2}\log{2\pi
    e\sigma_{\textbf{k}}^{2}}.$ |  | (8) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{H}(\textbf{k})=-\sum_{i}p(\textbf{k}_{i})\log{p(\textbf{k}_{i})}=\frac{1}{2}\log{2\pi
    e\sigma_{\textbf{k}}^{2}}.$ |  | (8) |'
- en: 'To maximize the entropy $\mathcal{H}(\textbf{q})\propto{\sigma_{\textbf{q}}^{2}}$
    to optimize the total entropy of query and key for all layers and heads. Specifically,
    we re-scale the entropy loss as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化熵$\mathcal{H}(\textbf{q})\propto{\sigma_{\textbf{q}}^{2}}$，以优化所有层和头的查询和键的总熵。具体地，我们重新缩放熵损失如下：
- en: '|  | $1$2 |  | (9) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: where $L$ denote the number of layers and heads, respectively. To prevent the
    occurrence of NaNs when scaling the loss with the log operation, we increment
    the deviation product by 1.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$L$分别表示层数和头数。为了防止在通过对数操作缩放损失时出现NaN，我们将偏差乘积增加1。
- en: 'Next, we focus on fixing the distribution pattern issue in the attention map.
    As shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Quantized Self-Attention Module
    ‣ 3 Analysis ‣ EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training
    for the Acceleration of Lightweight LLMs on the Edge"), the column distribution
    pattern with the initial tokens from the FP16 counterpart disappears after quantization
    in the quantized attention map. To minimize the difference between the quantized
    attention map and the FP16 counterpart, we introduce a distribution loss $\mathcal{L}_{D}$
    in each layer as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们关注于修正注意力图中的分布模式问题。如图[3](#S3.F3 "Figure 3 ‣ 3.1 Quantized Self-Attention
    Module ‣ 3 Analysis ‣ EdgeQAT: Entropy and Distribution Guided Quantization-Aware
    Training for the Acceleration of Lightweight LLMs on the Edge")所示，量化后的注意力图中，来自FP16对应的初始token的列分布模式在量化后消失。为了最小化量化注意力图与FP16对应之间的差异，我们在每一层引入了分布损失$\mathcal{L}_{D}$，其定义如下：'
- en: '|  | $1$2 |  | (10) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (10) |'
- en: We re-scale the loss with the logarithmic operation to match the scale of the
    original loss.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过对数操作重新缩放损失，以匹配原始损失的尺度。
- en: 4.3 Token Importance-Aware Adaptive Quantization
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 Token重要性感知自适应量化
- en: Computer specifications follow binary conventions like 8 bits, 32 bits, etc.,
    due to the binary nature of internal representation. Recently, an increasing number
    of architectures are advancing direct support for 4-bit operations, as exemplified
    by RISC-V and similar platforms. In practice, 8-bit weight quantization is widely
    adopted to keep high accuracy with fast inference. However, using the same 8 bits
    for all quantized weights or activations is less flexible and can not make use
    of the innovative features with 4-bit operations in edge computing. To retain
    a high quantization accuracy and benefit from 4-bit operations on edge devices,
    we propose the token importance-aware adaptive quantization method for the quantization
    of activations with mixed bit widths, to dynamically assign more bits (8 bits)
    for important activations and fewer bits (4 bits) for unimportant ones, equivalently
    achieving non-power-of-two quantization with lower memory usage and faster computations
    compared with only using 8 bits during training and generation processes. Our
    algorithm is strategically aligned with the forefront of innovation in mobile
    computing, specifically in response to the trend set by industry leaders like
    Snapdragon. Tailored for seamless integration with 4-bit quantization and low-bit
    inference, our framework ensures compatibility and optimization for the dynamic
    landscape of mobile edge computing.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机规格遵循二进制约定，如8位、32位等，因内部表示的二进制性质。最近，越来越多的架构在直接支持4位操作方面取得了进展，例如RISC-V及类似平台。实际上，为了保持高精度和快速推断，8位权重量化被广泛采用。然而，对于所有量化权重或激活都使用相同的8位则灵活性较差，无法充分利用4位操作在边缘计算中的创新特性。为了在边缘设备上保持高量化精度并受益于4位操作，我们提出了一种基于令牌重要性的自适应量化方法，用于混合位宽的激活量化，动态分配更多位（8位）给重要激活，分配较少位（4位）给不重要的激活，相比仅使用8位的训练和生成过程，实现了低内存使用和更快的计算。我们的算法战略性地与移动计算的前沿创新保持一致，特别是响应了如Snapdragon等行业领先者设定的趋势。专为与4位量化和低位推断无缝集成而设计，我们的框架确保了与移动边缘计算动态环境的兼容性和优化。
- en: 'With the analysis in Section [3.2](#S3.SS2 "3.2 Token Importance ‣ 3 Analysis
    ‣ EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the
    Acceleration of Lightweight LLMs on the Edge"), we identify the token importance
    based on the attentivity of each token to the first initial token. Subsequently,
    we allocate more bits (8 bits) for quantizing important tokens, while assigning
    fewer bits (4 bits) for inattentive tokens. Thus, the token adaptive quantization
    can be represented as follows,'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '根据第[3.2](#S3.SS2 "3.2 Token Importance ‣ 3 Analysis ‣ EdgeQAT: Entropy and
    Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight
    LLMs on the Edge")节的分析，我们根据每个令牌对第一个初始令牌的关注度识别令牌的重要性。随后，我们为量化重要令牌分配更多位（8位），为不关注的令牌分配较少位（4位）。因此，令牌自适应量化可以表示如下，'
- en: '|  | $1$2 |  | (11) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (11) |'
- en: where $\textbf{x}_{i}$ while the rest are considered unimportant. The quantization
    of activations can be represented as
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\textbf{x}_{i}$被视为重要，而其余则被视为不重要。激活量化可以表示为
- en: '|  | $\displaystyle\mathcal{Q}(\textbf{x}_{i})=\lfloor\text{CLIP}(\frac{\textbf{x}_{i}}{\alpha_{\textbf{
    {x}}}},-2^{\beta(\textbf{x}_{i})-1},2^{\beta(\textbf{x}_{i})-1}-1)\rceil,\forall
    i\in[1,N].$ |  | (12) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{Q}(\textbf{x}_{i})=\lfloor\text{CLIP}(\frac{\textbf{x}_{i}}{\alpha_{\textbf{
    {x}}}},-2^{\beta(\textbf{x}_{i})-1},2^{\beta(\textbf{x}_{i})-1}-1)\rceil,\forall
    i\in[1,N].$ |  | (12) |'
- en: '| Dataset | # Bits | FP16 | W8A8 | W4A8 | W4A4 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | # 位 | FP16 | W8A8 | W4A8 | W4A4 |'
- en: '| Method | / | NIPQ | PACT | LSQ | Ours | NIPQ | PACT | LSQ | Ours | NIPQ |
    PACT | LSQ | Ours |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | / | NIPQ | PACT | LSQ | 我们的方法 | NIPQ | PACT | LSQ | 我们的方法 | NIPQ | PACT
    | LSQ | 我们的方法 |'
- en: '| BLiMP | Anaphor Agr. | 89.8 | 85.5 | 86.4 | 85.4 | 88.1 | 58.1 | 86.6 | 85.4
    | 87.6 | 66.2 | 85.8 | 82.4 | 85.7 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| BLiMP | 指代一致性 | 89.8 | 85.5 | 86.4 | 85.4 | 88.1 | 58.1 | 86.6 | 85.4 | 87.6
    | 66.2 | 85.8 | 82.4 | 85.7 |'
- en: '| Arg. Structure | 73.1 | 70.9 | 70.7 | 70.5 | 72.2 | 55.5 | 70.3 | 70.9 |
    72.3 | 54.4 | 69.6 | 71.0 | 71.3 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 参数结构 | 73.1 | 70.9 | 70.7 | 70.5 | 72.2 | 55.5 | 70.3 | 70.9 | 72.3 | 54.4
    | 69.6 | 71.0 | 71.3 |'
- en: '| Binding | 72.7 | 71.1 | 71.0 | 70.8 | 72.3 | 61.7 | 70.6 | 70.9 | 72.2 |
    51.5 | 68.2 | 71.5 | 72.4 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 绑定 | 72.7 | 71.1 | 71.0 | 70.8 | 72.3 | 61.7 | 70.6 | 70.9 | 72.2 | 51.5
    | 68.2 | 71.5 | 72.4 |'
- en: '| Control/Raising | 67.5 | 65.5 | 64.6 | 66.1 | 66.7 | 54.7 | 64.0 | 65.8 |
    66.7 | 53.6 | 63.6 | 65.4 | 66.3 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 控制/提升 | 67.5 | 65.5 | 64.6 | 66.1 | 66.7 | 54.7 | 64.0 | 65.8 | 66.7 | 53.6
    | 63.6 | 65.4 | 66.3 |'
- en: '| Det.-Noun Agr. | 90.8 | 86.9 | 86.3 | 87.5 | 89.2 | 54.2 | 86.6 | 86.7 |
    89.1 | 53.4 | 84.8 | 87.1 | 87.5 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 确定-名词一致性 | 90.8 | 86.9 | 86.3 | 87.5 | 89.2 | 54.2 | 86.6 | 86.7 | 89.1 |
    53.4 | 84.8 | 87.1 | 87.5 |'
- en: '| Ellipsis | 73.3 | 60.4 | 59.7 | 63.9 | 69.4 | 29.9 | 59.7 | 62.1 | 69.8 |
    33.8 | 56.8 | 63.2 | 65.1 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 省略号 | 73.3 | 60.4 | 59.7 | 63.9 | 69.4 | 29.9 | 59.7 | 62.1 | 69.8 | 33.8
    | 56.8 | 63.2 | 65.1 |'
- en: '| Filler-Gap | 71.8 | 70.2 | 69.0 | 70.2 | 72.1 | 66.7 | 69.3 | 69.5 | 72.0
    | 61.1 | 66.8 | 70.2 | 70.4 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 填空 | 71.8 | 70.2 | 69.0 | 70.2 | 72.1 | 66.7 | 69.3 | 69.5 | 72.0 | 61.1
    | 66.8 | 70.2 | 70.4 |'
- en: '| Irregular Forms | 93.1 | 94.6 | 94.8 | 92.7 | 95.0 | 45.8 | 95.2 | 93.3 |
    94.9 | 52.2 | 93.7 | 94.1 | 94.9 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 不规则形式 | 93.1 | 94.6 | 94.8 | 92.7 | 95.0 | 45.8 | 95.2 | 93.3 | 94.9 | 52.2
    | 93.7 | 94.1 | 94.9 |'
- en: '| Island Effects | 51.2 | 48.2 | 49.2 | 48.2 | 51.7 | 43.6 | 50.0 | 48.9 |
    52.1 | 48.5 | 43.3 | 48.2 | 51.3 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 島屿效应 | 51.2 | 48.2 | 49.2 | 48.2 | 51.7 | 43.6 | 50.0 | 48.9 | 52.1 | 48.5
    | 43.3 | 48.2 | 51.3 |'
- en: '| NPI Licensing | 56.5 | 50.0 | 52.1 | 49.5 | 58.3 | 26.8 | 52.2 | 51.4 | 57.7
    | 36.6 | 48.2 | 50.9 | 44.5 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| NPI许可 | 56.5 | 50.0 | 52.1 | 49.5 | 58.3 | 26.8 | 52.2 | 51.4 | 57.7 | 36.6
    | 48.2 | 50.9 | 44.5 |'
- en: '| Quantifiers | 73.3 | 73.7 | 75.8 | 82.4 | 79.0 | 57.2 | 78.2 | 79.4 | 79.3
    | 42.7 | 78.0 | 73.4 | 80.0 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 量词 | 73.3 | 73.7 | 75.8 | 82.4 | 79.0 | 57.2 | 78.2 | 79.4 | 79.3 | 42.7
    | 78.0 | 73.4 | 80.0 |'
- en: '| Subj.-Verb Agr. | 75.4 | 68.4 | 67.8 | 68.1 | 73.2 | 46.3 | 67.7 | 67.5 |
    74.0 | 48.6 | 64.5 | 68.0 | 71.6 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 主谓一致 | 75.4 | 68.4 | 67.8 | 68.1 | 73.2 | 46.3 | 67.7 | 67.5 | 74.0 | 48.6
    | 64.5 | 68.0 | 71.6 |'
- en: '| BLiMP Suppl. | Hypernym | 49.3 | 48.0 | 49.0 | 49.6 | 48.9 | 49.5 | 48.7
    | 48.7 | 49.6 | 50.9 | 50.3 | 49.3 | 50.5 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| BLiMP补充 | 上位词 | 49.3 | 48.0 | 49.0 | 49.6 | 48.9 | 49.5 | 48.7 | 48.7 | 49.6
    | 50.9 | 50.3 | 49.3 | 50.5 |'
- en: '| QA Congruence easy | 51.6 | 48.4 | 51.5 | 46.8 | 50.1 | 35.9 | 50.0 | 46.8
    | 50.1 | 37.5 | 48.4 | 46.8 | 50.1 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| QA一致性易 | 51.6 | 48.4 | 51.5 | 46.8 | 50.1 | 35.9 | 50.0 | 46.8 | 50.1 | 37.5
    | 48.4 | 46.8 | 50.1 |'
- en: '| QA Congruence tricky | 41.8 | 40.6 | 40.0 | 40.6 | 41.3 | 34.5 | 40.6 | 40.6
    | 41.3 | 33.9 | 39.3 | 40.6 | 41.9 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| QA一致性难 | 41.8 | 40.6 | 40.0 | 40.6 | 41.3 | 34.5 | 40.6 | 40.6 | 41.3 | 33.9
    | 39.3 | 40.6 | 41.9 |'
- en: '| Subj.-Aux. Inversion | 88.5 | 89.1 | 87.9 | 87.3 | 88.5 | 67.8 | 89.8 | 83.6
    | 89.2 | 54.6 | 87.3 | 85.8 | 89.0 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 主谓辅助倒装 | 88.5 | 89.1 | 87.9 | 87.3 | 88.5 | 67.8 | 89.8 | 83.6 | 89.2 | 54.6
    | 87.3 | 85.8 | 89.0 |'
- en: '| Turn Taking | 66.1 | 58.2 | 57.1 | 58.9 | 61.5 | 43.2 | 57.5 | 59.6 | 61.8
    | 51.4 | 55.7 | 59.2 | 60.1 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 轮流发言 | 66.1 | 58.2 | 57.1 | 58.9 | 61.5 | 43.2 | 57.5 | 59.6 | 61.8 | 51.4
    | 55.7 | 59.2 | 60.1 |'
- en: '|  | Total Average | 69.7 | 66.5 | 66.6 | 67.0 | 69.3 | 48.9 | 66.9 | 66.5
    | 69.4 | 48.9 | 64.9 | 66.3 | 67.8 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | 总体平均 | 69.7 | 66.5 | 66.6 | 67.0 | 69.3 | 48.9 | 66.9 | 66.5 | 69.4 |
    48.9 | 64.9 | 66.3 | 67.8 |'
- en: 'Table 1: LLaMA-58M quantization results on the BLiMP dataset, including the
    BLiMP Supplement.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：LLaMA-58M在BLiMP数据集上的量化结果，包括BLiMP补充材料。
- en: 4.4 Adaptive Training Pipeline
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 自适应训练管道
- en: 'We visualize our training pipeline in Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Preliminary
    ‣ 4 Methodology ‣ EdgeQAT: Entropy and Distribution Guided Quantization-Aware
    Training for the Acceleration of Lightweight LLMs on the Edge"). We use the FP16
    model (colored in yellow) to distill the quantized model (colored in blue) in
    QAT. We apply soft distillation, which trains a student model to mimic a teacher
    model by minimizing the KL divergence between their softmax outputs Hinton et al.
    ([2015](#bib.bib14)). The distillation loss is defined as:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图[4](#S4.F4 "Figure 4 ‣ 4.1 Preliminary ‣ 4 Methodology ‣ EdgeQAT: Entropy
    and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight
    LLMs on the Edge")中可视化了我们的训练管道。我们使用FP16模型（用黄色标记）来蒸馏量化模型（用蓝色标记）。我们应用软蒸馏，通过最小化它们的softmax输出之间的KL散度来训练学生模型以模仿教师模型，Hinton等人（[2015](#bib.bib14)）。蒸馏损失定义为：'
- en: '|  | $\mathcal{L}_{distill}=(1-\gamma)\mathcal{L}_{CE}+\gamma\tau^{2}\mathcal{L}_{KL},$
    |  | (13) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{distill}=(1-\gamma)\mathcal{L}_{CE}+\gamma\tau^{2}\mathcal{L}_{KL},$
    |  | (13) |'
- en: where $\tau$. In the quantization modules, the tokens are adaptively quantized
    with either 8 bits or 4 bits based on their scores (colored in red) generated
    from the most recent attention map.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在量化模块中，标记根据它们从最新的注意力图生成的得分（用红色标记）被自适应量化为8位或4位。
- en: The entropy loss $\mathcal{L}_{E}$ (both colored in green) are added to the
    total loss for optimization during training as follows,
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 熵损失 $\mathcal{L}_{E}$（用绿色标记）被添加到总损失中以进行优化，如下所示。
- en: '|  | $\mathcal{L}_{total}=\mathcal{L}_{distill}+r_{E}\cdot\mathcal{L}_{E}+r_{D}\cdot\mathcal{L}_{D}.$
    |  | (14) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{total}=\mathcal{L}_{distill}+r_{E}\cdot\mathcal{L}_{E}+r_{D}\cdot\mathcal{L}_{D}.$
    |  | (14) |'
- en: The ratios $r_{E}$ to facilitate the better optimization.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 比率 $r_{E}$ 以促进更好的优化。
- en: 4.5 Hardware Implementations
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 硬件实现
- en: We deploy the quantized model obtained from our method on mobile phone, Raspberry
    Pi, and FPGA. We integrate our computational graph into llama.cpp Gerganov ([2023](#bib.bib13))
    engine on mobile and Raspberry Pi. To handle asymmetric operations, such as 4-bit
    & 8-bit multiplications, we utilize uniform 8-bit integer operators. The unused
    low-bit weights are strategically stored in byte-aligned memory units, minimizing
    bit wastage and addressing memory constraints on edge devices. Our hardware implementation
    enhances efficiency and universality for edge computing scenarios. FPGAs, with
    limited off-chip memory, can also benefit from our quantization schemes. We implement
    our inference engine with the quantization scheme based on existing LLM FPGA implementations Chen
    et al. ([2023](#bib.bib5)). We built 4-bit and 8-bit systolic array architecture
    for the Multipy-Accumulate Circuit (MAC). DSP-packing is further applied to MAC
    for better utilization of DSP resources. For non-linear operations, we implement
    floating-point-based kernels.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在手机、Raspberry Pi和FPGA上部署了从我们的方法获得的量化模型。我们将计算图集成到移动设备和Raspberry Pi上的llama.cpp
    Gerganov ([2023](#bib.bib13))引擎中。为了处理不对称操作，如4-bit和8-bit乘法，我们使用了均匀的8-bit整数运算符。未使用的低位权重被战略性地存储在字节对齐的内存单元中，最小化了位浪费，并解决了边缘设备上的内存限制。我们的硬件实现提高了边缘计算场景的效率和通用性。FPGA由于其有限的片外内存，也可以从我们的量化方案中受益。我们实现了基于现有LLM
    FPGA实现 Chen et al. ([2023](#bib.bib5))的量化方案的推理引擎。我们为乘加电路（MAC）构建了4-bit和8-bit的 systolic
    array 架构。进一步对MAC应用了DSP打包，以更好地利用DSP资源。对于非线性操作，我们实现了基于浮点数的内核。
- en: 5 Experimental Results
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验结果
- en: 5.1 Experiment Setup
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: 'For the verification of our proposed methods, as the computation resources
    are limited, we experiment with lightweight LLMs, including LLaMA-58M Touvron
    et al. ([2023](#bib.bib27)); Timiryasov and Tastet ([2023](#bib.bib26)) and GPT2-97M Radford
    et al. ([2019a](#bib.bib22)). We adopt the pretrain datasets from the work Yang
    et al. ([2023](#bib.bib34)) and then perform regex-based cleaning on them. The
    cleaned datasets are tokenized using BytePair Encoding (BPE) with a vocabulary
    size of 16000. The models are then evaluated on BLiMP Warstadt et al. ([2020](#bib.bib30))
    for the zero-shot test and (Super)GLUE Wang et al. ([2019](#bib.bib29)) for the
    fine-tuning test. In the absence of prior QAT studies for LLMs, we compare with
    well-known static quantization methods as baselines, including NIPQ Park et al.
    ([2022](#bib.bib21)), PACT Choi et al. ([2018](#bib.bib6)), and LSQ Esser et al.
    ([2019](#bib.bib9)). The same fine-tuning recipe based on the pretrain recipe
    of the work Yang et al. ([2023](#bib.bib34)) is adopted for our QAT method and
    the baselines. To make a fair comparison with the same bit width, the adaptive
    quantization is only adopted in Sec. [5.4](#S5.SS4 "5.4 Adaptive Quantization
    Results ‣ 5 Experimental Results ‣ EdgeQAT: Entropy and Distribution Guided Quantization-Aware
    Training for the Acceleration of Lightweight LLMs on the Edge") and Figure [5](#S5.F5
    "Figure 5 ‣ 5.3 Main Results of Model Performance ‣ 5 Experimental Results ‣ EdgeQAT:
    Entropy and Distribution Guided Quantization-Aware Training for the Acceleration
    of Lightweight LLMs on the Edge"). Each QAT experiment is conducted on one NVIDIA
    TITAN RTX GPU within one day.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '为验证我们提出的方法，由于计算资源有限，我们使用了轻量级的LLM，包括LLaMA-58M Touvron et al. ([2023](#bib.bib27))；Timiryasov和Tastet
    ([2023](#bib.bib26)) 以及GPT2-97M Radford et al. ([2019a](#bib.bib22))。我们采用了Yang
    et al. ([2023](#bib.bib34))工作的预训练数据集，并对其进行基于正则表达式的清理。清理后的数据集使用BytePair Encoding
    (BPE)进行标记化，词汇表大小为16000。然后，模型在BLiMP Warstadt et al. ([2020](#bib.bib30))上进行零-shot测试，并在(Super)GLUE
    Wang et al. ([2019](#bib.bib29))上进行微调测试。由于缺乏LLM的QAT研究，我们将其与众所周知的静态量化方法进行对比，包括NIPQ
    Park et al. ([2022](#bib.bib21))，PACT Choi et al. ([2018](#bib.bib6))和LSQ Esser
    et al. ([2019](#bib.bib9))。我们的QAT方法和基线方法都采用了基于Yang et al. ([2023](#bib.bib34))工作预训练配方的相同微调配方。为了在相同位宽下进行公平比较，适应性量化仅在Sec.
    [5.4](#S5.SS4 "5.4 自适应量化结果 ‣ 5 实验结果 ‣ EdgeQAT: 熵与分布指导的量化感知训练用于加速边缘轻量级LLM")和Figure
    [5](#S5.F5 "图5 ‣ 5.3 模型性能主要结果 ‣ 5 实验结果 ‣ EdgeQAT: 熵与分布指导的量化感知训练用于加速边缘轻量级LLM")中采用。每个QAT实验在一个NVIDIA
    TITAN RTX GPU上进行，时间限制为一天。'
- en: 5.2 Hardware Deployment
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 硬件部署
- en: We opt for the Oneplus 11 smartphone as our mobile platform, and this device
    is powered by the Snapdragon 8 Gen 2. All available cores have been utilized for
    multi-thread computation. Similarly, for Raspberry Pi 5 equipped with BCM2712
    quad-core Arm Cortex A76 processor, we allocate the computation among 4 cores.
    The latency has been reported via 100 iterations for each test based on llama.cpp.
    Similarly, for the FPGA evaluations, we make use of the AMD Alveo U280 FPGA from
    the Open Cloud Testbed (OCT), an open-source cloud platform for research Zink
    et al. ([2021](#bib.bib37)). We implement the proposed design using the XDMA platform
    running at 200MHz. For the testing, we preload the inputs and parameters to the
    onboard HBM and measure the performance results through 10000 iterators of the
    accelerators.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了 Oneplus 11 智能手机作为我们的移动平台，该设备由 Snapdragon 8 Gen 2 提供动力。所有可用的核心都已用于多线程计算。类似地，对于配备
    BCM2712 四核 Arm Cortex A76 处理器的 Raspberry Pi 5，我们在 4 个核心之间分配计算。延迟通过每个测试 100 次迭代进行报告，基于
    llama.cpp。类似地，对于 FPGA 评估，我们使用来自 Open Cloud Testbed (OCT) 的 AMD Alveo U280 FPGA，这是一个用于研究的开源云平台
    Zink 等人（[2021](#bib.bib37)）。我们使用运行在 200MHz 的 XDMA 平台实现所提出的设计。测试时，我们将输入和参数预加载到板载
    HBM 中，并通过 10000 次加速器迭代测量性能结果。
- en: '| Method | FP16 | NIPQ | PACT | LSQ | Ours |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | FP16 | NIPQ | PACT | LSQ | 我们的方法 |'
- en: '| Anaphor Agr. | 87.0 | 38.1 | 69.8 | 84.0 | 84.5 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 指代一致 | 87.0 | 38.1 | 69.8 | 84.0 | 84.5 |'
- en: '| Arg. Structure | 71.3 | 57.4 | 63.7 | 70.7 | 71.7 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 参数结构 | 71.3 | 57.4 | 63.7 | 70.7 | 71.7 |'
- en: '| Binding | 70.2 | 49.8 | 64.4 | 69.2 | 69.8 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 绑定 | 70.2 | 49.8 | 64.4 | 69.2 | 69.8 |'
- en: '| Control/Raising | 66.1 | 54.2 | 62.6 | 65.1 | 65.3 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 控制/提升 | 66.1 | 54.2 | 62.6 | 65.1 | 65.3 |'
- en: '| Det.-Noun Agr. | 87.4 | 51.4 | 72.3 | 86.8 | 86.0 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 冠词-名词一致 | 87.4 | 51.4 | 72.3 | 86.8 | 86.0 |'
- en: '| Ellipsis | 62.1 | 39.6 | 39.2 | 59.8 | 59.9 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 省略 | 62.1 | 39.6 | 39.2 | 59.8 | 59.9 |'
- en: '| Filler-Gap | 70.7 | 43.3 | 63.2 | 69.6 | 70.4 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 填充-缺口 | 70.7 | 43.3 | 63.2 | 69.6 | 70.4 |'
- en: '| Irregular Forms | 94.1 | 52.3 | 90.0 | 94.3 | 95.4 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 不规则形式 | 94.1 | 52.3 | 90.0 | 94.3 | 95.4 |'
- en: '| Island Effects | 47.2 | 59.7 | 44.9 | 46.6 | 46.8 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 岛屿效应 | 47.2 | 59.7 | 44.9 | 46.6 | 46.8 |'
- en: '| NPI Licensing | 48.5 | 71.3 | 44.4 | 47.3 | 44.8 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| NPI 许可 | 48.5 | 71.3 | 44.4 | 47.3 | 44.8 |'
- en: '| Quantifiers | 68.0 | 27.5 | 46.7 | 66.1 | 69.4 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 量词 | 68.0 | 27.5 | 46.7 | 66.1 | 69.4 |'
- en: '| Subj.-Verb Agr. | 66.2 | 48.1 | 55.5 | 64.8 | 66.0 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 主谓一致 | 66.2 | 48.1 | 55.5 | 64.8 | 66.0 |'
- en: '| Average | 69.9 | 49.4 | 59.7 | 68.7 | 69.2 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 69.9 | 49.4 | 59.7 | 68.7 | 69.2 |'
- en: 'Table 2: GPT2-97M Results with W4A4 Setting on the BLiMP Main Dataset.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：BLiMP 主数据集上的 GPT2-97M 结果，W4A4 设置。
- en: 5.3 Main Results of Model Performance
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 模型性能的主要结果
- en: 'We first verify the effectiveness of our proposed EdgeQAT framework on the
    BLiMP Warstadt et al. ([2020](#bib.bib30)) dataset with zero-shot (i.e., no fine-tuning)
    evaluations, and the results are shown in Table [1](#S4.T1 "Table 1 ‣ 4.3 Token
    Importance-Aware Adaptive Quantization ‣ 4 Methodology ‣ EdgeQAT: Entropy and
    Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight
    LLMs on the Edge"). We compare our method with the other three QAT works, including
    NIPQ, PACT, and LSQ, under different bit-width settings including W8A8 (meaning
    8-bit weight and 8-bit activation quantization), W4A8, and W4A4. As observed,
    our proposed EdgeQAT framework achieves better performance than all other three
    works on the average accuracy of all subdatasets in the BLiMP dataset. Our method
    achieves the best performance on most of the subdatasets across three bit-width
    configurations. Especially for the W4A8 setting, which is the most useful in practical
    applications, our method achieves an average accuracy of 69.4%, which is close
    to the performance of the FP16 model (only 0.3% drop) and even surpasses the W8A8
    setting (69.3%). For the W4A4 setting, our method maintains an average accuracy
    of 67.8%, showcasing a clear advantage over other methods. Only our method can
    achieve a competitive average accuracy close to that of the FP16 model, while
    the baselines usually suffer from substantial accuracy drops. NIPQ fails to restore
    the accuracy when the model weights are quantized to 4 bits. For PACT, it is sensitive
    to the bit width of the activations, as evidenced by the poor results under the
    W4A4 setting. The LSQ method consistently produces models with an average accuracy
    of about 66% to 67%, which is still lower than our method.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先验证了我们提出的 EdgeQAT 框架在 BLiMP Warstadt 等人 ([2020](#bib.bib30)) 数据集上的有效性，采用零样本（即不进行微调）评估，结果如表格[1](#S4.T1
    "Table 1 ‣ 4.3 Token Importance-Aware Adaptive Quantization ‣ 4 Methodology ‣
    EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration
    of Lightweight LLMs on the Edge") 所示。我们将我们的方法与其他三种 QAT 方法（包括 NIPQ、PACT 和 LSQ）在不同的位宽设置下进行比较，包括
    W8A8（即 8 位权重和 8 位激活量化）、W4A8 和 W4A4。观察结果表明，我们提出的 EdgeQAT 框架在 BLiMP 数据集上所有子数据集的平均准确率上优于其他三种方法。我们的方法在三个位宽配置中大多数子数据集上实现了最佳性能。特别是在
    W4A8 设置下，这是在实际应用中最有用的设置，我们的方法实现了 69.4% 的平均准确率，接近 FP16 模型的性能（仅下降 0.3%），甚至超过了 W8A8
    设置（69.3%）。对于 W4A4 设置，我们的方法保持了 67.8% 的平均准确率，显示出明显的优势。只有我们的方法能够实现接近 FP16 模型的竞争性平均准确率，而基线方法通常会遭遇显著的准确率下降。NIPQ
    在模型权重量化到 4 位时无法恢复准确率。对于 PACT，它对激活的位宽非常敏感，W4A4 设置下的结果很差。LSQ 方法始终产生平均准确率约为 66% 到
    67% 的模型，这仍低于我们的方法。'
- en: We also compare with the PTQ work ZeroQuant-FP Wu et al. ([2023](#bib.bib32))
    under the W4A8 setting. ZeroQuant-FP can achieve an average accuracy of 66.7%
    on BLiMP. Although it is better than the QAT works including NIPQ, PACT, and LSQ,
    our method still performs better than ZeroQuant-FP with non-marginal improvements.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在 W4A8 设置下与 PTQ 领域的 ZeroQuant-FP Wu 等人 ([2023](#bib.bib32)) 进行了比较。ZeroQuant-FP
    在 BLiMP 上的平均准确率为 66.7%。虽然它优于包括 NIPQ、PACT 和 LSQ 在内的 QAT 方法，但我们的方法仍然优于 ZeroQuant-FP，具有明显的改进。
- en: 'Additionally, we deliver the evaluation results of the GPT2-97M model with
    the W4A4 setting to verify the generalization of our method in Table [2](#S5.T2
    "Table 2 ‣ 5.2 Hardware Deployment ‣ 5 Experimental Results ‣ EdgeQAT: Entropy
    and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight
    LLMs on the Edge"). We conduct the experiments on the BLiMP main dataset. Our
    method can achieve the highest average accuracy with the best performance on most
    subdatasets, demonstrating clear advantages over QAT baselines. Among the baselines
    struggling to restore the average accuracy, the NIPQ and PACT perform much worse
    with large margins.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们提供了在 W4A4 设置下对 GPT2-97M 模型的评估结果，以验证我们方法的泛化能力，结果见表格[2](#S5.T2 "Table 2
    ‣ 5.2 Hardware Deployment ‣ 5 Experimental Results ‣ EdgeQAT: Entropy and Distribution
    Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on
    the Edge")。我们在 BLiMP 主要数据集上进行了实验。我们的方法能够在大多数子数据集上实现最高的平均准确率，表现优于大多数 QAT 基线方法，显示出明显的优势。在那些难以恢复平均准确率的基线方法中，NIPQ
    和 PACT 的表现差距较大。'
- en: '| Method | FP16 | NIPQ | PACT | LSQ | Ours |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | FP16 | NIPQ | PACT | LSQ | 我们的方法 |'
- en: '| CoLA | 69.5 | 33.3 | 69.3 | 68.6 | 68.4 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| CoLA | 69.5 | 33.3 | 69.3 | 68.6 | 68.4 |'
- en: '| SST-2 | 87.2 | 49.4 | 85.4 | 84.6 | 84.1 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 | 87.2 | 49.4 | 85.4 | 84.6 | 84.1 |'
- en: '| MRPC | 63.2 | 32.2 | 69.4 | 69.4 | 69.5 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| MRPC | 63.2 | 32.2 | 69.4 | 69.4 | 69.5 |'
- en: '| QQP | 84.3 | 42.4 | 82.5 | 83.9 | 84.1 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| QQP | 84.3 | 42.4 | 82.5 | 83.9 | 84.1 |'
- en: '| MNLI | 72.9 | 35.4 | 67.5 | 70.8 | 70.8 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| MNLI | 72.9 | 35.4 | 67.5 | 70.8 | 70.8 |'
- en: '| MNLI-mm | 73.7 | 35.8 | 69.1 | 71.5 | 71.1 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| MNLI-mm | 73.7 | 35.8 | 69.1 | 71.5 | 71.1 |'
- en: '| QNLI | 81.1 | 47.2 | 74.4 | 78.4 | 79.4 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| QNLI | 81.1 | 47.2 | 74.4 | 78.4 | 79.4 |'
- en: '| RTE | 61.6 | 50.5 | 48.5 | 57.5 | 53.5 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| RTE | 61.6 | 50.5 | 48.5 | 57.5 | 53.5 |'
- en: '| BoolQ | 67.2 | 58.4 | 60.3 | 63.7 | 62.9 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | 67.2 | 58.4 | 60.3 | 63.7 | 62.9 |'
- en: '| MultiRC | 58.9 | 53.2 | 46.1 | 46.6 | 54.1 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| MultiRC | 58.9 | 53.2 | 46.1 | 46.6 | 54.1 |'
- en: '| WSC | 61.4 | 61.4 | 53.0 | 42.1 | 56.6 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| WSC | 61.4 | 61.4 | 53.0 | 42.1 | 56.6 |'
- en: '| Average | 71.0 | 45.4 | 65.9 | 67.0 | 68.6 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 71.0 | 45.4 | 65.9 | 67.0 | 68.6 |'
- en: 'Table 3: LLaMA-58M results with W4A4 setting on the (Super)GLUE dataset.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 在(Super)GLUE数据集上使用W4A4设置的LLaMA-58M结果。'
- en: 'To demonstrate the effectiveness of the EdgeQAT framework, we further finetune
    the models from different QAT frameworks on the (Super)GLUE dataset and show the
    evaluation results in Table [3](#S5.T3 "Table 3 ‣ 5.3 Main Results of Model Performance
    ‣ 5 Experimental Results ‣ EdgeQAT: Entropy and Distribution Guided Quantization-Aware
    Training for the Acceleration of Lightweight LLMs on the Edge"). To make a fair
    comparison, we use the same finetuning recipe for all methods. As observed, similarly,
    our method achieves the best performance in average accuracy compared with QAT
    baselines. While NIPQ still does not work well with more training efforts, PACT
    and LSQ fail to restore the average accuracy.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '为了展示EdgeQAT框架的有效性，我们进一步在(Super)GLUE数据集上对来自不同QAT框架的模型进行微调，并在表 [3](#S5.T3 "表
    3 ‣ 5.3 模型性能的主要结果 ‣ 5 实验结果 ‣ EdgeQAT: 基于熵和分布的量化感知训练以加速边缘的轻量级LLMs")中展示评估结果。为了公平比较，我们对所有方法使用相同的微调配方。观察表明，我们的方法在平均准确率方面取得了最佳表现，相较于QAT基线。虽然NIPQ在更多训练努力下仍表现不佳，但PACT和LSQ未能恢复平均准确率。'
- en: '![Refer to caption](img/973d21b54014953975631203ebef1114.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/973d21b54014953975631203ebef1114.png)'
- en: 'Figure 5: Adaptive quantization results with 4-bit weights and adaptive activations
    (4-bit or 8-bit) on the BLiMP dataset. The average accuracy is reported.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 在BLiMP数据集上使用4位权重和自适应激活（4位或8位）的自适应量化结果。报告了平均准确率。'
- en: 5.4 Adaptive Quantization Results
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 自适应量化结果
- en: 'Following the binary conventions with binary operators (e.g., 4-bit or 8-bit
    multipliers) on the edge, we stick to 4-bit and 8-bit quantization settings to
    ensure compatibility with edge devices. We show the results of adaptive activation
    quantization (4 bits or 8 bits as in Equation [11](#S4.E11 "In 4.3 Token Importance-Aware
    Adaptive Quantization ‣ 4 Methodology ‣ EdgeQAT: Entropy and Distribution Guided
    Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge"))
    with 4 bits for weights colored with orange in Figure [5](#S5.F5 "Figure 5 ‣ 5.3
    Main Results of Model Performance ‣ 5 Experimental Results ‣ EdgeQAT: Entropy
    and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight
    LLMs on the Edge"). We identify the token importance based on the first initial
    token of the column distribution pattern as shown in Section [3.2](#S3.SS2 "3.2
    Token Importance ‣ 3 Analysis ‣ EdgeQAT: Entropy and Distribution Guided Quantization-Aware
    Training for the Acceleration of Lightweight LLMs on the Edge"). Important tokens
    are quantized into 8 bits while 4 bits are assigned to the remaining inattentive
    tokens. We vary the containment ratio of 8 bits (i.e., the important token ratio
    $\rho$) from 0% to 100% to show the variance of accuracy. The accuracy of the
    model improves as the proportion of 8-bit activations increases, validating the
    effectiveness of our token importance identification method. Besides, compared
    with the equivalent (in terms of bits) non-power-of-two quantization (colored
    in blue), which uses the same bit-width for all activations (e.g., the case with
    25% 8 bits and 75% 4 bits is equivalent to 5 bits for all), our adaptive method
    performs better, demonstrating the advantages with more bits for more important
    activations.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 根据边缘设备上的二进制运算符（例如，4位或8位乘法器）的二进制约定，我们坚持使用4位和8位量化设置，以确保与边缘设备的兼容性。我们展示了适应性激活量化（4位或8位，如方程[11](#S4.E11
    "在 4.3 Token Importance-Aware Adaptive Quantization ‣ 4 方法 ‣ EdgeQAT：基于熵和分布的量化感知训练用于加速轻量级LLMs的边缘计算")）的结果，其中图[5](#S5.F5
    "图 5 ‣ 5.3 模型性能的主要结果 ‣ 5 实验结果 ‣ EdgeQAT：基于熵和分布的量化感知训练用于加速轻量级LLMs的边缘计算")中橙色标记的权重使用4位。我们根据列分布模式的初始Token来识别Token的重要性，如第[3.2](#S3.SS2
    "3.2 Token Importance ‣ 3 分析 ‣ EdgeQAT：基于熵和分布的量化感知训练用于加速轻量级LLMs的边缘计算")节所示。重要的Token被量化为8位，而4位分配给剩余的不重要的Token。我们将8位的占比（即重要Token的比例$\rho$）从0%变化到100%来展示准确性的变化。随着8位激活的比例增加，模型的准确性提高，验证了我们Token重要性识别方法的有效性。此外，与等效（位数相同的）非二进制量化（蓝色标记）相比，后者对所有激活使用相同的位宽（例如，25%
    8位和75% 4位相当于所有激活使用5位），我们的自适应方法表现更好，展示了对重要激活使用更多位的优势。
- en: '| Edge | Oneplus 11 | Raspberry Pi 5 | AMD u280 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Edge | Oneplus 11 | Raspberry Pi 5 | AMD u280 |'
- en: '| # Bits | ms / Token | ms / Token | ms / Token |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| # 位 | 每个Token的毫秒数 | 每个Token的毫秒数 | 每个Token的毫秒数 |'
- en: '| LLaMA-58M |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-58M |'
- en: '| FP16 | 4.54 | 1$\times$ |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 4.54 | 1$\times$ |'
- en: '| W8A8 | 4.12 | 1.10$\times$ |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 4.12 | 1.10$\times$ |'
- en: '| W4A4 | 3.90 | 1.16$\times$ |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | 3.90 | 1.16$\times$ |'
- en: '| GPT2-97M |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| GPT2-97M |'
- en: '| FP16 | 6.22 | 1$\times$ |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 6.22 | 1$\times$ |'
- en: '| W8A8 | 5.44 | 1.14$\times$ |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 5.44 | 1.14$\times$ |'
- en: '| W4A4 | 5.10 | 1.22$\times$ |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | 5.10 | 1.22$\times$ |'
- en: 'Table 4: Latency results of LLaMA-58M and GPT2-97M on edge devices including
    the Oneplus 11 smartphone, Raspberry Pi 5, and FPGA.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：LLaMA-58M和GPT2-97M在包括Oneplus 11智能手机、Raspberry Pi 5和FPGA在内的边缘设备上的延迟结果。
- en: 5.5 Hardware Efficiency
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 硬件效率
- en: 'We show the latency results of LLaMA-58M and GPT2-97M on the Oneplus 11 smartphone,
    Raspberry Pi 5, and FPGA in Table [4](#S5.T4 "Table 4 ‣ 5.4 Adaptive Quantization
    Results ‣ 5 Experimental Results ‣ EdgeQAT: Entropy and Distribution Guided Quantization-Aware
    Training for the Acceleration of Lightweight LLMs on the Edge"). We can successfully
    achieve the acceleration of the token generation for two different models on three
    different devices. In particular, we can achieve 2.37$\times$ speedup, which benefits
    from the flexibility of implementing custom data paths for sub-8bit operations
    on FPGAs.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[4](#S5.T4 "表 4 ‣ 5.4 自适应量化结果 ‣ 5 实验结果 ‣ EdgeQAT：基于熵和分布的量化感知训练用于加速轻量级LLMs的边缘计算")中展示了LLaMA-58M和GPT2-97M在Oneplus
    11智能手机、Raspberry Pi 5和FPGA上的延迟结果。我们成功地在三种不同的设备上实现了两种不同模型的Token生成加速。特别是，我们实现了2.37$\times$的加速，这得益于在FPGA上为小于8位的操作实现自定义数据路径的灵活性。
- en: 6 Conclusion and Limitation
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论和限制
- en: In this paper, we introduce EdgeQAT, an entropy and distribution guided QAT
    framework designed to accelerate lightweight LLMs on edge devices. We incorporate
    the maximum entropy theory to optimize the quantized query and key in the self-attention
    mechanism, and mitigate the information loss of the quantized attention map with
    a distribution guided loss. Besides, we adaptively quantize tokens with different
    bit widths based on their importance, which further reduces the average bit width
    for quantization. We effectively restore the model performance to that of FP16
    counterparts and achieve up to 2.37$\times$ speedup on edge devices. However,
    we mainly experiment with lightweight LLMs due to resource constraints. We will
    verify our method on larger models if more data and computation resources are
    available.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们介绍了 EdgeQAT，这是一种基于熵和分布引导的 QAT 框架，旨在加速边缘设备上的轻量级 LLM。我们结合最大熵理论来优化自注意力机制中的量化查询和键，并通过分布引导的损失来减轻量化注意力图的
    信息丢失。此外，我们根据标记的重要性自适应地量化具有不同位宽的标记，从而进一步减少量化的平均位宽。我们有效地将模型性能恢复到 FP16 同类水平，并在边缘设备上实现了高达
    2.37$\times$ 的加速。然而，由于资源限制，我们主要在轻量级 LLM 上进行实验。如果有更多的数据和计算资源，我们将验证我们的方法在更大模型上的表现。
- en: References
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等人 (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等人. 2023. GPT-4 技术报告。*arXiv 预印本 arXiv:2303.08774*。
- en: Bengio et al. (2013) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013.
    Estimating or propagating gradients through stochastic neurons for conditional
    computation. *arXiv preprint arXiv:1308.3432*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等人 (2013) Yoshua Bengio, Nicholas Léonard, 和 Aaron Courville. 2013. 估计或传播通过随机神经元的梯度以进行条件计算。*arXiv
    预印本 arXiv:1308.3432*。
- en: Brown et al. (2020a) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020a. Language models are few-shot learners. *NeurIPS*,
    33:1877–1901.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020a) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等人. 2020a. 语言模型是少样本学习者。*NeurIPS*, 33:1877–1901。
- en: Brown et al. (2020b) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020b. [Language models are few-shot learners](http://arxiv.org/abs/2005.14165).
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020b) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei. 2020b. [语言模型是少样本学习者](http://arxiv.org/abs/2005.14165)。
- en: Chen et al. (2023) Hongzheng Chen, Jiahao Zhang, Yixiao Du, Shaojie Xiang, Zichao
    Yue, Niansong Zhang, Yaohui Cai, and Zhiru Zhang. 2023. Understanding the potential
    of fpga-based spatial acceleration for large language model inference. *arXiv
    preprint arXiv:2312.15159*.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2023) Hongzheng Chen, Jiahao Zhang, Yixiao Du, Shaojie Xiang, Zichao
    Yue, Niansong Zhang, Yaohui Cai, 和 Zhiru Zhang. 2023. 理解基于 FPGA 的空间加速在大型语言模型推理中的潜力。*arXiv
    预印本 arXiv:2312.15159*。
- en: 'Choi et al. (2018) Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce
    I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. 2018. Pact:
    Parameterized clipping activation for quantized neural networks. *arXiv preprint
    arXiv:1805.06085*.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Choi 等人 (2018) Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen
    Chuang, Vijayalakshmi Srinivasan, 和 Kailash Gopalakrishnan. 2018. Pact: 参数化剪裁激活用于量化神经网络。*arXiv
    预印本 arXiv:1805.06085*。'
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 (2022) Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    2022. Llm. int8 (): 适用于大规模变换器的 8 位矩阵乘法。*arXiv 预印本 arXiv:2208.07339*。'
- en: 'Dong et al. (2023) Peiyan Dong, Mengshu Sun, Alec Lu, Yanyue Xie, Kenneth Liu,
    Zhenglun Kong, Xin Meng, Zhengang Li, Xue Lin, Zhenman Fang, et al. 2023. Heatvit:
    Hardware-efficient adaptive token pruning for vision transformers. In *2023 IEEE
    International Symposium on High-Performance Computer Architecture (HPCA)*, pages
    442–455\. IEEE.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong 等（2023）Peiyan Dong, Mengshu Sun, Alec Lu, Yanyue Xie, Kenneth Liu, Zhenglun
    Kong, Xin Meng, Zhengang Li, Xue Lin, Zhenman Fang 等。2023. Heatvit: 硬件高效的自适应标记剪枝用于视觉变换器。在*2023
    IEEE 高性能计算架构国际研讨会（HPCA）*，页 442–455。IEEE。'
- en: Esser et al. (2019) Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar
    Appuswamy, and Dharmendra S Modha. 2019. Learned step size quantization. *arXiv
    preprint arXiv:1902.08153*.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esser 等（2019）Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar
    Appuswamy 和 Dharmendra S Modha. 2019. 学习步长量化。*arXiv 预印本 arXiv:1902.08153*。
- en: 'Frantar and Alistarh (2023) Elias Frantar and Dan Alistarh. 2023. Sparsegpt:
    Massive language models can be accurately pruned in one-shot. In *International
    Conference on Machine Learning*, pages 10323–10337\. PMLR.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 和 Alistarh（2023）Elias Frantar 和 Dan Alistarh. 2023. Sparsegpt: 大型语言模型可以在一次剪枝中准确修剪。在*国际机器学习会议*，页
    10323–10337。PMLR。'
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等（2022）Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh.
    2022. Gptq: 生成预训练变换器的准确后训练量化。*arXiv 预印本 arXiv:2210.17323*。'
- en: Fu et al. (2023) Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot.
    2023. Specializing smaller language models towards multi-step reasoning. *arXiv
    preprint arXiv:2301.12726*.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等（2023）Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal 和 Tushar Khot. 2023. 将较小的语言模型专注于多步推理。*arXiv
    预印本 arXiv:2301.12726*。
- en: 'Gerganov (2023) Georgi Gerganov. 2023. [llama.cpp: Low-latency audio streaming
    library for c++.](https://github.com/ggerganov/llama.cpp)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gerganov（2023）Georgi Gerganov. 2023. [llama.cpp: C++ 的低延迟音频流库](https://github.com/ggerganov/llama.cpp)。'
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等（2015）Geoffrey Hinton, Oriol Vinyals 和 Jeff Dean. 2015. 蒸馏神经网络中的知识。*arXiv
    预印本 arXiv:1503.02531*。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685*.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等（2021）Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang 和 Weizhu Chen. 2021. Lora: 大型语言模型的低秩适应。*arXiv 预印本 arXiv:2106.09685*。'
- en: 'Kim et al. (2024) Minchul Kim, Shangqian Gao, Yen-Chang Hsu, Yilin Shen, and
    Hongxia Jin. 2024. Token fusion: Bridging the gap between token pruning and token
    merging. In *Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision*, pages 1383–1392.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等（2024）Minchul Kim, Shangqian Gao, Yen-Chang Hsu, Yilin Shen 和 Hongxia
    Jin. 2024. Token fusion: 缩小标记剪枝与标记合并之间的差距。在*IEEE/CVF 计算机视觉冬季会议论文集*，页 1383–1392。'
- en: 'Kong et al. (2022) Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu,
    Mengshu Sun, Xuan Shen, Geng Yuan, Bin Ren, Hao Tang, et al. 2022. Spvit: Enabling
    faster vision transformers via latency-aware soft token pruning. In *European
    Conference on Computer Vision*, pages 620–640\. Springer.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kong 等（2022）Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu
    Sun, Xuan Shen, Geng Yuan, Bin Ren, Hao Tang 等。2022. Spvit: 通过延迟感知的软标记剪枝实现更快的视觉变换器。在*欧洲计算机视觉会议*，页
    620–640。Springer。'
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. 2023. Awq: Activation-aware weight quantization for llm compression
    and acceleration. *arXiv preprint arXiv:2306.00978*.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等（2023）Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang 和 Song
    Han. 2023. Awq: 具有激活感知的权重量化用于 LLM 压缩与加速。*arXiv 预印本 arXiv:2306.00978*。'
- en: 'Liu et al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    2023. Llm-qat: Data-free quantization aware training for large language models.
    *arXiv preprint arXiv:2305.17888*.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2023）Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock,
    Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi 和 Vikas Chandra. 2023.
    Llm-qat: 无数据量化感知训练用于大型语言模型。*arXiv 预印本 arXiv:2305.17888*。'
- en: Messerschmitt (1971) D. Messerschmitt. 1971. [Quantizing for maximum output
    entropy (corresp.)](https://doi.org/10.1109/TIT.1971.1054681). *IEEE Transactions
    on Information Theory*, 17(5):612–612.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Messerschmitt（1971）D. Messerschmitt. 1971. [最大输出熵的量化（通信）](https://doi.org/10.1109/TIT.1971.1054681)。*IEEE
    信息理论汇刊*，17(5):612–612。
- en: 'Park et al. (2022) Sein Park, Junhyuk So, Juncheol Shin, and Eunhyeok Park.
    2022. Nipq: Noise injection pseudo quantization for automated dnn optimization.
    *arXiv preprint arXiv:2206.00820*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park 等 (2022) Sein Park，Junhyuk So，Juncheol Shin 和 Eunhyeok Park。2022年。Nipq:
    噪声注入伪量化用于自动化 DNN 优化。*arXiv 预印本 arXiv:2206.00820*。'
- en: Radford et al. (2019a) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019a. Language models are unsupervised multitask
    learners.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2019a) Alec Radford，Jeff Wu，Rewon Child，David Luan，Dario Amodei 和
    Ilya Sutskever。2019a年。语言模型是无监督的多任务学习者。
- en: Radford et al. (2019b) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019b. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2019b) Alec Radford，Jeffrey Wu，Rewon Child，David Luan，Dario Amodei，Ilya
    Sutskever 等。2019b年。语言模型是无监督的多任务学习者。*OpenAI 博客*，1(8):9。
- en: 'Shen et al. (2023) Xuan Shen, Peiyan Dong, Lei Lu, Zhenglun Kong, Zhengang
    Li, Ming Lin, Chao Wu, and Yanzhi Wang. 2023. Agile-quant: Activation-guided quantization
    for faster inference of llms on the edge. *arXiv preprint arXiv:2312.05693*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen 等 (2023) Xuan Shen，Peiyan Dong，Lei Lu，Zhenglun Kong，Zhengang Li，Ming Lin，Chao
    Wu 和 Yanzhi Wang。2023年。Agile-quant: 针对边缘设备上更快推理的激活引导量化。*arXiv 预印本 arXiv:2312.05693*。'
- en: Shen et al. (2022) Xuan Shen, Zhenglun Kong, Minghai Qin, Peiyan Dong, Geng
    Yuan, Xin Meng, Hao Tang, Xiaolong Ma, and Yanzhi Wang. 2022. The lottery ticket
    hypothesis for vision transformers. *arXiv preprint arXiv:2211.01484*.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等 (2022) Xuan Shen，Zhenglun Kong，Minghai Qin，Peiyan Dong，Geng Yuan，Xin
    Meng，Hao Tang，Xiaolong Ma 和 Yanzhi Wang。2022年。视觉变换器的幸运彩票假说。*arXiv 预印本 arXiv:2211.01484*。
- en: 'Timiryasov and Tastet (2023) Inar Timiryasov and Jean-Loup Tastet. 2023. Baby
    llama: knowledge distillation from an ensemble of teachers trained on a small
    dataset with no performance penalty. *arXiv preprint arXiv:2308.02019*.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Timiryasov 和 Tastet (2023) Inar Timiryasov 和 Jean-Loup Tastet。2023年。Baby llama:
    从小数据集上训练的一组教师中进行知识蒸馏而不影响性能。*arXiv 预印本 arXiv:2308.02019*。'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023. Llama: Open and efficient foundation language models.
    *arXiv*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 (2023) Hugo Touvron，Thibaut Lavril，Gautier Izacard，Xavier Martinet，Marie-Anne
    Lachaux，Timothée Lacroix，Baptiste Rozière，Naman Goyal，Eric Hambro，Faisal Azhar，Aurelien
    Rodriguez，Armand Joulin，Edouard Grave 和 Guillaume Lample。2023年。Llama: 开放且高效的基础语言模型。*arXiv*。'
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *NeurIPS*, 30.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等 (2017) Ashish Vaswani，Noam Shazeer，Niki Parmar，Jakob Uszkoreit，Llion
    Jones，Aidan N Gomez，Łukasz Kaiser 和 Illia Polosukhin。2017年。注意力机制是你所需要的。*NeurIPS*，30。
- en: 'Wang et al. (2019) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet
    Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue:
    A stickier benchmark for general-purpose language understanding systems. *Advances
    in neural information processing systems*, 32.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 (2019) Alex Wang，Yada Pruksachatkun，Nikita Nangia，Amanpreet Singh，Julian
    Michael，Felix Hill，Omer Levy 和 Samuel Bowman。2019年。Superglue: 一个更具粘性的通用语言理解系统基准。*神经信息处理系统进展*，32。'
- en: 'Warstadt et al. (2020) Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey,
    Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. 2020. [BLiMP: The Benchmark of
    Linguistic Minimal Pairs for English](https://doi.org/10.1162/tacl_a_00321). *Transactions
    of the Association for Computational Linguistics*, 8:377–392.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Warstadt 等 (2020) Alex Warstadt，Alicia Parrish，Haokun Liu，Anhad Mohananey，Wei
    Peng，Sheng-Fu Wang 和 Samuel R. Bowman。2020年。[BLiMP: 英语语言学最小对比基准](https://doi.org/10.1162/tacl_a_00321)。*计算语言学协会会刊*，8:377–392。'
- en: 'Workshop et al. (2022) BigScience Workshop, Teven Le Scao, Angela Fan, Christopher
    Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha
    Luccioni, François Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Workshop 等 (2022) BigScience Workshop，Teven Le Scao，Angela Fan，Christopher
    Akiki，Ellie Pavlick，Suzana Ilić，Daniel Hesslow，Roman Castagné，Alexandra Sasha
    Luccioni，François Yvon 等。2022年。Bloom: 一个 176b-参数的开放访问多语言模型。*arXiv 预印本 arXiv:2211.05100*。'
- en: 'Wu et al. (2023) Xiaoxia Wu, Zhewei Yao, and Yuxiong He. 2023. Zeroquant-fp:
    A leap forward in llms post-training w4a8 quantization using floating-point formats.
    *arXiv preprint arXiv:2307.09782*.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等 (2023) Xiaoxia Wu，Zhewei Yao 和 Yuxiong He。2023年。Zeroquant-fp: 在 llms 后训练
    w4a8 量化中使用浮点格式的突破。*arXiv 预印本 arXiv:2307.09782*。'
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization
    for large language models. In *International Conference on Machine Learning*,
    pages 38087–38099\. PMLR.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    和 Song Han。2023年。Smoothquant: 大规模语言模型的准确且高效的后训练量化。在*国际机器学习大会*，页码38087–38099。PMLR。'
- en: Yang et al. (2023) Yahan Yang, Elior Sulem, Insup Lee, and Dan Roth. 2023. [Penn
    & BGU BabyBERTa+ for Strict-Small BabyLM Challenge](https://cogcomp.seas.upenn.edu/papers/YSLR23b.pdf).
    Technical report.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023) Yahan Yang, Elior Sulem, Insup Lee, 和 Dan Roth。2023年。[Penn
    & BGU BabyBERTa+ for Strict-Small BabyLM Challenge](https://cogcomp.seas.upenn.edu/papers/YSLR23b.pdf)。技术报告。
- en: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. 2022. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. *Advances in Neural Information Processing
    Systems*, 35:27168–27183.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, 和 Yuxiong He。2022年。Zeroquant: 高效且经济的大规模变压器后训练量化。*神经信息处理系统进展*，35:27168–27183。'
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. 2022. Opt: Open pre-trained transformer language models. *arXiv*.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin等。2022年。Opt:
    开放预训练的变压器语言模型。*arXiv*。'
- en: 'Zink et al. (2021) Michael Zink, David Irwin, Emmanuel Cecchet, Hakan Saplakoglu,
    Orran Krieger, Martin Herbordt, Michael Daitzman, Peter Desnoyers, Miriam Leeser,
    and Suranga Handagala. 2021. The open cloud testbed (oct): A platform for research
    into new cloud technologies. In *2021 IEEE 10th International Conference on Cloud
    Networking (CloudNet)*, pages 140–147\. IEEE.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zink et al. (2021) Michael Zink, David Irwin, Emmanuel Cecchet, Hakan Saplakoglu,
    Orran Krieger, Martin Herbordt, Michael Daitzman, Peter Desnoyers, Miriam Leeser,
    和 Suranga Handagala。2021年。开放云测试平台 (oct): 一个用于研究新云技术的平台。在*2021 IEEE第十届国际云网络会议 (CloudNet)*，页码140–147。IEEE。'
