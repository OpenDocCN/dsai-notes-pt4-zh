- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:53:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:06
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Training LLMs over Neurally Compressed Text
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在神经压缩文本上训练大型语言模型（LLMs）
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.03626](https://ar5iv.labs.arxiv.org/html/2404.03626)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.03626](https://ar5iv.labs.arxiv.org/html/2404.03626)
- en: Brian Lester^a  Jaehoon Lee^a  Alex Alemi^a  Jeffrey Pennington^a
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Brian Lester^a  Jaehoon Lee^a  Alex Alemi^a  Jeffrey Pennington^a
- en: Adam Roberts^a  Jascha Sohl-Dickstein^b  Noah Constant^a
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Adam Roberts^a  Jascha Sohl-Dickstein^b  Noah Constant^a
- en: ^aGoogle DeepMind  ^bAnthropic
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^aGoogle DeepMind  ^bAnthropic
- en: '{brianlester, nconstant}@google.com Work done while at Google DeepMind.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{brianlester, nconstant}@google.com 工作完成于 Google DeepMind。'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In this paper, we explore the idea of training large language models (LLMs)
    over highly compressed text. While standard subword tokenizers compress text by
    a small factor, neural text compressors can achieve much higher rates of compression.
    If it were possible to train LLMs directly over neurally compressed text, this
    would confer advantages in training and serving efficiency, as well as easier
    handling of long text spans. The main obstacle to this goal is that strong compression
    tends to produce opaque outputs that are not well-suited for learning. In particular,
    we find that text naïvely compressed via Arithmetic Coding is not readily learnable
    by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression
    technique whereby text is segmented into blocks that each compress to the same
    bit length. Using this method, we demonstrate effective learning over neurally
    compressed text that improves with scale, and outperforms byte-level baselines
    by a wide margin on perplexity and inference speed benchmarks. While our method
    delivers worse perplexity than subword tokenizers for models trained with the
    same parameter count, it has the benefit of shorter sequence lengths. Shorter
    sequence lengths require fewer autoregressive generation steps, and reduce latency.
    Finally, we provide extensive analysis of the properties that contribute to learnability,
    and offer concrete suggestions for how to further improve the performance of high-compression
    tokenizers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了在高度压缩文本上训练大型语言模型（LLMs）的想法。尽管标准的子词标记化器以小的系数压缩文本，但神经文本压缩器可以实现更高的压缩率。如果能够直接在神经压缩文本上训练LLMs，这将带来训练和服务效率上的优势，并且更容易处理长文本片段。实现这一目标的主要障碍在于强压缩往往会产生不透明的输出，这些输出不适合学习。特别是，我们发现通过算术编码天真压缩的文本不能被LLMs轻易学习。为了解决这个问题，我们提出了
    Equal-Info Windows，这是一种新颖的压缩技术，通过将文本分割成每个块压缩到相同比特长度的块。使用这种方法，我们展示了在神经压缩文本上有效的学习，随着规模的扩大而提高，并在困惑度和推理速度基准上大幅超过字节级基准。虽然我们的方法在困惑度上不如相同参数数量的子词标记化器，但它的好处是序列长度较短。较短的序列长度需要更少的自回归生成步骤，从而减少延迟。最后，我们提供了对影响可学习性的属性的广泛分析，并提供了具体建议，以进一步提高高压缩标记化器的性能。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Today’s large language models (LLMs) are almost exclusively trained over subword
    tokens. The tokenizers used to produce these tokens—often BPE [[23](#bib.bib23),
    [56](#bib.bib56)] or Unigram [[37](#bib.bib37)], as implemented by the SentencePiece
    library [[38](#bib.bib38)]—are compressors that typically achieve ~4$\times$ more
    text per token, allowing it to model longer-distance dependencies, ingest more
    pretraining data, and predict more text at inference time, all without increasing
    compute.²²2The increased cost of the input embedding and final softmax layers
    due to increased vocabulary size is negligible for all but the smallest models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的大型语言模型（LLMs）几乎完全是在子词标记上进行训练的。用于生成这些标记的标记化器——通常是BPE [[23](#bib.bib23), [56](#bib.bib56)]
    或 Unigram [[37](#bib.bib37)]，由 SentencePiece 库 [[38](#bib.bib38)] 实现——是压缩器，通常能够每个标记处理约4$\times$更多的文本，从而可以建模更长距离的依赖关系，摄取更多的预训练数据，并在推理时预测更多的文本，而不会增加计算成本。²²2由于词汇表大小的增加，输入嵌入和最终
    softmax 层的成本有所增加，但对除了最小模型外的模型影响微乎其微。
- en: Given these advantages, it raises the question, could we compress text further
    to achieve even greater gains? It is well known that autoregressive language models
    can be turned into lossless text compressors, and recent work has shown that LLMs
    can easily achieve 12$\times$ bit-level compression rate. Can we simply train
    an LLM over this neurally compressed text?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些优势，这就引出了一个问题，我们是否可以进一步压缩文本以获得更大的收益？众所周知，自回归语言模型可以被转化为无损文本压缩器，最近的研究表明，LLMs
    可以轻松实现12$\times$比特级压缩率。我们是否可以直接在这种神经压缩文本上训练LLM？
- en: '![Refer to caption](img/d4508e8e58791c3ff7e25e430b693ce2.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/d4508e8e58791c3ff7e25e430b693ce2.png)'
- en: 'Figure 1: An overview of our approach for training an LLM (M2) over neurally
    compressed text. First, M1 is trained as a standard byte-level language model—given
    a leftward context, M1 assigns a probability to each possible following byte.
    Next, corpus text is compressed into a bitstream using M1 as a compressor. Specifically,
    the probabilities that M1 assigns at each text position are fed into a compression
    algorithm like Arithmetic Coding that supports using dynamic symbol probabilities.
    Finally, this bitstream is chunked into tokens (e.g., 8-bit chunks), and M2 is
    trained as a language model over compressed text.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们用于训练LLM（M2）的神经压缩文本的概述。首先，M1作为标准的字节级语言模型进行训练——在给定左侧上下文的情况下，M1为每个可能的后续字节分配一个概率。接下来，使用M1作为压缩器将语料库文本压缩成比特流。具体而言，M1在每个文本位置分配的概率被输入到一个支持使用动态符号概率的压缩算法，如算术编码。最后，这个比特流被分块为令牌（例如，8位块），M2作为压缩文本上的语言模型进行训练。
- en: In this paper we explore various options for doing so, focusing primarily on
    the idea of using Arithmetic Coding (AC) [[73](#bib.bib73)], which is known to
    reach the near-optimal compression rate for a particular model that assigns probabilities
    to text continuations. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Training
    LLMs over Neurally Compressed Text") presents our high-level approach. First,
    a small language model “M1” is trained over raw byte sequences. Next, this frozen
    model is used to compress pretraining corpus text by applying a standard compression
    algorithm like AC. The resulting compressed bitstream is then chunked into tokens,
    which are used to train “M2”, a language model that directly reads and writes
    neural-compressed text.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了各种选项，主要集中在使用算术编码（AC）[[73](#bib.bib73)]的想法上，该算法被认为可以达到对特定模型的近似最优压缩率，该模型为文本续写分配概率。图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 基于神经压缩文本训练LLMs")展示了我们的高层次方法。首先，训练一个小的语言模型“M1”以处理原始字节序列。接下来，使用该冻结模型通过应用标准压缩算法如AC来压缩预训练语料库文本。得到的压缩比特流然后被分块为令牌，这些令牌用于训练“M2”，一个直接读取和写入神经压缩文本的语言模型。
- en: Given a perfect probabilistic model of the raw byte sequence, the compression
    step would output a fully-compressed bitstream that would be indistinguishable
    from random noise, and hence unlearnable by M2. In reality, M1 can never be perfect [[78](#bib.bib78)],
    so the M1-compressed output will still contain learnable patterns. We explore
    whether using compression powered by a relatively small M1 is able to “remove”
    the simple structure that M1 understands from the input—e.g., patterns of spelling,
    word frequency, and basic grammar—while retaining any higher-level structure that
    M1 fails to model—e.g., patterns requiring “deeper” reasoning and long range coherence.
    A larger M2 would then learn to model this higher-level structure, without needing
    to relearn the low-level structure removed by M1.⁴⁴4Intuitively, training M2 could
    be seen as analogous to fitting the residuals of M1 [[21](#bib.bib21)]. In theory,
    this process could be repeated by training an even-larger M3 model on text compressed
    by M2, and so on.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个完美的原始字节序列概率模型，压缩步骤将输出一个完全压缩的比特流，这将与随机噪声无法区分，因此M2无法学习。实际上，M1永远无法完美[[78](#bib.bib78)]，因此M1压缩的输出仍然会包含可学习的模式。我们探索了使用相对较小的M1驱动的压缩是否能够“移除”M1从输入中理解的简单结构——例如，拼写模式、词频和基本语法——同时保留M1未能建模的任何高级结构——例如，需要“更深”推理和长距离连贯性的模式。然后，较大的M2将学习建模这种高级结构，而无需重新学习M1移除的低级结构。⁴⁴4直观上，训练M2可以类比为拟合M1的残差[[21](#bib.bib21)]。理论上，这个过程可以通过训练一个更大的M3模型来重复，针对M2压缩的文本，以此类推。
- en: In practice, we find that text compressed via Arithmetic Coding is not readily
    learnable by a standard transformer-based LLM, with resulting models predicting
    tokens at chance. Interestingly, this result holds even when M1 is reduced to
    a context-free unigram model, suggesting that the challenge of modeling AC-compressed
    text stems from the difficulty of learning the AC compression and decompression
    process itself. We verify this hypothesis by showing that even the sub-tasks of
    AC-compressing and AC-decompressing text are not learned well beyond a few initial
    tokens.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中，我们发现通过算术编码压缩的文本对标准的基于变换器的语言模型（LLM）来说不容易学习，导致生成的模型预测的标记仅仅是随机的。有趣的是，即使将 M1
    简化为一个无上下文的单词模型，这一结果仍然成立，这表明对 AC 压缩文本建模的挑战在于学习 AC 压缩和解压过程本身的难度。我们通过展示即使是 AC 压缩和
    AC 解压文本的子任务在初始几个标记之后也无法很好地学习来验证这一假设。
- en: To aid learnability, we propose compression via Equal-Info Windows, a simple
    technique that breaks text into contiguous windows and compresses them via Arithmetic
    Coding independently. Rather than splitting text into windows of equal text length,
    we track the number of bits output by the compressor, and close each window just
    before it exceeds a set information threshold (e.g., 32 bits of information).
    This has the advantage that when chunking the subsequent bitstream into M2 tokens,
    there is a stable mapping from N tokens to one window (e.g., four 8-bit tokens $\Rightarrow$ one
    32-bit window). At each window boundary, we reset both AC algorithm and the M1
    model context. This ensures that each window may be mapped back onto raw text
    without any additional information.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高学习能力，我们提出了通过 Equal-Info 窗口进行压缩，这是一种简单的技术，它将文本分成连续的窗口，并通过算术编码独立压缩它们。与将文本分成等长窗口不同，我们跟踪压缩机输出的位数，并在每个窗口刚好在设定的信息阈值（例如
    32 位信息）之前关闭窗口。这具有一个优势，即在将后续的比特流分割成 M2 标记时，存在一个稳定的映射从 N 个标记到一个窗口（例如，四个 8 位标记 $\Rightarrow$ 一个
    32 位窗口）。在每个窗口边界处，我们重置 AC 算法和 M1 模型上下文。这确保了每个窗口可以被映射回原始文本，而无需任何额外的信息。
- en: Through ablations on window size and M2 vocabulary size, we find that Equal-Info
    Windows make learning of AC-compressed text possible across a range of settings.
    However, we also observe that learning progresses gradually, starting with tokens
    at the left edge of each window, and for longer windows, the model learns little
    about the tokens near the right edge. Our best-performing setting uses short 16-bit
    windows that each correspond to a single 16-bit M2 token. Despite resetting the
    compression algorithm every 16 bits, we still achieve ~5.3$\times$ token-level
    compression overall, which exceeds standard subword tokenizers. Remarkably, our
    best M2 models outperform byte-level baselines on perplexity benchmarks (bits/byte)
    for fixed computation budget (FLOPs/byte). This shows that learning over neural-compressed
    text can be effective.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对窗口大小和 M2 词汇表大小的消融实验，我们发现 Equal-Info 窗口使得在各种设置下学习 AC 压缩文本成为可能。然而，我们也观察到学习是逐渐进行的，从每个窗口的左边缘的标记开始，对于较长的窗口，模型对靠近右边缘的标记了解甚少。我们表现最好的设置使用短的
    16 位窗口，每个窗口对应一个 16 位 M2 标记。尽管每 16 位重置一次压缩算法，但我们仍然总体上实现了 ~5.3$\times$ 的标记级压缩，超过了标准的子词标记器。值得注意的是，我们表现最好的
    M2 模型在固定计算预算（FLOPs/byte）下的困惑度基准（bits/byte）上超过了字节级基线。这表明对神经压缩文本的学习可以是有效的。
- en: At the same time, our best M2 models underperform subword baselines. We suspect
    this is due at least in part to the relatively unstable mappings our neural tokenizers
    induce between words and tokens. By contrast, standard subword tokenizers induce
    essentially stable word-to-token mappings, which likely makes the token sequences
    they output well-suited for LLM training.⁵⁵5See [Appendix L](#A12 "Appendix L
    Corner Cases of Tokenization lead to Unstable Mappings ‣ Training LLMs over Neurally
    Compressed Text") for some counterexamples to subword tokenizers producing stable
    word-to-token mappings. We illustrate this contrast through qualitative examples.
    Whether a neural tokenizer can reach a high level of compression while maintaining
    high learnability for LLM training is an interesting question for future research.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，我们的最佳M2模型表现不如子词基准模型。我们怀疑这至少部分是由于我们的神经分词器在单词和标记之间引起的相对不稳定的映射。相比之下，标准的子词分词器引起的单词到标记的映射本质上是稳定的，这可能使得它们输出的标记序列非常适合用于LLM训练。⁵⁵5有关子词分词器产生稳定的单词到标记映射的一些反例，请参见[附录L](#A12
    "附录L：分词的角落案例导致不稳定的映射 ‣ 在神经压缩文本上训练LLMs")。我们通过定性例子来说明这种对比。神经分词器是否能够在保持高学习性的同时达到高压缩水平是未来研究中的一个有趣问题。
- en: 'Our main contributions are as follows: (1) Outline advantages and challenges
    of training over neural compressed text. (2) Compare LLMs trained over different
    tokenizers along two axes: bits/byte and FLOPs/byte. (3) Show that standard LLMs
    can’t learn to model vanilla AC-compressed text. (4) Show that GZip-compressed
    text is learnable by standard LLMs, but not competitive. (5) Propose compression
    via Equal-Info Windows, and show that it enables learning over neural compressed
    text.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献如下：(1) 概述在神经压缩文本上训练的优势和挑战。(2) 在两个轴上比较不同分词器训练的LLMs：bits/byte 和 FLOPs/byte。(3)
    显示标准LLMs不能学习建模普通AC压缩文本。(4) 显示标准LLMs可以学习GZip压缩文本，但不具竞争力。(5) 提出通过Equal-Info Windows进行压缩，并展示它如何在神经压缩文本上进行学习。
- en: 2 Motivation and Background
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 动机和背景
- en: 2.1 Advantages of Training over Neural-Compressed Text
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 在神经压缩文本上训练的优势
- en: Training LLMs over compressed text is appealing for many reasons. We discuss
    three advantages in detail below.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在压缩文本上训练LLMs因多种原因而具有吸引力。我们将在下面详细讨论三种优势。
- en: Efficiency
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 效率
- en: The most straightforward advantage is efficiency. By compressing the same text
    into a shorter token sequence, the model can process more text for the same computational
    cost. In particular, a model trained over $C{\mkern-1.0mu\times\mkern-1.0mu}{}$
    more text during training compared to a model trained over raw text, given an
    equal compute budget. Increasing the amount of data seen in pretraining is often
    an effective means of improving performance [[35](#bib.bib35), [30](#bib.bib30)].
    Processing text more efficiently also confers benefits at inference time, reducing
    the serving cost for handling a request of a given prompt and continuation length.
    In addition to reducing the raw compute needed for inference, compression can
    also improve inference latency, since generating better-compressed output requires
    fewer sequential autoregressive steps.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的优势是效率。通过将相同的文本压缩成更短的标记序列，模型可以在相同的计算成本下处理更多的文本。特别是，在相同的计算预算下，与使用原始文本训练的模型相比，一个在$C{\mkern-1.0mu\times\mkern-1.0mu}{}$更多文本上训练的模型能够处理更多的文本。增加在预训练中看到的数据量通常是提高性能的有效手段[[35](#bib.bib35),
    [30](#bib.bib30)]。更高效地处理文本在推理时也带来了好处，减少了处理给定提示和继续长度请求的服务成本。除了减少推理所需的原始计算外，压缩还可以提高推理延迟，因为生成更好压缩输出需要的序列自回归步骤更少。
- en: Longer Context
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更长的上下文
- en: A second advantage is that working with compressed text allows modeling longer
    contextual dependencies. In vanilla transformer-based models, computation for
    the self-attention layer scales quadratically with the sequence length, $O(n^{2}d)$
    bytes) are modest when viewed merely as perplexity gains [[51](#bib.bib51)], the
    ability to condition on long context is critical for many applications, such as
    retrieving content from a document, or answering a coding question provided documentation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个优势是处理压缩文本允许建模更长的上下文依赖。在普通的基于变换器的模型中，自注意力层的计算随着序列长度呈二次方增长，$O(n^{2}d)$ 字节）在仅仅作为困惑度增益时可能看起来很小[[51](#bib.bib51)]，但在许多应用中，例如从文档中检索内容或回答提供文档的编码问题，能够条件化长上下文至关重要。
- en: Distribution of Compute
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算分配
- en: A third potential advantage of training over compressed text is that information
    will be spread more uniformly across the sequence. By the nature of compression,
    a text span that is relatively predictable (e.g., a boilerplate notice) will be
    more compressible than a span with high perplexity (e.g., a unique product serial
    number). When an LLM is trained over well-compressed text, each token will represent
    roughly an equal amount of information. Since the LLM allocates equal compute
    to each token, this amounts to allocating *more* compute for “harder” text spans.
    This adaptivity is similar in spirit to “Adaptive Computation Time” (ACT) [[27](#bib.bib27)],
    which learns to allocate additional compute at some sequence positions in an end-to-end
    manner, but with the advantage that in our case the computation remains “dense”—identical
    operations are applied at each position.⁷⁷7It should be noted that ACT learns
    to allocate more compute where it is *useful*, as opposed to merely where the
    predictions are hard. For example, ACT learns to not waste compute on inherently
    unpredictable text spans. We expect that as a heuristic, allocating more compute
    to higher-perplexity text spans is valuable, but leave this to future work to
    verify.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 训练压缩文本的第三个潜在优势是信息会在序列中更均匀地分布。由于压缩的性质，相对可预测的文本跨度（例如，模板通知）将比具有高困惑度的跨度（例如，唯一的产品序列号）更易压缩。当
    LLM 在经过良好压缩的文本上训练时，每个令牌将表示大致相等的信息量。由于 LLM 为每个令牌分配相等的计算，这相当于为“更难”的文本跨度分配*更多*的计算。这种适应性在精神上类似于“自适应计算时间”（ACT）[[27](#bib.bib27)]，它学习在某些序列位置以端到端的方式分配额外的计算，但我们的情况具有计算保持“密集”的优势——在每个位置应用相同的操作。⁷⁷需要注意的是，ACT
    学会在*有用*的地方分配更多计算，而不仅仅是在预测困难的地方。例如，ACT 学会不在固有不可预测的文本跨度上浪费计算。我们预计，作为一种启发式方法，为高困惑度的文本跨度分配更多计算是有价值的，但这需要未来的工作来验证。
- en: 2.2 Challenges of Training over Compressed Text
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 在压缩文本上训练的挑战
- en: Learnability
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可学习性
- en: It is not at all obvious what types of compression are “transparent” enough
    to be learnable through a standard LLM training process. Strong compression can
    be seen as removing as much redundant or predictable information from a sequence
    as possible. Consequently, the bitstream output by a good compressor is inherently
    hard to distinguish from random noise. In this work, we explore the setting where
    M2—the model trained over compressed text—has a larger capacity than M1, the model
    used for compression. In principle, this setup should allow M2 to extract additional
    information from the signal even after M1 has compressed it. However, for strong
    enough M1 compression, the resulting bitstream may be too noisy to detect any
    signal.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 并不明显哪些类型的压缩足够“透明”，可以通过标准的 LLM 训练过程进行学习。强压缩可以视为从序列中去除尽可能多的冗余或可预测的信息。因此，由优秀压缩器输出的比特流本质上很难与随机噪声区分开来。在这项工作中，我们探讨了
    M2——一个在压缩文本上训练的模型——的容量大于 M1，即用于压缩的模型。原则上，这种设置应允许 M2 在 M1 已经压缩了信号后提取额外的信息。然而，对于足够强的
    M1 压缩，结果比特流可能噪声太大，无法检测到任何信号。
- en: As a prerequisite for M2 to effectively predict continuations of compressed
    text, we anticipate that it is necessary for M2 to have the ability to decompress
    bits $\rightarrow$ bits. These sub-tasks are challenging in their own right. First,
    M2 needs to accurately “simulate” M1 in order to know the probabilities it assigns
    to the text, which determine the output of compression.⁸⁸8For Arithmetic Coding,
    not only would M2 need to know the probabilities M1 assigns to the observed text,
    but it would also need to know the probabilities assigned to many *unobserved*
    symbols. This is because Arithmetic Coding operates over *cumulative* probabilities,
    i.e., the probability that the next symbol is e or any alphabetically preceding
    symbol. Training models to mimic other models can be difficult [[41](#bib.bib41)],
    and even in settings where models do learn to copy the behavior of another network
    [[29](#bib.bib29)], this is often only when looking at which symbol was assigned
    the highest probability—the actual probabilities assigned often differ [[60](#bib.bib60)].
    Second, M2 needs to learn the compression procedure itself. In our case, this
    means tracking the Arithmetic Coding algorithm, which requires maintaining high-precision
    numerical state across long contexts. We investigate these sub-tasks in detail
    in [Section 5.2](#S5.SS2 "5.2 Transformers struggle to learn Arithmetic Coding
    ‣ 5 Additional Experiments ‣ Training LLMs over Neurally Compressed Text").
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 M2 有效预测压缩文本延续的前提，我们预计 M2 需要具备将比特解压缩为比特的能力。这些子任务本身就具有挑战性。首先，M2 需要准确“模拟” M1
    以了解它对文本分配的概率，这决定了压缩的输出。⁸⁸8对于算术编码，M2 不仅需要知道 M1 分配给观察到的文本的概率，还需要知道分配给许多*未观察到*符号的概率。这是因为算术编码是基于
    *累积* 概率进行的，即下一个符号是 e 或任何按字母顺序排列的前一个符号的概率。训练模型以模拟其他模型可能很困难 [[41](#bib.bib41)]，即使在模型确实学会模仿另一网络的行为
    [[29](#bib.bib29)] 的情况下，这通常也仅仅是在查看哪个符号被分配了最高概率时——实际分配的概率往往不同 [[60](#bib.bib60)]。其次，M2
    需要学习压缩过程本身。在我们的情况下，这意味着跟踪算术编码算法，这要求在长上下文中保持高精度的数值状态。我们在[第 5.2 节](#S5.SS2 "5.2
    Transformers struggle to learn Arithmetic Coding ‣ 5 Additional Experiments ‣
    Training LLMs over Neurally Compressed Text")中详细调查了这些子任务。
- en: A further learnability challenge is the high level of context sensitivity needed
    to interpret a bitstream of compressed text. When chunked into tokens, a particular
    bit subsequence (e.g., 10111001) can map onto the same token despite having no
    stable “meaning” across occurrences. We show examples in [Section 6.1](#S6.SS1
    "6.1 EqualInfoAC is less stable and less semantic than SentencePiece ‣ 6 Analysis
    ‣ Training LLMs over Neurally Compressed Text"), where a token maps to many different
    underlying text forms, necessitating strong contextual understanding. While LLMs
    are robust to some level of polysemy, as highlighted by the success of Hash Embeddings
    [[62](#bib.bib62)] where multiple unrelated words share a single token representation,
    we suspect this has its limits.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的可学习性挑战是解释压缩文本的比特流所需的高度上下文敏感性。当分割成标记时，特定的比特子序列（例如，10111001）可以映射到相同的标记，尽管它在出现中没有稳定的“意义”。我们在[第
    6.1 节](#S6.SS1 "6.1 EqualInfoAC is less stable and less semantic than SentencePiece
    ‣ 6 Analysis ‣ Training LLMs over Neurally Compressed Text")中展示了示例，其中一个标记映射到许多不同的基础文本形式，需要强大的上下文理解。尽管
    LLM 对某些程度的多义性有较强的鲁棒性，正如 Hash Embeddings [[62](#bib.bib62)] 的成功所强调的，其中多个不相关的词共享一个标记表示，但我们怀疑这有其极限。
- en: Numerical Stability
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数值稳定性
- en: An additional technical challenge is that compression methods can be sensitive
    to the precise model probabilities used. To achieve lossless compression in our
    setup, it is critical that the M1 probabilities match during compression and decompression.
    This can be hard to guarantee in practice, as there are many sources of numerical
    noise in LLM inference, especially when running on parallel hardware. An expanded
    discussion of numerical stability issues can be found in [Section 3.7](#S3.SS7
    "3.7 Numerical Stability ‣ 3 Methods ‣ Training LLMs over Neurally Compressed
    Text").
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个额外的技术挑战是压缩方法可能对所使用的精确模型概率敏感。在我们的设置中，实现无损压缩的关键是 M1 概率在压缩和解压缩过程中必须匹配。这在实际中很难保证，因为
    LLM 推理中有许多数值噪声的来源，尤其是在并行硬件上运行时。有关数值稳定性问题的扩展讨论可以在[第 3.7 节](#S3.SS7 "3.7 Numerical
    Stability ‣ 3 Methods ‣ Training LLMs over Neurally Compressed Text")找到。
- en: Multi-Model Inference
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多模型推理
- en: Finally, a specific challenge of training over neural compressed text is that
    multiple models need to be stored and run side-by-side in order to perform inference.
    We assume that if M1 is relatively small, this additional overhead is not a significant
    drawback compared to a standard tokenizer, which is also a separate model that
    is needed to tokenize text input and detokenize LLM outputs. In evaluating our
    approach, we include M1 compute in our calculations of total inference cost (FLOPs/byte).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，训练神经压缩文本的一个具体挑战是需要同时存储和运行多个模型以进行推理。我们假设如果 M1 相对较小，这种额外的开销与标准分词器相比不会是一个显著的缺点，因为标准分词器也是一个单独的模型，需要对文本输入进行分词和对
    LLM 输出进行还原。在评估我们的方法时，我们将 M1 的计算纳入总推理成本（FLOPs/byte）的计算中。
- en: 2.3 Compression
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 压缩
- en: In this work, we focus on lossless compression, which aims to encode a sequence
    of input symbols, $x_{0:N}=\{x_{0},x_{1},\dots,x_{N}\}\in X^{|V|}$.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们专注于无损压缩，旨在对输入符号序列进行编码，$x_{0:N}=\{x_{0},x_{1},\dots,x_{N}\}\in X^{|V|}$。
- en: The “coding” component of a compression algorithm converts the input sequence
    to a bitstream of length $\ell(x_{0:N})$.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩算法的“编码”组件将输入序列转换为长度为$\ell(x_{0:N})$的比特流。
- en: 2.4 Arithmetic Coding
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 算术编码
- en: 'Arithmetic Coding [[53](#bib.bib53), [49](#bib.bib49)] uses a model $\hat{p}$.
    Thus:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 算术编码[[53](#bib.bib53), [49](#bib.bib49)]使用了一个模型 $\hat{p}$。因此：
- en: '|  | $1$2 |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $w\in X$ is used as the compressed representation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $w\in X$ 被用作压缩表示。
- en: Equivalently, the binary expansion can be seen as maintaining a bitstream prefix
    $b$.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 等效地，二进制扩展可以看作是维护一个比特流前缀 $b$。
- en: '|  | $\displaystyle B_{j}(b,0)$ |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B_{j}(b,0)$ |  |'
- en: '|  | $\displaystyle B_{j}(b,1)$ |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B_{j}(b,1)$ |  |'
- en: Once the final interval $I_{N}$ is the final compressed representation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦最终区间 $I_{N}$ 就是最终的压缩表示。
- en: 'The coding component of Arithmetic Coding is nearly optimal: the output bitstream
    will have a length of $-\lceil\log\hat{p}(x_{0:N})\rceil+1$ being assigned to
    all tokens.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 算术编码的编码组件几乎是最优的：输出的比特流将具有长度为$-\lceil\log\hat{p}(x_{0:N})\rceil+1$的所有令牌。
- en: 2.5 Related Work
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 相关工作
- en: Recent work has looked at using large language models for compression, but has
    not to our knowledge attempted to train subsequent models over the resulting compressed
    output. Works like [[16](#bib.bib16)] use a transformer language model as the
    modeling component of Arithmetic Coding, but they do not train over compressed
    output nor do they make modifications to the compression algorithm to facilitate
    learnability by downstream models. Additionally, they focus on the setting of
    compressing fixed-size sequences of bytes. By contrast, our models operate over
    input sequences of fixed *token* length. This allows for models with higher compression
    rates to leverage longer contexts, as more bytes are included in the input.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究已经探讨了使用大型语言模型进行压缩，但据我们所知，还没有尝试在生成的压缩输出上训练后续模型。像[[16](#bib.bib16)]这样的工作使用了变压器语言模型作为算术编码的建模组件，但他们既没有在压缩输出上进行训练，也没有对压缩算法进行修改以促进下游模型的可学习性。此外，他们专注于压缩固定大小字节序列的设置。相比之下，我们的模型操作的是固定*token*长度的输入序列。这允许具有更高压缩率的模型利用更长的上下文，因为输入中包含了更多的字节。
- en: '[[63](#bib.bib63)] proposes changes to Arithmetic Coding to make it more amenable
    to use with LLMs—namely, they rank sort the logits from the model before creating
    text intervals, $I_{i}(x_{0:N})$. This could help alleviate issues stemming from
    errors in M2’s simulation of M1\. However, they do not train models on top of
    their compressed output.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[[63](#bib.bib63)] 提出了对算术编码的修改，以使其更适合与 LLMs 一起使用——即，他们在创建文本区间 $I_{i}(x_{0:N})$
    之前对模型的 logits 进行排序。这可以帮助缓解 M2 在模拟 M1 时出现的错误问题。然而，他们并没有在其压缩输出上训练模型。'
- en: Some approaches to “token-free” (i.e., purely character- or byte-level) language
    modeling down-sample the input sequence via convolutions [[13](#bib.bib13), [61](#bib.bib61)],
    which could be seen as a form of end-to-end neural tokenization. However one important
    distinction is that the resulting tokenization is “soft”—outputting high-dimensional
    vectors and not implying a discrete segmentation—in contrast to our tokenization
    that outputs discrete tokens.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一些“无token”（即纯字符级或字节级）语言建模方法通过卷积[[13](#bib.bib13), [61](#bib.bib61)]对输入序列进行下采样，这可以视为一种端到端的神经分词。然而，一个重要的区别是，结果的分词是“软”的——输出高维向量而不是离散分割——与我们输出离散令牌的分词方法不同。
- en: Methods for learning *discrete* tokenization end-to-end have also been proposed
    [[11](#bib.bib11), [25](#bib.bib25)]. In the case of MANTa [[25](#bib.bib25)],
    the learned segmentation appears to be fairly semantic (i.e., respecting word
    and morpheme boundaries), which could be an advantage over our approach. However,
    they lack our bias towards encoding an equal amount of information per token.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 也提出了端到端学习*离散*分词的方法 [[11](#bib.bib11), [25](#bib.bib25)]。以 MANTa [[25](#bib.bib25)]
    为例，学习到的分割似乎相当语义化（即，尊重词和语素边界），这可能是相对于我们方法的一个优势。然而，他们缺乏我们对每个标记编码等量信息的偏向。
- en: In modeling audio, it is common practice to use learned tokenizers that compress
    the raw input signal to discrete tokens from a fixed-size codebook [[64](#bib.bib64),
    [3](#bib.bib3), [12](#bib.bib12), [6](#bib.bib6)]. However, this compression is
    lossy, whereas we focus on lossless compression.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在音频建模中，常见的做法是使用学习到的分词器，将原始输入信号压缩为来自固定大小代码本的离散标记 [[64](#bib.bib64), [3](#bib.bib3),
    [12](#bib.bib12), [6](#bib.bib6)]。然而，这种压缩是有损的，而我们关注的是无损压缩。
- en: Other recent work focuses on using the “modeling” component from well-known
    compressors to do other tasks. [[34](#bib.bib34)] uses the model from GZip to
    perform text classification. [[68](#bib.bib68)] uses the Arithmetic Decoding algorithm
    with an LLM as the model to do diverse parallel sampling from that LLM. One could
    imagine that the “model” of our compressors (M1) is a teacher for M2, but unlike
    these other applications, the M1 values are not used outside of compression.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其他最近的研究关注于使用著名压缩算法中的“建模”组件来执行其他任务。[[34](#bib.bib34)] 使用 GZip 中的模型来进行文本分类。[[68](#bib.bib68)]
    使用算术解码算法，并将 LLM 作为模型来从该 LLM 中进行多样的并行采样。可以想象，我们压缩算法的“模型”（M1）是 M2 的老师，但与这些其他应用不同，M1
    的值在压缩之外并未使用。
- en: '[[40](#bib.bib40)] also explores learning over compressed text, but with several
    key differences. First, they use n-gram language models [[57](#bib.bib57)] while
    we use LLMs. Second, their model is conditioned on compressed bitstreams but produces
    a distribution over the raw, uncompressed, bytes while our M2 models predict directly
    in the compressed space. Additionally, they only consider static Huffman coding
    [[32](#bib.bib32)] as the algorithm to compress model inputs. While this avoids
    the context sensitivity issues we outline in [Section 2.2](#S2.SS2 "2.2 Challenges
    of Training over Compressed Text ‣ 2 Motivation and Background ‣ Training LLMs
    over Neurally Compressed Text"), it results in a far worse compression rate compared
    to the adaptive compression methods we use. One important distinction is that
    their equal-information windows are overlapping, and used as a sliding window
    to provide context to their n-gram language model. By contrast our equal-information
    windows are non-overlapping, and used to segment text into a series of equal-length
    bitstrings that can be interpreted independently by M2, and whose boundaries are
    easily identifiable, as they map to a fixed number of M2 tokens.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[[40](#bib.bib40)] 也探索了对压缩文本进行学习，但有几个关键差异。首先，他们使用的是 n-gram 语言模型 [[57](#bib.bib57)]，而我们使用的是
    LLM。其次，他们的模型是基于压缩比特流进行条件化的，但产生的是原始未压缩字节的分布，而我们的 M2 模型直接在压缩空间中进行预测。此外，他们仅考虑静态霍夫曼编码
    [[32](#bib.bib32)] 作为压缩模型输入的算法。虽然这避免了我们在[第2.2节](#S2.SS2 "2.2 Challenges of Training
    over Compressed Text ‣ 2 Motivation and Background ‣ Training LLMs over Neurally
    Compressed Text")中列出的上下文敏感性问题，但与我们使用的自适应压缩方法相比，其压缩率远差于我们的方法。一个重要的区别是，他们的等信息窗口是重叠的，并用作滑动窗口来为其
    n-gram 语言模型提供上下文。相比之下，我们的等信息窗口是不重叠的，用于将文本分割成一系列可以由 M2 独立解释的等长比特串，并且其边界易于识别，因为它们映射到固定数量的
    M2 标记。'
- en: Concurrently, [[26](#bib.bib26)] explores how the compression performance of
    a tokenizer correlates with downstream model performance. They find that tokenizers
    that compress better perform better, which generally aligns with our findings,
    particularly in the large vocabulary setting, see [Fig. 6](#S4.F6 "In Short windows
    are the best ‣ 4 Results ‣ Training LLMs over Neurally Compressed Text"). However,
    we find that using the strongest compressors is detrimental to learnability, as
    seen in the AC line in [Fig. 3](#S4.F3 "In 4 Results ‣ Training LLMs over Neurally
    Compressed Text"). These conflicting results likely stem from differences in tokenization
    strategy. Their work is restricted to BPE-based compressors while we explore stronger
    compressors built on LLMs and Arithmetic Coding. The qualitative differences between
    these classes of tokenizers are explored more in [Section 6.1](#S6.SS1 "6.1 EqualInfoAC
    is less stable and less semantic than SentencePiece ‣ 6 Analysis ‣ Training LLMs
    over Neurally Compressed Text").
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，[[26](#bib.bib26)] 探索了标记器的压缩性能如何与下游模型性能相关。他们发现，压缩效果更好的标记器表现更好，这通常与我们的发现一致，特别是在大词汇设置下，参见
    [图6](#S4.F6 "简而言之，窗口最佳 ‣ 4 结果 ‣ 训练 LLMs 在神经压缩文本上")。然而，我们发现使用最强的压缩器对可学习性有害，如 [图3](#S4.F3
    "在 4 结果 ‣ 训练 LLMs 在神经压缩文本上") 中的 AC 线所示。这些矛盾的结果可能源于分词策略的差异。他们的工作仅限于基于 BPE 的压缩器，而我们则探索了基于
    LLM 和算术编码的更强压缩器。这些类别标记器之间的定性差异在 [第6.1节](#S6.SS1 "6.1 EqualInfoAC 比 SentencePiece
    更不稳定且语义更差 ‣ 6 分析 ‣ 训练 LLMs 在神经压缩文本上") 中进行了更多探讨。
- en: 3 Methods
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: For each experiment, we compress long contiguous sequences of training data
    using different methods. For several, we use M1—a byte-level language model—as
    $\hat{p}$ in the compression algorithm. We then chunk the compressed output into
    tokens and train M2 models over those tokens.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个实验，我们使用不同的方法压缩长连贯的训练数据序列。对于一些实验，我们使用 M1——一个字节级语言模型——作为压缩算法中的 $\hat{p}$。然后，我们将压缩输出拆分成标记，并在这些标记上训练
    M2 模型。
- en: 3.1 Training Data
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 训练数据
- en: All training data used is English web text from C4 (en 3.1.0) [[52](#bib.bib52)].
    After tokenization, each document in C4 has an  token appended to it. We
    concatenate $128$ bytes. Despite the document breaks, we consider these long sequences
    “continguous” for the training of language models. These sequences are then split
    into individual examples, which are shuffled using the deterministic dataset functionality
    from SeqIO [[54](#bib.bib54)].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所有使用的训练数据是来自 C4 (en 3.1.0) 的英文网页文本 [[52](#bib.bib52)]。在分词后，每个 C4 文档都会附加一个 
    标记。我们将 $128$ 字节拼接在一起。尽管存在文档断裂，我们仍将这些长序列视为训练语言模型的“连贯”序列。这些序列随后被拆分成单独的示例，并使用 SeqIO
    的确定性数据集功能进行洗牌 [[54](#bib.bib54)]。
- en: 3.2 Training M1
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 训练 M1
- en: The model used for compression is a decoder-only Transformer model [[67](#bib.bib67)].
    It uses the $3$ bits/byte, a standard measure of perplexity, see [Section 3.8](#S3.SS8
    "3.8 Evaluation ‣ 3 Methods ‣ Training LLMs over Neurally Compressed Text"). M1
    and M2 are both trained on the C4 training data, but the final validation data
    used to evaluate M2 is unseen during M1 training, therefore there is no information
    leakage. This is similar to how LLM tokenizers are often trained on same dataset
    that the LLM is subsequently trained on.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 用于压缩的模型是一个仅解码器的 Transformer 模型 [[67](#bib.bib67)]。它使用 $3$ bits/byte，这是一个标准的困惑度度量，参见
    [第3.8节](#S3.SS8 "3.8 评估 ‣ 3 方法 ‣ 训练 LLMs 在神经压缩文本上")。M1 和 M2 都是在 C4 训练数据上训练的，但用来评估
    M2 的最终验证数据在 M1 训练期间未曾见过，因此不存在信息泄漏。这类似于 LLM 标记器通常在与 LLM 后续训练相同的数据集上进行训练。
- en: 3.3 Compression Methods
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 压缩方法
- en: When compressing C4 training data, we use an example length of $10{,}240$ steps
    without repeating data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在压缩 C4 训练数据时，我们使用 $10{,}240$ 步的示例长度而不重复数据。
- en: 'Arithmetic Coding: In this setting, we use a decoder-only transformer language
    model to model $\hat{p}$, are calculated using the probabilities for the next
    token output by the transformer.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 算术编码：在这种设置下，我们使用一个仅解码器的 Transformer 语言模型来建模 $\hat{p}$，这些值是使用 Transformer 输出的下一个标记的概率计算得出的。
- en: The compressor model is run over contiguous text sequences of $10{,}240$. This
    suggests the performance drop from long sequences has minimal effect on compression,
    or that the increased contextual information makes up this difference.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩模型在 $10{,}240$ 的连贯文本序列上运行。这表明长序列的性能下降对压缩的影响很小，或者说增加的上下文信息弥补了这个差异。
- en: We will see that text compressed in this straightforward manner is not readily
    learnable by M2\. Thus, we explore alternative compression methods that modify
    the “modeling” and “coding” components for better learnability. [Table 2](#S3.T2
    "In 3.4 Tokenization of Compressed Text ‣ 3 Methods ‣ Training LLMs over Neurally
    Compressed Text") shows how our different approaches affect the compression ratio.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到，采用这种简单直接的方式压缩的文本，M2\ 并不能很容易地学习。因此，我们探索了修改“建模”和“编码”组件的替代压缩方法，以提高可学习性。[表
    2](#S3.T2 "在 3.4 压缩文本的标记化 ‣ 3 种方法 ‣ 训练 LLMs 在神经压缩文本上")展示了我们不同方法对压缩比的影响。
- en: 'Static Logits Arithmetic Coding: One potential difficulty of learning over
    compressed text is that the “modeling” component of the compression algorithm
    is hard to learn—that is, the second language model (M2) has trouble learning
    to simulate the probabilities the compressor model (M1) assigns to bytes.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 静态 logits 算术编码：对压缩文本进行学习的一个潜在难点是，压缩算法的“建模”组件难以学习，也就是说，第二语言模型 (M2) 很难学习模拟压缩模型
    (M1) 分配给字节的概率。
- en: To weaken the compressor model, we replace the context-sensitive LM model with
    a static byte unigram model—that is, the model’s distribution is the same for
    all byte tokens in the input, i.e., $\hat{p}(x_{i}|x_{0},\dots,x_{i-1})=\hat{p}(x_{i})$.
    This distribution is estimated using the byte unigram statistics from the C4 training
    data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了削弱压缩模型，我们用静态字节 unigram 模型替换了上下文敏感的 LM 模型，即模型的分布对于输入中的所有字节标记都是相同的，即 $\hat{p}(x_{i}|x_{0},\dots,x_{i-1})=\hat{p}(x_{i})$。这个分布是通过
    C4 训练数据中的字节 unigram 统计数据来估计的。
- en: 'Equal Information Windows: The difficulty in modeling compressed text could
    also be because the “coding” component of the compression algorithm is hard to
    learn. That is, the language model is not able to track the state variables used
    in Arithmetic Coding.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 等信息窗口：建模压缩文本的困难也可能是因为压缩算法的“编码”组件难以学习。也就是说，语言模型无法跟踪算术编码中使用的状态变量。
- en: '![Refer to caption](img/8fe25c45191027ed56647cde49433169.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8fe25c45191027ed56647cde49433169.png)'
- en: 'Figure 2: Under “Equal-Info Windows”, text is encoded into a series of N-bit
    windows. To determine each successive window, the remaining text is encoded byte-by-byte
    via Arithmetic Coding until no more bytes can be added without exceeding the target
    bit threshold, here $16$ bits. Both M1 and the AC algorithm are reset at each
    step, so no information persists across windows.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在“等信息窗口”下，文本被编码成一系列 N 位窗口。为了确定每个连续的窗口，剩余的文本通过算术编码逐字节编码，直到无法再添加字节而不超过目标位阈值，此处为
    $16$ 位。每一步都重置 M1 和 AC 算法，因此窗口之间没有信息保留。
- en: Our proposed method of weakening the coding component of Arithmetic Coding compression
    is to reset the AC encoder once it has output a set number of bits, creating windows
    of fixed size where each window is an independently AC-compressed sequence. This
    process is illustrated in [Fig. 2](#S3.F2 "In 3.3 Compression Methods ‣ 3 Methods
    ‣ Training LLMs over Neurally Compressed Text"). Windows will represent a variable
    amount of text, but as each window is created via compression, we expect roughly
    the same amount of information per window.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的削弱算术编码压缩的编码组件的方法是，在 AC 编码器输出一定数量的位后重置它，创建固定大小的窗口，每个窗口都是独立的 AC 压缩序列。这个过程在
    [图 2](#S3.F2 "在 3.3 压缩方法 ‣ 3 种方法 ‣ 训练 LLMs 在神经压缩文本上") 中有所说明。窗口将代表可变量的文本，但由于每个窗口是通过压缩创建的，我们预计每个窗口大致包含相同数量的信息。
- en: In addition to resetting the AC encoder, we also reset the M1 model’s context.
    This means that each $W$ bits of output can be decoded independently, at the cost
    of a weaker M1 model due to the lack of context. As each window is fully self-contained,
    the model no longer has to learn to track Arithmetic Coding state variables over
    long distances.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 除了重置 AC 编码器外，我们还重置了 M1 模型的上下文。这意味着每 $W$ 位输出可以独立解码，但由于缺乏上下文，M1 模型的效果会较弱。由于每个窗口都是完全自包含的，模型不再需要学习长期跟踪算术编码的状态变量。
- en: In cases where “spare bits” are available at the end of a window (but not enough
    to add an additional symbol of text), we pad with zeros. This complicates the
    decoding algorithm, but the compression scheme remains lossless. See [Appendix I](#A9
    "Appendix I M2 Can Handle Padding Zeros at the End of a Window ‣ Training LLMs
    over Neurally Compressed Text") for further discussion and an alternative padding
    approach that gives similar results.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在窗口末尾有“剩余位”时（但不足以添加一个额外的文本符号），我们用零填充。这会使解码算法复杂化，但压缩方案仍保持无损。有关进一步讨论和另一种填充方法的相似结果，请参见
    [附录 I](#A9 "附录 I M2 可以处理窗口末尾的填充零 ‣ 训练 LLM 时使用神经压缩文本")。
- en: When compressing an additional character would result in a bitstream that is
    greater than $W$.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当压缩一个额外字符会导致比特流大于 $W$ 时。
- en: We use $b$).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 $b$）。
- en: 'GZip: As a baseline, we also explore training over text compressed using GZip
    [[17](#bib.bib17)] as implemented in the Python [[65](#bib.bib65)] zlib library
    using the default compression level. GZip uses the DEFLATE algorithm—a combination
    of Huffman Trees [[32](#bib.bib32)] and LZ77 [[77](#bib.bib77)]. First LZ77 is
    used to replace repeated substrings in the text with pointers back to the original
    substring. Then a Huffman Tree is built for the current—LZ77 compressed—example
    and used to compress it. Note that this setting is dynamic, as the Huffman tree,
    and hence the binary codes for each character, are unique to the example. These
    experiments explore a setting where both the modeling and coding components of
    compression are different from Arithmetic Coding.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: GZip：作为基准，我们还探索了对使用 GZip [[17](#bib.bib17)] 压缩的文本进行训练，GZip 是在 Python [[65](#bib.bib65)]
    的 zlib 库中实现的，使用默认的压缩级别。GZip 使用 DEFLATE 算法——这是 Huffman 树 [[32](#bib.bib32)] 和 LZ77
    [[77](#bib.bib77)] 的组合。首先，LZ77 用于将文本中的重复子串替换为指向原始子串的指针。然后，为当前的 LZ77 压缩示例构建一个 Huffman
    树，并用于压缩它。注意，这种设置是动态的，因为 Huffman 树以及每个字符的二进制代码对每个示例都是唯一的。这些实验探索了压缩的建模和编码组件不同于算术编码的设置。
- en: 3.4 Tokenization of Compressed Text
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 压缩文本的词元化
- en: 'Table 1: “Token” vs. “bit” compression ratios. Larger vocabularies require
    more bits to store each token, and thus incur a cost in terms of absolute compression.
    However, when trying to minimize the compute an LLM uses to process a given piece
    of text, token sequence length is what matters.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：“词元”与“位”压缩比。更大的词汇表需要更多的位来存储每个词元，因此在绝对压缩方面会产生成本。然而，当试图最小化 LLM 处理给定文本所使用的计算时，词元序列长度才是关键。
- en: '| Method | Token Compression Ratio | Bit Compression Ratio |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 词元压缩比 | 位压缩比 |'
- en: '| --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| SentencePiece | $4.28$ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| SentencePiece | $4.28$ |'
- en: '| AC$[v\mathord{=}256]$ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| AC$[v\mathord{=}256]$ |'
- en: '| AC$[v\mathord{=}65\text{k}]$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| AC$[v\mathord{=}65\text{k}]$ |'
- en: Most compression methods output a bitstream, but training M2 directly over bits
    would not be ideal. As M1 was trained over UTF-8 bytes, the bit-level output of
    compression would result in M2 being applied to much longer sequences. Additionally,
    models are generally trained with vocabulary sizes much larger than two. Thus,
    we need a method to segment the bitstream into tokens, creating a more standard
    sequence for training language models.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数压缩方法输出比特流，但直接对比特进行训练 M2 并不理想。由于 M1 是在 UTF-8 字节上训练的，压缩的比特级输出将导致 M2 应用于更长的序列。此外，模型通常使用比两个更大的词汇表大小进行训练。因此，我们需要一种方法将比特流分段为词元，创建更标准的序列以训练语言模型。
- en: We convert the bitstream into a token sequence by grouping every $N$. As the
    tokens are created from the compressed bitstream, we expect the distribution of
    tokens to be more uniform than the usual Zipfian [[76](#bib.bib76)] distribution
    of word or subword tokens, allowing us to use larger vocabularies without encountering
    issues of rare or unattested tokens.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过每 $N$ 分组将比特流转换为词元序列。由于词元是从压缩的比特流中创建的，我们期望词元的分布比通常的 Zipfian [[76](#bib.bib76)]
    单词或子词词元分布更均匀，使我们能够使用更大的词汇表而不遇到稀有或未证明的词元问题。
- en: Throughout this work, we focus on the “token compression ratio” $L_{iT}/L_{oT}$-bit
    tokens from the output of Arithmetic Coding does not change the bit compression
    ratio—the total number of bits is unchanged—but it does reduce the number of tokens
    in the sequence, and thus the number of tokens the LLM must process. We compute
    compression ratios over the C4 dev set, which is unseen during M1 training.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本工作中，我们专注于“标记压缩比” $L_{iT}/L_{oT}$——来自算术编码的输出的位标记不会改变位压缩比——总位数保持不变——但确实减少了序列中的标记数量，从而减少了
    LLM 必须处理的标记数量。我们计算了 C4 开发集上的压缩比，这在 M1 训练期间未见过。
- en: To highlight the differences between the tokenization methods above, we measure
    the performance (as bits/byte on a sample of the C4 validation set) of two trivial
    models for each tokenizer in [Table 3](#S3.T3 "In 3.4 Tokenization of Compressed
    Text ‣ 3 Methods ‣ Training LLMs over Neurally Compressed Text"). The “uniform”
    model naïvely assigns equal probability to each token, regardless of context.
    The “unigram” model also ignores context, but assigns probabilities based on the
    global token frequencies observed in the training data. With byte-level tokenization,
    each UTF-8 byte encodes to a single $8$ bits/byte. For more powerful tokenizers,
    the uniform model is stronger, indicating that the tokenizer itself has some language
    modeling ability. We observe that our compression-based tokenizers (AC, EqualInfoAC
    and GZip) output a near-uniform distribution of tokens across their vocabulary.
    This is reflected in the near-zero gain over “uniform” achieved by modeling unigram
    statistics.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突显上述分词方法之间的差异，我们测量了每个分词器的两个简单模型的性能（以 C4 验证集样本中的位/字节为单位），见[表 3](#S3.T3 "在 3.4
    压缩文本的分词 ‣ 3 种方法 ‣ 在神经压缩文本上训练 LLMs")。 “均匀”模型天真地给每个标记分配相等的概率，无论上下文如何。 “单字模型”也忽略上下文，但根据训练数据中观察到的全局标记频率分配概率。使用字节级分词，每个
    UTF-8 字节编码为 $8$ 位/字节。对于更强大的分词器，均匀模型更强，表明分词器本身具有一定的语言建模能力。我们观察到，我们的基于压缩的分词器（AC、EqualInfoAC
    和 GZip）在其词汇表中输出了近乎均匀的标记分布。这在“均匀”模型通过建模单字统计量实现的接近零增益中得以体现。
- en: 'Table 2: Weakening the “model” or “coding” component of Arithmetic Coding reduces
    the compression rate. The reduction of M1 to a static unigram distribution results
    in the worst compression ratio. When using EqualInfoAC, M1 is weaker, as it has
    less context, and coding is weaker, as padding is often required at the end of
    windows. The compression ratio improves with larger window sizes.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 削弱算术编码的“模型”或“编码”组件会降低压缩率。将 M1 减少到静态单字分布会导致最差的压缩比。使用 EqualInfoAC 时，M1 较弱，因为其上下文较少，编码也较弱，因为窗口结束时通常需要填充。随着窗口大小的增加，压缩比有所改善。'
- en: '| Method | Compression Ratio |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 压缩比 |'
- en: '| --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| AC$[v\mathord{=}256]$ |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| AC$[v\mathord{=}256]$ |'
- en: '| StaticAC$[v\mathord{=}256]$ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| StaticAC$[v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}32,\,v\mathord{=}256]$ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}32,\,v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}64,\,v\mathord{=}256]$ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}64,\,v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}128,\,v\mathord{=}256]$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}128,\,v\mathord{=}256]$ |'
- en: 'Table 3: Bits/byte ($\downarrow$) performance of two trivial models across
    tokenizers. “Uniform” assigns equal probability to each token. “Unigram” assigns
    probabilities based on the empirical token frequencies. As the compression-based
    tokenizers output near-uniform distributions over tokens, there is little gain
    in modeling unigram statistics. Thus, learning over this data requires modeling
    longer contexts.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 各分词器中两个简单模型的位/字节 ($\downarrow$) 性能。 “均匀”模型为每个标记分配相等的概率。 “单字模型”根据经验标记频率分配概率。由于基于压缩的分词器输出了接近均匀的标记分布，因此在建模单字统计量时几乎没有增益。因此，在这些数据上进行学习需要建模更长的上下文。'
- en: '| Method | Uniform bits/byte | Unigram bits/byte | $\Delta$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 均匀位/字节 | 单字位/字节 | $\Delta$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Bytes | $8.000$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 字节 | $8.000$ |'
- en: '| SentencePiece | $3.497$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| SentencePiece | $3.497$ |'
- en: '| AC$[v\mathord{=}256]$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| AC$[v\mathord{=}256]$ |'
- en: '| StaticAC$[v\mathord{=}256]$ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| StaticAC$[v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$ |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}32,\,v\mathord{=}256]$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}32,\,v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}64,\,v\mathord{=}256]$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}64,\,v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}128,\,v\mathord{=}256]$ |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}128,\,v\mathord{=}256]$ |'
- en: '| GZip$[v\mathord{=}256]$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| GZip$[v\mathord{=}256]$ |'
- en: 3.5 Training M2 on Compressed Data
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 在压缩数据上训练 M2
- en: Each M2 model is trained for $200{,}000$%) are non-padding tokens; see [Appendix C](#A3
    "Appendix C The Amount of Raw Text Bytes Seen by M2 ‣ Training LLMs over Neurally
    Compressed Text") for details and [Table 13](#A3.T13 "In Appendix C The Amount
    of Raw Text Bytes Seen by M2 ‣ Training LLMs over Neurally Compressed Text") for
    the exact size of each dataset. As methods with higher compression ratios cover
    more raw text per token, we also include the total number of bytes in each dataset.
    Shuffling of training sets is seeded, and dataset state is checkpointed during
    training, so each training run results in the model seeing each example exactly
    once.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 M2 模型训练 $200{,}000$%) 是非填充令牌；详见[附录 C](#A3 "附录 C M2 看到的原始文本字节量 ‣ 在神经压缩文本上训练
    LLMs")以及[表 13](#A3.T13 "在附录 C 中 M2 看到的原始文本字节量 ‣ 在神经压缩文本上训练 LLMs")以获取每个数据集的确切大小。由于压缩比更高的方法覆盖每个令牌的原始文本更多，我们还包括了每个数据集的总字节数。训练集的洗牌是有种子的，并且在训练期间数据集状态被检查点化，因此每次训练运行都会使模型准确看到每个示例一次。
- en: Models are trained at four sizes, as shown in [Table 4](#S3.T4 "In 3.5 Training
    M2 on Compressed Data ‣ 3 Methods ‣ Training LLMs over Neurally Compressed Text"),
    with $25$ tokens. Thus, when training on 16-bit tokens, twice as many bytes are
    seen per example and in training overall, as compared to 8-bit tokens. All other
    hyperparameters match those used in M1.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在四种大小下进行训练，如[表 4](#S3.T4 "在 3.5 训练 M2 在压缩数据上 ‣ 3 方法 ‣ 在神经压缩文本上训练 LLMs")所示，使用
    $25$ 个令牌。因此，当在 16 位令牌上进行训练时，相比于 8 位令牌，每个示例和整体训练过程中看到的字节数是两倍。所有其他超参数与 M1 中使用的相匹配。
- en: 'Table 4: Model sizes used in our experiments, and corresponding hyperparameter
    settings. Note, model parameter counts exclude embedding table parameters.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：我们实验中使用的模型大小以及相应的超参数设置。注意，模型参数计数不包括嵌入表参数。
- en: '| Parameter Count | Embedding Dim | #Heads | #Layers | Head Dim | MLP Dim |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 参数数量 | 嵌入维度 | #头 | #层 | 头维度 | MLP 维度 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| $3$ |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| $3$ |'
- en: '| $25$ |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| $25$ |'
- en: '| $113$ |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| $113$ |'
- en: '| $403$ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| $403$ |'
- en: '| $2$ |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| $2$ |'
- en: 3.6 Baselines
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 基线
- en: We compare our M2 models against baseline models trained with two standard tokenization
    methods, described below. All hyperparameters, including sequence length ($512$),
    match those used for our M2 training above.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的 M2 模型与使用两种标准分词方法训练的基线模型进行比较，详细描述如下。所有超参数，包括序列长度（$512$），与我们上述 M2 训练中使用的参数相匹配。
- en: Bytes
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 字节
- en: These baselines train directly over UTF-8 bytes, using the byte tokenizer from
    ByT5 [[74](#bib.bib74)]. The models see $26.2$ billion bytes total (see [Table 13](#A3.T13
    "In Appendix C The Amount of Raw Text Bytes Seen by M2 ‣ Training LLMs over Neurally
    Compressed Text")).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基线直接在 UTF-8 字节上训练，使用来自 ByT5 的字节分词器 [[74](#bib.bib74)]。这些模型总共看到 $26.2$ 亿字节（见[表
    13](#A3.T13 "在附录 C 中 M2 看到的原始文本字节量 ‣ 在神经压缩文本上训练 LLMs")）。
- en: SentencePiece
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SentencePiece
- en: These baselines train on text tokenized with the SentencePiece vocabulary of
    $32{,}000$ billion bytes total (see [Table 13](#A3.T13 "In Appendix C The Amount
    of Raw Text Bytes Seen by M2 ‣ Training LLMs over Neurally Compressed Text")).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基线在使用 SentencePiece 词汇表的文本上进行训练，总共 $32{,}000$ 亿字节（见[表 13](#A3.T13 "在附录 C 中
    M2 看到的原始文本字节量 ‣ 在神经压缩文本上训练 LLMs")）。
- en: 3.7 Numerical Stability
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 数值稳定性
- en: Arithmetic Coding depends on the creation of “intervals” that cover each symbol
    in the vocabulary based on the quantized cumulative distribution of a model’s
    logits when predicting the next token. As such, a small change in the logits due
    to numerical noise can result in vastly different output bitstreams. This can
    make the practical use of neural language models in compression difficult. Common
    sources of noise include changes in batch size, parallel computation, changes
    to compute infrastructure (CPU vs. GPU vs. TPU, different TPU topology, etc.),
    changes to inference (computing the logits for the whole sequence at once vs. computing
    logits for a single token at a time using KV caches), and changes to the longest
    sequence length in the batch.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 算术编码依赖于创建“区间”，这些区间基于模型的逻辑值的量化累积分布来覆盖词汇表中的每个符号，用于预测下一个令牌。因此，由于数值噪声引起的逻辑值的小变化可能会导致输出比特流发生巨大变化。这可能使得在压缩中实际使用神经语言模型变得困难。常见的噪声源包括批量大小的变化、并行计算、计算基础设施的变化（CPU
    vs. GPU vs. TPU，不同的 TPU 拓扑等）、推理的变化（一次计算整个序列的逻辑值 vs. 使用 KV 缓存逐个计算单个令牌的逻辑值）以及批次中最长序列长度的变化。
- en: Methods like the rank-sorted algorithm used in LLMZip [[63](#bib.bib63)] may
    help alleviate these issues as only the order of tokens needs to match between
    settings. The development of alternate methods of LLM-based compression should
    keep numerical stability issues in mind and ideally alleviate these issues in
    the design of the algorithm. Increasing the level of quantization could also help
    reduce numerical noise issues, as differences would mostly be lost in quantization,
    but this would have a negative impact on the compression ratio.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 像 LLMZip [[63](#bib.bib63)] 中使用的排序算法等方法可能有助于缓解这些问题，因为只需在设置之间匹配 token 的顺序。开发基于
    LLM 的压缩的替代方法应考虑数值稳定性问题，并在算法设计中理想地缓解这些问题。增加量化级别也可能有助于减少数值噪声问题，因为差异大多会在量化中丢失，但这会对压缩比产生负面影响。
- en: 3.8 Evaluation
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8 评估
- en: As the tokenization scheme varies across the approaches we consider, models
    cannot be directly compared on “per-token” metrics such as negative log likelihood
    loss $\ell$, which scales the model’s loss by the token-level compression rate.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们考虑的各方法的分词方案各异，因此模型无法在“每个 token”指标上直接比较，例如负对数似然损失 $\ell$，该指标按 token 级别的压缩率来缩放模型的损失。
- en: We also compare models on how much computation (FLOPs) is required to perform
    inference over a given length of raw text (bytes). More specifically, we calculate
    M2’s expected FLOPs/byte by scaling FLOPs/token—approximated by $2\,\times\,\texttt{params}$
    (excluding embedding parameters) following [[35](#bib.bib35)]—by the token-level
    compression rate (as tokens/byte). For methods using an M1 model during compression,
    the FLOPs/byte cost of M1 is added.^(10)^(10)10While there is a computational
    cost to running GZip over the input text, we ignore it as it is insubstantial
    compared to the cost of running M2 model inference. For more details on the evaluation
    metrics see [Appendix G](#A7 "Appendix G Evaluation Details ‣ Training LLMs over
    Neurally Compressed Text").
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还比较了模型在对给定长度的原始文本（字节）进行推理时所需的计算量（FLOPs）。更具体地说，我们通过将 FLOPs/token 以 token 级别的压缩率（按
    token/字节）进行缩放来计算 M2 的预期 FLOPs/字节——FLOPs/token 近似为 $2\,\times\,\texttt{params}$（不包括嵌入参数），如
    [[35](#bib.bib35)] 所示。对于在压缩过程中使用 M1 模型的方法，还会添加 M1 的 FLOPs/字节成本。^(10)^(10)10 尽管运行
    GZip 对输入文本有计算成本，但由于其与运行 M2 模型推理的成本相比微不足道，因此我们忽略了它。有关评估指标的更多细节，请参见 [附录 G](#A7 "附录
    G 评估详情 ‣ 在神经压缩文本上训练 LLMs")。
- en: We evaluate models on a sample of the C4 validation set. During evaluation,
    the model is run over $20$. Thus, the variance introduced from sampling the validation
    set is negligible. See [Appendix B](#A2 "Appendix B Variance ‣ Training LLMs over
    Neurally Compressed Text") for more information about variance.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 C4 验证集的样本上评估模型。在评估期间，模型在 $20$ 上运行。因此，从验证集采样引入的方差可以忽略不计。有关方差的更多信息，请参见 [附录
    B](#A2 "附录 B 方差 ‣ 在神经压缩文本上训练 LLMs")。
- en: 4 Results
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果
- en: '![Refer to caption](img/607d636837180ddc0c5b9331051d2fd6.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/607d636837180ddc0c5b9331051d2fd6.png)'
- en: 'Figure 3: Models trained over compressed text are compared against baseline
    models in terms of bits/byte ($\downarrow$ outperforming the Bytes baseline at
    all sizes. While SentencePiece performs the best, the gap between EqualInfoAC
    and SentencePiece narrows with scale. See [Appendix A](#A1 "Appendix A Numerical
    Values ‣ Training LLMs over Neurally Compressed Text") for the exact values used
    in this and other graphs.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：对比基线模型，训练在压缩文本上的模型在每字节位数 ($\downarrow$) 方面的表现。虽然 SentencePiece 的表现最佳，但 EqualInfoAC
    与 SentencePiece 之间的差距随着规模的扩大而缩小。有关此图及其他图中使用的确切值，请参见 [附录 A](#A1 "附录 A 数值 ‣ 在神经压缩文本上训练
    LLMs")。
- en: Simple methods of training over neural-compressed text fail
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练神经压缩文本的简单方法失败
- en: As seen in [Fig. 3](#S4.F3 "In 4 Results ‣ Training LLMs over Neurally Compressed
    Text"), the most obvious approach—compression using Arithmetic Coding with M1
    assigning next-token probabilities—fails to learn anything. Regardless of scale,
    the model only learns to output a uniform distribution over tokens, the performance
    of which is denoted by the dashed line. As the Arithmetic Coding procedure is
    near optimal [[45](#bib.bib45)], the compression ratio is essentially determined
    by the loss of M1\. Thus, even though the M2 model learns nothing useful, when
    scaled by the compression rate, this setting ends up with the same performance
    as the M1 model. Similarly, models trained over data compressed with StaticAC—where
    M1 is replaced with a static unigram model—fail to learn. This result suggests
    that the difficultly in learning stems from the complexity or brittleness of the
    Arithmetic Coding process itself, rather than from M2’s inability to model M1\.
    Note that the weak “modeling” component of this compression scheme results in
    a much lower compression rate and thus worse bits/byte performance, despite the
    model also learning a uniform distribution.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 3](#S4.F3 "在 4 结果 ‣ 基于神经压缩文本的 LLM 训练")所示，最明显的方法——使用算术编码进行压缩，M1 负责分配下一个令牌的概率——无法学习任何内容。无论规模如何，该模型只学会输出令牌上的均匀分布，其性能由虚线表示。由于算术编码过程接近最优[[45](#bib.bib45)]，压缩比基本上由
    M1 的损失决定。因此，即使 M2 模型没有学习到有用的东西，当按压缩率进行缩放时，这种设置的性能也与 M1 模型相同。同样，使用 StaticAC 压缩的数据上训练的模型——其中
    M1 被静态 unigram 模型替代——也无法学习。这一结果表明，学习困难源于算术编码过程本身的复杂性或脆弱性，而不是 M2 无法建模 M1。请注意，这种压缩方案中“建模”部分的弱点导致压缩比显著降低，从而使得位/字节性能更差，尽管模型也学习了均匀分布。
- en: SentencePiece is a strong baseline
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SentencePiece 是一个强有力的基准
- en: Our SentencePiece baseline outperforms all other methods, including our Bytes
    baseline, across all model sizes. On the surface, this result seems to run counter
    to the recent findings of [[16](#bib.bib16)], where their byte-level models outperformed
    subword (BPE) models at medium and large scales. The discrepancy is due to prioritizing
    different metrics. They report the model’s bit compression rate on fixed-length
    ($2{,}048$ bytes (but never evaluated on this ability), and are allotted fewer
    inference FLOPs to process the same text, as compared to the byte-level models.
    Additionally, *bit* compression ratio penalizes subword models for having larger
    vocabulary sizes. By contrast, our evaluation tests what perplexity models achieve
    on sequences of the same length they were trained on, and compares models at matching
    FLOPs/byte cost. This aligns with our end goal, which is to train an LLM that
    achieves the best perplexity at whatever sequence length it can handle, given
    a fixed budget for training and inference.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 SentencePiece 基准在所有模型规模中均优于其他方法，包括我们的字节基准。从表面上看，这一结果似乎与[[16](#bib.bib16)]的最新发现相悖，其中他们的字节级模型在中型和大型规模上优于子词（BPE）模型。这个差异是由于优先考虑了不同的指标。他们报告了模型在固定长度
    ($2{,}048$ 字节) 下的位压缩率（但从未评估过这个能力），并且在处理相同文本时分配的推理 FLOP 更少，与字节级模型相比。此外，*位* 压缩比对子词模型的词汇表大小较大进行惩罚。相比之下，我们的评估测试模型在训练时相同长度的序列上达到的困惑度，并比较具有匹配的
    FLOP/字节成本的模型。这与我们的最终目标一致，即在给定固定训练和推理预算的情况下，训练出一个在能够处理的任何序列长度上都能获得最佳困惑度的 LLM。
- en: '![Refer to caption](img/bd3e43703b6dfcc0c2c847599cca34f5.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bd3e43703b6dfcc0c2c847599cca34f5.png)'
- en: 'Figure 4: Comparing models in terms of bits/byte ($\downarrow$ model is on
    this frontier.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：按位/字节 ($\downarrow$ 模型位于此前沿。
- en: Equal-Info Windows make AC learnable
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Equal-Info 窗口使得 AC 可学习
- en: '[Fig. 3](#S4.F3 "In 4 Results ‣ Training LLMs over Neurally Compressed Text")
    shows that EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$ curve compared to the
    Bytes curve—due to shorter sequence lengths.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3](#S4.F3 "在 4 结果 ‣ 基于神经压缩文本的 LLM 训练") 显示了 EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$
    曲线与字节曲线的对比——由于序列长度较短。'
- en: Using $16$ outperforms the Bytes baseline at all model sizes. It underperforms
    the SentencePiece baseline, but the gap diminishes with scale.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 $16$ 在所有模型规模中优于字节基准。虽然它表现不如 SentencePiece 基准，但随着规模的增大，这一差距在缩小。
- en: However, EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$ take fewer autoregressive
    steps to generate the same text than models using SentencePiece encoding. This
    has the potential to reduce generation latency, at the cost of reduced compute
    efficiency. This is a tradeoff that is often worth making in production. For instance,
    speculative decoding [[42](#bib.bib42)] is a popular approach that performs redundant
    computation in order to potentially accelerate auto-regressive steps.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$生成相同文本所需的自回归步骤比使用SentencePiece编码的模型要少。这有可能减少生成延迟，但以减少计算效率为代价。这是一种在生产中常常值得做出的权衡。例如，投机解码[[42](#bib.bib42)]是一种常见的方法，它执行冗余计算以可能加速自回归步骤。
- en: It is noteworthy that the EqualInfoAC M2 models learn well despite being trained
    on data that has nearly uniform unigram statistics, as we saw in [Table 3](#S3.T3
    "In 3.4 Tokenization of Compressed Text ‣ 3 Methods ‣ Training LLMs over Neurally
    Compressed Text"). In the best case, our $2$% fewer FLOPs/byte.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管EqualInfoAC M2模型在几乎均匀的单词统计数据上进行训练，但它们仍然能够很好地学习，正如我们在[表3](#S3.T3 "在
    3.4 压缩文本的标记化 ‣ 3 方法 ‣ 在神经压缩文本上训练LLMs")中看到的那样。在最佳情况下，我们减少了$2$%的FLOPs/字节。
- en: It is apparent from [Fig. 3](#S4.F3 "In 4 Results ‣ Training LLMs over Neurally
    Compressed Text") that if FLOPs/byte were held constant, SentencePiece would achieve
    slightly better bits/byte than EqualInfoAC. However there is another axis along
    which EqualInfoAC may still be preferred. Setting aside inference FLOPs, all our
    SentencePiece models require $23$ model to recover bits/byte performance while
    retaining the reduced latency. This can be seen visually in [Fig. 4](#S4.F4 "In
    SentencePiece is a strong baseline ‣ 4 Results ‣ Training LLMs over Neurally Compressed
    Text").
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图3](#S4.F3 "在 4 结果 ‣ 在神经压缩文本上训练LLMs")可以明显看出，如果保持FLOPs/字节不变，SentencePiece会比EqualInfoAC稍微好一点。然而，EqualInfoAC还有另一个可能的优势。撇开推断FLOPs不谈，我们所有的SentencePiece模型都需要$23$个模型才能恢复比特/字节性能，同时保持较低的延迟。这可以在[图4](#S4.F4
    "在 SentencePiece 是强基线 ‣ 4 结果 ‣ 在神经压缩文本上训练LLMs")中直观地看到。
- en: GZip is not competitive
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GZip不具竞争力
- en: Training over GZip-compressed text is relatively ineffective. M2’s performance
    when trained over GZip highlights a counter-intuitive trend. While the GZip M2
    models actually learn, it would still be preferable to train over AC-compressed
    text—even though those models do not learn. This is due to the weak compression
    offered by GZip. The poor compression rate, coupled with weak learning, means
    that the GZip M2 models’ bits/byte performance lags behind even the $3$m parameter
    M1 model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对GZip压缩文本的训练相对无效。M2在GZip压缩数据上的表现突显了一个违反直觉的趋势。尽管GZip M2模型确实能够学习，但仍然更倾向于使用AC压缩文本进行训练——即便那些模型无法学习。这是因为GZip提供的压缩效果较弱。GZip的较差压缩率，加上较弱的学习能力，意味着GZip
    M2模型的比特/字节性能甚至落后于$3$m参数的M1模型。
- en: '![Refer to caption](img/e55fc3e505648431fb2b14a03e1b77ee.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e55fc3e505648431fb2b14a03e1b77ee.png)'
- en: (a) Controlling for Compression Ratio
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 控制压缩比
- en: '![Refer to caption](img/1538e1bc1a45e92cf6b9bd67bb91b79c.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1538e1bc1a45e92cf6b9bd67bb91b79c.png)'
- en: (b) Ignoring Compression Ratio
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 忽略压缩比
- en: 'Figure 5: Performance of EqualInfoAC across various window sizes, $b$ is second-best.
    This is due to the higher compression rate achieved by longer Equal Info Windows.
    When evaluating tokens/byte (right), a monotonic trend emerges, showing that shorter
    windows are easier to learn.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：EqualInfoAC在各种窗口大小下的表现，$b$是第二最佳。这是由于较长的Equal Info窗口实现了更高的压缩率。在评估令牌/字节（右侧）时，出现了单调趋势，显示短窗口更易于学习。
- en: Short windows are the best
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 短窗口效果最佳
- en: We see a similar effect in [Fig. 5](#S4.F5 "In GZip is not competitive ‣ 4 Results
    ‣ Training LLMs over Neurally Compressed Text"), which ablates the EqualInfoAC
    window size. In terms of bits/byte, the shortest $16$-bit windows, performance
    improvements with scale are small, but present; see [Table 10](#A1.T10 "In Appendix
    A Numerical Values ‣ Training LLMs over Neurally Compressed Text") for exact numbers.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图5](#S4.F5 "在 GZip 不具竞争力 ‣ 4 结果 ‣ 在神经压缩文本上训练LLMs")中，我们可以看到类似的效果，该图消融了EqualInfoAC的窗口大小。在比特/字节方面，最短的$16$-位窗口的性能改进随着规模的增大而减少，但仍然存在；具体数字见[表10](#A1.T10
    "在附录A 数值 ‣ 在神经压缩文本上训练LLMs")。
- en: '![Refer to caption](img/3ab3d6f83df7acaa7a313efc5f4e796a.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3ab3d6f83df7acaa7a313efc5f4e796a.png)'
- en: 'Figure 6: Using a larger vocabulary for Arithmetic Coding derived methods improves
    both perplexity (lower bits/byte) as well as token compression rate (lower FLOPs/byte).
    Among settings where the M2 model actually learns, training over GZip-compressed
    data is the only case where increasing vocabulary size to $65$k does not help
    performance.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：使用更大的词汇量对算术编码衍生方法进行改进，不仅提高了困惑度（较低的bits/byte），还提高了标记压缩率（较低的FLOPs/byte）。在M2模型实际学习的设置中，唯一一个将词汇量增加到
    $65$k 但未能提升性能的情况是训练GZip压缩数据。
- en: Larger M2 vocabulary is helpful
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更大的M2词汇量是有帮助的
- en: Tokenizing compressed text using a larger $16$ text mapping. This could be one
    reason for the performance gain; see [Section 6.1](#S6.SS1 "6.1 EqualInfoAC is
    less stable and less semantic than SentencePiece ‣ 6 Analysis ‣ Training LLMs
    over Neurally Compressed Text") for more discussion of “stability”.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更大的 $16$ 文本映射对压缩文本进行分词。这可能是性能提升的一个原因；请参见 [第6.1节](#S6.SS1 "6.1 EqualInfoAC
    比 SentencePiece 更不稳定且语义更少 ‣ 6 分析 ‣ 基于神经压缩文本训练LLMs")，以获取更多关于“稳定性”的讨论。
- en: Emergence with scale is unlikely
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 规模上的突现不太可能
- en: Given the recent findings of [[55](#bib.bib55)], we anticipate that continuing
    to scale models beyond $2$ billion parameters is unlikely to deliver an “emergent”
    ability to learn over AC-compressed text, since the bits/byte metric we use is
    smooth.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 根据最近的发现 [[55](#bib.bib55)]，我们预测，继续将模型规模扩大到 $2$ 亿参数以上不太可能获得在AC压缩文本上学习的“突现”能力，因为我们使用的bits/byte度量是平滑的。
- en: Results persist under “scaling laws” paradigm
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果在“缩放法则”范式下依然有效
- en: When scaling models, [[30](#bib.bib30)] recommend that training tokens should
    be scaled linearly with model size. However, in our experiments above, all models
    see the same number of tokens, regardless of model size. Consequently, our largest
    models may be somewhat “undertrained”.^(12)^(12)12The undertraining of our $2$
    steps, showing the models have not yet converged. To test whether following the
    “scaling laws” recommendation influences our results, we reevaluate our models
    at earlier checkpoints selected to maintain a constant ratio of training data
    to model size. We find that all core trends are unchanged in this setting. See
    [Appendix D](#A4 "Appendix D Scaling Curves with Scaled Training Data ‣ Training
    LLMs over Neurally Compressed Text") for details.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展模型时，[[30](#bib.bib30)] 建议训练标记应与模型规模线性扩展。然而，在我们上述实验中，所有模型看到的标记数量是相同的，无论模型规模如何。因此，我们最大的模型可能会有些“欠训练”。^(12)^(12)12
    欠训练显示模型尚未收敛。为了测试是否遵循“缩放法则”推荐影响我们的结果，我们在较早的检查点重新评估模型，这些检查点选择以保持训练数据与模型规模的比例不变。我们发现，在这种设置下所有核心趋势保持不变。详情请见
    [附录D](#A4 "附录D 规模化训练数据的缩放曲线 ‣ 基于神经压缩文本训练LLMs")。
- en: 5 Additional Experiments
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 额外实验
- en: At this point, we have established that while the simplest approaches to training
    over compressed text fail, there are alternate compression schemes that are learnable.
    In this section, we conduct additional experiments to shed light on which aspects
    of different compression methods are difficult to learn and what contributes to
    their learnability.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经确认虽然对压缩文本的最简单训练方法失败，但存在可学习的替代压缩方案。在本节中，我们进行额外实验，以揭示不同压缩方法中哪些方面难以学习，以及哪些因素促进了它们的可学习性。
- en: 5.1 Bitstream tokenization is not the main source of difficulty
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 位流分词不是主要难点
- en: The compression algorithms we consider output a bitstream, which we later chunk
    into tokens of a fixed bit depth (e.g., $8$-bit tokens). As such, it is common
    for the bits representing a single character or UTF-8 byte to be split across
    multiple tokens. Compounding this issue is that the value of these tokens are
    contextually determined and may differ depending on the surrounding bytes.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑的压缩算法输出位流，我们随后将其分块为固定位深度的标记（例如，$8$-位标记）。因此，表示单个字符或UTF-8字节的位通常会跨多个标记进行拆分。进一步加剧这一问题的是，这些标记的值是上下文决定的，可能会根据周围的字节有所不同。
- en: The fact that both $8$. We use the same hyperparameters as in [Section 3](#S3
    "3 Methods ‣ Training LLMs over Neurally Compressed Text"). Working at the bit
    level means that the output sequence is now longer than the input sequence, which
    was UTF-8 bytes. As such, this setting is not practical in the real world.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，$8$ 的情况相同。我们使用与 [第3节](#S3 "3 方法 ‣ 基于神经压缩文本训练LLMs") 相同的超参数。在位级别操作意味着输出序列现在比输入序列更长，输入序列为UTF-8字节。因此，这种设置在实际应用中并不实用。
- en: When trained to convergence, the two models have cross entropy losses of $0.693$.
    This failure mode is the same as in [Fig. 3](#S4.F3 "In 4 Results ‣ Training LLMs
    over Neurally Compressed Text"), which suggests that AC encoding itself is the
    main source of difficulty, as opposed to any issue around tokenization or vocabulary
    size.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练到收敛时，这两个模型的交叉熵损失为 $0.693$。这种失败模式与[图 3](#S4.F3 "在 4 结果 ‣ 在神经压缩文本上训练 LLMs")中的相同，这表明算术编码本身是主要的困难来源，而不是与标记化或词汇表大小有关的任何问题。
- en: 5.2 Transformers struggle to learn Arithmetic Coding
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 变换器难以学习算术编码
- en: Arithmetic Coding is a sequential algorithm that involves tracking multiple
    state variables as the input (byte) sequence is consumed. Each token in the output
    sequence represents multiple transformations of these variables, e.g., $8$m model
    has fewer layers—we see in practice that the Arithmetic Coding algorithm is still
    difficult to learn.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 算术编码是一个顺序算法，涉及在输入（字节）序列被消耗时跟踪多个状态变量。输出序列中的每个标记代表这些变量的多个转换，例如，$8$m 模型层数较少——我们在实践中看到算术编码算法仍然很难学习。
- en: 'Table 5: Transformers struggle to learn Arithmetic Coding. In the sequence-to-sequence
    setting, a model that learns AC compression/decompression should have an accuracy
    of $100$ accuracy.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：变换器难以学习算术编码。在序列到序列的设置中，学习 AC 压缩/解压缩的模型应具有 $100$ 的准确率。
- en: '| Task | Accuracy | Cross Entropy |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 准确率 | 交叉熵 |'
- en: '| --- | --- | --- |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Decompression | $76.98$ |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 解压缩 | $76.98$ |'
- en: '| Byte Level LM | $76.86$ |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 字节级 LM | $76.86$ |'
- en: '| Compression | $1.7$ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 压缩 | $1.7$ |'
- en: To directly diagnose the ability to track Arithmetic Coding, we format AC compression
    and decompression as sequence-to-sequence tasks. The input provides the model
    with the true text, so we expect a model that is able to learn Arithmetic Coding
    should achieve an accuracy of $100$m parameters. Loss, gradients, and evaluation
    metrics are only computed on the target tokens.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直接诊断跟踪算术编码的能力，我们将 AC 压缩和解压缩格式化为序列到序列的任务。输入为模型提供真实文本，因此我们期望能够学习算术编码的模型应达到 $100$m
    参数的准确率。损失、梯度和评估指标仅计算目标标记。
- en: In the decompression task, the target tokens are bytes. By ignoring the inputs
    and just modeling the outputs, the decompression model can achieve decent performance
    without actually leveraging the input data. To control for this, we also train
    a byte-level language model baseline on the same sequence-to-sequence data, excluding
    the input tokens. If the decompression model is actually learning to decompress
    Arithmetic Coding, we would expect stronger performance than the byte-level baseline.
    As we see in [Table 5](#S5.T5 "In 5.2 Transformers struggle to learn Arithmetic
    Coding ‣ 5 Additional Experiments ‣ Training LLMs over Neurally Compressed Text"),
    the baseline model, which does not see the input tokens, has the same performance
    as the decompression model.^(14)^(14)14The slight gain is statistically insignificant,
    $(p=0.07)$. Clearly, the models trained for decompression are not actually learning
    to do decompression.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在解压缩任务中，目标标记是字节。通过忽略输入并仅建模输出，解压缩模型可以在不实际利用输入数据的情况下实现良好的性能。为此，我们还在相同的序列到序列数据上训练了一个字节级语言模型基准，不包括输入标记。如果解压缩模型确实在学习解压算术编码，我们会期望其表现优于字节级基准。正如我们在[表5](#S5.T5
    "在 5.2 变换器难以学习算术编码 ‣ 5 额外实验 ‣ 在神经压缩文本上训练 LLMs")中看到的那样，基准模型（未见输入标记）与解压缩模型的表现相同。^(14)^(14)14
    轻微的提升在统计上不显著，$(p=0.07)$。显然，训练用于解压缩的模型实际上并未学习到解压缩。
- en: The model trained for compression actually shows some signs of learning. Training
    a language model directly on the compressed output results in the model learning
    a uniform distribution over tokens, see [Fig. 3](#S4.F3 "In 4 Results ‣ Training
    LLMs over Neurally Compressed Text"). When the model is able to attend to the
    input text, we see that the performance in [Table 5](#S5.T5 "In 5.2 Transformers
    struggle to learn Arithmetic Coding ‣ 5 Additional Experiments ‣ Training LLMs
    over Neurally Compressed Text") is better than the uniform distribution (which
    would have a cross entropy loss of $5.545$ it should be able to achieve.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 训练用于压缩的模型实际上显示出一些学习的迹象。在压缩输出上直接训练语言模型会导致模型学习到标记上的均匀分布，见[图 3](#S4.F3 "在 4 结果
    ‣ 在神经压缩文本上训练 LLMs")。当模型能够关注输入文本时，我们看到在[表5](#S5.T5 "在 5.2 变换器难以学习算术编码 ‣ 5 额外实验
    ‣ 在神经压缩文本上训练 LLMs")中的表现优于均匀分布（其交叉熵损失为 $5.545$，模型应该能够达到）。
- en: We also find training on these sequence-to-sequence datasets to be less stable
    than training on the language modeling datasets. In our experiments, large performance
    swings and divergence were relatively common.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还发现，相比于语言建模数据集，在这些序列到序列数据集上的训练更不稳定。在我们的实验中，大幅性能波动和发散是相对常见的。
- en: 5.3 Larger vocabulary helps beyond increasing the compression ratio
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 更大的词汇量有助于提高压缩比
- en: Our best results training over compressed text use EqualInfoAC with $16$ batches.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在压缩文本上训练的最佳结果使用了 EqualInfoAC 和 $16$ 批次。
- en: '[Table 6](#S5.T6 "In 5.3 Larger vocabulary helps beyond increasing the compression
    ratio ‣ 5 Additional Experiments ‣ Training LLMs over Neurally Compressed Text")
    shows that even in this setting, the model with larger vocabulary is stronger.^(16)^(16)16It
    may be possible to achieve further gains by increasing the token bit depth further.
    However, most deep learning frameworks do not support using unsigned data types
    for inputs, and the resulting large vocabulary size can cause a computational
    bottleneck in the final softmax layer. In fact, *most* of the bits/byte gain ($84$
    model uses exactly one token to represent each equal-info window. We’ll see in
    the next section that in EqualInfoAC settings with multiple tokens per window,
    any non-initial tokens are highly context-dependent, and learning proceeds on
    a curriculum from the “easy” window-initial tokens to the “harder” window-final
    tokens.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 6](#S5.T6 "在 5.3 大词汇量有助于提高压缩比 ‣ 5 额外实验 ‣ 在神经压缩文本上训练 LLMs") 显示即使在这种情况下，具有较大词汇量的模型也更强。^(16)^(16)16通过进一步增加令牌位深度可能会获得更多的收益。然而，大多数深度学习框架不支持使用无符号数据类型作为输入，导致的大词汇量可能会在最终的
    softmax 层造成计算瓶颈。事实上，*大多数* 位/字节的收益（$84$ 模型使用一个令牌精确表示每个等信息窗口。在下节中，我们将看到在具有多个令牌的
    EqualInfoAC 设置中，任何非初始令牌都高度依赖上下文，学习过程是从“简单”的窗口初始令牌到“更难”的窗口最终令牌的课程。'
- en: 'Table 6: Most of the gain of increasing vocabulary from $256$k remains even
    in the “byte matched” setting, where the models train over the same number of
    raw bytes. Performance gains seen between settings are all statistically significant.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：即使在“字节匹配”设置中，从 $256$k 增加词汇量的收益大部分仍然存在，其中模型训练使用相同数量的原始字节。设置之间的性能增益均具有统计显著性。
- en: '| Tokenization | Comparison | Bits/Byte |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 标记化 | 比较 | 位/字节 |'
- en: '| --- | --- | --- |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$ |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$ |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$ |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$ |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$ |'
- en: 6 Analysis
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 分析
- en: In this section we examine how neural compression based tokenizers differ from
    standard tokenizers, and conduct additional analysis on training dynamics and
    learnability of compressed data. This analysis leads us to several recommendations
    for future work developing new compression schemes that aim to be learnable by
    transformer models while delivering stronger compression than subword tokenizers.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们将检查神经压缩基础的标记器如何与标准标记器不同，并对压缩数据的训练动态和可学习性进行额外分析。这项分析为我们提出了几个未来工作的建议，以开发新的压缩方案，这些方案旨在被变换器模型学习，同时提供比子词标记器更强的压缩效果。
- en: 6.1 EqualInfoAC is less stable and less semantic than SentencePiece
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 EqualInfoAC 比 SentencePiece 更不稳定且语义性更差
- en: 'Table 7: Comparing tokenization under SentencePiece vs. EqualInfoAC. SentencePiece
    gives a fairly stable text $\rightarrow$ is less stable and less semantic. Each
    occurrence of “elephants” maps to different tokens, and most tokens fail to align
    with meaningful linguistic boundaries (e.g., word or morpheme).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：比较 SentencePiece 和 EqualInfoAC 下的标记化。SentencePiece 提供了相对稳定的文本 $\rightarrow$
    更不稳定且语义性更差。每次出现的“大象”映射到不同的令牌，大多数令牌未能与有意义的语言边界（例如，词或语素）对齐。
- en: '| Input Text | The three currently living species are: African savanna elephants,
    African forest elephants, and the Asian elephants. |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 输入文本 | 三种目前存在的物种是：非洲草原大象、非洲森林大象和亚洲大象。 |'
- en: '| --- | --- |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| SentencePiece Tokens | [The] [ three] [ currently] [ living] [ species] [ are]
    [:] [ African] [ ] [s] [a] [v] [anna] [ elephant] [s] [,] [ African] [forest]
    [ elephant] [s] [,] [ and] [ the] [ Asian] [ elephant] [s] [.] |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| SentencePiece 令牌 | [这] [ 三] [ 种] [ 目前] [ 存在的] [ 物种] [ 是] [:] [ 非洲] [ ] [的]
    [草原] [大象] [,] [ 非洲] [ 森林] [ 大象] [,] [ 以及] [ 亚洲] [ 大象] [ 。] |'
- en: '| EqualInfoAC [b=16, v=65k] Tokens | [The th] [ree c] [urrently l] [iving ]
    [species] [ are] [: A] [frica] [n sav] [anna] [ ele] [pha] [nts, ] [Afr] [ican ]
    [forest ] [eleph] [ants, ] [and the ] [Asi] [an e] [lep] [hant] [s.] |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC [b=16, v=65k] 标记 | [The th] [ree c] [urrently l] [iving ] [species]
    [ are] [: A] [frica] [n sav] [anna] [ ele] [pha] [nts, ] [Afr] [ican ] [forest
    ] [eleph] [ants, ] [and the ] [Asi] [an e] [lep] [hant] [s.] |'
- en: While the performance of our EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$
    model approaches that of our SentencePiece baseline, qualitative analysis shows
    that the two tokenization schemes differ in many regards. [Table 7](#S6.T7 "In
    6.1 EqualInfoAC is less stable and less semantic than SentencePiece ‣ 6 Analysis
    ‣ Training LLMs over Neurally Compressed Text") illustrates some of these differences.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的 EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$ 模型的表现接近于 SentencePiece
    基准，但定性分析表明，这两种分词方案在许多方面存在差异。[表7](#S6.T7 "在6.1中 EqualInfoAC 比 SentencePiece 更不稳定且语义更少
    ‣ 6 分析 ‣ 基于神经压缩文本训练 LLM") 说明了这些差异的一些例子。
- en: 'First, we observe that SentencePiece produces a relatively stable text $\rightarrow$ token
    mapping.^(17)^(17)17See [Appendix L](#A12 "Appendix L Corner Cases of Tokenization
    lead to Unstable Mappings ‣ Training LLMs over Neurally Compressed Text") for
    some corner cases where this is not the case. For example, “elephants” appears
    three times in the sentence, and maps stably to the same two-token sequence in
    all cases: [ elephant] [s]. Similarly, both occurrences of “African” map to the
    same token: [ African]. By comparison, the EqualInfoAC tokenization is relatively
    unstable, with each occurrence of these words being segmented in a different way
    and yielding a different token sequence.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们观察到 SentencePiece 产生了相对稳定的文本$\rightarrow$标记映射。^(17)^(17)17有关此情况的某些角落案例，请参见
    [附录 L](#A12 "附录 L 分词的角落案例导致不稳定的映射 ‣ 基于神经压缩文本训练 LLM")。例如，“elephants” 在句子中出现三次，并在所有情况下稳定地映射到相同的两个标记序列：[
    elephant] [s]。类似地，“African”的两个出现都映射到相同的标记：[ African]。相比之下，EqualInfoAC 分词相对不稳定，每次出现这些词的方式不同，并产生不同的标记序列。
- en: Second, we find that the SentencePiece tokenization is more “semantic”, by which
    we mean that the segmentation it induces aligns better with meaningful linguistic
    units—words and morphemes. While there are some exceptions, e.g. “savanna” being
    parsed as [s] [a] [v] [anna], the more common case is whole words being parsed
    as single tokens (e.g., currently), or into meaningful morphemes (e.g., elephant-s).
    By comparison, EqualInfoAC tokenization appears to almost entirely disregard word
    and morpheme boundaries. As one example, we see “Asian elephants.” parsed as [Asi] [an e] [lep] [hant] [s.].
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们发现 SentencePiece 分词更具“语义性”，即其划分更好地对齐了有意义的语言单位——词汇和语素。虽然存在一些例外，例如“savanna”被解析为
    [s] [a] [v] [anna]，但更常见的情况是整个词被解析为单个标记（例如 currently），或被划分为有意义的语素（例如 elephant-s）。相比之下，EqualInfoAC
    分词似乎几乎完全忽视了词汇和语素边界。例如，我们看到“Asian elephants.” 被解析为 [Asi] [an e] [lep] [hant] [s.]。
- en: Despite these differences, there is an important *similarity* between SentencePiece
    and EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$, will always map to
    the same output text. This “transparent decoding” property likely makes it easier
    for a downstream model to learn over these tokens.^(18)^(18)18Padding to reach
    a specific window size can require extra computation to discern between padding
    and characters that compress to all zeros, however we find in [Appendix I](#A9
    "Appendix I M2 Can Handle Padding Zeros at the End of a Window ‣ Training LLMs
    over Neurally Compressed Text") that it is not an issue for M2 models.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些差异，SentencePiece 和 EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$
    之间存在一个重要的*相似性*，即它们将始终映射到相同的输出文本。这种“透明解码”属性可能使下游模型更容易学习这些标记。^(18)^(18)18为了达到特定的窗口大小，填充可能需要额外的计算来区分填充和压缩为全零的字符，然而我们在[附录
    I](#A9 "附录 I M2 可以处理窗口末尾的填充零 ‣ 基于神经压缩文本训练 LLM") 中发现这对于 M2 模型不是问题。
- en: 'Table 8: Window-initial tokens have stable token $\rightarrow$ and show the
    full window text in a random sample of cases where a specific token appears at
    the first or second position within the window.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：窗口初始标记具有稳定的标记$\rightarrow$，并展示了在窗口内特定标记出现在第一个或第二个位置的随机样本中的完整窗口文本。
- en: '| Token | Window Position | Window Text |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | 窗口位置 | 窗口文本 |'
- en: '| --- | --- | --- |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $151$ | [lew ] / [lea] / [led] / [len] / [less] / [led] / [les] / [lew ]
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| $151$ | [lew ] / [lea] / [led] / [len] / [less] / [led] / [les] / [lew ]
    |'
- en: '|  | $2$ | [thoug] / [ust] / [ this] / [etti] / [npo] / [thoug] / [ un] / [imag]
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$ | [thoug] / [ust] / [ this] / [etti] / [npo] / [thoug] / [ un] / [imag]
    |'
- en: '| $185$ | [ord a] / [or k] / [ord] / [or f] / [or al] / [or a ] / [ore i] /
    [ora] |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| $185$ | [ord a] / [or k] / [ord] / [or f] / [or al] / [or a ] / [ore i] /
    [ora] |'
- en: '|  | $2$ | [ery] / [s may] / [cian] / [onte] / [h de] / [cri] / [opp] / [ides]
    |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$ | [ery] / [s may] / [cian] / [onte] / [h de] / [cri] / [opp] / [ides]
    |'
- en: When we move to versions of EqualInfoAC that contain *multiple* tokens per window,
    such as EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$ always maps to the prefix
    “le-”. However, when occurring as the *second* token within a two-token window,
    there are no apparent correspondences between window text.^(19)^(19)19A repeated
    text substring that happens to be aligned with a window multiple times is one
    of the few cases where the second token will represent the same text. As EqualInfoAC
    window length increases, the proportion of tokens that are stable decreases. This
    may explain the observed difficulty of learning over longer windows. The window
    text for all instances of these tokens can be seen in [Appendix M](#A13 "Appendix
    M Window Text Patterns and Token Positions ‣ Training LLMs over Neurally Compressed
    Text").
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们转到包含*多个*标记的EqualInfoAC版本时，例如EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$始终映射到前缀“le-”。然而，当作为两标记窗口中的*第二*个标记出现时，窗口文本之间没有明显的对应关系。^(19)^(19)19A
    一个与窗口对齐多次的重复文本子串是少数几种情况下第二个标记将表示相同文本的情况之一。随着EqualInfoAC窗口长度的增加，稳定标记的比例减少。这可能解释了在较长窗口上学习的难度。所有这些标记的窗口文本可以在[附录M](#A13
    "附录 M 窗口文本模式和标记位置 ‣ 基于神经压缩文本的LLM训练")中查看。
- en: '![Refer to caption](img/0e379f70eb19ceebfdb928959017c3d5.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0e379f70eb19ceebfdb928959017c3d5.png)'
- en: 'Figure 7: An illustration of the mapping between characters (bottom), bits
    (middle) and tokens (top) in the EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$-bit
    token tends to only cover one or two characters.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$-位标记在字符（底部）、位（中间）和标记（顶部）之间映射的示意图，通常只覆盖一到两个字符。
- en: Note that [Table 8](#S6.T8 "In 6.1 EqualInfoAC is less stable and less semantic
    than SentencePiece ‣ 6 Analysis ‣ Training LLMs over Neurally Compressed Text")
    examines window $\rightarrow$-bit tokens.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，[表8](#S6.T8 "在 6.1 中 EqualInfoAC 比 SentencePiece 更不稳定和语义更少 ‣ 6 分析 ‣ 基于神经压缩文本的LLM训练")
    研究了窗口 $\rightarrow$-位标记。
- en: '[Fig. 7](#S6.F7 "In 6.1 EqualInfoAC is less stable and less semantic than SentencePiece
    ‣ 6 Analysis ‣ Training LLMs over Neurally Compressed Text") also highlights that
    window-initial characters are not being well compressed, with the window-initial
    token often only covering one or two characters. This is due to our EqualInfoAC
    procedure fully resetting M1’s context at every window boundary. With no context,
    M1 cannot make confident predictions, leading to more bits being needed to represent
    the initial character. On the positive side, this setup guarantees that a window
    can be decoded in isolation, which should aid learning. However it is worth exploring
    in future work whether maintaining some M1 context across windows could improve
    the compression ratio without hurting learnability.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7](#S6.F7 "在 6.1 中 EqualInfoAC 比 SentencePiece 更不稳定和语义更少 ‣ 6 分析 ‣ 基于神经压缩文本的LLM训练")
    还突出了窗口初始字符没有很好地压缩，窗口初始标记通常只覆盖一到两个字符。这是由于我们的EqualInfoAC程序在每个窗口边界完全重置M1的上下文。没有上下文，M1无法做出自信的预测，从而需要更多的位来表示初始字符。积极的一面是，这种设置确保了窗口可以孤立解码，这应有助于学习。然而，值得在未来的工作中探讨是否在窗口之间保持一些M1上下文可以在不影响学习性的情况下改善压缩比。'
- en: 6.2 AC decoding is learned step-by-step
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 AC解码是逐步学习的
- en: '![Refer to caption](img/95f798f8c3c6d93b90eb97d3099b85c7.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/95f798f8c3c6d93b90eb97d3099b85c7.png)'
- en: (a) Accuracy per token position
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 标记位置的准确度
- en: '![Refer to caption](img/621c9c433646ebbd6d1f7694fd9ec16f.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/621c9c433646ebbd6d1f7694fd9ec16f.png)'
- en: (b) Increase over “trivial” accuracy per token position
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 相对于“琐碎”标记位置的准确度提升
- en: 'Figure 8: Earlier tokens within the $8$ bits.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：$8$位中的早期标记。
- en: As Arithmetic Coding is a sequential (left-to-right) and contextual algorithm,
    the text represented by a given token will differ based on the previous token.
    As such, a model should perform better on a token if it has a strong understanding
    of the token before it. When using EqualInfoAC compression, each window represents
    an independent Arithmetic Coding document. As we move deeper into the window,
    more and more AC decompression must be done to understand the token.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 由于算术编码是一种顺序（从左到右）且具有上下文的算法，给定的令牌所表示的文本会根据前一个令牌而有所不同。因此，如果模型对前一个令牌有较强的理解，那么对当前令牌的表现应该会更好。在使用EqualInfoAC压缩时，每个窗口表示一个独立的算术编码文档。随着我们进入窗口的深处，需要进行越来越多的AC解码才能理解令牌。
- en: To understand how a token’s position within a window affects learning, we track
    across training the average accuracy at each position within the $8$ steps of
    training. Looking at accuracy increase highlights the “sequential learning” trend
    by discounting any part of accuracy that is text independent. In particular, we
    note that window-final tokens have a non-uniform distribution due to the use of
    window-final padding bits (see our EqualInfoAC formulation in [Section 3.3](#S3.SS3
    "3.3 Compression Methods ‣ 3 Methods ‣ Training LLMs over Neurally Compressed
    Text")), which can be learned without any understanding of the text.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解令牌在窗口中的位置如何影响学习，我们跟踪了训练过程中每个位置的平均准确性，覆盖了$8$个训练步骤。观察准确性的增加可以突出“顺序学习”的趋势，排除任何与文本无关的准确性部分。特别是，我们注意到由于窗口最终填充位的使用（见我们在[第3.3节](#S3.SS3
    "3.3 压缩方法 ‣ 3 方法 ‣ 神经压缩文本中的LLM训练")的EqualInfoAC公式），窗口末尾的令牌具有非均匀分布，这些令牌可以在没有任何文本理解的情况下被学习。
- en: 'We observe two interesting trends. First, there is a clear ordering as to when
    the model starts to make meaningful (non-trivial) progress on a given position.
    The initial token (#1) is learned first, followed fairly quickly by #2 and then
    #3\. Later tokens are only “unlocked” after $10{,}000$ training steps, suggesting
    that the ability to model these tokens builds on a foundation of understanding
    the preceding tokens within the window.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到两个有趣的趋势。首先，模型开始在给定位置上取得有意义（非平凡）进展的时间有一个明确的顺序。最初的令牌（#1）首先被学习，随后是#2，然后是#3。较后的令牌只有在$10{,}000$训练步骤之后才会被“解锁”，这表明建模这些令牌的能力建立在对窗口内前置令牌的理解基础上。
- en: 'The second trend concerns the accuracy reached at each position. Here, we observe
    an increase in accuracy from #1 < #2 < #3, followed by a decrease from #3 < #4
    < #5 and so on.^(22)^(22)22The final token #8 also fits this trend when looking
    at the increase over non-trivial accuracy. The raw accuracy in this position is
    higher than previous tokens #4–7, due to the skewed distribution introduced by
    window-final padding. We interpret the increase across the first three positions
    as due to the benefit of extra leftward context. This is akin to the initial byte
    in a word being harder to predict than the following bytes. The decreasing performance
    at tokens #4 and beyond suggests the model is unable to track AC decompression
    indefinitely. While the model clearly learns to decompress longer sequences as
    training progresses, reliably decoding past $32$ bits of AC output appears to
    be a challenge.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '第二个趋势涉及到每个位置达到的准确性。在这里，我们观察到准确性从#1 < #2 < #3增加，然后从#3 < #4 < #5等下降。^(22)^(22)22
    最终令牌#8在查看非平凡准确性增加时也符合这一趋势。这个位置的原始准确性高于之前的令牌#4–7，原因是窗口末尾填充引入的偏斜分布。我们将前三个位置的准确性增加解释为额外左侧上下文的好处。这类似于一个词中的初始字节比后续字节更难预测。令牌#4及以后位置的性能下降表明模型无法无限期跟踪AC解码。虽然模型随着训练的进展明显学会了解码更长的序列，但可靠地解码超过$32$位的AC输出似乎是一项挑战。'
- en: 6.3 Learnable distributions are less uniform
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 可学习分布的均匀性较差
- en: '![Refer to caption](img/8c0a973290d92fa7adb5d95c9a385967.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8c0a973290d92fa7adb5d95c9a385967.png)'
- en: (a) *bit* n-grams counting all overlapping occurrences
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: (a) *位* n-grams 计算所有重叠出现的情况
- en: '![Refer to caption](img/889f46c68ac7320752c940db1b171136.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/889f46c68ac7320752c940db1b171136.png)'
- en: (b) n-bit *tokens* following our M2 tokenization
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 我们的M2分词法中的n-bit *令牌*
- en: 'Figure 9: As the bitstream is grouped into larger units, the empirical distribution
    moves away from uniform. We plot KL divergence of observed n-gram distributions
    from the uniform distribution, across various n-gram sizes. While AC compressed
    data would be difficult to distinguish from random data, we find there are still
    patterns to capture when using other compression schemes, particularly for GZip
    and shorter EqualInfoAC windows. Compared to the left plot, we find that the tokenized
    bitstream (see [Section 3.4](#S3.SS4 "3.4 Tokenization of Compressed Text ‣ 3
    Methods ‣ Training LLMs over Neurally Compressed Text")) has even more information
    for M2 to capture.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：随着比特流被分组为更大的单元，经验分布逐渐远离均匀分布。我们绘制了观察到的n-gram分布与均匀分布之间的KL散度，涵盖各种n-gram大小。尽管AC压缩数据难以与随机数据区分，但我们发现使用其他压缩方案（特别是GZip和较短的EqualInfoAC窗口）时，仍然存在可以捕获的模式。与左侧的图相比，我们发现标记化后的比特流（参见[第3.4节](#S3.SS4
    "3.4 节压缩文本的标记化 ‣ 3 种方法 ‣ 在神经压缩文本上训练LLMs")）为M2提供了更多的信息。
- en: A well-known result in the compression literature is that there can be no recursive
    compression [[45](#bib.bib45)]. The compression algorithm removes information
    captured by its model, resulting in a uniform output that appears random to the
    original model. However, our setting is not recursive compression. Instead, a
    separate and larger model is trained on the compressed output, which should be
    able to capture new patterns in the bitstream.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩文献中的一个著名结果是不存在递归压缩[[45](#bib.bib45)]。压缩算法移除了模型捕获的信息，导致输出均匀，看起来对原始模型是随机的。然而，我们的设置不是递归压缩。相反，一个单独且更大的模型在压缩输出上进行训练，应该能够捕获比特流中的新模式。
- en: Despite this, the output of compression using M1 appears very uniform, as evidenced
    by the minimal gains from modeling the unigram token distribution in [Table 3](#S3.T3
    "In 3.4 Tokenization of Compressed Text ‣ 3 Methods ‣ Training LLMs over Neurally
    Compressed Text"). Therefore, it seems reasonable that this uniformity could make
    it hard for M2 to learn (as all patterns must be contextual). We investigate this
    by plotting the KL divergence [[39](#bib.bib39)] between the observed empirical
    distribution and a uniform distribution for different segmentations of the bitstream.
    If the underlying distribution of bits was truly random and independent, then
    the distribution of unigrams for some bitstream segmentation should remain uniform
    as $p(b_{i},\dots,b_{i+n})=\prod_{j=i}^{i+n}(p(b_{j}))$.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，使用M1进行压缩的输出看起来非常均匀，这从[表3](#S3.T3 "在3.4 节压缩文本的标记化 ‣ 3 种方法 ‣ 在神经压缩文本上训练LLMs")中对单字
    token 分布建模获得的最小增益可以看出。因此，这种均匀性使得M2学习变得困难（因为所有模式必须是上下文相关的）似乎是合理的。我们通过绘制观察到的经验分布与均匀分布之间的KL散度[[39](#bib.bib39)]，来探讨这个问题，针对不同的比特流分段。如果比特的底层分布确实是随机和独立的，那么某些比特流分段的单字分布应该保持均匀，因为$p(b_{i},\dots,b_{i+n})=\prod_{j=i}^{i+n}(p(b_{j}))$。
- en: We segment the bitstream either into bit n-grams, where successive n-grams are
    allowed to overlap, or into n-bit tokens, following our M2 tokenization procedure—see
    [Section 3.4](#S3.SS4 "3.4 Tokenization of Compressed Text ‣ 3 Methods ‣ Training
    LLMs over Neurally Compressed Text"). We only plot tokenization into $n$ setting.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将比特流分段为比特n-grams，其中连续的n-grams可以重叠，或分段为n-bit tokens，按照我们的M2标记化程序——参见[第3.4节](#S3.SS4
    "3.4 节压缩文本的标记化 ‣ 3 种方法 ‣ 在神经压缩文本上训练LLMs")。我们仅绘制了$n$设置的标记化。
- en: As a baseline, we used the cryptographic secrets package in Python to generate
    bitstreams that should be truly random and independent. As such, the KL divergence
    should remain at $0$. This holds, but it is obfuscated by the log scaling in [Fig. 9](#S6.F9
    "In 6.3 Learnable distributions are less uniform ‣ 6 Analysis ‣ Training LLMs
    over Neurally Compressed Text"). In fact, the magnitude of the noise for settings
    such as GZip and EqualInfoAC is larger than for AC or RNG. This noise behavior
    is seen in [Fig. 12](#A10.F12 "In Appendix J Entropy Estimation ‣ Training LLMs
    over Neurally Compressed Text"). See [Appendix J](#A10 "Appendix J Entropy Estimation
    ‣ Training LLMs over Neurally Compressed Text") for more information on entropy
    estimation and bias correction.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基准，我们使用Python中的加密秘密包生成应该是真正随机和独立的比特流。因此，KL散度应该保持在$0$。这确实成立，但在[图9](#S6.F9 "6.3
    学习分布不均匀 ‣ 6 分析 ‣ 在神经压缩文本上训练LLMs")中被对数缩放所掩盖。事实上，GZip和EqualInfoAC等设置的噪声幅度大于AC或RNG。这种噪声行为在[图12](#A10.F12
    "附录J 熵估计 ‣ 在神经压缩文本上训练LLMs")中可以看到。有关熵估计和偏差校正的更多信息，请参见[附录J](#A10 "附录J 熵估计 ‣ 在神经压缩文本上训练LLMs")。
- en: The AC and RNG lines in [Fig. 9](#S6.F9 "In 6.3 Learnable distributions are
    less uniform ‣ 6 Analysis ‣ Training LLMs over Neurally Compressed Text") are
    very similar and their sampling noise intervals have large overlaps. This suggests
    that the data generated by AC compression with M1 is difficult to distinguish
    from random data.^(24)^(24)24For 
    decimal places. This is a possible explanation for why M2 models trained on AC
    data only learn to output a uniform distribution, as seen in [Fig. 3](#S4.F3 "In
    4 Results ‣ Training LLMs over Neurally Compressed Text").
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9](#S6.F9 "6.3 学习分布不均匀 ‣ 6 分析 ‣ 在神经压缩文本上训练LLMs")中的AC和RNG线非常相似，它们的采样噪声区间有很大的重叠。这表明AC压缩与M1生成的数据难以与随机数据区分。^(24)^(24)24For
     decimal places。这可能解释了为什么在AC数据上训练的M2模型只学习输出均匀分布，如[图3](#S4.F3
    "4 结果 ‣ 在神经压缩文本上训练LLMs")中所见。'
- en: In [Fig. 9](#S6.F9 "In 6.3 Learnable distributions are less uniform ‣ 6 Analysis
    ‣ Training LLMs over Neurally Compressed Text"), we see that GZip is the least
    uniform, which is expected as it has the worst compression rate among these settings.
    However, the segmentation into tokens does not result in much extra information.
    This is again suggestive that the differences between the “coding” components
    of GZip and Arithmetic Coding are important for learnability. It is also a possible
    explanation of why GZip is the one setting where using $16$-bit tokens does not
    improve performance.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图9](#S6.F9 "6.3 学习分布不均匀 ‣ 6 分析 ‣ 在神经压缩文本上训练LLMs")中，我们看到GZip是不均匀性最差的，这很正常，因为它在这些设置中具有最差的压缩率。然而，将文本分割成令牌并未带来太多额外的信息。这再次表明GZip和算术编码的“编码”组件之间的差异对可学习性至关重要。这也是GZip是唯一一个使用$16$-bit令牌未能提高性能的设置的可能解释。
- en: Similarly, [Fig. 9](#S6.F9 "In 6.3 Learnable distributions are less uniform
    ‣ 6 Analysis ‣ Training LLMs over Neurally Compressed Text") shows that EqualInfoAC$[b\mathord{=}16]$,
    suggesting that weakening the “coding” component of Arithmetic Coding is a more
    effective way to retain information and increase learnability for M2.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，[图9](#S6.F9 "6.3 学习分布不均匀 ‣ 6 分析 ‣ 在神经压缩文本上训练LLMs") 显示了EqualInfoAC$[b\mathord{=}16]$，这表明削弱算术编码的“编码”组件是保留信息和提高M2可学习性的更有效方法。
- en: 7 Conclusion
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: We have shown there is promise in the idea of training LLMs over neural-compressed
    text. In the best case, this will allow training over text that is better compressed
    than standard subword token sequences, while maintaining learnability. This an
    appealing prospect, as models that read and write more text per token are more
    efficient to train and serve, and can model longer dependencies.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了在神经压缩文本上训练LLMs的想法具有前景。在最佳情况下，这将允许在比标准子词令牌序列更好压缩的文本上进行训练，同时保持可学习性。这是一个有吸引力的前景，因为每个令牌读取和写入更多文本的模型在训练和服务上更高效，并且可以建模更长的依赖关系。
- en: While the “very simplest” approach does not work (training directly over a tokenized
    AC-encoded bitstream), we showed that a relatively simple modification—compression
    via Equal Info Windows—already brings us within striking distance of popular tokenizers.
    When measured in terms of perplexity achievable at fixed inference cost (FLOPs/byte),
    we find that our method outperforms raw byte-level models, and comes increasingly
    close to the performance of SentencePiece tokenization as scale increases to $2$
    billion parameters.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管“最简单”的方法（直接在标记化的 AC 编码比特流上训练）不起作用，我们展示了相对简单的修改——通过 Equal Info Windows 进行压缩——已经让我们接近流行的标记化工具。根据在固定推断成本（FLOPs/byte）下可达到的困惑度测量，我们发现我们的方法优于原始字节级模型，并且随着规模增加到
    $2$ 亿参数，我们的方法越来越接近 SentencePiece 标记化的性能。
- en: While bespoke compression methods have developed around different modalities
    (e.g., text, audio, images, video) and different applications (e.g., delta-of-delta
    for regular repeating timestamps [[50](#bib.bib50)]), to our knowledge, no efficient
    compression methods have been designed specifically for use as LLM tokenizers.
    We are optimistic that future work will create such methods. Compared to today’s
    subword tokenizers, we expect these methods (i) will deliver higher compression
    rates, (ii) will come closer to equal information per token, thus allocating compute
    more effectively, and (iii) will give models a more direct view of the underlying
    raw text, thus helping on spelling and pronunciation tasks. As a tradeoff, we
    expect these neural tokenizers will be *somewhat* less stable in their text $\leftrightarrow$ token
    mapping, but perhaps not so unstable as our approach here. In particular, we think
    it is worth exploring methods under which a given word typically maps to a relatively
    small number (tens not thousands) of relatable token sequences.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管针对不同的模态（例如，文本、音频、图像、视频）和不同的应用（例如，针对常规重复时间戳的 delta-of-delta [[50](#bib.bib50)]）已经发展出了定制的压缩方法，但据我们所知，还没有专门为
    LLM 标记化设计的高效压缩方法。我们对未来的工作能够创造出这样的方案持乐观态度。与当前的子词标记化工具相比，我们预计这些方法（i）将提供更高的压缩率，（ii）将更接近每个标记的等量信息，从而更有效地分配计算资源，以及（iii）将使模型对底层原始文本有更直接的视角，从而在拼写和发音任务上提供帮助。作为权衡，我们预计这些神经标记器在文本 $\leftrightarrow$ 标记映射上将*有些*不稳定，但可能不会比我们这里的方法更不稳定。特别是，我们认为值得探索的方法是使给定单词通常映射到相对较少的（几十而不是几千）相关标记序列。
- en: One direction we left unexplored is the idea of passing information between
    the compressing model (M1) and the LLM trained over compressed text (M2). Some
    additional signal of M1’s internal state or output may be helpful for M2 to accurately
    simulate M1, which is a prerequisite to flawlessly encoding and decoding M1-compressed
    text.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未探索的一个方向是传递信息的想法，在压缩模型（M1）和基于压缩文本训练的 LLM（M2）之间。一些关于 M1 内部状态或输出的额外信号可能对 M2
    准确模拟 M1 是有帮助的，这是一致地编码和解码 M1 压缩文本的先决条件。
- en: For hill-climbing in this space, we found it useful to iterate on the sequence-to-sequence
    sub-tasks of compression and decompression, which should, in theory, be learnable
    with high accuracy. Specifically, if future work can devise a strong (~$10\times$)
    compressor that a transformer can be trained to accurately encode and decode,
    we expect that this will be an ideal candidate for tokenizing text for LLMs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在此领域的爬山算法，我们发现迭代压缩和解压缩的序列到序列子任务是有用的，这在理论上应该可以高精度地学习。具体来说，如果未来的工作能够设计出一个强大的（~$10\times$）压缩器，使得变压器可以被训练以准确地编码和解码，我们预计这将是一个理想的文本标记化候选者。
- en: Acknowledgements
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank Ben Adlam, Grégoire Delétang, Rosanne Liu, and Colin
    Raffel for detailed comments on an earlier draft. We’re also grateful to Peter
    Liu, both for helpful discussion, as well as for building some of the infrastructure
    that made our experiments easier to run. Finally, we thank Doug Eck, Noah Fiedel,
    and the PAGI team for ongoing guidance and feedback.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢 Ben Adlam、Grégoire Delétang、Rosanne Liu 和 Colin Raffel 对早期草稿的详细评论。我们还感激
    Peter Liu，既感谢他提供的有益讨论，也感谢他为我们的实验提供了便利的基础设施。最后，我们感谢 Doug Eck、Noah Fiedel 和 PAGI
    团队的持续指导和反馈。
- en: References
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Ainslie, S. Ontanon, C. Alberti, V. Cvicek, Z. Fisher, P. Pham, A. Ravula,
    S. Sanghai, Q. Wang, and L. Yang. [ETC: Encoding Long and Structured Inputs in
    Transformers](https://aclanthology.org/2020.emnlp-main.19). In B. Webber, T. Cohn,
    Y. He, and Y. Liu, editors, Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing (EMNLP), pages 268–284, Online, Nov. 2020\. Association
    for Computational Linguistics.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Ainslie, S. Ontanon, C. Alberti, V. Cvicek, Z. Fisher, P. Pham, A. Ravula,
    S. Sanghai, Q. Wang, 和 L. Yang. [ETC: 在变换器中编码长而结构化的输入](https://aclanthology.org/2020.emnlp-main.19)。在
    B. Webber, T. Cohn, Y. He, 和 Y. Liu 主编的2020年自然语言处理实证方法会议论文集（EMNLP），第268–284页，在线，2020年11月。计算语言学协会。'
- en: '[2] R. Al-Rfou, D. Choe, N. Constant, M. Guo, and L. Jones. [Character-Level
    Language Modeling with Deeper Self-Attention](https://ojs.aaai.org/index.php/AAAI/article/view/4182).
    Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):3159–3166,
    Jul. 2019.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] R. Al-Rfou, D. Choe, N. Constant, M. Guo, 和 L. Jones. [字符级语言建模与更深层的自注意力](https://ojs.aaai.org/index.php/AAAI/article/view/4182)。人工智能会议论文集，33(01):3159–3166,
    2019年7月。'
- en: '[3] A. Baevski, H. Zhou, A. Mohamed, and M. Auli. [wav2vec 2.0: A Framework
    for Self-Supervised Learning of Speech Representations](https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf).
    In Proceedings of the 34th International Conference on Neural Information Processing
    Systems, NeurIPS’20, Red Hook, NY, USA, 2020\. Curran Associates Inc.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Baevski, H. Zhou, A. Mohamed, 和 M. Auli. [wav2vec 2.0: 自监督语音表示学习框架](https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf)。收录于第34届国际神经信息处理系统大会，NeurIPS’20，纽约州红钩，2020年。Curran
    Associates Inc.'
- en: '[4] J. Ballé, S. J. Hwang, and E. Agustsson. [TensorFlow Compression: Learned
    Data Compression](http://github.com/tensorflow/compression), 2024.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. Ballé, S. J. Hwang, 和 E. Agustsson. [TensorFlow Compression: 学习型数据压缩](http://github.com/tensorflow/compression)，2024年。'
- en: '[5] I. Beltagy, M. E. Peters, and A. Cohan. [Longformer: The Long-Document
    Transformer](http://arxiv.org/abs/2004.05150), Dec. 2020.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] I. Beltagy, M. E. Peters, 和 A. Cohan. [Longformer: 长文档变换器](http://arxiv.org/abs/2004.05150)，2020年12月。'
- en: '[6] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi,
    D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi, and N. Zeghidour. [AudioLM:
    A Language Modeling Approach to Audio Generation](https://arxiv.org/abs/2209.03143).
    IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:2523–2533,
    2023.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi,
    D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi, 和 N. Zeghidour. [AudioLM:
    一种音频生成的语言建模方法](https://arxiv.org/abs/2209.03143)。IEEE/ACM音频、语音和语言处理交易，31:2523–2533,
    2023年。'
- en: '[7] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin,
    G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. [JAX: composable
    transformations of Python+NumPy programs](http://github.com/google/jax), 2018.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin,
    G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, 和 Q. Zhang. [JAX: Python+NumPy程序的可组合转换](http://github.com/google/jax)，2018年。'
- en: '[8] P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L. Mercer. [The Mathematics
    of Statistical Machine Translation: Parameter Estimation](https://aclanthology.org/J93-2003/).
    Comput. Linguist., 19(2):263–311, jun 1993.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, 和 R. L. Mercer. [统计机器翻译的数学：参数估计](https://aclanthology.org/J93-2003/)。计算语言学，19(2):263–311,
    1993年6月。'
- en: '[9] R. Child, S. Gray, A. Radford, and I. Sutskever. [Generating Long Sequences
    with Sparse Transformers](http://arxiv.org/abs/1904.10509), Apr. 2019.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] R. Child, S. Gray, A. Radford, 和 I. Sutskever. [用稀疏变换器生成长序列](http://arxiv.org/abs/1904.10509)，2019年4月。'
- en: '[10] D. Choe, R. Al-Rfou, M. Guo, H. Lee, and N. Constant. [Bridging the Gap
    for Tokenizer-Free Language Models](http://arxiv.org/abs/1908.10322). CoRR, abs/1908.10322,
    2019.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] D. Choe, R. Al-Rfou, M. Guo, H. Lee, 和 N. Constant. [弥合无分词器语言模型的差距](http://arxiv.org/abs/1908.10322)。CoRR,
    abs/1908.10322, 2019年。'
- en: '[11] J. Chung, S. Ahn, and Y. Bengio. [Hierarchical Multiscale Recurrent Neural
    Networks](https://openreview.net/forum?id=S1di0sfgl). In International Conference
    on Learning Representations, 2017.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Chung, S. Ahn, 和 Y. Bengio. [层次多尺度递归神经网络](https://openreview.net/forum?id=S1di0sfgl)。国际表示学习大会，2017年。'
- en: '[12] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu.
    [w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised
    Speech Pre-Training](https://arxiv.org/abs/2108.06209). In 2021 IEEE Automatic
    Speech Recognition and Understanding Workshop (ASRU), pages 244–250, 2021.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, 和 Y. Wu. [w2v-BERT：结合对比学习和掩蔽语言建模的自监督语音预训练](https://arxiv.org/abs/2108.06209)。在2021年IEEE自动语音识别与理解研讨会（ASRU）上，页244–250，2021年。'
- en: '[13] J. H. Clark, D. Garrette, I. Turc, and J. Wieting. [Canine: Pre-training
    an Efficient Tokenization-Free Encoder for Language Representation](https://aclanthology.org/2022.tacl-1.5).
    Transactions of the Association for Computational Linguistics, 10:73–91, 2022.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. H. Clark, D. Garrette, I. Turc, 和 J. Wieting. [Canine：为语言表示预训练高效的无标记编码器](https://aclanthology.org/2022.tacl-1.5)。计算语言学协会会刊，10:73–91，2022年。'
- en: '[14] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, and R. Salakhutdinov. [Transformer-XL:
    Attentive Language Models beyond a Fixed-Length Context](https://aclanthology.org/P19-1285).
    In Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, pages 2978–2988, Florence, Italy, July 2019. Association for Computational
    Linguistics.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, 和 R. Salakhutdinov. [Transformer-XL：超越固定长度上下文的关注语言模型](https://aclanthology.org/P19-1285)。在第57届计算语言学协会年会论文集中，页2978–2988，意大利佛罗伦萨，2019年7月。计算语言学协会。'
- en: '[15] S. DeDeo, R. X. D. Hawkins, S. Klingenstein, and T. Hitchcock. [Bootstrap
    Methods for the Empirical Study of Decision-Making and Information Flows in Social
    Systems](https://www.mdpi.com/1099-4300/15/6/2246). Entropy. An International
    and Interdisciplinary Journal of Entropy and Information Studies, 15(6):2246–2276,
    2013.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. DeDeo, R. X. D. Hawkins, S. Klingenstein, 和 T. Hitchcock. [用于社会系统决策和信息流的实证研究的自助法](https://www.mdpi.com/1099-4300/15/6/2246)。熵：熵与信息研究的国际跨学科期刊，15(6):2246–2276，2013年。'
- en: '[16] G. Delétang, A. Ruoss, P.-A. Duquenne, E. Catt, T. Genewein, C. Mattern,
    J. Grau-Moya, L. K. Wenliang, M. Aitchison, L. Orseau, M. Hutter, and J. Veness.
    [Language Modeling Is Compression](https://arxiv.org/abs/2309.10668). In ICLR,
    2024.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] G. Delétang, A. Ruoss, P.-A. Duquenne, E. Catt, T. Genewein, C. Mattern,
    J. Grau-Moya, L. K. Wenliang, M. Aitchison, L. Orseau, M. Hutter, 和 J. Veness.
    [语言建模即压缩](https://arxiv.org/abs/2309.10668)。在ICLR，2024年。'
- en: '[17] P. Deutsch. [GZIP file format specification](https://www.rfc-editor.org/rfc/rfc1952.txt).
    RFC 1952, May 1996.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] P. Deutsch. [GZIP文件格式规范](https://www.rfc-editor.org/rfc/rfc1952.txt)。RFC
    1952，1996年5月。'
- en: '[18] P. Deutsch and J.-L. Gailly. [ZLIB Compressed Data Format Specification](https://www.rfc-editor.org/rfc/rfc1950.txt).
    RFC 1950, May 1996.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] P. Deutsch 和 J.-L. Gailly. [ZLIB压缩数据格式规范](https://www.rfc-editor.org/rfc/rfc1950.txt)。RFC
    1950，1996年5月。'
- en: '[19] J. Duda. [Asymmetric Numeral Systems: Entropy Coding Combining Speed of
    Huffman Coding with Compression Rate of Arithmetic Coding](http://arxiv.org/abs/1311.2540),
    Jan. 2014.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Duda. [非对称数字系统：结合哈夫曼编码的速度与算术编码的压缩率的熵编码](http://arxiv.org/abs/1311.2540)，2014年1月。'
- en: '[20] B. Efron. [Bootstrap Methods: Another Look at the Jackknife](http://www.jstor.org/stable/2958830).
    The Annals of Statistics, 7(1):1–26, 1979.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] B. Efron. [自助法：对杰克刀法的另一个视角](http://www.jstor.org/stable/2958830)。统计年鉴，7(1):1–26，1979年。'
- en: '[21] J. H. Friedman. [Greedy Function Approximation: A Gradient Boosting Machine](https://doi.org/10.1214/aos/1013203451).
    The Annals of Statistics, 29(5):1189–1232, 2001.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. H. Friedman. [贪婪函数逼近：一种梯度提升机器](https://doi.org/10.1214/aos/1013203451)。统计年鉴，29(5):1189–1232，2001年。'
- en: '[22] K. Fukushima. [Cognitron: A Self-Organizing Multilayered Neural Network](https://link.springer.com/article/10.1007/BF00342633).
    Biological Cybernetics, 20:121–136, 1975.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] K. Fukushima. [Cognitron: 自组织多层神经网络](https://link.springer.com/article/10.1007/BF00342633)。生物信息学，20:121–136，1975年。'
- en: '[23] P. Gage. [A New Algorithm for Data Compression](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM).
    C Users Journal, 12(2):23–38, 1994.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] P. Gage. [一种新的数据压缩算法](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)。C用户期刊，12(2):23–38，1994年。'
- en: '[24] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang,
    H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy. [The Pile: An 800GB Dataset
    of Diverse Text for Language Modeling](http://arxiv.org/abs/2101.00027), Dec.
    2020.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang,
    H. He, A. Thite, N. Nabeshima, S. Presser, 和 C. Leahy. [The Pile：用于语言建模的800GB多样化文本数据集](http://arxiv.org/abs/2101.00027)，2020年12月。'
- en: '[25] N. Godey, R. Castagné, É. de la Clergerie, and B. Sagot. [MANTa: Efficient
    Gradient-Based Tokenization for End-to-End Robust Language Modeling](https://aclanthology.org/2022.findings-emnlp.207).
    In Findings of the Association for Computational Linguistics: EMNLP 2022, pages
    2859–2870, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational
    Linguistics.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] N. Godey, R. Castagné, É. de la Clergerie, 和 B. Sagot. [MANTa：高效的基于梯度的标记化用于端到端鲁棒语言建模](https://aclanthology.org/2022.findings-emnlp.207)。在计算语言学协会发现：EMNLP
    2022的会议记录中，页2859–2870，阿布扎比，阿联酋，2022年12月。计算语言学协会。'
- en: '[26] O. Goldman, A. Caciularu, M. Eyal, K. Cao, I. Szpektor, and R. Tsarfaty.
    [Unpacking Tokenization: Evaluating Text Compression and its Correlation with
    Model Performance](http://arxiv.org/abs/2403.06265), Mar. 2024.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] O. Goldman, A. Caciularu, M. Eyal, K. Cao, I. Szpektor, 和 R. Tsarfaty.
    [解读标记化：评估文本压缩及其与模型性能的关联](http://arxiv.org/abs/2403.06265)，2024年3月。'
- en: '[27] A. Graves. [Adaptive Computation Time for Recurrent Neural Networks](http://arxiv.org/abs/1603.08983),
    Feb. 2017.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. Graves. [递归神经网络的自适应计算时间](http://arxiv.org/abs/1603.08983)，2017年2月。'
- en: '[28] J. Heek, A. Levskaya, A. Oliver, M. Ritter, B. Rondepierre, A. Steiner,
    and M. van Zee. [Flax: A neural network library and ecosystem for JAX](http://github.com/google/flax),
    2020.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. Heek, A. Levskaya, A. Oliver, M. Ritter, B. Rondepierre, A. Steiner,
    和 M. van Zee. [Flax：JAX的神经网络库和生态系统](http://github.com/google/flax)，2020年。'
- en: '[29] G. Hinton, O. Vinyals, and J. Dean. [Distilling the Knowledge in a Neural
    Network](http://arxiv.org/abs/1503.02531), Mar. 2015.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] G. Hinton, O. Vinyals, 和 J. Dean. [神经网络中的知识蒸馏](http://arxiv.org/abs/1503.02531)，2015年3月。'
- en: '[30] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford,
    D. de las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland,
    K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan,
    E. Elsen, O. Vinyals, J. W. Rae, and L. Sifre. [Training Compute-Optimal Large
    Language Models](https://openreview.net/forum?id=iBBcRUlOAPR). In Advances in
    Neural Information Processing Systems, 2022.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford,
    D. de las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland,
    K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan,
    E. Elsen, O. Vinyals, J. W. Rae, 和 L. Sifre. [训练计算最优的大型语言模型](https://openreview.net/forum?id=iBBcRUlOAPR)。在神经信息处理系统进展中，2022年。'
- en: '[31] P. G. Howard and J. S. Vitter. [Analysis of Arithmetic Coding for Data
    Compression](https://www.sciencedirect.com/science/article/pii/0306457392900669).
    Information Processing & Management, 28(6):749–763, 1992.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] P. G. Howard 和 J. S. Vitter. [数据压缩的算术编码分析](https://www.sciencedirect.com/science/article/pii/0306457392900669)。信息处理与管理，28(6):749–763，1992年。'
- en: '[32] D. A. Huffman. [A Method for the Construction of Minimum-Redundancy Codes](https://ieeexplore.ieee.org/document/4051119).
    Proceedings of the IRE, 40(9):1098–1101, 1952.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] D. A. Huffman. [构建最小冗余编码的方法](https://ieeexplore.ieee.org/document/4051119)。IRE会议录，40(9):1098–1101，1952年。'
- en: '[33] J. D. Hunter. [Matplotlib: A 2D Graphics Environment](https://ieeexplore.ieee.org/document/4160265).
    Computing in Science & Engineering, 9(3):90–95, 2007.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] J. D. Hunter. [Matplotlib: 二维图形环境](https://ieeexplore.ieee.org/document/4160265)。计算科学与工程，9(3):90–95，2007年。'
- en: '[34] Z. Jiang, M. Y. R. Yang, M. Tsirlin, R. Tang, and J. Lin. [Less is More:
    Parameter-Free Text Classification with Gzip](http://arxiv.org/abs/2212.09410),
    Dec. 2022.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Z. Jiang, M. Y. R. Yang, M. Tsirlin, R. Tang, 和 J. Lin. [少即是多：无参数文本分类与Gzip](http://arxiv.org/abs/2212.09410)，2022年12月。'
- en: '[35] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,
    S. Gray, A. Radford, J. Wu, and D. Amodei. [Scaling Laws for Neural Language Models](http://arxiv.org/abs/2001.08361),
    Jan. 2020.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,
    S. Gray, A. Radford, J. Wu, 和 D. Amodei. [神经语言模型的扩展定律](http://arxiv.org/abs/2001.08361)，2020年1月。'
- en: '[36] N. Kitaev, L. Kaiser, and A. Levskaya. [Reformer: The Efficient Transformer](https://openreview.net/forum?id=rkgNKkHtvB).
    In International Conference on Learning Representations, 2020.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] N. Kitaev, L. Kaiser, 和 A. Levskaya. [Reformer: 高效变换器](https://openreview.net/forum?id=rkgNKkHtvB)。在国际学习表征会议，2020年。'
- en: '[37] T. Kudo. [Subword Regularization: Improving Neural Network Translation
    Models with Multiple Subword Candidates](https://aclanthology.org/P18-1007). In
    Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers), pages 66–75, Melbourne, Australia, July 2018\. Association
    for Computational Linguistics.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] T. Kudo. [子词正则化：通过多子词候选改进神经网络翻译模型](https://aclanthology.org/P18-1007)。在第56届计算语言学协会年会（第1卷：长篇论文）的会议记录中，页66–75，澳大利亚墨尔本，2018年7月。计算语言学协会。'
- en: '[38] T. Kudo and J. Richardson. [SentencePiece: A Simple and Language Independent
    Subword Tokenizer and Detokenizer for Neural Text Processing](https://aclanthology.org/D18-2012).
    In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations, pages 66–71, Brussels, Belgium, Nov. 2018\.
    Association for Computational Linguistics.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] T. Kudo 和 J. Richardson. [SentencePiece: 一种简单且语言无关的子词分词器和去分词器](https://aclanthology.org/D18-2012)。收录于2018年自然语言处理实证方法会议：系统演示，页码66–71，比利时布鲁塞尔，2018年11月。计算语言学协会。'
- en: '[39] S. Kullback and R. A. Leibler. [On Information and Sufficiency](https://doi.org/10.1214/aoms/1177729694).
    The Annals of Mathematical Statistics, 22(1):79–86, 1951.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Kullback 和 R. A. Leibler. [信息与充分性](https://doi.org/10.1214/aoms/1177729694)。数学统计年鉴，22(1):79–86，1951年。'
- en: '[40] M. O. Külekci. [Compressed Context Modeling for Text Compression](https://ieeexplore.ieee.org/document/5749495).
    In 2011 Data Compression Conference, pages 373–382, 2011.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] M. O. Külekci. [用于文本压缩的压缩上下文建模](https://ieeexplore.ieee.org/document/5749495)。2011年数据压缩大会，页码373–382，2011年。'
- en: '[41] B. Lester, J. Yurtsever, S. Shakeri, and N. Constant. [Reducing Retraining
    by Recycling Parameter-Efficient Prompts](https://arxiv.org/abs/2208.05577). arXiv
    preprint arXiv:2208.05577, aug 2022.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] B. Lester, J. Yurtsever, S. Shakeri 和 N. Constant. [通过回收参数高效的提示减少再训练](https://arxiv.org/abs/2208.05577)。arXiv
    预印本 arXiv:2208.05577，2022年8月。'
- en: '[42] Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers
    via speculative decoding, 2023.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Leviathan, M. Kalman 和 Y. Matias. 通过推测解码实现快速推理，2023年。'
- en: '[43] Y. Leviathan, M. Kalman, and Y. Matias. [Fast Inference from Transformers
    via Speculative Decoding](https://proceedings.mlr.press/v202/leviathan23a.html).
    In Proceedings of the 40th International Conference on Machine Learning, volume
    202 of Proceedings of Machine Learning Research, pages 19274–19286\. PMLR, 23–29
    Jul 2023.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Y. Leviathan, M. Kalman 和 Y. Matias. [通过推测解码实现来自变换器的快速推理](https://proceedings.mlr.press/v202/leviathan23a.html)。收录于第40届国际机器学习大会，机器学习研究会议论文集第202卷，页码19274–19286。PMLR，2023年7月23–29日。'
- en: '[44] R. Liu, D. Garrette, C. Saharia, W. Chan, A. Roberts, S. Narang, I. Blok,
    R. Mical, M. Norouzi, and N. Constant. [Character-Aware Models Improve Visual
    Text Rendering](https://aclanthology.org/2023.acl-long.900). In Proceedings of
    the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pages 16270–16297, Toronto, Canada, July 2023\. Association for
    Computational Linguistics.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] R. Liu, D. Garrette, C. Saharia, W. Chan, A. Roberts, S. Narang, I. Blok,
    R. Mical, M. Norouzi 和 N. Constant. [字符感知模型改善视觉文本渲染](https://aclanthology.org/2023.acl-long.900)。收录于第61届计算语言学协会年会（第1卷：长篇论文），页码16270–16297，加拿大多伦多，2023年7月。计算语言学协会。'
- en: '[45] M. Mahoney. [Data Compression Explained](https://mattmahoney.net/dc/dce.html).
    2013.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] M. Mahoney. [数据压缩解释](https://mattmahoney.net/dc/dce.html)。2013年。'
- en: '[46] G. Miller. [Note on the Bias of Information Estimates](https://www.scienceopen.com/document?vid=357d299f-62fa-4bda-8dd2-e4d5b5abde5d).
    In Information Theory in Psychology: Problems and Methods, pages 95–100\. Free
    Press, 1955.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] G. Miller. [关于信息估计偏差的说明](https://www.scienceopen.com/document?vid=357d299f-62fa-4bda-8dd2-e4d5b5abde5d)。收录于《心理学中的信息理论：问题与方法》，页码95–100。自由出版社，1955年。'
- en: '[47] V. Nair and G. E. Hinton. [Rectified Linear Units Improve Restricted Boltzmann
    Machines](https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf). In Proceedings
    of the 27th International Conference on International Conference on Machine Learning,
    ICML’10, pages 807–814, Madison, WI, USA, 2010\. Omnipress.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] V. Nair 和 G. E. Hinton. [整流线性单元改善限制玻尔兹曼机](https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)。收录于第27届国际机器学习大会（ICML’10）会议论文集，页码807–814，2010年，威斯康星州麦迪逊，USA。Omnipress。'
- en: '[48] L. Paninski. [Estimation of Entropy and Mutual Information](https://doi.org/10.1162/089976603321780272).
    Neural Computation, 15(6):1191–1253, June 2003.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] L. Paninski. [熵和互信息的估计](https://doi.org/10.1162/089976603321780272)。神经计算，15(6):1191–1253，2003年6月。'
- en: '[49] R. Pasco. [Source Coding Algorithms for Fast Data Compression (Ph.D. Thesis
    Abstr.)](https://ieeexplore.ieee.org/document/1055739). IEEE Transactions on Information
    Theory, 23(4):548–548, 1977.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] R. Pasco. [用于快速数据压缩的源编码算法（博士论文摘要）](https://ieeexplore.ieee.org/document/1055739)。IEEE
    信息理论汇刊，23(4):548–548，1977年。'
- en: '[50] T. Pelkonen, S. Franklin, J. Teller, P. Cavallaro, Q. Huang, J. Meza,
    and K. Veeraraghavan. [Gorilla: a Fast, Scalable, In-Memory Time Series Database](https://doi.org/10.14778/2824032.2824078).
    Proc. VLDB Endow., 8(12):1816–1827, aug 2015.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] T. Pelkonen, S. Franklin, J. Teller, P. Cavallaro, Q. Huang, J. Meza,
    和 K. Veeraraghavan. [Gorilla：一个快速、可扩展的内存时间序列数据库](https://doi.org/10.14778/2824032.2824078)。《VLDB
    会议论文集》，8(12):1816–1827, 2015年8月。'
- en: '[51] O. Press, N. Smith, and M. Lewis. [Train Short, Test Long: Attention with
    Linear Biases Enables Input Length Extrapolation](https://openreview.net/forum?id=R8sQPpGCv0).
    In International Conference on Learning Representations, 2022.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] O. Press, N. Smith, 和 M. Lewis. [训练短，测试长：带有线性偏置的注意力机制使输入长度外推成为可能](https://openreview.net/forum?id=R8sQPpGCv0)。在《国际学习表征会议》，2022年。'
- en: '[52] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li, and P. J. Liu. [Exploring the Limits of Transfer Learning with a Unified
    Text-to-Text Transformer](https://jmlr.org/papers/v21/20-074.html). Journal of
    Machine Learning Research (JMLR 2020), 21(140):1–67, 2020.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li, 和 P. J. Liu. [探索统一文本到文本转换器的迁移学习极限](https://jmlr.org/papers/v21/20-074.html)。《机器学习研究杂志（JMLR
    2020）》, 21(140):1–67, 2020年。'
- en: '[53] J. J. Rissanen. [Generalized Kraft Inequality and Arithmetic Coding](https://ieeexplore.ieee.org/document/5391119).
    IBM Journal of Research and Development, 20(3):198–203, 1976.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] J. J. Rissanen. [广义克拉夫特不等式与算术编码](https://ieeexplore.ieee.org/document/5391119)。《IBM研究与发展杂志》，20(3):198–203,
    1976年。'
- en: '[54] A. Roberts, H. W. Chung, G. Mishra, A. Levskaya, J. Bradbury, D. Andor,
    S. Narang, B. Lester, C. Gaffney, A. Mohiuddin, C. Hawthorne, A. Lewkowycz, A. Salcianu,
    M. van Zee, J. Austin, S. Goodman, L. B. Soares, H. Hu, S. Tsvyashchenko, A. Chowdhery,
    J. Bastings, J. Bulian, X. Garcia, J. Ni, A. Chen, K. Kenealy, K. Han, M. Casbon,
    J. H. Clark, S. Lee, D. Garrette, J. Lee-Thorp, C. Raffel, N. Shazeer, M. Ritter,
    M. Bosma, A. Passos, J. Maitin-Shepard, N. Fiedel, M. Omernick, B. Saeta, R. Sepassi,
    A. Spiridonov, J. Newlan, and A. Gesmundo. [Scaling Up Models and Data with t5x
    and seqio](http://jmlr.org/papers/v24/23-0795.html). Journal of Machine Learning
    Research, 24(377):1–8, 2023.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] A. Roberts, H. W. Chung, G. Mishra, A. Levskaya, J. Bradbury, D. Andor,
    S. Narang, B. Lester, C. Gaffney, A. Mohiuddin, C. Hawthorne, A. Lewkowycz, A.
    Salcianu, M. van Zee, J. Austin, S. Goodman, L. B. Soares, H. Hu, S. Tsvyashchenko,
    A. Chowdhery, J. Bastings, J. Bulian, X. Garcia, J. Ni, A. Chen, K. Kenealy, K.
    Han, M. Casbon, J. H. Clark, S. Lee, D. Garrette, J. Lee-Thorp, C. Raffel, N.
    Shazeer, M. Ritter, M. Bosma, A. Passos, J. Maitin-Shepard, N. Fiedel, M. Omernick,
    B. Saeta, R. Sepassi, A. Spiridonov, J. Newlan, 和 A. Gesmundo. [使用 t5x 和 seqio
    扩展模型和数据](http://jmlr.org/papers/v24/23-0795.html)。《机器学习研究杂志》，24(377):1–8, 2023年。'
- en: '[55] R. Schaeffer, B. Miranda, and S. Koyejo. [Are Emergent Abilities of Large
    Language Models a Mirage?](https://arxiv.org/abs/2304.15004) In Thirty-Seventh
    Conference on Neural Information Processing Systems, 2023.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] R. Schaeffer, B. Miranda, 和 S. Koyejo. [大型语言模型的涌现能力是幻影吗？](https://arxiv.org/abs/2304.15004)
    在《第37届神经信息处理系统大会》，2023年。'
- en: '[56] R. Sennrich, B. Haddow, and A. Birch. [Neural Machine Translation of Rare
    Words with Subword Units](https://aclanthology.org/P16-1162). In Proceedings of
    the 54th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pages 1715–1725, Berlin, Germany, Aug. 2016\. Association for
    Computational Linguistics.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] R. Sennrich, B. Haddow, 和 A. Birch. [使用子词单元的稀有词汇神经机器翻译](https://aclanthology.org/P16-1162)。在《第54届计算语言学协会年会会议论文集（第1卷：长论文）》中，页码1715–1725，德国柏林，2016年8月。计算语言学协会。'
- en: '[57] C. E. Shannon. [A Mathematical Theory of Communication](http://plan9.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf).
    The Bell System Technical Journal, 27:379–423, 1948.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] C. E. Shannon. [通信的数学理论](http://plan9.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf)。《贝尔系统技术杂志》，27:379–423,
    1948年。'
- en: '[58] P. Shaw, J. Uszkoreit, and A. Vaswani. [Self-Attention with Relative Position
    Representations](https://aclanthology.org/N18-2074). In Proceedings of the 2018
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464–468,
    New Orleans, Louisiana, June 2018\. Association for Computational Linguistics.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] P. Shaw, J. Uszkoreit, 和 A. Vaswani. [带有相对位置表示的自注意力机制](https://aclanthology.org/N18-2074)。在《2018年北美计算语言学协会会议：人类语言技术会议（第2卷：短论文）》中，页码464–468，美国路易斯安那州新奥尔良，2018年6月。计算语言学协会。'
- en: '[59] N. Shazeer and M. Stern. [Adafactor: Adaptive Learning Rates with Sublinear
    Memory Cost](https://proceedings.mlr.press/v80/shazeer18a.html). In Proceedings
    of the 35th International Conference on Machine Learning, volume 80 of Proceedings
    of Machine Learning Research, pages 4596–4604\. PMLR, July 2018.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] N. Shazeer 和 M. Stern. [Adafactor：具有次线性内存成本的自适应学习率](https://proceedings.mlr.press/v80/shazeer18a.html)。发表于第35届国际机器学习大会论文集，机器学习研究论文集第80卷，页码4596–4604\.
    PMLR，2018年7月。'
- en: '[60] S. Stanton, P. Izmailov, P. Kirichenko, A. Alemi, and A. G. Wilson. [Does
    Knowledge Distillation Really Work?](https://arxiv.org/abs/1911.09189) 2021.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] S. Stanton, P. Izmailov, P. Kirichenko, A. Alemi, 和 A. G. Wilson. [知识蒸馏真的有效吗？](https://arxiv.org/abs/1911.09189)
    2021年。'
- en: '[61] Y. Tay, V. Q. Tran, S. Ruder, J. Gupta, H. W. Chung, D. Bahri, Z. Qin,
    S. Baumgartner, C. Yu, and D. Metzler. [Charformer: Fast Character Transformers
    via Gradient-based Subword Tokenization](https://openreview.net/forum?id=JtBRnrlOEFN).
    In International Conference on Learning Representations, 2022.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Y. Tay, V. Q. Tran, S. Ruder, J. Gupta, H. W. Chung, D. Bahri, Z. Qin,
    S. Baumgartner, C. Yu, 和 D. Metzler. [Charformer：通过基于梯度的子词标记化实现快速字符Transformers](https://openreview.net/forum?id=JtBRnrlOEFN)。在国际学习表示大会，2022年。'
- en: '[62] D. Tito Svenstrup, J. Hansen, and O. Winther. [Hash Embeddings for Efficient
    Word Representations](https://proceedings.neurips.cc/paper_files/paper/2017/file/f0f6ba4b5e0000340312d33c212c3ae8-Paper.pdf).
    In Advances in Neural Information Processing Systems, volume 30\. Curran Associates,
    Inc., 2017.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] D. Tito Svenstrup, J. Hansen, 和 O. Winther. [哈希嵌入用于高效的词表示](https://proceedings.neurips.cc/paper_files/paper/2017/file/f0f6ba4b5e0000340312d33c212c3ae8-Paper.pdf)。发表于神经信息处理系统进展，卷30\.
    Curran Associates, Inc.，2017年。'
- en: '[63] C. S. K. Valmeekam, K. Narayanan, D. Kalathil, J.-F. Chamberland, and
    S. Shakkottai. [LLMZip: Lossless Text Compression using Large Language Models](http://arxiv.org/abs/2306.04050),
    June 2023.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] C. S. K. Valmeekam, K. Narayanan, D. Kalathil, J.-F. Chamberland, 和 S.
    Shakkottai. [LLMZip：使用大型语言模型的无损文本压缩](http://arxiv.org/abs/2306.04050)，2023年6月。'
- en: '[64] A. van den Oord, O. Vinyals, and K. Kavukcuoglu. [Neural Discrete Representation
    Learning](https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf).
    In Proceedings of the 31st International Conference on Neural Information Processing
    Systems, NeurIPS’17, page 6309–6318, Red Hook, NY, USA, 2017\. Curran Associates
    Inc.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] A. van den Oord, O. Vinyals, 和 K. Kavukcuoglu. [神经离散表示学习](https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf)。发表于第31届神经信息处理系统国际大会论文集，NeurIPS’17，页码6309–6318，美国纽约州雷德胡克，2017年\.
    Curran Associates Inc.'
- en: '[65] G. Van Rossum and F. L. Drake. Python 3 Reference Manual. CreateSpace,
    Scotts Valley, CA, 2009.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] G. Van Rossum 和 F. L. Drake. Python 3 参考手册。CreateSpace，CA州斯科茨谷，2009年。'
- en: '[66] D. Varis and O. Bojar. [Sequence Length is a Domain: Length-based Overfitting
    in Transformer Models](https://aclanthology.org/2021.emnlp-main.650). In Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing, pages
    8246–8257, Online and Punta Cana, Dominican Republic, Nov. 2021\. Association
    for Computational Linguistics.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] D. Varis 和 O. Bojar. [序列长度是一个领域：Transformer模型中的长度相关过拟合](https://aclanthology.org/2021.emnlp-main.650)。发表于2021年自然语言处理实证方法会议论文集，页码8246–8257，在线和多米尼加共和国蓬塔卡纳，2021年11月\.
    计算语言学协会。'
- en: '[67] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin. [Attention Is All You Need](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf).
    In Advances in Neural Information Processing Systems 30, pages 5998–6008\. Curran
    Associates, Inc., 2017.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin. [注意力机制即一切所需](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)。发表于神经信息处理系统进展30，页码5998–6008\.
    Curran Associates, Inc.，2017年。'
- en: '[68] L. Vilnis, Y. Zemlyanskiy, P. Murray, A. T. Passos, and S. Sanghai. [Arithmetic
    Sampling: Parallel Diverse Decoding for Large Language Models](https://proceedings.mlr.press/v202/vilnis23a.html).
    In Proceedings of the 40th International Conference on Machine Learning, volume
    202 of Proceedings of Machine Learning Research, pages 35120–35136\. PMLR, 23–29
    Jul 2023.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] L. Vilnis, Y. Zemlyanskiy, P. Murray, A. T. Passos, 和 S. Sanghai. [算术采样：大型语言模型的并行多样解码](https://proceedings.mlr.press/v202/vilnis23a.html)。发表于第40届国际机器学习大会论文集，机器学习研究论文集第202卷，页码35120–35136\.
    PMLR，2023年7月23–29日。'
- en: '[69] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau,
    E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett,
    J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson,
    C. J. Carey, İ. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold,
    R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H.
    Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors. [SciPy 1.0:
    Fundamental Algorithms for Scientific Computing in Python](https://scipy.org/).
    Nature Methods, 17:261–272, 2020.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau,
    E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett,
    J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson,
    C. J. Carey, İ. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold,
    R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H.
    Ribeiro, F. Pedregosa, P. van Mulbregt, 和 SciPy 1.0 贡献者。 [SciPy 1.0: Python科学计算的基础算法](https://scipy.org/)。自然方法，17:261–272,
    2020。'
- en: '[70] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. Linformer: Self-Attention
    with Linear Complexity, 2020.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] S. Wang, B. Z. Li, M. Khabsa, H. Fang, 和 H. Ma. Linformer: 线性复杂度的自注意力，2020年。'
- en: '[71] M. L. Waskom. [Seaborn: Statistical Data Visualization](https://doi.org/10.21105/joss.03021).
    Journal of Open Source Software, 6(60):3021, 2021.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] M. L. Waskom. [Seaborn: 统计数据可视化](https://doi.org/10.21105/joss.03021)。开放源软件杂志，6(60):3021,
    2021年。'
- en: '[72] B. L. Welch. [The Generalization of “Student’s” Problem when Several Different
    Population Variances are Involved](http://www.jstor.org/stable/2332510). Biometrika,
    34(1/2):28–35, 1947.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] B. L. Welch. [当涉及多个不同总体方差时，“学生问题”的推广](http://www.jstor.org/stable/2332510)。生物统计学，34(1/2):28–35,
    1947。'
- en: '[73] I. H. Witten, R. M. Neal, and J. G. Cleary. [Arithmetic Coding for Data
    Compression](https://doi.org/10.1145/214762.214771). Communications of The Acm,
    30(6):520–540, June 1987.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] I. H. Witten, R. M. Neal, 和 J. G. Cleary. [用于数据压缩的算术编码](https://doi.org/10.1145/214762.214771)。ACM通讯，30(6):520–540,
    1987年6月。'
- en: '[74] L. Xue, A. Barua, N. Constant, R. Al-Rfou, S. Narang, M. Kale, A. Roberts,
    and C. Raffel. [ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte
    Models](https://aclanthology.org/2022.tacl-1.17). Transactions of the Association
    for Computational Linguistics, 10:291–306, 2022.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] L. Xue, A. Barua, N. Constant, R. Al-Rfou, S. Narang, M. Kale, A. Roberts,
    和 C. Raffel. [ByT5: 迈向无标记的未来，使用预训练的字节到字节模型](https://aclanthology.org/2022.tacl-1.17)。计算语言学协会会刊，10:291–306,
    2022。'
- en: '[75] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon,
    P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer
    sequences. Advances in Neural Information Processing Systems, 33, 2020.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon,
    P. Pham, A. Ravula, Q. Wang, L. Yang, 等. Big bird: 用于更长序列的变换器。神经信息处理系统进展，33，2020年。'
- en: '[76] G. K. Zipf. The Psycho-Biology of Language. Houghton Mifflin, 1935.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] G. K. Zipf. 《语言的心理生物学》。霍顿·米夫林，1935年。'
- en: '[77] J. Ziv and A. Lempel. [A Universal Algorithm for Sequential Data Compression](https://courses.cs.duke.edu/spring03/cps296.5/papers/ziv_lempel_1977_universal_algorithm.pdf).
    IEEE Transactions on Information Theory, 23(3):337–343, 1977.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] J. Ziv 和 A. Lempel. [用于顺序数据压缩的通用算法](https://courses.cs.duke.edu/spring03/cps296.5/papers/ziv_lempel_1977_universal_algorithm.pdf)。IEEE信息理论学报，23(3):337–343,
    1977。'
- en: '[78] A. Zvonkin and L. Levin. [The Complexity of Finite Objects and the Development
    of the Concepts of Information and Randomness by Means of the Theory of Algorithms](https://www.cs.bu.edu/fac/lnd/dvi/ZL-e.pdf).
    Russian Mathematical Surveys, 25:83, 10 2007.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] A. Zvonkin 和 L. Levin. [有限对象的复杂性及通过算法理论发展信息和随机性的概念](https://www.cs.bu.edu/fac/lnd/dvi/ZL-e.pdf)。俄罗斯数学评论，25:83,
    2007年10月。'
- en: Appendix A Numerical Values
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 数值
- en: 'Table 9: Numerical values from [Fig. 3](#S4.F3 "In 4 Results ‣ Training LLMs
    over Neurally Compressed Text"). Methods that use $16$). Note: One thousand million
    is used over one billion to make comparison of FLOPs/byte values easier.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 数值来自 [图 3](#S4.F3 "在 4 结果 ‣ 在神经压缩文本上训练 LLMs")。使用$16$的 方法。注: 使用十亿而非一百亿以便于比较FLOPs/字节值。'
- en: '| Dataset | Size | bits/byte | FLOPs/byte |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 大小 | 位/字节 | FLOPs/字节 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Bytes | $25$M |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 字节数 | $25$M |'
- en: '|  | $113$M |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: '|  | uniform | $8.00$ | - |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  | uniform | $8.00$ | - |'
- en: '| SentencePiece | $25$M |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| SentencePiece | $25$M |'
- en: '|  | $113$M |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: '|  | uniform | $3.47$ | - |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '|  | uniform | $3.47$ | - |'
- en: '| AC$[v\mathord{=}256]$M |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| AC$[v\mathord{=}256]$M |'
- en: '|  | $113$M |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: '|  | uniform | $1.46$ | - |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  | uniform | $1.46$ | - |'
- en: '| AC$[v\mathord{=}65\text{k}]$M |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| AC$[v\mathord{=}65\text{k}]$M |'
- en: '|  | $113$M |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: '| StaticAC$[v\mathord{=}256]$M |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| StaticAC$[v\mathord{=}256]$M |'
- en: '|  | $113$M |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: '|  | uniform | $4.62$ | - |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|  | uniform | $4.62$ | - |'
- en: '| StaticAC$[v\mathord{=}65\text{k}]$M |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| StaticAC$[v\mathord{=}65\text{k}]$M |'
- en: '|  | $113$M |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$M |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$M |'
- en: '|  | $113$M |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: '|  | uniform | $3.01$ | - |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '|  | uniform | $3.01$ | - |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$M |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$M |'
- en: '|  | $113$M |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: '| GZip$[v\mathord{=}256]$M |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| GZip$[v\mathord{=}256]$M |'
- en: '|  | $113$M |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: '|  | uniform | $3.59$ | - |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  | uniform | $3.59$ | - |'
- en: '| GZip$[v\mathord{=}65\text{k}]$M |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| GZip$[v\mathord{=}65\text{k}]$M |'
- en: '|  | $113$M |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: 'Table 10: Numerical values from [Fig. 5](#S4.F5 "In GZip is not competitive
    ‣ 4 Results ‣ Training LLMs over Neurally Compressed Text"). Values for EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$
    showed slight improvements beyond the significant digits shown here as the model
    scales.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 10: 数值来源于[图 5](#S4.F5 "GZip 并不具备竞争力 ‣ 4 结果 ‣ 对神经压缩文本的 LLM 训练")。EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$
    的值随着模型的扩展显示出超出这里所示显著数字的小幅改进。'
- en: '| Dataset | Size | bits/byte | FLOPs/byte |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| Dataset | Size | bits/byte | FLOPs/byte |'
- en: '| --- | --- | --- | --- |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| EqualInfoAC$[b\mathord{=}32,\,v\mathord{=}256]$M |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}32,\,v\mathord{=}256]$M |'
- en: '|  | $113$M |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: '| EqualInfoAC$[b\mathord{=}32,\,v\mathord{=}65\text{k}]$M |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}32,\,v\mathord{=}65\text{k}]$M |'
- en: '|  | $113$M |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: '| EqualInfoAC$[b\mathord{=}64,\,v\mathord{=}256]$M |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}64,\,v\mathord{=}256]$M |'
- en: '|  | $113$M |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: '| EqualInfoAC$[b\mathord{=}64,\,v\mathord{=}65\text{k}]$M |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}64,\,v\mathord{=}65\text{k}]$M |'
- en: '|  | $113$M |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: '| EqualInfoAC$[b\mathord{=}128,\,v\mathord{=}256]$M |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}128,\,v\mathord{=}256]$M |'
- en: '|  | $113$M |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: '| EqualInfoAC$[b\mathord{=}128,\,v\mathord{=}65\text{k}]$M |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}128,\,v\mathord{=}65\text{k}]$M |'
- en: '|  | $113$M |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$M |'
- en: '|  | $403$M |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$M |'
- en: '|  | $2$M |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$M |'
- en: 'Table 11: Numerical values from [Fig. 11](#A9.F11 "In Appendix I M2 Can Handle
    Padding Zeros at the End of a Window ‣ Training LLMs over Neurally Compressed
    Text"), comparing our multiple implementations of EqualInfoAC.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 11: 数值来源于[图 11](#A9.F11 "在附录 I M2 能处理窗口末尾的填充零 ‣ 对神经压缩文本的 LLM 训练")，比较了我们多种
    EqualInfoAC 实现的效果。'
- en: '| Dataset | Compression Ratio | Size | bits/byte | FLOPs/byte |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| Dataset | Compression Ratio | Size | bits/byte | FLOPs/byte |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$M |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$M |'
- en: '|  |  | $113$M |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $113$M |'
- en: '|  |  | $403$M |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $403$M |'
- en: '|  |  | $2$M |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $2$M |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$M |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$M |'
- en: '|  |  | $113$M |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $113$M |'
- en: '|  |  | $403$M |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $403$M |'
- en: '|  |  | $2$M |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $2$M |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$M |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$M |'
- en: '|  |  | $113$M |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $113$M |'
- en: '|  |  | $403$M |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $403$M |'
- en: '|  |  | $2$M |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $2$M |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$M |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$M |'
- en: '|  |  | $113$M |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $113$M |'
- en: '|  |  | $403$M |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $403$M |'
- en: '|  |  | $2$M |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $2$M |'
- en: '[Table 9](#A1.T9 "In Appendix A Numerical Values ‣ Training LLMs over Neurally
    Compressed Text") includes the specific values used to create [Fig. 3](#S4.F3
    "In 4 Results ‣ Training LLMs over Neurally Compressed Text"). Similarly, [Table 10](#A1.T10
    "In Appendix A Numerical Values ‣ Training LLMs over Neurally Compressed Text")
    includes the values used to create [Fig. 5](#S4.F5 "In GZip is not competitive
    ‣ 4 Results ‣ Training LLMs over Neurally Compressed Text"). The numerical values
    from [Fig. 6](#S4.F6 "In Short windows are the best ‣ 4 Results ‣ Training LLMs
    over Neurally Compressed Text") can be found across [Table 9](#A1.T9 "In Appendix
    A Numerical Values ‣ Training LLMs over Neurally Compressed Text") and [Table 10](#A1.T10
    "In Appendix A Numerical Values ‣ Training LLMs over Neurally Compressed Text").
    [Table 11](#A1.T11 "In Appendix A Numerical Values ‣ Training LLMs over Neurally
    Compressed Text") includes the numerical values from [Fig. 11](#A9.F11 "In Appendix
    I M2 Can Handle Padding Zeros at the End of a Window ‣ Training LLMs over Neurally
    Compressed Text").'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '[表9](#A1.T9 "附录A 数值 ‣ 在神经压缩文本上训练LLMs")包含了用于生成[图3](#S4.F3 "在4 结果 ‣ 在神经压缩文本上训练LLMs")的具体值。类似地，[表10](#A1.T10
    "附录A 数值 ‣ 在神经压缩文本上训练LLMs")包含了用于生成[图5](#S4.F5 "GZip不具竞争力 ‣ 4 结果 ‣ 在神经压缩文本上训练LLMs")的值。[图6](#S4.F6
    "短窗口是最好的 ‣ 4 结果 ‣ 在神经压缩文本上训练LLMs")中的数值可以在[表9](#A1.T9 "附录A 数值 ‣ 在神经压缩文本上训练LLMs")和[表10](#A1.T10
    "附录A 数值 ‣ 在神经压缩文本上训练LLMs")中找到。[表11](#A1.T11 "附录A 数值 ‣ 在神经压缩文本上训练LLMs")包含了[图11](#A9.F11
    "附录I M2可以处理窗口末尾的填充零 ‣ 在神经压缩文本上训练LLMs")中的数值。'
- en: Appendix B Variance
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 方差
- en: Sampling from the validation set was seeded. For a given seed, the same batches
    are sampled at each evaluation step within a training run. Similarly, when models
    of a different size are trained on the same compressed data, the same evaluation
    batches are sampled, allowing for fair comparison. As the Bytes and SentencePiece
    baselines use deterministic datasets, the validation seed is not used. Instead
    the “start_step” is incremented by $20$ batches.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 从验证集进行的采样是有种子的。对于给定的种子，每次评估步骤中都会在训练运行内采样相同的批次。类似地，当不同大小的模型在相同的压缩数据上进行训练时，采样相同的评估批次，以便进行公平比较。由于Bytes和SentencePiece基准使用确定性数据集，因此不使用验证种子。相反，“start_step”增加$20$批次。
- en: Model initialization and the order of the training data is controlled by the
    training seed. This seed was also changed during variance testing. During training,
    the dataset is checkpointed and therefore each example is seen exactly once. The
    exact order of the training data is determined by the seed. As the Bytes and SentencePiece
    baselines use deterministic datasets, the training order is fixed.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 模型初始化和训练数据的顺序由训练种子控制。在方差测试期间也更改了这个种子。在训练过程中，数据集被检查点，因此每个样本只会被看到一次。训练数据的确切顺序由种子决定。由于Bytes和SentencePiece基准使用确定性数据集，训练顺序是固定的。
- en: $5$m parameters were trained with different seeds (both validation and training)
    for each compression method and the two baselines. The mean and standard deviation
    can be found in [Table 12](#A2.T12 "In Appendix B Variance ‣ Training LLMs over
    Neurally Compressed Text"). The variance is so low that we only report single
    values for most other experimental settings, such as larger models.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: $5$m 参数在每种压缩方法和两个基准上使用不同的种子（包括验证和训练）进行训练。均值和标准差可以在[表12](#A2.T12 "附录B 方差 ‣ 在神经压缩文本上训练LLMs")中找到。方差非常低，因此我们在大多数其他实验设置中仅报告单个值，例如更大的模型。
- en: 'Table 12: Variance in performance is low. Even with maximum changes between
    runs—different evaluation samples, different training orders, and different parameter
    initialization—there is very little variance in final performance. Statistics
    were calculated over $5$m parameter training runs for each method.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 表12：性能的方差很低。即使在运行之间存在最大变化——不同的评估样本、不同的训练顺序和不同的参数初始化——最终性能的方差也非常小。统计数据是对每种方法的$5$m参数训练运行进行计算的。
- en: '| Method | bits/byte |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 比特/字节 |'
- en: '| --- | --- |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Bytes | $1.2899\pm 0.0020$ |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| Bytes | $1.2899\pm 0.0020$ |'
- en: '| SentencePiece | $1.1171\pm 0.0006$ |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| SentencePiece | $1.1171\pm 0.0006$ |'
- en: '| AC$[v\mathord{=}256]$ |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| AC$[v\mathord{=}256]$ |'
- en: '| StaticAC$[v\mathord{=}256]$ |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| StaticAC$[v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$ |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}32,\,v\mathord{=}256]$ |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}32,\,v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}64,\,v\mathord{=}256]$ |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}64,\,v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}128,\,v\mathord{=}256]$ |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}128,\,v\mathord{=}256]$ |'
- en: '| GZip$[v\mathord{=}256]$ |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| GZip$[v\mathord{=}256]$ |'
- en: Training models of size $403$ re-runs in the most problematic case.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 $403$ 大小的模型在最具问题的情况下重新运行。
- en: Appendix C The Amount of Raw Text Bytes Seen by M2
  id: totrans-448
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C M2 看到的原始文本字节量
- en: '[Table 13](#A3.T13 "In Appendix C The Amount of Raw Text Bytes Seen by M2 ‣
    Training LLMs over Neurally Compressed Text") shows the number of tokens and bytes
    found in the training dataset for each compression method. During the data generation
    process, sequences of $10{,}240$ fewer tokens. All compression datasets are created
    from the same source sequences, thus the underlying byte sequences compressed
    by weaker methods are prefixes of the underlying sequences compressed by stronger
    methods.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 13](#A3.T13 "在附录 C M2 看到的原始文本字节量 ‣ 在神经压缩文本上训练 LLMs") 显示了每种压缩方法在训练数据集中发现的令牌和字节数量。在数据生成过程中，序列的数量减少了
    $10{,}240$ 个令牌。所有压缩数据集都来源于相同的源序列，因此，弱方法压缩的底层字节序列是强方法压缩的底层序列的前缀。'
- en: 'Table 13: Compression ratios (bytes $/$.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：压缩比（字节 $/$。
- en: '| Method | Compression Ratio | Tokens | Bytes |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 压缩比 | 令牌 | 字节 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Bytes | $1.0$ |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 字节 | $1.0$ |'
- en: '| SentencePiece | $4.28$ |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| SentencePiece | $4.28$ |'
- en: '| AC$[v\mathord{=}256]$ |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| AC$[v\mathord{=}256]$ |'
- en: '| StaticAC$[v\mathord{=}256]$ |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| StaticAC$[v\mathord{=}256]$ |'
- en: '| GZip$[v\mathord{=}256]$ |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| GZip$[v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$ |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}32,\,v\mathord{=}256]$ |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}32,\,v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}64,\,v\mathord{=}256]$ |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}64,\,v\mathord{=}256]$ |'
- en: '| EqualInfoAC$[b\mathord{=}128,\,v\mathord{=}256]$ |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}128,\,v\mathord{=}256]$ |'
- en: '| AC$[v\mathord{=}65\text{k}]$ |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| AC$[v\mathord{=}65\text{k}]$ |'
- en: '| StaticAC$[v\mathord{=}65\text{k}]$ |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| StaticAC$[v\mathord{=}65\text{k}]$ |'
- en: '| GZip$[v\mathord{=}65\text{k}]$ |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| GZip$[v\mathord{=}65\text{k}]$ |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$ |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$ |'
- en: '| EqualInfoAC$[b\mathord{=}32,\,v\mathord{=}65\text{k}]$ |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}32,\,v\mathord{=}65\text{k}]$ |'
- en: '| EqualInfoAC$[b\mathord{=}64,\,v\mathord{=}65\text{k}]$ |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}64,\,v\mathord{=}65\text{k}]$ |'
- en: '| EqualInfoAC$[b\mathord{=}128,\,v\mathord{=}65\text{k}]$ |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}128,\,v\mathord{=}65\text{k}]$ |'
- en: Appendix D Scaling Curves with Scaled Training Data
  id: totrans-469
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 使用缩放训练数据的缩放曲线
- en: '[[30](#bib.bib30)] found that when scaling models, the training data should
    be scaled with the model size. As such, when comparing settings with constant
    training FLOPs, a large part of the FLOPs budget should be used by adding more
    training data. We apply this technique to compensate for our $2$k steps. Otherwise,
    the settings match those in [Fig. 3](#S4.F3 "In 4 Results ‣ Training LLMs over
    Neurally Compressed Text"). Numerical values used in the graph can be found in
    [Table 14](#A4.T14 "In Appendix D Scaling Curves with Scaled Training Data ‣ Training
    LLMs over Neurally Compressed Text").'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '[[30](#bib.bib30)] 发现，当模型规模扩大时，训练数据也应随着模型规模进行缩放。因此，当比较具有固定训练 FLOPs 的设置时，FLOPs
    预算中的大部分应通过增加更多训练数据来使用。我们应用这种技术来弥补我们的 $2$k 步骤。否则，设置与 [图 3](#S4.F3 "在 4 结果 ‣ 在神经压缩文本上训练
    LLMs") 中的设置相匹配。图中的数值可以在 [表 14](#A4.T14 "在附录 D 缩放曲线与缩放训练数据 ‣ 在神经压缩文本上训练 LLMs")
    中找到。'
- en: Scaling the training data adjusts the absolute slopes of the lines for all models
    that learn. Models that do not learn still only predict a uniform distribution.
    The trends between settings are unchanged. Thus we opt to plot the versions where
    training data is held constant across model sizes.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放训练数据调整了所有学习模型的绝对斜率。未学习的模型仍然仅预测均匀分布。设置之间的趋势保持不变。因此，我们选择绘制在模型大小之间训练数据保持不变的版本。
- en: '![Refer to caption](img/5adef71f610860135c0b0276ab634f2d.png)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5adef71f610860135c0b0276ab634f2d.png)'
- en: 'Figure 10: Training language models over compressed text while scaling training
    data with model size results in steeper slopes. When scaling model size, it has
    been found that the training data should be scaled proportionally [[30](#bib.bib30)].
    We apply this scaling technique by plotting values for smaller models at earlier
    training steps. The trends are similar to [Fig. 3](#S4.F3 "In 4 Results ‣ Training
    LLMs over Neurally Compressed Text"), even down to things like where the EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$m
    parameter models).'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：在压缩文本上训练语言模型，并随着模型大小扩展训练数据，会导致更陡的斜率。当扩展模型大小时，发现训练数据也应该按比例扩展[[30](#bib.bib30)]。我们通过在早期训练步骤绘制较小模型的值来应用这种扩展技术。这些趋势与[图
    3](#S4.F3 "在 4 结果 ‣ 训练 LLMs 通过神经压缩文本")类似，甚至包括诸如 EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$m
    参数模型的位置。
- en: 'Table 14: Numerical values from [Fig. 10](#A4.F10 "In Appendix D Scaling Curves
    with Scaled Training Data ‣ Training LLMs over Neurally Compressed Text"). Values
    for the uniform distribution and FLOPs/byte values can be found in [Table 9](#A1.T9
    "In Appendix A Numerical Values ‣ Training LLMs over Neurally Compressed Text").'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：来自[图 10](#A4.F10 "在附录 D 扩展曲线与扩展训练数据 ‣ 训练 LLMs 通过神经压缩文本")的数值。均匀分布和 FLOPs/字节值可以在[表
    9](#A1.T9 "在附录 A 数值 ‣ 训练 LLMs 通过神经压缩文本")中找到。
- en: '| Dataset | Size | Step | bits/byte |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 大小 | 步骤 | 位/字节 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Bytes | $25$ |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| Bytes | $25$ |'
- en: '|  | $113$ |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$ |'
- en: '|  | $403$ |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$ |'
- en: '|  | $2$ |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$ |'
- en: '| SentencePiece | $25$ |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| SentencePiece | $25$ |'
- en: '|  | $113$ |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$ |'
- en: '|  | $403$ |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$ |'
- en: '|  | $2$ |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$ |'
- en: '| AC$[v\mathord{=}256]$ |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| AC$[v\mathord{=}256]$ |'
- en: '|  | $113$ |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$ |'
- en: '|  | $403$ |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$ |'
- en: '|  | $2$ |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$ |'
- en: '| AC$[v\mathord{=}65\text{k}]$ |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| AC$[v\mathord{=}65\text{k}]$ |'
- en: '|  | $113$ |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$ |'
- en: '|  | $403$ |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$ |'
- en: '|  | $2$ |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$ |'
- en: '| StaticAC$[v\mathord{=}256]$ |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| StaticAC$[v\mathord{=}256]$ |'
- en: '|  | $113$ |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$ |'
- en: '|  | $403$ |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$ |'
- en: '|  | $2$ |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$ |'
- en: '| StaticAC$[v\mathord{=}65\text{k}]$ |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| StaticAC$[v\mathord{=}65\text{k}]$ |'
- en: '|  | $113$ |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$ |'
- en: '|  | $403$ |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$ |'
- en: '|  | $2$ |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$ |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$ |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$ |'
- en: '|  | $113$ |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$ |'
- en: '|  | $403$ |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$ |'
- en: '|  | $2$ |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$ |'
- en: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$ |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}65\text{k}]$ |'
- en: '|  | $113$ |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$ |'
- en: '|  | $403$ |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$ |'
- en: '|  | $2$ |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$ |'
- en: '| GZip$[v\mathord{=}256]$ |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| GZip$[v\mathord{=}256]$ |'
- en: '|  | $113$ |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$ |'
- en: '|  | $403$ |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$ |'
- en: '|  | $2$ |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$ |'
- en: '| GZip$[v\mathord{=}65\text{k}]$ |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| GZip$[v\mathord{=}65\text{k}]$ |'
- en: '|  | $113$ |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '|  | $113$ |'
- en: '|  | $403$ |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|  | $403$ |'
- en: '|  | $2$ |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$ |'
- en: Appendix E GZip Headers and Footers
  id: totrans-517
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E GZip 头部和尾部
- en: GZip compressed documents have both a header—two bytes that identify the file
    type—and a footer—two bytes representing the Adler-$32$-bit vocabulary does not
    help. In this work, we use the version of GZip datasets that include the header
    and footers.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: GZip 压缩文档包含头部——两个字节标识文件类型——和尾部——两个字节表示 Adler-$32$ 位词汇，这对性能没有帮助。在这项工作中，我们使用包含头部和尾部的
    GZip 数据集版本。
- en: 'Table 15: Removal of the GZip header and footer results in minimal performance
    differences.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 表 15：移除 GZip 头部和尾部会导致性能差异最小。
- en: '| Method | bits/byte |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位/字节 |'
- en: '| --- | --- |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GZip$[v\mathord{=}256]$ | 2.33 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| GZip$[v\mathord{=}256]$ | 2.33 |'
- en: '|     $-$header/footer | 2.35 |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '|     $-$header/footer | 2.35 |'
- en: '| GZip$[v\mathord{=}65\text{k}]$ | 2.91 |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| GZip$[v\mathord{=}65\text{k}]$ | 2.91 |'
- en: '|     $-$header/footer | 2.92 |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '|     $-$header/footer | 2.92 |'
- en: Appendix F Arithmetic Coding Details
  id: totrans-526
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 算术编码细节
- en: While it is easiest to imagine the narrowing of the bit interval process in
    Arithmetic Coding as a second step after the character interval $I_{n}$ or the
    overlap conditions outlined above happen again. This is repeated until a bit interval
    that is enclosed by the final interval is found.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最容易将算术编码中的位区间收窄过程想象为在字符区间 $I_{n}$ 或上述重叠条件发生后作为第二步。这会重复直到找到一个被最终区间包围的位区间。
- en: This fact is critical in finite precision implementations. Once a bit is locked
    in, it can be emitted. This allows for the rescaling of the current interval and
    is how over/underflow is avoided.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点在有限精度实现中至关重要。一旦一个位被锁定，它就可以被发射。这允许重新缩放当前区间，并且这是如何避免溢出/欠流的方式。
- en: Appendix G Evaluation Details
  id: totrans-529
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 评估细节
- en: In our experiments, different settings have different vocabulary size, tokenization,
    and has a different amount of underlying text due to variations in compression
    rate. Thus, they are not directly comparable using “per-token” versions metrics
    like the cross-entropy, negative log likelihood loss, or perplexity. To address
    this, we convert our token-level negative log likelihood loss, $\ell$. Note that
    we use “per byte” metrics over “per character” metrics as there is ambiguity as
    to what counts as a character when working with UTF-8 Unicode.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，不同的设置具有不同的词汇量、分词方式，并且由于压缩率的变化，底层文本量也不同。因此，使用“每个标记”的度量（如交叉熵、负对数似然损失或困惑度）进行直接比较是不可能的。为了解决这个问题，我们将标记级别的负对数似然损失转换为
    $\ell$。请注意，我们使用“每字节”度量而不是“每字符”度量，因为在处理 UTF-8 Unicode 时，对什么算作字符存在歧义。
- en: As is common in evaluation of work related to compression, instead of the negative
    log likelihood loss $\ell_{\text{byte}}$, when the input tokens represent bytes.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 与压缩相关的工作评估中常见的做法是，当输入标记表示字节时，不使用负对数似然损失 $\ell_{\text{byte}}$。
- en: As one of the main advantages of an M2 model that processes compressed text
    is that it needs to be run over fewer tokens, we also compare models based on
    the amount of FLOPs required during inference. Different compression methods result
    in different sequence lengths for the M2 model to process. Therefore, we need
    to standardize our FLOPs measurement to the byte-level so that it is comparable
    across methods. We start with FLOPs/token—approximated by $2\times\text{num\_params}$
    (not including embedding parameters) following [[35](#bib.bib35)]—and divide it
    by that method’s token-level compression rate to get the FLOPs/byte, just like
    the bits/byte conversion. For methods that require running an M1 model over each
    byte, the FLOPs/byte cost of the M1 model is added. Note, while there is a computational
    cost to running GZip over the input text, we ignore it as it is insubstantial
    compared to the cost of running model inference.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 由于处理压缩文本的 M2 模型的主要优势之一是它需要处理的标记较少，因此我们还根据推理过程中所需的 FLOPs 量来比较模型。不同的压缩方法会导致 M2
    模型处理不同的序列长度。因此，我们需要将 FLOPs 测量标准化到字节级别，以便在方法之间进行比较。我们从 FLOPs/标记 开始——由 $2\times\text{num\_params}$（不包括嵌入参数）近似得到
    [[35](#bib.bib35)]——然后将其除以该方法的标记级别压缩率，以获得 FLOPs/字节，就像比特/字节转换一样。对于需要在每个字节上运行 M1
    模型的方法，还会添加 M1 模型的 FLOPs/字节成本。请注意，尽管运行 GZip 对输入文本有计算成本，但我们忽略它，因为与运行模型推理的成本相比，它微不足道。
- en: Evaluation of language models is often done by running the model on the entire
    validation set, moving the sliding window formed by the model’s context window
    by a single token at each step. This yields stronger models by providing the most
    context possible when making predictions for a token. As we care about relative
    performances between methods, opposed to absolute performance, we opt to evaluate
    the model on a sample of the C4 validation set. During evaluation, the model is
    run over $20$. Thus, the variance introduced from sampling the validation set
    is negligible. See [Appendix B](#A2 "Appendix B Variance ‣ Training LLMs over
    Neurally Compressed Text") for more information.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的评估通常通过在整个验证集上运行模型来完成，每一步将模型的上下文窗口滑动一个标记。这通过提供尽可能多的上下文来预测一个标记，从而产生更强的模型。由于我们关心的是方法之间的相对性能，而不是绝对性能，我们选择在
    C4 验证集的一个样本上评估模型。在评估过程中，模型在 $20$ 上运行。因此，从验证集中抽样引入的方差可以忽略不计。有关更多信息，请参见 [附录 B](#A2
    "Appendix B Variance ‣ Training LLMs over Neurally Compressed Text")。
- en: Appendix H Alternative Compression Methods
  id: totrans-534
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 替代压缩方法
- en: H.1 Equal-Text Windows
  id: totrans-535
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.1 Equal-Text Windows
- en: We also considered what is essentially the inverse of Equal-Info Windows—Equal-Text
    Windows. Instead of consuming a variable amount of text and outputting a consistent
    number of bits, Equal-Text Windows feed a consistent amount of text into the Arithmetic
    Coder which is compressed to a variable number of bits.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还考虑了与 Equal-Info Windows 实质上相反的 Equal-Text Windows。Equal-Text Windows 输入固定量的文本到算术编码器中，输出一个可变数量的比特，而不是消耗可变量的文本并输出一致数量的比特。
- en: To deal with this variability, we thought M2 would require delimiter tokens
    between windows in order to tell which tokens are part of the same independently
    compressed chunk. We thought this would hurt the compression rate too much, especially
    for the short AC compressed windows that we found most effective in [Fig. 5](#S4.F5
    "In GZip is not competitive ‣ 4 Results ‣ Training LLMs over Neurally Compressed
    Text").
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这种变异性，我们认为M2需要在窗口之间使用分隔符令牌，以区分哪些令牌是同一独立压缩块的一部分。我们认为这会对压缩率造成过大的影响，特别是对于我们在[图5](#S4.F5
    "在GZip中不具竞争力 ‣ 4 结果 ‣ 在神经压缩文本上训练LLMs")中发现最有效的短AC压缩窗口。
- en: Further exploration of this method, especially to see if the delimiters are
    actually required, would be interesting future work as the Equal-Text Windows
    algorithm is much simpler than Equal-Info Windows.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步探索这种方法，特别是查看是否实际上需要分隔符，将是有趣的未来工作，因为Equal-Text Windows算法比Equal-Info Windows算法简单得多。
- en: H.2 Huffman Coding
  id: totrans-539
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.2 哈夫曼编码
- en: We also considered using Huffman Coding [[32](#bib.bib32)] as a baseline compression
    implementation. As most implementations use static probabilities for characters,
    we thought the compression rate would be too low to be competitive. With static
    Huffman Coding, it is much easier to create a map between bitstream subsequences
    and characters, which may result in being more learnable by M2 models. However,
    this is because the coding component assigns each character a whole number of
    bits, resulting in a less optimal coding compared to Arithmetic Coding. Huffman
    Coding can be made adaptive by updating the induced codebook periodically, based
    on newer data. When considering bit-level compression, adaptive Huffman Coding
    performs similar to static Huffman Coding [[45](#bib.bib45)]. However, when considering
    token-level compression, and the fact that the adaptive distribution will come
    from M1, not unigrams of the recent data, training M2 models on adaptive Huffman
    Coding could be interesting future work. As Huffman coding is part of the GZip
    algorithm, we opted to not explore using just Huffman Coding.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还考虑使用哈夫曼编码[[32](#bib.bib32)]作为基线压缩实现。由于大多数实现使用字符的静态概率，我们认为压缩率会太低，无法具备竞争力。使用静态哈夫曼编码，创建比特流子序列和字符之间的映射要容易得多，这可能使M2模型更容易学习。然而，这是因为编码组件为每个字符分配了一个完整的位数，相比于算术编码，导致编码不够优化。哈夫曼编码可以通过定期更新生成的代码书来实现自适应，基于更新的数据。在考虑位级压缩时，自适应哈夫曼编码的表现与静态哈夫曼编码相似[[45](#bib.bib45)]。然而，在考虑标记级压缩以及自适应分布将来自M1，而不是最近数据的单词时，训练M2模型在自适应哈夫曼编码上可能是有趣的未来工作。由于哈夫曼编码是GZip算法的一部分，我们选择不只使用哈夫曼编码进行探索。
- en: H.3 Asymmetric Numeral Systems
  id: totrans-541
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.3 非对称数字系统
- en: Another compression algorithm we considered was Asymmetric Numeral Systems (ANS)
    [[19](#bib.bib19)]. ANS has strong coding performance and is amenable to adaptive
    probabilities. The internal state is only a single natural number, which may be
    easier for an LLM to track than the two real numbers used in AC. However, the
    encoding and decoding algorithm are more like a stack, where the encoder runs
    left to right while the decoder runs right to left. By the time the full input
    is seen, there are no more computation steps for the LLM to actually decode. Thus,
    we opted to not explore ANS in this work. However, the simpler state is appealing
    and using ANS for compression would be of interest as future work.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑的另一种压缩算法是非对称数字系统（ANS）[[19](#bib.bib19)]。ANS具有强大的编码性能，并且适用于自适应概率。内部状态仅为一个自然数，这可能比AC中使用的两个实数更容易被LLM跟踪。然而，编码和解码算法更像是一个堆栈，编码器从左到右运行，而解码器从右到左运行。当完全输入被看到时，LLM实际上没有更多的计算步骤来解码。因此，我们选择不在本研究中探索ANS。然而，较简单的状态是有吸引力的，将ANS用于压缩将作为未来工作的兴趣。
- en: Appendix I M2 Can Handle Padding Zeros at the End of a Window
  id: totrans-543
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录I M2可以处理窗口末尾的填充零
- en: In the implementation of EqualInfoAC$[b\mathord{=}W]$ bits, padding of the compressed
    bitstream without that additional character must be done.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 在EqualInfoAC$[b\mathord{=}W]$位的实现中，必须对没有额外字符的压缩比特流进行填充。
- en: In cases where the final character in the window only adds zeros to the bitstream,
    it is unclear at first glance if that final character was included in the window,
    or if it was omitted and the trailing zeros are all padding. However, the compression
    scheme is still lossless if we are consistent in our encoding. By always including
    the most input characters possible in each window, we know that, during decoding,
    if the addition of a final character (which is compressed to all zeros) still
    results in the same compressed bitstream, then that final character is part of
    that window. The decoding algorithm also knows when to stop adding characters
    to input—when the addition of a new character would generate more than $W$ bits.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 在窗口中的最后一个字符仅为比特流添加零的情况下，初看时不清楚该最后一个字符是否包含在窗口中，还是被省略了而尾部的零只是填充。然而，只要我们在编码中保持一致，压缩方案仍然是无损的。通过在每个窗口中始终包含尽可能多的输入字符，我们知道，在解码过程中，如果添加一个最终字符（被压缩为全零）仍然产生相同的压缩比特流，那么这个最终字符是该窗口的一部分。解码算法也知道何时停止添加字符到输入——当添加新字符会生成超过$W$位时。
- en: This kind of padding is present in many Arithmetic Coding implementations and
    is generally solved by either giving the AC decoder the original input sequence
    length and the compressed message, or by the AC decoder using a special termination
    character. These fixes aim to identify padding in a single run of the AC decoder,
    but would be difficult to apply in our setting. Passing the number of tokens present
    in a window to M2 would be possible during training, but it would make inference
    much more complex (requiring a solution such as M2 generating fertility scores
    that specify how many characters the generated tokens represent [[8](#bib.bib8)]).
    As such, we achieve lossless compression by allowing the AC decoder to be run
    multiple times as the decoding algorithm nears the end of the input, incrementing
    the sequence length until we find the sequence that matches the compressed output.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 这种填充在许多算术编码实现中存在，通常通过给AC解码器原始输入序列长度和压缩消息，或者通过AC解码器使用特殊的终止字符来解决。这些修复旨在在AC解码器的单次运行中识别填充，但在我们的设置中应用起来会很困难。在训练期间将窗口中存在的标记数量传递给M2是可能的，但这会使推理变得更加复杂（需要类似M2生成生育分数来指定生成的标记表示多少字符[[8](#bib.bib8)]）。因此，我们通过允许AC解码器在解码算法接近输入结束时多次运行，增加序列长度，直到找到与压缩输出匹配的序列，从而实现无损压缩。
- en: This window decoding algorithm is not well aligned with how transformers processes
    data. It essentially involves a look-ahead where the AC decoder is run over prospective
    inputs and if they are inconsistent with the compressed tokens, backtracking is
    done and decisions about the previous tokens are made. In contrast, the transformer
    has a fairly fixed budget when processing a single window, just the layers in
    the model and the part of the sequence that is inside that window.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 这种窗口解码算法与变压器处理数据的方式不太一致。它本质上涉及到前瞻，其中AC解码器会运行在潜在的输入上，如果它们与压缩的标记不一致，则会回溯并对之前的标记做出决策。相比之下，变压器在处理单个窗口时有一个相对固定的预算，即模型中的层和窗口内的序列部分。
- en: An alternative windowed compression scheme more inline with transformer computation
    is to avoid input characters that compress to all zeros. During compression, when
    a window is about to be emitted and an additional character would fit into the
    window, but compresses to all zeros, we opt to not include this character. That
    is, we compress as many characters into the window as possible, while ensuring
    that each new character results in a change in the bitstream compared to the previous
    value (plus padding). This results in a much simpler decoding algorithm where
    new input characters are added until the correct compressed bitstream is emitted.
    As we always include the least number of characters that can possibly output this
    bitstream, we know the input without needing to look-ahead at the result of compressing
    the next character. As our normal implementation compresses the most number of
    tokens possible into the current window, this version results in a reduction in
    compression rate, dropping from $2.66$.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 与变压器计算更一致的另一种窗口压缩方案是避免压缩为全零的输入字符。在压缩过程中，当一个窗口即将被发出并且一个额外字符可以适应窗口但压缩为全零时，我们选择不包含此字符。也就是说，我们尽可能将更多字符压缩到窗口中，同时确保每个新字符导致比特流相比于之前的值（加上填充）发生变化。这导致了一个更简单的解码算法，其中新的输入字符被添加直到发出正确的压缩比特流。由于我们总是包含最少数量的字符以可能输出这个比特流，我们知道输入而无需查看下一个字符的压缩结果。由于我们正常的实现将最多数量的标记压缩到当前窗口中，这个版本导致压缩率下降，从$2.66$。
- en: '![Refer to caption](img/8681c664da597857086697f81ac88e6d.png)![Refer to caption](img/31ea8cfadc9839eaa2a2b754c4c1dd27.png)'
  id: totrans-549
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8681c664da597857086697f81ac88e6d.png)![参见说明](img/31ea8cfadc9839eaa2a2b754c4c1dd27.png)'
- en: 'Figure 11: The model is able to effectively discern between padding and trailing
    zeros that represent an input character in our implementation of EqualInfoAC.
    When using EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$, the absolute bits/byte
    performance is greater using the new implementation, but the reduction in compression
    rate means the original implementation is still preferred when the inference cost
    is considered. This is especially clear in the right graph, where the original
    implementation’s superior compression rate is obvious.'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：在我们的EqualInfoAC实现中，该模型能够有效地区分填充和表示输入字符的尾随零。当使用EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$时，新实现的绝对bits/byte性能更高，但压缩率的降低意味着在考虑推理成本时仍然首选原始实现。这在右侧图中尤为明显，原始实现的优越压缩率显而易见。
- en: In [Fig. 11](#A9.F11 "In Appendix I M2 Can Handle Padding Zeros at the End of
    a Window ‣ Training LLMs over Neurally Compressed Text") we see a comparison when
    training M2 models over data compressed with each method. We see that when using
    the new implementation with EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$, the
    new compression implementation makes slight, but still greater than one standard
    deviation, improvement in terms of bits/byte. However, the reduction in the compression
    ratio means training models over this implementation will lose some of the computational
    advantages that training over compressed text yields. Thus, it fails to fully
    eclipse the original implementation. Numerical values can be found in [Table 11](#A1.T11
    "In Appendix A Numerical Values ‣ Training LLMs over Neurally Compressed Text").
    It is clear that the model is able to discern between trailing zeros that represent
    characters and those the represent padding. Thus, we opt to use the implementation
    that maximized the compression ratio throughout this work.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图11](#A9.F11 "附录I M2可以处理窗口末尾的填充零 ‣ 训练LLMs的神经压缩文本")中，我们看到在对使用每种方法压缩的数据训练M2模型时的比较。我们看到，当使用新的EqualInfoAC$[b\mathord{=}16,\,v\mathord{=}256]$实现时，新的压缩实现略有改进，但仍大于一个标准偏差，表现为bits/byte。然而，压缩比的降低意味着对这种实现训练模型将失去一些训练压缩文本所带来的计算优势。因此，它未能完全超越原始实现。数值可以在[表11](#A1.T11
    "附录A 数值 ‣ 训练LLMs的神经压缩文本")中找到。显然，该模型能够区分表示字符的尾随零和表示填充的零。因此，我们选择在整个工作中使用最大化压缩比的实现。
- en: Appendix J Entropy Estimation
  id: totrans-552
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录J 熵估计
- en: (a) *bit* n-grams counting all overlapping occurrences
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: (a) *比特* n-gram 计算所有重叠的出现
- en: '![Refer to caption](img/891028397535a917f032478d97942ed2.png)'
  id: totrans-554
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/891028397535a917f032478d97942ed2.png)'
- en: (b) n-bit *tokens* following our M2 tokenization
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: (b) n-bit *标记* 按照我们的M2标记化
- en: 'Figure 12: The amount of noise in the entropy estimate grows as the length
    of bit segments grow. Larger segmentations of the bitstream result in larger vocabularies
    and therefore require larger sample sizes for accurate entropy estimates. For
    each setting, we plot the $5\%$ are noisier than AC, despite this not being apparent
    in [Fig. 9](#S6.F9 "In 6.3 Learnable distributions are less uniform ‣ 6 Analysis
    ‣ Training LLMs over Neurally Compressed Text").'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：随着比特段长度的增加，熵估计中的噪声量也随之增加。比特流的更大分段会导致更大的词汇量，因此需要更大的样本量来进行准确的熵估计。对于每种设置，我们绘制了$5\%$比AC更嘈杂，尽管在[图9](#S6.F9
    "在6.3 可学习分布不再均匀 ‣ 6 分析 ‣ 基于神经压缩文本的LLMs训练")中这一点并不明显。
- en: To account for noise in the entropy estimation, we partition the data into $100$
    and that settings like GZip and EqualInfoAC are noisier than AC and RNG. These
    trends are seen in [Fig. 12](#A10.F12 "In Appendix J Entropy Estimation ‣ Training
    LLMs over Neurally Compressed Text") where the entropy has been normalized based
    on the mean entropy calculated across the partitions.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 为了考虑熵估计中的噪声，我们将数据分成$100$个部分，并且像GZip和EqualInfoAC这样的设置比AC和RNG更嘈杂。这些趋势在[图12](#A10.F12
    "在附录J 熵估计 ‣ 基于神经压缩文本的LLMs训练")中可以看到，其中熵已根据跨分区计算的均值熵进行归一化。
- en: '![Refer to caption](img/c7e3962c4c87f75e38bf1f0dfa02f5ce.png)'
  id: totrans-558
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c7e3962c4c87f75e38bf1f0dfa02f5ce.png)'
- en: (a) N-Grams
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: (a) N-Grams
- en: '![Refer to caption](img/412cccf35e38cc4bc1f22743325d3da8.png)'
  id: totrans-560
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/412cccf35e38cc4bc1f22743325d3da8.png)'
- en: (b) Tokens
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Tokens
- en: 'Figure 13: Bias corrected KL divergence between the observed and uniform distributions
    for different segmentations of the bitstream. This plot is similar to [Fig. 9](#S6.F9
    "In 6.3 Learnable distributions are less uniform ‣ 6 Analysis ‣ Training LLMs
    over Neurally Compressed Text"), however, the KL divergence calculations use the
    entropy of the observed distribution after applying the Miller-Madow bias correction.
    After applying bias correction, we see that the expected $0$s above the 50th percentile
    for RNG, however, it is hard to disentangle the two as their 5th percentile lines
    are similar.'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：观察分布与均匀分布之间的偏差修正KL散度，对于比特流的不同分段。此图与[图9](#S6.F9 "在6.3 可学习分布不再均匀 ‣ 6 分析 ‣
    基于神经压缩文本的LLMs训练")类似，然而KL散度计算使用的是应用米勒-马多偏差修正后的观察分布的熵。应用偏差修正后，我们看到RNG的50th百分位数以上期望值为$0$，然而由于它们的5th百分位数线相似，很难将两者区分开。
- en: The maximum likelihood, or plug-in, estimator of entropy, $\hat{H}=-\sum_{x\in\mathcal{X}}\hat{p}(x)\log_{2}\hat{p}(x)$
    is the size of the current segmentation.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 熵的最大似然估计器或插值估计器，$\hat{H}=-\sum_{x\in\mathcal{X}}\hat{p}(x)\log_{2}\hat{p}(x)$，是当前分段的大小。
- en: When we plot the KL divergence between the Miller-Madow estimated entropy and
    the uniform distribution, we see that the percentile interval for the RNG baseline
    now includes $0$, we opt to plot the plug-in estimator in [Fig. 9](#S6.F9 "In
    6.3 Learnable distributions are less uniform ‣ 6 Analysis ‣ Training LLMs over
    Neurally Compressed Text") instead of the Miller-Madow estimator.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们绘制米勒-马多估计熵与均匀分布之间的KL散度时，我们发现RNG基线的百分位区间现在包括$0$，我们选择在[图9](#S6.F9 "在6.3 可学习分布不再均匀
    ‣ 6 分析 ‣ 基于神经压缩文本的LLMs训练")中绘制插值估计器，而不是米勒-马多估计器。
- en: Appendix K Analysis Implementation
  id: totrans-565
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录K 分析实现
- en: Matplolib [[33](#bib.bib33)] and Seaborn [[71](#bib.bib71)] were used to make
    all the included graphs.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了Matplotlib [[33](#bib.bib33)] 和Seaborn [[71](#bib.bib71)]来绘制所有包含的图表。
- en: Statistical significance tests were done using Welch’s t-test [[72](#bib.bib72)]
    using the function scipy.stats.ttest_ind_from_stats from SciPy [[69](#bib.bib69)].
    We used $p<0.05$ as the statistical significance threshold.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Welch t检验[[72](#bib.bib72)]进行统计显著性测试，采用了SciPy库中的`scipy.stats.ttest_ind_from_stats`函数[[69](#bib.bib69)]。我们使用$p<0.05$作为统计显著性阈值。
- en: Appendix L Corner Cases of Tokenization lead to Unstable Mappings
  id: totrans-568
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录L 标记化的角落案例导致不稳定的映射
- en: There are some cases where SentencePiece does not have stable text $\rightarrow$ [6406].
    When you look at the surface text substring “chair”, it seems to map to multiple
    tokens, however when you look at the full surface term “chairs” the stability
    returns. This is in contrast to a byte-level vocabulary where the text “chair”
    always maps to [102, 107, 100, 108, 117], even as part of the text “chairs” where
    an extra [118] is appended to the end. While the loss of shared representations
    of clearly related concepts in unfortunate, the performance of modern models based
    on this kind of tokenization shows that it is well handled by the model. While
    these edge cases exist, they are rare enough that the SentencePiece tokenizer
    should be considered stable.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 有些情况下 SentencePiece 的文本不稳定 $\rightarrow$ [6406]。当你查看表面文本子串“chair”时，它似乎映射到多个标记，但当你查看完整的表面术语“chairs”时，稳定性会恢复。这与字节级词汇表形成对比，在字节级词汇表中，文本“chair”总是映射到
    [102, 107, 100, 108, 117]，即使在文本“chairs”中，额外的 [118] 也会附加在末尾。尽管明显相关概念共享表示的丧失是不幸的，但基于这种标记化的现代模型的表现表明，模型处理得很好。虽然这些边缘情况存在，但它们足够少见，因此
    SentencePiece 分词器应该被视为稳定的。
- en: Similarly, there are cases where the initial token $\rightarrow$ is stable as
    no characters cross windows. Therefore, we consider EqualInfoAC stable enough
    to enable learnability by M2.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在初始标记 $\rightarrow$情况下，如果没有字符跨越窗口，则该标记是稳定的。因此，我们认为 EqualInfoAC 足够稳定，以使 M2
    能够进行学习。
- en: Interestingly, [[40](#bib.bib40)] point out this same issue, where a fixed size
    view of a variable length stream can cause false equivalencies when prefixes match.
    Similar to our findings, they find the models do have some limited ability to
    deal with these situations.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，[[40](#bib.bib40)] 指出同样的问题，其中固定大小的可变长度流视图可能在前缀匹配时导致错误的等效性。与我们的发现类似，他们发现模型确实具有处理这些情况的有限能力。
- en: Appendix M Window Text Patterns and Token Positions
  id: totrans-572
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 M 窗口文本模式和标记位置
- en: We tokenize $20$ of attested tokens appear more than once. [Table 16](#A13.T16
    "In Appendix M Window Text Patterns and Token Positions ‣ Training LLMs over Neurally
    Compressed Text") shows all the window text for repeated tokens.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 $20$ 个经证实的标记进行了标记，这些标记出现了多次。 [表 16](#A13.T16 "附录 M 窗口文本模式和标记位置 ‣ 在神经压缩文本上训练
    LLMs") 显示了重复标记的所有窗口文本。
- en: 'Table 16: The deduplicated window text from all instances of tokens that appear
    multiple times when we tokenized $20$.'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 表 16：标记化 $20$ 时出现多次的所有标记的去重窗口文本。
- en: '| Token | Window Position | Window Text |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | 窗口位置 | 窗口文本 |'
- en: '| $185$ | [or ] / [or a ] / [or ac] / [or al] / [or cr] / [or d] / [or f] /
    [or h] |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| $185$ | [or ] / [or a ] / [or ac] / [or al] / [or cr] / [or d] / [or f] /
    [or h] |'
- en: '|  |  | [or hi] / [or i] / [or k] / [or ma] / [or pr] / [or r] / [or s] / [or se]
    |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [or hi] / [or i] / [or k] / [or ma] / [or pr] / [or r] / [or s] / [or se]
    |'
- en: '|  |  | [or su] / [or t] / [or to] / [or v] / [or wha] / [or y] / [or yo] /
    [or, t] |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [or su] / [or t] / [or to] / [or v] / [or wha] / [or y] / [or yo] /
    [or, t] |'
- en: '|  |  | [or-] / [or.] / [ora] / [orc] / [orce ] / [ord] / [ord a] / [order]
    |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [or-] / [or.] / [ora] / [orc] / [orce ] / [ord] / [ord a] / [order]
    |'
- en: '|  |  | [ore a] / [ore e] / [ore ev] / [ore g] / [ore i] |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [ore a] / [ore e] / [ore ev] / [ore g] / [ore i] |'
- en: '|  | $2$ | [ 4] / [ of F] / [ records ] / [. Lo] / [Alt] / [OI] / [ase ] /
    [at y] |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$ | [ 4] / [ of F] / [ records ] / [. Lo] / [Alt] / [OI] / [ase ] /
    [at y] |'
- en: '|  |  | [cian] / [cri] / [d. I] / [ery] / [h de] / [hen s] / [ides] / [n ne]
    |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [cian] / [cri] / [d. I] / [ery] / [h de] / [hen s] / [ides] / [n ne]
    |'
- en: '|  |  | [oft] / [om i] / [onte] / [opp] / [pir] / [rev] / [reve] / [s may]
    |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [oft] / [om i] / [onte] / [opp] / [pir] / [rev] / [reve] / [s may]
    |'
- en: '|  |  | [tion a] / [y do] / [y t] |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [tion a] / [y do] / [y t] |'
- en: '| $151$ | [le] / [le s] / [le t] / [le. ] / [lea] / [lec] / [led] / [led ]
    |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| $151$ | [le] / [le s] / [le t] / [le. ] / [lea] / [lec] / [led] / [led ]
    |'
- en: '|  |  | [led t] / [leg] / [lege] / [leh] / [lem ] / [leme] / [lems] / [len]
    |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [led t] / [leg] / [lege] / [leh] / [lem ] / [leme] / [lems] / [len]
    |'
- en: '|  |  | [ler] / [les] / [less] / [let] / [lett] / [level] / [lew ] / [ley]
    / [lf ] |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [ler] / [les] / [less] / [let] / [lett] / [level] / [lew ] / [ley]
    / [lf ] |'
- en: '|  | $2$ | [ all ] / [ nut] / [ this] / [ un] / [. I w] / [Ni] / [as t] / [ceed ]
    |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '|  | $2$ | [ all ] / [ nut] / [ this] / [ un] / [. I w] / [Ni] / [as t] / [ceed ]
    |'
- en: '|  |  | [choos] / [e Mi] / [e-li] / [etti] / [imag] / [ion a] / [k a] / [ne a]
    |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [choos] / [e Mi] / [e-li] / [etti] / [imag] / [ion a] / [k a] / [ne a]
    |'
- en: '|  |  | [ng up] / [niversi] / [npo] / [nt pr] / [pi] / [rvices] / [s T] / [s your]
    |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [ng up] / [niversi] / [npo] / [nt pr] / [pi] / [rvices] / [s T] / [s your]
    |'
- en: '|  |  | [s?] / [so c] / [stag] / [thou] / [thoug] / [ust] / [ust ] |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [s?] / [so c] / [stag] / [thou] / [thoug] / [ust] / [ust ] |'
