- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:50:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LLM-FP4: 4-Bit Floating-Point Quantized Transformers'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM-FP4：4位浮点量化变换器
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.16836](https://ar5iv.labs.arxiv.org/html/2310.16836)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.16836](https://ar5iv.labs.arxiv.org/html/2310.16836)
- en: Shih-yang Liu^∗¹, Zechun Liu^∗², Xijie Huang¹, Pingcheng Dong¹, Kwang-Ting Cheng¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Shih-yang Liu^∗¹, Zechun Liu^∗², Xijie Huang¹, Pingcheng Dong¹, Kwang-Ting Cheng¹
- en: ¹Hong Kong University of Science and Technology, ²Meta Reality Labs
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹香港科技大学，²Meta Reality Labs
- en: '{sliuau, xhuangbs, pingcheng.dong}@connect.ust.hk'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{sliuau, xhuangbs, pingcheng.dong}@connect.ust.hk'
- en: zechunliu@meta.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: zechunliu@meta.com
- en: timcheng@ust.hk
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: timcheng@ust.hk
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'We propose LLM-FP4 for quantizing both weights and activations in large language
    models (LLMs) down to 4-bit floating-point values, in a post-training manner.
    Existing post-training quantization (PTQ) solutions are primarily integer-based
    and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point
    (FP) quantization is more flexible and can better handle long-tail or bell-shaped
    distributions, and it has emerged as a default choice in many hardware platforms.
    One characteristic of FP quantization is that its performance largely depends
    on the choice of exponent bits and clipping range. In this regard, we construct
    a strong FP-PTQ baseline by searching for the optimal quantization parameters.
    Furthermore, we observe a high inter-channel variance and low intra-channel variance
    pattern in activation distributions, which adds activation quantization difficulty.
    We recognize this pattern to be consistent across a spectrum of transformer models
    designed for diverse tasks, such as LLMs, BERT, and Vision Transformer models.
    To tackle this, we propose per-channel activation quantization and show that these
    additional scaling factors can be reparameterized as exponential biases of weights,
    incurring a negligible cost. Our method, for the first time, can quantize both
    weights and activations in the LLaMA-13B to only 4-bit and achieves an average
    score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8
    lower than the full-precision model, significantly outperforming the previous
    state-of-the-art by 12.7 points. Code is available at: [https://github.com/nbasyl/LLM-FP4](https://github.com/nbasyl/LLM-FP4).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了LLM-FP4，用于将大型语言模型（LLMs）的权重和激活量化到4位浮点值，采用训练后量化的方式。现有的训练后量化（PTQ）方案主要基于整数，并在低于8位的位宽下表现不佳。与整数量化相比，浮点（FP）量化更具灵活性，能更好地处理长尾或钟形分布，并且在许多硬件平台上已成为默认选择。FP量化的一个特征是其性能在很大程度上依赖于指数位和裁剪范围的选择。在这方面，我们通过寻找最佳量化参数构建了一个强大的FP-PTQ基线。此外，我们观察到激活分布中存在高通道间方差和低通道内方差的模式，这增加了激活量化的难度。我们发现这一模式在为多种任务设计的变换器模型中是一致的，例如LLMs、BERT和Vision
    Transformer模型。为了解决这个问题，我们提出了每通道激活量化，并展示了这些额外的缩放因子可以重新参数化为权重的指数偏置，且成本几乎可以忽略。我们的方法首次将LLaMA-13B的权重和激活量化到仅4位，并在常识零-shot推理任务中取得了63.1的平均分，仅比全精度模型低5.8分，显著超越了之前的最先进水平12.7分。代码可在：[https://github.com/nbasyl/LLM-FP4](https://github.com/nbasyl/LLM-FP4)获取。
- en: '^*^*footnotetext: These authors contributed equally to this work'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ^*^*脚注：这些作者对本研究做出了同等贡献
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Since the introduction of transformer architecture Vaswani et al. ([2017](#bib.bib27)),
    transformers have superseded recursive neural networks, emerging as the dominant
    architecture in numerous natural language processing (NLP) tasks Kenton and Toutanova
    ([2019](#bib.bib13)); Lewis et al. ([2020](#bib.bib16)). The transformative impact
    of the transformer has been further propelled by the emergence of models like
    GPT Brown et al. ([2020](#bib.bib4)); OpenAI ([2023](#bib.bib23)), catapulting
    the popularity of this architecture to new heights. Meanwhile, the versatility
    of transformers extends beyond NLP, encompassing diverse domains such as vision [Dosovitskiy
    et al.](#bib.bib10) ; Touvron et al. ([2021](#bib.bib25)), audio Akbari et al.
    ([2021](#bib.bib1)), etc. This trend towards a unified architecture for different
    modalities represents a groundbreaking development within the realm of deep learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自从变换器架构由Vaswani等人（[2017](#bib.bib27)）引入以来，变换器已取代递归神经网络，成为众多自然语言处理（NLP）任务中的主导架构 Kenton和Toutanova（[2019](#bib.bib13)）；Lewis等人（[2020](#bib.bib16)）。变换器的变革性影响随着像GPT Brown等人（[2020](#bib.bib4)）；OpenAI（[2023](#bib.bib23)）等模型的出现而进一步推动，将这种架构的流行度提升到新的高度。同时，变换器的多功能性不仅限于NLP，还涵盖了诸如视觉 [Dosovitskiy等人](#bib.bib10)；Touvron等人（[2021](#bib.bib25)）、音频 Akbari等人（[2021](#bib.bib1)）等不同领域。这种面向不同模态的统一架构趋势代表了深度学习领域中的一个突破性发展。
- en: However, the advancements in transformer performance are accompanied by a corresponding
    increase in model size and computational costs Kaplan et al. ([2020](#bib.bib12)).
    This poses significant challenges when attempting to leverage the full potential
    of transformer models in use cases where memory or computational resources are
    limited. Despite the extensive research and widespread adoption of transformers,
    the field of transformer compression remains relatively underexplored. To address
    this gap, our study focuses on the compression of transformers, especially through
    floating-point post-training quantization techniques.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，变换器性能的提升伴随着模型规模和计算成本的相应增加 Kaplan等人（[2020](#bib.bib12)）。这在试图在内存或计算资源有限的用例中充分利用变换器模型的全部潜力时，带来了重大挑战。尽管对变换器进行了广泛的研究和广泛应用，但变换器压缩领域仍然相对欠缺探索。为了解决这一问题，我们的研究集中于变换器的压缩，特别是通过浮点后训练量化技术进行压缩。
- en: Post-training quantization (PTQ) offers the advantages of simple to use with
    minimal fine-tuning requirements Nagel et al. ([2020](#bib.bib21)); Cai et al.
    ([2020](#bib.bib5)). Existing PTQ solutions for transformers primarily focus on
    integer (INT) quantization Liu et al. ([2021](#bib.bib19)); Yuan et al. ([2022](#bib.bib32)),
    which can be effective in certain scenarios but often break down when bit widths
    are below 8 bit. On the other hand, floating-point (FP) quantization has gained
    significant traction as a more flexible alternative, capable of better accommodating
    various activation and weight distributions. In fact, FP8 has emerged as the default
    choice in various hardware platforms, including the NVIDIA H100.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化（PTQ）具有使用简单、微调要求少的优点 Nagel等人（[2020](#bib.bib21)）；Cai等人（[2020](#bib.bib5)）。现有的变换器PTQ解决方案主要关注整数（INT）量化 Liu等人（[2021](#bib.bib19)）；Yuan等人（[2022](#bib.bib32)），这种方法在某些场景下有效，但当位宽低于8位时，往往效果不佳。另一方面，浮点（FP）量化作为一种更灵活的替代方案，能够更好地适应各种激活和权重分布。实际上，FP8已成为各种硬件平台上的默认选择，包括NVIDIA
    H100。
- en: Different from integer (INT) quantization, a particular challenge in floating-point
    (FP) quantization is how to select appropriate exponent bits and scale parameters.
    Improper parameter choices can lead to subpar or divergent quantization results.
    To tackle this challenge, we introduce a robust recipe for FP quantization, which
    leverage layer-wise reconstruction to jointly search for optimal exponent bits
    and maximum values. Compared to previous approaches that utilize gradient updates
    for exponent bits Kuzmin et al. ([2022](#bib.bib14)), our search-based method
    proves to be more stable and consistently delivers desirable quantization results,
    which establishes a strong baseline for FP-PTQ.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与整数（INT）量化不同，浮点（FP）量化的一个特别挑战是如何选择合适的指数位和缩放参数。不正确的参数选择可能导致较差或发散的量化结果。为了解决这个挑战，我们引入了一种强健的FP量化方案，该方案利用逐层重建来联合搜索最佳指数位和最大值。与之前利用梯度更新指数位的方法 Kuzmin等人（[2022](#bib.bib14)）相比，我们的基于搜索的方法证明了其更稳定，并
    consistently 提供了令人满意的量化结果，为FP-PTQ建立了强有力的基准。
- en: Furthermore, our investigation uncovers an intriguing pattern of activation
    distributions in transformers, characterized by high inter-channel variance and
    low intra-channel variance. Similar patterns are also observed in previous works Xiao
    et al. ([2022](#bib.bib31)); Dettmers et al. ([2022](#bib.bib7)), while we argue
    that this pattern is inherent to transformer architectures and not limited to
    specific tasks, as we have observed consistent patterns not only in large language
    models but also in BERT model and even vision transformers. Motivated by these
    findings, we introduce a novel pre-shifted exponent bias for FP quantization of
    transformers. Concretely, we leverage the per-channel activation variance computed
    from calibration data and reparameterize these scales as the exponential bias
    of the corresponding FP quantized weight vectors. This approach effectively addresses
    the challenge posed by high inter-channel variance while incurring negligible
    computational cost.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的研究揭示了变压器中的激活分布的一个有趣模式，特征是高通道间方差和低通道内方差。类似的模式也在之前的工作中观察到 Xiao et al. ([2022](#bib.bib31))；Dettmers
    et al. ([2022](#bib.bib7))，但我们认为这种模式是变压器架构固有的，并不限于特定任务，因为我们观察到的一致模式不仅存在于大型语言模型中，还存在于BERT模型甚至视觉变压器中。受到这些发现的启发，我们引入了一种用于变压器FP量化的新型预移位指数偏差。具体来说，我们利用从校准数据中计算出的每通道激活方差，将这些尺度重新参数化为相应FP量化权重向量的指数偏差。这种方法有效地解决了高通道间方差带来的挑战，同时计算成本几乎可以忽略不计。
- en: 'In summary, we study floating-point post-training quantization (PTQ) for transformer
    architectures, and the contribution of this paper includes:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们研究了变压器架构的浮点后训练量化（PTQ），本文的贡献包括：
- en: $\bullet$ We propose a search-based framework for determining the optimal exponent
    bias and maximal quantization value. This method outperforms existing techniques
    in terms of stability and performance, establishing a strong baseline for floating-point
    post-training quantization.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 我们提出了一种基于搜索的框架来确定最优指数偏差和最大量化值。这种方法在稳定性和性能方面优于现有技术，为浮点后训练量化建立了强有力的基准。
- en: $\bullet$ We propose a novel technique, pre-shifted exponent bias, which effectively
    addresses the challenge of high inter-channel variance in the transformer with
    negligible computational overhead.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 我们提出了一种新型技术，预移位指数偏差，它有效地解决了变压器中的高通道间方差问题，计算开销几乎可以忽略不计。
- en: $\bullet$70% compared to the previous SoTA.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 相较于之前的SoTA提高了70%。
- en: $\bullet$ We further extend our method to BERT and vision transformers. It surpasses
    the previous best 4-bit quantized BERT by 7.8 points on GLUE dataset and achieves
    31.4 points higher accuracy compared to the previous SoTA ViT quantization method
    for 4-bit DeiT-S on ImageNet dataset.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 我们进一步将我们的方法扩展到BERT和视觉变压器。它在GLUE数据集上超越了之前最佳的4位量化BERT 7.8分，并且在ImageNet数据集上与之前的SoTA
    ViT量化方法相比，4位DeiT-S的准确率提高了31.4分。
- en: 2 Related Works
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Post-Training Quantization
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 后训练量化
- en: Model quantization can be mainly categorized into quantization-aware training
    (QAT) and post-training quantization (PTQ), depending on whether it involves additional
    training for weight fine-tuning or not. Most PTQ studies are primarily focused
    on convolutional neural networks (CNNs) Nagel et al. ([2020](#bib.bib21)); Li
    et al. ([2021](#bib.bib17)); Wu et al. ([2020](#bib.bib30)); Cai et al. ([2020](#bib.bib5));
    Nagel et al. ([2019](#bib.bib22)). However, with the growing popularity of transformer-based
    models, only a limited number of works Bondarenko et al. ([2021](#bib.bib3));
    Yuan et al. ([2022](#bib.bib32)); Ding et al. ([2022](#bib.bib9)) have been conducted
    to realize PTQ on transformers. Moreover, the existing works primarily focus on
    visual transformer models and exhibit inferior performance when the bit width
    is below 8\. Therefore, in this work, we delve into the challenges of the low-bit
    PTQ for language transformers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 模型量化主要可以分为量化感知训练（QAT）和后训练量化（PTQ），具体取决于是否涉及额外的权重微调训练。大多数PTQ研究主要集中在卷积神经网络（CNNs）上
    Nagel et al. ([2020](#bib.bib21))；Li et al. ([2021](#bib.bib17))；Wu et al. ([2020](#bib.bib30))；Cai
    et al. ([2020](#bib.bib5))；Nagel et al. ([2019](#bib.bib22))。然而，随着基于变压器模型的日益流行，只有少数研究
    Bondarenko et al. ([2021](#bib.bib3))；Yuan et al. ([2022](#bib.bib32))；Ding et
    al. ([2022](#bib.bib9)) 已经开展了变压器的PTQ工作。此外，现有的工作主要集中在视觉变压器模型上，并且当位宽低于8位时表现较差。因此，在这项工作中，我们*深入探讨*了语言变压器的低位PTQ挑战。
- en: 2.2 Floating-Point Quantization
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 浮点量化
- en: Floating-point (FP) quantization has emerged as a promising alternative to integer
    quantization due to its ability to handle long-tail distributions, and offers
    increased flexibility Kuzmin et al. ([2022](#bib.bib14)). Additionally, modern
    GPUs such as H100 Micikevicius et al. ([2022](#bib.bib20)) now support FP quantization.
    Nonetheless, minimal research has been conducted on FP quantization. Only Kuzmin
    et al. ([2022](#bib.bib14)) proposes a general FP8 quantization scheme primarily
    for vision tasks, and Zhang et al. ([2023](#bib.bib33)) adopts a mixture of FP
    and INT formats quantization for LLMs. In this work, we propose FPQ baseline as
    a general guideline for low-bit floating-point PTQ to compress language transformer
    models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 浮点（FP）量化由于其处理长尾分布的能力，已成为整数量化的有前景的替代方案，并提供了更大的灵活性 Kuzmin 等人 ([2022](#bib.bib14))。此外，现代
    GPU 如 H100 Micikevicius 等人 ([2022](#bib.bib20)) 现在支持 FP 量化。然而，关于 FP 量化的研究仍然很少。只有
    Kuzmin 等人 ([2022](#bib.bib14)) 提出了主要用于视觉任务的通用 FP8 量化方案，而 Zhang 等人 ([2023](#bib.bib33))
    则采用了混合 FP 和 INT 格式量化的 LLM。在这项工作中，我们提出了 FPQ 基线作为低比特浮点 PTQ 的通用指导方针，用于压缩语言变换器模型。
- en: 3 Preliminaries
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 初步知识
- en: 3.1 Formulation of Floating-Point Variables
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 浮点变量的表述
- en: 'A standard floating-point number is represented as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 标准浮点数表示为：
- en: '|  | $X_{\rm{FP}}=(-1)^{s}2^{p-b}(1+\frac{d_{1}}{2}+\frac{d_{2}}{2^{2}}+...+\frac{d_{m}}{2^{m}})$
    |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $X_{\rm{FP}}=(-1)^{s}2^{p-b}(1+\frac{d_{1}}{2}+\frac{d_{2}}{2^{2}}+...+\frac{d_{m}}{2^{m}})$
    |  | (1) |'
- en: where $s\in\{0,1\}$.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s\in\{0,1\}$。
- en: '![Refer to caption](img/7c4ae43273e13a2b585456ae7d11f7ed.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7c4ae43273e13a2b585456ae7d11f7ed.png)'
- en: 'Figure 1: An illustration of floating-point (FP) quantization process using
    FP5 (E2M2) positive axis. The real-valued clipped $X_{\rm R}^{\prime\prime}$ (Eq. [8](#S3.E8
    "In 3.2 Floating-Point Quantization Process ‣ 3 Preliminaries ‣ LLM-FP4: 4-Bit
    Floating-Point Quantized Transformers")).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1：使用 FP5（E2M2）正轴的浮点（FP）量化过程的示意图。实值剪裁的 $X_{\rm R}^{\prime\prime}$（公式[8](#S3.E8
    "在 3.2 浮点量化过程 ‣ 3 初步知识 ‣ LLM-FP4: 4 位浮点量化变换器")）。'
- en: 3.2 Floating-Point Quantization Process
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 浮点量化过程
- en: 'In integer quantization, the real-valued variable $X_{\rm R}$ with the following
    formula:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在整数量化中，实值变量 $X_{\rm R}$ 使用以下公式：
- en: '|  |  | $\displaystyle X_{\rm INT}=\alpha\!\left\lfloor{\rm Clip}\!\left(\frac{X_{\rm
    R}}{\alpha},Q_{min},Q_{max}\!\right)\right\rceil$ |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle X_{\rm INT}=\alpha\!\left\lfloor{\rm Clip}\!\left(\frac{X_{\rm
    R}}{\alpha},Q_{min},Q_{max}\!\right)\right\rceil$ |  | (2) |'
- en: where $\lfloor\cdot\rceil$ in two steps.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lfloor\cdot\rceil$ 分为两个步骤。
- en: '(1) Scale and clip. In FP quantization, we also scale and clip the real-valued
    variable before quantization as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 缩放和剪裁。在 FP 量化中，我们还在量化之前对实值变量进行缩放和剪裁，如下所示：
- en: '|  | $\displaystyle\vspace{-0.5em}X_{\rm R}^{\prime}={\rm Clip}\!\left(X_{\rm
    R},Q_{min},Q_{max}\!\right)$ |  | (3) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\vspace{-0.5em}X_{\rm R}^{\prime}={\rm Clip}\!\left(X_{\rm
    R},Q_{min},Q_{max}\!\right)$ |  | (3) |'
- en: 'where the min/max value range of signed floating-point quantization can be
    calculated from Eq.[1](#S3.E1 "In 3.1 Formulation of Floating-Point Variables
    ‣ 3 Preliminaries ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers"):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，带符号浮点量化的最小/最大值范围可以从公式[1](#S3.E1 "在 3.1 浮点变量的表述 ‣ 3 初步知识 ‣ LLM-FP4: 4 位浮点量化变换器")
    计算得出：'
- en: '|  | $Q_{max}=-Q_{min}=(2-2^{-m})2^{2^{e}-b-1}$ |  | (4) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q_{max}=-Q_{min}=(2-2^{-m})2^{2^{e}-b-1}$ |  | (4) |'
- en: 'Here the integer exponent bias $b$. Therefore, for simplicity, we reformulate
    Eq. [3](#S3.E3 "In 3.2 Floating-Point Quantization Process ‣ 3 Preliminaries ‣
    LLM-FP4: 4-Bit Floating-Point Quantized Transformers") as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '这里是整数指数偏差 $b$。因此，为了简化，我们将公式[3](#S3.E3 "在 3.2 浮点量化过程 ‣ 3 初步知识 ‣ LLM-FP4: 4 位浮点量化变换器")
    重新表述为：'
- en: '|  | $\displaystyle\vspace{-0.4em}X_{\rm R}^{\prime\prime}={\rm Clip}\!\left(X_{\rm
    R},\tilde{Q}_{min},\tilde{Q}_{max}\!\right),$ |  | (5) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\vspace{-0.4em}X_{\rm R}^{\prime\prime}={\rm Clip}\!\left(X_{\rm
    R},\tilde{Q}_{min},\tilde{Q}_{max}\!\right),$ |  | (5) |'
- en: where
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $\displaystyle\vspace{-0.4em}\tilde{Q}_{max}=\alpha Q_{max}$ |  | (6)
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\vspace{-0.4em}\tilde{Q}_{max}=\alpha Q_{max}$ |  | (6)
    |'
- en: '|  |  | $\displaystyle=\alpha\cdot 2^{-b}\cdot(2-2^{-m})2^{2^{e}-0-1}$ |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\alpha\cdot 2^{-b}\cdot(2-2^{-m})2^{2^{e}-0-1}$ |  |'
- en: '|  |  | $\displaystyle=2^{-\tilde{b}}\cdot(2-2^{-m})2^{2^{e}-0-1}$ |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=2^{-\tilde{b}}\cdot(2-2^{-m})2^{2^{e}-0-1}$ |  |'
- en: 'Note that we combine the tensor-wise real-valued scaling factor $\alpha$ from
    Eq. [6](#S3.E6 "In 3.2 Floating-Point Quantization Process ‣ 3 Preliminaries ‣
    LLM-FP4: 4-Bit Floating-Point Quantized Transformers") as:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，我们将公式[6](#S3.E6 "在 3.2 浮点量化过程 ‣ 3 初步知识 ‣ LLM-FP4: 4 位浮点量化变换器") 中的张量级实值缩放因子
    $\alpha$ 组合为：'
- en: '|  | $\vspace{-0.4em}\tilde{b}=2^{e}-{\rm log}_{2}{\tilde{Q}_{max}}+{\rm log}_{2}({2-2^{-m}})-1$
    |  | (7) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\vspace{-0.4em}\tilde{b}=2^{e}-{\rm log}_{2}{\tilde{Q}_{max}}+{\rm log}_{2}({2-2^{-m}})-1$
    |  | (7) |'
- en: '(2) Compare and quantize. Different from integer quantization, which simply
    utilizes the rounding function to convert the real-valued variables to quantized
    ones, in floating-point quantization, there is an additional step of comparing
    $X_{\rm R}^{\prime\prime}$ with quantization levels and then quantize:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 比较和量化。不同于整数量化，它仅使用舍入函数将实值变量转换为量化值，在浮点量化中，存在额外的步骤，即将 $X_{\rm R}^{\prime\prime}$
    与量化级别进行比较，然后进行量化：
- en: '|  | $\displaystyle\vspace{-1em}X_{\rm{FP}}=\tilde{\alpha}\cdot v\cdot\left\lfloor\frac{X_{\rm
    R}^{\prime\prime}}{\tilde{\alpha}\cdot v}\right\rceil\vspace{-1em}$ |  | (8) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\vspace{-1em}X_{\rm{FP}}=\tilde{\alpha}\cdot v\cdot\left\lfloor\frac{X_{\rm
    R}^{\prime\prime}}{\tilde{\alpha}\cdot v}\right\rceil\vspace{-1em}$ |  | (8) |'
- en: where $X_{\rm R}^{\prime\prime}$ is an integer power of 2.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $X_{\rm R}^{\prime\prime}$ 是 2 的整数次幂。
- en: '|  | $1$2 |  | (11) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (11) |'
- en: 'Here we select the quantization level $v$. Then the floating-point quantized
    variables can be derived with Eq.[8](#S3.E8 "In 3.2 Floating-Point Quantization
    Process ‣ 3 Preliminaries ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers").
    The illustration of the quantization process is in Fig. [1](#S3.F1 "Figure 1 ‣
    3.1 Formulation of Floating-Point Variables ‣ 3 Preliminaries ‣ LLM-FP4: 4-Bit
    Floating-Point Quantized Transformers"), detailed explanation can also be found
    in Micikevicius et al. ([2022](#bib.bib20)).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里我们选择量化级别 $v$。然后，浮点量化变量可以通过公式[8](#S3.E8 "在 3.2 浮点量化过程 ‣ 3 基础知识 ‣ LLM-FP4:
    4 位浮点量化变换器")推导出。量化过程的插图见图[1](#S3.F1 "图 1 ‣ 3.1 浮点变量的公式 ‣ 3 基础知识 ‣ LLM-FP4: 4 位浮点量化变换器")，详细解释也可以在
    Micikevicius 等人 ([2022](#bib.bib20)) 中找到。'
- en: 3.3 Floating-Point Matrix Multiplication
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 浮点矩阵乘法
- en: 'With the floating-point quantized variables, the matrix multiplication is formulated
    as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用浮点量化变量，矩阵乘法被公式化为：
- en: '|  | $1$2 |  | (12) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (12) |'
- en: 'Here in per-tensor activation quantization and per-channel weight quantization,
    $\mathbf{X}_{\rm FP}^{i,:}$ times the corresponding quantized activation and weight
    vectors. We depict all the possible quantization granularity options that support
    such efficient matrix multiplication in Appendix [D](#A4 "Appendix D Efficient
    Matrix Multiplication ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '在每个张量激活量化和每通道权重量化中，$\mathbf{X}_{\rm FP}^{i,:}$ 乘以相应的量化激活和权重向量。我们在附录[D](#A4
    "附录 D 高效矩阵乘法 ‣ LLM-FP4: 4位浮点量化变换器")中描述了所有支持高效矩阵乘法的量化粒度选项。'
- en: 4 Method
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法
- en: In this section, we begin by introducing our joint format and max value search,
    which establishes our strong baseline and already achieves state-of-the-art results
    at 8-bit and 6-bit quantization. Then we present an efficient pre-shifted exponent
    bias to tackle the catastrophic high inter-channel activation variance in transformer
    models and push the quantization limit to 4-bit.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们首先介绍了我们的联合格式和最大值搜索，这建立了我们强大的基线，并且在 8 位和 6 位量化中已经实现了最先进的结果。然后，我们展示了一种高效的预移位指数偏差，以解决变换器模型中灾难性的高通道间激活方差问题，并将量化极限推向
    4 位。
- en: 4.1 Joint Format and Max Value Search
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 联合格式和最大值搜索
- en: 'The objective of post-training quantization is to minimize the perturbation
    ($\delta\mathbf{X}=\mathbf{X}_{\rm FP}-\mathbf{X}_{\rm R}$) introduced by quantization
    to the pre-trained real-valued network:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化的目标是最小化量化引入的扰动 ($\delta\mathbf{X}=\mathbf{X}_{\rm FP}-\mathbf{X}_{\rm R}$)
    对预训练实值网络的影响：
- en: '|  | ${\rm min}\ \mathbb{E}[\mathcal{L}(\mathbf{X}_{\rm R}+\delta\mathbf{X})-\mathcal{L}(\mathbf{X}_{\rm
    R})]$ |  | (13) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\rm min}\ \mathbb{E}[\mathcal{L}(\mathbf{X}_{\rm R}+\delta\mathbf{X})-\mathcal{L}(\mathbf{X}_{\rm
    R})]$ |  | (13) |'
- en: 'In this study, we adopt the setting presented in Choukroun et al. ([2019](#bib.bib6));
    Wu et al. ([2020](#bib.bib30)), which assumes a positive correlation between the
    change in the intermediate output of the quantized model and Eq. [13](#S4.E13
    "In 4.1 Joint Format and Max Value Search ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers"). Therefore, minimizing the distance between the intermediate
    output of the quantized layer ($\hat{\mathbf{O}}$) leads to minimize Eq. [13](#S4.E13
    "In 4.1 Joint Format and Max Value Search ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers"). Hence, the objective loss metric is formulated as:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '在本研究中，我们采用了Choukroun等人([2019](#bib.bib6))和Wu等人([2020](#bib.bib30))提出的设置，这些设置假设量化模型中间输出的变化与方程[13](#S4.E13
    "In 4.1 Joint Format and Max Value Search ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers")之间存在正相关。因此，最小化量化层中间输出（$\hat{\mathbf{O}}$）之间的距离将最小化方程[13](#S4.E13
    "In 4.1 Joint Format and Max Value Search ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers")。因此，目标损失度量被公式化为：'
- en: '|  | ${\rm min}\ (\hat{\mathbf{O}}-\mathbf{O})^{2}$ |  | (14) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\rm min}\ (\hat{\mathbf{O}}-\mathbf{O})^{2}$ |  | (14) |'
- en: which is used to search for the optimal FP quantization function in the following
    proposed framework.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在以下提出的框架中搜索最佳FP量化函数。
- en: 'The challenges in FP quantization arise from its sensitivity to the quantization
    format and clipping range. Undesirable format selection will result in a catastrophic
    error rate. In addition, we observe that the optimal clipping range varies depending
    on the format used. Previous work Kuzmin et al. ([2022](#bib.bib14)) on floating-point
    (FP) quantization-aware training (QAT) proposed to learn both the FP format and
    maximum value with gradients. However, we find this method suffers from over-fitting
    in PTQ, with accuracy being even worse than naïve MinMax method, details can be
    found in Appendix [E](#A5 "Appendix E Learning Format and Maximum Value ‣ LLM-FP4:
    4-Bit Floating-Point Quantized Transformers"). Instead, we propose a search-based
    algorithm that jointly determines the optimal format and its associated clipping
    range to address this challenge.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 'FP量化中的挑战来自于对量化格式和裁剪范围的敏感性。不良的格式选择将导致灾难性的错误率。此外，我们观察到最佳裁剪范围因使用的格式而异。以往的工作Kuzmin等人([2022](#bib.bib14))提出了通过梯度学习FP格式和最大值的浮点（FP）量化感知训练（QAT）。然而，我们发现这种方法在PTQ中容易过拟合，准确度甚至比简单的MinMax方法还差，详细信息见附录[E](#A5
    "Appendix E Learning Format and Maximum Value ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers")。因此，我们提出了一种基于搜索的算法，以共同确定最佳格式及其相关的裁剪范围来解决这一挑战。'
- en: 'The searching process is conducted layer by layer with the metric of minimizing
    Eq. [14](#S4.E14 "In 4.1 Joint Format and Max Value Search ‣ 4 Method ‣ LLM-FP4:
    4-Bit Floating-Point Quantized Transformers"). The output of matrix multiplication
    corresponding to each sub-module is denoted as $\mathbf{O}=\mathbf{X}\mathbf{Y}$
    or another activation tensor.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '搜索过程按层进行，以最小化方程[14](#S4.E14 "In 4.1 Joint Format and Max Value Search ‣ 4 Method
    ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers")的度量为准。矩阵乘法对应的每个子模块的输出记作$\mathbf{O}=\mathbf{X}\mathbf{Y}$或其他激活张量。'
- en: The search space of $q$.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: $q$的搜索空间。
- en: 'The search process is outlined in Alg.[1](#alg1 "Algorithm 1 ‣ 4.1 Joint Format
    and Max Value Search ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers").
    We search the quantization scheme in all the matrix multiplication layers in parallel
    following Yuan et al. ([2022](#bib.bib32)); Bai et al. ([2022](#bib.bib2)). The
    algorithm can be divided into two parts. (1) Do forward propagation to store the
    intermediate raw output of each layer $l$. (2) Iteratively update the optimal
    format and biases for each layer for three rounds by minimizing the reconstruction
    metric (Eq. [14](#S4.E14 "In 4.1 Joint Format and Max Value Search ‣ 4 Method
    ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers")). We name this search-based
    framework as Floating Point Quantization Baseline (FPQ baseline), and it can already
    achieve state-of-the-art results on both 8-bit and 6-bit settings.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '搜索过程在算法[1](#alg1 "Algorithm 1 ‣ 4.1 Joint Format and Max Value Search ‣ 4 Method
    ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers")中进行了概述。我们在所有矩阵乘法层中并行搜索量化方案，参考了Yuan等人([2022](#bib.bib32))和Bai等人([2022](#bib.bib2))。该算法可分为两个部分。(1)
    执行前向传播以存储每层$l$的中间原始输出。(2) 通过最小化重建度量（方程[14](#S4.E14 "In 4.1 Joint Format and Max
    Value Search ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers")）迭代更新每层的最佳格式和偏差三轮。我们将此基于搜索的框架命名为浮点量化基线（FPQ
    baseline），它在8位和6位设置下均能实现最先进的结果。'
- en: Algorithm 1 FPQ baseline
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 FPQ基线
- en: '1:Input: Calibration dataset, Full-precision Model $M$ that minimizes Eq.[14](#S4.E14
    "In 4.1 Joint Format and Max Value Search ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers")15:     end for16:end for'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '1:输入：标定数据集，最小化方程式 Eq.[14](#S4.E14 "在 4.1 联合格式和最大值搜索 ‣ 4 方法 ‣ LLM-FP4: 4 位浮点量化变换器")
    的全精度模型 $M$15:     结束 for16:结束 for'
- en: 4.2 Pre-Shifted Exponent Bias
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 预偏移指数偏差
- en: 'In transformer architectures, we observed an intriguing phenomenon of high
    inter-channel variance. As shown in Fig.[2](#S4.F2 "Figure 2 ‣ 4.2 Pre-Shifted
    Exponent Bias ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers"),
    the magnitudes of values within the same channel are close to each other but exhibit
    significant differences across different channels. This phenomenon is not only
    observed in language models (i.e., LLaMA and BERT) but also significant in vision
    transformer models. Since outlier channels are often orders of magnitude bigger
    than the rest, they will dominate the quantization precision of the quantized
    tensor, resulting in less representation capacity for those channels with smaller
    magnitudes Xiao et al. ([2022](#bib.bib31)). This makes tensor-wise or token-wise
    scaling factor insufficient for accurate activations quantization.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '在变换器架构中，我们观察到了一个有趣的现象：高通道间方差。如图[2](#S4.F2 "图 2 ‣ 4.2 预偏移指数偏差 ‣ 4 方法 ‣ LLM-FP4:
    4 位浮点量化变换器")所示，同一通道内的值的幅度接近，但不同通道之间存在显著差异。这个现象不仅在语言模型（即 LLaMA 和 BERT）中观察到，在视觉变换器模型中也很明显。由于异常通道通常比其他通道大几个数量级，它们会主导量化张量的量化精度，导致幅度较小的通道的表示能力较差
    Xiao 等人 ([2022](#bib.bib31))。这使得按张量或按标记的缩放因子对于准确的激活量化来说是不够的。'
- en: 'However, applying per-channel scaling factors for activations poses challenges
    to efficient matrix multiplication, because the scaling factor is not a shared
    constant along the multiplication direction and cannot be extracted as Eq. [12](#S3.E12
    "In 3.3 Floating-Point Matrix Multiplication ‣ 3 Preliminaries ‣ LLM-FP4: 4-Bit
    Floating-Point Quantized Transformers"). To address this challenge, we introduce
    pre-shifted exponent bias, which allows us to calculate per-channel scaling factors
    from activations. These scaling factors are then re-parameterized as the exponent
    biases of the corresponding weights. This method effectively handles high inter-channel
    variance while maintaining nearly identical efficiency to per-tensor quantization.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，应用每通道缩放因子来处理激活会对高效的矩阵乘法带来挑战，因为缩放因子不是沿着乘法方向的共享常数，不能像方程式 [12](#S3.E12 "在 3.3
    浮点矩阵乘法 ‣ 3 基础 ‣ LLM-FP4: 4 位浮点量化变换器") 中那样提取。为了解决这一挑战，我们引入了预偏移指数偏差，这使我们能够从激活中计算每通道的缩放因子。然后，这些缩放因子被重新参数化为相应权重的指数偏差。这种方法有效地处理了高通道间方差，同时保持了与按张量量化几乎相同的效率。'
- en: 'Recalling in Eq. [7](#S3.E7 "In 3.2 Floating-Point Quantization Process ‣ 3
    Preliminaries ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers"), we extracted
    the tensor-wise integer exponent bias $b$. Then, the floating-point quantization
    formula in Eq. [15](#S4.E15 "In 4.2 Pre-Shifted Exponent Bias ‣ 4 Method ‣ LLM-FP4:
    4-Bit Floating-Point Quantized Transformers") becomes:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '回忆在方程式 [7](#S3.E7 "在 3.2 浮点量化过程 ‣ 3 基础 ‣ LLM-FP4: 4 位浮点量化变换器") 中，我们提取了张量级整数指数偏差
    $b$。然后，方程式 [15](#S4.E15 "在 4.2 预偏移指数偏差 ‣ 4 方法 ‣ LLM-FP4: 4 位浮点量化变换器") 中的浮点量化公式变为：'
- en: '|  | $\vspace{-0.4em}\!\!X_{\rm FP}\!=\!2^{-\tilde{b}}(-1)^{s}2^{p-0}(1+\!\frac{d_{1}}{2}+\frac{d_{2}}{2^{2}}+...+\frac{d_{m}}{2^{m}})$
    |  | (15) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\vspace{-0.4em}\!\!X_{\rm FP}\!=\!2^{-\tilde{b}}(-1)^{s}2^{p-0}(1+\!\frac{d_{1}}{2}+\frac{d_{2}}{2^{2}}+...+\frac{d_{m}}{2^{m}})$
    |  | (15) |'
- en: We note that after the bias is absorbed in the scaling factor, the original
    bias term ($b^{ori}$).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，在缩放因子中吸收了偏差后，原始偏差项 ($b^{ori}$)。
- en: '![Refer to caption](img/75dcd1de36ce945c770391fc687f66bc.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/75dcd1de36ce945c770391fc687f66bc.png)'
- en: 'Figure 2: Magnitude of the output activations of the feed-forward network blocks
    in LLaMA-7B, BERT, and DeiT.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLaMA-7B、BERT 和 DeiT 中前馈网络块的输出激活幅度。
- en: '![Refer to caption](img/75a05aade796e55f730e4f5517d51ae5.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/75a05aade796e55f730e4f5517d51ae5.png)'
- en: 'Figure 3: Overview of pre-shifted exponent bias method: (a) Search phase: The
    real-valued channel-wise scaling exponent bias for activations ($\tilde{\mathbf{b}}_{j}$
    are re-parameterized into the weight tensor. The weights are pre-computed to apply
    the bias, therefore this is a one-time cost. (c) Inference phase: The method leverages
    efficient matrix multiplication between low-bit floating-point matrices.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：预移位指数偏置方法概述：（a）搜索阶段：激活的实值通道级缩放指数偏置（$\tilde{\mathbf{b}}_{j}$）被重新参数化到权重张量中。权重被预计算以应用偏置，因此这是一次性成本。（c）推理阶段：该方法利用低位浮点矩阵之间的高效矩阵乘法。
- en: 'Then the calculation of the channel-wise integer bias vector ($\mathbf{b}^{ori}$)
    from the per-channel maximum values:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从每个通道的最大值计算通道级整数偏置向量（$\mathbf{b}^{ori}$）：
- en: '|  | $\vspace{-0.4em}\!\!\!\tilde{\mathbf{b}}_{j}\!=\!2^{e}\!-\!{\rm log}_{2}({{\rm
    max}(&#124;\mathbf{X}^{:,j}_{\rm R}&#124;})\!)\!+\!{\rm log}_{2}({2\!-\!2^{-m}})\!-\!1$
    |  | (16) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $\vspace{-0.4em}\!\!\!\tilde{\mathbf{b}}_{j}\!=\!2^{e}\!-\!{\rm log}_{2}({{\rm
    max}(&#124;\mathbf{X}^{:,j}_{\rm R}&#124;})\!)\!+\!{\rm log}_{2}({2\!-\!2^{-m}})\!-\!1$
    |  | (16) |'
- en: 'Here $\mathbf{X}^{:,j}_{\rm R}$ to a tensor-wise real-valued scaling factor
    plus a channel-wise integer scaling factor:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $\mathbf{X}^{:,j}_{\rm R}$ 变为张量级实值缩放因子加上通道级整数缩放因子：
- en: '|  | $\displaystyle\vspace{-0.4em}\tilde{\mathbf{b}}$ |  | (17) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\vspace{-0.4em}\tilde{\mathbf{b}}$ |  | (17) |'
- en: '|  |  | $\displaystyle=\tilde{\rho}+clip(\lfloor\tilde{\mathbf{b}}-\tilde{\rho}\rceil,0,2^{e-1})$
    |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\tilde{\rho}+clip(\lfloor\tilde{\mathbf{b}}-\tilde{\rho}\rceil,0,2^{e-1})$
    |  |'
- en: 'where $\tilde{\rho}\in\mathbb{R}^{1}$ can be rewrote as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tilde{\rho}\in\mathbb{R}^{1}$ 可以重写如下：
- en: '|  | $\displaystyle\vspace{-0.4em}\!\!X_{\rm FP}$ |  | (18) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\vspace{-0.4em}\!\!X_{\rm FP}$ |  | (18) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'Note that the bias $\mathbf{b}^{ori}$ becomes:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，偏置 $\mathbf{b}^{ori}$ 变为：
- en: '|  | $\displaystyle\!\!X_{\rm FP}$ |  | (19) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\!\!X_{\rm FP}$ |  | (19) |'
- en: 'and the corresponding weight element in $j^{th}$ becomes:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的 $j^{th}$ 权重元素变为：
- en: '|  | $1$2 |  | (20) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (20) |'
- en: 'As result, efficient matrix multiplication in Eq.[12](#S3.E12 "In 3.3 Floating-Point
    Matrix Multiplication ‣ 3 Preliminaries ‣ LLM-FP4: 4-Bit Floating-Point Quantized
    Transformers") is reformulated as:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，等式中高效的矩阵乘法[12](#S3.E12 "在 3.3 浮点矩阵乘法 ‣ 3 初步工作 ‣ LLM-FP4：4 位浮点量化变换器") 被重新公式化为：
- en: '|  | $1$2 |  | (21) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (21) |'
- en: where $\odot$].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\odot$]。
- en: Combining pre-shifted exponent bias method with the joint format and max-value
    search framework(FPQ baseline), we name our method as (FPQ), short for Floating
    Point Quantization.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 结合预移位指数偏置方法与联合格式及最大值搜索框架（FPQ 基线），我们将我们的方法命名为（FPQ），即浮点量化的简称。
- en: 5 Experiments
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'To validate the effectiveness of the proposed method, we conduct experiments
    on LLaMA Touvron et al. ([2023](#bib.bib26)) and BERT Devlin et al. ([2019](#bib.bib8))
    models in [5.2.1](#S5.SS2.SSS1 "5.2.1 LLM Zero-Shot Reasoning ‣ 5.2 Main Results
    ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers") and Sections [5.2.2](#S5.SS2.SSS2
    "5.2.2 BERT Model ‣ 5.2 Main Results ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers"). Further, in Section [5.2.3](#S5.SS2.SSS3 "5.2.3 Generalizability
    on Vision Transformer ‣ 5.2 Main Results ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers") we show that our method also generalizes well to vision
    transformer architectures. We present ablation studies on the calibration size
    and search range in Section [5.3](#S5.SS3 "5.3 Ablation Study ‣ 5 Experiments
    ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers"), and analyze the hardware
    costs of implementing FP operators in Section [5.4](#S5.SS4 "5.4 Hardware Cost
    ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '为了验证所提方法的有效性，我们在 LLaMA Touvron et al. ([2023](#bib.bib26)) 和 BERT Devlin et
    al. ([2019](#bib.bib8)) 模型上进行实验，实验内容在 [5.2.1](#S5.SS2.SSS1 "5.2.1 LLM Zero-Shot
    Reasoning ‣ 5.2 Main Results ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point Quantized
    Transformers") 和 [5.2.2](#S5.SS2.SSS2 "5.2.2 BERT Model ‣ 5.2 Main Results ‣ 5
    Experiments ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers") 节中进行。此外，在
    [5.2.3](#S5.SS2.SSS3 "5.2.3 Generalizability on Vision Transformer ‣ 5.2 Main
    Results ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers")
    节中我们展示了该方法对视觉变换器架构的良好泛化能力。我们在 [5.3](#S5.SS3 "5.3 Ablation Study ‣ 5 Experiments
    ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers") 节中展示了关于校准大小和搜索范围的消融研究，并在
    [5.4](#S5.SS4 "5.4 Hardware Cost ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers") 节中分析了实施 FP 操作符的硬件成本。'
- en: 5.1 Experiments Details
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验细节
- en: 'We adopt per-tensor quantization for activation and per-channel quantization
    for weight. We employ layer reconstruction following the settings of Yuan et al.
    ([2022](#bib.bib32)); Nagel et al. ([2020](#bib.bib21)), and parallel quantization
    based on the approach outlined in Bai et al. ([2022](#bib.bib2)); Yuan et al.
    ([2022](#bib.bib32)). A more detailed discussion regarding our implementation
    decisions can be found in Appendix [F](#A6 "Appendix F Reconstruction Choices
    ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers"). For LLaMA models, we
    quantize all the weight and activation tensors in fully-connected layers for a
    fair comparison with previous work Xiao et al. ([2022](#bib.bib31)); Liu et al.
    ([2023](#bib.bib18)). For BERT and ViT models, both fully-connected layers and
    activation-activation multiplication tensors in the self-attention module are
    quantized. Note that for FPQ on BERT Devlin et al. ([2019](#bib.bib8)) and ViTs
    models, the reconstruction metric Eq. [14](#S4.E14 "In 4.1 Joint Format and Max
    Value Search ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers")
    is substituted with a Hessian approximation loss metric. This substitution is
    further detailed in Appendix [A](#A1 "Appendix A Hessian-Based Loss Metric ‣ LLM-FP4:
    4-Bit Floating-Point Quantized Transformers").'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '我们采用了每个张量的激活量化和每个通道的权重量化。我们按照 Yuan 等人 ([2022](#bib.bib32))、Nagel 等人 ([2020](#bib.bib21))
    的设置进行层重建，并基于 Bai 等人 ([2022](#bib.bib2)) 和 Yuan 等人 ([2022](#bib.bib32)) 的方法进行并行量化。关于我们实施决策的更详细讨论可以在附录
    [F](#A6 "附录 F 重建选择 ‣ LLM-FP4: 4 位浮点量化变换器") 中找到。对于 LLaMA 模型，我们对所有全连接层的权重和激活张量进行量化，以便与之前的工作
    Xiao 等人 ([2022](#bib.bib31))、Liu 等人 ([2023](#bib.bib18)) 进行公平比较。对于 BERT 和 ViT
    模型，我们对自注意力模块中的全连接层和激活-激活乘积张量进行量化。请注意，对于 BERT Devlin 等人 ([2019](#bib.bib8)) 和 ViTs
    模型上的 FPQ，重建度量 Eq. [14](#S4.E14 "在 4.1 联合格式和最大值搜索 ‣ 4 方法 ‣ LLM-FP4: 4 位浮点量化变换器")
    被替换为 Hessian 近似损失度量。这种替换在附录 [A](#A1 "附录 A 基于 Hessian 的损失度量 ‣ LLM-FP4: 4 位浮点量化变换器")
    中有进一步详细说明。'
- en: 5.2 Main Results
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 主要结果
- en: '| Quant Method | #Bits (E/W/A) | # Calib | BoolQ | PIQA | HellaSwag | WinoGrande
    | ARC-e | ARC-c | Avg. |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 量化方法 | #Bits (E/W/A) | # 校准 | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e
    | ARC-c | 平均值 |'
- en: '| LLaMA-7B Full-precision | 16/16/16 | - | 75.1 | 78.7 | 56.9 | 69.9 | 75.3
    | 41.9 | 66.3 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B 全精度 | 16/16/16 | - | 75.1 | 78.7 | 56.9 | 69.9 | 75.3 | 41.9 | 66.3
    |'
- en: '| MinMax INT Quant | 8/8/8 | 32 | 64.3 | 66.8 | 40.5 | 57.4 | 59.0 | 29.6 |
    52.9 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| MinMax INT 量化 | 8/8/8 | 32 | 64.3 | 66.8 | 40.5 | 57.4 | 59.0 | 29.6 | 52.9
    |'
- en: '| MinMax FP Quant (E4M3) | 8/8/8 | 32 | 74.9 | 78.6 | 56.8 | 69.5 | 75.5 |
    41.6 | 66.1 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E4M3) | 8/8/8 | 32 | 74.9 | 78.6 | 56.8 | 69.5 | 75.5 | 41.6
    | 66.1 |'
- en: '| SmoothQuant Xiao et al. ([2022](#bib.bib31)) | 16/8/8 | 512 | 74.0 | 77.5
    | 55.0 | 69.6 | 74.4 | 37.4 | 64.6 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant Xiao 等人 ([2022](#bib.bib31)) | 16/8/8 | 512 | 74.0 | 77.5 | 55.0
    | 69.6 | 74.4 | 37.4 | 64.6 |'
- en: '| FPQ baseline | 8/8/8 | 32 | 75.8 | 78.3 | 55.9 | 69.5 | 75.6 | 41.3 | 66.1
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| FPQ 基线 | 8/8/8 | 32 | 75.8 | 78.3 | 55.9 | 69.5 | 75.6 | 41.3 | 66.1 |'
- en: '| FPQ | 8/8/8 | 32 | 75.6 | 78.2 | 56.6 | 70.2 | 74.6 | 40.7 | 66.0 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| FPQ | 8/8/8 | 32 | 75.6 | 78.2 | 56.6 | 70.2 | 74.6 | 40.7 | 66.0 |'
- en: '| MinMax INT Quant | 4/4/16 | 32 | 64.1 | 76.1 | 51.6 | 66.3 | 72.4 | 40.0
    | 61.7 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| MinMax INT 量化 | 4/4/16 | 32 | 64.1 | 76.1 | 51.6 | 66.3 | 72.4 | 40.0 | 61.7
    |'
- en: '| MinMax FP Quant (E2M1) | 4/4/16 | 32 | 73.0 | 77.9 | 55.2 | 69.1 | 73.6 |
    40.9 | 64.9 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E2M1) | 4/4/16 | 32 | 73.0 | 77.9 | 55.2 | 69.1 | 73.6 | 40.9
    | 64.9 |'
- en: '| GPTQ Frantar et al. ([2023](#bib.bib11)) | 4/4/16 | 128 | 73.3 | 77.9 | 54.9
    | 67.9 | 72.7 | 37.4 | 64.0 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ Frantar 等人 ([2023](#bib.bib11)) | 4/4/16 | 128 | 73.3 | 77.9 | 54.9
    | 67.9 | 72.7 | 37.4 | 64.0 |'
- en: '| FPQ baseline | 4/4/16 | 32 | 74.8 | 77.9 | 55.6 | 69.5 | 75.2 | 41.0 | 65.7
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| FPQ 基线 | 4/4/16 | 32 | 74.8 | 77.9 | 55.6 | 69.5 | 75.2 | 41.0 | 65.7 |'
- en: '| FPQ | 4/4/16 | 32 | 74.2 | 77.8 | 55.8 | 69.9 | 74.9 | 40.4 | 65.5 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| FPQ | 4/4/16 | 32 | 74.2 | 77.8 | 55.8 | 69.9 | 74.9 | 40.4 | 65.5 |'
- en: '| MinMax INT Quant | 4/4/8 | 32 | 50.4 | 56.5 | 27.9 | 46.5 | 36.1 | 21.2 |
    39.7 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| MinMax INT 量化 | 4/4/8 | 32 | 50.4 | 56.5 | 27.9 | 46.5 | 36.1 | 21.2 | 39.7
    |'
- en: '| MinMax FP Quant (E2M1/E4M3) | 4/4/8 | 32 | 73.0 | 77.5 | 55.0 | 69.3 | 73.6
    | 40.9 | 64.9 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E2M1/E4M3) | 4/4/8 | 32 | 73.0 | 77.5 | 55.0 | 69.3 | 73.6
    | 40.9 | 64.9 |'
- en: '| FPQ baseline | 4/4/8 | 32 | 75.0 | 77.6 | 55.9 | 69.9 | 74.3 | 39.4 | 65.3
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| FPQ 基线 | 4/4/8 | 32 | 75.0 | 77.6 | 55.9 | 69.9 | 74.3 | 39.4 | 65.3 |'
- en: '| FPQ | 4/4/8 | 32 | 75.0 | 77.7 | 55.5 | 69.8 | 74.5 | 39.9 | 65.4 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| FPQ | 4/4/8 | 32 | 75.0 | 77.7 | 55.5 | 69.8 | 74.5 | 39.9 | 65.4 |'
- en: '| MinMax INT Quant | 4/4/4 | 32 | 54.1 | 51.7 | 25.6 | 49.8 | 24.7 | 22.9 |
    38.1 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| MinMax INT 量化 | 4/4/4 | 32 | 54.1 | 51.7 | 25.6 | 49.8 | 24.7 | 22.9 | 38.1
    |'
- en: '| MinMax FP Quant (E2M1) | 4/4/4 | 32 | 47.3 | 53.1 | 25.7 | 50.7 | 25.1 |
    22.4 | 37.4 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E2M1) | 4/4/4 | 32 | 47.3 | 53.1 | 25.7 | 50.7 | 25.1 | 22.4
    | 37.4 |'
- en: '| SmoothQuant Xiao et al. ([2022](#bib.bib31)) | 16/4/4 | 512 | 54.1 | 62.8
    | 41.5 | 52.6 | 50.6 | 32.9 | 49.1 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant Xiao 等 ([2022](#bib.bib31)) | 16/4/4 | 512 | 54.1 | 62.8 | 41.5
    | 52.6 | 50.6 | 32.9 | 49.1 |'
- en: '| LLM-QAT Liu et al. ([2023](#bib.bib18)) | 16/4/4 | (QAT) | 63.5 | 64.3 |
    55.6 | 52.9 | 50.3 | 30.2 | 52.8 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QAT Liu 等 ([2023](#bib.bib18)) | 16/4/4 | (QAT) | 63.5 | 64.3 | 55.6
    | 52.9 | 50.3 | 30.2 | 52.8 |'
- en: '| FPQ baseline | 4/4/4 | 32 | 57.4 | 56.6 | 30.2 | 51.1 | 37.7 | 23.2 | 42.7
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| FPQ 基线 | 4/4/4 | 32 | 57.4 | 56.6 | 30.2 | 51.1 | 37.7 | 23.2 | 42.7 |'
- en: '| FPQ | 4/4/4 | 32 | 64.2 | 73.5 | 47.8 | 63.7 | 65.9 | 33.6 | 58.1 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| FPQ | 4/4/4 | 32 | 64.2 | 73.5 | 47.8 | 63.7 | 65.9 | 33.6 | 58.1 |'
- en: '| LLaMA-13B Full-precision | 16/16/16 | - | 77.9 | 79.2 | 59.9 | 72.6 | 77.4
    | 46.4 | 68.9 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-13B 全精度 | 16/16/16 | - | 77.9 | 79.2 | 59.9 | 72.6 | 77.4 | 46.4 |
    68.9 |'
- en: '| MinMax INT Quant | 8/8/8 | 32 | 60.6 | 69.6 | 46.0 | 61.5 | 63.3 | 32.8 |
    55.6 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| MinMax INT 量化 | 8/8/8 | 32 | 60.6 | 69.6 | 46.0 | 61.5 | 63.3 | 32.8 | 55.6
    |'
- en: '| MinMax FP Quant (E4M3) | 8/8/8 | 32 | 78.0 | 79.1 | 60.0 | 72.3 | 77.2 |
    47.1 | 68.9 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E4M3) | 8/8/8 | 32 | 78.0 | 79.1 | 60.0 | 72.3 | 77.2 | 47.1
    | 68.9 |'
- en: '| SmoothQuant Xiao et al. ([2022](#bib.bib31)) | 16/8/8 | 512 | 76.5 | 78.0
    | 58.0 | 72.1 | 76.3 | 45.5 | 68.2 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant Xiao 等 ([2022](#bib.bib31)) | 16/8/8 | 512 | 76.5 | 78.0 | 58.0
    | 72.1 | 76.3 | 45.5 | 68.2 |'
- en: '| FPQ baseline | 8/8/8 | 32 | 78.0 | 79.1 | 59.9 | 72.3 | 77.2 | 47.1 | 68.9
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| FPQ 基线 | 8/8/8 | 32 | 78.0 | 79.1 | 59.9 | 72.3 | 77.2 | 47.1 | 68.9 |'
- en: '| FPQ | 8/8/8 | 32 | 78.1 | 78.5 | 59.1 | 72.4 | 76.4 | 46.1 | 68.4 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| FPQ | 8/8/8 | 32 | 78.1 | 78.5 | 59.1 | 72.4 | 76.4 | 46.1 | 68.4 |'
- en: '| MinMax INT Quant | 4/4/8 | 32 | 52.1 | 65.0 | 36.4 | 53.9 | 52.3 | 29.0 |
    48.1 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| MinMax INT 量化 | 4/4/8 | 32 | 52.1 | 65.0 | 36.4 | 53.9 | 52.3 | 29.0 | 48.1
    |'
- en: '| MinMax FP Quant (E2M1/E4M3) | 4/4/8 | 32 | 78.0 | 78.9 | 58.0 | 71.6 | 76.0
    | 44.8 | 67.9 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E2M1/E4M3) | 4/4/8 | 32 | 78.0 | 78.9 | 58.0 | 71.6 | 76.0
    | 44.8 | 67.9 |'
- en: '| FPQ baseline | 4/4/8 | 32 | 76.2 | 78.2 | 57.9 | 71.9 | 75.1 | 43.9 | 67.2
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| FPQ 基线 | 4/4/8 | 32 | 76.2 | 78.2 | 57.9 | 71.9 | 75.1 | 43.9 | 67.2 |'
- en: '| FPQ | 4/4/8 | 32 | 76.4 | 78.5 | 58.2 | 72.1 | 75.2 | 44.7 | 67.5 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| FPQ | 4/4/8 | 32 | 76.4 | 78.5 | 58.2 | 72.1 | 75.2 | 44.7 | 67.5 |'
- en: '| MinMax INT Quant | 4/4/4 | 32 | 54.5 | 52.7 | 25.5 | 51.1 | 25.3 | 22.1 |
    38.5 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| MinMax INT 量化 | 4/4/4 | 32 | 54.5 | 52.7 | 25.5 | 51.1 | 25.3 | 22.1 | 38.5
    |'
- en: '| MinMax FP Quant (E2M1) | 4/4/4 | 32 | 45.8 | 51.7 | 25.5 | 49.5 | 25.0 |
    22.8 | 36.7 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E2M1) | 4/4/4 | 32 | 45.8 | 51.7 | 25.5 | 49.5 | 25.0 | 22.8
    | 36.7 |'
- en: '| SmoothQuant Xiao et al. ([2022](#bib.bib31)) | 16/4/4 | 512 | 57.6 | 61.3
    | 56.0 | 52.6 | 49.9 | 25.1 | 50.4 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant Xiao 等 ([2022](#bib.bib31)) | 16/4/4 | 512 | 57.6 | 61.3 | 56.0
    | 52.6 | 49.9 | 25.1 | 50.4 |'
- en: '| FPQ baseline | 4/4/4 | 32 | 54.3 | 57.7 | 35.7 | 52.2 | 41.1 | 25.7 | 44.5
    |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| FPQ 基线 | 4/4/4 | 32 | 54.3 | 57.7 | 35.7 | 52.2 | 41.1 | 25.7 | 44.5 |'
- en: '| FPQ | 4/4/4 | 32 | 71.9 | 74.8 | 53.3 | 66.7 | 71.7 | 39.9 | 63.1 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| FPQ | 4/4/4 | 32 | 71.9 | 74.8 | 53.3 | 66.7 | 71.7 | 39.9 | 63.1 |'
- en: 'Table 1: Zero-shot performance on common sense reasoning tasks with LLaMA Touvron
    et al. ([2023](#bib.bib26)) models. We denote E/W/A as the bit-width of word embeddings,
    model weight and activations, respectively.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：LLaMA Touvron 等 ([2023](#bib.bib26)) 模型在常识推理任务上的零样本表现。我们用 E/W/A 表示词嵌入、模型权重和激活的位宽。
- en: '| Quant Method | #Bits (E/W/A) | # Calib | MNLI[-m] | QQP | QNLI | SST-2 |
    CoLA | STS-B | MRPC | RTE | Avg. |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 量化方法 | #Bits (E/W/A) | # Calib | MNLI[-m] | QQP | QNLI | SST-2 | CoLA | STS-B
    | MRPC | RTE | 平均 |'
- en: '| (Full-precision) | 32-32-32 | - | 84.9 | 91.4 | 92.1 | 93.2 | 59.7 | 90.1
    | 86.3 | 72.2 | 83.7 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| (全精度) | 32-32-32 | - | 84.9 | 91.4 | 92.1 | 93.2 | 59.7 | 90.1 | 86.3 | 72.2
    | 83.7 |'
- en: '| MinMax INT Quant | 8/8/8 | 128 | 77.0 | 89.9 | 88.9 | 92.9 | 51.8 | 88.2
    | 83.8 | 71.5 | 80.5 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| MinMax INT 量化 | 8/8/8 | 128 | 77.0 | 89.9 | 88.9 | 92.9 | 51.8 | 88.2 | 83.8
    | 71.5 | 80.5 |'
- en: '| MinMax FP Quant (E2M5) | 8/8/8 | 128 | 78.9 | 90.8 | 88.6 | 92.9 | 52.7 |
    88.4 | 84.3 | 69.0 | 80.7 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E2M5) | 8/8/8 | 128 | 78.9 | 90.8 | 88.6 | 92.9 | 52.7 | 88.4
    | 84.3 | 69.0 | 80.7 |'
- en: '| MinMax FP Quant (E3M4) | 8/8/8 | 128 | 84.5 | 90.9 | 91.5 | 93.2 | 58.3 |
    89.3 | 87.7 | 71.8 | 83.4 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E3M4) | 8/8/8 | 128 | 84.5 | 90.9 | 91.5 | 93.2 | 58.3 | 89.3
    | 87.7 | 71.8 | 83.4 |'
- en: '| MinMax FP Quant (E4M3) | 8/8/8 | 128 | 84.7 | 90.9 | 91.7 | 93.0 | 58.6 |
    89.3 | 86.5 | 72.2 | 83.4 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E4M3) | 8/8/8 | 128 | 84.7 | 90.9 | 91.7 | 93.0 | 58.6 | 89.3
    | 86.5 | 72.2 | 83.4 |'
- en: '| MinMax FP Quant (E5M2) | 8/8/8 | 128 | 84.1 | 90.9 | 91.4 | 93.6 | 58.1 |
    89.2 | 87.5 | 71.8 | 83.3 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E5M2) | 8/8/8 | 128 | 84.1 | 90.9 | 91.4 | 93.6 | 58.1 | 89.2
    | 87.5 | 71.8 | 83.3 |'
- en: '| FPQ baseline | 8/8/8 | 128 | 84.6 | 90.9 | 91.7 | 93.1 | 58.6 | 89.3 | 88.0
    | 72.2 | 83.5 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| FPQ 基线 | 8/8/8 | 128 | 84.6 | 90.9 | 91.7 | 93.1 | 58.6 | 89.3 | 88.0 | 72.2
    | 83.5 |'
- en: '| FPQ | 8/8/8 | 128 | 84.6 | 91.0 | 91.6 | 93.3 | 58.8 | 89.3 | 88.0 | 72.2
    | 83.6 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| FPQ | 8/8/8 | 128 | 84.6 | 91.0 | 91.6 | 93.3 | 58.8 | 89.3 | 88.0 | 72.2
    | 83.6 |'
- en: '| MinMax INT Quant | 6/6/6 | 128 | 31.9 | 62.0 | 52.8 | 58.8 | 0.0 | 12.7 |
    32.1 | 52.7 | 37.9 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| MinMax INT 量化 | 6/6/6 | 128 | 31.9 | 62.0 | 52.8 | 58.8 | 0.0 | 12.7 | 32.1
    | 52.7 | 37.9 |'
- en: '| MinMax FP Quant (E2M3) | 6/6/6 | 128 | 43.5 | 85.4 | 79.4 | 90.5 | 45.2 |
    86.0 | 66.9 | 59.9 | 69.6 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E2M3) | 6/6/6 | 128 | 43.5 | 85.4 | 79.4 | 90.5 | 45.2 | 86.0
    | 66.9 | 59.9 | 69.6 |'
- en: '| MinMax FP Quant (E3M2) | 6/6/6 | 128 | 83.9 | 90.8 | 90.8 | 92.2 | 58.2 |
    88.6 | 87.0 | 72.2 | 83.0 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E3M2) | 6/6/6 | 128 | 83.9 | 90.8 | 90.8 | 92.2 | 58.2 | 88.6
    | 87.0 | 72.2 | 83.0 |'
- en: '| MinMax FP Quant (E4M1) | 6/6/6 | 128 | 84.4 | 90.2 | 90.1 | 92.2 | 58.2 |
    89.2 | 85.3 | 69.7 | 82.4 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E4M1) | 6/6/6 | 128 | 84.4 | 90.2 | 90.1 | 92.2 | 58.2 | 89.2
    | 85.3 | 69.7 | 82.4 |'
- en: '| FPQ baseline | 6/6/6 | 128 | 84.6 | 90.9 | 91.2 | 93.2 | 58.8 | 88.7 | 87.5
    | 70.8 | 83.2 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| FPQ 基线 | 6/6/6 | 128 | 84.6 | 90.9 | 91.2 | 93.2 | 58.8 | 88.7 | 87.5 | 70.8
    | 83.2 |'
- en: '| FPQ | 6/6/6 | 128 | 84.5 | 90.8 | 91.6 | 93.1 | 57.3 | 89.3 | 88.7 | 71.8
    | 83.2 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| FPQ | 6/6/6 | 128 | 84.5 | 90.8 | 91.6 | 93.1 | 57.3 | 89.3 | 88.7 | 71.8
    | 83.2 |'
- en: '| MinMax INT Quant | 4/4/8 | 128 | 33.1 | 63.8 | 60.1 | 49.3 | 0.0 | 44.0 |
    50.2 | 49.1 | 43.7 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| MinMax INT 量化 | 4/4/8 | 128 | 33.1 | 63.8 | 60.1 | 49.3 | 0.0 | 44.0 | 50.2
    | 49.1 | 43.7 |'
- en: '| MinMax FP Quant (E2M1) | 4/4/8 | 128 | 60.6 | 70.9 | 77.4 | 79.9 | 5.5 |
    78.6 | 46.8 | 56.6 | 59.5 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E2M1) | 4/4/8 | 128 | 60.6 | 70.9 | 77.4 | 79.9 | 5.5 | 78.6
    | 46.8 | 56.6 | 59.5 |'
- en: '| MREM-S Bai et al. ([2022](#bib.bib2)) | 4/4/8 | 4096 | 83.5 | 90.2 | 91.2
    | 91.4 | 55.1 | 89.1 | 84.8 | 71.8 | 82.1 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| MREM-S Bai 等人 ([2022](#bib.bib2)) | 4/4/8 | 4096 | 83.5 | 90.2 | 91.2 | 91.4
    | 55.1 | 89.1 | 84.8 | 71.8 | 82.1 |'
- en: '| MREM-P Bai et al. ([2022](#bib.bib2)) | 4/4/8 | 4096 | 83.4 | 90.2 | 91.0
    | 91.5 | 54.7 | 89.1 | 86.3 | 71.1 | 82.2 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| MREM-P Bai 等人 ([2022](#bib.bib2)) | 4/4/8 | 4096 | 83.4 | 90.2 | 91.0 | 91.5
    | 54.7 | 89.1 | 86.3 | 71.1 | 82.2 |'
- en: '| FPQ baseline | 4/4/8 | 128 | 84.4 | 90.6 | 91.4 | 92.9 | 58.6 | 83.7 | 88.2
    | 73.3 | 82.9 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| FPQ 基线 | 4/4/8 | 128 | 84.4 | 90.6 | 91.4 | 92.9 | 58.6 | 83.7 | 88.2 | 73.3
    | 82.9 |'
- en: '| FPQ | 4/4/8 | 128 | 84.5 | 90.6 | 91.1 | 92.7 | 58.8 | 89.3 | 88.7 | 73.3
    | 83.6 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| FPQ | 4/4/8 | 128 | 84.5 | 90.6 | 91.1 | 92.7 | 58.8 | 89.3 | 88.7 | 73.3
    | 83.6 |'
- en: '| MinMax INT Quant | 4/4/4 | 128 | 31.8 | 39.7 | 50.5 | 49.1 | 0.0 | 6.7 |
    31.6 | 54.5 | 32.9 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| MinMax INT 量化 | 4/4/4 | 128 | 31.8 | 39.7 | 50.5 | 49.1 | 0.0 | 6.7 | 31.6
    | 54.5 | 32.9 |'
- en: '| MinMax FP Quant (E2M1) | 4/4/4 | 128 | 33.6 | 54.0 | 50.6 | 50.8 | 0.0 |
    0.0 | 31.6 | 52.0 | 34.1 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| MinMax FP 量化 (E2M1) | 4/4/4 | 128 | 33.6 | 54.0 | 50.6 | 50.8 | 0.0 | 0.0
    | 31.6 | 52.0 | 34.1 |'
- en: '| BrecQ Li et al. ([2021](#bib.bib17)) | 8/4/4 | 4096 | 31.9 | 62.3 | 50.7
    | 50.9 | 0.9 | 6.4 | 31.7 | 52.3 | 35.8 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| BrecQ Li 等人 ([2021](#bib.bib17)) | 8/4/4 | 4096 | 31.9 | 62.3 | 50.7 | 50.9
    | 0.9 | 6.4 | 31.7 | 52.3 | 35.8 |'
- en: '| QDrop Wei et al. ([2022](#bib.bib29)) | 8/4/4 | 4096 | 71.4 | 79.0 | 76.8
    | 88.1 | 40.9 | 81.9 | 79.2 | 60.7 | 72.3 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| QDrop Wei 等人 ([2022](#bib.bib29)) | 8/4/4 | 4096 | 71.4 | 79.0 | 76.8 | 88.1
    | 40.9 | 81.9 | 79.2 | 60.7 | 72.3 |'
- en: '| FPQ baseline | 4/4/4 | 128 | 38.9 | 68.3 | 55.3 | 83.6 | 10.6 | 0.0 | 43.8
    | 55.2 | 44.5 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| FPQ 基线 | 4/4/4 | 128 | 38.9 | 68.3 | 55.3 | 83.6 | 10.6 | 0.0 | 43.8 | 55.2
    | 44.5 |'
- en: '| FPQ | 4/4/4 | 128 | 82.3 | 89.2 | 86.6 | 91.5 | 52.6 | 85.5 | 83.8 | 69.0
    | 80.1 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| FPQ | 4/4/4 | 128 | 82.3 | 89.2 | 86.6 | 91.5 | 52.6 | 85.5 | 83.8 | 69.0
    | 80.1 |'
- en: 'Table 2: Results on the GLUE development set with BERT Bai et al. ([2022](#bib.bib2))
    model. We denote E/W/A as the bit-width of word embeddings, model weight and activations,
    respectively.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：BERT Bai 等人 ([2022](#bib.bib2)) 模型在GLUE开发集上的结果。我们将E/W/A表示为词嵌入、模型权重和激活的位宽。
- en: 5.2.1 LLM Zero-Shot Reasoning
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 LLM 零样本推理
- en: 'We evaluate the effectiveness of FPQ for LLaMA-7B/ LLaMA-13B Touvron et al.
    ([2023](#bib.bib26)) on common sense zero-shot reasoning tasks. For the calibration
    data, we sample 32 random segments with 2048 tokens length from the C4 Raffel
    et al. ([2020](#bib.bib24)) dataset following the setting of GPTQ Frantar et al.
    ([2023](#bib.bib11)). The data preprocessing and score calculation are based on
    EleutherAI evaluation harness¹¹1https://github.com/EleutherAI/lm-evaluation-harness.
    In Table [1](#S5.T1 "Table 1 ‣ 5.2 Main Results ‣ 5 Experiments ‣ LLM-FP4: 4-Bit
    Floating-Point Quantized Transformers"), we compare FPQ to the floating-point
    PTQ baselines, and state-of-the-art PTQ and QAT methods, including SmoothQuant Xiao
    et al. ([2022](#bib.bib31)) and GPTQ Frantar et al. ([2023](#bib.bib11)), and
    LLM-QAT Liu et al. ([2023](#bib.bib18)).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了**FPQ**在LLaMA-7B/ LLaMA-13B Touvron 等人 ([2023](#bib.bib26)) 上的效果，针对常识零样本推理任务。对于校准数据，我们从C4
    Raffel 等人 ([2020](#bib.bib24)) 数据集中随机抽取了32个长度为2048个token的片段，遵循了GPTQ Frantar 等人
    ([2023](#bib.bib11)) 的设置。数据预处理和评分计算基于EleutherAI评估工具¹¹1https://github.com/EleutherAI/lm-evaluation-harness。在表[1](#S5.T1
    "表1 ‣ 5.2 主要结果 ‣ 5 实验 ‣ LLM-FP4: 4位浮点量化变换器")中，我们将**FPQ**与浮点PTQ基线，以及最先进的PTQ和QAT方法进行比较，包括SmoothQuant
    Xiao 等人 ([2022](#bib.bib31)) 和GPTQ Frantar 等人 ([2023](#bib.bib11))，以及LLM-QAT Liu
    等人 ([2023](#bib.bib18))。'
- en: In general, all methods, except for the naïve MinMax INT Quantization, produce
    comparable outcomes in the 8-bit setting on both LLaMA-7B and LLaMA-13B. Additionally,
    we observe that the naïve MinMax FP Quantization achieves nearly lossless results
    and even surpasses the state-of-the-art integer post-training quantization method,
    SmoothQuant (Xiao et al., 2022), which indicates that floating-point quantization
    naturally has a strong capability in handling the distributions in transformers.
    However, both MinMax FP Quant and FPQ baseline fail when pushing the quantization
    precision to ultra-low 4/4/4 bit setting, with $28.9\%$ accuracy drop on LLaMA-7B/13B
    with 4/4/4 bit-width, outperforming SmoothQuant Xiao et al. ([2022](#bib.bib31))
    by a large margin, yet with less bit-width and smaller calibration size. Moreover,
    FPQ even achieves 5.3% accuracy improvements compared to LLM-QAT Liu et al. ([2023](#bib.bib18))
    in the 4/4/4 setting and 1.5% over GPTQ Frantar et al. ([2023](#bib.bib11)) in
    the 4/4/16 configuration on LLaMA-7B.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，除了简单的 MinMax INT 量化方法外，所有方法在 LLaMA-7B 和 LLaMA-13B 的 8-bit 设置下均产生了相当的结果。此外，我们观察到简单的
    MinMax FP 量化方法实现了几乎无损的结果，甚至超越了最先进的整数后训练量化方法 SmoothQuant（Xiao 等人，2022），这表明浮点量化在处理变换器中的分布方面自然具有很强的能力。然而，当将量化精度推向超低的
    4/4/4 位设置时，无论是 MinMax FP Quant 还是 FPQ 基线都失败了，在 4/4/4 位宽下 LLaMA-7B/13B 上的准确率下降了
    $28.9\%$，虽然在 bit-width 较小和校准数据量较少的情况下优于 SmoothQuant（Xiao 等人，[2022](#bib.bib31)），但仍表现不佳。此外，与
    LLM-QAT Liu 等人 ([2023](#bib.bib18)) 在 4/4/4 设置下相比，FPQ 甚至取得了 5.3% 的准确率提升，并在 4/4/16
    配置下比 GPTQ Frantar 等人 ([2023](#bib.bib11)) 提高了 1.5%，在 LLaMA-7B 上表现出色。
- en: For practitioners, a crucial consideration is determining the appropriate quantization
    methods for various bit-widths. Therefore, based on our findings, we offer two
    recommendations that balance the trade-off between accuracy and search/optimization
    efficiency. First of all, since the difference between MinMax FP Quant and the
    rest of the methods is marginal for the 8/8/8 setting, we recommend simply using
    the MinMax FP Quant method for the 8/8/8 setting as the MinMax method does not
    involve search process. However, for more demanding scenarios, especially with
    activation quantization to 4 bits, we recommend employing FPQ for minimizing accuracy
    degradation with negligible inference overhead.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从业者来说，一个关键的考虑因素是确定不同位宽的适当量化方法。因此，基于我们的发现，我们提供了两个推荐，以平衡准确性与搜索/优化效率之间的权衡。首先，由于
    8/8/8 设置下 MinMax FP Quant 和其他方法之间的差异很小，我们建议在 8/8/8 设置中简单使用 MinMax FP Quant 方法，因为
    MinMax 方法不涉及搜索过程。然而，对于更苛刻的场景，特别是激活量化到 4 位时，我们建议采用 FPQ 以最小化准确性下降，同时推理开销几乎可以忽略不计。
- en: 5.2.2 BERT Model
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 BERT 模型
- en: 'We evaluate the proposed quantization techniques for BERT model on GLUE tasks
    Wang et al. ([2019](#bib.bib28)). Full-precision BERT-base models fine-tuned on
    GLUE datasets are obtained from Huggingface public repository²²2https://huggingface.co/textattack/bert-base-uncased-{TASK_NAME}.
    We randomly sample 128 data from the training set as the calibration set. In Table
    [2](#S5.T2 "Table 2 ‣ 5.2 Main Results ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers"), FPQ demonstrates remarkable performance, achieving absolute
    average accuracy improvements of $44.3\%$ over QDrop Wei et al. ([2022](#bib.bib29))
    with 4/4/4 bit setting. Further, with 4-bit weight and 8-bit activation, MREM-S/MREM-P Bai
    et al. ([2022](#bib.bib2)) present a 1.6/1.5% accuracy gap to the full-precision
    model with 4096 calibration data, while FPQ achieves almost no accuracy loss with
    only 128 calibration data points.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了在 GLUE 任务上对 BERT 模型提出的量化技术 Wang 等人 ([2019](#bib.bib28))。从 Huggingface
    公共仓库中获得了在 GLUE 数据集上微调的全精度 BERT-base 模型²²2https://huggingface.co/textattack/bert-base-uncased-{TASK_NAME}。我们从训练集中随机抽取了
    128 条数据作为校准集。在表 [2](#S5.T2 "Table 2 ‣ 5.2 Main Results ‣ 5 Experiments ‣ LLM-FP4:
    4-Bit Floating-Point Quantized Transformers") 中，FPQ 展示了显著的性能，使用 4/4/4 位设置时，相比于
    QDrop Wei 等人 ([2022](#bib.bib29)) 提高了 $44.3\%$ 的绝对平均准确率。此外，在 4-bit 权重和 8-bit 激活的条件下，MREM-S/MREM-P
    Bai 等人 ([2022](#bib.bib2)) 在 4096 个校准数据下，相比全精度模型呈现 1.6/1.5% 的准确率差距，而 FPQ 在仅用 128
    个校准数据点的情况下几乎没有准确率损失。'
- en: 5.2.3 Generalizability on Vision Transformer
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 视觉变换器的泛化能力
- en: 'Based on our findings that vision transformers also exhibit a consistent activation
    distribution pattern as language transformers, characterized by high inter-channel
    variance and low intra-channel variance, as detailed in Fig. [2](#S4.F2 "Figure
    2 ‣ 4.2 Pre-Shifted Exponent Bias ‣ 4 Method ‣ LLM-FP4: 4-Bit Floating-Point Quantized
    Transformers"), we extended our proposed methods to ViT and compared FPQ with
    floating-point PTQ baselines and state-of-the-art PTQ method for ViT on the ImageNet
    classification task. Table [3](#S5.T3 "Table 3 ‣ 5.2.3 Generalizability on Vision
    Transformer ‣ 5.2 Main Results ‣ 5 Experiments ‣ LLM-FP4: 4-Bit Floating-Point
    Quantized Transformers") shows that findings on ViT are consistent with that on
    language models: previous state-of-the-art integer-based methods struggled to
    maintain reasonable accuracy when quantizing the transformer to lower bits. In
    comparison, the proposed FPQ outperformed both PTQ4ViT and APQ-ViT on 6 bits,
    and also achieved 40.9% and 31.5% absolute accuracy improvement over PTQ4ViT and
    APQ-ViT on DeiT-S in the 4-bit configuration.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '基于我们的发现，视觉变换器也表现出与语言变换器一致的激活分布模式，特征是高通道间方差和低通道内方差，如图 [2](#S4.F2 "图 2 ‣ 4.2
    预偏移指数偏差 ‣ 4 方法 ‣ LLM-FP4: 4 位浮点量化变换器") 所示，我们将我们提出的方法扩展到 ViT，并在 ImageNet 分类任务中将
    FPQ 与浮点 PTQ 基线和最先进的 PTQ 方法进行了比较。表 [3](#S5.T3 "表 3 ‣ 5.2.3 在视觉变换器上的可泛化性 ‣ 5.2 主要结果
    ‣ 5 实验 ‣ LLM-FP4: 4 位浮点量化变换器") 显示 ViT 的发现与语言模型上的一致：以前最先进的基于整数的方法在将变换器量化为较低位时难以保持合理的准确性。相比之下，所提出的
    FPQ 在 6 位上超越了 PTQ4ViT 和 APQ-ViT，并在 4 位配置的 DeiT-S 上分别比 PTQ4ViT 和 APQ-ViT 提高了 40.9%
    和 31.5% 的绝对准确性。'
- en: '| W/A | Quant Method | Deit-S | Deit-B | ViT-S |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| W/A | 量化方法 | Deit-S | Deit-B | ViT-S |'
- en: '| Full-prec | - | 79.9 | 81.8 | 81.4 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 完整精度 | - | 79.9 | 81.8 | 81.4 |'
- en: '| 6/6 | PTQ4ViTYuan et al. ([2022](#bib.bib32)) | 76.3 | 80.3 | 78.6 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 6/6 | PTQ4ViTYuan 等 ([2022](#bib.bib32)) | 76.3 | 80.3 | 78.6 |'
- en: '| 6/6 | APQ-ViTDing et al. ([2022](#bib.bib9)) | 77.8 | 80.4 | 79.2 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 6/6 | APQ-ViTDing 等 ([2022](#bib.bib9)) | 77.8 | 80.4 | 79.2 |'
- en: '| 6/6 | MinMax FP Quant (E3M2) | 79.3 | 81.7 | 80.7 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 6/6 | MinMax FP 量化 (E3M2) | 79.3 | 81.7 | 80.7 |'
- en: '| 6/6 | FPQ baseline | 79.43 | 81.7 | 80.9 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 6/6 | FPQ 基线 | 79.43 | 81.7 | 80.9 |'
- en: '| 6/6 | FPQ | 79.5 | 81.8 | 81.1 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 6/6 | FPQ | 79.5 | 81.8 | 81.1 |'
- en: '| 4/4 | PTQ4ViTYuan et al. ([2022](#bib.bib32)) | 34.1 | 64.4 | 42.6 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 4/4 | PTQ4ViTYuan 等 ([2022](#bib.bib32)) | 34.1 | 64.4 | 42.6 |'
- en: '| 4/4 | APQ-ViT Ding et al. ([2022](#bib.bib9)) | 43.6 | 67.5 | 48.0 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 4/4 | APQ-ViT Ding 等 ([2022](#bib.bib9)) | 43.6 | 67.5 | 48.0 |'
- en: '| 4/4 | MinMax FP Quant (E2M1) | 0.4 | 0.1 | 0.1 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 4/4 | MinMax FP 量化 (E2M1) | 0.4 | 0.1 | 0.1 |'
- en: '| 4/4 | FPQ baseline | 6.57 | 0.71 | 0.3 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 4/4 | FPQ 基线 | 6.57 | 0.71 | 0.3 |'
- en: '| 4/4 | FPQ | 75.0 | 79.4 | 73.2 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 4/4 | FPQ | 75.0 | 79.4 | 73.2 |'
- en: 'Table 3: Comparison on the ImageNet dataset with vision transformer structures.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 在 ImageNet 数据集上比较视觉变换器结构。'
- en: 5.3 Ablation Study
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 消融研究
- en: 'In this section, we first compare the influence of different calibration sizes
    on FPQ. We vary the calibration size in $\{32,64,128,256\}$ and test on MNLI,
    QQP, and CoLA. Table [4](#S5.T4 "Table 4 ‣ 5.3 Ablation Study ‣ 5 Experiments
    ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers") shows that the evaluation
    on MNLI and QQP is more robust to different settings, and the variance is more
    significant on CoLA. We observe that FPQ performs well with a calibration set
    size of 128 data points. However, we also find that it remains robust and maintains
    competitive accuracy even with limited access to calibration data, such as when
    using as few as 32 data points.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们首先比较不同校准大小对 FPQ 的影响。我们在 $\{32,64,128,256\}$ 中变化校准大小，并在 MNLI、QQP 和 CoLA
    上进行测试。表 [4](#S5.T4 "表 4 ‣ 5.3 消融研究 ‣ 5 实验 ‣ LLM-FP4: 4 位浮点量化变换器") 显示 MNLI 和 QQP
    的评估对不同设置更为稳健，而 CoLA 上的方差更为显著。我们观察到 FPQ 在校准集大小为 128 个数据点时表现良好。然而，我们也发现即使在校准数据访问有限的情况下，如仅使用
    32 个数据点，它仍然保持稳健并保持竞争力的准确性。'
- en: We investigate the robustness of FPQ to different search ranges $(\gamma_{1},\gamma_{2})$,
    as long as the search range is not overly aggressive.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查了 FPQ 对不同搜索范围 $(\gamma_{1},\gamma_{2})$ 的稳健性，只要搜索范围不是过于激进。
- en: '| E/W/A | #Calib | MNLI-M | QQP | CoLA |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| E/W/A | #校准 | MNLI-M | QQP | CoLA |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 4/4/4 | 32 | 81.5 | 89.4 | 44.4 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 4/4/4 | 32 | 81.5 | 89.4 | 44.4 |'
- en: '| 4/4/4 | 64 | 81.8 | 89.4 | 47.9 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 4/4/4 | 64 | 81.8 | 89.4 | 47.9 |'
- en: '| 4/4/4 | 128 | 82.3 | 89.2 | 52.6 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 4/4/4 | 128 | 82.3 | 89.2 | 52.6 |'
- en: '| 4/4/4 | 256 | 81.9 | 89.0 | 52.9 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 4/4/4 | 256 | 81.9 | 89.0 | 52.9 |'
- en: '| 6/6/6 | 32 | 84.8 | 90.8 | 55.0 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 6/6/6 | 32 | 84.8 | 90.8 | 55.0 |'
- en: '| 6/6/6 | 64 | 84.7 | 90.9 | 58.2 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 6/6/6 | 64 | 84.7 | 90.9 | 58.2 |'
- en: '| 6/6/6 | 128 | 84.5 | 90.8 | 57.3 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 6/6/6 | 128 | 84.5 | 90.8 | 57.3 |'
- en: '| 6/6/6 | 256 | 84.6 | 90.8 | 57.6 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 6/6/6 | 256 | 84.6 | 90.8 | 57.6 |'
- en: 'Table 4: Ablation studies of different calibration sizes.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：不同校准尺寸的消融研究。
- en: '| E/W/A | $\gamma_{{}_{1}}$ | MNLI-M | QQP | CoLA |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| E/W/A | $\gamma_{{}_{1}}$ | MNLI-M | QQP | CoLA |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 4/4/4 | 0.01, 1.2 | 82.3 | 89.2 | 52.6 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 4/4/4 | 0.01, 1.2 | 82.3 | 89.2 | 52.6 |'
- en: '| 4/4/4 | 0.1, 1.2 | 82.2 | 89.1 | 53.6 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 4/4/4 | 0.1, 1.2 | 82.2 | 89.1 | 53.6 |'
- en: '| 4/4/4 | 0.5, 1.5 | 82.3 | 88.4 | 52.8 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 4/4/4 | 0.5, 1.5 | 82.3 | 88.4 | 52.8 |'
- en: '| 6/6/6 | 0.01, 1.2 | 84.5 | 90.8 | 57.3 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 6/6/6 | 0.01, 1.2 | 84.5 | 90.8 | 57.3 |'
- en: '| 6/6/6 | 0.1,1.2 | 84.7 | 90.8 | 57.5 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 6/6/6 | 0.1,1.2 | 84.7 | 90.8 | 57.5 |'
- en: '| 6/6/6 | 0.5,1.5 | 84.7 | 90.8 | 57.8 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 6/6/6 | 0.5,1.5 | 84.7 | 90.8 | 57.8 |'
- en: 'Table 5: Ablation studies of different search range.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：不同搜索范围的消融研究。
- en: 5.4 Hardware Cost
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 硬件成本
- en: We further examine the hardware utilization of low-bit INT, FP, and mixed-format
    FP multiplication operators, including adder, multiplier, and multiply-accumulate
    (MAC) units, in terms of hardware area. Mixed-format FP refers to the multiplication
    of floating-point numbers with different formats, e.g., E2M1 multiplies with E1M2\.
    We implemented the MAC operator by Verilog HDL and utilized Cadence Genus to obtain
    the synthesized area under TSMC 40nm technology and 0.5GHz clock frequency.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步探讨了低位INT、FP和混合格式FP乘法运算符的硬件利用率，包括加法器、乘法器和乘加（MAC）单元，从硬件面积的角度进行分析。混合格式FP指的是不同格式的浮点数相乘，例如E2M1与E1M2相乘。我们通过Verilog
    HDL实现了MAC运算符，并利用Cadence Genus在TSMC 40nm技术和0.5GHz时钟频率下获取了综合面积。
- en: 'Table [6](#S5.T6 "Table 6 ‣ 5.4 Hardware Cost ‣ 5 Experiments ‣ LLM-FP4: 4-Bit
    Floating-Point Quantized Transformers") illustrates the hardware cost of the INT
    and FP operators, with the multiplier being the primary cost for INT and the adder
    for FP. Notably, the disparity between FP4 and INT4 adders is small, while INT
    has twice the hardware cost for the multiplier. Moreover, the mixed-format FP4
    operator has comparable hardware area as the standard FP4 operator. These findings
    indicate that the proposed FPQ approach imposes negligible overhead in terms of
    hardware implementation when compared to the standard FP operators and the hardware
    cost for FP is comparable with INT.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '表[6](#S5.T6 "表 6 ‣ 5.4 硬件成本 ‣ 5 实验 ‣ LLM-FP4: 4位浮点量化转换器")展示了INT和FP运算符的硬件成本，其中INT的主要成本是乘法器，FP的主要成本是加法器。值得注意的是，FP4和INT4加法器之间的差异很小，而INT的乘法器硬件成本是FP的两倍。此外，混合格式FP4运算符的硬件面积与标准FP4运算符相当。这些发现表明，与标准FP运算符相比，提出的FPQ方法在硬件实现方面几乎没有额外开销，FP的硬件成本与INT相当。'
- en: '| Format | Adder($\mu m^{2}$) |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | 加法器($\mu m^{2}$) |'
- en: '| INT4 | 93 | 182 | 410 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| INT4 | 93 | 182 | 410 |'
- en: '| INT6 | 132 | 340 | 529 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| INT6 | 132 | 340 | 529 |'
- en: '| E2M1 | 111 | 92 | 443 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| E2M1 | 111 | 92 | 443 |'
- en: '| E3M2 | 223 | 138 | 498 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| E3M2 | 223 | 138 | 498 |'
- en: '| E2M1 * E1M2 | 105 | 107 | 432 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| E2M1 * E1M2 | 105 | 107 | 432 |'
- en: 'Table 6: Area differences of INT, FP and mixed Format FP operators across different
    bit-widths.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：不同位宽下INT、FP和混合格式FP运算符的面积差异。
- en: 6 Conclusion
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This paper presents the first successful demonstration of 4-bit floating-point
    post-training quantization for weights, activations, and embeddings in natural
    language transformer architectures, including both large language models and BERT
    model. We also extend our method to vision transformers and observe its robust
    generalization ability. Our approach involves a practical search-based technique
    which establishes a strong baseline and achieves state-of-the-art results for
    6-bit and 8-bit quantization. Furthermore, we address the challenge of high inter-channel
    variance in transformers by proposing pre-shifted exponent bias, which proves
    highly effective in achieving accurate 4-bit quantization.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本文首次成功演示了自然语言转换器架构中，权重、激活和嵌入的4位浮点后训练量化，包括大型语言模型和BERT模型。我们还将方法扩展到视觉转换器，并观察到其强大的泛化能力。我们的方法涉及一种实用的基于搜索的技术，建立了强大的基准，并在6位和8位量化中取得了最先进的结果。此外，我们通过提出预偏移指数偏置来应对转换器中高通道间方差的问题，这在实现准确的4位量化方面表现出极高的有效性。
- en: Acknowledgement
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This research is supported by National Natural Science Foundation of China/
    HKSAR Research Grants Council Joint Research Scheme under Grant $NHKUST627/20$.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到中国国家自然科学基金/香港特别行政区研究资助局联合研究计划的资助，资助号为$NHKUST627/20$。
- en: Limitations
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: Our experiments were conducted on publicly available datasets with finite sentence
    lengths, and the generalizability of our method to extremely long sequences or
    streaming data has not been verified and may require further investigation. In
    addition, it remains to be seen how our proposed method can generalize to other
    domains beyond language and vision, such as audio. It would also be interesting
    to see the applicability of our method to generative tasks and other applications.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验是在公开可用的数据集上进行的，这些数据集具有有限的句子长度，尚未验证我们的方法对极长序列或流数据的泛化能力，可能需要进一步调查。此外，尚不清楚我们提出的方法如何推广到语言和视觉之外的其他领域，例如音频。也很有趣的是观察我们的方法在生成任务和其他应用中的适用性。
- en: References
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: References
- en: 'Akbari et al. (2021) Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang,
    Shih-Fu Chang, Yin Cui, and Boqing Gong. 2021. Vatt: Transformers for multimodal
    self-supervised learning from raw video, audio and text. *Advances in Neural Information
    Processing Systems*, 34:24206–24221.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akbari et al. (2021) Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang,
    Shih-Fu Chang, Yin Cui, 和 Boqing Gong. 2021. Vatt：用于原始视频、音频和文本的多模态自监督学习的变换器。*神经信息处理系统进展*，34:24206–24221。
- en: Bai et al. (2022) Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, and
    Michael Lyu. 2022. [Towards efficient post-training quantization of pre-trained
    language models](https://openreview.net/forum?id=tvDRmAxGIjw). In *Advances in
    Neural Information Processing Systems*.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. (2022) Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, 和
    Michael Lyu. 2022. [针对预训练语言模型的高效后训练量化](https://openreview.net/forum?id=tvDRmAxGIjw)。在
    *神经信息处理系统进展*。
- en: Bondarenko et al. (2021) Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
    2021. [Understanding and overcoming the challenges of efficient transformer quantization](http://arxiv.org/abs/2109.12948).
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bondarenko et al. (2021) Yelysei Bondarenko, Markus Nagel, 和 Tijmen Blankevoort.
    2021. [理解和克服高效变换器量化的挑战](http://arxiv.org/abs/2109.12948)。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020. 语言模型是少量样本学习者。*神经信息处理系统进展*，33:1877–1901。
- en: 'Cai et al. (2020) Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W
    Mahoney, and Kurt Keutzer. 2020. Zeroq: A novel zero shot quantization framework.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 13169–13178.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai et al. (2020) Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W
    Mahoney, 和 Kurt Keutzer. 2020. Zeroq：一种新型零样本量化框架。在 *IEEE/CVF计算机视觉与模式识别会议论文集*，页13169–13178。
- en: Choukroun et al. (2019) Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev.
    2019. [Low-bit quantization of neural networks for efficient inference](http://arxiv.org/abs/1902.06822).
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choukroun et al. (2019) Yoni Choukroun, Eli Kravchik, Fan Yang, 和 Pavel Kisilev.
    2019. [神经网络的低位量化以实现高效推断](http://arxiv.org/abs/1902.06822)。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Llm.int8(): 8-bit matrix multiplication for transformers at scale. *Advances
    in Neural Information Processing Systems*, 35:30318–30332.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    2022. Llm.int8()：大规模变换器的8位矩阵乘法。*神经信息处理系统进展*，35:30318–30332。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [Bert: Pre-training of deep bidirectional transformers for language
    understanding](http://arxiv.org/abs/1810.04805).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2019. [Bert：深度双向变换器的预训练以进行语言理解](http://arxiv.org/abs/1810.04805)。
- en: Ding et al. (2022) Yifu Ding, Haotong Qin, Qinghua Yan, Zhenhua Chai, Junjie
    Liu, Xiaolin Wei, and Xianglong Liu. 2022. [Towards accurate post-training quantization
    for vision transformer](https://doi.org/10.1145/3503161.3547826). In *Proceedings
    of the 30th ACM International Conference on Multimedia*, MM ’22, page 5380–5388,
    New York, NY, USA. Association for Computing Machinery.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. (2022) Yifu Ding, Haotong Qin, Qinghua Yan, Zhenhua Chai, Junjie
    Liu, Xiaolin Wei, 和 Xianglong Liu. 2022. [精准后训练量化视觉变换器](https://doi.org/10.1145/3503161.3547826)。在
    *第30届ACM国际多媒体会议论文集*，MM ’22，页5380–5388，美国纽约。计算机协会。
- en: '(10) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition
    at scale. In *International Conference on Learning Representations*.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(10) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, 等. 一张图像值16x16个词: 大规模图像识别的Transformers. 见于*国际学习表征会议*。'
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2023. [GPTQ: Accurate post-training compression for generative pretrained
    transformers](https://openreview.net/forum?id=tcbBPnfwxS). In *International Conference
    on Learning Representations*.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan
    Alistarh. 2023. [GPTQ: 生成预训练Transformers的准确后训练压缩](https://openreview.net/forum?id=tcbBPnfwxS).
    见于*国际学习表征会议*。'
- en: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    2020. Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, 和 Dario Amodei.
    2020. 神经语言模型的扩展规律. *arXiv 预印本 arXiv:2001.08361*。
- en: 'Kenton and Toutanova (2019) Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina
    Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of NAACL-HLT*, pages 4171–4186.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kenton and Toutanova (2019) Jacob Devlin Ming-Wei Chang Kenton 和 Lee Kristina
    Toutanova. 2019. Bert: 深度双向Transformers的语言理解预训练. 见于*NAACL-HLT会议论文集*，第4171–4186页。'
- en: 'Kuzmin et al. (2022) Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel,
    Jorn Peters, and Tijmen Blankevoort. 2022. Fp8 quantization: The power of the
    exponent. *Advances in Neural Information Processing Systems*, 35:14651–14662.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kuzmin et al. (2022) Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel,
    Jorn Peters, 和 Tijmen Blankevoort. 2022. Fp8量化: 指数的力量. *神经信息处理系统进展*, 35:14651–14662。'
- en: 'Lee et al. (2023) Jemin Lee, Yongin Kwon, Jeman Park, Misun Yu, and Hwanjun
    Song. 2023. [Q-hyvit: Post-training quantization for hybrid vision transformer
    with bridge block reconstruction](http://arxiv.org/abs/2303.12557).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee et al. (2023) Jemin Lee, Yongin Kwon, Jeman Park, Misun Yu, 和 Hwanjun Song.
    2023. [Q-hyvit: 具有桥接块重建的混合视觉Transformer的后训练量化](http://arxiv.org/abs/2303.12557)。'
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
    Bart: Denoising sequence-to-sequence pre-training for natural language generation,
    translation, and comprehension. In *Proceedings of the 58th Annual Meeting of
    the Association for Computational Linguistics*, pages 7871–7880.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, 和 Luke Zettlemoyer. 2020. Bart:
    用于自然语言生成、翻译和理解的去噪序列到序列预训练. 见于*第58届计算语言学协会年会论文集*，第7871–7880页。'
- en: 'Li et al. (2021) Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, and Shi Gu. 2021. Brecq: Pushing the limit of post-training
    quantization by block reconstruction. *arXiv preprint arXiv:2102.05426*.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2021) Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, 和 Shi Gu. 2021. Brecq: 通过块重建推动后训练量化的极限. *arXiv 预印本 arXiv:2102.05426*。'
- en: 'Liu et al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    2023. Llm-qat: Data-free quantization aware training for large language models.
    *arXiv preprint arXiv:2305.17888*.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra.
    2023. Llm-qat: 大型语言模型的数据无关量化感知训练. *arXiv 预印本 arXiv:2305.17888*。'
- en: Liu et al. (2021) Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and
    Wen Gao. 2021. Post-training quantization for vision transformer. *Advances in
    Neural Information Processing Systems*, 34:28092–28103.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021) Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, 和 Wen
    Gao. 2021. 视觉Transformer的后训练量化. *神经信息处理系统进展*, 34:28092–28103。
- en: Micikevicius et al. (2022) Paulius Micikevicius, Dusan Stosic, Neil Burgess,
    Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke,
    Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart Oberman, Mohammad Shoeybi,
    Michael Siu, and Hao Wu. 2022. [Fp8 formats for deep learning](http://arxiv.org/abs/2209.05433).
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micikevicius et al. (2022) Paulius Micikevicius, Dusan Stosic, Neil Burgess,
    Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke,
    Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart Oberman, Mohammad Shoeybi,
    Michael Siu, 和 Hao Wu. 2022. [深度学习的Fp8格式](http://arxiv.org/abs/2209.05433)。
- en: Nagel et al. (2020) Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. 2020. Up or down? adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning*, pages 7197–7206\.
    PMLR.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel et al. (2020) Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, 和 Tijmen Blankevoort. 2020. 上还是下？训练后量化的自适应舍入。发表于 *国际机器学习会议*，第 7197–7206
    页。PMLR。
- en: Nagel et al. (2019) Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max
    Welling. 2019. [Data-free quantization through weight equalization and bias correction](http://arxiv.org/abs/1906.04721).
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel et al. (2019) Markus Nagel, Mart van Baalen, Tijmen Blankevoort, 和 Max
    Welling. 2019. [无数据量化通过权重平衡和偏差修正](http://arxiv.org/abs/1906.04721)。
- en: OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. *ArXiv*, abs/2303.08774.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. GPT-4 技术报告。*ArXiv*，abs/2303.08774。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu. 2020. 探索统一文本到文本变换器的迁移学习极限。*机器学习研究杂志*，21(1):5485–5551。
- en: Touvron et al. (2021) Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
    Massa, Alexandre Sablayrolles, and Hervé Jégou. 2021. Training data-efficient
    image transformers & distillation through attention. In *International conference
    on machine learning*, pages 10347–10357\. PMLR.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2021) Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
    Massa, Alexandre Sablayrolles, 和 Hervé Jégou. 2021. 训练数据高效的图像变换器与通过注意力的蒸馏。发表于
    *国际机器学习会议*，第 10347–10357 页。PMLR。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等等。2023. Llama: 开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*。'
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems*, 30.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 2017. 注意力即一切。*神经信息处理系统进展*，30。
- en: 'Wang et al. (2019) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. 2019. [Glue: A multi-task benchmark and analysis
    platform for natural language understanding](http://arxiv.org/abs/1804.07461).'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2019) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, 和 Samuel R. Bowman. 2019. [Glue: 自然语言理解的多任务基准和分析平台](http://arxiv.org/abs/1804.07461)。'
- en: 'Wei et al. (2022) Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei
    Yu. 2022. [QDrop: Randomly dropping quantization for extremely low-bit post-training
    quantization](https://openreview.net/forum?id=ySQH0oDyp7). In *International Conference
    on Learning Representations*.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei et al. (2022) Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, 和 Fengwei
    Yu. 2022. [QDrop: 随机丢弃量化用于极低比特训练后量化](https://openreview.net/forum?id=ySQH0oDyp7)。发表于
    *国际学习表征会议*。'
- en: 'Wu et al. (2020) Di Wu, Qi Tang, Yongle Zhao, Ming Zhang, Ying Fu, and Debing
    Zhang. 2020. [Easyquant: Post-training quantization via scale optimization](http://arxiv.org/abs/2006.16669).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. (2020) Di Wu, Qi Tang, Yongle Zhao, Ming Zhang, Ying Fu, 和 Debing
    Zhang. 2020. [Easyquant: 通过尺度优化的训练后量化](http://arxiv.org/abs/2006.16669)。'
- en: 'Xiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth,
    and Song Han. 2022. Smoothquant: Accurate and efficient post-training quantization
    for large language models. *arXiv preprint arXiv:2211.10438*.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth,
    和 Song Han. 2022. Smoothquant: 大型语言模型的准确且高效的训练后量化。*arXiv 预印本 arXiv:2211.10438*。'
- en: 'Yuan et al. (2022) Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, and Guangyu
    Sun. 2022. Ptq4vit: Post-training quantization for vision transformers with twin
    uniform quantization. In *Computer Vision–ECCV 2022: 17th European Conference,
    Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XII*, pages 191–207\.
    Springer.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan et al. (2022) Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, 和 Guangyu
    Sun. 2022. Ptq4vit: 具有双重均匀量化的视觉变换器训练后量化。发表于 *计算机视觉–ECCV 2022: 第 17 届欧洲会议，特拉维夫，以色列，2022
    年 10 月 23–27 日，会议录，第 XII 部分*，第 191–207 页。Springer。'
- en: Zhang et al. (2023) Yijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting
    Cao, Fan Yang, Mao Yang, Shanghang Zhang, and Ningyi Xu. 2023. [Integer or floating
    point? new outlooks for low-bit quantization on large language models](http://arxiv.org/abs/2305.12356).
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023）张怡佳、赵灵然、曹世杰、王文强、曹婷、杨凡、杨茂、张尚航、许宁义。2023。[整数还是浮点？大语言模型低位量化的新展望](http://arxiv.org/abs/2305.12356)。
- en: Appendix A Hessian-Based Loss Metric
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 基于Hessian的损失度量
- en: 'The objective of post-training quantization is to minimize the perturbation
    ($\delta\mathbf{X}=\mathbf{X}_{\rm FP}-\mathbf{X}_{\rm R}$) introduced by quantization
    to the pre-trained real-valued network:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化的目标是最小化量化对预训练的真实值网络引入的扰动（$\delta\mathbf{X}=\mathbf{X}_{\rm FP}-\mathbf{X}_{\rm
    R}$）：
- en: '|  | ${\rm min}\ \mathbb{E}[\mathcal{L}(\mathbf{X}_{\rm R}+\delta\mathbf{X})-\mathcal{L}(\mathbf{X}_{\rm
    R})]$ |  | (22) |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\rm min}\ \mathbb{E}[\mathcal{L}(\mathbf{X}_{\rm R}+\delta\mathbf{X})-\mathcal{L}(\mathbf{X}_{\rm
    R})]$ |  | (22) |'
- en: Following the Taylor series expansion, we have
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 根据泰勒级数展开，我们有
- en: '|  |  | $\displaystyle\mathbb{E}[\mathcal{L}(\mathbf{X}_{\rm R}+\delta\mathbf{X})-\mathcal{L}(\mathbf{X}_{\rm
    R})]$ |  | (23) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbb{E}[\mathcal{L}(\mathbf{X}_{\rm R}+\delta\mathbf{X})-\mathcal{L}(\mathbf{X}_{\rm
    R})]$ |  | (23) |'
- en: '|  | $\displaystyle\approx$ |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\approx$ |  |'
- en: '|  | $\displaystyle\approx$ |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\approx$ |  |'
- en: Here, $\bar{\mathbf{g}}^{(\mathbf{X})}$ can be neglected.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，可以忽略$\bar{\mathbf{g}}^{(\mathbf{X})}$。
- en: 'The Hessian matrix $\bar{\mathbf{H}}^{(\mathbf{X})}$ is computed as:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian矩阵$\bar{\mathbf{H}}^{(\mathbf{X})}$的计算方式如下：
- en: '|  | $\bar{\mathbf{H}}^{(\mathbf{X})}=\mathbf{J}^{T}_{\mathbf{O}}(\mathbf{X})\bar{\mathbf{H}}^{(\mathbf{O})}\mathbf{J}_{\mathbf{O}}(\mathbf{X})$
    |  | (24) |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bar{\mathbf{H}}^{(\mathbf{X})}=\mathbf{J}^{T}_{\mathbf{O}}(\mathbf{X})\bar{\mathbf{H}}^{(\mathbf{O})}\mathbf{J}_{\mathbf{O}}(\mathbf{X})$
    |  | (24) |'
- en: 'where $\mathbf{J}_{\mathbf{O}}(\mathbf{X})$. We then substitute the above equation
    back to equation LABEL:eq:target :'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{J}_{\mathbf{O}}(\mathbf{X})$。然后我们将上述方程代入方程 LABEL:eq:target：
- en: '|  |  | $\displaystyle\ \delta\mathbf{X}^{T}\bar{\mathbf{H}}^{(\mathbf{X})}\delta\mathbf{X}$
    |  | (25) |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\ \delta\mathbf{X}^{T}\bar{\mathbf{H}}^{(\mathbf{X})}\delta\mathbf{X}$
    |  | (25) |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle\approx$ |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\approx$ |  |'
- en: Here $\hat{\mathbf{O}}$ using first-order Taylor expansion.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里$\hat{\mathbf{O}}$使用了一阶泰勒展开。
- en: 'Nevertheless, the calculation of $\bar{\mathbf{H}}^{(\mathbf{O})}$ following
    Li et al. ([2021](#bib.bib17)); Yuan et al. ([2022](#bib.bib32)), and the new
    Hessian-based metric becomes:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，计算$\bar{\mathbf{H}}^{(\mathbf{O})}$遵循Li等人（[2021](#bib.bib17)）；Yuan等人（[2022](#bib.bib32)），新的基于Hessian的度量变为：
- en: '|  | $\displaystyle\mathbb{E}[(\hat{\mathbf{O}}-\mathbf{O})^{T}diag((\frac{\partial
    L}{\partial\mathbf{O}_{1}})^{2},...,(\frac{\partial L}{\partial\mathbf{O}_{n}})^{2}(\hat{\mathbf{O}}-\mathbf{O})]$
    |  | (26) |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}[(\hat{\mathbf{O}}-\mathbf{O})^{T}diag((\frac{\partial
    L}{\partial\mathbf{O}_{1}})^{2},...,(\frac{\partial L}{\partial\mathbf{O}_{n}})^{2}(\hat{\mathbf{O}}-\mathbf{O})]$
    |  | (26) |'
- en: Here, each entry of $\mathbf{O}$. In this study, this hessian-based metric is
    used as the reconstruction metric to search for the optimal FP quantization function
    for both the weight and activation when performing layer-wise reconstruction in
    BERT and Vision Transformer models.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个条目为$\mathbf{O}$。在本研究中，这种基于Hessian的度量被用作重建度量，以在BERT和Vision Transformer模型中执行逐层重建时搜索最优的浮点量化函数，包括权重和激活。
- en: '![Refer to caption](img/689372918aada6548f5b9f3b4765008c.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/689372918aada6548f5b9f3b4765008c.png)'
- en: 'Figure 4: Quantization error of different formats for BERT layers.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：BERT 层的不同格式量化误差。
- en: Appendix B Quantization Error of Different Floating-Point Formats
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 不同浮点格式的量化误差
- en: 'Figure [4](#A1.F4 "Figure 4 ‣ Appendix A Hessian-Based Loss Metric ‣ LLM-FP4:
    4-Bit Floating-Point Quantized Transformers") compares the quantization error
    of different formats in 8-bit quantization, including ${\rm INT8}$. We apply these
    formats to different BERT modules in the first, fifth, and last layers. The figures
    demonstrate that the optimal FP formats differs depending on the specific module
    that we are quantizing.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [4](#A1.F4 "图 4 ‣ 附录 A 基于Hessian的损失度量 ‣ LLM-FP4: 4位浮点量化变换器") 比较了8位量化中不同格式的量化误差，包括${\rm
    INT8}$。我们将这些格式应用于BERT的第一层、第五层和最后一层的不同模块。图示结果表明，最佳的浮点格式取决于我们量化的具体模块。'
- en: '![Refer to caption](img/fac5540ed69ca0ec2715181fec28ffad.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/fac5540ed69ca0ec2715181fec28ffad.png)'
- en: 'Figure 5: Magnitude of the output activations of different modules in BERT
    (left column), and DeiT-S (right column).'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：BERT中不同模块的输出激活幅度（左列），以及DeiT-S（右列）。
- en: '![Refer to caption](img/e3a4aa923e6fac0022f319db27bc8dd3.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/e3a4aa923e6fac0022f319db27bc8dd3.png)'
- en: 'Figure 6: Magnitude of the output activations of different modules in LLaMA-7B.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：LLaMA-7B 中不同模块的输出激活幅度。
- en: Appendix C Inter-Channel Variance Visualization
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 通道间方差可视化
- en: 'Figure [5](#A2.F5 "Figure 5 ‣ Appendix B Quantization Error of Different Floating-Point
    Formats ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers") and [6](#A2.F6
    "Figure 6 ‣ Appendix B Quantization Error of Different Floating-Point Formats
    ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers") depict the output of
    different fully-connected layers in BERT for the MNLI task, DeiT-S for the ImageNet-1K
    task, and LLaMA-7B for the zero-shot reasoning task. The visualizations reveal
    a noticeable inter-channel variance presented in both language and vision transformers.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#A2.F5 "Figure 5 ‣ Appendix B Quantization Error of Different Floating-Point
    Formats ‣ LLM-FP4: 4-Bit Floating-Point Quantized Transformers") 和[6](#A2.F6 "Figure
    6 ‣ Appendix B Quantization Error of Different Floating-Point Formats ‣ LLM-FP4:
    4-Bit Floating-Point Quantized Transformers") 描述了 BERT 在 MNLI 任务、DeiT-S 在 ImageNet-1K
    任务和 LLaMA-7B 在零-shot 推理任务中的不同全连接层的输出。这些可视化揭示了语言和视觉变换器中存在的显著通道间方差。'
- en: Appendix D Efficient Matrix Multiplication
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 高效矩阵乘法
- en: 'Figure [7](#A4.F7 "Figure 7 ‣ Appendix D Efficient Matrix Multiplication ‣
    LLM-FP4: 4-Bit Floating-Point Quantized Transformers") displays a comprehensive
    list of all the granularity options that allow for efficient matrix multiplication.
    While per-token quantization theoretically provides greater precision in terms
    of quantization granularity, the accuracy gains achieved through this method are
    minimal and do not justify the additional computational overhead required. As
    a result, we have opted to use per-tensor quantization when quantizing activations.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '图[7](#A4.F7 "Figure 7 ‣ Appendix D Efficient Matrix Multiplication ‣ LLM-FP4:
    4-Bit Floating-Point Quantized Transformers") 显示了所有能够实现高效矩阵乘法的粒度选项的详细列表。虽然按令牌量化理论上提供了更高的量化粒度精度，但通过这种方法获得的准确性提升是微小的，并且不足以证明所需额外计算开销的合理性。因此，我们选择在量化激活时使用按张量量化。'
- en: '![Refer to caption](img/89de879feeeadcfbbc0364055b04de22.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/89de879feeeadcfbbc0364055b04de22.png)'
- en: 'Figure 7: Quantization granularity options that support efficient matrix multiplication.
    The dimensions that share the same scaling factor are indicated with red dotted
    frames'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：支持高效矩阵乘法的量化粒度选项。共享相同缩放因子的维度以红色虚线框标出。
- en: Appendix E Learning Format and Maximum Value
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 学习格式与最大值
- en: We compare the previous gradient-based method Kuzmin et al. ([2022](#bib.bib14))
    with the proposed search-based method for finding the optimal format and maximum
    value. On DeiT-S, the learnable method only achieves 74.38% accuracy for an 8-bit
    quantized model on ImageNet, in contrast, FPQ can attain an almost loss-less result
    of 79.88%. We analyze the gradients for the number of exponent bits $e$ derived
    in Kuzmin et al. ([2022](#bib.bib14)) and observe that each time the exponent
    bits change, the gradients experience exponential variations, leading to high
    instability. Based on this observation, we assert that employing a search-based
    method to determine the optimal formats is crucial in post-training quantization
    (PTQ).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将之前的基于梯度的方法 Kuzmin 等人 ([2022](#bib.bib14)) 与提出的基于搜索的方法进行比较，以寻找最佳格式和最大值。在 DeiT-S
    上，可学习的方法在 ImageNet 上的 8 位量化模型仅能达到 74.38% 的准确率，相比之下，FPQ 可以实现几乎无损的 79.88% 结果。我们分析了
    Kuzmin 等人 ([2022](#bib.bib14)) 中导出的指数位 $e$ 的梯度，并观察到每次指数位变化时，梯度会经历指数级变化，导致高度不稳定。基于这一观察，我们断言，在后训练量化
    (PTQ) 中，采用基于搜索的方法来确定最佳格式至关重要。
- en: Appendix F Reconstruction Choices
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 重构选择
- en: 'The previous works on integer post-training quantization involves breaking
    down the target model into sub-modules and reconstructing them separately Nagel
    et al. ([2020](#bib.bib21)); Li et al. ([2021](#bib.bib17)); Bai et al. ([2022](#bib.bib2));
    Yuan et al. ([2022](#bib.bib32)). This addresses the problem of over-fitting,
    given that only a limited amount of unlabeled calibration data is available. In
    this study we find the layer-wise reconstruction and parallel quantization works
    best for floating-point PTQ:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 之前关于整数后训练量化的研究涉及将目标模型分解为子模块并分别重构它们 Nagel 等人 ([2020](#bib.bib21)); Li 等人 ([2021](#bib.bib17));
    Bai 等人 ([2022](#bib.bib2)); Yuan 等人 ([2022](#bib.bib32))。这解决了过拟合问题，因为只有有限的未标记校准数据可用。在这项研究中，我们发现层级重构和并行量化在浮点
    PTQ 中效果最佳：
- en: 'Layer Reconstruction: Recent research Li et al. ([2021](#bib.bib17)); Bai et al.
    ([2022](#bib.bib2)) suggests increasing the reconstruction granularity from layer
    reconstruction Nagel et al. ([2020](#bib.bib21)) to block reconstruction Li et al.
    ([2021](#bib.bib17)) or even larger granularity Lee et al. ([2023](#bib.bib15)).
    This is achieved by jointly optimizing all the linear layers or matrix multiplication
    components within each module to prevent the propagation of reconstruction errors
    among the layers. Despite this, we have observed that increasing the reconstruction
    granularity does not improve the accuracy of FPQ baseline or sometimes even lead
    to worse results. Therefore, we choose layer reconstruction.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 层重建：最近的研究 Li et al. ([2021](#bib.bib17)); Bai et al. ([2022](#bib.bib2)) 表明，从层重建
    Nagel et al. ([2020](#bib.bib21)) 增加重建粒度到块重建 Li et al. ([2021](#bib.bib17)) 或者更大的粒度
    Lee et al. ([2023](#bib.bib15)) 是有益的。这是通过联合优化每个模块中的所有线性层或矩阵乘法组件来实现的，以防止重建误差在层之间传播。尽管如此，我们观察到增加重建粒度并没有提高
    FPQ 基线的准确性，有时甚至会导致更差的结果。因此，我们选择层重建。
- en: 'Parallel Quantization: Sequential quantization is the most commonly used approach
    Wu et al. ([2020](#bib.bib30)); Nagel et al. ([2020](#bib.bib21)); Li et al. ([2021](#bib.bib17))
    where modules are quantized consecutively based on their sequential order, and
    the input for the current calibrating module is generated using all the previously
    quantized modules. However, some recent works Yuan et al. ([2022](#bib.bib32));
    Bai et al. ([2022](#bib.bib2)) proposed a new parallel quantization framework.
    This framework uses the raw output of the full-precision modules as input and
    makes the calibration of each module independent from one another. In this work,
    we use parallel quantization, as it yields better results than its sequential
    counterparts.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 并行量化：顺序量化是最常用的方法 Wu et al. ([2020](#bib.bib30)); Nagel et al. ([2020](#bib.bib21));
    Li et al. ([2021](#bib.bib17))，其中模块按照顺序逐个量化，当前校准模块的输入是使用所有先前量化的模块生成的。然而，一些最近的工作
    Yuan et al. ([2022](#bib.bib32)); Bai et al. ([2022](#bib.bib2)) 提出了一个新的并行量化框架。该框架使用全精度模块的原始输出作为输入，使每个模块的校准彼此独立。在这项工作中，我们使用并行量化，因为它比顺序量化方法效果更佳。
