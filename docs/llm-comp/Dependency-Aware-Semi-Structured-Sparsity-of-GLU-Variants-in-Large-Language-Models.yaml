- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:04:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:53
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language
    Models
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型中 GLU 变体的依赖感知半结构稀疏性
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.01943](https://ar5iv.labs.arxiv.org/html/2405.01943)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.01943](https://ar5iv.labs.arxiv.org/html/2405.01943)
- en: Zhiyu Guo    Hidetaka Kamigaito    Taro Wanatnabe
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 郭智宇    上田英高    渡边太郎
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The rapid advancement in Large Language Models (LLMs) has markedly enhanced
    the capabilities of language understanding and generation. However, the substantial
    model size poses hardware challenges, affecting both memory size for serving and
    inference latency for token generation. To address those challenges, we propose
    Dependency-aware Semi-structured Sparsity (DaSS), a novel method for the recent
    prevalent SwiGLU-based LLMs pruning. Our approach incorporates structural dependency
    into the weight magnitude-based unstructured pruning. We introduce an MLP-specific
    pruning metric that evaluates the importance of each weight by jointly considering
    its magnitude and its corresponding MLP intermediate activation norms. DaSS facilitates
    a balance between the adaptability offered by unstructured pruning and the structural
    consistency inherent in dependency-based structured pruning. Empirical evaluations
    on Mistral and LLaMA2 model families demonstrate that DaSS not only outperforms
    both SparseGPT and Wanda in achieving hardware-friendly N:M sparsity patterns
    but also maintains the computational efficiency of Wanda.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的快速进展显著提升了语言理解和生成的能力。然而，模型体积庞大带来了硬件挑战，影响了服务所需的内存大小和生成标记的推理延迟。为了解决这些挑战，我们提出了依赖感知半结构稀疏性（DaSS），这是一种用于最近流行的基于
    SwiGLU 的 LLM 剪枝的新方法。我们的方法将结构依赖性纳入基于权重幅度的非结构化剪枝中。我们引入了一种针对 MLP 的特定剪枝度量，通过联合考虑权重的幅度及其对应的
    MLP 中间激活规范来评估每个权重的重要性。DaSS 在非结构化剪枝提供的适应性和基于依赖的结构化剪枝固有的结构一致性之间实现了平衡。对 Mistral 和
    LLaMA2 模型家族的实证评估表明，DaSS 不仅在实现硬件友好的 N:M 稀疏模式方面优于 SparseGPT 和 Wanda，而且还保持了 Wanda
    的计算效率。
- en: Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent years have witnessed the great success of Transformer-based Large Language
    Models (LLMs) (Brown et al., [2020](#bib.bib4); Chowdhery et al., [2023](#bib.bib6);
    OpenAI, [2023](#bib.bib34)) across various challenging tasks, such as mathematical
    reasoning (Cobbe et al., [2021](#bib.bib8)), code generation (Chen et al., [2021](#bib.bib5)).
    However, the practical use of these models for inference has faced a major obstacle
    due to the substantial computational resources they consume. To tackle this, several
    methods for efficient LLMs inference have been applied, including training compact
    models using more data (Touvron et al., [2023a](#bib.bib42)), model quantization
    (Dettmers et al., [2022](#bib.bib11); Frantar et al., [2023](#bib.bib15)), and
    grouped query attention (Shazeer, [2019](#bib.bib38); Ainslie et al., [2023](#bib.bib1)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，基于 Transformer 的大型语言模型（LLMs）（Brown et al., [2020](#bib.bib4); Chowdhery
    et al., [2023](#bib.bib6); OpenAI, [2023](#bib.bib34)）在各种具有挑战性的任务中取得了巨大成功，如数学推理（Cobbe
    et al., [2021](#bib.bib8)），代码生成（Chen et al., [2021](#bib.bib5)）。然而，这些模型在实际推理中的应用面临着一个主要障碍，即它们消耗的计算资源非常庞大。为了解决这个问题，已经应用了几种高效的
    LLM 推理方法，包括使用更多数据训练紧凑模型（Touvron et al., [2023a](#bib.bib42)）、模型量化（Dettmers et al.,
    [2022](#bib.bib11); Frantar et al., [2023](#bib.bib15)），以及分组查询注意力（Shazeer, [2019](#bib.bib38);
    Ainslie et al., [2023](#bib.bib1)）。
- en: 'Neural network pruning (Han et al., [2015](#bib.bib20); Wen et al., [2016](#bib.bib44))
    is a widely adopted technique for compressing models, leading to a substantial
    reduction in model size and inference latency. Pruning can be categorized into
    two main approaches: unstructured pruning (Sun et al., [2023](#bib.bib40); Frantar
    & Alistarh, [2023](#bib.bib14)), which involves the removal of specific weights,
    and structured pruning (Zhang et al., [2023](#bib.bib49); Ma et al., [2023](#bib.bib29)),
    which entails the removal of complete rows or columns of weights. In contrast
    to structured pruning, which struggles with performance in LLMs even at low sparsity
    levels, unstructured pruning (Sun et al., [2023](#bib.bib40); Frantar & Alistarh,
    [2023](#bib.bib14)) exhibits promising results without additional retraining,
    and achieves practical speedup in Nvidia GPU through more stringent N:M sparsity
    pattern (Mishra et al., [2021](#bib.bib33); Kurtic et al., [2023](#bib.bib25)).
    SparseGPT (Frantar & Alistarh, [2023](#bib.bib14)) tackles the issue of pruning
    LLMs by considering it as a problem of layerwise reconstruction. It exclusively
    depends on weight updates aimed at maintaining the input-output relationship for
    each layer. Motivated by the emergent large magnitude activations in LLMs (Dettmers
    et al., [2022](#bib.bib11)), Wanda (Sun et al., [2023](#bib.bib40)) further eases
    the computational load in SparseGPT by solely employing the product derived from
    weights and input activation magnitudes.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络剪枝（Han et al., [2015](#bib.bib20)；Wen et al., [2016](#bib.bib44)）是一种广泛采用的模型压缩技术，能够显著减少模型大小和推理延迟。剪枝可以分为两种主要方法：无结构剪枝（Sun
    et al., [2023](#bib.bib40)；Frantar & Alistarh, [2023](#bib.bib14)），涉及特定权重的移除，以及结构剪枝（Zhang
    et al., [2023](#bib.bib49)；Ma et al., [2023](#bib.bib29)），涉及完整行或列权重的移除。与结构剪枝相比，后者即使在低稀疏度下也面临性能问题，无结构剪枝（Sun
    et al., [2023](#bib.bib40)；Frantar & Alistarh, [2023](#bib.bib14)）在没有额外重训练的情况下表现出有前景的结果，并通过更严格的
    N:M 稀疏模式（Mishra et al., [2021](#bib.bib33)；Kurtic et al., [2023](#bib.bib25)）在
    Nvidia GPU 上实现了实际的加速。SparseGPT（Frantar & Alistarh, [2023](#bib.bib14)）通过将剪枝 LLMs
    视为逐层重建的问题来解决此问题。它专注于通过权重更新来维持每层的输入输出关系。受 LLMs 中出现的大幅度激活（Dettmers et al., [2022](#bib.bib11)）的启发，Wanda（Sun
    et al., [2023](#bib.bib40)）通过仅使用从权重和输入激活幅度中得出的乘积进一步减轻了 SparseGPT 的计算负担。
- en: '![Refer to caption](img/cb5edabb0905836aa19d6d027c7f5f16.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cb5edabb0905836aa19d6d027c7f5f16.png)'
- en: (a) Dependency-based Structured Sparsity
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 依赖基础结构稀疏性
- en: '![Refer to caption](img/5f90f8e5bcf90b8af80332ebcb1ab08f.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5f90f8e5bcf90b8af80332ebcb1ab08f.png)'
- en: (b) Wanda Unstructured Sparsity
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Wanda 无结构稀疏性
- en: '![Refer to caption](img/061178269c51845c51e64b91dc350c50.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/061178269c51845c51e64b91dc350c50.png)'
- en: (c) Dependency-aware Semi-structured Sparsity
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 依赖感知半结构稀疏性
- en: 'Figure 1: Illustration of our proposed Dependency-aware Semi-structured Sparsity
    (DaSS). In (a) dependency-based structured pruning (Ma et al., [2023](#bib.bib29)),
    all the weights connecting to the same intermediate neuron are removed or remain
    simultaneously. In (b) Wanda unstructured pruning (Sun et al., [2023](#bib.bib40)),
    it assigns greater emphasis to the weights corresponding to large input activations.
    For Gate-Proj and UP-Proj, the same number of weights are removed for each MLP
    neuron, regardless of whether some neurons have much larger activation norms.
    For Down-Proj, the weights corresponding to larger activation norms are more likely
    to be pruned. This can lead to a structural mismatch. In (c), all the weights
    corresponding to large intermediate activations are more likely to be reserved.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们提出的依赖感知半结构稀疏性（DaSS）的示意图。在（a）依赖基础结构剪枝（Ma et al., [2023](#bib.bib29)）中，所有连接到相同中间神经元的权重要么同时被移除，要么同时保留。在（b）Wanda
    无结构剪枝（Sun et al., [2023](#bib.bib40)）中，它对与大输入激活相关的权重赋予更大的重视。对于 Gate-Proj 和 UP-Proj，每个
    MLP 神经元移除的权重数量相同，无论某些神经元是否具有更大的激活范数。对于 Down-Proj，具有较大激活范数的权重更可能被剪枝。这可能导致结构不匹配。在（c）中，所有与大中间激活相关的权重更可能被保留。
- en: 'As the SwiGLU-based MLP (Shazeer, [2020](#bib.bib39)) module accounts for more
    than 80% parameters in the recent LLMs that use multi/grouped query attention
    (Chowdhery et al., [2023](#bib.bib6); Touvron et al., [2023b](#bib.bib43))¹¹1In
    LLaMA2-70B, the dimension of key and value is $\frac{1}{8}d$., its pruning emerges
    as a pivotal factor in determining the overall compression efficacy of LLMs. In
    dependency-aware structured pruning, it’s critical to consider that pruned parameters
    have dependencies with other parameters, owing to their interconnected nature
    (Ma et al., [2023](#bib.bib29); Fang et al., [2023](#bib.bib13)). In the context
    of MLP pruning, all the weights connected to each intermediate neuron should be
    preserved or pruned simultaneously. The precise coordination involved in pruning
    is crucial for upholding the model’s structural integrity and its functional capabilities.
    Although current unstructured pruning methods (Sun et al., [2023](#bib.bib40);
    Frantar & Alistarh, [2023](#bib.bib14)) effectively remove a significant number
    of redundant weights, they operate entirely locally within each linear layer without
    considering inter-dependencies to other layers. This can lead to a structural
    mismatch, which is more evident in Wanda as shown in Figure [1(b)](#S1.F1.sf2
    "Figure 1(b) ‣ Figure 1 ‣ 1 Introduction ‣ Dependency-Aware Semi-Structured Sparsity
    of GLU Variants in Large Language Models"): In Gate and Up projections, the same
    amount of parameters are pruned for each MLP neuron. However, the intermediate
    activation norms of SwiGLU are not uniformly distributed and some neurons have
    much larger norms than others. Based on Wanda pruning metric, more weights connected
    to neurons with large activation norms are preserved. At high sparsity, this problem
    gets magnified in Wanda causing significant drops in performance by the broken
    network.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于SwiGLU的MLP（Shazeer, [2020](#bib.bib39)）模块在最近使用多/分组查询注意力的LLM中占据了超过80%的参数（Chowdhery
    et al., [2023](#bib.bib6); Touvron et al., [2023b](#bib.bib43)）¹¹1在LLaMA2-70B中，键和值的维度是$\frac{1}{8}d$，其剪枝成为了决定LLM整体压缩效能的关键因素。在依赖感知的结构化剪枝中，考虑到剪枝的参数与其他参数存在依赖关系至关重要，因为它们之间存在相互连接（Ma
    et al., [2023](#bib.bib29); Fang et al., [2023](#bib.bib13)）。在MLP剪枝的背景下，与每个中间神经元连接的所有权重都应同时保留或剪枝。剪枝中的精确协调对维护模型的结构完整性和功能能力至关重要。尽管当前的无结构剪枝方法（Sun
    et al., [2023](#bib.bib40); Frantar & Alistarh, [2023](#bib.bib14)）有效地移除了大量冗余权重，但它们完全在每个线性层内局部操作，没有考虑与其他层的相互依赖。这可能导致结构不匹配，在Wanda中更为明显，如图[1(b)](#S1.F1.sf2
    "Figure 1(b) ‣ Figure 1 ‣ 1 Introduction ‣ Dependency-Aware Semi-Structured Sparsity
    of GLU Variants in Large Language Models")所示：在Gate和Up投影中，每个MLP神经元剪枝的参数数量相同。然而，SwiGLU的中间激活规范并不是均匀分布的，有些神经元的规范远大于其他神经元。基于Wanda剪枝指标，更多连接到具有大激活规范的神经元的权重被保留。在高稀疏度下，这个问题在Wanda中会被放大，导致网络性能显著下降。
- en: In order to overcome the limitations present in current pruning methodologies,
    we introduce a new paradigm, namely, Dependency-aware Semi-structured Sparsity
    (DaSS). This approach is specifically designed to navigate the middle ground between
    the flexibility of unstructured pruning and the structural consistency of dependency-based
    structured pruning. We examine the effectiveness of magnitude pruning in Gate
    and Up projections of SwiGLU, and we find that grouping weights per-input is surprisingly
    effective compared with Wanda (Sun et al., [2023](#bib.bib40)). Leveraging these
    newly acquired understandings, we present a new MLP pruning metric that assesses
    each weight’s importance based on the product of its magnitude and the norm of
    the corresponding MLP intermediate activations. Our proposed DaSS method, illustrated
    in Figure [1(c)](#S1.F1.sf3 "Figure 1(c) ‣ Figure 1 ‣ 1 Introduction ‣ Dependency-Aware
    Semi-Structured Sparsity of GLU Variants in Large Language Models"), embodies
    a semi-structured pattern that retains a degree of the adaptability inherent in
    unstructured pruning while incorporating the dependency-aware aspect of structured
    pruning. This balance allows for more precise pruning. DaSS can be easily extended
    to hardware-friendly N:M sparsity patterns.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服当前剪枝方法中的局限性，我们引入了一种新的范式，即依赖感知半结构化稀疏性（DaSS）。这种方法专门设计用来在非结构化剪枝的灵活性和基于依赖关系的结构化剪枝的一致性之间找到平衡。我们检查了在
    SwiGLU 的 Gate 和 Up 投影中的幅度剪枝效果，并发现按输入分组权重的效果相比 Wanda (Sun et al., [2023](#bib.bib40))
    出乎意料地有效。利用这些新获得的理解，我们提出了一种新的 MLP 剪枝指标，根据权重幅度和相应 MLP 中间激活的范数的乘积来评估每个权重的重要性。我们提出的
    DaSS 方法，如图 [1(c)](#S1.F1.sf3 "图 1(c) ‣ 图 1 ‣ 1 引言 ‣ GLU 变体在大型语言模型中的依赖感知半结构化稀疏性")
    所示，体现了一种半结构化的模式，保留了非结构化剪枝中固有的适应性，同时融合了结构化剪枝的依赖感知方面。这种平衡使得剪枝更加精准。DaSS 可以轻松扩展到适合硬件的
    N:M 稀疏模式。
- en: We perform extensive experiments on LLaMA2 and Mistral to evaluate DaSS across
    various tasks from language modeling, commonsense reasoning, and MMLU (Hendrycks
    et al., [2021](#bib.bib22)). In achieving hardware-friendly N:M sparsity patterns,
    DaSS consistently excels beyond the existing LLM pruning methods SparseGPT (Frantar
    & Alistarh, [2023](#bib.bib14)) and Wanda (Sun et al., [2023](#bib.bib40)), while
    maintaining the computational efficiency akin to Wanda. Impressively, DaSS outperforms
    SparseGPT at high sparsity even without weight update. Our work contributes fresh
    insights into the nuanced understanding of sparsity within large language models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 LLaMA2 和 Mistral 上进行了广泛的实验，以评估 DaSS 在语言建模、常识推理和 MMLU (Hendrycks et al.,
    [2021](#bib.bib22)) 等各种任务中的表现。在实现适合硬件的 N:M 稀疏模式方面，DaSS 一直优于现有的 LLM 剪枝方法 SparseGPT
    (Frantar & Alistarh, [2023](#bib.bib14)) 和 Wanda (Sun et al., [2023](#bib.bib40))，同时保持了类似
    Wanda 的计算效率。令人印象深刻的是，DaSS 即使在高稀疏情况下没有权重更新也优于 SparseGPT。我们的工作为对大型语言模型中稀疏性的细致理解提供了新的见解。
- en: 2 Preliminary Exploration
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 初步探索
- en: 2.1 Wanda Pruning Method
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 Wanda 剪枝方法
- en: 'In the context of LLM pruning, we denote a linear layer weight matrix $\mathbf{W}\in\mathbb{R}^{d_{\text{out}}\times
    d_{\text{in}}}$ is determined as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLM 剪枝的背景下，我们表示线性层权重矩阵 $\mathbf{W}\in\mathbb{R}^{d_{\text{out}}\times d_{\text{in}}}$
    的确定方式如下：
- en: '|  | $\mathbf{I}_{i,j}=\left&#124;\mathbf{W}_{i,j}\right&#124;\cdot\left\&#124;\mathbf{X}_{j}\right\&#124;_{2}$
    |  | (1) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{I}_{i,j}=\left\|\mathbf{W}_{i,j}\right\|\cdot\left\|\mathbf{X}_{j}\right\|_{2}$
    |  | (1) |'
- en: Here, $\mathbf{I}_{i,j}$. We call this as output-balanced granularity.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\mathbf{I}_{i,j}$。我们称之为输出平衡粒度。
- en: 2.2 GLU Variants for Transformer
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 GLU 变体在 Transformer 中的应用
- en: Gated Linear Units (GLU) (Dauphin et al., [2017](#bib.bib9)) are formed by the
    element-wise multiplication of two linear projections, with a sigmoid function
    applied to one projection before the multiplication. Shazeer ([2020](#bib.bib39))
    suggests an alternative design for the Transformer’s MLP layer that incorporates
    GLU variants, effectively replacing the conventional first linear transformation
    and activation function. More formally, we denote the $d_{\text{hidden}}$ by evaluating
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 门控线性单元（GLU）（Dauphin et al., [2017](#bib.bib9)）通过两个线性投影的逐元素乘法形成，其中对一个投影应用 sigmoid
    函数，然后进行乘法操作。Shazeer ([2020](#bib.bib39)) 提出了 Transformer 的 MLP 层的替代设计，融入了 GLU
    变体，有效地替代了传统的第一个线性变换和激活函数。更正式地，我们通过评估来表示 $d_{\text{hidden}}$
- en: '|  | $\mathbf{y}=\sigma(\mathbf{x}\mathbf{W_{1}})\otimes\mathbf{x}\mathbf{W_{2}}$
    |  | (2) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{y}=\sigma(\mathbf{x}\mathbf{W_{1}})\otimes\mathbf{x}\mathbf{W_{2}}$
    |  | (2) |'
- en: '|  | $\mathbf{z}=\mathbf{y}\mathbf{W_{3}}$ |  | (3) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{z}=\mathbf{y}\mathbf{W_{3}}$ |  | (3) |'
- en: We call $\mathbf{W_{1}},\mathbf{W_{2}},\mathbf{W_{3}}$ linear projections as
    Gate-Proj, Up-Proj, and Down-Proj, respectively. GLU variants that use Swish (Ramachandran
    et al., [2017](#bib.bib36)) and ReLU (Glorot et al., [2011](#bib.bib19)) activation
    functions in Eq.([2](#S2.E2 "Equation 2 ‣ 2.2 GLU Variants for Transformer ‣ 2
    Preliminary Exploration ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants
    in Large Language Models")) are called SwiGLU and ReGLU, respectively. SwiGLU
    is most widely used in the recent LLMs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将$\mathbf{W_{1}},\mathbf{W_{2}},\mathbf{W_{3}}$线性投影称为Gate-Proj、Up-Proj和Down-Proj。使用Swish
    (Ramachandran et al., [2017](#bib.bib36))和ReLU (Glorot et al., [2011](#bib.bib19))激活函数的GLU变体在方程([2](#S2.E2
    "Equation 2 ‣ 2.2 GLU Variants for Transformer ‣ 2 Preliminary Exploration ‣ Dependency-Aware
    Semi-Structured Sparsity of GLU Variants in Large Language Models"))中被分别称为SwiGLU和ReGLU。SwiGLU在近期的大型语言模型中使用最广泛。
- en: 2.3 Revisiting the Effectiveness of Magnitude Pruning
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 重新审视幅度修剪的有效性
- en: Sun et al. ([2023](#bib.bib40)) conducted ablation studies among different granularities
    of magnitude pruning, including input-balanced granularity, which sorts weights
    connecting to the same input neuron. Compared with Wanda pruning, input-balanced
    magnitude pruning is just slightly worse.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Sun等人 ([2023](#bib.bib40)) 进行了一系列不同粒度的幅度修剪的消融研究，包括输入平衡粒度，它对连接到相同输入神经元的权重进行排序。与Wanda修剪相比，输入平衡幅度修剪略差。
- en: Here, we further investigate the effectiveness of input-balanced magnitude pruning
    for the MLP module. We divide the SwiGLU-based MLP module into two groups, input
    projections (Gate-Proj and Up-Proj) and output projection (Down-Proj). We prune
    those two groups separately and compare Wanda pruning with input-balanced magnitude
    pruning. Table [1](#S2.T1 "Table 1 ‣ 2.3 Revisiting the Effectiveness of Magnitude
    Pruning ‣ 2 Preliminary Exploration ‣ Dependency-Aware Semi-Structured Sparsity
    of GLU Variants in Large Language Models") shows that input-balanced magnitude
    pruning is surprisingly better than Wanda pruning at higher sparsity ratios for
    input projections pruning. For the output projection, input-balanced magnitude
    pruning is consistently worse than Wanda pruning. This suggests the diminished
    importance of outlier features within input projections compared to the output
    projection. We analyze the layerwise intermediate activation outlier distribution
    in Appendix [B](#A2 "Appendix B Activation Outlier in SwiGLU ‣ Dependency-Aware
    Semi-Structured Sparsity of GLU Variants in Large Language Models"). However,
    in the older GeLU-based MLP expansion projection, we observe Wanda can achieve
    much better performance as shown in Appendix [C](#A3 "Appendix C GeLU-based MLP
    Input projection ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants in
    Large Language Models"), suggesting this is a special property of GLU variants.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们进一步研究了输入平衡幅度修剪在MLP模块中的有效性。我们将基于SwiGLU的MLP模块分为两组：输入投影（Gate-Proj和Up-Proj）和输出投影（Down-Proj）。我们分别修剪这两组，并将Wanda修剪与输入平衡幅度修剪进行比较。表[1](#S2.T1
    "Table 1 ‣ 2.3 Revisiting the Effectiveness of Magnitude Pruning ‣ 2 Preliminary
    Exploration ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large
    Language Models")显示，对于输入投影修剪，输入平衡幅度修剪在较高稀疏率下比Wanda修剪表现出惊人的优势。对于输出投影，输入平衡幅度修剪的表现始终逊色于Wanda修剪。这表明，相比于输出投影，输入投影中离群特征的重要性降低。我们在附录[B](#A2
    "Appendix B Activation Outlier in SwiGLU ‣ Dependency-Aware Semi-Structured Sparsity
    of GLU Variants in Large Language Models")中分析了逐层的中间激活离群分布。然而，在较旧的GeLU基础MLP扩展投影中，我们观察到Wanda可以取得更好的性能，如附录[C](#A3
    "Appendix C GeLU-based MLP Input projection ‣ Dependency-Aware Semi-Structured
    Sparsity of GLU Variants in Large Language Models")所示，这表明这是GLU变体的一个特殊属性。
- en: 'Table 1: Comparison of Wanda pruning and Input-balanced Magnitude pruning,
    perplexity with LLaMA2-7B on WikiText'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：Wanda修剪与输入平衡幅度修剪的比较，使用LLaMA2-7B在WikiText上的困惑度
- en: '|  | Up+Gate Projection | Down Projection |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | Up+Gate Projection | Down Projection |'
- en: '| Sparsity | 50% | 60% | 70% | 50% | 60% | 70% |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏率 | 50% | 60% | 70% | 50% | 60% | 70% |'
- en: '| Wanda | 6.05 | 7.12 | 12.42 | 5.76 | 6.11 | 6.97 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 6.05 | 7.12 | 12.42 | 5.76 | 6.11 | 6.97 |'
- en: '| Magnitude | 6.12 | 7.10 | 10.52 | 5.87 | 6.56 | 9.03 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 幅度 | 6.12 | 7.10 | 10.52 | 5.87 | 6.56 | 9.03 |'
- en: 2.4 Output-balanced Pruning Limitations
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 输出平衡修剪的局限性
- en: N:M sparsity pattern offers notable speed improvements on recent NVIDIA GPUs
    (Mishra et al., [2021](#bib.bib33); Kurtic et al., [2023](#bib.bib25)). It is
    specified that every group of M consecutive weights must include N zeros. SparseGPT
    (Frantar & Alistarh, [2023](#bib.bib14)) is not explicitly designed for output-balanced
    pruning granularity. When converting into an N:M sparsity pattern, SparseGPT forces
    every M consecutive weight in each row to have exactly N zeros. In such cases,
    SparseGPT (Frantar & Alistarh, [2023](#bib.bib14)) and Wanda (Sun et al., [2023](#bib.bib40))
    unanimously remove the same amount of weights for each output. Such methods often
    emphasize the significance of individual components within the weight matrix and
    overlook inter-dependencies to other layers in the network. For MLP input projections
    pruning, an equal amount of weights corresponding to each intermediate neuron
    are removed. However, for output projection based on SparseGPT and Wanda pruning
    metrics, the weights connecting to intermediate neurons with larger activations
    are more likely to be reserved leading to a structural mismatch.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: N:M 稀疏模式在最近的 NVIDIA GPU 上提供了显著的速度提升（Mishra 等，[2021](#bib.bib33)；Kurtic 等，[2023](#bib.bib25)）。规定每组
    M 个连续权重必须包含 N 个零。SparseGPT (Frantar & Alistarh，[2023](#bib.bib14)) 并未明确设计用于输出平衡的剪枝粒度。在转换为
    N:M 稀疏模式时，SparseGPT 强制每行中的每 M 个连续权重中必须有恰好 N 个零。在这种情况下，SparseGPT (Frantar & Alistarh，[2023](#bib.bib14))
    和 Wanda (Sun 等，[2023](#bib.bib40)) 一致地为每个输出移除相同数量的权重。这些方法通常强调权重矩阵中单个组件的重要性，而忽视了与网络中其他层的相互依赖性。对于
    MLP 输入投影剪枝，移除与每个中间神经元对应的相同数量的权重。然而，对于基于 SparseGPT 和 Wanda 剪枝度量的输出投影，连接到具有较大激活的中间神经元的权重更可能被保留，从而导致结构不匹配。
- en: 3 Dependency-aware Semi-structured Sparsity
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 依赖感知半结构化稀疏性
- en: In this section, we introduce Dependency-aware Semi-structured Sparsity (DaSS)
    for pruning MLP, which incorporates structural dependency into weight magnitude-based
    unstructured pruning method. An overview of DaSS and its comparison with the existing
    pruning method is shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Dependency-Aware
    Semi-Structured Sparsity of GLU Variants in Large Language Models").
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了针对 MLP 的依赖感知半结构化稀疏性（DaSS），该方法将结构依赖性纳入到基于权重幅度的非结构化剪枝方法中。DaSS 的概述及其与现有剪枝方法的比较见图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Dependency-Aware Semi-Structured Sparsity
    of GLU Variants in Large Language Models")。
- en: Here, we denote the transposed weight matrices $\mathbf{W_{1}},\mathbf{W_{2}},\mathbf{W_{3}}$.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们表示转置的权重矩阵 $\mathbf{W_{1}},\mathbf{W_{2}},\mathbf{W_{3}}$。
- en: Structure Dependency in MLP.
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MLP 中的结构依赖性。
- en: In dependency-based structured pruning (Ma et al., [2023](#bib.bib29); Fang
    et al., [2023](#bib.bib13)), the initial step is dedicated to recognizing groups
    of interconnected structures within the model. In terms of SwiGLU-based MLP module
    pruning, there are three projection matrices, all weights connecting to an identical
    intermediate neuron are collectively classified into the same dependency group.
    When pruning weights in $\mathbf{W^{(1)}_{i,:}}$ should also be emphasized similar
    importance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于依赖性的结构剪枝中（Ma 等，[2023](#bib.bib29)；Fang 等，[2023](#bib.bib13)），初始步骤专注于识别模型中的互连结构组。在基于
    SwiGLU 的 MLP 模块剪枝中，有三个投影矩阵，所有连接到相同中间神经元的权重被集合到相同的依赖组中。当剪枝 $\mathbf{W^{(1)}_{i,:}}$
    中的权重时，也应强调类似的重要性。
- en: Incorporating Dependency into Weight Importance.
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将依赖性纳入权重重要性。
- en: 'In dependency-based pruning, we assess the importance of each weight and then
    aggregate the importance scores within the same group as the group importance
    score. Consequently, each weight within the same dependency group shares a consistent
    importance score, ensuring their simultaneous retention or elimination. The importance
    score of each weight is equal to the group importance score. In DaSS pruning,
    we take both group importance and weight magnitude into consideration to evaluate
    the importance of each weight. LLM-pruner (Ma et al., [2023](#bib.bib29)) evaluates
    the group importance using gradient-based methods. However, computing gradient
    for LLMs will introduce a significant amount of memory cost and it is less practical
    for larger models. The existing unstructured pruning methods (Sun et al., [2023](#bib.bib40);
    Frantar & Alistarh, [2023](#bib.bib14)) are more efficient than gradient-based
    methods. By prioritizing the weights linked to outliers in intermediate activations,
    Wanda outperforms magnitude pruning in Down-Proj pruning. To minimize the impact
    on these critical outliers, it would be beneficial to also give greater significance
    to the weights that lead to outlier generation in both Gate-Proj and Up-Proj projections.
    Thus, we use the norm of intermediate activations $\|\mathbf{y}\|_{2}$ is determined
    as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于依赖的剪枝中，我们评估每个权重的重要性，然后将同一组内的重要性分数汇总为组的重要性分数。因此，同一依赖组中的每个权重共享一致的重要性分数，确保它们的同时保留或淘汰。每个权重的重要性分数等于组的重要性分数。在
    DaSS 剪枝中，我们综合考虑组的重要性和权重幅度来评估每个权重的重要性。LLM-pruner (Ma et al., [2023](#bib.bib29))
    使用基于梯度的方法评估组的重要性。然而，为 LLM 计算梯度会引入大量内存开销，对于更大的模型来说不太实用。现有的无结构剪枝方法 (Sun et al.,
    [2023](#bib.bib40); Frantar & Alistarh, [2023](#bib.bib14)) 比基于梯度的方法更高效。通过优先考虑与中间激活异常值相关的权重，Wanda
    在 Down-Proj 剪枝中优于幅度剪枝。为了最小化对这些关键异常值的影响，给与导致异常值生成的权重更大的重要性也是有益的，包括 Gate-Proj 和
    Up-Proj 投影中的权重。因此，我们使用中间激活值的范数 $\|\mathbf{y}\|_{2}$，其计算方式如下：
- en: '|  | $\mathbf{I}_{i,j}^{k}=\left&#124;\mathbf{W}_{i,j}^{k}\right&#124;\cdot\&#124;\mathbf{y}_{i}\&#124;_{2}^{\alpha}$
    |  | (4) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{I}_{i,j}^{k}=\left\|\mathbf{W}_{i,j}^{k}\right\|\cdot\|\mathbf{y}_{i}\|_{2}^{\alpha}$
    |  | (4) |'
- en: 'Where $k=1,2$ is determined as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $k=1,2$ 的计算方式如下：
- en: '|  | $\mathbf{I}_{i,j}^{3}=\left&#124;\mathbf{W}_{i,j}^{3}\right&#124;\cdot\&#124;\mathbf{y}_{j}\&#124;_{2}$
    |  | (5) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{I}_{i,j}^{3}=\left\|\mathbf{W}_{i,j}^{3}\right\|\cdot\|\mathbf{y}_{j}\|_{2}$
    |  | (5) |'
- en: In Down-Proj pruning, the pruning metric is the same as done with Wanda (Sun
    et al., [2023](#bib.bib40)). By augmenting intermediate activations into all three
    weight importance matrices, DaSS inherently assigns greater emphasis to weights
    corresponding to intermediate activation outliers, thereby facilitating more nuanced
    structural coordination among the entire MLP module.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Down-Proj 剪枝中，剪枝度量与 Wanda (Sun et al., [2023](#bib.bib40)) 中采用的相同。通过将中间激活值增加到所有三个权重重要性矩阵中，DaSS
    自然地对与中间激活异常值相关的权重赋予更大的重视，从而促进整个 MLP 模块的更细致的结构协调。
- en: Pruning Granularity.
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 剪枝粒度。
- en: Pruning LLaMA models in finer granularity can improve the performance (Sun et al.,
    [2023](#bib.bib40)). To incorporate intermediate activations into SwiGLU-based
    MLP pruning, each weight in the same comparison group should correspond to different
    intermediate activations. In Section 2.3, we show that input-balanced magnitude
    pruning can still achieve competitive results for Gate-Proj and Up-Proj pruning.
    In such pruning granularity, we can augment intermediate activations into Gate-Proj
    and Up-Proj pruning metric. In input-balanced pruning, we remove $s\%$ based on
    weight importance scores. DaSS uses output-balanced sparsity for Down-Proj pruning,
    which is the same as Wanda.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对 LLaMA 模型进行更细粒度的剪枝可以提高性能 (Sun et al., [2023](#bib.bib40))。为了将中间激活值纳入基于 SwiGLU
    的 MLP 剪枝中，每个相同比较组中的权重应对应不同的中间激活值。在第 2.3 节中，我们展示了输入平衡的幅度剪枝仍然可以为 Gate-Proj 和 Up-Proj
    剪枝实现具有竞争力的结果。在这样的剪枝粒度中，我们可以将中间激活值增加到 Gate-Proj 和 Up-Proj 剪枝度量中。在输入平衡剪枝中，我们根据权重重要性分数移除
    $s\%$ 的权重。DaSS 使用输出平衡稀疏性进行 Down-Proj 剪枝，这与 Wanda 相同。
- en: Extension to N:M Sparsity.
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩展到 N:M 稀疏性。
- en: 'The DaSS pruning design allows for easy adaptation to the N:M sparsity pattern.
    For Gate-Proj and Up-Proj the N: M sparsity pattern is formed on an input-balanced
    basis. This means that for weights connecting to each input neuron, out of every
    group of M consecutive weights, there are exactly N zeros included. For Down-Proj
    the N: M sparsity pattern is formed on an output-balanced basis.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: DaSS 修剪设计允许轻松适应 N:M 稀疏模式。对于 Gate-Proj 和 Up-Proj，N:M 稀疏模式是基于输入平衡的。这意味着对于每个输入神经元的连接权重，每
    M 个连续权重组中，恰好有 N 个零。对于 Down-Proj，N:M 稀疏模式是基于输出平衡的。
- en: Discussion.
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 讨论。
- en: 'In summary, our DaSS method offers multiple appealing aspects for pruning LLMs:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的 DaSS 方法在修剪 LLM 时提供了多个令人吸引的方面：
- en: '1.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: It retains the fundamental simplicity inherent in Wanda pruning method. Without
    weight updating, it still matches the performance of SparseGPT even at high sparsity
    as demonstrated in Section 4.4\. This demonstrates the consistently effective
    and efficient capability of the DaSS method in identifying sparse neural networks.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它保留了 Wanda 修剪方法固有的基本简洁性。即使在高稀疏情况下，没有权重更新，它仍能匹配 SparseGPT 的性能，如第 4.4 节所示。这证明了
    DaSS 方法在识别稀疏神经网络方面的一贯有效性和高效性。
- en: '2.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Unlike SparseGPT and Wanda that use input + intermediate activations for MLP
    pruning, DaSS only uses intermediate activations. By using intermediate activations
    as group importance indicator, DaSS prunes MLP module in a more comprehensive
    view that captures the collective importance of all the weights connecting to
    each intermediate neuron.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与使用输入+中间激活的 SparseGPT 和 Wanda 不同，DaSS 仅使用中间激活。通过使用中间激活作为组重要性指示器，DaSS 从更全面的角度修剪
    MLP 模块，捕捉所有连接到每个中间神经元的权重的集体重要性。
- en: '3.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: DaSS effectively explores the balance between unstructured pruning’s flexibility
    and the structural coherence in dependency-based structured pruning.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DaSS 有效地探索了无结构修剪的灵活性与基于依赖的结构化修剪中的结构一致性之间的平衡。
- en: 4 Experiments
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 个实验
- en: 4.1 Settings
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: Models.
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型。
- en: DaSS’s performance is evaluated over popular LLMs using SwiGLU, including the
    LLaMA2 model family (Touvron et al., [2023b](#bib.bib43)), which has models with
    parameters ranging between 7 billion and 70 billion, and also the Mistral-7B model
    (Jiang et al., [2023](#bib.bib24)). Among them, LLaMA2-70B and Mistral-7B use
    grouped-query attention (Ainslie et al., [2023](#bib.bib1)), the MLP module parameter
    accounts for around 80% of the total model parameters. To test the generalization
    ability to ReGLU models, we use ReluLLaMA (Team, [2023](#bib.bib41)), which is
    fine-tuned using ReGLU variant (Shazeer, [2020](#bib.bib39); Mirzadeh et al.,
    [2023](#bib.bib32)) based on LLaMA2 with small accuracy loss. The model configuration
    details are in Appendix [7](#A1.T7 "Table 7 ‣ Appendix A Model Configurations
    ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language
    Models"). We access the public checkpoints of the involved models provided by
    the HuggingFace Transformers library (Wolf et al., [2019](#bib.bib45)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: DaSS 的性能在使用 SwiGLU 的流行 LLM 上进行了评估，包括 LLaMA2 模型家族（Touvron 等，[2023b](#bib.bib43)），这些模型的参数范围在
    70 亿到 700 亿之间，还有 Mistral-7B 模型（Jiang 等，[2023](#bib.bib24)）。其中，LLaMA2-70B 和 Mistral-7B
    使用了分组查询注意力（Ainslie 等，[2023](#bib.bib1)），MLP 模块参数占总模型参数的约 80%。为了测试对 ReGLU 模型的泛化能力，我们使用了
    ReluLLaMA（Team，[2023](#bib.bib41)），该模型基于 LLaMA2 使用 ReGLU 变体（Shazeer，[2020](#bib.bib39)；Mirzadeh
    等，[2023](#bib.bib32)）进行微调，精度损失较小。模型配置详情见附录 [7](#A1.T7 "Table 7 ‣ Appendix A Model
    Configurations ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants in
    Large Language Models")。我们访问了 HuggingFace Transformers 库（Wolf 等，[2019](#bib.bib45)）提供的相关模型的公开检查点。
- en: Baseline Approaches.
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线方法。
- en: As shown in Sun et al. ([2023](#bib.bib40)), the pre-LLM methods become less
    effective for LLMs. We compare the performance with two LLM-specific one-shot
    pruning approaches, SparseGPT (Frantar & Alistarh, [2023](#bib.bib14)) and Wanda
    (Sun et al., [2023](#bib.bib40)). Those baseline methods utilize uniform layerwise
    sparsity that can be easily converted into hardware-friendly N:M sparsity pattern.
    We used the same calibration data set as SparseGPT and Wanda in their model pruning
    processes, consisting of 128 sequences of 2048 tokens each, randomly selected
    from the first shard of the C4 dataset (Raffel et al., [2020](#bib.bib35)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如Sun等人所示（[2023](#bib.bib40)），预先的LLM方法对于LLM的效果逐渐降低。我们与两种LLM特定的单次剪枝方法进行了性能比较：SparseGPT（Frantar
    & Alistarh，[2023](#bib.bib14)）和Wanda（Sun等人，[2023](#bib.bib40)）。这些基准方法利用了均匀的层级稀疏性，这可以轻松转化为硬件友好的N:M稀疏模式。我们使用了与SparseGPT和Wanda相同的校准数据集，用于他们的模型剪枝过程，该数据集包括128个序列，每个序列2048个令牌，随机从C4数据集的第一片段中选择（Raffel等人，[2020](#bib.bib35)）。
- en: Evaluation.
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估。
- en: 'To comprehensively evaluate the efficacy of our proposed method, three unique
    metrics are utilized to evaluate the performance of the pruned models: (1) perplexity
    (PPL) of language modeling (2) zero-shot accuracy on 5 commonsense reasoning tasks
    (3) 5-shot accuracy on Massive Multitask Language Understanding (MMLU) (Hendrycks
    et al., [2021](#bib.bib22)). Although perplexity has been regarded as a consistent
    and reliable metric for measuring compressed models in previous works (Dettmers
    & Zettlemoyer, [2023](#bib.bib10); Frantar & Alistarh, [2023](#bib.bib14)), a
    recent study indicates that perplexity fails to capture the change in capabilities
    of compressed LLMs on performing knowledge-intensive MMLU tasks (Jaiswal et al.,
    [2023](#bib.bib23)). For perplexity evaluation, we use the validation dataset
    of WikiText2 (Merity et al., [2017](#bib.bib31)). For zero-shot commonsense reasoning
    tasks, we choose five widely used tasks for accuracy evaluation: ARC (Easy and
    Challenge) (Clark et al., [2018](#bib.bib7)), HellaSwag (Zellers et al., [2019](#bib.bib48)),
    PiQA (Bisk et al., [2020](#bib.bib3)), and WinoGrande (Sakaguchi et al., [2021](#bib.bib37)),
    implemented in the Lm-Evaluation-Harness (Gao et al., [2021](#bib.bib17)). For
    5-shot MMLU evaluation, we use Chain-of-Thought Hub (Fu et al., [2023](#bib.bib16))
    which is based on the official implementation of MMLU (Hendrycks et al., [2021](#bib.bib22)).
    The MMLU encompasses 57 tasks, spanning from STEM, Humanities, Social Sciences,
    among others and we report the mean accuracy of 57 tasks. We evaluate the (1)
    perplexity of all the aforementioned models. To fully demonstrate the task-wise
    performance in different sparsity patterns, we report the (2) and (3) task performance
    of the largest LLaMA2-70B model.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面评估我们提出的方法的有效性，利用了三种独特的指标来评估剪枝模型的性能：（1）语言建模的困惑度（PPL）（2）5个常识推理任务上的零样本准确率（3）Massive
    Multitask Language Understanding (MMLU)（Hendrycks等人，[2021](#bib.bib22)）上的5样本准确率。虽然困惑度在以往的研究中被认为是测量压缩模型的一个一致且可靠的指标（Dettmers
    & Zettlemoyer，[2023](#bib.bib10)；Frantar & Alistarh，[2023](#bib.bib14)），但最近的一项研究表明困惑度未能捕捉压缩LLM在执行知识密集型MMLU任务时能力的变化（Jaiswal等人，[2023](#bib.bib23)）。对于困惑度评估，我们使用了WikiText2的验证数据集（Merity等人，[2017](#bib.bib31)）。对于零样本常识推理任务，我们选择了五个广泛使用的任务进行准确率评估：ARC（Easy
    和 Challenge）（Clark等人，[2018](#bib.bib7)），HellaSwag（Zellers等人，[2019](#bib.bib48)），PiQA（Bisk等人，[2020](#bib.bib3)），以及WinoGrande（Sakaguchi等人，[2021](#bib.bib37)），在Lm-Evaluation-Harness中实现（Gao等人，[2021](#bib.bib17)）。对于5样本MMLU评估，我们使用Chain-of-Thought
    Hub（Fu等人，[2023](#bib.bib16)），它基于MMLU的官方实现（Hendrycks等人，[2021](#bib.bib22)）。MMLU涵盖57个任务，包括STEM、人文学科、社会科学等，我们报告了57个任务的平均准确率。我们评估了（1）所有上述模型的困惑度。为了充分展示不同稀疏模式下的任务表现，我们报告了最大LLaMA2-70B模型的（2）和（3）任务表现。
- en: 'Table 2: WikiText perplexity of pruned LLaMA-2 and Mistral models'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：剪枝LLaMA-2和Mistral模型的WikiText困惑度
- en: '| Model Family | Mistral | Llama2 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 模型系列 | Mistral | Llama2 |'
- en: '| Size | 7B | 7B | 13B | 70B |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 7B | 7B | 13B | 70B |'
- en: '| Method | MLP Sparsity | PPL ($\downarrow$) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | MLP 稀疏性 | PPL ($\downarrow$) |'
- en: '| Dense | - | 5.25 | 5.47 | 4.88 | 3.32 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 稠密 | - | 5.25 | 5.47 | 4.88 | 3.32 |'
- en: '| SparseGPT | 4:8 | 7.36 | 7.33 | 6.29 | 4.66 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 4:8 | 7.36 | 7.33 | 6.29 | 4.66 |'
- en: '| Wanda | 4:8 | 7.38 | 7.63 | 6.42 | 4.59 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 4:8 | 7.38 | 7.63 | 6.42 | 4.59 |'
- en: '| DaSS | 4:8 | 7.06 | 7.26 | 6.16 | 4.41 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| DaSS | 4:8 | 7.06 | 7.26 | 6.16 | 4.41 |'
- en: '| SparseGPT | 2:4 | 8.86 | 8.72 | 7.30 | 5.32 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 2:4 | 8.86 | 8.72 | 7.30 | 5.32 |'
- en: '| Wanda | 2:4 | 9.24 | 9.55 | 7.68 | 5.35 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 2:4 | 9.24 | 9.55 | 7.68 | 5.35 |'
- en: '| DaSS | 2:4 | 8.39 | 8.48 | 6.90 | 4.91 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| DaSS | 2:4 | 8.39 | 8.48 | 6.90 | 4.91 |'
- en: '| SparseGPT | 50% | 6.20 | 6.38 | 5.60 | 4.06 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 50% | 6.20 | 6.38 | 5.60 | 4.06 |'
- en: '| Wanda | 50% | 6.25 | 6.50 | 5.67 | 4.07 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 50% | 6.25 | 6.50 | 5.67 | 4.07 |'
- en: '| DaSS | 50% | 6.15 | 6.44 | 5.59 | 4.00 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| DaSS | 50% | 6.15 | 6.44 | 5.59 | 4.00 |'
- en: 'Table 3: Downstream tasks performance of LLaMA2-70B model in different sparsity
    pattern. The MMLU scores of dense LLaMA2-70B and LLaMA2-34B reported in Touvron
    et al. ([2023b](#bib.bib43)) are 68.9 and 62.6 respectively.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：不同稀疏模式下LLaMA2-70B模型的下游任务性能。Touvron等人（[2023b](#bib.bib43)）报告的稠密LLaMA2-70B和LLaMA2-34B的MMLU分数分别为68.9和62.6。
- en: '| Sparsity | Methods | Commonsense Reasoning (0 shot) ($\uparrow$) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏性 | 方法 | 常识推理（0 shot）（$\uparrow$） |'
- en: '| PIQA | HellaSwag | Winogrande | ARC-e | ARC-c | Average | MMLU (5 shot) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| PIQA | HellaSwag | Winogrande | ARC-e | ARC-c | 平均值 | MMLU (5 shot) |'
- en: '| Dense | - | 82.15 | 66.05 | 77.98 | 82.55 | 54.35 | 72.62 | 69.10 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 稠密 | - | 82.15 | 66.05 | 77.98 | 82.55 | 54.35 | 72.62 | 69.10 |'
- en: '| 4:8 | SparseGPT | 80.52 | 61.00 | 77.03 | 79.85 | 50.60 | 69.80 | 60.24 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 4:8 | SparseGPT | 80.52 | 61.00 | 77.03 | 79.85 | 50.60 | 69.80 | 60.24 |'
- en: '| Wanda | 80.47 | 61.85 | 75.45 | 80.10 | 50.00 | 69.57 | 59.69 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 80.47 | 61.85 | 75.45 | 80.10 | 50.00 | 69.57 | 59.69 |'
- en: '| DaSS | 80.79 | 62.70 | 76.09 | 81.20 | 51.19 | 70.39 | 60.86 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| DaSS | 80.79 | 62.70 | 76.09 | 81.20 | 51.19 | 70.39 | 60.86 |'
- en: '| DaSS+skip 1/4 | 81.34 | 63.10 | 76.09 | 80.05 | 50.68 | 70.25 | 65.82 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| DaSS+skip 1/4 | 81.34 | 63.10 | 76.09 | 80.05 | 50.68 | 70.25 | 65.82 |'
- en: '| 2:4 | SparseGPT | 79.00 | 59.00 | 76.64 | 78.50 | 47.87 | 68.20 | 56.99 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 2:4 | SparseGPT | 79.00 | 59.00 | 76.64 | 78.50 | 47.87 | 68.20 | 56.99 |'
- en: '| Wanda | 79.22 | 59.25 | 74.66 | 78.90 | 47.01 | 67.81 | 56.41 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 79.22 | 59.25 | 74.66 | 78.90 | 47.01 | 67.81 | 56.41 |'
- en: '| DaSS | 79.70 | 60.00 | 74.82 | 79.65 | 49.15 | 68.66 | 57.53 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| DaSS | 79.70 | 60.00 | 74.82 | 79.65 | 49.15 | 68.66 | 57.53 |'
- en: '| DaSS+skip 1/4 | 80.63 | 62.05 | 74.51 | 80.00 | 49.40 | 69.32 | 64.36 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| DaSS+skip 1/4 | 80.63 | 62.05 | 74.51 | 80.00 | 49.40 | 69.32 | 64.36 |'
- en: '| 50% | SparseGPT | 81.50 | 64.20 | 78.45 | 81.90 | 52.73 | 71.76 | 64.52 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 50% | SparseGPT | 81.50 | 64.20 | 78.45 | 81.90 | 52.73 | 71.76 | 64.52 |'
- en: '| Wanda | 81.01 | 64.30 | 77.35 | 80.95 | 52.05 | 71.13 | 62.72 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 81.01 | 64.30 | 77.35 | 80.95 | 52.05 | 71.13 | 62.72 |'
- en: '| DaSS | 81.18 | 64.60 | 77.90 | 81.35 | 51.71 | 71.35 | 63.27 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| DaSS | 81.18 | 64.60 | 77.90 | 81.35 | 51.71 | 71.35 | 63.27 |'
- en: '| DaSS+skip 1/4 | 81.23 | 64.45 | 77.27 | 81.35 | 52.81 | 71.42 | 66.81 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| DaSS+skip 1/4 | 81.23 | 64.45 | 77.27 | 81.35 | 52.81 | 71.42 | 66.81 |'
- en: Sparsity.
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稀疏性。
- en: 'In the less interpretable perplexity evaluation, we only prune the MLP layers.
    In the task-wise evaluation of LLaMA2-70B model, we prune both attention and MLP
    modules in accordance with previous works to better understand the performance
    gap between the pruned and original models. For attention module pruning, we use
    Wanda method, which is more efficient than SaprseGPT. In LlaMA2-70B, as the MLP
    module accounts for more than 80% total parameters, we observe the choice of attention
    pruning has almost no impact on the final performance. We apply a uniform sparsity
    ratio across all the pruned layers and conduct evaluations using three sparsity
    types: unstructured sparsity, and semi-structured sparsities of 4:8 and 2:4.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在不太易解释的困惑度评估中，我们只修剪了MLP层。在LLaMA2-70B模型的任务评估中，我们根据之前的工作修剪了注意力和MLP模块，以更好地理解修剪模型与原始模型之间的性能差距。对于注意力模块修剪，我们使用Wanda方法，其效率比SaprseGPT更高。在LlaMA2-70B中，由于MLP模块占总参数的80%以上，我们观察到注意力修剪的选择对最终性能几乎没有影响。我们在所有修剪层中应用均匀的稀疏比，并使用三种稀疏类型进行评估：非结构化稀疏性和4:8及2:4的半结构化稀疏性。
- en: 4.2 Language Modeling
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 语言建模
- en: This evaluation involved a detailed examination of all the opened LLaMA2 models
    and Mistral 7B model. In perplexity evaluation, as shown in Table [2](#S4.T2 "Table
    2 ‣ Evaluation. ‣ 4.1 Settings ‣ 4 Experiments ‣ Dependency-Aware Semi-Structured
    Sparsity of GLU Variants in Large Language Models"), our method consistently achieves
    better performance than SparseGPT and Wanda in more constrained and practical
    N:M sparsity pattern. As indicated by Sun et al. ([2023](#bib.bib40)), where weight
    updates can improve the performance in N:M sparsity pattern, our method shows
    superior performance even without computationally expensive weight updates. For
    Mistral 7B and LLaMA2 70B models with larger MLP layers, our method also outperforms
    SparseGPT in unstructured sparsity.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这项评估对所有打开的LLaMA2模型和Mistral 7B模型进行了详细检查。在困惑度评估中，如表[2](#S4.T2 "Table 2 ‣ Evaluation.
    ‣ 4.1 Settings ‣ 4 Experiments ‣ Dependency-Aware Semi-Structured Sparsity of
    GLU Variants in Large Language Models")所示，我们的方法在更受限且实际的N:M稀疏模式下，始终表现出比SparseGPT和Wanda更好的性能。如Sun等人（[2023](#bib.bib40)）所示，尽管权重更新可以改善N:M稀疏模式下的性能，我们的方法即使在没有计算开销大的权重更新的情况下也显示出优越的表现。对于具有更大MLP层的Mistral
    7B和LLaMA2 70B模型，我们的方法在非结构化稀疏性方面也优于SparseGPT。
- en: 4.3 Downstream Tasks
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 下游任务
- en: While perplexity has been a popular metric in earlier LLM compression works
    (Frantar & Alistarh, [2023](#bib.bib14); Dettmers & Zettlemoyer, [2023](#bib.bib10)),
    it essentially measures the confidence of a language model in text prediction
    and doesn’t always reflect its proficiency in performing downstream tasks (Jaiswal
    et al., [2023](#bib.bib23)). Apart from assessing perplexity, we comprehensively
    evaluate the performance of pruned LLaMA2-70B models in downstream tasks.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然困惑度在早期 LLM 压缩工作中一直是一个流行的指标（Frantar & Alistarh，[2023](#bib.bib14)；Dettmers
    & Zettlemoyer，[2023](#bib.bib10)），但它本质上衡量的是语言模型在文本预测中的信心，并不总是反映其在下游任务中的能力（Jaiswal
    等人，[2023](#bib.bib23)）。除了评估困惑度外，我们还全面评估了剪枝后的 LLaMA2-70B 模型在下游任务中的表现。
- en: Comparison with Baselines. In Table [3](#S4.T3 "Table 3 ‣ Evaluation. ‣ 4.1
    Settings ‣ 4 Experiments ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants
    in Large Language Models"), we present the performance of different sparse LLaMA2-70B
    models on downstream tasks with prompting. The results show that our method outperforms
    SparseGPT and Wanda in most tasks at semi-structured N:M sparsity pattern including
    the aggregated task MMLU. The only exception is that SparseGPT outperforms both
    Wanda and DaSS in Winogrande task. For unstructured sparsity, our method outperforms
    Wanda sharing the same complexity level. It is noteworthy that the improvement
    of DaSS over Wanda becomes more pronounced in the challenging MMLU task, where
    it achieves an increase in accuracy of 1.17 and 1.12 for the 4:8 and 2:4 sparsity
    patterns, respectively.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与基线的比较。在表 [3](#S4.T3 "Table 3 ‣ Evaluation. ‣ 4.1 Settings ‣ 4 Experiments ‣
    Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models")
    中，我们展示了不同稀疏 LLaMA2-70B 模型在下游任务中的表现。结果表明，我们的方法在大多数任务中优于 SparseGPT 和 Wanda，包括汇总任务
    MMLU，使用半结构化的 N:M 稀疏模式。唯一的例外是 SparseGPT 在 Winogrande 任务中优于 Wanda 和 DaSS。对于未结构化的稀疏性，我们的方法在复杂度相同的情况下优于
    Wanda。值得注意的是，DaSS 相对于 Wanda 的改进在具有挑战性的 MMLU 任务中变得更加显著，其中在 4:8 和 2:4 稀疏模式下分别提高了
    1.17 和 1.12 的准确度。
- en: Partial N:M Sparsity & Layer Sensitivity. In accordance with the observations
    with Jaiswal et al. ([2023](#bib.bib23)), we see a considerable performance degradation
    in knowledge-intensive MMLU task, in which only unstructured 50% sparsity models
    outperform the dense LLaMA2-34B model. As GPU plays a more important role in larger
    model inference, it is essential to improve the performance of pruned models in
    hardware-friendly N:M sparsity. Uniform reduction of the overall sparsity level
    is not feasible for N:M sparsity, Frantar & Alistarh ([2023](#bib.bib14)) suggests
    a specific subset of layers can be chosen for full N:M sparsification. Here we
    skip pruning 1/4 consecutive layers (20 layers) and result in a final 37.5% sparsity
    ratio. We use the first 10 tasks of MMLU to study pruning sensitivity. We divide
    the model into 4 consecutive parts and study skipping pruning each part. As shown
    in Table [4](#S4.T4 "Table 4 ‣ 4.3 Downstream Tasks ‣ 4 Experiments ‣ Dependency-Aware
    Semi-Structured Sparsity of GLU Variants in Large Language Models"), the earlier
    layers are more sensitive than the later ones in knowledge-intensitive tasks,
    which is contradictory to the findings in Frantar & Alistarh ([2023](#bib.bib14))
    using perplexity.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 部分 N:M 稀疏性与层敏感性。根据 Jaiswal 等人的观察（[2023](#bib.bib23)），我们在知识密集型 MMLU 任务中看到性能显著下降，其中仅有未结构化的
    50% 稀疏模型优于稠密的 LLaMA2-34B 模型。由于 GPU 在更大模型推理中的作用越来越重要，改善硬件友好的 N:M 稀疏模型的性能是至关重要的。整体稀疏水平的均匀减少对于
    N:M 稀疏性是不切实际的，Frantar 和 Alistarh（[2023](#bib.bib14)）建议选择特定的层子集进行完全的 N:M 稀疏化。我们跳过了
    1/4 连续层（20 层）的剪枝，最终得到了 37.5% 的稀疏比。我们使用 MMLU 的前 10 个任务来研究剪枝敏感性。我们将模型分为 4 个连续部分，并研究跳过剪枝每一部分。如表
    [4](#S4.T4 "Table 4 ‣ 4.3 Downstream Tasks ‣ 4 Experiments ‣ Dependency-Aware
    Semi-Structured Sparsity of GLU Variants in Large Language Models") 所示，早期层在知识密集型任务中比后期层更敏感，这与
    Frantar 和 Alistarh（[2023](#bib.bib14)）使用困惑度的发现相矛盾。
- en: 'Table 4: MMLU subset accuracy after skipping pruning 20 layers at various start
    indices in 4:8 sparsity.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在 4:8 稀疏性中跳过剪枝 20 层后的 MMLU 子集准确度，起始索引不同。
- en: '| Start Index | 0 | 10 | 20 | 40 | 60 | Dense |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 起始索引 | 0 | 10 | 20 | 40 | 60 | 稠密 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Acc (%) | 60.12 | 61.72 | 59.75 | 56.54 | 56.83 | 64.86 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 (%) | 60.12 | 61.72 | 59.75 | 56.54 | 56.83 | 64.86 |'
- en: We continue to search for the better skipping layers with the start layer index
    range in [0,20]. We found that starting skipping from layer 10 can achieve the
    best performance in the subset. Then we test its results in the full MMLU tasks.
    As shown in Table [3](#S4.T3 "Table 3 ‣ Evaluation. ‣ 4.1 Settings ‣ 4 Experiments
    ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language
    Models"), skipping sensitive 1/4 layers can significantly improve the performance
    of pruned models, especially for N:M sparsity. We can achieve sparse models that
    perform better than LLaMA2-34B. Although partial N:M sparsity models have more
    parameters than smaller dense models, training many smaller dense models like
    LLaMA2-34B is still computationally expensive. Efficient post-training pruning
    enables us to easily adjust the accuracy-efficiency trade-off in real-world applications.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续在起始层索引范围 [0,20] 中寻找更好的跳层。我们发现，从第 10 层开始跳层可以在子集中实现最佳性能。然后我们在完整的 MMLU 任务中测试其结果。如表
    [3](#S4.T3 "表 3 ‣ 评估 ‣ 4.1 设置 ‣ 4 实验 ‣ GLU 变体的大型语言模型中的依赖感知半结构稀疏性") 所示，跳过敏感的 1/4
    层可以显著提高剪枝模型的性能，尤其是对于 N:M 稀疏性。我们可以获得比 LLaMA2-34B 更好的稀疏模型。尽管部分 N:M 稀疏模型比较小的稠密模型有更多的参数，但训练许多像
    LLaMA2-34B 这样的较小稠密模型仍然计算开销很大。高效的后训练剪枝使我们能够轻松调整实际应用中的准确性和效率的权衡。
- en: An extensive line of work (Haviv et al., [2023](#bib.bib21); Meng et al., [2022](#bib.bib30);
    Geva et al., [2023](#bib.bib18)) discussed how factual knowledge is stored in
    Transformer parameters, and early to middle MLP sublayers are demonstrated to
    be crucial for memorized predictions. Interestingly, the layer sensitivity results
    have a strong correlation with the findings in those works. We also apply the
    searched layer index in MMLU tasks to commonsense reasoning tasks, and we see
    fewer performance gains, which highlights the importance of task-specific pruning.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 大量研究（Haviv et al., [2023](#bib.bib21); Meng et al., [2022](#bib.bib30); Geva
    et al., [2023](#bib.bib18)）讨论了事实知识如何存储在 Transformer 参数中，并且早期到中期的 MLP 子层被证明对记忆预测至关重要。有趣的是，层敏感性结果与这些研究中的发现有很强的相关性。我们还将搜索到的层索引应用于
    MMLU 任务与常识推理任务，发现性能提升较少，这突显了任务特定剪枝的重要性。
- en: 4.4 Generalization to ReGLU
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 泛化到 ReGLU
- en: We test the generalization ability to another GLU variant, ReGLU. In Table [5](#S4.T5
    "Table 5 ‣ 4.4 Generalization to ReGLU ‣ 4 Experiments ‣ Dependency-Aware Semi-Structured
    Sparsity of GLU Variants in Large Language Models"), we also observed DaSS outperform
    SparseGPT and Wanda in N:M sparsity pattern. This demonstrates the generalization
    ability of our method to other GLU variants. Since augmenting input activations
    is important for the expanding projection of non-GLU MLP modules, our methods
    cannot achieve better performance than Wanda in older LLMs that do not use GLU
    variants.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试了对另一个 GLU 变体 ReGLU 的泛化能力。在表 [5](#S4.T5 "表 5 ‣ 4.4 泛化到 ReGLU ‣ 4 实验 ‣ GLU
    变体的大型语言模型中的依赖感知半结构稀疏性") 中，我们还观察到 DaSS 在 N:M 稀疏模式中优于 SparseGPT 和 Wanda。这证明了我们的方法对其他
    GLU 变体的泛化能力。由于增强输入激活对扩展非 GLU MLP 模块的投影很重要，我们的方法在不使用 GLU 变体的较旧 LLM 中无法比 Wanda 获得更好的性能。
- en: 'Table 5: WikiText perplexity of ReluLLaMA models'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: ReluLLaMA 模型的 WikiText 困惑度'
- en: '| Size | 7B | 13B |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Size | 7B | 13B |'
- en: '| Method | MLP Sparsity | PPL ($\downarrow$) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Method | MLP 稀疏性 | PPL ($\downarrow$) |'
- en: '| Dense | - | 6.15 | 5.50 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Dense | - | 6.15 | 5.50 |'
- en: '| SparseGPT | 4:8 | 8.44 | 7.45 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 4:8 | 8.44 | 7.45 |'
- en: '| Wanda | 4:8 | 9.21 | 7.74 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 4:8 | 9.21 | 7.74 |'
- en: '| DaSS | 4:8 | 8.19 | 7.16 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| DaSS | 4:8 | 8.19 | 7.16 |'
- en: '| SparseGPT | 2:4 | 10.26 | 8.98 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 2:4 | 10.26 | 8.98 |'
- en: '| Wanda | 2:4 | 12.68 | 9.87 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 2:4 | 12.68 | 9.87 |'
- en: '| DaSS | 2:4 | 9.61 | 8.18 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| DaSS | 2:4 | 9.61 | 8.18 |'
- en: '| SparseGPT | 50% | 7.22 | 6.39 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 50% | 7.22 | 6.39 |'
- en: '| Wanda | 50% | 7.52 | 6.53 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 50% | 7.52 | 6.53 |'
- en: '| DaSS | 50% | 7.24 | 6.40 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| DaSS | 50% | 7.24 | 6.40 |'
- en: 4.5 Performance Analysis
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 性能分析
- en: Sparsity Variation.
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稀疏性变异。
- en: Figure [2](#S4.F2 "Figure 2 ‣ Sparsity Variation. ‣ 4.5 Performance Analysis
    ‣ 4 Experiments ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants in
    Large Language Models") illustrates a comparison of the mean zero-shot task accuracy
    at varying levels of sparsity for the MLP component of the LLaMA2-70B model. It’s
    evident that DaSS pruning maintains competitive performance which closely matches
    that of SparseGPT across the entire range of sparsity ratios tested. Notably,
    DaSS achieves such performance without the need for weight updates, suggesting
    its effectiveness and efficiency in locating sparse neural networks. On the other
    hand, output-balanced Wanda pruning shows a significant decline in accuracy as
    the sparsity ratio increases. This suggests that Wanda pruning may suffer from
    structural mismatch issues within the MLP layer, which become more pronounced
    at higher sparsity levels. As a result, the neural network’s performance deteriorates,
    potentially leading to a dysfunctional model at extreme sparsity ratios.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S4.F2 "图 2 ‣ 稀疏性变化 ‣ 4.5 性能分析 ‣ 4 实验 ‣ 大型语言模型中GLU变体的依赖感知半结构化稀疏性")展示了在不同稀疏水平下，LLaMA2-70B模型MLP组件的平均零-shot任务准确性对比。显然，DaSS剪枝保持了竞争力的性能，与SparseGPT在测试的所有稀疏比率范围内表现相近。值得注意的是，DaSS在没有权重更新的情况下实现了这种性能，表明其在定位稀疏神经网络方面的有效性和高效性。另一方面，输出平衡的Wanda剪枝在稀疏比率增加时准确性显著下降。这表明Wanda剪枝可能存在MLP层内的结构不匹配问题，在较高稀疏水平下变得更加明显。因此，神经网络的性能恶化，极端稀疏比率下可能导致模型功能失效。
- en: '![Refer to caption](img/09f1ef1d7a48abb2c5dabb444b2dcf09.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/09f1ef1d7a48abb2c5dabb444b2dcf09.png)'
- en: 'Figure 2: Mean zero-shot tasks performance at different MLP sparsity ratios.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：不同MLP稀疏比率下的平均零-shot任务性能。
- en: Robustness to calibration samples.
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对校准样本的鲁棒性。
- en: Ashkboos et al. ([2023](#bib.bib2)) observes that the intermediate activations
    of SwiGLU-based MLP layers exhibit high variance, primarily caused by the Hadamard
    product of the preceding two outputs. This leads to diminished accuracy in 4-bit
    quantization. In Figure [3](#S4.F3 "Figure 3 ‣ Robustness to calibration samples.
    ‣ 4.5 Performance Analysis ‣ 4 Experiments ‣ Dependency-Aware Semi-Structured
    Sparsity of GLU Variants in Large Language Models"), we present how varying the
    number of sequences sampled for calibration affects the performance of pruning
    methods. Even though only using intermediate activations, our method demonstrates
    minimal sensitivity to changes in the number of calibration samples.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Ashkboos等人（[2023](#bib.bib2)）观察到，基于SwiGLU的MLP层的中间激活显示出较高的方差，主要是由于前两个输出的Hadamard积。这导致了4位量化的准确性降低。在图[3](#S4.F3
    "图 3 ‣ 对校准样本的鲁棒性 ‣ 4.5 性能分析 ‣ 4 实验 ‣ 大型语言模型中GLU变体的依赖感知半结构化稀疏性")中，我们展示了采样的序列数量变化如何影响剪枝方法的性能。尽管只使用中间激活，我们的方法对校准样本数量的变化表现出最小的敏感性。
- en: '![Refer to caption](img/dea6f2f4e6249ce61d9f7a73aebcd59f.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/dea6f2f4e6249ce61d9f7a73aebcd59f.png)'
- en: 'Figure 3: Robustness to calibration samples.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：对校准样本的鲁棒性。
- en: 4.6 Running Time Analysis
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 运行时间分析
- en: Pruning speed
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 剪枝速度
- en: As observed in our preceding study on layer sensitivity, the efficiency of the
    pruning process is crucial when numerous iterations are necessary to reach the
    desired performance for specific tasks. For DaSS and Wanda, the computational
    complexity is quantified as $O(d^{2})$. We recorded the overall time taken for
    pruning MLP layers, not including the forward pass process, in accordance with
    the approach described by Sun et al. ([2023](#bib.bib40)). We use a single A6000
    48GB GPU to prune the 7B and 13B models, and use 8 A100 40GB GPUs to prune the
    larger 70B model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在之前关于层敏感性的研究中观察到的，当需要多次迭代以达到特定任务的期望性能时，剪枝过程的效率至关重要。对于DaSS和Wanda，计算复杂度量化为$O(d^{2})$。我们记录了修剪MLP层所需的总时间，但不包括前向传播过程，按照Sun等人（[2023](#bib.bib40)）描述的方法进行。我们使用单个A6000
    48GB GPU修剪7B和13B模型，并使用8个A100 40GB GPUs修剪更大的70B模型。
- en: As demonstrated in Table [6](#S4.T6 "Table 6 ‣ Pruning speed ‣ 4.6 Running Time
    Analysis ‣ 4 Experiments ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants
    in Large Language Models"), the computational overhead incurred by DaSS is minimal,
    especially when compared to SparseGPT. While the processing speed of DaSS is marginally
    slower than that of Wanda, this can be attributed to Wanda’s more efficient approach
    of sorting weights exclusively along the last dimension. Nonetheless, given the
    substantial improvements in accuracy achieved by DaSS, this additional computational
    time is justifiable and beneficial.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[6](#S4.T6 "Table 6 ‣ Pruning speed ‣ 4.6 Running Time Analysis ‣ 4 Experiments
    ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language
    Models")所示，DaSS 的计算开销是最小的，特别是与 SparseGPT 相比。虽然 DaSS 的处理速度比 Wanda 稍慢，但这可以归因于 Wanda
    在仅沿最后一个维度排序权重的更高效方法。然而，考虑到 DaSS 实现的显著准确性提升，这额外的计算时间是可以理解的并且是有益的。
- en: 'Table 6: Pruning speed (in seconds) comparison.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：剪枝速度（以秒为单位）的比较。
- en: '| Method | Mistral | LLaMA-2 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Mistral | LLaMA-2 |'
- en: '| 7B | 7B | 13B | 70B |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 7B | 7B | 13B | 70B |'
- en: '| SparseGPT | 155 | 185 | 206 | 1092.76 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 155 | 185 | 206 | 1092.76 |'
- en: '| Wanda | 0.60 | 0.54 | 0.85 | 14.13 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 0.60 | 0.54 | 0.85 | 14.13 |'
- en: '| DaSS | 0.81 | 0.81 | 1.02 | 20.70 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| DaSS | 0.81 | 0.81 | 1.02 | 20.70 |'
- en: 5 Related Work
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: 'Pruning LLM. Neural network pruning in LLM can be broadly categorized into
    two groups: structured pruning (Ma et al., [2023](#bib.bib29); Zhang et al., [2023](#bib.bib49))
    and unstructured pruning (Frantar & Alistarh, [2023](#bib.bib14); Sun et al.,
    [2023](#bib.bib40)). Ma et al. ([2023](#bib.bib29)) proposes a dependency detection
    algorithm to detect and prune non-critical grouped structures followed by LoRA
    fine-tuning. Although structured pruning can usually have better hardware efficiency,
    the accuracy drops a lot even at a low compression rate. Unstructured pruning
    can yield a higher compression rate and achieve acceleration on Nvidia’s GPUs
    by employing a hardware-friendly N:M sparsity pattern. SparseGPT (Frantar & Alistarh,
    [2023](#bib.bib14)) leverages the Hessian inverse for pruning and reduces reconstruction
    error of dense and sparse weights by subsequent weight updates. Wanda (Sun et al.,
    [2023](#bib.bib40)) employs an efficient method that augments input activations
    into weight magnitudes, and matches the performance of SparseGPT at medium sparsity.
    Our work incorporates dependency information into unstructured pruning, achieving
    a novel pruning paradigm.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝 LLM。在 LLM 中，神经网络剪枝大致可以分为两类：结构化剪枝（Ma 等， [2023](#bib.bib29)；Zhang 等， [2023](#bib.bib49)）和非结构化剪枝（Frantar
    & Alistarh， [2023](#bib.bib14)；Sun 等， [2023](#bib.bib40)）。Ma 等（[2023](#bib.bib29)）提出了一种依赖检测算法，用于检测和剪枝非关键分组结构，随后进行
    LoRA 微调。尽管结构化剪枝通常具有更好的硬件效率，但即使在较低的压缩率下，准确性也会大幅下降。非结构化剪枝可以获得更高的压缩率，并通过采用适合硬件的 N:M
    稀疏模式在 Nvidia 的 GPU 上实现加速。SparseGPT（Frantar & Alistarh， [2023](#bib.bib14)）利用 Hessian
    逆矩阵进行剪枝，并通过随后的权重更新减少稠密和稀疏权重的重构误差。Wanda（Sun 等， [2023](#bib.bib40)）采用一种高效的方法，将输入激活增强为权重幅度，并在中等稀疏度下与
    SparseGPT 的性能匹配。我们的工作将依赖信息融入到非结构化剪枝中，实现了一种新型剪枝范式。
- en: Inherent Sparsity of Transformer MLP. Interestingly, sparsity within the MLP
    activations of trained Transformer-based models occurs innately even without applying
    explicit regularizations or constraints (Zhang et al., [2022](#bib.bib50); Li
    et al., [2023](#bib.bib26); Dong et al., [2023](#bib.bib12)). Such a phenomenon
    is prevalent in learned Transformers, including other zero-saturating functions.
    Liu et al. ([2023](#bib.bib28)); Mirzadeh et al. ([2023](#bib.bib32)); Zhang et al.
    ([2022](#bib.bib50)) achieve actual LLM inference speedup by only performing computation
    corresponding to the activating neuron for a given input. They do not actually
    reduce the model size since they mainly reduce I/O and computation latency in
    a selective weights loading manner, and thus, these methods are less applicable
    in large batch-size inference settings. Our work investigates the weight sparsity
    in MLP module by considering corresponding intermediate activations.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器 MLP 的固有稀疏性。 有趣的是，即使不应用显式的正则化或约束，经过训练的基于变压器的模型的 MLP 激活中的稀疏性也会自然发生（Zhang et
    al., [2022](#bib.bib50); Li et al., [2023](#bib.bib26); Dong et al., [2023](#bib.bib12)）。这种现象在学习到的变压器中普遍存在，包括其他零饱和函数。
    Liu et al. ([2023](#bib.bib28)); Mirzadeh et al. ([2023](#bib.bib32)); Zhang et
    al. ([2022](#bib.bib50)) 通过仅执行与给定输入相关的激活神经元的计算，实现了实际的 LLM 推理加速。他们并没有实际减少模型的大小，因为他们主要通过选择性权重加载的方式减少
    I/O 和计算延迟，因此这些方法在大批量推理设置中适用性较差。我们的工作通过考虑相应的中间激活来研究 MLP 模块中的权重稀疏性。
- en: Outlier-dependent LLM Compression. Outlier features, defined as features with
    magnitudes substantially larger than others, are a notable characteristic of LLMs
    (Dettmers et al., [2022](#bib.bib11)). Despite making up only a small fraction
    of all feature dimensions, these outliers play a critical role in attention and
    predictive performance. Such observation has impeded the development of some LLM-specific
    quantization methods (Dettmers et al., [2022](#bib.bib11); Xiao et al., [2023](#bib.bib46);
    Lin et al., [2023](#bib.bib27); Ashkboos et al., [2023](#bib.bib2)) to handle
    outliers more effectively. Wanda (Sun et al., [2023](#bib.bib40)) broadens these
    insights, revealing that outlier features are significant in deciding weight importance
    when pruning LLMs. Our analysis diverges from conventional wisdom by demonstrating
    that, in the context of SwiGLU-based MLP input projections, the influence of outlier
    features is not as pronounced as previously assumed, prompting a reevaluation
    of their role in LLM compression strategies.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 离群点依赖的 LLM 压缩。离群特征定义为幅度远大于其他特征的特征，是 LLM 的一个显著特征（Dettmers et al., [2022](#bib.bib11)）。尽管只占所有特征维度的一小部分，这些离群点在注意力和预测性能中发挥了关键作用。这种观察阻碍了某些
    LLM 特定量化方法的发展（Dettmers et al., [2022](#bib.bib11); Xiao et al., [2023](#bib.bib46);
    Lin et al., [2023](#bib.bib27); Ashkboos et al., [2023](#bib.bib2)），以更有效地处理离群点。Wanda
    (Sun et al., [2023](#bib.bib40)) 扩展了这些见解，揭示了在修剪 LLM 时离群特征在决定权重重要性方面的重要性。我们的分析与传统观点有所不同，表明在
    SwiGLU 基础的 MLP 输入投影的背景下，离群特征的影响不像以前假设的那样显著，从而促使我们重新评估它们在 LLM 压缩策略中的作用。
- en: 6 Conclusion
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We propose the Dependency-aware Semi-structured Sparsity (DaSS) method which
    effectively addresses the challenges of pruning SwiGLU-based MLP modules LLMs.
    DaSS strikes a unique balance between the adaptability of unstructured pruning
    and the orderliness of structured pruning. By leveraging the MLP intermediate
    activation norms as the group importance indicator, we develop a novel pruning
    metric that assesses weight importance in a more structurally consistent manner.
    Empirical evaluations on the Mistral and LLaMA2 model families demonstrate that
    DaSS surpasses state-of-the-art LLM pruning methods like SparseGPT and Wanda in
    achieving hardware-friendly N:M sparsity patterns. Our research further reveals
    that selectively skipping the pruning of earlier layers can lead to significant
    performance improvements in knowledge-intensive tasks. The efficiency of the pruning
    process becomes paramount when multiple iterations are essential to tailor and
    optimize the sparsity of LLMs for varied tasks. This underscores the potential
    of our DaSS method in facilitating rapid and task-specific model pruning to achieve
    optimal performance.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了依赖感知半结构稀疏性（DaSS）方法，能够有效解决修剪基于 SwiGLU 的 MLP 模块 LLMs 的挑战。DaSS 在无结构修剪的适应性与结构化修剪的有序性之间取得了独特的平衡。通过利用
    MLP 中间激活范数作为组重要性指标，我们开发了一种新颖的修剪度量，以更结构化的一致方式评估权重重要性。对 Mistral 和 LLaMA2 模型系列的实证评估表明，DaSS
    在实现硬件友好的 N:M 稀疏模式方面超越了最先进的 LLM 修剪方法，如 SparseGPT 和 Wanda。我们的研究进一步揭示，选择性跳过早期层的修剪可以在知识密集型任务中显著提高性能。当多个迭代对于调整和优化
    LLM 稀疏性以适应不同任务至关重要时，修剪过程的效率变得至关重要。这突显了我们 DaSS 方法在促进快速和任务特定模型修剪以实现最佳性能的潜力。
- en: Impact Statements
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 影响声明
- en: The present study is focused on advancing the field of large language model
    deployment with an emphasis on efficiency. While there are numerous societal implications
    of our work, we believe none require specific emphasis in this context.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当前研究专注于提高大型语言模型部署的效率。虽然我们的工作有许多社会影响，但我们认为在此背景下没有哪个需要特别强调。
- en: References
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ainslie et al. (2023) Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy,
    Y., Lebrón, F., and Sanghai, S. Gqa: Training generalized multi-query transformer
    models from multi-head checkpoints. *arXiv preprint arXiv:2305.13245*, 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安斯利等（2023）安斯利，J.，李-索普，J.，德容，M.，泽姆连斯基，Y.，勒布朗，F.，和桑盖。Gqa：从多头检查点训练通用多查询变换器模型。*arXiv
    预印本 arXiv:2305.13245*，2023。
- en: Ashkboos et al. (2023) Ashkboos, S., Markov, I., Frantar, E., Zhong, T., Wang,
    X., Ren, J., Hoefler, T., and Alistarh, D. Towards end-to-end 4-bit inference
    on generative large language models. *the 3rd NeurIPS Workshop on Efficient Natural
    Language and Speech Processing (ENLSP)*, 2023.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿什克布斯等（2023）阿什克布斯，马克洛夫，弗兰塔尔，钟婷，王鑫，任洁，霍弗勒，阿利斯塔赫。朝着生成大型语言模型的端到端 4-bit 推理迈进。*第3届
    NeurIPS 高效自然语言和语音处理研讨会 (ENLSP)*，2023。
- en: 'Bisk et al. (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning
    about physical commonsense in natural language. In *Proceedings of the AAAI conference
    on artificial intelligence*, volume 34, pp.  7432–7439, 2020.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比斯克等（2020）比斯克，Y.，泽勒斯，R.，高俊，崔洋等。Piqa：在自然语言中推理物理常识。在*AAAI 人工智能会议论文集*，第34卷，第7432–7439页，2020。
- en: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language
    models are few-shot learners. *Advances in neural information processing systems*,
    33:1877–1901, 2020.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布朗等（2020）布朗，T.，曼恩，B.，赖德，N.，苏比亚，M.，卡普兰，J. D.，达里瓦尔，P.，尼拉坎坦，A.，夏姆，P.，萨斯特里，G.，阿斯克尔，A.
    等。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901，2020。
- en: Chen et al. (2021) Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,
    Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating
    large language models trained on code. *arXiv preprint arXiv:2107.03374*, 2021.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021）陈敏，特沃雷克，君浩，袁琦，平托，H. P. d. O.，卡普兰，爱德华兹，布尔达，约瑟夫，布罗克曼等。评估基于代码训练的大型语言模型。*arXiv
    预印本 arXiv:2107.03374*，2021。
- en: 'Chowdhery et al. (2023) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P.,
    Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
    N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J.,
    Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat,
    S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L.,
    Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi,
    R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,
    Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X.,
    Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck,
    D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with
    pathways. *Journal of Machine Learning Research*, 24(240):1–113, 2023. URL [http://jmlr.org/papers/v24/22-1144.html](http://jmlr.org/papers/v24/22-1144.html).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等人（2023）Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P.,
    Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
    N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J.,
    Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat,
    S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L.,
    Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi,
    R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,
    Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X.,
    Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck,
    D., Dean, J., Petrov, S., 和 Fiedel, N. Palm: 通过路径扩展语言建模。*机器学习研究期刊*，24(240):1–113，2023年。网址
    [http://jmlr.org/papers/v24/22-1144.html](http://jmlr.org/papers/v24/22-1144.html)。'
- en: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try
    arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*, 2018.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人（2018）Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick,
    C., 和 Tafjord, O. 认为你已经解决了问答问题？试试arc，AI2推理挑战。*arXiv 预印本 arXiv:1803.05457*，2018年。
- en: Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,
    Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training
    verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*, 2021.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人（2021）Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser,
    L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., 等。训练验证器解决数学文字题。*arXiv 预印本
    arXiv:2110.14168*，2021年。
- en: Dauphin et al. (2017) Dauphin, Y. N., Fan, A., Auli, M., and Grangier, D. Language
    modeling with gated convolutional networks. In *International conference on machine
    learning*, pp.  933–941\. PMLR, 2017.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dauphin 等人（2017）Dauphin, Y. N., Fan, A., Auli, M., 和 Grangier, D. 使用门控卷积网络进行语言建模。发表于
    *国际机器学习会议*，第933–941页。PMLR，2017年。
- en: 'Dettmers & Zettlemoyer (2023) Dettmers, T. and Zettlemoyer, L. The case for
    4-bit precision: k-bit inference scaling laws. In *International Conference on
    Machine Learning*, pp.  7750–7774\. PMLR, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 和 Zettlemoyer（2023）Dettmers, T. 和 Zettlemoyer, L. 4位精度的案例：k位推理扩展定律。发表于
    *国际机器学习会议*，第7750–7774页。PMLR，2023年。
- en: 'Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,
    L. GPT3.int8(): 8-bit matrix multiplication for transformers at scale. In Oh,
    A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), *Advances in Neural Information
    Processing Systems*, 2022. URL [https://openreview.net/forum?id=dXiGWqBoxaD](https://openreview.net/forum?id=dXiGWqBoxaD).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人（2022）Dettmers, T., Lewis, M., Belkada, Y., 和 Zettlemoyer, L. GPT3.int8():
    大规模变压器的8位矩阵乘法。发表于 Oh, A. H., Agarwal, A., Belgrave, D., 和 Cho, K.（编），*神经信息处理系统进展*，2022年。网址
    [https://openreview.net/forum?id=dXiGWqBoxaD](https://openreview.net/forum?id=dXiGWqBoxaD)。'
- en: Dong et al. (2023) Dong, H., Chen, B., and Chi, Y. Towards structured sparsity
    in transformers for efficient inference. In *Workshop on Efficient Systems for
    Foundation Models @ ICML2023*, 2023. URL [https://openreview.net/forum?id=c4m0BkO4OL](https://openreview.net/forum?id=c4m0BkO4OL).
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等人（2023）Dong, H., Chen, B., 和 Chi, Y. 在变压器中实现结构化稀疏性以提高推理效率。发表于 *ICML2023基础模型高效系统研讨会*，2023年。网址
    [https://openreview.net/forum?id=c4m0BkO4OL](https://openreview.net/forum?id=c4m0BkO4OL)。
- en: 'Fang et al. (2023) Fang, G., Ma, X., Song, M., Mi, M. B., and Wang, X. Depgraph:
    Towards any structural pruning. In *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, pp.  16091–16101, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fang 等人（2023）Fang, G., Ma, X., Song, M., Mi, M. B., 和 Wang, X. Depgraph: 面向任何结构的剪枝。发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，第16091–16101页，2023年。'
- en: 'Frantar & Alistarh (2023) Frantar, E. and Alistarh, D. SparseGPT: Massive language
    models can be accurately pruned in one-shot. In Krause, A., Brunskill, E., Cho,
    K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), *Proceedings of the 40th
    International Conference on Machine Learning*, volume 202 of *Proceedings of Machine
    Learning Research*, pp.  10323–10337\. PMLR, 23–29 Jul 2023. URL [https://proceedings.mlr.press/v202/frantar23a.html](https://proceedings.mlr.press/v202/frantar23a.html).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar & Alistarh (2023) Frantar, E. 和 Alistarh, D. SparseGPT：大型语言模型可以在一次剪枝中准确修剪。在Krause,
    A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., 和 Scarlett, J. (编辑)，*第40届国际机器学习大会论文集*，第202卷的*机器学习研究论文集*，第10323–10337页。PMLR，2023年7月23–29日。URL
    [https://proceedings.mlr.press/v202/frantar23a.html](https://proceedings.mlr.press/v202/frantar23a.html)。
- en: 'Frantar et al. (2023) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. OPTQ: Accurate quantization for generative pre-trained transformers. In *The
    Eleventh International Conference on Learning Representations*, 2023. URL [https://openreview.net/forum?id=tcbBPnfwxS](https://openreview.net/forum?id=tcbBPnfwxS).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar et al. (2023) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. OPTQ：生成预训练变换器的准确量化。在*第十一届国际学习表征会议*，2023。URL [https://openreview.net/forum?id=tcbBPnfwxS](https://openreview.net/forum?id=tcbBPnfwxS)。
- en: 'Fu et al. (2023) Fu, Y., Ou, L., Chen, M., Wan, Y., Peng, H., and Khot, T.
    Chain-of-thought hub: A continuous effort to measure large language models’ reasoning
    performance. *arXiv preprint arXiv:2305.17306*, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2023) Fu, Y., Ou, L., Chen, M., Wan, Y., Peng, H., and Khot, T. 思维链中心：持续努力衡量大型语言模型的推理性能。*arXiv预印本
    arXiv:2305.17306*，2023。
- en: Gao et al. (2021) Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,
    C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot
    language model evaluation, September 2021. URL [https://doi.org/10.5281/zenodo.5371629](https://doi.org/10.5281/zenodo.5371629).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2021) Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,
    C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds,
    L., Tang, E., Thite, A., Wang, B., Wang, K., 和 Zou, A. 少样本语言模型评估框架，2021年9月。URL
    [https://doi.org/10.5281/zenodo.5371629](https://doi.org/10.5281/zenodo.5371629)。
- en: Geva et al. (2023) Geva, M., Bastings, J., Filippova, K., and Globerson, A.
    Dissecting recall of factual associations in auto-regressive language models.
    In *The 2023 Conference on Empirical Methods in Natural Language Processing*,
    2023. URL [https://openreview.net/forum?id=F1G7y94K02](https://openreview.net/forum?id=F1G7y94K02).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geva et al. (2023) Geva, M., Bastings, J., Filippova, K., and Globerson, A.
    解剖自回归语言模型中的事实关联回忆。在*2023年自然语言处理实证方法会议*，2023。URL [https://openreview.net/forum?id=F1G7y94K02](https://openreview.net/forum?id=F1G7y94K02)。
- en: Glorot et al. (2011) Glorot, X., Bordes, A., and Bengio, Y. Deep sparse rectifier
    neural networks. In *Proceedings of the fourteenth international conference on
    artificial intelligence and statistics*, pp.  315–323\. JMLR Workshop and Conference
    Proceedings, 2011.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glorot et al. (2011) Glorot, X., Bordes, A., 和 Bengio, Y. 深度稀疏整流神经网络。在*第十四届人工智能与统计国际会议论文集*，第315–323页。JMLR工作坊与会议论文集，2011。
- en: Han et al. (2015) Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights
    and connections for efficient neural network. *Advances in neural information
    processing systems*, 28, 2015.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2015) Han, S., Pool, J., Tran, J., and Dally, W. 学习权重和连接以提高神经网络的效率。*神经信息处理系统进展*，28，2015。
- en: Haviv et al. (2023) Haviv, A., Cohen, I., Gidron, J., Schuster, R., Goldberg,
    Y., and Geva, M. Understanding transformer memorization recall through idioms.
    In *Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics*, pp.  248–264, 2023.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haviv et al. (2023) Haviv, A., Cohen, I., Gidron, J., Schuster, R., Goldberg,
    Y., 和 Geva, M. 通过习语理解变换器记忆回忆。在*第17届欧洲计算语言学协会会议论文集*，第248–264页，2023。
- en: Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding.
    In *International Conference on Learning Representations*, 2021. URL [https://openreview.net/forum?id=d7KBjmI3GmQ](https://openreview.net/forum?id=d7KBjmI3GmQ).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D., and Steinhardt, J. 测量大规模多任务语言理解。在*国际学习表征会议*，2021。URL [https://openreview.net/forum?id=d7KBjmI3GmQ](https://openreview.net/forum?id=d7KBjmI3GmQ)。
- en: 'Jaiswal et al. (2023) Jaiswal, A., Gan, Z., Du, X., Zhang, B., Wang, Z., and
    Yang, Y. Compressing llms: The truth is rarely pure and never simple. *arXiv preprint
    arXiv:2310.01382*, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaiswal et al. (2023) Jaiswal, A., Gan, Z., Du, X., Zhang, B., Wang, Z., and
    Yang, Y. 压缩LLMs：真相很少是纯粹的，也从未简单。*arXiv预印本 arXiv:2310.01382*，2023。
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*, 2023.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot,
    D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., 等.
    Mistral 7b。*arXiv 预印本 arXiv:2310.06825*, 2023。
- en: Kurtic et al. (2023) Kurtic, E., Kuznedelev, D., Frantar, E., Goin, M., and
    Alistarh, D. Sparse finetuning for inference acceleration of large language models.
    *arXiv preprint arXiv:2310.06927*, 2023.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurtic 等 (2023) Kurtic, E., Kuznedelev, D., Frantar, E., Goin, M., 和 Alistarh,
    D. 稀疏微调用于大型语言模型的推理加速。*arXiv 预印本 arXiv:2310.06927*, 2023。
- en: 'Li et al. (2023) Li, Z., You, C., Bhojanapalli, S., Li, D., Rawat, A. S., Reddi,
    S. J., Ye, K., Chern, F., Yu, F., Guo, R., and Kumar, S. The lazy neuron phenomenon:
    On emergence of activation sparsity in transformers. In *The Eleventh International
    Conference on Learning Representations*, 2023. URL [https://openreview.net/forum?id=TJ2nxciYCk-](https://openreview.net/forum?id=TJ2nxciYCk-).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2023) Li, Z., You, C., Bhojanapalli, S., Li, D., Rawat, A. S., Reddi,
    S. J., Ye, K., Chern, F., Yu, F., Guo, R., 和 Kumar, S. 懒惰神经元现象: 转换器中激活稀疏性的出现。在
    *第十一届国际学习表征大会*, 2023. 网址 [https://openreview.net/forum?id=TJ2nxciYCk-](https://openreview.net/forum?id=TJ2nxciYCk-)。'
- en: 'Lin et al. (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han,
    S. Awq: Activation-aware weight quantization for llm compression and acceleration.
    *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等 (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., 和 Han, S. AWQ:
    激活感知权重量化用于 LLM 压缩和加速。*arXiv 预印本 arXiv:2306.00978*, 2023。'
- en: 'Liu et al. (2023) Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,
    Shrivastava, A., Zhang, C., Tian, Y., Re, C., et al. Deja vu: Contextual sparsity
    for efficient llms at inference time. In *International Conference on Machine
    Learning*, pp.  22137–22176\. PMLR, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2023) Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava,
    A., Zhang, C., Tian, Y., Re, C., 等. Déjà vu: 推理时高效 LLM 的上下文稀疏性。在 *国际机器学习大会*, 第
    22137–22176 页。PMLR, 2023。'
- en: 'Ma et al. (2023) Ma, X., Fang, G., and Wang, X. Llm-pruner: On the structural
    pruning of large language models. *Advances in Neural Information Processing Systems*,
    2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等 (2023) Ma, X., Fang, G., 和 Wang, X. LLM-Pruner: 关于大型语言模型的结构剪枝。*神经信息处理系统进展*,
    2023。'
- en: Meng et al. (2022) Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating
    and editing factual associations in gpt. *Advances in Neural Information Processing
    Systems*, 35:17359–17372, 2022.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng 等 (2022) Meng, K., Bau, D., Andonian, A., 和 Belinkov, Y. 在 GPT 中定位和编辑事实关联。*神经信息处理系统进展*,
    35:17359–17372, 2022。
- en: Merity et al. (2017) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
    sentinel mixture models. In *International Conference on Learning Representations*,
    2017. URL [https://openreview.net/forum?id=Byj72udxe](https://openreview.net/forum?id=Byj72udxe).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等 (2017) Merity, S., Xiong, C., Bradbury, J., 和 Socher, R. 指针哨兵混合模型。在
    *国际学习表征大会*, 2017. 网址 [https://openreview.net/forum?id=Byj72udxe](https://openreview.net/forum?id=Byj72udxe)。
- en: 'Mirzadeh et al. (2023) Mirzadeh, I., Alizadeh, K., Mehta, S., Del Mundo, C. C.,
    Tuzel, O., Samei, G., Rastegari, M., and Farajtabar, M. Relu strikes back: Exploiting
    activation sparsity in large language models. *arXiv preprint arXiv:2310.04564*,
    2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mirzadeh 等 (2023) Mirzadeh, I., Alizadeh, K., Mehta, S., Del Mundo, C. C.,
    Tuzel, O., Samei, G., Rastegari, M., 和 Farajtabar, M. ReLU 反击: 利用大型语言模型中的激活稀疏性。*arXiv
    预印本 arXiv:2310.04564*, 2023。'
- en: Mishra et al. (2021) Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic,
    D., Venkatesh, G., Yu, C., and Micikevicius, P. Accelerating sparse deep neural
    networks. *arXiv preprint arXiv:2104.08378*, 2021.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra 等 (2021) Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic, D.,
    Venkatesh, G., Yu, C., 和 Micikevicius, P. 加速稀疏深度神经网络。*arXiv 预印本 arXiv:2104.08378*,
    2021。
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. GPT-4 技术报告, 2023。
- en: Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer
    learning with a unified text-to-text transformer. *The Journal of Machine Learning
    Research*, 21(1):5485–5551, 2020.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等 (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
    M., Zhou, Y., Li, W., 和 Liu, P. J. 探索统一文本到文本转换器的迁移学习极限。*机器学习研究期刊*, 21(1):5485–5551,
    2020。
- en: 'Ramachandran et al. (2017) Ramachandran, P., Zoph, B., and Le, Q. V. Swish:
    a self-gated activation function. *arXiv: Neural and Evolutionary Computing*,
    2017. URL [https://api.semanticscholar.org/CorpusID:196158220](https://api.semanticscholar.org/CorpusID:196158220).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ramachandran 等 (2017) Ramachandran, P., Zoph, B., 和 Le, Q. V. Swish: 自门控激活函数。*arXiv:
    神经与进化计算*, 2017. 网址 [https://api.semanticscholar.org/CorpusID:196158220](https://api.semanticscholar.org/CorpusID:196158220)。'
- en: 'Sakaguchi et al. (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. Winogrande: An adversarial winograd schema challenge at scale. *Communications
    of the ACM*, 64(9):99–106, 2021.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi 等人（2021） Sakaguchi, K., Bras, R. L., Bhagavatula, C., 和 Choi, Y. **Winogrande**：大规模对抗性
    Winograd 语义挑战。*ACM 通讯*，64(9)：99–106，2021。
- en: 'Shazeer (2019) Shazeer, N. Fast transformer decoding: One write-head is all
    you need. *arXiv preprint arXiv:1911.02150*, 2019.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer（2019） Shazeer, N. 快速变换器解码：**一个**写头就是你所需要的。*arXiv 预印本 arXiv:1911.02150*，2019。
- en: Shazeer (2020) Shazeer, N. Glu variants improve transformer. *arXiv preprint
    arXiv:2002.05202*, 2020.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer（2020） Shazeer, N. **Glu** 变体改进变换器。*arXiv 预印本 arXiv:2002.05202*，2020。
- en: Sun et al. (2023) Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and
    effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2023） Sun, M., Liu, Z., Bair, A., 和 Kolter, J. Z. 一种简单而有效的大型语言模型剪枝方法。*arXiv
    预印本 arXiv:2306.11695*，2023。
- en: Team (2023) Team, S. Sparse large language models with relu activation, 2023.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team（2023） Team, S. 带 ReLU 激活的稀疏大型语言模型，2023。
- en: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation
    language models, 2023a.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2023a） Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., 和 Lample, G. **Llama**：开放且高效的基础语言模型，2023a。
- en: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D.,
    Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J.,
    Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M.,
    Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
    Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez,
    A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned
    chat models, 2023b.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2023b） Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D.,
    Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J.,
    Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.
    M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J.
    X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S.,
    Rodriguez, A., Stojnic, R., Edunov, S., 和 Scialom, T. **Llama 2**：开放基础和微调聊天模型，2023b。
- en: Wen et al. (2016) Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H. Learning structured
    sparsity in deep neural networks. *Advances in neural information processing systems*,
    29, 2016.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等人（2016） Wen, W., Wu, C., Wang, Y., Chen, Y., 和 Li, H. 在深度神经网络中学习结构稀疏性。*神经信息处理系统进展*，29，2016。
- en: 'Wolf et al. (2019) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
    Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Huggingface’s
    transformers: State-of-the-art natural language processing. *arXiv preprint arXiv:1910.03771*,
    2019.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf 等人（2019） Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi,
    A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., 等。**Huggingface** 的变换器：最先进的自然语言处理。*arXiv
    预印本 arXiv:1910.03771*，2019。
- en: 'Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and
    Han, S. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In *International Conference on Machine Learning*, pp.  38087–38099\.
    PMLR, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人（2023） Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., 和 Han, S.
    **Smoothquant**：针对大型语言模型的准确且高效的后训练量化。 在*国际机器学习会议*，第 38087–38099 页。PMLR，2023。
- en: 'Yin et al. (2023) Yin, L., Wu, Y., Zhang, Z., Hsieh, C.-Y., Wang, Y., Jia,
    Y., Pechenizkiy, M., Liang, Y., Wang, Z., and Liu, S. Outlier weighed layerwise
    sparsity (owl): A missing secret sauce for pruning llms to high sparsity. *arXiv
    preprint arXiv:2310.05175*, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等人（2023） Yin, L., Wu, Y., Zhang, Z., Hsieh, C.-Y., Wang, Y., Jia, Y., Pechenizkiy,
    M., Liang, Y., Wang, Z., 和 Liu, S. **Outlier weighed layerwise sparsity (owl)**：修剪大型语言模型到高稀疏性的一个缺失的秘密配方。*arXiv
    预印本 arXiv:2310.05175*，2023。
- en: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers 等人（2019） Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., 和 Choi, Y.
    **Hellaswag**：机器真的能完成你的句子吗？ *arXiv 预印本 arXiv:1905.07830*，2019。
- en: Zhang et al. (2023) Zhang, M., Shen, C., Yang, Z., Ou, L., Yu, X., Zhuang, B.,
    et al. Pruning meets low-rank parameter-efficient fine-tuning. *arXiv preprint
    arXiv:2305.18403*, 2023.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023) Zhang, M., Shen, C., Yang, Z., Ou, L., Yu, X., Zhuang, B.,
    et al. 剪枝遇上低秩参数高效微调。*arXiv 预印本 arXiv:2305.18403*，2023年。
- en: 'Zhang et al. (2022) Zhang, Z., Lin, Y., Liu, Z., Li, P., Sun, M., and Zhou,
    J. MoEfication: Transformer feed-forward layers are mixtures of experts. In Muresan,
    S., Nakov, P., and Villavicencio, A. (eds.), *Findings of the Association for
    Computational Linguistics: ACL 2022*, pp.  877–890, Dublin, Ireland, May 2022\.
    Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.71.
    URL [https://aclanthology.org/2022.findings-acl.71](https://aclanthology.org/2022.findings-acl.71).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022) Zhang, Z., Lin, Y., Liu, Z., Li, P., Sun, M., and Zhou,
    J. MoEfication: Transformer feed-forward layers are mixtures of experts. In Muresan,
    S., Nakov, P., and Villavicencio, A. (eds.), *Association for Computational Linguistics:
    ACL 2022*，第 877–890 页，爱尔兰都柏林，2022年5月。计算语言学协会。doi: 10.18653/v1/2022.findings-acl.71.
    网址 [https://aclanthology.org/2022.findings-acl.71](https://aclanthology.org/2022.findings-acl.71)。'
- en: Appendix A Model Configurations
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 模型配置
- en: 'Here are the configurations of the models used in the paper. We don’t use LLaMA2-34B
    since it was not released. ReluLLaMA uses the same configuration as LLaMA2, the
    only difference is the activation function. Here is the link to ReluLLaMA:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是论文中使用模型的配置。我们没有使用 LLaMA2-34B，因为它尚未发布。ReluLLaMA 使用与 LLaMA2 相同的配置，唯一的区别是激活函数。以下是
    ReluLLaMA 的链接：
- en: 'ReluLLaMA-7B: https://huggingface.co/SparseLLM/ReluLLaMA-7B'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 'ReluLLaMA-7B: https://huggingface.co/SparseLLM/ReluLLaMA-7B'
- en: 'ReluLLaMA-13B: https://huggingface.co/SparseLLM/ReluLLaMA-13B'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 'ReluLLaMA-13B: https://huggingface.co/SparseLLM/ReluLLaMA-13B'
- en: 'Table 7: Model configurations of Llama2 and Mistral models.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: Llama2 和 Mistral 模型的配置。'
- en: '| Model | Param | Layers | Hidden | Intermediate | Query Heads | KV Head |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数 | 层数 | 隐藏 | 中间 | 查询头 | KV 头 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| LlaMa2-7B | 7B | 32 | 4096 | 11008 | 32 | 32 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| LlaMa2-7B | 7B | 32 | 4096 | 11008 | 32 | 32 |'
- en: '| LlaMa2-13B | 13B | 40 | 5120 | 13824 | 40 | 40 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| LlaMa2-13B | 13B | 40 | 5120 | 13824 | 40 | 40 |'
- en: '| LlaMa2-70B | 70B | 80 | 8192 | 28672 | 64 | 8 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| LlaMa2-70B | 70B | 80 | 8192 | 28672 | 64 | 8 |'
- en: '| Mistral-7B | 7B | 32 | 4096 | 14336 | 32 | 8 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 7B | 32 | 4096 | 14336 | 32 | 8 |'
- en: Appendix B Activation Outlier in SwiGLU
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B SwiGLU 中的激活异常值
- en: '![Refer to caption](img/ea3bcb01f9485bfbc48d896e86b50818.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ea3bcb01f9485bfbc48d896e86b50818.png)'
- en: 'Figure 4: Layerwise Activation Outlier distribution of LLaMA2-7B'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: LLaMA2-7B 的层级激活异常值分布'
- en: In Section 2.3, we show the importance of augmenting input magnitude is useful.
    In this part, we target to identify the existence of outliers in the intermediate
    activations of SwiGLU. Directly analyzing it like model hidden states Dettmers
    et al. ([2022](#bib.bib11)); Xiao et al. ([2023](#bib.bib46)) is challenging since
    it demonstrates high variance. (Ashkboos et al., [2023](#bib.bib2)). Wanda (Sun
    et al., [2023](#bib.bib40)) shows $\ell_{2}$ as an empirical value 7\. The layerwise
    distribution of the SwiGLU intermediate activation outlier is shown in Figure
    [4](#A2.F4 "Figure 4 ‣ Appendix B Activation Outlier in SwiGLU ‣ Dependency-Aware
    Semi-Structured Sparsity of GLU Variants in Large Language Models").
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2.3 节中，我们展示了增加输入幅度的重要性。在这一部分，我们旨在识别 SwiGLU 中间激活的异常值。直接像模型隐藏状态 Dettmers et
    al. ([2022](#bib.bib11)); Xiao et al. ([2023](#bib.bib46)) 那样分析是具有挑战性的，因为它展示了高方差。
    (Ashkboos et al., [2023](#bib.bib2))。Wanda (Sun et al., [2023](#bib.bib40)) 显示了
    $\ell_{2}$ 作为经验值 7。SwiGLU 中间激活异常值的层级分布见图 [4](#A2.F4 "Figure 4 ‣ Appendix B Activation
    Outlier in SwiGLU ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants
    in Large Language Models")。
- en: Appendix C GeLU-based MLP Input projection
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 基于 GeLU 的 MLP 输入投影
- en: In the older MLP module, unlike GLU variants, there are only two projection
    matrices. We investigate magnitude pruning in different granularity for the MLP
    input projection matrix. We use the Falcon-7B model, which uses GeLU-based MLP.
    ”Layer Magnitude” means we group weights in the entire weight matrix using magnitude
    pruning. In Table [8](#A3.T8 "Table 8 ‣ Appendix C GeLU-based MLP Input projection
    ‣ Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language
    Models"), we surprisingly find the input-balanced magnitude pruning achieves much
    worse results. This is in contrast to the results in GLU variants. The results
    show the huge significance of input activations for GeLU-based input projection
    pruning. Since our DaSS method needs to group weights per input, our method is
    less applicable to older MLP modules. However, GeLU-based MLP is rarely used in
    the current state-of-the-art LLMs.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在较旧的MLP模块中，与GLU变体不同，仅有两个投影矩阵。我们在不同粒度下调查了MLP输入投影矩阵的幅度剪枝。我们使用了基于GeLU的MLP的Falcon-7B模型。“层幅度”意味着我们通过幅度剪枝对整个权重矩阵中的权重进行分组。在表[8](#A3.T8
    "Table 8 ‣ Appendix C GeLU-based MLP Input projection ‣ Dependency-Aware Semi-Structured
    Sparsity of GLU Variants in Large Language Models")中，我们惊讶地发现输入平衡幅度剪枝的效果远不如预期。这与GLU变体的结果形成对比。结果显示，GeLU基于的输入投影剪枝中输入激活的重要性。由于我们的DaSS方法需要按输入分组权重，我们的方法不太适用于较旧的MLP模块。然而，基于GeLU的MLP在当前最先进的LLM中很少使用。
- en: 'Table 8: The Wikitext perplexity results of GeLU input projection at different
    sparsity ratio'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：不同稀疏比率下GeLU输入投影的Wikitext困惑度结果
- en: '| Sparsity | 50% | 60% | 70% |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏性 | 50% | 60% | 70% |'
- en: '| Wanda | 7.17 | 8.72 | 18.95 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 7.17 | 8.72 | 18.95 |'
- en: '| Output-balanced Magnitude | 8.46 | 15.27 | 498.77 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 输出平衡幅度 | 8.46 | 15.27 | 498.77 |'
- en: '| Input-balanced Magnitude | 1708 | 10587 | 13706 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 输入平衡幅度 | 1708 | 10587 | 13706 |'
- en: '| Layer Magnitude | 21.82 | 1895 | 12896 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 层幅度 | 21.82 | 1895 | 12896 |'
