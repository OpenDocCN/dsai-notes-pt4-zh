- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:52:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:52:30
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Effectively Compress KV Heads for LLM
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有效压缩KV头部以优化LLM
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.07056](https://ar5iv.labs.arxiv.org/html/2406.07056)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.07056](https://ar5iv.labs.arxiv.org/html/2406.07056)
- en: Hao Yu^(1,2)  Zelan Yang³  Shen Li³  Yong Li³  Jianxin Wu^(1,2)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Hao Yu^(1,2)  Zelan Yang³  Shen Li³  Yong Li³  Jianxin Wu^(1,2)
- en: ¹State Key Laboratory for Novel Software Technology, Nanjing University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹南京大学新型软件技术国家重点实验室
- en: ²School of Artificial Intelligence, Nanjing University  ³Alibaba Inc.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²南京大学人工智能学院  ³阿里巴巴公司
- en: yuh@lamda.nju.edu.cn
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: yuh@lamda.nju.edu.cn
- en: '{yangzelan.yzl,litan.ls,jiufeng.ly}@alibaba-inc.com'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{yangzelan.yzl,litan.ls,jiufeng.ly}@alibaba-inc.com'
- en: wujx2001@gmail.com J. Wu is the corresponding author.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: wujx2001@gmail.com J. Wu是通讯作者。
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The advent of pre-trained large language models (LLMs) has revolutionized various
    natural language processing tasks. These models predominantly employ an auto-regressive
    decoding mechanism that utilizes Key-Value (KV) caches to eliminate redundant
    calculations for previous tokens. Nevertheless, as context lengths and batch sizes
    increase, the linear expansion in memory footprint of KV caches becomes a key
    bottleneck of LLM deployment, which decreases generation speeds significantly.
    To mitigate this issue, previous techniques like multi-query attention (MQA) and
    grouped-query attention (GQA) have been developed, in order to reduce KV heads
    to accelerate inference with comparable accuracy to multi-head attention (MHA).
    Despite their effectiveness, existing strategies for compressing MHA often overlook
    the intrinsic properties of the KV caches. In this work, we explore the low-rank
    characteristics of the KV caches and propose a novel approach for compressing
    KV heads. In particular, we carefully optimize the MHA-to-GQA transformation to
    minimize compression error, and to remain compatible with rotary position embeddings
    (RoPE), we also introduce specialized strategies for key caches with RoPE. We
    demonstrate that our method can compress half or even three-quarters of KV heads
    while maintaining performance comparable to the original LLMs, which presents
    a promising direction for more efficient LLM deployment in resource-constrained
    environments.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练大型语言模型（LLMs）的出现彻底改变了各种自然语言处理任务。这些模型主要采用自回归解码机制，利用Key-Value (KV)缓存来消除对先前标记的冗余计算。然而，随着上下文长度和批量大小的增加，KV缓存的线性扩展在内存占用方面成为LLM部署的主要瓶颈，这显著降低了生成速度。为了缓解这一问题，之前开发了多查询注意力（MQA）和分组查询注意力（GQA）等技术，以减少KV头部，加快推理速度，同时保持与多头注意力（MHA）相当的准确性。尽管这些技术有效，但现有的MHA压缩策略通常忽视了KV缓存的内在特性。在这项工作中，我们探讨了KV缓存的低秩特性，并提出了一种压缩KV头部的新方法。特别地，我们仔细优化了MHA到GQA的转换，以最小化压缩误差，并为了与旋转位置嵌入（RoPE）兼容，我们还为RoPE的键缓存引入了专门的策略。我们展示了我们的方法可以在保持与原始LLMs性能相当的情况下，压缩一半甚至三分之二的KV头部，这为在资源有限的环境中更高效的LLM部署提供了一个有希望的方向。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In recent years, the emergence of pre-trained large language models (LLMs) [[16](#bib.bib16);
    [34](#bib.bib34); [39](#bib.bib39)] has been a cornerstone in redefining performance
    benchmarks across various natural language processing (NLP) tasks. These models
    frequently exhibit capabilities that are on par with human levels of comprehension
    and generation. In general, LLMs are built upon the neural structure of Transformers [[35](#bib.bib35)],
    which requires a computational cost quadratic to the input sequence’s length and
    makes long sequence inference intractable. To mitigate this issue, the auto-regressive
    decoding LLMs support Key-Value (KV) caches, i.e., to cache the previous context’s
    intermediate key and value states in memory. KV caches can avoid redundant computation
    of previous tokens, thereby expediting the inference process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，预训练的大型语言模型（LLMs）[[16](#bib.bib16); [34](#bib.bib34); [39](#bib.bib39)]的出现成为重新定义各种自然语言处理（NLP）任务性能基准的基石。这些模型经常展现出与人类理解和生成能力相当的性能。一般来说，LLMs是基于Transformer的神经结构[[35](#bib.bib35)]构建的，这需要与输入序列长度的平方成正比的计算成本，从而使得长序列推理变得不可行。为了解决这个问题，自回归解码LLMs支持Key-Value
    (KV)缓存，即将先前上下文的中间键值状态缓存到内存中。KV缓存可以避免重复计算先前的标记，从而加快推理过程。
- en: Nevertheless, all outputs of key and value weights for each block need to be
    cached during inference, thus KV cache parameters are often extremely high. As
    a result, the expansion of sequence lengths and batch sizes often causes a linear
    increase in memory footprint for past KV caches, further resulting in the decoding
    process during LLM inference being memory-bound [[19](#bib.bib19)]. Furthermore,
    the current trend to support longer contexts exacerbates this issue. Consequently,
    a sharp increase in KV caches can significantly slow down model inference and
    such a heavy memory footprint presents a key challenge in LLM deployments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，每个块的键和值权重的所有输出在推理过程中需要缓存，因此KV缓存参数通常非常高。因此，序列长度和批量大小的扩展通常导致过去KV缓存的内存占用线性增加，从而导致LLM推理过程中的解码过程被内存限制[[19](#bib.bib19)]。此外，目前支持更长上下文的趋势加剧了这一问题。因此，KV缓存的急剧增加可能会显著减慢模型推理速度，而如此大的内存占用在LLM部署中提出了一个关键挑战。
- en: To address these challenges, researchers proposed multi-query attention (MQA) [[30](#bib.bib30)]
    and grouped-query attention (GQA) [[1](#bib.bib1)], which use only one key-value
    head to correspond to all or multiple query heads, respectively. MQA and GQA can
    greatly reduce the sizes of KV caches during LLM inference, and achieve comparable
    results as the original MHA mechanism. In particular, to effectively reduce KV
    heads, the original GQA paper compares three compression strategies, i.e., randomly
    keeping several heads, specifying the first head for each group, and averaging
    KV heads in one group. They showed that directly mean-pooling KV head weights
    achieves the best accuracies. However, we find that this compression strategy
    ignores the inherent characteristics of KV caches, thus resulting in suboptimal
    model initialization. Therefore, all the training data and abundant GPU times
    are needed to fine-tune the compressed model after averaging the KV heads. This
    high computing resource requirement presents a huge challenge to compressing KV
    heads. As a direct consequence, attempts to compress KV heads of pre-trained LLMs
    remain rare, and researchers now prefer to train GQA models or compress KV caches
    directly.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，研究人员提出了多查询注意力（MQA）[[30](#bib.bib30)]和分组查询注意力（GQA）[[1](#bib.bib1)]，分别使用一个键值头来对应所有或多个查询头。MQA和GQA可以大幅减少LLM推理过程中的KV缓存大小，并且实现与原始MHA机制相当的结果。特别地，为了有效减少KV头，原始GQA论文比较了三种压缩策略，即随机保留若干头、为每组指定第一个头以及对一组中的KV头进行平均。他们表明，直接对KV头权重进行均值池化能获得最佳准确率。然而，我们发现这种压缩策略忽略了KV缓存的固有特性，从而导致了次优的模型初始化。因此，需要所有训练数据和大量GPU时间来在对KV头进行平均后微调压缩模型。这一高计算资源需求对KV头的压缩提出了巨大的挑战。因此，尝试压缩预训练LLM的KV头仍然很少，研究人员现在更倾向于训练GQA模型或直接压缩KV缓存。
- en: In this paper, we investigate the low-rank property of KV caches and our observations
    reveal that only a small number of top singular values need to be retained to
    keep most of the KV cache energy. Inspired by this finding, we propose to compress
    KV heads with low-rank decomposition. Our idea stems from a simple but crucial
    realization, i.e., when compressing a deep learning model, we should focus on
    minimizing the loss of the model outputs, rather than the model weights [[12](#bib.bib12)].
    We first group KV heads and perform SVD for each group of KV caches. Then we calculate
    low-rank approximations for those KV caches. In general, these low-rank compression
    weights can be incorporated into the original model to convert MHA into GQA pattern
    seamlessly. In addition, when LLM applies RoPE, the original method of weight
    fusing to reduce key heads will be invalid. To solve this issue, we propose several
    special strategies. In this case, although the compressed weights could not be
    incorporated into the original model, those key caches are successfully compressed
    and the generation speed of LLMs can still be improved.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们研究了KV缓存的低秩特性，我们的观察表明，仅需保留少量前几个奇异值即可保持大部分KV缓存能量。受这一发现的启发，我们提出了使用低秩分解来压缩KV头。我们的想法源于一个简单但关键的认识，即在压缩深度学习模型时，我们应该关注于最小化模型输出的损失，而不是模型权重[[12](#bib.bib12)]。我们首先对KV头进行分组，并对每组KV缓存执行SVD。然后，我们计算这些KV缓存的低秩近似。一般而言，这些低秩压缩权重可以被纳入原始模型中，以无缝地将MHA转换为GQA模式。此外，当LLM应用RoPE时，原始的权重融合方法以减少键头将失效。为解决此问题，我们提出了几种特殊策略。在这种情况下，尽管压缩后的权重不能纳入原始模型，但这些键缓存仍成功压缩，并且LLM的生成速度仍然可以得到提升。
- en: 'Compared with average pooling KV head or low-rank approximating head weights
    directly, our work can find a better initial weight for a compressed model, so
    KV caches can be effectively compressed and fewer training samples and computing
    resources are needed to restore the precision of the compressed model. We list
    our contributions as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接使用平均池化KV头或低秩近似头权重相比，我们的工作可以为压缩模型找到更好的初始权重，从而使KV缓存可以有效地压缩，并且恢复压缩模型的精度所需的训练样本和计算资源更少。我们的贡献如下：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We carefully study the characteristics of KV caches and prove the low-rank property
    of KV caches. That is, keeping only a small number of singular values in the KV
    caches can preserve most of the context information.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们仔细研究了KV缓存的特性，并证明了KV缓存的低秩特性。也就是说，只保留少量的奇异值在KV缓存中可以保留大部分上下文信息。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To effectively compress KV caches, we propose a novel framework to reduce KV
    heads, i.e., convert the original MHA into GQA pattern by low-rank decomposition.
    Besides, we also propose special strategies to deal with the attention layer with
    RoPE.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了有效压缩KV缓存，我们提出了一种新颖的框架来减少KV头，即通过低秩分解将原始MHA转换为GQA模式。此外，我们还提出了处理RoPE注意力层的特殊策略。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Extensive experiments have proved the effectiveness of our approach. On different
    LLM series models, our framework can compress half and even three-quarters of
    KV heads, and maintain comparable results as the original models, proving its
    wide applicability and high efficiency in different scenarios.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大量实验证明了我们方法的有效性。在不同的LLM系列模型上，我们的框架可以压缩一半甚至四分之三的KV头，并保持与原模型相当的结果，证明了它在不同场景中的广泛适用性和高效性。
- en: 2 Related Work
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Our work is connected to several themes in the literature, which we describe
    next.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作与文献中的几个主题相关，接下来我们将描述这些主题。
- en: 2.1 Large Language Models (LLMs)
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 大型语言模型（LLMs）
- en: Large language models [[3](#bib.bib3); [16](#bib.bib16); [34](#bib.bib34); [39](#bib.bib39);
    [7](#bib.bib7)] are designed to understand and generate human languages. In recent
    years LLMs have developed rapidly and consistently show excellent performances
    across various NLP tasks. These breakthroughs in performance can be partly attributed
    to the powerful modeling capabilities of its multi-head attention mechanism. To
    introduce positional information into attention, researchers have proposed various
    positional embeddings. In this paper, we will show that our framework can easily
    handle those models with different positional embeddings.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型[[3](#bib.bib3); [16](#bib.bib16); [34](#bib.bib34); [39](#bib.bib39);
    [7](#bib.bib7)]旨在理解和生成自然语言。近年来，LLMs迅速发展，并在各种NLP任务中 consistently 展现出卓越的表现。这些性能突破部分归功于其多头注意力机制的强大建模能力。为了将位置信息引入注意力机制，研究人员提出了各种位置嵌入。在本文中，我们将展示我们的框架可以轻松处理具有不同位置嵌入的模型。
- en: 2.2 Weights Compression for Transformers
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 Transformer的权重压缩
- en: In general, the common building block of LLM is transformer [[35](#bib.bib35)],
    which tends to have a large number of parameters and is computationally intensive.
    Weight compression technology can significantly reduce memory footprint and speed
    up inference. Therefore, there is a lot of research work trying to compress transformer
    models by various strategies, such as pruning, low-rank approximation, knowledge
    distillation, etc. Mickel et al. [[22](#bib.bib22)] found that only a few heads
    have a significant effect on translation tasks, and most heads can be pruned without
    any precision loss. AFM [[12](#bib.bib12)] low-rank decomposed fully-connected
    (FC) weights by PCA. GQA [[1](#bib.bib1)] introduced grouped-query attention and
    averaged KV head weights to convert a multi-head checkpoint into a multi-query
    checkpoint. LLM-Pruner [[37](#bib.bib37)] applied gradients to estimate the importance
    of model weights and pruned less significant coupled structures within the model
    based on this estimation. MiniLLM [[11](#bib.bib11)] proposed the reverse KL loss
    to distill LLMs into smaller language models. To the best of our knowledge, although
    weight compression is widespread, this is the first attempt to compress LLM’s
    KV heads with low-rank approximation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，LLM 的常见构建模块是 transformer [[35](#bib.bib35)]，它往往有大量的参数，并且计算密集。权重压缩技术可以显著减少内存占用并加快推理速度。因此，很多研究工作尝试通过各种策略压缩
    transformer 模型，例如剪枝、低秩近似、知识蒸馏等。Mickel 等 [[22](#bib.bib22)] 发现只有少数几个头对翻译任务有显著影响，大多数头可以在不损失精度的情况下进行剪枝。AFM
    [[12](#bib.bib12)] 通过 PCA 对全连接（FC）权重进行了低秩分解。GQA [[1](#bib.bib1)] 引入了分组查询注意力并平均了
    KV 头权重，以将多头检查点转换为多查询检查点。LLM-Pruner [[37](#bib.bib37)] 应用梯度来估计模型权重的重要性，并基于这种估计剪枝模型中不重要的耦合结构。MiniLLM
    [[11](#bib.bib11)] 提出了反向 KL 损失来将 LLM 蒸馏成更小的语言模型。据我们所知，尽管权重压缩已经很普遍，但这是首次尝试使用低秩近似来压缩
    LLM 的 KV 头。
- en: 2.3 KV Cache Optimization
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 KV 缓存优化
- en: KV cache compression is harder than weight compression since they are more sensitive
    and are related to model inputs. Some previous works tried to quantize KV caches.
    To achieve data-free distillation, LLM-QAT [[18](#bib.bib18)] leveraged generations
    by a pre-trained model and quantized KV caches. Some previous works dropped unimportant
    tokens with pre-designed criteria. H2O [[40](#bib.bib40)] and FastGen [[10](#bib.bib10)]
    utilized accumulated attention scores for token importance and effectively reduced
    the cache size by dropping tokens with lower scores. Scissorhands [[19](#bib.bib19)]
    found only some pivotal tokens have a substantial influence at one step and significantly
    influence future generations. Gear [[15](#bib.bib15)] first quantized KV caches
    and then employed a low-rank matrix to approximate the quantization error. Multi-head
    Latent Attention (MLA) [[7](#bib.bib7)] introduced low-rank key-value joint compression,
    which applied a latent vector to generate KV caches implicitly and required LLM
    to be trained from scratch, while our focus is on optimizing the existing MHA
    mechanism. The effort to compress attention heads remains scarce nowadays, and
    our work may potentially be combined with these previous efforts to achieve a
    higher compression ratio.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: KV 缓存压缩比权重压缩更困难，因为它们更加敏感且与模型输入相关。一些先前的工作尝试对 KV 缓存进行量化。为了实现无数据蒸馏，LLM-QAT [[18](#bib.bib18)]
    利用预训练模型生成的数据并对 KV 缓存进行量化。一些先前的工作根据预先设计的标准删除了不重要的令牌。H2O [[40](#bib.bib40)] 和 FastGen
    [[10](#bib.bib10)] 利用累积的注意力分数来评估令牌的重要性，通过删除低分数的令牌有效地减少了缓存的大小。Scissorhands [[19](#bib.bib19)]
    发现只有一些关键令牌在某一步有显著影响，并且对未来的生成有显著影响。Gear [[15](#bib.bib15)] 首先对 KV 缓存进行量化，然后采用低秩矩阵来近似量化误差。多头潜在注意力（MLA）[[7](#bib.bib7)]
    引入了低秩键值联合压缩，通过潜在向量隐式生成 KV 缓存，并要求 LLM 从头开始训练，而我们的重点是优化现有的 MHA 机制。目前压缩注意力头的努力仍然很少，我们的工作可能与这些先前的工作相结合，以实现更高的压缩比。
- en: 3 Methods
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 种方法
- en: We describe our framework in this section. First we introduce the traditional
    MHA & GQA algorithms, and the naive algorithm that converts MHA to GQA. Then we
    present our finding that KV caches generally emerge as a low-rank characteristic.
    Based on our finding, we present a KV head compression framework that uses SVD
    to low-rank approximate KV caches. In addition, we also present a comparison method
    that directly approximates KV head weights with SVD. Finally, we propose several
    special policies to deal with RoPE.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了我们的框架。首先，我们介绍传统的 MHA 和 GQA 算法，以及将 MHA 转换为 GQA 的简单算法。然后，我们提出了 KV 缓存通常表现为低秩特性的发现。基于我们的发现，我们提出了一个使用
    SVD 对 KV 缓存进行低秩近似的 KV 头压缩框架。此外，我们还提出了一种直接使用 SVD 近似 KV 头权重的比较方法。最后，我们提出了几种特殊策略来处理
    RoPE。
- en: 3.1 Preliminaries
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 初步介绍
- en: Transformer applies a multi-head attention (MHA) mechanism to capture different
    subspace representations inside the input sequence. Giving an input $x\in\mathbb{R}^{l\times
    d}$ heads, i.e.,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 应用多头注意力（MHA）机制来捕捉输入序列中的不同子空间表示。给定一个输入 $x\in\mathbb{R}^{l\times d}$
    头，即
- en: '|  |  | $\displaystyle\operatorname{MHA}(x)=[H_{1},\dots,H_{h}]W_{O},$ |  |
    (1) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\operatorname{MHA}(x)=[H_{1},\dots,H_{h}]W_{O},$ |  |
    (1) |'
- en: '|  |  | $\displaystyle H_{i}=\operatorname{Softmax}(Q_{i}K_{i}^{\top}/\sqrt{d_{h}})V_{i},$
    |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle H_{i}=\operatorname{Softmax}(Q_{i}K_{i}^{\top}/\sqrt{d_{h}})V_{i},$
    |  | (2) |'
- en: where $[\cdot]$ at every head and every layer are cached for subsequent generation,
    which results in the initial KV caches, i.e.,
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中每个头和每层的 $[\cdot]$ 被缓存以供后续生成，这导致了初始 KV 缓存，即
- en: '|  | $\displaystyle K^{(0)}$ |  | (3) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K^{(0)}$ |  | (3) |'
- en: '|  | $\displaystyle V^{(0)}$ |  | (4) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle V^{(0)}$ |  | (4) |'
- en: Here $K^{(0)},V^{(0)}\in\mathbb{R}^{l\times d}$. This strategy avoids the recalculation
    of previous tokens and significantly increases the generation speed. However,
    the sizes of KV caches can increase dramatically in long context inference, which
    results in speed degradation due to heavy memory bandwidth overhead.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $K^{(0)},V^{(0)}\in\mathbb{R}^{l\times d}$。该策略避免了重新计算以前的标记，并显著提高了生成速度。然而，在长上下文推断中，KV
    缓存的大小可能会显著增加，这会导致由于内存带宽开销过重而使速度下降。
- en: It can be seen that the sizes of KV caches are proportional to the number of
    KV heads. To effectively reduce their memory footprint, previous researchers proposed
    multi-query attention (MQA) [[30](#bib.bib30)] and grouped-query attention (GQA) [[1](#bib.bib1)].
    In the original GQA paper, to compress MHA into the GQA mechanism, they directly
    average key and value head weights in each group. $h$ matrices in GQA are calculated
    by
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看出，KV 缓存的大小与 KV 头的数量成正比。为了有效减少它们的内存占用，之前的研究者提出了多查询注意力（MQA）[[30](#bib.bib30)]和分组查询注意力（GQA）[[1](#bib.bib1)]。在原始
    GQA 论文中，为了将 MHA 压缩成 GQA 机制，他们直接对每组中的关键和价值头权重进行平均。GQA 中的 $h$ 矩阵通过以下公式计算
- en: '|  | $\displaystyle\tilde{W}_{K_{i}}$ |  | (5) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{W}_{K_{i}}$ |  | (5) |'
- en: '|  | $\displaystyle\tilde{W}_{V_{i}}$ |  | (6) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{W}_{V_{i}}$ |  | (6) |'
- en: where $i\in\{0,1,\cdots,g-1\}$. However, after analyzing KV caches, we identify
    that the current strategy is suboptimal as it overlooks the intrinsic characteristics
    of KV caches. Specifically, we have discovered that KV caches typically exhibit
    a low-rank property. Based on our findings, we propose a more efficient compression
    framework.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $i\in\{0,1,\cdots,g-1\}$。然而，在分析 KV 缓存后，我们发现当前策略是次优的，因为它忽略了 KV 缓存的内在特征。具体来说，我们发现
    KV 缓存通常表现出低秩特性。基于我们的发现，我们提出了一个更高效的压缩框架。
- en: 3.2 KV Caches are Low-rank!
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 KV 缓存是低秩的！
- en: To reveal the low-rank property of KV caches, we construct a small verification
    experiment, i.e., we evaluate LLaMA2-7B [[34](#bib.bib34)] on the C4 [[27](#bib.bib27)]
    training dataset. In particular, we sample 128 sequences from the C4 training
    set and each sample is 2048 tokens long. We perform model inference on LLaMA2
    and collect KV caches. KV cache sizes in each block are 262144$\times$4096\. Then
    we perform SVD on those caches.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了揭示 KV 缓存的低秩特性，我们构建了一个小型验证实验，即在 C4 [[27](#bib.bib27)] 训练数据集上评估 LLaMA2-7B [[34](#bib.bib34)]。具体来说，我们从
    C4 训练集中抽取 128 个序列，每个样本长度为 2048 个标记。我们在 LLaMA2 上进行模型推断并收集 KV 缓存。每个块中的 KV 缓存大小为
    262144$\times$4096。然后我们对这些缓存进行 SVD。
- en: Figure [1](#S3.F1 "Figure 1 ‣ 3.3 Effectively Transfer MHA to GQA ‣ 3 Methods
    ‣ Effectively Compress KV Heads for LLM") shows the percentage of the sum of these
    top singular values to the sum of all singular values when 25% and 50% of the
    highest singular values are retained. In particular, we report the rank of key
    cache that before (i.e., K Cache w/o RoPE) and after (i.e., K Cache w/ RoPE) performing
    RoPE. Two conclusions can be drawn from this experiment. First, only 25% of the
    highest singular values need to be retained to get most of the energy. Second,
    RoPE generally reduces the rank of key cache. Those phenomenons indicate that
    for LLMs, KV caches are likely to be low-rank. To ensure that most of the energy
    of KV caches is preserved, we only need to keep part of the output dimensions
    of key and value head weights.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1](#S3.F1 "图 1 ‣ 3.3 有效地将MHA转移到GQA ‣ 3 方法 ‣ 有效压缩LLM的KV头")显示了保留25%和50%最高奇异值时这些最高奇异值的总和占所有奇异值总和的百分比。特别地，我们报告了RoPE前（即K
    Cache w/o RoPE）和RoPE后（即K Cache w/ RoPE）关键缓存的秩。这个实验得出了两个结论。首先，仅需保留25%最高的奇异值即可获得大部分能量。其次，RoPE通常会降低关键缓存的秩。这些现象表明，对于LLM，KV缓存可能是低秩的。为了确保大部分KV缓存的能量被保留，我们只需保留部分关键和价值头权重的输出维度。
- en: 3.3 Effectively Transfer MHA to GQA
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 有效地将MHA转移到GQA
- en: '![Refer to caption](img/59d216a61ae1b649bd82b1aa43a2b3f7.png)![Refer to caption](img/f0daef4dba0638af6c838e5d16828aa9.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/59d216a61ae1b649bd82b1aa43a2b3f7.png)![参考说明](img/f0daef4dba0638af6c838e5d16828aa9.png)'
- en: 'Figure 1: Ratio of energy kept in each KV cache for LLaMA2-7B when 25% (left)
    and 50% (right) dimensions are retained. The $x$-axis is the block index.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：当保留25%（左）和50%（右）维度时，LLaMA2-7B中每个KV缓存中保留的能量比例。 $x$-轴是块索引。
- en: '![Refer to caption](img/527d526a361472c98b467fd108c8e801.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/527d526a361472c98b467fd108c8e801.png)'
- en: 'Figure 2: Illustration of compressing key heads into GQA pattern. Note that
    the strategy of compressing value heads is similar to this.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：将关键头压缩为GQA模式的示意图。请注意，压缩值头的策略与此类似。
- en: Now we present our compression algorithm. Inspired by PCA [[36](#bib.bib36)]
    and AFM [[12](#bib.bib12)], we compress KV caches by taking advantage of low-rank
    approximation and then incorporate compression weights into the model. We illustrate
    how to compress the key heads in Figure [2](#S3.F2 "Figure 2 ‣ 3.3 Effectively
    Transfer MHA to GQA ‣ 3 Methods ‣ Effectively Compress KV Heads for LLM"). The
    strategy of reducing value heads is similar to this, except that those compression
    weights are fused into the value matrices $W_{V}$.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们介绍我们的压缩算法。受到PCA [[36](#bib.bib36)] 和 AFM [[12](#bib.bib12)] 的启发，我们通过利用低秩近似来压缩KV缓存，然后将压缩权重融入模型中。我们在图[2](#S3.F2
    "图 2 ‣ 3.3 有效地将MHA转移到GQA ‣ 3 方法 ‣ 有效压缩LLM的KV头")中说明了如何压缩关键头。减少值头的策略类似，只是这些压缩权重被融合到值矩阵
    $W_{V}$ 中。
- en: In particular, our compression algorithm needs a pre-designed dataset to calculate
    compression weights. Given an MHA checkpoint with $h$, i.e.,
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们的压缩算法需要一个预设计的数据集来计算压缩权重。给定一个具有 $h$ 的MHA检查点，即，
- en: '|  | $\displaystyle\tilde{K}_{i}$ |  | (7) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{K}_{i}$ |  | (7) |'
- en: '|  | $\displaystyle\tilde{V}_{i}$ |  | (8) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{V}_{i}$ |  | (8) |'
- en: where $\tilde{K}_{i},\tilde{V}_{i}\in\mathbb{R}^{l\times td_{h}}$ as
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tilde{K}_{i},\tilde{V}_{i}\in\mathbb{R}^{l\times td_{h}}$。
- en: '|  | $\displaystyle\tilde{K}_{i}$ |  | (9) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{K}_{i}$ |  | (9) |'
- en: '|  | $\displaystyle\tilde{V}_{i}$ |  | (10) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{V}_{i}$ |  | (10) |'
- en: where $\Psi^{i}_{d_{h}},\Omega^{i}_{d_{h}}\in\mathbb{R}^{d_{h}\times td_{h}}$-th
    key and value matrices in GQA as
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Psi^{i}_{d_{h}},\Omega^{i}_{d_{h}}\in\mathbb{R}^{d_{h}\times td_{h}}$-th关键和价值矩阵在GQA中为
- en: '|  | $\displaystyle\tilde{W}_{K_{i}}$ |  | (11) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{W}_{K_{i}}$ |  | (11) |'
- en: '|  | $\displaystyle\tilde{W}_{V_{i}}$ |  | (12) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{W}_{V_{i}}$ |  | (12) |'
- en: where $\tilde{W}_{K_{i}},\tilde{W}_{V_{i}}\in\mathbb{R}^{d\times d_{h}}$, then
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tilde{W}_{K_{i}},\tilde{W}_{V_{i}}\in\mathbb{R}^{d\times d_{h}}$，然后
- en: '|  | $\displaystyle\tilde{W}_{Q_{i}}$ |  | (13) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{W}_{Q_{i}}$ |  | (13) |'
- en: '|  | $\displaystyle\tilde{W}_{O_{i}}$ |  | (14) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{W}_{O_{i}}$ |  | (14) |'
- en: where $\Psi^{p}_{d_{h},q},\Omega^{p}_{d_{h},q}\in\mathbb{R}^{d_{h}\times d_{h}}$,
    respectively. We name this method SVD-*a*, as it performs SVD on the output activations.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Psi^{p}_{d_{h},q},\Omega^{p}_{d_{h},q}\in\mathbb{R}^{d_{h}\times d_{h}}$。我们将这种方法称为SVD-*a*，因为它对输出激活进行SVD。
- en: Except for this strategy which compresses KV heads with calibration sets, another
    low-rank compression strategy that does not require data is to directly perform
    SVD on KV weights, i.e.,
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这种使用校准集压缩KV头的策略外，另一种不需要数据的低秩压缩策略是直接对KV权重进行SVD，即，
- en: '|  | $\displaystyle\hat{W}_{K_{i}}$ |  | (15) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{W}_{K_{i}}$ |  | (15) |'
- en: '|  | $\displaystyle\hat{W}_{V_{i}}$ |  | (16) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{W}_{V_{i}}$ |  | (16) |'
- en: Then we perform SVD on $\hat{W}_{K_{i}}$. We treat this strategy as a baseline
    method and name it SVD-*w*, as it low-rank decomposes model weight. Later we will
    show that this SVD-*w* is not as effective as our approach that uses KV caches’
    low-rank characteristic.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对$\hat{W}_{K_{i}}$执行SVD。我们将这一策略视为基准方法，命名为SVD-*w*，因为它低秩分解了模型权重。稍后我们将展示，SVD-*w*的效果不如我们使用KV缓存低秩特性的方案。
- en: After obtaining those compressed models, we use LoRA [[14](#bib.bib14)] to fine-tune
    them. We will show that by removing half or even three-quarters of KV heads, our
    compressed LLMs can still achieve comparable accuracies as the original model.
    Note that our compression method is orthogonal to the fine-tuning strategy. If
    more computing resources are available, full-parameter fine-tuning is also a viable
    strategy to potentially achieve better accuracy.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得这些压缩模型后，我们使用LoRA [[14](#bib.bib14)]对它们进行微调。我们将展示，通过去除一半甚至四分之三的KV头，我们的压缩LLMs仍然可以实现与原始模型相当的准确度。请注意，我们的压缩方法与微调策略是正交的。如果有更多计算资源，全面参数微调也是一种可行的策略，可能实现更好的准确度。
- en: 3.4 Deal with RoPE
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 处理RoPE
- en: There is a special case when LLMs adopt RoPE [[32](#bib.bib32)], which inserts
    a relative position embedding between $W_{Q}$. At this time, the attention score
    calculation becomes
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当LLMs采用RoPE [[32](#bib.bib32)]时，存在一种特殊情况，它在$W_{Q}$之间插入了相对位置嵌入。此时，注意力分数计算变为
- en: '|  | $\displaystyle q_{m}k_{n}^{\top}=(x_{m}W_{q}R_{\theta,m}^{d})(x_{n}W_{k}R_{\theta,n}^{d})^{\top},$
    |  | (17) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q_{m}k_{n}^{\top}=(x_{m}W_{q}R_{\theta,m}^{d})(x_{n}W_{k}R_{\theta,n}^{d})^{\top},$
    |  | (17) |'
- en: where $x_{i}\in\mathbb{R}^{1\times d}$.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$x_{i}\in\mathbb{R}^{1\times d}$。
- en: To deal with this difficulty, we come up with several strategies. First, we
    do not divide key heads but directly calculate $\Psi$ during inference. Although
    the compressed model is no longer in the GQA pattern, key cache compression is
    achieved. Therefore, the original calculation becomes
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一困难，我们提出了几种策略。首先，我们不分割关键头，而是在推理过程中直接计算$\Psi$。尽管压缩模型不再采用GQA模式，但实现了关键缓存压缩。因此，原始计算变成了
- en: '|  | $1$2 |  | (18) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (18) |'
- en: In particular, we will calculate $(x_{m}W_{q}R_{\theta,m}^{d})\Psi_{d_{h}}^{\top}$
    only contains one token during the generation process, but key cache often has
    many tokens. RoPE does not involve value caches, so the compression strategy for
    value caches remains the same. Later our experiments will prove that all our three
    strategies can reduce KV caches with faster generation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将计算$(x_{m}W_{q}R_{\theta,m}^{d})\Psi_{d_{h}}^{\top}$在生成过程中仅包含一个token，但关键缓存通常包含多个token。RoPE不涉及值缓存，因此值缓存的压缩策略保持不变。稍后的实验将证明，我们的三种策略都能加快生成速度并减少KV缓存。
- en: 4 Experiment
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: We now evaluate our methods in this section. More results can be found in the
    appendix.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在本节中评估我们的方法。更多结果可以在附录中找到。
- en: 4.1 Settings
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: Foundation models. We evaluate our framework on the 7B1 model of BLOOMZ [[24](#bib.bib24)],
    and the 7B, 13B models of LLaMA2 [[34](#bib.bib34)]. BLOOMZ is BLOOM’s [[16](#bib.bib16)]
    supervised fine-tuning (SFT) version using the open-sourced xP3 [[24](#bib.bib24)]
    dataset.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型。我们在BLOOMZ [[24](#bib.bib24)]的7B1模型和LLaMA2 [[34](#bib.bib34)]的7B、13B模型上评估了我们的框架。BLOOMZ是BLOOM
    [[16](#bib.bib16)]的监督微调（SFT）版本，使用开源的xP3 [[24](#bib.bib24)]数据集。
- en: Compression. For BLOOMZ-7B1, we sampled 1‰ data from the xP3 dataset. The original
    xP3 includes 83.6M samples, thus we randomly sampled 83.6K sampled from it. For
    LLaMA2 models, following QLoRA [[8](#bib.bib8)], we use the FLANv2 [[20](#bib.bib20)]
    dataset and extract 23K data from it. We concatenate the input and output of each
    sample for inference, and then collect KV caches in each block to calculate compression
    weights.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩。对于BLOOMZ-7B1，我们从xP3数据集中抽样了1‰数据。原始xP3包括83.6M样本，因此我们随机抽取了83.6K样本。对于LLaMA2模型，参照QLoRA
    [[8](#bib.bib8)]，我们使用FLANv2 [[20](#bib.bib20)]数据集，并从中提取了23K数据。我们将每个样本的输入和输出连接在一起进行推理，然后在每个块中收集KV缓存以计算压缩权重。
- en: Here we reduce half and three-quarters KV heads. The original BLOOMZ-7B1 and
    LLaMA2-7B have MHA structures that contain 32 heads, and we compress all attention
    layers in the models to 16 or 8 heads. For BLOOMZ-7B1, we continue to reduce KV
    heads into 4\. LLaMA2-13B contains 40 heads and we reduce them to 20 or 10 heads.
    Note that BLOOMZ uses ALiBi [[26](#bib.bib26)] and does not involve RoPE so it
    is a standard GQA mechanism after compression.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们将KV头减少了一半和四分之三。原始的BLOOMZ-7B1和LLaMA2-7B具有包含32个头的MHA结构，我们将模型中的所有注意力层压缩为16或8个头。对于BLOOMZ-7B1，我们继续将KV头减少为4。LLaMA2-13B包含40个头，我们将其减少为20或10个头。请注意，BLOOMZ使用ALiBi [[26](#bib.bib26)]，不涉及RoPE，因此在压缩后它是一个标准的GQA机制。
- en: Fine-tuning. We apply LoRA [[14](#bib.bib14)] to fine-tune the compressed model.
    For 7B size models, we set LoRA’s $r=256$80G A100 GPU to fine-tune the compressed
    model and set batch size per device as 1\. We set the gradient accumulation steps
    as 16 and the LoRA dropout rate as 0.05\. Weight decay is 0.05 and AdamW [[21](#bib.bib21)]
    is used. Note for the BLOOMZ-7B1 model, we continue to apply the same 83.6K sub-dataset
    to fine-tune the compressed model. For LLaMA2 models, we select 232K data from
    FLANv2 [[20](#bib.bib20)] as the training dataset.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 微调。我们应用LoRA [[14](#bib.bib14)]来微调压缩模型。对于7B大小的模型，我们设置LoRA的$r=256$，在80G A100 GPU上微调压缩模型，并将每个设备的批量大小设置为1。我们将梯度累积步数设置为16，LoRA的丢弃率为0.05。权重衰减为0.05，使用AdamW [[21](#bib.bib21)]。请注意，对于BLOOMZ-7B1模型，我们继续应用相同的83.6K子数据集来微调压缩模型。对于LLaMA2模型，我们选择了FLANv2 [[20](#bib.bib20)]中的232K数据作为训练数据集。
- en: Evaluation metrics. For BLOOMZ-7B1, we follow the evaluation metrics of the
    original paper, i.e., we evaluate the zero-shot accuracy on XCOPA [[25](#bib.bib25)],
    XNLI [[6](#bib.bib6)], XWinoGrad [[33](#bib.bib33)] and XStoryCloze [[17](#bib.bib17)]
    with Bulgarian (BG), German (DE), Greek (EL), Russian(RU), Thai (TH), Turkish
    (TR), Japanese (JP), Estonian (ET), Haitian (HT), Italian (IT), Quechua (QU) and
    Burmese (MY) language questions and English prompts. For LLaMA2 models, we evaluate
    the perplexity on WikiText2 [[31](#bib.bib31)] and C4 [[27](#bib.bib27)]. We further
    assess the zero-shot commonsense question answering (QA) ability on tasks covering
    SIQA [[29](#bib.bib29)], HellaSwag [[38](#bib.bib38)], PIQA [[2](#bib.bib2)],
    WinoGrande [[28](#bib.bib28)], ARC [[5](#bib.bib5)], BoolQ [[4](#bib.bib4)], and
    OpenBookQA [[23](#bib.bib23)]. We also evaluate both the zero-shot and five-shot
    performance of the LLMs on the Massively Multitask Language Understanding (MMLU)
    benchmark [[13](#bib.bib13)]. It consists of 57 language tasks including humanities,
    STEM, social science, etc. We adopt lm-eval-harness [[9](#bib.bib9)] to produce
    the accuracy results. Besides, we also report throughput in an 80G A100 GPU. Since
    the prefilling stage is computation-bound while the decoding stage is memory-bound,
    and here our goal is to compress KV caches, we set the context length to 2048
    and calculate throughput when generating the 2050th token for simplicity.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。对于BLOOMZ-7B1，我们遵循原论文的评估指标，即，我们在XCOPA [[25](#bib.bib25)]、XNLI [[6](#bib.bib6)]、XWinoGrad [[33](#bib.bib33)]和XStoryCloze [[17](#bib.bib17)]上评估零-shot准确率，涵盖保加利亚语
    (BG)、德语 (DE)、希腊语 (EL)、俄语 (RU)、泰语 (TH)、土耳其语 (TR)、日语 (JP)、爱沙尼亚语 (ET)、海地克里奥尔语 (HT)、意大利语
    (IT)、凯丘亚语 (QU) 和缅甸语 (MY) 的问题以及英语提示。对于LLaMA2模型，我们评估WikiText2 [[31](#bib.bib31)]和C4 [[27](#bib.bib27)]上的困惑度。我们进一步评估零-shot常识问答
    (QA) 能力，涵盖SIQA [[29](#bib.bib29)]、HellaSwag [[38](#bib.bib38)]、PIQA [[2](#bib.bib2)]、WinoGrande [[28](#bib.bib28)]、ARC [[5](#bib.bib5)]、BoolQ [[4](#bib.bib4)]
    和OpenBookQA [[23](#bib.bib23)]任务。我们还评估了LLMs在大规模多任务语言理解 (MMLU) 基准 [[13](#bib.bib13)]上的零-shot和五-shot性能。该基准包括57个语言任务，包括人文、STEM、社会科学等。我们采用lm-eval-harness [[9](#bib.bib9)]来生成准确率结果。此外，我们还报告了在80G
    A100 GPU上的吞吐量。由于预填充阶段受计算限制，而解码阶段受内存限制，并且我们的目标是压缩KV缓存，我们将上下文长度设置为2048，并在生成第2050个标记时计算吞吐量以简化处理。
- en: 4.2 Main Results
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主要结果
- en: We report the number (#) of KV heads, throughput, and accuracies of BLOOMZ-7B1
    and LLaMA2 in Tables [1](#S4.T1 "Table 1 ‣ 4.2 Main Results ‣ 4 Experiment ‣ Effectively
    Compress KV Heads for LLM") and [2](#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiment
    ‣ Effectively Compress KV Heads for LLM"), respectively. Further accuracies of
    eight zero-shot commonsense question answering datasets are shown in Table [3](#S4.T3
    "Table 3 ‣ 4.2 Main Results ‣ 4 Experiment ‣ Effectively Compress KV Heads for
    LLM"). Note that for BLOOMZ-7B1, the ‘AVG.’ column is not the direct average accuracy
    of the four datasets, as each dataset contains a different number of sub-datasets.
    Therefore, we report the average accuracy of those sub-datasets. Detailed results
    are shown in the appendix. For LLaMA2 models, we abbreviate WikiText2, HellaSwag,
    WinoGrande, and OpenBookQA to W2, HLSW, WG, and OBQA, respectively. ARC-e and
    ARC-c stand for ARC-easy and ARC-challenge tasks, respectively.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[1](#S4.T1 "表 1 ‣ 4.2 主要结果 ‣ 4 实验 ‣ 有效压缩 KV 头")和[2](#S4.T2 "表 2 ‣ 4.2 主要结果
    ‣ 4 实验 ‣ 有效压缩 KV 头")中报告了 BLOOMZ-7B1 和 LLaMA2 的 KV 头数量 (#)、吞吐量和准确率。进一步的八个零样本常识问答数据集的准确率显示在表[3](#S4.T3
    "表 3 ‣ 4.2 主要结果 ‣ 4 实验 ‣ 有效压缩 KV 头")中。注意，对于 BLOOMZ-7B1，“平均值”列不是四个数据集的直接平均准确率，因为每个数据集包含不同数量的子数据集。因此，我们报告了这些子数据集的平均准确率。详细结果见附录。对于
    LLaMA2 模型，我们将 WikiText2、HellaSwag、WinoGrande 和 OpenBookQA 简写为 W2、HLSW、WG 和 OBQA。ARC-e
    和 ARC-c 分别代表 ARC-easy 和 ARC-challenge 任务。
- en: 'Table 1: Performances of BLOOMZ-7B1 with our KV heads compression framework.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 使用我们的 KV 头压缩框架的 BLOOMZ-7B1 性能。'
- en: '| #KV Heads | Throughput (token/s) | Accuracy | AVG. |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| #KV 头 | 吞吐量 (token/s) | 准确率 | 平均值 |'
- en: '| XNLI | XWinoGrad | XCOPA | XStoryCloze |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| XNLI | XWinoGrad | XCOPA | XStoryCloze |'
- en: '| 32 | 8.56 | 39.73 | 51.49 | 51.03 | 54.25 | 47.25 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 8.56 | 39.73 | 51.49 | 51.03 | 54.25 | 47.25 |'
- en: '| 16 | 14.41 | 39.48 | 51.47 | 52.97 | 54.06 | 47.85 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 14.41 | 39.48 | 51.47 | 52.97 | 54.06 | 47.85 |'
- en: '| 8 | 23.24 | 39.03 | 50.45 | 51.60 | 52.37 | 46.84 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 23.24 | 39.03 | 50.45 | 51.60 | 52.37 | 46.84 |'
- en: '| 4 | 34.78 | 38.45 | 50.29 | 50.27 | 50.89 | 45.92 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 34.78 | 38.45 | 50.29 | 50.27 | 50.89 | 45.92 |'
- en: 'Table 2: Performances of LLaMA2 models with our KV heads compression framework.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 使用我们的 KV 头压缩框架的 LLaMA2 模型性能。'
- en: '| Model | #KV Heads | Throughput (token/s) | Perplexity ($\downarrow$) | MMLU
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | #KV 头 | 吞吐量 (token/s) | 困惑度 ($\downarrow$) | MMLU |'
- en: '| W2 | C4 | 0-shot | 5-shot |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| W2 | C4 | 0-shot | 5-shot |'
- en: '| LLaMA2-7B | 32 | 8.05 | 5.47 | 6.98 | 41.79 | 45.82 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | 32 | 8.05 | 5.47 | 6.98 | 41.79 | 45.82 |'
- en: '| 16 | 13.41 | 7.08 | 9.12 | 48.32 | 48.74 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 13.41 | 7.08 | 9.12 | 48.32 | 48.74 |'
- en: '| 8 | 20.81 | 9.17 | 11.24 | 44.62 | 45.54 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 20.81 | 9.17 | 11.24 | 44.62 | 45.54 |'
- en: '| LLaMA2-13B | 40 | 5.04 | 4.88 | 6.47 | 52.12 | 55.17 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B | 40 | 5.04 | 4.88 | 6.47 | 52.12 | 55.17 |'
- en: '| 20 | 8.60 | 6.51 | 8.01 | 53.65 | 54.71 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 8.60 | 6.51 | 8.01 | 53.65 | 54.71 |'
- en: '| 10 | 13.93 | 8.40 | 9.86 | 49.65 | 50.73 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 13.93 | 8.40 | 9.86 | 49.65 | 50.73 |'
- en: 'Table 3: Accuracies in the commonsense QA datasets with different #KV heads
    on LLaMA2 models.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: LLaMA2 模型在不同 #KV 头下的常识问答数据集准确率。'
- en: '| Model | #KV Heads | BoolQ | PIQA | SIQA | HLSW | WG | ARC-e | ARC-c | OBQA
    | Avg. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | #KV 头 | BoolQ | PIQA | SIQA | HLSW | WG | ARC-e | ARC-c | OBQA | 平均值
    |'
- en: '| LLaMA2-7B | 32 | 77.77 | 79.05 | 32.91 | 76.00 | 69.22 | 74.58 | 46.25 |
    44.20 | 62.50 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | 32 | 77.77 | 79.05 | 32.91 | 76.00 | 69.22 | 74.58 | 46.25 |
    44.20 | 62.50 |'
- en: '| 16 | 83.61 | 77.97 | 32.80 | 72.53 | 73.56 | 78.45 | 49.66 | 46.40 | 64.37
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 83.61 | 77.97 | 32.80 | 72.53 | 73.56 | 78.45 | 49.66 | 46.40 | 64.37
    |'
- en: '| 8 | 80.58 | 76.77 | 32.91 | 66.67 | 67.88 | 73.32 | 43.34 | 43.80 | 60.66
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 80.58 | 76.77 | 32.91 | 66.67 | 67.88 | 73.32 | 43.34 | 43.80 | 60.66
    |'
- en: '| LLaMA2-13B | 40 | 80.61 | 80.52 | 33.11 | 79.38 | 72.30 | 77.40 | 49.06 |
    45.20 | 64.70 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B | 40 | 80.61 | 80.52 | 33.11 | 79.38 | 72.30 | 77.40 | 49.06 |
    45.20 | 64.70 |'
- en: '| 20 | 85.78 | 80.41 | 32.04 | 77.84 | 74.59 | 80.22 | 54.35 | 46.80 | 66.50
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 85.78 | 80.41 | 32.04 | 77.84 | 74.59 | 80.22 | 54.35 | 46.80 | 66.50
    |'
- en: '| 10 | 83.24 | 78.45 | 33.11 | 73.00 | 70.80 | 75.97 | 49.06 | 41.80 | 63.18
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 83.24 | 78.45 | 33.11 | 73.00 | 70.80 | 75.97 | 49.06 | 41.80 | 63.18
    |'
- en: As these results exhibited, when compressing half of KV heads, our algorithm
    can maintain the same or even higher accuracy, while guaranteeing more than 50%
    throughput improvement at the decoding process. This phenomenon indicates that
    our framework is a powerful solution in scenarios where memory efficiency is required.
    When further compressing three-quarters of KV heads, BLOOM-7B1 and LLaMA2-13B
    lost more KV cache information, resulting in a slight accuracy drop. However,
    those results are still comparable with the original models’ accuracies. These
    results demonstrate that our approach is an effective strategy to compress KV
    heads and reduce KV cache sizes, thereby alleviating the heavy memory bandwidth
    pressure during the LLM generation phase.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如这些结果所示，当压缩一半的 KV 头时，我们的算法可以保持相同甚至更高的准确度，同时在解码过程中保证了超过 50% 的吞吐量提升。这一现象表明我们的框架在需要内存效率的场景中是一种强大的解决方案。当进一步压缩四分之三的
    KV 头时，BLOOM-7B1 和 LLaMA2-13B 丧失了更多的 KV 缓存信息，导致了轻微的准确度下降。然而，这些结果仍与原始模型的准确度相当。这些结果证明我们的方法是压缩
    KV 头和减少 KV 缓存大小的有效策略，从而减轻 LLM 生成阶段的内存带宽压力。
- en: 4.3 Ablation Studies
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: We further perform several analyses to explore the impact of different modules
    of our method.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步进行了若干分析，以探讨我们方法中不同模块的影响。
- en: 'Table 4: Performances of LLaMA2 models with different initialization strategies.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 不同初始化策略下的 LLaMA2 模型表现。'
- en: '| #KV Heads | Methods | Stage | Perplexity ($\downarrow$) | MMLU | QA AVG.
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| #KV Heads | 方法 | 阶段 | 困惑度 ($\downarrow$) | MMLU | QA 平均值 |'
- en: '| W2 | C4 | 0-shot | 5-shot |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| W2 | C4 | 0-shot | 5-shot |'
- en: '| 16 | Mean-pool | Initialization | 1079.43 | 625.68 | 22.94 | 24.94 | 34.88
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 均值池化 | 初始化 | 1079.43 | 625.68 | 22.94 | 24.94 | 34.88 |'
- en: '| Fine-tune | 1079.43 | 625.68 | 35.25 | 35.41 | 56.57 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 1079.43 | 625.68 | 35.25 | 35.41 | 56.57 |'
- en: '| SVD-*w* | Initialization | 807.62 | 624.78 | 23.11 | 23.17 | 35.46 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| SVD-*w* | 初始化 | 807.62 | 624.78 | 23.11 | 23.17 | 35.46 |'
- en: '| Fine-tune | 10.44 | 12.28 | 42.46 | 39.38 | 62.10 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 10.44 | 12.28 | 42.46 | 39.38 | 62.10 |'
- en: '| SVD-*a* | Initialization | 13.57 | 18.57 | 28.05 | 25.97 | 48.87 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| SVD-*a* | 初始化 | 13.57 | 18.57 | 28.05 | 25.97 | 48.87 |'
- en: '| Fine-tune | 7.08 | 9.12 | 48.32 | 48.74 | 64.37 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 7.08 | 9.12 | 48.32 | 48.74 | 64.37 |'
- en: '| 8 | Mean-pool | Initialization | 4113.79 | 2381.27 | 25.00 | 24.95 | 34.69
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 均值池化 | 初始化 | 4113.79 | 2381.27 | 25.00 | 24.95 | 34.69 |'
- en: '| Fine-tune | 63.32 | 37.87 | 26.63 | 25.30 | 40.79 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 63.32 | 37.87 | 26.63 | 25.30 | 40.79 |'
- en: '| SVD-*w* | Initialization | 3888.63 | 2350.09 | 23.42 | 22.94 | 35.43 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| SVD-*w* | 初始化 | 3888.63 | 2350.09 | 23.42 | 22.94 | 35.43 |'
- en: '| Fine-tune | 15.44 | 17.13 | 24.30 | 23.69 | 55.86 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 15.44 | 17.13 | 24.30 | 23.69 | 55.86 |'
- en: '| SVD-*a* | Initialization | 194.61 | 141.84 | 23.61 | 23.93 | 37.59 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| SVD-*a* | 初始化 | 194.61 | 141.84 | 23.61 | 23.93 | 37.59 |'
- en: '| Fine-tune | 9.17 | 11.24 | 44.62 | 45.54 | 60.66 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 9.17 | 11.24 | 44.62 | 45.54 | 60.66 |'
- en: 'Table 5: Performances of LLaMA2 models with different initialization data sizes.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 不同初始化数据规模下的 LLaMA2 模型表现。'
- en: '| #KV Heads | Data | Perplexity ($\downarrow$) | MMLU | QA AVG. |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| #KV Heads | 数据 | 困惑度 ($\downarrow$) | MMLU | QA 平均值 |'
- en: '| W2 | C4 | 0-shot | 5-shot |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| W2 | C4 | 0-shot | 5-shot |'
- en: '| 16 | 23K | 13.57 | 18.57 | 28.05 | 25.97 | 48.87 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 23K | 13.57 | 18.57 | 28.05 | 25.97 | 48.87 |'
- en: '| 46K | 13.83 | 18.89 | 27.96 | 25.89 | 48.50 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 46K | 13.83 | 18.89 | 27.96 | 25.89 | 48.50 |'
- en: '| 92K | 13.93 | 18.96 | 28.11 | 25.98 | 48.66 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 92K | 13.93 | 18.96 | 28.11 | 25.98 | 48.66 |'
- en: '|  | 232K | 13.92 | 18.93 | 27.89 | 25.87 | 48.70 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | 232K | 13.92 | 18.93 | 27.89 | 25.87 | 48.70 |'
- en: '| 8 | 23K | 194.61 | 141.84 | 23.61 | 23.93 | 37.59 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 23K | 194.61 | 141.84 | 23.61 | 23.93 | 37.59 |'
- en: '| 46K | 197.23 | 142.80 | 23.76 | 23.81 | 37.46 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 46K | 197.23 | 142.80 | 23.76 | 23.81 | 37.46 |'
- en: '| 92K | 202.05 | 145.44 | 23.62 | 23.79 | 37.34 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 92K | 202.05 | 145.44 | 23.62 | 23.79 | 37.34 |'
- en: '| 232K | 202.38 | 145.43 | 23.74 | 24.01 | 37.48 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 232K | 202.38 | 145.43 | 23.74 | 24.01 | 37.48 |'
- en: 'Table 6: Performances of LLaMA2 models with different fine-tuning data sizes.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 不同微调数据规模下的 LLaMA2 模型表现。'
- en: '| #KV Heads | Data | Perplexity ($\downarrow$) | MMLU | QA AVG. |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| #KV Heads | 数据 | 困惑度 ($\downarrow$) | MMLU | QA 平均值 |'
- en: '| W2 | C4 | 0-shot | 5-shot |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| W2 | C4 | 0-shot | 5-shot |'
- en: '| 16 | 23K | 7.29 | 9.41 | 43.49 | 44.16 | 61.93 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 23K | 7.29 | 9.41 | 43.49 | 44.16 | 61.93 |'
- en: '| 46K | 7.38 | 9.35 | 42.42 | 44.62 | 61.53 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 46K | 7.38 | 9.35 | 42.42 | 44.62 | 61.53 |'
- en: '| 92K | 7.26 | 9.20 | 45.00 | 46.36 | 62.57 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 92K | 7.26 | 9.20 | 45.00 | 46.36 | 62.57 |'
- en: '| 232K | 7.08 | 9.12 | 48.32 | 48.74 | 64.37 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 232K | 7.08 | 9.12 | 48.32 | 48.74 | 64.37 |'
- en: '| 8 | 23K | 9.62 | 12.11 | 32.71 | 35.37 | 57.13 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 23K | 9.62 | 12.11 | 32.71 | 35.37 | 57.13 |'
- en: '| 46K | 9.48 | 11.84 | 39.93 | 40.10 | 57.53 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 46K | 9.48 | 11.84 | 39.93 | 40.10 | 57.53 |'
- en: '| 92K | 9.73 | 11.50 | 40.53 | 42.90 | 59.04 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 92K | 9.73 | 11.50 | 40.53 | 42.90 | 59.04 |'
- en: '| 232K | 9.17 | 11.24 | 44.62 | 45.54 | 60.66 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 232K | 9.17 | 11.24 | 44.62 | 45.54 | 60.66 |'
- en: Comparing different compression strategies. Here we compare three different
    compression strategies, namely directly mean-pooling KV head weights [[1](#bib.bib1)],
    performing SVD for KV head weights (i.e., SVD-*w*), and our methods (i.e., SVD-*a*).
    All other settings are consistent except for the compression strategy. We apply
    LLaMA2-7B with 16 and 8 KV heads and report the results of direct compression
    (i.e., the ‘Initialization’ rows) and further fine-tuning (i.e., the ‘Fine-tune’
    rows). Due to page limitation, we report the average accuracy of eight zero-shot
    commonsense QA datasets here (i.e., the ‘QA AVG.’ column).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 比较不同的压缩策略。在这里我们比较了三种不同的压缩策略，即直接均值池化KV头权重[[1](#bib.bib1)]，对KV头权重进行SVD（即SVD-*w*），以及我们的方法（即SVD-*a*）。除了压缩策略外，所有其他设置都保持一致。我们应用16和8个KV头的LLaMA2-7B，并报告直接压缩（即“Initialization”行）和进一步微调（即“Fine-tune”行）的结果。由于页面限制，我们在这里报告了八个零样本常识QA数据集的平均准确率（即“QA
    AVG.”列）。
- en: As we can see from Table [4](#S4.T4 "Table 4 ‣ 4.3 Ablation Studies ‣ 4 Experiment
    ‣ Effectively Compress KV Heads for LLM"), both mean-pooling and SVD-*w* lead
    to collapse before fine-tuning, and our method far exceeds them both before &
    after fine-tuning. Those results reveal the effectiveness of our algorithm. That
    is, with limited computing and data resources, our SVD-*a* can significantly reduce
    KV cache sizes and are very practical, while other strategies are not.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从表[4](#S4.T4 "Table 4 ‣ 4.3 Ablation Studies ‣ 4 Experiment ‣ Effectively Compress
    KV Heads for LLM")可以看出，无论是均值池化还是SVD-*w*都在微调之前发生了崩溃，而我们的方法在微调前后都远超它们。这些结果揭示了我们算法的有效性。也就是说，在有限的计算和数据资源下，我们的SVD-*a*可以显著减少KV缓存的大小，并且非常实用，而其他策略则不具备这一优势。
- en: Influence of different data sizes for initialization and fine-tuning. Here we
    investigate the effect of the size of different data sets. Similarly, we apply
    LLaMA2-7B with 16 and 8 KV heads as the baseline. In particular, in our original
    settings, we calculate the compression weights with 23K samples for initialization
    and fine-tune the compression model with 232K samples. Here we sample 46K and
    92K data from the original FLANv2 dataset and calculate the compression weights.
    Then based on the model compressing with 23K samples, we continue to fine-tune
    the model with 23K, 46K, 92K, and 232K samples respectively.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 不同数据规模对初始化和微调的影响。在这里我们调查了不同数据集大小的效果。类似地，我们使用16和8个KV头的LLaMA2-7B作为基线。特别地，在我们的原始设置中，我们使用23K样本计算压缩权重，并用232K样本微调压缩模型。在这里，我们从原始的FLANv2数据集中抽取了46K和92K的数据，并计算了压缩权重。然后基于用23K样本压缩的模型，我们继续分别用23K、46K、92K和232K样本微调模型。
- en: Results of different dataset sizes on model initialization and fine-tuning are
    shown in Tables [5](#S4.T5 "Table 5 ‣ 4.3 Ablation Studies ‣ 4 Experiment ‣ Effectively
    Compress KV Heads for LLM") and [6](#S4.T6 "Table 6 ‣ 4.3 Ablation Studies ‣ 4
    Experiment ‣ Effectively Compress KV Heads for LLM"), respectively. As we can
    see, the data size required for initialization is not high, while the fine-tuning
    process is relatively more data-hungry. During initialization, excess data does
    not necessarily yield additional benefits. However, the more data for fine-tuning,
    the better the compression model will perform. This phenomenon reveals that our
    method can further achieve higher precision if there are more data and computational
    resources.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 不同数据集大小对模型初始化和微调的结果分别展示在表格[5](#S4.T5 "Table 5 ‣ 4.3 Ablation Studies ‣ 4 Experiment
    ‣ Effectively Compress KV Heads for LLM")和[6](#S4.T6 "Table 6 ‣ 4.3 Ablation Studies
    ‣ 4 Experiment ‣ Effectively Compress KV Heads for LLM")中。我们可以看到，初始化所需的数据量不高，而微调过程相对需要更多的数据。在初始化阶段，过多的数据不一定会带来额外的好处。然而，微调所需的数据越多，压缩模型的性能会越好。这一现象揭示了我们的方法如果有更多的数据和计算资源，可以进一步实现更高的精度。
- en: 'Table 7: LLaMA2: different epoch v.s. data.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：LLaMA2：不同的epoch与数据。
- en: '| #KV Heads | Data | Epoch | Perplexity ($\downarrow$) | MMLU | QA AVG. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| #KV Heads | Data | Epoch | Perplexity ($\downarrow$) | MMLU | QA AVG. |'
- en: '| W2 | C4 | 0-shot | 5-shot |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| W2 | C4 | 0-shot | 5-shot |'
- en: '| 16 | 46K | 1 | 7.38 | 9.35 | 42.42 | 44.62 | 61.53 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 46K | 1 | 7.38 | 9.35 | 42.42 | 44.62 | 61.53 |'
- en: '| 46K | 2 | 7.64 | 9.60 | 45.37 | 45.77 | 62.90 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 46K | 2 | 7.64 | 9.60 | 45.37 | 45.77 | 62.90 |'
- en: '| 92K | 1 | 7.26 | 9.20 | 45.00 | 46.36 | 62.57 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 92K | 1 | 7.26 | 9.20 | 45.00 | 46.36 | 62.57 |'
- en: '| 8 | 46K | 1 | 9.48 | 11.84 | 39.93 | 40.10 | 57.53 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 46K | 1 | 9.48 | 11.84 | 39.93 | 40.10 | 57.53 |'
- en: '| 46K | 2 | 9.56 | 12.72 | 41.57 | 41.77 | 58.76 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 46K | 2 | 9.56 | 12.72 | 41.57 | 41.77 | 58.76 |'
- en: '| 92K | 1 | 9.73 | 11.50 | 40.53 | 42.90 | 59.04 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 92K | 1 | 9.73 | 11.50 | 40.53 | 42.90 | 59.04 |'
- en: Enlarge data sizes v.s. increase training epochs. Here we investigate the effect
    of training time on the results, i.e., given the same training GPU resources,
    whether it is more beneficial to continue enlarging the training dataset size
    or increasing the training epoch. Similarly, we train the compressed LLaMA2-7B
    with 2 epoch and 46K training samples, and compare the results of training 1 epoch
    with 92K data. The results are shown in Table [7](#S4.T7 "Table 7 ‣ 4.3 Ablation
    Studies ‣ 4 Experiment ‣ Effectively Compress KV Heads for LLM"). It can be seen
    that the addition of high-quality samples is more helpful to the model accuracies
    than increasing training epochs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 扩大数据规模与增加训练周期。我们在这里探讨训练时间对结果的影响，即在相同的训练GPU资源下，继续扩大训练数据集的规模或增加训练周期哪个更有益。类似地，我们用2个周期和46K训练样本训练了压缩后的LLaMA2-7B，并比较了用92K数据训练1个周期的结果。结果如表[7](#S4.T7
    "Table 7 ‣ 4.3 Ablation Studies ‣ 4 Experiment ‣ Effectively Compress KV Heads
    for LLM")所示。可以看出，增加高质量样本对模型准确率的帮助大于增加训练周期。
- en: 5 Conclusion, Limitation, and Future Work
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论、局限性与未来工作
- en: In this paper, we proposed a low-rank decomposition framework to compress KV
    heads. We first discovered that KV caches are low-rank, and based on this finding
    we converted the original MHA architecture to GQA mechanism by low-rank decomposition.
    We also proposed several special strategies to handle the attention layer with
    RoPE. With half or even three-quarters of KV cache compressed, our approach can
    recover LLM’s accuracy with limited training and data resources, while this is
    not possible with the previous KV head compression method.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种低秩分解框架来压缩KV头。我们首先发现KV缓存是低秩的，基于这一发现，我们将原始MHA架构通过低秩分解转换为GQA机制。我们还提出了几种特殊策略来处理带有RoPE的注意力层。通过压缩一半甚至四分之三的KV缓存，我们的方法可以在有限的训练和数据资源下恢复LLM的准确性，而这是之前的KV头压缩方法无法做到的。
- en: Although our compression framework can significantly reduce KV caches and improve
    the speed of LLM generation, our approach currently has some shortcomings. First
    of all, when our method is used in the attention layer with RoPE, the key head
    compression weights cannot be incorporated into the original model, resulting
    in additional parameters and computational overhead, which will slightly slow
    down the prefill stage. So how to compress key heads with RoPE more elegantly
    will be an interesting direction in the future. In addition, when heavily compressing
    KV heads or the model size is too large, LLM’s accuracy will decrease to some
    extent. Therefore, how to better deal with these situations is also a future direction.
    Besides, our approach can potentially be combined with previous KV cache compression
    methods to achieve an extreme KV cache compression ratio, which we leave as a
    viable direction for the future.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的压缩框架可以显著减少KV缓存并提高LLM生成的速度，但我们的方法目前仍有一些不足。首先，当我们的方法用于带有RoPE的注意力层时，关键头的压缩权重无法并入原始模型，导致额外的参数和计算开销，这会稍微减慢预填充阶段。因此，如何更优雅地与RoPE压缩关键头将是未来一个有趣的方向。此外，当KV头严重压缩或模型规模过大时，LLM的准确性会在一定程度上下降。因此，如何更好地处理这些情况也是未来的一个方向。此外，我们的方法可以与之前的KV缓存压缩方法结合，实现极致的KV缓存压缩比，这也是我们留给未来的一个可行方向。
- en: References
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico
    Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models
    from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical
    Methods in Natural Language Processing, pages 4895–4901, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico
    Lebron, 和 Sumit Sanghai。GQA：从多头检查点训练通用多查询变换器模型。发表于2023年自然语言处理实证方法会议论文集，页4895–4901，2023年。'
- en: '[2] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi.
    Piqa: Reasoning about physical commonsense in natural language. In Proceedings
    of the AAAI Conference on Artificial Intelligence, pages 7432–7439, 2020.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, 和 Yejin Choi。Piqa：推理自然语言中的物理常识。发表于AAAI人工智能会议论文集，页7432–7439，2020年。'
- en: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. In Advances in Neural Information
    Processing Systems 33, volume 33, pages 1877–1901, 2020.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] 汤姆·布朗、本杰明·曼、尼克·赖德、梅拉妮·苏比亚、贾瑞德·D·卡普兰、普拉富拉·达里瓦尔、阿尔文德·内拉坎坦、普拉纳夫·夏姆、吉里什·萨斯特里、阿曼达·阿斯克尔等。语言模型是少样本学习者。发表于《神经信息处理系统进展》第33卷，页码1877–1901，2020年。'
- en: '[4] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
    Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of
    natural yes/no questions. In Proceedings of the Conference of the North American
    Chapter of the Association for Computational Linguistics, pages 2924–2936, 2019.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] 克里斯托弗·克拉克、肯顿·李、明伟·张、汤姆·克维亚特科夫斯基、迈克尔·柯林斯和克里斯蒂娜·图塔诺娃。BoolQ：探索自然的是/否问题的意外困难。发表于《北美计算语言学协会会议论文集》，页码2924–2936，2019年。'
- en: '[5] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 彼得·克拉克、艾萨克·考威、奥伦·埃齐奥尼、图沙尔·科特、阿希什·萨巴瓦尔、卡里萨·肖尼克和欧文德·塔福尔德。认为你已经解决了问答问题？试试arc，ai2推理挑战。arXiv预印本
    arXiv:1803.05457，2018年。'
- en: '[6] Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman,
    Holger Schwenk, and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence
    representations. In Proceedings of the 2018 Conference on Empirical Methods in
    Natural Language Processing, pages 2475–2485, 2018.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 亚历克西斯·孔诺、鲁提·瑞诺特、吉约姆·兰普尔、阿迪娜·威廉姆斯、塞缪尔·博曼、霍尔格·施文克和维塞林·斯托扬诺夫。XNLI：评估跨语言句子表示。发表于《2018年自然语言处理实证方法会议论文集》，页码2475–2485，2018年。'
- en: '[7] DeepSeek-AI. DeepSeek-V2: A strong, economical, and efficient mixture-of-experts
    language model. arXiv preprint arXiv:2405.04434, 2024.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] DeepSeek-AI。DeepSeek-V2：一种强大、经济和高效的混合专家语言模型。arXiv预印本 arXiv:2405.04434，2024年。'
- en: '[8] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA:
    Efficient finetuning of quantized llms. In Advances in Neural Information Processing
    Systems 36, volume 36, pages 10088–10115, 2024.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 蒂姆·德特梅尔斯、阿尔蒂多罗·帕尼奥尼、阿里·霍尔茨曼和卢克·泽特尔莫耶。QLoRA：量化LLMs的高效微调。发表于《神经信息处理系统进展》第36卷，页码10088–10115，2024年。'
- en: '[9] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
    Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al.
    A framework for few-shot language model evaluation, 2021.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 莱昂·高、乔纳森·托、斯特拉·比德曼、席德·布莱克、安东尼·迪波菲、查尔斯·福斯特、劳伦斯·戈尔丁、杰弗里·许、凯尔·麦克唐奈、尼克拉斯·穆恩尼霍夫等。少样本语言模型评估框架，2021年。'
- en: '[10] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng
    Gao. Model tells you what to discard: Adaptive KV cache compression for LLMs.
    In International Conference on Learning Representations, 2024.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 苏玉歌、张云南、刘丽媛、张敏佳、韩佳伟和高剑峰。模型告诉你该丢弃什么：LLMs的自适应KV缓存压缩。发表于国际学习表示会议，2024年。'
- en: '[11] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. MiniLLM: Knowledge distillation
    of large language models. In International Conference on Learning Representations,
    2024.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 郭宇贤、李东、傅如维和黄敏丽。MiniLLM：大型语言模型的知识蒸馏。发表于国际学习表示会议，2024年。'
- en: '[12] Yu Hao and Wu Jianxin. Compressing transformers: Features are low-rank,
    but weights are not! In Proceedings of the AAAI Conference on Artificial Intelligence,
    pages 11007–11015, 2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 俞浩和吴建新。压缩变换器：特征是低秩的，但权重不是！发表于《人工智能AAA会议论文集》，页码11007–11015，2023年。'
- en: '[13] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,
    Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding.
    In International Conference on Learning Representations, 2021.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 丹·亨德里克斯、科林·伯恩斯、史蒂文·巴萨特、安迪·佐乌、曼塔斯·马泽卡、道恩·宋和雅各布·斯坦赫特。测量大规模多任务语言理解。发表于国际学习表示会议，2021年。'
- en: '[14] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
    Lu Wang, Weizhu Chen, et al. LoRA: Low-rank adaptation of large language models.
    In International Conference on Learning Representations, 2021.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 爱德华·J·胡、菲利普·沃利斯、泽元·艾伦-朱、袁智李、沈旺、卢旺、魏铸·陈等。LoRA：大型语言模型的低秩适应。发表于国际学习表示会议，2021年。'
- en: '[15] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar
    Krishna, and Tuo Zhao. Gear: An efficient kv cache compression recipefor near-lossless
    generative inference of llm. arXiv preprint arXiv:2403.05527, 2024.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 郝康、张青如、苏维克·昆杜、郑建华、刘枣兴、图沙尔·克里希纳和赵拓。Gear：一种高效的KV缓存压缩方案，用于近乎无损的LLM生成推理。arXiv预印本
    arXiv:2403.05527，2024年。'
- en: '[16] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
    Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, et al. Bloom: A 176b-parameter
    open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 特文·勒·斯考, 安吉拉·范, 克里斯托弗·阿基基, 艾莉·帕夫里克, 苏珊娜·伊利奇, 丹尼尔·赫斯洛, 罗曼·卡斯塔涅, 亚历山德拉·萨莎·卢乔尼,
    等。Bloom: 一种1760亿参数的开放访问多语言模型。arXiv 预印本 arXiv:2211.05100，2022年。'
- en: '[17] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen,
    Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru,
    Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke
    Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. Few-shot
    learning with multilingual generative language models. In Proceedings of the 2022
    Conference on Empirical Methods in Natural Language Processing, pages 9019–9052,
    2022.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 维多利亚·林, 托多尔·米哈伊洛夫, 米克尔·阿特克斯, 汤天禄, 陈硕辉, 丹尼尔·西米格, 米勒·奥特, 纳曼·戈亚尔, 施鲁蒂·博萨尔,
    京飞·杜, 拉马坎特·帕苏努鲁, 山姆·施莱弗, 普尼特·辛格·库拉, 维什拉夫·乔杜里, 布赖恩·奥赫罗, 杰夫·王, 卢克·泽特尔莫耶, 佐尔尼察·科扎雷瓦,
    莫娜·迪亚布, 维塞林·斯托亚诺夫, 和李贤。使用多语言生成语言模型进行少样本学习。载于《2022年自然语言处理经验方法会议论文集》，9019–9052页，2022年。'
- en: '[18] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar
    Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. LLM-QAT: Data-free
    quantization aware training for large language models. arXiv preprint arXiv:2305.17888,
    2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 刘泽春, 巴拉斯·奥古兹, 赵长生, 恩尼·张, 皮埃尔·斯托克, 雅沙尔·梅赫达德, 杨洋·石, 拉古拉曼·克里希纳穆尔提, 和维卡斯·钱德拉。LLM-QAT:
    针对大型语言模型的数据无关量化感知训练。arXiv 预印本 arXiv:2305.17888，2023年。'
- en: '[19] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo
    Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting
    the persistence of importance hypothesis for llm kv cache compression at test
    time. In Advances in Neural Information Processing Systems 36, volume 36, pages
    52342–52364, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 刘自畅, 阿迪蒂亚·德赛, 梁方硕, 王伟涛, 维克托·谢, 赵卓徐, 阿纳斯塔修斯·基里迪斯, 和安舒马利·施里瓦斯塔瓦。Scissorhands:
    利用重要性假设的持久性进行 LLM KV 缓存压缩测试。载于《神经信息处理系统进展》第36卷，第36卷，52342–52364页，2023年。'
- en: '[20] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay,
    Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing
    data and methods for effective instruction tuning. In International Conference
    on Machine Learning, pages 22631–22648, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 谢恩·朗普雷, 乐厚, 杜吴, 阿尔伯特·韦布森, 玄元·钟, 易泰, 丹尼·周, 郭志伟, 巴雷特·佐夫, 杰森·魏, 等。FLAN 集合：为有效的指令调优设计数据和方法。载于《国际机器学习会议》，22631–22648页，2023年。'
- en: '[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization.
    In International Conference on Learning Representations, 2018.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 伊利亚·洛什基洛夫 和 弗兰克·胡特。解耦的权重衰减正则化。载于《国际学习表征会议》，2018年。'
- en: '[22] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better
    than one? In Advances in Neural Information Processing Systems 32, volume 32,
    2019.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 保罗·米歇尔, 奥梅尔·莱维, 和格雷厄姆·纽比格。十六个头真的比一个好？载于《神经信息处理系统进展》第32卷，第32卷，2019年。'
- en: '[23] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a
    suit of armor conduct electricity? a new dataset for open book question answering.
    In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
    pages 2381–2391, 2018.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 托多尔·米哈伊洛夫, 彼得·克拉克, 图沙尔·科特, 和阿什什·萨巴尔瓦尔。盔甲能导电吗？一个新的开放书籍问答数据集。载于《自然语言处理经验方法会议论文集》，2381–2391页，2018年。'
- en: '[24] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella
    Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf,
    et al. Crosslingual generalization through multitask finetuning. In Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pages 15991–16111, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 尼克拉斯·穆恩尼霍夫, 汤姆·王, 林唐·苏塔维卡, 亚当·罗伯茨, 斯特拉·比德曼, 特文·勒·斯考, M·赛夫·巴里, 盛申, 郑新永,
    海莉·肖尔科普夫, 等。通过多任务微调实现跨语言泛化。载于《第61届计算语言学协会年会会议论文集（卷1：长篇论文）》，15991–16111页，2023年。'
- en: '[25] Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulić,
    and Anna Korhonen. XCOPA: A multilingual dataset for causal commonsense reasoning.
    In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP), pages 2362–2376, 2020.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 埃多阿尔多·玛利亚·庞蒂, 戈兰·格拉瓦什, 奥尔加·马耶夫斯卡, 刘乾初, 伊万·武利奇, 和安娜·科霍宁。XCOPA: 用于因果常识推理的多语言数据集。载于《2020年自然语言处理经验方法会议论文集（EMNLP）》，2362–2376页，2020年。'
- en: '[26] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention
    with linear biases enables input length extrapolation. In International Conference
    on Learning Representations, 2022.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Ofir Press、Noah Smith 和 Mike Lewis。《短训练，长测试：带有线性偏差的注意力使输入长度外推成为可能》。收录于《国际学习表示会议》，2022年。'
- en: '[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of machine learning
    research, 21(140):1–67, 2020.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J Liu。《利用统一的文本到文本 Transformer 探索迁移学习的极限》。机器学习研究杂志，第21卷（140）：第1–67页，2020年。'
- en: '[28] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
    WinoGrande: An adversarial winograd schema challenge at scale. In Communications
    of the ACM, pages 99–106, 2021.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Keisuke Sakaguchi、Ronan Le Bras、Chandra Bhagavatula 和 Yejin Choi。《WinoGrande:
    大规模对抗性 Winograd 语料库挑战》。收录于《ACM 通讯》，第99–106页，2021年。'
- en: '[29] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi.
    Social IQa: Commonsense reasoning about social interactions. In Proceedings of
    the Conference on Empirical Methods in Natural Language Processing and the International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463–4473,
    2019.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Maarten Sap、Hannah Rashkin、Derek Chen、Ronan Le Bras 和 Yejin Choi。《Social
    IQa: 关于社会互动的常识推理》。收录于《自然语言处理实证方法会议与国际联合自然语言处理会议（EMNLP-IJCNLP）》论文集，第4463–4473页，2019年。'
- en: '[30] Noam Shazeer. Fast transformer decoding: One write-head is all you need.
    arXiv preprint arXiv:1911.02150, 2019.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Noam Shazeer。《快速 Transformer 解码：一个写头就够了》。arXiv 预印本 arXiv:1911.02150，2019年。'
- en: '[31] Merity Stephen, Xiong Caiming, Bradbury James, Socher Richard, et al.
    Pointer sentinel mixture models. In International Conference on Learning Representations,
    2017.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Merity Stephen、Xiong Caiming、Bradbury James、Socher Richard 等。《Pointer
    Sentinel 混合模型》。收录于《国际学习表示会议》，2017年。'
- en: '[32] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng
    Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing,
    568:127063, 2024.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Jianlin Su、Murtadha Ahmed、Yu Lu、Shengfeng Pan、Wen Bo 和 Yunfeng Liu。《Roformer:
    带有旋转位置嵌入的增强 Transformer》。Neurocomputing，第568卷，第127063页，2024年。'
- en: '[33] Alexey Tikhonov and Max Ryabinin. It’s All in the Heads: Using Attention
    Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning. In Findings
    of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3534–3546,
    2021.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Alexey Tikhonov 和 Max Ryabinin。《一切都在头部：利用注意力头作为常识推理中的跨语言转移基线》。收录于《计算语言学协会会议：ACL-IJCNLP
    2021》论文集，第3534–3546页，2021年。'
- en: '[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, et al. Llama 2: Open foundation
    and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra 等。《Llama 2: 开放的基础和微调的聊天模型》。arXiv 预印本 arXiv:2307.09288，2023年。'
- en: '[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    In Advances in Neural Information Processing Systems 30, volume 30, pages 5998–6008,
    2017.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan
    N Gomez、Łukasz Kaiser 和 Illia Polosukhin。《Attention is All You Need》。收录于《神经信息处理系统进展
    30》第30卷，第5998–6008页，2017年。'
- en: '[36] Jianxin Wu. Essentials of Pattern Recognition: An Accessible Approach.
    Cambridge University Press, 2020.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Jianxin Wu。《模式识别基础：一种可访问的方法》。剑桥大学出版社，2020年。'
- en: '[37] Gongfan Fang Xinyin Ma and Xinchao Wang. Llm-pruner: On the structural
    pruning of large language models. In Advances in Neural Information Processing
    Systems 36, volume 36, pages 21702–21720, 2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Gongfan Fang、Xinyin Ma 和 Xinchao Wang。《Llm-pruner: 大型语言模型的结构性剪枝》。收录于《神经信息处理系统进展
    36》第36卷，第21702–21720页，2023年。'
- en: '[38] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th
    Annual Meeting of the Association for Computational Linguistics, page 4791–4800,
    2019.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Rowan Zellers、Ari Holtzman、Yonatan Bisk、Ali Farhadi 和 Yejin Choi。《HellaSwag:
    机器真的能完成你的句子吗？》收录于《第57届计算语言学协会年会》论文集，第4791–4800页，2019年。'
- en: '[39] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, et al. OPT: Open pre-trained transformer language
    models. arXiv preprint arXiv:2205.01068, 2022.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Susan Zhang、Stephen Roller、Naman Goyal、Mikel Artetxe、Moya Chen、Shuohui
    Chen、Christopher Dewan、Mona Diab 等。《OPT: 开放的预训练 Transformer 语言模型》。arXiv 预印本 arXiv:2205.01068，2022年。'
- en: '[40] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
    Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang "Atlas"
    Wang, and Beidi Chen. H2O: Heavy-hitter oracle for efficient generative inference
    of large language models. In Advances in Neural Information Processing Systems
    36, volume 36, pages 34661–34710, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
    Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang "Atlas"
    Wang, 和 Beidi Chen. H2O: 高效生成大语言模型推理的重型预言机。发表于 Advances in Neural Information
    Processing Systems 36，第 36 卷，第 34661–34710 页，2023 年。'
- en: Appendix A The running time of compression
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 压缩的运行时间
- en: Here we report the time required to compress LLaMA2 models. The results are
    shown in Table [8](#A1.T8 "Table 8 ‣ Appendix A The running time of compression
    ‣ Effectively Compress KV Heads for LLM"). Numbers in the table are measured in
    hours. As we can see, our algorithm requires very little time to compute the compression
    weights, and the bottleneck of the entire framework is the time spent fine-tuning
    the compressed models.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们报告了压缩 LLaMA2 模型所需的时间。结果见表 [8](#A1.T8 "Table 8 ‣ Appendix A The running
    time of compression ‣ Effectively Compress KV Heads for LLM")。表中的数字以小时为单位。正如我们所见，我们的算法计算压缩权重所需的时间非常少，而整个框架的瓶颈是精调压缩模型所花费的时间。
- en: 'Table 8: Hours needed to calculate compressing weights and fine-tune the model.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 计算压缩权重和精调模型所需的小时数。'
- en: '| Size | Initialization | Fine-tuning | Size | Initialization | Fine-tuning
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Size | Initialization | Fine-tuning | Size | Initialization | Fine-tuning
    |'
- en: '| Head = 16 | Head = 8 | Head = 16 | Head = 8 | Head = 20 | Head = 10 | Head
    = 20 | Head = 10 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| Head = 16 | Head = 8 | Head = 16 | Head = 8 | Head = 20 | Head = 10 | Head
    = 20 | Head = 10 |'
- en: '| 7B | 0.30 | 43.40 | 36.14 | 13B | 0.68 | 103.47 | 87.01 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 7B | 0.30 | 43.40 | 36.14 | 13B | 0.68 | 103.47 | 87.01 |'
- en: Appendix B Detailed results of compressed models
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 压缩模型的详细结果
- en: In Table [9](#A2.T9 "Table 9 ‣ Appendix B Detailed results of compressed models
    ‣ Effectively Compress KV Heads for LLM") we report detailed accuracies of BLOOMZ-7B1
    and its compressed version on each sub-dataset. For LLaMA2 models in the MMLU
    datasets, we report the accuracies in Table [10](#A2.T10 "Table 10 ‣ Appendix
    B Detailed results of compressed models ‣ Effectively Compress KV Heads for LLM").
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [9](#A2.T9 "Table 9 ‣ Appendix B Detailed results of compressed models ‣
    Effectively Compress KV Heads for LLM") 中，我们报告了 BLOOMZ-7B1 及其压缩版本在每个子数据集上的详细准确度。对于
    MMLU 数据集中的 LLaMA2 模型，我们在表格 [10](#A2.T10 "Table 10 ‣ Appendix B Detailed results
    of compressed models ‣ Effectively Compress KV Heads for LLM") 中报告了准确度。
- en: 'Table 9: Detailed accuracies of BLOOMZ-7B1 with different KV heads.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: BLOOMZ-7B1 使用不同 KV 头的详细准确度。'
- en: '| #KV Heads | Dataset | Average ACC. |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| #KV Heads | Dataset | Average ACC. |'
- en: '| 32 | XNLI | XWinoGrad | 47.25 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 32 | XNLI | XWinoGrad | 47.25 |'
- en: '| BG | DE | EL | RU | TH | TR | JP | RU |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| BG | DE | EL | RU | TH | TR | JP | RU |'
- en: '| 40.30 | 42.80 | 38.74 | 42.70 | 37.69 | 36.15 | 50.26 | 52.70 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 40.30 | 42.80 | 38.74 | 42.70 | 37.69 | 36.15 | 50.26 | 52.70 |'
- en: '| XCOPA | XStoryCloze |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| XCOPA | XStoryCloze |'
- en: '| ET | HT | IT | QU | TH | TR | MY | RU |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| ET | HT | IT | QU | TH | TR | MY | RU |'
- en: '| 48.20 | 49.40 | 57.00 | 48.00 | 55.20 | 48.40 | 49.46 | 59.03 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 48.20 | 49.40 | 57.00 | 48.00 | 55.20 | 48.40 | 49.46 | 59.03 |'
- en: '| 16 | XNLI | XWinoGrad | 47.85 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 16 | XNLI | XWinoGrad | 47.85 |'
- en: '| BG | DE | EL | RU | TH | TR | JP | RU |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| BG | DE | EL | RU | TH | TR | JP | RU |'
- en: '| 39.86 | 42.92 | 38.51 | 42.62 | 36.99 | 36.00 | 50.43 | 52.51 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 39.86 | 42.92 | 38.51 | 42.62 | 36.99 | 36.00 | 50.43 | 52.51 |'
- en: '| XCOPA | XStoryCloze |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| XCOPA | XStoryCloze |'
- en: '| ET | HT | IT | QU | TH | TR | MY | RU |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| ET | HT | IT | QU | TH | TR | MY | RU |'
- en: '| 50.00 | 51.40 | 57.20 | 51.40 | 59.20 | 48.60 | 50.09 | 58.03 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 50.00 | 51.40 | 57.20 | 51.40 | 59.20 | 48.60 | 50.09 | 58.03 |'
- en: '| 8 | XNLI | XWinoGrad | 46.84 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 8 | XNLI | XWinoGrad | 46.84 |'
- en: '| BG | DE | EL | RU | TH | TR | JP | RU |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| BG | DE | EL | RU | TH | TR | JP | RU |'
- en: '| 39.27 | 42.16 | 38.35 | 41.57 | 37.00 | 35.85 | 49.16 | 51.75 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 39.27 | 42.16 | 38.35 | 41.57 | 37.00 | 35.85 | 49.16 | 51.75 |'
- en: '| XCOPA | XStoryCloze |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| XCOPA | XStoryCloze |'
- en: '| ET | HT | IT | QU | TH | TR | MY | RU |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| ET | HT | IT | QU | TH | TR | MY | RU |'
- en: '| 50.00 | 49.80 | 55.60 | 51.40 | 54.40 | 48.40 | 49.34 | 55.39 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 50.00 | 49.80 | 55.60 | 51.40 | 54.40 | 48.40 | 49.34 | 55.39 |'
- en: '| 4 | XNLI | XWinoGrad | 45.92 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 4 | XNLI | XWinoGrad | 45.92 |'
- en: '| BG | DE | EL | RU | TH | TR | JP | RU |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| BG | DE | EL | RU | TH | TR | JP | RU |'
- en: '| 38.82 | 40.76 | 37.70 | 41.43 | 36.38 | 35.61 | 48.91 | 51.68 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 38.82 | 40.76 | 37.70 | 41.43 | 36.38 | 35.61 | 48.91 | 51.68 |'
- en: '| XCOPA | XStoryCloze |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| XCOPA | XStoryCloze |'
- en: '| ET | HT | IT | QU | TH | TR | MY | RU |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| ET | HT | IT | QU | TH | TR | MY | RU |'
- en: '| 49.20 | 47.40 | 48.80 | 51.20 | 55.60 | 49.40 | 47.85 | 53.94 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 49.20 | 47.40 | 48.80 | 51.20 | 55.60 | 49.40 | 47.85 | 53.94 |'
- en: 'Table 10: Detailed accuracies of LLaMA2 models with different KV heads in MMLU
    datasets.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: LLaMA2 模型在 MMLU 数据集中使用不同 KV 头的详细准确度。'
- en: '| Model | #KV Heads | MMLU ($0$-shot) |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Model | #KV Heads | MMLU ($0$-shot) |'
- en: '| Hums. | STEM | Social | Other | Avg. | Hums. | STEM | Social | Other | Avg.
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 人文 | STEM | 社会 | 其他 | 平均 | 人文 | STEM | 社会 | 其他 | 平均 |'
- en: '| LLaMA2-7B | 32 | 39.64 | 34.25 | 47.35 | 47.18 | 41.79 | 43.32 | 36.98 |
    51.77 | 52.69 | 45.82 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | 32 | 39.64 | 34.25 | 47.35 | 47.18 | 41.79 | 43.32 | 36.98 |
    51.77 | 52.69 | 45.82 |'
- en: '| 16 | 44.99 | 39.42 | 55.96 | 54.81 | 48.32 | 44.48 | 39.93 | 56.68 | 56.26
    | 48.74 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 44.99 | 39.42 | 55.96 | 54.81 | 48.32 | 44.48 | 39.93 | 56.68 | 56.26
    | 48.74 |'
- en: '| 8 | 41.06 | 37.36 | 52.58 | 49.50 | 44.62 | 41.11 | 38.31 | 54.01 | 51.21
    | 45.54 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 41.06 | 37.36 | 52.58 | 49.50 | 44.62 | 41.11 | 38.31 | 54.01 | 51.21
    | 45.54 |'
- en: '| LLaMA2-13B | 40 | 47.99 | 42.21 | 61.23 | 59.41 | 52.12 | 53.43 | 43.84 |
    63.21 | 61.35 | 55.17 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B | 40 | 47.99 | 42.21 | 61.23 | 59.41 | 52.12 | 53.43 | 43.84 |
    63.21 | 61.35 | 55.17 |'
- en: '| 20 | 49.54 | 43.48 | 62.98 | 60.93 | 53.65 | 50.97 | 44.24 | 63.99 | 61.83
    | 54.71 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 49.54 | 43.48 | 62.98 | 60.93 | 53.65 | 50.97 | 44.24 | 63.99 | 61.83
    | 54.71 |'
- en: '| 10 | 46.25 | 40.22 | 57.62 | 56.49 | 49.65 | 46.89 | 41.42 | 59.90 | 56.90
    | 50.73 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 46.25 | 40.22 | 57.62 | 56.49 | 49.65 | 46.89 | 41.42 | 59.90 | 56.90
    | 50.73 |'
- en: Appendix C Detailed results of different compression strategies
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 不同压缩策略的详细结果
- en: In Tables [11](#A3.T11 "Table 11 ‣ Appendix C Detailed results of different
    compression strategies ‣ Effectively Compress KV Heads for LLM") and [12](#A3.T12
    "Table 12 ‣ Appendix C Detailed results of different compression strategies ‣
    Effectively Compress KV Heads for LLM"), we compare the influence of three different
    compression strategies in detail, which can better reflect the advantage of our
    compression strategy. Although the model accuracy collapses after direct compression,
    our compression model can quickly recover the accuracy after further fine-tuning.
    These results demonstrate the effectiveness of our framework.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [11](#A3.T11 "表 11 ‣ 附录 C 不同压缩策略的详细结果 ‣ 有效压缩 KV 头以提高 LLM") 和 [12](#A3.T12
    "表 12 ‣ 附录 C 不同压缩策略的详细结果 ‣ 有效压缩 KV 头以提高 LLM") 中，我们详细比较了三种不同压缩策略的影响，这能更好地体现我们压缩策略的优势。尽管模型在直接压缩后准确度下降，但我们的压缩模型可以在进一步微调后迅速恢复准确度。这些结果证明了我们框架的有效性。
- en: 'Table 11: Detailed accuracies in MMLU of LLaMA2 models with different initialization
    strategies.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: LLaMA2 模型在 MMLU 上的详细准确率，不同初始化策略的比较。'
- en: '| #KV Heads | Methods | Stage | MMLU (0-shot) | MMLU (5-shot) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| #KV 头 | 方法 | 阶段 | MMLU (0-shot) | MMLU (5-shot) |'
- en: '| Hums. | STEM | Social | Other | Avg. | Hums. | STEM | Social | Other | Avg.
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 人文 | STEM | 社会 | 其他 | 平均 | 人文 | STEM | 社会 | 其他 | 平均 |'
- en: '| 16 | Mean-pool | Init. | 24.21 | 21.22 | 21.71 | 23.98 | 22.94 | 24.21 |
    21.22 | 21.71 | 23.98 | 22.94 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 均值池化 | 初始化 | 24.21 | 21.22 | 21.71 | 23.98 | 22.94 | 24.21 | 21.22 |
    21.71 | 23.98 | 22.94 |'
- en: '| Fine-tune | 32.99 | 31.18 | 37.60 | 40.49 | 35.25 | 33.24 | 20.83 | 38.25
    | 40.52 | 35.41 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 32.99 | 31.18 | 37.60 | 40.49 | 35.25 | 33.24 | 20.83 | 38.25 | 40.52
    | 35.41 |'
- en: '| SVD-*w* | Init. | 24.63 | 21.44 | 21.94 | 23.66 | 23.11 | 24.21 | 21.73 |
    21.84 | 24.40 | 23.17 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| SVD-*w* | 初始化 | 24.63 | 21.44 | 21.94 | 23.66 | 23.11 | 24.21 | 21.73 | 21.84
    | 24.40 | 23.17 |'
- en: '| Fine-tune | 38.98 | 34.09 | 49.17 | 49.57 | 42.46 | 37.98 | 30.76 | 44.59
    | 45.09 | 39.38 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 38.98 | 34.09 | 49.17 | 49.57 | 42.46 | 37.98 | 30.76 | 44.59 | 45.09
    | 39.38 |'
- en: '| SVD-*a* | Init. | 25.06 | 29.72 | 31.04 | 27.94 | 28.05 | 25.48 | 24.10 |
    24.44 | 30.13 | 25.97 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| SVD-*a* | 初始化 | 25.06 | 29.72 | 31.04 | 27.94 | 28.05 | 25.48 | 24.10 | 24.44
    | 30.13 | 25.97 |'
- en: '| Fine-tune | 44.99 | 39.42 | 55.96 | 54.81 | 48.32 | 44.48 | 39.93 | 56.68
    | 56.26 | 48.74 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 44.99 | 39.42 | 55.96 | 54.81 | 48.32 | 44.48 | 39.93 | 56.68 | 56.26
    | 48.74 |'
- en: '| 8 | Mean-pool | Init. | 24.82 | 24.52 | 24.80 | 25.97 | 25.00 | 24.23 | 27.40
    | 23.56 | 24.23 | 24.95 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 均值池化 | 初始化 | 24.82 | 24.52 | 24.80 | 25.97 | 25.00 | 24.23 | 27.40 |
    23.56 | 24.23 | 24.95 |'
- en: '| Fine-tune | 23.80 | 29.24 | 30.26 | 23.80 | 26.63 | 24.12 | 29.08 | 25.22
    | 23.30 | 25.30 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 23.80 | 29.24 | 30.26 | 23.80 | 26.63 | 24.12 | 29.08 | 25.22 | 23.30
    | 25.30 |'
- en: '| SVD-*w* | Init. | 24.00 | 22.49 | 22.62 | 24.27 | 23.42 | 24.40 | 21.38 |
    21.61 | 23.62 | 22.94 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| SVD-*w* | 初始化 | 24.00 | 22.49 | 22.62 | 24.27 | 23.42 | 24.40 | 21.38 | 21.61
    | 23.62 | 22.94 |'
- en: '| Fine-tune | 24.97 | 23.28 | 23.14 | 25.46 | 24.30 | 24.25 | 23.88 | 22.07
    | 24.24 | 23.69 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 24.97 | 23.28 | 23.14 | 25.46 | 24.30 | 24.25 | 23.88 | 22.07 | 24.24
    | 23.69 |'
- en: '| SVD-*a* | Init. | 24.08 | 22.90 | 23.53 | 23.69 | 23.61 | 24.08 | 24.55 |
    22.91 | 24.07 | 23.93 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| SVD-*a* | 初始化 | 24.08 | 22.90 | 23.53 | 23.69 | 23.61 | 24.08 | 24.55 | 22.91
    | 24.07 | 23.93 |'
- en: '| Fine-tune | 41.06 | 37.36 | 52.58 | 49.50 | 44.62 | 41.11 | 38.31 | 54.01
    | 51.21 | 45.54 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 41.06 | 37.36 | 52.58 | 49.50 | 44.62 | 41.11 | 38.31 | 54.01 | 51.21
    | 45.54 |'
- en: 'Table 12: Detailed accuracies in zero-shot commonsense QA datasets of LLaMA2
    models with different initialization strategies.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12: LLaMA2 模型在零-shot 常识问答数据集上的详细准确率，不同初始化策略的比较。'
- en: '| #KV Heads | Methods | Stage | Commonsense QA (0-shot) | AVG. |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| #KV 头 | 方法 | 阶段 | 常识问答 (0-shot) | 平均 |'
- en: '| BoolQ | PIQA | SIQA | HLSW | WG | ARC-e | ARC-c | OBQA |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | PIQA | SIQA | HLSW | WG | ARC-e | ARC-c | OBQA |'
- en: '| 16 | Mean-pool | Init. | 37.80 | 50.76 | 32.70 | 26.47 | 50.28 | 26.68 |
    26.71 | 27.60 | 34.88 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 平均池化 | 初始 | 37.80 | 50.76 | 32.70 | 26.47 | 50.28 | 26.68 | 26.71 |
    27.60 | 34.88 |'
- en: '| Fine-tune | 73.27 | 74.37 | 34.29 | 60.44 | 62.51 | 70.12 | 40.36 | 37.20
    | 56.57 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 73.27 | 74.37 | 34.29 | 60.44 | 62.51 | 70.12 | 40.36 | 37.20 | 56.57
    |'
- en: '| SVD-*w* | Init. | 38.47 | 53.10 | 31.58 | 27.88 | 51.62 | 28.83 | 24.23 |
    28.00 | 35.46 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| SVD-*w* | 初始 | 38.47 | 53.10 | 31.58 | 27.88 | 51.62 | 28.83 | 24.23 | 28.00
    | 35.46 |'
- en: '| Fine-tune | 79.45 | 77.80 | 32.96 | 69.20 | 69.69 | 76.30 | 46.59 | 44.80
    | 62.10 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 79.45 | 77.80 | 32.96 | 69.20 | 69.69 | 76.30 | 46.59 | 44.80 | 62.10
    |'
- en: '| SVD-*a* | Init. | 60.21 | 69.26 | 33.27 | 51.93 | 59.67 | 51.60 | 31.83 |
    33.20 | 48.87 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| SVD-*a* | 初始 | 60.21 | 69.26 | 33.27 | 51.93 | 59.67 | 51.60 | 31.83 | 33.20
    | 48.87 |'
- en: '| Fine-tune | 83.61 | 77.97 | 32.80 | 72.53 | 73.56 | 78.45 | 49.66 | 46.40
    | 64.37 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 83.61 | 77.97 | 32.80 | 72.53 | 73.56 | 78.45 | 49.66 | 46.40 | 64.37
    |'
- en: '| 8 | Mean-pool | Init. | 37.83 | 49.89 | 33.06 | 26.28 | 48.93 | 26.22 | 27.30
    | 28.00 | 34.69 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 平均池化 | 初始 | 37.83 | 49.89 | 33.06 | 26.28 | 48.93 | 26.22 | 27.30 | 28.00
    | 34.69 |'
- en: '| Fine-tune | 60.03 | 58.38 | 32.91 | 38.48 | 50.75 | 37.92 | 24.49 | 23.40
    | 40.79 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 60.03 | 58.38 | 32.91 | 38.48 | 50.75 | 37.92 | 24.49 | 23.40 | 40.79
    |'
- en: '| SVD-*w* | Init. | 42.60 | 50.76 | 34.14 | 26.59 | 51.22 | 26.39 | 25.94 |
    25.80 | 35.43 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| SVD-*w* | 初始 | 42.60 | 50.76 | 34.14 | 26.59 | 51.22 | 26.39 | 25.94 | 25.80
    | 35.43 |'
- en: '| Fine-tune | 69.60 | 73.99 | 32.75 | 60.30 | 63.93 | 70.24 | 39.68 | 36.40
    | 55.86 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 69.60 | 73.99 | 32.75 | 60.30 | 63.93 | 70.24 | 39.68 | 36.40 | 55.86
    |'
- en: '| SVD-*a* | Init. | 46.33 | 56.96 | 32.80 | 30.30 | 51.70 | 32.53 | 24.32 |
    25.80 | 37.59 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| SVD-*a* | 初始 | 46.33 | 56.96 | 32.80 | 30.30 | 51.70 | 32.53 | 24.32 | 25.80
    | 37.59 |'
- en: '| Fine-tune | 80.58 | 76.77 | 32.91 | 66.67 | 67.88 | 73.32 | 43.34 | 43.80
    | 60.66 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 80.58 | 76.77 | 32.91 | 66.67 | 67.88 | 73.32 | 43.34 | 43.80 | 60.66
    |'
