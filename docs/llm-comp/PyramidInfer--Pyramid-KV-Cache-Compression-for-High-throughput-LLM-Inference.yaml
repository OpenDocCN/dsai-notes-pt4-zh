- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:52:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:52:54
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'PyramidInfer: Pyramid KV 缓存压缩用于高吞吐量 LLM 推理'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.12532](https://ar5iv.labs.arxiv.org/html/2405.12532)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.12532](https://ar5iv.labs.arxiv.org/html/2405.12532)
- en: 'Dongjie Yang^(1,), Xiaodong Han², Yan Gao², Yao Hu², Shilin Zhang³, Hai Zhao^(1,)¹¹footnotemark:
    1^,'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '杨东杰^(1,)，韩晓东²，高岩²，胡瑶²，张士林³，赵海^(1,)¹¹脚注标记: 1^,'
- en: ¹ Shanghai Jiao Tong University, ² Xiaohongshu Inc.,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 上海交通大学，² 小红书公司，
- en: ³ South China University of Technology
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 华南理工大学
- en: ¹{djyang.tony@,zhaohai@cs.}sjtu.edu.cn,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹{djyang.tony@,zhaohai@cs.}sjtu.edu.cn,
- en: ²{shuweng,yadun,xiaohou}@xiaohongshu.com   Dongjie Yang and Hai Zhao are with
    the Department of Computer Science and Engineering, Shanghai Jiao Tong University,
    and also with Key Laboratory of Shanghai Education Commission for Intelligent
    Interaction and Cognitive Engineering, Shanghai Jiao Tong University.  Corresponding
    author; This paper was partially supported by Joint Research Project of Yangtze
    River Delta Science and Technology Innovation Community (No. 2022CSJGG1400).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ²{shuweng,yadun,xiaohou}@xiaohongshu.com   杨东杰和赵海来自上海交通大学计算机科学与工程系，同时也隶属于上海交通大学智能互动与认知工程教育部重点实验室。  通讯作者；本论文部分由长三角科技创新共同体联合研究项目（编号：2022CSJGG1400）资助。
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have shown remarkable comprehension abilities but
    face challenges in GPU memory usage during inference, hindering their scalability
    for real-time applications like chatbots. To accelerate inference, we store computed
    keys and values (KV cache) in the GPU memory. Existing methods study the KV cache
    compression to reduce memory by pruning the pre-computed KV cache. However, they
    neglect the inter-layer dependency between layers and huge memory consumption
    in pre-computation. To explore these deficiencies, we find that the number of
    crucial keys and values that influence future generations decreases layer by layer
    and we can extract them by the consistency in attention weights. Based on the
    findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise
    retaining crucial context. PyramidInfer saves significant memory by computing
    fewer keys and values without sacrificing performance. Experimental results show
    PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU
    memory reduction in KV cache. Our code is available in [https://github.com/mutonix/pyramidinfer](https://github.com/mutonix/pyramidinfer).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）展示了显著的理解能力，但在推理过程中面临 GPU 内存使用的挑战，这限制了它们在实时应用（如聊天机器人）中的扩展性。为了加速推理，我们将计算出的键和值（KV
    缓存）存储在 GPU 内存中。现有方法研究了 KV 缓存的压缩，通过修剪预计算的 KV 缓存来减少内存。然而，它们忽略了层间的依赖关系以及预计算中巨大的内存消耗。为了探讨这些不足，我们发现影响未来生成的关键键和值的数量逐层减少，我们可以通过注意力权重的一致性提取它们。基于这些发现，我们提出了
    PyramidInfer，这是一种通过逐层保留关键上下文来压缩 KV 缓存的方法。PyramidInfer 通过计算更少的键和值来节省大量内存，而不会牺牲性能。实验结果表明，与
    Accelerate 相比，PyramidInfer 在 KV 缓存中实现了 54% 以上的 GPU 内存减少，吞吐量提高了 2.2 倍。我们的代码可以在
    [https://github.com/mutonix/pyramidinfer](https://github.com/mutonix/pyramidinfer)
    获取。
- en: 'PyramidInfer: Pyramid KV Cache Compression'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'PyramidInfer: Pyramid KV 缓存压缩'
- en: for High-throughput LLM Inference
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 用于高吞吐量 LLM 推理
- en: 'Dongjie Yang^(1,)^†^†thanks:   Dongjie Yang and Hai Zhao are with the Department
    of Computer Science and Engineering, Shanghai Jiao Tong University, and also with
    Key Laboratory of Shanghai Education Commission for Intelligent Interaction and
    Cognitive Engineering, Shanghai Jiao Tong University., Xiaodong Han², Yan Gao²,
    Yao Hu², Shilin Zhang³, Hai Zhao^(1,)¹¹footnotemark: 1^,^†^†thanks:   Corresponding
    author; This paper was partially supported by Joint Research Project of Yangtze
    River Delta Science and Technology Innovation Community (No. 2022CSJGG1400). ¹
    Shanghai Jiao Tong University, ² Xiaohongshu Inc., ³ South China University of
    Technology ¹{djyang.tony@,zhaohai@cs.}sjtu.edu.cn, ²{shuweng,yadun,xiaohou}@xiaohongshu.com'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '杨东杰^(1,)^†^†感谢:   杨东杰和赵海来自上海交通大学计算机科学与工程系，同时也隶属于上海交通大学智能互动与认知工程教育部重点实验室。，韩晓东²，
    高岩²， 胡瑶²， 张士林³， 赵海^(1,)¹¹脚注标记: 1^,^†^†感谢:   通讯作者；本论文部分由长三角科技创新共同体联合研究项目（编号：2022CSJGG1400）资助。¹
    上海交通大学，² 小红书公司，³ 华南理工大学 ¹{djyang.tony@,zhaohai@cs.}sjtu.edu.cn, ²{shuweng,yadun,xiaohou}@xiaohongshu.com'
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) (OpenAI, [2023](#bib.bib18); Anthropic, [2023](#bib.bib3);
    Jiang et al., [2023](#bib.bib13)) like GPT4 have demonstrated the unprecedented
    ability of remarkable comprehension in human languages. However, these large models
    meet up with a substantial challenge of immense GPU memory usage in the inference,
    due to the model and computational complexity. This hinders deploying LLMs at
    scale to meet the thousands of demands for chatting with chatbots.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）（OpenAI, [2023](#bib.bib18)；Anthropic, [2023](#bib.bib3)；Jiang 等,
    [2023](#bib.bib13)）如 GPT4 展示了在人类语言理解上的前所未有的能力。然而，这些大型模型面临着在推理过程中巨大的 GPU 内存使用挑战，由于模型和计算复杂性，这阻碍了大规模部署
    LLM 以满足与聊天机器人进行聊天的数千个需求。
- en: '![Refer to caption](img/17650b4a34ec596f6d9d8c421e8d44ed.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/17650b4a34ec596f6d9d8c421e8d44ed.png)'
- en: 'Figure 1: Inference in the prefill phase: all models of different sizes have
    the prompts of 64 $\times$ 2k. LLM consumes huge GPU memory in the KV cache compared
    to the small model. PyramidInfer can reduce over 54% GPU memory usage in the KV
    cache while having more than 2x throughput.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：预填充阶段的推理：不同大小的所有模型具有 64 $\times$ 2k 的提示。与小模型相比，LLM 在 KV 缓存中消耗了巨大的 GPU 内存。PyramidInfer
    可以在 KV 缓存中减少超过 54% 的 GPU 内存使用，同时具有超过 2 倍的吞吐量。
- en: 'Different from training, models in the inference do not need to record the
    optimizer states, activations, or gradients. As LLMs are mostly Transformer-based
    auto-regressive models, the GPU memory usage mainly consists of two parts: model
    parameters and KV cache. KV cache presents the keys and values previously computed
    in the attention. We store the KV cache in the GPU memory and reuse it in future
    generations to avoid re-computation. The KV cache mechanism has been widely used
    to improve the inference speed (Touvron et al., [2023](#bib.bib23); Zhang et al.,
    [2022](#bib.bib25)). However, the KV cache consumes huge GPU memory, especially
    for LLMs. For example, in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ PyramidInfer:
    Pyramid KV Cache Compression for High-throughput LLM Inference"), for a model
    with 7 billion parameters, the parameters only consume 14 GB of memory but the
    KV cache requires around 72 GB. The KV cache has the potential to consume memory
    several times the size of the model. It demonstrates a great challenge that the
    throughput of LLM inference is constrained by how much data (KV cache) we can
    put in the GPU besides the model.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '与训练不同，推理中的模型不需要记录优化器状态、激活或梯度。由于 LLM 主要是基于 Transformer 的自回归模型，GPU 内存使用主要包括两个部分：模型参数和
    KV 缓存。KV 缓存呈现了在注意力中之前计算的键和值。我们将 KV 缓存存储在 GPU 内存中，并在未来生成中重用，以避免重新计算。KV 缓存机制已被广泛使用以提高推理速度（Touvron
    等, [2023](#bib.bib23)；Zhang 等, [2022](#bib.bib25)）。然而，KV 缓存消耗巨大的 GPU 内存，尤其是对于
    LLM。例如，在图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ PyramidInfer: Pyramid KV Cache
    Compression for High-throughput LLM Inference") 中，对于一个具有 70 亿个参数的模型，参数仅消耗 14 GB
    内存，但 KV 缓存需要大约 72 GB。KV 缓存有可能消耗比模型大几倍的内存。这表明 LLM 推理的吞吐量受到我们可以在 GPU 中放入多少数据（KV
    缓存）的限制，而不仅仅是模型。'
- en: '![Refer to caption](img/ee099e347c75408f2f0780722f099614.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ee099e347c75408f2f0780722f099614.png)'
- en: 'Figure 2: Comparison between PyramidInfer and other methods: (a) StreamingLLM
    only reserves the first and recent tokens thus losing memorization of the previous
    context. (b) H[2]O/Scissorhands compress the KV cache without difference for all
    the layers. They suffer great information loss by compressing too much in the
    shallow layers. (c) Different from the above methods that can only compress after
    the KV cache has been computed, PyramidInfer can compress the KV cache in the
    prefill phase. PyramidInfer only computes crucial keys and values to do inference
    thus reducing more GPU memory and bringing higher throughput.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：PyramidInfer 与其他方法的比较：（a）StreamingLLM 仅保留第一个和最近的 token，因此丧失了对之前上下文的记忆。（b）H[2]O/Scissorhands
    对所有层的 KV 缓存进行无差别压缩。它们通过在浅层压缩过多信息而遭受巨大信息丢失。（c）与上述方法只能在 KV 缓存计算后进行压缩不同，PyramidInfer
    可以在预填充阶段压缩 KV 缓存。PyramidInfer 仅计算关键的键和值进行推理，从而减少更多的 GPU 内存并带来更高的吞吐量。
- en: 'We break down LLM inference into two phases: prefill phase and generation phase
    (Brown et al., [2020](#bib.bib4); Radford et al., [2019](#bib.bib19)). In the
    prefill phase, the prompt is computed in parallel to generate the first token,
    and the initial KV cache is pre-filled. In the generation phase, the model decodes
    the next token one by one and appends the keys and values of the newly decoded
    token to the old KV cache. Recent studies Zhang et al. ([2023](#bib.bib26)); Liu
    et al. ([2023](#bib.bib16)); Ge et al. ([2023](#bib.bib9)) compress the KV cache
    to reduce GPU memory usage. However, as shown in Figure [2](#S1.F2 "Figure 2 ‣
    1 Introduction ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput
    LLM Inference"), they all only reduce the KV cache that has been already computed
    rather than reducing the KV cache to be computed. They have to prefill the initial
    KV cache before they can start to compress, which neglects the great GPU memory
    consumption of computing the initial KV cache, especially for longer prompts and
    larger models. If the model can not process the prompt in the prefill phase, these
    methods are no longer applicable as their compression starts in the generation
    phase. In this paper, we focus on how to further compress the KV cache in the
    prefill phase besides the generation phase. We give out our findings and then
    propose our method PyramidInfer inspired by these findings.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将LLM推理分解为两个阶段：预填充阶段和生成阶段（Brown et al., [2020](#bib.bib4); Radford et al.,
    [2019](#bib.bib19)）。在预填充阶段，提示被并行计算以生成第一个令牌，同时初始的KV缓存被预填充。在生成阶段，模型逐个解码下一个令牌，并将新解码令牌的键和值附加到旧的KV缓存中。近期研究
    Zhang et al. ([2023](#bib.bib26)); Liu et al. ([2023](#bib.bib16)); Ge et al.
    ([2023](#bib.bib9)) 压缩KV缓存以减少GPU内存使用。然而，如图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference")
    所示，它们仅减少已经计算的KV缓存，而不是减少待计算的KV缓存。它们必须在开始压缩之前预填充初始KV缓存，这忽略了计算初始KV缓存时对GPU内存的巨大消耗，尤其是对于较长的提示和更大的模型。如果模型无法在预填充阶段处理提示，那么这些方法就不再适用，因为它们的压缩从生成阶段开始。本文重点讨论如何在预填充阶段进一步压缩KV缓存，而不仅仅是在生成阶段。我们提出了我们的发现，并根据这些发现提出了我们的PyramidInfer方法。'
- en: During the training, all input tokens predict the tokens next to themselves
    in an one-to-one teacher-forcing way (Lamb et al., [2016](#bib.bib15)). During
    the inference, the tokens except for the last token no longer need to predict
    the next tokens but they still record this redundant information in keys and values.
    We call this Inference Context Redundancy (ICR) hypothesis. It inspires us to
    compress the KV cache by only computing the keys and values that record the context
    information.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，所有输入令牌都以一对一的教师强制方式预测自己旁边的令牌（Lamb et al., [2016](#bib.bib15)）。在推理过程中，除了最后一个令牌外，令牌不再需要预测下一个令牌，但它们仍在键和值中记录这些冗余信息。我们称之为推理上下文冗余（ICR）假设。它启发我们通过仅计算记录上下文信息的键和值来压缩KV缓存。
- en: Another challenge arises as the initial KV cache is reused multiple times for
    generating future tokens, necessitating careful retention of context information
    during compression. Inspired by the work (Liu et al., [2023](#bib.bib16)), we
    further explore what parts of the KV cache are always crucial for future generations.
    We observe that queries of recent tokens closer to the last token are more consistent
    in attending to the same context keys and values, denoted as the Pivotal Context
    (PvC). We call this phenomenon as Recent Attention Consistency (RAC). The consistency
    of attention weights in recent tokens indicates that we can leverage it as the
    oracle to select the crucial KV cache for future generations in advance.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是初始KV缓存被多次重用以生成未来的令牌，这要求在压缩过程中仔细保留上下文信息。受到工作（Liu et al., [2023](#bib.bib16)）的启发，我们进一步探讨KV缓存中哪些部分对未来生成始终至关重要。我们观察到，靠近最后一个令牌的最近令牌的查询在关注相同上下文键和值时更一致，称为关键上下文（PvC）。我们将这种现象称为最近注意力一致性（RAC）。最近令牌中注意力权重的一致性表明，我们可以利用它作为先知，以提前选择对未来生成至关重要的KV缓存。
- en: Based on our observations, we propose the PyramidInfer, an effective method
    of reducing the KV cache both in the prefill and generation phase by layer-wise
    selecting the PvCs. In PyramidInfer, the PvCs are gradually reduced as the layers
    get deeper where the KV cache is like a pyramid. We showcase the capability of
    PyramidInfer on a wide range of tasks using OpenCompass (Contributors, [2023](#bib.bib7))
    on models of different types and sizes. The results show that PyramidInfer has
    higher throughput than the full cache method Accelerate and Deepspeed by 2.2x
    and 1.4x, KV cache compression method H[2]O by 2.4x with over 54% less GPU memory
    in KV cache.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们的观察，我们提出了 PyramidInfer，这是一种有效的 KV 缓存减少方法，通过逐层选择 PvC 来减少预填充和生成阶段的 KV 缓存。在
    PyramidInfer 中，随着层数的加深，PvC 会逐渐减少，KV 缓存呈现出金字塔形态。我们展示了 PyramidInfer 在使用 OpenCompass（Contributors,
    [2023](#bib.bib7)）的各种任务中的能力，涵盖了不同类型和大小的模型。结果表明，PyramidInfer 在 KV 缓存方面的吞吐量比全缓存方法
    Accelerate 和 Deepspeed 高 2.2 倍和 1.4 倍，比 KV 缓存压缩方法 H[2]O 高 2.4 倍，同时 KV 缓存中的 GPU
    内存减少了超过 54%。
- en: 2 Related Work
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Due to the increasing demands for chatting with chatbots, efficient strategies
    are required to process thousands of queries to maximize the throughput. The fundamental
    way to improve the throughput is to put more data (larger batch) into the GPU
    memory to utilize the GPU parallelism better.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对聊天机器人的需求增加，需要高效的策略来处理成千上万的查询，以最大化吞吐量。提高吞吐量的根本方法是将更多的数据（更大的批量）放入 GPU 内存，以更好地利用
    GPU 并行性。
- en: Inference Parallelism
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 推理并行
- en: One way is to enlarge the GPU memory. We can borrow the techniques used in training
    to accelerate the inference, e.g., pipeline parallelism (Huang et al., [2019](#bib.bib11)),
    KV cache offload (Sheng et al., [2023](#bib.bib21)), etc. These methods leverage
    multiple GPUs or even RAM to make up bigger space for input data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是扩大 GPU 内存。我们可以借用训练中使用的技术来加速推理，例如，流水线并行（Huang et al., [2019](#bib.bib11)）、KV
    缓存卸载（Sheng et al., [2023](#bib.bib21)）等。这些方法利用多个 GPU 甚至 RAM 来提供更大的输入数据空间。
- en: KV Cache Reduction
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: KV 缓存减少
- en: However, if we have limited GPU memory, another way is to reduce the KV cache.
    For optimization in the CUDA, FlashAttention 2 (Dao, [2023](#bib.bib8)) reduces
    the number of reads/writes between GPU HBM and GPU on-chip SRAM. PagedAttention
    (Kwon et al., [2023](#bib.bib14)) borrows the virtual memory techniques to achieve
    near-zero waste in KV cache memory.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果 GPU 内存有限，另一种方法是减少 KV 缓存。针对 CUDA 的优化，FlashAttention 2（Dao, [2023](#bib.bib8)）减少了
    GPU HBM 和 GPU 内存芯片 SRAM 之间的读写次数。PagedAttention（Kwon et al., [2023](#bib.bib14)）借用了虚拟内存技术，以实现
    KV 缓存内存的接近零浪费。
- en: 'Besides CUDA methods, we can optimize the KV cache from the model itself. From
    Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ PyramidInfer: Pyramid KV Cache
    Compression for High-throughput LLM Inference"), StreamingLLM (Xiao et al., [2023](#bib.bib24))
    reserves the recent context to enable unlimited input by sacrificing memorization
    of the history. Other methods like H[2]O Zhang et al. ([2023](#bib.bib26)) and
    Scissorhands (Liu et al., [2023](#bib.bib16)) leverage the attention to compress
    the KV cache. However, they treat the compression of different layers as the same
    thing and can not compress in the prefill phase. Our method PyramidInfer takes
    the difference in layers into account and realizes the compression in both the
    prefill and generation phases, thus better reducing the KV cache while maintaining
    the generation quality.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '除了 CUDA 方法外，我们还可以从模型本身优化 KV 缓存。从图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ PyramidInfer:
    Pyramid KV Cache Compression for High-throughput LLM Inference") 中可以看到，StreamingLLM（Xiao
    et al., [2023](#bib.bib24)）保留了最近的上下文，通过牺牲历史记忆来实现无限输入。其他方法如 H[2]O Zhang et al.
    ([2023](#bib.bib26)) 和 Scissorhands（Liu et al., [2023](#bib.bib16)）利用注意力来压缩 KV
    缓存。然而，它们将不同层的压缩视为相同的，并且无法在预填充阶段进行压缩。我们的方法 PyramidInfer 考虑了层间的差异，实现了在预填充和生成阶段的压缩，从而在保持生成质量的同时更好地减少
    KV 缓存。'
- en: '![Refer to caption](img/8f8494f2f6f543d5b650f16734137858.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8f8494f2f6f543d5b650f16734137858.png)'
- en: 'Figure 3: For each layer, we reserve the keys and values with top-$p$.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：对于每一层，我们保留了前 $p$ 的键和值。
- en: '![Refer to caption](img/0a8a7e5c22be73c5f9896a477e657c72.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0a8a7e5c22be73c5f9896a477e657c72.png)'
- en: 'Figure 4: The perplexity standard deviations when only PvCs are reserved at
    each layer.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：仅保留每层的 PvC 时的困惑度标准差。
- en: 3 Observation and Insight
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 观察与洞察
- en: We verify the hypotheses of Inference Context Redundancy and Recent Attention
    Consistency, which inspire us to design the method PyramidInfer.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们验证了推理上下文冗余和最近注意力一致性的假设，这些假设启发我们设计了 PyramidInfer 方法。
- en: 3.1 Inference Context Redundancy
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 推理上下文冗余
- en: Different from teacher-forcing in the training, only the last token has to predict
    the next token in the inference. We suppose there exist keys and values of the
    context that record the redundant information to predict the next token in the
    training but are not useful for inference. We call this the Inference Context
    Redundancy (ICR) hypothesis.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练中的教师强制不同，在推理中只有最后一个标记需要预测下一个标记。我们假设存在记录冗余信息的上下文键和值，这些信息在训练中用于预测下一个标记，但在推理中并不有用。我们称之为推理上下文冗余（ICR）假设。
- en: 3.1.1 Pivotal Context
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 关键上下文
- en: To verify the hypothesis, we design an experiment based on 40-layer LLaMA 2-13B
    to find out if this redundancy exists in the KV cache. In this experiment, we
    only reserve a proportion of keys and values of certain layers while other layers
    remain fixed and see how the perplexity of model output will change. This selected
    proportion consists of the important keys and values with the top-$p$ attention
    weights, denoted as the Pivotal Context (PvC).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这一假设，我们设计了一个基于 40 层 LLaMA 2-13B 的实验，以找出 KV 缓存中是否存在这种冗余。在这个实验中，我们仅保留某些层级的键和值的一个比例，而其他层级保持不变，并观察模型输出的困惑度将如何变化。这个选择的比例由注意力权重最高的关键键和值组成，称为关键上下文（PvC）。
- en: 'As shown in Figure [3](#S2.F3 "Figure 3 ‣ KV Cache Reduction ‣ 2 Related Work
    ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference"),
    we show that, for most of the layers, as the retention ratio of PvC decreases,
    the perplexity of the output will increase. However, as the layer becomes deeper
    (larger index), we find that the influence of shorter PvC tends to be smaller.
    For example, after Layer 27, the perplexity remains stable even with 80% keys
    and values are evicted. In Figure [4](#S2.F4 "Figure 4 ‣ KV Cache Reduction ‣
    2 Related Work ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput
    LLM Inference"), we compute the standard deviations across the retention ratios
    of all the layers and observe they obey a power law distribution. It indicates
    most of the keys and values should be retained as the layers are shallow and the
    redundancy in the KV cache sharply increases as the layers become deeper. This
    growing redundancy guides us to minimize the KV cache while maximizing the performance.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [3](#S2.F3 "Figure 3 ‣ KV Cache Reduction ‣ 2 Related Work ‣ PyramidInfer:
    Pyramid KV Cache Compression for High-throughput LLM Inference") 所示，我们展示了对于大多数层级，随着
    PvC 保留比例的下降，输出的困惑度会增加。然而，随着层级的加深（索引增大），我们发现较短的 PvC 的影响往往较小。例如，在第 27 层之后，即使 80%
    的键和值被逐出，困惑度也保持稳定。在图 [4](#S2.F4 "Figure 4 ‣ KV Cache Reduction ‣ 2 Related Work
    ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference")
    中，我们计算了所有层级保留比例的标准偏差，并观察到它们服从幂律分布。这表明大多数键和值在层级较浅时应被保留，而随着层级的加深，KV 缓存中的冗余显著增加。这种冗余的增长指导我们在最大化性能的同时最小化
    KV 缓存。'
- en: 3.1.2 Discussion
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 讨论
- en: How does the model gather information to predict the next token?
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型如何收集信息以预测下一个标记？
- en: 'Generating the next token can be considered as a process that the last token
    gathers the information from the context based on the attention weights. In Figure
    [3](#S2.F3 "Figure 3 ‣ KV Cache Reduction ‣ 2 Related Work ‣ PyramidInfer: Pyramid
    KV Cache Compression for High-throughput LLM Inference"), we observe from the
    view of the last token. In the shallow layer, the information in the context is
    distributed in most of the tokens in the context. As the layer goes deeper, only
    limited keys and values contribute to the next token prediction.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '生成下一个标记可以被视为一个过程，其中最后一个标记根据注意力权重从上下文中收集信息。在图 [3](#S2.F3 "Figure 3 ‣ KV Cache
    Reduction ‣ 2 Related Work ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput
    LLM Inference") 中，我们从最后一个标记的视角进行观察。在浅层中，上下文中的信息分布在上下文中的大多数标记中。随着层级的加深，只有有限的键和值对下一个标记的预测做出贡献。'
- en: 'The inference process differs from training because all the input tokens predict
    the next tokens. At this time, keys and values store two kinds of information:
    1) the information to predict what the token is next to it; 2) the context information
    for future tokens to leverage. So far, we have verified that PvCs are the crucial
    keys and values that are useful for inference. On the other hand, we want to verify
    the non-PvC that may play a more important role in teacher-forcing prediction
    instead of being the context. As non-PvCs are trivial in PyramidInfer, we discuss
    it in the Appendix [3.2.2](#S3.SS2.SSS2.Px4 "Further Verification of ICR about
    the Role of Non-PvCs ‣ 3.2.2 Discussion ‣ 3.2 Recent Attention Consistency ‣ 3
    Observation and Insight ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput
    LLM Inference").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '推断过程与训练不同，因为所有输入标记预测下一个标记。此时，键和值存储两种信息：1) 预测下一个标记的信息；2) 供未来标记利用的上下文信息。到目前为止，我们已经验证了
    PvCs 是对推断有用的关键和值。另一方面，我们希望验证非 PvC 是否在教师强迫预测中发挥了更重要的作用，而不是作为上下文。由于非 PvCs 在 PyramidInfer
    中是微不足道的，我们在附录 [3.2.2](#S3.SS2.SSS2.Px4 "进一步验证非 PvCs 在 ICR 中的作用 ‣ 3.2.2 讨论 ‣ 3.2
    最近的注意力一致性 ‣ 3 观察与洞察 ‣ PyramidInfer: 高吞吐量 LLM 推断的金字塔 KV 缓存压缩") 中讨论了这一点。'
- en: '![Refer to caption](img/5a19717a8ead9b65c92ae25fe7076eae.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5a19717a8ead9b65c92ae25fe7076eae.png)'
- en: (a) Separate PvC overlap ratios of recent tokens.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 最近标记的单独 PvC 重叠比率。
- en: '![Refer to caption](img/07bea5f0f44683a650af236fc624d1a0.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/07bea5f0f44683a650af236fc624d1a0.png)'
- en: (b) Ensemble PvC overlap ratios of recent tokens.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 最近标记的集合 PvC 重叠比率。
- en: 'Figure 5: PvC overlap ratio heatmap.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: PvC 重叠比率热图。'
- en: 3.2 Recent Attention Consistency
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 最近的注意力一致性
- en: In the verification of ICR, we use the attention weights to find PvCs. However,
    in an attention layer, there are several attention weights for one token $x_{i}$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ICR 的验证中，我们使用注意力权重来查找 PvCs。然而，在一个注意力层中，一个标记 $x_{i}$ 有多个注意力权重。
- en: 3.2.1 PvC Consistency
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 PvC 一致性
- en: We convert this goal to finding if there exist keys and values that are frequently
    attended by subsequent tokens. First of all, we define a relative distance of
    how far the context token $x_{i}$) as the PvC selection baseline.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这一目标转换为寻找是否存在被后续标记频繁关注的键和值。首先，我们定义上下文标记 $x_{i}$ 的相对距离作为 PvC 选择基线。
- en: 'After the setup, we want to measure how much the overlap will be that the PvCs
    of recent tokens are consistent with the PvC of the last token. If there is overlap,
    we can infer the intersection should be the shared PvC where many subsequent tokens
    are consistently interested. Thus for each layer $l$ of PvCs as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 设置完成后，我们希望测量最近的标记的 PvC 与最后一个标记的 PvC 一致性有多少。如果存在重叠，我们可以推断交集应该是许多后续标记一致关注的共享 PvC。因此，对于每一层
    $l$ 的 PvCs，如下所示：
- en: '|  | $1$2 |  | (1) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: 'From the results in Figure [5(a)](#S3.F5.sf1 "In Figure 5 ‣ How does the model
    gather information to predict the next token? ‣ 3.1.2 Discussion ‣ 3.1 Inference
    Context Redundancy ‣ 3 Observation and Insight ‣ PyramidInfer: Pyramid KV Cache
    Compression for High-throughput LLM Inference"), the recent tokens in $S_{r}$,
    we only have about 83% PvC contributes to the prediction, which suffers a great
    context information loss.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '从图 [5(a)](#S3.F5.sf1 "在图 5 中 ‣ 模型如何收集信息以预测下一个标记？ ‣ 3.1.2 讨论 ‣ 3.1 推断上下文冗余 ‣
    3 观察与洞察 ‣ PyramidInfer: 高吞吐量 LLM 推断的金字塔 KV 缓存压缩") 的结果来看，在 $S_{r}$ 中的最近标记中，我们只有大约
    83% 的 PvC 贡献于预测，这导致了巨大的上下文信息丢失。'
- en: 'Fortunately, the PvC selections from recent tokens have high consistency and
    we can integrate multiple tokens to select the shared ones. In Figure [5(b)](#S3.F5.sf2
    "In Figure 5 ‣ How does the model gather information to predict the next token?
    ‣ 3.1.2 Discussion ‣ 3.1 Inference Context Redundancy ‣ 3 Observation and Insight
    ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference"),
    we integrate the attention weights by averaging weights of subsequent $[d,d+10\%]$
    which is 20% ahead.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '幸运的是，最近标记的 PvC 选择具有很高的一致性，我们可以整合多个标记来选择共享的标记。在图 [5(b)](#S3.F5.sf2 "在图 5 中 ‣
    模型如何收集信息以预测下一个标记？ ‣ 3.1.2 讨论 ‣ 3.1 推断上下文冗余 ‣ 3 观察与洞察 ‣ PyramidInfer: 高吞吐量 LLM
    推断的金字塔 KV 缓存压缩") 中，我们通过对 $[d,d+10\%]$ 的后续标记权重进行平均来整合注意力权重，这在 20% 之前。'
- en: '![Refer to caption](img/506bef3044752d94bce243dd0767ffd3.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/506bef3044752d94bce243dd0767ffd3.png)'
- en: 'Figure 6: The overview of the PyramidInfer.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: PyramidInfer 的概述。'
- en: Algorithm 1 One forward pass in PyramidInfer
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 PyramidInfer 中的一次前向传播
- en: 'Input: KV cache $KV$'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '输入: KV 缓存 $KV$'
- en: 'Output: updated KV cache $KV$'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：更新后的KV缓存 $KV$
- en: for layer $l$
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于层 $l$
- en: 3.2.2 Discussion
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 讨论
- en: Why do the deeper layers tend to have lower PvC overlap ratios?
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么深层的PvC重叠比例往往较低？
- en: 'If we check overlap ratios along the layer axis, we find that only shallow
    layers have relatively high ratios. It is because in deeper layers there is context
    redundancy: Only a small number of keys and values have high weights that are
    always selected as PvCs; The others have similar low weights so they are not always
    selected, which results in lower overlap ratios. This phenomenon is consistent
    with the power law distribution observed in ICR, which is further discussed later.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们沿着层轴检查重叠比例，我们会发现只有浅层具有相对较高的比例。这是因为在深层中存在上下文冗余：只有少量键和值具有高权重，始终被选为PvC；其他键和值具有类似的低权重，因此并不总是被选中，这导致了较低的重叠比例。这种现象与ICR中观察到的幂律分布一致，后续将进一步讨论。
- en: Context information is mostly stored in the shared PvCs.
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 上下文信息主要存储在共享PvC中。
- en: 'In Figure [5(b)](#S3.F5.sf2 "In Figure 5 ‣ How does the model gather information
    to predict the next token? ‣ 3.1.2 Discussion ‣ 3.1 Inference Context Redundancy
    ‣ 3 Observation and Insight ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput
    LLM Inference"), the consistent PvC overlap ratios from small $d$ show that wherever
    recent tokens are, they only leverage nearly the same number of keys and values
    in the context. These keys and values, also known as shared PvCs, store most of
    the context information.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[5(b)](#S3.F5.sf2 "图5 ‣ 模型如何收集信息以预测下一个标记？ ‣ 3.1.2 讨论 ‣ 3.1 推理上下文冗余 ‣ 3 观察与洞察
    ‣ PyramidInfer：高吞吐量LLM推理的金字塔KV缓存压缩")中，从小 $d$ 处观察到的一致PvC重叠比例表明，无论最近的标记在哪里，它们仅利用上下文中的几乎相同数量的键和值。这些键和值，也称为共享PvC，存储了大部分上下文信息。
- en: The Association between ICR and RAC
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ICR与RAC之间的关联
- en: 'In Section [3.2.2](#S3.SS2.SSS2 "3.2.2 Discussion ‣ 3.2 Recent Attention Consistency
    ‣ 3 Observation and Insight ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput
    LLM Inference"), we mention the phenomenon that deeper layers have lower PvC overlap
    ratios is consistent with the power law distribution observed in Figure [4](#S2.F4
    "Figure 4 ‣ KV Cache Reduction ‣ 2 Related Work ‣ PyramidInfer: Pyramid KV Cache
    Compression for High-throughput LLM Inference"). This is because, as we observe
    alone the layer index of the heatmap, we find that the color quickly deepens by
    a large gap where the depth change is approximate to the power law distribution.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在[3.2.2](#S3.SS2.SSS2 "3.2.2 讨论 ‣ 3.2 最近的注意力一致性 ‣ 3 观察与洞察 ‣ PyramidInfer：高吞吐量LLM推理的金字塔KV缓存压缩")节中，我们提到深层的PvC重叠比例较低的现象与图[4](#S2.F4
    "图4 ‣ KV缓存减少 ‣ 2 相关工作 ‣ PyramidInfer：高吞吐量LLM推理的金字塔KV缓存压缩")中观察到的幂律分布一致。这是因为，当我们仅观察热图的层索引时，我们发现颜色在深度变化接近幂律分布的地方迅速加深。
- en: The insight behind these two power law distributions is the same. The high redundancy
    in deeper layers indicates that most of the keys and values are useless for inference.
    These non-PvCs all have similarly low attention weights, resulting in limited
    influence on the perplexity and few opportunities to be selected as PvCs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种幂律分布背后的洞察是相同的。深层中的高冗余性表明大多数键和值对推理是无用的。这些非PvC都有类似的低注意力权重，导致对困惑度的影响有限，并且被选为PvC的机会较少。
- en: Further Verification of ICR about the Role of Non-PvCs
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关于非PvC角色的ICR进一步验证
- en: 'To complete the verification of ICR, we have to verify the non-PvCs are redundant
    because they carry the information of predicting the tokens next to themselves
    instead of context information. In Figure [7](#S3.F7 "Figure 7 ‣ Further Verification
    of ICR about the Role of Non-PvCs ‣ 3.2.2 Discussion ‣ 3.2 Recent Attention Consistency
    ‣ 3 Observation and Insight ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput
    LLM Inference"), to better illustrate, we divide the keys and values of one layer
    into two main parts, PvCs and non-PvCs. For the PvCs, we further divide them into
    shared PvCs and non-shared PvCs.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成对ICR的验证，我们必须验证非PvC是冗余的，因为它们携带的是预测紧挨在自己后的标记的信息，而不是上下文信息。在图[7](#S3.F7 "图7
    ‣ 关于非PvC角色的ICR进一步验证 ‣ 3.2.2 讨论 ‣ 3.2 最近的注意力一致性 ‣ 3 观察与洞察 ‣ PyramidInfer：高吞吐量LLM推理的金字塔KV缓存压缩")中，为了更好地说明，我们将一层的键和值分为两个主要部分：PvC和非PvC。对于PvC，我们进一步将其划分为共享PvC和非共享PvC。
- en: '![Refer to caption](img/0a15cfebce363d124efbf86991cfdef0.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0a15cfebce363d124efbf86991cfdef0.png)'
- en: 'Figure 7: The composition of the keys and values of one layer.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：一层的键和值的组成。
- en: 'In Figure [5(a)](#S3.F5.sf1 "In Figure 5 ‣ How does the model gather information
    to predict the next token? ‣ 3.1.2 Discussion ‣ 3.1 Inference Context Redundancy
    ‣ 3 Observation and Insight ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput
    LLM Inference"), we demonstrate that there is an 87% overlap between tokens and
    the last token in terms of PvC, as denoted as shared PvC. We first identify the
    role of the remaining 13% of keys and values where these non-shared PvCs are not
    used in PyramidInfer. The non-shared PvCs are also assigned high attention weights
    by the current token, which means they are useful for predicting the token next
    to the current token. It is interesting to see what these non-shared PvCs are
    from the perspective of the subsequent tokens: Will they also consider these keys
    and values important?'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '在图[5(a)](#S3.F5.sf1 "图5 ‣ 模型如何收集信息以预测下一个token？ ‣ 3.1.2 讨论 ‣ 3.1 推理上下文冗余 ‣ 3
    观察与见解 ‣ PyramidInfer: 高吞吐量LLM推理的金字塔KV缓存压缩")中，我们展示了token与最后一个token在PvC方面有87%的重叠，称为共享PvC。我们首先识别剩余13%键和值的作用，其中这些非共享PvC未在PyramidInfer中使用。非共享PvC也被当前token分配了高关注权重，这意味着它们对预测下一个token非常有用。从后续token的角度来看，观察这些非共享PvC是很有趣的：它们是否也认为这些键和值很重要？'
- en: We use the recent sequence ratio of 20% to select the shared PvCs. We extract
    non-shared PvCs from the tokens with $10\%<d<20\%$.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用最近的序列比例20%来选择共享PvC。从$10\%<d<20\%$的token中提取非共享PvC。
- en: 'From Figure [8](#S3.F8 "Figure 8 ‣ Further Verification of ICR about the Role
    of Non-PvCs ‣ 3.2.2 Discussion ‣ 3.2 Recent Attention Consistency ‣ 3 Observation
    and Insight ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM
    Inference"), we can draw conclusions for these three parts of the KV cache:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '从图[8](#S3.F8 "图8 ‣ 关于非-PvC作用的进一步验证 ‣ 3.2.2 讨论 ‣ 3.2 最近注意力一致性 ‣ 3 观察与见解 ‣ PyramidInfer:
    高吞吐量LLM推理的金字塔KV缓存压缩")中，我们可以得出KV缓存这三部分的结论：'
- en: '1.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The shared PvCs are the keys and values that subsequent tokens collectively
    pay attention to.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 共享PvC是后续token集体关注的键和值。
- en: '2.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The non-shared PvCs seldom appear in non-shared PvCs of other tokens. It means
    that non-shared PvCs are mostly highly interested in by the current token, with
    less attention from subsequent tokens. They are mainly used to predict the token
    next to themself in a teacher-forcing way, which is especially useful in training.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非共享PvC在其他token的非共享PvC中很少出现。这意味着非共享PvC主要受到当前token的高度关注，后续token关注较少。它们主要用于以教师强制方式预测下一个token，这在训练中尤其有用。
- en: '3.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Among the non-PvCs, a significant portion is occupied by non-shared PvCs of
    other tokens.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在非PvC中，大量被其他token的非共享PvC所占据。
- en: So far, we have completely verified the Inference Context Redundancy hypothesis
    that the tokens except for the last token no longer need to predict the next tokens
    but they still record this redundant information to predict the next tokens in
    keys and values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经完全验证了推理上下文冗余假设，即除了最后一个token之外的token不再需要预测下一个token，但它们仍然记录这些冗余信息以便在键和值中预测下一个token。
- en: '![Refer to caption](img/a51bffb7b8253703c20e849ef54c836d.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a51bffb7b8253703c20e849ef54c836d.png)'
- en: 'Figure 8: The overlap ratios between non-shared PvCs and non-shared PvCs of
    other tokens (blue) and the overlap ratios between non-shared PvCs and non-PvCs
    of other tokens (orange).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：非共享PvC与其他token的非共享PvC（蓝色）之间的重叠比率和非共享PvC与其他token的非PvC（橙色）之间的重叠比率。
- en: 4 Layer-wise PvC Selection
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 层级PvC选择
- en: Based on the observations, we design the PyramidInfer, a method to highly increase
    the inference throughput by layer-wise selecting the PvCs to compress the KV cache
    for each layer.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 根据观察，我们设计了PyramidInfer，这是一种通过逐层选择PvC来压缩每层的KV缓存，从而大幅提高推理吞吐量的方法。
- en: 4.1 Method
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 方法
- en: 'As shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ PyramidInfer: Pyramid
    KV Cache Compression for High-throughput LLM Inference"), PyramidInfer can not
    only reduce the KV cache in the generation phase but also in the prefill phase
    without computing the complete keys and values of the prompt for all the layers.
    Following the inference process, we introduce the PyramidInfer in the prefill
    phase and generation phase separately and see how PyramidInfer can save lots of
    GPU memory by carefully selecting the PvCs.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ PyramidInfer: Pyramid KV Cache Compression
    for High-throughput LLM Inference") 所示，PyramidInfer 不仅可以减少生成阶段的 KV 缓存，还可以在预填充阶段减少，而无需计算提示的所有层的完整键和值。在推理过程中，我们分别介绍了
    PyramidInfer 在预填充阶段和生成阶段的应用，看看 PyramidInfer 如何通过精心选择 PvCs 节省大量的 GPU 内存。'
- en: Prefill Phase
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 预填充阶段
- en: In the prefill phase, we have to process the prompt to prefill the initial KV
    cache. Different from the common inference process that reserves all keys and
    values of the prompt, PyramidInfer only reserves the PvCs of each layer as the
    initial KV cache.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在预填充阶段，我们必须处理提示以预填充初始 KV 缓存。与保留提示的所有键和值的常见推理过程不同，PyramidInfer 仅保留每一层的 PvCs 作为初始
    KV 缓存。
- en: Similarly, we divide the input sequence into recent sequence $S_{r}$ to reduce
    the length of PvCs in deeper layers. Therefore, the PvCs of the deeper layers
    are shorter and the KV cache becomes a "pyramid".
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将输入序列划分为最近序列 $S_{r}$ 以减少更深层次的 PvCs 长度。因此，更深层次的 PvCs 较短，KV 缓存变成了“金字塔”。
- en: The layer-wise PvC selection saves much more GPU memory than other methods computing
    the whole prompt in the prefill phase. Besides the prefill phase, PyramidInfer
    continues to boost efficiency in the generation phase because LLMs only need to
    reuse a smaller initial KV cache.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 层级 PvC 选择比其他在预填充阶段计算整个提示的方法节省了更多的 GPU 内存。除了预填充阶段，PyramidInfer 还在生成阶段继续提升效率，因为
    LLM 只需要重用较小的初始 KV 缓存。
- en: Generation Phase
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 生成阶段
- en: 'As we have reserved the initial PvCs as the KV cache, what we should do in
    the generation phase is to update these PvCs according to the new recent tokens.
    As shown in Figure [6](#S3.F6.1 "Figure 6 ‣ 3.2.1 PvC Consistency ‣ 3.2 Recent
    Attention Consistency ‣ 3 Observation and Insight ‣ PyramidInfer: Pyramid KV Cache
    Compression for High-throughput LLM Inference"), we maintain a sliding recent
    window to update the newly generated token to be new recent tokens. Based on the
    new $S_{r}$, we update the PvCs of the KV cache where the operation is the same
    as the prefill phase. By controlling the length of the PvC of each layer, we can
    easily tune the compression ratio and even support unlimited input like StreamingLLM
    by maintaining a fixed number of PvCs in the KV cache.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '由于我们已将初始 PvCs 保留为 KV 缓存，在生成阶段我们需要根据新的最近标记更新这些 PvCs。如图 [6](#S3.F6.1 "Figure
    6 ‣ 3.2.1 PvC Consistency ‣ 3.2 Recent Attention Consistency ‣ 3 Observation and
    Insight ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference")
    所示，我们维护一个滑动最近窗口，以将新生成的标记更新为新的最近标记。根据新的 $S_{r}$，我们更新 KV 缓存中的 PvCs，其操作与预填充阶段相同。通过控制每层
    PvC 的长度，我们可以轻松调整压缩比，甚至通过在 KV 缓存中保持固定数量的 PvCs 来支持像 StreamingLLM 这样的无限输入。'
- en: '![Refer to caption](img/25938c46bbf1a3e1d7253b2ef9cf068b.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/25938c46bbf1a3e1d7253b2ef9cf068b.png)'
- en: 'Figure 9: Benchmark results of comparison between models with full cache, "local"
    strategy, and PyramidInfer.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 模型全缓存、“局部”策略与 PyramidInfer 之间的比较基准结果。'
- en: 5 Evaluation
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估
- en: 'Table 1: The evaluation of inference methods using an A100 80GB GPU on LLaMA
    2-13B and 70B. Length: prefill length + generation length. Bsz: batch size. KV
    mem.: GPU memory usage (GB) of the KV cache. Thr.: throughput (token/s)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 在 LLaMA 2-13B 和 70B 上使用 A100 80GB GPU 评估推理方法。长度: 预填充长度 + 生成长度。Bsz: 批处理大小。KV
    内存: KV 缓存的 GPU 内存使用量 (GB)。吞吐量: 吞吐量 (标记/秒)'
- en: '| Model | Bsz | Length | Method | KV Mem. | Thr. |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Bsz | 长度 | 方法 | KV 内存 | 吞吐量 |'
- en: '| 13B | 32 | 512+256 | Accelerate | 24.2 (100%) | 621 (1.0x) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 13B | 32 | 512+256 | 加速 | 24.2 (100%) | 621 (1.0x) |'
- en: '| Deepspeed | 24.2 (100%) | 934 (1.5x) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Deepspeed | 24.2 (100%) | 934 (1.5x) |'
- en: '| H[2]O | 21.6 (89.2%) | 584 (0.9x) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| H[2]O | 21.6 (89.2%) | 584 (0.9x) |'
- en: '| PyramidInfer | 11.0 (45.4%) | 1389 (2.2x) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| PyramidInfer | 11.0 (45.4%) | 1389 (2.2x) |'
- en: '| 70B | 8 | 256+128 | Accelerate/ Deepspeed/H[2]O | OOM | - |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 70B | 8 | 256+128 | 加速/Deepspeed/H[2]O | OOM | - |'
- en: '| PyramidInfer | 4.2 | 20 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| PyramidInfer | 4.2 | 20 |'
- en: '![Refer to caption](img/51fe1a209ffacba2927070e4612d2023.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/51fe1a209ffacba2927070e4612d2023.png)'
- en: 'Figure 10: $S_{r}$ ratio ablation study.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: $S_{r}$ 比率消融研究。'
- en: 5.1 Basic Evaluation
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基本评估
- en: We evaluate PyramidInfer on various tasks and models to showcase that PyramidInfer
    can largely reduce the GPU memory and increase the throughput while maintaining
    the generation quality.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在各种任务和模型上评估 PyramidInfer，以展示 PyramidInfer 能在保持生成质量的同时，大幅减少 GPU 内存并提高吞吐量。
- en: Experimental Setup
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 实验设置
- en: 'We choose four kinds of scenarios: 1) Language modeling: we measure the perplexity
    on wikitext-v2 (Merity et al., [2016](#bib.bib17)). 2) LLM benchmarks: we evaluate
    on MMLU (Hendrycks et al., [2021](#bib.bib10)) and BBH (Srivastava et al., [2022](#bib.bib22))
    for language understanding, GSM8K (Cobbe et al., [2021](#bib.bib6)) for mathematical
    reasoning, HumanEval (Chen et al., [2021](#bib.bib5)) for coding. 3) Conversation:
    We evaluate on MT-Bench (Zheng et al., [2023](#bib.bib27)) to see how PyramidInfer
    can handle multi-turn conversation. 4) Long context: we evaluate on long text
    summarization of the LEval (An et al., [2023](#bib.bib2)) to see if PyramidInfer
    can maintain the quality while accepting longer input. We evaluate these tasks
    on LLaMA 2 (Touvron et al., [2023](#bib.bib23)), LLaMA 2-Chat, Vicuna 1.5-16k
    (Zheng et al., [2023](#bib.bib27)) and CodeLLaMA (Rozière et al., [2023](#bib.bib20))
    with different sizes (7B, 13B, 34B and 70B) ¹¹1We quantize the 34B and 70B models
    to INT8 data type to reduce the computational cost.. We set the full KV cache
    method as the baseline. Besides that, we also include the "local" strategy as
    another baseline that reserves only the recent KV cache.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了四种场景：1) 语言建模：我们在 wikitext-v2 上测量困惑度（Merity et al., [2016](#bib.bib17)）。2)
    LLM 基准：我们在 MMLU（Hendrycks et al., [2021](#bib.bib10)）和 BBH（Srivastava et al.,
    [2022](#bib.bib22)）上评估语言理解能力，在 GSM8K（Cobbe et al., [2021](#bib.bib6)）上评估数学推理能力，在
    HumanEval（Chen et al., [2021](#bib.bib5)）上评估编码能力。3) 对话：我们在 MT-Bench（Zheng et al.,
    [2023](#bib.bib27)）上评估 PyramidInfer 如何处理多轮对话。4) 长上下文：我们在 LEval（An et al., [2023](#bib.bib2)）的长文本摘要上评估
    PyramidInfer 是否能在接受更长输入的同时保持质量。我们在不同规模（7B、13B、34B 和 70B） 的 LLaMA 2（Touvron et
    al., [2023](#bib.bib23)）、LLaMA 2-Chat、Vicuna 1.5-16k（Zheng et al., [2023](#bib.bib27)）和
    CodeLLaMA（Rozière et al., [2023](#bib.bib20)）上进行评估。我们将 34B 和 70B 模型量化为 INT8 数据类型以减少计算成本。我们将完整
    KV 缓存方法作为基线。此外，我们还包括了“局部”策略作为另一个基线，它仅保留最近的 KV 缓存。
- en: In addition, we showcase how much PyramidInfer can save GPU memory and improve
    the throughput. We compare the efficiency of PyramidInfer with other full cache
    methods, including Accelerate (HuggingFace, [2021](#bib.bib12)), Deepspeed²²2[https://github.com/microsoft/DeepSpeedExamples/tree/master/inference](https://github.com/microsoft/DeepSpeedExamples/tree/master/inference)
    (Aminabadi et al., [2022](#bib.bib1)). We also select H[2]O³³3[https://github.com/FMInference/H2O](https://github.com/FMInference/H2O)
    (Zhang et al., [2023](#bib.bib26)), a KV cache compression method, as another
    baseline. It is noted that PyramidInfer is orthogonal to the non-KV-compression
    methods like Deepspeed to improve efficiency further.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们展示了 PyramidInfer 能节省多少 GPU 内存并提高吞吐量。我们将 PyramidInfer 的效率与其他完整缓存方法进行比较，包括
    Accelerate（HuggingFace, [2021](#bib.bib12)）、Deepspeed²²2[https://github.com/microsoft/DeepSpeedExamples/tree/master/inference](https://github.com/microsoft/DeepSpeedExamples/tree/master/inference)（Aminabadi
    et al., [2022](#bib.bib1)）。我们还选择了 H[2]O³³3[https://github.com/FMInference/H2O](https://github.com/FMInference/H2O)（Zhang
    et al., [2023](#bib.bib26)），作为另一基线。需要注意的是，PyramidInfer 与像 Deepspeed 这样的非 KV 压缩方法是正交的，可以进一步提高效率。
- en: Benchmark Result
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基准结果
- en: 'In Figure [9](#S4.F9 "Figure 9 ‣ Generation Phase ‣ 4.1 Method ‣ 4 Layer-wise
    PvC Selection ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput
    LLM Inference"), we evaluate the LLMs with different compression ratios. We show
    that PyramidInfer maintains the generation quality with much less GPU memory compared
    with the full cache baseline. PyramidInfer also outperforms the "local" strategy
    with a large gap across different types and sizes of models and tasks.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [9](#S4.F9 "Figure 9 ‣ Generation Phase ‣ 4.1 Method ‣ 4 Layer-wise PvC
    Selection ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM
    Inference") 中，我们评估了不同压缩比的 LLM。我们展示了 PyramidInfer 在 GPU 内存大幅减少的情况下仍能保持生成质量，并且 PyramidInfer
    在不同类型和规模的模型和任务中，相比于“局部”策略有着显著的优势。'
- en: In the LEval that tests the long context ability, we show that the "local" strategy
    that is similar to the technique used in StreamingLLM causes a huge decline in
    memorization of history. PyramidInfer can accept longer input with less GPU memory
    without sacrificing too much performance.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试长上下文能力的 LEval 中，我们展示了与 StreamingLLM 使用的技术类似的“局部”策略会导致历史记忆大幅下降。PyramidInfer
    能在不牺牲太多性能的情况下，使用更少的 GPU 内存接受更长的输入。
- en: Efficiency Result
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 效率结果
- en: 'In Table [10](#S5.F10 "Figure 10 ‣ 5 Evaluation ‣ PyramidInfer: Pyramid KV
    Cache Compression for High-throughput LLM Inference"), we fix the input length
    and the batch size. For LLaMA 2-13B, PyramidInfer showcases 2.24x throughput than
    full cache using Accelerate with 54.6% less GPU memory in the KV cache. For LLaMA
    2-70B, PyramidInfer can still generate in the prefill phase compared to other
    me. Existing KV cache compression methods like H[2]O can not even process the
    prompt and strike the OOM before the start of compression.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [10](#S5.F10 "图10 ‣ 5 评估 ‣ PyramidInfer: 高吞吐量LLM推理的金字塔KV缓存压缩") 中，我们固定了输入长度和批量大小。对于LLaMA
    2-13B，PyramidInfer展示了比使用Accelerate的完整缓存高出2.24倍的吞吐量，同时KV缓存的GPU内存减少了54.6%。对于LLaMA
    2-70B，PyramidInfer在预填充阶段仍能生成内容，而其他方法则不能处理提示，并在压缩开始之前发生OOM。'
- en: 'In Table [2](#S5.T2 "Table 2 ‣ Efficiency Result ‣ 5.1 Basic Evaluation ‣ 5
    Evaluation ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM
    Inference"), we exhaust the memory of an 80GB A100 GPU to test the maximum throughput
    by maximizing the batch sizes. PyramidInfer enables more than 2x batch size than
    others and has higher throughput than full cache methods Accelerate and Deepspeed
    by 2.8x and 1.7x, KV cache compression method H[2]O by 2.1x. PyramidInfer can
    also be utilized to enhance Deepspeed by increasing the throughput by 1.9x.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [2](#S5.T2 "表2 ‣ 效率结果 ‣ 5.1 基本评估 ‣ 5 评估 ‣ PyramidInfer: 高吞吐量LLM推理的金字塔KV缓存压缩")
    中，我们耗尽了一台80GB A100 GPU的内存，通过最大化批量大小来测试最大吞吐量。PyramidInfer使批量大小比其他方法大于2倍，并且比完整缓存方法Accelerate和Deepspeed分别高出2.8倍和1.7倍，比KV缓存压缩方法H[2]O高出2.1倍。PyramidInfer还可以通过将吞吐量提高1.9倍来增强Deepspeed。'
- en: 'Table 2: We exhaust the memory of an A100 80GB GPU to find out the maximum
    throughput of these methods on LLaMA 2-13B. We set the input length to 512+256\.
    Lat.: latency to generate one token (ms/token).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：我们耗尽了一台80GB A100 GPU的内存，以找出这些方法在LLaMA 2-13B上的最大吞吐量。我们将输入长度设置为512+256。延迟：生成一个标记的延迟（毫秒/标记）。
- en: '| Method | Max Bsz | Lat. | Thr. |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 最大批量 | 延迟 | 吞吐量 |'
- en: '| Accelerate | 42 | 1.72 (100%) | 581 (1.0x) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Accelerate | 42 | 1.72 (100%) | 581 (1.0x) |'
- en: '| Deepspeed | 40 | 1.03 (59.8%) | 972 (1.6x) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Deepspeed | 40 | 1.03 (59.8%) | 972 (1.6x) |'
- en: '| H[2]O | 48 | 1.39 (80.8%) | 769 (1.3x) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| H[2]O | 48 | 1.39 (80.8%) | 769 (1.3x) |'
- en: '| PyramidInfer | 88 | 0.59 (34.3%) | 1678 (2.8x) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| PyramidInfer | 88 | 0.59 (34.3%) | 1678 (2.8x) |'
- en: '| PyramidInfer +Deepspeed | 86 | 0.53 (30.8%) | 1887 (3.2x) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| PyramidInfer +Deepspeed | 86 | 0.53 (30.8%) | 1887 (3.2x) |'
- en: 5.2 Ablation Study
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 消融研究
- en: 'We conduct the ablation studies using the LLaMA 2-13B model to explore the
    PyramidInfer by answering the following questions: 1) Which way should we choose
    to gradually reduce the PvC length as the layer becomes deeper without sacrificing
    too much performance? 2) What proportion of the input should we partition as the
    recent sequence $S_{r}$?'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用LLaMA 2-13B模型进行消融研究，通过回答以下问题来探索PyramidInfer：1）在不牺牲太多性能的情况下，我们应该选择哪种方式来逐渐减少PvC长度，随着层数的增加？2）我们应该将输入的什么比例划分为最近的序列
    $S_{r}$？
- en: 'Table 3: PvC length decay ablation study.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：PvC长度衰减消融研究。
- en: '| Strategy | PPL | GSM8K | MMLU |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | PPL | GSM8K | MMLU |'
- en: '| Reduce more | 4.93 | 26.82 | 53.1 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 减少更多 | 4.93 | 26.82 | 53.1 |'
- en: '| Reduce uniformly | 4.55 | 28.32 | 54.8 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 均匀减少 | 4.55 | 28.32 | 54.8 |'
- en: '| Reduce less (PyramidInfer) | 4.20 | 29.56 | 55.7 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 减少较少（PyramidInfer） | 4.20 | 29.56 | 55.7 |'
- en: '| Reduce None (Full cache) | 4.42 | 28.58 | 55.4 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 不减少（完整缓存） | 4.42 | 28.58 | 55.4 |'
- en: PvC Length Decay
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PvC长度衰减
- en: 'Based on ICR, we gradually reduce the length of PvCs for each layer as the
    layer becomes deeper to maximize efficiency. However, excessive reduction of PvC
    length in shallow layers may lead to the loss of context information. We try to
    find out which way is the best to reduce the PvC length. Under the same compression
    ratio of 60%, we compare three patterns: 1) reduce more PvC length in shallow
    layers but less in the deeper layers (reduce 15% cache in the first 50% layers).
    2) uniformly reduce the PvC length (reduce 10% cache in the first 50% layers);
    3) obey the power law pattern based on ICR to reduce less at first (reduce 7%
    cache in the first 50% layers).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 基于ICR，我们逐渐减少每层的PvC长度，以最大化效率。然而，在浅层过度减少PvC长度可能会导致上下文信息的丢失。我们尝试找出哪种方式减少PvC长度最好。在相同的60%压缩比下，我们比较了三种模式：1）在浅层减少更多的PvC长度，但在深层减少较少（在前50%层中减少15%的缓存）。2）均匀减少PvC长度（在前50%层中减少10%的缓存）；3）基于ICR遵循幂律模式，初期减少较少（在前50%层中减少7%的缓存）。
- en: 'The result in Table [3](#S5.T3 "Table 3 ‣ 5.2 Ablation Study ‣ 5 Evaluation
    ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference")
    demonstrates that following the power law pattern is the best way to reduce the
    PvC length and even slightly improve performance on downstream tasks.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '表[3](#S5.T3 "表3 ‣ 5.2 消融研究 ‣ 5 评估 ‣ PyramidInfer: 高吞吐量LLM推理的金字塔KV缓存压缩")中的结果表明，遵循幂律模式是减少PvC长度的最佳方法，甚至可以在下游任务中略微提高性能。'
- en: Recent Sequence Ratio
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 最近序列比率
- en: In PyramidInfer, we select the recent tokens of the input as the recent sequence
    $S_{r}$ ratio should be.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyramidInfer中，我们选择输入的最新标记作为最近序列$S_{r}$的比率。
- en: 'In Figure [10](#S5.F10 "Figure 10 ‣ 5 Evaluation ‣ PyramidInfer: Pyramid KV
    Cache Compression for High-throughput LLM Inference"), we set the GPU memory usage
    of the KV cache of the full cache method as the 100% baseline and test how the
    perplexity will change with different $S_{r}$ ratio. Thus we can choose 40% as
    a trade-off between performance and GPU memory usage.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '在图[10](#S5.F10 "图10 ‣ 5 评估 ‣ PyramidInfer: 高吞吐量LLM推理的金字塔KV缓存压缩")中，我们将全缓存方法的KV缓存GPU内存使用量设定为100%的基准，并测试不同$S_{r}$比率下困惑度的变化。因此，我们可以选择40%作为性能与GPU内存使用之间的折中点。'
- en: 6 Conclusion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We alleviate the difficulty of deploying LLMs at scale by introducing PyramidInfer,
    a novel method that efficiently compresses the KV cache during both prefill and
    generation phases. Inspired by ICR and RAC, PyramidInfer significantly reduces
    GPU memory usage without compromising model performance. Experimental results
    present PyramidInfer is a promising solution for optimizing LLM deployment in
    resource-constrained environments.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过引入PyramidInfer来缓解大规模部署LLMs的难度，这是一种在预填充和生成阶段都有效压缩KV缓存的新方法。受ICR和RAC的启发，PyramidInfer在不妥协模型性能的情况下显著减少了GPU内存使用。实验结果表明，PyramidInfer是优化资源受限环境下LLM部署的有希望的解决方案。
- en: Limitations
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: 'Despite the effective strategy to reduce the keys and values to be computed
    by selecting the PvCs, PyramidInfer has to bring in additional computation so
    that it has limited speedup with a small batch size, as discussed in Appendix
    [A.1](#A1.SS1 "A.1 Additional Computational Cost in PyramidInfer ‣ Appendix A
    Extended Experiments and Details ‣ PyramidInfer: Pyramid KV Cache Compression
    for High-throughput LLM Inference").'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管通过选择PvCs来减少计算的键和值是有效的策略，PyramidInfer仍需进行额外的计算，因此在小批量情况下其速度提升有限，如附录[A.1](#A1.SS1
    "A.1 PyramidInfer中的额外计算成本 ‣ 附录A 扩展实验与细节 ‣ PyramidInfer: 高吞吐量LLM推理的金字塔KV缓存压缩")中所述。'
- en: Besides that, we are the pioneers in compressing the KV cache in the prefill
    phase, which is an area not fully explored. PyramidInfer is not a method to compress
    the KV cache losslessly in the prefill stage and more effective methods can be
    explored in future works.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们是预填充阶段压缩KV缓存的开创者，这一领域尚未充分探索。PyramidInfer并不是一种在预填充阶段无损压缩KV缓存的方法，未来的工作可以探索更有效的方法。
- en: References
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Aminabadi et al. (2022) Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia
    Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith,
    Olatunji Ruwase, and Yuxiong He. 2022. [Deepspeed inference: Enabling efficient
    inference of transformer models at unprecedented scale](http://arxiv.org/abs/2207.00032).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Aminabadi et al. (2022) Reza Yazdani Aminabadi、Samyam Rajbhandari、Minjia Zhang、Ammar
    Ahmad Awan、李成、杜力、Elton Zheng、Jeff Rasley、Shaden Smith、Olatunji Ruwase、何雨雄。2022年。[Deepspeed推理:
    实现前所未有规模的变换器模型高效推理](http://arxiv.org/abs/2207.00032)。'
- en: 'An et al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang,
    Lingpeng Kong, and Xipeng Qiu. 2023. [L-eval: Instituting standardized evaluation
    for long context language models](http://arxiv.org/abs/2307.11088).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'An et al. (2023) 陈欣安、宫珊珊、钟明、李木开、张军、孔凌鹏、邱熙鹏。2023年。[L-eval: 为长上下文语言模型制定标准化评估](http://arxiv.org/abs/2307.11088)。'
- en: Anthropic (2023) Anthropic. 2023. Introducing claude. [https://www.anthropic.com/index/introducing-claude](https://www.anthropic.com/index/introducing-claude).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic (2023) Anthropic。2023年。介绍claude。[https://www.anthropic.com/index/introducing-claude](https://www.anthropic.com/index/introducing-claude)。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](http://arxiv.org/abs/2005.14165).
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等（2020）Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei。2020年。[语言模型是少量学习者](http://arxiv.org/abs/2005.14165)。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. 2021. [Evaluating large language models trained on code](http://arxiv.org/abs/2107.03374).
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2021）Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, 和 Wojciech
    Zaremba。2021年。[评估在代码上训练的大型语言模型](http://arxiv.org/abs/2107.03374)。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021. [Training verifiers to solve
    math word problems](http://arxiv.org/abs/2110.14168).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe等（2021）Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo
    Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse, 和 John Schulman。2021年。[训练验证器解决数学单词问题](http://arxiv.org/abs/2110.14168)。
- en: 'Contributors (2023) OpenCompass Contributors. 2023. Opencompass: A universal
    evaluation platform for foundation models. [https://github.com/open-compass/opencompass](https://github.com/open-compass/opencompass).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Contributors（2023）OpenCompass Contributors。2023年。Opencompass：一个用于基础模型的通用评估平台。
    [https://github.com/open-compass/opencompass](https://github.com/open-compass/opencompass)。
- en: 'Dao (2023) Tri Dao. 2023. FlashAttention-2: Faster attention with better parallelism
    and work partitioning.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao（2023）Tri Dao。2023年。FlashAttention-2：更快的注意力机制，具有更好的并行性和工作分区。
- en: 'Ge et al. (2023) Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han,
    and Jianfeng Gao. 2023. Model tells you what to discard: Adaptive kv cache compression
    for llms. *arXiv preprint arXiv:2310.01801*.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge等（2023）Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, 和 Jianfeng
    Gao。2023年。模型告诉你要丢弃什么：适应性kv缓存压缩用于llms。*arXiv预印本arXiv:2310.01801*。
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. [Measuring massive multitask
    language understanding](http://arxiv.org/abs/2009.03300).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks等（2021）Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song, 和 Jacob Steinhardt。2021年。[测量大规模多任务语言理解](http://arxiv.org/abs/2009.03300)。
- en: 'Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat,
    Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu,
    and Zhifeng Chen. 2019. [Gpipe: Efficient training of giant neural networks using
    pipeline parallelism](http://arxiv.org/abs/1811.06965).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang等（2019）Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu
    Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, 和 Zhifeng
    Chen。2019年。[Gpipe: 使用流水线并行训练巨型神经网络](http://arxiv.org/abs/1811.06965)。'
- en: HuggingFace (2021) HuggingFace. 2021. Hugging face accelerate. [https://huggingface.co/docs/accelerate/index](https://huggingface.co/docs/accelerate/index).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HuggingFace (2021) HuggingFace。2021。Hugging Face Accelerate。 [https://huggingface.co/docs/accelerate/index](https://huggingface.co/docs/accelerate/index)。
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. [Mistral 7b](http://arxiv.org/abs/2310.06825).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, 和
    William El Sayed。2023。[Mistral 7b](http://arxiv.org/abs/2310.06825)。
- en: Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. [Efficient
    memory management for large language model serving with pagedattention](http://arxiv.org/abs/2309.06180).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等人 (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,
    Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, 和 Ion Stoica。2023。[大语言模型服务的高效内存管理与分页注意力](http://arxiv.org/abs/2309.06180)。
- en: 'Lamb et al. (2016) Alex M Lamb, Anirudh Goyal ALIAS PARTH GOYAL, Ying Zhang,
    Saizheng Zhang, Aaron C Courville, and Yoshua Bengio. 2016. Professor forcing:
    A new algorithm for training recurrent networks. *Advances in neural information
    processing systems*, 29.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lamb 等人 (2016) Alex M Lamb, Anirudh Goyal ALIAS PARTH GOYAL, Ying Zhang, Saizheng
    Zhang, Aaron C Courville, 和 Yoshua Bengio。2016。教授强制：一种用于训练递归网络的新算法。*神经信息处理系统进展*，29。
- en: 'Liu et al. (2023) Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor
    Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. 2023. [Scissorhands:
    Exploiting the persistence of importance hypothesis for llm kv cache compression
    at test time](http://arxiv.org/abs/2305.17118).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023) Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor
    Xie, Zhaozhuo Xu, Anastasios Kyrillidis, 和 Anshumali Shrivastava。2023。[剪刀手：利用重要性假设的持久性进行
    LLM KV 缓存压缩](http://arxiv.org/abs/2305.17118)。
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. 2016. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等人 (2016) Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard Socher。2016。指针哨兵混合模型。*arXiv
    预印本 arXiv:1609.07843*。
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI。2023。[GPT-4 技术报告](http://arxiv.org/abs/2303.08774)。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever 等人。2019。语言模型是无监督的多任务学习者。*OpenAI 博客*，1(8):9。
- en: 'Rozière et al. (2023) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton
    Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal
    Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel
    Synnaeve. 2023. [Code llama: Open foundation models for code](http://arxiv.org/abs/2308.12950).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rozière 等人 (2023) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla,
    Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin,
    Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton
    Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal
    Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, 和 Gabriel
    Synnaeve。2023。[代码驼鹿：开放代码基础模型](http://arxiv.org/abs/2308.12950)。
- en: 'Sheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez,
    Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. 2023. [Flexgen: High-throughput
    generative inference of large language models with a single gpu](http://arxiv.org/abs/2303.06865).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheng 等人 (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin,
    Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy
    Liang, Christopher Ré, Ion Stoica, 和 Ce Zhang。2023。[Flexgen：使用单个 GPU 的大语言模型高吞吐量生成推理](http://arxiv.org/abs/2303.06865)。
- en: 'Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
    Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya
    Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying
    and extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava 等人 (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal
    Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta,
    Adrià Garriga-Alonso 等人。2022。超越模仿游戏：量化和外推语言模型的能力。*arXiv 预印本 arXiv:2206.04615*。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. [Llama
    2: Open foundation and fine-tuned chat models](http://arxiv.org/abs/2307.09288).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人（2023）Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, 和 Thomas Scialom。2023。[Llama
    2: 开放基础和微调聊天模型](http://arxiv.org/abs/2307.09288)。'
- en: Xiao et al. (2023) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. 2023. [Efficient streaming language models with attention sinks](http://arxiv.org/abs/2309.17453).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人（2023）Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, 和 Mike Lewis。2023。[具有注意力汇的高效流式语言模型](http://arxiv.org/abs/2309.17453)。
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. 2022. Opt: Open pre-trained transformer language models. *arXiv preprint
    arXiv:2205.01068*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人（2022）Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin 等。2022。Opt:
    开放预训练变换器语言模型。*arXiv 预印本 arXiv:2205.01068*。'
- en: 'Zhang et al. (2023) Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang
    Wang, and Beidi Chen. 2023. [H[2]o: Heavy-hitter oracle for efficient generative
    inference of large language models](http://arxiv.org/abs/2306.14048).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人（2023）Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang
    Wang, 和 Beidi Chen。2023。[H[2]o: 高效生成推理大语言模型的重型命中预言机](http://arxiv.org/abs/2306.14048)。'
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. [Judging llm-as-a-judge with
    mt-bench and chatbot arena](http://arxiv.org/abs/2306.05685).
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人（2023）Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph
    E. Gonzalez, 和 Ion Stoica。2023。[使用 mt-bench 和聊天机器人竞技场判断 llm-as-a-judge](http://arxiv.org/abs/2306.05685)。
- en: Appendix A Extended Experiments and Details
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 扩展实验和细节
- en: A.1 Additional Computational Cost in PyramidInfer
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 PyramidInfer 的额外计算成本
- en: '![Refer to caption](img/fc1418900428d66327262073cfc2a29f.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fc1418900428d66327262073cfc2a29f.png)'
- en: 'Figure 11: Comparison between PyramidInfer and full cache baseline with different
    batch sizes on the LLaMA 2-7B model with input length of 512+256.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：在 LLaMA 2-7B 模型上，输入长度为 512+256 的不同批量大小下，PyramidInfer 与完整缓存基线的比较。
- en: 'In Section [4](#S4 "4 Layer-wise PvC Selection ‣ PyramidInfer: Pyramid KV Cache
    Compression for High-throughput LLM Inference"), we introduce how PyramidInfer
    improves the inference throughput by selecting the PvCs based on the attention
    of $S_{r}$ while others can be neglected.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [4](#S4 "4 层次 PvC 选择 ‣ PyramidInfer：用于高吞吐量 LLM 推理的 Pyramid KV 缓存压缩") 节中，我们介绍了
    PyramidInfer 如何通过基于 $S_{r}$ 的注意力选择 PvCs 来提高推理吞吐量，而其他部分可以被忽略。
- en: 'To evaluate the influence of the additional cost, we gradually increase the
    batch size of the models and compare the throughput between PyramidInfer and the
    full cache baseline. As shown in Figure [11](#A1.F11 "Figure 11 ‣ A.1 Additional
    Computational Cost in PyramidInfer ‣ Appendix A Extended Experiments and Details
    ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference"),
    PyramidInfer has limited acceleration with a small batch size because the additional
    computation offsets the acceleration from the reduced KV cache. As the batch size
    increases, this cost becomes trivial compared to the acceleration brought by the
    PyramidInfer.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '为评估附加成本的影响，我们逐渐增加模型的批处理大小，并比较 PyramidInfer 和完整缓存基线之间的吞吐量。如图 [11](#A1.F11 "Figure
    11 ‣ A.1 Additional Computational Cost in PyramidInfer ‣ Appendix A Extended Experiments
    and Details ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM
    Inference") 所示，由于附加计算抵消了减少 KV 缓存带来的加速，PyramidInfer 在小批量时的加速有限。随着批量大小的增加，这一成本相对于
    PyramidInfer 带来的加速变得微不足道。'
- en: A.2 Position Encoding
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 位置编码
- en: 'As we reduce the number of keys and values of each layer, some positions of
    keys and values are missing. There are two choices to obtain the new position
    encoding: 1) re-encode the positions from position 0 in order; 2) gather the scattered
    original position encodings of the keys and values. As shown in Table [4](#A1.T4
    "Table 4 ‣ A.2 Position Encoding ‣ Appendix A Extended Experiments and Details
    ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference"),
    we experiment on these two choices on LLaMA 2-13B and find that the latter one
    has a slightly better performance in the downstream tasks.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '随着我们减少每层的键和值的数量，一些键和值的位置会缺失。获取新的位置编码有两种选择：1) 从位置 0 开始重新编码位置；2) 收集散落的原始键和值的位置编码。如表
    [4](#A1.T4 "Table 4 ‣ A.2 Position Encoding ‣ Appendix A Extended Experiments
    and Details ‣ PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM
    Inference") 所示，我们对这两种选择在 LLaMA 2-13B 上进行了实验，发现后一种在下游任务中表现略好。'
- en: 'Table 4: Position encoding comparison.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 位置编码比较。'
- en: '| Strategy | GSM8K | MMLU |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | GSM8K | MMLU |'
- en: '| Re-encode | 29.12 | 55.5 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 重新编码 | 29.12 | 55.5 |'
- en: '| Gather | 29.56 | 55.7 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 收集 | 29.56 | 55.7 |'
