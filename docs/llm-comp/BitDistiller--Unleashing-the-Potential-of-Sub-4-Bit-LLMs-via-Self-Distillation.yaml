- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:59:19'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:59:19
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BitDistiller：通过自蒸馏释放Sub-4-Bit LLMs的潜力
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10631](https://ar5iv.labs.arxiv.org/html/2402.10631)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10631](https://ar5iv.labs.arxiv.org/html/2402.10631)
- en: Dayou Du¹, Yijia Zhang², Shijie Cao³, Jiaqi Guo³, Ting Cao³, Xiaowen Chu¹, Ningyi
    Xu²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Dayou Du¹, Yijia Zhang², Shijie Cao³, Jiaqi Guo³, Ting Cao³, Xiaowen Chu¹, Ningyi
    Xu²
- en: ¹The Hong Kong University of Science and Technology (Guangzhou)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹香港科技大学（广州）
- en: ²Shanghai Jiao Tong University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²上海交通大学
- en: ³Microsoft Research Asia
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³微软亚洲研究院
- en: ddu487@connect.hkust-gz.edu.cn, {zhangyijia, xuningyi}@sjtu.edu.cn,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ddu487@connect.hkust-gz.edu.cn, {zhangyijia, xuningyi}@sjtu.edu.cn,
- en: '{shijiecao, jiaqiguo, ting.cao}@microsoft.com, xwchu@ust.hk'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{shijiecao, jiaqiguo, ting.cao}@microsoft.com, xwchu@ust.hk'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The upscaling of Large Language Models (LLMs) has yielded impressive advances
    in natural language processing, yet it also poses significant deployment challenges.
    Weight quantization has emerged as a widely embraced solution to reduce memory
    and computational demands. This paper introduces BitDistiller, a framework that
    synergizes Quantization-Aware Training (QAT) with Knowledge Distillation (KD)
    to boost the performance of LLMs at ultra-low precisions (sub-4-bit). Specifically,
    BitDistiller first incorporates a tailored asymmetric quantization and clipping
    technique to maximally preserve the fidelity of quantized weights, and then proposes
    a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which
    is employed in a self-distillation manner to enable faster convergence and superior
    model performance. Empirical evaluations demonstrate that BitDistiller significantly
    surpasses existing methods in both 3-bit and 2-bit configurations on general language
    understanding and complex reasoning benchmarks. Notably, BitDistiller is shown
    to be more cost-effective, demanding fewer data and training resources. The code
    is available at [https://github.com/DD-DuDa/BitDistiller](https://github.com/DD-DuDa/BitDistiller).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的扩展在自然语言处理领域取得了显著进展，但也带来了显著的部署挑战。权重量化作为一种广泛接受的解决方案，旨在减少内存和计算需求。本文介绍了BitDistiller，一个将量化感知训练（QAT）与知识蒸馏（KD）相结合的框架，以提升LLMs在超低精度（sub-4-bit）下的性能。具体而言，BitDistiller首先采用定制的不对称量化和裁剪技术，最大限度地保留量化权重的保真度，然后提出了一种新颖的信心感知Kullback-Leibler散度（CAKLD）目标，该目标以自蒸馏的方式使用，从而实现更快的收敛和更优的模型性能。实证评估表明，BitDistiller在3-bit和2-bit配置下在通用语言理解和复杂推理基准测试中显著超越了现有方法。值得注意的是，BitDistiller被证明具有更高的性价比，所需的数据和训练资源更少。代码可在
    [https://github.com/DD-DuDa/BitDistiller](https://github.com/DD-DuDa/BitDistiller)
    获取。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Scaling up model sizes has been pivotal to the success of large language models
    (LLMs), yielding unprecedented performance across diverse natural language processing
    tasks Brown et al. ([2020](#bib.bib4)); Touvron et al. ([2023](#bib.bib41)); Kaplan
    et al. ([2020](#bib.bib20)). However, such escalating model size poses significant
    challenges in deployment, particularly on resource-constrained devices, due to
    the substantial memory footprint and computational requirements.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 模型规模的扩大对大型语言模型（LLMs）的成功至关重要，带来了在各种自然语言处理任务中前所未有的表现 Brown et al. ([2020](#bib.bib4));
    Touvron et al. ([2023](#bib.bib41)); Kaplan et al. ([2020](#bib.bib20))。然而，这种模型规模的增加在部署时带来了重大挑战，特别是在资源有限的设备上，因为它对内存和计算要求有着巨大的负担。
- en: Weight quantization has emerged as a popular strategy to enhance the efficiency
    and accessibility of LLMs by reducing model size with minimal performance loss Gholami
    et al. ([2022](#bib.bib15)). In practice, 4-bit quantization has been widely adopted,
    offering a balance between a considerable compression ratio and the preservation
    of LLM capabilities Lin et al. ([2023](#bib.bib26)); Frantar et al. ([2022](#bib.bib13));
    Liu et al. ([2023a](#bib.bib27)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 权重量化已成为提高LLMs效率和可达性的流行策略，通过减少模型规模而性能损失最小化 Gholami et al. ([2022](#bib.bib15))。在实际应用中，4-bit量化被广泛采用，提供了较大的压缩比和LLM能力的保持 Lin
    et al. ([2023](#bib.bib26)); Frantar et al. ([2022](#bib.bib13)); Liu et al. ([2023a](#bib.bib27))。
- en: '![Refer to caption](img/bd8c4cfebad674b30763c028df07249d.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bd8c4cfebad674b30763c028df07249d.png)'
- en: 'Figure 1: Bit-Level scaling laws for code generation performance for 3B to
    34B parameter coder models. BitDistiller outperforms existing QAT methods in both
    3-bit and 2-bit settings. Details in Table [2](#S4.T2 "Table 2 ‣ 4.3 Evaluation
    on Reasoning Tasks ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential of
    Sub-4-Bit LLMs via Self-Distillation").'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '图1：3B到34B参数编码器模型的代码生成性能的位级缩放规律。BitDistiller在3-bit和2-bit设置中均优于现有的QAT方法。详细信息见表 [2](#S4.T2
    "Table 2 ‣ 4.3 Evaluation on Reasoning Tasks ‣ 4 Experiments ‣ BitDistiller: Unleashing
    the Potential of Sub-4-Bit LLMs via Self-Distillation")。'
- en: 'However, sub-4-bit quantization significantly degrades the fidelity of model
    weights, leading to deteriorated model performance, especially in smaller models
    or tasks requiring complex reasoning Dettmers and Zettlemoyer ([2023](#bib.bib12)).
    To address this, researchers have developed various Post-Training Quantization
    (PTQ) and Quantization-Aware Training (QAT) methods Chee et al. ([2023](#bib.bib5));
    Shao et al. ([2023](#bib.bib38)). PTQ, while appealing without retraining, struggles
    to preserve model performance at very low precisions. In contrast, QAT incorporates
    quantization into the training loop, enabling dynamic adaptation to reduced precision
    and thus maintaining higher accuracy Liu et al. ([2023b](#bib.bib28)); Kim et al.
    ([2023a](#bib.bib22)). Despite its early promise, two fundamental challenges are
    essential for achieving high model performance in extreme low-bit QAT: how to
    maximally preserve weight fidelity during quantization, and how to effectively
    learn low-bit representations during training.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，亚4-bit量化显著降低了模型权重的保真度，导致模型性能下降，特别是在较小的模型或需要复杂推理的任务中 Dettmers和Zettlemoyer（[2023](#bib.bib12)）。为了解决这一问题，研究人员开发了各种后训练量化（PTQ）和量化感知训练（QAT）方法 Chee等（[2023](#bib.bib5)）；Shao等（[2023](#bib.bib38)）。虽然PTQ在无需重新训练的情况下具有吸引力，但在非常低的精度下难以保持模型性能。相比之下，QAT将量化纳入训练循环，能够动态适应减少的精度，从而保持更高的准确性 Liu等（[2023b](#bib.bib28)）；Kim等（[2023a](#bib.bib22)）。尽管早期有希望，但在极低位QAT中实现高模型性能的两个基本挑战是：如何在量化过程中最大限度地保持权重的保真度，以及如何在训练过程中有效地学习低位表示。
- en: In this work, we present BitDistiller, a novel framework that synergizes QAT
    with Knowledge Distillation (KD) to significantly boost the performance of sub-4-bit
    quantized LLMs. To minimize quantization error, BitDistiller employs a tailored
    asymmetric quantization and clipping strategy to maintain the capabilities of
    the full-precision model as much as possible, particularly at ultra-low-bit levels.
    For efficient and effective low-bit representation learning, BitDistiller leverages
    a simple yet effective self-distillation approach, wherein the full-precision
    model acts as its own teacher to refine the low-bit student model. Notably, BitDistiller
    innovates with a Confidence-Aware Kullback-Leibler divergence (CAKLD) objective
    that optimizes knowledge transferring efficacy, enabling faster convergence and
    enhanced model performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了BitDistiller，这是一个新颖的框架，将QAT与知识蒸馏（KD）相结合，以显著提升亚4-bit量化LLM的性能。为了最小化量化误差，BitDistiller采用了量身定制的非对称量化和裁剪策略，以尽可能保持全精度模型的能力，特别是在超低位数水平下。为了高效且有效地进行低位表示学习，BitDistiller利用了一种简单而有效的自蒸馏方法，其中全精度模型充当其自身的教师，以优化低位学生模型。值得注意的是，BitDistiller通过一个信心感知的Kullback-Leibler散度（CAKLD）目标进行创新，该目标优化了知识传递的效果，从而实现了更快的收敛速度和增强的模型性能。
- en: 'Our empirical evaluations, conducted on a diverse suite of general language
    understanding and complex reasoning tasks including mathematics and coding, demonstrate
    that BitDistiller significantly outperforms existing PTQ and QAT methods in the
    realm of sub-4-bit quantization. As illustrated in Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via
    Self-Distillation"), BitDistiller achieves the most favorable scaling law in both
    3-bit and 2-bit configurations on the code reasoning benchmark. Moreover, BitDistiller
    is demonstrated to be more cost-effective, requiring less training data and fewer
    training resources, thereby marking a significant advancement toward deploying
    robust Large Language Models on resource-constrained devices.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在包括数学和编码在内的各种通用语言理解和复杂推理任务上进行的实证评估表明，BitDistiller 在 4 位以下量化领域显著优于现有的 PTQ
    和 QAT 方法。如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ BitDistiller: Unleashing the
    Potential of Sub-4-Bit LLMs via Self-Distillation") 所示，BitDistiller 在代码推理基准测试中，在
    3 位和 2 位配置下都达到了最有利的扩展规律。此外，BitDistiller 被证明更具成本效益，需要更少的训练数据和资源，从而在资源受限设备上部署强大的大型语言模型方面取得了显著进展。'
- en: 2 Background and Related Work
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与相关工作
- en: 2.1 Weight Quantization for LLMs
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 大型语言模型的权重量化
- en: PTQ and QAT
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PTQ 和 QAT
- en: PTQ is directly applied to pre-trained models without additional training. PTQ
    for LLMs typically employs techniques that either adjust quantization error Frantar
    et al. ([2022](#bib.bib13)); Chee et al. ([2023](#bib.bib5)) or prioritize salient
    weights Dettmers et al. ([2023b](#bib.bib11)); Lin et al. ([2023](#bib.bib26));
    Kim et al. ([2023b](#bib.bib23)). However, the lack of retraining with PTQ may
    cause notable decreases in model performance at extremely low precisions. In contrast,
    QAT integrates quantization into the training phase, enabling the model to learn
    better representations for low-bit weights, as demonstrated by approaches like
    LLM-QAT Liu et al. ([2023b](#bib.bib28)), OmniQuant Shao et al. ([2023](#bib.bib38)),
    PB-LLM Shang et al. ([2023](#bib.bib37)), and BitNet Wang et al. ([2023](#bib.bib42)).
    Despite improved model performance, QAT is still challenged by the need of extensive
    training and data, with significant potential for further optimization and enhancement.
    In this work, we harness the synergy of QAT and KD to enhance the performance
    of quantized LLMs, especially at sub-4-bit settings.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: PTQ 直接应用于预训练模型，无需额外训练。对于大型语言模型（LLMs），PTQ 通常采用技术来调整量化误差 Frantar 等人（[2022](#bib.bib13)）；Chee
    等人（[2023](#bib.bib5)）或优先考虑显著的权重 Dettmers 等人（[2023b](#bib.bib11)）；Lin 等人（[2023](#bib.bib26)）；Kim
    等人（[2023b](#bib.bib23)）。然而，PTQ 不进行再训练可能会导致模型在极低精度下的性能显著下降。相比之下，QAT 将量化集成到训练阶段，使模型能够学习更好的低位权重表示，如
    LLM-QAT Liu 等人（[2023b](#bib.bib28)），OmniQuant Shao 等人（[2023](#bib.bib38)），PB-LLM Shang
    等人（[2023](#bib.bib37)），以及 BitNet Wang 等人（[2023](#bib.bib42)）等方法所示。尽管模型性能有所提高，QAT
    仍然面临需要大量训练和数据的挑战，并且具有进一步优化和增强的显著潜力。在这项工作中，我们利用 QAT 和 KD 的协同作用来提升量化 LLM 的性能，特别是在低于
    4 位的设置下。
- en: Granularity and Format Optimizations
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 粒度与格式优化
- en: Extensive research indicates that adopting finer-grained quantization approaches,
    such as group-wise quantization, can achieve higher accuracy compared to layer-wise
    or channel-wise methods Shen et al. ([2020](#bib.bib39)); Frantar et al. ([2022](#bib.bib13)).
    Floating-point formats (FP8/FP4/NF4) have been demonstrated to deliver superior
    accuracy compared to integer formats (INT8/INT4) in LLM quantization Kuzmin et al.
    ([2022](#bib.bib24)); Dettmers and Zettlemoyer ([2023](#bib.bib12)); Zhang et al.
    ([2023b](#bib.bib49)). Notably, asymmetric quantization methods, particularly
    for floating-point formats, outperform their symmetric counterparts by better
    accommodating the distribution of model weights Zhang et al. ([2023a](#bib.bib48)).
    BitDistiller aligns with these insights, employing finer granularity and asymmetric
    techniques for quantization.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛的研究表明，采用更细粒度的量化方法，如分组量化，相比于层级或通道级方法可以实现更高的准确性 Shen 等人（[2020](#bib.bib39)）；Frantar
    等人（[2022](#bib.bib13)）。在 LLM 量化中，浮点格式（FP8/FP4/NF4）已被证明比整数格式（INT8/INT4）提供更高的准确性 Kuzmin
    等人（[2022](#bib.bib24)）；Dettmers 和 Zettlemoyer（[2023](#bib.bib12)）；Zhang 等人（[2023b](#bib.bib49)）。值得注意的是，特别是浮点格式的非对称量化方法，通过更好地适应模型权重的分布，相较于对称量化方法表现更佳 Zhang
    等人（[2023a](#bib.bib48)）。BitDistiller 符合这些见解，采用更细粒度和非对称的量化技术。
- en: 2.2 Knowledge Distillation for LLMs
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 大型语言模型的知识蒸馏
- en: In the realm of LLMs, white-box knowledge distillation (KD) has become increasingly
    prevalent due to the accessible distribution of the teacher model, which facilitates
    the transmission of knowledge representations to the student model Hinton et al.
    ([2015](#bib.bib18)); Zhu et al. ([2023](#bib.bib51)). Notably, MINILLM Gu et al.
    ([2023](#bib.bib16)) utilizes the reverse KLD to ensure the accuracy and fidelity
    of language generation. GKD Agarwal et al. ([2023](#bib.bib1)) has explored alternative
    divergences called the generalized Jensen–Shannon divergence (JSD) and addressed
    the distribution mismatch by sampling outputs from the student model during training.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLM 领域，由于教师模型的可访问分布使得知识表示的传递变得更加容易，白盒知识蒸馏（KD）变得越来越普遍 Hinton 等 ([2015](#bib.bib18))；Zhu
    等 ([2023](#bib.bib51))。特别地，MINILLM Gu 等 ([2023](#bib.bib16)) 利用反向 KLD 来确保语言生成的准确性和保真度。GKD
    Agarwal 等 ([2023](#bib.bib1)) 探讨了称为广义 Jensen–Shannon 散度（JSD）的替代散度，并通过在训练过程中从学生模型中采样输出来解决分布不匹配的问题。
- en: To attain exceedingly high compression ratios, a promising method is to combine
    KD with model quantization, where KD can be effectively used to mitigate the accuracy
    decline of quantized models Zhang et al. ([2020](#bib.bib47)); Kim et al. ([2022](#bib.bib21)).
    In cutting-edge research applying QAT-based KD for LLMs, TSLD Kim et al. ([2023a](#bib.bib22))
    considers risks of overfitting and conducts logit distillation with ground truth
    loss. Similarly, LLM-QAT leverages randomly teacher-generated data for data-free
    distillation. In distinction from TSLD and LLM-QAT, we achieve better performance
    and cost-efficiency in the extremely low-bit quantization level.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现极高的压缩比，一种有前途的方法是将 KD 与模型量化结合，其中 KD 可以有效地减轻量化模型的准确性下降 Zhang 等 ([2020](#bib.bib47))；Kim
    等 ([2022](#bib.bib21))。在应用 QAT 基础 KD 的 LLM 前沿研究中，TSLD Kim 等 ([2023a](#bib.bib22))
    考虑了过拟合的风险，并进行了与真实标签损失的 logit 蒸馏。类似地，LLM-QAT 利用随机生成的数据进行无数据蒸馏。与 TSLD 和 LLM-QAT
    不同，我们在极低位量化水平上实现了更好的性能和成本效益。
- en: 3 Methodology
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: '![Refer to caption](img/8f4ce7d34a29c9a88806d355a560837c.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8f4ce7d34a29c9a88806d355a560837c.png)'
- en: 'Figure 2: Depiction of the QAT-based KD framework of BitDistiller.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：BitDistiller 的 QAT 基础 KD 框架示意图。
- en: 'In this section, we introduce BitDistiller, a QAT with self-distillation framework
    for LLMs, as illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3 Methodology ‣ BitDistiller:
    Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation"). To maximally
    preserve weight fidelity during quantization, we first present an asymmetric quantization
    and clipping method (see Section [3.1](#S3.SS1 "3.1 Asymmetric Quantization and
    Clipping ‣ 3 Methodology ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation")). Second, to counteract the performance degradation
    caused by precision reduction, we adopt Knowledge Distillation and propose a novel
    Confidence-Aware KL divergence (CAKLD) objective, in which the full-precision
    model acts as a teacher and the low-precision one plays a student (see Section [3.2](#S3.SS2
    "3.2 Self Distillation with CAKLD ‣ 3 Methodology ‣ BitDistiller: Unleashing the
    Potential of Sub-4-Bit LLMs via Self-Distillation")).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们介绍了 BitDistiller，这是一种用于 LLM 的自蒸馏框架的 QAT，如图 [2](#S3.F2 "Figure 2 ‣ 3
    Methodology ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation")
    所示。为了在量化过程中最大限度地保持权重保真度，我们首先介绍了一种非对称量化和剪裁方法（见第 [3.1](#S3.SS1 "3.1 Asymmetric Quantization
    and Clipping ‣ 3 Methodology ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation") 节）。其次，为了抵消精度降低引起的性能退化，我们采用知识蒸馏，并提出了一种新颖的信心感知 KL 散度（CAKLD）目标，其中全精度模型充当教师，低精度模型则充当学生（见第
    [3.2](#S3.SS2 "3.2 Self Distillation with CAKLD ‣ 3 Methodology ‣ BitDistiller:
    Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation") 节）。'
- en: 'Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Methodology ‣ BitDistiller: Unleashing
    the Potential of Sub-4-Bit LLMs via Self-Distillation") outlines the process of
    BitDistiller. Given the full-precision weight w, BitDistiller adopts the asymmetric
    clipping to alleviate outliers in w (Line [4](#alg1.l4 "In Algorithm 1 ‣ 3 Methodology
    ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation")),
    prior to the training loop. Then, in each training step, BitDistiller forwards
    the model with the quantized weights ($w^{t}_{Q}$), computes the loss with the
    proposed CAKLD objective (Line [8](#alg1.l8 "In Algorithm 1 ‣ 3 Methodology ‣
    BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation")-[9](#alg1.l9
    "In Algorithm 1 ‣ 3 Methodology ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation")), and updates the full-precision weights (Line [11](#alg1.l11
    "In Algorithm 1 ‣ 3 Methodology ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation")-[12](#alg1.l12 "In Algorithm 1 ‣ 3 Methodology ‣
    BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation")) Bengio
    et al. ([2013](#bib.bib2)). When the training finishes, BitDistiller returns the
    final quantized weights.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 [1](#alg1 "算法 1 ‣ 3 方法 ‣ BitDistiller: 通过自我蒸馏释放 Sub-4-Bit LLM 的潜力") 概述了
    BitDistiller 的过程。给定全精度权重 w，BitDistiller 采用非对称剪辑来缓解 w 中的异常值（第 [4](#alg1.l4 "在算法
    1 ‣ 3 方法 ‣ BitDistiller: 通过自我蒸馏释放 Sub-4-Bit LLM 的潜力") 行），在训练循环之前。然后，在每一步训练中，BitDistiller
    使用量化权重（$w^{t}_{Q}$）前向传递模型，使用提出的 CAKLD 目标计算损失（第 [8](#alg1.l8 "在算法 1 ‣ 3 方法 ‣ BitDistiller:
    通过自我蒸馏释放 Sub-4-Bit LLM 的潜力") 行-[9](#alg1.l9 "在算法 1 ‣ 3 方法 ‣ BitDistiller: 通过自我蒸馏释放
    Sub-4-Bit LLM 的潜力") 行），并更新全精度权重（第 [11](#alg1.l11 "在算法 1 ‣ 3 方法 ‣ BitDistiller:
    通过自我蒸馏释放 Sub-4-Bit LLM 的潜力") 行-[12](#alg1.l12 "在算法 1 ‣ 3 方法 ‣ BitDistiller: 通过自我蒸馏释放
    Sub-4-Bit LLM 的潜力") 行）Bengio 等（[2013](#bib.bib2)）。训练结束后，BitDistiller 返回最终的量化权重。'
- en: Algorithm 1 BitDistiller
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 BitDistiller
- en: 1:Full-precision weight $w$;
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 1:全精度权重 $w$；
- en: 3.1 Asymmetric Quantization and Clipping
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 非对称量化和剪辑
- en: The adoption of finer granularities, or smaller group sizes, in weight quantization
    of LLMs inherently leads to asymmetrical distributions and the presence of outliers
    in weight groups. Proper management of asymmetry is crucial to maintaining model
    performance in low-bit PTQ regimes. Our investigation reveals that the effects
    of asymmetry are more prominent in extremely low-bit QAT, such as 3-bit and 2-bit
    configurations, necessitating tailored strategies to address these challenges.
    Therefore, in BitDistiller, we adopt asymmetric quantization techniques coupled
    with asymmetric clipping strategies to enhance the representational fidelity of
    quantized weights and maximally preserve the capabilities of the full-precision
    model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLM 的权重量化中采用更精细的粒度或较小的组大小，本质上会导致权重组的分布不对称和异常值的存在。妥善管理这种不对称性对于在低位 PTQ 状态下保持模型性能至关重要。我们的调查揭示，在极低位
    QAT 中，如 3 位和 2 位配置，不对称性的影响更加显著，需要量身定制的策略来应对这些挑战。因此，在 BitDistiller 中，我们采用了非对称量化技术，并结合非对称剪辑策略，以提高量化权重的表示精度，并最大限度地保留全精度模型的能力。
- en: Asymmetric Quantization
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**非对称量化**'
- en: Previous studies have shown that floating-point formats (e.g., FP, NF) often
    outperform integer formats (INT) in LLM quantization Dettmers et al. ([2023a](#bib.bib10));
    Liu et al. ([2023a](#bib.bib27)). However, as the quantization level falls to
    2-bit, we observed a notable decline in the effectiveness of FP/NF formats. This
    advantage of FP/NF formats is attributed to their non-uniform nature, which can
    capture a wider range of values. Such a non-uniform distribution aligns better
    with the natural distribution of weight tensors in LLMs. In 2-bit cases, the limited
    representational capacity, offering only four distinct values, undermines the
    benefits of non-uniform distribution and impedes the efficient utilization of
    each numerical value. In light of these findings, we employ NF formats for quantization
    above 2-bit, while opting for the INT format at the 2-bit level.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究表明，浮点格式（例如，FP，NF）在 LLM 量化中通常优于整数格式（INT） Dettmers 等人（[2023a](#bib.bib10)）；Liu
    等人（[2023a](#bib.bib27)）。然而，随着量化级别降至 2 位，我们观察到 FP/NF 格式的有效性显著下降。FP/NF 格式的这一优势归因于它们的不均匀特性，可以捕捉更广泛的值。这种不均匀分布更好地与
    LLM 中权重张量的自然分布对齐。在 2 位情况下，有限的表示能力（仅提供四个不同的值）削弱了不均匀分布的好处，并阻碍了每个数值的有效利用。鉴于这些发现，我们在
    2 位以上使用 NF 格式进行量化，而在 2 位级别选择 INT 格式。
- en: 'For NF formats (e.g., NF3), we adopt the AFPQ method Zhang et al. ([2023a](#bib.bib48))
    to enable asymmetric quantization, which establishes separate scales, $s_{pos}$,
    as shown in Equation [1](#S3.E1 "In Asymmetric Quantization ‣ 3.1 Asymmetric Quantization
    and Clipping ‣ 3 Methodology ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation"). For INT formats (e.g., INT2), we utilize conventional
    asymmetric methods with a single scale and a designated zero point, as detailed
    in Equation [2](#S3.E2 "In Asymmetric Quantization ‣ 3.1 Asymmetric Quantization
    and Clipping ‣ 3 Methodology ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 NF 格式（例如，NF3），我们采用 AFPQ 方法 Zhang 等人（[2023a](#bib.bib48)）来实现非对称量化，该方法建立了独立的尺度
    $s_{pos}$，如方程 [1](#S3.E1 "在非对称量化 ‣ 3.1 非对称量化与剪裁 ‣ 3 方法 ‣ BitDistiller：通过自蒸馏释放子
    4 位 LLM 的潜力") 中所示。对于 INT 格式（例如，INT2），我们利用传统的非对称方法，采用单一尺度和指定的零点，如方程 [2](#S3.E2
    "在非对称量化 ‣ 3.1 非对称量化与剪裁 ‣ 3 方法 ‣ BitDistiller：通过自蒸馏释放子 4 位 LLM 的潜力") 中详细说明。
- en: '|  | $1$2 |  | (1) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: '|  | $INT\text{-}Asym:Q(w)=\lfloor\frac{w-z}{s}\rceil$ |  | (2) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $INT\text{-}Asym:Q(w)=\lfloor\frac{w-z}{s}\rceil$ |  | (2) |'
- en: Asymmetric Clipping
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非对称剪裁
- en: The strategy of clipping, which involves constraining the range of weight values,
    has been recognized for its contribution to maintaining high accuracy after quantization Sakr
    et al. ([2022](#bib.bib36)); Shao et al. ([2023](#bib.bib38)). However, naive
    clipping methods often fall short in effectiveness, while advanced clipping techniques
    come at a high computational cost which is prohibitive for practical QAT use Li
    et al. ([2019](#bib.bib25)); Jung et al. ([2019](#bib.bib19)). To circumvent these
    limitations, we propose the use of asymmetric clipping solely during the initial
    phase, prior to the commencement of QAT. Asymmetric clipping at initialization
    provides a good starting point that significantly contributes to the final overall
    quantized model accuracy without incurring the prohibitive costs associated with
    iterative clipping optimization.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 剪裁策略涉及限制权重值的范围，已被认为有助于在量化后保持高精度 Sakr 等人（[2022](#bib.bib36)）；Shao 等人（[2023](#bib.bib38)）。然而，简单的剪裁方法往往效果不佳，而先进的剪裁技术则伴随着高计算成本，这对实际
    QAT 使用是不可接受的 Li 等人（[2019](#bib.bib25)）；Jung 等人（[2019](#bib.bib19)）。为了规避这些限制，我们建议在
    QAT 开始之前，仅在初始阶段使用非对称剪裁。初始化阶段的非对称剪裁提供了一个良好的起点，显著有助于最终的整体量化模型精度，而不会带来与迭代剪裁优化相关的高昂成本。
- en: 'To enable asymmetric clipping for QAT initialization, given input features
    $X$, for each layer of the model. These values aim to minimize the output difference
    after quantization. Formally, the objective is to optimize the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 QAT 初始化阶段启用非对称剪裁，给定输入特征 $X$，对模型的每一层进行。这些值旨在最小化量化后的输出差异。形式上，目标是优化以下内容：
- en: '|  | $\displaystyle\alpha^{*},\beta^{*}$ |  | (3) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\alpha^{*},\beta^{*}$ |  | (3) |'
- en: '|  |  | $\displaystyle w_{c}=Clip(w,\alpha,\beta)$ |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle w_{c}=Clip(w,\alpha,\beta)$ |  |'
- en: '|  |  | $\displaystyle\begin{cases}\alpha\in[\min\_val,0)\\ \beta\in(0,\max\_val]\end{cases}$
    |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\begin{cases}\alpha\in[\min\_val,0)\\ \beta\in(0,\max\_val]\end{cases}$
    |  |'
- en: 'To demonstrate the efficacy of asymmetric quantization and clipping, we conduct
    a tensor-wise analysis. We selected a random weight tensor from the LLaMa-2-7B
    model and focused on a single output channel. As illustrated in Figure [3](#S3.F3
    "Figure 3 ‣ Asymmetric Clipping ‣ 3.1 Asymmetric Quantization and Clipping ‣ 3
    Methodology ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation"),
    our approach to asymmetric quantization and clipping achieves higher fidelity
    preservation compared to symmetric quantization. A more detailed ablation study
    on the impact of asymmetric quantization and clipping on model performance is
    presented in Table [3](#S4.T3 "Table 3 ‣ Asymmetric Quantization and Clipping
    ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation") in Section [4.4](#S4.SS4 "4.4 Ablation
    Studies ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation").'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '为了证明非对称量化和裁剪的有效性，我们进行了张量级分析。我们从 LLaMa-2-7B 模型中选择了一个随机权重张量，并集中于一个单一的输出通道。如图
    [3](#S3.F3 "图 3 ‣ 非对称裁剪 ‣ 3.1 非对称量化和裁剪 ‣ 3 方法 ‣ BitDistiller: 通过自我蒸馏释放 Sub-4-Bit
    LLMs 的潜力") 所示，我们的方法在非对称量化和裁剪方面相比对称量化实现了更高的保真度。关于非对称量化和裁剪对模型性能影响的更详细消融研究见于表 [3](#S4.T3
    "表 3 ‣ 非对称量化和裁剪 ‣ 4.4 消融研究 ‣ 4 实验 ‣ BitDistiller: 通过自我蒸馏释放 Sub-4-Bit LLMs 的潜力")，在第
    [4.4](#S4.SS4 "4.4 消融研究 ‣ 4 实验 ‣ BitDistiller: 通过自我蒸馏释放 Sub-4-Bit LLMs 的潜力") 节中。'
- en: '![Refer to caption](img/fa407fbb932d7c053a83677c5265e04f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/fa407fbb932d7c053a83677c5265e04f.png)'
- en: 'Figure 3: (Top) The original weight distribution of a single output channel
    in the final down projection layer of LLaMA-2-7B. (Middle&Bottom) The weight distribution
    after symmetric quantization and asymmetric quantization and clipping, both using
    3-bit quantization with the group size of 128.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: （顶部）LLaMA-2-7B 最终降维层中单一输出通道的原始权重分布。（中部和底部）对称量化和非对称量化及裁剪后的权重分布，两者均使用 3
    位量化和 128 的组大小。'
- en: 3.2 Self Distillation with CAKLD
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 使用 CAKLD 的自我蒸馏
- en: 'To better counteract the performance degradation caused by precision reduction,
    we propose to adopt Knowledge Distillation (KD) in QAT, where the full-precision
    model acts as a teacher and its quantized variant plays a student:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地抵消由精度降低引起的性能下降，我们建议在 QAT 中采用知识蒸馏（KD），其中全精度模型充当教师，其量化变体则作为学生：
- en: '|  | $\mathcal{L}=\mathcal{D}(P_{T}\parallel P_{S}),$ |  | (4) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\mathcal{D}(P_{T}\parallel P_{S}),$ |  | (4) |'
- en: where $\mathcal{D}$ denote the full-precision and quantized model, respectively.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{D}$ 分别表示全精度模型和量化模型。
- en: The intuition for KD is two-fold. First, learning the token-level probability
    distributions potentially helps the quantized model better imitate its full-precision
    counterpart Hinton et al. ([2015](#bib.bib18)), thereby re-gaining the strong
    downstream performance. Second, owing to the generative nature of LLM, it is easy
    to scale up the data size for QAT with the full-precision model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: KD 的直观理解有两个方面。首先，学习令牌级概率分布可以帮助量化模型更好地模仿其全精度对应模型 Hinton 等人 ([2015](#bib.bib18))，从而重新获得强大的下游性能。其次，由于
    LLM 的生成特性，可以轻松地扩展 QAT 的数据规模，使用全精度模型。
- en: The divergence $\mathcal{D}$) on instruction tuning Chung et al. ([2022](#bib.bib7)),
    while Forward KL promotes mode-covering and is superior on general text generation
    tasks like summarization Narayan et al. ([2018](#bib.bib32)). To provide a general
    receipt for QAT, we aim to seek a way to trade off the mode-seeking and mode-covering
    behaviors automatically, instead of manual selection according to some empirical
    understanding of downstream tasks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 发散 $\mathcal{D}$ 在指令调优 Chung 等人 ([2022](#bib.bib7)) 中，而正向 KL 促进模式覆盖，并在像总结这样的通用文本生成任务中表现更优，如
    Narayan 等人 ([2018](#bib.bib32))。为了提供 QAT 的通用方案，我们旨在寻求一种方式，自动权衡模式寻求和模式覆盖行为，而不是根据对下游任务的某些经验理解进行手动选择。
- en: '![Refer to caption](img/75d031142eb9c9ae65736f2a7c89d024.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/75d031142eb9c9ae65736f2a7c89d024.png)'
- en: 'Figure 4: Comparison of Reverse KL, Forward KL and CAKLD, when a Gaussian distribution
    tries to fit a Gaussian mixture (Teacher).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 反向 KL、正向 KL 和 CAKLD 的比较，当高斯分布试图拟合高斯混合模型（教师）时。'
- en: 'To this end, we propose a novel Confidence-Aware KL divergence, shorted as
    CAKLD. It blends the Reverse KL and Forward KL with a coefficient $\gamma$ estimated
    by the averaged token probability, so that the mode-seeking and mode-covering
    behaviors can be automatically traded off according to the full-precision model’s
    confidence on the training data:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们提出了一种新颖的置信度感知 KL 散度，简称 CAKLD。它通过系数 $\gamma$ 估计的平均令牌概率来混合 Reverse KL 和 Forward
    KL，从而根据全精度模型对训练数据的信心自动权衡模式寻求和模式覆盖行为：
- en: '|  | $$\displaystyle\begin{split}\mathcal{D}_{CAKLD}(P_{T}\parallel P_{S})&amp;=\gamma\mathcal{D}_{KL}(P_{S}\parallel
    P_{T})\\ &amp;+(1-\gamma)\mathcal{D}_{KL}(P_{T}\parallel P_{S})\\'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle\begin{split}\mathcal{D}_{CAKLD}(P_{T}\parallel P_{S})&amp;=\gamma\mathcal{D}_{KL}(P_{S}\parallel
    P_{T})\\ &amp;+(1-\gamma)\mathcal{D}_{KL}(P_{T}\parallel P_{S})\\'
- en: \end{split}$$ |  | (5) |
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: \end{split}$$ |  | (5) |
- en: '|  | $$\displaystyle\begin{split}\mathcal{D}_{KL}(P_{T}\parallel P_{S})=&amp;\mathbb{E}_{(x,y)\sim\mathbb{D}}[\frac{1}{&#124;\{y\}&#124;}\sum^{&#124;\{y\}&#124;}_{i=1}\\
    &amp;\mathbb{E}_{c\sim P_{T}(\cdot&#124;x,y_{<i})}[\log{\frac{P_{T}(c&#124;x,y_{<i})}{P_{S}(c&#124;x,y_{<i})}}]]\\'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle\begin{split}\mathcal{D}_{KL}(P_{T}\parallel P_{S})=&amp;\mathbb{E}_{(x,y)\sim\mathbb{D}}[\frac{1}{&#124;\{y\}&#124;}\sum^{&#124;\{y\}&#124;}_{i=1}\\
    &amp;\mathbb{E}_{c\sim P_{T}(\cdot&#124;x,y_{<i})}[\log{\frac{P_{T}(c&#124;x,y_{<i})}{P_{S}(c&#124;x,y_{<i})}}]]\\'
- en: \end{split}$$ |  |
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: \end{split}$$ |  |
- en: '|  | $1$2 |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'Intuitively, when the full-precision model is confident on the training data,
    CAKLD will prefer more on the mode-seeking behaviors. Otherwise, CAKLD will advocate
    more on the mode-covering behaviors, as the full-precision model is not certain
    about the data and modeling its single mode is suboptimal. Figure [4](#S3.F4 "Figure
    4 ‣ 3.2 Self Distillation with CAKLD ‣ 3 Methodology ‣ BitDistiller: Unleashing
    the Potential of Sub-4-Bit LLMs via Self-Distillation") visualizes the difference
    between Reverse KLD, Forward KLD and CAKLD when a Gaussian distribution tries
    to fit a Gaussian mixture. It is clear that CAKLD manages to trade off mode-seeking
    and mode-covering behaviors with the coefficient. For a detailed performance comparison
    and in-depth analysis, please refer to Figure [6](#S4.F6 "Figure 6 ‣ Distillation
    Objectives ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ BitDistiller: Unleashing the
    Potential of Sub-4-Bit LLMs via Self-Distillation") and Appendix [A.2](#A1.SS2
    "A.2 Implementation Details and Analysis of Confidence-Aware KLD ‣ Appendix A
    Appendix ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation")'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '直观上，当全精度模型对训练数据有信心时，CAKLD 更倾向于模式寻求行为。否则，CAKLD 更倾向于模式覆盖行为，因为全精度模型对数据的不确定性使得建模其单一模式是次优的。图 [4](#S3.F4
    "图 4 ‣ 3.2 使用 CAKLD 进行自蒸馏 ‣ 3 方法论 ‣ BitDistiller: 通过自蒸馏释放子 4 位 LLM 的潜力") 形象展示了当高斯分布试图拟合高斯混合时，Reverse
    KLD、Forward KLD 和 CAKLD 之间的差异。显然，CAKLD 成功地通过系数权衡了模式寻求和模式覆盖行为。有关详细的性能比较和深入分析，请参见图 [6](#S4.F6
    "图 6 ‣ 蒸馏目标 ‣ 4.4 消融研究 ‣ 4 实验 ‣ BitDistiller: 通过自蒸馏释放子 4 位 LLM 的潜力") 和附录 [A.2](#A1.SS2
    "A.2 置信度感知 KLD 的实现细节与分析 ‣ 附录 A 附录 ‣ BitDistiller: 通过自蒸馏释放子 4 位 LLM 的潜力")'
- en: 4 Experiments
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: '| LLaMA-2-7B | PPL $\downarrow$ | MMLU (5s) | PIQA | Hella. | Wino. | ARC-c
    | Avg |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | PPL $\downarrow$ | MMLU (5s) | PIQA | Hella. | Wino. | ARC-c
    | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| BF16 | 5.47 | 46.45 | 77.86 | 57.14 | 68.35 | 43.34 | 58.63 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| BF16 | 5.47 | 46.45 | 77.86 | 57.14 | 68.35 | 43.34 | 58.63 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | RTN | 6.65 | 38.65 | 75.24 | 53.70 | 67.32 | 38.56 | 54.69 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN | 6.65 | 38.65 | 75.24 | 53.70 | 67.32 | 38.56 | 54.69 |'
- en: '| 3 Bits | GPTQ | 6.38 | 39.57 | 75.46 | 51.68 | 67.16 | 38.39 | 54.45 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 3 位 | GPTQ | 6.38 | 39.57 | 75.46 | 51.68 | 67.16 | 38.39 | 54.45 |'
- en: '| g128 | AWQ | 6.71 | 39.68 | 76.27 | 55.14 | 67.56 | 40.61 | 55.85 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| g128 | AWQ | 6.71 | 39.68 | 76.27 | 55.14 | 67.56 | 40.61 | 55.85 |'
- en: '| OmniQuant | 6.10 | 41.22 | 77.47 | 54.41 | 67.09 | 39.08 | 55.85 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 6.10 | 41.22 | 77.47 | 54.41 | 67.09 | 39.08 | 55.85 |'
- en: '|  | LLM-QAT | 6.02 | 41.32 | 77.26 | 54.74 | 68.35 | 40.61 | 56.46 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-QAT | 6.02 | 41.32 | 77.26 | 54.74 | 68.35 | 40.61 | 56.46 |'
- en: '|  | BitDistiller (ours) | 5.97 | 43.65 | 76.99 | 55.38 | 68.35 | 41.21 | 57.12
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | BitDistiller (我们) | 5.97 | 43.65 | 76.99 | 55.38 | 68.35 | 41.21 | 57.12
    |'
- en: '|  | RTN | 3453 | 24.12 | 53.43 | 26.33 | 49.96 | 21.58 | 35.08 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN | 3453 | 24.12 | 53.43 | 26.33 | 49.96 | 21.58 | 35.08 |'
- en: '|  | GPTQ | NaN | 23.12 | 49.51 | 25.04 | 49.57 | 22.69 | 33.99 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | NaN | 23.12 | 49.51 | 25.04 | 49.57 | 22.69 | 33.99 |'
- en: '| 2 Bits | AWQ | 2.2e5 | 25.38 | 52.39 | 25.70 | 50.12 | 21.33 | 34.98 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 2 位 | AWQ | 2.2e5 | 25.38 | 52.39 | 25.70 | 50.12 | 21.33 | 34.98 |'
- en: '| g128 | OmniQuant | 12.84 | 25.42 | 58.92 | 29.20 | 50.83 | 19.45 | 36.76
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| g128 | OmniQuant | 12.84 | 25.42 | 58.92 | 29.20 | 50.83 | 19.45 | 36.76
    |'
- en: '|  | LLM-QAT | 9.30 | 23.62 | 70.08 | 43.79 | 61.64 | 29.09 | 45.64 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-QAT | 9.30 | 23.62 | 70.08 | 43.79 | 61.64 | 29.09 | 45.64 |'
- en: '|  | BitDistiller (ours) | 8.08 | 29.25 | 73.61 | 48.70 | 61.09 | 33.27 | 49.18
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | BitDistiller（我们的方法） | 8.08 | 29.25 | 73.61 | 48.70 | 61.09 | 33.27 | 49.18
    |'
- en: 'Table 1: General language task results of BitDistiller versus established PTQ
    and QAT methods on LLaMA-2-7B Model. Our method achieves leading performance in
    both 3-bit and 2-bit quantization.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：BitDistiller 与现有 PTQ 和 QAT 方法在 LLaMA-2-7B 模型上的通用语言任务结果。我们的方法在 3 位和 2 位量化中都达到了领先的性能。
- en: We evaluate BitDistiller on the LLaMA-2 Touvron et al. ([2023](#bib.bib41))
    families and domain-specific LLMs with sub-4–bit quantization. We have set up
    comparative experiments to demonstrate the proficiency of our method against existing
    PTQ and QAT methods. Our findings illustrate that BitDistiller substantially enhances
    both the general language performance and the accuracy of reasoning tasks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 LLaMA-2 Touvron 等人（[2023](#bib.bib41)）系列和具有小于 4 位量化的特定领域 LLMs 上评估了 BitDistiller。我们设置了对比实验，以展示我们方法相较于现有的
    PTQ 和 QAT 方法的优越性。我们的研究结果表明，BitDistiller 显著提升了通用语言表现和推理任务的准确性。
- en: 4.1 Experimental Settings
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Tasks and Models
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务与模型
- en: Following Frantar et al. ([2022](#bib.bib13)); Lin et al. ([2023](#bib.bib26)),
    we benchmark LLaMA-2 Touvron et al. ([2023](#bib.bib41)) on general language tasks,
    including language modeling tasks (WikiText-2 Merity et al. ([2016](#bib.bib31))),
    common sense QA benchmarks (PIQA Bisk et al. ([2020](#bib.bib3)), HellaSwag Zellers
    et al. ([2019](#bib.bib46)), WinoGrande Sakaguchi et al. ([2021](#bib.bib35)),
    ARC Clark et al. ([2018](#bib.bib8))) and in-context learning ability (MMLU Hendrycks
    et al. ([2020](#bib.bib17))) under a few-shot setting. We also consider the complex
    reasoning tasks and evaluate various sizes of domain-specific LLMs, including
    WizardCoder Luo et al. ([2023](#bib.bib30)) on LLM-Humaneval-Benchmarks Chen et al.
    ([2021](#bib.bib6)) in the setting of greedy decode, and MetaMath Yu et al. ([2023](#bib.bib44))
    on GSM8K Cobbe et al. ([2021](#bib.bib9)). To evaluate the domain-specific LLMs
    of smaller sizes, we finetune OpenLLaMA-3B Geng and Liu ([2023](#bib.bib14)) with
    domain-specific datasets.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Frantar 等人（[2022](#bib.bib13)）；Lin 等人（[2023](#bib.bib26)）的研究，我们在 LLaMA-2
    Touvron 等人（[2023](#bib.bib41)）上进行通用语言任务的基准测试，包括语言建模任务（WikiText-2 Merity 等人（[2016](#bib.bib31)））、常识问答基准（PIQA
    Bisk 等人（[2020](#bib.bib3)）、HellaSwag Zellers 等人（[2019](#bib.bib46)）、WinoGrande
    Sakaguchi 等人（[2021](#bib.bib35)）、ARC Clark 等人（[2018](#bib.bib8)））和上下文学习能力（MMLU
    Hendrycks 等人（[2020](#bib.bib17)））的少样本设置。我们还考虑了复杂推理任务，并评估了各种尺寸的特定领域 LLMs，包括在 LLM-Humaneval-Benchmarks
    Chen 等人（[2021](#bib.bib6)）的贪婪解码设置下的 WizardCoder Luo 等人（[2023](#bib.bib30)），以及在
    GSM8K Cobbe 等人（[2021](#bib.bib9)）上的 MetaMath Yu 等人（[2023](#bib.bib44)）。为了评估较小尺寸的特定领域
    LLMs，我们对 OpenLLaMA-3B Geng 和 Liu（[2023](#bib.bib14)）进行特定领域数据集的微调。
- en: Baselines
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线
- en: 'PTQ baselines include vanilla round-to-nearest (RTN), GPTQ Frantar et al. ([2022](#bib.bib13)),
    AWQ Lin et al. ([2023](#bib.bib26)), Omniquant Shao et al. ([2023](#bib.bib38))
    and QuIP Chee et al. ([2023](#bib.bib5)). QAT baselines include LLM-QAT Liu et al.
    ([2023b](#bib.bib28)) and TSLD Kim et al. ([2023a](#bib.bib22)). Detailed PTQ
    and QAT settings can be found in appendix [A.1](#A1.SS1 "A.1 Details of PTQ and
    QAT Configuration ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation").'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'PTQ 基线包括原始的四舍五入（RTN）、GPTQ Frantar 等人（[2022](#bib.bib13)）、AWQ Lin 等人（[2023](#bib.bib26)）、Omniquant
    Shao 等人（[2023](#bib.bib38)）和 QuIP Chee 等人（[2023](#bib.bib5)）。QAT 基线包括 LLM-QAT
    Liu 等人（[2023b](#bib.bib28)）和 TSLD Kim 等人（[2023a](#bib.bib22)）。详细的 PTQ 和 QAT 设置可以在附录
    [A.1](#A1.SS1 "A.1 Details of PTQ and QAT Configuration ‣ Appendix A Appendix
    ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation")
    中找到。'
- en: Quantization and Distillation
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化与蒸馏
- en: 'We focus on 3-bit/2-bit group-wise quantization, with a group size of 128 (represented
    as ’g’) as the default setting except for the 3B models with a group size of 64
    because of the dimension constraint. Following Liu et al. ([2023b](#bib.bib28));
    Kim et al. ([2023a](#bib.bib22)), we utilize logits distillation. Prior to QAT,
    the coefficient $\gamma$. The implementation details and example analysis of CAKLD
    are available in Appendix [A.2](#A1.SS2 "A.2 Implementation Details and Analysis
    of Confidence-Aware KLD ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation").'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专注于 3 位/2 位组内量化，默认设置的组大小为 128（表示为 ’g’），除了 3B 模型的组大小为 64 由于维度限制。根据 Liu 等人 ([2023b](#bib.bib28))；Kim
    等人 ([2023a](#bib.bib22))，我们利用 logits 蒸馏。在 QAT 之前，系数 $\gamma$。CAKLD 的实现细节和示例分析可在附录
    [A.2](#A1.SS2 "A.2 信心感知 KLD 的实现细节和分析 ‣ 附录 A 附录 ‣ BitDistiller：通过自蒸馏释放子 4 位 LLM
    的潜力") 中找到。
- en: Training Datasets
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练数据集
- en: We use the instruction-tuning data from Alpaca Taori et al. ([2023](#bib.bib40))
    and the training set of WikiText-2 for general language tasks. For code understanding
    and generation, we use Evol-Instruct-Code Rosh ([2023](#bib.bib34)). For math
    reasoning we use MetaMathQA Yu et al. ([2023](#bib.bib44)).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用来自 Alpaca Taori 等人 ([2023](#bib.bib40)) 的指令调优数据和 WikiText-2 的训练集用于一般语言任务。对于代码理解和生成，我们使用
    Evol-Instruct-Code Rosh ([2023](#bib.bib34))。对于数学推理，我们使用 MetaMathQA Yu 等人 ([2023](#bib.bib44))。
- en: 'Given the instruction prompt $x$. (See Appendix [A.3](#A1.SS3 "A.3 Training
    Datasets Examples ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation") for more details of training datasets
    composition).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 给定指令提示 $x$。 (有关训练数据集组成的更多细节，请参见附录 [A.3](#A1.SS3 "A.3 训练数据集示例 ‣ 附录 A 附录 ‣ BitDistiller：通过自蒸馏释放子
    4 位 LLM 的潜力")。)
- en: Training Implementation
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练实现
- en: We leverage DeepSpeed Rasley et al. ([2020](#bib.bib33)) and HuggingFace repository Wolf
    et al. ([2020](#bib.bib43)) to devise a QAT-based KD framework enabling the distillation
    of models up to 34B. The model optimization is facilitated through the AdamW optimizer
    Loshchilov and Hutter ([2017](#bib.bib29)), applied with zero weight decay. We
    initialize the constant learning rate to 8e-6 and set the sequence length to 1024
    for the code-related task and 512 for others.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用 DeepSpeed Rasley 等人 ([2020](#bib.bib33)) 和 HuggingFace 仓库 Wolf 等人 ([2020](#bib.bib43))
    制定了一个基于 QAT 的 KD 框架，使得可以蒸馏高达 34B 的模型。模型优化通过 AdamW 优化器 Loshchilov 和 Hutter ([2017](#bib.bib29))
    进行，并应用零权重衰减。我们将常量学习率初始化为 8e-6，并为代码相关任务设置序列长度为 1024，其它任务为 512。
- en: 4.2 Evaluation on Language Modeling Tasks
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 语言建模任务评估
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation") presents a comparative analysis of BitDistiller’s
    performance against previous PTQ and QAT methods on general language tasks. BitDistiller
    surpasses competing methods in terms of WikiText-2 perplexity and MMLU (5-shot)
    accuracy. Furthermore, BitDistiller demonstrates consistent performance across
    various QA benchmarks. Notably, in 2-bit weight quantization, BitDistiller substantially
    increases the average accuracy by +3.54% over LLM-QAT Liu et al. ([2023b](#bib.bib28))
    and by +12.43% compared to the leading PTQ method Shao et al. ([2023](#bib.bib38)).
    Similar results on LLaMA-2-13B can be found in Table [9](#A1.T9 "Table 9 ‣ A.5
    Integration with AWQ For Quantization Strategies ‣ Appendix A Appendix ‣ BitDistiller:
    Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation") in the Appendix
    [A.4](#A1.SS4 "A.4 Evaluation of General Language Tasks on LLaMA-2-13B ‣ Appendix
    A Appendix ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation").'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S4.T1 "表 1 ‣ 4 实验 ‣ BitDistiller：通过自蒸馏释放子 4 位 LLM 的潜力") 展示了 BitDistiller
    在一般语言任务上与以往 PTQ 和 QAT 方法的性能比较。BitDistiller 在 WikiText-2 困惑度和 MMLU（5-shot）准确率方面超越了竞争方法。此外，BitDistiller
    在各种 QA 基准测试中表现稳定。特别是在 2 位权重量化中，BitDistiller 将平均准确率提升了 +3.54% 相比于 LLM-QAT Liu 等人
    ([2023b](#bib.bib28))，并且比领先的 PTQ 方法 Shao 等人 ([2023](#bib.bib38)) 提高了 +12.43%。LLaMA-2-13B
    上的类似结果可以在附录 [A.4](#A1.SS4 "A.4 在 LLaMA-2-13B 上评估一般语言任务 ‣ 附录 A 附录 ‣ BitDistiller：通过自蒸馏释放子
    4 位 LLM 的潜力") 的表 [9](#A1.T9 "表 9 ‣ A.5 与 AWQ 结合的量化策略 ‣ 附录 A 附录 ‣ BitDistiller：通过自蒸馏释放子
    4 位 LLM 的潜力") 中找到。
- en: 4.3 Evaluation on Reasoning Tasks
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 推理任务评估
- en: '| Domain-specific LLMs | HumanEval @WizardCoder | GSM8K @MetaMath |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 领域特定 LLMs | HumanEval @WizardCoder | GSM8K @MetaMath |'
- en: '| --- | --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 3B | 7B | 13B | 34B | 3B | 7B | 13B |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 3B | 7B | 13B | 34B | 3B | 7B | 13B |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| BF16 | 23.17 | 54.88 | 62.80 | 71.95 | 36.40 | 66.41 | 72.30 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| BF16 | 23.17 | 54.88 | 62.80 | 71.95 | 36.40 | 66.41 | 72.30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | RTN | 4.27 | 34.15 | 50.00 | 33.54 | 17.50 | 59.30 | 68.51 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN | 4.27 | 34.15 | 50.00 | 33.54 | 17.50 | 59.30 | 68.51 |'
- en: '|  | GPTQ | 4.30 | 46.34 | 55.48 | 63.41 | 6.72 | 62.11 | 68.75 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 4.30 | 46.34 | 55.48 | 63.41 | 6.72 | 62.11 | 68.75 |'
- en: '| 3 Bits | AWQ | 16.46 | 45.73 | 53.04 | 67.07 | 21.87 | 62.34 | 68.67 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 3 Bits | AWQ | 16.46 | 45.73 | 53.04 | 67.07 | 21.87 | 62.34 | 68.67 |'
- en: '| g128 | OmniQuant | 10.36 | 44.51 | 54.88 | 68.90 | 23.67 | 61.70 | 68.28
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| g128 | OmniQuant | 10.36 | 44.51 | 54.88 | 68.90 | 23.67 | 61.70 | 68.28
    |'
- en: '|  | LLM-QAT | 18.29 | 48.78 | 57.92 | 66.46 | 26.25 | 60.78 | 66.62 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-QAT | 18.29 | 48.78 | 57.92 | 66.46 | 26.25 | 60.78 | 66.62 |'
- en: '|  | BitDistiller (ours) | 20.73 | 53.66 | 63.41 | 69.51 | 32.50 | 64.38 |
    69.69 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | BitDistiller（我们的） | 20.73 | 53.66 | 63.41 | 69.51 | 32.50 | 64.38 | 69.69
    |'
- en: '|  | RTN | 0.0 | 0.0 | 0.0 | 0.61 | 0.0 | 0.0 | 7.89 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN | 0.0 | 0.0 | 0.0 | 0.61 | 0.0 | 0.0 | 7.89 |'
- en: '|  | GPTQ | 0.0 | 0.0 | 1.83 | 3.65 | 0.0 | 0.0 | 11.43 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 0.0 | 0.0 | 1.83 | 3.65 | 0.0 | 0.0 | 11.43 |'
- en: '| 2 Bits | AWQ | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 7.89 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 2 Bits | AWQ | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 7.89 |'
- en: '| g128 | OmniQuant | 0.0 | 0.0 | 20.12 | 26.83 | 0.0 | 0.0 | 9.45 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| g128 | OmniQuant | 0.0 | 0.0 | 20.12 | 26.83 | 0.0 | 0.0 | 9.45 |'
- en: '|  | LLM-QAT | 0.0 | 14.63 | 15.21 | 29.27 | 6.56 | 23.13 | 36.64 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-QAT | 0.0 | 14.63 | 15.21 | 29.27 | 6.56 | 23.13 | 36.64 |'
- en: '|  | BitDistiller (ours) | 7.31 | 36.59 | 42.07 | 46.34 | 16.09 | 51.02 | 61.33
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | BitDistiller（我们的） | 7.31 | 36.59 | 42.07 | 46.34 | 16.09 | 51.02 | 61.33
    |'
- en: 'Table 2: Reasoning task results of BitDistiller versus established PTQ and
    QAT methods on domain-specific LLMs. Our method achieves leading performance in
    both 3-bit and 2-bit quantization.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：BitDistiller 与已建立的 PTQ 和 QAT 方法在特定领域 LLMs 上的推理任务结果。我们的方法在 3-bit 和 2-bit
    量化中都取得了领先的表现。
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.3 Evaluation on Reasoning Tasks ‣ 4 Experiments
    ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation")
    demonstrates the superior performance of BitDistiller on reasoning-based benchmarks,
    including HumanEval and GSM8K, across a range of domain-specific language model
    families. BitDistiller achieves improvements over other methods in both 3-bit
    and 2-bit quantization. Especially in 2-bit quantization, while other methods
    exhibit significant performance drops, BitDistiller maintains a commendable level
    of accuracy. Detailedly, our method outperforms LLM-QAT by a remarkable margin
    of 24.69%, achieving an accuracy of 61.33% on complex mathematical reasoning tasks.
    These outcomes bolster the potential for implementing ultra-low-precision inference
    deployment in practical reasoning tasks without substantially compromising performance.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [2](#S4.T2 "Table 2 ‣ 4.3 Evaluation on Reasoning Tasks ‣ 4 Experiments ‣
    BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation")
    展示了 BitDistiller 在基于推理的基准测试中的优越表现，包括 HumanEval 和 GSM8K，在各种领域特定的语言模型家族中。BitDistiller
    在 3-bit 和 2-bit 量化中都取得了改进。特别是在 2-bit 量化中，尽管其他方法表现显著下降，BitDistiller 仍保持了可观的准确度。详细而言，我们的方法在复杂数学推理任务上比
    LLM-QAT 高出 24.69%，达到了 61.33% 的准确率。这些结果增强了在实际推理任务中实施超低精度推断部署的潜力，而不会大幅降低性能。'
- en: 4.4 Ablation Studies
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 消融研究
- en: Asymmetric Quantization and Clipping
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非对称量化和剪裁
- en: '| LLaMA-2-7B | PPL $\downarrow$ |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | PPL $\downarrow$ |'
- en: '| (start $\mapsto$ end ) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| (start $\mapsto$ end) |'
- en: '| 3 Bits | NF-Sym | 6.45 $\mapsto$ 39.27 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 3 Bits | NF-Sym | 6.45 $\mapsto$ 39.27 |'
- en: '| g128 | $\rightarrow$ 42.61 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| g128 | $\rightarrow$ 42.61 |'
- en: '| $+$ 43.65 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| $+$ 43.65 |'
- en: '| 2 Bits | INT-Sym | 2.4e5 $\mapsto$ 26.03 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 2 Bits | INT-Sym | 2.4e5 $\mapsto$ 26.03 |'
- en: '| g128 | $\rightarrow$ 24.82 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| g128 | $\rightarrow$ 24.82 |'
- en: '| $+$ 29.25 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| $+$ 29.25 |'
- en: 'Table 3: Ablation study of asymmetric quantization and clipping on WikiText2
    perplexity and MMLU (5-shot). The "start $\mapsto$ end" notation denotes the metric
    values before and after training.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在 WikiText2 困惑度和 MMLU（5-shot）上对非对称量化和剪裁的消融研究。“start $\mapsto$ end”表示训练前后的度量值。
- en: In this ablation study, we evaluate the efficacy of quantization strategies
    on the LLaMA-2-7B model. Our approach examines the impact of asymmetric quantization
    and clipping techniques within QAT. We specifically assess the 3-bit and 2-bit
    quantization levels, reporting our findings in terms of Perplexity (PPL) and MMLU
    (5-shot).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项消融研究中，我们评估了量化策略在 LLaMA-2-7B 模型上的有效性。我们的方法考察了非对称量化和剪裁技术在 QAT 中的影响。我们特别评估了
    3-bit 和 2-bit 量化级别，并报告了困惑度（PPL）和 MMLU（5-shot）的结果。
- en: 'As demonstrated in Table [3](#S4.T3 "Table 3 ‣ Asymmetric Quantization and
    Clipping ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ BitDistiller: Unleashing the
    Potential of Sub-4-Bit LLMs via Self-Distillation"), asymmetric quantization significantly
    enhances model performance. Notably, under a 2-bit configuration, PPL can be reduced
    from 3.4e2 to 16.94 in post-training. Furthermore, the application of asymmetric
    clipping during initialization yields additional performance gains upon training
    completion. See Appendix  [A.5](#A1.SS5 "A.5 Integration with AWQ For Quantization
    Strategies ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation") for integration with other PTQ methods.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [3](#S4.T3 "表 3 ‣ 非对称量化与剪切 ‣ 4.4 消融研究 ‣ 4 实验 ‣ BitDistiller: 通过自我蒸馏释放 Sub-4-Bit
    LLMs 的潜力") 所示，非对称量化显著提升了模型性能。特别是，在 2 位配置下，PPL 可以从 3.4e2 降低到 16.94，且在训练后进一步应用非对称剪切会带来额外的性能提升。有关与其他
    PTQ 方法的集成，请参见附录 [A.5](#A1.SS5 "A.5 与 AWQ 集成的量化策略 ‣ 附录 A 附录 ‣ BitDistiller: 通过自我蒸馏释放
    Sub-4-Bit LLMs 的潜力")。'
- en: Data Generation
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据生成
- en: '![Refer to caption](img/28340878193de2c22949d5511b09e50c.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/28340878193de2c22949d5511b09e50c.png)'
- en: (a)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/4d98085dba963f0ca59ca540e18c3a5e.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4d98085dba963f0ca59ca540e18c3a5e.png)'
- en: (b)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 5: Comparative analysis of using various data generation methods on
    WizardCoder-7B. (a) shows the per-token cross-entropy loss. (b) presents the HumanEval
    Pass@1\. (‘QAT w.o. KD’ indicates the baseline where only the ground truth dataset
    is used for supervised fine-tuning, without knowledge distillation.)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 使用不同数据生成方法在 WizardCoder-7B 上的比较分析。(a) 显示了每个 token 的交叉熵损失。(b) 展示了 HumanEval
    Pass@1\.（‘QAT w.o. KD’ 表示仅使用真实数据集进行监督微调的基线，没有知识蒸馏。）'
- en: In our analysis, we meticulously evaluated the logit information of the teacher
    model by computing the cross-entropy loss (CELoss) for various outputs $y$.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分析中，我们通过计算不同输出 $y$ 的交叉熵损失 (CELoss) 来细致评估教师模型的 logit 信息。
- en: Distillation Objectives
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 蒸馏目标
- en: 'In Figure [6](#S4.F6 "Figure 6 ‣ Distillation Objectives ‣ 4.4 Ablation Studies
    ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via
    Self-Distillation"), we demonstrate the effectiveness of our proposed Confidence-Aware
    KL Divergence (CAKLD) by showcasing performance indicators for reasoning tasks
    under different objective functions. Our findings show that CAKLD outperforms
    other objective functions. Though JSD also has a bounded coefficient for interpolation,
    in practice we observe that it has a weak ability to converge for QAT.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [6](#S4.F6 "图 6 ‣ 蒸馏目标 ‣ 4.4 消融研究 ‣ 4 实验 ‣ BitDistiller: 通过自我蒸馏释放 Sub-4-Bit
    LLMs 的潜力") 中，我们展示了我们提出的 **Confidence-Aware KL Divergence (CAKLD)** 的有效性，通过展示不同目标函数下的推理任务性能指标。我们的发现表明
    CAKLD 优于其他目标函数。尽管 JSD 也有一个有限的插值系数，但在实际应用中我们观察到它在 QAT 上的收敛能力较弱。'
- en: '![Refer to caption](img/970b5ee32452d2e5dd447101a902b61e.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/970b5ee32452d2e5dd447101a902b61e.png)'
- en: 'Figure 6: Performance comparison between different objective functions on WizardCoder-7B
    and MetaMath-7B with domain-specific tasks.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 在 WizardCoder-7B 和 MetaMath-7B 上，不同目标函数在特定领域任务中的性能比较。'
- en: 4.5 Analysis and Discussion
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 分析与讨论
- en: Comparison with QuIP
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与 QuIP 的比较
- en: 'QuIP enhances 2-bit PTQ for LLMs through incoherence processing. Its subsequent
    iteration, QuIP#¹¹1[https://cornell-relaxml.github.io/quip-sharp/](https://cornell-relaxml.github.io/quip-sharp/),
    refines this approach by shifting from scalar quantization to vector quantization
    via lattice codebooks, significantly narrowing the performance gap with 16-bit
    models. For a consistent comparison, we utilize the BF16 pretrained model and
    then apply Quip(#) and BitDistiller. As shown in Table [4](#S4.T4 "Table 4 ‣ Comparison
    with QuIP ‣ 4.5 Analysis and Discussion ‣ 4 Experiments ‣ BitDistiller: Unleashing
    the Potential of Sub-4-Bit LLMs via Self-Distillation"), our BitDistiller surpasses
    QuIP across all benchmarks. In comparison with QuIP#, BitDistiller retains its
    superior performance in language modeling and programming, while QuIP# outperforms
    in mathematical reasoning. Being orthogonal to QAT with distillation, PTQ incorporating
    incoherence processing and vector quantization could potentially serve as an effective
    initialization method for BitDistiller. We intend to explore whether the integration
    of QuIP(#) into BitDistiller can further improve the performance of low-bit models.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: QuIP通过不一致性处理增强了LLMs的2位PTQ。其后续版本QuIP#¹¹1[https://cornell-relaxml.github.io/quip-sharp/](https://cornell-relaxml.github.io/quip-sharp/)，通过从标量量化转向通过格子码本的向量量化，显著缩小了与16位模型的性能差距。为了进行一致的比较，我们使用BF16预训练模型，然后应用Quip(#)和BitDistiller。如表[4](#S4.T4
    "表4 ‣ 与QuIP的比较 ‣ 4.5 分析与讨论 ‣ 4 实验 ‣ BitDistiller：通过自蒸馏释放子4位LLMs的潜力")所示，我们的BitDistiller在所有基准测试中都超越了QuIP。与QuIP#的比较中，BitDistiller在语言建模和编程中保持其优越性能，而QuIP#在数学推理中表现更佳。作为与蒸馏的QAT正交的PTQ，不一致性处理和向量量化的PTQ可能作为BitDistiller的有效初始化方法。我们计划探索QuIP(#)的集成是否能进一步提高低位模型的性能。
- en: '| Method | LLaMA-2-7B | WizardCoder-7B | MetaMath-7B |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LLaMA-2-7B | WizardCoder-7B | MetaMath-7B |'
- en: '| --- | --- | --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| PPL$\downarrow$ | MMLU (5s) | QA-avg | HumanEval | GSM8K |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| PPL$\downarrow$ | MMLU (5s) | QA-avg | HumanEval | GSM8K |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| BF16 | 5.47 | 46.45 | 61.67 | 54.88 | 66.41 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| BF16 | 5.47 | 46.45 | 61.67 | 54.88 | 66.41 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 2 Bits | Quip | 728.15 | 24.30 | 38.19 | 0.0 | 0.0 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 2位 | Quip | 728.15 | 24.30 | 38.19 | 0.0 | 0.0 |'
- en: '| g128 | Quip# | 8.97 | 30.90 | 52.40 | 12.96 | 60.00 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| g128 | Quip# | 8.97 | 30.90 | 52.40 | 12.96 | 60.00 |'
- en: '| BitDistiller | 8.08 | 29.25 | 54.17 | 36.58 | 51.02 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| BitDistiller | 8.08 | 29.25 | 54.17 | 36.58 | 51.02 |'
- en: 'Table 4: Performance comparison of 2-bit quantized models using QuIP, QuIP#,
    and BitDistiller on LLaMA-2-7B, WizardCoder-7B, and MetaMath-7B.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：使用QuIP、QuIP#和BitDistiller对LLaMA-2-7B、WizardCoder-7B和MetaMath-7B的2位量化模型的性能比较。
- en: Comparison with TSLD
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与TSLD的比较
- en: 'Prior work Kim et al. ([2023a](#bib.bib22)) introduced Token-Scaled Logit Distillation
    (TSLD) to alleviate overfitting during QAT. To facilitate a direct and fair comparison
    between TSLD and our CAKLD, we incorporate TSLD into the BitDistiller framework
    by replacing CAKLD with TSLD while keeping all other settings unchanged. As depicted
    in Figure [7](#S4.F7 "Figure 7 ‣ Comparison with TSLD ‣ 4.5 Analysis and Discussion
    ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via
    Self-Distillation"), CAKLD not only converges more rapidly but also delivers superior
    overall performance compared to TSLD.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的工作Kim等人（[2023a](#bib.bib22)）引入了Token-Scaled Logit Distillation (TSLD)以减轻QAT期间的过拟合。为了便于TSLD和我们CAKLD之间的直接和公平比较，我们将TSLD融入BitDistiller框架中，通过用TSLD替换CAKLD而保持所有其他设置不变。如图[7](#S4.F7
    "图7 ‣ 与TSLD的比较 ‣ 4.5 分析与讨论 ‣ 4 实验 ‣ BitDistiller：通过自蒸馏释放子4位LLMs的潜力")所示，CAKLD不仅收敛更快，而且总体性能优于TSLD。
- en: '![Refer to caption](img/b483fc97a611cbacac720db02e970718.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b483fc97a611cbacac720db02e970718.png)'
- en: 'Figure 7: Comparison of TSLD and CAKLD on perplexity (left) and reasoning tasks
    performance (right).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：TSLD和CAKLD在困惑度（左）和推理任务性能（右）的比较。
- en: Effectiveness of Self-Distillation
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自蒸馏的有效性
- en: 'Table [5](#S4.T5 "Table 5 ‣ Effectiveness of Self-Distillation ‣ 4.5 Analysis
    and Discussion ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation") compares 2-bit QAT performance using the LLaMA-2-7B
    or larger LLaMA-2-13B as the teacher model. Surprisingly, in practice the larger
    13B model didn’t improve accuracy, hinting that a teacher with the same model
    architecture as the student may enhance weight alignment and probability distribution
    matching, thereby improving model effectiveness. Further investigation and deeper
    analysis are needed in future work to fully understand the implications of different
    teacher-student sizes and architectures in QAT.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [5](#S4.T5 "表 5 ‣ 自蒸馏的有效性 ‣ 4.5 分析与讨论 ‣ 4 实验 ‣ BitDistiller: 通过自蒸馏释放子 4-bit
    LLM 的潜力") 比较了使用 LLaMA-2-7B 或更大的 LLaMA-2-13B 作为教师模型的 2-bit QAT 性能。令人惊讶的是，在实践中，较大的
    13B 模型并未提高准确性，这表明具有相同模型架构的教师可能会改善权重对齐和概率分布匹配，从而提升模型效果。未来的工作需要进一步调查和深入分析，以全面理解不同教师-学生大小和架构在
    QAT 中的影响。'
- en: '| LLaMA-2-7B | Quantized Student | Teacher | PPL $\downarrow$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | 量化学生 | 教师 | PPL $\downarrow$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 2 Bits | 7B | 13B | 8.12 | 28.27 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 2 Bits | 7B | 13B | 8.12 | 28.27 |'
- en: '| g128 | 7B | 7B | 8.08 | 29.25 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| g128 | 7B | 7B | 8.08 | 29.25 |'
- en: 'Table 5: Performance comparison of 2-bit quantized models using LLaMA-2-13B
    and LLaMA-2-7B as the teacher model.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：使用 LLaMA-2-13B 和 LLaMA-2-7B 作为教师模型的 2-bit 量化模型性能比较。
- en: Training Efficiency
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练效率
- en: 'Table [6](#S4.T6 "Table 6 ‣ Training Efficiency ‣ 4.5 Analysis and Discussion
    ‣ 4 Experiments ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via
    Self-Distillation") highlights the efficiency of BitDistiller compared to LLM-QAT
    Liu et al. ([2023b](#bib.bib28)) in quantizing the WizardCoder-7B model. The results
    demonstrate a dramatic reduction in the total time required for quantization:
    BitDistiller completes the process in approximately 3 hours on a single A100-80G
    GPU, as opposed to the hundreds of GPU hours required by LLM-QAT. (Original LLM-QAT
    uses 64 GPUs. For a direct and fair comparison, we evaluate the GPU hours needed
    for LLM-QAT on a single GPU.)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [6](#S4.T6 "表 6 ‣ 训练效率 ‣ 4.5 分析与讨论 ‣ 4 实验 ‣ BitDistiller: 通过自蒸馏释放子 4-bit
    LLM 的潜力") 突出了 BitDistiller 相较于 LLM-QAT Liu 等 ([2023b](#bib.bib28)) 在量化 WizardCoder-7B
    模型方面的效率。结果表明，量化所需的总时间大幅减少：BitDistiller 在一块 A100-80G GPU 上完成该过程约需 3 小时，而 LLM-QAT
    需要数百小时的 GPU 时间。（原始的 LLM-QAT 使用了 64 块 GPU。为了直接和公平地比较，我们评估了 LLM-QAT 在单块 GPU 上所需的
    GPU 小时。）'
- en: '| Method | Devices | #Data | Time (Hours) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 设备 | 数据量 | 时间（小时） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Data Gen | Quant Init | QAT | Total |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 数据生成 | 量化初始化 | QAT | 总计 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| LLM-QAT | 1 * A100 80G | 100K | 270 | 0 | 10.64 | 280.64 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QAT | 1 * A100 80G | 100K | 270 | 0 | 10.64 | 280.64 |'
- en: '| BitDistiller | 2K | 1.47 | 0.63 | 0.92 | 3.02 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| BitDistiller | 2K | 1.47 | 0.63 | 0.92 | 3.02 |'
- en: 'Table 6: Time required for LLM-QAT and BitDistiller to quantize WizardCoder-7B
    on a NVIDIA A100-80G.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：LLM-QAT 和 BitDistiller 在 NVIDIA A100-80G 上量化 WizardCoder-7B 所需的时间。
- en: 5 Conclusion
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: BitDistiller leverages QAT with self-distillation to boost sub-4-bit LLM performance.
    The asymmetric quantization and clipping strategies, coupled with the innovative
    CAKLD objective, facilitate faster learning and superior performance. BitDistiller
    outperforms existing PTQ and QAT methods, achieving notable improvements in 3/2-bit
    settings across diverse language and reasoning tasks. Moreover, BitDistiller is
    more cost-efficient with fewer data and training resources required.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: BitDistiller 利用自蒸馏的 QAT 来提升子 4-bit LLM 的性能。非对称量化和剪切策略，加上创新的 CAKLD 目标，促进了更快的学习和更优异的性能。BitDistiller
    在 3/2-bit 设置中超越了现有的 PTQ 和 QAT 方法，在各种语言和推理任务中取得了显著的改善。此外，BitDistiller 需要的数据和训练资源更少，从而具有更高的成本效益。
- en: Limitations
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: Despite the promising results demonstrated by BitDistiller, it is important
    to acknowledge certain limitations and areas for future investigation.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 BitDistiller 展示了有希望的结果，但仍需认识到某些限制和未来研究的领域。
- en: A key limitation lies in the empirical nature of our findings. For instance,
    the reason behind the counterintuitive outcome where a 7B model outperforms a
    13B model as a teacher during the distillation of a 2-bit 7B student model. Having
    the same model architecture may be the reason but not detailed explained and understood.
    This highlights the need for a deeper investigation and theoretical exploration
    to complement our empirical observations.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键的限制在于我们发现的经验性质。例如，在将 2-bit 7B 学生模型进行蒸馏时，7B 模型作为教师的表现优于 13B 模型这一反直觉结果的原因。相同的模型架构可能是原因，但尚未详细解释和理解。这突显了需要更深入的研究和理论探索来补充我们的经验观察。
- en: Looking ahead, we aim to extend BitDistiller to the realm of 1-bit (binary)
    quantization. While this presents a more challenging scenario, it also offers
    the potential for significant advancements in efficient LLM inference as binary
    weights enables computation with only additions and without multiplications.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，我们计划将 BitDistiller 扩展到 1-bit（二进制）量化领域。尽管这提出了更具挑战性的情境，但也为高效 LLM 推理带来了显著进展的潜力，因为二进制权重使得计算仅需加法而无需乘法。
- en: Moreover, the current iteration of BitDistiller applies exclusively to scalar
    quantization. As future work, we plan to explore the adaptation of BitDistiller
    to vector quantization. Preliminary research in this area indicates that vector
    quantization could yield substantial benefits, and incorporating it into our framework
    represents a natural and promising progression of our research.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当前版本的 BitDistiller 专门应用于标量量化。作为未来的工作，我们计划探索将 BitDistiller 适配于向量量化。初步研究表明，向量量化可能带来显著的好处，将其纳入我们的框架代表了我们研究的自然和有前景的进展。
- en: Acknowledgements
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank the HPC-AI-Integrated Intelligent Computing center of
    HKUST(GZ) for providing some of the hardware platforms in this project.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢香港科技大学（广州）HPC-AI-Integrated Intelligent Computing 中心为本项目提供的一些硬件平台。
- en: References
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Agarwal et al. (2023) Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela
    Ramos, Matthieu Geist, and Olivier Bachem. 2023. Gkd: Generalized knowledge distillation
    for auto-regressive sequence models. *arXiv preprint arXiv:2306.13649*.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Agarwal 等人（2023）Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos,
    Matthieu Geist 和 Olivier Bachem. 2023. Gkd: 自回归序列模型的广义知识蒸馏。*arXiv 预印本 arXiv:2306.13649*。'
- en: Bengio et al. (2013) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013.
    Estimating or propagating gradients through stochastic neurons for conditional
    computation. *arXiv preprint arXiv:1308.3432*.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等人（2013）Yoshua Bengio, Nicholas Léonard 和 Aaron Courville. 2013. 通过随机神经元估计或传播梯度以进行条件计算。*arXiv
    预印本 arXiv:1308.3432*。
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    2020. Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk 等人（2020）Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi 等人。2020.
    Piqa: 关于自然语言中的物理常识的推理。在*AAAI 人工智能会议论文集*，第 34 卷，第 7432–7439 页。'
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等人。2020. 语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901。
- en: 'Chee et al. (2023) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher
    De Sa. 2023. Quip: 2-bit quantization of large language models with guarantees.
    *arXiv preprint arXiv:2307.13304*.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chee 等人（2023）Jerry Chee, Yaohui Cai, Volodymyr Kuleshov 和 Christopher De Sa.
    2023. Quip: 2-bit 量化大规模语言模型的保证。*arXiv 预印本 arXiv:2307.13304*。'
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. 2021. [Evaluating large language models trained on code](http://arxiv.org/abs/2107.03374).
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2021）Mark Chen、Jerry Tworek、Heewoo Jun、Qiming Yuan、Henrique Ponde de Oliveira
    Pinto、Jared Kaplan、Harri Edwards、Yuri Burda、Nicholas Joseph、Greg Brockman、Alex
    Ray、Raul Puri、Gretchen Krueger、Michael Petrov、Heidy Khlaaf、Girish Sastry、Pamela
    Mishkin、Brooke Chan、Scott Gray、Nick Ryder、Mikhail Pavlov、Alethea Power、Lukasz
    Kaiser、Mohammad Bavarian、Clemens Winter、Philippe Tillet、Felipe Petroski Such、Dave
    Cummings、Matthias Plappert、Fotios Chantzis、Elizabeth Barnes、Ariel Herbert-Voss、William
    Hebgen Guss、Alex Nichol、Alex Paino、Nikolas Tezak、Tang Jie、Igor Babuschkin、Suchir
    Balaji、Shantanu Jain、William Saunders、Christopher Hesse、Andrew N. Carr、Jan Leike、Josh
    Achiam、Vedant Misra、Evan Morikawa、Alec Radford、Matthew Knight、Miles Brundage、Mira
    Murati、Katie Mayer、Peter Welinder、Bob McGrew、Dario Amodei、Sam McCandlish、Ilya
    Sutskever和Wojciech Zaremba。2021年。[Evaluating large language models trained on
    code](http://arxiv.org/abs/2107.03374)。
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    2022. Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钟等（2022）钟亨元、侯乐、Shayne Longpre、Barret Zoph、Yi Tay、William Fedus、李云轩、王学志、Mostafa
    Dehghani、Siddhartha Brahma等。2022年。Scaling instruction-finetuned language models。*arXiv
    preprint arXiv:2210.11416*。
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved
    question answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark等（2018）Peter Clark、Isaac Cowhey、Oren Etzioni、Tushar Khot、Ashish Sabharwal、Carissa
    Schoenick和Oyvind Tafjord。2018年。Think you have solved question answering? try arc,
    the ai2 reasoning challenge。*arXiv preprint arXiv:1803.05457*。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe等（2021）Karl Cobbe、Vineet Kosaraju、Mohammad Bavarian、Mark Chen、Heewoo Jun、Lukasz
    Kaiser、Matthias Plappert、Jerry Tworek、Jacob Hilton、Reiichiro Nakano等。2021年。Training
    verifiers to solve math word problems。*arXiv preprint arXiv:2110.14168*。
- en: 'Dettmers et al. (2023a) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023a. Qlora: Efficient finetuning of quantized llms. *arXiv preprint
    arXiv:2305.14314*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers等（2023a）Tim Dettmers、Artidoro Pagnoni、Ari Holtzman和Luke Zettlemoyer。2023a。Qlora:
    Efficient finetuning of quantized llms。*arXiv preprint arXiv:2305.14314*。'
- en: 'Dettmers et al. (2023b) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. 2023b. Spqr: A sparse-quantized representation for near-lossless
    llm weight compression. *arXiv preprint arXiv:2306.03078*.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers等（2023b）Tim Dettmers、Ruslan Svirschevski、Vage Egiazarian、Denis Kuznedelev、Elias
    Frantar、Saleh Ashkboos、Alexander Borzunov、Torsten Hoefler和Dan Alistarh。2023b。Spqr:
    A sparse-quantized representation for near-lossless llm weight compression。*arXiv
    preprint arXiv:2306.03078*。'
- en: 'Dettmers and Zettlemoyer (2023) Tim Dettmers and Luke Zettlemoyer. 2023. The
    case for 4-bit precision: k-bit inference scaling laws. In *International Conference
    on Machine Learning*, pages 7750–7774\. PMLR.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers和Zettlemoyer（2023）Tim Dettmers和Luke Zettlemoyer。2023年。The case for
    4-bit precision: k-bit inference scaling laws。发表于*International Conference on
    Machine Learning*，页码7750–7774。PMLR。'
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar等（2022）Elias Frantar、Saleh Ashkboos、Torsten Hoefler和Dan Alistarh。2022年。Gptq:
    Accurate post-training quantization for generative pre-trained transformers。*arXiv
    preprint arXiv:2210.17323*。'
- en: 'Geng and Liu (2023) Xinyang Geng and Hao Liu. 2023. [Openllama: An open reproduction
    of llama](https://github.com/openlm-research/open_llama).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '耿和刘（2023）耿新阳和刘浩。2023年。[Openllama: An open reproduction of llama](https://github.com/openlm-research/open_llama)。'
- en: Gholami et al. (2022) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W
    Mahoney, and Kurt Keutzer. 2022. A survey of quantization methods for efficient
    neural network inference. In *Low-Power Computer Vision*, pages 291–326\. Chapman
    and Hall/CRC.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gholami et al. (2022) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael
    W Mahoney, 和 Kurt Keutzer. 2022. 高效神经网络推理的量化方法综述。 在 *低功耗计算机视觉*, 页 291–326。Chapman
    and Hall/CRC。
- en: Gu et al. (2023) Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Knowledge
    distillation of large language models. *arXiv preprint arXiv:2306.08543*.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. (2023) Yuxian Gu, Li Dong, Furu Wei, 和 Minlie Huang. 2023. 大型语言模型的知识蒸馏。
    *arXiv 预印本 arXiv:2306.08543*。
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. *arXiv preprint arXiv:2009.03300*.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, 和 Jacob Steinhardt. 2020. 测量大规模多任务语言理解。 *arXiv 预印本
    arXiv:2009.03300*。
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, 和 Jeff Dean. 2015. 提炼神经网络中的知识。
    *arXiv 预印本 arXiv:1503.02531*。
- en: Jung et al. (2019) Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon
    Han, Youngjun Kwak, Sung Ju Hwang, and Changkyu Choi. 2019. Learning to quantize
    deep networks by optimizing quantization intervals with task loss. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    4350–4359.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jung et al. (2019) Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon
    Han, Youngjun Kwak, Sung Ju Hwang, 和 Changkyu Choi. 2019. 通过优化量化区间与任务损失来学习量化深度网络。
    在 *IEEE/CVF 计算机视觉与模式识别会议论文集*, 页 4350–4359。
- en: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    2020. Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, 和 Dario Amodei.
    2020. 神经语言模型的规模定律。 *arXiv 预印本 arXiv:2001.08361*。
- en: Kim et al. (2022) Minsoo Kim, Sihwa Lee, Sukjin Hong, Du-Seong Chang, and Jungwook
    Choi. 2022. Understanding and improving knowledge distillation for quantization-aware
    training of large transformer encoders. *arXiv preprint arXiv:2211.11014*.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2022) Minsoo Kim, Sihwa Lee, Sukjin Hong, Du-Seong Chang, 和 Jungwook
    Choi. 2022. 理解和改进大规模变换器编码器的量化感知训练中的知识蒸馏。 *arXiv 预印本 arXiv:2211.11014*。
- en: Kim et al. (2023a) Minsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong
    Chang, Wonyong Sung, and Jungwook Choi. 2023a. Token-scaled logit distillation
    for ternary weight generative language models. *arXiv preprint arXiv:2308.06744*.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2023a) Minsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong
    Chang, Wonyong Sung, 和 Jungwook Choi. 2023a. 针对三元权重生成语言模型的令牌缩放 logit 蒸馏。 *arXiv
    预印本 arXiv:2308.06744*。
- en: 'Kim et al. (2023b) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. 2023b. Squeezellm: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim et al. (2023b) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, 和 Kurt Keutzer. 2023b. Squeezellm: 密集与稀疏量化。
    *arXiv 预印本 arXiv:2306.07629*。'
- en: 'Kuzmin et al. (2022) Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel,
    Jorn Peters, and Tijmen Blankevoort. 2022. Fp8 quantization: The power of the
    exponent. *Advances in Neural Information Processing Systems*, 35:14651–14662.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kuzmin et al. (2022) Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel,
    Jorn Peters, 和 Tijmen Blankevoort. 2022. Fp8 量化: 指数的力量。 *神经信息处理系统进展*, 35:14651–14662。'
- en: Li et al. (2019) Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie Yan,
    and Rui Fan. 2019. Fully quantized network for object detection. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pages
    2810–2819.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2019) Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie Yan,
    和 Rui Fan. 2019. 完全量化网络用于目标检测。 在 *IEEE/CVF 计算机视觉与模式识别会议论文集*, 页 2810–2819。
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. 2023. Awq: Activation-aware weight quantization for llm compression
    and acceleration. *arXiv preprint arXiv:2306.00978*.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. 2023. Awq: 激活感知权重量化用于 LLM 压缩和加速。 *arXiv 预印本 arXiv:2306.00978*。'
- en: 'Liu et al. (2023a) Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong,
    and Kwang-Ting Cheng. 2023a. Llm-fp4: 4-bit floating-point quantized transformers.
    *arXiv preprint arXiv:2310.16836*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023a) Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong,
    和 Kwang-Ting Cheng. 2023a. Llm-fp4: 4 位浮点量化变换器。 *arXiv 预印本 arXiv:2310.16836*。'
- en: 'Liu et al. (2023b) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    2023b. Llm-qat: Data-free quantization aware training for large language models.
    *arXiv preprint arXiv:2305.17888*.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023b) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra.
    2023b. Llm-qat: 数据无关的量化感知训练用于大型语言模型. *arXiv 预印本 arXiv:2305.17888*。'
- en: Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled
    weight decay regularization. *arXiv preprint arXiv:1711.05101*.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov 和 Hutter (2017) Ilya Loshchilov 和 Frank Hutter. 2017. 解耦权重衰减正则化.
    *arXiv 预印本 arXiv:1711.05101*。
- en: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering
    code large language models with evol-instruct.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, 和 Daxin Jiang. 2023. Wizardcoder: 赋能代码大型语言模型的
    evol-instruct。'
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. 2016. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher. 2016. 指针哨兵混合模型. *arXiv 预印本 arXiv:1609.07843*。
- en: Narayan et al. (2018) Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018.
    [Don’t give me the details, just the summary! topic-aware convolutional neural
    networks for extreme summarization](https://doi.org/10.18653/v1/D18-1206). In
    *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*,
    pages 1797–1807, Brussels, Belgium. Association for Computational Linguistics.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayan et al. (2018) Shashi Narayan, Shay B. Cohen, 和 Mirella Lapata. 2018.
    [不要给我细节，只要总结！主题感知卷积神经网络用于极端摘要](https://doi.org/10.18653/v1/D18-1206). 发表在 *2018
    年自然语言处理实证方法会议论文集*，第1797–1807页，比利时布鲁塞尔。计算语言学协会。
- en: 'Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
    Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning
    models with over 100 billion parameters. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*, pages 3505–3506.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, 和 Yuxiong
    He. 2020. Deepspeed: 系统优化使得训练超过 1000 亿参数的深度学习模型成为可能. 发表在 *第26届 ACM SIGKDD 知识发现与数据挖掘国际会议论文集*，第3505–3506页。'
- en: 'Rosh (2023) Nick Rosh. 2023. Evol-teacher: Recreating wizardcoder. [https://github.com/nickrosh/evol-teacher](https://github.com/nickrosh/evol-teacher).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rosh (2023) Nick Rosh. 2023. Evol-teacher: 重现 wizardcoder. [https://github.com/nickrosh/evol-teacher](https://github.com/nickrosh/evol-teacher)。'
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at
    scale. *Communications of the ACM*, 64(9):99–106.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    和 Yejin Choi. 2021. Winogrande: 大规模对抗 Winograd 语法挑战. *ACM 通讯*，64(9):99–106。'
- en: Sakr et al. (2022) Charbel Sakr, Steve Dai, Rangha Venkatesan, Brian Zimmer,
    William Dally, and Brucek Khailany. 2022. Optimal clipping and magnitude-aware
    differentiation for improved quantization-aware training. In *International Conference
    on Machine Learning*, pages 19123–19138\. PMLR.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakr et al. (2022) Charbel Sakr, Steve Dai, Rangha Venkatesan, Brian Zimmer,
    William Dally, 和 Brucek Khailany. 2022. 为改进量化感知训练的最佳裁剪和幅度感知微分. 发表在 *国际机器学习会议*，第19123–19138页。PMLR。
- en: 'Shang et al. (2023) Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. 2023.
    Pb-llm: Partially binarized large language models. *arXiv preprint arXiv:2310.00034*.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shang et al. (2023) Yuzhang Shang, Zhihang Yuan, Qiang Wu, 和 Zhen Dong. 2023.
    Pb-llm: 部分二值化的大型语言模型. *arXiv 预印本 arXiv:2310.00034*。'
- en: 'Shao et al. (2023) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 2023. Omniquant:
    Omnidirectionally calibrated quantization for large language models. *arXiv preprint
    arXiv:2308.13137*.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shao et al. (2023) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, 和 Ping Luo. 2023. Omniquant:
    针对大型语言模型的全方向校准量化. *arXiv 预印本 arXiv:2308.13137*。'
- en: 'Shen et al. (2020) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based
    ultra low precision quantization of bert. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, volume 34, pages 8815–8821.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen et al. (2020) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W Mahoney, 和 Kurt Keutzer. 2020. Q-bert: 基于 Hessian 的极低精度量化
    BERT. 发表在 *AAAI 人工智能会议论文集*，第34卷，第8815–8821页。'
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。'
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv 预印本 arXiv:2307.09288*。'
- en: 'Wang et al. (2023) Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie
    Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. 2023. Bitnet:
    Scaling 1-bit transformers for large language models. *arXiv preprint arXiv:2310.11453*.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023) Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie
    Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, 和 Furu Wei. 2023. Bitnet: Scaling
    1-bit transformers for large language models. *arXiv 预印本 arXiv:2310.11453*。'
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander Rush. 2020. [Transformers: State-of-the-art natural language processing](https://doi.org/10.18653/v1/2020.emnlp-demos.6).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations*, pages 38–45, Online. Association for Computational
    Linguistics.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    和 Alexander Rush. 2020. [Transformers: State-of-the-art natural language processing](https://doi.org/10.18653/v1/2020.emnlp-demos.6).
    在 *2020年自然语言处理方法会议论文集: 系统展示* 中，页38–45，在线。计算语言学协会。'
- en: 'Yu et al. (2023) Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying
    Liu, Yu Zhang, JamesT. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023.
    Metamath: Bootstrap your own mathematical questions for large language models.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu et al. (2023) Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying
    Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, 和 Weiyang Liu. 2023.
    Metamath: Bootstrap your own mathematical questions for large language models。'
- en: Yuan et al. (2023) Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi
    Tan, and Chang Zhou. 2023. Scaling relationship on learning mathematical reasoning
    with large language models. *arXiv preprint arXiv:2308.01825*.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2023) Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi
    Tan, 和 Chang Zhou. 2023. Scaling relationship on learning mathematical reasoning
    with large language models. *arXiv 预印本 arXiv:2308.01825*。
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? *arXiv
    preprint arXiv:1905.07830*.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    和 Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? *arXiv
    预印本 arXiv:1905.07830*。'
- en: 'Zhang et al. (2020) Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,
    Xin Jiang, and Qun Liu. 2020. Ternarybert: Distillation-aware ultra-low bit bert.
    *arXiv preprint arXiv:2009.12812*.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2020) Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,
    Xin Jiang, 和 Qun Liu. 2020. Ternarybert: Distillation-aware ultra-low bit bert.
    *arXiv 预印本 arXiv:2009.12812*。'
- en: 'Zhang et al. (2023a) Yijia Zhang, Sicheng Zhang, Shijie Cao, Dayou Du, Jianyu
    Wei, Ting Cao, and Ningyi Xu. 2023a. [Afpq: Asymmetric floating point quantization
    for llms](http://arxiv.org/abs/2311.01792).'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023a) Yijia Zhang, Sicheng Zhang, Shijie Cao, Dayou Du, Jianyu
    Wei, Ting Cao, 和 Ningyi Xu. 2023a. [Afpq: Asymmetric floating point quantization
    for llms](http://arxiv.org/abs/2311.01792)。'
- en: Zhang et al. (2023b) Yijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting
    Cao, Fan Yang, Mao Yang, Shanghang Zhang, and Ningyi Xu. 2023b. Integer or floating
    point? new outlooks for low-bit quantization on large language models. *arXiv
    preprint arXiv:2305.12356*.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023b) Yijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting
    Cao, Fan Yang, Mao Yang, Shanghang Zhang, 和 Ningyi Xu. 2023b. Integer or floating
    point? new outlooks for low-bit quantization on large language models. *arXiv
    预印本 arXiv:2305.12356*。
- en: 'Zhou et al. (2023) Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna
    Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, and Rishabh Agarwal.
    2023. [Distillspec: Improving speculative decoding via knowledge distillation](http://arxiv.org/abs/2310.08461).'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2023) Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna
    Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, 和 Rishabh Agarwal.
    2023. [Distillspec: 通过知识蒸馏改进推测解码](http://arxiv.org/abs/2310.08461)。'
- en: Zhu et al. (2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023.
    A survey on model compression for large language models. *arXiv preprint arXiv:2308.07633*.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, 和 Weiping Wang. 2023.
    关于大型语言模型的模型压缩调查。*arXiv 预印本 arXiv:2308.07633*。
- en: Appendix A Appendix
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Details of PTQ and QAT Configuration
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 PTQ 和 QAT 配置的详细信息
- en: '![Refer to caption](img/1c6ce01f964e49e43f9daebd164c770d.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1c6ce01f964e49e43f9daebd164c770d.png)'
- en: 'Figure 8: Comparative Evaluation of PTQ Methods Using Various Calibration Datasets'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：使用各种校准数据集的 PTQ 方法的比较评估
- en: 'We evaluate PTQ methods by examining the impact of different calibration dataset
    distributions. Illustrated in Figure [8](#A1.F8 "Figure 8 ‣ A.1 Details of PTQ
    and QAT Configuration ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation"), calibrating with domain-specific data
    significantly enhances task-specific performance. For a fair comparison, all PTQ
    methods utilize the default calibration datasets for general language tasks and
    domain-specific calibration datasets Rosh ([2023](#bib.bib34)); Yu et al. ([2023](#bib.bib44))
    for reasoning tasks.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过检查不同校准数据集分布的影响来评估 PTQ 方法。如图[8](#A1.F8 "Figure 8 ‣ A.1 Details of PTQ and
    QAT Configuration ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation")所示，使用领域特定数据进行校准显著提升了任务特定的性能。为了进行公平比较，所有
    PTQ 方法都使用了默认的通用语言任务校准数据集和领域特定校准数据集 Rosh ([2023](#bib.bib34))；Yu et al. ([2023](#bib.bib44))
    用于推理任务。'
- en: Regarding QAT methods, it should be noted that the use of symmetric quantization
    in LLM-QAT results in degradation when grouped quantization is applied. To ensure
    a fair comparison, we replicate the approach with our setup and employ asymmetric
    uniform quantization.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 QAT 方法，需要注意的是，当应用分组量化时，LLM-QAT 中使用对称量化会导致性能下降。为了确保公平比较，我们使用我们的设置重复该方法，并采用非对称均匀量化。
- en: A.2 Implementation Details and Analysis of Confidence-Aware KLD
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 置信度感知 KLD 的实施细节和分析
- en: '![Refer to caption](img/0e4e27a88e81ac19cfe70a0b07f1bfe5.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0e4e27a88e81ac19cfe70a0b07f1bfe5.png)'
- en: (a) Text Generation Task
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 文本生成任务
- en: '![Refer to caption](img/afdd956b78fde7f3585d939656b3f85d.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/afdd956b78fde7f3585d939656b3f85d.png)'
- en: (b) Reasoning Task
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 推理任务
- en: 'Figure 9: Per-token confidence scores when teacher model (full-precision) conducting
    text generation task and reasoning task.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：教师模型（全精度）进行文本生成任务和推理任务时的每个 token 的置信度分数。
- en: 'We use a straightforward method in the pre-calculation of the coefficient $\gamma$.
    We utilize ten batches of training data to perform forward passes without updating
    parameters. Subsequently, we obtain the logits from the teacher model to compute
    the average token probability. In Figure [9](#A1.F9 "Figure 9 ‣ A.2 Implementation
    Details and Analysis of Confidence-Aware KLD ‣ Appendix A Appendix ‣ BitDistiller:
    Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation"), we have conducted
    analysis by examining the confidence scores of the teacher model in various tasks
    during next-word prediction. This analysis reveals that confidence levels can
    vary in text generation tasks, in contrast to reasoning tasks where each step
    is critical. Notably, in text generation tasks using LLMs, relying solely on the
    highest conditional probability through Greedy Search may result in local optima,
    overlooking more optimal sequences. These observations advocate for a mean-seeking
    Kullback-Leibler (KL) approach, encouraging the student model to encompass all
    potential modes of the teacher, thereby more effectively capturing the teacher’s
    general generative capabilities. In reasoning tasks, where the teacher model shows
    high confidence in next-word predictions, the student model should concentrate
    on learning the predominant mode from the teacher. Our proposed method, CAKLD,
    is designed to balance these two distinct modes effectively.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在系数$\gamma$的预计算中使用了一种直接的方法。我们利用十批训练数据进行前向传播，但不更新参数。随后，我们从教师模型中获得logits以计算平均token概率。在图[9](#A1.F9
    "图 9 ‣ A.2 实现细节和自信感知KLD分析 ‣ 附录A ‣ BitDistiller: 通过自蒸馏释放亚4位LLMs的潜力")中，我们通过检查教师模型在不同任务中的自信分数进行分析。这项分析表明，文本生成任务中的自信水平可能会有所不同，而推理任务中每一步都至关重要。值得注意的是，在使用LLMs进行文本生成任务时，仅依靠Greedy
    Search中的最高条件概率可能会导致局部最优，忽视更优的序列。这些观察结果支持采用均值寻求的Kullback-Leibler（KL）方法，鼓励学生模型涵盖教师模型的所有潜在模式，从而更有效地捕捉教师的总体生成能力。在推理任务中，教师模型在下一个词预测中表现出高自信度时，学生模型应集中学习教师的主要模式。我们提出的方法CAKLD旨在有效平衡这两种不同的模式。'
- en: A.3 Training Datasets Examples
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 训练数据集示例
- en: 'For general language tasks, we mix token sequences from Alpaca and WikiText-2
    datasets with a ratio of 2:1\. Since WikiText-2 lacks explicit instructions, we
    utilize the first 128 tokens from the corpus as the input prompt for the teacher
    model’s generation process, setting the temperature to 0.7\. For tasks related
    to code understanding and generation, we employ the Evol-Instruct-Code dataset.
    For mathematical reasoning, we utilize MetaMathQA. Examples of the training data
    utilized are shown in Table [8](#A1.T8 "Table 8 ‣ A.5 Integration with AWQ For
    Quantization Strategies ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential
    of Sub-4-Bit LLMs via Self-Distillation").'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '对于一般语言任务，我们将来自Alpaca和WikiText-2数据集的token序列按2:1的比例混合。由于WikiText-2缺乏明确的指令，我们使用语料库中的前128个tokens作为教师模型生成过程的输入提示，将温度设置为0.7。对于与代码理解和生成相关的任务，我们使用Evol-Instruct-Code数据集。对于数学推理，我们使用MetaMathQA。所使用的训练数据示例见表[8](#A1.T8
    "表 8 ‣ A.5 与AWQ的量化策略集成 ‣ 附录A ‣ BitDistiller: 通过自蒸馏释放亚4位LLMs的潜力")。'
- en: It is essential to highlight that our self-distillation process utilizes only
    a small portion of the involved datasets.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，我们的自蒸馏过程仅使用了涉及数据集的一小部分。
- en: A.4 Evaluation of General Language Tasks on LLaMA-2-13B
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 对LLaMA-2-13B的一般语言任务的评估
- en: 'Additional results of the General Language Tasks for LLaMA-2-13B are shown
    in Table [9](#A1.T9 "Table 9 ‣ A.5 Integration with AWQ For Quantization Strategies
    ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs
    via Self-Distillation").'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '表[9](#A1.T9 "表 9 ‣ A.5 与AWQ的量化策略集成 ‣ 附录A ‣ BitDistiller: 通过自蒸馏释放亚4位LLMs的潜力")中显示了LLaMA-2-13B的一般语言任务的附加结果。'
- en: A.5 Integration with AWQ For Quantization Strategies
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 与AWQ的量化策略集成
- en: '| LLaMA-2-7B | PPL $\downarrow$ |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | PPL $\downarrow$ |'
- en: '| (start $\mapsto$ end ) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| (start $\mapsto$ end ) |'
- en: '|  | INT-Asym | 6.65 $\mapsto$ 6.15 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | INT-Asym | 6.65 $\mapsto$ 6.15 |'
- en: '| 3 Bits | AWQ | 6.48 $\mapsto$ 6.09 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 3 Bits | AWQ | 6.48 $\mapsto$ 6.09 |'
- en: '| g128 | Clip-Asym | 6.21 $\mapsto$ 6.00 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| g128 | Clip-Asym | 6.21 $\mapsto$ 6.00 |'
- en: '|  | AWQ + Clip-Asym | 6.18 $\mapsto$ 6.00 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | AWQ + Clip-Asym | 6.18 $\mapsto$ 6.00 |'
- en: '|  | INT-Asym | 3.4e2 $\mapsto$ 16.94 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | INT-Asym | 3.4e2 $\mapsto$ 16.94 |'
- en: '| 2 Bits | AWQ | 2.2e5 $\mapsto$ Inf |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 2 Bits | AWQ | 2.2e5 $\mapsto$ Inf |'
- en: '| g128 | Clip-Asym | 17.98 $\mapsto$ 8.08 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| g128 | Clip-Asym | 17.98 $\mapsto$ 8.08 |'
- en: '|  | AWQ + Clip-Asym | 16.61 $\mapsto$ 8.13 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | AWQ + Clip-Asym | 16.61 $\mapsto$ 8.13 |'
- en: 'Table 7: Results of quantization initialization for QAT combining with AWQ
    on PPL of WikiText-2.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：QAT与AWQ结合在WikiText-2 PPL上的量化初始化结果。
- en: 'As shown in Table [7](#A1.T7 "Table 7 ‣ A.5 Integration with AWQ For Quantization
    Strategies ‣ Appendix A Appendix ‣ BitDistiller: Unleashing the Potential of Sub-4-Bit
    LLMs via Self-Distillation"), we explore the efficacy of combining asymmetric
    clipping with AWQ during the self-distillation process. Our results indicate that
    asymmetric clipping significantly enhances robustness in sub-4-bit quantization
    scenarios. For instance, at the 2-bit quantization level, both INT-Asym and AWQ
    methods are unable to complete the task. Conversely, Clip-Asym not only succeeds
    but also achieves a marked improvement in perplexity. It is also noteworthy that
    while integrating AWQ prior to QAT yields improvements initially, there is no
    additional performance gain after training. This suggests that a straightforward
    clipping approach is sufficiently effective for initializing QAT.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[7](#A1.T7 "表 7 ‣ A.5 与AWQ集成的量化策略 ‣ 附录A 附录 ‣ BitDistiller：通过自我蒸馏释放亚4位LLMs的潜力")所示，我们探索了在自我蒸馏过程中将非对称剪枝与AWQ结合的效果。我们的结果表明，非对称剪枝显著提高了亚4位量化场景中的鲁棒性。例如，在2位量化水平下，INT-Asym和AWQ方法都无法完成任务。相反，Clip-Asym不仅成功完成任务，还显著改善了困惑度。值得注意的是，虽然在QAT之前集成AWQ最初会有所改善，但训练后没有额外的性能提升。这表明，简单的剪枝方法对于初始化QAT已经足够有效。
- en: '| Tasks | Resources | Number | Instruction | Teacher Response |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 资源 | 数量 | 指示 | 教师回应 |'
- en: '| General Language Tasks | Alpaca | 4K | Find the five steps to meditate |
    1\. Find a quiet place to meditate$\backslash$n5\. When thoughts invade your mind,
    gently bring your attention back to your breath |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 一般语言任务 | Alpaca | 4K | 找到冥想的五个步骤 | 1\. 找一个安静的地方进行冥想$\backslash$n5\. 当思想侵入你的脑海时，轻柔地将注意力重新集中在你的呼吸上
    |'
- en: '| General Language Tasks | WikiText | 2K | In December 1856 , Rice brought
    forward two bills in Congress : an enabling act that would allow Minnesota to
    form a state cons | itution, and a bill that would allow Missouri to secede from
    the Union. The bills were defeated, and Rice was removed from the chairmanship
    of the House Judiciary Committee. …… |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 一般语言任务 | WikiText | 2K | 1856年12月，赖斯在国会提出了两项法案：一项使明尼苏达州能够制定州宪法的授权法案，以及一项允许密苏里州脱离联邦的法案。这些法案被否决，赖斯被免去了众议院司法委员会主席职务。……
    |'
- en: '| Code-related Tasks | Evol-Instruct-Code | 2K | Create a nested loop to print
    every combination of numbers between 0-9, excluding any combination that contains
    the number 5\. Additionally, exclude any combination that contains a repeating
    digit. Implement the solution without using any built-in functions or libraries
    to check for repeating digits. | Here’s one way to implement the solution:$\textbackslash$n
    for i in range(10):…… |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 代码相关任务 | Evol-Instruct-Code | 2K | 创建一个嵌套循环，打印0-9之间的所有组合，排除任何包含数字5的组合。此外，排除任何包含重复数字的组合。实现方案时不要使用任何内置函数或库来检查重复数字。
    | 这是实现方案的一种方法：$\textbackslash$n for i in range(10):…… |'
- en: '| Math-related Tasks | MetaMathQA | 2K | A store is comparing their profits
    throughout the year. They had profits of $1,500 in the first quarter of the year,
    $3,000 in the third quarter, and $2,000 in the fourth quarter. If their annual
    profits are $8,000, how much profit, in dollars, did they make in the second quarter?
    | $\backslash$nTo find x, we need to isolate it on one side of the equation…….
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 数学相关任务 | MetaMathQA | 2K | 一家商店在全年比较其利润。他们在第一季度的利润为$1,500，在第三季度的利润为$3,000，在第四季度的利润为$2,000。如果他们的年利润为$8,000，那么第二季度的利润是多少美元？
    | $\backslash$n要找出x，我们需要将其在方程的一边孤立出来……. |'
- en: 'Table 8: The Training Dataset examples for different tasks.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：不同任务的训练数据集示例。
- en: '| LLaMA-2-13B | PPL $\downarrow$ | MMLU (5s) | PIQA | Hella. | Wino. | ARC-c
    | Avg |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | PPL $\downarrow$ | MMLU (5s) | PIQA | Hella. | Wino. | ARC-c
    | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| BF16 | 4.88 | 55.54 | 79.16 | 60.13 | 72.14 | 48.12 | 63.02 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| BF16 | 4.88 | 55.54 | 79.16 | 60.13 | 72.14 | 48.12 | 63.02 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | RTN | 5.52 | 50.74 | 78.35 | 57.75 | 71.11 | 43.86 | 60.36 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN | 5.52 | 50.74 | 78.35 | 57.75 | 71.11 | 43.86 | 60.36 |'
- en: '| 3 Bits | GPTQ | 5.41 | 50.63 | 77.26 | 56.84 | 70.72 | 42.83 | 59.66 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 3 位 | GPTQ | 5.41 | 50.63 | 77.26 | 56.84 | 70.72 | 42.83 | 59.66 |'
- en: '| g128 | AWQ | 5.47 | 49.64 | 77.09 | 57.52 | 70.32 | 43.86 | 59.69 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| g128 | AWQ | 5.47 | 49.64 | 77.09 | 57.52 | 70.32 | 43.86 | 59.69 |'
- en: '| OmniQuant | 5.48 | 48.97 | 77.64 | 57.08 | 70.88 | 44.28 | 59.77 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 5.48 | 48.97 | 77.64 | 57.08 | 70.88 | 44.28 | 59.77 |'
- en: '|  | LLM-QAT | 5.32 | 51.60 | 78.29 | 58.45 | 70.56 | 44.62 | 60.70 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-QAT | 5.32 | 51.60 | 78.29 | 58.45 | 70.56 | 44.62 | 60.70 |'
- en: '|  | BitDistiller (ours) | 5.20 | 53.21 | 78.67 | 58.66 | 71.59 | 46.67 | 61.76
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  | BitDistiller（我们的） | 5.20 | 53.21 | 78.67 | 58.66 | 71.59 | 46.67 | 61.76
    |'
- en: '|  | RTN | 109.21 | 24.74 | 57.56 | 32.56 | 50.75 | 21.84 | 37.49 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN | 109.21 | 24.74 | 57.56 | 32.56 | 50.75 | 21.84 | 37.49 |'
- en: '|  | GPTQ | 15.08 | 23.70 | 56.04 | 30.99 | 51.22 | 19.28 | 36.25 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 15.08 | 23.70 | 56.04 | 30.99 | 51.22 | 19.28 | 36.25 |'
- en: '| 2 Bits | AWQ | 1.2e5 | 27.04 | 53.16 | 25.82 | 51.70 | 23.04 | 36.15 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 2位 | AWQ | 1.2e5 | 27.04 | 53.16 | 25.82 | 51.70 | 23.04 | 36.15 |'
- en: '| g128 | OmniQuant | 25.69 | 26.09 | 61.81 | 31.92 | 51.38 | 22.27 | 38.69
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| g128 | OmniQuant | 25.69 | 26.09 | 61.81 | 31.92 | 51.38 | 22.27 | 38.69
    |'
- en: '|  | LLM-QAT | 7.80 | 29.37 | 74.10 | 49.49 | 63.14 | 33.87 | 49.99 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-QAT | 7.80 | 29.37 | 74.10 | 49.49 | 63.14 | 33.87 | 49.99 |'
- en: '|  | BitDistiller (ours) | 6.78 | 37.50 | 75.84 | 51.30 | 65.90 | 37.46 | 53.60
    |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | BitDistiller（我们的） | 6.78 | 37.50 | 75.84 | 51.30 | 65.90 | 37.46 | 53.60
    |'
- en: 'Table 9: General language task results of BitDistiller versus established PTQ
    and QAT Methods on LLaMA-2-13B Model. Our method achieves leading performance
    in both 3-bit and 2-bit quantization.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: BitDistiller 与已建立的 PTQ 和 QAT 方法在 LLaMA-2-13B 模型上的一般语言任务结果。我们的方法在 3 位和
    2 位量化中均取得了领先的表现。'
