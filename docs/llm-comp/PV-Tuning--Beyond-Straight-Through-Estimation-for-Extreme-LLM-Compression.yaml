- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:52:49'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:52:49
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PV-Tuning：超越直通估计的极端 LLM 压缩
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14852](https://ar5iv.labs.arxiv.org/html/2405.14852)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14852](https://ar5iv.labs.arxiv.org/html/2405.14852)
- en: Vladimir Malinovskii^†
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 弗拉基米尔·马林诺夫斯基^†
- en: Yandex, HSE University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Yandex，HSE University
- en: '&Denis Mazur^†'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '&德尼斯·马祖尔^†'
- en: MIPT^⋄, Researchcore
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: MIPT^⋄，Researchcore
- en: '&Ivan Ilin^†'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&伊万·伊林^†'
- en: ​​AI Initiative, KAUST^∗​​
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ​​AI Initiative，KAUST^∗​​
- en: '&Denis Kuznedelev'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&德尼斯·库兹涅代列夫'
- en: Yandex, Skoltech
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Yandex，Skoltech
- en: '&Konstantin Burlachenko'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '&康斯坦丁·布拉赫琴科'
- en: ​​AI Initiative, KAUST^∗​​
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ​​AI Initiative，KAUST^∗​​
- en: '&Kai Yi'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '&凯·易'
- en: ​​AI Initiative, KAUST^∗​​
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ​​AI Initiative，KAUST^∗​​
- en: '&Dan Alistarh^‡'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '&丹·阿利斯塔赫^‡'
- en: IST Austria, NeuralMagic
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: IST Austria，NeuralMagic
- en: '&Peter Richtarik^‡'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '&彼得·里赫塔里克^‡'
- en: ​​AI Initiative, KAUST^∗​​
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ​​AI Initiative，KAUST^∗​​
- en: Abstract
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: There has been significant interest in “extreme” compression of large language
    models (LLMs), i.e., to 1-2 bits per parameter, which allows such models to be
    executed efficiently on resource-constrained devices. Existing work focused on
    improved one-shot quantization techniques and weight representations; yet, purely
    post-training approaches are reaching diminishing returns in terms of the accuracy-vs-bit-width
    trade-off. State-of-the-art quantization methods such as QuIP# and AQLM include
    fine-tuning (part of) the compressed parameters over a limited amount of calibration
    data; however, such fine-tuning techniques over compressed weights often make
    exclusive use of *straight-through estimators (STE)*, whose performance is not
    well-understood in this setting. In this work, we question the use of STE for
    extreme LLM compression, showing that it can be sub-optimal, and perform a systematic
    study of quantization-aware fine-tuning strategies for LLMs. We propose PV-Tuning
    — a representation-agnostic framework that generalizes and improves upon existing
    fine-tuning strategies, and provides convergence guarantees in restricted cases.
    On the practical side, when used for 1-2 bit vector quantization, PV-Tuning outperforms
    prior techniques for highly-performant models such as Llama and Mistral. Using
    PV-Tuning, we achieve the first Pareto-optimal quantization for Llama-2 family
    models at 2 bits per parameter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对“大型语言模型（LLMs）”的“极端”压缩，即每个参数1-2位，比重非常大，这种压缩使得这些模型能够在资源受限的设备上高效执行，产生了极大的兴趣。现有工作集中在改进一次性量化技术和权重表示上；然而，纯粹的后训练方法在准确性与位宽权衡方面逐渐显现出回报递减的趋势。最先进的量化方法如
    QuIP# 和 AQLM 包括在有限的校准数据上微调（部分）压缩参数；然而，这些微调技术通常完全依赖于*直通估计器（STE）*，在这种环境下，其性能并不被很好地理解。在这项工作中，我们质疑
    STE 在极端 LLM 压缩中的使用，表明其可能是次优的，并对 LLM 的量化感知微调策略进行系统研究。我们提出了 PV-Tuning——一个与表示无关的框架，它在现有微调策略的基础上进行了概括和改进，并在受限情况下提供了收敛保证。在实际应用方面，当用于
    1-2 位向量量化时，PV-Tuning 在高性能模型如 Llama 和 Mistral 上超越了先前的技术。使用 PV-Tuning，我们实现了 Llama-2
    系列模型在每个参数 2 位的首个 Pareto-optimal 量化。
- en: '^($\dagger$)^($\dagger$)footnotetext: Equal contribution.  $\ddagger$ Moscow
    Institute of Physics and Technology, Russia^*^*footnotetext: King Abdullah University
    of Science and Technology, Saudi Arabia'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\dagger$)^($\dagger$)脚注：平等贡献。 $\ddagger$ 俄罗斯莫斯科物理技术学院^*^*脚注：沙特阿拉伯阿卜杜拉国王科技大学
- en: 1 Introduction
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent years have seen the development of ever more capable large language models,
    attracting immense interest from both researchers and industry. One of the driving
    factors behind progress in this area is the availability of powerful open LLMs
    such as Llama [[63](#bib.bib63)], Mistral [[31](#bib.bib31), [32](#bib.bib32)],
    or Phi [[38](#bib.bib38)]. The main advantage of open LLMs is that they can be
    run and fine-tuned locally by end users; however, as state-of-the-art LLMs grow
    larger, they also become harder to run on commodity hardware. For instance, in
    order to fit the best available Llama-3 model on a consumer GPU, the model would
    have to be compressed to below 2 bits per parameter¹¹1At the time of writing,
    the best open model (Llama-3 70B) takes up 130GB in FP16, while most consumer
    GPUs have 8-24GiB DRAM, some of which must be reserved for the attention cache..
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，越来越多功能强大的大型语言模型得到了发展，吸引了研究人员和工业界的极大关注。这一领域进展的推动因素之一是强大的开放 LLM 的出现，如 Llama [[63](#bib.bib63)]、Mistral [[31](#bib.bib31),
    [32](#bib.bib32)]，或 Phi [[38](#bib.bib38)]。开放 LLM 的主要优势在于用户可以在本地运行和微调；然而，随着最先进
    LLM 的规模不断扩大，它们也变得越来越难以在普通硬件上运行。例如，为了将最佳的 Llama-3 模型适配到消费级 GPU 上，该模型必须压缩到每个参数低于
    2 位¹¹1撰写时，最佳的开放模型（Llama-3 70B）在 FP16 下占用 130GB 的内存，而大多数消费级 GPU 拥有 8-24GiB 的 DRAM，其中一些必须保留用于注意力缓存。。
- en: 'To achieve such “extreme” degrees of compression accurately, researchers have
    proposed a variety of techniques, which can be roughly categorized into i) better
    quantized weight representations and ii) better algorithms to learn these representations.
    The weight representations used for extreme quantization include group quantization [[20](#bib.bib20),
    [18](#bib.bib18)], sparse high-precision outliers [[15](#bib.bib15), [29](#bib.bib29)],
    incoherence processing of the weights [[8](#bib.bib8), [64](#bib.bib64)], or additive
    and residual quantization [[19](#bib.bib19), [66](#bib.bib66)]. In turn, the calibration
    algorithms also vary between data-free methods [[18](#bib.bib18)], layer-wise
    calibration [[20](#bib.bib20), [16](#bib.bib16)], block-wise or global fine-tuning [[19](#bib.bib19),
    [65](#bib.bib65)] or even quantization-aware training [[72](#bib.bib72), [69](#bib.bib69)].
    However, the weight representation and the fine-tuning algorithm are largely orthogonal:
    most popular quantized representations could be obtained layer-wise in one-shot,
    fine-tuned layer-wise to a variety of optimization objectives, or even trained
    entirely from scratch.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确实现如此“极端”的压缩程度，研究人员提出了各种技术，这些技术大致可以分为 i) 更好的量化权重表示和 ii) 更好的算法来学习这些表示。用于极端量化的权重表示包括组量化 [[20](#bib.bib20),
    [18](#bib.bib18)]、稀疏高精度异常值 [[15](#bib.bib15), [29](#bib.bib29)]、权重的非一致性处理 [[8](#bib.bib8),
    [64](#bib.bib64)]，或加法和残差量化 [[19](#bib.bib19), [66](#bib.bib66)]。相应地，校准算法也有所不同，包括无数据方法 [[18](#bib.bib18)]、逐层校准 [[20](#bib.bib20),
    [16](#bib.bib16)]、块级或全局微调 [[19](#bib.bib19), [65](#bib.bib65)]，甚至量化感知训练 [[72](#bib.bib72),
    [69](#bib.bib69)]。然而，权重表示和微调算法在很大程度上是正交的：大多数流行的量化表示可以通过逐层一次性获得，逐层微调以适应各种优化目标，甚至可以从头开始训练。
- en: '![Refer to caption](img/69eb4804436f0917cfa08d545a1f9b1b.png)![Refer to caption](img/1de3e9479540f40a459a704b1e8572b7.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/69eb4804436f0917cfa08d545a1f9b1b.png)![参见说明](img/1de3e9479540f40a459a704b1e8572b7.png)'
- en: 'Figure 1: WikiText-2 perplexity (left) and average zero-shot accuracy (right)
    of 2-bit quantized Llama 2 models as a function of model size (GiB). See detailed
    setup in Section [4.3](#S4.SS3 "4.3 Large-scale Evaluation & Discussion ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：WikiText-2 困惑度（左）和 2 位量化 Llama 2 模型的平均零-shot 准确率（右）随模型大小（GiB）的变化。详细设置请参见第 [4.3](#S4.SS3
    "4.3 大规模评估与讨论 ‣ 4 实验 ‣ PV-Tuning：超越直通估计实现极端 LLM 压缩") 节。
- en: 'Surprisingly, there is a clear disparity between the degree of interest shown
    to accurate one-shot quantization versus accurate fine-tuning. Specifically, one-shot
    quantization is very well-studied, to the extent that, as shown in Figure [2](#S4.F2
    "Figure 2 ‣ 4.1 Evaluating quantized representations with finetuning ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"),
    improvements in this direction are clearly saturating. At the same time, the impact
    of fine-tuning strategy is largely unknown: while many recent works use some form
    of fine-tuning [[58](#bib.bib58), [19](#bib.bib19), [65](#bib.bib65)], they typically
    consider a single fine-tuning regimen based on straight-through estimation (STE) [[5](#bib.bib5),
    [13](#bib.bib13)]. Thus, given the multitude of representations considered, it
    is not at all clear whether current fine-tuning strategies are optimal.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '令人惊讶的是，对准确的一次性量化和准确的微调的关注程度存在明显差异。具体来说，一次性量化得到了非常深入的研究，以至于，如图[2](#S4.F2 "Figure
    2 ‣ 4.1 Evaluating quantized representations with finetuning ‣ 4 Experiments ‣
    PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")所示，这方面的改进显然已接近饱和。与此同时，微调策略的影响仍然
    largely unknown：虽然许多近期工作使用了某种形式的微调[[58](#bib.bib58), [19](#bib.bib19), [65](#bib.bib65)]，但它们通常考虑的是基于
    straight-through estimation (STE) 的单一微调方案[[5](#bib.bib5), [13](#bib.bib13)]。因此，考虑到众多的表示方式，目前的微调策略是否最佳仍然不清楚。'
- en: 'In this work, we analyze the problem of fine-tuning over highly-compressed
    weights from the optimization perspective. We begin by analyzing popular fine-tuning
    strategies for extreme LLM quantization. The key challenge in this context is
    that the quantized representations may contain both continuous and discrete variables:
    while continuous parameters, such as learnable scales or codebooks, can be optimized
    by backpropagation, the discrete parameters (e.g., integer assignments for the
    weights) cannot. Existing fine-tuning techniques either do not optimize over discrete
    parameters at all [[65](#bib.bib65), [19](#bib.bib19)] or fine-tune them using
    heuristics such as STE or stochastic rounding [[2](#bib.bib2)]. Unfortunately,
    these methods are not well-justified for weight quantization from the point of
    view of optimization theory, and, as we show in Section [3](#S3 "3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression"), can provide poor practical performance.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们从优化的角度分析了对高度压缩权重进行微调的问题。我们首先分析了极端 LLM 量化的流行微调策略。在这种情况下，主要的挑战是量化表示可能包含连续变量和离散变量：虽然连续参数，例如可学习的尺度或码本，可以通过反向传播进行优化，但离散参数（例如，权重的整数分配）则不能。现有的微调技术要么完全不优化离散参数[[65](#bib.bib65),
    [19](#bib.bib19)]，要么使用 STE 或随机舍入等启发式方法来微调它们[[2](#bib.bib2)]。不幸的是，从优化理论的角度来看，这些方法在权重量化方面并没有得到很好的证明，正如我们在第[3](#S3
    "3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")节中展示的那样，它们可能会提供较差的实际性能。'
- en: 'We propose an alternative solution: instead of following heuristic gradient
    estimates, our approach follows the actual gradient of the objective in a small
    subspace of optimized parameters where it can be meaningfully improved. Following
    this insight, we formulate the *PV-tuning framework* for fine-tuning arbitrary
    quantized representations. We update both discrete and continuous components to
    minimize a global objective function, such as the KL divergence relative to the
    original model predictions. Our results show that this strategy leads to significant
    improvements across weight representations, achieving new state-of-the-art in
    compression-accuracy trade-offs.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一个替代解决方案：我们的做法是沿着优化参数的小子空间中目标的实际梯度进行优化，而不是依赖启发式梯度估计。根据这一见解，我们制定了*PV-tuning
    框架*来微调任意量化表示。我们更新离散和连续组件，以最小化全局目标函数，例如相对于原始模型预测的 KL 散度。我们的结果表明，这种策略在权重表示中带来了显著的改进，实现了压缩与准确度权衡的新最先进水平。
- en: 'The main contributions of our work can be summarized as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的主要贡献可以总结如下：
- en: '1.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We analyze the problem for training discrete quantized representations for better
    understanding of the limitations of existing optimization algorithms. We then
    propose a novel algorithm inspired by compressed gradient methods that addresses
    these limitations. When compared to straight-through estimation and stochastic
    rounding, our approach 1) can be shown to converge to a stable solution; and 2)
    this solution is significantly more accurate in practice.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们分析了离散量化表示训练的问题，以便更好地理解现有优化算法的局限性。然后，我们提出了一种新的算法，受压缩梯度方法的启发，解决了这些局限性。与直接估计和随机四舍五入相比，我们的方法1）可以证明收敛到稳定的解决方案；2）这个解决方案在实践中显著更准确。
- en: '2.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We generalize the proposed algorithm into the PV-Tuning framework²²2The official
    implementation is available at [https://github.com/Vahe1994/AQLM/tree/pv-tuning](https://github.com/Vahe1994/AQLM/tree/pv-tuning).,
    which can minimize a global objective function over a general quantized representation,
    by optimizing both continuous and discrete parameters via a variant of coordinate
    descent.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将所提出的算法推广到PV-Tuning框架²²2官方实现可以在[https://github.com/Vahe1994/AQLM/tree/pv-tuning](https://github.com/Vahe1994/AQLM/tree/pv-tuning)中找到。该框架可以通过优化连续和离散参数的坐标下降变体，最小化全局目标函数，从而在通用量化表示上进行优化。
- en: '3.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We demonstrate that PV-tuning can improve quantized model accuracy for leading
    existing approaches, including GPTQ and AQLM, on popular LLMs including Llama-2
    & 3 and Mistral. Our procedure achieves state-of-the-art accuracy (measured through
    perplexity) in 1- and 2-bit quantization regimes while using the same amount of
    calibration data as the original algorithms. Importantly, the PV-tuned models
    use the same underlying weight representations, and are compatible with existing
    inference kernels. In terms of accuracy per model size, PV-tuning of vector quantization
    outperforms all prior techniques in the 1-3 bits/parameter range, and is the first
    to achieve Pareto-optimal quantization for Llama 2 models at around 2 bits per
    parameter.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了PV-tuning可以提高主流现有方法（包括GPTQ和AQLM）在流行LLMs（包括Llama-2 & 3和Mistral）上的量化模型准确性。我们的过程在1位和2位量化范围内达到了最先进的准确性（通过困惑度测量），同时使用与原始算法相同的标定数据量。重要的是，PV-tuned模型使用相同的基础权重表示，并且与现有的推理内核兼容。在模型大小的准确性方面，PV-tuning的向量量化在1-3位/参数范围内优于所有之前的技术，并且首次在每参数大约2位的范围内实现了Llama
    2模型的帕累托最优量化。
- en: 2 Background
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2 背景
- en: Post-Training LLM Quantization (PTQ).
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 后训练大规模语言模型量化（PTQ）。
- en: There has been significant interest in PTQ methods [[45](#bib.bib45), [23](#bib.bib23)]
    that would scale to LLMs. Early work [[15](#bib.bib15), [72](#bib.bib72), [46](#bib.bib46)]
    used direct round-to-nearest (RTN) quantization over weight groups of well-chosen
    size. GPTQ [[20](#bib.bib20)] improved upon these results significantly via an
    accurate one-shot solver for minimizing layer-wise compression errors. Next, AWQ [[39](#bib.bib39)]
    improved upon these results by employing per-channel scaling to reduce the error
    on important weights while SqueezeLLM [[33](#bib.bib33)] implemented non-uniform
    quantization. QuIP [[8](#bib.bib8)] proposed a more accurate weight representation
    by leveraging incoherence matrices. Another line of works [[16](#bib.bib16), [36](#bib.bib36)]
    proposes an improved quantized weight representation, which saves a small fraction
    of outliers in full precision. Other recent works propose augmenting quantized
    representations with lowrank “adapters” that compensate quantization error [[26](#bib.bib26),
    [77](#bib.bib77)]. Recently, BiLLM [[29](#bib.bib29)] developed residual binarization
    that stores salient weights in progressively higher bitwidth, quantizing models
    to nearly 1 bit per parameter at non-catastrophic accuracy loss.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于能够扩展到大规模语言模型（LLMs）的PTQ方法，已经引起了显著关注[[45](#bib.bib45), [23](#bib.bib23)]。早期的工作[[15](#bib.bib15),
    [72](#bib.bib72), [46](#bib.bib46)]使用了针对权重组的直接四舍五入（RTN）量化。GPTQ[[20](#bib.bib20)]通过一种精确的一次性求解器显著改进了这些结果，以最小化层级压缩误差。接着，AWQ[[39](#bib.bib39)]通过采用每通道缩放改进了这些结果，以减少重要权重的误差，而SqueezeLLM[[33](#bib.bib33)]实现了非均匀量化。QuIP[[8](#bib.bib8)]通过利用不连贯矩阵提出了一种更准确的权重表示。另一系列工作[[16](#bib.bib16),
    [36](#bib.bib36)]提出了一种改进的量化权重表示，保存了小部分精度损失的离群点。其他近期的工作提出了用低秩“适配器”增强量化表示，以补偿量化误差[[26](#bib.bib26),
    [77](#bib.bib77)]。最近，BiLLM[[29](#bib.bib29)]开发了残差二值化，逐渐以更高的位宽存储显著权重，将模型量化为每个参数接近1位的精度，几乎没有灾难性的精度损失。
- en: 'Currently, the state-of-the-art methods in terms of accuracy-vs-size are QuIP# [[65](#bib.bib65)]
    and AQLM [[19](#bib.bib19)]. Both methods work roughly by mapping weight groups
    to points on highly-dimensional lattices, which are either chosen to satisfy some
    optimality properties (for QuIP#) or are learned (for AQLM). Interestingly, AQLM
    showed that fine-tuning the continuous parameters (codebooks) can improve accuracy
    significantly relative to pure one-shot compression; a variant of this approach
    was also adopted by QuIP#. PV-Tuning is compatible with both methods: as we show,
    it can lead to state-of-the-art compression results for such representations.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当前在准确度与模型大小之间权衡的最先进方法是 QuIP# [[65](#bib.bib65)] 和 AQLM [[19](#bib.bib19)]。这两种方法的基本原理是将权重组映射到高维格点上，这些格点要么是选择满足某些最优性属性的（对于
    QuIP#），要么是通过学习得到的（对于 AQLM）。有趣的是，AQLM 显示，微调连续参数（代码本）可以相对于纯一次性压缩显著提高准确度；QuIP# 也采纳了这一方法的一个变体。PV-Tuning
    与这两种方法兼容：正如我们所示，它可以为这些表示带来最先进的压缩结果。
- en: Fine-tuning over Quantized Weights.
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对量化权重的微调。
- en: As mentioned above, the two SOTA quantization techniques apply fine-tuning,
    but only update *continuous* parameters, such as quantization scales. When optimizing
    over *discrete* parameter sets, a standard choice in deep learning is the Straight-Through
    Estimator (STE) [[5](#bib.bib5), [13](#bib.bib13), [67](#bib.bib67)]. Prior work
    on LLM compression proposed to update both continuous and discrete parameters,
    via STE, both for post-training quantization [[72](#bib.bib72), [58](#bib.bib58)]
    and for training quantized networks from scratch [[29](#bib.bib29)]. However,
    it was observed early on that STE leads to instability when fine-tuning heavily
    quantized LLMs [[72](#bib.bib72)]. While early results suggest that STE can perform
    well when training quantized models from scratch [[41](#bib.bib41)], this behavior
    is yet to be validated for highly-performant multi-billion-parameter models, which
    are the focus of our work.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，两种最先进的量化技术应用了微调，但仅更新 *连续* 参数，如量化尺度。在优化 *离散* 参数集时，深度学习中的一个标准选择是直通估计器（STE）[[5](#bib.bib5),
    [13](#bib.bib13), [67](#bib.bib67)]。先前的工作在 LLM 压缩中提出通过 STE 更新连续和离散参数，无论是用于训练后量化
    [[72](#bib.bib72), [58](#bib.bib58)] 还是从头训练量化网络 [[29](#bib.bib29)]。然而，早期观察到 STE
    在微调高度量化的 LLM 时会导致不稳定 [[72](#bib.bib72)]。虽然早期结果表明 STE 在从头训练量化模型时表现良好 [[41](#bib.bib41)]，但这一行为尚需在高性能的数十亿参数模型上验证，这也是我们工作的重点。
- en: In summary, the two standard approaches for fine-tuning quantized LLMs are 1)
    fine-tuning only over the continuous parameters, such as quantization scales,
    which heavily limits the number of trainable parameters; and 2) optimizing all
    parameters via the STE, which however is known to be quite noisy especially for
    extreme quantization. In this context, our work proposes alternative approaches
    in the post-training compression setting, which lead to state-of-the-art results
    relative to both options.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，微调量化大型语言模型的两种标准方法是：1) 仅微调连续参数，如量化尺度，这极大限制了可训练参数的数量；2) 通过 STE 优化所有参数，但 STE
    在极端量化情况下已知会产生较大的噪声。在这种背景下，我们的工作提出了在训练后压缩设置中的替代方法，这些方法相对于这两种选择都能达到最先进的结果。
- en: 3 Fine-Tuning Quantized Models
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3 量化模型的微调
- en: 'In this section, we study the problem of fine-tuning quantized models to minimize
    a global objective, such as cross-entropy. Section [3.1](#S3.SS1 "3.1 Problem
    description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression") formulates this problem from an optimization
    perspective and introduces our notation. In Section [3.2](#S3.SS2 "3.2 Linearized
    V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression"), we analyze several
    popular strategies for solving this problem and highlight some of their limitations.
    To circumvent these limitations, we propose an alternative optimization algorithm
    in Section [3.3](#S3.SS3 "3.3 Linearized subspace V step ‣ 3 Fine-Tuning Quantized
    Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    and discuss implementation details in Section [3.4](#S3.SS4 "3.4 Implementation
    details ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们研究了微调量化模型以最小化全局目标（如交叉熵）的问题。第[3.1](#S3.SS1 "3.1 Problem description ‣
    3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")节从优化的角度对这个问题进行了公式化，并介绍了我们的符号。第[3.2](#S3.SS2 "3.2
    Linearized V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized
    Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")节中，我们分析了几种解决此问题的流行策略，并强调了它们的一些局限性。为了规避这些局限性，我们在第[3.3](#S3.SS3
    "3.3 Linearized subspace V step ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")节中提出了一种替代的优化算法，并在第[3.4](#S3.SS4
    "3.4 Implementation details ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression")节中讨论了实现细节。'
- en: 3.1 Problem description
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1 问题描述
- en: Consider the problem of minimizing objective (loss) $\phi$,
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑最小化目标（损失）$\phi$的问题，
- en: '|  | $\min_{x\in\mathbb{R}^{d}_{c}}\phi(x),$ |  | (1) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{x\in\mathbb{R}^{d}_{c}}\phi(x),$ |  | (1) |'
- en: where $\phi:\mathbb{R}^{d}\to\mathbb{R}$
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi:\mathbb{R}^{d}\to\mathbb{R}$
- en: Useful notation. A vector $x\in\mathbb{R}^{d}_{c}$ characterized by
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有用的符号。一个向量$x\in\mathbb{R}^{d}_{c}$ 由以下特征描述
- en: '|  | $x_{i}=x_{j}\quad\Leftrightarrow\quad\exists k\;:\;i\in P_{k}\text{ and
    }j\in P_{k}.$ |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{i}=x_{j}\quad\Leftrightarrow\quad\exists k\;:\;i\in P_{k}\text{ 和
    }j\in P_{k}.$ |  |'
- en: Let’s denote $P(x):=\{P_{1}(x),\dots,P_{c}(x)\}$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 设我们记作 $P(x):=\{P_{1}(x),\dots,P_{c}(x)\}$。
- en: PV method. Following this notation, we define an optimization algorithm that
    alternates between optimizing $\phi$. From a practitioner’s point of view, these
    represent optimizing continuous parameters (scales, codebooks, zeros) and discrete
    codes (assignments), respectively.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: PV 方法。根据这个符号，我们定义了一种优化算法，该算法在优化$\phi$之间交替进行。从实践者的角度来看，这分别代表优化连续参数（尺度、代码本、零值）和离散代码（分配）。
- en: $\diamond$, consider the mapping
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: $\diamond$，考虑映射
- en: '|  | $M_{P}(x)=M_{P,\phi}(x):=\arg\min_{y\in\mathbb{R}^{d}}\{\phi(y)\,:\,P(y)\supseteq
    P(x)\}.$ |  | (2) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $M_{P}(x)=M_{P,\phi}(x):=\arg\min_{y\in\mathbb{R}^{d}}\{\phi(y)\,:\,P(y)\supseteq
    P(x)\}.$ |  | (2) |'
- en: Notice that, necessarily, $M_{P}(x)\in\mathbb{R}^{d}_{\leq c}$-dimensional space.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到，$M_{P}(x)\in\mathbb{R}^{d}_{\leq c}$-维空间。
- en: $\diamond$, we define the mapping
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: $\diamond$，我们定义映射
- en: '|  | $M_{V}(y)=M_{V,\phi}(y):=\arg\min_{x\in\mathbb{R}^{d}}\{\phi(x)\,:\,V(x)\subseteq
    V(y)\}.$ |  | (3) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $M_{V}(y)=M_{V,\phi}(y):=\arg\min_{x\in\mathbb{R}^{d}}\{\phi(x)\,:\,V(x)\subseteq
    V(y)\}.$ |  | (3) |'
- en: Likewise, $M_{V}(y)\in\mathbb{R}^{d}_{\leq c}$).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，$M_{V}(y)\in\mathbb{R}^{d}_{\leq c}$)。
- en: Algorithm 1 PV algorithm
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 PV 算法
- en: '1:  Initialization: starting point $x^{0}\in\mathbb{R}^{d}_{\leq c}$ (V step:
    discrete)5:  end for'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  初始化：起始点$x^{0}\in\mathbb{R}^{d}_{\leq c}$（V步骤：离散）5:  结束循环'
- en: 'Our key algorithmic idea, in its simplest form, is to optimize $\phi$ operators.
    (We will propose several more practically-useful approximations and variations
    later; see Sections [3.2](#S3.SS2 "3.2 Linearized V step & gradient-based discrete
    updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")–[3.3](#S3.SS3 "3.3 Linearized subspace
    V step ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression") and also Appendix [B](#A2 "Appendix B Approximate
    PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression").) This resulting method, which we call the PV method, is formalized
    as Algorithm [1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression"). Our key guarantee for the PV method is formalized in the next result.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的关键算法思想，在最简单的形式下，是优化$\phi$操作符。（我们将在后面提出几种更实用的近似和变体；参见[3.2](#S3.SS2 "3.2 线性化V步骤和基于梯度的离散更新
    ‣ 3 精细调整量化模型 ‣ PV调优：极端LLM压缩的超越直通估计")–[3.3](#S3.SS3 "3.3 线性化子空间V步骤 ‣ 3 精细调整量化模型
    ‣ PV调优：极端LLM压缩的超越直通估计")以及附录[B](#A2 "附录 B 近似PV算法 ‣ 附录 ‣ PV调优：极端LLM压缩的超越直通估计")）。我们称之为PV方法的方法形式化为算法[1](#alg1
    "算法 1 ‣ 3.1 问题描述 ‣ 3 精细调整量化模型 ‣ PV调优：极端LLM压缩的超越直通估计")。我们对PV方法的关键保证在下一个结果中形式化。
- en: Theorem 3.1  (Convergence of the PV method).
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 3.1  （PV方法的收敛性）。
- en: Assume $\phi$ converges.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 假设$\phi$收敛。
- en: 'The proof can be found in Appendix [A.1](#A1.SS1 "A.1 Proof of Theorem 3.1
    ‣ Appendix A Proofs ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression"). Note that we do not claim that the method converges
    to a minimizer of $\phi$; the optimization problem is too difficult for us to
    be able to guarantee this. However, as we shall see in the numerical results,
    we nevertheless obtain great empirical performance, especially when coupling the
    PV approach with some additional algorithmic tricks.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 证明见附录[A.1](#A1.SS1 "A.1 定理 3.1 的证明 ‣ 附录 A 证明 ‣ 附录 ‣ PV调优：极端LLM压缩的超越直通估计")。请注意，我们并不声称该方法收敛到$\phi$的极小值；优化问题过于复杂，我们无法保证这一点。然而，正如我们将在数值结果中看到的那样，我们仍然获得了很好的经验性能，特别是当将PV方法与一些附加算法技巧结合时。
- en: This general approach is popular in “shallow” machine learning problems; for
    instance, if $\phi(x)=\|x-z\|^{2}$, the approach is related to the EM algorithm [[14](#bib.bib14)].
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这种通用方法在“浅层”机器学习问题中很受欢迎；例如，如果$\phi(x)=\|x-z\|^{2}$，则该方法与EM算法相关[[14](#bib.bib14)]。
- en: 'In turn, we apply the PV method to obtaining highly-accurate quantized LLMs.
    Applying the PV method “as is”, would be infeasible in practice: computing the
    P and V mappings requires solving difficult optimization problems especially due
    to LLM parameter scales. However, both mappings can be approximated. Computing
    the P step ($M_{P}(\cdot)$. We dedicate the next two sections to this task.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，我们应用PV方法来获得高精度的量化LLM。将PV方法“按原样”应用在实践中是不切实际的：计算P和V映射需要解决困难的优化问题，特别是由于LLM参数的规模。然而，两个映射都可以被近似。计算P步骤（$M_{P}(\cdot)$）。我们将接下来的两个部分专门用于此任务。
- en: 3.2 Linearized V step & gradient-based discrete updates
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2 线性化V步骤和基于梯度的离散更新
- en: 'The V mapping ([3](#S3.E3 "Equation 3 ‣ 3.1 Problem description ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression")) can be approximated by solving a discrete least squares problem
    using an approximation of $\phi(x)$:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: V映射（[3](#S3.E3 "方程 3 ‣ 3.1 问题描述 ‣ 3 精细调整量化模型 ‣ PV调优：极端LLM压缩的超越直通估计")）可以通过使用$\phi(x)$的近似来解决离散最小二乘问题来进行近似。
- en: '|  | $\textstyle\phi(x)\approx\widetilde{\phi}_{y}(x):=\phi(y)+\left\langle\nabla\phi(y),x-y\right\rangle+\frac{L}{2}{\left\&#124;x-y\right\&#124;}^{2},$
    |  | (4) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textstyle\phi(x)\approx\widetilde{\phi}_{y}(x):=\phi(y)+\left\langle\nabla\phi(y),x-y\right\rangle+\frac{L}{2}{\left\&#124;x-y\right\&#124;}^{2},$
    |  | (4) |'
- en: 'where $$L>:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$$L>:'
- en: '|  | $1$2 |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'Our first lemma shows that we can replace $\widetilde{\phi}_{y}$, disregarding
    the constraint:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个引理表明，我们可以在忽略约束的情况下替换$\widetilde{\phi}_{y}$：
- en: Lemma 3.2.
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 3.2。
- en: For any $y\in\mathbb{R}^{d}_{\leq c}$ where
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何$y\in\mathbb{R}^{d}_{\leq c}$，其中
- en: '|  | $1$2 |  | (5) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: 'The proof can be found in Appendix [A.2](#A1.SS2 "A.2 Proof of Lemma 3.2 ‣
    Appendix A Proofs ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for
    Extreme LLM Compression"). To summarize, the V step of the PV method (Algorithm [1](#alg1
    "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")), i.e., $x=M_{V,\phi}(y),$
    can be approximated via the “linearized V step”'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 证明可以在附录 [A.2](#A1.SS2 "A.2 引理 3.2 的证明 ‣ 附录 A 证明 ‣ 附录 ‣ PV 调优：超越直通估计以实现极端 LLM
    压缩") 中找到。总之，PV 方法的 V 步骤（算法 [1](#alg1 "算法 1 ‣ 3.1 问题描述 ‣ 3 微调量化模型 ‣ PV 调优：超越直通估计以实现极端
    LLM 压缩")），即 $x=M_{V,\phi}(y)$，可以通过“线性化 V 步骤”来近似。
- en: '|  | $x:=M_{V,\phi}(y)\approx M_{V,\widehat{\phi}_{y}}(y):=\hat{x}.$ |  | (6)
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $x:=M_{V,\phi}(y)\approx M_{V,\widehat{\phi}_{y}}(y):=\hat{x}.$ |  | (6)
    |'
- en: Our next lemma says that the above approximation is in a certain sense natural
    reasonable provided that $\phi$, i.e., provided that
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个引理表示，上述近似在某种意义上是自然合理的，前提是 $\phi$，即，前提是
- en: '|  | $1$2 |  | (7) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: Lemma 3.3  (Monotonicity).
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 3.3（单调性）。
- en: 'Let $y\in\mathbb{R}^{d}_{\leq c}$ by the linearized V step ([6](#S3.E6 "Equation
    6 ‣ 3.2 Linearized V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized
    Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $y\in\mathbb{R}^{d}_{\leq c}$ 为线性化 V 步骤 ([6](#S3.E6 "方程 6 ‣ 3.2 线性化 V 步骤与基于梯度的离散更新
    ‣ 3 微调量化模型 ‣ PV 调优：超越直通估计以实现极端 LLM 压缩"))。
- en: 'Indeed, the point $\hat{x}$. Of course, one hopes that the loss will strictly
    decrease so that the method makes progress. From a practical perspective, the
    key advantage of linearized V step is that it can be performed much faster compared
    to the vanilla V step. The proof of Lemma [3.3](#S3.Thmtheorem3 "Lemma 3.3 (Monotonicity).
    ‣ 3.2 Linearized V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized
    Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    can be found in Appendix [A.3](#A1.SS3 "A.3 Proof of Lemma 3.3 ‣ Appendix A Proofs
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，点 $\hat{x}$。当然，希望损失函数会严格减少，从而使方法取得进展。从实际角度看，线性化 V 步骤的主要优点在于，相较于普通的 V 步骤，它可以更快地执行。引理
    [3.3](#S3.Thmtheorem3 "引理 3.3（单调性）。 ‣ 3.2 线性化 V 步骤与基于梯度的离散更新 ‣ 3 微调量化模型 ‣ PV 调优：超越直通估计以实现极端
    LLM 压缩") 的证明可以在附录 [A.3](#A1.SS3 "A.3 引理 3.3 的证明 ‣ 附录 A 证明 ‣ 附录 ‣ PV 调优：超越直通估计以实现极端
    LLM 压缩") 中找到。
- en: 'Note that since $\widehat{\phi}_{y}(x)$ error (see Appendix [D](#A4 "Appendix
    D Efficient Linearized V Step for Vector Quantization ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression"))'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于 $\widehat{\phi}_{y}(x)$ 的误差（见附录 [D](#A4 "附录 D 向量量化的高效线性化 V 步骤 ‣ 附录 ‣
    PV 调优：超越直通估计以实现极端 LLM 压缩")）
- en: 'Key challenge. The main caveat with linearized V step is that it may be impossible
    to make small gradient-based updates to low-bitwidth discrete weights. More specifically,
    in ([6](#S3.E6 "Equation 6 ‣ 3.2 Linearized V step & gradient-based discrete updates
    ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")), one must update the discrete assignments to approximate
    $y^{k}-\frac{1}{L}\nabla\phi(y^{k})$ can be very large, or, from a practitioner’s
    point of view, where one needs a small learning rate. In practice, as we explore
    in Section [4.2](#S4.SS2 "4.2 Evaluating Fine-tuning Algorithms ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"),
    the lowest learning rate where the algorithm makes any updates at all is already
    too large for optimization, leading to divergence.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 关键挑战。线性化 V 步骤的主要警告是，可能无法对低位宽离散权重进行小幅基于梯度的更新。更具体地说，在 ([6](#S3.E6 "方程 6 ‣ 3.2
    线性化 V 步骤与基于梯度的离散更新 ‣ 3 微调量化模型 ‣ PV 调优：超越直通估计以实现极端 LLM 压缩")) 中，必须更新离散分配以近似 $y^{k}-\frac{1}{L}\nabla\phi(y^{k})$
    可能非常大，或者从实践者的角度看，需要一个小的学习率。在实践中，正如我们在第 [4.2](#S4.SS2 "4.2 评估微调算法 ‣ 4 实验 ‣ PV 调优：超越直通估计以实现极端
    LLM 压缩") 节中探讨的那样，算法进行任何更新的最低学习率已经过高，导致发散。
- en: 'Many popular strategies for discrete fine-tuning can be seen as attempts to
    reconcile coarse low-precision weights with the need to make small updates. These
    include straight-through estimation, stochastic rounding, or adding regularizers
    that push the solution to ([6](#S3.E6 "Equation 6 ‣ 3.2 Linearized V step & gradient-based
    discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) away from $y^{k}$. We review straight-through
    estimation in Appendix [E](#A5 "Appendix E Straight-through Gradient Estimation
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    and stochastic rounding in Appendix [F](#A6 "Appendix F Stochastic Rounding ‣
    Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '许多流行的离散微调策略可以被视为试图调和粗略的低精度权重与需要进行小幅更新的需求。这些包括直通估计、随机舍入，或添加正则化项以将解决方案推向（[6](#S3.E6
    "方程 6 ‣ 3.2 线性化 V 步骤和基于梯度的离散更新 ‣ 3 微调量化模型 ‣ PV-Tuning: 超越直通估计进行极端 LLM 压缩")）远离
    $y^{k}$。我们在附录 [E](#A5 "附录 E 直通梯度估计 ‣ 附录 ‣ PV-Tuning: 超越直通估计进行极端 LLM 压缩") 中回顾了直通估计，在附录
    [F](#A6 "附录 F 随机舍入 ‣ 附录 ‣ PV-Tuning: 超越直通估计进行极端 LLM 压缩") 中回顾了随机舍入。'
- en: 'Algorithm 2 PV-Tuning: Optimization'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 2 PV-Tuning: 优化'
- en: 0:initial parameters $x^{0}\in\mathbb{R}^{d}_{c}$8:  end for
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '0:初始参数 $x^{0}\in\mathbb{R}^{d}_{c}$8: end for'
- en: 'Algorithm 3 PV-Tuning: Implementation, one step'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 3 PV-Tuning: 实现，一步'
- en: '0:quantized model, subspace size tau1:  deq_model := dequantize_weights(model)2:  for $t=1,\dots,T$
    V step: choose a subspace s and update codes11:  update ​​=​​ adam(grad_phi) ​​​-​​
    deq_model.weight12:  s = choose_subspace(update, tau)13:  model.codes[s] = find_nearest(update[s])'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '0:量化模型，子空间大小 tau1: deq_model := dequantize_weights(model)2: for $t=1,\dots,T$
    V 步骤：选择一个子空间 s 并更新代码11: update = adam(grad_phi) - deq_model.weight12: s = choose_subspace(update,
    tau)13: model.codes[s] = find_nearest(update[s])'
- en: 3.3 Linearized subspace V step
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3 线性化子空间 V 步骤
- en: 'Here we ask the following question: Can we modify the PV method so as to force
    the V step to make a larger update? In other words, we need an optimization algorithm
    that updates quantized weights either by a sufficiently large increment, or not
    at all.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提出以下问题：我们可以修改 PV 方法以强制 V 步骤进行更大的更新吗？换句话说，我们需要一个优化算法，该算法要么通过足够大的增量更新量化权重，要么完全不更新。
- en: A natural example of such an algorithm is coordinate descent (CD) [[40](#bib.bib40),
    [53](#bib.bib53)], or more generally, subspace descent [[24](#bib.bib24), [35](#bib.bib35)].
    Instead of updating all parameters by a small margin, CD in each iteration chooses
    a single parameter, and makes a large update instead. This strategy can be generalized
    to updating more parameters at the same time, which leads to subspace descent
    methods.⁵⁵5Very closely related methods include block coordinate descent and compressed
    gradient descent with sparsification operators such as RandK or TopK [[3](#bib.bib3),
    [6](#bib.bib6)]. The parameters to be updated can be chosen either greedily, (e.g.,
    several $i\in[d]$), or at random, or through a variety of other means.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这种算法的一个自然例子是坐标下降 (CD) [[40](#bib.bib40), [53](#bib.bib53)]，或者更一般地说，子空间下降 [[24](#bib.bib24),
    [35](#bib.bib35)]。CD 在每次迭代中选择一个单一参数，而不是通过小幅度更新所有参数，而是进行较大的更新。这种策略可以推广到同时更新更多参数，这就导致了子空间下降方法。⁵⁵5
    非常相关的方法包括块坐标下降和使用稀疏化操作符（如 RandK 或 TopK）进行的压缩梯度下降 [[3](#bib.bib3), [6](#bib.bib6)]。待更新的参数可以通过贪婪的方式选择（例如，多个
    $i\in[d]$），也可以随机选择，或者通过各种其他方式选择。
- en: 'Let $\mathcal{S}^{k}\subset[d]$. We now formulate the linearized subspace V
    step:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\mathcal{S}^{k}\subset[d]$。我们现在制定线性化子空间 V 步骤：
- en: '|  | $1$2 |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  | (8) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (8) |'
- en: 'and <math id=$$ estimates in Appendix [G](#A7 "Appendix G On 𝐿-smoothness of
    LLM Objective in Sparse Subspaces ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '和 <math id=$$ 估计见附录 [G](#A7 "附录 G 关于稀疏子空间中 LLM 目标的 L-光滑性 ‣ 附录 ‣ PV-Tuning:
    超越直通估计进行极端 LLM 压缩")。'
- en: Note that, necessarily, $x^{+}_{i}=y_{i}$, for example.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，必需的是，例如，$x^{+}_{i}=y_{i}$。
- en: In practice, it means that *the algorithm can apply large updates to quantized
    LLM weights, with the caveat that should only update a fraction of them at a time*.
    This allows us to perform the linearized V step with sufficiently large “learning
    rate” to make non-trivial (i.e., $x^{k+1}\neq y^{k}$) improvements to quantized
    weights even without straight-through estimation or stochastic rounding.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这意味着*算法可以对量化LLM权重应用大更新，但要注意每次只更新其中的一部分*。这使我们能够以足够大的“学习率”执行线性化V步骤，即使在没有直通估计或随机舍入的情况下，也能对量化权重进行非平凡（即$x^{k+1}\neq
    y^{k}$）的改进。
- en: 'We formulate the full procedure in Algorithm [2](#alg2 "Algorithm 2 ‣ 3.2 Linearized
    V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression"). The algorithm
    performs the P step by directly optimizing $V(x)$.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在算法 [2](#alg2 "算法 2 ‣ 3.2 线性化 V 步骤与基于梯度的离散更新 ‣ 3 微调量化模型 ‣ PV调整：超越直通估计实现极端LLM压缩")中制定了完整的过程。该算法通过直接优化$V(x)$来执行P步骤。
- en: 3.4 Implementation details
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4 实施细节
- en: 'To speed up convergence, we use adaptive learning rates for both P and V steps.
    In Eq. [8](#S3.E8 "Equation 8 ‣ 3.3 Linearized subspace V step ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression"), we replace $\nabla\phi(y)$ weights with the largest update norm
    within each weight matrix.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快收敛速度，我们对P和V步骤使用自适应学习率。在 Eq. [8](#S3.E8 "方程 8 ‣ 3.3 线性化子空间 V 步骤 ‣ 3 微调量化模型
    ‣ PV调整：超越直通估计实现极端LLM压缩")中，我们将$\nabla\phi(y)$权重替换为每个权重矩阵中的最大更新范数。
- en: 'This could be further improved through better techniques for choosing $\mathcal{S}^{k}$
    explored in Appendix [O](#A15 "Appendix O Small-Scale Experiments and Interpretation
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").
    We also found that, despite the fact that PV-tuning by itself outperforms straight-through
    estimation, we could achieve slightly better accuracy by combining PV-tuning with
    straight-through estimation. We explore this in more detail in Section [4.2](#S4.SS2
    "4.2 Evaluating Fine-tuning Algorithms ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过改进选择$\mathcal{S}^{k}$的技术进一步提升，具体内容见附录 [O](#A15 "附录 O 小规模实验与解释 ‣ 附录 ‣ PV调整：超越直通估计实现极端LLM压缩")。我们还发现，尽管PV调整本身优于直通估计，通过将PV调整与直通估计结合使用，我们可以获得略微更好的准确度。我们将在第 [4.2节](#S4.SS2
    "4.2 评估微调算法 ‣ 4 实验 ‣ PV调整：超越直通估计实现极端LLM压缩")中对此进行更详细的探讨。
- en: 'We describe our approach for preparing the calibration data in Appendix [H](#A8
    "Appendix H Calibration Data Matters ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"). We found that the preprocessing used
    in several recent PTQ works introduce a small bias when sampling the calibration
    data, leading to somewhat worse fine-tuning accuracy. For fairness, we always
    compare representations (Section [4.1](#S4.SS1 "4.1 Evaluating quantized representations
    with finetuning ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")) and algorithms (Section [4.2](#S4.SS2 "4.2 Evaluating
    Fine-tuning Algorithms ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")) using the same pre-processing.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在附录 [H](#A8 "附录 H 校准数据的重要性 ‣ 附录 ‣ PV调整：超越直通估计实现极端LLM压缩")中描述了准备校准数据的方法。我们发现，最近几篇PTQ相关工作的预处理在采样校准数据时引入了小的偏差，导致微调准确度有所下降。为了公平起见，我们总是使用相同的预处理来比较表示（第 [4.1节](#S4.SS1
    "4.1 评估量化表示与微调 ‣ 4 实验 ‣ PV调整：超越直通估计实现极端LLM压缩")）和算法（第 [4.2节](#S4.SS2 "4.2 评估微调算法
    ‣ 4 实验 ‣ PV调整：超越直通估计实现极端LLM压缩")）。
- en: Fine-tuning efficiency. The most compute-intensive part of PV tuning is computing
    the gradients $\nabla\phi(\cdot)$.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 微调效率。PV调整中最计算密集的部分是计算梯度$\nabla\phi(\cdot)$。
- en: 4 Experiments
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Evaluating quantized representations with finetuning
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1 评估量化表示与微调
- en: 'Before evaluating PV-tuning, we need to choose the quantized representation
    to be fine-tuned. We therefore compare popular weight representations from recent
    works on LLM quantization (see Section [2](#S2.SS0.SSS0.Px1 "Post-Training LLM
    Quantization (PTQ). ‣ 2 Background ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")). To better isolate the effect of the weight representation,
    we evaluate them in three configurations: i) when quantizing a single LLM layer,
    in terms of MSE, ii) full model quantization in terms of perplexity without finetuning
    and iii) with finetuning.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '在评估PV调整之前，我们需要选择要进行微调的量化表示。因此，我们比较了近期关于LLM量化的流行权重表示（参见第[2](#S2.SS0.SSS0.Px1
    "Post-Training LLM Quantization (PTQ). ‣ 2 Background ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")节）。为了更好地隔离权重表示的影响，我们在三种配置中对它们进行评估：i) 在量化单个LLM层时，以MSE为标准，ii)
    在未微调的情况下进行全模型量化，以困惑度为标准，iii) 进行微调时。'
- en: 'We compare several recently proposed quantized representations (see details
    in Appendix [J](#A10 "Appendix J Additional Details for Section 4.1 ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '我们比较了几种近期提出的量化表示（详见附录[J](#A10 "Appendix J Additional Details for Section 4.1
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")）：'
- en: '1.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'GPTQ: scalar uniform quantization with channel-wise and block-wise scales [[20](#bib.bib20)],'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GPTQ: 带有通道级和块级缩放的标量均匀量化[[20](#bib.bib20)],'
- en: '2.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'SpQR: an extension of block-wise GPTQ with learned sparse outliers [[16](#bib.bib16)],'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SpQR: 基于学习的稀疏异常值的块级GPTQ扩展[[16](#bib.bib16)],'
- en: '3.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'VQ: basic vector quantization with a single codebook  [[66](#bib.bib66)] with
    multi-step training.'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'VQ: 带有单一代码本的基本向量量化[[66](#bib.bib66)]，采用多步骤训练。'
- en: '4.'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'AQLM: additive vector quantization with multiple learned codebooks [[19](#bib.bib19)],'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'AQLM: 带有多个学习代码本的加法向量量化[[19](#bib.bib19)],'
- en: '5.'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'QuIP#: vector quantization with lattices and incoherence processing [[65](#bib.bib65)],'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'QuIP#: 带有格点和不一致处理的向量量化[[65](#bib.bib65)],'
- en: '6.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'VQ/AQ + outliers: vector/additive quantization with sparse outliers via pruning [[61](#bib.bib61),
    [7](#bib.bib7)],'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'VQ/AQ + outliers: 通过修剪的稀疏异常值的向量/加法量化[[61](#bib.bib61), [7](#bib.bib7)],'
- en: '7.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'VQ/AQ + lowrank: vector/additive quantization with Low-Rank Compensation (LoRC) [[73](#bib.bib73)],'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'VQ/AQ + lowrank: 带有低秩补偿（LoRC）的向量/加法量化[[73](#bib.bib73)],'
- en: '![Refer to caption](img/064ad80c11bbc5b9488313ad92e20fbc.png)![Refer to caption](img/ed91444e22fed637394a55efd0a2f12b.png)![Refer
    to caption](img/94dd797315fa09921daace201e4538e5.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/064ad80c11bbc5b9488313ad92e20fbc.png)![参考说明](img/ed91444e22fed637394a55efd0a2f12b.png)![参考说明](img/94dd797315fa09921daace201e4538e5.png)'
- en: 'Figure 2: (left) L2 errors for 17th layer of Llama 2 7B with different representations.
    Full model perplexity on WikiText-2 is reported without finetuning (middle) and
    with fine-tuning (right).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '图2: （左）不同表示下Llama 2 7B第17层的L2误差。未微调时的全模型困惑度（中）和微调时的困惑度（右）。'
- en: 'We run all three experiments on Llama 2 7B model [[63](#bib.bib63)], calibrating
    on the RedPajama [[11](#bib.bib11)] dataset that best approximates the original
    pre-training data. When evaluating single layer errors, we report the L2 error
    in attention query projection outputs of a fixed transformer block, with other
    blocks exhibiting similar behavior. For full model evaluation, we report quantized
    model perplexity on WikiText-2 [[42](#bib.bib42)] dataset. We use the same data
    splits and preprocessing as in most recent PTQ works [[20](#bib.bib20), [39](#bib.bib39),
    [16](#bib.bib16), [64](#bib.bib64), [19](#bib.bib19), [65](#bib.bib65)], including
    the biased preprocessing step that we mentioned in [3.4](#S3.SS4 "3.4 Implementation
    details ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"). For fine-tuning, we train continuous
    parameters only, using the approach from [[65](#bib.bib65)]. To compare these
    diverse representations, we evaluate their quantization errors as a function of
    average number of bits per parameter. To get a diverse set of bits per parameter,
    we vary the hyperparameters such as wbits, block size, codebook and group size
    for vector quantization and the frequency of outliers.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 Llama 2 7B 模型上运行了所有三种实验 [[63](#bib.bib63)]，并在 RedPajama [[11](#bib.bib11)]
    数据集上进行校准，该数据集最好地逼近了原始预训练数据。在评估单层错误时，我们报告固定变换器块的注意力查询投影输出的 L2 错误，其他块表现出类似的行为。对于完整模型评估，我们报告量化模型在
    WikiText-2 [[42](#bib.bib42)] 数据集上的困惑度。我们使用与大多数最近的 PTQ 工作相同的数据拆分和预处理 [[20](#bib.bib20),
    [39](#bib.bib39), [16](#bib.bib16), [64](#bib.bib64), [19](#bib.bib19), [65](#bib.bib65)]，包括我们在
    [3.4](#S3.SS4 "3.4 Implementation details ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression") 中提到的有偏预处理步骤。对于微调，我们仅训练连续参数，使用
    [[65](#bib.bib65)] 的方法。为了比较这些不同的表示，我们评估它们的量化错误，作为每个参数平均位数的函数。为了获得每个参数的多样化位数，我们变化了超参数，如
    wbits、块大小、码本和向量量化的组大小，以及异常值的频率。'
- en: 'Figure [2](#S4.F2 "Figure 2 ‣ 4.1 Evaluating quantized representations with
    finetuning ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation for
    Extreme LLM Compression") summarizes our findings. Overall, vector quantization
    methods (VQ, QuIP and AQLM) outperform their scalar counterparts. Outliers and
    low-rank compensation both reduce error, but this improvement comes at the cost
    of extra bits per parameter. Interestingly, the improvement from outliers is significantly
    smaller when both methods have access to fine-tuning. We attribute this to the
    fact fine-tuning can compensate the error in “important” weights by adjusting
    the rest of the model.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [2](#S4.F2 "Figure 2 ‣ 4.1 Evaluating quantized representations with finetuning
    ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression") 总结了我们的发现。总体而言，向量量化方法 (VQ、QuIP 和 AQLM) 优于它们的标量对应物。异常值和低秩补偿都减少了误差，但这种改进以每个参数的额外位数为代价。有趣的是，当两种方法都可以进行微调时，异常值带来的改进显著较小。我们将此归因于微调可以通过调整模型的其他部分来补偿“重要”权重的错误。'
- en: Our main takeaway is that for sub 2 bits per parameter, the vector quantization
    (VQ) representation can achieve near-optimal quantization accuracy, whether or
    not it uses outliers, LoRC or incoherence processing. Naturally, this does not
    reduce the value of prior works since they were designed for different scenarios,
    typically with a higher number of bits per parameter.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要收获是，对于每个参数小于 2 位的情况，向量量化 (VQ) 表示可以实现接近最优的量化准确度，无论它是否使用异常值、LoRC 或不一致处理。自然地，这并不降低之前工作的价值，因为它们是为不同的场景设计的，通常具有较高的每参数位数。
- en: 'Table 1: Comparing different fine-tuning strategies for VQ, GPTQ and AQLM on
    Llama 2 7B in terms of perplexity on WikiText-2, C4 and average zero-shot accuracy
    on tasks from Section [4.3](#S4.SS3 "4.3 Large-scale Evaluation & Discussion ‣
    4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression").'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1：比较 Llama 2 7B 上 VQ、GPTQ 和 AQLM 的不同微调策略，以 WikiText-2、C4 的困惑度和来自第 [4.3](#S4.SS3
    "4.3 Large-scale Evaluation & Discussion ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression") 节的任务的平均零-shot 准确度为标准。'
- en: '| Fine-tuning Method | GPTQ 2.14 bit/w | VQ, 1.58 bit/w | AQLM, 2.01 bit/w
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 微调方法 | GPTQ 2.14 bit/w | VQ, 1.58 bit/w | AQLM, 2.01 bit/w |'
- en: '| Wiki2$\downarrow$ |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Wiki2$\downarrow$ |'
- en: '| Calibration only (no global fine-tuning) | 3290 | 4125 | 29.0 | 20.26 | 20.09
    | 43.42 | 7.38 | 9.34 | 53.2 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 仅校准（无全球微调） | 3290 | 4125 | 29.0 | 20.26 | 20.09 | 43.42 | 7.38 | 9.34 | 53.2
    |'
- en: '| Continuous params only [[65](#bib.bib65), [19](#bib.bib19)] | 16.77 | 17.53
    | 46.27 | 8.17 | 10.99 | 52.14 | 6.69 | 8.77 | 56.57 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 仅连续参数 [[65](#bib.bib65), [19](#bib.bib19)] | 16.77 | 17.53 | 46.27 | 8.17
    | 10.99 | 52.14 | 6.69 | 8.77 | 56.57 |'
- en: '| Naive Linearized PV (no subspace) | 16.73 | 17.48 | 47.68 | 8.19 | 10.94
    | 52.08 | 6.68 | 8.75 | 56.51 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 天真线性化 PV（无子空间） | 16.73 | 17.48 | 47.68 | 8.19 | 10.94 | 52.08 | 6.68 | 8.75
    | 56.51 |'
- en: '| Stochastic Rounding [[49](#bib.bib49)] (tuned) | 11.97 | 13.07 | 49.79 |
    8.02 | 10.64 | 52.31 | 6.56 | 8.39 | 56.68 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 随机舍入 [[49](#bib.bib49)]（调优） | 11.97 | 13.07 | 49.79 | 8.02 | 10.64 | 52.31
    | 6.56 | 8.39 | 56.68 |'
- en: '| Straight Through Estimation [[71](#bib.bib71)] | 8.79 | 11.04 | 50.61 | 7.76
    | 10.26 | 52.58 | 6.41 | 8.63 | 57.04 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 直通估计 [[71](#bib.bib71)] | 8.79 | 11.04 | 50.61 | 7.76 | 10.26 | 52.58 | 6.41
    | 8.63 | 57.04 |'
- en: '| Subspace Linearized PV (ours, $\tau{=}0.01$) | 8.49 | 10.78 | 52.17 | 7.38
    | 9.47 | 53.36 | 6.13 | 8.35 | 57.81 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 子空间线性化 PV（我们的方法，$\tau{=}0.01$） | 8.49 | 10.78 | 52.17 | 7.38 | 9.47 | 53.36
    | 6.13 | 8.35 | 57.81 |'
- en: '| Subspace Linearized PV+STE ($\tau{=}0.01$) | 8.43 | 10.82 | 51.90 | 7.32
    | 9.35 | 55.22 | 5.90 | 7.43 | 58.19 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 子空间线性化 PV+STE ($\tau{=}0.01$) | 8.43 | 10.82 | 51.90 | 7.32 | 9.35 | 55.22
    | 5.90 | 7.43 | 58.19 |'
- en: 4.2 Evaluating Fine-tuning Algorithms
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2 微调算法评估
- en: 'Next, we compare different fine-tuning strategies and ablate our PV-tuning
    protocol. We design our protocol to be representation-agnostic, i.e. compatible
    with different quantized representations. To showcase this, we pick three methods
    from the previous section: GPTQ, VQ and AQLM.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们比较不同的微调策略并剖析我们的 PV 调优协议。我们设计的协议是表示无关的，即兼容不同的量化表示。为了展示这一点，我们从上一节中挑选了三种方法：GPTQ、VQ
    和 AQLM。
- en: 'These methods differ not only in their weight representations, but also in
    how they search for the optimal codes. Namely, GPTQ can scale the target weight
    and round it to nearest 2-bit integer. In turn, VQ quantizes weights as a group
    and must find the nearest vector from its codebook, and AQLM uses a multi-step
    beam search procedure to choose the best combination of codes from both codebooks.
    Our PV-Tuning implementation uses these search algorithms during the subspace
    linearized V step (find_nearest in Alg. [3](#alg3 "Algorithm 3 ‣ 3.2 Linearized
    V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")). We describe
    the full PV configuration for each method in Appendix [K](#A11 "Appendix K Additional
    Details for Section 4.2 ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression").'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '这些方法不仅在权重表示上有所不同，还在如何寻找最佳代码上有所差异。即，GPTQ 可以缩放目标权重并将其舍入到最近的 2 位整数。而 VQ 将权重作为一个组进行量化，必须从其代码本中找到最接近的向量，AQLM
    使用多步束搜索程序从两个代码本中选择最佳的代码组合。我们的 PV-Tuning 实现使用这些搜索算法来执行子空间线性化 V 步骤（在算法 [3](#alg3
    "Algorithm 3 ‣ 3.2 Linearized V step & gradient-based discrete updates ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression")中的 find_nearest）。我们在附录 [K](#A11 "Appendix K Additional Details for
    Section 4.2 ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression") 中描述了每种方法的完整 PV 配置。'
- en: 'We compare PV tuning against several popular fine-tuning regimens found in
    the literature. Our first baseline is fine-tuning only continuous parameters,
    e.g., codebooks or input/output embeddings [[65](#bib.bib65), [68](#bib.bib68)].
    The second baseline is training with Straight Through Estimation (STE) [[69](#bib.bib69),
    [71](#bib.bib71)]. We also test stochastic rounding as described in Appendix [F](#A6
    "Appendix F Stochastic Rounding ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"). Finally, we evaluate PV tuning combined
    with STE, but otherwise the same configuration. We set the subspace size $\tau$,
    also known as known as trust ratio [[75](#bib.bib75)]. The results in Table [1](#S4.T1
    "Table 1 ‣ 4.1 Evaluating quantized representations with finetuning ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    show that PV-Tuning consistently finds better quantized models, with STE coming
    consistently second. We explore this further by combining subspace updates with
    STE, which leads to slightly better perplexity and accuracy in most (but not all)
    setups.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将 PV 调优与文献中发现的几种流行微调方案进行了比较。我们的第一个基线是仅微调连续参数，例如代码本或输入/输出嵌入 [[65](#bib.bib65),
    [68](#bib.bib68)]。第二个基线是使用直通估计（STE）进行训练 [[69](#bib.bib69), [71](#bib.bib71)]。我们还测试了附录 [F](#A6
    "Appendix F Stochastic Rounding ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression") 中描述的随机舍入。最后，我们评估了与 STE 结合的 PV 调优，但配置相同。我们设置了子空间大小
    $\tau$，也称为信任比 [[75](#bib.bib75)]。表 [1](#S4.T1 "Table 1 ‣ 4.1 Evaluating quantized
    representations with finetuning ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression") 中的结果显示，PV-Tuning 一贯地找到了更好的量化模型，而 STE
    始终排在第二。我们通过将子空间更新与 STE 结合进一步探索，这在大多数（但不是所有）设置中略微改善了困惑度和准确性。'
- en: 4.3 Large-scale Evaluation & Discussion
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3 大规模评估与讨论
- en: Finally, we evaluate the resulting PV algorithm with a vector quantization backbone
    and KL objective on a range of popular LLM models. For this section, our goal
    is to evaluate our approach holistically for different models and target bit-widths,
    comparing against the best known baselines in common settings. To that end, we
    evaluate on Llama 2 & 3 [[63](#bib.bib63)], Mistral 7B [[31](#bib.bib31)] and
    Phi-3 Mini-4k-Instruct [[1](#bib.bib1)] at 1–2.5 bits per parameter.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对使用向量量化骨干和KL目标的PV算法在一系列流行的LLM模型上进行了评估。在这一部分中，我们的目标是全面评估我们的方法，比较不同模型和目标位宽，在常见设置中与已知最佳基线进行比较。为此，我们对Llama
    2 & 3 [[63](#bib.bib63)]、Mistral 7B [[31](#bib.bib31)]和Phi-3 Mini-4k-Instruct [[1](#bib.bib1)]在每个参数1–2.5位下进行了评估。
- en: We report perplexity on WikiText-2 [[42](#bib.bib42)] and C4 [[50](#bib.bib50)]
    validation sets, zero-shot accuracy on WinoGrande [[55](#bib.bib55)], PiQA [[62](#bib.bib62)],
    HellaSwag [[76](#bib.bib76)], ARC-easy and ARC-challenge [[10](#bib.bib10)] via
    the LM Eval Harness [[22](#bib.bib22)]. We follow the exact evaluation setup from
    GPTQ [[20](#bib.bib20)]. We compare against QuIP [[64](#bib.bib64)], BiLLM [[29](#bib.bib29)],
    PB-LLM [[57](#bib.bib57)], DB-LLM [[9](#bib.bib9)], AQLM [[19](#bib.bib19)], OneBit [[71](#bib.bib71)],
    QuIP# [[65](#bib.bib65)], the latter three using fine-tuning. For Llama 3, we
    use baselines from [[30](#bib.bib30)] and re-evaluate perplexity in our setup.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告了在WikiText-2 [[42](#bib.bib42)]和C4 [[50](#bib.bib50)]验证集上的困惑度，在WinoGrande [[55](#bib.bib55)]、PiQA [[62](#bib.bib62)]、HellaSwag [[76](#bib.bib76)]、ARC-easy和ARC-challenge [[10](#bib.bib10)]上的零-shot准确率，通过LM
    Eval Harness [[22](#bib.bib22)]。我们遵循了GPTQ [[20](#bib.bib20)]的确切评估设置。我们与QuIP [[64](#bib.bib64)]、BiLLM [[29](#bib.bib29)]、PB-LLM [[57](#bib.bib57)]、DB-LLM [[9](#bib.bib9)]、AQLM [[19](#bib.bib19)]、OneBit [[71](#bib.bib71)]、QuIP# [[65](#bib.bib65)]进行比较，后三者使用了微调。对于Llama
    3，我们使用了[[30](#bib.bib30)]中的基线，并在我们的设置中重新评估困惑度。
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.3 Large-scale Evaluation & Discussion ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    summarizes our findings: PV-tuning with vector quantization outperforms all known
    methods for 1- and 2-bit per weight. The closest competitors on Llama 2 are QuIP#,
    AQLM and OneBit, all of which use fine-tuning. The improvements on Llama 3 are
    also remarkable, as this model is notoriously hard to compress [[30](#bib.bib30)].
    We report additional evaluations in Appendix [L](#A12 "Appendix L Additional Details
    and Evaluations for Section 4.3 ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression").'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '表[2](#S4.T2 "Table 2 ‣ 4.3 Large-scale Evaluation & Discussion ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")总结了我们的发现：PV-tuning与向量量化在每个权重1位和2位的情况下优于所有已知方法。Llama
    2上最接近的竞争者是QuIP#、AQLM和OneBit，它们都使用了微调。Llama 3的改进也非常显著，因为这个模型在压缩方面非常困难 [[30](#bib.bib30)]。我们在附录[L](#A12
    "Appendix L Additional Details and Evaluations for Section 4.3 ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")中报告了更多评估结果。'
- en: 'Pareto-optimality. A key practical question concerns obtaining optimal quality
    for the target model size, where a smaller model compressed to 3-4 bits often
    dominates a larger model compressed to 1-bit. The best known Pareto-optimal bit-width
    for Llama 2 is 2.5 [[19](#bib.bib19)]: compressing a larger model to less than
    2.5 bits per weight is inferior to a smaller model quantized to the same total
    number of bytes. From this perspective, PV-tuning pushes the Pareto-optimal frontier
    for Llama 2 to 2.0 bits. This is easiest to see in Table [8](#A12.T8 "Table 8
    ‣ Appendix L Additional Details and Evaluations for Section 4.3 ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression"): a 2-bit 13B
    model outperforms any 7B quantization and is comparable with the 16-bit 7B model.
    The same holds for the 2-bit 70B model.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '帕累托最优性。一个关键的实际问题是如何在目标模型大小下获得最佳质量，其中一个压缩到3-4位的小模型通常优于一个压缩到1位的大模型。已知的Llama 2的最佳帕累托最优位宽是2.5 [[19](#bib.bib19)]：将较大模型压缩到每个权重少于2.5位不如将较小模型量化到相同总字节数。从这个角度来看，PV-tuning将Llama
    2的帕累托最优前沿推向2.0位。这在表[8](#A12.T8 "Table 8 ‣ Appendix L Additional Details and Evaluations
    for Section 4.3 ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for
    Extreme LLM Compression")中最容易看到：一个2位的13B模型优于任何7B量化，并且与16位的7B模型相当。2位70B模型也是如此。'
- en: Fine-tuning efficiency. One limitation of our algorithm is that it requires
    more compute and memory during the fine-tuning procedure. The 7B models can be
    fine-tuned on a single GPU, our 70B runs require a server with $8{\times}A100$
    or rely on RAM offloading. PV-Tuning shares this drawback with prior methods based
    on STE [[19](#bib.bib19), [65](#bib.bib65)], as both methods need gradients w.r.t.
    dequantized weights. Our longest training run took 2 days on 8 GPUs to outperform
    all baselines and 8 days to fully converge.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 微调效率。我们算法的一个限制是它在微调过程中需要更多的计算和内存。7B 模型可以在单个 GPU 上进行微调，而我们的 70B 运行需要一个配备 $8{\times}A100$
    的服务器或依赖 RAM 卸载。PV-Tuning 与基于 STE [[19](#bib.bib19), [65](#bib.bib65)] 的先前方法具有相同的缺点，因为这两种方法都需要对解量化权重进行梯度计算。我们最长的训练运行花费了
    8 个 GPU 2 天来超越所有基线，完全收敛则需 8 天。
- en: 'Inference speed. One advantage of PV-tuning is that it does not change the
    underlying compressed representation, so we can just simply existing high-performance
    inference kernels, but providing higher accuracy. Specifically, VQ+PV can reuse
    efficient kernels from [[19](#bib.bib19), [65](#bib.bib65)], while GPTQ+PV can
    use ExLlamaV2 kernels [[12](#bib.bib12)]. We illustrate speedups in Appendix [M](#A13
    "Appendix M Inference Speed with Vector Quantization Kernels ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression").'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 推理速度。PV-tuning 的一个优势在于它不会改变底层压缩表示，因此我们可以简单地使用现有的高性能推理内核，同时提供更高的准确性。具体来说，VQ+PV
    可以重用来自 [[19](#bib.bib19), [65](#bib.bib65)] 的高效内核，而 GPTQ+PV 可以使用 ExLlamaV2 内核
    [[12](#bib.bib12)]。我们在附录 [M](#A13 "附录 M 向量量化内核的推理速度 ‣ 附录 ‣ PV-Tuning：超越直通估计的极端
    LLM 压缩") 中说明了加速效果。
- en: 'Table 2: Quantized model perplexity on WikiText2$\downarrow$ mean higher /
    lower is better.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：WikiText2$\downarrow$ 上的量化模型困惑度，均值越高/低越好。
- en: '| Size | Method | Avg bits | Wiki2$\downarrow$ |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 尺寸 | 方法 | 平均比特 | Wiki2$\downarrow$ |'
- en: '| Llama 2 model family |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 模型系列 |'
- en: '| 7B | – | 16 | 5.12 | 6.63 | 64.80 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 7B | – | 16 | 5.12 | 6.63 | 64.80 |'
- en: '| BiLLM | 1.08 | 32.48 | 40.52 | 41.68 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM | 1.08 | 32.48 | 40.52 | 41.68 |'
- en: '| OneBit | 1.01 | 9.73 | 11.11 | 50.06 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| OneBit | 1.01 | 9.73 | 11.11 | 50.06 |'
- en: '| PV-Tuning | 1.02 | 8.28 | 10.37 | 50.66 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.02 | 8.28 | 10.37 | 50.66 |'
- en: '| AQLM | 2.02 | 6.64 | 8.56 | 56.47 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 2.02 | 6.64 | 8.56 | 56.47 |'
- en: '| QuIP# | 2.01 | 6.19 | 8.16 | 57.51 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# | 2.01 | 6.19 | 8.16 | 57.51 |'
- en: '| DB-LLM | 2.01 | 7.23 | 9.62 | 55.12 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| DB-LLM | 2.01 | 7.23 | 9.62 | 55.12 |'
- en: '| PV-Tuning | 2.02 | 5.84 | 7.62 | 61.35 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.02 | 5.84 | 7.62 | 61.35 |'
- en: '| 13B | – | 16 | 4.57 | 6.05 | 67.82 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 13B | – | 16 | 4.57 | 6.05 | 67.82 |'
- en: '| AQLM | 1.97 | 5.65 | 7.51 | 60.59 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 1.97 | 5.65 | 7.51 | 60.59 |'
- en: '| QuIP# | 2.01 | 5.35 | 7.20 | 61.45 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# | 2.01 | 5.35 | 7.20 | 61.45 |'
- en: '| DB-LLM | 2.01 | 6.19 | 8.38 | 59.41 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| DB-LLM | 2.01 | 6.19 | 8.38 | 59.41 |'
- en: '| PV-Tuning | 1.97 | 5.12 | 6.83 | 64.92 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.97 | 5.12 | 6.83 | 64.92 |'
- en: '| PV-Tuning | 2.19 | 5.05 | 6.74 | 66.05 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.19 | 5.05 | 6.74 | 66.05 |'
- en: '| 70B | – | 16 | 3.12 | 4.97 | 72.40 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 70B | – | 16 | 3.12 | 4.97 | 72.40 |'
- en: '| AQLM | 2.07 | 3.94 | 5.72 | 68.75 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 2.07 | 3.94 | 5.72 | 68.75 |'
- en: '| QuIP# | 2.01 | 3.91 | 5.71 | 68.60 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# | 2.01 | 3.91 | 5.71 | 68.60 |'
- en: '| DB-LLM | 2.01 | 4.64 | 6.77 | 65.83 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| DB-LLM | 2.01 | 4.64 | 6.77 | 65.83 |'
- en: '| PV-Tuning | 2.07 | 3.78 | 5.56 | 70.72 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.07 | 3.78 | 5.56 | 70.72 |'
- en: '| PV-Tuning | 1.14 | 5.52 | 7.50 | 64.58 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.14 | 5.52 | 7.50 | 64.58 |'
- en: '| Size | Method | Avg bits | Wiki2$\downarrow$ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 尺寸 | 方法 | 平均比特 | Wiki2$\downarrow$ |'
- en: '| Llama 3 model family |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Llama 3 模型系列 |'
- en: '| 8B | – | 16 | 5.54 | 7.10 | 68.61 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 8B | – | 16 | 5.54 | 7.10 | 68.61 |'
- en: '| BiLLM | 1.1 | 28.8 | 257 | 37.90 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM | 1.1 | 28.8 | 257 | 37.90 |'
- en: '| PV-Tuning | 1.01 | 11.17 | 11.63 | 50.01 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.01 | 11.17 | 11.63 | 50.01 |'
- en: '| QuIP | 2.01 | 76.95 | 98.47 | 36.8 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 2.01 | 76.95 | 98.47 | 36.8 |'
- en: '| DB-LLM | 2.01 | 12.77 | 14.82 | 51.8 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| DB-LLM | 2.01 | 12.77 | 14.82 | 51.8 |'
- en: '| PV-Tuning | 2.01 | 6.99 | 8.29 | 64.36 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.01 | 6.99 | 8.29 | 64.36 |'
- en: '| 70B | – | 16 | 2.59 | 5.78 | 75.37 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 70B | – | 16 | 2.59 | 5.78 | 75.37 |'
- en: '| BiLLM | 1.1 | 15.26 | 65.07 | 44.2 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM | 1.1 | 15.26 | 65.07 | 44.2 |'
- en: '| PV-Tuning | 1.01 | 8.67 | 9.68 | 51.47 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.01 | 8.67 | 9.68 | 51.47 |'
- en: '| QuIP | 2.00 | 11.63 | 18.54 | 48.71 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 2.00 | 11.63 | 18.54 | 48.71 |'
- en: '| PB-LLM | 2.00 | 10.33 | 28.89 | 46.04 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM | 2.00 | 10.33 | 28.89 | 46.04 |'
- en: '| PV-Tuning | 2.07 | 4.57 | 6.56 | 70.38 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.07 | 4.57 | 6.56 | 70.38 |'
- en: '| Mistral 7B v0.1 (A) and Phi 3 Mini-4k-Instruct (B) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Mistral 7B v0.1 (A) 和 Phi 3 Mini-4k-Instruct (B) |'
- en: '| $\underset{(A)}{\text{7B}}$ | – | 16 | 4.78 | 5.71 | 69.38 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| $\underset{(A)}{\text{7B}}$ | – | 16 | 4.78 | 5.71 | 69.38 |'
- en: '| QuIP# | 2.01 | 6.02 | 6.84 | 62.20 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# | 2.01 | 6.02 | 6.84 | 62.20 |'
- en: '| PV-Tuning | 2.01 | 5.29 | 6.17 | 66.32 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.01 | 5.29 | 6.17 | 66.32 |'
- en: '| $\underset{(B)}{\text{3.8B}}$ | – | 16 | 5.83 | 9.35 | 70.5 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| $\underset{(B)}{\text{3.8B}}$ | – | 16 | 5.83 | 9.35 | 70.5 |'
- en: '| AQLM | 2.03 | 8.85 | 12.19 | 60.4 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 2.03 | 8.85 | 12.19 | 60.4 |'
- en: '| PV-Tuning | 2.03 | 6.88 | 10.08 | 65.70 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.03 | 6.88 | 10.08 | 65.70 |'
- en: 5 Conclusions
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5 结论
- en: Limitations. We focus our effort on evaluating PV-Tuning with multiple setups
    and models, but we spent relatively little effort tuning our algorithm for each
    specific setup. For instance, we always use constant learning rate and $\tau$
    with no schedule, and always train on the same data. While this shows robustness
    of PV-Tuning, it also means that our results may be improved with better hyperparameters.
    Furthermore, the algorithm could achieve better accuracy by simply training longer
    and on more data.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 限制。我们将精力集中在评估多个设置和模型下的PV-Tuning上，但在为每个特定设置调整算法方面投入相对较少。例如，我们总是使用固定的学习率和$\tau$，且没有时间表，并且总是在相同的数据上进行训练。虽然这显示了PV-Tuning的鲁棒性，但也意味着我们的结果可能通过更好的超参数得到改进。此外，通过简单地延长训练时间和增加数据量，算法可能会实现更好的准确性。
- en: 'Future work. This work opens several new research directions. The first is
    about how to choose $\mathcal{S}^{k}$: while we found that a greedy strategy works
    in practice, there may be fundamentally better ways. Another direction is applying
    PV-Tuning to other quantization niches: our evaluation focuses on extreme weight-only
    quantization, but the proposed algorithm can be used in weight + activation setting
    or quantize vision models. Overall, PV-Tuning shows how an insight from optimization
    theory can improve LLM quantization and we are excited to see how this develops
    in future research.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 未来工作。该工作开启了几个新的研究方向。第一个是如何选择$\mathcal{S}^{k}$：虽然我们发现贪婪策略在实践中有效，但可能存在根本上更好的方法。另一个方向是将PV-Tuning应用于其他量化领域：我们的评估集中在极端的仅权重量化，但所提出的算法可以用于权重+激活设置或量化视觉模型。总体而言，PV-Tuning展示了优化理论中的见解如何改善LLM量化，我们期待看到这一领域在未来研究中的发展。
- en: 6 Acknowledgements
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6 致谢
- en: Authors would like to thank Vage Egiazarian, Andrei Panferov and Ruslan Svirschevski
    for their help and advice on AQLM codebase and running large-scale experiments.
    We also thank Philip Zmushko and Artem Fedorov for helpful discussions during
    the early stages of our research. Finally, we thank the open-source contributors
    from llamacpp⁶⁶6[https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)
    and the LocalLlama⁷⁷7[https://reddit.com/r/LocalLaMA](https://reddit.com/r/LocalLaMA)
    community for discussions and inspirations on practical use cases of quantized
    language models.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢Vage Egiazarian、Andrei Panferov和Ruslan Svirschevski对AQLM代码库和大规模实验的帮助和建议。我们还感谢Philip
    Zmushko和Artem Fedorov在我们研究初期的有益讨论。最后，我们感谢来自llamacpp⁶⁶6[https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)的开源贡献者和LocalLlama⁷⁷7[https://reddit.com/r/LocalLaMA](https://reddit.com/r/LocalLaMA)社区对量化语言模型实际应用案例的讨论和启发。
- en: References
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla,
    N. Bach, A. Bahree, A. Bakhtiari, H. Behl, A. Benhaim, M. Bilenko, J. Bjorck,
    S. Bubeck, M. Cai, C. C. T. Mendes, W. Chen, V. Chaudhary, P. Chopra, A. D. Giorno,
    G. de Rosa, M. Dixon, R. Eldan, D. Iter, A. Garg, A. Goswami, S. Gunasekar, E. Haider,
    J. Hao, R. J. Hewett, J. Huynh, M. Javaheripi, X. Jin, P. Kauffmann, N. Karampatziakis,
    D. Kim, M. Khademi, L. Kurilenko, J. R. Lee, Y. T. Lee, Y. Li, C. Liang, W. Liu,
    E. Lin, Z. Lin, P. Madan, A. Mitra, H. Modi, A. Nguyen, B. Norick, B. Patra, D. Perez-Becker,
    T. Portet, R. Pryzant, H. Qin, M. Radmilac, C. Rosset, S. Roy, O. Ruwase, O. Saarikivi,
    A. Saied, A. Salim, M. Santacroce, S. Shah, N. Shang, H. Sharma, X. Song, M. Tanaka,
    X. Wang, R. Ward, G. Wang, P. Witte, M. Wyatt, C. Xu, J. Xu, S. Yadav, F. Yang,
    Z. Yang, D. Yu, C. Zhang, C. Zhang, J. Zhang, L. L. Zhang, Y. Zhang, Y. Zhang,
    Y. Zhang, and X. Zhou. Phi-3 technical report: A highly capable language model
    locally on your phone, 2024.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla,
    N. Bach, A. Bahree, A. Bakhtiari, H. Behl, A. Benhaim, M. Bilenko, J. Bjorck,
    S. Bubeck, M. Cai, C. C. T. Mendes, W. Chen, V. Chaudhary, P. Chopra, A. D. Giorno,
    G. de Rosa, M. Dixon, R. Eldan, D. Iter, A. Garg, A. Goswami, S. Gunasekar, E.
    Haider, J. Hao, R. J. Hewett, J. Huynh, M. Javaheripi, X. Jin, P. Kauffmann, N.
    Karampatziakis, D. Kim, M. Khademi, L. Kurilenko, J. R. Lee, Y. T. Lee, Y. Li,
    C. Liang, W. Liu, E. Lin, Z. Lin, P. Madan, A. Mitra, H. Modi, A. Nguyen, B. Norick,
    B. Patra, D. Perez-Becker, T. Portet, R. Pryzant, H. Qin, M. Radmilac, C. Rosset,
    S. Roy, O. Ruwase, O. Saarikivi, A. Saied, A. Salim, M. Santacroce, S. Shah, N.
    Shang, H. Sharma, X. Song, M. Tanaka, X. Wang, R. Ward, G. Wang, P. Witte, M.
    Wyatt, C. Xu, J. Xu, S. Yadav, F. Yang, Z. Yang, D. Yu, C. Zhang, C. Zhang, J.
    Zhang, L. L. Zhang, Y. Zhang, Y. Zhang, Y. Zhang, 和 X. Zhou. Phi-3技术报告：一种高性能的手机本地语言模型，2024。'
- en: '[2] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. QSGD: Randomized
    quantization for communication-efficient stochastic gradient descent. In Conference
    on Neural Information Processing Systems (NeurIPS), 2017.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] D. Alistarh, D. Grubic, J. Li, R. Tomioka, 和 M. Vojnovic. QSGD：用于通信高效的随机化量化的随机梯度下降方法。在神经信息处理系统会议（NeurIPS），2017。'
- en: '[3] D. Alistarh, T. Hoefler, M. Johansson, S. Khirirat, N. Konstantinov, and
    C. Renggli. The convergence of sparsified gradient methods. In Advances in Neural
    Information Processing Systems, 2018.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] D. Alistarh, T. Hoefler, M. Johansson, S. Khirirat, N. Konstantinov, 和
    C. Renggli. 稀疏梯度方法的收敛性。发表于《神经信息处理系统进展》，2018。'
- en: '[4] A. Babenko and V. Lempitsky. Additive quantization for extreme vector compression.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 931–938, 2014.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] A. Babenko 和 V. Lempitsky. 极端向量压缩的加法量化。发表于《IEEE计算机视觉与模式识别会议论文集》，第931–938页，2014。'
- en: '[5] Y. Bengio, N. Léonard, and A. Courville. Estimating or propagating gradients
    through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432,
    2013.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Y. Bengio, N. Léonard, 和 A. Courville. 通过随机神经元估计或传播梯度以进行条件计算。arXiv 预印本
    arXiv:1308.3432, 2013。'
- en: '[6] A. Beznosikov, S. Horváth, P. Richtárik, and M. Safaryan. On biased compression
    for distributed learning. arXiv:2002.12410, 2020.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Beznosikov, S. Horváth, P. Richtárik, 和 M. Safaryan. 关于分布式学习的偏置压缩。arXiv:2002.12410,
    2020。'
- en: '[7] V. Boža. Fast and optimal weight update for pruned large language models,
    2024.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] V. Boža. 为剪枝的大型语言模型提供快速而优化的权重更新，2024。'
- en: '[8] J. Chee, Y. Cai, V. Kuleshov, and C. D. Sa. Quip: 2-bit quantization of
    large language models with guarantees, 2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Chee, Y. Cai, V. Kuleshov, 和 C. D. Sa. Quip: 具有保证的大型语言模型的 2 位量化，2023。'
- en: '[9] H. Chen, C. Lv, L. Ding, H. Qin, X. Zhou, Y. Ding, X. Liu, M. Zhang, J. Guo,
    X. Liu, and D. Tao. Db-llm: Accurate dual-binarization for efficient llms, 2024.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] H. Chen, C. Lv, L. Ding, H. Qin, X. Zhou, Y. Ding, X. Liu, M. Zhang, J.
    Guo, X. Liu, 和 D. Tao. Db-llm: 准确的双重二值化以提高效率的 llms，2024。'
- en: '[10] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
    and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning
    challenge. arXiv preprint arXiv:1803.05457, 2018.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
    和 O. Tafjord. 认为你已经解决了问答问题？试试 ARC，AI2 推理挑战。arXiv 预印本 arXiv:1803.05457, 2018。'
- en: '[11] T. Computer. Redpajama: an open dataset for training large language models,
    October 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] T. Computer. Redpajama: 一个用于训练大型语言模型的开放数据集，2023年10月。'
- en: '[12] E. contributors. ExLlamaV2\. A fast inference library for running LLMs
    locally on modern consumer-class GPUs. Open-source library developed by turboderp
    and controbutors.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] E. contributors. ExLlamaV2\. 一种用于在现代消费级 GPU 上本地运行 LLMs 的快速推理库。由 turboderp
    和贡献者开发的开源库。'
- en: '[13] M. Courbariaux, Y. Bengio, and J.-P. David. Training deep neural networks
    with low precision multiplications. arXiv preprint arXiv:1412.7024, 2014.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] M. Courbariaux, Y. Bengio, 和 J.-P. David. 使用低精度乘法训练深度神经网络。arXiv 预印本 arXiv:1412.7024,
    2014。'
- en: '[14] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from
    incomplete data via the em algorithm. Journal of the royal statistical society:
    series B (methodological), 39(1):1–22, 1977.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] A. P. Dempster, N. M. Laird, 和 D. B. Rubin. 通过 EM 算法从不完整数据中获得最大似然。皇家统计学会期刊：B系列（方法论），39(1):1–22,
    1977。'
- en: '[15] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit
    matrix multiplication for transformers at scale. Advances in Neural Information
    Processing Systems 35: Annual Conference on Neural Information Processing Systems
    2022, NeurIPS 2022, 2022.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] T. Dettmers, M. Lewis, Y. Belkada, 和 L. Zettlemoyer. LLM.int8(): 用于大规模变换器的
    8 位矩阵乘法。发表于《神经信息处理系统进展 35：神经信息处理系统年会 2022》，NeurIPS 2022，2022。'
- en: '[16] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar,
    S. Ashkboos, A. Borzunov, T. Hoefler, and D. Alistarh. Spqr: A sparse-quantized
    representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078,
    2023.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar,
    S. Ashkboos, A. Borzunov, T. Hoefler, 和 D. Alistarh. SPQR：一种稀疏量化表示，用于近乎无损的 llm
    权重压缩。arXiv 预印本 arXiv:2306.03078, 2023。'
- en: '[17] T. Dettmers and T. von Koeller. Accessible large language models via k-bit
    quantization for pytorch.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] T. Dettmers 和 T. von Koeller. 通过 k-bit 量化实现对 pytorch 的大型语言模型的访问。'
- en: '[18] T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference
    scaling laws. arXiv preprint arXiv:2212.09720, 2022.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] T. Dettmers 和 L. Zettlemoyer. 4 位精度的案例：k-bit 推理缩放法则。arXiv 预印本 arXiv:2212.09720,
    2022。'
- en: '[19] V. Egiazarian, A. Panferov, D. Kuznedelev, E. Frantar, A. Babenko, and
    D. Alistarh. Extreme compression of large language models via additive quantization.
    arXiv preprint arXiv:2401.06118, 2024.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] V. Egiazarian, A. Panferov, D. Kuznedelev, E. Frantar, A. Babenko, 和 D.
    Alistarh. 通过加法量化实现对大型语言模型的极端压缩。arXiv 预印本 arXiv:2401.06118, 2024。'
- en: '[20] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training
    quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323,
    2022.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] E. Frantar, S. Ashkboos, T. Hoefler 和 D. Alistarh。Gptq：生成预训练变换器的准确后训练量化。arXiv预印本arXiv:2210.17323，2022年。'
- en: '[21] Y. Freund and R. E. Schapire. Large margin classification using the perceptron
    algorithm. In Proceedings of the eleventh annual conference on Computational learning
    theory, pages 209–217, 1998.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. Freund 和 R. E. Schapire。使用感知机算法的大间隔分类。发表于第十一届计算学习理论年会论文集，页码209–217，1998年。'
- en: '[22] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding,
    J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite,
    B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation,
    Sept. 2021.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding,
    J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite,
    B. Wang, K. Wang 和 A. Zou。少样本语言模型评估框架，2021年9月。'
- en: '[23] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer. A
    survey of quantization methods for efficient neural network inference. arXiv preprint
    arXiv:2103.13630, 2021.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney 和 K. Keutzer。高效神经网络推断的量化方法综述。arXiv预印本arXiv:2103.13630，2021年。'
- en: '[24] R. M. Gower and P. Richtárik. Stochastic dual ascent for solving linear
    systems. arXiv:1512.06890, 2015.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] R. M. Gower 和 P. Richtárik。求解线性系统的随机对偶上升法。arXiv:1512.06890，2015年。'
- en: '[25] A. Griewank and A. Walther. Algorithm 799: revolve: an implementation
    of checkpointing for the reverse or adjoint mode of computational differentiation.
    ACM Transactions on Mathematical Software (TOMS), 26(1):19–45, 2000.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. Griewank 和 A. Walther。算法799：revolve：用于计算微分反向模式或伴随模式的检查点实现。ACM数学软件交易（TOMS），26(1)：19–45，2000年。'
- en: '[26] H. Guo, P. Greengard, E. P. Xing, and Y. Kim. Lq-lora: Low-rank plus quantized
    matrix decomposition for efficient language model finetuning, 2024.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] H. Guo, P. Greengard, E. P. Xing 和 Y. Kim。Lq-lora：用于高效语言模型微调的低秩加量化矩阵分解，2024年。'
- en: '[27] K. Helwegen, J. Widdicombe, L. Geiger, Z. Liu, K.-T. Cheng, and R. Nusselder.
    Latent weights do not exist: Rethinking binarized neural network optimization.
    Advances in neural information processing systems, 32, 2019.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] K. Helwegen, J. Widdicombe, L. Geiger, Z. Liu, K.-T. Cheng 和 R. Nusselder。潜在权重并不存在：重新思考二值化神经网络优化。神经信息处理系统进展，32，2019年。'
- en: '[28] G. Hinton. Neural networks for machine learning, coursera (video lectures).,
    2012.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] G. Hinton。机器学习中的神经网络，coursera（视频讲座），2012年。'
- en: '[29] W. Huang, Y. Liu, H. Qin, Y. Li, S. Zhang, X. Liu, M. Magno, and X. Qi.
    Billm: Pushing the limit of post-training quantization for llms. arXiv preprint
    arXiv:2402.04291, 2024.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] W. Huang, Y. Liu, H. Qin, Y. Li, S. Zhang, X. Liu, M. Magno 和 X. Qi。Billm：推动LLMs后训练量化的极限。arXiv预印本arXiv:2402.04291，2024年。'
- en: '[30] W. Huang, X. Ma, H. Qin, X. Zheng, C. Lv, H. Chen, J. Luo, X. Qi, X. Liu,
    and M. Magno. How good are low-bit quantized llama3 models? an empirical study,
    2024.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] W. Huang, X. Ma, H. Qin, X. Zheng, C. Lv, H. Chen, J. Luo, X. Qi, X. Liu
    和 M. Magno。低比特量化Llama3模型的表现如何？一项实证研究，2024年。'
- en: '[31] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l.
    Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv
    preprint arXiv:2310.06825, 2023.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D.
    d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier 等。Mistral 7b。arXiv预印本arXiv:2310.06825，2023年。'
- en: '[32] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford,
    D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts.
    arXiv preprint arXiv:2401.04088, 2024.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford,
    D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand 等。专家混合模型。arXiv预印本arXiv:2401.04088，2024年。'
- en: '[33] S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney,
    and K. Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629,
    2023.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney
    和 K. Keutzer。Squeezellm：稠密和稀疏量化。arXiv预印本arXiv:2306.07629，2023年。'
- en: '[34] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. International
    Conference on Learning Representations (ICLR), 2015.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] D. P. Kingma 和 J. Ba。Adam：一种随机优化方法。国际学习表征会议（ICLR），2015年。'
- en: '[35] D. Kozak, S. Becker, A. Doostan, and L. Tenorio. Stochastic subspace descent.
    arXiv preprint arXiv:1904.01145, 2019.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] D. Kozak, S. Becker, A. Doostan 和 L. Tenorio。随机子空间下降法。arXiv预印本arXiv:1904.01145，2019年。'
- en: '[36] C. Lee, J. Jin, T. Kim, H. Kim, and E. Park. Owq: Outlier-aware weight
    quantization for efficient fine-tuning and inference of large language models,
    2024.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] C. Lee, J. Jin, T. Kim, H. Kim 和 E. Park。Owq：用于大规模语言模型高效微调和推断的异常感知权重量化，2024年。'
- en: '[37] S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li, A. Paszke,
    J. Smith, B. Vaughan, P. Damania, and S. Chintala. Pytorch distributed: Experiences
    on accelerating data parallel training, 2020.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li, A. Paszke,
    J. Smith, B. Vaughan, P. Damania, 和 S. Chintala. PyTorch分布式: 加速数据并行训练的经验，2020年。'
- en: '[38] Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. Lee.
    Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463,
    2023.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, 和 Y. T. Lee.
    教科书就是你所需的ii: phi-1.5技术报告。arXiv预印本 arXiv:2309.05463，2023年。'
- en: '[39] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han. Awq: Activation-aware
    weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978,
    2023.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, 和 S. Han. Awq: 激活感知权重量化用于大语言模型压缩与加速。arXiv预印本
    arXiv:2306.00978，2023年。'
- en: '[40] Z.-Q. Luo and P. Tseng. On the convergence of the coordinate descent method
    for convex differentiable minimization. Journal of Optimization Theory and Applications,
    72(1):7–35, 1992.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Z.-Q. Luo 和 P. Tseng. 关于凸可微最小化的坐标下降法的收敛性。优化理论与应用杂志，72(1):7–35，1992年。'
- en: '[41] S. Ma, H. Wang, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, R. Wang, J. Xue,
    and F. Wei. The era of 1-bit llms: All large language models are in 1.58 bits,
    2024.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Ma, H. Wang, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, R. Wang, J.
    Xue, 和 F. Wei. 1-bit大语言模型的时代: 所有大语言模型均在1.58位，2024年。'
- en: '[42] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture
    models. arXiv preprint arXiv:1609.07843, 2016.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] S. Merity, C. Xiong, J. Bradbury, 和 R. Socher. 指针哨兵混合模型。arXiv预印本 arXiv:1609.07843，2016年。'
- en: '[43] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y. Y. Wong, Z. Chen,
    D. Arfeen, R. Abhyankar, and Z. Jia. Specinfer: Accelerating generative llm serving
    with speculative inference and token tree verification, 2023.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y. Y. Wong, Z. Chen,
    D. Arfeen, R. Abhyankar, 和 Z. Jia. Specinfer: 通过投机推断和令牌树验证加速生成大语言模型服务，2023年。'
- en: '[44] R. Mises and H. Pollaczek-Geiringer. Praktische verfahren der gleichungsauflösung.
    ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik
    und Mechanik, 9(1):58–77, 1929.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] R. Mises 和 H. Pollaczek-Geiringer. 实用方程解法。ZAMM应用数学与力学杂志/应用数学与力学期刊，9(1):58–77，1929年。'
- en: '[45] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort.
    Up or down? Adaptive rounding for post-training quantization. In International
    Conference on Machine Learning (ICML), 2020.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, 和 T. Blankevoort. 向上还是向下？后训练量化的自适应舍入。发表于国际机器学习会议（ICML），2020年。'
- en: '[46] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee. nuQmm: Quantized
    matmul for efficient inference of large-scale generative language models. arXiv
    preprint arXiv:2206.09557, 2022.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, 和 D. Lee. nuQmm: 用于大规模生成语言模型高效推断的量化矩阵乘法。arXiv预印本
    arXiv:2206.09557，2022年。'
- en: '[47] J. Park and E. K. Ryu. Exact optimal accelerated complexity for fixed-point
    iterations. In Proceedings of the 39th International Conference on Machine Learning,
    2022.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] J. Park 和 E. K. Ryu. 固定点迭代的精确最优加速复杂度。发表于第39届国际机器学习大会论文集，2022年。'
- en: '[48] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison,
    A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch:
    An imperative style, high-performance deep learning library. In Advances in Neural
    Information Processing Systems (NeurIPS). Neural Information Processing Systems
    Foundation, 2019.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M.
    Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, 和 S. Chintala.
    PyTorch: 一种命令式风格的高性能深度学习库。发表于《神经信息处理系统进展》（NeurIPS）。神经信息处理系统基金会，2019年。'
- en: '[49] A. Polino, R. Pascanu, and D. Alistarh. Model compression via distillation
    and quantization. arXiv preprint arXiv:1802.05668, 2018.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] A. Polino, R. Pascanu, 和 D. Alistarh. 通过蒸馏和量化进行模型压缩。arXiv预印本 arXiv:1802.05668，2018年。'
- en: '[50] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li, and P. Liu. Exploring the limits of transfer learning with a unified text-to-text
    transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li, 和 P. Liu. 探索统一文本到文本变换器的迁移学习极限。机器学习研究杂志，21(140):1–67，2020年。'
- en: '[51] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: memory optimizations
    toward training trillion parameter models. In Proceedings of the International
    Conference for High Performance Computing, Networking, Storage and Analysis, SC
    ’20\. IEEE Press, 2020.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] S. Rajbhandari, J. Rasley, O. Ruwase, 和 Y. He. Zero: 训练万亿参数模型的内存优化。在国际高性能计算、网络、存储与分析会议上，SC
    ’20。IEEE Press, 2020.'
- en: '[52] J. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase, S. Yang, M. Zhang,
    D. Li, and Y. He. Zero-offload: Democratizing billion-scale model training, 2021.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] J. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase, S. Yang, M. Zhang,
    D. Li, 和 Y. He. Zero-offload: 普及亿级模型训练，2021.'
- en: '[53] P. Richtárik and M. Takáč. Parallel coordinate descent methods for big
    data optimization. Mathematical Programming, 156(1-2):433–484, 2016.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] P. Richtárik 和 M. Takáč. 大数据优化的并行坐标下降方法。Mathematical Programming, 156(1-2):433–484,
    2016.'
- en: '[54] F. Rosenblatt. The perceptron - a perceiving and recognizing automaton.
    Technical Report 85-460-1, Cornell Aeronautical Laboratory, Ithaca, New York,
    January 1957.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] F. Rosenblatt. 感知机 - 一个感知和识别的自动机。技术报告 85-460-1, 康奈尔航空实验室, 伊萨卡, 纽约, 1957年1月.'
- en: '[55] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: an
    adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99–106, 2021.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] K. Sakaguchi, R. L. Bras, C. Bhagavatula, 和 Y. Choi. Winogrande: 大规模对抗性
    Winograd 语料库挑战。Commun. ACM, 64(9):99–106, 2021.'
- en: '[56] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné,
    A. S. Luccioni, F. Yvon, M. Gallé, et al. Bloom: A 176b-parameter open-access
    multilingual language model. arXiv preprint arXiv:2211.05100, 2022.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné,
    A. S. Luccioni, F. Yvon, M. Gallé, 等. Bloom: 一个 176b-参数的开放多语言模型。arXiv 预印本 arXiv:2211.05100,
    2022.'
- en: '[57] Y. Shang, Z. Yuan, Q. Wu, and Z. Dong. Pb-llm: Partially binarized large
    language models, 2023.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Y. Shang, Z. Yuan, Q. Wu, 和 Z. Dong. Pb-llm: 部分二值化的大型语言模型，2023.'
- en: '[58] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang, P. Gao, Y. Qiao,
    and P. Luo. Omniquant: Omnidirectionally calibrated quantization for large language
    models. arXiv preprint arXiv:2308.13137, 2023.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang, P. Gao, Y.
    Qiao, 和 P. Luo. Omniquant: 面向大型语言模型的全方向标定量化。arXiv 预印本 arXiv:2308.13137, 2023.'
- en: '[59] A. Shekhovtsov and V. Yanush. Reintroducing straight-through estimators
    as principled methods for stochastic binary networks. In DAGM German Conference
    on Pattern Recognition, pages 111–126\. Springer, 2021.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] A. Shekhovtsov 和 V. Yanush. 重新引入直通估计器作为随机二值网络的原则性方法。在 DAGM 德国模式识别会议上，页
    111–126。Springer, 2021.'
- en: '[60] M. Spallanzani, G. P. Leonardi, and L. Benini. Training quantised neural
    networks with ste variants: the additive noise annealing algorithm. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 470–479,
    2022.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] M. Spallanzani, G. P. Leonardi, 和 L. Benini. 使用 STE 变体训练量化神经网络：加性噪声退火算法。在
    IEEE/CVF 计算机视觉与模式识别会议上，页 470–479, 2022.'
- en: '[61] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter. A simple and effective pruning
    approach for large language models, 2024.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] M. Sun, Z. Liu, A. Bair, 和 J. Z. Kolter. 一种简单而有效的大型语言模型剪枝方法，2024.'
- en: '[62] S. Tata and J. M. Patel. PiQA: An algebra for querying protein data sets.
    In International Conference on Scientific and Statistical Database Management,
    2003.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] S. Tata 和 J. M. Patel. PiQA: 用于查询蛋白质数据集的代数。在国际科学与统计数据库管理会议上，2003.'
- en: '[63] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation
    language models. arXiv preprint arXiv:2302.13971, 2023.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, 等. Llama: 开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971,
    2023.'
- en: '[64] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, and C. De Sa. Quip#: Even better
    llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint
    arXiv:2402.04396, 2024.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, 和 C. De Sa. Quip#: 具有 Hadamard
    不一致性和格子码本的更佳 LLM 量化。arXiv 预印本 arXiv:2402.04396, 2024.'
- en: '[65] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, and C. D. Sa. Quip#: Even better
    llm quantization with hadamard incoherence and lattice codebooks, 2024.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, 和 C. D. Sa. Quip#: 具有 Hadamard
    不一致性和格子码本的更佳 LLM 量化，2024.'
- en: '[66] M. van Baalen, A. Kuzmin, M. Nagel, P. Couperus, C. Bastoul, E. Mahurin,
    T. Blankevoort, and P. Whatmough. Gptvq: The blessing of dimensionality for llm
    quantization. arXiv preprint arXiv:2402.15319, 2024.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] M. van Baalen, A. Kuzmin, M. Nagel, P. Couperus, C. Bastoul, E. Mahurin,
    T. Blankevoort, 和 P. Whatmough. Gptvq: 维度的祝福用于 LLM 量化。arXiv 预印本 arXiv:2402.15319,
    2024.'
- en: '[67] A. van den Oord, O. Vinyals, and k. kavukcuoglu. Neural discrete representation
    learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
    and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30\.
    Curran Associates, Inc., 2017.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] A. van den Oord, O. Vinyals, 和 k. kavukcuoglu. 神经离散表示学习。在 I. Guyon, U.
    V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, 和 R. Garnett 编辑的《神经信息处理系统进展》，第
    30 卷。Curran Associates, Inc., 2017 年。'
- en: '[68] H. Vanholder. Efficient inference with TensorRT. NVIDIA GTC On-Demand.
    Slides available at https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=23425-efficient+inference+with+tensorrt,
    2017.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] H. Vanholder. 使用 TensorRT 的高效推断。NVIDIA GTC 按需视频。幻灯片可见于 [https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=23425-efficient+inference+with+tensorrt](https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=23425-efficient+inference+with+tensorrt)，2017
    年。'
- en: '[69] H. Wang, S. Ma, L. Dong, S. Huang, H. Wang, L. Ma, F. Yang, R. Wang, Y. Wu,
    and F. Wei. Bitnet: Scaling 1-bit transformers for large language models. arXiv
    preprint arXiv:2310.11453, 2023.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] H. Wang, S. Ma, L. Dong, S. Huang, H. Wang, L. Ma, F. Yang, R. Wang, Y.
    Wu, 和 F. Wei. Bitnet：为大型语言模型扩展 1-bit 变换器。arXiv 预印本 arXiv:2310.11453，2023 年。'
- en: '[70] B. Widrow and M. A. Lehr. 30 years of adaptive neural networks: perceptron,
    madaline, and backpropagation. Proc. IEEE, 78:1415–1442, 1990.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] B. Widrow 和 M. A. Lehr. 30 年的自适应神经网络：感知机、Madaline 和反向传播。IEEE 会议论文集，78:1415–1442，1990
    年。'
- en: '[71] Y. Xu, X. Han, Z. Yang, S. Wang, Q. Zhu, Z. Liu, W. Liu, and W. Che. Onebit:
    Towards extremely low-bit large language models, 2024.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Y. Xu, X. Han, Z. Yang, S. Wang, Q. Zhu, Z. Liu, W. Liu, 和 W. Che. Onebit：面向极低比特的大型语言模型，2024
    年。'
- en: '[72] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant:
    Efficient and affordable post-training quantization for large-scale transformers.
    arXiv preprint arXiv:2206.01861, 2022.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, 和 Y. He. Zeroquant：大规模变换器的高效且经济的后训练量化。arXiv
    预印本 arXiv:2206.01861，2022 年。'
- en: '[73] Z. Yao, X. Wu, C. Li, S. Youn, and Y. He. Exploring post-training quantization
    in llms from comprehensive study to low rank compensation. In Proceedings of the
    AAAI Conference on Artificial Intelligence, volume 38, pages 19377–19385, 2024.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Z. Yao, X. Wu, C. Li, S. Youn, 和 Y. He. 从全面研究到低秩补偿：探索 llms 的后训练量化。在《人工智能
    AAAI 会议论文集》第 38 卷，第 19377–19385 页，2024 年。'
- en: '[74] P. Yin, J. Lyu, S. Zhang, S. Osher, Y. Qi, and J. Xin. Understanding straight-through
    estimator in training activation quantized neural nets. arXiv preprint arXiv:1903.05662,
    2019.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] P. Yin, J. Lyu, S. Zhang, S. Osher, Y. Qi, 和 J. Xin. 理解训练激活量化神经网络中的直通估计器。arXiv
    预印本 arXiv:1903.05662，2019 年。'
- en: '[75] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song,
    J. Demmel, K. Keutzer, and C.-J. Hsieh. Large batch optimization for deep learning:
    Training bert in 76 minutes. In International Conference on Learning Representations
    (ICLR), 2020.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song,
    J. Demmel, K. Keutzer, 和 C.-J. Hsieh. 深度学习的大批量优化：在 76 分钟内训练 BERT。在《国际学习表示会议（ICLR）》，2020
    年。'
- en: '[76] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag:
    Can a machine really finish your sentence? In A. Korhonen, D. R. Traum, and L. Màrquez,
    editors, Proceedings of the 57th Conference of the Association for Computational
    Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long
    Papers, pages 4791–4800\. Association for Computational Linguistics, 2019.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, 和 Y. Choi. Hellaswag：机器真的能完成你的句子吗？在
    A. Korhonen, D. R. Traum, 和 L. Màrquez 编辑的《第 57 届计算语言学协会会议论文集，ACL 2019》，意大利佛罗伦萨，2019
    年 7 月 28 日至 8 月 2 日，第 1 卷：长篇论文，第 4791–4800 页。计算语言学协会，2019 年。'
- en: '[77] C. Zhang, J. Cheng, G. A. Constantinides, and Y. Zhao. Lqer: Low-rank
    quantization error reconstruction for llms, 2024.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] C. Zhang, J. Cheng, G. A. Constantinides, 和 Y. Zhao. Lqer：针对 llms 的低秩量化误差重建，2024
    年。'
- en: '[78] P. Zhang, G. Zeng, T. Wang, and W. Lu. Tinyllama: An open-source small
    language model, 2024.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] P. Zhang, G. Zeng, T. Wang, 和 W. Lu. Tinyllama：一个开源的小型语言模型，2024 年。'
- en: '[79] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models.
    arXiv preprint arXiv:2205.01068, 2022.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin, 等. Opt：开放的预训练变换器语言模型。arXiv 预印本 arXiv:2205.01068，2022
    年。'
- en: Appendix
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Contents
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 目录
- en: '[1 Introduction](#S1 "In PV-Tuning: Beyond Straight-Through Estimation for
    Extreme LLM Compression")'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 Introduction](#S1 "在 PV-Tuning：超越直通估计以实现极端 LLM 压缩")'
- en: '[2 Background](#S2 "In PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression")'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 Background](#S2 "在 PV-Tuning：超越直通估计以实现极端 LLM 压缩")'
- en: '[3 Fine-Tuning Quantized Models](#S3 "In PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 微调量化模型](#S3 "在 PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[3.1 Problem description](#S3.SS1 "In 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1 问题描述](#S3.SS1 "在 3 微调量化模型中 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[3.2 Linearized V step & gradient-based discrete updates](#S3.SS2 "In 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression")'
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2 线性化 V 步骤与基于梯度的离散更新](#S3.SS2 "在 3 微调量化模型中 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[3.3 Linearized subspace V step](#S3.SS3 "In 3 Fine-Tuning Quantized Models
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.3 线性化子空间 V 步骤](#S3.SS3 "在 3 微调量化模型中 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[3.4 Implementation details](#S3.SS4 "In 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.4 实施细节](#S3.SS4 "在 3 微调量化模型中 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[4 Experiments](#S4 "In PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression")'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 实验](#S4 "在 PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[4.1 Evaluating quantized representations with finetuning](#S4.SS1 "In 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1 评估量化表示与微调](#S4.SS1 "在 4 实验中 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[4.2 Evaluating Fine-tuning Algorithms](#S4.SS2 "In 4 Experiments ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2 评估微调算法](#S4.SS2 "在 4 实验中 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[4.3 Large-scale Evaluation & Discussion](#S4.SS3 "In 4 Experiments ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3 大规模评估与讨论](#S4.SS3 "在 4 实验中 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[5 Conclusions](#S5 "In PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression")'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 结论](#S5 "在 PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[6 Acknowledgements](#S6 "In PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6 致谢](#S6 "在 PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[A Proofs](#A1 "In Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A 证明](#A1 "在附录 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[A.1 Proof of Theorem 3.1](#A1.SS1 "In Appendix A Proofs ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.1 定理 3.1 的证明](#A1.SS1 "在附录 A 证明 ‣ 附录 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[A.2 Proof of Lemma 3.2](#A1.SS2 "In Appendix A Proofs ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.2 引理 3.2 的证明](#A1.SS2 "在附录 A 证明 ‣ 附录 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[A.3 Proof of Lemma 3.3](#A1.SS3 "In Appendix A Proofs ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.3 引理 3.3 的证明](#A1.SS3 "在附录 A 证明 ‣ 附录 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[B Approximate PV Algorithm](#A2 "In Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B 近似 PV 算法](#A2 "在附录 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[B.1 Approximate V step, variant 1 (non-accelerated)](#A2.SS1 "In Appendix
    B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B.1 近似 V 步骤，变体 1（未加速）](#A2.SS1 "在附录 B 近似 PV 算法 ‣ 附录 ‣ PV 调优：超越直接估计用于极端 LLM
    压缩")'
- en: '[B.2 Approximate V step, variant 2 (accelerated)](#A2.SS2 "In Appendix B Approximate
    PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression")'
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B.2 近似 V 步骤，变体 2（加速）](#A2.SS2 "在附录 B 近似 PV 算法 ‣ 附录 ‣ PV 调优：超越直接估计用于极端 LLM
    压缩")'
- en: '[C Generalization to Other Quantization Algorithms](#A3 "In Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C 对其他量化算法的推广](#A3 "在附录 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[D Efficient Linearized V Step for Vector Quantization](#A4 "In Appendix ‣
    PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D 向量量化的高效线性化 V 步骤](#A4 "在附录 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[E Straight-through Gradient Estimation](#A5 "In Appendix ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E 直接梯度估计](#A5 "在附录 ‣ PV 调优：超越直接估计用于极端 LLM 压缩")'
- en: '[F Stochastic Rounding](#A6 "In Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[F 随机舍入](#A6 "在附录 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩")'
- en: '[G On $L$-smoothness of LLM Objective in Sparse Subspaces](#A7 "In Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[G 在稀疏子空间中的 LLM 目标的 $L$-光滑性](#A7 "在附录 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩")'
- en: '[H Calibration Data Matters](#A8 "In Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H 校准数据的重要性](#A8 "在附录 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩")'
- en: '[I Additional Engineering Considerations](#A9 "In Appendix ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[I 附加工程考虑因素](#A9 "在附录 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩")'
- en: '[J Additional Details for Section 4.1](#A10 "In Appendix ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[附录 J 第 4.1 节的额外细节](#A10 "在附录 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩")'
- en: '[K Additional Details for Section 4.2](#A11 "In Appendix ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[K 第 4.2 节的额外细节](#A11 "在附录 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩")'
- en: '[L Additional Details and Evaluations for Section 4.3](#A12 "In Appendix ‣
    PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[L 第 4.3 节的额外细节和评估](#A12 "在附录 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩")'
- en: '[M Inference Speed with Vector Quantization Kernels](#A13 "In Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M 使用向量量化内核的推理速度](#A13 "在附录 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩")'
- en: '[N The Choice of the Initial Point $x^{0}$](#A14 "In Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[N 初始点 $x^{0}$ 的选择](#A14 "在附录 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩")'
- en: '[N.1 Clipping of $x^{\star}$](#A14.SS1 "In Appendix N The Choice of the Initial
    Point 𝑥⁰ ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression")'
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[N.1 对 $x^{\star}$ 的剪辑](#A14.SS1 "在附录 N 初始点 𝑥⁰ 的选择 ‣ 附录 ‣ PV-Tuning: 超越直通估计的极端
    LLM 压缩")'
- en: '[N.2 Random $x^{0}\in\mathbb{R}^{d}_{c}$](#A14.SS2 "In Appendix N The Choice
    of the Initial Point 𝑥⁰ ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[N.2 随机 $x^{0}\in\mathbb{R}^{d}_{c}$](#A14.SS2 "在附录 N 初始点 𝑥⁰ 的选择 ‣ 附录 ‣ PV-Tuning:
    超越直通估计的极端 LLM 压缩")'
- en: '[O Small-Scale Experiments and Interpretation](#A15 "In Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[O 小规模实验与解释](#A15 "在附录 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩")'
- en: '[O.1 Objective function](#A15.SS1 "In Appendix O Small-Scale Experiments and
    Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for
    Extreme LLM Compression")'
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[O.1 目标函数](#A15.SS1 "在附录 O 小规模实验与解释 ‣ 附录 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩")'
- en: '[O.2 Tiny-scale experiments ($d=6$)](#A15.SS2 "In Appendix O Small-Scale Experiments
    and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[O.2 微小规模实验 ($d=6$)](#A15.SS2 "在附录 O 小规模实验与解释 ‣ 附录 ‣ PV-Tuning: 超越直通估计的极端 LLM
    压缩")'
- en: '[O.3 Small-scale experiments ($d=100$)](#A15.SS3 "In Appendix O Small-Scale
    Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")'
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[O.3 小规模实验 ($d=100$)](#A15.SS3 "在附录 O 小规模实验与解释 ‣ 附录 ‣ PV-Tuning: 超越直通估计的极端
    LLM 压缩")'
- en: '[O.4 Linearized PV](#A15.SS4 "In Appendix O Small-Scale Experiments and Interpretation
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[O.4 线性化 PV](#A15.SS4 "在附录 O 小规模实验与解释 ‣ 附录 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩")'
- en: '[O.5 Linearized PV + sparse updates](#A15.SS5 "In Appendix O Small-Scale Experiments
    and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[O.5 线性化 PV + 稀疏更新](#A15.SS5 "在附录 O 小规模实验与解释 ‣ 附录 ‣ PV-Tuning: 超越直通估计的极端 LLM
    压缩")'
- en: '[P PV^+ Algorithm](#A16 "In Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[P PV^+ 算法](#A16 "在附录 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩")'
- en: '[Q Broader Impact](#A17 "In Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Q 更广泛的影响](#A17 "在附录 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩")'
- en: Appendix A Proofs
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 A 证明
- en: 'A.1 Proof of Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1 (Convergence of the
    PV method). ‣ 3.1 Problem description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1 定理 [3.1](#S3.Thmtheorem1 "定理 3.1（PV 方法的收敛性）。 ‣ 3.1 问题描述 ‣ 3 精细调优量化模型 ‣ PV
    调优：超越直通估计以进行极端 LLM 压缩")
- en: 'Part (i): First, by assumption, we know that $x^{0}\in\mathbb{R}^{d}_{\leq
    c}$. The claim now follows by induction.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 第 (i) 部分：首先，根据假设，我们知道 $x^{0}\in\mathbb{R}^{d}_{\leq c}$。现在通过归纳法可以得出结论。
- en: 'Part (ii): Since'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 第 (ii) 部分：由于
- en: '|  | $y^{k}=\arg\min_{y\in\mathbb{R}^{d}}\{\phi(y)\;:\;P(y)\supseteq P(x^{k})\}$
    |  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | $y^{k}=\arg\min_{y\in\mathbb{R}^{d}}\{\phi(y)\;:\;P(y)\supseteq P(x^{k})\}$
    |  |'
- en: and because $y=x^{k}$. Further, since
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 并且因为 $y=x^{k}$。进一步地，因为
- en: '|  | $x^{k+1}=\arg\min_{x\in\mathbb{R}^{d}}\{\phi(x)\;:\;V(x)\subseteq V(y^{k})\}$
    |  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  | $x^{k+1}=\arg\min_{x\in\mathbb{R}^{d}}\{\phi(x)\;:\;V(x)\subseteq V(y^{k})\}$
    |  |'
- en: and because $x=y^{k}$. In summary,
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 并且因为 $x=y^{k}$。总结，
- en: '|  | $\phi(x^{k+1})\leq\phi(y^{k})\leq\phi(x^{k}).$ |  |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '|  | $\phi(x^{k+1})\leq\phi(y^{k})\leq\phi(x^{k}).$ |  |'
- en: 'Part (iii): In view of part (ii), the sequence $\{\phi(x^{k})\}_{k=0}^{\infty}$
    is non-increasing. By assumption, it is bounded below. Hence, it converges to
    its infimum:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 第 (iii) 部分：考虑第 (ii) 部分，序列 $\{\phi(x^{k})\}_{k=0}^{\infty}$ 是非递增的。根据假设，它是有下界的。因此，它收敛到它的下确界：
- en: '|  | $\lim_{k\to\infty}\phi(x^{k})=\inf_{k\in\{0,1,\dots\}}\phi(x^{k}).$ |  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|  | $\lim_{k\to\infty}\phi(x^{k})=\inf_{k\in\{0,1,\dots\}}\phi(x^{k}).$ |  |'
- en: 'A.2 Proof of Lemma [3.2](#S3.Thmtheorem2 "Lemma 3.2\. ‣ 3.2 Linearized V step
    & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2 引理 [3.2](#S3.Thmtheorem2 "引理 3.2。 ‣ 3.2 线性化 V 步骤与基于梯度的离散更新 ‣ 3 精细调优量化模型
    ‣ PV 调优：超越直通估计以进行极端 LLM 压缩")
- en: '|  | $\displaystyle M_{V,\widetilde{\phi}_{y}}(y)$ |  |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle M_{V,\widetilde{\phi}_{y}}(y)$ |  |'
- en: '|  |  | $\displaystyle=$ |  |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ |  |'
- en: '|  |  | $\displaystyle=$ |  |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ |  |'
- en: '|  |  | $\displaystyle=$ |  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ |  |'
- en: '|  |  | $\displaystyle=$ |  |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ |  |'
- en: '|  |  | $\displaystyle=$ |  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ |  |'
- en: '|  |  | $\displaystyle=$ |  |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ |  |'
- en: 'A.3 Proof of Lemma [3.3](#S3.Thmtheorem3 "Lemma 3.3 (Monotonicity). ‣ 3.2 Linearized
    V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")'
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3 引理 [3.3](#S3.Thmtheorem3 "引理 3.3（单调性）。 ‣ 3.2 线性化 V 步骤与基于梯度的离散更新 ‣ 3 精细调优量化模型
    ‣ PV 调优：超越直通估计以进行极端 LLM 压缩")
- en: First, note that
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，注意到
- en: '|  | $\displaystyle\phi(\hat{x})$ |  | (9) |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi(\hat{x})$ |  | (9) |'
- en: '|  |  | $\displaystyle\overset{\text{Lemma}~{}\ref{lem:prox}}{=}$ |  |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{\text{引理}~{}\ref{lem:prox}}{=}$ |  |'
- en: '|  |  | $\displaystyle\overset{\eqref{eq:09u0fd9hfd}}{=}$ |  |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{\eqref{eq:09u0fd9hfd}}{=}$ |  |'
- en: '|  |  | $\displaystyle\overset{\eqref{eq:09-98u98yfhd}}{=}$ |  |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{\eqref{eq:09-98u98yfhd}}{=}$ |  |'
- en: 'Since $y\in\mathbb{R}^{d}_{\leq c}$, we can bound the last expression in ([9](#A1.E9
    "Equation 9 ‣ A.3 Proof of Lemma 3.3 ‣ Appendix A Proofs ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) from below via'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $y\in\mathbb{R}^{d}_{\leq c}$，我们可以通过以下方式从下界对 ([9](#A1.E9 "公式 9 ‣ A.3 引理 3.3
    的证明 ‣ 附录 A 证明 ‣ 附录 ‣ PV 调优：超越直通估计以进行极端 LLM 压缩")) 的最后一个表达式进行界限：
- en: '|  | $\displaystyle\phi(\hat{x})$ |  |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi(\hat{x})$ |  |'
- en: '|  |  | $\displaystyle\overset{\eqref{eq:09u0fd9hfd}}{=}$ |  |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{\eqref{eq:09u0fd9hfd}}{=}$ |  |'
- en: Finally, since $x=y$, we can upper bound the same expression via
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，因为 $x=y$，我们可以通过以下方式对相同的表达式进行上界：
- en: '|  | $\displaystyle\phi(\hat{x})$ |  |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi(\hat{x})$ |  |'
- en: '|  |  | $\displaystyle=$ |  |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ |  |'
- en: Appendix B Approximate PV Algorithm
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 B 近似 PV 算法
- en: We now introduce the pseudocode of an approximate PV meta-algorithm; the idea
    is to replace the P and V steps with some approximate computations to be defined
    later.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在介绍一个近似 PV 元算法的伪代码；其思想是用一些待后续定义的近似计算来替代 P 和 V 步骤。
- en: Algorithm 4 Approximate PV Algorithm
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 4 近似 PV 算法
- en: '1:  Parameters: starting point $x^{0}\in\mathbb{R}^{d}_{\leq c}$ (for example,
    we can use the method from Section [B.1](#A2.SS1 "B.1 Approximate V step, variant
    1 (non-accelerated) ‣ Appendix B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression") or the method
    from Section [B.2](#A2.SS2 "B.2 Approximate V step, variant 2 (accelerated) ‣
    Appendix B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"))5:  end for'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  参数：起始点 $x^{0}\in\mathbb{R}^{d}_{\leq c}$（例如，我们可以使用 [B.1](#A2.SS1 "B.1 Approximate
    V step, variant 1 (non-accelerated) ‣ Appendix B Approximate PV Algorithm ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    节中的方法或 [B.2](#A2.SS2 "B.2 Approximate V step, variant 2 (accelerated) ‣ Appendix
    B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression") 节中的方法）5:  结束 for'
- en: Next, we describe two new approximations of the V step.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们描述 V 步骤的两种新近似。
- en: B.1 Approximate V step, variant 1 (non-accelerated)
  id: totrans-362
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1 近似 V 步骤，变体 1（非加速）
- en: 'We now describe an algorithm computing an approximation to $M_{V,\phi}(y)$:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在描述一个计算 $M_{V,\phi}(y)$ 近似的算法：
- en: '1.'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Start with some $y\in\mathbb{R}^{d}_{c}$
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从一些 $y\in\mathbb{R}^{d}_{c}$ 开始
- en: '2.'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Set $z^{0}=y$
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置 $z^{0}=y$
- en: '3.'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'For $t=0,\dots,T-1$ iterate:'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 $t=0,\dots,T-1$ 进行迭代：
- en: (i)
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (i)
- en: Define $\widehat{\phi}_{z^{t}}(\cdot):={\left\|\cdot-\left(z^{t}-\frac{1}{L}\nabla\phi(z^{t})\right)\right\|}^{2}$
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义 $\widehat{\phi}_{z^{t}}(\cdot):={\left\|\cdot-\left(z^{t}-\frac{1}{L}\nabla\phi(z^{t})\right)\right\|}^{2}$
- en: (ii)
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (ii)
- en: Set $z^{t+1}=M_{V,\widehat{\phi}_{z^{t}}}(z^{t})$
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置 $z^{t+1}=M_{V,\widehat{\phi}_{z^{t}}}(z^{t})$
- en: '4.'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Output: $z^{T}$'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出：$z^{T}$
- en: The method is constructed so that $z^{T}\approx M_{V,\phi}(y)$ may be advantageous.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法构建的目的是使得 $z^{T}\approx M_{V,\phi}(y)$ 可能具有优势。
- en: B.2 Approximate V step, variant 2 (accelerated)
  id: totrans-377
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.2 近似 V 步骤，变体 2（加速）
- en: 'We now describe a different algorithm for computing an approximation to $M_{V,\phi}(y)$:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在描述一个不同的算法来计算 $M_{V,\phi}(y)$ 的近似：
- en: '1.'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Start with some $y\in\mathbb{R}^{d}_{c}$
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从一些 $y\in\mathbb{R}^{d}_{c}$ 开始
- en: '2.'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Choose a suitable decreasing sequence of positive scalars $\{\alpha_{t}\}$
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择一个合适的递减正标量序列 $\{\alpha_{t}\}$
- en: '3.'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Set $z^{0}=y$
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置 $z^{0}=y$
- en: '4.'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'For $t=0,\dots,T-1$ iterate:'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 $t=0,\dots,T-1$ 进行迭代：
- en: (i)
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (i)
- en: Define $\widehat{\phi}_{z^{t}}(\cdot):={\left\|\cdot-\left(z^{t}-\frac{1}{L}\nabla\phi(z^{t})\right)\right\|}^{2}$
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义 $\widehat{\phi}_{z^{t}}(\cdot):={\left\|\cdot-\left(z^{t}-\frac{1}{L}\nabla\phi(z^{t})\right)\right\|}^{2}$
- en: (ii)
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (ii)
- en: Set $z^{t+1}=\left(1-\alpha_{t}\right)M_{V,\widehat{\phi}_{z^{t}}}(z^{t})+\alpha_{t}z^{0}$
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置 $z^{t+1}=\left(1-\alpha_{t}\right)M_{V,\widehat{\phi}_{z^{t}}}(z^{t})+\alpha_{t}z^{0}$
- en: '5.'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Output: $z^{T}$'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出：$z^{T}$
- en: The method is constructed so that $z^{T}\approx M_{V,\phi}(y)$. This approach
    is based on Halpern acceleration of fixed point methods [[47](#bib.bib47)], and
    as such, may be sometimes effective.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法构建的目的是使得 $z^{T}\approx M_{V,\phi}(y)$。这种方法基于 Halpern 加速固定点方法 [[47](#bib.bib47)]，因此有时可能会有效。
- en: Appendix C Generalization to Other Quantization Algorithms
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 C 其他量化算法的推广
- en: 'In Section [3.1](#S3.SS1 "3.1 Problem description ‣ 3 Fine-Tuning Quantized
    Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"),
    we define $\mathbb{R}^{d}_{\leq c}$ centroids found by clustering the weights.
    Below, we show how this can be generalized to linear quantization, vector quantization,
    additive quantization, and others.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [3.1](#S3.SS1 "3.1 Problem description ‣ 3 Fine-Tuning Quantized Models ‣
    PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression") 节中，我们定义了通过聚类权重找到的
    $\mathbb{R}^{d}_{\leq c}$ 类别中心。下面，我们展示如何将其推广到线性量化、矢量量化、加性量化等。'
- en: Linear quantization is the most basic and widely used type of quantization where
    weights are stored as integers, possibly multiplied by a scale and added to a
    zero point. The simplest way to account for this quantization type is to declare
    that weight values are integers up to $c$. Both options lead to equivalent fine-tuning
    algorithms where the V step does not change and the P step has an additional condition.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 线性量化是最基本和广泛使用的量化类型，其中权重以整数形式存储，可能乘以一个缩放因子并加上零点。处理这种量化类型的最简单方法是声明权重值为最大 $c$ 的整数。两种选项都导致等效的微调算法，其中
    V 步骤保持不变，而 P 步骤有额外的条件。
- en: Next, let us discuss vector quantization. Consider a quantization that splits
    $x$.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论矢量量化。考虑将 $x$ 切分的量化。
- en: 'Both the P and V steps for vector-quantized weights follow the same general
    principle: P-step can be approximated by backprop with slightly more trainable
    parameters. In turn, the V step can be done by trying all values in V(x) and selecting
    the one with the lowest $\widehat{\phi}(\cdot)$. A more compute-efficient version
    of the V step for this case is described in Appendix [D](#A4 "Appendix D Efficient
    Linearized V Step for Vector Quantization ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression").'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '向量量化权重的 P 步骤和 V 步骤遵循相同的基本原则：P 步骤可以通过反向传播和稍多的可训练参数进行近似。反过来，V 步骤可以通过尝试 V(x) 中的所有值并选择具有最低
    $\widehat{\phi}(\cdot)$ 的值来完成。附录 [D](#A4 "Appendix D Efficient Linearized V Step
    for Vector Quantization ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")中描述了此情况的 V 步骤更计算高效版本。'
- en: RVQ and Additive Quantization can be treated as learning two separate sets of
    vector-quantized parameters. However, a more efficient way would be to run the
    V step to find the best combination of codes via beam search [[4](#bib.bib4)].
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: RVQ 和加性量化可以视为学习两个独立的向量量化参数集合。然而，更有效的方法是运行 V 步骤，通过束搜索 [[4](#bib.bib4)] 找到最佳的代码组合。
- en: 'Quantization with sparse outliers [[15](#bib.bib15), [16](#bib.bib16), [39](#bib.bib39)]
    can be seen as learning two distinct matrices with different definitions of $\mathbb{R}^{d}_{c}$:
    one is quantized and the other is sparse (for outliers). Similarly, quantized
    weights with low-rank adapters (e.g. [[26](#bib.bib26)]) can treat the adapter
    as an additional non-quantized parameter for the P step. This makes PV-tuning
    potentially extensible to neural network pruning.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏离群值的量化 [[15](#bib.bib15), [16](#bib.bib16), [39](#bib.bib39)] 可以被看作是学习两个不同定义的
    $\mathbb{R}^{d}_{c}$ 矩阵：一个是量化的，另一个是稀疏的（用于离群值）。类似地，具有低秩适配器的量化权重（例如 [[26](#bib.bib26)]）可以将适配器视为
    P 步的一个额外非量化参数。这使得 PV 调优在神经网络剪枝方面具有潜在的扩展性。
- en: Appendix D Efficient Linearized V Step for Vector Quantization
  id: totrans-401
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 D 高效线性化 V 步骤用于向量量化
- en: 'As we describe in Section [3.2](#S3.SS2 "3.2 Linearized V step & gradient-based
    discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"), the linearized V step minimizes the
    squared error between quantized weights and the updated “target” weights. Here,
    we explain how one can compute and minimize the squared error efficiently in practice.
    To simplify the notation for this section, we define the objective as $\|x-B\|^{2}$
    norm, this objective can be re-written as follows:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '如我们在第 [3.2](#S3.SS2 "3.2 Linearized V step & gradient-based discrete updates
    ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")节中描述的，线性化 V 步骤最小化量化权重和更新的“目标”权重之间的平方误差。在这里，我们解释了如何在实践中有效地计算和最小化平方误差。为了简化本节的符号，我们将目标定义为
    $\|x-B\|^{2}$ 范数，该目标可以重新写为如下：'
- en: '|  | $\&#124;x-B\&#124;^{2}=\&#124;x\&#124;^{2}-2\langle x,B\rangle+\&#124;B\&#124;^{2}.$
    |  | (10) |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '|  | $\&#124;x-B\&#124;^{2}=\&#124;x\&#124;^{2}-2\langle x,B\rangle+\&#124;B\&#124;^{2}.$
    |  | (10) |'
- en: 'Consider the first term: $\|x\|^{2}=\sum_{i=1}^{d}x_{i}^{2}$ unique terms.
    Abusing your notation, this can be rewritten as'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑第一个项：$\|x\|^{2}=\sum_{i=1}^{d}x_{i}^{2}$ 个唯一项。滥用你的符号，这可以重写为
- en: '|  | $\&#124;x\&#124;^{2}=\sum_{i}^{{\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}c}}V_{i}(x)^{2}\cdot&#124;P_{i}(x)&#124;,$
    |  |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  | $\&#124;x\&#124;^{2}=\sum_{i}^{{\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}c}}V_{i}(x)^{2}\cdot&#124;P_{i}(x)&#124;,$
    |  |'
- en: where $V_{i}(x)$ is the number of such elements.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $V_{i}(x)$ 是这些元素的数量。
- en: 'The second term is also a sum of $c$ unique values:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 第二项也是 $c$ 个唯一值的总和：
- en: '|  | $1$2 |  |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: The third term does not depend on $x$.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 第三项不依赖于 $x$。
- en: 'If you know the objective for some given $x$ as follows:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道某个给定 $x$ 的目标如下：
- en: '|  | $\mathcal{N}_{1}(x):=\{\hat{x}\in\mathbb{R}^{d}_{\leq{c}}:V(\hat{x})=V(x),\,\&#124;x-\hat{x}\&#124;_{0}=1\}$
    |  | (11) |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{N}_{1}(x):=\{\hat{x}\in\mathbb{R}^{d}_{\leq{c}}:V(\hat{x})=V(x),\,\&#124;x-\hat{x}\&#124;_{0}=1\}$
    |  | (11) |'
- en: Consider one $\hat{x}\in\mathcal{N}_{1}(x)$. Then,
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个 $\hat{x}\in\mathcal{N}_{1}(x)$。然后，
- en: '|  | $1$2 |  | (12) |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (12) |'
- en: '|  | $\phi(\hat{x})-\phi(x)={\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\hat{x}_{k}^{2}-x_{k}^{2}}-{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}2\cdot(\hat{x}_{k}-x_{k})\cdot
    B_{k}}+{\color[rgb]{.5,.5,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\pgfsys@color@gray@stroke{.5}\pgfsys@color@gray@fill{.5}0}\\
    $ |  | (13) |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  | $\phi(\hat{x})-\phi(x)={\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\hat{x}_{k}^{2}-x_{k}^{2}}-{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}2\cdot(\hat{x}_{k}-x_{k})\cdot
    B_{k}}+{\color[rgb]{.5,.5,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\pgfsys@color@gray@stroke{.5}\pgfsys@color@gray@fill{.5}0}\\
    $ |  | (13) |'
- en: 'Note that, for any $\forall\hat{x}\in\mathcal{N}_{1}(x)$. This allows for an
    efficient local search algorithm:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于任何$\forall\hat{x}\in\mathcal{N}_{1}(x)$。这允许一个高效的局部搜索算法：
- en: '1.'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Let $x^{0}$
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让$x^{0}$
- en: '2.'
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Compute and save all $2c^{2}$ possible red and blue values
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算并保存所有$2c^{2}$可能的红色和蓝色值
- en: '3.'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Compute $\phi_{0}=\phi(x^{0})$
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算$\phi_{0}=\phi(x^{0})$
- en: '4.'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'for t = 0, …:'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于t = 0, …：
- en: '5.'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'for $\hat{x}\in\mathcal{N}_{1}(x^{t})$:'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于$\hat{x}\in\mathcal{N}_{1}(x^{t})$：
- en: '6.'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: find $k:\hat{x}_{k}\neq x^{t}_{k}$)
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 找到$k:\hat{x}_{k}\neq x^{t}_{k}$)
- en: '7.'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: compute $1$2
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算$1$2
- en: '8.'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: $x^{t+1}:=\underset{\hat{x}\in\mathcal{N}_{1}(x^{t})}{\arg\min}\phi(\hat{x})$
      (minimum from array of pre-computed values)
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $x^{t+1}:=\underset{\hat{x}\in\mathcal{N}_{1}(x^{t})}{\arg\min}\phi(\hat{x})$
      （从预计算值数组中取最小值）
- en: In practice, this can be extended from greedy (local) search to semi-greedy
    beam search. These practical algorithms are described in AQ, LSQ, and LSQ++. Algorithms
    for $\|A(x-B)\|^{2}$ are explained in AQLM and probably other works.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这可以从贪婪（局部）搜索扩展到半贪婪束搜索。这些实际算法在AQ、LSQ和LSQ++中有所描述。$\|A(x-B)\|^{2}$的算法在AQLM和其他可能的工作中解释。
- en: Appendix E Straight-through Gradient Estimation
  id: totrans-433
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录E 直通梯度估计
- en: Straight-through gradient estimation is a technique for training neural networks
    with discrete components that ignores these discrete components during backpropagation.
    Its usage goes back to the Perceptron introduced by Rosenblatt [[54](#bib.bib54)].
    There, the artificial neuron uses a step function as activation, but the training
    procedure treats this function as though it was identity. Subsequent works introduce
    variations of this idea, extend it to multi-layer networks [[28](#bib.bib28)],
    discuss its convergence properties [[70](#bib.bib70), [21](#bib.bib21), [74](#bib.bib74)].
    Other works use straight-through estimation or similar techniques to training
    neural network with quantized weights [[27](#bib.bib27), [59](#bib.bib59), [60](#bib.bib60)].
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 直通梯度估计是一种训练具有离散组件的神经网络的技术，它在反向传播过程中忽略这些离散组件。它的使用可以追溯到Rosenblatt引入的感知器[[54](#bib.bib54)]。在那里，人工神经元使用阶跃函数作为激活，但训练过程将此函数视为身份函数。后续的工作引入了这种思想的变体，将其扩展到多层网络[[28](#bib.bib28)]，讨论其收敛性质[[70](#bib.bib70),
    [21](#bib.bib21), [74](#bib.bib74)]。其他工作使用直通估计或类似技术来训练具有量化权重的神经网络[[27](#bib.bib27),
    [59](#bib.bib59), [60](#bib.bib60)]。
- en: 'STE for LLM quantization. As we discuss in Section [2](#S2.SS0.SSS0.Px2 "Fine-tuning
    over Quantized Weights. ‣ 2 Background ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression"), straight-through estimation introduces an auxiliary
    non-quantized weight tensor that is updated using the gradients $\nabla\phi(y)$
    will no longer be the solution to Equation ([6](#S3.E6 "Equation 6 ‣ 3.2 Linearized
    V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM量化的STE。如我们在第[2](#S2.SS0.SSS0.Px2 "对量化权重的微调。 ‣ 2 背景 ‣ PV-Tuning: 超越直通估计的极端LLM压缩")节中讨论的，直通估计引入了一个辅助的非量化权重张量，该张量使用梯度$\nabla\phi(y)$进行更新，这将不再是方程([6](#S3.E6
    "方程 6 ‣ 3.2 线性化V步骤及基于梯度的离散更新 ‣ 3 微调量化模型 ‣ PV-Tuning: 超越直通估计的极端LLM压缩"))的解。'
- en: 'This strategy prevents the algorithm from stalling, but it does so at the cost
    of convergence guarantees [[74](#bib.bib74)]. When applied to extreme LLM quantization
    (Section [4.2](#S4.SS2 "4.2 Evaluating Fine-tuning Algorithms ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"),
    straight-through estimation initially improves $y^{k}$, but then stops improving
    and oscillates. We also tried several a variant of straight-through estimation [[60](#bib.bib60)]
    that introduce stochasticity to forward pass. When applied to extreme LLM quantization,
    this variant did not diverge like naive STE, but trained much slower and did not
    reach the same optimum as “deterministic” STE. We attribute this to the fact that
    adding noise during training can slow down convergence, which also applies to
    stochastic rounding (Appendix  [F](#A6 "Appendix F Stochastic Rounding ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '这种策略防止了算法停滞，但代价是失去了收敛保证 [[74](#bib.bib74)]。当应用于极端 LLM 量化时（第 [4.2](#S4.SS2 "4.2
    Evaluating Fine-tuning Algorithms ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression") 节），直接通过估计最初改善了 $y^{k}$，但随后停止改进并开始振荡。我们还尝试了几种引入随机性的直通估计变体
    [[60](#bib.bib60)]。当应用于极端 LLM 量化时，这种变体没有像简单的 STE 那样发散，但训练速度更慢，未能达到“确定性” STE 的相同最优值。我们将此归因于训练过程中添加噪声可能会减慢收敛速度，这同样适用于随机舍入（附录
    [F](#A6 "Appendix F Stochastic Rounding ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")）。'
- en: Appendix F Stochastic Rounding
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 F 随机舍入
- en: 'Stochastic (or probabilistic) rounding is one of the techniques that can circumvent
    stalling when training low-precision weights. To recall, the linearized V step
    ([6](#S3.E6 "Equation 6 ‣ 3.2 Linearized V step & gradient-based discrete updates
    ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")) can be seen as rounding $y^{+}:=y-\frac{1}{L}\nabla\phi(y)$
    for left and right. The probability of rounding is inversely proportional to the
    rounding error (distance), or, in terms of the objective,'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '随机（或概率）舍入是一个可以绕过训练低精度权重时停滞的技术。回顾一下，线性化的 V 步骤（[6](#S3.E6 "Equation 6 ‣ 3.2 Linearized
    V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")）可以被视为对 $y^{+}:=y-\frac{1}{L}\nabla\phi(y)$
    进行左右舍入。舍入的概率与舍入误差（距离）成反比，或者，从目标的角度来看，'
- en: '|  | $1$2 |  |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: This way,
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，
- en: '|  | $\underset{p(\text{round to }x)}{E}x=y-\frac{1}{L}\nabla\phi(y).$ |  |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underset{p(\text{round to }x)}{E}x=y-\frac{1}{L}\nabla\phi(y).$ |  |'
- en: 'The main drawback of stochastic rounding is t introduces noise, it changes
    the underlying optimization problem. Intuitively, if the optimal $x^{\star}$ is
    adjacent to a significantly worse solution, the method may oscillate between rounding
    to either side. This rounding noise increases further as we consider lower quantization
    width. In Section [4.2](#S4.SS2 "4.2 Evaluating Fine-tuning Algorithms ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    we exploit his phenomenon for real-world LLMs and find that stochastic rounding
    converges find significantly worse solutions, presumably because at every step,
    some portion of LLM weights will be rounded poorly. On top of that, when used
    for vector quantization, stochastic rounding is either intractable or biased.'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '随机舍入的主要缺点是它引入了噪声，改变了基础的优化问题。直观地说，如果最优的 $x^{\star}$ 邻近一个明显更差的解决方案，该方法可能会在舍入到任一侧之间振荡。考虑到更低量化宽度时，这种舍入噪声会进一步增加。在第
    [4.2](#S4.SS2 "4.2 Evaluating Fine-tuning Algorithms ‣ 4 Experiments ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression") 节中，我们利用了这种现象用于真实世界的
    LLM，并发现随机舍入会收敛到明显更差的解决方案，可能是因为每一步中，部分 LLM 权重会被较差地舍入。除此之外，当用于向量量化时，随机舍入要么不可行，要么存在偏差。'
- en: 'Stochastic rounding for vector quantization. To recall, stochastic rounding
    for non-vector quantization needs to find two quantized values: the nearest neighbor
    above the solution, and the nearest neighbor below it. It will then round to either
    of the two values inversely proportionally to their rounding errors.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 向量量化的随机舍入。回顾一下，非向量量化的随机舍入需要找到两个量化值：一个是解决方案上方的最近邻，另一个是下方的最近邻。然后，它将以与这些舍入误差的反比的方式进行舍入。
- en: However, this intuition no longer works if you consider more complex quantization
    schemes such as vector quantization, additive quantization, quantized low-rank
    adapters, and others. In vector quantization, a group of weights is encoded jointly
    as one vector from a fixed set (usually called codebook or lattice). For simplicity,
    let us consider the case where the weight group size equals 2, i.e. weights are
    quantized in pairs.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果考虑更复杂的量化方案，如矢量量化、加性量化、量化低秩适配器等，这一直觉将不再适用。在矢量量化中，一组权重作为一个向量从一个固定集合中编码（通常称为码本或格点）。为了简单起见，我们考虑权重组大小等于
    2 的情况，即权重以对的形式量化。
- en: For a pair of two eights, we can no longer rely on the fact that they have one
    neighbor from above and one from below. Instead, they may have any number of adjacent
    "clusters" they can be rounded to. Intuitively, a pair of weights is a point in
    2-dimensional that can have neighbors from left, right, top, bottom, and any diagonals.
    Formally, to determine a full list of neighbors, we can run Delaunay triangulation
    on all vectors from the codebook (or lattice) plus the target vector that needs
    to be rounded, then find all points that share a triangle with the target vector.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一对两个八，我们不能再依赖于它们有一个来自上方的邻居和一个来自下方的邻居。相反，它们可能有任何数量的相邻“簇”可以舍入。直观上，一对权重是二维空间中的一个点，可以有来自左、右、上、下及任何对角线的邻居。形式上，为了确定完整的邻居列表，我们可以对码本（或格点）中的所有向量加上需要舍入的目标向量运行
    Delaunay 三角剖分，然后找到与目标向量共享一个三角形的所有点。
- en: 'Unfortunately, this procedure can be very expensive, especially for higher-dimensional
    vectors. A popular practical approximation to stochastic rounding for vector quantizations
    is to find K (e.g. 2) nearest vectors from the codebook, then use the probability
    formula from scalar stochastic rounding:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这一过程可能非常昂贵，特别是对于高维向量。一个流行的实际近似方法是对矢量量化进行随机舍入，通过从码本中找到 K 个（例如 2）最近的向量，然后使用标量随机舍入的概率公式：
- en: '|  | $p(\text{round to }x_{i})=\widehat{\phi}(x_{i})^{-1/2}/(\sum^{K}_{j}\widehat{\phi}(x_{j})^{-1/2})$
    |  |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(\text{round to }x_{i})=\widehat{\phi}(x_{i})^{-1/2}/(\sum^{K}_{j}\widehat{\phi}(x_{j})^{-1/2})$
    |  |'
- en: However, unlike the original stochastic rounding, this approximation does not
    guarantee that
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与原始的随机舍入不同，这种近似方法并不能保证
- en: '|  | $\underset{p(\text{round to }x_{i})}{E}x_{i}=y-\frac{1}{L}\nabla\phi(y).$
    |  | (14) |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underset{p(\text{round to }x_{i})}{E}x_{i}=y-\frac{1}{L}\nabla\phi(y).$
    |  | (14) |'
- en: For a (counter)example, if there is a high density of codes on one side of the
    target, all K (e.g. 2) nearest neighbors will be on the same side. As a result,
    this approximate stochastic rounding is biased and further changes the optimization
    result.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个（反例），如果目标的一侧存在高密度的码，则所有 K（例如 2）个最近的邻居将位于同一侧。因此，这种近似随机舍入是有偏的，并进一步改变优化结果。
- en: Appendix G On $L$-smoothness of LLM Objective in Sparse Subspaces
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 G 关于稀疏子空间中 LLM 目标的 $L$-光滑性
- en: The classical definition of $L$ represented by requirement
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 由要求表示的 $L$ 的经典定义
- en: '|  | $\&#124;\nabla f(x)-\nabla f(y)\&#124;\leq L\&#124;x-y\&#124;,\qquad\forall
    x,y\in\mathbb{R}^{d}.$ |  |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '|  | $\&#124;\nabla f(x)-\nabla f(y)\&#124;\leq L\&#124;x-y\&#124;,\qquad\forall
    x,y\in\mathbb{R}^{d}.$ |  |'
- en: If function $f(x)$.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 如果函数 $f(x)$。
- en: If the domain of function $f(x)$ we have
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有函数 $f(x)$ 的定义域
- en: '|  | $1$2 |  |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Sparse sub-spaces satisfy this requirement; therefore, this theoretical observation
    is valid in this circumstance.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏子空间满足这一要求，因此在这种情况下，这一理论观察是有效的。
- en: Another observation is that the definition of $L$ space.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个观察是 $L$ 空间的定义。
- en: Below we demonstrate two approximate schemas for evaluating $L$.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们演示了两种评估 $L$ 的近似方案。
- en: 'Schema I: Estimating $L$ along the trajectory of iterates without capturing
    local curvature.'
  id: totrans-460
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方案 I：在不捕捉局部曲率的情况下沿迭代轨迹估计 $L$。
- en: After running GD for $10$ with approximated as
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 GD 进行 $10$ 次后，近似为
- en: '|  | $1$2 |  |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'Results are presented in Tables [3](#A7.T3 "Table 3 ‣ Schema I: Estimating
    𝐿 along the trajectory of iterates without capturing local curvature. ‣ Appendix
    G On 𝐿-smoothness of LLM Objective in Sparse Subspaces ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression"), [4](#A7.T4 "Table
    4 ‣ Schema I: Estimating 𝐿 along the trajectory of iterates without capturing
    local curvature. ‣ Appendix G On 𝐿-smoothness of LLM Objective in Sparse Subspaces
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").
    From them, we can see that $\hat{L}$.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 结果见表格 [3](#A7.T3 "表格 3 ‣ 方案 I：沿迭代轨迹估计 𝐿，而不捕捉局部曲率。 ‣ 附录 G 关于稀疏子空间中的 LLM 目标的 𝐿-平滑性
    ‣ 附录 ‣ PV-Tuning：超越极端 LLM 压缩的直接估计")，[4](#A7.T4 "表格 4 ‣ 方案 I：沿迭代轨迹估计 𝐿，而不捕捉局部曲率。
    ‣ 附录 G 关于稀疏子空间中的 LLM 目标的 𝐿-平滑性 ‣ 附录 ‣ PV-Tuning：超越极端 LLM 压缩的直接估计")。从中我们可以看到 $\hat{L}$。
- en: 'Table 3: Estimated $L$ along the GD trajectory for LLama-160m (Schema I).'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：LLama-160m 的 GD 轨迹上的估计 $L$（方案 I）。
- en: '| Subspace Size | Number of Trainable Parameters | Estimated $\hat{L}$ |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| 子空间大小 | 可训练参数数量 | 估计的 $\hat{L}$ |'
- en: '| 5% | 2.36M | 10.01 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| 5% | 2.36M | 10.01 |'
- en: '| 10% | 8.26M | 14.40 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| 10% | 8.26M | 14.40 |'
- en: '| 20% | 17.69M | 305.72 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| 20% | 17.69M | 305.72 |'
- en: '| 30% | 24.77M | 498.16 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 24.77M | 498.16 |'
- en: '| 40% | 36.57M | 919.82 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 40% | 36.57M | 919.82 |'
- en: '| 60% | 60.75M | 5454.79 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| 60% | 60.75M | 5454.79 |'
- en: '| 70% | 85.52M | 6915.06 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 70% | 85.52M | 6915.06 |'
- en: '| 85% | 102.04M | 7043.19 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| 85% | 102.04M | 7043.19 |'
- en: '| 100% | 113.25M | 7251.50 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| 100% | 113.25M | 7251.50 |'
- en: 'Table 4: Estimated $L$ along the GD trajectory for TinyLlama-1.1B (Schema I).'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4：TinyLlama-1.1B 的 GD 轨迹上的估计 $L$（方案 I）。
- en: '| Subspace Size | Number of Trainable Parameters | Estimated $\hat{L}$ |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| 子空间大小 | 可训练参数数量 | 估计的 $\hat{L}$ |'
- en: '| 5% | 13.11M | 33.24 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| 5% | 13.11M | 33.24 |'
- en: '| 10% | 49.28M | 143.00 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| 10% | 49.28M | 143.00 |'
- en: '| 20% | 136.84M | 2159.22 |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| 20% | 136.84M | 2159.22 |'
- en: '| 30% | 242.75M | 2369.63 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 242.75M | 2369.63 |'
- en: '| 40% | 369.10M | 2638.11 |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| 40% | 369.10M | 2638.11 |'
- en: '| 60% | 582.48M | 5185.92 |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| 60% | 582.48M | 5185.92 |'
- en: '| 70% | 684.20M | 5901.73 |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 70% | 684.20M | 5901.73 |'
- en: '| 85% | 831.52M | 6091.04 |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| 85% | 831.52M | 6091.04 |'
- en: '| 100% | 968.88M | 9480.57 |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| 100% | 968.88M | 9480.57 |'
- en: 'Schema II: Estimating $L$ along the sequence of iterates with capturing local
    curvature.'
  id: totrans-486
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方案 II：沿迭代序列估计 $L$ 并捕捉局部曲率。
- en: The previous schema used a fixed sequence of iterates $s=\{x_{1},x_{2},\dots,x_{10}\}$-smoothness
    constant as
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的方案使用了一个固定的迭代序列 $s=\{x_{1},x_{2},\dots,x_{10}\}$-平滑常数为
- en: '|  | $\hat{L}=\underset{x_{i}\in s}{\max}\left(\&#124;\nabla^{2}f(x_{i})\&#124;\right).$
    |  |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{L}=\underset{x_{i}\in s}{\max}\left(\&#124;\nabla^{2}f(x_{i})\&#124;\right).$
    |  |'
- en: Therefore this schema exploits a sequence of points $s$. Computing any spectral
    information for a matrix with big dimensions can be challenging.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个方案利用了一个点的序列 $s$。计算大维度矩阵的任何谱信息可能会很具挑战性。
- en: 'The approximate schema that we have used to compute $\|\nabla^{2}f(x)\|$:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来计算 $\|\nabla^{2}f(x)\|$ 的大致方案：
- en: '|  | $\nabla f(x+r)-\nabla f(x)=\nabla^{2}f(x)\cdot r+\mathcal{O}\left(\&#124;r\&#124;^{2}\right)$
    |  |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla f(x+r)-\nabla f(x)=\nabla^{2}f(x)\cdot r+\mathcal{O}\left(\&#124;r\&#124;^{2}\right)$
    |  |'
- en: 'The $K$. In fact Power Iteration does not converge in case of degeneracy such
    as a situation when the matrix has two maximum eigenvalues in absolute values
    but with opposite signs, and the convergence rate is determined by the absolute
    value of the ratio of the second-largest-magnitude eigenvalue to the first. We
    ignore these aspects in our heuristic approximate Algorithm [5](#alg5 "Algorithm
    5 ‣ Schema II: Estimating 𝐿 along the sequence of iterates with capturing local
    curvature. ‣ Appendix G On 𝐿-smoothness of LLM Objective in Sparse Subspaces ‣
    Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: $K$。实际上，幂迭代在矩阵存在两个最大特征值但符号相反的情况下不会收敛，收敛速度由第二大特征值与第一大特征值的绝对值之比决定。在我们的启发式近似算法中忽略了这些方面
    [5](#alg5 "算法 5 ‣ 方案 II：沿迭代序列估计 𝐿 并捕捉局部曲率。 ‣ 附录 G 关于稀疏子空间中的 LLM 目标的 𝐿-平滑性 ‣ 附录
    ‣ PV-Tuning：超越极端 LLM 压缩的直接估计")。
- en: Algorithm 5 Approximate Matrix-free Algorithm for Computing $\|\nabla f(x)\|$
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 5 近似无矩阵算法计算 $\|\nabla f(x)\|$
- en: '1:  Parameters: Point $x\in\mathbb{R}^{d}$.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 1:  参数：点 $x\in\mathbb{R}^{d}$。
- en: 'Results are presented in Tables [5](#A7.T5 "Table 5 ‣ Schema II: Estimating
    𝐿 along the sequence of iterates with capturing local curvature. ‣ Appendix G
    On 𝐿-smoothness of LLM Objective in Sparse Subspaces ‣ Appendix ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression"), [6](#A7.T6 "Table 6
    ‣ Schema II: Estimating 𝐿 along the sequence of iterates with capturing local
    curvature. ‣ Appendix G On 𝐿-smoothness of LLM Objective in Sparse Subspaces ‣
    Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").
    From them, we can see that also this notion of $\hat{L}$-the smooth constant is
    non-increasing similar to previous estimation method.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 结果展示在表格 [5](#A7.T5 "表 5 ‣ Schema II：估计 𝐿 在迭代序列中，捕捉局部曲率。 ‣ 附录 G 关于 LLM 目标在稀疏子空间中的
    𝐿-平滑性 ‣ 附录 ‣ PV-Tuning：超越直接估计以实现极端 LLM 压缩") 和 [6](#A7.T6 "表 6 ‣ Schema II：估计 𝐿
    在迭代序列中，捕捉局部曲率。 ‣ 附录 G 关于 LLM 目标在稀疏子空间中的 𝐿-平滑性 ‣ 附录 ‣ PV-Tuning：超越直接估计以实现极端 LLM
    压缩")。从中我们可以看到，这个 $\hat{L}$-平滑常数的概念也与以前的估计方法类似，表现为非递增。
- en: 'Table 5: Estimated $L$ along the GD iterates for LLama-160m with local curvature
    (Schema II).'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：LLama-160m 在局部曲率（Schema II）下 GD 迭代中的估计 $L$。
- en: '| Subspace Size | Number of Trainable Parameters | Estimated $\hat{L}$ |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| 子空间大小 | 可训练参数数量 | 估计的 $\hat{L}$ |'
- en: '| 5% | 2.36M | 10.96 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| 5% | 2.36M | 10.96 |'
- en: '| 10% | 8.26M | 791.30 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| 10% | 8.26M | 791.30 |'
- en: '| 20% | 17.69M | 878.37 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| 20% | 17.69M | 878.37 |'
- en: '| 30% | 24.77M | 1202.00 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 24.77M | 1202.00 |'
- en: '| 40% | 36.57M | 1918.04 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| 40% | 36.57M | 1918.04 |'
- en: '| 60% | 60.75M | 5262.77 |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| 60% | 60.75M | 5262.77 |'
- en: '| 70% | 85.52M | 5325.83 |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| 70% | 85.52M | 5325.83 |'
- en: '| 85% | 102.04M | 5901.21 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| 85% | 102.04M | 5901.21 |'
- en: '| 100% | 113.25M | 11522.45 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| 100% | 113.25M | 11522.45 |'
- en: 'Table 6: Estimated $L$ along the GD iterates for TinyLlama-1.1B with local
    curvature (Schema II).'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：TinyLlama-1.1B 在局部曲率（Schema II）下 GD 迭代中的估计 $L$。
- en: '| Subspace Size | Number of Trainable Parameters | Estimated $\hat{L}$ |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| 子空间大小 | 可训练参数数量 | 估计的 $\hat{L}$ |'
- en: '| 5% | 13.11M | 40.30 |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| 5% | 13.11M | 40.30 |'
- en: '| 10% | 49.28M | 146.93 |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| 10% | 49.28M | 146.93 |'
- en: '| 20% | 136.84M | 4366.38 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| 20% | 136.84M | 4366.38 |'
- en: '| 30% | 242.75M | 4487.38 |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 242.75M | 4487.38 |'
- en: '| 40% | 369.10M | 6767.58 |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| 40% | 369.10M | 6767.58 |'
- en: '| 60% | 582.48M | 8983.85 |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| 60% | 582.48M | 8983.85 |'
- en: '| 70% | 684.20M | 15445.54 |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| 70% | 684.20M | 15445.54 |'
- en: '| 85% | 831.52M | 21167.06 |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| 85% | 831.52M | 21167.06 |'
- en: '| 100% | 968.88M | 28629.24 |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| 100% | 968.88M | 28629.24 |'
- en: Appendix H Calibration Data Matters
  id: totrans-518
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 H 校准数据的重要性
- en: For a fair comparison, we run our algorithm using the same calibration data
    as the baseline algorithms, typically a sample from RedPajama [[11](#bib.bib11)].
    However, the way we handle this calibration data differs from most baselines [[19](#bib.bib19),
    [16](#bib.bib16), [65](#bib.bib65)].
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公平比较，我们使用与基线算法相同的校准数据运行我们的算法，通常是来自 RedPajama [[11](#bib.bib11)] 的样本。然而，我们处理这些校准数据的方式与大多数基线算法
    [[19](#bib.bib19), [16](#bib.bib16), [65](#bib.bib65)] 的方式不同。
- en: When analyzing their codebase, we found that these algorithms resample calibration
    data by taking a random excerpt of a fixed length from a random document in the
    calibration data, both sampled uniformly. However, with this approach, the data
    from longer documents (e.g. books) are underrepresented compared to shorter ones
    (e.g. news articles), which biases the calibration data.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析他们的代码库时，我们发现这些算法通过从校准数据中的随机文档中抽取固定长度的随机片段来重新采样校准数据，所有样本均匀分布。然而，这种方法下，来自较长文档（例如书籍）的数据相对于较短的文档（例如新闻文章）被低估，这会使校准数据产生偏差。
- en: Upon further investigation, we believe that new methods blindly copied this
    code from each other, going back to GPTQ [[20](#bib.bib20)] and possibly further.
    This choice was harmless for GPTQ since it requires relatively little calibration
    data; however, full model fine-tuning like in QuIP# [[8](#bib.bib8)] and AQLM [[19](#bib.bib19)],
    works better on unbiased data.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 经过进一步调查，我们认为新的方法盲目地相互复制了这段代码，回溯至 GPTQ [[20](#bib.bib20)] 及可能更早。这一选择对 GPTQ 无害，因为它需要的校准数据相对较少；然而，像
    QuIP# [[8](#bib.bib8)] 和 AQLM [[19](#bib.bib19)] 这样完全的模型微调，在无偏数据上效果更佳。
- en: To remove the bias, we use standard⁸⁸8from e.g. [https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py)
    LM preprocessing that concatenates all documents, then splits them into evenly
    sized chunks that become training sequences. The benefits from debiasing range
    from insignificant to as large as 0.15 perplexity for some models. To compensate
    for that, we run experiments with the same preprocessing protocol unless explicitly
    stated otherwise.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除偏差，我们使用标准⁸⁸8，如[https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py)的语言模型预处理方法，该方法将所有文档连接起来，然后将其拆分成大小均匀的块，作为训练序列。去偏差的好处从微不足道到对于某些模型高达0.15的困惑度不等。为了弥补这一点，除非另有明确说明，我们会使用相同的预处理协议进行实验。
- en: Appendix I Additional Engineering Considerations
  id: totrans-523
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 I 额外的工程考虑
- en: 'When done naively, the longest operation is the discrete update ([8](#S3.E8
    "Equation 8 ‣ 3.3 Linearized subspace V step ‣ 3 Fine-Tuning Quantized Models
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    that runs discrete optimization on all LLM parameters. For scalar quantization,
    this step does simple rounding and runs nearly instantly. In turn, applying it
    to vector quantization requires solving a discrete optimization algorithm (e.g.
    beam search) for every group of weights. However, since equation ([8](#S3.E8 "Equation
    8 ‣ 3.3 Linearized subspace V step ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) can only update
    weights within $S^{k}$, we can skip discrete optimization for any weight that
    was not among the chosen few. As a result, when training models with up to 70
    billion parameters, we search less than one billion times per step.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 当进行得很简单时，最长的操作是离散更新（[8](#S3.E8 "方程 8 ‣ 3.3 线性化子空间 V 步骤 ‣ 3 微调量化模型 ‣ PV 调整：超越直接估计的极端
    LLM 压缩")），该操作在所有LLM参数上运行离散优化。对于标量量化，这一步骤仅进行简单的舍入，并几乎瞬间完成。反过来，将其应用于向量量化需要为每组权重解决离散优化算法（例如束搜索）。然而，由于方程（[8](#S3.E8
    "方程 8 ‣ 3.3 线性化子空间 V 步骤 ‣ 3 微调量化模型 ‣ PV 调整：超越直接估计的极端 LLM 压缩")）只能更新$S^{k}$内的权重，因此我们可以跳过对未被选择的权重的离散优化。因此，当训练高达70亿参数的模型时，我们每一步的搜索次数少于十亿次。
- en: 'The next longest operation is computing the gradients $\nabla\phi(\cdot)$,
    needed both for P and V steps. This involves running LLM multiple forward and
    backward passes on batches of texts and accumulating the gradients. To reduce
    the overhead from gradient computation, we compute the gradients once using mixed
    precision, then reuse these gradients for one P and one V step, respectively.
    In other words, we switch from alternating P and V steps to performing these steps
    simultaneously. We also reuse these gradients to update any parameters not affected
    by quantization: input embeddings, normalization layers, biases, etc.'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来最长的操作是计算梯度 $\nabla\phi(\cdot)$，这在P和V步骤中都需要。这涉及对文本批次进行多次前向和后向传递，并累积梯度。为了减少梯度计算的开销，我们一次使用混合精度计算梯度，然后分别重用这些梯度进行一个P步骤和一个V步骤。换句话说，我们将P和V步骤交替执行切换为同时执行这些步骤。我们还重用这些梯度来更新不受量化影响的任何参数：输入嵌入、归一化层、偏置等。
- en: To limit VRAM usage, we use gradient checkpointing [[25](#bib.bib25)], batch
    accumulation. For larger models, we also use parameter sharding⁹⁹9We use PyTorch
    FullyShardedDataParallel [[48](#bib.bib48), [37](#bib.bib37)] and wrap discrete
    weights as in bitsandbytes [[17](#bib.bib17)]  [[51](#bib.bib51)] and optimizer
    offloading [[52](#bib.bib52)]. We need these techniques so that smaller 7B LLMs
    can be trained on a single GPU and larger ones with 70B parameters fit into a
    single machine with 8${\times}$ longer than tuning continuous parameters and uses
    more memory (during training) to hold the gradients w.r.t. dequantized weights.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 为了限制VRAM使用，我们使用梯度检查点[[25](#bib.bib25)]，批次累积。对于较大的模型，我们还使用参数分片⁹⁹9，我们使用PyTorch
    FullyShardedDataParallel[[48](#bib.bib48)，[37](#bib.bib37)]，并像bitsandbytes[[17](#bib.bib17)]
    [[51](#bib.bib51)]一样包装离散权重，以及优化器卸载[[52](#bib.bib52)]。我们需要这些技术，以便较小的7B LLM可以在单个GPU上进行训练，而更大的70B参数模型则能适配到一个具有8${\times}$比调整连续参数更长的单台机器中，并在训练期间使用更多内存来保存相对于去量化权重的梯度。
- en: 'Appendix J Additional Details for Section [4.1](#S4.SS1 "4.1 Evaluating quantized
    representations with finetuning ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")'
  id: totrans-527
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '附录J 第[4.1](#S4.SS1 "4.1 Evaluating quantized representations with finetuning
    ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression")节的附加细节'
- en: Here, we describe some of the implementation details we used to optimize different
    quantized representations. For every optimizations, we check that this optimization
    improves the algorithm in both MSE and perplexity and does not require additional
    resources that would result in unfair comparison.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们描述了一些用于优化不同量化表示的实现细节。对于每次优化，我们检查该优化是否在MSE和困惑度上改进了算法，并且不会需要额外的资源，从而导致不公平的比较。
- en: Vector Quantziation. The original algorithm quantizes all weights a single pass
    over input channels. We found that it works slightly better if we make multiple
    such passes and, between passes, update codes by Adam to minimize the same objective [[66](#bib.bib66)].
    This is resembles QuIP# with no RHT & lattices or AQLM with no additive quantization.
    For simplicity, we also use a single shared codebook (e.g. instead of groupwise
    codebooks).
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 向量量化。原始算法在输入通道上进行单次量化。我们发现，如果进行多次这样的遍历，并在遍历之间通过Adam更新代码以最小化相同的目标[[66](#bib.bib66)]，效果会略好。这类似于没有RHT和网格的QuIP#，或没有加法量化的AQLM。为简便起见，我们还使用一个共享代码本（例如，代替按组代码本）。
- en: VQ+outliers To select outlier coordinates, we use [https://github.com/fmfi-compbio/admm-pruning](https://github.com/fmfi-compbio/admm-pruning)
    that outperforms the SpQR outlier criterion [[16](#bib.bib16)] in both L2 error
    and perplexity (when both criteria are used alongside vector quantization). We
    re-run the ADMM procedure multiple times during optimization, resulting in an
    EM-like algorithm.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: VQ+异常值 为了选择异常坐标，我们使用[https://github.com/fmfi-compbio/admm-pruning](https://github.com/fmfi-compbio/admm-pruning)，它在L2误差和困惑度上都优于SpQR异常值标准[[16](#bib.bib16)]（当两个标准与向量量化一起使用时）。我们在优化过程中多次重新运行ADMM过程，最终得到一个类似EM的算法。
- en: 'VQ+lowrank. We experimented with two different initializations for low-rank
    correction: a) quantizing weight matrix, then training LoRC on quantization error,
    as in [[73](#bib.bib73)] and b) initializing LoRC to approximate the reference
    weight matrix, the applying vector quantization to LoRC errors. Of these two approaches,
    we found that the latter one produces a (slightly) better solution in both MSE
    and perplexity.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: VQ+低秩。我们尝试了两种不同的低秩校正初始化方式：a) 量化权重矩阵，然后在量化误差上训练LoRC，如[[73](#bib.bib73)]所示，b)
    初始化LoRC以近似参考权重矩阵，然后将向量量化应用于LoRC误差。在这两种方法中，我们发现后者在MSE和困惑度上都产生了（略微）更好的结果。
- en: 'Appendix K Additional Details for Section [4.2](#S4.SS2 "4.2 Evaluating Fine-tuning
    Algorithms ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation for
    Extreme LLM Compression")'
  id: totrans-532
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '附录K 第[4.2](#S4.SS2 "4.2 Evaluating Fine-tuning Algorithms ‣ 4 Experiments ‣
    PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")节的附加细节'
- en: 'VQ: vector quantization as a simple near-optimal algorithm. We use a single
    16-bit codebook with group size 16 (over input dimension) and per-channel trainable
    scales over output dimension. During P step, we update the codebook, scales and
    non-quantized model layers by backprop. During V step, we try every code in the
    codebook and choose the one that minimizes ([6](#S3.E6 "Equation 6 ‣ 3.2 Linearized
    V step & gradient-based discrete updates ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 'VQ: 向量量化作为一种简单的近似最优算法。我们使用一个16位代码本，组大小为16（相对于输入维度），并且每个通道的可训练尺度相对于输出维度。在P步骤中，我们通过反向传播更新代码本、尺度和未量化的模型层。在V步骤中，我们尝试代码本中的每个代码，选择最小化([6](#S3.E6
    "Equation 6 ‣ 3.2 Linearized V step & gradient-based discrete updates ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression"))的那个。'
- en: 'GPTQ: scalar quantization with block-wise scales and zero points. We use 2-bit
    base codes and block size 128\. During P step, we update the scales and non-quantized
    parameters by backprop. In turn, V step performs simple rounding to nearest (scaled)
    integer.^(10)^(10)10We also found that we can perform V step by running the entire
    OPTQ calibration while using $y-\frac{1}{L}\nabla\phi(y)$ as target weights, showing
    the flexibility of the general PV framework. This variant converges to the same
    values, but is much slower due to having to re-accumulate the L2 error hessians
    for each V step..'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ：使用块级标量量化及零点。我们使用2位基本代码和块大小128。在P步骤中，我们通过反向传播更新标量和未量化的参数。接着，V步骤对最近的（缩放的）整数进行简单舍入。^(10)^(10)10我们还发现可以通过运行整个OPTQ校准并使用$y-\frac{1}{L}\nabla\phi(y)$作为目标权重来执行V步骤，这展示了通用PV框架的灵活性。这个变体收敛到相同的值，但由于每个V步骤需要重新累积L2误差Hessian，因此速度较慢。
- en: 'AQLM: we perform scalar quantization with two 8-bit codebooks and group size
    8 and channel-wise steps. This was originally published in [[19](#bib.bib19)]
    as the “speed-optimized” configuration capable of fast lookup-based inference.
    During P step, we update both codebooks, as well as scales an non-quantized parameters
    by backprop. On V step, we run beam search with beam size 8 with gradient-updated
    dequantized weight as target.'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: AQLM：我们使用两个8位代码本和组大小为8以及按通道的步长进行标量量化。这最初在[[19](#bib.bib19)]中作为“速度优化”配置发布，能够进行快速查找的推断。在P步骤中，我们更新两个代码本以及通过反向传播更新标量和未量化的参数。在V步骤中，我们使用梯度更新的去量化权重作为目标运行大小为8的束搜索。
- en: Training. We minimize Kullback–Leibler divergence as our loss function for all
    three representations. More specifically, we fine-tune the quantized “student”
    model to approximate the predictions (logits) of a “teacher” — the same model
    prior to quantization. We fine-tune on the same RedPajama sample as in calibration.
    More specifically, we use the official one-billion-token sample^(11)^(11)11https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample
    provided by the dataset authors [[11](#bib.bib11)]. We use a batch size of $2^{20}$
    smaller batches.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 训练。我们将Kullback–Leibler散度最小化作为所有三种表示的损失函数。更具体地说，我们对量化后的“学生”模型进行微调，以近似“教师”模型的预测（logits）——即量化前的同一模型。我们在与校准相同的RedPajama样本上进行微调。更具体地说，我们使用由数据集作者提供的官方十亿令牌样本^(11)^(11)11https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample。我们使用批量大小为$2^{20}$的较小批量。
- en: 'Hyperparameter tuning: we tune the hyperparameters for each method individually.
    For all algorithms, we tune learning rate on a logarithmic scale out of (1e-4,
    3e-4, 1e-3, 3e-3, 1e-2). For methods involving discrete optimization, we tune
    learning rate for P and V step separately. The optimal configuration for STE and
    stochastic rounding is to use learning rate 3e-4 for both codes and codebooks.
    The optimal configuration for subspace linearized PV and the same with STE is
    to use learning rate 3e-4 for P step and 3e-3 for codes. Curiously, the subspace
    methods are stable even with larger step sizes for codes, e.g. 1e-2, whereas unrestricted
    methods (e.g. pure STE) are not.'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整：我们单独调整每种方法的超参数。对于所有算法，我们在（1e-4, 3e-4, 1e-3, 3e-3, 1e-2）范围内对学习率进行对数刻度调整。对于涉及离散优化的方法，我们分别调整P步骤和V步骤的学习率。STE和随机舍入的最佳配置是对代码和代码本都使用学习率3e-4。子空间线性化PV及其与STE相同的最佳配置是对P步骤使用学习率3e-4，对代码使用学习率3e-3。有趣的是，即使对于代码使用较大的步长（例如1e-2），子空间方法仍然稳定，而不受限制的方法（例如纯STE）则不稳定。
- en: 'For stochastic rounding, we found that the unbiased rounding [[49](#bib.bib49)]
    causes the model quality to quickly degrade, likely due to the fact that the algorithm
    makes too many weight changes due to rounding. The results we reported in Table [1](#S4.T1
    "Table 1 ‣ 4.1 Evaluating quantized representations with finetuning ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    use stochastic rounding with temperature 0.2\. In other words, we measure the
    distances to 2 nearest bins and round to each bin proportionally to $\text{distance}^{-5}$.
    We also tried gradually annealing the rounding temperature to 0, but achieved
    only insignificant improvements in accuracy and perplexity (<0.01). To simplify
    evaluation, we do not use annealing in Table[1](#S4.T1 "Table 1 ‣ 4.1 Evaluating
    quantized representations with finetuning ‣ 4 Experiments ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression").'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机舍入，我们发现无偏舍入 [[49](#bib.bib49)] 导致模型质量迅速下降，可能是因为该算法由于舍入而发生了过多的权重变化。我们在表[1](#S4.T1
    "表 1 ‣ 4.1 评估量化表示与微调 ‣ 4 实验 ‣ PV 调整：超越直通估计以极端 LLM 压缩")中报告的结果使用了温度为 0.2 的随机舍入。换句话说，我们测量距离
    2 个最近箱的距离，并按 $\text{distance}^{-5}$ 比例舍入到每个箱子。我们还尝试逐渐将舍入温度退火到 0，但仅取得了准确度和困惑度(<0.01)上的微小改善。为了简化评估，我们在表[1](#S4.T1
    "表 1 ‣ 4.1 评估量化表示与微调 ‣ 4 实验 ‣ PV 调整：超越直通估计以极端 LLM 压缩")中不使用退火。
- en: For PV-tuning and PV-tuning with STE, we always set $\tau$. However, we didn’t
    see this in practice.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 PV 调整和带有 STE 的 PV 调整，我们总是设置 $\tau$。然而，我们在实践中并没有看到这一点。
- en: We also found that Lamb^(12)^(12)12Lamb is a variant of Adam that limits the
    norm of weight updates relative to the norm of weights. [[75](#bib.bib75)] is
    more stable when training with large batch sizes, but converges to approximately
    the same accuracy. We use $\beta_{1}{=}0.9$, same as in most LLM training configurations [[63](#bib.bib63),
    [79](#bib.bib79), [56](#bib.bib56)]. We do not use learning rate decay for simplicity.
    It is likely possibly to improve our results by annealing the learning rates during
    training or using a warmup. We intentionally avoid this to reduce the number of
    “moving parts” and simplify evaluation. In future, we plan to release PV-tuned
    models with a more careful choice of training hyperparameters. Overall, we found
    that PV-tuning is about as sensitive to hyperparameters as continuous-only LLM
    finetuning [[65](#bib.bib65), [19](#bib.bib19), [58](#bib.bib58)].
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还发现 Lamb^(12)^(12)12Lamb 是 Adam 的一个变体，它限制了权重更新的范数相对于权重的范数。[[75](#bib.bib75)]
    在使用大批量训练时更为稳定，但收敛到大致相同的准确度。我们使用 $\beta_{1}{=}0.9$，与大多数 LLM 训练配置相同 [[63](#bib.bib63),
    [79](#bib.bib79), [56](#bib.bib56)]。为了简化起见，我们不使用学习率衰减。通过在训练过程中退火学习率或使用预热，可能会改善我们的结果。我们故意避免这样做以减少“活动部件”的数量，并简化评估。未来，我们计划发布具有更精细训练超参数选择的
    PV 调整模型。总体而言，我们发现 PV 调整对超参数的敏感性与仅连续 LLM 微调的敏感性大致相当 [[65](#bib.bib65), [19](#bib.bib19),
    [58](#bib.bib58)]。
- en: 'Appendix L Additional Details and Evaluations for Section [4.3](#S4.SS3 "4.3
    Large-scale Evaluation & Discussion ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")'
  id: totrans-541
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 L 额外细节和第[4.3](#S4.SS3 "4.3 大规模评估与讨论 ‣ 4 实验 ‣ PV 调整：超越直通估计以极端 LLM 压缩")节的评估
- en: In this section, we report additional results for Llama 2 & 3, Mistral and Phi-3
    and discuss baselines. In this section, we always evaluate PV-tuning for vector
    quantization, using 14-16 bits per codebook for a group of 8 or 16 weights, with
    each combination fitting a particualr niche. For instance, 16 bits per 8 weights
    is slightly over 2 bits per weight, whereas 14 bits per 16 weights is either at
    or below 1 bit per weight, depending on the model size.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们报告了 Llama 2 & 3、Mistral 和 Phi-3 的额外结果，并讨论了基准。在本节中，我们始终评估 PV 调整的向量量化，使用每个代码本
    14-16 位用于一组 8 或 16 个权重，每种组合适合特定的细分市场。例如，8 个权重的 16 位略多于每个权重 2 位，而 16 个权重的 14 位则根据模型大小处于每个权重
    1 位或以下。
- en: 'We use Llama 2 models as our main benchmark as they are well studied in the
    PTQ community. Here, we gather the latest state-of-the-art algorithms at the time
    of publication and group them according to their target number of bits, roughly
    1-1.7 bits per weight (Table [7](#A12.T7 "Table 7 ‣ Appendix L Additional Details
    and Evaluations for Section 4.3 ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) and 2-2.5 bits per weight (Table [8](#A12.T8
    "Table 8 ‣ Appendix L Additional Details and Evaluations for Section 4.3 ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用 Llama 2 模型作为主要基准，因为它们在 PTQ 社区中得到了广泛研究。在这里，我们收集了截至出版时最新的最先进算法，并根据其目标比特数进行分组，大致为每个权重
    1-1.7 比特（表 [7](#A12.T7 "Table 7 ‣ Appendix L Additional Details and Evaluations
    for Section 4.3 ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for
    Extreme LLM Compression")）和每个权重 2-2.5 比特（表 [8](#A12.T8 "Table 8 ‣ Appendix L Additional
    Details and Evaluations for Section 4.3 ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")）。'
- en: Both our training runs and almost all baselines use the same sample of RedPajama
    data from previous sections^(13)^(13)13[https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample).
    The only exception to this is OneBit that uses a corpora of LLM outputs gathered
    specifically for that paper [[71](#bib.bib71)].
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练运行和几乎所有基线使用来自前面章节的相同 RedPajama 数据样本 ^(13)^(13)13[https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample)。唯一的例外是
    OneBit，它使用了专门为该论文收集的 LLM 输出语料库 [[71](#bib.bib71)]。
- en: The source code for this method was unavailable until very recently. The official
    link [https://github.com/xuyuzhuang11/OneBit](https://github.com/xuyuzhuang11/OneBit)
    used to point to an empty repository until the code was released in a commit [https://github.com/xuyuzhuang11/OneBit/commit/380a6aedc3c060993056ff50b79065e893be99ae](https://github.com/xuyuzhuang11/OneBit/commit/380a6aedc3c060993056ff50b79065e893be99ae)
    on May 10th. Thus, unfortunately, we did not have time to make OneBit compatible
    with models except Llama 2 7B and 13B that were featured in the original paper.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的源代码直到最近才公开。官方链接 [https://github.com/xuyuzhuang11/OneBit](https://github.com/xuyuzhuang11/OneBit)
    曾指向一个空的代码库，直到代码在 5 月 10 日发布于 [https://github.com/xuyuzhuang11/OneBit/commit/380a6aedc3c060993056ff50b79065e893be99ae](https://github.com/xuyuzhuang11/OneBit/commit/380a6aedc3c060993056ff50b79065e893be99ae)
    的提交中。因此，不幸的是，我们没有时间将 OneBit 使其兼容除原文中的 Llama 2 7B 和 13B 之外的模型。
- en: For Llama 3, we evaluate PV-tuning of vector quantization against the baselines
    introduced in [[30](#bib.bib30)]. Curiously, their paper seems to compute perplexity
    differently than our paper. Since our protocol matches with most prior works [[20](#bib.bib20),
    [16](#bib.bib16), [19](#bib.bib19), [65](#bib.bib65)], we chose to re-evaluate
    the results from [[30](#bib.bib30)] with our perplexity code and not the other
    way around. We calibrate using the official code ^(14)^(14)14[https://github.com/Macaronlin/LLaMA3-Quantization](https://github.com/Macaronlin/LLaMA3-Quantization)
    and reuse published models where available.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Llama 3，我们评估了向量量化的 PV-tuning 对比 [[30](#bib.bib30)] 中介绍的基线。奇怪的是，他们的论文似乎以不同于我们论文的方式计算困惑度。由于我们的协议与大多数先前的工作
    [[20](#bib.bib20), [16](#bib.bib16), [19](#bib.bib19), [65](#bib.bib65)] 一致，我们选择用我们的困惑度代码重新评估
    [[30](#bib.bib30)] 的结果，而不是反向操作。我们使用官方代码进行校准 ^(14)^(14)14[https://github.com/Macaronlin/LLaMA3-Quantization](https://github.com/Macaronlin/LLaMA3-Quantization)
    并在有发布模型的情况下重复使用。
- en: 'Table 7: Evaluation of quantized Llama 2 models for 1-1.7 bits per weight,
    grouped by bitwidth. We report perplexity on WikiText2 [[42](#bib.bib42)] & C4 [[50](#bib.bib50)]
    and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks.
    Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy).'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：按比特宽度分组的 1-1.7 比特每权重量化 Llama 2 模型的评估。我们报告了 WikiText2 [[42](#bib.bib42)]
    和 C4 [[50](#bib.bib50)] 上的困惑度以及零-shot 准确度。平均值是 5 个零-shot 任务的平均准确度。主要指标是 Wiki2
    (PPL)、C4 (PPL) 和平均值 (准确度)。
- en: '| Size | Method | Avg bits | Wiki2$\downarrow$ |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| 尺寸 | 方法 | 平均比特 | Wiki2$\downarrow$ |'
- en: '| 7B | – | 16.00 | 5.12 | 6.63 | 43.43 | 76.3 | 57.14 | 78.07 | 69.06 | 64.80
    |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| 7B | – | 16.00 | 5.12 | 6.63 | 43.43 | 76.3 | 57.14 | 78.07 | 69.06 | 64.80
    |'
- en: '| BiLLM | 1.08 | 32.48 | 40.52 | 24.4 | 36.2 | 34.8 | 60.6 | 52.4 | 41.68 |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM | 1.08 | 32.48 | 40.52 | 24.4 | 36.2 | 34.8 | 60.6 | 52.4 | 41.68 |'
- en: '| OneBit | 1.0 | 9.73 | 11.11 | 29.61 | 41.58 | 52.58 | 68.12 | 58.41 | 50.06
    |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| OneBit | 1.0 | 9.73 | 11.11 | 29.61 | 41.58 | 52.58 | 68.12 | 58.41 | 50.06
    |'
- en: '| PV-Tuning | 1.02 | 8.28 | 10.37 | 25.85 | 57.58 | 40.88 | 68.99 | 57.77 |
    50.21 |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.02 | 8.28 | 10.37 | 25.85 | 57.58 | 40.88 | 68.99 | 57.77 |
    50.21 |'
- en: '| PB-LLM | 1.70 | 69.20 | 80.15 | 25.00 | 28.00 | 27.70 | 53.80 | 49.30 | 36.76
    |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM | 1.70 | 69.20 | 80.15 | 25.00 | 28.00 | 27.70 | 53.80 | 49.30 | 36.76
    |'
- en: '| PV-Tuning | 1.58 | 7.32 | 9.35 | 29.44 | 64.14 | 46.03 | 73.12 | 63.38 |
    55.22 |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.58 | 7.32 | 9.35 | 29.44 | 64.14 | 46.03 | 73.12 | 63.38 |
    55.22 |'
- en: '| 13B | – | 16.00 | 4.57 | 6.05 | 48.38 | 79.42 | 60.03 | 79.05 | 72.22 | 67.82
    |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| 13B | – | 16.00 | 4.57 | 6.05 | 48.38 | 79.42 | 60.03 | 79.05 | 72.22 | 67.82
    |'
- en: '| BiLLM | 1.10 | 16.77 | 27.54 | 21.84 | 46.84 | 30.97 | 60.61 | 56.75 | 43.40
    |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM | 1.10 | 16.77 | 27.54 | 21.84 | 46.84 | 30.97 | 60.61 | 56.75 | 43.40
    |'
- en: '| OneBit | 1.00 | 8.76 | 10.15 | 33.62 | 43.10 | 56.43 | 70.13 | 61.72 | 53.00
    |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| OneBit | 1.00 | 8.76 | 10.15 | 33.62 | 43.10 | 56.43 | 70.13 | 61.72 | 53.00
    |'
- en: '| PV-Tuning | 0.97 | 7.23 | 9.31 | 30.8 | 63.09 | 47.03 | 72.25 | 62.35 | 55.10
    |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 0.97 | 7.23 | 9.31 | 30.8 | 63.09 | 47.03 | 72.25 | 62.35 | 55.10
    |'
- en: '| PB-LLM | 1.70 | 151.09 | 144.59 | 21.89 | 35.08 | 24.82 | 54.17 | 52.76 |
    37.74 |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM | 1.70 | 151.09 | 144.59 | 21.89 | 35.08 | 24.82 | 54.17 | 52.76 |
    37.74 |'
- en: '| PV-Tuning | 1.37 | 6.65 | 8.72 | 34.04 | 67.38 | 49.14 | 73.94 | 65.51 |
    58.00 |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.37 | 6.65 | 8.72 | 34.04 | 67.38 | 49.14 | 73.94 | 65.51 |
    58.00 |'
- en: '| 70B | – | 16.00 | 3.12 | 4.97 | 54.35 | 82.74 | 64.79 | 82.15 | 77.98 | 72.40
    |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| 70B | – | 16.00 | 3.12 | 4.97 | 54.35 | 82.74 | 64.79 | 82.15 | 77.98 | 72.40
    |'
- en: '| BiLLM | 1.08 | 8.41 | 15.19 | 38.91 | 67.3 | 45.71 | 69.7 | 67.64 | 57.85
    |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM | 1.08 | 8.41 | 15.19 | 38.91 | 67.3 | 45.71 | 69.7 | 67.64 | 57.85
    |'
- en: '| PV-Tuning | 1.01 | 6.09 | 8.20 | 38.31 | 71.80 | 53.98 | 75.24 | 68.43 |
    61.55 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.01 | 6.09 | 8.20 | 38.31 | 71.80 | 53.98 | 75.24 | 68.43 |
    61.55 |'
- en: '| PB-LLM | 1.70 | 28.37 | 32.63 | 39.89 | 49.50 | 36.62 | 61.43 | 62.80 | 50.05
    |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM | 1.70 | 28.37 | 32.63 | 39.89 | 49.50 | 36.62 | 61.43 | 62.80 | 50.05
    |'
- en: '| PV-Tuning | 1.14 | 5.52 | 7.50 | 42.66 | 74.96 | 56.42 | 77.37 | 71.51 |
    64.58 |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.14 | 5.52 | 7.50 | 42.66 | 74.96 | 56.42 | 77.37 | 71.51 |
    64.58 |'
- en: 'Table 8: Evaluation of quantized Llama 2 models for 2-2.3 bits per weight,
    grouped by bitwidth. We report perplexity on WikiText2 [[42](#bib.bib42)] & C4 [[50](#bib.bib50)]
    and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks.
    Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy).'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：按位宽分组的量化 Llama 2 模型评估。我们报告了 WikiText2 [[42](#bib.bib42)] 和 C4 [[50](#bib.bib50)]
    上的困惑度和零样本准确性。平均值是 5 个零样本任务的平均准确性。主要指标是 Wiki2 (PPL)、C4 (PPL) 和平均值 (准确性)。
- en: '| Size | Method | Avg bits | Wiki2$\downarrow$ |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 方法 | 平均位数 | Wiki2$\downarrow$ |'
- en: '| 7B | – | 16.00 | 5.12 | 6.63 | 43.43 | 76.30 | 57.14 | 78.07 | 69.06 | 64.80
    |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| 7B | – | 16.00 | 5.12 | 6.63 | 43.43 | 76.30 | 57.14 | 78.07 | 69.06 | 64.80
    |'
- en: '| QUIP# | 2.02 | 8.22 | 11.01 | 34.60 | 64.60 | 48.34 | 75.10 | 64.90 | 57.51
    |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| QUIP# | 2.02 | 8.22 | 11.01 | 34.60 | 64.60 | 48.34 | 75.10 | 64.90 | 57.51
    |'
- en: '| AQLM | 2.02 | 6.64 | 8.56 | 33.28 | 61.87 | 49.49 | 73.56 | 64.17 | 56.47
    |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 2.02 | 6.64 | 8.56 | 33.28 | 61.87 | 49.49 | 73.56 | 64.17 | 56.47
    |'
- en: '| PV-Tuning | 2.02 | 5.84 | 7.62 | 38.40 | 71.17 | 53.50 | 76.99 | 66.69 |
    61.35 |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.02 | 5.84 | 7.62 | 38.40 | 71.17 | 53.50 | 76.99 | 66.69 |
    61.35 |'
- en: '| AQLM | 2.29 | 6.29 | 8.11 | 34.90 | 66.50 | 50.88 | 74.92 | 65.67 | 58.57
    |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 2.29 | 6.29 | 8.11 | 34.90 | 66.50 | 50.88 | 74.92 | 65.67 | 58.57
    |'
- en: '| PV-Tuning | 2.29 | 5.84 | 7.62 | 38.91 | 72.90 | 53.94 | 77.37 | 67.72 |
    62.17 |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.29 | 5.84 | 7.62 | 38.91 | 72.90 | 53.94 | 77.37 | 67.72 |
    62.17 |'
- en: '| 13B | – | 16.00 | 4.57 | 6.05 | 48.38 | 79.42 | 60.03 | 79.05 | 72.22 | 67.82
    |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| 13B | – | 16.00 | 4.57 | 6.05 | 48.38 | 79.42 | 60.03 | 79.05 | 72.22 | 67.82
    |'
- en: '| QUIP# | 2.01 | 6.06 | 8.07 | 39.50 | 69.30 | 53.44 | 77.30 | 67.70 | 61.45
    |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| QUIP# | 2.01 | 6.06 | 8.07 | 39.50 | 69.30 | 53.44 | 77.30 | 67.70 | 61.45
    |'
- en: '| AQLM | 1.97 | 5.65 | 7.51 | 37.80 | 69.78 | 53.74 | 76.22 | 65.43 | 60.59
    |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 1.97 | 5.65 | 7.51 | 37.80 | 69.78 | 53.74 | 76.22 | 65.43 | 60.59
    |'
- en: '| PV-Tuning | 1.97 | 5.12 | 6.83 | 43.00 | 75.38 | 57.96 | 78.24 | 70.01 |
    64.92 |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.97 | 5.12 | 6.83 | 43.00 | 75.38 | 57.96 | 78.24 | 70.01 |
    64.92 |'
- en: '| AQLM | 2.19 | 5.41 | 7.21 | 41.98 | 75.04 | 55.49 | 76.99 | 69.53 | 63.81
    |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 2.19 | 5.41 | 7.21 | 41.98 | 75.04 | 55.49 | 76.99 | 69.53 | 63.81
    |'
- en: '| PV-Tuning | 2.19 | 5.05 | 6.74 | 45.65 | 77.57 | 58.00 | 78.07 | 70.96 |
    66.05 |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.19 | 5.05 | 6.74 | 45.65 | 77.57 | 58.00 | 78.07 | 70.96 |
    66.05 |'
- en: '| 70B | – | 16.00 | 3.12 | 4.97 | 54.35 | 82.74 | 64.79 | 82.15 | 77.98 | 72.40
    |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| 70B | – | 16.00 | 3.12 | 4.97 | 54.35 | 82.74 | 64.79 | 82.15 | 77.98 | 72.40
    |'
- en: '| QUIP# | 2.00 | 4.16 | 6.01 | 48.70 | 77.30 | 60.79 | 80.30 | 75.90 | 68.60
    |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| QUIP# | 2.00 | 4.16 | 6.01 | 48.70 | 77.30 | 60.79 | 80.30 | 75.90 | 68.60
    |'
- en: '| AQLM | 2.07 | 3.94 | 5.72 | 51.96 | 81.44 | 61.46 | 80.25 | 76.64 | 70.35
    |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 2.07 | 3.94 | 5.72 | 51.96 | 81.44 | 61.46 | 80.25 | 76.64 | 70.35
    |'
- en: '| PV-Tuning | 2.07 | 3.78 | 5.56 | 51.88 | 81.02 | 63.07 | 80.74 | 76.87 |
    70.72 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.07 | 3.78 | 5.56 | 51.88 | 81.02 | 63.07 | 80.74 | 76.87 |
    70.72 |'
- en: 'Table 9: Evaluation of quantized Llama 3 models for 1-2.3 bits per weight,
    grouped by bitwidth. We report perplexity on WikiText2 [[42](#bib.bib42)] & C4 [[50](#bib.bib50)]
    and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks.
    Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy).'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：针对1-2.3位每权重的量化Llama 3模型的评估，按位宽分组。我们报告了在WikiText2 [[42](#bib.bib42)] 和C4 [[50](#bib.bib50)]上的困惑度和零-shot准确率。平均值是5个零-shot任务的平均准确率。主要指标是Wiki2
    (PPL)、C4 (PPL) 和平均 (准确率)。
- en: '| Size | Method | Avg bits | Wiki2$\downarrow$ |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 方法 | 平均位数 | Wiki2$\downarrow$ |'
- en: '| 8B | – | 16.00 | 5.54 | 7.10 | 50.43 | 80.09 | 60.19 | 79.71 | 72.61 | 68.60
    |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| 8B | – | 16.00 | 5.54 | 7.10 | 50.43 | 80.09 | 60.19 | 79.71 | 72.61 | 68.60
    |'
- en: '| BiLLM | 1.10 | 28.80 | 65.00 | 17.70 | 36.00 | 28.90 | 56.10 | 51.00 | 37.90
    |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM | 1.10 | 28.80 | 65.00 | 17.70 | 36.00 | 28.90 | 56.10 | 51.00 | 37.90
    |'
- en: '| PV-Tuning | 1.01 | 11.13 | 11.63 | 25.43 | 59.09 | 41.01 | 68.26 | 56.27
    | 50.01 |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.01 | 11.13 | 11.63 | 25.43 | 59.09 | 41.01 | 68.26 | 56.27
    | 50.01 |'
- en: '| PB-LLM | 1.70 | 35.68 | 197.56 | 17.50 | 31.70 | 27.70 | 52.50 | 50.40 |
    36.00 |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM | 1.70 | 35.68 | 197.56 | 17.50 | 31.70 | 27.70 | 52.50 | 50.40 |
    36.00 |'
- en: '| PV-Tuning | 1.54 | 9.43 | 10.26 | 32.68 | 65.78 | 46.66 | 72.63 | 64.40 |
    56.43 |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.54 | 9.43 | 10.26 | 32.68 | 65.78 | 46.66 | 72.63 | 64.40 |
    56.43 |'
- en: '| QuIP | 2.00 | 76.95 | 98.47 | 21.30 | 29.00 | 29.20 | 52.90 | 51.70 | 36.80
    |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 2.00 | 76.95 | 98.47 | 21.30 | 29.00 | 29.20 | 52.90 | 51.70 | 36.80
    |'
- en: '| PB-LLM | 2.00 | 21.74 | 61.04 | 17.20 | 37.80 | 29.80 | 57.00 | 52.50 | 38.80
    |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM | 2.00 | 21.74 | 61.04 | 17.20 | 37.80 | 29.80 | 57.00 | 52.50 | 38.80
    |'
- en: '| DB-LLM | 2.00 | 12.77 | 14.82 | 28.20 | 59.10 | 42.10 | 68.90 | 60.40 | 51.80
    |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| DB-LLM | 2.00 | 12.77 | 14.82 | 28.20 | 59.10 | 42.10 | 68.90 | 60.40 | 51.80
    |'
- en: '| PV-Tuning | 2.00 | 6.99 | 8.29 | 42.75 | 75.84 | 55.52 | 77.75 | 69.93 |
    64.36 |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.00 | 6.99 | 8.29 | 42.75 | 75.84 | 55.52 | 77.75 | 69.93 |
    64.36 |'
- en: '| PV-Tuning | 2.30 | 6.76 | 8.10 | 42.32 | 75.46 | 56.21 | 78.45 | 71.67 |
    64.82 |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.30 | 6.76 | 8.10 | 42.32 | 75.46 | 56.21 | 78.45 | 71.67 |
    64.82 |'
- en: '| 70B | – | 16.00 | 2.59 | 5.78 | 60.41 | 86.7 | 66.34 | 82.48 | 80.9 | 75.366
    |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| 70B | – | 16.00 | 2.59 | 5.78 | 60.41 | 86.7 | 66.34 | 82.48 | 80.9 | 75.366
    |'
- en: '| BiLLM | 1.10 | 15.26 | 65.07 | 25.11 | 46.42 | 37.48 | 58.21 | 53.63 | 44.17
    |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM | 1.10 | 15.26 | 65.07 | 25.11 | 46.42 | 37.48 | 58.21 | 53.63 | 44.17
    |'
- en: '| PV-Tuning | 1.00 | 8.67 | 9.68 | 25.51 | 54.34 | 48.71 | 65.56 | 63.22 |
    51.47 |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.00 | 8.67 | 9.68 | 25.51 | 54.34 | 48.71 | 65.56 | 63.22 |
    51.47 |'
- en: '| PB-LLM | 1.70 | 16.27 | 54.03 | 25.8 | 49.90 | 34.90 | 56.5 | 53.10 | 44.1
    |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM | 1.70 | 16.27 | 54.03 | 25.8 | 49.90 | 34.90 | 56.5 | 53.10 | 44.1
    |'
- en: '| PV-Tuning | 1.14 | 7.76 | 8.93 | 33.28 | 63.89 | 53.39 | 69.86 | 69.61 |
    58.01 |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.14 | 7.76 | 8.93 | 33.28 | 63.89 | 53.39 | 69.86 | 69.61 |
    58.01 |'
- en: '| QuIP | 2.00 | 11.63 | 18.54 | 26.50 | 48.90 | 40.90 | 65.30 | 61.70 | 48.70
    |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 2.00 | 11.63 | 18.54 | 26.50 | 48.90 | 40.90 | 65.30 | 61.70 | 48.70
    |'
- en: '| PB-LLM | 2.00 | 10.33 | 28.89 | 25.10 | 40.60 | 42.70 | 65.20 | 56.40 | 46.00
    |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM | 2.00 | 10.33 | 28.89 | 25.10 | 40.60 | 42.70 | 65.20 | 56.40 | 46.00
    |'
- en: '| PV-Tuning | 2.07 | 4.55 | 6.54 | 50.77 | 80.22 | 63.85 | 79.22 | 78.06 |
    70.42 |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.07 | 4.55 | 6.54 | 50.77 | 80.22 | 63.85 | 79.22 | 78.06 |
    70.42 |'
- en: 'Table 10: Evaluation of quantized Mistral v0.1 7B (A) and Phi 3-Mini-4k-Instruct
    3.8B (B) models for 1-2.3 bits per weight, grouped by bitwidth. We report perplexity
    on WikiText2 [[42](#bib.bib42)] & C4 [[50](#bib.bib50)] and zero-shot accuracy.
    The Average is the mean accuracy of 5 zero-shot tasks. Primary metrics are Wiki2
    (PPL), C4 (PPL) and Average (Accuracy).'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：针对1-2.3位每权重的量化Mistral v0.1 7B (A)和Phi 3-Mini-4k-Instruct 3.8B (B)模型的评估，按位宽分组。我们报告了在WikiText2
    [[42](#bib.bib42)] 和C4 [[50](#bib.bib50)]上的困惑度和零-shot准确率。平均值是5个零-shot任务的平均准确率。主要指标是Wiki2
    (PPL)、C4 (PPL) 和平均 (准确率)。
- en: '| Size | Method | Avg bits | Wiki2$\downarrow$ |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 方法 | 平均位数 | Wiki2$\downarrow$ |'
- en: '| $\underset{(A)}{\text{7B}}$ | – | 16.00 | 4.77 | 5.71 | 50.43 | 80.09 | 60.19
    | 79.71 | 72.61 | 68.60 |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| $\underset{(A)}{\text{7B}}$ | – | 16.00 | 4.77 | 5.71 | 50.43 | 80.09 | 60.19
    | 79.71 | 72.61 | 68.60 |'
- en: '| AQLM | 1.01 | 70.88 | 34.67 | 19.11 | 27.36 | 26.3 | 52.99 | 48.38 | 34.83
    |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 1.01 | 70.88 | 34.67 | 19.11 | 27.36 | 26.3 | 52.99 | 48.38 | 34.83
    |'
- en: '| PV-Tuning | 1.01 | 7.58 | 8.14 | 27.73 | 60.82 | 44.33 | 69.97 | 60.38 |
    52.65 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.01 | 7.58 | 8.14 | 27.73 | 60.82 | 44.33 | 69.97 | 60.38 |
    52.65 |'
- en: '| QuIP# | 2.01 | 6.02 | 6.84 | 39.76 | 72.14 | 52.95 | 76.71 | 69.30 | 62.20
    |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# | 2.01 | 6.02 | 6.84 | 39.76 | 72.14 | 52.95 | 76.71 | 69.30 | 62.20
    |'
- en: '| AQLM | 2.01 | 6.19 | 6.90 | 30.8 | 49.87 | 25.63 | 56.53 | 57.06 | 43.98
    |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 2.01 | 6.19 | 6.90 | 30.8 | 49.87 | 25.63 | 56.53 | 57.06 | 43.98
    |'
- en: '| PV-Tuning | 2.01 | 5.29 | 6.17 | 44.20 | 77.36 | 58.21 | 79.05 | 72.77 |
    66.32 |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.01 | 5.29 | 6.17 | 44.20 | 77.36 | 58.21 | 79.05 | 72.77 |
    66.32 |'
- en: '| AQLM | 2.27 | 5.78 | 6.55 | 42.06 | 75.17 | 55.09 | 76.93 | 70.24 | 63.90
    |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 2.27 | 5.78 | 6.55 | 42.06 | 75.17 | 55.09 | 76.93 | 70.24 | 63.90
    |'
- en: '| PV-Tuning | 2.27 | 5.22 | 6.10 | 45.31 | 77.57 | 58.61 | 79.22 | 70.96 |
    66.33 |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.27 | 5.22 | 6.10 | 45.31 | 77.57 | 58.61 | 79.22 | 70.96 |
    66.33 |'
- en: '| $\underset{(B)}{\text{3.8B}}$ | – | 16.00 | 4.77 | 5.71 | 60.41 | 86.7 |
    66.34 | 82.48 | 80.90 | 75.37 |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| $\underset{(B)}{\text{3.8B}}$ | – | 16.00 | 4.77 | 5.71 | 60.41 | 86.7 |
    66.34 | 82.48 | 80.90 | 75.37 |'
- en: '| AQLM | 1.03 | 102.54 | 85.20 | 18.86 | 28.16 | 27.13 | 53.54 | 49.80 | 35.50
    |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 1.03 | 102.54 | 85.20 | 18.86 | 28.16 | 27.13 | 53.54 | 49.80 | 35.50
    |'
- en: '| PV-Tuning | 1.03 | 11.71 | 14.59 | 21.50 | 49.87 | 34.62 | 65.67 | 54.70
    | 45.27 |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.03 | 11.71 | 14.59 | 21.50 | 49.87 | 34.62 | 65.67 | 54.70
    | 45.27 |'
- en: '| AQLM | 1.60 | 34.36 | 35.16 | 19.62 | 35.10 | 29.71 | 57.24 | 51.7 | 38.67
    |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 1.60 | 34.36 | 35.16 | 19.62 | 35.10 | 29.71 | 57.24 | 51.7 | 38.67
    |'
- en: '| PV-Tuning | 1.60 | 9.21 | 12.16 | 30.89 | 63.55 | 43.09 | 70.35 | 61.4 |
    53.86 |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 1.60 | 9.21 | 12.16 | 30.89 | 63.55 | 43.09 | 70.35 | 61.4 |
    53.86 |'
- en: '| AQLM | 2.03 | 8.85 | 12.19 | 41.04 | 74.49 | 47.25 | 72.36 | 66.93 | 60.41
    |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 2.03 | 8.85 | 12.19 | 41.04 | 74.49 | 47.25 | 72.36 | 66.93 | 60.41
    |'
- en: '| PV-Tuning | 2.03 | 6.88 | 10.08 | 46.84 | 78.24 | 53.75 | 78.67 | 71.03 |
    65.71 |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.03 | 6.88 | 10.08 | 46.84 | 78.24 | 53.75 | 78.67 | 71.03 |
    65.71 |'
- en: '| AQLM | 2.30 | 8.07 | 11.23 | 47.01 | 78.03 | 50.51 | 75.95 | 70.8 | 64.46
    |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 2.30 | 8.07 | 11.23 | 47.01 | 78.03 | 50.51 | 75.95 | 70.8 | 64.46
    |'
- en: '| PV-Tuning | 2.30 | 6.63 | 9.89 | 50.51 | 79.5 | 55.32 | 79.49 | 73.01 | 67.57
    |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| PV-Tuning | 2.30 | 6.63 | 9.89 | 50.51 | 79.5 | 55.32 | 79.49 | 73.01 | 67.57
    |'
- en: Appendix M Inference Speed with Vector Quantization Kernels
  id: totrans-623
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 M 使用向量量化内核的推理速度
- en: 'In this section, we demonstrate that PV-Tuning can achieve speedups by using
    fast inference kernels from the underlying quantized representation. Since our
    main experiments use vector quantization, we adopt simplified version of AQLM
    inference kernels with one codebook of size 16 for groups of 8 consecutive weights.
    This kernel is not written by us: it was added to the official AQLM implementation
    by an open-source contributor.'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们演示了 PV-Tuning 通过使用来自基础量化表示的快速推理内核来实现加速。由于我们的主要实验使用向量量化，我们采用了简化版的 AQLM
    推理内核，该内核具有一个大小为 16 的代码本，用于 8 个连续权重的组。这个内核不是我们编写的：它是由一个开源贡献者添加到官方 AQLM 实现中的。
- en: We adapt this inference code to our codebase and switch it to using group size
    16 to support out 1.1-1.58 bit models. We evaluate inference speeds on a single
    Nvidia RTX 3090 GPU using transformers with cuda graphs^(15)^(15)15The specific
    version of each library can be found in ‘requirements.txt‘.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个推理代码适配到我们的代码库，并将其切换到使用 16 的组大小，以支持 1.1-1.58 位模型。我们使用具有 cuda 图的 transformers
    在单个 Nvidia RTX 3090 GPU 上评估推理速度^(15)^(15)15 每个库的具体版本可以在‘requirements.txt’中找到。
- en: 'We were able to acheve 47.4 tokens per second for 7B model, 32.8 tokens per
    second for 13B model and 7.2 tokens per second for Llama 2 70B model. Compared
    to 16-bit inference code, this results in speedups of 14%, 22% and 28% respectively.
    Note that PV-Tuning does not make the model inherently faster than, for instance,
    AQLM. However, it can enable faster inference indirectly: by achieving sufficient
    quality with lower bit models and allowing practitioners to deploy smaller and
    faster models.'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够在 7B 模型下实现每秒 47.4 个 token，在 13B 模型下实现每秒 32.8 个 token，在 Llama 2 70B 模型下实现每秒
    7.2 个 token。与 16 位推理代码相比，这分别带来了 14%、22% 和 28% 的加速。需要注意的是，PV-Tuning 并不会使模型本身比例如
    AQLM 更快。然而，它可以通过实现足够的质量并允许从业者部署更小更快的模型，从而间接实现更快的推理。
- en: Appendix N The Choice of the Initial Point $x^{0}$
  id: totrans-627
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 N 初始点 $x^{0}$ 的选择
- en: 'Algorithm [1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression") converges from any initial point $x^{0}\in\mathbb{R}^{d}_{c}$. In
    this section, we discuss two possible variants of instantiating the initial point.'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 [1](#alg1 "算法 1 ‣ 3.1 问题描述 ‣ 3 微调量化模型 ‣ PV-Tuning: 超越直通估计的极端 LLM 压缩") 可以从任何初始点
    $x^{0}\in\mathbb{R}^{d}_{c}$ 收敛。在本节中，我们讨论了初始化点的两种可能变体。'
- en: N.1 Clipping of $x^{\star}$
  id: totrans-629
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: N.1 $x^{\star}$ 的裁剪
- en: 'Notation: $[d]:=\{1,\cdots,d\}$.'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 符号： $[d]:=\{1,\cdots,d\}$。
- en: 'Assume we have the vector $x$ in the following way:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有如下的向量 $x$：
- en: '|  | $C(x)=\tilde{x}:\quad V(\tilde{x})\subseteq V(x)\quad\text{and}\quad&#124;V(\tilde{x})&#124;\leq
    c$ |  | (15) |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
  zh: '|  | $C(x)=\tilde{x}:\quad V(\tilde{x})\subseteq V(x)\quad\text{且}\quad&#124;V(\tilde{x})&#124;\leq
    c$ |  | (15) |'
- en: We can come up with different variants of clipping operators $C(x)$.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提出不同的裁剪算子 $C(x)$ 的变体。
- en: Example N.1.
  id: totrans-634
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 示例 N.1
- en: If $x=\{1,1,3,5,9\}$.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 $x=\{1,1,3,5,9\}$。
- en: N.2 Random $x^{0}\in\mathbb{R}^{d}_{c}$
  id: totrans-636
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: N.2 随机 $x^{0}\in\mathbb{R}^{d}_{c}$
- en: 'Now let us define the algorithm for generating random points from $\mathbb{R}^{d}_{c}$
    (Alg. [6](#alg6 "Algorithm 6 ‣ N.2 Random 𝑥⁰∈ℝ^𝑑_𝑐 ‣ Appendix N The Choice of
    the Initial Point 𝑥⁰ ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")).'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: '现在让我们定义生成来自 $\mathbb{R}^{d}_{c}$ 的随机点的算法（算法 [6](#alg6 "Algorithm 6 ‣ N.2 Random
    𝑥⁰∈ℝ^𝑑_𝑐 ‣ Appendix N The Choice of the Initial Point 𝑥⁰ ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")）。'
- en: Algorithm 6 Generation of Random Point $x^{0}\in\mathbb{R}^{d}_{c}$
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 6 生成随机点 $x^{0}\in\mathbb{R}^{d}_{c}$
- en: '1:  Parameters: dimensionality $d$.'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 参数：维度 $d$。'
- en: Different random $x^{0}\in\mathbb{R}^{d}_{c}$ as the number of runs with different
    random initialization.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 随机 $x^{0}\in\mathbb{R}^{d}_{c}$ 作为不同随机初始化的运行次数。
- en: Appendix O Small-Scale Experiments and Interpretation
  id: totrans-641
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 O 小规模实验与解释
- en: O.1 Objective function
  id: totrans-642
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: O.1 目标函数
- en: Let $c\in[d]$ and consider the problem
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $c\in[d]$ 并考虑以下问题
- en: '|  | $\min_{x\in\mathbb{R}^{d}_{\leq c}}{\phi(x)},$ |  | (16) |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{x\in\mathbb{R}^{d}_{\leq c}}{\phi(x)},$ |  | (16) |'
- en: where $\phi:\mathbb{R}^{d}\to\mathbb{R}$ distinct values. In other words, the
    cardinality of the set
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi:\mathbb{R}^{d}\to\mathbb{R}$ 具有不同的值。换句话说，集合的基数
- en: '|  | $V(x):=\{x_{1},\cdots,x_{d}\}$ |  | (17) |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '|  | $V(x):=\{x_{1},\cdots,x_{d}\}$ |  | (17) |'
- en: is at most $c$. For small experiments, we aim to minimize the following objective
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 最大为 $c$。对于小规模实验，我们的目标是最小化以下目标
- en: '|  | $1$2 |  | (18) |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (18) |'
- en: where $a_{i}\in\mathbb{R}^{d}$ and
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a_{i}\in\mathbb{R}^{d}$ 并且
- en: '|  | $$\Lambda=\text{diag}(a_{1},\cdots,a_{d})=\begin{bmatrix}a_{1}&amp;\cdots&amp;0\\
    \vdots&amp;\ddots&amp;0\\'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\Lambda=\text{diag}(a_{1},\cdots,a_{d})=\begin{bmatrix}a_{1}&amp;\cdots&amp;0\\
    \vdots&amp;\ddots&amp;0\\'
- en: 0&amp;0&amp;a_{d}\end{bmatrix}.$$ |  | (19) |
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 0&amp;0&amp;a_{d}\end{bmatrix}.$$ |  | (19) |
- en: 'We set $a_{i}=\nicefrac{{i}}{{d}}$ denote the vector to which the Algorithm
    [1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning Quantized Models
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    converges.'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: '我们设 $a_{i}=\nicefrac{{i}}{{d}}$ 表示算法 [1](#alg1 "Algorithm 1 ‣ 3.1 Problem description
    ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression") 收敛到的向量。'
- en: O.2 Tiny-scale experiments ($d=6$)
  id: totrans-653
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: O.2 微小规模实验 ($d=6$)
- en: 'We applied PV algorithm to the problem ([16](#A15.E16 "Equation 16 ‣ O.1 Objective
    function ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣
    PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")) with
    $\phi(x)$.'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将 PV 算法应用于问题 ([16](#A15.E16 "Equation 16 ‣ O.1 Objective function ‣ Appendix
    O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) 与 $\phi(x)$。'
- en: '![Refer to caption](img/86a3345ebd7c17d2bb1641106ba6830c.png)'
  id: totrans-655
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/86a3345ebd7c17d2bb1641106ba6830c.png)'
- en: (a) PV algorithm with different values of $c\in[1,6]$.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: (a) PV 算法与不同的 $c\in[1,6]$ 值。
- en: '![Refer to caption](img/2f898afad03643ed21a4a55c194e8eff.png)'
  id: totrans-657
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2f898afad03643ed21a4a55c194e8eff.png)'
- en: (b) The influence of P and V steps on the loss function $\phi(x)$.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: (b) P 和 V 步骤对损失函数 $\phi(x)$ 的影响。
- en: 'Figure 3: PV algorithm ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣
    3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")) applied on the very small dimensional ($d=6$ is
    chosen randomly using the ng algorithm ([6](#alg6 "Algorithm 6 ‣ N.2 Random 𝑥⁰∈ℝ^𝑑_𝑐
    ‣ Appendix N The Choice of the Initial Point 𝑥⁰ ‣ Appendix ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression")).'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: PV 算法 ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression")) 应用于非常小的维度 ($d=6$ 是使用 ng 算法 ([6](#alg6 "Algorithm 6 ‣ N.2 Random
    𝑥⁰∈ℝ^𝑑_𝑐 ‣ Appendix N The Choice of the Initial Point 𝑥⁰ ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) 随机选择的。'
- en: When $c=1$.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 当 $c=1$ 时。
- en: 'Note that as we increase the maximum number of unique elements $c$, when the
    loss is zero (the plot [3(a)](#A15.F3.sf1 "Figure 3(a) ‣ Figure 3 ‣ O.2 Tiny-scale
    experiments (𝑑=6, 𝑐∈[1,6]) ‣ Appendix O Small-Scale Experiments and Interpretation
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")
    is in logarithmic scale and that is why we cannot see the last line).'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，当我们增加最大唯一元素数 $c$ 时，当损失为零时（图 [3(a)](#A15.F3.sf1 "Figure 3(a) ‣ Figure 3
    ‣ O.2 Tiny-scale experiments (𝑑=6, 𝑐∈[1,6]) ‣ Appendix O Small-Scale Experiments
    and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression") 以对数尺度显示，因此我们无法看到最后一行）。'
- en: 'We run PV algorithm ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression")) with different random starting points $x^{0}\in\mathbb{R}^{d}_{c}$,
    we used this value for all further experiments.'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用不同的随机起始点 $x^{0}\in\mathbb{R}^{d}_{c}$ 运行 PV 算法（[1](#alg1 "算法 1 ‣ 3.1 问题描述
    ‣ 3 精细调整量化模型 ‣ PV 调优：超越极端 LLM 压缩的直通估计")），我们在所有进一步的实验中都使用了这个值。
- en: 'Each of the two steps of the PV algorithm contributes to the convergence. To
    observe this we plotted the loss function over the iterates and explicitly marked
    the progress of both P and V steps (Fig. [3(b)](#A15.F3.sf2 "Figure 3(b) ‣ Figure
    3 ‣ O.2 Tiny-scale experiments (𝑑=6, 𝑐∈[1,6]) ‣ Appendix O Small-Scale Experiments
    and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")). We can see that we have progress during each of
    these steps and one single P and V step is not enough to obtain a solution even
    in this very small and simple case.'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: PV 算法的两个步骤都有助于收敛。为了观察这一点，我们绘制了迭代过程中的损失函数，并明确标记了 P 和 V 步骤的进展（图 [3(b)](#A15.F3.sf2
    "图 3(b) ‣ 图 3 ‣ O.2 微型实验（𝑑=6, 𝑐∈[1,6]） ‣ 附录 O 小规模实验及解释 ‣ 附录 ‣ PV 调优：超越极端 LLM 压缩的直通估计")）。我们可以看到，在每个步骤中都有进展，即使在这个非常小且简单的情况下，单独的
    P 和 V 步骤也不足以获得解决方案。
- en: These simple experiments demonstrate that
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 这些简单的实验表明
- en: '1.'
  id: totrans-665
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: larger $c$ (smaller compress ratio) provides better final accuracy
  id: totrans-666
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 较大的 $c$（较小的压缩比）提供了更好的最终准确度
- en: '2.'
  id: totrans-667
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: several P and V steps are needed to converge to the solution
  id: totrans-668
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要几个 P 和 V 步骤才能收敛到解
- en: O.3 Small-scale experiments ($d=100$)
  id: totrans-669
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: O.3 小规模实验（$d=100$）
- en: 'The problem of the algorithm ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem description
    ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")) is that the V step requires the full parameter
    search which gives us the complexity $\mathcal{O}(c^{d})$. Even for small tasks,
    this becomes unpractical to solve.'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的问题（[1](#alg1 "算法 1 ‣ 3.1 问题描述 ‣ 3 精细调整量化模型 ‣ PV 调优：超越极端 LLM 压缩的直通估计")）在于
    V 步骤需要进行全面的参数搜索，这给出了复杂度 $\mathcal{O}(c^{d})$。即使对于小任务，这也变得不可行。
- en: Example O.1.
  id: totrans-671
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 示例 O.1.
- en: Let us take $d=100$ years to make a single V step.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以 $d=100$ 年来完成一个 V 步骤。
- en: 'Let us consider special sets of function $\phi(x)$ that we will call separable
    functions. This class of functions should satisfy the assumption ([O.2](#A15.Thmtheorem2
    "Assumption O.2 (Separable function). ‣ O.3 Small-scale experiments (𝑑=100, 𝑐∈[1,100])
    ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑我们称之为可分离函数的特殊函数集。这类函数应该满足假设（[O.2](#A15.Thmtheorem2 "假设 O.2（可分离函数）。 ‣ O.3
    小规模实验（𝑑=100, 𝑐∈[1,100]） ‣ 附录 O 小规模实验及解释 ‣ 附录 ‣ PV 调优：超越极端 LLM 压缩的直通估计")）。
- en: Assumption O.2  (Separable function).
  id: totrans-674
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 O.2（可分离函数）。
- en: The function $\phi(x):\mathbb{R}^{d}\to\mathbb{R}$.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 $\phi(x):\mathbb{R}^{d}\to\mathbb{R}$。
- en: Example O.3.
  id: totrans-676
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 示例 O.3.
- en: 'The objective function ([18](#A15.E18 "Equation 18 ‣ O.1 Objective function
    ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) is a sum of
    squares that can be written in the following form:'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数（[18](#A15.E18 "方程 18 ‣ O.1 目标函数 ‣ 附录 O 小规模实验及解释 ‣ 附录 ‣ PV 调优：超越极端 LLM 压缩的直通估计")）是平方和，可以写成以下形式：
- en: '|  | $1$2 |  | (20) |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (20) |'
- en: 'where $\phi_{i}(x_{i})=a_{i}(x_{i}-x^{\star}_{i})^{2}$. Hence, the objective
    ([18](#A15.E18 "Equation 18 ‣ O.1 Objective function ‣ Appendix O Small-Scale
    Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) is a separable function.'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi_{i}(x_{i})=a_{i}(x_{i}-x^{\star}_{i})^{2}$。因此，目标函数（[18](#A15.E18 "方程
    18 ‣ O.1 目标函数 ‣ 附录 O 小规模实验及解释 ‣ 附录 ‣ PV 调优：超越极端 LLM 压缩的直通估计")）是一个可分离函数。
- en: 'One can show that for separable functions ([O.2](#A15.Thmtheorem2 "Assumption
    O.2 (Separable function). ‣ O.3 Small-scale experiments (𝑑=100, 𝑐∈[1,100]) ‣ Appendix
    O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) the algorithm ([1](#alg1 "Algorithm
    1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression")) can be written in the
    form ([7](#alg7 "Algorithm 7 ‣ O.3 Small-scale experiments (𝑑=100, 𝑐∈[1,100])
    ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")). Hence, for
    separable functions, we can compute the V step in $\mathcal{O}(c\cdot d)$), which
    makes the algorithm ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning
    Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression")) practical to use.'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，对于可分函数（[O.2](#A15.Thmtheorem2 "假设 O.2 (可分函数) ‣ O.3 小规模实验 (𝑑=100, 𝑐∈[1,100])
    ‣ 附录 O 小规模实验与解释 ‣ 附录 ‣ PV 调优：超越直接估计的极端 LLM 压缩")），该算法（[1](#alg1 "算法 1 ‣ 3.1 问题描述
    ‣ 3 微调量化模型 ‣ PV 调优：超越直接估计的极端 LLM 压缩")）可以写成形式（[7](#alg7 "算法 7 ‣ O.3 小规模实验 (𝑑=100,
    𝑐∈[1,100]) ‣ 附录 O 小规模实验与解释 ‣ 附录 ‣ PV 调优：超越直接估计的极端 LLM 压缩")）。因此，对于可分函数，我们可以在 $\mathcal{O}(c\cdot
    d)$ 的时间复杂度下计算 V 步骤，这使得该算法（[1](#alg1 "算法 1 ‣ 3.1 问题描述 ‣ 3 微调量化模型 ‣ PV 调优：超越直接估计的极端
    LLM 压缩")）在实际中具有可用性。
- en: Algorithm 7 PV Algorithm with Optimized V step
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 7 PV 算法与优化的 V 步骤
- en: '1:  Initialization: starting point $x^{0}\in\mathbb{R}^{d}_{c}$ (Optimized
    V step)6:     end for7:  end for'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 初始化：起始点 $x^{0}\in\mathbb{R}^{d}_{c}$（优化的 V 步骤）6:   结束 for 循环7:   结束 for
    循环'
- en: 'Then we ran the optimized PV algorithm ([7](#alg7 "Algorithm 7 ‣ O.3 Small-scale
    experiments (𝑑=100, 𝑐∈[1,100]) ‣ Appendix O Small-Scale Experiments and Interpretation
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    on the quadratic objective ([18](#A15.E18 "Equation 18 ‣ O.1 Objective function
    ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")). The results
    are presented in (Figure [4](#A15.F4 "Figure 4 ‣ O.3 Small-scale experiments (𝑑=100,
    𝑐∈[1,100]) ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")).
    Number of runs with different random initial points $r=50$.'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在二次目标上运行了优化的 PV 算法（[7](#alg7 "算法 7 ‣ O.3 小规模实验 (𝑑=100, 𝑐∈[1,100]) ‣ 附录 O
    小规模实验与解释 ‣ 附录 ‣ PV 调优：超越直接估计的极端 LLM 压缩")）。结果展示在 (图 [4](#A15.F4 "图 4 ‣ O.3 小规模实验
    (𝑑=100, 𝑐∈[1,100]) ‣ 附录 O 小规模实验与解释 ‣ 附录 ‣ PV 调优：超越直接估计的极端 LLM 压缩") )。不同随机初始点的运行次数
    $r=50$。
- en: For these experiments, we see the same results as for tiny scale with $d=6$.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些实验，我们观察到与小规模实验中 $d=6$ 相同的结果。
- en: 'As it was mentioned before, the algorithm ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem
    description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) is conceptual only and cannot be used
    in the raw form in practice. That is why we need to move to the experiments with
    linearized V step.'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，算法（[1](#alg1 "算法 1 ‣ 3.1 问题描述 ‣ 3 微调量化模型 ‣ PV 调优：超越直接估计的极端 LLM 压缩")）只是一个概念模型，不能直接用于实际应用中。这就是为什么我们需要进行线性化
    V 步骤的实验。
- en: '![Refer to caption](img/2aa185008f5a15c3872200e20feca913.png)'
  id: totrans-686
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2aa185008f5a15c3872200e20feca913.png)'
- en: (a) PV algorithm with different values of $c\in[10,100]$.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: (a) PV 算法在不同的 $c\in[10,100]$ 值下。
- en: '![Refer to caption](img/599f63a45396efe4ea84ce8ee8588144.png)'
  id: totrans-688
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/599f63a45396efe4ea84ce8ee8588144.png)'
- en: (b) The influence of P and V steps on the loss function $\phi(x)$.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: (b) P 和 V 步骤对损失函数 $\phi(x)$ 的影响。
- en: 'Figure 4: Optimized PV algorithm ([7](#alg7 "Algorithm 7 ‣ O.3 Small-scale
    experiments (𝑑=100, 𝑐∈[1,100]) ‣ Appendix O Small-Scale Experiments and Interpretation
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    applied on the quadratic objective ([18](#A15.E18 "Equation 18 ‣ O.1 Objective
    function ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣
    PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")),
    $d=100$.'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：优化 PV 算法 ([7](#alg7 "算法 7 ‣ O.3 小规模实验（𝑑=100, 𝑐∈[1,100]） ‣ 附录 O 小规模实验和解释
    ‣ 附录 ‣ PV 调优：超越直通估计进行极端 LLM 压缩")) 应用于二次目标 ([18](#A15.E18 "方程 18 ‣ O.1 目标函数 ‣ 附录
    O 小规模实验和解释 ‣ 附录 ‣ PV 调优：超越直通估计进行极端 LLM 压缩"))，$d=100$。
- en: O.4 Linearized PV
  id: totrans-691
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: O.4 线性化 PV
- en: 'Linearized V step ([B.1](#A2.SS1 "B.1 Approximate V step, variant 1 (non-accelerated)
    ‣ Appendix B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) allows us to greatly reduce the cost
    of one V step. We ran Linearized PV on the quadratic objective ([18](#A15.E18
    "Equation 18 ‣ O.1 Objective function ‣ Appendix O Small-Scale Experiments and
    Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for
    Extreme LLM Compression")) (Fig. [5](#A15.F5 "Figure 5 ‣ O.4 Linearized PV ‣ Appendix
    O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")).'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 线性化 V 步骤 ([B.1](#A2.SS1 "B.1 近似 V 步骤，变体 1（非加速） ‣ 附录 B 近似 PV 算法 ‣ 附录 ‣ PV 调优：超越直通估计进行极端
    LLM 压缩")) 使我们能够大幅减少一次 V 步骤的成本。我们在二次目标 ([18](#A15.E18 "方程 18 ‣ O.1 目标函数 ‣ 附录 O
    小规模实验和解释 ‣ 附录 ‣ PV 调优：超越直通估计进行极端 LLM 压缩")) 上运行了线性化 PV (图 [5](#A15.F5 "图 5 ‣ O.4
    线性化 PV ‣ 附录 O 小规模实验和解释 ‣ 附录 ‣ PV 调优：超越直通估计进行极端 LLM 压缩"))。
- en: '![Refer to caption](img/6a99b9060b05905a92f9adf443cb8e81.png)'
  id: totrans-693
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6a99b9060b05905a92f9adf443cb8e81.png)'
- en: '(a) Linearized PV algorithm ([B.1](#A2.SS1 "B.1 Approximate V step, variant
    1 (non-accelerated) ‣ Appendix B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) with different
    $T\in[1,5]$ provides faster convergence rates, but the same final accuracy.'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 线性化 PV 算法 ([B.1](#A2.SS1 "B.1 近似 V 步骤，变体 1（非加速） ‣ 附录 B 近似 PV 算法 ‣ 附录 ‣ PV
    调优：超越直通估计进行极端 LLM 压缩")) 在不同的 $T\in[1,5]$ 下提供了更快的收敛速度，但最终精度相同。
- en: '![Refer to caption](img/a61fe5caab4a6c3ca870b14aab93ecf4.png)'
  id: totrans-695
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a61fe5caab4a6c3ca870b14aab93ecf4.png)'
- en: (b) The influence of P and V steps on the loss function $\phi(x)$ linearized
    V steps on each iteration.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: (b) P 和 V 步骤对损失函数 $\phi(x)$ 的影响，线性化 V 步骤在每次迭代中的表现。
- en: 'Figure 5: Experiments with Linearized PV algorithm ([B.1](#A2.SS1 "B.1 Approximate
    V step, variant 1 (non-accelerated) ‣ Appendix B Approximate PV Algorithm ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：线性化 PV 算法的实验 ([B.1](#A2.SS1 "B.1 近似 V 步骤，变体 1（非加速） ‣ 附录 B 近似 PV 算法 ‣ 附录
    ‣ PV 调优：超越直通估计进行极端 LLM 压缩"))。
- en: 'From the first experiment (Fig. [5(a)](#A15.F5.sf1 "Figure 5(a) ‣ Figure 5
    ‣ O.4 Linearized PV ‣ Appendix O Small-Scale Experiments and Interpretation ‣
    Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    we can see that'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 从第一次实验 (图 [5(a)](#A15.F5.sf1 "图 5(a) ‣ 图 5 ‣ O.4 线性化 PV ‣ 附录 O 小规模实验和解释 ‣ 附录
    ‣ PV 调优：超越直通估计进行极端 LLM 压缩")) 我们可以看到
- en: '1.'
  id: totrans-699
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: increasing $T$ to save computations.
  id: totrans-700
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增加 $T$ 以节省计算。
- en: '2.'
  id: totrans-701
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Linearized PV algorithm ([B.1](#A2.SS1 "B.1 Approximate V step, variant 1 (non-accelerated)
    ‣ Appendix B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) converges to a worse accuracy than the
    exact PV ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning Quantized
    Models ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  id: totrans-702
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性化 PV 算法 ([B.1](#A2.SS1 "B.1 近似 V 步骤，变体 1（非加速） ‣ 附录 B 近似 PV 算法 ‣ 附录 ‣ PV 调优：超越直通估计进行极端
    LLM 压缩")) 收敛到比精确 PV ([1](#alg1 "算法 1 ‣ 3.1 问题描述 ‣ 3 微调量化模型 ‣ PV 调优：超越直通估计进行极端
    LLM 压缩")) 更差的精度。
- en: '3.'
  id: totrans-703
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Linearized PV has to make more iterations than PV to converge
  id: totrans-704
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性化 PV 需要比 PV 更多的迭代才能收敛。
- en: 'The second plot (Fig.[5(b)](#A15.F5.sf2 "Figure 5(b) ‣ Figure 5 ‣ O.4 Linearized
    PV ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) demonstrates
    the effect of multiple Linearized V step. We can see that the largest effect comes
    from the first V step.'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: '第二个图(图.[5(b)](#A15.F5.sf2 "Figure 5(b) ‣ Figure 5 ‣ O.4 Linearized PV ‣ Appendix
    O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"))展示了多个线性化V步的效果。我们可以看到，最大效果来自第一次V步。'
- en: O.5 Linearized PV + sparse updates
  id: totrans-706
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: O.5 线性化的PV + 稀疏更新
- en: 'In previous section ([O.4](#A15.SS4 "O.4 Linearized PV ‣ Appendix O Small-Scale
    Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) we have seen that Linearized PV algorithm
    converges to a worse accuracy than the exact PV ([1](#alg1 "Algorithm 1 ‣ 3.1
    Problem description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")).'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: '在前一节（[O.4](#A15.SS4 "O.4 Linearized PV ‣ Appendix O Small-Scale Experiments
    and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")）中，我们已经看到线性化PV算法收敛到比精确PV（[1](#alg1 "Algorithm 1 ‣
    3.1 Problem description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")）更差的准确性。'
- en: 'Linearized PV ([B.1](#A2.SS1 "B.1 Approximate V step, variant 1 (non-accelerated)
    ‣ Appendix B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) with combination of sparse updates ([3.3](#S3.SS3
    "3.3 Linearized subspace V step ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) is intended
    to mitigate this issue.'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: '带有稀疏更新的线性化PV（[B.1](#A2.SS1 "B.1 Approximate V step, variant 1 (non-accelerated)
    ‣ Appendix B Approximate PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")）结合稀疏更新（[3.3](#S3.SS3 "3.3 Linearized
    subspace V step ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")）旨在缓解这个问题。'
- en: 'On (Fig. [6(a)](#A15.F6.sf1 "Figure 6(a) ‣ Figure 6 ‣ O.5 Linearized PV + sparse
    updates ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")) we can see comparison
    of three methods: exact PV ([1](#alg1 "Algorithm 1 ‣ 3.1 Problem description ‣
    3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")) – red line, Linearized PV ([B.1](#A2.SS1 "B.1 Approximate
    V step, variant 1 (non-accelerated) ‣ Appendix B Approximate PV Algorithm ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    – blue line and Linearized PV with sparse updates ([3.3](#S3.SS3 "3.3 Linearized
    subspace V step ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) – green line.'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: '在(图 [6(a)](#A15.F6.sf1 "Figure 6(a) ‣ Figure 6 ‣ O.5 Linearized PV + sparse
    updates ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression"))中，我们可以看到三种方法的比较：精确的PV（[1](#alg1
    "Algorithm 1 ‣ 3.1 Problem description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression")）——红线，线性化的PV（[B.1](#A2.SS1
    "B.1 Approximate V step, variant 1 (non-accelerated) ‣ Appendix B Approximate
    PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression")）——蓝线，和带有稀疏更新的线性化PV（[3.3](#S3.SS3 "3.3 Linearized subspace V
    step ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")）——绿线。'
- en: From this experiment we observe
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个实验中我们观察到
- en: '1.'
  id: totrans-711
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Linearized PV with sparse updates converges to a better accuracy than Linearized
    PV
  id: totrans-712
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 带有稀疏更新的线性化PV收敛到比线性化PV更好的准确性
- en: '2.'
  id: totrans-713
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: inearized PV + sparse updates has to make more iterations than Linearized PV
    and exact PV to converge
  id: totrans-714
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性化PV + 稀疏更新必须比线性化PV和精确PV进行更多的迭代才能收敛
- en: Hence, this approach helps us to converge to a better accuracy, but with a price
    of larger number of iterations to converge.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种方法帮助我们达到更好的准确性，但需要更多的迭代次数来收敛。
- en: '![Refer to caption](img/9b96b9ca4b4ccecc940bf57d95677391.png)'
  id: totrans-716
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9b96b9ca4b4ccecc940bf57d95677391.png)'
- en: (a) Influence of sparse updates
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 稀疏更新的影响
- en: '![Refer to caption](img/385223201fec832196535f08533a1901.png)'
  id: totrans-718
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/385223201fec832196535f08533a1901.png)'
- en: (b) Different choise of sampling for choosing $\mathcal{S}^{k}$
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 选择$\mathcal{S}^{k}$的不同采样方式
- en: 'Figure 6: Comparison of the exact PV algorithm ([1](#alg1 "Algorithm 1 ‣ 3.1
    Problem description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) with Linearized PV ([B.1](#A2.SS1 "B.1
    Approximate V step, variant 1 (non-accelerated) ‣ Appendix B Approximate PV Algorithm
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    and Linearized PV + sparse updates ([3.3](#S3.SS3 "3.3 Linearized subspace V step
    ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")).'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：精确 PV 算法 ([1](#alg1 "算法 1 ‣ 3.1 问题描述 ‣ 3 微调量化模型 ‣ PV 调整：超越直通估计的极端 LLM 压缩"))
    与线性化 PV ([B.1](#A2.SS1 "B.1 近似 V 步骤，变体 1（非加速） ‣ 附录 B 近似 PV 算法 ‣ 附录 ‣ PV 调整：超越直通估计的极端
    LLM 压缩")) 以及线性化 PV + 稀疏更新 ([3.3](#S3.SS3 "3.3 线性化子空间 V 步骤 ‣ 3 微调量化模型 ‣ PV 调整：超越直通估计的极端
    LLM 压缩")) 的比较。
- en: 'We can use different rules for choosing the subspace $\mathcal{S}^{k}$ which
    produce different convergence rates and the final accuracy levels ([6(b)](#A15.F6.sf2
    "Figure 6(b) ‣ Figure 6 ‣ O.5 Linearized PV + sparse updates ‣ Appendix O Small-Scale
    Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")).'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用不同的规则来选择子空间 $\mathcal{S}^{k}$，这些规则会产生不同的收敛速度和最终的准确度水平 ([6(b)](#A15.F6.sf2
    "图 6(b) ‣ 图 6 ‣ O.5 线性化 PV + 稀疏更新 ‣ 附录 O 小规模实验和解释 ‣ 附录 ‣ PV 调整：超越直通估计的极端 LLM 压缩"))。
- en: '![Refer to caption](img/640063e0b0e21e1dfc9f48fcf500cef7.png)'
  id: totrans-722
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/640063e0b0e21e1dfc9f48fcf500cef7.png)'
- en: (a) The behaviour of $L_{\mathcal{S}^{k}}$ in Linearized PV + sparce updates.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: (a) $L_{\mathcal{S}^{k}}$ 在线性化 PV + 稀疏更新中的行为。
- en: '![Refer to caption](img/6b7e4031d81614f0784601761abc66d4.png)'
  id: totrans-724
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6b7e4031d81614f0784601761abc66d4.png)'
- en: '(b) Degradation of dimensionality of $V(x)$ for Linearized PV algorithm ([B.1](#A2.SS1
    "B.1 Approximate V step, variant 1 (non-accelerated) ‣ Appendix B Approximate
    PV Algorithm ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme
    LLM Compression")).'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 线性化 PV 算法 ([B.1](#A2.SS1 "B.1 近似 V 步骤，变体 1（非加速） ‣ 附录 B 近似 PV 算法 ‣ 附录 ‣ PV
    调整：超越直通估计的极端 LLM 压缩")) 中 $V(x)$ 的维度降解。
- en: 'Figure 7: Comparison of the exact PV algorithm ([1](#alg1 "Algorithm 1 ‣ 3.1
    Problem description ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")) with Linearized PV ([B.1](#A2.SS1 "B.1
    Approximate V step, variant 1 (non-accelerated) ‣ Appendix B Approximate PV Algorithm
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    and Linearized PV + Sparse Updates ([3.3](#S3.SS3 "3.3 Linearized subspace V step
    ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")).'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：精确 PV 算法 ([1](#alg1 "算法 1 ‣ 3.1 问题描述 ‣ 3 微调量化模型 ‣ PV 调整：超越直通估计的极端 LLM 压缩"))
    与线性化 PV ([B.1](#A2.SS1 "B.1 近似 V 步骤，变体 1（非加速） ‣ 附录 B 近似 PV 算法 ‣ 附录 ‣ PV 调整：超越直通估计的极端
    LLM 压缩")) 以及线性化 PV + 稀疏更新 ([3.3](#S3.SS3 "3.3 线性化子空间 V 步骤 ‣ 3 微调量化模型 ‣ PV 调整：超越直通估计的极端
    LLM 压缩")) 的比较。
- en: 'In large scale experiments we used sparse updates with $\mathcal{S}^{k}$ sampling
    can be even more advanced providing better final accuracy at cost of larger number
    of iterations to converge – purple and orange lines (Fig. [6(b)](#A15.F6.sf2 "Figure
    6(b) ‣ Figure 6 ‣ O.5 Linearized PV + sparse updates ‣ Appendix O Small-Scale
    Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")).'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模实验中，我们使用了带有 $\mathcal{S}^{k}$ 采样的稀疏更新，这可能更加先进，在收敛所需的迭代次数增加的情况下提供更好的最终准确度
    —— 紫色和橙色线（图 [6(b)](#A15.F6.sf2 "图 6(b) ‣ 图 6 ‣ O.5 线性化 PV + 稀疏更新 ‣ 附录 O 小规模实验和解释
    ‣ 附录 ‣ PV 调整：超越直通估计的极端 LLM 压缩"))。
- en: We can see that Linearized PV with sparse updates can converge to even a better
    accuracy than the exact PV algorithm. The problem of Linearized PV algorithm is
    that we come to the local minimum and cannot get out of that minima because of
    the small value of the gradient (we are near to a real solution) and small stepsize.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，线性化 PV 与稀疏更新的组合能够收敛到比精确 PV 算法更好的准确度。线性化 PV 算法的问题是，我们会陷入局部最小值，无法摆脱这个最小值，因为梯度值很小（我们接近于真实解）和步长很小。
- en: 'Linearized PV with sparse updates allows us to mitigate this problem by reducing
    the subspace from $\mathbb{R}^{d}$ changes for Linearized PV with different sparse
    updates sampling methods (Fig. [7(a)](#A15.F7.sf1 "Figure 7(a) ‣ Figure 7 ‣ O.5
    Linearized PV + sparse updates ‣ Appendix O Small-Scale Experiments and Interpretation
    ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")).'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏更新的线性化 PV 使我们能够通过减少从 $\mathbb{R}^{d}$ 变化的子空间来缓解这个问题，适用于具有不同稀疏更新采样方法的线性化 PV（图
    [7(a)](#A15.F7.sf1 "图 7(a) ‣ 图 7 ‣ O.5 线性化 PV + 稀疏更新 ‣ 附录 O 小规模实验与解释 ‣ 附录 ‣ PV
    调整：超越直通估计的极端 LLM 压缩"))。
- en: Appendix P PV^+ Algorithm
  id: totrans-730
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 P PV^+ 算法
- en: 'We can have degradation of $|V(x^{k})|$ in ([18](#A15.E18 "Equation 18 ‣ O.1
    Objective function ‣ Appendix O Small-Scale Experiments and Interpretation ‣ Appendix
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression"))
    are equal to one).'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ([18](#A15.E18 "方程 18 ‣ O.1 目标函数 ‣ 附录 O 小规模实验与解释 ‣ 附录 ‣ PV 调整：超越直通估计的极端 LLM
    压缩")) 中，$|V(x^{k})|$ 的降级等于一。
- en: '1.'
  id: totrans-732
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Degradation during the P step: Let $x^{\star}=\{0,2,1\}$.'
  id: totrans-733
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: P 步骤中的降级：设 $x^{\star}=\{0,2,1\}$。
- en: '2.'
  id: totrans-734
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Degradation during the V step: Let $x^{\star}=\{2,10,0,11\}$.'
  id: totrans-735
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: V 步骤中的降级：设 $x^{\star}=\{2,10,0,11\}$。
- en: In real experiments we observe decreasing of $|V(x)|$ during the first several
    iterations. We can use this gap to find even better solution with improved final
    accuracy and faster convergence rate.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际实验中，我们观察到在前几个迭代过程中 $|V(x)|$ 逐渐减少。我们可以利用这个差距找到更好的解决方案，从而提高最终的准确性和加快收敛速度。
- en: To add additional unique element in $V(x)$ to become larger up to some upper
    bound.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 向 $V(x)$ 中添加额外的唯一元素，使其变大直到某个上限。
- en: 'Let $\phi(\cdot)$:'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\phi(\cdot)$：
- en: '|  | $W(x,x^{\star},c)=V(x^{\star})\setminus V(x):\quad&#124;W(x,x^{\star},c)&#124;=c-&#124;V(x)&#124;$
    |  | (21) |'
  id: totrans-739
  prefs: []
  type: TYPE_TB
  zh: '|  | $W(x,x^{\star},c)=V(x^{\star})\setminus V(x):\quad\|W(x,x^{\star},c)\|=c-\|V(x)\|$
    |  | (21) |'
- en: Algorithm 8 PV^+ Algorithm
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 8 PV^+ 算法
- en: '1:  Parameters: starting point $x^{0}\in\mathbb{R}^{d}_{\leq c}$ (Modified
    V step)5:  end for'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 参数：起始点 $x^{0}\in\mathbb{R}^{d}_{\leq c}$ (修改的 V 步骤) 5: 结束 for'
- en: Theorem P.1.
  id: totrans-742
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 P.1.
- en: Assume $\phi$. Then the algorithm PV^+ has the following guarantees
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\phi$。那么算法 PV^+ 有以下保证
- en: (i)
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (i)
- en: $y^{k},x^{k}\in\mathbb{R}^{d}_{\leq\hat{c}}$,
  id: totrans-745
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $y^{k},x^{k}\in\mathbb{R}^{d}_{\leq\hat{c}}$，
- en: (ii)
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (ii)
- en: $\phi(x^{k+1})\leq\phi(y^{k})\leq\phi(x^{k})$, and
  id: totrans-747
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\phi(x^{k+1})\leq\phi(y^{k})\leq\phi(x^{k})$，并且
- en: (iii)
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (iii)
- en: the sequence $\{\phi(x^{k})\}_{k\geq 0}$ converges to some value, which is smaller
    or equal to one produced by PV algorithm
  id: totrans-749
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 序列 $\{\phi(x^{k})\}_{k\geq 0}$ 收敛到某个值，这个值小于或等于 PV 算法生成的值。
- en: Proof.
  id: totrans-750
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'Part (ii): Since'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 部分 (ii)：因为
- en: '|  | $1$2 |  |'
  id: totrans-752
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: and because $x=y^{k}$.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 并且因为 $x=y^{k}$。
- en: 'The rest of the proof is identical to ([A.1](#A1.SS1 "A.1 Proof of Theorem
    3.1 ‣ Appendix A Proofs ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")). ∎'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 证明的其余部分与 ([A.1](#A1.SS1 "A.1 定理 3.1 的证明 ‣ 附录 A 证明 ‣ 附录 ‣ PV 调整：超越直通估计的极端 LLM
    压缩")) 相同。 ∎
- en: Appendix Q Broader Impact
  id: totrans-755
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 Q 更广泛的影响
- en: The main impact of our work, both positive and negative, is in the ability to
    deploy higher-quality LLMs to run on memory-limited devices like desktops, laptops,
    and phones. On the positive side, this would allow practitioners to develop offline
    LLM applications (e.g. translate service), lower-latency chat assistants that
    are not dependant on network latency, or privacy-sensitive LLM applications where
    the user’s private data never leaves their device. Furthermore, this can facilitate
    the creation of free open-source software based on LLMs by eliminating the need
    to maintain costly inference servers on the backend. Since phones are everywhere
    and LLMs are powerful general-purpose tools, PV-tuned models could significantly
    impact how the general population uses LLMs to complete tasks.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的主要影响，无论是正面还是负面，都在于能够在内存有限的设备（如台式机、笔记本电脑和手机）上部署更高质量的 LLM。从积极方面来看，这将允许从业者开发离线
    LLM 应用（例如翻译服务）、不依赖于网络延迟的低延迟聊天助手，或用户的私人数据不会离开其设备的隐私敏感型 LLM 应用。此外，这还可以通过消除维护昂贵推理服务器的需求，促进基于
    LLM 的免费开源软件的创建。由于手机无处不在，而 LLM 是强大的通用工具，PV 调整的模型可能会显著影响普通人如何使用 LLM 完成任务。
- en: However, LLMs are still a dual-use technology with the potential for significant
    benefits and serious harm. Risks range from deliberate misuse (e.g. spam generation)
    and accidental misuse to negative economic side-effects. An upper bound on these
    risks is that PV tuning does not create new (potentially risky) LLM capabilities,
    merely making existing ones more accessible.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LLMs 仍然是一种双用途技术，具有显著的好处和严重的危害。风险包括故意滥用（如垃圾邮件生成）和意外滥用，以及负面的经济副作用。这些风险的上限是
    PV 调优不会创造新的（可能具有风险的）LLM 能力，而只是使现有能力更易于访问。
- en: NeurIPS Paper Checklist
  id: totrans-758
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NeurIPS 论文检查表
- en: '1.'
  id: totrans-759
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Claims
  id: totrans-760
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 声明
- en: 'Question: Do the main claims made in the abstract and introduction accurately
    reflect the paper’s contributions and scope?'
  id: totrans-761
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：摘要和引言中所做的主要声明是否准确反映了论文的贡献和范围？
- en: 'Answer: [Yes]'
  id: totrans-762
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: Our main claims are that the PV-tuning algorithm 1) achievstate-of-the-artart
    quantized LLM quality for 1-2 bits per parameter (backed by Section [4.3](#S4.SS3
    "4.3 Large-scale Evaluation & Discussion ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression")), 2) Paretoeto optimal at around 2 bits
    (also Section [4.3](#S4.SS3 "4.3 Large-scale Evaluation & Discussion ‣ 4 Experiments
    ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression")),
    3) and is compatible with various methods (Section [4.2](#S4.SS2 "4.2 Evaluating
    Fine-tuning Algorithms ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation
    for Extreme LLM Compression")). In the introduction, we also claim that the advanced
    quantized representations, such as those having sparse outliers, do not give significant
    benefit on top of simple vector quantization with finetuning: this part is backed
    by [4.1](#S4.SS1 "4.1 Evaluating quantized representations with finetuning ‣ 4
    Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").'
  id: totrans-763
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：我们主要的声明是 PV-tuning 算法 1) 实现了 1-2 位参数的最先进量化 LLM 质量（由第[4.3](#S4.SS3 "4.3 大规模评估与讨论
    ‣ 4 实验 ‣ PV-Tuning：超越直接估计进行极端LLM压缩")节支持），2) 在大约 2 位时达到帕累托最优（同样是第[4.3](#S4.SS3
    "4.3 大规模评估与讨论 ‣ 4 实验 ‣ PV-Tuning：超越直接估计进行极端LLM压缩")节），3) 并且与各种方法兼容（第[4.2](#S4.SS2
    "4.2 评估微调算法 ‣ 4 实验 ‣ PV-Tuning：超越直接估计进行极端LLM压缩")节）。在引言中，我们还声明先进的量化表示，如具有稀疏离群值的表示，并没有在简单的向量量化和微调的基础上带来显著的好处：这部分由第[4.1](#S4.SS1
    "4.1 评估量化表示与微调 ‣ 4 实验 ‣ PV-Tuning：超越直接估计进行极端LLM压缩")节支持。
- en: 'Guidelines:'
  id: totrans-764
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-765
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the abstract and introduction do not include the claims
    made in the paper.
  id: totrans-766
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: “NA”答案意味着摘要和引言中没有包含论文中所做的声明。
- en: •
  id: totrans-767
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The abstract and/or introduction should clearly state the claims made, including
    the contributions made in the paper and important assumptions and limitations.
    A No or NA answer to this question will not be perceived well by the reviewers.
  id: totrans-768
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 摘要和/或引言应明确说明所做的声明，包括论文中的贡献和重要的假设及局限性。对这个问题的否定或“NA”回答将不会被审稿人接受。
- en: •
  id: totrans-769
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The claims made should match theoretical and experimental results, and reflect
    how much the results can be expected to generalize to other settings.
  id: totrans-770
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所做的声明应与理论和实验结果相匹配，并反映结果在其他环境中的普遍适用性。
- en: •
  id: totrans-771
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It is fine to include aspirational goals as motivation as long as it is clear
    that these goals are not attained by the paper.
  id: totrans-772
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含雄心勃勃的目标作为动机是可以的，只要这些目标与论文中所达到的目标明确区分开来。
- en: '2.'
  id: totrans-773
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Limitations
  id: totrans-774
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 局限性
- en: 'Question: Does the paper discuss the limitations of the work performed by the
    authors?'
  id: totrans-775
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否讨论了作者所做工作的局限性？
- en: 'Answer: [Yes]'
  id: totrans-776
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: We discuss the methodological limitations of our study near
    the end, after Section [4.3](#S4.SS3 "4.3 Large-scale Evaluation & Discussion
    ‣ 4 Experiments ‣ PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM
    Compression"). We also explain limitations for practitioners in Section [3.4](#S3.SS4
    "3.4 Implementation details ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond
    Straight-Through Estimation for Extreme LLM Compression").'
  id: totrans-777
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：我们在第[4.3](#S4.SS3 "4.3 大规模评估与讨论 ‣ 4 实验 ‣ PV-Tuning：超越直接估计进行极端LLM压缩")节接近尾声时讨论了我们研究的方法论局限性。我们还在第[3.4](#S3.SS4
    "3.4 实施细节 ‣ 3 微调量化模型 ‣ PV-Tuning：超越直接估计进行极端LLM压缩")节中解释了针对实践者的局限性。
- en: 'Guidelines:'
  id: totrans-778
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-779
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper has no limitation while the answer No means
    that the paper has limitations, but those are not discussed in the paper.
  id: totrans-780
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案 NA 表示论文没有限制，而答案 No 表示论文存在限制，但这些限制在论文中没有讨论。
- en: •
  id: totrans-781
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors are encouraged to create a separate "Limitations" section in their
    paper.
  id: totrans-782
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鼓励作者在论文中创建一个单独的“限制”部分。
- en: •
  id: totrans-783
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should point out any strong assumptions and how robust the results
    are to violations of these assumptions (e.g., independence assumptions, noiseless
    settings, model well-specification, asymptotic approximations only holding locally).
    The authors should reflect on how these assumptions might be violated in practice
    and what the implications would be.
  id: totrans-784
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应指出任何强假设以及结果对这些假设违反的稳健性（例如，独立性假设、无噪声环境、模型良好规范、渐近近似仅在局部有效）。作者应反思这些假设在实践中可能会如何被违反以及这会带来什么影响。
- en: •
  id: totrans-785
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should reflect on the scope of the claims made, e.g., if the approach
    was only tested on a few datasets or with a few runs. In general, empirical results
    often depend on implicit assumptions, which should be articulated.
  id: totrans-786
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应反思所做声明的范围，例如，方法是否仅在少数数据集或少数几次运行中进行了测试。通常，实证结果往往依赖于隐含假设，这些假设应被明确说明。
- en: •
  id: totrans-787
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should reflect on the factors that influence the performance of
    the approach. For example, a facial recognition algorithm may perform poorly when
    the image resolution is low or images are taken in low lighting. Or a speech-to-text
    system might not be used reliably to provide closed captions for online lectures
    because it fails to handle technical jargon.
  id: totrans-788
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应反思影响方法性能的因素。例如，面部识别算法在图像分辨率低或图像在低光照条件下拍摄时可能表现不佳。或者语音转文本系统可能无法可靠地为在线讲座提供字幕，因为它无法处理技术术语。
- en: •
  id: totrans-789
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should discuss the computational efficiency of the proposed algorithms
    and how they scale with dataset size.
  id: totrans-790
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应讨论所提出算法的计算效率以及它们如何随数据集大小的变化而变化。
- en: •
  id: totrans-791
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If applicable, the authors should discuss possible limitations of their approach
    to addressing problems of privacy and fairness.
  id: totrans-792
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果适用，作者应讨论其方法在解决隐私和公平性问题上的可能限制。
- en: •
  id: totrans-793
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While the authors might fear that complete honesty about limitations might be
    used by reviewers as grounds for rejection, a worse outcome might be that reviewers
    discover limitations that aren’t acknowledged in the paper. The authors should
    use their best judgment and recognize that individual actions in favor of transparency
    play an important role in developing norms that preserve the integrity of the
    community. Reviewers will be specifically instructed to not penalize honesty concerning
    limitations.
  id: totrans-794
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管作者可能担心对限制的完全诚实可能被评审员用作拒稿的理由，但更糟糕的结果可能是评审员发现论文中未承认的限制。作者应根据自己的最佳判断，认识到个人在透明度方面的行动在发展保持社区诚信的规范中起着重要作用。评审员将特别指示不要因为诚实地讨论限制而给予惩罚。
- en: '3.'
  id: totrans-795
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Theory Assumptions and Proofs
  id: totrans-796
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理论假设和证明
- en: 'Question: For each theoretical result, does the paper provide the full set
    of assumptions and a complete (and correct) proof?'
  id: totrans-797
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：对于每个理论结果，论文是否提供了完整的假设集和完整（且正确）的证明？
- en: 'Answer: [Yes]'
  id: totrans-798
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: We carefully introduced the assumptions (e.g. that $\phi(\cdot)$
    is L-smooth) and provided proofs in appendix. To the best of our knowledge, these
    proofs are both correct and complete.'
  id: totrans-799
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：我们仔细介绍了假设（例如，$\phi(\cdot)$ 是 L-光滑的）并在附录中提供了证明。根据我们所知，这些证明都是正确和完整的。
- en: 'Guidelines:'
  id: totrans-800
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-801
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include theoretical results.
  id: totrans-802
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案 NA 表示论文不包括理论结果。
- en: •
  id: totrans-803
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
  id: totrans-804
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文中的所有定理、公式和证明应编号并交叉引用。
- en: •
  id: totrans-805
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All assumptions should be clearly stated or referenced in the statement of any
    theorems.
  id: totrans-806
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有假设应在任何定理的陈述中明确说明或引用。
- en: •
  id: totrans-807
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The proofs can either appear in the main paper or the supplemental material,
    but if they appear in the supplemental material, the authors are encouraged to
    provide a short proof sketch to provide intuition.
  id: totrans-808
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 证明可以出现在正文中或补充材料中，但如果出现在补充材料中，鼓励作者提供简短的证明概要以提供直观理解。
- en: •
  id: totrans-809
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Inversely, any informal proof provided in the core of the paper should be complemented
    by formal proofs provided the in appendix or supplemental material.
  id: totrans-810
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相反，论文核心部分提供的任何非正式证明应由附录或补充材料中的正式证明补充。
- en: •
  id: totrans-811
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Theorems and Lemmas that the proof relies upon should be properly referenced.
  id: totrans-812
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 证明所依赖的定理和引理应适当引用。
- en: '4.'
  id: totrans-813
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Experimental Result Reproducibility
  id: totrans-814
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验结果的可重复性
- en: 'Question: Does the paper fully disclose all the information needed to reproduce
    the main experimental results of the paper to the extent that it affects the main
    claims and/or conclusions of the paper (regardless of whether the code and data
    are provided or not)?'
  id: totrans-815
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否充分披露了重现论文主要实验结果所需的所有信息，以至于影响论文的主要主张和/或结论（无论是否提供代码和数据）？
- en: 'Answer: [Yes]'
  id: totrans-816
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: We train open-access LLMs on open datasets and release our full
    training code. We do our best to provide instructions and hyperparameters in the
    code, though running our algorithm in different conditions may require basic tuning.'
  id: totrans-817
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 说明：我们在开放数据集上训练开放访问的LLMs，并发布了完整的训练代码。我们尽力在代码中提供说明和超参数，但在不同条件下运行算法可能需要基本的调整。
- en: 'Guidelines:'
  id: totrans-818
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-819
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include experiments.
  id: totrans-820
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案NA意味着论文不包含实验。
- en: •
  id: totrans-821
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'If the paper includes experiments, a No answer to this question will not be
    perceived well by the reviewers: Making the paper reproducible is important, regardless
    of whether the code and data are provided or not.'
  id: totrans-822
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果论文包含实验，回答“No”会被审稿人视为不满：无论是否提供代码和数据，使论文可重复都是重要的。
- en: •
  id: totrans-823
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the contribution is a dataset and/or model, the authors should describe the
    steps taken to make their results reproducible or verifiable.
  id: totrans-824
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果贡献是一个数据集和/或模型，作者应描述为了使结果可重复或可验证所采取的步骤。
- en: •
  id: totrans-825
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Depending on the contribution, reproducibility can be accomplished in various
    ways. For example, if the contribution is a novel architecture, describing the
    architecture fully might suffice, or if the contribution is a specific model and
    empirical evaluation, it may be necessary to either make it possible for others
    to replicate the model with the same dataset, or provide access to the model.
    In general. releasing code and data is often one good way to accomplish this,
    but reproducibility can also be provided via detailed instructions for how to
    replicate the results, access to a hosted model (e.g., in the case of a large
    language model), releasing of a model checkpoint, or other means that are appropriate
    to the research performed.
  id: totrans-826
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据贡献的不同，可通过多种方式实现可重复性。例如，如果贡献是一个新颖的架构，那么完全描述该架构可能就足够了；如果贡献是一个特定模型和经验评估，可能需要让其他人能够用相同的数据集重复该模型，或者提供对模型的访问权限。一般来说，发布代码和数据通常是一种良好的实现方式，但可重复性也可以通过详细的重复结果的说明、对托管模型（例如在大型语言模型的情况下）的访问、模型检查点的发布或其他适合所做研究的方式提供。
- en: •
  id: totrans-827
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While NeurIPS does not require releasing code, the conference does require all
    submissions to provide some reasonable avenue for reproducibility, which may depend
    on the nature of the contribution. For example
  id: totrans-828
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然NeurIPS不要求发布代码，但会议确实要求所有提交提供某种合理的可重复性途径，这可能依赖于贡献的性质。例如，
- en: (a)
  id: totrans-829
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: If the contribution is primarily a new algorithm, the paper should make it clear
    how to reproduce that algorithm.
  id: totrans-830
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果贡献主要是一个新的算法，论文应明确如何重复该算法。
- en: (b)
  id: totrans-831
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: If the contribution is primarily a new model architecture, the paper should
    describe the architecture clearly and fully.
  id: totrans-832
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果贡献主要是一个新的模型架构，论文应清晰全面地描述该架构。
- en: (c)
  id: totrans-833
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: If the contribution is a new model (e.g., a large language model), then there
    should either be a way to access this model for reproducing the results or a way
    to reproduce the model (e.g., with an open-source dataset or instructions for
    how to construct the dataset).
  id: totrans-834
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果贡献是一个新模型（例如大型语言模型），那么应该有一种访问该模型以重复结果的方法，或者有一种重复模型的方法（例如，使用开源数据集或提供构建数据集的说明）。
- en: (d)
  id: totrans-835
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (d)
- en: We recognize that reproducibility may be tricky in some cases, in which case
    authors are welcome to describe the particular way they provide for reproducibility.
    In the case of closed-source models, it may be that access to the model is limited
    in some way (e.g., to registered users), but it should be possible for other researchers
    to have some path to reproducing or verifying the results.
  id: totrans-836
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们认识到在某些情况下，可重复性可能会很棘手，在这种情况下，作者可以描述他们提供可重复性的方法。对于闭源模型，模型的访问可能会有某些限制（例如，仅限注册用户），但应能为其他研究人员提供一些重复或验证结果的途径。
- en: '5.'
  id: totrans-837
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Open access to data and code
  id: totrans-838
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据和代码的开放访问
- en: 'Question: Does the paper provide open access to the data and code, with sufficient
    instructions to faithfully reproduce the main experimental results, as described
    in supplemental material?'
  id: totrans-839
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否提供了数据和代码的开放访问，并提供了足够的说明以忠实重现主要实验结果，如补充材料中所述？
- en: 'Answer: [Yes]'
  id: totrans-840
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: As we state above, we release the full implementation for the
    PV algorithm with requisite instructions. We do not introduce new datasets and
    use openly available ones. We also plan to release the main quantized models in
    the non-anonymized version of the paper, since it would be impractical to upload
    them with the supplementary zip archive.'
  id: totrans-841
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：正如我们上面所述，我们发布了PV算法的完整实现及所需说明。我们不引入新的数据集，而是使用公开可用的数据集。我们还计划在论文的非匿名版本中发布主要量化模型，因为将其与补充的压缩档案一起上传是不切实际的。
- en: 'Guidelines:'
  id: totrans-842
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-843
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means the paper does not include experiments requiring code.
  id: totrans-844
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案NA意味着论文不包括需要代码的实验。
- en: •
  id: totrans-845
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy))
    for more details.
  id: totrans-846
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参见NeurIPS代码和数据提交指南（[https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)）以获取更多细节。
- en: •
  id: totrans-847
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While we encourage the release of code and data, we understand that this might
    not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply
    for not including code, unless this is central to the contribution (e.g., for
    a new open-source benchmark).
  id: totrans-848
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然我们鼓励发布代码和数据，但我们理解这可能不可行，因此“否”是可以接受的回答。论文不能仅因未包含代码而被拒绝，除非这对贡献至关重要（例如，对于新的开源基准）。
- en: •
  id: totrans-849
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The instructions should contain the exact command and environment needed to
    run to reproduce the results. See the NeurIPS code and data submission guidelines
    ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy))
    for more details.
  id: totrans-850
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指令应包含运行所需的精确命令和环境以重现结果。有关更多细节，请参见NeurIPS代码和数据提交指南（[https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)）。
- en: •
  id: totrans-851
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should provide instructions on data access and preparation, including
    how to access the raw data, preprocessed data, intermediate data generated data,
    etc.
  id: totrans-852
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应提供数据访问和准备的说明，包括如何访问原始数据、预处理数据、中间生成的数据等。
- en: •
  id: totrans-853
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should provide scripts to reproduce all experimental results for
    the new proposed method and baselines. If only a subset of experiments are reproducible,
    they should state which ones are omitted from the script and why.
  id: totrans-854
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应提供脚本以重现所有新提出方法和基线的实验结果。如果只有一部分实验是可重现的，他们应说明哪些实验被排除在脚本之外以及原因。
- en: •
  id: totrans-855
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: At submission time, to preserve anonymity, the authors should release anonymized
    versions (if applicable).
  id: totrans-856
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在提交时，为了保密，作者应发布匿名版本（如适用）。
- en: •
  id: totrans-857
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Providing as much information as possible in supplemental material (appended
    to the paper) is recommended, but including URLs to data and code is permitted.
  id: totrans-858
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推荐在附加材料中提供尽可能多的信息（附加到论文中），但允许包括数据和代码的URL。
- en: '6.'
  id: totrans-859
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Experimental Setting/Details
  id: totrans-860
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验设置/细节
- en: 'Question: Does the paper specify all the training and test details (e.g., data
    splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary
    to understand the results?'
  id: totrans-861
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否具体说明了所有必要的训练和测试细节（例如，数据分割、超参数、如何选择这些参数、优化器类型等），以理解结果？
- en: 'Answer: [Yes]'
  id: totrans-862
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: We describe our setup in Sections [3.4](#S3.SS4 "3.4 Implementation
    details ‣ 3 Fine-Tuning Quantized Models ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression") and [4](#S4 "4 Experiments ‣ PV-Tuning:
    Beyond Straight-Through Estimation for Extreme LLM Compression"), with additional
    hyperparameters baked into the supplementary code.'
  id: totrans-863
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：我们在第[3.4](#S3.SS4 "3.4 实现细节 ‣ 3 微调量化模型 ‣ PV-Tuning：超越直通估计以进行极端LLM压缩")和第[4](#S4
    "4 实验 ‣ PV-Tuning：超越直通估计以进行极端LLM压缩")节中描述了我们的设置，并在补充代码中融入了附加超参数。
- en: 'Guidelines:'
  id: totrans-864
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-865
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include experiments.
  id: totrans-866
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案NA意味着论文不包括实验。
- en: •
  id: totrans-867
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The experimental setting should be presented in the core of the paper to a level
    of detail that is necessary to appreciate the results and make sense of them.
  id: totrans-868
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验设置应在论文的核心部分呈现，详尽到能够理解结果并理解其意义的程度。
- en: •
  id: totrans-869
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The full details can be provided either with the code, the in appendix, or as
    supplemental material.
  id: totrans-870
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完整的细节可以通过代码、附录或补充材料提供。
- en: '7.'
  id: totrans-871
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: Experiment Statistical Significance
  id: totrans-872
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验统计显著性
- en: 'Question: Does the paper report error bars suitably and correctly defined or
    other appropriate information about the statistical significance of the experiments?'
  id: totrans-873
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否适当地报告了误差范围或其他关于实验统计显著性的相关信息？
- en: 'Answer: [No]'
  id: totrans-874
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[否]
- en: 'Justification: We report error bars for small-scale experiments [O](#A15 "Appendix
    O Small-Scale Experiments and Interpretation ‣ Appendix ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"). For full fine-tuning runs, we do not
    include error bars since running those would be prohibitively costly for us.'
  id: totrans-875
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 说明：我们报告了小规模实验的误差范围[O](#A15 "附录 O 小规模实验及解释 ‣ 附录 ‣ PV 调优：超越直通估计以实现极端 LLM 压缩")。对于完全的微调运行，由于成本过高，我们未包括误差范围。
- en: 'Guidelines:'
  id: totrans-876
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-877
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include experiments.
  id: totrans-878
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案 NA 表示论文没有包含实验。
- en: •
  id: totrans-879
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should answer "Yes" if the results are accompanied by error bars,
    confidence intervals, or statistical significance tests, at least for the experiments
    that support the main claims of the paper.
  id: totrans-880
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果结果附带误差范围、置信区间或统计显著性检验，作者应回答“是”，至少对于支持论文主要主张的实验。
- en: •
  id: totrans-881
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The factors of variability that the error bars are capturing should be clearly
    stated (for example, train/test split, initialization, random drawing of some
    parameter, or overall run with given experimental conditions).
  id: totrans-882
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 误差范围所捕捉的变异因素应当明确说明（例如，训练/测试拆分、初始化、某些参数的随机抽取，或在给定实验条件下的整体运行）。
- en: •
  id: totrans-883
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The method for calculating the error bars should be explained (closed form formula,
    call to a library function, bootstrap, etc.)
  id: totrans-884
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算误差范围的方法应当解释清楚（封闭形式公式、调用库函数、引导法等）
- en: •
  id: totrans-885
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The assumptions made should be given (e.g., Normally distributed errors).
  id: totrans-886
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应该给出所做的假设（例如，误差正态分布）。
- en: •
  id: totrans-887
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It should be clear whether the error bar is the standard deviation or the standard
    error of the mean.
  id: totrans-888
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应明确误差范围是标准差还是均值的标准误。
- en: •
  id: totrans-889
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It is OK to report 1-sigma error bars, but one should state it. The authors
    should preferably report a 2-sigma error bar and then state that they have a 96%CI
    if the hypothesis of Normality of errors is not verified.
  id: totrans-890
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 报告1σ误差范围是可以的，但应当说明。作者最好报告2σ误差范围，并且说明如果误差的正态性假设未得到验证，他们有96%置信区间。
- en: •
  id: totrans-891
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For asymmetric distributions, the authors should be careful not to show in tables
    or figures symmetric error bars that would yield results that are out of range
    (e.g. negative error rates).
  id: totrans-892
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于非对称分布，作者应小心不要在表格或图形中展示对称的误差范围，这可能会导致结果超出范围（例如负误差率）。
- en: •
  id: totrans-893
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If error bars are reported in tables or plots, The authors should explain in
    the text how they were calculated and reference the corresponding figures or tables
    in the text.
  id: totrans-894
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果在表格或图表中报告了误差范围，作者应在文本中解释其计算方法，并在文本中引用相应的图表。
- en: '8.'
  id: totrans-895
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: Experiments Compute Resources
  id: totrans-896
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验计算资源
- en: 'Question: For each experiment, does the paper provide sufficient information
    on the computer resources (type computing workers, memory, time of execution)
    needed to reproduce the experiments?'
  id: totrans-897
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：对于每个实验，论文是否提供了足够的信息关于计算资源（计算工作者类型、内存、执行时间）以重现实验？
- en: 'Answer: [Yes]'
  id: totrans-898
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: We report the hardware setting, calibration setting, time, and
    memory requirements in Section [4](#S4 "4 Experiments ‣ PV-Tuning: Beyond Straight-Through
    Estimation for Extreme LLM Compression"), which is sufficient for practitioners
    to reproduce our results. We omit some details, e.g. which runs were restarted
    due to unrelated server infrastructure issues.'
  id: totrans-899
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 说明：我们在第[4](#S4 "4 实验 ‣ PV 调优：超越直通估计以实现极端 LLM 压缩")节中报告了硬件设置、校准设置、时间和内存要求，这足以让从业者重现我们的结果。我们省略了一些细节，例如因无关的服务器基础设施问题而重启的运行。
- en: 'Guidelines:'
  id: totrans-900
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-901
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include experiments.
  id: totrans-902
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案 NA 表示论文没有包含实验。
- en: •
  id: totrans-903
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should indicate the type of compute worker CPU or GPU, internal cluster,
    or cloud provider, including relevant memory and storage.
  id: totrans-904
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应指明计算工作者的CPU或GPU类型、内部集群或云服务提供商，包括相关的内存和存储。
- en: •
  id: totrans-905
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should provide the amount of compute required for each of the individual
    experimental runs as well as estimate the total compute.
  id: totrans-906
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应提供每个实验运行所需的计算量以及总计算量的估算。
- en: •
  id: totrans-907
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should disclose whether the full research project required mocomputingute
    than the experiments reported in the paper (e.g., preliminary or failed experiments
    that didn’t make it into the paper).
  id: totrans-908
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应披露完整的研究项目是否需要比论文中报告的实验更多的计算资源（例如，未包含在论文中的初步或失败实验）。
- en: '9.'
  id: totrans-909
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: Code Of Ethics
  id: totrans-910
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 伦理规范
- en: 'Question: Does the research conducted in the paper conform, in every respect,
    with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines](https://neurips.cc/public/EthicsGuidelines)?'
  id: totrans-911
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文中进行的研究是否在各个方面符合 NeurIPS 伦理规范 [https://neurips.cc/public/EthicsGuidelines](https://neurips.cc/public/EthicsGuidelines)？
- en: 'Answer: [Yes]'
  id: totrans-912
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: Our research is focused on the base capability and accessibility
    of LLMs. While working on LLMs always has potential externalities, our specific
    work adheres to the ethics guidelines.'
  id: totrans-913
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 说明：我们的研究专注于 LLM 的基础能力和可访问性。虽然研究 LLM 总是具有潜在的外部效应，但我们的具体工作遵循伦理指南。
- en: 'Guidelines:'
  id: totrans-914
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-915
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
  id: totrans-916
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案 NA 表示作者没有审查 NeurIPS 伦理规范。
- en: •
  id: totrans-917
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the authors answer No, they should explain the special circumstances that
    require a deviation from the Code of Ethics.
  id: totrans-918
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果作者回答“否”，他们应解释需要偏离伦理规范的特殊情况。
- en: •
  id: totrans-919
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should make sure to preserve anonymity (eg. if there is a special
    consideration due to laws or regulations in their jurisdiction).
  id: totrans-920
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应确保保持匿名（例如，如果法律或法规在其管辖区内有特殊规定）。
- en: '10.'
  id: totrans-921
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '10.'
- en: Broader Impacts
  id: totrans-922
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更广泛的影响
- en: 'Question: Does the paper discuss both potential positive societal impacts and
    negative societal impacts of the work performed?'
  id: totrans-923
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否讨论了该工作可能带来的正面社会影响和负面社会影响？
- en: 'Answer: [Yes]'
  id: totrans-924
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: While our work is more concerned with fundamental matters of
    discrete optimization and LLM quantization, we provide a brief overview of its
    societal impacts in Appendix [Q](#A17 "Appendix Q Broader Impact ‣ Appendix ‣
    PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression").'
  id: totrans-925
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 说明：虽然我们的工作更关注离散优化和 LLM 量化的基础问题，但我们在附录[Q](#A17 "附录 Q 更广泛的影响 ‣ 附录 ‣ PV-Tuning：超越直通估计的极端
    LLM 压缩")中简要概述了其社会影响。
- en: 'Guidelines:'
  id: totrans-926
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-927
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that there is no societal impact on the work performed.
  id: totrans-928
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案 NA 表示该工作没有社会影响。
- en: •
  id: totrans-929
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the authors answer NA or No, they should explain why their work has no societal
    impact or why the paper does not address societal impact.
  id: totrans-930
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果作者回答 NA 或“否”，他们应解释为什么他们的工作没有社会影响或为什么论文没有涉及社会影响。
- en: •
  id: totrans-931
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Examples of negative societal impacts include potential malicious or unintended
    uses (e.g., disinformation, generating fake profiles, surveillance), fairness
    considerations (e.g., deployment of technologies that could make decisions that
    unfairly impact specific groups), privacy considerations, and security considerations.
  id: totrans-932
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 负面社会影响的例子包括潜在的恶意或非预期用途（例如，虚假信息，生成虚假资料，监视），公平性考虑（例如，部署可能对特定群体产生不公平影响的技术），隐私考虑和安全性考虑。
- en: •
  id: totrans-933
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The conference expects that many papers will be foundational research and not
    tied to particular applications, let alone deployments. However, if there is a
    direct path to any negative applications, the authors should point it out. For
    example, it is legitimate to point out that an improvement in the quality of generative
    models could be used to generate deepfakes for disinformation. On the other hand,
    it is not needed to point out that a generic algorithm for optimizing neural networks
    could enable people to train models that generate Deepfakes faster.
  id: totrans-934
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 会议期望许多论文将是基础研究，而不与特定应用程序或部署相关。然而，如果存在直接的负面应用途径，作者应指出。例如，指出生成模型质量的提升可能被用于生成虚假信息的深度伪造是合情合理的。另一方面，不需要指出优化神经网络的通用算法可能使人们更快地训练出深度伪造模型。
- en: •
  id: totrans-935
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should consider possible harms that could arise when the technology
    is being used as intended and functioning correctly harms that could arise when
    the technology is being used as intended but gives incorrect results, and harms
    following from (intentional or unintentional) misuse of the technology.
  id: totrans-936
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应考虑在技术按预期使用并正常运行时可能产生的潜在危害，以及当技术按预期使用但给出错误结果时可能产生的危害，以及由于（有意或无意）误用技术而产生的危害。
- en: •
  id: totrans-937
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If there are negative societal impacts, the authors could also discuss possible
    mitigation strategies (e.g., gated release of models, providing defenses in addition
    to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system
    learns from feedback over time, improving the efficiency and accessibility of
    ML).
  id: totrans-938
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果存在负面社会影响，作者还可以讨论可能的缓解策略（例如，模型的分级发布、提供防御措施以补充攻击、监控误用的机制、监控系统如何从反馈中学习的机制、提高ML的效率和可及性）。
- en: '11.'
  id: totrans-939
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '11.'
- en: Safeguards
  id: totrans-940
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 保护措施
- en: 'Question: Does the paper describe safeguards that have been put in place for
    responsible release of data or models that have a high risk for misuse (e.g.,
    pre-trained language models, image generators, or scraped datasets)?'
  id: totrans-941
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否描述了为负责任地发布具有高误用风险的数据或模型（例如预训练语言模型、图像生成器或抓取的数据集）所采取的保护措施？
- en: 'Answer: [N/A]'
  id: totrans-942
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[N/A]
- en: 'Justification: Our work does not release any newer models, and quantizing existing
    models typically results in a less capable (and therefore less risky) model.'
  id: totrans-943
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：我们的工作没有发布任何更新的模型，而对现有模型进行量化通常会导致能力下降（因此风险更小）。
- en: 'Guidelines:'
  id: totrans-944
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-945
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper poses no such risks.
  id: totrans-946
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”意味着论文没有提出此类风险。
- en: •
  id: totrans-947
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Released models that have a high risk for misuse or dual-use should be released
    with necessary safeguards to allow for controlled use of the model, for example
    by requiring that users adhere to usage guidelines or restrictions to access the
    model or implementing safety filters.
  id: totrans-948
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 发布具有高风险的模型时，应该采取必要的保护措施，以允许模型的受控使用，例如通过要求用户遵守使用指南或限制访问模型，或实施安全过滤器。
- en: •
  id: totrans-949
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Datasets that have been scraped from the Internet could pose safety risks. The
    authors should describe how they avoided releasing unsafe images.
  id: totrans-950
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从互联网抓取的数据集可能存在安全风险。作者应描述他们如何避免发布不安全的图像。
- en: •
  id: totrans-951
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We recognize that providing effective safeguards is challenging, and many papers
    do not require this, but we encourage authors to take this into account and make
    the best faith effort.
  id: totrans-952
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们认识到提供有效的保护措施是具有挑战性的，许多论文不要求这样做，但我们鼓励作者考虑这一点，并尽力做到最好。
- en: '12.'
  id: totrans-953
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '12.'
- en: Licenses for existing assets
  id: totrans-954
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现有资产的许可证
- en: 'Question: Are the creators or original owners of assets (e.g., code, data,
    models), used in the paper, properly credited, and are the license and terms of
    use explicitly mentioned and properly respected?'
  id: totrans-955
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：在论文中使用的资产（例如代码、数据、模型）的创作者或原始所有者是否得到了适当的致谢？许可证和使用条款是否明确提及并得到适当尊重？
- en: 'Answer: [Yes]'
  id: totrans-956
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[Yes]
- en: 'Justification: We use academically published artifacts (datasets, models, etc)
    and cite their respective authors.'
  id: totrans-957
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：我们使用学术发布的文献（数据集、模型等），并引用其各自的作者。
- en: 'Guidelines:'
  id: totrans-958
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-959
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not use existing assets.
  id: totrans-960
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”意味着论文不使用现有资产。
- en: •
  id: totrans-961
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should cite the original paper that produced the code package or
    dataset.
  id: totrans-962
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应引用生成代码包或数据集的原始论文。
- en: •
  id: totrans-963
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should state which version of the asset is used and, if possible,
    include a URL.
  id: totrans-964
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应说明使用了哪个版本的资产，并且如果可能，包含一个网址。
- en: •
  id: totrans-965
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The name of the license (e.g., CC-BY 4.0) should be included for each asset.
  id: totrans-966
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应包括每个资产的许可证名称（例如CC-BY 4.0）。
- en: •
  id: totrans-967
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For scraped data from a particular source (e.g., website), the copyright and
    terms of service of that source should be provided.
  id: totrans-968
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于从特定来源（例如网站）抓取的数据，应该提供该来源的版权和服务条款。
- en: •
  id: totrans-969
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If assets are released, the license, copyright information, and terms of use
    in the package should be provided. For popular datasets, [paperswithcode.com/datasets](paperswithcode.com/datasets)
    has curated licenses for some datasets. Their licensing guide can help determine
    the license of a dataset.
  id: totrans-970
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果资产被发布，包中应提供许可证、版权信息和使用条款。对于流行的数据集，[paperswithcode.com/datasets](paperswithcode.com/datasets)已经为一些数据集策划了许可证。他们的许可证指南可以帮助确定数据集的许可证。
- en: •
  id: totrans-971
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For existing datasets that are re-packaged, both the original license and the
    license of the derived asset (if it has changed) should be provided.
  id: totrans-972
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于重新打包的现有数据集，应提供原始许可证和派生资产的许可证（如果有所更改）。
- en: •
  id: totrans-973
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If this information is not available online, the authors are encouraged to reach
    out to the asset’s creators.
  id: totrans-974
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果这些信息在网上不可用，鼓励作者联系资产的创建者。
- en: '13.'
  id: totrans-975
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '13.'
- en: New Assets
  id: totrans-976
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新资产
- en: 'Question: Are new assets introduced in the paper well documented and is the
    documentation provided alongside the assets?'
  id: totrans-977
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文中介绍的新资产是否有充分的文档记录，并且文档是否与资产一同提供？
- en: 'Answer: [Yes]'
  id: totrans-978
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[Yes]
- en: 'Justification: We release the code and provide documentation in the form of
    README and detailed docstrings. Both are included in the supplementary archive.'
  id: totrans-979
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：我们发布了代码，并提供了以 README 和详细文档字符串形式的文档。两者都包含在补充档案中。
- en: 'Guidelines:'
  id: totrans-980
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-981
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not release new assets.
  id: totrans-982
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案 NA 表示论文不发布新资产。
- en: •
  id: totrans-983
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Researchers should communicate the details of the dataset/code/model as part
    of their submissions via structured templates. This includes details about training,
    license, limitations, etc.
  id: totrans-984
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 研究人员应通过结构化模板在提交中沟通数据集/代码/模型的详细信息。这包括培训、许可证、限制等方面的详细信息。
- en: •
  id: totrans-985
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should discuss whether and how consent was obtained from people whose
    asset is used.
  id: totrans-986
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应讨论是否以及如何获得了使用资产的人的同意。
- en: •
  id: totrans-987
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: At submission time, remember to anonymize your assets (if applicable). You can
    either create an anonymized URL or include an anonymized zip file.
  id: totrans-988
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提交时，请记得匿名化你的资产（如适用）。你可以创建一个匿名化的 URL 或包含一个匿名化的压缩文件。
- en: '14.'
  id: totrans-989
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '14.'
- en: Crowdsourcing and Research with Human Subjects
  id: totrans-990
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 众包和涉及人类受试者的研究
- en: 'Question: For crowdsourcing experiments and research with human subjects, does
    the paper include the full text of instructions given to participants and screenshots,
    if applicable, as well as details about compensation (if any)?'
  id: totrans-991
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：对于众包实验和涉及人类受试者的研究，论文是否包括给参与者的完整指令文本和截图（如果适用），以及关于补偿（如果有的话）的详细信息？
- en: 'Answer: [N/A]'
  id: totrans-992
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[N/A]
- en: 'Justification: We do not use human subjects in our experiments.'
  id: totrans-993
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：我们在实验中没有使用人类受试者。
- en: 'Guidelines:'
  id: totrans-994
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-995
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not involve crowdsourcing nor research
    with human subjects.
  id: totrans-996
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案 NA 表示论文不涉及众包或人类受试者研究。
- en: •
  id: totrans-997
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Including this information in the supplemental material is fine, but if the
    main contribution of the paper involves human subjects, then as much detail as
    possible should be included in the main paper.
  id: totrans-998
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将这些信息包括在补充材料中是可以的，但如果论文的主要贡献涉及人类受试者，则应尽可能在主论文中包含详细信息。
- en: •
  id: totrans-999
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: According to the NeurIPS Code of Ethics, workers involved in data collection,
    curation, or other labor should be paid at least the minimum wage in the country
    of the data collector.
  id: totrans-1000
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据 NeurIPS 行为准则，从事数据收集、整理或其他劳动的工人应至少获得数据收集国的最低工资。
- en: '15.'
  id: totrans-1001
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '15.'
- en: Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
    Subjects
  id: totrans-1002
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 机构审查委员会（IRB）批准或人类受试者研究的等效批准
- en: 'Question: Does the paper describe potential risks incurred by study participants,
    whether such risks were disclosed to the subjects, and whether Institutional Review
    Board (IRB) approvals (or an equivalent approval/review based on the requirements
    of your country or institution) were obtained?'
  id: totrans-1003
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否描述了研究参与者可能面临的风险，这些风险是否已告知受试者，以及是否获得了机构审查委员会（IRB）批准（或根据你所在国家或机构的要求获得了相应的批准/审查）？
- en: 'Answer: [N/A]'
  id: totrans-1004
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[N/A]
- en: 'Justification: We did not conduct any research on human subjects.'
  id: totrans-1005
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：我们没有进行任何关于人类受试者的研究。
- en: 'Guidelines:'
  id: totrans-1006
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-1007
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not involve crowdsourcing nor research
    with human subjects.
  id: totrans-1008
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案 NA 表示论文不涉及众包或人类受试者研究。
- en: •
  id: totrans-1009
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Depending on the country in which research is conducted, IRB approval (or equivalent)
    may be required for any human subjects research. If you obtained IRB approval,
    you should clearly state this in the paper.
  id: totrans-1010
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据研究所在国家的不同，人类受试者研究可能需要 IRB 批准（或等效批准）。如果你获得了 IRB 批准，你应在论文中明确说明。
- en: •
  id: totrans-1011
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We recognize that the procedures for this may vary significantly between institutions
    and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and
    the guidelines for their institution.
  id: totrans-1012
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们认识到，机构和地点之间的程序可能会有显著差异，我们期望作者遵守 NeurIPS 行为准则和其机构的指南。
- en: •
  id: totrans-1013
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For initial submissions, do not include any information that would break anonymity
    (if applicable), such as the institution conducting the review.
  id: totrans-1014
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于初步提交，请不要包含任何可能破坏匿名性的的信息（如适用），例如进行审查的机构。
