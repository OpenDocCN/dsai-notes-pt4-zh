- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:51:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:53
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对LLM推理中KV缓存和网络瓶颈的零延迟QKV压缩
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.04107](https://ar5iv.labs.arxiv.org/html/2408.04107)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.04107](https://ar5iv.labs.arxiv.org/html/2408.04107)
- en: Zeyu Zhang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 张泽宇
- en: University of Virginia    Haiying Shen
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 弗吉尼亚大学    沈海鹰
- en: University of Virginia
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 弗吉尼亚大学
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In large-language models, memory constraints in the key-value cache (KVC) pose
    a challenge during inference, especially with long prompts. In this work, we observed
    that compressing KV values is more effective than compressing the model regarding
    accuracy and job completion time (JCT). However, quantizing KV values and dropping
    less-important tokens incur significant runtime computational time overhead, delaying
    JCT. These methods also cannot reduce computation time or high network communication
    time overhead in sequence-parallelism (SP) frameworks for long prompts. To tackle
    these issues, based on our insightful observations from experimental analysis,
    we propose ZDC, a Zero-delay QKV Compression system that eliminates time overhead
    and even reduces computation and communication time of the model operations. ZDC
    innovatively embeds compression and decompression operations within model operations
    and adaptively determines compression ratios at a hybrid layer-token level. Further,
    it enables a communication-efficient SP inference framework. Trace-driven experiments
    demonstrate that ZDC achieves up to 80% lower average JCT, 35% lower average perplexity,
    and 2.8× higher throughput with the same latency compared to state-of-the-art
    compression methods. ZDC also reduces the average JCT of current LLM serving systems
    by up to 91% with the constraint of 0.1 perplexity increase. We open-sourced the
    code.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在大语言模型中，关键值缓存（KVC）的内存限制在推理过程中尤其是面对长提示时构成挑战。在这项工作中，我们观察到，压缩KV值比压缩模型在准确性和工作完成时间（JCT）方面更为有效。然而，量化KV值和丢弃不重要的标记会带来显著的运行时计算时间开销，导致JCT延迟。这些方法也不能减少序列并行（SP）框架中长提示的计算时间或高网络通信时间开销。为了解决这些问题，基于我们从实验分析中得到的深入观察，我们提出了ZDC，一个零延迟QKV压缩系统，消除了时间开销，并进一步减少了模型操作的计算和通信时间。ZDC创新性地将压缩和解压缩操作嵌入模型操作中，并在混合层-标记级别自适应确定压缩比例。此外，它还实现了一个通信高效的SP推理框架。追踪驱动的实验表明，与最先进的压缩方法相比，ZDC在相同延迟下实现了高达80%的平均JCT降低、35%的平均困惑度降低和2.8倍的吞吐量提升。ZDC还在困惑度增加0.1的限制下，将当前LLM服务系统的平均JCT降低了高达91%。我们已将代码开源。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Generative large language models (LLMs) [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)] have transformed natural language
    processing (NLP), showcasing remarkable performance across various tasks like
    text completion [[7](#bib.bib7)], dialogue generation [[8](#bib.bib8)], code synthesis
    [[9](#bib.bib9)], language translation [[10](#bib.bib10)], text summarization
    [[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)], and document classification
    [[14](#bib.bib14), [15](#bib.bib15)]. The LLM models are primarily inspired by
    the transformer architecture [[16](#bib.bib16)], which employs self-attention
    to capture long-range dependencies in sequences.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性大语言模型（LLMs）[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6)] 已经彻底改变了自然语言处理（NLP），在文本完成[[7](#bib.bib7)]、对话生成[[8](#bib.bib8)]、代码合成[[9](#bib.bib9)]、语言翻译[[10](#bib.bib10)]、文本摘要[[11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)]和文档分类[[14](#bib.bib14), [15](#bib.bib15)]等各种任务中表现出了卓越的性能。LLM模型主要受到变换器架构[[16](#bib.bib16)]的启发，该架构利用自注意力机制捕捉序列中的长期依赖关系。
- en: During LLM inference, text generation starts with providing an initial input,
    known as a prompt, which can vary from a book to just a few words. LLM generates
    the first token via prompt processing, followed by text generation to generate
    new tokens. LLMs typically consist of multiple transformer layers, where each
    layer receives an embedding matrix $E$) components for each token, serving as
    inputs to the self-attention layer. The KV values of newly generated tokens are
    stored in the key-value cache (KVC) to avoid redundant computations. During token
    generation, the KV values of previous tokens are fetched from the KVC and fed
    into the attention layer. The resulting self-attention output guides token generation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLM 推断过程中，文本生成从提供初始输入开始，称为提示词，提示词可以是一本书或仅仅是几句话。LLM 通过提示词处理生成第一个令牌，然后进行文本生成以生成新的令牌。LLM
    通常由多个变换器层组成，每一层接收每个令牌的嵌入矩阵 $E$) 组件，作为自注意力层的输入。新生成的令牌的 KV 值存储在键值缓存 (KVC) 中，以避免冗余计算。在令牌生成过程中，从
    KVC 中获取先前令牌的 KV 值并输入到注意力层中。生成的自注意力输出指导令牌生成。
- en: The primary challenge of LLM inference is memory constraints within KVC. Alongside
    storing the model and activations, LLMs often require substantial memory for KV
    values. These constraints limit GPU utilization and consequently decrease throughput.
    The issue is exacerbated by long prompts, which can contain more than 4K tokens
    [[17](#bib.bib17)], particularly in applications such as coding assistance [[9](#bib.bib9)],
    book summarization [[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)] and
    document classification [[14](#bib.bib14), [15](#bib.bib15)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 推断的主要挑战是 KVC 中的内存限制。除了存储模型和激活值外，LLM 通常还需要大量内存来存储 KV 值。这些限制限制了 GPU 的利用率，从而减少了吞吐量。长提示词的问题使情况更加严重，这些提示词可能包含超过
    4K 个令牌 [[17](#bib.bib17)]，特别是在如编码辅助 [[9](#bib.bib9)]、书籍总结 [[11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13)] 和文档分类 [[14](#bib.bib14), [15](#bib.bib15)] 等应用中。
- en: To address KVC constraints, various model compression methods have been proposed
    [[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24)]. In our study, we made an observation that
    compressing KV values offers more benefits in terms of accuracy and job completion
    time (JCT) compared to compressing the model itself (O[1](#Thmobs1 "O 1 ‣ 3.1
    Advantages of QKV Compression Compared to Model Compression ‣ 3 Experimental Analysis
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference")). Existing methods compress KV values using quantization [[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)]. Another approach is KVC eviction, which involves
    dropping less-important tokens [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)].
    However, both approaches incur significant runtime computational time overhead,
    resulting in notable delays in JCT (O[2](#Thmobs2 "O 2 ‣ 3.2 Time Overhead of
    Current and Potential Compression Methods ‣ 3 Experimental Analysis ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")).
    This is primarily due to the time-consuming decompression operations and identification
    of less-important tokens. Consequently, these methods are unsuitable for online
    LLM applications such as coding assistance and dialogue generation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对 KVC 限制，提出了各种模型压缩方法 [[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24)]。在我们的研究中，我们观察到压缩
    KV 值在准确性和作业完成时间 (JCT) 上提供了比压缩模型本身更多的好处 (O[1](#Thmobs1 "O 1 ‣ 3.1 Advantages of
    QKV Compression Compared to Model Compression ‣ 3 Experimental Analysis ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference"))。现有方法通过量化
    [[25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)] 来压缩 KV 值。另一种方法是 KVC 驱逐，即丢弃不重要的令牌
    [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]。然而，这两种方法都会导致显著的运行时计算时间开销，导致
    JCT 显著延迟 (O[2](#Thmobs2 "O 2 ‣ 3.2 Time Overhead of Current and Potential Compression
    Methods ‣ 3 Experimental Analysis ‣ Zero-Delay QKV Compression for Mitigating
    KV Cache and Network Bottlenecks in LLM Inference"))。这主要是由于耗时的解压操作和识别不重要令牌的过程。因此，这些方法不适用于在线
    LLM 应用，如编码辅助和对话生成。
- en: Furthermore, for long prompts, relying solely on compression methods proves
    insufficient. For instance, compressing a 64K prompt by 75% still results in a
    size that cannot be accommodated in a GPU memory. To tackle this issue, sequence
    parallelism (SP) employing multiple GPUs can process sequence partitions in parallel.
    However, it leads to high network communication overhead for aggregating all QKV
    values across GPUs. Unfortunately, existing compression approaches fail to mitigate
    this high communication time overhead (O[5](#Thmobs5 "O 5 ‣ 3.4 High Communication
    Time in SP ‣ 3 Experimental Analysis ‣ Zero-Delay QKV Compression for Mitigating
    KV Cache and Network Bottlenecks in LLM Inference")) or computation time because
    they only compress KV values after transmission and computation since decompressed
    data is required for computation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于长提示，仅依靠压缩方法是不够的。例如，将一个64K提示压缩75%后，得到的大小仍然无法适应GPU内存。为了解决这个问题，使用多个GPU的序列并行（SP）可以并行处理序列分区。然而，这会导致高网络通信开销，因为需要在GPU之间汇总所有的QKV值。不幸的是，现有的压缩方法未能减轻这种高通信时间开销（O[5](#Thmobs5
    "O 5 ‣ 3.4 High Communication Time in SP ‣ 3 Experimental Analysis ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")）或计算时间，因为它们仅在传输和计算后压缩KV值，因为计算需要解压缩的数据。
- en: To tackle these issues, we leverage insights from our experimental analysis
    and propose ZDC, a Zerod-delay QKV Compression system that not only eliminates
    the compression time overhead but also reduces computation and communication time.
    It comprises three key components as below.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些问题，我们利用实验分析中的见解，提出了ZDC，即零延迟QKV压缩系统，该系统不仅消除了压缩时间开销，还减少了计算和通信时间。它包含以下三个关键组件。
- en: SVD-based zero-delay compression. Based on LLM model operations, we establish
    two lemmas indicating conditions to eliminate 1) compression matrix computation
    time, 2) compression time, and 3) decompression time. We find that the Singular
    Value Decomposition (SVD) compression method meets these conditions. Consequently,
    ZDC utilizes SVD to derive compression matrices for each head of every transformer
    layer offline to achieve 1). ZDC seamlessly integrates compression and decompression
    operations on $Q$ into model inference operations. Consequently, the delays associated
    with compression and decompression are concealed within the model operation. Specifically,
    to achieve 2), the compression matrices for the QKV matrices are embedded within
    the model’s parameters. To achieve 3), ZDC uses the matrix multiplication of compressed
    matrices to directly yield an approximation of the result obtained by multiplying
    their uncompressed counterparts in model operations. This way, ZDC compresses
    the QKV data before computation and network communication in SP, thus significantly
    reducing both latencies.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于SVD的零延迟压缩。基于LLM模型操作，我们建立了两个引理，指明了消除1）压缩矩阵计算时间，2）压缩时间和3）解压缩时间的条件。我们发现，奇异值分解（SVD）压缩方法满足这些条件。因此，ZDC利用SVD离线推导每个变换器层每个头的压缩矩阵，以实现1）。ZDC将压缩和解压缩操作无缝集成到$Q$的模型推理操作中。因此，与压缩和解压缩相关的延迟被隐藏在模型操作中。具体而言，为了实现2），QKV矩阵的压缩矩阵被嵌入到模型的参数中。为了实现3），ZDC使用压缩矩阵的矩阵乘法来直接得出通过在模型操作中乘以未压缩矩阵得到的结果的近似值。通过这种方式，ZDC在SP中的计算和网络通信之前压缩QKV数据，从而显著减少了延迟。
- en: Adaptive hybrid compression ratio determination. Based on our observations that
    closer model layers share more important and unimportant tokens, and shallower
    layers contain more unimportant tokens (O[3](#Thmobs3 "O 3 ‣ 3.3 Token Importance
    Features in Layers ‣ 3 Experimental Analysis ‣ Zero-Delay QKV Compression for
    Mitigating KV Cache and Network Bottlenecks in LLM Inference") and O[4](#Thmobs4
    "O 4 ‣ 3.3 Token Importance Features in Layers ‣ 3 Experimental Analysis ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")),
    to improve the trade-off between the effectiveness of QKV compression and accuracy
    decrease, ZDC adaptively determines the compression ratios in a hybrid layer-token
    manner with low overhead. First, it offline determines layers that have the same
    important and unimportant token sets. Second, it determines different compression
    ratios for important and unimportant tokens in each layer and selects more tokens
    as important tokens in deeper layers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应混合压缩比确定。基于我们观察到的更接近的模型层共享更多重要和不重要的token，以及较浅层包含更多不重要的token（O[3](#Thmobs3
    "O 3 ‣ 3.3 Token Importance Features in Layers ‣ 3 Experimental Analysis ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")和O[4](#Thmobs4
    "O 4 ‣ 3.3 Token Importance Features in Layers ‣ 3 Experimental Analysis ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")），为了改善QKV压缩效果与准确性下降之间的权衡，ZDC自适应地以混合层-token的方式确定压缩比，开销低。首先，它离线确定具有相同重要和不重要token集合的层。其次，它为每一层中的重要和不重要token确定不同的压缩比，并在较深的层中选择更多的token作为重要token。
- en: Communication-efficient sequence parallelism. It is the first communication-efficient
    sequence parallelism system for long prompt inference, incorporating a sequence
    parallelism framework and reducing its communication overhead.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 高效通信序列并行。它是第一个针对长提示推断的高效通信序列并行系统，结合了序列并行框架并减少了通信开销。
- en: 'In summary, our work has the following contributions:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的工作有以下贡献：
- en: $\bullet$
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We derive insightful observations from our experimental analysis, laying the
    groundwork for ZDC.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从实验分析中得出了有洞察力的观察，为ZDC奠定了基础。
- en: $\bullet$
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We propose ZDC as the first solution to not only achieve zero delay but also
    reduce computation time and communication time in SP for long prompts, while also
    minimizing data volume in KVC.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出ZDC作为第一个不仅实现零延迟，而且减少长提示中SP的计算时间和通信时间的解决方案，同时在KVC中最小化数据量。
- en: $\bullet$
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Comprehensive experiments demonstrate that ZDC achieves up to 80% lower JCT
    and 35% lower perplexity compared to state-of-the-art method methods. ZDC also
    reduces the average JCT of current LLM serving systems by up to 91% with the constraint
    of 0.1 perplexity increase.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合实验表明，与最先进的方法相比，ZDC实现了高达80%的JCT降低和35%的困惑度降低。ZDC还在0.1困惑度增加的限制下，将当前LLM服务系统的平均JCT降低了高达91%。
- en: Note that ZDC is orthogonal to and can complement the existing KV quantization
    and KV eviction methods, reducing their time overhead while maintaining accuracy.
    We open-sourced the code of ZDC [[31](#bib.bib31)].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，ZDC与现有的KV量化和KV驱逐方法是正交的，并且可以补充这些方法，减少它们的时间开销，同时保持准确性。我们开源了ZDC的代码[[31](#bib.bib31)]。
- en: 2 Background
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 Attention Mechanism of Transformers
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 变压器的注意力机制
- en: '![Refer to caption](img/8a4f1ae91ed6580f3bc71ace4582203f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8a4f1ae91ed6580f3bc71ace4582203f.png)'
- en: 'Figure 1: The overview of a typical transformer layer.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：典型变压器层的概述。
- en: 'A typical transformer-based LLM comprises multiple identical transformer layers,
    as depicted in Fig.[1](#S2.F1 "Figure 1 ‣ 2.1 Attention Mechanism of Transformers
    ‣ 2 Background ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network
    Bottlenecks in LLM Inference"). Each transformer layer mainly consists of a multi-layer
    perceptron (MLP) and a multi-head attention mechanism, which encompasses three
    main parts, as highlighted in red. The mechanism enables the model to evaluate
    the interdependencies among all tokens in a sentence across different aspects
    represented by different attention heads. Within head $h$ through QKV generation
    ( 1):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '一个典型的基于 Transformer 的 LLM 包含多个相同的 Transformer 层，如图 [1](#S2.F1 "Figure 1 ‣ 2.1
    Attention Mechanism of Transformers ‣ 2 Background ‣ Zero-Delay QKV Compression
    for Mitigating KV Cache and Network Bottlenecks in LLM Inference") 所示。每个 Transformer
    层主要由一个多层感知机（MLP）和一个多头注意力机制组成，其中包括三个主要部分，如红色所示。该机制使模型能够评估句子中所有 token 之间的相互依赖关系，通过不同的注意力头表示不同的方面。在头
    $h$ 内，通过 QKV 生成（ 1):'
- en: '|  | $Q^{h}=EW_{Q}^{h}\mbox{, ~{}}K^{h}=EW_{K}^{h}\mbox{, ~{}}V^{h}=EW_{V}^{h},\vspace{-0.05in}$
    |  | (1) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{h}=EW_{Q}^{h}\mbox{, ~{}}K^{h}=EW_{K}^{h}\mbox{, ~{}}V^{h}=EW_{V}^{h},\vspace{-0.05in}$
    |  | (1) |'
- en: 'where $W_{Q}^{h}$:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $W_{Q}^{h}$:'
- en: '|  | $O^{h}=\mbox{Softmax}(\frac{Q^{h}(K^{h})^{T}}{\sqrt{d_{h}}})V^{h}=P^{h}V^{h}.\vspace{-0.05in}$
    |  | (2) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $O^{h}=\mbox{Softmax}(\frac{Q^{h}(K^{h})^{T}}{\sqrt{d_{h}}})V^{h}=P^{h}V^{h}.\vspace{-0.05in}$
    |  | (2) |'
- en: 'The softmax function operates row-wise on the input matrix $[a_{i,j}]$ as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 函数对输入矩阵 $[a_{i,j}]$ 进行逐行操作，如下所示：
- en: '|  | $\frac{exp(a_{i,j})}{\sum_{k=1}^{t_{i}}exp(a_{i,k})},\vspace{-0in}$ |  |
    (3) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{exp(a_{i,j})}{\sum_{k=1}^{t_{i}}exp(a_{i,k})},\vspace{-0in}$ |  |
    (3) |'
- en: 'where $t_{i}$ to produce the linear layer output:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t_{i}$ 用于生成线性层输出：
- en: '|  | $O_{L}=[O^{1},O^{2},...,O^{N_{h}}]W_{L}=OW_{L}.\vspace{-0.05in}$ |  |
    (4) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $O_{L}=[O^{1},O^{2},...,O^{N_{h}}]W_{L}=OW_{L}.\vspace{-0.05in}$ |  |
    (4) |'
- en: Table [1](#S2.T1 "Table 1 ‣ 2.2 Singular Value Decomposition ‣ 2 Background
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference") lists the primary notations used in the paper.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S2.T1 "Table 1 ‣ 2.2 Singular Value Decomposition ‣ 2 Background ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")
    列出了论文中使用的主要符号。
- en: 2.2 Singular Value Decomposition
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 奇异值分解
- en: '![Refer to caption](img/486ad58392df3ce3a145fe100fdb93fa.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/486ad58392df3ce3a145fe100fdb93fa.png)'
- en: 'Figure 2: SVD for data compression.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 数据压缩的 SVD。'
- en: SVD is widely used for dimensionality reduction in datasets, aiming to retain
    as much original information as possible. Given a matrix $A$ as the compression
    matrix, as depicted in Fig. [2](#S2.F2 "Figure 2 ‣ 2.2 Singular Value Decomposition
    ‣ 2 Background ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network
    Bottlenecks in LLM Inference").
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: SVD 被广泛应用于数据集的降维，旨在保留尽可能多的原始信息。给定一个矩阵 $A$ 作为压缩矩阵，如图 [2](#S2.F2 "Figure 2 ‣ 2.2
    Singular Value Decomposition ‣ 2 Background ‣ Zero-Delay QKV Compression for Mitigating
    KV Cache and Network Bottlenecks in LLM Inference") 所示。
- en: '| $d$ | The number of heads |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| $d$ | 头的数量 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| $N_{l}$) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| $N_{l}$) |'
- en: '| $E$ | Query matrix |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| $E$ | 查询矩阵 |'
- en: '| $K$ | Value matrix |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| $K$ | 值矩阵 |'
- en: '| $g$ | Rotation matrix |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| $g$ | 旋转矩阵 |'
- en: '| $p$ | Parameter matrix |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| $p$ | 参数矩阵 |'
- en: 'Table 1: Notations used in the paper.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 论文中使用的符号。'
- en: 3 Experimental Analysis
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验分析
- en: '| Trace | Input length | Output length | Arrival rate (req/s) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 跟踪 | 输入长度 | 输出长度 | 到达率 (req/s) |'
- en: '|  | avg | min | max | avg | min | max |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | 平均值 | 最小值 | 最大值 | 平均值 | 最小值 | 最大值 |'
- en: '| The Pile | 2.5K | 184 | 19.2K | 446 | 252 | 1.1K | 32 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| The Pile | 2.5K | 184 | 19.2K | 446 | 252 | 1.1K | 32 |'
- en: '| ShareGPT | 161 | 16 | 3.2K | 338 | 19 | 991 | 128 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| ShareGPT | 161 | 16 | 3.2K | 338 | 19 | 991 | 128 |'
- en: 'Table 2: Trace properties and experiment settings.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 跟踪属性和实验设置。'
- en: In the experimental analysis, unless otherwise specified, we utilized Meta Llama-2
    [[6](#bib.bib6)] with a model size of 13B and a dataset combined from "The Pile"
    [[33](#bib.bib33)] and ShareGPT [[34](#bib.bib34)]. “The Pile” is a dataset collected
    from 22 topics (e.g., GitHub, ArXiv, and Wikipedia) by EleutherAI [[33](#bib.bib33)]
    and used to train LLM. As it does not provide output sequences, we fed its input
    sequences into Llama-2 [[6](#bib.bib6)] to obtain the ground-truth output. Table [2](#S3.T2
    "Table 2 ‣ 3 Experimental Analysis ‣ Zero-Delay QKV Compression for Mitigating
    KV Cache and Network Bottlenecks in LLM Inference") lists the features of the
    datasets.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验分析中，除非另有说明，我们使用了Meta Llama-2 [[6](#bib.bib6)]，其模型大小为13B，并结合了“The Pile” [[33](#bib.bib33)]
    和ShareGPT [[34](#bib.bib34)]的数据集。“The Pile”是由EleutherAI [[33](#bib.bib33)]从22个主题（例如GitHub、ArXiv和Wikipedia）收集的数据集，用于训练LLM。由于它不提供输出序列，我们将其输入序列馈入Llama-2
    [[6](#bib.bib6)] 以获取真实输出。表[2](#S3.T2 "Table 2 ‣ 3 Experimental Analysis ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")列出了数据集的特征。
- en: We employed two AWS p4de.24xlarge instances [[35](#bib.bib35)] located in two
    servers. Each instance is equipped with 8 NVIDIA A100 GPUs (each with 80 GiB memory),
    96 vCPUs, and 1152 GiB host memory. Long prompts were partitioned into 2K-token
    segments and processed using SP since 2K typically is the maximum number of input
    tokens permitted for a single iteration in an LLM [[36](#bib.bib36)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了两台AWS p4de.24xlarge实例[[35](#bib.bib35)]，分别位于两个服务器中。每个实例配备了8个NVIDIA A100
    GPU（每个GPU有80 GiB内存）、96个vCPUs和1152 GiB主机内存。长提示被分割成2K-token的片段，并使用SP进行处理，因为2K通常是LLM中单次迭代允许的最大输入token数量[[36](#bib.bib36)]。
- en: As in [[37](#bib.bib37)], we executed 16 concurrent clients to dispatch requests
    from a single dataset. The requests and partitioned requests were allocated to
    the 16 GPUs in a round-robin manner. If there were insufficient GPUs available
    to process a long prompt length, the request would be queued until the requisite
    GPU resources became available. Our system was built on FastGen [[37](#bib.bib37)],
    which builds upon and outperforms vLLM [[38](#bib.bib38)]. FastGen enhances upon
    vLLM’s capabilities by segmenting long prompts into chunks and consolidating each
    chunk with other token generation tasks within a single iteration to maximize
    throughput. We used Ulysses [[39](#bib.bib39)], originally designed for training,
    as the SP framework for inference.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如[[37](#bib.bib37)]所示，我们执行了16个并发客户端从单一数据集派发请求。这些请求和分割后的请求被以轮询方式分配给16个GPU。如果处理长提示长度的GPU不足，请求将被排队，直到所需的GPU资源变得可用。我们的系统建立在FastGen
    [[37](#bib.bib37)]上，FastGen基于vLLM [[38](#bib.bib38)]并超越了其性能。FastGen通过将长提示分段并将每个分段与其他token生成任务整合到单次迭代中，以最大化吞吐量，增强了vLLM的能力。我们使用了Ulysses
    [[39](#bib.bib39)]，该框架最初设计用于训练，现在作为推理的SP框架。
- en: 3.1 Advantages of QKV Compression Compared to Model Compression
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**QKV压缩**相比于模型压缩的优点'
- en: 'To compress model parameters (including activations) and QKV, we employed 4-bit
    quantization. For QKV compression, we employed two methods: 1) compressing QKV
    before step 1,
    necessitating decompression before softmax computation, and 2) compressing QKV
    only after step 2,
    thereby avoiding additional computation time. Fig. [3(a)](#S3.F3.sf1 "Figure 3(a)
    ‣ Figure 3 ‣ 3.1 Advantages of QKV Compression Compared to Model Compression ‣
    3 Experimental Analysis ‣ Zero-Delay QKV Compression for Mitigating KV Cache and
    Network Bottlenecks in LLM Inference") compares the average perplexity for requests
    with sequence length (the sum of input and output lengths) within a specified
    range. QKV compression of methods 1 and 2 exhibits 12-32% and 15-36% lower average
    perplexity compared to model compression. This discrepancy arises because QKV
    compression only sacrifices precision at attention layers, whereas model compression
    incurs precision loss at both attention and MLP layers. QKV of method 1 has 2-5%
    higher average perplexity than method 2 because it has compression and decompression
    of QKV for communication, leading to precision loss.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了压缩模型参数（包括激活值）和QKV，我们采用了4-bit量化。对于QKV压缩，我们使用了两种方法：1）在第1步骤之前压缩QKV，需要在softmax计算之前进行解压缩；2）仅在第2步骤之后压缩QKV，从而避免了额外的计算时间。图 [3(a)](#S3.F3.sf1
    "图 3(a) ‣ 图 3 ‣ 3.1 QKV压缩相较于模型压缩的优势 ‣ 3 实验分析 ‣ 零延迟QKV压缩以缓解LLM推理中的KV缓存和网络瓶颈") 比较了在指定范围内序列长度（输入和输出长度之和）请求的平均困惑度。与模型压缩相比，方法1和方法2的QKV压缩分别显示出低12-32%和15-36%的平均困惑度。这一差异产生的原因在于QKV压缩仅在注意力层上牺牲了精度，而模型压缩在注意力层和MLP层都发生了精度损失。方法1的QKV比方法2的平均困惑度高2-5%，因为它涉及QKV的压缩和解压缩用于通信，从而导致精度损失。
- en: '![Refer to caption](img/d5fb51fa816fa1d5f57a388ff9106a90.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d5fb51fa816fa1d5f57a388ff9106a90.png)'
- en: (a) Perplexity.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 困惑度。
- en: '![Refer to caption](img/ffa5de202b0c647f700a77ef0169e41a.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ffa5de202b0c647f700a77ef0169e41a.png)'
- en: (b) JCT.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: (b) JCT。
- en: 'Figure 3: 4-bit quantization on the model and QKV.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：模型和QKV上的4-bit量化。
- en: Fig. [3(b)](#S3.F3.sf2 "Figure 3(b) ‣ Figure 3 ‣ 3.1 Advantages of QKV Compression
    Compared to Model Compression ‣ 3 Experimental Analysis ‣ Zero-Delay QKV Compression
    for Mitigating KV Cache and Network Bottlenecks in LLM Inference") compares the
    average JCT decomposed into different components, where “Compression” includes
    the time for quantization and dequantization. QKV compression of methods 1 and
    2 exhibits 28-51% and 15-52% lower average JCT than model compression overall.
    Additionally, they demonstrate 17-83% and 43-85% lower average time overhead than
    model compression because the amount of a request’s KV data in every iteration
    is smaller than the number of model parameters. Moreover, method 1 showcases a
    65-79% lower average communication time than model compression and method 2 for
    long prompts utilizing SP because method 1’s QKV compression reduces the amount
    of QKV data to be transferred. Finally, all methods exhibit similar attention
    and non-attention computation times because these operations are performed on
    the original decompressed data. Method 1 underperforms Method 2 for short prompts
    due to its additional decompression time but outperforms method 2 for long prompts
    because of the saved communication time in SP.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3(b)](#S3.F3.sf2 "Figure 3(b) ‣ Figure 3 ‣ 3.1 Advantages of QKV Compression
    Compared to Model Compression ‣ 3 Experimental Analysis ‣ Zero-Delay QKV Compression
    for Mitigating KV Cache and Network Bottlenecks in LLM Inference") 比较了不同组件的平均
    JCT，其中“压缩”包括量化和反量化的时间。方法 1 和方法 2 的 QKV 压缩总体上比模型压缩低 28-51% 和 15-52%。此外，它们的平均时间开销比模型压缩低
    17-83% 和 43-85%，因为每次迭代中请求的 KV 数据量比模型参数的数量要小。此外，方法 1 在使用 SP 的长提示下，比模型压缩和方法 2 展现了
    65-79% 的较低平均通信时间，因为方法 1 的 QKV 压缩减少了需要传输的 QKV 数据量。最后，所有方法在注意力和非注意力计算时间上表现相似，因为这些操作是在原始解压数据上进行的。由于额外的解压时间，方法
    1 在短提示下表现不如方法 2，但由于在 SP 中节省了通信时间，方法 1 在长提示下优于方法 2。
- en: O 1
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: O 1
- en: Compressing QKV results in lower perplexity and lower JCT compared to compressing
    model parameters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与压缩模型参数相比，压缩 QKV 结果具有更低的困惑度和更低的 JCT。
- en: 3.2 Time Overhead of Current and Potential Compression Methods
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 当前和潜在压缩方法的时间开销
- en: '![Refer to caption](img/034545649d03e093b47cd1a0e718c2ae.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/034545649d03e093b47cd1a0e718c2ae.png)'
- en: 'Figure 4: Average JCT.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：平均 JCT。
- en: We compared a KV eviction method (H2O [[28](#bib.bib28)]) and a KV compression
    method (GEAR [[25](#bib.bib25)]). Fig. [4](#S3.F4 "Figure 4 ‣ 3.2 Time Overhead
    of Current and Potential Compression Methods ‣ 3 Experimental Analysis ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")
    shows the average JCT versus the sequence lengths within specific ranges. The
    compression/eviction time and communication time of H2O amount to 17-28% and 0-27%
    of the average JCT, respectively; those of GEAR are 20-26% and 0-23%. It is noteworthy
    that 28% of JCT equals 7s. These metrics tend to increase as sequence lengths
    grow due to the processing and transfer of more tokens.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了一种 KV 驱逐方法（H2O [[28](#bib.bib28)]）和一种 KV 压缩方法（GEAR [[25](#bib.bib25)]）。图[4](#S3.F4
    "Figure 4 ‣ 3.2 Time Overhead of Current and Potential Compression Methods ‣ 3
    Experimental Analysis ‣ Zero-Delay QKV Compression for Mitigating KV Cache and
    Network Bottlenecks in LLM Inference") 显示了在特定范围内序列长度与平均 JCT 的关系。H2O 的压缩/驱逐时间和通信时间分别占平均
    JCT 的 17-28% 和 0-27%；GEAR 的分别为 20-26% 和 0-23%。值得注意的是，28% 的 JCT 相当于 7 秒。这些指标随着序列长度的增加而增加，因为处理和传输的
    tokens 数量增多。
- en: '![Refer to caption](img/bde3555e04731c1c53201bd65c58c2c1.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bde3555e04731c1c53201bd65c58c2c1.png)'
- en: 'Figure 5: JCT decomposition.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：JCT 分解。
- en: We then study employing SVD using the traditional compression methodology (as
    explained in [§ 2.1](#S2.SS1 "2.1 Attention Mechanism of Transformers ‣ 2 Background
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference")) for QKV compression online. Its time overhead mainly stems from
    several stages, including rotation matrix $R$, compression, and decompression
    overhead account for 45-62%, 19-30%, 9-13%, and 10%-12% of JCT, respectively,
    increasing as the sequence length increases.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着研究了使用传统压缩方法（如 [§ 2.1](#S2.SS1 "2.1 Attention Mechanism of Transformers ‣
    2 Background ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network
    Bottlenecks in LLM Inference")）对 QKV 进行在线压缩的 SVD。其时间开销主要来自几个阶段，包括旋转矩阵 $R$、压缩和解压缩的开销，分别占
    JCT 的 45-62%、19-30%、9-13% 和 10%-12%，随着序列长度的增加而增加。
- en: O 2
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: O 2
- en: Both current KVC compression methods and SVD significantly increase JCT (e.g.,
    55% of JCT) due to their inherent time overhead and their inability to reduce
    communication overhead in SP.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的KVC压缩方法和SVD显著增加了JCT（例如，JCT增加了55%），这是由于它们固有的时间开销和无法减少SP中的通信开销。
- en: 3.3 Token Importance Features in Layers
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 层中的标记重要性特征
- en: 'We investigate the distribution of attention scores in each layer of the model.
    Varying parameters of different models may lead to different attention scores.
    Therefore, we conducted additional tests on the OPT 13B model [[40](#bib.bib40)].
    Figure [6](#S3.F6 "Figure 6 ‣ 3.3 Token Importance Features in Layers ‣ 3 Experimental
    Analysis ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks
    in LLM Inference") illustrates the cumulative distribution function (CDF) of layers
    versus attention scores. We observe that for both OPT and Llama-2, the numerical
    range of the attention scores gradually narrows down from the shallow layer 1
    to the deep layer 40\. Specifically, the range of OPT decreases from [-92, 95]
    to [-4, 5], while that of Llama-2 decreases from [-106, 101] to [-6, 4]. This
    suggests that after converting scores into probabilities via the softmax function,
    probabilities in shallow layers become highly polarized towards 0 and 1, with
    many being extremely close to zero and some nearing 1\. In contrast, probabilities
    in deep layers tend to be close to each other and contribute similarly to the
    attention output. Therefore, we make an observation that:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了模型中每一层的注意力得分分布。不同模型的参数变化可能导致不同的注意力得分。因此，我们对OPT 13B模型进行了额外的测试[[40](#bib.bib40)]。图[6](#S3.F6
    "图6 ‣ 3.3 层中的标记重要性特征 ‣ 3 实验分析 ‣ 零延迟QKV压缩以缓解LLM推理中的KV缓存和网络瓶颈")展示了层与注意力得分的累积分布函数（CDF）。我们观察到，对于OPT和Llama-2，注意力得分的数值范围从浅层1逐渐缩小到深层40。例如，OPT的范围从[-92,
    95]减少到[-4, 5]，而Llama-2的范围从[-106, 101]减少到[-6, 4]。这表明，在通过softmax函数将得分转换为概率后，浅层的概率变得高度极化，接近0和1，其中许多接近于零，一些接近于1。而深层的概率趋向于彼此接近，并且对注意力输出的贡献类似。因此，我们得出这样的观察：
- en: O 3
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: O 3
- en: Shallower layers tend to have more unimportant tokens compared to deep layers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于深层，浅层往往包含更多的不重要标记。
- en: '![Refer to caption](img/addcca655160664325a16fc2c08766c6.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/addcca655160664325a16fc2c08766c6.png)'
- en: (a) OPT 13B.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: (a) OPT 13B。
- en: '![Refer to caption](img/7cc353b8e933727a3740861db7a5c689.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/7cc353b8e933727a3740861db7a5c689.png)'
- en: (b) Llama-2 13B.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Llama-2 13B。
- en: 'Figure 6: CDF of attention scores among different layers.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：不同层之间注意力得分的CDF。
- en: '![Refer to caption](img/e7e332c78a9fbb3f87557238dd215728.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e7e332c78a9fbb3f87557238dd215728.png)'
- en: (a) OPT 13B.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: (a) OPT 13B。
- en: '![Refer to caption](img/54cef5a9dfdc8850e564e26175b05bdb.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/54cef5a9dfdc8850e564e26175b05bdb.png)'
- en: (b) Llama-2 13B.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Llama-2 13B。
- en: 'Figure 7: CDF of layer pairs v.s. unimportant token repetition ratio for different
    layer distances $\Delta$.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：不同层距$\Delta$下，层对的不重要标记重复比的CDF。
- en: '![Refer to caption](img/1eba0224090c4087e12eac4a73431f0a.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1eba0224090c4087e12eac4a73431f0a.png)'
- en: (a) OPT 13B.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (a) OPT 13B。
- en: '![Refer to caption](img/6b95eedffecad8b067feffb55ac43246.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/6b95eedffecad8b067feffb55ac43246.png)'
- en: (b) Llama-2 13B.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Llama-2 13B。
- en: 'Figure 8: CDF of layer pairs v.s. important token repetition ratio for different
    layer distances $\Delta$.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：不同层距$\Delta$下，层对的重要标记重复比的CDF。
- en: '![Refer to caption](img/ae3b54759fa6e1f567c157cdf004e096.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/ae3b54759fa6e1f567c157cdf004e096.png)'
- en: 'Figure 9: The JCT of sequences w/ and w/o SP.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：有无SP的序列JCT。
- en: To determine whether different layers share the same unimportant tokens, we
    recorded 35% of tokens with the lowest average scores in each layer. Then, we
    calculated the repetition ratio of those low-score tokens between a layer and
    other layers. We use $\Delta$. We can make the same observations as Fig. [7](#S3.F7
    "Figure 7 ‣ 3.3 Token Importance Features in Layers ‣ 3 Experimental Analysis
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference").
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定不同层是否共享相同的不重要标记，我们记录了每层中35%的最低平均得分标记。然后，我们计算了这些低得分标记在一个层和其他层之间的重复比。我们使用了$\Delta$。我们可以得到与图[7](#S3.F7
    "图7 ‣ 3.3 层中的标记重要性特征 ‣ 3 实验分析 ‣ 零延迟QKV压缩以缓解LLM推理中的KV缓存和网络瓶颈")相同的观察结果。
- en: O 4
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: O 4
- en: Closer model layers tend to share more the same important and unimportant tokens.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 较近的模型层通常共享更多相同的重要和不重要的标记。
- en: 3.4 High Communication Time in SP
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 SP中的高通信时间
- en: Without SP, a long prompt is processed on one GPU many times sequentially. Fig. [9](#S3.F9
    "Figure 9 ‣ 3.3 Token Importance Features in Layers ‣ 3 Experimental Analysis
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference") shows the average JCT for sequences within a specific length range
    w/ and w/o SP. The average JCT with SP is 46-85% lower than that w/o SP. It reduces
    the average JCT for 18-20K sequence length from 76s to 17s. The communication
    time for SP accounts for 19-45% of its average JCT.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有SP，一个长的提示会在一个GPU上多次顺序处理。图[9](#S3.F9 "Figure 9 ‣ 3.3 Token Importance Features
    in Layers ‣ 3 Experimental Analysis ‣ Zero-Delay QKV Compression for Mitigating
    KV Cache and Network Bottlenecks in LLM Inference")显示了在特定长度范围内有SP和没有SP的序列的平均JCT。使用SP的平均JCT比没有SP低46-85%。它将18-20K序列长度的平均JCT从76秒减少到17秒。SP的通信时间占其平均JCT的19-45%。
- en: O 5
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: O 5
- en: SP drastically reduces JCT, yet network communication remains a substantial
    component (e.g., 45%) of JCT.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: SP大幅减少了JCT，但网络通信仍然是JCT的重要组成部分（例如，45%）。
- en: 4 Rationales of the Design of ZDC
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 ZDC设计的理由
- en: 'O[2](#Thmobs2 "O 2 ‣ 3.2 Time Overhead of Current and Potential Compression
    Methods ‣ 3 Experimental Analysis ‣ Zero-Delay QKV Compression for Mitigating
    KV Cache and Network Bottlenecks in LLM Inference") underscores the importance
    of minimizing compression time overhead to avoid adverse effects on JCT. In the
    pursuit of reducing data volume transferred in the network for ML training and
    inference services, quantization techniques [[41](#bib.bib41), [42](#bib.bib42),
    [22](#bib.bib22), [43](#bib.bib43)] are commonly employed, albeit introducing
    additional time for compression and decompression. Even with methods mitigating
    these overheads by performing computation directly on compressed data [[41](#bib.bib41)],
    compression and decompression time remains. We present three questions:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: O[2](#Thmobs2 "O 2 ‣ 3.2 Time Overhead of Current and Potential Compression
    Methods ‣ 3 Experimental Analysis ‣ Zero-Delay QKV Compression for Mitigating
    KV Cache and Network Bottlenecks in LLM Inference")强调了最小化压缩时间开销的重要性，以避免对JCT产生不利影响。在减少网络中传输的数据量以用于ML训练和推理服务的过程中，通常会使用量化技术[[41](#bib.bib41),
    [42](#bib.bib42), [22](#bib.bib22), [43](#bib.bib43)]，尽管这些技术引入了额外的压缩和解压时间。即便采用直接对压缩数据进行计算的方法来减轻这些开销[[41](#bib.bib41)]，压缩和解压时间仍然存在。我们提出了三个问题：
- en: $\bullet$
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Can we eliminate compression time?
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们能否消除压缩时间？
- en: $\bullet$
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Can we eliminate decompression time?
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们能否消除解压时间？
- en: $\bullet$
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Can we eliminate the compression matrix computation time?
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们能否消除压缩矩阵计算时间？
- en: Given that attention computation entails matrix multiplications, a feasible
    strategy involves embedding the compression matrix into model parameters ([§ 4.1](#S4.SS1
    "4.1 Eliminating Compression Time ‣ 4 Rationales of the Design of ZDC ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")).
    Consequently, the compressed matrix can be derived directly from input-parameter
    multiplication. Subsequently, when the compressed data is multiplied with other
    compressed data, we can simultaneously obtain the product of uncompressed matrix
    data ([§ 4.2](#S4.SS2 "4.2 Eliminating Decompression Time ‣ 4 Rationales of the
    Design of ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network
    Bottlenecks in LLM Inference")). Additionally, we will design an offline approach
    for computing the compression matrices [§ 4.3](#S4.SS3 "4.3 Eliminate Compression
    Matrix Computation Time and Suitability of SVD ‣ 4 Rationales of the Design of
    ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks
    in LLM Inference"). Further details are elucidated in the ensuing subsections.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于注意力计算涉及矩阵乘法，一种可行的策略是将压缩矩阵嵌入到模型参数中（[§ 4.1](#S4.SS1 "4.1 Eliminating Compression
    Time ‣ 4 Rationales of the Design of ZDC ‣ Zero-Delay QKV Compression for Mitigating
    KV Cache and Network Bottlenecks in LLM Inference")）。因此，压缩矩阵可以通过输入-参数乘法直接得出。随后，当压缩数据与其他压缩数据相乘时，我们可以同时获得未压缩矩阵数据的乘积（[§
    4.2](#S4.SS2 "4.2 Eliminating Decompression Time ‣ 4 Rationales of the Design
    of ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks
    in LLM Inference")）。此外，我们还将设计一种离线方法来计算压缩矩阵（[§ 4.3](#S4.SS3 "4.3 Eliminate Compression
    Matrix Computation Time and Suitability of SVD ‣ 4 Rationales of the Design of
    ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks
    in LLM Inference")）。进一步的细节将在后续子节中阐述。
- en: 4.1 Eliminating Compression Time
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 消除压缩时间
- en: We illustrate this rationale using $Q$ is not compressed before being utilized
    for computation to avoid the decompression prior to computation. Decompression
    happens when KV data is fetched from KVC (as shown in the bottom of Fig. [1](#S2.F1
    "Figure 1 ‣ 2.1 Attention Mechanism of Transformers ‣ 2 Background ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")).
    This method cannot save communication time in SP or computation time.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用 $Q$ 在计算之前不进行压缩来说明这一原理，以避免计算之前的解压缩。解压缩发生在从 KVC 获取 KV 数据时（如图 [1](#S2.F1
    "图 1 ‣ 2.1 变压器的注意机制 ‣ 2 背景 ‣ 解决 LLM 推理中的 KV 缓存和网络瓶颈的零延迟 QKV 压缩") 底部所示）。这种方法无法节省
    SP 中的通信时间或计算时间。
- en: '![Refer to caption](img/dfa7d7ebe2bdde6d21b84e42af1ca673.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/dfa7d7ebe2bdde6d21b84e42af1ca673.png)'
- en: 'Figure 10: An example of eliminating compression.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：消除压缩的示例。
- en: To design a compression method devoid of compression time overhead, we can leverage
    the associativity inherent in matrix multiplication. We could pre-multiply $W_{Q}$
    before the self-attention layer, it reduces both the communication time in SP
    and the computation time of the model operation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设计一个没有压缩时间开销的方法，我们可以利用矩阵乘法中的结合性。我们可以在自注意力层之前预先乘以 $W_{Q}$，这减少了 SP 中的通信时间和模型操作的计算时间。
- en: Lemma 1
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 1
- en: Leveraging the associativity of matrix multiplication can eliminate the compression
    time overhead with a pre-determined compression matrix.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 利用矩阵乘法的结合性可以通过预先确定的压缩矩阵消除压缩时间开销。
- en: 4.2 Eliminating Decompression Time
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 消除解压缩时间
- en: Given the compressed vectors $Q^{\prime}$ with some degree of accuracy loss.
    However, this introduces additional time overhead as indicated by O[2](#Thmobs2
    "O 2 ‣ 3.2 Time Overhead of Current and Potential Compression Methods ‣ 3 Experimental
    Analysis ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks
    in LLM Inference"). To tackle this challenge, we leverage the matrix multiplication
    inherent in model operations to eliminate these decompression operations.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 给定具有一定精度损失的压缩向量 $Q^{\prime}$。然而，这会引入额外的时间开销，如 O[2](#Thmobs2 "O 2 ‣ 3.2 当前和潜在压缩方法的时间开销
    ‣ 3 实验分析 ‣ 解决 LLM 推理中的 KV 缓存和网络瓶颈的零延迟 QKV 压缩") 所示。为了解决这个挑战，我们利用模型操作中固有的矩阵乘法来消除这些解压缩操作。
- en: Since rotating row vectors of two matrices in Euclidean space preserves the
    relative relationships between vectors, it does not alter the multiplication result
    of one matrix and the transpose of the other matrix. If we have a square rotation
    matrix $R$ and the decompression operation are concurrently conducted.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在欧几里得空间中旋转两个矩阵的行向量保持了向量之间的相对关系，因此不会改变一个矩阵与另一个矩阵转置的乘积结果。如果我们有一个方阵旋转矩阵 $R$，并且解压缩操作是同时进行的。
- en: Recall that given the self-attention output matrix $[P^{1}V^{1},...,P^{N_{h}}V^{N_{h}}]$
    for compression.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，给定自注意力输出矩阵 $[P^{1}V^{1},...,P^{N_{h}}V^{N_{h}}]$ 进行压缩。
- en: Unlike the online generation of $Q$ (based on Eq. ([4](#S2.E4 "Equation 4 ‣
    2.1 Attention Mechanism of Transformers ‣ 2 Background ‣ Zero-Delay QKV Compression
    for Mitigating KV Cache and Network Bottlenecks in LLM Inference"))).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于 Eq. ([4](#S2.E4 "方程 4 ‣ 2.1 变压器的注意机制 ‣ 2 背景 ‣ 解决 LLM 推理中的 KV 缓存和网络瓶颈的零延迟
    QKV 压缩")) 的 $Q$ 的在线生成不同。
- en: Lemma 2
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 2
- en: We can directly multiply compressed matrices without introducing decompression
    overhead to obtain the original product of uncompressed matrices if we can find
    a common rotation matrix $R$ for them in which the rotated vectors have many near-zero
    dimensions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能找到一个共同的旋转矩阵 $R$，使得旋转后的向量有许多接近零的维度，我们可以直接乘以压缩矩阵，而无需引入解压缩开销，以获得未压缩矩阵的原始乘积。
- en: '![Refer to caption](img/4635bed8ce89479af172c930727ebbe5.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4635bed8ce89479af172c930727ebbe5.png)'
- en: 'Figure 11: Illustration of ZDC’s zero-delay compression.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：ZDC 零延迟压缩的示意图。
- en: 4.3 Eliminate Compression Matrix Computation Time and Suitability of SVD
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消除压缩矩阵计算时间及 SVD 的适用性
- en: To address the challenge of eliminating rotation matrix computation time, we
    propose finding a common $R$ for dimension reduction. At the same time, we will
    check if SVD satisfies the conditions in both Lemma[1](#Thmlem1 "Lemma 1 ‣ 4.1
    Eliminating Compression Time ‣ 4 Rationales of the Design of ZDC ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")
    and Lemma[2](#Thmlem2 "Lemma 2 ‣ 4.2 Eliminating Decompression Time ‣ 4 Rationales
    of the Design of ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and
    Network Bottlenecks in LLM Inference").
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决消除旋转矩阵计算时间的问题，我们提出找到一个通用的$R$用于降维。同时，我们将检查SVD是否满足引理[1](#Thmlem1 "引理 1 ‣ 4.1
    消除压缩时间 ‣ 4 ZDC设计的理由 ‣ 减少KV缓存和网络瓶颈的零延迟QKV压缩")和引理[2](#Thmlem2 "引理 2 ‣ 4.2 消除解压缩时间
    ‣ 4 ZDC设计的理由 ‣ 减少KV缓存和网络瓶颈的零延迟QKV压缩")的条件。
- en: Finding common rotation matrix offline. Different heads capture varying information
    about words [[44](#bib.bib44)], implying that $Q$ pair within each head.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 离线寻找通用旋转矩阵。不同的头部捕捉关于词汇的不同信息[[44](#bib.bib44)]，暗示$Q$对在每个头部内。
- en: We use historical data of $Q$ pair.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用$Q$对的历史数据。
- en: '![Refer to caption](img/a203e9245f40fb56b02f13aac7eb1a74.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a203e9245f40fb56b02f13aac7eb1a74.png)'
- en: 'Figure 12: Finding $R$.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：找到$R$。
- en: To verify the effectiveness of this method, we used “The Pile” dataset with
    around 10G prompt and output tokens and Llama-2 13B to conduct an experiment.
    We use 50% of the tokens for deriving the common $R$s can be a common rotation
    matrix in Lemma [2](#Thmlem2 "Lemma 2 ‣ 4.2 Eliminating Decompression Time ‣ 4
    Rationales of the Design of ZDC ‣ Zero-Delay QKV Compression for Mitigating KV
    Cache and Network Bottlenecks in LLM Inference") and if they are effective for
    other tokens received online as implied in Lemma [1](#Thmlem1 "Lemma 1 ‣ 4.1 Eliminating
    Compression Time ‣ 4 Rationales of the Design of ZDC ‣ Zero-Delay QKV Compression
    for Mitigating KV Cache and Network Bottlenecks in LLM Inference").
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为验证该方法的有效性，我们使用了包含约10G提示和输出标记的“The Pile”数据集，并使用Llama-2 13B进行实验。我们使用50%的标记来推导通用的$R$s是否可以作为引理[2](#Thmlem2
    "引理 2 ‣ 4.2 消除解压缩时间 ‣ 4 ZDC设计的理由 ‣ 减少KV缓存和网络瓶颈的零延迟QKV压缩")中的通用旋转矩阵，并验证它们是否对在线接收到的其他标记有效，如引理[1](#Thmlem1
    "引理 1 ‣ 4.1 消除压缩时间 ‣ 4 ZDC设计的理由 ‣ 减少KV缓存和网络瓶颈的零延迟QKV压缩")中所暗示的。
- en: '![Refer to caption](img/af8ba9c5d28f7b3686db2eb9dbab906a.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/af8ba9c5d28f7b3686db2eb9dbab906a.png)'
- en: (a) QK.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (a) QK。
- en: '![Refer to caption](img/833e44411a28fe0273964066a964e073.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/833e44411a28fe0273964066a964e073.png)'
- en: (b) V$W_{L}$.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: (b) V$W_{L}$。
- en: 'Figure 13: Singular values for QK and V$W_{L}$.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：QK和V$W_{L}$的奇异值。
- en: '![Refer to caption](img/129ec344c41d3b14af79b8c46de91afd.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/129ec344c41d3b14af79b8c46de91afd.png)'
- en: (a) QK.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: (a) QK。
- en: '![Refer to caption](img/7653784715a071d0278fa29bfdcf01d6.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7653784715a071d0278fa29bfdcf01d6.png)'
- en: (b) V$W_{L}$.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: (b) V$W_{L}$。
- en: 'Figure 14: Applicability of $R$.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：$R$的适用性。
- en: We use $A$.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用$A$。
- en: Lemma 3
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理3
- en: SVD satisfies the conditions in Lemma[1](#Thmlem1 "Lemma 1 ‣ 4.1 Eliminating
    Compression Time ‣ 4 Rationales of the Design of ZDC ‣ Zero-Delay QKV Compression
    for Mitigating KV Cache and Network Bottlenecks in LLM Inference") and Lemma[2](#Thmlem2
    "Lemma 2 ‣ 4.2 Eliminating Decompression Time ‣ 4 Rationales of the Design of
    ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks
    in LLM Inference") for being the zero-delay compression method.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: SVD满足引理[1](#Thmlem1 "引理 1 ‣ 4.1 消除压缩时间 ‣ 4 ZDC设计的理由 ‣ 减少KV缓存和网络瓶颈的零延迟QKV压缩")和引理[2](#Thmlem2
    "引理 2 ‣ 4.2 消除解压缩时间 ‣ 4 ZDC设计的理由 ‣ 减少KV缓存和网络瓶颈的零延迟QKV压缩")的零延迟压缩方法的条件。
- en: 5 Design of ZDC
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 ZDC的设计
- en: 5.1 SVD-based Zero-Delay Compression
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基于SVD的零延迟压缩
- en: Fig. [11](#S4.F11 "Figure 11 ‣ 4.2 Eliminating Decompression Time ‣ 4 Rationales
    of the Design of ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and
    Network Bottlenecks in LLM Inference") demonstrates the workflow of how ZDC performs
    the zero-delay compression. It consists of 1) Offline rotation matrix computation
    ([§ 5.1](#S5.SS1 "5.1 SVD-based Zero-Delay Compression ‣ 5 Design of ZDC ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")),
    2) QK compression and decompression ([fig. 15](#S5.F15 "In 5.1 SVD-based Zero-Delay
    Compression ‣ 5 Design of ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache
    and Network Bottlenecks in LLM Inference")) and 3) V$W_{L}$ compression and decompression
    ([§ 5.1](#S5.SS1 "5.1 SVD-based Zero-Delay Compression ‣ 5 Design of ZDC ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")),
    as explained in the following sections.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [11](#S4.F11 "图 11 ‣ 4.2 消除解压时间 ‣ 4 ZDC 设计的合理性 ‣ 零延迟 QKV 压缩以缓解 LLM 推理中的 KV
    缓存和网络瓶颈") 展示了 ZDC 如何执行零延迟压缩的工作流程。它包括 1) 离线旋转矩阵计算 ([§ 5.1](#S5.SS1 "5.1 基于 SVD
    的零延迟压缩 ‣ 5 ZDC 设计 ‣ 零延迟 QKV 压缩以缓解 LLM 推理中的 KV 缓存和网络瓶颈"))，2) QK 压缩和解压 ([图 15](#S5.F15
    "在 5.1 基于 SVD 的零延迟压缩 ‣ 5 ZDC 设计 ‣ 零延迟 QKV 压缩以缓解 LLM 推理中的 KV 缓存和网络瓶颈")) 和 3) V$W_{L}$
    压缩和解压 ([§ 5.1](#S5.SS1 "5.1 基于 SVD 的零延迟压缩 ‣ 5 ZDC 设计 ‣ 零延迟 QKV 压缩以缓解 LLM 推理中的
    KV 缓存和网络瓶颈"))，如以下章节所述。
- en: Offline rotation matrix computation. The offline rotation matrix computation,
    depicted as 1
    in Fig. [11](#S4.F11 "Figure 11 ‣ 4.2 Eliminating Decompression Time ‣ 4 Rationales
    of the Design of ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and
    Network Bottlenecks in LLM Inference"), is explained in [§ 4.3](#S4.SS3 "4.3 Eliminate
    Compression Matrix Computation Time and Suitability of SVD ‣ 4 Rationales of the
    Design of ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network
    Bottlenecks in LLM Inference"). Due to its resource- and time-consuming nature,
    we propose employing pruning and K-means clustering to address this challenge.
    K-means is a common quantization technique that consolidates vectors within each
    cluster by averaging them into a single vector.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 离线旋转矩阵计算。离线旋转矩阵计算，如图 [11](#S4.F11 "图 11 ‣ 4.2 消除解压时间 ‣ 4 ZDC 设计的合理性 ‣ 零延迟 QKV
    压缩以缓解 LLM 推理中的 KV 缓存和网络瓶颈") 中的 1
    所示，详细解释见 [§ 4.3](#S4.SS3 "4.3 消除压缩矩阵计算时间与 SVD 的适用性 ‣ 4 ZDC 设计的合理性 ‣ 零延迟 QKV 压缩以缓解
    LLM 推理中的 KV 缓存和网络瓶颈")。由于其资源和时间的消耗，我们建议采用剪枝和 K-means 聚类来解决这一挑战。K-means 是一种常见的量化技术，通过将每个簇中的向量平均合并成一个向量来整合向量。
- en: We first prune data within the same topic of the sequences, under the rationale
    that similar or duplicated data samples may not significantly enhance overall
    accuracy [[45](#bib.bib45)]. In our previous experiment (for Fig. [13](#S4.F13
    "Figure 13 ‣ 4.3 Eliminate Compression Matrix Computation Time and Suitability
    of SVD ‣ 4 Rationales of the Design of ZDC ‣ Zero-Delay QKV Compression for Mitigating
    KV Cache and Network Bottlenecks in LLM Inference")), we observed that randomly
    dropping 50% of sequences in each topic before running K-means produces a set
    of QK vectors with similar singular values to those obtained from directly running
    K-means on all tokens’ QK vectors, as shown in Fig. [15](#S5.F15 "Figure 15 ‣
    5.1 SVD-based Zero-Delay Compression ‣ 5 Design of ZDC ‣ Zero-Delay QKV Compression
    for Mitigating KV Cache and Network Bottlenecks in LLM Inference"). The same observation
    was made for V$W_{L}$s.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先修剪相同主题下的序列数据，理由是相似或重复的数据样本可能不会显著提高整体准确性[[45](#bib.bib45)]。在我们之前的实验中（见图 [13](#S4.F13
    "图 13 ‣ 4.3 消除压缩矩阵计算时间和SVD的适用性 ‣ 4 ZDC的设计理由 ‣ 零延迟QKV压缩以缓解LLM推理中的KV缓存和网络瓶颈")），我们观察到在运行K-means之前随机丢弃每个主题中的50%序列会生成一组QK向量，其奇异值与直接在所有标记的QK向量上运行K-means得到的奇异值相似，如图
    [15](#S5.F15 "图 15 ‣ 5.1 基于SVD的零延迟压缩 ‣ 5 ZDC的设计 ‣ 零延迟QKV压缩以缓解LLM推理中的KV缓存和网络瓶颈")所示。对V$W_{L}$s也做了相同的观察。
- en: '![Refer to caption](img/b69eaff4bb02f55b1bfca08c3407d5cd.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b69eaff4bb02f55b1bfca08c3407d5cd.png)'
- en: 'Figure 15: Average singular value.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：平均奇异值。
- en: QK compression and decompression. In the offline procedure, we perform $W_{Q}^{R,h}=W_{Q}^{h}R^{h}$
    based on Lemma[2](#Thmlem2 "Lemma 2 ‣ 4.2 Eliminating Decompression Time ‣ 4 Rationales
    of the Design of ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and
    Network Bottlenecks in LLM Inference") without necessitating specific decompression
    operations (3).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: QK 压缩和解压。在离线过程中，我们根据引理 [2](#Thmlem2 "引理 2 ‣ 4.2 消除解压时间 ‣ 4 ZDC的设计理由 ‣ 零延迟QKV压缩以缓解LLM推理中的KV缓存和网络瓶颈")
    执行 $W_{Q}^{R,h}=W_{Q}^{h}R^{h}$，而无需特定的解压操作 (3)。
- en: V$W_{L}$.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: V$W_{L}$。
- en: 5.2 Adaptive Hybrid Compression Ratio Determination
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 自适应混合压缩比例确定
- en: '![Refer to caption](img/e512dce410fcf1a72e285b7ada7d1b94.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e512dce410fcf1a72e285b7ada7d1b94.png)'
- en: 'Figure 16: Adaptive hybrid compression ratio.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：自适应混合压缩比例。
- en: Model layers contribute differently to the output [[16](#bib.bib16)], and the
    same applies to the sequence tokens within a layer [[46](#bib.bib46)]. However,
    the importance of generated tokens cannot be determined in advance. Therefore,
    instead of setting the compression ratio $p$). Further, we select more tokens
    as important tokens in deeper layers.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 模型层对输出的贡献各不相同[[16](#bib.bib16)]，同样的，层内的序列标记也是如此[[46](#bib.bib46)]。然而，生成的标记的重要性不能事先确定。因此，我们没有设置压缩比例$p$。此外，我们选择更多的标记作为较深层次中的重要标记。
- en: '![Refer to caption](img/0dd4d9484c384fe45483281c1794664c.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0dd4d9484c384fe45483281c1794664c.png)'
- en: 'Figure 17: Average score summation time ratio over JCT.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：JCT的平均分数总和时间比例。
- en: 'When evaluating token importance, existing methods typically utilize the sum
    of scores [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]: $\sum_{h=1}^{N_{h}}\sum_{k=1}^{t_{i}}s_{k}^{h}$
    proportion of tokens from the top are classified as important tokens while the
    remaining tokens are classified as unimportant tokens. Fig. [17](#S5.F17 "Figure
    17 ‣ 5.2 Adaptive Hybrid Compression Ratio Determination ‣ 5 Design of ZDC ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")
    compares the average score summation time ratio over JCT using the two methods.
    As the sequence length increases, the average JCT ratio of the previous method
    increases from 3.3-6.5%, while our method always keeps close to 0.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估标记重要性时，现有方法通常利用得分的总和[[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]：$\sum_{h=1}^{N_{h}}\sum_{k=1}^{t_{i}}s_{k}^{h}$从顶部的标记中分类为重要标记，而其余标记分类为不重要标记。图[17](#S5.F17
    "Figure 17 ‣ 5.2 Adaptive Hybrid Compression Ratio Determination ‣ 5 Design of
    ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks
    in LLM Inference")比较了使用这两种方法的平均得分总和时间比。随着序列长度的增加，前一种方法的平均JCT比率从3.3%增加到6.5%，而我们的方法始终接近0。
- en: O[4](#Thmobs4 "O 4 ‣ 3.3 Token Importance Features in Layers ‣ 3 Experimental
    Analysis ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks
    in LLM Inference") suggests that neighboring model layers often share similar
    sets of important and unimportant tokens. Therefore, these classifications can
    be extended to the adjacent layers once we identify the important and unimportant
    token sets within a layer. However, determining which adjacent layers share similar
    sets presents a challenge. To overcome this obstacle, we offline identify layers
    that have the same important and unimportant token sets (i.e., repetition ratio
     95%).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: O[4](#Thmobs4 "O 4 ‣ 3.3 Token Importance Features in Layers ‣ 3 Experimental
    Analysis ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks
    in LLM Inference")表明，相邻的模型层通常共享相似的重要和不重要的标记集。因此，一旦我们识别了层内的重要和不重要的标记集，这些分类可以扩展到相邻的层。然而，确定哪些相邻层共享相似的集合是一个挑战。为了解决这个问题，我们离线识别具有相同重要和不重要标记集的层（即重复率 95%）。
- en: 'Accuracy-oriented Compression Ratio Determination. When no compression ratio
    is used ($p=0$. The problem of determining the compression ratio can be formulated
    as:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以准确度为导向的压缩比确定。当没有使用压缩比时($p=0$)，压缩比的确定问题可以被表述为：
- en: '|  | $1$2 |  | (5) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: 'subject to:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 约束条件：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: In addition to the $T_{Q_{d}}$ pair.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 除了$T_{Q_{d}}$对。
- en: Because the time the model spends on attention calculation, QKV transmission,
    and loading KV data from memory is positively correlated with the compression
    ratio, our objective is to maximize the compression ratios without falling below
    the required $Q_{d}$ requirement, we enumerate through different solutions and
    find a near-optimal combination.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型在注意力计算、QKV传输和从内存加载KV数据的时间与压缩比正相关，我们的目标是最大化压缩比，同时不低于所需的$Q_{d}$要求，我们通过枚举不同的解决方案来找到接近最佳的组合。
- en: 5.3 Communication-Efficient SP
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 通信高效的SP
- en: Long-prompt applications, like coding assistance, demand prompt responses. To
    address this challenge, SP can be employed. Surprisingly, although SP is utilized
    in training systems like Megatron [[36](#bib.bib36)] and Ulysses [[39](#bib.bib39)],
    no inference systems leveraging SP have been proposed to our knowledge. Given
    that Ulysses outperforms Megatron by up to 3x in throughput [[39](#bib.bib39)],
    we propose an SP-based inference framework built upon Ulysses, incorporating ZDC
    to alleviate network communication overhead. We hope for widespread adoption of
    this framework for LLM inference, as it addresses the network bottleneck hindering
    prompt response.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 长提示应用，如编码辅助，要求快速响应。为了解决这个挑战，可以使用SP。令人惊讶的是，尽管SP在像Megatron [[36](#bib.bib36)]和Ulysses
    [[39](#bib.bib39)]这样的训练系统中得到了应用，但我们所知尚未提出利用SP的推理系统。鉴于Ulysses在吞吐量上比Megatron高出多达3倍
    [[39](#bib.bib39)]，我们提出了一个基于SP的推理框架，建立在Ulysses之上，结合ZDC来减轻网络通信开销。我们希望这个框架在LLM推理中得到广泛采用，因为它解决了阻碍快速响应的网络瓶颈问题。
- en: Figure [18](#S5.F18 "Figure 18 ‣ 5.3 Communication-Efficient SP ‣ 5 Design of
    ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks
    in LLM Inference") illustrates this framework. This architecture of an attention
    block parallelizes a sequence (i.e., token embeddings) into four partitions, each
    to one GPU. There are four attention heads in the example. Each sequence partition
    undergoes linear projection to form Q, K, and V tensors. Subsequently, the four
    GPUs execute an all-to-all operation to gather tokens along the sequence dimension
    and distribute heads, ensuring each GPU has Q, K, and V tensors for one head (e.g.,
    GPU 0 for head 0). Attention [[16](#bib.bib16)] computation $Attention(Q,K,V)$
    in Eq. ([2](#S2.E2 "Equation 2 ‣ 2.1 Attention Mechanism of Transformers ‣ 2 Background
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference")) is then performed. The resulting tensors undergo a second all-to-all
    operation to gather heads and sequence partitions, enabling each GPU to obtain
    the tensors for its token partition across all four heads. Then, the second projection
    (i.e., post-self-attention linear) is triggered. Leveraging ZDC, Q, K, and V are
    compressed during the projection operation when they are calculated. Consequently,
    transmitting compressed Q, K, and V tensors in the first all-to-all communication
    significantly reduces communication time overhead. Subsequently, the attention
    operation and the subsequent projection concurrently decompress Q, K, and V tensors,
    yielding approximate original results.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图[18](#S5.F18 "图 18 ‣ 5.3 通信高效的 SP ‣ 5 ZDC 设计 ‣ 零延迟 QKV 压缩以缓解 LLM 推理中的 KV 缓存和网络瓶颈")
    说明了这一框架。这种注意力块的架构将一个序列（即，令牌嵌入）并行划分为四个分区，每个分区分配给一个 GPU。示例中有四个注意力头。每个序列分区经过线性投影形成
    Q、K 和 V 张量。随后，四个 GPU 执行全到全操作以沿序列维度收集令牌并分配头，确保每个 GPU 拥有一个头的 Q、K 和 V 张量（例如，GPU 0
    负责头 0）。接着，进行注意力[[16](#bib.bib16)]计算 $Attention(Q,K,V)$（见 Eq. ([2](#S2.E2 "公式 2
    ‣ 2.1 Transformer 的注意力机制 ‣ 2 背景 ‣ 零延迟 QKV 压缩以缓解 LLM 推理中的 KV 缓存和网络瓶颈")））。结果张量经历第二次全到全操作以收集头和序列分区，使每个
    GPU 获取其在所有四个头中的令牌分区的张量。然后，触发第二次投影（即，自注意力后的线性投影）。利用 ZDC，Q、K 和 V 在计算时会在投影操作中被压缩。因此，在第一次全到全通信中传输压缩后的
    Q、K 和 V 张量显著减少了通信时间开销。随后，注意力操作和后续投影同时解压 Q、K 和 V 张量，产生接近原始的结果。
- en: '![Refer to caption](img/7c37326905c9f02b5df39efc2827f9ca.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7c37326905c9f02b5df39efc2827f9ca.png)'
- en: 'Figure 18: Data flow of the attention mechanism in sequence parallelism framework.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：序列并行框架中注意力机制的数据流。
- en: 6 Performance Evaluation
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 性能评估
- en: 6.1 Implementation
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 实现
- en: We implemented ZDC on FastGen using PyTorch and developed our custom OPT and
    Llama-2 model classes for the ease of modifying attention layers to enable SP.
    Leveraging pre-trained models from Hugging Face [[47](#bib.bib47)], we customized
    their parameters by dumping and loading them into our model classes. To enable
    SP, we integrated part of the distributed attention code from DeepSpeed-Ulysses [[39](#bib.bib39)],
    particularly the communication code for all-to-all operations. We developed custom
    CUDA kernel functions for QKV generation and post-self-attention linear layer
    with dimension dropping. Additionally, we modified the softmax kernel function
    to facilitate lightweight determination of token importance by reusing softmax
    denominators. For request handling, we developed a client module for sending requests
    and a server front-end for request reception. To manage resource allocation, a
    proxy load balancer was implemented in Python. The load balancer allocates requests
    and partitioned requests to the 16 GPUs in a round-robin manner if GPU resources
    are available.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 FastGen 上使用 PyTorch 实现了 ZDC，并开发了自定义的 OPT 和 Llama-2 模型类，以便于修改注意力层以启用 SP。利用来自
    Hugging Face 的预训练模型[[47](#bib.bib47)]，我们通过转储和加载这些模型参数到我们的模型类中来定制它们。为了启用 SP，我们从
    DeepSpeed-Ulysses[[39](#bib.bib39)] 集成了部分分布式注意力代码，特别是全到全操作的通信代码。我们为 QKV 生成和自注意力后的线性层开发了自定义
    CUDA 内核函数，并进行了维度丢弃。此外，我们还修改了 softmax 内核函数，通过重用 softmax 分母来实现轻量级的令牌重要性判定。为了处理请求，我们开发了一个客户端模块用于发送请求，一个服务器前端用于接收请求。为了管理资源分配，我们在
    Python 中实现了一个代理负载均衡器。负载均衡器将请求和分区请求以轮询的方式分配给 16 个 GPU，如果 GPU 资源可用的话。
- en: 6.2 Compared Methods
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 比较方法
- en: We compared ZDC with H2O [[28](#bib.bib28)] and the GEAR [[25](#bib.bib25)]
    (the only archived paper we found so far). We also include Oracle which is ZDC
    that uses the optimal solution for Eq. ([5](#S5.E5 "Equation 5 ‣ 5.2 Adaptive
    Hybrid Compression Ratio Determination ‣ 5 Design of ZDC ‣ Zero-Delay QKV Compression
    for Mitigating KV Cache and Network Bottlenecks in LLM Inference")). To show the
    effectiveness of ZDC in complementing existing KVC compression and eviction methods,
    we also tested on H2O-ZDC and GEAR-ZDC, which use ZDC to compress data in model
    operations.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将ZDC与H2O [[28](#bib.bib28)]和GEAR [[25](#bib.bib25)]（我们目前找到的唯一归档论文）进行了比较。我们还包括了Oracle，它是使用最优解的ZDC，用于Eq. ([5](#S5.E5
    "Equation 5 ‣ 5.2 Adaptive Hybrid Compression Ratio Determination ‣ 5 Design of
    ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks
    in LLM Inference"))。为了展示ZDC在补充现有KVC压缩和驱逐方法方面的有效性，我们还测试了H2O-ZDC和GEAR-ZDC，这些方法使用ZDC在模型操作中压缩数据。
- en: 6.3 Experimental Setup
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 实验设置
- en: In the evaluation, we used the same server setting as in Sec. [3](#S3 "3 Experimental
    Analysis ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks
    in LLM Inference") unless otherwise specified. We tested OPT with 13B, 66B, and
    175B parameters [[40](#bib.bib40)] and Llama-2 with 7B, 13B, and 70B parameters [[6](#bib.bib6)].
    Each model used “The Pile” and ShareGPT. Using tensor parallelism [[48](#bib.bib48)],
    we partitioned the 66B and 70B model to 4 GPUs and the 175B model to 8 GPUs. All
    rotation matrices were computed from 50% of the “The Pile” with a data pruning
    ratio of 50% and $k=1$M for K-means.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估中，除非另有说明，我们使用了与第[3](#S3 "3 Experimental Analysis ‣ Zero-Delay QKV Compression
    for Mitigating KV Cache and Network Bottlenecks in LLM Inference")节相同的服务器设置。我们测试了13B、66B和175B参数的OPT [[40](#bib.bib40)]和7B、13B和70B参数的Llama-2 [[6](#bib.bib6)]。每个模型使用了“The
    Pile”和ShareGPT。通过张量并行 [[48](#bib.bib48)]，我们将66B和70B模型分配到4个GPU上，将175B模型分配到8个GPU上。所有旋转矩阵是从50%的“The
    Pile”中计算的，数据剪枝比例为50%，$k=1$M用于K-means。
- en: When comparing JCT and the normalized latency (s/tokens) [[49](#bib.bib49),
    [38](#bib.bib38)], to ensure all methods maintain ${Q_{d}}=0.1$, and the parameter
    settings for other methods.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较JCT和标准化延迟（s/tokens） [[49](#bib.bib49), [38](#bib.bib38)]时，为确保所有方法保持${Q_{d}}=0.1$，以及其他方法的参数设置。
- en: 6.4 Overall Performance
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 总体性能
- en: Fig. [19](#S6.F19 "Figure 19 ‣ 6.4 Overall Performance ‣ 6 Performance Evaluation
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference") shows the average JCT over requests for all methods on all models.
    ZDC has 58-76% and 62-80% lower average JCT than H2O and GEAR, respectively, on
    two datasets. ZDC outperforms H2O and GEAR because it eliminates the compression
    and decompression overhead and reduces the attention computation time and communication
    time of $Q$. In addition, ZDC has the adaptive hybrid compression ratio determination
    method to maximize the compression degree while maintaining the perplexity. H2O
    and GEAR generate high compression delay and cannot reduce the computation or
    communication time. H2O-ZDC and GEAR-ZDC improve H2O and GEAR by 23-47% and 29-51%,
    respectively, because SVD further compresses the hidden dimension without losing
    much data information. ZDC has only 2-4% higher average JCT than Oracle on two
    datasets, demonstrating its regression-based prediction achieves the near-optimal.
    This time overhead is negligible. ZDC’s time overhead also includes the lightweight
    token determination, which is also negligible, as shown in Fig. [17](#S5.F17 "Figure
    17 ‣ 5.2 Adaptive Hybrid Compression Ratio Determination ‣ 5 Design of ZDC ‣ Zero-Delay
    QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference").
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图[19](#S6.F19 "Figure 19 ‣ 6.4 Overall Performance ‣ 6 Performance Evaluation
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference")显示了所有模型上所有方法的平均JCT。ZDC在两个数据集上比H2O和GEAR分别低58-76%和62-80%的平均JCT。ZDC优于H2O和GEAR，因为它消除了压缩和解压缩的开销，并减少了$Q$的注意力计算时间和通信时间。此外，ZDC具有自适应混合压缩比确定方法，以在保持困惑度的同时最大化压缩度。H2O和GEAR产生高压缩延迟，无法减少计算或通信时间。由于SVD进一步压缩了隐藏维度而没有丢失太多数据，H2O-ZDC和GEAR-ZDC分别提高了H2O和GEAR的性能23-47%和29-51%。ZDC在两个数据集上的平均JCT仅比Oracle高2-4%，显示其回归预测接近最优。这个时间开销是微不足道的。ZDC的时间开销还包括轻量级的令牌确定，这也很微不足道，如图[17](#S5.F17
    "Figure 17 ‣ 5.2 Adaptive Hybrid Compression Ratio Determination ‣ 5 Design of
    ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks
    in LLM Inference")所示。
- en: '![Refer to caption](img/e00fd98a77cca8323d3d70edfcf7caf8.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e00fd98a77cca8323d3d70edfcf7caf8.png)'
- en: (a) The Pile.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: (a) The Pile.
- en: '![Refer to caption](img/dd2fa82e8ca1c064e92a64af540a205e.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/dd2fa82e8ca1c064e92a64af540a205e.png)'
- en: (b) ShareGPT.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: (b) ShareGPT.
- en: 'Figure 19: Average JCT of requests.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：请求的平均 JCT。
- en: '![Refer to caption](img/44ed860748e7c4251952549150455a32.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/44ed860748e7c4251952549150455a32.png)'
- en: (a) The Pile.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: (a) The Pile。
- en: '![Refer to caption](img/cabecc83756311efb3a65d849886a6df.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cabecc83756311efb3a65d849886a6df.png)'
- en: (b) ShareGPT.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: (b) ShareGPT。
- en: 'Figure 20: Average perplexity of requests.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：请求的平均困惑度。
- en: '![Refer to caption](img/02d57ed0170534a361b6874e063445c6.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/02d57ed0170534a361b6874e063445c6.png)'
- en: (a) Llama-2 7B (The Pile).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Llama-2 7B（The Pile）。
- en: '![Refer to caption](img/1f32a741c30ef77cdec34d72988d59ce.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1f32a741c30ef77cdec34d72988d59ce.png)'
- en: (b) Llama-2 13B (The Pile).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Llama-2 13B（The Pile）。
- en: '![Refer to caption](img/9aaff83f64b696ed51a5416e9e1b5b45.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9aaff83f64b696ed51a5416e9e1b5b45.png)'
- en: (c) Llama-2 70B (The Pile).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Llama-2 70B（The Pile）。
- en: '![Refer to caption](img/f2723119b8b68fc13eda4cb21f8fb5ee.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f2723119b8b68fc13eda4cb21f8fb5ee.png)'
- en: (d) OPT 13B (The Pile).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: (d) OPT 13B（The Pile）。
- en: '![Refer to caption](img/6c010fbddb7a662d9906baa3f2758e2a.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6c010fbddb7a662d9906baa3f2758e2a.png)'
- en: (e) OPT 66B (The Pile).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: (e) OPT 66B（The Pile）。
- en: '![Refer to caption](img/81f2a9ff4aa1de9ed787ce084c3cba65.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/81f2a9ff4aa1de9ed787ce084c3cba65.png)'
- en: (f) OPT 175B (The Pile).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: (f) OPT 175B（The Pile）。
- en: (g) Llama-2 7B (ShareGPT).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: (g) Llama-2 7B（ShareGPT）。
- en: '![Refer to caption](img/512dade2f9b0ae4453b1869774f4f1b3.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/512dade2f9b0ae4453b1869774f4f1b3.png)'
- en: (h) Llama-2 13B (ShareGPT).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: (h) Llama-2 13B（ShareGPT）。
- en: '![Refer to caption](img/3a09c87bd4e0fbcd31c354d95a81156e.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3a09c87bd4e0fbcd31c354d95a81156e.png)'
- en: (i) Llama-2 70B (ShareGPT).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: (i) Llama-2 70B（ShareGPT）。
- en: '![Refer to caption](img/43f2cffe8cacd1f18a344bfb74ef0563.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/43f2cffe8cacd1f18a344bfb74ef0563.png)'
- en: (j) OPT 13B (ShareGPT).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: (j) OPT 13B（ShareGPT）。
- en: '![Refer to caption](img/e9af3036d90b3610550a0cd14cacf654.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e9af3036d90b3610550a0cd14cacf654.png)'
- en: (k) OPT 66B (ShareGPT).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: (k) OPT 66B（ShareGPT）。
- en: '![Refer to caption](img/afb17b2b9a3523cdfa1f9533b4bb08b8.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/afb17b2b9a3523cdfa1f9533b4bb08b8.png)'
- en: (l) OPT 175B (ShareGPT).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: (l) OPT 175B（ShareGPT）。
- en: 'Figure 21: Normalized latency v.s. request rate.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：归一化延迟与请求率。
- en: Fig. [20](#S6.F20 "Figure 20 ‣ 6.4 Overall Performance ‣ 6 Performance Evaluation
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference") shows the average perplexity over requests for those methods.
    Achieving the same average JCT, ZDC has 25-32% and 27-35% lower average perplexity
    than H2O and GEAR, respectively, on the two datasets. ZDC outperforms H2O and
    GEAR because they have a higher eviction ratio and compression ratio to achieve
    a lower JCT, increasing the perplexity. H2O-ZDC and GEAR-ZDC improve H2O and GEAR
    by 7-13% and 6-16%, respectively, due to the same reasons above.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [20](#S6.F20 "图 20 ‣ 6.4 总体性能 ‣ 6 性能评估 ‣ 零延迟 QKV 压缩以缓解 LLM 推理中的 KV 缓存和网络瓶颈")
    显示了这些方法的请求平均困惑度。在实现相同的平均 JCT 时，ZDC 在两个数据集上比 H2O 和 GEAR 的平均困惑度分别低 25-32% 和 27-35%。ZDC
    超越 H2O 和 GEAR 的原因在于它们具有更高的驱逐率和压缩率，从而实现了较低的 JCT，但增加了困惑度。H2O-ZDC 和 GEAR-ZDC 分别提高了
    H2O 和 GEAR 7-13% 和 6-16%，原因如上所述。
- en: Fig. [21](#S6.F21 "Figure 21 ‣ 6.4 Overall Performance ‣ 6 Performance Evaluation
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference") shows the normalized latency versus request rates for those methods
    on each model and dataset. As the request rate rises, the latency initially escalates
    gradually but then suddenly surges. This phenomenon occurs when the request rate
    exceeds the capacity of the serving system, leading to an infinite growth in the
    queue length and subsequent latency increase for the requests. For “The Pile”,
    ZDC can sustain 1.6×–1.9× and 1.7×–2.2× higher request rates compared to H2O and
    GEAR for OPT 175B and sustain 1.8×-2.4× and 1.9×-2.8× higher request rates for
    other models, while sustaining the same latency. For ShareGPT, ZDC can sustain
    1.3×–1.6× and 1.5×–1.9× higher request rates compared to H2O and GEAR for OPT
    175B and sustain 1.5×-2× and 1.7×-2.3× higher request rates for other models.
    The reasons for the superior performance of ZDC are the same as in Fig. [19](#S6.F19
    "Figure 19 ‣ 6.4 Overall Performance ‣ 6 Performance Evaluation ‣ Zero-Delay QKV
    Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference").
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [21](#S6.F21 "Figure 21 ‣ 6.4 Overall Performance ‣ 6 Performance Evaluation
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference") 显示了这些方法在每个模型和数据集上的标准化延迟与请求率的关系。随着请求率的上升，延迟最初逐渐增加，但随后突然激增。这种现象发生在请求率超过服务系统的容量时，导致队列长度无限增长，并随之导致请求延迟增加。对于“The
    Pile”，ZDC 相比 H2O 和 GEAR 能承受 1.6×–1.9× 和 1.7×–2.2× 更高的请求率，且维持相同的延迟。对于其他模型，ZDC 能承受
    1.8×–2.4× 和 1.9×–2.8× 更高的请求率，同时保持相同的延迟。对于 ShareGPT，ZDC 相比 H2O 和 GEAR 能承受 1.3×–1.6×
    和 1.5×–1.9× 更高的请求率，且对其他模型维持 1.5×-2× 和 1.7×-2.3× 更高的请求率。ZDC 表现优越的原因与图 [19](#S6.F19
    "Figure 19 ‣ 6.4 Overall Performance ‣ 6 Performance Evaluation ‣ Zero-Delay QKV
    Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference")
    相同。
- en: ZDC’s benefit on ShareGPT is lower than “The Pile” dataset by 19-31%. This is
    because “The Pile” has long prompts, posing a much more severe burden on the KVC
    and communication bandwidth, while requests in ShareGPT are unlikely to use SP.
    We notice that as the model size becomes 175B, ZDC’s benefit diminishes by 17-25%
    on average. This is because the models run on multiple GPUs, mitigating the burden
    on the KVC. As we used two servers in this experiment, the communication bottleneck
    may not be as severe as the KVC bottleneck. Our experiment on more servers later
    shows that when more servers are used, the communication bottleneck is more severe
    than the KVC bottleneck, and then the benefit of ZDC is higher in the big model.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ZDC 在 ShareGPT 上的收益比“ The Pile” 数据集低 19-31%。这是因为“ The Pile” 有较长的提示，对 KVC 和通信带宽造成了更严重的负担，而
    ShareGPT 中的请求不太可能使用 SP。我们注意到，当模型规模达到 175B 时，ZDC 的收益平均下降了 17-25%。这是因为模型运行在多个 GPU
    上，减轻了对 KVC 的负担。由于我们在这个实验中使用了两台服务器，通信瓶颈可能不像 KVC 瓶颈那么严重。我们后续在更多服务器上的实验表明，当使用更多服务器时，通信瓶颈比
    KVC 瓶颈更严重，因此大模型中 ZDC 的收益更高。
- en: We observe that the request rate that ZDC can sustain while maintaining similar
    latency is only 2-4% lower than Oracle. The results indicate that the regression
    model can provide high accuracy in determining the compression ratios.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，ZDC 在维持类似延迟的同时能够承受的请求率比 Oracle 仅低 2-4%。结果表明，回归模型可以在确定压缩比方面提供高准确度。
- en: In addition, when ZDC complements H2O and GEAR, it can improve their throughput
    by 37-46% and improve their latency by 41-55%, indicating its complementary nature.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当 ZDC 补充 H2O 和 GEAR 时，它可以将它们的吞吐量提高 37-46%，并将延迟提高 41-55%，这表明它的互补性。
- en: 6.5 Ablation Testing
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 脱落测试
- en: We also tested the variants of ZDC as follows to evaluate the effectiveness
    of each individual method.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还测试了 ZDC 的各种变体，以评估每种方法的有效性。
- en: $\bullet$
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: ZDC/ZO is ZDC without zero overhead (/ZO). It performs compression and decompression
    online.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ZDC/ZO 是没有零开销的 ZDC（/ZO）。它在在线执行压缩和解压缩。
- en: $\bullet$
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: ZDC/OC is ZDC without Offline rotation matrix Computation (/OC). It uses SVD
    to compute the $R$s online.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ZDC/OC 是没有离线旋转矩阵计算的 ZDC（/OC）。它使用 SVD 在线计算 $R$s。
- en: $\bullet$
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: ZDC/DT is ZDC without using Different compression ratios on the important and
    unimportant Tokens (/DT). We set the same drop ratio for all tokens, which is
    0.35 for $Q_{d}=0.1$.
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ZDC/DT 是不使用重要和不重要 Tokens 上不同压缩比的 ZDC（/DT）。我们为所有 tokens 设置了相同的丢弃比例，即 $Q_{d}=0.1$
    时为 0.35。
- en: $\bullet$
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: ZDC/DL is ZDC without using different ratios of important tokens on Different
    Layers (/DL). We set the same ratio $g=0.5$.
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ZDC/DL 是没有在不同层中使用重要token不同比率的ZDC (/DL)。我们设置相同的比率 $g=0.5$。
- en: $\bullet$
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: ZDC/LT is ZDC without the Lightweight Token determination (/LT). We compute
    the sum of attention scores for every layer online as in [[28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30)].
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ZDC/LT 是没有轻量级token判定的ZDC (/LT)。我们在线计算每一层的注意力得分，参见[[28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30)]。
- en: '![Refer to caption](img/a73ee2bafeb8c3f83a12b1bb5885067d.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a73ee2bafeb8c3f83a12b1bb5885067d.png)'
- en: (a) The Pile.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: (a) The Pile。
- en: '![Refer to caption](img/129a2d798315e6b98b6778538cdc2b06.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/129a2d798315e6b98b6778538cdc2b06.png)'
- en: (b) ShareGPT.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: (b) ShareGPT。
- en: 'Figure 22: Average JCT of requests for individual methods.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：各个方法的请求平均JCT。
- en: '![Refer to caption](img/adb061a853e82286cf1357a5592ab419.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/adb061a853e82286cf1357a5592ab419.png)'
- en: (a) The Pile.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: (a) The Pile。
- en: '![Refer to caption](img/8e1c0fb7e30e9f2b3abc79ae7728fb3b.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8e1c0fb7e30e9f2b3abc79ae7728fb3b.png)'
- en: (b) ShareGPT.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: (b) ShareGPT。
- en: 'Figure 23: Avg perplexity of requests for individual methods.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图23：各个方法的请求平均困惑度。
- en: Fig. [22](#S6.F22 "Figure 22 ‣ 6.5 Ablation Testing ‣ 6 Performance Evaluation
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference") shows the average JCT over requests for those individual methods.
    ZDC/OC, ZDC/DL, ZDC/DT, ZDC/ZO, and ZDC/LT have 85-117%, 28-47%, 22-37%, 18-29%,
    and 15-19% higher average JCT than ZDC. The result means that the offline rotation
    matrix computation contributes the most in reducing JCT; the method to set different
    ratios of important tokens in different layers contributes the second, followed
    by the method that sets different compression ratios for important and unimportant
    tokens. The SVD-based zero-delay compression is the next, and the lightweight
    token determination contributes the least. The results indicate that each method
    is effective in reducing JCT.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图[22](#S6.F22 "Figure 22 ‣ 6.5 Ablation Testing ‣ 6 Performance Evaluation ‣
    Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference")展示了这些单独方法的请求平均JCT。ZDC/OC、ZDC/DL、ZDC/DT、ZDC/ZO和ZDC/LT的平均JCT分别比ZDC高出85-117%、28-47%、22-37%、18-29%和15-19%。结果表明，离线旋转矩阵计算在减少JCT方面贡献最大；其次是设置不同层中重要token比率的方法，其次是对重要token和不重要token设置不同压缩比率的方法。基于SVD的零延迟压缩排在其后，而轻量级token判定贡献最小。这些结果表明，每种方法在减少JCT方面都是有效的。
- en: Fig. [23](#S6.F23 "Figure 23 ‣ 6.5 Ablation Testing ‣ 6 Performance Evaluation
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference") shows the average perplexity over requests for these methods when
    they have the same average JCT. ZDC/OC, ZDC/DL, ZDC/DT, ZDC/ZO, and ZDC/LT have
    49-69%, 10-22%, 10-18%, 9-15%, and 8-14% higher average perplexity than ZDC. ZDC
    has lower perplexity than others because other methods need to increase the compression
    ratios to have the same average JCT as ZDC, increasing the perplexity. The contribution
    level of each method is consistent with the previous ones.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图[23](#S6.F23 "Figure 23 ‣ 6.5 Ablation Testing ‣ 6 Performance Evaluation ‣
    Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference")展示了这些方法在具有相同平均JCT时的请求平均困惑度。ZDC/OC、ZDC/DL、ZDC/DT、ZDC/ZO和ZDC/LT的平均困惑度比ZDC高出49-69%、10-22%、10-18%、9-15%和8-14%。ZDC的困惑度低于其他方法，因为其他方法需要增加压缩比率以保持与ZDC相同的平均JCT，从而增加了困惑度。每种方法的贡献水平与之前的一致。
- en: 6.6 Sensitivity Testing
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 敏感性测试
- en: To understand how different $g$, while ShareGPT has 49-56% JCT increase. This
    is because the former has much longer sequence lengths and poses a more severe
    burden on the KVC and communication bandwidth in SP.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 理解不同的 $g$，同时ShareGPT的JCT增加了49-56%。这是因为前者的序列长度要长得多，对KVC和SP中的通信带宽造成了更严重的负担。
- en: '![Refer to caption](img/78acf9889acb49a35e1bef23b4db0f14.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/78acf9889acb49a35e1bef23b4db0f14.png)'
- en: 'Figure 24: Different $g$.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图24：不同的 $g$。
- en: Fig. [25](#S6.F25 "Figure 25 ‣ 6.6 Sensitivity Testing ‣ 6 Performance Evaluation
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference") shows the average perplexity and average JCT with different $p$
    do not have a wide range between the highest value and the lowest value to allow
    a high dimension drop ratio, as shown in Fig. [13](#S4.F13 "Figure 13 ‣ 4.3 Eliminate
    Compression Matrix Computation Time and Suitability of SVD ‣ 4 Rationales of the
    Design of ZDC ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network
    Bottlenecks in LLM Inference").
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [25](#S6.F25 "Figure 25 ‣ 6.6 Sensitivity Testing ‣ 6 Performance Evaluation
    ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference") 显示了不同 $p$ 下的平均困惑度和平均 JCT 并没有很大的范围，允许较高的维度下降比率，如图 [13](#S4.F13
    "Figure 13 ‣ 4.3 Eliminate Compression Matrix Computation Time and Suitability
    of SVD ‣ 4 Rationales of the Design of ZDC ‣ Zero-Delay QKV Compression for Mitigating
    KV Cache and Network Bottlenecks in LLM Inference") 所示。
- en: '![Refer to caption](img/bc8e8e2a05f99bc2de4e47ea9253d05d.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bc8e8e2a05f99bc2de4e47ea9253d05d.png)'
- en: (a) Llama-2 13B.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Llama-2 13B。
- en: '![Refer to caption](img/0d6393e05d248dcbb71a415bd316dc2f.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0d6393e05d248dcbb71a415bd316dc2f.png)'
- en: (b) Llama-2 70B.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Llama-2 70B。
- en: 'Figure 25: Different $p$.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '图 25: 不同的 $p$。'
- en: '![Refer to caption](img/c2ce4a7e6767a423d4f0d850b1d01a3a.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c2ce4a7e6767a423d4f0d850b1d01a3a.png)'
- en: (a) Different data pruning ratios.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 不同的数据剪枝比率。
- en: '![Refer to caption](img/5de36abf0b9e66e32500d6b5644bcea1.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5de36abf0b9e66e32500d6b5644bcea1.png)'
- en: (b) Different K in k-means method.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: (b) k-means 方法中的不同 $k$。
- en: 'Figure 26: Different data pruning ratios and $k$ in K-means.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '图 26: k-means 中不同的数据剪枝比率和 $k$。'
- en: Fig. [26(a)](#S6.F26.sf1 "Figure 26(a) ‣ Figure 26 ‣ 6.6 Sensitivity Testing
    ‣ 6 Performance Evaluation ‣ Zero-Delay QKV Compression for Mitigating KV Cache
    and Network Bottlenecks in LLM Inference") shows the average perplexity over requests
    and the rotation matrix computing time with different data pruning ratios. We
    set $p=0.4$. We used "The Pile" to compute rotation matrices because ShareGPT
    does not have topic categories on which we can prune data in each topic. As the
    pruning ratio increases from 0.1 to 0.8, the average perplexities on 13B increase
    from 16.7 to 25.6 and that of 70B increases from 14.12 to 29.04 on “The Pile”;
    they increase from 21.2 to 34.21 and from 18.96 to 46.37 on ShareGPT. The perplexity
    increases little until the pruning ratio exceeds 0.6 for 13B and 70B on both datasets.
    70B experiences more accuracy decrease because it has more layers and heads and
    is sensitive to the quality of rotation matrices. Further, ShareGPT experiences
    more accuracy decrease because the rotation matrices it used were computed offline
    based on ”The Pile”. The rotation matrix computing time for 13B decreases from
    82.2min to 7.8min, and that for 70B decreases from 238min to 27.6min when the
    pruning ratio increases from 0.1 to 0.8\. This is because a lower pruning ratio
    leaves more data to compute. In addition, since 70B has more matrices, its computing
    time is more affected.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [26(a)](#S6.F26.sf1 "Figure 26(a) ‣ Figure 26 ‣ 6.6 Sensitivity Testing ‣
    6 Performance Evaluation ‣ Zero-Delay QKV Compression for Mitigating KV Cache
    and Network Bottlenecks in LLM Inference") 显示了不同数据剪枝比率下的平均困惑度和旋转矩阵计算时间。我们设置了 $p=0.4$。我们使用了
    "The Pile" 来计算旋转矩阵，因为 ShareGPT 没有可以在每个主题中剪枝数据的主题类别。随着剪枝比率从 0.1 增加到 0.8，13B 的平均困惑度从
    16.7 增加到 25.6，70B 的困惑度从 14.12 增加到 29.04；在 ShareGPT 上，13B 的困惑度从 21.2 增加到 34.21，70B
    的困惑度从 18.96 增加到 46.37。当剪枝比率超过 0.6 时，13B 和 70B 在两个数据集上的困惑度增加较少。由于 70B 具有更多的层和头部，对旋转矩阵的质量更敏感，因此其准确度下降更大。此外，ShareGPT
    的准确度下降更多，因为其使用的旋转矩阵是基于 "The Pile" 离线计算的。当剪枝比率从 0.1 增加到 0.8 时，13B 的旋转矩阵计算时间从 82.2
    分钟减少到 7.8 分钟，而 70B 的计算时间从 238 分钟减少到 27.6 分钟。这是因为较低的剪枝比率留下了更多的数据进行计算。此外，由于 70B
    具有更多的矩阵，其计算时间受到更大影响。
- en: Fig. [26(b)](#S6.F26.sf2 "Figure 26(b) ‣ Figure 26 ‣ 6.6 Sensitivity Testing
    ‣ 6 Performance Evaluation ‣ Zero-Delay QKV Compression for Mitigating KV Cache
    and Network Bottlenecks in LLM Inference") shows the average perplexity over requests
    and the computing time for finding all rotation matrices with different $k$ increase
    won’t affect the accuracy.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [26(b)](#S6.F26.sf2 "Figure 26(b) ‣ Figure 26 ‣ 6.6 Sensitivity Testing ‣
    6 Performance Evaluation ‣ Zero-Delay QKV Compression for Mitigating KV Cache
    and Network Bottlenecks in LLM Inference") 显示了在不同 $k$ 增加时，请求的平均困惑度和找到所有旋转矩阵的计算时间，发现这不会影响准确性。
- en: 6.7 Scalability Testing
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7 扩展性测试
- en: We built 2, 4, 6, and 8 servers, each with 8, 4, 2, and 2 GPUs, respectively,
    to test the scalability when the number of servers increases. We tested Llama-2
    70B and 175B with “The Pile”. Fig. [27](#S6.F27 "Figure 27 ‣ 6.7 Scalability Testing
    ‣ 6 Performance Evaluation ‣ Zero-Delay QKV Compression for Mitigating KV Cache
    and Network Bottlenecks in LLM Inference") shows the throughput (req/s) of the
    three methods with different numbers of servers for the two models. For 70B, when
    the number of servers increases from 2 to 8, the throughput of ZDC decreases by
    13%, 34%, and 43%; H2O decreases by 43%, 70%, and 84%; GEAR decreases by 41%,
    78%, and 87%. For 175B, ZDC decreases by 16%, 39, and 47%; H2O decreases by 61%,
    83%, and 95%; GEAR decreases by 64%, 89%, and 96%. ZDC’s performance decrease
    is 48% and 51% lower than H2O and GEAR when the number of servers is 8 for 70B.
    For 175B, ZDC has 77% and 86% lower performance decrease than H2O and GEAR. It
    is because ZDC reduces the communication overhead among servers, especially when
    the model is large. Therefore, ZDC is more scalable than the comparison methods.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了 2、4、6 和 8 台服务器，每台服务器分别配备 8、4、2 和 2 个 GPU，以测试在增加服务器数量时的可扩展性。我们用“The Pile”测试了
    Llama-2 70B 和 175B。图 [27](#S6.F27 "Figure 27 ‣ 6.7 Scalability Testing ‣ 6 Performance
    Evaluation ‣ Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks
    in LLM Inference") 显示了这三种方法在不同数量的服务器下对两个模型的吞吐量 (req/s)。对于 70B，当服务器数量从 2 增加到 8
    时，ZDC 的吞吐量减少了 13%、34% 和 43%；H2O 减少了 43%、70% 和 84%；GEAR 减少了 41%、78% 和 87%。对于 175B，ZDC
    减少了 16%、39% 和 47%；H2O 减少了 61%、83% 和 95%；GEAR 减少了 64%、89% 和 96%。当服务器数量为 8 时，70B
    的 ZDC 性能下降比 H2O 和 GEAR 分别低 48% 和 51%。对于 175B，ZDC 的性能下降比 H2O 和 GEAR 分别低 77% 和 86%。这是因为
    ZDC 减少了服务器之间的通信开销，尤其是当模型很大时。因此，ZDC 比比较方法具有更好的可扩展性。
- en: '![Refer to caption](img/8d01c4245b3b5d6932d35a93a4346d53.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8d01c4245b3b5d6932d35a93a4346d53.png)'
- en: (a) Llama-2 70B.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Llama-2 70B。
- en: '![Refer to caption](img/97c4133d99a9edeb7a8fa8636f476f68.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/97c4133d99a9edeb7a8fa8636f476f68.png)'
- en: (b) Llama-2 175B.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Llama-2 175B。
- en: 'Figure 27: Throughput for two models.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '图 27: 两个模型的吞吐量。'
- en: 6.8 Comparison with vLLM and FastGen
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8 与 vLLM 和 FastGen 的比较
- en: We also built ZDC on vLLM (ZDC-vLLM) and compared ZDC and ZDC-vLLM with vLLM
    and FastGen. Fig. [28](#S6.F28 "Figure 28 ‣ 6.8 Comparison with vLLM and FastGen
    ‣ 6 Performance Evaluation ‣ Zero-Delay QKV Compression for Mitigating KV Cache
    and Network Bottlenecks in LLM Inference") shows the average JCT over requests
    of them for all models. ZDC and ZDC-vLLM use the same parameter setting as before
    to satisfy $Q_{d}=0.1$. For “The Pile”, ZDC outperforms FastGen by 84-89%, and
    ZDC-vLLM outperforms vLLM by 85-91%. It is because “The Pile” has many long sequences,
    and vLLM and FastGen conduct prompt processing in multiple iterations without
    SP. For ShareGPT, ZDC outperforms FastGen by 47-57%, and ZDC-vLLM outperforms
    vLLM by 45-60%. The benefit decreases compared to “The Pile” because ShareGPT
    has short sequences. Therefore, ZDC is applicable to different LLM systems to
    enhance their performance.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在 vLLM 上构建了 ZDC（ZDC-vLLM），并将 ZDC 和 ZDC-vLLM 与 vLLM 和 FastGen 进行了比较。图 [28](#S6.F28
    "Figure 28 ‣ 6.8 Comparison with vLLM and FastGen ‣ 6 Performance Evaluation ‣
    Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in
    LLM Inference") 显示了所有模型的平均 JCT。ZDC 和 ZDC-vLLM 使用与之前相同的参数设置，以满足$Q_{d}=0.1$。对于“The
    Pile”，ZDC 比 FastGen 高出 84-89%，而 ZDC-vLLM 比 vLLM 高出 85-91%。这是因为“The Pile”包含许多长序列，而
    vLLM 和 FastGen 在没有 SP 的情况下进行多次迭代的提示处理。对于 ShareGPT，ZDC 比 FastGen 高出 47-57%，而 ZDC-vLLM
    比 vLLM 高出 45-60%。由于 ShareGPT 有较短的序列，因此相比“The Pile”，收益有所下降。因此，ZDC 可应用于不同的 LLM 系统以提升其性能。
- en: '![Refer to caption](img/125839e73a231cc2332568cc1668647c.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/125839e73a231cc2332568cc1668647c.png)'
- en: (a) The Pile.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: (a) The Pile。
- en: '![Refer to caption](img/16a8a6cd22c586e6d038f86d382e9c98.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/16a8a6cd22c586e6d038f86d382e9c98.png)'
- en: (b) ShareGPT.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: (b) ShareGPT。
- en: 'Figure 28: Average JCT.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '图 28: 平均 JCT。'
- en: 6.9 Attention and Communication Improvement
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9 注意力和通信改进
- en: Fig. [29](#S6.F29 "Figure 29 ‣ 6.9 Attention and Communication Improvement ‣
    6 Performance Evaluation ‣ Zero-Delay QKV Compression for Mitigating KV Cache
    and Network Bottlenecks in LLM Inference") shows the average time ratio of attention
    computation and communication over JCT for Llama-2 using “The Pile” with different
    $p$ increases from 0.3 to 0.7, it can save attention time from 21-28% to 62-64%
    and communication time from 28% to 72%.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [29](#S6.F29 "Figure 29 ‣ 6.9 Attention and Communication Improvement ‣ 6
    Performance Evaluation ‣ Zero-Delay QKV Compression for Mitigating KV Cache and
    Network Bottlenecks in LLM Inference") 显示了 Llama-2 使用“The Pile”时，注意力计算和通信的平均时间比随着$p$从0.3增加到0.7的变化情况，它可以将注意力时间从21-28%节省到62-64%，通信时间从28%节省到72%。
- en: '![Refer to caption](img/a922f79fbde9e8dc91107a03b47715db.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a922f79fbde9e8dc91107a03b47715db.png)'
- en: (a) Attention.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 注意力。
- en: '![Refer to caption](img/5d48acdb098946bc9d05374adbd70844.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5d48acdb098946bc9d05374adbd70844.png)'
- en: (b) Communication.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 通信。
- en: 'Figure 29: Average time ratio over JCT.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图29：JCT上的平均时间比例。
- en: 7 Limitations and Future Work
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制与未来工作
- en: Compression ratio and important-token determination. In this study, we utilize
    a regression model to determine these settings. Our experiments reveal that factors
    such as model size and data characteristics influence these determinations, and
    there remains a gap compared to the Oracle solution. In future work, we aim to
    investigate the feasibility of employing more accurate approaches, such as reinforcement
    learning-based methods, for these determinations.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩比和重要令牌确定。在本研究中，我们利用回归模型来确定这些设置。实验结果表明，模型大小和数据特征等因素会影响这些确定，并且与Oracle解决方案相比仍有差距。未来工作中，我们旨在调查使用更准确的方法，如基于强化学习的方法，来进行这些确定的可行性。
- en: Communication among more servers. In our study, we only tested 8 servers maximally,
    which proved adequate for managing the workload. However, in practical scenarios,
    prompts with significantly longer lengths may necessitate additional servers,
    resulting in increased communication overhead. We will assess the efficacy of
    ZDC in mitigating communication overhead for such extended prompts and heavier
    workloads and further refine ZDC to accommodate this scenario.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器间的通信。在我们的研究中，我们最多测试了8台服务器，这证明足以管理工作负载。然而，在实际场景中，长度显著更长的提示可能需要额外的服务器，从而增加通信开销。我们将评估ZDC在减轻这种扩展提示和更重工作负载的通信开销方面的有效性，并进一步优化ZDC以适应这种情况。
- en: Job scheduling among servers. In our study, we distributed requests to servers
    in a round-robin manner. In future work, we aim to investigate optimal job scheduling
    strategies among different servers to maximize overall system performance.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器间的作业调度。在我们的研究中，我们以轮询方式分配请求到服务器。未来工作中，我们将探讨不同服务器之间的最佳作业调度策略，以最大化整体系统性能。
- en: 8 Related Work
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 相关工作
- en: 'KVC compression. KVC compression methods fall into two categories: KVC eviction [[28](#bib.bib28),
    [30](#bib.bib30), [29](#bib.bib29)] and KVC quantization [[25](#bib.bib25), [26](#bib.bib26),
    [27](#bib.bib27)]. H2O [[28](#bib.bib28)], Scissorhands [[29](#bib.bib29)], and
    adaptive KV eviction [[30](#bib.bib30)] identify important tokens based on attention
    scores, retaining those with high scores in KVC. Adaptive KV eviction additionally
    considers correlations between nearby tokens, preserving special tokens and punctuation.
    GEAR [[25](#bib.bib25)], Kivi [[26](#bib.bib26)], and CacheGen [[27](#bib.bib27)]
    quantize KV data and adapt the quantization level to meet different compression
    degrees. However, these methods incur extra compression and decompression overhead,
    leading to a significant increase in JCT.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: KVC压缩。KVC压缩方法分为两类：KVC驱逐[[28](#bib.bib28), [30](#bib.bib30), [29](#bib.bib29)]和KVC量化[[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)]。H2O[[28](#bib.bib28)]、Scissorhands[[29](#bib.bib29)]和自适应KV驱逐[[30](#bib.bib30)]根据注意力分数识别重要令牌，将高分的令牌保留在KVC中。自适应KV驱逐还考虑附近令牌之间的关联，保留特殊令牌和标点符号。GEAR[[25](#bib.bib25)]、Kivi[[26](#bib.bib26)]和CacheGen[[27](#bib.bib27)]对KV数据进行量化，并调整量化水平以满足不同的压缩程度。然而，这些方法会带来额外的压缩和解压缩开销，导致JCT显著增加。
- en: Model compression. Compression finds wide application in model training and
    inference tasks. Zero++[[50](#bib.bib50)] and THC[[41](#bib.bib41)] compress model
    gradients with reduced overhead for training tasks using quantization. Various
    quantization approaches [[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20),
    [24](#bib.bib24)] have been proposed to compress model parameters post-training,
    aiming to shrink model size. Additionally, other methods [[21](#bib.bib21), [22](#bib.bib22),
    [23](#bib.bib23)] quantize model activations alongside parameters for inference.
    PIT [[51](#bib.bib51)] leverages Permutation Invariant Transformation to transform
    sparsely located micro-tiles into GPU-efficient dense tiles without altering computation
    results to enhance computation efficiency.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩。压缩在模型训练和推理任务中得到广泛应用。Zero++[[50](#bib.bib50)]和THC[[41](#bib.bib41)]通过量化技术减少训练任务的开销来压缩模型梯度。各种量化方法[[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [24](#bib.bib24)]已被提出，用于训练后压缩模型参数，旨在缩小模型大小。此外，还有其他方法[[21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23)]量化模型激活和参数以进行推理。PIT [[51](#bib.bib51)]利用置换不变变换将稀疏位置的微块转换为GPU高效的密集块，且不改变计算结果，从而提高计算效率。
- en: Sequence parallelism and job scheduling. To handle long sequences in training,
    SP [[36](#bib.bib36), [52](#bib.bib52), [39](#bib.bib39)] was proposed. Other
    recent studies [[53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56),
    [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66)]
    proposed transformer variants to handle long sequences in training. However, previous
    methods do not focus on inference. To handle long prompts in inference, some methods
    (e.g., FastGen [[37](#bib.bib37)] and SARATHI [[67](#bib.bib67)]) were proposed
    to chunk a long prompt and batch the chunks with token generation tasks.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 序列并行性和作业调度。为了处理训练中的长序列，提出了 SP [[36](#bib.bib36), [52](#bib.bib52), [39](#bib.bib39)]。其他近期研究
    [[53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57),
    [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62),
    [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66)] 提出了变换器变体来处理训练中的长序列。然而，以前的方法并未关注推理。为了处理推理中的长提示，一些方法（例如，FastGen
    [[37](#bib.bib37)] 和 SARATHI [[67](#bib.bib67)]) 被提出以将长提示分块，并将块与令牌生成任务批处理。
- en: 9 Conclusion
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论
- en: In this paper, we introduce the ZDC compression system, the first work that
    achieves zero delay in response time for LLM inference while simultaneously reducing
    data volume in both KV cache, computation time, and network communication within
    the SP framework for long prompts. Drawing insights from trace-based experimental
    measurements, ZDC incorporates SVD-based zero-delay compression, adaptive hybrid
    compression ratio determination, and a communication-efficient sequence parallelism
    framework. Our trace-based real experiments show that ZDC outperforms previous
    methods by reducing up to 80% JCT and 35% perplexity. Moreover, ZDC achieves optimality
    within 4% of the Oracle, which has full knowledge of the optimal parameter settings.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了 ZDC 压缩系统，这是首个在 LLM 推理中实现零延迟响应时间的工作，同时在 SP 框架内减少 KV 缓存、计算时间和网络通信的数据量。根据基于跟踪的实验测量，ZDC
    包括基于 SVD 的零延迟压缩、自适应混合压缩比确定以及高效通信的序列并行框架。我们的基于跟踪的实际实验表明，ZDC 通过减少最多 80% 的 JCT 和
    35% 的困惑度，超越了以前的方法。此外，ZDC 在最优性上达到了与 Oracle 相差 4% 以内的水平，Oracle 具有关于最优参数设置的全部知识。
- en: References
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    Improving language understanding by generative pre-training. 2018.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever 等。通过生成预训练提高语言理解。2018年。'
- en: '[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:
    pre-training of deep bidirectional transformers for language understanding. CoRR,
    abs/1810.04805, 2018.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova。BERT: 用于语言理解的深度双向变换器的预训练。CoRR,
    abs/1810.04805, 2018年。'
- en: '[3] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
    et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9,
    2019.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever
    等。语言模型是无监督的多任务学习者。OpenAI 博客, 1(8):9, 2019年。'
- en: '[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
    Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
    Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
    models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan,
    and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,
    pages 1877–1901\. Curran Associates, Inc., 2020.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
    Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
    Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 语言模型是少量样本学习者。在
    H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, 和 H. Lin 编辑的《神经信息处理系统进展》第33卷，第1877–1901页。Curran
    Associates, Inc., 2020。'
- en: '[5] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli,
    Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models
    can teach themselves to use tools, 2023.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli,
    Luke Zettlemoyer, Nicola Cancedda, 和 Thomas Scialom。Toolformer: 语言模型可以自我学习使用工具，2023年。'
- en: '[6] Meta llama-2 models. [https://huggingface.co/models?sort=trending&search=meta+Llama-2](https://huggingface.co/models?sort=trending&search=meta+Llama-2),
    2024.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Meta llama-2 模型。 [https://huggingface.co/models?sort=trending&search=meta+Llama-2](https://huggingface.co/models?sort=trending&search=meta+Llama-2)，2024
    年。'
- en: '[7] Textsynth: Text completion. [https://textsynth.com/completion.html](https://textsynth.com/completion.html).'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Textsynth：文本补全。 [https://textsynth.com/completion.html](https://textsynth.com/completion.html)。'
- en: '[8] Openai: ChatGPT. [https://chat.openai.com/](https://chat.openai.com/),
    [Accessed in Aug. 2023].'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Openai：ChatGPT。 [https://chat.openai.com/](https://chat.openai.com/)，[访问时间：2023
    年 8 月]。'
- en: '[9] GitHub Copilot. [https://github.com/features/copilot/](https://github.com/features/copilot/),
    2024.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] GitHub Copilot。 [https://github.com/features/copilot/](https://github.com/features/copilot/)，2024
    年。'
- en: '[10] Speaking your language: The transformer in machine translation. [https://blog.huawei.com/2022/02/01/speaking-your-language-transformer-machine-translation/](https://blog.huawei.com/2022/02/01/speaking-your-language-transformer-machine-translation/).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 说你自己的语言：变换器在机器翻译中的应用。 [https://blog.huawei.com/2022/02/01/speaking-your-language-transformer-machine-translation/](https://blog.huawei.com/2022/02/01/speaking-your-language-transformer-machine-translation/)。'
- en: '[11] Elozino Egonmwan and Yllias Chali. Transformer-based model for single
    documents neural summarization. In Proceedings of the 3rd Workshop on Neural Generation
    and Translation, pages 70–79, Hong Kong, November 2019\. Association for Computational
    Linguistics.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Elozino Egonmwan 和 Yllias Chali. 基于变换器的单文档神经摘要模型。在第三届神经生成与翻译研讨会论文集，第 70–79
    页，香港，2019 年 11 月。计算语言学协会。'
- en: '[12] Zi Gong, Cuiyun Gao, Yasheng Wang, Wenchao Gu, Yun Peng, and Zenglin Xu.
    Source code summarization with structural relative position guided transformer.
    In 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering
    (SANER), pages 13–24, 2022.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Zi Gong, Cuiyun Gao, Yasheng Wang, Wenchao Gu, Yun Peng 和 Zenglin Xu.
    基于结构相对位置引导的变换器源代码总结。在 2022 IEEE 国际软件分析、演化和重工程会议（SANER），第 13–24 页，2022 年。'
- en: '[13] Haopeng Zhang, Xiao Liu, and Jiawei Zhang. HEGEL: Hypergraph transformer
    for long document summarization, 2022.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Haopeng Zhang, Xiao Liu 和 Jiawei Zhang. HEGEL：用于长文档总结的超图变换器，2022 年。'
- en: '[14] Ashutosh Adhikari, Achyudh Ram, Raphael Tang, and Jimmy Lin. Docbert:
    BERT for document classification. CoRR, abs/1904.08398, 2019.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Ashutosh Adhikari, Achyudh Ram, Raphael Tang 和 Jimmy Lin. Docbert：用于文档分类的
    BERT。CoRR, abs/1904.08398, 2019 年。'
- en: '[15] Xiang Dai, Ilias Chalkidis, Sune Darkner, and Desmond Elliott. Revisiting
    transformer-based models for long document classification, 2022.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Xiang Dai, Ilias Chalkidis, Sune Darkner 和 Desmond Elliott. 重新审视基于变换器的长文档分类模型，2022
    年。'
- en: '[16] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
    and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30\.
    Curran Associates, Inc., 2017.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Ł ukasz Kaiser 和 Illia Polosukhin. 注意力机制是你所需的一切。在 I. Guyon, U.
    Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan 和 R. Garnett 编辑的《神经信息处理系统进展》，第
    30 卷。Curran Associates, Inc., 2017 年。'
- en: '[17] Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Scaling transformer
    to 1m tokens and beyond with rmt. ArXiv, abs/2304.11062, 2023.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Aydar Bulatov, Yuri Kuratov 和 Mikhail S. Burtsev. 使用 rmt 扩展变换器到 1m 令牌及以上。ArXiv,
    abs/2304.11062, 2023 年。'
- en: '[18] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh. Gptq：用于生成预训练变换器的准确后训练量化。arXiv
    预印本 arXiv:2210.17323, 2022 年。'
- en: '[19] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,
    K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems,
    volume 35, pages 27168–27183\. Curran Associates, Inc., 2022.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li 和 Yuxiong He. Zeroquant：针对大规模变换器的高效且经济的后训练量化。在 S. Koyejo, S. Mohamed, A. Agarwal,
    D. Belgrave, K. Cho 和 A. Oh 编辑的《神经信息处理系统进展》，第 35 卷，第 27168–27183 页。Curran Associates,
    Inc., 2022 年。'
- en: '[20] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit
    inference scaling laws. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara
    Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th
    International Conference on Machine Learning, volume 202 of Proceedings of Machine
    Learning Research, pages 7750–7774\. PMLR, 23–29 Jul 2023.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Tim Dettmers 和 Luke Zettlemoyer。4 位精度的理由：k 位推理缩放规律。在 Andreas Krause, Emma
    Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato 和 Jonathan Scarlett
    编辑的《第 40 届国际机器学习会议论文集》，第 202 卷，机器学习研究论文集，页面 7750–7774。PMLR，2023 年 7 月 23–29 日。'
- en: '[21] Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He. A comprehensive
    study on post-training quantization for large language models. arXiv preprint
    arXiv:2303.08302, 2023.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn 和 Yuxiong He。大型语言模型后训练量化的全面研究。arXiv
    预印本 arXiv:2303.08302，2023 年。'
- en: '[22] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3.int8():
    8-bit matrix multiplication for transformers at scale. In S. Koyejo, S. Mohamed,
    A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information
    Processing Systems, volume 35, pages 30318–30332\. Curran Associates, Inc., 2022.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Tim Dettmers, Mike Lewis, Younes Belkada 和 Luke Zettlemoyer。Gpt3.int8()：大规模变换器的
    8 位矩阵乘法。在 S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho 和 A. Oh 编辑的《神经信息处理系统进展》第
    35 卷，页面 30318–30332。Curran Associates, Inc., 2022 年。'
- en: '[23] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In International Conference on Machine Learning, pages 38087–38099\.
    PMLR, 2023.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth 和 Song
    Han。Smoothquant：针对大型语言模型的精确高效的后训练量化。在国际机器学习会议上，页面 38087–38099。PMLR，2023 年。'
- en: '[24] Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok
    Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. LUT-GEMM: Quantized
    matrix multiplication based on LUTs for efficient inference in large-scale generative
    language models, 2024.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok
    Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee 和 Dongsoo Lee。LUT-GEMM：基于 LUT
    的量化矩阵乘法，用于大规模生成语言模型的高效推理，2024 年。'
- en: '[25] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar
    Krishna, and Tuo Zhao. GEAR: An efficient KV cache compression recipe for near-lossless
    generative inference of LLM. arXiv preprint arXiv:2403.05527, 2024.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar
    Krishna 和 Tuo Zhao。GEAR：一种高效的 KV 缓存压缩方案，用于近乎无损的 LLM 生成推理。arXiv 预印本 arXiv:2403.05527，2024
    年。'
- en: '[26] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir
    Braverman, Beidi Chen, and Xia Hu. KIVI : Plug-and-play 2bit kv cache quantization
    with streaming asymmetric quantization. arXiv preprint arXiv:2402.02750, 2023.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir
    Braverman, Beidi Chen 和 Xia Hu。KIVI：即插即用的 2bit kv 缓存量化与流式不对称量化。arXiv 预印本 arXiv:2402.02750，2023
    年。'
- en: '[27] Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng
    Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael Maire,
    Henry Hoffmann, Ari Holtzman, and Junchen Jiang. Cachegen: Kv cache compression
    and streaming for fast large language model serving. In Proceedings of the ACM
    SIGCOMM 2024 Conference, ACM SIGCOMM ’24, page 38–56, New York, NY, USA, 2024\.
    Association for Computing Machinery.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng
    Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael Maire,
    Henry Hoffmann, Ari Holtzman 和 Junchen Jiang。Cachegen：KV 缓存压缩与流式处理，以快速服务大型语言模型。在
    ACM SIGCOMM 2024 会议论文集中，ACM SIGCOMM ’24，第 38–56 页，美国纽约，2024 年。计算机协会。'
- en: '[28] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
    Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang "Atlas"
    Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference
    of large language models. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt,
    and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36,
    pages 34661–34710\. Curran Associates, Inc., 2023.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
    Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang “Atlas”
    Wang 和 Beidi Chen。H2o：高效生成推理的大型语言模型的重击者预言器。在 A. Oh, T. Neumann, A. Globerson,
    K. Saenko, M. Hardt 和 S. Levine 编辑的《神经信息处理系统进展》第 36 卷，页面 34661–34710。Curran Associates,
    Inc., 2023 年。'
- en: '[29] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo
    Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting
    the persistence of importance hypothesis for llm kv cache compression at test
    time. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,
    editors, Advances in Neural Information Processing Systems, volume 36, pages 52342–52364\.
    Curran Associates, Inc., 2023.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo
    Xu, Anastasios Kyrillidis, 和 Anshumali Shrivastava。Scissorhands: 利用重要性假设的持久性进行
    llm kv 缓存压缩。发表于 A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, 和 S. Levine
    编辑的《神经信息处理系统进展》，第36卷，第52342–52364页。Curran Associates, Inc., 2023年。'
- en: '[30] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng
    Gao. Model tells you what to discard: Adaptive kv cache compression for llms.
    arXiv preprint arXiv:2310.01801, 2023.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, 和 Jianfeng
    Gao。模型告诉你要丢弃什么：适应性 kv 缓存压缩用于 llms。arXiv 预印本 arXiv:2310.01801，2023年。'
- en: '[31] ZeroC code. [https://anonymous.4open.science/r/ZeroC](https://anonymous.4open.science/r/ZeroC),
    2024.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] ZeroC 代码。 [https://anonymous.4open.science/r/ZeroC](https://anonymous.4open.science/r/ZeroC)，2024年。'
- en: '[32] Singular value decomposition solver from scipy. [https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html),
    2024.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 来自 scipy 的奇异值分解求解器。 [https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html)，2024年。'
- en: '[33] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
    Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and
    Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling.
    arXiv preprint arXiv:2101.00027, 2020.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
    Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, 和 Connor
    Leahy。The Pile: 一个用于语言建模的800GB多样文本数据集。arXiv 预印本 arXiv:2101.00027，2020年。'
- en: '[34] ShareGPT team. [https://sharegpt.com/](https://sharegpt.com/), 2024.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] ShareGPT 团队。 [https://sharegpt.com/](https://sharegpt.com/)，2024年。'
- en: '[35] Amazon EC2 P4 instances. [https://aws.amazon.com/ec2/instance-types/p4/](https://aws.amazon.com/ec2/instance-types/p4/).'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Amazon EC2 P4 实例。 [https://aws.amazon.com/ec2/instance-types/p4/](https://aws.amazon.com/ec2/instance-types/p4/)。'
- en: '[36] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
    Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer,
    Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. Efficient large-scale language
    model training on gpu clusters using megatron-lm. In Proceedings of the International
    Conference for High Performance Computing, Networking, Storage and Analysis, SC
    ’21, New York, NY, USA, 2021\. Association for Computing Machinery.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
    Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer,
    Bryan Catanzaro, Amar Phanishayee, 和 Matei Zaharia。使用 megatron-lm 在 GPU 集群上高效训练大规模语言模型。发表于国际高性能计算、网络、存储与分析会议，SC
    ’21，美国纽约，2021年。计算机协会。'
- en: '[37] Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff
    Rasley, Samyam Rajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash Bakhtiari,
    Lev Kurilenko, and Yuxiong He. Deepspeed-fastgen: High-throughput text generation
    for llms via mii and deepspeed-inference. arXiv preprint arXiv:2401.08671, 2024.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff
    Rasley, Samyam Rajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash Bakhtiari,
    Lev Kurilenko, 和 Yuxiong He。Deepspeed-fastgen: 通过 mii 和 deepspeed-inference 实现
    llms 的高吞吐量文本生成。arXiv 预印本 arXiv:2401.08671，2024年。'
- en: '[38] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao
    Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for
    large language model serving with pagedattention. In Proceedings of the 29th Symposium
    on Operating Systems Principles, SOSP ’23, page 611–626, New York, NY, USA, 2023\.
    Association for Computing Machinery.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody
    Hao Yu, Joseph Gonzalez, Hao Zhang, 和 Ion Stoica。使用分页注意力进行大型语言模型服务的高效内存管理。发表于第29届操作系统原理研讨会，SOSP
    ’23，第611–626页，美国纽约，2023年。计算机协会。'
- en: '[39] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon
    Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations
    for enabling training of extreme long sequence transformer models, 2023.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen
    Leon Song, Samyam Rajbhandari, 和 Yuxiong He。Deepspeed ulysses: 使极长序列变换器模型训练成为可能的系统优化，2023年。'
- en: '[40] Facebook opt models. [https://huggingface.co/models?sort=trending&search=facebook+opt](https://huggingface.co/models?sort=trending&search=facebook+opt),
    2024.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Facebook OPT 模型. [https://huggingface.co/models?sort=trending&search=facebook+opt](https://huggingface.co/models?sort=trending&search=facebook+opt),
    2024.'
- en: '[41] Minghao Li, Ran Ben Basat, Shay Vargaftik, ChonLam Lao, Kevin Xu, Michael
    Mitzenmacher, and Minlan Yu. THC: Accelerating distributed deep learning using
    tensor homomorphic compression. In 21st USENIX Symposium on Networked Systems
    Design and Implementation (NSDI 24), pages 1191–1211, Santa Clara, CA, April 2024.
    USENIX Association.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] 李明浩, 兰·本·巴萨特, 沙伊·瓦尔加夫蒂克, 曹林·劳, 徐凯文, 迈克尔·米岑马赫, 和闵兰·余. THC: 使用张量同态压缩加速分布式深度学习.
    在第21届USENIX网络系统设计与实现研讨会（NSDI 24）上，第1191–1211页, 加州圣克拉拉, 2024年4月. USENIX协会.'
- en: '[42] Davis Blalock and John Guttag. Multiplying matrices without multiplying.
    In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International
    Conference on Machine Learning, volume 139 of Proceedings of Machine Learning
    Research, pages 992–1004\. PMLR, 18–24 Jul 2021.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] 戴维斯·布拉洛克 和 约翰·古塔格. 无需相乘的矩阵乘法. 在 Marina Meila 和 Tong Zhang 编辑的《第38届国际机器学习会议论文集》中，第139卷，《机器学习研究论文集》，第992–1004页\.
    PMLR, 2021年7月18–24日.'
- en: '[43] Yongkweon Jeon, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Jeongin Yun,
    and Dongsoo Lee. Biqgemm: matrix multiplication with lookup table for binary-coding-based
    quantized dnns. SC ’20\. IEEE Press, 2020.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] 田永权, 朴培松, 权世政, 金炳旭, 郑寅, 和李东秀. Biqgemm: 带有查找表的矩阵乘法，用于基于二进制编码的量化深度神经网络.
    SC ’20\. IEEE出版社, 2020年.'
- en: '[44] Alessandro Raganato and Jörg Tiedemann. An analysis of encoder representations
    in transformer-based machine translation. In Tal Linzen, Grzegorz Chrupała, and
    Afra Alishahi, editors, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing
    and Interpreting Neural Networks for NLP, pages 287–297, Brussels, Belgium, November
    2018\. Association for Computational Linguistics.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] 亚历山德罗·拉加纳托 和 约尔格·提德曼. 基于 Transformer 的机器翻译中编码器表示的分析. 在 Tal Linzen, Grzegorz
    Chrupała 和 Afra Alishahi 编辑的《2018年EMNLP研讨会BlackboxNLP: 分析和解释NLP的神经网络论文集》中，第287–297页,
    比利时布鲁塞尔, 2018年11月\. 计算语言学协会.'
- en: '[45] Angelos Katharopoulos and François Fleuret. Not all samples are created
    equal: Deep learning with importance sampling. In International conference on
    machine learning, pages 2525–2534\. PMLR, 2018.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] 安吉洛斯·卡萨罗普洛斯 和 弗朗索瓦·弗吕雷. 并非所有样本都是平等的: 使用重要性采样的深度学习. 在国际机器学习会议上，第2525–2534页\.
    PMLR, 2018年.'
- en: '[46] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph
    Hassoun, and Kurt Keutzer. Learned token pruning for transformers. In Proceedings
    of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD
    ’22, page 784–794, New York, NY, USA, 2022. Association for Computing Machinery.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] 金世勋, 沈盛, 大卫·索斯利, 阿米尔·戈拉米, 权宇石, 约瑟夫·哈松, 和库尔特·库特泽. 用于 Transformers 的学习型标记修剪.
    在第28届ACM SIGKDD知识发现与数据挖掘会议论文集（KDD ’22）上，第784–794页, 纽约, NY, USA, 2022年. 计算机协会.'
- en: '[47] Hugging Face: The AI community building the future. [https://huggingface.co/](https://huggingface.co/),
    2024.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Hugging Face: 构建未来的人工智能社区. [https://huggingface.co/](https://huggingface.co/),
    2024.'
- en: '[48] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared
    Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language
    models using model parallelism. CoRR, abs/1909.08053, 2019.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] 穆罕默德·肖耶比, 莫斯托法·帕特瓦里, 劳尔·普里, 帕特里克·勒格雷斯利, 贾瑞德·卡斯帕, 和布莱恩·卡坦扎罗. Megatron-lm:
    使用模型并行训练数十亿参数的语言模型. CoRR, abs/1909.08053, 2019.'
- en: '[49] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon
    Chun. Orca: A distributed serving system for Transformer-Based generative models.
    In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI
    22), pages 521–538, Carlsbad, CA, July 2022\. USENIX Association.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] 柳庆仁, 朱成政, 金健宇, 金素晶, 和全炳根. Orca: 用于基于 Transformer 的生成模型的分布式服务系统. 在第16届USENIX操作系统设计与实现研讨会（OSDI
    22）上，第521–538页，加州卡尔斯巴德, 2022年7月\. USENIX协会.'
- en: '[50] Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari,
    Olatunji Ruwase, Feng Yan, Lei Yang, and Yuxiong He. Zero++: Extremely efficient
    collective communication for giant model training. arXiv preprint arXiv:2306.10209,
    2023.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] 王冠华, 秦赫扬, 山姆·阿德·雅各布斯, 康纳·霍尔姆斯, 萨姆扬·拉吉班达里, 奥拉图恩吉·鲁瓦斯, 冯岩, 雷阳, 和余雄赫. Zero++:
    用于大型模型训练的极其高效的集体通信. arXiv 预印本 arXiv:2306.10209, 2023.'
- en: '[51] Ningxin Zheng, Huiqiang Jiang, Quanlu Zhang, Zhenhua Han, Lingxiao Ma,
    Yuqing Yang, Fan Yang, Chengruidong Zhang, Lili Qiu, Mao Yang, and Lidong Zhou.
    PIT: Optimization of dynamic sparse deep learning models via permutation invariant
    transformation. In Proceedings of the 29th Symposium on Operating Systems Principles,
    SOSP ’23, page 331–347, New York, NY, USA, 2023\. Association for Computing Machinery.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Ningxin Zheng、Huiqiang Jiang、Quanlu Zhang、Zhenhua Han、Lingxiao Ma、Yuqing
    Yang、Fan Yang、Chengruidong Zhang、Lili Qiu、Mao Yang 和 Lidong Zhou。PIT：通过置换不变变换优化动态稀疏深度学习模型。在第
    29 届操作系统原理研讨会论文集，SOSP ’23，页码 331–347，美国纽约，2023。计算机协会。'
- en: '[52] Shenggui Li, Fuzhao Xue, Yongbin Li, and Yang You. Sequence parallelism:
    Long sequence training from system perspective. CoRR, abs/2105.13120, 2021.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Shenggui Li、Fuzhao Xue、Yongbin Li 和 Yang You。序列并行性：从系统角度看长序列训练。CoRR，abs/2105.13120，2021。'
- en: '[53] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer:
    Self-attention with linear complexity. CoRR, abs/2006.04768, 2020.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Sinong Wang、Belinda Z. Li、Madian Khabsa、Han Fang 和 Hao Ma。Linformer：具有线性复杂度的自注意力。CoRR，abs/2006.04768，2020。'
- en: '[54] Genta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, and
    Pascale Fung. Lightweight and efficient end-to-end speech recognition using low-rank
    transformer. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP), pages 6144–6148, 2020.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Genta Indra Winata、Samuel Cahyawijaya、Zhaojiang Lin、Zihan Liu 和 Pascale
    Fung。使用低秩变压器的轻量级和高效的端到端语音识别。在 ICASSP 2020 - 2020 IEEE 国际声学、语音和信号处理会议（ICASSP）上，页码
    6144–6148，2020。'
- en: '[55] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.
    Transformers are rnns: Fast autoregressive transformers with linear attention.
    CoRR, abs/2006.16236, 2020.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Angelos Katharopoulos、Apoorv Vyas、Nikolaos Pappas 和 François Fleuret。变压器是
    RNN：具有线性注意力的快速自回归变压器。CoRR，abs/2006.16236，2020。'
- en: '[56] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
    Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz
    Kaiser, David Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention
    with performers. CoRR, abs/2009.14794, 2020.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Krzysztof Choromanski、Valerii Likhosherstov、David Dohan、Xingyou Song、Andreea
    Gane、Tamás Sarlós、Peter Hawkins、Jared Davis、Afroz Mohiuddin、Lukasz Kaiser、David
    Belanger、Lucy J. Colwell 和 Adrian Weller。重新思考带有执行者的注意力。CoRR，abs/2009.14794，2020。'
- en: '[57] Zhen Qin, XiaoDong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes,
    and Yiran Zhong. The devil in linear transformer, 2022.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Zhen Qin、XiaoDong Han、Weixuan Sun、Dongxu Li、Lingpeng Kong、Nick Barnes
    和 Yiran Zhong。线性变压器中的恶魔，2022。'
- en: '[58] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and
    Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant
    neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings
    of the 36th International Conference on Machine Learning, volume 97 of Proceedings
    of Machine Learning Research, pages 3744–3753\. PMLR, 09–15 Jun 2019.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Juho Lee、Yoonho Lee、Jungtaek Kim、Adam Kosiorek、Seungjin Choi 和 Yee Whye
    Teh。集合变压器：基于注意力的置换不变神经网络框架。在 Kamalika Chaudhuri 和 Ruslan Salakhutdinov 编辑的第 36
    届国际机器学习会议论文集中，卷 97，机器学习研究论文集，页码 3744–3753。PMLR，2019 年 6 月 9–15 日。'
- en: '[59] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman,
    and Joao Carreira. Perceiver: General perception with iterative attention. In
    Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference
    on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages
    4651–4664\. PMLR, 18–24 Jul 2021.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Andrew Jaegle、Felix Gimeno、Andy Brock、Oriol Vinyals、Andrew Zisserman 和
    Joao Carreira。Perceiver：具有迭代注意力的通用感知。在 Marina Meila 和 Tong Zhang 编辑的第 38 届国际机器学习会议论文集中，卷
    139，机器学习研究论文集，页码 4651–4664。PMLR，2021 年 7 月 18–24 日。'
- en: '[60] Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma,
    and Luke Zettlemoyer. Luna: Linear unified nested attention. In M. Ranzato, A. Beygelzimer,
    Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information
    Processing Systems, volume 34, pages 2441–2453\. Curran Associates, Inc., 2021.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Xuezhe Ma、Xiang Kong、Sinong Wang、Chunting Zhou、Jonathan May、Hao Ma 和 Luke
    Zettlemoyer。Luna：线性统一嵌套注意力。在 M. Ranzato、A. Beygelzimer、Y. Dauphin、P.S. Liang 和
    J. Wortman Vaughan 编辑的《神经信息处理系统进展》中，卷 34，页码 2441–2453。Curran Associates, Inc.，2021。'
- en: '[61] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le,
    and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length
    context. CoRR, abs/1901.02860, 2019.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Zihang Dai、Zhilin Yang、Yiming Yang、Jaime G. Carbonell、Quoc V. Le 和 Ruslan
    Salakhutdinov。Transformer-xl：超越固定长度上下文的注意力语言模型。CoRR，abs/1901.02860，2019。'
- en: '[62] Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Scaling transformer
    to 1m tokens and beyond with rmt, 2023.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 艾达尔·布拉托夫、尤里·库拉托夫、米哈伊尔·S·布尔特谢夫。将变压器扩展至 1 万个标记及以上，采用 RMT，2023年。'
- en: '[63] Yuhuai Wu, Markus N. Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing
    transformers, 2022.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] 吴宇怀、马库斯·N·拉比、德莱斯利·哈钦斯、克里斯蒂安·谢杰迪。记忆变压器，2022年。'
- en: '[64] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao,
    and Furu Wei. Augmenting language models with long-term memory, 2023.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] 王伟志、董磊、程浩、刘晓东、严希峰、高剑锋、魏福如。通过长期记忆增强语言模型，2023年。'
- en: '[65] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui
    Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000
    tokens, 2023.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] 丁家瑜、马书铭、董磊、张星星、黄少涵、王文辉、郑南宁、魏福如。Longnet：将变压器扩展至 1,000,000,000 个标记，2023年。'
- en: '[66] Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan
    Sung, and Yinfei Yang. Longt5: Efficient text-to-text transformer for long sequences.
    arXiv preprint arXiv:2112.07916, 2021.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] 郭曼迪、乔舒亚·安斯利、大卫·乌斯、圣地亚哥·奥坦农、倪剑默、宋云轩、杨银飞。Longt5：高效的文本到文本变压器用于长序列。arXiv 预印本
    arXiv:2112.07916，2021年。'
- en: '[67] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S.
    Gulavani, and Ramachandran Ramjee. Sarathi: Efficient llm inference by piggybacking
    decodes with chunked prefills. ArXiv, abs/2308.16369, 2023.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] 阿梅耶·阿格拉瓦尔、阿希什·潘瓦尔、贾雅什里·莫汉、尼彭·夸特拉、巴尔伽夫·S·古拉瓦尼、拉马钱德兰·拉姆吉。Sarathi：通过分块预填充高效的
    LLM 推理。ArXiv，abs/2308.16369，2023年。'
