- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:52:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:52:27'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ExCP: 通过权重-动量联合压缩的极端LLM检查点压缩'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11257](https://ar5iv.labs.arxiv.org/html/2406.11257)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11257](https://ar5iv.labs.arxiv.org/html/2406.11257)
- en: Wenshuo Li    Xinghao Chen    Han Shu    Yehui Tang    Yunhe Wang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 李文硕    陈兴浩    施涵    唐业辉    王云和
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLM) have recently attracted significant attention in
    the field of artificial intelligence. However, the training process of these models
    poses significant challenges in terms of computational and storage capacities,
    thus compressing checkpoints has become an urgent problem. In this paper, we propose
    a novel Extreme Checkpoint Compression (ExCP) framework, which significantly reduces
    the required storage of training checkpoints while achieving nearly lossless performance.
    We first calculate the residuals of adjacent checkpoints to obtain the essential
    but sparse information for higher compression ratio. To further excavate the redundancy
    parameters in checkpoints, we then propose a weight-momentum joint shrinking method
    to utilize another important information during the model optimization, i.e.,
    momentum. In particular, we exploit the information of both model and optimizer
    to discard as many parameters as possible while preserving critical information
    to ensure optimal performance. Furthermore, we utilize non-uniform quantization
    to further compress the storage of checkpoints. We extensively evaluate our proposed
    ExCP framework on several models ranging from 410M to 7B parameters and demonstrate
    significant storage reduction while maintaining strong performance. For instance,
    we achieve approximately $70\times$ compression for the Pythia-410M model, with
    the final performance being as accurate as the original model on various downstream
    tasks. Codes will be available at https://github.com/Gaffey/ExCP.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）近期在人工智能领域引起了广泛关注。然而，这些模型的训练过程在计算和存储能力方面面临重大挑战，因此压缩检查点已成为一个迫切的问题。本文提出了一种新颖的极端检查点压缩（ExCP）框架，该框架在实现几乎无损性能的同时，显著减少了训练检查点所需的存储空间。我们首先计算相邻检查点的残差，以获得高压缩比所需的基本但稀疏的信息。为了进一步挖掘检查点中的冗余参数，我们提出了一种权重-动量联合压缩方法，利用模型优化过程中另一重要信息，即动量。特别是，我们利用模型和优化器的双重信息，尽可能丢弃多余的参数，同时保留关键的信息以确保最佳性能。此外，我们利用非均匀量化进一步压缩检查点的存储。我们在多个参数范围从410M到7B的模型上广泛评估了我们提出的ExCP框架，展示了显著的存储减少，同时保持了强大的性能。例如，对于Pythia-410M模型，我们实现了约$70\times$的压缩，最终性能在各种下游任务中与原始模型一样准确。代码将会发布在
    [https://github.com/Gaffey/ExCP](https://github.com/Gaffey/ExCP)。
- en: Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Model (LLM) (Brown et al., [2020](#bib.bib3); Touvron et al.,
    [2023](#bib.bib27); [Wang et al.,](#bib.bib28) ; Chowdhery et al., [2023](#bib.bib6);
    Team et al., [2023](#bib.bib26)) has attracted the attention of the vast majority
    of academia and industry concentrated on Artificial Intelligence (AI). The current
    LLM can conduct daily conversations with humans, ask questions and answer questions,
    help humans extract information from articles and charts, and even complete professional-related
    tasks such as consultation and programming, which greatly improves the efficiency
    of human-computer interaction. Thousands of laboratories and companies are involved
    in the training of the LLMs. Computing power and storage have become key resources
    in the LLM era. Training an LLM requires up to thousands of GPUs or computing
    cards like TPUs or Ascends, and it is difficult to keep such a large computing
    cluster running completely smoothly. At the same time, researchers are also faced
    with the need to interrupt training at any time to adjust training data and hyperparameters.
    Sometimes it is even necessary to go back to earlier checkpoints to solve problems
    introduced during training. Therefore, frequent saving of checkpoints has become
    a must during the whole training process.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM） （Brown 等，[2020](#bib.bib3)；Touvron 等，[2023](#bib.bib27)；[Wang 等](#bib.bib28)；Chowdhery
    等，[2023](#bib.bib6)；Team 等，[2023](#bib.bib26)）引起了绝大多数学术界和工业界对人工智能（AI）的关注。目前的LLM可以与人类进行日常对话，提出和回答问题，帮助人类从文章和图表中提取信息，甚至完成专业相关任务如咨询和编程，从而大大提高了人机交互的效率。成千上万的实验室和公司参与了LLM的训练。计算能力和存储已经成为LLM时代的关键资源。训练LLM需要多达数千个GPU或类似TPU或Ascend的计算卡，保持如此庞大的计算集群完全顺畅运行是困难的。同时，研究人员也面临着随时中断训练以调整训练数据和超参数的需要。有时甚至需要回到早期的检查点来解决训练过程中引入的问题。因此，频繁保存检查点在整个训练过程中已成为必不可少的环节。
- en: '![Refer to caption](img/f17129329fe31f22dc35c7e191faf0f9.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f17129329fe31f22dc35c7e191faf0f9.png)'
- en: 'Figure 1: The number of parameters of some LLMs and the general training process
    of LLMs. (a) Parameters of some recent LLMs, most of them contain billions of
    weights and keep getting larger in trend. (b) The training of LLMs consists of
    several stages with variety of schemes and data. A large quantity of checkpoints
    would be stored in each stage. Considering the magnitude of LLMs’ parameters,
    extremely high capacity storage is needed for training of LLMs, which could cost
    tens of millions of dollars.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：一些LLM的参数数量以及LLM的总体训练过程。（a）一些近期LLM的参数，大多数包含数十亿的权重，并且趋势上越来越大。（b）LLM的训练包括几个阶段，采用各种方案和数据。每个阶段都会存储大量的检查点。考虑到LLM参数的规模，LLM训练需要极高容量的存储，这可能花费数千万美元。
- en: 'Take the open source model Pythia (Biderman et al., [2023](#bib.bib2)) as an
    example, the checkpoint of the largest version Pythia-12B model takes more than
    24GB to save. Not to mention the relevant momentum states of the optimizer. Adam
    optimizer requires twice the storage space of the weight. The training process
    of Pythia-12B saves 154 checkpoints which requires about 11TB storage, which would
    cost $5000 a month on a general cloud server to store these checkpoints. And this
    is just an entry-level scenario for large company. Conservative estimates suggest
    that the largest models of the most advanced LLMs, such as the GPT series and
    Gemini series, has the number of parameters on the order of hundreds to thousands
    of billions. Some publicly available data is shown Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum
    Joint Shrinking"). Larger models also require more checkpoints and longer training
    time. So the total cost of storage for a cutting-edge LLM may grow to tens of
    millions of dollars.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '以开源模型Pythia （Biderman 等，[2023](#bib.bib2)）为例，最大版本Pythia-12B模型的检查点保存需要超过24GB。更不用说优化器的相关动量状态了。Adam优化器需要比权重多两倍的存储空间。Pythia-12B的训练过程保存了154个检查点，这需要约11TB的存储空间，在普通云服务器上存储这些检查点每月需花费5000美元。这只是大型公司的一种入门级场景。保守估计，最先进的LLM的最大模型，如GPT系列和Gemini系列，其参数数量达到了数百亿到数千亿。一些公开的数据如图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum
    Joint Shrinking")所示。更大的模型还需要更多的检查点和更长的训练时间。因此，前沿LLM的总存储成本可能会增长到数千万美元。'
- en: In view of the above problems, compressing model checkpoints has become a very
    urgent need. Model compression itself is not a new topic. The model size is compressed
    to reduce the storage occupied by checkpoints (Han et al., [2015](#bib.bib14);
    Hu et al., [2020](#bib.bib15); Eisenman et al., [2022](#bib.bib11); Chen et al.,
    [2020](#bib.bib5); Jin et al., [2023](#bib.bib17); Agrawal et al., [2023](#bib.bib1))
    or compress the calculation amount of the model to improve the model’s inference
    performance (Liu et al., [2017](#bib.bib19); Tang et al., [2020](#bib.bib25);
    Dettmers et al., [2022](#bib.bib9); Xiao et al., [2023](#bib.bib31); Chen et al.,
    [2022](#bib.bib4); Shu et al., [2023](#bib.bib23); Wu et al., [2023](#bib.bib30)).
    These researches have drawn attentions of researchers in the past ten years. However,
    previous checkpoints compression work concerns more about the size of weight checkpoints
    instead of the whole training states, so there is a lack of relevant researches
    on momentum states compression. In addition, the similarity of adjacent checkpoints
    should also be considered in the compression pipeline. This feature can improve
    the pruning ratio instead of simply reducing the final size using some encoding
    techniques.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 针对上述问题，压缩模型检查点已成为一个非常紧迫的需求。模型压缩本身并不是一个新话题。通过压缩模型大小来减少检查点占用的存储空间（Han et al.,
    [2015](#bib.bib14); Hu et al., [2020](#bib.bib15); Eisenman et al., [2022](#bib.bib11);
    Chen et al., [2020](#bib.bib5); Jin et al., [2023](#bib.bib17); Agrawal et al.,
    [2023](#bib.bib1)）或压缩模型计算量以提升模型推理性能（Liu et al., [2017](#bib.bib19); Tang et al.,
    [2020](#bib.bib25); Dettmers et al., [2022](#bib.bib9); Xiao et al., [2023](#bib.bib31);
    Chen et al., [2022](#bib.bib4); Shu et al., [2023](#bib.bib23); Wu et al., [2023](#bib.bib30)）。这些研究在过去十年中引起了研究人员的关注。然而，以往的检查点压缩工作更多关注权重检查点的大小，而不是整个训练状态，因此缺乏关于动量状态压缩的相关研究。此外，压缩流程中还应考虑相邻检查点的相似性。该特性可以改善剪枝比例，而不仅仅是通过一些编码技术减少最终大小。
- en: 'In this paper, we propose a checkpoints compression framework that does not
    rely on training code and information. We calculate the residual value of adjacent
    checkpoints, apply weight-momentum joint pruning, and then non-uniformly quantize
    the weights and momentum states to extremely compress the checkpoints. Meanwhile,
    our residual compression strategy ensures that we can resume the training from
    compressed checkpoints nearly lossless. Our main contributions are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种不依赖于训练代码和信息的检查点压缩框架。我们计算相邻检查点的残差值，应用权重-动量联合剪枝，然后对权重和动量状态进行非均匀量化，以极大压缩检查点。同时，我们的残差压缩策略确保了从压缩检查点恢复训练几乎无损。我们的主要贡献如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a checkpoints compression framework which contains residual calculation,
    weight-momentum joint pruning and non-uniform quantization. This framework makes
    full use of the characteristics of checkpoints compression, achieving almost lossless
    training recovery while achieving a high compression ratio.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个包含残差计算、权重-动量联合剪枝和非均匀量化的检查点压缩框架。该框架充分利用了检查点压缩的特性，实现了几乎无损的训练恢复，同时实现了高压缩比。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We derive a weight-momentum joint pruning method, and prove the convergence
    of the optimizer under this pruning method. This is the first work to our knowledge
    that jointly considers both weights and momentum states pruning.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们推导了一种权重-动量联合剪枝方法，并证明了该剪枝方法下优化器的收敛性。这是我们所知的首个同时考虑权重和动量状态剪枝的工作。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct experiments on various models and evaluation benchmarks. Our compressed
    model achieves up to 70$\times$ nearly lossless compression on Pythia-410M model,
    which could largely reduce the storage of saving checkpoints.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在各种模型和评估基准上进行实验。我们的压缩模型在Pythia-410M模型上实现了高达70$\times$的几乎无损压缩，这可以大幅减少保存检查点的存储。
- en: 2 Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Large Language Model
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 大语言模型
- en: 'Recently, the emergence of large language model and the corresponding strong
    capabilities in various natural language processing (NLP) applications have drawn
    widespread attention in research society. The demonstrated powerful abilities
    by the model scaling have furthermore increased the parameters of large language
    models. The remarkable work GPT3 (Brown et al., [2020](#bib.bib3)) shows impressive
    performance on solving real-world NLP tasks. However, as shown in Table [1](#S2.T1
    "Table 1 ‣ 2.1 Large Language Model ‣ 2 Related Work ‣ ExCP: Extreme LLM Checkpoint
    Compression via Weight-Momentum Joint Shrinking"), the model contains 175 billion
    parameters and requires large amount of hardware resources to be trained and stored.
    A single training checkpoint of GPT3 can reach up to 2.3TB. Following large language
    models such as PaLM (Chowdhery et al., [2023](#bib.bib6)) and LLaMA (Touvron et al.,
    [2023](#bib.bib27)) consume comparable or even more hardware resources. Due to
    the huge resource consumption and common training failures, the checkpoints of
    LLMs should be updated and stored frequently, which could occupy much more resources
    of the communication bandwidth and storage devices. Thus, the exploration of redundancy
    in LLM checkpoint is meaningful and necessary, which can save the memory consumption
    in great extent and make the training procedure more efficient and more affordable.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，大型语言模型及其在各种自然语言处理 (NLP) 应用中的强大能力引起了研究界的广泛关注。模型扩展所展示的强大能力进一步增加了大型语言模型的参数。GPT3
    (Brown et al., [2020](#bib.bib3)) 的卓越工作在解决实际 NLP 任务方面表现出色。然而，如表 [1](#S2.T1 "Table
    1 ‣ 2.1 Large Language Model ‣ 2 Related Work ‣ ExCP: Extreme LLM Checkpoint Compression
    via Weight-Momentum Joint Shrinking") 所示，该模型包含 1750 亿个参数，需要大量硬件资源来训练和存储。GPT3 的单个训练检查点可以达到
    2.3TB。随后的大型语言模型如 PaLM (Chowdhery et al., [2023](#bib.bib6)) 和 LLaMA (Touvron et
    al., [2023](#bib.bib27)) 消耗了相当多甚至更多的硬件资源。由于巨大的资源消耗和常见的训练失败，LLM 的检查点应该频繁更新和存储，这可能会占用更多的通信带宽和存储设备资源。因此，探索
    LLM 检查点中的冗余是有意义且必要的，这可以大大节省内存消耗，使训练过程更加高效和经济。'
- en: 'Table 1: The parameter and checkpoint size of part LLMs. High-capacity storage
    devices are essential for checkpoints for LLM training process.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：部分 LLM 的参数和检查点大小。高容量存储设备对于 LLM 训练过程中的检查点至关重要。
- en: '| Model | Param. | Storage |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数 | 存储 |'
- en: '| --- | --- | --- |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GPT3 (Brown et al., [2020](#bib.bib3)) | 175B | 2.3TB |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| GPT3 (Brown et al., [2020](#bib.bib3)) | 175B | 2.3TB |'
- en: '| PaLM (Chowdhery et al., [2023](#bib.bib6)) | 540B | $\sim$7TB |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| PaLM (Chowdhery et al., [2023](#bib.bib6)) | 540B | $\sim$7TB |'
- en: '| LLaMA-70B (Touvron et al., [2023](#bib.bib27)) | 75B | 1.0TB |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-70B (Touvron et al., [2023](#bib.bib27)) | 75B | 1.0TB |'
- en: '| PanGu-$\pi$  ([Wang et al.,](#bib.bib28) ) | 7B | 99GB |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| PanGu-$\pi$ ([Wang et al.,](#bib.bib28)) | 7B | 99GB |'
- en: 2.2 Compression Method
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 压缩方法
- en: Data Compression Methods. Compression methods for efficient storage of data
    have been investigated for long decades. These previous methods can be categorized
    into two types, the lossy and the lossless. Lossy compression methods like JPEG (Rao
    & Hwang, [1996](#bib.bib20)) and MP3 (Sterne, [2012](#bib.bib24)) are widely used
    in the compression of image and video data which does not require precise restoration.
    Huffman coding (Huffman, [1952](#bib.bib16)) is a classic lossless compression
    method, which statisticizes the frequency of the characters to get an optimized
    coding length according to different frequency of occurrence. The lossless compression
    method can be easily applied to the checkpoints of LLMs. However, the generalizability
    of the data compression method determines that the compression rate would be relatively
    low when applied to LLM checkpoints. Specialized compression method should be
    investigated and designed to achieve a higher compression rate for heavy intrinsic
    redundancy of LLM checkpoint.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据压缩方法。为有效存储数据而研究的压缩方法已经有几十年的历史。这些以往的方法可以分为两类：有损压缩和无损压缩。有损压缩方法，如 JPEG (Rao &
    Hwang, [1996](#bib.bib20)) 和 MP3 (Sterne, [2012](#bib.bib24))，广泛用于图像和视频数据的压缩，这些数据不要求精确恢复。霍夫曼编码
    (Huffman, [1952](#bib.bib16)) 是一种经典的无损压缩方法，它通过统计字符的频率来根据不同的出现频率获取优化的编码长度。无损压缩方法可以很容易地应用于
    LLM 的检查点。然而，数据压缩方法的通用性决定了，当应用于 LLM 检查点时，压缩率相对较低。因此，应研究和设计专门的压缩方法，以实现更高的压缩率，以应对
    LLM 检查点的重内在冗余。
- en: Neural Network Compression Methods. The neural network compression methods have
    been explored by many work due to the increasing model size and computation resources.
    DeepCompression (Han et al., [2015](#bib.bib14)) utilizes network pruning, quantization
    and huffman coding to obtain a compact neural network. Llm (Dettmers et al., [2022](#bib.bib9))
    and Smoothquant (Xiao et al., [2023](#bib.bib31)) adopt the quantization to compress
    the large language models. These network compression methods could reduce the
    quantity or bit-width of parameters in neural networks but are often highly related
    to the training targets. Thus, these methods cannot be generally applied to compress
    the checkpoints with various task background. Moreover, re-training or finetuning
    is often necessary for compression methods like network pruning (Liu et al., [2017](#bib.bib19))
    and quantization-aware training (Esser et al., [2019](#bib.bib12)), which could
    be extremely computationally expensive especially for large language models with
    huge training data and huge amount of network parameters.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络压缩方法。由于模型规模和计算资源的增加，许多工作探索了神经网络压缩方法。DeepCompression (Han et al., [2015](#bib.bib14))利用网络剪枝、量化和霍夫曼编码来获得紧凑的神经网络。Llm (Dettmers
    et al., [2022](#bib.bib9))和Smoothquant (Xiao et al., [2023](#bib.bib31))采用量化来压缩大型语言模型。这些网络压缩方法可以减少神经网络中参数的数量或位宽，但通常与训练目标高度相关。因此，这些方法不能普遍应用于压缩具有各种任务背景的检查点。此外，像网络剪枝 (Liu
    et al., [2017](#bib.bib19))和量化感知训练 (Esser et al., [2019](#bib.bib12))这样的压缩方法通常需要重新训练或微调，这可能会非常耗费计算资源，特别是对于具有大量训练数据和网络参数的大型语言模型。
- en: Compression methods for checkpoints. As the deep neural network model getting
    larger and the training cost getting more expensive, some research work begin
    to focus on the compression of checkpoints. LC-Checkpoint (Chen et al., [2020](#bib.bib5))
    proposes a lossy compression scheme for checkpoint constructions on the assumption
    of SGD optimizer. Check-N-Run (Eisenman et al., [2022](#bib.bib11)) applies differential
    and quantization for recommendation models. Delta-DNN  (Hu et al., [2020](#bib.bib15))
    focuses on the storage of floating point numbers and records the differential
    of two neighboring versions. QD-Compressor (Jin et al., [2023](#bib.bib17)) further
    develops a layer-based quantization and achieves higher compression ratio. When
    these methods applied on large-scale models of LLM, undesirable accuracy degradation
    would occur due to the uniform and constant quantization strategy during training
    procedure. Recent DynaQuant (Agrawal et al., [2023](#bib.bib1)) tackles this issue
    by precisely compressing model parameters based on different contributions to
    the final result quality with an efficient dynamic quantization configuration
    and a quantization-aware delta encoding scheme. However, most of previous work
    focus on the compression of model parameters while ignoring the momentum states
    of optimizer, which occupy more memory storage and exist more redundancy in LLM
    checkpoints.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点的压缩方法。随着深度神经网络模型的规模增大和训练成本的提高，一些研究工作开始关注检查点的压缩。LC-Checkpoint (Chen et al.,
    [2020](#bib.bib5))提出了一种基于SGD优化器假设的有损压缩方案。Check-N-Run (Eisenman et al., [2022](#bib.bib11))对推荐模型应用了差分和量化。Delta-DNN
     (Hu et al., [2020](#bib.bib15))关注浮点数的存储，并记录两个相邻版本的差分。QD-Compressor (Jin et al.,
    [2023](#bib.bib17))进一步开发了基于层的量化，并实现了更高的压缩比。当这些方法应用于大规模LLM模型时，由于训练过程中的均匀和固定量化策略，会出现不希望的精度下降。最近，DynaQuant (Agrawal
    et al., [2023](#bib.bib1))通过基于对最终结果质量的不同贡献精确压缩模型参数，并结合高效的动态量化配置和量化感知的差分编码方案，解决了这个问题。然而，大多数先前的工作集中在模型参数的压缩上，而忽略了优化器的动量状态，这在LLM检查点中占用了更多的内存存储，并存在更多的冗余。
- en: 3 Our Method
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 我们的方法
- en: '![Refer to caption](img/9339295cdae03d9936a18ef26d44aae1.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9339295cdae03d9936a18ef26d44aae1.png)'
- en: 'Figure 2: Framework of our proposed compression process. We calculate the residual
    $\Delta\mathcal{W}_{t}$.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：我们提出的压缩过程框架。我们计算残差$\Delta\mathcal{W}_{t}$。
- en: A checkpoint $\mathcal{P}_{t}$ of the optimizer momentum.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器动量的一个检查点$\mathcal{P}_{t}$。
- en: '|  | $\mathcal{P}_{t}=\{\mathcal{W}_{t},\mathcal{O}_{t}\}.$ |  | (1) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{P}_{t}=\{\mathcal{W}_{t},\mathcal{O}_{t}\}.$ |  | (1) |'
- en: Saving checkpoints for $T$.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 保存$T$的检查点。
- en: '|  | $\mathcal{P}=\{\mathcal{P}_{1},\mathcal{P}_{2},\cdots,\mathcal{P}_{t}\,\cdots,\mathcal{P}_{T}\}.$
    |  | (2) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{P}=\{\mathcal{P}_{1},\mathcal{P}_{2},\cdots,\mathcal{P}_{t}\,\cdots,\mathcal{P}_{T}\}.$
    |  | (2) |'
- en: For the widely used Adam optimizer, the parameters with most significant storage
    cost are the first-order and second-order moments $v_{t}$, i.e.,
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于广泛使用的Adam优化器，存储成本最高的参数是一级和二级动量$v_{t}$，即，
- en: '|  | $\mathcal{O}_{t}=\{v_{t},m_{t}\}.$ |  | (3) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{O}_{t}=\{v_{t},m_{t}\}.$ |  | (3) |'
- en: Note that some variables such as learning rate and weight decay etc. are also
    stored in the optimizer checkpoint, but can be simply neglected when compared
    with moments.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意一些变量如学习率和权重衰减等也存储在优化器检查点中，但与动量相比，这些变量可以简单忽略。
- en: In the traditional pruning-related work, researchers only concern about the
    weights of models, since the main purpose of pruning is reducing the overhead
    of calculation and storage during the inference stage. However, when we turn to
    the checkpoint compression during the training process, the pruning of momentum
    is also important to reduce the total size of training checkpoints. Take the most
    general optimizer Adam used in LLM training as an example, it saves the first-order
    and second-order moment of gradients which require double storage of weights.
    Therefore, we have to take both model weights and optimizer momentum states into
    consideration for extreme compression of model training checkpoints.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的剪枝相关工作中，研究人员仅关注模型的权重，因为剪枝的主要目的是减少推理阶段的计算和存储开销。然而，当我们转向训练过程中的检查点压缩时，动量的剪枝也很重要，以减少训练检查点的总大小。以LLM训练中使用的最通用优化器Adam为例，它保存梯度的一阶和二阶动量，这需要双倍于权重的存储。因此，我们必须同时考虑模型权重和优化器动量状态，以实现模型训练检查点的极限压缩。
- en: 3.1 Residual Checkpoint
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 残差检查点
- en: During the $t^{th}$ is defined as
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在$t^{th}$定义为
- en: '|  | $\Delta\mathcal{P}_{t}=\{\Delta\mathcal{W}_{t},\mathcal{O}_{t}\}=\{\mathcal{W}_{t}-\mathcal{W}_{t-1},\mathcal{O}_{t}\}.$
    |  | (4) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta\mathcal{P}_{t}=\{\Delta\mathcal{W}_{t},\mathcal{O}_{t}\}=\{\mathcal{W}_{t}-\mathcal{W}_{t-1},\mathcal{O}_{t}\}.$
    |  | (4) |'
- en: 'There is a comparison between the pruning on residual checkpoint and pruning
    on original checkpoint in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Residual Checkpoint
    ‣ 3 Our Method ‣ ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum
    Joint Shrinking"). We plot the histogram of the original weights, weights after
    direct pruning and weights after pruning on residual checkpoints. We find that
    pruning the residual checkpoint has almost no impact on the parameter distribution.
    This helps us to further prune the parameters.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '图[3](#S3.F3 "Figure 3 ‣ 3.1 Residual Checkpoint ‣ 3 Our Method ‣ ExCP: Extreme
    LLM Checkpoint Compression via Weight-Momentum Joint Shrinking")中展示了对残差检查点剪枝与对原始检查点剪枝的比较。我们绘制了原始权重、直接剪枝后的权重和残差检查点剪枝后的权重的直方图。我们发现，对残差检查点进行剪枝对参数分布几乎没有影响。这有助于我们进一步剪枝参数。'
- en: '![Refer to caption](img/192eafad1944a52c11629981639119a2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/192eafad1944a52c11629981639119a2.png)'
- en: 'Figure 3: Weights distribution for original weights, pruning on residual checkpoints
    and pruning on original weights. We plot the histogram of random 100k non-zero
    weights of each case for clarity. The range of bins are bounded by (mean - 3 *
    std, mean + 3 * std) and 256 bins are used.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：原始权重、对残差检查点剪枝和对原始权重剪枝的权重分布。为了清晰起见，我们绘制了每种情况的100k个随机非零权重的直方图。箱子的范围由（均值 - 3
    * 标准差，均值 + 3 * 标准差）限定，使用了256个箱子。
- en: 3.2 Joint Weight-Momentum Pruning
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 联合权重-动量剪枝
- en: Weight pruning is a common way to discard unimportant values while maintaining
    the performance to the maximum extent. For the checkpoint compression, we need
    to obtain the corresponding pruning masks for model weights and momentum states,
    which are denoted as $\mathcal{M}_{w}$, respectively. Intuitive way for pruning
    model weights and momentum states is to discard values with some pre-defined metric.
    However, this separate strategy leads to sub-optimal solution since there are
    strong relations between model weights and momentum states. Therefore, in this
    paper we propose a novel joint weight-momentum pruning method that obtains better
    performance for checkpoint compression.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 权重剪枝是一种在保持性能的最大程度下丢弃不重要值的常见方法。对于检查点压缩，我们需要分别获得模型权重和动量状态的相应剪枝掩码，分别表示为$\mathcal{M}_{w}$。剪枝模型权重和动量状态的直观方法是丢弃具有一些预定义度量的值。然而，这种单独的策略会导致次优解决方案，因为模型权重和动量状态之间存在强关系。因此，本文提出了一种新颖的联合权重-动量剪枝方法，以获得更好的检查点压缩性能。
- en: 'Weight Pruning. For weights pruning, using the magnitude or the gradients of
    weights as an indicator is the common practice. There is a little difference between
    our weight pruning task and the general one. As we introduced in Section [3.1](#S3.SS1
    "3.1 Residual Checkpoint ‣ 3 Our Method ‣ ExCP: Extreme LLM Checkpoint Compression
    via Weight-Momentum Joint Shrinking"), we need to prune the residual values of
    weights of two adjacent checkpoints instead of the original value of weights.
    Thus we recommend to use the second-order moment of gradients of weights as an
    indicator, since they can represent the statistical average of the weight change
    during training process. We use the indicator to calculate the pruning threshold
    of each layer and the formula is shown below,'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '权重剪枝。对于权重剪枝，使用权重的幅值或梯度作为指标是常见的做法。我们的权重剪枝任务与一般任务之间有些许差异。正如我们在第[3.1节](#S3.SS1
    "3.1 Residual Checkpoint ‣ 3 Our Method ‣ ExCP: Extreme LLM Checkpoint Compression
    via Weight-Momentum Joint Shrinking")中介绍的，我们需要剪枝两个相邻检查点的权重残差值，而不是权重的原始值。因此，我们建议使用权重梯度的二阶矩作为指标，因为它们可以表示训练过程中的权重变化的统计平均值。我们使用该指标来计算每层的剪枝阈值，公式如下，'
- en: '|  | $$r_{w}=\frac{\alpha}{\sqrt{m_{t}}}\times\text{median}(\mathcal{W}),\\
    \mathcal{M}_{w}(i)=\mathds{1}_{w_{t}(i)> |  | (5) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $$r_{w}=\frac{\alpha}{\sqrt{m_{t}}}\times\text{median}(\mathcal{W}),\\
    \mathcal{M}_{w}(i)=\mathds{1}_{w_{t}(i)> |  | (5) |'
- en: In which $\mathcal{W}$ is a hyperparameter. After determining the pruning threshold
    of each layer, we prune the residual of weights to zero by magnitude for each
    layer.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{W}$ 是一个超参数。在确定每层的剪枝阈值后，我们通过幅值将每层的残差权重剪枝为零。
- en: Momentum Pruning. For momentum pruning, we use the first-order moment as an
    indicator to determine whether to prune this momentum states or not. We give a
    brief proof in the next section to explain why we choose it. Besides, if a specific
    location of weights is pruned, intuitively it is not important to preserve the
    corresponding momentum states. We prune the parameters of momentum following the
    formula below, in which $\beta$ is a hyper-parameter.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 动量剪枝。对于动量剪枝，我们使用一阶矩作为指标来确定是否剪枝这些动量状态。我们在下一节中给出了一个简要证明，解释为什么选择它。此外，如果剪枝了权重的特定位置，直观上不需要保留相应的动量状态。我们按照以下公式剪枝动量参数，其中
    $\beta$ 是一个超参数。
- en: '|  | Question:
    What kind of exercise do you suggest I do indoors? Original PanGu-$\pi$-1B: There
    are a lot of great exercises you can do indoors to help improve your fitness,
    such as: 1\. Yoga: Yoga is a great way to improve flexibility, strength, and balance.
    There are many online videos and classes available that can help you learn the
    basics. 2\. Bodyweight exercises: You can do bodyweight exercises such as push-ups,
    squats, lunges, and planks to build strength and improve your overall fitness.
    3\. Dancing: Dancing is a fun way to improve your cardiovascular fitness, coordination,
    and balance. There are many online tutorials and classes available that can help
    you learn the basics. 4\. Yoga for beginners: If you’re new to yoga, there are
    many beginner classes available that can help you learn the basics. 5\. Pilates:
    Pilates is a great way to improve your core strength, flexibility, and balance.
    There are many online videos and classes available that can help you learn the
    basics. Remember to start slow and gradually increase the intensity and duration
    of your workouts as you get stronger and more confident.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 'Question:
    What kind of exercise do you suggest I do indoors? Original PanGu-$\pi$-1B: There
    are a lot of great exercises you can do indoors to help improve your fitness,
    such as: 1\. Yoga: Yoga is a great way to improve flexibility, strength, and balance.
    There are many online videos and classes available that can help you learn the
    basics. 2\. Bodyweight exercises: You can do bodyweight exercises such as push-ups,
    squats, lunges, and planks to build strength and improve your overall fitness.
    3\. Dancing: Dancing is a fun way to improve your cardiovascular fitness, coordination,
    and balance. There are many online tutorials and classes available that can help
    you learn the basics. 4\. Yoga for beginners: If you’re new to yoga, there are
    many beginner classes available that can help you learn the basics. 5\. Pilates:
    Pilates is a great way to improve your core strength, flexibility, and balance.
    There are many online videos and classes available that can help you learn the
    basics. Remember to start slow and gradually increase the intensity and duration
    of your workouts as you get stronger and more confident.'
- en: 'Figure 5: Q&A example to show the difference between our compressed model and
    the original model.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：问答示例，展示我们压缩模型与原始模型之间的差异。
- en: 'We show a question & answer results for PanGu-$\pi$-1B model in Figure [5](#S4.F5
    "Figure 5 ‣ 4.3 Experimental Results ‣ 4 Experiments ‣ ExCP: Extreme LLM Checkpoint
    Compression via Weight-Momentum Joint Shrinking") to show the difference between
    our compressed model and the original model. In this example, our compressed model
    shows a better understanding to the limit indoors and gives a better answer. It
    proves that our compressed model perfroms even better than the original one in
    some cases. More results are shown in the Appendix [B](#A2 "Appendix B More Visualization
    of Compressed LLM. ‣ ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum
    Joint Shrinking").'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图[5](#S4.F5 "Figure 5 ‣ 4.3 Experimental Results ‣ 4 Experiments ‣ ExCP:
    Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking")中展示了PanGu-$\pi$-1B模型的问答结果，以展示我们压缩模型与原始模型之间的差异。在这个例子中，我们的压缩模型对室内限制的理解更好，并给出了更好的答案。这证明了在某些情况下，我们的压缩模型甚至比原始模型表现更好。更多结果见附录[B](#A2
    "Appendix B More Visualization of Compressed LLM. ‣ ExCP: Extreme LLM Checkpoint
    Compression via Weight-Momentum Joint Shrinking")。'
- en: 4.4 Ablation Studies
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 消融研究
- en: 'Table 5: Ablation study of our methods. Applying residual, joint-prune and
    quantization together achieves the best size while the average accuracy is almost
    lossless.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：我们方法的消融研究。应用残差、联合剪枝和量化一起实现了最佳大小，而平均准确度几乎没有损失。
- en: '| method | Size | Avg Acc |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 大小 | 平均准确度 |'
- en: '| --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| residual | prune | quant |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 残差 | 剪枝 | 量化 |'
- en: '| --- | --- | --- |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  |  |  | 4070M | 43.11 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 4070M | 43.11 |'
- en: '| ✓ |  |  | 3484M | 43.11 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| ✓ |  |  | 3484M | 43.11 |'
- en: '|  | ✓ |  | 324M | 29.95 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | ✓ |  | 324M | 29.95 |'
- en: '|  |  | ✓ | 492M | 40.17 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ✓ | 492M | 40.17 |'
- en: '| ✓ | ✓ |  | 276M | 42.92 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ |  | 276M | 42.92 |'
- en: '| ✓ |  | ✓ | 493M | 42.94 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| ✓ |  | ✓ | 493M | 42.94 |'
- en: '| ✓ | ✓ | ✓ | 61M | 42.93 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✓ | 61M | 42.93 |'
- en: 'We also do some ablation studies to show that every method in our compression
    pipeline is of vital importance. The results are shown in Table [5](#S4.T5 "Table
    5 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ ExCP: Extreme LLM Checkpoint Compression
    via Weight-Momentum Joint Shrinking"). Although calculating the residual of adjacent
    models cannot bring a significant storage reduce, it plays an important role in
    the whole pipeline. Directly pruning weights may harm the accuracy largely, while
    the residual of adjacent models does not have this problem. Joint-pruning and
    quantization on residual checkpoint separately compress the model by 15$\times$.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还进行了一些消融研究，以表明我们压缩管道中的每种方法都是至关重要的。结果显示在表[5](#S4.T5 "Table 5 ‣ 4.4 Ablation
    Studies ‣ 4 Experiments ‣ ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum
    Joint Shrinking")中。虽然计算相邻模型的残差无法显著减少存储，但在整个管道中发挥了重要作用。直接剪枝权重可能会大大损害准确性，而相邻模型的残差没有这个问题。对残差检查点进行联合剪枝和量化分别将模型压缩了15$\times$。'
- en: 'Table 6: Ablation study of different quantization bins. We choose 4 bit in
    all other experiments since it achieves better performance-size trade-off.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：不同量化箱的消融研究。由于4位在所有其他实验中表现出更好的性能-大小权衡，我们选择了4位。
- en: '| Quant bins | Size | Avg Acc |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 量化箱 | 大小 | 平均准确度 |'
- en: '| --- | --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 2 bit | 43M | 42.46 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 2位 | 43M | 42.46 |'
- en: '| 4 bit | 61M | 42.93 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 4位 | 61M | 42.93 |'
- en: '| 8 bit | 87M | 42.90 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 8 bit | 87M | 42.90 |'
- en: 'We explore the influence of different quantization bins. From Table [6](#S4.T6
    "Table 6 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ ExCP: Extreme LLM Checkpoint
    Compression via Weight-Momentum Joint Shrinking"), we can find that quantization
    below 4 bit cannot bring a significant storage reduce, so we choose 4 bit which
    achieves better performance-size trade-off. In some cases which extreme small
    checkpoint size is required, 2 bit could be used to further compress the checkpoints
    a little bit more.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '我们探讨了不同量化位数的影响。从表格 [6](#S4.T6 "表 6 ‣ 4.4 消融研究 ‣ 4 实验 ‣ ExCP: 通过权重-动量联合缩小进行极限
    LLM 检查点压缩") 中，我们可以发现低于 4 位的量化无法带来显著的存储减少，因此我们选择了 4 位，这在性能-大小权衡方面表现更好。在某些极端需要小检查点大小的情况下，2
    位可以用来进一步压缩检查点。'
- en: 'Table 7: Comparison of different compression algorithms. We choose 7zip in
    all other experiments since it outperforms other algorithms.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 不同压缩算法的比较。我们在所有其他实验中选择了 7zip，因为它优于其他算法。'
- en: '|  | zip | rar | rar4 | bz2 | 7z |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | zip | rar | rar4 | bz2 | 7z |'
- en: '| size | 73M | 70M | 69M | 64M | 61M |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| size | 73M | 70M | 69M | 64M | 61M |'
- en: 'We also evaluate different compression algorithms to compact the final checkpoint
    files. The results are shown in Table [7](#S4.T7 "Table 7 ‣ 4.4 Ablation Studies
    ‣ 4 Experiments ‣ ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum
    Joint Shrinking"). The 7zip compression with LZMA2 algorithm achieves the best
    compression ratio, which leads to about 20% less storage, and we apply 7zip with
    the ultra compression ratio on all other experiments.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还评估了不同的压缩算法来压缩最终的检查点文件。结果显示在表格 [7](#S4.T7 "表 7 ‣ 4.4 消融研究 ‣ 4 实验 ‣ ExCP:
    通过权重-动量联合缩小进行极限 LLM 检查点压缩") 中。使用 LZMA2 算法的 7zip 压缩实现了最佳的压缩比，这使得存储减少了约 20%，我们在所有其他实验中应用了
    7zip 的超高压缩比。'
- en: 5 Conclusion
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we discuss the extreme compression of LLM checkpoint. We propose
    a checkpoint compression framework which contains residual calculation, weights-momentum
    joint pruning and non-uniform quantization. We derive the criterion for weight-momentum
    joint-pruning and prove the convergence of the pruned momentum states. Experimental
    results show the effectiveness of our methods. We compress Pythia-410M by $\sim
    70\times$ while achieving nearly lossless results on down-stream evaluations.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们讨论了 LLM 检查点的极限压缩。我们提出了一个检查点压缩框架，该框架包含残差计算、权重-动量联合剪枝和非均匀量化。我们推导了权重-动量联合剪枝的标准，并证明了剪枝动量状态的收敛性。实验结果表明我们的方法的有效性。我们将
    Pythia-410M 压缩了约 $\sim 70\times$，同时在下游评估中实现了几乎无损的结果。
- en: In the future, we would try to extend the experiments to different tasks such
    as multi-modal large models and visual large models. And different types of neural
    networks including transformers, CNNs and RNNs would be taken into consideration.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，我们将尝试将实验扩展到多模态大型模型和视觉大型模型等不同任务中。此外，还会考虑包括变换器、卷积神经网络和递归神经网络在内的不同类型的神经网络。
- en: Impact Statement
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 影响声明
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了旨在推动机器学习领域发展的工作。我们的工作有许多潜在的社会影响，但我们认为没有必要在这里特别突出这些影响。
- en: References
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Agrawal et al. (2023) Agrawal, A., Reddy, S., Bhattamishra, S., Nookala, V.
    P. S., Vashishth, V., Rong, K., and Tumanov, A. Dynaquant: Compressing deep learning
    training checkpoints via dynamic quantization. *arXiv preprint arXiv:2306.11800*,
    2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Agrawal 等 (2023) Agrawal, A., Reddy, S., Bhattamishra, S., Nookala, V. P. S.,
    Vashishth, V., Rong, K., 和 Tumanov, A. Dynaquant: 通过动态量化压缩深度学习训练检查点。*arXiv 预印本
    arXiv:2306.11800*，2023。'
- en: 'Biderman et al. (2023) Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley,
    H., O’Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff,
    E., et al. Pythia: A suite for analyzing large language models across training
    and scaling. In *International Conference on Machine Learning*, pp. 2397–2430\.
    PMLR, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Biderman 等 (2023) Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H.,
    O’Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E.,
    等. Pythia: 用于分析大型语言模型的套件，涵盖训练和扩展。发表于 *国际机器学习会议*，第 2397–2430 页。PMLR，2023。'
- en: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language
    models are few-shot learners. *Advances in neural information processing systems*,
    33:1877–1901, 2020.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,
    P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., 等。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901，2020年。
- en: 'Chen et al. (2022) Chen, X., Zhang, Y., and Wang, Y. Mtp: multi-task pruning
    for efficient semantic segmentation networks. In *2022 IEEE International Conference
    on Multimedia and Expo (ICME)*, pp.  1–6\. IEEE, 2022.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2022）Chen, X., Zhang, Y., 和 Wang, Y. Mtp: 高效语义分割网络的多任务剪枝。收录于*2022 IEEE
    国际多媒体与博览会（ICME）*，第 1–6 页。IEEE，2022年。'
- en: Chen et al. (2020) Chen, Y., Liu, Z., Ren, B., and Jin, X. On efficient constructions
    of checkpoints. *arXiv preprint arXiv:2009.13003*, 2020.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2020）Chen, Y., Liu, Z., Ren, B., 和 Jin, X. 关于检查点的高效构造。*arXiv 预印本 arXiv:2009.13003*，2020年。
- en: 'Chowdhery et al. (2023) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm:
    Scaling language modeling with pathways. *Journal of Machine Learning Research*,
    24(240):1–113, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等（2023）Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., 等。Palm: 通过路径扩展语言建模。*机器学习研究期刊*，24(240):1–113，2023年。'
- en: 'Contributors (2023) Contributors, O. Opencompass: A universal evaluation platform
    for foundation models. [https://github.com/open-compass/opencompass](https://github.com/open-compass/opencompass),
    2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Contributors（2023）Contributors, O. Opencompass: 一种通用的基础模型评估平台。 [https://github.com/open-compass/opencompass](https://github.com/open-compass/opencompass)，2023年。'
- en: Défossez et al. (2020) Défossez, A., Bottou, L., Bach, F., and Usunier, N. A
    simple convergence proof of adam and adagrad. *arXiv preprint arXiv:2003.02395*,
    2020.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Défossez 等（2020）Défossez, A., Bottou, L., Bach, F., 和 Usunier, N. Adam 和 Adagrad
    的简单收敛性证明。*arXiv 预印本 arXiv:2003.02395*，2020年。
- en: 'Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,
    L. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*, 2022.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2022）Dettmers, T., Lewis, M., Belkada, Y., 和 Zettlemoyer, L. Llm.
    int8 (): 大规模变换器的 8 位矩阵乘法。*arXiv 预印本 arXiv:2208.07339*，2022年。'
- en: 'Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., et al. An image is worth 16x16 words: Transformers for image recognition at
    scale. *arXiv preprint arXiv:2010.11929*, 2020.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等（2020）Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., 等。一张图像值 16x16 个词：用于大规模图像识别的变换器。*arXiv 预印本 arXiv:2010.11929*，2020年。
- en: 'Eisenman et al. (2022) Eisenman, A., Matam, K. K., Ingram, S., Mudigere, D.,
    Krishnamoorthi, R., Nair, K., Smelyanskiy, M., and Annavaram, M. $\{$: A checkpointing
    system for training deep learning recommendation models. In *19th USENIX Symposium
    on Networked Systems Design and Implementation (NSDI 22)*, pp.  929–943, 2022.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eisenman 等（2022）Eisenman, A., Matam, K. K., Ingram, S., Mudigere, D., Krishnamoorthi,
    R., Nair, K., Smelyanskiy, M., 和 Annavaram, M. $\{$：一种用于训练深度学习推荐模型的检查点系统。收录于*第
    19 届 USENIX 网络系统设计与实现研讨会（NSDI 22）*，第 929–943 页，2022年。
- en: Esser et al. (2019) Esser, S. K., McKinstry, J. L., Bablani, D., Appuswamy,
    R., and Modha, D. S. Learned step size quantization. *arXiv preprint arXiv:1902.08153*,
    2019.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esser 等（2019）Esser, S. K., McKinstry, J. L., Bablani, D., Appuswamy, R., 和 Modha,
    D. S. 学习步长量化。*arXiv 预印本 arXiv:1902.08153*，2019年。
- en: 'Gao et al. (2020) Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,
    Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The pile: An 800gb
    dataset of diverse text for language modeling. *arXiv preprint arXiv:2101.00027*,
    2020.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等（2020）Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster,
    C., Phang, J., He, H., Thite, A., Nabeshima, N., 等。The pile: 一个包含多样文本的 800GB 数据集，用于语言建模。*arXiv
    预印本 arXiv:2101.00027*，2020年。'
- en: 'Han et al. (2015) Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. *arXiv
    preprint arXiv:1510.00149*, 2015.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等（2015）Han, S., Mao, H., 和 Dally, W. J. 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。*arXiv
    预印本 arXiv:1510.00149*，2015年。
- en: 'Hu et al. (2020) Hu, Z., Zou, X., Xia, W., Jin, S., Tao, D., Liu, Y., Zhang,
    W., and Zhang, Z. Delta-dnn: Efficiently compressing deep neural networks via
    exploiting floats similarity. In *Proceedings of the 49th International Conference
    on Parallel Processing*, pp.  1–12, 2020.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等（2020）Hu, Z., Zou, X., Xia, W., Jin, S., Tao, D., Liu, Y., Zhang, W., 和
    Zhang, Z. Delta-dnn: 通过利用浮点相似性高效压缩深度神经网络。收录于*第 49 届国际并行处理大会论文集*，第 1–12 页，2020年。'
- en: Huffman (1952) Huffman, D. A. A method for the construction of minimum-redundancy
    codes. *Proceedings of the IRE*, 40(9):1098–1101, 1952.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huffman (1952) Huffman, D. A. 一种构建最小冗余编码的方法。*IRE 会议记录*，40(9):1098–1101，1952年。
- en: Jin et al. (2023) Jin, H., Wu, D., Zhang, S., Zou, X., Jin, S., Tao, D., Liao,
    Q., and Xia, W. Design of a quantization-based dnn delta compression framework
    for model snapshots and federated learning. *IEEE Transactions on Parallel and
    Distributed Systems*, 34(3):923–937, 2023.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin et al. (2023) Jin, H., Wu, D., Zhang, S., Zou, X., Jin, S., Tao, D., Liao,
    Q., 和 Xia, W. 基于量化的 DNN Delta 压缩框架的设计，用于模型快照和联邦学习。*IEEE Transactions on Parallel
    and Distributed Systems*，34(3):923–937，2023年。
- en: 'Kingma & Ba (2014) Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization.
    *arXiv preprint arXiv:1412.6980*, 2014.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma & Ba (2014) Kingma, D. P. 和 Ba, J. Adam: 一种用于随机优化的方法。*arXiv 预印本 arXiv:1412.6980*，2014年。'
- en: Liu et al. (2017) Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang,
    C. Learning efficient convolutional networks through network slimming. In *Proceedings
    of the IEEE international conference on computer vision*, pp.  2736–2744, 2017.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2017) Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., 和 Zhang, C.
    通过网络瘦身学习高效卷积网络。在 *IEEE 国际计算机视觉会议论文集*，第2736–2744页，2017年。
- en: Rao & Hwang (1996) Rao, K. R. and Hwang, J. J. *Techniques and standards for
    image, video, and audio coding*. Prentice-Hall, Inc., 1996.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rao & Hwang (1996) Rao, K. R. 和 Hwang, J. J. *图像、视频和音频编码的技术和标准*。Prentice-Hall,
    Inc.，1996年。
- en: Sashank et al. (2018) Sashank, J. R., Satyen, K., and Sanjiv, K. On the convergence
    of adam and beyond. In *International conference on learning representations*,
    volume 5, 2018.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sashank et al. (2018) Sashank, J. R., Satyen, K., 和 Sanjiv, K. 关于 Adam 的收敛性及其扩展。在
    *国际学习表征会议*，第5卷，2018年。
- en: Shi & Li (2021) Shi, N. and Li, D. Rmsprop converges with proper hyperparameter.
    In *International conference on learning representation*, 2021.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi & Li (2021) Shi, N. 和 Li, D. Rmsprop 通过适当的超参数收敛。在 *国际学习表征会议*，2021年。
- en: 'Shu et al. (2023) Shu, H., Li, W., Tang, Y., Zhang, Y., Chen, Y., Li, H., Wang,
    Y., and Chen, X. Tinysam: Pushing the envelope for efficient segment anything
    model. *arXiv preprint arXiv:2312.13789*, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shu et al. (2023) Shu, H., Li, W., Tang, Y., Zhang, Y., Chen, Y., Li, H., Wang,
    Y., 和 Chen, X. Tinysam: 推动高效分割模型的极限。*arXiv 预印本 arXiv:2312.13789*，2023年。'
- en: 'Sterne (2012) Sterne, J. *MP3: The meaning of a format*. Duke University Press,
    2012.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sterne (2012) Sterne, J. *MP3: 格式的意义*。Duke University Press，2012年。'
- en: 'Tang et al. (2020) Tang, Y., Wang, Y., Xu, Y., Tao, D., Xu, C., Xu, C., and
    Xu, C. Scop: Scientific control for reliable neural network pruning. *Advances
    in Neural Information Processing Systems*, 33:10936–10947, 2020.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tang et al. (2020) Tang, Y., Wang, Y., Xu, Y., Tao, D., Xu, C., Xu, C., 和 Xu,
    C. Scop: 用于可靠神经网络剪枝的科学控制。*神经信息处理系统进展*，33:10936–10947，2020年。'
- en: 'Team et al. (2023) Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B.,
    Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family
    of highly capable multimodal models. *arXiv preprint arXiv:2312.11805*, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team et al. (2023) Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B.,
    Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., 等。Gemini: 一系列高能力的多模态模型。*arXiv
    预印本 arXiv:2312.11805*，2023年。'
- en: 'Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama:
    Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., 等。Llama: 开放和高效的基础语言模型。*arXiv
    预印本 arXiv:2302.13971*，2023年。'
- en: '(28) Wang, Y., Chen, H., Tang, Y., Guo, T., Han, K., Nie, Y., Wang, X., Hu,
    H., Bai, Z., Wang, Y., et al. Pangu-$\pi$: Enhancing language model architectures
    via nonlinearity compensation.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(28) Wang, Y., Chen, H., Tang, Y., Guo, T., Han, K., Nie, Y., Wang, X., Hu,
    H., Bai, Z., Wang, Y., 等。Pangu-$\pi$: 通过非线性补偿增强语言模型架构。'
- en: 'Wang et al. (2023) Wang, Y., Chen, H., Tang, Y., Guo, T., Han, K., Nie, Y.,
    Wang, X., Hu, H., Bai, Z., Wang, Y., et al. Pangu–$\pi$: Enhancing language model
    architectures via nonlinearity compensation. *arXiv preprint arXiv:2312.17276*,
    2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023) Wang, Y., Chen, H., Tang, Y., Guo, T., Han, K., Nie, Y.,
    Wang, X., Hu, H., Bai, Z., Wang, Y., 等。Pangu–$\pi$: 通过非线性补偿增强语言模型架构。*arXiv 预印本
    arXiv:2312.17276*，2023年。'
- en: 'Wu et al. (2023) Wu, X., Zeng, F., Wang, X., Wang, Y., and Chen, X. Ppt: Token
    pruning and pooling for efficient vision transformers. *arXiv preprint arXiv:2310.01812*,
    2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. (2023) Wu, X., Zeng, F., Wang, X., Wang, Y., 和 Chen, X. Ppt: 高效视觉变换器的标记剪枝和池化。*arXiv
    预印本 arXiv:2310.01812*，2023年。'
- en: 'Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and
    Han, S. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In *International Conference on Machine Learning*, pp. 38087–38099\.
    PMLR, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao等（2023）Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., 和Han, S. Smoothquant:
    用于大语言模型的准确高效的训练后量化。在*国际机器学习会议*，第38087–38099页。PMLR，2023。'
- en: Zhang et al. (2022) Zhang, Y., Chen, C., Shi, N., Sun, R., and Luo, Z.-Q. Adam
    can converge without any modification on update rules. *Advances in neural information
    processing systems*, 35:28386–28399, 2022.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2022）Zhang, Y., Chen, C., Shi, N., Sun, R., and Luo, Z.-Q. Adam可以在不修改更新规则的情况下收敛。*神经信息处理系统进展*，35:28386–28399，2022。
- en: Appendix A Convergence Analysis.
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 收敛分析。
- en: Theorem A.1.
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理A.1。
- en: According the convergence analysis in Adam (Kingma & Ba, [2014](#bib.bib18)),
    assume that the function $f_{t}$.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Adam（Kingma & Ba，[2014](#bib.bib18)）中的收敛分析，假设函数$f_{t}$。
- en: '|  | $\displaystyle\tilde{R}(T)\leq$ |  | (11) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{R}(T)\leq$ |  | (11) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Proof.
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: According the convergence analysis in Adam (Kingma & Ba, [2014](#bib.bib18)),
    assume that the function $f_{t}$.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Adam（Kingma & Ba，[2014](#bib.bib18)）中的收敛分析，假设函数$f_{t}$。
- en: '|  | $1$2 |  | (12) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (12) |'
- en: 'where $R(T)$ is the regret:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$R(T)$是遗憾：
- en: '|  | $R(T)=\sum_{t=1}^{T}[f_{t}(\theta_{t})-f_{t}(\theta^{*})]$ |  | (13) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(T)=\sum_{t=1}^{T}[f_{t}(\theta_{t})-f_{t}(\theta^{*})]$ |  | (13) |'
- en: 'This theorem could be obtained by the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定理可以通过以下方式获得：
- en: '|  | $\displaystyle R(T)\leq$ |  | (14) |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R(T)\leq$ |  | (14) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle+\sum_{i=1}^{d}\sum_{t=1}^{T}\frac{\beta_{1,t}}{2\alpha_{t}\left(1-\beta_{1,t}\right)}\left(\theta_{,i}^{*}-\theta_{t,i}\right)^{2}\sqrt{\widehat{v}_{t,i}}$
    |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\sum_{i=1}^{d}\sum_{t=1}^{T}\frac{\beta_{1,t}}{2\alpha_{t}\left(1-\beta_{1,t}\right)}\left(\theta_{,i}^{*}-\theta_{t,i}\right)^{2}\sqrt{\widehat{v}_{t,i}}$
    |  |'
- en: '|  | $\displaystyle R(T)\leq$ |  | (15) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R(T)\leq$ |  | (15) |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: In our method, we prune some variables for the momentum, i.e., a mask $\mathcal{M}_{o}$.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的方法中，我们修剪了一些动量变量，即一个掩码$\mathcal{M}_{o}$。
- en: 'From Eq. [14](#A1.E14 "Equation 14 ‣ Proof. ‣ Appendix A Convergence Analysis.
    ‣ ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking"),
    we have:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '从方程[14](#A1.E14 "Equation 14 ‣ Proof. ‣ Appendix A Convergence Analysis. ‣
    ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking")中，我们得到：'
- en: '|  | $\displaystyle\tilde{R}(T)\leq$ |  | (16) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{R}(T)\leq$ |  | (16) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle+\sum_{i=1}^{d}\sum_{t=1}^{T}\frac{\beta_{1,t}}{2\alpha_{t}\left(1-\beta_{1,t}\right)}\left(\theta_{,i}^{*}-\theta_{t,i}\right)^{2}\sqrt{\widehat{v}_{t,i}}$
    |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\sum_{i=1}^{d}\sum_{t=1}^{T}\frac{\beta_{1,t}}{2\alpha_{t}\left(1-\beta_{1,t}\right)}\left(\theta_{,i}^{*}-\theta_{t,i}\right)^{2}\sqrt{\widehat{v}_{t,i}}$
    |  |'
- en: 'Under similar assumption as Eq. [17](#A1.E17 "Equation 17 ‣ Proof. ‣ Appendix
    A Convergence Analysis. ‣ ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum
    Joint Shrinking"), we could have the following regret bound:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '在与方程[17](#A1.E17 "Equation 17 ‣ Proof. ‣ Appendix A Convergence Analysis. ‣
    ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking")类似的假设下，我们可以得到以下遗憾界限：'
- en: '|  | $\displaystyle\tilde{R}(T)\leq$ |  | (17) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{R}(T)\leq$ |  | (17) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: ∎
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Appendix B More Visualization of Compressed LLM.
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 压缩LLM的更多可视化。
- en: 'Here we show more visualization results in Figure [6](#A2.F6 "Figure 6 ‣ Appendix
    B More Visualization of Compressed LLM. ‣ ExCP: Extreme LLM Checkpoint Compression
    via Weight-Momentum Joint Shrinking") and Figure [7](#A2.F7 "Figure 7 ‣ Appendix
    B More Visualization of Compressed LLM. ‣ ExCP: Extreme LLM Checkpoint Compression
    via Weight-Momentum Joint Shrinking").'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '在图[6](#A2.F6 "Figure 6 ‣ Appendix B More Visualization of Compressed LLM. ‣
    ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking")和图[7](#A2.F7
    "Figure 7 ‣ Appendix B More Visualization of Compressed LLM. ‣ ExCP: Extreme LLM
    Checkpoint Compression via Weight-Momentum Joint Shrinking")中，我们展示了更多的可视化结果。'
- en: 'Question:
    What is the area of a circle with a radius of two meters? Original PanGu-$\pi$-1B:
    The seven continents are: 1\. Asia 2\. Africa 3\. North America 4\. South America
    5\. Antarctica 6\. Europe 7\. Australia'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 'Question:
    What is the area of a circle with a radius of two meters? Original PanGu-$\pi$-1B:
    The seven continents are: 1\. Asia 2\. Africa 3\. North America 4\. South America
    5\. Antarctica 6\. Europe 7\. Australia'
- en: 'Figure 6: Q&A example to show the difference between our compressed model and
    the original model.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：Q&A示例，展示我们的压缩模型与原始模型之间的差异。
- en: 'Question:
    How to find a suitable picnic spot? Original PanGu-$\pi$-1B: 1\. Understand your
    environment: Be sure to study your chosen location, the time and weather, and
    the type of subject you are photographing. 2\. Choose the right camera: Not all
    cameras are created equal, so take the time to research and choose the camera
    that best suits your needs. 3\. Get the right lighting: The lighting can dramatically
    affect the mood and tone of your photos. Try to capture the light sources that
    are most effective for your subject. 4\. Experiment with composition: Composition
    is an essential part of photography, so experiment with different angles, vantage
    points, and compositions to see what works best for your subject. 5\. Edit your
    photos: Once you have your compelling landscape photos, it’s time to edit them.
    You can use software to add filters, adjust brightness and contrast, and enhance
    the overall look of your photos. 6\. Share your photos: Take a camera and post
    your photos on social media platforms like Instagram, Facebook, and YouTube. This
    will allow others to view and appreciate your work, and also help you gain more
    followers.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 'Question:
    How to find a suitable picnic spot? Original PanGu-$\pi$-1B: 1\. Understand your
    environment: Be sure to study your chosen location, the time and weather, and
    the type of subject you are photographing. 2\. Choose the right camera: Not all
    cameras are created equal, so take the time to research and choose the camera
    that best suits your needs. 3\. Get the right lighting: The lighting can dramatically
    affect the mood and tone of your photos. Try to capture the light sources that
    are most effective for your subject. 4\. Experiment with composition: Composition
    is an essential part of photography, so experiment with different angles, vantage
    points, and compositions to see what works best for your subject. 5\. Edit your
    photos: Once you have your compelling landscape photos, it’s time to edit them.
    You can use software to add filters, adjust brightness and contrast, and enhance
    the overall look of your photos. 6\. Share your photos: Take a camera and post
    your photos on social media platforms like Instagram, Facebook, and YouTube. This
    will allow others to view and appreciate your work, and also help you gain more
    followers.'
- en: 'Figure 7: Q&A example to show the difference between our compressed model and
    the original model.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：Q&A示例，展示我们的压缩模型与原始模型之间的差异。
