- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:04:35'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:35
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level
    Pretrained Code LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次上下文剪枝：利用仓库级预训练代码大语言模型优化实际代码补全
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.18294](https://ar5iv.labs.arxiv.org/html/2406.18294)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.18294](https://ar5iv.labs.arxiv.org/html/2406.18294)
- en: Lei Zhang^(1,2)  Yunshui Li^(1,2)  Jiaming Li^(1,2)  Xiaobo Xia³  Jiaxi Yang^(1,2)
     Run Luo^(1,2)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 张磊^(1,2)  李云水^(1,2)  李佳明^(1,2)  夏博博³  杨佳希^(1,2)  罗润^(1,2)
- en: Minzheng Wang^(2,5)  Longze Chen^(1,2)  Junhao Liu⁴  Min Yang^(1,2)²²2Min Yang
    is the corresponding author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 王敏征^(2,5)  陈龙泽^(1,2)  刘俊浩⁴  杨敏^(1,2)²² 杨敏是通讯作者。
- en: ¹Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹中国科学院深圳先进技术研究院
- en: ²University of Chinese Academy of Sciences
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²中国科学院大学
- en: ³The University of Sydney  ⁴University of California, Irvine
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³悉尼大学  ⁴加州大学欧文分校
- en: ⁵MAIS, Institute of Automation, Chinese Academy of Sciences
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵中国科学院自动化研究所MAIS
- en: '{lei.zhang2, min.yang}@siat.ac.cn'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '{lei.zhang2, min.yang}@siat.ac.cn'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Some recently developed code large language models (Code LLMs) have been pretrained
    on repository-level code data (Repo-Code LLMs), enabling these models to recognize
    repository structures and utilize cross-file information for code completion.
    However, in real-world development scenarios, simply concatenating the entire
    code repository often exceeds the context window limits of these Repo-Code LLMs,
    leading to significant performance degradation. In this study, we conducted extensive
    preliminary experiments and analyses on six Repo-Code LLMs. The results indicate
    that maintaining the topological dependencies of files and increasing the code
    file content in the completion prompts can improve completion accuracy; pruning
    the specific implementations of functions in all dependent files does not significantly
    reduce the accuracy of completions. Based on these findings, we proposed a strategy
    named Hierarchical Context Pruning (HCP) to construct completion prompts with
    high informational code content. The HCP models the code repository at the function
    level, maintaining the topological dependencies between code files while removing
    a large amount of irrelevant code content, significantly reduces the input length
    for repository-level code completion. We applied the HCP strategy in experiments
    with six Repo-Code LLMs, and the results demonstrate that our proposed method
    can significantly enhance completion accuracy while substantially reducing the
    length of input. Our code and data are available at [https://github.com/Hambaobao/HCP-Coder](https://github.com/Hambaobao/HCP-Coder).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近开发的一些代码大语言模型（Code LLMs）已在仓库级代码数据上进行预训练（Repo-Code LLMs），使这些模型能够识别仓库结构并利用跨文件信息进行代码补全。然而，在实际开发场景中，仅仅连接整个代码仓库往往超出了这些Repo-Code
    LLMs的上下文窗口限制，导致显著的性能下降。在这项研究中，我们对六个Repo-Code LLMs进行了广泛的初步实验和分析。结果表明，保持文件的拓扑依赖关系并增加补全提示中的代码文件内容可以提高补全准确性；修剪所有依赖文件中函数的具体实现不会显著降低补全的准确性。基于这些发现，我们提出了一种名为层次上下文剪枝（HCP）的策略，以构建具有高信息量代码内容的补全提示。HCP在函数级建模代码仓库，保持代码文件之间的拓扑依赖关系，同时去除大量无关代码内容，显著减少了仓库级代码补全的输入长度。我们在六个Repo-Code
    LLMs的实验中应用了HCP策略，结果表明我们提出的方法可以显著提高补全准确性，同时大幅减少输入长度。我们的代码和数据可以在[https://github.com/Hambaobao/HCP-Coder](https://github.com/Hambaobao/HCP-Coder)获取。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Code completion tools based on code large language models (Chen et al., [2021](#bib.bib8);
    Nijkamp et al., [2023b](#bib.bib30); Li et al., [2023](#bib.bib21); Fried et al.,
    [2023](#bib.bib14); Allal et al., [2023](#bib.bib1)), such as *GitHub Copilot*¹¹1[https://github.com/features/copilot](https://github.com/features/copilot),
    have been widely adopted in daily development practices and have significantly
    enhanced the productivity of developers. As research (Bavarian et al., [2022](#bib.bib4);
    Sun et al., [2024](#bib.bib37)) on code large language models (Code LLMs) continues
    to evolve, some recently developed Code LLMs (Guo et al., [2024](#bib.bib15);
    Lozhkov et al., [2024](#bib.bib26); Team et al., [2024](#bib.bib38)) have been
    trained on repository-level code data (Repo-Code LLMs) to overcome the limitations
    of previous models trained on file-level data, which struggled to recognize repository
    structures and integrate code across multiple files for completion tasks. However,
    in real-world development scenarios, simply concatenating the entire code repository
    often exceeds the context window size of these Repo-Code LLMs, leading to significant
    performance degradation and increased inference latency. How to effectively utilize
    the capabilities of these Repo-Code LLMs to integrate cross-file information and
    construct high-quality completion prompts within the model’s context window limits
    remains an area for further exploration.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大规模语言模型的代码补全工具（Chen et al., [2021](#bib.bib8); Nijkamp et al., [2023b](#bib.bib30);
    Li et al., [2023](#bib.bib21); Fried et al., [2023](#bib.bib14); Allal et al.,
    [2023](#bib.bib1)），例如*GitHub Copilot*¹¹1[https://github.com/features/copilot](https://github.com/features/copilot)，已被广泛应用于日常开发实践，并显著提高了开发者的生产力。随着对大规模语言模型（Code
    LLMs）的研究（Bavarian et al., [2022](#bib.bib4); Sun et al., [2024](#bib.bib37)）的不断发展，一些新近开发的Code
    LLMs（Guo et al., [2024](#bib.bib15); Lozhkov et al., [2024](#bib.bib26); Team
    et al., [2024](#bib.bib38)）已经在仓库级代码数据（Repo-Code LLMs）上进行训练，以克服之前模型在文件级数据上训练的局限性，这些模型在识别仓库结构和整合跨多个文件的代码以进行补全任务时表现不佳。然而，在现实开发场景中，仅仅将整个代码仓库串联起来，往往会超过这些Repo-Code
    LLMs的上下文窗口大小，导致性能显著下降和推理延迟增加。如何有效利用这些Repo-Code LLMs的能力，在模型的上下文窗口限制内整合跨文件信息并构建高质量的补全提示，仍然是需要进一步探索的领域。
- en: 'In this study, we initially evaluated six Repo-Code LLMs on the CrossCodeEval
    (Ding et al., [2023](#bib.bib11)) benchmark and conducted a detailed analysis
    of completion errors (Appendix [A](#A1 "Appendix A Error Description and Analysis
    ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level
    Pretrained Code LLMs")). The errors identified were categorized into eight distinct
    classes (Section [4.2](#S4.SS2 "4.2 Completion Error Analysis ‣ 4 Preliminary
    Studies ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs")). Subsequently, considering the characteristics
    of the decoder architecture in Code LLMs, we analyzed the impact of topological
    dependencies among code files on completion accuracy (Section [4.3](#S4.SS3 "4.3
    Topological Dependency Analysis ‣ 4 Preliminary Studies ‣ Hierarchical Context
    Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs")). We found that maintaining the dependencies between code files and
    including more file information leads to higher accuracy. Additionally, we conducted
    experiments to analyze the impact of content from files at different dependency
    levels on completion accuracy (Section [4.4](#S4.SS4 "4.4 Cross-File Content Analysis
    ‣ 4 Preliminary Studies ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs")). We discovered that
    even pruning away the specific implementations of functions in all dependent files
    does not significantly reduce the accuracy of completions. Based on the results
    of these preliminary experiments, we proposed a strategy named Hierarchical Context
    Pruning (HCP) to construct high-quality completion prompts. The HCP models the
    code repository at the function level, retaining the topological dependencies
    between files while eliminating a large amount of irrelevant code content. In
    our experiments, the HCP successfully reduced the input from over 50,000 tokens
    to approximately 8,000 tokens, and significantly enhanced the accuracy of completions.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们最初在 CrossCodeEval (Ding et al., [2023](#bib.bib11)) 基准上评估了六个 Repo-Code
    LLM，并对完成错误进行了详细分析（附录 [A](#A1 "附录 A 错误描述与分析 ‣ 层次上下文修剪：优化基于代码库预训练的代码 LLM 进行实际代码完成")）。识别出的错误被分类为八个不同的类别（第
    [4.2](#S4.SS2 "4.2 完成错误分析 ‣ 4 初步研究 ‣ 层次上下文修剪：优化基于代码库预训练的代码 LLM 进行实际代码完成") 节）。随后，考虑到
    Code LLMs 中解码器架构的特点，我们分析了代码文件之间拓扑依赖对完成准确性的影响（第 [4.3](#S4.SS3 "4.3 拓扑依赖分析 ‣ 4 初步研究
    ‣ 层次上下文修剪：优化基于代码库预训练的代码 LLM 进行实际代码完成") 节）。我们发现，保持代码文件之间的依赖关系并包括更多文件信息会提高准确性。此外，我们进行了实验，分析了不同依赖级别的文件内容对完成准确性的影响（第
    [4.4](#S4.SS4 "4.4 跨文件内容分析 ‣ 4 初步研究 ‣ 层次上下文修剪：优化基于代码库预训练的代码 LLM 进行实际代码完成") 节）。我们发现，即使修剪掉所有依赖文件中函数的具体实现，也不会显著降低完成的准确性。基于这些初步实验的结果，我们提出了一种名为层次上下文修剪（HCP）的策略，用于构建高质量的完成提示。HCP
    在函数级别对代码库进行建模，保留文件之间的拓扑依赖，同时消除大量不相关的代码内容。在我们的实验中，HCP 成功地将输入从超过 50,000 个标记减少到约
    8,000 个标记，并显著提高了完成的准确性。
- en: 'In summary, our contributions are threefold:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献有三方面：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We conducted experiments on six Repo-Code LLMs and found that: maintaining
    the topological dependencies of files and increasing the content of code files
    in the completion prompts can enhance completion accuracy; pruning the specific
    implementations of functions in all dependent files does not significantly reduce
    the accuracy of completions.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对六个 Repo-Code LLM 进行了实验，发现：保持文件的拓扑依赖和增加完成提示中的代码文件内容可以提高完成准确性；修剪所有依赖文件中函数的具体实现不会显著降低完成的准确性。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Based on the results of preliminary experiments, we proposed a strategy named
    Hierarchical Context Pruning (HCP) for constructing high-quality completion prompts,
    which models the code repository at the function level, retaining the topological
    dependencies between files while eliminating a large amount of irrelevant code
    content.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于初步实验结果，我们提出了一种名为层次上下文修剪（HCP）的策略，用于构建高质量的完成提示，该策略在函数级别对代码库进行建模，保留文件之间的拓扑依赖，同时消除大量不相关的代码内容。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We applied the HCP strategy in experiments with six Repo-Code LLMs, and the
    results demonstrate that our proposed method can significantly enhance completion
    accuracy while substantially reducing the length of input.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在六个 Repo-Code LLMs 的实验中应用了 HCP 策略，结果表明，我们提出的方法可以显著提高完成准确性，同时大幅减少输入长度。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Code Large Language Models
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 代码大语言模型
- en: 2.1.1 Infilling Code LLMs
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 填充代码 LLMs
- en: Infilling scenarios constitute the majority of code completion tasks in the
    real world. Bavarian et al. ([2022](#bib.bib4)) demonstrates that pre-training
    Code LLMs with a certain proportion of fill-in-the-middle format code data can
    enable the Code LLMs to fill in middle code based on the surrounding context,
    without compromising their original left-to-right generation performance. Based
    on the findings of Bavarian et al. ([2022](#bib.bib4)), many subsequent Code LLMs
    (Fried et al., [2023](#bib.bib14); Allal et al., [2023](#bib.bib1); Nijkamp et al.,
    [2023a](#bib.bib29); Li et al., [2023](#bib.bib21); Rozière et al., [2024](#bib.bib34);
    Guo et al., [2024](#bib.bib15); Pinnaparaju et al., [2024](#bib.bib33); Lozhkov
    et al., [2024](#bib.bib26)) have emerged with the capability to perform infilling.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 填充场景构成了现实世界中大多数代码完成任务。Bavarian 等人（[2022](#bib.bib4)）证明，通过对 Code LLMs 进行一定比例的填充中间代码数据的预训练，可以使
    Code LLMs 基于周围上下文填充中间代码，而不会影响其原有的从左到右生成性能。基于 Bavarian 等人（[2022](#bib.bib4)）的发现，许多后续的
    Code LLMs（Fried 等人，[2023](#bib.bib14)；Allal 等人，[2023](#bib.bib1)；Nijkamp 等人，[2023a](#bib.bib29)；Li
    等人，[2023](#bib.bib21)；Rozière 等人，[2024](#bib.bib34)；Guo 等人，[2024](#bib.bib15)；Pinnaparaju
    等人，[2024](#bib.bib33)；Lozhkov 等人，[2024](#bib.bib26)）已经具备了填充功能。
- en: 2.1.2 Instruction Code LLMs
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 指令代码 LLMs
- en: Pretrained Code LLMs are traditionally used only for continuation tasks such
    as code completion. Inspired by works on instruction tuning large language models
    (Ouyang et al., [2022](#bib.bib31); Li et al., [2024b](#bib.bib23)), many studies
    (Wang et al., [2023a](#bib.bib42); Luo et al., [2023](#bib.bib27); Muennighoff
    et al., [2024](#bib.bib28); Xu et al., [2023](#bib.bib45); Wang et al., [2024b](#bib.bib41);
    Zheng et al., [2024](#bib.bib54)) have attempted to finetune Code LLMs using code
    instruction data. This finetuning unlocks the potential of Code LLMs, enabling
    them to perform more complex coding tasks based on user instructions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的 Code LLMs 传统上仅用于继续任务，如代码完成。受到指令调优大型语言模型（Ouyang 等人，[2022](#bib.bib31)；Li
    等人，[2024b](#bib.bib23)）工作的启发，许多研究（Wang 等人，[2023a](#bib.bib42)；Luo 等人，[2023](#bib.bib27)；Muennighoff
    等人，[2024](#bib.bib28)；Xu 等人，[2023](#bib.bib45)；Wang 等人，[2024b](#bib.bib41)；Zheng
    等人，[2024](#bib.bib54)）尝试使用代码指令数据对 Code LLMs 进行微调。这种微调解锁了 Code LLMs 的潜力，使其能够根据用户指令执行更复杂的编码任务。
- en: '![Refer to caption](img/9031b176d3e2b81b24e9fb01c98607a2.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9031b176d3e2b81b24e9fb01c98607a2.png)'
- en: 'Figure 1: The error class distribution of the completion results of the DeepseekCoder,
    Starcoder2 and CodeGemma models on the CrossCodeEval: Python benchmark.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1：DeepseekCoder、Starcoder2 和 CodeGemma 模型在 CrossCodeEval: Python 基准上的完成结果错误分类分布。'
- en: 2.2 Code Benchmarks
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 代码基准
- en: 2.2.1 Code Completion Benchmarks
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 代码完成基准
- en: HumanEval (Chen et al., [2021](#bib.bib8)) consists of 164 manually crafted
    Python code problems, with an average of 7.7 tests each test case. MBPP (Austin
    et al., [2021](#bib.bib2)) is designed for individuals with entry-level programming
    skills. It comprises 974 concise Python functions, each with an accompanying description
    in English, a specified function signature, and three manually crafted test cases
    for verification. MultiPL-E (Cassano et al., [2022](#bib.bib6)) introduces itself
    as a novel benchmarking framework designed for multilingual contexts, building
    upon HumanEval (Chen et al., [2021](#bib.bib8)) and MBPP (Austin et al., [2021](#bib.bib2)).
    APPS (Hendrycks et al., [2021](#bib.bib16)) is a benchmark including 10K less-restricted
    problems for code generation. CodeContests (Li et al., [2022](#bib.bib22)) is
    a dataset specifically for competitive programming problems.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: HumanEval（Chen 等人，[2021](#bib.bib8)）包含 164 个手工制作的 Python 代码问题，每个测试用例平均有 7.7
    个测试。MBPP（Austin 等人，[2021](#bib.bib2)）为初级编程技能者设计，包含 974 个简洁的 Python 函数，每个函数附有英文描述、指定的函数签名和三个手工制作的测试用例用于验证。MultiPL-E（Cassano
    等人，[2022](#bib.bib6)）作为一种新型基准框架，旨在多语言环境下，基于 HumanEval（Chen 等人，[2021](#bib.bib8)）和
    MBPP（Austin 等人，[2021](#bib.bib2)）构建。APPS（Hendrycks 等人，[2021](#bib.bib16)）是一个包含
    10K 个较少限制的代码生成问题的基准。CodeContests（Li 等人，[2022](#bib.bib22)）是专门针对竞赛编程问题的数据集。
- en: 2.2.2 Infilling Code Benchmarks
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 填充代码基准
- en: Fried et al. ([2023](#bib.bib14)) constructed single-line and multi-line infilling
    completion tasks based on HumanEval, and Bavarian et al. ([2022](#bib.bib4)) expanded
    upon it to create randomspan infilling completion tasks, ultimately resulting
    in the current HumanEval-Infilling benchmark. Allal et al. ([2023](#bib.bib1))
    created an Infilling benchmark that includes languages from Java, JavaScript,
    and Python 3, utilizing a line exactly match method for evaluation. Lai et al.
    ([2022](#bib.bib19)) presents a benchmark for evaluating the performance of Code
    LLMs in completing tasks related to Python scientific computing libraries, encompassing
    both regular completion and insertion (infilling) tasks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Fried 等人 ([2023](#bib.bib14)) 基于 HumanEval 构建了单行和多行填充完成任务，Bavarian 等人 ([2022](#bib.bib4))
    在此基础上扩展，创建了 randomspan 填充完成任务，最终形成了当前的 HumanEval-Infilling 基准。Allal 等人 ([2023](#bib.bib1))
    创建了一个包括 Java、JavaScript 和 Python 3 语言的填充基准，采用完全匹配的方法进行评估。Lai 等人 ([2022](#bib.bib19))
    提出了一个基准，用于评估 Code LLMs 在完成与 Python 科学计算库相关的任务中的表现，包括常规完成和插入（填充）任务。
- en: 2.3 Repo-level Code Completion
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 库级代码完成
- en: Some benchmarks for repository-level code completion have been proposed to evaluate
    the performance of code models in real-world completion tasks, such as CrossCodeEval
    (Ding et al., [2023](#bib.bib11)), Repo-Bench (Liu et al., [2023](#bib.bib25)),
    CoderEval (Zhang et al., [2024b](#bib.bib53)), and EvoCodeBench (Li et al., [2024a](#bib.bib20)).
    A lot of studies (Shrivastava et al., [2023](#bib.bib36); Zhang et al., [2023a](#bib.bib50);
    Bi et al., [2024](#bib.bib5); Phan et al., [2024](#bib.bib32); Liang et al., [2024](#bib.bib24))
    have focused on improving the accuracy of repository-level code completion tasks.
    However, most of these studies overlook the unique aspects of their Fill-in-the-Middle
    (FIM) capacities. Furthermore, despite the recent development of repository-level
    pretrained Code LLMs designed to process large-scale repository data, research
    on these models remains relatively limited.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了一些用于评估代码模型在实际完成任务中的性能的库级代码完成基准，如 CrossCodeEval (Ding 等人, [2023](#bib.bib11))、Repo-Bench
    (Liu 等人, [2023](#bib.bib25))、CoderEval (Zhang 等人, [2024b](#bib.bib53)) 和 EvoCodeBench
    (Li 等人, [2024a](#bib.bib20))。许多研究 (Shrivastava 等人, [2023](#bib.bib36)；Zhang 等人,
    [2023a](#bib.bib50)；Bi 等人, [2024](#bib.bib5)；Phan 等人, [2024](#bib.bib32)；Liang
    等人, [2024](#bib.bib24)) 专注于提高库级代码完成任务的准确性。然而，这些研究中的大多数忽视了其 Fill-in-the-Middle
    (FIM) 能力的独特方面。此外，尽管最近开发了用于处理大规模库数据的预训练 Code LLMs，但对这些模型的研究仍然相对有限。
- en: 3 Experiments Setup
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置
- en: 3.1 Dataset & Evaluation Metrics
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集与评估指标
- en: To assess the code completion performance of Code LLMs in real development scenarios,
    we utilized CrossCodeEval (Ding et al., [2023](#bib.bib11)) as the evaluation
    dataset. The CrossCodeEval (Ding et al., [2023](#bib.bib11)) benchmark provides
    test cases that require the use of cross-file code information for completion.
    Without loss of generality, in this study, we have chosen Python language as the
    primary language for our research.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 Code LLMs 在真实开发场景中的代码完成性能，我们使用了 CrossCodeEval (Ding 等人, [2023](#bib.bib11))
    作为评估数据集。CrossCodeEval (Ding 等人, [2023](#bib.bib11)) 基准提供了需要使用跨文件代码信息进行完成的测试用例。在本研究中，我们选择了
    Python 语言作为主要研究语言，以便于不失一般性地进行分析。
- en: 'We used the original data from CrossCodeEval, retaining the original repository
    structure. For each test case, we first identified the file for completion and
    the cursor’s position (the line and column where the completion occurs). We then
    removed the code after the cursor in that line to form authentic completion test
    cases. Ultimately, we obtained 2,655 real-world completion tests. Following the
    CrossCodeEval evaluation protocol, we evaluated the completion results using two
    metrics: *Exact Match* (EM) and *Edit Similarity* (ES).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 CrossCodeEval 的原始数据，保留了原始库结构。对于每个测试用例，我们首先确定了完成的文件及光标的位置（完成发生的行和列）。然后，我们去除了该行中光标后的代码，以形成真实的完成测试用例。最终，我们获得了
    2,655 个真实世界的完成测试。按照 CrossCodeEval 评估协议，我们使用两个指标来评估完成结果：*Exact Match* (EM) 和 *Edit
    Similarity* (ES)。
- en: 3.2 Models & Prompt Templates
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 模型与提示模板
- en: 'The code large language models pretrained with repository-level code data include
    specific tokens used to describe the repository structure in the prompt. Table
    [6](#A2.T6 "Table 6 ‣ Appendix B Special Tokens & Prompt Templates ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") in appendix displays the special tokens used by DeepseekCoder, Starcoder2
    and CodeGemma. The specific prompt templates used by DeepseekCoder, Starcoder2
    and CodeGemma are shown in Table [7](#A2.T7 "Table 7 ‣ Appendix B Special Tokens
    & Prompt Templates ‣ Hierarchical Context Pruning: Optimizing Real-World Code
    Completion with Repository-Level Pretrained Code LLMs").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的代码大语言模型使用了存储库级代码数据，包括在提示中用于描述存储库结构的特定标记。附录中的表[6](#A2.T6 "表6 ‣ 附录B 特殊标记与提示模板
    ‣ 分层上下文修剪：优化真实世界代码完成与存储库级预训练代码LLMs")显示了DeepseekCoder、Starcoder2和CodeGemma使用的特殊标记。DeepseekCoder、Starcoder2和CodeGemma使用的特定提示模板显示在表[7](#A2.T7
    "表7 ‣ 附录B 特殊标记与提示模板 ‣ 分层上下文修剪：优化真实世界代码完成与存储库级预训练代码LLMs")中。
- en: 3.3 Hardware & Hyperparameters
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 硬件与超参数
- en: All the expiriments were conducted on NVIDIA A100 GPUs. We employ greedy decoding
    strategy for all the models, and set max_new_tokens to $32$, respectively. All
    the prompts longer than the model_max_length are truncated from the left.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验都在NVIDIA A100 GPU上进行。我们对所有模型采用贪婪解码策略，并将max_new_tokens设置为$32$。所有超过model_max_length的提示都从左侧截断。
- en: '| XF-Context | Baseline Evaluation |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| XF-Context | 基准评估 |'
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
- en: '| Infile-Only | 16.72 | 56.58 | 28.14 | 68.36 | 21.92 | 61.49 | 22.98 | 63.58
    | 20.64 | 56.26 | 30.58 | 70.36 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 仅文件内 | 16.72 | 56.58 | 28.14 | 68.36 | 21.92 | 61.49 | 22.98 | 63.58 | 20.64
    | 56.26 | 30.58 | 70.36 |'
- en: '| RAG-BM25 | 17.28 | 58.18 | 32.65 | 71.78 | 24.45 | 63.84 | 26.26 | 65.32
    | 22.89 | 57.73 | 32.89 | 70.81 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| RAG-BM25 | 17.28 | 58.18 | 32.65 | 71.78 | 24.45 | 63.84 | 26.26 | 65.32
    | 22.89 | 57.73 | 32.89 | 70.81 |'
- en: '| Random-All | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 随机-所有 | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09 | 26.93
    | 62.13 | 36.69 | 74.42 |'
- en: 'Table 1: The completion results of the baseline methods. EM denotes Exact Match,
    and ES denotes Edit Similarity.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：基准方法的完成结果。EM表示精确匹配，ES表示编辑相似度。
- en: '| XF-Context | Topoligical Dependency Analysis |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| XF-Context | 拓扑依赖分析 |'
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
- en: '| D-Level: 1 | 15.44 | 55.03 | 33.03 | 70.77 | 26.18 | 64.15 | 28.51 | 66.91
    | 24.37 | 58.79 | 34.65 | 73.01 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 依赖级别：1 | 15.44 | 55.03 | 33.03 | 70.77 | 26.18 | 64.15 | 28.51 | 66.91 |
    24.37 | 58.79 | 34.65 | 73.01 |'
- en: '| D-Level: 2 | 13.63 | 53.45 | 33.56 | 70.74 | 26.70 | 64.58 | 29.45 | 67.03
    | 25.31 | 59.27 | 35.67 | 73.26 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 依赖级别：2 | 13.63 | 53.45 | 33.56 | 70.74 | 26.70 | 64.58 | 29.45 | 67.03 |
    25.31 | 59.27 | 35.67 | 73.26 |'
- en: '| D-Level: 3 | 13.26 | 53.17 | 33.07 | 70.51 | 26.82 | 64.56 | 29.23 | 67.01
    | 25.35 | 59.30 | 35.93 | 73.34 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 依赖级别：3 | 13.26 | 53.17 | 33.07 | 70.51 | 26.82 | 64.56 | 29.23 | 67.01 |
    25.35 | 59.30 | 35.93 | 73.34 |'
- en: '| D-Level: 4 | 13.37 | 53.20 | 33.22 | 70.57 | 26.59 | 64.46 | 29.53 | 67.07
    | 25.54 | 59.42 | 36.12 | 73.54 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 依赖级别：4 | 13.37 | 53.20 | 33.22 | 70.57 | 26.59 | 64.46 | 29.53 | 67.07 |
    25.54 | 59.42 | 36.12 | 73.54 |'
- en: '| D-Level: $\infty$ | 5.76 | 46.22 | 35.29 | 71.51 | 30.43 | 67.34 | 33.03
    | 69.57 | 29.08 | 62.91 | 39.32 | 75.35 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 依赖级别：$\infty$ | 5.76 | 46.22 | 35.29 | 71.51 | 30.43 | 67.34 | 33.03 | 69.57
    | 29.08 | 62.91 | 39.32 | 75.35 |'
- en: 'Table 2: Comparison of completion results using different context dependency
    levels across 6 models. All the prompts is truncated to the max context window
    of the Code LLMs from the left. *$\infty$* denotes the prompt including all files
    in the repository.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：使用不同上下文依赖级别对6种模型的完成结果进行比较。所有提示都被截断到Code LLMs的最大上下文窗口的左侧。*$\infty$*表示提示包括了存储库中的所有文件。
- en: 4 Preliminary Studies
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 初步研究
- en: 4.1 Baseline Evaluation
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基准评估
- en: 4.1.1 Infile Only
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 仅文件内
- en: 'We initially evaluated the model’s completion ability using only information
    from the current file, with results presented in Table [1](#S3.T1 "Table 1 ‣ 3.3
    Hardware & Hyperparameters ‣ 3 Experiments Setup ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs")
    under the Infile-Only row. The completion results are less than satisfactory.
    Even the best-performing model achieved an accuracy of only about 30%.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '我们最初仅使用当前文件的信息来评估模型的完成能力，结果见表 [1](#S3.T1 "Table 1 ‣ 3.3 Hardware & Hyperparameters
    ‣ 3 Experiments Setup ‣ Hierarchical Context Pruning: Optimizing Real-World Code
    Completion with Repository-Level Pretrained Code LLMs") 中的 Infile-Only 行。完成结果不尽如人意。即使是表现最佳的模型，准确率也仅约为
    30%。'
- en: '![Refer to caption](img/b53ed69fcfe81a58572b6acd4667017c.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b53ed69fcfe81a58572b6acd4667017c.png)'
- en: 'Figure 2: The distribution of tokenized prompt lengths in the CrossCodeEval
    benchmark. The x-aixs represents the dependent level, and the y-axis represents
    the number of tokens.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: CrossCodeEval 基准测试中标记化提示长度的分布。x 轴表示依赖级别，y 轴表示令牌数量。'
- en: 4.1.2 RAG-BM25
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 RAG-BM25
- en: 'We subsequently evaluated the effect of using Retrieval Augmented Generation
    (RAG) method to retrieve relevant code snippets to assist with completion. Following
    the setup of CrossCodeEval, we chunk the repository code into units of 10 lines,
    and use BM25 as similarity metric for retrieving relevant code snippets. We select
    the top-5 relevant snippets as cross-file information, which are placed at the
    beginning of the prompt to assist with code generation. The results are shown
    in Table [1](#S3.T1 "Table 1 ‣ 3.3 Hardware & Hyperparameters ‣ 3 Experiments
    Setup ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with
    Repository-Level Pretrained Code LLMs") under the RAG-BM25 row.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '我们随后评估了使用检索增强生成（RAG）方法来检索相关代码片段以辅助完成的效果。根据 CrossCodeEval 的设置，我们将代码库代码分块为 10
    行，并使用 BM25 作为相似性度量来检索相关代码片段。我们选择前 5 个相关片段作为跨文件信息，这些片段放在提示的开头以辅助代码生成。结果显示在表 [1](#S3.T1
    "Table 1 ‣ 3.3 Hardware & Hyperparameters ‣ 3 Experiments Setup ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") 的 RAG-BM25 行下。'
- en: '| XF-Context | Cross-File Content Analysis |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| XF-Context | 跨文件内容分析 |'
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
- en: '| P-Level: 0 | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| P-Level: 0 | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
- en: '| P-Level: 1 | 6.55 | 46.58 | 36.20 | 71.90 | 30.73 | 67.97 | 34.43 | 70.65
    | 29.30 | 63.46 | 39.55 | 75.70 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| P-Level: 1 | 6.55 | 46.58 | 36.20 | 71.90 | 30.73 | 67.97 | 34.43 | 70.65
    | 29.30 | 63.46 | 39.55 | 75.70 |'
- en: '| P-Level: 2 | 9.83 | 49.63 | 34.73 | 70.89 | 30.02 | 66.41 | 31.26 | 68.24
    | 27.34 | 61.13 | 38.31 | 74.32 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| P-Level: 2 | 9.83 | 49.63 | 34.73 | 70.89 | 30.02 | 66.41 | 31.26 | 68.24
    | 27.34 | 61.13 | 38.31 | 74.32 |'
- en: '| + D-level:1 | 9.45 | 49.44 | 36.87 | 72.14 | 29.91 | 66.96 | 32.62 | 69.11
    | 28.93 | 62.03 | 39.17 | 75.16 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| + D-level:1 | 9.45 | 49.44 | 36.87 | 72.14 | 29.91 | 66.96 | 32.62 | 69.11
    | 28.93 | 62.03 | 39.17 | 75.16 |'
- en: '| + D-level:2 | 8.70 | 48.61 | 36.38 | 71.66 | 29.64 | 66.99 | 32.96 | 69.13
    | 28.44 | 61.76 | 39.06 | 74.91 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| + D-level:2 | 8.70 | 48.61 | 36.38 | 71.66 | 29.64 | 66.99 | 32.96 | 69.13
    | 28.44 | 61.76 | 39.06 | 74.91 |'
- en: 'Table 3: The results of completion using cross-file information with different
    pruning levels. *+ D-level:x* denotes the model uses the cross-file information
    with dependency level x.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 使用不同剪枝级别的跨文件信息完成结果。*+ D-level:x* 表示模型使用了依赖级别为 x 的跨文件信息。'
- en: 4.1.3 Randomly Concatenating All Files
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 随机拼接所有文件
- en: 'Additionally, we concatenated all repository code files randomly according
    to the pre-trained formats of various Repo-Code LLMs to create completion prompts,
    which were then input into the models for completion. The evaluation results are
    shown in Table [1](#S3.T1 "Table 1 ‣ 3.3 Hardware & Hyperparameters ‣ 3 Experiments
    Setup ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with
    Repository-Level Pretrained Code LLMs") under the Random-All row. We observed
    that supplying the model with more information from the repository’s code led
    to superior performance compared to RAG. However, the input length of the model
    is limited by its context window, thereby transforming this scenario into a constrained
    optimization problem. The constrained optimization goal is expressed as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们根据各种 Repo-Code LLMs 的预训练格式，随机连接所有仓库代码文件以创建补全提示，然后将其输入模型进行补全。评估结果显示在表 [1](#S3.T1
    "Table 1 ‣ 3.3 Hardware & Hyperparameters ‣ 3 Experiments Setup ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") 的 Random-All 行中。我们观察到，向模型提供更多来自仓库代码的信息，相比 RAG，能够提升性能。然而，模型的输入长度受到其上下文窗口的限制，从而将这一场景转化为一个受限优化问题。受限优化目标如下所示：'
- en: '|  |  | $\displaystyle\max_{\mathcal{P}}\text{Quality}(\mathcal{P})\quad\text{s.t.}\quad\text{Length}(\mathcal{P})\leq
    L$ |  | (1) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\max_{\mathcal{P}}\text{Quality}(\mathcal{P})\quad\text{s.t.}\quad\text{Length}(\mathcal{P})\leq
    L$ |  | (1) |'
- en: where $\mathcal{P}$ represents context window size of the model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{P}$ 代表模型的上下文窗口大小。
- en: 4.2 Completion Error Analysis
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 补全错误分析
- en: 'To further investigate the issues of repository-level pre-trained Code LLMs
    in real-world completion tasks, we sampled 200 error examples from each model’s
    *Random-All* evaluation results for error analysis. Ultimately, we categorized
    the issues present in these models into eight classes: *Parameter Value Error*,
    *Non-existent Method Call*, *Improper Method Invocation*, *Missing Method Invocation*,
    *Redundant Content Generation*, *Partial Content Missing*, *Incorrect Content
    Generation*, and *Exact Match Error*. Figure [1](#S2.F1 "Figure 1 ‣ 2.1.2 Instruction
    Code LLMs ‣ 2.1 Code Large Language Models ‣ 2 Related Work ‣ Hierarchical Context
    Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") shows the error distribution statistics for six Repo-Code LLMs. In
    the appendix [A](#A1 "Appendix A Error Description and Analysis ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs"), we provide examples of each type of error along with corresponding
    error analysis.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步探讨仓库级预训练 Code LLMs 在实际补全任务中的问题，我们从每个模型的 *Random-All* 评估结果中抽取了 200 个错误示例进行错误分析。最终，我们将这些模型中的问题分类为八类：*参数值错误*，*不存在的方法调用*，*不当的方法调用*，*缺失的方法调用*，*冗余内容生成*，*部分内容缺失*，*不正确的内容生成*，以及*精确匹配错误*。图
    [1](#S2.F1 "Figure 1 ‣ 2.1.2 Instruction Code LLMs ‣ 2.1 Code Large Language Models
    ‣ 2 Related Work ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") 显示了六个 Repo-Code LLMs 的错误分布统计。在附录
    [A](#A1 "Appendix A Error Description and Analysis ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs")
    中，我们提供了每种错误类型的示例以及相应的错误分析。'
- en: '![Refer to caption](img/d798e966f40b8e99cbe6fce546731ce7.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d798e966f40b8e99cbe6fce546731ce7.png)'
- en: 'Figure 3: The framework of hierarchical context pruning for improving the performance
    of code large language models in real-world code completion tasks.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：改进代码大型语言模型在实际代码补全任务中的性能的层次上下文剪枝框架。
- en: 4.3 Topological Dependency Analysis
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 拓扑依赖分析
- en: Definition 1.
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义 1。
- en: '(Dependency Level) Let $F$ represent a specific file. We define the dependency
    levels as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: （依赖级别）令 $F$ 代表一个特定的文件。我们定义依赖级别如下：
- en: '|  |  | $\displaystyle I(f)=\{g\mid g\text{ is imported by }f\}$ |  | (2) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle I(f)=\{g\mid g\text{ is imported by }f\}$ |  | (2) |'
- en: '|  |  | $\displaystyle D_{0}(f)=\{f\}$ |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle D_{0}(f)=\{f\}$ |  |'
- en: '|  |  | $\displaystyle D_{i+1}(f)=D_{i}(f)\cup I(D_{i}(f))$ |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle D_{i+1}(f)=D_{i}(f)\cup I(D_{i}(f))$ |  |'
- en: 'We first identified the file requiring completion, then extracted all the import
    statements from the file with *Tree-Sitter*²²2[https://tree-sitter.github.io/tree-sitter](https://tree-sitter.github.io/tree-sitter),
    and used a breadth-first search (BFS) method to progressively add dependent files.
    Algorithm [1](#alg1 "Algorithm 1 ‣ Appendix F Dependency Search Algorithm ‣ D.2
    Hit Count Changes ‣ D.1 Complete Experimental Results ‣ Appendix D Dependency
    Level Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") in appendix shows our specific dependency
    modeling process.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先识别出需要补全的文件，然后使用*Tree-Sitter*²²2[https://tree-sitter.github.io/tree-sitter](https://tree-sitter.github.io/tree-sitter)提取文件中的所有导入语句，并使用广度优先搜索（BFS）方法逐步添加依赖文件。附录中的算法[1](#alg1
    "Algorithm 1 ‣ Appendix F Dependency Search Algorithm ‣ D.2 Hit Count Changes
    ‣ D.1 Complete Experimental Results ‣ Appendix D Dependency Level Analysis ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs")展示了我们具体的依赖建模过程。'
- en: 'Figure [13](#A3.F13 "Figure 13 ‣ Appendix C Prompt Length Distribution ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") illustrates the growth in the number of dependent files (calculated
    by the length of the tokenized prompt) as the number of dependency layers increases.
    We used median and average as statistical measures and found that in the vast
    majority of cases, the number of dependent files for a single file increases slowly
    after reaching four layers of dependencies. This suggests that using four layers
    of dependencies is sufficient to cover most scenarios. We further define:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [13](#A3.F13 "Figure 13 ‣ Appendix C Prompt Length Distribution ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") 展示了依赖层数增加时，依赖文件数量的增长（通过分词提示的长度计算）。我们使用了中位数和平均数作为统计度量，发现绝大多数情况下，在达到四层依赖后，单个文件的依赖文件数量增长缓慢。这表明，使用四层依赖足以覆盖大多数场景。我们进一步定义：'
- en: '|  |  | $\displaystyle D_{\infty}(f)=D_{4}(f)\cup\{F\setminus D_{4}(f)\}$ |  |
    (3) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle D_{\infty}(f)=D_{4}(f)\cup\{F\setminus D_{4}(f)\}$ |  |
    (3) |'
- en: to represent the prompt including all files in the repository.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 用于表示包括存储库中所有文件的提示。
- en: 'In Table [2](#S3.T2 "Table 2 ‣ 3.3 Hardware & Hyperparameters ‣ 3 Experiments
    Setup ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with
    Repository-Level Pretrained Code LLMs"), the D-level rows show the results of
    completion using cross-file information with different dependency levels. The
    results indicate that although the maximum dependency depth of most files reaches
    4 levels, only the information provided by $D_{1}(f)$ files, there are many other
    useful files within the repository.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [2](#S3.T2 "Table 2 ‣ 3.3 Hardware & Hyperparameters ‣ 3 Experiments Setup
    ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level
    Pretrained Code LLMs") 中，D-level 行展示了使用不同依赖级别的跨文件信息进行补全的结果。结果表明，尽管大多数文件的最大依赖深度达到
    4 层，但仅依赖于 $D_{1}(f)$ 文件提供的信息，存储库中仍有许多其他有用的文件。'
- en: 4.4 Cross-File Content Analysis
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 跨文件内容分析
- en: Definition 2.
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义 2。
- en: '(Pruning Level) We define the pruning levels into three categories:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: （修剪级别）我们将修剪级别定义为三个类别：
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'P-Level 0: No pruning is applied to the file content.'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'P-Level 0: 文件内容未进行任何修剪。'
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'P-Level 1: All global context content is removed from the file.'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'P-Level 1: 文件中的所有全局上下文内容被移除。'
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'P-Level 2: All global context content, function bodies and class method bodies
    are removed from the file.'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'P-Level 2: 文件中的所有全局上下文内容、函数体和类方法体被移除。'
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.1.2 RAG-BM25 ‣ 4.1 Baseline Evaluation ‣ 4 Preliminary
    Studies ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") presents the results of completion
    using cross-file information with different pruning levels. We can see that the
    results of *P-level:1* outperform those of *P-level:0*, indicating that the Global
    Context information from cross-file content has minimal impact on the completion
    of the current file. Additionally, the results of *P-level:2* are only slightly
    worse than those of $D_{\infty}(f)$. This suggests that the specific implementations
    of most cross-file functions have minimal impact on the completion of the current
    file, and retaining only the function header information is sufficient.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表[3](#S4.T3 "表 3 ‣ 4.1.2 RAG-BM25 ‣ 4.1 基线评估 ‣ 4 初步研究 ‣ 层次上下文剪枝：通过仓库级预训练代码 LLMs
    优化真实世界的代码完成")展示了使用不同剪枝级别的跨文件信息完成的结果。我们可以看到，*P-level:1*的结果优于*P-level:0*，这表明跨文件内容的全局上下文信息对当前文件的完成影响较小。此外，*P-level:2*的结果仅比$D_{\infty}(f)$稍差。这表明，大多数跨文件函数的具体实现对当前文件的完成影响较小，仅保留函数头信息已足够。
- en: 5 Hierarchical Context Pruning
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 层次上下文剪枝
- en: 'Based on the analysis results concerning the dependencies and content of the
    files, we attempt to construct a hierarchical context prompt based on the importance
    and relevance of the repository content. This approach aims to enhance the accuracy
    of code completion models while effectively reducing the length of the context.
    Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Completion Error Analysis ‣ 4 Preliminary Studies
    ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level
    Pretrained Code LLMs") shows the specific process for constructing a hierarchical
    context prompt.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对文件之间依赖关系和内容的分析结果，我们尝试根据仓库内容的重要性和相关性构建层次上下文提示。此方法旨在提高代码完成模型的准确性，同时有效减少上下文的长度。图[3](#S4.F3
    "图 3 ‣ 4.2 完成错误分析 ‣ 4 初步研究 ‣ 层次上下文剪枝：通过仓库级预训练代码 LLMs 优化真实世界的代码完成")展示了构建层次上下文提示的具体过程。
- en: 5.1 Fine-grained Repository Modeling
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 细粒度仓库建模
- en: 'In order to precisely control the content within the code repository, we employ
    *Tree-Sitter* to parse the files within the repository. We model the content using
    three types of nodes:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了精确控制代码仓库中的内容，我们使用*Tree-Sitter*来解析仓库中的文件。我们使用三种类型的节点来建模这些内容：
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Function Node: Represents a function or a class method within a code file.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Function Node: 表示代码文件中的一个函数或类方法。'
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Class Node: Represents a class in a code file, consisting of the class’s name,
    attributes, and Function Nodes.'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Class Node: 表示代码文件中的一个类，包括类的名称、属性和函数节点。'
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'File Node: Represents a code file, comprising Nodes that represent the functions
    and classes within the file, along with global context information.'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'File Node: 表示一个代码文件，包括表示文件中函数和类的节点，以及全局上下文信息。'
- en: 5.2 Hierarchical Context
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 层次上下文
- en: 'As shown in the top right of Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Completion Error
    Analysis ‣ 4 Preliminary Studies ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs"), following the settings
    in Section [4.3](#S4.SS3 "4.3 Topological Dependency Analysis ‣ 4 Preliminary
    Studies ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs"), we conduct a dependency analysis
    on the files in the repository. We perform a topological sort based on the dependency
    relationships, centering around the file currently being completed. According
    to the experimental results in Section [4.3](#S4.SS3 "4.3 Topological Dependency
    Analysis ‣ 4 Preliminary Studies ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs"), only files at dependency
    level 1 significantly enhance completion accuracy. Therefore, we select files
    designated as $D_{1}(f)$ to serve as dependency files. Ultimately, the files in
    the repository are categorized into three types: *current file*, *dependency files*,
    and *other files*. We will apply different strategies to optimize each type of
    file.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [3](#S4.F3 "图 3 ‣ 4.2 完成错误分析 ‣ 4 初步研究 ‣ 层次上下文剪枝：优化实际代码完成与仓库级预训练代码 LLM") 的右上角所示，根据第
    [4.3](#S4.SS3 "4.3 拓扑依赖分析 ‣ 4 初步研究 ‣ 层次上下文剪枝：优化实际代码完成与仓库级预训练代码 LLM") 节的设置，我们对仓库中的文件进行了依赖分析。我们根据依赖关系进行拓扑排序，围绕当前正在完成的文件进行。根据第
    [4.3](#S4.SS3 "4.3 拓扑依赖分析 ‣ 4 初步研究 ‣ 层次上下文剪枝：优化实际代码完成与仓库级预训练代码 LLM") 节的实验结果，只有依赖级别为
    1 的文件显著提高了完成准确度。因此，我们选择指定为 $D_{1}(f)$ 的文件作为依赖文件。最终，仓库中的文件分为三类：*当前文件*、*依赖文件* 和
    *其他文件*。我们将应用不同的策略来优化每种类型的文件。
- en: '| XF-Context | Hierarchical Context Pruning (Top-p: 1.0) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| XF-Context | 层次上下文剪枝（Top-p: 1.0） |'
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
- en: '| Random-All | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Random-All | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
- en: '| Top-k: 0 | 9.45 | 49.44 | 36.87 | 72.14 | 29.91 | 66.96 | 32.62 | 69.11 |
    28.93 | 62.03 | 39.17 | 75.16 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Top-k: 0 | 9.45 | 49.44 | 36.87 | 72.14 | 29.91 | 66.96 | 32.62 | 69.11 |
    28.93 | 62.03 | 39.17 | 75.16 |'
- en: '| Top-k: 5 | 9.64 | 49.78 | 39.74 | 73.90 | 32.68 | 69.05 | 35.76 | 71.41 |
    31.26 | 63.74 | 42.44 | 76.95 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Top-k: 5 | 9.64 | 49.78 | 39.74 | 73.90 | 32.68 | 69.05 | 35.76 | 71.41 |
    31.26 | 63.74 | 42.44 | 76.95 |'
- en: '| Top-k: 10 | 9.91 | 49.85 | 40.30 | 74.56 | 34.15 | 69.37 | 36.47 | 71.50
    | 31.82 | 64.34 | 42.63 | 77.35 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Top-k: 10 | 9.91 | 49.85 | 40.30 | 74.56 | 34.15 | 69.37 | 36.47 | 71.50
    | 31.82 | 64.34 | 42.63 | 77.35 |'
- en: 'Table 4: The results of completion using hierarchical context pruning with
    different top-k values.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 使用不同 top-k 值进行层次上下文剪枝的完成结果。'
- en: '| XF-Context | Hierarchical Context Pruning (Top-k: 5) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| XF-Context | 层次上下文剪枝（Top-k: 5） |'
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
- en: '| Random-All | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Random-All | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
- en: '| Top-p: 0.1 | 14.27 | 53.94 | 37.85 | 73.11 | 32.99 | 68.75 | 34.16 | 70.43
    | 29.19 | 62.09 | 40.98 | 76.26 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Top-p: 0.1 | 14.27 | 53.94 | 37.85 | 73.11 | 32.99 | 68.75 | 34.16 | 70.43
    | 29.19 | 62.09 | 40.98 | 76.26 |'
- en: '| Top-p: 0.2 | 13.52 | 53.20 | 38.04 | 73.13 | 33.15 | 68.59 | 34.84 | 70.40
    | 29.72 | 62.32 | 40.94 | 76.25 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Top-p: 0.2 | 13.52 | 53.20 | 38.04 | 73.13 | 33.15 | 68.59 | 34.84 | 70.40
    | 29.72 | 62.32 | 40.94 | 76.25 |'
- en: '| Top-p: 0.3 | 12.88 | 52.60 | 38.49 | 73.19 | 32.84 | 68.31 | 35.22 | 70.64
    | 30.13 | 62.77 | 41.21 | 76.20 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Top-p: 0.3 | 12.88 | 52.60 | 38.49 | 73.19 | 32.84 | 68.31 | 35.22 | 70.64
    | 30.13 | 62.77 | 41.21 | 76.20 |'
- en: 'Table 5: The results of completion using hierarchical context pruning with
    different top-p values.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 使用不同 top-p 值进行层次上下文剪枝的完成结果。'
- en: Current File.
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 当前文件。
- en: For the current file, any content within the file may be needed during completion,
    so we retain all content of the file and convert it into the Fill-in-the-middle
    (FIM) format.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于当前文件，完成过程中可能需要文件中的任何内容，因此我们保留文件的所有内容，并将其转换为填充中间（FIM）格式。
- en: Dependency Files.
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 依赖文件。
- en: 'According to the experimental results in Section [4.4](#S4.SS4 "4.4 Cross-File
    Content Analysis ‣ 4 Preliminary Studies ‣ Hierarchical Context Pruning: Optimizing
    Real-World Code Completion with Repository-Level Pretrained Code LLMs"), removing
    the global context across files does not affect the accuracy of completions. Therefore,
    for dependency files, we remove all global context from these files.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '根据第[4.4节](#S4.SS4 "4.4 Cross-File Content Analysis ‣ 4 Preliminary Studies
    ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level
    Pretrained Code LLMs")中的实验结果，移除跨文件的全局上下文不会影响完成的准确性。因此，对于依赖文件，我们移除这些文件中的所有全局上下文。'
- en: Other Files.
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他文件。
- en: We refer to files other than the current file and its direct dependency files,
    namely $\{F\setminus D_{1}(f)\}\setminus f\}$, collectively as other files. For
    the content in *other files*, we remove all global context, and then we employ
    function-level sampling and pruning methods to optimize the content of these files.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将当前文件及其直接依赖文件之外的文件，统称为其他文件。对于*其他文件*中的内容，我们移除所有全局上下文，然后我们采用函数级别的采样和剪枝方法来优化这些文件的内容。
- en: '![Refer to caption](img/e54bb90bdb16946bf3b7f6b5dc8f2a41.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e54bb90bdb16946bf3b7f6b5dc8f2a41.png)'
- en: 'Figure 4: left: Comparison of completion results using random-all and the hierarchical
    context pruning across six models. middle: Comparison of throughput using random-all
    and the hierarchical context pruning across six models. right: Comparison of prompt
    length using random-all and the hierarchical context pruning of different top-p
    values (top-k=5).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：左：使用随机全部和分层上下文剪枝在六个模型中的完成结果比较。中：使用随机全部和分层上下文剪枝在六个模型中的吞吐量比较。右：使用随机全部和不同top-p值（top-k=5）的分层上下文剪枝在提示长度中的比较。
- en: 5.3 Function-level Sampling
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 函数级采样
- en: In this study, we used OpenAI’s text-embedding API³³3openai-text-embedding-ada-002
    to embed each function (or class method) and query code snippet in the repository.
    We then used the pre-computed similarity of embeddings between the query and candidate
    functions (or class methods) as an indicator of relevance. We select the code
    from the current line of completion and the 10 lines before and after it as a
    query to find functions and class methods most relevant to the current completion
    content.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们使用了OpenAI的text-embedding API³³3openai-text-embedding-ada-002来嵌入每个函数（或类方法）和代码库中的查询代码片段。然后，我们使用查询与候选函数（或类方法）之间的预计算嵌入相似度作为相关性的指标。我们从当前完成的代码行以及前后各10行中选择代码作为查询，以找到与当前完成内容最相关的函数和类方法。
- en: 'We implemented two sampling strategies (top-k and top-p) and designed distinct
    content pruning strategies for the functions (or class methods) sampled under
    each strategy, see Section [5.4](#S5.SS4 "5.4 Function-level Pruning ‣ 5 Hierarchical
    Context Pruning ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs").'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '我们实施了两种采样策略（top-k 和 top-p），并为每种策略下的函数（或类方法）设计了不同的内容剪枝策略，参见第[5.4节](#S5.SS4 "5.4
    Function-level Pruning ‣ 5 Hierarchical Context Pruning ‣ Hierarchical Context
    Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs")。'
- en: 5.4 Function-level Pruning
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 函数级剪枝
- en: 'According to the experimental results in Section [4.4](#S4.SS4 "4.4 Cross-File
    Content Analysis ‣ 4 Preliminary Studies ‣ Hierarchical Context Pruning: Optimizing
    Real-World Code Completion with Repository-Level Pretrained Code LLMs"), the global
    context from all non-current files and most of the function bodies (or class method
    bodies) within the code repository can be pruned. Appropriately pruning low-relevance
    content can significantly reduce the length of the prompt input to the model.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '根据第[4.4节](#S4.SS4 "4.4 Cross-File Content Analysis ‣ 4 Preliminary Studies
    ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level
    Pretrained Code LLMs")中的实验结果，可以剪枝来自所有非当前文件以及代码库中大多数函数体（或类方法体）的全局上下文。适当地剪枝低相关性内容可以显著减少输入模型的提示长度。'
- en: 'Let $F$ represent the functions sampled using the top-p strategy:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 设$F$表示使用top-p策略采样的函数：
- en: '|  | $\displaystyle F_{k}$ |  | (4) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle F_{k}$ |  | (4) |'
- en: '|  | $\displaystyle F_{p}$ |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle F_{p}$ |  |'
- en: where $F_{k}\subseteq F_{p}$ was completely pruned.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$F_{k}\subseteq F_{p}$被完全剪枝。
- en: Top-k Context Pruning.
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Top-k上下文剪枝。
- en: For functions (or class methods) within the set $F_{k}$, we retained their entire
    content.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于集合$F_{k}$中的函数（或类方法），我们保留了它们的全部内容。
- en: Top-p Context Pruning.
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Top-p上下文剪枝。
- en: For functions (or class methods) in the set $F_{p}$, we prune their implementations
    and retained only their function headers (or class method headers).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于集合 $F_{p}$ 中的函数（或类方法），我们修剪了它们的实现，只保留了它们的函数头（或类方法头）。
- en: 5.5 File-level Relevance Ranking
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 文件级相关性排名
- en: Each function or class method in the repository has a similarity score. We assign
    different relevance weights to functions sampled using different sampling strategies.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 存储库中的每个函数或类方法都有一个相似性分数。我们对使用不同采样策略采样的函数分配不同的相关性权重。
- en: '|  | $\displaystyle W(f)$ | $$\displaystyle=\begin{cases}1.0,\quad\forall f\in
    F_{k}\\ 0.5,\quad\forall f\in F_{p}\setminus F_{k}\\'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $\displaystyle W(f)$ | $$\displaystyle=\begin{cases}1.0,\quad\forall f\in
    F_{k}\\ 0.5,\quad\forall f\in F_{p}\setminus F_{k}\\'
- en: 0.0,\quad\forall f\in F\setminus(F_{k}\cup F_{p})\\
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 0.0,\quad\forall f\in F\setminus(F_{k}\cup F_{p})\\
- en: \end{cases}$$ |  | (5) |
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}$$ |  | (5) |
- en: where $\text{Top}_{k}(F)$ represent the functions with the highest relevance
    scores sampled using the top-k and top-p strategies, respectively.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\text{Top}_{k}(F)$ 代表使用 top-k 和 top-p 策略分别采样的具有最高相关性分数的函数。
- en: 'The similarity of a class is defined as the weighted sum of its class methods:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 类的相似性定义为其类方法的加权总和：
- en: '|  |  | $\displaystyle S(c)=\sum_{m\in c}W(m)*S(m)$ |  | (6) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle S(c)=\sum_{m\in c}W(m)*S(m)$ |  | (6) |'
- en: where, $c$ represents the class method.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$c$ 代表类方法。
- en: 'The similarity of a file is defined as the weighted sum of its functions and
    classes:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 文件的相似性定义为其函数和类的加权总和：
- en: '|  |  | $\displaystyle S(f)=\sum_{x\in\mathcal{F}}W(x)*S(x)+\sum_{c\in\mathcal{C}}S(c)$
    |  | (7) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle S(f)=\sum_{x\in\mathcal{F}}W(x)*S(x)+\sum_{c\in\mathcal{C}}S(c)$
    |  | (7) |'
- en: where, $\mathcal{F}$ represent the set of functions and classes in the file,
    respectively.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{F}$ 代表文件中的函数和类集合。
- en: Finally, we sort the files at the file-level according to the relevance score
    to determine their relative positions in the prompt.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们根据文件级的相关性分数对文件进行排序，以确定它们在提示中的相对位置。
- en: 5.6 Experimental Results
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 实验结果
- en: 'We initially fixed top-p at 1.0 and tested the impact of different top-k values
    on completion accuracy. Table [4](#S5.T4 "Table 4 ‣ 5.2 Hierarchical Context ‣
    5 Hierarchical Context Pruning ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs") presents some of
    the experimental results, while Table [11](#A4.T11 "Table 11 ‣ D.2 Hit Count Changes
    ‣ D.1 Complete Experimental Results ‣ Appendix D Dependency Level Analysis ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") in the Appendix [E](#A5 "Appendix E Complete Funcation-Level Sampling
    Experiment Results ‣ D.2 Hit Count Changes ‣ D.1 Complete Experimental Results
    ‣ Appendix D Dependency Level Analysis ‣ Hierarchical Context Pruning: Optimizing
    Real-World Code Completion with Repository-Level Pretrained Code LLMs") provides
    a more comprehensive results. We observed that increasing the top-k value beyond
    5 did not result in significant improvements in accuracy. Therefore, we conclude
    that a top-k value of 5 is sufficient.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '我们最初将 top-p 固定为 1.0，并测试了不同 top-k 值对完成准确性的影响。表 [4](#S5.T4 "Table 4 ‣ 5.2 Hierarchical
    Context ‣ 5 Hierarchical Context Pruning ‣ Hierarchical Context Pruning: Optimizing
    Real-World Code Completion with Repository-Level Pretrained Code LLMs") 展示了一些实验结果，而附录
    [E](#A5 "Appendix E Complete Funcation-Level Sampling Experiment Results ‣ D.2
    Hit Count Changes ‣ D.1 Complete Experimental Results ‣ Appendix D Dependency
    Level Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") 的表 [11](#A4.T11 "Table 11 ‣ D.2 Hit
    Count Changes ‣ D.1 Complete Experimental Results ‣ Appendix D Dependency Level
    Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") 提供了更全面的结果。我们观察到将 top-k 值增加到 5 以上并未显著提高准确性。因此，我们得出结论，top-k
    值为 5 已经足够。'
- en: 'We further fixed the top-k value at 5 and tested the impact of varying top-p
    values (ranging from 0.1 to 0.9) on completion accuracy. Partial experimental
    results are presented in Table [5](#S5.T5 "Table 5 ‣ 5.2 Hierarchical Context
    ‣ 5 Hierarchical Context Pruning ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs"), with more comprehensive
    results available in Table [12](#A4.T12 "Table 12 ‣ D.2 Hit Count Changes ‣ D.1
    Complete Experimental Results ‣ Appendix D Dependency Level Analysis ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") in Appendix [E](#A5 "Appendix E Complete Funcation-Level Sampling
    Experiment Results ‣ D.2 Hit Count Changes ‣ D.1 Complete Experimental Results
    ‣ Appendix D Dependency Level Analysis ‣ Hierarchical Context Pruning: Optimizing
    Real-World Code Completion with Repository-Level Pretrained Code LLMs"). Our observations
    indicate that increasing the top-p value enhances completion accuracy; however,
    beyond a top-p value of 0.3, the improvement in accuracy slows considerably. Thus,
    we consider 0.3 to be a reasonable value.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步将 top-k 值固定为 5，并测试了不同 top-p 值（范围从 0.1 到 0.9）对完成准确性的影响。部分实验结果见表 [5](#S5.T5
    "Table 5 ‣ 5.2 Hierarchical Context ‣ 5 Hierarchical Context Pruning ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs")，更全面的结果请参见附录 [E](#A5 "Appendix E Complete Funcation-Level Sampling
    Experiment Results ‣ D.2 Hit Count Changes ‣ D.1 Complete Experimental Results
    ‣ Appendix D Dependency Level Analysis ‣ Hierarchical Context Pruning: Optimizing
    Real-World Code Completion with Repository-Level Pretrained Code LLMs") 中的表 [12](#A4.T12
    "Table 12 ‣ D.2 Hit Count Changes ‣ D.1 Complete Experimental Results ‣ Appendix
    D Dependency Level Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs")。我们的观察结果表明，增加 top-p
    值可以提高完成准确性；然而，超过 0.3 的 top-p 值时，准确性的提升速度显著减慢。因此，我们认为 0.3 是一个合理的值。'
- en: 'Figure [4](#S5.F4 "Figure 4 ‣ Other Files. ‣ 5.2 Hierarchical Context ‣ 5 Hierarchical
    Context Pruning ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") visually compares the Hierarchical
    Context Pruning (HCP) strategy (top-k=5, top-p=0.3) with the method of randomly
    concatenating all repository code files across three dimensions: completion accuracy,
    throughput rate, and input length. The visualization shows that, compared to random
    concatenation, HCP significantly reduces input length (enhancing throughput) while
    improving the model’s completion accuracy.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [4](#S5.F4 "Figure 4 ‣ Other Files. ‣ 5.2 Hierarchical Context ‣ 5 Hierarchical
    Context Pruning ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") 直观地将层次化上下文修剪（HCP）策略（top-k=5，top-p=0.3）与在三个维度上（完成准确性、吞吐率和输入长度）随机连接所有代码库文件的方法进行比较。可视化显示，与随机连接相比，HCP
    显著减少了输入长度（提升了吞吐率），同时提高了模型的完成准确性。'
- en: 6 Conclusion
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this study, we evaluated six Code LLMs pre-trained with repository-level
    code data. We conducted a detailed error analysis on these Code LLMs, performed
    topological dependency analysis on files within the code repositories, and analyzed
    the content of these files. Based on the results of these experiments, we proposed
    a strategy named Hierarchical Context Pruning to construct high-quality prompt
    inputs. Finally, we conducted experiments on six Repo-Code LLMs to verify the
    effectiveness of the proposed method.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们评估了六个使用代码库级数据预训练的 Code LLMs。我们对这些 Code LLMs 进行了详细的错误分析，对代码库中的文件进行了拓扑依赖分析，并分析了这些文件的内容。根据这些实验的结果，我们提出了一种名为层次化上下文修剪的策略，以构建高质量的提示输入。最后，我们在六个
    Repo-Code LLMs 上进行了实验，以验证所提方法的有效性。
- en: Limitations
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: Benchmark.
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基准测试
- en: 'In this study, we utilized the CrossCodeEval benchmark for evaluation. However,
    as demonstrated in the error analysis presented in Sections [4.2](#S4.SS2 "4.2
    Completion Error Analysis ‣ 4 Preliminary Studies ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs")
    and Appendix [A](#A1 "Appendix A Error Description and Analysis ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs"), while the evaluation method based on exact matches is convenient
    and quick, it does not provide comprehensive results. Therefore, there may be
    a discrepancy between the evaluation outcomes and the actual capabilities of the
    model.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们使用了CrossCodeEval基准进行评估。然而，正如在第[4.2](#S4.SS2 "4.2 完成错误分析 ‣ 4 初步研究 ‣ 分层上下文修剪：通过存储库级预训练代码LLM优化现实世界代码补全")节和附录[A](#A1
    "附录A 错误描述和分析 ‣ 分层上下文修剪：通过存储库级预训练代码LLM优化现实世界代码补全")中所示，基于精确匹配的评估方法虽然方便快捷，但未能提供全面的结果。因此，评估结果可能与模型的实际能力存在差异。
- en: Function-level Sampling.
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 函数级采样。
- en: In this study, sampling functions and class methods based on relevance required
    the use of a text embedding model. When the number of code files in the repository
    is excessive, this may reduce the sampling rate, leading to increased completion
    latency.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，基于相关性的采样函数和类方法需要使用文本嵌入模型。当存储库中的代码文件数量过多时，这可能会降低采样率，从而导致补全延迟增加。
- en: Ethical Statements
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: This study does not involve human participants, personal data, or hazardous
    materials, and primarily focuses on computational model performance. All resources
    used are open-source or properly licensed, ensuring compliance with relevant standards.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究不涉及人类参与者、个人数据或危险材料，主要集中在计算模型性能上。所有使用的资源都是开源的或获得了适当许可，确保符合相关标准。
- en: References
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Allal et al. (2023) Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao
    Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra,
    and Alex Gu et al. 2023. [Santacoder: don’t reach for the stars!](http://arxiv.org/abs/2301.03988)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Allal et al. (2023) Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao
    Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra,
    and Alex Gu et al. 2023. [Santacoder: 不要追求星辰！](http://arxiv.org/abs/2301.03988)'
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, and Charles Sutton. 2021. [Program synthesis with large language models](http://arxiv.org/abs/2108.07732).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, and Charles Sutton. 2021. [使用大型语言模型进行程序合成](http://arxiv.org/abs/2108.07732)。
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
    and Juanzi Li. 2023. [Longbench: A bilingual, multitask benchmark for long context
    understanding](http://arxiv.org/abs/2308.14508).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
    and Juanzi Li. 2023. [Longbench: 一个用于长上下文理解的双语多任务基准](http://arxiv.org/abs/2308.14508)。'
- en: Bavarian et al. (2022) Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman,
    Christine McLeavey, Jerry Tworek, and Mark Chen. 2022. [Efficient training of
    language models to fill in the middle](http://arxiv.org/abs/2207.14255).
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bavarian et al. (2022) Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman,
    Christine McLeavey, Jerry Tworek, and Mark Chen. 2022. [有效训练语言模型以填补中间部分](http://arxiv.org/abs/2207.14255)。
- en: Bi et al. (2024) Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan,
    Fangxin Lu, Zili Zhang, Yulei Sui, Xuanhua Shi, and Hai Jin. 2024. [Iterative
    refinement of project-level code context for precise code generation with compiler
    feedback](http://arxiv.org/abs/2403.16792).
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bi et al. (2024) Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan,
    Fangxin Lu, Zili Zhang, Yulei Sui, Xuanhua Shi, and Hai Jin. 2024. [通过编译器反馈迭代优化项目级代码上下文以实现精确的代码生成](http://arxiv.org/abs/2403.16792)。
- en: 'Cassano et al. (2022) Federico Cassano, John Gouwar, Daniel Nguyen, Sydney
    Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane
    Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda.
    2022. [Multipl-e: A scalable and extensible approach to benchmarking neural code
    generation](http://arxiv.org/abs/2208.08227).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cassano et al. (2022) Federico Cassano, John Gouwar, Daniel Nguyen, Sydney
    Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn
    Jane Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda.
    2022. [Multipl-e: 可扩展和可扩展的神经代码生成基准方法](http://arxiv.org/abs/2208.08227)。'
- en: 'Chen et al. (2024a) Longze Chen, Ziqiang Liu, Wanwei He, Yunshui Li, Run Luo,
    and Min Yang. 2024a. [Long context is not long at all: A prospector of long-dependency
    data for large language models](http://arxiv.org/abs/2405.17915).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2024a) Longze Chen、Ziqiang Liu、Wanwei He、Yunshui Li、Run Luo 和 Min
    Yang。2024年。[长上下文根本不长：大语言模型长依赖数据的探索者](http://arxiv.org/abs/2405.17915)。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, and et al. 2021. [Evaluating large language models trained on code](http://arxiv.org/abs/2107.03374).
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2021) Mark Chen、Jerry Tworek、Heewoo Jun、Qiming Yuan、Henrique Ponde
    de Oliveira Pinto、Jared Kaplan、Harri Edwards、Yuri Burda、Nicholas Joseph、Greg Brockman
    和等。2021年。[评估在代码上训练的大型语言模型](http://arxiv.org/abs/2107.03374)。
- en: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 2023. [Extending context window of large language models via positional
    interpolation](http://arxiv.org/abs/2306.15595).
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) Shouyuan Chen、Sherman Wong、Liangjian Chen 和 Yuandong Tian。2023年。[通过位置插值扩展大型语言模型的上下文窗口](http://arxiv.org/abs/2306.15595)。
- en: 'Chen et al. (2024b) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
    Liu, Song Han, and Jiaya Jia. 2024b. [Longlora: Efficient fine-tuning of long-context
    large language models](http://arxiv.org/abs/2309.12307).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2024b) Yukang Chen、Shengju Qian、Haotian Tang、Xin Lai、Zhijian Liu、Song
    Han 和 Jiaya Jia。2024年。[Longlora: 长上下文大型语言模型的高效微调](http://arxiv.org/abs/2309.12307)。'
- en: 'Ding et al. (2023) Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding,
    Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia,
    Dan Roth, and Bing Xiang. 2023. [Crosscodeeval: A diverse and multilingual benchmark
    for cross-file code completion](http://arxiv.org/abs/2310.11248).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding et al. (2023) Yangruibo Ding、Zijian Wang、Wasi Uddin Ahmad、Hantian Ding、Ming
    Tan、Nihal Jain、Murali Krishna Ramanathan、Ramesh Nallapati、Parminder Bhatia、Dan
    Roth 和 Bing Xiang。2023年。[Crosscodeeval: 用于跨文件代码补全的多样化和多语言基准](http://arxiv.org/abs/2310.11248)。'
- en: 'Ding et al. (2024) Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan
    Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. 2024. [Longrope: Extending
    llm context window beyond 2 million tokens](http://arxiv.org/abs/2402.13753).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding et al. (2024) Yiran Ding、Li Lyna Zhang、Chengruidong Zhang、Yuanyuan Xu、Ning
    Shang、Jiahang Xu、Fan Yang 和 Mao Yang。2024年。[Longrope: 扩展 llm 上下文窗口超过 200 万个标记](http://arxiv.org/abs/2402.13753)。'
- en: Fang et al. (2024) Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, and Daniel
    Kang. 2024. [Llm agents can autonomously hack websites](http://arxiv.org/abs/2402.06664).
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2024) Richard Fang、Rohan Bindu、Akul Gupta、Qiusi Zhan 和 Daniel Kang。2024年。[Llm
    agents can autonomously hack websites](http://arxiv.org/abs/2402.06664)。
- en: 'Fried et al. (2023) Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric
    Wallace, Freda Shi, Ruiqi Zhong, Wen tau Yih, Luke Zettlemoyer, and Mike Lewis.
    2023. [Incoder: A generative model for code infilling and synthesis](http://arxiv.org/abs/2204.05999).'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fried et al. (2023) Daniel Fried、Armen Aghajanyan、Jessy Lin、Sida Wang、Eric
    Wallace、Freda Shi、Ruiqi Zhong、Wen tau Yih、Luke Zettlemoyer 和 Mike Lewis。2023年。[Incoder:
    一种用于代码填充和合成的生成模型](http://arxiv.org/abs/2204.05999)。'
- en: 'Guo et al. (2024) Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng
    Liang. 2024. [Deepseek-coder: When the large language model meets programming
    – the rise of code intelligence](http://arxiv.org/abs/2401.14196).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo et al. (2024) Daya Guo、Qihao Zhu、Dejian Yang、Zhenda Xie、Kai Dong、Wentao
    Zhang、Guanting Chen、Xiao Bi、Y. Wu、Y. K. Li、Fuli Luo、Yingfei Xiong 和 Wenfeng Liang。2024年。[Deepseek-coder:
    当大型语言模型遇上编程——代码智能的崛起](http://arxiv.org/abs/2401.14196)。'
- en: Hendrycks et al. (2021) Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas
    Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song,
    and Jacob Steinhardt. 2021. [Measuring coding challenge competence with apps](http://arxiv.org/abs/2105.09938).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. (2021) Dan Hendrycks、Steven Basart、Saurav Kadavath、Mantas Mazeika、Akul
    Arora、Ethan Guo、Collin Burns、Samir Puranik、Horace He、Dawn Song 和 Jacob Steinhardt。2021年。[通过应用程序测量编码挑战能力](http://arxiv.org/abs/2105.09938)。
- en: 'Holt et al. (2024) Samuel Holt, Max Ruiz Luyten, and Mihaela van der Schaar.
    2024. [L2mac: Large language model automatic computer for extensive code generation](http://arxiv.org/abs/2310.02003).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Holt et al. (2024) Samuel Holt、Max Ruiz Luyten 和 Mihaela van der Schaar。2024年。[L2mac:
    大型语言模型自动计算机用于广泛的代码生成](http://arxiv.org/abs/2310.02003)。'
- en: 'Jimenez et al. (2024) Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu
    Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. [Swe-bench: Can language
    models resolve real-world github issues?](http://arxiv.org/abs/2310.06770)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jimenez et al. (2024) Carlos E. Jimenez、John Yang、Alexander Wettig、Shunyu Yao、Kexin
    Pei、Ofir Press 和 Karthik Narasimhan。2024年。[Swe-bench: 语言模型能解决现实世界的 GitHub 问题吗？](http://arxiv.org/abs/2310.06770)'
- en: 'Lai et al. (2022) Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi
    Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu.
    2022. [Ds-1000: A natural and reliable benchmark for data science code generation](http://arxiv.org/abs/2211.11501).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lai等人（2022）Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong,
    Luke Zettlemoyer, Scott Wen tau Yih, Daniel Fried, Sida Wang, 和 Tao Yu. 2022.
    [Ds-1000: 一个自然且可靠的数据科学代码生成基准](http://arxiv.org/abs/2211.11501)。'
- en: 'Li et al. (2024a) Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin.
    2024a. [Evocodebench: An evolving code generation benchmark aligned with real-world
    code repositories](http://arxiv.org/abs/2404.00599).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等人（2024a）Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, 和 Zhi Jin. 2024a. [Evocodebench:
    与真实世界代码库对齐的演进代码生成基准](http://arxiv.org/abs/2404.00599)。'
- en: 'Li et al. (2023) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    and et al. 2023. [Starcoder: may the source be with you!](http://arxiv.org/abs/2305.06161)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等人（2023）Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis
    Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, 等等.
    2023. [Starcoder: 愿源代码与你同在！](http://arxiv.org/abs/2305.06161)'
- en: Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin
    Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin,
    Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James
    Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas,
    Koray Kavukcuoglu, and Oriol Vinyals. 2022. [Competition-level code generation
    with alphacode](https://doi.org/10.1126/science.abq1158). *Science*, 378(6624):1092–1097.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人（2022）Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
    Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas
    Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen,
    Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel
    J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray
    Kavukcuoglu, 和 Oriol Vinyals. 2022. [竞争级代码生成与alphacode](https://doi.org/10.1126/science.abq1158).
    *科学*, 378(6624):1092–1097。
- en: Li et al. (2024b) Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang,
    Lei Zhang, Shuzheng Si, Ling-Hao Chen, Junhao Liu, Tongliang Liu, Fei Huang, and
    Yongbin Li. 2024b. [One-shot learning as instruction data prospector for large
    language models](http://arxiv.org/abs/2312.10302).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人（2024b）Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang,
    Shuzheng Si, Ling-Hao Chen, Junhao Liu, Tongliang Liu, Fei Huang, 和 Yongbin Li.
    2024b. [一次性学习作为大语言模型的指令数据探测器](http://arxiv.org/abs/2312.10302)。
- en: 'Liang et al. (2024) Ming Liang, Xiaoheng Xie, Gehao Zhang, Xunjin Zheng, Peng
    Di, wei jiang, Hongwei Chen, Chengpeng Wang, and Gang Fan. 2024. [Repofuse: Repository-level
    code completion with fused dual context](http://arxiv.org/abs/2402.14323).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang等人（2024）Ming Liang, Xiaoheng Xie, Gehao Zhang, Xunjin Zheng, Peng Di,
    wei jiang, Hongwei Chen, Chengpeng Wang, 和 Gang Fan. 2024. [Repofuse: 融合双重上下文的库级代码补全](http://arxiv.org/abs/2402.14323)。'
- en: 'Liu et al. (2023) Tianyang Liu, Canwen Xu, and Julian McAuley. 2023. [Repobench:
    Benchmarking repository-level code auto-completion systems](http://arxiv.org/abs/2306.03091).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu等人（2023）Tianyang Liu, Canwen Xu, 和 Julian McAuley. 2023. [Repobench: 库级代码自动补全系统的基准测试](http://arxiv.org/abs/2306.03091)。'
- en: 'Lozhkov et al. (2024) Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico
    Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu,
    Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, and Arthur Zucker et al.
    2024. [Starcoder 2 and the stack v2: The next generation](http://arxiv.org/abs/2402.19173).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lozhkov等人（2024）Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano,
    Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang
    Wei, Tianyang Liu, Max Tian, Denis Kocetkov, 和 Arthur Zucker 等等. 2024. [Starcoder
    2 和 stack v2: 下一代](http://arxiv.org/abs/2402.19173)。'
- en: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. [Wizardcoder:
    Empowering code large language models with evol-instruct](http://arxiv.org/abs/2306.08568).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo等人（2023）Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, 和 Daxin Jiang. 2023. [Wizardcoder: 用evol-instruct赋能代码大型语言模型](http://arxiv.org/abs/2306.08568)。'
- en: 'Muennighoff et al. (2024) Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai
    Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra,
    and Shayne Longpre. 2024. [Octopack: Instruction tuning code large language models](http://arxiv.org/abs/2308.07124).'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Muennighoff等人（2024）Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng,
    Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, 和
    Shayne Longpre. 2024. [Octopack: 指令调整代码大型语言模型](http://arxiv.org/abs/2308.07124)。'
- en: 'Nijkamp et al. (2023a) Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio
    Savarese, and Yingbo Zhou. 2023a. [Codegen2: Lessons for training llms on programming
    and natural languages](http://arxiv.org/abs/2305.02309).'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nijkamp et al. (2023a) 埃里克·奈坎普、日向晃、蔡名雄、西尔维奥·萨瓦雷斯、周英波。2023a年。[Codegen2: 训练大型语言模型在编程和自然语言上的经验教训](http://arxiv.org/abs/2305.02309)。'
- en: 'Nijkamp et al. (2023b) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023b. [Codegen: An open
    large language model for code with multi-turn program synthesis](http://arxiv.org/abs/2203.13474).'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nijkamp et al. (2023b) 埃里克·奈坎普、博·庞、日向晃、李福·图、黄华、周英波、西尔维奥·萨瓦雷斯、蔡名雄。2023b年。[Codegen:
    一个用于多轮程序合成的开源大型语言模型](http://arxiv.org/abs/2203.13474)。'
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](http://arxiv.org/abs/2203.02155).
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) 龙·欧阳、杰夫·吴、徐江、迪奥戈·阿尔梅达、卡罗尔·L·温赖特、帕梅拉·米什金、钟张、桑蒂尼·阿加瓦尔、卡塔里娜·斯拉玛、亚历克斯·雷、约翰·舒尔曼、雅各布·希尔顿、弗雷泽·凯尔顿、卢克·米勒、玛迪·西门斯、阿曼达·阿斯克尔、彼得·韦林德、保罗·克里斯蒂亚诺、简·莱克、瑞安·洛。2022年。[通过人类反馈训练语言模型以遵循指令](http://arxiv.org/abs/2203.02155)。
- en: 'Phan et al. (2024) Huy N. Phan, Hoang N. Phan, Tien N. Nguyen, and Nghi D. Q.
    Bui. 2024. [Repohyper: Better context retrieval is all you need for repository-level
    code completion](http://arxiv.org/abs/2403.06095).'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Phan et al. (2024) 惠·N·潘、黄·N·潘、田·N·阮、阮·D·Q·武。2024年。[Repohyper: 更好的上下文检索就是你在仓库级代码补全中所需的一切](http://arxiv.org/abs/2403.06095)。'
- en: Pinnaparaju et al. (2024) Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung,
    Jonathan Tow, James Baicoianu, Ashish Datta, Maksym Zhuravinskyi, Dakota Mahan,
    Marco Bellagente, Carlos Riquelme, and Nathan Cooper. 2024. [Stable code technical
    report](http://arxiv.org/abs/2404.01226).
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinnaparaju et al. (2024) 尼基尔·皮纳帕拉朱、瑞辛斯·阿迪提扬、杜伊·冯、乔纳森·托、詹姆斯·拜科扬努、阿希什·达塔、马克西姆·朱拉文斯基、达科塔·马汉、马尔科·贝拉根特、卡洛斯·里克尔梅、内森·库珀。2024年。[稳定代码技术报告](http://arxiv.org/abs/2404.01226)。
- en: 'Rozière et al. (2024) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre,
    Tal Remez, Jérémy Rapin, and Artyom Kozhevnikov et al. 2024. [Code llama: Open
    foundation models for code](http://arxiv.org/abs/2308.12950).'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rozière et al. (2024) 巴普蒂斯特·罗济埃、乔纳斯·盖林、法比安·格洛克尔、斯滕·苏特拉、伊泰·加特、肖青·艾伦·谭、约西·阿迪、景宇·刘、罗曼·索维斯特、塔尔·瑞梅兹、杰雷米·拉潘、阿尔乔姆·科热夫尼科夫等。2024年。[Code
    llama: 用于代码的开源基础模型](http://arxiv.org/abs/2308.12950)。'
- en: 'Shi et al. (2024) Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang
    Wu, Yuanda Zhu, Joyce Ho, Carl Yang, and May D. Wang. 2024. [Ehragent: Code empowers
    large language models for few-shot complex tabular reasoning on electronic health
    records](http://arxiv.org/abs/2401.07128).'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi et al. (2024) 温琪·施、冉旭、俞辰·庄、岳宇、杰瑜·张、杭武、远达·朱、乔伊斯·霍、卡尔·杨、梅·D·王。2024年。[Ehragent:
    代码赋能大型语言模型在电子健康记录中的少样本复杂表格推理](http://arxiv.org/abs/2401.07128)。'
- en: Shrivastava et al. (2023) Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow.
    2023. [Repository-level prompt generation for large language models of code](http://arxiv.org/abs/2206.12839).
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shrivastava et al. (2023) 迪莎·施里瓦斯塔瓦、雨果·拉罗谢尔、丹尼尔·塔洛。2023年。[针对大型语言模型的仓库级提示生成](http://arxiv.org/abs/2206.12839)。
- en: 'Sun et al. (2024) Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang
    Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, Qipeng
    Guo, Xipeng Qiu, Pengcheng Yin, Xiaoli Li, Fei Yuan, Lingpeng Kong, Xiang Li,
    and Zhiyong Wu. 2024. [A survey of neural code intelligence: Paradigms, advances
    and beyond](http://arxiv.org/abs/2403.14734).'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2024) 苏启时、陈志睿、徐方志、程坎之、马昌、张月尧、王佳宁、韩承成、朱仁宇、袁帅、郭琪鹏、邱希鹏、尹鹏程、李晓莉、袁飞、孔令鹏、李翔、吴志勇。2024年。[神经代码智能调查：范式、进展及未来展望](http://arxiv.org/abs/2403.14734)。
- en: 'Team et al. (2024) CodeGemma Team, Ale Jakse Hartman, Andrea Hu, Christopher A.
    Choquette-Choo, Heri Zhao, Jane Fine, Jeffrey Hui, Jingyue Shen, Joe Kelley, Joshua
    Howland, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Nam Nguyen, Paul Michel, Peter
    Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, Siqi Zuo, Tris
    Warkentin, and Zhitao Gong. 2024. [Codegemma: Open code models based on gemma](https://goo.gle/codegemma).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team et al. (2024) CodeGemma团队、艾尔·贾克斯·哈特曼、安德烈亚·胡、克里斯托弗·A·肖凯特-楚、赫里·赵、简·范、杰弗里·辉、晶月·沈、乔·凯利、约书亚·霍兰、克什提·班萨尔、卢克·维尔尼斯、马特奥·维斯、南·阮、保罗·米歇尔、彼得·周、普拉提克·乔希、拉文·库马尔、萨尔马德·哈什米、舒布汉·阿格拉瓦尔、四起·左、特里斯·沃肯廷、和志涛·龚。2024年。[Codegemma:
    基于Gemma的开源代码模型](https://goo.gle/codegemma)。'
- en: Thakur et al. (2024) Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin,
    and Swarat Chaudhuri. 2024. [An in-context learning agent for formal theorem-proving](http://arxiv.org/abs/2310.04353).
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thakur et al. (2024) 阿米塔尤什·塔库尔、乔治·祖卡拉斯、温夜名、吉米·辛、斯瓦拉特·乔杜里。2024。 [用于形式定理证明的上下文学习代理](http://arxiv.org/abs/2310.04353)。
- en: Wang et al. (2024a) Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu
    Li, Hao Peng, and Heng Ji. 2024a. [Executable code actions elicit better llm agents](http://arxiv.org/abs/2402.01030).
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2024a) 王星耀、陈扬毅、袁立凡、张易哲、李云竹、彭浩、季衡。2024a。 [可执行代码操作引发更好的LLM代理](http://arxiv.org/abs/2402.01030)。
- en: 'Wang et al. (2024b) Yejie Wang, Keqing He, Guanting Dong, Pei Wang, Weihao
    Zeng, Muxi Diao, Yutao Mou, Mengdi Zhang, Jingang Wang, Xunliang Cai, and Weiran
    Xu. 2024b. [Dolphcoder: Echo-locating code large language models with diverse
    and multi-objective instruction tuning](http://arxiv.org/abs/2402.09136).'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2024b) 叶杰·王、何克卿、董冠廷、王佩、曾伟豪、刁木熙、牟玉涛、张萌迪、王静刚、蔡训良、徐伟然。2024b。 [Dolphcoder:
    通过多样化和多目标指令调优实现代码大型语言模型的回声定位](http://arxiv.org/abs/2402.09136)。'
- en: Wang et al. (2023a) Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel,
    Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith,
    Iz Beltagy, and Hannaneh Hajishirzi. 2023a. [How far can camels go? exploring
    the state of instruction tuning on open resources](http://arxiv.org/abs/2306.04751).
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023a) 王一中、哈米什·艾维森、普拉迪普·达西吉、杰克·赫塞尔、图莎尔·科特、卡亚提·拉维·昌杜、大卫·瓦登、凯尔西·麦克米伦、诺亚·A·史密斯、伊兹·贝尔塔吉、汉南赫·哈吉什尔齐。2023a。
    [骆驼能走多远？探索开放资源上的指令调优现状](http://arxiv.org/abs/2306.04751)。
- en: Wang et al. (2023b) Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig.
    2023b. [Execution-based evaluation for open-domain code generation](http://arxiv.org/abs/2212.10481).
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) 王志若、周书艳、丹尼尔·弗里德、格雷厄姆·纽比格。2023b。 [基于执行的开放领域代码生成评估](http://arxiv.org/abs/2212.10481)。
- en: Xiao et al. (2024) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. 2024. [Efficient streaming language models with attention sinks](http://arxiv.org/abs/2309.17453).
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. (2024) 肖光轩、田远东、陈贝迪、韩松、迈克·刘易斯。2024。 [具有注意力沉没的高效流式语言模型](http://arxiv.org/abs/2309.17453)。
- en: 'Xu et al. (2023) Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia
    Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao,
    Lingpeng Kong, Bailin Wang, Caiming Xiong, and Tao Yu. 2023. [Lemur: Harmonizing
    natural language and code for language agents](http://arxiv.org/abs/2310.06830).'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2023) 许一恒、苏洪金、陈星、米博宇、刘倩、施伟佳、惠斌远、周凡、刘艺涛、谢天宝、程周军、赵思恒、孔令鹏、王百林、熊财名、余涛。2023。
    [Lemur: 使自然语言和代码在语言代理中和谐统一](http://arxiv.org/abs/2310.06830)。'
- en: 'Yang et al. (2024) John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret,
    Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. [Swe-agent: Agent-computer
    interfaces enable automated software engineering](https://api.semanticscholar.org/CorpusID:270063685).'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2024) 约翰·杨、卡洛斯·E·吉门内斯、亚历山大·韦蒂格、基利安·利雷特、姚顺宇、卡尔蒂克·纳拉西姆汉、奥菲尔·普雷斯。2024。
    [Swe-agent: 代理计算机接口实现自动化软件工程](https://api.semanticscholar.org/CorpusID:270063685)。'
- en: 'Yang et al. (2023) John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu
    Yao. 2023. [Intercode: Standardizing and benchmarking interactive coding with
    execution feedback](http://arxiv.org/abs/2306.14898).'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2023) 约翰·杨、阿克沙拉·普拉巴卡尔、卡尔蒂克·纳拉西姆汉、姚顺宇。2023。 [Intercode: 标准化和基准测试交互式编码及执行反馈](http://arxiv.org/abs/2306.14898)。'
- en: Yin et al. (2022) Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming
    Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski,
    Alex Polozov, and Charles Sutton. 2022. [Natural language to code generation in
    interactive data science notebooks](http://arxiv.org/abs/2212.09248).
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2022) 尹鹏程、李文定、肖克凡、阿比谢克·拉奥、温夜名、史肯森、乔舒亚·霍兰、佩奇·贝利、米歇尔·卡塔斯塔、亨里克·米哈勒夫斯基、亚历克斯·波洛佐夫、查尔斯·萨顿。2022。
    [自然语言到代码生成在互动数据科学笔记本中的应用](http://arxiv.org/abs/2212.09248)。
- en: 'Zelikman et al. (2024) Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman
    Kalai. 2024. [Self-taught optimizer (stop): Recursively self-improving code generation](http://arxiv.org/abs/2310.02304).'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zelikman et al. (2024) 埃里克·泽利克曼、埃莉安娜·洛奇、莱斯特·麦基、亚当·陶曼·卡莱。2024。 [自学优化器（STOP）：递归自我改进的代码生成](http://arxiv.org/abs/2310.02304)。
- en: 'Zhang et al. (2023a) Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu,
    Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. 2023a. [Repocoder: Repository-level
    code completion through iterative retrieval and generation](http://arxiv.org/abs/2303.12570).'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023a) 张凤基、陈贝、张跃、张杰基、刘瑾、赞道广、毛易、楼建广、陈伟柱。2023a。 [Repocoder: 通过迭代检索和生成实现库级代码补全](http://arxiv.org/abs/2303.12570)。'
- en: 'Zhang et al. (2023b) Lei Zhang, Yunshui Li, Ziqiang Liu, Jiaxi yang, Junhao
    Liu, and Min Yang. 2023b. [Marathon: A race through the realm of long context
    with large language models](http://arxiv.org/abs/2312.09542).'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023b）Lei Zhang、Yunshui Li、Ziqiang Liu、Jiaxi Yang、Junhao Liu 和 Min Yang。2023b。[马拉松：在长上下文领域中使用大型语言模型的竞赛](http://arxiv.org/abs/2312.09542)。
- en: 'Zhang et al. (2024a) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong
    Sun. 2024a. [$\infty$bench: Extending long context evaluation beyond 100k tokens](http://arxiv.org/abs/2402.13718).'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2024a）Xinrong Zhang、Yingfa Chen、Shengding Hu、Zihang Xu、Junhao Chen、Moo
    Khai Hao、Xu Han、Zhen Leng Thai、Shuo Wang、Zhiyuan Liu 和 Maosong Sun。2024a。[ $\infty$bench：将长上下文评估扩展到
    100k 令牌以上](http://arxiv.org/abs/2402.13718)。
- en: Zhang et al. (2024b) Yakun Zhang, Wenjie Zhang, Dezhi Ran, Qihao Zhu, Chengfeng
    Dou, Dan Hao, Tao Xie, and Lu Zhang. 2024b. [Learning-based widget matching for
    migrating gui test cases](https://doi.org/10.1145/3597503.3623322). In *Proceedings
    of the 46th IEEE/ACM International Conference on Software Engineering*, ICSE ’24\.
    ACM.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2024b）Yakun Zhang、Wenjie Zhang、Dezhi Ran、Qihao Zhu、Chengfeng Dou、Dan
    Hao、Tao Xie 和 Lu Zhang。2024b。[基于学习的小部件匹配用于迁移 GUI 测试用例](https://doi.org/10.1145/3597503.3623322)。在
    *第46届IEEE/ACM国际软件工程会议论文集*，ICSE ’24。ACM。
- en: 'Zheng et al. (2024) Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen
    Lin, Jie Fu, Wenhu Chen, and Xiang Yue. 2024. [Opencodeinterpreter: Integrating
    code generation with execution and refinement](http://arxiv.org/abs/2402.14658).'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2024）Tianyu Zheng、Ge Zhang、Tianhao Shen、Xueling Liu、Bill Yuchen Lin、Jie
    Fu、Wenhu Chen 和 Xiang Yue。2024。[Opencodeinterpreter：将代码生成与执行和完善集成](http://arxiv.org/abs/2402.14658)。
- en: Appendix A Error Description and Analysis
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 错误描述和分析
- en: 'In this section, we present detailed instances of various error types in model
    completions, accompanied by in-depth explanations and analyses of these errors.
    Figures [5](#A1.F5 "Figure 5 ‣ A.1 Redundant Content Generation ‣ Appendix A Error
    Description and Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs")-[12](#A1.F12 "Figure
    12 ‣ A.8 Incorrect Content Generation ‣ Appendix A Error Description and Analysis
    ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level
    Pretrained Code LLMs") depict representatives for each error category. Each figure
    is bifurcated, with the left panel showing the output generated by the code model
    and the right panel presenting the corresponding ground truth. Errors in model
    completions are emphasized in red italic text, whereas the ground truth is denoted
    in green italic.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们呈现了模型完成中的各种错误类型的详细实例，并附有对这些错误的深入解释和分析。图 [5](#A1.F5 "图 5 ‣ A.1 冗余内容生成
    ‣ 附录 A 错误描述和分析 ‣ 分层上下文修剪：优化具有仓库级预训练代码 LLM 的实际代码完成")-[12](#A1.F12 "图 12 ‣ A.8 错误内容生成
    ‣ 附录 A 错误描述和分析 ‣ 分层上下文修剪：优化具有仓库级预训练代码 LLM 的实际代码完成") 描绘了每种错误类别的代表性实例。每个图分为左右两部分，左侧面板显示由代码模型生成的输出，右侧面板展示相应的实际结果。模型完成中的错误用红色斜体文字强调，而实际结果用绿色斜体文字表示。
- en: A.1 Redundant Content Generation
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 冗余内容生成
- en: 'Redundant Content Generation means that the method is correctly called, but
    unnecessary additional content is generated. Figure [5](#A1.F5 "Figure 5 ‣ A.1
    Redundant Content Generation ‣ Appendix A Error Description and Analysis ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") illustrates an example of a Redundant Content Generation error. The
    ground truth specifies active is False, yet the model’s completion includes not
    only active is False but also additional irrelevant content.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 冗余内容生成指的是方法正确调用，但生成了不必要的额外内容。图 [5](#A1.F5 "图 5 ‣ A.1 冗余内容生成 ‣ 附录 A 错误描述和分析 ‣
    分层上下文修剪：优化具有仓库级预训练代码 LLM 的实际代码完成") 说明了冗余内容生成错误的一个示例。实际结果指定 active 为 False，但模型的完成不仅包括
    active 为 False，还包括额外的无关内容。
- en: '![Refer to caption](img/3d3b1f0fdf4de3ab27a9c48313806a12.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3d3b1f0fdf4de3ab27a9c48313806a12.png)'
- en: 'Figure 5: An example of redundant content generation error.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：冗余内容生成错误的示例。
- en: A.2 Partial Content Missing
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 部分内容缺失
- en: 'Partial Content Missing indicates that the right method is called, but the
    generated content is incomplete, although this might still be acceptable to the
    user. Figure [6](#A1.F6 "Figure 6 ‣ A.2 Partial Content Missing ‣ Appendix A Error
    Description and Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs") presents an example
    of a Partial Content Missing error. The ground truth is MinGrid and not game_name.startswith(’MiniGrid-’),
    but the code completion model only managed to replicate a portion of this ground
    truth.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 部分内容缺失表示调用了正确的方法，但生成的内容不完整，尽管这可能仍然对用户来说是可以接受的。图 [6](#A1.F6 "图 6 ‣ A.2 部分内容缺失
    ‣ 附录 A 错误描述与分析 ‣ 层级上下文剪枝：优化基于仓库级预训练代码 LLM 的实际代码补全") 展示了一个部分内容缺失错误的示例。实际值是 MinGrid，而不是
    game_name.startswith(’MiniGrid-’)，但代码补全模型仅复制了实际值的一部分。
- en: '![Refer to caption](img/71b30d7b6ed5af1a4c9f4c3528f77fe4.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/71b30d7b6ed5af1a4c9f4c3528f77fe4.png)'
- en: 'Figure 6: An example of partial content missing error.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 部分内容缺失错误的示例。'
- en: A.3 Parameter Value Error
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 参数值错误
- en: 'The Parameter Value Error reflects the situation where the function call is
    correct, but the passed parameter values are incorrect. Figure [7](#A1.F7 "Figure
    7 ‣ A.3 Parameter Value Error ‣ Appendix A Error Description and Analysis ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") displays an instance of a Parameter Value Error. The code completion
    model correctly invokes the class method, but the parameters it employs differ
    from those specified in the ground truth.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 参数值错误反映了函数调用正确，但传递的参数值不正确的情况。图 [7](#A1.F7 "图 7 ‣ A.3 参数值错误 ‣ 附录 A 错误描述与分析 ‣
    层级上下文剪枝：优化基于仓库级预训练代码 LLM 的实际代码补全") 展示了一个参数值错误的实例。代码补全模型正确调用了类方法，但所用的参数与实际值有所不同。
- en: '![Refer to caption](img/70d58d849937050933b1b98e8ff2478c.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/70d58d849937050933b1b98e8ff2478c.png)'
- en: 'Figure 7: An example of parameter value error.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 参数值错误的示例。'
- en: A.4 Exact Match Error
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 精确匹配错误
- en: 'Exact Match Error is a misjudgment due to the limitations of the exact match
    metric, such as using default values or specific strings when calling a function.
    Figure [8](#A1.F8 "Figure 8 ‣ A.4 Exact Match Error ‣ Appendix A Error Description
    and Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") illustrates an example of an Exact
    Match Error. The content completed by the code model is syntactically correct
    and semantically accurate, differing only slightly in textual terms from the ground
    truth. To avoid such misjudgments, a more reasonable evaluation method is necessary
    to assess the completion results.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 精确匹配错误是由于精确匹配指标的限制，例如在调用函数时使用默认值或特定字符串而产生的误判。图 [8](#A1.F8 "图 8 ‣ A.4 精确匹配错误
    ‣ 附录 A 错误描述与分析 ‣ 层级上下文剪枝：优化基于仓库级预训练代码 LLM 的实际代码补全") 说明了一个精确匹配错误的示例。代码模型生成的内容在语法上是正确的，在语义上也是准确的，只是与实际值在文本上略有不同。为了避免这种误判，需要一种更合理的评估方法来评估补全结果。
- en: '![Refer to caption](img/ca5e7bd7c324141d6354ed1a79c6dc41.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ca5e7bd7c324141d6354ed1a79c6dc41.png)'
- en: 'Figure 8: An example of exact match error.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 精确匹配错误的示例。'
- en: A.5 Non-existent Method Call
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 不存在的方法调用
- en: 'Non-existent Method Call indicates a call to a function, method, or property
    that does not exist. Figure [9](#A1.F9 "Figure 9 ‣ A.5 Non-existent Method Call
    ‣ Appendix A Error Description and Analysis ‣ Hierarchical Context Pruning: Optimizing
    Real-World Code Completion with Repository-Level Pretrained Code LLMs") presents
    an example of a Non-existent Method Call error. The ground truth refers to a class
    method within the session class; however, the content generated by the code completion
    model erroneously calls a method that does not exist in the session class. This
    error can be regarded as a form of hallucination in the context of code completion.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 不存在的方法调用表示调用了一个不存在的函数、方法或属性。图 [9](#A1.F9 "图 9 ‣ A.5 不存在的方法调用 ‣ 附录 A 错误描述与分析
    ‣ 层级上下文剪枝：优化基于仓库级预训练代码 LLM 的实际代码补全") 展示了一个不存在的方法调用错误的示例。实际值指向会话类中的一个类方法；然而，代码补全模型生成的内容错误地调用了会话类中不存在的方法。此错误可以视为代码补全中的一种幻觉。
- en: '![Refer to caption](img/9f6ff1444dd0f40630382fb5bcc7a1d0.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9f6ff1444dd0f40630382fb5bcc7a1d0.png)'
- en: 'Figure 9: An example of non-existent method call error.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 不存在的方法调用错误的示例。'
- en: A.6 Improper Method Invocation
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.6 不当方法调用
- en: 'Improper Method Invocation represents the situation where the call is made
    to an existing method, but a different, more appropriate method should have been
    used. Figure [10](#A1.F10 "Figure 10 ‣ A.6 Improper Method Invocation ‣ Appendix
    A Error Description and Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs") showcases an example
    of an Improper Method Invocation error. The code completion model generated a
    call to the class method Transformer within the llp class, whereas the correct
    content should have invoked the class method Geometric within the same class.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '不当方法调用（Improper Method Invocation）表示调用了一个已存在的方法，但应该使用一个不同的、更合适的方法。图 [10](#A1.F10
    "Figure 10 ‣ A.6 Improper Method Invocation ‣ Appendix A Error Description and
    Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") 展示了一个不当方法调用错误的示例。代码补全模型生成了对类方法 Transformer
    的调用，该方法位于 llp 类中，而正确的内容应该是在同一类中调用类方法 Geometric。'
- en: '![Refer to caption](img/8eb9665b71cb825dc9ce871156328a10.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/8eb9665b71cb825dc9ce871156328a10.png)'
- en: 'Figure 10: An example of improper method invocation error.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：不当方法调用错误示例。
- en: A.7 Missing Method Invocation
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.7 缺失方法调用
- en: 'Missing Method Invocation indicates that a function or method should have been
    called to achieve functionality, but the model failed to make this call. Figure
    [11](#A1.F11 "Figure 11 ‣ A.7 Missing Method Invocation ‣ Appendix A Error Description
    and Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") illustrates an example of a Missing
    Method Invocation error. The ground truth involves calling the class method paginate
    from the query class to obtain the queried variable. However, the code completion
    model failed to complete this method invocation and instead achieved the same
    functionality through multiple alternative class methods.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '缺失方法调用（Missing Method Invocation）表示应该调用某个函数或方法以实现功能，但模型未能进行此调用。图 [11](#A1.F11
    "Figure 11 ‣ A.7 Missing Method Invocation ‣ Appendix A Error Description and
    Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") 说明了一个缺失方法调用错误的示例。实际情况涉及从查询类中调用类方法
    paginate 以获取查询的变量。然而，代码补全模型未能完成此方法调用，而是通过多个替代类方法实现了相同的功能。'
- en: '![Refer to caption](img/adea5e3b43a5e644da334af94f496e6a.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/adea5e3b43a5e644da334af94f496e6a.png)'
- en: 'Figure 11: An example of missing method invocation error.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：缺失方法调用错误示例。
- en: A.8 Incorrect Content Generation
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.8 不正确内容生成
- en: 'Incorrect Content Generation represents the situation where the generated content
    is illogical, irrelevant to the current code context, or completely incorrect.
    Figure [12](#A1.F12 "Figure 12 ‣ A.8 Incorrect Content Generation ‣ Appendix A
    Error Description and Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World
    Code Completion with Repository-Level Pretrained Code LLMs") depicts an example
    of an Incorrect Content Generation error. The content produced by the code completion
    model is entirely unrelated to the ground truth and also lacks relevance to the
    current code context.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '不正确内容生成（Incorrect Content Generation）表示生成的内容不合逻辑、与当前代码上下文无关，或完全错误。图 [12](#A1.F12
    "Figure 12 ‣ A.8 Incorrect Content Generation ‣ Appendix A Error Description and
    Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") 描述了一个不正确内容生成错误的示例。代码补全模型生成的内容与实际情况完全无关，并且与当前代码上下文也缺乏相关性。'
- en: '![Refer to caption](img/5d47350813cabb23d7941c6d18ee5269.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/5d47350813cabb23d7941c6d18ee5269.png)'
- en: 'Figure 12: An example of incorrect content generation error.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：不正确内容生成错误示例。
- en: Appendix B Special Tokens & Prompt Templates
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 特殊标记与提示模板
- en: 'Table [6](#A2.T6 "Table 6 ‣ Appendix B Special Tokens & Prompt Templates ‣
    Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level
    Pretrained Code LLMs") shows the special tokens used by DeepseekCoder, Starcoder2,
    and CodeGemma for fill-in-the-middle code completion. The prompt templates for
    DeepseekCoder, Starcoder2, and CodeGemma are shown in Table [7](#A2.T7 "Table
    7 ‣ Appendix B Special Tokens & Prompt Templates ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs").
    Both Starcoder2 and CodeGemma utilize special tokens for segmenting code files,
    whereas DeepseekCoder does not employ such tokens, despite being trained on repository-level
    code data.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6](#A2.T6 "表 6 ‣ 附录 B 特殊标记和提示模板 ‣ 分层上下文修剪：使用库级预训练代码 LLM 优化真实世界代码补全") 显示了
    DeepseekCoder、Starcoder2 和 CodeGemma 用于填补中间代码的特殊标记。DeepseekCoder、Starcoder2 和
    CodeGemma 的提示模板显示在表 [7](#A2.T7 "表 7 ‣ 附录 B 特殊标记和提示模板 ‣ 分层上下文修剪：使用库级预训练代码 LLM 优化真实世界代码补全")
    中。Starcoder2 和 CodeGemma 都使用特殊标记来分隔代码文件，而 DeepseekCoder 尽管在库级代码数据上进行训练，但并不使用这些标记。
- en: '| Model |               Special Tokens |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |               特殊标记 |'
- en: '| --- | --- |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| DeepseekCoder | <&#124;fim_begin&#124;>,<&#124;fim_hole&#124;>,<&#124;fim_end&#124;>
    |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| DeepseekCoder | <&#124;fim_begin&#124;>,<&#124;fim_hole&#124;>,<&#124;fim_end&#124;>
    |'
- en: '| Starcoder2 | ,,,,,
    |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Starcoder2 | ,,,,,
    |'
- en: '| CodeGemma | <&#124;file_separator&#124;>,<&#124;fim_prefix&#124;>,<&#124;fim_suffix&#124;>,<&#124;fim_middle&#124;>
    |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| CodeGemma | <&#124;file_separator&#124;>,<&#124;fim_prefix&#124;>,<&#124;fim_suffix&#124;>,<&#124;fim_middle&#124;>
    |'
- en: 'Table 6: Special tokens used by DeepseekCoder, Starcoder2 and CodeGemma for
    fill-in-the-middle code completion.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：DeepseekCoder、Starcoder2 和 CodeGemma 用于填补中间代码的特殊标记。
- en: '| Model |             Fill-in-the-Middle Prompt Template |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |             填补中间提示模板 |'
- en: '| --- | --- |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| DeepseekCoder | #file_path0\ncode0\n#file_path1\ncode1\n#file_path2\ncode2\n#file_path3\n
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| DeepseekCoder | #file_path0\ncode0\n#file_path1\ncode1\n#file_path2\ncode2\n#file_path3\n
    |'
- en: '| <&#124;fim_begin&#124;>prefix_code<&#124;fim_hole&#124;>suffix_code<&#124;fim_end&#124;>
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| <&#124;fim_begin&#124;>prefix_code<&#124;fim_hole&#124;>suffix_code<&#124;fim_end&#124;>
    |'
- en: '| Starcoder2 | reponamefile_path0\ncode0file_path1
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| Starcoder2 | reponamefile_path0\ncode0file_path1
    |'
- en: '| prefix_codesuffix_code |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| prefix_codesuffix_code |'
- en: '| CodeGemma | <&#124;file_separator&#124;>file_path0\ncode0file_path1\n
    |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| CodeGemma | <&#124;file_separator&#124;>file_path0\ncode0file_path1\n
    |'
- en: '| <&#124;fim_prefix&#124;>prefix_code<&#124;fim_suffix&#124;>suffix_code<&#124;fim_middle&#124;>
    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| <&#124;fim_prefix&#124;>prefix_code<&#124;fim_suffix&#124;>suffix_code<&#124;fim_middle&#124;>
    |'
- en: 'Table 7: Prompt templates for DeepseekCoder, Starcoder2 and CodeGemma.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：DeepseekCoder、Starcoder2 和 CodeGemma 的提示模板。
- en: Appendix C Prompt Length Distribution
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 提示长度分布
- en: 'Table [8](#A3.T8 "Table 8 ‣ Appendix C Prompt Length Distribution ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") presents the average and median lengths of input sequences for three
    code completion models when utilizing contexts of varying dependency levels. Notably,
    Level $\infty$, which incorporates the entire repository code into the input,
    results in an average input sequence length exceeding 50,000, far surpassing the
    context window supported by these models. To more visually observe the changes
    in input sequence length with respect to dependency levels, Figure [13](#A3.F13
    "Figure 13 ‣ Appendix C Prompt Length Distribution ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs")
    was created. It is evident that the median input sequence length begins to converge
    once the dependency level reaches 2, and the average input sequence length also
    starts to stabilize after reaching a dependency level of 3.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [8](#A3.T8 "表 8 ‣ 附录 C 提示长度分布 ‣ 分层上下文修剪：使用库级预训练代码 LLM 优化真实世界代码补全") 展示了在使用不同依赖级别的上下文时，三种代码补全模型的输入序列的平均长度和中位数长度。值得注意的是，Level
    $\infty$ 将整个库代码纳入输入，导致平均输入序列长度超过 50,000，远远超出了这些模型支持的上下文窗口。为了更直观地观察输入序列长度随着依赖级别的变化，创建了图
    [13](#A3.F13 "图 13 ‣ 附录 C 提示长度分布 ‣ 分层上下文修剪：使用库级预训练代码 LLM 优化真实世界代码补全")。显然，中位数输入序列长度在依赖级别达到
    2 时开始趋于稳定，而平均输入序列长度在依赖级别达到 3 后也开始稳定。
- en: '![Refer to caption](img/1b8f8a3eda657bb047e0c6377ba8ef06.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1b8f8a3eda657bb047e0c6377ba8ef06.png)'
- en: 'Figure 13: The distribution of tokenized prompt lengths in the CrossCodeEval
    benchmark. The x-aixs represents the dependent level, and the y-axis represents
    the number of tokens. denotes the median value of the tokenized prompt length.
    denotes the average value of the tokenized prompt length.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: CrossCodeEval 基准测试中标记化提示长度的分布。x 轴表示依赖级别，y 轴表示标记数。表示标记化提示长度的中位数。表示标记化提示长度的平均值。'
- en: '| Model | CrossCodeEval Benchmark: Python |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | CrossCodeEval 基准测试: Python |'
- en: '| Level 0 | Level 1 | Level 2 | Level 3 | Level 4 | Level $\infty$ |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 级别 0 | 级别 1 | 级别 2 | 级别 3 | 级别 4 | 级别 $\infty$ |'
- en: '| Median | Average | Median | Average | Median | Average | Median | Average
    | Median | Average | Median | Average |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 中位数 | 平均值 | 中位数 | 平均值 | 中位数 | 平均值 | 中位数 | 平均值 | 中位数 | 平均值 | 中位数 | 平均值 |'
- en: '| DeepseekCoder | 1,445 | 2,272 | 3,161 | 5,248 | 4,434 | 7,938 | 4,559 | 8,967
    | 4,640 | 9,252 | 44,475 | 58,217 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| DeepseekCoder | 1,445 | 2,272 | 3,161 | 5,248 | 4,434 | 7,938 | 4,559 | 8,967
    | 4,640 | 9,252 | 44,475 | 58,217 |'
- en: '| Starcoder2 | 1,245 | 1,967 | 2,694 | 4,513 | 3,796 | 6,815 | 3,904 | 7,695
    | 3,958 | 7,940 | 38,174 | 50,632 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Starcoder2 | 1,245 | 1,967 | 2,694 | 4,513 | 3,796 | 6,815 | 3,904 | 7,695
    | 3,958 | 7,940 | 38,174 | 50,632 |'
- en: '| CodeGemma | 1,309 | 2,050 | 2,817 | 4,739 | 3,989 | 7,179 | 4,110 | 8,112
    | 4,164 | 8,369 | 39,647 | 52,875 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| CodeGemma | 1,309 | 2,050 | 2,817 | 4,739 | 3,989 | 7,179 | 4,110 | 8,112
    | 4,164 | 8,369 | 39,647 | 52,875 |'
- en: 'Table 8: The median and average tokenized prompt lengths of the DeepseekCoder,
    Starcoder2 and CodeGemma models on the CrossCodeEval: Python benchmark.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: DeepseekCoder、Starcoder2 和 CodeGemma 模型在 CrossCodeEval: Python 基准测试中的标记化提示长度的中位数和平均值。'
- en: Appendix D Dependency Level Analysis
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 依赖级别分析
- en: D.1 Complete Experimental Results
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 完整实验结果
- en: 'Table [8](#A3.T8 "Table 8 ‣ Appendix C Prompt Length Distribution ‣ Hierarchical
    Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained
    Code LLMs") documents the comprehensive experimental results of repository file
    dependency analyses across six code completion models. It is observed that when
    the length of the input sequence exceeds the model’s context window, there is
    a significant decrease in completion accuracy. However, truncating the input sequence
    from the left to fit within the model’s context window size reveals that greater
    amounts of code repository content can enhance completion accuracy. Additionally,
    it was found that the DeepseekCoder-1.3B model exhibits a severe performance degradation
    in completion accuracy as the number of repository files increases.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [8](#A3.T8 "表 8 ‣ 附录 C 提示长度分布 ‣ 层次上下文剪枝：优化具有库级预训练代码 LLM 的实际代码完成") 记录了六个代码完成模型在库文件依赖分析中的全面实验结果。观察到当输入序列的长度超过模型的上下文窗口时，完成准确度显著下降。然而，将输入序列从左侧截断以适应模型的上下文窗口大小显示，更多的代码库内容可以提高完成准确度。此外，还发现
    DeepseekCoder-1.3B 模型在库文件数量增加时表现出严重的完成准确度下降。
- en: '| Dependency | Topological Dependency Analysis |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 依赖性 | 拓扑依赖分析 |'
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
- en: '| Dep-Level: 0 | 16.72 | 56.60 | 28.14 | 68.40 | 21.92 | 61.45 | 23.16 | 63.62
    | 20.60 | 55.97 | 30.40 | 69.76 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 依赖级别: 0 | 16.72 | 56.60 | 28.14 | 68.40 | 21.92 | 61.45 | 23.16 | 63.62 |
    20.60 | 55.97 | 30.40 | 69.76 |'
- en: '| \hdashline   + left truncate | 16.72 | 56.58 | 28.14 | 68.36 | 21.92 | 61.49
    | 22.98 | 63.58 | 20.64 | 56.26 | 30.58 | 70.36 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline   + 左截断 | 16.72 | 56.58 | 28.14 | 68.36 | 21.92 | 61.49 | 22.98
    | 63.58 | 20.64 | 56.26 | 30.58 | 70.36 |'
- en: '| Dep-Level: 1 | 14.99 | 54.33 | 32.20 | 68.57 | 26.33 | 64.54 | 28.66 | 67.00
    | 23.16 | 55.00 | 32.17 | 65.77 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 依赖级别: 1 | 14.99 | 54.33 | 32.20 | 68.57 | 26.33 | 64.54 | 28.66 | 67.00 |
    23.16 | 55.00 | 32.17 | 65.77 |'
- en: '| \hdashline   + left truncate | 15.44 | 55.03 | 33.03 | 70.77 | 26.18 | 64.15
    | 28.51 | 66.91 | 24.37 | 58.79 | 34.65 | 73.01 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline   + 左截断 | 15.44 | 55.03 | 33.03 | 70.77 | 26.18 | 64.15 | 28.51
    | 66.91 | 24.37 | 58.79 | 34.65 | 73.01 |'
- en: '| Dep-Level: 2 | 12.73 | 51.72 | 30.21 | 65.46 | 26.63 | 64.50 | 29.83 | 67.03
    | 21.24 | 49.62 | 28.36 | 57.76 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 依赖级别: 2 | 12.73 | 51.72 | 30.21 | 65.46 | 26.63 | 64.50 | 29.83 | 67.03 |
    21.24 | 49.62 | 28.36 | 57.76 |'
- en: '| \hdashline   + left truncate | 13.63 | 53.45 | 33.56 | 70.74 | 26.70 | 64.58
    | 29.45 | 67.03 | 25.31 | 59.27 | 35.67 | 73.26 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline   + 左截断 | 13.63 | 53.45 | 33.56 | 70.74 | 26.70 | 64.58 | 29.45
    | 67.03 | 25.31 | 59.27 | 35.67 | 73.26 |'
- en: '| Dep-Level: 3 | 12.28 | 50.90 | 28.93 | 63.67 | 26.74 | 64.52 | 29.42 | 66.58
    | 20.30 | 47.64 | 27.16 | 55.66 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 依赖级别：3 | 12.28 | 50.90 | 28.93 | 63.67 | 26.74 | 64.52 | 29.42 | 66.58 |
    20.30 | 47.64 | 27.16 | 55.66 |'
- en: '| \hdashline   + left truncate | 13.26 | 53.17 | 33.07 | 70.51 | 26.82 | 64.56
    | 29.23 | 67.01 | 25.35 | 59.30 | 35.93 | 73.34 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline   + 左截断 | 13.26 | 53.17 | 33.07 | 70.51 | 26.82 | 64.56 | 29.23
    | 67.01 | 25.35 | 59.30 | 35.93 | 73.34 |'
- en: '| Dep-Level: 4 | 12.13 | 50.69 | 28.44 | 63.15 | 26.48 | 64.30 | 29.68 | 66.84
    | 20.08 | 47.29 | 26.93 | 55.16 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 依赖级别：4 | 12.13 | 50.69 | 28.44 | 63.15 | 26.48 | 64.30 | 29.68 | 66.84 |
    20.08 | 47.29 | 26.93 | 55.16 |'
- en: '| \hdashline   + left truncate | 13.37 | 53.20 | 33.22 | 70.57 | 26.59 | 64.46
    | 29.53 | 67.07 | 25.54 | 59.42 | 36.12 | 73.54 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline   + 左截断 | 13.37 | 53.20 | 33.22 | 70.57 | 26.59 | 64.46 | 29.53
    | 67.07 | 25.54 | 59.42 | 36.12 | 73.54 |'
- en: '| Dep-Level: $\infty$ | 1.32 | 28.04 | 7.08 | 17.53 | 18.19 | 51.92 | 24.52
    | 54.73 | 1.54 | 6.17 | 1.85 | 3.88 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 依赖级别：$\infty$ | 1.32 | 28.04 | 7.08 | 17.53 | 18.19 | 51.92 | 24.52 | 54.73
    | 1.54 | 6.17 | 1.85 | 3.88 |'
- en: '| \hdashline   + left truncate | 5.76 | 46.22 | 35.29 | 71.51 | 30.43 | 67.34
    | 33.03 | 69.57 | 29.08 | 62.91 | 39.32 | 75.35 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline   + 左截断 | 5.76 | 46.22 | 35.29 | 71.51 | 30.43 | 67.34 | 33.03
    | 69.57 | 29.08 | 62.91 | 39.32 | 75.35 |'
- en: 'Table 9: Comparison of completion results using different context dependency
    levels across 6 models. EM denotes Exact Match, and ES denotes Edit Similarity.
    *$\infty$* denotes the prompt including all files in the repository. *+left truncate*
    denotes the prompt is truncated to the max context window of LLMs from the left.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 9：使用不同上下文依赖级别在 6 种模型中的补全结果比较。EM 表示精确匹配，ES 表示编辑相似度。*$\infty$* 表示提示包括库中的所有文件。*+左截断*
    表示提示从左侧截断到 LLMs 的最大上下文窗口。
- en: D.2 Hit Count Changes
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 命中次数变化
- en: 'Table [10](#A4.T10 "Table 10 ‣ D.2 Hit Count Changes ‣ D.1 Complete Experimental
    Results ‣ Appendix D Dependency Level Analysis ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs")
    collates the variations in correct and incorrect completions across six code completion
    models when input contexts of different dependency levels are used. It is evident
    that as the dependency level increases, the variations in the model’s completion
    results become more stable. This stability arises because the changes in the model’s
    input context diminish as the dependency level is elevated. This also indicates
    that augmenting the model’s input with additional content can enhance completion
    accuracy, albeit at the risk of turning some originally correct completions into
    incorrect ones.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [10](#A4.T10 "表格 10 ‣ D.2 命中次数变化 ‣ D.1 完整实验结果 ‣ 附录 D 依赖级别分析 ‣ 层次化上下文修剪：利用库级预训练代码
    LLMs 优化真实世界代码补全") 汇总了在使用不同依赖级别的输入上下文时，六种代码补全模型的正确和错误补全的变化。显然，随着依赖级别的增加，模型补全结果的变化变得更加稳定。这种稳定性产生的原因是，随着依赖级别的提升，模型输入上下文的变化减少。这也表明，通过额外内容增强模型的输入可以提高补全准确性，但也有可能将一些原本正确的补全变为错误补全。
- en: We also observed that the DeepseekCoder series of models lack special tokens
    for delineating repository files; however, this deficiency does not result in
    more pronounced fluctuations in the outcomes. This suggests that the DeepseekCoder
    models are capable of effectively distinguishing between different files in the
    repository, even without the aid of special tokens.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还观察到，DeepseekCoder 系列模型缺少用于区分库文件的特殊标记；然而，这一缺陷并没有导致结果出现更明显的波动。这表明，DeepseekCoder
    模型能够在没有特殊标记的情况下有效区分库中的不同文件。
- en: '| XF-Context | Hit Count Changes |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| XF-Context | 命中次数变化 |'
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
- en: '| Infile-Only | +444 | +747 | +582 | +610 | +548 | +812 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 仅文件内 | +444 | +747 | +582 | +610 | +548 | +812 |'
- en: '|    0 $\rightarrow$ 1 | -108  +74 | -47  +177 | -44  +157 | -37  +184 | -31  +130
    | -68  +176 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|    0 $\rightarrow$ 1 | -108  +74 | -47  +177 | -44  +157 | -37  +184 | -31  +130
    | -68  +176 |'
- en: '| Level: 1 | +408 | +877 | +695 | +755 | +647 | +920 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 级别：1 | +408 | +877 | +695 | +755 | +647 | +920 |'
- en: '|    1 $\rightarrow$ 2 | -61  +13 | -33  +47 | -41  +55 | -33  +58 | -30  +55
    | -44  +71 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|    1 $\rightarrow$ 2 | -61  +13 | -33  +47 | -41  +55 | -33  +58 | -30  +55
    | -44  +71 |'
- en: '| Level: 2 | +362 | +891 | +709 | +782 | +672 | +947 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 级别：2 | +362 | +891 | +709 | +782 | +672 | +947 |'
- en: '|    2 $\rightarrow$ 3 | -15  +5 | -20  +7 | -13  +16 | -19  +13 | -10  +11
    | -11  +18 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|    2 $\rightarrow$ 3 | -15  +5 | -20  +7 | -13  +16 | -19  +13 | -10  +11
    | -11  +18 |'
- en: '| Level: 3 | +352 | +878 | +712 | +776 | +673 | +954 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 级别：3 | +352 | +878 | +712 | +776 | +673 | +954 |'
- en: '|    3 $\rightarrow$ 4 | -3  +6 | -1  +5 | -10  +4 | -3  +11 | -3  +8 | -5  +10
    |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|    3 $\rightarrow$ 4 | -3  +6 | -1  +5 | -10  +4 | -3  +11 | -3  +8 | -5  +10
    |'
- en: '| Level: 4 | +355 | +882 | +706 | +784 | +678 | +959 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| Level: 4 | +355 | +882 | +706 | +784 | +678 | +959 |'
- en: '| 4 $\rightarrow$ | -238  +36 | -135  +190 | -55  +157 | -68  +161 | -45  +139
    | -72  +157 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 4 $\rightarrow$ | -238  +36 | -135  +190 | -55  +157 | -68  +161 | -45  +139
    | -72  +157 |'
- en: '| Level: $\infty$ | +153 | +937 | +808 | +877 | +772 | +1044 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| Level: $\infty$ | +153 | +937 | +808 | +877 | +772 | +1044 |'
- en: 'Table 10: The changes in the hit counts of correct and incorrect completions
    across six code completion models when using different context dependency levels.
    The green values denote the number of test samples that were originally correct
    but became incorrect as the dependency level of the input context increased. The
    red values represent the number of test samples that were initially incorrect
    but became correct with the elevation of the input context’s dependency level.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: 在使用不同上下文依赖级别的情况下，六种代码补全模型的正确和错误完成的命中次数变化。绿色值表示测试样本在原本正确但随着输入上下文的依赖级别增加而变为错误的数量。红色值表示测试样本在初始时错误但随着输入上下文依赖级别的提升变为正确的数量。'
- en: '| XF-Context | Hierarchical Context Pruning (Top-p: 1.0) |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| XF-Context | 分层上下文剪枝（Top-p: 1.0） |'
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
- en: '| Random-All | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| Random-All | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
- en: '| Top-k: 5 | 9.64 | 49.78 | 39.74 | 73.90 | 32.68 | 69.05 | 35.76 | 71.41 |
    31.26 | 63.74 | 42.44 | 76.95 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| Top-k: 5 | 9.64 | 49.78 | 39.74 | 73.90 | 32.68 | 69.05 | 35.76 | 71.41 |
    31.26 | 63.74 | 42.44 | 76.95 |'
- en: '| Top-k: 10 | 9.91 | 49.85 | 40.30 | 74.56 | 34.15 | 69.37 | 36.47 | 71.50
    | 31.82 | 64.34 | 42.63 | 77.35 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| Top-k: 10 | 9.91 | 49.85 | 40.30 | 74.56 | 34.15 | 69.37 | 36.47 | 71.50
    | 31.82 | 64.34 | 42.63 | 77.35 |'
- en: '| Top-k: 15 | 9.23 | 49.23 | 40.75 | 74.59 | 33.96 | 69.41 | 36.62 | 71.59
    | 31.86 | 64.53 | 42.55 | 77.06 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| Top-k: 15 | 9.23 | 49.23 | 40.75 | 74.59 | 33.96 | 69.41 | 36.62 | 71.59
    | 31.86 | 64.53 | 42.55 | 77.06 |'
- en: '| Top-k: 20 | 9.01 | 48.95 | 41.24 | 74.57 | 34.37 | 69.81 | 36.66 | 71.56
    | 31.93 | 64.67 | 42.85 | 77.52 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| Top-k: 20 | 9.01 | 48.95 | 41.24 | 74.57 | 34.37 | 69.81 | 36.66 | 71.56
    | 31.93 | 64.67 | 42.85 | 77.52 |'
- en: '| Top-k: 25 | 8.93 | 48.82 | 40.34 | 74.47 | 33.73 | 69.62 | 37.00 | 71.91
    | 32.46 | 64.81 | 43.11 | 77.47 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| Top-k: 25 | 8.93 | 48.82 | 40.34 | 74.47 | 33.73 | 69.62 | 37.00 | 71.91
    | 32.46 | 64.81 | 43.11 | 77.47 |'
- en: '| Top-k: 30 | 8.44 | 48.48 | 39.74 | 74.17 | 33.28 | 69.42 | 36.14 | 71.29
    | 32.46 | 64.81 | 42.44 | 77.20 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| Top-k: 30 | 8.44 | 48.48 | 39.74 | 74.17 | 33.28 | 69.42 | 36.14 | 71.29
    | 32.46 | 64.81 | 42.44 | 77.20 |'
- en: 'Table 11: The results of completion using hierarchical context pruning with
    different top-k values.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: 使用不同 top-k 值进行分层上下文剪枝的完成结果。'
- en: '| XF-Context | Hierarchical Context Pruning (Top-k: 5) |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| XF-Context | 分层上下文剪枝（Top-k: 5） |'
- en: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| DScoder-1.3B | DScoder-6.7B | Starcoder2-3B | Starcoder2-7B | CodeGemma-2B
    | CodeGemma-7B |'
- en: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| EM | ES | EM | ES | EM | ES | EM | ES | EM | ES | EM | ES |'
- en: '| Random-All | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Random-All | 6.18 | 46.19 | 33.94 | 70.98 | 28.32 | 66.87 | 31.45 | 69.09
    | 26.93 | 62.13 | 36.69 | 74.42 |'
- en: '| Top-p: 0.1 | 14.27 | 53.94 | 37.85 | 73.11 | 32.99 | 68.75 | 34.16 | 70.43
    | 29.19 | 62.09 | 40.98 | 76.26 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| Top-p: 0.1 | 14.27 | 53.94 | 37.85 | 73.11 | 32.99 | 68.75 | 34.16 | 70.43
    | 29.19 | 62.09 | 40.98 | 76.26 |'
- en: '| Top-p: 0.2 | 13.52 | 53.20 | 38.04 | 73.13 | 33.15 | 68.59 | 34.84 | 70.40
    | 29.72 | 62.32 | 40.94 | 76.25 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| Top-p: 0.2 | 13.52 | 53.20 | 38.04 | 73.13 | 33.15 | 68.59 | 34.84 | 70.40
    | 29.72 | 62.32 | 40.94 | 76.25 |'
- en: '| Top-p: 0.3 | 12.88 | 52.60 | 38.49 | 73.19 | 32.84 | 68.31 | 35.22 | 70.64
    | 30.13 | 62.77 | 41.21 | 76.20 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| Top-p: 0.3 | 12.88 | 52.60 | 38.49 | 73.19 | 32.84 | 68.31 | 35.22 | 70.64
    | 30.13 | 62.77 | 41.21 | 76.20 |'
- en: '| Top-p: 0.4 | 11.60 | 51.58 | 38.42 | 72.92 | 32.81 | 68.51 | 35.07 | 70.55
    | 30.40 | 62.97 | 40.98 | 76.30 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| Top-p: 0.4 | 11.60 | 51.58 | 38.42 | 72.92 | 32.81 | 68.51 | 35.07 | 70.55
    | 30.40 | 62.97 | 40.98 | 76.30 |'
- en: '| Top-p: 0.5 | 11.49 | 51.50 | 38.95 | 73.31 | 32.88 | 68.67 | 34.73 | 70.33
    | 30.17 | 62.73 | 41.32 | 76.15 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| Top-p: 0.5 | 11.49 | 51.50 | 38.95 | 73.31 | 32.88 | 68.67 | 34.73 | 70.33
    | 30.17 | 62.73 | 41.32 | 76.15 |'
- en: '| Top-p: 0.6 | 11.00 | 51.14 | 38.87 | 73.33 | 32.32 | 68.56 | 34.73 | 70.38
    | 30.02 | 63.15 | 41.58 | 76.20 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| Top-p: 0.6 | 11.00 | 51.14 | 38.87 | 73.33 | 32.32 | 68.56 | 34.73 | 70.38
    | 30.02 | 63.15 | 41.58 | 76.20 |'
- en: '| Top-p: 0.7 | 11.11 | 51.21 | 38.83 | 73.52 | 31.94 | 68.14 | 34.92 | 70.74
    | 30.47 | 63.13 | 41.77 | 76.40 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| Top-p: 0.7 | 11.11 | 51.21 | 38.83 | 73.52 | 31.94 | 68.14 | 34.92 | 70.74
    | 30.47 | 63.13 | 41.77 | 76.40 |'
- en: '| Top-p: 0.8 | 10.40 | 50.39 | 38.95 | 73.56 | 31.79 | 68.34 | 34.80 | 70.59
    | 30.17 | 63.05 | 41.81 | 76.41 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| Top-p: 0.8 | 10.40 | 50.39 | 38.95 | 73.56 | 31.79 | 68.34 | 34.80 | 70.59
    | 30.17 | 63.05 | 41.81 | 76.41 |'
- en: '| Top-p: 0.9 | 10.40 | 50.08 | 38.61 | 73.13 | 31.94 | 68.26 | 34.54 | 70.22
    | 30.43 | 63.18 | 41.81 | 76.51 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| Top-p: 0.9 | 10.40 | 50.08 | 38.61 | 73.13 | 31.94 | 68.26 | 34.54 | 70.22
    | 30.43 | 63.18 | 41.81 | 76.51 |'
- en: 'Table 12: The results of completion using hierarchical context pruning with
    different top-p values.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：使用不同 top-p 值进行层次上下文剪枝的补全结果。
- en: Appendix E Complete Funcation-Level Sampling Experiment Results
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 完整的函数级采样实验结果
- en: 'Due to space constraints, we report only a subset of the results from the function-level
    sampling experiments in the main body. Tables [11](#A4.T11 "Table 11 ‣ D.2 Hit
    Count Changes ‣ D.1 Complete Experimental Results ‣ Appendix D Dependency Level
    Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") and [12](#A4.T12 "Table 12 ‣ D.2
    Hit Count Changes ‣ D.1 Complete Experimental Results ‣ Appendix D Dependency
    Level Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") provide a comprehensive statistical
    overview of the complete sampling experiments.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 由于篇幅限制，我们在正文中仅报告了函数级采样实验结果的一个子集。表格 [11](#A4.T11 "表 11 ‣ D.2 命中次数变化 ‣ D.1 完整实验结果
    ‣ 附录 D 依赖级别分析 ‣ 层次上下文剪枝：通过库级预训练代码 LLMs 优化实际代码补全") 和 [12](#A4.T12 "表 12 ‣ D.2 命中次数变化
    ‣ D.1 完整实验结果 ‣ 附录 D 依赖级别分析 ‣ 层次上下文剪枝：通过库级预训练代码 LLMs 优化实际代码补全") 提供了完整采样实验的综合统计概述。
- en: E.1 Top-k Sampling
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 Top-k 采样
- en: 'Table [11](#A4.T11 "Table 11 ‣ D.2 Hit Count Changes ‣ D.1 Complete Experimental
    Results ‣ Appendix D Dependency Level Analysis ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs")
    details the results of top-k sampling, where top-p is fixed at 1.0. It is observed
    that increasing the value of k does not significantly enhance the accuracy of
    completions; improvements become negligible when k exceeds 5.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [11](#A4.T11 "表 11 ‣ D.2 命中次数变化 ‣ D.1 完整实验结果 ‣ 附录 D 依赖级别分析 ‣ 层次上下文剪枝：通过库级预训练代码
    LLMs 优化实际代码补全") 详细说明了 top-k 采样的结果，其中 top-p 固定为 1.0。观察到，增加 k 值并不会显著提高补全的准确性；当 k
    超过 5 时，改进效果变得微乎其微。
- en: E.2 Top-p Sampling
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 Top-p 采样
- en: 'Table [12](#A4.T12 "Table 12 ‣ D.2 Hit Count Changes ‣ D.1 Complete Experimental
    Results ‣ Appendix D Dependency Level Analysis ‣ Hierarchical Context Pruning:
    Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs"),
    on the other hand, presents the outcomes of top-p sampling with top-k fixed at
    5. Here, increasing the value of p does not yield significant improvements, particularly
    when p exceeds 0.3.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [12](#A4.T12 "表 12 ‣ D.2 命中次数变化 ‣ D.1 完整实验结果 ‣ 附录 D 依赖级别分析 ‣ 层次上下文剪枝：通过库级预训练代码
    LLMs 优化实际代码补全") 则展示了 top-p 采样的结果，其中 top-k 固定为 5。在这里，增加 p 值不会带来显著的改进，特别是当 p 超过
    0.3 时。
- en: Appendix F Dependency Search Algorithm
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 依赖搜索算法
- en: 'Algorithm [1](#alg1 "Algorithm 1 ‣ Appendix F Dependency Search Algorithm ‣
    D.2 Hit Count Changes ‣ D.1 Complete Experimental Results ‣ Appendix D Dependency
    Level Analysis ‣ Hierarchical Context Pruning: Optimizing Real-World Code Completion
    with Repository-Level Pretrained Code LLMs") delineates the specific process we
    employed for dependency modeling within code repositories. For more detailed implementation
    specifics, please visit our code repository.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 [1](#alg1 "算法 1 ‣ 附录 F 依赖搜索算法 ‣ D.2 命中次数变化 ‣ D.1 完整实验结果 ‣ 附录 D 依赖级别分析 ‣ 层次上下文剪枝：通过库级预训练代码
    LLMs 优化实际代码补全") 描述了我们在代码库中进行依赖建模时所采用的具体过程。有关更详细的实施细节，请访问我们的代码库。
- en: Algorithm 1 Dependency Search Algorithm for Python Files
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 Python 文件的依赖搜索算法
- en: '1:Input: file - initial Python file, maxDepth - maximum search depth2:Output:
    List of dependent files3:function FindDependencies(file, maxDepth)4:     queue
    = [(file, 0)]5:     visited = set()6:     while queue do7:         currentFile,
    currentDepth = queue.pop(0)8:         if currentDepth > maxDepth then9:              break10:         end if11:         imports
    = extractImports(currentFile)12:         for imp in imports do13:              if imp
    is local and imp not in visited then14:                  visited.add(imp)15:                  queue.append((imp,
    currentDepth + 1))16:              end if17:         end for18:     end while19:     return
    visited20:end function21:function extractImports(file)22:     Use Tree-Sitter
    to parse file and extract all import statements23:     return list of imports24:end function'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '1:输入: 文件 - 初始 Python 文件, maxDepth - 最大搜索深度2:输出: 依赖文件列表3:函数 FindDependencies(file,
    maxDepth)4:     queue = [(file, 0)]5:     visited = set()6:     while queue do7:         currentFile,
    currentDepth = queue.pop(0)8:         if currentDepth > maxDepth then9:              break10:         end
    if11:         imports = extractImports(currentFile)12:         for imp in imports
    do13:              if imp is local and imp not in visited then14:                  visited.add(imp)15:                  queue.append((imp,
    currentDepth + 1))16:              end if17:         end for18:     end while19:     return
    visited20:end function21:函数 extractImports(file)22:     使用 Tree-Sitter 解析文件并提取所有
    import 语句23:     返回 import 列表24:end function'
- en: Appendix G Additional Related Work
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 其他相关工作
- en: G.1 Long Context Code Large Language Models
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.1 长上下文代码大型语言模型
- en: Research on optimizing large language models for long contexts has been underway
    for some time, with many innovative long-context optimization techniques (Chen
    et al., [2023](#bib.bib9), [2024b](#bib.bib10); Xiao et al., [2024](#bib.bib44);
    Ding et al., [2024](#bib.bib12); Chen et al., [2024a](#bib.bib7)) and evaluation
    sets (Bai et al., [2023](#bib.bib3); Zhang et al., [2023b](#bib.bib51), [2024a](#bib.bib52))
    being proposed and widely applied. Some Code LLMs (Guo et al., [2024](#bib.bib15);
    Lozhkov et al., [2024](#bib.bib26)) utilize these techniques for fine-tuning to
    extend their context windows. Larger context windows allow Code LLMs to receive
    and process more complex code content, such as repository-level code completion
    and repository-level code repair (Ding et al., [2023](#bib.bib11); Liu et al.,
    [2023](#bib.bib25); Zhang et al., [2024b](#bib.bib53); Li et al., [2024a](#bib.bib20)).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 关于优化大语言模型以处理长上下文的研究已经进行了一段时间，提出了许多创新的长上下文优化技术（Chen et al., [2023](#bib.bib9),
    [2024b](#bib.bib10); Xiao et al., [2024](#bib.bib44); Ding et al., [2024](#bib.bib12);
    Chen et al., [2024a](#bib.bib7)）和评估集（Bai et al., [2023](#bib.bib3); Zhang et al.,
    [2023b](#bib.bib51), [2024a](#bib.bib52)），并被广泛应用。一些代码 LLM（Guo et al., [2024](#bib.bib15);
    Lozhkov et al., [2024](#bib.bib26)）利用这些技术进行微调，以扩展其上下文窗口。更大的上下文窗口使代码 LLM 能够接收和处理更复杂的代码内容，例如库级代码完成和库级代码修复（Ding
    et al., [2023](#bib.bib11); Liu et al., [2023](#bib.bib25); Zhang et al., [2024b](#bib.bib53);
    Li et al., [2024a](#bib.bib20)）。
- en: G.2 Code Agent
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.2 代码代理
- en: Research on Code Agents (Yang et al., [2024](#bib.bib46); Fang et al., [2024](#bib.bib13);
    Thakur et al., [2024](#bib.bib39); Shi et al., [2024](#bib.bib35)) focuses on
    developing intelligent systems that assist in software development by automating
    tasks like code generation and debugging (Holt et al., [2024](#bib.bib17); Wang
    et al., [2023b](#bib.bib43); Yin et al., [2022](#bib.bib48); Zelikman et al.,
    [2024](#bib.bib49)) . The use of Code LLMs has proven effective in understanding
    complex code structures and semantics (Wang et al., [2024a](#bib.bib40); Yang
    et al., [2023](#bib.bib47)). These models have been further refined to handle
    specific software development tasks, including repository-level code analysis
    and automated error correction (Jimenez et al., [2024](#bib.bib18)).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 代码代理的研究（Yang et al., [2024](#bib.bib46); Fang et al., [2024](#bib.bib13); Thakur
    et al., [2024](#bib.bib39); Shi et al., [2024](#bib.bib35)）集中于开发智能系统，这些系统通过自动化任务（如代码生成和调试）（Holt
    et al., [2024](#bib.bib17); Wang et al., [2023b](#bib.bib43); Yin et al., [2022](#bib.bib48);
    Zelikman et al., [2024](#bib.bib49)）来协助软件开发。使用代码 LLM 已被证明在理解复杂的代码结构和语义方面有效（Wang
    et al., [2024a](#bib.bib40); Yang et al., [2023](#bib.bib47)）。这些模型已进一步完善，以处理特定的软件开发任务，包括库级代码分析和自动化错误修正（Jimenez
    et al., [2024](#bib.bib18)）。
