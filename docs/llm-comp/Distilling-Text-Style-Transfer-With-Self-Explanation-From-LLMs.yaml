- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:59:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:59:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Distilling Text Style Transfer With Self-Explanation From LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从LLMs中提炼带有自我解释的文本风格迁移
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01106](https://ar5iv.labs.arxiv.org/html/2403.01106)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01106](https://ar5iv.labs.arxiv.org/html/2403.01106)
- en: Chiyu Zhang^(1,2,⋆)    Honglong Cai²    Yuezhang (Music) Li²    Yuexin Wu²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 张启宇^(1,2,⋆)    蔡宏龙²    李悦璋（音乐）²    吴月鑫²
- en: Le Hou²     Muhammad Abdul-Mageed^(1,3)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 侯乐²    穆罕默德·阿卜杜勒-马吉德^(1,3)
- en: ¹ The University of British Columbia    ² Google
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 英属哥伦比亚大学    ² 谷歌
- en: ³Department of NLP & ML, MBZUAI
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³自然语言处理与机器学习系，MBZUAI
- en: chiyuzh@mail.ubc.ca, {honglongcai, lyzmuisc}@google.com
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: chiyuzh@mail.ubc.ca, {honglongcai, lyzmuisc}@google.com
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Text Style Transfer (TST) seeks to alter the style of text while retaining its
    core content. Given the constraints of limited parallel datasets for TST, we propose
    CoTeX, a framework that leverages large language models (LLMs) alongside chain-of-thought
    (CoT) prompting to facilitate TST. CoTeX distills the complex rewriting and reasoning
    capabilities of LLMs into more streamlined models capable of working with both
    non-parallel and parallel data. Through experimentation across four TST datasets,
    CoTeX is shown to surpass traditional supervised fine-tuning and knowledge distillation
    methods, particularly in low-resource settings. We conduct a comprehensive evaluation,
    comparing CoTeX against current unsupervised, supervised, in-context learning
    (ICL) techniques, and instruction-tuned LLMs. Furthermore, CoTeX distinguishes
    itself by offering transparent explanations for its style transfer process.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 文本风格迁移（TST）旨在在保留核心内容的同时改变文本风格。鉴于TST的并行数据集有限的限制，我们提出了CoTeX，一个利用大型语言模型（LLMs）结合思维链（CoT）提示来促进TST的框架。CoTeX将LLMs复杂的重写和推理能力提炼成更简化的模型，能够处理非并行和并行数据。通过对四个TST数据集的实验，CoTeX被证明超越了传统的监督微调和知识蒸馏方法，特别是在资源稀缺的环境中。我们进行了全面的评估，将CoTeX与当前的无监督、监督、上下文学习（ICL）技术和指令调优LLMs进行了比较。此外，CoTeX通过提供其风格迁移过程的透明解释来区分自己。
- en: Distilling Text Style Transfer With Self-Explanation From LLMs
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从LLMs中提炼带有自我解释的文本风格迁移
- en: Chiyu Zhang^(1,2,⋆)    Honglong Cai²    Yuezhang (Music) Li²    Yuexin Wu² Le
    Hou²     Muhammad Abdul-Mageed^(1,3) ¹ The University of British Columbia    ² Google
    ³Department of NLP & ML, MBZUAI chiyuzh@mail.ubc.ca, {honglongcai, lyzmuisc}@google.com
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 张启宇^(1,2,⋆)    蔡宏龙²    李悦璋（音乐）²    吴月鑫² 侯乐²    穆罕默德·阿卜杜勒-马吉德^(1,3) ¹ 英属哥伦比亚大学
       ² 谷歌 ³自然语言处理与机器学习系，MBZUAI chiyuzh@mail.ubc.ca, {honglongcai, lyzmuisc}@google.com
- en: ^†^† ^⋆Work done during internship at Google.![Refer to caption](img/8ad259d2ddbe1da444850d246349cc0a.png)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^† ^⋆工作在谷歌实习期间完成。![参考说明](img/8ad259d2ddbe1da444850d246349cc0a.png)
- en: 'Figure 1: Overview of CoTeX framework. We use few-shot CoT prompting to generate
    reasoning paths and transferred texts from an LLM and then train a smaller task-specific
    model with generated data.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：CoTeX框架概述。我们使用少量样本的CoT提示来生成推理路径和从LLM转移的文本，然后用生成的数据训练一个更小的任务特定模型。
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: TST aims to rephrase a source text $s$ Jin et al. ([2022](#bib.bib17)). The
    term “style" can encompass the personal characteristics of an author, such as
    age, and pragmatic use like formality or toxicity. To develop TST systems using
    supervised methods, several human-annotated datasets have emerged Rao and Tetreault
    ([2018](#bib.bib37)). For instance, Rao and Tetreault ([2018](#bib.bib37)) introduced
    a corpus for formality style transfer, transforming informal language to its formal
    counterpart and vice versa. Nonetheless, supervised parallel data, crucial for
    training deep neural networks, is scarce and costly to obtain. Hence, unsupervised
    methodologies Shen et al. ([2017](#bib.bib42)); Liu et al. ([2021](#bib.bib24))
    have been proposed to manage stylistic attributes without relying on parallel
    data. Liu et al. ([2022](#bib.bib22)) and Zhang et al. ([2020](#bib.bib58)) create
    pseudo-parallel data from unlabeled samples via diverse data augmentation with
    task-specific knowledge. Works by Gong et al. ([2019](#bib.bib9)); Wang et al.
    ([2019a](#bib.bib47)); Reid and Zhong ([2021](#bib.bib38)) employ an auxiliary
    style classifier to steer the transfer direction. Meanwhile, Krishna et al. ([2020](#bib.bib18))
    and Hallinan et al. ([2023b](#bib.bib11)) deploy multiple style-specific models
    to produce various styles individually. Of late, LLMs have demonstrated exceptional
    prowess across diverse NLP tasks. Studies like Reif et al. ([2022](#bib.bib39));
    Pu and Demberg ([2023](#bib.bib35)) have found that extremely large LMs, with
    over 100B parameters, are adept at TST with ICL. Drawing from these findings,
    our paper uses LLMs to generate pseudo-parallel data and distills the TST skills
    of the LLM into a compact student model. Moreover, we enhance distillation and
    efficiency using CoT prompting.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: TST旨在重新措辞源文本$s$ Jin等人（[2022](#bib.bib17)）。“风格”一词可以涵盖作者的个人特征，如年龄，以及形式上的使用，例如正式性或毒性。为了使用监督方法开发TST系统，出现了几个人工标注的数据集Rao和Tetreault（[2018](#bib.bib37)）。例如，Rao和Tetreault（[2018](#bib.bib37)）引入了一个形式风格转移的语料库，将非正式语言转换为正式语言，反之亦然。然而，监督的平行数据对于训练深度神经网络至关重要，但稀缺且获取成本高。因此，提出了无监督的方法Shen等人（[2017](#bib.bib42)）；Liu等人（[2021](#bib.bib24)）来管理风格属性，而无需依赖平行数据。Liu等人（[2022](#bib.bib22)）和Zhang等人（[2020](#bib.bib58)）通过多样的数据增强和任务特定知识从未标记的样本中创建伪平行数据。Gong等人（[2019](#bib.bib9)）；Wang等人（[2019a](#bib.bib47)）；Reid和Zhong（[2021](#bib.bib38)）采用辅助风格分类器来引导转移方向。同时，Krishna等人（[2020](#bib.bib18)）和Hallinan等人（[2023b](#bib.bib11)）部署多个风格特定的模型来分别生成各种风格。最近，LLMs在各种NLP任务中表现出了卓越的能力。研究如Reif等人（[2022](#bib.bib39)）；Pu和Demberg（[2023](#bib.bib35)）发现，超过100B参数的超大LMs在使用ICL进行TST时表现出色。基于这些发现，我们的论文使用LLMs生成伪平行数据，并将LLM的TST技能提炼为一个紧凑的学生模型。此外，我们还通过CoT提示来提升蒸馏和效率。
- en: LLMs have demonstrated impressive performance across various tasks and reasoning
    capabilities. CoT prompting Wei et al. ([2022](#bib.bib51)) is a promising technique
    that extracts these reasoning skills and enhances accuracy in target tasks. However,
    deploying these enormous LLMs poses computational and practical challenges. Recent
    studies Huang et al. ([2022](#bib.bib15)); Wang et al. ([2023a](#bib.bib48));
    Hsieh et al. ([2023](#bib.bib14)) have thus turned to offline knowledge distillation
    (KD) Hinton et al. ([2015](#bib.bib13)) to condense these reasoning capabilities
    into a smaller model. Using CoT rationales can also increase distillation efficiency
    with less data Li et al. ([2022](#bib.bib21)); Shridhar et al. ([2023](#bib.bib43)).
    Concurrently, Saakyan and Muresan ([2023](#bib.bib40)) examine CoT prompting combined
    with domain expert feedback for improved formality transfer. Nevertheless, the
    potential of CoT prompting and KD to enrich a broader range of TST tasks remains
    underexplored.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在各种任务和推理能力方面表现出色。CoT提示技术Wei等人（[2022](#bib.bib51)）是一种有前景的技术，它提取这些推理技能并提高目标任务的准确性。然而，部署这些庞大的LLMs带来了计算和实际挑战。最近的研究Huang等人（[2022](#bib.bib15)）；Wang等人（[2023a](#bib.bib48)）；Hsieh等人（[2023](#bib.bib14)）因此转向离线知识蒸馏（KD）Hinton等人（[2015](#bib.bib13)）以将这些推理能力浓缩到一个较小的模型中。使用CoT理据还可以在较少的数据下提高蒸馏效率Li等人（[2022](#bib.bib21)）；Shridhar等人（[2023](#bib.bib43)）。与此同时，Saakyan和Muresan（[2023](#bib.bib40)）研究了结合领域专家反馈的CoT提示，以改进正式性转移。然而，CoT提示和KD在丰富更广泛的TST任务方面的潜力仍未被充分探索。
- en: In this paper, we present CoTeX framework, using CoT prompting to improve TST.
    It identifies cues for TST and clarifies the rewriting process ($\S$ [4](#S4 "4
    Results ‣ Distilling Text Style Transfer With Self-Explanation From LLMs")).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了CoTeX框架，使用CoT提示来改进TST。它识别TST的线索并阐明重写过程（$\S$ [4](#S4 "4 Results ‣ Distilling
    Text Style Transfer With Self-Explanation From LLMs")）。
- en: 2 Method
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: 2.1 Data Generation
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 数据生成
- en: We employ CoT combined with instruction prompting to extract rationales from
    LLMs regarding the TST process. We have two different settings (target-blind and
    target-aware) to generate rationales.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们结合CoT和指令提示来从LLMs中提取有关TST过程的理由。我们有两种不同的设定（目标盲和目标感知）来生成理由。
- en: Target-Blind (TB). We first explore our method in the target-blind setting where
    we only give a source text and the name of the desired target style. This setting
    can be adaptable to a broader range of style transfer directions. As shown in
    the left side of Figure [1](#S0.F1 "Figure 1 ‣ Distilling Text Style Transfer
    With Self-Explanation From LLMs"), each input example is constructed using an
    instruction template, $p_{tb}$ examples created by humans as context before the
    actual input. In our implementation, we employ three manually crafted examples
    as few-shot prompts.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Target-Blind (TB)。我们首先在目标盲设定中探讨我们的方法，其中我们仅提供源文本和所需目标风格的名称。该设定可适应更广泛的风格转换方向。如图[1](#S0.F1
    "Figure 1 ‣ Distilling Text Style Transfer With Self-Explanation From LLMs")左侧所示，每个输入示例使用一个指令模板构建，$p_{tb}$
    示例由人工创建，作为实际输入之前的上下文。在我们的实现中，我们使用三个手工制作的示例作为少量示例提示。
- en: Target-Aware (TA). For datasets with supervised parallel data, we use the instruction
    template $p_{ta}$ in our experiments.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Target-Aware (TA)。对于具有监督平行数据的数据集，我们在实验中使用指令模板$p_{ta}$。
- en: 2.2 Training Student Models
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 训练学生模型
- en: We leverage the LLM-generated data to finetune smaller, task-specific student
    models. For the data generated in the target-blind setting, we utilize the instruction
    template $p_{tb}$ employing the conventional cross-entropy loss.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用LLM生成的数据来微调更小的、任务特定的学生模型。对于目标盲设定中生成的数据，我们使用指令模板$p_{tb}$，采用传统的交叉熵损失。
- en: 3 Experiments
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: 3.1 Datasets and Metric
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集和指标
- en: We employ four public datasets across three style transfer directions, chosen
    for their inclusion of human-annotated parallel data in both training and evaluation
    sets. This facilitates direct comparisons between different settings.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了四个公共数据集，涵盖三种风格转移方向，选择它们是因为这些数据集中包含了用于训练和评估的人工标注平行数据。这有助于在不同设定之间进行直接比较。
- en: Formality Transfer. We use GYAFC dataset from Rao and Tetreault ([2018](#bib.bib37))
    and focus on the informal to formal language transfer direction. GYAFC dataset
    includes two domains, Family & Relationships (F&R) and Entertainment & Music (E&M).
     Detoxification. ParaDetox Logacheva et al. ([2022b](#bib.bib26)) is a parallel
    dataset for text detoxification.  Shakespeare to Modern English. Xu et al. ([2012](#bib.bib56))
    introduce a human-annotated dataset for translating text between William Shakespeare’s
    plays and their modernized versions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正式性转移。我们使用Rao和Tetreault ([2018](#bib.bib37))的GYAFC数据集，关注从非正式到正式语言的转移方向。GYAFC数据集包括两个领域：家庭与关系（F&R）和娱乐与音乐（E&M）。毒化去除。ParaDetox
    Logacheva等人 ([2022b](#bib.bib26)) 是一个用于文本毒化去除的平行数据集。莎士比亚到现代英语。Xu等人 ([2012](#bib.bib56))
    介绍了一个人工标注的数据集，用于翻译莎士比亚戏剧文本与其现代化版本之间的文本。
- en: Low-Resource Training. Our method offers advantages in low-resource settings,
    as the CoT is poised to enhance the learning efficiency of student models and
    bolster their generalizability. Thus, we create smaller training sets by randomly
    sampling training data, ranging from 1K to 20K.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 低资源训练。我们的方法在低资源环境中具有优势，因为CoT有助于提高学生模型的学习效率并增强其泛化能力。因此，我们通过随机采样训练数据创建了较小的训练集，范围从1K到20K。
- en: Evaluation Metric. We report BLEU, leveraging the Sacre-BLEU Python library Post
    ([2018](#bib.bib33)), as main metric for evaluation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 评价指标。我们报告BLEU，利用Sacre-BLEU Python库Post ([2018](#bib.bib33))，作为主要评估指标。
- en: 3.2 Model Comparison.
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 模型比较。
- en: 'In low-resource settings, CoTeX is compared to (1) SFT: conventional supervised
    fine-tuning using parallel data, (2) teacher LLM: the teacher model evaluated
    on the Test set via few-shot ICL, i.e., using the three-shot prompt and template
    $p_{tb}$ described in Section [2](#S2 "2 Method ‣ Distilling Text Style Transfer
    With Self-Explanation From LLMs"), and (3) Distill: traditional offline knowledge
    distillation, which relies solely on LLM-generated pseudo-parallel data without
    a CoT path.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在资源有限的环境中，将CoTeX与以下进行比较：(1) SFT：使用平行数据的传统监督微调，(2) teacher LLM：通过少量示例ICL在测试集上评估的教师模型，即使用第[2](#S2
    "2 Method ‣ Distilling Text Style Transfer With Self-Explanation From LLMs")节中描述的三次示例提示和模板$p_{tb}$，以及(3)
    Distill：传统的离线知识蒸馏，完全依赖LLM生成的伪平行数据而没有CoT路径。
- en: 'For comprehensive evaluations, CoTeX is further compared with (1) Prompt&Rank:
    a SoTA in-context learning method for TST Suzgun et al. ([2022](#bib.bib44)),
    and (2) instruction-tuned LLMs: open-source LLMs assessed through three-shot ICL
    using the same prompt and template described in Section [2](#S2 "2 Method ‣ Distilling
    Text Style Transfer With Self-Explanation From LLMs"); these LLMs include Alpaca
    7B Taori et al. ([2023](#bib.bib45)), Vicuna 7B Chiang et al. ([2023](#bib.bib4)),
    LLaMA2-Chat 7B Touvron et al. ([2023](#bib.bib46)), and FlanT5-XL Chung et al.
    ([2022](#bib.bib5)) (with 3B parameters). Additionally, for each dataset, comparisons
    are made with existing dataset-specific unsupervised and supervised methods. Unsupervised
    methods include DualRL Luo et al. ([2019](#bib.bib28)), STRAP Krishna et al. ([2020](#bib.bib18)),
    DLS He et al. ([2020](#bib.bib12)), and TSST Xiao et al. ([2021](#bib.bib54))
    for formality transfer; Mask&Infill Wu et al. ([2019](#bib.bib53)) and CondBERT Dale
    et al. ([2021](#bib.bib6)) for detoxification; and STRAP and TSST for modernizing
    Shakespearean text. Supervised methods include Multi-NMT Niu et al. ([2018](#bib.bib30)),
    GPT-CAT Wang et al. ([2019b](#bib.bib50)), and SemiFST Liu et al. ([2022](#bib.bib22))
    for formality transfer; ParaDetox Logacheva et al. ([2022a](#bib.bib25)) for detoxification;
    and PointerS2S Jhamtani et al. ([2017](#bib.bib16)) for modernizing Shakespearean
    text.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行全面评估，CoTeX进一步与以下方法进行比较：(1) Prompt&Rank：一种用于TST的最先进的上下文学习方法Suzgun等([2022](#bib.bib44))，和(2)
    指令调优LLMs：通过使用第[2](#S2 "2 Method ‣ Distilling Text Style Transfer With Self-Explanation
    From LLMs")节中描述的相同提示和模板的三次示例ICL评估的开源LLMs；这些LLMs包括Alpaca 7B Taori等([2023](#bib.bib45))、Vicuna
    7B Chiang等([2023](#bib.bib4))、LLaMA2-Chat 7B Touvron等([2023](#bib.bib46))和FlanT5-XL
    Chung等([2022](#bib.bib5))（参数为3B）。此外，对于每个数据集，还与现有的数据集特定的无监督和监督方法进行比较。无监督方法包括DualRL
    Luo等([2019](#bib.bib28))、STRAP Krishna等([2020](#bib.bib18))、DLS He等([2020](#bib.bib12))和TSST
    Xiao等([2021](#bib.bib54))用于形式转移；Mask&Infill Wu等([2019](#bib.bib53))和CondBERT Dale等([2021](#bib.bib6))用于脱毒；以及STRAP和TSST用于现代化莎士比亚文本。监督方法包括Multi-NMT
    Niu等([2018](#bib.bib30))、GPT-CAT Wang等([2019b](#bib.bib50))和SemiFST Liu等([2022](#bib.bib22))用于形式转移；ParaDetox
    Logacheva等([2022a](#bib.bib25))用于脱毒；以及PointerS2S Jhamtani等([2017](#bib.bib16))用于现代化莎士比亚文本。
- en: 3.3 Implementation
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 实现
- en: We employ PaLM2 Unicorn Anil et al. ([2023](#bib.bib1)) as our LLM for data
    generation. In the target-blind setting, we generate a CoT path and a transferred
    text.²²2Our ancillary study also examines the generation of multiple pairs of
    CoT paths and transferred text. For the target-aware approach, we solely produce
    a CoT path. Both approaches use a temperature of 0.7\. Afterward, we finetune
    a T5-large model (with 770M parameters) Raffel et al. ([2020](#bib.bib36)) with
    the curated dataset.³³3We provide a concise experiment of using T5-XL model in
    Appendix [D](#A4 "Appendix D Experiment with T5-XL ‣ Distilling Text Style Transfer
    With Self-Explanation From LLMs"). We finetune T5 for 2,000 steps with a learning
    rate of $1e-3$ and batch size of 128\. We evaluate validation performance every
    16 steps and report test result of the best step.⁴⁴4More details about hyperparameters
    are in Appendix [B](#A2 "Appendix B Hyperparameter for Training Student Model
    ‣ Distilling Text Style Transfer With Self-Explanation From LLMs").
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用PaLM2 Unicorn Anil等([2023](#bib.bib1))作为我们的LLM进行数据生成。在目标盲设定下，我们生成一个CoT路径和一个转移文本。²²2我们的附属研究还考察了生成多个CoT路径和转移文本对的过程。对于目标感知的方法，我们仅生成一个CoT路径。这两种方法都使用0.7的温度。之后，我们用策划的数据集对T5-large模型（参数为770M）Raffel等([2020](#bib.bib36))进行微调。³³3我们在附录[D](#A4
    "Appendix D Experiment with T5-XL ‣ Distilling Text Style Transfer With Self-Explanation
    From LLMs")中提供了使用T5-XL模型的简要实验。我们将T5微调2,000步，学习率为$1e-3$，批量大小为128\. 每16步评估一次验证性能，并报告最佳步骤的测试结果。⁴⁴4关于超参数的更多细节见附录[B](#A2
    "Appendix B Hyperparameter for Training Student Model ‣ Distilling Text Style
    Transfer With Self-Explanation From LLMs")。
- en: '![Refer to caption](img/076f10ada3168642eed4e245a1dadf0b.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/076f10ada3168642eed4e245a1dadf0b.png)'
- en: 'Figure 2: Test results of low-resource settings.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 低资源设置下的测试结果。'
- en: 4 Results
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果
- en: We now present your experimental results. CoTeX-TB and CoTeX-TA denote models
    trained using datasets created through target-blind and target-aware methods,
    respectively.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在展示你的实验结果。CoTeX-TB 和 CoTeX-TA 分别表示通过目标盲和目标感知方法创建的数据集训练的模型。
- en: Low-Resource Settings. We first examine CoTeX’s impact in low-resource context.
    Figure [2](#S3.F2 "Figure 2 ‣ 3.3 Implementation ‣ 3 Experiments ‣ Distilling
    Text Style Transfer With Self-Explanation From LLMs") shows CoTeX’s performance
    in both target-blind and target-aware settings across varying training data sizes.
    In both formality transfer datasets, CoTeX-TB outperforms SFT-T5 and Distill-T5\.
    This advantage is noticeable with limited data, specifically under 10K. For instance,
    using just 1K samples from the informal-formal (E&M) dataset, the BLEU scores
    for SFT, CoTeX-TB, and CoTeX-TA are 55.13, 68.62, and 65.40, respectively. We
    find that both CoTeX-TB and CoTeX-TA outperform or match the LLM’s performance
    on the two formality datasets. In translating Shakespearean to modern English,
    CoTeX-TB exhibits significant superiority over SFT-T5 and Distill-T5 across all
    data sizes. We believe that such an enhancement can be attributed to the high
    quality of LLM generations. LLM with few-shot in-context learning obtains a BLEU
    score of 32.43\. Though CoTeX-TB underperforms SFT on detoxification, CoTeX-TA
    still outperforms SFT in most data sizes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 低资源设置。我们首先考察 CoTeX 在低资源环境中的影响。图 [2](#S3.F2 "图 2 ‣ 3.3 实现 ‣ 3 实验 ‣ 从 LLMs 自我解释中提取文本风格转移")
    显示了 CoTeX 在不同训练数据量下的目标盲和目标感知设置中的表现。在两个格式化转移数据集中，CoTeX-TB 的表现优于 SFT-T5 和 Distill-T5。这个优势在数据量有限的情况下，尤其是在
    10K 以下，尤为明显。例如，仅使用 1K 个来自非正式-正式 (E&M) 数据集的样本，SFT、CoTeX-TB 和 CoTeX-TA 的 BLEU 分别为
    55.13、68.62 和 65.40。我们发现 CoTeX-TB 和 CoTeX-TA 在这两个格式化数据集上都优于或匹配 LLM 的表现。在将莎士比亚文翻译为现代英语时，CoTeX-TB
    在所有数据量下都显著优于 SFT-T5 和 Distill-T5。我们认为这种提升可归因于 LLM 生成的高质量。LLM 在少量样本的上下文学习中获得了 32.43
    的 BLEU 分数。尽管 CoTeX-TB 在解毒任务上表现不如 SFT，CoTeX-TA 在大多数数据量下仍优于 SFT。
- en: '|  | Method | BLEU | Method | BLEU |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | BLEU | 方法 | BLEU |'
- en: '|  | Formality (F&R) | Formality (E&M) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | 格式化 (F&R) | 格式化 (E&M) |'
- en: '| Unsup. | DualRL | 53.01 | DLS | 23.09 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 无监督 | DualRL | 53.01 | DLS | 23.09 |'
- en: '| TSST | 60.99 | STRAP | 31.39 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| TSST | 60.99 | STRAP | 31.39 |'
- en: '| \cdashline1-5 ICL | Prompt&Rank | 30.60 | Prompt&Rank | 30.96 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-5 ICL | Prompt&Rank | 30.60 | Prompt&Rank | 30.96 |'
- en: '| Alpaca | 41.85 | Alpaca | 52.40 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca | 41.85 | Alpaca | 52.40 |'
- en: '| Vicuna | 37.09 | Vicuna | 46.47 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna | 37.09 | Vicuna | 46.47 |'
- en: '| LLaMA2-C. | 19.62 | LLaMA2-C. | 25.14 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-C. | 19.62 | LLaMA2-C. | 25.14 |'
- en: '| FlanT5-XL | 55.70 | FlanT5-XL | 42.58 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-XL | 55.70 | FlanT5-XL | 42.58 |'
- en: '| \cdashline1-5 Sup. | Multi-NMT^† | 75.35 | Multi-NMT^† | 72.01 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-5 有监督 | Multi-NMT^† | 75.35 | Multi-NMT^† | 72.01 |'
- en: '| GPT-CAT^† | 77.26 | GPT-CAT^† | 71.39 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| GPT-CAT^† | 77.26 | GPT-CAT^† | 71.39 |'
- en: '| SemiFST | 80.32 | SemiFST | 76.87 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| SemiFST | 80.32 | SemiFST | 76.87 |'
- en: '| SFT (ours) | 77.12 | SFT (ours) | 73.01 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| SFT (我们的) | 77.12 | SFT (我们的) | 73.01 |'
- en: '| \cdashline1-5 | Distill (ours) | 64.79 | Distill (ours) | 64.31 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-5 | Distill (我们的) | 64.79 | Distill (我们的) | 64.31 |'
- en: '| \cdashline1-5 | CoTex-TB | 72.05 | CoTex-TB | 71.70 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-5 | CoTex-TB | 72.05 | CoTex-TB | 71.70 |'
- en: '|  | CoTex-TA | 77.13 | CoTex-TA | 74.65 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | CoTex-TA | 77.13 | CoTex-TA | 74.65 |'
- en: '|  | Detoxification | Modernizing Shake. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | 解毒 | 现代化 Shake. |'
- en: '| Unsup. | Mask&Infill^∗ | 44.77 | DLS | 12.85 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 无监督 | Mask&Infill^∗ | 44.77 | DLS | 12.85 |'
- en: '| CondBERT^∗ | 48.89 | STRAP | 19.96 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| CondBERT^∗ | 48.89 | STRAP | 19.96 |'
- en: '| \cdashline1-5 ICL | Prompt&Rank | 11.06 | Prompt&Rank | 20.87 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-5 ICL | Prompt&Rank | 11.06 | Prompt&Rank | 20.87 |'
- en: '| Alpaca | 24.32 | Alpaca | 24.33 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca | 24.32 | Alpaca | 24.33 |'
- en: '| Vicuna | 34.54 | Vicuna | 17.76 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna | 34.54 | Vicuna | 17.76 |'
- en: '| LLaMA2-C. | 14.65 | LLaMA2-C. | 25.19 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-C. | 14.65 | LLaMA2-C. | 25.19 |'
- en: '| FlanT5-XL | 50.13 | FlanT5-XL | 21.55 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-XL | 50.13 | FlanT5-XL | 21.55 |'
- en: '| \cdashline1-5 Sup. | ParaDetox | 53.98 | PointerS2S | 30.78 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-5 有监督 | ParaDetox | 53.98 | PointerS2S | 30.78 |'
- en: '| SFT (ours) | 52.88 | SFT (ours) | 22.69 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| SFT (我们的) | 52.88 | SFT (我们的) | 22.69 |'
- en: '| \cdashline1-5 | Distill (ours) | 43.97 | Distill (ours) | 22.88 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-5 | Distill (我们的) | 43.97 | Distill (我们的) | 22.88 |'
- en: '| \cdashline1-5 | CoTex-TB | 48.53 | CoTex-TB | 26.79 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-5 | CoTex-TB | 48.53 | CoTex-TB | 26.79 |'
- en: '|  | CoTex-TA | 54.79 | CoTex-TA | 25.70 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | CoTex-TA | 54.79 | CoTex-TA | 25.70 |'
- en: 'Table 1: Comparing to previous methods. The best-performed method is in bold.
    The best method without utilizing a full parallel Train set is underscored. Unsup.:
    unsupervised, Sup.: supervised, ^†: Take from Liu et al. ([2022](#bib.bib22)).
    ^∗: Utilize outputs from implementation of Logacheva et al. ([2022b](#bib.bib26)).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1：与以前的方法比较。表现最好的方法用粗体标出。没有利用完整并行训练集的最佳方法用下划线标出。Unsup.: 无监督，Sup.: 有监督，^†:
    取自 Liu 等人 ([2022](#bib.bib22))。^∗: 利用 Logacheva 等人 ([2022b](#bib.bib26)) 实现的输出。'
- en: Utilizing the Full Dataset. Training student models with CoTeX on all training
    samples of each dataset, we present comparative results in Tables [1](#S4.T1 "Table
    1 ‣ 4 Results ‣ Distilling Text Style Transfer With Self-Explanation From LLMs").⁵⁵5CoTeX-TB
    setting utilizes the source text from training sample while keeping the target
    undisclosed. Given that many unsupervised TST studies have not reported BLEU scores,
    we compute BLEU scores for their public outputs using our evaluation scripts to
    ensure a fair comparison. CoTeX-TB surpasses previous unsupervised methods, the
    SoTA ICL method Prompt&Rank, and instruction-tuned LLMs across both domains within
    the formality transfer dataset. Although CoTeX-TA does not exceed the performance
    of SoTA supervised methods, SemiFST, for formality transfer, it is noteworthy
    that our method does not depend on task-specific data augmentation strategies
    or knowledge, offering greater flexibility. In the detoxification task, our results
    are compared with the top-performing model from Logacheva et al. ([2022b](#bib.bib26)).
    CoTeX-TA outperforms previous supervised methods, while CoTeX-TB falls slightly
    short of CondBERT, which employs additional style-conditional LMs for transfer
    control. FlanT5-XL, an instruction-tuned LLM, leads in ICL performance with a
    BLEU score of 50.13. For translating Shakespearean to modern English, CoTeX-TB
    shows marked improvements over both unsupervised and ICL methods, attributed to
    the superior quality of LLM generations in this specific transfer task.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 利用完整数据集。使用 CoTeX 在每个数据集的所有训练样本上训练学生模型，我们在表中展示了比较结果 [1](#S4.T1 "表 1 ‣ 4 结果 ‣
    从 LLMs 中提取自我解释的文本风格迁移")。⁵⁵5CoTeX-TB 设置利用来自训练样本的源文本，同时保持目标文本不公开。鉴于许多无监督 TST 研究未报告
    BLEU 分数，我们使用我们的评估脚本计算其公开输出的 BLEU 分数，以确保公平比较。CoTeX-TB 超越了以前的无监督方法、SoTA ICL 方法 Prompt&Rank
    和跨领域的指令调优 LLMs。在形式转移数据集中，尽管 CoTeX-TA 的表现没有超过 SoTA 有监督方法 SemiFST，但值得注意的是，我们的方法不依赖于特定任务的数据增强策略或知识，提供了更大的灵活性。在去毒化任务中，我们的结果与
    Logacheva 等人 ([2022b](#bib.bib26)) 的顶级模型进行了比较。CoTeX-TA 超越了以前的有监督方法，而 CoTeX-TB
    略微逊色于 CondBERT，它采用额外的风格条件 LMs 进行转移控制。FlanT5-XL 作为一个指令调优 LLM，在 ICL 性能中领先，BLEU 分数为
    50.13。对于将莎士比亚式英语翻译为现代英语，CoTeX-TB 显示出明显的改进，归因于 LLM 生成在这一特定转移任务中的优越质量。
- en: Increasing Synthetic Data per Source Text. For CoTeX-TB, we conduct an ancillary
    study to explore the benefits of employing multiple CoT paths with synthetic target
    texts for a source text. Given a source text $s_{i}$ values.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 增加每个源文本的合成数据。对于 CoTeX-TB，我们进行了一项附加研究，以探索使用多条 CoT 路径与合成目标文本对源文本的好处。给定一个源文本 $s_{i}$
    的值。
- en: '![Refer to caption](img/f643c70bcc660d5233da764d2b04f86a.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f643c70bcc660d5233da764d2b04f86a.png)'
- en: 'Figure 3: Scaling up synthetic data per source.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：按源扩展合成数据。
- en: '| Source Text | just want to know HAVE YOU BEEN TO THE DOC YET |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 源文本 | 只是想知道你是否去过医生 |'
- en: '| CoTeX Generation | The original text is informal. The use of all caps and
    the absence of punctuation are informal. The use of “DOC" is a misspelling of
    “doctor". [[Transferred]]: I just want to know if you have been to the doctor
    yet. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| CoTeX 生成 | 原文是非正式的。使用全大写字母和缺乏标点符号都显得非正式。使用“DOC”是“doctor”的拼写错误。[[转移]]: 我只是想知道你是否已经去看医生了。
    |'
- en: '| Reference | I want to know if you have been to the doctor yet. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 我想知道你是否已经去看医生了。 |'
- en: '| Source Text | bembie hit the nail on the head |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 源文本 | bembie 一针见血 |'
- en: '| CoTeX Generation | The original text is informal. The use of “bembie" is
    a misspelling of “bembie". [[Transferred]]: Bembie hit the nail on the head. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| CoTeX 生成 | 原文是非正式的。使用“bembie”是“bembie”的拼写错误。[[转移]]: Bembie 一针见血。 |'
- en: '| Reference | Bembie reached the proper conclusion. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | Bembie 达到了正确的结论。 |'
- en: 'Table 2: Case study on CoTeX-TB generations.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：CoTeX-TB 生成的案例研究。
- en: Qualitative Study. We now present a qualitative study to delve into rewriting
    rationales generated by CoTeX-TB. Examples are showcased in Table [2](#S4.T2 "Table
    2 ‣ 4 Results ‣ Distilling Text Style Transfer With Self-Explanation From LLMs"),
    derived from Test set of Formality (F&R) which transfers from informal to formal
    text. We sort generations by their BLEU scores against gold references and select
    random high and low-scoring samples. The first example, obtained BLEU of 100,
    correctly identifies informal components, fixes informal spellings, and yields
    a formal and grammatical sentence. The second example (BLEU=7.27) misses comprehending
    the idiom “hit the nail on the head” from the source, without translating it into
    a formal expression. Nevertheless, we note that the LLM (i.e., PaLM2) can appropriately
    adapt this idiom to “accurately identified the key point”. This leads us to hypothesize
    that a smaller LM exhibits potential limitations in its ability to understand
    implicit style cues.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 定性研究。我们现在展示一项定性研究，以深入探讨 CoTeX-TB 生成的重写推理。示例展示在表[2](#S4.T2 "Table 2 ‣ 4 Results
    ‣ Distilling Text Style Transfer With Self-Explanation From LLMs")中，来源于形式性（F&R）测试集，该测试集将非正式文本转化为正式文本。我们根据生成文本与金标准参考的
    BLEU 分数进行排序，并选择了随机的高分和低分样本。第一个示例获得了 100 的 BLEU 分数，正确识别了非正式成分，修正了非正式拼写，并生成了正式且语法正确的句子。第二个示例（BLEU=7.27）未能理解源文本中的“hit
    the nail on the head”这个习语，也没有将其转化为正式表达。然而，我们注意到 LLM（即 PaLM2）可以将这个习语恰当地转化为“准确识别了关键点”。这使我们假设较小的语言模型在理解隐含风格提示方面可能存在潜在的局限性。
- en: Human Evaluation on Generated Reasonings. To assess the quality of model-generated
    rationales (i.e., CoT path) for the rewriting process, we conduct a human evaluation.
    Following previous works Wang et al. ([2023b](#bib.bib49)); Wu et al. ([2023](#bib.bib52)),
    we develop our evaluation protocol and instructions as shown in Table [5](#A5.T5
    "Table 5 ‣ Appendix E Human Evaluation on Generated Reasonings ‣ Distilling Text
    Style Transfer With Self-Explanation From LLMs") in Appendix. We assemble a team
    of four human experts to undertake this evaluation. Each annotator was tasked
    with reviewing 50 generated rationales across different models and transfer tasks.
    For each evaluation, the dataset provided included the source text, a generated
    rationale for the rewriting process, and the resultant transferred text. As depicted
    in Figure [4](#S4.F4 "Figure 4 ‣ 4 Results ‣ Distilling Text Style Transfer With
    Self-Explanation From LLMs"), although CoTeX-TB lags behind the teacher model
    (PaLM2 Unicorn), 100% of its responses in the detoxification task and 74% in the
    formality transfer task are deemed acceptable.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 生成推理的人类评估。为了评估模型生成的推理（即 CoT 路径）的质量，我们进行了一项人类评估。根据以往的研究 Wang et al. ([2023b](#bib.bib49))；Wu
    et al. ([2023](#bib.bib52))，我们制定了我们的评估协议和说明，详见附录中的表格[5](#A5.T5 "Table 5 ‣ Appendix
    E Human Evaluation on Generated Reasonings ‣ Distilling Text Style Transfer With
    Self-Explanation From LLMs")。我们组建了一个由四名人类专家组成的团队来进行此评估。每位标注员的任务是审查不同模型和迁移任务中的
    50 个生成的推理。每次评估时，提供的数据集包括源文本、用于重写过程的生成推理以及结果转化后的文本。如图[4](#S4.F4 "Figure 4 ‣ 4 Results
    ‣ Distilling Text Style Transfer With Self-Explanation From LLMs")所示，尽管 CoTeX-TB
    落后于教师模型（PaLM2 Unicorn），其在解毒任务中的响应 100% 被认为是可接受的，而在形式转移任务中的接受率为 74%。
- en: $0$#
    of examplesRate-ARate-BRate-CRate-D
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: $0$#
    of examplesRate-ARate-BRate-CRate-D
- en: 'Figure 4: Human evaluation results of CoT reasoning paths of 50 samples. Form.:
    formality transfer, Detox.: Detoxification.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4：50 个样本的 CoT 推理路径的人类评估结果。Form.: 形式转移，Detox.: 解毒。'
- en: 5 Related Work
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: When parallel TST datasets are available, numerous studies Rao and Tetreault
    ([2018](#bib.bib37)); Shang et al. ([2019](#bib.bib41)); Chawla and Yang ([2020](#bib.bib3));
    Lai et al. ([2021](#bib.bib19)) have utilized a sequence-to-sequence framework
    for supervised training TST models. To improve model efficacy, multitask learning Niu
    et al. ([2018](#bib.bib30)); Xu et al. ([2019](#bib.bib55)), lexically constrained
    decoding Post and Vilar ([2018](#bib.bib34)), and task-specific data augmentation Zhang
    et al. ([2020](#bib.bib58)); Liu et al. ([2022](#bib.bib22)) have been incorporated.
    Addressing the scarcity of parallel data, unsupervised methods have been developed
    for TST, employing methodologies like disentanglement of latent representations Liu
    et al. ([2020](#bib.bib23)); Nangi et al. ([2021](#bib.bib29)); Yi et al. ([2021](#bib.bib57)),
    prototype editing Li et al. ([2018](#bib.bib20)), style rewriting using attribute-specific
    LMs Krishna et al. ([2020](#bib.bib18)), and reinforcement learning Luo et al.
    ([2019](#bib.bib28)); Hallinan et al. ([2023a](#bib.bib10)). Our CoTeX framework
    explores both parallel and non-parallel data landscapes. The advent of LLMs has
    introduced ICL for executing TST with few-shot prompts, bypassing the need for
    model parameter updates Reif et al. ([2022](#bib.bib39)); Suzgun et al. ([2022](#bib.bib44)).
    Yet, these methods typically lack interpretability. In parallel, Saakyan and Muresan
    ([2023](#bib.bib40)) employ CoT prompting alongside domain expert feedback to
    enhance formality transfer and interpretability. Our CoTeX extends to broader
    range of TST directions, aiming to utilize CoT to provide rewriting explanations
    and minimize the requirement for human intervention.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当平行 TST 数据集可用时，许多研究 Rao 和 Tetreault ([2018](#bib.bib37)); Shang 等 ([2019](#bib.bib41));
    Chawla 和 Yang ([2020](#bib.bib3)); Lai 等 ([2021](#bib.bib19)) 已利用序列到序列框架进行监督训练
    TST 模型。为了提高模型效能，已融入多任务学习 Niu 等 ([2018](#bib.bib30)); Xu 等 ([2019](#bib.bib55))、词汇约束解码
    Post 和 Vilar ([2018](#bib.bib34)) 和任务特定数据增强 Zhang 等 ([2020](#bib.bib58)); Liu
    等 ([2022](#bib.bib22))。针对平行数据稀缺的问题，已开发出无监督方法用于 TST，采用了诸如潜在表示解耦 Liu 等 ([2020](#bib.bib23));
    Nangi 等 ([2021](#bib.bib29)); Yi 等 ([2021](#bib.bib57))、原型编辑 Li 等 ([2018](#bib.bib20))、使用属性特定
    LM 的风格重写 Krishna 等 ([2020](#bib.bib18)) 和强化学习 Luo 等 ([2019](#bib.bib28)); Hallinan
    等 ([2023a](#bib.bib10)) 等方法。我们的 CoTeX 框架探索了平行和非平行数据环境。LLMs 的出现引入了 ICL，通过少量示例进行
    TST，绕过了模型参数更新的需要 Reif 等 ([2022](#bib.bib39)); Suzgun 等 ([2022](#bib.bib44))。然而，这些方法通常缺乏可解释性。与此同时，Saakyan
    和 Muresan ([2023](#bib.bib40)) 通过 CoT 提示结合领域专家反馈来增强正式性转移和可解释性。我们的 CoTeX 扩展到更广泛的
    TST 方向，旨在利用 CoT 提供重写解释，并最小化对人工干预的需求。
- en: 6 Conclusion
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We introduced CoTeX, a novel approach for TST. Through CoT prompting, we elicit
    the rationals for the style rewriting process from LLMs and then distill both
    the TST and reasoning capabilities into smaller task-specific models. CoTeX demonstrated
    its efficiency and effectiveness with and without utilizing parallel data, especially
    in low-resource scenarios. The CoT reasoning from CoTeX bolstered the explainability
    of TST models.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 CoTeX，一种用于 TST 的新颖方法。通过 CoT 提示，我们从 LLMs 中引出风格重写过程的理性依据，然后将 TST 和推理能力提炼到更小的任务特定模型中。CoTeX
    在利用和不利用平行数据的情况下都展示了其效率和效果，特别是在低资源场景下。CoTeX 的 CoT 推理增强了 TST 模型的可解释性。
- en: 7 Limitations
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制
- en: TST Directions.
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TST 方向。
- en: We incorporate three style transfer directions to enable a clear comparison
    between target-blind and target-aware CoTeX. Benefiting from the powerful capacity
    of LLMs, we believe that our method could be extended to a broader array of TST
    directions (e.g., sentiment transfer). We plan to explore more transfer directions
    in future work.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们结合了三种风格转换方向，以便清晰地比较目标盲和目标感知的 CoTeX。得益于 LLMs 的强大能力，我们相信我们的方法可以扩展到更广泛的 TST 方向（例如，情感转换）。我们计划在未来的工作中探索更多转换方向。
- en: Model Selection.
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型选择。
- en: We only use T5-large as the student model in the paper. We also conduct a concise
    study to apply CoTeX to the T5-XL model. As results shown in Appendix [D](#A4
    "Appendix D Experiment with T5-XL ‣ Distilling Text Style Transfer With Self-Explanation
    From LLMs"), our CoTeX-TA still outperforms SFT on ParaDetox dataset.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本文仅使用 T5-large 作为学生模型。我们还进行了一个简要研究，将 CoTeX 应用到 T5-XL 模型上。如附录[D](#A4 "附录 D T5-XL
    实验 ‣ 通过自我解释从 LLMs 提取文本风格转换")中所示，我们的 CoTeX-TA 在 ParaDetox 数据集上仍优于 SFT。
- en: Evaluation Metrics.
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估指标。
- en: 'Unlike previous studies Krishna et al. ([2020](#bib.bib18)); Liu et al. ([2022](#bib.bib22)),
    we abstain from using other automatic metrics (e.g., BERTscore for meaning preservation)
    to evaluate our models. Our decision is grounded in two main reasons: (1) While
    these automatic evaluations consider three facets, i.e., preservation of semantic
    meaning, accuracy of style transfer, and fluency they lack an effective methodology
    for aggregating these metrics to convey the overall performance Ostheimer et al.
    ([2023](#bib.bib31)); (2) Our preliminary experiments involving these automatic
    metrics revealed a misalignment between their outcomes and the BLEU score derived
    from human-annotated references. We thus opt to report the BLEU score in the paper.
    Detailed results from our preliminary tests are presented in Appendix [C](#A3
    "Appendix C Preliminary Test on Evaluation Metrics ‣ Distilling Text Style Transfer
    With Self-Explanation From LLMs").'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的研究 Krishna 等人 ([2020](#bib.bib18)); Liu 等人 ([2022](#bib.bib22)) 不同，我们选择不使用其他自动化指标（例如，BERTscore
    用于语义保留）来评估我们的模型。我们的决定基于两个主要原因：(1) 虽然这些自动化评估考虑了三个方面，即语义意义的保留、风格转移的准确性和流畅性，但它们缺乏有效的方法来整合这些指标以传达整体性能
    Ostheimer 等人 ([2023](#bib.bib31)); (2) 我们的初步实验涉及这些自动化指标时，发现其结果与从人工标注的参考中得出的 BLEU
    分数存在不一致。因此，我们选择在论文中报告 BLEU 分数。我们初步测试的详细结果见附录 [C](#A3 "附录 C 初步测试评估指标 ‣ 从 LLMs 提炼文本风格转移")。
- en: 8 Ethical Consideration
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 伦理考虑
- en: The primary objective of training CoTeX model is to achieve more computationally
    efficient and effective models for TST. We focus on the positive TST directions,
    such as language detoxification. We use an LLM to generate rationales alongside
    transferred text, which are subsequently distilled into smaller LMs. It’s important
    to acknowledge that the LLM’s generation might encompass societal biases Lucy
    and Bamman ([2021](#bib.bib27)) or hallucinations Zhang et al. ([2023](#bib.bib59)),
    and student models trained with this data could inherit these characteristics
    of the teacher LLM. Additionally, our CoTeX-TA relies on datasets from prior research.
    Thus, any biases present in the original annotation processes of these datasets
    might also be reflected in our trained models. We expect the ongoing work Ouyang
    et al. ([2022](#bib.bib32)); Dev et al. ([2022](#bib.bib7)) of improving LM’s
    social fairness, faithfulness, and trustworthiness could benefit both teacher
    and student models.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 CoTeX 模型的主要目标是实现更具计算效率和效果的 TST 模型。我们专注于积极的 TST 方向，如语言去毒化。我们使用 LLM 生成与转移文本相关的理由，这些理由随后被提炼成较小的
    LMs。重要的是要认识到，LLM 的生成可能包含社会偏见 Lucy 和 Bamman ([2021](#bib.bib27)) 或虚假信息 Zhang 等人
    ([2023](#bib.bib59))，并且使用这些数据训练的学生模型可能会继承这些特征。此外，我们的 CoTeX-TA 依赖于先前研究的数据集。因此，这些数据集的原始注释过程中的任何偏见也可能反映在我们训练的模型中。我们期望
    Ouyang 等人 ([2022](#bib.bib32)); Dev 等人 ([2022](#bib.bib7)) 继续改进 LM 的社会公平性、忠实性和可信度的工作，能使教师模型和学生模型都受益。
- en: References
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Anil et al. (2023) Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern,
    Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder,
    Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan
    Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma,
    Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,
    Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
    Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng,
    Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez,
    and et al. 2023. [Palm 2 technical report](https://doi.org/10.48550/arXiv.2305.10403).
    *CoRR*, abs/2305.10403.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil 等人 (2023) Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern,
    Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder,
    Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan
    Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma,
    Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,
    Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
    Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng,
    Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez,
    等人. 2023. [Palm 2 技术报告](https://doi.org/10.48550/arXiv.2305.10403). *CoRR*, abs/2305.10403.
- en: 'Babakov et al. (2023) Nikolay Babakov, David Dale, Ilya Gusev, Irina Krotova,
    and Alexander Panchenko. 2023. Don’t lose the message while paraphrasing: A study
    on content preserving style transfer. In *Natural Language Processing and Information
    Systems*, pages 47–61, Cham. Springer Nature Switzerland.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Babakov 等人 (2023) Nikolay Babakov, David Dale, Ilya Gusev, Irina Krotova 和 Alexander
    Panchenko. 2023. 在改写时不要丢失信息：一种内容保持的风格迁移研究。发表于 *自然语言处理与信息系统*，第47–61页，Cham。施普林格自然瑞士。
- en: 'Chawla and Yang (2020) Kunal Chawla and Diyi Yang. 2020. [Semi-supervised formality
    style transfer using language model discriminator and mutual information maximization](https://doi.org/10.18653/v1/2020.findings-emnlp.212).
    In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages
    2340–2354, Online. Association for Computational Linguistics.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chawla 和 Yang (2020) Kunal Chawla 和 Diyi Yang. 2020. [使用语言模型鉴别器和互信息最大化的半监督正式风格迁移](https://doi.org/10.18653/v1/2020.findings-emnlp.212)。在
    *计算语言学协会发现: EMNLP 2020*，第2340–2354页，在线。计算语言学协会。'
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. [Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang 等人 (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu,
    Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
    Stoica 和 Eric P. Xing. 2023. [Vicuna: 一个开源聊天机器人，其质量接近 90%* 的 ChatGPT](https://lmsys.org/blog/2023-03-30-vicuna/)。'
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert
    Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery,
    Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M.
    Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts,
    Denny Zhou, Quoc V. Le, and Jason Wei. 2022. [Scaling instruction-finetuned language
    models](https://doi.org/10.48550/ARXIV.2210.11416). *CoRR*, abs/2210.11416.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等人 (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert
    Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery,
    Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew
    M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts,
    Denny Zhou, Quoc V. Le 和 Jason Wei. 2022. [扩展指令微调语言模型](https://doi.org/10.48550/ARXIV.2210.11416)。*CoRR*，abs/2210.11416。
- en: Dale et al. (2021) David Dale, Anton Voronov, Daryna Dementieva, Varvara Logacheva,
    Olga Kozlova, Nikita Semenov, and Alexander Panchenko. 2021. [Text detoxification
    using large pre-trained neural models](https://doi.org/10.18653/v1/2021.emnlp-main.629).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 7979–7996, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dale 等人 (2021) David Dale, Anton Voronov, Daryna Dementieva, Varvara Logacheva,
    Olga Kozlova, Nikita Semenov 和 Alexander Panchenko. 2021. [利用大型预训练神经模型进行文本解毒](https://doi.org/10.18653/v1/2021.emnlp-main.629)。在
    *2021年自然语言处理实证方法会议论文集*，第7979–7996页，在线及多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Dev et al. (2022) Sunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz, Jiao
    Sun, Yu Hou, Mattie Sanseverino, Jiin Kim, Akihiro Nishi, Nanyun Peng, and Kai-Wei
    Chang. 2022. [On measures of biases and harms in NLP](https://aclanthology.org/2022.findings-aacl.24).
    In *Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022,
    Online only, November 20-23, 2022*, pages 246–267\. Association for Computational
    Linguistics.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dev 等人 (2022) Sunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz, Jiao Sun,
    Yu Hou, Mattie Sanseverino, Jiin Kim, Akihiro Nishi, Nanyun Peng 和 Kai-Wei Chang.
    2022. [关于 NLP 中偏见和伤害的测量](https://aclanthology.org/2022.findings-aacl.24)。在 *Association
    for Computational Linguistics: AACL-IJCNLP 2022 的发现, 仅在线, 2022年11月20-23日*，第246–267页。计算语言学协会。'
- en: 'Gao et al. (2021) Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. [SimCSE:
    Simple contrastive learning of sentence embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.552).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 6894–6910, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等人 (2021) Tianyu Gao, Xingcheng Yao 和 Danqi Chen. 2021. [SimCSE: 简单的句子嵌入对比学习](https://doi.org/10.18653/v1/2021.emnlp-main.552)。在
    *2021年自然语言处理实证方法会议论文集*，第6894–6910页，在线及多米尼加共和国蓬塔卡纳。计算语言学协会。'
- en: 'Gong et al. (2019) Hongyu Gong, Suma Bhat, Lingfei Wu, JinJun Xiong, and Wen-mei
    Hwu. 2019. [Reinforcement learning based text style transfer without parallel
    training corpus](https://doi.org/10.18653/v1/N19-1320). In *Proceedings of the
    2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    3168–3180, Minneapolis, Minnesota. Association for Computational Linguistics.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong 等（2019）Hongyu Gong, Suma Bhat, Lingfei Wu, JinJun Xiong, 和 Wen-mei Hwu。2019年。[基于强化学习的文本风格转换，无需平行训练语料库](https://doi.org/10.18653/v1/N19-1320)。在*2019年北美计算语言学协会会议：人类语言技术会议论文集，第1卷（长篇和短篇论文）*，页码
    3168–3180，明尼阿波利斯，美国明尼苏达州。计算语言学协会。
- en: 'Hallinan et al. (2023a) Skyler Hallinan, Faeze Brahman, Ximing Lu, Jaehun Jung,
    Sean Welleck, and Yejin Choi. 2023a. [STEER: unified style transfer with expert
    reinforcement](https://aclanthology.org/2023.findings-emnlp.506). In *Findings
    of the Association for Computational Linguistics: EMNLP 2023, Singapore, December
    6-10, 2023*, pages 7546–7562\. Association for Computational Linguistics.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hallinan 等（2023a）Skyler Hallinan, Faeze Brahman, Ximing Lu, Jaehun Jung, Sean
    Welleck, 和 Yejin Choi。2023a年。[STEER：具有专家强化的统一风格转换](https://aclanthology.org/2023.findings-emnlp.506)。在*计算语言学协会发现：EMNLP
    2023，新加坡，2023年12月6-10日*，页码 7546–7562。计算语言学协会。
- en: 'Hallinan et al. (2023b) Skyler Hallinan, Alisa Liu, Yejin Choi, and Maarten
    Sap. 2023b. [Detoxifying text with MaRCo: Controllable revision with experts and
    anti-experts](https://doi.org/10.18653/v1/2023.acl-short.21). In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    2: Short Papers)*, pages 228–242, Toronto, Canada. Association for Computational
    Linguistics.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hallinan 等（2023b）Skyler Hallinan, Alisa Liu, Yejin Choi, 和 Maarten Sap。2023b年。[使用MaRCo进行文本去毒化：与专家和反专家的可控修订](https://doi.org/10.18653/v1/2023.acl-short.21)。在*第61届计算语言学协会年会会议论文集（第2卷：短篇论文）*，页码
    228–242，多伦多，加拿大。计算语言学协会。
- en: He et al. (2020) Junxian He, Xinyi Wang, Graham Neubig, and Taylor Berg-Kirkpatrick.
    2020. [A probabilistic formulation of unsupervised text style transfer](https://openreview.net/forum?id=HJlA0C4tPS).
    In *8th International Conference on Learning Representations, ICLR 2020, Addis
    Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2020）Junxian He, Xinyi Wang, Graham Neubig, 和 Taylor Berg-Kirkpatrick。2020年。[无监督文本风格转换的概率公式化](https://openreview.net/forum?id=HJlA0C4tPS)。在*第8届国际学习表征会议，ICLR
    2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26-30日*。OpenReview.net。
- en: Hinton et al. (2015) Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.
    [Distilling the knowledge in a neural network](http://arxiv.org/abs/1503.02531).
    *CoRR*, abs/1503.02531.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等（2015）Geoffrey E. Hinton, Oriol Vinyals, 和 Jeffrey Dean。2015年。[蒸馏神经网络中的知识](http://arxiv.org/abs/1503.02531)。*CoRR*，abs/1503.02531。
- en: 'Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023.
    [Distilling step-by-step! outperforming larger language models with less training
    data and smaller model sizes](https://doi.org/10.18653/v1/2023.findings-acl.507).
    In *Findings of the Association for Computational Linguistics: ACL 2023, Toronto,
    Canada, July 9-14, 2023*, pages 8003–8017\. Association for Computational Linguistics.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh 等（2023）Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa
    Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, 和 Tomas Pfister。2023年。[逐步蒸馏！用更少的训练数据和更小的模型规模超越更大的语言模型](https://doi.org/10.18653/v1/2023.findings-acl.507)。在*计算语言学协会发现：ACL
    2023，多伦多，加拿大，2023年7月9-14日*，页码 8003–8017。计算语言学协会。
- en: Huang et al. (2022) Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi
    Wang, Hongkun Yu, and Jiawei Han. 2022. [Large language models can self-improve](https://doi.org/10.48550/arXiv.2210.11610).
    *CoRR*, abs/2210.11610.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2022）Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang,
    Hongkun Yu, 和 Jiawei Han。2022年。[大型语言模型可以自我改进](https://doi.org/10.48550/arXiv.2210.11610)。*CoRR*，abs/2210.11610。
- en: Jhamtani et al. (2017) Harsh Jhamtani, Varun Gangal, Eduard Hovy, and Eric Nyberg.
    2017. [Shakespearizing modern language using copy-enriched sequence to sequence
    models](https://doi.org/10.18653/v1/W17-4902). In *Proceedings of the Workshop
    on Stylistic Variation*, pages 10–19, Copenhagen, Denmark. Association for Computational
    Linguistics.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jhamtani 等（2017）Harsh Jhamtani, Varun Gangal, Eduard Hovy, 和 Eric Nyberg。2017年。[使用复制增强序列到序列模型将现代语言莎士比亚化](https://doi.org/10.18653/v1/W17-4902)。在*风格变异研讨会会议论文集*，页码
    10–19，哥本哈根，丹麦。计算语言学协会。
- en: 'Jin et al. (2022) Di Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova, and Rada
    Mihalcea. 2022. [Deep Learning for Text Style Transfer: A Survey](https://doi.org/10.1162/coli_a_00426).
    *Computational Linguistics*, 48(1):155–205.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等人（2022）Di Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova, 和 Rada Mihalcea。2022。[深度学习用于文本风格转换：综述](https://doi.org/10.1162/coli_a_00426)。*计算语言学*，48(1):155–205。
- en: Krishna et al. (2020) Kalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.
    [Reformulating unsupervised style transfer as paraphrase generation](https://doi.org/10.18653/v1/2020.emnlp-main.55).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2020, Online, November 16-20, 2020*, pages 737–762\. Association
    for Computational Linguistics.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishna 等人（2020）Kalpesh Krishna, John Wieting, 和 Mohit Iyyer。2020。[将无监督风格转换重新表述为释义生成](https://doi.org/10.18653/v1/2020.emnlp-main.55)。发表于*2020
    年自然语言处理实证方法会议论文集，EMNLP 2020，在线，2020年11月16-20日*，第737–762页。计算语言学协会。
- en: 'Lai et al. (2021) Huiyuan Lai, Antonio Toral, and Malvina Nissim. 2021. [Thank
    you BART! rewarding pre-trained models improves formality style transfer](https://doi.org/10.18653/v1/2021.acl-short.62).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 2: Short Papers)*, pages 484–494, Online. Association for Computational
    Linguistics.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai 等人（2021）Huiyuan Lai, Antonio Toral, 和 Malvina Nissim。2021。[感谢 BART！奖励预训练模型改进正式风格转换](https://doi.org/10.18653/v1/2021.acl-short.62)。发表于*第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第2卷：短篇论文）*，第484–494页，在线。计算语言学协会。
- en: 'Li et al. (2018) Juncen Li, Robin Jia, He He, and Percy Liang. 2018. [Delete,
    retrieve, generate: a simple approach to sentiment and style transfer](https://doi.org/10.18653/v1/n18-1169).
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New
    Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)*, pages 1865–1874\.
    Association for Computational Linguistics.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2018）Juncen Li, Robin Jia, He He, 和 Percy Liang。2018。[删除、检索、生成：一种简单的情感和风格转换方法](https://doi.org/10.18653/v1/n18-1169)。发表于*2018年北美计算语言学协会年会：人类语言技术会议论文集，NAACL-HLT
    2018，美国路易斯安那州新奥尔良，2018年6月1-6日，第1卷（长篇论文）*，第1865–1874页。计算语言学协会。
- en: Li et al. (2022) Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang,
    Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, Wenhu Chen, and Xifeng Yan.
    2022. [Explanations from large language models make small reasoners better](https://doi.org/10.48550/arXiv.2210.06726).
    *CoRR*, abs/2210.06726.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2022）Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun
    Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, Wenhu Chen, 和 Xifeng Yan。2022。[来自大型语言模型的解释使小型推理器表现更佳](https://doi.org/10.48550/arXiv.2210.06726)。*CoRR*，abs/2210.06726。
- en: 'Liu et al. (2022) Ao Liu, An Wang, and Naoaki Okazaki. 2022. [Semi-supervised
    formality style transfer with consistency training](https://doi.org/10.18653/v1/2022.acl-long.321).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022*,
    pages 4689–4701\. Association for Computational Linguistics.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2022）Ao Liu, An Wang, 和 Naoaki Okazaki。2022。[基于一致性训练的半监督正式风格转换](https://doi.org/10.18653/v1/2022.acl-long.321)。发表于*第60届计算语言学协会年会（第1卷：长篇论文），ACL
    2022，爱尔兰都柏林，2022年5月22-27日*，第4689–4701页。计算语言学协会。
- en: 'Liu et al. (2020) Dayiheng Liu, Jie Fu, Yidan Zhang, Chris Pal, and Jiancheng
    Lv. 2020. Revision in continuous space: Unsupervised text style transfer without
    adversarial learning. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 34, pages 8376–8383.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2020）Dayiheng Liu, Jie Fu, Yidan Zhang, Chris Pal, 和 Jiancheng Lv。2020。在连续空间中的修订：无对抗学习的无监督文本风格转换。发表于*AAAI
    人工智能会议论文集*，第34卷，第8376–8383页。
- en: Liu et al. (2021) Ruibo Liu, Chongyang Gao, Chenyan Jia, Guangxuan Xu, and Soroush
    Vosoughi. 2021. Non-parallel text style transfer with self-parallel supervision.
    In *International Conference on Learning Representations*.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2021）Ruibo Liu, Chongyang Gao, Chenyan Jia, Guangxuan Xu, 和 Soroush Vosoughi。2021。具有自监督的非平行文本风格转换。发表于*国际学习表征会议*。
- en: 'Logacheva et al. (2022a) Varvara Logacheva, Daryna Dementieva, Irina Krotova,
    Alena Fenogenova, Irina Nikishina, Tatiana Shavrina, and Alexander Panchenko.
    2022a. [A study on manual and automatic evaluation for text style transfer: The
    case of detoxification](https://doi.org/10.18653/v1/2022.humeval-1.8). In *Proceedings
    of the 2nd Workshop on Human Evaluation of NLP Systems (HumEval)*, pages 90–101,
    Dublin, Ireland. Association for Computational Linguistics.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logacheva et al. (2022a) Varvara Logacheva, Daryna Dementieva, Irina Krotova,
    Alena Fenogenova, Irina Nikishina, Tatiana Shavrina, 和 Alexander Panchenko. 2022a.
    [文本风格转换的人工与自动评估研究：去毒化案例](https://doi.org/10.18653/v1/2022.humeval-1.8)。在*第2届NLP系统人工评估研讨会（HumEval）*，第90–101页，爱尔兰都柏林。计算语言学协会。
- en: 'Logacheva et al. (2022b) Varvara Logacheva, Daryna Dementieva, Sergey Ustyantsev,
    Daniil Moskovskiy, David Dale, Irina Krotova, Nikita Semenov, and Alexander Panchenko.
    2022b. [ParaDetox: Detoxification with parallel data](https://doi.org/10.18653/v1/2022.acl-long.469).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 6804–6818, Dublin, Ireland. Association
    for Computational Linguistics.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logacheva et al. (2022b) Varvara Logacheva, Daryna Dementieva, Sergey Ustyantsev,
    Daniil Moskovskiy, David Dale, Irina Krotova, Nikita Semenov, 和 Alexander Panchenko.
    2022b. [ParaDetox：使用并行数据进行去毒化](https://doi.org/10.18653/v1/2022.acl-long.469)。在*第60届计算语言学协会年会（第1卷：长文）*，第6804–6818页，爱尔兰都柏林。计算语言学协会。
- en: Lucy and Bamman (2021) Li Lucy and David Bamman. 2021. [Gender and representation
    bias in GPT-3 generated stories](https://doi.org/10.18653/v1/2021.nuse-1.5). In
    *Proceedings of the Third Workshop on Narrative Understanding*, pages 48–55, Virtual.
    Association for Computational Linguistics.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lucy and Bamman (2021) Li Lucy 和 David Bamman. 2021. [GPT-3生成故事中的性别和表征偏差](https://doi.org/10.18653/v1/2021.nuse-1.5)。在*第三届叙事理解研讨会论文集*，第48–55页，虚拟。计算语言学协会。
- en: Luo et al. (2019) Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang,
    Xu Sun, and Zhifang Sui. 2019. [A dual reinforcement learning framework for unsupervised
    text style transfer](https://doi.org/10.24963/ijcai.2019/711). In *Proceedings
    of the Twenty-Eighth International Joint Conference on Artificial Intelligence,
    IJCAI 2019, Macao, China, August 10-16, 2019*, pages 5116–5122\. ijcai.org.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2019) Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang,
    Xu Sun, 和 Zhifang Sui. 2019. [一种用于无监督文本风格转换的双重强化学习框架](https://doi.org/10.24963/ijcai.2019/711)。在*第二十八届国际人工智能联合会议论文集，IJCAI
    2019，澳门，中国，2019年8月10-16日*，第5116–5122页。ijcai.org。
- en: 'Nangi et al. (2021) Sharmila Reddy Nangi, Niyati Chhaya, Sopan Khosla, Nikhil
    Kaushik, and Harshit Nyati. 2021. [Counterfactuals to control latent disentangled
    text representations for style transfer](https://doi.org/10.18653/v1/2021.acl-short.7).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 2: Short Papers)*, pages 40–48, Online. Association for Computational
    Linguistics.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nangi et al. (2021) Sharmila Reddy Nangi, Niyati Chhaya, Sopan Khosla, Nikhil
    Kaushik, 和 Harshit Nyati. 2021. [用于风格转换的潜在解缠结文本表示的反事实控制](https://doi.org/10.18653/v1/2021.acl-short.7)。在*第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第2卷：短文）*，第40–48页，在线。计算语言学协会。
- en: Niu et al. (2018) Xing Niu, Sudha Rao, and Marine Carpuat. 2018. [Multi-task
    neural models for translating between styles within and across languages](https://aclanthology.org/C18-1086/).
    In *Proceedings of the 27th International Conference on Computational Linguistics,
    COLING 2018, Santa Fe, New Mexico, USA, August 20-26, 2018*, pages 1008–1021\.
    Association for Computational Linguistics.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu et al. (2018) Xing Niu, Sudha Rao, 和 Marine Carpuat. 2018. [用于在风格之间及跨语言翻译的多任务神经模型](https://aclanthology.org/C18-1086/)。在*第27届国际计算语言学大会，COLING
    2018，新墨西哥州圣菲，美国，2018年8月20-26日*，第1008–1021页。计算语言学协会。
- en: 'Ostheimer et al. (2023) Phil Ostheimer, Mayank Kumar Nagda, Marius Kloft, and
    Sophie Fellenz. 2023. [A call for standardization and validation of text style
    transfer evaluation](https://doi.org/10.18653/v1/2023.findings-acl.687). In *Findings
    of the Association for Computational Linguistics: ACL 2023*, pages 10791–10815,
    Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ostheimer et al. (2023) Phil Ostheimer, Mayank Kumar Nagda, Marius Kloft, 和
    Sophie Fellenz. 2023. [呼吁标准化和验证文本风格转换评估](https://doi.org/10.18653/v1/2023.findings-acl.687)。在*计算语言学协会发现：ACL
    2023*，第10791–10815页，加拿大多伦多。计算语言学协会。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html).
    In *NeurIPS*.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等 (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F. Christiano, Jan Leike, 和 Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html)。在
    *NeurIPS*。
- en: 'Post (2018) Matt Post. 2018. [A call for clarity in reporting BLEU scores](https://doi.org/10.18653/v1/W18-6319).
    In *Proceedings of the Third Conference on Machine Translation: Research Papers*,
    pages 186–191, Brussels, Belgium. Association for Computational Linguistics.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Post (2018) Matt Post. 2018. [A call for clarity in reporting BLEU scores](https://doi.org/10.18653/v1/W18-6319)。在
    *第三届机器翻译会议：研究论文集*，第186–191页，比利时布鲁塞尔。计算语言学协会。
- en: 'Post and Vilar (2018) Matt Post and David Vilar. 2018. [Fast lexically constrained
    decoding with dynamic beam allocation for neural machine translation](https://doi.org/10.18653/V1/N18-1119).
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New
    Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)*, pages 1314–1324\.
    Association for Computational Linguistics.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Post 和 Vilar (2018) Matt Post 和 David Vilar. 2018. [Fast lexically constrained
    decoding with dynamic beam allocation for neural machine translation](https://doi.org/10.18653/V1/N18-1119)。在
    *2018年计算语言学协会北美章节会议：人类语言技术, NAACL-HLT 2018, 美国路易斯安那州新奥尔良, 2018年6月1-6日, 第1卷（长篇论文）*，第1314–1324页。计算语言学协会。
- en: 'Pu and Demberg (2023) Dongqi Pu and Vera Demberg. 2023. [Chatgpt vs human-authored
    text: Insights into controllable text summarization and sentence style transfer](https://doi.org/10.18653/v1/2023.acl-srw.1).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics: Student Research Workshop, ACL 2023, Toronto, Canada, July 9-14,
    2023*, pages 1–18\. Association for Computational Linguistics.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pu 和 Demberg (2023) Dongqi Pu 和 Vera Demberg. 2023. [Chatgpt vs human-authored
    text: Insights into controllable text summarization and sentence style transfer](https://doi.org/10.18653/v1/2023.acl-srw.1)。在
    *第61届计算语言学协会年会：学生研究研讨会, ACL 2023, 加拿大多伦多, 2023年7月9-14日*，第1–18页。计算语言学协会。'
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](http://jmlr.org/papers/v21/20-074.html).
    *J. Mach. Learn. Res.*, 21:140:1–140:67.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等 (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J. Liu. 2020. [Exploring the
    limits of transfer learning with a unified text-to-text transformer](http://jmlr.org/papers/v21/20-074.html)。*J.
    Mach. Learn. Res.*，21:140:1–140:67。
- en: 'Rao and Tetreault (2018) Sudha Rao and Joel Tetreault. 2018. [Dear sir or madam,
    may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality
    style transfer](https://doi.org/10.18653/v1/N18-1012). In *Proceedings of the
    2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 129–140,
    New Orleans, Louisiana. Association for Computational Linguistics.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rao 和 Tetreault (2018) Sudha Rao 和 Joel Tetreault. 2018. [Dear sir or madam,
    may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality
    style transfer](https://doi.org/10.18653/v1/N18-1012)。在 *2018年计算语言学协会北美章节会议：人类语言技术,
    第1卷（长篇论文）*，第129–140页，美国路易斯安那州新奥尔良。计算语言学协会。'
- en: 'Reid and Zhong (2021) Machel Reid and Victor Zhong. 2021. [LEWIS: Levenshtein
    editing for unsupervised text style transfer](https://doi.org/10.18653/v1/2021.findings-acl.344).
    In *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*,
    pages 3932–3944, Online. Association for Computational Linguistics.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reid 和 Zhong (2021) Machel Reid 和 Victor Zhong. 2021. [LEWIS: Levenshtein editing
    for unsupervised text style transfer](https://doi.org/10.18653/v1/2021.findings-acl.344)。在
    *计算语言学协会发现：ACL-IJCNLP 2021*，第3932–3944页，在线。计算语言学协会。'
- en: 'Reif et al. (2022) Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris
    Callison-Burch, and Jason Wei. 2022. [A recipe for arbitrary text style transfer
    with large language models](https://doi.org/10.18653/v1/2022.acl-short.94). In
    *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
    (Volume 2: Short Papers)*, pages 837–848, Dublin, Ireland. Association for Computational
    Linguistics.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reif 等（2022）Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch,
    和 Jason Wei。2022年。[使用大型语言模型进行任意文本风格转换的配方](https://doi.org/10.18653/v1/2022.acl-short.94)。发表于
    *第60届计算语言学协会年会（卷2：短篇论文）*，第837–848页，爱尔兰都柏林。计算语言学协会。
- en: 'Saakyan and Muresan (2023) Arkadiy Saakyan and Smaranda Muresan. 2023. [ICLEF:
    in-context learning with expert feedback for explainable style transfer](https://doi.org/10.48550/ARXIV.2309.08583).
    *CoRR*, abs/2309.08583.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saakyan 和 Muresan（2023）Arkadiy Saakyan 和 Smaranda Muresan。2023年。[ICLEF：使用专家反馈进行可解释风格转换的上下文学习](https://doi.org/10.48550/ARXIV.2309.08583)。*CoRR*，abs/2309.08583。
- en: 'Shang et al. (2019) Mingyue Shang, Piji Li, Zhenxin Fu, Lidong Bing, Dongyan
    Zhao, Shuming Shi, and Rui Yan. 2019. [Semi-supervised text style transfer: Cross
    projection in latent space](https://doi.org/10.18653/v1/D19-1499). In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP
    2019, Hong Kong, China, November 3-7, 2019*, pages 4936–4945\. Association for
    Computational Linguistics.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang 等（2019）Mingyue Shang, Piji Li, Zhenxin Fu, Lidong Bing, Dongyan Zhao,
    Shuming Shi, 和 Rui Yan。2019年。[半监督文本风格转换：在潜在空间中的交叉投影](https://doi.org/10.18653/v1/D19-1499)。发表于
    *2019年自然语言处理经验方法会议暨第9届国际联合自然语言处理大会，EMNLP-IJCNLP 2019，中国香港，2019年11月3-7日*，第4936–4945页。计算语言学协会。
- en: 'Shen et al. (2017) Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi S. Jaakkola.
    2017. [Style transfer from non-parallel text by cross-alignment](https://proceedings.neurips.cc/paper/2017/hash/2d2c8394e31101a261abf1784302bf75-Abstract.html).
    In *Advances in Neural Information Processing Systems 30: Annual Conference on
    Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
    USA*, pages 6830–6841.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等（2017）Tianxiao Shen, Tao Lei, Regina Barzilay, 和 Tommi S. Jaakkola。2017年。[通过交叉对齐进行非平行文本的风格转换](https://proceedings.neurips.cc/paper/2017/hash/2d2c8394e31101a261abf1784302bf75-Abstract.html)。发表于
    *神经信息处理系统进展 30：2017年神经信息处理系统年会，2017年12月4-9日，美国加州长滩*，第6830–6841页。
- en: 'Shridhar et al. (2023) Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan.
    2023. [Distilling reasoning capabilities into smaller language models](https://doi.org/10.18653/v1/2023.findings-acl.441).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    7059–7073, Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar 等（2023）Kumar Shridhar, Alessandro Stolfo, 和 Mrinmaya Sachan。2023年。[将推理能力提炼到较小的语言模型中](https://doi.org/10.18653/v1/2023.findings-acl.441)。发表于
    *计算语言学协会发现：ACL 2023*，第7059–7073页，加拿大多伦多。计算语言学协会。
- en: 'Suzgun et al. (2022) Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022.
    [Prompt-and-rerank: A method for zero-shot and few-shot arbitrary textual style
    transfer with small language models](https://doi.org/10.18653/v1/2022.emnlp-main.141).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 2195–2222, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suzgun 等（2022）Mirac Suzgun, Luke Melas-Kyriazi, 和 Dan Jurafsky。2022年。[Prompt-and-rerank：一种针对小型语言模型的零样本和少样本任意文本风格转换方法](https://doi.org/10.18653/v1/2022.emnlp-main.141)。发表于
    *2022年自然语言处理经验方法会议论文集*，第2195–2222页，阿布扎比，阿联酋。计算语言学协会。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori 等（2023）Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen
    Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto。2023年。《斯坦福 alpaca：一种遵循指令的
    llama 模型》。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. [Llama
    2: Open foundation and fine-tuned chat models](https://doi.org/10.48550/ARXIV.2307.09288).
    *CoRR*, abs/2307.09288.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron等人（2023）**Hugo Touvron**, **Louis Martin**, **Kevin Stone**, **Peter
    Albert**, **Amjad Almahairi**, **Yasmine Babaei**, **Nikolay Bashlykov**, **Soumya
    Batra**, **Prajjwal Bhargava**, **Shruti Bhosale**, **Dan Bikel**, **Lukas Blecher**,
    **Cristian Canton-Ferrer**, **Moya Chen**, **Guillem Cucurull**, **David Esiobu**,
    **Jude Fernandes**, **Jeremy Fu**, **Wenyin Fu**, **Brian Fuller**, **Cynthia
    Gao**, **Vedanuj Goswami**, **Naman Goyal**, **Anthony Hartshorn**, **Saghar Hosseini**,
    **Rui Hou**, **Hakan Inan**, **Marcin Kardas**, **Viktor Kerkez**, **Madian Khabsa**,
    **Isabel Kloumann**, **Artem Korenev**, **Punit Singh Koura**, **Marie-Anne Lachaux**,
    **Thibaut Lavril**, **Jenya Lee**, **Diana Liskovich**, **Yinghai Lu**, **Yuning
    Mao**, **Xavier Martinet**, **Todor Mihaylov**, **Pushkar Mishra**, **Igor Molybog**,
    **Yixin Nie**, **Andrew Poulton**, **Jeremy Reizenstein**, **Rashi Rungta**, **Kalyan
    Saladi**, **Alan Schelten**, **Ruan Silva**, **Eric Michael Smith**, **Ranjan
    Subramanian**, **Xiaoqing Ellen Tan**, **Binh Tang**, **Ross Taylor**, **Adina
    Williams**, **Jian Xiang Kuan**, **Puxin Xu**, **Zheng Yan**, **Iliyan Zarov**,
    **Yuchen Zhang**, **Angela Fan**, **Melanie Kambadur**, **Sharan Narang**, **Aurélien
    Rodriguez**, **Robert Stojnic**, **Sergey Edunov** 和 **Thomas Scialom**。2023。
    [Llama 2: 开放基础和微调聊天模型](https://doi.org/10.48550/ARXIV.2307.09288)。 *CoRR*，abs/2307.09288。'
- en: 'Wang et al. (2019a) Ke Wang, Hang Hua, and Xiaojun Wan. 2019a. [Controllable
    unsupervised text attribute transfer via editing entangled latent representation](https://proceedings.neurips.cc/paper/2019/hash/8804f94e16ba5b680e239a554a08f7d2-Abstract.html).
    In *Advances in Neural Information Processing Systems 32: Annual Conference on
    Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
    Vancouver, BC, Canada*, pages 11034–11044.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2019a）**Ke Wang**, **Hang Hua** 和 **Xiaojun Wan**。2019a。 [可控无监督文本属性转移通过编辑纠缠的潜在表示](https://proceedings.neurips.cc/paper/2019/hash/8804f94e16ba5b680e239a554a08f7d2-Abstract.html)。在
    *神经信息处理系统进展第32卷：2019年神经信息处理系统年会（NeurIPS 2019），2019年12月8-14日，加拿大温哥华*，第11034–11044页。
- en: 'Wang et al. (2023a) Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing
    Yin, and Xiang Ren. 2023a. [SCOTT: Self-consistent chain-of-thought distillation](https://doi.org/10.18653/v1/2023.acl-long.304).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 5546–5558, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2023a）**Peifeng Wang**, **Zhengyang Wang**, **Zheng Li**, **Yifan Gao**,
    **Bing Yin** 和 **Xiang Ren**。2023a。 [SCOTT: 自洽链式思维蒸馏](https://doi.org/10.18653/v1/2023.acl-long.304)。在
    *第61届计算语言学协会年会论文集（第1卷：长篇论文）*，第5546–5558页，加拿大多伦多。计算语言学协会。'
- en: 'Wang et al. (2023b) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. [Self-instruct:
    Aligning language models with self-generated instructions](https://doi.org/10.18653/v1/2023.acl-long.754).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*,
    pages 13484–13508\. Association for Computational Linguistics.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2023b）**Yizhong Wang**, **Yeganeh Kordi**, **Swaroop Mishra**, **Alisa
    Liu**, **Noah A. Smith**, **Daniel Khashabi** 和 **Hannaneh Hajishirzi**。2023b。
    [Self-instruct: 将语言模型与自生成指令对齐](https://doi.org/10.18653/v1/2023.acl-long.754)。在
    *第61届计算语言学协会年会论文集（第1卷：长篇论文），ACL 2023，加拿大多伦多，2023年7月9-14日*，第13484–13508页。计算语言学协会。'
- en: Wang et al. (2019b) Yunli Wang, Yu Wu, Lili Mou, Zhoujun Li, and Wenhan Chao.
    2019b. [Harnessing pre-trained neural networks with rules for formality style
    transfer](https://doi.org/10.18653/v1/D19-1365). In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong,
    China, November 3-7, 2019*, pages 3571–3576\. Association for Computational Linguistics.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2019b）王云力，吴宇，牟莉莉，李周俊，赵文汉。2019b。[利用带规则的预训练神经网络进行正式风格转移](https://doi.org/10.18653/v1/D19-1365)。在*2019年自然语言处理经验方法会议及第九届国际联合自然语言处理会议论文集，EMNLP-IJCNLP
    2019，香港，中国，2019年11月3-7日*，第3571–3576页。计算语言学协会。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. [Chain-of-thought
    prompting elicits reasoning in large language models](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html).
    In *NeurIPS*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等（2022）杰森·魏，王学之，戴尔·舒尔曼斯，马尔滕·博斯马，布赖恩·伊赫特，夏飞，艾德·H·池，刘奎，周丹尼。2022年。[思维链提示在大型语言模型中引发推理](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)。在*NeurIPS*。
- en: 'Wu et al. (2023) Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed,
    and Alham Fikri Aji. 2023. [Lamini-lm: A diverse herd of distilled models from
    large-scale instructions](https://doi.org/10.48550/arXiv.2304.14402). *CoRR*,
    abs/2304.14402.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2023）吴名浩，阿卜杜勒·瓦希德，张驰宇，穆罕默德·阿卜杜勒-马吉德，阿尔哈姆·菲克里·阿吉。2023年。[Lamini-lm：来自大规模指令的多样化蒸馏模型](https://doi.org/10.48550/arXiv.2304.14402)。*CoRR*，abs/2304.14402。
- en: 'Wu et al. (2019) Xing Wu, Tao Zhang, Liangjun Zang, Jizhong Han, and Songlin
    Hu. 2019. [Mask and infill: Applying masked language model for sentiment transfer](https://doi.org/10.24963/ijcai.2019/732).
    In *Proceedings of the Twenty-Eighth International Joint Conference on Artificial
    Intelligence, IJCAI-19*, pages 5271–5277\. International Joint Conferences on
    Artificial Intelligence Organization.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2019）吴星，张涛，臧亮军，韩继中，胡松林。2019年。[掩码与填充：应用掩码语言模型进行情感转移](https://doi.org/10.24963/ijcai.2019/732)。在*第二十八届国际人工智能联合会议论文集，IJCAI-19*，第5271–5277页。国际人工智能联合会议组织。
- en: Xiao et al. (2021) Fei Xiao, Liang Pang, Yanyan Lan, Yan Wang, Huawei Shen,
    and Xueqi Cheng. 2021. [Transductive learning for unsupervised text style transfer](https://doi.org/10.18653/v1/2021.emnlp-main.195).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November,
    2021*, pages 2510–2521\. Association for Computational Linguistics.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萧等（2021）萧飞，庞亮，兰艳艳，王岩，华为·申，程学启。2021年。[用于无监督文本风格转移的传导学习](https://doi.org/10.18653/v1/2021.emnlp-main.195)。在*2021年自然语言处理经验方法会议论文集，EMNLP
    2021，虚拟活动/多米尼加共和国蓬塔卡纳，2021年11月7-11日*，第2510–2521页。计算语言学协会。
- en: Xu et al. (2019) Ruochen Xu, Tao Ge, and Furu Wei. 2019. [Formality style transfer
    with hybrid textual annotations](http://arxiv.org/abs/1903.06353). *CoRR*, abs/1903.06353.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 徐等（2019）徐若晨，葛涛，魏福如。2019年。[通过混合文本注释进行正式风格转移](http://arxiv.org/abs/1903.06353)。*CoRR*，abs/1903.06353。
- en: 'Xu et al. (2012) Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and Colin
    Cherry. 2012. [Paraphrasing for style](https://aclanthology.org/C12-1177/). In
    *COLING 2012, 24th International Conference on Computational Linguistics, Proceedings
    of the Conference: Technical Papers, 8-15 December 2012, Mumbai, India*, pages
    2899–2914\. Indian Institute of Technology Bombay.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 徐等（2012）徐伟，艾伦·里特，比尔·多伦，拉尔夫·格里希曼，科林·切瑞。2012年。[风格的意译](https://aclanthology.org/C12-1177/)。在*COLING
    2012，第24届国际计算语言学会议，会议论文集：技术论文，2012年12月8-15日，孟买，印度*，第2899–2914页。印度理工学院孟买分校。
- en: Yi et al. (2021) Xiaoyuan Yi, Zhenghao Liu, Wenhao Li, and Maosong Sun. 2021.
    Text style transfer via learning style instance supported latent space. In *Proceedings
    of the Twenty-Ninth International Conference on International Joint Conferences
    on Artificial Intelligence*, pages 3801–3807.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易等（2021）易晓元，刘正浩，李文浩，孙茂松。2021年。通过学习风格实例支持的潜在空间进行文本风格转移。在*第二十九届国际联合会议论文集*，第3801–3807页。
- en: Zhang et al. (2020) Yi Zhang, Tao Ge, and Xu Sun. 2020. [Parallel data augmentation
    for formality style transfer](https://doi.org/10.18653/v1/2020.acl-main.294).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, ACL 2020, Online, July 5-10, 2020*, pages 3221–3228\. Association
    for Computational Linguistics.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2020）Yi Zhang, Tao Ge, 和 Xu Sun。2020年。[形式风格转移的平行数据增强](https://doi.org/10.18653/v1/2020.acl-main.294)。在*第58届计算语言学协会年会（ACL
    2020），线上，2020年7月5-10日*，第3221–3228页。计算语言学协会。
- en: 'Zhang et al. (2023) Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen
    Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu,
    Wei Bi, Freda Shi, and Shuming Shi. 2023. [Siren’s song in the AI ocean: A survey
    on hallucination in large language models](https://doi.org/10.48550/arXiv.2309.01219).
    *CoRR*, abs/2309.01219.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2023）Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu,
    Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei
    Bi, Freda Shi, 和 Shuming Shi。2023年。[AI海洋中的海妖之歌：大型语言模型中的幻觉调查](https://doi.org/10.48550/arXiv.2309.01219)。*CoRR*,
    abs/2309.01219。
- en: Appendices
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Method
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 方法
- en: Figure [5](#A1.F5 "Figure 5 ‣ Appendix A Method ‣ Distilling Text Style Transfer
    With Self-Explanation From LLMs") shows an input example for target-aware data
    generation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5](#A1.F5 "Figure 5 ‣ Appendix A Method ‣ Distilling Text Style Transfer With
    Self-Explanation From LLMs")展示了目标感知数据生成的输入示例。
- en: '![Refer to caption](img/b2fdb23c61937f25c7dcc3808c34e425.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b2fdb23c61937f25c7dcc3808c34e425.png)'
- en: 'Figure 5: Few-shot chain-of-thought prompting for data generation with supervised
    data (target-aware setting). We use the few-shot prompts that include a few examples
    to guide LLM to generate desired outputs in a standard format.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：少样本链式思维提示用于监督数据（目标感知设置）的数据生成。我们使用包含几个示例的少样本提示来引导LLM生成所需格式的输出。
- en: Appendix B Hyperparameter for Training Student Model
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 学生模型训练的超参数
- en: 'We set the maximal input and output sequence lengths to 512 and 256, respectively.
    To optimize the T5 model’s finetuning, we search both the learning rate and batch
    size within specified search spaces: $lr\in\{1e-3,5e-4,1e-5\}$. We finetune T5
    for 2,000 steps, evaluate performance on the validation set every 16 steps, and
    report the test performance on the best step. All T5 models are trained on four
    V3 TPUs.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将最大输入和输出序列长度设置为512和256，分别。为了优化T5模型的微调，我们在指定的搜索空间内搜索学习率和批次大小：$lr\in\{1e-3,5e-4,1e-5\}$。我们对T5进行2000步的微调，每16步在验证集上评估性能，并报告最佳步骤的测试性能。所有T5模型均在四台V3
    TPU上训练。
- en: Appendix C Preliminary Test on Evaluation Metrics
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 对评估指标的初步测试
- en: 'In our preliminary experiment, we evaluate model performance with several automatic
    metrics utilized by previous works Krishna et al. ([2020](#bib.bib18)); Luo et al.
    ([2019](#bib.bib28)); Reif et al. ([2022](#bib.bib39)). These automatic metrics
    have been widely used in unsupervised TST due to their independence from human-labeled
    parallel data. However, we find that the outcomes from these metrics do not align
    with the reference-BLEU score derived from human-annotated references. These automatic
    metrics evaluate transferred text from three aspects:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的初步实验中，我们使用了Krishna等人（[2020](#bib.bib18)）、Luo等人（[2019](#bib.bib28)）和Reif等人（[2022](#bib.bib39)）所采用的几种自动度量指标来评估模型性能。这些自动度量指标由于不依赖于人工标注的平行数据，已被广泛应用于无监督TST中。然而，我们发现这些指标的结果与由人工标注参考得出的参考-BLEU分数不一致。这些自动度量指标从三个方面评估转移文本：
- en: '1.'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Similarity: To evaluate the similarity between the source text and the transferred
    text, we employ BERTscore and self-BLEU. For BERTscore calculations, we use the
    SimCSE-large model Gao et al. ([2021](#bib.bib8)) as the backbone.'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相似性：为了评估源文本与转移文本之间的相似性，我们使用BERTscore和self-BLEU进行评估。对于BERTscore的计算，我们使用SimCSE-large模型Gao等人（[2021](#bib.bib8)）作为基础。
- en: '2.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Transfer Accuracy: To evaluate the efficacy of the style transfer, we employ
    a classifier Babakov et al. ([2023](#bib.bib2)) to determine whether the transferred
    text successfully achieves the desired style.'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转移准确性：为了评估风格转移的效果，我们采用分类器Babakov等人（[2023](#bib.bib2)）来确定转移文本是否成功实现了期望的风格。
- en: '3.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Fluency: To access the fluency of the transferred text, we compute its perplexity
    using GPT. Additionally, we utilize a classifier trained on the Corpus of Linguistic
    Acceptability (CoLA) from Krishna et al. ([2020](#bib.bib18)) to determine the
    grammaticality of the transferred text.'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 流畅度：为了评估转移文本的流畅度，我们使用 GPT 计算其困惑度。此外，我们还利用 Krishnan 等人（[2020](#bib.bib18)）在语言可接受性语料库
    (CoLA) 上训练的分类器来确定转移文本的语法性。
- en: In this preliminary experiment, we conduct experiments using varying training
    sizes from the formality transfer (F&R) dataset. These experiments are carried
    out in a target-blind setting, where we finetune a T5-large model using the synthetic
    data generated from LLM. For assessing transfer accuracy, we employ a binary classifier
    introduced by Babakov et al. ([2023](#bib.bib2)), which is a RoBERTa-base model
    finetuned on the GYAFC’s training set. This classifier achieves a test accuracy
    of 0.91\. As Table [3](#A3.T3 "Table 3 ‣ Appendix C Preliminary Test on Evaluation
    Metrics ‣ Distilling Text Style Transfer With Self-Explanation From LLMs") shows,
    the outcomes from these metrics did not correspond well with the reference-BLEU
    score. We thus opt to report the BLEU score in the paper.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个初步实验中，我们使用来自形式转移（F&R）数据集的不同训练规模进行实验。这些实验是在目标盲设置下进行的，我们使用从 LLM 生成的合成数据对 T5-large
    模型进行微调。为了评估转移准确性，我们使用 Babakov 等人（[2023](#bib.bib2)）介绍的二分类器，这是一种在 GYAFC 训练集上微调的
    RoBERTa-base 模型。该分类器在测试集上的准确率为 0.91。正如表 [3](#A3.T3 "Table 3 ‣ Appendix C Preliminary
    Test on Evaluation Metrics ‣ Distilling Text Style Transfer With Self-Explanation
    From LLMs") 所示，这些指标的结果与参考-BLEU 分数并不完全一致。因此，我们选择在论文中报告 BLEU 分数。
- en: '| # of data | Ref-BLEU | BERTScore | Self-BLEU | Tra. Acc. | PPL | CoLA |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 数据量 | Ref-BLEU | BERTScore | Self-BLEU | 转换准确率 | PPL | CoLA |'
- en: '| 1000 | 72.54 | 0.96 | 45.34 | 0.94 | 61.15 | 0.95 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 72.54 | 0.96 | 45.34 | 0.94 | 61.15 | 0.95 |'
- en: '| 2000 | 73.13 | 0.96 | 46.89 | 0.93 | 59.02 | 0.95 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 2000 | 73.13 | 0.96 | 46.89 | 0.93 | 59.02 | 0.95 |'
- en: '| 5000 | 71.92 | 0.96 | 50.71 | 0.90 | 65.54 | 0.94 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 5000 | 71.92 | 0.96 | 50.71 | 0.90 | 65.54 | 0.94 |'
- en: '| 10000 | 72.86 | 0.96 | 51.15 | 0.89 | 63.98 | 0.95 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 10000 | 72.86 | 0.96 | 51.15 | 0.89 | 63.98 | 0.95 |'
- en: '| 20000 | 72.90 | 0.96 | 49.89 | 0.90 | 64.66 | 0.94 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 20000 | 72.90 | 0.96 | 49.89 | 0.90 | 64.66 | 0.94 |'
- en: 'Table 3: Preliminary result on GYAFC (F&R) for investigating evaluation metrics.
    Tra. Acc.: transfer accuracy, PPL: perplexity.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：GYAFC (F&R) 上评估指标的初步结果。转移准确率：转换准确性，PPL：困惑度。
- en: Appendix D Experiment with T5-XL
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D T5-XL 实验
- en: We conduct a concise experiment to apply our CoTeX to T5-XL (containing 3B parameters).
    As Table [4](#A4.T4 "Table 4 ‣ Appendix D Experiment with T5-XL ‣ Distilling Text
    Style Transfer With Self-Explanation From LLMs") shows, our CoTeX-TA outperforms
    SFT across all the data sizes.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一项简要实验，将我们的 CoTeX 应用到 T5-XL（包含 3B 参数）上。正如表 [4](#A4.T4 "Table 4 ‣ Appendix
    D Experiment with T5-XL ‣ Distilling Text Style Transfer With Self-Explanation
    From LLMs") 所示，我们的 CoTeX-TA 在所有数据规模上都优于 SFT。
- en: '| # Data | SFT | CoTex-TB | CoTex-TA |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 数据量 | SFT | CoTex-TB | CoTex-TA |'
- en: '| 1000 | 49.15 | 46.64 | 53.93 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 49.15 | 46.64 | 53.93 |'
- en: '| 2000 | 51.58 | 47.56 | 54.26 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 2000 | 51.58 | 47.56 | 54.26 |'
- en: '| 5000 | 52.91 | 47.92 | 54.83 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 5000 | 52.91 | 47.92 | 54.83 |'
- en: '| 10000 | 52.32 | 47.96 | 55.13 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 10000 | 52.32 | 47.96 | 55.13 |'
- en: '| 15000 | 52.88 | 48.47 | 55.19 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 15000 | 52.88 | 48.47 | 55.19 |'
- en: 'Table 4: Finetuning T5-XL on detoxification dataset with our CoTeX or SFT.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在去毒数据集上用我们的 CoTeX 或 SFT 微调 T5-XL。
- en: Appendix E Human Evaluation on Generated Reasonings
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 对生成推理的人工评估
- en: Table [5](#A5.T5 "Table 5 ‣ Appendix E Human Evaluation on Generated Reasonings
    ‣ Distilling Text Style Transfer With Self-Explanation From LLMs") shows our human
    evaluation protocol and instructions.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [5](#A5.T5 "Table 5 ‣ Appendix E Human Evaluation on Generated Reasonings
    ‣ Distilling Text Style Transfer With Self-Explanation From LLMs") 显示了我们的人工评估协议和说明。
- en: '| Level | Criteria |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 级别 | 标准 |'
- en: '| Rate A | • Valid, acceptable and satisfying (subject to the annotator) response;
    • Accurately identified the most cues for text style transfer; • The reasoning
    path can directly lead to the transferred text. |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 评级 A | • 有效、可接受且令人满意的（视注释者而定）回应； • 准确识别了文本风格转移的主要线索； • 推理路径可以直接引导到转移后的文本。
    |'
- en: '| Rate B | • The response is acceptable but has minor errors that can be improved;
    • Mirror errors include out-of-context content, minimal factual errors, missing
    many cues for text style transfer, etc. |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 评级 B | • 回应是可接受的，但有一些小错误可以改进； • 镜像错误包括内容脱离上下文、少量事实错误、遗漏了许多文本风格转移线索等。 |'
- en: '| Rate C | • The response is relevant but it has significant errors in the
    content; • Cannot identify any correct cues for text style transfer. • The reasoning
    path cannot lead to the transferred text. |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 评级 C | • 回答相关，但内容有显著错误；• 无法识别任何正确的文本风格转移提示。• 推理路径无法得出转移后的文本。 |'
- en: '| Rate D | • Invalid and unacceptable response; • Nothing related to the text
    style transfer task. |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 评级 D | • 无效且不可接受的回答；• 与文本风格转移任务无关。 |'
- en: '| Instruction: This task is text styles transfer that transfers a {$source_style}
    source text to a target text with style {$target_style}. Each example includes
    a source text and the corresponding model-generated rationales of the rewriting
    process as well as the transferred text. You evaluate the rationales of the rewriting
    process and do not take the quality of the transferred text into account. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 指示：此任务是文本风格转移，将{$source_style}源文本转移为具有{$target_style}风格的目标文本。每个示例包括源文本以及对应的模型生成的重写过程的理由和转移后的文本。你需要评估重写过程的理由，而不考虑转移后的文本质量。
    |'
- en: 'Table 5: Human evaluation protocol and instruction. We adapt the evaluation
    criteria from Wu et al. ([2023](#bib.bib52)) and Wang et al. ([2023b](#bib.bib49)).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：人工评估协议和指示。我们从Wu等人（[2023](#bib.bib52)）和Wang等人（[2023b](#bib.bib49)）处调整了评估标准。
