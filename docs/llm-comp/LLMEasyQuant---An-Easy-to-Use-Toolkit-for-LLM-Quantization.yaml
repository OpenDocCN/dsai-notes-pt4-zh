- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:47:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:53
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: LLMEasyQuant - An Easy to Use Toolkit for LLM Quantization
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMEasyQuant - 一个易于使用的 LLM 量化工具包
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.19657](https://ar5iv.labs.arxiv.org/html/2406.19657)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.19657](https://ar5iv.labs.arxiv.org/html/2406.19657)
- en: Dong Liu¹ Meng Jiang² Kaiser Pister¹ dliu328@wisc.edu mjiang2@nd.edu kaiser@cs.wisc.edu
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Dong Liu¹ Meng Jiang² Kaiser Pister¹ dliu328@wisc.edu mjiang2@nd.edu kaiser@cs.wisc.edu
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Currently, there are many quantization methods appeared for LLM quantization,
    yet few are user-friendly and easy to be deployed locally. Packages like TensorRT[[NVI24](#bib.bibx4)]
    and Quanto[[Fac24](#bib.bibx1)] have many underlying structures and self-invoking
    internal functions, which are not conducive to developers’ personalized development
    and learning for deployment. Therefore, we develop LLMEasyQuant, it is a package
    aiming to for easy quantization deployment which is user-friendly and suitable
    for beginners’ learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，针对 LLM 量化出现了许多量化方法，但少数方法对用户友好且易于本地部署。像 TensorRT[[NVI24](#bib.bibx4)] 和 Quanto[[Fac24](#bib.bibx1)]
    这样的包具有许多底层结构和自调用的内部函数，这不利于开发人员个性化开发和学习部署。因此，我们开发了 LLMEasyQuant，它是一个旨在实现易于量化部署的包，用户友好，适合初学者学习。
- en: The code of LLMEasyQuant can be found at [code link](https://github.com/NoakLiu/LLMEasyQuant)¹¹1[https://github.com/NoakLiu/LLMEasyQuant](https://github.com/NoakLiu/LLMEasyQuant).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLMEasyQuant 的代码可以在 [代码链接](https://github.com/NoakLiu/LLMEasyQuant)¹¹1[https://github.com/NoakLiu/LLMEasyQuant](https://github.com/NoakLiu/LLMEasyQuant)
    找到。
- en: ¹University of Wisconsin-Madison,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ¹威斯康星大学麦迪逊分校，
- en: ²University of Notre Dame
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ²圣母大学
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Quantization is the process of mapping a large set of input values to a smaller
    set of output values, often integers. It is a key technique in digital signal
    processing where continuous signals are mapped to discrete digital values, and
    it reduces the data’s precision to make storage and computation more efficient
    while attempting to retain essential information.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是将大量输入值映射到较小的输出值集（通常是整数）的过程。这是数字信号处理中的关键技术，其中连续信号被映射到离散数字值，并且它通过减少数据的精度来提高存储和计算的效率，同时尝试保留重要信息。
- en: With the development of Large Language Models (LLMs), the models have grown
    extremely large, so the memory usage and inference speed are greatly limited by
    the size of the model. Consequently, as one of the most popular technique for
    model compression, quantization has many variants now used for LLM compression
    and inference acceleration. The goal of quantization in LLMs is to reduce their
    size while minimizing its influence on inference speed.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大语言模型（LLMs）的发展，这些模型变得非常庞大，因此内存使用和推理速度都受到模型大小的极大限制。因此，作为模型压缩的最流行技术之一，量化现在有许多变体用于
    LLM 压缩和推理加速。LLM 量化的最终目标是减少模型大小，同时尽量减少对推理速度的影响。
- en: 'The quantization process can be described by:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 量化过程可以描述为：
- en: '|  | $Q(x)=\text{clamp}\left(\left\lfloor\frac{x-\text{min}(X)}{\Delta}+0.5\right\rfloor+z,-128,127\right)$
    |  |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(x)=\text{clamp}\left(\left\lfloor\frac{x-\text{min}(X)}{\Delta}+0.5\right\rfloor+z,-128,127\right)$
    |  |'
- en: 'where:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $x$,
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $x$，
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\Delta$,
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\Delta$，
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $z$ or calculated to shift the scale,
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $z$ 或计算以移动尺度，
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: clamp function ensures values are kept within the $[-128,127]$ range.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: clamp 函数确保值保持在 $[-128,127]$ 范围内。
- en: 2 Methodology
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: 'LLMEasyQuant is a toolkit designed to simplify the process of deploying quantization
    on large language models (LLMs). Its functions are included as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LLMEasyQuant 是一个旨在简化在大语言模型（LLMs）上部署量化过程的工具包。其功能包括：
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'absmax: $\text{scale}=\frac{127}{\max(|X|)}$'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: absmax：$\text{scale}=\frac{127}{\max(|X|)}$
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'zeropoint: Shifting the tensor values based on a computed zero-point'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: zeropoint：基于计算的零点调整张量值
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'symquant: Symmetric quantization, a quantization method for scaling based on
    the absolute maximum value.[[FFBL18](#bib.bibx2)]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: symquant：对称量化，一种基于绝对最大值的缩放量化方法。[[FFBL18](#bib.bibx2)]
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'smoothquant: A quantization method to achieve efficient post-training quantizatio
    for LLMs.[[XLS^+24](#bib.bibx6)]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: smoothquant：一种实现高效后训练量化的量化方法。[[XLS^+24](#bib.bibx6)]
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'zeroquant: Adjust the numeric range of input data so that zero values in the
    original data can be represented exactly in the quantized format. [[YAZ^+22](#bib.bibx7)]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: zeroquant：调整输入数据的数值范围，以便原始数据中的零值可以在量化格式中准确表示。[[YAZ^+22](#bib.bibx7)]
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'simquant: A quantization technique by KV Cache Quant. [[HKM^+24](#bib.bibx3)]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: simquant：由 KV Cache Quant 提供的量化技术。[[HKM^+24](#bib.bibx3)]
- en: 3 Implementation
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实现
- en: '3.1 ZeroQuant: Zero-point Quantization'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 零点量化：ZeroQuant
- en: 3.1.1 Definition and Concept
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 定义和概念
- en: Zero-point quantization, or ZeroQuant [[YAZ^+22](#bib.bibx7)], focuses on adjusting
    the numeric range of data so that zero values are represented exactly by zero
    in the quantized format. This technique is particularly useful in optimizing quantization
    schemes where the preservation of zero values is crucial, such as in sparse datasets
    used in machine learning and signal processing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 零点量化或 ZeroQuant [[YAZ^+22](#bib.bibx7)] 侧重于调整数据的数值范围，使零值在量化格式中准确表示为零。这种技术在优化量化方案时尤其有用，尤其是在零值保留至关重要的稀疏数据集，如机器学习和信号处理中的数据集。
- en: 3.1.2 Mathematical Formulation
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 数学公式
- en: 'The quantization process using ZeroQuant can be described by the following
    formula:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ZeroQuant 的量化过程可以用以下公式描述：
- en: '|  | $Q(x)=\text{clamp}\left(\left\lfloor\frac{x-\text{min}(X)}{\Delta}+z\right\rfloor,-128,127\right)$
    |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(x)=\text{clamp}\left(\left\lfloor\frac{x-\text{min}(X)}{\Delta}+z\right\rfloor,-128,127\right)$
    |  |'
- en: 'where:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $x$ is the value to be quantized,
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $x$ 是待量化的值，
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\text{min}(X)$ is the minimum value in the dataset,
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\text{min}(X)$ 是数据集中的最小值，
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\Delta$,
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\Delta$，
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $z$ maps to 0,
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $z$ 映射到 0，
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The output is clamped to the range $[-128,127]$, typical of 8-bit signed integers.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出被限制在 $[-128,127]$ 范围内，这个范围是 8 位带符号整数的典型范围。
- en: 3.2 Symmetric 8-bit Quantization
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 对称 8 位量化
- en: 3.2.1 Concept
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 概念
- en: Symmetric 8-bit quantization [[FFBL18](#bib.bibx2)] involves mapping both positive
    and negative values of a dataset uniformly around zero, optimizing the quantization
    process for symmetric data distributions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对称 8 位量化 [[FFBL18](#bib.bibx2)] 涉及将数据集的正负值均匀地映射到零周围，优化对称数据分布的量化过程。
- en: 3.2.2 Mathematical Formulation
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 数学公式
- en: 'The symmetric quantization can be described with:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对称量化可以用以下公式描述：
- en: '|  | $Q(x)=\text{clamp}\left(\text{round}\left(\frac{x}{s}\right),-128,127\right),\quad
    s=\frac{\text{max}(&#124;X&#124;)}{127.5}$ |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(x)=\text{clamp}\left(\text{round}\left(\frac{x}{s}\right),-128,127\right),\quad
    s=\frac{\text{max}(|X|)}{127.5}$ |  |'
- en: where $s$ is the scale factor calculated based on the maximum absolute value
    of the data, ensuring all quantized values fall within the 8-bit integer range
    of -128 to 127.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s$ 是根据数据的最大绝对值计算的缩放因子，确保所有量化值都在 -128 到 127 的 8 位整数范围内。
- en: 3.3 Layer-by-Layer Quantization
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 逐层量化
- en: 3.3.1 Concept
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 概念
- en: Layer-by-layer quantization [[TOW^+23](#bib.bibx5)] is a technique applied in
    deep learning where each layer of a neural network is quantized independently
    to maintain accuracy while reducing the overall model size and computational demand.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 逐层量化 [[TOW^+23](#bib.bibx5)] 是一种应用于深度学习的技术，其中神经网络的每一层都独立量化，以保持准确性，同时减少整体模型大小和计算需求。
- en: 3.3.2 Quantization Procedure
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 量化过程
- en: 'The process involves:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 过程包括：
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Calculating the quantization parameters for each layer separately.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分别计算每层的量化参数。
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adjusting each layer’s weights and biases according to the quantization rules
    defined for symmetric or asymmetric quantization methods.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据对称或非对称量化方法定义的量化规则调整每层的权重和偏置。
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Optionally recalibrating layer parameters during or after the quantization to
    optimize performance.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在量化期间或之后选择性地重新校准层参数以优化性能。
- en: 3.3.3 Implementation Example
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 实现示例
- en: 'Here is an example of applying layer-by-layer quantization:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个逐层量化的应用示例：
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 3.4 Symmetric 8-bit and Layer-by-Layer Quantization
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 对称 8 位和逐层量化
- en: 3.4.1 Layer-by-Layer ZeroQuant Function
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 分层零点量化函数
- en: 'Applies quantization per layer in a model:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型中每一层应用量化：
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Encode input and compute unquantized outputs.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码输入并计算未量化的输出。
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'For each layer $i$:'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每层 $i$：
- en: –
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Freeze all but layer $i$.
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除第 $i$ 层外，冻结所有层。
- en: –
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Update $i$-th layer’s parameters by quantization.
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过量化更新第 $i$ 层的参数。
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Compute loss and update using:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算损失并使用以下公式更新：
- en: '|  | $\text{loss}=\text{MSE}(\text{teacher\_outputs}[i],\text{quantized\_outputs}[i])$
    |  |'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\text{loss}=\text{MSE}(\text{teacher\_outputs}[i],\text{quantized\_outputs}[i])$
    |  |'
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Optimize with gradient descent.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用梯度下降进行优化。
- en: '3.5 SimQuant: A Novel Approach by KV Cache Quant'
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '3.5 SimQuant: 基于 KV Cache 量化的创新方法'
- en: 3.5.1 Definition and Concept
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1 定义和概念
- en: SimQuant [[HKM^+24](#bib.bibx3)], developed by KV Cache Quant, represents a
    groundbreaking advance in the field of data quantization. This technique is designed
    to optimize the efficiency of data representation while maintaining high accuracy,
    particularly in environments where computational resources and storage are limited.
    Unlike traditional quantization methods that apply fixed parameters across various
    datasets, SimQuant introduces a dynamic adjustment mechanism that tailors the
    quantization process to the specific statistical properties of the dataset in
    use.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: SimQuant [[HKM^+24](#bib.bibx3)]，由 KV Cache Quant 开发，代表了数据量化领域的突破性进展。这项技术旨在优化数据表示的效率，同时保持高准确性，特别是在计算资源和存储有限的环境中。与传统的固定参数量化方法不同，SimQuant
    引入了动态调整机制，将量化过程调整到数据集的特定统计属性。
- en: 3.5.2 Mathematical Formulation
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2 数学公式
- en: 'The core of the SimQuant technique is based on the formula:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: SimQuant 技术的核心基于以下公式：
- en: '|  | $Q(x)=\text{round}\left(\frac{x-\min(X)}{\Delta}\right)+Z$ |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(x)=\text{round}\left(\frac{x-\min(X)}{\Delta}\right)+Z$ |  |'
- en: 'where:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $x$ is the data point being quantized,
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $x$ 是正在量化的数据点，
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\min(X)$ is the minimum value in the dataset,
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\min(X)$ 是数据集中的最小值，
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\Delta$ represents the quantization interval, which is adaptively calculated,
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\Delta$ 代表量化间隔，采用自适应计算方式，
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $Z$ is the zero-point adjustment to center the quantization range.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $Z$ 是用于将量化范围居中的零点调整。
- en: 3.5.3 Algorithmic Steps
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3 算法步骤
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Analysis: Initially, SimQuant analyzes the statistical distribution of
    the dataset to determine optimal quantization parameters.'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据分析：最初，SimQuant 分析数据集的统计分布，以确定最佳的量化参数。
- en: •
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Parameter Adjustment: It then adjusts $\Delta$ dynamically during the quantization
    process to minimize information loss.'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数调整：然后在量化过程中动态调整 $\Delta$ 以最小化信息损失。
- en: •
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Quantization Application: Finally, the data is quantized using the calculated
    parameters, ensuring that the most critical information is retained with minimal
    resource usage.'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化应用：最后，使用计算出的参数对数据进行量化，确保在资源使用最少的情况下保留最关键的信息。
- en: 3.5.4 Quantization Process
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.4 量化过程
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Calculate range values:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算范围值：
- en: '|  | $\text{vals}_{\text{min}}=\min(X_{\text{channel}}),\quad\text{vals}_{\text{max}}=\max(X_{\text{channel}})$
    |  |'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\text{vals}_{\text{min}}=\min(X_{\text{channel}}),\quad\text{vals}_{\text{max}}=\max(X_{\text{channel}})$
    |  |'
- en: •
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Compute scale $s$:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算尺度 $s$：
- en: '|  | $s=\frac{2^{\text{bits}}-1}{\text{vals}_{\text{max}}-\text{vals}_{\text{min}}},\quad
    z=-\text{vals}_{\text{min}}\cdot s$ |  |'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $s=\frac{2^{\text{bits}}-1}{\text{vals}_{\text{max}}-\text{vals}_{\text{min}}},\quad
    z=-\text{vals}_{\text{min}}\cdot s$ |  |'
- en: •
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Apply quantization:'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应用量化：
- en: '|  | $X_{\text{quant}}=\text{clamp}(\lfloor X\cdot s+z+0.5\rfloor,0,2^{\text{bits}}-1)$
    |  |'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $X_{\text{quant}}=\text{clamp}(\lfloor X\cdot s+z+0.5\rfloor,0,2^{\text{bits}}-1)$
    |  |'
- en: 3.5.5 Dequantization Process
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.5 去量化过程
- en: '|  | $X_{\text{dequant}}=\frac{X_{\text{quant}}-z}{s}$ |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $X_{\text{dequant}}=\frac{X_{\text{quant}}-z}{s}$ |  |'
- en: 3.5.6 Pseudocode for SimQuant
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.6 SimQuant 的伪代码
- en: Algorithm 1 KVQuant Quantization
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 KVQuant 量化
- en: 1:function QuantizeKVCache(Key, Value, scaling_factor, zero_point)2:     for each
    layer $l$7:end function
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 1:函数 QuantizeKVCache(Key, Value, scaling_factor, zero_point)2:     对每一层 $l$
    7:结束函数
- en: '3.6 SmoothQuant: Smoothing Quantization Process'
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 SmoothQuant：平滑量化过程
- en: 3.6.1 Definition and Concept
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.1 定义和概念
- en: SmoothQuant [[XLS^+24](#bib.bibx6)] is a post-training quantization (PTQ) method
    designed to facilitate accurate and efficient quantization of large language models
    (LLMs). It aims to achieve 8-bit quantization for both weights and activations
    without retraining, ensuring minimal loss in model accuracy. SmoothQuant addresses
    the challenge of activation quantization by redistributing the quantization difficulty
    from activations to weights, which are inherently easier to quantize.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: SmoothQuant [[XLS^+24](#bib.bibx6)] 是一种后训练量化 (PTQ) 方法，旨在实现大规模语言模型 (LLMs) 的准确高效量化。它旨在在不重新训练的情况下实现权重和激活的
    8 位量化，确保模型准确性损失最小。SmoothQuant 通过将量化难度从激活转移到权重，解决了激活量化的挑战，因为权重固有上更易于量化。
- en: The key concept behind SmoothQuant is the smoothing of activation outliers,
    which typically hinder effective quantization. By migrating the quantization difficulty
    from activations to weights, SmoothQuant ensures that both can be quantized to
    8-bit integers (INT8) effectively. This migration is achieved through a mathematically
    equivalent transformation that adjusts the scale of activations and weights.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: SmoothQuant 的关键概念是平滑激活异常值，这些异常值通常会阻碍有效的量化。通过将量化难度从激活迁移到权重，SmoothQuant 确保这两者都可以有效地量化为
    8 位整数（INT8）。这一迁移是通过数学上等效的变换来实现的，该变换调整激活和权重的尺度。
- en: 3.6.2 Mathematical Formulation
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.2 数学公式
- en: 'The quantization and smoothing process of the ‘SmoothQuantMatrix‘ can be mathematically
    described as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ‘SmoothQuantMatrix‘ 的量化和光滑过程可以数学上描述如下：
- en: '|  | $\text{smoothed\_X}=X\cdot s,\quad\text{dequantized\_X}=\frac{\text{smoothed\_X}}{s}$
    |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{smoothed\_X}=X\cdot s,\quad\text{dequantized\_X}=\frac{\text{smoothed\_X}}{s}$
    |  |'
- en: 'where the scale factor $s$ is calculated using the activity scales act_scales
    and the weight scales of the features:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中尺度因子 $s$ 是使用活动尺度 act_scales 和特征的权重尺度计算的：
- en: '|  | $s=\left(\frac{\text{act\_scales}^{\alpha}}{\text{weight\_scales}^{1-\alpha}}\right)$
    |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $s=\left(\frac{\text{act\_scales}^{\alpha}}{\text{weight\_scales}^{1-\alpha}}\right)$
    |  |'
- en: Here, $\alpha$ is a parameter that determines the balance between the activity
    and the weight scales, affecting the smoothness of the quantization.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\alpha$ 是一个参数，用于确定活动尺度与权重尺度之间的平衡，影响量化的平滑度。
- en: 3.6.3 SmoothQuant Methodology
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.3 SmoothQuant 方法论
- en: SmoothQuant smooths activation outliers by migrating quantization difficulty
    from activations to weights. The following pseudocode outlines the main steps
    of the SmoothQuant process.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: SmoothQuant 通过将量化难度从激活迁移到权重来平滑激活异常值。以下伪代码概述了 SmoothQuant 过程的主要步骤。
- en: Algorithm 2 SmoothQuant Process
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 SmoothQuant 过程
- en: '1:Input: Activation tensor $X$'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 1:输入：激活张量 $X$
- en: 3.6.4 Implementation Details
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.4 实现细节
- en: The ‘SmoothQuantMatrix‘ class computes the smoothing scales based on the provided
    activity scales and the inherent data characteristics, dynamically adjusting each
    feature’s scale. This customization allows the quantization process to be more
    adaptive and sensitive to the underlying data distribution.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ‘SmoothQuantMatrix‘ 类根据提供的活动尺度和固有数据特征计算平滑尺度，动态调整每个特征的尺度。这种定制化使得量化过程能够更具适应性，并对基础数据分布更为敏感。
- en: 4 result
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果
- en: '![Refer to caption](img/338681f2ebfc73eb30a4f4794f8e125b.png)![Refer to caption](img/1ea71519c9b53ccb88f848c093afd2cf.png)![Refer
    to caption](img/1ef9481844d88d567594f174640c8f2a.png)![Refer to caption](img/28e4e803c5fadaa145ef1bd728d6fff7.png)![Refer
    to caption](img/f72f183b613beb9d079eeab45e89d0f4.png)![Refer to caption](img/da7c03e748f25f82bfb3723e128a3df1.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/338681f2ebfc73eb30a4f4794f8e125b.png)![参见说明](img/1ea71519c9b53ccb88f848c093afd2cf.png)![参见说明](img/1ef9481844d88d567594f174640c8f2a.png)![参见说明](img/28e4e803c5fadaa145ef1bd728d6fff7.png)![参见说明](img/f72f183b613beb9d079eeab45e89d0f4.png)![参见说明](img/da7c03e748f25f82bfb3723e128a3df1.png)'
- en: 'Figure 1: Quantized Weights Distribution'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：量化权重分布
- en: '![Refer to caption](img/84666aacf31790d82254b97e73ed40ec.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/84666aacf31790d82254b97e73ed40ec.png)'
- en: 'Figure 2: Performance Comparison after Quantization on GPT'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：GPT 量化后的性能比较
- en: '| Models | Perplexity (ppl) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 困惑度 (ppl) |'
- en: '| --- | --- |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GPT-2 | 4.01 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 | 4.01 |'
- en: '| GPT-2 INT8 | 6.83 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 INT8 | 6.83 |'
- en: '| GPT-2 AbsMax Quantize | 9.32 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 AbsMax 量化 | 9.32 |'
- en: '| GPT-2 ZeroPoint Quantize | 8.93 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 零点量化 | 8.93 |'
- en: '| GPT-2 Smooth Quant Apply | 6.31 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 平滑量化应用 | 6.31 |'
- en: '| GPT-2 Sim Quantize | 7.16 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 对称量化 8bit | 7.01 |'
- en: '| GPT-2 Sym Quantize 8bit | 7.01 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 对称量化 8bit | 7.01 |'
- en: '| GPT-2 Sym Quantize 8bit ZeroQuant Func | 7.37 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 对称量化 8bit ZeroQuant 功能 | 7.37 |'
- en: 'Table 1: Perplexity Analysis of Quantization Models'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：量化模型的困惑度分析
- en: 5 Discussions
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: 5.1 Simplification of Quantization Processes
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 量化过程的简化
- en: LLMEasyQuant provides a user-friendly interface that simplifies the application
    of quantization techniques, making it accessible to both novices and experienced
    users without requiring deep technical knowledge of the underlying algorithms.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: LLMEasyQuant 提供了一个用户友好的界面，简化了量化技术的应用，使得新手和经验丰富的用户都能轻松使用，而无需深入了解底层算法的技术细节。
- en: 5.2 Customization and Flexibility
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 定制化与灵活性
- en: Despite its simplicity, the package offers extensive customization options that
    allow users to tailor the quantization process to their specific needs, balancing
    efficiency and performance according to the model’s deployment context.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其简单，但该软件包提供了广泛的定制选项，使用户能够根据模型的部署环境平衡效率和性能，调整量化过程以满足具体需求。
- en: 5.3 Efficiency in Deployment
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 部署效率
- en: Optimized for performance, LLMEasyQuant helps reduce the computational load
    and memory usage of models, facilitating their deployment on devices with limited
    resources such as mobile phones and embedded systems.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: LLMEasyQuant在性能上经过优化，有助于减少模型的计算负担和内存使用，从而便于在资源有限的设备如手机和嵌入式系统上进行部署。
- en: References
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Fac24] Hugging Face. Optimum-quanto. [https://github.com/huggingface/optimum-quanto](https://github.com/huggingface/optimum-quanto),
    2024.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fac24] Hugging Face. Optimum-quanto. [https://github.com/huggingface/optimum-quanto](https://github.com/huggingface/optimum-quanto),
    2024.'
- en: '[FFBL18] Julian Faraone, Nicholas Fraser, Michaela Blott, and Philip H. W.
    Leong. Syq: Learning symmetric quantization for efficient deep neural networks,
    2018.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FFBL18] Julian Faraone, Nicholas Fraser, Michaela Blott, 和 Philip H. W. Leong.
    Syq: 学习对称量化以提高深度神经网络的效率, 2018.'
- en: '[HKM^+24] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney,
    Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million
    context length llm inference with kv cache quantization, 2024.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HKM^+24] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney,
    Yakun Sophia Shao, Kurt Keutzer, 和 Amir Gholami. Kvquant: 通过kv缓存量化实现1000万上下文长度的llm推理,
    2024.'
- en: '[NVI24] NVIDIA. Tensorrt-llm. [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM),
    2024.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NVI24] NVIDIA. Tensorrt-llm. [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM),
    2024.'
- en: '[TOW^+23] Chen Tang, Kai Ouyang, Zhi Wang, Yifei Zhu, Yaowei Wang, Wen Ji,
    and Wenwu Zhu. Mixed-precision neural network quantization via learned layer-wise
    importance, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TOW^+23] Chen Tang, Kai Ouyang, Zhi Wang, Yifei Zhu, Yaowei Wang, Wen Ji,
    和 Wenwu Zhu. 通过学习的层级重要性进行混合精度神经网络量化, 2023.'
- en: '[XLS^+24] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and
    Song Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models, 2024.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XLS^+24] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, 和
    Song Han. Smoothquant: 对大规模语言模型的准确且高效的后训练量化, 2024.'
- en: '[YAZ^+22] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers, 2022.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[YAZ^+22] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, 和 Yuxiong He. Zeroquant: 对大规模变换器的高效且经济的后训练量化, 2022.'
