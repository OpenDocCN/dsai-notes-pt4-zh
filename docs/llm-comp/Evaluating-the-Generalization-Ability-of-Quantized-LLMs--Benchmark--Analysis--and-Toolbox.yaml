- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:48:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:03
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis,
    and Toolbox'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估量化LLM的泛化能力：基准测试、分析和工具箱
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.12928](https://ar5iv.labs.arxiv.org/html/2406.12928)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.12928](https://ar5iv.labs.arxiv.org/html/2406.12928)
- en: Yijun Liu¹  Yuan Meng¹  Fang Wu²  Shenhao Peng^(2,5)  Hang Yao²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 刘益俊¹  孟源¹  吴芳²  彭申浩^(2,5)  姚杭²
- en: Chaoyu Guan²  Chen Tang¹  Xinzhu Ma^(3,4)  Zhi Wang¹  Wenwu Zhu¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 关超宇²  唐晨¹  马新竹^(3,4)  王志¹  朱文武¹
- en: ¹Tsinghua University ²Tsingmao Intelligence  ³CUHK  ⁴Shanghai AI Lab  ⁵HUST
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹清华大学 ²青茂智能  ³香港中文大学  ⁴上海人工智能实验室  ⁵华中科技大学
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) have exhibited exciting progress in multiple scenarios,
    while the huge computational demands hinder their deployments in lots of real-world
    applications. As an effective means to reduce memory footprint and inference cost,
    quantization also faces challenges in performance degradation at low bit-widths.
    Understanding the impact of quantization on LLM capabilities, especially the *generalization
    ability*, is crucial. However, the community’s main focus remains on the algorithms
    and models of quantization, with insufficient attention given to whether the quantized
    models can retain the strong generalization abilities of LLMs. In this work, we
    fill this gap by providing a comprehensive benchmark suite for this research topic,
    including an evaluation system, detailed analyses, and a general toolbox. Specifically,
    based on the dominant pipeline in LLM quantization, we primarily explore the impact
    of calibration data distribution on the generalization of quantized LLMs and conduct
    the benchmark using more than 40 datasets within two main scenarios. Based on
    this benchmark, we conduct extensive experiments with two well-known LLMs (English
    and Chinese) and four quantization algorithms to investigate this topic in-depth,
    yielding several counter-intuitive and valuable findings, *e.g.*, models quantized
    using a calibration set with the same distribution as the test data are not necessarily
    optimal. Besides, to facilitate future research, we also release a modular-designed
    toolbox, which decouples the overall pipeline into several separate components,
    e.g., base LLM module, dataset module, quantizer module, etc. and allows subsequent
    researchers to easily assemble their methods through a simple configuration. Our
    benchmark suite is publicly available at [https://github.com/TsingmaoAI/MI-optimize](https://github.com/TsingmaoAI/MI-optimize).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在多个场景中展现了令人兴奋的进展，但巨大的计算需求阻碍了它们在许多实际应用中的部署。作为一种有效的减少内存占用和推理成本的手段，量化在低位宽下也面临性能下降的挑战。理解量化对LLM能力的影响，特别是*泛化能力*，至关重要。然而，社区的主要关注点仍然集中在量化算法和模型上，对量化模型是否能保持LLM的强泛化能力关注不足。在这项工作中，我们通过提供一个全面的基准测试套件填补了这一空白，包括评估系统、详细分析和通用工具箱。具体而言，基于LLM量化的主流流程，我们主要探讨了校准数据分布对量化LLM泛化的影响，并在两个主要场景下使用40多个数据集进行基准测试。基于此基准测试，我们进行了大量实验，涉及两种知名LLM（英文和中文）和四种量化算法，深入研究了这一主题，获得了若干反直觉且有价值的发现，*例如*，使用与测试数据分布相同的校准集进行量化的模型不一定是最优的。此外，为了促进未来的研究，我们还发布了一个模块化设计的工具箱，将整体流程解耦为几个独立的组件，如基础LLM模块、数据集模块、量化器模块等，并允许后续研究人员通过简单的配置轻松组装他们的方法。我们的基准测试套件在[https://github.com/TsingmaoAI/MI-optimize](https://github.com/TsingmaoAI/MI-optimize)上公开提供。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'In recent years, large language models (LLMs) have made groundbreaking advancements,
    demonstrating remarkable results and outstanding *generalization ability* across
    various tasks ([56](#bib.bib56); [72](#bib.bib72); [1](#bib.bib1); [54](#bib.bib54)).
    For example, given a few prompt examples or questions, LLMs can produce insightful
    answers within the unseen domain ([47](#bib.bib47); [5](#bib.bib5)). However,
    while LLMs exhibit remarkable capabilities, their substantial size makes real-world
    implementation cost-prohibitive. To address this challenge, model quantization
    has emerged as a prevailing technique for reducing the memory footprint of LLMs ([14](#bib.bib14);
    [31](#bib.bib31); [10](#bib.bib10); [6](#bib.bib6); [66](#bib.bib66); [62](#bib.bib62);
    [51](#bib.bib51)). Specifically, quantization reduces the model size by replacing
    high-precision floating-point numbers with lower-precision integers (*e.g.*, from
    FP16 to INT4) ([40](#bib.bib40); [17](#bib.bib17); [75](#bib.bib75)). Currently,
    to avoid the substantial retraining costs of LLMs, the quantization methods for
    large models primarily employ post-training quantization (PTQ) ([14](#bib.bib14);
    [31](#bib.bib31); [10](#bib.bib10); [6](#bib.bib6)), which leverages calibration
    data to optimize the error caused by the quantization. Given the prevalent view
    that LLM capabilities stem from their extensive parameter count ([24](#bib.bib24)),
    a critical question emerges:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）取得了突破性的进展，在各种任务中展现了显著的成果和卓越的*泛化能力*（[56](#bib.bib56); [72](#bib.bib72);
    [1](#bib.bib1); [54](#bib.bib54)）。例如，给定一些提示示例或问题，LLMs可以在未见领域内提供有洞察力的回答（[47](#bib.bib47);
    [5](#bib.bib5)）。然而，尽管LLMs展现了显著的能力，其巨大的体积使得实际应用成本高昂。为了解决这一挑战，模型量化已成为降低LLMs内存占用的主要技术（[14](#bib.bib14);
    [31](#bib.bib31); [10](#bib.bib10); [6](#bib.bib6); [66](#bib.bib66); [62](#bib.bib62);
    [51](#bib.bib51)）。具体来说，量化通过将高精度浮点数替换为低精度整数（*例如*，从FP16到INT4）来减少模型的大小（[40](#bib.bib40);
    [17](#bib.bib17); [75](#bib.bib75)）。目前，为了避免LLMs的高昂再训练成本，大型模型的量化方法主要采用训练后量化（PTQ）（[14](#bib.bib14);
    [31](#bib.bib31); [10](#bib.bib10); [6](#bib.bib6)），该方法利用标定数据来优化由量化引起的误差。鉴于广泛认为LLMs的能力源于其庞大的参数数量（[24](#bib.bib24)），一个关键问题浮现：
- en: '*Can the quantized LLMs still retain their strong generalization ability?*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*量化后的LLMs是否仍然保持强大的泛化能力？*'
- en: While some works have acknowledged this issue ([33](#bib.bib33); [21](#bib.bib21);
    [30](#bib.bib30); [19](#bib.bib19); [23](#bib.bib23)), there is still a lack of
    systematic evaluation regarding the generalization performance of LLMs after quantization,
    particularly considering the impact of *calibration data* introduced during the
    quantization process.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些研究已经认识到这个问题（[33](#bib.bib33); [21](#bib.bib21); [30](#bib.bib30); [19](#bib.bib19);
    [23](#bib.bib23)），但对于量化后LLMs的泛化性能，尤其是量化过程中引入的*标定数据*的影响，仍缺乏系统的评估。
- en: '![Refer to caption](img/a36767890caa39c8bc9c1d1e8077b758.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a36767890caa39c8bc9c1d1e8077b758.png)'
- en: 'Figure 1: We show the pipeline of model quantization and the data required
    at each stage (Top).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们展示了模型量化的流程以及每个阶段所需的数据（顶部）。
- en: The calibration data used in previous works generally share the same distribution
    with pre-training data (S1), and the relation between calibration data and test
    data should be further discussed (S2).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的工作中使用的标定数据通常与预训练数据（S1）具有相同的分布，而标定数据与测试数据之间的关系需要进一步讨论（S2）。
- en: 'As shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Evaluating the Generalization
    Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox"), the process of
    model quantization encompasses three distinct stages: pre-training, quantization,
    and inference, utilizing pre-training data, calibration data, and test data, respectively.
    Existing quantization researches typically use a standard calibration set, which
    is usually a subset of the pre-training data (Scenario 1, S1), and evaluate on
    several fixed datasets ([44](#bib.bib44); [7](#bib.bib7); [53](#bib.bib53); [50](#bib.bib50);
    [71](#bib.bib71)). However, because using task-specific data for model calibration
    is a more reasonable choice in practical applications, the relationship between
    the distribution of *calibration data* and *test data* and its impact on the generalization
    ability of quantized models is a more worthy research topic that has not been
    deeply explored (Scenario 2, S2). In this work, to answer the abovementioned question
    and bridge the gap between academic research and practical implementation, we
    provide a platform to evaluate the generalization ability of quantized LLMs, covering
    *benchmarks*, *analyses*, and a modular-designed *toolbox*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Evaluating the Generalization Ability
    of Quantized LLMs: Benchmark, Analysis, and Toolbox")所示，模型量化过程包括三个不同阶段：预训练、量化和推理，分别利用预训练数据、校准数据和测试数据。现有的量化研究通常使用标准的校准集，该校准集通常是预训练数据的一个子集（情境
    1，S1），并在几个固定数据集上进行评估 ([44](#bib.bib44); [7](#bib.bib7); [53](#bib.bib53); [50](#bib.bib50);
    [71](#bib.bib71))。然而，由于在实际应用中，使用任务特定的数据进行模型校准是一个更合理的选择，因此*校准数据*和*测试数据*的分布之间的关系及其对量化模型泛化能力的影响是一个值得深入研究的课题（情境
    2，S2）。在本研究中，为了回答上述问题并弥合学术研究与实际应用之间的差距，我们提供了一个平台来评估量化 LLM 的泛化能力，涵盖了*基准测试*、*分析*和一个模块化设计的*工具箱*。'
- en: 'Benchmark evaluation. As shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis,
    and Toolbox"), we build the benchmark based on the two scenarios:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '基准测试评估。如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Evaluating the Generalization
    Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox")所示，我们根据这两种情境构建了基准测试：'
- en: '$\bullet$ In S1 (Section [2](#S2 "2 S1: Generalization Assessment of Quantized
    LLMs with Standard Setting ‣ Evaluating the Generalization Ability of Quantized
    LLMs: Benchmark, Analysis, and Toolbox")), beyond the existing research, we collect
    the most comprehensive evaluation of test datasets to date, covering 9 categories
    and 26 datasets. We use C4 ([48](#bib.bib48)) as the calibration dataset, and
    quantize LLaMA-7B ([54](#bib.bib54)) model by two methods ([14](#bib.bib14); [10](#bib.bib10))
    across three weight bit-widths.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 在 S1（第[2](#S2 "2 S1: Generalization Assessment of Quantized LLMs
    with Standard Setting ‣ Evaluating the Generalization Ability of Quantized LLMs:
    Benchmark, Analysis, and Toolbox")节），除了现有的研究外，我们收集了迄今为止最全面的测试数据集评估，涵盖 9 个类别和 26
    个数据集。我们使用 C4 ([48](#bib.bib48)) 作为校准数据集，并通过两种方法 ([14](#bib.bib14); [10](#bib.bib10))
    在三个权重量化位宽下对 LLaMA-7B ([54](#bib.bib54)) 模型进行量化。'
- en: '$\bullet$ In S2 (Section [3](#S3 "3 S2: Generalization Assessment of Quantized
    LLMs with Domain Shifts ‣ Evaluating the Generalization Ability of Quantized LLMs:
    Benchmark, Analysis, and Toolbox")), our benchmark covers 19 datasets with two
    types of distribution shifts between calibration data and test data: *cross-dataset*
    and *cross-subject*. We consider both English and Chinese domains for the cross-dataset
    setting. Besides, our benchmark also includes a more challenging cross-subject
    setting, e.g. from humanities to social science. To our knowledge, no prior work
    has investigated the generalization of quantized models in a cross-subject setting.
    For all settings, our benchmark builds the Independent and Identically Distribution
    (I.I.D) and Out-of-Distribution (OOD) evaluations by adjusting the calibration
    data distributions. In our experiments, we quantize LLaMA-7B ([54](#bib.bib54))
    and Baichuan2-7B-Base ([11](#bib.bib11)) for English and Chinese models with four
    methods ([14](#bib.bib14); [10](#bib.bib10); [31](#bib.bib31); [62](#bib.bib62))
    across three weight bit-widths.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 在 S2 (第[3](#S3 "3 S2: Generalization Assessment of Quantized LLMs
    with Domain Shifts ‣ Evaluating the Generalization Ability of Quantized LLMs:
    Benchmark, Analysis, and Toolbox")节)中，我们的基准涵盖了 19 个数据集，具有两种类型的分布转移：*跨数据集* 和 *跨主体*。我们考虑了跨数据集设置中的英语和中文领域。此外，我们的基准还包括一个更具挑战性的跨主体设置，例如从人文学科到社会科学。根据我们的了解，之前的工作尚未研究量化模型在跨主体设置中的泛化能力。对于所有设置，我们的基准通过调整标定数据分布来建立独立同分布（I.I.D）和分布外（OOD）评估。在我们的实验中，我们对
    LLaMA-7B ([54](#bib.bib54)) 和 Baichuan2-7B-Base ([11](#bib.bib11)) 进行了量化，涵盖英语和中文模型，使用了四种方法 ([14](#bib.bib14);
    [10](#bib.bib10); [31](#bib.bib31); [62](#bib.bib62))，并跨越三种权重位宽。'
- en: 'The generalization performance of quantized models is assessed using zero-shot
    and few-shot evaluation for all experiments, and we summarize the key features
    of our benchmark in Tab. [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Evaluating the
    Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox").'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '量化模型的泛化性能通过零样本和少样本评估来评估，并且我们在表 [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Evaluating
    the Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox")
    中总结了我们基准的关键特性。'
- en: Empirical findings. Based on the experiments, we observe several counter-intuitive
    phenomena, e.g.,
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 实证发现。根据实验结果，我们观察到一些违反直觉的现象，例如，
- en: $\bullet$ *Tasks vary significantly in their sensitivity to quantization, and
    even the same tasks exhibit different sensitivities on different datasets*. For
    example, natural language inference tasks are the least sensitive across various
    tasks and MC-TACO ([74](#bib.bib74)) varies more than ARC-Easy ([7](#bib.bib7))
    in the zero-shot setting. We also observe that lower bit quantization even yields
    improved performance in some settings, such as GLUE-SST and GLUE-QNLI ([57](#bib.bib57))
    in the zero-shot setting.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ *任务对量化的敏感性差异很大，即使是相同的任务在不同的数据集上也会表现出不同的敏感性*。例如，自然语言推理任务在各种任务中最不敏感，而
    MC-TACO ([74](#bib.bib74)) 在零样本设置中比 ARC-Easy ([7](#bib.bib7)) 变化更大。我们还观察到，在某些设置下，低位量化甚至可以提高性能，例如
    GLUE-SST 和 GLUE-QNLI ([57](#bib.bib57)) 在零样本设置中。
- en: $\bullet$ *Consistency between calibration data and test distribution does not
    always yield optimal performance*, which is significantly correlated to the evaluation
    tasks and the magnitude of the distribution shift. For example, in cross-dataset
    tasks (English), there is often an optimal calibration dataset for the same task,
    which is *not* I.I.D data and can vary depending on the base quantization algorithm.
    For cross-subject tasks, except for the SpQR algorithm, which generally favors
    I.I.D data, the regularity of results in other settings is not obvious.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ *标定数据与测试分布之间的一致性并不总是能产生最佳性能*，这与评估任务和分布转移的幅度有显著关联。例如，在跨数据集任务（英语）中，通常存在一个最佳的标定数据集，这些数据集*不是*
    I.I.D 数据，并且可能会因基础量化算法的不同而有所变化。对于跨主体任务，除了通常偏向 I.I.D 数据的 SpQR 算法外，其他设置中的结果规律性并不明显。
- en: 'Table 1: Summary of the proposed benchmark.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 提出的基准的总结。'
- en: '| Scenario | Distribution Shift | Task Language | Weight Precision | Model
    | Benchmark & Dataset | Results |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 分布转移 | 任务语言 | 权重精度 | 模型 | 基准 & 数据集 | 结果 |'
- en: '| S1 | - | English | {16, 4, 3, 2} | LLaMA2-7B ([54](#bib.bib54)) | WinoGrande ([50](#bib.bib50)),
    WSC273 ([28](#bib.bib28)), HellaSwag ([71](#bib.bib71)) | Fig. [2](#S2.F2 "Figure
    2 ‣ 2 S1: Generalization Assessment of Quantized LLMs with Standard Setting ‣
    Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis,
    and Toolbox") |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| S1 | - | 英语 | {16, 4, 3, 2} | LLaMA2-7B ([54](#bib.bib54)) | WinoGrande ([50](#bib.bib50)),
    WSC273 ([28](#bib.bib28)), HellaSwag ([71](#bib.bib71)) | 图 [2](#S2.F2 "Figure
    2 ‣ 2 S1: 标准设置下量化LLMs的泛化能力评估 ‣ 评估量化LLMs的泛化能力：基准测试、分析和工具箱") |'
- en: '| SWAG ([70](#bib.bib70)), PIQA ([53](#bib.bib53)), MathQA ([2](#bib.bib2)),
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| SWAG ([70](#bib.bib70)), PIQA ([53](#bib.bib53)), MathQA ([2](#bib.bib2)),
    |'
- en: '| Mutual, Mutual_Plus ([8](#bib.bib8)), CrowS-Pairs ([42](#bib.bib42)), |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| Mutual, Mutual_Plus ([8](#bib.bib8)), CrowS-Pairs ([42](#bib.bib42)), |'
- en: '| Toxigen ([18](#bib.bib18)),PubMedQA ([22](#bib.bib22)), OpenBookQA ([38](#bib.bib38)),
    SciQ ([59](#bib.bib59)), |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| Toxigen ([18](#bib.bib18)),PubMedQA ([22](#bib.bib22)), OpenBookQA ([38](#bib.bib38)),
    SciQ ([59](#bib.bib59)), |'
- en: '| ARC-Easy, ARC-Challenge ([7](#bib.bib7)), MC-TACO ([74](#bib.bib74)), RACE ([27](#bib.bib27)),
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| ARC-Easy, ARC-Challenge ([7](#bib.bib7)), MC-TACO ([74](#bib.bib74)), RACE ([27](#bib.bib27)),
    |'
- en: '| QA4MRE ([45](#bib.bib45)), GLUE (6 datasets) ([57](#bib.bib57)), ANLI ([43](#bib.bib43)),
    BLiMP ([58](#bib.bib58)) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| QA4MRE ([45](#bib.bib45)), GLUE (6 datasets) ([57](#bib.bib57)), ANLI ([43](#bib.bib43)),
    BLiMP ([58](#bib.bib58)) |'
- en: '| S2 | Cross-dataset | English | {4, 3} | LLaMA2-7B ([54](#bib.bib54)) | BOSS
    (16 datasets) ([69](#bib.bib69)) | Tab. [2](#S3.T2 "Table 2 ‣ 3 S2: Generalization
    Assessment of Quantized LLMs with Domain Shifts ‣ Evaluating the Generalization
    Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox") |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 跨数据集 | 英语 | {4, 3} | LLaMA2-7B ([54](#bib.bib54)) | BOSS (16 datasets) ([69](#bib.bib69))
    | 表 [2](#S3.T2 "Table 2 ‣ 3 S2: 跨数据集下量化LLMs的泛化能力评估 ‣ 评估量化LLMs的泛化能力：基准测试、分析和工具箱")
    |'
- en: '| S2 | Cross-dataset | Chinese | {4, 3, 2} | Baichuan2-7B ([64](#bib.bib64))
    | C-EVAL ([20](#bib.bib20)), CMMLU ([29](#bib.bib29)) | Tab. [3](#S3.T3 "Table
    3 ‣ 3 S2: Generalization Assessment of Quantized LLMs with Domain Shifts ‣ Evaluating
    the Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox")
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 跨数据集 | 中文 | {4, 3, 2} | Baichuan2-7B ([64](#bib.bib64)) | C-EVAL ([20](#bib.bib20)),
    CMMLU ([29](#bib.bib29)) | 表 [3](#S3.T3 "Table 3 ‣ 3 S2: 跨数据集下量化LLMs的泛化能力评估 ‣
    评估量化LLMs的泛化能力：基准测试、分析和工具箱") |'
- en: '| S2 | Cross-subject | Chinese | {4, 3, 2} | Baichuan2-7B ([64](#bib.bib64))
    | C-EVAL ([20](#bib.bib20)) | Tab. LABEL:tab:cds_subject |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 跨学科 | 中文 | {4, 3, 2} | Baichuan2-7B ([64](#bib.bib64)) | C-EVAL ([20](#bib.bib20))
    | 表 LABEL:tab:cds_subject |'
- en: 'Toolbox. To support this work and facilitate future research, we develop a
    modular-designed code library. Specifically, this toolbox decouples the overall
    pipeline shown in Fig.[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Evaluating the Generalization
    Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox") into several separate
    components, e.g., LLM module, dataset module, quantizer module, etc., and provides
    common choices for each component and easy-to-use interface for possible extensions
    (see Section [4](#S4 "4 MI-optimize: A LLM Quantization Toolbox ‣ Evaluating the
    Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox") and
    Fig. [4](#S4 "4 MI-optimize: A LLM Quantization Toolbox ‣ Evaluating the Generalization
    Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox") for more details
    of the toolbox). This toolbox will be open-sourced along with the benchmark to
    facilitate future quantization applications and research.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '工具箱。为了支持本研究并促进未来的研究，我们开发了一个模块化设计的代码库。具体来说，这个工具箱将图 [1](#S1.F1 "Figure 1 ‣ 1
    介绍 ‣ 评估量化LLMs的泛化能力：基准测试、分析和工具箱")中显示的整体流程拆分为多个独立的组件，例如LLM模块、数据集模块、量化器模块等，并为每个组件提供常见的选项和易于使用的接口，以便进行可能的扩展（有关工具箱的更多细节，请参见第
    [4](#S4 "4 MI-optimize: A LLM Quantization Toolbox ‣ 评估量化LLMs的泛化能力：基准测试、分析和工具箱")节和图
    [4](#S4 "4 MI-optimize: A LLM Quantization Toolbox ‣ 评估量化LLMs的泛化能力：基准测试、分析和工具箱")）。这个工具箱将与基准测试一起开源，以促进未来的量化应用和研究。'
- en: '2 S1: Generalization Assessment of Quantized LLMs with Standard Setting'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '2 S1: 标准设置下量化LLMs的泛化能力评估'
- en: '![Refer to caption](img/059b47978e5a5c1b69afbe01d811ec35.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/059b47978e5a5c1b69afbe01d811ec35.png)'
- en: 'Figure 2: S1: evaluation of quantized LLaMA2-7B on several standard datasets.
    Quantization methods include GPTQ and SpQR. Quantization bits include W4A16, W3A16,
    and W2A16, with W16A16 used as reference. The left figure shows 5-shot results,
    while the right figure shows 0-shot results. Different background colors represent
    different task types.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：S1：对多个标准数据集上量化LLaMA2-7B的评估。量化方法包括GPTQ和SpQR。量化位数包括W4A16、W3A16和W2A16，以W16A16作为参考。左图显示了5-shot结果，右图显示了0-shot结果。不同的背景颜色代表不同的任务类型。
- en: To assess the difference in generalization ability, it is necessary to ensure
    that all other settings remain consistent except for the quantization process.
    To maintain consistency in the data encountered by the model before and after
    quantization, we strive to use calibration data during quantization that is as
    similar as possible to the data used during the pre-training phase of the LLM,
    namely the dataset C4 ([48](#bib.bib48)) derived from pre-training data. The experimental
    setting is consistent with the evaluation settings used previously for quantized
    models ([14](#bib.bib14); [10](#bib.bib10); [21](#bib.bib21); [33](#bib.bib33);
    [30](#bib.bib30)). We utilize the LM Evaluation Harness ([15](#bib.bib15)) with
    recommended parameters to conduct zero-shot and few-shot tests on the following
    tasks. We provide full configurations in the *supplemental material*, as well
    as code that we plan to release publicly.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估泛化能力的差异，需要确保除了量化过程外，所有其他设置保持一致。为了在量化前后保持模型遇到的数据的一致性，我们努力在量化过程中使用尽可能与LLM预训练阶段使用的数据相似的校准数据，即来源于预训练数据的C4数据集 ([48](#bib.bib48))。实验设置与之前对量化模型使用的评估设置一致 ([14](#bib.bib14);
    [10](#bib.bib10); [21](#bib.bib21); [33](#bib.bib33); [30](#bib.bib30))。我们利用带有推荐参数的LM
    Evaluation Harness ([15](#bib.bib15))对以下任务进行零样本和少样本测试。我们在*补充材料*中提供完整的配置以及我们计划公开发布的代码。
- en: 'The 26 datasets we evaluate can be divided into nine categories: ❶common sense
    reasoning, ❷mathematical reasoning, ❸multi-turn dialogue reasoning, ❹bias diagnosis
    and mitigation, ❺scientific knowledge question answering, ❻reading comprehension,
    ❼natural language inference, ❽sentiment analysis, and ❾syntax phenomena evaluation.
    The common sense reasoning datasets include WinoGrande ([50](#bib.bib50)), WSC273 ([28](#bib.bib28)),
    GLUE-WNLI ([57](#bib.bib57)), HellaSwag ([71](#bib.bib71)), SWAG ([70](#bib.bib70)),
    and PIQA ([53](#bib.bib53)). The mathematical reasoning datasets include MathQA([2](#bib.bib2)).
    The multi-turn dialogue reasoning datasets include Mutual and Mutual_Plus ([8](#bib.bib8)).
    The bias diagnosis and mitigation datasets include CrowS-Pairs ([42](#bib.bib42))
    and Toxigen ([18](#bib.bib18)). The scientific knowledge question answering datasets
    include PubMedQA ([22](#bib.bib22)), OpenBookQA ([38](#bib.bib38)), SciQ ([59](#bib.bib59)),
    ARC-Easy, ARC-Challenge ([7](#bib.bib7)), and MC-TACO ([74](#bib.bib74)). The
    reading comprehension datasets include RACE ([27](#bib.bib27)) and QA4MRE ([45](#bib.bib45)).
    The natural language inference datasets include GLUE-MNLI, GLUE-MNLI-Mismatched,
    GLUE-RTE, GLUE-QNLI ([57](#bib.bib57)), and ANLI ([43](#bib.bib43)). The sentiment
    analysis dataset includes GLUE-SST ([57](#bib.bib57)). The syntax phenomena evaluation
    dataset includes BLiMP ([58](#bib.bib58)).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估的26个数据集可以分为九类：❶常识推理，❷数学推理，❸多轮对话推理，❹偏见诊断和缓解，❺科学知识问答，❻阅读理解，❼自然语言推断，❽情感分析，以及❾语法现象评估。常识推理数据集包括WinoGrande ([50](#bib.bib50))、WSC273 ([28](#bib.bib28))、GLUE-WNLI ([57](#bib.bib57))、HellaSwag ([71](#bib.bib71))、SWAG ([70](#bib.bib70))和PIQA ([53](#bib.bib53))。数学推理数据集包括MathQA([2](#bib.bib2))。多轮对话推理数据集包括Mutual和Mutual_Plus ([8](#bib.bib8))。偏见诊断和缓解数据集包括CrowS-Pairs ([42](#bib.bib42))和Toxigen ([18](#bib.bib18))。科学知识问答数据集包括PubMedQA ([22](#bib.bib22))、OpenBookQA ([38](#bib.bib38))、SciQ ([59](#bib.bib59))、ARC-Easy、ARC-Challenge ([7](#bib.bib7))和MC-TACO ([74](#bib.bib74))。阅读理解数据集包括RACE ([27](#bib.bib27))和QA4MRE ([45](#bib.bib45))。自然语言推断数据集包括GLUE-MNLI、GLUE-MNLI-Mismatched、GLUE-RTE、GLUE-QNLI ([57](#bib.bib57))和ANLI ([43](#bib.bib43))。情感分析数据集包括GLUE-SST ([57](#bib.bib57))。语法现象评估数据集包括BLiMP ([58](#bib.bib58))。
- en: 'We present the experimental results for both the 5-shot and 0-shot scenarios
    in Fig. [2](#S2.F2 "Figure 2 ‣ 2 S1: Generalization Assessment of Quantized LLMs
    with Standard Setting ‣ Evaluating the Generalization Ability of Quantized LLMs:
    Benchmark, Analysis, and Toolbox"). It can be observed that when quantizing model
    weights to 3-4 bits, the performance degradation of all methods is not very pronounced.
    In some cases, quantizing to 4 bits even leads to higher model performance compared
    to full precision. However, when weights are quantized to 2 bits, GPTQ exhibits
    a significant performance drop on most tasks. Compared to other methods, SPQR
    maintains relatively good performance at 2 bits, which may be attributed to SPQR’s
    ability to identify and isolate outlier weights. Additionally, we found that the
    relative difference in performance degradation after quantization across different
    datasets for the same task is not significant, whereas the relative difference
    in performance degradation after quantization between datasets for different tasks
    is considerable. This suggests that the sensitivity to quantization is similar
    for the same tasks but varies for different tasks. For instance, in natural language
    inference tasks, the performance drop of quantized models is minimal across all
    datasets, while in scientific knowledge question answering tasks and common sense
    reasoning tasks, the performance drop is more significant. In the case of the
    5-shot scenario, the performance degradation caused by quantization is relatively
    smoother compared to 0-shot scenario, especially noticeable in scientific knowledge
    question answering tasks. For example, on the SciQ dataset, in the 5-shot scenario,
    the performance of GPTQ decreases from 0.96 at 4 bits to 0.69 at 2 bits, whereas
    in the 0-shot scenario, it drops from 0.91 at 4 bits to 0.51.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图Fig. [2](#S2.F2 "Figure 2 ‣ 2 S1: Generalization Assessment of Quantized
    LLMs with Standard Setting ‣ Evaluating the Generalization Ability of Quantized
    LLMs: Benchmark, Analysis, and Toolbox")中展示了5-shot和0-shot场景的实验结果。可以观察到，当将模型权重量化到3-4位时，所有方法的性能下降并不明显。在某些情况下，量化到4位甚至会导致模型性能优于全精度。然而，当权重量化到2位时，GPTQ在大多数任务上的性能显著下降。与其他方法相比，SPQR在2位时保持了相对较好的性能，这可能归因于SPQR识别和隔离异常权重的能力。此外，我们发现，相同任务在不同数据集上量化后的性能下降的相对差异不大，而不同任务之间的数据集量化后的性能下降差异则较大。这表明，对于相同任务，量化的敏感度类似，但对于不同任务则有所变化。例如，在自然语言推理任务中，量化模型的性能下降在所有数据集上都很小，而在科学知识问答任务和常识推理任务中，性能下降更为显著。在5-shot场景下，量化造成的性能下降相比于0-shot场景较为平缓，尤其在科学知识问答任务中尤为明显。例如，在SciQ数据集上，在5-shot场景下，GPTQ的性能从4位的0.96下降到2位的0.69，而在0-shot场景下，从4位的0.91下降到0.51。'
- en: Overall, when most methods quantize weights to 3-4 bits precision, models can
    still achieve performance close to that of full precision models on most tasks.
    This indicates that under moderate quantization, models still retain strong generalization
    capabilities. Additionally, different tasks exhibit varying sensitivities to quantization,
    with natural language inference tasks showing lower sensitivity while scientific
    knowledge question answering tasks and common sense reasoning tasks emerge higher
    sensitivity. In the 5-shot scenario, the performance degradation due to quantization
    is relatively smoother compared to the 0-shot scenario.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，当大多数方法将权重量化到3-4位精度时，模型在大多数任务上的表现仍接近全精度模型。这表明，在适度量化下，模型仍保持强大的泛化能力。此外，不同任务对量化的敏感度不同，自然语言推理任务的敏感度较低，而科学知识问答任务和常识推理任务的敏感度较高。在5-shot场景下，与0-shot场景相比，量化导致的性能下降相对较平缓。
- en: '3 S2: Generalization Assessment of Quantized LLMs with Domain Shifts'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '3 S2: 量化LLMs在领域转移下的泛化评估'
- en: 'This section investigates novel generalization scenarios in quantization, where
    different generalization scenarios serve as instantiations of the framework. The
    distribution shift we consider primarily pertains to the shift from calibration
    data to test data. Types of distribution shift include *cross-dataset* distribution
    shift and *cross-subject* distribution shift, aimed at studying the impact of
    distribution shift from calibration data to test data on quantized model performance.
    Cross-dataset distribution shift refers to using different datasets as calibration
    set, while cross-subject distribution shift refers to using different subjects
    from the same dataset as calibration set. Experiments will encompass two main
    categories: *English* *cross-dataset* distribution shift experiments on the out-of-distribution
    generalization benchmark BOSS, and *Chinese* *cross-subject* distribution shift
    experiments as well as *cross-dataset* distribution shift experiments on Chinese
    domain-specific tasks.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨量化中的新颖泛化场景，其中不同的泛化场景作为框架的实例。我们考虑的分布转移主要涉及从校准数据到测试数据的转移。分布转移的类型包括*跨数据集*分布转移和*跨主体*分布转移，旨在研究从校准数据到测试数据的分布转移对量化模型性能的影响。跨数据集分布转移是指使用不同的数据集作为校准集，而跨主体分布转移是指使用来自同一数据集的不同主体作为校准集。实验将包括两个主要类别：在超出分布泛化基准
    BOSS 上的*英文* *跨数据集*分布转移实验，以及*中文* *跨主体*分布转移实验和*跨数据集*分布转移实验，专注于中文领域特定任务。
- en: 'Table 2: Cross-dataset distribution shift evaluation on BOSS. "Calib." represents
    the calibration dataset, and "Gene." represents generalization scenario. To save
    space, abbreviations are used for datasets. Each row presents experimental results
    using different datasets as calibration sets on the same test dataset. Results
    with colored backgrounds indicate I.I.D results, while those without color represent
    OOD results. The higher the metric, the better the performance. Bold results indicate
    the best performance on the same test dataset. Note: Some datasets could not be
    used as calibration sets due to insufficient memory resources.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 BOSS 上的跨数据集分布转移评估。 "Calib." 代表校准数据集，"Gene." 代表泛化场景。为了节省空间，数据集使用了缩写。每行展示了使用不同数据集作为校准集在相同测试数据集上的实验结果。具有彩色背景的结果表示
    I.I.D 结果，而没有颜色的结果代表 OOD 结果。指标越高，性能越好。粗体结果表示在相同测试数据集上的最佳表现。注意：由于内存资源不足，某些数据集无法用作校准集。
- en: '| Method | EQA | SA | NLI | TD |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | EQA | SA | NLI | TD |'
- en: '| GPTQ | Test | Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. | Test |
    Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 测试 | 泛化 | W/A | 校准 | 测试 | 泛化 | W/A | 校准 | 测试 | 泛化 | W/A | 校准 | 测试
    | 泛化 | W/A | 校准 |'
- en: '| SQ | AQA | NQA | SQA | AZ | DS | SE | SST | MN | AN | WN | CN | CC | AC |
    IH | TG |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| SQ | AQA | NQA | SQA | AZ | DS | SE | SST | MN | AN | WN | CN | CC | AC |
    IH | TG |'
- en: '| SQ | 0-shot | 4/16 | 53.84 | 52.73 | 54.69 | 57.31 | AZ | 0-shot | 4/16 |
    70.81 | 17.87 | 63.18 | 72.08 | MN | 0-shot | 4/16 | 0.36 | 0.23 | 0.22 | - |
    CC | 0-shot | 4/16 | 23.90 | 26.96 | 52.52 | 53.32 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| SQ | 0-shot | 4/16 | 53.84 | 52.73 | 54.69 | 57.31 | AZ | 0-shot | 4/16 |
    70.81 | 17.87 | 63.18 | 72.08 | MN | 0-shot | 4/16 | 0.36 | 0.23 | 0.22 | - |
    CC | 0-shot | 4/16 | 23.90 | 26.96 | 52.52 | 53.32 |'
- en: '| 3/16 | 45.31 | 48.86 | 49.49 | 50.79 | 3/16 | 38.06 | 0.38 | 0.26 | 0.04
    | 3/16 | 0.00 | 0.00 | 0.00 | - | 3/16 | 0.60 | 2.45 | 9.70 | 10.60 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 45.31 | 48.86 | 49.49 | 50.79 | 3/16 | 38.06 | 0.38 | 0.26 | 0.04
    | 3/16 | 0.00 | 0.00 | 0.00 | - | 3/16 | 0.60 | 2.45 | 9.70 | 10.60 |'
- en: '| 1-shot | 4/16 | 67.04 | 65.97 | 67.06 | 68.16 | 3-shot | 4/16 | 83.69 | 56.66
    | 80.79 | 82.55 | 3-shot | 4/16 | 49.69 | 32.81 | 34.93 | - | 2-shot | 4/16 |
    91.80 | 87.46 | 91.71 | 91.84 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/16 | 67.04 | 65.97 | 67.06 | 68.16 | 3-shot | 4/16 | 83.69 | 56.66
    | 80.79 | 82.55 | 3-shot | 4/16 | 49.69 | 32.81 | 34.93 | - | 2-shot | 4/16 |
    91.80 | 87.46 | 91.71 | 91.84 |'
- en: '| 3/16 | 60.76 | 58.84 | 63.34 | 63.01 | 3/16 | 74.54 | 24.86 | 59.06 | 59.79
    | 3/16 | 34.12 | 31.79 | 31.82 | - | 3/16 | 89.11 | 35.94 | 91.96 | 90.35 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 60.76 | 58.84 | 63.34 | 63.01 | 3/16 | 74.54 | 24.86 | 59.06 | 59.79
    | 3/16 | 34.12 | 31.79 | 31.82 | - | 3/16 | 89.11 | 35.94 | 91.96 | 90.35 |'
- en: '| AQA | 0-shot | 4/16 | 28.00 | 27.12 | 28.40 | 30.40 | DS | 0-shot | 4/16
    | 46.10 | 21.37 | 31.82 | 46.79 | AN | 0-shot | 4/16 | 1.07 | 0.52 | 0.93 | -
    | AC | 0-shot | 4/16 | 19.12 | 5.93 | 7.84 | 17.02 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| AQA | 0-shot | 4/16 | 28.00 | 27.12 | 28.40 | 30.40 | DS | 0-shot | 4/16
    | 46.10 | 21.37 | 31.82 | 46.79 | AN | 0-shot | 4/16 | 1.07 | 0.52 | 0.93 | -
    | AC | 0-shot | 4/16 | 19.12 | 5.93 | 7.84 | 17.02 |'
- en: '| 3/16 | 21.81 | 25.28 | 23.35 | 24.99 | 3/16 | 17.59 | 1.72 | 0.01 | 0.00
    | 3/16 | 4.17 | 0.00 | 0.00 | - | 3/16 | 0.76 | 1.72 | 0.19 | 0.57 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 21.81 | 25.28 | 23.35 | 24.99 | 3/16 | 17.59 | 1.72 | 0.01 | 0.00
    | 3/16 | 4.17 | 0.00 | 0.00 | - | 3/16 | 0.76 | 1.72 | 0.19 | 0.57 |'
- en: '| 1-shot | 4/16 | 35.50 | 36.11 | 31.97 | 35.77 | 3-shot | 4/16 | 54.40 | 38.78
    | 52.54 | 55.50 | 3-shot | 4/16 | 34.34 | 33.76 | 33.24 | - | 2-shot | 4/16 |
    15.87 | 17.59 | 15.87 | 16.25 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/16 | 35.50 | 36.11 | 31.97 | 35.77 | 3-shot | 4/16 | 54.40 | 38.78
    | 52.54 | 55.50 | 3-shot | 4/16 | 34.34 | 33.76 | 33.24 | - | 2-shot | 4/16 |
    15.87 | 17.59 | 15.87 | 16.25 |'
- en: '| 3/16 | 31.39 | 29.54 | 31.60 | 32.24 | 3/16 | 54.68 | 36.05 | 33.86 | 43.46
    | 3/16 | 30.97 | 33.69 | 33.28 | - | 3/16 | 60.23 | 90.35 | 15.87 | 56.02 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 31.39 | 29.54 | 31.60 | 32.24 | 3/16 | 54.68 | 36.05 | 33.86 | 43.46
    | 3/16 | 30.97 | 33.69 | 33.28 | - | 3/16 | 60.23 | 90.35 | 15.87 | 56.02 |'
- en: '| NQA | 0-shot | 4/16 | 37.94 | 38.76 | 38.63 | 38.23 | SE | 0-shot | 4/16
    | 18.32 | 8.21 | 15.60 | 26.43 | WN | 0-shot | 4/16 | 0.09 | 0.04 | 0.11 | - |
    IH | 0-shot | 4/16 | 37.37 | 22.55 | 33.90 | 40.82 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| NQA | 0-shot | 4/16 | 37.94 | 38.76 | 38.63 | 38.23 | SE | 0-shot | 4/16
    | 18.32 | 8.21 | 15.60 | 26.43 | WN | 0-shot | 4/16 | 0.09 | 0.04 | 0.11 | - |
    IH | 0-shot | 4/16 | 37.37 | 22.55 | 33.90 | 40.82 |'
- en: '| 3/16 | 31.36 | 33.79 | 33.37 | 34.45 | 3/16 | 4.83 | 0.09 | 0.20 | 0.01 |
    3/16 | 0.49 | 0.00 | 0.00 | - | 3/16 | 11.27 | 7.32 | 4.53 | 13.18 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 31.36 | 33.79 | 33.37 | 34.45 | 3/16 | 4.83 | 0.09 | 0.20 | 0.01 |
    3/16 | 0.49 | 0.00 | 0.00 | - | 3/16 | 11.27 | 7.32 | 4.53 | 13.18 |'
- en: '| 1-shot | 4/16 | 48.55 | 49.30 | 49.73 | 49.09 | 3-shot | 4/16 | 42.96 | 28.55
    | 42.99 | 44.75 | 3-shot | 4/16 | 41.51 | 43.34 | 47.53 | - | 2-shot | 4/16 |
    62.36 | 63.46 | 62.00 | 62.29 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/16 | 48.55 | 49.30 | 49.73 | 49.09 | 3-shot | 4/16 | 42.96 | 28.55
    | 42.99 | 44.75 | 3-shot | 4/16 | 41.51 | 43.34 | 47.53 | - | 2-shot | 4/16 |
    62.36 | 63.46 | 62.00 | 62.29 |'
- en: '| W3A16 | 44.38 | 43.35 | 46.95 | 45.61 | 3/16 | 42.36 | 22.67 | 35.54 | 29.40
    | 3/16 | 38.83 | 48.09 | 48.15 | - | 3/16 | 63.52 | 90.35 | 61.83 | 61.77 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| W3A16 | 44.38 | 43.35 | 46.95 | 45.61 | 3/16 | 42.36 | 22.67 | 35.54 | 29.40
    | 3/16 | 38.83 | 48.09 | 48.15 | - | 3/16 | 63.52 | 90.35 | 61.83 | 61.77 |'
- en: '| SQA | 0-shot | 4/16 | 42.58 | 45.72 | 46.21 | 44.20 | SST | 0-shot | 4/16
    | 49.15 | 20.73 | 27.12 | 44.98 | CN | 0-shot | 4/16 | 0.06 | 0.00 | 0.00 | -
    | TG | 0-shot | 4/16 | 48.44 | 36.72 | 44.84 | 57.97 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| SQA | 0-shot | 4/16 | 42.58 | 45.72 | 46.21 | 44.20 | SST | 0-shot | 4/16
    | 49.15 | 20.73 | 27.12 | 44.98 | CN | 0-shot | 4/16 | 0.06 | 0.00 | 0.00 | -
    | TG | 0-shot | 4/16 | 48.44 | 36.72 | 44.84 | 57.97 |'
- en: '| 3/16 | 30.19 | 26.99 | 28.49 | 33.73 | 3/16 | 7.82 | 1.04 | 0.00 | 0.00 |
    3/16 | 0.06 | 1.12 | 1.45 | - | 3/16 | 12.81 | 9.53 | 2.19 | 14.06 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 30.19 | 26.99 | 28.49 | 33.73 | 3/16 | 7.82 | 1.04 | 0.00 | 0.00 |
    3/16 | 0.06 | 1.12 | 1.45 | - | 3/16 | 12.81 | 9.53 | 2.19 | 14.06 |'
- en: '| 1-shot | 4/16 | 56.04 | 61.89 | 60.92 | 62.17 | 3-shot | 4/16 | 60.50 | 33.25
    | 45.24 | 51.11 | 3-shot | 4/16 | 35.23 | 36.35 | 32.44 | - | 2-shot | 4/16 |
    72.03 | 75.47 | 67.81 | 68.40 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/16 | 56.04 | 61.89 | 60.92 | 62.17 | 3-shot | 4/16 | 60.50 | 33.25
    | 45.24 | 51.11 | 3-shot | 4/16 | 35.23 | 36.35 | 32.44 | - | 2-shot | 4/16 |
    72.03 | 75.47 | 67.81 | 68.40 |'
- en: '| 3/16 | 43.46 | 42.83 | 45.17 | 48.82 | 3/16 | 54.37 | 33.25 | 35.46 | 50.20
    | 3/16 | 29.54 | 29.03 | 33.39 | - | 3/16 | 70.47 | 90.35 | 57.50 | 62.19 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 43.46 | 42.83 | 45.17 | 48.82 | 3/16 | 54.37 | 33.25 | 35.46 | 50.20
    | 3/16 | 29.54 | 29.03 | 33.39 | - | 3/16 | 70.47 | 90.35 | 57.50 | 62.19 |'
- en: '| SpQR | Test | Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. | Test |
    Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 测试 | 基因 | W/A | 校准 | 测试 | 基因 | W/A | 校准 | 测试 | 基因 | W/A | 校准 | 测试
    | 基因 | W/A | 校准 |'
- en: '| SQ | AQA | NQA | SQA | AZ | DS | SE | SST | MN | AN | WN | CN | CC | AC |
    IH | TG |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| SQ | AQA | NQA | SQA | AZ | DS | SE | SST | MN | AN | WN | CN | CC | AC |
    IH | TG |'
- en: '| SQ | 0-shot | 4/16 | 57.03 | 49.87 | 53.00 | 54.36 | AZ | 0-shot | 4/16 |
    63.34 | 62.46 | 72.52 | 83.14 | MN | 0-shot | 4/16 | 0.57 | 0.02 | 0.13 | - |
    CC | 0-shot | 4/16 | 61.73 | 59.48 | 58.92 | 37.48 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| SQ | 0-shot | 4/16 | 57.03 | 49.87 | 53.00 | 54.36 | AZ | 0-shot | 4/16 |
    63.34 | 62.46 | 72.52 | 83.14 | MN | 0-shot | 4/16 | 0.57 | 0.02 | 0.13 | - |
    CC | 0-shot | 4/16 | 61.73 | 59.48 | 58.92 | 37.48 |'
- en: '| 3/16 | 52.37 | 45.90 | 54.55 | 58.36 | 3/16 | 72.38 | 55.79 | 37.28 | 27.84
    | 3/16 | 0.00 | 0.01 | 0.00 | - | 3/16 | 36.90 | 2.54 | 15.42 | 22.38 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 52.37 | 45.90 | 54.55 | 58.36 | 3/16 | 72.38 | 55.79 | 37.28 | 27.84
    | 3/16 | 0.00 | 0.01 | 0.00 | - | 3/16 | 36.90 | 2.54 | 15.42 | 22.38 |'
- en: '| 1-shot | 4/16 | 66.45 | 66.80 | 67.41 | 67.21 | 3-shot | 4/16 | 79.65 | 69.31
    | 85.44 | 82.91 | 3-shot | 4/16 | 36.19 | 40.45 | 41.62 | - | 2-shot | 4/16 |
    90.65 | 89.27 | 91.74 | 84.69 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/16 | 66.45 | 66.80 | 67.41 | 67.21 | 3-shot | 4/16 | 79.65 | 69.31
    | 85.44 | 82.91 | 3-shot | 4/16 | 36.19 | 40.45 | 41.62 | - | 2-shot | 4/16 |
    90.65 | 89.27 | 91.74 | 84.69 |'
- en: '| 3/16 | 65.12 | 65.55 | 68.65 | 66.95 | 3/16 | 83.68 | 86.30 | 72.18 | 83.50
    | 3/16 | 32.39 | 40.31 | 38.47 | - | 3/16 | 87.70 | 91.76 | 86.99 | 83.56 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 65.12 | 65.55 | 68.65 | 66.95 | 3/16 | 83.68 | 86.30 | 72.18 | 83.50
    | 3/16 | 32.39 | 40.31 | 38.47 | - | 3/16 | 87.70 | 91.76 | 86.99 | 83.56 |'
- en: '| AQA | 0-shot | 4/16 | 30.59 | 25.11 | 27.60 | 29.50 | DS | 0-shot | 4/16
    | 35.47 | 43.53 | 40.85 | 50.40 | AN | 0-shot | 4/16 | 0.86 | 0.07 | 0.28 | -
    | AC | 0-shot | 4/16 | 10.13 | 4.97 | 12.05 | 13.58 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| AQA | 0-shot | 4/16 | 30.59 | 25.11 | 27.60 | 29.50 | DS | 0-shot | 4/16
    | 35.47 | 43.53 | 40.85 | 50.40 | AN | 0-shot | 4/16 | 0.86 | 0.07 | 0.28 | -
    | AC | 0-shot | 4/16 | 10.13 | 4.97 | 12.05 | 13.58 |'
- en: '| 3/16 | 26.35 | 21.43 | 27.55 | 30.36 | 3/16 | 41.87 | 31.17 | 15.42 | 29.10
    | 3/16 | 0.00 | 0.07 | 0.00 | - | 3/16 | 2.49 | 0.76 | 7.84 | 2.87 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 26.35 | 21.43 | 27.55 | 30.36 | 3/16 | 41.87 | 31.17 | 15.42 | 29.10
    | 3/16 | 0.00 | 0.07 | 0.00 | - | 3/16 | 2.49 | 0.76 | 7.84 | 2.87 |'
- en: '| 1-shot | 4/16 | 37.64 | 36.63 | 36.94 | 35.42 | 3-shot | 4/16 | 50.82 | 46.67
    | 57.74 | 56.34 | 3-shot | 4/16 | 33.17 | 33.31 | 33.79 | - | 2-shot | 4/16 |
    16.44 | 21.03 | 15.87 | 20.46 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/16 | 37.64 | 36.63 | 36.94 | 35.42 | 3-shot | 4/16 | 50.82 | 46.67
    | 57.74 | 56.34 | 3-shot | 4/16 | 33.17 | 33.31 | 33.79 | - | 2-shot | 4/16 |
    16.44 | 21.03 | 15.87 | 20.46 |'
- en: '| 3/16 | 34.61 | 34.75 | 37.49 | 33.10 | 3/16 | 59.10 | 54.80 | 52.56 | 56.02
    | 3/16 | 33.66 | 31.93 | 33.14 | - | 3/16 | 15.87 | 15.87 | 19.31 | 15.87 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 34.61 | 34.75 | 37.49 | 33.10 | 3/16 | 59.10 | 54.80 | 52.56 | 56.02
    | 3/16 | 33.66 | 31.93 | 33.14 | - | 3/16 | 15.87 | 15.87 | 19.31 | 15.87 |'
- en: '| NQA | 0-shot | 4/16 | 40.30 | 38.01 | 39.40 | 38.22 | SE | 0-shot | 4/16
    | 14.62 | 23.36 | 19.85 | 33.24 | WN | 0-shot | 4/16 | 0.28 | 0.00 | 0.00 | -
    | IH | 0-shot | 4/16 | 42.21 | 41.79 | 40.12 | 31.76 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| NQA | 0-shot | 4/16 | 40.30 | 38.01 | 39.40 | 38.22 | SE | 0-shot | 4/16
    | 14.62 | 23.36 | 19.85 | 33.24 | WN | 0-shot | 4/16 | 0.28 | 0.00 | 0.00 | -
    | IH | 0-shot | 4/16 | 42.21 | 41.79 | 40.12 | 31.76 |'
- en: '| 3/16 | 35.79 | 33.27 | 40.80 | 38.77 | 3/16 | 16.05 | 10.22 | 4.75 | 7.30
    | 3/16 | 0.00 | 0.06 | 0.00 | - | 3/16 | 31.32 | 6.78 | 17.68 | 16.96 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 35.79 | 33.27 | 40.80 | 38.77 | 3/16 | 16.05 | 10.22 | 4.75 | 7.30
    | 3/16 | 0.00 | 0.06 | 0.00 | - | 3/16 | 31.32 | 6.78 | 17.68 | 16.96 |'
- en: '| 1-shot | 4/16 | 49.61 | 49.12 | 49.70 | 48.47 | 3-shot | 4/16 | 44.48 | 44.15
    | 44.25 | 44.39 | 3-shot | 4/16 | 43.28 | 43.77 | 41.79 | - | 2-shot | 4/16 |
    64.24 | 65.85 | 62.14 | 66.07 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/16 | 49.61 | 49.12 | 49.70 | 48.47 | 3-shot | 4/16 | 44.48 | 44.15
    | 44.25 | 44.39 | 3-shot | 4/16 | 43.28 | 43.77 | 41.79 | - | 2-shot | 4/16 |
    64.24 | 65.85 | 62.14 | 66.07 |'
- en: '| 3/16 | 48.25 | 46.61 | 48.99 | 47.79 | 3/16 | 53.16 | 43.63 | 41.76 | 44.77
    | 3/16 | 39.09 | 47.32 | 40.77 | - | 3/16 | 62.95 | 63.14 | 63.17 | 64.37 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 48.25 | 46.61 | 48.99 | 47.79 | 3/16 | 53.16 | 43.63 | 41.76 | 44.77
    | 3/16 | 39.09 | 47.32 | 40.77 | - | 3/16 | 62.95 | 63.14 | 63.17 | 64.37 |'
- en: '| SQA | 0-shot | 4/16 | 46.45 | 42.62 | 44.30 | 45.10 | SST | 0-shot | 4/16
    | 46.02 | 29.47 | 44.72 | 55.67 | CN | 0-shot | 4/16 | 0.00 | 0.22 | 0.45 | -
    | TG | 0-shot | 4/16 | 54.37 | 52.66 | 51.09 | 39.53 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| SQA | 0-shot | 4/16 | 46.45 | 42.62 | 44.30 | 45.10 | SST | 0-shot | 4/16
    | 46.02 | 29.47 | 44.72 | 55.67 | CN | 0-shot | 4/16 | 0.00 | 0.22 | 0.45 | -
    | TG | 0-shot | 4/16 | 54.37 | 52.66 | 51.09 | 39.53 |'
- en: '| 3/16 | 36.90 | 44.57 | 42.88 | 39.31 | 3/16 | 23.08 | 14.87 | 3.65 | 6.52
    | 3/16 | 0.06 | 0.00 | 0.89 | - | 3/16 | 41.88 | 9.69 | 19.38 | 37.34 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 36.90 | 44.57 | 42.88 | 39.31 | 3/16 | 23.08 | 14.87 | 3.65 | 6.52
    | 3/16 | 0.06 | 0.00 | 0.89 | - | 3/16 | 41.88 | 9.69 | 19.38 | 37.34 |'
- en: '| 1-shot | 4/16 | 61.63 | 57.77 | 61.79 | 60.55 | 3-shot | 4/16 | 55.41 | 42.37
    | 58.54 | 59.32 | 3-shot | 4/16 | 36.13 | 34.84 | 34.23 | - | 2-shot | 4/16 |
    69.84 | 76.56 | 61.41 | 77.60 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/16 | 61.63 | 57.77 | 61.79 | 60.55 | 3-shot | 4/16 | 55.41 | 42.37
    | 58.54 | 59.32 | 3-shot | 4/16 | 36.13 | 34.84 | 34.23 | - | 2-shot | 4/16 |
    69.84 | 76.56 | 61.41 | 77.60 |'
- en: '| 3/16 | 48.86 | 59.19 | 56.34 | 55.06 | 3/16 | 63.49 | 60.37 | 53.98 | 61.80
    | 3/16 | 35.29 | 35.90 | 33.17 | - | 3/16 | 73.13 | 66.88 | 68.44 | 77.03 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 48.86 | 59.19 | 56.34 | 55.06 | 3/16 | 63.49 | 60.37 | 53.98 | 61.80
    | 3/16 | 35.29 | 35.90 | 33.17 | - | 3/16 | 73.13 | 66.88 | 68.44 | 77.03 |'
- en: '| AWQ | Test | Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. | Test |
    Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 测试 | 基因 | W/A | 校准 | 测试 | 基因 | W/A | 校准 | 测试 | 基因 | W/A | 校准 | 测试 |
    基因 | W/A | 校准 |'
- en: '| SQ | AQA | NQA | SQA | AZ | DS | SE | SST | MN | AN | WN | CN | CC | AC |
    IH | TG |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| SQ | AQA | NQA | SQA | AZ | DS | SE | SST | MN | AN | WN | CN | CC | AC |
    IH | TG |'
- en: '| SQ | 0-shot | 4/16 | 56.73 | 55.09 | 52.09 | 50.21 | AZ | 0-shot | 4/16 |
    - | 5.42 | 35.23 | 33.65 | MN | 0-shot | 4/16 | 0.48 | 0.14 | 0.06 | - | CC |
    0-shot | 4/16 | 50.17 | 66.60 | 42.19 | 42.11 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| SQ | 0-shot | 4/16 | 56.73 | 55.09 | 52.09 | 50.21 | AZ | 0-shot | 4/16 |
    - | 5.42 | 35.23 | 33.65 | MN | 0-shot | 4/16 | 0.48 | 0.14 | 0.06 | - | CC |
    0-shot | 4/16 | 50.17 | 66.60 | 42.19 | 42.11 |'
- en: '| 3/16 | 48.32 | 37.95 | 44.45 | 40.30 | 3/16 | - | 39.41 | 70.10 | 35.95 |
    3/16 | 0.00 | 0.01 | 0.01 | - | 3/16 | 41.96 | 39.03 | 46.95 | 14.72 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 48.32 | 37.95 | 44.45 | 40.30 | 3/16 | - | 39.41 | 70.10 | 35.95 |
    3/16 | 0.00 | 0.01 | 0.01 | - | 3/16 | 41.96 | 39.03 | 46.95 | 14.72 |'
- en: '| 1-shot | 4/16 | 66.57 | 66.91 | 67.02 | 66.21 | 3-shot | 4/16 | - | 83.64
    | 83.73 | 78.06 | 3-shot | 4/16 | 42.20 | 38.37 | 36.05 | - | 2-shot | 4/16 |
    91.84 | 91.63 | 90.80 | 89.31 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/16 | 66.57 | 66.91 | 67.02 | 66.21 | 3-shot | 4/16 | - | 83.64
    | 83.73 | 78.06 | 3-shot | 4/16 | 42.20 | 38.37 | 36.05 | - | 2-shot | 4/16 |
    91.84 | 91.63 | 90.80 | 89.31 |'
- en: '| 3/16 | 59.81 | 61.81 | 61.27 | 61.38 | 3/16 | - | 88.73 | 90.16 | 88.92 |
    3/16 | 35.44 | 34.22 | 35.34 | - | 3/16 | 36.43 | 73.04 | 90.93 | 27.24 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 59.81 | 61.81 | 61.27 | 61.38 | 3/16 | - | 88.73 | 90.16 | 88.92 |
    3/16 | 35.44 | 34.22 | 35.34 | - | 3/16 | 36.43 | 73.04 | 90.93 | 27.24 |'
- en: '| AQA | 0-shot | 4/16 | 29.73 | 29.20 | 28.34 | 27.57 | DS | 0-shot | 4/16
    | - | 2.36 | 20.10 | 22.19 | AN | 0-shot | 4/16 | 0.59 | 0.07 | 0.07 | - | AC
    | 0-shot | 4/16 | 9.56 | 11.85 | 11.28 | 5.55 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| AQA | 0-shot | 4/16 | 29.73 | 29.20 | 28.34 | 27.57 | DS | 0-shot | 4/16
    | - | 2.36 | 20.10 | 22.19 | AN | 0-shot | 4/16 | 0.59 | 0.07 | 0.07 | - | AC
    | 0-shot | 4/16 | 9.56 | 11.85 | 11.28 | 5.55 |'
- en: '| 3/16 | 23.02 | 17.58 | 20.37 | 18.62 | 3/16 | - | 8.76 | 27.09 | 11.87 |
    3/16 | 0.00 | 0.00 | 0.00 | - | 3/16 | 5.74 | 4.59 | 4.21 | 1.15 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 23.02 | 17.58 | 20.37 | 18.62 | 3/16 | - | 8.76 | 27.09 | 11.87 |
    3/16 | 0.00 | 0.00 | 0.00 | - | 3/16 | 5.74 | 4.59 | 4.21 | 1.15 |'
- en: '| 1-shot | 4/16 | 35.76 | 37.01 | 37.55 | 36.78 | 3-shot | 4/16 | - | 53.91
    | 55.92 | 50.95 | 3-shot | 4/16 | 33.66 | 33.66 | 33.66 | - | 2-shot | 4/16 |
    15.87 | 15.87 | 16.06 | 16.63 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/16 | 35.76 | 37.01 | 37.55 | 36.78 | 3-shot | 4/16 | - | 53.91
    | 55.92 | 50.95 | 3-shot | 4/16 | 33.66 | 33.66 | 33.66 | - | 2-shot | 4/16 |
    15.87 | 15.87 | 16.06 | 16.63 |'
- en: '| 3/16 | 31.64 | 33.04 | 32.88 | 33.46 | 3/16 | - | 50.95 | 56.24 | 59.05 |
    3/16 | 33.69 | 32.55 | 33.69 | - | 3/16 | 24.86 | 18.93 | 16.06 | 56.02 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 31.64 | 33.04 | 32.88 | 33.46 | 3/16 | - | 50.95 | 56.24 | 59.05 |
    3/16 | 33.69 | 32.55 | 33.69 | - | 3/16 | 24.86 | 18.93 | 16.06 | 56.02 |'
- en: '| NQA | 0-shot | 4/16 | 39.20 | 38.58 | 39.47 | 38.10 | SE | 0-shot | 4/16
    | - | 4.19 | 18.90 | 14.96 | WN | 0-shot | 4/16 | 0.30 | 0.17 | 0.02 | - | IH
    | 0-shot | 4/16 | 37.59 | 44.64 | 34.09 | 27.16 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| NQA | 0-shot | 4/16 | 39.20 | 38.58 | 39.47 | 38.10 | SE | 0-shot | 4/16
    | - | 4.19 | 18.90 | 14.96 | WN | 0-shot | 4/16 | 0.30 | 0.17 | 0.02 | - | IH
    | 0-shot | 4/16 | 37.59 | 44.64 | 34.09 | 27.16 |'
- en: '| 3/16 | 35.75 | 31.27 | 32.91 | 33.69 | 3/16 | - | 5.52 | 14.95 | 5.49 | 3/16
    | 0.00 | 0.00 | 0.00 | - | 3/16 | 20.22 | 17.97 | 25.72 | 4.73 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 35.75 | 31.27 | 32.91 | 33.69 | 3/16 | - | 5.52 | 14.95 | 5.49 | 3/16
    | 0.00 | 0.00 | 0.00 | - | 3/16 | 20.22 | 17.97 | 25.72 | 4.73 |'
- en: '| 1-shot | 4/16 | 43.25 | 43.18 | 43.39 | 42.56 | 3-shot | 4/16 | - | 45.03
    | 45.44 | 43.77 | 3-shot | 4/16 | 40.02 | 39.40 | 38.23 | - | 2-shot | 4/16 |
    62.36 | 62.46 | 65.03 | 64.67 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/16 | 43.25 | 43.18 | 43.39 | 42.56 | 3-shot | 4/16 | - | 45.03
    | 45.44 | 43.77 | 3-shot | 4/16 | 40.02 | 39.40 | 38.23 | - | 2-shot | 4/16 |
    62.36 | 62.46 | 65.03 | 64.67 |'
- en: '| 3/16 | 41.02 | 40.50 | 41.27 | 41.26 | 3/16 | - | 38.53 | 55.02 | 44.50 |
    3/16 | 37.11 | 44.38 | 37.17 | - | 3/16 | 61.85 | 63.03 | 61.88 | 61.79 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 41.02 | 40.50 | 41.27 | 41.26 | 3/16 | - | 38.53 | 55.02 | 44.50 |
    3/16 | 37.11 | 44.38 | 37.17 | - | 3/16 | 61.85 | 63.03 | 61.88 | 61.79 |'
- en: '| SQA | 0-shot | 4/16 | 43.83 | 43.07 | 44.32 | 44.20 | SST | 0-shot | 4/16
    | - | 2.09 | 11.47 | 19.17 | CN | 0-shot | 4/16 | 3.35 | 1.56 | 3.41 | - | TG
    | 0-shot | 4/16 | 49.38 | 52.5 | 40.31 | 36.56 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| SQA | 0-shot | 4/16 | 43.83 | 43.07 | 44.32 | 44.20 | SST | 0-shot | 4/16
    | - | 2.09 | 11.47 | 19.17 | CN | 0-shot | 4/16 | 3.35 | 1.56 | 3.41 | - | TG
    | 0-shot | 4/16 | 49.38 | 52.5 | 40.31 | 36.56 |'
- en: '| 3/16 | 35.10 | 29.62 | 29.55 | 32.07 | 3/16 | - | 3.39 | 30.77 | 8.21 | 3/16
    | 3.07 | 0.06 | 1.79 | - | 3/16 | 26.72 | 20.00 | 37.03 | 8.44 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 35.10 | 29.62 | 29.55 | 32.07 | 3/16 | - | 3.39 | 30.77 | 8.21 | 3/16
    | 3.07 | 0.06 | 1.79 | - | 3/16 | 26.72 | 20.00 | 37.03 | 8.44 |'
- en: '| 1-shot | 4/16 | 48.12 | 48.39 | 49.37 | 47.24 | 3-shot | 4/16 | - | 58.28
    | 58.80 | 51.76 | 3-shot | 4/16 | 33.84 | 34.51 | 33.28 | - | 2-shot | 4/16 |
    65.31 | 65.47 | 71.25 | 75.16 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/16 | 48.12 | 48.39 | 49.37 | 47.24 | 3-shot | 4/16 | - | 58.28
    | 58.80 | 51.76 | 3-shot | 4/16 | 33.84 | 34.51 | 33.28 | - | 2-shot | 4/16 |
    65.31 | 65.47 | 71.25 | 75.16 |'
- en: '| 3/16 | 40.48 | 39.14 | 39.84 | 43.61 | 3/16 | - | 57.11 | 64.93 | 65.84 |
    3/16 | 28.14 | 33.61 | 29.87 | - | 3/16 | 68.75 | 74.22 | 63.91 | 67.66 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 40.48 | 39.14 | 39.84 | 43.61 | 3/16 | - | 57.11 | 64.93 | 65.84 |
    3/16 | 28.14 | 33.61 | 29.87 | - | 3/16 | 68.75 | 74.22 | 63.91 | 67.66 |'
- en: '| SQ | Test | Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. | Test | Gene.
    | W/A | Calib. | Test | Gene. | W/A | Calib. |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| SQ | Test | Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. | Test | Gene.
    | W/A | Calib. | Test | Gene. | W/A | Calib. |'
- en: '| SQ | AQA | NQA | SQA | AZ | DS | SE | SST | MN | AN | WN | CN | CC | AC |
    IH | TG |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| SQ | AQA | NQA | SQA | AZ | DS | SE | SST | MN | AN | WN | CN | CC | AC |
    IH | TG |'
- en: '| SQ | 0-shot | 4/8 | 35.34 | 39.17 | 40.12 | 40.64 | AZ | 0-shot | 4/8 | 0.00
    | 0.87 | 0.00 | 0.01 | MN | 0-shot | 4/8 | 0.01 | 0.00 | 0.00 | - | CC | 0-shot
    | 4/8 | 0.03 | 0.00 | 0.00 | 0.00 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| SQ | 0-shot | 4/8 | 35.34 | 39.17 | 40.12 | 40.64 | AZ | 0-shot | 4/8 | 0.00
    | 0.87 | 0.00 | 0.01 | MN | 0-shot | 4/8 | 0.01 | 0.00 | 0.00 | - | CC | 0-shot
    | 4/8 | 0.03 | 0.00 | 0.00 | 0.00 |'
- en: '| 3/8 | 0.01 | 0.01 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.01 | 0.00 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 3/8 | 0.01 | 0.01 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.01 | 0.00 |'
- en: '| 1-shot | 4/8 | 28.01 | 56.13 | 55.12 | 56.59 | 3-shot | 4/8 | 54.76 | 88.36
    | 85.90 | 84.85 | 3-shot | 4/8 | 31.70 | 32.86 | 34.54 | - | 2-shot | 4/8 | 90.39
    | 65.29 | 65.08 | 87.63 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/8 | 28.01 | 56.13 | 55.12 | 56.59 | 3-shot | 4/8 | 54.76 | 88.36
    | 85.90 | 84.85 | 3-shot | 4/8 | 31.70 | 32.86 | 34.54 | - | 2-shot | 4/8 | 90.39
    | 65.29 | 65.08 | 87.63 |'
- en: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
- en: '| AQA | 0-shot | 4/8 | 14.22 | 18.10 | 18.18 | 18.17 | DS | 0-shot | 4/8 |
    0.00 | 0.02 | 0.00 | 0.00 | AN | 0-shot | 4/8 | 0.03 | 0.00 | 0.00 | - | AC |
    0-shot | 4/8 | 0.19 | 0.19 | 0.19 | 0.00 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| AQA | 0-shot | 4/8 | 14.22 | 18.10 | 18.18 | 18.17 | DS | 0-shot | 4/8 |
    0.00 | 0.02 | 0.00 | 0.00 | AN | 0-shot | 4/8 | 0.03 | 0.00 | 0.00 | - | AC |
    0-shot | 4/8 | 0.19 | 0.19 | 0.19 | 0.00 |'
- en: '| 3/8 | 0.01 | 0.00 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 3/8 | 0.01 | 0.00 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
- en: '| 1-shot | 4/8 | 28.01 | 28.91 | 27.96 | 29.13 | 3-shot | 4/8 | 47.01 | 50.42
    | 50.00 | 34.88 | 3-shot | 4/8 | 32.97 | 33.93 | 33.14 | - | 2-shot | 4/8 | 18.16
    | 23.71 | 53.15 | 23.33 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/8 | 28.01 | 28.91 | 27.96 | 29.13 | 3-shot | 4/8 | 47.01 | 50.42
    | 50.00 | 34.88 | 3-shot | 4/8 | 32.97 | 33.93 | 33.14 | - | 2-shot | 4/8 | 18.16
    | 23.71 | 53.15 | 23.33 |'
- en: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
- en: '| NQA | 0-shot | 4/8 | 24.26 | 27.83 | 23.95 | 24.07 | SE | 0-shot | 4/8 |
    0.00 | 0.00 | 0.00 | 0.01 | WN | 0-shot | 4/8 | 0.00 | 0.00 | 0.00 | - | IH |
    0-shot | 4/8 | 0.09 | 0.01 | 0.00 | 0.04 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| NQA | 0-shot | 4/8 | 24.26 | 27.83 | 23.95 | 24.07 | SE | 0-shot | 4/8 |
    0.00 | 0.00 | 0.00 | 0.01 | WN | 0-shot | 4/8 | 0.00 | 0.00 | 0.00 | - | IH |
    0-shot | 4/8 | 0.09 | 0.01 | 0.00 | 0.04 |'
- en: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.02 | 0.00 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.02 | 0.00 |'
- en: '| 1-shot | 4/8 | 30.69 | 32.16 | 29.83 | 33.18 | 3-shot | 4/8 | 47.03 | 34.43
    | 43.43 | 34.27 | 3-shot | 4/8 | 47.32 | 46.70 | 47.15 | - | 2-shot | 4/8 | 61.87
    | 60.73 | 59.28 | 57.42 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/8 | 30.69 | 32.16 | 29.83 | 33.18 | 3-shot | 4/8 | 47.03 | 34.43
    | 43.43 | 34.27 | 3-shot | 4/8 | 47.32 | 46.70 | 47.15 | - | 2-shot | 4/8 | 61.87
    | 60.73 | 59.28 | 57.42 |'
- en: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
- en: '| SQA | 0-shot | 4/8 | 19.92 | 20.30 | 19.07 | 18.07 | SST | 0-shot | 4/8 |
    0.00 | 0.00 | 0.00 | 0.00 | CN | 0-shot | 4/8 | 0.00 | 0.00 | 0.00 | - | TG |
    0-shot | 4/8 | 0.63 | 0.00 | 0.16 | 0.00 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| SQA | 0-shot | 4/8 | 19.92 | 20.30 | 19.07 | 18.07 | SST | 0-shot | 4/8 |
    0.00 | 0.00 | 0.00 | 0.00 | CN | 0-shot | 4/8 | 0.00 | 0.00 | 0.00 | - | TG |
    0-shot | 4/8 | 0.63 | 0.00 | 0.16 | 0.00 |'
- en: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.31 | 0.00 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.01 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.31 | 0.00 |'
- en: '| 1-shot | 4/8 | 25.64 | 17.73 | 21.70 | 21.10 | 3-shot | 4/8 | 26.47 | 53.06
    | 55.02 | 36.90 | 3-shot | 4/8 | 26.02 | 27.19 | 14.91 | - | 2-shot | 4/8 | 58.28
    | 65.31 | 59.06 | 59.38 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 1-shot | 4/8 | 25.64 | 17.73 | 21.70 | 21.10 | 3-shot | 4/8 | 26.47 | 53.06
    | 55.02 | 36.90 | 3-shot | 4/8 | 26.02 | 27.19 | 14.91 | - | 2-shot | 4/8 | 58.28
    | 65.31 | 59.06 | 59.38 |'
- en: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 | 3/8 |
    0.00 | 0.00 | 0.00 | - | 3/8 | 0.00 | 0.00 | 0.00 | 0.00 |'
- en: We evaluate *cross-dataset* distribution shift experiments on the OOD benchmark
    BOSS ([69](#bib.bib69)) in NLP. Previous work in NLP concerning OOD mostly considers
    distribution shifts from various sources, e.g. from movies to Twitter ([68](#bib.bib68)).
    GLUE-X ([65](#bib.bib65)) and BOSS ([69](#bib.bib69)) represent pioneering efforts
    in benchmarking OOD generalization in NLP. BOSS, building upon GLUE-X, improves
    by employing SimCSE scores for detection analysis and identifying dataset pairs
    exhibiting the lowest semantic similarity. These pairs are then utilized for training
    and testing, constructing a benchmark consisting of five downstream tasks. Each
    downstream task comprises an in-domain (ID) dataset and three OOD datasets.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 NLP 的 OOD 基准 BOSS ([69](#bib.bib69)) 上评估了*跨数据集*分布迁移实验。NLP 中以前关于 OOD 的工作主要考虑了来自各种来源的分布迁移，例如从电影到
    Twitter ([68](#bib.bib68))。GLUE-X ([65](#bib.bib65)) 和 BOSS ([69](#bib.bib69))
    代表了在 NLP 中对 OOD 泛化的开创性努力。BOSS 在 GLUE-X 的基础上，通过采用 SimCSE 分数进行检测分析并识别语义相似度最低的数据集对，从而改进了检测。这些对被用于训练和测试，构建了一个包含五个下游任务的基准。每个下游任务包含一个领域内
    (ID) 数据集和三个 OOD 数据集。
- en: 'To evaluate the generalization ability of quantized models in cross-dataset
    distribution shift experiments, we randomly sample 300 samples from the test set
    of each OOD dataset within the BOSS benchmark as its corresponding training set,
    serving as the calibration set for the quantization process. For each downstream
    task, we utilize the training set from different datasets as the calibration set
    for the quantization process and test on the corresponding I.I.D and OOD test
    sets. In our experiments, we employ LLaMA2-7B ([54](#bib.bib54)) as the target
    for quantization and selected four PTQ methods: GPTQ ([14](#bib.bib14)), AWQ ([31](#bib.bib31)),
    SpQR ([10](#bib.bib10)), and SmoothQuant ([62](#bib.bib62)). Given that there
    is not much difference in performance between excessively high bits and full precision,
    and too low a bit has already lost basic performance in these tasks, we quantize
    the model weights to 3-4 bits with SmoothQuant quantizing the activations to 8
    bits. We test two forms: 0-shot and few-shot.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估量化模型在跨数据集分布迁移实验中的泛化能力，我们从BOSS基准测试中每个OOD数据集的测试集中随机抽取300个样本作为其对应的训练集，作为量化过程的校准集。对于每个下游任务，我们利用来自不同数据集的训练集作为量化过程的校准集，并在相应的I.I.D和OOD测试集上进行测试。在我们的实验中，我们采用LLaMA2-7B
    ([54](#bib.bib54))作为量化目标，并选择了四种PTQ方法：GPTQ ([14](#bib.bib14))、AWQ ([31](#bib.bib31))、SpQR
    ([10](#bib.bib10))和SmoothQuant ([62](#bib.bib62))。由于过高的位数与全精度之间的性能差异不大，而过低的位数已失去这些任务的基本性能，我们将模型权重量化为3-4位，SmoothQuant将激活量化为8位。我们测试了两种形式：0-shot和few-shot。
- en: 'We present the results in Tab. [2](#S3.T2 "Table 2 ‣ 3 S2: Generalization Assessment
    of Quantized LLMs with Domain Shifts ‣ Evaluating the Generalization Ability of
    Quantized LLMs: Benchmark, Analysis, and Toolbox"). We evaluate four downstream
    tasks in BOSS: EQA, SA, NLI, and TD. Each downstream task consists of four datasets,
    with each dataset tested using four datasets as calibration set. The following
    conclusions can be observed:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格[2](#S3.T2 "Table 2 ‣ 3 S2: Generalization Assessment of Quantized LLMs
    with Domain Shifts ‣ Evaluating the Generalization Ability of Quantized LLMs:
    Benchmark, Analysis, and Toolbox")中展示了结果。我们在BOSS中评估了四个下游任务：EQA、SA、NLI和TD。每个下游任务由四个数据集组成，每个数据集使用四个数据集作为校准集进行测试。可以观察到以下结论：'
- en: For datasets with poor performance or even close to zero, few-shot learning
    significantly improves the performance. For the EQA task with both 4-bit and 3-bit
    quantization and the SA task with 4-bit quantization, where satisfactory performance
    can be achieved, there is a relatively slight improvement with few-shot learning
    compared to 0-shot. However, for the SA task with 3-bit quantization, the NLI
    task with both 4-bit and 3-bit quantization and the TD task with 4-bit and 3-bit
    quantization with poor performance, few-shot learning shows a qualitative leap
    compared to 0-shot. Especially on some datasets where the 0-shot performance is
    nearly zero, few-shot learning achieves accuracy ranging from 80% to 90%. This
    indicates that LLM can benefit from the examples provided to solve complex tasks,
    yielding significant improvements ([73](#bib.bib73)). However, for quantized models
    with severe performance degradation, such as those quantized to 3 bits using SmoothQuant,
    few-shot learning cannot improve the quantized model’s performance.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于性能较差甚至接近于零的数据集，few-shot学习显著提升了性能。对于4位和3位量化的EQA任务，以及4位量化的SA任务，虽然可以达到令人满意的性能，但与0-shot相比，few-shot学习的提升相对较小。然而，对于3位量化的SA任务、4位和3位量化的NLI任务以及4位和3位量化的TD任务，few-shot学习相比0-shot表现出了质的飞跃。尤其是在一些0-shot性能接近于零的数据集上，few-shot学习达到了80%到90%的准确率。这表明LLM可以从提供的示例中获益以解决复杂任务，获得显著的改进
    ([73](#bib.bib73))。然而，对于性能严重退化的量化模型，例如使用SmoothQuant量化为3位的模型，few-shot学习不能改善量化模型的性能。
- en: For the same test dataset, it’s not necessarily the case that using I.I.D dataset
    as calibration set yield superior performance; rather, there exist one or more
    datasets that demonstrate enhanced performance when used as calibration set. Across
    the same test dataset, the variance in performance when using different datasets
    as calibration set can be substantial, differing by as much as 70%. Counterintuitively,
    the overlap between background-colored and bolded data is not high, indicating
    that using I.I.D datasets as calibration sets does not necessarily result in higher
    performance. Instead, for each task, there are one or more datasets for which
    performance improves when used as the calibration set, and this characteristic
    is method-dependent. For EQA task, when quantized using the GPTQ and SpQR, the
    performance using NQA and SQA as calibration set generally exceeds that of SQ
    and AQA. For SA task, when quantized using the GPTQ method, performance significantly
    improves when using AZ and SST as calibration set compared to SE and DS. For NLI
    task, all methods maintain decent performance when using the MN dataset as the
    calibration set. For TD task, when quantified using the GPTQ method, performance
    consistently outperforms other datasets when TG is used as the calibration set.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于相同的测试数据集，使用I.I.D数据集作为校准集并不一定能获得更优的性能；相反，存在一个或多个数据集在作为校准集时表现更好。在相同测试数据集下，使用不同数据集作为校准集时，性能差异可能很大，最高可达70%。违反直觉的是，背景颜色标记的数据与粗体数据的重叠并不高，表明使用I.I.D数据集作为校准集并不一定能获得更高的性能。相反，对于每个任务，存在一个或多个数据集，在作为校准集时性能有所提高，这一特性依赖于方法。对于EQA任务，当使用GPTQ和SpQR进行量化时，使用NQA和SQA作为校准集的性能通常优于SQ和AQA。对于SA任务，当使用GPTQ方法进行量化时，使用AZ和SST作为校准集的性能显著优于SE和DS。对于NLI任务，当使用MN数据集作为校准集时，所有方法都保持了较好的性能。对于TD任务，当使用GPTQ方法进行量化时，使用TG作为校准集的性能始终优于其他数据集。
- en: 'Table 3: Cross-dataset distribution shift in Chinese domain specific task.
    To save space, abbreviations are used for datasets. Each row presents the 0-shot
    and 5-shot experimental results using different datasets as calibration sets on
    the same test dataset. Results with colored backgrounds indicate I.I.D results,
    while those without color represent OOD results. The higher the metric, the better
    the performance. Bold results indicate the best performance on the same test dataset.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 中文领域特定任务中的跨数据集分布偏移。为了节省空间，数据集使用缩写。每一行展示了在相同测试数据集上，使用不同数据集作为校准集的0-shot和5-shot实验结果。背景颜色标记的结果表示I.I.D结果，而没有颜色的结果表示OOD结果。指标越高，性能越好。粗体结果表示在相同测试数据集上的最佳性能。'
- en: '| Method |  |  | 0-shot | 5-shot |  |  | 0-shot | 5-shot |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |  |  | 0-shot | 5-shot |  |  | 0-shot | 5-shot |'
- en: '| GPTQ | Test | W/A | Calib. | Test | W/A | Calib. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 测试 | W/A | 校准 | 测试 | W/A | 校准 |'
- en: '| CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM |'
- en: '| CE-HM | 4/16 | 39.4 | 37.9 | 53.2 | 52.1 | CM-HM | 4/16 | 50.0 | 50.7 | 59.1
    | 59.1 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| CE-HM | 4/16 | 39.4 | 37.9 | 53.2 | 52.1 | CM-HM | 4/16 | 50.0 | 50.7 | 59.1
    | 59.1 |'
- en: '| 3/16 | 30.0 | 28.0 | 38.1 | 41.9 | 3/16 | 32.3 | 30.6 | 52.4 | 54.4 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 30.0 | 28.0 | 38.1 | 41.9 | 3/16 | 32.3 | 30.6 | 52.4 | 54.4 |'
- en: '| 2/16 | 25.1 | 24.4 | 23.9 | 23.4 | 2/16 | 25.3 | 23.7 | 25.9 | 24.4 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 25.1 | 24.4 | 23.9 | 23.4 | 2/16 | 25.3 | 23.7 | 25.9 | 24.4 |'
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | W/A | 校准 | 测试 | W/A | 校准 |'
- en: '| CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS |'
- en: '| CE-SS | 4/16 | 36.9 | 35.4 | 58.8 | 57.5 | CM-SS | 4/16 | 53.9 | 54.0 | 63.1
    | 63.8 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| CE-SS | 4/16 | 36.9 | 35.4 | 58.8 | 57.5 | CM-SS | 4/16 | 53.9 | 54.0 | 63.1
    | 63.8 |'
- en: '| 3/16 | 34.6 | 30.3 | 51.9 | 47.5 | 3/16 | 32.8 | 34.3 | 55.4 | 54.6 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 34.6 | 30.3 | 51.9 | 47.5 | 3/16 | 32.8 | 34.3 | 55.4 | 54.6 |'
- en: '| 2/16 | 25.1 | 23.9 | 25.9 | 24.7 | 2/16 | 25.7 | 26.2 | 25.6 | 25.3 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 25.1 | 23.9 | 25.9 | 24.7 | 2/16 | 25.7 | 26.2 | 25.6 | 25.3 |'
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | W/A | 校准 | 测试 | W/A | 校准 |'
- en: '| CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST |'
- en: '| CE-ST | 4/16 | 30.4 | 26.0 | 41.8 | 39.2 | CM-ST | 4/16 | 39.3 | 35.2 | 43.1
    | 43.8 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| CE-ST | 4/16 | 30.4 | 26.0 | 41.8 | 39.2 | CM-ST | 4/16 | 39.3 | 35.2 | 43.1
    | 43.8 |'
- en: '| 3/16 | 28.1 | 25.7 | 33.9 | 35.5 | 3/16 | 29.9 | 25.7 | 38.6 | 37.7 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 28.1 | 25.7 | 33.9 | 35.5 | 3/16 | 29.9 | 25.7 | 38.6 | 37.7 |'
- en: '| 2/16 | 24.6 | 25.4 | 24.5 | 25.0 | 2/16 | 26.2 | 25.7 | 24.5 | 25.2 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 24.6 | 25.4 | 24.5 | 25.0 | 2/16 | 26.2 | 25.7 | 24.5 | 25.2 |'
- en: '| SpQR | Test | W/A | Calib. | Test | W/A | Calib. |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 测试 | W/A | 校准 | 测试 | W/A | 校准 |'
- en: '| CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM |'
- en: '| CE-HM | 4/16 | 38.5 | 36.3 | 53.8 | 52.5 | CM-HM | 4/16 | 52.9 | 49.3 | 59.0
    | 59.5 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| CE-HM | 4/16 | 38.5 | 36.3 | 53.8 | 52.5 | CM-HM | 4/16 | 52.9 | 49.3 | 59.0
    | 59.5 |'
- en: '| 3/16 | 36.0 | 34.6 | 47.9 | 46.6 | 3/16 | 49.5 | 38.1 | 57.1 | 56.9 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 36.0 | 34.6 | 47.9 | 46.6 | 3/16 | 49.5 | 38.1 | 57.1 | 56.9 |'
- en: '| 2/16 | 30.1 | 30.9 | 37.4 | 34.5 | 2/16 | 39.3 | 26.0 | 47.5 | 46.3 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 30.1 | 30.9 | 37.4 | 34.5 | 2/16 | 39.3 | 26.0 | 47.5 | 46.3 |'
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | W/A | 校准 | 测试 | W/A | 校准 |'
- en: '| CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS |'
- en: '| CE-SS | 4/16 | 38.2 | 38.9 | 60.0 | 57.7 | CM-SS | 4/16 | 54.8 | 54.3 | 63.8
    | 64.7 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| CE-SS | 4/16 | 38.2 | 38.9 | 60.0 | 57.7 | CM-SS | 4/16 | 54.8 | 54.3 | 63.8
    | 64.7 |'
- en: '| 3/16 | 39.8 | 34.7 | 56.1 | 53.3 | 3/16 | 52.8 | 51.1 | 59.4 | 60.2 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 39.8 | 34.7 | 56.1 | 53.3 | 3/16 | 52.8 | 51.1 | 59.4 | 60.2 |'
- en: '| 2/16 | 30.1 | 32.1 | 39.5 | 37.3 | 2/16 | 38.8 | 39.7 | 44.2 | 47.1 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 30.1 | 32.1 | 39.5 | 37.3 | 2/16 | 38.8 | 39.7 | 44.2 | 47.1 |'
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | W/A | 校准 | 测试 | W/A | 校准 |'
- en: '| CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST |'
- en: '| CE-ST | 4/16 | 32.2 | 30.3 | 41.5 | 41.1 | CM-ST | 4/16 | 40.4 | 39.5 | 43.7
    | 43.3 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| CE-ST | 4/16 | 32.2 | 30.3 | 41.5 | 41.1 | CM-ST | 4/16 | 40.4 | 39.5 | 43.7
    | 43.3 |'
- en: '| 3/16 | 31.1 | 28.4 | 37.5 | 37.8 | 3/16 | 37.4 | 37.8 | 40.8 | 41.4 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 31.1 | 28.4 | 37.5 | 37.8 | 3/16 | 37.4 | 37.8 | 40.8 | 41.4 |'
- en: '| 2/16 | 27.8 | 27.7 | 32.2 | 30.6 | 2/16 | 31.8 | 31.9 | 35.9 | 35.6 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 27.8 | 27.7 | 32.2 | 30.6 | 2/16 | 31.8 | 31.9 | 35.9 | 35.6 |'
- en: '| AWQ | Test | W/A | Calib. | Test | W/A | Calib. |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 测试 | W/A | 校准 | 测试 | W/A | 校准 |'
- en: '| CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM |'
- en: '| CE-HM | 4/16 | 36.5 | 35.6 | 47.7 | 49.0 | CM-HM | 4/16 | 47.8 | 53.2 | 58.5
    | 58.2 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| CE-HM | 4/16 | 36.5 | 35.6 | 47.7 | 49.0 | CM-HM | 4/16 | 47.8 | 53.2 | 58.5
    | 58.2 |'
- en: '| 3/16 | 26.7 | 29.7 | 41.1 | 40.8 | 3/16 | 42.6 | 50.5 | 48.0 | 49.5 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 26.7 | 29.7 | 41.1 | 40.8 | 3/16 | 42.6 | 50.5 | 48.0 | 49.5 |'
- en: '| 2/16 | 24.2 | 24.3 | 24.0 | 23.3 | 2/16 | 25.9 | 42.4 | 25.8 | 23.4 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 24.2 | 24.3 | 24.0 | 23.3 | 2/16 | 25.9 | 42.4 | 25.8 | 23.4 |'
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | W/A | 校准 | 测试 | W/A | 校准 |'
- en: '| CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS |'
- en: '| CE-SS | 4/16 | 32.2 | 34.9 | 57.5 | 56.7 | CM-SS | 4/16 | 51.3 | 52.4 | 62.2
    | 61.4 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| CE-SS | 4/16 | 32.2 | 34.9 | 57.5 | 56.7 | CM-SS | 4/16 | 51.3 | 52.4 | 62.2
    | 61.4 |'
- en: '| 3/16 | 32.6 | 31.5 | 42.7 | 40.5 | 3/16 | 40.1 | 42.1 | 50.5 | 50.8 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 32.6 | 31.5 | 42.7 | 40.5 | 3/16 | 40.1 | 42.1 | 50.5 | 50.8 |'
- en: '| 2/16 | 24.8 | 25.0 | 24.9 | 25.7 | 2/16 | 24.8 | 24.9 | 24.8 | 24.7 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 24.8 | 25.0 | 24.9 | 25.7 | 2/16 | 24.8 | 24.9 | 24.8 | 24.7 |'
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | W/A | 校准 | 测试 | W/A | 校准 |'
- en: '| CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST |'
- en: '| CE-ST | 4/16 | 26.6 | 29.4 | 39.1 | 38.6 | CM-ST | 4/16 | 36.7 | 35.3 | 41.0
    | 41.6 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| CE-ST | 4/16 | 26.6 | 29.4 | 39.1 | 38.6 | CM-ST | 4/16 | 36.7 | 35.3 | 41.0
    | 41.6 |'
- en: '| 3/16 | 26.2 | 27.1 | 31.9 | 34.0 | 3/16 | 31.7 | 31.7 | 36.3 | 35.5 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 26.2 | 27.1 | 31.9 | 34.0 | 3/16 | 31.7 | 31.7 | 36.3 | 35.5 |'
- en: '| 2/16 | 25.1 | 24.9 | 25.7 | 25.2 | 2/16 | 24.6 | 24.6 | 24.1 | 24.5 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 25.1 | 24.9 | 25.7 | 25.2 | 2/16 | 24.6 | 24.6 | 24.1 | 24.5 |'
- en: '| SQ | Test | W/A | Calib. | Test | W/A | Calib. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| SQ | 测试 | W/A | 校准 | 测试 | W/A | 校准 |'
- en: '| CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM | CE-HM | CM-HM |'
- en: '| CE-HM | 4/8 | 27.2 | 27.2 | 24.7 | 24.5 | CM-HM | 4/8 | 31.6 | 29.8 | 29.4
    | 27.1 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| CE-HM | 4/8 | 27.2 | 27.2 | 24.7 | 24.5 | CM-HM | 4/8 | 31.6 | 29.8 | 29.4
    | 27.1 |'
- en: '| 3/8 | 25.5 | 25.5 | 24.9 | 23.9 | 3/8 | 24.7 | 24.8 | 25.3 | 23.9 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 3/8 | 25.5 | 25.5 | 24.9 | 23.9 | 3/8 | 24.7 | 24.8 | 25.3 | 23.9 |'
- en: '| 2/8 | 27.1 | 24.2 | 25.5 | 24.2 | 2/8 | 24.1 | 25.5 | 24.8 | 25.3 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 2/8 | 27.1 | 24.2 | 25.5 | 24.2 | 2/8 | 24.1 | 25.5 | 24.8 | 25.3 |'
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | W/A | 校准 | 测试 | W/A | 校准 |'
- en: '| CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS | CE-SS | CM-SS |'
- en: '| CE-SS | 4/8 | 27.4 | 26.7 | 24.4 | 24.5 | CM-SS | 4/8 | 33.1 | 28.2 | 28.7
    | 25.8 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| CE-SS | 4/8 | 27.4 | 26.7 | 24.4 | 24.5 | CM-SS | 4/8 | 33.1 | 28.2 | 28.7
    | 25.8 |'
- en: '| 3/8 | 26.1 | 25.0 | 26.2 | 24.4 | 3/8 | 25.0 | 25.1 | 24.7 | 24.6 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 3/8 | 26.1 | 25.0 | 26.2 | 24.4 | 3/8 | 25.0 | 25.1 | 24.7 | 24.6 |'
- en: '| 2/8 | 26.6 | 25.1 | 25.3 | 23.3 | 2/8 | 24.3 | 25.3 | 25.2 | 25.3 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 2/8 | 26.6 | 25.1 | 25.3 | 23.3 | 2/8 | 24.3 | 25.3 | 25.2 | 25.3 |'
- en: '| Test | W/A | Calib. | Test | W/A | Calib. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | W/A | 校准 | 测试 | W/A | 校准 |'
- en: '| CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST | CE-ST | CM-ST |'
- en: '| CE-ST | 4/8 | 32.2 | 26.2 | 25.5 | 23.9 | CM-ST | 4/8 | 28.2 | 27.7 | 26.9
    | 43.3 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| CE-ST | 4/8 | 32.2 | 26.2 | 25.5 | 23.9 | CM-ST | 4/8 | 28.2 | 27.7 | 26.9
    | 43.3 |'
- en: '| 3/8 | 31.1 | 27.4 | 24.8 | 25.6 | 3/8 | 25.4 | 24.2 | 24.4 | 41.4 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 3/8 | 31.1 | 27.4 | 24.8 | 25.6 | 3/8 | 25.4 | 24.2 | 24.4 | 41.4 |'
- en: '| 2/8 | 27.8 | 26.8 | 24.9 | 26.8 | 2/8 | 24.8 | 24.9 | 24.6 | 35.6 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 2/8 | 27.8 | 26.8 | 24.9 | 26.8 | 2/8 | 24.8 | 24.9 | 24.6 | 35.6 |'
- en: 'Table 4: Cross-subject distribution shift in Chinese domain-specific task.
    To save space, abbreviations are used for datasets. Each row presents the experimental
    results using different datasets as calibration sets on the same test dataset.
    Results with colored backgrounds indicate I.I.D results, while those without color
    represent OOD results. The higher the metric, the better the performance. Bold
    results indicate the best performance on the same test set.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：中文领域特定任务中的跨主题分布变化。为节省空间，数据集使用缩写。每行展示了在相同测试数据集上使用不同数据集作为校准集的实验结果。带有背景色的结果表示
    I.I.D 结果，而没有颜色的结果表示 OOD 结果。指标越高，性能越好。粗体结果表示在相同测试集上最佳性能。
- en: '| Meth. | Test | Gene. | W/A | Gene. | Test | Gene. | W/A | Gene. | Test |
    Gene. | W/A | Gene. |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 测试 | 基因 | W/A | 基因 | 测试 | 基因 | W/A | 基因 | 测试 | 基因 | W/A | 基因 |'
- en: '| HM | SS | ST | HM | SS | ST | HM | SS | ST |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| HM | SS | ST | HM | SS | ST | HM | SS | ST |'
- en: '| GPTQ | HM | 0-shot | 4/16 | 39.4 | 36.4 | 37.6 | SS | 0-shot | 4/16 | 38.8
    | 36.9 | 38.9 | ST | 0-shot | 4/16 | 30.4 | 28.4 | 30.4 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | HM | 0-shot | 4/16 | 39.4 | 36.4 | 37.6 | SS | 0-shot | 4/16 | 38.8
    | 36.9 | 38.9 | ST | 0-shot | 4/16 | 30.4 | 28.4 | 30.4 |'
- en: '| 3/16 | 30.0 | 30.5 | 29.2 | 3/16 | 29.6 | 34.6 | 30.4 | 3/16 | 25.9 | 28.3
    | 28.1 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 30.0 | 30.5 | 29.2 | 3/16 | 29.6 | 34.6 | 30.4 | 3/16 | 25.9 | 28.3
    | 28.1 |'
- en: '| 2/16 | 25.1 | 24.1 | 26.2 | 2/16 | 27.3 | 25.1 | 25.2 | 2/16 | 24.9 | 24.8
    | 24.6 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 25.1 | 24.1 | 26.2 | 2/16 | 27.3 | 25.1 | 25.2 | 2/16 | 24.9 | 24.8
    | 24.6 |'
- en: '| 5-shot | 4/16 | 53.2 | 52.9 | 52.2 | 5-shot | 4/16 | 58.9 | 58.8 | 60.1 |
    5-shot | 4/16 | 40.9 | 40.4 | 41.8 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 5-shot | 4/16 | 53.2 | 52.9 | 52.2 | 5-shot | 4/16 | 58.9 | 58.8 | 60.1 |
    5-shot | 4/16 | 40.9 | 40.4 | 41.8 |'
- en: '| 3/16 | 38.1 | 43.5 | 39.9 | 3/16 | 42.5 | 51.9 | 48.2 | 3/16 | 29.7 | 34.1
    | 33.9 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 38.1 | 43.5 | 39.9 | 3/16 | 42.5 | 51.9 | 48.2 | 3/16 | 29.7 | 34.1
    | 33.9 |'
- en: '| 2/16 | 23.9 | 26.2 | 23.7 | 2/16 | 24.3 | 25.9 | 24.6 | 2/16 | 27.3 | 25.1
    | 24.5 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 23.9 | 26.2 | 23.7 | 2/16 | 24.3 | 25.9 | 24.6 | 2/16 | 27.3 | 25.1
    | 24.5 |'
- en: '| SpQR | HM | 0-shot | 4/16 | 38.5 | 38.0 | 40.9 | SS | 0-shot | 4/16 | 39.3
    | 38.2 | 41.3 | ST | 0-shot | 4/16 | 30.3 | 29.9 | 32.2 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | HM | 0-shot | 4/16 | 38.5 | 38.0 | 40.9 | SS | 0-shot | 4/16 | 39.3
    | 38.2 | 41.3 | ST | 0-shot | 4/16 | 30.3 | 29.9 | 32.2 |'
- en: '| 3/16 | 36.0 | 39.0 | 38.9 | 3/16 | 34.8 | 39.8 | 39.0 | 3/16 | 30.5 | 29.1
    | 31.1 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 36.0 | 39.0 | 38.9 | 3/16 | 34.8 | 39.8 | 39.0 | 3/16 | 30.5 | 29.1
    | 31.1 |'
- en: '| 2/16 | 30.1 | 29.9 | 29.2 | 2/16 | 28.7 | 30.1 | 30.6 | 2/16 | 26.1 | 26.6
    | 27.8 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 30.1 | 29.9 | 29.2 | 2/16 | 28.7 | 30.1 | 30.6 | 2/16 | 26.1 | 26.6
    | 27.8 |'
- en: '| 5-shot | 4/16 | 53.8 | 51.0 | 52.6 | 5-shot | 4/16 | 59.3 | 60.0 | 59.6 |
    5-shot | 4/16 | 41.4 | 41.0 | 41.5 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 5-shot | 4/16 | 53.8 | 51.0 | 52.6 | 5-shot | 4/16 | 59.3 | 60.0 | 59.6 |
    5-shot | 4/16 | 41.4 | 41.0 | 41.5 |'
- en: '| 3/16 | 47.9 | 45.8 | 46.5 | 3/16 | 52.8 | 56.1 | 53.0 | 3/16 | 36.5 | 37.6
    | 37.5 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 47.9 | 45.8 | 46.5 | 3/16 | 52.8 | 56.1 | 53.0 | 3/16 | 36.5 | 37.6
    | 37.5 |'
- en: '| 2/16 | 37.4 | 35.0 | 37.7 | 2/16 | 40.6 | 39.5 | 45.0 | 2/16 | 28.3 | 28.0
    | 32.2 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 37.4 | 35.0 | 37.7 | 2/16 | 40.6 | 39.5 | 45.0 | 2/16 | 28.3 | 28.0
    | 32.2 |'
- en: '| AWQ | HM | 0-shot | 4/16 | 36.5 | 34.2 | 33.4 | SS | 0-shot | 4/16 | 35.2
    | 32.2 | 31.4 | ST | 0-shot | 4/16 | 28.5 | 28.5 | 26.6 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | HM | 0-shot | 4/16 | 36.5 | 34.2 | 33.4 | SS | 0-shot | 4/16 | 35.2
    | 32.2 | 31.4 | ST | 0-shot | 4/16 | 28.5 | 28.5 | 26.6 |'
- en: '| 3/16 | 26.7 | 32.1 | 27.5 | 3/16 | 28.3 | 32.6 | 28.2 | 3/16 | 27.7 | 28.9
    | 26.2 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 26.7 | 32.1 | 27.5 | 3/16 | 28.3 | 32.6 | 28.2 | 3/16 | 27.7 | 28.9
    | 26.2 |'
- en: '| 2/16 | 24.2 | 24.2 | 24.6 | 2/16 | 24.9 | 24.8 | 25.2 | 2/16 | 24.9 | 24.8
    | 25.1 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 24.2 | 24.2 | 24.6 | 2/16 | 24.9 | 24.8 | 25.2 | 2/16 | 24.9 | 24.8
    | 25.1 |'
- en: '| 5-shot | 4/16 | 47.7 | 49.7 | 51.2 | 5-shot | 4/16 | 53.4 | 57.5 | 56.6 |
    5-shot | 4/16 | 37.7 | 38.5 | 39.1 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 5-shot | 4/16 | 47.7 | 49.7 | 51.2 | 5-shot | 4/16 | 53.4 | 57.5 | 56.6 |
    5-shot | 4/16 | 37.7 | 38.5 | 39.1 |'
- en: '| 3/16 | 41.1 | 38.4 | 37.4 | 3/16 | 44.0 | 42.7 | 38.7 | 3/16 | 31.9 | 31.0
    | 31.9 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 41.1 | 38.4 | 37.4 | 3/16 | 44.0 | 42.7 | 38.7 | 3/16 | 31.9 | 31.0
    | 31.9 |'
- en: '| 2/16 | 24.0 | 24.6 | 23.8 | 2/16 | 23.9 | 24.9 | 25.1 | 2/16 | 25.2 | 25.3
    | 25.7 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 2/16 | 24.0 | 24.6 | 23.8 | 2/16 | 23.9 | 24.9 | 25.1 | 2/16 | 25.2 | 25.3
    | 25.7 |'
- en: '| SQ | HM | 0-shot | 4/8 | 27.2 | 28.9 | 27.4 | SS | 0-shot | 4/8 | 28.3 |
    27.4 | 28.2 | ST | 0-shot | 4/8 | 26.8 | 28.0 | 25.4 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| SQ | HM | 0-shot | 4/8 | 27.2 | 28.9 | 27.4 | SS | 0-shot | 4/8 | 28.3 |
    27.4 | 28.2 | ST | 0-shot | 4/8 | 26.8 | 28.0 | 25.4 |'
- en: '| 3/8 | 25.5 | 23.9 | 26.4 | 3/8 | 26.4 | 26.1 | 25.5 | 3/8 | 26.6 | 25.2 |
    26.7 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 3/8 | 25.5 | 23.9 | 26.4 | 3/8 | 26.4 | 26.1 | 25.5 | 3/8 | 26.6 | 25.2 |
    26.7 |'
- en: '| 2/8 | 27.1 | 25.2 | 24.8 | 2/8 | 26.2 | 26.6 | 26.4 | 2/8 | 26.4 | 26.4 |
    25.7 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 2/8 | 27.1 | 25.2 | 24.8 | 2/8 | 26.2 | 26.6 | 26.4 | 2/8 | 26.4 | 26.4 |
    25.7 |'
- en: '| 5-shot | 4/8 | 24.7 | 24.2 | 24.9 | 5-shot | 4/8 | 26.0 | 24.4 | 24.3 | 5-shot
    | 4/8 | 24.8 | 24.3 | 25.5 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 5-shot | 4/8 | 24.7 | 24.2 | 24.9 | 5-shot | 4/8 | 26.0 | 24.4 | 24.3 | 5-shot
    | 4/8 | 24.8 | 24.3 | 25.5 |'
- en: '| 3/8 | 24.9 | 26.4 | 26.2 | 3/8 | 24.7 | 26.2 | 25.9 | 3/8 | 26.8 | 25.3 |
    24.8 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 3/8 | 24.9 | 26.4 | 26.2 | 3/8 | 24.7 | 26.2 | 25.9 | 3/8 | 26.8 | 25.3 |
    24.8 |'
- en: '| 2/8 | 25.5 | 26.4 | 24.2 | 2/8 | 26.5 | 25.3 | 24.9 | 2/8 | 26.6 | 26.8 |
    24.9 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 2/8 | 25.5 | 26.4 | 24.2 | 2/8 | 26.5 | 25.3 | 24.9 | 2/8 | 26.6 | 26.8 |
    24.9 |'
- en: Chinese Domain-specific Tasks. We evaluate *cross-dataset* distribution shift
    experiments and *cross-subject* distribution shift experiments on the Chinese
    domain-specific datasets C-EVAL ([20](#bib.bib20)) and CMMLU ([29](#bib.bib29)).
    C-EVAL serves as a comprehensive benchmark for evaluating Chinese LLM. It consists
    of 13,948 multiple-choice questions covering 52 different subjects categorized
    into Humanities, Social Sciences, STEM, and Other. CMMLU is another Chinese evaluation
    dataset designed specifically to assess the advanced knowledge and reasoning abilities
    of LLM in the context of the Chinese language and culture. It encompasses 67 different
    subjects categorized into Humanities, Social Sciences, STEM, and Chinese specific
    and others.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 中文领域特定任务。我们评估在中文领域特定数据集 C-EVAL ([20](#bib.bib20)) 和 CMMLU ([29](#bib.bib29))
    上的*跨数据集*分布偏移实验和*跨学科*分布偏移实验。C-EVAL 作为评估中文 LLM 的全面基准，包括 13,948 道多项选择题，涵盖人文学科、社会科学、STEM
    和其他 52 个不同学科。CMMLU 是另一个专门设计用于评估 LLM 在中文语言和文化背景下的高级知识和推理能力的中文评估数据集。它涵盖了 67 个不同的学科，分类为人文学科、社会科学、STEM
    和中文特定及其他。
- en: 'Both C-EVAL and CMMLU, two Chinese-specific domain datasets, include Humanities,
    Social Sciences, and STEM three subject categories. We design cross-dataset distribution
    shift experiments based on the same subject categories. For each subject test,
    we respectively utilize the corresponding subjects from C-EVAL and CMMLU as calibration
    set to assess the impact of different datasets as calibration set on the test
    results. Additionally, we conducted cross-subject distribution shift experiments
    on the C-EVAL dataset. For each subject test, we use Humanities, Social Sciences,
    and STEM as calibration set to evaluate the influence of different subject subsets
    as calibration set on the test results. Since both C-EVAL and CMMLU lack training
    datasets, we used the validation dataset of C-EVAL as the training dataset and
    randomly sampled 300 samples from the test dataset of CMMLU as the training dataset.
    We utilize the Chinese LLM Baichuan2-7B-Base ([64](#bib.bib64)) as the quantization
    target and selecte four PTQ methods: GPTQ ([14](#bib.bib14)), AWQ ([31](#bib.bib31)),
    SpQR ([10](#bib.bib10)), and SmoothQuant ([62](#bib.bib62)). We quantize the weights
    to 2-4 bits, with SmoothQuant quantizing the activations to 8 bits, and test both
    0-shot and 5-shot forms.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: C-EVAL 和 CMMLU 两个中文特定领域数据集都包含人文学科、社会科学和 STEM 三个学科类别。我们设计了基于相同学科类别的跨数据集分布偏移实验。对于每个学科测试，我们分别利用
    C-EVAL 和 CMMLU 中的相应学科作为校准集，以评估不同数据集作为校准集对测试结果的影响。此外，我们还在 C-EVAL 数据集上进行了跨学科分布偏移实验。对于每个学科测试，我们使用人文学科、社会科学和
    STEM 作为校准集，以评估不同学科子集作为校准集对测试结果的影响。由于 C-EVAL 和 CMMLU 都缺乏训练数据集，我们使用 C-EVAL 的验证数据集作为训练数据集，并从
    CMMLU 的测试数据集中随机抽取 300 个样本作为训练数据集。我们使用中文 LLM Baichuan2-7B-Base ([64](#bib.bib64))
    作为量化目标，并选择了四种 PTQ 方法：GPTQ ([14](#bib.bib14))、AWQ ([31](#bib.bib31))、SpQR ([10](#bib.bib10))
    和 SmoothQuant ([62](#bib.bib62))。我们将权重量化为 2-4 位，SmoothQuant 将激活量化为 8 位，并测试了 0-shot
    和 5-shot 两种形式。
- en: 'The results of cross-dataset distribution shift experiments on C-EVAL and CMMLU
    are presented in Tab. [3](#S3.T3 "Table 3 ‣ 3 S2: Generalization Assessment of
    Quantized LLMs with Domain Shifts ‣ Evaluating the Generalization Ability of Quantized
    LLMs: Benchmark, Analysis, and Toolbox"). We observe that performance generally
    improves when using I.I.D datasets as calibration set, while performance tends
    to degrade when using OOD datasets as calibration set. This contrasts with our
    findings in OOD Benchmark BOSS, suggesting that there is not a golden dataset
    that consistently improves test accuracy when used as a calibration set. The inconsistency
    in conclusions may stem from the fact that the distribution shift experiment in
    this setting is slightly more challenging compared to the distribution shift experiment
    tested on the BOSS dataset. The distribution differences among datasets in the
    BOSS benchmark are relatively small, so higher-quality datasets may result in
    higher accuracy for the quantized model. Additionally, the subjects included in
    the same subject category in C-EVAL and CMMLU are not entirely consistent, and
    the distribution differences within the same subject between the two datasets
    may be larger. In cases of greater distribution disparity, using I.I.D datasets
    as calibration set may lead to better performance.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '跨数据集分布偏移实验在C-EVAL和CMMLU上的结果展示在表格[3](#S3.T3 "表格 3 ‣ 3 S2: 量化LLMs的领域偏移的泛化评估 ‣
    评估量化LLMs的泛化能力：基准测试、分析和工具箱")中。我们观察到，当使用I.I.D数据集作为校准集时，性能通常会提高，而当使用OOD数据集作为校准集时，性能往往会下降。这与我们在OOD基准测试BOSS中的发现相反，表明没有一个黄金数据集能够持续提高测试准确率。结论的不一致可能源于该设置中的分布偏移实验比在BOSS数据集上测试的分布偏移实验稍微更具挑战性。BOSS基准测试中的数据集之间的分布差异相对较小，因此高质量的数据集可能会提高量化模型的准确性。此外，C-EVAL和CMMLU中同一主题类别包含的主体不完全一致，两数据集中同一主题之间的分布差异可能更大。在分布差异较大的情况下，使用I.I.D数据集作为校准集可能会导致更好的性能。'
- en: The results of cross-subject distribution shift experiments on C-EVAL are presented
    in Tab. LABEL:tab:cds_subject. The results tend to be more random, and no conclusion
    can be drawn that using any particular dataset as calibration set or an I.I.D
    dataset as calibration set results in higher test accuracy. Cross-subject distribution
    shift is significantly more challenging compared to previous cross-dataset distribution
    shifts. This is because, in previous settings, different datasets are from the
    same task type or domain, whereas the cross-subject distribution shift experiments
    on C-EVAL directly span from one domain to another. This may cause the quantized
    model to fail in obtaining accurate quantization parameters from the calibration
    set, ultimately leading to poor performance or unpredictable results.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 跨学科分布偏移实验在C-EVAL上的结果展示在表格LABEL:tab:cds_subject中。结果趋于更随机，并且无法得出使用任何特定数据集作为校准集或使用I.I.D数据集作为校准集可以提高测试准确率的结论。与之前的跨数据集分布偏移相比，跨学科分布偏移显著更具挑战性。这是因为，在之前的设置中，不同数据集来自相同的任务类型或领域，而C-EVAL上的跨学科分布偏移实验直接从一个领域跨越到另一个领域。这可能导致量化模型无法从校准集中获取准确的量化参数，最终导致性能差或结果不可预测。
- en: '4 MI-optimize: A LLM Quantization Toolbox'
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 MI-optimize：一个LLM量化工具箱
- en: Overview. MI-optimize is a versatile tool designed for the quantization and
    evaluation of LLMs.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 概述。MI-optimize是一个多功能工具，旨在对LLMs进行量化和评估。
- en: '![Refer to caption](img/eeecb1a6d0175c30f840f3676fba0bdb.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/eeecb1a6d0175c30f840f3676fba0bdb.png)'
- en: 'Figure 3: Overview of the Quantization and Evaluation Framework.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：量化和评估框架概述。
- en: 'The library’s seamless integration of various quantization methods and evaluation
    techniques empowers users to customize their approaches according to specific
    requirements and constraints, providing a high level of flexibility. Although
    LLMs excel in various NLP tasks, their computational and memory demands may limit
    their deployment in real-time applications and on resource-constrained devices.
    MI-optimize addresses this challenge by employing quantization techniques to compress
    these models, ensuring they maintain performance while remaining adaptable to
    a wide range of scenarios. Fig. [3](#S4.F3 "Figure 3 ‣ 4 MI-optimize: A LLM Quantization
    Toolbox ‣ Evaluating the Generalization Ability of Quantized LLMs: Benchmark,
    Analysis, and Toolbox") illustrates the framework of MI-optimize, which comprises
    five main modules: the Configuration, Quantization, Evaluation, Inference, and
    Execution modules.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '该库无缝集成了各种量化方法和评估技术，使用户能够根据特定要求和限制定制其方法，提供了高度的灵活性。尽管 LLM 在各种 NLP 任务中表现出色，但其计算和内存需求可能限制其在实时应用和资源受限设备上的部署。MI-optimize
    通过采用量化技术来压缩这些模型，以确保它们在保持性能的同时适应各种场景。图 [3](#S4.F3 "Figure 3 ‣ 4 MI-optimize: A
    LLM Quantization Toolbox ‣ Evaluating the Generalization Ability of Quantized
    LLMs: Benchmark, Analysis, and Toolbox") 说明了 MI-optimize 的框架，该框架包括五个主要模块：配置、量化、评估、推理和执行模块。'
- en: Experimental Setup and Results. To validate the framework’s capability of combining
    mixed quantization methods, we conduct experiments using the LLaMA-2-7B model ([54](#bib.bib54)).
    We test the model using SmoothQuant and a combination of SmoothQuant for activations
    and GPTQ for weight quantization on WikiText-2 (Wiki2) ([37](#bib.bib37)), Penn
    Treebank (PTB) ([35](#bib.bib35)), and C4 ([48](#bib.bib48)) datasets, and measure
    the perplexity (PPL) of the quantized models. Quantization is implemented using
    PyTorch. All quantization experiments are exclusively conducted on the LLaMA-2-7B
    model, utilizing a single NVIDIA V100 GPU. For calibration, we utilize a dataset
    consisting of 128 random segments, each containing 512 tokens, extract from the
    C4 dataset. These segments represent generic text data, sourced from randomly
    crawled websites, ensuring that the quantization process does not rely on task-specific
    information. Our quantization setup employ SmoothQuant with default activation
    quantization of 8 bits. We utilize groupwise quantization with a group size of
    128.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置和结果。为了验证框架结合混合量化方法的能力，我们使用 LLaMA-2-7B 模型 ([54](#bib.bib54)) 进行实验。我们测试了使用
    SmoothQuant 和 SmoothQuant 与 GPTQ 组合的模型，在 WikiText-2 (Wiki2) ([37](#bib.bib37))、Penn
    Treebank (PTB) ([35](#bib.bib35)) 和 C4 ([48](#bib.bib48)) 数据集上进行激活量化和权重量化，并测量量化模型的困惑度
    (PPL)。量化使用 PyTorch 实现。所有量化实验专门在 LLaMA-2-7B 模型上进行，利用一台 NVIDIA V100 GPU。为校准，我们使用了一个由
    128 个随机片段组成的数据集，每个片段包含 512 个标记，提取自 C4 数据集。这些片段代表了通用文本数据，来源于随机抓取的网站，确保量化过程不依赖于特定任务的信息。我们的量化设置使用了默认的
    8 位激活量化的 SmoothQuant。我们利用了大小为 128 的组量化。
- en: 'Table 5: Perplexity (PPL) of the LLaMA-2-7B model using SmoothQuant and a combination
    of SmoothQuant for activations and GPTQ for weight quantization on the WikiText-2
    (Wiki2), Penn Treebank (PTB), and C4 datasets.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：使用 SmoothQuant 和 SmoothQuant 与 GPTQ 组合进行激活量化以及权重量化的 LLaMA-2-7B 模型在 WikiText-2
    (Wiki2)、Penn Treebank (PTB) 和 C4 数据集上的困惑度 (PPL)。
- en: '| Method | W/A | Wiki2 | C4 | PTB |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | W/A | Wiki2 | C4 | PTB |'
- en: '| Baseline | 16/16 | 5.47 | 37.92 | 7.22 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 16/16 | 5.47 | 37.92 | 7.22 |'
- en: '| Smoothquant | 8/8 | 19.70 | 3026.75 | 11.27 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Smoothquant | 8/8 | 19.70 | 3026.75 | 11.27 |'
- en: '| Smoothquant+GPTQ | 8/8 | 21.18 | 3110.05 | 11.27 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| Smoothquant+GPTQ | 8/8 | 21.18 | 3110.05 | 11.27 |'
- en: '| Smoothquant | 4/8 | 34.87 | 5133.82 | 20.82 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Smoothquant | 4/8 | 34.87 | 5133.82 | 20.82 |'
- en: '| Smoothquant+GPTQ | 4/8 | 22.95 | 1359.59 | 13.39 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Smoothquant+GPTQ | 4/8 | 22.95 | 1359.59 | 13.39 |'
- en: '| Smoothquant | 3/8 | 24041.06 | 42625.86 | 29585.39 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Smoothquant | 3/8 | 24041.06 | 42625.86 | 29585.39 |'
- en: '| Smoothquant+GPTQ | 3/8 | 290.77 | - | 231.02 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Smoothquant+GPTQ | 3/8 | 290.77 | - | 231.02 |'
- en: 'The results presented in Tab. [5](#S4.T5 "Table 5 ‣ 4 MI-optimize: A LLM Quantization
    Toolbox ‣ Evaluating the Generalization Ability of Quantized LLMs: Benchmark,
    Analysis, and Toolbox") indicate several key findings. Comparing SmoothQuant with
    SmoothQuant + GPTQ configurations, it is evident that the latter consistently
    outperforms the former across all bit-width settings. This suggests that the combined
    use of SmoothQuant and GPTQ leads to a notable improvement in model performance.
    Particularly, at bit-widths of 4 and 3, the SmoothQuant + GPTQ method demonstrates
    a significant reduction in perplexity compared to SmoothQuant alone, indicating
    the pronounced effectiveness of GPTQ in reducing perplexity.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '表[5](#S4.T5 "Table 5 ‣ 4 MI-optimize: A LLM Quantization Toolbox ‣ Evaluating
    the Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox")中展示的结果表明了几个关键发现。比较SmoothQuant与SmoothQuant
    + GPTQ配置，可以明显看出后者在所有位宽设置下都始终优于前者。这表明SmoothQuant和GPTQ的结合使用显著提高了模型性能。特别是在位宽为4和3时，SmoothQuant
    + GPTQ方法相比于仅使用SmoothQuant的困惑度有显著降低，显示了GPTQ在降低困惑度方面的显著效果。'
- en: 5 Related Work
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Quantization of LLMs. Quantization techniques for LLMs mainly include Post-Training
    Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ does not require
    retraining the model and is typically suitable for situations with limited computational
    resources ([14](#bib.bib14); [10](#bib.bib10); [31](#bib.bib31); [62](#bib.bib62);
    [6](#bib.bib6); [66](#bib.bib66); [51](#bib.bib51)). QAT simulates the effects
    of quantization throughout the entire training process, enabling the model to
    adapt to low-precision representations during training, which typically leads
    to higher performance ([34](#bib.bib34); [9](#bib.bib9)). It’s worth noting that
    in this paper, we consider applying quantization directly on the pretrained LLMs
    instead of performing quantization-aware finetuning for the quantized LLMs (such
    as variants of QLoRA ([9](#bib.bib9); [67](#bib.bib67); [63](#bib.bib63))) because
    the latter typically needs the former for initialization.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的量化。LLMs的量化技术主要包括后训练量化（PTQ）和量化感知训练（QAT）。PTQ不需要重新训练模型，通常适用于计算资源有限的情况 ([14](#bib.bib14);
    [10](#bib.bib10); [31](#bib.bib31); [62](#bib.bib62); [6](#bib.bib6); [66](#bib.bib66);
    [51](#bib.bib51))。QAT在整个训练过程中模拟量化的效果，使模型在训练期间适应低精度表示，通常会导致更高的性能 ([34](#bib.bib34);
    [9](#bib.bib9))。值得注意的是，在本文中，我们考虑直接对预训练的LLMs进行量化，而不是对量化LLMs进行量化感知微调（例如QLoRA的变体 ([9](#bib.bib9);
    [67](#bib.bib67); [63](#bib.bib63)))，因为后者通常需要前者进行初始化。
- en: Evaluation of quantized LLMs. Numerous studies have undertaken evaluations of
    the performance of quantized LLMs ([14](#bib.bib14); [10](#bib.bib10); [31](#bib.bib31);
    [62](#bib.bib62); [6](#bib.bib6); [21](#bib.bib21); [61](#bib.bib61); [30](#bib.bib30);
    [33](#bib.bib33); [23](#bib.bib23); [19](#bib.bib19)). The majority of assessments
    employ fixed calibration set, primarily focusing on language modeling tasks ([48](#bib.bib48);
    [35](#bib.bib35); [37](#bib.bib37)) and standard NLP tasks ([71](#bib.bib71);
    [44](#bib.bib44); [53](#bib.bib53); [7](#bib.bib7); [50](#bib.bib50); [38](#bib.bib38);
    [39](#bib.bib39)). Certain investigations have deviated from the practice of using
    fixed calibration set, extending them to encompass a broader spectrum of crawled
    web text and pre-training data, while also conducting multiple random samplings
    for calibration set selection ([61](#bib.bib61)). Additionally, certain studies
    have conducted assessments encompassing a broader array of downstream task types
    and datasets, approaching the evaluation from various angles ([33](#bib.bib33);
    [21](#bib.bib21); [30](#bib.bib30)).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 量化LLMs的评估。许多研究对量化LLMs的性能进行了评估 ([14](#bib.bib14); [10](#bib.bib10); [31](#bib.bib31);
    [62](#bib.bib62); [6](#bib.bib6); [21](#bib.bib21); [61](#bib.bib61); [30](#bib.bib30);
    [33](#bib.bib33); [23](#bib.bib23); [19](#bib.bib19))。大多数评估采用固定的标定集，主要关注语言建模任务 ([48](#bib.bib48);
    [35](#bib.bib35); [37](#bib.bib37))和标准NLP任务 ([71](#bib.bib71); [44](#bib.bib44);
    [53](#bib.bib53); [7](#bib.bib7); [50](#bib.bib50); [38](#bib.bib38); [39](#bib.bib39))。某些研究偏离了使用固定标定集的做法，将其扩展到更广泛的爬取网页文本和预训练数据，并进行了多次随机采样以选择标定集 ([61](#bib.bib61))。此外，一些研究还进行了更广泛的下游任务类型和数据集的评估，从多个角度进行评价 ([33](#bib.bib33);
    [21](#bib.bib21); [30](#bib.bib30))。
- en: 6 Conclusion
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We investigated the generalization ability of quantized LLMs, proposing two
    evaluation scenarios and testing them on our own implemented platform. Drawing
    from our evaluation results, we found some underutilized datasets that exhibit
    quantization performance that deviates from conventional expectations. These findings
    warrant further investigation to elucidate the underlying mechanisms and optimize
    quantization strategies for such datasets. Our work unveils the significant role
    of distribution discrepancies between calibration and test data for quantization.
    We uncover the existence of cross-dataset optimal calibration data for specific
    tasks, prompting the development of novel methods for optimizing calibration data
    collection, which is overlooked in the current field of model quantization. Lastly,
    we provided a modular and scalable toolbox to this topic to facilitate future
    research.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查了量化 LLM 的泛化能力，提出了两种评估场景并在我们自己实施的平台上进行了测试。根据评估结果，我们发现一些未被充分利用的数据集表现出与传统期望不同的量化性能。这些发现需要进一步研究，以阐明潜在机制并优化这些数据集的量化策略。我们的工作揭示了校准数据与测试数据之间分布差异在量化中的重要作用。我们发现特定任务的跨数据集最佳校准数据的存在，促使开发优化校准数据收集的新方法，这在当前的模型量化领域被忽视。最后，我们提供了一个模块化和可扩展的工具箱，以促进未来的研究。
- en: References
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
    Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) Josh Achiam、Steven Adler、Sandhini Agarwal、Lama Ahmad、Ilge Akkaya、Florencia
    Leoni Aleman、Diogo Almeida、Janko Altenschmidt、Sam Altman、Shyamal Anadkat 等。GPT-4
    技术报告。arXiv 预印本 arXiv:2303.08774，2023。
- en: '(2) Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi,
    and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving
    with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) Aida Amini、Saadia Gabriel、Peter Lin、Rik Koncel-Kedziorski、Yejin Choi 和 Hannaneh
    Hajishirzi。MathQA：朝着可解释的数学文字题解法与基于操作的形式化方法迈进。arXiv 预印本 arXiv:1905.13319，2019。
- en: '(3) Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus
    Stenetorp. Beat the ai: Investigating adversarial human annotation for reading
    comprehension. Transactions of the Association for Computational Linguistics,
    8:662–678, 2020.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) Max Bartolo、Alastair Roberts、Johannes Welbl、Sebastian Riedel 和 Pontus Stenetorp。击败
    AI：调查针对阅读理解的对抗性人工注释。计算语言学协会会刊，8:662–678，2020。
- en: (4) Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman.
    Nuanced metrics for measuring unintended bias with real data for text classification.
    In Companion proceedings of the 2019 world wide web conference, pages 491–500,
    2019.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4) Daniel Borkan、Lucas Dixon、Jeffrey Sorensen、Nithum Thain 和 Lucy Vasserman。使用真实数据测量意外偏差的细化指标。2019
    年全球网络大会论文集，页 491–500，2019。
- en: (5) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
    Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
    Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (5) Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell 等。语言模型是少样本学习者。神经信息处理系统进展，33:1877–1901，2020。
- en: '(6) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip:
    2-bit quantization of large language models with guarantees. Advances in Neural
    Information Processing Systems, 36, 2024.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (6) Jerry Chee、Yaohui Cai、Volodymyr Kuleshov 和 Christopher M De Sa。Quip：具有保证的大型语言模型的
    2 位量化。神经信息处理系统进展，36，2024。
- en: (7) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (7) Peter Clark、Isaac Cowhey、Oren Etzioni、Tushar Khot、Ashish Sabharwal、Carissa
    Schoenick 和 Oyvind Tafjord。认为你已经解决了问题回答？试试 arc，AI2 推理挑战。arXiv 预印本 arXiv:1803.05457，2018。
- en: '(8) Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang, and Ming Zhou. Mutual: A dataset
    for multi-turn dialogue reasoning. arXiv preprint arXiv:2004.04494, 2020.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (8) Leyang Cui、Yu Wu、Shujie Liu、Yue Zhang 和 Ming Zhou。Mutual：一个用于多轮对话推理的数据集。arXiv
    预印本 arXiv:2004.04494，2020。
- en: '(9) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms. Advances in Neural Information Processing
    Systems, 36, 2024.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (9) Tim Dettmers、Artidoro Pagnoni、Ari Holtzman 和 Luke Zettlemoyer。Qlora：量化 LLM
    的高效微调。神经信息处理系统进展，36，2024。
- en: '(10) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev,
    Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
    Spqr: A sparse-quantized representation for near-lossless llm weight compression.
    arXiv preprint arXiv:2306.03078, 2023.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(10) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev,
    Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, 和 Dan Alistarh。Spqr:
    一种用于近无损大语言模型权重压缩的稀疏量化表示。arXiv 预印本 arXiv:2306.03078, 2023。'
- en: '(11) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang,
    and Jie Tang. Glm: General language model pretraining with autoregressive blank
    infilling. arXiv preprint arXiv:2103.10360, 2021.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(11) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang,
    和 Jie Tang。Glm: 自回归空白填充的通用语言模型预训练。arXiv 预印本 arXiv:2103.10360, 2021。'
- en: '(12) Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik,
    and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search
    engine. arXiv preprint arXiv:1704.05179, 2017.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(12) Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik,
    和 Kyunghyun Cho。Searchqa: 一个通过搜索引擎上下文增强的新问答数据集。arXiv 预印本 arXiv:1704.05179, 2017。'
- en: '(13) Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn
    Seybolt, Munmun De Choudhury, and Diyi Yang. Latent hatred: A benchmark for understanding
    implicit hate speech. arXiv preprint arXiv:2109.05322, 2021.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(13) Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn
    Seybolt, Munmun De Choudhury, 和 Diyi Yang。潜在仇恨: 理解隐性仇恨言论的基准。arXiv 预印本 arXiv:2109.05322,
    2021。'
- en: '(14) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(14) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh。Gptq: 准确的生成预训练变换器后训练量化。arXiv
    预印本 arXiv:2210.17323, 2022。'
- en: (15) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
    Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al.
    A framework for few-shot language model evaluation. Version v0\. 0.1\. Sept, page 8,
    2021.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (15) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
    Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff 等。少样本语言模型评估框架。版本
    v0\. 0.1\. 9 月，第 8 页, 2021。
- en: (16) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
    Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al.
    A framework for few-shot language model evaluation. Version v0\. 0.1\. Sept, page 8,
    2021.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (16) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
    Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff 等。少样本语言模型评估框架。版本
    v0\. 0.1\. 9 月，第 8 页, 2021。
- en: (17) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and
    Kurt Keutzer. A survey of quantization methods for efficient neural network inference.
    In Low-Power Computer Vision, pages 291–326\. Chapman and Hall/CRC, 2022.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (17) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, 和 Kurt
    Keutzer。高效神经网络推理的量化方法调查。在《低功耗计算机视觉》一书中，第 291–326 页。Chapman 和 Hall/CRC, 2022。
- en: '(18) Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar
    Ray, and Ece Kamar. Toxigen: A large-scale machine-generated dataset for adversarial
    and implicit hate speech detection. arXiv preprint arXiv:2203.09509, 2022.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(18) Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar
    Ray, 和 Ece Kamar。Toxigen: 一个大规模机器生成的数据集，用于对抗性和隐含仇恨言论检测。arXiv 预印本 arXiv:2203.09509,
    2022。'
- en: (19) Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen,
    Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. How good are low-bit quantized
    llama3 models? an empirical study. arXiv preprint arXiv:2404.14047, 2024.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (19) Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen,
    Jie Luo, Xiaojuan Qi, Xianglong Liu, 和 Michele Magno。低比特量化 llama3 模型的表现如何? 一项实证研究。arXiv
    预印本 arXiv:2404.14047, 2024。
- en: '(20) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun
    Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: A multi-level
    multi-discipline chinese evaluation suite for foundation models. Advances in Neural
    Information Processing Systems, 36, 2024.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(20) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun
    Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu 等。C-eval: 一个多层次多学科的中文评估套件，用于基础模型。神经信息处理系统进展,
    36, 2024。'
- en: '(21) Ajay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, and Yinfei
    Yang. Compressing llms: The truth is rarely pure and never simple. arXiv preprint
    arXiv:2310.01382, 2023.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(21) Ajay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, 和 Yinfei
    Yang。压缩大语言模型: 事实往往不纯粹且从不简单。arXiv 预印本 arXiv:2310.01382, 2023。'
- en: '(22) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua
    Lu. Pubmedqa: A dataset for biomedical research question answering. arXiv preprint
    arXiv:1909.06146, 2019.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(22) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, 和 Xinghua Lu。Pubmedqa:
    用于生物医学研究问题回答的数据集。arXiv 预印本 arXiv:1909.06146, 2019。'
- en: (23) Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, and
    Deyi Xiong. A comprehensive evaluation of quantization strategies for large language
    models. arXiv preprint arXiv:2402.16775, 2024.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (23) 任任金、杜江村、黄无畏、刘伟、栾剑、王斌、以及熊德义。对大规模语言模型量化策略的全面评估。arXiv 预印本 arXiv:2402.16775，2024年。
- en: (24) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,
    Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws
    for neural language models. arXiv preprint arXiv:2001.08361, 2020.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (24) 贾雷德·卡普兰、山姆·麦肯德利什、汤姆·亨尼根、汤姆·B·布朗、本杰明·切斯、雷温·查尔德、斯科特·格雷、亚历克·拉德福德、杰弗里·吴、以及达里奥·阿莫代伊。神经语言模型的规模定律。arXiv
    预印本 arXiv:2001.08361，2020年。
- en: '(25) Yuta Koreeda and Christopher D Manning. Contractnli: A dataset for document-level
    natural language inference for contracts. arXiv preprint arXiv:2110.01799, 2021.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(25) 小林优太和克里斯托弗·D·曼宁。ContractNLI: 一个用于合同文档级自然语言推理的数据集。arXiv 预印本 arXiv:2110.01799，2021年。'
- en: (26) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao
    Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for
    large language model serving with pagedattention. In Proceedings of the 29th Symposium
    on Operating Systems Principles, pages 611–626, 2023.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (26) 权宇硕、李卓涵、庄思远、盛英、郑莲敏、尤浩、戈登·约瑟夫、张浩、以及伊昂·斯托伊卡。面向大规模语言模型服务的高效内存管理与分页注意力。在第29届操作系统原理研讨会论文集，页611–626，2023年。
- en: '(27) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race:
    Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683,
    2017.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(27) 赖国坤、谢启哲、刘汉晓、杨益铭、和爱德华·霍维。Race: 大规模阅读理解数据集来自考试。arXiv 预印本 arXiv:1704.04683，2017年。'
- en: (28) Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema
    challenge. In Thirteenth international conference on the principles of knowledge
    representation and reasoning, 2012.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (28) 赫克托·莱维斯克、欧内斯特·戴维斯、以及利奥拉·摩根斯坦。Winograd模式挑战。在第十三届知识表示和推理原则国际会议，2012年。
- en: '(29) Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong,
    Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding
    in chinese. arXiv preprint arXiv:2306.09212, 2023.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(29) 李浩南、张艺璇、傅杰里·科托、杨一飞、赵海、龚叶云、段楠、以及提摩太·鲍德温。Cmmlu: 测量中文的大规模多任务语言理解。arXiv 预印本
    arXiv:2306.09212，2023年。'
- en: (30) Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen
    Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Evaluating quantized large language
    models. arXiv preprint arXiv:2402.18158, 2024.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (30) 李世尧、宁雪飞、王露宁、刘腾轩、石祥生、严胜根、戴国浩、杨华中、以及王宇。评估量化的大规模语言模型。arXiv 预印本 arXiv:2402.18158，2024年。
- en: '(31) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978, 2023.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(31) 林骥、唐佳名、唐浩天、尚洋、邓星宇、和宋寒。AWQ: 针对LLM压缩和加速的激活感知权重量化。arXiv 预印本 arXiv:2306.00978，2023年。'
- en: '(32) Alisa Liu, Swabha Swayamdipta, Noah A Smith, and Yejin Choi. Wanli: Worker
    and ai collaboration for natural language inference dataset creation. arXiv preprint
    arXiv:2201.05955, 2022.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(32) 刘阿丽莎、斯瓦巴·斯瓦扬迪普塔、诺亚·A·史密斯、和叶金·崔。WANLI: 工人和AI协作生成自然语言推理数据集。arXiv 预印本 arXiv:2201.05955，2022年。'
- en: '(33) Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang
    Li, Bolin Ding, and Ji-Rong Wen. Do emergent abilities exist in quantized large
    language models: An empirical study. arXiv preprint arXiv:2307.08072, 2023.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(33) 刘佩玉、刘子康、赵泽峰、高大伟、赵伟欣、李雅良、丁博霖、以及温季荣。量化的大规模语言模型中是否存在突现能力: 一项实证研究。arXiv 预印本
    arXiv:2307.08072，2023年。'
- en: '(34) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar
    Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free
    quantization aware training for large language models. arXiv preprint arXiv:2305.17888,
    2023.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(34) 刘泽春、巴拉斯·奥古兹、赵长生、厄尔尼·张、皮埃尔·斯托克、亚沙尔·梅赫达德、施杨阳、拉古拉曼·克里希纳穆尔西、以及维卡斯·钱德拉。LLM-QAT:
    面向大规模语言模型的数据无关量化感知训练。arXiv 预印本 arXiv:2305.17888，2023年。'
- en: '(35) Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann
    Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating
    predicate argument structure. In Human Language Technology: Proceedings of a Workshop
    held at Plainsboro, New Jersey, March 8-11, 1994, 1994.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(35) 米奇·马库斯、格雷斯·金、玛丽·安·马尔辛凯维茨、罗伯特·麦金太尔、安·比斯、马克·弗格森、凯伦·卡茨、以及布丽塔·沙斯伯格。宾州树库: 标注谓词论元结构。在人类语言技术:
    在新泽西州普莱恩斯伯勒举行的研讨会论文集，1994年3月8-11日，1994年。'
- en: '(36) Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding
    rating dimensions with review text. In Proceedings of the 7th ACM conference on
    Recommender systems, pages 165–172, 2013.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(36) Julian McAuley 和 Jure Leskovec。隐藏因素和隐藏主题: 通过评论文本理解评分维度。在第七届 ACM 推荐系统会议论文集中，页码
    165–172，2013年。'
- en: (37) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (37) Stephen Merity、Caiming Xiong、James Bradbury 和 Richard Socher。Pointer Sentinel
    混合模型。arXiv 预印本 arXiv:1609.07843，2016年。
- en: (38) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit
    of armor conduct electricity? a new dataset for open book question answering.
    arXiv preprint arXiv:1809.02789, 2018.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (38) Todor Mihaylov、Peter Clark、Tushar Khot 和 Ashish Sabharwal。盔甲能导电吗？一个新的开放书籍问答数据集。arXiv
    预印本 arXiv:1809.02789，2018年。
- en: (39) Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv
    Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and evaluation
    framework for deeper understanding of commonsense stories. arXiv preprint arXiv:1604.01696,
    2016.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (39) Nasrin Mostafazadeh、Nathanael Chambers、Xiaodong He、Devi Parikh、Dhruv Batra、Lucy
    Vanderwende、Pushmeet Kohli 和 James Allen。一个用于深入理解常识故事的语料库和评估框架。arXiv 预印本 arXiv:1604.01696，2016年。
- en: (40) Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart
    Van Baalen, and Tijmen Blankevoort. A white paper on neural network quantization.
    arXiv preprint arXiv:2106.08295, 2021.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (40) Markus Nagel、Marios Fournarakis、Rana Ali Amjad、Yelysei Bondarenko、Mart
    Van Baalen 和 Tijmen Blankevoort。关于神经网络量化的白皮书。arXiv 预印本 arXiv:2106.08295，2021年。
- en: '(41) Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio Sebastiani, and Veselin
    Stoyanov. Semeval-2016 task 4: Sentiment analysis in twitter. arXiv preprint arXiv:1912.01973,
    2019.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(41) Preslav Nakov、Alan Ritter、Sara Rosenthal、Fabrizio Sebastiani 和 Veselin
    Stoyanov。Semeval-2016 任务 4: Twitter 情感分析。arXiv 预印本 arXiv:1912.01973，2019年。'
- en: '(42) Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R Bowman. Crows-pairs:
    A challenge dataset for measuring social biases in masked language models. arXiv
    preprint arXiv:2010.00133, 2020.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(42) Nikita Nangia、Clara Vania、Rasika Bhalerao 和 Samuel R Bowman。CROWS-PAIRS:
    一个用于测量掩码语言模型社会偏见的挑战数据集。arXiv 预印本 arXiv:2010.00133，2020年。'
- en: '(43) Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and
    Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding.
    arXiv preprint arXiv:1910.14599, 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(43) Yixin Nie、Adina Williams、Emily Dinan、Mohit Bansal、Jason Weston 和 Douwe
    Kiela。对抗性 NLI: 一个新的自然语言理解基准。arXiv 预印本 arXiv:1910.14599，2019年。'
- en: '(44) Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham,
    Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández.
    The lambada dataset: Word prediction requiring a broad discourse context. arXiv
    preprint arXiv:1606.06031, 2016.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(44) Denis Paperno、Germán Kruszewski、Angeliki Lazaridou、Quan Ngoc Pham、Raffaella
    Bernardi、Sandro Pezzelle、Marco Baroni、Gemma Boleda 和 Raquel Fernández。LAMBADA
    数据集: 需要广泛话语背景的词预测。arXiv 预印本 arXiv:1606.06031，2016年。'
- en: '(45) Anselmo Peñas, Eduard Hovy, Pamela Forner, Álvaro Rodrigo, Richard Sutcliffe,
    and Roser Morante. Qa4mre 2011-2013: Overview of question answering for machine
    reading evaluation. In Information Access Evaluation. Multilinguality, Multimodality,
    and Visualization: 4th International Conference of the CLEF Initiative, CLEF 2013,
    Valencia, Spain, September 23-26, 2013\. Proceedings 4, pages 303–320\. Springer,
    2013.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(45) Anselmo Peñas、Eduard Hovy、Pamela Forner、Álvaro Rodrigo、Richard Sutcliffe
    和 Roser Morante。Qa4mre 2011-2013: 机器阅读评估中的问题回答概述。信息获取评估: 多语言性、多模态性和可视化: 第四届 CLEF
    会议，CLEF 2013，西班牙瓦伦西亚，2013年9月23-26日。会议论文集 4，页码 303–320。Springer，2013年。'
- en: '(46) Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. Dynasent:
    A dynamic benchmark for sentiment analysis. arXiv preprint arXiv:2012.15349, 2020.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(46) Christopher Potts、Zhengxuan Wu、Atticus Geiger 和 Douwe Kiela。Dynasent:
    一种用于情感分析的动态基准。arXiv 预印本 arXiv:2012.15349，2020年。'
- en: (47) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
    et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9,
    2019.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (47) Alec Radford、Jeffrey Wu、Rewon Child、David Luan、Dario Amodei、Ilya Sutskever
    等。语言模型是无监督的多任务学习者。OpenAI 博客，1(8):9，2019年。
- en: (48) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of machine learning
    research, 21(140):1–67, 2020.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (48) Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J Liu。探索统一文本到文本变换器的迁移学习极限。《机器学习研究期刊》，21(140):1–67，2020年。
- en: '(49) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad:
    100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250,
    2016.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (49) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev 和 Percy Liang。Squad：用于机器理解文本的
    100,000+ 问题。arXiv 预印本 arXiv:1606.05250，2016 年。
- en: '(50) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
    Winogrande: An adversarial winograd schema challenge at scale. Communications
    of the ACM, 64(9):99–106, 2021.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (50) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula 和 Yejin Choi。Winogrande：大规模对抗性
    Winograd 语法挑战。ACM 通讯，64(9)：99–106，2021 年。
- en: '(51) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian
    Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. arXiv preprint arXiv:2308.13137,
    2023.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (51) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian
    Li, Kaipeng Zhang, Peng Gao, Yu Qiao 和 Ping Luo。Omniquant：面向大型语言模型的全方向校准量化。arXiv
    预印本 arXiv:2308.13137，2023 年。
- en: (52) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,
    Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality
    over a sentiment treebank. In Proceedings of the 2013 conference on empirical
    methods in natural language processing, pages 1631–1642, 2013.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (52) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,
    Andrew Y Ng 和 Christopher Potts。用于情感树库的语义组合递归深度模型。在 2013 年自然语言处理实证方法会议论文集，页面 1631–1642，2013
    年。
- en: '(53) Sandeep Tata and Jignesh M Patel. Piqa: An algebra for querying protein
    data sets. In 15th International Conference on Scientific and Statistical Database
    Management, 2003., pages 141–150\. IEEE, 2003.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (53) Sandeep Tata 和 Jignesh M Patel。Piqa：一个用于查询蛋白质数据集的代数。在第十五届国际科学与统计数据库管理会议，2003
    年，页面 141–150。IEEE，2003 年。
- en: '(54) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (54) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar 等。Llama：开放而高效的基础语言模型。arXiv 预印本 arXiv:2302.13971，2023 年。
- en: '(55) Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni,
    Philip Bachman, and Kaheer Suleman. Newsqa: A machine comprehension dataset. arXiv
    preprint arXiv:1611.09830, 2016.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (55) Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni,
    Philip Bachman 和 Kaheer Suleman。Newsqa：一个机器理解数据集。arXiv 预印本 arXiv:1611.09830，2016
    年。
- en: (56) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (56) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser 和 Illia Polosukhin。Attention is all you need。神经信息处理系统进展，30，2017
    年。
- en: '(57) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
    Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural
    language understanding. arXiv preprint arXiv:1804.07461, 2018.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (57) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy 和 Samuel
    R Bowman。Glue：一个多任务基准和自然语言理解分析平台。arXiv 预印本 arXiv:1804.07461，2018 年。
- en: '(58) Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng,
    Sheng-Fu Wang, and Samuel R Bowman. Blimp: The benchmark of linguistic minimal
    pairs for english. Transactions of the Association for Computational Linguistics,
    8:377–392, 2020.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (58) Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu
    Wang 和 Samuel R Bowman。Blimp：英语语言最小对的基准。计算语言学协会会刊，8：377–392，2020 年。
- en: (59) Johannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple
    choice science questions. arXiv preprint arXiv:1707.06209, 2017.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (59) Johannes Welbl, Nelson F Liu 和 Matt Gardner。众包多项选择科学问题。arXiv 预印本 arXiv:1707.06209，2017
    年。
- en: (60) Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge
    corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426,
    2017.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (60) Adina Williams, Nikita Nangia 和 Samuel R Bowman。通过推理进行句子理解的广泛覆盖挑战语料库。arXiv
    预印本 arXiv:1704.05426，2017 年。
- en: (61) Miles Williams and Nikolaos Aletras. How does calibration data affect the
    post-training pruning and quantization of large language models? arXiv preprint
    arXiv:2311.09755, 2023.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (61) Miles Williams 和 Nikolaos Aletras。校准数据如何影响大型语言模型的后训练修剪和量化？arXiv 预印本 arXiv:2311.09755，2023
    年。
- en: '(62) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In International Conference on Machine Learning, pages 38087–38099\.
    PMLR, 2023.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (62) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, 和 Song
    Han. Smoothquant：针对大型语言模型的准确且高效的后训练量化。在国际机器学习会议上，第 38087–38099 页。PMLR，2023。
- en: '(63) Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang,
    Zhensu Chen, Xiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank
    adaptation of large language models. arXiv preprint arXiv:2309.14717, 2023.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (63) Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang,
    Zhensu Chen, Xiaopeng Zhang, 和 Qi Tian. Qa-lora：针对大型语言模型的量化感知低秩适应。arXiv 预印本 arXiv:2309.14717，2023。
- en: '(64) Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin,
    Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language
    models. arXiv preprint arXiv:2309.10305, 2023.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (64) Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin,
    Chenxu Lv, Da Pan, Dian Wang, Dong Yan 等人。Baichuan 2：开放的大规模语言模型。arXiv 预印本 arXiv:2309.10305，2023。
- en: '(65) Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu,
    Jindong Wang, Xing Xie, and Yue Zhang. Glue-x: Evaluating natural language understanding
    models from an out-of-distribution generalization perspective. arXiv preprint
    arXiv:2211.08073, 2022.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (65) Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu,
    Jindong Wang, Xing Xie, 和 Yue Zhang. Glue-x：从分布外泛化的角度评估自然语言理解模型。arXiv 预印本 arXiv:2211.08073，2022。
- en: '(66) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. Advances in Neural Information Processing Systems,
    35:27168–27183, 2022.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (66) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, 和 Yuxiong He. Zeroquant：高效且经济的后训练量化用于大规模变换器。神经信息处理系统进展，35:27168–27183，2022。
- en: '(67) Ke Yi, Yuhui Xu, Heng Chang, Chen Tang, Yuan Meng, Tong Zhang, and Jia
    Li. One quantllm for all: Fine-tuning quantized llms once for efficient deployments.
    arXiv preprint arXiv:2405.20202, 2024.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (67) Ke Yi, Yuhui Xu, Heng Chang, Chen Tang, Yuan Meng, Tong Zhang, 和 Jia Li.
    一种量化大语言模型的通用方法：一次性微调以实现高效部署。arXiv 预印本 arXiv:2405.20202，2024。
- en: (68) Han Yu, Jiashuo Liu, Xingxuan Zhang, Jiayun Wu, and Peng Cui. A survey
    on evaluation of out-of-distribution generalization. arXiv preprint arXiv:2403.01874,
    2024.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (68) Han Yu, Jiashuo Liu, Xingxuan Zhang, Jiayun Wu, 和 Peng Cui. 关于分布外泛化评估的调查。arXiv
    预印本 arXiv:2403.01874，2024。
- en: '(69) Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi
    Cheng, Heng Ji, Zhiyuan Liu, and Maosong Sun. Revisiting out-of-distribution robustness
    in nlp: Benchmarks, analysis, and llms evaluations. Advances in Neural Information
    Processing Systems, 36, 2024.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (69) Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi
    Cheng, Heng Ji, Zhiyuan Liu, 和 Maosong Sun. 重访 NLP 中的分布外鲁棒性：基准测试、分析和大语言模型评估。神经信息处理系统进展，36，2024。
- en: '(70) Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale
    adversarial dataset for grounded commonsense inference. arXiv preprint arXiv:1808.05326,
    2018.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (70) Rowan Zellers, Yonatan Bisk, Roy Schwartz, 和 Yejin Choi. Swag：一个用于基础常识推理的大规模对抗数据集。arXiv
    预印本 arXiv:1808.05326，2018。
- en: '(71) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830,
    2019.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (71) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, 和 Yejin Choi. Hellaswag：机器真的能完成你的句子吗？arXiv
    预印本 arXiv:1905.07830，2019。
- en: '(72) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open
    pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (72) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin 等人。Opt：开放的预训练变换器语言模型。arXiv
    预印本 arXiv:2205.01068，2022。
- en: (73) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
    Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large
    language models. arXiv preprint arXiv:2303.18223, 2023.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (73) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
    Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong 等人。大型语言模型的调查。arXiv 预印本 arXiv:2303.18223，2023。
- en: '(74) Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. " going on a vacation"
    takes longer than" going for a walk": A study of temporal commonsense understanding.
    arXiv preprint arXiv:1909.03065, 2019.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (74) Ben Zhou, Daniel Khashabi, Qiang Ning, 和 Dan Roth. “去度假”比“去散步”更久：对时间常识理解的研究。arXiv
    预印本 arXiv:1909.03065，2019。
- en: (75) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A survey on model
    compression for large language models. arXiv preprint arXiv:2308.07633, 2023.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (75) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, 和 Weiping Wang. 关于大型语言模型的模型压缩综述。arXiv预印本arXiv:2308.07633，2023。
- en: Appendix A More Details of MI-optimize
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A MI-optimize的更多细节
- en: 'Fig. [3](#S4.F3 "Figure 3 ‣ 4 MI-optimize: A LLM Quantization Toolbox ‣ Evaluating
    the Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox")
    illustrates the framework of MI-optimize, which comprises five main modules: the
    Configuration, Quant, Evaluation, Inference, and Execution modules. Combining
    these modules forms a cohesive pipeline that provides researchers with a reliable
    experimental environment, with each module responsible for a specific step in
    the pipeline. The subsequent sections will provide a detailed description of the
    implementation of each module.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '图[3](#S4.F3 "图 3 ‣ 4 MI-optimize: LLM量化工具箱 ‣ 评估量化LLMs的泛化能力：基准、分析和工具箱")展示了MI-optimize的框架，该框架由五个主要模块组成：配置、量化、评估、推理和执行模块。这些模块的结合形成了一个连贯的管道，为研究人员提供了一个可靠的实验环境，每个模块负责管道中的特定步骤。后续部分将详细描述每个模块的实现。'
- en: •
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Configuration Module: Manages all parameters involved in the framework, including
    default settings, quantization configurations, and evaluation configurations.'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 配置模块：管理框架中涉及的所有参数，包括默认设置、量化配置和评估配置。
- en: •
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Module: Contains various pre-trained models such as LLaMA [[54](#bib.bib54)],
    Baichuan [[64](#bib.bib64)], ChatGLM [[11](#bib.bib11)], and custom user models.'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型模块：包含各种预训练模型，如LLaMA [[54](#bib.bib54)]、Baichuan [[64](#bib.bib64)]、ChatGLM [[11](#bib.bib11)]以及自定义用户模型。
- en: •
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dataset Module: Handles different datasets, including Chinese domain-specific
    datasets (e.g., C-EVAL [[20](#bib.bib20)] and CMMLU [[29](#bib.bib29)]), the BOSS
    benchmark [[69](#bib.bib69)], general datasets (e.g., Amazon reviews, Dynasent),
    and LM-EVAL datasets (e.g., Winogrande, WSC273).'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集模块：处理不同的数据集，包括中文领域特定的数据集（例如，C-EVAL [[20](#bib.bib20)]和CMMLU [[29](#bib.bib29)]）、BOSS基准 [[69](#bib.bib69)]、通用数据集（例如，Amazon评论、Dynasent）以及LM-EVAL数据集（例如，Winogrande、WSC273）。
- en: •
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Quant Module: Responsible for loading pre-trained models, applying various
    quantization methods (e.g., GPTQ [[14](#bib.bib14)], AWQ [[31](#bib.bib31)], SPQR [[10](#bib.bib10)]),
    and performing the actual model quantization.'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化模块：负责加载预训练模型，应用各种量化方法（例如，GPTQ [[14](#bib.bib14)]、AWQ [[31](#bib.bib31)]、SPQR [[10](#bib.bib10)]）并执行实际的模型量化。
- en: •
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Inference & Eval Module: Exports the quantized model, runs inference using
    engines such as VLLM [[26](#bib.bib26)] and TensorRT, and evaluates benchmark
    performance.'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理与评估模块：导出量化模型，使用如VLLM [[26](#bib.bib26)]和TensorRT等引擎进行推理，并评估基准测试性能。
- en: •
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Execution Module: Oversees the primary tasks of model quantization, benchmarking,
    and the combined process of quantization and evaluation.'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行模块：负责模型量化、基准测试以及量化和评估的综合过程的主要任务。
- en: Key Features Supported by MI-optimize.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: MI-optimize支持的关键特性。
- en: •
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Quantization of LLMs to reduce computational and memory requirements: MI-optimize
    focuses on reducing the computational and memory footprint of large language models
    through advanced quantization techniques, making them more suitable for deployment
    in resource-limited environments.'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对LLMs进行量化以减少计算和内存需求：MI-optimize专注于通过先进的量化技术减少大型语言模型的计算和内存占用，使其更适合在资源有限的环境中部署。
- en: •
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Support for various quantization algorithms: The framework supports a wide
    range of quantization algorithms, including RTN, GPTQ [[14](#bib.bib14)], AWQ [[31](#bib.bib31)],
    SpQR [[10](#bib.bib10)], ZeroQuant [[66](#bib.bib66)], SmoothQuant [[62](#bib.bib62)],
    QuIP [[6](#bib.bib6)], and FP8\. This flexibility allows users to choose the most
    appropriate method for their specific use case, optimizing performance and resource
    usage.'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对各种量化算法的支持：该框架支持多种量化算法，包括RTN、GPTQ [[14](#bib.bib14)]、AWQ [[31](#bib.bib31)]、SpQR [[10](#bib.bib10)]、ZeroQuant [[66](#bib.bib66)]、SmoothQuant [[62](#bib.bib62)]、QuIP [[6](#bib.bib6)]和FP8。这种灵活性允许用户根据特定的使用案例选择最合适的方法，以优化性能和资源使用。
- en: •
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Evaluation on OOD tasks using benchmarks: MI-optimize includes tools for evaluating
    quantized models on out-of-distribution (OOD) tasks using established benchmarks
    such as BOSS. This ensures that the models maintain their performance even when
    encountering data that differs from their training set.'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用基准对OOD任务进行评估：MI-optimize包括用于使用已建立的基准（如BOSS）评估量化模型在分布外（OOD）任务上的工具。这确保模型在遇到与训练集不同的数据时，仍能保持其性能。
- en: •
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Support for multiple datasets: The framework supports multiple datasets for
    both calibration and testing purposes. Users can also incorporate custom datasets
    to better align the model’s performance with their specific requirements.'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 支持多个数据集：框架支持用于校准和测试目的的多个数据集。用户还可以加入自定义数据集，以更好地使模型的性能符合其具体需求。
- en: •
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Command-line interface for easy integration and automation: MI-optimize provides
    a command-line interface that facilitates easy integration into existing workflows
    and automation of the quantization and evaluation processes, streamlining the
    deployment pipeline.'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 便于集成和自动化的命令行接口：MI-optimize提供了一个命令行接口，方便将其集成到现有工作流程中，并自动化量化和评估过程，从而简化部署流程。
- en: •
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Support for combination of quantization methods: The framework allows for the
    combination of different quantization methods within the same model. Different
    layers can apply different quantization algorithms, and even multiple quantization
    algorithms can be applied to the same layer. This granular control helps optimize
    model performance and efficiency.'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 支持量化方法的组合：框架允许在同一模型内组合不同的量化方法。不同的层可以应用不同的量化算法，甚至可以在同一层上应用多个量化算法。这种细粒度的控制有助于优化模型的性能和效率。
- en: •
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ease of adding new quantization algorithms: Researchers can easily add new
    quantization algorithms to the MI-optimize repository. This modularity ensures
    that the framework remains up-to-date with the latest advancements in quantization
    techniques.'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新量化算法的易于添加：研究人员可以轻松地将新的量化算法添加到MI-optimize库中。这种模块化确保了框架与最新的量化技术进展保持同步。
- en: •
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Customer tools for model quantization and evaluation: Customers can install
    the tools provided by MI-optimize to quantize and evaluate their own models. This
    empowers users to tailor the framework to their specific needs, ensuring optimal
    model performance in their applications.'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型量化和评估的客户工具：客户可以安装由MI-optimize提供的工具，以量化和评估他们自己的模型。这使得用户能够根据自己的具体需求调整框架，确保模型在其应用中的最佳性能。
- en: Appendix B Limitation and Future works
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 限制与未来工作
- en: Despite comprehensive evaluation on over 50 datasets, our study acknowledges
    the need for a more thorough assessment of models and quantization algorithms.
    Future work could involve a more extensive evaluation framework. Additionally,
    the developed toolbox does not yet support all quantization algorithms and large
    models. Further development is warranted to expand its capabilities.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在超过50个数据集上进行了全面评估，我们的研究仍认识到对模型和量化算法进行更深入评估的必要性。未来的工作可能涉及更广泛的评估框架。此外，开发的工具箱尚不支持所有量化算法和大模型，需要进一步开发以扩展其功能。
- en: Appendix C Datasets
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 数据集
- en: 'In this section, we present all the datasets utilized in the experiments, encompassing
    their evaluated tasks and abilities, assessment metrics, and dataset sizes. Tab. [6](#A3.T6
    "Table 6 ‣ C.2 Datasets in S2 ‣ Appendix C Datasets ‣ Evaluating the Generalization
    Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox") and [7](#A3.T7 "Table
    7 ‣ C.2 Datasets in S2 ‣ Appendix C Datasets ‣ Evaluating the Generalization Ability
    of Quantized LLMs: Benchmark, Analysis, and Toolbox") provide a comprehensive
    summary of all the datasets.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们介绍了实验中使用的所有数据集，包括其评估任务和能力、评估指标以及数据集大小。表[6](#A3.T6 "Table 6 ‣ C.2 Datasets
    in S2 ‣ Appendix C Datasets ‣ Evaluating the Generalization Ability of Quantized
    LLMs: Benchmark, Analysis, and Toolbox")和[7](#A3.T7 "Table 7 ‣ C.2 Datasets in
    S2 ‣ Appendix C Datasets ‣ Evaluating the Generalization Ability of Quantized
    LLMs: Benchmark, Analysis, and Toolbox")提供了所有数据集的综合总结。'
- en: C.1 Datasets in S1
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 S1中的数据集
- en: Common sense reasoning. WinoGrande [[50](#bib.bib50)] is a large-scale coreference
    resolution task dataset derived from extensive internet text, aimed at addressing
    ambiguous and complex coreference relationships. WSC273 [[28](#bib.bib28)] comprises
    273 coreference resolution problems derived from the classic Winograd Schema Challenge,
    primarily assessing the common-sense reasoning capabilities of natural language
    understanding systems. GLUE-WNLI [[57](#bib.bib57)] is designed to test coreference
    resolution capability, which involves determining which noun a pronoun in a sentence
    refers to. It is sourced from the Winograd Schema Challenge. HellaSwag [[71](#bib.bib71)]
    is generated from web videos and Wikipedia articles and is used to infer the most
    suitable continuation for text segments in multiple-choice tasks. SWAG [[70](#bib.bib70)]
    is generated based on video descriptions, aiming to predict plausible subsequent
    scenarios for video events. PIQA [[53](#bib.bib53)] is a dataset for reasoning
    about physical common sense, derived from physics problems and solutions, designed
    to evaluate algorithms’ reasoning abilities in physical environments.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 常识推理。WinoGrande [[50](#bib.bib50)] 是一个大规模的共指解析任务数据集，源自广泛的互联网文本，旨在解决模糊和复杂的共指关系。WSC273 [[28](#bib.bib28)]
    包含273个共指解析问题，源自经典的 Winograd Schema Challenge，主要评估自然语言理解系统的常识推理能力。GLUE-WNLI [[57](#bib.bib57)]
    旨在测试共指解析能力，即确定句子中代词所指的名词。它来源于 Winograd Schema Challenge。HellaSwag [[71](#bib.bib71)]
    由网页视频和维基百科文章生成，用于推断多项选择任务中文本片段的最合适续写。SWAG [[70](#bib.bib70)] 基于视频描述生成，旨在预测视频事件的合理后续情景。PIQA [[53](#bib.bib53)]
    是一个物理常识推理数据集，来源于物理问题和解决方案，旨在评估算法在物理环境中的推理能力。
- en: Mathematical reasoning. MathQA [[2](#bib.bib2)] is collected from the MathQA
    website, consisting of 37,200 mathematical questions, with the task being to automatically
    answer mathematical questions.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 数学推理。MathQA [[2](#bib.bib2)] 来自 MathQA 网站，包含37,200个数学问题，任务是自动回答数学问题。
- en: Multi-turn dialogue reasoning. MuTual [[8](#bib.bib8)] and Mutual_plus [[8](#bib.bib8)]
    is a retrieval-based dataset for multi-turn dialogue reasoning, which is modified
    from Chinese high school English listening comprehension test data.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 多轮对话推理。MuTual [[8](#bib.bib8)] 和 Mutual_plus [[8](#bib.bib8)] 是一个基于检索的多轮对话推理数据集，改编自中国高中英语听力理解测试数据。
- en: Bias diagnosis and mitigation. CrowS-Pairs [[42](#bib.bib42)] is derived from
    a wide range of internet text and is designed to evaluate social biases in language
    models. Toxigen [[18](#bib.bib18)] is for implicit hate speech detection.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见诊断和缓解。CrowS-Pairs [[42](#bib.bib42)] 来源于广泛的互联网文本，旨在评估语言模型中的社会偏见。Toxigen [[18](#bib.bib18)]
    用于隐性仇恨言论检测。
- en: Scientific knowledge question answering. PubMedQA [[22](#bib.bib22)] is a biomedical
    question answering dataset sourced from PubMed articles, aimed at evaluating systems’
    understanding and answering capabilities of biomedical texts. OpenBookQA [[38](#bib.bib38)]
    is a new kind of question-answering dataset modeled after open book exams for
    assessing human understanding of a subject. It originates from open science education
    resources. SciQ [[59](#bib.bib59)] is a high-quality, science-themed multiple-choice
    dataset constructed manually. ARC-Easy [[7](#bib.bib7)] originates from science
    exams administered in American elementary through high schools, assessing fundamental
    scientific knowledge. ARC-Challenge [[7](#bib.bib7)] presents challenging scientific
    questions aimed at testing higher-level scientific comprehension and reasoning
    abilities. MC-TACO [[74](#bib.bib74)] consists of temporal common-sense questions
    sourced from a wide range of internet texts, designed for temporal common-sense
    reasoning tasks.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 科学知识问答。PubMedQA [[22](#bib.bib22)] 是一个生物医学问答数据集，来源于 PubMed 文章，旨在评估系统对生物医学文本的理解和回答能力。OpenBookQA [[38](#bib.bib38)]
    是一种新的问答数据集，模拟开放书籍考试，用于评估对学科的理解。它来源于开放科学教育资源。SciQ [[59](#bib.bib59)] 是一个高质量的科学主题选择题数据集，手工构建。ARC-Easy [[7](#bib.bib7)]
    源自美国小学至高中进行的科学考试，评估基础科学知识。ARC-Challenge [[7](#bib.bib7)] 提出具有挑战性的科学问题，旨在测试更高层次的科学理解和推理能力。MC-TACO [[74](#bib.bib74)]
    由来自各种互联网文本的时间常识问题组成，旨在进行时间常识推理任务。
- en: Reading comprehension. RACE [[27](#bib.bib27)] is a large-scale reading comprehension
    dataset sourced from English exams for Chinese middle school and high school students,
    aimed at testing reading comprehension abilities. QA4MRE [[45](#bib.bib45)] is
    created for the CLEF 2011/2012/2013 shared tasks, aimed at testing cross-domain
    reading comprehension abilities.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读理解。RACE [[27](#bib.bib27)] 是一个大规模阅读理解数据集，来源于中国中学和高中学生的英语考试，旨在测试阅读理解能力。QA4MRE [[45](#bib.bib45)]
    为CLEF 2011/2012/2013共享任务创建，旨在测试跨领域阅读理解能力。
- en: Natural language inference. GLUE-MNLI [[57](#bib.bib57)] is a natural language
    inference dataset comprising pairs of sentences sourced from various text genres
    such as novels, telephone conversations, and news articles. GLUE-MNLI-Mismatched [[57](#bib.bib57)]
    is utilized to evaluate the generalization capability of models on unseen text
    genres, with sentence pairs sourced from the same origins as GLUE-MNLI. GLUE-RTE [[57](#bib.bib57)]
    is sourced from news reports and Wikipedia. GLUE-QNLI [[57](#bib.bib57)] originates
    from the Stanford University’s SQuAD dataset. ANLI [[43](#bib.bib43)] is a large-scale
    adversarial natural language inference dataset divided into three difficulty levels.
    It is constructed by employing adversarial search techniques to generate challenging
    questions based on human annotations.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言推理。GLUE-MNLI [[57](#bib.bib57)] 是一个自然语言推理数据集，包含来自各种文本类型（如小说、电话对话和新闻文章）的句子对。GLUE-MNLI-Mismatched [[57](#bib.bib57)]
    用于评估模型在未见文本类型上的泛化能力，句子对的来源与GLUE-MNLI相同。GLUE-RTE [[57](#bib.bib57)] 来源于新闻报道和维基百科。GLUE-QNLI [[57](#bib.bib57)]
    源自斯坦福大学的SQuAD数据集。ANLI [[43](#bib.bib43)] 是一个大规模的对抗自然语言推理数据集，分为三个难度等级。它通过对抗搜索技术生成基于人工注释的具有挑战性的问题。
- en: Sentiment analysis. GLUE-SST [[57](#bib.bib57)] is sourced from movie reviews,
    and its task involves sentiment classification, which entails determining the
    emotional inclination of a sentence.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析。GLUE-SST [[57](#bib.bib57)] 来源于电影评论，其任务涉及情感分类，即确定句子的情感倾向。
- en: Syntax phenomena evaluation. BLiMP [[58](#bib.bib58)] is a challenge set for
    evaluating what language models know about major grammatical phenomena in English.
    BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating
    specific contrasts in syntax, morphology, or semantics. The data is automatically
    generated according to expert-crafted grammars.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 句法现象评估。BLiMP [[58](#bib.bib58)] 是一个用于评估语言模型对英语主要语法现象了解程度的挑战集。BLiMP包含67个子数据集，每个子数据集包含1000个最小对，用以隔离句法、形态学或语义中的具体对比。数据根据专家设计的语法自动生成。
- en: C.2 Datasets in S2
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 S2中的数据集
- en: Extractive question answering in BOSS. SQuAD [[49](#bib.bib49)] is a collection
    of question-answer pairs derived from Wikipedia articles. AdversarialQA [[3](#bib.bib3)]
    formulates adversarial questions within the SQuAD context, utilizing a collaborative
    process involving both human annotators and models. NewsQA [[55](#bib.bib55)]
    crafts questions based on CNN news articles, each demanding reasoning for answers,
    rather than relying solely on lexical overlap and textual entailment. SearchQA [[12](#bib.bib12)]
    employs a reverse construction approach, utilizing the Google search engine to
    fetch pertinent contexts for each question-answer pair from the J!Archive website.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: BOSS中的抽取式问答。SQuAD [[49](#bib.bib49)] 是一个从维基百科文章中衍生的问题-答案对集合。AdversarialQA [[3](#bib.bib3)]
    在SQuAD背景下制定对抗性问题，利用人类标注者和模型的协作过程。NewsQA [[55](#bib.bib55)] 基于CNN新闻文章制定问题，每个问题要求推理回答，而不仅仅依赖词汇重叠和文本蕴含。SearchQA [[12](#bib.bib12)]
    采用反向构造方法，利用Google搜索引擎从J!Archive网站中获取每个问题-答案对的相关背景。
- en: Sentiment analysis in BOSS. Amazon [[36](#bib.bib36)] is a dataset comprising
    reviews across 29 distinct product categories from the Amazon website. DynaSent [[46](#bib.bib46)]
    constructs a dataset by identifying challenging sentences from existing collections
    and generating adversarial counterparts through human-and-model collaborative
    annotation. SemEval [[41](#bib.bib41)] offers a three-class sentiment analysis
    dataset centered on Twitter content. SST [[52](#bib.bib52)] features sentence-level
    movie reviews sourced from the Rotten Tomatoes website.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: BOSS中的情感分析。Amazon [[36](#bib.bib36)] 是一个数据集，包含来自Amazon网站的29个不同产品类别的评论。DynaSent [[46](#bib.bib46)]
    通过从现有集合中识别具有挑战性的句子并通过人类和模型的协作注释生成对抗样本来构建数据集。SemEval [[41](#bib.bib41)] 提供了一个以Twitter内容为中心的三类情感分析数据集。SST [[52](#bib.bib52)]
    包含从Rotten Tomatoes网站获取的句子级电影评论。
- en: Natural language inference in BOSS. MNLI [[60](#bib.bib60)] offers sentence
    pairs across ten diverse categories of written and verbal communication, showcasing
    various styles, topics, and formalities. ANLI [[43](#bib.bib43)] is an adversarial
    dataset created using a human-and-model-in-the-loop method, featuring premises
    primarily sourced from Wikipedia and hypotheses crafted by human adversaries.
    ContractNLI [[25](#bib.bib25)] treats individual contracts as premises and applies
    a consistent set of hypotheses across the dataset. WANLI [[32](#bib.bib32)] is
    generated by GPT-3, containing examples that include challenging patterns initially
    identified in MNLI.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: BOSS中的自然语言推理。MNLI [[60](#bib.bib60)] 提供了跨越十种不同类别的书面和口头交流的句子对，展示了各种风格、主题和正式程度。ANLI [[43](#bib.bib43)]
    是一个使用人类和模型内对抗方法创建的对抗数据集，前提主要来自维基百科，假设由人类对手设计。ContractNLI [[25](#bib.bib25)] 将单个合同视为前提，并在整个数据集中应用一致的假设。WANLI [[32](#bib.bib32)]
    由GPT-3生成，包含了MNLI中最初识别的具有挑战性的模式的例子。
- en: Toxic detection in BOSS. Civil Comments [[4](#bib.bib4)] features public comments
    from the Civil Comments platform, encompassing a diverse user base and various
    subtypes of toxic text. AdvCivil introduces a new toxic dataset, derived from
    Civil Comments through textual adversarial attacks within an automated model-in-the-loop
    adversarial pipeline. Implicit Hate [[13](#bib.bib13)] includes toxic tweets that
    are both explicit and implicit, with the latter capable of evading keyword-based
    toxic detection systems. ToxiGen [[18](#bib.bib18)] is generated by GPT-3 and
    contains subtly and implicitly toxic texts targeting 13 minority groups.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: BOSS中的有毒检测。Civil Comments [[4](#bib.bib4)] 包含来自Civil Comments平台的公共评论，涵盖了多样的用户群体和各种类型的有毒文本。AdvCivil通过自动化的模型内对抗流水线，从Civil
    Comments中引入了一个新的有毒数据集，经过文本对抗攻击。Implicit Hate [[13](#bib.bib13)] 包含显性和隐性有毒推文，其中隐性有毒内容能够规避基于关键词的有毒检测系统。ToxiGen [[18](#bib.bib18)]
    由GPT-3生成，包含针对13个少数群体的微妙且隐性的有毒文本。
- en: Chinese domain-specific. C-Eval [[20](#bib.bib20)] is a comprehensive Chinese
    evaluation suite for foundation models. It consists of 13948 multi-choice questions
    spanning 52 diverse disciplines and four difficulty levels, primarily encompassing
    humanities, social sciences, STEM, and other 4 categories. CMMLU [[29](#bib.bib29)]
    is a comprehensive Chinese evaluation benchmark designed specifically to assess
    language models’ knowledge and reasoning abilities within Chinese contexts. CMMLU
    covers 67 topics ranging from fundamental subjects to advanced professional levels.
    It encompasses topics such as STEM requiring calculation and reasoning, humanities
    and social sciences necessitating knowledge, and everyday knowledge such as Chinese
    driving rules.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 中文领域特定。C-Eval [[20](#bib.bib20)] 是一个全面的中文基础模型评估套件。它由13948个多项选择题组成，涵盖52个不同学科和四个难度级别，主要包括人文学科、社会科学、STEM和其他4个类别。CMMLU [[29](#bib.bib29)]
    是一个全面的中文评估基准，专门设计用于评估语言模型在中文环境中的知识和推理能力。CMMLU涵盖67个主题，从基础学科到高级专业水平，包括需要计算和推理的STEM科目、人文学科和社会科学知识，以及如中国驾驶规则等日常知识。
- en: 'Table 6: Summary of the datasets in S1.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：S1中数据集的汇总。
- en: '| Scenario | Task&Ability | Dataset | Gene. | Metric | Size |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 任务&能力 | 数据集 | 生成. | 指标 | 大小 |'
- en: '| S1 | Common sense reasoning | WinoGrande [[50](#bib.bib50)] | 0/5 | Acc |
    1267 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 常识推理 | WinoGrande [[50](#bib.bib50)] | 0/5 | Acc | 1267 |'
- en: '| S1 | Common sense reasoning | WSC273 [[28](#bib.bib28)] | 0/5 | Acc | 273
    |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 常识推理 | WSC273 [[28](#bib.bib28)] | 0/5 | Acc | 273 |'
- en: '| S1 | Common sense reasoning | GLUE-WNLI [[57](#bib.bib57)] | 0/5 | Acc |
    71 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 常识推理 | GLUE-WNLI [[57](#bib.bib57)] | 0/5 | Acc | 71 |'
- en: '| S1 | Common sense reasoning | HellaSwag [[71](#bib.bib71)] | 0/5 | Acc |
    10042 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 常识推理 | HellaSwag [[71](#bib.bib71)] | 0/5 | Acc | 10042 |'
- en: '| S1 | Common sense reasoning | SWAG [[70](#bib.bib70)] | 0/5 | Acc | 20006
    |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 常识推理 | SWAG [[70](#bib.bib70)] | 0/5 | Acc | 20006 |'
- en: '| S1 | Common sense reasoning | PIQA [[53](#bib.bib53)] | 0/5 | Acc | 1838
    |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 常识推理 | PIQA [[53](#bib.bib53)] | 0/5 | Acc | 1838 |'
- en: '| S1 | Mathematical reasoning | MathQA [[2](#bib.bib2)] | 0/5 | Acc | 2985
    |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 数学推理 | MathQA [[2](#bib.bib2)] | 0/5 | Acc | 2985 |'
- en: '| S1 | Multi-turn dialogue reasoning | Mutual [[8](#bib.bib8)] | 0/5 | R2 |
    886 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 多轮对话推理 | Mutual [[8](#bib.bib8)] | 0/5 | R2 | 886 |'
- en: '| S1 | Multi-turn dialogue reasoning | Mutual_Plus [[8](#bib.bib8)] | 0/5 |
    R2 | 886 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 多轮对话推理 | Mutual_Plus [[8](#bib.bib8)] | 0/5 | R2 | 886 |'
- en: '| S1 | Bias diagnosis and mitigation | CrowS-Pairs [[42](#bib.bib42)] | 0 |
    Pct_stereotype | 6708 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 偏见诊断与缓解 | CrowS-Pairs [[42](#bib.bib42)] | 0 | Pct_stereotype | 6708
    |'
- en: '| S1 | Bias diagnosis and mitigation | Toxigen [[18](#bib.bib18)] | 0/5 | Acc
    | 940 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 偏见诊断与缓解 | Toxigen [[18](#bib.bib18)] | 0/5 | 准确率 | 940 |'
- en: '| S1 | Scientific knowledge question answering | PubMedQA [[22](#bib.bib22)]
    | 0/5 | Acc | 1000 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 科学知识问答 | PubMedQA [[22](#bib.bib22)] | 0/5 | 准确率 | 1000 |'
- en: '| S1 | Scientific knowledge question answering | OpenBookQA [[38](#bib.bib38)]
    | 0/5 | Acc | 500 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 科学知识问答 | OpenBookQA [[38](#bib.bib38)] | 0/5 | 准确率 | 500 |'
- en: '| S1 | Scientific knowledge question answering | SciQ [[59](#bib.bib59)] |
    0/5 | Acc | 1000 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 科学知识问答 | SciQ [[59](#bib.bib59)] | 0/5 | 准确率 | 1000 |'
- en: '| S1 | Scientific knowledge question answering | ARC-Easy [[7](#bib.bib7)]
    | 0/5 | Acc | 2376 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 科学知识问答 | ARC-Easy [[7](#bib.bib7)] | 0/5 | 准确率 | 2376 |'
- en: '| S1 | Scientific knowledge question answering | ARC-Challenge [[7](#bib.bib7)]
    | 0/5 | Acc | 1172 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 科学知识问答 | ARC-Challenge [[7](#bib.bib7)] | 0/5 | 准确率 | 1172 |'
- en: '| S1 | Scientific knowledge question answering | MC-TACO [[74](#bib.bib74)]
    | 0/5 | F1 | 9442 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 科学知识问答 | MC-TACO [[74](#bib.bib74)] | 0/5 | F1 | 9442 |'
- en: '| S1 | Reading comprehension | RACE [[27](#bib.bib27)] | 0/5 | Acc | 1045 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 阅读理解 | RACE [[27](#bib.bib27)] | 0/5 | 准确率 | 1045 |'
- en: '| S1 | Reading comprehension | QA4MRE [[45](#bib.bib45)] | 0/5 | Acc | 564
    |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 阅读理解 | QA4MRE [[45](#bib.bib45)] | 0/5 | 准确率 | 564 |'
- en: '| S1 | Natural language inference | GLUE-MNLI [[57](#bib.bib57)] | 0/5 | Acc
    | 9815 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 自然语言推理 | GLUE-MNLI [[57](#bib.bib57)] | 0/5 | 准确率 | 9815 |'
- en: '| S1 | Natural language inference | GLUE-MNLI-Mismatched [[57](#bib.bib57)]
    | 0/5 | Acc | 9832 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 自然语言推理 | GLUE-MNLI-Mismatched [[57](#bib.bib57)] | 0/5 | 准确率 | 9832
    |'
- en: '| S1 | Natural language inference | GLUE-RTE [[57](#bib.bib57)] | 0/5 | Acc
    | 277 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 自然语言推理 | GLUE-RTE [[57](#bib.bib57)] | 0/5 | 准确率 | 277 |'
- en: '| S1 | Natural language inference | GLUE-QNLI [[57](#bib.bib57)] | 0/5 | Acc
    | 5463 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 自然语言推理 | GLUE-QNLI [[57](#bib.bib57)] | 0/5 | 准确率 | 5463 |'
- en: '| S1 | Natural language inference | ANLI [[43](#bib.bib43)] | 0/5 | Acc | 3200
    |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 自然语言推理 | ANLI [[43](#bib.bib43)] | 0/5 | 准确率 | 3200 |'
- en: '| S1 | Sentiment analysis | GLUE-SST [[57](#bib.bib57)] | 0/5 | Acc | 872 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 情感分析 | GLUE-SST [[57](#bib.bib57)] | 0/5 | 准确率 | 872 |'
- en: '| S1 | Syntax phenomena evaluation | BLiMP [[58](#bib.bib58)] | 5 | Acc | 67000
    |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 句法现象评估 | BLiMP [[58](#bib.bib58)] | 5 | 准确率 | 67000 |'
- en: 'Table 7: Summary of the datasets in S2.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: S2数据集总结'
- en: '| Scenario | Task&Ability | Dataset | Gene. | Metric | Size |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 任务与能力 | 数据集 | 类型 | 评价指标 | 规模 |'
- en: '| S2 | Extractive question answering | SQuAD [[49](#bib.bib49)] | 0/1 | F1
    | 10570 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 提取式问答 | SQuAD [[49](#bib.bib49)] | 0/1 | F1 | 10570 |'
- en: '| S2 | Extractive question answering | AdversarialQA [[3](#bib.bib3)] | 0/1
    | F1 | 2694 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 提取式问答 | AdversarialQA [[3](#bib.bib3)] | 0/1 | F1 | 2694 |'
- en: '| S2 | Extractive question answering | NewsQA [[55](#bib.bib55)] | 0/1 | F1
    | 3912 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 提取式问答 | NewsQA [[55](#bib.bib55)] | 0/1 | F1 | 3912 |'
- en: '| S2 | Extractive question answering | SearchQA [[12](#bib.bib12)] | 0/1 |
    F1 | 16680 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 提取式问答 | SearchQA [[12](#bib.bib12)] | 0/1 | F1 | 16680 |'
- en: '| S2 | Sentiment analysis | Amazon [[36](#bib.bib36)] | 0/3 | Acc | 38905 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 情感分析 | Amazon [[36](#bib.bib36)] | 0/3 | 准确率 | 38905 |'
- en: '| S2 | Sentiment analysis | DynaSent [[46](#bib.bib46)] | 0/3 | Acc | 4020
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 情感分析 | DynaSent [[46](#bib.bib46)] | 0/3 | 准确率 | 4020 |'
- en: '| S2 | Sentiment analysis | SemEval [[41](#bib.bib41)] | 0/3 | Acc | 20322
    |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 情感分析 | SemEval [[41](#bib.bib41)] | 0/3 | 准确率 | 20322 |'
- en: '| S2 | Sentiment analysis | SST [[52](#bib.bib52)] | 0/3 | Acc | 767 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 情感分析 | SST [[52](#bib.bib52)] | 0/3 | 准确率 | 767 |'
- en: '| S2 | Natural language inferenc | MNLI [[60](#bib.bib60)] | 0/3 | Acc | 9815
    |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 自然语言推理 | MNLI [[60](#bib.bib60)] | 0/3 | 准确率 | 9815 |'
- en: '| S2 | Natural language inferenc | ANLI [[43](#bib.bib43)] | 0/3 | Acc | 2900
    |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 自然语言推理 | ANLI [[43](#bib.bib43)] | 0/3 | 准确率 | 2900 |'
- en: '| S2 | Natural language inferenc | ContractNLI [[25](#bib.bib25)] | 0/3 | Acc
    | 1791 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 自然语言推理 | ContractNLI [[25](#bib.bib25)] | 0/3 | 准确率 | 1791 |'
- en: '| S2 | Natural language inferenc | WANLI [[32](#bib.bib32)] | 0/3 | Acc | 4700
    |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 自然语言推理 | WANLI [[32](#bib.bib32)] | 0/3 | 准确率 | 4700 |'
- en: '| S2 | Toxic detection | Civil Comments [[4](#bib.bib4)] | 0/2 | Acc | 97320
    |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 有害信息检测 | Civil Comments [[4](#bib.bib4)] | 0/2 | 准确率 | 97320 |'
- en: '| S2 | Toxic detection | AdvCivil | 0/2 | Acc | 523 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 有害信息检测 | AdvCivil | 0/2 | 准确率 | 523 |'
- en: '| S2 | Toxic detection | Implicit Hate [[13](#bib.bib13)] | 0/2 | Acc | 21180
    |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 有害信息检测 | Implicit Hate [[13](#bib.bib13)] | 0/2 | 准确率 | 21180 |'
- en: '| S2 | Toxic detection | ToxiGen [[18](#bib.bib18)] | 0/2 | Acc | 641 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 有害信息检测 | ToxiGen [[18](#bib.bib18)] | 0/2 | 准确率 | 641 |'
- en: '| S2 | Chinese domainspecific | CEVAL [[20](#bib.bib20)] | 0/5 | Acc | 13948
    |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 中文领域特定 | CEVAL [[20](#bib.bib20)] | 0/5 | 准确率 | 13948 |'
- en: '| S2 | Chinese domainspecific | CMMLU [[29](#bib.bib29)] | 0/5 | Acc | 11917
    |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 中文领域特定 | CMMLU [[29](#bib.bib29)] | 0/5 | 准确率 | 11917 |'
- en: Appendix D Experiment Details
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 实验细节
- en: In this section, we will present all the details of our experiment, including
    hardware resources, experimental setup, hyperparameter selection, and data selection.
    Besides, Our benchmark suite is publicly available at [https://github.com/TsingmaoAI/MI-optimize](https://github.com/TsingmaoAI/MI-optimize).
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将呈现我们实验的所有细节，包括硬件资源、实验设置、超参数选择和数据选择。此外，我们的基准测试套件在[https://github.com/TsingmaoAI/MI-optimize](https://github.com/TsingmaoAI/MI-optimize)上公开提供。
- en: D.1 Hardware Resources
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 硬件资源
- en: In our experiments, we utilize one computer with 8 AMD Aldebaran GPUs and two
    computers with 2 NVIDIA Tesla V100 GPUs each. Specifically, each AMD Aldebaran
    GPU has 64GB of memory, totaling 512GB. Each NVIDIA Tesla V100 GPU has 32GB of
    memory, totaling 128GB.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们使用一台配备8个AMD Aldebaran GPU的计算机和两台各配备2个NVIDIA Tesla V100 GPU的计算机。具体来说，每个AMD
    Aldebaran GPU具有64GB内存，总计512GB。每个NVIDIA Tesla V100 GPU具有32GB内存，总计128GB。
- en: D.2 Experiment Details in S1
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 实验细节在S1
- en: Experimental Setup. We quantize LLaMA2-7B [[54](#bib.bib54)] using the GPTQ [[14](#bib.bib14)],
    SpQR [[10](#bib.bib10)] methods. We quantize the weights to 2-4 bits and test
    16 bits as reference. The quantization is implemented using our custom toolbox,
    maintaining consistency with the original method in all experimental details.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置。我们使用GPTQ [[14](#bib.bib14)]、SpQR [[10](#bib.bib10)]方法对LLaMA2-7B [[54](#bib.bib54)]进行量化。我们将权重量化为2-4位，并测试16位作为参考。量化是通过我们自定义的工具箱实现的，所有实验细节与原始方法保持一致。
- en: Hyperparameter Selection. For the GPTQ [[14](#bib.bib14)] method, we set the
    group-size parameter to 128 and apply block-sequential as well as layer-sequential
    quantization. For the SpQR [[10](#bib.bib10)] method, we set the group-size parameter
    to 128 and apply block-sequential quantization. Throughout the quantization process,
    we use 128 calibration examples. In the few-shot setting, the number of selected
    examples corresponds to LM Evaluation Harness [[16](#bib.bib16)], remaining at
    5-shot.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数选择。对于GPTQ [[14](#bib.bib14)]方法，我们将group-size参数设置为128，并应用块顺序和层顺序量化。对于SpQR [[10](#bib.bib10)]方法，我们将group-size参数设置为128，并应用块顺序量化。在整个量化过程中，我们使用128个校准样本。在少样本设置中，所选样本数量对应于LM
    Evaluation Harness [[16](#bib.bib16)]，保持在5-shot。
- en: Data Selection. We follow GPTQ [[14](#bib.bib14)] and randomly sample 128 samples
    from C4-en-val [[48](#bib.bib48)] as the calibration set with a random seed of
    42\. For the selection of test data, we use the test splits of ANLI [[43](#bib.bib43)],
    ARC [[7](#bib.bib7)], CrowSPairs [[42](#bib.bib42)], GLUE-MNLI-Mismatched [[57](#bib.bib57)],
    MathQA [[2](#bib.bib2)], MCTACO [[74](#bib.bib74)], OpenBookQA [[38](#bib.bib38)],
    RACE [[27](#bib.bib27)], SciQ [[59](#bib.bib59)], Toxigen [[18](#bib.bib18)],
    and WSC273 [[28](#bib.bib28)] as the test set. We use the validation splits of
    GLUE-SST, GLUE-MNLI, GLUE-QNLI, GLUE-WNLI, GLUE-RTE [[57](#bib.bib57)], HellaSwag [[71](#bib.bib71)],
    Mutual [[8](#bib.bib8)], PIQA [[53](#bib.bib53)], SWAG [[70](#bib.bib70)], WinoGrande [[50](#bib.bib50)]
    as the test set. Additionally, we use the train splits of BLiMP [[58](#bib.bib58)],
    PubMedQA [[22](#bib.bib22)], and QA4MRE [[45](#bib.bib45)] as the test set. For
    the selection of examples in the few-shot setting, we use the default setting.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 数据选择。我们遵循GPTQ [[14](#bib.bib14)]，并从C4-en-val [[48](#bib.bib48)]中随机抽取128个样本作为校准集，随机种子为42\.
    对于测试数据的选择，我们使用ANLI [[43](#bib.bib43)]、ARC [[7](#bib.bib7)]、CrowSPairs [[42](#bib.bib42)]、GLUE-MNLI-Mismatched [[57](#bib.bib57)]、MathQA [[2](#bib.bib2)]、MCTACO [[74](#bib.bib74)]、OpenBookQA [[38](#bib.bib38)]、RACE [[27](#bib.bib27)]、SciQ [[59](#bib.bib59)]、Toxigen [[18](#bib.bib18)]和WSC273 [[28](#bib.bib28)]的测试集。我们使用GLUE-SST、GLUE-MNLI、GLUE-QNLI、GLUE-WNLI、GLUE-RTE [[57](#bib.bib57)]、HellaSwag [[71](#bib.bib71)]、Mutual [[8](#bib.bib8)]、PIQA [[53](#bib.bib53)]、SWAG [[70](#bib.bib70)]、WinoGrande [[50](#bib.bib50)]的验证集作为测试集。此外，我们还使用BLiMP [[58](#bib.bib58)]、PubMedQA [[22](#bib.bib22)]和QA4MRE [[45](#bib.bib45)]的训练集作为测试集。对于少样本设置中的示例选择，我们使用默认设置。
- en: D.3 Experiment Details in S2
  id: totrans-423
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 实验细节在S2
- en: D.3.1 BOSS
  id: totrans-424
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.3.1 BOSS
- en: Experimental Setup. We quantize LLaMA2-7B using the GPTQ [[14](#bib.bib14)],
    SpQR [[10](#bib.bib10)], awq [[31](#bib.bib31)], and Smoothquant [[62](#bib.bib62)]
    methods. We quantize the weights to 3-4 bits, and for smoothquant, we further
    quantize the activations to 8 bits. The quantization is implemented using our
    custom toolbox, maintaining consistency with the original method in all experimental
    details.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置。我们使用GPTQ [[14](#bib.bib14)]、SpQR [[10](#bib.bib10)]、awq [[31](#bib.bib31)]和Smoothquant [[62](#bib.bib62)]方法对LLaMA2-7B进行量化。我们将权重量化为3-4位，对于smoothquant，我们进一步将激活量化为8位。量化是通过我们自定义的工具箱实现的，所有实验细节与原始方法保持一致。
- en: 'Hyperparameter Selection. For the GPTQ [[14](#bib.bib14)] method, we set the
    group-size parameter to 128 and apply block-sequential as well as layer-sequential
    quantization. For the SpQR [[10](#bib.bib10)] method, we set the group-size parameter
    to 128 and apply block-sequential quantization. For the AWQ [[31](#bib.bib31)]
    method, we set the group-size parameter to 128\. Throughout the quantization process,
    we use 128 calibration examples. In the few-shot setting, the number of selected
    examples corresponds to those in BOSS. Specifically, EQA is 1-shot, SA and NLI
    are 3-shot, and TD is 2-shot. The prompt template is presented in Tab. [8](#A4.T8
    "Table 8 ‣ D.3.2 Chinese domain-specific ‣ D.3 Experiment Details in S2 ‣ Appendix
    D Experiment Details ‣ Evaluating the Generalization Ability of Quantized LLMs:
    Benchmark, Analysis, and Toolbox").'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '超参数选择。对于GPTQ [[14](#bib.bib14)]方法，我们将组大小参数设置为128，并应用块序列和层序列量化。对于SpQR [[10](#bib.bib10)]方法，我们将组大小参数设置为128，并应用块序列量化。对于AWQ [[31](#bib.bib31)]方法，我们将组大小参数设置为128。在量化过程中，我们使用128个校准样本。在少样本设置中，选择的示例数量与BOSS中的一致。具体而言，EQA为1-shot，SA和NLI为3-shot，TD为2-shot。提示模板见Tab. [8](#A4.T8
    "Table 8 ‣ D.3.2 Chinese domain-specific ‣ D.3 Experiment Details in S2 ‣ Appendix
    D Experiment Details ‣ Evaluating the Generalization Ability of Quantized LLMs:
    Benchmark, Analysis, and Toolbox")。'
- en: Data Selection. For the calibration set, we use 128 calibration examples. For
    SQuAD [[49](#bib.bib49)] dataset in EQA, Amazon [[36](#bib.bib36)] dataset in
    SA, MNLI [[60](#bib.bib60)] dataset in NLI, and Civil Comments [[4](#bib.bib4)]
    dataset in TD, as the original datasets include train and test splits, we directly
    select the first 128 instances from the train split as the calibration set. For
    the remaining datasets, given that the original datasets exclusively contain a
    test split, we randomly sample 300 instances from the test split to form a train
    split, subsequently removing the sampled data from the test split. We use the
    first 128 instances from the sampled train split as the calibration set. The random
    seed is set to 42\. The code for processing the original BOSS benchmark will be
    placed in our GitHub repository. Concerning the selection of examples in the few-shot
    setting, we maintain consistency with BOSS. For datasets lacking examples, we
    appropriately select suitable samples from the portion of the train split not
    chosen as part of the calibration set. For the test data, we use the test split
    of each dataset as the testing dataset.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 数据选择。对于校准集，我们使用128个校准样本。对于EQA中的SQuAD [[49](#bib.bib49)]数据集、SA中的Amazon [[36](#bib.bib36)]数据集、NLI中的MNLI [[60](#bib.bib60)]数据集以及TD中的Civil
    Comments [[4](#bib.bib4)]数据集，由于原始数据集包括训练集和测试集的划分，我们直接从训练集中选择前128个实例作为校准集。对于其余的数据集，由于原始数据集仅包含测试集，我们从测试集中随机抽取300个实例以形成训练集，并随后从测试集中删除这些抽取的数据。我们使用从抽取的训练集中获得的前128个实例作为校准集。随机种子设置为42。处理原始BOSS基准测试的代码将放置在我们的GitHub仓库中。关于少样本设置中的示例选择，我们保持与BOSS的一致性。对于缺少示例的数据集，我们从未选择作为校准集的训练集部分中适当选择合适的样本。对于测试数据，我们使用每个数据集的测试集作为测试数据集。
- en: D.3.2 Chinese domain-specific
  id: totrans-428
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.3.2 中文领域特定
- en: Experimental Setup. We quantize Baichuan2-7B-Base [[64](#bib.bib64)] using the
    GPTQ [[14](#bib.bib14)], SpQR [[10](#bib.bib10)], AWQ [[31](#bib.bib31)], and
    Smoothquant [[62](#bib.bib62)] methods. We quantize the weights to 3-4 bits, and
    for smoothquant, we further quantize the activations to 8 bits. The quantization
    is implemented using our custom toolbox, maintaining consistency with the original
    method in all experimental details. Since the test split of C-EVAL was not publicly
    available, we upload the test answers to the official platform to obtain the results.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置。我们使用GPTQ [[14](#bib.bib14)]、SpQR [[10](#bib.bib10)]、AWQ [[31](#bib.bib31)]和Smoothquant [[62](#bib.bib62)]方法对Baichuan2-7B-Base [[64](#bib.bib64)]进行量化。我们将权重量化为3-4位，对于smoothquant，我们进一步将激活量化为8位。量化是使用我们自定义的工具箱实现的，确保所有实验细节与原始方法一致。由于C-EVAL的测试集没有公开，我们将测试答案上传到官方平台以获取结果。
- en: Hyperparameter Selection. For the GPTQ [[14](#bib.bib14)] method, we set the
    group-size parameter to 128 and apply block-sequential as well as layer-sequential
    quantization. For the SpQR [[10](#bib.bib10)] method, we set the group-size parameter
    to 128 and apply block-sequential quantization. For the AWQ [[31](#bib.bib31)]
    method, we set the group-size parameter to 128\. Throughout the quantization process,
    we use 128 calibration examples. In the few-shot setting, the number of selected
    examples corresponds to those in C-EVAL [[20](#bib.bib20)] and CMMLU [[29](#bib.bib29)],
    remaining at 5-shot.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数选择。对于 GPTQ [[14](#bib.bib14)] 方法，我们将组大小参数设置为 128，并应用块序列和层序量化。对于 SpQR [[10](#bib.bib10)]
    方法，我们将组大小参数设置为 128，并应用块序列量化。对于 AWQ [[31](#bib.bib31)] 方法，我们将组大小参数设置为 128。在整个量化过程中，我们使用
    128 个校准示例。在少量样本设置中，所选示例的数量对应于 C-EVAL [[20](#bib.bib20)] 和 CMMLU [[29](#bib.bib29)]
    中的数量，保持为 5-shot。
- en: 'Data Selection. For the calibration set, we use 128 calibration examples. For
    C-EVAL [[20](#bib.bib20)], we utilize its validation split as the calibration
    set. For CMMLU [[29](#bib.bib29)], we randomly select 300 instances from its test
    split for the train split, subsequently removing the sampled data from the test
    split. We use the first 128 instances from the sampled train split as the calibration
    set. The random seed is set to 42\. As for the selection of examples in the few-shot
    setting, we remain consistent with the official standards of C-EVAL and CMMLU.
    The prompt template is presented in Tab. [8](#A4.T8 "Table 8 ‣ D.3.2 Chinese domain-specific
    ‣ D.3 Experiment Details in S2 ‣ Appendix D Experiment Details ‣ Evaluating the
    Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox").'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 数据选择。对于校准集，我们使用 128 个校准示例。对于 C-EVAL [[20](#bib.bib20)]，我们利用其验证拆分作为校准集。对于 CMMLU [[29](#bib.bib29)]，我们从其测试拆分中随机选择
    300 个实例作为训练拆分，然后从测试拆分中移除采样数据。我们使用从采样训练拆分中获取的前 128 个实例作为校准集。随机种子设置为 42。至于少量样本设置中的示例选择，我们保持与
    C-EVAL 和 CMMLU 的官方标准一致。提示模板见表 [8](#A4.T8 "表 8 ‣ D.3.2 中国特定领域 ‣ D.3 实验细节在 S2 ‣
    附录 D 实验细节 ‣ 量化 LLM 的泛化能力评估：基准、分析和工具箱")。
- en: 'Table 8: Prompts for BOSS and Chinese domain-specific tasks. We maintain consistency
    with the official template provided by BOSS [[69](#bib.bib69)] and C-EVAL [[20](#bib.bib20)].'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：BOSS 和中国特定领域任务的提示。我们保持与 BOSS [[69](#bib.bib69)] 和 C-EVAL [[20](#bib.bib20)]
    提供的官方模板的一致性。
- en: '| Task | Prompt |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 提示 |'
- en: '| EQA | ### Instruction ### Solve the extractive question answering task. Refering
    to the passage below and extract answer for the question. The answer should be
    the shortest phrase as it can be. ### Format ### Passage: {{Passage}} // Question:
    {{Question}} // Answer: {{Answer}}. ### Input ### Passage: {{input_1}} // Question:
    {{input_2}} // Answer: |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| EQA | ### 指令 ### 解决抽取式问答任务。参阅下面的段落并提取问题的答案。答案应尽可能简短。 ### 格式 ### 段落：{{Passage}}
    // 问题：{{Question}} // 答案：{{Answer}}。 ### 输入 ### 段落：{{input_1}} // 问题：{{input_2}}
    // 答案： |'
- en: '| SA | ### Instruction ### Solve the sentiment analysis task. Options for sentiment:
    negative, positive, neutral. ### Format ### Text: {{Text}} // Prediction: {{Prediction}}
    ### Input ### Text: {{input}} // Prediction: |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| SA | ### 指令 ### 解决情感分析任务。情感选项：负面、正面、中立。 ### 格式 ### 文本：{{Text}} // 预测：{{Prediction}}
    ### 输入 ### 文本：{{input}} // 预测： |'
- en: '| NLI | ### Instruction ### Solve the NLI task. Options for entailment relationship:
    entailment, neutral, contradiction. ### Format ### Premise: {{Premise}} // Hypothesis:
    {{Hypothesis}} // Prediction: {{Prediction}} ### Input ### Premise: {{input_1}}
    // Hypothesis: {{input_2}} // Prediction: |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| NLI | ### 指令 ### 解决自然语言推断任务。关系选项：蕴含、中立、矛盾。 ### 格式 ### 前提：{{Premise}} // 假设：{{Hypothesis}}
    // 预测：{{Prediction}} ### 输入 ### 前提：{{input_1}} // 假设：{{input_2}} // 预测： |'
- en: '| TD | ### Instruction ### Solve the toxic detection task. Options for toxicity:
    benign, toxic. ### Format ### Text: {{Text}} // Prediction: {{Prediction}} ###
    Input ### Text: {{input}} // Prediction: |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| TD | ### 指令 ### 解决毒性检测任务。毒性选项：良性、有毒。 ### 格式 ### 文本：{{Text}} // 预测：{{Prediction}}
    ### 输入 ### 文本：{{input}} // 预测： |'
- en: '| CDS | 以下是中国考试的单项选择题，请选出其中的正确答案。 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| CDS | 以下是中国考试的单项选择题，请选出其中的正确答案。 |'
- en: Appendix E The Robustness of Data Selection with respect to Random Seed
  id: totrans-439
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 随机种子对数据选择的稳健性
- en: In the experiments conducted in the main text, we employ a random seed for the
    selection of train split and calibration set. In this section, we will alter the
    random seed to observe the sensitivity of the experiments to the random seed.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在主要文本中进行的实验中，我们使用了一个随机种子来选择训练分割和校准集。在本节中，我们将更改随机种子以观察实验对随机种子的敏感性。
- en: '![Refer to caption](img/d66263d7187a5d58db8142915960056c.png)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d66263d7187a5d58db8142915960056c.png)'
- en: 'Figure 4: S1: evaluation of quantized LLaMA2-7B on several standard datasets.
    Quantization methods include GPTQ. Quantization bits include W4A16, W3A16, and
    W2A16, with W16A16 used as reference. The left figure shows 5-shot results, while
    the right figure shows 0-shot results. Different background colors represent different
    task types. The random seed is 42.'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：S1：对量化的 LLaMA2-7B 在多个标准数据集上的评估。量化方法包括 GPTQ。量化位数包括 W4A16、W3A16 和 W2A16，以
    W16A16 作为参考。左侧图显示了 5-shot 结果，而右侧图显示了 0-shot 结果。不同的背景颜色代表不同的任务类型。随机种子为 42。
- en: '![Refer to caption](img/577e6d2859ff69ff531f515e8822dbc5.png)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/577e6d2859ff69ff531f515e8822dbc5.png)'
- en: 'Figure 5: S1: evaluation of quantized LLaMA2-7B retested on several standard
    datasets. Quantization methods include GPTQ. Quantization bits include W4A16,
    W3A16, and W2A16, with W16A16 used as reference. The left figure shows 5-shot
    results, while the right figure shows 0-shot results. Different background colors
    represent different task types. The random seed is 567.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：S1：对量化的 LLaMA2-7B 在多个标准数据集上的评估。量化方法包括 GPTQ。量化位数包括 W4A16、W3A16 和 W2A16，以
    W16A16 作为参考。左侧图显示了 5-shot 结果，而右侧图显示了 0-shot 结果。不同的背景颜色代表不同的任务类型。随机种子为 567。
- en: 'In S1, we randomly sampled 128 samples from c4-en-val as the calibration set
    and set the random seed to 42\. We then modify the random seed to 567 and retest
    the GPTQ [[14](#bib.bib14)] method. The results are presented in Fig. [4](#A5.F4
    "Figure 4 ‣ Appendix E The Robustness of Data Selection with respect to Random
    Seed ‣ Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis,
    and Toolbox") and [5](#A5.F5 "Figure 5 ‣ Appendix E The Robustness of Data Selection
    with respect to Random Seed ‣ Evaluating the Generalization Ability of Quantized
    LLMs: Benchmark, Analysis, and Toolbox"). We observe that the vast majority of
    datasets exhibited strong robustness to the selection of the calibration set,
    with performance trends remaining nearly identical across different random seeds.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在 S1 中，我们从 c4-en-val 随机抽取了 128 个样本作为校准集，并将随机种子设置为 42。我们接着将随机种子修改为 567 并重新测试
    GPTQ [[14](#bib.bib14)] 方法。结果展示在图 [4](#A5.F4 "图 4 ‣ 附录 E 关于随机种子的数据显示的鲁棒性 ‣ 量化
    LLM 的泛化能力评估：基准、分析和工具箱") 和 [5](#A5.F5 "图 5 ‣ 附录 E 关于随机种子的数据显示的鲁棒性 ‣ 量化 LLM 的泛化能力评估：基准、分析和工具箱")
    中。我们观察到，绝大多数数据集对校准集的选择表现出强大的鲁棒性，性能趋势在不同随机种子下几乎保持一致。
- en: 'In Cross-dataset distribution shift evaluation on BOSS in S2, we randomly sample
    some examples from the test split as the train split and use them as the calibration
    set, setting the random seed to 42\. We modify the random seed to 567 and retest
    the SA and NLI experiments using GPTQ [[14](#bib.bib14)] method. We present the
    average results with random seeds 42 and 567 in Tab. [9](#A5.T9 "Table 9 ‣ Appendix
    E The Robustness of Data Selection with respect to Random Seed ‣ Evaluating the
    Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox").
    The results indicate a certain robustness of the distribution shift experiment
    on BOSS towards the selection of the calibration set. For SA task, performance
    remains consistently better when using Amazon [[36](#bib.bib36)] as the calibration
    set across different random seeds, and using SemEval [[41](#bib.bib41)] as the
    calibration set performs better in most cases. However, the performance has consistently
    been poor when using DynaSent [[46](#bib.bib46)] as the calibration set. For NLI
    task, performance remains consistently better when using MNLI [[60](#bib.bib60)]
    as the calibration set across different random seeds.'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '在 BOSS 上的跨数据集分布转移评估中，我们随机抽取一些测试集样本作为训练集，并使用它们作为校准集，将随机种子设置为 42。我们将随机种子修改为 567，并使用
    GPTQ [[14](#bib.bib14)] 方法重新测试 SA 和 NLI 实验。我们在表 [9](#A5.T9 "Table 9 ‣ Appendix
    E The Robustness of Data Selection with respect to Random Seed ‣ Evaluating the
    Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox") 中展示了随机种子
    42 和 567 的平均结果。结果表明 BOSS 上的分布转移实验对校准集的选择具有一定的鲁棒性。对于 SA 任务，当使用 Amazon [[36](#bib.bib36)]
    作为校准集时，性能在不同随机种子下始终较好，而使用 SemEval [[41](#bib.bib41)] 作为校准集在大多数情况下表现更好。然而，当使用 DynaSent
    [[46](#bib.bib46)] 作为校准集时，性能始终较差。对于 NLI 任务，当使用 MNLI [[60](#bib.bib60)] 作为校准集时，性能在不同随机种子下始终较好。'
- en: 'Table 9: Cross-dataset distribution shift evaluation retested on Boss. The
    result represents the average values obtained with random seeds 42 and 567\. "Calib."
    represents the calibration dataset, and "Gene." represents generalization scenario.
    To save space, abbreviations are used for datasets. Each row presents experimental
    results using different datasets as calibration sets on the same test dataset.
    The higher the metric, the better the performance. The two best performances are
    denoted in descending order with red and orange respectively. Note: Some datasets
    could not be used as calibration sets due to insufficient memory resources.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：在 Boss 上重新测试的跨数据集分布转移评估。结果代表使用随机种子 42 和 567 获取的平均值。 “Calib.” 表示校准数据集，“Gene.”
    表示泛化场景。为节省空间，数据集使用缩写。每一行展示了使用不同的数据集作为校准集在相同测试数据集上的实验结果。指标越高，性能越好。两个最佳性能用红色和橙色分别标出。注意：由于内存资源不足，某些数据集无法作为校准集使用。
- en: '| Method | SA | NLI |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SA | NLI |'
- en: '| GPTQ | Test | Gene. | W/A | Calib. | Test | Gene. | W/A | Calib. |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 测试 | 泛化 | W/A | 校准 | 测试 | 泛化 | W/A | 校准 |'
- en: '| AZ | DS | SE | SST | MN | AN | WN | CN |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| AZ | DS | SE | SST | MN | AN | WN | CN |'
- en: '| AZ | 0-shot | 4/16 | 65.84 | 46.90 | 66.49 | 53.61 | MN | 0-shot | 4/16 |
    0.25 | 0.31 | 0.25 | - |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| AZ | 0-shot | 4/16 | 65.84 | 46.90 | 66.49 | 53.61 | MN | 0-shot | 4/16 |
    0.25 | 0.31 | 0.25 | - |'
- en: '| 3/16 | 19.14 | 0.50 | 21.41 | 0.03 | 3/16 | 0.03 | 0.00 | 0.00 | - |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 19.14 | 0.50 | 21.41 | 0.03 | 3/16 | 0.03 | 0.00 | 0.00 | - |'
- en: '| 3-shot | 4/16 | 78.47 | 70.35 | 81.43 | 80.32 | 3-shot | 4/16 | 43.28 | 34.18
    | 41.46 | - |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 3-shot | 4/16 | 78.47 | 70.35 | 81.43 | 80.32 | 3-shot | 4/16 | 43.28 | 34.18
    | 41.46 | - |'
- en: '| 3/16 | 80.73 | 41.28 | 70.79 | 70.23 | 3/16 | 32.95 | 33.02 | 32.01 | - |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 80.73 | 41.28 | 70.79 | 70.23 | 3/16 | 32.95 | 33.02 | 32.01 | - |'
- en: '| DS | 0-shot | 4/16 | 41.85 | 30.55 | 40.89 | 25.15 | AN | 0-shot | 4/16 |
    0.74 | 0.57 | 0.74 | - |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| DS | 0-shot | 4/16 | 41.85 | 30.55 | 40.89 | 25.15 | AN | 0-shot | 4/16 |
    0.74 | 0.57 | 0.74 | - |'
- en: '| 3/16 | 8.80 | 1.17 | 10.57 | 0.00 | 3/16 | 2.26 | 0.00 | 0.00 |  |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 8.80 | 1.17 | 10.57 | 0.00 | 3/16 | 2.26 | 0.00 | 0.00 |  |'
- en: '| 3-shot | 4/16 | 53.88 | 45.50 | 54.15 | 52.38 | 3-shot | 4/16 | 34.1 | 33.52
    | 33.76 | - |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| 3-shot | 4/16 | 53.88 | 45.50 | 54.15 | 52.38 | 3-shot | 4/16 | 34.1 | 33.52
    | 33.76 | - |'
- en: '| 3/16 | 53.86 | 40.25 | 44.26 | 48.91 | 3/16 | 32.25 | 33.33 | 34.19 | - |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 53.86 | 40.25 | 44.26 | 48.91 | 3/16 | 32.25 | 33.33 | 34.19 | - |'
- en: '| SE | 0-shot | 4/16 | 19.97 | 14.07 | 22.08 | 14.27 | WN | 0-shot | 4/16 |
    0.09 | 0.08 | 0.10 | - |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| SE | 0-shot | 4/16 | 19.97 | 14.07 | 22.08 | 14.27 | WN | 0-shot | 4/16 |
    0.09 | 0.08 | 0.10 | - |'
- en: '| 3/16 | 2.48 | 0.10 | 8.25 | 0.02 | 3/16 | 0.27 | 0.00 | 0.00 | - |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 2.48 | 0.10 | 8.25 | 0.02 | 3/16 | 0.27 | 0.00 | 0.00 | - |'
- en: '| 3-shot | 4/16 | 41.09 | 36.48 | 43.41 | 44.05 | 3-shot | 4/16 | 42.16 | 42.15
    | 39.925 | - |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| 3-shot | 4/16 | 41.09 | 36.48 | 43.41 | 44.05 | 3-shot | 4/16 | 42.16 | 42.15
    | 39.925 | - |'
- en: '| 3/16 | 42.69 | 27.98 | 38.57 | 36.48 | 3/16 | 43.16 | 43.36 | 46.97 | - |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 42.69 | 27.98 | 38.57 | 36.48 | 3/16 | 43.16 | 43.36 | 46.97 | - |'
- en: '| SST | 0-shot | 4/16 | 44.13 | 33.505 | 37.16 | 25.56 | CN | 0-shot | 4/16
    | 0.03 | 0.50 | 0.00 | - |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| SST | 0-shot | 4/16 | 44.13 | 33.505 | 37.16 | 25.56 | CN | 0-shot | 4/16
    | 0.03 | 0.50 | 0.00 | - |'
- en: '| 3/16 | 3.93 | 0.52 | 5.09 | 0.00 | 3/16 | 0.03 | 0.56 | 0.73 | - |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 3.93 | 0.52 | 5.09 | 0.00 | 3/16 | 0.03 | 0.56 | 0.73 | - |'
- en: '| 3-shot | 4/16 | 54.83 | 44.01 | 52.61 | 48.11 | 3-shot | 4/16 | 35.93 | 36.67
    | 32.27 | - |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| 3-shot | 4/16 | 54.83 | 44.01 | 52.61 | 48.11 | 3-shot | 4/16 | 35.93 | 36.67
    | 32.27 | - |'
- en: '| 3/16 | 57.17 | 44.33 | 46.68 | 52.29 | 3/16 | 28.28 | 20.41 | 26.13 | - |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| 3/16 | 57.17 | 44.33 | 46.68 | 52.29 | 3/16 | 28.28 | 20.41 | 26.13 | - |'
