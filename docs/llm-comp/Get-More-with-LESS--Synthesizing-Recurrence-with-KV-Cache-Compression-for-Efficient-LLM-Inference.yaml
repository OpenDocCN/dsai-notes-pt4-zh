- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:53:40'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:40
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient
    LLM Inference'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用LESS：通过KV缓存压缩合成递归以提高LLM推理效率
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.09398](https://ar5iv.labs.arxiv.org/html/2402.09398)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.09398](https://ar5iv.labs.arxiv.org/html/2402.09398)
- en: Harry Dong
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Harry Dong
- en: 'CMU Department of Electrical and Computer Engineering, Carnegie Mellon University,
    USA; Emails: {harryd,xinyuya2,yuejiec,beidic}@andrew.cmu.edu.    Xinyu Yang¹¹footnotemark:
    1'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'CMU 电气与计算机工程系，卡内基梅隆大学，美国；邮箱：{harryd,xinyuya2,yuejiec,beidic}@andrew.cmu.edu。
       Xinyu Yang¹¹footnotemark: 1'
- en: CMU    Zhenyu Zhang
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: CMU    Zhenyu Zhang
- en: 'UT Austin Department of Electrical and Computer Engineering, University of
    Texas at Austin, USA; Emails: {zhenyu.zhang,atlaswang}@utexas.edu.    Zhangyang
    (Atlas) Wang²²footnotemark: 2'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'UT Austin 电气与计算机工程系，德克萨斯大学奥斯汀分校，美国；邮箱：{zhenyu.zhang,atlaswang}@utexas.edu。
       Zhangyang (Atlas) Wang²²footnotemark: 2'
- en: 'UT Austin    Yuejie Chi¹¹footnotemark: 1'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 'UT Austin    Yuejie Chi¹¹footnotemark: 1'
- en: 'CMU    Beidi Chen¹¹footnotemark: 1'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 'CMU    Beidi Chen¹¹footnotemark: 1'
- en: CMU & Meta Meta AI (FAIR), USA.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CMU & Meta Meta AI (FAIR)，美国。
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Many computational factors limit broader deployment of large language models.
    In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache,
    a computational shortcut that requires storing previous KV pairs during decoding.
    While existing KV cache methods approach this problem by pruning or evicting large
    swaths of relatively less important KV pairs to dramatically reduce the memory
    footprint of the cache, they can have limited success in tasks that require recollecting
    a majority of previous tokens. To alleviate this issue, we propose LESS, a simple
    integration of a (nearly free) constant sized cache with eviction-based cache
    methods, such that all tokens can be queried at later decoding steps. Its ability
    to retain information throughout time shows merit on a variety of tasks where
    we demonstrate LESS can help reduce the performance gap from caching everything,
    sometimes even matching it, all while being efficient. Code can be found at [https://github.com/hdong920/LESS](https://github.com/hdong920/LESS).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 许多计算因素限制了大语言模型的广泛部署。本文关注的是由键值（KV）缓存引起的内存瓶颈，KV缓存是一种在解码过程中需要存储先前KV对的计算快捷方式。虽然现有的KV缓存方法通过修剪或驱逐相对不重要的大量KV对来显著减少缓存的内存占用，但在需要回忆大多数先前标记的任务中，它们的效果可能有限。为了解决这个问题，我们提出了LESS，这是一种将（几乎免费的）常量大小缓存与基于驱逐的缓存方法简单集成的方案，使所有标记可以在后续解码步骤中查询。其在时间跨度内保留信息的能力在各种任务中显示出优越性，我们展示了LESS可以帮助减少与缓存所有内容相比的性能差距，有时甚至能够匹敌，同时保持高效。代码可以在[https://github.com/hdong920/LESS](https://github.com/hdong920/LESS)找到。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Throughout its lifetime, the transformer architecture [[VSP^+17](#bib.bibx56)]
    has made strides in natural language processing [[LWLQ22](#bib.bibx32)], computer
    vision [[KNH^+22](#bib.bibx26)], healthcare [[NBZ^+23](#bib.bibx35)], and many
    other domains. Large language models (LLMs) [[ZRG^+22](#bib.bibx58), [SFA^+22](#bib.bibx48),
    [FZS22](#bib.bibx16), [ADF^+23](#bib.bibx2), [TMS^+23](#bib.bibx55), [TAB^+23](#bib.bibx52),
    [JSR^+24](#bib.bibx23)] take transformers to the extreme by scaling the model,
    data, and context lengths to extraordinary levels. This has been remarkably useful
    for complex tasks such as chatbots, long document tasks, and biological sequences.
    However, during deployment, these tasks require generating long sequences or inputting
    large batch sizes, which places an immense computational burden on the key-value
    (KV) cache [[PDC^+23](#bib.bibx39)], the storage of all previous keys and values
    at each layer to bypass recomputing them at future decoding steps. While this
    significantly saves computation, the tradeoff is an explosion of memory consumption.
    In such scenarios, the KV cache size often eclipses the model size. For instance,
    the Llama 2 7B model [[TMS^+23](#bib.bibx55)] occupies about 26 GB of memory,
    but the KV cache for an input of batch size 64 and sequence length 1024 occupies
    64 GB of memory, nearly 2.5 times the model size. Hence, addressing this accessibility
    issue is imperative as LLMs continue to scale and break tight deployment constraints.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在其整个生命周期中，变换器架构 [[VSP^+17](#bib.bibx56)] 在自然语言处理 [[LWLQ22](#bib.bibx32)]、计算机视觉
    [[KNH^+22](#bib.bibx26)]、医疗保健 [[NBZ^+23](#bib.bibx35)] 和许多其他领域取得了进展。大型语言模型（LLMs）
    [[ZRG^+22](#bib.bibx58), [SFA^+22](#bib.bibx48), [FZS22](#bib.bibx16), [ADF^+23](#bib.bibx2),
    [TMS^+23](#bib.bibx55), [TAB^+23](#bib.bibx52), [JSR^+24](#bib.bibx23)] 通过将模型、数据和上下文长度扩展到极端水平，将变换器技术推向了极致。这对于复杂任务，如聊天机器人、长文档任务和生物序列，极为有用。然而，在部署过程中，这些任务需要生成长序列或输入大量批量，这对关键-值（KV）缓存
    [[PDC^+23](#bib.bibx39)] 造成了巨大的计算负担，即每一层存储所有先前的键和值，以避免在未来解码步骤中重新计算它们。虽然这显著节省了计算，但代价是内存消耗的爆炸。在这种情况下，KV
    缓存的大小往往超越了模型的大小。例如，Llama 2 7B 模型 [[TMS^+23](#bib.bibx55)] 占用大约 26 GB 的内存，但批量大小为
    64 和序列长度为 1024 的 KV 缓存占用 64 GB 的内存，几乎是模型大小的 2.5 倍。因此，解决这一可访问性问题是关键的，因为 LLMs 继续扩展并突破严格的部署限制。
- en: '![Refer to caption](img/28968750e99f44aa08c51497d903a9c0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/28968750e99f44aa08c51497d903a9c0.png)'
- en: 'Figure 1: Toy (top row) and Llama 2 7B (bottom row) example decoder attention
    maps with $\operatorname{\text{H}_{2}\text{O}}$ as the underlying sparse policy.
    In the top row, red/pink and grey squares are positive and zero attention probabilities,
    respectively. In the bottom row, darker colors indicate larger attention probabilities.
    Sparse attention policies zero out many positive attention probabilities. Our
    method, LESS, ensures all previous tokens will have some contribution to the attention
    layer output to better retain information.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Toy（上排）和 Llama 2 7B（下排）的示例解码器注意力图，使用 $\operatorname{\text{H}_{2}\text{O}}$
    作为底层稀疏策略。在上排中，红色/粉色和灰色方块分别表示正注意力概率和零注意力概率。在下排中，较深的颜色表示较大的注意力概率。稀疏注意力策略将许多正注意力概率归零。我们的方法
    LESS 确保所有先前的标记对注意力层输出都有一定的贡献，以更好地保留信息。
- en: Thankfully, there have been initiatives to reduce the KV cache size. A line
    of work, in which we refer to as sparse policies or algorithms, explores the selection
    of the best subset of KV pairs to cache [[ZSZ^+23](#bib.bibx59), [LDL^+23](#bib.bibx29),
    [HWX^+23](#bib.bibx22), [XTC^+23](#bib.bibx57)]. Although very promising, these
    methods are inevitably and irrecoverably discarding KV pairs deemed, in one way
    or another, less important than others, leading to gaps in attention maps
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 幸好，已经有一些减少 KV 缓存大小的举措。一些工作，我们称之为稀疏策略或算法，探讨了选择最佳的 KV 对缓存 [[ZSZ^+23](#bib.bibx59),
    [LDL^+23](#bib.bibx29), [HWX^+23](#bib.bibx22), [XTC^+23](#bib.bibx57)]。尽管这些方法非常有前景，但它们不可避免地且不可恢复地丢弃了以某种方式被认为不如其他对重要的
    KV 对，导致注意力图中的空白。
- en: '![Refer to caption](img/dc96ba89a1ddd270d7c1f4f572bc17ac.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dc96ba89a1ddd270d7c1f4f572bc17ac.png)'
- en: 'Figure 2: Incorrect summary by Falcon 7B with sparse policy $\operatorname{\text{H}_{2}\text{O}}$.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Falcon 7B 在稀疏策略 $\operatorname{\text{H}_{2}\text{O}}$ 下的错误总结。
- en: 'as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Get More with LESS:
    Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference").
    Consequently, they are boldly assuming tokens that are unimportant now will not
    hold significance at future decoding steps, a faulty conjecture for tasks that
    deviate from this pattern. For instance, using sparse policy $\operatorname{\text{H}_{2}\text{O}}$
    [[ZSZ^+23](#bib.bibx59)] on Falcon 7B [[AAA^+23](#bib.bibx1)] to summarize an
    article [[BBC15](#bib.bibx3), [NCL18](#bib.bibx36)] produces a factually incorrect
    summary in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Get More with LESS:
    Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference").
    For the full article, see Figure [13](#A2.F13 "Figure 13 ‣ Appendix B Generation
    Outputs ‣ Get More with LESS: Synthesizing Recurrence with KV Cache Compression
    for Efficient LLM Inference") in Appendix [B](#A2 "Appendix B Generation Outputs
    ‣ Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient
    LLM Inference").'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Get More with LESS: Synthesizing
    Recurrence with KV Cache Compression for Efficient LLM Inference")所示。因此，他们大胆假设当前不重要的标记在未来解码步骤中不会具有重要性，这对于偏离这种模式的任务来说是一种错误的猜测。例如，在
    Falcon 7B [[AAA^+23](#bib.bibx1)] 上使用稀疏策略 $\operatorname{\text{H}_{2}\text{O}}$
    [[ZSZ^+23](#bib.bibx59)] 来总结一篇文章 [[BBC15](#bib.bibx3), [NCL18](#bib.bibx36)] 会在图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Get More with LESS: Synthesizing Recurrence with
    KV Cache Compression for Efficient LLM Inference")中产生一个事实不正确的总结。有关完整文章，请参见图[13](#A2.F13
    "Figure 13 ‣ Appendix B Generation Outputs ‣ Get More with LESS: Synthesizing
    Recurrence with KV Cache Compression for Efficient LLM Inference")，见附录[B](#A2
    "Appendix B Generation Outputs ‣ Get More with LESS: Synthesizing Recurrence with
    KV Cache Compression for Efficient LLM Inference")。'
- en: One way to combat information loss is to cache more tokens, but this is far
    from memory efficient. An ideal KV cache policy should 1) minimize performance
    degradation from a full cache, 2) scale at a much slower rate than the full KV
    cache, and 3) be cheap to integrate into existing pretrained LLMs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗信息丢失的一种方法是缓存更多标记，但这远非内存高效。理想的KV缓存策略应该 1) 最小化满缓存带来的性能下降，2) 比全KV缓存的扩展速度慢得多，3)
    易于集成到现有的预训练LLM中。
- en: '![Refer to caption](img/5dcfcedd724bda720a0ddb1de97992a3.png)![Refer to caption](img/35328e5be2d965a56baefd4df1327888.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5dcfcedd724bda720a0ddb1de97992a3.png)![参见说明](img/35328e5be2d965a56baefd4df1327888.png)'
- en: 'Figure 3: Attention residuals exploration in Llama 2 7B on WikiText [[MXBS16](#bib.bibx34)].
    Mean and 1000 sample relative singular value plots of true attention outputs and
    residuals from top-$512$ caching with and without low-rank approximations (right).
    A rank-4 approximation virtually recovers the original performance.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在 WikiText [[MXBS16](#bib.bibx34)] 上对 Llama 2 7B 的注意力残差探索。真实注意力输出和来自顶级-$512$
    缓存的低秩近似（右）的1000个样本相对奇异值图。一个秩为4的近似几乎恢复了原始性能。
- en: 'Fortunately, with some investigation into the residual between full and sparse
    attention outputs, a better strategy emerges. First, define the residual as $\bm{\Delta}_{\bm{A}}=\bm{A}-\bm{A}_{\text{sparse}}$
    — based on Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Get More with LESS:
    Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference"),
    a similar observation to Chen et al. [[CDW^+21](#bib.bibx6)]. Even a very low-rank
    approximation can nearly negate the performance degradation from sparse caching.
    In turn, this finding motivates the use of low-rank methods to approximate the
    residuals for efficient caches.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '幸运的是，通过一些对全注意力和稀疏注意力输出之间残差的研究，一种更好的策略出现了。首先，将残差定义为 $\bm{\Delta}_{\bm{A}}=\bm{A}-\bm{A}_{\text{sparse}}$
    —— 基于图[3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Get More with LESS: Synthesizing
    Recurrence with KV Cache Compression for Efficient LLM Inference")，这一观察与 Chen
    等人 [[CDW^+21](#bib.bibx6)] 的观察类似。即使是非常低秩的近似也可以几乎抵消稀疏缓存带来的性能下降。反过来，这一发现促使了使用低秩方法来近似残差以实现高效缓存。'
- en: 'We propose LESS (Low-rank Embedding Sidekick with Sparse policy) to learn the
    residual between the original attention output and the attention output approximated
    by a sparse policy. LESS does this by accumulating information that would have
    been discarded by sparse policies into a constant-sized low-rank cache or state,
    allowing for queries to still access information to recover previously omitted
    regions in attention maps (see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient
    LLM Inference")).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提出LESS（低秩嵌入助手与稀疏策略）来学习原始注意力输出与稀疏策略近似的注意力输出之间的残差。LESS通过将稀疏策略会丢弃的信息累积到恒定大小的低秩缓存或状态中，从而允许查询仍能访问信息，恢复在注意力图中先前遗漏的区域（参见图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Get More with LESS: Synthesizing Recurrence
    with KV Cache Compression for Efficient LLM Inference")）。'
- en: 'We show that LESS makes significant progress towards an ideal cache:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了LESS在实现理想缓存方面取得了显著进展。
- en: '1.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Performance Improvement: LESS synthesizes sparse KV policies with low-rank
    states to bridge the performance gap on a variety of tasks where these sparse
    algorithms show cracks of weakness. In fact, LESS improves the performance much
    more than simply dedicating that memory to storing more KV pairs.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能提升：LESS将稀疏KV策略与低秩状态合成，以弥补这些稀疏算法在各种任务中表现出的性能差距。实际上，LESS的性能提升远超仅仅将那部分内存用于存储更多KV对的效果。
- en: '2.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Constant Low-rank Cache Size: Low-rank caches in LESS occupy constant memory
    with respect to the sequence length, and in our experiments, the extra storage
    to accommodate LESS is nearly free, taking up the equivalent space of only 4 extra
    KV pairs in our experiments. Inspired by recurrent networks, the low-rank state
    stores new information by recursive updates rather than concatenation. As each
    sample has its own cache, LESS provides the same proportional cache reduction
    for small and large batch sizes.'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 恒定低秩缓存大小：LESS中的低秩缓存相对于序列长度占用恒定内存，在我们的实验中，容纳LESS的额外存储几乎是免费的，仅占用相当于4个额外KV对的空间。受到递归网络的启发，低秩状态通过递归更新而非连接来存储新信息。由于每个样本都有自己的缓存，LESS为小批量和大批量大小提供相同比例的缓存减少。
- en: '3.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Cheap Integration: Changes to the LLMs’ architectures are small and do not
    perturb the original weights. The only modifications to LLMs will be the addition
    of tiny multilayer perceptions (MLPs) at each attention layer. For example, using
    LESS with Llama 2 13B adds fewer than 2% of the total number of parameters. In
    addition, we can train LESS at each attention layer independently from all others,
    bypassing expensive end-to-end training. Trained once, LESS can transfer to more
    relaxed settings while maintaining comparable performance, further extending its
    applicability.'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 低成本集成：对LLMs架构的修改很小，不会扰动原有权重。对LLMs唯一的修改是每个注意力层添加了微小的多层感知机（MLPs）。例如，使用LESS与Llama
    2 13B结合，只增加了不到2%的总参数量。此外，我们可以在每个注意力层独立训练LESS，绕过昂贵的端到端训练。训练一次后，LESS可以在更宽松的环境下转移，同时保持类似的性能，进一步扩展了其适用性。
- en: Our comprehensive experiments on Llama 2 [[TMS^+23](#bib.bibx55)] and Falcon
    [[AAA^+23](#bib.bibx1)] with different sparse policies [[ZSZ^+23](#bib.bibx59),
    [HWX^+23](#bib.bibx22), [XTC^+23](#bib.bibx57)] on a variety of tasks demonstrates
    LESS as a highly performative method that reduces KV cache memory. For instance,
    LESS recovers more than 40% of the Rouge-1 degradation caused by a sparse policy
    on the CNN/DailyMail dataset [[HKG^+15](#bib.bibx21), [SLM17](#bib.bibx50)] with
    Falcon 7B. Finally, we provide an implementation of LESS that reduces the latency
    by up to $1.3\times$ from the full cache.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Llama 2 [[TMS^+23](#bib.bibx55)] 和 Falcon [[AAA^+23](#bib.bibx1)] 上进行的全面实验，使用不同的稀疏策略
    [[ZSZ^+23](#bib.bibx59), [HWX^+23](#bib.bibx22), [XTC^+23](#bib.bibx57)] 处理各种任务，表明LESS是一种高效的方法，能够减少KV缓存内存。例如，LESS恢复了Falcon
    7B在CNN/DailyMail数据集 [[HKG^+15](#bib.bibx21), [SLM17](#bib.bibx50)] 中由于稀疏策略引起的超过40%的Rouge-1降级。最后，我们提供了一个LESS的实现，能够将延迟减少高达$1.3\times$。
- en: Notation.
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 符号。
- en: We use unbolded letters (e.g. $a,A$.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用非粗体字母（例如 $a,A$）。
- en: 2 Background & Intuition
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与直觉
- en: 'We start by building the intuition behind LESS. Sparse and low-rank caches
    individually have noteworthy advantages but also severe drawbacks. Understanding
    the mechanisms of both (Section [2.1](#S2.SS1 "2.1 KV Cache Policies ‣ 2 Background
    & Intuition ‣ Get More with LESS: Synthesizing Recurrence with KV Cache Compression
    for Efficient LLM Inference") and [2.2](#S2.SS2 "2.2 Low-rank Attention ‣ 2 Background
    & Intuition ‣ Get More with LESS: Synthesizing Recurrence with KV Cache Compression
    for Efficient LLM Inference")) allows us to effectively synthesize sparse and
    low-rank structures to create LESS. In Section [2.3](#S2.SS3 "2.3 Sparse and Low-rank
    Decomposition ‣ 2 Background & Intuition ‣ Get More with LESS: Synthesizing Recurrence
    with KV Cache Compression for Efficient LLM Inference"), we show that this type
    of synthesis is a principled approach which has also found success in other areas.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先建立对 LESS 的直觉。稀疏和低秩缓存各自都有显著的优点，但也有严重的缺点。理解这两者的机制（第 [2.1](#S2.SS1 "2.1 KV
    Cache Policies ‣ 2 Background & Intuition ‣ Get More with LESS: Synthesizing Recurrence
    with KV Cache Compression for Efficient LLM Inference") 和 [2.2](#S2.SS2 "2.2 Low-rank
    Attention ‣ 2 Background & Intuition ‣ Get More with LESS: Synthesizing Recurrence
    with KV Cache Compression for Efficient LLM Inference") 节）使我们能够有效地综合稀疏和低秩结构，创造出
    LESS。在第 [2.3](#S2.SS3 "2.3 Sparse and Low-rank Decomposition ‣ 2 Background &
    Intuition ‣ Get More with LESS: Synthesizing Recurrence with KV Cache Compression
    for Efficient LLM Inference") 节中，我们展示了这种综合方法是一种有原则的方法，并且在其他领域也取得了成功。'
- en: 2.1 KV Cache Policies
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 KV 缓存策略
- en: 'Many current methods to reduce the KV cache footprint involve keeping a tiny
    subset of the keys and values either with some pruning policy [[LDL^+23](#bib.bibx29),
    [ZSZ^+23](#bib.bibx59), [HWX^+23](#bib.bibx22), [XTC^+23](#bib.bibx57), [GZL^+23](#bib.bibx20)]
    or a local attention mechanism [[CGRS19](#bib.bibx7), [PVU^+18](#bib.bibx42)].
    The former method can be applied directly to trained models whereas the latter
    typically cannot, so with limited compute, deploying a KV cache pruning policy
    is more practical. Such methods take advantage of the observation that many tokens
    are irrelevant for attention in some tasks and thus omitting them leads to negligible
    performance loss. For instance, one of our baselines, $\operatorname{\text{H}_{2}\text{O}}$
    [[ZSZ^+23](#bib.bibx59)], continuously accumulates attention probabilities at
    each generation step to identify a set of heavy-hitting tokens to be cached together
    with the most recent tokens. Not explicitly designed for KV cache compression,
    algorithms for infinite inference [[HWX^+23](#bib.bibx22), [XTC^+23](#bib.bibx57)]
    maintain a full cache, but as the input sequence exceeds the maximum context length
    of a model, KV pairs in the middle of the sequence are dropped. Staying within
    the maximum context length, this results in a cache that maintains the most recent
    and first few tokens. Regardless of the sparse method, maintaining a tight KV
    cache budget can seriously impair model performance, as we will see in Section [4](#S4
    "4 Experiments ‣ Get More with LESS: Synthesizing Recurrence with KV Cache Compression
    for Efficient LLM Inference").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '目前减少 KV 缓存占用的方法之一是通过一些剪枝策略 [[LDL^+23](#bib.bibx29), [ZSZ^+23](#bib.bibx59),
    [HWX^+23](#bib.bibx22), [XTC^+23](#bib.bibx57), [GZL^+23](#bib.bibx20)] 或本地注意力机制
    [[CGRS19](#bib.bibx7), [PVU^+18](#bib.bibx42)] 保留一个小的键值子集。前者方法可以直接应用于训练后的模型，而后者通常不能，因此在计算资源有限的情况下，部署
    KV 缓存剪枝策略更加实际。这些方法利用了一个观察，即在某些任务中许多标记对注意力无关，因此省略它们会导致微不足道的性能损失。例如，我们的一个基准 $\operatorname{\text{H}_{2}\text{O}}$
    [[ZSZ^+23](#bib.bibx59)] 在每次生成步骤中持续积累注意力概率，以识别一组重度标记，并将其与最新的标记一起缓存。虽然并非专门为 KV
    缓存压缩设计，但无限推理算法 [[HWX^+23](#bib.bibx22), [XTC^+23](#bib.bibx57)] 维持一个完整的缓存，但当输入序列超出模型的最大上下文长度时，序列中间的
    KV 对会被丢弃。保持在最大上下文长度内，这会导致一个缓存，保持最新和最初的几个标记。无论稀疏方法如何，保持紧凑的 KV 缓存预算都可能严重影响模型性能，正如我们将在第
    [4](#S4 "4 Experiments ‣ Get More with LESS: Synthesizing Recurrence with KV Cache
    Compression for Efficient LLM Inference") 节中看到的那样。'
- en: There also exist promising non-eviction based methods. CacheGen’s KV cache compression
    at the bit-level takes a query-agnostic approach [[LLD^+23](#bib.bibx31)]. In
    vision tasks, token merging is an effective way to cut down the number of tokens
    to process [[BFD^+22](#bib.bibx4), [RPH^+22](#bib.bibx43)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 也存在一些有前景的非驱逐方法。CacheGen 在比特级别的 KV 缓存压缩采用了查询无关的方法 [[LLD^+23](#bib.bibx31)]。在视觉任务中，标记合并是减少处理标记数量的有效方法
    [[BFD^+22](#bib.bibx4), [RPH^+22](#bib.bibx43)]。
- en: 2.2 Low-rank Attention
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 低秩注意力
- en: Low-rank structures in attention have been explored extensively [[TDBM22](#bib.bibx54)],
    namely from the lens of recurrent neural networks (RNNs). Unlike transformers,
    RNNs integrate information from all previous tokens into hidden states, analogous
    low-rank structures to KV caches that organically occupy constant memory. In fact,
    this feature of RNNs over transformers has motivated research in alternative architectures
    [[DFS^+22](#bib.bibx14), [PMN^+23](#bib.bibx40), [PAA^+23](#bib.bibx38), [SDH^+23](#bib.bibx47),
    [GD23](#bib.bibx18)], but for now, their adoption in LLMs is very limited compared
    to transformers. Though not as performative as these alternative architectures,
    linear transformers that break apart the attention operation into kernels also
    maintain a constant sized KV cache [[TBY^+19](#bib.bibx53), [KVPF20](#bib.bibx28),
    [CLD^+20](#bib.bibx9), [PPY^+21](#bib.bibx41)] by reformulating the cache into
    an RNN hidden state. These types of caching mechanisms are low-rank since information
    is condensed along the sequence axis, rather than explicitly maintaining individual
    tokens. This is possible when we replace the softmax with a separable similarity
    metric $\phi(\bm{q}_{t})\psi(\bm{K}_{t})^{\top}$ are such that
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力中的低秩结构已经被广泛探索 [[TDBM22](#bib.bibx54)]，特别是从递归神经网络（RNNs）的角度来看。与 transformers
    不同，RNNs 从所有之前的 token 中整合信息到隐藏状态中，类似于 KV 缓存的低秩结构，天然占用常量内存。实际上，这一 RNNs 的特性相较于 transformers
    激发了对替代架构的研究 [[DFS^+22](#bib.bibx14), [PMN^+23](#bib.bibx40), [PAA^+23](#bib.bibx38),
    [SDH^+23](#bib.bibx47), [GD23](#bib.bibx18)]，但目前在 LLMs 中的采用与 transformers 相比非常有限。虽然这些替代架构的表现不如这些替代架构，但将注意力操作拆分为核的线性
    transformers 也保持了一个常量大小的 KV 缓存 [[TBY^+19](#bib.bibx53), [KVPF20](#bib.bibx28),
    [CLD^+20](#bib.bibx9), [PPY^+21](#bib.bibx41)]，通过将缓存重新表述为 RNN 隐藏状态。这些类型的缓存机制是低秩的，因为信息沿序列轴被压缩，而不是显式地保持单个
    token。当我们用可分离的相似度度量 $\phi(\bm{q}_{t})\psi(\bm{K}_{t})^{\top}$ 替换 softmax 时，便可以实现这一点。
- en: '|  | $1$2 |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: we just need to cache hidden states $\bm{H}_{t}=\psi(\bm{K}_{t})^{\top}\bm{V}_{t}\in\mathbb{R}^{R\times
    D}$ for inference which can be expressed recursively as
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需缓存隐藏状态 $\bm{H}_{t}=\psi(\bm{K}_{t})^{\top}\bm{V}_{t}\in\mathbb{R}^{R\times
    D}$ 进行推理，这可以递归地表示为
- en: '|  | $\displaystyle\bm{H}_{t+1}$ |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{H}_{t+1}$ |  |'
- en: '|  | $\displaystyle\bm{z}_{t+1}$ |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{z}_{t+1}$ |  |'
- en: for each new KV pair $(\bm{k}_{t},\bm{v}_{t})$ [[CTTS23](#bib.bibx12)]. With
    this formulation, transformers act like RNNs which occupy constant memory during
    generation by not appending but updating hidden states during each generation
    step. Since LLMs are not typically trained in this fashion, a major challenge
    is to induce this property without significant computation or adjustment to the
    original weights [[KPZ^+21](#bib.bibx27)]. While its dilution of information restricts
    its performance when specific tokens need to be recalled with strong signals [[KHQJ18](#bib.bibx25)],
    this is exactly what a sparse KV cache algorithm can do, so we can fully take
    advantage of its infinite compression capability to obtain some high level representation
    of the less important tokens, meaning kernelized attention is a good candidate
    method for LESS to learn the residual.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个新的 KV 对 $(\bm{k}_{t},\bm{v}_{t})$ [[CTTS23](#bib.bibx12)]。在这种形式下，transformers
    像 RNNs 一样，在生成过程中通过更新而不是附加隐藏状态来占用常量内存。由于 LLMs 通常不是以这种方式训练的，因此主要挑战是引入这一特性而不对原始权重进行显著的计算或调整
    [[KPZ^+21](#bib.bibx27)]。尽管其信息稀释限制了在需要强信号回忆特定 token 时的性能 [[KHQJ18](#bib.bibx25)]，但这正是稀疏
    KV 缓存算法能够做到的，因此我们可以充分利用其无限压缩能力来获得一些较少重要 token 的高级表示，这意味着核化注意力是 LESS 学习残差的一个良好候选方法。
- en: 2.3 Sparse and Low-rank Decomposition
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 稀疏和低秩分解
- en: LESS follows a rich history of decomposing structures into sparse and low-rank
    components. Particularly, the study of robust principal component analysis [[CLMW11](#bib.bibx10),
    [CSPW11](#bib.bibx11)] has shown this type of decomposition greatly enhances approximation
    accuracy and expressibility beyond just sparse or low-rank matrices alone. Its
    success has spread to deep learning areas such as efficient attention [[CDW^+21](#bib.bibx6)],
    model compression [[LYZ^+23](#bib.bibx33)], and fine-tuning [[NTA24](#bib.bibx37)].
    Likewise, we take inspiration from these works in our design.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: LESS 继承了将结构分解为稀疏和低秩组件的丰富历史。特别是，稳健主成分分析 [[CLMW11](#bib.bibx10), [CSPW11](#bib.bibx11)]
    的研究表明，这种类型的分解极大地提高了近似精度和表达能力，超越了仅稀疏或低秩矩阵的范围。其成功已扩展到深度学习领域，如高效注意力 [[CDW^+21](#bib.bibx6)],
    模型压缩 [[LYZ^+23](#bib.bibx33)] 和微调 [[NTA24](#bib.bibx37)]。同样，我们在设计中受到这些工作的启发。
- en: 3 Method
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: '![Refer to caption](img/41317eb2b5de58c1c05e9279d27fd509.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/41317eb2b5de58c1c05e9279d27fd509.png)'
- en: 'Figure 4: LESS algorithm during inference. At each decoding step, attention
    is calculated as in ([3](#S3.E3 "In Attention Calculation. ‣ 3.1 KV Caching with
    LESS ‣ 3 Method ‣ Get More with LESS: Synthesizing Recurrence with KV Cache Compression
    for Efficient LLM Inference")). To prepare for the next decoding step, the cache
    is updated by placing the most recent KV pair into the sparse policy cache, and
    if it has exceeded capacity, a KV pair will be evicted and integrated into the
    low-rank cache $\bm{H}_{t}$ before being deleted.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4：推理过程中 LESS 算法。在每个解码步骤中，注意力计算如 ([3](#S3.E3 "In Attention Calculation. ‣
    3.1 KV Caching with LESS ‣ 3 Method ‣ Get More with LESS: Synthesizing Recurrence
    with KV Cache Compression for Efficient LLM Inference")) 所示。为了准备下一步解码，缓存通过将最新的
    KV 对放入稀疏策略缓存中来更新，如果超出容量，则一个 KV 对将被驱逐，并在删除之前集成到低秩缓存 $\bm{H}_{t}$ 中。'
- en: 'When we convert the intuition in Section [2](#S2 "2 Background & Intuition
    ‣ Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient
    LLM Inference") into an algorithm, a couple technical challenges arise. One challenge
    is finding an effective way to mix attention probabilities produced by sparse
    policies and low-rank kernels. Second, we need to design a framework general enough
    to work with a broad class of sparse policies. In some cases, different sparse
    policies may be preferable, so our method should be compatible with many sparse
    policies. Third, our method should be cheap compute to develop. We show that LESS
    overcomes all these challenges in a two step process: attention computation followed
    by cache updates.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '当我们将第[2](#S2 "2 Background & Intuition ‣ Get More with LESS: Synthesizing Recurrence
    with KV Cache Compression for Efficient LLM Inference")节中的直觉转换为算法时，会出现几个技术挑战。一个挑战是找到一种有效的方式来混合由稀疏策略和低秩核生成的注意力概率。第二，我们需要设计一个足够通用的框架，以适应广泛的稀疏策略。在某些情况下，不同的稀疏策略可能更为合适，因此我们的方法应该与许多稀疏策略兼容。第三，我们的方法应该便于计算开发。我们展示了
    LESS 如何通过两个步骤的过程克服所有这些挑战：注意力计算，随后是缓存更新。'
- en: 3.1 KV Caching with LESS
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 使用 LESS 的 KV 缓存
- en: We propose LESS, a general method to synthesize low-rank caches with any eviction-based
    sparse KV cache policy, $\mathfrak{C}$ is the number of cached pairs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了 LESS，一种通用的方法，用于与任何基于驱逐的稀疏 KV 缓存策略合成低秩缓存，其中 $\mathfrak{C}$ 是缓存对的数量。
- en: Letting $\operatorname{\bm{\cdot}}$, we define our kernels as
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 设定 $\operatorname{\bm{\cdot}}$，我们将核定义为
- en: '|  | $\displaystyle\phi(\bm{q})$ |  | (1) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi(\bm{q})$ |  | (1) |'
- en: '|  | $\displaystyle\psi(\bm{k})$ |  | (2) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\psi(\bm{k})$ |  | (2) |'
- en: for activation functions $\sigma_{\operatorname{\bm{\cdot}}}$, then the result
    would be the original attention probabilities.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于激活函数 $\sigma_{\operatorname{\bm{\cdot}}}$，那么结果将是原始的注意力概率。
- en: Attention Calculation.
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力计算。
- en: 'Now, we describe the attention calculation procedure summarized in Algorithm [1](#alg1
    "Algorithm 1 ‣ Cache Updates. ‣ 3.1 KV Caching with LESS ‣ 3 Method ‣ Get More
    with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM
    Inference"). At step $t$, by computing'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，我们描述在算法[1](#alg1 "Algorithm 1 ‣ Cache Updates. ‣ 3.1 KV Caching with LESS
    ‣ 3 Method ‣ Get More with LESS: Synthesizing Recurrence with KV Cache Compression
    for Efficient LLM Inference")中总结的注意力计算过程。在步骤 $t$ 时，通过计算'
- en: '|  | $1$2 |  | (3) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: During the prompting phase (i.e. $t=0$.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示阶段（即 $t=0$）。
- en: Cache Updates.
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缓存更新。
- en: 'With the attention computed, we need to prepare the necessary ingredients for
    iteration $t+1$:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算出注意力之后，我们需要准备好 $t+1$ 迭代所需的必要条件：
- en: '|  | $\displaystyle\bm{H}_{t+1}$ |  | (4) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{H}_{t+1}$ |  | (4) |'
- en: '|  | $\displaystyle\bm{z}_{t+1}$ |  | (5) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{z}_{t+1}$ |  | (5) |'
- en: 'After this, $\mathcal{D}_{t+1}$ are updated recursively by keys and values
    that have been newly pruned at each decoding step. As such, they are constant
    size repositories of information from all deleted KV pairs which becomes clear
    when we expand the recursion:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，$\mathcal{D}_{t+1}$ 会根据每个解码步骤中新修剪的键和值递归更新。因此，它们是所有已删除 KV 对的信息的常量大小存储库，当我们展开递归时这一点变得清晰：
- en: '|  | $\displaystyle\bm{H}_{t+1}$ |  | (6) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{H}_{t+1}$ |  | (6) |'
- en: and similarly for $\bm{z}_{t+1}$.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以及类似地对 $\bm{z}_{t+1}$。
- en: Algorithm 1 Generation Step with LESS
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 使用 LESS 的生成步骤
- en: 'Input: $\mathfrak{C},\bm{q}_{t},\bm{k}_{t},\bm{v}_{t}$'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$\mathfrak{C},\bm{q}_{t},\bm{k}_{t},\bm{v}_{t}$
- en: 3.2 Implementation Details
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 实现细节
- en: Inexpensive Training.
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 低成本训练。
- en: With our inference-time protocol outlined, we now describe how we can cheaply
    train our kernel functions $\phi$, this does not impede parallelism along the
    sequence axis because we can just construct the full attention matrix where entries
    not computed by sparsely cached KV pairs, as determined by whichever sparse policy
    we train on, will be found by the kernel functions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在概述了我们的推理时间协议之后，我们现在描述如何便宜地训练我们的内核函数$\phi$，这不会妨碍沿序列轴的并行性，因为我们可以构建完整的注意力矩阵，其中未由稀疏缓存KV对计算的条目，将由内核函数找到。
- en: All training runs used identical hyperparameters for simplicity. LESS was trained
    using Adam [[KB14](#bib.bibx24)] for 40 epochs with an initial learning rate of
    0.001 which halved every 10 epochs. We fixed the hidden layer dimension $R^{\prime}=512$
    to aggregate attention scores across all query attention heads to determine KV
    pairs to evict.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所有训练运行使用相同的超参数以简化操作。LESS使用Adam[[KB14](#bib.bibx24)]进行训练，共40个周期，初始学习率为0.001，每10个周期减少一半。我们固定隐藏层维度$R^{\prime}=512$，以聚合所有查询注意力头的注意力分数，以确定要驱逐的KV对。
- en: 'We find that the kernel initialization is critical. As we will show in our
    experiments (Section [4](#S4 "4 Experiments ‣ Get More with LESS: Synthesizing
    Recurrence with KV Cache Compression for Efficient LLM Inference")), the sparse
    policies already have decent performance which we want to use as a starting point.
    As such, we add learnable scalars between layers in $\psi$, so the influence of
    LESS during the first few gradient steps is small. In this way, the sparse policy
    acts as a warm start, and we can immediately reduce the sparse policy’s residual.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '我们发现内核初始化至关重要。正如我们在实验中展示的（第[4](#S4 "4 Experiments ‣ Get More with LESS: Synthesizing
    Recurrence with KV Cache Compression for Efficient LLM Inference")节），稀疏策略已经具有不错的性能，我们希望将其作为起点。因此，我们在$\psi$中的层之间添加了可学习的标量，使LESS在前几个梯度步骤中的影响较小。这样，稀疏策略作为一个热启动，我们可以立即减少稀疏策略的残差。'
- en: Efficient Generation.
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高效生成。
- en: We also develop an implementation that enhances throughput and reduces the latency
    of LLM generation of LESS. For the sparse cache, we adapt the implementation from
    Zhang et al. [[ZSZ^+23](#bib.bibx59)] to support any KV cache eviction algorithm
    efficiently. To avoid data movement in memory, we directly replace the evicted
    KV pair with the newly-added one. As our kernels are small MLPs with GELUs, we
    implement a fused linear kernel that absorbs the activation into the layer before
    to avoid writing the intermediate results to DRAM for the low-rank cache.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还开发了一种实现方法，提升了LESS的LLM生成的吞吐量和降低了延迟。对于稀疏缓存，我们从Zhang等人[[ZSZ^+23](#bib.bibx59)]那里改编了实现方法，以高效支持任何KV缓存驱逐算法。为了避免内存中的数据移动，我们直接用新添加的KV对替换被驱逐的KV对。由于我们的内核是小型MLP与GELUs，我们实现了一种融合的线性内核，将激活吸收到前一层中，从而避免将中间结果写入DRAM，以适应低秩缓存。
- en: 4 Experiments
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'Here, we demonstrate the impressive performance of LESS across multiple datasets,
    models (Llama 2 and Falcon), sparse policies [[ZSZ^+23](#bib.bibx59), [HWX^+23](#bib.bibx22),
    [XTC^+23](#bib.bibx57)], and sparsity levels, despite allocating only approximately
    4 tokens of storage to the low-rank state. In Section [4.1](#S4.SS1 "4.1 Language
    Modeling & Classification ‣ 4 Experiments ‣ Get More with LESS: Synthesizing Recurrence
    with KV Cache Compression for Efficient LLM Inference"), LESS achieves the closest
    performance to the full cache in language modeling and classification tasks. For
    example, evaluated with $2\%\operatorname{\text{H}_{2}\text{O}}$ higher) compared
    to full caching. Finally, in Section [4.4](#S4.SS4 "4.4 Empirical Analysis and
    Ablations ‣ 4 Experiments ‣ Get More with LESS: Synthesizing Recurrence with KV
    Cache Compression for Efficient LLM Inference"), we discuss different characteristics
    of LESS, namely the recovery of true attention probabilities, kernel size scaling,
    and capabilities for long sequences.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们展示了LESS在多个数据集、模型（Llama 2 和 Falcon）、稀疏策略[[ZSZ^+23](#bib.bibx59), [HWX^+23](#bib.bibx22),
    [XTC^+23](#bib.bibx57)]和稀疏级别上的出色性能，尽管只为低秩状态分配了大约4个令牌的存储。在第[4.1](#S4.SS1 "4.1 Language
    Modeling & Classification ‣ 4 Experiments ‣ Get More with LESS: Synthesizing Recurrence
    with KV Cache Compression for Efficient LLM Inference")节中，LESS在语言建模和分类任务中达到了与完整缓存最接近的性能。例如，评估结果显示，$2\%\operatorname{\text{H}_{2}\text{O}}$相比于完整缓存的性能更高。最后，在第[4.4](#S4.SS4
    "4.4 Empirical Analysis and Ablations ‣ 4 Experiments ‣ Get More with LESS: Synthesizing
    Recurrence with KV Cache Compression for Efficient LLM Inference")节中，我们讨论了LESS的不同特性，包括真实注意力概率的恢复、内核大小缩放以及对长序列的能力。'
- en: We explore two sparse policies, $\operatorname{\text{H}_{2}\text{O}}$ can have
    an even split.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了两种稀疏策略，$\operatorname{\text{H}_{2}\text{O}}$ 可以进行均匀分割。
- en: 'Table 1: Token counts at different sparsity levels.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同稀疏级别下的代币计数。
- en: '| Model | Max Length | # Tokens at 2%/5%/10% |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 最大长度 | 2%/5%/10% 代币数量 |'
- en: '| --- | --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Llama 2 | 4096 | 80 / 204 / 408 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 | 4096 | 80 / 204 / 408 |'
- en: '| Falcon | 2048 | 40 / 102 / 204 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Falcon | 2048 | 40 / 102 / 204 |'
- en: For our experiments, we set the kernel size $R=8$ is the percent cache sparsity
    LESS was trained on with some sparse policy depending on the context.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们设置了内核大小 $R=8$，这是LESS在某些稀疏策略下训练的百分比缓存稀疏度。
- en: '![Refer to caption](img/9a1b5a68ad70c4f354dfc148de168d1a.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9a1b5a68ad70c4f354dfc148de168d1a.png)'
- en: 'Figure 5: Experimental setup. First, a sparse policy is chosen as the underlying
    policy behind all methods. Then, we compare performance among the full cache model,
    Baseline, Baseline+, and LESS. Baseline+ and LESS use the same amount of storage
    which is slightly larger than the requirements of Baseline.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：实验设置。首先，选择一个稀疏策略作为所有方法的基础策略。然后，我们比较完整缓存模型、基线模型、基线+模型和LESS模型之间的性能。基线+和LESS使用相同数量的存储，比基线略大。
- en: 4.1 Language Modeling & Classification
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 语言建模与分类
- en: We start with validating our method trained at different sparsity levels on
    some language modeling and classification tasks at different sparsity levels using
    Language Modeling Evaluation Harness [[GTA^+23](#bib.bibx19)]. For these tasks,
    we use the same setup as in training by masking out query-key interactions depending
    on the sparse policy and having LESS capture the masked probabilities. In addition,
    we purposefully mismatch training and testing sparsity levels to uncover insight
    on the transferability between test sparsity levels. To illustrate why a learned
    kernel is necessary, we also evaluate $\operatorname{\text{H}_{2}\text{O}}$+Performer.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始验证在不同稀疏级别上训练的方法在一些语言建模和分类任务中的表现，这些任务使用语言建模评估工具 [[GTA^+23](#bib.bibx19)]。对于这些任务，我们使用与训练相同的设置，通过根据稀疏策略屏蔽查询-键交互，并让LESS捕捉屏蔽概率。此外，我们故意不匹配训练和测试的稀疏级别，以揭示测试稀疏级别之间的迁移性。为了说明学习的内核为何必要，我们还评估了
    $\operatorname{\text{H}_{2}\text{O}}$+Performer。
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.1 Language Modeling & Classification ‣ 4 Experiments
    ‣ Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient
    LLM Inference") shows Llama 2 7B performance on WikiText [[MXBS16](#bib.bibx34)]
    and PG-19 [[RPJ^+19](#bib.bibx44), [GBB^+20](#bib.bibx17)] using $\operatorname{\text{H}_{2}\text{O}}$+Performer
    suggests that learned kernels are needed to make a noticeable improvement. Moreover,
    LESS trained at one sparsity level can often generalize reasonably to higher sparsity
    levels especially on WikiText, even sometimes matching the performance of ones
    trained at the test sparsity level. The reverse is less effective but can still
    be better than the baselines. However, all methods are still quite far from the
    full cache performance.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S4.T2 "表2 ‣ 4.1 语言建模与分类 ‣ 4 实验 ‣ 使用 LESS 获取更多：通过 KV 缓存压缩合成递归以提高 LLM 推理效率")
    显示了 Llama 2 7B 在 WikiText [[MXBS16](#bib.bibx34)] 和 PG-19 [[RPJ^+19](#bib.bibx44),
    [GBB^+20](#bib.bibx17)] 上的表现，使用 $\operatorname{\text{H}_{2}\text{O}}$+Performer
    表明需要学习的内核才能取得显著的改进。此外，LESS 在一个稀疏级别下训练的模型通常能够合理地推广到更高的稀疏级别，尤其是在 WikiText 上，有时甚至能够与在测试稀疏级别训练的模型性能相匹配。相反的效果较差，但仍然可以比基线方法更好。然而，所有方法的表现仍然远低于完整缓存性能。
- en: 'Table 2: Llama 2 7B WikiText and PG-19 word perplexities with $\operatorname{\text{H}_{2}\text{O}}$
    as the primary underlying sparse policy. Numeric column names indicate the sparsity
    levels during test time. Lower is better.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：Llama 2 7B WikiText 和 PG-19 使用 $\operatorname{\text{H}_{2}\text{O}}$ 作为主要稀疏策略的词困惑度。数字列名表示测试时的稀疏级别。数值越低越好。
- en: '| $\operatorname{\text{H}_{2}\text{O}}$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| $\operatorname{\text{H}_{2}\text{O}}$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| WikiText |  |  |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| WikiText |  |  |  |'
- en: '| Full Cache | 8.791 | 8.791 | 8.791 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 完整缓存 | 8.791 | 8.791 | 8.791 |'
- en: '| Baseline | 13.333 | 9.863 | 9.295 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Baseline | 13.333 | 9.863 | 9.295 |'
- en: '| Baseline+ | 12.718 | 9.842 | 9.288 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Baseline+ | 12.718 | 9.842 | 9.288 |'
- en: '| $\operatorname{\text{H}_{2}\text{O}}$+Performer | 13.332 | 9.863 | 9.296
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| $\operatorname{\text{H}_{2}\text{O}}$+Performer | 13.332 | 9.863 | 9.296
    |'
- en: '| LESS (2%) | 10.745 | 9.658 | 9.261 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| LESS (2%) | 10.745 | 9.658 | 9.261 |'
- en: '| LESS (5%) | 11.321 | 9.657 | 9.239 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| LESS (5%) | 11.321 | 9.657 | 9.239 |'
- en: '| LESS (10%) | 14.577 | 9.693 | 9.230 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| LESS (10%) | 14.577 | 9.693 | 9.230 |'
- en: '| PG-19 |  |  |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| PG-19 |  |  |  |'
- en: '| Full Cache | 23.787 | 23.787 | 23.787 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 完整缓存 | 23.787 | 23.787 | 23.787 |'
- en: '| Baseline | 37.013 | 27.939 | 25.451 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 37.013 | 27.939 | 25.451 |'
- en: '| Baseline+ | 35.832 | 27.829 | 25.429 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 基线+ | 35.832 | 27.829 | 25.429 |'
- en: '| $\operatorname{\text{H}_{2}\text{O}}$+Performer | 36.996 | 27.938 | 25.451
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| $\operatorname{\text{H}_{2}\text{O}}$+Performer | 36.996 | 27.938 | 25.451
    |'
- en: '| LESS (2%) | 32.157 | 27.887 | 26.322 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| LESS (2%) | 32.157 | 27.887 | 26.322 |'
- en: '| LESS (5%) | 33.195 | 27.089 | 25.979 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| LESS (5%) | 33.195 | 27.089 | 25.979 |'
- en: '| LESS (10%) | 41.204 | 27.201 | 25.134 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| LESS (10%) | 41.204 | 27.201 | 25.134 |'
- en: Evaluation results [[CLC^+19](#bib.bibx8), [CWL^+20](#bib.bibx13)] with $\Lambda$,
    LESS closes the gap from full caching but cannot match the performance completely.
    While LESS is efficacious for language modeling and classification, we also want
    to assess its utility for generation where the KV cache storage becomes a critical
    bottleneck.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 评价结果 [[CLC^+19](#bib.bibx8), [CWL^+20](#bib.bibx13)] 表明，LESS 缩小了与全缓存的差距，但不能完全匹配其性能。尽管
    LESS 在语言建模和分类中有效，我们还希望评估其在生成中的实用性，因为 KV 缓存存储成为一个关键瓶颈。
- en: 'Table 3: Llama 2 7B performance on WikiText (word perplexity), MuTual (16-shot
    R@1), and BoolQ (10-shot accuracy) with 5% $\Lambda$-masking as the primary underlying
    sparse policy.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: Llama 2 7B 在 WikiText（词困惑度）、MuTual（16-shot R@1）和 BoolQ（10-shot 准确率）上的表现，使用
    5% $\Lambda$-掩码作为主要的稀疏策略。'
- en: '| $\Lambda$) | MuTual | BoolQ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| $\Lambda$) | MuTual | BoolQ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Full Cache | 8.79 | 55.08 | 80.40 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 全缓存 | 8.79 | 55.08 | 80.40 |'
- en: '| Baseline | 10.66 | 53.50 | 77.28 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 10.66 | 53.50 | 77.28 |'
- en: '| Baseline+ | 10.64 | 53.27 | 77.46 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 基线+ | 10.64 | 53.27 | 77.46 |'
- en: '| LESS (5%) | 10.12 | 54.51 | 78.65 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| LESS (5%) | 10.12 | 54.51 | 78.65 |'
- en: 4.2 Summarization
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 摘要
- en: Now, we move on to generation, specifically summarization, to test the ability
    to generate longer and coherent sequences by synthesizing numerous tokens. Unlike
    in our language modeling evaluations, the model will have access to all tokens
    during the prompting phase with the sparse policy and LESS only kicking in during
    the subsequent generation steps. Consequently, generation sparse policies are
    fundamentally different from the language modeling masks LESS is trained on, yet
    despite this, we show that our method maintains its superior performance.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们转向生成，特别是摘要，测试通过合成大量令牌生成更长且连贯序列的能力。与语言建模评估不同，模型将在提示阶段访问所有令牌，稀疏策略和 LESS 仅在随后的生成步骤中生效。因此，生成稀疏策略本质上不同于
    LESS 训练时的语言建模掩码，但我们展示了我们的方法保持了其卓越的性能。
- en: 'Table 4: Llama 2 13B and Falcon 7B generation quality comparison on CNN/DailyMail
    and XSum with 408 sparse tokens (10% and 20% of the context lengths of Llama 2
    and Falcon, respectively) with $\operatorname{\text{H}_{2}\text{O}}$ as the primary
    underlying sparse policy. Llama 2 13B is given 5 shots while Falcon 7B is given
    3 shots due to its shorter context length. Values are in the format [Rouge-1/2/L].'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: Llama 2 13B 和 Falcon 7B 在 CNN/DailyMail 和 XSum 上的生成质量比较，其中 408 个稀疏令牌（分别占
    Llama 2 和 Falcon 上下文长度的 10% 和 20%）使用 $\operatorname{\text{H}_{2}\text{O}}$ 作为主要的稀疏策略。Llama
    2 13B 使用了 5 次，而 Falcon 7B 因上下文长度较短使用了 3 次。值的格式为 [Rouge-1/2/L]。'
- en: '| $\operatorname{\text{H}_{2}\text{O}}$ Method | CNN/DailyMail | XSum |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| $\operatorname{\text{H}_{2}\text{O}}$ 方法 | CNN/DailyMail | XSum |'
- en: '| Llama 2 13B |  |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 13B |  |  |'
- en: '| Full Cache | 27.55/9.96/25.80 | 33.14/13.05/27.33 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 全缓存 | 27.55/9.96/25.80 | 33.14/13.05/27.33 |'
- en: '| Baseline | 23.57/7.35/22.04 | 33.09/13.09/27.44 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 23.57/7.35/22.04 | 33.09/13.09/27.44 |'
- en: '| Baseline+ | 23.40/7.31/21.88 | 33.09/13.06/27.41 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 基线+ | 23.40/7.31/21.88 | 33.09/13.06/27.41 |'
- en: '| LESS (2%) | 25.27/7.76/23.64 | 33.40/12.98/27.41 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| LESS (2%) | 25.27/7.76/23.64 | 33.40/12.98/27.41 |'
- en: '| LESS (5%) | 24.45/7.70/22.87 | 33.15/13.02/27.39 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| LESS (5%) | 24.45/7.70/22.87 | 33.15/13.02/27.39 |'
- en: '| Falcon 7B |  |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Falcon 7B |  |  |'
- en: '| Full Cache | 25.92/8.52/24.15 | 27.17/8.83/22.67 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 全缓存 | 25.92/8.52/24.15 | 27.17/8.83/22.67 |'
- en: '| Baseline | 21.26/5.95/19.73 | 24.50/7.65/20.50 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 21.26/5.95/19.73 | 24.50/7.65/20.50 |'
- en: '| Baseline+ | 21.31/6.16/19.75 | 24.55/7.66/20.56 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 基线+ | 21.31/6.16/19.75 | 24.55/7.66/20.56 |'
- en: '| LESS (5%) | 23.00/6.28/21.28 | 24.94/8.17/20.94 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| LESS (5%) | 23.00/6.28/21.28 | 24.94/8.17/20.94 |'
- en: '| LESS (10%) | 23.22/6.37/21.53 | 25.21/8.28/21.17 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| LESS (10%) | 23.22/6.37/21.53 | 25.21/8.28/21.17 |'
- en: 'Table 5: Llama 2 7B performance on MultiNews (1-shot), CNN/DailyNews (5 shot),
    and XSum (5-shot) with 5% and 10% $\operatorname{\text{H}_{2}\text{O}}$ as the
    primary underlying test sparse policies. Values are in the format [Rouge-1]/[Rouge-2]/[Rouge-L].'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: Llama 2 7B 在 MultiNews（1-shot）、CNN/DailyNews（5-shot）和 XSum（5-shot）上的表现，使用
    5% 和 10% $\operatorname{\text{H}_{2}\text{O}}$ 作为主要的测试稀疏策略。值的格式为 [Rouge-1]/[Rouge-2]/[Rouge-L]。'
- en: '| $\operatorname{\text{H}_{2}\text{O}}$ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| $\operatorname{\text{H}_{2}\text{O}}$ |'
- en: '| MultiNews |  |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| MultiNews |  |  |'
- en: '| Full Cache | 23.79/6.87/21.35 | 23.79/6.87/21.35 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 全缓存 | 23.79/6.87/21.35 | 23.79/6.87/21.35 |'
- en: '| Baseline | 13.38/3.25/12.25 | 19.44/4.97/17.73 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 13.38/3.25/12.25 | 19.44/4.97/17.73 |'
- en: '| Baseline+ | 13.58/3.32/12.41 | 19.44/4.96/17.72 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 基线+ | 13.58/3.32/12.41 | 19.44/4.96/17.72 |'
- en: '| LESS (2%) | 15.31/3.73/14.03 | 20.32/5.24/18.51 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| LESS (2%) | 15.31/3.73/14.03 | 20.32/5.24/18.51 |'
- en: '| LESS (5%) | 15.42/3.80/14.14 | 20.55/5.29/18.70 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| LESS (5%) | 15.42/3.80/14.14 | 20.55/5.29/18.70 |'
- en: '| CNN/DailyMail |  |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| CNN/DailyMail |  |  |'
- en: '| Full Cache | 26.25/9.34/24.40 | 26.25/9.34/24.40 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 全缓存 | 26.25/9.34/24.40 | 26.25/9.34/24.40 |'
- en: '| Baseline | 18.18/4.92/16.89 | 20.04/6.09/18.66 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 18.18/4.92/16.89 | 20.04/6.09/18.66 |'
- en: '| Baseline+ | 18.24/4.91/16.85 | 20.15/6.21/18.73 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 基线+ | 18.24/4.91/16.85 | 20.15/6.21/18.73 |'
- en: '| LESS (2%) | 18.71/5.40/17.34 | 20.76/6.40/19.32 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| LESS (2%) | 18.71/5.40/17.34 | 20.76/6.40/19.32 |'
- en: '| LESS (5%) | 19.21/5.44/17.80 | 22.29/6.85/20.69 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| LESS (5%) | 19.21/5.44/17.80 | 22.29/6.85/20.69 |'
- en: '| XSum |  |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| XSum |  |  |'
- en: '| Full Cache | 30.65/11.11/25.40 | 30.65/11.11/25.40 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 全缓存 | 30.65/11.11/25.40 | 30.65/11.11/25.40 |'
- en: '| Baseline | 29.03/10.77/24.28 | 30.68/11.54/25.58 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 29.03/10.77/24.28 | 30.68/11.54/25.58 |'
- en: '| Baseline+ | 28.94/10.78/24.15 | 30.64/11.49/25.59 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 基线+ | 28.94/10.78/24.15 | 30.64/11.49/25.59 |'
- en: '| LESS (2%) | 30.72/11.53/25.57 | 30.34/10.98/25.31 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| LESS (2%) | 30.72/11.53/25.57 | 30.34/10.98/25.31 |'
- en: '| LESS (5%) | 30.03/11.19/25.03 | 30.82/11.17/25.56 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| LESS (5%) | 30.03/11.19/25.03 | 30.82/11.17/25.56 |'
- en: 'In Tables [4](#S4.T4 "Table 4 ‣ 4.2 Summarization ‣ 4 Experiments ‣ Get More
    with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM
    Inference") and [5](#S4.T5 "Table 5 ‣ 4.2 Summarization ‣ 4 Experiments ‣ Get
    More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient
    LLM Inference"), we see LESS achieves better ROUGE [[Lin04](#bib.bibx30)] scores
    than purely $\operatorname{\text{H}_{2}\text{O}}$ underperforms compared to the
    full cache. Like in language modeling, we again see that the improvement from
    Baseline to Baseline+ pales in comparison to the improvement induced by LESS,
    sometimes even matching the full cache performance as in XSum. Again, we also
    see the transferability of LESS to other sparsity levels. See Appendix [B](#A2
    "Appendix B Generation Outputs ‣ Get More with LESS: Synthesizing Recurrence with
    KV Cache Compression for Efficient LLM Inference") for example generation outputs.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [4](#S4.T4 "表 4 ‣ 4.2 摘要 ‣ 4 实验 ‣ 通过 LESS 获得更多：利用 KV 缓存压缩合成递归以实现高效的 LLM 推理")
    和 [5](#S4.T5 "表 5 ‣ 4.2 摘要 ‣ 4 实验 ‣ 通过 LESS 获得更多：利用 KV 缓存压缩合成递归以实现高效的 LLM 推理")
    中，我们看到 LESS 相较于完全缓存表现出更好的 ROUGE [[Lin04](#bib.bibx30)] 得分。与全缓存相比，纯粹的 $\operatorname{\text{H}_{2}\text{O}}$
    表现较差。与语言建模中的情况一样，我们再次看到从基线到基线+的改进相较于 LESS 引起的改进相形见绌，有时甚至与全缓存性能相匹配，如在 XSum 中。我们还看到
    LESS 对其他稀疏性水平的可转移性。有关生成输出的示例，请参见附录 [B](#A2 "附录 B 生成输出 ‣ 通过 LESS 获得更多：利用 KV 缓存压缩合成递归以实现高效的
    LLM 推理")。
- en: 4.3 Latency and Throughput
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 延迟与吞吐量
- en: 'Table 6: Llama 2 7B and 13B’s generation throughput (tokens/s) and latency
    (s) on an A100 GPU. In the sequence length column, we use "5000 + 5000" to denote
    a prompt length of 5000 and a generation length of 5000\. "OOM" stands for out-of-memory.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：Llama 2 7B 和 13B 在 A100 GPU 上的生成吞吐量（tokens/s）和延迟（s）。在序列长度列中，我们使用“5000 +
    5000”表示 5000 的提示长度和 5000 的生成长度。“OOM”表示内存溢出。
- en: '| Seq. length | Model size | Batch size | Metric | Full Cache | Baseline+ |
    LESS (5%) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 序列长度 | 模型大小 | 批量大小 | 指标 | 全缓存 | 基线+ | LESS (5%) |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 5000+5000 | 13B | 4 | latency | 257.3 | 185.2 | 204.7 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 5000+5000 | 13B | 4 | 延迟 | 257.3 | 185.2 | 204.7 |'
- en: '| 2048+2048 | 7B | 24 | latency | 116.7 | 78.3 | 95.1 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 2048+2048 | 7B | 24 | 延迟 | 116.7 | 78.3 | 95.1 |'
- en: '| 2048+2048 | 7B | 24 | throughput | 421.2 | 627.7 | 516.9 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 2048+2048 | 7B | 24 | 吞吐量 | 421.2 | 627.7 | 516.9 |'
- en: '| 2048+2048 | 7B | 64 | throughput | OOM | 819.2 | 699.2 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 2048+2048 | 7B | 64 | 吞吐量 | OOM | 819.2 | 699.2 |'
- en: Following Sheng et al. [[SZY^+23](#bib.bibx51)], we benchmark the generation
    throughput and latency of LESS on an NVIDIA A100 80G GPU using FP16 precision.
    We focus on the Llama 2 7B and 13B models, with all speedup results tested end-to-end
    with both prompting and generation phases. To measure its performance when generating
    long sequences or inputting large batch sizes, we use synthetic datasets where
    all prompts are padded to the same length and batched together. The same number
    of tokens are generated for each prompt. We test different combinations of prompt
    and generation lengths.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Sheng等人[[SZY^+23](#bib.bibx51)]，我们在使用FP16精度的NVIDIA A100 80G GPU上基准测试了LESS的生成吞吐量和延迟。我们关注于Llama
    2 7B和13B模型，所有的加速结果都在提示和生成阶段的端到端测试中得到验证。为了测量其生成长序列或输入大批量时的性能，我们使用了合成数据集，其中所有的提示都被填充到相同的长度并一起批量处理。每个提示生成相同数量的令牌。我们测试了不同的提示和生成长度组合。
- en: 'Table [6](#S4.T6 "Table 6 ‣ 4.3 Latency and Throughput ‣ 4 Experiments ‣ Get
    More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient
    LLM Inference") shows results with sequence lengths from 4K to 10K. With the same
    batch size, LESS reduces the latency by $1.1-1.3\times$.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表[6](#S4.T6 "表 6 ‣ 4.3 延迟与吞吐量 ‣ 4 实验 ‣ 通过KV缓存压缩进行更高效的LLM推理")显示了序列长度从4K到10K的结果。在相同的批量大小下，LESS将延迟减少了$1.1-1.3\times$。
- en: 4.4 Empirical Analysis and Ablations
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 实证分析与消融实验
- en: Now that we have shown that LESS is simple and effective, we share some interesting
    characteristics of our method.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经展示了LESS的简单和有效性，我们分享一些我们方法的有趣特点。
- en: Reconstructing Attention Probabilities.
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重建注意力概率。
- en: 'Sparse KV cache policies can delete tokens that may be needed later on. A way
    to see this is to construct the sparse attention matrix and compare with the full
    one. In Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Get More with LESS: Synthesizing
    Recurrence with KV Cache Compression for Efficient LLM Inference"), $\operatorname{\text{H}_{2}\text{O}}$
    zeroes out many relatively high attention probabilities with a bias towards keeping
    early tokens. More examples are in Appendix [A](#A1 "Appendix A Attention Matrix
    Visualizations ‣ Get More with LESS: Synthesizing Recurrence with KV Cache Compression
    for Efficient LLM Inference"). Visually, LESS provides a sketch of the deleted
    tokens which appears to reasonably reconstruct trends.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏KV缓存策略可能会删除以后可能需要的令牌。可以通过构建稀疏注意力矩阵并与完整矩阵进行比较来观察这一点。在图[1](#S1.F1 "图 1 ‣ 1 引言
    ‣ 通过KV缓存压缩进行更高效的LLM推理")中，$\operatorname{\text{H}_{2}\text{O}}$将许多相对较高的注意力概率归零，倾向于保留早期的令牌。更多示例见附录[A](#A1
    "附录 A 注意力矩阵可视化 ‣ 通过KV缓存压缩进行更高效的LLM推理")。从视觉上看，LESS提供了删除令牌的轮廓，这些轮廓似乎合理地重建了趋势。
- en: Numerically, we measure the similarity of each row in the attention matrix with
    corresponding rows produced by $\operatorname{\text{H}_{2}\text{O}}$, is defined
    as
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 数值上，我们测量了注意力矩阵中每一行与$\operatorname{\text{H}_{2}\text{O}}$生成的对应行的相似性，定义为
- en: '|  | $\displaystyle\mathcal{H}(\bm{p},\bm{q})\coloneqq\&#124;\sqrt{\bm{p}}-\sqrt{\bm{q}}\&#124;_{2}/\sqrt{2}$
    |  | (7) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{H}(\bm{p},\bm{q})\coloneqq\&#124;\sqrt{\bm{p}}-\sqrt{\bm{q}}\&#124;_{2}/\sqrt{2}$
    |  | (7) |'
- en: 'where the square root is elementwise. The value of $\mathcal{H}(\bm{p},\bm{q})$
    ranges from 0 to 1, where a lower value indicates greater similarity. In Figure [6](#S4.F6
    "Figure 6 ‣ Reconstructing Attention Probabilities. ‣ 4.4 Empirical Analysis and
    Ablations ‣ 4 Experiments ‣ Get More with LESS: Synthesizing Recurrence with KV
    Cache Compression for Efficient LLM Inference"), we see that our method more accurately
    replicates the original attention probability distributions as measured by the
    Hellinger distance. We choose to aggregate each layer separately since the attention
    distribution patterns tend to vary dramatically throughout the model.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其中平方根是逐元素的。$\mathcal{H}(\bm{p},\bm{q})$的值范围从0到1，其中值越低表示相似度越高。在图[6](#S4.F6 "图
    6 ‣ 重建注意力概率 ‣ 4.4 实证分析与消融实验 ‣ 4 实验 ‣ 通过KV缓存压缩进行更高效的LLM推理")中，我们看到我们的方法能够更准确地复制原始的注意力概率分布，这一点通过Hellinger距离进行测量。我们选择分别汇总每一层，因为注意力分布模式在整个模型中往往会有显著变化。
- en: '![Refer to caption](img/9822b56f577a1d6fb406c9d98ec088e0.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9822b56f577a1d6fb406c9d98ec088e0.png)'
- en: 'Figure 6: Layer-wise Llama 2 7B mean Hellinger distance from original attention
    probabilities, aggregated across WikiText evaluation samples. The underlying sparse
    policy is $\operatorname{\text{H}_{2}\text{O}}$. Here, LESS is evaluated based
    on their training sparsity percentages.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：Layer-wise Llama 2 7B 从原始注意力概率中计算的均值 Hellinger 距离，汇总于 WikiText 评估样本中。底层稀疏策略是
    $\operatorname{\text{H}_{2}\text{O}}$。这里，LESS 根据它们的训练稀疏性百分比进行评估。
- en: Larger Kernels.
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更大的内核。
- en: In our experiments, we fixed $R=8$ is less than shifting more of the KV cache
    to the sparse policy, suggesting that a small low-rank cache is enough.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们固定 $R=8$ 小于将更多 KV 缓存转移到稀疏策略，这表明小型低秩缓存已经足够。
- en: 'Figure 7: Llama 2 7B WikiText word perplexity (lower is better) as the kernel
    size quadruples, compared against Baseline+ which occupies the same space. The
    sparse KV cache policy is $\operatorname{\text{H}_{2}\text{O}}$.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：Llama 2 7B WikiText 单词困惑度（越低越好）随着内核大小增加四倍，与占据相同空间的 Baseline+ 进行比较。稀疏 KV
    缓存策略是 $\operatorname{\text{H}_{2}\text{O}}$。
- en: Providing Hope for Long Sequences.
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为长序列提供希望。
- en: 'Model performance appears to be highly correlated with the input sequence length
    regardless of the caching method. As shown in Figure [8](#S4.F8 "Figure 8 ‣ Providing
    Hope for Long Sequences. ‣ 4.4 Empirical Analysis and Ablations ‣ 4 Experiments
    ‣ Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient
    LLM Inference"), even the full cache model performance drops dramatically and
    immediately as the prompt length increases. Baseline+ and LESS (1% $\operatorname{\text{H}_{2}\text{O}}$)
    appear to perform similarly for shorter sequences but diverge for longer sequences
    where we see LESS is more performative. This follows our intuition since for sparse
    cache policies, a smaller fraction of KV pairs is saved as the sequence length
    increases, so more information is omitted. This is where a low-rank state can
    help to recover some of this information.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能似乎与输入序列长度高度相关，无论缓存方法如何。如图 [8](#S4.F8 "图 8 ‣ 为长序列提供希望 ‣ 4.4 实证分析与消融 ‣ 4 实验
    ‣ 通过 LESS 获得更多：通过 KV 缓存压缩合成递归以提高 LLM 推理效率") 所示，即使是完整缓存模型的性能也会在提示长度增加时急剧下降。Baseline+
    和 LESS（1% $\operatorname{\text{H}_{2}\text{O}}$）在较短的序列中表现类似，但在较长的序列中表现分化，LESS
    显得更具性能。这符合我们的直觉，因为对于稀疏缓存策略，随着序列长度的增加，保存的 KV 对的比例较小，因此更多的信息被省略。在这里，低秩状态可以帮助恢复这些信息。
- en: '![Refer to caption](img/8e34a885d41f4965003611e99504d37d.png)![Refer to caption](img/059f499ac7bf401a2e79756128773977.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8e34a885d41f4965003611e99504d37d.png)![参考说明](img/059f499ac7bf401a2e79756128773977.png)'
- en: 'Figure 8: Relationship between Rouge-1 score and prompt length for Llama 2
    7B with different cache methods on CNN/DailyMail (left) and XSum (right). The
    test sparse KV cache policy is 5% $\operatorname{\text{H}_{2}\text{O}}$ is 10%
    of the dataset size.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：Llama 2 7B 在 CNN/DailyMail（左）和 XSum（右）上的不同缓存方法的 Rouge-1 分数与提示长度之间的关系。测试稀疏
    KV 缓存策略是 5% $\operatorname{\text{H}_{2}\text{O}}$ 是数据集大小的 10%。
- en: 5 Conclusion and Future Work
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与未来工作
- en: To tackle the KV cache bottleneck, we introduce LESS which has demonstrated
    itself to be an effective way to boost eviction-based KV cache algorithms. Motivated
    by the necessity to maintain information that would have been discarded, the constant-sized
    LESS recovers a significant portion of the performance lost due to maintaining
    a small cache across a variety of scenarios and intensities, despite being cheap
    to train and deploy. There are many exciting avenues of work that can enhance
    LESS or build upon it, such as improving kernel design and investigating the residual
    of LESS. Such directions will further push the performance of a condensed KV cache
    to that of a complete cache, allowing LLMs to accomplish the same tasks with less.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决 KV 缓存瓶颈，我们引入了 LESS，它已证明是一种有效的提升基于驱逐的 KV 缓存算法的方法。受到维护本可能被丢弃的信息的必要性的驱动，固定大小的
    LESS 恢复了由于在各种场景和强度下维持小缓存而丧失的显著性能，尽管它便于训练和部署。许多激动人心的工作方向可以增强 LESS 或在其基础上构建，例如改进内核设计和研究
    LESS 的残差。这些方向将进一步推动浓缩 KV 缓存的性能，接近完整缓存，使 LLM 能够以更少的代价完成相同的任务。
- en: Acknowledgements
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The work of H. Dong is supported in part by the Liang Ji-Dian Graduate Fellowship,
    the Michel and Kathy Doreau Graduate Fellowship in Electrical and Computer Engineering,
    and the Wei Shen and Xuehong Zhang Presidential Fellowship at Carnegie Mellon
    University. The work of Y. Chi is supported in part by the grants NSF DMS-2134080
    and ONR N00014-19-1-2404.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: H. Dong 的工作部分由梁继电研究生奖学金、Michel 和 Kathy Doreau 电气与计算机工程研究生奖学金以及卡内基梅隆大学的 Wei Shen
    和 Xuehong Zhang 总统奖学金资助。Y. Chi 的工作部分由 NSF DMS-2134080 和 ONR N00014-19-1-2404 资助。
- en: References
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[AAA^+23] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru,
    M. Debbah, E. Goffinet, D. Heslow, J. Launay, Q. Malartic, B. Noune, B. Pannier,
    and G. Penedo. Falcon-40B: an open large language model with state-of-the-art
    performance. 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AAA^+23] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru,
    M. Debbah, E. Goffinet, D. Heslow, J. Launay, Q. Malartic, B. Noune, B. Pannier,
    和 G. Penedo. Falcon-40B: 一个具有最先进性能的开放大型语言模型。2023。'
- en: '[ADF^+23] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,
    S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv
    preprint arXiv:2305.10403, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ADF^+23] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,
    S. Shakeri, E. Taropa, P. Bailey, Z. Chen, 等。Palm 2 技术报告。arXiv 预印本 arXiv:2305.10403，2023。'
- en: '[BBC15] BBC. Fracking still opposed in wales, ministers tell councils. The
    British Broadcasting Corporation, 2015.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BBC15] BBC. 威尔士仍然反对水力压裂，部长们告诉地方议会。《英国广播公司》，2015。'
- en: '[BFD^+22] D. Bolya, C.-Y. Fu, X. Dai, P. Zhang, C. Feichtenhofer, and J. Hoffman.
    Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BFD^+22] D. Bolya, C.-Y. Fu, X. Dai, P. Zhang, C. Feichtenhofer, 和 J. Hoffman.
    Token merging: 你的 vit 但更快。arXiv 预印本 arXiv:2210.09461，2022。'
- en: '[Bru15] B. Brumfield. Death toll rises quickly as conflict rages in yemen.
    The Cable News Network, 2015.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bru15] B. Brumfield. 随着冲突在也门的持续，死亡人数迅速上升。《有线电视新闻网》，2015。'
- en: '[CDW^+21] B. Chen, T. Dao, E. Winsor, Z. Song, A. Rudra, and C. Ré. Scatterbrain:
    Unifying sparse and low-rank attention. Advances in Neural Information Processing
    Systems, 34:17413–17426, 2021.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CDW^+21] B. Chen, T. Dao, E. Winsor, Z. Song, A. Rudra, 和 C. Ré. Scatterbrain:
    统一稀疏和低秩注意力。《神经信息处理系统进展》，34:17413–17426，2021。'
- en: '[CGRS19] R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences
    with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CGRS19] R. Child, S. Gray, A. Radford, 和 I. Sutskever. 生成长序列的稀疏变换器。arXiv 预印本
    arXiv:1904.10509，2019。'
- en: '[CLC^+19] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova.
    Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv
    preprint arXiv:1905.10044, 2019.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CLC^+19] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, 和 K. Toutanova.
    Boolq: 探索自然的是/否问题的意外难度。arXiv 预印本 arXiv:1905.10044，2019。'
- en: '[CLD^+20] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos,
    P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with
    performers. arXiv preprint arXiv:2009.14794, 2020.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CLD^+20] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T.
    Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, 等。用 performers 重新思考注意力。arXiv
    预印本 arXiv:2009.14794，2020。'
- en: '[CLMW11] E. J. Candès, X. Li, Y. Ma, and J. Wright. Robust principal component
    analysis? Journal of the ACM (JACM), 58(3):1–37, 2011.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CLMW11] E. J. Candès, X. Li, Y. Ma, 和 J. Wright. 鲁棒主成分分析？《ACM 学报》（JACM），58(3):1–37，2011。'
- en: '[CSPW11] V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A. S. Willsky.
    Rank-sparsity incoherence for matrix decomposition. SIAM Journal on Optimization,
    21(2):572–596, 2011.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CSPW11] V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, 和 A. S. Willsky. 矩阵分解的秩-稀疏不相容性。《SIAM
    优化期刊》，21(2):572–596，2011。'
- en: '[CTTS23] Y. Chen, Q. Tao, F. Tonin, and J. A. Suykens. Primal-attention: Self-attention
    through asymmetric kernel svd in primal representation. arXiv preprint arXiv:2305.19798,
    2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CTTS23] Y. Chen, Q. Tao, F. Tonin, 和 J. A. Suykens. Primal-attention: 通过不对称核
    svd 进行自注意力的原始表示。arXiv 预印本 arXiv:2305.19798，2023。'
- en: '[CWL^+20] L. Cui, Y. Wu, S. Liu, Y. Zhang, and M. Zhou. Mutual: A dataset for
    multi-turn dialogue reasoning. arXiv preprint arXiv:2004.04494, 2020.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CWL^+20] L. Cui, Y. Wu, S. Liu, Y. Zhang, 和 M. Zhou. Mutual: 一个用于多轮对话推理的数据集。arXiv
    预印本 arXiv:2004.04494，2020。'
- en: '[DFS^+22] T. Dao, D. Y. Fu, K. K. Saab, A. W. Thomas, A. Rudra, and C. Ré.
    Hungry hungry hippos: Towards language modeling with state space models. arXiv
    preprint arXiv:2212.14052, 2022.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DFS^+22] T. Dao, D. Y. Fu, K. K. Saab, A. W. Thomas, A. Rudra, 和 C. Ré. Hungry
    hungry hippos: 向着使用状态空间模型的语言建模迈进。arXiv 预印本 arXiv:2212.14052，2022。'
- en: '[FLS^+19] A. R. Fabbri, I. Li, T. She, S. Li, and D. R. Radev. Multi-news:
    a large-scale multi-document summarization dataset and abstractive hierarchical
    model, 2019.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FLS^+19] A. R. Fabbri, I. Li, T. She, S. Li, 和 D. R. Radev. Multi-news: 一个大规模的多文档总结数据集和抽象层次模型，2019。'
- en: '[FZS22] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to
    trillion parameter models with simple and efficient sparsity. The Journal of Machine
    Learning Research, 23(1):5232–5270, 2022.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FZS22] W. Fedus, B. Zoph, 和 N. Shazeer. Switch 变换器: 通过简单高效的稀疏性扩展到万亿参数模型. 机器学习研究杂志,
    23(1):5232–5270, 2022.'
- en: '[GBB^+20] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang,
    H. He, A. Thite, N. Nabeshima, et al. The pile: An 800gb dataset of diverse text
    for language modeling. arXiv preprint arXiv:2101.00027, 2020.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GBB^+20] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J.
    Phang, H. He, A. Thite, N. Nabeshima, 等. The pile: 一个 800GB 的多样文本数据集用于语言建模. arXiv
    预印本 arXiv:2101.00027, 2020.'
- en: '[GD23] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective
    state spaces. arXiv preprint arXiv:2312.00752, 2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GD23] A. Gu 和 T. Dao. Mamba: 具有选择状态空间的线性时间序列建模. arXiv 预印本 arXiv:2312.00752,
    2023.'
- en: '[GTA^+23] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster,
    L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa,
    J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite,
    B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation,
    12 2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GTA^+23] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster,
    L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa,
    J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite,
    B. Wang, K. Wang, 和 A. Zou. 一种少样本语言模型评估框架, 2023年12月.'
- en: '[GZL^+23] S. Ge, Y. Zhang, L. Liu, M. Zhang, J. Han, and J. Gao. Model tells
    you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801,
    2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GZL^+23] S. Ge, Y. Zhang, L. Liu, M. Zhang, J. Han, 和 J. Gao. 模型告诉你该丢弃什么:
    针对 LLMs 的自适应 KV 缓存压缩. arXiv 预印本 arXiv:2310.01801, 2023.'
- en: '[HKG^+15] K. M. Hermann, T. KociskÃœ, E. Grefenstette, L. Espeholt, W. Kay,
    M. Suleyman, and P. Blunsom. Teaching machines to read and comprehend. In NIPS,
    pages 1693–1701, 2015.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HKG^+15] K. M. Hermann, T. KociskÃœ, E. Grefenstette, L. Espeholt, W. Kay,
    M. Suleyman, 和 P. Blunsom. 教授机器阅读和理解. 见于 NIPS, 页1693–1701, 2015.'
- en: '[HWX^+23] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, and S. Wang. Lm-infinite:
    Simple on-the-fly length generalization for large language models. arXiv preprint
    arXiv:2308.16137, 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HWX^+23] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, 和 S. Wang. LM-infinite:
    针对大型语言模型的简单即时长度泛化. arXiv 预印本 arXiv:2308.16137, 2023.'
- en: '[JSR^+24] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford,
    D. S. Chaplot, D. de las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour,
    G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux, P. Stock, S. Subramanian,
    S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, and
    W. E. Sayed. Mixtral of experts, 2024.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[JSR^+24] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford,
    D. S. Chaplot, D. de las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour,
    G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux, P. Stock, S. Subramanian,
    S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, 和
    W. E. Sayed. Mixtral of experts, 2024.'
- en: '[KB14] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.
    arXiv preprint arXiv:1412.6980, 2014.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KB14] D. P. Kingma 和 J. Ba. Adam: 一种随机优化方法. arXiv 预印本 arXiv:1412.6980, 2014.'
- en: '[KHQJ18] U. Khandelwal, H. He, P. Qi, and D. Jurafsky. Sharp nearby, fuzzy
    far away: How neural language models use context. arXiv preprint arXiv:1805.04623,
    2018.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KHQJ18] U. Khandelwal, H. He, P. Qi, 和 D. Jurafsky. 近处锐利，远处模糊: 神经语言模型如何使用上下文.
    arXiv 预印本 arXiv:1805.04623, 2018.'
- en: '[KNH^+22] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah.
    Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s):1–41,
    2022.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KNH^+22] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, 和 M. Shah.
    视觉中的变换器: 一项调查. ACM 计算调查 (CSUR), 54(10s):1–41, 2022.'
- en: '[KPZ^+21] J. Kasai, H. Peng, Y. Zhang, D. Yogatama, G. Ilharco, N. Pappas,
    Y. Mao, W. Chen, and N. A. Smith. Finetuning pretrained transformers into rnns.
    arXiv preprint arXiv:2103.13076, 2021.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KPZ^+21] J. Kasai, H. Peng, Y. Zhang, D. Yogatama, G. Ilharco, N. Pappas,
    Y. Mao, W. Chen, 和 N. A. Smith. 将预训练变换器微调为 RNNs. arXiv 预印本 arXiv:2103.13076, 2021.'
- en: '[KVPF20] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers
    are rnns: Fast autoregressive transformers with linear attention. In International
    conference on machine learning, pages 5156–5165\. PMLR, 2020.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KVPF20] A. Katharopoulos, A. Vyas, N. Pappas, 和 F. Fleuret. 变换器即 RNNs: 快速自回归变换器与线性注意力.
    见于国际机器学习会议, 页5156–5165. PMLR, 2020.'
- en: '[LDL^+23] Z. Liu, A. Desai, F. Liao, W. Wang, V. Xie, Z. Xu, A. Kyrillidis,
    and A. Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis
    for llm kv cache compression at test time. arXiv preprint arXiv:2305.17118, 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LDL^+23] Z. Liu, A. Desai, F. Liao, W. Wang, V. Xie, Z. Xu, A. Kyrillidis,
    和 A. Shrivastava. Scissorhands: 利用重要性持久性假设进行 LLM KV 缓存压缩. arXiv 预印本 arXiv:2305.17118,
    2023.'
- en: '[Lin04] C.-Y. Lin. ROUGE: A package for automatic evaluation of summaries.
    In Text Summarization Branches Out, pages 74–81, Barcelona, Spain, July 2004\.
    Association for Computational Linguistics.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lin04] C.-Y. Lin. ROUGE：自动评估摘要的工具包。在《文本摘要扩展》一书中，第74–81页，西班牙巴塞罗那，2004年7月。计算语言学协会。'
- en: '[LLD^+23] Y. Liu, H. Li, K. Du, J. Yao, Y. Cheng, Y. Huang, S. Lu, M. Maire,
    H. Hoffmann, A. Holtzman, et al. Cachegen: Fast context loading for language model
    applications. arXiv preprint arXiv:2310.07240, 2023.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLD^+23] Y. Liu, H. Li, K. Du, J. Yao, Y. Cheng, Y. Huang, S. Lu, M. Maire,
    H. Hoffmann, A. Holtzman 等. Cachegen：语言模型应用的快速上下文加载。arXiv 预印本 arXiv:2310.07240,
    2023年。'
- en: '[LWLQ22] T. Lin, Y. Wang, X. Liu, and X. Qiu. A survey of transformers. AI
    Open, 2022.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LWLQ22] T. Lin, Y. Wang, X. Liu 和 X. Qiu. 变压器调查。AI Open, 2022年。'
- en: '[LYZ^+23] Y. Li, Y. Yu, Q. Zhang, C. Liang, P. He, W. Chen, and T. Zhao. Losparse:
    Structured compression of large language models based on low-rank and sparse approximation.
    arXiv preprint arXiv:2306.11222, 2023.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LYZ^+23] Y. Li, Y. Yu, Q. Zhang, C. Liang, P. He, W. Chen 和 T. Zhao. Losparse：基于低秩和稀疏近似的大型语言模型的结构化压缩。arXiv
    预印本 arXiv:2306.11222, 2023年。'
- en: '[MXBS16] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel
    mixture models, 2016.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MXBS16] S. Merity, C. Xiong, J. Bradbury 和 R. Socher. 指针哨兵混合模型，2016年。'
- en: '[NBZ^+23] S. Nerella, S. Bandyopadhyay, J. Zhang, M. Contreras, S. Siegel,
    A. Bumin, B. Silva, J. Sena, B. Shickel, A. Bihorac, et al. Transformers in healthcare:
    A survey. arXiv preprint arXiv:2307.00067, 2023.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NBZ^+23] S. Nerella, S. Bandyopadhyay, J. Zhang, M. Contreras, S. Siegel,
    A. Bumin, B. Silva, J. Sena, B. Shickel, A. Bihorac 等. 医疗保健中的变压器：综述。arXiv 预印本
    arXiv:2307.00067, 2023年。'
- en: '[NCL18] S. Narayan, S. B. Cohen, and M. Lapata. Don’t give me the details,
    just the summary! topic-aware convolutional neural networks for extreme summarization.
    arXiv preprint arXiv:1808.08745, 2018.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NCL18] S. Narayan, S. B. Cohen 和 M. Lapata. 不要给我细节，只要总结！基于主题的卷积神经网络用于极端摘要。arXiv
    预印本 arXiv:1808.08745, 2018年。'
- en: '[NTA24] M. Nikdan, S. Tabesh, and D. Alistarh. Rosa: Accurate parameter-efficient
    fine-tuning via robust adaptation. arXiv preprint arXiv:2401.04679, 2024.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NTA24] M. Nikdan, S. Tabesh 和 D. Alistarh. Rosa：通过鲁棒适应实现准确的参数高效微调。arXiv 预印本
    arXiv:2401.04679, 2024年。'
- en: '[PAA^+23] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao,
    X. Cheng, M. Chung, M. Grella, K. K. GV, et al. Rwkv: Reinventing rnns for the
    transformer era. arXiv preprint arXiv:2305.13048, 2023.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PAA^+23] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao,
    X. Cheng, M. Chung, M. Grella, K. K. GV 等. Rwkv：为变压器时代重新发明 RNNs。arXiv 预印本 arXiv:2305.13048,
    2023年。'
- en: '[PDC^+23] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek,
    K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. Proceedings
    of Machine Learning and Systems, 5, 2023.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PDC^+23] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek,
    K. Xiao, S. Agrawal 和 J. Dean. 高效扩展变压器推理。机器学习与系统会议论文集，第5卷，2023年。'
- en: '[PMN^+23] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio,
    S. Ermon, and C. Ré. Hyena hierarchy: Towards larger convolutional language models.
    arXiv preprint arXiv:2302.10866, 2023.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PMN^+23] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y.
    Bengio, S. Ermon 和 C. Ré. Hyena 层级：朝着更大卷积语言模型的目标。arXiv 预印本 arXiv:2302.10866, 2023年。'
- en: '[PPY^+21] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong.
    Random feature attention. arXiv preprint arXiv:2103.02143, 2021.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PPY^+21] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith 和 L. Kong.
    随机特征注意力。arXiv 预印本 arXiv:2103.02143, 2021年。'
- en: '[PVU^+18] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku,
    and D. Tran. Image transformer. In International conference on machine learning,
    pages 4055–4064\. PMLR, 2018.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PVU^+18] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku
    和 D. Tran. 图像变压器。国际机器学习会议论文集，第4055–4064页。PMLR, 2018年。'
- en: '[RPH^+22] C. Renggli, A. S. Pinto, N. Houlsby, B. Mustafa, J. Puigcerver, and
    C. Riquelme. Learning to merge tokens in vision transformers. arXiv preprint arXiv:2202.12015,
    2022.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RPH^+22] C. Renggli, A. S. Pinto, N. Houlsby, B. Mustafa, J. Puigcerver 和
    C. Riquelme. 学习在视觉变压器中合并标记。arXiv 预印本 arXiv:2202.12015, 2022年。'
- en: '[RPJ^+19] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap.
    Compressive transformers for long-range sequence modelling. arXiv preprint, 2019.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RPJ^+19] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier 和 T. P. Lillicrap.
    用于长范围序列建模的压缩变压器。arXiv 预印本, 2019年。'
- en: '[RR07] A. Rahimi and B. Recht. Random features for large-scale kernel machines.
    Advances in neural information processing systems, 20, 2007.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RR07] A. Rahimi 和 B. Recht. 大规模核机器的随机特征。神经信息处理系统进展，第20卷，2007年。'
- en: '[RSR^+19] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
    Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with
    a unified text-to-text transformer. arXiv e-prints, 2019.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RSR^+19] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
    Y. Zhou, W. Li, 和 P. J. Liu. 探索通过统一的文本到文本变换器的迁移学习极限。arXiv 电子预印本, 2019年。'
- en: '[SDH^+23] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei.
    Retentive network: A successor to transformer for large language models. arXiv
    preprint arXiv:2307.08621, 2023.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SDH^+23] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, 和 F. Wei.
    保留网络：大语言模型的变换器继任者。arXiv 预印本 arXiv:2307.08621, 2023年。'
- en: '[SFA^+22] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné,
    A. S. Luccioni, F. Yvon, M. Gallé, et al. Bloom: A 176b-parameter open-access
    multilingual language model. arXiv preprint arXiv:2211.05100, 2022.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SFA^+22] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné,
    A. S. Luccioni, F. Yvon, M. Gallé 等. **Bloom**：一个176b参数的开源多语言模型。arXiv 预印本 arXiv:2211.05100,
    2022年。'
- en: '[Sha19] N. Shazeer. Fast transformer decoding: One write-head is all you need.
    arXiv preprint arXiv:1911.02150, 2019.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sha19] N. Shazeer. 快速变换器解码：**一个写头**就足够了。arXiv 预印本 arXiv:1911.02150, 2019年。'
- en: '[SLM17] A. See, P. J. Liu, and C. D. Manning. Get to the point: Summarization
    with pointer-generator networks. In Proceedings of the 55th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–1083,
    Vancouver, Canada, July 2017\. Association for Computational Linguistics.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SLM17] A. See, P. J. Liu, 和 C. D. Manning. **总结要点**：使用指针生成网络进行总结。在第55届计算语言学协会年会上（第一卷：长篇论文）上，页码1073–1083，加拿大温哥华，2017年7月。计算语言学协会。'
- en: '[SZY^+23] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, D. Y. Fu, Z. Xie,
    B. Chen, C. W. Barrett, J. Gonzalez, P. Liang, C. Ré, I. Stoica, and C. Zhang.
    High-throughput generative inference of large language models with a single gpu.
    In International Conference on Machine Learning, 2023.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SZY^+23] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, D. Y. Fu, Z. Xie,
    B. Chen, C. W. Barrett, J. Gonzalez, P. Liang, C. Ré, I. Stoica, 和 C. Zhang. 使用单个
    GPU 进行大语言模型的高吞吐量生成推断。国际机器学习会议, 2023年。'
- en: '[TAB^+23] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,
    J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal
    models. arXiv preprint arXiv:2312.11805, 2023.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TAB^+23] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,
    J. Schalkwyk, A. M. Dai, A. Hauth 等. **Gemini**：一个功能强大的多模态模型家族。arXiv 预印本 arXiv:2312.11805,
    2023年。'
- en: '[TBY^+19] Y.-H. H. Tsai, S. Bai, M. Yamada, L.-P. Morency, and R. Salakhutdinov.
    Transformer dissection: a unified understanding of transformer’s attention via
    the lens of kernel. arXiv preprint arXiv:1908.11775, 2019.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TBY^+19] Y.-H. H. Tsai, S. Bai, M. Yamada, L.-P. Morency, 和 R. Salakhutdinov.
    变换器解剖：通过核的视角统一理解变换器的注意力。arXiv 预印本 arXiv:1908.11775, 2019年。'
- en: '[TDBM22] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler. Efficient transformers:
    A survey, 2022.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TDBM22] Y. Tay, M. Dehghani, D. Bahri, 和 D. Metzler. 高效变换器：一项综述, 2022年。'
- en: '[TMS^+23] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,
    N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation
    and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TMS^+23] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,
    N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale 等. **Llama 2**：开源基础和微调聊天模型。arXiv
    预印本 arXiv:2307.09288, 2023年。'
- en: '[VSP^+17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
    Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural
    information processing systems, 30, 2017.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[VSP^+17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
    Gomez, Ł. Kaiser, 和 I. Polosukhin. **注意力机制**是你所需要的一切。神经信息处理系统进展，30, 2017年。'
- en: '[XTC^+23] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming
    language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XTC^+23] G. Xiao, Y. Tian, B. Chen, S. Han, 和 M. Lewis. 使用**注意力汇**的高效流式语言模型。arXiv
    预印本 arXiv:2309.17453, 2023年。'
- en: '[ZRG^+22] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig,
    P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer. Opt: Open pre-trained transformer
    language models, 2022.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZRG^+22] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig,
    P. S. Koura, A. Sridhar, T. Wang, 和 L. Zettlemoyer. **Opt**：开源预训练变换器语言模型, 2022年。'
- en: '[ZSZ^+23] Z. Zhang, Y. Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song,
    Y. Tian, C. Ré, C. Barrett, et al. H $\_2$ o: Heavy-hitter oracle for efficient
    generative inference of large language models. arXiv preprint arXiv:2306.14048,
    2023.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZSZ^+23] Z. Zhang, Y. Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song,
    Y. Tian, C. Ré, C. Barrett 等. H $\_2$ o: 高效生成推断大语言模型的重型预言机。arXiv 预印本 arXiv:2306.14048,
    2023年。'
- en: Appendix A Attention Matrix Visualizations
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 注意力矩阵可视化
- en: 'This section provides some qualitative results on attention matrix approximations
    by sparse policies and LESS. While low-rank caches LESS cannot perfectly recover
    all the missing information, it visually is able to reconstruct a patterns that
    are completely ignored by sparse policies. We can also see the idiosyncrasies
    of the sparse policies and LESS, such as $\operatorname{\text{H}_{2}\text{O}}$-masking’s
    tendency to miss influential tokens which are captured by LESS, as show in Figure [11](#A1.F11
    "Figure 11 ‣ Appendix A Attention Matrix Visualizations ‣ Get More with LESS:
    Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference").'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了稀疏策略和 LESS 对注意力矩阵近似的一些定性结果。虽然低秩缓存 LESS 无法完美恢复所有缺失的信息，但它在视觉上能够重建那些稀疏策略完全忽略的模式。我们还可以看到稀疏策略和
    LESS 的特性差异，例如 $\operatorname{\text{H}_{2}\text{O}}$-masking 忽略的重要标记，而这些标记被 LESS
    捕捉到，如图 [11](#A1.F11 "图 11 ‣ 附录 A 注意力矩阵可视化 ‣ 通过 KV 缓存压缩实现高效 LLM 推理的更多成果")所示。
- en: '![Refer to caption](img/35039329d347339e3783bf68aeb5ac4d.png)![Refer to caption](img/5c6aaefaa09acd497f2eb3a83b8e3eab.png)![Refer
    to caption](img/ca6b2d230e18cbc0b51b7e83b4bd192f.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/35039329d347339e3783bf68aeb5ac4d.png)![参见说明](img/5c6aaefaa09acd497f2eb3a83b8e3eab.png)![参见说明](img/ca6b2d230e18cbc0b51b7e83b4bd192f.png)'
- en: 'Figure 9: Example attention probability matrices from passing a single input
    into Falcon 7B. From top to bottom, the rows consist of attention maps from the
    original model, 10% $\operatorname{\text{H}_{2}\text{O}}$). Darker pixels indicate
    larger probability weights. Only the first 1024 tokens are displayed.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：将单一输入传入 Falcon 7B 后的示例注意力概率矩阵。从上到下，这些行包含了来自原始模型的注意力图，10% $\operatorname{\text{H}_{2}\text{O}}$)。更深的像素表示更大的概率权重。仅显示前
    1024 个标记。
- en: '![Refer to caption](img/f76a178667596c920ebb6d733d75f114.png)![Refer to caption](img/d6a379bfd1e895546136364aa60cfcaf.png)![Refer
    to caption](img/a6106e64efb27191b76c4d72e0fd96ab.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f76a178667596c920ebb6d733d75f114.png)![参见说明](img/d6a379bfd1e895546136364aa60cfcaf.png)![参见说明](img/a6106e64efb27191b76c4d72e0fd96ab.png)'
- en: 'Figure 10: Example attention probability matrices from passing a single input
    into Llama 2 7B. From top to bottom, the rows consist of attention maps from the
    original model, 5% $\operatorname{\text{H}_{2}\text{O}}$). Darker pixels indicate
    larger probability weights. Only the first 1024 tokens are displayed.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：将单一输入传入 Llama 2 7B 后的示例注意力概率矩阵。从上到下，这些行包含了来自原始模型的注意力图，5% $\operatorname{\text{H}_{2}\text{O}}$)。更深的像素表示更大的概率权重。仅显示前
    1024 个标记。
- en: '![Refer to caption](img/74dd1686a614c20700896fa92bdcf75d.png)![Refer to caption](img/744ca7836b7cb09e2e7020be03bbbbe1.png)![Refer
    to caption](img/652402401107af6ac942aae5df40547c.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/74dd1686a614c20700896fa92bdcf75d.png)![参见说明](img/744ca7836b7cb09e2e7020be03bbbbe1.png)![参见说明](img/652402401107af6ac942aae5df40547c.png)'
- en: 'Figure 11: Example attention probability matrices from passing a single input
    into Llama 2 7B. From top to bottom, the rows consist of attention maps from the
    original model, 5% $\Lambda$). Darker pixels indicate larger probability weights.
    Only the first 1024 tokens are displayed.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：将单一输入传入 Llama 2 7B 后的示例注意力概率矩阵。从上到下，这些行包含了来自原始模型的注意力图，5% $\Lambda$)。更深的像素表示更大的概率权重。仅显示前
    1024 个标记。
- en: Appendix B Generation Outputs
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 生成输出
- en: 'We include a couple examples of generation outputs in Figure [12](#A2.F12 "Figure
    12 ‣ Appendix B Generation Outputs ‣ Get More with LESS: Synthesizing Recurrence
    with KV Cache Compression for Efficient LLM Inference") and Figure [13](#A2.F13
    "Figure 13 ‣ Appendix B Generation Outputs ‣ Get More with LESS: Synthesizing
    Recurrence with KV Cache Compression for Efficient LLM Inference"). In both cases,
    the full cache, LESS, and Baseline+ models attempt to summarize news articles.
    We see in Figure [12](#A2.F12 "Figure 12 ‣ Appendix B Generation Outputs ‣ Get
    More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient
    LLM Inference") that LESS is able to produce the same concise summary as the full
    cache while Baseline+ produces rambling text. In Figure [13](#A2.F13 "Figure 13
    ‣ Appendix B Generation Outputs ‣ Get More with LESS: Synthesizing Recurrence
    with KV Cache Compression for Efficient LLM Inference"), we observe that LESS
    completely changes the meaning of the summary from $\operatorname{\text{H}_{2}\text{O}}$
    alone–Baseline+ is factually incorrect based on the article.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图[12](#A2.F12 "Figure 12 ‣ Appendix B Generation Outputs ‣ Get More with
    LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference")和图[13](#A2.F13
    "Figure 13 ‣ Appendix B Generation Outputs ‣ Get More with LESS: Synthesizing
    Recurrence with KV Cache Compression for Efficient LLM Inference")中包含了一些生成输出的示例。在这两种情况下，完整缓存、LESS和Baseline+模型尝试总结新闻文章。我们在图[12](#A2.F12
    "Figure 12 ‣ Appendix B Generation Outputs ‣ Get More with LESS: Synthesizing
    Recurrence with KV Cache Compression for Efficient LLM Inference")中看到，LESS能够生成与完整缓存相同的简明总结，而Baseline+生成了啰嗦的文本。在图[13](#A2.F13
    "Figure 13 ‣ Appendix B Generation Outputs ‣ Get More with LESS: Synthesizing
    Recurrence with KV Cache Compression for Efficient LLM Inference")中，我们观察到LESS完全改变了总结的意思，而Baseline+根据文章的内容是事实错误的。'
- en: '![Refer to caption](img/5029f551358e9c53f44eb16c013de968.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5029f551358e9c53f44eb16c013de968.png)'
- en: 'Figure 12: Example 5-shot (not shown) CNN/DailyMail summary generation results
    produced by variations of Llama 2 7B with an underlying sparse policy of 2% $\operatorname{\text{H}_{2}\text{O}}$.
    For brevity, only the start and end of the article are shown with the middle omitted
    with an ellipsis. LESS produces the same concise summary as the full cache while
    Baseline+ produces rambling text, exceeding the 3 sentence requirement by the
    prompt. The original article is from [[Bru15](#bib.bibx5)].'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：示例5-shot（未显示）CNN/DailyMail总结生成结果，由不同的Llama 2 7B变体产生，其底层稀疏策略为2% $\operatorname{\text{H}_{2}\text{O}}$。为简洁起见，只显示了文章的开头和结尾，中间部分省略了省略号。LESS生成了与完整缓存相同的简明总结，而Baseline+生成了啰嗦的文本，超出了提示要求的3句。原始文章来自[[Bru15](#bib.bibx5)]。
- en: '![Refer to caption](img/89f5ae44d613ff3eaef2ad82194dcad1.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/89f5ae44d613ff3eaef2ad82194dcad1.png)'
- en: 'Figure 13: Example 3-shot (not shown) XSum summary generation results produced
    by variations of Falcon 7B. Models were evaluated with 20% $\operatorname{\text{H}_{2}\text{O}}$.
    The summary by Baseline+ is factually incorrect based on the article, while LESS
    preserves the meaning better. The original article is from [[BBC15](#bib.bibx3)].'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：示例3-shot（未显示）XSum总结生成结果，由不同的Falcon 7B变体产生。模型的评估使用了20% $\operatorname{\text{H}_{2}\text{O}}$。Baseline+生成的总结根据文章内容是事实错误的，而LESS则更好地保留了意思。原始文章来自[[BBC15](#bib.bibx3)]。
