- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:04:28'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:28
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'MINI-LLM: 针对大型语言模型的内存高效结构化剪枝'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.11681](https://ar5iv.labs.arxiv.org/html/2407.11681)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.11681](https://ar5iv.labs.arxiv.org/html/2407.11681)
- en: Hongrong Cheng¹    Miao Zhang²¹¹1Corresponding author.    Javen Qinfeng Shi¹
    ¹University of Adelaide, Adelaide, Australia
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 洪荣程¹    张苗²¹¹1通讯作者。    施佳文¹ ¹阿德莱德大学，澳大利亚阿德莱德
- en: ²Harbin Institute of Technology, Shenzhen, China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ²哈尔滨工业大学，中国深圳
- en: '{hongrong.cheng, javen.shi}@adelaide.edu.au, zhangmiao@hit.edu.cn'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{hongrong.cheng, javen.shi}@adelaide.edu.au, zhangmiao@hit.edu.cn'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'As Large Language Models (LLMs) grow dramatically in size, there is an increasing
    trend in compressing and speeding up these models. Previous studies have highlighted
    the usefulness of gradients for importance scoring in neural network compressing,
    especially in pruning medium-size networks. However, the substantial memory requirements
    involved in calculating gradients with backpropagation impede the utilization
    of gradients in guiding LLM pruning. As a result, most pruning strategies for
    LLMs rely on gradient-free criteria, such as weight magnitudes or a mix of magnitudes
    and activations. In this paper, we devise a hybrid pruning criterion, which appropriately
    integrates magnitude, activation, and gradient to capitalize on feature map sensitivity
    for pruning LLMs. To overcome memory requirement barriers, we estimate gradients
    using only forward passes. Based on this, we propose a Memory-effIcieNt structured
    prunIng procedure for LLMs (MINI-LLM) to remove no-critical channels and multi-attention
    heads. Experimental results demonstrate the superior performance of MINI-LLM over
    existing gradient-free methods on three LLMs: LLaMA, BLOOM, and OPT across various
    downstream tasks (classification, multiple-choice, and generation), while MINI-LLM
    maintains a GPU memory footprint akin to gradient-free methods.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）规模的急剧增长，压缩和加速这些模型的趋势日益增加。以往的研究强调了梯度在神经网络压缩中用于重要性评分的有效性，特别是在剪枝中等规模网络时。然而，反向传播计算梯度所涉及的巨大内存需求阻碍了梯度在指导LLM剪枝中的利用。因此，大多数LLM剪枝策略依赖于无梯度标准，如权重大小或大小与激活的混合。本文提出了一种混合剪枝标准，该标准适当地整合了幅度、激活和梯度，以利用特征图的敏感性进行LLM剪枝。为了克服内存需求障碍，我们仅通过前向传递来估算梯度。在此基础上，我们提出了一种内存高效的结构化剪枝程序（MINI-LLM），以去除非关键通道和多头注意力。实验结果表明，MINI-LLM在三种LLM（LLaMA、BLOOM和OPT）的各种下游任务（分类、多项选择和生成）上优于现有的无梯度方法，同时MINI-LLM保持了类似于无梯度方法的GPU内存占用。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The advent of pre-trained Large Language Models (LLMs), such as GPT-4 OpenAI
    ([2023](#bib.bib37)) and LLaMA Touvron et al. ([2023](#bib.bib44)), has made remarkable
    processes across various complex Natural Language Processing (NLP) tasks, such
    as natural language generation Wu et al. ([2020](#bib.bib48)), question answering Brown
    et al. ([2020](#bib.bib2)), and recommendation system Wu et al. ([2023](#bib.bib49)).
    However, this remarkable capability usually entails a large model size, resulting
    in significant computational costs in terms of storage, memory, and computation
    time, which presents considerable difficulties during the training and deployment
    phases. To this end, there has been considerable interest in compressing LLMs Ma
    et al. ([2023](#bib.bib30)); Dettmers et al. ([2023](#bib.bib7)); Frantar and
    Alistarh ([2023](#bib.bib8)); Xiao et al. ([2023](#bib.bib51)); Li et al. ([2020](#bib.bib26))
    to make them more practical for various tasks. Neural network pruning Ma et al.
    ([2023](#bib.bib30)); Frantar and Alistarh ([2023](#bib.bib8)); Sun et al. ([2024](#bib.bib42));
    Xia et al. ([2024](#bib.bib50)), as one of the indispensable approaches for compressing
    and accelerating neural networks, has recently found its way into LLMs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的大型语言模型（LLMs），如GPT-4 OpenAI ([2023](#bib.bib37)) 和 LLaMA Touvron et al. ([2023](#bib.bib44))，在自然语言生成
    Wu et al. ([2020](#bib.bib48))、问答 Brown et al. ([2020](#bib.bib2)) 和推荐系统 Wu et
    al. ([2023](#bib.bib49)) 等各种复杂自然语言处理（NLP）任务中取得了显著进展。然而，这种显著能力通常需要较大的模型尺寸，从而导致存储、内存和计算时间的显著计算成本，这在训练和部署阶段带来了相当大的困难。为此，压缩LLMs
    以使其在各种任务中更具实用性引起了相当大的兴趣 Ma et al. ([2023](#bib.bib30)); Dettmers et al. ([2023](#bib.bib7));
    Frantar and Alistarh ([2023](#bib.bib8)); Xiao et al. ([2023](#bib.bib51)); Li
    et al. ([2020](#bib.bib26))。神经网络剪枝 Ma et al. ([2023](#bib.bib30)); Frantar and
    Alistarh ([2023](#bib.bib8)); Sun et al. ([2024](#bib.bib42)); Xia et al. ([2024](#bib.bib50))，作为压缩和加速神经网络的必备方法之一，最近也开始应用于LLMs。
- en: 'In the traditional pruning methods Molchanov et al. ([2017](#bib.bib34)); Lee
    et al. ([2019](#bib.bib23)); Sanh et al. ([2020](#bib.bib39)); Liu et al. ([2021](#bib.bib29));
    Fu et al. ([2022](#bib.bib10)) for compressing small or medium-size models, gradients
    of loss functions w.r.t. weights, masks, or feature maps have demonstrated more
    reliable performance than gradient-free methods (e.g., magnitude-based methods)
    in discriminating important weights/channels. For example, Lee et al. ([2019](#bib.bib23))
    exploits the first-order Taylor expansion to identify the connection sensitivity
    caused by setting some weights to zero, which outperforms magnitude-based methods
    and obtains extremely sparse networks with similar accuracy as the reference networks.
    However, due to the huge number of parameters in LLMs, computing gradients with
    backpropagation requires a prohibitive amount of memory. LLM-Pruner Ma et al.
    ([2023](#bib.bib30)), which uses gradients calculated via backpropagation for
    structured pruning, consumes about twice the GPU resources compared to magnitude-based
    methods during pruning LLaMA-7B, as illustrated in Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models")²²2We obtained the data on an NVIDIA A100 (40GB). Even though the recovery
    stage can be executed on a single RTX 4090 (24GB) with the help of Low-Rank Adaption
    (LoRA) Hu et al. ([2022](#bib.bib14)), the GPU consumption during the pruning
    stage has become the bottleneck for GPU resource usage in the entire pruning framework.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '在传统的剪枝方法 Molchanov et al. ([2017](#bib.bib34)); Lee et al. ([2019](#bib.bib23));
    Sanh et al. ([2020](#bib.bib39)); Liu et al. ([2021](#bib.bib29)); Fu et al. ([2022](#bib.bib10))
    中，通过对权重、掩码或特征图的损失函数梯度的分析，在区分重要权重/通道方面表现得比无梯度方法（例如基于幅度的方法）更为可靠。例如，Lee et al. ([2019](#bib.bib23))
    利用一阶泰勒展开来识别将一些权重设置为零所引起的连接敏感性，这比基于幅度的方法更具优势，并获得了与参考网络具有相似准确度的极度稀疏网络。然而，由于LLMs中参数的巨大数量，通过反向传播计算梯度需要大量的内存。LLM-Pruner
    Ma et al. ([2023](#bib.bib30)) 使用通过反向传播计算的梯度进行结构化剪枝，在剪枝LLaMA-7B时消耗的GPU资源大约是基于幅度的方法的两倍，如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models")²²2 我们在NVIDIA A100（40GB）上获得了数据。尽管恢复阶段可以在单个RTX
    4090（24GB）上通过低秩适应（LoRA） Hu et al. ([2022](#bib.bib14)) 执行，但剪枝阶段的GPU消耗已成为整个剪枝框架中GPU资源使用的瓶颈。'
- en: To avoid incurring untenable memory costs for computing gradients on LLMs, some
    pruning methods Frantar and Alistarh ([2023](#bib.bib8)); Sun et al. ([2024](#bib.bib42))
    constructed gradient-free criteria. For example, SparseGPT Frantar and Alistarh
    ([2023](#bib.bib8)) employed the combination of weight magnitude and the inverse
    Hessian matrix which is formed from the product of given input features to score
    weights for unstructured pruning. However, the computation of the inverse Hessian
    matrix is resource-intensive, and the utility of gradient information remains
    under-exploited. Some pruning methods, such as Sheared LLaMA [Xia et al. ([2024](#bib.bib50))],
    combine pruning with pre-training, which requires substantial GPU resources. For
    example, Sheared LLaMA requires 8 A100 (80GB) GPUs for pruning and 16 for pre-training.
    Since this paper focuses on memory-efficient pruning methods and follows the mainstream
    framework that starts with pruning and then fine-tuning, without pre-training,
    although outstanding in performance, these methods are not within the scope of
    our discussion.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在计算大规模语言模型（LLMs）的梯度时产生难以承担的内存开销，一些修剪方法，如 Frantar 和 Alistarh ([2023](#bib.bib8))，Sun
    等 ([2024](#bib.bib42))，构建了无梯度的标准。例如，SparseGPT Frantar 和 Alistarh ([2023](#bib.bib8))
    采用了权重幅度和由给定输入特征的乘积形成的逆 Hessian 矩阵的组合来对无结构修剪的权重进行评分。然而，计算逆 Hessian 矩阵是资源密集型的，梯度信息的利用仍然未得到充分开发。一些修剪方法，如
    Sheared LLaMA [Xia 等 ([2024](#bib.bib50))]，将修剪与预训练结合起来，这需要大量的 GPU 资源。例如，Sheared
    LLaMA 需要 8 个 A100（80GB）GPU 用于修剪，16 个用于预训练。由于本文关注于内存高效的修剪方法，并遵循从修剪到微调的主流框架，不包括预训练，尽管在性能上表现出色，这些方法不在我们讨论的范围内。
- en: In this paper, we propose the Memory-effIcieNt structured prunIng procedure
    for LLMs (MINI-LLM), which scores weights using estimated gradients with only
    forward passes. We make this approach tractable by contributing multiple techniques.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种针对 LLMs 的内存高效结构化修剪过程（MINI-LLM），该方法仅通过前向传递来使用估计梯度进行权重评分。我们通过贡献多种技术使这种方法变得可行。
- en: '![Refer to caption](img/5cbe9ccac74ab77d1f17a87b471341bd.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5cbe9ccac74ab77d1f17a87b471341bd.png)'
- en: 'Figure 1: The peak GPU-memory Usage for pruning LLaMA-7B. The backpropagation
    gradient-based pruning method, LLM-Pruner, consumes about twice the GPU resources
    compared to gradient-free methods and our method MINI-LLM during pruning LLaMA-7B.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：修剪 LLaMA-7B 的峰值 GPU 内存使用情况。与无梯度方法和我们的方法 MINI-LLM 在修剪 LLaMA-7B 期间相比，基于反向传播梯度的修剪方法
    LLM-Pruner 消耗了大约两倍的 GPU 资源。
- en: 'Our main contributions are as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '• '
- en: We design a novel pruning criterion called Feature Map Sensitivity (FMS) score,
    integrating weight magnitude, activation, and gradient. This criterion optimally
    utilizes the pivotal information from the three critical aspects, which facilitates
    a more nuanced assessment of feature map sensitivity and provides effective scoring
    in LLMs.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了一种新颖的修剪标准，称为特征图敏感性（FMS）评分，综合了权重幅度、激活和梯度。该标准优化地利用了三个关键方面的核心信息，有助于对特征图敏感性进行更细致的评估，并提供了有效的
    LLM 评分。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '• '
- en: We propose a structured pruning framework for LLMs called MINI-LLM which utilizes
    estimated gradients with only forward passes by using comparable GPU memory usage
    to gradient-free methods, significantly improving GPU memory efficiency over traditional
    backpropagation gradients.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种名为 MINI-LLM 的 LLM 结构化修剪框架，该框架利用仅通过前向传递的估计梯度，使用与无梯度方法相当的 GPU 内存，显著提高了相对于传统反向传播梯度的
    GPU 内存效率。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '• '
- en: 'The experiments on three types of LLMs: LLaMA, BLOOM, and OPT over different
    downstream tasks (classification, multiple-choices, and generation) demonstrate
    that the novel pruning criterion FMS can effectively boost the performance of
    gradient-based methods. Additionally, our proposed gradient-based structured pruning
    method MINI-LLM steadily exceeds gradient-free pruning methods in performance
    and rivals or surpasses backpropagation gradient-based method at times, while
    using similar GPU memory as gradient-free methods.'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对三种类型的 LLMs（LLaMA、BLOOM 和 OPT）在不同下游任务（分类、多项选择和生成）上的实验表明，新颖的修剪标准 FMS 能有效提升基于梯度的方法的性能。此外，我们提出的基于梯度的结构化修剪方法
    MINI-LLM 在性能上稳定超越了无梯度修剪方法，有时与反向传播梯度方法相当或超过，同时使用的 GPU 内存与无梯度方法相似。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Structured/Unstructured/Semi-structured LLM pruning. The pruning methods for
    LLMs can still be generally categorized as unstructured (Sun et al. ([2024](#bib.bib42));
    Frantar et al. ([2022](#bib.bib9))), semi-structured (Frantar and Alistarh ([2023](#bib.bib8))),
    and structured (Ma et al. ([2023](#bib.bib30)); Wang et al. ([2020b](#bib.bib46)))
    pruning methods, similar to the categorization for pruning small and mid-size
    neural networks. Unstructured pruning achieves substantial sparsity by directly
    setting weights or their masks to zero while maintaining a comparable performance
    compared to the vanilla models. However, the irregular sparsity results in no
    compression in the model size, and actual acceleration necessitates the support
    of specialized software/hardware. In contrast, structured pruning discards the
    whole grouped parameters (such as channels and attention heads), leading to physically
    reduced model size and enabling inference acceleration without any special requirements
    of software/hardware (Zhou et al. ([2022](#bib.bib55)); Frantar and Alistarh ([2023](#bib.bib8))).
    Semi-structure pruning, such as 2:4 or 4:8 patterns in Frantar and Alistarh ([2023](#bib.bib8)),
    provides a balance between performance and hardware speedup. In this paper, we
    focus on structured pruning for LLMs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化/非结构化/半结构化的LLM剪枝。LLM的剪枝方法通常可以分为非结构化（Sun et al. ([2024](#bib.bib42)); Frantar
    et al. ([2022](#bib.bib9)))、半结构化（Frantar and Alistarh ([2023](#bib.bib8)))和结构化（Ma
    et al. ([2023](#bib.bib30)); Wang et al. ([2020b](#bib.bib46)))剪枝方法，这与小型和中型神经网络的剪枝分类类似。非结构化剪枝通过直接将权重或其掩码设置为零，来实现显著的稀疏性，同时保持与原始模型相当的性能。然而，不规则的稀疏性导致模型大小没有压缩，实际的加速需要专门的软件/硬件支持。相比之下，结构化剪枝会丢弃整个分组的参数（例如通道和注意力头），从而物理上减少模型大小，并实现推理加速而不需要特别的软件/硬件要求（Zhou
    et al. ([2022](#bib.bib55)); Frantar and Alistarh ([2023](#bib.bib8)))。半结构化剪枝，如Frantar
    and Alistarh ([2023](#bib.bib8))中的2:4或4:8模式，在性能和硬件加速之间提供了平衡。在本文中，我们重点关注LLM的结构化剪枝。
- en: Pruning criteria for LLMs. Neural network pruning methods search for an optimal
    subnetwork by removing unimportant weights. As one of the most popular criterion
    factors, gradients have already been demonstrated effective in constructing scoring
    functions for pruning small or medium-size networks Liu et al. ([2021](#bib.bib29));
    Fu et al. ([2022](#bib.bib10)); Wang et al. ([2020a](#bib.bib45)); Yu et al. ([2022](#bib.bib52));
    Molchanov et al. ([2019](#bib.bib35)); Kwon et al. ([2022](#bib.bib21)). However,
    calculating gradients using backpropagation is highly resource-intensive for GPU
    memory, making it challenging to implement for LLMs, where meeting such high memory
    demands is difficult. For example, LLM-Pruner Ma et al. ([2023](#bib.bib30)) employs
    gradients calculated through backpropagation for LLM pruning, but the GPU memory
    required for pruning exceeds that of fine-tuning. To this end, there are some
    gradient-free pruning methods (Frantar and Alistarh ([2023](#bib.bib8)); Sun et
    al. ([2024](#bib.bib42)); Nova et al. ([2023](#bib.bib36)); Kurtic et al. ([2023](#bib.bib20));
    Li et al. ([2022b](#bib.bib28))). Most of them are centered on post-training (retraining-free)
    approaches that involve pruning while concurrently compensating for performance.
    For instance, Wanda Sun et al. ([2024](#bib.bib42)) multiplies weight magnitude
    and the corresponding activation to implement unstructured post-training pruning
    for LLMs. Even though these gradient-free methods are GPU memory efficient, the
    utility of gradient remains under-exploited. This paper actively seeks an effective
    estimation method to overcome memory requirement barriers related to computing
    gradients with backpropagation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的剪枝标准。神经网络剪枝方法通过去除不重要的权重来寻找最优子网络。作为最受欢迎的标准因素之一，梯度已经被证明在构建剪枝小型或中型网络的评分函数方面有效 Liu
    et al. ([2021](#bib.bib29)); Fu et al. ([2022](#bib.bib10)); Wang et al. ([2020a](#bib.bib45));
    Yu et al. ([2022](#bib.bib52)); Molchanov et al. ([2019](#bib.bib35)); Kwon et
    al. ([2022](#bib.bib21))。然而，使用反向传播计算梯度对于GPU内存的资源消耗极大，这使得在满足高内存需求的LLMs中实施变得具有挑战性。例如，LLM-Pruner Ma
    et al. ([2023](#bib.bib30))采用通过反向传播计算的梯度进行LLM剪枝，但剪枝所需的GPU内存超过了微调所需的内存。为此，存在一些无梯度剪枝方法（Frantar
    and Alistarh ([2023](#bib.bib8)); Sun et al. ([2024](#bib.bib42)); Nova et al.
    ([2023](#bib.bib36)); Kurtic et al. ([2023](#bib.bib20)); Li et al. ([2022b](#bib.bib28)))。其中大多数集中在后训练（无重新训练）方法上，这些方法在剪枝的同时补偿性能。例如，Wanda Sun
    et al. ([2024](#bib.bib42)) 通过将权重大小与相应的激活值相乘，实施LLM的无结构后训练剪枝。尽管这些无梯度方法在GPU内存使用上很高效，但梯度的作用仍未得到充分发挥。本文积极寻求一种有效的估计方法，以克服与反向传播计算梯度相关的内存需求障碍。
- en: Zeroth-Order optimization. Zeroth-Order (ZO) optimization can fall in the general
    class of weight perturbation methods. An early method referred to as the Finite
    Difference Stochastic Approximation (FDSA) Kiefer and Wolfowitz. ([1952](#bib.bib16))
    estimated the gradient by using $2d$ value would result in serious computational
    challenges. A more efficient gradient estimation method is Simultaneous Perturbation
    Stochastic Approximation (SPSA) Spall ([1992](#bib.bib40)); Li et al. ([2022a](#bib.bib27))
    which approximates gradients using only two forward passes. In previous work,
    ZO gradient estimation was used for solving optimization-related problems, such
    as for model training. For example, Malladi et al. Malladi et al. ([2023](#bib.bib31))
    propose a Memory-efficient ZO-SGD (MeZO) to adapt SPSA to fine-tuning LLMs in
    a memory-efficient way. In this paper, for the first time, we apply ZO gradient
    estimation for pruning LLMs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 零阶优化。零阶（ZO）优化可以归入权重扰动方法的一般类别。一种早期方法称为有限差分随机近似（FDSA） Kiefer 和 Wolfowitz ([1952](#bib.bib16))
    通过使用 $2d$ 值来估计梯度，这将带来严重的计算挑战。一种更高效的梯度估计方法是同时扰动随机近似（SPSA） Spall ([1992](#bib.bib40));
    Li et al. ([2022a](#bib.bib27))，它仅通过两次前向传播来近似梯度。在之前的工作中，ZO梯度估计被用于解决优化相关问题，例如模型训练。例如，Malladi
    et al. Malladi et al. ([2023](#bib.bib31)) 提出了一个内存高效的ZO-SGD（MeZO），以内存高效的方式将SPSA适应于LLM的微调。本文首次将ZO梯度估计应用于LLM的剪枝。
- en: 3 Method
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'In this section, we propose a Memory-effIcieNt structured prunIng procedure
    for LLMs termed MINI-LLM. We start by describing a new pruning criterion that
    evaluates feature map saliency from three critical factors: gradient, weight magnitude,
    and activation. To evaluate gradients in a memory-efficient way, we exploit ZO
    gradients to approximate the backpropagation based gradients. Finally, to recover
    performance, we utilize LoRA Hu et al. ([2022](#bib.bib14)) to fine-tune the pruned
    model, which has high training throughput, but low GPU memory requirement.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提出了一种用于 LLMs 的高效内存结构剪枝程序，称为 MINI-LLM。我们首先描述了一种新的剪枝标准，它从三个关键因素：梯度、权重幅度和激活中评估特征图的显著性。为了以高效的内存方式评估梯度，我们利用
    ZO 梯度来近似反向传播的梯度。最后，为了恢复性能，我们利用 LoRA Hu 等人（[2022](#bib.bib14)）对剪枝后的模型进行微调，该方法具有高训练吞吐量，但
    GPU 内存需求低。
- en: 3.1 Pruning Criterion in MINI-LLM
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 MINI-LLM 中的剪枝标准
- en: 'Problem Definition. The pruning problem for LLMs starts from a pre-trained
    dense model $W_{0}\in\mathbb{R}^{d}$ (i.e., the percentage of removed weights),
    our goal is to remove the weights that has the least impact on the model’s prediction.
    Therefore, neural network pruning can be formulated as the following constrained
    optimization problem:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 问题定义。LLMs 的剪枝问题从一个预训练的密集模型 $W_{0}\in\mathbb{R}^{d}$（即，移除的权重百分比）开始，我们的目标是移除对模型预测影响最小的权重。因此，神经网络剪枝可以表述为以下约束优化问题：
- en: '|  |  | $1$2 |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (1) |'
- en: '|  |  | $\displaystyle\text{s.t.}\quad\lvert&#124;\hat{W}_{0}\rvert&#124;_{0}\leq\lvert&#124;W_{0}\rvert&#124;_{0}\times(1-p),$
    |  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\text{s.t.}\quad\lvert&#124;\hat{W}_{0}\rvert&#124;_{0}\leq\lvert&#124;W_{0}\rvert&#124;_{0}\times(1-p),$
    |  |'
- en: where $\ell(\cdot)$ norm.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\ell(\cdot)$ 范数。
- en: 'Gradient-based Pruning. To evaluate the significance of a specific weight $W_{l}^{k}$
    is included versus when it is excluded from the model (i.e., LLaMA-7B). Thus,
    the loss change can be formulated as LeCun et al. ([1989](#bib.bib22)):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的剪枝。为了评估特定权重 $W_{l}^{k}$ 包含与排除在模型中（即 LLaMA-7B）的重要性，因此，损失变化可以表述为 LeCun 等人（[1989](#bib.bib22)）：
- en: '|  | $\displaystyle\Delta\mathcal{L}$ |  | (2) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta\mathcal{L}$ |  | (2) |'
- en: '|  |  | $\displaystyle\approx\Delta W^{T}\frac{\partial\mathcal{L}(\mathcal{D})}{\partial
    W_{l}^{k}}-\frac{1}{2}\Delta W^{T}H\Delta W$ |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\approx\Delta W^{T}\frac{\partial\mathcal{L}(\mathcal{D})}{\partial
    W_{l}^{k}}-\frac{1}{2}\Delta W^{T}H\Delta W$ |  |'
- en: '|  |  | $\displaystyle=W_{l}^{k}\frac{\partial\mathcal{L}(\mathcal{D})}{\partial
    W_{l}^{k}}-\frac{1}{2}W_{l}^{k}HW_{l}^{k},$ |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=W_{l}^{k}\frac{\partial\mathcal{L}(\mathcal{D})}{\partial
    W_{l}^{k}}-\frac{1}{2}W_{l}^{k}HW_{l}^{k},$ |  |'
- en: 'where $\Delta W=W_{l}^{k}$ can be approximated as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Delta W=W_{l}^{k}$ 可以近似为：
- en: '|  | $\Delta\mathcal{L}=\mathcal{L}_{W_{l}^{k}}(\mathcal{D})-\mathcal{L}_{W_{l}^{k}=0}(\mathcal{D})\approx
    W_{l}^{k}\frac{\partial\mathcal{L}(\mathcal{D})}{\partial W_{l}^{k}}.$ |  | (3)
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta\mathcal{L}=\mathcal{L}_{W_{l}^{k}}(\mathcal{D})-\mathcal{L}_{W_{l}^{k}=0}(\mathcal{D})\approx
    W_{l}^{k}\frac{\partial\mathcal{L}(\mathcal{D})}{\partial W_{l}^{k}}.$ |  | (3)
    |'
- en: 'Activation-based Pruning. As recently observed in LLMs larger than 6.7B (Dettmers
    et al. ([2022](#bib.bib6))), a small set of hidden state features emerges with
    significantly larger magnitudes (outliers) than the remainders and zeroing out
    these features causes a significant degradation of performance. The vanilla scoring
    function Eq. ([3](#S3.E3 "In 3.1 Pruning Criterion in MINI-LLM ‣ 3 Method ‣ MINI-LLM:
    Memory-Efficient Structured Pruning for Large Language Models")) does not highlight
    the unique characteristics of LLMs compared with smaller models. Given the $l$,
    which most closely approximates the initial output as determined by the squared
    error metric. Assuming that the activation and weight matrices possess a suitable
    rectangular shape, the neural network pruning is defined as the following optimization
    problem Frantar and Alistarh ([2023](#bib.bib8)):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '基于激活的剪枝。正如最近在大于 6.7B 的 LLMs 中观察到的（Dettmers 等人（[2022](#bib.bib6)）），出现了一小部分隐藏状态特征，其幅度明显大于其余特征，并且将这些特征归零会导致性能显著下降。原始评分函数公式（[3](#S3.E3
    "In 3.1 Pruning Criterion in MINI-LLM ‣ 3 Method ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models")）未能突出 LLMs 相对于较小模型的独特特性。给定 $l$，它最接近初始输出，根据平方误差度量确定。假设激活和权重矩阵具有合适的矩形形状，则神经网络剪枝被定义为以下优化问题（Frantar
    和 Alistarh（[2023](#bib.bib8)））：'
- en: '|  |  | $\displaystyle\mathop{\textrm{min}}\limits_{\hat{W}_{l}}\ \lvert&#124;\hat{W}_{l}X_{l}-W_{l}X_{l}\rvert&#124;_{2}^{2}=\lvert&#124;\Delta
    W_{l}X_{l}\rvert&#124;_{2}^{2},$ |  | (4) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathop{\textrm{min}}\limits_{\hat{W}_{l}}\ \lvert&#124;\hat{W}_{l}X_{l}-W_{l}X_{l}\rvert&#124;_{2}^{2}=\lvert&#124;\Delta
    W_{l}X_{l}\rvert&#124;_{2}^{2},$ |  | (4) |'
- en: '|  |  | $\displaystyle\text{s.t.}\quad\hat{W}_{l}\in\mathcal{C}.$ |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\text{s.t.}\quad\hat{W}_{l}\in\mathcal{C}.$ |  |'
- en: 'To evaluate the significance of a specific weight $W_{l}^{k}$ is preserved
    versus when it is excluded from the model and write the formulation as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 评估特定权重 $W_{l}^{k}$ 被保留相较于从模型中排除时的意义，并将其公式写作：
- en: '|  | $\lvert&#124;\Delta W_{l}X_{l}\rvert&#124;_{2}=\left&#124;W_{l}^{k}X_{l}^{k}\right&#124;,$
    |  | (5) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\lvert&#124;\Delta W_{l}X_{l}\rvert&#124;_{2}=\left&#124;W_{l}^{k}X_{l}^{k}\right&#124;,$
    |  | (5) |'
- en: where $\Delta W_{l}=W_{l}^{k}$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Delta W_{l}=W_{l}^{k}$。
- en: 'Feature Map Sensitivity (FMS). Eq. ([5](#S3.E5 "In 3.1 Pruning Criterion in
    MINI-LLM ‣ 3 Method ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large
    Language Models")) only considers the output changes in a single layer and does
    not take into account the global loss change across the entire network. We notice
    that, with the weight gradients, the global loss change can be quantified with
    weight change as shown in Eq. ([3](#S3.E3 "In 3.1 Pruning Criterion in MINI-LLM
    ‣ 3 Method ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models")). To calculate the salience of each weight relative to the change in
    global loss and layer-wise output, we measure $\Delta W_{l}$ from Eq. ([3](#S3.E3
    "In 3.1 Pruning Criterion in MINI-LLM ‣ 3 Method ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models")) and propose a heuristic hybrid
    sensitivity scoring function called Feature Map Sensitivity (FMS) as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 特征图敏感性（FMS）。Eq. ([5](#S3.E5 "在 3.1 MINI-LLM 剪枝标准 ‣ 3 方法 ‣ MINI-LLM：用于大语言模型的内存高效结构化剪枝"))
    仅考虑单层的输出变化，而未考虑整个网络的全局损失变化。我们注意到，利用权重梯度，可以通过权重变化量化全局损失变化，如 Eq. ([3](#S3.E3 "在
    3.1 MINI-LLM 剪枝标准 ‣ 3 方法 ‣ MINI-LLM：用于大语言模型的内存高效结构化剪枝")) 所示。为了计算每个权重相对于全局损失和层级输出变化的显著性，我们测量
    $\Delta W_{l}$ 来自 Eq. ([3](#S3.E3 "在 3.1 MINI-LLM 剪枝标准 ‣ 3 方法 ‣ MINI-LLM：用于大语言模型的内存高效结构化剪枝"))
    并提出一种启发式混合敏感性评分函数，称为特征图敏感性（FMS），其定义如下：
- en: '|  | $S(W_{l}^{k})_{ours}=\left&#124;W_{l}^{k}\frac{\partial\mathcal{L}(\mathcal{D})}{\partial
    W_{l}^{k}}X_{l}^{k}\right&#124;.$ |  | (6) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $S(W_{l}^{k})_{ours}=\left&#124;W_{l}^{k}\frac{\partial\mathcal{L}(\mathcal{D})}{\partial
    W_{l}^{k}}X_{l}^{k}\right&#124;.$ |  | (6) |'
- en: 'Compared to Eq. ([3](#S3.E3 "In 3.1 Pruning Criterion in MINI-LLM ‣ 3 Method
    ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models")) and
    Eq. ([5](#S3.E5 "In 3.1 Pruning Criterion in MINI-LLM ‣ 3 Method ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models")), our criterion Eq. ([6](#S3.E6
    "In 3.1 Pruning Criterion in MINI-LLM ‣ 3 Method ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models")) integrates magnitude, activation,
    and gradient to optimally utilize the pivotal information from the three critical
    aspects, so as calculate the feature map sensitivity along with the loss changes.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Eq. ([3](#S3.E3 "在 3.1 MINI-LLM 剪枝标准 ‣ 3 方法 ‣ MINI-LLM：用于大语言模型的内存高效结构化剪枝"))
    和 Eq. ([5](#S3.E5 "在 3.1 MINI-LLM 剪枝标准 ‣ 3 方法 ‣ MINI-LLM：用于大语言模型的内存高效结构化剪枝"))
    相比，我们的标准 Eq. ([6](#S3.E6 "在 3.1 MINI-LLM 剪枝标准 ‣ 3 方法 ‣ MINI-LLM：用于大语言模型的内存高效结构化剪枝"))
    综合了幅度、激活和梯度，以最佳利用来自三个关键方面的核心信息，从而计算特征图的敏感性以及损失的变化。
- en: 3.2 Pruning with Estimated Gradients
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 使用估计梯度进行剪枝
- en: 'Let $\mathcal{L}(W;\mathcal{B})$. The following Definition [7](#S3.E7 "In Definition
    1 (ZO Gradient Estimation.). ‣ 3.2 Pruning with Estimated Gradients ‣ 3 Method
    ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models") describes
    a classical ZO gradient estimation based on SPSA (Spall ([1992](#bib.bib40))).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让 $\mathcal{L}(W;\mathcal{B})$。以下定义 [7](#S3.E7 "在定义 1 (ZO 梯度估计)。 ‣ 3.2 使用估计梯度进行剪枝
    ‣ 3 方法 ‣ MINI-LLM：用于大语言模型的内存高效结构化剪枝") 描述了基于 SPSA（Spall ([1992](#bib.bib40)）的经典
    ZO 梯度估计。
- en: Definition 1  (ZO Gradient Estimation.).
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1 (ZO 梯度估计)。
- en: Given a model with parameters $W\in\mathbb{R}^{d}$ is as
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个参数为 $W\in\mathbb{R}^{d}$ 的模型，如下所示
- en: '|  |  $1$2  |  | (7) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  |  $1$2  |  | (7) |'
- en: where $\nabla\mathcal{L}(W;\mathcal{B})$) is then
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\nabla\mathcal{L}(W;\mathcal{B})$) 然后为
- en: '|  | $\hat{g}_{l}^{k}=\frac{\mathcal{L}(W+\epsilon z;\mathcal{B})-\mathcal{L}(W-\epsilon
    z;\mathcal{B})}{2\epsilon z_{l}^{k}},$ |  | (8) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{g}_{l}^{k}=\frac{\mathcal{L}(W+\epsilon z;\mathcal{B})-\mathcal{L}(W-\epsilon
    z;\mathcal{B})}{2\epsilon z_{l}^{k}},$ |  | (8) |'
- en: 'where $z_{l}^{k}\in z$. In this way, the practical pruning score used in our
    MINI-LLM is defined as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $z_{l}^{k}\in z$。这样，我们在 MINI-LLM 中使用的实际剪枝评分定义为：
- en: '|  | $\hat{S}(W_{l}^{k})_{ours}=\left&#124;W_{l}^{k}\hat{g}_{l}^{k}X_{l}^{k}\right&#124;.$
    |  | (9) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{S}(W_{l}^{k})_{ours}=\left&#124;W_{l}^{k}\hat{g}_{l}^{k}X_{l}^{k}\right&#124;.$
    |  | (9) |'
- en: 'Dependency-aware structured LLM pruning. To maintain structural integrity,
    it is crucial for structured pruning to identify groups of interdependent structures
    within LLMs. Following Ma et al. Ma et al. ([2023](#bib.bib30)), we prune heads
    for Multi-Head Attention (MHA) and channels for Feed-Forward Network (FFN), respectively.
    We arrange the interconnected weights into groups and determine the sensitivity
    of each group (a set of coupled structures) defined as $G=\{W_{i}\}_{i=1}^{M}$
    is the number of interdependent structures in the group. Our structured pruning
    approach MINI-LLM is outlined in Algorithm [1](#alg1 "Algorithm 1 ‣ 3.2 Pruning
    with Estimated Gradients ‣ 3 Method ‣ MINI-LLM: Memory-Efficient Structured Pruning
    for Large Language Models").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '依赖感知的结构化LLM剪枝。为了保持结构完整性，结构化剪枝至关重要，需要识别LLM中的互依结构组。遵循 Ma et al. Ma et al. ([2023](#bib.bib30))，我们分别剪枝多头注意力（MHA）的头部和前馈网络（FFN）的通道。我们将相互连接的权重排列成组，并确定每组的敏感度（一组耦合结构），定义为
    $G=\{W_{i}\}_{i=1}^{M}$ 是组中互依结构的数量。我们的结构化剪枝方法MINI-LLM在算法 [1](#alg1 "Algorithm
    1 ‣ 3.2 Pruning with Estimated Gradients ‣ 3 Method ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models") 中概述。'
- en: Algorithm 1 The structured pruning algorithm MINI-LLM
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 结构化剪枝算法 MINI-LLM
- en: 'Input: Dataset $\mathcal{D}$.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：数据集 $\mathcal{D}$。
- en: 'Output: The pruned model'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：剪枝后的模型
- en: 1:  Clear every weight’s sensitivity score $\hat{S}(W_{l}^{k})=0$;9:  end for10:  return
    the pruned model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 清除每个权重的敏感度分数 $\hat{S}(W_{l}^{k})=0$；9: 结束 for10: 返回剪枝后的模型。'
- en: 3.3 Recovery with Low-rank Approximation
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 低秩近似恢复
- en: After pruning, we need a recovery stage to regain the performance. Due to the
    huge number of parameters, full fine-tuning becomes less feasible. LoRA Hu et
    al. ([2022](#bib.bib14)), as one of the most popular Parameter-Efficient Fine-Tuning
    (PEFT) methods He et al. ([2023](#bib.bib12)); Li and Liang ([2021](#bib.bib25));
    Jia et al. ([2022](#bib.bib15)); Chavan et al. ([2023](#bib.bib3)); Lester et
    al. ([2021](#bib.bib24)), has demonstrated strong capability for performance recovery,
    while significantly reducing GPU memory usage Dettmers et al. ([2023](#bib.bib7)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝后，我们需要一个恢复阶段来恢复性能。由于参数数量巨大，完全微调变得不太可行。LoRA Hu et al. ([2022](#bib.bib14))，作为最受欢迎的参数高效微调（PEFT）方法之一
    He et al. ([2023](#bib.bib12))；Li and Liang ([2021](#bib.bib25))；Jia et al. ([2022](#bib.bib15))；Chavan
    et al. ([2023](#bib.bib3))；Lester et al. ([2021](#bib.bib24))，已展示了强大的性能恢复能力，同时显著减少了GPU内存使用
    Dettmers et al. ([2023](#bib.bib7))。
- en: 'To facilitate this, we fine-tune the pruned models by employing LoRA which
    only updates two injected low-rank decomposition matrices that are attached to
    a frozen pre-trained weight matrix. Given two low-rank matrices $A\in\mathbb{R}^{r\times
    k}$ ), the forward computation can be written as:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于这一点，我们通过使用LoRA对剪枝后的模型进行微调，LoRA仅更新附加在冻结的预训练权重矩阵上的两个注入的低秩分解矩阵。给定两个低秩矩阵 $A\in\mathbb{R}^{r\times
    k}$，前向计算可以写成：
- en: '|  | $f(x)=xW_{0}+xBA,$ |  | (10) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(x)=xW_{0}+xBA,$ |  | (10) |'
- en: where $x\in\mathbb{R}^{n\times d}$.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x\in\mathbb{R}^{n\times d}$。
- en: 4 Experiments
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this section, we evaluate the performance of our MINI-LLM on three kinds
    of LLMs, covering a wide range of tasks. We first introduce the experimental setup,
    then present the main results and provide ablation studies for further analysis.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了我们的MINI-LLM在三种类型的LLM上的性能，涵盖了广泛的任务。我们首先介绍实验设置，然后呈现主要结果，并提供消融研究以便进一步分析。
- en: 4.1 Experimental Setup
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'Models, datasets, and evaluation metrics. To verify the effectiveness and versatility
    of our MINI-LLM, we test it over three open-source LLMs with different structures:
    LLaMA-7B Touvron et al. ([2023](#bib.bib44)), BLOOM-7B Workshop ([2023](#bib.bib47)),
    and OPT-6.7B Zhang et al. ([2022](#bib.bib54)). All models undergo evaluation
    in a task-agnostic framework. We assess the zero-shot ability of pruned models
    on WikiText2 Merity et al. ([2016](#bib.bib33)) and PTB Marcus et al. ([1993](#bib.bib32))
    for language generation with the perplexity (PPL)³³3https://huggingface.co/spaces/evaluate-metric/perplexity
    analysis, and smaller is better. Besides, we follow LLaMA to implement zero-shot
    task classification and multiple-choice on four common sense reasoning datasets:
    BoolQ Clark et al. ([2019](#bib.bib5)), PIQA Bisk et al. ([2020](#bib.bib1)),
    HellaSwag Zellers et al. ([2019](#bib.bib53)), and WinoGrande Sakaguchi et al.
    ([2021](#bib.bib38)). In addition to zero-shot evaluation, we conduct experiments
    on few-shot tasks to evaluate pruned LLMs’ ability to learn in context. We choose
    the Massive Multitask Language Understanding benchmark (MMLU) [Hendrycks et al.
    ([2021](#bib.bib13))] and conduct a 5-shot evaluation to remain consistent with
    the evaluation approach described by [Touvron et al. ([2023](#bib.bib44))]. In
    task classification and multiple-choice on common sense reasoning datasets, as
    well as on MMLU, classification accuracy is used as the performance metric.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 模型、数据集和评估指标。为了验证我们MINI-LLM的有效性和多样性，我们在三种不同结构的开源LLM上进行了测试：LLaMA-7B Touvron 等
    ([2023](#bib.bib44))，BLOOM-7B Workshop ([2023](#bib.bib47))，以及OPT-6.7B Zhang 等
    ([2022](#bib.bib54))。所有模型都在任务无关的框架中进行评估。我们评估修剪模型在WikiText2 Merity 等 ([2016](#bib.bib33))
    和PTB Marcus 等 ([1993](#bib.bib32)) 上的零-shot能力，使用困惑度（PPL）³³3https://huggingface.co/spaces/evaluate-metric/perplexity分析，数值越小越好。此外，我们跟随LLaMA实现了在四个常识推理数据集上的零-shot任务分类和多项选择：BoolQ
    Clark 等 ([2019](#bib.bib5))，PIQA Bisk 等 ([2020](#bib.bib1))，HellaSwag Zellers
    等 ([2019](#bib.bib53))，以及WinoGrande Sakaguchi 等 ([2021](#bib.bib38))。除了零-shot评估外，我们还对少-shot任务进行了实验，以评估修剪LLM在上下文中学习的能力。我们选择了大规模多任务语言理解基准（MMLU）[Hendrycks
    等 ([2021](#bib.bib13))]，并进行5-shot评估，以保持与[Touvron 等 ([2023](#bib.bib44))]描述的评估方法一致。在任务分类和常识推理数据集上的多项选择以及MMLU上，使用分类准确率作为性能指标。
- en: '| Prune ratio | Method | GPU (GB) | WikiText2$\downarrow$ |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 修剪比例 | 方法 | GPU (GB) | WikiText2$\downarrow$ |'
- en: '| 0% | LLaMA-7B Touvron et al. ([2023](#bib.bib44))* | 0 | - | - | 76.50 |
    79.8 | 76.10 | 70.10 | 75.63 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 0% | LLaMA-7B Touvron 等 ([2023](#bib.bib44))* | 0 | - | - | 76.50 | 79.8
    | 76.10 | 70.10 | 75.63 |'
- en: '| LLaMA-7B Ma et al. ([2023](#bib.bib30))* | 0 | 12.62 | 22.15 | 73.18 | 78.35
    | 72.99 | 67.01 | 72.88 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B Ma 等 ([2023](#bib.bib30))* | 0 | 12.62 | 22.15 | 73.18 | 78.35 |
    72.99 | 67.01 | 72.88 |'
- en: '| 20% w/ tune | LLM-Pruner Ma et al. ([2023](#bib.bib30))* | 40.00 | 17.58
    | 30.11 | 64.62 | 77.20 | 68.80 | 63.14 | 68.44 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 20% 带调优 | LLM-Pruner Ma 等 ([2023](#bib.bib30))* | 40.00 | 17.58 | 30.11 |
    64.62 | 77.20 | 68.80 | 63.14 | 68.44 |'
- en: '| magnitude-l1 | 21.60 | 24.32 | 43.19 | 58.47 | 75.35 | 65.40 | 60.93 | 65.04
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l1 | 21.60 | 24.32 | 43.19 | 58.47 | 75.35 | 65.40 | 60.93 | 65.04
    |'
- en: '| magnitude-l2 | 21.60 | 24.23 | 36.16 | 65.02 | 75.14 | 65.07 | 62.12 | 66.84
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l2 | 21.60 | 24.23 | 36.16 | 65.02 | 75.14 | 65.07 | 62.12 | 66.84
    |'
- en: '| SparseGPT Frantar and Alistarh ([2023](#bib.bib8)) | 40.00 | 20.84 | 36.23
    | 55.05 | 75.84 | 66.88 | 61.64 | 64.85 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT Frantar 和 Alistarh ([2023](#bib.bib8)) | 40.00 | 20.84 | 36.23
    | 55.05 | 75.84 | 66.88 | 61.64 | 64.85 |'
- en: '| Wanda Sun et al. ([2024](#bib.bib42)) | 21.90 | 20.36 | 36.15 | 60.92 | 74.70
    | 66.70 | 62.33 | 66.16 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Wanda Sun 等 ([2024](#bib.bib42)) | 21.90 | 20.36 | 36.15 | 60.92 | 74.70
    | 66.70 | 62.33 | 66.16 |'
- en: '|  | MINI-LLM (ours) | 22.40 | 18.32 | 32.54 | 66.76 | 75.46 | 65.61 | 62.43
    | 67.57 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | MINI-LLM (我们的方法) | 22.40 | 18.32 | 32.54 | 66.76 | 75.46 | 65.61 | 62.43
    | 67.57 |'
- en: '| 30% w/ tune | LLM-Pruner Ma et al. ([2023](#bib.bib30)) | 37.16 | 21.55 |
    37.67 | 64.89 | 73.72 | 63.45 | 62.67 | 66.18 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 30% 带调优 | LLM-Pruner Ma 等 ([2023](#bib.bib30)) | 37.16 | 21.55 | 37.67 |
    64.89 | 73.72 | 63.45 | 62.67 | 66.18 |'
- en: '| magnitude-l1 | 20.10 | 31.17 | 54.28 | 61.80 | 73.23 | 58.02 | 56.91 | 62.49
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l1 | 20.10 | 31.17 | 54.28 | 61.80 | 73.23 | 58.02 | 56.91 | 62.49
    |'
- en: '| magnitude-l2 | 20.10 | 31.11 | 51.29 | 61.89 | 73.45 | 57.84 | 58.72 | 62.98
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l2 | 20.10 | 31.11 | 51.29 | 61.89 | 73.45 | 57.84 | 58.72 | 62.98
    |'
- en: '| SparseGPT Frantar and Alistarh ([2023](#bib.bib8)) | 40.00 | 26.82 | 45.71
    | 56.64 | 73.45 | 61.23 | 59.51 | 62.71 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT Frantar 和 Alistarh ([2023](#bib.bib8)) | 40.00 | 26.82 | 45.71
    | 56.64 | 73.45 | 61.23 | 59.51 | 62.71 |'
- en: '| Wanda Sun et al. ([2024](#bib.bib42)) | 20.33 | 27.08 | 46.25 | 56.97 | 74.27
    | 59.27 | 60.46 | 62.74 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Wanda Sun 等 ([2024](#bib.bib42)) | 20.33 | 27.08 | 46.25 | 56.97 | 74.27
    | 59.27 | 60.46 | 62.74 |'
- en: '|  | MINI-LLM (ours) | 20.90 | 24.28 | 39.02 | 64.55 | 73.74 | 58.74 | 58.93
    | 63.99 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | MINI-LLM (我们的方法) | 20.90 | 24.28 | 39.02 | 64.55 | 73.74 | 58.74 | 58.93
    | 63.99 |'
- en: '| 40% w/ tune | LLM-Pruner Ma et al. ([2023](#bib.bib30)) | 36.13 | 28.10 |
    48.66 | 60.46 | 71.33 | 55.62 | 56.43 | 60.96 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 40% 微调 | LLM-Pruner Ma 等人 ([2023](#bib.bib30)) | 36.13 | 28.10 | 48.66 |
    60.46 | 71.33 | 55.62 | 56.43 | 60.96 |'
- en: '| magnitude-l1 | 18.80 | 43.96 | 66.63 | 47.03 | 70.89 | 49.79 | 52.96 | 55.17
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l1 | 18.80 | 43.96 | 66.63 | 47.03 | 70.89 | 49.79 | 52.96 | 55.17
    |'
- en: '| magnitude-l2 | 18.80 | 45.26 | 67.68 | 48.72 | 71.65 | 50.21 | 53.43 | 56.00
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l2 | 18.80 | 45.26 | 67.68 | 48.72 | 71.65 | 50.21 | 53.43 | 56.00
    |'
- en: '| SparseGPT Frantar and Alistarh ([2023](#bib.bib8)) | 40.00 | 37.16 | 66.12
    | 58.26 | 71.44 | 53.91 | 56.99 | 60.15 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT Frantar 和 Alistarh ([2023](#bib.bib8)) | 40.00 | 37.16 | 66.12
    | 58.26 | 71.44 | 53.91 | 56.99 | 60.15 |'
- en: '| Wanda Sun et al. ([2024](#bib.bib42)) | 19.13 | 36.44 | 66.37 | 49.91 | 71.38
    | 53.85 | 58.72 | 58.47 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Wanda Sun 等人 ([2024](#bib.bib42)) | 19.13 | 36.44 | 66.37 | 49.91 | 71.38
    | 53.85 | 58.72 | 58.47 |'
- en: '|  | MINI-LLM (ours) | 19.83 | 31.78 | 49.23 | 63.65 | 71.59 | 53.31 | 55.56
    | 61.02 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | MINI-LLM（我们的） | 19.83 | 31.78 | 49.23 | 63.65 | 71.59 | 53.31 | 55.56
    | 61.02 |'
- en: '| 50% w/ tune | LLM-Pruner Ma et al. ([2023](#bib.bib30))* | 35.00 | 38.12
    | 66.35 | 60.28 | 69.31 | 47.06 | 53.43 | 57.52 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 50% 微调 | LLM-Pruner Ma 等人 ([2023](#bib.bib30))* | 35.00 | 38.12 | 66.35 |
    60.28 | 69.31 | 47.06 | 53.43 | 57.52 |'
- en: '| magnitude-l1 | 17.59 | 61.39 | 91.79 | 40.73 | 66.32 | 42.66 | 51.85 | 50.39
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l1 | 17.59 | 61.39 | 91.79 | 40.73 | 66.32 | 42.66 | 51.85 | 50.39
    |'
- en: '| magnitude-l2 | 17.59 | 58.12 | 89.67 | 38.50 | 67.08 | 43.47 | 52.80 | 50.46
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l2 | 17.59 | 58.12 | 89.67 | 38.50 | 67.08 | 43.47 | 52.80 | 50.46
    |'
- en: '| SparseGPT Frantar and Alistarh ([2023](#bib.bib8)) | 40.00 | 49.52 | 82.28
    | 42.84 | 68.01 | 45.38 | 55.96 | 53.05 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT Frantar 和 Alistarh ([2023](#bib.bib8)) | 40.00 | 49.52 | 82.28
    | 42.84 | 68.01 | 45.38 | 55.96 | 53.05 |'
- en: '| Wanda Sun et al. ([2024](#bib.bib42)) | 18.82 | 45.98 | 78.82 | 40.49 | 69.04
    | 45.10 | 54.93 | 52.39 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Wanda Sun 等人 ([2024](#bib.bib42)) | 18.82 | 45.98 | 78.82 | 40.49 | 69.04
    | 45.10 | 54.93 | 52.39 |'
- en: '|  | MINI-LLM (ours) | 18.82 | 44.69 | 69.83 | 61.35 | 67.85 | 45.39 | 53.12
    | 56.93 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | MINI-LLM（我们的） | 18.82 | 44.69 | 69.83 | 61.35 | 67.85 | 45.39 | 53.12
    | 56.93 |'
- en: 'Table 1: Zero-shot performance of the pruned LLaMA-7B models. “Prune Ratio”
    refers to the proportion of parameters removed relative to the original number
    of parameters. “GPU (GB)” indicates the peak GPU memory usage for pruning. “Average”
    is calculated among four classification datasets. Bold/Underline mark the best/second
    best performance at the same compression rate with fine-tuning, respectively,
    excluding LLM-Pruner in the comparison. An asterisk (*) signifies the results
    are taken directly from the corresponding papers.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：修剪后的 LLaMA-7B 模型的零-shot 性能。“修剪比”指相对于原始参数数量的参数移除比例。“GPU (GB)”表示修剪时的峰值 GPU
    内存使用量。“平均”是四个分类数据集中的计算结果。粗体/下划线标记了在相同压缩率下经过微调的最佳/第二最佳性能（不包括比较中的 LLM-Pruner）。星号（*）表示结果直接来自相应的论文。
- en: Pruning and fine-tuning settings. Our MINI-LLM conducts in a one-shot pruning
    framework. That is scoring only once and then pruning the network to the target
    prune ratio Cheng et al. ([2023](#bib.bib4)). In the model pruning process, we
    use 10 randomly selected samples from Bookcorpus Zhu et al. ([2015](#bib.bib56))
    as the calibration data for evaluating the weight gradients and 128 samples for
    computing each layer’s input (i.e., activation). Due to the varying sensitivity
    of each layer to pruning Ma et al. ([2023](#bib.bib30)), the first four layers
    and the last three layers are retained. During the recovery phase, we utilize
    the Alpaca-cleaned Taori et al. ([2023](#bib.bib43)) as the training dataset,
    which contains approximately 50k samples, to fine-tune the pruned models with
    a batch size of 64\. Following [Ma et al. ([2023](#bib.bib30))], the learning
    rate is set to 1e-4 and a total of 2 epochs. Each pruned model is recovered by
    an Adam optimizer Kingma and Ba ([2015](#bib.bib17)) paired with a cosine decay
    schedule for the learning rate. We set LoRA $r=8$, and attach LoRA modules on
    all linear layers of the base model. In the inference stage, all the evaluations
    are implemented with a context length of 128.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪和微调设置。我们的 MINI-LLM 在单次修剪框架下进行，即仅评分一次，然后将网络修剪到目标修剪比 Cheng 等人 ([2023](#bib.bib4))。在模型修剪过程中，我们使用从
    Bookcorpus Zhu 等人 ([2015](#bib.bib56)) 中随机选取的 10 个样本作为评估权重梯度的校准数据，并使用 128 个样本计算每层的输入（即激活）。由于每层对修剪的敏感度不同
    Ma 等人 ([2023](#bib.bib30))，保留了前四层和最后三层。在恢复阶段，我们利用 Alpaca-cleaned Taori 等人 ([2023](#bib.bib43))
    作为训练数据集，该数据集包含约 50k 个样本，用于用 64 的批量大小微调修剪后的模型。按照 [Ma 等人 ([2023](#bib.bib30))]，学习率设置为
    1e-4，总共训练 2 个周期。每个修剪后的模型由 Adam 优化器 Kingma 和 Ba ([2015](#bib.bib17)) 配合余弦衰减学习率计划进行恢复。我们设置
    LoRA $r=8$，并在基模型的所有线性层上附加 LoRA 模块。在推理阶段，所有评估都在 128 的上下文长度下进行。
- en: 'Baselines. We compare MINI-LLM with four one-shot structured pruning methods
    for LLMs. Magnitude-l1/l2: pruning based on the absolute values or the l2-norm
    of weights, respectively. LLM-Pruner Ma et al. ([2023](#bib.bib30)): pruning using
    criterion Eq. ([3](#S3.E3 "In 3.1 Pruning Criterion in MINI-LLM ‣ 3 Method ‣ MINI-LLM:
    Memory-Efficient Structured Pruning for Large Language Models")) with backpropagation
    gradients. Wanda Sun et al. ([2024](#bib.bib42)): pruning based on the product
    of the magnitude of weights and their corresponding activations. Given that vanilla
    SparseGPT and Wanda are retraining-free unstructured methods, we adapt them for
    structured pruning with pruning and fine-tuning stages for a fair comparison while
    maintaining the same criterion. Except for LLM-pruner, which is a gradient-based
    method, the other methods are all gradient-free methods.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '基线方法。我们将MINI-LLM与四种一-shot结构化剪枝方法进行比较。Magnitude-l1/l2：分别基于权重的绝对值或l2-范数进行剪枝。LLM-Pruner
    Ma et al. ([2023](#bib.bib30))：使用准则Eq. ([3](#S3.E3 "In 3.1 Pruning Criterion in
    MINI-LLM ‣ 3 Method ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large
    Language Models"))和反向传播梯度进行剪枝。Wanda Sun et al. ([2024](#bib.bib42))：基于权重大小和相应激活的乘积进行剪枝。鉴于vanilla
    SparseGPT和Wanda是无需重新训练的无结构方法，我们将其调整为结构化剪枝，并通过剪枝和微调阶段进行公平比较，同时保持相同准则。除LLM-pruner外，其他方法均为无梯度方法。'
- en: 4.2 Main Results
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主要结果
- en: 'Zero-shot performance on LLaMA-7B. We prune LLaMA-7B with four prune ratios:
    from 20% to 50% and fine-tune the pruned models by using LoRA to restore model
    accuracy. The comparisons with the baselines are reported in Table [1](#S4.T1
    "Table 1 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models"). From the results, we see that
    our MINI-LLM consistently surpasses all the gradient-free methods and closely
    matches or even outperforms the LLM-Pruner with backpropagation gradients across
    four prune ratios. For example, at a 20% prune ratio, MINI-LLM achieves an average
    classification accuracy of 67.57% across four inference datasets, better than
    other gradient-free methods, and obtains 92.71% of the accuracy achieved by the
    original model. Although LLM-Pruner achieves better accuracy with 93.91% of the
    accuracy attained by the dense model, the peak GPU memory required for pruning
    by LLM-Pruner is approximately twice that of MINI-LLM. Moreover, although both
    Wanda and MINI-LLM use weight magnitude and activation, MINI-LLM performs better,
    which indicates that estimated gradients are beneficial in guiding pruning. In
    addition, at a 40% prune ratio, MINI-LLM achieves an average accuracy of 61.02%
    on the four tasks, even better than LLM-Pruner’s average accuracy of 60.96%.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLaMA-7B上的零-shot性能。我们以四种剪枝比例（从20%到50%）对LLaMA-7B进行剪枝，并通过使用LoRA微调剪枝后的模型以恢复模型精度。与基线方法的比较结果报告在表[1](#S4.T1
    "Table 1 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models")中。从结果来看，我们的MINI-LLM始终超越所有无梯度方法，并且在四种剪枝比例下与使用反向传播梯度的LLM-Pruner表现接近甚至更好。例如，在20%剪枝比例下，MINI-LLM在四个推理数据集上的平均分类精度达到67.57%，优于其他无梯度方法，并获得了原始模型精度的92.71%。尽管LLM-Pruner在密集模型中取得了93.91%的精度，但LLM-Pruner所需的最大GPU内存约为MINI-LLM的两倍。此外，尽管Wanda和MINI-LLM都使用权重大小和激活，MINI-LLM的表现更佳，这表明估计梯度在指导剪枝方面是有益的。此外，在40%剪枝比例下，MINI-LLM在四个任务上的平均精度为61.02%，甚至优于LLM-Pruner的平均精度60.96%。'
- en: 'However, similar to the observation in Ma et al. Ma et al. ([2023](#bib.bib30)),
    with a high prune ratio, such as 50%, an obvious performance decline is observed,
    as shown in Table [1](#S4.T1 "Table 1 ‣ 4.1 Experimental Setup ‣ 4 Experiments
    ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models"). In
    this situation, our MINI-LLM and LLM-Pruner only retain 78.11% and 78.92% of the
    dense model’s accuracy, respectively. Even for LLMs, structurally pruning under
    high prune ratios remains a major challenge.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，与Ma et al. ([2023](#bib.bib30))的观察类似，在高剪枝比例（如50%）下，观察到明显的性能下降，如表[1](#S4.T1
    "Table 1 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models")所示。在这种情况下，我们的MINI-LLM和LLM-Pruner仅保留了密集模型精度的78.11%和78.92%。即使对于LLMs，在高剪枝比例下进行结构化剪枝仍然是一个主要挑战。'
- en: 'Zero-shot performance on BLOOM-7B and OPT-6.7B. To validate MINI-LLM on other
    LLMs broadly, we prune both BLOOM-7B and OPT-6.7B with two prune ratios: 10% and
    30%, and fine-tune the pruned models to restore model accuracy. The results in
    Table [2](#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models") illustrate that our MINI-LLM steadily
    outperforms all gradient-free methods and exhibits performance comparable to,
    even surpasses at times, that of LLM-Pruner. For instance, at a 30% compression
    rate on BLOOM-7B, MINI-LLM achieves a perplexity of 54.07 on the WikiText2 dataset,
    obviously outperforming LLM-Pruner’s perplexity of 58.11\. Similarly, at a 30%
    compression rate on OPT-6.7B, MINI-LLM achieves a perplexity of 40.89 on the WikiText2
    dataset and 57.44 on the PTB dataset, outperforming LLM-Pruner’s perplexity of
    42.94 and 65.09, respectively. In addition, at a 10% prune ratio on OPT-6.7B,
    MINI-LLM achieves an average classification accuracy of 67.81% across four datasets
    and obtains 98.60% of the accuracy achieved by the original model, which is even
    better than LLM-Pruner’ s 67.50% and 98.15%. This demonstration validates the
    effectiveness of MINI-LLM in efficiently compressing models of various structures
    to a specified size, while optimizing memory usage.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '零-shot 性能在 BLOOM-7B 和 OPT-6.7B 上。为了广泛验证 MINI-LLM 在其他 LLM 上的效果，我们对 BLOOM-7B
    和 OPT-6.7B 进行两种剪枝比例：10% 和 30%，并对剪枝后的模型进行微调以恢复模型准确性。表格 [2](#S4.T2 "Table 2 ‣ 4.2
    Main Results ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for
    Large Language Models") 的结果表明，我们的 MINI-LLM 稳定地超越了所有无梯度方法，并且性能与 LLM-Pruner 相当，有时甚至超越。例如，在
    BLOOM-7B 上以 30% 压缩率，MINI-LLM 在 WikiText2 数据集上达到了 54.07 的困惑度，明显优于 LLM-Pruner 的
    58.11。类似地，在 OPT-6.7B 上以 30% 压缩率，MINI-LLM 在 WikiText2 数据集上达到了 40.89 的困惑度，在 PTB
    数据集上为 57.44，优于 LLM-Pruner 的 42.94 和 65.09。此外，在 OPT-6.7B 上以 10% 剪枝比例，MINI-LLM 在四个数据集上的平均分类准确率为
    67.81%，并达到了原始模型准确率的 98.60%，这比 LLM-Pruner 的 67.50% 和 98.15% 更好。这一结果验证了 MINI-LLM
    在高效压缩各种结构模型到指定大小的同时优化内存使用的有效性。'
- en: '| Prune ratio | Method | GPU (GB) | WikiText2$\downarrow$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝比例 | 方法 | GPU (GB) | WikiText2$\downarrow$ |'
- en: '| 0% | BLOOM-7B Workshop ([2023](#bib.bib47)) | 0 | 26.58 | 50.55 | 62.94 |
    73.61 | 59.69 | 64.4 | 65.16 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 0% | BLOOM-7B Workshop ([2023](#bib.bib47)) | 0 | 26.58 | 50.55 | 62.94 |
    73.61 | 59.69 | 64.4 | 65.16 |'
- en: '| 10% w/ tune | LLM-Pruner Ma et al. ([2023](#bib.bib30)) | 40.00 | 35.32 |
    75.21 | 62.14 | 72.14 | 55.39 | 57.85 | 61.88 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 10% 调优 | LLM-Pruner Ma 等 ([2023](#bib.bib30)) | 40.00 | 35.32 | 75.21 | 62.14
    | 72.14 | 55.39 | 57.85 | 61.88 |'
- en: '| magnitude-l1 | 22.04 | 40.89 | 92.87 | 59.28 | 71.82 | 52.44 | 56.21 | 59.94
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l1 | 22.04 | 40.89 | 92.87 | 59.28 | 71.82 | 52.44 | 56.21 | 59.94
    |'
- en: '| magnitude-l2 | 22.04 | 40.73 | 95.45 | 59.33 | 72.04 | 52.58 | 56.04 | 60.00
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l2 | 22.04 | 40.73 | 95.45 | 59.33 | 72.04 | 52.58 | 56.04 | 60.00
    |'
- en: '| SparseGPT Frantar and Alistarh ([2023](#bib.bib8)) | 40.00 | 40.42 | 92.15
    | 59.17 | 70.73 | 52.38 | 56.2 | 59.62 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT Frantar 和 Alistarh ([2023](#bib.bib8)) | 40.00 | 40.42 | 92.15
    | 59.17 | 70.73 | 52.38 | 56.2 | 59.62 |'
- en: '| Wanda Sun et al. ([2024](#bib.bib42)) | 22.81 | 40.81 | 93.60 | 59.94 | 72.25
    | 52.60 | 57.14 | 60.48 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Wanda Sun 等 ([2024](#bib.bib42)) | 22.81 | 40.81 | 93.60 | 59.94 | 72.25
    | 52.60 | 57.14 | 60.48 |'
- en: '|  | MINI-LLM (ours) | 25.03 | 38.12 | 86.23 | 59.97 | 72.05 | 53.54 | 56.43
    | 60.50 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | MINI-LLM (我们的方法) | 25.03 | 38.12 | 86.23 | 59.97 | 72.05 | 53.54 | 56.43
    | 60.50 |'
- en: '| 30% w/ tune | LLM-Pruner Ma et al. ([2023](#bib.bib30)) | 38.51 | 58.11 |
    147.52 | 62.11 | 67.79 | 44.04 | 53.28 | 56.81 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 30% 调优 | LLM-Pruner Ma 等 ([2023](#bib.bib30)) | 38.51 | 58.11 | 147.52 |
    62.11 | 67.79 | 44.04 | 53.28 | 56.81 |'
- en: '| magnitude-l1 | 19.49 | 87.25 | 166.21 | 61.04 | 65.40 | 41.46 | 51.70 | 54.90
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l1 | 19.49 | 87.25 | 166.21 | 61.04 | 65.40 | 41.46 | 51.70 | 54.90
    |'
- en: '| magnitude-l2 | 19.49 | 79.75 | 167.83 | 59.45 | 66.87 | 42.36 | 50.91 | 54.89
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l2 | 19.49 | 79.75 | 167.83 | 59.45 | 66.87 | 42.36 | 50.91 | 54.89
    |'
- en: '| SparseGPT Frantar and Alistarh ([2023](#bib.bib8)) | 40.00 | 75.51 | 173.51
    | 52.02 | 67.14 | 42.86 | 53.28 | 53.83 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT Frantar 和 Alistarh ([2023](#bib.bib8)) | 40.00 | 75.51 | 173.51
    | 52.02 | 67.14 | 42.86 | 53.28 | 53.83 |'
- en: '| Wanda Sun et al. ([2024](#bib.bib42)) | 20.34 | 84.89 | 170.16 | 53.61 |
    67.03 | 41.34 | 50.99 | 53.24 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Wanda Sun 等 ([2024](#bib.bib42)) | 20.34 | 84.89 | 170.16 | 53.61 | 67.03
    | 41.34 | 50.99 | 53.24 |'
- en: '|  | MINI-LLM (ours) | 22.11 | 54.07 | 121.61 | 62.17 | 68.82 | 44.95 | 51.93
    | 56.97 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | MINI-LLM (我们的方法) | 22.11 | 54.07 | 121.61 | 62.17 | 68.82 | 44.95 | 51.93
    | 56.97 |'
- en: '| 0% | OPT-6.7B Zhang et al. ([2022](#bib.bib54)) | 0 | 26.45 | 32.03 | 66.06
    | 76.55 | 67.21 | 65.27 | 68.77 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 0% | OPT-6.7B Zhang 等 ([2022](#bib.bib54)) | 0 | 26.45 | 32.03 | 66.06 |
    76.55 | 67.21 | 65.27 | 68.77 |'
- en: '| 10% w/ tune | LLM-Pruner Ma et al. ([2023](#bib.bib30)) | 38.00 | 27.89 |
    39.33 | 63.06 | 76.77 | 66.33 | 63.85 | 67.50 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 10% 调优 | LLM-Pruner Ma 等 ([2023](#bib.bib30)) | 38.00 | 27.89 | 39.33 | 63.06
    | 76.77 | 66.33 | 63.85 | 67.50 |'
- en: '| magnitude-l1 | 22.81 | 39.17 | 55.68 | 58.17 | 75.30 | 60.35 | 59.59 | 63.35
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l1 | 22.81 | 39.17 | 55.68 | 58.17 | 75.30 | 60.35 | 59.59 | 63.35
    |'
- en: '| magnitude-l2 | 22.81 | 39.40 | 54.49 | 59.08 | 74.86 | 60.45 | 60.22 | 63.65
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l2 | 22.81 | 39.40 | 54.49 | 59.08 | 74.86 | 60.45 | 60.22 | 63.65
    |'
- en: '| SparseGPT Frantar and Alistarh ([2023](#bib.bib8)) | 40.00 | 36.58 | 50.99
    | 61.93 | 74.81 | 61.25 | 60.46 | 64.61 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT Frantar 和 Alistarh ([2023](#bib.bib8)) | 40.00 | 36.58 | 50.99
    | 61.93 | 74.81 | 61.25 | 60.46 | 64.61 |'
- en: '| Wanda Sun et al. ([2024](#bib.bib42)) | 23.04 | 37.09 | 53.54 | 66.09 | 75.46
    | 62.24 | 62.59 | 66.60 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Wanda Sun 等人 ([2024](#bib.bib42)) | 23.04 | 37.09 | 53.54 | 66.09 | 75.46
    | 62.24 | 62.59 | 66.60 |'
- en: '|  | MINI-LLM (ours) | 23.65 | 30.15 | 38.64 | 65.90 | 76.12 | 65.66 | 63.54
    | 67.81 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | MINI-LLM（我们的） | 23.65 | 30.15 | 38.64 | 65.90 | 76.12 | 65.66 | 63.54
    | 67.81 |'
- en: '| 30% w/ tune | LLM-Pruner Ma et al. ([2023](#bib.bib30)) | 34.66 | 42.94 |
    65.09 | 61.93 | 73.83 | 56.98 | 59.98 | 63.60 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 30% w/ tune | LLM-Pruner Ma 等人 ([2023](#bib.bib30)) | 34.66 | 42.94 | 65.09
    | 61.93 | 73.83 | 56.98 | 59.98 | 63.60 |'
- en: '| magnitude-l1 | 19.91 | 81.96 | 104.01 | 48.50 | 69.59 | 44.99 | 53.75 | 54.21
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l1 | 19.91 | 81.96 | 104.01 | 48.50 | 69.59 | 44.99 | 53.75 | 54.21
    |'
- en: '| magnitude-l2 | 19.91 | 76.10 | 98.86 | 54.22 | 69.37 | 44.83 | 54.14 | 56.11
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| magnitude-l2 | 19.91 | 76.10 | 98.86 | 54.22 | 69.37 | 44.83 | 54.14 | 56.11
    |'
- en: '| SparseGPT Frantar and Alistarh ([2023](#bib.bib8)) | 40.00 | 77.00 | 103.61
    | 54.31 | 69.21 | 44.56 | 55.56 | 55.91 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT Frantar 和 Alistarh ([2023](#bib.bib8)) | 40.00 | 77.00 | 103.61
    | 54.31 | 69.21 | 44.56 | 55.56 | 55.91 |'
- en: '| Wanda Sun et al. ([2024](#bib.bib42)) | 20.07 | 82.93 | 107.32 | 60.34 |
    69.53 | 44.58 | 54.46 | 57.23 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Wanda Sun 等人 ([2024](#bib.bib42)) | 20.07 | 82.93 | 107.32 | 60.34 | 69.53
    | 44.58 | 54.46 | 57.23 |'
- en: '|  | MINI-LLM (ours) | 20.65 | 40.89 | 57.44 | 62.17 | 72.58 | 54.07 | 56.20
    | 61.26 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | MINI-LLM（我们的） | 20.65 | 40.89 | 57.44 | 62.17 | 72.58 | 54.07 | 56.20
    | 61.26 |'
- en: 'Table 2: Zero-shot performance of the pruned BLOOM-7B and OPT-6.7B. Columns
    is consistent with the definitions in Table [1](#S4.T1 "Table 1 ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large
    Language Models"). Unless otherwise specified, “Prune Ratio” and Bold/Underline
    have the same meaning as Table [1](#S4.T1 "Table 1 ‣ 4.1 Experimental Setup ‣
    4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models").'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2：剪枝 BLOOM-7B 和 OPT-6.7B 的零-shot 性能。各列与表 [1](#S4.T1 "Table 1 ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large
    Language Models") 中的定义一致。除非另有说明，"Prune Ratio" 和粗体/下划线在意义上与表 [1](#S4.T1 "Table
    1 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models") 相同。'
- en: 'In addition, we observe that the pruning outcomes achieved by gradient-free
    methods such as Wanda and magnitude l1/l2 shown in Table [2](#S4.T2 "Table 2 ‣
    4.2 Main Results ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning
    for Large Language Models") significantly fell short in comparison to gradient-based
    pruning methods such as LLM-Pruner and MINI-LLM at a prune ratio of 30% on the
    WikiText2 and PTB datasets for BLOOM and OPT. Using LLM-Pruner as a high-quality
    benchmark, we compare Wanda, representing gradient-free approaches, by assessing
    the similarity of their retained channels per layer against LLM-Pruner on the
    WikiText2 dataset. Similarly, we evaluate the similarity between LLM-Pruner and
    MINI-LLM. Specifically, the similarity is calculated by the formula: $\lvert|\text{Intersection}(A,B)\rvert|_{0}/\lvert|A\rvert|_{0}\times
    100\%$ denote the sets of the pruned channels obtained by LLM-Pruner and the examined
    method, respectively. The results are illustrated in Figure [2](#S4.F2 "Figure
    2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning
    for Large Language Models"). We can see that LLM-Pruner and MINI-LLM have more
    similar pruned channels compared to LLM-Pruner and Wanda. As a result, compared
    to gradient-free methods, the perplexity of MINI-LLM in Table [2](#S4.T2 "Table
    2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning
    for Large Language Models") is closer to the results of LLM-Pruner.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们观察到，表 [2](#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ MINI-LLM:
    Memory-Efficient Structured Pruning for Large Language Models") 中显示的像 Wanda 和幅度
    l1/l2 等无梯度方法的修剪结果，与 LLM-Pruner 和 MINI-LLM 等基于梯度的修剪方法在 30% 修剪比例下相比，显著不足。在 WikiText2
    数据集上，以 LLM-Pruner 作为高质量基准，我们通过评估 Wanda（代表无梯度方法）与 LLM-Pruner 保留通道的相似性来进行比较。同样，我们评估
    LLM-Pruner 和 MINI-LLM 之间的相似性。具体地，相似性通过以下公式计算：$\lvert|\text{Intersection}(A,B)\rvert|_{0}/\lvert|A\rvert|_{0}\times
    100\%$ 表示 LLM-Pruner 和待检查方法获得的修剪通道集合。结果如图 [2](#S4.F2 "Figure 2 ‣ 4.2 Main Results
    ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models") 所示。我们可以看到，与 LLM-Pruner 和 Wanda 相比，LLM-Pruner 和 MINI-LLM 的修剪通道更为相似。因此，与无梯度方法相比，表 [2](#S4.T2
    "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models") 中 MINI-LLM 的困惑度更接近 LLM-Pruner 的结果。'
- en: '![Refer to caption](img/70f2604d21fe01ee4b2e443475a6ae82.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/70f2604d21fe01ee4b2e443475a6ae82.png)'
- en: (a) BLOOM-7B
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (a) BLOOM-7B
- en: '![Refer to caption](img/2d1241d3e6369eef018115fb4bb73175.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2d1241d3e6369eef018115fb4bb73175.png)'
- en: (b) OPT-6.7B
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: (b) OPT-6.7B
- en: 'Figure 2: Similarity in pruned channels at the prune ratio of 30%. LLM-Pruner
    and MINI-LLM (ours) have more similar pruned channels compared to LLM-Pruner and
    Wanda.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在修剪比例为 30% 时，修剪通道的相似性。与 LLM-Pruner 和 Wanda 相比，LLM-Pruner 和 MINI-LLM（我们的）具有更相似的修剪通道。
- en: 'Zero-shot Performance on LLaMA-13B Due to the efficient approximation for the
    gradients of the pre-trained weights, MINI-LLM enables pruning on larger-scale
    LLMs, such as LLaMA-13B ⁴⁴4https://huggingface.co/huggyllama/llama-13b/tree/main.
    We prune LLaMA-13B with five pruning ratios: from 10% to 50% and present the zero-shot
    performance of the pruned LLaMA-13B without fine-tuning in Table [3](#S4.T3 "Table
    3 ‣ 4.2 Main Results ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning
    for Large Language Models") and with fine-tuning in Figure [3](#S4.F3 "Figure
    3 ‣ 4.2 Main Results ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning
    for Large Language Models"), respectively. Except for the model pruned with a
    ratio of 50%, for which fine-tuning is conducted for two epochs, the compressed
    models obtained from all other pruning ratios are fine-tuned for just one epoch.
    The other fine-tuning settings, such as the learning rate and batch size, are
    the same as those for recovering LLaMA-7B. We follow Wanda Sun et al. ([2024](#bib.bib42))
    to conduct inference with 2048 tokens.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '由于对预训练权重梯度的高效近似，MINI-LLM 使得在更大规模的 LLM 上进行修剪成为可能，例如 LLaMA-13B ⁴⁴4https://huggingface.co/huggyllama/llama-13b/tree/main。我们在
    LLaMA-13B 上进行五种修剪比例的修剪：从 10% 到 50%，并分别在表 [3](#S4.T3 "Table 3 ‣ 4.2 Main Results
    ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models") 中展示了未微调的修剪 LLaMA-13B 的零-shot 性能，在图 [3](#S4.F3 "Figure 3 ‣ 4.2 Main Results
    ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models") 中展示了微调后的性能。除去 50% 修剪比例的模型外，其他所有修剪比例得到的压缩模型仅微调了一个周期。其他微调设置，如学习率和批次大小，均与恢复
    LLaMA-7B 时相同。我们按照 Wanda Sun 等人 ([2024](#bib.bib42)) 的方法进行 2048 tokens 的推理。'
- en: 'As shown in Table [3](#S4.T3 "Table 3 ‣ 4.2 Main Results ‣ 4 Experiments ‣
    MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models"), MINI-LLM
    outperforms the magnitude-based method (l2-norm) significantly when fine-tuning
    is not applied. For example, with a pruning ratio of 30%, MINI-LLM achieves a
    perplexity of 11.04 compared to the magnitude-based method’s 316.65 on the WikiText2
    dataset. Similarly, as depicted in Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Main Results
    ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models"), MINI-LLM consistently maintains its substantial advantage over the magnitude-based
    method across a spectrum of pruning ratios when subjected to fine-tuning.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[3](#S4.T3 "表 3 ‣ 4.2 主要结果 ‣ 4 实验 ‣ MINI-LLM: 适用于大型语言模型的内存高效结构化剪枝")所示，当不进行微调时，MINI-LLM
    明显优于基于幅度的方法（l2-norm）。例如，在 30% 剪枝比率下，MINI-LLM 在 WikiText2 数据集上的困惑度为 11.04，而基于幅度的方法为
    316.65。类似地，如图[3](#S4.F3 "图 3 ‣ 4.2 主要结果 ‣ 4 实验 ‣ MINI-LLM: 适用于大型语言模型的内存高效结构化剪枝")所示，MINI-LLM
    在多种剪枝比率下在微调时始终保持其显著优势。'
- en: '| Method | Dataset | Prune Ratio |  |  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Method | Dataset | Prune Ratio |  |  |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0% | 10% | 20% | 30% | 40% | 50% |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 0% | 10% | 20% | 30% | 40% | 50% |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Magnitude (l2-norm) | WikiText2 | 5.09 | 13.78 | 21.42 | 316.65 | 3918.40
    | 12550.30 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude (l2-norm) | WikiText2 | 5.09 | 13.78 | 21.42 | 316.65 | 3918.40
    | 12550.30 |'
- en: '| MINI-LLM | 5.09 | 5.87 | 7.51 | 11.04 | 22.32 | 115.13 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| MINI-LLM | 5.09 | 5.87 | 7.51 | 11.04 | 22.32 | 115.13 |'
- en: '| Magnitude (l2-norm) | PTB | 19.24 | 50.89 | 91.87 | 694.34 | 4236.80 | 12847.92
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude (l2-norm) | PTB | 19.24 | 50.89 | 91.87 | 694.34 | 4236.80 | 12847.92
    |'
- en: '| MINI-LLM | 19.24 | 23.86 | 33.38 | 49.91 | 91.43 | 176.93 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| MINI-LLM | 19.24 | 23.86 | 33.38 | 49.91 | 91.43 | 176.93 |'
- en: 'Table 3: Zero-shot perplexity of the pruned LLaMA-13B when fine-tuning is not
    applied.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：未应用微调时剪枝 LLaMA-13B 的零样本困惑度。
- en: '![Refer to caption](img/05477c1ad50418f75a315a24da98dfc4.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/05477c1ad50418f75a315a24da98dfc4.png)'
- en: (a) WikiText2
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: (a) WikiText2
- en: '![Refer to caption](img/02055f5b79bc71495c23684c7fbb9f15.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/02055f5b79bc71495c23684c7fbb9f15.png)'
- en: (b) PTB
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: (b) PTB
- en: 'Figure 3: Zero-shot perplexity of the pruned LLaMA-13B models when fine-tuning
    is applied. MINI-LLM consistently maintains its substantial advantage over the
    magnitude-based method across a spectrum of pruning ratios.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：应用微调时剪枝 LLaMA-13B 模型的零样本困惑度。MINI-LLM 在各种剪枝比率下始终保持其对基于幅度的方法的显著优势。
- en: 'Few-shot performance on LLaMA-7B. In Table [4](#S4.T4 "Table 4 ‣ 4.2 Main Results
    ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models"), we report the mean accuracies for both dense LLMs and sparse LLMs with
    20% to 50% sparsity. In the few-shot setting, MINI-LLM performs competitively
    with other methods, including backpropagation gradient-based LLM-Pruner. Specifically,
    at a 20% prune ratio, MINI-LLM achieves an average accuracy of 26.60%, which surpasses
    SparseGPT’s 25.80% and LLM-Pruner’s 25.30%. Notably, estimated gradient-based
    MINI-LLM consistently surpasses backpropagation gradient-based LLM-Pruner. This
    performance is not observed in zero-shot setting.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '在 LLaMA-7B 上的少量样本性能。在表[4](#S4.T4 "表 4 ‣ 4.2 主要结果 ‣ 4 实验 ‣ MINI-LLM: 适用于大型语言模型的内存高效结构化剪枝")中，我们报告了稠密
    LLM 和稀疏 LLM 在 20% 到 50% 稀疏率下的平均准确率。在少量样本设置下，MINI-LLM 与其他方法（包括反向传播梯度基础的 LLM-Pruner）表现竞争力。具体来说，在
    20% 剪枝比率下，MINI-LLM 实现了 26.60% 的平均准确率，超越了 SparseGPT 的 25.80% 和 LLM-Pruner 的 25.30%。值得注意的是，估计的基于梯度的
    MINI-LLM 始终超越了反向传播梯度基础的 LLM-Pruner。这种性能在零样本设置中未被观察到。'
- en: '| Ratio | Method | MMLU (5-shot) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Ratio | Method | MMLU (5-shot) |'
- en: '| STEM | Humans | Social | Other | Avg. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| STEM | Humans | Social | Other | Avg. |'
- en: '| 0% | - | 32.60 | 34.10 | 40.40 | 40.90 | 36.70 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 0% | - | 32.60 | 34.10 | 40.40 | 40.90 | 36.70 |'
- en: '| 20% | SparseGPT Frantar and Alistarh ([2023](#bib.bib8)) | 25.30 | 25.90
    | 25.50 | 26.30 | 25.80 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 20% | SparseGPT Frantar 和 Alistarh ([2023](#bib.bib8)) | 25.30 | 25.90 |
    25.50 | 26.30 | 25.80 |'
- en: '| Wanda Sun et al. ([2024](#bib.bib42)) | 23.30 | 25.80 | 23.20 | 24.60 | 24.40
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Wanda Sun 等 ([2024](#bib.bib42)) | 23.30 | 25.80 | 23.20 | 24.60 | 24.40
    |'
- en: '| LLM-Pruner Ma et al. ([2023](#bib.bib30)) | 24.40 | 25.30 | 23.80 | 27.30
    | 25.30 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner Ma 等 ([2023](#bib.bib30)) | 24.40 | 25.30 | 23.80 | 27.30 | 25.30
    |'
- en: '|  | MINI-LLM (ours) | 25.50 | 25.90 | 26.20 | 29.00 | 26.60 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | MINI-LLM (我们的) | 25.50 | 25.90 | 26.20 | 29.00 | 26.60 |'
- en: '| 30% | SparseGPT Frantar and Alistarh ([2023](#bib.bib8)) | 25.80 | 25.70
    | 24.60 | 23.50 | 25.00 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 30% | SparseGPT Frantar 和 Alistarh ([2023](#bib.bib8)) | 25.80 | 25.70 |
    24.60 | 23.50 | 25.00 |'
- en: '| Wanda Sun et al. ([2024](#bib.bib42)) | 25.80 | 27.10 | 25.00 | 24.80 | 25.80
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Wanda Sun 等 ([2024](#bib.bib42)) | 25.80 | 27.10 | 25.00 | 24.80 | 25.80
    |'
- en: '| LLM-Pruner Ma et al. ([2023](#bib.bib30)) | 23.90 | 24.90 | 23.50 | 26.00
    | 24.60 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner Ma 等 ([2023](#bib.bib30)) | 23.90 | 24.90 | 23.50 | 26.00 | 24.60
    |'
- en: '|  | MINI-LLM (ours) | 24.10 | 24.80 | 25.70 | 26.20 | 25.20 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | MINI-LLM（我们的） | 24.10 | 24.80 | 25.70 | 26.20 | 25.20 |'
- en: '| 40% | SparseGPT Frantar and Alistarh ([2023](#bib.bib8)) | 26.10 | 25.50
    | 23.30 | 23.90 | 24.80 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 40% | SparseGPT Frantar 和 Alistarh ([2023](#bib.bib8)) | 26.10 | 25.50 |
    23.30 | 23.90 | 24.80 |'
- en: '| Wanda Sun et al. ([2024](#bib.bib42)) | 25.80 | 24.50 | 24.80 | 23.60 | 25.80
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Wanda Sun 等 ([2024](#bib.bib42)) | 25.80 | 24.50 | 24.80 | 23.60 | 25.80
    |'
- en: '| LLM-Pruner Ma et al. ([2023](#bib.bib30)) | 22.70 | 24.40 | 21.40 | 24.00
    | 23.30 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner Ma 等 ([2023](#bib.bib30)) | 22.70 | 24.40 | 21.40 | 24.00 | 23.30
    |'
- en: '|  | MINI-LLM (ours) | 26.20 | 24.10 | 27.50 | 27.50 | 26.10 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | MINI-LLM（我们的） | 26.20 | 24.10 | 27.50 | 27.50 | 26.10 |'
- en: '| 50% | SparseGPT Frantar and Alistarh ([2023](#bib.bib8)) | 26.40 | 24.70
    | 25.40 | 24.20 | 25.10 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 50% | SparseGPT Frantar 和 Alistarh ([2023](#bib.bib8)) | 26.40 | 24.70 |
    25.40 | 24.20 | 25.10 |'
- en: '| Wanda Sun et al. ([2024](#bib.bib42)) | 26.00 | 25.10 | 24.30 | 25.00 | 25.10
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Wanda Sun 等 ([2024](#bib.bib42)) | 26.00 | 25.10 | 24.30 | 25.00 | 25.10
    |'
- en: '| LLM-Pruner Ma et al. ([2023](#bib.bib30)) | 21.30 | 24.20 | 21.70 | 23.70
    | 22.90 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner Ma 等 ([2023](#bib.bib30)) | 21.30 | 24.20 | 21.70 | 23.70 | 22.90
    |'
- en: '|  | MINI-LLM (ours) | 26.30 | 24.80 | 25.00 | 25.30 | 25.30 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | MINI-LLM（我们的） | 26.30 | 24.80 | 25.00 | 25.30 | 25.30 |'
- en: 'Table 4: Few-shot performance of the pruned LLaMA-7B models. “Ratio”, Bold/Underline,
    “Avg.” have the same meaning as Table [1](#S4.T1 "Table 1 ‣ 4.1 Experimental Setup
    ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models")'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4：剪枝后的 LLaMA-7B 模型的少样本表现。“Ratio”、**Bold/Underline**、“Avg.” 在表 [1](#S4.T1
    "表 1 ‣ 4.1 实验设置 ‣ 4 实验 ‣ MINI-LLM: 面向大型语言模型的高效记忆结构剪枝") 中具有相同的含义。'
- en: 'Model size, complexity, and inference time. Table [5](#S4.T5 "Table 5 ‣ 4.3
    Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning
    for Large Language Models") shows the number of parameters, MACs, GPU memory requirements,
    and total inference time for running the original model and the pruned LLaMA-7B
    models at different prune ratios. The results indicate that when the model is
    pruned by 50%, the total inference time is reduced to 58% and the GPU memory usage
    concurrently drops to 50% of its original values, respectively. The evaluation
    is conducted in the inference mode and the sequence length is set to 64\. The
    inference time is tested under the test dataset of WikiText2 on a single NVIDIA
    GeForce RTX 3090Ti (24GB).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '模型大小、复杂性和推理时间。表 [5](#S4.T5 "表 5 ‣ 4.3 消融研究 ‣ 4 实验 ‣ MINI-LLM: 面向大型语言模型的高效记忆结构剪枝")
    显示了原始模型和剪枝后的 LLaMA-7B 模型在不同剪枝比率下的参数数量、MACs、GPU 内存要求和总推理时间。结果表明，当模型剪枝 50% 时，总推理时间减少到
    58%，同时 GPU 内存使用量降至原始值的 50%。评估在推理模式下进行，序列长度设置为 64。推理时间在单个 NVIDIA GeForce RTX 3090Ti（24GB）的
    WikiText2 测试数据集下进行测试。'
- en: 4.3 Ablation Study
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: 'Efficacy of estimated gradients on LLaMA-7B. To enhance GPU memory efficiency
    over traditional backpropagation gradients, we utilize the classical ZO gradient
    estimation based on SPSA to approximately compute weight gradients with only forward
    passes for LLM pruning. Although SPSA-based ZO optimization is theoretically founded
    (Spall ([1992](#bib.bib40), [1997](#bib.bib41)); Gasnikov et al. ([2022](#bib.bib11))),
    we especially reveal the effectiveness of the estimated gradients for guiding
    pruning LLMs in Figure [4](#S4.F4 "Figure 4 ‣ 4.3 Ablation Study ‣ 4 Experiments
    ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models"). As
    we can see, the results demonstrate that our score function FMS, $\left|W\hat{\nabla}\mathcal{L}(W)X\right|$
    compared to Wanda’s. This indicates that the performance improvement over Wanda’s
    comes from the estimated gradient information. This observation underscores the
    superior performance and effectiveness of gradient-based pruning methods in our
    experiments. However, as we previously mentioned, gradients based on backpropagation
    lead to substantial memory consumption and are less feasible. Therefore, gradient
    estimation based on the forward passes becomes valuable, allowing the criterion
    to incorporate guidance information from gradients.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '估计梯度在 LLaMA-7B 上的效果。为了提高 GPU 内存效率，相较于传统的反向传播梯度，我们利用基于 SPSA 的经典 ZO 梯度估计，通过仅进行前向传播来近似计算权重梯度，以用于
    LLM 剪枝。尽管基于 SPSA 的 ZO 优化在理论上已有基础（Spall ([1992](#bib.bib40), [1997](#bib.bib41));
    Gasnikov et al. ([2022](#bib.bib11)))，我们特别揭示了在图 [4](#S4.F4 "Figure 4 ‣ 4.3 Ablation
    Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large
    Language Models") 中，估计梯度在指导剪枝 LLMs 方面的有效性。如我们所见，结果显示我们的评分函数 FMS，$\left|W\hat{\nabla}\mathcal{L}(W)X\right|$
    相比于 Wanda 的。这表明，相对于 Wanda 的，性能提升来源于估计梯度信息。这一观察突显了我们实验中基于梯度的剪枝方法的优越性能和有效性。然而，正如我们之前提到的，基于反向传播的梯度会导致大量内存消耗且不够可行。因此，基于前向传播的梯度估计变得非常有价值，允许准则结合来自梯度的指导信息。'
- en: '![Refer to caption](img/51f534e80914735417882e4120ad97b0.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/51f534e80914735417882e4120ad97b0.png)'
- en: (a) WikiText2
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: (a) WikiText2
- en: '![Refer to caption](img/81e69457070e53bc2f2beca14d2c5eb4.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/81e69457070e53bc2f2beca14d2c5eb4.png)'
- en: (b) PTB
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: (b) PTB
- en: 'Figure 4: The outcomes of gradient-based vs. gradient-free criteria for pruning
    LLaMA-7B. The results demonstrate that our score function FMS consistently yields
    better performance compared to Wanda’s pruning criterion.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 基于梯度与非梯度准则在剪枝 LLaMA-7B 上的结果。结果表明，我们的评分函数 FMS 一致地表现出比 Wanda 的剪枝准则更好的性能。'
- en: '| Ratio | #Params | #MACs | GPU Memory | Inference Time |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 比例 | 参数数 | MACs | GPU 内存 | 推理时间 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0% | 6.74B | 424.02G | 12884.5MB | 88.81s |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 0% | 6.74B | 424.02G | 12884.5MB | 88.81s |'
- en: '| 20% | 5.42B | 340.48G | 10375.5MB | 71.77s |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 20% | 5.42B | 340.48G | 10375.5MB | 71.77s |'
- en: '| 50% | 3.39B | 279.37G | 6519.0MB | 51.18s |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 3.39B | 279.37G | 6519.0MB | 51.18s |'
- en: 'Table 5: Model size, complexity, and inference time of the original model and
    the pruned LLaMA-7B models. “Inference Time” means the total inference time on
    WikiText2 test dataset. The evaluation is conducted in inference mode with a sequence
    length of 64, and the inference time is tested on a single NVIDIA GeForce RTX
    3090 Ti (24GB).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 原始模型和剪枝后的 LLaMA-7B 模型的模型大小、复杂性和推理时间。“推理时间”指的是 WikiText2 测试数据集上的总推理时间。评估在推理模式下进行，序列长度为
    64，推理时间在单个 NVIDIA GeForce RTX 3090 Ti (24GB) 上测试。'
- en: 'Efficacy of Estimated Gradients on BLOOM-7B and OPT-6.7B In the main body,
    we explored the effectiveness of the estimated gradients for guiding the pruning
    of LLaMA-7B. Here, we delve further into the effectiveness of the estimated gradients
    in guiding the pruning process for BLOOM-7B ⁵⁵5https://huggingface.co/bigscience/bloom-7b1/tree/main
    and OPT-6.7B ⁶⁶6https://huggingface.co/facebook/opt-6.7b/tree/main. The experimental
    results presented in Table [6](#S4.T6 "Table 6 ‣ 4.3 Ablation Study ‣ 4 Experiments
    ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models") indicate
    that our score function FMS, $|W\hat{\nabla}\mathcal{L}(W)X|$.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '估计梯度在 BLOOM-7B 和 OPT-6.7B 上的效果。在主体部分，我们探讨了估计梯度在指导剪枝 LLaMA-7B 方面的有效性。在这里，我们进一步探讨了估计梯度在指导剪枝
    BLOOM-7B ⁵⁵5https://huggingface.co/bigscience/bloom-7b1/tree/main 和 OPT-6.7B ⁶⁶6https://huggingface.co/facebook/opt-6.7b/tree/main
    过程中的有效性。表 [6](#S4.T6 "Table 6 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM:
    Memory-Efficient Structured Pruning for Large Language Models") 中呈现的实验结果表明，我们的评分函数
    FMS，$|W\hat{\nabla}\mathcal{L}(W)X|$。'
- en: '| Prune Ratio | Model/Criterion | WikiText2$\downarrow$ |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝比例 | 模型/准则 | WikiText2$\downarrow$ |'
- en: '| 0% | BLOOM-7B | 26.58 | - | 50.55 | - |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 0% | BLOOM-7B | 26.58 | - | 50.55 | - |'
- en: '| 10% | $&#124;WX&#124;$ | 40.81 | 2.69 | 93.60 | 7.37 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 10% | $\left\|WX\right\|$ | 40.81 | 2.69 | 93.60 | 7.37 |'
- en: '| w/ tune | $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 38.12 | 86.23 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| w/ tune | $\left\|W\hat{\nabla}\mathcal{L}(W)X\right\|$ | 38.12 | 86.23 |'
- en: '| 30% | $&#124;WX&#124;$ | 84.89 | 30.82 | 170.16 | 48.55 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 30% | $\left\|WX\right\|$ | 84.89 | 30.82 | 170.16 | 48.55 |'
- en: '| w/ tune | $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 54.07 | 121.61 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| w/ tune | $\left\|W\hat{\nabla}\mathcal{L}(W)X\right\|$ | 54.07 | 121.61
    |'
- en: '| 0% | OPT-6.7B | 26.45 | - | 32.03 | - |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 0% | OPT-6.7B | 26.45 | - | 32.03 | - |'
- en: '| 10% | $&#124;WX&#124;$ | 37.09 | 6.94 | 53.54 | 14.90 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 10% | $\left\|WX\right\|$ | 37.09 | 6.94 | 53.54 | 14.90 |'
- en: '| w/ tune | $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 30.15 | 38.64 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| w/ tune | $\left\|W\hat{\nabla}\mathcal{L}(W)X\right\|$ | 30.15 | 38.64 |'
- en: '| 30% | $&#124;WX&#124;$ | 82.93 | 42.04 | 107.32 | 49.88 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 30% | $\left\|WX\right\|$ | 82.93 | 42.04 | 107.32 | 49.88 |'
- en: '| w/ tune | $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 40.89 | 57.44 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| w/ tune | $\left\|W\hat{\nabla}\mathcal{L}(W)X\right\|$ | 40.89 | 57.44 |'
- en: 'Table 6: The outcomes of gradient-based vs. gradient-free criteria for pruning
    BLOOM-7B and OPT-6.7B. $\Delta$ value indicates a greater improvement in performance.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：用于修剪BLOOM-7B和OPT-6.7B的基于梯度与非梯度标准的结果。$\Delta$值表示性能的显著改善。
- en: 'Efficacy of activation on LLaMA-7B. Dettmers et al. ([2022](#bib.bib6)); Kovaleva
    et al. ([2021](#bib.bib18)) identified a distinct property of LLMs that a few
    hidden state features possess notably high magnitudes. Eliminating these features
    results in a considerable decline in performance. As argued in Section [3.1](#S3.SS1
    "3.1 Pruning Criterion in MINI-LLM ‣ 3 Method ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models"), the vanilla pruning criterion $\left|W\nabla\mathcal{L}(W)\right|$,
    almost does not require additional GPU memory overhead.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '激活对LLaMA-7B的效果。Dettmers 等人（[2022](#bib.bib6)）；Kovaleva 等人（[2021](#bib.bib18)）识别出LLM的一个独特属性，即一些隐藏状态特征的幅度显著较高。消除这些特征会导致性能显著下降。如[3.1](#S3.SS1
    "3.1 Pruning Criterion in MINI-LLM ‣ 3 Method ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models")节所述，普通的修剪标准$\left|W\nabla\mathcal{L}(W)\right|$几乎不需要额外的GPU内存开销。'
- en: '| Ratio | Criterion | WikiText2$\downarrow$ |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Ratio | Criterion | WikiText2$\downarrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 20% | $\left&#124;W\nabla\mathcal{L}(W)\right&#124;$ | 17.79 | 0.34 | 30.57
    | -0.12 | 68.44 | 0.91 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 20% | $\left\|W\nabla\mathcal{L}(W)\right\|$ | 17.79 | 0.34 | 30.57 | -0.12
    | 68.44 | 0.91 |'
- en: '| $\left&#124;W\nabla\mathcal{L}(W)X\right&#124;$ | 17.45 | 30.69 | 69.35 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| $\left\|W\nabla\mathcal{L}(W)X\right\|$ | 17.45 | 30.69 | 69.35 |'
- en: '| 30% | $\left&#124;W\nabla\mathcal{L}(W)\right&#124;$ | 21.55 | 0.38 | 37.67
    | 0.80 | 66.18 | 0.39 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 30% | $\left\|W\nabla\mathcal{L}(W)\right\|$ | 21.55 | 0.38 | 37.67 | 0.80
    | 66.18 | 0.39 |'
- en: '| $\left&#124;W\nabla\mathcal{L}(W)X\right&#124;$ | 21.17 | 36.87 | 66.57 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| $\left\|W\nabla\mathcal{L}(W)X\right\|$ | 21.17 | 36.87 | 66.57 |'
- en: '| 40% | $\left&#124;W\nabla\mathcal{L}(W)\right&#124;$ | 28.10 | 0.08 | 48.66
    | 2.05 | 60.96 | 1.11 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 40% | $\left\|W\nabla\mathcal{L}(W)\right\|$ | 28.10 | 0.08 | 48.66 | 2.05
    | 60.96 | 1.11 |'
- en: '| $\left&#124;W\nabla\mathcal{L}(W)X\right&#124;$ | 28.02 | 46.61 | 62.07 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| $\left\|W\nabla\mathcal{L}(W)X\right\|$ | 28.02 | 46.61 | 62.07 |'
- en: '| 50% | $\left&#124;W\nabla\mathcal{L}(W)\right&#124;$ | 39.48 | 0.38 | 66.37
    | 3.53 | 57.52 | 0.94 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 50% | $\left\|W\nabla\mathcal{L}(W)\right\|$ | 39.48 | 0.38 | 66.37 | 3.53
    | 57.52 | 0.94 |'
- en: '| $\left&#124;W\nabla\mathcal{L}(W)X\right&#124;$ | 39.10 | 62.84 | 58.46 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| $\left\|W\nabla\mathcal{L}(W)X\right\|$ | 39.10 | 62.84 | 58.46 |'
- en: 'Table 7: Zero-shot performance of the pruned LLaMA-7B models achieved by using
    backpropagation gradient-based pruning criterion with/without activations. “Ratio”
    refers to the prune ratio. $\Delta$ value indicates a greater improvement in performance.
    “Avg.” has the same meaning as “Average” in Table [1](#S4.T1 "Table 1 ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large
    Language Models").'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '表7：使用反向传播梯度修剪标准（带/不带激活）对修剪后的LLaMA-7B模型的零-shot性能。 “Ratio”指的是修剪比例。$\Delta$值表示性能的显著改善。“Avg.”在表[1](#S4.T1
    "Table 1 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models")中与“Average”具有相同含义。'
- en: 'In Table [7](#S4.T7 "Table 7 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM:
    Memory-Efficient Structured Pruning for Large Language Models"), gradients in
    both pruning criteria are calculated by backpropagation. In contrast, the results
    in Table [8](#S4.T8 "Table 8 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM:
    Memory-Efficient Structured Pruning for Large Language Models") demonstrate the
    effectiveness of activation in estimated gradients-based pruning criteria. Similar
    to Table [7](#S4.T7 "Table 7 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM:
    Memory-Efficient Structured Pruning for Large Language Models"), the difference
    between the two criteria in Table [8](#S4.T8 "Table 8 ‣ 4.3 Ablation Study ‣ 4
    Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models") also lies in whether they include activation information or not. The
    results in Table [8](#S4.T8 "Table 8 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM:
    Memory-Efficient Structured Pruning for Large Language Models") indicate that
    using the pruning criterion with activation consistently yielded better results.
    For example, on the WikiText2 dataset, the model pruned with 50% of its parameters
    using $|W\hat{\nabla}\mathcal{L}(W)X|$. Switching to the PTB dataset on the same
    compression rate, also yielded positive results, with the model’s perplexity dropping
    from 75.50 to 69.83, confirming the efficacy of the activation-inclusive pruning
    criterion across diverse data.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[7](#S4.T7 "Table 7 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models")中，两种剪枝标准的梯度都是通过反向传播计算的。相比之下，表[8](#S4.T8
    "Table 8 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models")的结果展示了激活在基于估计梯度的剪枝标准中的有效性。类似于表[7](#S4.T7 "Table
    7 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models")，表[8](#S4.T8 "Table 8 ‣ 4.3 Ablation Study
    ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models")中的两个标准之间的差异也在于是否包含激活信息。表[8](#S4.T8 "Table 8 ‣ 4.3 Ablation Study ‣ 4 Experiments
    ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models")的结果表明，使用包含激活的剪枝标准
    consistently yield better results。例如，在WikiText2数据集上，使用$|W\hat{\nabla}\mathcal{L}(W)X|$剪枝50%参数的模型表现良好。在相同压缩率下转到PTB数据集也得到了积极结果，模型的困惑度从75.50降到69.83，验证了在多样数据中包含激活的剪枝标准的有效性。'
- en: 'Consequently, the results in Table [7](#S4.T7 "Table 7 ‣ 4.3 Ablation Study
    ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models") and Table [8](#S4.T8 "Table 8 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣
    MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models") demonstrate
    our pruning criterion FWS overall outperforms its counterpart without activation,
    whether the gradients are backpropagated or approximated.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，表[7](#S4.T7 "Table 7 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models")和表[8](#S4.T8 "Table 8 ‣ 4.3 Ablation
    Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large
    Language Models")的结果表明，我们的剪枝标准FWS总体上优于其没有激活的对手，无论梯度是通过反向传播计算还是近似计算。'
- en: '| Ratio | Criterion | WikiText2$\downarrow$ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 比例 | 标准 | WikiText2$\downarrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 20% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 20.40 | 2.08 | 34.17 |
    1.63 | 67.24 | 0.33 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 20% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 20.40 | 2.08 | 34.17 |
    1.63 | 67.24 | 0.33 |'
- en: '| $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 18.32 | 32.54 | 67.57 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 18.32 | 32.54 | 67.57 |'
- en: '| 30% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 25.09 | 0.81 | 40.89 |
    1.87 | 61.97 | 2.02 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 30% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 25.09 | 0.81 | 40.89 |
    1.87 | 61.97 | 2.02 |'
- en: '| $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 24.28 | 39.02 | 63.99 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 24.28 | 39.02 | 63.99 |'
- en: '| 40% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 35.25 | 3.47 | 52.30 |
    3.07 | 58.95 | 2.07 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 40% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 35.25 | 3.47 | 52.30 |
    3.07 | 58.95 | 2.07 |'
- en: '| $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 31.78 | 49.23 | 61.02 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 31.78 | 49.23 | 61.02 |'
- en: '| 50% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 53.23 | 8.54 | 75.50 |
    5.67 | 54.73 | 2.20 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 50% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 53.23 | 8.54 | 75.50 |
    5.67 | 54.73 | 2.20 |'
- en: '| $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 44.69 | 69.83 | 56.93 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 44.69 | 69.83 | 56.93 |'
- en: 'Table 8: Zero-shot performance of the pruned LLaMA-7B models achieved by using
    ZO gradient-based pruning criterion with/without activations. The columns have
    the same meaning as Table [7](#S4.T7 "Table 7 ‣ 4.3 Ablation Study ‣ 4 Experiments
    ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models").'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 使用ZO梯度基础修剪准则带/不带激活对剪枝后的LLaMA-7B模型的零-shot性能评估。各列的含义与表 [7](#S4.T7 "Table
    7 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models")相同。'
- en: 'Efficacy of activations on BLOOM-7B and OPT-6.7B Table [9](#S4.T9 "Table 9
    ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning
    for Large Language Models"), Table [10](#S4.T10 "Table 10 ‣ 4.3 Ablation Study
    ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models") and Table [11](#S4.T11 "Table 11 ‣ 4.3 Ablation Study ‣ 4 Experiments
    ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models") show
    the zero-shot results of the pruned models with/without fine-tuning to discover
    the effectiveness of activations for guiding pruning LLMs. From the results in
    Table [9](#S4.T9 "Table 9 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models"), it is evident that incorporating
    activations $|W\nabla\mathcal{L}(W)X|$, the perplexity result decreased by 0.51.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '激活对BLOOM-7B和OPT-6.7B的有效性表 [9](#S4.T9 "Table 9 ‣ 4.3 Ablation Study ‣ 4 Experiments
    ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models")、表 [10](#S4.T10
    "Table 10 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models")和表 [11](#S4.T11 "Table 11 ‣ 4.3 Ablation Study
    ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models")展示了带/不带微调的剪枝模型的零-shot结果，以发现激活在指导剪枝LLMs中的有效性。从表 [9](#S4.T9 "Table 9 ‣ 4.3
    Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning
    for Large Language Models")中的结果可以明显看出，加入激活 $|W\nabla\mathcal{L}(W)X|$，困惑度结果减少了0.51。'
- en: '| Prune Ratio | Model/Criterion | WikiText2$\downarrow$ |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝比例 | 模型/准则 | WikiText2$\downarrow$ |'
- en: '| 0% | BLOOM-7B | 26.58 | - | 50.55 | - |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 0% | BLOOM-7B | 26.58 | - | 50.55 | - |'
- en: '| 10% | $&#124;W\nabla\mathcal{L}(W)&#124;$ | 38.12 | 2.59 | 86.23 | 8.63 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 10% | $&#124;W\nabla\mathcal{L}(W)&#124;$ | 38.12 | 2.59 | 86.23 | 8.63 |'
- en: '| w/ tune | $&#124;W\nabla\mathcal{L}(W)X&#124;$ | 35.53 | 77.60 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| w/ tune | $&#124;W\nabla\mathcal{L}(W)X&#124;$ | 35.53 | 77.60 |'
- en: '| 30% | $&#124;W\nabla\mathcal{L}(W)&#124;$ | 58.11 | -0.24 | 147.52 | 9.72
    |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 30% | $&#124;W\nabla\mathcal{L}(W)&#124;$ | 58.11 | -0.24 | 147.52 | 9.72
    |'
- en: '| w/ tune | $&#124;W\nabla\mathcal{L}(W)X&#124;$ | 58.35 | 137.80 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| w/ tune | $&#124;W\nabla\mathcal{L}(W)X&#124;$ | 58.35 | 137.80 |'
- en: '| 0% | OPT-6.7B | 26.45 | - | 32.03 | - |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 0% | OPT-6.7B | 26.45 | - | 32.03 | - |'
- en: '| 10% | $&#124;W\nabla\mathcal{L}(W)&#124;$ | 27.89 | -0.38 | 39.33 | 0.39
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 10% | $&#124;W\nabla\mathcal{L}(W)&#124;$ | 27.89 | -0.38 | 39.33 | 0.39
    |'
- en: '| w/ tune | $&#124;W\nabla\mathcal{L}(W)X&#124;$ | 28.27 | 38.94 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| w/ tune | $&#124;W\nabla\mathcal{L}(W)X&#124;$ | 28.27 | 38.94 |'
- en: '| 30% | $&#124;W\nabla\mathcal{L}(W)&#124;$ | 42.94 | 0.75 | 65.09 | 0.51 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 30% | $&#124;W\nabla\mathcal{L}(W)&#124;$ | 42.94 | 0.75 | 65.09 | 0.51 |'
- en: '| w/ tune | $&#124;W\nabla\mathcal{L}(W)X&#124;$ | 42.19 | 64.58 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| w/ tune | $&#124;W\nabla\mathcal{L}(W)X&#124;$ | 42.19 | 64.58 |'
- en: 'Table 9: Zero-shot perplexity of the pruned BLOOM-7B and OPT-6.7B models achieved
    by using backpropagation gradient-based pruning criterion with/without activations.
    The columns have the same meaning as Table [6](#S4.T6 "Table 6 ‣ 4.3 Ablation
    Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large
    Language Models").'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 使用带/不带激活的反向传播梯度基础修剪准则对剪枝后的BLOOM-7B和OPT-6.7B模型进行的零-shot困惑度评估。各列的含义与表 [6](#S4.T6
    "Table 6 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models")相同。'
- en: 'In Table [9](#S4.T9 "Table 9 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM:
    Memory-Efficient Structured Pruning for Large Language Models"), gradients are
    computed by backpropagation. In contrast, the results presented in Table [10](#S4.T10
    "Table 10 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models") and [11](#S4.T11 "Table 11 ‣ 4.3 Ablation
    Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large
    Language Models") highlight the effectiveness of incorporating activations in
    estimated gradient-based pruning criteria when fine-tuning is applied or not.
    For instance, when fine-tuning is not applied, the model pruned with 30% of BLOOM-7B’s
    parameters using $|W\hat{\nabla}\mathcal{L}(W)X|$. In contrast, after undergoing
    fine-tuning, although the increase in performance becomes less pronounced as shown
    in Table [11](#S4.T11 "Table 11 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM:
    Memory-Efficient Structured Pruning for Large Language Models"), it is still evident
    that activations play a significant role in enhancing performance.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[9](#S4.T9 "Table 9 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models")中，梯度通过反向传播计算。相比之下，表[10](#S4.T10
    "Table 10 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models")和[11](#S4.T11 "Table 11 ‣ 4.3 Ablation Study
    ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language
    Models")展示了在应用或不应用调优时，加入激活的估计梯度剪枝标准的有效性。例如，当不进行调优时，使用$|W\hat{\nabla}\mathcal{L}(W)X|$剪枝30%的BLOOM-7B参数。相比之下，经过调优后，尽管性能提升的幅度如表[11](#S4.T11
    "Table 11 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models")所示变得不那么明显，但仍然可以看出，激活在提升性能方面发挥了重要作用。'
- en: '| Prune Ratio | Model/Criterion | WikiText2$\downarrow$ |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝比例 | 模型/标准 | WikiText2$\downarrow$ |'
- en: '| 0% | BLOOM-7B | 26.58 | - | 50.55 | - |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 0% | BLOOM-7B | 26.58 | - | 50.55 | - |'
- en: '| 10% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 78.21 | 4.74 | 231.67 |
    27.22 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 10% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 78.21 | 4.74 | 231.67 |
    27.22 |'
- en: '| w/o tune | $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 73.47 | 204.45 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 无调优 | $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 73.47 | 204.45 |'
- en: '| 30% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 106.07 | 14.64 | 239.96.52
    | 31.48 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 30% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 106.07 | 14.64 | 239.96.52
    | 31.48 |'
- en: '| w/o tune | $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 91.43 | 208.48 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 无调优 | $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 91.43 | 208.48 |'
- en: 'Table 10: Zero-shot perplexity of the pruned BLOOM-7B achieved by using ZO
    gradient-based pruning criterion with/without activations. The columns have the
    same meaning as Table [6](#S4.T6 "Table 6 ‣ 4.3 Ablation Study ‣ 4 Experiments
    ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models").'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '表10：使用ZO梯度剪枝标准的剪枝BLOOM-7B的零-shot困惑度，包含/不包含激活。各列含义与表[6](#S4.T6 "Table 6 ‣ 4.3
    Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning
    for Large Language Models")相同。'
- en: '| Prune Ratio | Model/Criterion | WikiText2$\downarrow$ |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝比例 | 模型/标准 | WikiText2$\downarrow$ |'
- en: '| 0% | OPT-6.7B | 26.45 | - | 32.03 | - |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 0% | OPT-6.7B | 26.45 | - | 32.03 | - |'
- en: '| 10% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 30.51 | 0.36 | 39.17 |
    0.53 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 10% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 30.51 | 0.36 | 39.17 |
    0.53 |'
- en: '| w/ tune | $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 30.15 | 38.64 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 有调优 | $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 30.15 | 38.64 |'
- en: '| 30% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 43.87 | 2.98 | 58.12 |
    0.68 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 30% | $&#124;W\hat{\nabla}\mathcal{L}(W)&#124;$ | 43.87 | 2.98 | 58.12 |
    0.68 |'
- en: '| w/ tune | $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 40.89 | 57.44 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 有调优 | $&#124;W\hat{\nabla}\mathcal{L}(W)X&#124;$ | 40.89 | 57.44 |'
- en: 'Table 11: Zero-shot perplexity of the pruned OPT-6.7B achieved by using ZO
    gradient-based pruning criterion with/without activations. The columns have the
    same meaning as Table [6](#S4.T6 "Table 6 ‣ 4.3 Ablation Study ‣ 4 Experiments
    ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models").'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '表11：使用ZO梯度剪枝标准的剪枝OPT-6.7B的零-shot困惑度，包含/不包含激活。各列含义与表[6](#S4.T6 "Table 6 ‣ 4.3
    Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning
    for Large Language Models")相同。'
- en: 'Layer Sensitivity for Pruning Based on the findings in [Ma et al. ([2023](#bib.bib30))]
    that the first and last layers significantly affect the model’s performance, we
    investigate the impact of involving different ranges of layers in the pruning
    process on LLaMA-7B’s performance. It includes analyzing the performance of models
    with pruning applied from the 1st to the 30th layer (represented by layer-1-30
    in Figure [5](#S4.F5 "Figure 5 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM:
    Memory-Efficient Structured Pruning for Large Language Models")), from the 3rd
    to the 30th layer (layer-3-30), from the 4th to the 29th layer (layer-4-29), and
    from the 5th to the 28th layer (layer-5-28) ⁷⁷7LLaMA-7B has 32 layers, from 0th
    to the 31st layer.. From the results in Figure [5](#S4.F5 "Figure 5 ‣ 4.3 Ablation
    Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large
    Language Models"), it is evident that layer-1-30 has the worst performance, while
    layer-3-30 and layer-4-29 have comparably better performance for both pruning
    methods. In contrast, the models derived from layer-5-28 pruning exhibit varying
    responses to different pruning methods. For instance, for MINI-LLM, their performance
    is similar to that of the layer-4-29 models, whereas, for LLM-Pruner [Ma et al.
    ([2023](#bib.bib30))], their performance is somewhat inferior compared to both
    layer-4-29 and layer-3-30 models. Since the layer-4-29 pruning demonstrates consistent
    performance across various pruning methods, we conduct pruning for layer-4-29
    in all LLaMA-7B experiments.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '基于[Ma et al. ([2023](#bib.bib30))]的研究发现，首层和末层对模型性能有显著影响，我们调查了在修剪过程中涉及不同层次范围对
    LLaMA-7B 性能的影响。这包括分析从第 1 层到第 30 层（图 [5](#S4.F5 "图 5 ‣ 4.3 消融研究 ‣ 4 实验 ‣ MINI-LLM:
    大型语言模型的记忆高效结构化修剪")）应用修剪的模型，从第 3 层到第 30 层（layer-3-30），从第 4 层到第 29 层（layer-4-29），以及从第
    5 层到第 28 层（layer-5-28）。LLaMA-7B 共有 32 层，从第 0 层到第 31 层。从图 [5](#S4.F5 "图 5 ‣ 4.3
    消融研究 ‣ 4 实验 ‣ MINI-LLM: 大型语言模型的记忆高效结构化修剪") 中的结果可以看出，layer-1-30 的表现最差，而 layer-3-30
    和 layer-4-29 对于两种修剪方法的表现相对较好。相比之下，来自 layer-5-28 修剪的模型对不同的修剪方法有不同的响应。例如，对于 MINI-LLM，其性能与
    layer-4-29 模型类似，而对于 LLM-Pruner [Ma et al. ([2023](#bib.bib30))]，其性能相对较差，与 layer-4-29
    和 layer-3-30 模型相比有所逊色。由于 layer-4-29 修剪在各种修剪方法中表现一致，我们在所有 LLaMA-7B 实验中选择了 layer-4-29
    进行修剪。'
- en: '![Refer to caption](img/1587c20104f58297cfa9d5a7dd8f94ef.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1587c20104f58297cfa9d5a7dd8f94ef.png)'
- en: (a) MINI-LLM (ours)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MINI-LLM（我们的）
- en: '![Refer to caption](img/e00dec4ed27fbbed87943485dfb7b05d.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e00dec4ed27fbbed87943485dfb7b05d.png)'
- en: (b) LLM-Pruner
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LLM-Pruner
- en: 'Figure 5: The zero-shot perplexity of the pruned models achieved by enabling
    different ranges of layers involved in pruning LLaMA-7B on the PTB dataset. Layer-1-30
    has the worst performance, while layer-3-30 and layer-4-29 have comparably better
    performance for both pruning methods. In contrast, the models derived from layer-5-28
    pruning exhibit varying responses to different pruning methods.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在 PTB 数据集上对 LLaMA-7B 进行不同层次修剪的零-shot 困惑度。Layer-1-30 的表现最差，而 layer-3-30 和
    layer-4-29 对于两种修剪方法的表现则相对较好。相比之下，来自 layer-5-28 修剪的模型对不同的修剪方法表现出不同的响应。
- en: 'Considering the inferior performance of layer-1-31, in Figure [6](#S4.F6 "Figure
    6 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models"), we only display the proportions of layers
    involved in pruning for layer-3-30, layer-4-29, and layer-5-28\. In addition,
    we present the average perplexity of LLM-Pruner and MINI-LLM for the three ranges
    of layers, marked with the orange or the blue five-pointed stars in Figure [6](#S4.F6
    "Figure 6 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models"), which is averaged over three pruning ratios:
    30%, 40%, and 50 % on WikiText2 and PTB. The average perplexity results in Figure [6](#S4.F6
    "Figure 6 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient Structured
    Pruning for Large Language Models") illustrate that our MINI-LLM exhibits performance
    close to, even surpasses at times, that of LLM-Pruner. For instance, for layer-5-28,
    MINI-LLM achieves an average perplexity of 51.87 on PTB, outperforming LLM-Pruner’s
    53.73\. These results demonstrate that MINI-LLM is a memory-efficient and effective
    method for gradient-based pruning.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑到层-1-31 的表现较差，在图 [6](#S4.F6 "图 6 ‣ 4.3 消融研究 ‣ 4 实验 ‣ MINI-LLM: 适用于大型语言模型的内存高效结构化修剪")
    中，我们仅展示了层-3-30、层-4-29 和层-5-28 的修剪比例。此外，我们展示了 LLM-Pruner 和 MINI-LLM 在这三个层范围的平均困惑度，这些数据在图
    [6](#S4.F6 "图 6 ‣ 4.3 消融研究 ‣ 4 实验 ‣ MINI-LLM: 适用于大型语言模型的内存高效结构化修剪") 中用橙色或蓝色五角星标记，数据平均覆盖了三个修剪比例：30%、40%
    和 50%，分别针对 WikiText2 和 PTB。图 [6](#S4.F6 "图 6 ‣ 4.3 消融研究 ‣ 4 实验 ‣ MINI-LLM: 适用于大型语言模型的内存高效结构化修剪")
    中的平均困惑度结果表明，MINI-LLM 的表现接近甚至在某些情况下超越了 LLM-Pruner。例如，在层-5-28 上，MINI-LLM 在 PTB 上的平均困惑度为
    51.87，优于 LLM-Pruner 的 53.73。这些结果表明 MINI-LLM 是一种内存高效且有效的基于梯度的修剪方法。'
- en: '![Refer to caption](img/e1f8f85bdcd731c16fb1465bf396b30d.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e1f8f85bdcd731c16fb1465bf396b30d.png)'
- en: (a) WikiText2
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: (a) WikiText2
- en: '![Refer to caption](img/a0f23475494f95b4bc2d976e824617ec.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a0f23475494f95b4bc2d976e824617ec.png)'
- en: (b) PTB
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: (b) PTB
- en: 'Figure 6: The percentage of different ranges of layers involved in pruning
    LLaMA-7B and the average perplexity. MINI-LLM exhibits performance close to, even
    surpasses LLM-Pruner.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：涉及修剪 LLaMA-7B 的不同层范围的百分比以及平均困惑度。MINI-LLM 的性能接近甚至超越了 LLM-Pruner。
- en: 4.4 Generations From Pruned Model
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 从修剪模型生成的内容
- en: 'Table [12](#S4.T12 "Table 12 ‣ 4.4 Generations From Pruned Model ‣ 4 Experiments
    ‣ MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models") shows
    the generation examples of the original and the pruned LLaMA-7B models achieved
    by MINI-LLM. The five experimental instructions encompass math, common sense,
    translation, and writing tasks. From the responses presented in Table [12](#S4.T12
    "Table 12 ‣ 4.4 Generations From Pruned Model ‣ 4 Experiments ‣ MINI-LLM: Memory-Efficient
    Structured Pruning for Large Language Models"), it is evident that when pruning
    20% of the parameters, the pruned model maintains high performance in these tasks.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [12](#S4.T12 "表 12 ‣ 4.4 从修剪模型生成的内容 ‣ 4 实验 ‣ MINI-LLM: 适用于大型语言模型的内存高效结构化修剪")
    显示了 MINI-LLM 实现的原始和修剪后的 LLaMA-7B 模型的生成示例。这五个实验指令涵盖了数学、常识、翻译和写作任务。从表 [12](#S4.T12
    "表 12 ‣ 4.4 从修剪模型生成的内容 ‣ 4 实验 ‣ MINI-LLM: 适用于大型语言模型的内存高效结构化修剪") 中展示的响应可以看出，当修剪
    20% 的参数时，修剪后的模型在这些任务中仍能保持高性能。'
- en: '| Model | Generated Sentences |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 生成的句子 |'
- en: '| --- | --- |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| LLaMA-7B (ratio=0%) | Instruction: What are Fibonacci numbers? Response:
    Fibonacci numbers is a sequence of integer numbers, generated by a simple formula
    which has the form of sum of two numbers. This sequence has a few very interesting
    properties which can be summarized as follows: Every Fibonacci number is the sum
    of two preceding Fibonacci numbers. The formula is Fn= Fn-1 + Fn-2\. Start with
    1,2,3,5,8,13,21,34,55,89,… |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B (比率=0%) | 指令：什么是斐波那契数？响应：斐波那契数是一个整数序列，由一个简单的公式生成，该公式的形式是两个数字之和。这个序列有一些非常有趣的性质，可以总结如下：每个斐波那契数都是两个前面的斐波那契数之和。公式是
    Fn= Fn-1 + Fn-2。以 1,2,3,5,8,13,21,34,55,89,… 开始 |'
- en: '| Instruction: What is the difference between a cat and a tiger? Response:
    A tiger is a member of the cat family, but is a big, powerful animal with striped
    fur. It is bigger than a normal housecat. The tiger is native to India and southwestern
    Asia. In contrast, a cat is smaller than the tiger. There are three main types
    of cats. Most cats are of the domestic. |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 指令：猫和老虎有什么区别？回复：老虎是猫科家族的一员，但它是一种体型大、力量强大的动物，拥有条纹毛发。它比普通的家猫要大。老虎原产于印度和南亚。而猫则比老虎小。有三种主要的猫。大多数猫是家猫。|'
- en: '| Instruction: AI can create a logo in seconds Response: When you think about
    a logo, the first thought may be to sit down and draw something, or even create
    a vector file. But the truth is that it is possible to do this easily and quickly
    by using AI. So, the question is, why not use it? For example, I was recently
    working with the logo of an Italian company and I only needed to sit down at the
    computer for 5 minutes to create the basic elements that I would then change to
    meet the needs of the brand. In just 2 minutes you can create a 3D logo. |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 指令：AI可以在几秒钟内创建一个logo。回复：当你想到一个logo时，第一个念头可能是坐下来画点东西，甚至创建一个矢量文件。但事实是，使用AI可以轻松而快速地完成这一任务。那么问题是，为什么不使用它呢？例如，我最近在处理一家意大利公司的logo时，只需要坐在电脑前5分钟，就创建了基本元素，然后根据品牌需求进行修改。仅用2分钟你就可以创建一个3D
    logo。|'
- en: '| Instruction: Compose a poem about summer. Response: Sandy toes, Cooling ocean,
    Water lilies, the warm sun, Making the clouds, dissolve, and the birds sing. Their
    songs lull me to sleep, With soft breeze. Summer is, the best, because there is
    no one to bother, in the summer days. |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 指令：写一首关于夏天的诗。回复：沙滩上的脚趾，凉爽的海洋，水 lilies，温暖的太阳，融化云朵，鸟儿歌唱。它们的歌声使我入睡，伴着轻柔的微风。夏天是最好的，因为在夏天的日子里没有人打扰。|'
- en: '| Instruction: Translate the English sentence ”large language models” into
    Spanish. Response: The answer for the English sentence is ”grandes modelos lenguas”
    in Spanish. |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 指令：将英文句子 ”large language models” 翻译成西班牙语。回复：英文句子的答案是西班牙语中的”grandes modelos
    lenguas”。|'
- en: '| LLaMA-7B (ratio=20%) | Instruction: What are Fibonacci numbers? Response:
    In mathematics and physics, the Fibonacci numbers are the sequence of integers
    starting from the 2nd number as the sum of the previous two numbers in the sequence:
    1, 1, 2, 3, 5, 8, 13, 21, 34, 55,… |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B (比例=20%) | 指令：什么是斐波那契数？回复：在数学和物理学中，斐波那契数列是从第二个数字开始的整数序列，每个数字是前两个数字的和：1，1，2，3，5，8，13，21，34，55，…|'
- en: '| Instruction: What is the difference between a cat and a tiger? Response:
    The cat and the tiger are both Felines, which means both species belong to the
    same Biological Taxonomic Classification family. However, the difference between
    a cat and a tiger lies primarily in the physical differences between the two species.
    A cat is generally smaller in size, with an average weight of 5 - 10 pounds (2.7
    - 4.4 kilograms). Cats are more domesticated, often being kept as pets. They are
    known for being more social, and their distinctive meowing, hisses, and growls
    are used to communicate with other felines. On the other hand, tigers are bigger
    in size, with an average weight of around 125 pounds (69.5 kilograms). They are
    also more wild, with little domestication. |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 指令：猫和老虎有什么区别？回复：猫和老虎都是猫科动物，这意味着这两种动物属于同一个生物分类学家族。然而，猫和老虎之间的区别主要在于两者之间的物理差异。猫的体型一般较小，平均体重为5
    - 10磅（2.7 - 4.4千克）。猫更为驯化，通常作为宠物饲养。它们以更为社交著称，猫特有的喵喵叫、嘶嘶声和咆哮声用于与其他猫科动物交流。而老虎体型较大，平均体重大约为125磅（69.5千克）。它们也更为野生，几乎没有驯化。|'
- en: '| Instruction: AI can create a logo in seconds Response: AI can create a logo
    in seconds, a visual designer can bring your brand idea to life. When it comes
    to creating a brand identity, our first step is to conduct a thorough market analysis
    and develop a brand position. From there we can create a creative brand identity
    that is in line with the core values of your business. Our creative design team
    is trained in building brands that are aesthetically pleasing, easy to recognize
    and communicate a clear message to your target audience. The design can incorporate
    many elements including illustrations, photography, infographics, videos, typography
    and layout. |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 指令：AI 可以在几秒钟内创建一个标志。 响应：AI 可以在几秒钟内创建一个标志，而视觉设计师则可以将你的品牌理念付诸实践。 在创建品牌身份时，我们的第一步是进行彻底的市场分析并制定品牌定位。
    从那里，我们可以创建一个符合你业务核心价值的创意品牌身份。 我们的创意设计团队经过培训，能够建立既美观又易于识别的品牌，并向目标受众传达清晰的信息。 设计可以包含许多元素，包括插图、摄影、信息图表、视频、排版和布局。
    |'
- en: '| Instruction: Compose a poem about summer. Response: A summer sun is all aglow,
    Reflecting in the calm, clear waters below, Tossed by gentle breeze, Warm and
    golden, Far beyond sight. A perfect sight for sight, In an endless horizon, Reflecting
    in the calm, Clear waters below. A summer sun, That never sets, Far beyond sight.
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 指令：写一首关于夏天的诗。 响应：夏日的太阳熠熠生辉， 在宁静清澈的水面下反射， 被温柔的微风吹拂， 温暖而金黄， 遥不可及。 完美的景象， 在无尽的地平线上，
    在宁静的水面下反射。 夏日的太阳， 永不落下， 遥不可及。 |'
- en: '| Instruction: Translate the English sentence ”large language models” into
    Spanish. Response: Here’s a suggested sentence in Spanish: ”Modelo grande de lenguas.”
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 指令：将英文句子”large language models”翻译成西班牙语。 响应：这是一个西班牙语的建议句子：“Modelo grande de
    lenguas。” |'
- en: 'Table 12: Generated Examples from the original and pruned LLaMA-7B.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：来自原始和剪枝后的 LLaMA-7B 的生成示例。
- en: 5 Conclusion
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: 'In this paper, we presented MINI-LLM, an one-shot structured pruning approach
    designed to address the high GPU memory demands of computing backpropagation-based
    gradients of pre-trained LLMs. First, we proposed a novel criterion called the
    Feature Map Sensitivity (FMS) score which integrates magnitude, activation, and
    gradient information to guide the pruning process effectively. By employing estimated
    gradients based on forward passes, MINI-LLM not only reduces the GPU memory requirement
    for gradient-guided pruning but also achieves superior performance compared to
    existing gradient-free methods. Our extensive experiments on three LLMs: LLaMA,
    BLOOM, and OPT, across various downstream tasks demonstrate MINI-LLM’s effectiveness
    and efficiency in GPU memory usage. In the future, our objective is to further
    enhance the pruning results of MINI-LLM at higher compression rates.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了MINI-LLM，这是一种一次性结构化剪枝方法，旨在解决计算基于反向传播的预训练LLM梯度的高GPU内存需求。 首先，我们提出了一种新颖的标准，称为特征图敏感度（FMS）得分，该得分整合了幅度、激活和梯度信息，以有效指导剪枝过程。
    通过采用基于前向传递的估计梯度，MINI-LLM不仅减少了梯度引导剪枝所需的GPU内存，而且在性能上优于现有的无梯度方法。 我们在三个LLM：LLaMA、BLOOM和OPT上的广泛实验，涵盖了各种下游任务，展示了MINI-LLM在GPU内存使用上的有效性和效率。
    未来，我们的目标是进一步提高MINI-LLM在更高压缩率下的剪枝结果。
- en: References
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao,
    and Yejin Choi. PIQA: Reasoning about physical commonsense in natural language.
    In AAAI, 2020.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao
    和 Yejin Choi. PIQA: 关于自然语言中的物理常识的推理。 见AAAI，2020。'
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, et al. Language models
    are few-shot learners. Advances in neural information processing systems, 33:1877–1901,
    2020.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder 等。语言模型是少量样本学习者。 神经信息处理系统进展，33:1877–1901，2020。
- en: 'Chavan et al. [2023] Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, and
    Zhiqiang Shen. One-for-All: Generalized lora for parameter-efficient fine-tuning.
    arXiv preprint arXiv:2306.07967, 2023.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chavan et al. [2023] Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing 和 Zhiqiang
    Shen. One-for-All: 通用的lora用于参数高效微调。 arXiv 预印本 arXiv:2306.07967，2023。'
- en: Cheng et al. [2023] Hongrong Cheng, Miao Zhang, and Javen Qinfeng Shi. A survey
    on deep neural network pruning-taxonomy, comparison, analysis, and recommendations.
    arXiv preprint arXiv:2308.06767, 2023.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. [2023] Hongrong Cheng, Miao Zhang 和 Javen Qinfeng Shi. 深度神经网络剪枝的调查——分类、比较、分析和建议。
    arXiv 预印本 arXiv:2308.06767，2023。
- en: 'Clark et al. [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty
    of natural yes/no questions. In NAACL, 2019.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人 [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, 和 Kristina Toutanova. BoolQ：探索自然是/否问题的惊人难度。在 NAACL，2019年。
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    LLM.int8(): 8-bit matrix multiplication for transformers at scale. In NeurIPS,
    2022.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等人 [2022] Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    LLM.int8()：大规模变换器的 8 位矩阵乘法。在 NeurIPS，2022年。
- en: 'Dettmers et al. [2023] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. arXiv preprint arXiv:2305.14314,
    2023.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等人 [2023] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer.
    QLoRA：量化 LLM 的高效微调。arXiv 预印本 arXiv:2305.14314，2023年。
- en: 'Frantar and Alistarh [2023] Elias Frantar and Dan Alistarh. SparseGPT: Massive
    language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774,
    2023.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 和 Alistarh [2023] Elias Frantar 和 Dan Alistarh. SparseGPT：大规模语言模型可以一次性准确剪枝。arXiv
    预印本 arXiv:2301.00774，2023年。
- en: 'Frantar et al. [2022] Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal
    brain compression: A framework for accurate post-training quantization and pruning.
    In NeurIPS, 2022.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等人 [2022] Elias Frantar, Sidak Pal Singh, 和 Dan Alistarh. 最优大脑压缩：用于准确后训练量化和剪枝的框架。发表于
    NeurIPS，2022年。
- en: 'Fu et al. [2022] Yonggan Fu, Haichuan Yang, Jiayi Yuan, Meng Li, Cheng Wan,
    Raghuraman Krishnamoorthi, Vikas Chandra, and Yingyan Lin. Depthshrinker: a new
    compression paradigm towards boosting real-hardware efficiency of compact neural
    networks. In ICML, 2022.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等人 [2022] Yonggan Fu, Haichuan Yang, Jiayi Yuan, Meng Li, Cheng Wan, Raghuraman
    Krishnamoorthi, Vikas Chandra, 和 Yingyan Lin. Depthshrinker：一种新的压缩范式，旨在提升紧凑神经网络的实际硬件效率。在
    ICML，2022年。
- en: Gasnikov et al. [2022] Alexander Gasnikov, Darina Dvinskikh, Pavel Dvurechensky,
    Eduard Gorbunov, Aleksander Beznosikov, and Alexander Lobanovu. Randomized gradient-free
    methods in convex optimization. arXiv preprint arXiv:2211.13566, 2022.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gasnikov 等人 [2022] Alexander Gasnikov, Darina Dvinskikh, Pavel Dvurechensky,
    Eduard Gorbunov, Aleksander Beznosikov, 和 Alexander Lobanovu. 凸优化中的随机梯度无关方法。arXiv
    预印本 arXiv:2211.13566，2022年。
- en: He et al. [2023] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang.
    Sensitivity-aware visual parameter-efficient tuning. In ICCV, 2023.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2023] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, 和 Bohan Zhuang.
    关注灵敏度的视觉参数高效调优。在 ICCV，2023年。
- en: Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. In International Conference on Learning Representations (ICLR),
    2021.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人 [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song, 和 Jacob Steinhardt. 测量大规模多任务语言理解。在国际学习表征会议（ICLR），2021年。
- en: 'Hu et al. [2022] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language
    models. In ICLR poster, 2022.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2022] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, 和 Weizhu Chen. LoRA：大规模语言模型的低秩适应。在 ICLR 海报，2022年。
- en: Jia et al. [2022] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge
    Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, 2022.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等人 [2022] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge
    Belongie, Bharath Hariharan, 和 Ser-Nam Lim. 视觉提示调优。在 ECCV，2022年。
- en: Kiefer and Wolfowitz. [1952] J. Kiefer and J. Wolfowitz. Stochastic estimation
    of the maximum of a regression function. Ann. Math. Statist, 23:462–466, 1952.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiefer 和 Wolfowitz [1952] J. Kiefer 和 J. Wolfowitz. 回归函数最大值的随机估计。Ann. Math.
    Statist，23:462–466，1952年。
- en: 'Kingma and Ba [2015] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. In ICLR, 2015.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Ba [2015] Diederik P. Kingma 和 Jimmy Ba. Adam：一种随机优化方法。在 ICLR，2015年。
- en: 'Kovaleva et al. [2021] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and
    Anna Rumshisky. BERT Busters: Outlier dimensions that disrupt transformers. In
    ACL, 2021.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kovaleva 等人 [2021] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, 和 Anna
    Rumshisky. BERT 破坏者：扰乱变换器的离群维度。在 ACL，2021年。
- en: 'Kurtic et al. [2022] Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar,
    Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal BERT
    surgeon: Scalable and accurate second-order pruning for large language models.
    In EMNLP, 2022.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurtic 等人 [2022] Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark
    Kurtz, Benjamin Fineran, Michael Goin, 和 Dan Alistarh. 最佳 BERT 外科医生：大规模语言模型的可扩展和准确的二阶剪枝。在
    EMNLP，2022年。
- en: 'Kurtic et al. [2023] Eldar Kurtic, Elias Frantar, and Dan Alistarh. ZipLM:
    Inference-aware structured pruning of language models. In NeurIPS, 2023.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurtic et al. [2023] Eldar Kurtic, Elias Frantar, 和 Dan Alistarh. ZipLM：面向推理的结构化语言模型修剪。见
    NeurIPS，2023年。
- en: Kwon et al. [2022] Woosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun,
    Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers.
    In NeurIPS, 2022.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon et al. [2022] Woosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun,
    Kurt Keutzer, 和 Amir Gholami. 针对变换器的快速后训练修剪框架。见 NeurIPS，2022年。
- en: LeCun et al. [1989] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage.
    In NIPS, pages 598–605, 1989.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. [1989] Yann LeCun, John Denker, 和 Sara Solla. 最优脑损伤。见 NIPS，第598–605页，1989年。
- en: 'Lee et al. [2019] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip H. S. Torr.
    SNIP: Single-shot network pruning based on connection sensitivity. In ICLR, 2019.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. [2019] Namhoon Lee, Thalaiyasingam Ajanthan, 和 Philip H. S. Torr.
    SNIP：基于连接敏感性的单次网络修剪。见 ICLR，2019年。
- en: Lester et al. [2021] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning. In EMNLP, 2021.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester et al. [2021] Brian Lester, Rami Al-Rfou, 和 Noah Constant. 参数高效提示调整的规模效应。见
    EMNLP，2021年。
- en: 'Li and Liang [2021] Xiang Lisa Li and Percy Liang. Prefix-Tuning: Optimizing
    continuous prompts for generation. In IJCNLP, 2021.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li and Liang [2021] Xiang Lisa Li 和 Percy Liang. Prefix-Tuning：优化生成的连续提示。见 IJCNLP，2021年。
- en: 'Li et al. [2020] Yawei Li, Shuhang Gu, Christoph Mayer, Luc Van Gool, and Radu
    Timofte. Group sparsity: The hinge between filter pruning and decomposition for
    network compression. In CVPR, 2020.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2020] Yawei Li, Shuhang Gu, Christoph Mayer, Luc Van Gool, 和 Radu
    Timofte. 群体稀疏性：网络压缩中滤波器修剪和分解的桥梁。见 CVPR，2020年。
- en: 'Li et al. [2022a] Shiru Li, Yong Xia, and Zi Xu. Simultaneous perturbation
    stochastic approximation: towards one-measurement per iteration. arXiv preprint
    arXiv:2203.03075, 2022.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2022a] Shiru Li, Yong Xia, 和 Zi Xu. 同时扰动随机近似：迈向每次迭代一次测量。arXiv 预印本
    arXiv:2203.03075，2022年。
- en: Li et al. [2022b] Yuchao Li, Fuli Luo, Chuanqi Tan, Mengdi Wang, Songfang Huang,
    Shen Li, and Junjie Bai. Parameter-efficient sparsity for large language models
    fine-tuning. In IJCAI, 2022.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2022b] Yuchao Li, Fuli Luo, Chuanqi Tan, Mengdi Wang, Songfang Huang,
    Shen Li, 和 Junjie Bai. 大型语言模型微调的参数高效稀疏性。见 IJCAI，2022年。
- en: Liu et al. [2021] Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jing-Hao
    Xue, Xinjiang Wang, Yimin Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang. Group
    fisher pruning for practical network compression. In ICML, 2021.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2021] Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jing-Hao
    Xue, Xinjiang Wang, Yimin Chen, Wenming Yang, Qingmin Liao, 和 Wayne Zhang. 实用网络压缩的群体
    Fisher 修剪。见 ICML，2021年。
- en: 'Ma et al. [2023] Xinyin Ma, Gongfan Fang, and Xinchao Wang. LLM-Pruner: On
    the structural pruning of large language models. In NeurIPS, 2023.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. [2023] Xinyin Ma, Gongfan Fang, 和 Xinchao Wang. LLM-Pruner：大型语言模型的结构化修剪。见
    NeurIPS，2023年。
- en: Malladi et al. [2023] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian,
    Jason D. Lee, Danqi Chen, and Sanjeev Arora. Fine-tuning language models with
    just forward passes. In NeurIPS, 2023.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malladi et al. [2023] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian,
    Jason D. Lee, Danqi Chen, 和 Sanjeev Arora. 仅通过前向传递微调语言模型。见 NeurIPS，2023年。
- en: 'Marcus et al. [1993] Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
    Building a large annotated corpus of English: The penn treebank. Computational
    Linguistics, 19:313–330, 1993.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marcus et al. [1993] Mitchell Marcus, Beatrice Santorini, 和 Mary Ann Marcinkiewicz.
    构建一个大型标注的英语语料库：宾夕法尼亚树库。计算语言学，19:313–330，1993年。
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher. 指针哨兵混合模型。arXiv 预印本 arXiv:1609.07843，2016年。
- en: Molchanov et al. [2017] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
    and Jan Kautz. Pruning convolutional neural networks for resource-efficient inference.
    In ICLR, 2017.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molchanov et al. [2017] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
    和 Jan Kautz. 为资源高效推理修剪卷积神经网络。见 ICLR，2017年。
- en: Molchanov et al. [2019] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio,
    and Jan Kautz. Importance estimation for neural network pruning. In CVPR, 2019.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molchanov et al. [2019] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio,
    和 Jan Kautz. 神经网络修剪的重要性估计。见 CVPR，2019年。
- en: Nova et al. [2023] Azade Nova, Hanjun Dai, and Dale Schuurmans. Gradient-free
    structured pruning with unlabeled data. In ICML, 2023.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nova et al. [2023] Azade Nova, Hanjun Dai, 和 Dale Schuurmans. 无标签数据的无梯度结构化修剪。见
    ICML，2023年。
- en: OpenAI [2023] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774,
    2023.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI. GPT-4 技术报告。arXiv 预印本 arXiv:2303.08774，2023年。
- en: Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. WinoGrande:an adversarial winograd schema challenge at scale.
    Communications of the ACM, 64:99–106, 2021.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi 等人 [2021] 坂口圭介、Ronan Le Bras、Chandra Bhagavatula 和 Yejin Choi。WinoGrande：规模化的对抗性
    Winograd 语法挑战。发表在 ACM 通讯，64:99–106，2021 年。
- en: 'Sanh et al. [2020] Victor Sanh, Thomas Wolf, and Alexander M. Rush. Movement
    pruning: Adaptive sparsity by fine-tuning. In NeurIPS, 2020.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等人 [2020] Victor Sanh、Thomas Wolf 和 Alexander M. Rush。运动修剪：通过微调实现自适应稀疏性。发表在
    NeurIPS，2020 年。
- en: Spall [1992] James C. Spall. Multivariate stochastic approximation using a simultaneous
    perturbation gradient approximation. IEEE Transactions on Automatic Control, 37:332–341,
    1992.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spall [1992] 詹姆斯 C. Spall。使用同步扰动梯度近似的多变量随机逼近。发表在 IEEE 自动控制学报，37:332–341，1992
    年。
- en: Spall [1997] James C. Spall. A one-measurement form of simultaneous perturbation
    stochastic approximation. Automatics, 33:109–112, 1997.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spall [1997] 詹姆斯 C. Spall。一种同步扰动随机逼近的一次测量形式。发表在自动化学报，33:109–112，1997 年。
- en: Sun et al. [2024] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A
    simple and effective pruning approach for large language models. In Proceedings
    of International Conference on Learning Representations (ICLR) poster, 2024.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2024] 孙名杰、刘庄、Anna Bair 和 J. Zico Kolter。一种简单有效的大型语言模型修剪方法。发表在国际学习表征会议（ICLR）海报，2024
    年。
- en: 'Taori et al. [2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori 等人 [2023] 罗汉 Taori、易山 Gulrajani、张天亿、Yann Dubois、李学辰、Carlos Guestrin、Percy
    Liang 和 Tatsunori B. Hashimoto。斯坦福 alpaca：一个指令跟随的 llama 模型。[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)，2023
    年。
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al.
    LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971,
    2023.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 [2023] Hugo Touvron、Thibaut Lavril、Gautier Izacard 等。LLaMA：开放且高效的基础语言模型。arXiv
    预印本 arXiv:2302.13971，2023 年。
- en: Wang et al. [2020a] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning
    tickets before training by preserving gradient flow. In ICLR, 2020.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2020a] 王超奇、张国栋 和 Roger Grosse。通过保留梯度流在训练前挑选赢票。发表在 ICLR，2020 年。
- en: Wang et al. [2020b] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning
    of large language models. In EMNLP, 2020.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2020b] 王自恒、Jeremy Wohlwend 和 Tao Lei。大型语言模型的结构化修剪。发表在 EMNLP，2020 年。
- en: 'Workshop [2023] BigScience Workshop. BLOOM: A 176b-parameter open-access multilingual
    language model. arXiv preprint arXiv:2211.05100, 2023.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Workshop [2023] BigScience Workshop。BLOOM：一个 176b 参数的开放访问多语言模型。arXiv 预印本 arXiv:2211.05100，2023
    年。
- en: Wu et al. [2020] Yiquan Wu, Kun Kuang, Yating Zhang, Xiaozhong Liu, Changlong
    Sun, Jun Xiao, Yueting Zhuang, Luo Si, and Fei Wu. De-biased court’s view generation
    with causality. In EMNLP, pages 763–780, 2020.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2020] 余权吴、阮坤、张亚婷、刘晓中、孙长龙、肖军、庄跃廷、罗思、吴飞。基于因果关系的去偏法院视图生成。发表在 EMNLP，页码 763–780，2020
    年。
- en: Wu et al. [2023] Likang Wu, Zhi Zheng, Zhaopeng Qiu, et al. A survey on large
    language models for recommendation. arXiv preprint arXiv:2305.19860, 2023.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2023] 吴立康、郑志 和 邱兆鹏 等。关于推荐系统的大型语言模型的调查。arXiv 预印本 arXiv:2305.19860，2023
    年。
- en: 'Xia et al. [2024] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared
    LLaMA: Accelerating language model pre-training via structured pruning. In ICLR,
    2024.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia 等人 [2024] 夏孟洲、高天宇、曾志远 和 陈丹琪。Sheared LLaMA：通过结构化修剪加速语言模型的预训练。发表在 ICLR，2024
    年。
- en: 'Xiao et al. [2023] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. SmoothQuant: Accurate and efficient post-training quantization for
    large language models. In ICML, 2023.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人 [2023] 肖光轩、Ji Lin、Mickael Seznec、吴浩、Julien Demouth 和 Song Han。SmoothQuant：大型语言模型的准确高效后训练量化。发表在
    ICML，2023 年。
- en: 'Yu et al. [2022] Xin Yu, Thiago Serra, Srikumar Ramalingam, and Shandian Zhe.
    The combinatorial brain surgeon: Pruning weights that cancel one another in neural
    networks. In ICML, 2022.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等人 [2022] 辛 Yu、Thiago Serra、Srikumar Ramalingam 和 Shandian Zhe。组合脑外科医生：修剪神经网络中相互抵消的权重。发表在
    ICML，2022 年。
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In ACL,
    2019.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers 等人 [2019] 罗温 Zellers、Ari Holtzman、Yonatan Bisk、Ali Farhadi 和 Yejin Choi。Hellaswag：机器真的能完成你的句子吗？发表在
    ACL，2019 年。
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, et al. OPT: Open
    pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal 等. OPT：开放预训练变换器语言模型。arXiv
    预印本 arXiv:2205.01068，2022。
- en: 'Zhou et al. [2022] Minxuan Zhou, Weihong Xu, Jaeyoung Kang, and Tajana Rosing.
    TransPIM: A memory-based acceleration via software-hardware co-design for transformer.
    In HPCA, 2022.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. [2022] Minxuan Zhou, Weihong Xu, Jaeyoung Kang 和 Tajana Rosing.
    TransPIM：一种通过软件-硬件协同设计的基于内存的加速方法，用于变换器。发表于 HPCA，2022。
- en: 'Zhu et al. [2015] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov,
    Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies:
    Towards story-like visual explanations by watching movies and reading books. In
    ICCV, 2015.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2015] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel
    Urtasun, Antonio Torralba 和 Sanja Fidler. 对齐书籍和电影：通过观看电影和阅读书籍迈向类似故事的视觉解释。发表于 ICCV，2015。
