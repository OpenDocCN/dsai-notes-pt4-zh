- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:05:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:05:39
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Can Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic Reasoning?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低秩知识蒸馏在LLMs中对微电子推理是否有用？
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.13808](https://ar5iv.labs.arxiv.org/html/2406.13808)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.13808](https://ar5iv.labs.arxiv.org/html/2406.13808)
- en: Nirjhor Rouf^∗ ECE Department,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Nirjhor Rouf^∗ 电气与计算机工程系，
- en: North Carolina State University.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 北卡罗来纳州立大学。
- en: nrouf2@ncsu.edu    Fin Amin^∗ ECE Department,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: nrouf2@ncsu.edu    Fin Amin^∗ 电气与计算机工程系，
- en: North Carolina State University.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 北卡罗来纳州立大学。
- en: samin2@ncsu.edu    Paul D. Franzon ECE Department,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: samin2@ncsu.edu    Paul D. Franzon 电气与计算机工程系，
- en: North Carolina State University.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 北卡罗来纳州立大学。
- en: paulf@ncsu.edu
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: paulf@ncsu.edu
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In this work, we present empirical results regarding the feasibility of using
    offline large language models (LLMs) in the context of electronic design automation
    (EDA). The goal is to investigate and evaluate a contemporary language model’s
    (Llama-2-7B) ability to function as a microelectronic Q&A expert as well as its
    reasoning, and generation capabilities in solving microelectronic-related problems.
    Llama-2-7B was tested across a variety of adaptation methods, including introducing
    a novel low-rank knowledge distillation (LoRA-KD) scheme. Our experiments produce
    both qualitative and quantitative results. Furthermore, we release our evaluation
    benchmark along with the code necessary to replicate our experiments at [github.com/FinAminToastCrunch](https://github.com/FinAminToastCrunch).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们展示了关于在电子设计自动化（EDA）领域中使用离线大型语言模型（LLMs）可行性的实证结果。目标是调查和评估一种现代语言模型（Llama-2-7B）作为微电子问答专家的能力，以及其在解决微电子相关问题中的推理和生成能力。Llama-2-7B通过各种适应方法进行了测试，包括引入一种新型的低秩知识蒸馏（LoRA-KD）方案。我们的实验产生了定性和定量结果。此外，我们还发布了我们的评估基准，并附上了重现我们实验所需的代码，网址为[github.com/FinAminToastCrunch](https://github.com/FinAminToastCrunch)。
- en: 'Index Terms:'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: 'LLMs for EDA education, LLM fine-tuning, knowledge-distillation, RAG, Low-Rank
    adaptation^*^*footnotetext: These authors contributed equally to this work.^*^*footnotetext:
    To appear in IEEE International Workshop on LLM-Aided Design (LAD’24)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs用于EDA教育，LLM微调，知识蒸馏，RAG，低秩适应^*^*脚注：这些作者对本工作做出了相同的贡献。^*^*脚注：将出现在IEEE国际LLM辅助设计研讨会（LAD’24）
- en: I Introduction and Motivation
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言与动机
- en: The emergence of Large Language Models (LLM) has revolutionized the field of
    natural language processing. At present, LLMs are garnering significant research
    interests for domain-specific tasks. In the field of electronic design automation
    (EDA) in particular, applications of LLMs are still at the nascent stage. However,
    it is very apparent that the effective use of LLMs in EDA can improve manufacturing
    yields by streamlining the design flow when it comes to IC design. Recently published
    works showed the successful use of LLMs in chip design [[3](#bib.bib3), [13](#bib.bib13),
    [2](#bib.bib2)]. Additionally, LLMs have also shown significant proficiency in
    the analysis of designed systems [[8](#bib.bib8)] and even in reviewing and analysis
    of design specifications of VLSI systems [[11](#bib.bib11)]. Development of open-source
    benchmarks such as VerilogEval [[14](#bib.bib14)] is also facilitating future
    research in this field. Similarly, LLMs can be useful in enhancing productivity.
    Internal studies carried out at Nvidia have shown that checklist related tasks
    can take up to 60% of an engineer’s time and thus bottleneck productivity [[13](#bib.bib13)].
    An LLM-based engineering assistant can certainly reduce this bottleneck by helping
    with engineering knowledge dissemination.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）的出现已彻底改变了自然语言处理领域。目前，LLMs正引起对特定领域任务的重大研究兴趣。特别是在电子设计自动化（EDA）领域，LLMs的应用仍处于起步阶段。然而，很明显，在EDA中有效使用LLMs可以通过简化IC设计流程来提高制造产量。最近发布的研究显示了LLMs在芯片设计中的成功应用
    [[3](#bib.bib3), [13](#bib.bib13), [2](#bib.bib2)]。此外，LLMs在设计系统的分析[[8](#bib.bib8)]，甚至在VLSI系统设计规范的审查和分析中[[11](#bib.bib11)]也显示出了显著的能力。像VerilogEval
    [[14](#bib.bib14)]这样的开源基准的发展也促进了该领域未来的研究。同样，LLMs在提高生产力方面也很有用。Nvidia内部的研究表明，检查清单相关任务可能占据工程师高达60%的时间，从而成为生产力的瓶颈[[13](#bib.bib13)]。基于LLM的工程助手确实可以通过帮助传播工程知识来减少这一瓶颈。
- en: '![Refer to caption](img/a440d19f8d15f48e613979d5d1afe9fb.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a440d19f8d15f48e613979d5d1afe9fb.png)'
- en: 'Figure 1: LoRA-KD works by first fine-tuning the teacher model using LoRA.
    Afterward, the teacher is frozen and its outputs are used for equation [4](#S3.E4
    "In III-B Knowledge Distillation ‣ III Adaptation Techniques for LLMs ‣ Can Low-Rank
    Knowledge Distillation in LLMs be Useful for Microelectronic Reasoning?"). Note
    that only the low-rank $A$ parameters of the student are updated.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LoRA-KD的工作原理是首先使用LoRA对教师模型进行微调。之后，冻结教师模型，并使用其输出进行方程[4](#S3.E4 "在III-B知识蒸馏
    ‣ III LLM的适应技术 ‣ 低秩知识蒸馏在LLMs中对微电子推理是否有用？")。注意，只有学生模型的低秩$A$参数被更新。
- en: However, several key challenges must be addressed for more effective and efficient
    application of LLMs in EDA. One big concern is the unintentional data retention
    of DNNs from training sets [[9](#bib.bib9)]. There are two aspects of this issue.
    Firstly, classified IP designs can be leaked if the API stores user input. Secondly,
    when trying to complete a user request, the LLM can inadvertently use copy-righted
    IP designs without attributing references to them–potentially causing downstream
    legal trouble. Another major challenge is the heavy computational resource requirement
    of LLMs. For example, Meta’s Llama2-70B requires 130 GB memory to load [[17](#bib.bib17)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了更有效和高效地应用LLMs于EDA，必须解决几个关键挑战。一个主要的问题是DNN从训练集中无意间保留数据[[9](#bib.bib9)]。这个问题有两个方面。首先，如果API存储用户输入，则可能会泄露分类的IP设计。其次，在尝试完成用户请求时，LLM可能无意中使用了受版权保护的IP设计而没有引用参考，这可能导致下游法律问题。另一个主要挑战是LLMs对计算资源的高要求。例如，Meta的Llama2-70B需要130
    GB内存来加载[[17](#bib.bib17)]。
- en: '![Refer to caption](img/b6f42c27dc53b454e870e9c166e471b2.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b6f42c27dc53b454e870e9c166e471b2.png)'
- en: 'Figure 2: These charts show histograms of which configurations were ranked
    in the top half and declared the worst according to third-year microelectronics
    students. Survey participants had to order the outputs of each configuration on
    15 questions. A total of 51 rankings were considered after filtering for quality.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：这些图表显示了哪些配置被排名在前半部分以及哪些被第三年微电子学学生评为最差的直方图。调查参与者需要在15个问题上对每个配置的输出进行排序。经过质量筛选后，共考虑了51个排名。
- en: 'Choosing the appropriate LLM for EDA applications is also a big challenge and
    here, the proprietary vs open-source debate must be addressed. While proprietary
    models, such as ChatGPT-4 [[1](#bib.bib1)] are powerful, they have limited accessibility,
    store user data/designs, and are pay-to-use. Additionally, the inability to fine-tune
    them hinders their capabilities in domain-specific EDA tasks. On the other hand,
    open-source LLMs offering better accessibility are restricted by limited scale
    and resources compared to their proprietary counterparts resulting in lower performance
    [[21](#bib.bib21)]. In this work, we explore the feasibility of adapting the open-source
    Llama-2-7B for use in EDA education. We focus on this model in particular because
    it can be used on consumer hardware. Our contributions are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适合EDA应用的LLM也是一个重大挑战，在这里，必须解决专有模型与开源模型的辩论。虽然像ChatGPT-4[[1](#bib.bib1)]这样的专有模型功能强大，但它们的可访问性有限，存储用户数据/设计，并且需要付费使用。此外，无法对其进行微调限制了它们在领域特定EDA任务中的能力。另一方面，开源LLMs虽然提供了更好的可访问性，但与专有模型相比，其规模和资源有限，导致性能较低[[21](#bib.bib21)]。在这项工作中，我们探讨了将开源Llama-2-7B适应用于EDA教育的可行性。我们特别关注这个模型，因为它可以在消费级硬件上使用。我们的贡献如下：
- en: '1.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: A quantitative and qualitative analysis of Llama-2-7B adapted in various ways
    for EDA usage. This investigation allows us to understand the impact of fine-tuning,
    distillation, and retrieval augmentation on the model’s performance in the context
    of EDA knowledge.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对Llama-2-7B在EDA应用中的各种适应方式进行了定量和定性分析。这项研究使我们能够理解微调、蒸馏和检索增强在EDA知识背景下对模型性能的影响。
- en: '2.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We introduce and evaluate a novel fine-tuning method, Low-Rank Knowledge Distillation
    (LoRA-KD).
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍并评估了一种新颖的微调方法——低秩知识蒸馏（LoRA-KD）。
- en: '3.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The release of a benchmark, RAQ, designed for evaluating LLMs on EDA knowledge,
    aimed at facilitating future research and development in the field.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 发布了一个基准RAQ，旨在评估LLMs在EDA知识上的表现，以促进该领域未来的研究和发展。
- en: II Prior Work on LLMs for EDA
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 关于LLMs在EDA中的先前研究
- en: In the burgeoning field of EDA, early explorations into the applications of
    LLMs have already returned promising results, particularly in the nuanced areas
    of chip design, debugging, and script generation. Recently developed LLM-powered
    ChatEDA is capable of streamlining the IC design flow from RTL to GDSII [[3](#bib.bib3)].
    ChatEDA integrates Automage, a fine-tuned LLM based on Llama-2-70B architecture,
    with an EDA tool. Automage serves as an interface that accepts human requests
    and manipulates the EDA tool through API for task completion. ChatEDA was tested
    on performance evaluation, parameter grid search, parameter tuning, customized
    optimization and clock period minimization.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在蓬勃发展的EDA领域，早期对LLM应用的探索已经返回了令人鼓舞的结果，特别是在芯片设计、调试和脚本生成等细微领域。最近开发的LLM驱动的ChatEDA能够简化从RTL到GDSII的IC设计流程
    [[3](#bib.bib3)]。ChatEDA将基于Llama-2-70B架构的微调LLM Automage与EDA工具集成在一起。Automage作为一个接口，接受人类请求，并通过API操控EDA工具以完成任务。ChatEDA在性能评估、参数网格搜索、参数调优、定制优化和时钟周期最小化等方面进行了测试。
- en: On the other hand, Nvidia took a slightly different approach with their ChatNeMo,
    a Llama-2 based LLM for chip design which contributes greatly to improving productivity
    as an engineering chatbot assistant. It is also capable of generating EDA scripts
    and, bug summarization and analysis [[13](#bib.bib13)]. ChatNeMo outperforms GPT-4
    at engineering assistant chatbot and EDA script generation tasks while showing
    comparable performance at bug summarization and analysis whereas ChatEDA has shown
    comparable or better performance than GPT-4 in all its evaluated cases.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Nvidia采取了稍微不同的方法，推出了基于Llama-2的LLM ChatNeMo，用于芯片设计，这大大提高了作为工程聊天助手的生产力。它还能够生成EDA脚本、进行错误总结和分析
    [[13](#bib.bib13)]。ChatNeMo在工程助手聊天机器人和EDA脚本生成任务中优于GPT-4，而在错误总结和分析方面表现相当。相比之下，ChatEDA在所有评估案例中表现出与GPT-4相当或更好的性能。
- en: Another work [[2](#bib.bib2)] explores the possibility of LLM applications in
    conversational hardware design by having a hardware engineer co-architect a microprocessor
    architecture with GPT-4 and this design was sent to tapeout. In addition to these,
    the possibility of LLM applications in generating VLSI design specifications has
    also been explored. SpecLLM has shown significant proficiency in assisting engineers
    in generating and reviewing architecture specifications [[11](#bib.bib11)].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项工作 [[2](#bib.bib2)] 探讨了在对话硬件设计中应用大型语言模型（LLM）的可能性，通过让硬件工程师与GPT-4共同设计微处理器架构，并将这一设计发送到制造环节。除了这些，还探讨了LLM在生成VLSI设计规范中的应用可能性。SpecLLM在协助工程师生成和审查架构规范方面表现出了显著的熟练度
    [[11](#bib.bib11)]。
- en: Hardware security assessment is one more field which has studied the feasibility
    of language models. The authors of [[8](#bib.bib8)] present an automated flow
    to identify suitable modules in large HDL databases for hardware trojan insertion
    using a general-purpose LLM. The model’s ability to pinpoint candidate modules
    for the attack can be indicative of its significant comprehension of RTL codes
    and system design.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件安全评估是另一个研究语言模型可行性的领域。[[8](#bib.bib8)]的作者提出了一种自动化流程，用于在大型HDL数据库中识别适合硬件木马插入的模块，使用通用LLM。模型能够准确识别攻击候选模块，这可能表明其对RTL代码和系统设计有深刻的理解。
- en: 'TABLE I: Configurations’ Performance on Reasoning and Accuracy Questions'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 配置在推理和准确性问题上的表现'
- en: '| RAQ: Reasoning | Ground Truth | 70B Baseline | 70B LoRA | 7B Baseline | 7B
    LoRA | 7B LoRA-KD | 7B RAG |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| RAQ: 推理 | 真实情况 | 70B 基线 | 70B LoRA | 7B 基线 | 7B LoRA | 7B LoRA-KD | 7B RAG
    |'
- en: '| 1a | Increase | Increase | Increase | Increase | Increase | Increase | Decrease
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 1a | 增加 | 增加 | 增加 | 增加 | 增加 | 增加 | 减少 |'
- en: '| 1b | 3 $\mu$m | 330 nm |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 1b | 3 $\mu$m | 330 nm |'
- en: '| 2a | 0.775 mA | 5.58 mA | 5.58 mA | 1.28 A | 1.395 A | 0.06 A | × |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 2a | 0.775 mA | 5.58 mA | 5.58 mA | 1.28 A | 1.395 A | 0.06 A | × |'
- en: '| 2b | 1.55 V | 11.16 V | 8.6 V | 0.83 V | 0.647 V | 0.7 V | × |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 2b | 1.55 V | 11.16 V | 8.6 V | 0.83 V | 0.647 V | 0.7 V | × |'
- en: '| 3a | 2 V | 2 V | 2 V | 5 V | 0.625 V | 5 V | 6 V |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 3a | 2 V | 2 V | 2 V | 5 V | 0.625 V | 5 V | 6 V |'
- en: '| 3b | 3 k$\Omega$ | × |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 3b | 3 k$\Omega$ | × |'
- en: '| 4 | 2 | 2 | 2 | 2 | 2 | 2 | 2 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 2 | 2 | 2 | 2 | 2 | 2 | 2 |'
- en: '| 5a | 0.66 k$\Omega$ | × |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 5a | 0.66 k$\Omega$ | × |'
- en: '| 5b | 0.667 k$\Omega$ | × |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 5b | 0.667 k$\Omega$ | × |'
- en: '| RAQ: T/F Accuracy | - | 84% | 84% | 72% | 76% | 76% | 80% |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| RAQ: T/F 准确率 | - | 84% | 84% | 72% | 76% | 76% | 80% |'
- en: Evaluated on the reasoning and T/F questions from the RAQ benchmark. Note that
    some of the reasoning questions required multi-step thinking, eg. based on your
    answer to part a, what is part b? The $\times$ symbol denotes that the model refused
    to answer the question due to fallacious “ethical reasons.”
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAQ基准测试的推理和T/F问题上进行评估。请注意，有些推理问题需要多步骤思考，例如，根据你对第a部分的回答，第b部分是什么？符号 $\times$
    表示模型由于“伦理原因”拒绝回答问题。
- en: III Adaptation Techniques for LLMs
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 适应技术用于LLMs
- en: III-A Low-Rank Adaptation
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 低秩适应
- en: 'Low-rank adaptation (LoRA) addresses many issues associated with adapting LLMs
    for domain-specific usage[[6](#bib.bib6)]. This method bypasses the expensive
    backpropagation of gradients across all parameters by keeping the backbone model
    frozen. This is done by assuming that the update to the model’s weights have low-rank.
    In other words, instead of updating the backbone, we learn parameters $A$ which
    learn the required changes to the output of the backbone. More explicitly, if
    we write the parameter update equation, LoRA makes the following approximation:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应（LoRA）解决了许多与将LLMs适应于特定领域使用相关的问题[[6](#bib.bib6)]。该方法通过保持主干模型冻结，避免了对所有参数的梯度进行昂贵的反向传播。这是通过假设对模型权重的更新是低秩的来完成的。换句话说，LoRA
    不更新主干，而是学习参数 $A$，这些参数学习对主干输出所需的更改。更明确地说，如果我们写出参数更新方程，LoRA 进行如下近似：
- en: '|  | $\displaystyle\Theta_{t+1}$ |  | (1) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Theta_{t+1}$ |  | (1) |'
- en: '|  |  | $\displaystyle\approx\Theta_{0}-\alpha BA$ |  | (2) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\approx\Theta_{0}-\alpha BA$ |  | (2) |'
- en: '|  | $\displaystyle\text{i.e. }\eta\nabla_{\Theta}\mathcal{L}(\Theta_{t})$
    |  | (3) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{i.e. }\eta\nabla_{\Theta}\mathcal{L}(\Theta_{t})$
    |  | (3) |'
- en: Where $\Theta\in\mathbb{R}^{d\times k},B\in\mathbb{R}^{d\times r},\text{ and
    }A\in\mathbb{R}^{r\times k}$ LoRA provides a resource-efficient update to the
    backbone.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Theta\in\mathbb{R}^{d\times k},B\in\mathbb{R}^{d\times r},\text{ 和 }A\in\mathbb{R}^{r\times
    k}$ LoRA 提供了一种资源高效的更新方式。
- en: III-B Knowledge Distillation
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 知识蒸馏
- en: 'Knowledge Distillation (KD) [[5](#bib.bib5)] is a knowledge-transfer technique
    where a larger (teacher) network produces soft targets for a smaller (student)
    model. This can play a pivotal role in reducing the performance gap between larger
    and smaller models. Fine-tuning a smaller (student) model through KD can show
    improved performance compared to a normally fine-tuned small model. As an example,
    the authors of DistilBERT show that they can retain 97% of the original BERT’s
    performance despite a significantly smaller parameter count [[15](#bib.bib15)].
    This indicates that a smaller model that can be deployed on weaker hardware, e.g.
    personal computer, can maintain feasibility in handling complex tasks related
    to EDA. Written explicitly, the loss used for KD is:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏（KD）[[5](#bib.bib5)] 是一种知识转移技术，其中较大的（教师）网络为较小的（学生）模型生成软目标。这在缩小大型模型与小型模型之间的性能差距方面起着关键作用。通过KD对较小的（学生）模型进行微调，表现可能优于通常微调的小型模型。例如，DistilBERT的作者展示了尽管参数数量显著减少，但他们能够保留原始BERT性能的97%
    [[15](#bib.bib15)]。这表明，即使在较弱的硬件上（如个人电脑）可以部署的较小模型，也能在处理与EDA相关的复杂任务时保持可行性。明确地说，用于KD的损失是：
- en: '|  | $\displaystyle\mathcal{L}_{KD}$ |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{KD}$ |  |'
- en: '|  |  | $\displaystyle+\alpha\mathcal{L}_{Dist}(student(x),teacher(x))$ |  |
    (4) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\alpha\mathcal{L}_{Dist}(student(x),teacher(x))$ |  |
    (4) |'
- en: Where $\mathcal{L}_{y}$ is the loss between what the student predicted and the
    teacher predicted on an input. More elaborate distillation techniques exist, for
    example, patient KD aims at having the student mimic the teacher’s intermediate
    layers in addition to the teacher’s outputs [[16](#bib.bib16)]. We refer readers
    to [[21](#bib.bib21)] for further exploration.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_{y}$ 是学生预测结果与教师预测结果在输入上的损失。存在更复杂的蒸馏技术，例如，患者KD旨在使学生模仿教师的中间层，除了教师的输出
    [[16](#bib.bib16)]。我们建议读者参考 [[21](#bib.bib21)] 以进一步探索。
- en: III-C Low-Rank Knowledge Distillation (LoRA-KD)
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 低秩知识蒸馏（LoRA-KD）
- en: Although not entirely unprecedented, the combination of low-rank approximations
    and knowledge distillation is far less explored in the context of LLM fine-tuning.
    The authors of LoSparse [[12](#bib.bib12)] introduce a new compression scheme
    for transformers [[18](#bib.bib18)] based on a truncated singular value decomposition.
    In their experiments, they find that combining this parameter compression scheme
    with knowledge distillation further improves performance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管并非完全前所未有，但在 LLM 微调的背景下，低秩近似和知识蒸馏的结合仍然鲜有探索。LoSparse 的作者[[12](#bib.bib12)] 基于截断奇异值分解引入了一种新的变换器压缩方案[[18](#bib.bib18)]。在他们的实验中，他们发现将这种参数压缩方案与知识蒸馏结合起来，可以进一步提升性能。
- en: In our work, we reformulate this concept in accordance with figure [1](#S1.F1
    "Figure 1 ‣ I Introduction and Motivation ‣ Can Low-Rank Knowledge Distillation
    in LLMs be Useful for Microelectronic Reasoning?"). We begin by fine-tuning the
    teacher (Llama-2-70B) using LoRA. Afterwards, we fine-tune the student (Llama-2-7B)
    via LoRA using $\mathcal{L}_{KD}$. We hypothesize that, if the updates to the
    teacher can be done in a low-rank fashion, then the underlying knowledge being
    learned is also low-rank; therefore, the knowledge to be distilled to the student
    is also low-rank.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，我们根据图 [1](#S1.F1 "图 1 ‣ I 引言与动机 ‣ 低秩知识蒸馏在 LLM 中对微电子推理是否有用？") 重新定义了这一概念。我们首先使用
    LoRA 对教师（Llama-2-70B）进行微调。随后，我们通过 LoRA 使用 $\mathcal{L}_{KD}$ 对学生（Llama-2-7B）进行微调。我们假设，如果对教师的更新可以以低秩方式进行，那么正在学习的基础知识也是低秩的；因此，蒸馏到学生的知识也应为低秩。
- en: 'TABLE II: Comparison of Model/Adaptation Combinations Evaluated by Human Expert
    and GPT-4.5 Turbo via Likert Scale.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：由人工专家和 GPT-4.5 Turbo 通过李克特量表评估的模型/适应组合比较。
- en: '|  | Human Expert | GPT-4.5 Turbo | Pearson Correlation |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | 人工专家 | GPT-4.5 Turbo | 皮尔逊相关性 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Configuration | Accuracy | Quality | Accuracy | Quality | Accuracy | Quality
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 配置 | 准确度 | 质量 | 准确度 | 质量 | 准确度 | 质量 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 70B Baseline | 4.2[$\pm$1.95] | 0.51 | 0.43 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 70B 基线 | 4.2[$\pm$1.95] | 0.51 | 0.43 |'
- en: '| 70B LoRA | 4.35[$\pm$1.67] | 0.47 | 0.51 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 70B LoRA | 4.35[$\pm$1.67] | 0.47 | 0.51 |'
- en: '| 7B Baseline | 3.4[$\pm$1.93] | 0.53 | 0.57 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 7B 基线 | 3.4[$\pm$1.93] | 0.53 | 0.57 |'
- en: '| 7B LoRA | 3.5[$\pm$1.90] | 0.66 | 0.73 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 7B LoRA | 3.5[$\pm$1.90] | 0.66 | 0.73 |'
- en: '| 7B LoRA-KD | 3.525[$\pm$1.60] | 0.60 | 0.59 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 7B LoRA-KD | 3.525[$\pm$1.60] | 0.60 | 0.59 |'
- en: '| 7B RAG | 4.2[$\pm$1.94] | 0.12 | 0.13 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 7B RAG | 4.2[$\pm$1.94] | 0.12 | 0.13 |'
- en: Each response to the 40 qualitative questions was evaluated on a 7-point Likert
    scale by a human expert and GPT-4.5 Turbo. $7$ denotes “strongly disagreed with.”
    The subcolumns correspond to how much the evaluator agreed/disagreed with the
    accuracy/quality of the response. The standard deviations across the questions
    are written in sub-scripts. The correlation quantifies the consistency between
    the human expert and GPT-4.5 Turbo.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对 40 个定性问题的每个回答都由人工专家和 GPT-4.5 Turbo 通过 7 分李克特量表进行评估。$7$ 表示“强烈不同意”。子列对应于评估者对回答的准确性/质量的同意/不同意程度。问题的标准偏差以子脚本形式书写。相关性量化了人工专家与
    GPT-4.5 Turbo 之间的一致性。
- en: 'There are several advantages to doing this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做有几个优点：
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: As with ordinary RAG, a pre-trained model can be repurposed via hot-swapping
    the adaptation layer. For example, EDA educators can use LoRA-KD to learn separate
    (small) adaptation layers for English and Spanish in the context of a bilingual
    classroom.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与普通 RAG 一样，预训练模型可以通过热交换适应层重新利用。例如，EDA 教育者可以在双语课堂的背景下使用 LoRA-KD 学习针对英语和西班牙语的独立（小型）适应层。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: KD has been used to enhance domain-adaptation tasks. We hypothesize that the
    dark knowledge distilled from the teacher to the student will facilitate enhanced
    reasoning capabilities [[20](#bib.bib20)].
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: KD 被用来增强领域适应任务。我们假设从教师到学生的暗知识的提炼将促进推理能力的提升[[20](#bib.bib20)]。
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The training process remains fast. In our experiments, fine-tuning the student
    via LoRA-KD did not take much more time than ordinary LoRA.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练过程保持快速。在我们的实验中，通过 LoRA-KD 微调学生所需的时间并不比普通 LoRA 多。
- en: III-D Retrieval Augmented Generation (RAG)
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 检索增强生成（RAG）
- en: 'RAG[[10](#bib.bib10)] operates by integrating a neural retriever with a sequence-to-sequence
    (seq2seq) generator. The retriever produces a distribution, $p_{r}(z|x)$. RAG’s
    seq2seq probability distribution is defined as:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: RAG[[10](#bib.bib10)] 通过将神经检索器与序列到序列（seq2seq）生成器集成来工作。检索器生成一个分布，$p_{r}(z|x)$。RAG
    的 seq2seq 概率分布定义为：
- en: '|  | $\displaystyle p(y&#124;x)$ |  | (5) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p(y&#124;x)$ |  | (5) |'
- en: This method combines the strengths of pre-trained parametric models with non-parametric
    external knowledge sources. For our work, we use the pre-trained MiniLM model
    [[19](#bib.bib19)] as the retriever and the pre-trained Llama-2-7B as the generator.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法结合了预训练参数模型的优势与非参数外部知识源。对于我们的工作，我们使用了预训练的MiniLM模型[[19](#bib.bib19)]作为检索器，以及预训练的Llama-2-7B作为生成器。
- en: IV Fine-tuning Dataset and the RAQ Benchmark
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 微调数据集与RAQ基准
- en: Our fine-tuning dataset consists of several well-known textbooks on microelectronics,
    VLSI circuit design, and fabrication technologies. In addition to these, we also
    included some recently published works related to DDR5 design and its corresponding
    JEDEC standard. After filtering the data, the number of tokens was calculated
    using Llama-2 tokenizer. The dataset contains 3,168,414 tokens and 12,988 unique
    tokens. Due to copyright reasons, we cannot release the fine-tuning dataset. However,
    we list all the components of the dataset within the appendix so that readers
    can assemble it themselves.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的微调数据集包括了几本关于微电子学、VLSI电路设计和制造技术的著名教科书。此外，我们还包含了一些与DDR5设计及其相应JEDEC标准相关的最近出版的作品。经过数据筛选后，使用Llama-2分词器计算了标记的数量。数据集包含3,168,414个标记和12,988个独特标记。由于版权原因，我们不能发布微调数据集。然而，我们在附录中列出了数据集的所有组成部分，以便读者可以自行组装。
- en: We created a benchmark to evaluate the performance of the different models which
    includes 70 carefully-curated domain-specific questions. Among them, there are
    40 qualitative questions and 25 true/false questions. The 65 aforementioned questions
    are meant to evaluate the accuracy and quality of the LLM’s responses on domain
    knowledge. Furthermore, 5 questions are designed to evaluate the models’ capabilities
    to reason upon circuit design decisions based on given specifications. Hence,
    we name it the Reasoning-Accuracy-Quality (RAQ) benchmark.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个基准来评估不同模型的表现，包括70个经过精心挑选的领域特定问题。其中有40个定性问题和25个是非问题。这65个问题旨在评估LLM在领域知识上的准确性和质量。此外，5个问题旨在评估模型基于给定规格对电路设计决策的推理能力。因此，我们将其命名为推理-准确性-质量（RAQ）基准。
- en: V Setup and Experiments
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 设置与实验
- en: 'To assess the suitability of various adaptation methods, we performed four
    experiments using the RAQ Benchmark:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估各种适应方法的适用性，我们使用RAQ基准进行了四次实验：
- en: '1.'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Student Survey. We selected 15 questions which would be most relevant for a
    third-year undergraduate microelectronics classroom. We recorded the responses
    from each configuration and asked students to provide the ordinal rankings in
    terms of what they preferred. To ensure quality, we kept the configurations anonymous
    and asked students to explain why they ranked the best/worst models as they did.
    After pruning low-quality submissions, we had 51 rankings.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学生调查。我们选择了15个对三年级本科微电子学课堂最相关的问题。我们记录了每个配置的响应，并要求学生按他们的偏好提供排序。为了确保质量，我们保持配置的匿名，并要求学生解释为什么他们将最佳/最差模型排序为这样。经过筛选低质量提交后，我们得到了51个排名。
- en: '2.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: True/False Q&A. We prompted each configuration to answer true or false to determine
    accuracy. This portion was taken from the T/F section.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是非问答。我们让每个配置回答真假问题以确定准确性。这部分内容来自T/F部分。
- en: '3.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Likert Test. Each configuration was asked to answer all 40 qualitative questions.
    Using a 7-point Likert scale, the responses were scrutinized in terms of accuracy
    and subjective quality. We¹¹1We recognize there could be bias if we, the authors,
    evaluate these models. To promote transparency, we release the model responses
    on our GitHub (human expert) and ChatGPT-4.5 Turbo were the evaluators.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Likert测试。每个配置都被要求回答所有40个定性问题。使用7分Likert量表，评估了回答的准确性和主观质量。我们认识到如果由我们作者评估这些模型可能会有偏见。为了促进透明度，我们在我们的GitHub上发布了模型的响应（人类专家）和ChatGPT-4.5
    Turbo是评估者。
- en: '4.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Reasoning Test. We tested each configuration with 5 reasoning questions. These
    questions have unambiguous or numerical answers. Generated responses were compared
    against ground truth values.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理测试。我们用5个推理问题测试了每个配置。这些问题有明确的或数值的答案。生成的回答与真实值进行了比较。
- en: For all experiments, we use $\mathtt{LoRA\_Rank=4}$ for KD.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有实验中，我们使用$\mathtt{LoRA\_Rank=4}$进行KD。
- en: VI Results and Conclusion
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结果与结论
- en: In this work, we try to explore the feasibility of using language models in
    EDA education. Table [I](#S2.T1 "TABLE I ‣ II Prior Work on LLMs for EDA ‣ Can
    Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic Reasoning?")
    investigates the configurations’ capabilities to reason based on the given information
    and optimize a given design. A few interesting observations were made while evaluating
    the models on reasoning/optimization questions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们尝试探索在EDA教育中使用语言模型的可行性。表格[I](#S2.T1 "TABLE I ‣ II Prior Work on LLMs
    for EDA ‣ Can Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic
    Reasoning?")调查了配置在基于给定信息进行推理和优化设计方面的能力。在评估模型的推理/优化问题时，做出了一些有趣的观察。
- en: '1.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: All the models had difficulty with numerical calculations and assigning proper
    units to a calculated value.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有模型在数值计算和为计算值分配适当单位方面都存在困难。
- en: '2.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: In our experiments, the models performed better when they were asked the different
    sections of the questions one by one in separate prompts, rather than putting
    all the questions in a single prompt.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的实验中，当模型逐一接收问题的不同部分时表现更好，而不是将所有问题放在一个提示中。
- en: '3.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: RAG tended to refuse answering due to dubious ethical reasons regarding the
    “danger of transistors.”
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RAG由于对“晶体管的危险”存在可疑的伦理原因而倾向于拒绝回答。
- en: An analysis of the data presented in tables [I](#S2.T1 "TABLE I ‣ II Prior Work
    on LLMs for EDA ‣ Can Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic
    Reasoning?") and [II](#S3.T2 "TABLE II ‣ III-C Low-Rank Knowledge Distillation
    (LoRA-KD) ‣ III Adaptation Techniques for LLMs ‣ Can Low-Rank Knowledge Distillation
    in LLMs be Useful for Microelectronic Reasoning?") gives insights into the strengths
    and weaknesses of the configurations. For the Likert test and true/false accuracy,
    7B RAG performs strongly but for reasoning/optimization, it exhibits a sharp decline
    in performance. This indicates that RAG alone cannot improve performance across
    all areas. On the contrary, the various fine-tuned versions manage to perform
    well on the reasoning portion while remaining within half a standard deviation
    from RAG on the Likert test.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对表格[I](#S2.T1 "TABLE I ‣ II Prior Work on LLMs for EDA ‣ Can Low-Rank Knowledge
    Distillation in LLMs be Useful for Microelectronic Reasoning?")和[II](#S3.T2 "TABLE
    II ‣ III-C Low-Rank Knowledge Distillation (LoRA-KD) ‣ III Adaptation Techniques
    for LLMs ‣ Can Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic
    Reasoning?")中呈现的数据进行分析，提供了关于配置优缺点的见解。对于Likert测试和真/假准确性，7B RAG表现强劲，但对于推理/优化，它的表现急剧下降。这表明RAG单独无法在所有领域提高性能。相反，各种微调版本在推理部分表现良好，同时在Likert测试中与RAG保持在半个标准差以内。
- en: The responses collected from the students underscore each configuration’s communication
    skills and human expectations which can serve as an important guideline when fine-tuning
    an LLM. An improvement of LoRA-KD over LoRA can be observed in figure [2](#S1.F2
    "Figure 2 ‣ I Introduction and Motivation ‣ Can Low-Rank Knowledge Distillation
    in LLMs be Useful for Microelectronic Reasoning?") where the responses generated
    by 7B LoRA-KD were far less likely to be ranked last. Another interesting facet
    is the agreement between the students with respect to the question (i.e. the entropy).
    For example, for Q14, there was high agreement that the 70B Baseline did the worst.
    On the other hand, for Q15, the students seemed split between whether 7B RAG,
    7B LoRA, or 70B LoRA was the worst.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从学生那里收集的反馈突显了每种配置的沟通技能和人类期望，这可以作为微调LLM时的重要指导方针。在图[2](#S1.F2 "Figure 2 ‣ I Introduction
    and Motivation ‣ Can Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic
    Reasoning?")中可以观察到LoRA-KD相对于LoRA的改进，其中7B LoRA-KD生成的回应排名靠后的可能性要小得多。另一个有趣的方面是学生们对问题的共识（即熵）。例如，对于Q14，有很高的共识认为70B
    Baseline表现最差。另一方面，对于Q15，学生们在7B RAG、7B LoRA和70B LoRA中谁表现最差的问题上似乎分歧较大。
- en: While the existing works are significant milestones of the application of LLMs
    in EDA, its potential in this field has yet to be fully realized. Development
    of specialized large language models capable of understanding the intricacies
    of domain-specific EDA tasks is crucial for its continued applications in EDA
    [[4](#bib.bib4)]. This study highlights some strengths and weaknesses of different
    open-source offline LLM configurations.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现有的工作在LLM在EDA中的应用上是重要的里程碑，但其在这一领域的潜力尚未完全实现。开发能够理解领域特定EDA任务复杂性的专门大语言模型对其在EDA中的持续应用至关重要[[4](#bib.bib4)]。这项研究突出了不同开源离线LLM配置的一些优点和缺点。
- en: References
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
    Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia
    Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat等。GPT-4技术报告。arXiv预印本arXiv:2303.08774，2023年。'
- en: '[2] Jason Blocklove, Siddharth Garg, Ramesh Karri, and Hammond Pearce. Chip-chat:
    Challenges and opportunities in conversational hardware design. In 2023 ACM/IEEE
    5th Workshop on Machine Learning for CAD (MLCAD), pages 1–6\. IEEE, 2023.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Jason Blocklove, Siddharth Garg, Ramesh Karri, 和 Hammond Pearce. Chip-chat:
    对话式硬件设计中的挑战与机遇。发表于2023年ACM/IEEE第五届CAD机器学习研讨会（MLCAD），页码1–6\. IEEE，2023年。'
- en: '[3] Zhuolun He, Haoyuan Wu, Xinyun Zhang, Xufeng Yao, Su Zheng, Haisheng Zheng,
    and Bei Yu. Chateda: A large language model powered autonomous agent for eda,
    2024.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Zhuolun He, Haoyuan Wu, Xinyun Zhang, Xufeng Yao, Su Zheng, Haisheng Zheng,
    和 Bei Yu. Chateda: 面向EDA的大型语言模型驱动的自主代理，2024年。'
- en: '[4] Zhuolun He and Bei Yu. Large language models for eda: Future or mirage?
    In Proceedings of the 2024 International Symposium on Physical Design, ISPD ’24,
    page 65–66, New York, NY, USA, 2024\. Association for Computing Machinery.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Zhuolun He 和 Bei Yu. 面向EDA的大型语言模型: 未来还是幻影？发表于2024年国际物理设计研讨会，ISPD ’24，页码65–66，纽约，NY，USA，2024年\.
    计算机协会。'
- en: '[5] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge
    in a neural network, 2015.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Geoffrey Hinton, Oriol Vinyals, 和 Jeff Dean. 神经网络中的知识蒸馏，2015年。'
- en: '[6] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language
    models. arXiv preprint arXiv:2106.09685, 2021.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, 和 Weizhu Chen. Lora: 大型语言模型的低秩适应。arXiv预印本arXiv:2106.09685，2021年。'
- en: '[7] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
    arXiv preprint arXiv:1412.6980, 2014.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Diederik P Kingma 和 Jimmy Ba. Adam: 一种随机优化方法。arXiv预印本arXiv:1412.6980，2014年。'
- en: '[8] Georgios Kokolakis, Athanasios Moschos, and Angelos D Keromytis. Harnessing
    the power of general-purpose llms in hardware trojan design.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Georgios Kokolakis, Athanasios Moschos, 和 Angelos D Keromytis. 在硬件特洛伊木马设计中利用通用llms的力量。'
- en: '[9] Ehsan Latif, Luyang Fang, Ping Ma, and Xiaoming Zhai. Knowledge distillation
    of llm for automatic scoring of science education assessments, 2024.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Ehsan Latif, Luyang Fang, Ping Ma, 和 Xiaoming Zhai. 针对科学教育评估的自动评分的llm知识蒸馏，2024年。'
- en: '[10] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
    et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances
    in Neural Information Processing Systems, 33:9459–9474, 2020.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel等。用于知识密集型NLP任务的检索增强生成。神经信息处理系统进展，33:9459–9474，2020年。'
- en: '[11] Mengming Li, Wenji Fang, Qijun Zhang, and Zhiyao Xie. Specllm: Exploring
    generation and review of vlsi design specification with large language model,
    2024.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Mengming Li, Wenji Fang, Qijun Zhang, 和 Zhiyao Xie. Specllm: 利用大型语言模型探索VLSI设计规范的生成和审查，2024年。'
- en: '[12] Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen,
    and Tuo Zhao. Losparse: Structured compression of large language models based
    on low-rank and sparse approximation. In International Conference on Machine Learning,
    pages 20336–20350\. PMLR, 2023.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen,
    和 Tuo Zhao. Losparse: 基于低秩和稀疏逼近的大型语言模型的结构化压缩。发表于国际机器学习大会，页码20336–20350\. PMLR，2023年。'
- en: '[13] Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng, Nathaniel
    Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee, Ismet
    Bayraktaroglu, Bonita Bhaskaran, Bryan Catanzaro, Arjun Chaudhuri, Sharon Clay,
    Bill Dally, Laura Dang, Parikshit Deshpande, Siddhanth Dhodhi, Sameer Halepete,
    Eric Hill, Jiashang Hu, Sumit Jain, Ankit Jindal, Brucek Khailany, George Kokai,
    Kishor Kunal, Xiaowei Li, Charley Lind, Hao Liu, Stuart Oberman, Sujeet Omar,
    Sreedhar Pratty, Jonathan Raiman, Ambar Sarkar, Zhengjiang Shao, Hanfei Sun, Pratik P
    Suthar, Varun Tej, Walker Turner, Kaizhe Xu, and Haoxing Ren. Chipnemo: Domain-adapted
    llms for chip design, 2024.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng, Nathaniel
    Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee, Ismet
    Bayraktaroglu, Bonita Bhaskaran, Bryan Catanzaro, Arjun Chaudhuri, Sharon Clay,
    Bill Dally, Laura Dang, Parikshit Deshpande, Siddhanth Dhodhi, Sameer Halepete,
    Eric Hill, Jiashang Hu, Sumit Jain, Ankit Jindal, Brucek Khailany, George Kokai,
    Kishor Kunal, Xiaowei Li, Charley Lind, Hao Liu, Stuart Oberman, Sujeet Omar,
    Sreedhar Pratty, Jonathan Raiman, Ambar Sarkar, Zhengjiang Shao, Hanfei Sun, Pratik
    P Suthar, Varun Tej, Walker Turner, Kaizhe Xu, 和 Haoxing Ren. Chipnemo: 面向芯片设计的领域适应llms，2024年。'
- en: '[14] Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren. Invited
    paper: Verilogeval: Evaluating large language models for verilog code generation.
    In 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD), pages
    1–8, 2023.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 刘明杰、内森尼尔·平克尼、布鲁斯克·凯拉尼和任浩星。《邀请论文：Verilogeval：评估大型语言模型在Verilog代码生成中的表现》。发表于2023
    IEEE/ACM国际计算机辅助设计会议（ICCAD），第1–8页，2023年。'
- en: '[15] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert,
    a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint
    arXiv:1910.01108, 2019.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 维克托·桑、莉桑德·德布特、朱利安·肖蒙和托马斯·沃尔夫。《Distilbert：BERT的蒸馏版：更小、更快、更便宜、更轻》，arXiv预印本arXiv:1910.01108，2019年。'
- en: '[16] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation
    for bert model compression. arXiv preprint arXiv:1908.09355, 2019.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 孙思琪、余程、闵哲·甘和刘静静。《BERT模型压缩的患者知识蒸馏》，arXiv预印本arXiv:1908.09355，2019年。'
- en: '[17] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 雨果·图夫朗、路易斯·马丁、凯文·斯通、彼得·阿尔伯特、阿姆贾德·阿尔迈赫里、雅斯敏·巴巴伊、尼古拉·巴什利科夫、苏米亚·巴特拉、普拉贾瓦尔·巴尔戈瓦、施鲁提·博萨尔等。《Llama
    2：开放基础和微调聊天模型》，arXiv预印本arXiv:2307.09288，2023年。'
- en: '[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 阿什什·瓦斯瓦尼、诺亚姆·沙泽尔、尼基·帕尔玛、雅各布·乌斯科雷特、利昂·琼斯、艾登·N·戈麦斯、卢卡斯·凯泽和伊利亚·波洛苏金。《注意力机制即一切》，神经信息处理系统进展，30，2017年。'
- en: '[19] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm:
    Deep self-attention distillation for task-agnostic compression of pre-trained
    transformers, 2020.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 王文辉、魏富如、董李、包航波、杨楠、周铭。《Minilm：深度自注意力蒸馏用于任务无关的预训练变换器压缩》，2020年。'
- en: '[20] Yufei Wang, Haoliang Li, Lap-pui Chau, and Alex C. Kot. Embracing the
    dark knowledge: Domain generalization using regularized knowledge distillation.
    In Proceedings of the 29th ACM International Conference on Multimedia, MM ’21,
    page 2595–2604, New York, NY, USA, 2021\. Association for Computing Machinery.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 王玉飞、李浩良、周立培、亚历克斯·C·科特。《拥抱黑暗知识：使用正则化知识蒸馏进行领域泛化》。发表于第29届ACM国际多媒体会议（MM ’21），第2595–2604页，美国纽约，2021年。计算机协会。'
- en: '[21] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li,
    Can Xu, Dacheng Tao, and Tianyi Zhou. A survey on knowledge distillation of large
    language models, 2024.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 徐晓寒、李铭、陶崇阳、沈涛、雷诺德·郑、李金阳、徐灿、陶大成、周天义。《大语言模型的知识蒸馏调查》，2024年。'
- en: VII Appendix
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 附录
- en: VII-A Fine-tuning Sources
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 微调来源
- en: 'The following sources were used in fine-tuning. An enumerated list is also
    available on our github:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 微调中使用了以下来源。我们github上也有一个列举列表：
- en: '1.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Fundamentals of Microelectronics - 2nd Edition - Behzad Razavi
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 《微电子学基础 - 第2版 - 贝赫扎德·拉扎维》
- en: '2.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Electronic Devices and Circuit Theory - 11th Edition - Robert L. BoyleStad and
    Louis Nashelsky
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 《电子设备与电路理论 - 第11版 - 罗伯特·L·博伊尔斯塔德和路易斯·纳谢尔斯基》
- en: '3.'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: CMOS VLSI Design - 4th Edition - Neil H. E. Weste and David M. Harris
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 《CMOS VLSI设计 - 第4版 - 尼尔·H·E·韦斯特和大卫·M·哈里斯》
- en: '4.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Fundamentals of Semiconductor Manufacturing and Process Control - Gary S. May
    and Costas J. Spanos
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 《半导体制造与工艺控制基础 - 加里·S·梅和科斯塔斯·J·斯潘诺斯》
- en: '5.'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Fabrication Engineering at the Micro and Nanoscale - 3rd Edition - Stephen A.
    Campbell
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 《微纳尺度的制造工程 - 第3版 - 斯蒂芬·A·坎贝尔》
- en: '6.'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: JEDEC Standard - Graphics Double Data Rate (GDDR5) SGRAM Standard
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC标准 - 图形双数据速率（GDDR5）SGRAM标准
- en: '7.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: JEDEC Standard - Compression Attached Memory Module (CAMM2) Common Standard
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC标准 - 压缩附加内存模块（CAMM2）通用标准
- en: '8.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: JEDEC Standard - DDR5 Clocked Small Outline Dual Inline Memory Module (CSODIMM)
    Common Standard
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC标准 - DDR5时钟小外形双列直插内存模块（CSODIMM）通用标准
- en: '9.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: DDR5 Clocked Unbuffered Dual Inline Memory Module (CUDIMM) Common Specification
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DDR5时钟非缓冲双列直插内存模块（CUDIMM）通用规格
- en: '10.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '10.'
- en: JEDEC Standard - DDR5 262 Pin SODIMM Connector Performance Standard
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC标准 - DDR5 262针SODIMM连接器性能标准
- en: '11.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '11.'
- en: JEDEC Standard - DDR5 Unbuffered Dual Inline Memory Module (UDIMM) Common Standard
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC标准 - DDR5非缓冲双列直插内存模块（UDIMM）通用标准
- en: '12.'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '12.'
- en: JEDEC Standard - DDR5 288 Pin U/R/LR DIMM Connector Performance Standard
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC标准 - DDR5 288针U/R/LR DIMM连接器性能标准
- en: '13.'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '13.'
- en: JEDEC Standard - DDR5 Load Reduced (LRDIMM) and Registered Dual Inline Memory
    Module (RDIMM) Common Specification
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC标准 - DDR5负载降低（LRDIMM）和注册双列直插内存模块（RDIMM）通用规格
- en: '14.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '14.'
- en: JEDEC Standard - DDR5 Clock Driver Definition (DDR5CKD01)
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC 标准 - DDR5 时钟驱动器定义 (DDR5CKD01)
- en: '15.'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '15.'
- en: JEDEC Standard - DDR5 Small Outline Dual Inline Memory Module (SODIMM) Common
    Standard
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC 标准 - DDR5 小型轮廓双列直插内存模块 (SODIMM) 通用标准
- en: '16.'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '16.'
- en: JEDEC Standard - DDR5 Registering Clock Driver Definition (DDR5RCD03)
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC 标准 - DDR5 注册时钟驱动器定义 (DDR5RCD03)
- en: '17.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '17.'
- en: JEDEC Standard - DDR5 DIMM Labels
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC 标准 - DDR5 DIMM 标签
- en: '18.'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '18.'
- en: JEDEC Standard - GDDR5 Measurement Procedures
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC 标准 - GDDR5 测量程序
- en: '19.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '19.'
- en: JEDEC Standard - DDR5 Serial Presence Detect (SPD) Contents
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC 标准 - DDR5 串行存在检测 (SPD) 内容
- en: '20.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '20.'
- en: JEDEC Standard - Graphics Double Data Rate (GDDR5X) SGRAM Standard
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC 标准 - 图形双倍数据率 (GDDR5X) SGRAM 标准
- en: '21.'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '21.'
- en: JEDEC Standard - DDR5 SDRAM
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEDEC 标准 - DDR5 SDRAM
- en: '22.'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '22.'
- en: Improving Memory Reliability by Bounding DRAM Faults - KJERSTEN CRISS, KULJIT
    BAINS, RAJAT AGARWAL, TANJ BENNETT, TERRY GRUNZKE, JANGRYUL KEITH KIM, HOEJU CHUNG,
    MUNSEON JANG
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过限制 DRAM 故障来提高内存可靠性 - KJERSTEN CRISS, KULJIT BAINS, RAJAT AGARWAL, TANJ BENNETT,
    TERRY GRUNZKE, JANGRYUL KEITH KIM, HOEJU CHUNG, MUNSEON JANG
- en: '23.'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '23.'
- en: Optimizing DDR5 address signal integrity using stochastic learning algorithms
    - Nitin Bhagwath, Daniel DeAraujo, Jayaprakash Balachandran, BaekKyu Choi
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用随机学习算法优化 DDR5 地址信号完整性 - Nitin Bhagwath, Daniel DeAraujo, Jayaprakash Balachandran,
    BaekKyu Choi
- en: '24.'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '24.'
- en: DDR5 Electrical Challenges in High-Speed Server Design - Douglas Winterberg,
    Vijender Kumar, Tom Chen, Bhyrav Mutnury
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DDR5 高速服务器设计中的电气挑战 - Douglas Winterberg, Vijender Kumar, Tom Chen, Bhyrav Mutnury
- en: '25.'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '25.'
- en: Modeling of DDR5 Signaling from Jitter Sequences to Accurate Bit Error Rate
    (BER) - Alaeddin A. Aydiner, Yunhui Chu, Oleg Mikulchenko, Jin Yan, Robert J.
    Friar, Ellen Yan Fu
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从抖动序列到准确比特错误率 (BER) 的 DDR5 信号建模 - Alaeddin A. Aydiner, Yunhui Chu, Oleg Mikulchenko,
    Jin Yan, Robert J. Friar, Ellen Yan Fu
- en: '26.'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '26.'
- en: LPDDR5 (6.4 Gbps) 1-tap DFE Optimal Weight Determination - Sunil Gupta, Ph.D.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LPDDR5 (6.4 Gbps) 1-tap DFE 最优权重确定 - Sunil Gupta, Ph.D.
- en: '27.'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '27.'
- en: Far-End Crosstalk Mitigation for Transmission Lines in DDR5 Using Glass-Weave
    Coating Structure - Xiao-Bo Yu, Qiang-Ming Cai, Liang Zhang, Chao Zhang, Lin Zhu,
    Xin Cao, and Jun Fan
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用玻璃织物涂层结构减少 DDR5 中的远端串扰 - Xiao-Bo Yu, Qiang-Ming Cai, Liang Zhang, Chao Zhang,
    Lin Zhu, Xin Cao 和 Jun Fan
- en: '28.'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '28.'
- en: Simulating DDR5 Systems with Clocked Receivers - Matthew Leslie, Justin Butterfield,
    Randy Wolff
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用时钟接收器模拟 DDR5 系统 - Matthew Leslie, Justin Butterfield, Randy Wolff
- en: '29.'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '29.'
- en: Design and Analysis of Power Integrity of DDR5 Dual In-Line Memory Modules -
    Shinyoung Park, Vinod Arjun Huddar
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DDR5 双列直插内存模块的电源完整性设计与分析 - Shinyoung Park, Vinod Arjun Huddar
- en: '30.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '30.'
- en: Deterministic Policy Gradient-based Reinforcement Learning for DDR5 Memory Signaling
    Architecture Optimization considering Signal Integrity - Daehwan Lho, Hyunwook
    Park, Keunwoo Kim, Seongguk Kim, Boogyo Sim, Kyungjune Son, Keeyoung Son, Jihun
    Kim, Seonguk Choi, Joonsang Park, Haeyeon Kim, Kyubong Kong, Joungho Kim
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于确定性策略梯度的强化学习用于 DDR5 内存信号架构优化考虑信号完整性 - Daehwan Lho, Hyunwook Park, Keunwoo
    Kim, Seongguk Kim, Boogyo Sim, Kyungjune Son, Keeyoung Son, Jihun Kim, Seonguk
    Choi, Joonsang Park, Haeyeon Kim, Kyubong Kong, Joungho Kim
- en: '31.'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '31.'
- en: 'Advancing DDR5 Test and Measurements: Fine-tuning a Large Language Model AI
    Expert in DDR5 Protocols - Xinran Li'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推进 DDR5 测试和测量：微调大语言模型 AI 专家在 DDR5 协议中的应用 - Xinran Li
- en: '32.'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '32.'
- en: DDR5 Design Challenges - Nitin Bhagwath, Randy Wolff, Shinichiro Ikeda, Eiji
    Fujine, Ryo Shibata, Yumiko Sugaya, Megumi Ono
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DDR5 设计挑战 - Nitin Bhagwath, Randy Wolff, Shinichiro Ikeda, Eiji Fujine, Ryo
    Shibata, Yumiko Sugaya, Megumi Ono
- en: '33.'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '33.'
- en: Advanced Measurement and Simulation Approach for DDR5 On-chip SI/PI with the
    Probing Package - WonSuk Choi, SangKeun Kwak, Jaeseok Park, Jiyoung Do, Byeongseon
    Yun, Yoo-jeong Kwon, Dongyeop Kim, Kyudong Lee, Tae young Kim, Wonyoung Kim, Kyoungsun
    Kim, Sung Joo Park, Jeonghyeon Cho and Hoyoung Song
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DDR5 芯片内部 SI/PI 高级测量和模拟方法 - WonSuk Choi, SangKeun Kwak, Jaeseok Park, Jiyoung
    Do, Byeongseon Yun, Yoo-jeong Kwon, Dongyeop Kim, Kyudong Lee, Tae young Kim,
    Wonyoung Kim, Kyoungsun Kim, Sung Joo Park, Jeonghyeon Cho 和 Hoyoung Song
