- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:04:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:11
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Scissorhands: 利用重要性假设的持久性进行LLM KV缓存压缩'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.17118](https://ar5iv.labs.arxiv.org/html/2305.17118)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.17118](https://ar5iv.labs.arxiv.org/html/2305.17118)
- en: Zichang Liu
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 刘子畅
- en: Department of Computer Science
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Rice University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 莱斯大学
- en: Houston, TX 77005
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 休斯顿，TX 77005
- en: zichangliu@rice.edu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: zichangliu@rice.edu
- en: '&Aditya Desai'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '&阿迪提亚·德赛'
- en: Department of Computer Science
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Rice University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 莱斯大学
- en: Houston, TX 77005
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 休斯顿，TX 77005
- en: Aditya.P.Desai@rice.edu
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Aditya.P.Desai@rice.edu
- en: '&Fangshuo Liao'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '&方硕·廖'
- en: Department of Computer Science
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Rice University
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 莱斯大学
- en: Houston, TX 77005
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 休斯顿，TX 77005
- en: Fangshuo.Liao@rice.edu
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Fangshuo.Liao@rice.edu
- en: '&Weitao Wang'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '&魏涛·王'
- en: Department of Computer Science
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Rice University
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 莱斯大学
- en: Houston, TX 77005
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 休斯顿，TX 77005
- en: wtwang@rice.edu
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: wtwang@rice.edu
- en: '&Victor Xie'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '&维克托·谢'
- en: Department of Computer Science
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Rice University
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 莱斯大学
- en: Houston, TX 77005
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 休斯顿，TX 77005
- en: vyx2@rice.edu
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: vyx2@rice.edu
- en: '&Zhaozhuo Xu'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '&赵卓旭'
- en: Department of Computer Science
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Rice University
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 莱斯大学
- en: Houston, TX 77005
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 休斯顿，TX 77005
- en: zhaozhuoxu@gmail.com
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: zhaozhuoxu@gmail.com
- en: '&Anastasios Kyrillidis'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '&阿纳斯塔西奥斯·基里利迪斯'
- en: Department of Computer Science
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Rice University
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 莱斯大学
- en: Houston, TX 77005
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 休斯顿，TX 77005
- en: anastasios@rice.edu
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: anastasios@rice.edu
- en: '&Anshumali Shrivastava'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '&安舒马利·施里瓦斯塔瓦'
- en: Department of Computer Science
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Rice University
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 莱斯大学
- en: Houston, TX 77025
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 休斯顿，TX 77025
- en: anshumali@rice.edu
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: anshumali@rice.edu
- en: Abstract
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models(LLMs) have sparked a new wave of exciting AI applications.
    Hosting these models at scale requires significant memory resources. One crucial
    memory bottleneck for the deployment stems from the context window. It is commonly
    recognized that model weights are memory hungry; however, the size of key-value
    embedding stored during the generation process (KV cache) can easily surpass the
    model size. The enormous size of the KV cache puts constraints on the inference
    batch size, which is crucial for high throughput inference workload. Inspired
    by an interesting observation of the attention scores, we hypothesize *the persistence
    of importance*: only pivotal tokens, which had a substantial influence at one
    step, will significantly influence future generations. Based on our empirical
    verification and theoretical analysis around this hypothesis, we propose Scissorhands,
    a system that maintains the memory usage of KV cache under a fixed budget without
    finetuning the model. We validate that Scissorhands reduces the inference memory
    usage of the KV cache by up to 5$\times$ without compromising model quality. We
    further demonstrate that Scissorhands can be combined with 4-bit quantization
    for further compression'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）引发了一波新的激动人心的AI应用。大规模托管这些模型需要大量的内存资源。一个关键的内存瓶颈来自于上下文窗口。人们普遍认识到模型权重占用大量内存；然而，在生成过程中存储的键值嵌入（KV缓存）的大小可以轻易超出模型的大小。KV缓存的巨大规模限制了推理批次的大小，这对高吞吐量推理工作负载至关重要。受到注意力得分有趣观察的启发，我们提出了*重要性的持久性*假设：只有在某一步骤中产生重大影响的关键标记，会对未来的生成产生显著影响。基于我们对这一假设的实证验证和理论分析，我们提出了Scissorhands，一个在不调整模型的情况下，在固定预算内维持KV缓存内存使用的系统。我们验证了Scissorhands能够将KV缓存的推理内存使用减少多达5$\times$，而不影响模型质量。我们进一步展示了Scissorhands可以与4位量化结合以进一步压缩
- en: 1 Introduction
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models(LLMs), trained on immense amounts of text data, have demonstrated
    an incredible ability to generate text that is both logically connected and contextually
    relevant [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)].
    LLM inference follows an autoregressive fashion, generating one token at each
    step conditioned on the previous steps. At each step, the key-value embedding
    in attention is stored in memory to avoid repetitive key-value projection computation
    at future steps. Unfortunately, the memory of the key-value cache( KV cache),
    including prompts and previously generated tokens, can be surprisingly large.
    Using OPT-175B as an example, the impressive 175 billion parameters consume around
    325 GB of memory. At the same time, at batch size 128 and sequence length 2048,
    the KV cache requires around 950 GB of memory, three times larger than the model
    weights. Considering that 8 Nvidia A100-80GB offers 640GB GPU memory, the memory
    usage of the KV cache is truly concerning.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），在海量文本数据上进行训练，展示了生成逻辑连贯且上下文相关的文本的惊人能力[[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]。LLM推理遵循自回归方式，每一步生成一个标记，并以之前的步骤为条件。在每一步中，注意力中的键值嵌入会存储在内存中，以避免未来步骤中重复计算键值投影。不幸的是，键值缓存（KV缓存）的内存，包括提示和先前生成的标记，可能会非常庞大。以OPT-175B为例，其令人印象深刻的1750亿参数消耗约325
    GB的内存。同时，在批量大小128和序列长度2048的情况下，KV缓存需要约950 GB的内存，约为模型权重的三倍。考虑到8张Nvidia A100-80GB显卡提供640GB的GPU内存，KV缓存的内存使用确实令人担忧。
- en: LLMs are typically deployed on fixed memory hardware, and the size of model
    weights is also fixed once deployed. Apart from a small memory buffer typically
    reserved for communication and computation, the rest of the available memory is
    for the KV cache. The size of the KV cache depends on batch size, sequence length,
    and model dimension. Thus, at a given inference sequence length, compression in
    the KV cache memory translates almost linearly into an increase in the batch size.
    And any increase in batch size is significant for high-throughput inference systems [[6](#bib.bib6),
    [7](#bib.bib7)].
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: LLM通常部署在固定内存的硬件上，并且模型权重的大小一旦部署也会固定。除了通常保留用于通信和计算的小内存缓冲区外，其余的可用内存用于KV缓存。KV缓存的大小取决于批量大小、序列长度和模型维度。因此，在给定的推理序列长度下，KV缓存内存的压缩几乎线性地转化为批量大小的增加。任何批量大小的增加对高吞吐量推理系统都是显著的[[6](#bib.bib6),
    [7](#bib.bib7)]。
- en: '![Refer to caption](img/60d39d01d5b353176139bb696b2998dc.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/60d39d01d5b353176139bb696b2998dc.png)'
- en: (a) Attention map at position 178
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 位置178的注意力图
- en: '![Refer to caption](img/e3888f95d1049ced6b7ba4fada79603e.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e3888f95d1049ced6b7ba4fada79603e.png)'
- en: (b) Attention map at position 228
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 位置228的注意力图
- en: '![Refer to caption](img/2211ecfc1fe28cc899cd963c34f8c45c.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2211ecfc1fe28cc899cd963c34f8c45c.png)'
- en: (c) Attention map at position 278
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 位置278的注意力图
- en: 'Figure 1: Repetitive Attention Pattern. We plot the attention map at three
    token positions in a sentence. Only five attention heads are plotted for a clearer
    presentation. We discretize the attention score such that the high score is dark
    green, and the low score is light green. In Figure [1(a)](#S1.F1.sf1 "In Figure
    1 ‣ 1 Introduction ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time"), the token at position 178 pays heavy
    attention to positions 27, 63, 98, etc. This pattern is also present in the attention
    maps of position 228 and position 278\.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '图1：重复的注意力模式。我们绘制了句子中三个标记位置的注意力图。为了更清晰的展示，仅绘制了五个注意力头。我们离散化了注意力得分，高分为深绿色，低分为浅绿色。在图[1(a)](#S1.F1.sf1
    "图1 ‣ 1 介绍 ‣ Scissorhands: 利用重要性假设的持久性进行LLM KV缓存压缩的测试时间")中，位置178的标记对位置27、63、98等位置给予了很大的注意力。这种模式在位置228和位置278的注意力图中也存在。'
- en: Quantization and sparsity approaches [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)] have been
    studied in LLMs to reduce the model sizes. However, compressing the KV cache remains
    an open but challenging problem. First, training models at the scale of hundreds
    of billions of parameters on a large amount of data is prohibitively expensive.
    Thus, an ideal compression algorithm should be applicable without training. Second,
    emerging applications such as dialogue systems require an extremely long context
    window. The maximum sequence length of LLMs is growing to over 32K [[15](#bib.bib15)].
    The size of the KV cache also grows linearly with sequence length. For scalability,
    an ideal compression algorithm should reduce the memory from the sequence length
    dimension. At last, compression should preserve LLMs’ quality and in-context learning
    ability.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 量化和稀疏性方法 [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)] 已在LLM中研究以减少模型规模。然而，压缩KV缓存仍然是一个尚未解决但具有挑战性的问题。首先，在大量数据上训练规模达到数百亿参数的模型代价高昂。因此，理想的压缩算法应该能够在不训练的情况下应用。其次，新兴应用如对话系统需要极长的上下文窗口。LLM的最大序列长度已经增长到超过32K
    [[15](#bib.bib15)]。KV缓存的大小也随着序列长度线性增长。为了扩展性，理想的压缩算法应该减少序列长度维度的内存。最后，压缩应保留LLM的质量和上下文学习能力。
- en: 'We go beyond the traditional model compression techniques to achieve such demanding
    requirements. We envision that not all tokens must be stored in memory for LLM
    to understand the context. Just like humans can skim through an article and grasp
    the main idea, LLMs may also be able to skim and comprehend. It is commonly observed
    that the attention score from one token follows a strong power law distribution [[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)], meaning
    that one token will only heavily attend to a small number of tokens. More importantly,
    we observe Repetitive Attention Pattern from different tokens in the sequence
    in a trained LLM( Figure  [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time")). Certain tokens are more important throughout the paragraph. Specifically,
    for two different tokens, there are similarities between who they are heavily
    attending to and similarities between who they are ignoring.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '我们超越了传统的模型压缩技术，以满足这些苛刻的要求。我们设想LLM在理解上下文时不必存储所有的标记。就像人类可以快速浏览文章并抓住主要思想一样，LLM也可能能够快速浏览和理解。普遍观察到，一个标记的注意力分数遵循强的幂律分布
    [[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)]，这意味着一个标记只会对少数标记给予强烈关注。更重要的是，我们在训练后的LLM中观察到不同标记之间的重复注意模式（图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time")）。某些标记在整个段落中更为重要。具体而言，对于两个不同的标记，它们所关注的对象之间存在相似性，同时它们忽略的对象之间也存在相似性。'
- en: 'Inspired by the above observation, we articulate the Persistence of Importance
    Hypothesis: *Only pivotal tokens, which had a substantial influence at one previous
    step, will have a significant influence at a future step.* This hypothesis, if
    true, suggests that it is possible to foresee which token is likely to be important
    for future generations. Fortunately, we empirically verify that later tokens in
    the sentence mostly only attend to tokens that were heavily attended from the
    early tokens in a sentence. And the overlapping ratio is surprisingly high, over
    90% in most of the transformer layers (Figure  [2](#S3.F2 "Figure 2 ‣ 3.2 The
    Persistence of Importance Hypothesis ‣ 3 The Persistence of Importance Hypothesis
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time")).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '根据上述观察，我们阐述了重要性持久性假设：*只有在一个先前步骤中具有显著影响的关键标记，才会在未来步骤中产生重要影响*。如果这个假设是正确的，那么可以预测哪些标记可能对未来的生成具有重要性。幸运的是，我们通过实证验证了句子中的后续标记大多只关注句子早期标记中被强烈关注的标记。而且重叠比例出乎意料地高，大多数变换器层中超过90%（图
    [2](#S3.F2 "Figure 2 ‣ 3.2 The Persistence of Importance Hypothesis ‣ 3 The Persistence
    of Importance Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time")）。'
- en: 'Table 1: The memory consumption of model weights and KV cache for three different
    LLMs at batch size 128 and sequence length 2048 shows that the KV cache dominates
    the memory consumption.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在批量大小为128和序列长度为2048时，三种不同LLM的模型权重和KV缓存的内存消耗显示，KV缓存占据了内存消耗的主要部分。
- en: '| Model | # of Layer | Hidden Size | Weights (GB) | KV cache (GB) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 层数 | 隐藏层大小 | 权重 (GB) | KV缓存 (GB) |'
- en: '| OPT-175B | 96 | 12288 | 325 | 1152 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175B | 96 | 12288 | 325 | 1152 |'
- en: '| LLaMA-65B | 80 | 8192 | 130 | 640 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-65B | 80 | 8192 | 130 | 640 |'
- en: '| BLOOM | 70 | 14336 | 352 | 950 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | 70 | 14336 | 352 | 950 |'
- en: 'Based on the above two findings, we present Scissorhands that exploits persistence
    of importance hypothesis to realize LLM inference with a compressed KV cache.
    In Section [4](#S4 "4 Sequential Token Generation Under budget ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time"), we present an efficient algorithm such that the size of KV cache
    is always less than a predetermined budget. And a theoretical guarantee justifies
    that such a compressed KV cache can approximate the attention output. In Section [5](#S5
    "5 Empirical Evaluation ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time"), we empirically evaluate
    Scissorhands and show that Scissorhands reduces the memory usage of KV cache up
    to $5\times$ without degradation on model quality. Reduction in the KV cache can
    directly result in a larger batch size. Further, we adopt quantization and show
    its compatibility with Scissorhands.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '基于上述两项发现，我们提出了Scissorhands，它利用重要性假设的持久性，通过压缩KV缓存来实现LLM推理。在第[4](#S4 "4 Sequential
    Token Generation Under budget ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time")节中，我们提出了一种高效的算法，使得KV缓存的大小始终小于预定预算。理论保证了这种压缩的KV缓存可以近似注意力输出。在第[5](#S5
    "5 Empirical Evaluation ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time")节中，我们对Scissorhands进行了实证评估，结果表明，Scissorhands将KV缓存的内存使用减少了多达$5\times$，而不会降低模型质量。KV缓存的减少可以直接导致更大的批量大小。此外，我们采用了量化技术，并展示了其与Scissorhands的兼容性。'
- en: 2 Problem Description and Related Work
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题描述和相关工作
- en: This paper considers the LLM inference workflow, specifically focusing on the
    memory usage for storing the keys and values in attention. Let $d$ transformer
    layer.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本文考虑了LLM推理工作流程，特别关注存储注意力中的键和值的内存使用情况。设$d$为变换器层。
- en: 'The standard LLM inference consists of two stages: prompting and token generation.
    In the prompting stage, the model takes the prompt sentences as the input, and
    the key/value embedding in attention is stored as a cache to reduce repetitive
    computation. Denote $x_{\text{prompt}}^{i}=[x_{1}^{i},...,x_{p}^{i}],x_{\text{prompt}}^{i}\in{\mathbb{R}}^{b\times
    p\times d}$.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 标准LLM推理包括两个阶段：提示阶段和标记生成阶段。在提示阶段，模型将提示句子作为输入，并将注意力中的键/值嵌入存储为缓存，以减少重复计算。设$x_{\text{prompt}}^{i}=[x_{1}^{i},...,x_{p}^{i}],x_{\text{prompt}}^{i}\in{\mathbb{R}}^{b\times
    p\times d}$。
- en: In the generation stage, the model starts with the stored KV cache in the prompting
    stage and generates one token at each step. At each step, the KV cache gets updated.
    Given the input to attention at step $t$.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成阶段，模型从提示阶段存储的KV缓存开始，每一步生成一个标记。每一步中，KV缓存都会更新。给定在第$t$步对注意力的输入。
- en: 2.1 LLM Inference Memory Breakdown
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM推理内存分解
- en: 'In this section, we provide the memory consumption breakdown of LLMs. The memory
    footprint consists of three parts: model weights, KV cache, and activation buffer.
    The size of model weights depends on model configuration, such as the number of
    transformer layers and hidden size. The size of the KV cache depends on model
    configurations, sequence length, and batch size. The size of the activation buffer
    depends on parallel strategy, model configurations, and implementation. The size
    of the activation buffer is considerably smaller than the previous two. As shown
    in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time"), the size
    of the KV cache, 2.5$\times$ larger than model weights, can quickly become the
    bottleneck in memory consumption. At the same time, much research has been spent
    on extending the length of the context window. GPT-4-32K can process up to 32,768
    tokens [[15](#bib.bib15)]. Longer sequence length would make the KV cache memory
    problem even more severe.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们提供了LLMs的内存消耗细节。内存占用包括三部分：模型权重、KV缓存和激活缓冲区。模型权重的大小取决于模型配置，如变换器层数和隐藏层大小。KV缓存的大小取决于模型配置、序列长度和批处理大小。激活缓冲区的大小取决于并行策略、模型配置和实现。激活缓冲区的大小明显小于前两者。如表[1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time")所示，KV缓存的大小是模型权重的2.5$\times$，因此在内存消耗中可能迅速成为瓶颈。同时，许多研究致力于扩展上下文窗口的长度。GPT-4-32K可以处理多达32,768个tokens[[15](#bib.bib15)]。更长的序列长度会使KV缓存内存问题变得更加严重。'
- en: 'Assuming LLM generates until its maximum sequence length, we summarize the
    maximum batch size before going out of GPU memory on a box of 8 A100 80GB GPU
    in Table [2](#S2.T2 "Table 2 ‣ 2.1 LLM Inference Memory Breakdown ‣ 2 Problem
    Description and Related Work ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time"). At the GPT-3 scale with
    a maximum sequence length of 2048, batch size cannot exceed 35 without offloading.
    Small batch size limits the model inference throughput.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '假设 LLM 生成到其最大序列长度，我们在表格[2](#S2.T2 "Table 2 ‣ 2.1 LLM Inference Memory Breakdown
    ‣ 2 Problem Description and Related Work ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time")中总结了在一台配备8个A100
    80GB GPU的计算机上，GPU内存使用前的最大批处理大小。在GPT-3规模下，最大序列长度为2048，批处理大小如果不进行卸载的话不能超过35。小批处理大小限制了模型推理的吞吐量。'
- en: 'Table 2: This table summarizes the maximum batch size before hitting out of
    memory on a box of 8 A100 80GB GPU when models are deployed with its maximum sequence
    length.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表格2：此表格总结了在模型以其最大序列长度部署时，达到GPU内存限制前的最大批处理大小。
- en: '| Model | OPT-175B | LLaMA-65B | BLOOM |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | OPT-175B | LLaMA-65B | BLOOM |'
- en: '| Maximum Batch Size | 34 | 102 | 36 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 最大批处理大小 | 34 | 102 | 36 |'
- en: 2.2 Efficient Attention
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 高效注意力
- en: Computing the attention matrix necessitates a time complexity of $O(n^{2})$
    is the sequence length. As a result, a line of work has been proposed to mitigate
    the computation burden of the attention mechanism [[16](#bib.bib16), [17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)]. These approaches exploit
    low-rank or sparsification to approximate the attention output. Besides, [[21](#bib.bib21)]
    realized exact efficient attention with wall-clock speed by optimizing the number
    of memory reads and writes. However, these approaches were evaluated mostly for
    training, focused on computation complexity, and did not address the KV-Cache
    memory usage introduced by auto-regressive language models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 计算注意力矩阵的时间复杂度为 $O(n^{2})$，其中 n 为序列长度。因此，已经提出了一系列工作来减轻注意力机制的计算负担[[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)]。这些方法利用低秩或稀疏化来近似注意力输出。此外，[[21](#bib.bib21)]
    通过优化内存读写次数实现了精确高效的注意力计算。然而，这些方法主要针对训练进行了评估，重点关注计算复杂性，并未解决自回归语言模型引入的KV-Cache内存使用问题。
- en: Recently, there is active research attempting to apply quantization or pruning
    in LLM  [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14)]. However, they mostly focus on reducing the
    size of model weights. Flexgen [[7](#bib.bib7)] applies quantization and sparsification
    to the KV cache; however, the memory of the KV cache is not reduced regarding
    sequence lengths. It stores the quantized KV cache for all tokens in CPU memory
    and loads all attention keys from CPU memory to compute attention scores.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，有活跃的研究尝试在大型语言模型中应用量化或剪枝[[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)]。然而，它们主要集中在减少模型权重的大小上。Flexgen
    [[7](#bib.bib7)] 将量化和稀疏化应用于KV缓存；然而，KV缓存的内存没有根据序列长度减少。它将所有标记的量化KV缓存存储在CPU内存中，并从CPU内存中加载所有注意力键来计算注意力分数。
- en: 3 The Persistence of Importance Hypothesis
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 重要性持久性假设
- en: 'We first present one interesting observation upon which the persistence of
    importance hypothesis is derived in Section [3.1](#S3.SS1 "3.1 Repetitive Attention
    Pattern. ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands: Exploiting
    the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test
    Time"). In Section [3.2](#S3.SS2 "3.2 The Persistence of Importance Hypothesis
    ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time"), we discuss
    the hypothesis in detail with empirical verification. Then, in Section [3.3](#S3.SS3
    "3.3 Attention Weights Decides the Pivotal Tokens ‣ 3 The Persistence of Importance
    Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time"), we provide theoretical intuition
    on the reason behind such model behaviors.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先展示了一个有趣的观察结果，该结果是第[3.1](#S3.SS1 "3.1 重复注意力模式. ‣ 3 重要性持久性假设 ‣ 剪刀手：在测试时利用重要性持久性假设进行LLM
    KV缓存压缩")节中推导出重要性持久性假设的基础。在第[3.2](#S3.SS2 "3.2 重要性持久性假设 ‣ 3 重要性持久性假设 ‣ 剪刀手：在测试时利用重要性持久性假设进行LLM
    KV缓存压缩")节中，我们详细讨论了该假设，并进行了实证验证。然后，在第[3.3](#S3.SS3 "3.3 注意力权重决定关键标记 ‣ 3 重要性持久性假设
    ‣ 剪刀手：在测试时利用重要性持久性假设进行LLM KV缓存压缩")节中，我们提供了对这种模型行为背后原因的理论直觉。
- en: 3.1 Repetitive Attention Pattern.
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 重复注意力模式。
- en: Observation. We are interested in the attention score from the position $t$
    indicates an averaging mixing score. High attention scores are marked with dark
    green.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 观察。我们对从位置$t$指示的注意力分数感兴趣，这表示平均混合分数。高注意力分数标记为深绿色。
- en: 'Result. High attention scores are observed at the same set of tokens from various
    positions in the sentence. In all three plots, we see dark green at sequence positions
    27, 63, 98, 121, 152, and 177, suggesting that these tokens received high attention
    at all three positions. We observe similar model behavior at different transformer
    layers with different text inputs. More plots are in Appendix [A](#A1 "Appendix
    A More Observation Plots ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time").'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。从句子中的不同位置观察到相同标记的高注意力分数。在所有三个图中，我们看到在序列位置27、63、98、121、152和177处有深绿色，表明这些标记在这三个位置都受到高度关注。我们在不同的变换器层上观察到类似的模型行为，输入的文本不同。更多的图表见附录[A](#A1
    "附录 A 更多观察图 ‣ 剪刀手：在测试时利用重要性持久性假设进行LLM KV缓存压缩")。
- en: 'Implication. Even though small differences exist, repetitive attention patterns
    are evident in the attention maps. There exist specific tokens that keep receiving
    high attention. Meanwhile, these attention maps show sparsity: only a few tokens
    have high attention scores.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 含义。尽管存在小的差异，但注意力图中明显可见重复的注意力模式。存在一些特定的标记不断受到高度关注。同时，这些注意力图显示出稀疏性：只有少数几个标记具有较高的注意力分数。
- en: 3.2 The Persistence of Importance Hypothesis
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 重要性持久性假设
- en: The repetitive attention pattern suggests that specific tokens are influential
    throughout the sequence. A stricter claim is that these tokens are the only ones
    that could be significant for a future step. Thus, we articulate the *persistence
    of importance hypothesis*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 重复的注意力模式表明，特定的标记在整个序列中具有影响力。更严格的说法是，这些标记是未来步骤中唯一可能重要的标记。因此，我们阐述了*重要性持久性假设*。
- en: The Persistence of Importance Hypothesis. *With a trained autoregressive language
    model, only pivotal tokens, which had a substantial influence at one previous
    step, will have a significant influence at a future step.*
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 重要性持久性假设。*使用经过训练的自回归语言模型，只有在之前步骤中具有显著影响的关键令牌，在未来步骤中才会有显著影响。*
- en: If true, this hypothesis indicates the possibility of foreseeing what information
    in the previous sequences could be vital for future steps. This hypothesis is
    trivial when pivotal tokens include all tokens in the entire sentences. However,
    a much more interesting case is when pivotal tokens are a subset of previous words.
    This would enable us to reduce the size of the KV cache by throwing away the embedding
    of non-important tokens.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果假设为真，这表明可以预测前序列中的哪些信息可能对未来步骤至关重要。当关键令牌包括整个句子中的所有令牌时，这一假设是显而易见的。然而，更有趣的情况是当关键令牌是先前单词的一个子集时。这将使我们能够通过丢弃不重要的令牌的嵌入来减少KV缓存的大小。
- en: '![Refer to caption](img/d6bd22fa047f7060f64d8f84bb9a2b2a.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d6bd22fa047f7060f64d8f84bb9a2b2a.png)'
- en: (a) Persistence Ratio
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 持久性比率
- en: '![Refer to caption](img/8970ae54d855e6b06c059bd3c2b76585.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8970ae54d855e6b06c059bd3c2b76585.png)'
- en: (b) Size of $S_{0\rightarrow t}$
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: (b) $S_{0\rightarrow t}$ 的大小
- en: 'Figure 2: In this figure, we plot the persistence ratio and the corresponding
    size of the pivotal token set. The persistence ratio is over 95% in most of the
    layers, with decreases at the later layers. Meanwhile, the number of pivotal tokens
    is considerably smaller than the sequence length. This suggests that the pivotal
    tokens of later half sentences are almost all included in the set of first halves.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在这个图中，我们绘制了持久性比率和相应的关键令牌集的大小。大多数层中的持久性比率超过95%，在后续层中有所下降。同时，关键令牌的数量明显小于序列长度。这表明后半句的关键令牌几乎都包含在前半句的集合中。
- en: Pivotal Token. One natural indication of a token’s influence is the attention
    score. We consider a token pivotal for position $t$.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 关键令牌。一个自然的令牌影响指示是注意力得分。我们将令牌视为位置 $t$ 的关键。
- en: '|  | $S_{a\rightarrow b}=\cup^{t=b}_{t=a}S_{t}$ |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $S_{a\rightarrow b}=\cup^{t=b}_{t=a}S_{t}$ |  |'
- en: Verification. We measure *persistence ratio* as an empirical test the hypothesis.
    *Persistence ratio* measures how many tokens in the pivotal token sets of the
    later part of the sentence are also in the pivotal token sets of the initial part
    of the sentence. Let $l$. Formally,
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 验证。我们测量*持久性比率*作为对假设的实证测试。*持久性比率*衡量句子后半部分的关键令牌集中的多少个令牌也出现在句子前半部分的关键令牌集中。设 $l$。形式上，
- en: '|  | $1$2 |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: At the same time, we measure $\frac{|S_{0\rightarrow t}|}{t}$, which suggests
    an average score.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们测量 $\frac{|S_{0\rightarrow t}|}{t}$，这表明一个平均分数。
- en: 'Result. We present our main results in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 The
    Persistence of Importance Hypothesis ‣ 3 The Persistence of Importance Hypothesis
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time"). First, given the current criterion of pivotal
    token and $t$ is considerably smaller than half of the sentence length. This verifies
    that we are not considering the trivial case of our hypothesis. Second, the persistence
    ratio is generally over 95%, with dips in the later transformer layers. The pivotal
    token set of the later half sentences is mostly included in the set of the first
    half sentences. Combining these two pieces of empirical evidence, we see positive
    evidence for our hypothesis test.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '结果。我们在图[2](#S3.F2 "Figure 2 ‣ 3.2 The Persistence of Importance Hypothesis
    ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time")中展示了我们的主要结果。首先，鉴于当前的关键令牌标准和
    $t$ 远小于句子长度的一半。这验证了我们并没有考虑假设的显而易见情况。其次，持久性比率通常超过95%，在后续变换器层中有所下降。后半句的关键令牌集大多数包含在前半句的集合中。结合这两项实证证据，我们看到了支持我们假设测试的积极证据。'
- en: Implication. The hypothesis provides insights for understanding the behavior
    of LLMs and opens up new opportunities for reducing the KV cache memory. The hypothesis
    suggests the possibility of predicting the potentially influential tokens for
    future steps. The non-influential tokens are unnecessary to store in the memory,
    as they are unlikely to have high attention scores. This reduces the number of
    tokens stored in the KV cache and the computation required at the attention.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 含义。这个假设为理解大语言模型（LLMs）的行为提供了洞见，并开辟了减少KV缓存内存的新机会。该假设提出了预测未来步骤中可能具有影响力的令牌的可能性。那些不具有影响力的令牌不需要存储在内存中，因为它们不太可能具有高的注意力分数。这减少了存储在KV缓存中的令牌数量以及注意力计算所需的计算量。
- en: 3.3 Attention Weights Decides the Pivotal Tokens
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 注意力权重决定关键令牌
- en: 'In the previous section, we verified that the significant tokens would continue
    to be significant. In this section, we try to understand the reasons for such
    phenomena. We consider the token generation process of a simplified model: a single-layer
    transformer model with single-head attention.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们验证了重要令牌将继续保持重要。在本节中，我们尝试理解这种现象的原因。我们考虑一个简化模型的令牌生成过程：一个具有单头注意力的单层变换器模型。
- en: '|  | $1$2 |  | (1) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: $x_{t}\in\mathbb{R}^{1\times d}$ denotes the MLP block following attention block,
    a two-layer MLP with skip connections, given by
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: $x_{t}\in\mathbb{R}^{1\times d}$ 表示跟随注意力块的MLP块，一个具有跳跃连接的两层MLP，如下给出
- en: '|  | $\mathcal{F}(x)=x+W_{2}\texttt{relu}(W_{1}x)$ |  | (2) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{F}(x)=x+W_{2}\texttt{relu}(W_{1}x)$ |  | (2) |'
- en: We are interested in the attention scores $\alpha_{t}=\texttt{softmax}(\nicefrac{{1}}{{t}}\cdot
    x_{t}W_{Q}W_{K}^{\top}X_{t-1}^{\top})$
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对注意力分数 $\alpha_{t}=\texttt{softmax}(\nicefrac{{1}}{{t}}\cdot x_{t}W_{Q}W_{K}^{\top}X_{t-1}^{\top})$
    感兴趣
- en: Theorem 3.1.
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 3.1。
- en: Let $A=W_{V}W_{O}W_{Q}W_{K}^{\top}$, it holds that
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $A=W_{V}W_{O}W_{Q}W_{K}^{\top}$，则有
- en: '|  | $1$2 |  | (3) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: 'The proof is provided in Appendix [B](#A2 "Appendix B Proofs ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time"). Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.3 Attention Weights
    Decides the Pivotal Tokens ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time") shows that under an assumption on the MLP in ([2](#S3.E2 "In 3.3
    Attention Weights Decides the Pivotal Tokens ‣ 3 The Persistence of Importance
    Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time")), for all $x_{\ell}$.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '证明见附录 [B](#A2 "附录 B 证明 ‣ Scissorhands: 利用重要性假设的持久性进行LLM KV缓存压缩")。定理 [3.1](#S3.Thmtheorem1
    "定理 3.1\. ‣ 3.3 注意力权重决定关键令牌 ‣ 3 重要性假设的持久性 ‣ Scissorhands: 利用重要性假设的持久性进行LLM KV缓存压缩")
    表明，在对MLP的一个假设下 ([2](#S3.E2 "在3.3 注意力权重决定关键令牌 ‣ 3 重要性假设的持久性 ‣ Scissorhands: 利用重要性假设的持久性进行LLM
    KV缓存压缩"))，对于所有 $x_{\ell}$。'
- en: 'Our theorem shows that the property in Equation ([3](#S3.E3 "In Theorem 3.1\.
    ‣ 3.3 Attention Weights Decides the Pivotal Tokens ‣ 3 The Persistence of Importance
    Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time")) property only holds for $x_{\ell}$
    as a pivotal token. Each attention is learned to identify some subspace. Only
    those tokens embedded inside these regions are pivotal for this attention. This
    would explain why only some specific tokens are always relevant.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的定理表明，方程中的性质 ([3](#S3.E3 "在定理3.1\. ‣ 3.3 注意力权重决定关键令牌 ‣ 3 重要性假设的持久性 ‣ Scissorhands:
    利用重要性假设的持久性进行LLM KV缓存压缩")) 的性质仅对 $x_{\ell}$ 作为关键令牌成立。每个注意力机制被学习以识别某些子空间。仅那些嵌入在这些区域内的令牌对于此注意力机制是关键的。这将解释为什么只有一些特定的令牌总是相关的。'
- en: 4 Sequential Token Generation Under budget
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 在预算下的顺序令牌生成
- en: Algorithm 1 Inference with Budget KV cache
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 预算下的推断
- en: 'Input: Memory Budget $B$end while'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：内存预算 $B$end while
- en: Algorithm 2 Compress KV Cache
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 压缩 KV 缓存
- en: 'Input: Key Cache $\bar{\mathcal{K}}\in\mathbf{R}^{n\times d}$'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：键缓存 $\bar{\mathcal{K}}\in\mathbf{R}^{n\times d}$
- en: 'In this section, we present Scissorhands, which reduces the KV cache memory
    from the sequence length dimension without fine-tuning the model. In Section [4.1](#S4.SS1
    "4.1 Budget KV Cache for Single Attention Head ‣ 4 Sequential Token Generation
    Under budget ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time"), we describe how Scissorhands maintains
    the KV cache under a given budget. Section [4.2](#S4.SS2 "4.2 Theoretical Analysis.
    ‣ 4 Sequential Token Generation Under budget ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time") provides
    a theoretical analysis of the algorithm and the approximation error.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们介绍了Scissorhands，它在不对模型进行微调的情况下，从序列长度维度减少了KV缓存内存。在第[4.1节](#S4.SS1 "4.1
    Budget KV Cache for Single Attention Head ‣ 4 Sequential Token Generation Under
    budget ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for
    LLM KV Cache Compression at Test Time")中，我们描述了Scissorhands如何在给定预算下维护KV缓存。第[4.2节](#S4.SS2
    "4.2 Theoretical Analysis. ‣ 4 Sequential Token Generation Under budget ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time")提供了该算法及其近似误差的理论分析。'
- en: 4.1 Budget KV Cache for Single Attention Head
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 单一注意力头的KV缓存预算
- en: In this section, for the sake of the discussion, we drop the layer number notation
    $i$ can be written as,
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，为了讨论的方便，我们省略层编号符号 $i$ 可以写作，
- en: '|  | $1$2 |  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'Intuition. As shown in Section [3](#S3 "3 The Persistence of Importance Hypothesis
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time"), the attention scores $\alpha_{t,i}$ follow a
    strong power-law distribution. For the autoregressive generation process, if there
    exists an oracle such that we can identify the heavy score tokens before the future
    generation step, then the memory of the KV cache can be significantly reduced
    by only storing the heavy score tokens. Fortunately, the persistence of importance
    hypothesis provides us with such an oracle. It states that only historical tokens
    with significant contributions toward previous generated tokens will have significant
    contributions toward future tokens.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '直观。正如第[3节](#S3 "3 The Persistence of Importance Hypothesis ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time")所示，注意力得分$\alpha_{t,i}$遵循强幂律分布。对于自回归生成过程，如果存在一个预言者，我们可以在未来生成步骤之前识别出重得分标记，则通过仅存储重得分标记，可以显著减少KV缓存的内存。幸运的是，重要性持久性假设为我们提供了这样的预言者。它指出，只有对先前生成的标记具有显著贡献的历史标记，才会对未来的标记产生显著贡献。'
- en: Challenges. LLMs are deployed on hardware with a fixed memory. The algorithm
    should maintain the cache under fixed memory to meet the hard requirement. Further,
    LLMs are already computationally intensive. The algorithm should avoid introducing
    much extra burden on computation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战。LLM被部署在具有固定内存的硬件上。该算法应在固定内存下维护缓存，以满足硬性要求。此外，LLM本身已经计算密集。该算法应避免对计算引入过多额外负担。
- en: A fixed memory budget for one attention head is $B$ previous tokens. We describe
    the problem as follows,
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一个注意力头的固定内存预算是$B$个先前的标记。我们将问题描述如下，
- en: Definition 4.1  (Sequential generation at an attention head under budget $B$).
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义4.1（在预算$B$下的注意力头的序列生成）。
- en: Given a stream of token embedding, including prompt and previously generated
    tokens, denotes their input to the head as $\{x_{1},\ldots,x_{t},\ldots\}$.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个标记嵌入流，包括提示和先前生成的标记，记为$\{x_{1},\ldots,x_{t},\ldots\}$。
- en: 'Approach. Inspired by the textbook solution of reservoir sampling and the Least
    Recent Usage cache replacement algorithm, Scissorhands reserves a fixed memory
    buffer for the KV cache. When the buffer is full, Scissorhands drops stored but
    non-influential tokens from the cache. Naturally, attention scores are used as
    indicators of influence. We present the main algorithm in Algorithm [1](#alg1
    "Algorithm 1 ‣ 4 Sequential Token Generation Under budget ‣ Scissorhands: Exploiting
    the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test
    Time") and Algorithm [2](#alg2 "Algorithm 2 ‣ 4 Sequential Token Generation Under
    budget ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for
    LLM KV Cache Compression at Test Time"). The influence measure is collected over
    a history window to reduce variance. And recent tokens are always kept because
    of the lack of information on their importance. In practice, $w$ in all our experiments.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '方法。受到储备抽样教科书解决方案和最少最近使用缓存替换算法的启发，Scissorhands 为 KV 缓存保留了固定的内存缓冲区。当缓冲区满时，Scissorhands
    从缓存中删除存储但不重要的标记。自然地，注意力分数被用作影响的指标。我们在算法 [1](#alg1 "Algorithm 1 ‣ 4 Sequential
    Token Generation Under budget ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time") 和算法 [2](#alg2 "Algorithm
    2 ‣ 4 Sequential Token Generation Under budget ‣ Scissorhands: Exploiting the
    Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time")
    中展示了主要算法。影响度测量是通过历史窗口收集的，以减少方差。最近的标记始终被保留，因为缺乏关于其重要性的信息。在实践中，我们所有实验中的 $w$。'
- en: With a sampled KV cache, attention output can be computed by the following estimator
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用采样的 KV 缓存，注意力输出可以通过以下估算器计算
- en: '|  | $1$2 |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'Overhead Tradeoff At the compression step, an extra attention computation is
    introduced to collect the importance measurements over a history window. However,
    such compression is not required at every generation step. $m$ in our experiment.
    Further, steps after the compression have reduced attention computation because
    of the reduction in the KV cache. On the other hand, one can trade a tiny amount
    of memory to avoid the overhead by maintaining the importance record during generation
    steps in Algorithm [1](#alg1 "Algorithm 1 ‣ 4 Sequential Token Generation Under
    budget ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for
    LLM KV Cache Compression at Test Time").'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '开销权衡。在压缩步骤中，引入了额外的注意力计算，以收集历史窗口中的重要性测量。然而，这种压缩并不是在每个生成步骤中都需要的。在我们的实验中为 $m$。此外，压缩后的步骤由于
    KV 缓存的减少而减少了注意力计算。另一方面，可以通过在生成步骤中保持重要性记录来用少量内存避免开销，这在算法 [1](#alg1 "Algorithm 1
    ‣ 4 Sequential Token Generation Under budget ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time") 中有所体现。'
- en: 'Allocating Budgets Across Attention Heads. An LLM typically consists of $L$
    heads. A total memory budget has to be distributed over layers and heads. Within
    each transformer layer, the budget is distributed evenly across heads. Within
    the entire model, we distributed the budget according to Figure [2](#S3.F2 "Figure
    2 ‣ 3.2 The Persistence of Importance Hypothesis ‣ 3 The Persistence of Importance
    Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time"). The rule of thumb is to allocate
    more budget to later layers to compensate for the lower persistence ratio.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '跨注意力头分配预算。一个 LLM 通常由 $L$ 个头组成。总内存预算必须分配到层和头上。在每个变换器层中，预算在头部之间均匀分配。在整个模型中，我们根据图 [2](#S3.F2
    "Figure 2 ‣ 3.2 The Persistence of Importance Hypothesis ‣ 3 The Persistence of
    Importance Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time") 分配预算。经验法则是将更多的预算分配给后面的层，以弥补较低的持久性比率。'
- en: 4.2 Theoretical Analysis.
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 理论分析。
- en: 'We study how much the tokens generated by the compressed KV cache deviate from
    the tokens generated by the original transformer using our simplified model in
    ([1](#S3.E1 "In 3.3 Attention Weights Decides the Pivotal Tokens ‣ 3 The Persistence
    of Importance Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time")). Let $\{\tilde{x}_{t}\}_{t=0}^{T}$:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '我们研究了压缩的 KV 缓存生成的标记与原始变换器生成的标记之间的偏差，使用我们简化的模型在 ([1](#S3.E1 "In 3.3 Attention
    Weights Decides the Pivotal Tokens ‣ 3 The Persistence of Importance Hypothesis
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time")) 中。设 $\{\tilde{x}_{t}\}_{t=0}^{T}$：'
- en: '|  | $1$2 |  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'Notice that when $m=1$ tokens. If the ranking of the attention scores does
    not change in each iteration, Algorithm [2](#alg2 "Algorithm 2 ‣ 4 Sequential
    Token Generation Under budget ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time") will always drop tokens
    with the smallest attention scores.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当 $m=1$ 时的 tokens。如果每次迭代中注意力分数的排名不变，算法 [2](#alg2 "算法 2 ‣ 4 序列令牌生成在预算下 ‣ Scissorhands：利用重要性假设在测试时进行
    LLM KV 缓存压缩") 将始终丢弃注意力分数最小的 tokens。
- en: For reference purposes, let $\{x_{t}\}_{t=0}^{T}$.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 供参考，设 $\{x_{t}\}_{t=0}^{T}$。
- en: Theorem 4.1.
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 4.1。
- en: 'Let $\lambda_{1},\lambda_{2}$ in ([2](#S3.E2 "In 3.3 Attention Weights Decides
    the Pivotal Tokens ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time")). Let'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\lambda_{1},\lambda_{2}$ 在 ([2](#S3.E2 "在 3.3 注意力权重决定关键令牌 ‣ 3 重要性假设的持久性 ‣
    Scissorhands：利用重要性假设在测试时进行 LLM KV 缓存压缩")) 中。设
- en: '|  | $1$2 |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: and assume that each $\beta_{t,j}=cv_{t,j}$
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 并假设每个 $\beta_{t,j}=cv_{t,j}$
- en: '|  | $1$2 |  | (4) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: 'The definition of $\beta_{t,j}$ depends on the distribution that the attention
    scores are fitted to and is always less than one. With a strong power-law distribution,
    this term provides a further decrease to the error bound in ([4](#S4.E4 "In Theorem
    4.1\. ‣ 4.2 Theoretical Analysis. ‣ 4 Sequential Token Generation Under budget
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time")).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: $\beta_{t,j}$ 的定义取决于注意力分数拟合的分布，并且总是小于 1。在强幂律分布下，该项进一步减少了在 ([4](#S4.E4 "定理 4.1.
    ‣ 4.2 理论分析。 ‣ 4 序列令牌生成在预算下 ‣ Scissorhands：利用重要性假设在测试时进行 LLM KV 缓存压缩")) 中的误差界限。
- en: '![Refer to caption](img/5c767dd741c32213f1e9c92639ec5c03.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5c767dd741c32213f1e9c92639ec5c03.png)'
- en: (a) Language Modeling
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 语言建模
- en: '![Refer to caption](img/5283f95c7ca3e73190da32ae7a53896d.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5283f95c7ca3e73190da32ae7a53896d.png)'
- en: (b) OPT-6B Five shot
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: (b) OPT-6B 五次实验
- en: '![Refer to caption](img/a0f96821cfd748671e212dd25f52a02b.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a0f96821cfd748671e212dd25f52a02b.png)'
- en: (c) OPT-13B Five shot
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: (c) OPT-13B 五次实验
- en: '![Refer to caption](img/2ebccccf90225bccac2ae15344e628e0.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2ebccccf90225bccac2ae15344e628e0.png)'
- en: (d) OPT-30B Five shot
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: (d) OPT-30B 五次实验
- en: 'Figure 3: This figure shows the accuracy trend of Scissorhands on language
    modeling dataset and downstream tasks with different KV cache compression. In
    general, Scissorhands incurs no accuracy drop until $5\times$ compression on OPT-66B.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：该图展示了 Scissorhands 在语言建模数据集和不同 KV 缓存压缩下游任务上的准确性趋势。一般来说，Scissorhands 在 OPT-66B
    上的压缩达到 $5\times$ 时无准确性下降。
- en: 5 Empirical Evaluation
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实证评估
- en: In this section, we present the results that demonstrate Scissorhands achieves
    up to 5$\times$ reduction in the KV cache memory compared to the standard model
    with no accuracy loss. We also show that Scissorhands is compatible with 4-bit
    quantization.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了 Scissorhands 实现了相比于标准模型的 KV 缓存内存高达 5$\times$ 的减少，且没有准确性损失。我们还展示了
    Scissorhands 兼容 4 位量化。
- en: 'Experiment Setting. We compare the accuracy of Scissorhands-OPT against the
    original OPT on one language model datasets C4 [[22](#bib.bib22)] and a number
    of few-shot downstream tasks: Hellaswag [[26](#bib.bib26)], MathQA [[27](#bib.bib27)],
    PIQA [[28](#bib.bib28)], Winogrande [[29](#bib.bib29)]. We use lm-eval-harness [[30](#bib.bib30)]
    to evaluate few-shot tasks. Our experiments are conducted on NVIDIA 4 A100 40GB
    GPU servers.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置。我们在一个语言模型数据集 C4 [[22](#bib.bib22)] 和若干个少量样本下游任务上比较了 Scissorhands-OPT 与原始
    OPT 的准确性：Hellaswag [[26](#bib.bib26)], MathQA [[27](#bib.bib27)], PIQA [[28](#bib.bib28)],
    Winogrande [[29](#bib.bib29)]。我们使用 lm-eval-harness [[30](#bib.bib30)] 来评估少量样本任务。我们的实验在
    NVIDIA 4 A100 40GB GPU 服务器上进行。
- en: No Accuracy Drop untill 5$\times$ compression for OPT-66B. Similar to the language
    modeling setting, Scissorhands performs better at larger models. Generally, accuracy
    is maintained with 15% - 30% of the original KV cache size.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对 OPT-66B 进行高达 5$\times$ 压缩无准确性下降。类似于语言建模设置，Scissorhands 在较大的模型上表现更好。一般来说，准确性在原始
    KV 缓存大小的 15% - 30% 之间保持不变。
- en: 'Table 3: Applying 4-bit quantization on top of Scissorhands on Hellaswag.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在 Hellaswag 上应用 4 位量化到 Scissorhands。
- en: '| OPT-6B |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6B |'
- en: '| Original | Scissorhands | Scissorhands+ 4-bit |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | Scissorhands | Scissorhands+ 4-bit |'
- en: '| 0.702 | 0.706 | 0.704 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 0.702 | 0.706 | 0.704 |'
- en: '| OPT-13B |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13B |'
- en: '| Original | Scissorhands | Scissorhands+ 4-bit |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | Scissorhands | Scissorhands+ 4-bit |'
- en: '| 0.720 | 0.720 | 0.720 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 0.720 | 0.720 | 0.720 |'
- en: 'Compatible with 4-bit Quantization We test the compatibility of quantization
    and Scissorhands at $2\times$ compression. We adopt 4-bit quantization following [[7](#bib.bib7)].
    Even Hellaswag is most sensitive based on Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Theoretical
    Analysis. ‣ 4 Sequential Token Generation Under budget ‣ Scissorhands: Exploiting
    the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test
    Time"), adding quantization doesn’t introduce compounded errors.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '兼容4位量化 我们在$2\times$压缩下测试了量化与Scissorhands的兼容性。我们采用了[[7](#bib.bib7)]的4位量化方法。即便根据图Figure
    [3](#S4.F3 "Figure 3 ‣ 4.2 Theoretical Analysis. ‣ 4 Sequential Token Generation
    Under budget ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time")，Hellaswag最为敏感，添加量化也不会引入额外的错误。'
- en: '![Refer to caption](img/07932f92ed79bc7ef99d49298b56e348.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/07932f92ed79bc7ef99d49298b56e348.png)'
- en: 'Figure 4: Score between OPT and Scissorhands.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：OPT与Scissorhands之间的得分。
- en: Ablation on Attention Score Error. We present the change ratio in attention
    score between original OPT-13B and Scissorhands OPT-13B at $3\times$. Thus, Scissorhands
    gives the most similar output as the original model at all layers.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力得分误差的消融实验。我们展示了在$3\times$下，原始OPT-13B和Scissorhands OPT-13B之间注意力得分的变化比率。因此，Scissorhands在所有层次上给出了与原始模型最相似的输出。
- en: 6 Discussion, Limitation, and Future Work
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论、局限性和未来工作
- en: 'We discover repetitive attention patterns given trained language models. One
    interesting question that needs to be answered is whether such behavior is a model
    architecture bias or an unexpected training outcome. For such purpose, we perform
    the same experiment with a randomly initialized OPT, and compare it against the
    results presented in Section [3.1](#S3.SS1 "3.1 Repetitive Attention Pattern.
    ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time"). As shown
    in Figure [5](#S6.F5 "Figure 5 ‣ 6 Discussion, Limitation, and Future Work ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time"), the repetitive attention pattern does not exist in randomly initialized
    models. Apart from an efficiency deployment perspective, could such repetitive
    attention patterns contribute to some known problem in language generation such
    as repetitions? It may be worth investigating the relationship between repetitive
    attention patterns and undesired generations.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '我们发现了训练语言模型中重复的注意力模式。一个有趣的问题是，这种行为是否是模型架构偏差还是意外的训练结果。为此，我们用随机初始化的OPT进行了相同的实验，并将其与第[3.1](#S3.SS1
    "3.1 Repetitive Attention Pattern. ‣ 3 The Persistence of Importance Hypothesis
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time")节中呈现的结果进行了比较。如图Figure [5](#S6.F5 "Figure 5 ‣ 6
    Discussion, Limitation, and Future Work ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time")所示，随机初始化的模型中不存在重复的注意力模式。除了效率部署的角度外，这种重复的注意力模式是否可能对语言生成中的某些已知问题如重复有所贡献？可能值得研究重复注意力模式与不期望生成之间的关系。'
- en: '![Refer to caption](img/3c840df93a7f7410b75e4de05b7126c2.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/3c840df93a7f7410b75e4de05b7126c2.png)'
- en: (a) Attention map of the token at position 178
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 位置178的token注意力图
- en: '![Refer to caption](img/ecc78c7433806556bc0eec60e18e5a42.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ecc78c7433806556bc0eec60e18e5a42.png)'
- en: (b) Attention map of the token at position 228
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 位置228的token注意力图
- en: 'Figure 5: We plot the attention map corresponding to Section [3.1](#S3.SS1
    "3.1 Repetitive Attention Pattern. ‣ 3 The Persistence of Importance Hypothesis
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time") but with a randomly initialized OPT. We observe
    no repetitive attention for a randomly initialized model.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '图5：我们绘制了与第[3.1](#S3.SS1 "3.1 Repetitive Attention Pattern. ‣ 3 The Persistence
    of Importance Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time")节对应的注意力图，但使用了随机初始化的OPT。我们观察到随机初始化的模型中没有重复的注意力。'
- en: Due to the limitation of the server in academics, the largest model we can fit
    is OPT-66B. We try to understand the behavior and verify the generality across
    the different models and datasets. However, we cannot access the training process
    and fail to know exactly how an LLM is trained to exhibit such behavior. Experiments
    with the large model create carbon dioxide emissions. However, our work improves
    the efficiency of LLM, and we foresee no negative impacts.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于学术界服务器的限制，我们能使用的最大模型是 OPT-66B。我们尝试理解其行为并验证不同模型和数据集上的普遍性。然而，我们无法访问训练过程，因此无法确切了解
    LLM 是如何训练以表现出这种行为的。使用大型模型的实验会产生二氧化碳排放。然而，我们的工作提高了 LLM 的效率，我们预见不会有负面影响。
- en: 7 Conclusion
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: Inspired by our intriguing findings that pivotal tokens exert a lasting influence
    on future steps, we developed Scissorhands to leverage this observation to reduce
    the memory usage of KV cache. Our method achieves memory reductions of $5\times$
    in the KV cache without compromising the performance of LLMs. Furthermore, we
    demonstrate the compatibility of Scissorhands with quantization techniques, opening
    up the possibility of reducing memory usage in both the representation and sequence
    length dimensions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 受到我们有趣发现的启发，即关键令牌对未来步骤产生持久影响，我们开发了 Scissorhands 来利用这一观察，减少 KV 缓存的内存使用。我们的方法在不影响
    LLM 性能的情况下，实现了 KV 缓存内存减少 $5\times$。此外，我们展示了 Scissorhands 与量化技术的兼容性，这为在表示和序列长度维度上减少内存使用开辟了可能性。
- en: References
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,
    Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill,
    et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258,
    2021.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,
    Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill,
    等人。关于基础模型的机会与风险。arXiv 预印本 arXiv:2108.07258, 2021年。'
- en: '[2] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
    Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.
    Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
    Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, 等人。语言模型的整体评估。arXiv
    预印本 arXiv:2211.09110, 2022年。'
- en: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    等人。语言模型是少量样本学习者。发表于《神经信息处理系统进展》，33:1877–1901, 2020年。'
- en: '[4] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
    Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What
    makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
    Hajishirzi, and Luke Zettlemoyer. 重新思考演示的作用：是什么让上下文学习有效？arXiv 预印本 arXiv:2202.12837,
    2022年。'
- en: '[5] Stephanie CY Chan, Adam Santoro, Andrew Kyle Lampinen, Jane X Wang, Aaditya K
    Singh, Pierre Harvey Richemond, James McClelland, and Felix Hill. Data distributional
    properties drive emergent in-context learning in transformers. In Advances in
    Neural Information Processing Systems, 2022.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Stephanie CY Chan, Adam Santoro, Andrew Kyle Lampinen, Jane X Wang, Aaditya
    K Singh, Pierre Harvey Richemond, James McClelland, and Felix Hill. 数据分布属性驱动了变换器中的新兴上下文学习。发表于《神经信息处理系统进展》，2022年。'
- en: '[6] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,
    Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently
    scaling transformer inference, 2022.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,
    Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 高效扩展变换器推理，2022年。'
- en: '[7] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y.
    Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang,
    Christopher Ré, Ion Stoica, and Ce Zhang. High-throughput generative inference
    of large language models with a single gpu, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel
    Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang,
    Christopher Ré, Ion Stoica, and Ce Zhang. 单个 GPU 上的大型语言模型的高吞吐量生成推理，2023年。'
- en: '[8] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, 和 Yuxiong He. Zeroquant: 对大规模变换器进行高效且经济的后训练量化。arXiv 预印本 arXiv:2206.01861,
    2022。'
- en: '[9] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee,
    and Dongsoo Lee. nuqmm: Quantized matmul for efficient inference of large-scale
    generative language models. arXiv preprint arXiv:2206.09557, 2022.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee,
    和 Dongsoo Lee. nuqmm: 高效推理大规模生成语言模型的量化矩阵乘法。arXiv 预印本 arXiv:2206.09557, 2022。'
- en: '[10] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8
    (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339,
    2022.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer. Llm. int8
    (): 用于大规模变换器的8位矩阵乘法。arXiv 预印本 arXiv:2208.07339, 2022。'
- en: '[11] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh. Gptq:
    生成预训练变换器的精确后训练量化。arXiv 预印本 arXiv:2210.17323, 2022。'
- en: '[12] Elias Frantar and Dan Alistarh. Massive language models can be accurately
    pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Elias Frantar 和 Dan Alistarh. 大型语言模型可以在一次性训练中精确修剪。arXiv 预印本 arXiv:2301.00774,
    2023。'
- en: '[13] Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati,
    Katrin Kirchhoff, and Dan Roth. Rethinking the role of scale for in-context learning:
    An interpretability-based case study at 66 billion scale. arXiv preprint arXiv:2212.09095,
    2022.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati,
    Katrin Kirchhoff, 和 Dan Roth. 重新审视规模在上下文学习中的作用: 一个基于可解释性的66亿规模案例研究。arXiv 预印本 arXiv:2212.09095,
    2022。'
- en: '[14] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han.
    Smoothquant: Accurate and efficient post-training quantization for large language
    models. arXiv preprint arXiv:2211.10438, 2022.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, 和 Song Han. Smoothquant:
    对大型语言模型进行精确且高效的后训练量化。arXiv 预印本 arXiv:2211.10438, 2022。'
- en: '[15] OpenAI. Gpt-4 technical report, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] OpenAI. Gpt-4 技术报告, 2023。'
- en: '[16] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient
    transformer. In 8th International Conference on Learning Representations, ICLR
    2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Nikita Kitaev, Lukasz Kaiser, 和 Anselm Levskaya. Reformer: 高效的变换器。在第八届国际学习表示大会，ICLR
    2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26-30日。OpenReview.net, 2020。'
- en: '[17] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer:
    Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, 和 Hao Ma. Linformer:
    具有线性复杂度的自注意力机制。arXiv 预印本 arXiv:2006.04768, 2020。'
- en: '[18] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li,
    Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re. Mongoose: A learnable
    lsh framework for efficient neural network training. In International Conference
    on Learning Representations, 2021.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li,
    Tri Dao, Zhao Song, Anshumali Shrivastava, 和 Christopher Re. Mongoose: 一个可学习的
    lsh 框架，用于高效的神经网络训练。在国际学习表示大会，2021年。'
- en: '[19] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher
    Ré. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information
    Processing Systems, 34:17413–17426, 2021.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, 和 Christopher
    Ré. Scatterbrain: 统一稀疏和低秩注意力。神经信息处理系统进展, 34:17413–17426, 2021。'
- en: '[20] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou
    Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,
    Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking
    attention with performers. In 9th International Conference on Learning Representations,
    ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou
    Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,
    Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, 和 Adrian Weller. 重新思考使用
    Performers 的注意力机制。在第九届国际学习表示大会，ICLR 2021，虚拟活动，奥地利，2021年5月3-7日。OpenReview.net,
    2021。'
- en: '[21] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.
    Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, 和 Christopher Ré. Flashattention:
    快速且内存高效的精确注意力机制。2022年。'
- en: '[22] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of
    transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J. Liu。探索通过统一的文本到文本转换器的迁移学习极限。arXiv 电子打印，2019年。'
- en: '[23] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,
    Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali
    Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer
    language models, 2022.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Susan Zhang、Stephen Roller、Naman Goyal、Mikel Artetxe、Moya Chen、Shuohui
    Chen、Christopher Dewan、Mona Diab、Xian Li、Xi Victoria Lin、Todor Mihaylov、Myle Ott、Sam
    Shleifer、Kurt Shuster、Daniel Simig、Punit Singh Koura、Anjali Sridhar、Tianlu Wang
    和 Luke Zettlemoyer。OPT：开放的预训练变换器语言模型，2022年。'
- en: '[24] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a
    suit of armor conduct electricity? a new dataset for open book question answering.
    In EMNLP, 2018.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Todor Mihaylov、Peter Clark、Tushar Khot 和 Ashish Sabharwal。盔甲能导电吗？一个用于开放书籍问答的新数据集。发表于
    EMNLP，2018年。'
- en: '[25] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models, 2016.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Stephen Merity、Caiming Xiong、James Bradbury 和 Richard Socher。指针守卫混合模型，2016年。'
- en: '[26] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th
    Annual Meeting of the Association for Computational Linguistics, 2019.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Rowan Zellers、Ari Holtzman、Yonatan Bisk、Ali Farhadi 和 Yejin Choi。Hellaswag：机器真的能完成你的句子吗？发表于第57届计算语言学协会年会论文集，2019年。'
- en: '[27] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
    Sutskever, et al. Language models are unsupervised multitask learners. OpenAI
    blog, 1(8):9, 2019.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Alec Radford、Jeffrey Wu、Rewon Child、David Luan、Dario Amodei、Ilya Sutskever
    等。语言模型是无监督的多任务学习者。OpenAI 博客，1(8):9，2019年。'
- en: '[28] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi.
    Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth
    AAAI Conference on Artificial Intelligence, 2020.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Yonatan Bisk、Rowan Zellers、Ronan Le Bras、Jianfeng Gao 和 Yejin Choi。Piqa：在自然语言中推理物理常识。发表于第34届
    AAAI 人工智能大会，2020年。'
- en: '[29] Sakaguchi Keisuke, Le Bras Ronan, Bhagavatula Chandra, and Choi Yejin.
    Winogrande: An adversarial winograd schema challenge at scale. In Communications
    of the ACM, 2019.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Sakaguchi Keisuke、Le Bras Ronan、Bhagavatula Chandra 和 Choi Yejin。Winogrande：大规模的对抗性
    Winograd 方案挑战。发表于《ACM 通讯》，2019年。'
- en: '[30] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
    Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason
    Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy
    Zou. A framework for few-shot language model evaluation. In Version v0\. 0.1\.
    Sept. Zenodo, September 2021.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Leo Gao、Jonathan Tow、Stella Biderman、Sid Black、Anthony DiPofi、Charles
    Foster、Laurence Golding、Jeffrey Hsu、Kyle McDonell、Niklas Muennighoff、Jason Phang、Laria
    Reynolds、Eric Tang、Anish Thite、Ben Wang、Kevin Wang 和 Andy Zou。一个用于少样本语言模型评估的框架。Zenodo，第
    v0\. 0.1 版，2021年9月。'
- en: Appendix
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A More Observation Plots
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 更多观察图
- en: A.1 Repetitive Attention Pattern
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 重复注意力模式
- en: 'We provide the attention map similar to Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time") but from a different transformer layer on the
    same text in Figure [6](#A1.F6 "Figure 6 ‣ A.1 Repetitive Attention Pattern ‣
    Appendix A More Observation Plots ‣ Scissorhands: Exploiting the Persistence of
    Importance Hypothesis for LLM KV Cache Compression at Test Time"), Figure [7](#A1.F7
    "Figure 7 ‣ A.1 Repetitive Attention Pattern ‣ Appendix A More Observation Plots
    ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV
    Cache Compression at Test Time"), Figure [8](#A1.F8 "Figure 8 ‣ A.1 Repetitive
    Attention Pattern ‣ Appendix A More Observation Plots ‣ Scissorhands: Exploiting
    the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test
    Time") and Figure [9](#A1.F9 "Figure 9 ‣ A.1 Repetitive Attention Pattern ‣ Appendix
    A More Observation Plots ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time"). A repetitive pattern and
    attention sparsity can be observed across layers.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了类似于图[1](#S1.F1 "图 1 ‣ 1 引言 ‣ Scissorhands：利用重要性假设在测试时进行 LLM KV 缓存压缩")的注意力图，但来自不同的
    transformer 层，文本与图[6](#A1.F6 "图 6 ‣ A.1 重复的注意力模式 ‣ 附录 A 更多观察图 ‣ Scissorhands：利用重要性假设在测试时进行
    LLM KV 缓存压缩")、图[7](#A1.F7 "图 7 ‣ A.1 重复的注意力模式 ‣ 附录 A 更多观察图 ‣ Scissorhands：利用重要性假设在测试时进行
    LLM KV 缓存压缩")、图[8](#A1.F8 "图 8 ‣ A.1 重复的注意力模式 ‣ 附录 A 更多观察图 ‣ Scissorhands：利用重要性假设在测试时进行
    LLM KV 缓存压缩")和图[9](#A1.F9 "图 9 ‣ A.1 重复的注意力模式 ‣ 附录 A 更多观察图 ‣ Scissorhands：利用重要性假设在测试时进行
    LLM KV 缓存压缩")中相同。可以观察到跨层的重复模式和注意力稀疏性。
- en: '![Refer to caption](img/a398efb25a424f20aa2202cd17e300ed.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/a398efb25a424f20aa2202cd17e300ed.png)'
- en: (a) Attention map at position 178
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 位置 178 的注意力图
- en: '![Refer to caption](img/5e617d2f657862d153e2cccf2badacff.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/5e617d2f657862d153e2cccf2badacff.png)'
- en: (b) Attention map at position 228
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 位置 228 的注意力图
- en: '![Refer to caption](img/c3d0bcabd8edb13b8ccf3b18aebb8b83.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/c3d0bcabd8edb13b8ccf3b18aebb8b83.png)'
- en: (c) Attention map at position 278
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 位置 278 的注意力图
- en: 'Figure 6: Attention Map at Layer 5'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：第 5 层的注意力图
- en: '![Refer to caption](img/5e46d5afc656fd0ab919b02eb706cb26.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/5e46d5afc656fd0ab919b02eb706cb26.png)'
- en: (a) Attention map at position 178
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 位置 178 的注意力图
- en: '![Refer to caption](img/d55f138e2d7a6b096da565582c22de39.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/d55f138e2d7a6b096da565582c22de39.png)'
- en: (b) Attention map at position 228
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 位置 228 的注意力图
- en: '![Refer to caption](img/83c0081ce52cbf3a8b944c722da63f75.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/83c0081ce52cbf3a8b944c722da63f75.png)'
- en: (c) Attention map at position 278
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 位置 278 的注意力图
- en: 'Figure 7: Attention Map at Layer 10'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：第 10 层的注意力图
- en: '![Refer to caption](img/95ca115856694b0972de01ba95d02373.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/95ca115856694b0972de01ba95d02373.png)'
- en: (a) Attention map at position 178
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 位置 178 的注意力图
- en: '![Refer to caption](img/57a3a2d7d686ab9061893b86ebeead42.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/57a3a2d7d686ab9061893b86ebeead42.png)'
- en: (b) Attention map at position 228
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 位置 228 的注意力图
- en: '![Refer to caption](img/f9bad591e2641c922f20a046fd398958.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/f9bad591e2641c922f20a046fd398958.png)'
- en: (c) Attention map at position 278
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 位置 278 的注意力图
- en: 'Figure 8: Attention Map at Layer 15'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：第 15 层的注意力图
- en: '![Refer to caption](img/6bba20c9bd16369ae2e5b6bd4ffec9ff.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/6bba20c9bd16369ae2e5b6bd4ffec9ff.png)'
- en: (a) Attention map at position 178
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 位置 178 的注意力图
- en: '![Refer to caption](img/1e9e3f44e495498ebe49e971cc6eef46.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/1e9e3f44e495498ebe49e971cc6eef46.png)'
- en: (b) Attention map at position 228
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 位置 228 的注意力图
- en: '![Refer to caption](img/6ad2ecaa8a3ca4ea9e555b3fb9669273.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/6ad2ecaa8a3ca4ea9e555b3fb9669273.png)'
- en: (c) Attention map at position 278
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 位置 278 的注意力图
- en: 'Figure 9: Attention Map at Layer 20'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：第 20 层的注意力图
- en: A.2 Cross Layer Cosine Similarity
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 跨层余弦相似度
- en: 'In Section [3.3](#S3.SS3 "3.3 Attention Weights Decides the Pivotal Tokens
    ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time"), our analysis
    assumes a large cosine similarity between the input and output of $\mathcal{F}$
    is extremely high.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[3.3节](#S3.SS3 "3.3 注意力权重决定关键标记 ‣ 3 重要性假设的持久性 ‣ Scissorhands：利用重要性假设在测试时进行
    LLM KV 缓存压缩")中，我们的分析假设输入和输出的 $\mathcal{F}$ 之间的余弦相似度极高。
- en: '![Refer to caption](img/ee769d6ae8a125b9dacaac59af14ae4a.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/ee769d6ae8a125b9dacaac59af14ae4a.png)'
- en: (a) Cosine Similarity
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 余弦相似度
- en: '![Refer to caption](img/0b6593d0e98fc63e90d3fb73e98d5d02.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/0b6593d0e98fc63e90d3fb73e98d5d02.png)'
- en: (b) Norm Comparision
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 范数比较
- en: 'Figure 10: $x$ is high in cosine similarity'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: $x$ 在余弦相似度中很高'
- en: Appendix B Proofs
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 证明
- en: 'B.1 Proof of Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.3 Attention Weights
    Decides the Pivotal Tokens ‣ 3 The Persistence of Importance Hypothesis ‣ Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time")'
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'B.1 定理 [3.1](#S3.Thmtheorem1 "定理 3.1。 ‣ 3.3 注意力权重决定关键标记 ‣ 3 重要性假设的持久性 ‣ Scissorhands:
    利用重要性假设的持久性进行LLM KV缓存压缩")'
- en: 'We consider the token generation process of a simplified model: a single-layer
    transformer model with single-head attention.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑简化模型的标记生成过程：一个单层的 Transformer 模型，带有单头注意力。
- en: '|  | $1$2 |  | (5) |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: $x_{t}\in\mathbb{R}^{1\times d}$ denotes the MLP block following attention block,
    a two-layer MLP with skip connections, given by
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: $x_{t}\in\mathbb{R}^{1\times d}$ 表示跟随注意力块的 MLP 块，即一个带有跳过连接的两层 MLP。
- en: '|  | $\mathcal{F}(x)=x+W_{2}\texttt{relu}(W_{1}x)$ |  | (6) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{F}(x)=x+W_{2}\texttt{relu}(W_{1}x)$ |  | (6) |'
- en: 'We are interested in the attention scores $\alpha_{t}=\texttt{softmax}(\nicefrac{{1}}{{t}}\cdot
    x_{t}W_{Q}W_{K}^{\top}X_{t-1}^{\top})$. We first re-state the Theorem [3.1](#S3.Thmtheorem1
    "Theorem 3.1\. ‣ 3.3 Attention Weights Decides the Pivotal Tokens ‣ 3 The Persistence
    of Importance Hypothesis ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time") below.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对注意力分数 $\alpha_{t}=\texttt{softmax}(\nicefrac{{1}}{{t}}\cdot x_{t}W_{Q}W_{K}^{\top}X_{t-1}^{\top})$
    感兴趣。我们首先重新陈述定理 [3.1](#S3.Thmtheorem1 "定理 3.1。 ‣ 3.3 注意力权重决定关键标记 ‣ 3 重要性假设的持久性
    ‣ Scissorhands: 利用重要性假设的持久性进行LLM KV缓存压缩") 如下。'
- en: Theorem B.1.
  id: totrans-254
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 B.1。
- en: Let $A=W_{V}W_{O}W_{Q}W_{K}^{\top}$, it holds that
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $A=W_{V}W_{O}W_{Q}W_{K}^{\top}$，则有
- en: '|  | $1$2 |  | (7) |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: As a preparation of the proof, we first show two lemmas.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 作为证明的准备，我们首先展示两个引理。
- en: Lemma B.1.
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 B.1。
- en: Let $x_{1},x_{2}\in\mathbb{R}^{1\times m}$ we have
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $x_{1},x_{2}\in\mathbb{R}^{1\times m}$ 我们有
- en: '|  | $\displaystyle\left&#124;x_{1}y^{\top}-x_{2}y^{\top}\right&#124;\leq\sqrt{2\delta}\left\&#124;y\right\&#124;_{2}$
    |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left&#124;x_{1}y^{\top}-x_{2}y^{\top}\right&#124;\leq\sqrt{2\delta}\left\&#124;y\right\&#124;_{2}$
    |  |'
- en: Proof.
  id: totrans-261
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Let $x_{2}=x_{2}^{\parallel}+x_{2}^{\perp}$ where
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $x_{2}=x_{2}^{\parallel}+x_{2}^{\perp}$ 其中
- en: '|  | $\displaystyle x_{2}^{\parallel}=x_{1}x_{2}^{\top}\cdot x_{1};\quad x_{2}^{\perp}=x_{2}-x_{2}^{\parallel}$
    |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle x_{2}^{\parallel}=x_{1}x_{2}^{\top}\cdot x_{1};\quad x_{2}^{\perp}=x_{2}-x_{2}^{\parallel}$
    |  |'
- en: Then it is easy to see that $x_{2}^{\perp}x_{1}^{\top}=0$. By the Pythagorean
    Theorem, we have
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 那么很容易看出 $x_{2}^{\perp}x_{1}^{\top}=0$。根据毕达哥拉斯定理，我们有
- en: '|  | $\displaystyle\left\&#124;x_{2}^{\perp}\right\&#124;_{2}^{2}=\left\&#124;x_{2}\right\&#124;_{2}^{2}-\left\&#124;x_{2}^{\parallel}\right\&#124;_{2}^{2}=\delta(2-\delta)$
    |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left\&#124;x_{2}^{\perp}\right\&#124;_{2}^{2}=\left\&#124;x_{2}\right\&#124;_{2}^{2}-\left\&#124;x_{2}^{\parallel}\right\&#124;_{2}^{2}=\delta(2-\delta)$
    |  |'
- en: Therefore, we have
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有
- en: '|  | $\displaystyle\left\&#124;x_{1}-x_{2}\right\&#124;_{2}^{2}$ |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left\&#124;x_{1}-x_{2}\right\&#124;_{2}^{2}$ |  |'
- en: '|  |  | $\displaystyle=\left\&#124;\left(1-x_{1}x_{2}^{\top}\right)x_{1}-x_{2}^{\perp}\right\&#124;_{2}^{2}$
    |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\left\&#124;\left(1-x_{1}x_{2}^{\top}\right)x_{1}-x_{2}^{\perp}\right\&#124;_{2}^{2}$
    |  |'
- en: '|  |  | $\displaystyle=\left(1-x_{1}x_{2}^{\top}\right)^{2}+\left\&#124;x_{2}^{\perp}\right\&#124;_{2}^{2}$
    |  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\left(1-x_{1}x_{2}^{\top}\right)^{2}+\left\&#124;x_{2}^{\perp}\right\&#124;_{2}^{2}$
    |  |'
- en: '|  |  | $\displaystyle=2\delta$ |  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=2\delta$ |  |'
- en: Thus, the Cauchy-Schwarz inequality implies
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，柯西-施瓦兹不等式意味着
- en: '|  | $\displaystyle\left&#124;x_{1}y^{\top}-x_{2}y^{\top}\right&#124;\leq\left\&#124;x_{1}-x_{2}\right\&#124;_{2}\cdot\left\&#124;y\right\&#124;_{2}=\sqrt{2\delta}\left\&#124;y\right\&#124;_{2}$
    |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left&#124;x_{1}y^{\top}-x_{2}y^{\top}\right&#124;\leq\left\&#124;x_{1}-x_{2}\right\&#124;_{2}\cdot\left\&#124;y\right\&#124;_{2}=\sqrt{2\delta}\left\&#124;y\right\&#124;_{2}$
    |  |'
- en: ∎
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Lemma B.2.
  id: totrans-274
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 B.2。
- en: Let $\ell\in[t]$. Then we have
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\ell\in[t]$。那么我们有
- en: '|  | $1$2 |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-277
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Notice that
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到
- en: '|  | $\displaystyle a_{t}=\alpha_{t}X_{t-1}W_{V}W_{O}=\left(\sum_{j=1}^{t-1}\alpha_{t,j}x_{j}\right)W_{V}W_{O}$
    |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle a_{t}=\alpha_{t}X_{t-1}W_{V}W_{O}=\left(\sum_{j=1}^{t-1}\alpha_{t,j}x_{j}\right)W_{V}W_{O}$
    |  |'
- en: Thus, we have
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有
- en: '|  | $1$2 |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Therefore
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: '|  | $\displaystyle\left&#124;a_{t}W_{Q}W_{K}^{\top}x_{\ell}^{\top}-\alpha_{t,\ell}x_{\ell}Ax_{\ell}^{\top}\right&#124;$
    |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left&#124;a_{t}W_{Q}W_{K}^{\top}x_{\ell}^{\top}-\alpha_{t,\ell}x_{\ell}Ax_{\ell}^{\top}\right&#124;$
    |  |'
- en: '|  |  | $\displaystyle\leq\sum_{j=1,j\neq\ell}^{t-1}\alpha_{t,j}\left&#124;x_{j}Ax_{\ell}^{\top}\right&#124;$
    |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\sum_{j=1,j\neq\ell}^{t-1}\alpha_{t,j}\left&#124;x_{j}Ax_{\ell}^{\top}\right&#124;$
    |  |'
- en: '|  |  | $\displaystyle\leq\epsilon x_{\ell}Ax_{\ell}^{\top}\sum_{j=1,j\neq\ell}^{t-1}\alpha_{t,j}$
    |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\epsilon x_{\ell}Ax_{\ell}^{\top}\sum_{j=1,j\neq\ell}^{t-1}\alpha_{t,j}$
    |  |'
- en: '|  |  | $\displaystyle\leq\epsilon x_{\ell}Ax_{\ell}^{\top}$ |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\epsilon x_{\ell}Ax_{\ell}^{\top}$ |  |'
- en: where in the second inequality we use $\epsilon^{-1}\left|x_{j}Ax_{\ell}^{\top}\right|\leq
    x_{\ell}Ax_{\ell}^{\top}$. This implies that
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 其中在第二个不等式中我们使用了 $\epsilon^{-1}\left|x_{j}Ax_{\ell}^{\top}\right|\leq x_{\ell}Ax_{\ell}^{\top}$。这意味着
- en: '|  | $1$2 |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: ∎
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Now we proceed to the main body of the proof. Assume that $\left\|x_{\ell}\right\|_{2}=1$,
    then we have
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进入证明的主体部分。假设 $\left\|x_{\ell}\right\|_{2}=1$，则我们有
- en: '|  | $1$2 |  |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Recall that $\lambda_{Q},\lambda_{K}$, we have
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 记住 $\lambda_{Q},\lambda_{K}$，我们有
- en: '|  | $1$2 |  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Notice that
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到
- en: '|  | $\displaystyle\left\&#124;a_{t}\right\&#124;_{2}$ |  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left\|a_{t}\right\|_{2}$ |  |'
- en: '|  |  | $\displaystyle\leq\lambda_{O}\lambda_{V}\left\&#124;\sum_{j=1}^{t-1}\alpha_{t,j}x_{j}\right\&#124;_{2}$
    |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\lambda_{O}\lambda_{V}\left\|\sum_{j=1}^{t-1}\alpha_{t,j}x_{j}\right\|_{2}$
    |  |'
- en: '|  |  | $\displaystyle\leq\lambda_{O}\lambda_{V}\sum_{j=1}^{t-1}\alpha_{t,j}\left\&#124;x_{j}\right\&#124;_{2}$
    |  |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\lambda_{O}\lambda_{V}\sum_{j=1}^{t-1}\alpha_{t,j}\left\|x_{j}\right\|_{2}$
    |  |'
- en: '|  |  | $\displaystyle=\lambda_{O}\lambda_{V}$ |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\lambda_{O}\lambda_{V}$ |  |'
- en: Then since $\delta\leq\left(\frac{c\epsilon}{\lambda_{Q}\lambda_{K}\lambda_{V}\lambda_{O}}\right)^{2}$,
    we have
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 然后由于 $\delta\leq\left(\frac{c\epsilon}{\lambda_{Q}\lambda_{K}\lambda_{V}\lambda_{O}}\right)^{2}$，我们有
- en: '|  | $1$2 |  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'Since by Lemma ([B.2](#A2.Thmlemma2 "Lemma B.2\. ‣ B.1 Proof of Theorem 3.1
    ‣ Appendix B Proofs ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time")), we have'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '由于根据引理 ([B.2](#A2.Thmlemma2 "引理 B.2 ‣ B.1 定理 3.1 证明 ‣ 附录 B 证明 ‣ Scissorhands:
    利用重要性持久性假设进行 LLM KV 缓存压缩的测试时"))，我们有'
- en: '|  | $1$2 |  |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: It must hold that
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 必须满足
- en: '|  | $1$2 |  |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Since $x_{\ell}^{\top}ax_{\ell}\geq c$, it holds that
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $x_{\ell}^{\top}ax_{\ell}\geq c$，所以
- en: '|  | $\displaystyle\frac{2c\epsilon}{\left\&#124;a_{t}\right\&#124;_{2}}\leq\frac{2\epsilon}{\left\&#124;a_{t}\right\&#124;_{2}}x_{\ell}^{\top}ax_{\ell}$
    |  |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{2c\epsilon}{\left\|a_{t}\right\|_{2}}\leq\frac{2\epsilon}{\left\|a_{t}\right\|_{2}}x_{\ell}^{\top}ax_{\ell}$
    |  |'
- en: which implies that
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着
- en: '|  | $1$2 |  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Therefore
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: '|  | $1$2 |  |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'B.2 Proof of Theorem [4.1](#S4.Thmtheorem1 "Theorem 4.1\. ‣ 4.2 Theoretical
    Analysis. ‣ 4 Sequential Token Generation Under budget ‣ Scissorhands: Exploiting
    the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test
    Time")'
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'B.2 定理 [4.1](#S4.Thmtheorem1 "定理 4.1 ‣ 4.2 理论分析 ‣ 4 在预算下的序列标记生成 ‣ Scissorhands:
    利用重要性持久性假设进行 LLM KV 缓存压缩的测试时")'
- en: 'Let $\{\tilde{x}_{t}\}_{t=0}^{T}$:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\{\tilde{x}_{t}\}_{t=0}^{T}$：
- en: '|  | $1$2 |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'Notice that when $m=1$ tokens. If the ranking of the attention scores does
    not change in each iteration, Algorithm [2](#alg2 "Algorithm 2 ‣ 4 Sequential
    Token Generation Under budget ‣ Scissorhands: Exploiting the Persistence of Importance
    Hypothesis for LLM KV Cache Compression at Test Time") will always drop tokens
    with the smallest attention scores.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '注意到当 $m=1$ 的标记时。如果每次迭代中注意力分数的排名没有变化，算法 [2](#alg2 "算法 2 ‣ 4 在预算下的序列标记生成 ‣ Scissorhands:
    利用重要性持久性假设进行 LLM KV 缓存压缩的测试时") 将始终丢弃具有最小注意力分数的标记。'
- en: For reference purposes, let $\{x_{t}\}_{t=0}^{T}$.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 供参考，设 $\{x_{t}\}_{t=0}^{T}$。
- en: Theorem B.2.
  id: totrans-316
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 B.2。
- en: 'Let $\lambda_{1},\lambda_{2}$ in ([6](#A2.E6 "In B.1 Proof of Theorem 3.1 ‣
    Appendix B Proofs ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis
    for LLM KV Cache Compression at Test Time")). Let'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '设 $\lambda_{1},\lambda_{2}$ 在 ([6](#A2.E6 "在 B.1 定理 3.1 证明 ‣ 附录 B 证明 ‣ Scissorhands:
    利用重要性持久性假设进行 LLM KV 缓存压缩的测试时")). 设'
- en: '|  | $1$2 |  |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: and assume that each $\beta_{t,j}=cv_{t,j}$
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 并且假设每个 $\beta_{t,j}=cv_{t,j}$
- en: '|  | $1$2 |  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Define $m_{k,j}=\mathbb{I}\left\{j\in S_{t}\right\}$ can be written as
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 $m_{k,j}=\mathbb{I}\left\{j\in S_{t}\right\}$ 可以写成
- en: '|  | $1$2 |  | (8) |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (8) |'
- en: Our first lemma shows the Lipschitzness of the attention module.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个引理展示了注意力模块的 Lipschitz 连续性。
- en: Lemma B.3.
  id: totrans-324
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 B.3。
- en: Consider two sequences of tokens $\{x_{i}\}_{i=1}^{t}$. Then we have
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个标记序列 $\{x_{i}\}_{i=1}^{t}$。然后我们有
- en: '|  | $1$2 |  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-327
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We can decompose the difference as
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将差异分解为
- en: '|  | $1$2 |  |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: By the Lipschitzness of softmax, we have
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 softmax 的 Lipschitz 连续性，我们有
- en: '|  | $1$2 |  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle\quad\quad\leq\frac{1}{t}\left\&#124;x_{t}W_{Q}W_{K}^{\top}\left(X_{t-1}-Y_{t-1}\right)^{\top}\right\&#124;_{2}$
    |  |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\quad\quad\leq\frac{1}{t}\left\&#124;x_{t}W_{Q}W_{K}^{\top}\left(X_{t-1}-Y_{t-1}\right)^{\top}\right\&#124;_{2}$
    |  |'
- en: '|  | $\displaystyle\quad\quad\leq\frac{1}{t}\lambda_{Q}\lambda_{K}\left\&#124;x_{t}\right\&#124;_{2}\left\&#124;X_{t-1}-Y_{t-1}\right\&#124;_{2}$
    |  |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\quad\quad\leq\frac{1}{t}\lambda_{Q}\lambda_{K}\left\&#124;x_{t}\right\&#124;_{2}\left\&#124;X_{t-1}-Y_{t-1}\right\&#124;_{2}$
    |  |'
- en: Since $\left\|x_{t}\right\|_{2}=1$, we have
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $\left\|x_{t}\right\|_{2}=1$，我们有
- en: '|  | $1$2 |  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Similarly,
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，
- en: '|  | $1$2 |  |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle\quad\quad\leq\frac{1}{t}\left\&#124;(x_{t}-y_{t})W_{Q}W_{K}^{\top}Y_{t-1}^{\top}\right\&#124;_{2}$
    |  |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\quad\quad\leq\frac{1}{t}\left\&#124;(x_{t}-y_{t})W_{Q}W_{K}^{\top}Y_{t-1}^{\top}\right\&#124;_{2}$
    |  |'
- en: '|  | $\displaystyle\quad\quad\leq\frac{1}{t}\lambda_{Q}\lambda_{K}\left\&#124;Y_{t-1}\right\&#124;_{F}\left\&#124;x_{t}-y_{t}\right\&#124;_{2}$
    |  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\quad\quad\leq\frac{1}{t}\lambda_{Q}\lambda_{K}\left\&#124;Y_{t-1}\right\&#124;_{F}\left\&#124;x_{t}-y_{t}\right\&#124;_{2}$
    |  |'
- en: Since $\left\|x_{t}-y_{t}\right\|_{2}=\Delta_{t}$, we have
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $\left\|x_{t}-y_{t}\right\|_{2}=\Delta_{t}$，我们有
- en: '|  | $1$2 |  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Combining the two bounds gives
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 结合这两个界限得到
- en: '|  | $1$2 |  |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: ∎
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Our second lemma shows the difference between the output of the sampled and
    vanilla transformer when the input is the same.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个引理展示了当输入相同时，采样变压器与普通变压器输出之间的差异。
- en: Lemma B.4.
  id: totrans-348
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 B.4。
- en: Let $\tilde{a}_{t}$ as
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 令 $\tilde{a}_{t}$ 为
- en: '|  | $1$2 |  | (9) |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: Assume that $\left\|x_{j}\right\|_{2}=1$. Then we have
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 $\left\|x_{j}\right\|_{2}=1$。那么我们有
- en: '|  | $\left\&#124;\tilde{a}_{t}-b_{t}\right\&#124;_{2}\leq\lambda_{V}\lambda_{O}\sum_{j\notin\hat{S}_{t}}\beta_{t,j}$
    |  |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left\&#124;\tilde{a}_{t}-b_{t}\right\&#124;_{2}\leq\lambda_{V}\lambda_{O}\sum_{j\notin\hat{S}_{t}}\beta_{t,j}$
    |  |'
- en: Proof.
  id: totrans-353
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: A direction computation yields
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 方向计算得到
- en: '|  | $1$2 |  |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Thus, $\left\|\tilde{a}_{t}-b_{t}\right\|_{2}$ can be bounded as
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，$\left\|\tilde{a}_{t}-b_{t}\right\|_{2}$ 可以被界限为
- en: '|  | $1$2 |  |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: since $\left\|\tilde{x}_{j}\right\|_{2}=1$ can be written as
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $\left\|\tilde{x}_{j}\right\|_{2}=1$ 可以写作
- en: '|  | $1$2 |  |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Furthermore, for all $j\notin\hat{S}_{t}$, we have
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于所有 $j\notin\hat{S}_{t}$，我们有
- en: '|  | $\tilde{\alpha}_{t,j}=\frac{\exp\left(r_{t,j}\right)}{\sum_{i\in\hat{S}_{t}}\exp\left(r_{t,i}\right)}$
    |  |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{\alpha}_{t,j}=\frac{\exp\left(r_{t,j}\right)}{\sum_{i\in\hat{S}_{t}}\exp\left(r_{t,i}\right)}$
    |  |'
- en: Therefore, for all $j\in\hat{S}_{t}$, we have
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于所有 $j\in\hat{S}_{t}$，我们有
- en: '|  | $\displaystyle\beta_{t,j}-\tilde{\alpha}_{t,j}$ |  |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\beta_{t,j}-\tilde{\alpha}_{t,j}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\tilde{\alpha}_{t,j}\sum_{i\notin\hat{S}_{t}}\beta_{t,j}$
    |  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\tilde{\alpha}_{t,j}\sum_{i\notin\hat{S}_{t}}\beta_{t,j}$
    |  |'
- en: Therefore, the bound of $\left\|\tilde{a}_{t}-b_{t}\right\|_{2}$ can be written
    as
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，$\left\|\tilde{a}_{t}-b_{t}\right\|_{2}$ 的界限可以写作
- en: '|  | $1$2 |  |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where the last equality follows from $\sum_{j\in\hat{S}_{t}}\tilde{\alpha}_{t,j}=1$.
    ∎
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最后的等式源于 $\sum_{j\in\hat{S}_{t}}\tilde{\alpha}_{t,j}=1$。∎
- en: 'Our last lemma shows the Lipschitzness of the MLP in ([6](#A2.E6 "In B.1 Proof
    of Theorem 3.1 ‣ Appendix B Proofs ‣ Scissorhands: Exploiting the Persistence
    of Importance Hypothesis for LLM KV Cache Compression at Test Time")).'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后的引理展示了MLP的Lipschitz性质，详见([6](#A2.E6 "在 B.1 证明定理 3.1 ‣ 附录 B 证明 ‣ 剪刀手：利用重要性假设的持久性进行
    LLM KV 缓存压缩"))。
- en: Lemma B.5.
  id: totrans-370
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 B.5。
- en: Let $\lambda_{1},\lambda_{2}$, we have
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 令 $\lambda_{1},\lambda_{2}$，我们有
- en: '|  | $\left\&#124;\mathcal{F}(x_{1})-\mathcal{F}(x_{2})\right\&#124;\leq\left(1+\lambda_{1}\lambda_{2}\right)\left\&#124;x_{1}-x_{2}\right\&#124;_{2}$
    |  |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left\&#124;\mathcal{F}(x_{1})-\mathcal{F}(x_{2})\right\&#124;\leq\left(1+\lambda_{1}\lambda_{2}\right)\left\&#124;x_{1}-x_{2}\right\&#124;_{2}$
    |  |'
- en: Proof.
  id: totrans-373
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Direct computation yields
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 直接计算得到
- en: '|  | $\displaystyle\left\&#124;\mathcal{F}(x_{1})-\mathcal{F}(x_{2})\right\&#124;$
    |  |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left\&#124;\mathcal{F}(x_{1})-\mathcal{F}(x_{2})\right\&#124;$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle\leq\left\&#124;x_{1}-x_{2}\right\&#124;_{2}+\lambda_{2}\left\&#124;W_{1}\left(x_{1}-x_{2}\right)\right\&#124;_{2}$
    |  |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\left\&#124;x_{1}-x_{2}\right\&#124;_{2}+\lambda_{2}\left\&#124;W_{1}\left(x_{1}-x_{2}\right)\right\&#124;_{2}$
    |  |'
- en: '|  |  | $\displaystyle\leq\left\&#124;x_{1}-x_{2}\right\&#124;_{2}+\lambda_{1}\lambda_{1}\left\&#124;x_{1}-x_{2}\right\&#124;_{2}$
    |  |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\left\&#124;x_{1}-x_{2}\right\&#124;_{2}+\lambda_{1}\lambda_{1}\left\&#124;x_{1}-x_{2}\right\&#124;_{2}$
    |  |'
- en: '|  |  | $\displaystyle=(1+\lambda_{1}\lambda_{2})\left\&#124;x_{1}-x_{2}\right\&#124;_{2}$
    |  |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=(1+\lambda_{1}\lambda_{2})\left\&#124;x_{1}-x_{2}\right\&#124;_{2}$
    |  |'
- en: where in the third inequality we use the fact that relu$(\cdot)$-Lipschitz.
    ∎
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 其中在第三个不等式中，我们利用了 relu$(\cdot)$ 的 Lipschitz 特性。∎
- en: Now we turn to the proof of our main theorem. Combining all of the results,
    we have
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向主定理的证明。结合所有结果，我们得到
- en: '|  | $\displaystyle a_{t}-\tilde{a}_{t}$ |  |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle a_{t}-\tilde{a}_{t}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Therefore, by triangle inequality, we have
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过三角不等式，我们有
- en: '|  | $\left\&#124;a_{t}-\tilde{a}_{t}\right\&#124;_{2}\leq\left\&#124;\mathcal{T}_{1}\right\&#124;_{2}+\left\&#124;\mathcal{T}_{2}\right\&#124;_{2}+\left\&#124;\mathcal{T}_{3}\right\&#124;_{2}$
    |  | (10) |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left\&#124;a_{t}-\tilde{a}_{t}\right\&#124;_{2}\leq\left\&#124;\mathcal{T}_{1}\right\&#124;_{2}+\left\&#124;\mathcal{T}_{2}\right\&#124;_{2}+\left\&#124;\mathcal{T}_{3}\right\&#124;_{2}$
    |  | (10) |'
- en: To start, the magnitude of $\mathcal{T}_{1}$ can be bounded as
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，$\mathcal{T}_{1}$ 的大小可以被界定为
- en: '|  | $\displaystyle\left\&#124;\mathcal{T}_{1}\right\&#124;_{2}$ |  |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left\&#124;\mathcal{T}_{1}\right\&#124;_{2}$ |  |'
- en: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\left\&#124;\sum_{j=1}^{t-1}\alpha_{t,j}(x_{t,j}-\tilde{x}_{t,j})\right\&#124;$
    |  |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\left\&#124;\sum_{j=1}^{t-1}\alpha_{t,j}(x_{t,j}-\tilde{x}_{t,j})\right\&#124;$
    |  |'
- en: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\sum_{j=1}^{t-1}\alpha_{t,j}\left\&#124;x_{t,j}-\tilde{x}_{t,j}\right\&#124;_{2}$
    |  |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\sum_{j=1}^{t-1}\alpha_{t,j}\left\&#124;x_{t,j}-\tilde{x}_{t,j}\right\&#124;_{2}$
    |  |'
- en: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\Delta_{t}\sum_{j=1}^{t-1}\alpha_{t,j}$
    |  |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\Delta_{t}\sum_{j=1}^{t-1}\alpha_{t,j}$
    |  |'
- en: '|  |  | $\displaystyle=\lambda_{V}\lambda_{O}\Delta_{t}$ |  |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\lambda_{V}\lambda_{O}\Delta_{t}$ |  |'
- en: where in the third inequality we use $\left\|x_{t,j}-\tilde{x}_{t,j}\right\|_{2}=\Delta_{t}$
    to get that
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个不等式中，我们使用 $\left\|x_{t,j}-\tilde{x}_{t,j}\right\|_{2}=\Delta_{t}$ 得到
- en: '|  | $\displaystyle\left\&#124;\mathcal{T}_{2}\right\&#124;_{2}$ |  |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left\&#124;\mathcal{T}_{2}\right\&#124;_{2}$ |  |'
- en: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\left\&#124;\left(\sum_{j=0}^{t-1}(\alpha_{t,j}-\beta_{t,j})\tilde{x}_{j}\right)\right\&#124;_{2}$
    |  |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\left\&#124;\left(\sum_{j=0}^{t-1}(\alpha_{t,j}-\beta_{t,j})\tilde{x}_{j}\right)\right\&#124;_{2}$
    |  |'
- en: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\sum_{j=0}^{t-1}\left&#124;\alpha_{t,j}-\beta_{t,j}\right&#124;\left\&#124;\tilde{x}_{j}\right\&#124;_{2}$
    |  |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\sum_{j=0}^{t-1}\left&#124;\alpha_{t,j}-\beta_{t,j}\right&#124;\left\&#124;\tilde{x}_{j}\right\&#124;_{2}$
    |  |'
- en: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\left\&#124;\alpha_{t}-\beta_{t}\right\&#124;_{1}$
    |  |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\lambda_{V}\lambda_{O}\left\&#124;\alpha_{t}-\beta_{t}\right\&#124;_{1}$
    |  |'
- en: '|  |  | $\displaystyle\leq\sqrt{t-1}\lambda_{V}\lambda_{O}\left\&#124;\alpha_{t}-\beta_{t}\right\&#124;_{2}$
    |  |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\sqrt{t-1}\lambda_{V}\lambda_{O}\left\&#124;\alpha_{t}-\beta_{t}\right\&#124;_{2}$
    |  |'
- en: '|  |  | $\displaystyle\leq 2\left(1-\frac{1}{t}\right)\lambda_{Q}\lambda_{K}\lambda_{V}\lambda_{O}\Delta_{t}$
    |  |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq 2\left(1-\frac{1}{t}\right)\lambda_{Q}\lambda_{K}\lambda_{V}\lambda_{O}\Delta_{t}$
    |  |'
- en: 'Lastly, to bound the magnitude of $\mathcal{T}_{3}$, we use Lemma [B.4](#A2.Thmlemma4
    "Lemma B.4\. ‣ B.2 Proof of Theorem 4.1 ‣ Appendix B Proofs ‣ Scissorhands: Exploiting
    the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test
    Time") to get that'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了界定 $\mathcal{T}_{3}$ 的大小，我们使用引理 [B.4](#A2.Thmlemma4 "引理 B.4\. ‣ 定理 4.1
    的证明 ‣ 附录 B 证明 ‣ 剪刀手：利用重要性假设的持久性进行 LLM KV 缓存压缩的测试时") 得到
- en: '|  | $\left\&#124;\mathcal{T}_{3}\right\&#124;_{2}\leq 2\lambda_{V}\lambda_{O}\sum_{j\notin\hat{S}_{t}}\beta_{t,j}$
    |  |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left\&#124;\mathcal{T}_{3}\right\&#124;_{2}\leq 2\lambda_{V}\lambda_{O}\sum_{j\notin\hat{S}_{t}}\beta_{t,j}$
    |  |'
- en: 'Putting things together for ([10](#A2.E10 "In B.2 Proof of Theorem 4.1 ‣ Appendix
    B Proofs ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for
    LLM KV Cache Compression at Test Time")), we have'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 综合考虑 ([10](#A2.E10 "在 B.2 定理 4.1 的证明 ‣ 附录 B 证明 ‣ 剪刀手：利用重要性假设的持久性进行 LLM KV 缓存压缩的测试时"))，我们有
- en: '|  | $1$2 |  |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'By Lemma [B.5](#A2.Thmlemma5 "Lemma B.5\. ‣ B.2 Proof of Theorem 4.1 ‣ Appendix
    B Proofs ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for
    LLM KV Cache Compression at Test Time") we can further show that'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 根据引理 [B.5](#A2.Thmlemma5 "引理 B.5\. ‣ 定理 4.1 的证明 ‣ 附录 B 证明 ‣ 剪刀手：利用重要性假设的持久性进行
    LLM KV 缓存压缩的测试时")，我们可以进一步证明
- en: '|  | $1$2 |  |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'By Theorem [B.3](#A2.Thmtheorem3 "Theorem B.3\. ‣ B.3 Budgeted Cache ‣ Appendix
    B Proofs ‣ Scissorhands: Exploiting the Persistence of Importance Hypothesis for
    LLM KV Cache Compression at Test Time"), we have that with probability at least
    $1-T_{\max}\exp\left(-\frac{\epsilon^{2}b^{2}(T_{\min}-1)}{(k-2)^{2}(u-b)^{2}}\right)-T_{\max}\exp\left(-\frac{2(T_{\min}-1)(1-\nicefrac{{B}}{{T_{\max}}})^{2}}{(1-\epsilon)^{2}}\right)$
    that'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定理[B.3](#A2.Thmtheorem3 "定理 B.3\. ‣ B.3 预算缓存 ‣ 附录 B 证明 ‣ 剪刀手：利用重要性假设在测试时对
    LLM KV 缓存进行压缩")，我们可以得出，概率至少为 $1-T_{\max}\exp\left(-\frac{\epsilon^{2}b^{2}(T_{\min}-1)}{(k-2)^{2}(u-b)^{2}}\right)-T_{\max}\exp\left(-\frac{2(T_{\min}-1)(1-\nicefrac{{B}}{{T_{\max}}})^{2}}{(1-\epsilon)^{2}}\right)$。
- en: '|  | $1$2 |  |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Given that $\mathbb{E}\left[\left\|x_{t}-\tilde{x}_{t}\right\|\right]\leq 2\Delta_{\max}$,
    we have
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 $\mathbb{E}\left[\left\|x_{t}-\tilde{x}_{t}\right\|\right]\leq 2\Delta_{\max}$，我们有
- en: '|  | $\displaystyle\mathbb{E}\left[\left\&#124;x_{t+1}-\tilde{x}_{t+1}\right\&#124;_{2}\right]$
    |  |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}\left[\left\|x_{t+1}-\tilde{x}_{t+1}\right\|_{2}\right]$
    |  |'
- en: '|  |  | $\displaystyle\leq 4\lambda_{V}\lambda_{O}(1+\lambda_{1}\lambda_{2})(1+\lambda_{Q}\lambda_{K})\Delta_{\max}$
    |  |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq 4\lambda_{V}\lambda_{O}(1+\lambda_{1}\lambda_{2})(1+\lambda_{Q}\lambda_{K})\Delta_{\max}$
    |  |'
- en: Thus, as long as $\lambda_{V}\lambda_{O}(1+\lambda_{1}\lambda_{2})(1+\lambda_{Q}\lambda_{K})\leq\frac{1}{2}$,
    we can guarantee that
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，只要 $\lambda_{V}\lambda_{O}(1+\lambda_{1}\lambda_{2})(1+\lambda_{Q}\lambda_{K})\leq\frac{1}{2}$，我们可以保证
- en: '|  | $\mathbb{E}\left[\left\&#124;x_{t+1}-\tilde{x}_{t+1}\right\&#124;_{2}\right]\leq
    2\Delta_{\max}$ |  |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}\left[\left\|x_{t+1}-\tilde{x}_{t+1}\right\|_{2}\right]\leq
    2\Delta_{\max}$ |  |'
- en: Thus, for all $t\in[T_{\min},T_{\max}]$, we have that
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于所有 $t\in[T_{\min},T_{\max}]$，我们有
- en: '|  | $1$2 |  |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: B.3 Budgeted Cache
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 预算缓存
- en: Theorem B.3.
  id: totrans-418
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 B.3。
- en: Let $\beta_{t,j}$ that
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\beta_{t,j}$ 为
- en: '|  | $1$2 |  | (11) |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (11) |'
- en: We consider the case of maintaining a budget of $B$
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑维持预算 $B$ 的情况
- en: '|  | $\displaystyle c=\left(\int_{0}^{u-b}(x+b)^{-k}dx\right)^{-1}=\frac{k-1}{b^{1-k}-u^{1-k}}$
    |  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c=\left(\int_{0}^{u-b}(x+b)^{-k}dx\right)^{-1}=\frac{k-1}{b^{1-k}-u^{1-k}}$
    |  |'
- en: To start, we notice that
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们注意到
- en: '|  | $\int x(x+b)^{-k}=-\frac{(x+b)^{1-k}((k-1)x+b)}{(k-1)(k-2)}:=g(x)$ |  |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  | $\int x(x+b)^{-k}=-\frac{(x+b)^{1-k}((k-1)x+b)}{(k-1)(k-2)}:=g(x)$ |  |'
- en: Let $C=\sum_{j=1}^{t-1}v_{j}$ is
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $C=\sum_{j=1}^{t-1}v_{j}$ 是
- en: '|  | $\displaystyle\mathbb{E}\left[C\right]=(t-1)\mathbb{E}\left[v_{1}\right]$
    |  |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}\left[C\right]=(t-1)\mathbb{E}\left[v_{1}\right]$
    |  |'
- en: '|  |  | $\displaystyle=(t-1)\frac{k-1}{b^{1-k}-u^{1-k}}(g(u)-g(0))$ |  |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=(t-1)\frac{k-1}{b^{1-k}-u^{1-k}}(g(u)-g(0))$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Let $\Delta=\frac{b^{2-k}-(k-1)u^{2-k}+(k-2)bu^{1-k}}{b^{1-k}-u^{1-k}}$. By
    Hoeffding’s inequality, we have that
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\Delta=\frac{b^{2-k}-(k-1)u^{2-k}+(k-2)bu^{1-k}}{b^{1-k}-u^{1-k}}$。根据霍夫丁不等式，我们有
- en: '|  | $\mathbb{P}\left(C\leq(1-\epsilon)\mathbb{E}\left[C\right]\right)\leq\exp\left(-\frac{2\epsilon^{2}\mathbb{E}\left[C\right]^{2}}{(t-1)(u-b)^{2}}\right)$
    |  |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{P}\left(C\leq(1-\epsilon)\mathbb{E}\left[C\right]\right)\leq\exp\left(-\frac{2\epsilon^{2}\mathbb{E}\left[C\right]^{2}}{(t-1)(u-b)^{2}}\right)$
    |  |'
- en: This implies that with probability at least $1-\exp\left(-\frac{2\epsilon^{2}\Delta^{2}(t-1)}{(k-2)^{2}(u-b)^{2}}\right)$
    we have
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，概率至少为 $1-\exp\left(-\frac{2\epsilon^{2}\Delta^{2}(t-1)}{(k-2)^{2}(u-b)^{2}}\right)$
    我们有
- en: '|  | $C\geq(1-\epsilon)\Delta\frac{t-1}{k-2}$ |  |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '|  | $C\geq(1-\epsilon)\Delta\frac{t-1}{k-2}$ |  |'
- en: Now, we proceed to bound $\sum_{j\notin\hat{S}_{t}}\beta_{t,j}$. Its expectation
    is given by
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们继续界定 $\sum_{j\notin\hat{S}_{t}}\beta_{t,j}$。其期望值为
- en: '|  | $\displaystyle\mathbb{E}\left[C^{-1}\sum_{j=1}^{t-1}\mathbb{I}\left\{v_{j}\leq\gamma\right\}v_{j}\right]$
    |  |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}\left[C^{-1}\sum_{j=1}^{t-1}\mathbb{I}\left\{v_{j}\leq\gamma\right\}v_{j}\right]$
    |  |'
- en: '|  |  | $\displaystyle=\frac{k-2}{\Delta(1-\epsilon)}\cdot\frac{k-1}{b^{1-k}-u^{1-k}}\int_{0}^{\gamma}x(x+b)^{-k}dx$
    |  |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{k-2}{\Delta(1-\epsilon)}\cdot\frac{k-1}{b^{1-k}-u^{1-k}}\int_{0}^{\gamma}x(x+b)^{-k}dx$
    |  |'
- en: '|  |  | $\displaystyle=\frac{(k-1)(k-2)}{\Delta(1-\epsilon)\left(b^{1-k}-u^{1-k}\right)}\left(g(\gamma)-g(0)\right)$
    |  |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{(k-1)(k-2)}{\Delta(1-\epsilon)\left(b^{1-k}-u^{1-k}\right)}\left(g(\gamma)-g(0)\right)$
    |  |'
- en: We pause here and study how small can we choose $\gamma$. Notice that
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里暂停一下，研究我们可以选择多小的 $\gamma$。注意到
- en: '|  | $1$2 |  |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: By Hoeffding’s inequality again, we have that
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 再次应用霍夫丁不等式，我们有
- en: '|  | $1$2 |  |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Enforcing $\sum_{j=1}^{t-1}\mathbb{I}\left\{v_{j}\leq\gamma\right\}\geq T_{\max}-B$.
    Therefore
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 强制 $\sum_{j=1}^{t-1}\mathbb{I}\left\{v_{j}\leq\gamma\right\}\geq T_{\max}-B$。因此
- en: '|  | $1$2 |  |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: We further notice that
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步注意到
- en: '|  | $1$2 |  |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: This gives
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出
- en: '|  | $\displaystyle\mathbb{E}\left[C^{-1}\sum_{j=1}^{t-1}\mathbb{I}\left\{v_{j}\leq\gamma\right\}v_{j}\right]$
    |  |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}\left[C^{-1}\sum_{j=1}^{t-1}\mathbb{I}\left\{v_{j}\leq\gamma\right\}v_{j}\right]$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Notice that if $u\geq 5b$, we have
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果 $u\geq 5b$，我们有
- en: '|  | $\Delta=b-(k-1)\left(\frac{u}{b}\right)^{1-k}\cdot\frac{b-u}{b^{1-k}-u^{1-k}}\leq
    0.98b$ |  |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta=b-(k-1)\left(\frac{u}{b}\right)^{1-k}\cdot\frac{b-u}{b^{1-k}-u^{1-k}}\leq
    0.98b$ |  |'
- en: Therefore
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: '|  | $1$2 |  |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: holds with probability at least $1$2. Taking a union bound gives the desired
    result.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 以至少 $1$2 的概率成立。取联合界限得到了所需结果。
