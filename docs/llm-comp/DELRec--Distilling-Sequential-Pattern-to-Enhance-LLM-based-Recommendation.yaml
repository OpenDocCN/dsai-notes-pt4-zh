- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 19:05:48'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:05:48'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'DELRec: Distilling Sequential Pattern to Enhance LLM-based Recommendation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'DELRec: 提取顺序模式以增强基于LLM的推荐'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11156](https://ar5iv.labs.arxiv.org/html/2406.11156)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11156](https://ar5iv.labs.arxiv.org/html/2406.11156)
- en: Guohao Sun Donghua UniversitySongjiang QuShanghai ShiChina [111@dhu.com](mailto:111@dhu.com)
     and  Haoyi Zhang Donghua UniversitySongjiang QuShanghai ShiChina [222@dhu.com](mailto:222@dhu.com)(2018;
    20 February 2007; 12 March 2009; 5 June 2009)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Guohao Sun 东华大学 松江区 上海市 中国 [111@dhu.com](mailto:111@dhu.com) 和 Haoyi Zhang 东华大学
    松江区 上海市 中国 [222@dhu.com](mailto:222@dhu.com) (2018; 2007年2月20日; 2009年3月12日; 2009年6月5日)
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Sequential recommendation (SR) tasks enhance recommendation accuracy by capturing
    the connection between users’ past interactions and their changing preferences.
    Conventional models often focus solely on capturing sequential patterns within
    the training data, neglecting the broader context and semantic information embedded
    in item titles from external sources. This limits their predictive power and adaptability.
    Recently, large language models (LLMs) have shown promise in SR tasks due to their
    advanced understanding capabilities and strong generalization abilities. Researchers
    have attempted to enhance LLMs’ recommendation performance by incorporating information
    from SR models. However, previous approaches have encountered problems such as
    1) only influencing LLMs at the result level; 2) increased complexity of LLMs
    recommendation methods leading to reduced interpretability; 3) incomplete understanding
    and utilization of SR models information by LLMs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序推荐（SR）任务通过捕捉用户过去的互动与其变化偏好之间的联系来提高推荐准确性。传统模型通常只关注捕捉训练数据中的顺序模式，忽略了来自外部来源的物品标题中嵌入的更广泛的背景和语义信息。这限制了它们的预测能力和适应性。最近，大型语言模型（LLMs）在SR任务中显示出了潜力，因为它们具有先进的理解能力和强大的泛化能力。研究人员尝试通过结合SR模型的信息来增强LLMs的推荐性能。然而，之前的方法遇到了以下问题：1)
    仅在结果层面影响LLMs；2) LLMs推荐方法复杂性增加，导致可解释性降低；3) LLMs对SR模型信息的理解和利用不完整。
- en: 'To address these problems, we proposes a novel framework, DELRec, which aims
    to extract knowledge from SR models and enable LLMs to easily comprehend and utilize
    this supplementary information for more effective sequential recommendations.
    DELRec consists of two main stages: 1) SR Models Pattern Distilling, focusing
    on extracting behavioral patterns exhibited by SR models using soft prompts through
    two well-designed strategies; 2) LLMs-based Sequential Recommendation, aiming
    to fine-tune LLMs to effectively use the distilled auxiliary information to perform
    SR tasks. Extensive experimental results conducted on three real datasets validate
    the effectiveness of the DELRec framework.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为解决这些问题，我们提出了一个新颖的框架DELRec，旨在从SR模型中提取知识，并使LLMs能够轻松理解和利用这些补充信息，以便进行更有效的顺序推荐。DELRec包含两个主要阶段：1)
    SR模型模式提取，专注于通过两种精心设计的策略使用软提示提取SR模型所展示的行为模式；2) 基于LLMs的顺序推荐，旨在微调LLMs以有效利用提取的辅助信息来执行SR任务。对三个真实数据集进行的大量实验结果验证了DELRec框架的有效性。
- en: 'Large language Model, Sequential Recommendation, Pattern Distillation^†^†copyright:
    acmlicensed^†^†journalyear: 2018^†^†doi: XXXXXXX.XXXXXXX^†^†conference: Make sure
    to enter the correct conference title from your rights confirmation emai; June
    03–05, 2018; Woodstock, NY^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs: Information
    systems Recommender systems'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '大型语言模型，顺序推荐，模式提取^†^†版权: acm许可^†^†期刊年: 2018^†^†doi: XXXXXXX.XXXXXXX^†^†会议: 请确保输入正确的会议标题，来自您的权利确认邮件；2018年6月3-5日；纽约伍德斯托克^†^†isbn:
    978-1-4503-XXXX-X/18/06^†^†ccs: 信息系统 推荐系统'
- en: 1\. INTRODUCTION
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Sequential recommendation (SR) tasks aim to improve the accuracy of recommendations
    by understanding and modeling the relationship between users’ interaction history
    and their evolving preferences. However, traditional SR models only capture sequential
    patterns within training data, often overlooking the broader context and semantic
    information embedded in item titles that can be obtained from external sources.
    These limitations restrict their predictive ability and adaptability to constantly
    changing scenarios.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 序列推荐（SR）任务旨在通过理解和建模用户互动历史与其不断变化的偏好之间的关系来提高推荐准确性。然而，传统SR模型仅捕捉训练数据中的序列模式，往往忽视了从外部来源获得的包含在项目标题中的更广泛的上下文和语义信息。这些限制限制了它们的预测能力和适应不断变化场景的能力。
- en: Recently, large language models (LLMs) have shown promise in SR tasks due to
    their advanced comprehension abilities and powerful generalization capabilities.
    As LLMs are trained on vast datasets containing abundant information, including
    inherent item features and details, they can infer user preferences and predict
    future actions by leveraging LLMs’ understanding of item attributes and reasoning
    based on world knowledge. However, using LLMs directly as sequential recommenders
    can pose certain problems. For instance, due to a lack of domain-specific expertise
    in recommendation or an incomplete understanding of the recommendation patterns
    in SR tasks, LLMs often exhibit poor performance when directly used as recommender.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，由于其先进的理解能力和强大的泛化能力，大型语言模型（LLMs）在SR任务中显示出了潜力。由于LLMs在包含丰富信息的庞大数据集上进行训练，包括固有的项目特征和细节，它们可以通过利用LLMs对项目属性的理解和基于世界知识的推理来推断用户偏好和预测未来行为。然而，直接将LLMs用作序列推荐器可能会存在一些问题。例如，由于缺乏领域特定的推荐专业知识或对SR任务中的推荐模式理解不完整，LLMs在直接作为推荐器使用时往往表现不佳。
- en: 'Therefore, researchers have previously proposed providing LLMs with auxiliary
    information from conventional SR models. These approaches aim to assist LLMs in
    making more accurate recommendations when performing SR tasks. We can roughly
    categorize the alignment of SR models with LLMs’ recommendation into three paradigms:
    1) providing SR models information in textual form to LLMs; 2) combining the embeddings
    from SR models encodings with those from LLM encodings; 3) supplying LLMs with
    embeddings derived from SR models encodings. They are illustrated in Figure [1](#S1.F1
    "Figure 1 ‣ 1\. INTRODUCTION ‣ DELRec: Distilling Sequential Pattern to Enhance
    LLM-based Recommendation"). However, previous methods have encountered certain
    issues.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，研究人员之前提出了为LLMs提供来自传统SR模型的辅助信息。这些方法旨在帮助LLMs在执行SR任务时做出更准确的推荐。我们可以大致将SR模型与LLMs推荐的对齐分为三种范式：1）以文本形式向LLMs提供SR模型信息；2）将SR模型编码的嵌入与LLMs编码的嵌入结合；3）向LLMs提供来自SR模型编码的嵌入。它们在图[1](#S1.F1
    "图1 ‣ 1\. 引言 ‣ DELRec：提炼序列模式以增强基于LLM的推荐")中进行了说明。然而，以前的方法遇到了一些问题。
- en: '![Refer to caption](img/2c4c381867900f7ec3bcf7388cb02a04.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2c4c381867900f7ec3bcf7388cb02a04.png)'
- en: Figure 1\. Demonstration of three paradigms of the alignment of SR models with
    LLMs’ recommendation
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. SR模型与LLMs推荐对齐的三种范式演示
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLMs Prompt with SR Text: This paradigm typically involves directly incorporating
    the recommendation results or textual information from conventional SR models
    into the prompt. However, this paradigm often suffers from subpar recommendation
    performance due to the limited information provided by the prompt. The prompt
    can only assist LLMs in making decisions based on the results but cannot guide
    LLMs from the perspective of the recommendation process. One fundamental reason
    is that natural language is often insufficient for accurately and comprehensively
    describing the specific recommendation behavior patterns of SR models.'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLMs Prompt with SR Text：这一范式通常涉及将传统SR模型的推荐结果或文本信息直接纳入提示中。然而，这一范式通常因提示提供的信息有限而导致推荐性能不佳。提示只能帮助LLMs基于结果做出决策，却无法从推荐过程的角度引导LLMs。一个根本原因是自然语言往往无法准确全面地描述SR模型的具体推荐行为模式。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLMs Encoding with SR Embedding: This paradigm instead of using LLMs as recommenders
    and utilizes their encoding and representation capabilities. It typically involves
    utilizing LLMs to encode a given text or sequence and simultaneously employing
    conventional models to obtain item or user encodings. Subsequently, these two
    types of embeddings are combined and processed in various ways to generate recommendation
    scores for items. Although this paradigm enables the integration of information
    from both SR models and LLMs, it also introduces challenges in comprehending and
    interpreting recommendations. This may potentially undermine some key advantages
    of using LLMs for recommendations, such as their simplicity and interpretability.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLMs 编码与 SR 嵌入：这一范式不再将 LLMs 作为推荐器使用，而是利用其编码和表示能力。它通常涉及利用 LLMs 对给定文本或序列进行编码，同时使用传统模型获取项目或用户的编码。随后，这两种嵌入被结合并以各种方式处理，以生成项目的推荐分数。尽管这一范式实现了
    SR 模型和 LLMs 信息的整合，但它也带来了理解和解释推荐的挑战。这可能会削弱使用 LLMs 进行推荐的一些关键优势，例如其简洁性和可解释性。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLMs Prompt with SR Embedding: This paradigm combines some advantages from
    the previous paradigms by typically merging embeddings from SR models with a prompt
    before inputting them into LLMs to generate item recommendations. This paradigm
    uses embeddings encoded by SR models as auxiliary information for the recommendation
    process provided to LLMs and often involves a projector to align the dimensions
    of SR model’s embeddings with the language space of LLMs. However, due to poor
    projector design or changes in embedding dimensions that result in information
    loss, LLMs may not fully comprehend the meanings conveyed by these embeddings.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLMs 提示与 SR 嵌入：这一范式结合了之前范式的一些优点，通常将 SR 模型的嵌入与提示合并，然后输入到 LLMs 中以生成项目推荐。该范式使用由
    SR 模型编码的嵌入作为提供给 LLMs 的推荐过程的辅助信息，并通常涉及一个投影器，以将 SR 模型的嵌入维度与 LLMs 的语言空间对齐。然而，由于投影器设计不佳或嵌入维度的变化导致信息丢失，LLMs
    可能无法完全理解这些嵌入所传达的含义。
- en: 'To tackle the aforementioned problems, we propose Distilling Sequential Pattern
    to Enhance LLM-based Recommendation (DELRec) framework, which aims to distill
    the behavioral patterns of SR models and empower LLMs to easily comprehend and
    leverage this supplementary information for more effective sequential recommendations.
    DELRec is roughly shown in Figure [2](#S1.F2 "Figure 2 ‣ 1\. INTRODUCTION ‣ DELRec:
    Distilling Sequential Pattern to Enhance LLM-based Recommendation"), and it contains:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决上述问题，我们提出了**Distilling Sequential Pattern to Enhance LLM-based Recommendation
    (DELRec)** 框架，旨在蒸馏 SR 模型的行为模式，并赋能 LLMs 轻松理解和利用这些补充信息，以实现更有效的顺序推荐。DELRec 大致如图 [2](#S1.F2
    "Figure 2 ‣ 1\. INTRODUCTION ‣ DELRec: Distilling Sequential Pattern to Enhance
    LLM-based Recommendation") 所示，包含以下内容：'
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SR Models Pattern Distilling: Rather than inputting encoded information from
    SR models or LLMs as previous methods did, the approach of SR Models Pattern Distilling
    is inspired by knowledge distillation techniques used in LLMs. The objective is
    to distill the recommendation patterns and information of conventional SR models
    understandable to LLMs. This involves using LLMs to extract useful knowledge into
    soft prompts. Through two learning components, namely SR Models Temporal Analysis
    and Recommendation Pattern Simulating, LLMs are empowered to comprehend and simulate
    recommendation process employed by SR models effectively. This is a process of
    transforming the knowledge of SR models into a form that LLMs can understand and
    use.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SR 模型模式蒸馏：SR 模型模式蒸馏的方法并不像之前的方法那样输入 SR 模型或 LLMs 编码的信息，而是受到 LLMs 知识蒸馏技术的启发。其目的是将传统
    SR 模型的推荐模式和信息蒸馏为 LLMs 可以理解的形式。这涉及使用 LLMs 提取有用的知识转化为软提示。通过两个学习组件，即 SR 模型时间分析和推荐模式模拟，LLMs
    被赋予了有效理解和模拟 SR 模型推荐过程的能力。这是将 SR 模型的知识转化为 LLMs 能够理解和使用的形式的过程。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLMs-based Sequential Recommendation: After getting the distilled SR knowledge
    in the first stage for SR tasks, we propose LLMs-based Sequential Recommendation
    for effectively instructing LLMs. Instead of using a projector for embedding mapping,
    we insert the learned soft prompts directly into the prompt, and then fine-tune
    the LLMs to adapt to the learning tasks that utilize auxiliary information.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于LLMs的序列推荐：在第一阶段获得提炼的SR知识后，我们提出了基于LLMs的序列推荐，以有效指导LLMs。我们不再使用投影器进行嵌入映射，而是将学习到的软提示直接插入提示中，然后微调LLMs以适应利用辅助信息的学习任务。
- en: '![Refer to caption](img/fe3a850704dfedd8ddd08e04e18c31d9.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fe3a850704dfedd8ddd08e04e18c31d9.png)'
- en: Figure 2\. Demonstration of the paradigm of DELRec with proposed learning components
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 演示了DELRec的范式及其提出的学习组件
- en: \Description
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: 'The main contributions of our work are summarized as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的主要贡献总结如下：
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Proposing two novel components in DELRec to distill the sequential recommendation
    patterns of SR models in soft prompts as accurately as possible.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出了DELRec中的两个新组件，以尽可能准确地提炼SR模型在软提示中的序列推荐模式。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Designing an ingenious method to fine-tune LLMs to enable them to use the distilled
    auxiliary information appropriately, thereby reducing information loss.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设计一种巧妙的方法来微调LLMs，使其能够适当地使用提炼的辅助信息，从而减少信息丢失。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Conducting extensive experiments to demonstrate the effectiveness of DELRec.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 进行广泛的实验以展示DELRec的有效性。
- en: 2\. PRELIMINARY
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 初步
- en: 2.1\. Task Formulation
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 任务制定
- en: We consider a recommender system with a set of users $U$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个包含用户集合$U$的推荐系统。
- en: Different from conventional SR models, we leverage LLMs to solve the recommendation
    task in an instruction following paradigm. Specifically, for each user $u$.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的SR模型不同，我们利用LLMs在指令跟随范式中解决推荐任务。具体而言，对于每个用户$u$。
- en: 2.2\. Prompt Tuning
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 提示调整
- en: 'Prompt tuning stands out as a sophisticated technique that refines the ability
    of LLMs to conform to specific linguistic tasks and patterns, through allowing
    soft prompts within the prompt $P$ with an emphasis on the target learning objective:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 提示调整是一种精细化的技术，通过允许在提示$P$中使用软提示来提高LLMs适应特定语言任务和模式的能力，重点关注目标学习目标：
- en: '| (1) |  | $1$2 |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $1$2 |  |'
- en: where $\Phi$.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\Phi$。
- en: 2.3\. Parameter Efficient Fine-Tuning
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 参数高效微调
- en: 'The comprehensive fine-tuning of all parameters within LLMs demands considerable
    time and computational resources. To mitigate this issue, the approach of Parameter-Efficient
    Fine-Tuning (PEFT) concentrates on adjusting a minimal subset of parameters, thereby
    reducing computational demands while maintaining notable performance levels. An
    example of a PEFT method is AdaLoRA (Adaptive LoRA), which is a method to optimize
    the number of trainable parameters for weight matrices and layers, unlike LoRA
    which evenly distributes parameters across all modules. It allocates more parameters
    to important weight matrices and layers, while less important ones receive fewer
    parameters. The optimization goal for AdaLoRA is formulated as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLMs进行全面微调所有参数需要大量的时间和计算资源。为了解决这个问题，参数高效微调（PEFT）方法专注于调整最小的参数子集，从而减少计算需求，同时保持显著的性能水平。PEFT方法的一个例子是AdaLoRA（自适应LoRA），这是一种优化权重矩阵和层的可训练参数数量的方法，不同于将参数均匀分配到所有模块的LoRA。它将更多的参数分配给重要的权重矩阵和层，而将较少的重要性分配给较少的参数。AdaLoRA的优化目标如下：
- en: '| (2) |  | $1$2 |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $1$2 |  |'
- en: where AdaLoRA introduces the parameters $\Theta$.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，AdaLoRA引入了参数$\Theta$。
- en: '![Refer to caption](img/1ecbe8efcd0166c9117d676a4c602215.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1ecbe8efcd0166c9117d676a4c602215.png)'
- en: Figure 3\. Illustrating the proposed DELRec that distills the recommendation
    behavior patterns and information of conventional SR models, with soft prompts
    can more easily align with LLMs and facilitate their understanding.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 说明了提出的DELRec，它提炼了传统SR模型的推荐行为模式和信息，通过软提示可以更容易地与LLMs对齐，并促进其理解。
- en: \Description
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: 3\. METHODOLOGY
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 方法学
- en: 'To distill the recommendation behavior patterns of conventional SR models that
    LLMs can understand, and to utilize them in sequential recommendation tasks based
    on LLMs, we propose the DELRec framework, as presented in Figure [3](#S2.F3 "Figure
    3 ‣ 2.3\. Parameter Efficient Fine-Tuning ‣ 2\. PRELIMINARY ‣ DELRec: Distilling
    Sequential Pattern to Enhance LLM-based Recommendation"). Specifically, it involves
    two key stages. In the first stage, We do not directly use discrete hard prompts
    in the whole prompt as usual to add auxiliary information of SR models to LLMs
    or manually describe the recommendation process of SR models. Instead, we insert
    a series of soft prompts into the prompt and freeze the parameters of LLMs, allowing
    LLMs to learn the recommendation information and patterns of conventional SR models
    through our proposed SR Models Pattern Distilling.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提炼传统SR模型的推荐行为模式，使LLMs能够理解，并在基于LLM的顺序推荐任务中利用这些模式，我们提出了DELRec框架，如图[3](#S2.F3
    "图 3 ‣ 2.3\. 参数高效微调 ‣ 2\. 初步 ‣ DELRec：提炼顺序模式以增强基于LLM的推荐")所示。具体而言，它涉及两个关键阶段。在第一阶段，我们不会像往常一样直接在整个提示中使用离散硬提示来向LLMs添加SR模型的辅助信息或手动描述SR模型的推荐过程。相反，我们将一系列软提示插入提示中，并冻结LLMs的参数，使LLMs通过我们提出的SR模型模式提炼来学习传统SR模型的推荐信息和模式。
- en: Then, in the second stage, align the knowledge distilled from the SR models
    with LLM-based recommendation tasks, namely, insert the soft prompts learned in
    the first stage into the prompt and freeze the parameters of soft prompts. Then,
    fine-tune LLMs to make more accurate sequential recommendations using the auxiliary
    information from SR models. We now turn our attention to the specific architecture
    and training approach of DELRec.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在第二阶段，将从SR模型中提炼的知识与基于LLM的推荐任务对齐，即将第一阶段学到的软提示插入提示中，并冻结软提示的参数。然后，微调LLMs，以利用SR模型中的辅助信息进行更准确的顺序推荐。我们现在转向DELRec的具体架构和训练方法。
- en: 3.1\. Hybrid Prompt Construction
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 混合提示构造
- en: First, we will introduce the concepts of hard prompt and soft prompt involved
    in the DELRec framework, as well as the construction of our hybrid prompt.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍DELRec框架中涉及的硬提示和软提示的概念，以及混合提示的构造。
- en: 'Hard Prompt. In conventional LLM recommendation tasks, hard prompts are commonly
    used to construct the prompt or directly provide guidance information within the
    general prompt for LLMs, ($e.g.$ as depicted in Figure [4](#S3.F4 "Figure 4 ‣
    3.1\. Hybrid Prompt Construction ‣ 3\. METHODOLOGY ‣ DELRec: Distilling Sequential
    Pattern to Enhance LLM-based Recommendation") ).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 硬提示。在传统的LLM推荐任务中，硬提示通常用于构造提示或直接在一般提示中提供指导信息（如图[4](#S3.F4 "图 4 ‣ 3.1\. 混合提示构造
    ‣ 3\. 方法论 ‣ DELRec：提炼顺序模式以增强基于LLM的推荐")所示）。
- en: '![Refer to caption](img/9388e19153306bec18883c4151253a24.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9388e19153306bec18883c4151253a24.png)'
- en: Figure 4\. Demonstration of a general prompt that typically relies entirely
    on hard prompts to provide information for LLMs. We use movie recommendations
    as the background for the prompt and SASRec as the example SR model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 展示了一个通常完全依赖硬提示为LLMs提供信息的一般提示。我们以电影推荐作为提示的背景，并以SASRec作为示例SR模型。
- en: 'Hard prompts also known as discrete prompts, are composed of specific vocabulary.
    These prompts are artificially designed and do not change during the training
    process of the LLMs. Explicitly, They are a set of fixed words that instruct the
    models how to perform in specific tasks. Denote hard prompts as $hp_{i}$ is entirely
    constructed by hard prompts:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 硬提示，也称为离散提示，由特定的词汇组成。这些提示是人工设计的，并且在LLMs的训练过程中不会改变。明确地说，它们是一组固定的词汇，用于指导模型如何在特定任务中执行。表示硬提示为$hp_{i}$完全由硬提示构成：
- en: '| (3) |  | $\displaystyle P_{0}=\{hp_{1},hp_{2},...,hp_{l}\},$ |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\displaystyle P_{0}=\{hp_{1},hp_{2},...,hp_{l}\},$ |  |'
- en: 'here, $l$ is processed by the LLM tokenizer and word embedding layer, it will
    become the corresponding embeddings in the language space. We can represent this
    process as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$l$通过LLM分词器和词嵌入层处理后，将变成语言空间中的相应嵌入。我们可以将此过程表示如下：
- en: '| (4) |  | $\displaystyle E_{0}=\sum_{i=1}^{l}f_{tkz}(hp_{i})\in\mathbb{R}^{l\times
    d^{n}},$ |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle E_{0}=\sum_{i=1}^{l}f_{tkz}(hp_{i})\in\mathbb{R}^{l\times
    d^{n}},$ |  |'
- en: where $E_{0}$ indicates the LLM tokenizer and word embedding layer.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$E_{0}$表示LLM分词器和词嵌入层。
- en: 'Soft Prompt. Although hard prompts usually correspond to natural language and
    are easily understood by humans, the purpose of prompt construction is to find
    a method that allows LLMs to effectively perform a task. Rather than being for
    human consumption, it is not necessary to limit the prompt to human-interpretable
    natural language. Therefore, unlike the general prompt $P_{0}$, we will insert
    a portion of soft prompts into the construction of the hybrid prompt, as shown
    in Figure [5](#S3.F5 "Figure 5 ‣ 3.1\. Hybrid Prompt Construction ‣ 3\. METHODOLOGY
    ‣ DELRec: Distilling Sequential Pattern to Enhance LLM-based Recommendation").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '软提示。尽管硬提示通常对应自然语言并且易于理解，但提示构建的目的在于找到一种方法，使 LLM 能够有效地执行任务。提示不一定限于人为可解释的自然语言。因此，与一般提示
    $P_{0}$ 不同，我们将在混合提示的构建中插入部分软提示，如图 [5](#S3.F5 "图 5 ‣ 3.1\. 混合提示构建 ‣ 3\. 方法 ‣ DELRec:
    提炼序列模式以增强基于 LLM 的推荐") 所示。'
- en: '![Refer to caption](img/09c83b987baa0ac833fbf1bc1ec2d939.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/09c83b987baa0ac833fbf1bc1ec2d939.png)'
- en: Figure 5\. Demonstration of a hybrid prompt inserting a series of soft prompts
    when constructs the prompt, and these soft prompts are directly randomly initialized
    as word embeddings.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 演示了在构建提示时插入一系列软提示的混合提示，这些软提示被直接随机初始化为词嵌入。
- en: 'These soft prompts remove the constraint of hard prompts that the embedding
    of the prompt words can only be the embedding of natural language words. These
    soft prompts can be adjusted according to the training data from downstream tasks,
    allowing us to provide some ”only LLMs understand” knowledge to the LLMs in the
    prompt, and this knowledge is difficult or impossible for us to describe in natural
    language. Formally, we denote soft prompts as $sp_{j}$ is constructed by both
    hard and soft prompts:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些软提示去除了硬提示的限制，即提示词的嵌入只能是自然语言词的嵌入。这些软提示可以根据下游任务的训练数据进行调整，使我们能够在提示中向 LLM 提供一些“仅
    LLM 理解”的知识，而这些知识对我们来说难以或不可能用自然语言描述。形式上，我们将软提示表示为 $sp_{j}$，它由硬提示和软提示构成：
- en: '| (5) |  | $1$2 |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $1$2 |  |'
- en: 'where $k$ will also become word embeddings. However, unlike hard prompts that
    will correspond to a fixed position in the language space, soft prompts will be
    processed into randomly initialized embeddings. As LLMs learn the target task,
    the position of the soft prompts in the language space will change:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $k$ 也将成为词嵌入。然而，与硬提示对应语言空间中的固定位置不同，软提示将被处理为随机初始化的嵌入。随着 LLM 学习目标任务，软提示在语言空间中的位置将发生变化：
- en: '| (6) |  | $\displaystyle E_{1}=\sum_{j=1}^{k}f_{iniz}(sp_{j})\in\mathbb{R}^{k\times
    d^{n}},$ |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\displaystyle E_{1}=\sum_{j=1}^{k}f_{iniz}(sp_{j})\in\mathbb{R}^{k\times
    d^{n}},$ |  |'
- en: where $E_{1}$ indicates the process of randomly initializing to the same dimension
    as the word embeddings in the language space of LLMs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $E_{1}$ 表示随机初始化为与 LLM 语言空间中的词嵌入相同的维度的过程。
- en: Prompt Design. In our prompt design, inspired by previous research, since LLMs
    cannot understand the semantic information of id-based item representations well,
    we will use pure text to represent the user’s interaction sequence and candidate
    item set ( $e.g.,$ L.A. Story (1991), Tin Cup (1996), …, Men in Black (1997) ).
    And based on the aforementioned hard prompts and soft prompts, we will design
    various hybrid prompts corresponding to different tasks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 提示设计。在我们的提示设计中，受到先前研究的启发，由于 LLM 无法很好地理解基于 ID 的项表示的语义信息，我们将使用纯文本来表示用户的交互序列和候选项集（$e.g.,$
    《洛杉矶故事》（1991）、《钉子户》（1996）、……、《黑衣人》（1997））。基于上述硬提示和软提示，我们将设计各种混合提示，以对应不同的任务。
- en: Specifically, our prompt design will be different in two stages. In the SR Models
    Pattern Distilling stage, we will design different prompts for each of the two
    components, aiming to better distill the recommendation behavior patterns and
    information of SR models. In the LLMs-based sequential recommendation stage, the
    goal of our prompt design is to enable LLMs to better use the information distilled
    in the first stage to make accurate recommendations. The two stages described
    above are introduced next.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们的提示设计将在两个阶段中有所不同。在 SR 模型模式提炼阶段，我们将为两个组件中的每一个设计不同的提示，旨在更好地提炼 SR 模型的推荐行为模式和信息。在基于
    LLM 的序列推荐阶段，我们的提示设计目标是使 LLM 能够更好地利用第一阶段中提炼的信息进行准确的推荐。上述两个阶段将在接下来的部分中介绍。
- en: 3.2\. SR Models Pattern Distilling
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. SR 模型模式提炼
- en: Previous research has shown that providing LLMs with information from conventional
    recommendation models will enhance the performance of LLMs as recommenders. Inspired
    by this, we will provide LLMs with better, more understandable and usable information.
    To this end, we propose the SR Models Pattern Distilling, and use the soft prompts
    mentioned above to more accurately capture the recommendation behavior patterns
    of conventional SR models for LLMs. Specifically, the SR Models Pattern Distilling
    stage is divided into two components, namely SR Models Temporal Analysis and Recommendation
    Pattern Simulating. Next, we will introduce these in detail one by one.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以往研究表明，向LLMs提供来自传统推荐模型的信息将增强LLMs作为推荐者的表现。受此启发，我们将向LLMs提供更好、更易理解和可用的信息。为此，我们提出了SR模型模式提取，并使用上述软提示更准确地捕捉传统SR模型的推荐行为模式。具体而言，SR模型模式提取阶段分为两个组件，即SR模型时序分析和推荐模式模拟。接下来，我们将逐一详细介绍这些组件。
- en: SR Models Temporal Analysis. Since one of the focuses of SR tasks is to recommend
    items that are temporally closer based on the user’s interaction sequence, which
    exhibits strong temporal dynamics, it is crucial to perform a temporal analysis
    of SR models and providing similar temporal knowledge to LLMs in order to better
    simulate the recommendation patterns of SR models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: SR模型时序分析。由于SR任务的一个重点是根据用户的互动序列推荐时序上更接近的项，这展示了强烈的时序动态，因此对SR模型进行时序分析并向LLMs提供类似的时序知识，以更好地模拟SR模型的推荐模式是至关重要的。
- en: Most SR models ($e.g.$ SASRec) achieve this by aggregating the features of items
    in user interaction sequence to the most recent item in the sequence. Our idea
    is to enable LLMs to similarly recognize and learn the importance of ”the most
    recent item”, thereby acquiring relevant temporal knowledge. Therefore, our proposed
    strategy is to provide the interaction sequence and target item, and let the LLMs
    predict the most recent item in the sequence——a behavior we refer to as PMRI (Predicting
    Most Recent Item).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数SR模型（例如SASRec）通过将用户互动序列中项的特征聚合到序列中最新项来实现这一点。我们的想法是使LLMs类似地识别并学习“最新项”的重要性，从而获取相关的时序知识。因此，我们提出的策略是提供互动序列和目标项，并让LLMs预测序列中的最新项——我们称之为PMRI（预测最新项）。
- en: '![Refer to caption](img/751f29802883923cc3e1dff58fd6b3b1.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/751f29802883923cc3e1dff58fd6b3b1.png)'
- en: Figure 6\. Demonstration of the prompt for SR Models Temporal Analysis. The
    previous part of the interaction sequence is used as ICL to provide LLMs with
    examples that bridge the gap between previous and subsequent parts. LLMs are then
    tasked with PMRI, enabling them to learn a similar process to temporal feature
    aggregation of SR models.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. SR模型时序分析的提示演示。互动序列的前部分被用作ICL，向LLMs提供示例，以弥合前后部分之间的差距。然后，LLMs的任务是PMRI，使其能够学习类似于SR模型的时序特征聚合过程。
- en: 'Specifically, our strategy will allow LLMs to perform PMRI on the sequences
    and we will also provide In-Context Learning (ICL) in an ingenious way to not
    only help LLMs enhance their learning efficiency and quality, but also increase
    LLMs’ awareness of temporal coherence, our strategy is as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们的策略将允许LLMs在序列上执行PMRI，并且我们还将以巧妙的方式提供上下文学习（ICL），不仅帮助LLMs提高学习效率和质量，还增加LLMs对时序连贯性的意识，我们的策略如下：
- en: Given the user interaction sequence $$I_{1:n-1}=(I_{1},I_{2},...,I_{\alpha-1},\\
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 给定用户互动序列$$I_{1:n-1}=(I_{1},I_{2},...,I_{\alpha-1},\\
- en: 'I_{\alpha},...,I_{n-2},I_{n-1})$$, then we inform LLMs that the $\alpha$ are
    optimized by minimizing the loss function of SR Models Temporal Analysis:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: I_{\alpha},...,I_{n-2},I_{n-1})$$，然后我们告知LLMs，$\alpha$通过最小化SR模型时序分析的损失函数来优化：
- en: '| (7) |  | $1$2 |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $1$2 |  |'
- en: where $D_{0}=\{(x_{i}^{0},y_{i}^{0})\}_{i=1,...,N}$ contains the prompt and
    masked item in the aforementioned.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$D_{0}=\{(x_{i}^{0},y_{i}^{0})\}_{i=1,...,N}$包含上述的提示和掩码项。
- en: Recommendation Pattern Simulating. Besides SR Models Temporal Analysis, it is
    also essential for LLMs to be able to simulate conventional SR models in making
    similar recommendations, which enables the distillation from the recommendation
    knowledge of SR models into soft prompts.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐模式模拟。除了SR模型时序分析外，LLMs还需要能够模拟传统SR模型进行类似的推荐，这使得从SR模型的推荐知识中提取软提示成为可能。
- en: '![Refer to caption](img/563c2fe9ba573091dbd7057517334b16.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/563c2fe9ba573091dbd7057517334b16.png)'
- en: Figure 7\. Demonstration of the prompt for Recommendation Pattern Simulating.
    We will use LLMs to learn from the recommendation results of SR models, thereby
    simulating the recommendation patterns of SR models.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 推荐模式模拟的提示演示。我们将使用 LLMs 从 SR 模型的推荐结果中学习，从而模拟 SR 模型的推荐模式。
- en: 'Specifically, we will have LLMs simulate the recommendation patterns of SR
    models as closely as possible and let LLMs predict the recommendation results
    of SR models ( rather than the ground truth ) based on the user interaction sequence.
    This process can be described as: given the user interaction sequence $I_{1:n-1}=(I_{1},I_{2},...,I_{n-1})$,
    LLMs update the parameters of soft prompts, allowing LLMs to fit the results of
    the SR model well. The prompt of the task is shown in Figure [7](#S3.F7 "Figure
    7 ‣ 3.2\. SR Models Pattern Distilling ‣ 3\. METHODOLOGY ‣ DELRec: Distilling
    Sequential Pattern to Enhance LLM-based Recommendation"). Specifically, the loss
    function of Recommendation Pattern Simulating task can be formulated as:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '具体而言，我们将让 LLMs 尽可能接近地模拟 SR 模型的推荐模式，并基于用户交互序列让 LLMs 预测 SR 模型的推荐结果（而不是实际结果）。这个过程可以描述为：给定用户交互序列
    $I_{1:n-1}=(I_{1},I_{2},...,I_{n-1})$，LLMs 更新软提示的参数，使得 LLMs 能够很好地拟合 SR 模型的结果。任务的提示如图
    [7](#S3.F7 "图 7 ‣ 3.2\. SR 模型模式提炼 ‣ 3\. 方法论 ‣ DELRec: 提炼序列模式以增强基于 LLM 的推荐") 所示。具体来说，推荐模式模拟任务的损失函数可以表示为：'
- en: '| (8) |  | $1$2 |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $1$2 |  |'
- en: where $D_{1}=\{(x_{i}^{1},y_{i}^{1})\}_{i=1,...,N}$ consists of the prompt and
    SR models predicted item in the aforementioned Recommendation Pattern Simulating
    step.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D_{1}=\{(x_{i}^{1},y_{i}^{1})\}_{i=1,...,N}$ 包括提示和 SR 模型在上述推荐模式模拟步骤中预测的项目。
- en: 'After obtaining the loss functions for SR Models Temporal Analysis and Recommendation
    Pattern Simulating, we will proceed to update the parameters of soft prompts in
    a multi-task learning (MTL) manner, allowing LLMs to learn from two target tasks
    simultaneously, thereby achieving the distillation of recommendation behavior
    patterns for SR models. The learning objective can be defined as:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得 SR 模型时间分析和推荐模式模拟的损失函数后，我们将进行多任务学习（MTL）方式更新软提示的参数，使 LLMs 能够同时学习两个目标任务，从而实现
    SR 模型的推荐行为模式的提炼。学习目标可以定义为：
- en: '| (9) |  | $1$2 |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $1$2 |  |'
- en: where $\lambda_{1}$ during training.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{1}$ 在训练过程中。
- en: 3.3\. LLMs-based Sequential Recommendation
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 基于 LLMs 的序列推荐
- en: 'In the first stage (SR Models Pattern Distilling), we successfully distilled
    the recommendation patterns from the SR models. In previous research, to enable
    LLMs to utilize auxiliary information from conventional SR models (such as item
    embeddings), people often used projectors ($e.g.,$ MLP, Tiny Transformers) to
    map the embeddings into the language space of LLMs. However, this approach often
    suffers from poorly designed projectors, which may fail to fully convey the information
    embedded in the original embeddings to LLMs or limit their generalization capabilities,
    etc. Therefore, the soft prompts we distilled can achieve plug-and-play and overcome
    these issues. The prompt is shown in Figure [8](#S3.F8 "Figure 8 ‣ 3.3\. LLMs-based
    Sequential Recommendation ‣ 3\. METHODOLOGY ‣ DELRec: Distilling Sequential Pattern
    to Enhance LLM-based Recommendation").'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '在第一阶段（SR 模型模式提炼）中，我们成功地从 SR 模型中提炼了推荐模式。在之前的研究中，为了使 LLMs 能够利用来自传统 SR 模型的辅助信息（如项目嵌入），人们通常使用投影器（例如，MLP、Tiny
    Transformers）将嵌入映射到 LLMs 的语言空间。然而，这种方法通常会受到设计不佳的投影器的困扰，这可能导致无法充分传达原始嵌入中的信息给 LLMs
    或限制它们的泛化能力等。因此，我们提炼的软提示可以实现即插即用，并克服这些问题。提示如图 [8](#S3.F8 "图 8 ‣ 3.3\. 基于 LLMs 的序列推荐
    ‣ 3\. 方法论 ‣ DELRec: 提炼序列模式以增强基于 LLM 的推荐") 所示。'
- en: '![Refer to caption](img/d04d8809104a2e30bc92f8cac8fba59f.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d04d8809104a2e30bc92f8cac8fba59f.png)'
- en: Figure 8\. Demonstration of the prompt for LLMs-based Sequential Recommendation.
    We will provide LLMs with the recommendation patterns and information of SR models
    ($i.e.,$ the soft prompts) distilled from the first phase and guide the LLMs to
    use this auxiliary information to predict the ground truth.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 基于 LLMs 的序列推荐的提示演示。我们将向 LLMs 提供从第一阶段提炼的 SR 模型的推荐模式和信息（即，软提示），并指导 LLMs
    利用这些辅助信息预测真实结果。
- en: 'Specifically, we directly incorporate the learned soft prompts $sp_{1:k}=(sp_{1},sp_{2},...,sp_{k})$:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们直接引入学习到的软提示 $sp_{1:k}=(sp_{1},sp_{2},...,sp_{k})$：
- en: '| (10) |  | $\displaystyle I_{n}=LLM(P1(sp_{1:k}))$ |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\displaystyle I_{n}=LLM(P1(sp_{1:k}))$ |  |'
- en: where $P1(\cdot)$ indicates that LLMs utilize the prompt to perform SR tasks.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P1(\cdot)$ 表示 LLM 使用提示来执行 SR 任务。
- en: 'LLMs Fine-tuning. However, considering there may be noise or harmful information
    in soft prompts, which may guide LLMs to make predictions that are more inclined
    to SR models’ predictions than to ground truth. Therefore, we need to fine-tune
    the parameters of LLMs to guide them to regard soft prompts as reference more.
    Formally, given a user interaction sequence $I_{1:n-1}=(I_{1},I_{2},...,I_{n-1})$
    and fine-tune the LLMs using PEFT (AdaLora). Formally, the learning objectives
    of the LLMs-based Sequential Recommendation can be described as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 微调。然而，考虑到软提示中可能存在噪音或有害信息，这可能导致 LLMs 更倾向于 SR 模型的预测而不是实际情况。因此，我们需要微调 LLMs
    的参数，以引导它们更多地将软提示视为参考。正式地，给定一个用户互动序列 $I_{1:n-1}=(I_{1},I_{2},...,I_{n-1})$，并使用
    PEFT（AdaLora）对 LLMs 进行微调。正式地，基于 LLM 的序列推荐的学习目标可以描述如下：
- en: '| (11) |  | $1$2 |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $1$2 |  |'
- en: where $\overline{D}=\{(\overline{x_{i}},\overline{y_{i}})\}_{i=1,...,N}$.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\overline{D}=\{(\overline{x_{i}},\overline{y_{i}})\}_{i=1,...,N}$。
- en: 4\. EXPERIMENTS
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. 实验
- en: In this section, we assess the performance of our proposed framework, DELRec,
    on three real-world datasets. We compare it against various baselines, including
    conventional SR models and LLMs-based models.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了我们提出的框架DELRec在三个实际数据集上的表现。我们将其与各种基线进行比较，包括传统的 SR 模型和基于 LLM 的模型。
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ1: Whether the proposed framework outperforms baseline methods, including
    the deep learning models and other LLM based models, for SR?'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ1：提出的框架是否优于包括深度学习模型和其他基于 LLM 的模型在内的基线方法？
- en: •
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ2: Are our proposed DELRec able to learn meaningful recommendation behavior
    patterns or information?'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ2：我们提出的DELRec能否学习到有意义的推荐行为模式或信息？
- en: •
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ3: How can key components affect our proposed method. Specifically, how is
    the efficacy of the proposed SR Models Temporal Analysis and Recommendation Pattern
    Simulating?'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ3：关键组件如何影响我们提出的方法。具体来说，提出的 SR 模型的时间分析和推荐模式模拟的有效性如何？
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ4: How do hyperparameters influence DELRec?'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ4：超参数如何影响 DELRec？
- en: 4.1\. Setup
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1. 设置
- en: 4.1.1\. Datasets.
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1. 数据集
- en: We evaluate the proposed DELRec and baseline methods on three real-world datasets
    in sequential recommendations, namely MovieLens-1M and Beauty, as well as Steam.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在三个实际数据集（即 MovieLens-1M、Beauty 和 Steam）上评估了提出的 DELRec 和基线方法在序列推荐中的表现。
- en: •
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MovieLens-1M is a commonly used movie recommendation dataset that includes ratings
    given by users to movies and the titles of those movies.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MovieLens-1M 是一个常用的电影推荐数据集，包括用户对电影的评分和电影的标题。
- en: •
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Beauty is a dataset containing user feedback on beauty products from Amazon
    website.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Beauty 是一个包含来自亚马逊网站的美容产品用户反馈的数据集。
- en: •
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Steam not only contains user reviews of video games on the Steam Store, but
    also covers a variety of game titles.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Steam 不仅包含 Steam 商店上关于视频游戏的用户评论，还涵盖了各种游戏标题。
- en: 'We show the detailed statistics of the datasets in Table [1](#S4.T1 "Table
    1 ‣ 4.1.1\. Datasets. ‣ 4.1\. Setup ‣ 4\. EXPERIMENTS ‣ DELRec: Distilling Sequential
    Pattern to Enhance LLM-based Recommendation"). For all datasets, we follow [SASRec]
    in treating users’ implicit feedback as interactions between users and items,
    and determine the sequence order of inputs based on timestamps. Subsequently,
    we filter out users and items with fewer than 5 interactions. Meanwhile, we arrange
    them in chronological order as [LLaRA] do, and divide the data into training,
    validation, and test sets in an 8:1:1 ratio. This division method ensures that
    interactions used for training do not appear in subsequent data, thereby avoiding
    any potential information leakage.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表[1](#S4.T1 "Table 1 ‣ 4.1.1\. Datasets. ‣ 4.1\. Setup ‣ 4\. EXPERIMENTS
    ‣ DELRec: Distilling Sequential Pattern to Enhance LLM-based Recommendation")中展示了数据集的详细统计信息。对于所有数据集，我们按照[SASRec]将用户的隐式反馈视为用户与项目之间的互动，并根据时间戳确定输入的序列顺序。随后，我们筛选出互动次数少于
    5 次的用户和项目。同时，我们按照[LLaRA]的做法按时间顺序排列这些数据，并将数据分为训练集、验证集和测试集，比例为 8:1:1。这种划分方法确保了用于训练的互动不会出现在后续数据中，从而避免了潜在的信息泄漏。'
- en: Table 1\. Statistics of Datasets
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. 数据集统计
- en: '| Dataset | $\#$interaction |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | $\#$互动 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| MovieLens | 6,040 | 3,416 | 100,000 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| MovieLens | 6,040 | 3,416 | 100,000 |'
- en: '| Steam | 11,938 | 3,581 | 274,726 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Steam | 11,938 | 3,581 | 274,726 |'
- en: '| Beauty | 22,363 | 12,099 | 198,474 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Beauty | 22,363 | 12,099 | 198,474 |'
- en: 4.1.2\. Baselines.
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2. 基线
- en: To demonstrate the effectiveness of our DELRec framework, we use two types of
    baselines.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们DELRec框架的有效性，我们使用了两种基准。
- en: •
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Conventional SR Models: The first type includes conventional SR models: GRU4Rec[16]
    (based on RNN), Caser[44] (based on CNN) and SASRec[25] (based on attention),
    which are often used as standard comparisons.'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 传统的 SR 模型：第一类包括传统的 SR 模型：GRU4Rec[16]（基于 RNN）、Caser[44]（基于 CNN）和 SASRec[25]（基于注意力机制），这些模型常被用作标准比较。
- en: •
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLMs-based Models: The second type of baseline includes: (1) Bert-Large is
    a milestone LLM capable of performing Masked language modeling (MLM) tasks. (2)
    FLan-T5-Large/XXL are well-known open-source LLMs with an encoder-decoder structure.
    (3) LlamaRec uses a traditional model to recall items and construct a candidate
    set and a verbalizer to directly output item rankings. (4) RecRanker cleverly
    samples items and users and inputs the results of conventional recommendation
    models into the prompt. (5) LLaRA inserts the embedding of items encoded by the
    SR model into the prompt. For the validity of the experiment, we have replaced
    the backbone of LLMs-based baselines with FLan-T5-XL.'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于 LLMs 的模型：第二类基准包括：（1）Bert-Large 是一个里程碑式的 LLM，能够执行掩码语言建模（MLM）任务。（2）FLan-T5-Large/XXL
    是知名的开源 LLM，具有编码器-解码器结构。（3）LlamaRec 使用传统模型来回忆项目并构建候选集和 verbalizer 以直接输出项目排名。（4）RecRanker
    智巧地抽样项目和用户，并将传统推荐模型的结果输入提示中。（5）LLaRA 将 SR 模型编码的项目嵌入到提示中。为了实验的有效性，我们已经将基于 LLMs
    的基准模型的基础更换为 FLan-T5-XL。
- en: 4.1.3\. Implementation Details.
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 实施细节。
- en: For our proposed DELRec framework, we choose FLan-T5-XL as our backbone and
    we will also use FLAN-T5-Large to perform ablation experiments. It’s worth noting
    that the backbone of our proposed framework can also use open-source Decoder-Only
    structured LLMs, such as Llama2[Llama], and is not constrained by the types of
    LLMs. For the training of conventional SR models, we use the Adam optimizer, with
    a learning rate of 1e-3 and a batch size of 128\. For the first stage of DELRec
    (SR Models Pattern Distilling), for the length of user interaction sequences $n$
    as in the first stage, and also use AdaLoRA and Lion optimizer, with a learning
    rate of 1e-4 and weight decay of 1e-6.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们提出的 DELRec 框架，我们选择 FLan-T5-XL 作为我们的基础，并且我们还将使用 FLAN-T5-Large 进行消融实验。值得注意的是，我们提出的框架的基础也可以使用开源的仅解码器结构
    LLMs，例如 Llama2[Llama]，并且不受限于 LLM 的类型。对于传统 SR 模型的训练，我们使用 Adam 优化器，学习率为 1e-3，批次大小为
    128。对于 DELRec 的第一阶段（SR 模型模式提炼），用户交互序列的长度 $n$ 以及使用 AdaLoRA 和 Lion 优化器，学习率为 1e-4，权重衰减为
    1e-6。
- en: 4.1.4\. Evaluation Metrics.
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4\. 评价指标。
- en: For ranking evaluation, we use top-$k$5.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于排名评估，我们使用 top-$k$5。
- en: Table 2\. Overall Performance
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 总体性能
- en: '|  |  | MovieLens-1M | Steam | Beauty |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MovieLens-1M | Steam | Beauty |'
- en: '|  |  | HR@1 | HR@5 | HR@1 | HR@5 | HR@1 | HR@5 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  |  | HR@1 | HR@5 | HR@1 | HR@5 | HR@1 | HR@5 |'
- en: '|  | Caser | 0.3150 | 0.6340 | 0.3767 | 0.6680 | 0.2241 | 0.4187 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | Caser | 0.3150 | 0.6340 | 0.3767 | 0.6680 | 0.2241 | 0.4187 |'
- en: '| Conventional | GRU4Rec | 0.3062 | 0.6295 | 0.3786 | 0.6835 | 0.2369 | 0.4544
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 传统 | GRU4Rec | 0.3062 | 0.6295 | 0.3786 | 0.6835 | 0.2369 | 0.4544 |'
- en: '|  | SASRec | 0.3341 | 0.6704 | 0.3852 | 0.6977 | 0.2573 | 0.4629 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | SASRec | 0.3341 | 0.6704 | 0.3852 | 0.6977 | 0.2573 | 0.4629 |'
- en: '|  | Bert-Large | 0.0306 | 0.0821 | 0.0201 | 0.0424 | 0.0166 | 0.0354 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | Bert-Large | 0.0306 | 0.0821 | 0.0201 | 0.0424 | 0.0166 | 0.0354 |'
- en: '|  | Flan-T5-Large | 0.0375 | 0.0703 | 0.0240 | 0.0493 | 0.0195 | 0.0346 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | Flan-T5-Large | 0.0375 | 0.0703 | 0.0240 | 0.0493 | 0.0195 | 0.0346 |'
- en: '| LLMs-based | Flan-T5-XL | 0.0938 | 0.2441 | 0.0723 | 0.1662 | 0.0652 | 0.1071
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 基于 LLMs | Flan-T5-XL | 0.0938 | 0.2441 | 0.0723 | 0.1662 | 0.0652 | 0.1071
    |'
- en: '|  | LlamaRec | 0.2870 | 0.5873 | 0.3511 | 0.6478 | 0.2361 | 0.4418 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | LlamaRec | 0.2870 | 0.5873 | 0.3511 | 0.6478 | 0.2361 | 0.4418 |'
- en: '|  | RecRanker | 0.3246 | 0.6292 | 0.3724 | 0.6537 | 0.2670 | 0.4943 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | RecRanker | 0.3246 | 0.6292 | 0.3724 | 0.6537 | 0.2670 | 0.4943 |'
- en: '|  | LLaRA | 0.3523 | 0.6553 | 0.4035 | 0.6911 | 0.3152 | 0.6063 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaRA | 0.3523 | 0.6553 | 0.4035 | 0.6911 | 0.3152 | 0.6063 |'
- en: '|  | DALRec (Caser) | 0.3664 | 0.6804 | 0.4157 | 0.6946 | 0.3249 | 0.6175 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | DALRec (Caser) | 0.3664 | 0.6804 | 0.4157 | 0.6946 | 0.3249 | 0.6175 |'
- en: '| Ours | DALRec (GRU4Rec) | 0.3635 | 0.6722 | 0.4296 | 0.7099 | 0.3413 | 0.6229
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | DALRec (GRU4Rec) | 0.3635 | 0.6722 | 0.4296 | 0.7099 | 0.3413 | 0.6229
    |'
- en: '|  | DALRec (SASRec) | 0.3701 | 0.6919 | 0.4372 | 0.7285 | 0.3477 | 0.6513
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | DALRec (SASRec) | 0.3701 | 0.6919 | 0.4372 | 0.7285 | 0.3477 | 0.6513
    |'
- en: 4.2\. Performance Comparison (RQ1)
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 性能比较（RQ1）
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.1.4\. Evaluation Metrics. ‣ 4.1\. Setup ‣ 4\.
    EXPERIMENTS ‣ DELRec: Distilling Sequential Pattern to Enhance LLM-based Recommendation")
    presents the performance of our method DELRec and various baselines under three
    evaluation metrics. Comparing DELRec with the aforementioned baseline models,
    we can derive the following observations.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S4.T2 "表2 ‣ 4.1.4\. 评估指标 ‣ 4.1\. 设置 ‣ 4\. 实验 ‣ DELRec：提取序列模式以增强基于LLM的推荐")展示了我们的方法DELRec与各种基线在三项评估指标下的表现。将DELRec与上述基线模型进行比较，我们可以得出以下观察结论。
- en: •
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DELRec outperforms all baseline models on the MovieLens-1M, Beauty, and Steam
    datasets. It achieves the highest HR@1, HR@5, and NDCG@5 scores compared to conventional
    SR models that only recommend through user interactions or LLMs that lack recommendation
    knowledge. The key reason behind this superior performance is that DELRec effectively
    combines the information from conventional SR models with the powerful reasoning
    capabilities and extensive world knowledge of LLMs to complete more accurate recommendations.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DELRec在MovieLens-1M、Beauty和Steam数据集上的表现优于所有基线模型。与仅通过用户交互推荐的传统SR模型或缺乏推荐知识的LLMs相比，它在HR@1、HR@5和NDCG@5评分上达到了最高水平。这种卓越表现的关键原因在于DELRec有效结合了传统SR模型的信息与LLMs强大的推理能力和广泛的世界知识，从而提供了更准确的推荐。
- en: •
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When comparing with some original open-source LLMs ($e.g.$, BERT, Flan-T5),
    it is evident that these baseline models not only underperform DELRec in recommendation
    tasks but also exhibit lower metrics compared to conventional SR models and other
    LLMs-based recommendation methods. The reason behind this discrepancy lies in
    the fact that while these LLMs possess strong generalization capabilities, they
    lack domain-specific knowledge and understanding of recommendation patterns, which
    hinders their performance in recommendation tasks. Therefore, providing appropriate
    auxiliary information to adapt LLMs to specific recommendation tasks becomes crucial.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与一些原始开源LLMs（例如，BERT、Flan-T5）相比，这些基线模型不仅在推荐任务中表现不如DELRec，而且与传统SR模型和其他基于LLMs的推荐方法相比，指标也较低。这种差异的原因在于，虽然这些LLMs具备强大的泛化能力，但缺乏领域特定的知识和对推荐模式的理解，这限制了它们在推荐任务中的表现。因此，为LLMs提供适当的辅助信息以适应特定推荐任务变得至关重要。
- en: •
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When considering the other LLMS-based improvements we have chosen, the reasons
    for DELRec’s superior performance can be analyzed from several perspectives. Firstly,
    while some methods enable LLMs to perform recommendation tasks ($e.g.$, LLaRA),
    while containing some pattern information from SR models, suffer from loss of
    information due to inconsistent dimensions and may not align perfectly with the
    linguistic meaning of LLMs, resulting in lower performance compared to DELRec.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在考虑我们选择的其他基于LLMS的改进时，DELRec卓越性能的原因可以从多个角度分析。首先，尽管一些方法使LLMs能够执行推荐任务（例如，LLaRA），并且包含了一些来自SR模型的模式信息，但由于维度不一致而导致信息丢失，并且可能与LLMs的语言意义不完全一致，导致性能低于DELRec。
- en: 4.3\. Ablation Studies (RQ2 & RQ3)
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 消融研究（RQ2 & RQ3）
- en: 'To address RQ2, we conducted an experiment on the soft prompts distilled in
    the initial stage of DELRec and we use SASRec as the backbone model. As these
    soft prompts do not correspond to natural language and are not easily interpretable
    by humans, we performed three transformations on a portion of the soft prompts
    to verify if they truly capture meaningful recommendation behavior patterns or
    information. These transformations include:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决RQ2，我们对DELRec初始阶段提取的软提示进行了实验，使用SASRec作为骨干模型。由于这些软提示不对应自然语言且不易被人类解读，我们对部分软提示进行了三种转换，以验证它们是否真正捕捉到有意义的推荐行为模式或信息。这些转换包括：
- en: Table 3\. Ablation analysis for learned soft prompts on three datasets (HR@1).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. 对三个数据集上学习的软提示的消融分析（HR@1）。
- en: '|  | MovieLens-1M | Steam | Beauty |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | MovieLens-1M | Steam | Beauty |'
- en: '| --- | --- | --- | --- |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| No Soft Prompts | 0.3020 | 0.3426 | 0.2965 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 无软提示 | 0.3020 | 0.3426 | 0.2965 |'
- en: '| Manual Construction | 0.3106 | 0.3608 | 0.2898 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 手动构建 | 0.3106 | 0.3608 | 0.2898 |'
- en: '| Random Soft Prompts | 0.2752 | 0.2977 | 0.2284 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 随机软提示 | 0.2752 | 0.2977 | 0.2284 |'
- en: '| Default | 0.3701 | 0.4372 | 0.3477 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 默认 | 0.3701 | 0.4372 | 0.3477 |'
- en: •
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'No Soft Prompts: We removed the soft prompts section and the part of instruction
    that directs LLMs to refer to auxiliary information from the SR models.'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无软提示：我们移除了软提示部分以及指示LLMs参考SR模型辅助信息的部分指令。
- en: •
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Manual Construction: Similar to the general prompt where hard prompts are used
    to construct auxiliary information, for constructing auxiliary information, we
    attempted to describe the recommendation process of SASRec model in natural language
    and replaced the original soft prompts with it.'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 手动构建：类似于使用硬提示构建辅助信息的一般提示，我们尝试用自然语言描述SASRec模型的推荐过程，并将原来的软提示替换为这种描述。
- en: •
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Random Soft Prompts: Soft prompts that have not undergone distillation in the
    first stage were directly initialized randomly and inserted into our prompt.'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机软提示：未经过提炼的软提示在第一阶段被随机初始化并插入到我们的提示中。
- en: 'Finally, the three transformed methods are compared with the complete DELRec
    after fine-tuning in the second stage. Table [3](#S4.T3 "Table 3 ‣ 4.3\. Ablation
    Studies (RQ2 & RQ3) ‣ 4\. EXPERIMENTS ‣ DELRec: Distilling Sequential Pattern
    to Enhance LLM-based Recommendation") shows the measurement metrics of DELRec
    under four different conditions. Based on our observations, we make the following
    inferences.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，将三种转换方法与第二阶段微调后的完整DELRec进行比较。表[3](#S4.T3 "Table 3 ‣ 4.3\. Ablation Studies
    (RQ2 & RQ3) ‣ 4\. EXPERIMENTS ‣ DELRec: Distilling Sequential Pattern to Enhance
    LLM-based Recommendation")展示了DELRec在四种不同条件下的测量指标。根据我们的观察，我们做出如下推断。'
- en: •
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In methods that solely utilize pure hard prompts without soft prompts or manual
    construction, the Manual Construction method enhances LLMs by describing the recommendation
    patterns of the SR model in nature language. However, due to inaccuracies or insufficient
    information in these descriptions, the metrics of this method only show slight
    improvements compared to the No Soft Prompts method.
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在仅使用纯硬提示而没有软提示或手动构建的方法中，手动构建方法通过自然语言描述SR模型的推荐模式来增强LLMs。然而，由于这些描述的准确性或信息不足，该方法的指标与无软提示方法相比仅有轻微改进。
- en: •
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Among the few baselines we selected, the Random Soft Prompts method performs
    poorly in terms of metrics. This can be attributed to random soft prompts being
    scattered throughout the semantic space with no meaningful context, resulting
    in strong noise and providing little assistance or potentially misleading LLMs.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们选择的少数基准中，随机软提示方法在指标方面表现不佳。这是因为随机软提示在语义空间中分散，没有有意义的上下文，导致强噪声，提供的帮助很少或可能误导LLMs。
- en: •
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Soft prompts that have undergone our designed SR Models Pattern Distilling approach
    surpass all three methods mentioned above. This indicates that our distillation
    method is able to effectively extract valuable recommendation patterns and information
    from SR models for LLMs.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 经我们设计的SR模型模式提炼方法处理过的软提示超过了上述所有三种方法。这表明，我们的提炼方法能够有效提取SR模型中有价值的推荐模式和信息，以供LLMs使用。
- en: 'We will verify the impact of various components in DELRec on the framework
    through the following ablation experiments (RQ3). The results are shown in Table
    [4](#S4.T4 "Table 4 ‣ 4.3\. Ablation Studies (RQ2 & RQ3) ‣ 4\. EXPERIMENTS ‣ DELRec:
    Distilling Sequential Pattern to Enhance LLM-based Recommendation"). We introduce
    the variants and analyze their effect respectively:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将通过以下消融实验（RQ3）验证DELRec中各种组件对框架的影响。结果见表[4](#S4.T4 "Table 4 ‣ 4.3\. Ablation
    Studies (RQ2 & RQ3) ‣ 4\. EXPERIMENTS ‣ DELRec: Distilling Sequential Pattern
    to Enhance LLM-based Recommendation")。我们介绍了这些变体，并分别分析了它们的效果：'
- en: •
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'w/o SMPD (SR Models Pattern Distilling): By eliminating the process of distilling
    recommendation behavior patterns from SR models in the first stage of DELRec,
    we observed a decline in performance. This is because LLMs lack auxiliary information
    from SR models, which hinders their ability to guide the recommendation process
    effectively.'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: w/o SMPD（SR模型模式提炼）：通过取消DELRec第一阶段中从SR模型提炼推荐行为模式的过程，我们观察到性能下降。这是因为LLMs缺乏来自SR模型的辅助信息，阻碍了它们有效指导推荐过程的能力。
- en: •
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'w/o LSR (LLMs-based Sequential Recommendation): After distillation in the first
    stage, excluding the fine-tuning process of LLMs in the second stage resulted
    in a decrease in metrics. This can be attributed to using information extracted
    directly from SR models, which introduces noise that may interfere with LLM recommendations.
    Additionally, LLMs are more likely to favor the items predicted by the SR model
    rather than the ground truth.'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无 LSR (基于 LLM 的顺序推荐)：在第一阶段提炼后，第二阶段排除 LLM 的微调过程导致指标下降。这可以归因于直接使用从 SR 模型中提取的信息，这引入了可能干扰
    LLM 推荐的噪声。此外，LLM 更倾向于青睐 SR 模型预测的项目，而不是实际情况。
- en: •
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'w/o SMTA (SR Models Temporal Analysis): Removing SR Models Temporal Analysis
    during the first stage leads to distilled soft prompts lacking temporal characteristics.
    As a result, there is insufficient guidance for LLMs to mimic feature aggregation
    processes similar to those employed by SR models.'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无 SMTA (SR 模型时间分析)：在第一阶段移除 SR 模型时间分析导致提炼出的软提示缺乏时间特征。因此，LLM 在模拟类似 SR 模型的特征聚合过程时缺乏足够的指导。
- en: •
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'w/o RPS (Recommendation Pattern Simulating): Eliminating Recommendation Pattern
    Simulating during the first stage disrupts alignment between prediction results
    of LLMs and those of SR models. Consequently, it becomes challenging for LLMs
    to effectively simulate overall recommendation behavior patterns exhibited by
    SR models.'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无 RPS (推荐模式模拟)：在第一阶段消除推荐模式模拟会干扰 LLM 预测结果与 SR 模型预测结果之间的对齐。因此，LLM 很难有效模拟 SR 模型所表现出的整体推荐行为模式。
- en: •
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'w Flan-T5-Large: In addition to conducting ablation experiments on components
    within DELRec framework, we also explored using Flan-T5-Large as a smaller-scale
    backbone language model within our framework. The experimental results indicated
    that both size and capacity of LLMs have an impact on DELRec’s performance.'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: w Flan-T5-Large：除了对 DELRec 框架内的组件进行消融实验外，我们还探索了在我们的框架内使用 Flan-T5-Large 作为较小规模的主干语言模型。实验结果表明，LLM
    的大小和能力对 DELRec 的性能有影响。
- en: Table 4\. Ablation analysis (HR@1) on three datasets.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 在三个数据集上的消融分析 (HR@1)。
- en: '|  | MovieLens-1M | Steam | Beauty |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | MovieLens-1M | Steam | Beauty |'
- en: '| --- | --- | --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| w/o SMPD | 0.3020 | 0.3426 | 0.2965 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 无 SMPD | 0.3020 | 0.3426 | 0.2965 |'
- en: '| w/o LSR | 0.2814 | 0.3235 | 0.2666 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 无 LSR | 0.2814 | 0.3235 | 0.2666 |'
- en: '| w/o SMTA | 0.3425 | 0.3710 | 0.2949 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 无 SMTA | 0.3425 | 0.3710 | 0.2949 |'
- en: '| w/o RPS | 0.3379 | 0.3555 | 0.3103 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 无 RPS | 0.3379 | 0.3555 | 0.3103 |'
- en: '| w Flan-T5-Large | 0.2592 | 0.3018 | 0.2384 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| w Flan-T5-Large | 0.2592 | 0.3018 | 0.2384 |'
- en: '| Default | 0.3701 | 0.4372 | 0.3477 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 默认 | 0.3701 | 0.4372 | 0.3477 |'
- en: 4.4\. Hyperparameter Analysis (RQ4)
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 超参数分析 (RQ4)
- en: We will conduct experiments on the hyperparameters in our proposed DELRec, including
    soft prompts size $k$, SASRec).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对我们提出的 DELRec 中的超参数进行实验，包括软提示的大小 $k$、SASRec)。
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Soft Prompts Size: Regarding the size of soft prompts, we examined its impact
    on DELRec’s performance. As depicted in Figure [9](#S4.F9 "Figure 9 ‣ 1st item
    ‣ 4.4\. Hyperparameter Analysis (RQ4) ‣ 4\. EXPERIMENTS ‣ DELRec: Distilling Sequential
    Pattern to Enhance LLM-based Recommendation"), we observed that DELRec’s performance
    metrics initially improve with an increase in $k$. However, after reaching a certain
    value, these metrics start to level off. This can be attributed to the fact that
    while soft prompts enhance prompt information through LLMs’ learning process,
    an excessive amount of soft prompts may introduce noise or potentially lead to
    overfitting. Consequently, after soft prompts reach a certain size, they will
    not significantly contribute to the improvement of overall performance.'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '软提示大小：关于软提示的大小，我们检查了其对 DELRec 性能的影响。如图 [9](#S4.F9 "图 9 ‣ 第 1 项 ‣ 4.4\. 超参数分析
    (RQ4) ‣ 4\. 实验 ‣ DELRec: 提炼顺序模式以增强基于 LLM 的推荐") 所示，我们观察到 DELRec 的性能指标随着 $k$ 的增加而最初提高。然而，在达到某个值后，这些指标开始趋于平稳。这可以归因于软提示通过
    LLM 的学习过程增强了提示信息，但过多的软提示可能会引入噪声或可能导致过拟合。因此，在软提示达到一定大小后，它们对整体性能的改善不会有显著贡献。'
- en: '![Refer to caption](img/c661d43391f996bbaab62615c22f0bef.png)'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/c661d43391f996bbaab62615c22f0bef.png)'
- en: Figure 9\. Performance comparison w.r.t different soft prompts size $k$ for
    training DELRec on the three datasets.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9\. 关于不同软提示大小 $k$ 的 DELRec 训练性能比较。
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Recommended Items Size: We investigated how the overall performance changes
    with varying sizes $h$ and overall performance. The variation observed can be
    explained by considering that providing SR model-recommended items helps LLMs
    understand recommendation patterns. However, if too large recommended items size
    is set, it may not only mislead LLMs but also result in excessively long prompts
    which could potentially impact LLMs’ attention mechanism.'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推荐项大小：我们调查了整体性能如何随不同大小$h$的变化而变化。观察到的变化可以通过考虑提供SR模型推荐的项目帮助LLMs理解推荐模式来解释。然而，如果设置的推荐项大小过大，可能不仅会误导LLMs，还会导致过长的提示，这可能会影响LLMs的注意力机制。
- en: '![Refer to caption](img/aeac882b6fb1d20473f6e28e4288322f.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/aeac882b6fb1d20473f6e28e4288322f.png)'
- en: Figure 10\. Performance comparison w.r.t different recommended items size $h$
    for training DELRec on the three datasets.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图10\. 针对在三个数据集上训练DELRec的不同推荐项大小$h$的性能比较。
- en: 4.5\. Case Study
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. 案例研究
- en: In order to further investigate the effectiveness of integrating recommendation
    behavior patterns from SR models with the world knowledge of LLMs, we conducted
    a comparative case study among FLan-T5-XL, SASRec, and DELRec.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探讨将SR模型中的推荐行为模式与LLMs的世界知识相结合的有效性，我们在FLan-T5-XL、SASRec和DELRec之间进行了比较案例研究。
- en: 'Here we choose a distinct example. For a user with a movie viewing history
    that includes ”American Beauty (1999)”, ”Legends of the Fall (1994)”, ”Gladiator
    (2000)”, ”Out of Sight (1998)”, ”GoldenEye (1995)”, ”Mission: Impossible (1996)”,
    ”Malice (1993)”, ”Amistad (1997)”, ”Jurassic Park (1993)” and ”Men in Black (1997)”.
    We have utilized Flan-T5-XL, SASRec, and DELRec to generate recommendations for
    this particular user.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们选择了一个不同的例子。对于一个电影观看历史包括“美国美人（1999年）”、“西部往事（1994年）”、“角斗士（2000年）”、“消失的视线（1998年）”、“黄金眼（1995年）”、“碟中谍（1996年）”、“恶意（1993年）”、“安史之乱（1997年）”、“侏罗纪公园（1993年）”和“黑衣人（1997年）”的用户，我们利用了Flan-T5-XL、SASRec和DELRec为该用户生成推荐。
- en: 'As shown in Figure [11](#S4.F11 "Figure 11 ‣ 4.5\. Case Study ‣ 4\. EXPERIMENTS
    ‣ DELRec: Distilling Sequential Pattern to Enhance LLM-based Recommendation"),
    we observe that based on the knowledge contained in Flan-T5-XL, it recommended
    the sequel film ”Men in Black II (2002)” to the user since their last watched
    movie was ”Men in Black (1997)”. On the other hand, SASRec predicted recommendations
    by considering the user’s most recent viewing history and suggested an action/sci-fi
    film called ”Aliens (1986)” which aligns with the theme of ”Men in Black (1997)”.
    In contrast, DELRec combined conventional recommendation patterns with rich world
    knowledge. It took into account the changing preferences of users from drama/classic
    to action/sci-fi genres and it recommended ”Back to the Future (1985)”, which
    indeed was the next interaction by the user.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[11](#S4.F11 "图11 ‣ 4.5\. 案例研究 ‣ 4\. 实验 ‣ DELRec：提炼序列模式以增强基于LLM的推荐")所示，我们观察到基于Flan-T5-XL所包含的知识，它向用户推荐了续集电影“黑衣人2（2002年）”，因为他们最后观看的电影是“黑衣人（1997年）”。另一方面，SASRec通过考虑用户的最新观看历史来预测推荐，并建议了一部名为“异形（1986年）”的动作/科幻电影，这与“黑衣人（1997年）”的主题相符。相比之下，DELRec结合了传统推荐模式和丰富的世界知识。它考虑了用户从戏剧/经典到动作/科幻类型的偏好变化，推荐了“回到未来（1985年）”，这确实是用户的下一个互动。
- en: '![Refer to caption](img/83701a0e38e3db486eb92a366a3f5188.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/83701a0e38e3db486eb92a366a3f5188.png)'
- en: 'Figure 11\. Case study comparison results of the effectiveness of three models
    in recommending movies: FLan-T5-XL, SASRec and DELRec.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图11\. 三种模型在电影推荐中的效果比较：FLan-T5-XL、SASRec和DELRec。
- en: 5\. RELATED WORK
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 相关工作
- en: In this section, we provide a literature review on LLMs for Sequential Recommendation
    and Prompt Tuning for Recommendation. Our work in this paper draws inspiration
    from these approaches to align SR models with LLMs-based recommendation.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了关于LLMs在序列推荐中的应用和推荐的提示调整的文献综述。我们论文中的工作从这些方法中汲取灵感，以使SR模型与基于LLMs的推荐对齐。
- en: 5.1\. LLMs for Sequential Recommendation
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. LLMs在序列推荐中的应用
- en: In the field of SR, recognizing the sequence of user interactions is crucial
    for predicting their next preference. Modern Sequential Recommender Systems (SRS)
    employ various techniques such as RNNs, CNNs, or transformers to identify sequential
    pattern in user interactions. For example, GRU4Rec utilizes GRU for analyzing
    session-based data, while Caser uses CNNs to model interactive data across multiple
    dimensions. SASRec incorporates an attention mechanism to assign weights automatically
    to different interactive items.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SR 领域，识别用户互动的序列对于预测他们的下一个偏好至关重要。现代序列推荐系统（SRS）采用 RNN、CNN 或 transformers 等技术来识别用户互动中的序列模式。例如，GRU4Rec
    使用 GRU 来分析基于会话的数据，而 Caser 使用 CNN 来建模跨多个维度的互动数据。SASRec 结合了注意力机制，以自动为不同的互动项分配权重。
- en: With the ongoing development of LLMs, researchers are exploring methods to integrate
    SR models with LLMs [33,55,51] in order to enhance the performance of SR tasks.
    LLM-TRSR segments a user’s historical behavior into multiple blocks and summarizes
    them using an LLM-based summarizer before inputting them into prompts for sequential
    recommendation. Tempura employs three incentive strategies to increase the temporal
    awareness of LLMs and uses prompt learning to enable LLMs to return and integrate
    multiple results. LLaRA leverages multimodal mapping by inserting prompts with
    item embeddings encoded by SR models then fine-tunes LLMs with item interaction
    relationships. However, previous methods have encountered challenges such as LLMs
    not fully utilizing this information and excessive complexity among other issues.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 LLM 的持续发展，研究人员正在探索将 SR 模型与 LLM 集成的方法 [33,55,51]，以提升 SR 任务的性能。LLM-TRSR 将用户的历史行为分割成多个块，并使用基于
    LLM 的总结器对其进行总结，然后将其输入到用于序列推荐的提示中。Tempura 采用三种激励策略来提高 LLM 的时间意识，并使用提示学习使 LLM 能够返回并整合多个结果。LLaRA
    通过插入由 SR 模型编码的项目嵌入的提示，利用多模态映射，然后用项目互动关系对 LLM 进行微调。然而，以往的方法遇到了一些挑战，例如 LLM 没有充分利用这些信息以及过度复杂等问题。
- en: 5.2\. Prompt Tuning for Recommendation
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 推荐系统中的 Prompt Tuning
- en: 'Prompt tuning is an effective paradigm where, specifically in the field of
    prompt tuning, prompts can be classified into two categories: hard and soft prompts.
    Hard prompts provide explicit textual information to language models for a given
    task prompt, while soft prompts can adapt and change based on specific tasks,
    thereby enhancing the performance of language models in recommendation tasks.
    Currently, most recommendation systems that are based on language models primarily
    utilize pure hard prompts to generate prompts for the language models. However,
    only a few methods have explored the use of prompt tuning in recommendation systems.
    For instance, RA-Rec[71] employs ID embeddings as soft prompts and incorporates
    an innovative alignment module along with an effective tuning method using a custom
    data structure for alignment.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Prompt tuning 是一种有效的范式，在 prompt tuning 领域中，prompts 可以分为两类：硬提示和软提示。硬提示为语言模型提供明确的文本信息用于特定任务提示，而软提示可以根据具体任务进行调整和变化，从而提升语言模型在推荐任务中的表现。目前，大多数基于语言模型的推荐系统主要使用纯硬提示来生成语言模型的提示。然而，只有少数方法探索了在推荐系统中使用
    prompt tuning。例如，RA-Rec[71] 使用 ID 嵌入作为软提示，并结合创新的对齐模块和有效的调优方法，使用自定义数据结构进行对齐。
- en: Although soft prompts are widely utilized in various other tasks involving LMs,
    they are rarely employed in LLM-based SR tasks.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管软提示在涉及语言模型的各种其他任务中得到广泛应用，但在基于 LLM 的 SR 任务中却很少使用。
- en: 6\. CONCLUSION
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: This work introduces a novel framework, DELRec, which aims to enhance the performance
    of LLMs in SR tasks. The framework achieves this by extracting behavioral patterns
    from conventional SR models. Through two main components, namely SR Models Pattern
    Distilling and LLM-based Sequential Recommendation. DELRec not only reduces information
    loss but also improves the recommendation effectiveness of LLMs. Extensive experiments
    on three real-world datasets have been conducted to validate the effectiveness
    of our proposed framework. Overall, DELRec offers a new perspective and approach
    for utilizing LLMs in complex sequential recommendation tasks, particularly in
    capturing semantic information and global context that traditional SR models fail
    to capture. The introduction of DELRec also provides valuable insights for future
    researchers in designing more efficient and accurate recommendation systems.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一个新颖的框架DELRec，该框架旨在提升LLMs在序列推荐（SR）任务中的表现。该框架通过从传统SR模型中提取行为模式来实现这一目标。通过两个主要组件，即SR模型模式提取和基于LLM的序列推荐，DELRec不仅减少了信息损失，还提高了LLMs的推荐效果。在三个真实世界的数据集上进行了大量实验，以验证我们提出的框架的有效性。总体而言，DELRec为在复杂的序列推荐任务中利用LLMs提供了新的视角和方法，特别是在捕捉传统SR模型未能捕捉到的语义信息和全局上下文方面。DELRec的引入也为未来的研究人员在设计更高效、准确的推荐系统方面提供了宝贵的见解。
- en: 7\. Citations and Bibliographies
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 引用和参考文献
- en: 'Some examples. A paginated journal article (Abril and Plant, [2007](#bib.bib3)),
    an enumerated journal article (Cohen et al., [2007](#bib.bib11)), a reference
    to an entire issue (Cohen, [1996](#bib.bib10)), a monograph (whole book) (Kosiur,
    [2001](#bib.bib24)), a monograph/whole book in a series (see 2a in spec. document)
    (Harel, [1979](#bib.bib18)), a divisible-book such as an anthology or compilation
    (Editor, [2007](#bib.bib13)) followed by the same example, however we only output
    the series if the volume number is given (Editor, [2008](#bib.bib14)) (so Editor00a’s
    series should NOT be present since it has no vol. no.), a chapter in a divisible
    book (Spector, [1990](#bib.bib35)), a chapter in a divisible book in a series
    (Douglass et al., [1998](#bib.bib12)), a multi-volume work as book (Knuth, [1997](#bib.bib23)),
    a couple of articles in a proceedings (of a conference, symposium, workshop for
    example) (paginated proceedings article) (Andler, [1979](#bib.bib4); Hagerup et al.,
    [1993](#bib.bib16)), a proceedings article with all possible elements (Smith,
    [2010](#bib.bib34)), an example of an enumerated proceedings article (Gundy et al.,
    [2007](#bib.bib15)), an informally published work (Harel, [1978](#bib.bib17)),
    a couple of preprints (Bornmann et al., [2019](#bib.bib8); Anzaroot et al., [2014](#bib.bib7)),
    a doctoral dissertation (Clarkson, [1985](#bib.bib9)), a master’s thesis: (Anisi,
    [2003](#bib.bib5)), an online document / world wide web resource (Thornburg, [2001](#bib.bib36);
    Ablamowicz and Fauser, [2007](#bib.bib2); Poker-Edge.Com, [2006](#bib.bib28)),
    a video game (Case 1) (Obama, [2008](#bib.bib27)) and (Case 2) (Novak, [2003](#bib.bib26))
    and (Lee, [2005](#bib.bib25)) and (Case 3) a patent (Scientist, [2009](#bib.bib33)),
    work accepted for publication (Rous, [2008](#bib.bib30)), ’YYYYb’-test for prolific
    author (Saeedi et al., [2010a](#bib.bib31)) and (Saeedi et al., [2010b](#bib.bib32)).
    Other cites might contain ’duplicate’ DOI and URLs (some SIAM articles) (Kirschmer
    and Voight, [2010](#bib.bib22)). Boris / Barbara Beeton: multi-volume works as
    books (Hörmander, [1985b](#bib.bib20)) and (Hörmander, [1985a](#bib.bib19)). A
    couple of citations with DOIs: (IEEE, [2004](#bib.bib21); Kirschmer and Voight,
    [2010](#bib.bib22)). Online citations: (TUG, [2017](#bib.bib37); Thornburg, [2001](#bib.bib36);
    Veytsman, [2017](#bib.bib38)). Artifacts: (R Core Team, [2019](#bib.bib29)) and
    (Anzaroot and McCallum, [2013](#bib.bib6)).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '一些示例。分页的期刊文章 (Abril 和 Plant, [2007](#bib.bib3))，编号的期刊文章 (Cohen 等, [2007](#bib.bib11))，对整期的引用
    (Cohen, [1996](#bib.bib10))，专著 (整本书) (Kosiur, [2001](#bib.bib24))，系列中的专著/整本书 (见特定文档中的
    2a) (Harel, [1979](#bib.bib18))，可分的书籍，如选集或汇编 (Editor, [2007](#bib.bib13))，后跟相同的示例，但如果给出卷号则仅输出系列
    (Editor, [2008](#bib.bib14)) (因此 Editor00a 的系列不应出现，因为没有卷号)，可分书籍中的章节 (Spector,
    [1990](#bib.bib35))，系列中的可分书籍中的章节 (Douglass 等, [1998](#bib.bib12))，作为书籍的多卷作品 (Knuth,
    [1997](#bib.bib23))，会议录中的几篇文章 (例如会议、研讨会、工作坊) (分页的会议录文章) (Andler, [1979](#bib.bib4);
    Hagerup 等, [1993](#bib.bib16))，包含所有可能元素的会议录文章 (Smith, [2010](#bib.bib34))，编号的会议录文章示例
    (Gundy 等, [2007](#bib.bib15))，非正式出版的作品 (Harel, [1978](#bib.bib17))，几篇预印本 (Bornmann
    等, [2019](#bib.bib8); Anzaroot 等, [2014](#bib.bib7))，博士论文 (Clarkson, [1985](#bib.bib9))，硕士论文
    (Anisi, [2003](#bib.bib5))，在线文档/全球网络资源 (Thornburg, [2001](#bib.bib36); Ablamowicz
    和 Fauser, [2007](#bib.bib2); Poker-Edge.Com, [2006](#bib.bib28))，视频游戏 (案例 1) (Obama,
    [2008](#bib.bib27)) 和 (案例 2) (Novak, [2003](#bib.bib26)) 和 (Lee, [2005](#bib.bib25))
    和 (案例 3) 专利 (Scientist, [2009](#bib.bib33))，已接受出版的工作 (Rous, [2008](#bib.bib30))，’YYYYb’-测试的高产作者
    (Saeedi 等, [2010a](#bib.bib31)) 和 (Saeedi 等, [2010b](#bib.bib32))。其他引用可能包含’重复’
    DOI 和网址 (一些 SIAM 文章) (Kirschmer 和 Voight, [2010](#bib.bib22))。Boris / Barbara
    Beeton: 作为书籍的多卷作品 (Hörmander, [1985b](#bib.bib20)) 和 (Hörmander, [1985a](#bib.bib19))。几篇带有
    DOI 的引用: (IEEE, [2004](#bib.bib21); Kirschmer 和 Voight, [2010](#bib.bib22))。在线引用:
    (TUG, [2017](#bib.bib37); Thornburg, [2001](#bib.bib36); Veytsman, [2017](#bib.bib38))。文献:
    (R Core Team, [2019](#bib.bib29)) 和 (Anzaroot 和 McCallum, [2013](#bib.bib6))。'
- en: References
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Ablamowicz and Fauser (2007) Rafal Ablamowicz and Bertfried Fauser. 2007. *CLIFFORD:
    a Maple 11 Package for Clifford Algebra Computations, version 11*. Retrieved February
    28, 2008 from [http://math.tntech.edu/rafal/cliff11/index.html](http://math.tntech.edu/rafal/cliff11/index.html)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ablamowicz 和 Fauser (2007) Rafal Ablamowicz 和 Bertfried Fauser. 2007. *CLIFFORD:
    一个用于 Clifford 代数计算的 Maple 11 包，第 11 版*。于 2008 年 2 月 28 日从 [http://math.tntech.edu/rafal/cliff11/index.html](http://math.tntech.edu/rafal/cliff11/index.html)
    获取'
- en: 'Abril and Plant (2007) Patricia S. Abril and Robert Plant. 2007. The patent
    holder’s dilemma: Buy, sell, or troll? *Commun. ACM* 50, 1 (Jan. 2007), 36–44.
    [https://doi.org/10.1145/1188913.1188915](https://doi.org/10.1145/1188913.1188915)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abril and Plant (2007) Patricia S. Abril 和 Robert Plant. 2007. 专利持有者的困境：购买、出售还是钓鱼？*Commun.
    ACM* 50, 1 (2007年1月), 36–44. [https://doi.org/10.1145/1188913.1188915](https://doi.org/10.1145/1188913.1188915)
- en: Andler (1979) Sten Andler. 1979. Predicate Path expressions. In *Proceedings
    of the 6th. ACM SIGACT-SIGPLAN symposium on Principles of Programming Languages*
    *(POPL ’79)*. ACM Press, New York, NY, 226–236. [https://doi.org/10.1145/567752.567774](https://doi.org/10.1145/567752.567774)
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andler (1979) Sten Andler. 1979. 谓词路径表达式. 在 *第六届 ACM SIGACT-SIGPLAN 编程语言原则研讨会论文集*
    *(POPL ’79)*. ACM出版社, 纽约, NY, 226–236. [https://doi.org/10.1145/567752.567774](https://doi.org/10.1145/567752.567774)
- en: Anisi (2003) David A. Anisi. 2003. *Optimal Motion Control of a Ground Vehicle*.
    Master’s thesis. Royal Institute of Technology (KTH), Stockholm, Sweden.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anisi (2003) David A. Anisi. 2003. *地面车辆的最佳运动控制*. 硕士论文. 瑞典皇家理工学院 (KTH), 斯德哥尔摩,
    瑞典.
- en: Anzaroot and McCallum (2013) Sam Anzaroot and Andrew McCallum. 2013. *UMass
    Citation Field Extraction Dataset*. Retrieved May 27, 2019 from [http://www.iesl.cs.umass.edu/data/data-umasscitationfield](http://www.iesl.cs.umass.edu/data/data-umasscitationfield)
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anzaroot and McCallum (2013) Sam Anzaroot 和 Andrew McCallum. 2013. *UMass 引用字段提取数据集*.
    取自 2019年5月27日 [http://www.iesl.cs.umass.edu/data/data-umasscitationfield](http://www.iesl.cs.umass.edu/data/data-umasscitationfield)
- en: Anzaroot et al. (2014) Sam Anzaroot, Alexandre Passos, David Belanger, and Andrew
    McCallum. 2014. Learning Soft Linear Constraints with Application to Citation
    Field Extraction. arXiv:1403.1349
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anzaroot et al. (2014) Sam Anzaroot, Alexandre Passos, David Belanger, 和 Andrew
    McCallum. 2014. 学习软线性约束及其在引用字段提取中的应用. arXiv:1403.1349
- en: Bornmann et al. (2019) Lutz Bornmann, K. Brad Wray, and Robin Haunschild. 2019.
    Citation concept analysis (CCA)—A new form of citation analysis revealing the
    usefulness of concepts for other researchers illustrated by two exemplary case
    studies including classic books by Thomas S. Kuhn and Karl R. Popper. arXiv:1905.12410 [cs.DL]
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bornmann et al. (2019) Lutz Bornmann, K. Brad Wray, 和 Robin Haunschild. 2019.
    引用概念分析 (CCA)—一种新型的引用分析方法，通过托马斯·S·库恩和卡尔·R·波普尔的经典书籍的两个示例案例揭示概念对其他研究者的有用性. arXiv:1905.12410
    [cs.DL]
- en: 'Clarkson (1985) Kenneth L. Clarkson. 1985. *Algorithms for Closest-Point Problems
    (Computational Geometry)*. Ph. D. Dissertation. Stanford University, Palo Alto,
    CA. UMI Order Number: AAT 8506171.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clarkson (1985) Kenneth L. Clarkson. 1985. *最近点问题的算法 (计算几何)*. 博士论文. 斯坦福大学,
    帕洛阿尔托, CA. UMI 订单号: AAT 8506171.'
- en: 'Cohen (1996) Jacques Cohen (Ed.). 1996\. Special issue: Digital Libraries.
    *Commun. ACM* 39, 11 (Nov. 1996).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen (1996) Jacques Cohen (Ed.). 1996. 特刊：数字图书馆. *Commun. ACM* 39, 11 (1996年11月).
- en: Cohen et al. (2007) Sarah Cohen, Werner Nutt, and Yehoshua Sagic. 2007. Deciding
    equivalances among conjunctive aggregate queries. *J. ACM* 54, 2, Article 5 (April
    2007), 50 pages. [https://doi.org/10.1145/1219092.1219093](https://doi.org/10.1145/1219092.1219093)
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen et al. (2007) Sarah Cohen, Werner Nutt, 和 Yehoshua Sagic. 2007. 决定结合聚合查询之间的等价性.
    *J. ACM* 54, 2, 第5篇 (2007年4月), 50页. [https://doi.org/10.1145/1219092.1219093](https://doi.org/10.1145/1219092.1219093)
- en: 'Douglass et al. (1998) Bruce P. Douglass, David Harel, and Mark B. Trakhtenbrot.
    1998. Statecarts in use: structured analysis and object-orientation. In *Lectures
    on Embedded Systems*, Grzegorz Rozenberg and Frits W. Vaandrager (Eds.). Lecture
    Notes in Computer Science, Vol. 1494\. Springer-Verlag, London, 368–394. [https://doi.org/10.1007/3-540-65193-4_29](https://doi.org/10.1007/3-540-65193-4_29)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Douglass et al. (1998) Bruce P. Douglass, David Harel, 和 Mark B. Trakhtenbrot.
    1998. Statecarts 的使用：结构分析与面向对象. 在 *嵌入式系统讲座*，Grzegorz Rozenberg 和 Frits W. Vaandrager
    (Eds.). 计算机科学讲义丛书，第1494卷. 施普林格出版社, 伦敦, 368–394. [https://doi.org/10.1007/3-540-65193-4_29](https://doi.org/10.1007/3-540-65193-4_29)
- en: Editor (2007) Ian Editor (Ed.). 2007. *The title of book one* (1st. ed.). The
    name of the series one, Vol. 9. University of Chicago Press, Chicago. [https://doi.org/10.1007/3-540-09237-4](https://doi.org/10.1007/3-540-09237-4)
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Editor (2007) Ian Editor (Ed.). 2007. *第一本书的标题* (第1版). 系列名, 第9卷. 芝加哥大学出版社, 芝加哥.
    [https://doi.org/10.1007/3-540-09237-4](https://doi.org/10.1007/3-540-09237-4)
- en: Editor (2008) Ian Editor (Ed.). 2008. *The title of book two* (2nd. ed.). University
    of Chicago Press, Chicago, Chapter 100. [https://doi.org/10.1007/3-540-09237-4](https://doi.org/10.1007/3-540-09237-4)
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Editor (2008) Ian Editor (Ed.). 2008. *第二本书的标题* (第2版). 芝加哥大学出版社, 芝加哥, 第100章.
    [https://doi.org/10.1007/3-540-09237-4](https://doi.org/10.1007/3-540-09237-4)
- en: 'Gundy et al. (2007) Matthew Van Gundy, Davide Balzarotti, and Giovanni Vigna.
    2007. Catch me, if you can: Evading network signatures with web-based polymorphic
    worms. In *Proceedings of the first USENIX workshop on Offensive Technologies*
    *(WOOT ’07)*. USENIX Association, Berkley, CA, Article 7, 9 pages.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gundy et al. (2007) Matthew Van Gundy、Davide Balzarotti 和 Giovanni Vigna。2007。*Catch
    me, if you can: Evading network signatures with web-based polymorphic worms*。发表于
    *Proceedings of the first USENIX workshop on Offensive Technologies* *(WOOT ’07)*。USENIX
    Association，Berkley，CA，第 7 篇，9 页。'
- en: Hagerup et al. (1993) Torben Hagerup, Kurt Mehlhorn, and J. Ian Munro. 1993.
    Maintaining Discrete Probability Distributions Optimally. In *Proceedings of the
    20th International Colloquium on Automata, Languages and Programming* *(Lecture
    Notes in Computer Science, Vol. 700)*. Springer-Verlag, Berlin, 253–264.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hagerup et al. (1993) Torben Hagerup、Kurt Mehlhorn 和 J. Ian Munro。1993。*Maintaining
    Discrete Probability Distributions Optimally*。发表于 *Proceedings of the 20th International
    Colloquium on Automata, Languages and Programming* *(Lecture Notes in Computer
    Science, Vol. 700)*。Springer-Verlag，Berlin，第 253–264 页。
- en: 'Harel (1978) David Harel. 1978. *LOGICS of Programs: AXIOMATICS and DESCRIPTIVE
    POWER*. MIT Research Lab Technical Report TR-200\. Massachusetts Institute of
    Technology, Cambridge, MA.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Harel (1978) David Harel。1978。*LOGICS of Programs: AXIOMATICS and DESCRIPTIVE
    POWER*。MIT Research Lab Technical Report TR-200。麻省理工学院，Cambridge，MA。'
- en: Harel (1979) David Harel. 1979. *First-Order Dynamic Logic*. Lecture Notes in
    Computer Science, Vol. 68. Springer-Verlag, New York, NY. [https://doi.org/10.1007/3-540-09237-4](https://doi.org/10.1007/3-540-09237-4)
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harel (1979) David Harel。1979。*First-Order Dynamic Logic*。Lecture Notes in Computer
    Science，第 68 卷。Springer-Verlag，New York，NY。[https://doi.org/10.1007/3-540-09237-4](https://doi.org/10.1007/3-540-09237-4)
- en: Hörmander (1985a) Lars Hörmander. 1985a. *The analysis of linear partial differential
    operators. III*. Grundlehren der Mathematischen Wissenschaften [Fundamental Principles
    of Mathematical Sciences], Vol. 275. Springer-Verlag, Berlin, Germany. viii+525
    pages. Pseudodifferential operators.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hörmander (1985a) Lars Hörmander。1985a。*The analysis of linear partial differential
    operators. III*。Grundlehren der Mathematischen Wissenschaften [Fundamental Principles
    of Mathematical Sciences]，第 275 卷。Springer-Verlag，Berlin，Germany。viii+525 页。伪微分算子。
- en: Hörmander (1985b) Lars Hörmander. 1985b. *The analysis of linear partial differential
    operators. IV*. Grundlehren der Mathematischen Wissenschaften [Fundamental Principles
    of Mathematical Sciences], Vol. 275. Springer-Verlag, Berlin, Germany. vii+352
    pages. Fourier integral operators.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hörmander (1985b) Lars Hörmander。1985b。*The analysis of linear partial differential
    operators. IV*。Grundlehren der Mathematischen Wissenschaften [Fundamental Principles
    of Mathematical Sciences]，第 275 卷。Springer-Verlag，Berlin，Germany。vii+352 页。傅里叶积分算子。
- en: IEEE (2004) IEEE 2004. IEEE TCSC Executive Committee. In *Proceedings of the
    IEEE International Conference on Web Services* *(ICWS ’04)*. IEEE Computer Society,
    Washington, DC, USA, 21–22. [https://doi.org/10.1109/ICWS.2004.64](https://doi.org/10.1109/ICWS.2004.64)
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IEEE (2004) IEEE 2004。IEEE TCSC Executive Committee。发表于 *Proceedings of the
    IEEE International Conference on Web Services* *(ICWS ’04)*。IEEE Computer Society，Washington，DC，USA，第
    21–22 页。[https://doi.org/10.1109/ICWS.2004.64](https://doi.org/10.1109/ICWS.2004.64)
- en: Kirschmer and Voight (2010) Markus Kirschmer and John Voight. 2010. Algorithmic
    Enumeration of Ideal Classes for Quaternion Orders. *SIAM J. Comput.* 39, 5 (Jan.
    2010), 1714–1747. [https://doi.org/10.1137/080734467](https://doi.org/10.1137/080734467)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirschmer and Voight (2010) Markus Kirschmer 和 John Voight。2010。*Algorithmic
    Enumeration of Ideal Classes for Quaternion Orders*。*SIAM J. Comput.* 39, 5 (2010年1月)，1714–1747。[https://doi.org/10.1137/080734467](https://doi.org/10.1137/080734467)
- en: 'Knuth (1997) Donald E. Knuth. 1997. *The Art of Computer Programming, Vol.
    1: Fundamental Algorithms (3rd. ed.)*. Addison Wesley Longman Publishing Co.,
    Inc.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Knuth (1997) Donald E. Knuth。1997。*The Art of Computer Programming, Vol. 1:
    Fundamental Algorithms (第 3 版)*。Addison Wesley Longman Publishing Co., Inc.'
- en: Kosiur (2001) David Kosiur. 2001. *Understanding Policy-Based Networking* (2nd.
    ed.). Wiley, New York, NY.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kosiur (2001) David Kosiur。2001。*Understanding Policy-Based Networking*（第 2
    版）。Wiley，New York，NY。
- en: 'Lee (2005) Newton Lee. 2005. Interview with Bill Kinder: January 13, 2005.
    Video. *Comput. Entertain.* 3, 1, Article 4 (Jan.-March 2005). [https://doi.org/10.1145/1057270.1057278](https://doi.org/10.1145/1057270.1057278)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee (2005) Newton Lee。2005。与 Bill Kinder 的访谈：2005年1月13日。视频。*Comput. Entertain.*
    3, 1，第 4 篇 (2005年1-3月)。 [https://doi.org/10.1145/1057270.1057278](https://doi.org/10.1145/1057270.1057278)
- en: 'Novak (2003) Dave Novak. 2003. Solder man. Video. In *ACM SIGGRAPH 2003 Video
    Review on Animation theater Program: Part I - Vol. 145 (July 27–27, 2003)*. ACM
    Press, New York, NY, 4. [https://doi.org/99.9999/woot07-S422](https://doi.org/99.9999/woot07-S422)
    [http://video.google.com/videoplay?docid=6528042696351994555](http://video.google.com/videoplay?docid=6528042696351994555)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Novak (2003) Dave Novak。2003。*Solder man*。视频。发表于 *ACM SIGGRAPH 2003 Video Review
    on Animation theater Program: Part I - Vol. 145 (July 27–27, 2003)*。ACM Press，New
    York，NY，4。[https://doi.org/99.9999/woot07-S422](https://doi.org/99.9999/woot07-S422)
    [http://video.google.com/videoplay?docid=6528042696351994555](http://video.google.com/videoplay?docid=6528042696351994555)'
- en: Obama (2008) Barack Obama. 2008. A more perfect union. Video. Retrieved March
    21, 2008 from [http://video.google.com/videoplay?docid=6528042696351994555](http://video.google.com/videoplay?docid=6528042696351994555)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Obama (2008) Barack Obama. 2008. 更加完美的联邦。视频。2008年3月21日检索自 [http://video.google.com/videoplay?docid=6528042696351994555](http://video.google.com/videoplay?docid=6528042696351994555)
- en: Poker-Edge.Com (2006) Poker-Edge.Com. 2006. Stats and Analysis. Retrieved June
    7, 2006 from [http://www.poker-edge.com/stats.php](http://www.poker-edge.com/stats.php)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poker-Edge.Com (2006) Poker-Edge.Com. 2006. 统计与分析。2006年6月7日检索自 [http://www.poker-edge.com/stats.php](http://www.poker-edge.com/stats.php)
- en: 'R Core Team (2019) R Core Team. 2019. *R: A Language and Environment for Statistical
    Computing*. R Foundation for Statistical Computing, Vienna, Austria. [https://www.R-project.org/](https://www.R-project.org/)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'R Core Team (2019) R Core Team. 2019. *R: 用于统计计算的语言和环境*. R 统计计算基金会，维也纳，奥地利。
    [https://www.R-project.org/](https://www.R-project.org/)'
- en: Rous (2008) Bernard Rous. 2008. The Enabling of Digital Libraries. *Digital
    Libraries* 12, 3, Article 5 (July 2008). To appear.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rous (2008) Bernard Rous. 2008. 数字图书馆的启用。*数字图书馆* 12, 3, 第5篇（2008年7月）。待发表。
- en: Saeedi et al. (2010a) Mehdi Saeedi, Morteza Saheb Zamani, and Mehdi Sedighi.
    2010a. A library-based synthesis methodology for reversible logic. *Microelectron.
    J.* 41, 4 (April 2010), 185–194.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saeedi et al. (2010a) Mehdi Saeedi, Morteza Saheb Zamani, 和 Mehdi Sedighi. 2010a.
    基于库的可逆逻辑合成方法。*Microelectron. J.* 41, 4（2010年4月），185–194。
- en: Saeedi et al. (2010b) Mehdi Saeedi, Morteza Saheb Zamani, Mehdi Sedighi, and
    Zahra Sasanian. 2010b. Synthesis of Reversible Circuit Using Cycle-Based Approach.
    *J. Emerg. Technol. Comput. Syst.* 6, 4 (Dec. 2010).
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saeedi et al. (2010b) Mehdi Saeedi, Morteza Saheb Zamani, Mehdi Sedighi, 和 Zahra
    Sasanian. 2010b. 基于循环的方法合成可逆电路。*J. Emerg. Technol. Comput. Syst.* 6, 4（2010年12月）。
- en: Scientist (2009) Joseph Scientist. 2009. The fountain of youth. Patent No. 12345,
    Filed July 1st., 2008, Issued Aug. 9th., 2009.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scientist (2009) Joseph Scientist. 2009. 青春之泉。专利号12345，申请日期：2008年7月1日，发放日期：2009年8月9日。
- en: 'Smith (2010) Stan W. Smith. 2010. An experiment in bibliographic mark-up: Parsing
    metadata for XML export. In *Proceedings of the 3rd. annual workshop on Librarians
    and Computers* *(LAC ’10, Vol. 3)*, Reginald N. Smythe and Alexander Noble (Eds.).
    Paparazzi Press, Milan Italy, 422–431. [https://doi.org/99.9999/woot07-S422](https://doi.org/99.9999/woot07-S422)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith (2010) Stan W. Smith. 2010. 一项关于书目标记的实验：解析用于XML导出的元数据。在*第三届年度图书馆员与计算机研讨会论文集*（*LAC
    ’10, 第3卷*），Reginald N. Smythe 和 Alexander Noble（编）。Paparazzi Press, 米兰，意大利，422–431。
    [https://doi.org/99.9999/woot07-S422](https://doi.org/99.9999/woot07-S422)
- en: Spector (1990) Asad Z. Spector. 1990. Achieving application requirements. In
    *Distributed Systems* (2nd. ed.), Sape Mullender (Ed.). ACM Press, New York, NY,
    19–33. [https://doi.org/10.1145/90417.90738](https://doi.org/10.1145/90417.90738)
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spector (1990) Asad Z. Spector. 1990. 实现应用需求。在*分布式系统*（第2版），Sape Mullender（编）。ACM
    Press, 纽约, NY, 19–33。 [https://doi.org/10.1145/90417.90738](https://doi.org/10.1145/90417.90738)
- en: Thornburg (2001) Harry Thornburg. 2001. *Introduction to Bayesian Statistics*.
    Retrieved March 2, 2005 from [http://ccrma.stanford.edu/~jos/bayes/bayes.html](http://ccrma.stanford.edu/~jos/bayes/bayes.html)
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thornburg (2001) Harry Thornburg. 2001. *贝叶斯统计导论*. 2005年3月2日检索自 [http://ccrma.stanford.edu/~jos/bayes/bayes.html](http://ccrma.stanford.edu/~jos/bayes/bayes.html)
- en: TUG (2017) TUG 2017. *Institutional members of the TeX Users Group*. Retrieved
    May 27, 2017 from [http://wwtug.org/instmem.html](http://wwtug.org/instmem.html)
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TUG (2017) TUG 2017. *TeX 用户组的机构成员*. 2017年5月27日检索自 [http://wwtug.org/instmem.html](http://wwtug.org/instmem.html)
- en: Veytsman (2017) Boris Veytsman. 2017. *acmart—Class for typesetting publications
    of ACM*. Retrieved May 27, 2017 from [http://www.ctan.org/pkg/acmart](http://www.ctan.org/pkg/acmart)
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Veytsman (2017) Boris Veytsman. 2017. *acmart—ACM出版物排版类*. 2017年5月27日检索自 [http://www.ctan.org/pkg/acmart](http://www.ctan.org/pkg/acmart)
