- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 19:03:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:03:47'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型告诉你该丢弃什么：针对 LLM 的自适应 KV 缓存压缩
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.01801](https://ar5iv.labs.arxiv.org/html/2310.01801)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.01801](https://ar5iv.labs.arxiv.org/html/2310.01801)
- en: Suyu Ge¹, Yunan Zhang^(1∗), Liyuan Liu^(2∗), Minjia Zhang², Jiawei Han¹, Jianfeng
    Gao²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Suyu Ge¹, Yunan Zhang^(1∗), Liyuan Liu^(2∗), Minjia Zhang², Jiawei Han¹, Jianfeng
    Gao²
- en: ¹University of Illinois Urbana-Champaign, ²Microsoft
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹伊利诺伊大学厄本那-香槟分校, ²微软
- en: '{suyuge2,yunanz2,hanj}@illinois.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{suyuge2,yunanz2,hanj}@illinois.edu'
- en: '{lucliu,minjiaz,jfgao}@microsoft.com Authors contributed equally to this research.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{lucliu,minjiaz,jfgao}@microsoft.com 作者对本研究贡献相同。'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this study, we introduce adaptive KV cache compression, a plug-and-play
    method that reduces the memory footprint of generative inference for Large Language
    Models (LLMs). Different from the conventional KV cache that retains key and value
    vectors for all context tokens, we conduct targeted profiling to discern the intrinsic
    structure of attention modules. Based on the recognized structure, we propose
    FastGen, which constructs the KV cache in an adaptive manner: evicting long-range
    contexts on attention heads emphasizing local contexts, discarding non-special
    tokens on attention heads centered on special tokens, and only employing the standard
    KV cache for attention heads that broadly attend to all tokens. Moreover, with
    the lightweight attention profiling used to guide the construction of the adaptive
    KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training.
    In our experiments across various asks, FastGen demonstrates substantial reduction
    on GPU memory consumption with negligible generation quality loss. We will release
    our code and the compatible CUDA kernel for reproducibility.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们介绍了自适应 KV 缓存压缩，这是一种即插即用的方法，可以减少大型语言模型（LLMs）生成推断的内存占用。与保留所有上下文标记的键和值向量的传统
    KV 缓存不同，我们进行针对性的分析以辨别注意力模块的内在结构。基于识别出的结构，我们提出了 FastGen，该方法以自适应的方式构建 KV 缓存：在强调局部上下文的注意力头上逐出远程上下文，在以特殊标记为中心的注意力头上丢弃非特殊标记，仅在广泛关注所有标记的注意力头上使用标准
    KV 缓存。此外，通过轻量级注意力分析指导自适应 KV 缓存的构建，FastGen 可以在无需资源密集型微调或重新训练的情况下进行部署。在我们对各种任务的实验中，FastGen
    在 GPU 内存消耗上表现出显著减少，生成质量损失几乎可以忽略不计。我们将发布我们的代码和兼容的 CUDA 内核以便复现。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Based on the Transformer architecture, autoregressive language models have attracted
    extensive attention (OpenAI, [2023](#bib.bib27); Touvron et al., [2023b](#bib.bib33)).
    Along with the increase of model size, these models present significant challenges
    in terms of computational complexity and GPU memory consumption (Shazeer et al.,
    [2017](#bib.bib29)). Since these models achieve remarkable success across diverse
    applications, there is a pressing need for serving these models in an economically
    feasible manner.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Transformer 架构，自回归语言模型受到了广泛关注（OpenAI, [2023](#bib.bib27); Touvron et al.,
    [2023b](#bib.bib33)）。随着模型规模的增加，这些模型在计算复杂度和 GPU 内存消耗方面面临重大挑战（Shazeer et al., [2017](#bib.bib29)）。由于这些模型在各种应用中取得了显著成功，因此迫切需要以经济可行的方式服务这些模型。
- en: The generative inference of LLMs usually involves using the *KV Cache* mechanism
    to improve the generation speed. KV cache stores previously computed Key/Value
    vectors in attention calculation and reuses those values for the current token
    generation. As such, it avoids recalculations of previous tokens at each token
    generation step at the cost of extra memory consumption. Despite being a prominent
    technique, the memory consumption of KV cache increases rapidly as the model size
    and generation length increase, drastically increasing the pressure of on-device
    memory.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的生成推断通常涉及使用 *KV 缓存* 机制来提高生成速度。KV 缓存存储先前计算的键/值向量，并在当前标记生成中重用这些值。因此，它避免了在每个标记生成步骤中重新计算先前的标记，但代价是额外的内存消耗。尽管是一种显著的技术，KV
    缓存的内存消耗随着模型规模和生成长度的增加而迅速增长，极大地增加了设备内存的压力。
- en: When memory usage exceeds GPU capacity, the generative inference of LLMs typically
    resort to offloading (Aminabadi et al., [2022](#bib.bib1); Sheng et al., [2023](#bib.bib30)).
    While these methods help mitigate the pressure on the scarce GPU memory from using
    KV cache, offloading KV cache to CPU/NVMe can still add non-trivial overhead to
    generative inference performance due to the limited PCIe bandwidth between the
    GPU and CPU on many devices. Therefore, it becomes a crucial task to reduce the
    memory footprint of KV cache without costly retraining or fine-tuning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当内存使用超过GPU容量时，LLMs的生成推理通常需要依赖于卸载（Aminabadi et al., [2022](#bib.bib1); Sheng
    et al., [2023](#bib.bib30)）。虽然这些方法有助于缓解使用KV缓存时对稀缺GPU内存的压力，但将KV缓存卸载到CPU/NVMe上仍可能由于许多设备上GPU和CPU之间有限的PCIe带宽而增加非微不足道的开销。因此，减少KV缓存的内存占用而不进行高成本的重新训练或微调成为一项重要任务。
- en: 'Our study starts from the observation (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs")) that
    there are abundant structures observed in attention modules (Michel et al., [2019](#bib.bib25);
    Voita et al., [2019](#bib.bib34); Clark et al., [2019](#bib.bib7); Wang et al.,
    [2020](#bib.bib35); Child et al., [2019](#bib.bib6)), and not all attention modules
    need to attend to all tokens (Liu et al., [2023b](#bib.bib24); Zhang et al., [2023](#bib.bib36);
    Liu et al., [2023a](#bib.bib23)). Intuitively, harvesting such structures and
    compressing cached vectors could substantially reduce memory consumption and accelerate
    text generation.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的研究始于观察（图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Model Tells You What to
    Discard: Adaptive KV Cache Compression for LLMs")），注意力模块中观察到丰富的结构（Michel et al.,
    [2019](#bib.bib25); Voita et al., [2019](#bib.bib34); Clark et al., [2019](#bib.bib7);
    Wang et al., [2020](#bib.bib35); Child et al., [2019](#bib.bib6)），并非所有注意力模块都需要关注所有的标记（Liu
    et al., [2023b](#bib.bib24); Zhang et al., [2023](#bib.bib36); Liu et al., [2023a](#bib.bib23)）。直观地，利用这些结构并压缩缓存向量可以显著减少内存消耗并加速文本生成。'
- en: Based on this intuition, we propose FastGen to *accelerate the generative inference
    by adaptively compressing the KV cache on the fly*. First, we employ an efficient
    profiling algorithm to recognize the structural patterns for attention modules.
    Under the guidance of this profiling, we then construct the KV cache for various
    modules adaptively. With this diagnose-before-compress approach, FastGen effectively
    reduces the memory footprint of KV cache while preserving the model quality.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一直觉，我们提出了FastGen，以*通过自适应地压缩KV缓存来加速生成推理*。首先，我们使用高效的分析算法来识别注意力模块的结构模式。在这一分析指导下，我们然后自适应地构建各种模块的KV缓存。通过这种先诊断后压缩的方法，FastGen有效地减少了KV缓存的内存占用，同时保持了模型质量。
- en: 'Input: Feasible Policy Set (${\mathcal{C}}$*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：可行的策略集（${\mathcal{C}}$*
- en: Algorithm 1 FastGen–Prompt Encoding.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 FastGen–Prompt编码。
- en: 'Input: Adaptive KV cache ($\{{\bm{C}}^{i},\hat{{\bm{K}}}^{i},\hat{{\bm{V}}}^{i}\}$*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：自适应KV缓存（$\{{\bm{C}}^{i},\hat{{\bm{K}}}^{i},\hat{{\bm{V}}}^{i}\}$*
- en: Algorithm 2 FastGen–Token Generation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 算法2 FastGen–Token生成。
- en: '![Refer to caption](img/f22e5d9e1eb3463f2703e7d20edc54c4.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f22e5d9e1eb3463f2703e7d20edc54c4.png)'
- en: 'Figure 1: Different attention heads usually have different structures. Left:
    Four common attention structures (more details are elaborated in Section [3](#S3
    "3 Adaptive KV Cache Compression ‣ Model Tells You What to Discard: Adaptive KV
    Cache Compression for LLMs") and Section [4](#S4 "4 Diversity and Stability of
    Attention Structures ‣ Model Tells You What to Discard: Adaptive KV Cache Compression
    for LLMs")). Right: Attention map compositions of three attention heads that are
    in the same layer.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '图1：不同的注意力头通常有不同的结构。左侧：四种常见的注意力结构（更多细节见第[3](#S3 "3 Adaptive KV Cache Compression
    ‣ Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs")节和第[4](#S4
    "4 Diversity and Stability of Attention Structures ‣ Model Tells You What to Discard:
    Adaptive KV Cache Compression for LLMs")节）。右侧：同一层中三个注意力头的注意力图组合。'
- en: '![Refer to caption](img/24e5dd0003d794ac53ad7c261598a3d4.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/24e5dd0003d794ac53ad7c261598a3d4.png)'
- en: 'Figure 2: Performance of Adaptive KV Cache (FastGen) and Fixed KV Cache (Frequency,
    Local, and Frequency+Local; [Zhang et al.](#bib.bib36), [2023](#bib.bib36) and
    [Liu et al.](#bib.bib23), [2023a](#bib.bib23)) on AlpacaEval.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：自适应KV缓存（FastGen）和固定KV缓存（频率、本地和频率+本地；[Zhang et al.](#bib.bib36)、[2023](#bib.bib36)
    和 [Liu et al.](#bib.bib23)、[2023a](#bib.bib23)）在AlpacaEval上的表现。
- en: In our study, FastGen recognizes five fundamental attention structures and applies
    them correspondingly. Specifically, some attention modules mostly attend to local
    contexts, for which we construct a KV cache that evicts long-range contexts; some
    primarily attend to specific tokens/punctuations, for which we create a KV cache
    that retains only special tokens/punctuations; some have attention maps that are
    column-wise sparse, for which we discard the least frequently attended tokens;
    and some broadly attend to all tokens, for which we employ the standard KV cache
    and store all tokens.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，FastGen 识别了五种基本的注意力结构，并相应地应用它们。具体来说，一些注意力模块主要关注局部上下文，我们为此构建了一个驱逐长距离上下文的
    KV 缓存；一些主要关注特定的标记/标点符号，我们创建了一个仅保留特殊标记/标点符号的 KV 缓存；一些具有列状稀疏的注意力图，我们丢弃了最少被关注的标记；还有一些广泛关注所有标记，我们采用了标准的
    KV 缓存并存储所有标记。
- en: In this way, FastGen is able to compress the KV cache while retaining the original
    functionality of attention modules. Remarkably, FastGen does not require any fine-tuning
    and can be applied in a plug-and-play manner. This is a big advantage of FastGen,
    because the training cost on extra-large models (Brown et al., [2020](#bib.bib3)),
    can hardly be afforded by many research labs or practitioners.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，FastGen 能够在保留注意力模块原有功能的同时压缩 KV 缓存。值得注意的是，FastGen 不需要任何微调，可以以即插即用的方式应用。这是
    FastGen 的一个重大优势，因为超大模型的训练成本（Brown 等，[2020](#bib.bib3)）很多研究实验室或从业者难以承担。
- en: 'We evaluate FastGen on Llama 1 (Touvron et al., [2023b](#bib.bib33)) with a
    suite of major benchmarks covering generative tasks in math, code, knowledge,
    and common sense reasoning. FastGen effectively performs KV cache compression
    with negligible generation quality loss (i.e., recover over 95% of attention scores
    with 35% cache compressed). Notably, as to the 30b model in Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Model Tells You What to Discard: Adaptive KV Cache
    Compression for LLMs"), FastGen (50% cache compressed) surpasses all fixed KV
    compression methods (15% cache compressed).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 Llama 1（Touvron 等，[2023b](#bib.bib33)）上评估了 FastGen，使用了一系列涵盖数学、代码、知识和常识推理的主要基准测试。FastGen
    有效地执行 KV 缓存压缩，几乎没有生成质量损失（即，恢复超过 95% 的注意力分数，同时缓存压缩 35%）。特别是，对于图 [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Model Tells You What to Discard: Adaptive KV Cache Compression
    for LLMs") 中的 30b 模型，FastGen（缓存压缩 50%）超越了所有固定 KV 压缩方法（缓存压缩 15%）。'
- en: 2 Related Work
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Token Dropping and KV Cache Compression. Many efforts have been made to improve
    the model efficiency for LLMs. For recurrent neural networks, one method is to
    skip multiple tokens at a given time step (Campos et al., [2017](#bib.bib4); Seo
    et al., [2017](#bib.bib28); Hansen et al., [2019](#bib.bib14)). Since Transformer
    models quickly attracted lots of attention, Goyal et al. ([2020](#bib.bib11))
    proposes to eliminate redundant words in BERT (Devlin et al., [2019](#bib.bib10))
    based on their attention scores, while Dai et al. ([2020](#bib.bib9)) compresses
    the input sequence by adding pooling layers to the encoding modules of the transformer
    architecture. Recently, Huang et al. ([2022](#bib.bib16)) adds a token selection
    task to the original BERT model that learns to select performance-crucial tokens,
    and Kim et al. ([2022](#bib.bib18)) designs a learnable threshold to detect unimportant
    tokens to prune. Meanwhile, many efforts have been made to explore the possibility
    of compressing the hidden state of tokens rather than explicitly reducing the
    sequence length (Guan et al., [2022](#bib.bib12); Sun et al., [2022](#bib.bib31);
    Zhou et al., [2020](#bib.bib38)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 标记丢弃和 KV 缓存压缩。许多努力已经投入到提高 LLM 模型效率的工作中。对于递归神经网络，一种方法是在给定时间步跳过多个标记（Campos 等，[2017](#bib.bib4);
    Seo 等，[2017](#bib.bib28); Hansen 等，[2019](#bib.bib14)）。由于 Transformer 模型迅速引起了大量关注，Goyal
    等（[2020](#bib.bib11)）提出基于注意力分数消除 BERT（Devlin 等，[2019](#bib.bib10)）中的冗余词，而 Dai
    等（[2020](#bib.bib9)）通过在 transformer 架构的编码模块中添加池化层来压缩输入序列。最近，Huang 等（[2022](#bib.bib16)）在原始
    BERT 模型中添加了一个标记选择任务，学习选择对性能至关重要的标记，Kim 等（[2022](#bib.bib18)）设计了一个可学习的阈值来检测不重要的标记以进行修剪。同时，许多努力已经投入到探索压缩标记隐藏状态的可能性，而不是显式减少序列长度（Guan
    等，[2022](#bib.bib12); Sun 等，[2022](#bib.bib31); Zhou 等，[2020](#bib.bib38)）。
- en: Nevertheless, these methods can only be applied to non-autoregressive models
    and typically require an additional re-training phrase, making them less suitable
    for auto-regressive LLMs like ChatGPT and Llama. Recognizing this gap, researchers
    started examining the potential of pruning tokens within the KV cache of auto-regressive
    LLMs. Mu et al. ([2023](#bib.bib26)) learns to compress the prompts into a few
    special tokens to reduce memory pressure during caching. However, the token prediction
    requires model re-training and could be an expensive overhead during inference.
    Meanwhile, several concurrent methods propose to leverage accumulated attention
    score as the criteria to identify important tokens in the KV cache (e.g., Sheng
    et al., [2023](#bib.bib30); Zhang et al., [2023](#bib.bib36); Liu et al., [2023a](#bib.bib23)).
    Instead of investigating a specific eviction policy, this study aims to synergistically
    coordinate diverse eviction policies to better align with model-specific attributes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些方法只能应用于非自回归模型，并且通常需要额外的重新训练阶段，使得它们不太适合像 ChatGPT 和 Llama 这样的自回归语言模型。认识到这一差距，研究人员开始研究在自回归语言模型的
    KV 缓存中修剪标记的潜力。Mu 等人 ([2023](#bib.bib26)) 学会了将提示压缩成少数特殊标记以减少缓存期间的内存压力。然而，标记预测需要重新训练模型，可能会在推断过程中产生昂贵的开销。与此同时，一些并行方法建议利用累积的注意力得分作为标准来识别
    KV 缓存中的重要标记（例如，Sheng 等人，[2023](#bib.bib30); Zhang 等人，[2023](#bib.bib36); Liu 等人，[2023a](#bib.bib23)）。本研究的目标是协同协调多种驱逐策略，以更好地与模型特定属性对齐，而不是研究特定的驱逐策略。
- en: Underlying Structure of Attention. Inspired by the success of Transformer, extensive
    studies have been conducted to explore the underlying mechanism of different self-attention
    heads. Voita et al. ([2019](#bib.bib34)) analyzed the self-attention heads in
    BERT using LRF (Bach et al., [2015](#bib.bib2)) and characterized them into interpretable
    roles, one of which is attending adjacent tokens all the time. Michel et al. ([2019](#bib.bib25))
    demonstrated that heads in the same layer could have different impact on the performance
    while the importance of each head changes across tasks. Clark et al. ([2019](#bib.bib7))
    and Kovaleva et al. ([2019](#bib.bib20)) identified such patterns as some heads
    primarily attend to separator tokens, adjacent tokens and a combination of these.
    While most previous studies mainly considered encoder models, FastGen is motivated
    by consistent patterns we have observed in decoder-only models. Like previous
    studies, FastGen also explores the structure of the attention mechanism to improve
    inference efficiency. But FastGan differs from previous studies by focusing on
    characterizing the KV cache of different attention heads.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力的基本结构。受到变换器成功的启发，已经进行了广泛的研究来探索不同自注意力头的基本机制。Voita 等人 ([2019](#bib.bib34))
    使用 LRF (Bach 等人，[2015](#bib.bib2)) 分析了 BERT 中的自注意力头，并将其特征化为可解释的角色，其中之一是始终关注相邻标记。Michel
    等人 ([2019](#bib.bib25)) 证明了同一层中的头对性能的影响可能不同，而每个头的重要性在不同任务中会有所变化。Clark 等人 ([2019](#bib.bib7))
    和 Kovaleva 等人 ([2019](#bib.bib20)) 识别出这些模式，即一些头主要关注分隔符标记、相邻标记及其组合。虽然大多数先前的研究主要考虑了编码器模型，FastGen
    的灵感来自我们在仅解码器模型中观察到的一致模式。与先前的研究类似，FastGen 也探索了注意力机制的结构以提高推断效率。但 FastGan 与以往的研究不同，专注于表征不同注意力头的
    KV 缓存。
- en: 3 Adaptive KV Cache Compression
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 自适应 KV 缓存压缩
- en: In this section we first introduce the problem formulation, and then present
    attention profiling and adaptive KV cache compression.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍问题的表述，然后展示注意力分析和自适应 KV 缓存压缩。
- en: 3.1 Generative Inference of Autoregressive LLMs
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 自回归语言模型的生成推断
- en: 'A typical generative model inference involves two steps: prompt encoding and
    token generation.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的生成模型推断涉及两个步骤：提示编码和标记生成。
- en: Prompt Encoding. When an autoregressive transformer-based LLM generates the
    $i$ tokens, i.e., the key and value vectors (KV vectors) of these tokens. To circumvent
    redundant KV vector computations when generating succeeding tokens, all KV vectors
    are stored in the *KV cache* once they are generated.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 提示编码。当自回归基于变换器的语言模型生成第 $i$ 个标记时，即这些标记的键和值向量（KV 向量）。为了避免在生成后续标记时重复计算 KV 向量，一旦生成所有
    KV 向量就会将其存储在 *KV 缓存* 中。
- en: Token Generation. Once prompt encoding is finished, the LLM generates the output
    token by token. At each generation step, the LLM needs to encode the new token(s)
    generated in the previous step. After a new token is generated, its associated
    KV vectors are appended to the current KV cache. Thus, the size of KV cache increases
    linearly with the number of tokens being generated.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌生成。一旦提示编码完成，LLM就会一个接一个地生成输出令牌。在每一步生成中，LLM需要对在前一步生成的新令牌进行编码。新令牌生成后，其相关的KV向量会附加到当前的KV缓存中。因此，KV缓存的大小会随着生成的令牌数量线性增加。
- en: 3.2 FastGen
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 FastGen
- en: 'As described in Section [2](#S2 "2 Related Work ‣ Model Tells You What to Discard:
    Adaptive KV Cache Compression for LLMs"), many previous studies of compressing
    KV cache for improving inference efficiency do not leverage the intricate attention
    structure in LLMs. As to be detailed in Section [4](#S4 "4 Diversity and Stability
    of Attention Structures ‣ Model Tells You What to Discard: Adaptive KV Cache Compression
    for LLMs"), attention heads in LLMs often function distinctively, indicating the
    need for tailoring the compression strategy to each individual attention head.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '正如第[2](#S2 "2 Related Work ‣ Model Tells You What to Discard: Adaptive KV Cache
    Compression for LLMs")节中所述，许多以前关于压缩KV缓存以提高推理效率的研究没有利用LLMs中复杂的注意力结构。正如第[4](#S4
    "4 Diversity and Stability of Attention Structures ‣ Model Tells You What to Discard:
    Adaptive KV Cache Compression for LLMs")节将详细说明的那样，LLMs中的注意力头通常功能各异，这表明需要为每个单独的注意力头量身定制压缩策略。'
- en: 'With these insights, we introduce FastGen: a dual-phase algorithm for crafting
    an adaptive KV cache. During the prompt encoding phase, model profiling is conducted
    to discern the behavior of various attention heads, so that we can choose the
    most appropriate compression strategy for each head. Then, in the token generation
    phase, instead of indiscriminately appending new KV vectors for each newly generated
    token, we manage the KV cache for each token based on its selected compression
    strategy.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些见解，我们介绍了FastGen：一种用于构建自适应KV缓存的双阶段算法。在提示编码阶段，进行模型分析以辨别各个注意力头的行为，从而为每个头选择最合适的压缩策略。然后，在令牌生成阶段，我们不是对每个新生成的令牌不加区分地附加新的KV向量，而是根据每个令牌的选择压缩策略来管理KV缓存。
- en: 3.3 Model Profiling
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 模型分析
- en: 'Model profiling is conducted based on the result of prompt encoding. Specifically,
    for a compression policy ${\bm{C}}$ with the minimum memory cost:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 模型分析是基于提示编码的结果进行的。具体来说，对于具有最小内存成本的压缩策略${\bm{C}}$：
- en: '|  | ${\bm{C}}^{*}=\operatorname*{arg\,min}_{{\bm{C}}\in{\mathcal{C}}}\;\mbox{CacheMemoryCost}({\bm{C}})\;\;\mbox{
    s.t. }\;\;&#124;{\bm{A}}-\mbox{softmax}({\bm{Q}}{\bm{K}}_{{\bm{C}}}^{T})&#124;\leq
    1-T,$ |  | (1) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{C}}^{*}=\operatorname*{arg\,min}_{{\bm{C}}\in{\mathcal{C}}}\;\mbox{CacheMemoryCost}({\bm{C}})\;\;\mbox{
    s.t. }\;\;&#124;{\bm{A}}-\mbox{softmax}({\bm{Q}}{\bm{K}}_{{\bm{C}}}^{T})&#124;\leq
    1-T,$ |  | (1) |'
- en: 'where ${\mathcal{C}}$. As to be discussed in Section [5](#S5 "5 Experiment
    ‣ Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"), FastGen
    is able to recover +95% of the attention map with +40% compression ratio for a
    65B model. The final prompt encoding algorithm that includes model profiling is
    presented in Algorithm [1](#algorithm1 "In 1 Introduction ‣ Model Tells You What
    to Discard: Adaptive KV Cache Compression for LLMs").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '其中${\mathcal{C}}$。正如第[5](#S5 "5 Experiment ‣ Model Tells You What to Discard:
    Adaptive KV Cache Compression for LLMs")节将讨论的那样，FastGen能够在65B模型中以+40%的压缩比恢复+95%的注意力图。最终的提示编码算法，包括模型分析，详见算法[1](#algorithm1
    "In 1 Introduction ‣ Model Tells You What to Discard: Adaptive KV Cache Compression
    for LLMs")。'
- en: 'Intrinsically, our method assumes that the structure of the attention map is
    stable across different attention heads at different positions. So, it is sufficient
    to use only the encoded prompt to select a proper compression policy. It is worth
    noting that existing literature has provided the theoretical justification for
    using solely encoded prompts to capture attention structures for the full contexts (Zhang
    et al., [2023](#bib.bib36); Liu et al., [2023a](#bib.bib23)). In our study, we
    also empirically verified this, as to be elaborated in Section [4](#S4 "4 Diversity
    and Stability of Attention Structures ‣ Model Tells You What to Discard: Adaptive
    KV Cache Compression for LLMs").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '从本质上讲，我们的方法假设在不同位置的不同注意力头之间，注意力图的结构是稳定的。因此，仅使用编码后的提示来选择合适的压缩策略就足够了。值得注意的是，现有文献已经提供了仅使用编码提示来捕获完整上下文中的注意力结构的理论依据（Zhang
    et al., [2023](#bib.bib36)；Liu et al., [2023a](#bib.bib23)）。在我们的研究中，我们也进行了实证验证，如第[4](#S4
    "4 Diversity and Stability of Attention Structures ‣ Model Tells You What to Discard:
    Adaptive KV Cache Compression for LLMs")节中将详细阐述的那样。'
- en: 3.4 KV Cache Compression Policies
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 KV 缓存压缩策略
- en: 'In our experiments we observe that a large number of attention heads closely
    follow certain patterns, as to be detailed in Section [4](#S4 "4 Diversity and
    Stability of Attention Structures ‣ Model Tells You What to Discard: Adaptive
    KV Cache Compression for LLMs"). Thus, in addition to the conventional full KV
    cache policy, we also consider four fundamental KV cache compression policies.
    While we mainly use these four fundamental KV cache compression policies for evaluation
    in this study, it is easy for FastGen to use numerous other strategies. The four
    KV cache compression policies are:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的实验中，我们观察到大量注意力头紧密跟随某些模式，如第 [4](#S4 "4 Diversity and Stability of Attention
    Structures ‣ Model Tells You What to Discard: Adaptive KV Cache Compression for
    LLMs") 节所述。因此，除了传统的完整 KV 缓存策略外，我们还考虑了四种基本的 KV 缓存压缩策略。虽然我们主要使用这四种基本的 KV 缓存压缩策略进行评估，但
    FastGen 也可以轻松使用其他众多策略。这四种 KV 缓存压缩策略是：'
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Special Tokens. We keep in KV cache only special tokens, such as the begin-of-the-sentence
    token $<$.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特殊标记。我们在 KV 缓存中只保留特殊标记，如句子开始标记 $<$。
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Punctuation. We keep in the KV cache only punctuation tokens like ”.”, ”:”,
    ”?”. This policy is referred to as ${\bm{C}}_{\mbox{punct.}}$.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标点符号。我们在 KV 缓存中只保留像 ”.”、”:”、”?” 这样的标点符号。这一策略被称为 ${\bm{C}}_{\mbox{punct.}}$。
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Locality This policy evicts long-range contexts. Once the relative distance
    between the context token and the current token exceeds a threshold, the KV cache
    of the context token will be evicted. The threshold is determined by a pre-defined
    ratio $r_{l}$.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 局部性 该策略驱逐长距离上下文。一旦上下文标记与当前标记之间的相对距离超过阈值，该上下文标记的 KV 缓存将被驱逐。阈值由预定义的比率 $r_{l}$
    决定。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Frequency (Heavy Hitter) This policy has been used in multiple previous studies (e.g.,
    Sheng et al., [2023](#bib.bib30); Zhang et al., [2023](#bib.bib36); Liu et al.,
    [2023a](#bib.bib23)). We monitor for each token its cumulative sum of attention
    score, then treat these scores as token *frequency* and only keep the most frequent
    tokens in the KV cache. The length budget of frequent tokens over the current
    sequence length is controlled by a ratio $r_{f}$.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 频率（重度参与者）这一策略已在多项前期研究中使用（例如，Sheng 等人，[2023](#bib.bib30)；Zhang 等人，[2023](#bib.bib36)；Liu
    等人，[2023a](#bib.bib23)）。我们监控每个标记的注意力得分的累计和，然后将这些得分视为标记的 *频率*，并仅在 KV 缓存中保留最频繁的标记。频繁标记在当前序列长度上的长度预算由比率
    $r_{f}$ 控制。
- en: Hybrid Policies.
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 混合策略。
- en: In practice, it is often necessary to use hybrid policies that combines the
    aforementioned compression policies. Since the total number of hybrid policies
    is hugh, in our study we use a greedy method to construct a small set of hybrid-policies
    as follows
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中，通常需要使用结合了上述压缩策略的混合策略。由于混合策略的总数巨大，在我们的研究中，我们使用贪婪方法构建一个小的混合策略集合，如下所示
- en: '|  | $\displaystyle{\mathcal{C}}=\{{\bm{C}}_{\mbox{special}},{\bm{C}}_{\mbox{special+punct.}},{\bm{C}}_{\mbox{special+punct.+frequent}},{\bm{C}}_{\mbox{special+punct.+frequent+local}},{\bm{C}}_{\mbox{full}}\},$
    |  | (2) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathcal{C}}=\{{\bm{C}}_{\mbox{special}},{\bm{C}}_{\mbox{special+punct.}},{\bm{C}}_{\mbox{special+punct.+frequent}},{\bm{C}}_{\mbox{special+punct.+frequent+local}},{\bm{C}}_{\mbox{full}}\},$
    |  | (2) |'
- en: where the sum of two compression strategies is to compute the union of their
    compressed KV cache, and ${\bm{C}}_{\mbox{full}}$ refers to full KV cache without
    compression.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 两种压缩策略的总和是计算它们压缩后的 KV 缓存的并集，而 ${\bm{C}}_{\mbox{full}}$ 指的是没有压缩的完整 KV 缓存。
- en: 'We use ${\bm{C}}_{\mbox{special}}$ is often used as a component to form hybrid
    policies due to its memory-efficiency, i.e., the number of punctuations in a sentence
    is small. The final algorithm for token generation is presented in Algorithm [2](#algorithm2
    "In 1 Introduction ‣ Model Tells You What to Discard: Adaptive KV Cache Compression
    for LLMs").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用 ${\bm{C}}_{\mbox{special}}$ 作为构建混合策略的组成部分，因为它具有内存效率，即句子中的标点符号数量很少。标记生成的最终算法在算法
    [2](#algorithm2 "In 1 Introduction ‣ Model Tells You What to Discard: Adaptive
    KV Cache Compression for LLMs") 中给出。'
- en: 4 Diversity and Stability of Attention Structures
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 注意力结构的多样性和稳定性
- en: In this section we present an empirical study to show the effectiveness of adaptive
    KV cache compression. First, we demonstrate that different attention heads typically
    possess distinct structures. Then, we show that these attention head structures
    remain relatively consistent across different attention heads at different positions.
    We do so by analyzing the attention scores of Llama 1 65B using random samples
    from GSM8k (Cobbe et al., [2021](#bib.bib8)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了自适应KV缓存压缩的有效性。首先，我们证明不同的注意力头通常具有不同的结构。然后，我们展示了这些注意力头结构在不同位置的不同注意力头之间保持相对一致。我们通过分析Llama
    1 65B的注意力分数，使用来自GSM8k（Cobbe等，[2021](#bib.bib8)）的随机样本来实现这一点。
- en: 4.1 Head Distinctive Attention Structure
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 头部特征注意力结构
- en: 'Figure 3: Attention profiling result distribution across different layers.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：不同层中注意力轮廓结果分布。
- en: Setting.
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置。
- en: 'We perform model profiling with a recover threshold of $0.95$ layers. The result
    is shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.1 Head Distinctive Attention Structure
    ‣ 4 Diversity and Stability of Attention Structures ‣ Model Tells You What to
    Discard: Adaptive KV Cache Compression for LLMs").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在恢复阈值为$0.95$的层上进行模型分析。结果如图[3](#S4.F3 "Figure 3 ‣ 4.1 Head Distinctive Attention
    Structure ‣ 4 Diversity and Stability of Attention Structures ‣ Model Tells You
    What to Discard: Adaptive KV Cache Compression for LLMs")所示。'
- en: Observation.
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 观察。
- en: 'Figure [3](#S4.F3 "Figure 3 ‣ 4.1 Head Distinctive Attention Structure ‣ 4
    Diversity and Stability of Attention Structures ‣ Model Tells You What to Discard:
    Adaptive KV Cache Compression for LLMs") shows that attention heads in different
    layers have vastly different structures. Specifically, for the initial and final
    layers, they have more attention heads assigned to the full KV cache, indicating
    attention heads in these layers are likely to attend to all tokens. Meanwhile,
    for middle layers, the attention map focuses on special tokens, indicating that
    most attention heads of these layers primarily attend to special tokens (i.e.,
    the accumulated attention score on special tokens is higher than $0.95$ for these
    attention heads). Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Model Tells You
    What to Discard: Adaptive KV Cache Compression for LLMs") shows the structure
    of different attention heads in the same layer. We see that attention structures
    differ across different layers and different heads.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '图[3](#S4.F3 "Figure 3 ‣ 4.1 Head Distinctive Attention Structure ‣ 4 Diversity
    and Stability of Attention Structures ‣ Model Tells You What to Discard: Adaptive
    KV Cache Compression for LLMs")显示了不同层的注意力头具有非常不同的结构。具体而言，对于初始层和最终层，它们有更多的注意力头分配到完整的KV缓存中，表明这些层中的注意力头很可能关注所有令牌。同时，对于中间层，注意力图聚焦于特殊令牌，表明这些层的大多数注意力头主要关注特殊令牌（即这些注意力头在特殊令牌上的累积注意力分数高于$0.95$）。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Model Tells You What to Discard: Adaptive KV Cache
    Compression for LLMs")展示了同一层中不同注意力头的结构。我们看到不同层和不同头的注意力结构有所不同。'
- en: These results indicate that it is suboptimal to apply the same KV cache to all
    layers without adaptation, and that it is beneficial to detect the structure of
    each attention head so as to select the optimal compression policy to construct
    the KV cache.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，未进行适配就将相同的KV缓存应用于所有层是不理想的，检测每个注意力头的结构以选择最佳的压缩策略来构建KV缓存是有益的。
- en: 4.2 Profile Tends to Be Consistent in One Sequence
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 在一个序列中轮廓趋于一致
- en: 'The previous section demonstrates the great potential for constructing an adaptive
    KV cache in accordance with the structure of different attention heads. Here,
    we show that it is sufficient to leverage only the user-provided prompts and conduct
    one-shot model profiling, as outlined in Section [3.3](#S3.SS3 "3.3 Model Profiling
    ‣ 3 Adaptive KV Cache Compression ‣ Model Tells You What to Discard: Adaptive
    KV Cache Compression for LLMs"). Specifically, we show that user-provided prompts
    share the same attention structure in the generation process.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '前一节展示了根据不同注意力头的结构构建自适应KV缓存的巨大潜力。在这里，我们表明仅利用用户提供的提示并进行一次性模型分析就足够了，如第[3.3](#S3.SS3
    "3.3 Model Profiling ‣ 3 Adaptive KV Cache Compression ‣ Model Tells You What
    to Discard: Adaptive KV Cache Compression for LLMs")节所述。具体而言，我们展示了用户提供的提示在生成过程中共享相同的注意力结构。'
- en: '![Refer to caption](img/30b781ad7e89337a575cbffa5ecea498.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/30b781ad7e89337a575cbffa5ecea498.png)'
- en: 'Figure 4: Accumulated attention score at 1st (prompt encoding), 10th, 20th,
    30th decoding steps.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：第1步（提示编码）、第10步、第20步、第30步解码时的累积注意力分数。
- en: Setting.
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置。
- en: 'Following Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Model Tells You What
    to Discard: Adaptive KV Cache Compression for LLMs"), we compute the accumulated
    attention score for attention heads in different layers of Llama 1 65B at multiple
    decoding steps (i.e., 1st, 10th, 20th, 30th). We visualized the resulting accumulated
    score in Figure [4](#S4.F4 "Figure 4 ‣ 4.2 Profile Tends to Be Consistent in One
    Sequence ‣ 4 Diversity and Stability of Attention Structures ‣ Model Tells You
    What to Discard: Adaptive KV Cache Compression for LLMs").'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '根据图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Model Tells You What to Discard:
    Adaptive KV Cache Compression for LLMs")，我们计算了Llama 1 65B在多个解码步骤（即第1、第10、第20、第30步）中不同层的注意力头的累计注意力分数。我们在图 [4](#S4.F4
    "Figure 4 ‣ 4.2 Profile Tends to Be Consistent in One Sequence ‣ 4 Diversity and
    Stability of Attention Structures ‣ Model Tells You What to Discard: Adaptive
    KV Cache Compression for LLMs")中可视化了结果累计分数。'
- en: Observation.
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 观察。
- en: Despite some fluctuations of accumulated attention scores across time steps,
    the pattern of the attention maps remains relatively stable. For example, Layer
    33 Head 0 and Layer 23 Head 2 almost only attend to the special token, while the
    locality and punctuation plays an important role in Layer 23 Head 0. As to Layer
    23 Head 3, more than 10% of the attention score is allocated to the others portion,
    making it suitable for a uncompressed KV cache ${\bm{C}}_{\mbox{full}}$.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管累计注意力分数在时间步之间存在一些波动，但注意力图的模式仍然相对稳定。例如，第33层第0头和第23层第2头几乎只关注特殊标记，而第23层第0头的局部性和标点符号起着重要作用。至于第23层第3头，超过10%的注意力分数分配给其他部分，这使得它适合用于未压缩的KV缓存${\bm{C}}_{\mbox{full}}$。
- en: 'In addition, we observe that a large portion of attention scores are on special
    tokens in all cases. This justifies the greed method we used to construct hybrid
    policies, as described in Section [3.4](#S3.SS4 "3.4 KV Cache Compression Policies
    ‣ 3 Adaptive KV Cache Compression ‣ Model Tells You What to Discard: Adaptive
    KV Cache Compression for LLMs").'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们观察到在所有情况下，大部分注意力分数都集中在特殊标记上。这证明了我们用于构建混合策略的贪婪方法，如第 [3.4](#S3.SS4 "3.4
    KV Cache Compression Policies ‣ 3 Adaptive KV Cache Compression ‣ Model Tells
    You What to Discard: Adaptive KV Cache Compression for LLMs")节所述。'
- en: 5 Experiment
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'We conduct comprehensive experiments to demonstrate the effectiveness of FastGen
    on memory footprint reduction and generation quality preserving. First, we report
    the trade-off between memory reduction and end-to-end generation quality in Section [5.1](#S5.SS1
    "5.1 Trade-off between performance and memory reduction ‣ 5 Experiment ‣ Model
    Tells You What to Discard: Adaptive KV Cache Compression for LLMs"), and discuss
    the compression ratio of FastGen in Section [5.2](#S5.SS2 "5.2 Memory Footprint
    Reduction Analysis ‣ 5 Experiment ‣ Model Tells You What to Discard: Adaptive
    KV Cache Compression for LLMs"). To demonstrate the superiority of FastGen on
    real-world systems, we demonstrate the end-to-end latency change in Section [5.3](#S5.SS3
    "5.3 End-to-end Latency Improvement ‣ 5 Experiment ‣ Model Tells You What to Discard:
    Adaptive KV Cache Compression for LLMs") and the profiling overhead in Section [5.4](#S5.SS4
    "5.4 Profiling Cost ‣ 5 Experiment ‣ Model Tells You What to Discard: Adaptive
    KV Cache Compression for LLMs"). Finally, we present ablation studies and discussions
    in Section [5.5](#S5.SS5 "5.5 Ablations ‣ 5 Experiment ‣ Model Tells You What
    to Discard: Adaptive KV Cache Compression for LLMs").'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进行了一系列全面的实验，以证明FastGen在内存占用减少和生成质量保持方面的有效性。首先，我们在第 [5.1](#S5.SS1 "5.1 Trade-off
    between performance and memory reduction ‣ 5 Experiment ‣ Model Tells You What
    to Discard: Adaptive KV Cache Compression for LLMs")节中报告了内存减少与端到端生成质量之间的权衡，并在第 [5.2](#S5.SS2
    "5.2 Memory Footprint Reduction Analysis ‣ 5 Experiment ‣ Model Tells You What
    to Discard: Adaptive KV Cache Compression for LLMs")节中讨论了FastGen的压缩比。为了展示FastGen在现实系统中的优越性，我们在第 [5.3](#S5.SS3
    "5.3 End-to-end Latency Improvement ‣ 5 Experiment ‣ Model Tells You What to Discard:
    Adaptive KV Cache Compression for LLMs")节中展示了端到端延迟的变化，并在第 [5.4](#S5.SS4 "5.4 Profiling
    Cost ‣ 5 Experiment ‣ Model Tells You What to Discard: Adaptive KV Cache Compression
    for LLMs")节中展示了分析开销。最后，我们在第 [5.5](#S5.SS5 "5.5 Ablations ‣ 5 Experiment ‣ Model
    Tells You What to Discard: Adaptive KV Cache Compression for LLMs")节中呈现了消融研究和讨论。'
- en: 5.1 Trade-off between performance and memory reduction
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 性能与内存减少之间的权衡
- en: '![Refer to caption](img/00b6a4e053fc87afc5b7fb31f43e7e47.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/00b6a4e053fc87afc5b7fb31f43e7e47.png)'
- en: 'Figure 5: Performance of Adaptive KV Cache (FastGen) and Fixed KV Cache (Frequency+Local;
    [Zhang et al.](#bib.bib36), [2023](#bib.bib36) and [Liu et al.](#bib.bib23), [2023a](#bib.bib23))
    of Llama 1 on GSM8k, HumanEval, NQ, and TQA.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：Llama 1 在GSM8k、HumanEval、NQ和TQA上的自适应KV缓存（FastGen）和固定KV缓存（Frequency+Local；[Zhang
    et al.](#bib.bib36)、[2023](#bib.bib36) 和 [Liu et al.](#bib.bib23)、[2023a](#bib.bib23)）的性能。
- en: Backbones.
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主干。
- en: We conduct experiments with both Llama 1 (Touvron et al., [2023a](#bib.bib32))
    and its fine-tuned variants, with model sizes ranging from 7B to 65B. For fined-tuned
    variants, we do not choose the open-sourced Llama 2-chat (Touvron et al., [2023b](#bib.bib33))
    model due to its grouped-query attention techniques. Instead, we use the original
    multi-head attention architecture in this study and leave the integration of grouped-query
    attention to future work. To prepare a comparable instruction-following model
    for analysis, we fine-tuned the Llama 1 model with open-sourced instruction-tuning
    datasets. Specifically, the fine-tuned variants are trained on LIMA¹¹1https://huggingface.co/datasets/GAIR/lima.
    data (Zhou et al., [2023](#bib.bib37)) and Open Assistant²²2https://huggingface.co/datasets/OpenAssistant/oasst1.
    (Köpf et al., [2023](#bib.bib19)) data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Llama 1 (Touvron et al., [2023a](#bib.bib32))及其微调变体上进行实验，模型规模从7B到65B。对于微调变体，我们没有选择开源的Llama
    2-chat (Touvron et al., [2023b](#bib.bib33))模型，因为其使用了分组查询注意力技术。相反，我们在本研究中使用了原始的多头注意力架构，并将分组查询注意力的集成留待未来工作。为了准备一个可比较的指令跟随模型进行分析，我们使用开源的指令微调数据集微调了Llama
    1模型。具体而言，微调变体在LIMA¹¹1https://huggingface.co/datasets/GAIR/lima. 数据 (Zhou et al.,
    [2023](#bib.bib37)) 和Open Assistant²²2https://huggingface.co/datasets/OpenAssistant/oasst1.
    (Köpf et al., [2023](#bib.bib19)) 数据上训练。
- en: Tasks.
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务。
- en: We use standard generation tasks to evaluate Llama 1 and our fine-tuned Llama
    1 models. For Llama 1, we choose four different tasks, including HumanEval (Chen
    et al., [2021](#bib.bib5)), GSM8k (Cobbe et al., [2021](#bib.bib8)), NQ (Kwiatkowski
    et al., [2019](#bib.bib21)) and TQA (Kembhavi et al., [2017](#bib.bib17)) to evaluate
    models’ abilities on different domains (code, math, question answering and reading
    comprehension). Note that in the four tasks, each testing sample is in a generative
    format, where answers are extracted after model generation finishes. This is crucial
    for a fair comparison on model’s generation quality. We evaluate the instruction
    finetuned LLaMa model on the instruction tuning benchmark AlpacaEval (Li et al.,
    [2023](#bib.bib22)), which consists of 805 question prompts from diverse domains.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用标准生成任务来评估Llama 1及其微调版本的Llama 1模型。对于Llama 1，我们选择了四个不同的任务，包括HumanEval (Chen
    et al., [2021](#bib.bib5))、GSM8k (Cobbe et al., [2021](#bib.bib8))、NQ (Kwiatkowski
    et al., [2019](#bib.bib21))和TQA (Kembhavi et al., [2017](#bib.bib17))，以评估模型在不同领域（代码、数学、问答和阅读理解）的能力。请注意，在这四个任务中，每个测试样本都是生成格式，答案在模型生成完成后提取。这对于公平比较模型的生成质量至关重要。我们在指令微调基准AlpacaEval
    (Li et al., [2023](#bib.bib22))上评估了指令微调的LLaMa模型，该基准包含来自不同领域的805个问题提示。
- en: Experiment Setup.
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验设置。
- en: The evaluation of the Llama 1 model follows the default setting and evaluation
    metrics on each benchmark. We calculate F1 scores for GSM8k, NQ and TQA, and use
    the code execution Pass@1 rate for HumanEval. While evaluating an instruction-tuning
    model remains challenging, we follow previous work (Zhou et al., [2023](#bib.bib37);
    Touvron et al., [2023b](#bib.bib33)) to use GPT4 as an evaluator for pair-wise
    comparison between two different model generations. For each prompt, we input
    the FastGen generation and the generation from the same model with Full KV Cache
    as a pair, and ask GPT4 to judge which one is better.We then calculate the win
    rate of FastGen over Full Cache. Hypothetically, the win rate of a lossless method
    should be around 50%. Aside from full-cache models, we also include non-adaptive
    KV cache methods for comparison. Specifically, we apply ${\bm{C}}_{\mbox{local}}$
    to control the pruned KV cache ratio. For generation, we use nucleus sampling (Holtzman
    et al., [2019](#bib.bib15)) with temperature T = 0.6, p = 0.9. Experiments are
    conducted on 8 NVIDIA A100 80GB GPUs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对Llama 1模型的评估遵循默认设置和每个基准的评估指标。我们计算GSM8k、NQ和TQA的F1分数，并使用代码执行Pass@1率来评估HumanEval。虽然评估指令微调模型仍然具有挑战性，我们遵循之前的工作
    (Zhou et al., [2023](#bib.bib37); Touvron et al., [2023b](#bib.bib33)) 使用GPT4作为评估者进行模型生成的对比。对于每个提示，我们将FastGen生成的结果与同一模型的Full
    KV Cache生成结果作为一对输入，并要求GPT4判断哪个更好。然后计算FastGen相对于Full Cache的胜率。假设无损方法的胜率应约为50%。除了全缓存模型外，我们还包括非自适应KV缓存方法进行比较。具体而言，我们应用
    ${\bm{C}}_{\mbox{local}}$ 来控制修剪的KV缓存比例。对于生成，我们使用nucleus sampling (Holtzman et
    al., [2019](#bib.bib15))，温度T = 0.6，p = 0.9。实验在8台NVIDIA A100 80GB GPU上进行。
- en: Main Results.
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主要结果。
- en: 'In Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Model Tells You What to Discard:
    Adaptive KV Cache Compression for LLMs") and Figure [5](#S5.F5 "Figure 5 ‣ 5.1
    Trade-off between performance and memory reduction ‣ 5 Experiment ‣ Model Tells
    You What to Discard: Adaptive KV Cache Compression for LLMs"), we present the
    model quality as a function of KV cache budget increasing from $30\%$ win rate,
    FastGen can get as much as 44.9% pruned ratio on Llama 1-65B, compared to 16.9%
    pruned ratio on Llama 1-7B. In all settings, FastGen shows consistent and significant
    improvement over non-adaptive compression methods. The results validate the effectiveness
    of adaptive KV cache compression using FastGen, despite its simplicity.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ 模型告诉你要丢弃什么：面向 LLM 的自适应 KV 缓存压缩") 和图 [5](#S5.F5 "图
    5 ‣ 5.1 性能与内存减少之间的权衡 ‣ 5 实验 ‣ 模型告诉你要丢弃什么：面向 LLM 的自适应 KV 缓存压缩") 中，我们展示了模型质量作为 KV
    缓存预算增加的函数，FastGen 可以在 Llama 1-65B 上实现多达 44.9% 的剪枝率，而 Llama 1-7B 上则为 16.9%。在所有设置中，FastGen
    都表现出了相对于非自适应压缩方法的持续和显著的改进。结果验证了使用 FastGen 进行自适应 KV 缓存压缩的有效性，尽管其方法简单。
- en: '| Model Size | KV Cache | Win rate |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 模型大小 | KV 缓存 | 胜率 |'
- en: '| Full | FastGen | Pruned ratio | $T$ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Full | FastGen | 剪枝率 | $T$ |'
- en: '| 7B | 32Gb | 14Gb | 56.6% | 91% | 30.8% |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 7B | 32Gb | 14Gb | 56.6% | 91% | 30.8% |'
- en: '| 20Gb | 39.8% | 95% | 37.7% |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 20Gb | 39.8% | 95% | 37.7% |'
- en: '| 27Gb | 16.9% | 98% | 47.4% |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 27Gb | 16.9% | 98% | 47.4% |'
- en: '| 13B | 63Gb | 29Gb | 53.4% | 91% | 32.0% |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 13B | 63Gb | 29Gb | 53.4% | 91% | 32.0% |'
- en: '| 38Gb | 39.0% | 95% | 39.9% |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 38Gb | 39.0% | 95% | 39.9% |'
- en: '| 51Gb | 18.3% | 98% | 48.7% |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 51Gb | 18.3% | 98% | 48.7% |'
- en: '| 30B | 158Gb | 69Gb | 56.7% | 93% | 37.0% |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 30B | 158Gb | 69Gb | 56.7% | 93% | 37.0% |'
- en: '| 81Gb | 48.8% | 95% | 42.5% |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 81Gb | 48.8% | 95% | 42.5% |'
- en: '| 115Gb | 27.4% | 98% | 47.5% |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 115Gb | 27.4% | 98% | 47.5% |'
- en: '| 65B | 320Gb | 140Gb | 56.3% | 93% | 40.9% |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 65B | 320Gb | 140Gb | 56.3% | 93% | 40.9% |'
- en: '| 176Gb | 44.9% | 95% | 44.2% |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 176Gb | 44.9% | 95% | 44.2% |'
- en: '| 205Gb | 36.0% | 98% | 49.8% |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 205Gb | 36.0% | 98% | 49.8% |'
- en: 'Table 1: Memory footprint reduction by FastGen. We compared the memory consumption
    between models with full KV cache, and models compressed by FastGen on fine-tuned
    Llama 1.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：FastGen 的内存占用减少。我们比较了完整 KV 缓存模型和通过 FastGen 压缩的模型在微调 Llama 1 上的内存消耗。
- en: 5.2 Memory Footprint Reduction Analysis
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 内存占用减少分析
- en: 'We report the KV cache memory footprint reduction in Table [1](#S5.T1 "Table
    1 ‣ Main Results. ‣ 5.1 Trade-off between performance and memory reduction ‣ 5
    Experiment ‣ Model Tells You What to Discard: Adaptive KV Cache Compression for
    LLMs"). For all the evaluated 7B-65B models, we evaluate the memory consumption
    with a fixed batch size of 16, sequence length of 512, and model weights in fp16
    format. We observe that FastGen substantially reduces the KV cache memory footprint
    across all model sizes, with more significant reductions for larger models. Taking
    a win rate over 45% as little-to-no quality regression, FastGen can achieve $\sim$20%
    in Llama 1-13B and Llama 1-7B.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表 [1](#S5.T1 "表 1 ‣ 主要结果 ‣ 5.1 性能与内存减少之间的权衡 ‣ 5 实验 ‣ 模型告诉你要丢弃什么：面向 LLM 的自适应
    KV 缓存压缩") 中报告了 KV 缓存内存占用的减少。对于所有评估的 7B-65B 模型，我们在固定批量大小 16、序列长度 512 和模型权重为 fp16
    格式的条件下评估内存消耗。我们观察到 FastGen 在所有模型尺寸中大幅减少了 KV 缓存内存占用，对较大的模型减少更为显著。以超过 45% 的胜率视为几乎没有质量回退，FastGen
    在 Llama 1-13B 和 Llama 1-7B 上可以实现约 20% 的减少。
- en: 5.3 End-to-end Latency Improvement
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 端到端延迟改进
- en: 'Table 2: End-to-end latency comparison on Llama 1-7B.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：Llama 1-7B 的端到端延迟比较。
- en: '| Batch size | 1 | 2 | 8 | 16 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 1 | 2 | 8 | 16 |'
- en: '| [prompt len, gen len] | [32,512] | [32,2048] | [32,8192] | [32,16384] | [512,32]
    | [512,512] | [4096,4096] | [512,512] | [4096,4096] | [512,512] |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| [prompt len, gen len] | [32,512] | [32,2048] | [32,8192] | [32,16384] | [512,32]
    | [512,512] | [4096,4096] | [512,512] | [4096,4096] | [512,512] |'
- en: '| HF | 13.35 | 57.37 | 299 | 799.14 | 1.12 | 19.16 | 167.64 | 23.44 | OOM |
    OOM |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| HF | 13.35 | 57.37 | 299 | 799.14 | 1.12 | 19.16 | 167.64 | 23.44 | OOM |
    OOM |'
- en: '| DS | 11.58 | 47.12 | 201.23 | 435.74 | 0.79 | 10.45 | 91.04 | 12.93 | 127.94
    | OOM |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| DS | 11.58 | 47.12 | 201.23 | 435.74 | 0.79 | 10.45 | 91.04 | 12.93 | 127.94
    | OOM |'
- en: '| FastGen | 11.21 | 44.6 | 179.43 | 359.83 | 0.73 | 9.71 | 76.93 | 10.57 |
    82.16 | OOM |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| FastGen | 11.21 | 44.6 | 179.43 | 359.83 | 0.73 | 9.71 | 76.93 | 10.57 |
    82.16 | OOM |'
- en: '| Speed-up(%) over HF | 16.03% | 22.30% | 40.00% | 55.00% | 34.80% | 49.30%
    | 54.10% | 54.90% | - | OOM |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 相对于 HF 的加速（%） | 16.03% | 22.30% | 40.00% | 55.00% | 34.80% | 49.30% | 54.10%
    | 54.90% | - | OOM |'
- en: '| Speed-up(%) over DS | 3.20% | 5.35% | 10.83% | 17.42% | 7.59% | 7.08% | 15.50%
    | 18.25% | 35.78% | OOM |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 相对于 DS 的加速（%） | 3.20% | 5.35% | 10.83% | 17.42% | 7.59% | 7.08% | 15.50%
    | 18.25% | 35.78% | OOM |'
- en: 'To analyze the end-to-end speedup of FastGen, we present the end-to-end latency
    improvement over full-cache setting and a strong model acceleration baseline in
    Table [2](#S5.T2 "Table 2 ‣ 5.3 End-to-end Latency Improvement ‣ 5 Experiment
    ‣ Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"). In
    the experiment, we record the total duration in seconds, measured from the start
    of prompt encoding, until the end of generation as the end-to-end latency. For
    the full-cache baseline, we adopt the widely used Hugging Face Accelerate (HF)
    (Gugger et al., [2022](#bib.bib13)), denoted as HF in Table [2](#S5.T2 "Table
    2 ‣ 5.3 End-to-end Latency Improvement ‣ 5 Experiment ‣ Model Tells You What to
    Discard: Adaptive KV Cache Compression for LLMs"). For FastGen, we implemented
    a customized kernel to handle the KV cache pruning operation. Specifically, we
    adapt the kernel from Deepspeed (DS) (Aminabadi et al., [2022](#bib.bib1)) by
    adding the KV cache sparsity operation. We include the Deepspeed performance for
    fair comparison, denoted as DS in Table [2](#S5.T2 "Table 2 ‣ 5.3 End-to-end Latency
    Improvement ‣ 5 Experiment ‣ Model Tells You What to Discard: Adaptive KV Cache
    Compression for LLMs"). All methods are tested on the same Nvidia V100 GPUs.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '为了分析 FastGen 的端到端加速效果，我们在表格 [2](#S5.T2 "Table 2 ‣ 5.3 End-to-end Latency Improvement
    ‣ 5 Experiment ‣ Model Tells You What to Discard: Adaptive KV Cache Compression
    for LLMs") 中展示了相对于全缓存设置和强基线模型加速的端到端延迟改进。在实验中，我们记录了从提示编码开始到生成结束的总持续时间，作为端到端延迟。对于全缓存基线，我们采用了广泛使用的
    Hugging Face Accelerate (HF) (Gugger et al., [2022](#bib.bib13))，在表格 [2](#S5.T2
    "Table 2 ‣ 5.3 End-to-end Latency Improvement ‣ 5 Experiment ‣ Model Tells You
    What to Discard: Adaptive KV Cache Compression for LLMs") 中标记为 HF。对于 FastGen，我们实现了一个自定义的内核来处理
    KV 缓存修剪操作。具体来说，我们通过添加 KV 缓存稀疏操作将内核从 Deepspeed (DS) (Aminabadi et al., [2022](#bib.bib1))
    进行适配。为了公平比较，我们包括了 Deepspeed 的性能，在表格 [2](#S5.T2 "Table 2 ‣ 5.3 End-to-end Latency
    Improvement ‣ 5 Experiment ‣ Model Tells You What to Discard: Adaptive KV Cache
    Compression for LLMs") 中标记为 DS。所有方法都在相同的 Nvidia V100 GPU 上进行测试。'
- en: 'As shown in Table [2](#S5.T2 "Table 2 ‣ 5.3 End-to-end Latency Improvement
    ‣ 5 Experiment ‣ Model Tells You What to Discard: Adaptive KV Cache Compression
    for LLMs"), we can observe that FastGen achieves significant end-to-end speed-up
    across all the generation settings. For the least significant case, FastGen can
    have a decent $16.04\%$, as the generation length grows from 512 to 16k. When
    comparing FastGen to DeepSpeed, we can still observe significant speed-up that
    gets bigger with batch size and generation length. Considering DeepSpeed is a
    full-stack optimized inference system, where not only attention computation is
    optimized, there is still much room to further improve FastGen by polishing the
    sparsity kernel. We leave this unique research and engineering challenge to future
    works.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格 [2](#S5.T2 "Table 2 ‣ 5.3 End-to-end Latency Improvement ‣ 5 Experiment
    ‣ Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs") 所示，我们可以观察到
    FastGen 在所有生成设置中都实现了显著的端到端加速。在最不显著的情况下，FastGen 在生成长度从 512 增长到 16k 时可以达到相当不错的 $16.04\%$。与
    DeepSpeed 比较时，我们仍然可以观察到显著的加速，并且随着批量大小和生成长度的增加而变得更大。考虑到 DeepSpeed 是一个全栈优化的推理系统，不仅注意力计算得到了优化，FastGen
    还有很大的提升空间，可以通过优化稀疏内核进一步改进。我们将这一独特的研究和工程挑战留给未来的工作。'
- en: 5.4 Profiling Cost
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 性能分析
- en: 'Table 3: Profiling time of Llama 1-65B. The Overall Generation Duration is
    measured from the start of decoding to the end of the generation length. The Profiling
    Duration is measured from the start of the decoding until Fastgen finishes the
    policy search.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：Llama 1-65B 的性能分析时间。总体生成持续时间从解码开始到生成长度结束。性能分析持续时间从解码开始到 Fastgen 完成策略搜索。
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Generation &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成 &#124;'
- en: '&#124; Length &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长度 &#124;'
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Overall Generation &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总体生成 &#124;'
- en: '&#124; Duration (s) &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 持续时间 (s) &#124;'
- en: '|'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Profiling &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 性能分析 &#124;'
- en: '&#124; Duration (s) &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 持续时间 (s) &#124;'
- en: '|'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Decoding Time &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 解码时间 &#124;'
- en: '&#124; Per Token (s) &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 每个 Token (s) &#124;'
- en: '| Profiling/Overall (%) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 性能分析/总体 (%) |'
- en: '| 128 | 30.98 | 0.11 | 0.10 | 0.35% |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 30.98 | 0.11 | 0.10 | 0.35% |'
- en: '| 256 | 50.1 | 0.11 | 0.10 | 0.21% |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 256 | 50.1 | 0.11 | 0.10 | 0.21% |'
- en: '| 512 | 94.98 | 0.11 | 0.10 | 0.12% |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 512 | 94.98 | 0.11 | 0.10 | 0.12% |'
- en: '| 1024 | 157.43 | 0.11 | 0.10 | 0.07% |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 1024 | 157.43 | 0.11 | 0.10 | 0.07% |'
- en: 'To better understand the overhead of the profiling step, we compare the profiling
    time with the total generation time across different generation lengths. We present
    the result in Table [3](#S5.T3 "Table 3 ‣ 5.4 Profiling Cost ‣ 5 Experiment ‣
    Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs").'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '为了更好地理解分析步骤的开销，我们比较了不同生成长度下分析时间与总生成时间的比例。结果如表 [3](#S5.T3 "Table 3 ‣ 5.4 Profiling
    Cost ‣ 5 Experiment ‣ Model Tells You What to Discard: Adaptive KV Cache Compression
    for LLMs") 所示。'
- en: We can observe that the profiling time only accounts for a very small percentage
    of the total generation duration, up to $0.35\%$ when the generation length comes
    to 1024.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，分析时间仅占总生成时间的一小部分，当生成长度达到1024时最多为 $0.35\%$。
- en: In terms of extra memory usage, it’s mainly introduced by one of the compression
    strategies, ${\bm{C}}_{\mbox{frequent}}$ compared to storing KV cache only, which
    is a negligible cost.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 就额外的内存使用而言，主要由一种压缩策略${\bm{C}}_{\mbox{frequent}}$引入，与仅存储KV缓存相比，这是一项微不足道的成本。
- en: 5.5 Ablations
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 消融实验
- en: For all the ablations, we use a fixed targeted recovery ratio T $=0.98$.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有的消融实验，我们使用固定的目标恢复比率T $=0.98$。
- en: How one policy affect all the other policies?
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个策略如何影响所有其他策略？
- en: 'We study the complementary effects of each policy on the combination of all
    other policies in our framework. We examine changes in pruned KV cache and win
    rate while fixing the targeted recovery ratio $T$ reduce more KV caches than the
    others. However, their standalone non-adaptive deployment yields suboptimal performance,
    as depicted in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Model Tells You
    What to Discard: Adaptive KV Cache Compression for LLMs"), further verifying the
    importance of adapting different compression policies.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '我们研究了每个策略对我们框架中所有其他策略组合的互补效果。我们在固定目标恢复比率$T$的情况下，检查修剪后的KV缓存和胜率，$T$减少的KV缓存比其他策略更多。然而，它们单独的非自适应部署表现次优，如图
    [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Model Tells You What to Discard: Adaptive
    KV Cache Compression for LLMs") 所示，进一步验证了适应不同压缩策略的重要性。'
- en: '| Feasible Policy Set | Pruned KV Ratio | Win Rate |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 可行的策略集 | 修剪后的KV比例 | 胜率 |'
- en: '| ${\mathcal{C}}$ | 36.04% | 49.75% |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| ${\mathcal{C}}$ | 36.04% | 49.75% |'
- en: '| $\{{\bm{C}}_{\mbox{punct.}},{\bm{C}}_{\mbox{punct.+frequent}},{\bm{C}}_{\mbox{punct.+frequent+local}},{\bm{C}}_{\mbox{full}}\}$
    | 31.16% | 47.64% |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| $\{{\bm{C}}_{\mbox{punct.}},{\bm{C}}_{\mbox{punct.+frequent}},{\bm{C}}_{\mbox{punct.+frequent+local}},{\bm{C}}_{\mbox{full}}\}$
    | 31.16% | 47.64% |'
- en: '| $\{{\bm{C}}_{\mbox{special}},{\bm{C}}_{\mbox{special+frequent}},{\bm{C}}_{\mbox{special+frequent+local}},{\bm{C}}_{\mbox{full}}\}$
    | 34.23% | 49.56% |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| $\{{\bm{C}}_{\mbox{special}},{\bm{C}}_{\mbox{special+frequent}},{\bm{C}}_{\mbox{special+frequent+local}},{\bm{C}}_{\mbox{full}}\}$
    | 34.23% | 49.56% |'
- en: '| $\{{\bm{C}}_{\mbox{special}},{\bm{C}}_{\mbox{special+punct.}},{\bm{C}}_{\mbox{special+punct.+frequent}},{\bm{C}}_{\mbox{full}}\}$
    | 30.18% | 49.06% |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| $\{{\bm{C}}_{\mbox{special}},{\bm{C}}_{\mbox{special+punct.}},{\bm{C}}_{\mbox{special+punct.+frequent}},{\bm{C}}_{\mbox{full}}\}$
    | 30.18% | 49.06% |'
- en: '| $\{{\bm{C}}_{\mbox{special}},{\bm{C}}_{\mbox{special+punct.}},{\bm{C}}_{\mbox{special+punct.+local}},{\bm{C}}_{\mbox{full}}\}$
    | 21.26% | 46.08% |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| $\{{\bm{C}}_{\mbox{special}},{\bm{C}}_{\mbox{special+punct.}},{\bm{C}}_{\mbox{special+punct.+local}},{\bm{C}}_{\mbox{full}}\}$
    | 21.26% | 46.08% |'
- en: 'Table 4: Complementary effects of each policy. We display the win rate of each
    method over full cache setting. We evaluate the fine-tuned Llama 1-65B on AlpacaEval
    with the same parameters.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：每种策略的互补效果。我们展示了每种方法在完整缓存设置下的胜率。我们使用相同参数在AlpacaEval上评估了微调的Llama 1-65B。
- en: '| Cache Order | Pruned KV Ratio | Win Rate |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 缓存顺序 | 修剪后的KV比例 | 胜率 |'
- en: '| ${\bm{C}}_{\mbox{special}}\to{\bm{C}}_{\mbox{punct.}}\to{\bm{C}}_{\mbox{frequent}}\to{\bm{C}}_{\mbox{local}}$
    | 36.04% | 49.75% |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| ${\bm{C}}_{\mbox{special}}\to{\bm{C}}_{\mbox{punct.}}\to{\bm{C}}_{\mbox{frequent}}\to{\bm{C}}_{\mbox{local}}$
    | 36.04% | 49.75% |'
- en: '| ${\bm{C}}_{\mbox{special}}\to{\bm{C}}_{\mbox{frequent}}\to{\bm{C}}_{\mbox{local}}\to{\bm{C}}_{\mbox{punct.}}$
    | 36.40% | 47.64% |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| ${\bm{C}}_{\mbox{special}}\to{\bm{C}}_{\mbox{frequent}}\to{\bm{C}}_{\mbox{local}}\to{\bm{C}}_{\mbox{punct.}}$
    | 36.40% | 47.64% |'
- en: 'Table 5: Policy order ablation on fine-tuned Llama 1-65B with AlpacaEval.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：对微调的Llama 1-65B进行的策略顺序消融实验，使用AlpacaEval。
- en: '![Refer to caption](img/686eedee136ad78f2a9e20acc612feac.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/686eedee136ad78f2a9e20acc612feac.png)'
- en: 'Figure 6: Hyper-parameter ablation on fine-tuned Llama 1-65B with AlpacaEval.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：对微调的Llama 1-65B进行的超参数消融实验，使用AlpacaEval。
- en: Which policy should we add first (and last)?
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们应该首先（和最后）添加哪种策略？
- en: 'As in Section [3.4](#S3.SS4 "3.4 KV Cache Compression Policies ‣ 3 Adaptive
    KV Cache Compression ‣ Model Tells You What to Discard: Adaptive KV Cache Compression
    for LLMs"), we use a greed method to construct adaptive KV cache. Here, we examine
    how the order of introducing each policy affects the performance. Similar to the
    previous study, we fix the targeted recovery ratio to 0.98, and keep allocating
    cache budget until the constructed cache hit the recovery ratio. For simplicity,
    we make every examined order opt-in the ${\bm{C}}_{\mbox{special}}$ leads to an
    improved KV cache compression ratio at the cost of generation quality.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[3.4节](#S3.SS4 "3.4 KV缓存压缩策略 ‣ 3 适应性KV缓存压缩 ‣ 模型告诉你丢弃什么：适应性KV缓存压缩用于LLMs")所述，我们使用贪婪方法来构建自适应KV缓存。在这里，我们考察了引入每个策略的顺序如何影响性能。与之前的研究类似，我们将目标恢复比率固定为0.98，并继续分配缓存预算，直到构建的缓存达到恢复比率。为简化起见，我们使每个检查过的顺序选择${\bm{C}}_{\mbox{special}}$，在生成质量的代价下提高了KV缓存压缩比率。
- en: Sensitivity Study.
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 敏感性研究。
- en: 'We analyze the sensitivity of selecting different hyper-parameters for FastGen,
    as illustrated in Figure [6](#S5.F6 "Figure 6 ‣ How one policy affect all the
    other policies? ‣ 5.5 Ablations ‣ 5 Experiment ‣ Model Tells You What to Discard:
    Adaptive KV Cache Compression for LLMs"). We observe that altering these hyper-parameters
    does not have a visible impact on the generation quality, as the model maintains
    a winrate over 45% in all situations. Meanwhile, it leads to a relative large
    change on the compression ratio. For example, changing the ratio for the frequency
    policy from 0.3 to 0.1 leads to more KV cache. In our experiments, we set the
    ratio to 0.3 for both $r_{l}$.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析了选择不同超参数对FastGen的敏感性，如图[6](#S5.F6 "图 6 ‣ 一个策略如何影响所有其他策略？ ‣ 5.5 消融实验 ‣ 5
    实验 ‣ 模型告诉你丢弃什么：适应性KV缓存压缩用于LLMs")所示。我们观察到，改变这些超参数对生成质量没有明显影响，因为模型在所有情况下的胜率均保持在45%以上。同时，它导致压缩比发生相对较大的变化。例如，将频率策略的比率从0.3更改为0.1会导致更多的KV缓存。在我们的实验中，我们将比率设置为0.3用于两个$r_{l}$。
- en: 6 Conclusion
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We have presented FastGen, a novel method that significantly improves the inference
    efficiency of LLMs, with no visible quality loss, using lightweight model profiling
    and adaptive key-value caching. Areas for future explorations include combining
    FastGen with other model compression techniques, such as quantization and distillation,
    and other efficient attention architectures, such as grouped-query attention.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了FastGen，这是一种新方法，显著提高了LLMs的推理效率，并且没有明显的质量损失，采用了轻量级模型分析和自适应键值缓存。未来的研究方向包括将FastGen与其他模型压缩技术（如量化和蒸馏）以及其他高效的注意力架构（如分组查询注意力）结合。
- en: References
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Aminabadi et al. (2022) Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia
    Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith,
    Olatunji Ruwase, and Yuxiong He. Deepspeed- inference: Enabling efficient inference
    of transformer models at unprecedented scale. *SC22: International Conference
    for High Performance Computing, Networking, Storage and Analysis*, pp.  1–15,
    2022.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Aminabadi 等（2022）雷扎·亚兹达尼·阿米纳巴迪、萨米亚姆·拉杰班达里、闵佳·张、阿玛尔·艾哈迈德·阿万、程丽、杜丽、埃尔顿·郑、杰夫·拉斯利、沙登·史密斯、奥拉图恩吉·鲁瓦塞和余雄·赫。Deepspeed-
    推理：在前所未有的规模上实现高效的变换器模型推理。*SC22: 高性能计算、网络、存储与分析国际会议*，第1–15页，2022年。'
- en: Bach et al. (2015) Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick
    Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations
    for non-linear classifier decisions by layer-wise relevance propagation. *PLoS
    ONE*, 10, 2015. URL [https://api.semanticscholar.org/CorpusID:9327892](https://api.semanticscholar.org/CorpusID:9327892).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bach 等（2015）塞巴斯蒂安·巴赫、亚历山大·宾德、格雷戈尔·蒙塔冯、弗雷德里克·克劳申、克劳斯-罗伯特·穆勒和沃伊切赫·萨梅克。关于通过逐层相关传播的非线性分类器决策的像素级解释。*PLoS
    ONE*，10，2015。网址 [https://api.semanticscholar.org/CorpusID:9327892](https://api.semanticscholar.org/CorpusID:9327892)。
- en: 'Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio
    Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), *Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei。语言模型是少样本学习者。收录于 Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
    Balcan, 和 Hsuan-Tien Lin（编），*神经信息处理系统进展 33：2020年神经信息处理系统年会，NeurIPS 2020，2020年12月6-12日，虚拟会议*，2020年。
- en: 'Campos et al. (2017) Víctor Campos, Brendan Jou, Xavier Giró i Nieto, Jordi
    Torres, and Shih-Fu Chang. Skip rnn: Learning to skip state updates in recurrent
    neural networks. *ArXiv*, abs/1708.06834, 2017. URL [https://api.semanticscholar.org/CorpusID:1859294](https://api.semanticscholar.org/CorpusID:1859294).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Campos 等人（2017）Víctor Campos, Brendan Jou, Xavier Giró i Nieto, Jordi Torres,
    和 Shih-Fu Chang。Skip rnn: 学习在递归神经网络中跳过状态更新。*ArXiv*, abs/1708.06834, 2017。网址 [https://api.semanticscholar.org/CorpusID:1859294](https://api.semanticscholar.org/CorpusID:1859294)。'
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. Evaluating large language models trained on code. *arXiv
    preprint arXiv:2107.03374*, 2021.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2021）Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman 等人。评估在代码上训练的大型语言模型。*arXiv 预印本 arXiv:2107.03374*，2021年。
- en: Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    Generating long sequences with sparse transformers. *CoRR*, abs/1904.10509, 2019.
    URL [http://arxiv.org/abs/1904.10509](http://arxiv.org/abs/1904.10509).
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child 等人（2019）Rewon Child, Scott Gray, Alec Radford, 和 Ilya Sutskever。生成长序列的稀疏变换器。*CoRR*,
    abs/1904.10509, 2019。网址 [http://arxiv.org/abs/1904.10509](http://arxiv.org/abs/1904.10509)。
- en: 'Clark et al. (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D.
    Manning. What does BERT look at? an analysis of BERT’s attention. In *Proceedings
    of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks
    for NLP*, pp.  276–286, Florence, Italy, August 2019\. Association for Computational
    Linguistics. doi: 10.18653/v1/W19-4828. URL [https://aclanthology.org/W19-4828](https://aclanthology.org/W19-4828).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark 等人（2019）Kevin Clark, Urvashi Khandelwal, Omer Levy, 和 Christopher D.
    Manning。BERT 看了什么？对 BERT 注意力的分析。收录于 *2019 ACL Workshop BlackboxNLP: 分析与解释 NLP
    神经网络*，页码 276–286，意大利佛罗伦萨，2019年8月。计算语言学协会。doi: 10.18653/v1/W19-4828。网址 [https://aclanthology.org/W19-4828](https://aclanthology.org/W19-4828)。'
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*, 2021.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人（2021）Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo
    Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano
    等人。训练验证器以解决数学应用题。*arXiv 预印本 arXiv:2110.14168*，2021年。
- en: 'Dai et al. (2020) Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer:
    Filtering out sequential redundancy for efficient language processing. In Hugo
    Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
    Lin (eds.), *Advances in Neural Information Processing Systems 33: Annual Conference
    on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
    virtual*, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/2cd2915e69546904e4e5d4a2ac9e1652-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/2cd2915e69546904e4e5d4a2ac9e1652-Abstract.html).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人（2020）Zihang Dai, Guokun Lai, Yiming Yang, 和 Quoc Le。漏斗变换器：过滤序列冗余以提高语言处理效率。收录于
    Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, 和 Hsuan-Tien
    Lin（编），*神经信息处理系统进展 33：2020年神经信息处理系统年会，NeurIPS 2020，2020年12月6-12日，虚拟会议*，2020年。网址
    [https://proceedings.neurips.cc/paper/2020/hash/2cd2915e69546904e4e5d4a2ac9e1652-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/2cd2915e69546904e4e5d4a2ac9e1652-Abstract.html)。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: pre-training of deep bidirectional transformers for language
    understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,
    June 2-7, 2019, Volume 1 (Long and Short Papers)*, pp.  4171–4186\. Association
    for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    BERT: 深度双向变换器的预训练用于语言理解。见 Jill Burstein, Christy Doran, 和 Thamar Solorio（编），*2019年北美计算语言学协会：人类语言技术会议录，NAACL-HLT
    2019，美国明尼阿波利斯，2019年6月2-7日，第1卷（长篇和短篇论文）*，第 4171–4186 页。计算语言学协会，2019年。doi: 10.18653/v1/n19-1423。网址
    [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423)。'
- en: 'Goyal et al. (2020) Saurabh Goyal, Anamitra R. Choudhury, Saurabh Raje, Venkatesan T.
    Chakaravarthy, Yogish Sabharwal, and Ashish Verma. Power-bert: Accelerating bert
    inference via progressive word-vector elimination. In *International Conference
    on Machine Learning*, 2020. URL [https://api.semanticscholar.org/CorpusID:219792793](https://api.semanticscholar.org/CorpusID:219792793).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goyal et al. (2020) Saurabh Goyal, Anamitra R. Choudhury, Saurabh Raje, Venkatesan
    T. Chakaravarthy, Yogish Sabharwal, 和 Ashish Verma. Power-bert: 通过渐进的词向量消除加速bert推理。在
    *国际机器学习大会*，2020年。网址 [https://api.semanticscholar.org/CorpusID:219792793](https://api.semanticscholar.org/CorpusID:219792793)。'
- en: 'Guan et al. (2022) Yue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin, and Minyi
    Guo. Transkimmer: Transformer learns to layer-wise skim. In Smaranda Muresan,
    Preslav Nakov, and Aline Villavicencio (eds.), *Proceedings of the 60th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    ACL 2022, Dublin, Ireland, May 22-27, 2022*, pp.  7275–7286\. Association for
    Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.502. URL [https://doi.org/10.18653/v1/2022.acl-long.502](https://doi.org/10.18653/v1/2022.acl-long.502).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guan et al. (2022) Yue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin, 和 Minyi
    Guo. Transkimmer: Transformer 学会逐层略读。见 Smaranda Muresan, Preslav Nakov, 和 Aline
    Villavicencio（编），*第60届计算语言学协会年会会议录（第1卷：长篇论文），ACL 2022，爱尔兰都柏林，2022年5月22-27日*，第
    7275–7286 页。计算语言学协会，2022年。doi: 10.18653/v1/2022.acl-long.502。网址 [https://doi.org/10.18653/v1/2022.acl-long.502](https://doi.org/10.18653/v1/2022.acl-long.502)。'
- en: 'Gugger et al. (2022) Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid,
    Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate:
    Training and inference at scale made simple, efficient and adaptable. [https://github.com/huggingface/accelerate](https://github.com/huggingface/accelerate),
    2022.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gugger et al. (2022) Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid,
    Zachary Mueller, Sourab Mangrulkar, Marc Sun, 和 Benjamin Bossan. Accelerate: 使大规模训练和推理变得简单、高效和可适应。[https://github.com/huggingface/accelerate](https://github.com/huggingface/accelerate)，2022年。'
- en: Hansen et al. (2019) Christian Hansen, Casper Hansen, Stephen Alstrup, Jakob Grue
    Simonsen, and Christina Lioma. Neural speed reading with structural-jump-lstm.
    *ArXiv*, abs/1904.00761, 2019. URL [https://api.semanticscholar.org/CorpusID:90258012](https://api.semanticscholar.org/CorpusID:90258012).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hansen et al. (2019) Christian Hansen, Casper Hansen, Stephen Alstrup, Jakob
    Grue Simonsen, 和 Christina Lioma. 具有结构跳跃LSTM的神经速度阅读。*ArXiv*，abs/1904.00761，2019年。网址
    [https://api.semanticscholar.org/CorpusID:90258012](https://api.semanticscholar.org/CorpusID:90258012)。
- en: Holtzman et al. (2019) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin
    Choi. The curious case of neural text degeneration. *arXiv preprint arXiv:1904.09751*,
    2019.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holtzman et al. (2019) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, 和 Yejin
    Choi. 神经文本退化的奇特案例。*arXiv 预印本 arXiv:1904.09751*，2019年。
- en: 'Huang et al. (2022) Xin Huang, Ashish Khetan, Rene Bidart, and Zohar Karnin.
    Pyramid-bert: Reducing complexity via successive core-set based token selection.
    In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022*, pp.  8798–8817\.
    Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.602.
    URL [https://doi.org/10.18653/v1/2022.acl-long.602](https://doi.org/10.18653/v1/2022.acl-long.602).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2022) Xin Huang, Ashish Khetan, Rene Bidart, 和 Zohar Karnin.
    Pyramid-bert: 通过连续核心集基础的标记选择来减少复杂性。见 Smaranda Muresan, Preslav Nakov, 和 Aline
    Villavicencio（编），*第60届计算语言学协会年会会议录（第1卷：长篇论文），ACL 2022，爱尔兰都柏林，2022年5月22-27日*，第
    8798–8817 页。计算语言学协会，2022年。doi: 10.18653/v1/2022.acl-long.602。网址 [https://doi.org/10.18653/v1/2022.acl-long.602](https://doi.org/10.18653/v1/2022.acl-long.602)。'
- en: Kembhavi et al. (2017) Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun
    Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader?
    textbook question answering for multimodal machine comprehension. *2017 IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*, pp.  5376–5384, 2017. URL
    [https://api.semanticscholar.org/CorpusID:1310550](https://api.semanticscholar.org/CorpusID:1310550).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kembhavi et al. (2017) Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun
    Choi, Ali Farhadi, 和 Hannaneh Hajishirzi. 你比六年级学生更聪明吗？多模态机器理解的教科书问答。*2017年IEEE计算机视觉与模式识别会议（CVPR）*，第
    5376–5384 页，2017。网址 [https://api.semanticscholar.org/CorpusID:1310550](https://api.semanticscholar.org/CorpusID:1310550)。
- en: 'Kim et al. (2022) Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk
    Kwon, Joseph Hassoun, and Kurt Keutzer. Learned token pruning for transformers.
    In Aidong Zhang and Huzefa Rangwala (eds.), *KDD ’22: The 28th ACM SIGKDD Conference
    on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022*,
    pp.  784–794\. ACM, 2022. doi: 10.1145/3534678.3539260. URL [https://doi.org/10.1145/3534678.3539260](https://doi.org/10.1145/3534678.3539260).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim et al. (2022) Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk
    Kwon, Joseph Hassoun, 和 Kurt Keutzer. 用于变压器的学习型标记修剪。收录于 Aidong Zhang 和 Huzefa
    Rangwala (编辑), *KDD ’22: 第28届ACM SIGKDD知识发现与数据挖掘会议，美国华盛顿特区，2022年8月14 - 18日*，第
    784–794 页。ACM，2022。doi: 10.1145/3534678.3539260。网址 [https://doi.org/10.1145/3534678.3539260](https://doi.org/10.1145/3534678.3539260)。'
- en: 'Köpf et al. (2023) Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris
    Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver
    Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri,
    Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. Openassistant
    conversations - democratizing large language model alignment. *CoRR*, abs/2304.07327,
    2023. doi: 10.48550/arXiv.2304.07327. URL [https://doi.org/10.48550/arXiv.2304.07327](https://doi.org/10.48550/arXiv.2304.07327).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Köpf et al. (2023) Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris
    Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver
    Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri,
    Andrew Maguire, Christoph Schuhmann, Huu Nguyen, 和 Alexander Mattick. Openassistant
    对话 - 使大型语言模型对齐民主化。*CoRR*，abs/2304.07327，2023。doi: 10.48550/arXiv.2304.07327。网址
    [https://doi.org/10.48550/arXiv.2304.07327](https://doi.org/10.48550/arXiv.2304.07327)。'
- en: 'Kovaleva et al. (2019) Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna
    Rumshisky. Revealing the dark secrets of BERT. In Kentaro Inui, Jing Jiang, Vincent
    Ng, and Xiaojun Wan (eds.), *Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019*,
    pp.  4364–4373\. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1445.
    URL [https://doi.org/10.18653/v1/D19-1445](https://doi.org/10.18653/v1/D19-1445).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kovaleva et al. (2019) Olga Kovaleva, Alexey Romanov, Anna Rogers, 和 Anna Rumshisky.
    揭示 BERT 的黑暗秘密。收录于 Kentaro Inui, Jing Jiang, Vincent Ng, 和 Xiaojun Wan (编辑), *2019年自然语言处理实证方法会议暨第九届国际联合自然语言处理会议（EMNLP-IJCNLP
    2019），中国香港，2019年11月3-7日*，第 4364–4373 页。计算语言学协会，2019。doi: 10.18653/v1/D19-1445。网址
    [https://doi.org/10.18653/v1/D19-1445](https://doi.org/10.18653/v1/D19-1445)。'
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones,
    Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural
    questions: a benchmark for question answering research. *Transactions of the Association
    of Computational Linguistics*, 2019.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones,
    Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, 和 Slav Petrov. 自然问题：一个问答研究基准。*计算语言学协会会刊*，2019。
- en: 'Li et al. (2023) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval:
    An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval),
    2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. Alpacaeval:
    一种自动评估指令跟随模型的工具。 [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)，2023。'
- en: 'Liu et al. (2023a) Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor
    Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands:
    Exploiting the persistence of importance hypothesis for LLM KV cache compression
    at test time. *CoRR*, abs/2305.17118, 2023a. doi: 10.48550/arXiv.2305.17118. URL
    [https://doi.org/10.48550/arXiv.2305.17118](https://doi.org/10.48550/arXiv.2305.17118).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2023a）Zichang Liu、Aditya Desai、Fangshuo Liao、Weitao Wang、Victor Xie、Zhaozhuo
    Xu、Anastasios Kyrillidis和Anshumali Shrivastava。Scissorhands：利用重要性假设在测试时对LLM KV缓存进行压缩。*CoRR*，abs/2305.17118，2023a年。doi：10.48550/arXiv.2305.17118。网址
    [https://doi.org/10.48550/arXiv.2305.17118](https://doi.org/10.48550/arXiv.2305.17118)。
- en: 'Liu et al. (2023b) Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan,
    Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Ré, and
    Beidi Chen. Deja vu: Contextual sparsity for efficient llms at inference time.
    In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,
    and Jonathan Scarlett (eds.), *International Conference on Machine Learning, ICML
    2023, 23-29 July 2023, Honolulu, Hawaii, USA*, volume 202 of *Proceedings of Machine
    Learning Research*, pp. 22137–22176\. PMLR, 2023b. URL [https://proceedings.mlr.press/v202/liu23am.html](https://proceedings.mlr.press/v202/liu23am.html).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2023b）Zichang Liu、Jue Wang、Tri Dao、Tianyi Zhou、Binhang Yuan、Zhao Song、Anshumali
    Shrivastava、Ce Zhang、Yuandong Tian、Christopher Ré和Beidi Chen。Deja vu：推理时LLMs的上下文稀疏。在Andreas
    Krause、Emma Brunskill、Kyunghyun Cho、Barbara Engelhardt、Sivan Sabato和Jonathan Scarlett（编辑）主编的*国际机器学习大会，ICML
    2023，2023年7月23-29日，夏威夷檀香山，美国*，第202卷*机器学习研究论文集*，第22137–22176页。PMLR，2023b年。网址 [https://proceedings.mlr.press/v202/liu23am.html](https://proceedings.mlr.press/v202/liu23am.html)。
- en: Michel et al. (2019) Paul Michel, Omer Levy, and Graham Neubig. Are sixteen
    heads really better than one? In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,
    E. Fox, and R. Garnett (eds.), *Advances in Neural Information Processing Systems*,
    volume 32\. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michel等（2019）Paul Michel、Omer Levy和Graham Neubig。十六个头真的比一个头更好吗？在H. Wallach、H.
    Larochelle、A. Beygelzimer、F. d'Alché-Buc、E. Fox和R. Garnett（编辑）主编的*Advances in
    Neural Information Processing Systems*，第32卷。Curran Associates, Inc.，2019年。网址 [https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf)。
- en: 'Mu et al. (2023) Jesse Mu, Xiang Lisa Li, and Noah D. Goodman. Learning to
    compress prompts with gist tokens. *CoRR*, abs/2304.08467, 2023. doi: 10.48550/arXiv.2304.08467.
    URL [https://doi.org/10.48550/arXiv.2304.08467](https://doi.org/10.48550/arXiv.2304.08467).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu等（2023）Jesse Mu、Xiang Lisa Li和Noah D. Goodman。学习通过gist tokens压缩提示。*CoRR*，abs/2304.08467，2023年。doi：10.48550/arXiv.2304.08467。网址
    [https://doi.org/10.48550/arXiv.2304.08467](https://doi.org/10.48550/arXiv.2304.08467)。
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. Gpt-4技术报告，2023年。
- en: Seo et al. (2017) Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi.
    Neural speed reading via skim-rnn. *ArXiv*, abs/1711.02085, 2017. URL [https://api.semanticscholar.org/CorpusID:3140413](https://api.semanticscholar.org/CorpusID:3140413).
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seo等（2017）Minjoon Seo、Sewon Min、Ali Farhadi和Hannaneh Hajishirzi。通过skim-rnn实现神经速读。*ArXiv*，abs/1711.02085，2017年。网址
    [https://api.semanticscholar.org/CorpusID:3140413](https://api.semanticscholar.org/CorpusID:3140413)。
- en: 'Shazeer et al. (2017) Noam M. Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
    Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. Outrageously large
    neural networks: The sparsely-gated mixture-of-experts layer. *ArXiv*, abs/1701.06538,
    2017. URL [https://api.semanticscholar.org/CorpusID:12462234](https://api.semanticscholar.org/CorpusID:12462234).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer等（2017）Noam M. Shazeer、Azalia Mirhoseini、Krzysztof Maziarz、Andy Davis、Quoc
    V. Le、Geoffrey E. Hinton和Jeff Dean。极其庞大的神经网络：稀疏门控专家混合层。*ArXiv*，abs/1701.06538，2017年。网址
    [https://api.semanticscholar.org/CorpusID:12462234](https://api.semanticscholar.org/CorpusID:12462234)。
- en: Sheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark W. Barrett, Joseph Gonzalez,
    Percy Liang, Christopher Ré, Ioan Cristian Stoica, and Ce Zhang. High-throughput
    generative inference of large language models with a single gpu. In *International
    Conference on Machine Learning*, 2023. URL [https://api.semanticscholar.org/CorpusID:257495837](https://api.semanticscholar.org/CorpusID:257495837).
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheng等（2023）Ying Sheng、Lianmin Zheng、Binhang Yuan、Zhuohan Li、Max Ryabinin、Daniel
    Y. Fu、Zhiqiang Xie、Beidi Chen、Clark W. Barrett、Joseph Gonzalez、Percy Liang、Christopher
    Ré、Ioan Cristian Stoica和Ce Zhang。利用单个GPU的高通量生成推断大型语言模型。在*国际机器学习大会*，2023年。网址 [https://api.semanticscholar.org/CorpusID:257495837](https://api.semanticscholar.org/CorpusID:257495837)。
- en: 'Sun et al. (2022) Tianxiang Sun, Xiangyang Liu, Wei Zhu, Zhichao Geng, Lingling
    Wu, Yilong He, Yuan Ni, Guotong Xie, Xuanjing Huang, and Xipeng Qiu. A simple
    hash-based early exiting approach for language understanding and generation. In
    Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), *Findings of
    the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May
    22-27, 2022*, pp.  2409–2421\. Association for Computational Linguistics, 2022.
    doi: 10.18653/v1/2022.findings-acl.189. URL [https://doi.org/10.18653/v1/2022.findings-acl.189](https://doi.org/10.18653/v1/2022.findings-acl.189).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等 (2022) Tianxiang Sun, Xiangyang Liu, Wei Zhu, Zhichao Geng, Lingling
    Wu, Yilong He, Yuan Ni, Guotong Xie, Xuanjing Huang, 和 Xipeng Qiu. 一种基于哈希的简单早期退出方法用于语言理解和生成。在
    Smaranda Muresan, Preslav Nakov, 和 Aline Villavicencio (编辑), *计算语言学协会会议论文集：ACL
    2022, 都柏林, 爱尔兰, 2022年5月22-27日*, 第2409–2421页。计算语言学协会, 2022年。doi: 10.18653/v1/2022.findings-acl.189。URL
    [https://doi.org/10.18653/v1/2022.findings-acl.189](https://doi.org/10.18653/v1/2022.findings-acl.189)。'
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, 等. Llama: 开放和高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*, 2023a。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023b.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, 和 Thomas Scialom. Llama 2:
    开放基础和微调聊天模型, 2023b。'
- en: 'Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the
    heavy lifting, the rest can be pruned, July 2019. URL [https://aclanthology.org/P19-1580](https://aclanthology.org/P19-1580).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voita 等 (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, 和 Ivan
    Titov. 分析多头自注意力：专业化头部做繁重工作，其余部分可以被修剪, 2019年7月。URL [https://aclanthology.org/P19-1580](https://aclanthology.org/P19-1580)。
- en: 'Wang et al. (2020) Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning
    of large language models. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu
    (eds.), *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2020, Online, November 16-20, 2020*, pp. 6151–6162\. Association
    for Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.496. URL
    [https://doi.org/10.18653/v1/2020.emnlp-main.496](https://doi.org/10.18653/v1/2020.emnlp-main.496).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 (2020) Ziheng Wang, Jeremy Wohlwend, 和 Tao Lei. 大型语言模型的结构化修剪。在 Bonnie
    Webber, Trevor Cohn, Yulan He, 和 Yang Liu (编辑), *2020年自然语言处理经验方法会议论文集, EMNLP 2020,
    在线, 2020年11月16-20日*, 第6151–6162页。计算语言学协会, 2020年。doi: 10.18653/v1/2020.emnlp-main.496。URL
    [https://doi.org/10.18653/v1/2020.emnlp-main.496](https://doi.org/10.18653/v1/2020.emnlp-main.496)。'
- en: 'Zhang et al. (2023) Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett,
    Zhangyang Wang, and Beidi Chen. H${}_{\mbox{2}}$o: Heavy-hitter oracle for efficient
    generative inference of large language models. *CoRR*, abs/2306.14048, 2023. doi:
    10.48550/arXiv.2306.14048. URL [https://doi.org/10.48550/arXiv.2306.14048](https://doi.org/10.48550/arXiv.2306.14048).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023) 张振宇、盛颖、周天一、陈天龙、郑连敏、蔡瑞斯、宋赵、田元东、克里斯托弗·雷、克拉克·W·巴雷特、王张扬、和陈贝迪。H${}_{\mbox{2}}$o:
    高重频预言机用于大型语言模型的高效生成推理。*CoRR*，abs/2306.14048，2023年。doi: 10.48550/arXiv.2306.14048。网址
    [https://doi.org/10.48550/arXiv.2306.14048](https://doi.org/10.48550/arXiv.2306.14048)。'
- en: 'Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh,
    Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: less is more for alignment.
    *CoRR*, abs/2305.11206, 2023. doi: 10.48550/arXiv.2305.11206. URL [https://doi.org/10.48550/arXiv.2305.11206](https://doi.org/10.48550/arXiv.2305.11206).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2023) 周春亭、刘鹏飞、许浦欣、伊耶尔、孙佳、毛玉宁、马学哲、艾维亚·埃夫拉特、余平、余莉莉、张苏珊、戈尔吉·戈什、迈克·刘易斯、卢克·泽特尔莫耶、和奥默尔·利维。LIMA:
    少即是多用于对齐。*CoRR*，abs/2305.11206，2023年。doi: 10.48550/arXiv.2305.11206。网址 [https://doi.org/10.48550/arXiv.2305.11206](https://doi.org/10.48550/arXiv.2305.11206)。'
- en: 'Zhou et al. (2020) Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian J. McAuley,
    Ke Xu, and Furu Wei. BERT loses patience: Fast and robust inference with early
    exit. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
    and Hsuan-Tien Lin (eds.), *Advances in Neural Information Processing Systems
    33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
    December 6-12, 2020, virtual*, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/d4dd111a4fd973394238aca5c05bebe3-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/d4dd111a4fd973394238aca5c05bebe3-Abstract.html).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2020) 周望春、徐灿文、葛涛、朱利安·J·麦考利、徐科、魏富如。BERT 失去耐心：通过提前退出实现快速而稳健的推理。在休戈·拉罗谢尔、马尔克·奥雷里奥·兰扎托、雷亚·哈德塞尔、玛丽亚-弗洛丽娜·巴尔坎、和玄天林（编辑），*神经信息处理系统进展
    33：2020年神经信息处理系统年会（NeurIPS 2020），2020年12月6-12日，虚拟*，2020年。网址 [https://proceedings.neurips.cc/paper/2020/hash/d4dd111a4fd973394238aca5c05bebe3-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/d4dd111a4fd973394238aca5c05bebe3-Abstract.html)。
