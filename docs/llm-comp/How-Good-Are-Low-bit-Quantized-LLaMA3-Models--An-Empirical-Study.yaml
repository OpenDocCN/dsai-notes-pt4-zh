- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:49:43'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:49:43'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低比特量化的LLaMA3模型效果如何？一项实证研究
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.14047](https://ar5iv.labs.arxiv.org/html/2404.14047)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.14047](https://ar5iv.labs.arxiv.org/html/2404.14047)
- en: Wei Huang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 魏·黄
- en: The University of Hong Kong
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 香港大学
- en: weih@connect.hku.hk
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: weih@connect.hku.hk
- en: Xudong Ma^∗
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 徐东·马^∗
- en: Beihang University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 北京航空航天大学
- en: macaronlin@buaa.edu.cn
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: macaronlin@buaa.edu.cn
- en: Haotong Qin^†
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 郝通·秦^†
- en: ETH Zurich
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ETH 苏黎世
- en: haotong.qin@pbl.ee.ethz.ch
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: haotong.qin@pbl.ee.ethz.ch
- en: Xingyu Zheng
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 星宇·郑
- en: Beihang University
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 北京航空航天大学
- en: xingyuzheng@buaa.edu.cn
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: xingyuzheng@buaa.edu.cn
- en: Chengtao Lv
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 程涛·吕
- en: Beihang University
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 北京航空航天大学
- en: lvchengtao@buaa.edu.cn
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: lvchengtao@buaa.edu.cn
- en: Hong Chen
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 洪·陈
- en: Beihang University
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 北京航空航天大学
- en: 18373205@buaa.edu.cn
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 18373205@buaa.edu.cn
- en: Jie Luo
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 杰·罗
- en: Beihang University
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 北京航空航天大学
- en: luojie@buaa.edu.cn
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: luojie@buaa.edu.cn
- en: Xiaojuan Qi
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 小娟·齐
- en: The University of Hong Kong
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 香港大学
- en: xjqi@eee.hku.hk
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: xjqi@eee.hku.hk
- en: Xianglong Liu
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 向龙·刘
- en: Beihang University
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 北京航空航天大学
- en: xlliu@buaa.edu.cn
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: xlliu@buaa.edu.cn
- en: Michele Magno
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 米歇尔·马格诺
- en: ETH Zurich
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ETH 苏黎世
- en: michele.magno@pbl.ee.ethz.ch  Equal Contribution. ^† Corresponding Author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: michele.magno@pbl.ee.ethz.ch  同等贡献。^† 通讯作者。
- en: Abstract
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Meta’s LLaMA family has become one of the most powerful open-source Large Language
    Model (LLM) series. Notably, LLaMA3 models have recently been released and achieve
    impressive performance across various with super-large scale pre-training on over
    15T tokens of data. Given the wide application of low-bit quantization for LLMs
    in resource-limited scenarios, we explore LLaMA3’s capabilities when quantized
    to low bit-width. This exploration holds the potential to unveil new insights
    and challenges for low-bit quantization of LLaMA3 and other forthcoming LLMs,
    especially in addressing performance degradation problems that suffer in LLM compression.
    Specifically, we evaluate the 10 existing post-training quantization and LoRA-finetuning
    methods of LLaMA3 on 1-8 bits and diverse datasets to comprehensively reveal LLaMA3’s
    low-bit quantization performance. Our experiment results indicate that LLaMA3
    still suffers non-negligent degradation in these scenarios, especially in ultra-low
    bit-width. This highlights the significant performance gap under low bit-width
    that needs to be bridged in future developments. We expect that this empirical
    study will prove valuable in advancing future models, pushing the LLMs to lower
    bit-width with higher accuracy for being practical. Our project is released on
    [https://github.com/Macaronlin/LLaMA3-Quantization](https://github.com/Macaronlin/LLaMA3-Quantization)
    and quantized LLaMA3 models are released in [https://huggingface.co/LLMQ](https://huggingface.co/LLMQ).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Meta的LLaMA系列已经成为最强大的开源大语言模型（LLM）系列之一。值得注意的是，LLaMA3模型最近已发布，并通过在超过15T的令牌数据上进行超大规模的预训练，实现了在各种任务中的出色性能。鉴于低比特量化在资源受限场景中对LLM的广泛应用，我们探索了LLaMA3在低比特宽度下的能力。这一探索有可能揭示出LLaMA3及其他未来LLM在低比特量化中的新见解和挑战，特别是在解决LLM压缩中遇到的性能退化问题。具体而言，我们评估了LLaMA3在1-8比特和不同数据集上的10种现有后训练量化和LoRA微调方法，以全面揭示LLaMA3在低比特量化下的性能。我们的实验结果表明，在这些场景中，LLaMA3仍然在超低比特宽度下表现出不可忽视的性能退化。这突显了低比特宽度下存在的显著性能差距，需要在未来的开发中弥合。我们期望这项实证研究将对推进未来模型、将LLM推向更低比特宽度和更高准确性具有价值。我们的项目已发布在[https://github.com/Macaronlin/LLaMA3-Quantization](https://github.com/Macaronlin/LLaMA3-Quantization)，量化后的LLaMA3模型已发布在[https://huggingface.co/LLMQ](https://huggingface.co/LLMQ)。
- en: 1 Introduction
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Launched by Meta in February 2023, the LLaMA [[18](#bib.bib18)] series¹¹1 [https://llama.meta.com](https://llama.meta.com)
    represents a breakthrough in autoregressive large language models (LLMs) using
    the Transformer [[19](#bib.bib19)] architecture. Right from its first version,
    with 13 billion parameters, it managed to outperform the much larger, closed-source
    GPT-3 model which boasts 175 billion parameters. On April 18, 2024, Meta introduced
    the LLaMA3 model, offering configurations of 8 billion and 70 billion parameters.
    Thanks to extensive pre-training on more than 15 trillion data tokens, the LLaMA3
    models²²2 [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)
    have achieved state-of-the-art (SOTA) performance across a broad range of tasks,
    establishing the LLaMA family as among the finest open-source LLMs available for
    a wide variety of applications and deployment scenarios.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由Meta于2023年2月推出的LLaMA [[18](#bib.bib18)]系列¹¹1 [https://llama.meta.com](https://llama.meta.com)代表了在使用Transformer [[19](#bib.bib19)]架构的自回归大型语言模型（LLMs）领域的突破。从其首个版本起，具有130亿参数的LLaMA就成功超越了参数高达1750亿的更大闭源GPT-3模型。2024年4月18日，Meta推出了LLaMA3模型，提供了80亿和700亿参数的配置。由于在超过15万亿数据标记上进行了广泛的预训练，LLaMA3模型²²2 [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)在广泛的任务中实现了最先进（SOTA）的性能，确立了LLaMA家族作为适用于各种应用和部署场景的最佳开源LLMs之一。
- en: Despite their impressive performance, deploying LLaMA3 models still poses significant
    challenges due to resource limitations in many scenarios. Fortunately, low-bit
    quantization has emerged as one of the most popular techniques for compressing
    LLMs. This technique reduces the memory and computational requirements of LLMs
    during inference, enabling them to run on resource-limited devices. Addressing
    the performance drop that occurs after compression is a major concern for current
    LLM quantization approaches. While numerous low-bit quantization methods have
    been proposed, their evaluations have primarily focused on the earlier and less
    capable LLaMA models (LLaMA1 and LLaMA2). Thus, LLaMA3 presents a new opportunity
    for the LLM community to assess the performance of quantization on cutting-edge
    LLMs and to understand the strengths and limitations of existing methods. In this
    empirical study, our aim is to analyze the capability of LLaMA3 to handle the
    challenges associated with degradation due to quantization.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLaMA3模型表现出色，但由于许多场景中的资源限制，部署这些模型仍面临重大挑战。幸运的是，低位量化已成为压缩LLMs的最受欢迎技术之一。这种技术在推理过程中减少了LLMs的内存和计算需求，使其能够在资源有限的设备上运行。解决压缩后出现的性能下降是当前LLM量化方法的一大关注点。虽然已提出了许多低位量化方法，但其评估主要集中在早期且能力较弱的LLaMA模型（LLaMA1和LLaMA2）上。因此，LLaMA3为LLM社区提供了一个评估量化对前沿LLMs性能影响的新机会，并了解现有方法的优缺点。在这项实证研究中，我们的目标是分析LLaMA3应对量化引起的降级挑战的能力。
- en: 'Our study sets out two primary technology tracks for quantizing LLMs: Post-Training
    Quantization (PTQ) and LoRA-FineTuning (LoRA-FT) quantization, with the aim of
    providing a comprehensive evaluation of the LLaMA3 models’ quantization. We explore
    a range of cutting-edge quantization methods across technical tracks (RTN, GPTQ [[6](#bib.bib6)],
    AWQ [[10](#bib.bib10)], SmoothQuant [[20](#bib.bib20)], PB-LLM [[16](#bib.bib16)],
    QuIP [[2](#bib.bib2)], DB-LLM [[3](#bib.bib3)], and BiLLM [[9](#bib.bib9)] for
    PTQ; QLoRA [[5](#bib.bib5)] and IR-QLoRA [[13](#bib.bib13)] for LoRA-FT), covering
    a wide spectrum from 1 to 8 bits and utilizing a diverse array of evaluation datasets,
    including WikiText2, C4, PTB, CommonSenseQA datasets (PIQA, ARC-e, ARC-c, HellaSwag,
    Winogrande), and MMLU benchmark. The overview of our study is presented as Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ How Good Are Low-bit Quantized LLaMA3 Models? An
    Empirical Study"). These evaluations assess the capabilities and limits of the
    LLaMA3 model under current LLM quantization techniques and serve as a source of
    inspiration for the design of future LLM quantization methods. The choice to focus
    specifically on the LLaMA3 model is motivated by its superior performance among
    all current open-source instruction-tuned LLMs across a variety of datasets^([2](#footnote2
    "footnote 2 ‣ 1 Introduction ‣ How Good Are Low-bit Quantized LLaMA3 Models? An
    Empirical Study")), including 5-shot MMLU, 0-shot GPQA, 0-shot HumanEval, 8-shot
    CoT GSM-8K, and 4-shot CoT MATH. Furthermore, we have made our project and the
    quantized models available to the public on [https://github.com/Macaronlin/LLaMA3-Quantization](https://github.com/Macaronlin/LLaMA3-Quantization)
    and [https://huggingface.co/LLMQ](https://huggingface.co/LLMQ), respectively.
    This not only aids in advancing the research within the LLM quantization community
    but also facilitates a broader understanding and application of effective quantization
    techniques.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究设定了两个主要的技术路径用于量化LLM：后训练量化（PTQ）和LoRA微调（LoRA-FT）量化，旨在对LLaMA3模型的量化进行全面评估。我们探讨了一系列前沿的量化方法涵盖技术路径（RTN、GPTQ [[6](#bib.bib6)]、AWQ [[10](#bib.bib10)]、SmoothQuant [[20](#bib.bib20)]、PB-LLM [[16](#bib.bib16)]、QuIP [[2](#bib.bib2)]、DB-LLM [[3](#bib.bib3)]
    和 BiLLM [[9](#bib.bib9)] 用于PTQ；QLoRA [[5](#bib.bib5)] 和 IR-QLoRA [[13](#bib.bib13)]
    用于LoRA-FT），涵盖了从1位到8位的广泛范围，并利用了多样的评估数据集，包括WikiText2、C4、PTB、常识QA数据集（PIQA、ARC-e、ARC-c、HellaSwag、Winogrande）和MMLU基准。我们的研究概述见图[1](#S1.F1
    "图1 ‣ 1 引言 ‣ 低位量化LLaMA3模型效果如何？一项实证研究")。这些评估评估了LLaMA3模型在当前LLM量化技术下的能力和局限性，并为未来LLM量化方法的设计提供了灵感。特别关注LLaMA3模型的选择是由于它在各种数据集中的表现优于所有当前开源的指令调优LLM^([2](#footnote2
    "脚注 2 ‣ 1 引言 ‣ 低位量化LLaMA3模型效果如何？一项实证研究"))，包括5-shot MMLU、0-shot GPQA、0-shot HumanEval、8-shot
    CoT GSM-8K 和4-shot CoT MATH。此外，我们已经将我们的项目和量化模型公开在 [https://github.com/Macaronlin/LLaMA3-Quantization](https://github.com/Macaronlin/LLaMA3-Quantization)
    和 [https://huggingface.co/LLMQ](https://huggingface.co/LLMQ) 上。这不仅有助于推动LLM量化社区的研究，还促进了对有效量化技术的更广泛理解和应用。
- en: '![Refer to caption](img/4d5915618156073a17d8a3d360282387.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4d5915618156073a17d8a3d360282387.png)'
- en: 'Figure 1: The overview of our empirical study'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们实证研究的概述
- en: 2 Empirical Evaluation
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 实证评估
- en: 2.1 Experiment Settings
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 实验设置
- en: Evaluated LLMs. We obtain the pre-trained LLaMA3-8B and -70B through the official
    repository^([2](#footnote2 "footnote 2 ‣ 1 Introduction ‣ How Good Are Low-bit
    Quantized LLaMA3 Models? An Empirical Study")).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 评估的LLM。我们通过官方库获取了预训练的LLaMA3-8B和-70B^([2](#footnote2 "脚注 2 ‣ 1 引言 ‣ 低位量化LLaMA3模型效果如何？一项实证研究"))。
- en: Quantization methods. To evaluate the performance of low-bit quantized LLaMA3,
    we select representative LLM quantization methods with extensive influence and
    functionality, including 8 PTQ methods and 2 LoRA-FT methods. The implementations
    of our evaluated quantization methods follow their open-source repositories³³3 [https://github.com/IST-DASLab/gptq](https://github.com/IST-DASLab/gptq),
    [https://github.com/mit-han-lab/llm-awq](https://github.com/mit-han-lab/llm-awq),
    [https://github.com/mit-han-lab/smoothquant](https://github.com/mit-han-lab/smoothquant),
    [https://github.com/Cornell-RelaxML/QuIP](https://github.com/Cornell-RelaxML/QuIP),
    [https://github.com/hahnyuan/PB-LLM](https://github.com/hahnyuan/PB-LLM), [https://github.com/Aaronhuang-778/BiLLM](https://github.com/Aaronhuang-778/BiLLM),
    [https://github.com/artidoro/qlora](https://github.com/artidoro/qlora), [https://github.com/htqin/IR-QLoRA](https://github.com/htqin/IR-QLoRA).
    We also used eight NVIDIA A800 with 80GB GPU memory for quantitative evaluation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 量化方法。为了评估低位量化 LLaMA3 的性能，我们选择了具有广泛影响力和功能性的代表性 LLM 量化方法，包括 8 种 PTQ 方法和 2 种 LoRA-FT
    方法。我们评估的量化方法的实现遵循其开源库³³3 [https://github.com/IST-DASLab/gptq](https://github.com/IST-DASLab/gptq)、[https://github.com/mit-han-lab/llm-awq](https://github.com/mit-han-lab/llm-awq)、[https://github.com/mit-han-lab/smoothquant](https://github.com/mit-han-lab/smoothquant)、[https://github.com/Cornell-RelaxML/QuIP](https://github.com/Cornell-RelaxML/QuIP)、[https://github.com/hahnyuan/PB-LLM](https://github.com/hahnyuan/PB-LLM)、[https://github.com/Aaronhuang-778/BiLLM](https://github.com/Aaronhuang-778/BiLLM)、[https://github.com/artidoro/qlora](https://github.com/artidoro/qlora)、[https://github.com/htqin/IR-QLoRA](https://github.com/htqin/IR-QLoRA)。我们还使用了
    8 台配备 80GB GPU 内存的 NVIDIA A800 进行定量评估。
- en: 'Table 1: Evaluation results of post-training quantization on LLaMA3-8B model'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：LLaMA3-8B 模型后训练量化的评估结果
- en: '| Method | #W | #A | #G | PPL$\downarrow$ |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | #W | #A | #G | PPL$\downarrow$ |'
- en: '| WikiText2 | C4 | PTB | PIQA | ARC-e | ARC-c | HellaSwag | Wino | Avg. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 | C4 | PTB | PIQA | ARC-e | ARC-c | HellaSwag | Wino | 平均 |'
- en: '| LLaMA3 | 16 | 16 | - | 6.1 | 9.2 | 10.6 | 79.9 | 80.1 | 50.4 | 60.2 | 72.8
    | 68.6 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3 | 16 | 16 | - | 6.1 | 9.2 | 10.6 | 79.9 | 80.1 | 50.4 | 60.2 | 72.8
    | 68.6 |'
- en: '| RTN | 4 | 16 | 128 | 8.5 | 13.4 | 14.5 | 76.6 | 70.1 | 45.0 | 56.8 | 71.0
    | 63.9 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 16 | 128 | 8.5 | 13.4 | 14.5 | 76.6 | 70.1 | 45.0 | 56.8 | 71.0
    | 63.9 |'
- en: '| 3 | 16 | 128 | 27.9 | 1.1e2 | 95.6 | 62.3 | 32.1 | 22.5 | 29.1 | 54.7 | 40.2
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 16 | 128 | 27.9 | 1.1e2 | 95.6 | 62.3 | 32.1 | 22.5 | 29.1 | 54.7 | 40.2
    |'
- en: '| 2 | 16 | 128 | 1.9E3 | 2.5E4 | 1.8E4 | 53.1 | 24.8 | 22.1 | 26.9 | 53.1 |
    36.0 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 16 | 128 | 1.9E3 | 2.5E4 | 1.8E4 | 53.1 | 24.8 | 22.1 | 26.9 | 53.1 |
    36.0 |'
- en: '| 8 | 16 | - | 6.2 | 9.5 | 11.2 | 79.7 | 80.8 | 50.4 | 60.1 | 73.4 | 68.9 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 16 | - | 6.2 | 9.5 | 11.2 | 79.7 | 80.8 | 50.4 | 60.1 | 73.4 | 68.9 |'
- en: '| 4 | 16 | - | 8.7 | 14.0 | 14.9 | 75.0 | 68.2 | 39.4 | 56.0 | 69.0 | 61.5
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 16 | - | 8.7 | 14.0 | 14.9 | 75.0 | 68.2 | 39.4 | 56.0 | 69.0 | 61.5
    |'
- en: '| 3 | 16 | - | 2.2E3 | 5.6E2 | 2.0E3 | 56.2 | 31.1 | 20.0 | 27.5 | 53.1 | 35.6
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 16 | - | 2.2E3 | 5.6E2 | 2.0E3 | 56.2 | 31.1 | 20.0 | 27.5 | 53.1 | 35.6
    |'
- en: '| 2 | 16 | - | 2.7E6 | 7.4E6 | 3.1E6 | 53.1 | 24.7 | 21.9 | 25.6 | 51.1 | 35.3
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 16 | - | 2.7E6 | 7.4E6 | 3.1E6 | 53.1 | 24.7 | 21.9 | 25.6 | 51.1 | 35.3
    |'
- en: '| GPTQ | 4 | 16 | 128 | 6.5 | 10.4 | 11.0 | 78.4 | 78.8 | 47.7 | 59.0 | 72.6
    | 67.3 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 16 | 128 | 6.5 | 10.4 | 11.0 | 78.4 | 78.8 | 47.7 | 59.0 | 72.6
    | 67.3 |'
- en: '| 3 | 16 | 128 | 8.2 | 13.7 | 15.2 | 74.9 | 70.5 | 37.7 | 54.3 | 71.1 | 61.7
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 16 | 128 | 8.2 | 13.7 | 15.2 | 74.9 | 70.5 | 37.7 | 54.3 | 71.1 | 61.7
    |'
- en: '| 2 | 16 | 128 | 2.1E2 | 4.1E4 | 9.1E2 | 53.9 | 28.8 | 19.9 | 27.7 | 50.5 |
    36.2 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 16 | 128 | 2.1E2 | 4.1E4 | 9.1E2 | 53.9 | 28.8 | 19.9 | 27.7 | 50.5 |
    36.2 |'
- en: '| 8 | 16 | - | 6.1 | 9.4 | 10.6 | 79.8 | 80.1 | 50.2 | 60.2 | 72.8 | 68.6 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 16 | - | 6.1 | 9.4 | 10.6 | 79.8 | 80.1 | 50.2 | 60.2 | 72.8 | 68.6 |'
- en: '| 4 | 16 | - | 7.0 | 11.8 | 14.4 | 76.8 | 74.3 | 42.4 | 57.4 | 72.8 | 64.8
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 16 | - | 7.0 | 11.8 | 14.4 | 76.8 | 74.3 | 42.4 | 57.4 | 72.8 | 64.8
    |'
- en: '| 3 | 16 | - | 13.0 | 45.9 | 37.0 | 60.8 | 38.8 | 22.3 | 41.8 | 60.9 | 44.9
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 16 | - | 13.0 | 45.9 | 37.0 | 60.8 | 38.8 | 22.3 | 41.8 | 60.9 | 44.9
    |'
- en: '| 2 | 16 | - | 5.7E4 | 1.0E5 | 2.7E5 | 52.8 | 25.0 | 20.5 | 26.6 | 49.6 | 34.9
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 16 | - | 5.7E4 | 1.0E5 | 2.7E5 | 52.8 | 25.0 | 20.5 | 26.6 | 49.6 | 34.9
    |'
- en: '| AWQ | 4 | 16 | 128 | 6.6 | 9.4 | 11.1 | 79.1 | 79.7 | 49.3 | 59.1 | 74.0
    | 68.2 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4 | 16 | 128 | 6.6 | 9.4 | 11.1 | 79.1 | 79.7 | 49.3 | 59.1 | 74.0
    | 68.2 |'
- en: '| 3 | 16 | 128 | 8.2 | 11.6 | 13.2 | 77.7 | 74.0 | 43.2 | 55.1 | 72.1 | 64.4
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 16 | 128 | 8.2 | 11.6 | 13.2 | 77.7 | 74.0 | 43.2 | 55.1 | 72.1 | 64.4
    |'
- en: '| 2 | 16 | 128 | 1.7E6 | 2.1E6 | 1.8E6 | 52.4 | 24.2 | 21.5 | 25.6 | 50.7 |
    34.9 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 16 | 128 | 1.7E6 | 2.1E6 | 1.8E6 | 52.4 | 24.2 | 21.5 | 25.6 | 50.7 |
    34.9 |'
- en: '| 8 | 16 | - | 6.1 | 8.9 | 10.6 | 79.6 | 80.3 | 50.5 | 60.2 | 72.8 | 68.7 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 16 | - | 6.1 | 8.9 | 10.6 | 79.6 | 80.3 | 50.5 | 60.2 | 72.8 | 68.7 |'
- en: '| 4 | 16 | - | 7.1 | 10.1 | 11.8 | 78.3 | 77.6 | 48.3 | 58.6 | 72.5 | 67.0
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 16 | - | 7.1 | 10.1 | 11.8 | 78.3 | 77.6 | 48.3 | 58.6 | 72.5 | 67.0
    |'
- en: '| 3 | 16 | - | 12.8 | 16.8 | 24.0 | 71.9 | 66.7 | 35.1 | 50.7 | 64.7 | 57.8
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 16 | - | 12.8 | 16.8 | 24.0 | 71.9 | 66.7 | 35.1 | 50.7 | 64.7 | 57.8
    |'
- en: '| 2 | 16 | - | 8.2E5 | 8.1E5 | 9.0E5 | 55.2 | 25.2 | 21.3 | 25.4 | 50.4 | 35.5
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 16 | - | 8.2E5 | 8.1E5 | 9.0E5 | 55.2 | 25.2 | 21.3 | 25.4 | 50.4 | 35.5
    |'
- en: '| QuIP | 4 | 16 | - | 6.5 | 11.1 | 9.5 | 78.2 | 78.2 | 47.4 | 58.6 | 73.2 |
    67.1 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 4 | 16 | - | 6.5 | 11.1 | 9.5 | 78.2 | 78.2 | 47.4 | 58.6 | 73.2 |
    67.1 |'
- en: '| 3 | 16 | - | 7.5 | 11.3 | 12.6 | 76.8 | 72.9 | 41.0 | 55.4 | 72.5 | 63.7
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 16 | - | 7.5 | 11.3 | 12.6 | 76.8 | 72.9 | 41.0 | 55.4 | 72.5 | 63.7
    |'
- en: '| 2 | 16 | - | 85.1 | 1.3E2 | 1.8E2 | 52.9 | 29.0 | 21.3 | 29.2 | 51.7 | 36.8
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 16 | - | 85.1 | 1.3E2 | 1.8E2 | 52.9 | 29.0 | 21.3 | 29.2 | 51.7 | 36.8
    |'
- en: '| DB-LLM | 2 | 16 | 128 | 13.6 | 19.2 | 23.8 | 68.9 | 59.1 | 28.2 | 42.1 |
    60.4 | 51.8 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| DB-LLM | 2 | 16 | 128 | 13.6 | 19.2 | 23.8 | 68.9 | 59.1 | 28.2 | 42.1 |
    60.4 | 51.8 |'
- en: '| PB-LLM | 2 | 16 | 128 | 24.7 | 79.2 | 65.6 | 57.0 | 37.8 | 17.2 | 29.8 |
    52.5 | 38.8 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM | 2 | 16 | 128 | 24.7 | 79.2 | 65.6 | 57.0 | 37.8 | 17.2 | 29.8 |
    52.5 | 38.8 |'
- en: '| 1.7 | 16 | 128 | 41.8 | 2.6E2 | 1.2E2 | 52.5 | 31.7 | 17.5 | 27.7 | 50.4
    | 36.0 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 1.7 | 16 | 128 | 41.8 | 2.6E2 | 1.2E2 | 52.5 | 31.7 | 17.5 | 27.7 | 50.4
    | 36.0 |'
- en: '| BiLLM | 1.1 | 16 | 128 | 28.3 | 2.9E2 | 94.7 | 56.1 | 36.0 | 17.7 | 28.9
    | 51.0 | 37.9 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM | 1.1 | 16 | 128 | 28.3 | 2.9E2 | 94.7 | 56.1 | 36.0 | 17.7 | 28.9
    | 51.0 | 37.9 |'
- en: '| SmoothQuant | 8 | 8 | - | 6.3 | 9.2 | 10.8 | 79.5 | 79.7 | 49.0 | 60.0 |
    73.2 | 68.3 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 8 | 8 | - | 6.3 | 9.2 | 10.8 | 79.5 | 79.7 | 49.0 | 60.0 |
    73.2 | 68.3 |'
- en: '| 6 | 6 | - | 7.7 | 11.8 | 12.5 | 76.8 | 75.5 | 45.0 | 56.9 | 69.0 | 64.6 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 6 | - | 7.7 | 11.8 | 12.5 | 76.8 | 75.5 | 45.0 | 56.9 | 69.0 | 64.6 |'
- en: '| 4 | 4 | - | 4.3E3 | 4.0E3 | 3.6E3 | 54.6 | 26.3 | 20.0 | 26.4 | 50.3 | 35.5
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 4 | - | 4.3E3 | 4.0E3 | 3.6E3 | 54.6 | 26.3 | 20.0 | 26.4 | 50.3 | 35.5
    |'
- en: Evaluation datasets. For the PTQ methods, we evaluate quantized LLaMA3 on the
    WikiText2 [[12](#bib.bib12)], PTB [[11](#bib.bib11)], and a portion of the C4
    dataset [[14](#bib.bib14)], using Perplexity (PPL) as the evaluation metric. Subsequently,
    we further conduct experiments on five zero-shot evaluation tasks (PIQA [[1](#bib.bib1)],
    Winogrande [[15](#bib.bib15)], ARC-e  [[4](#bib.bib4)], ARC-c [[4](#bib.bib4)],
    and Hellaswag [[22](#bib.bib22)]) to fully validate the quantized performance
    of LLaMA3. For the LoRA-FT methods, we conduct the evaluation on the 5-shot MMLU
    benchmark [[7](#bib.bib7)] while also validating the aforementioned 5 zero-shot
    datasets for the LoRA-FT methods.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 评估数据集。对于 PTQ 方法，我们使用 Perplexity (PPL) 作为评估指标，评估了量化的 LLaMA3 在 WikiText2 [[12](#bib.bib12)]、PTB [[11](#bib.bib11)]
    和部分 C4 数据集 [[14](#bib.bib14)] 上的表现。随后，我们还在五个零样本评估任务（PIQA [[1](#bib.bib1)]、Winogrande [[15](#bib.bib15)]、ARC-e
     [[4](#bib.bib4)]、ARC-c [[4](#bib.bib4)] 和 Hellaswag [[22](#bib.bib22)]）上进一步实验，以充分验证量化的
    LLaMA3 性能。对于 LoRA-FT 方法，我们在 5-shot MMLU 基准 [[7](#bib.bib7)] 上进行评估，同时也验证了上述 5 个零样本数据集。
- en: For the fairness of our evaluation, we uniformly use WikiText2 as the calibration
    dataset for all quantization methods, with a sample size of 128 and a consistent
    token sequence length of 2048\. Furthermore, for quantization methods requiring
    channel-wise grouping, we adopt a block size of 128 to balance performance and
    inference efficiency, which is a common practice in existing works.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公平评估，我们统一使用 WikiText2 作为所有量化方法的校准数据集，样本大小为 128，令牌序列长度为 2048。此外，对于需要通道分组的量化方法，我们采用
    128 的块大小，以平衡性能和推理效率，这在现有研究中是一种常见做法。
- en: 'Table 2: Evaluation results of post-training quantization on LLaMA3-70B model'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：LLaMA3-70B 模型后训练量化的评估结果
- en: '| Method | #W | #A | #G | PPL$\downarrow$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | #W | #A | #G | PPL$\downarrow$ |'
- en: '| WikiText2 | C4 | PTB | PIQA | ARC-e | ARC-c | HellaSwag | Wino | Avg. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 | C4 | PTB | PIQA | ARC-e | ARC-c | HellaSwag | Wino | 平均 |'
- en: '| LLaMA3 | 16 | 16 | - | 2.9 | 6.9 | 8.2 | 82.4 | 86.9 | 60.3 | 66.4 | 80.6
    | 75.3 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3 | 16 | 16 | - | 2.9 | 6.9 | 8.2 | 82.4 | 86.9 | 60.3 | 66.4 | 80.6
    | 75.3 |'
- en: '| RTN | 4 | 16 | 128 | 3.6 | 8.9 | 9.1 | 82.3 | 85.2 | 58.4 | 65.6 | 79.8 |
    74.3 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 16 | 128 | 3.6 | 8.9 | 9.1 | 82.3 | 85.2 | 58.4 | 65.6 | 79.8 |
    74.3 |'
- en: '| 3 | 16 | 128 | 11.8 | 22.0 | 26.3 | 64.2 | 48.9 | 25.1 | 41.1 | 60.5 | 48.0
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 16 | 128 | 11.8 | 22.0 | 26.3 | 64.2 | 48.9 | 25.1 | 41.1 | 60.5 | 48.0
    |'
- en: '| 2 | 16 | 128 | 4.6E5 | 4.7E5 | 3.8E5 | 53.2 | 23.9 | 22.1 | 25.8 | 53.0 |
    35.6 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 16 | 128 | 4.6E5 | 4.7E5 | 3.8E5 | 53.2 | 23.9 | 22.1 | 25.8 | 53.0 |
    35.6 |'
- en: '| GPTQ | 4 | 16 | 128 | 3.3 | 6.9 | 8.3 | 82.9 | 86.3 | 58.4 | 66.1 | 80.7
    | 74.9 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 16 | 128 | 3.3 | 6.9 | 8.3 | 82.9 | 86.3 | 58.4 | 66.1 | 80.7
    | 74.9 |'
- en: '| 3 | 16 | 128 | 5.2 | 10.5 | 9.7 | 80.6 | 79.6 | 52.1 | 63.5 | 77.1 | 70.6
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 16 | 128 | 5.2 | 10.5 | 9.7 | 80.6 | 79.6 | 52.1 | 63.5 | 77.1 | 70.6
    |'
- en: '| 2 | 16 | 128 | 11.9 | 22.8 | 31.6 | 62.7 | 38.9 | 24.6 | 41.0 | 59.9 | 45.4
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 16 | 128 | 11.9 | 22.8 | 31.6 | 62.7 | 38.9 | 24.6 | 41.0 | 59.9 | 45.4
    |'
- en: '| AWQ | 4 | 16 | 128 | 3.3 | 7.0 | 8.3 | 82.7 | 86.3 | 59.0 | 65.7 | 80.9 |
    74.9 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4 | 16 | 128 | 3.3 | 7.0 | 8.3 | 82.7 | 86.3 | 59.0 | 65.7 | 80.9 |
    74.9 |'
- en: '| 3 | 16 | 128 | 4.8 | 8.0 | 9.0 | 81.4 | 84.7 | 58.0 | 63.5 | 78.6 | 73.2
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 16 | 128 | 4.8 | 8.0 | 9.0 | 81.4 | 84.7 | 58.0 | 63.5 | 78.6 | 73.2
    |'
- en: '| 2 | 16 | 128 | 1.7E6 | 1.4E6 | 1.5E6 | 52.2 | 25.5 | 23.1 | 25.6 | 52.3 |
    35.7 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 16 | 128 | 1.7E6 | 1.4E6 | 1.5E6 | 52.2 | 25.5 | 23.1 | 25.6 | 52.3 |
    35.7 |'
- en: '| QuIP | 4 | 16 | - | 3.4 | 7.1 | 8.4 | 82.5 | 86.0 | 58.7 | 65.7 | 79.7 |
    74.5 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 4 | 16 | - | 3.4 | 7.1 | 8.4 | 82.5 | 86.0 | 58.7 | 65.7 | 79.7 |
    74.5 |'
- en: '| 3 | 16 | - | 4.7 | 8.0 | 8.9 | 82.3 | 83.3 | 54.9 | 63.9 | 78.4 | 72.5 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 16 | - | 4.7 | 8.0 | 8.9 | 82.3 | 83.3 | 54.9 | 63.9 | 78.4 | 72.5 |'
- en: '| 2 | 16 | - | 13.0 | 22.2 | 24.9 | 65.3 | 48.9 | 26.5 | 40.9 | 61.7 | 48.7
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 16 | - | 13.0 | 22.2 | 24.9 | 65.3 | 48.9 | 26.5 | 40.9 | 61.7 | 48.7
    |'
- en: '| PB-LLM | 2 | 16 | 128 | 11.6 | 34.5 | 27.2 | 65.2 | 40.6 | 25.1 | 42.7 |
    56.4 | 46.0 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM | 2 | 16 | 128 | 11.6 | 34.5 | 27.2 | 65.2 | 40.6 | 25.1 | 42.7 |
    56.4 | 46.0 |'
- en: '| 1.7 | 16 | 128 | 18.6 | 65.2 | 55.9 | 56.5 | 49.9 | 25.8 | 34.9 | 53.1 |
    44.1 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 1.7 | 16 | 128 | 18.6 | 65.2 | 55.9 | 56.5 | 49.9 | 25.8 | 34.9 | 53.1 |
    44.1 |'
- en: '| BiLLM | 1.1 | 16 | 128 | 17.1 | 77.7 | 54.2 | 58.2 | 46.4 | 25.1 | 37.5 |
    53.6 | 44.2 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM | 1.1 | 16 | 128 | 17.1 | 77.7 | 54.2 | 58.2 | 46.4 | 25.1 | 37.5 |
    53.6 | 44.2 |'
- en: '| SmoothQuant | 8 | 8 | - | 2.9 | 6.9 | 8.2 | 82.2 | 86.9 | 60.2 | 66.3 | 80.7
    | 75.3 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 8 | 8 | - | 2.9 | 6.9 | 8.2 | 82.2 | 86.9 | 60.2 | 66.3 | 80.7
    | 75.3 |'
- en: '| 6 | 6 | - | 2.9 | 6.9 | 8.2 | 82.4 | 87.0 | 59.9 | 66.1 | 80.6 | 75.2 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 6 | - | 2.9 | 6.9 | 8.2 | 82.4 | 87.0 | 59.9 | 66.1 | 80.6 | 75.2 |'
- en: '| 4 | 4 | - | 9.6 | 16.9 | 17.7 | 76.9 | 75.8 | 43.5 | 52.9 | 58.9 | 61.6 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 4 | - | 9.6 | 16.9 | 17.7 | 76.9 | 75.8 | 43.5 | 52.9 | 58.9 | 61.6 |'
- en: '2.2 Track1: Post-Training Quantization'
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '2.2 Track1: 后训练量化'
- en: As shown in Table [1](#S2.T1 "Table 1 ‣ 2.1 Experiment Settings ‣ 2 Empirical
    Evaluation ‣ How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study")
    and Table [2](#S2.T2 "Table 2 ‣ 2.1 Experiment Settings ‣ 2 Empirical Evaluation
    ‣ How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study"), we provide
    the performance of low-bit LLaMA3-8B and LLaMA3-70B with 8 different PTQ methods,
    respectively, covering a wide bit-width spectrum from 1 to 8-bit.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如表格[1](#S2.T1 "Table 1 ‣ 2.1 Experiment Settings ‣ 2 Empirical Evaluation ‣
    How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study")和表格[2](#S2.T2
    "Table 2 ‣ 2.1 Experiment Settings ‣ 2 Empirical Evaluation ‣ How Good Are Low-bit
    Quantized LLaMA3 Models? An Empirical Study")所示，我们提供了低比特LLaMA3-8B和LLaMA3-70B在8种不同PTQ方法下的性能数据，覆盖了从1到8位的广泛比特宽度范围。
- en: Among them, Round-To-Nearest (RTN) is a vanilla rounding quantization method.
    GPTQ [[6](#bib.bib6)] is currently one of the most efficient and effective weight-only
    quantization methods, which utilizes error compensation in quantization. But under
    2-3 bits, GPTQ causes severe accuracy collapse when quantized LLaMA3. AWQ [[10](#bib.bib10)]
    adopts an anomaly channel suppression approach to reduce the difficulty of weight
    quantization, and QuIP [[2](#bib.bib2)] ensures the incoherence between weights
    and Hessian by optimizing matrix computation. Both of them can keep LLaMA3’s capability
    at 3-bit and even push the 2-bit quantization to promising.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，Round-To-Nearest (RTN)是一种基础的四舍五入量化方法。GPTQ [[6](#bib.bib6)]目前是最有效且高效的仅权重量化方法之一，利用量化中的误差补偿。然而，在2-3位下，GPTQ在量化LLaMA3时会导致严重的准确性崩溃。AWQ [[10](#bib.bib10)]采用异常通道抑制方法来减少权重量化的难度，而QuIP [[2](#bib.bib2)]通过优化矩阵计算来确保权重与Hessian之间的不一致性。它们都能保持LLaMA3在3位下的能力，甚至将2位量化推向有希望的方向。
- en: The recent emergence of binarized LLM quantization methods has realized ultra-low
    bit-width LLM weight compression. PB-LLM [[16](#bib.bib16)] employs a mixed-precision
    quantization strategy, retaining a small portion of significant weight full-precision
    while quantizing the majority of weights to 1-bit. DB-LLM [[3](#bib.bib3)] achieves
    efficient LLM compression through double binarization weight splitting and proposes
    a deviation-aware distillation strategy to further enhance 2-bit LLM performance.
    BiLLM [[9](#bib.bib9)] further pushes the LLM quantization boundary to as low
    as 1.1-bit through residual approximation of salient weights and grouped quantization
    of non-salient weights. These LLM quantization methods specially designed for
    ultra-low bit-width can achieve higher accuracy of quantized LLaMA3-8B at $\leqslant$
    2-bit, far outperforms methods like GPTQ, AWQ, and QuIP under 2-bit (even 3-bit
    some cases).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最近出现的二值化LLM量化方法实现了超低比特宽度的LLM权重压缩。PB-LLM [[16](#bib.bib16)]采用混合精度量化策略，保留少量重要权重的全精度，同时将大部分权重量化为1位。DB-LLM [[3](#bib.bib3)]通过双重二值化权重分裂实现了高效的LLM压缩，并提出了偏差感知蒸馏策略，以进一步提升2位LLM的性能。BiLLM [[9](#bib.bib9)]通过显著权重的残差逼近和非显著权重的分组量化，将LLM量化的边界进一步推至低至1.1位。这些专门为超低比特宽度设计的LLM量化方法可以在$\leqslant$
    2位时实现更高的量化LLaMA3-8B准确性，远超GPTQ、AWQ和QuIP等方法在2位（甚至在某些情况下为3位）下的表现。
- en: We also perform LLaMA3 evaluation on quantized activations via SmoothQuant [[20](#bib.bib20)],
    which moves the quantization difficulty offline from activations to weights to
    smooth out activation outliers. Our evaluation shows that SmoothQuant can retain
    the accuracy of LLaMA3 with 8- and 6-bit weights and activations, but faces collapse
    at 4-bit.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过SmoothQuant对量化激活进行LLaMA3评估[[20](#bib.bib20)]，该方法将量化难度从激活移至权重，以平滑激活的异常值。我们的评估显示，SmoothQuant可以在8位和6位权重及激活下保持LLaMA3的准确性，但在4位下面临崩溃。
- en: Moreover, we find that the LLaMA3-70B model shows significant robustness for
    various quantization methods, even in ultra-low bit-width.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们发现LLaMA3-70B模型在各种量化方法下显示出显著的鲁棒性，即使是在超低位宽下也是如此。
- en: '2.3 Track2: LoRA-FineTuning Quantization'
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '2.3 Track2: LoRA-微调量化'
- en: 'Except for the PTQ methods, we also provide the performance of 4-bit LLaMA3-8B
    with 2 different LoRA-FT quantization methods as shown in Table [3](#S2.T3 "Table
    3 ‣ 2.3 Track2: LoRA-FineTuning Quantization ‣ 2 Empirical Evaluation ‣ How Good
    Are Low-bit Quantized LLaMA3 Models? An Empirical Study"), including QLoRA [[5](#bib.bib5)]
    and IR-QLoRA [[13](#bib.bib13)].'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '除了PTQ方法，我们还提供了4位LLaMA3-8B与2种不同LoRA-FT量化方法的性能，如表[3](#S2.T3 "Table 3 ‣ 2.3 Track2:
    LoRA-FineTuning Quantization ‣ 2 Empirical Evaluation ‣ How Good Are Low-bit Quantized
    LLaMA3 Models? An Empirical Study")所示，包括QLoRA[[5](#bib.bib5)]和IR-QLoRA[[13](#bib.bib13)]。'
- en: 'On the MMLU dataset, the most notable observation with LLaMA3-8B under LoRA-FT
    quantization is that low-rank finetuning on the Alpaca [[17](#bib.bib17)] dataset
    not only cannot compensate for the errors introduced by quantization, even making
    the degradation more severe. Specifically, various LoRA-FT quantization methods
    obtain worse performance quantized LLaMA3 under 4-bit compared with their 4-bit
    counterparts without LoRA-FT. This is in stark contrast to similar phenomena on
    LLaMA1 and LLaMA2, where, for the front one, the 4-bit low-rank finetuned quantized
    versions could even easily surpass the original FP16 counterpart on MMLU. According
    to our intuitive analysis, the main reason for this phenomenon is due to LLaMA3’s
    strong performance brought by its massive pre-scale training, which means the
    performance loss from the original model’s quantization cannot be compensated
    for by finetuning on a tiny set of data with low-rank parameters (which can be
    seen as a subset of the original model [[8](#bib.bib8), [5](#bib.bib5)]). Despite
    the significant drop from quantization that cannot be compensated by finetuning,
    4-bit LoRA-FT quantized LLaMA3-8B significantly outperforms LLaMA1-7B and LLaMA2-7B
    under various quantization methods. For instance, with the QLoRA method, 4-bit
    LLaMA3-8B has an average accuracy of 57.0 (FP16: 64.8), exceeding 4-bit LLaMA1-7B’s
    38.4 (FP16: 34.6) by 18.6, and surpassing 4-bit LLaMA2-7B’s 43.9 (FP16: 45.5)
    by 13.1 [[21](#bib.bib21), [13](#bib.bib13)]. This implies that a new LoRA-FT
    quantization paradigm is needed in the era of LLaMA3.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '在MMLU数据集上，LLaMA3-8B在LoRA-FT量化下最显著的观察结果是，Alpaca[[17](#bib.bib17)]数据集上的低秩微调不仅无法弥补量化引入的误差，甚至使降级更加严重。具体而言，各种LoRA-FT量化方法在4位下的量化LLaMA3性能不如没有LoRA-FT的4位版本。这与LLaMA1和LLaMA2的类似现象形成鲜明对比，对于前者，4位低秩微调量化版本甚至可以轻松超越原始FP16版本在MMLU上的表现。根据我们的直观分析，这一现象的主要原因在于LLaMA3通过大规模预训练所带来的强大性能，这意味着来自原始模型量化的性能损失无法通过在低秩参数的小数据集上进行微调来弥补（这可以看作是原始模型[[8](#bib.bib8),
    [5](#bib.bib5)]的一个子集）。尽管量化带来的显著下降无法通过微调弥补，但4位LoRA-FT量化的LLaMA3-8B在各种量化方法下明显优于LLaMA1-7B和LLaMA2-7B。例如，使用QLoRA方法，4位LLaMA3-8B的平均准确率为57.0（FP16:
    64.8），超越4位LLaMA1-7B的38.4（FP16: 34.6）18.6，以及超越4位LLaMA2-7B的43.9（FP16: 45.5）13.1[[21](#bib.bib21),
    [13](#bib.bib13)]。这意味着在LLaMA3时代需要一种新的LoRA-FT量化范式。'
- en: A similar phenomenon occurs with the CommonSenseQA benchmark. Compared to the
    4-bit counterparts without LoRA-FT, the performance of the models fine-tuned using
    QLoRA and IR-QLoRA also declined (e.g. QLoRA 2.8% vs IR-QLoRA 2.4% on average).
    This further demonstrates the strength of using high-quality datasets in LLaMA3,
    as the general dataset Alpaca does not contribute to the model’s performance in
    other tasks.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在CommonSenseQA基准测试中也出现了类似现象。与没有LoRA-FT的4位版本相比，使用QLoRA和IR-QLoRA微调的模型性能也有所下降（例如，QLoRA
    2.8% vs IR-QLoRA 2.4%）。这进一步证明了在LLaMA3中使用高质量数据集的优势，因为通用数据集Alpaca对模型在其他任务中的表现没有贡献。
- en: 'Table 3: LoRA-FT on LLaMA3-8B with Alpaca dataset'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '表3: 使用Alpaca数据集的LLaMA3-8B上的LoRA-FT'
- en: '| Method | #W | MMLU$\uparrow$ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | #W | MMLU$\uparrow$ |'
- en: '| Hums. | STEM | Social | Other | Avg. | PIQA | ARC-e | ARC-c | HellaSwag |
    Wino | Avg. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 人文 | STEM | 社会 | 其他 | 平均 | PIQA | ARC-e | ARC-c | HellaSwag | Wino | 平均 |'
- en: '| LLaMA3 | 16 | 59.0 | 55.3 | 76.0 | 71.5 | 64.8 | 79.9 | 80.1 | 50.4 | 60.2
    | 72.8 | 68.6 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3 | 16 | 59.0 | 55.3 | 76.0 | 71.5 | 64.8 | 79.9 | 80.1 | 50.4 | 60.2
    | 72.8 | 68.6 |'
- en: '| NormalFloat | 4 | 56.8 | 52.9 | 73.6 | 69.4 | 62.5 | 78.6 | 78.5 | 46.2 |
    58.8 | 74.3 | 67.3 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| NormalFloat | 4 | 56.8 | 52.9 | 73.6 | 69.4 | 62.5 | 78.6 | 78.5 | 46.2 |
    58.8 | 74.3 | 67.3 |'
- en: '| QLoRA | 4 | 50.3 | 49.3 | 65.8 | 64.2 | 56.7 | 76.6 | 74.8 | 45.0 | 59.4
    | 67.0 | 64.5 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| QLoRA | 4 | 50.3 | 49.3 | 65.8 | 64.2 | 56.7 | 76.6 | 74.8 | 45.0 | 59.4
    | 67.0 | 64.5 |'
- en: '| IR-QLoRA | 4 | 52.2 | 49.0 | 66.5 | 63.1 | 57.2 | 76.3 | 74.3 | 45.3 | 59.1
    | 69.5 | 64.9 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| IR-QLoRA | 4 | 52.2 | 49.0 | 66.5 | 63.1 | 57.2 | 76.3 | 74.3 | 45.3 | 59.1
    | 69.5 | 64.9 |'
- en: 3 Conclusion
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 结论
- en: Meta’s recently released LLaMA3 models have rapidly become the most powerful
    LLM series, capturing significant interest from researchers. Building on this
    momentum, our study aims to thoroughly evaluate the performance of LLaMA3 across
    a variety of low-bit quantization techniques, including post-training quantization
    and LoRA-finetuning quantization. Our goal is to assess the boundaries of its
    capabilities in scenarios with limited resources by leveraging existing LLM quantization
    technologies. Our findings indicate that while LLaMA3 still demonstrates superior
    performance after quantization, the performance degradation associated with quantization
    is significant and can even lead to larger declines in many cases. This discovery
    highlights the potential challenges of deploying LLaMA3 in resource-constrained
    environments and underscores the ample room for growth and improvement within
    the context of low-bit quantization. The empirical insights from our research
    are expected to be valuable for the development of future LLM quantization techniques,
    especially in terms of narrowing the performance gap with the original models.
    By addressing the performance degradation caused by low-bit quantization, we anticipate
    that subsequent quantization paradigms will enable LLMs to achieve stronger capabilities
    at a lower computational cost, ultimately driving the progress of generative artificial
    intelligence, as represented by LLMs, to new heights.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Meta最近发布的LLaMA3模型迅速成为最强大的LLM系列，吸引了大量研究人员的关注。在此基础上，我们的研究旨在全面评估LLaMA3在各种低比特量化技术中的表现，包括训练后量化和LoRA微调量化。我们的目标是通过利用现有的LLM量化技术，评估其在资源有限的情况下的能力边界。我们的研究结果表明，尽管LLaMA3在量化后仍表现优越，但量化所带来的性能下降是显著的，许多情况下甚至会导致更大的下降。这一发现突显了在资源受限环境中部署LLaMA3可能面临的挑战，并强调了在低比特量化背景下仍有巨大的成长和改进空间。我们研究中的经验洞察预计将对未来LLM量化技术的发展具有重要价值，特别是在缩小与原始模型之间的性能差距方面。通过解决低比特量化导致的性能退化问题，我们预计后续的量化范式将使LLMs在较低的计算成本下实现更强的能力，从而推动生成式人工智能（以LLMs为代表）的进步达到新的高度。
- en: References
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning
    about physical commonsense in natural language. In Proceedings of the AAAI conference
    on artificial intelligence, volume 34, pages 7432–7439, 2020.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Yonatan Bisk、Rowan Zellers、Jianfeng Gao、Yejin Choi 等人。Piqa：在自然语言中推理物理常识。收录于《人工智能AAAI会议论文集》，第34卷，页7432–7439，2020年。'
- en: '[2] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip:
    2-bit quantization of large language models with guarantees. Advances in Neural
    Information Processing Systems, 36, 2024.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Jerry Chee、Yaohui Cai、Volodymyr Kuleshov 和 Christopher M De Sa。Quip：具有保证的大型语言模型的2比特量化。《神经信息处理系统进展》，第36卷，2024年。'
- en: '[3] Hong Chen, Chengtao Lv, Liang Ding, Haotong Qin, Xiabin Zhou, Yifu Ding,
    Xuebo Liu, Min Zhang, Jinyang Guo, Xianglong Liu, et al. Db-llm: Accurate dual-binarization
    for efficient llms. arXiv preprint arXiv:2402.11960, 2024.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Hong Chen、Chengtao Lv、Liang Ding、Haotong Qin、Xiabin Zhou、Yifu Ding、Xuebo
    Liu、Min Zhang、Jinyang Guo、Xianglong Liu 等人。Db-llm：用于高效LLMs的准确双重二值化。arXiv预印本 arXiv:2402.11960，2024年。'
- en: '[4] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Peter Clark、Isaac Cowhey、Oren Etzioni、Tushar Khot、Ashish Sabharwal、Carissa
    Schoenick 和 Oyvind Tafjord。你认为你已经解决了问答问题？试试arc，AI2推理挑战。arXiv预印本 arXiv:1803.05457，2018年。'
- en: '[5] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms. Advances in Neural Information Processing
    Systems, 36, 2024.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 提姆·德特默斯、阿尔蒂多罗·帕尼奥尼、阿里·霍尔茨曼和卢克·泽特尔莫耶。Qlora：量化LLM的高效微调。《神经信息处理系统进展》，36，2024年。'
- en: '[6] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 埃利亚斯·弗兰塔尔、萨利赫·阿什克布斯、托尔斯滕·霍夫勒和丹·阿利斯塔赫。Gptq：生成预训练变换器的准确后训练量化。arXiv 预印本 arXiv:2210.17323，2022年。'
- en: '[7] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
    Song, and Jacob Steinhardt. Measuring massive multitask language understanding.
    arXiv preprint arXiv:2009.03300, 2020.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 丹·亨德里克斯、科林·伯恩斯、史蒂文·巴萨特、安迪·邹、曼塔斯·马泽卡、道恩·宋和雅各布·斯坦赫特。测量大规模多任务语言理解。arXiv 预印本
    arXiv:2009.03300，2020年。'
- en: '[8] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
    Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models.
    In International Conference on Learning Representations, 2021.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 爱德华·J·胡、菲利普·沃利斯、泽元·艾伦-朱、袁志·李、申旺、陆旺、魏诸·陈等。Lora：大规模语言模型的低秩适应。见《国际学习表示会议》，2021年。'
- en: '[9] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong
    Liu, Michele Magno, and Xiaojuan Qi. Billm: Pushing the limit of post-training
    quantization for llms. arXiv preprint arXiv:2402.04291, 2024.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 魏黄、杨东·刘、郝彤·秦、易·李、石铭·张、刘向龙、米歇尔·马格诺和肖娟·齐。Billm：推动LLM后训练量化的极限。arXiv 预印本 arXiv:2402.04291，2024年。'
- en: '[10] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978, 2023.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 姜林、蒋明·唐、郝天·唐、尚杨、邓兴宇和宋汉。Awq：用于LLM压缩和加速的激活感知权重量化。arXiv 预印本 arXiv:2306.00978，2023年。'
- en: '[11] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann
    Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating
    predicate argument structure. In Human Language Technology: Proceedings of a Workshop
    held at Plainsboro, New Jersey, March 8-11, 1994, 1994.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 米奇·马库斯、格蕾丝·金、玛丽·安·马尔辛基维奇、罗伯特·麦金太尔、安·比斯、马克·费格森、凯伦·卡茨和布里塔·沙斯伯格。宾州树库：谓词论元结构注释。见《人类语言技术：新泽西州普莱恩斯伯勒举行的研讨会论文集》，1994年3月8-11日，1994年。'
- en: '[12] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 斯蒂芬·梅里蒂、蔡名雄、詹姆斯·布拉德伯里和理查德·索赫。指针哨兵混合模型。arXiv 预印本 arXiv:1609.07843，2016年。'
- en: '[13] Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda
    Liu, Jie Luo, Xianglong Liu, and Michele Magno. Accurate lora-finetuning quantization
    of llms via information retention. arXiv preprint arXiv:2402.05445, 2024.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 郝彤·秦、马旭东、郑兴宇、李晓阳、杨张、刘守达、罗洁、刘向龙和米歇尔·马格诺。通过信息保留的准确Lora微调量化。arXiv 预印本 arXiv:2402.05445，2024年。'
- en: '[14] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. The Journal of Machine Learning
    Research, 21(1):5485–5551, 2020.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 科林·拉费尔、诺姆·沙泽尔、亚当·罗伯茨、凯瑟琳·李、沙然·纳朗、迈克尔·马特纳、阎启周、魏李和彼得·J·刘。探索统一的文本到文本变换器的迁移学习极限。《机器学习研究杂志》，21(1):5485–5551，2020年。'
- en: '[15] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
    Winogrande: An adversarial winograd schema challenge at scale. Communications
    of the ACM, 64(9):99–106, 2021.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 坂口启介、罗南·勒布拉斯、昌德拉·巴哈伽图拉和叶津·崔。Winogrande：大规模对抗性Winograd模式挑战。《ACM通讯》，64(9):99–106，2021年。'
- en: '[16] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. Pb-llm: Partially
    binarized large language models. arXiv preprint arXiv:2310.00034, 2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 余章尚、袁志航、吴强和董震。Pb-llm：部分二值化的大型语言模型。arXiv 预印本 arXiv:2310.00034，2023年。'
- en: '[17] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li,
    Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An
    instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 罗汉·塔奥里、伊尚·古尔贾尼、田一张、扬·杜布瓦、薛辰·李、卡洛斯·格斯特林、珀西·梁和辰井·B·哈希莫托。斯坦福阿帕卡：一个跟随指令的拉马模型。[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)，2023年。'
- en: '[18] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, 等等。 《Llama: 开放和高效的基础语言模型》。arXiv 预印本 arXiv:2302.13971，2023。'
- en: '[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 《注意力机制即是你所需要的一切》。神经信息处理系统进展，30，2017。'
- en: '[20] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In International Conference on Machine Learning, pages 38087–38099\.
    PMLR, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, 和 Song
    Han. 《Smoothquant: 大型语言模型的准确和高效的后训练量化》。在国际机器学习大会上，页面 38087–38099。PMLR，2023。'
- en: '[21] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang,
    Zhensu Chen, Xiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank
    adaptation of large language models. arXiv preprint arXiv:2309.14717, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang,
    Zhensu Chen, Xiaopeng Zhang, 和 Qi Tian. 《Qa-lora: 量化感知的大型语言模型低秩适配》。arXiv 预印本 arXiv:2309.14717，2023。'
- en: '[22] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830,
    2019.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, 和 Yejin Choi.
    《Hellaswag: 机器真的能完成你的句子吗？》arXiv 预印本 arXiv:1905.07830，2019。'
