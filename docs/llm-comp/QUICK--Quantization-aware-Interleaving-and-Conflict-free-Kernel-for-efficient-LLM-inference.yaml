- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:50:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:50:07'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient
    LLM inference'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'QUICK: 量化感知交错与无冲突内核以提高 LLM 推断效率'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10076](https://ar5iv.labs.arxiv.org/html/2402.10076)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10076](https://ar5iv.labs.arxiv.org/html/2402.10076)
- en: \NewEnviron
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \NewEnviron
- en: derivation
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 推导
- en: $\begin{split}\BODY\end{split}$ (1)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: $\begin{split}\BODY\end{split}$ (1)
- en: Taesu Kim       Jongho Lee       Daehyun Ahn       Sarang Kim Jiwoong Choi      
    Minkyu Kim       Hyungjun Kim SqueezeBits Inc
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Taesu Kim       Jongho Lee       Daehyun Ahn       Sarang Kim Jiwoong Choi      
    Minkyu Kim       Hyungjun Kim SqueezeBits Inc
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We introduce QUICK, a group of novel optimized CUDA kernels for the efficient
    inference of quantized Large Language Models (LLMs). QUICK addresses the shared
    memory bank-conflict problem of state-of-the-art mixed precision matrix multiplication
    kernels. Our method interleaves the quantized weight matrices of LLMs offline
    to skip the shared memory write-back after the dequantization. We demonstrate
    up to 1.91x speedup over existing kernels of AutoAWQ on larger batches and up
    to 1.94x throughput gain on representative LLM models on various NVIDIA GPU devices.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 QUICK，一组用于高效推断量化大型语言模型（LLMs）的新型优化 CUDA 内核。QUICK 解决了最先进的混合精度矩阵乘法内核的共享内存银行冲突问题。我们的方法离线交错
    LLM 的量化权重矩阵，以跳过反量化后的共享内存写回。我们展示了在较大批次上相比现有 AutoAWQ 内核的速度提升高达 1.91 倍，以及在各种 NVIDIA
    GPU 设备上的代表性 LLM 模型上吞吐量提高高达 1.94 倍。
- en: 'Code: https://github.com/SqueezeBits/QUICK'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '代码: https://github.com/SqueezeBits/QUICK'
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Enhancing the efficiency of Large Language Models (LLMs) has become increasingly
    crucial due to the escalating demand for deploying state-of-the-art models in
    real-world scenarios [[2](#bib.bib2), [8](#bib.bib8), [9](#bib.bib9), [15](#bib.bib15),
    [16](#bib.bib16)]. The improved performance of LLMs is attributed to their growing
    size, characterized by a trend toward larger models with parameter counts exceeding
    several hundred billion. However, the substantial size of these models has necessitated
    the adoption of model compression techniques such as quantization and pruning
    [[1](#bib.bib1), [4](#bib.bib4), [5](#bib.bib5), [11](#bib.bib11), [12](#bib.bib12),
    [17](#bib.bib17)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 提高大型语言模型（LLMs）的效率变得越来越重要，因为在现实世界场景中部署最先进模型的需求不断上升[[2](#bib.bib2), [8](#bib.bib8),
    [9](#bib.bib9), [15](#bib.bib15), [16](#bib.bib16)]。LLMs 的性能提升归因于其不断增长的规模，表现为参数量超过数百亿的大型模型的趋势。然而，这些模型的巨大规模要求采用模型压缩技术，如量化和剪枝[[1](#bib.bib1),
    [4](#bib.bib4), [5](#bib.bib5), [11](#bib.bib11), [12](#bib.bib12), [17](#bib.bib17)]。
- en: Among these techniques, weight-only quantization has garnered significant attention
    for its potential to compress the memory footprint of LLMs [[6](#bib.bib6), [11](#bib.bib11),
    [12](#bib.bib12)]. This approach aims to reduce model size and accelerate computation
    by quantizing weights to smaller bit sizes while retaining activation tensors
    at higher precision. Consequently, there is a growing need for fast mixed-precision
    General Matrix Multiplication (GEMM) kernels to support such operations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些技术中，仅权重量化由于其压缩 LLM 内存占用的潜力而受到广泛关注[[6](#bib.bib6), [11](#bib.bib11), [12](#bib.bib12)]。这种方法旨在通过将权重量化为较小的位数，同时保持激活张量的较高精度，从而减少模型大小并加速计算。因此，对于支持这种操作的快速混合精度通用矩阵乘法
    (GEMM) 核心的需求日益增长。
- en: Despite these advancements, existing open-source kernels for mixed-precision
    GEMM have demonstrated limitations in throughput, primarily due to the overhead
    associated with weight dequantization. Analysis of these kernels has revealed
    shared memory write-back bank conflicts during the dequantization process as a
    significant bottleneck. Leveraging this insight, we introduce QUICK, a solution
    designed to mitigate shared memory bank conflicts by reordering weight matrices
    offline.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了这些进展，但现有的开源混合精度 GEMM 核心在吞吐量方面显示出局限性，主要是由于与权重反量化相关的开销。对这些核心的分析揭示了在反量化过程中共享内存写回银行冲突作为一个主要瓶颈。基于这一洞察，我们引入了
    QUICK，这是一种旨在通过离线重新排序权重矩阵来缓解共享内存银行冲突的解决方案。
- en: 2 Preliminary
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 初步
- en: 2.1 Quantization and Dequantization
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 量化与反量化
- en: Quantization involves the reduction of precision or range of a continuous variable
    to a discrete set of values. This process is commonly employed to decrease the
    bit precision of tensors, thereby reducing the memory footprint of Neural Network
    models. When supported by appropriate computation kernels, quantization enables
    acceleration of the models with low-precision computations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 量化涉及将连续变量的精度或范围减少到离散值集的过程。这一过程通常用于减少张量的位精度，从而减少神经网络模型的内存占用。当计算内核支持时，量化可以加速低精度计算的模型。
- en: Given that LLMs typically encompass billions of parameters, researchers have
    explored quantization as a means to reduce memory usage and improve inference
    efficiency. Specifically, weight-only quantization focuses solely on quantizing
    the weights of the model while maintaining activations at a higher precision,
    such as 16-bit floating point [[6](#bib.bib6), [11](#bib.bib11), [12](#bib.bib12)].
    This strategy effectively reduces memory requirements by representing weights
    with fewer bits while retaining activation tensors in floating point precision.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于LLMs通常包含数十亿个参数，研究人员探索了量化作为减少内存使用和提高推理效率的方法。具体而言，权重量化仅关注于量化模型的权重，同时保持激活值在更高的精度下，例如16位浮点数[[6](#bib.bib6),
    [11](#bib.bib11), [12](#bib.bib12)]。这一策略通过用更少的位表示权重，同时保持激活张量为浮点数精度，从而有效减少内存需求。
- en: Weight-only quantization is generally recognized for dramatically reducing the
    memory requirements and preserving the performance of LLMs. However, since activations
    remain in higher precision, weights must undergo dequantization back to higher
    precision before being multiplied by activations during inference. This dequantization
    process has minimal impact on inference efficiency when the batch size is 1 since
    the computation is mainly memory-bounded in such case. However, for larger batch
    sizes, GEMMs are mostly computation-bounded, where mixed-precision GEMM operations
    become slower than their floating-point counterparts due to the overhead associated
    with dequantization.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 权重量化通常被认为能够显著减少内存需求并保持LLMs的性能。然而，由于激活值保持在更高的精度，权重必须在推理过程中乘以激活值之前进行去量化以恢复更高的精度。对于批量大小为1的情况，由于计算主要受内存限制，因此这种去量化过程对推理效率的影响微乎其微。然而，对于较大的批量大小，GEMMs主要受计算限制，其中混合精度的GEMM操作由于去量化相关的开销，比其浮点数对手慢。
- en: 2.2 GEMM kernel using Tensor Core
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 使用Tensor Core的GEMM内核
- en: A substantial portion of the computational workload associated with LLMs primarily
    comprises GEMMs. Optimizing GEMM operations plays a pivotal role in enhancing
    the overall efficiency of LLM inference. Particularly on NVIDIA GPUs, GEMM computation
    has relied on the tiling strategy, which is widely employed to maximize memory
    reuse through the utilization of shared memory, thereby achieving a more favorable
    compute-memory ratio.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大量的计算工作负载与LLMs（大规模语言模型）主要由GEMMs（广义矩阵乘法）组成。优化GEMM操作在提高LLM推理的整体效率中扮演着关键角色。特别是在NVIDIA
    GPUs上，GEMM计算依赖于分块策略，该策略广泛用于通过利用共享内存来最大化内存重用，从而实现更优的计算-内存比。
- en: '![Refer to caption](img/bf10619f108efbdd8b24c4e4a29c282b.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bf10619f108efbdd8b24c4e4a29c282b.png)'
- en: 'Figure 1: Data loading pattern of ldmatrix instruction for a single $8\times
    8$ matrix. Two half-precision elements are loaded to the destination register
    d0 per each thread lane in a warp.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：单个 $8\times 8$ 矩阵的ldmatrix指令的数据加载模式。每个线程通道中，两个半精度元素被加载到目标寄存器d0中。
- en: 'Recent advancements in NVIDIA GPUs have showcased significant performance improvements
    in GEMM computation through the utilization of Tensor Cores. These Tensor Core-based
    GEMMs leverage warp-level PTX instructions, namely ldmatrix and mma. The ldmatrix
    instruction efficiently loads multiple matrices across all threads within a warp
    from shared memory into designated registers. As illustrated in Figure [1](#S2.F1
    "Figure 1 ‣ 2.2 GEMM kernel using Tensor Core ‣ 2 Preliminary ‣ QUICK: Quantization-aware
    Interleaving and Conflict-free Kernel for efficient LLM inference"), this loading
    pattern assigns small fragments of a row to each thread, facilitating warp-level
    matrix multiply-accumulation using the subsequent mma instruction.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最近 NVIDIA GPU 的进展展示了通过利用 Tensor Cores 在 GEMM 计算中取得了显著的性能提升。这些基于 Tensor Core
    的 GEMM 利用 warp 级 PTX 指令，即 ldmatrix 和 mma。ldmatrix 指令高效地将多个矩阵从共享内存加载到 warp 内的指定寄存器。如图
    [1](#S2.F1 "图 1 ‣ 2.2 使用 Tensor Core 的 GEMM 核心 ‣ 2 初步 ‣ QUICK：量化感知交错和无冲突内核以高效推理
    LLM") 所示，此加载模式将行的小片段分配给每个线程，利用随后的 mma 指令实现 warp 级矩阵乘加。
- en: The mma instruction, following the ldmatrix operation, executes the matrix multiply-accumulate
    operation at the warp level. This instruction performs the multiply-accumulate
    operation on matrices, requiring specific data patterns for each row of the multiplicands,
    as well as the accumulators. As previously described, loading matrices into each
    thread from shared memory is efficiently achieved using the ldmatrix instruction.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: mma 指令在 ldmatrix 操作之后，在 warp 级别执行矩阵乘加操作。该指令对矩阵执行乘加操作，需要对乘数和累加器的每一行采用特定的数据模式。如前所述，通过
    ldmatrix 指令从共享内存加载矩阵到每个线程中是高效实现的。
- en: Tensor Core-based GEMM computation entails repetitive calls to these instructions,
    relying on shared memory to rearrange input tensors to align with the data access
    pattern required by the mma instruction. Compared to CUDA Core-based GEMM computation,
    Tensor Core-based approaches are renowned for achieving significantly higher throughput.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Tensor Core 的 GEMM 计算需要重复调用这些指令，依赖共享内存来重新排列输入张量，以对齐 mma 指令所需的数据访问模式。与基于 CUDA
    Core 的 GEMM 计算相比，基于 Tensor Core 的方法以显著更高的吞吐量而闻名。
- en: 2.3 Mixed precision GEMM kernel
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 混合精度 GEMM 核心
- en: '![Refer to caption](img/40522adf46da7733d37ab7fbd70428b9.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/40522adf46da7733d37ab7fbd70428b9.png)'
- en: 'Figure 2: Computation overview of original kernel and QUICK. Compared to original
    kernel, QUICK bypasses 3) shared memory write-back and 4) ldmatrix operation of
    dequantized weights by using interleaving data pattern.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：原始核心和 QUICK 的计算概述。与原始核心相比，QUICK 通过使用交错数据模式绕过 3) 共享内存写回和 4) 去量化权重的 ldmatrix
    操作。
- en: Mixed precision GEMM kernels find widespread application in the inference phase
    of weight-only quantized LLMs, owing to the inherent difference in bit precision
    between activation tensors and weight tensors.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 混合精度 GEMM 核心在仅权重量化的大型语言模型的推理阶段得到广泛应用，这归因于激活张量和权重张量之间固有的位精度差异。
- en: When employing weight-only quantization, it becomes necessary to dequantize
    the quantized weights before executing the matrix multiplication operation within
    the GEMM kernel, as recent NVIDIA GPUs’ Tensor Cores do not inherently support
    mixed-precision GEMMs. Consequently, numerous implementations of efficient mixed-precision
    GEMM kernels leveraging Tensor Cores adopt parallel dequantization of quantized
    weights.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当采用仅权重量化时，必须在 GEMM 核心内执行矩阵乘法操作之前对量化权重进行去量化，因为近期 NVIDIA GPU 的 Tensor Cores 不原生支持混合精度
    GEMM。因此，许多利用 Tensor Cores 的高效混合精度 GEMM 核心实现采用了量化权重的并行去量化。
- en: 'Typically, these kernels adhere to a common workflow for weight dequantization,
    as depicted in Figure [2](#S2.F2 "Figure 2 ‣ 2.3 Mixed precision GEMM kernel ‣
    2 Preliminary ‣ QUICK: Quantization-aware Interleaving and Conflict-free Kernel
    for efficient LLM inference"). They fetch quantized and packed weights from global
    memory to registers, dequantize weights using CUDA cores, and then write the dequantized
    weights back to shared memory for the following ldmatrix instruction. The dequantization
    process employing CUDA cores involves bitwise AND operations to extract target
    sub-byte weights, bitwise SHIFT operations to rearrange bit positions, and parallel
    half-precision additions and multiplications to apply zero points and scales.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，这些内核遵循一个共同的权重反量化工作流程，如图[2](#S2.F2 "Figure 2 ‣ 2.3 Mixed precision GEMM kernel
    ‣ 2 Preliminary ‣ QUICK: Quantization-aware Interleaving and Conflict-free Kernel
    for efficient LLM inference")所示。它们从全局内存中提取量化和打包的权重到寄存器，使用CUDA核心进行权重反量化，然后将反量化后的权重写回共享内存以供后续的ldmatrix指令使用。使用CUDA核心进行的反量化过程包括按位与操作以提取目标子字节权重、按位移位操作以重新排列位位置，以及并行的半精度加法和乘法以应用零点和比例因子。'
- en: Parallel dequantization involves expanding quantized weights to larger bit sizes.
    For example, from 128-bit weight vectors consisting of 32 4-bit weights, dequantization
    produces 512-bit weight vectors containing 32 16-bit floating-point weights. This
    results in quantized weights that are four times larger compared to their full
    precision counterparts under same bandwidth requirement, increasing the burden
    of shared memory write-back. The augmented quantity of weights exacerbates shared
    memory bank conflicts during the write-back process of dequantized weights.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 并行反量化涉及将量化权重扩展到更大的位数。例如，从由32个4位权重组成的128位权重向量，反量化生成包含32个16位浮点权重的512位权重向量。这导致量化权重在相同带宽要求下比其全精度对应物大四倍，增加了共享内存回写的负担。权重数量的增加加剧了反量化权重回写过程中共享内存银行冲突的问题。
- en: '![Refer to caption](img/e3cf0e4dbe2826e50f88b523466f39f7.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e3cf0e4dbe2826e50f88b523466f39f7.png)'
- en: 'Figure 3: Number of bank conflicts from benchmark result using NVIDIA Nsight
    Compute [[14](#bib.bib14)]. A matrix multiplication of shape $64\times 8192\times
    8192(M\times N\times K)$ was used as the workload.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：使用NVIDIA Nsight Compute的基准测试结果中的银行冲突数量[[14](#bib.bib14)]。作为工作负载使用了形状为$64\times
    8192\times 8192(M\times N\times K)$的矩阵乘法。
- en: 'Given that the ldmatrix instruction requires the weight matrices to be fully
    visible on shared memory, this significantly harms the end-to-end latency of mixed-precision
    matrix multiplication. Benchmarks conducted on state-of-the-art mixed-precision
    GEMM kernels using NVIDIA’s Nsight Compute [[14](#bib.bib14)] indicate a notable
    prevalence of shared memory bank conflicts stemming from the write-back after
    dequantization, as depicted in Figure [3](#S2.F3 "Figure 3 ‣ 2.3 Mixed precision
    GEMM kernel ‣ 2 Preliminary ‣ QUICK: Quantization-aware Interleaving and Conflict-free
    Kernel for efficient LLM inference"). Consequently, mixed-precision GEMM kernels
    often struggle to achieve enhanced throughput compared to half-precision GEMM
    kernels, particularly with larger batch sizes.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '鉴于ldmatrix指令要求权重矩阵在共享内存中完全可见，这显著影响了混合精度矩阵乘法的端到端延迟。在使用NVIDIA的Nsight Compute进行的最先进的混合精度GEMM内核基准测试中[[14](#bib.bib14)]，如图[3](#S2.F3
    "Figure 3 ‣ 2.3 Mixed precision GEMM kernel ‣ 2 Preliminary ‣ QUICK: Quantization-aware
    Interleaving and Conflict-free Kernel for efficient LLM inference")所示，表明共享内存银行冲突的显著发生主要源自反量化后的回写。因此，混合精度GEMM内核往往在处理较大批量时，无法比半精度GEMM内核获得更高的吞吐量。'
- en: 3 Avoiding Bank Conflict
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 避免银行冲突
- en: In this section, we propose QUICK, a novel way to remove the shared memory write-back
    bank conflicts of mixed precision matrix multiplication. To alleviate these conflicts
    effectively, our proposal involves reordering the quantized weight matrix offline
    to align with the load pattern required by the mma instruction without the ldmatrix
    instruction.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提出了QUICK，这是一种新颖的方式来消除混合精度矩阵乘法中的共享内存回写银行冲突。为了有效地缓解这些冲突，我们的提议涉及离线重新排序量化权重矩阵，以便与mma指令所需的加载模式对齐，而不使用ldmatrix指令。
- en: 3.1 Skipping Shared Memory Write-back During Mixed Precision GEMM
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 跳过混合精度GEMM中的共享内存回写
- en: 'As previously discussed, state-of-the-art mixed precision GEMM kernels rely
    on a specific sequence involving dequantization, shared memory write-back, ldmatrix,
    and mma. The ldmatrix instruction is responsible for loading operands for the
    subsequent mma instruction, adhering to a designated pattern among the threads
    within a warp. With this instruction, each thread in a warp loads fragments of
    a row, as depicted in Figure [1](#S2.F1 "Figure 1 ‣ 2.2 GEMM kernel using Tensor
    Core ‣ 2 Preliminary ‣ QUICK: Quantization-aware Interleaving and Conflict-free
    Kernel for efficient LLM inference").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '如前所述，最先进的混合精度GEMM内核依赖于特定的序列，包括反量化、共享内存写回、ldmatrix和mma。ldmatrix指令负责加载后续mma指令的操作数，按照warp内线程之间的指定模式进行。使用此指令，warp中的每个线程加载一行的片段，如图[1](#S2.F1
    "图 1 ‣ 2.2 使用Tensor Core的GEMM内核 ‣ 2 初步 ‣ QUICK: 面向量化的交错和无冲突内核以提高LLM推理效率")所示。'
- en: Using the ldmatrix instruction to load GEMM operands to registers is a straightforward
    approach for floating-point GEMM kernels because transferring data from global
    memory to shared memory can be efficiently executed. From the Ampere architecture
    and beyond, asynchronous CUDA memory copy supports pipelining the mma instruction
    with global memory load, thereby enhancing the performance of GEMM kernels. This
    enhancement occurs as the effective memory load overhead can be reduced to the
    copy from shared memory to registers. However, in the case of mixed precision
    GEMM, there exists a noticeable overhead due to shared memory write-back. This
    is because the loaded quantized weights must be dequantized using CUDA cores,
    and the resulting dequantized weights in registers must then be written back to
    shared memory to serve as operands for the ldmatrix instruction. This overhead
    is further exacerbated by numerous shared memory bank-conflict stalls, which ultimately
    degrade the throughput of kernels.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ldmatrix指令将GEMM操作数加载到寄存器对于浮点GEMM内核是一种直接的方法，因为从全局内存到共享内存的数据传输可以高效执行。从Ampere架构及以后，异步CUDA内存复制支持将mma指令与全局内存加载进行流水线处理，从而提升GEMM内核的性能。这种提升发生在有效内存加载开销可以减少到从共享内存到寄存器的复制。然而，在混合精度GEMM的情况下，由于共享内存写回，存在显著的开销。这是因为加载的量化权重必须使用CUDA核心进行反量化，结果中的反量化权重必须写回到共享内存中，以作为ldmatrix指令的操作数。这种开销因大量的共享内存银行冲突停滞而进一步加剧，最终降低了内核的吞吐量。
- en: From the data loading pattern of the ldmatrix instruction, we observe that this
    pattern can be pre-applied to the original data since the weight data remains
    static. Considering the static nature of weight matrices throughout deployment,
    it becomes feasible to bypass the ldmatrix instruction for quantized weight matrices
    via suitable reordering. In this scenario, a direct load from global memory to
    registers proves sufficient to meet the data pattern requirements essential for
    the mma operation. Consequently, we opt to rearrange the quantized weight matrices
    and bypass the ldmatrix instruction prior to the mma operation. Through the optimization
    of both the weight pattern and the associated computing kernel, we can successfully
    eliminate shared memory write-back bank conflicts, consequently improving the
    end-to-end latency of mixed precision GEMM. Importantly, since the total amount
    of quantized weights to be read from DRAM remains the same, the overall memory
    bandwidth requirement can be maintained at the same level.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从ldmatrix指令的数据加载模式中，我们观察到，由于权重数据保持静态，这种模式可以预先应用于原始数据。考虑到权重矩阵在整个部署过程中是静态的，因此可以通过适当的重新排序来绕过ldmatrix指令。
    在这种情况下，从全局内存直接加载到寄存器足以满足mma操作所需的数据模式要求。因此，我们选择在mma操作之前重新排列量化的权重矩阵并绕过ldmatrix指令。通过优化权重模式和相关计算内核，我们可以成功消除共享内存写回的银行冲突，从而改善混合精度GEMM的端到端延迟。重要的是，由于从DRAM读取的量化权重总量保持不变，因此总体内存带宽需求可以保持在相同水平。
- en: 3.2 Interleaving Data Pattern
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 交错数据模式
- en: '![Refer to caption](img/ed7d88b88ecc1a3992753c06d0b43ccd.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ed7d88b88ecc1a3992753c06d0b43ccd.png)'
- en: 'Figure 4: ldmatrix instruction-aware weight interleaving to avoid shared memory
    conflicts. With interleaved weight matrix, direct load from DRAM to registers
    for each thread without ldmatrix is possible. Note that the figure is illustrating
    a case of computing $8\times 4\times 8(M\times N\times K)$ GEMM with 32 threads
    and its corresponding interleaving pattern.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：ldmatrix指令感知的权重交错以避免共享内存冲突。使用交错的权重矩阵，可以直接从DRAM加载到每个线程的寄存器，而无需ldmatrix。请注意，该图展示了使用32个线程计算$8\times
    4\times 8(M\times N\times K)$ GEMM的情况及其对应的交错模式。
- en: 'The interleaving pattern of the quantized weight matrices corresponds to the
    data loading pattern of the ldmatrix instruction. To bypass the ldmatrix.sync.aligned.m8n8
    instruction of the quantized weight matrices, we rearrange the weights following
    the data loading pattern of the instruction, as illustrated in Figure [4](#S3.F4
    "Figure 4 ‣ 3.2 Interleaving Data Pattern ‣ 3 Avoiding Bank Conflict ‣ QUICK:
    Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference").
    Since the CUDA kernels of QUICK rely on the mma.m16n8k16 with half-precision,
    we further devise the reordering pattern to group quantized weights for two 8$\times$8
    weight blocks. This rearrangement pattern enhances the memory locality of quantized
    weights and eliminates shared memory write-back bank conflicts.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '量化权重矩阵的交错模式对应于ldmatrix指令的数据加载模式。为了绕过量化权重矩阵的ldmatrix.sync.aligned.m8n8指令，我们根据指令的数据加载模式重新排列权重，如图[4](#S3.F4
    "Figure 4 ‣ 3.2 Interleaving Data Pattern ‣ 3 Avoiding Bank Conflict ‣ QUICK:
    Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference")所示。由于QUICK的CUDA内核依赖于半精度的mma.m16n8k16，我们进一步设计了重新排序模式，将量化权重分组为两个8$\times$8的权重块。这种重新排列模式提升了量化权重的内存局部性，并消除了共享内存写回的银行冲突。'
- en: '![Refer to caption](img/b2adfa704eecfdd48912a97fc53f8480.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/b2adfa704eecfdd48912a97fc53f8480.png)'
- en: 'Figure 5: Parallel i4-f16 dequantization kernel-aware weight reordering.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：并行i4-f16反量化内核感知的权重重新排序。
- en: 'Moreover, QUICK implements an additional rearrangement of quantized weights
    based on the pattern of the dequantization kernel. QUICK utilizes a modified version
    of the parallel dequantization kernel from FasterTransformer [[13](#bib.bib13)].
    The kernel introduces a simple interleaved pattern, as shown in Figure [5](#S3.F5
    "Figure 5 ‣ 3.2 Interleaving Data Pattern ‣ 3 Avoiding Bank Conflict ‣ QUICK:
    Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference").
    To mitigate the overhead associated with rearranging the dequantized weights and
    further enhance data locality, an additional rearrangement pattern ensuring a
    sequential weight pattern after dequantization is applied.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，QUICK根据反量化内核的模式实现了量化权重的额外重新排列。QUICK利用了FasterTransformer中[[13](#bib.bib13)]的并行反量化内核的修改版本。该内核引入了一种简单的交错模式，如图[5](#S3.F5
    "Figure 5 ‣ 3.2 Interleaving Data Pattern ‣ 3 Avoiding Bank Conflict ‣ QUICK:
    Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference")所示。为了减轻重新排列反量化权重的开销并进一步增强数据局部性，应用了确保反量化后顺序权重模式的额外重新排列模式。'
- en: '![Refer to caption](img/f88214b7eea820fe10130c93976f13b8.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/f88214b7eea820fe10130c93976f13b8.png)'
- en: 'Figure 6: QUICK weight interleaving pattern. Note that the figure is illustrating
    a case of computing $8\times 4\times 8(M\times N\times K)$ GEMM with 32 threads
    and 32-bit packed i4-f16 dequantization kernel.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：QUICK权重交错模式。请注意，该图展示了使用32个线程和32位打包的i4-f16反量化内核计算$8\times 4\times 8(M\times
    N\times K)$ GEMM的情况。
- en: 'Both weight rearrangement patterns avoid shared memory write-backs and ensure
    the sequential weight pattern after dequantization can be applied concurrently,
    as the patterns are independent. QUICK integrates both patterns as described in
    Figure [6](#S3.F6 "Figure 6 ‣ 3.2 Interleaving Data Pattern ‣ 3 Avoiding Bank
    Conflict ‣ QUICK: Quantization-aware Interleaving and Conflict-free Kernel for
    efficient LLM inference"), achieving optimal end-to-end latency while reducing
    shared memory bank conflicts and enhancing data locality.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '两种权重重新排列模式都避免了共享内存写回，并确保反量化后的顺序权重模式可以并行应用，因为这些模式是独立的。QUICK集成了图[6](#S3.F6 "Figure
    6 ‣ 3.2 Interleaving Data Pattern ‣ 3 Avoiding Bank Conflict ‣ QUICK: Quantization-aware
    Interleaving and Conflict-free Kernel for efficient LLM inference")中描述的两种模式，实现了优化的端到端延迟，同时减少了共享内存银行冲突并增强了数据局部性。'
- en: 3.3 Tile Size Optimization
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 瓦片大小优化
- en: Optimizing the number of active warps per multiprocessor plays an important
    role in improving the performance of computation kernels. Achieving higher number
    of active warps per multiprocessor can be beneficial as it facilitates the interleaving
    of warps and enables better latency hiding. Several factors, including the number
    of required registers and the size of shared memory, can limit the number of active
    warps per multiprocessor. In addition to improving throughput by eliminating shared
    memory write-back bank conflicts, QUICK leverages the reduced shared memory usage
    within the computation kernel to further enhance computational throughput.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 优化每个多处理器的活动 warp 数量在提高计算内核性能方面起着重要作用。每个多处理器实现更多的活动 warps 可以有利于 warps 的交错并实现更好的延迟隐藏。包括所需寄存器数量和共享内存大小在内的几个因素可能限制每个多处理器的活动
    warp 数量。除了通过消除共享内存写回银行冲突来提高吞吐量外，QUICK 还利用计算内核中减少的共享内存使用来进一步提高计算吞吐量。
- en: Previous mixed precision GEMM kernels have utilized shared memory to store both
    activation and weight matrices, with benchmarks indicating that the shared memory
    size per warp exerts the greatest pressure on the number of active warps per multiprocessor.
    In contrast, QUICK avoids allocating shared memory for the weight matrices, thereby
    shifting the pressure from shared memory size to the number of required registers.
    Leveraging this opportunity, QUICK increases the tile size of mixed precision
    GEMM, further reducing DRAM accesses while maintaining similar theoretical multiprocessor
    occupancy. With increased number of activation values processed per computation
    tile, weight matrices need to be loaded less frequently from DRAM. This optimization
    results in a further increase in throughput for larger batch sizes, particularly
    those exceeding 32.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的混合精度 GEMM 内核利用共享内存来存储激活和权重矩阵，基准测试表明，每个 warp 的共享内存大小对每个多处理器的活动 warp 数量施加了最大的压力。相比之下，QUICK
    避免了为权重矩阵分配共享内存，从而将压力从共享内存大小转移到所需寄存器的数量。利用这一机会，QUICK 增加了混合精度 GEMM 的 tile 大小，进一步减少了
    DRAM 访问，同时保持了类似的理论多处理器占用率。随着每个计算 tile 处理的激活值数量的增加，权重矩阵从 DRAM 加载的频率减少。这一优化在较大的
    batch size，特别是超过 32 的 batch size 中，导致了吞吐量的进一步提升。
- en: 4 Experimental Results
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验结果
- en: In this section, we evaluate the performance improvement provided by QUICK in
    comparison to both the baseline fp16 kernel and AutoAWQ-Kernel. We first compare
    the efficiency of a single matrix multiplication, followed by the comparison of
    end-to-end token generation throughput across various LLMs. Furthermore, we also
    present the benchmark results showcasing the integration of QUICK with the vLLM [[10](#bib.bib10)]
    framework. Note that all experiments involving AutoAWQ-Kernel and QUICK are based
    on 4-bit weight-only quantization.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了 QUICK 相对于基线 fp16 内核和 AutoAWQ-Kernel 的性能提升。我们首先比较了单次矩阵乘法的效率，然后比较了各种
    LLM 的端到端 token 生成吞吐量。此外，我们还展示了 QUICK 与 vLLM [[10](#bib.bib10)] 框架集成的基准结果。请注意，所有涉及
    AutoAWQ-Kernel 和 QUICK 的实验均基于 4 位权重量化。
- en: 4.1 Matrix Multiplication Performance
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 矩阵乘法性能
- en: We initially evaluate the performance of QUICK with unit matrix multiplications,
    with the matrix multiplication dimensions set to $\textit{batch size}\times 8192\times
    8192(M\times N\times K)$1.91 times compared to AutoAWQ-Kernel.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先评估了 QUICK 在单位矩阵乘法中的性能，矩阵乘法的维度设置为 $\textit{batch size}\times 8192\times 8192(M\times
    N\times K)$，比 AutoAWQ-Kernel 提高了1.91倍。
- en: '![Refer to caption](img/f5ce7a2118449cfb79c01eccbf2c5b65.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f5ce7a2118449cfb79c01eccbf2c5b65.png)'
- en: 'Figure 7: Benchmark results of matrix multiplication kernels on various GPUs.
    The shape of matrices is set to $\textit{batch size}\times 8192\times 8192(M\times
    N\times K)$.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：在各种 GPU 上的矩阵乘法内核基准结果。矩阵的形状设置为 $\textit{batch size}\times 8192\times 8192(M\times
    N\times K)$。
- en: With larger batch sizes, the token generation process tends to become computation-bounded,
    making the overhead from the dequantization process more significant. As a result,
    AutoAWQ-Kernel tends to show prominently degraded throughput compared to fp16
    kernel when the batch size approaches 128. On the other hand, QUICK, by reducing
    shared memory bank conflict problem, occasionally demonstrates faster speeds than
    the fp16 kernel, even with larger batch sizes like 128.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 随着批量大小的增大，令牌生成过程往往变成计算受限，导致去量化过程的开销更加显著。因此，当批量大小接近 128 时，AutoAWQ-Kernel 的吞吐量相较于
    fp16 内核会显著下降。另一方面，QUICK 通过减少共享内存银行冲突问题，即使在较大的批量大小如 128 下，有时也表现出比 fp16 内核更快的速度。
- en: 4.2 End-to-end Throughput
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 端到端吞吐量
- en: '![Refer to caption](img/b304683dda936c55cbbeb48eecb1afc6.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b304683dda936c55cbbeb48eecb1afc6.png)'
- en: 'Figure 8: End-to-end token generation throughput benchmarks of (a) Mistral-7B
    [[8](#bib.bib8)] on RTX 4090, (b) Vicuna-13B [[3](#bib.bib3)] on RTX A6000, (c)
    LLaMA-2-13B [[15](#bib.bib15)] on L40, and (d) LLaMA-33B [[16](#bib.bib16)] on
    A100.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：在 RTX 4090 上的 (a) Mistral-7B [[8](#bib.bib8)]，在 RTX A6000 上的 (b) Vicuna-13B
    [[3](#bib.bib3)]，在 L40 上的 (c) LLaMA-2-13B [[15](#bib.bib15)]，以及在 A100 上的 (d) LLaMA-33B
    [[16](#bib.bib16)] 的端到端令牌生成吞吐量基准测试。
- en: 'To illustrate the advantages of QUICK in the inference of quantized LLMs, we
    further evaluate the end-to-end token generation speed of various LLMs. We conducted
    tests on four different models across four different GPUs: Mistral-7B [[8](#bib.bib8)]
    on RTX 4090, Vicuna-13B [[3](#bib.bib3)] on RTX A6000, LLaMA-2-13B [[15](#bib.bib15)]
    on L40, and LLaMA-33B [[16](#bib.bib16)] on A100. The token generation throughput
    at the decoding stage was measured in terms of tokens per second.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 QUICK 在量化 LLM 推理中的优势，我们进一步评估了各种 LLM 的端到端令牌生成速度。我们在四种不同的 GPU 上测试了四种不同的模型：在
    RTX 4090 上测试 Mistral-7B [[8](#bib.bib8)]，在 RTX A6000 上测试 Vicuna-13B [[3](#bib.bib3)]，在
    L40 上测试 LLaMA-2-13B [[15](#bib.bib15)]，在 A100 上测试 LLaMA-33B [[16](#bib.bib16)]。在解码阶段测量了每秒令牌生成吞吐量。
- en: As the batch size increases, the memory required to store activations and the
    KV cache also increases, leading to Out-of-Memory (OOM) problem. For example,
    when running Mistral-7B on an RTX 4090 GPU, it is impossible to run the fp16 model
    with batch size of 256 due to the OOM problem. Applying weight-only quantization
    reduces the amount of memory used to store weights, thereby enabling usage of
    more memory for storing activations and the KV cache. Consequently, larger batch
    inference becomes possible. Even with the same RTX 4090 GPU, a 4-bit quantized
    Mistral-7B can be operated at a batch size of 256\. Moreover, QUICK can achieve
    up to 1.94 times higher throughput compared to AutoAWQ-Kernel. Similar to the
    Matrix Multiplication performance mentioned in the previous section, QUICK demonstrates
    superior performance over the fp16 case even at larger batch sizes.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 随着批量大小的增加，存储激活和 KV 缓存所需的内存也会增加，导致内存溢出（OOM）问题。例如，在 RTX 4090 GPU 上运行 Mistral-7B
    时，由于 OOM 问题，无法以 256 的批量大小运行 fp16 模型。应用仅权重量化减少了用于存储权重的内存量，从而使更多的内存可用于存储激活和 KV 缓存。因此，更大的批量推理成为可能。即使在相同的
    RTX 4090 GPU 上，4 位量化的 Mistral-7B 也可以以 256 的批量大小运行。此外，QUICK 的吞吐量可以比 AutoAWQ-Kernel
    高出 1.94 倍。类似于前一节提到的矩阵乘法性能，QUICK 在较大的批量大小下也表现出优于 fp16 的性能。
- en: 4.3 vLLM Throughput
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 vLLM 吞吐量
- en: 'In this section, we present the throughput benchmark results of our initial
    version of vLLM [[10](#bib.bib10)] integrated with QUICK (Table [1](#S4.T1 "Table
    1 ‣ 4.3 vLLM Throughput ‣ 4 Experimental Results ‣ QUICK: Quantization-aware Interleaving
    and Conflict-free Kernel for efficient LLM inference")). Benchmarks were done
    using the throughput benchmark script and the recommended dataset within the vLLM
    [[10](#bib.bib10)] framework. Two models, Vicuna-13B [[3](#bib.bib3)] and Llama-2-70B
    [[15](#bib.bib15)], were benchmarked to demonstrate scenarios where the full precision
    model could and could not be loaded onto the GPU device. vLLM with QUICK demonstrated
    a throughput gain of 27-29% compared to the AWQ implementation in vLLM, and a
    33% throughput gain compared to the full precision model.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们展示了集成 QUICK 的 vLLM [[10](#bib.bib10)] 初始版本的吞吐量基准结果（表 [1](#S4.T1 "表 1
    ‣ 4.3 vLLM 吞吐量 ‣ 4 实验结果 ‣ QUICK: 量化感知交错和无冲突内核用于高效 LLM 推理")）。基准测试使用了吞吐量基准测试脚本和
    vLLM [[10](#bib.bib10)] 框架中的推荐数据集。对两个模型 Vicuna-13B [[3](#bib.bib3)] 和 Llama-2-70B
    [[15](#bib.bib15)]] 进行了基准测试，以展示全精度模型能够或不能加载到 GPU 设备上的场景。与 vLLM 中的 AWQ 实现相比，QUICK
    显示出 27-29% 的吞吐量增益，相较于全精度模型则显示出 33% 的吞吐量增益。'
- en: 'Table 1: Throughput benchmark results of Vicuna-13B [[3](#bib.bib3)] and Llama-2-70B
    [[15](#bib.bib15)] models with vLLM [[10](#bib.bib10)] integrated with QUICK.
    Benchmarks were conducted on a machine equipped with an i9-13900K CPU, 128GB RAM,
    and an A6000 GPU.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '表1: Vicuna-13B [[3](#bib.bib3)] 和 Llama-2-70B [[15](#bib.bib15)] 模型在集成QUICK的vLLM
    [[10](#bib.bib10)] 下的吞吐量基准结果。基准测试在一台配备i9-13900K CPU、128GB RAM和A6000 GPU的机器上进行。'
- en: '| Model | FP16 | AWQ | QUICK | Speedup | Speedup |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | FP16 | AWQ | QUICK | 加速比 | 加速比 |'
- en: '| (tokens/s) | (tokens/s) | (tokens/s) | (FP16) | (AWQ) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| (tokens/s) | (tokens/s) | (tokens/s) | (FP16) | (AWQ) |'
- en: '| Vicuna-13B | 985.2 | 1030.4 | 1308.6 | 33% | 27% |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13B | 985.2 | 1030.4 | 1308.6 | 33% | 27% |'
- en: '| Llama-2-70B | OOM | 224.3 | 290.2 | - | 29% |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-70B | OOM | 224.3 | 290.2 | - | 29% |'
- en: 5 Limitation and Future Work
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 限制与未来工作
- en: While the proposed QUICK technique has demonstrated enhanced throughput at larger
    batch sizes, such as 128, enabling the utilization of weight-only quantization
    for larger batch sizes, it still falls short of the efficiency achieved in the
    fp16 case, particularly at even larger batch sizes (> 512). Therefore, further
    research is needed to optimize the dequantization process further and enhance
    the efficiency of mixed precision GEMM kernels under such circumstances.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提出的QUICK技术在较大批量大小（如128）下显示出增强的吞吐量，使得可以利用仅权重量化进行更大批量大小的处理，但它在效率上仍不及fp16，特别是在更大的批量大小（>
    512）下。因此，需要进一步研究以优化反量化过程，并在这种情况下提升混合精度GEMM内核的效率。
- en: For instance, future works could focus on exploring methods to leverage the
    unused shared memory budget resulting from the direct dequantization of quantized
    weights at registers. Additional software optimizations, such as automated split-k
    parameter optimization, could be explored further to ensure optimal throughput
    considering the model, generation configuration, and the GPU device.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，未来的工作可以集中在探索如何利用因直接反量化量化权重而产生的未使用共享内存预算。还可以进一步探索额外的软件优化，如自动化split-k参数优化，以确保在考虑模型、生成配置和GPU设备时实现最佳吞吐量。
- en: 6 Conclusion
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this work, we introduce QUICK, a suite of optimized CUDA kernels designed
    for efficient execution of mixed precision GEMM operations. Previous implementations
    exhibited advantages only for small batch sizes due to shared memory bank conflict
    problem. QUICK, however, overcomes this limitation by employing an interleaving
    data pattern, which enables superior throughput over fp16 kernels even for larger
    batch sizes. Furthermore, QUICK has demonstrated enhanced end-to-end token generation
    throughput in various LLM inference frameworks, including AutoAWQ and vLLM.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了QUICK，一套为高效执行混合精度GEMM操作而优化的CUDA内核。之前的实现仅对小批量大小显示出优势，这是由于共享内存银行冲突问题。然而，QUICK通过采用交错数据模式克服了这一限制，使其在较大批量大小下也能优于fp16内核。此外，QUICK在各种LLM推理框架中，包括AutoAWQ和vLLM，展示了增强的端到端标记生成吞吐量。
- en: References
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten
    Hoefler, and James Hensman. Slicegpt: Compress large language models by deleting
    rows and columns. arXiv preprint arXiv:2401.15024, 2024.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten
    Hoefler, 和 James Hensman。Slicegpt: 通过删除行和列压缩大型语言模型。arXiv 预印本 arXiv:2401.15024，2024年。'
- en: '[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell
    等。语言模型是少样本学习者。神经信息处理系统进展，33:1877–1901，2020年。'
- en: '[3] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
    quality, March 2023.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    和 Eric P. Xing。Vicuna: 一个开源聊天机器人，令人印象深刻的gpt-4，质量达到90%* chatgpt，2023年3月。'
- en: '[4] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8():
    8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339,
    2022.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer。Llm.int8():
    用于大规模变换器的8位矩阵乘法。arXiv 预印本 arXiv:2208.07339，2022年。'
- en: '[5] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can
    be accurately pruned in one-shot. In International Conference on Machine Learning,
    pages 10323–10337\. PMLR, 2023.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Elias Frantar 和 Dan Alistarh。Sparsegpt: 大规模语言模型可以在一次性训练中准确地进行剪枝。在国际机器学习大会，页面
    10323–10337。PMLR，2023年。'
- en: '[6] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh。Gptq: 准确的后训练量化用于生成预训练变换器。arXiv
    预印本 arXiv:2210.17323，2022年。'
- en: '[7] Casper Hansen. Autoawq_kernels. https://github.com/casper-hansen/AutoAWQ_kernels,
    2023.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Casper Hansen。Autoawq_kernels。https://github.com/casper-hansen/AutoAWQ_kernels，2023年。'
- en: '[8] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
    Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample,
    Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra
    Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume
    Lample, Lucile Saulnier 等人。Mistral 7b。arXiv 预印本 arXiv:2310.06825，2023年。'
- en: '[9] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
    Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,
    Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088,
    2024.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
    Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,
    Florian Bressand 等人。Mixtral of experts。arXiv 预印本 arXiv:2401.04088，2024年。'
- en: '[10] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao
    Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management
    for large language model serving with pagedattention. In Proceedings of the ACM
    SIGOPS 29th Symposium on Operating Systems Principles, 2023.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody
    Hao Yu, Joseph E. Gonzalez, Hao Zhang 和 Ion Stoica。使用 pagedattention 为大规模语言模型服务进行高效的内存管理。在
    ACM SIGOPS 第29届操作系统原理研讨会论文集，2023年。'
- en: '[11] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park.
    Owq: Outlier-aware weight quantization for efficient fine-tuning and inference
    of large language models. arXiv preprint arXiv:2306.02272, 2024.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim 和 Eunhyeok Park。Owq:
    针对大规模语言模型的高效微调和推理的异常值感知权重量化。arXiv 预印本 arXiv:2306.02272，2024年。'
- en: '[12] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978, 2023.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang 和 Song Han。Awq:
    激活感知权重量化用于 LLM 压缩和加速。arXiv 预印本 arXiv:2306.00978，2023年。'
- en: '[13] NVIDIA. Faster transformer. https://github.com/NVIDIA/FasterTransformer,
    2022.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] NVIDIA。Faster transformer。https://github.com/NVIDIA/FasterTransformer，2022年。'
- en: '[14] NVIDIA. Nvidia nsight compute. https://docs.nvidia.com/nsight-compute/NsightCompute/index.html,
    2024.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] NVIDIA。Nvidia nsight compute。https://docs.nvidia.com/nsight-compute/NsightCompute/index.html，2024年。'
- en: '[15] Konstantinos I Roumeliotis, Nikolaos D Tselikas, and Dimitrios K Nasiopoulos.
    Llama 2: Early adopters’ utilization of meta’s new open-source pretrained model.
    2023.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Konstantinos I Roumeliotis, Nikolaos D Tselikas 和 Dimitrios K Nasiopoulos。Llama
    2: 早期采纳者对 Meta 新开放源码预训练模型的使用。2023年。'
- en: '[16] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar 等人。Llama: 开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971，2023年。'
- en: '[17] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In International Conference on Machine Learning, pages 38087–38099\.
    PMLR, 2023.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth 和 Song
    Han。Smoothquant: 针对大规模语言模型的准确高效的后训练量化。在国际机器学习大会，页面 38087–38099。PMLR，2023年。'
