- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:49:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:36
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Combining multiple post-training techniques to achieve most efficient quantized
    LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合多种后训练技术以实现最有效的量化 LLM
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.07135](https://ar5iv.labs.arxiv.org/html/2405.07135)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.07135](https://ar5iv.labs.arxiv.org/html/2405.07135)
- en: '{strip}Sayeh Sharify'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '{strip}Sayeh Sharify'
- en: d-Matrix
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: d-Matrix
- en: Santa Clara, CA, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 美国，加州，圣克拉拉
- en: sayehs@d-matrix.ai
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: sayehs@d-matrix.ai
- en: '&Zifei Xu'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&Zifei Xu'
- en: d-Matrix Santa Clara, CA, USA
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: d-Matrix 美国，加州，圣克拉拉
- en: xuzifei@d-matrix.ai
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: xuzifei@d-matrix.ai
- en: '&Wanzin Yazar'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '&Wanzin Yazar'
- en: d-Matrix
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: d-Matrix
- en: Santa Clara, CA, USA
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 美国，加州，圣克拉拉
- en: wyazar@d-matrix.ai
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: wyazar@d-matrix.ai
- en: '&Xin Wang'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '&Xin Wang'
- en: d-Matrix
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: d-Matrix
- en: Santa Clara, CA, USA
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 美国，加州，圣克拉拉
- en: xwang@d-matrix.ai
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: xwang@d-matrix.ai
- en: Abstract
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have distinguished themselves with outstanding
    performance in complex language modeling tasks, yet they come with significant
    computational and storage challenges. This paper explores the potential of quantization
    to mitigate these challenges. We systematically study the combined application
    of two well-known post-training techniques, SmoothQuant and GPTQ, and provide
    a comprehensive analysis of their interactions and implications for advancing
    LLM quantization. We enhance the versatility of both techniques by enabling quantization
    to microscaling (MX) formats, expanding their applicability beyond their initial
    fixed-point format targets. We show that by applying GPTQ and SmoothQuant, and
    employing MX formats for quantizing models, we can achieve a significant reduction
    in the size of OPT models by up to $4\times$.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在复杂语言建模任务中表现出色，但它们也带来了显著的计算和存储挑战。本文探讨了量化在缓解这些挑战中的潜力。我们系统地研究了两种知名后训练技术——SmoothQuant
    和 GPTQ 的联合应用，并提供了它们的互动和对 LLM 量化进展的影响的全面分析。通过使这两种技术能够应用于微缩（MX）格式，我们扩展了它们的适用范围超出最初的定点格式目标。我们展示了通过应用
    GPTQ 和 SmoothQuant，并使用 MX 格式对模型进行量化，我们可以显著减少 OPT 模型的大小，最多减少 $4\times$。
- en: '*Keywords* Microscaling Formats (MX), LLM Quantization, PTQ, GPTQ, SmoothQuant'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 微缩格式（MX），LLM 量化，PTQ，GPTQ，SmoothQuant'
- en: 1 Introduction
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have emerged as extremely powerful tools to comprehend
    and generate natural language. However, their intensive computational demand and
    energy consumption make widespread adoption of these models in everyday tasks
    to be challenging. One way to address these challenges is post-training quantization,
    a technique that involves reducing the precision of model parameters and/or activations
    from the original bit-width to formats with fewer bits. Quantization can significantly
    reduce the memory footprint and computational requirements of these models, making
    them more accessible and deployable on a wider range of hardware, including mobile
    devices and edge devices. However, previous work has shown that the activations
    of LLMs with more than 3B parameters are difficult to quantize due to the emergence
    of outliers with large magnitude, which leads to significant quantization errors
    and accuracy degradation [[1](#bib.bib1)]. To address this issue, Xiao et al.
    proposed SmoothQuant, a post-training quantization technique that smooths out
    the activation outliers by migrating the quantization difficulty from activations
    to weights with a mathematically equivalent transformation [[2](#bib.bib2)]. Similarly,
    Frantar et al. proposed GPTQ, a scalable one-shot quantization method that utilizes
    approximate second-order information to quantize weights of LLMs with high efficiency
    and accuracy [[3](#bib.bib3)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经成为理解和生成自然语言的极其强大的工具。然而，它们对计算的高需求和能源消耗使得在日常任务中广泛应用这些模型变得具有挑战性。一种解决这些挑战的方法是后训练量化，这是一种将模型参数和/或激活从原始位宽减少到较少位数格式的技术。量化可以显著减少这些模型的内存占用和计算需求，使其更易于访问和部署在更广泛的硬件上，包括移动设备和边缘设备。然而，先前的研究表明，具有超过
    30 亿参数的 LLM 的激活难以量化，因为出现了大幅度的离群值，这导致了显著的量化误差和准确性下降[[1](#bib.bib1)]。为了解决这个问题，Xiao
    等人提出了 SmoothQuant，这是一种后训练量化技术，通过将量化难度从激活迁移到权重来平滑激活的离群值，采用了数学上等效的转换[[2](#bib.bib2)]。类似地，Frantar
    等人提出了 GPTQ，这是一种可扩展的一次性量化方法，通过利用近似的二阶信息来高效且准确地量化 LLM 的权重[[3](#bib.bib3)]。
- en: 'To explore the quantization challenges in LLMs, we first examine the difficulty
    of activation quantization for LLMs by measuring the activation magnitude in a
    linear layer of DistilGPT2 [[4](#bib.bib4)], and demonstrate the presence of outliers
    across the activations, similar to the LLM.int8() work [[1](#bib.bib1)]. Then
    we systematically study the interaction between two widely adopted LLM quantization
    techniques, SmoothQuant and GPTQ, which have demonstrated excellent results in
    quantizing LLMs to the fixed-point formats [[2](#bib.bib2), [3](#bib.bib3)]. Furthermore,
    we enhance these two algorithms to support quantization to *microscaling (MX)*
    data formats [[5](#bib.bib5), [6](#bib.bib6)]. We assess the necessity of enabling
    SmoothQuant for matrix multiplications between activations and weights, as well
    as those between activations¹¹1In the transformer architecture, there are two
    matrix multiplications between activations: query state with the transpose of
    key state, and attention score with value state.. Finally, we analyze the effect
    of different quantization granularities (per-channel vs. per-tensor) and quantization
    ranges (symmetric vs. affine) for fixed-point quantization with SmoothQuant.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入探讨LLMs中的量化挑战，我们首先通过测量DistilGPT2线性层中的激活量，来检查LLMs的激活量化难度[[4](#bib.bib4)]，并展示了激活中的离群值，与LLM.int8()的工作类似[[1](#bib.bib1)]。然后我们系统地研究了两种广泛采用的LLM量化技术，即SmoothQuant和GPTQ之间的交互，这两种技术在将LLMs量化为定点格式方面表现出色[[2](#bib.bib2),
    [3](#bib.bib3)]。此外，我们增强了这两种算法，以支持对*微缩（MX）*数据格式的量化[[5](#bib.bib5), [6](#bib.bib6)]。我们评估了在激活和权重之间进行矩阵乘法时，启用SmoothQuant的必要性，以及激活之间的乘法¹¹1在变换器架构中，激活之间有两个矩阵乘法：查询状态与键状态的转置，和注意力分数与值状态。最后，我们分析了不同量化粒度（按通道与按张量）和量化范围（对称与仿射）对SmoothQuant的定点量化效果。
- en: '![Refer to caption](img/634d32b44a1ef254594862ddaaaf8c19.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/634d32b44a1ef254594862ddaaaf8c19.png)'
- en: (a) Activation (Original)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 激活（原始）
- en: '![Refer to caption](img/6c1d60416b23d0a2809d2adfbd31ad64.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6c1d60416b23d0a2809d2adfbd31ad64.png)'
- en: (b) Activation (SmoothQuant)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 激活（SmoothQuant）
- en: '![Refer to caption](img/f0787cd9542768bb31796fbc430a7a2d.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f0787cd9542768bb31796fbc430a7a2d.png)'
- en: (c) Weight (Original)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 权重（原始）
- en: '![Refer to caption](img/19ad9a8fc2b5ea2127c5db6f0a2254a8.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/19ad9a8fc2b5ea2127c5db6f0a2254a8.png)'
- en: (d) Weight (SmoothQuant)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 权重（SmoothQuant）
- en: 'Figure 1: Magnitude of the input activations and weights of the “lm-head” linear
    layer in DistilGPT2 before and after SmoothQuant. (a) There are a few outlier
    channels in the original activation tensor. (c) The weight tensor has a flat and
    uniform distribution. (b and d) By migrating the quantization difficulty from
    activations to weights we can greatly smooth activation outliers, while still
    maintaining an easy-to-quantize weight tensor.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：DistilGPT2中“lm-head”线性层的输入激活和权重的大小，在SmoothQuant之前和之后。（a）原始激活张量中有少量离群通道。（c）权重张量具有平坦且均匀的分布。（b和d）通过将量化难度从激活迁移到权重，我们可以大大平滑激活离群值，同时仍然保持易于量化的权重张量。
- en: Microscaling format.
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微缩格式。
- en: The microscaling (MX) format for deep neural net computation was proposed by
    prior work, first as MSFP [[7](#bib.bib7), [5](#bib.bib5)] and later subsumed
    by an emerging industry standard *microscaling formats* [[6](#bib.bib6)]. Specifically
    MXINT8, a microscaling format that enables high-accuracy inference using half
    the memory footprint and twice the throughput of FP16, is an emerging industry
    standard endorsed by Microsoft, AMD, Arm, Intel, Meta, NVIDIA, and Qualcomm [[6](#bib.bib6)],
    already seeing adoption in today’s hardware products, such as Qualcomm Cloud AI100
    Accelerator [[8](#bib.bib8)].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络计算的微缩（MX）格式由以前的工作提出，最初为MSFP[[7](#bib.bib7), [5](#bib.bib5)]，后来被新兴的行业标准*微缩格式*[[6](#bib.bib6)]所取代。具体来说，MXINT8是一种微缩格式，能够使用一半的内存占用和两倍的FP16吞吐量进行高精度推理，是微软、AMD、Arm、Intel、Meta、NVIDIA和Qualcomm等公司支持的新兴行业标准[[6](#bib.bib6)]，已经在今天的硬件产品中得到了应用，例如Qualcomm
    Cloud AI100加速器[[8](#bib.bib8)]。
- en: 'The MX format, as outlined in this paper, is characterized by three key components:
    1) the scale factor data type, 2) the data type and precision of individual elements,
    and 3) the scaling block size. The scale factor is applied uniformly across a
    block of individual elements. This paper specifically focuses on MX formats employing
    the *INT* data type for individual elements, thus termed *MXINT*.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本文所述的MX格式由三个关键组件组成：1）缩放因子数据类型，2）单个元素的数据类型和精度，3）缩放块大小。缩放因子均匀地应用于一块单独的元素。本文特别关注使用*INT*数据类型的MX格式，因此称之为*MXINT*。
- en: Notation.
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 记号。
- en: Throughout the paper we denote a microscaling (MX) format with scaling block
    size of $b$ integer bits and no fractional bits is denoted by INT-i.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在整篇论文中，我们用$ b $位整数位和没有小数位的微缩（MX）格式表示INT-i。
- en: 'Contributions:'
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 贡献：
- en: '1.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We enhance SmoothQuant and GPTQ to support quantization to microscaling (MX)
    data formats, extending their compatibility beyond the initially targeted fixed-point
    formats in the proposed methods.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们增强了SmoothQuant和GPTQ，以支持对微缩（MX）数据格式的量化，扩展了它们在提议方法中超出初始目标固定点格式的兼容性。
- en: '2.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We examine the necessity of enabling SmoothQuant for matrix multiplications
    between activations and weights, and those between activations. We show that enabling
    SmoothQuant only for the former operations is sufficient to preserve the perplexity
    of the baseline models.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们研究了启用SmoothQuant进行激活与权重之间的矩阵乘法以及激活之间的矩阵乘法的必要性。我们展示了仅对前者启用SmoothQuant足以保持基线模型的困惑度。
- en: '3.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We study the interaction of SmoothQuant and GPTQ and show that SmoothQuant and
    GPTQ are synergistic, an effect most prominent in smaller models.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们研究了SmoothQuant和GPTQ的互动，展示了SmoothQuant和GPTQ是协同的，这种效果在较小的模型中最为明显。
- en: 2 Activation Quantization Difficulty
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 激活量化难度
- en: Previous work has shown that LLMs are difficult to quantize due to the presence
    of activation outliers [[1](#bib.bib1), [9](#bib.bib9), [10](#bib.bib10)]. We
    verify this by visualizing the input activations and the weights of a linear layer
    in DistilGPT2 [[4](#bib.bib4)]. Figure [1(a)](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction
    ‣ Combining multiple post-training techniques to achieve most efficient quantized
    LLMs") illustrates the magnitude of the input activations for the “lm-head” layer
    of DistilGPT2\. The existing activation outliers in some of the channels dominate
    the maximum magnitude measurement, leading to few effective bits for non-outlier
    values using the per-tensor quantization scheme. This makes it difficult to quantize
    the activation tensor. On the other hand, as shown in Figure [1(c)](#S1.F1.sf3
    "In Figure 1 ‣ 1 Introduction ‣ Combining multiple post-training techniques to
    achieve most efficient quantized LLMs") the weight distribution of the same layer
    is quite uniform and flat, making its quantization easier compared to quantizing
    the activations. SmoothQuant proposes a technique to migrate the quantization
    difficulty from activations to weights, such that the “smoothed” activations and
    the adjusted weights are both easy to quantize [[2](#bib.bib2)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的工作表明，由于存在激活异常值，LLMs很难进行量化[[1](#bib.bib1), [9](#bib.bib9), [10](#bib.bib10)]。我们通过可视化DistilGPT2中的输入激活和线性层的权重来验证这一点[[4](#bib.bib4)]。图[1(a)](#S1.F1.sf1
    "图1 ‣ 1 引言 ‣ 结合多种后训练技术以实现最有效的量化LLMs")展示了DistilGPT2的“lm-head”层的输入激活幅度。某些通道中现有的激活异常值主导了最大幅度测量，导致使用每张量量化方案对非异常值的有效位数较少，这使得激活张量的量化变得困难。另一方面，如图[1(c)](#S1.F1.sf3
    "图1 ‣ 1 引言 ‣ 结合多种后训练技术以实现最有效的量化LLMs")所示，相同层的权重分布相当均匀和平坦，相比于量化激活，量化权重要更容易。SmoothQuant提出了一种将量化难度从激活迁移到权重的技术，使得“平滑”激活和调整后的权重都容易进行量化[[2](#bib.bib2)]。
- en: 3 SmoothQuant
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 SmoothQuant
- en: 'SmoothQuant (SQ) is a quantization method that targets both activations and
    weights of a model [[2](#bib.bib2)]. In this approach, the activation of a linear
    layer is scaled by a per-channel smoothing factor $s\in R^{C_{i}}$ to minimize
    quantization errors. Simultaneously, the weight of the layer is adjusted in the
    opposite direction to maintain the mathematical equivalence of the linear layer:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: SmoothQuant (SQ) 是一种量化方法，针对模型的激活和权重[[2](#bib.bib2)]。在这种方法中，线性层的激活通过每通道平滑因子 $s\in
    R^{C_{i}}$ 进行缩放，以最小化量化误差。同时，层的权重在相反的方向上进行调整，以保持线性层的数学等价性：
- en: '|  | $\textbf{Y}=(\textbf{X}\texttt{diag}(s)^{-1})\cdot(\texttt{diag}(s)\textbf{W})=\hat{\textbf{X}}\hat{\textbf{W}}$
    |  | (1) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{Y}=(\textbf{X}\texttt{diag}(s)^{-1})\cdot(\texttt{diag}(s)\textbf{W})=\hat{\textbf{X}}\hat{\textbf{W}}$
    |  | (1) |'
- en: 'In Equation [1](#S3.E1 "In 3 SmoothQuant ‣ Combining multiple post-training
    techniques to achieve most efficient quantized LLMs"), X is the original input
    activation with outliers, and $\hat{\textbf{X}}=\textbf{X}\texttt{diag}(s)^{-1}$
    is set to:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程[1](#S3.E1 "In 3 SmoothQuant ‣ Combining multiple post-training techniques
    to achieve most efficient quantized LLMs")中，X 是带有异常值的原始输入激活，而 $\hat{\textbf{X}}=\textbf{X}\texttt{diag}(s)^{-1}$
    被设置为：
- en: '|  | $s_{j}=\texttt{max}(&#124;\textbf{X}_{j}&#124;),\hskip 8.53581ptj={1,2,...,C_{i}}$
    |  | (2) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{j}=\texttt{max}(&#124;\textbf{X}_{j}&#124;),\hskip 8.53581ptj={1,2,...,C_{i}}$
    |  | (2) |'
- en: 'Where $C_{i}$ calibration samples from the calibration dataset (see Section [5.1](#S5.SS1
    "5.1 Setups ‣ 5 Experiments ‣ Combining multiple post-training techniques to achieve
    most efficient quantized LLMs") for more details). By dividing the input activation
    by the the scaling factor of Equation [2](#S3.E2 "In 3 SmoothQuant ‣ Combining
    multiple post-training techniques to achieve most efficient quantized LLMs"),
    all channels of the scaled input activation would have the same range, making
    quantization of the scaled tensor to be very easy. However, this will migrate
    the difficulty of the quantization completely to the weight side of a linear layer.
    To address this issue, Xiao et al. proposed a scaling formula that balances the
    quantization difficulty of activations and weights:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $C_{i}$ 是来自校准数据集的校准样本（详见第[5.1](#S5.SS1 "5.1 Setups ‣ 5 Experiments ‣ Combining
    multiple post-training techniques to achieve most efficient quantized LLMs")节）。通过将输入激活除以方程[2](#S3.E2
    "In 3 SmoothQuant ‣ Combining multiple post-training techniques to achieve most
    efficient quantized LLMs")中的缩放因子，所有缩放后的输入激活通道将具有相同的范围，使得缩放张量的量化变得非常容易。然而，这将完全将量化的难度转移到线性层的权重上。为了解决这个问题，Xiao
    等人提出了一种缩放公式，平衡了激活和权重的量化难度：
- en: '|  | $s_{j}=\texttt{max}(&#124;\textbf{X}_{j}&#124;)^{\alpha}/\texttt{max}(&#124;\textbf{W}_{j}&#124;)^{1-\alpha},\hskip
    8.53581ptj={1,2,...,C_{i}}$ |  | (3) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{j}=\texttt{max}(&#124;\textbf{X}_{j}&#124;)^{\alpha}/\texttt{max}(&#124;\textbf{W}_{j}&#124;)^{1-\alpha},\hskip
    8.53581ptj={1,2,...,C_{i}}$ |  | (3) |'
- en: Where $\alpha$ is a hyper-parameter that controls how much quantization difficulty
    we want to migrate from activations to weights. For quantization to the MX format
    using SmoothQuant, we directly calculated the SmoothQuant scaling factors, skipping
    the additional calibration phase required for quantization to fixed-point formats.
    For more details on the SmoothQuant algorithm refer to Xiao et al.’s work [[2](#bib.bib2)].
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 是一个超参数，用于控制我们希望将多少量化难度从激活迁移到权重。对于使用 SmoothQuant 的 MX 格式量化，我们直接计算了
    SmoothQuant 的缩放因子，跳过了固定点格式量化所需的额外校准阶段。有关 SmoothQuant 算法的更多详细信息，请参见 Xiao 等人的工作[[2](#bib.bib2)]。
- en: 'Algorithm 1 Enhanced GPTQ: Quantize W given inverse Hessian $\textbf{H}^{-1}=(2\textbf{X}\textbf{X}^{T}+\lambda\textbf{I})^{-1}$.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 增强版 GPTQ：给定逆 Hessian $\textbf{H}^{-1}=(2\textbf{X}\textbf{X}^{T}+\lambda\textbf{I})^{-1}$
    对 W 进行量化。
- en: 'Input: $W$end forReturn: Q'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$W$结束 返回：Q
- en: 4 GPTQ
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 GPTQ
- en: 'GPTQ is a post-training quantization (PTQ) method that uses second-order Hessian
    information for weight quantization in LLMs [[3](#bib.bib3)]. It employs layer-wise
    quantization for each layer $l$. In other words, GPTQ aims to find [[3](#bib.bib3)]:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ 是一种后训练量化（PTQ）方法，它使用二阶 Hessian 信息进行 LLMs 的权重量化[[3](#bib.bib3)]。它对每层 $l$
    进行逐层量化。换句话说，GPTQ 旨在寻找[[3](#bib.bib3)]：
- en: '|  | $\texttt{argmin}_{\hat{\textbf{W}}_{l}}&#124;&#124;\textbf{W}_{l}\textbf{X}_{l}-\hat{\textbf{W}_{l}}\textbf{X}_{l}&#124;&#124;^{2}_{2}$
    |  | (4) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\texttt{argmin}_{\hat{\textbf{W}}_{l}}&#124;&#124;\textbf{W}_{l}\textbf{X}_{l}-\hat{\textbf{W}_{l}}\textbf{X}_{l}&#124;&#124;^{2}_{2}$
    |  | (4) |'
- en: 'To solve equation [4](#S4.E4 "In 4 GPTQ ‣ Combining multiple post-training
    techniques to achieve most efficient quantized LLMs"), GPTQ quantizes each row
    of the weight matrix, W, independently, focusing on a single weight per row at
    a time. It consistently updates all not-yet-quantized weights to offset the error
    introduced by quantizing a single weight. Since the objective function in equation [4](#S4.E4
    "In 4 GPTQ ‣ Combining multiple post-training techniques to achieve most efficient
    quantized LLMs") is quadratic, its Hessian H can be calculated using the following
    formula, where $F$ denotes the set of remaining full-precision weights:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决方程[4](#S4.E4 "在4 GPTQ ‣ 结合多种后训练技术实现最有效的量化LLMs")，GPTQ独立地对权重矩阵W的每一行进行量化，集中在每行的一个权重上。它不断更新所有尚未量化的权重，以抵消量化单个权重所引入的误差。由于方程[4](#S4.E4
    "在4 GPTQ ‣ 结合多种后训练技术实现最有效的量化LLMs")中的目标函数是二次的，其Hessian H可以使用以下公式计算，其中$F$表示剩余的全精度权重集：
- en: '|  | $\textbf{H}_{F}=2\textbf{X}_{F}\textbf{X}_{F}^{T}$ |  | (5) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{H}_{F}=2\textbf{X}_{F}\textbf{X}_{F}^{T}$ |  | (5) |'
- en: 'Given H, the next to be quantized weight, $w_{q}$ to the nearest quantized
    value  [[3](#bib.bib3)]:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 给定H，接下来要量化的权重$w_{q}$取最近的量化值[[3](#bib.bib3)]：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: For all rows of W, GPTQ quantizes weights in the same order. This accelerates
    the process, as certain computations need to be performed only once for each column
    rather than once for each weight. Additionally, the vectorized implementation
    of GPTQ enables processing multiple rows of W simultaneously.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于W的所有行，GPTQ以相同的顺序对权重进行量化。这加快了过程，因为某些计算仅需对每列执行一次，而不是对每个权重执行一次。此外，GPTQ的矢量化实现使得可以同时处理W的多行。
- en: The GPTQ algorithm, as originally proposed, is designed for quantization to
    a fixed-point format. We have enhanced the algorithm to also support quantization
    to a *microscaling (MX) format*. Algorithm 1 provides pseudocode for the modified
    GPTQ, that enables MX quantization. Note that for quantizing W to a specific MX
    format, the micro-block size in the algorithm, $B_{2}$, should be a multiple of
    the block size of the MX format. Additionally, to reduce the GPU memory requirement,
    we have implemented the GPTQ quantization process on the actual layer inputs in
    the partially quantized model rather than on the layer inputs of the full precision
    model. For more details on the GPTQ algorithm refer to Frantar et al.’s work [[3](#bib.bib3)].
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ算法最初设计用于量化为定点格式。我们已增强该算法，以支持量化为*微缩（MX）格式*。算法1提供了修改后的GPTQ的伪代码，能够实现MX量化。注意，对于将W量化为特定的MX格式，算法中的微块大小$B_{2}$应为MX格式块大小的倍数。此外，为了减少GPU内存需求，我们已在部分量化模型的实际层输入上实现了GPTQ量化过程，而不是在全精度模型的层输入上。有关GPTQ算法的更多细节，请参考Frantar等人的工作[[3](#bib.bib3)]。
- en: '![Refer to caption](img/708a5cdbe196c164a23867bcde04598c.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/708a5cdbe196c164a23867bcde04598c.png)'
- en: 'Figure 2: Perplexity for OPT models with different sizes when quantized to
    INT-8, MXINT8-64, and MXINT4-128\. The Y-axis represents perplexity in logarithmic
    scale. The X-axis represents the OPT models with different sizes. The Per-tensor
    affine scheme is used for INT-8 quantization of both activations and weights.
    The figure shows that: a) For models larger than 6.7B parameters quantization
    to INT-8 significantly degrades model performance, this is shown by a cliff in
    the perplexity curve of INT-8 at 6.7B (the orange curve). b) There are no such
    cliffs with the MXINT quantizations. c) SmoothQuant mitigates the cliff, preserving
    the baseline (FP16) perplexity of OPT models across different scales when quantized
    to INT-8\. d) SmoothQuant is beneficial with more aggressive quantization to MXINT4,
    reducing the perplexity gap between the baseline model and the quantized models.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：在量化为INT-8、MXINT8-64和MXINT4-128时，不同大小的OPT模型的困惑度。Y轴表示对数尺度的困惑度。X轴表示不同大小的OPT模型。INT-8量化激活和权重时使用了Per-tensor
    affine方案。图示显示：a) 对于参数量大于6.7B的模型，量化为INT-8会显著降低模型性能，这在INT-8的困惑度曲线在6.7B处出现的悬崖（橙色曲线）中有所体现。b)
    MXINT量化没有这样的悬崖。c) SmoothQuant减轻了悬崖，保持了OPT模型在不同规模下量化为INT-8时的基线（FP16）困惑度。d) SmoothQuant在对MXINT4进行更激进的量化时效果显著，缩小了基线模型与量化模型之间的困惑度差距。
- en: 5 Experiments
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Setups
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 设置
- en: Models.
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型。
- en: We benchmarked different quantization methods on DistilGPT2 [[4](#bib.bib4)],
    the OPT [[11](#bib.bib11)], and the LLaMA [[12](#bib.bib12)] families. DistilGPT2
    is a distilled version of GPT-2 with only 82 million parameters. It is challenging
    to quantize a parameter-efficient model like DistilGPT2 as the model is already
    designed to be compact, and further reducing precision during quantization may
    hurt the model performance significantly, requiring careful optimization to maintain
    the desired balance between model size and accuracy. LLaMA and OPT are two families
    of open-source LLMs that are widely accepted among the machine learning community
    due to their superior performance compared to other open-source LLMs [[1](#bib.bib1),
    [3](#bib.bib3), [2](#bib.bib2), [13](#bib.bib13)]. LLaMA is also considered to
    be the foundation of many popular open-source models such as Alpaca [[14](#bib.bib14)],
    Vicuna [[15](#bib.bib15)], Guanaco [[16](#bib.bib16)], and Stable Beluga [[17](#bib.bib17)].
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 DistilGPT2 [[4](#bib.bib4)]、OPT [[11](#bib.bib11)] 和 LLaMA [[12](#bib.bib12)]
    系列上对不同的量化方法进行了基准测试。DistilGPT2 是 GPT-2 的一个精简版本，仅有 8200 万个参数。量化像 DistilGPT2 这样已经设计得非常紧凑的参数高效模型是有挑战的，因为进一步降低精度可能会显著影响模型性能，需要仔细优化以保持模型大小和准确性之间的理想平衡。LLaMA
    和 OPT 是两个在机器学习社区中广泛接受的开源 LLM 系列，由于其性能优越，相比其他开源 LLMs [[1](#bib.bib1), [3](#bib.bib3),
    [2](#bib.bib2), [13](#bib.bib13)]。LLaMA 还被认为是许多流行的开源模型的基础，如 Alpaca [[14](#bib.bib14)]、Vicuna
    [[15](#bib.bib15)]、Guanaco [[16](#bib.bib16)] 和 Stable Beluga [[17](#bib.bib17)]。
- en: Datasets.
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集。
- en: Following previous work  [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [13](#bib.bib13),
    [18](#bib.bib18), [19](#bib.bib19)], we measured the perplexity of quantized language
    models on WikiText-2 [[20](#bib.bib20)] as perplexity can stably reflect the performance
    of LLMs [[18](#bib.bib18), [13](#bib.bib13)]. Unless otherwise stated, the test
    split of the dataset is used to evaluate the models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 参考之前的工作 [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [13](#bib.bib13), [18](#bib.bib18),
    [19](#bib.bib19)]，我们测量了量化语言模型在 WikiText-2 [[20](#bib.bib20)] 上的困惑度，因为困惑度可以稳定地反映
    LLMs 的表现 [[18](#bib.bib18), [13](#bib.bib13)]。除非另有说明，否则数据集的测试集用于评估模型。
- en: Quantization formats.
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 量化格式。
- en: We evaluated models using different microscaling and fixed-point quantization
    formats; the same numerical format was applied for quantizing both activations
    and weights unless specified otherwise. For the fixed-point quantization, we calibrated
    the models using $128$ random input sentences from WikiText-2-train to estimate
    the dynamic range of activations. We utilized *MinMaxObserver* to find the range
    of activations, and calculated the zero-point and the scale parameters for the
    activations and weights in per-tensor granularity levels.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用不同的微缩和定点量化格式评估了模型；除非另有说明，否则对激活和权重的量化应用了相同的数值格式。对于定点量化，我们使用来自 WikiText-2-train
    的 $128$ 个随机输入句子对模型进行校准，以估计激活的动态范围。我们利用 *MinMaxObserver* 找到激活的范围，并计算了激活和权重的零点和缩放参数，按每个张量的粒度水平进行。
- en: Activation smoothing.
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 激活平滑。
- en: We calculated the per-channel scaling factor for activations and weights using
    the formula stated in Equation [1](#S3.E1 "In 3 SmoothQuant ‣ Combining multiple
    post-training techniques to achieve most efficient quantized LLMs"). As in the
    previous work, we consistently use a migration strength ($\alpha$ random sentences
    from the WikiText-2-train dataset. Once we calculated the scaling factors, we
    used the same values to evaluate the models with different quantization formats.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用公式计算了激活和权重的每通道缩放因子，公式见方程 [1](#S3.E1 "在 3 SmoothQuant ‣ 结合多种后训练技术以实现最有效的量化
    LLMs")。与之前的工作一样，我们始终使用迁移强度 ($\alpha$ 随机句子来自 WikiText-2-train 数据集。一旦计算出缩放因子，我们使用相同的值来评估具有不同量化格式的模型。
- en: Targeted layers.
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 目标层。
- en: Similar to the previous work [[2](#bib.bib2)], we apply smoothing on the input
    activation of the self-attention and the feed-forward layers of LLMs. Unless stated
    otherwise, we transform all Linear layers, HFConv1D layers, and Activation by
    Activation matrix multiplications to the specified quantization format while keeping
    the activation/weight in the original format for other layers including ReLU,
    GELU, Softmax, and LayerNorm.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于之前的工作 [[2](#bib.bib2)]，我们对 LLMs 的自注意力和前馈层的输入激活应用了平滑。除非另有说明，我们将所有 Linear 层、HFConv1D
    层和 Activation 层通过 Activation 矩阵乘法转换为指定的量化格式，同时保持 ReLU、GELU、Softmax 和 LayerNorm
    层的激活/权重为原始格式。
- en: 5.2 Results
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 结果
- en: 5.2.1 SmoothQuant effect on quantization
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 SmoothQuant 对量化的影响
- en: 'In this section, we evaluate the impact of the SmoothQuant technique on the
    quantization of LLMs from different families: OPT, DistilGPT2, LLaMA, and the
    llama2 models. We employ various fixed-point and MX formats with different bit-widths
    for our assessment.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了 SmoothQuant 技术对不同家族的 LLM（OPT、DistilGPT2、LLaMA 和 llama2 模型）量化的影响。我们采用了各种固定点和
    MX 格式，具有不同的位宽进行评估。
- en: 'Table 1: DistilGPT2 quantization results, with and without SmoothQuant. Act,
    Wgt, and PPL denote activation, weight, and perplexity, respectively. $\downarrow$:
    the lower the metric, the better the result. We used per-tensor affine quantization
    for the fixed-point formats. LR: Likelihood Ratio over SmoothQuant disabled.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：DistilGPT2 量化结果，使用和未使用 SmoothQuant。Act、Wgt 和 PPL 分别表示激活、权重和困惑度。$\downarrow$：指标越低，结果越好。我们对固定点格式使用了每张量的仿射量化。LR：禁用
    SmoothQuant 的可能性比率。
- en: '| Act & Wgt format | $\downarrow$PPL w/ SQ | LR |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Act & Wgt 格式 | $\downarrow$PPL w/ SQ | LR |'
- en: '| Baseline (FP32) | 46.06 | N/A | N/A |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 基线（FP32） | 46.06 | 不适用 | 不适用 |'
- en: '| MXINT16-64 | 46.06 | 46.06 | 1.00 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| MXINT16-64 | 46.06 | 46.06 | 1.00 |'
- en: '| MXINT16-32 | 46.06 | 46.06 | 1.00 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| MXINT16-32 | 46.06 | 46.06 | 1.00 |'
- en: '| INT-8 | 59.23 | 60.91 | 0.97 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| INT-8 | 59.23 | 60.91 | 0.97 |'
- en: '| MXINT8-128 | 46.49 | 46.27 | 1.00 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| MXINT8-128 | 46.49 | 46.27 | 1.00 |'
- en: '| MXINT8-16 | 46.22 | 46.10 | 1.00 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| MXINT8-16 | 46.22 | 46.10 | 1.00 |'
- en: '| INT-6 | 2230.21 | 1875.86 | 1.19 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| INT-6 | 2230.21 | 1875.86 | 1.19 |'
- en: '| MXINT6-128 | 54.09 | 51.91 | 1.04 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| MXINT6-128 | 54.09 | 51.91 | 1.04 |'
- en: '| MXINT6-16 | 48.42 | 47.70 | 1.02 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| MXINT6-16 | 48.42 | 47.70 | 1.02 |'
- en: '| INT-4 | 11559.05 | 7367.77 | 1.57 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| INT-4 | 11559.05 | 7367.77 | 1.57 |'
- en: '| MXINT4-128 | 1649.68 | 898.35 | 1.84 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| MXINT4-128 | 1649.68 | 898.35 | 1.84 |'
- en: '| MXINT4-16 | 153.87 | 123.53 | 1.25 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| MXINT4-16 | 153.87 | 123.53 | 1.25 |'
- en: 'Table 2: Quantization results for the LLaMA models with different sizes on
    WikiText-2-test, with and without SmoothQuant. Act, Wgt, SQ, and PPL denote activation,
    weight, SmoothQuant, and perplexity, respectively. $\downarrow$: the lower the
    metric, the better the result. We used per-tensor affine quantization for the
    fixed-point formats. LR: Likelihood Ratio of perplexity with SmoothQuant enabled
    over perplexity with SmoothQuant disabled.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同大小的 LLaMA 模型在 WikiText-2-test 上的量化结果，使用和未使用 SmoothQuant。Act、Wgt、SQ 和 PPL
    分别表示激活、权重、SmoothQuant 和困惑度。$\downarrow$：指标越低，结果越好。我们对固定点格式使用了每张量的仿射量化。LR：启用 SmoothQuant
    的困惑度与禁用 SmoothQuant 的困惑度的可能性比率。
- en: '| Model | LLaMA-7B | LLaMA-13B | LLaMA-30B |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | LLaMA-7B | LLaMA-13B | LLaMA-30B |'
- en: '| Act & Wgt | $\downarrow$PPL w/ | LR |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Act & Wgt | $\downarrow$PPL w/ | LR |'
- en: '| format | SQ | SQ | SQ | SQ | SQ | SQ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | SQ | SQ | SQ | SQ | SQ | SQ |'
- en: '| Baseline (FP16) | 5.67 | N/A | N/A | 5.09 | N/A | N/A | 4.10 | N/A | N/A
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 基线（FP16） | 5.67 | 不适用 | 不适用 | 5.09 | 不适用 | 不适用 | 4.10 | 不适用 | 不适用 |'
- en: '| MXINT16-64 | 5.67 | 5.67 | 1.00 | 5.09 | 5.09 | 1.00 | 4.10 | 4.10 | 1.00
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| MXINT16-64 | 5.67 | 5.67 | 1.00 | 5.09 | 5.09 | 1.00 | 4.10 | 4.10 | 1.00
    |'
- en: '| MXINT16-32 | 5.67 | 5.67 | 1.00 | 5.09 | 5.09 | 1.00 | 4.10 | 4.10 | 1.00
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| MXINT16-32 | 5.67 | 5.67 | 1.00 | 5.09 | 5.09 | 1.00 | 4.10 | 4.10 | 1.00
    |'
- en: '| INT-8 | 19.01 | 19.19 | 0.99 | 28.90 | 43.72 | 0.66 | 17.72 | 19.88 | 0.89
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| INT-8 | 19.01 | 19.19 | 0.99 | 28.90 | 43.72 | 0.66 | 17.72 | 19.88 | 0.89
    |'
- en: '| MXINT8-128 | 5.68 | 5.69 | 1.00 | 5.10 | 5.09 | 1.00 | 4.11 | 4.11 | 1.00
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| MXINT8-128 | 5.68 | 5.69 | 1.00 | 5.10 | 5.09 | 1.00 | 4.11 | 4.11 | 1.00
    |'
- en: '| MXINT8-16 | 5.68 | 5.68 | 1.00 | 5.09 | 5.09 | 1.00 | 4.10 | 4.10 | 1.00
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| MXINT8-16 | 5.68 | 5.68 | 1.00 | 5.09 | 5.09 | 1.00 | 4.10 | 4.10 | 1.00
    |'
- en: '| INT-6 | 45202.57 | 58760.27 | 0.77 | 65911.20 | 42067.46 | 1.56 | 27764.73
    | 31869.35 | 0.87 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| INT-6 | 45202.57 | 58760.27 | 0.77 | 65911.20 | 42067.46 | 1.56 | 27764.73
    | 31869.35 | 0.87 |'
- en: '| MXINT6-128 | 5.83 | 5.83 | 1.00 | 5.20 | 5.17 | 1.00 | 4.29 | 4.24 | 1.01
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| MXINT6-128 | 5.83 | 5.83 | 1.00 | 5.20 | 5.17 | 1.00 | 4.29 | 4.24 | 1.01
    |'
- en: '| MXINT6-16 | 5.72 | 5.72 | 1.00 | 5.12 | 5.12 | 1.00 | 4.17 | 4.14 | 1.01
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| MXINT6-16 | 5.72 | 5.72 | 1.00 | 5.12 | 5.12 | 1.00 | 4.17 | 4.14 | 1.01
    |'
- en: '| INT-4 | 185803.03 | 230625.14 | 0.81 | 158380.31 | 160879.97 | 0.98 | 152000.44
    | 173788.06 | 0.87 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| INT-4 | 185803.03 | 230625.14 | 0.81 | 158380.31 | 160879.97 | 0.98 | 152000.44
    | 173788.06 | 0.87 |'
- en: '| MXINT4-128 | 16.35 | 12.87 | 1.27 | 11.69 | 9.60 | 1.22 | 10.49 | 8.06 |
    1.30 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| MXINT4-128 | 16.35 | 12.87 | 1.27 | 11.69 | 9.60 | 1.22 | 10.49 | 8.06 |
    1.30 |'
- en: '| MXINT4-16 | 7.43 | 7.12 | 1.04 | 6.22 | 6.05 | 1.03 | 5.66 | 5.22 | 1.08
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| MXINT4-16 | 7.43 | 7.12 | 1.04 | 6.22 | 6.05 | 1.03 | 5.66 | 5.22 | 1.08
    |'
- en: OPT family.
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: OPT 家族。
- en: To explore the impact of SmoothQuant on the quantization of the OPT family [[11](#bib.bib11)],
    first we reproduce the observation of the *INT-8 quantization cliff phenomenon*
    without applying SmoothQuant [[1](#bib.bib1), [21](#bib.bib21)]. We measure the
    perplexity of OPT models with different sizes when quantized to INT-8 and show
    that with scaling up the model size beyond 6.7B parameters, performance of the
    quantized model catastrophically degrades. This is shown with a cliff in perplexity
    at the 6.7B point of the orange curve (Figure 2). We further verify that introducing
    SmoothQuant rectified this anomaly, resulting in perplexities monotonically decreasing
    with increasing model size [[2](#bib.bib2)].
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探究SmoothQuant对OPT家族量化的影响[[11](#bib.bib11)]，我们首先重现了*INT-8量化悬崖现象*，而未应用SmoothQuant[[1](#bib.bib1),
    [21](#bib.bib21)]。我们测量了不同大小的OPT模型在量化为INT-8时的困惑度，并展示了当模型规模超过6.7B参数时，量化模型的性能会急剧下降。这一点通过橙色曲线在6.7B点的困惑度悬崖（图2）得到了体现。我们进一步验证了引入SmoothQuant纠正了这一异常，使得困惑度随着模型大小的增加而单调下降[[2](#bib.bib2)]。
- en: Next, we ask *whether quantization to MX formats, e.g., MXINT8 and MXINT4, results
    in a similar quantization cliff anomaly?* To answer this question, we quantize
    the same models to the MXINT8 and the MXINT4 formats and show that quantization
    of the models to either of these formats does not result in a similar anomaly;
    see Figure [2](#S4.F2 "Figure 2 ‣ 4 GPTQ ‣ Combining multiple post-training techniques
    to achieve most efficient quantized LLMs"). Moreover, we demonstrate that across
    different model sizes, quantization to MXINT8 maintains the model perplexity while
    more aggressive quantization to MXINT4 penalizes perplexity by 13-44%. Enabling
    SmoothQuant improves performance of the MXINT4 models narrowing the perplexity
    gap between the baseline models and the quantized models to 1-12%. Table [6](#A1.T6
    "Table 6 ‣ A.1 Detailed Quantization Results of the OPT and llama2 Family w/ and
    w/o SmoothQuant ‣ Appendix A Appendix ‣ Combining multiple post-training techniques
    to achieve most efficient quantized LLMs") in the appendix section indicates the
    detailed quantization results for the OPT family.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们询问*量化到MX格式（例如，MXINT8和MXINT4）是否会导致类似的量化悬崖异常？* 为了回答这个问题，我们将相同的模型量化到MXINT8和MXINT4格式，并展示了这些格式的量化不会导致类似的异常；见图[2](#S4.F2
    "Figure 2 ‣ 4 GPTQ ‣ Combining multiple post-training techniques to achieve most
    efficient quantized LLMs")。此外，我们展示了在不同的模型大小下，量化到MXINT8保持了模型的困惑度，而对MXINT4的更激进量化使困惑度增加了13-44%。启用SmoothQuant提高了MXINT4模型的性能，使基准模型与量化模型之间的困惑度差距缩小至1-12%。附录部分的表[6](#A1.T6
    "Table 6 ‣ A.1 Detailed Quantization Results of the OPT and llama2 Family w/ and
    w/o SmoothQuant ‣ Appendix A Appendix ‣ Combining multiple post-training techniques
    to achieve most efficient quantized LLMs")指示了OPT家族的详细量化结果。
- en: DistilGPT2.
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: DistilGPT2。
- en: Table [1](#S5.T1 "Table 1 ‣ 5.2.1 SmoothQuant effect on quantization ‣ 5.2 Results
    ‣ 5 Experiments ‣ Combining multiple post-training techniques to achieve most
    efficient quantized LLMs") illustrates quantization results for DistilGPT2 with
    and without SmoothQuant using different quantization formats. We found that generally,
    aggressive quantization to fewer bits increases perplexity for both the fixed-point
    and the MX formats. However, for a given bit-width, using MX results in better
    perplexity compared to the fixed-point format. Remarkably, for almost all of the
    studied quantization bit-widths and formats, enabling SmoothQuant increases the
    quantization quality, leading to better perplexity. The advantage of enabling
    SmoothQuant is small with larger precisions. Under more restrictive bit-widths
    and larger block sizes, SmoothQuant becomes more advantageous.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表[1](#S5.T1 "Table 1 ‣ 5.2.1 SmoothQuant effect on quantization ‣ 5.2 Results
    ‣ 5 Experiments ‣ Combining multiple post-training techniques to achieve most
    efficient quantized LLMs")展示了使用不同量化格式的DistilGPT2的量化结果，无论是否使用SmoothQuant。我们发现，一般而言，对较少位数的激进量化会增加固定点和MX格式的困惑度。然而，在给定的位宽下，使用MX格式比固定点格式能获得更好的困惑度。值得注意的是，对于几乎所有研究的量化位宽和格式，启用SmoothQuant都能提高量化质量，从而获得更好的困惑度。在较高精度下，启用SmoothQuant的优势较小。在更严格的位宽和更大的块大小下，SmoothQuant变得更加有利。
- en: LLaMA family.
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLaMA家族。
- en: Table [2](#S5.T2 "Table 2 ‣ 5.2.1 SmoothQuant effect on quantization ‣ 5.2 Results
    ‣ 5 Experiments ‣ Combining multiple post-training techniques to achieve most
    efficient quantized LLMs") illustrates perplexity of the quantized *LLaMA* models [[12](#bib.bib12)]
    with three different sizes on WikiText-2-test using various MX and fixed-point
    formats. For all three models, aggressive quantization to small bit-widths significantly
    hurts the model performance, while quantizing to higher bit-widths has negligible
    effect on perplexity. For example, quantizing LLaMA-7B to MXINT16 preserves the
    baseline perplexity of the model regardless of the format block size, while quantizing
    to MXINT4-16 increases perplexity by $31\%$. Finally, for the studied models regardless
    of the quantization format and precision, the advantage of enabling SmoothQuant
    is marginal with larger models and higher quantization precisions and is especially
    prominent for smaller models with more restrictive bit-widths.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [2](#S5.T2 "表 2 ‣ 5.2.1 SmoothQuant 对量化的影响 ‣ 5.2 结果 ‣ 5 实验 ‣ 结合多种后训练技术以实现最有效的量化
    LLMs") 说明了使用不同 MX 和定点格式的三种不同尺寸的量化 *LLaMA* 模型 [[12](#bib.bib12)] 在 WikiText-2-test
    上的困惑度。对于所有三种模型，激进的量化到小位宽显著损害了模型性能，而量化到较高的位宽对困惑度几乎没有影响。例如，将 LLaMA-7B 量化为 MXINT16
    无论格式块大小如何都能保持模型的基线困惑度，而量化为 MXINT4-16 会使困惑度增加 $31\%$。最后，对于研究中的模型，无论量化格式和精度如何，启用
    SmoothQuant 的优势在较大的模型和更高的量化精度下较小，而在位宽更有限的小模型中尤为明显。
- en: Similarly, we assess the impact of SmoothQuant on the quantization of the *llama2*
    family [[22](#bib.bib22)] using various fixed-point and MX formats with different
    precisions. We observe similar trends to those identified in the LLaMA study.
    Detailed results of the experiment can be found in the Table [7](#A1.T7 "Table
    7 ‣ A.1 Detailed Quantization Results of the OPT and llama2 Family w/ and w/o
    SmoothQuant ‣ Appendix A Appendix ‣ Combining multiple post-training techniques
    to achieve most efficient quantized LLMs") of the appendix.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们评估了 SmoothQuant 对 *llama2* 系列 [[22](#bib.bib22)] 在不同精度的固定点和 MX 格式下的量化影响。我们观察到类似于
    LLaMA 研究中识别的趋势。实验的详细结果可以在附录的表格 [7](#A1.T7 "表 7 ‣ A.1 OPT 和 llama2 系列的详细量化结果（有无
    SmoothQuant） ‣ 附录 A 附录 ‣ 结合多种后训练技术以实现最有效的量化 LLMs") 中找到。
- en: 5.2.2 Activation by Activation SmoothQuant
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 激活与激活 SmoothQuant
- en: The previous work [[2](#bib.bib2)] enabled SmoothQuant for the input activations
    of all of the matrix multiplications for a given model, regardless of the type
    of their operands. In this section, we divide the multiplications to two categories
    based on their operand types, and study the necessity of enabling SmoothQuant
    for each category separately. The two categories are a) activation by activation
    multiplications which includes Query $\times$ Value operations of the self-attention
    blocks, and b) activation by weight multiplications which covers all Linear and
    HFConv1D layers of the networks. We refer to the former as *A-A SmoothQuant*,
    and the latter as *A-W SmoothQuant*, in the rest of the manuscript. In the aforementioned
    studies where SmoothQuant significantly improved the performance of the quantized
    models, *we ask whether the improvement was due to A-W or to A-A SmoothQuant,
    or their combination?*
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的工作 [[2](#bib.bib2)] 对给定模型的所有矩阵乘法输入激活启用了 SmoothQuant，无论操作数的类型。在本节中，我们根据操作数的类型将乘法分为两个类别，并分别研究启用
    SmoothQuant 的必要性。这两个类别是 a) 激活与激活乘法，包括自注意力块的 Query $\times$ Value 操作，以及 b) 激活与权重乘法，涵盖网络中的所有
    Linear 和 HFConv1D 层。我们在文中其余部分将前者称为 *A-A SmoothQuant*，将后者称为 *A-W SmoothQuant*。在上述研究中，SmoothQuant
    显著提升了量化模型的性能，*我们问，这种改进是由于 A-W 还是 A-A SmoothQuant，或者是两者的结合？*
- en: 'To answer this question, we performed ablation studies on the two types of
    SmoothQuant, on three models from three different LLM families: DistilGPT2, OPT-6.7B,
    and LLaMA-7B evaluated on WikiText-2-test (Table [3](#S5.T3 "Table 3 ‣ 5.2.2 Activation
    by Activation SmoothQuant ‣ 5.2 Results ‣ 5 Experiments ‣ Combining multiple post-training
    techniques to achieve most efficient quantized LLMs")). We found that for all
    three networks, A-W SmoothQuant is sufficient to lead to the observed accuracy
    improvement, and A-A SmoothQuant does not contribute to the improvement materially.
    Additionally, contrary to previous research [[2](#bib.bib2)], we found that further
    enabling A-A SmoothQuant on top of A-W SmoothQuant significantly degrades the
    performance of LLaMA-7B by $10\%$ when quantized to INT-8.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们对两种SmoothQuant进行了消融研究，使用来自三个不同LLM家族的三种模型：DistilGPT2、OPT-6.7B和LLaMA-7B，在WikiText-2-test上进行评估（表 [3](#S5.T3
    "表3 ‣ 5.2.2 按激活平滑量化 ‣ 5.2 结果 ‣ 5 实验 ‣ 结合多种后训练技术实现最有效的量化LLM")）。我们发现，对于所有三种网络，A-W
    SmoothQuant足以导致观察到的准确性提升，而A-A SmoothQuant在改善效果上没有实质性贡献。此外，与以往的研究[[2](#bib.bib2)]相反，我们发现进一步启用A-A
    SmoothQuant会显著降低LLaMA-7B在INT-8量化时的性能，降幅达到$10\%$。
- en: 'Table 3: Perplexity of the quantized models for DistilGPT2, OPT-6.7B, and LLaMA-7B
    on WikiText-2-test. For INT-8 quantization of activations and weights the per-tensor
    affine scheme is used. Act, Wgt, SQ, and PPL denote activation, weight, SmoothQuant,
    and perplexity, respectively. $\downarrow$: the lower the metric, the better the
    result. For all three models, only enabling A-W SmoothQuant is sufficient to reduce
    perplexity of the quantized models.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：DistilGPT2、OPT-6.7B和LLaMA-7B在WikiText-2-test上的量化模型的困惑度。对于激活和权重的INT-8量化，使用每个张量的仿射方案。Act、Wgt、SQ和PPL分别表示激活、权重、SmoothQuant和困惑度。$\downarrow$：指标越低，结果越好。对于所有三种模型，仅启用A-W
    SmoothQuant就足以减少量化模型的困惑度。
- en: '| Act & Wgt format | A:INT-8 | A:MXINT8 | A:MXINT8 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Act & Wgt格式 | A:INT-8 | A:MXINT8 | A:MXINT8 |'
- en: '| W:INT-8 | W:MXINT8 | W:MXINT4 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| W:INT-8 | W:MXINT8 | W:MXINT4 |'
- en: '| A-A SQ | A-W SQ | DistilGPT2 $\downarrow$PPL (FP32 PPL: 46.06) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| A-A SQ | A-W SQ | DistilGPT2 $\downarrow$PPL (FP32 PPL: 46.06) |'
- en: '| disabled | disabled | 59.23 | 46.37 | 72.97 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 禁用 | 59.23 | 46.37 | 72.97 |'
- en: '| disabled | enabled | 60.57 | 46.23 | 72.93 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 启用 | 60.57 | 46.23 | 72.93 |'
- en: '| enabled | disabled | 59.90 | 46.35 | 77.18 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 禁用 | 59.90 | 46.35 | 77.18 |'
- en: '| enabled | enabled | 60.91 | 46.23 | 77.24 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 启用 | 60.91 | 46.23 | 77.24 |'
- en: '| A-A SQ | A-W SQ | OPT-6.7B $\downarrow$PPL (FP16 PPL: 10.86) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| A-A SQ | A-W SQ | OPT-6.7B $\downarrow$PPL (FP16 PPL: 10.86) |'
- en: '| disabled | disabled | 15.01 | 10.86 | 12.33 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 禁用 | 15.01 | 10.86 | 12.33 |'
- en: '| disabled | enabled | 11.10 | 10.86 | 11.30 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 启用 | 11.10 | 10.86 | 11.30 |'
- en: '| enabled | disabled | 15.01 | 10.86 | 12.35 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 禁用 | 15.01 | 10.86 | 12.35 |'
- en: '| enabled | enabled | 11.17 | 10.86 | 11.29 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 启用 | 11.17 | 10.86 | 11.29 |'
- en: '| A-A SQ | A-W SQ | LLaMA-7B $\downarrow$PPL (FP16 PPL: 5.67) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| A-A SQ | A-W SQ | LLaMA-7B $\downarrow$PPL (FP16 PPL: 5.67) |'
- en: '| disabled | disabled | 19.01 | 5.68 | 6.31 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 禁用 | 19.01 | 5.68 | 6.31 |'
- en: '| disabled | enabled | 17.47 | 5.68 | 6.18 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 启用 | 17.47 | 5.68 | 6.18 |'
- en: '| enabled | disabled | 19.40 | 5.68 | 6.31 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 禁用 | 19.40 | 5.68 | 6.31 |'
- en: '| enabled | enabled | 19.19 | 5.68 | 6.18 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 启用 | 19.19 | 5.68 | 6.18 |'
- en: 5.2.3 SmoothQuant with different fixed-point quantization schemes
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 不同定点量化方案下的SmoothQuant
- en: In this section, we study the effect of different *quantization granularity
    (per-tensor vs per-channel)* and *quantization range (symmetric vs affine)* for
    fixed-point quantization, with and without enabling SmoothQuant.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们研究了不同的*量化粒度（每个张量与每个通道）*和*量化范围（对称与仿射）*对定点量化的影响，及其是否启用SmoothQuant。
- en: Quantization granularity.
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 量化粒度。
- en: Quantization can be done in different granularities. The per-tensor quantization
    uses a single scale and zero-point values to quantize the entire tensor while
    the per-channel quantization enables finer-grained quantization by using different
    scales and zero-point parameters for values associated with each channel of a
    given tensor.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 量化可以在不同的粒度下进行。每个张量的量化使用单一的比例和零点值来量化整个张量，而每个通道的量化则通过为每个张量的每个通道使用不同的比例和零点参数来实现更精细的量化。
- en: Quantization range.
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 量化范围。
- en: Quantization can have a symmetric or asymmetric range. In symmetric quantization
    we assume that the given tensor has the same negative and positive ranges and
    is symmetric around 0, while in affine quantization a zero-point offset is used
    to shift the quantization levels according to the negative and positive ranges
    of the given tensor.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 量化可以有对称或非对称范围。在对称量化中，我们假设给定张量具有相同的负值和正值范围，并且在 0 周围对称，而在仿射量化中，使用零点偏移来根据给定张量的负值和正值范围来调整量化级别。
- en: 'Table 4: Perplexity for DistilGPT2, OPT-6.7B, and LLaMA-7B on WikiText-2-test
    with different quantization schemes. Act, Wgt, SQ, PPL, and Quant. denote activation,
    weight, SmoothQuant, perplexity, and Quantization, respectively. Both activations
    and weights are quantized to INT-8\. Enabling A-W SmoothQuant is only beneficial
    for per-tensor affine calibrations. $\downarrow$: The lower the metric, the better
    the result.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：DistilGPT2、OPT-6.7B 和 LLaMA-7B 在 WikiText-2-test 上不同量化方案的困惑度。Act、Wgt、SQ、PPL
    和 Quant. 分别表示激活、权重、SmoothQuant、困惑度和量化。激活和权重均量化为 INT-8。启用 A-W SmoothQuant 对每张量仿射校准是有益的。$\downarrow$：指标越低，结果越好。
- en: '| Act & Wgt format | INT-8 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 激活与权重格式 | INT-8 |'
- en: '| $\downarrow$PPL w/o & w/ A-W SmoothQuant | w/o SQ | w/ SQ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| $\downarrow$PPL 无 A-W SmoothQuant 与 有 A-W SmoothQuant | 无 SQ | 有 SQ |'
- en: '| Quant. granularity | Quant. range | DistilGPT2 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 量化粒度 | 量化范围 | DistilGPT2 |'
- en: '| per-tensor | symmetric | 92.91 | 228.26 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 每张量 | 对称 | 92.91 | 228.26 |'
- en: '| per-tensor | affine | 59.23 | 60.57 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 每张量 | 仿射 | 59.23 | 60.57 |'
- en: '| per-channel | symmetric | 54.08 | 54.32 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 每通道 | 对称 | 54.08 | 54.32 |'
- en: '| per-channel | affine | 48.13 | 48.33 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 每通道 | 仿射 | 48.13 | 48.33 |'
- en: '| Quant. granularity | Quant. range | OPT-6.7B |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 量化粒度 | 量化范围 | OPT-6.7B |'
- en: '| per-tensor | symmetric | 93.19 | 13222.79 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 每张量 | 对称 | 93.19 | 13222.79 |'
- en: '| per-tensor | affine | 15.01 | 11.10 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 每张量 | 仿射 | 15.01 | 11.10 |'
- en: '| per-channel | symmetric | 12.91 | 13.17 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 每通道 | 对称 | 12.91 | 13.17 |'
- en: '| per-channel | affine | 10.96 | 11.09 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 每通道 | 仿射 | 10.96 | 11.09 |'
- en: '| Quant. granularity | Quant. range | LLaMA-7B |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 量化粒度 | 量化范围 | LLaMA-7B |'
- en: '| per-tensor | symmetric | 38.51 | 23.89 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 每张量 | 对称 | 38.51 | 23.89 |'
- en: '| per-tensor | affine | 19.01 | 17.38 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 每张量 | 仿射 | 19.01 | 17.38 |'
- en: '| per-channel | symmetric | 6.63 | 6.64 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 每通道 | 对称 | 6.63 | 6.64 |'
- en: '| per-channel | affine | 5.93 | 5.92 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 每通道 | 仿射 | 5.93 | 5.92 |'
- en: 'Table [4](#S5.T4 "Table 4 ‣ Quantization range. ‣ 5.2.3 SmoothQuant with different
    fixed-point quantization schemes ‣ 5.2 Results ‣ 5 Experiments ‣ Combining multiple
    post-training techniques to achieve most efficient quantized LLMs") reports quantization
    perplexity for DistilGPT2 on WikiText-2-test with different calibration techniques
    enabling and disabling SmoothQuant. Calibration is done using 128 random input
    sentences from WikiText-2-train. The INT-8 quantization format is used for both
    activation and weights. Based on the results of Section [5.2.2](#S5.SS2.SSS2 "5.2.2
    Activation by Activation SmoothQuant ‣ 5.2 Results ‣ 5 Experiments ‣ Combining
    multiple post-training techniques to achieve most efficient quantized LLMs"),
    with INT-8 quantization only enabling A-W SmoothQuant provides smaller perplexity
    numbers compared to enabling both A-A SmoothQuant and A-W SmoothQuant. Thus, in
    this study only the latter is enabled. We found that: a) generally the affine
    calibration method results in better perplexity compared to its corresponding
    symmetric technique regardless of enabling or disabling SmoothQuant; b) for per-tensor
    affine calibrations, enabling SmoothQuant improves quantization results significantly
    c) in most cases, with per-channel calibrations, enabling SmoothQuant slightly
    degrades perplexity; accordingly, SmoothQuant is not required with the per-channel
    calibration scheme.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [4](#S5.T4 "表 4 ‣ 量化范围 ‣ 5.2.3 不同固定点量化方案的 SmoothQuant ‣ 5.2 结果 ‣ 5 实验 ‣ 结合多种后训练技术实现最有效的量化
    LLM") 报告了 DistilGPT2 在 WikiText-2-test 上不同校准技术下启用和禁用 SmoothQuant 的量化困惑度。校准使用了来自
    WikiText-2-train 的 128 个随机输入句子。激活和权重均采用 INT-8 量化格式。根据第 [5.2.2](#S5.SS2.SSS2 "5.2.2
    激活通过激活 SmoothQuant ‣ 5.2 结果 ‣ 5 实验 ‣ 结合多种后训练技术实现最有效的量化 LLM") 节的结果，仅启用 A-W SmoothQuant
    的 INT-8 量化相比于同时启用 A-A SmoothQuant 和 A-W SmoothQuant 提供了更小的困惑度。因此，本研究中仅启用了后者。我们发现：a)
    一般来说，仿射校准方法的困惑度比其对应的对称技术更好，无论是否启用 SmoothQuant；b) 对于每张量仿射校准，启用 SmoothQuant 显著改善了量化结果；c)
    在大多数情况下，对于每通道校准，启用 SmoothQuant 会稍微降低困惑度；因此，对于每通道校准方案，不需要 SmoothQuant。
- en: 'Table 5: Quantization results for the OPT models with different sizes on WikiText-2-test,
    with enabling/disabling GPTQ and A-W SmoothQuant. A, W, SQ, and PPL denote activation,
    weight, SmoothQuant, and perplexity, respectively. $\downarrow$: the lower the
    metric, the better the result. We used per-tensor affine quantization for the
    INT-8 format.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：对不同大小的OPT模型在WikiText-2-test上的量化结果，启用/禁用GPTQ和A-W SmoothQuant。A、W、SQ和PPL分别表示激活、权重、SmoothQuant和困惑度。$\downarrow$：指标越低，结果越好。我们使用了INT-8格式的每个张量仿射量化。
- en: '| Act & Wgt format | A:INT-8 | A:MXINT8 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 激活 & 权重格式 | A:INT-8 | A:MXINT8 |'
- en: '| W:INT-8 | W:MXINT4 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| W:INT-8 | W:MXINT4 |'
- en: '| GPTQ | SQ | OPT-125M (FP16 $\downarrow$PPL: 27.66) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | SQ | OPT-125M (FP16 $\downarrow$PPL: 27.66) |'
- en: '| disabled | disabled | 37.75 | 39.52 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 禁用 | 37.75 | 39.52 |'
- en: '| disabled | enabled | 36.13 | 35.86 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 启用 | 36.13 | 35.86 |'
- en: '| enabled | disabled | 36.97 | 33.63 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 禁用 | 36.97 | 33.63 |'
- en: '| enabled | enabled | 35.36 | 31.66 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 启用 | 35.36 | 31.66 |'
- en: '| GPTQ | SQ | OPT-1.3B (FP16 $\downarrow$PPL: 14.63) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | SQ | OPT-1.3B (FP16 $\downarrow$PPL: 14.63) |'
- en: '| disabled | disabled | 16.43 | 18.94 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 禁用 | 16.43 | 18.94 |'
- en: '| disabled | enabled | 15.44 | 16.16 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 启用 | 15.44 | 16.16 |'
- en: '| enabled | disabled | 16.40 | 18.00 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 禁用 | 16.40 | 18.00 |'
- en: '| enabled | enabled | 15.41 | 15.52 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 启用 | 15.41 | 15.52 |'
- en: '| GPTQ | SQ | OPT-6.7B (FP16 $\downarrow$PPL: 10.86) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | SQ | OPT-6.7B (FP16 $\downarrow$PPL: 10.86) |'
- en: '| disabled | disabled | 14.92 | 12.19 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 禁用 | 14.92 | 12.19 |'
- en: '| disabled | enabled | 11.09 | 11.20 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 启用 | 11.09 | 11.20 |'
- en: '| enabled | disabled | 14.73 | 11.12 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 禁用 | 14.73 | 11.12 |'
- en: '| enabled | enabled | 11.09 | 11.03 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 启用 | 11.09 | 11.03 |'
- en: '| GPTQ | SQ | OPT-13B (FP16 $\downarrow$PPL: 10.12) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | SQ | OPT-13B (FP16 $\downarrow$PPL: 10.12) |'
- en: '| disabled | disabled | 255.85 | 11.52 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 禁用 | 255.85 | 11.52 |'
- en: '| disabled | enabled | 11.03 | 10.54 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 启用 | 11.03 | 10.54 |'
- en: '| enabled | disabled | 250.77 | 10.28 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 禁用 | 250.77 | 10.28 |'
- en: '| enabled | enabled | 10.98 | 10.31 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 启用 | 10.98 | 10.31 |'
- en: 5.2.4 SmoothQuant and GPTQ Interaction
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4 SmoothQuant和GPTQ的互动
- en: 'GPTQ has been shown to improve quality of models whose weights are quantized
    to lower than 8-bit integer formats [[3](#bib.bib3)]. Since we know from the above
    results that SmoothQuant also improves model accuracies in these modes, a natural
    question arises: *how do SmoothQuant and GPTQ interact when they are applied jointly?*'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 已显示GPTQ可以提高权重量化到低于8位整数格式的模型的质量[[3](#bib.bib3)]。由于我们从上述结果中知道SmoothQuant也能提高这些模式下的模型准确性，一个自然的问题出现了：*当SmoothQuant和GPTQ联合应用时，它们如何互动？*
- en: To answer this question, we conduct experiments on quantization of the OPT,
    and the LLaMA families where SmoothQuant and GPTQ are applied individually, as
    well as jointly to assess their impact on the quantization quality. As shown in
    Section [5.2.1](#S5.SS2.SSS1.Px1 "OPT family. ‣ 5.2.1 SmoothQuant effect on quantization
    ‣ 5.2 Results ‣ 5 Experiments ‣ Combining multiple post-training techniques to
    achieve most efficient quantized LLMs"), for both the OPT and LLaMA models quantization
    to MXINT8 maintains the model perplexity across different model sizes, thus there
    is no need to enable SmoothQuant or GPTQ with MXINT8 quantization. Accordingly,
    in this section we have considered the joint application of SmoothQuant and GPTQ
    only for quantization to MXINT4, and INT-8. Moreover, we showed that SmoothQuant
    is only beneficial with the per-tensor affine scheme (Section [5.2.3](#S5.SS2.SSS3
    "5.2.3 SmoothQuant with different fixed-point quantization schemes ‣ 5.2 Results
    ‣ 5 Experiments ‣ Combining multiple post-training techniques to achieve most
    efficient quantized LLMs")). Thus, in this section for INT-8 quantization of activations
    and weights this method is used.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们对OPT和LLaMA系列模型进行了量化实验，其中单独和联合应用了SmoothQuant和GPTQ，以评估它们对量化质量的影响。如在第[5.2.1](#S5.SS2.SSS1.Px1
    "OPT family. ‣ 5.2.1 SmoothQuant effect on quantization ‣ 5.2 Results ‣ 5 Experiments
    ‣ Combining multiple post-training techniques to achieve most efficient quantized
    LLMs")节所示，对于OPT和LLaMA模型，量化到MXINT8保持了模型的困惑度，因此无需启用SmoothQuant或GPTQ进行MXINT8量化。因此，在本节中，我们仅考虑了SmoothQuant和GPTQ联合应用于MXINT4和INT-8的量化。此外，我们表明SmoothQuant仅在每个张量的仿射方案下才有益（第[5.2.3](#S5.SS2.SSS3
    "5.2.3 SmoothQuant with different fixed-point quantization schemes ‣ 5.2 Results
    ‣ 5 Experiments ‣ Combining multiple post-training techniques to achieve most
    efficient quantized LLMs")节）。因此，在本节中，对激活和权重进行INT-8量化时使用了该方法。
- en: Table [5](#S5.T5 "Table 5 ‣ Quantization range. ‣ 5.2.3 SmoothQuant with different
    fixed-point quantization schemes ‣ 5.2 Results ‣ 5 Experiments ‣ Combining multiple
    post-training techniques to achieve most efficient quantized LLMs") illustrates
    our experiment results for the OPT family. We found that for large models with
    more than 6.7B parameters, enabling SmoothQuant is essential to preserve the perplexity
    of the baseline model, when quantizing to INT-8, and applying GPTQ on top of SmoothQuant
    only improves the performance of the quantized models slightly. With MX quantization,
    the advantageous of jointly enabling SmoothQuant and GPTQ over only applying SmoothQuant
    is small for large models. This becomes more advantageous with smaller models.
    For instance, perplexity of OPT-125M when quantized to MXINT4 is $39.52$.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [5](#S5.T5 "表 5 ‣ 量化范围。 ‣ 5.2.3 不同定点量化方案的 SmoothQuant ‣ 5.2 结果 ‣ 5 实验 ‣ 结合多种后训练技术实现最有效的量化
    LLM") 说明了我们对 OPT 系列的实验结果。我们发现，对于参数超过 6.7B 的大型模型，在量化为 INT-8 时启用 SmoothQuant 对于保持基线模型的困惑度至关重要，而在
    SmoothQuant 基础上应用 GPTQ 仅会略微改善量化模型的性能。对于大型模型，MX 量化下联合启用 SmoothQuant 和 GPTQ 的优势相比于仅应用
    SmoothQuant 较小。这在较小模型中变得更加有利。例如，OPT-125M 量化为 MXINT4 时的困惑度为 $39.52$。
- en: We conducted a similar experiment with DistilGPT2, the LLaMA, and the llama2
    families. In contrast to the findings from the OPT study, for these cases, the
    best results were achieved, except when quantizing llama2-13B to INT-8, with only
    applying GPTQ. However, for the INT-8 quantization of llama2-13B, SmoothQuant
    proved to be the solution to improve the perplexity degradation of the quantized
    model. Additional details on these experiments can be found in the appendix (Tables [8](#A1.T8
    "Table 8 ‣ DistilGPT2\. ‣ A.2 SmoothQuant and GPTQ Interaction for quantization
    of DistilGPT2 and the LLaMA family ‣ Appendix A Appendix ‣ Combining multiple
    post-training techniques to achieve most efficient quantized LLMs") and [9](#A1.T9
    "Table 9 ‣ LLaMA and llama2 family. ‣ A.2 SmoothQuant and GPTQ Interaction for
    quantization of DistilGPT2 and the LLaMA family ‣ Appendix A Appendix ‣ Combining
    multiple post-training techniques to achieve most efficient quantized LLMs")).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 DistilGPT2、LLaMA 和 llama2 系列进行了类似的实验。与 OPT 研究的发现相反，在这些情况下，除非将 llama2-13B
    量化为 INT-8，否则只应用 GPTQ 能获得最佳结果。然而，对于 llama2-13B 的 INT-8 量化，SmoothQuant 证明是改善量化模型困惑度下降的解决方案。有关这些实验的更多细节，请参见附录（表
    [8](#A1.T8 "表 8 ‣ DistilGPT2。 ‣ A.2 SmoothQuant 和 GPTQ 互动在 DistilGPT2 和 LLaMA
    系列量化中的应用 ‣ 附录 A 附录 ‣ 结合多种后训练技术实现最有效的量化 LLM") 和 [9](#A1.T9 "表 9 ‣ LLaMA 和 llama2
    系列。 ‣ A.2 SmoothQuant 和 GPTQ 互动在 DistilGPT2 和 LLaMA 系列量化中的应用 ‣ 附录 A 附录 ‣ 结合多种后训练技术实现最有效的量化
    LLM")）。
- en: '![Refer to caption](img/139548ac85ed521256c5e81a11090477.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/139548ac85ed521256c5e81a11090477.png)'
- en: 'Figure 3: Perplexity for the LLaMA and llama2 families when quantized to MXINT8,
    and MXINT4\. The Y-axis represents perplexity. The X-axis represents model parameter
    size including the additional scale parameters required by the SmoothQuant quantization
    method. Note that the GPTQ algorithm does not introduce any additional model parameters
    during the inference. A, W, and SQ denote activation, weight, and SmoothQuant.
    The points corresponding to the quantized models on Pareto frontiers are indicated
    by a gray circle.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：LLaMA 和 llama2 系列在量化为 MXINT8 和 MXINT4 时的困惑度。Y 轴表示困惑度。X 轴表示模型参数大小，包括 SmoothQuant
    量化方法所需的额外缩放参数。请注意，GPTQ 算法在推理过程中不会引入任何额外的模型参数。A、W 和 SQ 分别表示激活、权重和 SmoothQuant。对应于
    Pareto 前沿的量化模型的点由灰色圆圈表示。
- en: 5.2.5 Pareto frontier Study
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.5 Pareto 前沿研究
- en: The objective of a quantization method is to reduce the model size while preserving
    its accuracy. In the experiments conducted in this study, the concept of the *Pareto
    frontier* becomes relevant in determining the most suitable quantization method
    for each model under a size constraint. A model is deemed to be on the Pareto
    frontier if no other model exists with both a smaller size and lower perplexity.
    Figure [3](#S5.F3 "Figure 3 ‣ 5.2.4 SmoothQuant and GPTQ Interaction ‣ 5.2 Results
    ‣ 5 Experiments ‣ Combining multiple post-training techniques to achieve most
    efficient quantized LLMs") illustrates perplexity of the LLaMA and llama2 families
    on WikiText-2-test as a function of model parameter size. Points corresponding
    to the quantized models on Pareto frontiers are marked with a gray circle. We
    observe that, in general, for aggressive weight quantization to 4-bit (e.g., MXINT4),
    models quantized with GPTQ are positioned on Pareto frontiers while in the case
    of quantization to 6-bit (e.g., MXINT6), models quantized with SmoothQuant are
    found on Pareto frontiers. Lastly, for a more relaxed quantization to 8-bit (e.g.,
    MXINT8), neither GPTQ nor SmoothQuant is deemed necessary.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 量化方法的目标是减少模型大小，同时保持其准确性。在本研究中进行的实验中，*帕累托前沿*的概念在确定每个模型在尺寸约束下最适合的量化方法时变得相关。如果不存在同时具有更小尺寸和更低困惑度的模型，则认为该模型位于帕累托前沿。图[3](#S5.F3
    "图 3 ‣ 5.2.4 SmoothQuant和GPTQ的交互 ‣ 5.2 结果 ‣ 5 实验 ‣ 结合多种后训练技术以实现最有效的量化LLMs")展示了LLaMA和llama2系列在WikiText-2-test上的困惑度与模型参数大小的关系。对应帕累托前沿量化模型的点以灰色圆圈标记。我们观察到，通常情况下，对于4位（例如MXINT4）的激进权重量化，使用GPTQ量化的模型位于帕累托前沿，而对于6位（例如MXINT6）的量化，使用SmoothQuant量化的模型位于帕累托前沿。最后，对于更宽松的8位（例如MXINT8）量化，既不需要GPTQ也不需要SmoothQuant。
- en: Additionally, we explored Pareto frontiers for the OPT family revealing similar
    trends to those observed for the LLaMA and llama2 families. However, for the quantization
    of small OPT models to MXINT4 (e.g., OPT-1.3B and OPT-6.7B), the best perplexity
    is achieved when both GPTQ and SQ are applied jointly. Further details on the
    Pareto frontiers study of the OPT family can be found in Figure [4](#A1.F4 "Figure
    4 ‣ A.3 Pareto frontier study for the OPT family ‣ Appendix A Appendix ‣ Combining
    multiple post-training techniques to achieve most efficient quantized LLMs") in
    the appendix. Note that in none of the aforementioned studies, no fixed-point
    quantization points appear on the Pareto frontiers, indicating that, for the studied
    models and quantization ranges, the MX format is more suitable for quantizing
    the models compared to the fixed-point format with the same bit-width. Moreover,
    we studied the effects of block size granularity on quantization to microscaling
    and fixed-point formats, and found that regardless of the quantization granularity
    for quantization to 6-bit and 8-bit, the majority of points on the Pareto frontier
    are associated with MX data-types, more details is presented in Section [A.4](#A1.SS4
    "A.4 Block size granularity effect on quantization ‣ Appendix A Appendix ‣ Combining
    multiple post-training techniques to achieve most efficient quantized LLMs") of
    appendix.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们探讨了OPT系列的帕累托前沿，揭示了与LLaMA和llama2系列相似的趋势。然而，对于将小型OPT模型量化到MXINT4（例如，OPT-1.3B和OPT-6.7B），当同时应用GPTQ和SQ时，能够达到最佳困惑度。有关OPT系列帕累托前沿研究的更多细节，请参见附录中的图[4](#A1.F4
    "图 4 ‣ A.3 OPT系列的帕累托前沿研究 ‣ 附录A 附录 ‣ 结合多种后训练技术以实现最有效的量化LLMs")。请注意，在上述研究中，帕累托前沿上没有出现定点量化点，这表明对于研究的模型和量化范围，MX格式比具有相同比特宽度的定点格式更适合对模型进行量化。此外，我们研究了块大小粒度对微缩量化和定点格式的影响，发现无论是量化到6位还是8位的粒度如何，帕累托前沿上的大多数点与MX数据类型相关，更多细节请参见附录[A.4](#A1.SS4
    "A.4 块大小粒度对量化的影响 ‣ 附录A 附录 ‣ 结合多种后训练技术以实现最有效的量化LLMs")。
- en: 6 Related Work
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: Model quantization methods.
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型量化方法。
- en: 'Quantization is a technique that lowers the bit precision of deep learning
    models, effectively reducing model size and accelerating inference. There are
    two primary categories of quantization techniques: Quantization-Aware Training
    (QAT), which leverages backpropagation to update quantized weights [[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)], and Post-Training Quantization
    (PTQ), which typically requires no additional training. Quantization-aware training
    methods cannot easily scale up to quantize giant LLMs. Consequently, PTQ methods
    are commonly employed for quantizing LLMs [[27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33)].
    In this work, we studied the interaction of two PTQ methods, GPTQ [[3](#bib.bib3)]
    and SmoothQuant [[2](#bib.bib2)].'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种降低深度学习模型位精度的技术，有效地减少了模型大小并加快了推理速度。量化技术主要分为两类：量化感知训练（QAT），利用反向传播更新量化权重[[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)]，以及后训练量化（PTQ），通常不需要额外的训练。量化感知训练方法无法轻松扩展到量化大型LLMs。因此，PTQ方法通常用于量化LLMs[[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33)]。在这项工作中，我们研究了两种PTQ方法，即GPTQ[[3](#bib.bib3)]和SmoothQuant[[2](#bib.bib2)]的互动。
- en: Large Language Model quantization.
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 大型语言模型量化。
- en: With the recent open-source releases of language models like OPT [[11](#bib.bib11)]
    and LLaMA [[12](#bib.bib12)], researchers are actively working on developing cost-effective
    methods to compress these large networks for inference. Various approaches have
    been suggested to tackle the challenges of quantizing LLMs. ZeroQuant [[19](#bib.bib19)]
    and nuQmm [[34](#bib.bib34)] employ per-token and group-wise quantization schemes
    for LLMs, requiring customized CUDA kernels. ZeroQuant further proposes layer-wise
    knowledge distillation, similar to AdaQuant [[31](#bib.bib31)], but the largest
    evaluated model by both ZeroQuant and nuQmm has 20B parameters. LLM.int8() identifies
    activation outliers in a few feature dimensions as a hindrance to the quantization
    of larger models. To address this issue, it proposes to preserve those dimensions
    in higher precision using a mixed INT8/FP16 decomposition [[1](#bib.bib1)]. However,
    this implementation results in significant latency overhead, sometimes even slower
    than FP16 inference. Similarly, SpQR [[35](#bib.bib35)] and OWQ [[36](#bib.bib36)]
    propose to retain outlier features that are difficult to quantize in full-precision,
    while AWQ [[13](#bib.bib13)] mitigates the quantization error for the outliers
    using grid-searched channel-wise scaling. Additionally, Outlier Suppression [[9](#bib.bib9)]
    tackles activation outliers by utilizing non-scaling LayerNorm and token-wise
    clipping. Despite its success with smaller language models such as BERT [[37](#bib.bib37)]
    and BART [[38](#bib.bib38)], it falls short in maintaining accuracy for larger
    LLMs, while SmoothQuant and GPTQ both preserve the performance of LLMs up to 175B
    parameters [[2](#bib.bib2), [3](#bib.bib3)]. Lee et al., explored the combined
    use of GPTQ and SmoothQuant for quantizing LLMs, focusing solely on fixed-point
    data types in their study [[39](#bib.bib39)]. Trukhanov et al., proposed a technique
    for quantizing KV-cache to low-precision Block Floating-Point (BFP) formats without
    compromising the resulting model accuracy [[40](#bib.bib40)].
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 随着OPT[[11](#bib.bib11)]和LLaMA[[12](#bib.bib12)]等语言模型的近期开源发布，研究人员正积极开发具有成本效益的方法来压缩这些大型网络以用于推理。已经提出了各种方法来应对量化LLMs的挑战。ZeroQuant[[19](#bib.bib19)]和nuQmm[[34](#bib.bib34)]采用每个token和分组量化方案进行LLMs量化，需自定义CUDA内核。ZeroQuant进一步提出层级知识蒸馏，类似于AdaQuant[[31](#bib.bib31)]，但ZeroQuant和nuQmm评估的最大模型都有20B参数。LLM.int8()将激活异常值识别为量化大型模型的障碍。为解决此问题，提出使用混合INT8/FP16分解来保留那些维度的较高精度[[1](#bib.bib1)]。然而，这种实现导致了显著的延迟开销，有时甚至比FP16推理更慢。类似地，SpQR[[35](#bib.bib35)]和OWQ[[36](#bib.bib36)]提出保留难以量化的全精度异常特征，而AWQ[[13](#bib.bib13)]通过网格搜索的通道级缩放来减轻异常值的量化误差。此外，异常值抑制[[9](#bib.bib9)]通过利用非缩放的LayerNorm和token级裁剪来处理激活异常值。尽管在较小的语言模型如BERT[[37](#bib.bib37)]和BART[[38](#bib.bib38)]上取得了成功，但在保持较大LLMs的准确性方面仍有不足，而SmoothQuant和GPTQ都能保留高达175B参数的LLMs的性能[[2](#bib.bib2),
    [3](#bib.bib3)]。Lee等人探讨了GPTQ和SmoothQuant的联合使用来量化LLMs，并专注于其研究中的定点数据类型[[39](#bib.bib39)]。Trukhanov等人提出了一种将KV-cache量化为低精度块浮点（BFP）格式的技术，而不影响模型的准确性[[40](#bib.bib40)]。
- en: 7 Conclusion
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: To summarize, our study showed that the optimality of decisions on whether to
    combine the GPTQ and the SmoothQuant techniques depends on factors such as numerical
    precision, data type, model family, and model size. SmoothQuant can be effective
    for quantizing both activations and weights in large language models using different
    fixed-point and MX formats. This benefit is particularly pronounced within the
    quantization formats ranging from 6 to 8 bits. Conversely, GPTQ demonstrated superior
    effectiveness in scenarios involving more aggressive weight quantization, particularly
    to 4 bits and 6 bits. Notably, our findings indicated that for quantization to
    MXINT8, neither GPTQ nor SmoothQuant is necessary to preserve the baseline accuracy.
    We demonstrated that quantizations using different MX formats deliver better perplexity
    compared to fixed-point formats with the same bit-width when the per-tensor quantization
    scheme is employed.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的研究表明，是否结合GPTQ和SmoothQuant技术的决策最佳性取决于数值精度、数据类型、模型家族和模型大小等因素。SmoothQuant在使用不同的定点和MX格式对大型语言模型的激活和权重进行量化时可能很有效。这种好处在6到8位的量化格式中尤为明显。相反，GPTQ在涉及更激进的权重量化，特别是4位和6位时表现出更优越的效果。值得注意的是，我们的发现表明，对于MXINT8的量化，无论是GPTQ还是SmoothQuant都不需要保持基线准确度。我们证明了，当采用每张量量化方案时，使用不同MX格式的量化比使用相同位宽的定点格式提供了更好的困惑度。
- en: Additionally, contrary to the results of prior research [[2](#bib.bib2)], we
    illustrated that when applying SmoothQuant, it suffices to apply only the A-W
    SmoothQuant, as opposed to the original findings that recommended both A-W and
    A-A approaches. Throughout the paper, we have shown that by utilizing GPTQ and
    A-W SmoothQuant, and quantizing models to MX formats, we can significantly reduce
    the size of OPT models by up to $4\times$ with negligible perplexity degradation.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与先前研究[[2](#bib.bib2)]的结果相反，我们说明了在应用SmoothQuant时，仅应用A-W SmoothQuant就足够，而不是原先推荐的A-W和A-A两种方法。在本文中，我们展示了通过利用GPTQ和A-W
    SmoothQuant，并将模型量化为MX格式，我们可以显著减少OPT模型的大小，减少高达$4\times$，且几乎不会出现困惑度退化。
- en: References
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, “Llm. int8 (): 8-bit
    matrix multiplication for transformers at scale,” arXiv preprint arXiv:2208.07339,
    2022.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] T. Dettmers, M. Lewis, Y. Belkada 和 L. Zettlemoyer，“LLM. int8 (): 大规模变换器的8位矩阵乘法”，arXiv预印本
    arXiv:2208.07339，2022年。'
- en: '[2] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, “Smoothquant:
    Accurate and efficient post-training quantization for large language models,”
    in International Conference on Machine Learning, pp. 38087–38099, PMLR, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth 和 S. Han，"Smoothquant：大型语言模型的准确高效后训练量化"，《国际机器学习会议》，第38087–38099页，PMLR，2023年。'
- en: '[3] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, “GPTQ: Accurate post-training
    quantization for generative pre-trained transformers,” arXiv preprint arXiv:2210.17323,
    2022.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] E. Frantar, S. Ashkboos, T. Hoefler 和 D. Alistarh，“GPTQ：生成预训练变换器的准确后训练量化”，arXiv预印本
    arXiv:2210.17323，2022年。'
- en: '[4] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled version
    of bert: smaller, faster, cheaper and lighter,” in NeurIPS EMC Workshop, 2019.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] V. Sanh, L. Debut, J. Chaumond 和 T. Wolf，"Distilbert，BERT的蒸馏版本：更小、更快、更便宜、更轻"，《NeurIPS
    EMC Workshop》，2019年。'
- en: '[5] B. Darvish Rouhani, R. Zhao, V. Elango, R. Shafipour, M. Hall, M. Mesmakhosroshahi,
    A. More, L. Melnick, M. Golub, G. Varatkar, et al., “With shared microexponents,
    a little shifting goes a long way,” in Proceedings of the 50th Annual International
    Symposium on Computer Architecture, pp. 1–13, 2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] B. Darvish Rouhani, R. Zhao, V. Elango, R. Shafipour, M. Hall, M. Mesmakhosroshahi,
    A. More, L. Melnick, M. Golub, G. Varatkar 等，“通过共享微指数，一点点偏移带来长远的好处”，《第50届国际计算机架构年会论文集》，第1–13页，2023年。'
- en: '[6] B. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary,
    M. Cornea, E. Dellinger, K. Denolf, et al., “Microscaling data formats for deep
    learning,” arXiv preprint arXiv:2310.10537, 2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] B. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary,
    M. Cornea, E. Dellinger, K. Denolf 等，“深度学习的微缩数据格式”，arXiv预印本 arXiv:2310.10537，2023年。'
- en: '[7] B. Darvish Rouhani, D. Lo, R. Zhao, M. Liu, J. Fowers, K. Ovtcharov, A. Vinogradsky,
    S. Massengill, L. Yang, R. Bittner, et al., “Pushing the limits of narrow precision
    inferencing at cloud scale with microsoft floating point,” Advances in neural
    information processing systems, vol. 33, pp. 10271–10281, 2020.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] B. Darvish Rouhani, D. Lo, R. Zhao, M. Liu, J. Fowers, K. Ovtcharov, A.
    Vinogradsky, S. Massengill, L. Yang, R. Bittner 等，“在云规模下推动狭窄精度推理的极限，使用微软浮点”，《神经信息处理系统进展》，第33卷，第10271–10281页，2020。'
- en: '[8] Qualcomm, “Qualcomm Cloud AI 100 Accelerator.” [https://www.qualcomm.com/developer/blog/2024/01/qualcomm-cloud-ai-100-accelerates-large-language-model-inference-2x-using-microscaling-mx](https://www.qualcomm.com/developer/blog/2024/01/qualcomm-cloud-ai-100-accelerates-large-language-model-inference-2x-using-microscaling-mx),
    20234.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 高通，“Qualcomm Cloud AI 100 加速器。” [https://www.qualcomm.com/developer/blog/2024/01/qualcomm-cloud-ai-100-accelerates-large-language-model-inference-2x-using-microscaling-mx](https://www.qualcomm.com/developer/blog/2024/01/qualcomm-cloud-ai-100-accelerates-large-language-model-inference-2x-using-microscaling-mx)，20234。'
- en: '[9] X. Wei, Y. Zhang, X. Zhang, R. Gong, S. Zhang, Q. Zhang, F. Yu, and X. Liu,
    “Outlier suppression: Pushing the limit of low-bit transformer language models,”
    Advances in Neural Information Processing Systems, vol. 35, pp. 17402–17414, 2022.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] X. Wei, Y. Zhang, X. Zhang, R. Gong, S. Zhang, Q. Zhang, F. Yu, 和 X. Liu，“离群点抑制：推动低比特转换器语言模型的极限”，《神经信息处理系统进展》，第35卷，第17402–17414页，2022。'
- en: '[10] Y. Bondarenko, M. Nagel, and T. Blankevoort, “Understanding and overcoming
    the challenges of efficient transformer quantization,” arXiv preprint arXiv:2109.12948,
    2021.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Y. Bondarenko, M. Nagel, 和 T. Blankevoort，“理解并克服高效转换器量化的挑战”，arXiv 预印本
    arXiv:2109.12948，2021。'
- en: '[11] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin, et al., “OPT: Open pre-trained transformer language
    models,” arXiv preprint arXiv:2205.01068, 2022.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin 等，“OPT: 开放预训练转换器语言模型”，arXiv 预印本 arXiv:2205.01068，2022。'
- en: '[12] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., “LLaMA: Open and efficient
    foundation language models,” arXiv preprint arXiv:2302.13971, 2023.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar 等，“LLaMA: 开放且高效的基础语言模型”，arXiv 预印本 arXiv:2302.13971，2023。'
- en: '[13] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, “AWQ: Activation-aware
    weight quantization for llm compression and acceleration,” arXiv preprint arXiv:2306.00978,
    2023.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, 和 S. Han，“AWQ: 激活感知权重量化用于
    LLM 压缩和加速”，arXiv 预印本 arXiv:2306.00978，2023。'
- en: '[14] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,
    and T. B. Hashimoto, “Alpaca: A strong, replicable instruction-following model,”
    Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca.
    html, vol. 3, no. 6, p. 7, 2023.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,
    和 T. B. Hashimoto，“Alpaca: 一个强大且可复现的指令跟随模型”，斯坦福基础模型研究中心。 [https://crfm.stanford.edu/2023/03/13/alpaca.html](https://crfm.stanford.edu/2023/03/13/alpaca.html)，第3卷，第6期，第7页，2023。'
- en: '[15] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang,
    Y. Zhuang, J. E. Gonzalez, et al., “Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality,” See https://vicuna. lmsys. org (accessed 14
    April 2023), 2023.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang,
    Y. Zhuang, J. E. Gonzalez 等，“Vicuna: 一个开源聊天机器人，展示了**90%**的*chatgpt*质量”，见 [https://vicuna.lmsys.org](https://vicuna.lmsys.org)（访问日期：2023年4月14日），2023。'
- en: '[16] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “QLoRA: Efficient
    finetuning of quantized llms,” arXiv preprint arXiv:2305.14314, 2023.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] T. Dettmers, A. Pagnoni, A. Holtzman, 和 L. Zettlemoyer，“QLoRA: 高效微调量化
    LLM”，arXiv 预印本 arXiv:2305.14314，2023。'
- en: '[17] Stability AI, “Stable Beluga.” [https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models](https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models),
    2023.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Stability AI，“Stable Beluga。” [https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models](https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models)，2023。'
- en: '[18] T. Dettmers and L. Zettlemoyer, “The case for 4-bit precision: k-bit inference
    scaling laws,” in International Conference on Machine Learning, pp. 7750–7774,
    PMLR, 2023.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] T. Dettmers 和 L. Zettlemoyer，“支持4位精度的理由：k位推理缩放定律”，在《国际机器学习会议》中，第7750–7774页，PMLR，2023。'
- en: '[19] Z. Yao, R. Yazdani Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He, “Zeroquant:
    Efficient and affordable post-training quantization for large-scale transformers,”
    Advances in Neural Information Processing Systems, vol. 35, pp. 27168–27183, 2022.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Z. Yao, R. Yazdani Aminabadi, M. Zhang, X. Wu, C. Li, 和 Y. He，“Zeroquant：大规模变压器的高效且实惠的训练后量化，”《神经信息处理系统进展》，第35卷，第27168–27183页，2022年。'
- en: '[20] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel mixture
    models,” arXiv preprint arXiv:1609.07843, 2016.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] S. Merity, C. Xiong, J. Bradbury, 和 R. Socher，“指针哨兵混合模型，”arXiv预印本 arXiv:1609.07843，2016年。'
- en: '[21] A. Ahmadian, S. Dash, H. Chen, B. Venkitesh, S. Gou, P. Blunsom, A. Üstün,
    and S. Hooker, “Intriguing properties of quantization at scale,” arXiv preprint
    arXiv:2305.19268, 2023.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] A. Ahmadian, S. Dash, H. Chen, B. Venkitesh, S. Gou, P. Blunsom, A. Üstün,
    和 S. Hooker，“规模化量化的有趣属性，”arXiv预印本 arXiv:2305.19268，2023年。'
- en: '[22] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale, et al., “Llama 2: Open foundation and fine-tuned
    chat models,” arXiv preprint arXiv:2307.09288, 2023.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale，等，“Llama 2：开放基础和微调聊天模型，”arXiv预印本
    arXiv:2307.09288，2023年。'
- en: '[23] Y. Bengio, N. Léonard, and A. Courville, “Estimating or propagating gradients
    through stochastic neurons for conditional computation,” arXiv preprint arXiv:1308.3432,
    2013.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Y. Bengio, N. Léonard, 和 A. Courville，“通过随机神经元估计或传播梯度以实现条件计算，”arXiv预印本
    arXiv:1308.3432，2013年。'
- en: '[24] J. Choi, Z. Wang, S. Venkataramani, P. I.-J. Chuang, V. Srinivasan, and
    K. Gopalakrishnan, “Pact: Parameterized clipping activation for quantized neural
    networks,” arXiv preprint arXiv:1805.06085, 2018.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Choi, Z. Wang, S. Venkataramani, P. I.-J. Chuang, V. Srinivasan, 和
    K. Gopalakrishnan，“Pact：量化神经网络的参数化裁剪激活，”arXiv预印本 arXiv:1805.06085，2018年。'
- en: '[25] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. Van Baalen, and
    T. Blankevoort, “A white paper on neural network quantization,” arXiv preprint
    arXiv:2106.08295, 2021.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. Van Baalen, 和
    T. Blankevoort，“神经网络量化白皮书，”arXiv预印本 arXiv:2106.08295，2021年。'
- en: '[26] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, “A
    survey of quantization methods for efficient neural network inference,” in Low-Power
    Computer Vision, pp. 291–326, Chapman and Hall/CRC, 2022.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, 和 K. Keutzer，“高效神经网络推理的量化方法综述，”发表于《低功耗计算机视觉》，第291–326页，Chapman
    and Hall/CRC，2022年。'
- en: '[27] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and
    D. Kalenichenko, “Quantization and training of neural networks for efficient integer-arithmetic-only
    inference,” in Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp. 2704–2713, 2018.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, 和 D.
    Kalenichenko，“量化和训练神经网络以实现高效的整数算术推理，”发表于IEEE计算机视觉与模式识别会议论文集，第2704–2713页，2018年。'
- en: '[28] M. Nagel, M. v. Baalen, T. Blankevoort, and M. Welling, “Data-free quantization
    through weight equalization and bias correction,” in Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pp. 1325–1334, 2019.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] M. Nagel, M. v. Baalen, T. Blankevoort, 和 M. Welling，“通过权重均衡和偏差修正进行无数据量化，”发表于IEEE/CVF国际计算机视觉会议论文集，第1325–1334页，2019年。'
- en: '[29] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort,
    “Up or down? adaptive rounding for post-training quantization,” in International
    Conference on Machine Learning, pp. 7197–7206, PMLR, 2020.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, 和 T. Blankevoort，“向上还是向下？适应性舍入用于训练后的量化，”发表于国际机器学习会议论文集，第7197–7206页，PMLR，2020年。'
- en: '[30] P. Wang, Q. Chen, X. He, and J. Cheng, “Towards accurate post-training
    network quantization via bit-split and stitching,” in International Conference
    on Machine Learning, pp. 9847–9856, PMLR, 2020.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] P. Wang, Q. Chen, X. He, 和 J. Cheng，“通过位拆分和拼接实现准确的后训练网络量化，”发表于国际机器学习会议论文集，第9847–9856页，PMLR，2020年。'
- en: '[31] I. Hubara, Y. Nahshan, Y. Hanani, R. Banner, and D. Soudry, “Accurate
    post training quantization with small calibration sets,” in International Conference
    on Machine Learning, pp. 4466–4475, PMLR, 2021.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] I. Hubara, Y. Nahshan, Y. Hanani, R. Banner, 和 D. Soudry，“准确的后训练量化与小型校准集，”发表于国际机器学习会议论文集，第4466–4475页，PMLR，2021年。'
- en: '[32] Y. Li, R. Gong, X. Tan, Y. Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, and
    S. Gu, “Brecq: Pushing the limit of post-training quantization by block reconstruction,”
    arXiv preprint arXiv:2102.05426, 2021.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Y. Li, R. Gong, X. Tan, Y. Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, 和 S.
    Gu，“Brecq：通过块重建推动训练后量化的极限，”arXiv预印本 arXiv:2102.05426，2021年。'
- en: '[33] Z. Deng, X. Wang, S. Sharify, and M. Orshansky, “Mixed-precision quantization
    with cross-layer dependencies,” arXiv preprint arXiv:2307.05657, 2023.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Z. Deng, X. Wang, S. Sharify, 和 M. Orshansky，“具有跨层依赖的混合精度量化，”arXiv 预印本
    arXiv:2307.05657, 2023。'
- en: '[34] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee, “nuqmm: Quantized
    matmul for efficient inference of large-scale generative language models,” arXiv
    preprint arXiv:2206.09557, 2022.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, 和 D. Lee，“nuqmm: 用于大规模生成语言模型高效推理的量化矩阵乘法，”arXiv
    预印本 arXiv:2206.09557, 2022。'
- en: '[35] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar,
    S. Ashkboos, A. Borzunov, T. Hoefler, and D. Alistarh, “SpQR: A sparse-quantized
    representation for near-lossless llm weight compression,” arXiv preprint arXiv:2306.03078,
    2023.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar,
    S. Ashkboos, A. Borzunov, T. Hoefler, 和 D. Alistarh，“SpQR: 一种用于近乎无损 LLM 权重压缩的稀疏量化表示，”arXiv
    预印本 arXiv:2306.03078, 2023。'
- en: '[36] C. Lee, J. Jin, T. Kim, H. Kim, and E. Park, “OWQ: Outlier-aware weight
    quantization for efficient fine-tuning and inference of large language models,”
    in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, pp. 13355–13364,
    2024.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] C. Lee, J. Jin, T. Kim, H. Kim, 和 E. Park，“OWQ: 针对大规模语言模型高效微调和推理的异常值感知权重量化，”在
    AAAI 人工智能会议论文集中，第 38 卷，pp. 13355–13364, 2024。'
- en: '[37] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
    of deep bidirectional transformers for language understanding,” arXiv preprint
    arXiv:1810.04805, 2018.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova，“BERT: 深度双向变换器的语言理解预训练，”arXiv
    预印本 arXiv:1810.04805, 2018。'
- en: '[38] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov,
    and L. Zettlemoyer, “BART: Denoising sequence-to-sequence pre-training for natural
    language generation, translation, and comprehension,” arXiv preprint arXiv:1910.13461,
    2019.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V.
    Stoyanov, 和 L. Zettlemoyer，“BART: 用于自然语言生成、翻译和理解的去噪序列到序列预训练，”arXiv 预印本 arXiv:1910.13461,
    2019。'
- en: '[39] J. Lee, M. Kim, S. Baek, S. Hwang, W. Sung, and J. Choi, “Enhancing computation
    efficiency in large language models through weight and activation quantization,”
    in Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing, pp. 14726–14739, 2023.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] J. Lee, M. Kim, S. Baek, S. Hwang, W. Sung, 和 J. Choi，“通过权重和激活量化提升大型语言模型的计算效率，”在
    2023 年自然语言处理实证方法会议论文集中，pp. 14726–14739, 2023。'
- en: '[40] N. Trukhanov and I. Soloveychik, “Accurate block quantization in LLMs
    with outliers,” arXiv preprint arXiv:2403.20137, 2024.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] N. Trukhanov 和 I. Soloveychik，“具有异常值的 LLM 的精确块量化，”arXiv 预印本 arXiv:2403.20137,
    2024。'
- en: Appendix A Appendix
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Detailed Quantization Results of the OPT and llama2 Family w/ and w/o SmoothQuant
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 OPT 和 llama2 系列的详细量化结果（含与不含 SmoothQuant）
- en: 'Tables [6](#A1.T6 "Table 6 ‣ A.1 Detailed Quantization Results of the OPT and
    llama2 Family w/ and w/o SmoothQuant ‣ Appendix A Appendix ‣ Combining multiple
    post-training techniques to achieve most efficient quantized LLMs") and [7](#A1.T7
    "Table 7 ‣ A.1 Detailed Quantization Results of the OPT and llama2 Family w/ and
    w/o SmoothQuant ‣ Appendix A Appendix ‣ Combining multiple post-training techniques
    to achieve most efficient quantized LLMs") illustrate the perplexity of the OPT
    and llama2 family on WikiText-2-test, when quantized to various fixed-point and
    MX formats. Based on the experimental results of both families, we observe that:
    a) Aggressive quantization to small bit-widths significantly impairs model performance,
    whereas quantizing to higher bit-widths has a negligible effect on perplexity.
    b) Quantization results using different MXINT formats yield better perplexity
    compared to fixed-point formats with the same bit-width. c) Among MXINT formats
    with the same precision, enabling SmoothQuant is more advantageous when quantizing
    to formats with larger block sizes than to formats with smaller block sizes. d)
    SmoothQuant is effective with lower than 8-bit MXINT formats but not with 8-bit
    MXINT formats. e) Finally, for the studied models regardless of the quantization
    format and precision, the benefits of enabling SmoothQuant are marginal for larger
    models and higher quantization precisions, but notably pronounced for smaller
    models with more constrained bit-widths.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表[6](#A1.T6 "表6 ‣ A.1 OPT和llama2系列的详细量化结果，使用和不使用SmoothQuant ‣ 附录A 附录 ‣ 结合多种后训练技术以实现最有效的量化LLMs")和[7](#A1.T7
    "表7 ‣ A.1 OPT和llama2系列的详细量化结果，使用和不使用SmoothQuant ‣ 附录A 附录 ‣ 结合多种后训练技术以实现最有效的量化LLMs")展示了OPT和llama2系列在WikiText-2-test上的困惑度，当量化到各种固定点和MX格式时。根据这两个系列的实验结果，我们观察到：a)
    激进的量化到较小的比特宽度会显著影响模型性能，而量化到较高的比特宽度对困惑度的影响微乎其微。b) 使用不同MXINT格式的量化结果比使用相同比特宽度的固定点格式获得更好的困惑度。c)
    在具有相同精度的MXINT格式中，启用SmoothQuant在量化到较大块大小的格式时比在较小块大小的格式时更有优势。d) SmoothQuant在低于8位的MXINT格式中有效，但在8位的MXINT格式中无效。e)
    最后，对于所研究的模型，不论量化格式和精度如何，启用SmoothQuant的好处在较大模型和较高量化精度中边际效益不大，但在比特宽度更受限的小模型中尤为明显。
- en: 'Table 6: Quantization results for the OPT models with different sizes on WikiText-2-test,
    with and without SmoothQuant. Act, Wgt, SQ, and PPL denote activation, weight,
    SmoothQuant, and perplexity, respectively. $\downarrow$: The vocabulary size for
    WikiText-2 is around 50000; a perplexity number larger than 50000 is most likely
    due to a numerical issue. LR: Likelihood Ratio of PPL with SmoothQuant enabled
    over PPL with SmoothQuant disabled.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：OPT模型在WikiText-2-test上，使用和不使用SmoothQuant的量化结果。Act、Wgt、SQ和PPL分别表示激活、权重、SmoothQuant和困惑度。$\downarrow$：WikiText-2的词汇量约为50000；困惑度大于50000可能是由于数值问题。LR：启用SmoothQuant时的困惑度与禁用SmoothQuant时的困惑度的似然比。
- en: '| Model | OPT-1.3B | OPT-6.7B | OPT-30B |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | OPT-1.3B | OPT-6.7B | OPT-30B |'
- en: '| Act & Wgt | $\downarrow$PPL w/ | LR |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Act & Wgt | $\downarrow$PPL w/ | LR |'
- en: '| format | SQ | SQ | SQ | SQ | SQ | SQ |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | SQ | SQ | SQ | SQ | SQ | SQ |'
- en: '| Baseline (FP16) | 14.63 | N/A | N/A | 10.86 | N/A | N/A | 9.56 | N/A | N/A
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 基线（FP16） | 14.63 | N/A | N/A | 10.86 | N/A | N/A | 9.56 | N/A | N/A |'
- en: '| MXINT16-64 | 14.63 | 14.62 | 1.00 | 10.86 | 10.86 | 1.00 | 9.56 | 9.56 |
    1.00 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| MXINT16-64 | 14.63 | 14.62 | 1.00 | 10.86 | 10.86 | 1.00 | 9.56 | 9.56 |
    1.00 |'
- en: '| MXINT16-32 | 14.62 | 14.62 | 1.00 | 10.86 | 10.86 | 1.00 | 9.56 | 9.56 |
    1.00 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| MXINT16-32 | 14.62 | 14.62 | 1.00 | 10.86 | 10.86 | 1.00 | 9.56 | 9.56 |
    1.00 |'
- en: '| INT-8 | 16.48 | 15.44 | 1.07 | 15.01 | 11.17 | 1.34 | 315.83 | 9.79 | 32.26
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| INT-8 | 16.48 | 15.44 | 1.07 | 15.01 | 11.17 | 1.34 | 315.83 | 9.79 | 32.26
    |'
- en: '| MXINT8-128 | 17.21 | 14.63 | 1.18 | 10.86 | 10.86 | 1.00 | 9.56 | 9.56 |
    1.00 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| MXINT8-128 | 17.21 | 14.63 | 1.18 | 10.86 | 10.86 | 1.00 | 9.56 | 9.56 |
    1.00 |'
- en: '| MXINT8-64 | 14.81 | 14.63 | 1.01 | 10.86 | 10.86 | 1.00 | 9.56 | 9.56 | 1.00
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| MXINT8-64 | 14.81 | 14.63 | 1.01 | 10.86 | 10.86 | 1.00 | 9.56 | 9.56 | 1.00
    |'
- en: '| MXINT8-32 | 14.64 | 14.63 | 1.00 | 10.86 | 10.86 | 1.00 | 9.57 | 9.56 | 1.00
    |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| MXINT8-32 | 14.64 | 14.63 | 1.00 | 10.86 | 10.86 | 1.00 | 9.57 | 9.56 | 1.00
    |'
- en: '| MXINT8-16 | 14.63 | 14.63 | 1.00 | 10.87 | 10.86 | 1.00 | 9.58 | 9.56 | 1.00
    |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| MXINT8-16 | 14.63 | 14.63 | 1.00 | 10.87 | 10.86 | 1.00 | 9.58 | 9.56 | 1.00
    |'
- en: '| INT-6 | 4421.20 | 291.79 | 15.15 | 12392.76 | 121.52 | 101.98 | 11415.95
    | 1131.71 | 10.09 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| INT-6 | 4421.20 | 291.79 | 15.15 | 12392.76 | 121.52 | 101.98 | 11415.95
    | 1131.71 | 10.09 |'
- en: '| MXINT6-128 | 25.40 | 24.64 | 1.03 | 11.02 | 10.98 | 1.00 | 9.60 | 9.52 |
    1.01 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| MXINT6-128 | 25.40 | 24.64 | 1.03 | 11.02 | 10.98 | 1.00 | 9.60 | 9.52 |
    1.01 |'
- en: '| MXINT6-16 | 15.07 | 14.92 | 1.01 | 10.94 | 10.90 | 1.00 | 9.58 | 9.56 | 1.00
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| MXINT6-16 | 15.07 | 14.92 | 1.01 | 10.94 | 10.90 | 1.00 | 9.58 | 9.56 | 1.00
    |'
- en: '| INT-4 | 83368.28 | 37182.13 | 2.24 | 13645.72 | 12064.92 | 1.13 | 34580.63
    | 125530.61${}^{\texttt{+}}$ | 0.28 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| INT-4 | 83368.28 | 37182.13 | 2.24 | 13645.72 | 12064.92 | 1.13 | 34580.63
    | 125530.61${}^{\texttt{+}}$ | 0.28 |'
- en: '| MXINT4-128 | 2862.62 | 55.25 | 51.81 | 32.85 | 17.27 | 1.90 | 17.28 | 11.07
    | 1.56 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| MXINT4-128 | 2862.62 | 55.25 | 51.81 | 32.85 | 17.27 | 1.90 | 17.28 | 11.07
    | 1.56 |'
- en: '| MXINT4-16 | 25.04 | 22.76 | 1.10 | 13.82 | 14.26 | 0.97 | 10.60 | 10.47 |
    1.01 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| MXINT4-16 | 25.04 | 22.76 | 1.10 | 13.82 | 14.26 | 0.97 | 10.60 | 10.47 |
    1.01 |'
- en: 'Table 7: Quantization results for the llama2-7B and llama2-13B models on WikiText-2-test,
    with and without SmoothQuant. Act, Wgt, SQ, and PPL denote activation, weight,
    SmoothQuant, and perplexity, respectively. $\downarrow$: the lower the metric,
    the better the result. We used per-tensor affine quantization for the fixed-point
    formats. LR: Likelihood Ratio of PPL with SmoothQuant enabled over PPL with SmoothQuant
    disabled.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：llama2-7B 和 llama2-13B 模型在 WikiText-2-test 上的量化结果，启用和禁用 SmoothQuant。激活、权重、SQ
    和 PPL 分别表示激活、权重、SmoothQuant 和困惑度。$\downarrow$：指标越低，结果越好。我们对定点格式使用了每张量仿射量化。LR：SmoothQuant
    启用时 PPL 与 SmoothQuant 禁用时 PPL 的似然比。
- en: '| Model | llama2-7B | llama2-13B |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | llama2-7B | llama2-13B |'
- en: '| Act & Wgt | $\downarrow$PPL w/ | LR |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 激活与权重 | $\downarrow$PPL 启用 | LR |'
- en: '| format | SQ | SQ | SQ | SQ |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | SQ | SQ | SQ | SQ |'
- en: '| Baseline (FP16) | 5.11 | N/A | N/A | 4.57 | N/A | N/A |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 基线 (FP16) | 5.11 | 不适用 | 不适用 | 4.57 | 不适用 | 不适用 |'
- en: '| MXINT16-64 | 5.11 | 5.11 | 1.00 | 4.57 | 4.57 | 1.00 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| MXINT16-64 | 5.11 | 5.11 | 1.00 | 4.57 | 4.57 | 1.00 |'
- en: '| MXINT16-32 | 5.11 | 5.11 | 1.00 | 4.57 | 4.57 | 1.00 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| MXINT16-32 | 5.11 | 5.11 | 1.00 | 4.57 | 4.57 | 1.00 |'
- en: '| INT-8 | 237.40 | 498.39 | 0.48 | 186.04 | 61.65 | 3.02 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| INT-8 | 237.40 | 498.39 | 0.48 | 186.04 | 61.65 | 3.02 |'
- en: '| MXINT8-128 | 5.13 | 5.12 | 1.00 | 4.58 | 4.58 | 1.00 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| MXINT8-128 | 5.13 | 5.12 | 1.00 | 4.58 | 4.58 | 1.00 |'
- en: '| MXINT8-64 | 5.12 | 5.12 | 1.00 | 4.58 | 4.58 | 1.00 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| MXINT8-64 | 5.12 | 5.12 | 1.00 | 4.58 | 4.58 | 1.00 |'
- en: '| MXINT8-32 | 5.12 | 5.12 | 1.00 | 4.58 | 4.57 | 1.00 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| MXINT8-32 | 5.12 | 5.12 | 1.00 | 4.58 | 4.57 | 1.00 |'
- en: '| MXINT8-16 | 5.12 | 5.12 | 1.00 | 4.57 | 4.57 | 1.00 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| MXINT8-16 | 5.12 | 5.12 | 1.00 | 4.57 | 4.57 | 1.00 |'
- en: '| INT-6 | 53068.17 | 52256.23 | 1.02 | 25028.43 | 39820.93 | 0.63 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| INT-6 | 53068.17 | 52256.23 | 1.02 | 25028.43 | 39820.93 | 0.63 |'
- en: '| MXINT6-128 | 5.30 | 5.26 | 1.01 | 4.72 | 4.68 | 1.01 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| MXINT6-128 | 5.30 | 5.26 | 1.01 | 4.72 | 4.68 | 1.01 |'
- en: '| MXINT6-16 | 5.18 | 5.16 | 1.00 | 4.62 | 4.61 | 1.00 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| MXINT6-16 | 5.18 | 5.16 | 1.00 | 4.62 | 4.61 | 1.00 |'
- en: '| INT-4 | 45024.28 | 38934.63 | 1.16 | 140992.44 | 136368.02 | 1.03 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| INT-4 | 45024.28 | 38934.63 | 1.16 | 140992.44 | 136368.02 | 1.03 |'
- en: '| MXINT4-128 | 39.15 | 25.49 | 1.54 | 23.52 | 13.73 | 1.71 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| MXINT4-128 | 39.15 | 25.49 | 1.54 | 23.52 | 13.73 | 1.71 |'
- en: '| MXINT4-16 | 7.01 | 6.62 | 1.06 | 6.21 | 5.78 | 1.07 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| MXINT4-16 | 7.01 | 6.62 | 1.06 | 6.21 | 5.78 | 1.07 |'
- en: A.2 SmoothQuant and GPTQ Interaction for quantization of DistilGPT2 and the
    LLaMA family
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 SmoothQuant 与 GPTQ 在 DistilGPT2 和 LLaMA 家族量化中的交互
- en: DistilGPT2.
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: DistilGPT2。
- en: To understand how SmoothQuant interacts with GPTQ when quantizing a small model
    like DistilGPT2, we quantized this model to INT-8 and MXINT4 using these two methods
    individually, as well as in combination. (Table [8](#A1.T8 "Table 8 ‣ DistilGPT2\.
    ‣ A.2 SmoothQuant and GPTQ Interaction for quantization of DistilGPT2 and the
    LLaMA family ‣ Appendix A Appendix ‣ Combining multiple post-training techniques
    to achieve most efficient quantized LLMs")). We found that the best results for
    both quantization formats are achieved when only the GPTQ method is applied.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解 SmoothQuant 在对像 DistilGPT2 这样的小模型进行量化时与 GPTQ 的相互作用，我们分别使用这两种方法将该模型量化为 INT-8
    和 MXINT4，并且还进行了组合实验。（表 [8](#A1.T8 "表 8 ‣ DistilGPT2\. ‣ A.2 SmoothQuant 与 GPTQ
    在 DistilGPT2 和 LLaMA 家族量化中的交互 ‣ 附录 A 附录 ‣ 结合多种后训练技术实现最有效的量化 LLMs")）。我们发现，对于这两种量化格式，当仅应用
    GPTQ 方法时，效果最佳。
- en: 'Table 8: DistilGPT2 perplexity results on WikiText-2-test, with enabling/disabling
    GPTQ and A-W SmoothQuant. A, W, SQ, and PPL denote activation, weight, SmoothQuant,
    and perplexity, respectively. $\downarrow$: the lower the metric, the better the
    result. We used per-tensor affine quantization for the INT-8 format.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：DistilGPT2 在 WikiText-2-test 上的困惑度结果，启用/禁用 GPTQ 和 A-W SmoothQuant。A、W、SQ
    和 PPL 分别表示激活、权重、SmoothQuant 和困惑度。$\downarrow$：指标越低，结果越好。我们对 INT-8 格式使用了每张量仿射量化。
- en: '| Act & Wgt format | A:INT-8 | A:MXINT8 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 激活与权重格式 | A:INT-8 | A:MXINT8 |'
- en: '| --- | --- | --- |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| W:INT-8 | W:MXINT4 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| W:INT-8 | W:MXINT4 |'
- en: '| --- | --- |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GPTQ | A-W SQ | DistilGPT2 (FP32 $\downarrow$PPL: 46.07) |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | A-W SQ | DistilGPT2 (FP32 $\downarrow$PPL: 46.07) |'
- en: '| --- | --- | --- |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| disabled | disabled | 59.23 | 72.97 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 禁用 | 59.23 | 72.97 |'
- en: '| disabled | enabled | 60.57 | 72.93 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 禁用 | 启用 | 60.57 | 72.93 |'
- en: '| enabled | disabled | 49.82 | 53.93 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 禁用 | 49.82 | 53.93 |'
- en: '| enabled | enabled | 52.12 | 56.65 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 启用 | 启用 | 52.12 | 56.65 |'
- en: LLaMA and llama2 family.
  id: totrans-305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLaMA 和 llama2 家族。
- en: We conducted a similar experiment for the LLaMA and llama2 families, demonstrating
    that for all studied models and quantization formats, except when quantizing llama2-13B
    to INT-8, the best results were obtained when only GPTQ was applied. For the latter
    case, SmoothQuant was necessary to improve the perplexity degradation of the quantized
    model (Table [9](#A1.T9 "Table 9 ‣ LLaMA and llama2 family. ‣ A.2 SmoothQuant
    and GPTQ Interaction for quantization of DistilGPT2 and the LLaMA family ‣ Appendix
    A Appendix ‣ Combining multiple post-training techniques to achieve most efficient
    quantized LLMs")).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对LLaMA和llama2系列进行了类似的实验，结果表明，在所有研究的模型和量化格式中，除了将llama2-13B量化为INT-8的情况外，仅应用GPTQ时获得了最佳结果。对于后者的情况，SmoothQuant是必要的，以改善量化模型的困惑度退化（表 [9](#A1.T9
    "Table 9 ‣ LLaMA and llama2 family. ‣ A.2 SmoothQuant and GPTQ Interaction for
    quantization of DistilGPT2 and the LLaMA family ‣ Appendix A Appendix ‣ Combining
    multiple post-training techniques to achieve most efficient quantized LLMs")）。
- en: 'Table 9: Quantization results for the LLaMA and the llama2 families with different
    sizes on WikiText-2-test, with enabling/disabling GPTQ and A-W SmoothQuant. A,
    W, SQ, and PPL denote activation, weight, SmoothQuant, and perplexity, respectively.
    $\downarrow$: the lower the metric, the better the result. We used per-tensor
    affine quantization for the INT-8 format.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：LLaMA和llama2系列在WikiText-2-test上的不同尺寸的量化结果，包括启用/禁用GPTQ和A-W SmoothQuant。A、W、SQ和PPL分别表示激活、权重、SmoothQuant和困惑度。$\downarrow$：指标越低，结果越好。我们对INT-8格式使用了每个张量的仿射量化。
- en: '| Act & Wgt format | A:INT-8 | A:MXINT8 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Act & Wgt格式 | A:INT-8 | A:MXINT8 |'
- en: '| W:INT-8 | W:MXINT4 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| W:INT-8 | W:MXINT4 |'
- en: '| GPTQ | SQ | LLaMA-7B (FP16 $\downarrow$PPL: 5.67) |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | SQ | LLaMA-7B (FP16 $\downarrow$PPL: 5.67) |'
- en: '| disabled | disabled | 19.01 | 6.31 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| disabled | disabled | 19.01 | 6.31 |'
- en: '| disabled | enabled | 17.47 | 6.18 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| disabled | enabled | 17.47 | 6.18 |'
- en: '| enabled | disabled | 17.44 | 7.23 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| enabled | disabled | 17.44 | 7.23 |'
- en: '| enabled | enabled | 24.27 | 6.24 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| enabled | enabled | 24.27 | 6.24 |'
- en: '| GPTQ | SQ | LLaMA-13B (FP16 $\downarrow$PPL: 5.09) |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | SQ | LLaMA-13B (FP16 $\downarrow$PPL: 5.09) |'
- en: '| disabled | disabled | 28.90 | 5.43 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| disabled | disabled | 28.90 | 5.43 |'
- en: '| disabled | enabled | 32.86 | 5.47 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| disabled | enabled | 32.86 | 5.47 |'
- en: '| enabled | disabled | 28.41 | 5.32 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| enabled | disabled | 28.41 | 5.32 |'
- en: '| enabled | enabled | 31.82 | 5.37 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| enabled | enabled | 31.82 | 5.37 |'
- en: '| GPTQ | SQ | LLaMA-30B (FP16 $\downarrow$PPL: 4.10) |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | SQ | LLaMA-30B (FP16 $\downarrow$PPL: 4.10) |'
- en: '| disabled | disabled | 17.72 | 4.46 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| disabled | disabled | 17.72 | 4.46 |'
- en: '| disabled | enabled | 21.39 | 4.49 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| disabled | enabled | 21.39 | 4.49 |'
- en: '| enabled | disabled | 17.39 | 4.37 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| enabled | disabled | 17.39 | 4.37 |'
- en: '| enabled | enabled | 34.13 | 4.43 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| enabled | enabled | 34.13 | 4.43 |'
- en: '| GPTQ | SQ | llama2-7B (FP16 $\downarrow$PPL: 5.11) |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | SQ | llama2-7B (FP16 $\downarrow$PPL: 5.11) |'
- en: '| disabled | disabled | 237.40 | 5.56 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| disabled | disabled | 237.40 | 5.56 |'
- en: '| disabled | enabled | 303.96 | 5.61 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| disabled | enabled | 303.96 | 5.61 |'
- en: '| enabled | disabled | 231.23 | 5.47 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| enabled | disabled | 231.23 | 5.47 |'
- en: '| enabled | enabled | 239.88 | 5.50 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| enabled | enabled | 239.88 | 5.50 |'
- en: '| GPTQ | SQ | llama2-13B (FP16 $\downarrow$PPL: 4.57) |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | SQ | llama2-13B (FP16 $\downarrow$PPL: 4.57) |'
- en: '| disabled | disabled | 186.04 | 4.82 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| disabled | disabled | 186.04 | 4.82 |'
- en: '| disabled | enabled | 58.16 | 4.93 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| disabled | enabled | 58.16 | 4.93 |'
- en: '| enabled | disabled | 181.67 | 4.76 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| enabled | disabled | 181.67 | 4.76 |'
- en: '| enabled | enabled | 48.61 | 4.86 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| enabled | enabled | 48.61 | 4.86 |'
- en: A.3 Pareto frontier study for the OPT family
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 OPT系列的帕累托前沿研究
- en: Figure 4 illustrates the perplexity of the OPT family on WikiText-2-test when
    quantized to different formats. Each point on the figure corresponds to a quantization
    configuration, indicating the activation and weight formats and whether SmoothQuant
    and GPTQ are enabled or disabled. The quantized models positioned on Pareto frontiers
    are marked with a gray circle. We observe that, generally, for medium and large-sized
    models (e.g, OPT-13B, and OPT-30B) with restrictive quantization to MXINT4, the
    points appearing on the Pareto frontiers are those with only GPTQ enabled. However,
    for smaller models (e.g., OPT-1.3B), both GPTQ and SQ should be applied jointly
    to effectively reduce perplexity when aggressively quantizing the models to MXINT4\.
    For less restrictive quantization to MXINT6, only SmoothQuant is sufficient to
    mitigate perplexity degradation. Finally, with quantization to MXINT8, none of
    these techniques is required.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图4展示了OPT系列在WikiText-2-test数据集上量化到不同格式时的困惑度。图中的每个点对应一个量化配置，指示激活和权重格式以及SmoothQuant和GPTQ是否启用或禁用。位于帕累托前沿的量化模型用灰色圆圈标记。我们观察到，一般来说，对于中型和大型模型（例如，OPT-13B和OPT-30B）在严格量化为MXINT4时，出现在帕累托前沿的点仅在启用GPTQ的情况下出现。然而，对于较小的模型（例如，OPT-1.3B），在将模型积极量化为MXINT4时，应该同时应用GPTQ和SQ，以有效降低困惑度。对于不那么严格的MXINT6量化，只有SmoothQuant就足以减轻困惑度的下降。最后，对于MXINT8量化，这些技术都不再需要。
- en: '![Refer to caption](img/cb11eb5d8502f6dd0b7ead2349e0d765.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/cb11eb5d8502f6dd0b7ead2349e0d765.png)'
- en: 'Figure 4: Perplexity for the OPT family with different sizes when quantized
    to INT-8, MXINT8, and MXINT4\. The Y-axis represents perplexity. The X-axis represents
    model parameter size including the additional scale parameters required by the
    SmoothQuant quantization method. Note that the GPTQ algorithm does not introduce
    any additional model parameters during the inference. The Per-tensor affine scheme
    is used for INT-8 quantization of both activations and weights. A, W, and SQ denote
    activation, weight, and SmoothQuant, respectively. The points corresponding to
    the quantized models on Pareto frontiers are indicated by a gray circle.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：OPT系列不同大小模型在量化为INT-8、MXINT8和MXINT4时的困惑度。Y轴表示困惑度。X轴表示模型参数大小，包括SmoothQuant量化方法所需的额外尺度参数。请注意，GPTQ算法在推理过程中不会引入任何额外的模型参数。INT-8量化的激活和权重使用了每个张量仿射方案。A、W和SQ分别表示激活、权重和SmoothQuant。位于帕累托前沿的量化模型用灰色圆圈标记。
- en: A.4 Block size granularity effect on quantization
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 块大小对量化的粒度效应
- en: 'To study the effects of block size granularity on quantization to fixed-point
    and microscaling formats, we conducted a comprehensive study on DistilGPT2 and
    GPT-xl, the smallest and the largest networks from the GPT2 family. We considered
    quantization with block sizes of $16$ bits (Figure [5](#A1.F5 "Figure 5 ‣ A.4
    Block size granularity effect on quantization ‣ Appendix A Appendix ‣ Combining
    multiple post-training techniques to achieve most efficient quantized LLMs")).
    For integer formats, we applied an affine quantization scheme, incorporating zero
    points and scales in three different formats: FP8-e5m2, FP8-e4m3fn, and FP32.
    For MXINT, the scale data type is INT8, and zero point is not used. We found that
    (a) with extreme quantization to 4-bit (points on the left cluster), most points
    on the Pareto frontier align with INT8 quantization with zero points and scales
    in the FP8-e4m3fn format; additionally, for a quantization granularity of 16,
    the INT8 with FP32 scale/zero point format also appears on the Pareto frontier,
    and (b) for quantization to 6-bit and 8-bit, the middle and right clusters, the
    majority of points on the Pareto frontier are associated with MX data-types. Notably,
    for DistilGPT2, points corresponding to INT8-FP32 are also present on the Pareto
    frontiers when the quantization granularity is small (16 and 32).'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 为研究块大小粒度对定点量化和微缩格式的影响，我们对DistilGPT2和GPT-xl这两个来自GPT2家族的最小和最大网络进行了全面研究。我们考虑了$16$位块大小的量化（图[5](#A1.F5
    "Figure 5 ‣ A.4 Block size granularity effect on quantization ‣ Appendix A Appendix
    ‣ Combining multiple post-training techniques to achieve most efficient quantized
    LLMs")）。对于整数格式，我们采用了一个仿射量化方案，包含了零点和三种不同格式的比例：FP8-e5m2、FP8-e4m3fn和FP32。对于MXINT，比例数据类型为INT8，不使用零点。我们发现，（a）在极端量化到4位（左侧簇中的点）时，大多数在帕累托前沿的点与INT8量化对齐，并且零点和比例在FP8-e4m3fn格式中；此外，对于量化粒度为16时，INT8与FP32比例/零点格式也出现在帕累托前沿上，（b）对于6位和8位量化（中间和右侧簇），大多数在帕累托前沿的点与MX数据类型相关。特别是，对于DistilGPT2，当量化粒度较小（16和32）时，INT8-FP32对应的点也出现在帕累托前沿上。
- en: '![Refer to caption](img/3550ea52897e02f7be389353400560eb.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3550ea52897e02f7be389353400560eb.png)'
- en: (a) DistilGPT2
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: (a) DistilGPT2
- en: '![Refer to caption](img/cdae513ac2dbbd4a08835c4200046194.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cdae513ac2dbbd4a08835c4200046194.png)'
- en: (b) GPT2-XL
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GPT2-XL
- en: 'Figure 5: Perplexity for (a) DistilGPT2 and (b) GPT2-XL when quantized to 8,
    6, and 4 bit-widths with different quantization granularities. The Y-axis represents
    perplexity. The X-axis represents the model size in MB. The affine scheme is used
    for INT quantization of weights. The points corresponding to the quantized models
    on Pareto frontiers are indicated by a gray circle.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：当量化到8、6和4位宽度且采用不同的量化粒度时，(a) DistilGPT2 和 (b) GPT2-XL 的困惑度。Y轴表示困惑度。X轴表示模型大小（以MB为单位）。INT量化权重使用仿射方案。量化模型在帕累托前沿上的点由灰色圆圈表示。
