- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:59:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:59:06
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Divide-or-Conquer? Which Part Should You Distill Your LLM?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分而治之？你应该将你的大型语言模型（LLM）分解成哪些部分？
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15000](https://ar5iv.labs.arxiv.org/html/2402.15000)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15000](https://ar5iv.labs.arxiv.org/html/2402.15000)
- en: Zhuofeng Wu^†^‡, He Bai^‡, Aonan Zhang^‡,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhuofeng Wu^†^‡，He Bai^‡，Aonan Zhang^‡，
- en: Jiatao Gu^‡, VG Vinod Vydiswaran^†, Navdeep Jaitly^‡, Yizhe Zhang^‡
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Jiatao Gu^‡，VG Vinod Vydiswaran^†，Navdeep Jaitly^‡，Yizhe Zhang^‡
- en: ^†University of Michigan, ^‡Apple
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^†密歇根大学，^‡苹果
- en: '{zhuofeng,vgvinodv}@umich.edu,'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{zhuofeng,vgvinodv}@umich.edu，'
- en: '{hbai22,aonan_zhang,jgu32,njaitly,yizhe_zhang}@apple.com Work done during internship
    at Apple'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{hbai22,aonan_zhang,jgu32,njaitly,yizhe_zhang}@apple.com 在苹果公司实习期间完成的工作'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent methods have demonstrated that Large Language Models (LLMs) can solve
    reasoning tasks better when they are encouraged to solve subtasks of the main
    task first. In this paper we devise a similar strategy that breaks down reasoning
    tasks into a problem decomposition phase and a problem solving phase and show
    that the strategy is able to outperform a single stage solution. Further, we hypothesize
    that the decomposition should be easier to distill into a smaller model compared
    to the problem solving because the latter requires large amounts of domain knowledge
    while the former only requires learning general problem solving strategies. We
    propose methods to distill these two capabilities and evaluate their impact on
    reasoning outcomes and inference cost. We find that we can distill the problem
    decomposition phase and at the same time achieve good generalization across tasks,
    datasets, and models. However, it is harder to distill the problem solving capability
    without losing performance and the resulting distilled model struggles with generalization.
    These results indicate that by using smaller, distilled problem decomposition
    models in combination with problem solving LLMs we can achieve reasoning with
    cost-efficient inference and local adaptation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的方法表明，当大型语言模型（LLM）被鼓励首先解决主任务的子任务时，它们能够更好地解决推理任务。在本文中，我们设计了一个类似的策略，将推理任务分解为问题分解阶段和问题解决阶段，并展示了该策略能够优于单一阶段解决方案。此外，我们假设问题分解相比问题解决更容易被提炼到较小的模型中，因为后者需要大量的领域知识，而前者只需学习一般的问题解决策略。我们提出了提炼这两种能力的方法，并评估了它们对推理结果和推断成本的影响。我们发现我们可以提炼问题分解阶段，同时在任务、数据集和模型之间实现良好的泛化。然而，在不降低性能的情况下，提炼问题解决能力更为困难，结果是提炼后的模型在泛化方面表现不佳。这些结果表明，通过使用较小的、提炼后的问题分解模型与问题解决LLM结合，我们可以实现具有成本效益的推理和局部适应。
- en: Divide-or-Conquer? Which Part Should You Distill Your LLM?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 分而治之？你应该将你的大型语言模型（LLM）分解成哪些部分？
- en: 'Zhuofeng Wu^†^‡^†^†thanks: Work done during internship at Apple, He Bai^‡,
    Aonan Zhang^‡, Jiatao Gu^‡, VG Vinod Vydiswaran^†, Navdeep Jaitly^‡, Yizhe Zhang^‡
    ^†University of Michigan, ^‡Apple {zhuofeng,vgvinodv}@umich.edu, {hbai22,aonan_zhang,jgu32,njaitly,yizhe_zhang}@apple.com'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Zhuofeng Wu^†^‡^†^†感谢：在苹果公司实习期间完成的工作，He Bai^‡，Aonan Zhang^‡，Jiatao Gu^‡，VG Vinod
    Vydiswaran^†，Navdeep Jaitly^‡，Yizhe Zhang^‡ ^†密歇根大学，^‡苹果 {zhuofeng,vgvinodv}@umich.edu，{hbai22,aonan_zhang,jgu32,njaitly,yizhe_zhang}@apple.com
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: '![Refer to caption](img/69ceab0cdaf508a24080c1f3cc538360.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/69ceab0cdaf508a24080c1f3cc538360.png)'
- en: 'Figure 1: Reasoning with a long thought chain using the black box LLM can be
    expensive and inflexible. We propose to dissect the decomposition and solving
    of the task, and distill only the decomposition capability to a less costly and
    more flexible student model, while still maintaining the original performance.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：使用黑箱LLM进行长链推理可能代价高昂且不灵活。我们建议将任务的分解和解决进行剖析，并将仅分解能力提炼到一个成本更低且更灵活的学生模型中，同时保持原始性能。
- en: Large Language Models (LLMs), such as GPT-4 (OpenAI, [2023](#bib.bib18)), demonstrate
    exceptional abilities in solving knowledge-intensive tasks like Open Domain QA
    (ODQA) (Zhu et al., [2021](#bib.bib42)), math (Yue et al., [2023](#bib.bib39)),
    science (Taylor et al., [2022](#bib.bib27)) and autonomous agents (Yao et al.,
    [2022](#bib.bib38); Significant Gravitas, [2023](#bib.bib24); Wang et al., [2024](#bib.bib31)).
    However, the use of gigantic LLMs with hundreds of billions of parameters can
    be costly during inference, particularly when the reasoning chain generated is
    lengthy. Additionally, due to the opaque nature of these black box LLMs, they
    offer limited adaption options. There is a need to use cheaper and more flexible
    models to leverage the power of these black box LLMs for local adaptation and
    cost-efficient inference. Distilling the large LLMs would seem like a reasonable
    strategy, but it often results in a significant drop in performance for downstream
    tasks (Chiang et al., [2023b](#bib.bib4)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），例如 GPT-4（OpenAI，[2023](#bib.bib18)），在解决知识密集型任务方面展现了卓越的能力，如开放域问答（ODQA）（Zhu
    等，[2021](#bib.bib42)）、数学（Yue 等，[2023](#bib.bib39)）、科学（Taylor 等，[2022](#bib.bib27)）和自主智能体（Yao
    等，[2022](#bib.bib38)；Significant Gravitas，[2023](#bib.bib24)；Wang 等，[2024](#bib.bib31)）。然而，使用具有数百亿参数的巨大
    LLM 在推理时可能成本高昂，尤其是在生成的推理链很长时。此外，由于这些黑箱 LLM 的不透明性，它们提供的适应选项有限。因此，需要使用更便宜、更灵活的模型来利用这些黑箱
    LLM 的强大功能，实现本地适应和成本效益推理。对大型 LLM 进行蒸馏似乎是一个合理的策略，但通常会导致下游任务的性能显著下降（Chiang 等，[2023b](#bib.bib4)）。
- en: 'Previous studies (Weng, [2023](#bib.bib34); Wang et al., [2023](#bib.bib30))
    have indicated that effectively addressing such tasks requires the model to proficiently
    perform two essential capabilities simultaneously: 1) planning and decomposition,
    which involves breaking down complex objectives into smaller, more manageable
    subgoals to facilitate efficient handling of intricate tasks; and 2) execution
    and solving, which involves memorizing vast amounts of knowledge from extensive
    web training data and effectively recalling this information when needed to execute
    the problem-solving process. The first capability, decomposition, typically requires
    the model to engage in self-reflection on the input query and generate a Chain-of-Thoughts
    (CoT)-style reasoning chain (Wei et al., [2022](#bib.bib32)) to tackle the problem.
    Usually, these two abilities are intertwined in a single monolithic model throughout
    the problem-solving process (Zhou et al., [2022](#bib.bib41)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究（Weng，[2023](#bib.bib34)；Wang 等，[2023](#bib.bib30)）表明，有效解决这些任务需要模型同时具备两个基本能力：1）计划和分解，这涉及将复杂的目标拆分为更小、更可管理的子目标，以便高效处理复杂任务；2）执行和解决，这涉及记忆来自大量网络训练数据的知识，并在需要时有效地召回这些信息来执行问题解决过程。第一个能力，即分解，通常要求模型对输入查询进行自我反思，并生成链式思维（CoT）风格的推理链（Wei
    等，[2022](#bib.bib32)）来解决问题。通常，这两种能力在问题解决过程中是交织在一个单一的整体模型中的（Zhou 等，[2022](#bib.bib41)）。
- en: 'In this paper, we first investigate whether it is possible to decouple the
    decomposition and solving capabilities, and how to distill these capabilities
    into smaller models for faster inference. We then test several hypotheses: 1)
    distilling decomposition is easier than distilling solving. Decomposition primarily
    relies on semantic understanding and query parsing, while solving requires more
    domain expertise and knowledge. For example, decomposing the query “who is older,
    Messi or Ronaldo?” into “how old is Messi?”, “how old is Ronaldo?”, and “who is
    older?” only requires text comprehension, whereas solving the task necessitates
    memorization, retrieval, and utilization of information. We speculate that compressing
    the less knowledge-intensive decomposition is easier. 2) decomposition is more
    generalizable than solving. We hypothesize that decomposition can sometimes be
    abstracted into symbolic principles, making it more universally applicable across
    tasks, datasets, and models. This enables tasks and models to share a common decomposition
    engine and benefit from each other’s power, reducing the effort and costs involved
    in distilling a model for each individual task.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们首先探讨了是否可以解耦分解能力和解决能力，并如何将这些能力提炼到更小的模型中以加快推理速度。然后，我们测试了几个假设：1）分解比解决更容易提炼。分解主要依赖于语义理解和查询解析，而解决则需要更多的领域专业知识。例如，将“梅西和罗纳尔多谁年长？”的查询分解为“梅西多大了？”，“罗纳尔多多大了？”和“谁年长？”只需要文本理解，而解决任务则需要记忆、检索和利用信息。我们推测压缩知识密集度较低的分解更容易。2）分解比解决更具泛化性。我们假设分解有时可以抽象为符号原则，使其在任务、数据集和模型之间具有更普遍的适用性。这使得任务和模型能够共享一个通用的分解引擎，相互受益，减少为每个单独任务提炼模型所涉及的工作和成本。
- en: 'A natural question arises: is it possible to distill only the long reasoning
    chain, which accounts for most of the inference cost, but is relatively easier
    to distill? To this end, we propose and evaluate the distillation of only the
    decomposition capability from the LLM. We conduct experiments using a teacher
    model of GPT-3.5-turbo and a student model of vicuna-13B (Chiang et al., [2023a](#bib.bib3))
    on QA and mathematics datasets (Dua et al., [2019](#bib.bib7); Cobbe et al., [2021](#bib.bib5)).
    Our contributions include:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自然会产生一个问题：是否可以仅提炼出较长的推理链，它占据了大部分推理成本，但相对较容易提炼？为此，我们提出并评估了仅从LLM中提炼出分解能力的方法。我们使用GPT-3.5-turbo作为教师模型，vicuna-13B（Chiang等，[2023a](#bib.bib3)）作为学生模型，在QA和数学数据集（Dua等，[2019](#bib.bib7)；Cobbe等，[2021](#bib.bib5)）上进行实验。我们的贡献包括：
- en: '1.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We demonstrate that the decomposition capability is crucial for the complex
    reasoning of LLM. This capability can be dissected from the problem solving or
    task solving capability.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了分解能力对于LLM复杂推理的重要性。这种能力可以从问题解决或任务解决能力中剖析出来。
- en: '2.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We demonstrate the possibility and effectiveness of distilling only the query
    decomposition from the teacher model. The resulting distilled model can maintain
    most of the performance while significantly reducing inference costs. However,
    distilling the solving part of the LLM leads to a considerable decline in performance.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了仅从教师模型中提炼查询分解的可能性和有效性。最终的提炼模型可以保持大部分性能，同时显著降低推理成本。然而，提炼LLM的解决部分会导致性能显著下降。
- en: '3.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We show that the distilled query decomposition model exhibits good generalization
    across tasks, datasets, and models. However, the distilled solving for each task
    does not generalize well.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了提炼出的查询分解模型在任务、数据集和模型之间展现了良好的泛化能力。然而，每个任务的提炼解法泛化效果不佳。
- en: 2 Decoupling Decomposition and Solving
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 解耦分解和解决
- en: As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Divide-or-Conquer?
    Which Part Should You Distill Your LLM?"), a common approach to solving a reasoning
    task using an LLM involves directly generating a response to the instruction and
    question. This is referred to as the Single-Stage model. The conventional method
    for LLM, known as the Chain of Thought (CoT), instructs the model to “think step
    by step,” allowing the model to take more computational steps for difficult tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Divide-or-Conquer? Which Part Should
    You Distill Your LLM?")所示，使用LLM解决推理任务的常见方法是直接生成对指令和问题的回应。这被称为单阶段模型。LLM的传统方法，称为“思维链”（CoT），指示模型“逐步思考”，允许模型在处理困难任务时采取更多的计算步骤。
- en: However, CoT-style reasoning has limitations as it often struggles to generalize
    to problems beyond the scope of the in-context examples. To address this drawback,
    the most notable work is the Least-to-Most approach (Zhou et al., [2022](#bib.bib41)),
    where the model breaks down the original question into subquestions and answers
    them sequentially. These approaches have shown improved performance compared to
    CoT.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，CoT风格的推理有其局限性，因为它通常难以推广到超出上下文示例范围的问题。为了解决这一缺陷，最著名的工作是Least-to-Most方法（Zhou等，[2022](#bib.bib41)），该方法将原始问题分解为子问题并依次回答。与CoT相比，这些方法表现出更好的性能。
- en: For QA tasks, typically, the next subquestion is less dependent on the answer
    to the previous subquestions. Conveniently, we propose a static strategy similar
    to HuggingGPT (Shen et al., [2023](#bib.bib23)), where in the first Decomposition
    stage several decomposed subquestions are first generated to decompose the primary
    question. In the second Solving stage, these subquestions are then answered one
    by one to obtain the final answer. We refer to this line of models as the Two-Stage
    models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于QA任务，通常，下一个子问题对前一个子问题的答案依赖性较小。方便的是，我们提出了一种静态策略，类似于HuggingGPT（Shen等，[2023](#bib.bib23)），在第一阶段生成几个分解后的子问题以分解主要问题。在第二阶段，这些子问题会被逐一回答以获得最终答案。我们将这类模型称为两阶段模型。
- en: 3 Distill the Decomposition Capability
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 提炼分解能力
- en: Generating decomposed questions can be computationally expensive when the reasoning
    chain is long while using a black box LLM. Moreover, it is challenging to optimize
    or customize the decomposition process as it is performed by the black box model.
    Our proposal aims to address these issues by utilizing a smaller trainable student
    model, as a drop-in replacement for the large black box LLM for decomposition.
    To achieve this, we distill the decomposition capability from the teacher LLM,
    referred to as $\mathcal{T}$.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用黑箱LLM时，生成分解后的问题可能会很耗费计算资源，尤其是当推理链很长时。此外，由于分解过程由黑箱模型执行，因此优化或自定义分解过程也具有挑战性。我们的提议旨在解决这些问题，通过利用一个较小的可训练学生模型，作为大型黑箱LLM的替代品来进行分解。为此，我们从教师LLM（称为$\mathcal{T}$）中提炼分解能力。
- en: Generating Sub-questions from Teacher
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从教师生成子问题
- en: As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Divide-or-Conquer?
    Which Part Should You Distill Your LLM?"), we begin by gathering demonstrations
    from $\mathcal{T}$.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[1](#S1.F1 "图1 ‣ 1 引言 ‣ 分而治之？你应该提炼LLM的哪个部分？")所示，我们首先从$\mathcal{T}$中收集示例。
- en: 'Instruction
    for decomposition: $I_{\text{decomp}}$'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'Instruction
    for decomposition: $I_{\text{decomp}}$'
- en: $\mathcal{T}$.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{T}$。
- en: Decomposer Distillation
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Decomposer Distillation
- en: Given the sub-questions $\{S_{i}\}$.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 给定子问题$\{S_{i}\}$。
- en: Subquestions Screening via Ground-truth Answer
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过真实答案筛选子问题
- en: 'As an additional step, if the dataset comes with a corresponding ground-truth
    answer, denoted as $A$ is provided as the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外步骤，如果数据集提供了相应的真实答案，记作$A$，则如下：
- en: 'Instruction for solving: $I_{\text{ans}}$'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 'Instruction for solving: $I_{\text{ans}}$'
- en: We assume that, statistically speaking, good $\{S_{i}\}$.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设，从统计学角度来看，好的$\{S_{i}\}$。
- en: In Section [5.2](#S5.SS2 "5.2 Is Distilling Decomposition Easier than Distilling
    Solving? ‣ 5 Results ‣ Divide-or-Conquer? Which Part Should You Distill Your LLM?"),
    we compare the performance of the distilled decomposer trained using the entire
    set of demonstrations $\mathcal{S}_{D}$.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[5.2节](#S5.SS2 "5.2 提炼分解是否比提炼解决更容易？ ‣ 5 结果 ‣ 分而治之？你应该提炼LLM的哪个部分？")中，我们比较了使用整个演示集$\mathcal{S}_{D}$训练的提炼分解器的性能。
- en: '|  | Decomposer | Solver | Performance$\uparrow$ |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | Decomposer | Solver | Performance$\uparrow$ |'
- en: '|  | Model | Model | GSM8K (EM) | DROP (F1) | GSM8K($) | DROP($) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型 | 模型 | GSM8K (EM) | DROP (F1) | GSM8K($) | DROP($) |'
- en: '| Single-stage | - | GPT | 20.32 | 46.51 | -/0.01 | -/0.05 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 单阶段 | - | GPT | 20.32 | 46.51 | -/0.01 | -/0.05 |'
- en: '| - | Vicuna-13B | 9.40 | 26.68 | -/0.03 | -/0.03 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| - | Vicuna-13B | 9.40 | 26.68 | -/0.03 | -/0.03 |'
- en: '| Two-stage | GPT | GPT | 65.13 | 55.73 | 0.13/0.63 | 0.73/0.96 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 两阶段 | GPT | GPT | 65.13 | 55.73 | 0.13/0.63 | 0.73/0.96 |'
- en: '| Vicuna-13B | GPT | 62.93 | 47.13 | 0.02/0.67 | 0.07/0.96 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13B | GPT | 62.93 | 47.13 | 0.02/0.67 | 0.07/0.96 |'
- en: '| GPT | Vicuna-13B | 28.13 | 21.29 | 0.13/0.07 | 0.73/0.08 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| GPT | Vicuna-13B | 28.13 | 21.29 | 0.13/0.07 | 0.73/0.08 |'
- en: '| Vicuna-13B | Vicuna-13B | 28.51 | 20.90 | 0.02/0.08 | 0.07/0.08 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13B | Vicuna-13B | 28.51 | 20.90 | 0.02/0.08 | 0.07/0.08 |'
- en: '| w/o oracle answer $A$ | GPT | 67.02 | 55.19 | 0.01/0.62 | 0.06/0.96 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 无oracle答案 $A$ | GPT | 67.02 | 55.19 | 0.01/0.62 | 0.06/0.96 |'
- en: '| GPT | $\mathcal{S}_{E}$ | 48.98 | 13.37 | 0.13/0.09 | 0.73/0.06 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| GPT | $\mathcal{S}_{E}$ | 48.98 | 13.37 | 0.13/0.09 | 0.73/0.06 |'
- en: '| w/ oracle answer $A$ | GPT | 67.78 | 57.97 | 0.01/0.60 | 0.06/1.11 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 有oracle答案 $A$ | GPT | 67.78 | 57.97 | 0.01/0.60 | 0.06/1.11 |'
- en: '| GPT | $\mathcal{S}_{E}$ | 51.55 | 20.34 | 0.13/0.09 | 0.73/0.04 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| GPT | $\mathcal{S}_{E}$ | 51.55 | 20.34 | 0.13/0.09 | 0.73/0.04 |'
- en: 'Table 1: Comparison results on GSM8K and DROP datasets. Performance on GSM8K
    is assessed via the exact match score (EM), while DROP is evaluated using the
    F1 score. The inference expense is estimated based on average per sample cost
    for each dataset. $X/X$ indicates decomposition/solving cost.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：GSM8K和DROP数据集上的比较结果。GSM8K的性能通过精确匹配分数（EM）评估，而DROP则使用F1分数进行评估。推理费用根据每个数据集的平均每样本成本进行估算。$X/X$
    表示分解/解决成本。
- en: 4 Experiments
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验
- en: Datasets We assess the effectiveness of our pipeline on two distinct datasets.
    GSM8K (Cobbe et al., [2021](#bib.bib5)) focuses on mathematical reasoning and
    is composed of 7.5K training instances alongside 1K test problems. DROP (Dua et al.,
    [2019](#bib.bib7)) caters to Question Answering, containing 77.4K training samples
    and a 9.5K validation set. We use GSM8K test set and DROP development set for
    the evaluation as the DROP test set does not have oracle answer $A$, which limited
    the evaluation scenarios.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 我们在两个不同的数据集上评估了我们的管道的有效性。GSM8K (Cobbe et al., [2021](#bib.bib5)) 侧重于数学推理，由7.5K个训练实例和1K个测试问题组成。DROP (Dua
    et al., [2019](#bib.bib7)) 针对问答，包含77.4K个训练样本和9.5K个验证集。我们使用GSM8K测试集和DROP开发集进行评估，因为DROP测试集没有oracle答案$A$，这限制了评估场景。
- en: Teacher/Student Models
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 教师/学生模型
- en: 'We use GPT-3.5-Turbo-0615 model (Ouyang et al., [2022](#bib.bib19)) as the
    teacher model throughout our experiments. After training we employ different levels
    of teacher models to ensure a comprehensive evaluation: one open sourced model
    (vanilla Vicuna (Chiang et al., [2023b](#bib.bib4))) and three black box models
    (text-davinci-003 (Brown et al., [2020](#bib.bib2)), GPT-3.5-Turbo and GPT-4).
    All the student model is initialized from Vicuna-13b-v1.3 (Chiang et al., [2023a](#bib.bib3)).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实验中使用GPT-3.5-Turbo-0615模型 (Ouyang et al., [2022](#bib.bib19)) 作为教师模型。在训练之后，我们使用不同级别的教师模型来确保全面评估：一个开源模型（vanilla
    Vicuna (Chiang et al., [2023b](#bib.bib4))) 和三个黑箱模型（text-davinci-003 (Brown et al.,
    [2020](#bib.bib2))，GPT-3.5-Turbo和GPT-4）。所有学生模型均初始化自Vicuna-13b-v1.3 (Chiang et al.,
    [2023a](#bib.bib3))。
- en: Student solver Models
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 学生解算器模型
- en: To compare the performance of distilling decomposer with distilling solver,
    we conducted further training on several Vicuna models to mimic the behavior of
    the teacher as student solvers. Similar to the student decomposer, $\mathcal{S}_{E}$.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较解算器蒸馏与解算器蒸馏的性能，我们对几个Vicuna模型进行了进一步训练，以模拟教师作为学生解算器的行为。类似于学生解算器，$\mathcal{S}_{E}$。
- en: Furthermore, in scenarios where the oracle answer $A$.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在oracle答案$A$的场景中。
- en: Training Details
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练细节
- en: We use a batch size of 128, train for 3 epochs on DROP and train for 5 epochs
    on GSM8K dataset (until convergence), and set the learning rate to $2\cdot 10^{-5}$
    80G A100 GPUs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用128的批次大小，在DROP上训练3个周期，在GSM8K数据集上训练5个周期（直到收敛），学习率设置为$2\cdot 10^{-5}$ 80G
    A100 GPUs。
- en: Inference Cost Estimation
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理成本估算
- en: We calculate the cost based on GPT-3.5-turbo-1106 (175B), with a rate of $\$0.001$
    for 1000 output tokens.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据GPT-3.5-turbo-1106 (175B) 计算成本，1000个输出token的费用为$\$0.001$。
- en: 5 Results
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: 5.1 Decomposition is Essential for Reasoning
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 分解对于推理至关重要
- en: First, we explore the possibility of separating the Decomposition from Solving
    and assess the effectiveness of using an improved decomposition for complex reasoning
    tasks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们探讨了将分解与解决分离的可能性，并评估了使用改进的分解方法在复杂推理任务中的有效性。
- en: Previous studies (Press et al., [2022](#bib.bib21); Zhou et al., [2022](#bib.bib41))
    have demonstrated the utility of leveraging decomposed subquestions to enhance
    the question-answering capabilities of black-box models. They adopt interactive
    planning strategies, where the generation of each subquestion is conditioned on
    the answer of the previous subquestions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的研究 (Press et al., [2022](#bib.bib21); Zhou et al., [2022](#bib.bib41)) 已经证明了利用分解的子问题来增强黑箱模型问答能力的实用性。他们采用了交互式规划策略，其中每个子问题的生成都基于之前子问题的答案。
- en: As discussed in Section [2](#S2 "2 Decoupling Decomposition and Solving ‣ Divide-or-Conquer?
    Which Part Should You Distill Your LLM?"), we instead use a static strategy by
    breaking down the reasoning process into two separate stages of Decomposition
    and Solving. Table [1](#S3.T1 "Table 1 ‣ Subquestions Screening via Ground-truth
    Answer ‣ 3 Distill the Decomposition Capability ‣ Divide-or-Conquer? Which Part
    Should You Distill Your LLM?") (Single-stage GPT/Vicuna vs Two-stage GPT/Vicuna),
    shows that in general such a static strategy leads to performance gains over a
    Single-stage approach. This aligns with previous findings.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[2](#S2 "2 Decoupling Decomposition and Solving ‣ Divide-or-Conquer? Which
    Part Should You Distill Your LLM?")节中讨论的，我们采用了一种静态策略，将推理过程分解为两个独立的阶段：分解和解决。表[1](#S3.T1
    "Table 1 ‣ Subquestions Screening via Ground-truth Answer ‣ 3 Distill the Decomposition
    Capability ‣ Divide-or-Conquer? Which Part Should You Distill Your LLM?")（单阶段GPT/Vicuna与双阶段GPT/Vicuna）显示，这种静态策略通常比单阶段方法带来性能提升。这与之前的发现一致。
- en: We demonstrate in Table [1](#S3.T1 "Table 1 ‣ Subquestions Screening via Ground-truth
    Answer ‣ 3 Distill the Decomposition Capability ‣ Divide-or-Conquer? Which Part
    Should You Distill Your LLM?") (Two-stage models) that replacing a stronger decomposer
    (GPT) with a weaker decomposer (Vicuna) mostly results in a noticeable decrease
    in performance, with an exception of using Vicuna as solver on GSM8K. We hypothesize
    that the reason is the Vicuna solver is too erroneous to harness the improvement
    from the decomposition. We observe that the decrease is more significant when
    the solver is more powerful. This suggests that in order to achieve optimal performance,
    a stronger decomposer is essential.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[1](#S3.T1 "Table 1 ‣ Subquestions Screening via Ground-truth Answer ‣ 3
    Distill the Decomposition Capability ‣ Divide-or-Conquer? Which Part Should You
    Distill Your LLM?")（双阶段模型）中展示了，将一个更强的分解器（GPT）替换为一个较弱的分解器（Vicuna），通常会导致性能显著下降，除了在GSM8K上使用Vicuna作为解算器的情况。我们假设原因在于Vicuna解算器的错误过多，无法充分利用分解的改进。我们观察到，当解算器更强大时，性能下降更为显著。这表明，为了达到最佳性能，强大的分解器是必不可少的。
- en: 5.2 Is Distilling Decomposition Easier than Distilling Solving?
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 提取分解能力是否比提取解决能力更容易？
- en: Next, we investigate distilling knowledge from $\mathcal{T}$ approach results
    in significantly lower cost for the decomposition compared to using the teacher
    GPT model. The cost of the solver remains relatively unchanged.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们研究了从$\mathcal{T}$方法中提取知识，这与使用教师GPT模型相比，分解的成本显著降低。解算器的成本保持相对不变。
- en: We compare some decompositions from $\mathcal{T}$, exhibits a high degree of
    similarity to the teacher demonstration in the generated subquestions on the unseen
    test set. In contrast, the original Vicuna model often generates unhelpful questions
    that have the potential to distract the solver.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了从$\mathcal{T}$获得的一些分解，发现其生成的子问题在未见过的测试集上与教师演示具有高度相似性。相比之下，原始的Vicuna模型经常生成不具帮助性的问题，这些问题可能会分散解算器的注意力。
- en: One might naturally wonder, if a smaller student model can quickly imitate the
    decomposition abilities of the teacher model, why is it challenging to acquire
    this skill directly through student model’s initial pretraining. Our hypothesis
    is that the decomposition ability of a stronger teacher model is easy to distill
    but difficult to acquire. This skill is likely based on the thorough digestion
    and internalization of vast amounts of data during the intensive pretraining of
    the larger models. However, as it is more logical and abstract rather than being
    knowledge-intensive, a few demonstrations may already provide ample guidance to
    the student. To draw an imperfect analogy, finding a physics theorem from massive
    observation is much more challenging than learning the theorem.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可能会自然地疑问，如果一个较小的学生模型可以迅速模仿教师模型的分解能力，为什么直接通过学生模型的初始预训练难以获得这种技能。我们的假设是，一个更强大的教师模型的分解能力容易提取，但难以直接获得。这种技能可能基于在较大模型的密集预训练过程中对大量数据的彻底消化和内化。然而，由于这种能力更具逻辑性和抽象性而非知识密集性，一些演示可能已经能为学生提供足够的指导。打个不完美的比喻，从大量观察中找到一个物理定理远比学习该定理更具挑战性。
- en: With available oracle answers
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结合现有的oracle答案
- en: Sometimes, we have access to the oracle answers $A$, which can be used to further
    enhance the model’s performance on specific domains through local adaptation and
    additional finetuning. As a result, the performance on these target domain can
    be beyond the performance of the black-box teacher model. We explore the options
    to enhance the models via distillation or target domain finetuning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们可以访问到预言者的答案 $A$，这可以通过局部适应和额外的微调进一步提升模型在特定领域的性能。因此，这些目标领域的性能可能超过黑箱教师模型的性能。我们探索了通过蒸馏或目标领域微调来提升模型的选项。
- en: In these scenarios, we can possibly use $A$ outperforms the Teacher model in
    terms of F1 score.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些场景中，我们可能会使用 $A$ 在 F1 分数上超越教师模型。
- en: We also finetune another Vicuna model for the solver using the ground-truth
    answers, referred to as $\mathcal{S}_{E}$(solver).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对另一个 Vicuna 模型进行了微调，使用了真实答案，称之为 $\mathcal{S}_{E}$(解答者)。
- en: Failure modes for $\mathcal{S}_{E}$ models
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $\mathcal{S}_{E}$ 模型的失败模式
- en: According to our observations, we hypothesize that there are two primary failure
    modes of the $\mathcal{S}_{E}$ models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的观察，我们假设 $\mathcal{S}_{E}$ 模型有两种主要的失败模式。
- en: First, answering either subquestions or primary questions would require extensive
    world knowledge and commonsense, which can be difficult to compress into a student
    model that is hundreds of times smaller, using only a few demonstrations. In other
    words, a strong solving capability is knowledge-intensive. On the other hand,
    decomposition capability might be more compressible as it is typically more abstract,
    has lower information density, and is more universal than solving capability.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，回答子问题或主要问题都需要广泛的世界知识和常识，这很难通过仅使用少量示例压缩到一个体积小数百倍的学生模型中。换句话说，强大的解答能力是知识密集型的。另一方面，分解能力可能更具可压缩性，因为它通常更抽象、信息密度较低，并且比解答能力更具普遍性。
- en: Second, since we used the teacher’s answers to the subquestions $\{\hat{A^{s}_{i}}\}$.
    (Examples are provided in Appendix [C](#A3 "Appendix C Examples Where Solver Models
    Become Confounded by Subquestions ‣ 9 Limitation ‣ 8 Conclusion ‣ Complement LLMs
    with Small models ‣ 7 Related Work ‣ 6 Ablations ‣ Generalization to other solvers
    ‣ 5.3 Is Distilling Decomposition More Generalizable than Distilling Solving?
    ‣ 5 Results ‣ Divide-or-Conquer? Which Part Should You Distill Your LLM?").)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，由于我们使用了教师对子问题的答案 $\{\hat{A^{s}_{i}}\}$。 （示例见附录 [C](#A3 "附录 C 示例：解答者模型在子问题上的困惑
    ‣ 9 局限性 ‣ 8 结论 ‣ 用小模型补充 LLMs ‣ 7 相关工作 ‣ 6 消融实验 ‣ 向其他解答者的推广 ‣ 5.3 提取分解是否比提取解答更具普遍性？
    ‣ 5 结果 ‣ 分而治之？你应该提取 LLM 的哪一部分？")。
- en: Based on above findings, we experimented with excluding the $\{\hat{A^{s}_{i}}\}$,
    and show the comparison results in Appendix [A](#A1 "Appendix A Exclusion of Answers
    to Subquestions ‣ 9 Limitation ‣ 8 Conclusion ‣ Complement LLMs with Small models
    ‣ 7 Related Work ‣ 6 Ablations ‣ Generalization to other solvers ‣ 5.3 Is Distilling
    Decomposition More Generalizable than Distilling Solving? ‣ 5 Results ‣ Divide-or-Conquer?
    Which Part Should You Distill Your LLM?").
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述发现，我们尝试了排除 $\{\hat{A^{s}_{i}}\}$ 的情况，并在附录 [A](#A1 "附录 A 排除对子问题答案的影响 ‣ 9
    局限性 ‣ 8 结论 ‣ 用小模型补充 LLMs ‣ 7 相关工作 ‣ 6 消融实验 ‣ 向其他解答者的推广 ‣ 5.3 提取分解是否比提取解答更具普遍性？
    ‣ 5 结果 ‣ 分而治之？你应该提取 LLM 的哪一部分？") 中展示了比较结果。
- en: 5.3 Is Distilling Decomposition More Generalizable than Distilling Solving?
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 提取分解是否比提取解答更具普遍性？
- en: Generalization to other domains
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向其他领域的推广
- en: We then investigate whether the distilled decomposer, which is trained on a
    specific domain dataset, can be applied to out-of-domain datasets with distinct
    objectives. To test this, we perform a cross-domain evaluation on DROP and GSM8K,
    which require different expertise from the solver. The results, when the oracle
    answer is available, are presented in Table [3](#S5.T3 "Table 3 ‣ Generalization
    to other solvers ‣ 5.3 Is Distilling Decomposition More Generalizable than Distilling
    Solving? ‣ 5 Results ‣ Divide-or-Conquer? Which Part Should You Distill Your LLM?").
    Surprisingly, the distilled decomposer $\mathcal{S}_{D}$, which is fine-tuned
    on the original domain, the generalization to the other domain is poor regardless
    of the decomposer used. Some examples of cross-domain subquestion decomposition
    are shown in Table [5.3](#S5.SS3.SSS0.Px2 "Generalization to other solvers ‣ 5.3
    Is Distilling Decomposition More Generalizable than Distilling Solving? ‣ 5 Results
    ‣ Divide-or-Conquer? Which Part Should You Distill Your LLM?"). The results on
    the scenario with no oracle answer are consistent with Table [3](#S5.T3 "Table
    3 ‣ Generalization to other solvers ‣ 5.3 Is Distilling Decomposition More Generalizable
    than Distilling Solving? ‣ 5 Results ‣ Divide-or-Conquer? Which Part Should You
    Distill Your LLM?").
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着调查了在特定领域数据集上训练的提炼分解器是否可以应用于具有不同目标的领域外数据集。为测试这一点，我们在DROP和GSM8K上进行跨领域评估，这些数据集要求求解器具备不同的专业知识。结果（当存在oracle答案时）见表[3](#S5.T3
    "表 3 ‣ 其他求解器的泛化 ‣ 5.3 提炼分解是否比提炼求解更具泛化性? ‣ 5 结果 ‣ 分而治之？你应该对你的 LLM 进行哪部分提炼？")。令人惊讶的是，在原始领域上进行微调的提炼分解器$\mathcal{S}_{D}$，无论使用什么分解器，对其他领域的泛化能力都较差。跨领域子问题分解的一些示例见表[5.3](#S5.SS3.SSS0.Px2
    "其他求解器的泛化 ‣ 5.3 提炼分解是否比提炼求解更具泛化性? ‣ 5 结果 ‣ 分而治之？你应该对你的 LLM 进行哪部分提炼？")。在没有oracle答案的情况下的结果与表[3](#S5.T3
    "表 3 ‣ 其他求解器的泛化 ‣ 5.3 提炼分解是否比提炼求解更具泛化性? ‣ 5 结果 ‣ 分而治之？你应该对你的 LLM 进行哪部分提炼？")一致。
- en: Generalization to other solvers
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对其他求解器的泛化
- en: Next, we examine whether the distilled decomposer is compatible and universally
    suitable for different solvers. The results can be seen in Table [4](#S5.T4 "Table
    4 ‣ Generalization to other solvers ‣ 5.3 Is Distilling Decomposition More Generalizable
    than Distilling Solving? ‣ 5 Results ‣ Divide-or-Conquer? Which Part Should You
    Distill Your LLM?"). The performance of $\mathcal{S}_{D}$ is comparable to that
    of the teacher decomposer (GPT), and it shows overall improvements over a weaker
    decomposer (Vicuna) when connected to different solvers. We found that weaker
    solvers receive more performance gain compared to strong solvers, through upgrading
    to a distilled decomposer. We hypothesize that the reason lies in the fact that
    the weaker solver may be incapable of fully utilizing the benefits of the decomposition.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们检查了提炼的分解器是否与不同的求解器兼容，并且是否普遍适用。结果见表[4](#S5.T4 "表 4 ‣ 其他求解器的泛化 ‣ 5.3 提炼分解是否比提炼求解更具泛化性?
    ‣ 5 结果 ‣ 分而治之？你应该对你的 LLM 进行哪部分提炼？")。$\mathcal{S}_{D}$的表现与教师分解器（GPT）相当，并且当连接到不同的求解器时，相对于较弱的分解器（Vicuna）表现出整体改进。我们发现，通过升级到提炼的分解器，较弱的求解器相比强求解器获得了更多的性能提升。我们推测原因在于较弱的求解器可能无法充分利用分解的好处。
- en: '| Dataset: DROP | Models | Decomposed Sub-questions |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 数据集: DROP | 模型 | 分解的子问题 |'
- en: '| Premise $P$: How many field goals did both teams kick in the first half?
    | Vicuna-13B | 1\. Which teams played against each other? X 2\. What were the
    scores for each team during the game? X 3\. Which team had the lead at the end
    of the game? X |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 前提 $P$: 两队在上半场踢了多少次进球？ | Vicuna-13B | 1\. 哪些球队对阵？ X 2\. 比赛中每队的得分是多少？ X 3\.
    比赛结束时哪队领先？ X |'
- en: '| GPT-3.5 | 1\. How many field goals did the Raiders kick in the first half?
    2\. How many field goals did the Texans kick in the first half? 3\. What is the
    sum of the field goals kicked by both teams in the first half? |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 1\. Raiders在上半场踢了多少次进球？ 2\. Texans在上半场踢了多少次进球？ 3\. 两队在上半场踢进球的总和是多少？
    |'
- en: '| $\mathcal{S}_{D}$(DROP) In-Domain | 1\. How many field goals did the Raiders
    kick in the first half? 2\. How many field goals did the Texans kick in the first
    half? |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{S}_{D}$(DROP) 同领域 | 1\. Raiders在上半场踢了多少次进球？ 2\. Texans在上半场踢了多少次进球？
    |'
- en: '| $\mathcal{S}_{D}$(GSM) Cross-Domain | 1\. How many field goals did the Raiders
    kick in the first half? 2\. How many field goals did the Texans kick in the first
    half? |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{S}_{D}$(GSM) 跨领域 | 1\. Raiders 在上半场踢了多少个进球？ 2\. Texans 在上半场踢了多少个进球？
    |'
- en: '| Dataset: GSM8K | Models | Decomposed Sub-questions |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 数据集：GSM8K | 模型 | 分解子问题 |'
- en: '| Premise $P$: How much does Mark make in a week, in cents? | Vicuna-13B |
    1\. What is the rate per sentence that Publisher B pays Mark? X 2\. What is the
    total amount Publisher A pays Mark for editing 1000 sentences? 3\. What is the
    total amount Publisher B pays Mark for editing 1000 sentences? |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 前提 $P$：Mark 每周赚多少钱，以美分为单位？ | Vicuna-13B | 1\. 出版商 B 每句支付 Mark 多少？ X 2\. 出版商
    A 为编辑 1000 句子支付 Mark 的总金额是多少？ 3\. 出版商 B 为编辑 1000 句子支付 Mark 的总金额是多少？ |'
- en: '| GPT-3.5 | 1\. How many sentences does Mark edit each week for Publisher A?
    2\. How many sentences does Mark edit each week for Publisher B? 3\. How much
    does Mark make per sentence from Publisher B? |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 1\. Mark 每周为出版商 A 编辑多少句子？ 2\. Mark 每周为出版商 B 编辑多少句子？ 3\. Mark 从出版商
    B 每句能赚多少？ |'
- en: '| $\mathcal{S}_{D}$(GSM) In-Domain | 1\. How many sentences does Mark edit
    for Publisher A in a week? 2\. How many sentences does Mark edit for Publisher
    B in a week? 3\. What is the rate per sentence paid by Publisher B? |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{S}_{D}$(GSM) 领域内 | 1\. Mark 每周为出版商 A 编辑多少句子？ 2\. Mark 每周为出版商 B
    编辑多少句子？ 3\. 出版商 B 每句的支付率是多少？ |'
- en: '| $\mathcal{S}_{D}$(DROP) Cross-Domain | 1: How much does Publisher A pay Mark
    per sentence? 2: How much does Publisher B pay Mark per sentence? 3: How many
    sentences does Mark edit in a week? |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{S}_{D}$(DROP) 跨领域 | 1：出版商 A 每句支付 Mark 多少？ 2：出版商 B 每句支付 Mark 多少？
    3：Mark 每周编辑多少句子？ |'
- en: 'Table 2: Examples for decomposed subquestions from each method on GSM8K and
    DROP. $\mathcal{S}_{D}$’s demontration on GSM8K and DROP datasets, respectively.
    X indicates not helpful subquestions.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：每种方法在 GSM8K 和 DROP 上的分解子问题示例。$\mathcal{S}_{D}$ 在 GSM8K 和 DROP 数据集上的演示。X
    表示不有帮助的子问题。
- en: '| Decomposer | GPT | $\mathcal{S}_{D}$ | GPT | - |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 分解器 | GPT | $\mathcal{S}_{D}$ | GPT | - |'
- en: '| Solver | GPT | GPT | $\mathcal{S}_{E}$ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 解算器 | GPT | GPT | $\mathcal{S}_{E}$ |'
- en: '| Trained on | Evaluation on DROP |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 训练于 | 在 DROP 上的评估 |'
- en: '| GSM8K | 55.73 | 51.05 | 7.98 | 17.22 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GSM8K | 55.73 | 51.05 | 7.98 | 17.22 |'
- en: '| Trained on | Evaluation on GSM8K |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 训练于 | 在 GSM8K 上的评估 |'
- en: '| DROP | 65.13 | 63.15 | 11.30 | 3.41 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| DROP | 65.13 | 63.15 | 11.30 | 3.41 |'
- en: 'Table 3: Distilled student decomposers demonstrate strong generalization over
    out-domain datasets.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：精炼的学生分解器在领域外数据集上展示了强大的泛化能力。
- en: '| Decomposor | Solver | GSM8K | DROP |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 分解器 | 解算器 | GSM8K | DROP |'
- en: '| GPT-3.5-Turbo | Vicuna-13B | 28.0 | 33.78 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | Vicuna-13B | 28.0 | 33.78 |'
- en: '| GPT-3.5-Turbo | 66.0 | 59.38 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | 66.0 | 59.38 |'
- en: '| GPT-4 | 90.5 | 77.60 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 90.5 | 77.60 |'
- en: '| Vicuna-13B | Vicuna-13B | 29.5 | 26.56 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13B | Vicuna-13B | 29.5 | 26.56 |'
- en: '| GPT-3.5-Turbo | 57.0 | 47.31 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | 57.0 | 47.31 |'
- en: '| GPT-4 | 88.5 | 79.40 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 88.5 | 79.40 |'
- en: '| $\mathcal{S}_{D}$ | Vicuna-13B | 31.5 | 33.38 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{S}_{D}$ | Vicuna-13B | 31.5 | 33.38 |'
- en: '| GPT-3.5-Turbo | 66.5 | 61.94 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | 66.5 | 61.94 |'
- en: '| GPT-4 | 91.5 | 81.02 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 91.5 | 81.02 |'
- en: 'Table 4: Distilled student decomposers demonstrate consistent improvements
    over different solvers. Weaker solvers receive more gain.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：精炼的学生分解器在不同解算器上展示了持续的改进。较弱的解算器获得更多的提升。
- en: 6 Ablations
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 消融研究
- en: We provide an extensive evaluation of various instructions, and an exploration
    into the influence of the number of demonstrations in Appendix [B](#A2 "Appendix
    B Ablation Study over Instruction for Decomposition ‣ 9 Limitation ‣ 8 Conclusion
    ‣ Complement LLMs with Small models ‣ 7 Related Work ‣ 6 Ablations ‣ Generalization
    to other solvers ‣ 5.3 Is Distilling Decomposition More Generalizable than Distilling
    Solving? ‣ 5 Results ‣ Divide-or-Conquer? Which Part Should You Distill Your LLM?").
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了对各种指令的广泛评估，并在附录[B](#A2 "附录 B 关于分解指令的消融研究 ‣ 9 局限性 ‣ 8 结论 ‣ 小模型补充 LLM ‣ 7
    相关工作 ‣ 6 消融研究 ‣ 对其他解算器的泛化 ‣ 5.3 精炼分解是否比精炼解算更具泛化性？ ‣ 5 结果 ‣ 分而治之？你应该精炼 LLM 的哪个部分？")
    中探索了演示数量的影响。
- en: 7 Related Work
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 相关工作
- en: LLM Distillation Tremendous progress  (Jiao et al., [2020](#bib.bib11); Sun
    et al., [2019](#bib.bib25); Li et al., [2021](#bib.bib13)) has been made in terms
    of compressing large-scale pre-trained language models such as BERT (Devlin et al.,
    [2019](#bib.bib6)) or RoBERTa (Liu et al., [2019](#bib.bib15)). For generative
    models, compression is predominantly achieved by minimizing the K-L divergence
    between teacher and student distributions (Sanh et al., [2019](#bib.bib22); Gu
    et al., [2023](#bib.bib9)). A pivotal assumption underlying these methods is the
    full accessibility of the teacher model’s components. However, most powerful LLMs
    are black boxes, revealing only limited outputs. Given these constraints, several
    methodologies have emerged that train directly on data generated by teacher models (Chiang
    et al., [2023b](#bib.bib4); Taori et al., [2023](#bib.bib26)). We follow a similar
    distillation strategy but focus on the decomposition capability distillation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 蒸馏方面取得了巨大的进展（Jiao 等， [2020](#bib.bib11)；Sun 等， [2019](#bib.bib25)；Li 等，
    [2021](#bib.bib13)），在压缩大规模预训练语言模型如 BERT（Devlin 等， [2019](#bib.bib6)）或 RoBERTa（Liu
    等， [2019](#bib.bib15)）方面取得了显著进展。对于生成模型，压缩主要通过最小化教师和学生分布之间的 K-L 散度来实现（Sanh 等， [2019](#bib.bib22)；Gu
    等， [2023](#bib.bib9)）。这些方法的一个关键假设是教师模型组件的完全可访问性。然而，大多数强大的 LLM 是黑箱，只显示有限的输出。鉴于这些限制，出现了几种直接在教师模型生成的数据上进行训练的方法（Chiang
    等， [2023b](#bib.bib4)；Taori 等， [2023](#bib.bib26)）。我们遵循类似的蒸馏策略，但专注于能力分解蒸馏。
- en: Planning and Task Decomposition of LLM-powered Agent
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 驱动的代理的规划和任务分解
- en: Recent advances in LLM-powered systems have made it possible to create an end-to-end
    pipeline, opening up new possibilities for developing autonomous agents that can
    complete complex tasks using enhanced planning and memory capabilities. Promising
    works, such as ReAct (Yao et al., [2022](#bib.bib38)), HuggingGPT (Shen et al.,
    [2023](#bib.bib23)), AutoGPT (Significant Gravitas, [2023](#bib.bib24)), LangChain
    (Langchain-AI, [2023](#bib.bib12)), GPT-Engineer (Anton Osika, [2023](#bib.bib1))
    and BabyAGI (Nakajima, [2023](#bib.bib17)), have demonstrated significant potential
    in this field. These agents rely on the LLM to decompose larger tasks into more
    manageable components. Among them, some approaches (e.g., HuggingGPT) use a static
    planning strategy by first generating the complete plan via LLM and subsequently
    tackling each subtask. Other approaches (e.g., AutoGPT) adopt a dynamic and interactive
    planning strategy, where the generation of each action is conditioned on the outcome
    of the previous planning steps.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 驱动的系统的最新进展使得创建端到端管道成为可能，为开发能够利用增强规划和记忆能力完成复杂任务的自主体打开了新可能。诸如 ReAct（Yao 等，
    [2022](#bib.bib38)）、HuggingGPT（Shen 等， [2023](#bib.bib23)）、AutoGPT（Significant
    Gravitas， [2023](#bib.bib24)）、LangChain（Langchain-AI， [2023](#bib.bib12)）、GPT-Engineer（Anton
    Osika， [2023](#bib.bib1)）和 BabyAGI（Nakajima， [2023](#bib.bib17)）等有前景的工作已在该领域展示了显著的潜力。这些代理依赖于
    LLM 将较大的任务分解为更易于管理的组件。其中一些方法（例如，HuggingGPT）通过首先通过 LLM 生成完整计划，然后逐步处理每个子任务，采用静态规划策略。其他方法（例如，AutoGPT）则采用动态和交互式规划策略，每个行动的生成以先前规划步骤的结果为条件。
- en: LLM Reasoning Chain
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 推理链
- en: LLMs can benefit from explicit reasoning chains, as demonstrated by recent studies
    (Wei et al., [2022](#bib.bib32); Zheng et al., [2023](#bib.bib40)). The Chain
    of Thought (CoT) (Wei et al., [2022](#bib.bib32)) technique has become standard
    for enhancing model performance on complex tasks. Tree of Thoughts (Yao et al.,
    [2023](#bib.bib37)) decomposes the problem into multiple thought steps and generates
    multiple thoughts per step, creating a tree structure. The LLM+P approach (Liu
    et al., [2023](#bib.bib14)) incorporates an external classical planner for long-horizon
    planning and translates the plan back into natural language. Theoretical work
    (Feng et al., [2023](#bib.bib8)) has analyzed why CoT works by using circuit complexity
    theory. It shows that without CoT, the model size would need to be prohibitively
    large to achieve the same performance through direct reasoning.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 可以从显式推理链中获益，正如近期的研究所示（Wei 等， [2022](#bib.bib32)；Zheng 等， [2023](#bib.bib40)）。链式思维（CoT）（Wei
    等， [2022](#bib.bib32)）技术已成为提升模型在复杂任务上表现的标准方法。思想树（Yao 等， [2023](#bib.bib37)）将问题分解为多个思考步骤，并在每一步生成多个思考，创建了一个树状结构。LLM+P
    方法（Liu 等， [2023](#bib.bib14)）结合了外部经典规划器进行长远规划，并将计划转回自然语言。理论工作（Feng 等， [2023](#bib.bib8)）通过使用电路复杂性理论分析了为何
    CoT 有效。它表明，如果没有 CoT，模型的规模需要过于庞大才能通过直接推理达到相同的性能。
- en: However, CoT-style reasoning is limited by the fact that it often generalizes
    poorly to problems beyond the scope of the provided in-context examples (Zhou
    et al., [2022](#bib.bib41)). To address this, some studies have asked LLMs to
    decompose complex questions into subquestions following the Least-to-Most prompt
    (Zhou et al., [2022](#bib.bib41)). Others have used the self-ask method to elicit
    follow-up questions that aid in addressing the original inquiry (Press et al.,
    [2022](#bib.bib21)). Our work contributes to this line of research by extending
    the horizon to cost-efficient inference and generalization across tasks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，CoT 风格的推理受到限制，因为它通常对超出提供的上下文示例范围的问题泛化能力较差（Zhou 等人，[2022](#bib.bib41)）。为了解决这个问题，一些研究要求
    LLM 将复杂问题分解为子问题，遵循从最少到最多的提示（Zhou 等人，[2022](#bib.bib41)）。其他研究使用自问方法来引出后续问题，以帮助解决原始问题（Press
    等人，[2022](#bib.bib21)）。我们的工作通过扩展到成本效益高的推理和跨任务的泛化，贡献了这一研究方向。
- en: Question Decompostion Datasets and Approaches
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题分解数据集和方法
- en: A widely recognized dataset for question decomposition in the literature is
    QDMR (Wolfson et al., [2020](#bib.bib35)). It comprises an ordered list of sub-questions
    essential for addressing a primary question. Several previous works have been
    training question decomposers on the QDMR dataset (Guo et al., [2022](#bib.bib10);
    Zhu et al., [2023](#bib.bib43)). In contrast, some research does not rely on QDMR
    but employs their uniquely labeled data. For instance,  Min et al. ([2019](#bib.bib16))
    recast question decomposition as a span prediction problem and trained their model
    on a set of 400 labeled questions. Recognizing the challenges associated with
    obtaining reliable decomposition data,  Perez et al. ([2020](#bib.bib20)) introduced
    an unsupervised decomposition approach, capitalizing on the similarity between
    the primary question and 10M potential sub-questions mined for decomposition purposes.
    Our approach differs from the aforementioned methodologies because we extract
    the decomposition power solely from the teacher model, without relying on any
    annotated subquestion.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，一个广泛认可的用于问题分解的数据集是 QDMR (Wolfson 等人，[2020](#bib.bib35))。它包括一个有序的子问题列表，这些子问题对于解决主要问题至关重要。以前的一些工作在
    QDMR 数据集上训练了问题分解器（Guo 等人，[2022](#bib.bib10)；Zhu 等人，[2023](#bib.bib43)）。相比之下，一些研究不依赖于
    QDMR，而是采用了他们独特标记的数据。例如，Min 等人 ([2019](#bib.bib16)) 将问题分解重新定义为跨度预测问题，并在一组 400 个标记问题上训练了他们的模型。鉴于获取可靠的分解数据面临的挑战，Perez
    等人 ([2020](#bib.bib20)) 提出了无监督分解方法，利用了主要问题与 10M 个潜在子问题之间的相似性。我们的方法与上述方法不同，因为我们完全从教师模型中提取分解能力，而不依赖于任何标注的子问题。
- en: Complement LLMs with Small models
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 补充 LLM 的小型模型
- en: There have been studies that have emphasized the potential of smaller, task-specific
    models to complement the predictions of LLM. Xu et al. ([2023](#bib.bib36)) explored
    a framework in which candidates produced by these task-specific models are fed
    to an LM, with a primary focus on classification tasks. Welleck et al. ([2022](#bib.bib33))
    train a smaller model to iteratively improve sequences generated by LMs. Vernikos
    et al. ([2023](#bib.bib29)) have demonstrated that collecting multiple erroneous
    outputs from LMs and using a small corrector model to unify the generation can
    significantly reduce errors. Our work can also be seen as developing a smaller
    decomposer model to activate the best performance of a large-scale LM.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究强调了较小的、任务特定的模型有可能补充大规模语言模型（LLM）的预测。Xu 等人 ([2023](#bib.bib36)) 探索了一个框架，其中由这些任务特定模型生成的候选项被输入到语言模型（LM）中，主要关注分类任务。Welleck
    等人 ([2022](#bib.bib33)) 训练一个较小的模型以迭代改进由 LMs 生成的序列。Vernikos 等人 ([2023](#bib.bib29))
    证明了从 LMs 收集多个错误输出，并使用小型纠错模型统一生成结果，可以显著减少错误。我们的工作也可以被看作是在开发一个较小的分解模型，以激发大规模 LM
    的最佳性能。
- en: 8 Conclusion
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: Our investigation provides a fine-grained examination of the LLM’s capability
    on reasoning tasks, by disentangling the decomposition and solving aspects. Although
    both capacities are vital for reasoning, we demonstrate that decomposition is
    less dependent on specific knowledge and thus easier to distill compared to distilling
    solving capabilities, regardless of the availability of ground truth labels. Additionally,
    the distilled decomposer shows strong generalization abilities across different
    tasks, datasets and executor/solvers. For future work, it would be interesting
    to train universal decomposer models using data from various tasks, and explore
    the use of reinforcement learning to further enhance the decomposer, leveraging
    the signal from the solver outcome. Another possible direction for future work
    is to assess the effectiveness of our method in other long-horizon planning tasks,
    including LLM-powered agent, tool use, and multiturn decision making.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究对 LLM 在推理任务上的能力进行了细致的审查，拆解了分解和解决方面。虽然这两种能力对推理都很重要，但我们证明了分解不依赖于特定知识，因此比提炼解决能力更容易，不论是否有真实标签。此外，提炼后的分解器在不同任务、数据集和执行者/解决者之间表现出强大的泛化能力。未来的工作中，将使用来自各种任务的数据训练通用分解器模型，并探索利用强化学习进一步增强分解器，以利用解决者结果的信号，将会是有趣的另一个方向。另一个未来工作的可能方向是评估我们方法在其他长期规划任务中的有效性，包括
    LLM 驱动的代理、工具使用和多轮决策制定。
- en: 9 Limitation
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 限制
- en: Our work is built upon several assumptions. First, we assume that the teacher
    model is capable of breaking down queries effectively. Second, we assume that
    the student model has the capacity to learn the distilled planning from the teacher
    model. Lastly, we assume that the tasks involved in our work require long horizon
    planning capability. If any of these assumptions do not hold true, it would impact
    the effectiveness of our proposed method.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作建立在几个假设之上。首先，我们假设教师模型能够有效地分解查询。其次，我们假设学生模型有能力学习教师模型提炼出的规划。最后，我们假设我们工作涉及的任务需要长期规划能力。如果这些假设中的任何一个不成立，将会影响我们提出方法的有效性。
- en: It is important to note that we have only assessed the effectiveness of our
    model in the context of math and QA aspects. In order to fully complete our work,
    it would be necessary to evaluate our model on a broader range of planning tasks.
    This would include benchmarks related to tool use, LLM agents, and multiturn scenarios.
    Such evaluations would help verify the versatility and applicability of our proposed
    method.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，我们只评估了我们模型在数学和问答方面的有效性。为了全面完成我们的工作，有必要在更广泛的规划任务范围内评估我们的模型。这将包括与工具使用、LLM代理和多轮场景相关的基准测试。这些评估将有助于验证我们提出方法的通用性和适用性。
- en: References
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Anton Osika (2023) Anton Osika. 2023. GPT Engineer. [https://github.com/AntonOsika/gpt-engineer/commits?author=AntonOsika](https://github.com/AntonOsika/gpt-engineer/commits?author=AntonOsika).
    GitHub repository.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anton Osika (2023) Anton Osika. 2023. GPT 工程师。 [https://github.com/AntonOsika/gpt-engineer/commits?author=AntonOsika](https://github.com/AntonOsika/gpt-engineer/commits?author=AntonOsika)。
    GitHub 仓库。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等. 2020. 语言模型是少样本学习者。 *神经信息处理系统进展*, 33:1877–1901。
- en: 'Chiang et al. (2023a) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023a. [Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang 等 (2023a) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu,
    Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
    Stoica 和 Eric P. Xing. 2023a. [Vicuna: 一款以90%* chatgpt 质量打动 gpt-4 的开源聊天机器人](https://lmsys.org/blog/2023-03-30-vicuna/)。'
- en: 'Chiang et al. (2023b) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
    et al. 2023b. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
    quality. *See https://vicuna. lmsys. org (accessed 14 April 2023)*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang 等 (2023b) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu,
    Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez 等.
    2023b. Vicuna: 一款以90%* chatgpt 质量打动 gpt-4 的开源聊天机器人。 *请参见 https://vicuna.lmsys.org
    (访问日期：2023年4月14日)*。'
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等（2021）Karl Cobbe、Vineet Kosaraju、Mohammad Bavarian、Mark Chen、Heewoo Jun、Lukasz
    Kaiser、Matthias Plappert、Jerry Tworek、Jacob Hilton、Reiichiro Nakano 等。2021年。训练验证器以解决数学文字问题。*arXiv
    预印本 arXiv:2110.14168*。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [BERT: Pre-training of deep bidirectional transformers for language
    understanding](https://doi.org/10.18653/v1/N19-1423). In *Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等（2019）Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。2019年。[BERT：用于语言理解的深度双向变换器的预训练](https://doi.org/10.18653/v1/N19-1423)。在
    *2019年北美计算语言学协会：人类语言技术年会论文集第1卷（长篇和短篇论文）*，第4171–4186页，明尼阿波利斯，明尼苏达州。计算语言学协会。
- en: 'Dua et al. (2019) Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky,
    Sameer Singh, and Matt Gardner. 2019. [DROP: A reading comprehension benchmark
    requiring discrete reasoning over paragraphs](https://doi.org/10.18653/v1/N19-1246).
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers)*, pages 2368–2378, Minneapolis, Minnesota. Association for Computational
    Linguistics.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dua 等（2019）Dheeru Dua、王一中、Pradeep Dasigi、Gabriel Stanovsky、Sameer Singh 和 Matt
    Gardner。2019年。[DROP：一个需要对段落进行离散推理的阅读理解基准](https://doi.org/10.18653/v1/N19-1246)。在
    *2019年北美计算语言学协会：人类语言技术年会论文集第1卷（长篇和短篇论文）*，第2368–2378页，明尼阿波利斯，明尼苏达州。计算语言学协会。
- en: 'Feng et al. (2023) Guhao Feng, Yuntian Gu, Bohang Zhang, Haotian Ye, Di He,
    and Liwei Wang. 2023. Towards revealing the mystery behind chain of thought: a
    theoretical perspective. *arXiv preprint arXiv:2305.15408*.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等（2023）冯古浩、谷云天、张博航、叶昊天、贺迪和王利伟。2023年。揭示思维链背后的神秘：理论视角。*arXiv 预印本 arXiv:2305.15408*。
- en: Gu et al. (2023) Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Knowledge
    distillation of large language models. *arXiv preprint arXiv:2306.08543*.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等（2023）顾宇贤、董磊、魏府如和黄敏磊。2023年。大规模语言模型的知识蒸馏。*arXiv 预印本 arXiv:2306.08543*。
- en: Guo et al. (2022) Xiao-Yu Guo, Yuan-Fang Li, and Gholamreza Haffari. 2022. [Complex
    reading comprehension through question decomposition](https://aclanthology.org/2022.alta-1.5).
    In *Proceedings of the The 20th Annual Workshop of the Australasian Language Technology
    Association*, pages 31–40, Adelaide, Australia. Australasian Language Technology
    Association.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等（2022）郭晓宇、李元芳和Gholamreza Haffari。2022年。[通过问题分解进行复杂阅读理解](https://aclanthology.org/2022.alta-1.5)。在
    *第20届澳大利亚语言技术协会年会论文集*，第31–40页，阿德莱德，澳大利亚。澳大利亚语言技术协会。
- en: 'Jiao et al. (2020) Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
    Linlin Li, Fang Wang, and Qun Liu. 2020. [TinyBERT: Distilling BERT for natural
    language understanding](https://doi.org/10.18653/v1/2020.findings-emnlp.372).
    In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages
    4163–4174, Online. Association for Computational Linguistics.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiao 等（2020）焦晓琪、尹一春、尚利锋、江鑫、陈晓、李琳琳、王芳和刘群。2020年。[TinyBERT：为自然语言理解蒸馏BERT](https://doi.org/10.18653/v1/2020.findings-emnlp.372)。在
    *计算语言学协会会议成果：EMNLP 2020*，第4163–4174页，在线。计算语言学协会。
- en: Langchain-AI (2023) Langchain-AI. 2023. [Langchain Github Repository](https://github.com/langchain-ai/langchain).
    GitHub repository.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Langchain-AI（2023）Langchain-AI。2023年。[Langchain Github 存储库](https://github.com/langchain-ai/langchain)。GitHub
    存储库。
- en: Li et al. (2021) Lei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou, and Xu Sun.
    2021. [Dynamic knowledge distillation for pre-trained language models](https://doi.org/10.18653/v1/2021.emnlp-main.31).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 379–389, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2021）李磊、林彦凯、任书海、李鹏、周洁和孙旭。2021年。[预训练语言模型的动态知识蒸馏](https://doi.org/10.18653/v1/2021.emnlp-main.31)。在
    *2021年自然语言处理实证方法会议论文集*，第379–389页，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Liu et al. (2023) Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas, and Peter Stone. 2023. Llm+ p: Empowering large language models
    with optimal planning proficiency. *arXiv preprint arXiv:2304.11477*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023) Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas, 和 Peter Stone. 2023. Llm+ p: 赋予大型语言模型优化规划能力。*arXiv 预印本 arXiv:2304.11477*。'
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
    Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, 和 Veselin Stoyanov. 2019.
    Roberta: 一种强健优化的 BERT 预训练方法。*arXiv 预印本 arXiv:1907.11692*。'
- en: Min et al. (2019) Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi.
    2019. [Multi-hop reading comprehension through question decomposition and rescoring](https://doi.org/10.18653/v1/P19-1613).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 6097–6109, Florence, Italy. Association for Computational
    Linguistics.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min et al. (2019) Sewon Min, Victor Zhong, Luke Zettlemoyer, 和 Hannaneh Hajishirzi.
    2019. [通过问题分解和重新评分进行多跳阅读理解](https://doi.org/10.18653/v1/P19-1613)。在 *第57届计算语言学协会年会论文集*，页码
    6097–6109，意大利佛罗伦萨。计算语言学协会。
- en: Nakajima (2023) Yohei Nakajima. 2023. [Babyagi](https://github.com/yoheinakajima/babyagi).
    GitHub repository.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakajima (2023) Yohei Nakajima. 2023. [Babyagi](https://github.com/yoheinakajima/babyagi)。GitHub
    仓库。
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](https://api.semanticscholar.org/CorpusID:257532815).
    *ArXiv*, abs/2303.08774.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](https://api.semanticscholar.org/CorpusID:257532815)。*ArXiv*,
    abs/2303.08774。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, 等. 2022. 训练语言模型以遵循人类反馈的指示。*神经信息处理系统进展*, 35:27730–27744。
- en: Perez et al. (2020) Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho,
    and Douwe Kiela. 2020. [Unsupervised question decomposition for question answering](https://doi.org/10.18653/v1/2020.emnlp-main.713).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 8864–8880, Online. Association for Computational Linguistics.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez et al. (2020) Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho,
    和 Douwe Kiela. 2020. [无监督问题分解用于问答](https://doi.org/10.18653/v1/2020.emnlp-main.713)。在
    *2020年自然语言处理实证方法会议 (EMNLP) 会议论文集*，页码 8864–8880，在线。计算语言学协会。
- en: Press et al. (2022) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A
    Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap
    in language models. *arXiv preprint arXiv:2210.03350*.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press et al. (2022) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah
    A Smith, 和 Mike Lewis. 2022. 测量并缩小语言模型中的组合性差距。*arXiv 预印本 arXiv:2210.03350*。
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. Distilbert, a distilled version of bert: Smaller, faster, cheaper
    and lighter. *arXiv preprint arXiv:1910.01108*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, 和 Thomas Wolf.
    2019. Distilbert, BERT 的精简版：更小、更快、更便宜、更轻量。*arXiv 预印本 arXiv:1910.01108*。
- en: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its
    friends in huggingface. *arXiv preprint arXiv:2303.17580*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, 和 Yueting Zhuang. 2023. Hugginggpt: 使用 ChatGPT 和其在 HuggingFace 的伙伴解决 AI 任务。*arXiv
    预印本 arXiv:2303.17580*。'
- en: 'Significant Gravitas (2023) Significant Gravitas. 2023. [Auto-gpt: An Autonomous
    GPT-4 Experiment](https://github.com/Significant-Gravitas/Auto-GPT). GitHub repository.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Significant Gravitas (2023) Significant Gravitas. 2023. [Auto-gpt: 一个自主 GPT-4
    实验](https://github.com/Significant-Gravitas/Auto-GPT)。GitHub 仓库。'
- en: Sun et al. (2019) Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. [Patient
    knowledge distillation for BERT model compression](https://doi.org/10.18653/v1/D19-1441).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 4323–4332, Hong Kong, China. Association for Computational
    Linguistics.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2019) Siqi Sun, Yu Cheng, Zhe Gan, 和 Jingjing Liu. 2019. [用于 BERT
    模型压缩的患者知识蒸馏](https://doi.org/10.18653/v1/D19-1441)。在*2019 年自然语言处理实证方法会议暨第九届国际自然语言处理联合会议（EMNLP-IJCNLP）论文集*中，页面
    4323–4332，香港，中国。计算语言学学会。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B Hashimoto. 2023. Stanford
    alpaca：一个跟随指令的 Llama 模型。
- en: 'Taylor et al. (2022) Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom,
    Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
    2022. Galactica: A large language model for science. *arXiv preprint arXiv:2211.09085*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taylor et al. (2022) Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom,
    Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, 和 Robert Stojnic.
    2022. Galactica：一个用于科学的大型语言模型。*arXiv 预印本 arXiv:2211.09085*。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等. 2023. Llama 2：开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*。
- en: Vernikos et al. (2023) Giorgos Vernikos, Arthur Bražinskas, Jakub Adamek, Jonathan
    Mallinson, Aliaksei Severyn, and Eric Malmi. 2023. Small language models improve
    giants by rewriting their outputs. *arXiv preprint arXiv:2305.13514*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vernikos et al. (2023) Giorgos Vernikos, Arthur Bražinskas, Jakub Adamek, Jonathan
    Mallinson, Aliaksei Severyn, 和 Eric Malmi. 2023. 小型语言模型通过重写其输出改进大型模型。*arXiv 预印本
    arXiv:2305.13514*。
- en: Wang et al. (2023) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023. A survey on
    large language model based autonomous agents. *arXiv preprint arXiv:2308.11432*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, 等. 2023. 基于大语言模型的自主代理调查。*arXiv
    预印本 arXiv:2308.11432*。
- en: Wang et al. (2024) Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu
    Li, Hao Peng, and Heng Ji. 2024. Executable code actions elicit better llm agents.
    *arXiv preprint arXiv:2402.01030*.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2024) Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu
    Li, Hao Peng, 和 Heng Ji. 2024. 可执行代码动作引发更好的 LLM 代理。*arXiv 预印本 arXiv:2402.01030*。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等. 2022. Chain-of-thought prompting 引发大语言模型的推理。*神经信息处理系统进展*,
    35:24824–24837。
- en: Welleck et al. (2022) Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao
    Shen, Daniel Khashabi, and Yejin Choi. 2022. Generating sequences by learning
    to self-correct. *arXiv preprint arXiv:2211.00053*.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Welleck et al. (2022) Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao
    Shen, Daniel Khashabi, 和 Yejin Choi. 2022. 通过学习自我纠正生成序列。*arXiv 预印本 arXiv:2211.00053*。
- en: 'Weng (2023) Lilian Weng. 2023. [Llm powered autonomous agents](https://lilianweng.github.io/posts/2023-06-23).
    Accessed: 2024-02-13.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng (2023) Lilian Weng. 2023. [由 Llm 驱动的自主代理](https://lilianweng.github.io/posts/2023-06-23)。访问时间：2024-02-13。
- en: 'Wolfson et al. (2020) Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav
    Goldberg, Daniel Deutch, and Jonathan Berant. 2020. [Break it down: A question
    understanding benchmark](https://doi.org/10.1162/tacl_a_00309). *Transactions
    of the Association for Computational Linguistics*, 8:183–198.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolfson et al. (2020) Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav
    Goldberg, Daniel Deutch, 和 Jonathan Berant. 2020. [分解问题：一个问题理解基准](https://doi.org/10.1162/tacl_a_00309)。*计算语言学学会会刊*,
    8:183–198。
- en: Xu et al. (2023) Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu,
    and Julian McAuley. 2023. Small models are valuable plug-ins for large language
    models. *arXiv preprint arXiv:2305.08848*.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2023) Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu,
    和 Julian McAuley. 2023. 小型模型是大型语言模型有价值的插件。*arXiv 预印本 arXiv:2305.08848*。
- en: 'Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate
    problem solving with large language models. *arXiv preprint arXiv:2305.10601*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等（2023）Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths,
    Yuan Cao, 和 Karthik Narasimhan。2023年。思维树：利用大型语言模型进行深思熟虑的问题解决。*arXiv 预印本 arXiv:2305.10601*。
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等（2022）Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
    Narasimhan, 和 Yuan Cao。2022年。React：在语言模型中协同推理与行动。*arXiv 预印本 arXiv:2210.03629*。
- en: 'Yue et al. (2023) Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan
    Sun, Yu Su, and Wenhu Chen. 2023. Mammoth: Building math generalist models through
    hybrid instruction tuning. *arXiv preprint arXiv:2309.05653*.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yue 等（2023）Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun,
    Yu Su, 和 Wenhu Chen。2023年。Mammoth：通过混合指令调整构建数学通用模型。*arXiv 预印本 arXiv:2309.05653*。
- en: Zheng et al. (2023) Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and
    Yu Li. 2023. Progressive-hint prompting improves reasoning in large language models.
    *arXiv preprint arXiv:2304.09797*.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2023）Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, 和 Yu Li。2023年。渐进提示提示提高了大型语言模型的推理能力。*arXiv
    预印本 arXiv:2304.09797*。
- en: Zhou et al. (2022) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al.
    2022. Least-to-most prompting enables complex reasoning in large language models.
    *arXiv preprint arXiv:2205.10625*.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2022）Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales,
    Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le 等。2022年。最小到最大提示使大型语言模型能够进行复杂推理。*arXiv
    预印本 arXiv:2205.10625*。
- en: 'Zhu et al. (2021) Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya
    Poria, and Tat-Seng Chua. 2021. Retrieving and reading: A comprehensive survey
    on open-domain question answering. *arXiv preprint arXiv:2101.00774*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2021）Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria,
    和 Tat-Seng Chua。2021年。检索与阅读：开放域问答的综合调查。*arXiv 预印本 arXiv:2101.00774*。
- en: Zhu et al. (2023) Wang Zhu, Jesse Thomason, and Robin Jia. 2023. Chain-of-questions
    training with latent answers for robust multistep question answering. *arXiv preprint
    arXiv:2305.14901*.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2023）Wang Zhu, Jesse Thomason, 和 Robin Jia。2023年。利用潜在答案进行鲁棒的多步骤问题回答的链式问题训练。*arXiv
    预印本 arXiv:2305.14901*。
- en: Appendix
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Exclusion of Answers to Subquestions
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 排除子问题的答案
- en: '|  | Decomposer | Solver | Performance$\uparrow$ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | 分解器 | 解答者 | 性能$\uparrow$ |'
- en: '|  | Model | Model | GSM8K (EM) | DROP (F1) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型 | 模型 | GSM8K (EM) | DROP (F1) |'
- en: '| w/o oracle answer $A$(Direct) | 5.46 | 53.17 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 不带 oracle 答案 $A$(直接) | 5.46 | 53.17 |'
- en: '| GPT | $\mathcal{S}_{E}$ | 48.98 | 13.37 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| GPT | $\mathcal{S}_{E}$ | 48.98 | 13.37 |'
- en: '| w/ oracle answer $A$(Direct) | 6.44 | 72.55 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 带有 oracle 答案 $A$(直接) | 6.44 | 72.55 |'
- en: '| GPT | $\mathcal{S}_{E}$ | 51.55 | 20.34 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| GPT | $\mathcal{S}_{E}$ | 51.55 | 20.34 |'
- en: 'Table 5: Excluding answers to subquestions $\{\hat{A^{s}_{i}}\}$ from the target
    yields improved results over the DROP dataset, but leads to a decrease in performance
    over the GSM8K dataset.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：将子问题的答案 $\{\hat{A^{s}_{i}}\}$ 从目标中排除会使在DROP数据集上的结果得到改进，但会导致在GSM8K数据集上的性能下降。
- en: We hypothesize that for tasks involving mathematical reasoning, the answers
    typically necessitate some form of computation, making a step-by-step solution
    essential. Without this, setting a numerical value as the fine-tuning target almost
    invariably results in failure. Conversely, DROP, being a reading comprehension
    dataset, derives a significant portion of its answers directly from the provided
    text. In such scenarios, including answers to subquestions poses a risk of disrupting
    the answer distributions.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设，对于涉及数学推理的任务，答案通常需要某种形式的计算，因此逐步解决方案是必不可少的。没有这一点，将数值设定为微调目标几乎总是会导致失败。相反，DROP作为一个阅读理解数据集，其答案在很大程度上直接来源于提供的文本。在这种情况下，包括对子问题的答案存在扰乱答案分布的风险。
- en: The instruction for solving, denoted as $I^{\prime}_{\text{ans}}$. The only
    difference comes from the fine-tuning target.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 解决指令，记作 $I^{\prime}_{\text{ans}}$。唯一的区别来自微调目标。
- en: Appendix B Ablation Study over Instruction for Decomposition
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 分解指令的消融研究
- en: '| Decomposor | Solver | 0-shot | 1-shot |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 分解器 | 解答者 | 0-shot | 1-shot |'
- en: '| GPT-3.5-Turbo | GPT-3.5-Turbo | 66.0 | 70.0 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | GPT-3.5-Turbo | 66.0 | 70.0 |'
- en: '| GPT-4 | 90.5 | 91.5 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 90.5 | 91.5 |'
- en: '| Vicuna-13B | GPT-3.5-Turbo | 57.0 | 61.5 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13B | GPT-3.5-Turbo | 57.0 | 61.5 |'
- en: '| GPT-4 | 88.5 | 91.5 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 88.5 | 91.5 |'
- en: '| $\mathcal{S}_{D}$ | GPT-3.5-Turbo | 66.5 | 67.5 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{S}_{D}$ | GPT-3.5-Turbo | 66.5 | 67.5 |'
- en: '| GPT-4 | 91.5 | 91.5 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 91.5 | 91.5 |'
- en: 'Table 6: Impact of including demonstration in decomposition instruction, examined
    on a subset of GSM8K dataset.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 在分解指令中包含示例的影响，考察了 GSM8K 数据集的一个子集。'
- en: Prior research has demonstrated that incorporating demonstrations within prompts
    can significantly enhance the ability of Large Language Models to adhere to given
    instructions. Our findings in Table [6](#A2.T6 "Table 6 ‣ Appendix B Ablation
    Study over Instruction for Decomposition ‣ 9 Limitation ‣ 8 Conclusion ‣ Complement
    LLMs with Small models ‣ 7 Related Work ‣ 6 Ablations ‣ Generalization to other
    solvers ‣ 5.3 Is Distilling Decomposition More Generalizable than Distilling Solving?
    ‣ 5 Results ‣ Divide-or-Conquer? Which Part Should You Distill Your LLM?") further
    substantiate this, revealing that including a single-shot demonstration notably
    improves the quality of decomposed questions. This enhancement has been consistently
    observed across a variety of decomposers.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究表明，将示例纳入提示中可以显著增强大型语言模型遵循给定指令的能力。我们在表[6](#A2.T6 "Table 6 ‣ Appendix B Ablation
    Study over Instruction for Decomposition ‣ 9 Limitation ‣ 8 Conclusion ‣ Complement
    LLMs with Small models ‣ 7 Related Work ‣ 6 Ablations ‣ Generalization to other
    solvers ‣ 5.3 Is Distilling Decomposition More Generalizable than Distilling Solving?
    ‣ 5 Results ‣ Divide-or-Conquer? Which Part Should You Distill Your LLM?")中的发现进一步证实了这一点，揭示了包括一个单次示例显著提高了分解问题的质量。这种提升在各种分解器中都被一致观察到。
- en: '| Instruction | EM | f1 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 指令 | EM | f1 |'
- en: '| no restriction | 45.69 | 56.63 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 无限制 | 45.69 | 56.63 |'
- en: '| no more than four | 46.40 | 57.19 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 不超过四项 | 46.40 | 57.19 |'
- en: '| no more than three | 50.00 | 59.88 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 不超过三项 | 50.00 | 59.88 |'
- en: '| no more than two | 46.89 | 58.47 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 不超过两项 | 46.89 | 58.47 |'
- en: 'Table 7: Effect of limiting the maximum number of subquestions in decomposition
    instructions on a subset of the DROP dataset.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 限制分解指令中子问题最大数量对 DROP 数据集子集的影响。'
- en: We have conducted an ablation study focusing on the instructions used for question
    decomposition. Our goal is for the resulting subquestions to act as useful cues
    for the executor, all the while ensuring they do not introduce unnecessary information.
    Central to our design rationale is determining the optimal number of subquestions
    the decomposer should produce. More specifically, we analyzed outcomes where no
    restrictions were applied (removing the highlighted part in $I_{\text{decomp}}$)
    and compared these against scenarios with varying maximum numbers of subquestions
    allowed. The results of these investigations are detailed in Table [7](#A2.T7
    "Table 7 ‣ Appendix B Ablation Study over Instruction for Decomposition ‣ 9 Limitation
    ‣ 8 Conclusion ‣ Complement LLMs with Small models ‣ 7 Related Work ‣ 6 Ablations
    ‣ Generalization to other solvers ‣ 5.3 Is Distilling Decomposition More Generalizable
    than Distilling Solving? ‣ 5 Results ‣ Divide-or-Conquer? Which Part Should You
    Distill Your LLM?"). Our findings succinctly reveal that a cap of "no more than
    three subquestions" yields the most effective results.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一个专注于问题分解指令的消融研究。我们的目标是使结果子问题作为执行者的有用线索，同时确保它们不会引入不必要的信息。我们设计的核心依据是确定分解器应生成的最佳子问题数量。更具体地，我们分析了在没有限制的情况下的结果（移除
    $I_{\text{decomp}}$ 中的高亮部分），并将这些结果与允许的最大子问题数量不同的场景进行比较。这些调查结果的详细信息见表[7](#A2.T7
    "Table 7 ‣ Appendix B Ablation Study over Instruction for Decomposition ‣ 9 Limitation
    ‣ 8 Conclusion ‣ Complement LLMs with Small models ‣ 7 Related Work ‣ 6 Ablations
    ‣ Generalization to other solvers ‣ 5.3 Is Distilling Decomposition More Generalizable
    than Distilling Solving? ‣ 5 Results ‣ Divide-or-Conquer? Which Part Should You
    Distill Your LLM?")。我们的发现简洁地揭示了“最多三个子问题”的限制产生了最有效的结果。
- en: 'Instruction for decomposition:
    $I_{\text{decomp}}$'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 'Instruction for decomposition:
    $I_{\text{decomp}}$'
- en: Appendix C Examples Where Solver Models Become Confounded by Subquestions
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 解决器模型因子问问题而混淆的示例
- en: As illustrated in Figure [2](#A3.F2 "Figure 2 ‣ Appendix C Examples Where Solver
    Models Become Confounded by Subquestions ‣ 9 Limitation ‣ 8 Conclusion ‣ Complement
    LLMs with Small models ‣ 7 Related Work ‣ 6 Ablations ‣ Generalization to other
    solvers ‣ 5.3 Is Distilling Decomposition More Generalizable than Distilling Solving?
    ‣ 5 Results ‣ Divide-or-Conquer? Which Part Should You Distill Your LLM?"), up
    to the second subquestion, the solver model accurately responds that "The robe
    requires 2 bolts of blue fiber" and "it would need 1 bolt of white fiber." Nevertheless,
    the introduction of the third subquestion, closely resembling the second, leads
    to confusion. Consequently, the model deviates from its initial accuracy, culminating
    in an incorrect answer following this subquestion.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2](#A3.F2 "图 2 ‣ 附录 C 示例 ‣ 求解器模型被子问题困扰 ‣ 9 限制 ‣ 8 结论 ‣ 用小模型补充大模型 ‣ 7 相关工作
    ‣ 6 消融实验 ‣ 对其他求解器的泛化 ‣ 5.3 分解蒸馏是否比求解蒸馏更具泛化性？ ‣ 5 结果 ‣ 分而治之？你应该蒸馏 LLM 的哪个部分？")
    所示，到第二个子问题为止，求解器模型准确地回应了“这件长袍需要 2 卷蓝色纤维”和“还需要 1 卷白色纤维”。然而，引入第三个子问题（与第二个问题非常相似）会导致混淆。因此，模型偏离了最初的准确性，最终在回答这个子问题时出现了错误。
- en: '![Refer to caption](img/10d5955f1a7953525bee085e3afb1a0e.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/10d5955f1a7953525bee085e3afb1a0e.png)'
- en: 'Figure 2: Solver models get lost sometimes.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：求解器模型有时会迷失方向。
