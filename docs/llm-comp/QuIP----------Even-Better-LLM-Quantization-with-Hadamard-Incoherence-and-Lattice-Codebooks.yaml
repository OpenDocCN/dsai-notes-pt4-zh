- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:50:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:12
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'QuIP # # \# : Even Better LLM Quantization with Hadamard Incoherence and Lattice
    Codebooks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'QuIP # # \# : 更好的LLM量化，具有Hadamard不一致性和晶格码本'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.04396](https://ar5iv.labs.arxiv.org/html/2402.04396)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.04396](https://ar5iv.labs.arxiv.org/html/2402.04396)
- en: Albert Tseng    Jerry Chee    Qingyao Sun    Volodymyr Kuleshov    Christopher
    De Sa
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Albert Tseng    Jerry Chee    Qingyao Sun    Volodymyr Kuleshov    Christopher
    De Sa
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing
    their weights to low-precision. In this work, we introduce QuIP$\#$ outperforms
    existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast
    inference.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化（PTQ）通过将模型权重量化为低精度来减少LLMs的内存占用。在这项工作中，我们介绍了QuIP$\#$，它优于现有的PTQ方法，实现了PTQ规模的新行为，并支持快速推理。
- en: Machine Learning, ICML, Quantization, Large Language Models, LLMs, Low Precision,
    Inference, Systems, Hardware, 2 bit, QuIP, Incoherence Processing, Lattice Codebooks,
    Vector Quantization
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML，量化，大型语言模型，LLMs，低精度，推理，系统，硬件，2位，QuIP，不一致性处理，晶格码本，向量量化
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/51fdeabf9fc2b906f024787f6f223bb6.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/51fdeabf9fc2b906f024787f6f223bb6.png)'
- en: 'Figure 1: QuIP$\#$ 3-bit models also scale better than theoretically lossless
    4-bit models, a previously unseen result.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：QuIP$\#$ 3位模型比理论上无损的4位模型更具扩展性，这是一个前所未见的结果。
- en: '![Refer to caption](img/c5946710a724a1ff952a59572639ed51.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c5946710a724a1ff952a59572639ed51.png)'
- en: 'Figure 2: QuIP$\#$ performs incoherence processing with a Randomized Hadamard
    Transform and uses lattice codebooks to achieve state-of-the-art quantized models.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：QuIP$\#$ 使用随机Hadamard变换进行不一致性处理，并利用晶格码本实现了最先进的量化模型。
- en: Large language models (LLMs) have driven rapid advances across diverse fields
    such as natural language processing (Touvron et al., [2023b](#bib.bib30)), scientific
    modeling (Nguyen et al., [2023](#bib.bib24)), and program synthesis (Rozière et al.,
    [2024](#bib.bib25)). However, the massive size of these models poses significant
    challenges to their deployment. For example, the largest model in the Llama2 family
    has 70B parameters, and requires 140GB of GPU memory in native 16-bit precision
    (Touvron et al., [2023b](#bib.bib30)). This massive memory footprint motivates
    research into methods that can compress LLMs without sacrificing quality.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在自然语言处理（Touvron et al., [2023b](#bib.bib30)）、科学建模（Nguyen et al.,
    [2023](#bib.bib24)）和程序合成（Rozière et al., [2024](#bib.bib25)）等多个领域推动了快速的进步。然而，这些模型的巨大规模对其部署带来了显著挑战。例如，Llama2系列中最大的模型拥有70B参数，在本地16位精度下需要140GB的GPU内存（Touvron
    et al., [2023b](#bib.bib30)）。这种巨大的内存占用促使研究者探索能够压缩LLMs而不牺牲质量的方法。
- en: Post-training quantization (PTQ) reduces the memory requirements of large models
    by converting trained weights to a lower precision. For example, with 2-bit quantization,
    a 16-bit LLama2 model with 70B parameters fits on a single consumer-grade 24GB
    GPU and benefits from increased inference throughput (Cai et al., [2024](#bib.bib2)).
    However, 2-bit quantization also often reduces the quality of the model and pushes
    the limits of PTQ algorithms (Chee et al., [2023](#bib.bib4)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化（PTQ）通过将训练好的权重转换为较低精度，减少了大型模型的内存需求。例如，通过2位量化，具有70B参数的16位LLama2模型可以适配于单个消费级24GB
    GPU，并且推理吞吐量得到了提升（Cai et al., [2024](#bib.bib2)）。然而，2位量化通常会降低模型质量，并推高PTQ算法的极限（Chee
    et al., [2023](#bib.bib4)）。
- en: 'In this work, we introduce QuIP$\#$ includes an inter-layer fine-tuning algorithm
    that further improves quantization quality (Section [5](#S5 "5 Fine-Tuning During
    Quantization ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence and
    Lattice Codebooks")).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们介绍了QuIP$\#$，它包括一个层间微调算法，进一步提高了量化质量（见[第5节](#S5 "5 Fine-Tuning During
    Quantization ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence and
    Lattice Codebooks)")。'
- en: These developments allow QuIP$\#$ is also the first PTQ method where 3-bit models
    scale better than 4-bit models. This directly refutes Dettmers & Zettlemoyer ([2023](#bib.bib9))’s
    claim that 4-bit models are “optimal” and indicates that as the field of PTQ develops,
    2-bit models are likely to scale better than 3-bit models in the near future.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发展使得QuIP$\#$也成为第一个3位模型比4位模型扩展性更好的PTQ方法。这直接反驳了Dettmers & Zettlemoyer ([2023](#bib.bib9))声称4位模型是“最佳”的说法，并且表明随着PTQ领域的发展，2位模型在不久的将来可能会比3位模型更具扩展性。
- en: Finally, we note that QuIP$\#$ achieves over 50% of peak memory bandwidth on
    a NVIDIA RTX 4090, validating our design choices.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们注意到 QuIP$\#$ 在 NVIDIA RTX 4090 上实现了超过 50% 的峰值内存带宽，验证了我们的设计选择。
- en: In summary, we introduce QuIP$\#$, a post-training quantization method that
    achieves state-of-the-art results by
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们介绍了 QuIP$\#$，这是一种通过
- en: '1.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Performing incoherence processing with the Randomized Hadamard Transform, which
    has better incoherence properties and faster runtime than the Kronecker factorization
    in QuIP.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用随机哈达玛变换进行非一致性处理，该变换在非一致性属性和运行时间方面比 QuIP 中的克罗内克分解更优。
- en: '2.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Rounding incoherence-processed weight matrices with block adaptive rounding
    and codebooks based on the $E_{8}$ lattice, which achieves the highest 8-dimension
    unit ball packing density (kissing number).
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对经过非一致性处理的权重矩阵进行块自适应舍入，并使用基于 $E_{8}$ 格点的代码本，这实现了最高的 8 维单位球体包装密度（接吻数）。
- en: '3.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Introducing an inter-layer fine-tuning algorithm that further improves quantization
    quality.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 引入了一种层间微调算法，进一步提高量化质量。
- en: Algorithm 1 QuIP$\#$-NoFT)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 QuIP$\#$-NoFT)
- en: 0:  Weight $W\in\mathbb{R}^{m\times n}$
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '0:  权重 $W\in\mathbb{R}^{m\times n}$'
- en: Algorithm 2 QuIP$\#$ Inference (for a Linear Layer)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 QuIP$\#$ 推理（用于线性层）
- en: 0:  $\hat{W}$
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '0:  $\hat{W}$'
- en: 2 Background / Related Work
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景 / 相关工作
- en: 2.1 Compressing LLMs
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 压缩大语言模型（LLMs）
- en: A large body of work has focused on compressing LLMs, as doing so can directly
    benefit LLM inference at scale. Methods such as pruning, quantization aware training
    (QAT), and post-training quantization (PTQ) all focus on different areas of this
    problem and are not strictly orthogonal to each other. Pruning removes weights
    from models while preserving model quality and inference performance (Chee et al.,
    [2022](#bib.bib3); Sun et al., [2023](#bib.bib28)). QAT focuses on training models
    that are more “quantizable” but usually requires training models from scratch
    (Nagel et al., [2022](#bib.bib23)). PTQ, which QuIP$\#$ falls under, instead quantizes
    pre-trained models. PTQ generally requires much less compute than QAT and achieve
    competitive performance (Chee et al., [2023](#bib.bib4); Frantar et al., [2023](#bib.bib12);
    Shao et al., [2024](#bib.bib26); Egiazarian et al., [2024](#bib.bib10)). For the
    rest of this paper, we focus on the PTQ realm of LLM compression.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 大量工作集中在压缩大语言模型（LLMs）上，因为这样做可以直接使 LLM 推理在大规模下受益。诸如剪枝、量化感知训练（QAT）和后训练量化（PTQ）等方法都关注这一问题的不同方面，并且它们之间并非完全正交。剪枝在保留模型质量和推理性能的同时移除模型中的权重（Chee
    et al., [2022](#bib.bib3); Sun et al., [2023](#bib.bib28)）。QAT 专注于训练更“可量化”的模型，但通常需要从头开始训练模型（Nagel
    et al., [2022](#bib.bib23)）。PTQ，即 QuIP$\#$ 所属的方法，则对预训练模型进行量化。PTQ 通常比 QAT 需要的计算量少得多，并且表现具有竞争力（Chee
    et al., [2023](#bib.bib4); Frantar et al., [2023](#bib.bib12); Shao et al., [2024](#bib.bib26);
    Egiazarian et al., [2024](#bib.bib10)）。在本文的其余部分，我们将关注 LLM 压缩中的 PTQ 领域。
- en: 2.2 Quantization and Adaptive Rounding
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 量化与自适应舍入
- en: 'In QuIP$\#$, we follow existing state-of-the-art PTQ methods and round weights
    to minimize the per-layer proxy loss, as formalized by Nagel et al. ([2020](#bib.bib22)):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 QuIP$\#$ 中，我们遵循现有的最先进 PTQ 方法，并舍入权重以最小化每层的代理损失，如 Nagel et al. ([2020](#bib.bib22))
    所正式化的：
- en: '|  | $\displaystyle\ell(\hat{W})$ |  | (1) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\ell(\hat{W})$ |  | (1) |'
- en: '|  |  | $\displaystyle=\operatorname{tr}\left((\hat{W}-W)H(\hat{W}-W)^{T}\right).$
    |  | (2) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\operatorname{tr}\left((\hat{W}-W)H(\hat{W}-W)^{T}\right).$
    |  | (2) |'
- en: Here, $W\in\mathbb{R}^{m\times n}$ is to use adaptive rounding methods that
    iteratively round weight matrices by considering the current rounding error for
    that specific matrix. For example, the LDLQ¹¹1OPTQ(Frantar et al., [2023](#bib.bib12))
    and QuIP independently introduced alternative formulations of this rounding method,
    and QuIP showed them to be equivalent. LDLQ is the name given by QuIP. rounding
    algorithm iteratively rounds rows of model weights using linear feedback from
    quantization error of already rounded rows. LDLQ is optimal within the class of
    adaptive rounding methods with linear feedback and offers provably better error
    rates than nearest or stochastic rounding (Chee et al., [2023](#bib.bib4)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$W\in\mathbb{R}^{m\times n}$ 是使用自适应舍入方法，通过考虑特定矩阵的当前舍入误差来迭代舍入权重矩阵。例如，LDLQ¹¹1OPTQ（Frantar
    et al., [2023](#bib.bib12)）和 QuIP 独立地引入了这种舍入方法的替代公式，QuIP 证明了它们是等效的。LDLQ 是 QuIP
    给予舍入算法的名称，该算法使用线性反馈从已经舍入的行的量化误差中迭代地舍入模型权重的行。LDLQ 在具有线性反馈的自适应舍入方法中是最优的，并提供了比最近邻或随机舍入更好的误差率（Chee
    et al., [2023](#bib.bib4)）。
- en: 2.3 Incoherence Processing
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 非一致性处理
- en: Multiple works have observed that outliers in model activations and weights
    can hinder quantization quality, motivating methods that “suppress” outliers during
    quantization. For example, AWQ (Lin et al., [2023](#bib.bib20)) scales model weights
    by information from activations and OmniQuant (Shao et al., [2024](#bib.bib26))
    uses simple learnable model-preserving transformations. However, these heuristic-based
    approaches tend to fail at lower bitrates.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 多项研究观察到模型激活和权重中的异常值可能会阻碍量化质量，促使了一些“抑制”异常值的量化方法。例如，AWQ（Lin等人，[2023](#bib.bib20)）通过激活信息来缩放模型权重，OmniQuant（Shao等人，[2024](#bib.bib26)）使用简单的可学习模型保持变换。然而，这些基于启发式的方法在较低比特率下往往表现不佳。
- en: Instead, in QuIP, Chee et al. ([2023](#bib.bib4)) proposed that *incoherence*
    is important for LLM quantization. Informally, incoherent matrices have concentrated
    entry magnitudes—ruling out outliers. In LLMs, incoherent weight and Hessian matrices
    mean that both the thing being rounded (weights) and important rounding directions
    (Hessians) are not too large in any coordinate. This enables quantization with
    *provably* bounded error.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在QuIP中，Chee等人（[2023](#bib.bib4)）提出了*非一致性*对于LLM量化的重要性。非一致性矩阵的条目幅度集中——排除了异常值。在LLM中，非一致的权重和Hessian矩阵意味着被四舍五入的事物（权重）和重要的四舍五入方向（Hessian）在任何坐标中都不会太大。这使得量化具有*可证明*的有界误差。
- en: Definition 2.1  (Chee et al. ([2023](#bib.bib4))).
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2.1（Chee等人（[2023](#bib.bib4)））。
- en: A Hessian $H\in\mathbb{R}^{n\times n}$ has
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Hessian $H\in\mathbb{R}^{n\times n}$ 具有
- en: '|  | $\textstyle\max_{i,j}\;&#124;Q_{ij}&#124;=\max_{i,j}\;&#124;e_{i}^{T}Qe_{j}&#124;\leq\mu/\sqrt{n}.$
    |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textstyle\max_{i,j}\;\vert Q_{ij}\vert = \max_{i,j}\;\vert e_{i}^{T}Qe_{j}\vert
    \leq \mu/\sqrt{n}.$ |  |'
- en: A weight matrix $W\in\mathbb{R}^{m\times n}$-incoherent if
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 权重矩阵$W\in\mathbb{R}^{m\times n}$-非一致的条件是
- en: '|  | $\max_{i,j}\;\textstyle&#124;W_{ij}&#124;=\max_{i,j}\;&#124;e_{i}^{T}We_{j}&#124;\leq\mu\&#124;W\&#124;_{F}/\sqrt{mn}.$
    |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{i,j}\;\textstyle\vert W_{ij}\vert = \max_{i,j}\;\vert e_{i}^{T}We_{j}\vert
    \leq \mu\vert W\vert_{F}/\sqrt{mn}.$ |  |'
- en: To exploit incoherence, Chee et al. ([2023](#bib.bib4)) introduced *incoherence
    processing* as a part of their quantization method QuIP. QuIP’s incoherence processing
    works by conjugating $W$ to compute
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用非一致性，Chee等人（[2023](#bib.bib4)）将*非一致性处理*引入到他们的量化方法QuIP中。QuIP的非一致性处理通过共轭$W$来计算
- en: '|  | $\textstyle U^{T}(\operatorname{quantized}(\tilde{W})(Vx))\approx U^{T}(\tilde{W}(Vx))=Wx.$
    |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textstyle U^{T}(\operatorname{quantized}(\tilde{W})(Vx))\approx U^{T}(\tilde{W}(Vx))=Wx.$
    |  |'
- en: These structured orthogonal multiplies by a Kronecker product lead to a runtime
    overhead of $\Theta(n\sqrt{n}+m\sqrt{m})$.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结构化的正交乘法通过Kronecker积引入了$\Theta(n\sqrt{n}+m\sqrt{m})$的运行时开销。
- en: Incoherence processing can be seen as a principled alternative to more complicated
    and heuristic methods for outlier suppression. Methods such as grouping require
    extra storage and can negatively impact performance. For example, using a 16 bit
    scale per group of 64 weights requires an extra 0.25 bits per weight. This increase
    is significant in extreme compression regimes, whereas incoherence processing
    allows more bits to be spent on actually quantizing model weights.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 非一致性处理可以看作是更复杂的启发式异常值抑制方法的原则性替代方案。比如，分组方法需要额外的存储，并且可能对性能产生负面影响。例如，每组64个权重使用16位缩放需要额外的0.25位每个权重。在极端压缩条件下，这种增加是显著的，而非一致性处理允许将更多位数用于实际量化模型权重。
- en: 2.4 Vector Quantization
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 向量量化
- en: 'Prior PTQ works have focused on quantizing each scalar weight $W_{ij}$ with
    our rounding method BlockLDLQ in Section [4.1](#S4.SS1 "4.1 Adaptive Rounding
    for Vector Quantization ‣ 4 BlockLDLQ and Lattice Codebooks ‣ QuIP#: Even Better
    LLM Quantization with Hadamard Incoherence and Lattice Codebooks").'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '先前的PTQ工作集中在使用我们在第[4.1](#S4.SS1 "4.1 Adaptive Rounding for Vector Quantization
    ‣ 4 BlockLDLQ and Lattice Codebooks ‣ QuIP#: Even Better LLM Quantization with
    Hadamard Incoherence and Lattice Codebooks")节中介绍的四舍五入方法BlockLDLQ来量化每个标量权重$W_{ij}$。'
- en: 3 Incoherence Processing with the Randomized Hadamard Transform
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 使用随机Hadamard变换的非一致性处理
- en: Algorithm 3 Incoherence Processing with RHT (IP-RHT)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 使用RHT的非一致性处理（IP-RHT）
- en: 0:  $W\in\mathbb{R}^{m\times n},H\in\mathbb{R}^{n\times n}$
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 0:  $W\in\mathbb{R}^{m\times n},H\in\mathbb{R}^{n\times n}$
- en: 'In this section, we propose a way of improving the incoherence processing of
    QuIP by replacing the 2-factor Kronecker product by a Randomized Hadamard Transformation
    (RHT) (Halko et al., [2011](#bib.bib15)). This change yields three advantages:
    (1) the theoretical bound on the incoherence parameter $\mu$. Additionally, we
    show in Section [6.4](#S6.SS4 "6.4 Ablations ‣ 6 Experiments ‣ QuIP#: Even Better
    LLM Quantization with Hadamard Incoherence and Lattice Codebooks") that this change
    by itself improves the perplexity of quantized LLMs.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们提出了一种通过用随机 Hadamard 变换 (RHT) (Halko 等人，[2011](#bib.bib15)) 替代2因子 Kronecker
    积来改进 QuIP 不连贯处理的方法。这一改变带来了三个优势：(1) 对不连贯参数 $\mu$ 的理论界限。此外，我们在第[6.4](#S6.SS4 "6.4
    消融 ‣ 6 实验 ‣ QuIP#: 更佳的 LLM 量化，通过 Hadamard 不连贯和格码书")节中展示了这一改变本身如何改善量化 LLM 的困惑度。'
- en: 'Recall from section [2.3](#S2.SS3 "2.3 Incoherence Processing ‣ 2 Background
    / Related Work ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence
    and Lattice Codebooks") that one way to efficiently perform incoherence processing
    is to conjugate $W$. We will temporarily assume that all dimensions are powers
    of 2. Later in the section we will explain 2 methods for incoherence processing
    when the dimension is not a power of 2.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '从第[2.3](#S2.SS3 "2.3 不连贯处理 ‣ 2 背景 / 相关工作 ‣ QuIP#: 更佳的 LLM 量化，通过 Hadamard 不连贯和格码书")节回忆，效率地执行不连贯处理的一种方法是共轭$W$。我们将暂时假设所有维度都是2的幂。稍后在本节中，我们将解释当维度不是2的幂时的2种不连贯处理方法。'
- en: '{restatable}'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '{restatable}'
- en: lemmalemmahadincoh Let $H$, where
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: lemmalemmahadincoh 让 $H$，其中
- en: '|  | $\mu_{H}=\sqrt{2\log\left(\frac{2n^{2}}{\delta}\right)}\;\;\text{ and
    }\;\;\mu_{W}=2\log\left(\frac{4mn}{\delta}\right).$ |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mu_{H}=\sqrt{2\log\left(\frac{2n^{2}}{\delta}\right)}\;\;\text{ 和 }\;\;\mu_{W}=2\log\left(\frac{4mn}{\delta}\right).$
    |  |'
- en: 'In QuIP (Chee et al., [2023](#bib.bib4)), the 2-factor Kronecker approach achieves
    $\mu_{W}^{Kron}=A^{2}\log\left(4Cmn/\delta\right)^{2}$’s RHT achieves superior
    incoherence via a log dependence on the matrix size rather that the Kronecker
    method’s log-squared dependence. All of QuIP’s theory analyzing the proxy loss
    in Eq. ([1](#S2.E1 "Equation 1 ‣ 2.2 Quantization and Adaptive Rounding ‣ 2 Background
    / Related Work ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence
    and Lattice Codebooks")) still holds with the RHT, with the improved incoherence
    rates propagating through.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '在QuIP (Chee 等人，[2023](#bib.bib4))中，2因子 Kronecker 方法实现了 $\mu_{W}^{Kron}=A^{2}\log\left(4Cmn/\delta\right)^{2}$
    的RHT通过对矩阵大小的对数依赖，而不是 Kronecker 方法的对数平方依赖，实现了更优的不连贯性。QuIP 所有分析代理损失的理论在 Eq. ([1](#S2.E1
    "方程 1 ‣ 2.2 量化与自适应四舍五入 ‣ 2 背景 / 相关工作 ‣ QuIP#: 更佳的 LLM 量化，通过 Hadamard 不连贯和格码书"))中仍然适用，改进的不连贯率得以传播。'
- en: Now, what about dimensions $n$ add less than 0.01 bits per weight.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，维度 $n$ 小于0.01比特每权重怎么办？
- en: 'While the Hadamard conjecture states that $\exists H_{k}\forall k,4\mid k$ to
    new hardware. In practice, we find that the RFFT performs slightly worse than
    the RHT but still achieves strong results (Table [1](#S3.T1 "Table 1 ‣ 3 Incoherence
    Processing with the Randomized Hadamard Transform ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks")). We describe the RFFT in detail
    in Section [A.2](#A1.SS2 "A.2 Incoherence Processing with the Randomized Fast
    Fourier Transform (RFFT) ‣ Appendix A Concentration Inequalities for the Randomized
    Hadamard Transform and Fast Fourier Transform ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks") in the Appendix.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然 Hadamard 猜想声明 $\exists H_{k}\forall k,4\mid k$ 到新硬件。实际上，我们发现 RFFT 的表现略逊于
    RHT，但仍能取得强劲的结果（表 [1](#S3.T1 "表 1 ‣ 3 使用随机 Hadamard 变换的不连贯处理 ‣ QuIP#: 更佳的 LLM 量化，通过
    Hadamard 不连贯和格码书")）。我们在附录第[A.2](#A1.SS2 "A.2 使用随机快速傅里叶变换 (RFFT) 进行不连贯处理 ‣ 附录 A
    随机 Hadamard 变换和快速傅里叶变换的集中不等式 ‣ QuIP#: 更佳的 LLM 量化，通过 Hadamard 不连贯和格码书")节中详细描述了
    RFFT。'
- en: 'Table 1: RHT vs. RFFT incoherence processing using 2 Bit QuIP$\#$), context
    length 4096.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：使用2位 QuIP$\#$的RHT与RFFT不连贯处理，上下文长度为4096。
- en: '| Incoherence | 2-7B | 2-13B | 2-70B |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 不连贯 | 2-7B | 2-13B | 2-70B |'
- en: '| --- | --- | --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Hadamard | 8.22 | 6.06 | 4.16 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Hadamard | 8.22 | 6.06 | 4.16 |'
- en: '| Fourier | 8.30 | 6.08 | 4.17 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 傅里叶 | 8.30 | 6.08 | 4.17 |'
- en: 4 BlockLDLQ and Lattice Codebooks
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 BlockLDLQ 和格码书
- en: 'The proof of Lemma [3](#alg3 "Algorithm 3 ‣ 3 Incoherence Processing with the
    Randomized Hadamard Transform ‣ QuIP#: Even Better LLM Quantization with Hadamard
    Incoherence and Lattice Codebooks") tells us that the incoherence-processed weights
    follow a roughly ball-shaped sub-Gaussian distribution. However, rounding weights
    one at a time, as QuIP does with its LDLQ, ignores this shaping—producing a set
    of representable weight vectors that is shaped like a hypercube rather than a
    ball. Vector quantization (VQ) lets us shape codebooks to better match the source
    distribution. In Section [4.1](#S4.SS1 "4.1 Adaptive Rounding for Vector Quantization
    ‣ 4 BlockLDLQ and Lattice Codebooks ‣ QuIP#: Even Better LLM Quantization with
    Hadamard Incoherence and Lattice Codebooks"), we introduce BlockLDLQ, which iteratively
    rounds blocks of weights with VQ. Within BlockLDLQ’s VQ step, QuIP$\#$ (Viazovska,
    [2017](#bib.bib31)). E8P achieves good shaping while admitting fast inference
    from only needing to lookup from a size 256 codebook.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '引理 [3](#alg3 "算法 3 ‣ 3 使用随机 Hadamard 变换进行不相干性处理 ‣ QuIP#: 更好的 LLM 量化与 Hadamard
    不相干性和格代码簿") 的证明告诉我们，不相干性处理后的权重遵循大致为球形的亚高斯分布。然而，像 QuIP 使用 LDLQ 一样逐一舍入权重，会忽略这种形状—产生一组形状像超立方体而不是球体的可表示权重向量。向量量化
    (VQ) 让我们能够调整代码簿以更好地匹配源分布。在第 [4.1](#S4.SS1 "4.1 自适应舍入用于向量量化 ‣ 4 BlockLDLQ 和格代码簿
    ‣ QuIP#: 更好的 LLM 量化与 Hadamard 不相干性和格代码簿") 节中，我们介绍了 BlockLDLQ，它通过 VQ 迭代地舍入权重块。在
    BlockLDLQ 的 VQ 步骤中，QuIP$\#$（Viazovska, [2017](#bib.bib31)）。E8P 实现了良好的形状调整，同时由于只需要从大小为
    256 的代码簿中查找而实现了快速推理。'
- en: 4.1 Adaptive Rounding for Vector Quantization
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 向量量化的自适应舍入
- en: Chee et al. ([2023](#bib.bib4)) formulated a class of adaptive rounding algorithms
    with linear feedback. These methods round columns one at a time with linear feedback
    $a_{k}$ acts elementwise.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Chee 等人 ([2023](#bib.bib4)) 提出了一个具有线性反馈的自适应舍入算法类。这些方法一次舍入一列，线性反馈 $a_{k}$ 逐元素作用。
- en: The LDLQ algorithm sets U to be $L^{T}-I$. From QuIP, we know that LDLQ is optimal
    within adaptive rounding methods with linear feedback when rounding to the integers.
    However, LDLQ does not work with vector quantization, which rounds multiple columns
    together.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: LDLQ 算法将 U 设置为 $L^{T}-I$。从 QuIP 中，我们知道 LDLQ 在舍入到整数时在具有线性反馈的自适应舍入方法中是最优的。然而，LDLQ
    不适用于向量量化，因为它将多个列一起舍入。
- en: We propose to extend LDLQ to support vector quantization. Given a block size
    $g$ in a block-wise fashion via
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议扩展 LDLQ 以支持向量量化。给定一个块大小 $g$，通过块级方式
- en: '|  | $\hat{W}_{k}=\mathbf{Q}(W_{k}+(W_{:(k-1)}-\hat{W}_{:(k-1)})\mathbf{A}_{k}),$
    |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{W}_{k}=\mathbf{Q}(W_{k}+(W_{:(k-1)}-\hat{W}_{:(k-1)})\mathbf{A}_{k}),$
    |  |'
- en: where $\mathbf{A}_{k}\in\mathbb{R}^{n\times g}$. Then
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{A}_{k}\in\mathbb{R}^{n\times g}$。然后
- en: '|  | $\textstyle\mathbf{E}[\operatorname{tr}((\hat{W}-W)H(\hat{W}-W)^{T})]\leq\frac{gm\mu^{2}\sigma^{2}}{n}\operatorname{tr}(H^{1/2})^{2}.$
    |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textstyle\mathbf{E}[\operatorname{tr}((\hat{W}-W)H(\hat{W}-W)^{T})]\leq\frac{gm\mu^{2}\sigma^{2}}{n}\operatorname{tr}(H^{1/2})^{2}.$
    |  |'
- en: 'Observe that under the same conditions, just quantizing all blocks independently
    would yield $\mathbf{E}[\operatorname{tr}((\hat{W}-W)H(\hat{W}-W)^{T})]\leq gm\sigma^{2}\operatorname{tr}(H)$ 
    beyond Theorem [4.1](#S4.Ex5 "4.1 Adaptive Rounding for Vector Quantization ‣
    4 BlockLDLQ and Lattice Codebooks ‣ QuIP#: Even Better LLM Quantization with Hadamard
    Incoherence and Lattice Codebooks"), so (if desired) they are left as an exercise
    for the reader.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '注意到在相同条件下，仅独立量化所有块将导致 $\mathbf{E}[\operatorname{tr}((\hat{W}-W)H(\hat{W}-W)^{T})]\leq
    gm\sigma^{2}\operatorname{tr}(H)$ 超过定理 [4.1](#S4.Ex5 "4.1 自适应舍入用于向量量化 ‣ 4 BlockLDLQ
    和格代码簿 ‣ QuIP#: 更好的 LLM 量化与 Hadamard 不相干性和格代码簿")，因此（如果需要）它们留给读者作为练习。'
- en: 4.2 The E8P (“E8 Padded”) Codebook
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 E8P（“E8 填充”）代码簿
- en: BlockLDLQ relies on an internal vector quantization (VQ) step $\mathbf{Q}$.
    Since the codebook size is exponential in both the vector dimension and bitrate,
    VQ quickly becomes intractable at high dimensions or bitrates.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: BlockLDLQ 依赖于内部向量量化 (VQ) 步骤 $\mathbf{Q}$。由于代码簿大小在向量维度和比特率上都是指数级的，因此在高维度或高比特率下，VQ
    很快变得难以处理。
- en: For QuIP$\#$ whose sum is an even number, that is
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 QuIP$\#$ 其总和是偶数，即
- en: '|  | $\textstyle E_{8}=\left(\mathbb{Z}^{8}\cup\left(\mathbb{Z}^{8}+\frac{1}{2}\right)\right)\cap\left\{x\mid\mathbf{1}^{T}x\text{
    is even}\right\}.$ |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textstyle E_{8}=\left(\mathbb{Z}^{8}\cup\left(\mathbb{Z}^{8}+\frac{1}{2}\right)\right)\cap\left\{x\mid\mathbf{1}^{T}x\text{
    是偶数}\right\}.$ |  |'
- en: The construction of the E8P codebook starts with an equivalent way to write
    $E_{8}$ (keeping the same optimal packing density).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: E8P 代码簿的构建从一种等效的方式写出 $E_{8}$ 开始（保持相同的最佳打包密度）。
- en: $\hat{D}_{8}$-entry lattice codebook “E8P.”
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: $\hat{D}_{8}$-entry 格子代码本 “E8P”。
- en: '![Refer to caption](img/7e629a2444b3e422907b5af4ac9e28ac.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/7e629a2444b3e422907b5af4ac9e28ac.png)'
- en: 'Figure 3: Minimum achievable elementwise MSE of quantizing a Gaussian to various
    codebooks. $E_{8}$.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：将高斯量化到各种代码本的最小可实现逐元素均方误差（MSE）。$E_{8}$。
- en: 'Figure [3](#S4.F3 "Figure 3 ‣ 4.2 The E8P (“E8 Padded”) Codebook ‣ 4 BlockLDLQ
    and Lattice Codebooks ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence
    and Lattice Codebooks") plots the elementwise MSE of quantizing a standard multivariate
    Gaussian to various $k$. This figure illustrates the importance of dimension for
    vector quantization. Increasing the vector dimension decreases the error for the
    half integer grid, as the resulting codebook is closer in shape to the source
    distribution. Finally, while K-means on the source distribution would achieve
    lower MSE (Lloyd, [1982](#bib.bib21)), there are a number of practical reasons
    why a K-means based codebook would be less practical, including worse end-to-end
    empirical performance. We discuss this more in Section [C.3](#A3.SS3 "C.3 Why
    not K-Means? ‣ Appendix C E8P details ‣ QuIP#: Even Better LLM Quantization with
    Hadamard Incoherence and Lattice Codebooks").'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [3](#S4.F3 "Figure 3 ‣ 4.2 The E8P (“E8 Padded”) Codebook ‣ 4 BlockLDLQ and
    Lattice Codebooks ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence
    and Lattice Codebooks") 绘制了将标准多变量高斯量化到各种 $k$ 的逐元素均方误差（MSE）。此图说明了向量量化中维度的重要性。增加向量维度减少了半整数网格的误差，因为结果代码本在形状上更接近源分布。最后，虽然在源分布上进行K均值聚类会实现更低的MSE（Lloyd，[1982](#bib.bib21)），但基于K均值的代码本在实践中不够实用，包括较差的端到端实证性能。我们在第
    [C.3](#A3.SS3 "C.3 Why not K-Means? ‣ Appendix C E8P details ‣ QuIP#: Even Better
    LLM Quantization with Hadamard Incoherence and Lattice Codebooks") 节中对此进行了更多讨论。'
- en: 4.3 Scaling $E_{8}$ to Higher Bitrates
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 将 $E_{8}$ 扩展到更高比特率
- en: The $E_{8}$ with norm 4). One could also use more advanced multi-codebook quantization
    approaches other than RVQ, but we found that RVQ was sufficient to achieve strong
    quantization performance.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: $E_{8}$ 具有范数4）。也可以使用比RVQ更先进的多代码本量化方法，但我们发现RVQ足以实现强大的量化性能。
- en: 5 Fine-Tuning During Quantization
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 微调期间的量化
- en: Recent works have suggested that at extreme quantization levels (e.g. 2 bits),
    inter-layer interactions are a significant hurdle to lossless quantization (Shao
    et al., [2024](#bib.bib26); Egiazarian et al., [2024](#bib.bib10)). Here, we employ
    a simple fine-tuning algorithm that attempts to recover the original unquantized
    model during quantization. Our fine tuning method runs on a small development
    set and works by relaxing the sign vectors in the RHT to arbitrary real vectors
    after quantization.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，在极端量化水平下（例如2位），层间交互是无损量化的一个重大障碍（Shao等， [2024](#bib.bib26)；Egiazarian等，[2024](#bib.bib10)）。在这里，我们使用了一种简单的微调算法，试图在量化过程中恢复原始未量化模型。我们的微调方法在一个小的开发集上运行，并通过在量化后将RHT中的符号向量放松为任意实数向量来工作。
- en: Our fine tuning method contains two steps. First, we fine-tune within each transformer
    block by quantizing the first linear layer, fine-tuning the remaining parameters
    to match the unquantized model’s output activations on the unquantized model’s
    input activations, quantizing the second linear layer, fine-tuning, and so on
    until all linear layers are quantized. This step attempts to minimize the activation
    error caused by an individual linear layer during quantization, and it is parallelizable
    across transformer blocks as the activation error does not consider the effect
    of quantizing preceding blocks. The idea of fine-tuning on the level of a transformer
    block was previously proposed in Egiazarian et al. ([2024](#bib.bib10)); our methodology
    differs in that we set a different set of parameters to be trainable. In the second
    step, after all linear layers in the model are quantized, the unquantized parameters
    (layernorms, sign vectors, language model head) are fine-tuned to minimize activation
    error over the entire model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的微调方法包含两个步骤。首先，我们通过量化第一个线性层在每个变换器块内进行微调，调整其余参数以匹配未量化模型在未量化模型输入激活上的输出激活，量化第二个线性层，再进行微调，依此类推，直到所有线性层都被量化。此步骤旨在最小化量化过程中由单个线性层引起的激活误差，并且由于激活误差不考虑量化前面块的影响，因此可以在变换器块之间并行进行。变换器块级别的微调方法在Egiazarian等人的工作中（[2024](#bib.bib10)）已有提出；我们的方法不同之处在于我们设置了一组不同的可训练参数。在第二步中，在模型中的所有线性层都量化之后，对未量化的参数（层归一化、符号向量、语言模型头）进行微调，以最小化整个模型的激活误差。
- en: 'By optimizing the sign vectors as real vectors instead of binary vectors in
    both steps, we allow the incoherence processing step to shape the weight matrix
    to the codebook. While this means we must store the sign vectors in FP16 instead
    of as bitvectors, the size of LLM matrices means that the sign vectors still add
    less than 0.01 bits per weight. We describe these steps in more detail in Section
    [D](#A4 "Appendix D Fine-Tuning During Quantization ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks").'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '通过将符号向量优化为实数向量而非二进制向量，我们允许不一致性处理步骤将权重矩阵塑形为代码簿。虽然这意味着我们必须以 FP16 存储符号向量而非位向量，但
    LLM 矩阵的大小意味着符号向量每个权重增加的位数仍然小于 0.01。我们在第 [D](#A4 "附录 D 量化期间的微调 ‣ QuIP#: 更佳 LLM
    量化，利用 Hadamard 不一致性和格点代码簿") 节中更详细地描述了这些步骤。'
- en: 6 Experiments
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验
- en: 'Table 2: Llama 1 & 2 Wikitext2 and C4 perplexity ($\downarrow$), context length
    2048.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: Llama 1 & 2 Wikitext2 和 C4 困惑度（$\downarrow$），上下文长度 2048。'
- en: '|  |  | Wikitext 2 | C4 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Wikitext 2 | C4 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Method | Bits | 1-7 | 1-13 | 1-30 | 1-65 | 2-7 | 2-13 | 2-70 | 1-7 | 1-13
    | 1-30 | 1-65 | 2-7 | 2-13 | 2-70 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | 1-7 | 1-13 | 1-30 | 1-65 | 2-7 | 2-13 | 2-70 | 1-7 | 1-13 | 1-30
    | 1-65 | 2-7 | 2-13 | 2-70 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
- en: '| FP16 | 16 | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.32 | 7.08 | 6.61
    | 5.98 | 5.62 | 6.97 | 6.47 | 5.52 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 16 | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.32 | 7.08 | 6.61
    | 5.98 | 5.62 | 6.97 | 6.47 | 5.52 |'
- en: '| AWQ | 4 | 6.08 | 5.34 | 4.39 | 3.76 | 6.15 | 5.12 | - | 7.52 | 6.86 | 6.17
    | 5.77 | 7.68 | 6.74 | - |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4 | 6.08 | 5.34 | 4.39 | 3.76 | 6.15 | 5.12 | - | 7.52 | 6.86 | 6.17
    | 5.77 | 7.68 | 6.74 | - |'
- en: '| OmniQ | 4 | 5.86 | 5.21 | 4.25 | 3.71 | 5.74 | 5.02 | 3.47 | 7.34 | 6.76
    | 6.11 | 5.73 | 7.35 | 6.65 | 5.65 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| OmniQ | 4 | 5.86 | 5.21 | 4.25 | 3.71 | 5.74 | 5.02 | 3.47 | 7.34 | 6.76
    | 6.11 | 5.73 | 7.35 | 6.65 | 5.65 |'
- en: '| QuIP# no FT & no $E_{8}$ | 4 | 5.83 | 5.20 | 4.23 | 3.63 | 5.66 | 5.00 |
    3.42 | 7.25 | 6.70 | 6.06 | 5.68 | 7.17 | 6.59 | 5.59 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# 无 FT & 无 $E_{8}$ | 4 | 5.83 | 5.20 | 4.23 | 3.63 | 5.66 | 5.00 | 3.42
    | 7.25 | 6.70 | 6.06 | 5.68 | 7.17 | 6.59 | 5.59 |'
- en: '| QuIP# | 4 | 5.76 | 5.17 | 4.18 | 3.60 | 5.56 | 4.95 | 3.38 | 7.18 | 6.67
    | 6.03 | 5.66 | 7.07 | 6.54 | 5.56 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# | 4 | 5.76 | 5.17 | 4.18 | 3.60 | 5.56 | 4.95 | 3.38 | 7.18 | 6.67
    | 6.03 | 5.66 | 7.07 | 6.54 | 5.56 |'
- en: '| AWQ | 3 | 11.9 | 7.45 | 10.0 | 5.21 | 24.0 | 10.5 | - | 13.3 | 9.13 | 12.7
    | 7.11 | 23.9 | 13.1 | - |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 3 | 11.9 | 7.45 | 10.0 | 5.21 | 24.0 | 10.5 | - | 13.3 | 9.13 | 12.7
    | 7.11 | 23.9 | 13.1 | - |'
- en: '| OmniQ | 3 | 6.49 | 5.68 | 4.74 | 4.04 | 6.58 | 5.58 | 3.92 | 8.19 | 7.32
    | 6.57 | 6.07 | 8.65 | 7.44 | 6.06 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| OmniQ | 3 | 6.49 | 5.68 | 4.74 | 4.04 | 6.58 | 5.58 | 3.92 | 8.19 | 7.32
    | 6.57 | 6.07 | 8.65 | 7.44 | 6.06 |'
- en: '| QuIP# no FT & no $E_{8}$ | 3 | 6.29 | 5.52 | 4.54 | 3.91 | 6.19 | 5.34 |
    3.71 | 7.82 | 6.98 | 6.29 | 5.86 | 7.85 | 6.98 | 5.78 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# 无 FT & 无 $E_{8}$ | 3 | 6.29 | 5.52 | 4.54 | 3.91 | 6.19 | 5.34 | 3.71
    | 7.82 | 6.98 | 6.29 | 5.86 | 7.85 | 6.98 | 5.78 |'
- en: '| QuIP# | 3 | 5.98 | 5.31 | 4.36 | 3.78 | 5.79 | 5.10 | 3.56 | 7.39 | 6.83
    | 6.17 | 5.77 | 7.32 | 6.72 | 5.67 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# | 3 | 5.98 | 5.31 | 4.36 | 3.78 | 5.79 | 5.10 | 3.56 | 7.39 | 6.83
    | 6.17 | 5.77 | 7.32 | 6.72 | 5.67 |'
- en: '| OmniQ | 2 | 15.5 | 13.2 | 8.71 | 7.58 | 37.4 | 17.2 | 7.81 | 24.9 | 18.3
    | 13.9 | 10.8 | 90.6 | 26.8 | 12.3 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| OmniQ | 2 | 15.5 | 13.2 | 8.71 | 7.58 | 37.4 | 17.2 | 7.81 | 24.9 | 18.3
    | 13.9 | 10.8 | 90.6 | 26.8 | 12.3 |'
- en: '| QuIP# no FT & no $E_{8}$ | 2 | 9.95 | 7.18 | 5.80 | 5.02 | 12.3 | 7.60 |
    4.87 | 11.7 | 8.67 | 7.55 | 6.83 | 14.8 | 9.57 | 6.82 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# 无 FT & 无 $E_{8}$ | 2 | 9.95 | 7.18 | 5.80 | 5.02 | 12.3 | 7.60 | 4.87
    | 11.7 | 8.67 | 7.55 | 6.83 | 14.8 | 9.57 | 6.82 |'
- en: '| QuIP# | 2 | 6.86 | 5.97 | 5.02 | 4.36 | 6.66 | 5.74 | 4.16 | 8.36 | 7.48
    | 6.71 | 6.19 | 8.35 | 7.45 | 6.12 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# | 2 | 6.86 | 5.97 | 5.02 | 4.36 | 6.66 | 5.74 | 4.16 | 8.36 | 7.48
    | 6.71 | 6.19 | 8.35 | 7.45 | 6.12 |'
- en: 'Table 3: Zeroshot Accuracy (acc in LM Eval, not acc_norm), Llama 2.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: Zeroshot 准确率（LM Eval 中的 acc，不是 acc_norm），Llama 2。'
- en: '|  | 2-70 | 2-13 | 2-7 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | 2-70 | 2-13 | 2-7 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Method | Bits | ArcC | ArcE | PiQA | Wino | Bits | ArcC | ArcE | PiQA | Wino
    | Bits | ArcC | ArcE | PiQA | Wino |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | ArcC | ArcE | PiQA | Wino | 位数 | ArcC | ArcE | PiQA | Wino | 位数
    | ArcC | ArcE | PiQA | Wino |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
- en: '| FP16 | 16 | 51.1 | 77.7 | 81.1 | 77.0 | 16 | 45.6 | 73.3 | 73.5 | 69.6 |
    16 | 40.0 | 69.3 | 78.5 | 67.3 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 16 | 51.1 | 77.7 | 81.1 | 77.0 | 16 | 45.6 | 73.3 | 73.5 | 69.6 |
    16 | 40.0 | 69.3 | 78.5 | 67.3 |'
- en: '| OmniQ | 4 | 49.8 | 77.9 | 80.7 | 75.8 | 4 | 43.1 | 70.2 | 78.4 | 67.8 | 4
    | 37.9 | 67.8 | 77.1 | 67.0 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| OmniQ | 4 | 49.8 | 77.9 | 80.7 | 75.8 | 4 | 43.1 | 70.2 | 78.4 | 67.8 | 4
    | 37.9 | 67.8 | 77.1 | 67.0 |'
- en: '| QuIP | 4 | 47.0 | 74.3 | 80.3 | 76.0 | 4 | 44.9 | 73.3 | 79.0 | 69.7 | 4
    | - | - | - | - |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 4 | 47.0 | 74.3 | 80.3 | 76.0 | 4 | 44.9 | 73.3 | 79.0 | 69.7 | 4
    | - | - | - | - |'
- en: '| AQLM | 4.07 | 51.0 | 78.1 | 81.4 | 76.9 | 3.94 | 43.9 | 72.2 | 78.6 | 70.4
    | 4.04 | 40.3 | 68.9 | 77.7 | 67.3 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 4.07 | 51.0 | 78.1 | 81.4 | 76.9 | 3.94 | 43.9 | 72.2 | 78.6 | 70.4
    | 4.04 | 40.3 | 68.9 | 77.7 | 67.3 |'
- en: '| QuIP# | 4 | 50.6 | 78.1 | 81.4 | 77.1 | 4 | 45.5 | 73.9 | 78.9 | 69.9 | 4
    | 40.5 | 69.1 | 78.4 | 67.6 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# | 4 | 50.6 | 78.1 | 81.4 | 77.1 | 4 | 45.5 | 73.9 | 78.9 | 69.9 | 4
    | 40.5 | 69.1 | 78.4 | 67.6 |'
- en: '| OmniQ | 3 | 47.6 | 75.7 | 79.7 | 73.5 | 3 | 42.0 | 69.0 | 77.7 | 65.9 | 3
    | 35.3 | 62.6 | 73.6 | 63.6 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| OmniQ | 3 | 47.6 | 75.7 | 79.7 | 73.5 | 3 | 42.0 | 69.0 | 77.7 | 65.9 | 3
    | 35.3 | 62.6 | 73.6 | 63.6 |'
- en: '| QuIP | 3 | 46.3 | 73.2 | 80.0 | 74.6 | 3 | 41.5 | 70.4 | 76.9 | 69.9 | 3
    | - | - | - | - |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 3 | 46.3 | 73.2 | 80.0 | 74.6 | 3 | 41.5 | 70.4 | 76.9 | 69.9 | 3
    | - | - | - | - |'
- en: '| AQLM | 3.01 | 50.0 | 77.6 | 81.3 | 77.2 | 3.03 | 43.6 | 73.5 | 77.8 | 67.6
    | 3.04 | 38.7 | 67.8 | 76.6 | 68.4 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 3.01 | 50.0 | 77.6 | 81.3 | 77.2 | 3.03 | 43.6 | 73.5 | 77.8 | 67.6
    | 3.04 | 38.7 | 67.8 | 76.6 | 68.4 |'
- en: '| QuIP# | 3 | 50.9 | 77.7 | 81.4 | 76.4 | 3 | 44.0 | 72.5 | 78.4 | 69.1 | 3
    | 39.2 | 68.4 | 77.3 | 66.5 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# | 3 | 50.9 | 77.7 | 81.4 | 76.4 | 3 | 44.0 | 72.5 | 78.4 | 69.1 | 3
    | 39.2 | 68.4 | 77.3 | 66.5 |'
- en: '| OmniQ | 2 | 28.7 | 55.4 | 68.8 | 53.2 | 2 | 23.0 | 44.4 | 62.6 | 52.6 | 2
    | 21.6 | 35.2 | 57.5 | 51.5 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| OmniQ | 2 | 28.7 | 55.4 | 68.8 | 53.2 | 2 | 23.0 | 44.4 | 62.6 | 52.6 | 2
    | 21.6 | 35.2 | 57.5 | 51.5 |'
- en: '| QuIP | 2 | 34.0 | 62.2 | 74.8 | 67.5 | 2 | 23.5 | 45.2 | 62.0 | 52.8 | 2
    | 19.4 | 26.0 | 54.6 | 51.8 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 2 | 34.0 | 62.2 | 74.8 | 67.5 | 2 | 23.5 | 45.2 | 62.0 | 52.8 | 2
    | 19.4 | 26.0 | 54.6 | 51.8 |'
- en: '| AQLM | 2.07 | 47.9 | 77.7 | 80.4 | 75.9 | 1.97 | 38.5 | 67.0 | 75.1 | 69.5
    | 2.02 | 33.6 | 62.8 | 73.5 | 64.6 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 2.07 | 47.9 | 77.7 | 80.4 | 75.9 | 1.97 | 38.5 | 67.0 | 75.1 | 69.5
    | 2.02 | 33.6 | 62.8 | 73.5 | 64.6 |'
- en: '| QuIP# | 2 | 48.7 | 77.3 | 80.3 | 75.9 | 2 | 39.5 | 69.3 | 77.3 | 67.7 | 2
    | 34.6 | 64.6 | 75.1 | 64.9 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# | 2 | 48.7 | 77.3 | 80.3 | 75.9 | 2 | 39.5 | 69.3 | 77.3 | 67.7 | 2
    | 34.6 | 64.6 | 75.1 | 64.9 |'
- en: 'Table 4: Wikitext2 and C4 perplexity ($\downarrow$), context length 4096.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: Wikitext2 和 C4 困惑度（$\downarrow$），上下文长度 4096。'
- en: '|  |  | 2-7 |  |  | 2-13 |  |  | 2-70 |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 2-7 |  |  | 2-13 |  |  | 2-70 |  |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Method | Bits | W2 | C4 | Bits | W2 | C4 | Bits | W2 | C4 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | W2 | C4 | 位数 | W2 | C4 | 位数 | W2 | C4 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| FP16 | 16 | 5.12 | 6.63 | 16 | 4.57 | 6.05 | 16 | 3.12 | 4.97 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 16 | 5.12 | 6.63 | 16 | 4.57 | 6.05 | 16 | 3.12 | 4.97 |'
- en: '| QuIP# | 4 | 5.19 | 6.75 | 4 | 4.63 | 6.13 | 4 | 3.18 | 5.02 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# | 4 | 5.19 | 6.75 | 4 | 4.63 | 6.13 | 4 | 3.18 | 5.02 |'
- en: '| no FT | 4 | 5.22 | 6.79 | 4 | 4.65 | 6.15 | 4 | 3.18 | 5.02 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| no FT | 4 | 5.22 | 6.79 | 4 | 4.65 | 6.15 | 4 | 3.18 | 5.02 |'
- en: '|   no $E_{8}$ | 4 | 5.29 | 6.86 | 4 | 4.68 | 6.20 | 4 | 3.22 | 5.05 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|   无 $E_{8}$ | 4 | 5.29 | 6.86 | 4 | 4.68 | 6.20 | 4 | 3.22 | 5.05 |'
- en: '| QuIP | 4 | - | - | 4 | 4.76 | 6.29 | 4 | 3.58 | 5.38 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 4 | - | - | 4 | 4.76 | 6.29 | 4 | 3.58 | 5.38 |'
- en: '| AQLM | 4.04 | 5.21 | 6.74 | 3.94 | 4.64 | 6.14 | 4.07 | 3.17 | 5.01 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 4.04 | 5.21 | 6.74 | 3.94 | 4.64 | 6.14 | 4.07 | 3.17 | 5.01 |'
- en: '| QuIP# | 3 | 5.41 | 7.04 | 3 | 4.78 | 6.35 | 3 | 3.35 | 5.15 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# | 3 | 5.41 | 7.04 | 3 | 4.78 | 6.35 | 3 | 3.35 | 5.15 |'
- en: '| no FT | 3 | 5.60 | 7.34 | 3 | 4.90 | 6.50 | 3 | 3.41 | 5.20 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| no FT | 3 | 5.60 | 7.34 | 3 | 4.90 | 6.50 | 3 | 3.41 | 5.20 |'
- en: '|   no $E_{8}$ | 3 | 5.77 | 7.61 | 3 | 4.99 | 6.65 | 3 | 3.48 | 5.28 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|   无 $E_{8}$ | 3 | 5.77 | 7.61 | 3 | 4.99 | 6.65 | 3 | 3.48 | 5.28 |'
- en: '| QuIP | 3 | - | - | 3 | 5.12 | 6.79 | 3 | 3.87 | 5.67 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 3 | - | - | 3 | 5.12 | 6.79 | 3 | 3.87 | 5.67 |'
- en: '| AQLM | 3.04 | 5.46 | 7.10 | 3.03 | 4.83 | 6.37 | 3.01 | 3.36 | 5.17 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 3.04 | 5.46 | 7.10 | 3.03 | 4.83 | 6.37 | 3.01 | 3.36 | 5.17 |'
- en: '| QuIP# | 2 | 6.19 | 8.16 | 2 | 5.35 | 7.20 | 2 | 3.91 | 5.71 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| QuIP# | 2 | 6.19 | 8.16 | 2 | 5.35 | 7.20 | 2 | 3.91 | 5.71 |'
- en: '| no FT | 2 | 8.22 | 11.0 | 2 | 6.06 | 8.07 | 2 | 4.16 | 6.01 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| no FT | 2 | 8.22 | 11.0 | 2 | 6.06 | 8.07 | 2 | 4.16 | 6.01 |'
- en: '|   no $E_{8}$ | 2 | 11.2 | 14.5 | 2 | 7.04 | 9.37 | 2 | 4.58 | 6.51 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|   无 $E_{8}$ | 2 | 11.2 | 14.5 | 2 | 7.04 | 9.37 | 2 | 4.58 | 6.51 |'
- en: '| QuIP | 2 | - | - | 2 | 13.5 | 16.2 | 2 | 5.90 | 8.17 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 2 | - | - | 2 | 13.5 | 16.2 | 2 | 5.90 | 8.17 |'
- en: '| AQLM | 2.02 | 6.93 | 8.84 | 1.97 | 5.70 | 7.59 | 2.07 | 3.94 | 5.72 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| AQLM | 2.02 | 6.93 | 8.84 | 1.97 | 5.70 | 7.59 | 2.07 | 3.94 | 5.72 |'
- en: 'Table 5: 2 and 4 bit QuIP$\#$ peak memory bandwidth (1008GB/s) during generation
    and is fast and scalable.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 2 位和 4 位 QuIP$\#$ 生成期间的峰值内存带宽（1008GB/s），速度快且可扩展。'
- en: '| Model |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |'
- en: '&#124; 2 Bit &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2 位 &#124;'
- en: '&#124; tok/s &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; tok/s &#124;'
- en: '|'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2 Bit % &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2 位 % &#124;'
- en: '&#124; Mem BW &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存带宽 &#124;'
- en: '|'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 4 Bit &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 4 位 &#124;'
- en: '&#124; tok/s &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; tok/s &#124;'
- en: '|'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 4 Bit % &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 4 位 % &#124;'
- en: '&#124; Mem BW &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存带宽 &#124;'
- en: '|'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 2-7B | 170.50 | 29.60% | 117.73 | 40.87% |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 2-7B | 170.50 | 29.60% | 117.73 | 40.87% |'
- en: '| 2-13B | 104.83 | 33.80% | 71.09 | 45.84% |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 2-13B | 104.83 | 33.80% | 71.09 | 45.84% |'
- en: '| 1-30B | 51.60 | 38.39% | 32.50 | 48.36% |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 1-30B | 51.60 | 38.39% | 32.50 | 48.36% |'
- en: '| 2-70B | 32.74 | 56.84% | OOM | OOM |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 2-70B | 32.74 | 56.84% | OOM | OOM |'
- en: '![Refer to caption](img/58bc67c23ebcc80018db3cea8ce085a8.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/58bc67c23ebcc80018db3cea8ce085a8.png)'
- en: 'Figure 4: QuIP$\#$ 2 bit scales similarly to higher bitrates.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：QuIP$\#$ 2位的缩放方式类似于更高位速率。
- en: Our main experiments show the performance of QuIP$\#$ on the Llama 1 (Touvron
    et al., [2023a](#bib.bib29)) and 2 (Touvron et al., [2023b](#bib.bib30)) family
    of models. These models range in size from 7 billion to 70 billion parameters
    and offer good performance, making them suitable for understanding how quantization
    methods perform and scale. Additional results for other models are available in
    the Appendix.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要实验展示了QuIP$\#$在Llama 1 (Touvron et al., [2023a](#bib.bib29))和2 (Touvron
    et al., [2023b](#bib.bib30))系列模型上的性能。这些模型的参数范围从70亿到700亿，并提供了良好的性能，使其适合于了解量化方法的表现和扩展性。其他模型的附加结果见附录。
- en: 'In Section [6.1](#S6.SS1 "6.1 QuIP# on Llama Models ‣ 6 Experiments ‣ QuIP#:
    Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks"),
    we compare QuIP$\#$.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '在[6.1](#S6.SS1 "6.1 QuIP# on Llama Models ‣ 6 Experiments ‣ QuIP#: Even Better
    LLM Quantization with Hadamard Incoherence and Lattice Codebooks")节中，我们对QuIP$\#$进行了比较。'
- en: We report W$x$ numbers. Finally, we bold numbers in our tables when they are
    clearly better, such as a smaller model matching or outperforming a larger model
    or a similar sized model significantly outperforming another model.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告W$x$数字。最后，当表中的数字明显更好时，例如较小的模型与较大模型匹配或超越，或相似大小的模型显著超越另一模型时，我们将其加粗。
- en: 6.1 QuIP$\#$ on Llama Models
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 QuIP$\#$ 在Llama模型上的表现
- en: 'Table [2](#S6.T2 "Table 2 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks") shows a comparison of QuIP$\#$ without
    fine-tuning or lattice codebooks significantly outperforms OmniQuant and AWQ,
    which both rely on heuristics to reduce model outliers during quantization.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '表[2](#S6.T2 "Table 2 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks")显示，未经微调或使用格点码本的QuIP$\#$显著超越了OmniQuant和AWQ，后者依赖启发式方法来减少量化过程中的模型异常值。'
- en: 'Table [4](#S6.T4 "Table 4 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks") shows a comparison of QuIP$\#$ 3
    and 4 bit results presented in this paper use residual vector quantization; one
    could potentially achieve better numbers with more advanced multi-codebook quantization
    approaches.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '表[4](#S6.T4 "Table 4 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks")展示了本文中QuIP$\#$ 3位和4位结果的比较，这些结果使用了残差向量量化；通过更先进的多码本量化方法，可能会获得更好的数字。'
- en: 'Table [3](#S6.T3 "Table 3 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks") shows zeroshot results for QuIP$\#$.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '表[3](#S6.T3 "Table 3 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks")显示了QuIP$\#$的零样本结果。'
- en: 6.2 QuIP$\#$ Bit Scaling
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 QuIP$\#$ 位数缩放
- en: 'Figures [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks") (first page) and [4](#S6.F4
    "Figure 4 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization with Hadamard
    Incoherence and Lattice Codebooks") show how QuIP$\#$ 3 bit outperforms a theoretical
    lossless 4 bit model (FP16 at 4 bits). To the best of our knowledge, this is the
    first time a 3 bit PTQ method has outperformed a theoretical lossless 4 bit model
    and also the first time a 2 bit PTQ method has offered similar scaling to higher
    bitrates.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks")（第一页）和[4](#S6.F4 "Figure 4 ‣
    6 Experiments ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence
    and Lattice Codebooks")显示了QuIP$\#$ 3位如何超越了理论上无损的4位模型（4位的FP16）。据我们所知，这是第一次3位PTQ方法超越理论上无损的4位模型，也是第一次2位PTQ方法在扩展到更高位速率时表现出类似的扩展性。'
- en: 6.3 Efficient Inference with QuIP$\#$
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 QuIP$\#$的高效推理
- en: 'Table [5](#S6.T5 "Table 5 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks") shows 2 and 4 bit QuIP$\#$ inference
    is fast and scalable on modern GPUs.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '表[5](#S6.T5 "Table 5 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks")显示，2位和4位QuIP$\#$推理在现代GPU上快速且具有可扩展性。'
- en: 6.4 Ablations
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 消融实验
- en: 'Table [4](#S6.T4 "Table 4 ‣ 6 Experiments ‣ QuIP#: Even Better LLM Quantization
    with Hadamard Incoherence and Lattice Codebooks") also contains an ablation on
    the various components of QuIP$\#$’s RHT. The RHT offers stronger incoherence
    properties than the Kronecker factorization (Section [3](#S3 "3 Incoherence Processing
    with the Randomized Hadamard Transform ‣ QuIP#: Even Better LLM Quantization with
    Hadamard Incoherence and Lattice Codebooks")), which improves performance.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [4](#S6.T4 "表 4 ‣ 6 实验 ‣ QuIP#: 更佳的LLM量化，结合Hadamard不相干性和格子词典") 还包含对QuIP$\#$的各种组件的消融研究。RHT比Kronecker分解（第[3](#S3
    "3 随机Hadamard变换的不相干处理 ‣ QuIP#: 更佳的LLM量化，结合Hadamard不相干性和格子词典")节）具有更强的不相干性特性，从而提升了性能。'
- en: 7 Conclusion
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: We present QuIP$\#$ is the first PTQ method to achieve superior scaling at 3
    bits over 4 bits and similar scaling at 2 bits to higher bitrates. Our results
    indicate that, in the near future, 2 bit models are likely to scale better than
    3 bit ones.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了QuIP$\#$ 是首个在3位上实现优越扩展的PTQ方法，并且在2位上与更高比特率具有类似扩展。我们的结果表明，未来不久，2位模型可能比3位模型扩展得更好。
- en: 8 Impact Statement
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 影响声明
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍的工作旨在推进机器学习领域。我们的工作可能具有许多潜在的社会影响，但我们认为不需要在此特别强调。
- en: Acknowledgements
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank, in no particular order, David Hou for helping with the QuIP$\#$, and
    Together AI for compute resources.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢（无特定顺序）David Hou 对QuIP$\#$的帮助，以及Together AI 提供的计算资源。
- en: References
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Almazrouei et al. (2023) Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli,
    A., Cojocaru, R., Debbah, M., Étienne Goffinet, Hesslow, D., Launay, J., Malartic,
    Q., Mazzotta, D., Noune, B., Pannier, B., and Penedo, G. The falcon series of
    open language models, 2023.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Almazrouei et al. (2023) Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli,
    A., Cojocaru, R., Debbah, M., Étienne Goffinet, Hesslow, D., Launay, J., Malartic,
    Q., Mazzotta, D., Noune, B., Pannier, B., 和 Penedo, G. 鹰系列开放语言模型，2023年。
- en: 'Cai et al. (2024) Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D.,
    and Dao, T. Medusa: Simple llm inference acceleration framework with multiple
    decoding heads. *arXiv preprint arXiv: 2401.10774*, 2024.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai et al. (2024) Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D.,
    和 Dao, T. Medusa: 具有多个解码头的简单llm推理加速框架。*arXiv 预印本 arXiv: 2401.10774*，2024年。'
- en: Chee et al. (2022) Chee, J., Renz, M., Damle, A., and Sa, C. D. Model preserving
    compression for neural networks. In Oh, A. H., Agarwal, A., Belgrave, D., and
    Cho, K. (eds.), *Advances in Neural Information Processing Systems*, 2022. URL
    [https://openreview.net/forum?id=gt-l9Hu2ndd](https://openreview.net/forum?id=gt-l9Hu2ndd).
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chee et al. (2022) Chee, J., Renz, M., Damle, A., 和 Sa, C. D. 神经网络的模型保留压缩。在
    Oh, A. H., Agarwal, A., Belgrave, D., 和 Cho, K. (编辑)，《*神经信息处理系统的进展*》，2022年。网址
    [https://openreview.net/forum?id=gt-l9Hu2ndd](https://openreview.net/forum?id=gt-l9Hu2ndd)。
- en: 'Chee et al. (2023) Chee, J., Cai, Y., Kuleshov, V., and Sa, C. D. QuIP: 2-bit
    quantization of large language models with guarantees. In *Thirty-seventh Conference
    on Neural Information Processing Systems*, 2023. URL [https://openreview.net/forum?id=xrk9g5vcXR](https://openreview.net/forum?id=xrk9g5vcXR).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chee et al. (2023) Chee, J., Cai, Y., Kuleshov, V., 和 Sa, C. D. QuIP: 带有保证的大型语言模型的2位量化。在*第三十七届神经信息处理系统会议*，2023年。网址
    [https://openreview.net/forum?id=xrk9g5vcXR](https://openreview.net/forum?id=xrk9g5vcXR)。'
- en: 'Cochran et al. (1967) Cochran, W., Cooley, J., Favin, D., Helms, H., Kaenel,
    R., Lang, W., Maling, G., Nelson, D., Rader, C., and Welch, P. What is the fast
    fourier transform? *Proceedings of the IEEE*, 55(10):1664–1674, 1967. doi: 10.1109/PROC.1967.5957.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cochran et al. (1967) Cochran, W., Cooley, J., Favin, D., Helms, H., Kaenel,
    R., Lang, W., Maling, G., Nelson, D., Rader, C., 和 Welch, P. 快速傅里叶变换是什么？*IEEE
    会议论文集*，55(10):1664–1674，1967年。doi: 10.1109/PROC.1967.5957。'
- en: 'Computer (2023) Computer, T. Redpajama: An open source recipe to reproduce
    llama training dataset, 2023. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Computer (2023) Computer, T. Redpajama: 生成llama训练数据集的开源配方，2023年。网址 [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data)。'
- en: 'Dao (2023) Dao, T. FlashAttention-2: Faster attention with better parallelism
    and work partitioning. 2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao (2023) Dao, T. FlashAttention-2: 更快的注意力机制，具备更好的并行性和工作分配。2023年。'
- en: 'Dao et al. (2022) Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Ré, C. FlashAttention:
    Fast and memory-efficient exact attention with IO-awareness. In *Advances in Neural
    Information Processing Systems*, 2022.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao 等 (2022) Dao, T., Fu, D. Y., Ermon, S., Rudra, A., 和 Ré, C. 《FlashAttention：快速且内存高效的精确注意力与
    IO 兼容》。在 *神经信息处理系统进展*, 2022年。
- en: 'Dettmers & Zettlemoyer (2023) Dettmers, T. and Zettlemoyer, L. The case for
    4-bit precision: k-bit inference scaling laws. In Krause, A., Brunskill, E., Cho,
    K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), *Proceedings of the 40th
    International Conference on Machine Learning*, volume 202 of *Proceedings of Machine
    Learning Research*, pp.  7750–7774\. PMLR, 23–29 Jul 2023. URL [https://proceedings.mlr.press/v202/dettmers23a.html](https://proceedings.mlr.press/v202/dettmers23a.html).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers & Zettlemoyer (2023) Dettmers, T. 和 Zettlemoyer, L. 《4位精度的案例：k位推理扩展规律》。在
    Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., 和 Scarlett, J.
    (编辑), *第40届国际机器学习会议论文集*, 第202卷，*机器学习研究论文集*，第7750–7774页。PMLR, 2023年7月23–29日。网址
    [https://proceedings.mlr.press/v202/dettmers23a.html](https://proceedings.mlr.press/v202/dettmers23a.html)。
- en: Egiazarian et al. (2024) Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar,
    E., Babenko, A., and Alistarh, D. Extreme compression of large language models
    via additive quantization, 2024.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Egiazarian 等 (2024) Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar, E.,
    Babenko, A., 和 Alistarh, D. 《通过加性量化的极端压缩大语言模型》，2024年。
- en: 'Fino & Algazi (1976) Fino and Algazi. Unified matrix treatment of the fast
    walsh-hadamard transform. *IEEE Transactions on Computers*, C-25(11):1142–1146,
    1976. doi: 10.1109/TC.1976.1674569.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fino & Algazi (1976) Fino 和 Algazi. 《快速 Walsh-Hadamard 变换的统一矩阵处理》。*IEEE 计算机学报*,
    C-25(11):1142–1146, 1976。doi: 10.1109/TC.1976.1674569。'
- en: 'Frantar et al. (2023) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. OPTQ: Accurate quantization for generative pre-trained transformers. In *The
    Eleventh International Conference on Learning Representations*, 2023. URL [https://openreview.net/forum?id=tcbBPnfwxS](https://openreview.net/forum?id=tcbBPnfwxS).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等 (2023) Frantar, E., Ashkboos, S., Hoefler, T., 和 Alistarh, D. 《OPTQ：用于生成预训练变换器的精确量化》。在
    *第十一届国际表示学习会议*，2023年。网址 [https://openreview.net/forum?id=tcbBPnfwxS](https://openreview.net/forum?id=tcbBPnfwxS)。
- en: Gao et al. (2023) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
    A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot
    language model evaluation, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等 (2023) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
    A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., 和 Zou, A. 《少样本语言模型评估框架》，2023年12月。网址
    [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)。
- en: 'Gray (1984) Gray, R. Vector quantization. *IEEE ASSP Magazine*, 1(2):4–29,
    1984. doi: 10.1109/MASSP.1984.1162229.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gray (1984) Gray, R. 《向量量化》。*IEEE ASSP 杂志*, 1(2):4–29, 1984。doi: 10.1109/MASSP.1984.1162229。'
- en: 'Halko et al. (2011) Halko, N., Martinsson, P.-G., and Tropp, J. A. Finding
    structure with randomness: Probabilistic algorithms for constructing approximate
    matrix decompositions. *SIAM review*, 53(2):217–288, 2011.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Halko 等 (2011) Halko, N., Martinsson, P.-G., 和 Tropp, J. A. 《通过随机性寻找结构：用于构建近似矩阵分解的概率算法》。*SIAM
    综述*, 53(2):217–288, 2011。
- en: 'Hedayat & Wallis (1978) Hedayat, A. and Wallis, W. D. Hadamard Matrices and
    Their Applications. *The Annals of Statistics*, 6(6):1184 – 1238, 1978. doi: 10.1214/aos/1176344370.
    URL [https://doi.org/10.1214/aos/1176344370](https://doi.org/10.1214/aos/1176344370).'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hedayat & Wallis (1978) Hedayat, A. 和 Wallis, W. D. 《Hadamard 矩阵及其应用》。*统计年鉴*,
    6(6):1184 – 1238, 1978。doi: 10.1214/aos/1176344370。网址 [https://doi.org/10.1214/aos/1176344370](https://doi.org/10.1214/aos/1176344370)。'
- en: Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,
    B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F.,
    Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A.,
    Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril,
    T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts, 2024.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,
    B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F.,
    Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A.,
    Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril,
    T., Wang, T., Lacroix, T., 和 Sayed, W. E. 《Mixtral 专家》，2024年。
- en: 'Juang & Gray (1982) Juang, B.-H. and Gray, A. Multiple stage vector quantization
    for speech coding. In *ICASSP ’82\. IEEE International Conference on Acoustics,
    Speech, and Signal Processing*, volume 7, pp.  597–600, 1982. doi: 10.1109/ICASSP.1982.1171604.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Juang & Gray (1982) Juang, B.-H. 和 Gray, A. 语音编码的多阶段向量量化。见 *ICASSP ’82\. IEEE国际声学、语音和信号处理会议*，第7卷，页码
    597–600，1982年。doi: 10.1109/ICASSP.1982.1171604。'
- en: 'Kingma & Ba (2017) Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization,
    2017.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma & Ba (2017) Kingma, D. P. 和 Ba, J. Adam：一种随机优化方法，2017年。
- en: 'Lin et al. (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., Gan, C.,
    and Han, S. Awq: Activation-aware weight quantization for llm compression and
    acceleration, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., Gan, C., 和 Han,
    S. Awq：面向 LLM 压缩和加速的激活感知权重量化，2023年。
- en: 'Lloyd (1982) Lloyd, S. Least squares quantization in pcm. *IEEE Transactions
    on Information Theory*, 28(2):129–137, 1982. doi: 10.1109/TIT.1982.1056489.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lloyd (1982) Lloyd, S. PCM中的最小二乘量化。*IEEE信息理论汇刊*，28(2)：129–137，1982年。doi: 10.1109/TIT.1982.1056489。'
- en: Nagel et al. (2020) Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and
    Blankevoort, T. Up or down? Adaptive rounding for post-training quantization.
    In III, H. D. and Singh, A. (eds.), *Proceedings of the 37th International Conference
    on Machine Learning*, volume 119 of *Proceedings of Machine Learning Research*,
    pp.  7197–7206\. PMLR, 13–18 Jul 2020. URL [https://proceedings.mlr.press/v119/nagel20a.html](https://proceedings.mlr.press/v119/nagel20a.html).
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel 等 (2020) Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., 和 Blankevoort,
    T. 向上还是向下？适应性舍入用于训练后量化。见 III, H. D. 和 Singh, A.（编），*第37届国际机器学习大会论文集*，*机器学习研究论文集*第119卷，页码
    7197–7206。PMLR，2020年7月13–18日。网址 [https://proceedings.mlr.press/v119/nagel20a.html](https://proceedings.mlr.press/v119/nagel20a.html)。
- en: Nagel et al. (2022) Nagel, M., Fournarakis, M., Bondarenko, Y., and Blankevoort,
    T. Overcoming oscillations in quantization-aware training. In Chaudhuri, K., Jegelka,
    S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), *Proceedings of
    the 39th International Conference on Machine Learning*, volume 162 of *Proceedings
    of Machine Learning Research*, pp.  16318–16330\. PMLR, 17–23 Jul 2022. URL [https://proceedings.mlr.press/v162/nagel22a.html](https://proceedings.mlr.press/v162/nagel22a.html).
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel 等 (2022) Nagel, M., Fournarakis, M., Bondarenko, Y., 和 Blankevoort, T.
    克服量化感知训练中的振荡问题。见 Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G.,
    和 Sabato, S.（编），*第39届国际机器学习大会论文集*，*机器学习研究论文集*第162卷，页码 16318–16330。PMLR，2022年7月17–23日。网址
    [https://proceedings.mlr.press/v162/nagel22a.html](https://proceedings.mlr.press/v162/nagel22a.html)。
- en: 'Nguyen et al. (2023) Nguyen, E., Poli, M., Faizi, M., Thomas, A., Birch-Sykes,
    C., Wornow, M., Patel, A., Rabideau, C., Massaroli, S., Bengio, Y., Ermon, S.,
    Baccus, S. A., and Ré, C. Hyenadna: Long-range genomic sequence modeling at single
    nucleotide resolution. 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等 (2023) Nguyen, E., Poli, M., Faizi, M., Thomas, A., Birch-Sykes, C.,
    Wornow, M., Patel, A., Rabideau, C., Massaroli, S., Bengio, Y., Ermon, S., Baccus,
    S. A., 和 Ré, C. Hyenadna：单核苷酸分辨率的长距离基因组序列建模。2023年。
- en: 'Rozière et al. (2024) Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat,
    I., Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., Rapin, J., Kozhevnikov,
    A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong,
    W., Défossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N.,
    Scialom, T., and Synnaeve, G. Code llama: Open foundation models for code, 2024.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rozière 等 (2024) Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I.,
    Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., Rapin, J., Kozhevnikov,
    A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong,
    W., Défossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N.,
    Scialom, T., 和 Synnaeve, G. Code llama：面向代码的开放基础模型，2024年。
- en: 'Shao et al. (2024) Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z.,
    Zhang, K., Gao, P., Qiao, Y., and Luo, P. Omniquant: Omnidirectionally calibrated
    quantization for large language models. In *The Twelfth International Conference
    on Learning Representations*, 2024. URL [https://openreview.net/forum?id=8Wuvhh0LYW](https://openreview.net/forum?id=8Wuvhh0LYW).'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等 (2024) Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang,
    K., Gao, P., Qiao, Y., 和 Luo, P. Omniquant：面向大型语言模型的全方位校准量化。见 *第十二届国际表征学习会议*，2024年。网址
    [https://openreview.net/forum?id=8Wuvhh0LYW](https://openreview.net/forum?id=8Wuvhh0LYW)。
- en: (27) Sloane, N. Hadamard Matrices — neilsloane.com. [http://neilsloane.com/hadamard/](http://neilsloane.com/hadamard/).
    [Accessed 02-02-2024].
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (27) Sloane, N. Hadamard 矩阵 — neilsloane.com。 [http://neilsloane.com/hadamard/](http://neilsloane.com/hadamard/)。[访问日期：2024年2月2日]。
- en: Sun et al. (2023) Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and
    effective pruning approach for large language models. In *Workshop on Efficient
    Systems for Foundation Models @ ICML2023*, 2023. URL [https://openreview.net/forum?id=tz9JV2PRSv](https://openreview.net/forum?id=tz9JV2PRSv).
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等人（2023）Sun, M., Liu, Z., Bair, A., 和 Kolter, J. Z. 对大型语言模型的简单有效的剪枝方法。发表于*ICML2023基于基础模型的高效系统研讨会*，2023年。网址
    [https://openreview.net/forum?id=tz9JV2PRSv](https://openreview.net/forum?id=tz9JV2PRSv)。
- en: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation
    language models, 2023a.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron等人（2023a）Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., 和 Lample, G. Llama: 开放和高效的基础语言模型，2023a。'
- en: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D.,
    Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J.,
    Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M.,
    Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
    Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez,
    A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned
    chat models, 2023b.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron等人（2023b）Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D.,
    Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J.,
    Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.
    M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J.
    X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S.,
    Rodriguez, A., Stojnic, R., Edunov, S., 和 Scialom, T. Llama 2: 开放基础和微调对话模型，2023b。'
- en: 'Viazovska (2017) Viazovska, M. The sphere packing problem in dimension $8$.
    *Annals of Mathematics*, 185(3), May 2017. ISSN 0003-486X. doi: 10.4007/annals.2017.185.3.7.
    URL [http://dx.doi.org/10.4007/annals.2017.185.3.7](http://dx.doi.org/10.4007/annals.2017.185.3.7).'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Viazovska（2017）Viazovska, M. 8维的球体打包问题。*数学年刊*，185(3)，2017年5月。ISSN 0003-486X。doi:
    10.4007/annals.2017.185.3.7。网址 [http://dx.doi.org/10.4007/annals.2017.185.3.7](http://dx.doi.org/10.4007/annals.2017.185.3.7)。'
- en: Appendix A Concentration Inequalities for the Randomized Hadamard Transform
    and Fast Fourier Transform
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 随机Hadamard变换和快速傅里叶变换的集中不等式
- en: A.1 Incoherence Processing with the Randomized Hadamard Transform
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 使用随机Hadamard变换处理不一致性
- en: Lemma A.1.
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理A.1。
- en: For any non-negative real number $n$,
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何非负实数 $n$，
- en: '|  | $1$2 |  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We start with the following “standard” integral. For non-negative integer $m$,
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从以下“标准”积分开始。对于非负整数 $m$，
- en: '|  | $1$2 |  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: This means that
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着
- en: '|  | $\displaystyle\frac{1}{B\left(\frac{1}{2},n\right)}\int_{-1}^{+1}x^{2m}(1-x^{2})^{n-1}\;dx$
    |  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{1}{B\left(\frac{1}{2},n\right)}\int_{-1}^{+1}x^{2m}(1-x^{2})^{n-1}\;dx$
    |  |'
- en: '|  |  | $\displaystyle=\frac{\Gamma\left(m+\frac{1}{2}\right)\Gamma\left(n\right)}{\Gamma\left(m+n+\frac{1}{2}\right)}\cdot\frac{\Gamma\left(n+\frac{1}{2}\right)}{\Gamma\left(\frac{1}{2}\right)\Gamma\left(n\right)}$
    |  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{\Gamma\left(m+\frac{1}{2}\right)\Gamma\left(n\right)}{\Gamma\left(m+n+\frac{1}{2}\right)}\cdot\frac{\Gamma\left(n+\frac{1}{2}\right)}{\Gamma\left(\frac{1}{2}\right)\Gamma\left(n\right)}$
    |  |'
- en: '|  |  | $\displaystyle=\frac{\Gamma\left(m+\frac{1}{2}\right)\Gamma\left(n+\frac{1}{2}\right)}{\sqrt{\pi}\cdot\Gamma\left(m+n+\frac{1}{2}\right)}.$
    |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{\Gamma\left(m+\frac{1}{2}\right)\Gamma\left(n+\frac{1}{2}\right)}{\sqrt{\pi}\cdot\Gamma\left(m+n+\frac{1}{2}\right)}.$
    |  |'
- en: Applying the Legendre duplication formula, for integer $m$,
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 应用勒让德重复公式，对于整数 $m$，
- en: '|  | $\Gamma\left(m+\frac{1}{2}\right)=\frac{(2m)!\sqrt{\pi}}{4^{m}m!},$ |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Gamma\left(m+\frac{1}{2}\right)=\frac{(2m)!\sqrt{\pi}}{4^{m}m!},$ |  |'
- en: then
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '|  | $\displaystyle\frac{1}{B\left(\frac{1}{2},n\right)}\int_{-1}^{+1}x^{2m}(1-x^{2})^{n-1}\;dx$
    |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{1}{B\left(\frac{1}{2},n\right)}\int_{-1}^{+1}x^{2m}(1-x^{2})^{n-1}\;dx$
    |  |'
- en: '|  |  | $\displaystyle=\frac{(2m)!(2n)!(m+n)!}{m!n!(2m+2n)!}.$ |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{(2m)!(2n)!(m+n)!}{m!n!(2m+2n)!}.$ |  |'
- en: In particular, this means that
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，这意味着
- en: '|  | $\displaystyle\frac{1}{B\left(\frac{1}{2},n\right)}\int_{-1}^{+1}\exp(tx)(1-x^{2})^{n-1}\;dx$
    |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{1}{B\left(\frac{1}{2},n\right)}\int_{-1}^{+1}\exp(tx)(1-x^{2})^{n-1}\;dx$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\sum_{m=0}^{\infty}\frac{t^{2m}}{m!}\cdot\frac{(2n)!(m+n)!}{n!(2m+2n)!}$
    |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{m=0}^{\infty}\frac{t^{2m}}{m!}\cdot\frac{(2n)!(m+n)!}{n!(2m+2n)!}$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\sum_{m=0}^{\infty}\frac{t^{2m}}{m!}\cdot\frac{1}{2^{m}}\prod_{k=1}^{m}\frac{1}{2k+2n-1}$
    |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{m=0}^{\infty}\frac{t^{2m}}{m!}\cdot\frac{1}{2^{m}}\prod_{k=1}^{m}\frac{1}{2k+2n-1}$
    |  |'
- en: '|  |  | $\displaystyle\leq\sum_{m=0}^{\infty}\frac{t^{2m}}{m!}\cdot\frac{1}{2^{m}}\left(\frac{1}{2n+1}\right)^{m}$
    |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\sum_{m=0}^{\infty}\frac{t^{2m}}{m!}\cdot\frac{1}{2^{m}}\left(\frac{1}{2n+1}\right)^{m}$
    |  |'
- en: '|  |  | $\displaystyle=\sum_{m=0}^{\infty}\frac{1}{m!}\left(\frac{t^{2}}{4n+2}\right)^{m}$
    |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{m=0}^{\infty}\frac{1}{m!}\left(\frac{t^{2}}{4n+2}\right)^{m}$
    |  |'
- en: '|  |  | $\displaystyle=\exp\left(\frac{t^{2}}{4n+2}\right).$ |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\exp\left(\frac{t^{2}}{4n+2}\right).$ |  |'
- en: This proves the lemma
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了引理
- en: '|  | $1$2 |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: ∎
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Lemma A.2.
  id: totrans-252
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 A.2。
- en: Call $U\in\mathbb{R}^{nd\times nd}$ blocks are zero). Then
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $U\in\mathbb{R}^{nd\times nd}$ 的块为零）。然后
- en: '|  | $\mathbf{P}\left(\left&#124;b^{T}USx\right&#124;\geq a\right)\leq 2\exp\left(-\frac{a^{2}nd}{2\left\&#124;b\right\&#124;^{2}\left\&#124;x\right\&#124;^{2}}\right).$
    |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{P}\left(\left&#124;b^{T}USx\right&#124;\geq a\right)\leq 2\exp\left(-\frac{a^{2}nd}{2\left\&#124;b\right\&#124;^{2}\left\&#124;x\right\&#124;^{2}}\right).$
    |  |'
- en: Proof.
  id: totrans-255
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: If we let the $i$ dimensional space. Observe that this means that
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们让 $i$ 维空间。观察到这意味着
- en: '|  | $\mathbf{P}(z_{i})\propto(1-z_{i}^{2})^{\frac{d-1}{2}-1}.$ |  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{P}(z_{i})\propto(1-z_{i}^{2})^{\frac{d-1}{2}-1}.$ |  |'
- en: So,
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '|  | $\displaystyle\mathbf{E}\left[\exp\left(tb^{T}USx\right)\right]$ |  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{E}\left[\exp\left(tb^{T}USx\right)\right]$ |  |'
- en: '|  |  | $\displaystyle=\prod_{i=1}^{n}\mathbf{E}\left[\exp\left(t\left\&#124;b\right\&#124;\left\&#124;x_{i}\right\&#124;n^{-1/2}z_{i}\right)\right]$
    |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\prod_{i=1}^{n}\mathbf{E}\left[\exp\left(t\left\&#124;b\right\&#124;\left\&#124;x_{i}\right\&#124;n^{-1/2}z_{i}\right)\right]$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\prod_{i=1}^{n}\mathbf{E}\left[\exp\left(\frac{t^{2}\left\&#124;b\right\&#124;^{2}\left\&#124;x_{i}\right\&#124;^{2}}{2nd}\right)\right]$
    |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\prod_{i=1}^{n}\mathbf{E}\left[\exp\left(\frac{t^{2}\left\&#124;b\right\&#124;^{2}\left\&#124;x_{i}\right\&#124;^{2}}{2nd}\right)\right]$
    |  |'
- en: '|  |  | $\displaystyle=\mathbf{E}\left[\exp\left(\frac{t^{2}\left\&#124;b\right\&#124;^{2}\left\&#124;x\right\&#124;^{2}}{2nd}\right)\right],$
    |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbf{E}\left[\exp\left(\frac{t^{2}\left\&#124;b\right\&#124;^{2}\left\&#124;x\right\&#124;^{2}}{2nd}\right)\right],$
    |  |'
- en: 'where the the last line follows from Lemma [A.1](#A1.Ex8 "Lemma A.1\. ‣ A.1
    Incoherence Processing with the Randomized Hadamard Transform ‣ Appendix A Concentration
    Inequalities for the Randomized Hadamard Transform and Fast Fourier Transform
    ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks").
    It follows from the standard application of Markov’s inequality that for any ,'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '其中最后一行由引理 [A.1](#A1.Ex8 "引理 A.1\. ‣ A.1 随机哈达玛变换的不一致性处理 ‣ 附录 A 随机哈达玛变换和快速傅里叶变换的浓度不等式
    ‣ QuIP#: 通过哈达玛不一致性和格子代码本进行更好的 LLM 量化") 推导得出。由马尔可夫不等式的标准应用可以得出，对于任何 ，'
- en: '|  | $\mathbf{P}\left(\left&#124;b^{T}USx\right&#124;\geq a\right)\leq 2\exp\left(-\frac{a^{2}nd}{2\left\&#124;b\right\&#124;^{2}\left\&#124;x\right\&#124;^{2}}\right).$
    |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{P}\left(\left&#124;b^{T}USx\right&#124;\geq a\right)\leq 2\exp\left(-\frac{a^{2}nd}{2\left\&#124;b\right\&#124;^{2}\left\&#124;x\right\&#124;^{2}}\right).$
    |  |'
- en: This is what we wanted to show. ∎
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们想要证明的。∎
- en: Lemma A.3.
  id: totrans-267
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 A.3。
- en: Let $H\in\mathbb{R}^{n\times n}$,
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $H\in\mathbb{R}^{n\times n}$，
- en: '|  | $\operatorname{Prob}\left(\max_{i,j}\left&#124;e_{i}^{T}HSUe_{j}\right&#124;\geq\sqrt{\frac{2}{nd}\log\left(\frac{2n^{2}}{\epsilon}\right)}\right)\leq\epsilon$
    |  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname{Prob}\left(\max_{i,j}\left&#124;e_{i}^{T}HSUe_{j}\right&#124;\geq\sqrt{\frac{2}{nd}\log\left(\frac{2n^{2}}{\epsilon}\right)}\right)\leq\epsilon$
    |  |'
- en: and
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '|  | $\operatorname{Prob}\left(\max_{i,j}\left&#124;e_{i}^{T}FPUe_{j}\right&#124;\geq\sqrt{\frac{2}{nd}\log\left(\frac{2n^{2}}{\epsilon}\right)}\right)\leq\epsilon.$
    |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname{Prob}\left(\max_{i,j}\left&#124;e_{i}^{T}FPUe_{j}\right&#124;\geq\sqrt{\frac{2}{nd}\log\left(\frac{2n^{2}}{\epsilon}\right)}\right)\leq\epsilon.$
    |  |'
- en: That is, with probability at least $1-\epsilon$-incoherent, where
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，至少有 $1-\epsilon$ 的概率是不一致的，其中
- en: '|  | $\mu_{H}=\sqrt{2\log\left(\frac{2n^{2}}{\epsilon}\right)}.$ |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mu_{H}=\sqrt{2\log\left(\frac{2n^{2}}{\epsilon}\right)}.$ |  |'
- en: Proof.
  id: totrans-274
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'Setting $b=e_{i}$ in Lemma [A.2](#A1.Ex25 "Lemma A.2\. ‣ A.1 Incoherence Processing
    with the Randomized Hadamard Transform ‣ Appendix A Concentration Inequalities
    for the Randomized Hadamard Transform and Fast Fourier Transform ‣ QuIP#: Even
    Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks"),'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '在引理 [A.2](#A1.Ex25 "引理 A.2\. ‣ A.1 使用随机哈达码变换的不相干性处理 ‣ 附录 A 随机哈达码变换和快速傅里叶变换的集中不等式
    ‣ QuIP#: 更好的 LLM 量化与哈达码不相干性和晶格码本") 中设置 $b=e_{i}$，'
- en: '|  | $\mathbf{P}\left(\left&#124;e_{i}^{T}HSUe_{j}\right&#124;\geq a\right)\leq
    2\exp\left(-\frac{a^{2}nd}{2}\right).$ |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{P}\left(\left\|e_{i}^{T}HSUe_{j}\right\|\geq a\right)\leq 2\exp\left(-\frac{a^{2}nd}{2}\right).$
    |  |'
- en: By the union bound,
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 根据并集界，
- en: '|  | $1$2 |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Setting
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 设置
- en: '|  | $a^{2}=\frac{2}{nd}\log\left(\frac{2n^{2}}{\epsilon}\right)$ |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | $a^{2}=\frac{2}{nd}\log\left(\frac{2n^{2}}{\epsilon}\right)$ |  |'
- en: proves the lemma. The FFT case is identical. ∎
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 证明了该引理。FFT 情况下是相同的。 ∎
- en: Lemma A.4.
  id: totrans-282
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 A.4.
- en: Let $H_{L}\in\mathbb{R}^{m\times m}$,
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $H_{L}\in\mathbb{R}^{m\times m}$，
- en: '|  | $1$2 |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: and
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '|  | $1$2 |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: That is, with probability at least $1-\epsilon$-incoherent, where
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 即，以至少 $1-\epsilon$ 的概率不相干，其中
- en: '|  | $\mu_{W}=2\log\left(\frac{4mn}{\epsilon}\right).$ |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mu_{W}=2\log\left(\frac{4mn}{\epsilon}\right).$ |  |'
- en: Proof.
  id: totrans-289
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'From Lemma [A.2](#A1.Ex25 "Lemma A.2\. ‣ A.1 Incoherence Processing with the
    Randomized Hadamard Transform ‣ Appendix A Concentration Inequalities for the
    Randomized Hadamard Transform and Fast Fourier Transform ‣ QuIP#: Even Better
    LLM Quantization with Hadamard Incoherence and Lattice Codebooks"),'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '从引理 [A.2](#A1.Ex25 "引理 A.2\. ‣ A.1 使用随机哈达码变换的不相干性处理 ‣ 附录 A 随机哈达码变换和快速傅里叶变换的集中不等式
    ‣ QuIP#: 更好的 LLM 量化与哈达码不相干性和晶格码本")，'
- en: '|  | $\mathbf{P}\left(\left&#124;b^{T}USx\right&#124;\geq\left\&#124;b\right\&#124;\left\&#124;x\right\&#124;\sqrt{\frac{2}{n}\log\left(\frac{4mn}{\epsilon}\right)}\right)\leq\frac{\epsilon}{2mn}.$
    |  |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{P}\left(\left\|b^{T}USx\right\|\geq\left\|b\right\|\left\|x\right\|\sqrt{\frac{2}{n}\log\left(\frac{4mn}{\epsilon}\right)}\right)\leq\frac{\epsilon}{2mn}.$
    |  |'
- en: By applying this once on each side to the rows and columns respectively, and
    union bounding over the $mn$ entries, we get
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分别对行和列应用一次，并对 $mn$ 个条目进行并集界，我们得到
- en: '|  | $1$2 |  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: The proof in the FFT case is identical. ∎
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: FFT 情况下的证明是相同的。 ∎
- en: \lemmahadincoh
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: \lemmahadincoh
- en: '*'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '*'
- en: Proof.
  id: totrans-297
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'The incoherence of $H$ follows from the application of Lemma [A.4](#A1.Thmtheorem4
    "Lemma A.4\. ‣ A.1 Incoherence Processing with the Randomized Hadamard Transform
    ‣ Appendix A Concentration Inequalities for the Randomized Hadamard Transform
    and Fast Fourier Transform ‣ QuIP#: Even Better LLM Quantization with Hadamard
    Incoherence and Lattice Codebooks"). ∎'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '$H$ 的不相干性来自于引理 [A.4](#A1.Thmtheorem4 "引理 A.4\. ‣ A.1 使用随机哈达码变换的不相干性处理 ‣ 附录
    A 随机哈达码变换和快速傅里叶变换的集中不等式 ‣ QuIP#: 更好的 LLM 量化与哈达码不相干性和晶格码本") 的应用。 ∎'
- en: A.2 Incoherence Processing with the Randomized Fast Fourier Transform (RFFT)
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 使用随机快速傅里叶变换 (RFFT) 进行不相干性处理
- en: Algorithm 4 Incoherence Processing with RFFT (IP-RFFT)
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 4 使用 RFFT 进行不相干性处理 (IP-RFFT)
- en: 0:  $W\in\mathbb{R}^{m\times n},H\in\mathbb{R}^{n\times n}$
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 0:  $W\in\mathbb{R}^{m\times n},H\in\mathbb{R}^{n\times n}$
- en: Here we described the Randomized Fast Fourier Transform (RFFT), $x\to VSx$,
    and interpreting the corresponding 2-tuples as a complex number.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们描述了随机快速傅里叶变换 (RFFT)，$x\to VSx$，并将相应的 2-元组解释为一个复数。
- en: 'Incoherence processing via the RFFT achieves similar theoretical guarantees
    as the RHT, see Lemmas [A.3](#A1.Thmtheorem3 "Lemma A.3\. ‣ A.1 Incoherence Processing
    with the Randomized Hadamard Transform ‣ Appendix A Concentration Inequalities
    for the Randomized Hadamard Transform and Fast Fourier Transform ‣ QuIP#: Even
    Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks") and [A.4](#A1.Thmtheorem4
    "Lemma A.4\. ‣ A.1 Incoherence Processing with the Randomized Hadamard Transform
    ‣ Appendix A Concentration Inequalities for the Randomized Hadamard Transform
    and Fast Fourier Transform ‣ QuIP#: Even Better LLM Quantization with Hadamard
    Incoherence and Lattice Codebooks"). Ultimately the choice of the orthogonal transformation
    is up to the user. A Fourier transform works almost as well as a Hamard transform
    in practice (Table [1](#S3.T1 "Table 1 ‣ 3 Incoherence Processing with the Randomized
    Hadamard Transform ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence
    and Lattice Codebooks")), so if a fast Hadamard implementation is not available,
    the FFT is a good option.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '通过 RFFT 进行不一致性处理可以达到类似于 RHT 的理论保证，参见引理 [A.3](#A1.Thmtheorem3 "Lemma A.3\. ‣
    A.1 Incoherence Processing with the Randomized Hadamard Transform ‣ Appendix A
    Concentration Inequalities for the Randomized Hadamard Transform and Fast Fourier
    Transform ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence and
    Lattice Codebooks") 和 [A.4](#A1.Thmtheorem4 "Lemma A.4\. ‣ A.1 Incoherence Processing
    with the Randomized Hadamard Transform ‣ Appendix A Concentration Inequalities
    for the Randomized Hadamard Transform and Fast Fourier Transform ‣ QuIP#: Even
    Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks")。最终，正交变换的选择取决于用户。实际上，傅里叶变换的效果几乎与
    Hadamard 变换一样好（表 [1](#S3.T1 "Table 1 ‣ 3 Incoherence Processing with the Randomized
    Hadamard Transform ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence
    and Lattice Codebooks")），因此如果没有快速的 Hadamard 实现，FFT 是一个不错的选择。'
- en: Appendix B Block LDLQ
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 阻塞 LDLQ
- en: Lemma B.1.
  id: totrans-305
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 B.1。
- en: Let $H\in\mathbb{R}^{nd\times nd}$. Then
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $H\in\mathbb{R}^{nd\times nd}$。然后
- en: '|  | $\operatorname{tr}\left(D\right)\leq\operatorname{tr}\left(H^{1/2}\right)\cdot\left\&#124;H^{1/2}\odot
    M_{D}\right\&#124;_{2},$ |  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname{tr}\left(D\right)\leq\operatorname{tr}\left(H^{1/2}\right)\cdot\left\|H^{1/2}\odot
    M_{D}\right\|_{2},$ |  |'
- en: where $M_{D}=I\otimes\mathbf{1}_{d\times d}$ has
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M_{D}=I\otimes\mathbf{1}_{d\times d}$ 具有
- en: '|  | $\&#124;U_{ij}\&#124;\leq\frac{\mu}{\sqrt{nd}},$ |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | $\|U_{ij}\|\leq\frac{\mu}{\sqrt{nd}},$ |  |'
- en: then
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '|  | $\operatorname{tr}\left(D\right)\leq\frac{\mu^{2}}{n}\operatorname{tr}\left(H^{1/2}\right)^{2}.$
    |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname{tr}\left(D\right)\leq\frac{\mu^{2}}{n}\operatorname{tr}\left(H^{1/2}\right)^{2}.$
    |  |'
- en: Proof.
  id: totrans-312
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Consider the optimization problem
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑优化问题
- en: '|  | minimize: | $\displaystyle\operatorname{tr}\left(R^{T}HR\right)$ |  |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|  | 最小化: | $\displaystyle\operatorname{tr}\left(R^{T}HR\right)$ |  |'
- en: '|  | subject to: | $\displaystyle R\text{ unit block lower diagonal}.$ |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  | 约束条件: | $\displaystyle R\text{ 单位块下三角对角矩阵}.$ |  |'
- en: Observe that the derivative of the loss is
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到损失的导数是
- en: '|  | $\nabla f(R)=HR.$ |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla f(R)=HR.$ |  |'
- en: If $R=L^{-1}$.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 $R=L^{-1}$。
- en: Now, let $M$. Observe that
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，设 $M$。请注意
- en: '|  | $\displaystyle\left(I+\alpha M\odot H^{1/2}\right)^{T}\left(I+\alpha M\odot
    H^{1/2}\right)$ |  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left(I+\alpha M\odot H^{1/2}\right)^{T}\left(I+\alpha M\odot
    H^{1/2}\right)$ |  |'
- en: '|  |  | $\displaystyle\succeq I+\alpha(M+M^{T})\odot H^{1/2}$ |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\succeq I+\alpha(M+M^{T})\odot H^{1/2}$ |  |'
- en: '|  |  | $\displaystyle\succeq\alpha M_{D}\odot H^{1/2}+\alpha(M+M^{T})\odot
    H^{1/2}$ |  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\succeq\alpha M_{D}\odot H^{1/2}+\alpha(M+M^{T})\odot
    H^{1/2}$ |  |'
- en: '|  |  | $\displaystyle\succeq\alpha H^{1/2}.$ |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\succeq\alpha H^{1/2}.$ |  |'
- en: It follows by inverting both sides that $RR^{T}\preceq\alpha^{-1}H^{-1/2}$.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反转两边，可以得出 $RR^{T}\preceq\alpha^{-1}H^{-1/2}$。
- en: So, for this $R$,
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于这个 $R$，
- en: '|  | $\operatorname{tr}\left(R^{T}HR\right)=\operatorname{tr}\left(HRR^{T}\right)\leq\alpha^{-1}\operatorname{tr}\left(H^{1/2}\right).$
    |  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname{tr}\left(R^{T}HR\right)=\operatorname{tr}\left(HRR^{T}\right)\leq\alpha^{-1}\operatorname{tr}\left(H^{1/2}\right).$
    |  |'
- en: This proves the first part of the lemma. For the second part, observe that
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了引理的第一部分。对于第二部分，请注意
- en: '|  | $\displaystyle\left\&#124;H^{1/2}\odot M_{D}\right\&#124;_{2}$ |  |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left\|H^{1/2}\odot M_{D}\right\|_{2}$ |  |'
- en: '|  |  | $\displaystyle\leq\frac{\mu^{2}}{n}\operatorname{tr}\left(H^{1/2}\right).$
    |  |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\frac{\mu^{2}}{n}\operatorname{tr}\left(H^{1/2}\right).$
    |  |'
- en: This proves the lemma. ∎
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了引理。 ∎
- en: \thmLDLQ
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: \thmLDLQ
- en: '*'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '*'
- en: Proof.
  id: totrans-333
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: First recall that from the description of block LDLQ,
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 首先回顾一下块 LDLQ 的描述，
- en: '|  | $\hat{W}_{k}=\mathbf{Q}(W_{k}+(W_{:(k-1)}-\hat{W}_{:(k-1)})\mathbf{A}_{k}).$
    |  |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{W}_{k}=\mathbf{Q}(W_{k}+(W_{:(k-1)}-\hat{W}_{:(k-1)})\mathbf{A}_{k}).$
    |  |'
- en: We can also write this in matrix form in terms of the matrix $\mathbf{L}_{k}$
    as
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以用矩阵形式以矩阵 $\mathbf{L}_{k}$ 表示这个。
- en: '|  | $\hat{W}=\mathbf{Q}(W+(W-\hat{W})(\mathbf{L}^{T}-I)).$ |  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{W}=\mathbf{Q}(W+(W-\hat{W})(\mathbf{L}^{T}-I)).$ |  |'
- en: Here, $\mathbf{Q}$ denote the quantization error
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\mathbf{Q}$表示量化误差
- en: '|  | $\eta=(W+(W-\hat{W})(\mathbf{L}^{T}-I))-\mathbf{Q}(W+(W-\hat{W})(\mathbf{L}^{T}-I)).$
    |  |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  | $\eta=(W+(W-\hat{W})(\mathbf{L}^{T}-I))-\mathbf{Q}(W+(W-\hat{W})(\mathbf{L}^{T}-I)).$
    |  |'
- en: Then
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '|  | $\hat{W}=(W+(W-\hat{W})(\mathbf{L}^{T}-I))-\eta,$ |  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{W}=(W+(W-\hat{W})(\mathbf{L}^{T}-I))-\eta,$ |  |'
- en: which simplifies to
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这简化为
- en: '|  | $(W-\hat{W})\mathbf{L}^{T}=\eta.$ |  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  | $(W-\hat{W})\mathbf{L}^{T}=\eta.$ |  |'
- en: This means that
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着
- en: '|  | $1$2 |  |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: But by assumption, $\mathbf{E}[\eta\eta^{T}]\preceq m\sigma^{2}I$ rows), so
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 但根据假设，$\mathbf{E}[\eta\eta^{T}]\preceq m\sigma^{2}I$ 行，因此
- en: '|  | $\mathbf{E}[\operatorname{tr}((\hat{W}-W)H(\hat{W}-W)^{T})]\leq m\sigma^{2}\mathbf{E}[\operatorname{tr}(\mathbf{D})].$
    |  |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{E}[\operatorname{tr}((\hat{W}-W)H(\hat{W}-W)^{T})]\leq m\sigma^{2}\mathbf{E}[\operatorname{tr}(\mathbf{D})].$
    |  |'
- en: 'Combining this with the result of Lemma [B.1](#A2.Thmtheorem1 "Lemma B.1\.
    ‣ Appendix B Block LDLQ ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence
    and Lattice Codebooks") proves the theorem. ∎'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '将此与引理 [B.1](#A2.Thmtheorem1 "Lemma B.1\. ‣ Appendix B Block LDLQ ‣ QuIP#: Even
    Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks") 的结果结合，可以证明定理。
    ∎'
- en: Appendix C E8P details
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C E8P细节
- en: C.1 Constructing $S$
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 构造$S$
- en: We use the following 29 elements of $\hat{D}_{8}$ to 256 entries.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下29个元素的$\hat{D}_{8}$来表示256个条目。
- en: '[PRE0]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: C.2 Example Decoding with E8P
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 使用E8P的示例解码
- en: Here, we give an example of decoding with E8P. In this example, the first 8
    bits of the codeword encode the entry in $S$. In this example, let that be the
    vector
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们给出了使用E8P解码的一个示例。在这个示例中，码字的前8位编码了$S$中的条目。在这个示例中，让我们设它为向量
- en: '|  | $s=\left\{\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{3}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2}\right\},$
    |  |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  | $s=\left\{\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{3}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2}\right\},$
    |  |'
- en: which is not in $\hat{D_{8}}$. Then, the next 7 bits 1001011 would indicate
    that we need to negate the 1st, 2nd, 4th, and 7th from right bits. Since we need
    an odd number of sign flips, the 8th from right bit is also a sign flip. The sign-decoded
    vector is then
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这不在$\hat{D_{8}}$中。那么，接下来的7位1001011将指示我们需要否定从右边第1、第2、第4和第7位。由于我们需要奇数次符号翻转，第8位也需要符号翻转。符号解码的向量是
- en: '|  | $\left\{-\frac{1}{2},-\frac{1}{2},\frac{1}{2},\frac{3}{2},-\frac{1}{2},\frac{1}{2},-\frac{1}{2},-\frac{1}{2}\right\},$
    |  |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left\{-\frac{1}{2},-\frac{1}{2},\frac{1}{2},\frac{3}{2},-\frac{1}{2},\frac{1}{2},-\frac{1}{2},-\frac{1}{2}\right\},$
    |  |'
- en: which we can verify is in $E_{8}$, so the final decoded vector is
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以验证它在$E_{8}$中，因此最终解码向量是
- en: '|  | $\left\{-\frac{1}{4},-\frac{3}{4},\frac{3}{4},\frac{7}{4},-\frac{1}{4},\frac{3}{4},-\frac{1}{4},-\frac{1}{4}\right\},$
    |  |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left\{-\frac{1}{4},-\frac{3}{4},\frac{3}{4},\frac{7}{4},-\frac{1}{4},\frac{3}{4},-\frac{1}{4},-\frac{1}{4}\right\},$
    |  |'
- en: which is in $E_{8}+\frac{1}{4}$ as desired.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在$E_{8}+\frac{1}{4}$中，如所期望的那样。
- en: C.3 Why not K-Means?
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 为什么不是K均值？
- en: A significant motivating factor behind E8P is that post-incoherence processing,
    entries of $W$-dimensional Gaussian, we can get around this but sacrifice accuracy
    at the axis region. Second, using K-means requires storing a codebook in fp16,
    whereas the entries of E8P can be stored as 4 bit integers. This means that during
    inference, the source codebook for a 8 dimension K-means codebook will be 4 times
    larger than the source codebook of E8P, running the risk of a cache eviction.
    Finally, we observe that empirically, E8P actually outperforms K-means, which
    is somewhat interesting and suggests that allocating more information to the edge
    of the distribution, even after incoherence processing, is useful.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: E8P的一个重要动因是，经过不一致处理后，$W$维高斯条目的处理，我们可以规避这一点，但在轴区域牺牲了精度。其次，使用K均值需要将代码本存储为fp16，而E8P的条目可以存储为4位整数。这意味着在推理过程中，8维K均值代码本的源代码本将比E8P的源代码本大4倍，这有导致缓存驱逐的风险。最后，我们观察到，经验上，E8P实际上优于K均值，这一点有趣，表明即使在不一致处理后，将更多信息分配到分布边缘也是有用的。
- en: Appendix D Fine-Tuning During Quantization
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 量化过程中的微调
- en: 'In Algorithm [5](#alg5 "Algorithm 5 ‣ Appendix D Fine-Tuning During Quantization
    ‣ QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks")
    we describe our fine tuning procedure for QuIP$\#$.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '在算法 [5](#alg5 "Algorithm 5 ‣ Appendix D Fine-Tuning During Quantization ‣ QuIP#:
    Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks")
    中，我们描述了QuIP$\#$的微调过程。'
- en: Algorithm 5 QuIP$\#$ with Fine-Tuning
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 算法5 QuIP$\#$ 与微调
- en: 0:  Unquantized Model $M$ for early stopping.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 0:  用于早期停止的未量化模型$M$。
- en: Appendix E Additional Results
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 额外结果
- en: E.1 QuIP$\#$ on Mixtral 8x7B (Jiang et al., [2024](#bib.bib17)) and Falcon 180B
    (Almazrouei et al., [2023](#bib.bib1))
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 QuIP$\#$在 Mixtral 8x7B（Jiang et al., [2024](#bib.bib17)）和 Falcon 180B（Almazrouei
    et al., [2023](#bib.bib1)）上的表现
- en: 'Table 6: 2 bit QuIP$\#$ scales to different architectures without issue.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 2 位 QuIP$\#$ 可无问题地扩展到不同架构。'
- en: '| Model | Bits | Wiki2 | C4 | ArcC | ArcE | BoolQ | PiQA | Wino |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 位数 | Wiki2 | C4 | ArcC | ArcE | BoolQ | PiQA | Wino |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Mixtral-8x7B | 16 | 3.45 | 6.85 | 0.56 | 0.74 | 0.85 | 0.84 | 0.75 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | 16 | 3.45 | 6.85 | 0.56 | 0.74 | 0.85 | 0.84 | 0.75 |'
- en: '| Mixtral-8x7B | 2 | 4.69 | 8.25 | 0.49 | 0.68 | 0.81 | 0.80 | 0.73 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | 2 | 4.69 | 8.25 | 0.49 | 0.68 | 0.81 | 0.80 | 0.73 |'
- en: '| Falcon-180B | 16 | 3.30 | 6.31 | 0.61 | 0.82 | 0.87 | 0.85 | 0.81 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| Falcon-180B | 16 | 3.30 | 6.31 | 0.61 | 0.82 | 0.87 | 0.85 | 0.81 |'
- en: '| Falcon-180B | 2 | 4.18 | 7.06 | 0.58 | 0.81 | 0.84 | 0.84 | 0.81 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| Falcon-180B | 2 | 4.18 | 7.06 | 0.58 | 0.81 | 0.84 | 0.84 | 0.81 |'
- en: E.2 Zeroshot performance for ablation on lattice codebooks and fine-tuning
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 零样本性能在格点代码本和微调上的影响
- en: 'Table 7: Ablation on lattice codebooks and fine-tuning. QuIP$\#$ uses lattice
    codebooks and performs fine-tuning.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 在格点代码本和微调上的消融研究。QuIP$\#$使用格点代码本并进行微调。'
- en: '| Model | Method | Bits |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 位数 |'
- en: '&#124; ArcC &#124;'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ArcC &#124;'
- en: '&#124; (acc_norm) &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc_norm) &#124;'
- en: '|'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ArcE &#124;'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ArcE &#124;'
- en: '&#124; (acc_norm) &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc_norm) &#124;'
- en: '|'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; BoolQ &#124;'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BoolQ &#124;'
- en: '&#124; (acc) &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc) &#124;'
- en: '|'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PiQA &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PiQA &#124;'
- en: '&#124; (acc_norm) &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc_norm) &#124;'
- en: '|'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Wino &#124;'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Wino &#124;'
- en: '&#124; (acc) &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc) &#124;'
- en: '|'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2-70 | Native | 16 | 48.0 | 59.7 | 76.6 | 80.9 | 76.8 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 2-70 | 原生 | 16 | 48.0 | 59.7 | 76.6 | 80.9 | 76.8 |'
- en: '| 2-70 | QuIP# no FT & no $E_{8}$ | 4 | 49.4 | 60.1 | 77.6 | 80.7 | 76.1 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 2-70 | QuIP# 无 FT & 无 $E_{8}$ | 4 | 49.4 | 60.1 | 77.6 | 80.7 | 76.1 |'
- en: '| 2-70 | QuIP# No FT | 4 | 48.3 | 60.1 | 78.4 | 80.6 | 76.2 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 2-70 | QuIP# 无 FT | 4 | 48.3 | 60.1 | 78.4 | 80.6 | 76.2 |'
- en: '| 2-70 | QuIP# | 4 | 48.3 | 59.4 | 77.4 | 80.7 | 77.1 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 2-70 | QuIP# | 4 | 48.3 | 59.4 | 77.4 | 80.7 | 77.1 |'
- en: '| 2-70 | QuIP# no FT & no $E_{8}$ | 3 | 47.4 | 59.1 | 75.8 | 80.9 | 77.5 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 2-70 | QuIP# 无 FT & 无 $E_{8}$ | 3 | 47.4 | 59.1 | 75.8 | 80.9 | 77.5 |'
- en: '| 2-70 | QuIP# No FT | 3 | 47.9 | 59.9 | 78.8 | 79.9 | 77.0 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 2-70 | QuIP# 无 FT | 3 | 47.9 | 59.9 | 78.8 | 79.9 | 77.0 |'
- en: '| 2-70 | QuIP# | 3 | 48.4 | 59.5 | 74.8 | 80.3 | 76.4 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 2-70 | QuIP# | 3 | 48.4 | 59.5 | 74.8 | 80.3 | 76.4 |'
- en: '| 2-70 | QuIP# no FT & no $E_{8}$ | 2 | 43.5 | 56.2 | 75.1 | 78.1 | 76.0 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 2-70 | QuIP# 无 FT & 无 $E_{8}$ | 2 | 43.5 | 56.2 | 75.1 | 78.1 | 76.0 |'
- en: '| 2-70 | QuIP# No FT | 2 | 47.2 | 59.5 | 79.1 | 78.6 | 74.2 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 2-70 | QuIP# 无 FT | 2 | 47.2 | 59.5 | 79.1 | 78.6 | 74.2 |'
- en: '| 2-70 | QuIP# | 2 | 47.7 | 59.1 | 80.3 | 79.4 | 75.9 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 2-70 | QuIP# | 2 | 47.7 | 59.1 | 80.3 | 79.4 | 75.9 |'
- en: '| 2-13 | Native | 16 | 44.3 | 58.0 | 69.0 | 79.0 | 69.9 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 2-13 | 原生 | 16 | 44.3 | 58.0 | 69.0 | 79.0 | 69.9 |'
- en: '| 2-13 | QuIP# no FT & no $E_{8}$ | 4 | 43.7 | 58.6 | 70.1 | 78.7 | 69.6 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 2-13 | QuIP# 无 FT & 无 $E_{8}$ | 4 | 43.7 | 58.6 | 70.1 | 78.7 | 69.6 |'
- en: '| 2-13 | QuIP# No FT | 4 | 42.9 | 56.4 | 67.8 | 78.9 | 69.9 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 2-13 | QuIP# 无 FT | 4 | 42.9 | 56.4 | 67.8 | 78.9 | 69.9 |'
- en: '| 2-13 | QuIP# | 4 | 44.2 | 57.7 | 69.7 | 78.9 | 69.9 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 2-13 | QuIP# | 4 | 44.2 | 57.7 | 69.7 | 78.9 | 69.9 |'
- en: '| 2-13 | QuIP# no FT & no $E_{8}$ | 3 | 42.1 | 55.2 | 70.0 | 77.8 | 69.5 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 2-13 | QuIP# 无 FT & 无 $E_{8}$ | 3 | 42.1 | 55.2 | 70.0 | 77.8 | 69.5 |'
- en: '| 2-13 | QuIP# No FT | 3 | 41.9 | 57.7 | 73.3 | 78.1 | 68.0 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 2-13 | QuIP# 无 FT | 3 | 41.9 | 57.7 | 73.3 | 78.1 | 68.0 |'
- en: '| 2-13 | QuIP# | 3 | 43.3 | 57.7 | 69.8 | 78.4 | 69.1 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 2-13 | QuIP# | 3 | 43.3 | 57.7 | 69.8 | 78.4 | 69.1 |'
- en: '| 2-13 | QuIP# no FT & no $E_{8}$ | 2 | 36.3 | 50.8 | 67.4 | 73.4 | 63.1 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 2-13 | QuIP# 无 FT & 无 $E_{8}$ | 2 | 36.3 | 50.8 | 67.4 | 73.4 | 63.1 |'
- en: '| 2-13 | QuIP# No FT | 2 | 37.1 | 50.1 | 66.5 | 75.7 | 63.6 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 2-13 | QuIP# 无 FT | 2 | 37.1 | 50.1 | 66.5 | 75.7 | 63.6 |'
- en: '| 2-13 | QuIP# | 2 | 41.3 | 55.1 | 68.3 | 77.4 | 67.7 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 2-13 | QuIP# | 2 | 41.3 | 55.1 | 68.3 | 77.4 | 67.7 |'
- en: '| 2-7 | Native | 16 | 40.6 | 53.5 | 71.0 | 76.9 | 67.0 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 2-7 | 原生 | 16 | 40.6 | 53.5 | 71.0 | 76.9 | 67.0 |'
- en: '| 2-7 | QuIP# no FT & no $E_{8}$ | 4 | 39.5 | 51.9 | 71.3 | 76.6 | 67.3 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 2-7 | QuIP# 无 FT & 无 $E_{8}$ | 4 | 39.5 | 51.9 | 71.3 | 76.6 | 67.3 |'
- en: '| 2-7 | QuIP# No FT | 4 | 40.4 | 53.7 | 68.5 | 77.2 | 67.5 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 2-7 | QuIP# 无 FT | 4 | 40.4 | 53.7 | 68.5 | 77.2 | 67.5 |'
- en: '| 2-7 | QuIP# | 4 | 40.1 | 53.4 | 69.9 | 76.5 | 67.6 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 2-7 | QuIP# | 4 | 40.1 | 53.4 | 69.9 | 76.5 | 67.6 |'
- en: '| 2-7 | QuIP# no FT & no $E_{8}$ | 3 | 38.1 | 52.6 | 65.2 | 76.1 | 65.1 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 2-7 | QuIP# 无 FT & 无 $E_{8}$ | 3 | 38.1 | 52.6 | 65.2 | 76.1 | 65.1 |'
- en: '| 2-7 | QuIP# No FT | 3 | 37.7 | 53.1 | 70.6 | 76.7 | 67.6 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 2-7 | QuIP# 无 FT | 3 | 37.7 | 53.1 | 70.6 | 76.7 | 67.6 |'
- en: '| 2-7 | QuIP# | 3 | 39.4 | 53.8 | 69.7 | 76.1 | 66.5 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 2-7 | QuIP# | 3 | 39.4 | 53.8 | 69.7 | 76.1 | 66.5 |'
- en: '| 2-7 | QuIP# no FT & no $E_{8}$ | 2 | 29.2 | 42.5 | 63.3 | 68.0 | 59.0 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 2-7 | QuIP# 无 FT & 无 $E_{8}$ | 2 | 29.2 | 42.5 | 63.3 | 68.0 | 59.0 |'
- en: '| 2-7 | QuIP# No FT | 2 | 32.5 | 42.8 | 62.3 | 71.2 | 62.4 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 2-7 | QuIP# 无 FT | 2 | 32.5 | 42.8 | 62.3 | 71.2 | 62.4 |'
- en: '| 2-7 | QuIP# | 2 | 36.1 | 50.5 | 68.3 | 74.9 | 64.9 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 2-7 | QuIP# | 2 | 36.1 | 50.5 | 68.3 | 74.9 | 64.9 |'
- en: E.3 More Scaling Plots
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.3 更多缩放图
- en: '![Refer to caption](img/80de8e01bb1ca5194938bb72a20ea1a6.png)![Refer to caption](img/894b7e740a88b1a98978ea867154ad6e.png)![Refer
    to caption](img/7edf9e76a8e8f9c11e6a25a5d5b31b68.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/80de8e01bb1ca5194938bb72a20ea1a6.png)![参考标题](img/894b7e740a88b1a98978ea867154ad6e.png)![参考标题](img/7edf9e76a8e8f9c11e6a25a5d5b31b68.png)'
- en: 'Figure 5: QuIP$\#$ 2 and 3 bit scale better than AQLM 2 and 3 bit. (Top Right)
    Llama 2 C4 Perplexity. Context length 4096\. (Bottom) Llama 1 C4 Perplexity. Context
    length 2048.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：QuIP$\#$ 2 和 3 位在缩放方面优于 AQLM 2 和 3 位。（右上）Llama 2 C4 困惑度。上下文长度 4096。（底部）Llama
    1 C4 困惑度。上下文长度 2048。
- en: Appendix F Implementation Details
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 实现细节
- en: This section contains implementation details for our Llama experiments. These
    details also mostly apply to the Mixtral and Falcon numbers except we use the
    Falcon dataset (Almazrouei et al., [2023](#bib.bib1)) as it is publicly avaiable.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含我们 Llama 实验的实现细节。这些细节大部分也适用于 Mixtral 和 Falcon 数字，只是我们使用 Falcon 数据集（Almazrouei
    等，[2023](#bib.bib1)），因为它是公开可用的。
- en: F.1 Hessian Generation
  id: totrans-430
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 Hessian 生成
- en: Hessian matrices $H$ were generated with 6144 sequences of a model’s native
    context length (2048 for Llama 1, 4096 for Llama 2) from the RedPajama 1T (Computer,
    [2023](#bib.bib6)) dataset.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian 矩阵 $H$ 是用来自 RedPajama 1T (Computer, [2023](#bib.bib6)) 数据集的 6144 个序列生成的，该序列的长度为模型的原生上下文长度（Llama
    1 为 2048，Llama 2 为 4096）。
- en: F.2 Hadamard Matrices
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 Hadamard 矩阵
- en: We use Hadamard matrices available at Neil Sloane’s website ([Sloane,](#bib.bib27)
    ).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用在 Neil Sloane 网站上提供的 Hadamard 矩阵 ([Sloane,](#bib.bib27))。
- en: F.3 Perplexity and Zeroshot Evaluation
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.3 困惑度与零样本评估
- en: We use the OPTQ (Frantar et al., [2023](#bib.bib12)) “Wiktext2” and “C4” (not
    “C4 New”) sampling functions to calculate perplexity for our experiments. We use
    LM Eval (Gao et al., [2023](#bib.bib13)) to calculate zeroshot numbers.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 OPTQ（Frantar 等，[2023](#bib.bib12)）“Wiktext2”和“C4”（不是“C4 New”）采样函数来计算我们实验中的困惑度。我们使用
    LM Eval（Gao 等，[2023](#bib.bib13)）来计算零样本数据。
- en: F.4 Fine Tuning
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.4 微调
- en: For the within-transformer block section of fine-tuning, we use the Adam optimizer
    (Kingma & Ba, [2017](#bib.bib19)), a learning rate of $5\times 10^{-5}$ for everything
    else as above) for both the within-block and end to end fine tuning stages.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 对于变换器块内部微调部分，我们使用 Adam 优化器（Kingma & Ba，[2017](#bib.bib19)），学习率为 $5\times 10^{-5}$，用于上述所有其他设置，包括块内微调和端到端微调阶段。
- en: F.5 Hardware
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.5 硬件
- en: All experiments were run on NVIDIA A100 GPUs except for the timing numbers,
    which were measured on a NVIDIA RTX 4090 to see what was possible with the current
    state-of-the-art NVIDIA consumer GPU. We find that we can quantize Llama 2 70B
    without fine tuning in under 10 GPU-hours and with fine tuning in around 100 GPU-hours.
    Both numbers do not include Hessian generation, which can be done once for a model
    and reused across many different quantization experiments.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验均在 NVIDIA A100 GPU 上进行，除了计时数据，这些数据在 NVIDIA RTX 4090 上测量，以了解当前最先进的 NVIDIA
    消费级 GPU 的性能。我们发现，可以在不到 10 个 GPU 小时内量化 Llama 2 70B，微调则大约需要 100 个 GPU 小时。这两个数字都不包括
    Hessian 生成，Hessian 生成可以为模型一次性完成，并在许多不同的量化实验中重复使用。
- en: F.6 Code and Prequantized Models
  id: totrans-440
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.6 代码和预量化模型
- en: Our code is available at [https://github.com/Cornell-RelaxML/quip-sharp](https://github.com/Cornell-RelaxML/quip-sharp)
    and prequantized QuIP$\#$ models are available at [https://huggingface.co/relaxml](https://huggingface.co/relaxml).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码可以在 [https://github.com/Cornell-RelaxML/quip-sharp](https://github.com/Cornell-RelaxML/quip-sharp)
    上找到，预量化的 QuIP$\#$ 模型可以在 [https://huggingface.co/relaxml](https://huggingface.co/relaxml)
    上获取。
- en: Appendix G Example Generation
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 示例生成
- en: Below are some example generations from Llama 2 70B chat quantized with QuIP$\#$ to
    2 bits, truncated to 256 tokens.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从 Llama 2 70B 聊天模型量化到 2 位的 QuIP$\#$ 示例生成，截断为 256 个标记。
- en: 'Prompt: How much wood could
    a woodchuck chuck if a woodchuck could chuck wood? QuIP$\#$: Here is a simple
    example of a matrix multiply using CUDA:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果一只土拨鼠能投掷木材，它能投掷多少木材？ QuIP$\#$：这是一个使用
    CUDA 的矩阵乘法简单示例：
- en: '[PRE1]'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1]'
