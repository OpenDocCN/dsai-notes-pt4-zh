- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:48:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:11
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Low-Rank Quantization-Aware Training for LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对大语言模型的低秩量化感知训练
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.06385](https://ar5iv.labs.arxiv.org/html/2406.06385)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.06385](https://ar5iv.labs.arxiv.org/html/2406.06385)
- en: \newfloatcommand
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \newfloatcommand
- en: capbtabboxtable[][\FBwidth]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: capbtabboxtable[][\FBwidth]
- en: Yelysei Bondarenko, Riccardo Del Chiaro, Markus Nagel
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Yelysei Bondarenko, Riccardo Del Chiaro, Markus Nagel
- en: Qualcomm AI Research
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 高通人工智能研究
- en: Amsterdam, The Netherlands
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 荷兰阿姆斯特丹
- en: '{ybond, rdelchia, markusn}@qti.qualcomm.com Qualcomm AI Research is an initiative
    of Qualcomm Technologies, Inc.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{ybond, rdelchia, markusn}@qti.qualcomm.com 高通人工智能研究是高通技术公司的一个倡议。'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models (LLMs) are omnipresent, however their practical deployment
    is challenging due to their ever increasing computational and memory demands.
    Quantization is one of the most effective ways to make them more compute and memory
    efficient. Quantization-aware training (QAT) methods, generally produce the best
    quantized performance, however it comes at the cost of potentially long training
    time and excessive memory usage, making it impractical when applying for LLMs.
    Inspired by parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA)
    literature, we propose LR-QAT – a lightweight and memory-efficient QAT algorithm
    for LLMs. LR-QAT employs several components to save memory without sacrificing
    predictive performance: (a) low-rank auxiliary weights that are aware of the quantization
    grid; (b) a downcasting operator using fixed-point or double-packed integers and
    (c) checkpointing. Unlike most related work, our method (i) is inference-efficient,
    leading to no additional overhead compared to traditional PTQ; (ii) can be seen
    as a general extended pretraining framework, meaning that the resulting model
    can still be utilized for any downstream task afterwards; (iii) can be applied
    across a wide range of quantization settings, such as different choices quantization
    granularity, activation quantization, and seamlessly combined with many PTQ techniques.
    We apply LR-QAT to LLaMA-2/3 and Mistral model families and validate its effectiveness
    on several downstream tasks. Our method outperforms common post-training quantization
    (PTQ) approaches and reaches the same model performance as full-model QAT at the
    fraction of its memory usage. Specifically, we can train a 7B LLM on a single
    consumer grade GPU with 24GB of memory.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）无处不在，然而由于其不断增加的计算和内存需求，实际应用具有挑战性。量化是使其在计算和内存上更高效的有效方法之一。量化感知训练（QAT）方法通常能产生最佳的量化性能，但其代价是可能需要较长的训练时间和过度的内存使用，使得在LLMs上的应用不够实际。受到参数高效微调（PEFT）和低秩适应（LoRA）文献的启发，我们提出了LR-QAT——一种轻量级且内存高效的LLM量化感知训练算法。LR-QAT通过以下几个组件来节省内存而不牺牲预测性能：（a）感知量化网格的低秩辅助权重；（b）使用定点或双打包整数的降级运算符；（c）检查点记录。与大多数相关工作不同，我们的方法（i）推理效率高，相较于传统的PTQ没有额外的开销；（ii）可以看作是一种通用的扩展预训练框架，这意味着最终模型仍可用于任何下游任务；（iii）可以应用于各种量化设置，如不同的量化粒度、激活量化，并与许多PTQ技术无缝结合。我们将LR-QAT应用于LLaMA-2/3和Mistral模型系列，并在多个下游任务中验证了其有效性。我们的方法优于常见的后训练量化（PTQ）方法，并在内存使用上达到与全模型QAT相同的模型性能。具体而言，我们可以在具有24GB内存的单个消费级GPU上训练一个7B
    LLM。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In recent years, large language models (LLMs) have emerged as a powerful tool
    for a plethora of natural language processing tasks. As these models continue
    to grow in size and capability, addressing their ever increasing computational
    and memory demands becomes crucial for practical deployment, especially when considering
    resource-constrained edge devices.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）作为一种强大的工具在自然语言处理任务中崭露头角。随着这些模型在规模和能力上的不断增长，解决其日益增长的计算和内存需求对于实际应用变得至关重要，特别是在考虑到资源受限的边缘设备时。
- en: One of the most effective methods to tackle this problem is neural network quantization,
    which uses low-bit precision for weight and activation tensors. While recent post-training
    quantization (PTQ) methods can help with decreasing the model size and improving
    the computational efficiency of LLMs, they typically lead to subpar performance,
    especially in the case of low-bit ($\leq 4$) quantization. Quantization-aware
    training (QAT), conversely, yields significantly better model performance compared
    to PTQ. However, due to extreme model sizes of modern LLMs, using traditional
    QAT is very computationally expensive and requires a prohibitively high GPU memory
    usage, making it impractical.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的最有效方法之一是神经网络量化，它使用低位精度进行权重和激活张量的处理。虽然最近的后训练量化（PTQ）方法可以帮助减少模型大小并提高 LLM
    的计算效率，但它们通常会导致性能较差，尤其是在低位（$\leq 4$）量化的情况下。相比之下，量化感知训练（QAT）能够显著提升模型性能。然而，由于现代 LLM
    的极端模型大小，使用传统 QAT 是非常计算密集型的，并且需要过高的 GPU 内存使用，变得不切实际。
- en: Inspired by parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA)
    literature, we propose Low-Rank Quantization-Aware Training (LR-QAT) – a lightweight
    memory-efficient and inference-efficient QAT algorithm for LLMs. LR-QAT reduces
    the memory requirements of training a 7B LLM from >70GB of GPU memory to <21GB
    without degrading the predictive performance compared to traditional full-model
    QAT, making it possible to train such models on a single consumer grade GPU. Unlike
    most related work that combines low-rank adaptation with quantization, our method
    is also inference-efficient. After the training is complete, the auxiliary matrices
    are naturally absorbed into the quantized weight tensor without loss of accuracy
    and no extra overhead at inference time. LR-QAT is positioned as a general *extended
    pretraining* method, as opposed to being strictly a fine-tuning method – the resulting
    model is a low-bit general pretrained LLM, that can still be utilized for any
    task afterwards. If needed, our resulting low-bit pretrained LLM can be fine-tuned
    on specific downstream tasks or used with multiple LoRA adapters for rapid switching
    between various tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 受参数高效微调（PEFT）和低秩适配（LoRA）文献的启发，我们提出了低秩量化感知训练（LR-QAT）——一种轻量级的内存高效且推理高效的 QAT 算法，专为大语言模型（LLMs）设计。LR-QAT
    将训练一个 7B LLM 所需的 GPU 内存从超过 70GB 降低到不足 21GB，而不会降低预测性能，与传统的全模型 QAT 相比，使得在单个消费级 GPU
    上训练此类模型成为可能。与大多数将低秩适配与量化结合的相关工作不同，我们的方法也具有推理高效性。训练完成后，辅助矩阵自然地被吸收到量化权重张量中，精度没有损失，推理时也没有额外的开销。LR-QAT
    定位为一种通用的 *扩展预训练* 方法，而不是严格的微调方法——最终得到的模型是一个低位通用预训练 LLM，仍可用于之后的任何任务。如果需要，我们得到的低位预训练
    LLM 可以在特定下游任务上进行微调，或与多个 LoRA 适配器一起使用，以便在各种任务之间快速切换。
- en: 'LR-QAT introduces and combines several innovations designed to reduce memory
    use without sacrificing model performance: (1) a form of QAT with low-rank reparameterization,
    in which we place the low-rank weights in the integer domain to ensure they align
    with the quantization grid of the pretrained weights. This allows for seamless
    fusion during inference into a single low-bit integer matrix. (2) A downcasting
    operator that represents the frozen pretrained weights as low-bit INT-$b$) double-packed
    into INT8 or as fixed-point values stored in INT8. (3) Finally, we combine the
    proposed quantization formulation with gradient checkpointing to avoid aggressive
    memory spikes from storing some of the intermediate results in memory for the
    backward pass.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LR-QAT 引入并结合了几种旨在减少内存使用而不牺牲模型性能的创新：(1) 一种具有低秩重参数化的 QAT 形式，其中我们将低秩权重放置在整数域中，以确保它们与预训练权重的量化网格对齐。这允许在推理过程中无缝融合为单一的低位整数矩阵。(2)
    一种将冻结的预训练权重表示为低位 INT-$b$）的双重打包到 INT8 中或作为存储在 INT8 中的定点值的下转操作符。(3) 最后，我们将所提出的量化公式与梯度检查点结合使用，以避免在反向传递过程中因存储一些中间结果而导致的极端内存峰值。
- en: We apply LR-QAT to LLaMA-2/3 and Mistral model families and demonstrate its
    effectiveness on several general language modeling datasets and zero-shot evaluation
    on some of the common reasoning downstream tasks. Our method outperforms recent
    PTQ approaches and reaches the same predictive performance as full-model QAT at
    the fraction of its memory usage. Finally, our method can be applied across a
    wide range of quantization settings, including per-channel or per-block weight
    quantization, activation quantization, and can be combined with most of other
    PTQ techniques.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将LR-QAT应用于LLaMA-2/3和Mistral模型系列，并在几个通用语言建模数据集和一些常见推理下游任务的零样本评估中展示了其有效性。我们的方法优于近期的PTQ方法，并以较少的内存使用达到了与全模型QAT相同的预测性能。最后，我们的方法可以应用于各种量化设置，包括通道或块级权重量化、激活量化，并且可以与大多数其他PTQ技术结合使用。
- en: 2 Background and related work
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景和相关工作
- en: '![Refer to caption](img/60d160c2841cb777b482064476e777cd.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/60d160c2841cb777b482064476e777cd.png)'
- en: '![Refer to caption](img/bf6653e89698f29df02308456f696527.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bf6653e89698f29df02308456f696527.png)'
- en: 'Figure 1: *Left:* A schematic illustration of our proposed LR-QAT. $\mathbf{x}$,
    and BF16 compute data type).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：*左：* 我们提出的LR-QAT的示意图。$\mathbf{x}$，和BF16计算数据类型）。
- en: Neural network quantization is one of the most powerful ways to reduce model
    footprint, data transfer and compute requirements. By quantizing a model, high
    bit-width floating point weights and activations can be represented using low-bit
    numbers. On top of that, by using low-bit fixed-point representations, such as INT8,
    one can further reduce energy consumption since the fixed-point operations are
    more efficient than their floating-point counterparts. Quantizing to 8 bits or
    lower, however, typically introduces quantization noise in the model, resulting
    in a potential drop in accuracy/perplexity.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络量化是减少模型占用空间、数据传输和计算需求的最有效方法之一。通过量化模型，高位宽的浮点权重和激活可以用低位数表示。此外，通过使用如INT8的低位定点表示，可以进一步降低能耗，因为定点操作比浮点操作更高效。然而，量化到8位或更低位通常会在模型中引入量化噪声，导致准确性/困惑度的潜在下降。
- en: In this section, we provide a brief overview of uniform affine quantization
    and a summary of recent methods for LLM quantization. We will discuss some of
    the trade-offs of those techniques. Finally, we touch upon the challenges of LLM
    quantization and some of the limitations of current approaches.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要概述了均匀仿射量化和最近的LLM量化方法总结。我们将讨论这些技术的一些权衡。最后，我们将触及LLM量化的挑战以及当前方法的一些限制。
- en: Uniform affine quantization
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 均匀仿射量化
- en: 'We use the following definition of the quantization function:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下量化函数的定义：
- en: '|  | $\widehat{\mathbf{x}}:=q\left(\mathbf{x};\,s,z,b\right)=s\cdot\vphantom{\Bigg{(}}\Big{(}\,\smash{\underbrace{\operatorname{clip}\!\left(\left\lfloor{\frac{\mathbf{x}}{s}}\right\rceil+z;-2^{b-1},2^{b-1}-1\right)}_{\text{\normalsize$=:\mathbf{x}_{\mathbb{Z}}$}}}-z\Big{)},$
    |  | (1) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widehat{\mathbf{x}}:=q\left(\mathbf{x};\,s,z,b\right)=s\cdot\vphantom{\Bigg{(}}\Big{(}\,\smash{\underbrace{\operatorname{clip}\!\left(\left\lfloor{\frac{\mathbf{x}}{s}}\right\rceil+z;-2^{b-1},2^{b-1}-1\right)}_{\text{\normalsize$=:\mathbf{x}_{\mathbb{Z}}$}}}-z\Big{)},$
    |  | (1) |'
- en: where $\mathbf{x}$.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{x}$。
- en: Post-training quantization methods
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 后训练量化方法
- en: Post-training quantization (PTQ) algorithms take a pretrained high precision
    (FP32 / FP16 / BF16) network and convert it directly into a fixed-point network
    without the need for the original training pipeline [[2](#bib.bib2), [8](#bib.bib8),
    [11](#bib.bib11), [27](#bib.bib27), [34](#bib.bib34), [40](#bib.bib40), [47](#bib.bib47),
    [49](#bib.bib49), [50](#bib.bib50), [71](#bib.bib71)]. These methods are either
    data-free or only require a small calibration dataset and are generally quite
    easy to use. Having almost no hyperparameter tuning makes them usable via a single
    API call as a black-box method to quantize a pretrained neural network in a computationally
    efficient manner.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化（PTQ）算法将一个预训练的高精度（FP32 / FP16 / BF16）网络直接转换为定点网络，而无需原始的训练管道[[2](#bib.bib2),
    [8](#bib.bib8), [11](#bib.bib11), [27](#bib.bib27), [34](#bib.bib34), [40](#bib.bib40),
    [47](#bib.bib47), [49](#bib.bib49), [50](#bib.bib50), [71](#bib.bib71)]。这些方法要么无需数据，要么只需少量的校准数据集，通常使用起来非常简便。几乎无需超参数调整，使其能够通过单个API调用作为黑箱方法，以计算高效的方式量化预训练神经网络。
- en: Post-training quantization of LLMs is a challenging task due to presence of
    numerical outliers in weights and activations [[6](#bib.bib6), [7](#bib.bib7),
    [33](#bib.bib33), [14](#bib.bib14), [58](#bib.bib58)]. Existing LLM PTQ methods
    can be broadly categorized into *weights-only* quantization and *weight-activation*
    quantization.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的后训练量化是一项具有挑战性的任务，因为权重和激活中存在数值离群值[[6](#bib.bib6)、[7](#bib.bib7)、[33](#bib.bib33)、[14](#bib.bib14)、[58](#bib.bib58)]。现有的LLM
    PTQ方法可以大致分为*仅权重*量化和*权重-激活*量化。
- en: Weights-only quantization focuses on converting weights to low-bit values. For
    instance, GPTQ [[18](#bib.bib18)] employs second-order information to iteratively
    round grouped weights and correct the quantization error in the remaining groups.
    SpQR [[15](#bib.bib15)], AWQ [[41](#bib.bib41)] and OWQ [[35](#bib.bib35)] emphasize
    the importance of so-called “salient” weights that correspond to high-magnitude
    activations. Other recent W-only methods include [[30](#bib.bib30), [37](#bib.bib37),
    [46](#bib.bib46), [9](#bib.bib9)].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 仅权重量化专注于将权重转换为低位值。例如，GPTQ[[18](#bib.bib18)]利用二阶信息迭代地对分组权重进行取整，并纠正其余分组的量化误差。SpQR[[15](#bib.bib15)]、AWQ[[41](#bib.bib41)]和OWQ[[35](#bib.bib35)]强调了所谓的“显著”权重的重要性，这些权重对应于高幅度的激活。其他近期的W-only方法包括[[30](#bib.bib30)、[37](#bib.bib37)、[46](#bib.bib46)、[9](#bib.bib9)]。
- en: Weight-activation quantization compresses both weights and activations. SmoothQuant [[65](#bib.bib65)],
    LLM.int8() [[14](#bib.bib14)] and Outlier Suppression [[62](#bib.bib62)] achieve
    W8A8 quantization by managing activation outliers. LLM.int8() uses mixed-precision
    decomposition, while the other two employ channel-wise scaling. OmniQuant [[55](#bib.bib55)]
    modulates the extreme values of weights by optimizing the clipping threshold and
    shifts the challenge of quantization from activations to weights by employing
    the learnable equivalent transformation. Some of the other recent W&A PTQ methods
    are [[36](#bib.bib36), [43](#bib.bib43), [63](#bib.bib63), [68](#bib.bib68), [59](#bib.bib59),
    [67](#bib.bib67), [42](#bib.bib42)].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 权重-激活量化同时压缩权重和激活。SmoothQuant[[65](#bib.bib65)]、LLM.int8()[[14](#bib.bib14)]和Outlier
    Suppression[[62](#bib.bib62)]通过管理激活离群值实现W8A8量化。LLM.int8()使用混合精度分解，而另外两个则采用通道缩放。OmniQuant[[55](#bib.bib55)]通过优化剪切阈值来调节权重的极端值，并通过使用可学习的等效变换将量化挑战从激活转移到权重。一些其他近期的W&A
    PTQ方法包括[[36](#bib.bib36)、[43](#bib.bib43)、[63](#bib.bib63)、[68](#bib.bib68)、[59](#bib.bib59)、[67](#bib.bib67)、[42](#bib.bib42)]。
- en: Quantization-aware training methods
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化感知训练方法
- en: Quantization-aware training (QAT) methods [[4](#bib.bib4), [17](#bib.bib17),
    [22](#bib.bib22), [28](#bib.bib28), [34](#bib.bib34)] simulate quantization during
    training, allowing the model to find more optimal solutions compared to PTQ approaches.
    However, better accuracy/perplexity comes at the cost of neural network training,
    i.e., longer training times, increased memory usage, need for labeled data and
    hyperparameter search.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 量化感知训练（QAT）方法[[4](#bib.bib4)、[17](#bib.bib17)、[22](#bib.bib22)、[28](#bib.bib28)、[34](#bib.bib34)]在训练过程中模拟量化，使模型能够找到比PTQ方法更优的解决方案。然而，更好的准确度/困惑度以神经网络训练为代价，即更长的训练时间、增加的内存使用、对标记数据的需求以及超参数搜索。
- en: The excessive training cost and memory usage of traditional QAT methods make
    them unsuitable for quantizing modern LLMs. A few works that apply QAT to LLMs
    include LLM-QAT [[44](#bib.bib44)] that combine QAT with data-free knowledge distillation,
    and EdgeQAT [[56](#bib.bib56)] that only considers tiny (sub 100M parameter) language
    models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 传统QAT方法的过高训练成本和内存使用使其不适合量化现代LLM。应用QAT于LLM的一些工作包括LLM-QAT[[44](#bib.bib44)]，它将QAT与无数据知识蒸馏结合起来，以及EdgeQAT[[56](#bib.bib56)]，它仅考虑了小型（不足100M参数）语言模型。
- en: Low-rank adapters for fine-tuning
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用于微调的低秩适配器
- en: Low-rank adaptation (LoRA) [[25](#bib.bib25)] is a parameter efficient fine-tuning
    (PEFT) method that reduces memory requirements compared to standard training.
    LoRA freezes the pretrained weights $\bm{W}=\bm{W_{0}}$, LoRA computes
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适配（LoRA）[[25](#bib.bib25)]是一种参数高效的微调（PEFT）方法，相比于标准训练减少了内存需求。LoRA冻结了预训练的权重$\bm{W}=\bm{W_{0}}$，LoRA计算
- en: '|  | $\displaystyle\mathbf{y}=\bm{W_{0}}\mathbf{x}+\frac{\alpha}{r}\bm{A}\bm{B}\mathbf{x},$
    |  | (2) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{y}=\bm{W_{0}}\mathbf{x}+\frac{\alpha}{r}\bm{A}\bm{B}\mathbf{x},$
    |  | (2) |'
- en: where $\bm{A}\in\mathbb{R}^{m\times r}$).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\bm{A}\in\mathbb{R}^{m\times r}$）。
- en: Naturally, there have been several works that explored the combination of LoRA
    and quantization. QLoRA [[16](#bib.bib16)] quantizes the pretrained weights to
    4 bit using (a non-uniform) NF4 format and dequantizes them in the forward pass
    to further reduce fine-tuning memory footprint. QA-LoRA [[66](#bib.bib66)] uses
    INT4 quantization and introduces group-wise operators to enable quantization during
    inference stage. LoftQ [[39](#bib.bib39)] proposed an iterative SVD-based procedure
    for initializing $\bm{A}$ that yields faster fine-tuning convergence when used
    together with low-bit quantization. LQ-LoRA [[21](#bib.bib21)] further extends
    initialization technique from LoftQ to mixed precision and data aware cases. Other
    recent works include [[29](#bib.bib29), [70](#bib.bib70)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，已经有若干研究探讨了LoRA与量化的结合。QLoRA [[16](#bib.bib16)] 将预训练权重量化为4位，使用（非均匀的）NF4格式，并在前向传递中将其反量化，以进一步减少微调的内存占用。QA-LoRA [[66](#bib.bib66)]
    使用INT4量化，并引入了分组操作符以支持在推理阶段进行量化。LoftQ [[39](#bib.bib39)] 提出了基于迭代SVD的初始化程序，用于初始化$\bm{A}$，这种方法与低位量化结合使用时，能够加快微调收敛速度。LQ-LoRA [[21](#bib.bib21)]
    进一步扩展了LoftQ的初始化技术，涵盖了混合精度和数据感知的情况。其他最近的研究包括 [[29](#bib.bib29), [70](#bib.bib70)]。
- en: Finally, the closest work to ours is PEQA [[32](#bib.bib32)], that attempts
    to combine the benefits of inference-efficiency of QAT together with memory-efficiency
    of PEFT methods. However, their approach is different since they focus on a task-specific
    fine-tuning as opposed to being a general extended pretraining method. In addition
    to that, PEQA has significantly less degrees of freedom compared to our method,
    leading to a subpar performance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最接近我们工作的研究是PEQA [[32](#bib.bib32)]，该研究试图将QAT的推理效率与PEFT方法的内存效率结合起来。然而，他们的方法有所不同，因为他们专注于任务特定的微调，而不是通用的扩展预训练方法。此外，PEQA相比于我们的方法自由度显著较少，从而导致性能不如我们的方案。
- en: Motivation
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动机
- en: While generally fast and simple, PTQ suffers from limited performance in low-bit
    scenarios. Although QAT methods still perform well in low-bit regimes, their high
    training costs and memory usage make them impractical for LLMs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PTQ通常快速且简单，但在低位场景下其性能有限。尽管QAT方法在低位场景下仍表现良好，但其高训练成本和内存使用使其在LLMs中不够实际。
- en: LoRA-based methods address memory issues for efficient fine-tuning. However,
    in most cases they don’t consider efficient inference. The adapters $\bm{A}$ are
    dequantized to match the same data format, resulting in runtime overhead.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LoRA的方法解决了高效微调的内存问题。然而，在大多数情况下，它们没有考虑高效推理。适配器$\bm{A}$被反量化以匹配相同的数据格式，导致了运行时开销。
- en: Simply quantizing adapters after training will lead to a different quantization
    grid compared to $\bm{W_{0}}$). In addition to that, QA-LoRA and most of LoRA-based
    methods combine their proposed techniques with the task-specific fine-tuning,
    whereas we propose LR-QAT as an *extended pretraining* method.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在训练后对适配器进行量化将导致与$\bm{W_{0}}$不同的量化网格。此外，QA-LoRA和大多数基于LoRA的方法将其提出的技术与任务特定的微调结合起来，而我们提出LR-QAT作为*扩展预训练*方法。
- en: We are inspired by LoRA-based methods to make QAT more memory- and runtime-efficient.
    In addition to that, our goal is to design a method that is *inference-efficient*,
    *i.e*. where the low-rank adapters can be fused back into a low-bit integer matrix
    $\bm{W}_{\mathbb{Z}}$ without any loss of accuracy/perplexity, yielding PTQ level
    of inference efficiency. Contrary to QA-LoRA [[66](#bib.bib66)], we are not relaxing
    the quantization constraints – our method is applicable at any weight quantization
    granularity. Finally, we see our method as a general extended pretraining framework.
    The resulting model can afterwards still be used on any task. We summarize different
    trade-offs for the discussed techniques in Table [1](#S2.T1 "Table 1 ‣ Motivation
    ‣ 2 Background and related work ‣ Low-Rank Quantization-Aware Training for LLMs").
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们受LoRA基方法的启发，旨在使QAT在内存和运行时效率上更为高效。此外，我们的目标是设计一种*推理高效*的方法，即低秩适配器可以融合回低位整数矩阵$\bm{W}_{\mathbb{Z}}$而不损失准确性/困惑度，从而达到PTQ水平的推理效率。与QA-LoRA [[66](#bib.bib66)]
    相反，我们并没有放宽量化约束——我们的方法适用于任何权重量化粒度。最后，我们将我们的方法视为一种通用扩展预训练框架。结果模型之后仍可以用于任何任务。我们在表 [1](#S2.T1
    "Table 1 ‣ Motivation ‣ 2 Background and related work ‣ Low-Rank Quantization-Aware
    Training for LLMs")中总结了所讨论技术的不同权衡。
- en: '| Method | Accuracy | Memory efficiency | Inference efficiency |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 准确性 | 内存效率 | 推理效率 |'
- en: '| PTQ | ✕ | ✓ | ✓ |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| PTQ | ✕ | ✓ | ✓ |'
- en: '| (Full-model) QAT | ✓ | ✕ | ✓ |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| (Full-model) QAT | ✓ | ✕ | ✓ |'
- en: '| LoRA / PEFT | ✓ | ✓ | ✕ |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| LoRA / PEFT | ✓ | ✓ | ✕ |'
- en: '| LR-QAT (ours) | \textpdfrender TextRenderingMode=FillStroke, LineWidth=.5pt,
    ✓ | \textpdfrender TextRenderingMode=FillStroke, LineWidth=.5pt, ✓ | \textpdfrender
    TextRenderingMode=FillStroke, LineWidth=.5pt, ✓ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的方法） | \textpdfrender TextRenderingMode=FillStroke, LineWidth=.5pt,
    ✓ | \textpdfrender TextRenderingMode=FillStroke, LineWidth=.5pt, ✓ | \textpdfrender
    TextRenderingMode=FillStroke, LineWidth=.5pt, ✓ |'
- en: 'Table 1: A comparison between existing approaches and the proposed method.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：现有方法与提议方法的比较。
- en: 3 Method
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: We now discuss the components of LR-QAT followed by a formal definition of LR-QAT.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在讨论 LR-QAT 的组成部分，然后对 LR-QAT 进行正式定义。
- en: QAT with low-rank adapters
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 低秩适配器 QAT
- en: 'Let’s recall how traditional QAT works. Given a linear layer with a weight
    matrix $\bm{W}\in\mathbb{R}^{m\times k}$-bit symmetric uniform affine quantization,
    the quantization is simulated as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下传统 QAT 的工作原理。给定一个具有权重矩阵 $\bm{W}\in\mathbb{R}^{m\times k}$ 的线性层 - 位对称均匀仿射量化，量化的模拟如下：
- en: '|  | $\displaystyle\widehat{\bm{W}}:=\mathbf{s}\cdot\operatorname{clip}\left(\left\lfloor{\frac{\bm{W}}{\mathbf{s}}}\right\rceil,-2^{b-1},2^{b-1}-1\right),$
    |  | (3) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{\bm{W}}:=\mathbf{s}\cdot\operatorname{clip}\left(\left\lfloor{\frac{\bm{W}}{\mathbf{s}}}\right\rceil,-2^{b-1},2^{b-1}-1\right),$
    |  | (3) |'
- en: 'where weights $\bm{W}$. When applied to LLMs, it is straightforward to see
    that this procedure is very expensive: we have to learn a comparable number of
    parameters that was used for pretraining, leading to excessive memory usage.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在权重 $\bm{W}$ 的情况下。当应用于 LLM 时，很容易看出这一过程是非常昂贵的：我们必须学习与预训练时使用的参数数量相当的参数，导致过度的内存使用。
- en: To make this approach more practical we *freeze* the pretrained weights $\bm{W}$
    without loss of accuracy to facilitate efficient inference. To accommodate that,
    we put the auxiliary matrices inside the rounding operator as follows
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这种方法更具实用性，我们*冻结*了预训练的权重 $\bm{W}$，以便在不损失精度的情况下促进高效推断。为了适应这一点，我们将辅助矩阵放入舍入运算符中，如下所示：
- en: '|  | $\displaystyle\widehat{\bm{W}}:=\mathbf{s}\cdot\operatorname{clip}\left(\left\lfloor{\frac{\bm{W_{0}}}{\mathbf{s}}+\frac{\alpha}{r}\bm{A}\bm{B}}\right\rceil,-2^{b-1},2^{b-1}-1\right),$
    |  | (4) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{\bm{W}}:=\mathbf{s}\cdot\operatorname{clip}\left(\left\lfloor{\frac{\bm{W_{0}}}{\mathbf{s}}+\frac{\alpha}{r}\bm{A}\bm{B}}\right\rceil,-2^{b-1},2^{b-1}-1\right),$
    |  | (4) |'
- en: where we are using STE assumption for the rounding operation to compute the
    gradients of the loss w.r.t. $\bm{A}$) and are typically stored in higher precision
    formats such as BF16.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们使用 STE 假设来进行舍入操作，以计算相对于 $\bm{A}$ 的损失梯度，并且这些梯度通常以更高精度格式（如 BF16）存储。
- en: Downcasting operator
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下采样运算符
- en: The formulation ([4](#S3.E4 "In QAT with low-rank adapters ‣ 3 Method ‣ Low-Rank
    Quantization-Aware Training for LLMs")) is already significantly more memory efficient
    compared to standard full-model QAT ([3](#S3.E3 "In QAT with low-rank adapters
    ‣ 3 Method ‣ Low-Rank Quantization-Aware Training for LLMs")). We don’t need to
    compute neither gradients w.r.t. weights $\bm{W}$.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 公式 ([4](#S3.E4 "在低秩适配器 QAT ‣ 3 方法 ‣ 低秩量化感知训练用于LLMs")) 相比于标准全模型 QAT ([3](#S3.E3
    "在低秩适配器 QAT ‣ 3 方法 ‣ 低秩量化感知训练用于LLMs")) 已经显著节省了内存。我们不需要计算相对于权重 $\bm{W}$ 的梯度。
- en: 'Given that the weight matrix $\bm{W_{0}}$ during every forward pass. To ensure
    stable training, the scale generally needs to be stored in a high-precision format.
    Therefore, to simplify further, we propose the following variant of low-rank QAT:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到每次前向传递中的权重矩阵 $\bm{W_{0}}$。为了确保训练的稳定性，缩放因子通常需要以高精度格式存储。因此，为了简化处理，我们提出了以下低秩
    QAT 的变体：
- en: '|  | $\displaystyle\widehat{\bm{W}}:=\mathbf{s}\cdot\operatorname{clip}\left(\left\lfloor{\frac{\bm{W_{0}}}{\mathbf{s_{0}}}+\frac{\alpha}{r}\bm{A}\bm{B}}\right\rceil,-2^{b-1},2^{b-1}-1\right),$
    |  | (5) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{\bm{W}}:=\mathbf{s}\cdot\operatorname{clip}\left(\left\lfloor{\frac{\bm{W_{0}}}{\mathbf{s_{0}}}+\frac{\alpha}{r}\bm{A}\bm{B}}\right\rceil,-2^{b-1},2^{b-1}-1\right),$
    |  | (5) |'
- en: where we use the initial scale¹¹1A frozen scale obtained after initial range
    estimation before the training begins. $\mathbf{s_{0}}$ outside of the clipping
    operator can still be learned. Empirically, we found that ([5](#S3.E5 "In Downcasting
    operator ‣ 3 Method ‣ Low-Rank Quantization-Aware Training for LLMs")) performs
    consistently on par with or even slightly better compared to ([4](#S3.E4 "In QAT
    with low-rank adapters ‣ 3 Method ‣ Low-Rank Quantization-Aware Training for LLMs")).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们使用初始缩放¹¹1A 在训练开始前通过初始范围估计获得的冻结缩放因子。$\mathbf{s_{0}}$ 在舍入运算符之外仍然可以学习。通过经验，我们发现 ([5](#S3.E5
    "在下采样运算符 ‣ 3 方法 ‣ 低秩量化感知训练用于LLMs")) 的表现始终与 ([4](#S3.E4 "在低秩适配器 QAT ‣ 3 方法 ‣ 低秩量化感知训练用于LLMs"))
    相当，甚至略优。
- en: During training the pretrained weights are represented and stored as follows
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，预训练的权重表示和存储如下：
- en: '|  | $\displaystyle\bm{\Phi_{0}}:=\varphi\!\left(\frac{\bm{W_{0}}}{\mathbf{s_{0}}}\right),$
    |  | (6) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{\Phi_{0}}:=\varphi\!\left(\frac{\bm{W_{0}}}{\mathbf{s_{0}}}\right),$
    |  | (6) |'
- en: where $\varphi\!\left(\cdot\right)$ would cast the input to one of pre-existing
    floating-point formats, such as FP16, BF16, FP8 etc.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\varphi\!\left(\cdot\right)$ 将输入转换为已有的浮点格式之一，如 FP16、BF16、FP8 等。
- en: Inspired by traditional fixed point quantization, we also explore integer representations
    for $\varphi\!\left(\cdot\right)$ numbers can be *double-packed* into a single INT8
    number, leading to further memory savings. This is helpful because most of the
    common deep learning frameworks like PyTorch, at the time of writing this paper,
    don’t natively support low-bit formats such as INT4 yet.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 受传统定点量化的启发，我们还探索了整数表示，其中 $\varphi\!\left(\cdot\right)$ 数字可以 *双重打包* 成一个 INT8
    数字，从而进一步节省内存。这是有帮助的，因为大多数常见的深度学习框架，如 PyTorch，在撰写本文时，还不原生支持低位格式，如 INT4。
- en: Using $\varphi={\text{{\small{INT-}}$b$}}$ and potentially the need for better
    initialization of auxiliary matrices.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 $\varphi={\text{{\small{INT-}}$b$}}$ 并可能需要更好的辅助矩阵初始化。
- en: 'We address this problem in two distinct ways: We adapt and experiment with
    a variant of SVD-based initialization for low-rank matrices $\bm{A}$.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过两种不同的方式解决这个问题：我们调整并实验了一种基于 SVD 的低秩矩阵 $\bm{A}$ 初始化变体。
- en: 'Another way is to use INT8 storage type, allocate $b$ using fixed-point numbers.
    Assuming the rest of the computation is performed in BF16, we define the downcasting
    and the corresponding upcasting operators as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用 INT8 存储类型，使用定点数分配 $b$。假设其余计算在 BF16 中执行，我们定义了向下转型和相应的向上转型操作符如下：
- en: '|  | $\displaystyle\varphi\!\left(x\right)$ |  | (7) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\varphi\!\left(x\right)$ |  | (7) |'
- en: '|  | $\displaystyle\varphi^{-1}\!\left(x\right)$ |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\varphi^{-1}\!\left(x\right)$ |  |'
- en: A fixed-point number where $n$, which corresponds to Q3.5 and Q4.4, respectively.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一个定点数，其中 $n$ 分别对应 Q3.5 和 Q4.4。
- en: Gradient checkpointing
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度检查点
- en: Note that both in the original LoRA paper [[25](#bib.bib25)] and in the related
    work like QLoRA [[16](#bib.bib16)], there is no need to compute the product $\bm{A}\bm{B}$
    in ([5](#S3.E5 "In Downcasting operator ‣ 3 Method ‣ Low-Rank Quantization-Aware
    Training for LLMs")), and in the naïve implementation of our method, this product
    together with the results of some intermediate computations (e.g., after rounding
    and clipping) will be automatically kept in memory for the backward pass, leading
    to increased memory usage. To prevent this, we employ gradient checkpointing [[10](#bib.bib10)]
    on ([5](#S3.E5 "In Downcasting operator ‣ 3 Method ‣ Low-Rank Quantization-Aware
    Training for LLMs")). In other words, we recompute the quantizer function in the
    backward pass, leading to a slight runtime overhead but avoiding significantly
    increased memory usage.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在原始的 LoRA 论文 [[25](#bib.bib25)] 和相关工作如 QLoRA [[16](#bib.bib16)] 中，没有必要计算
    $\bm{A}\bm{B}$ 在 ([5](#S3.E5 "在向下转型操作符 ‣ 3 方法 ‣ 低秩量化感知训练用于 LLM"))，而在我们方法的朴素实现中，这个乘积以及一些中间计算的结果（例如，经过舍入和裁剪后的结果）将自动保存在内存中以便反向传播，从而导致内存使用增加。为了防止这种情况，我们在
    ([5](#S3.E5 "在向下转型操作符 ‣ 3 方法 ‣ 低秩量化感知训练用于 LLM")) 中使用了梯度检查点 [[10](#bib.bib10)]。换句话说，我们在反向传播中重新计算量化函数，导致轻微的运行时开销，但避免了显著增加的内存使用。
- en: LR-QAT
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LR-QAT
- en: Using the components described above, we define LR-QAT for a single layer with
    a (pretrained) weight matrix $\bm{W_{0}}$ as follows
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述描述的组件，我们为具有（预训练）权重矩阵 $\bm{W_{0}}$ 的单层定义 LR-QAT 如下：
- en: '|  | $\displaystyle\widehat{\bm{W}}:=\mathbf{s}\cdot\operatorname{clip}\left(\left\lfloor{\bm{\Phi_{0}}+\frac{\alpha}{r}\bm{A}\bm{B}}\right\rceil,-2^{b-1},2^{b-1}-1\right),$
    |  | (8) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{\bm{W}}:=\mathbf{s}\cdot\operatorname{clip}\left(\left\lfloor{\bm{\Phi_{0}}+\frac{\alpha}{r}\bm{A}\bm{B}}\right\rceil,-2^{b-1},2^{b-1}-1\right),$
    |  | (8) |'
- en: where $\mathbf{s}$ outside the rounding operation as shown in ([1](#S2.E1 "In
    Uniform affine quantization ‣ 2 Background and related work ‣ Low-Rank Quantization-Aware
    Training for LLMs")).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{s}$ 在舍入操作外，如 ([1](#S2.E1 "在均匀仿射量化 ‣ 2 背景和相关工作 ‣ 低秩量化感知训练用于 LLM"))。
- en: 4 Experiments
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: '{floatrow}![[Uncaptioned image]](img/cf4b19b102319f66e5b3e26afeda37be.png)\capbtabbox
    | $\varphi\!\left(\cdot\right)$ init. | W4pc | W3pc |'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '{floatrow}![[未标注的图片]](img/cf4b19b102319f66e5b3e26afeda37be.png)\capbtabbox
    | $\varphi\!\left(\cdot\right)$ 初始化 | W4pc | W3pc |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| WT-2 $\downarrow$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| WT-2 $\downarrow$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| FP32 | FP32 | LoRA | 5.69 | 69.28 | 6.21 | 66.62 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| FP32 | FP32 | LoRA | 5.69 | 69.28 | 6.21 | 66.62 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| FP16 | FP32 | LoRA | $+$0.01 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | FP32 | LoRA | $+$0.01 |'
- en: '| BF16 | FP32 | LoRA | $-$0.45 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| BF16 | FP32 | LoRA | $-$0.45 |'
- en: '| Q4.4 / Q3.5 | FP32 | LoRA | $-$0.31 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Q4.4 / Q3.5 | FP32 | LoRA | $-$0.31 |'
- en: '| Q4.4 / Q3.5 | BF16 | LoRA | $-$0.31 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Q4.4 / Q3.5 | BF16 | LoRA | $-$0.31 |'
- en: '| INT-4 / INT-3 | FP32 | LoRA | $+$30.30 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| INT-4 / INT-3 | FP32 | LoRA | $+$30.30 |'
- en: '| INT-4 / INT-3 | FP32 | LoftQ ($T=1$0.26 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| INT-4 / INT-3 | FP32 | LoftQ ($T=1$0.26 |'
- en: '| INT-4 / INT-3 | FP32 | LoftQ ($T=64$2.01 | \captionlistentry'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '| INT-4 / INT-3 | FP32 | LoftQ ($T=64$2.01 | \captionlistentry'
- en: '[figure]'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[figure]'
- en: 'Table 2: *Left*: The performance of LR-QAT ($\varphi{}={\text{{\small{Q4.4}}}}$,
    compute data type, and initialization method for low-rank auxiliary matrices.
    We report WikiText-2 (‘WT-2’) test set perplexity, lower is better, and average
    zero-shot accuracy of 6 tasks, higher is better. Numbers marked in bold are the
    best results.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：*左*：LR-QAT 的性能 ($\varphi{}={\text{{\small{Q4.4}}}}$，计算数据类型和低秩辅助矩阵的初始化方法。我们报告
    WikiText-2 (‘WT-2’) 测试集困惑度，越低越好，以及 6 个任务的平均零-shot 准确率，越高越好。粗体标记的数字是最佳结果。
- en: We assess the effectiveness of LR-QAT by conducting experiments on LLaMA-2 7B/13B [[61](#bib.bib61)],
    LLaMA-3 8B [[1](#bib.bib1)] and Mistral-0.1 7B [[31](#bib.bib31)]. To compare
    to relevant literature with weight-activation quantization, we further experiment
    with LLaMa-1 7B [[60](#bib.bib60)]. We first explore the impact of the choice
    of rank $r$. We then compare our method in terms of accuracy to standard full-model
    QAT, other baselines, and the related work. All detailed hyperparameters of our
    experiments are in Appendix [B](#A2 "Appendix B Experimental details ‣ Low-Rank
    Quantization-Aware Training for LLMs").
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在 LLaMA-2 7B/13B [[61](#bib.bib61)]、LLaMA-3 8B [[1](#bib.bib1)] 和 Mistral-0.1
    7B [[31](#bib.bib31)] 上进行实验来评估 LR-QAT 的有效性。为了与具有权重激活量化的相关文献进行比较，我们还在 LLaMa-1 7B
    [[60](#bib.bib60)] 上进行进一步实验。我们首先探讨了选择秩 $r$ 的影响。然后，我们将我们的方法与标准全模型 QAT、其他基线和相关工作在准确性方面进行比较。我们实验的所有详细超参数见附录
    [B](#A2 "Appendix B Experimental details ‣ Low-Rank Quantization-Aware Training
    for LLMs")。
- en: Quantization
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化
- en: We experiment with both weight-only and weight-activation quantization. The
    default settings are INT4 / INT3 per-channel (denoted ‘pc’) and group-wise weight
    quantization with a group size of 128 (denoted ‘g128’). We quantize all linear
    layers, except the classification head. In weight-activation quantization, defaults
    are INT4 per-channel weight and per-token activation quantization [[14](#bib.bib14)].
    Following OmniQuant [[55](#bib.bib55)], we quantize all inputs to matmuls with
    exception of the softmax output and additionally quantize the KV-cache as in LLM-QAT [[44](#bib.bib44)].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在权重仅量化和权重激活量化两种情况下进行了实验。默认设置为每通道 INT4 / INT3（标记为‘pc’）和组级权重量化，组大小为 128（标记为‘g128’）。我们量化所有线性层，除了分类头。在权重激活量化中，默认设置为
    INT4 每通道权重和每标记激活量化 [[14](#bib.bib14)]。遵循 OmniQuant [[55](#bib.bib55)]，我们量化所有输入到
    matmuls，除了 softmax 输出，并额外量化 KV 缓存，如 LLM-QAT [[44](#bib.bib44)] 所述。
- en: Evaluation
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估
- en: Following the previous work [[18](#bib.bib18), [65](#bib.bib65), [55](#bib.bib55),
    [44](#bib.bib44)], we evaluate quantized models by reporting the perplexity of
    language generation on WikiText-2 [[48](#bib.bib48)], using a sequence length
    of $2048$. We also report zero-shot accuracy on a set of common sense reasoning
    tasks including BoolQ [[12](#bib.bib12)], PIQA [[5](#bib.bib5)], Winogrande [[54](#bib.bib54)],
    ARC [[13](#bib.bib13)], and HellaSwag [[69](#bib.bib69)]. For zero-shot evaluation,
    we use the LM Evaluation Harness framework [[19](#bib.bib19)].
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的工作 [[18](#bib.bib18), [65](#bib.bib65), [55](#bib.bib55), [44](#bib.bib44)]，我们通过报告
    WikiText-2 [[48](#bib.bib48)] 上的语言生成困惑度来评估量化模型，使用的序列长度为 $2048$。我们还报告了在一组常识推理任务上的零-shot
    准确率，包括 BoolQ [[12](#bib.bib12)]、PIQA [[5](#bib.bib5)]、Winogrande [[54](#bib.bib54)]、ARC
    [[13](#bib.bib13)] 和 HellaSwag [[69](#bib.bib69)]。对于零-shot 评估，我们使用 LM Evaluation
    Harness 框架 [[19](#bib.bib19)]。
- en: Datasets and training
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集和训练
- en: We apply our methods to all linear layers in the attention blocks (both in self-attention
    and in the feed-forward network). We only train low-rank auxiliary matrices $\bm{A}$
    steps. We select hyperparameters based on the perplexity of a small subset of
    Wikipedia validation set (512 sequences).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的方法应用于注意力块中的所有线性层（包括自注意力和前馈网络）。我们仅训练低秩辅助矩阵 $\bm{A}$ 步骤。我们根据 Wikipedia 验证集（512
    个序列）的困惑度选择超参数。
- en: Baselines
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线
- en: We compare with round-to-nearest quantization (RTN), where we set the ranges
    based on minimizing the $L^{p}$-norms between quantized and unquantized weights
    and report the best performing configuration. We also use that as initialization
    for LR-QAT. For weight-only quantization, we compare with GPTQ [[18](#bib.bib18)],
    AWQ [[41](#bib.bib41)], OmniQuant [[55](#bib.bib55)], and our implementation of
    PEQA [[32](#bib.bib32)], where we use symmetric weight quantization and following
    the same experimental setup and best RTN initialization, for a fair comparison.
    Similar to our method, we also apply PEQA only to linear layers in the attention
    blocks and keep token embeddings, final classification head and LayerNorm parameters
    frozen.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与最接近量化 (RTN) 进行比较，其中我们根据最小化量化和未量化权重之间的 $L^{p}$-范数来设置范围，并报告最佳性能配置。我们还将其作为 LR-QAT
    的初始化。对于仅权重量化，我们与 GPTQ [[18](#bib.bib18)]、AWQ [[41](#bib.bib41)]、OmniQuant [[55](#bib.bib55)]
    以及我们实现的 PEQA [[32](#bib.bib32)] 进行比较，其中我们使用对称权重量化，并遵循相同的实验设置和最佳 RTN 初始化，以确保公平比较。类似于我们的方法，我们还仅将
    PEQA 应用到注意力块中的线性层，并保持 token 嵌入、最终分类头和 LayerNorm 参数冻结。
- en: For weight-activation quantization, we compare our method with RTN, SmoothQuant [[65](#bib.bib65)],
    LLM-QAT [[44](#bib.bib44)], Outlier Suppression$+$ [[63](#bib.bib63)], OmniQuant [[55](#bib.bib55)],
    and our implementation of PEQA [[32](#bib.bib32)]. Following [[44](#bib.bib44)],
    we compare to them in several different settings, where the weights, activations
    and KV cache values are quantized to different bitwidths (denoted as W-A-KV).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于权重-激活量化，我们将我们的方法与 RTN、SmoothQuant [[65](#bib.bib65)]、LLM-QAT [[44](#bib.bib44)]、Outlier
    Suppression$+$ [[63](#bib.bib63)]、OmniQuant [[55](#bib.bib55)] 以及我们实现的 PEQA [[32](#bib.bib32)]
    进行比较。按照 [[44](#bib.bib44)] 的方法，我们在多个不同的设置下进行比较，其中权重、激活和 KV 缓存值被量化为不同的位宽（记作 W-A-KV）。
- en: 'Table 3: A comparison of the proposed method ($\varphi{}={\text{{\small{Q4.4}}}}$)
    with the full-model QAT on LLaMA-2 7B with W4 per-channel quantization.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：提出的方法 ($\varphi{}={\text{{\small{Q4.4}}}}$) 与全模型 QAT 在 LLaMA-2 7B 上进行 W4
    每通道量化的比较。
- en: '| Method | GPU memory, GB | WikiText-2 perplexity $\downarrow$ |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | GPU 内存，GB | WikiText-2 困惑度 $\downarrow$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Full-model QAT | 71.9 | 5.74 | 69.29 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 全模型 QAT | 71.9 | 5.74 | 69.29 |'
- en: '| LR-QAT | 20.5 | 5.68 | 69.43 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT | 20.5 | 5.68 | 69.43 |'
- en: 4.1 The impact of rank $r$
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 排名 $r$ 的影响
- en: We investigate the effect of different values of rank $r$ in all our experiments²²2This
    amounts to only 1.2% of the total number of parameters for 7B LLaMA model..
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查了所有实验中不同排名 $r$ 值的影响²²2这只占 7B LLaMA 模型总参数数的 1.2%。。
- en: 4.2 The choice of the downcasting operator $\varphi\!\left(\cdot\right)$ initialization
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 向下转型算子 $\varphi\!\left(\cdot\right)$ 初始化的选择
- en: We study the effect of several choices of the downcasting operators discussed
    in Section [3](#S3 "3 Method ‣ Low-Rank Quantization-Aware Training for LLMs")
    and summarize results in Table [2](#S4.T2 "Table 2 ‣ 4 Experiments ‣ Low-Rank
    Quantization-Aware Training for LLMs"). We can see that by going from FP32 to
    BF16, and finally to an 8-bit fixed-point representation of $\bm{\Phi_{0}}$ still
    leads to a good model performance in the case of 4-bit weight quantization, it
    completely breaks for W3.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了第 [3](#S3 "3 Method ‣ Low-Rank Quantization-Aware Training for LLMs") 节讨论的几种向下转型算子的效果，并在表
    [2](#S4.T2 "Table 2 ‣ 4 Experiments ‣ Low-Rank Quantization-Aware Training for
    LLMs") 中总结了结果。我们可以看到，从 FP32 到 BF16，再到 $\bm{\Phi_{0}}$ 的 8 位固定点表示，在 4 位权重量化的情况下仍能保持良好的模型性能，但在
    W3 时完全失效。
- en: So far, we initialized matrices $\bm{A}$ step recovers almost all the predictive
    performance compared to a fixed-point representation. Increasing number of LoftQ
    steps, or applying it to a 4-bit case, however, did not help.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们初始化的矩阵 $\bm{A}$ 步骤几乎恢复了与固定点表示相比的所有预测性能。然而，增加 LoftQ 步骤的数量或将其应用于 4 位情况并没有帮助。
- en: Finally, when using the fixed point representation for $\bm{\Phi_{0}}$ with
    ‘LoRA’ initialization and BF16 compute data type.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用固定点表示 $\bm{\Phi_{0}}$，并使用 ‘LoRA’ 初始化和 BF16 计算数据类型。
- en: 'Table 4: Weight-only quantization results for LLaMA-2/3 and Mistral models.
    We report WikiText-2 test set perplexity (lower is better) and average zero-shot
    accuracy (higher is better). Models marked ‘L2’/‘L3’, and ‘M’ denote LLaMA-2/3
    and Mistral, respectively. Numbers marked in bold are SOTA or on par (within $0.05$).
    ^§Uses asymmetric weight quantization.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：LLaMA-2/3 和 Mistral 模型的仅权重量化结果。我们报告 WikiText-2 测试集困惑度（越低越好）和平均零-shot 准确率（越高越好）。标记为
    ‘L2’/‘L3’ 和 ‘M’ 的模型分别表示 LLaMA-2/3 和 Mistral。标记为粗体的数字为 SOTA 或持平（在 $0.05$ 以内）。^§使用不对称权重量化。
- en: '| # Bits | Method | WikiText-2 Perplexity $\downarrow$ |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| # 位数 | 方法 | WikiText-2 困惑度 $\downarrow$ |'
- en: '| L2-7B | L2-13B | L3-8B | M-7B | L2-7B | L2-13B | L3-8B | M-7B |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| L2-7B | L2-13B | L3-8B | M-7B | L2-7B | L2-13B | L3-8B | M-7B |'
- en: '| FP16 |  | 5.47 | 4.88 | 6.14 | 5.25 | 70.47 | 73.18 | 74.22 | 75.69 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| FP16 |  | 5.47 | 4.88 | 6.14 | 5.25 | 70.47 | 73.18 | 74.22 | 75.69 |'
- en: '| W4pc | RTN | 6.14 | 5.21 | 7.53 | 5.91 | 68.88 | 71.73 | 72.19 | 73.44 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| W4pc | RTN | 6.14 | 5.21 | 7.53 | 5.91 | 68.88 | 71.73 | 72.19 | 73.44 |'
- en: '| GPTQ^§ | 5.83 | 5.13 | - | - | - | - | - | - |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ^§ | 5.83 | 5.13 | - | - | - | - | - | - |'
- en: '| AWQ | 6.15 | 5.12 | - | - | - | - | - | - |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 6.15 | 5.12 | - | - | - | - | - | - |'
- en: '| OmniQuant^§ | 5.74 | 5.02 | - | - | - | - | - | - |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant^§ | 5.74 | 5.02 | - | - | - | - | - | - |'
- en: '| PEQA (our impl.) | 5.71 | 5.03 | 7.51 | 5.56 | 69.23 | 72.51 | 72.79 | 73.73
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 5.71 | 5.03 | 7.51 | 5.56 | 69.23 | 72.51 | 72.79 | 73.73 |'
- en: '| LR-QAT (ours) | 5.66 | 5.04 | 6.78 | 5.46 | 69.95 | 73.13 | 73.35 | 73.67
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 5.66 | 5.04 | 6.78 | 5.46 | 69.95 | 73.13 | 73.35 | 73.67 |'
- en: '| W3pc | RTN | 26.73 | 8.71 | 34.10 | 9.49 | 43.87 | 55.01 | 47.46 | 64.58
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| W3pc | RTN | 26.73 | 8.71 | 34.10 | 9.49 | 43.87 | 55.01 | 47.46 | 64.58
    |'
- en: '| GPTQ^§ | 8.37 | 6.44 | - | - | - | - | - | - |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ^§ | 8.37 | 6.44 | - | - | - | - | - | - |'
- en: '| AWQ | 24.00 | 10.45 | - | - | - | - | - | - |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 24.00 | 10.45 | - | - | - | - | - | - |'
- en: '| OmniQuant^§ | 6.58 | 5.58 | - | - | - | - | - | - |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant^§ | 6.58 | 5.58 | - | - | - | - | - | - |'
- en: '| PEQA (our impl.) | 6.45 | 5.73 | 27.45 | 6.51 | 65.44 | 69.81 | 50.28 | 71.02
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 6.45 | 5.73 | 27.45 | 6.51 | 65.44 | 69.81 | 50.28 | 71.02
    |'
- en: '| LR-QAT (ours) | 6.13 | 5.54 | 8.12 | 6.03 | 67.66 | 71.22 | 70.46 | 71.87
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 6.13 | 5.54 | 8.12 | 6.03 | 67.66 | 71.22 | 70.46 | 71.87 |'
- en: '| W4g128 | RTN | 5.78 | 5.04 | 6.96 | 5.49 | 69.75 | 72.94 | 72.30 | 75.07
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| W4g128 | RTN | 5.78 | 5.04 | 6.96 | 5.49 | 69.75 | 72.94 | 72.30 | 75.07
    |'
- en: '| GPTQ^§ | 5.61 | 4.98 | - | - | - | - | - | - |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ^§ | 5.61 | 4.98 | - | - | - | - | - | - |'
- en: '| AWQ | 5.62 | 4.97 | - | - | - | - | - | - |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 5.62 | 4.97 | - | - | - | - | - | - |'
- en: '| OmniQuant^§ | 5.58 | 4.95 | - | - | - | - | - | - |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant^§ | 5.58 | 4.95 | - | - | - | - | - | - |'
- en: '| PEQA (our impl.) | 5.67 | 5.02 | 6.89 | 5.48 | 69.64 | 72.80 | 72.99 | 73.34
    |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 5.67 | 5.02 | 6.89 | 5.48 | 69.64 | 72.80 | 72.99 | 73.34 |'
- en: '| LR-QAT (ours) | 5.59 | 4.97 | 6.62 | 5.36 | 69.88 | 72.89 | 73.72 | 75.18
    |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 5.59 | 4.97 | 6.62 | 5.36 | 69.88 | 72.89 | 73.72 | 75.18 |'
- en: '| W3g128 | RTN | 7.61 | 6.20 | 15.11 | 6.77 | 63.20 | 67.60 | 57.74 | 69.35
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| W3g128 | RTN | 7.61 | 6.20 | 15.11 | 6.77 | 63.20 | 67.60 | 57.74 | 69.35
    |'
- en: '| GPTQ^§ | 6.29 | 5.42 | - | - | - | - | - | - |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ^§ | 6.29 | 5.42 | - | - | - | - | - | - |'
- en: '| AWQ | 6.24 | 5.32 | - | - | - | - | - | - |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 6.24 | 5.32 | - | - | - | - | - | - |'
- en: '| OmniQuant^§ | 6.03 | 5.28 | - | - | - | - | - | - |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant^§ | 6.03 | 5.28 | - | - | - | - | - | - |'
- en: '| PEQA (our impl.) | 6.05 | 5.58 | 9.64 | 5.85 | 68.10 | 70.29 | 67.19 | 72.21
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 6.05 | 5.58 | 9.64 | 5.85 | 68.10 | 70.29 | 67.19 | 72.21 |'
- en: '| LR-QAT (ours) | 5.99 | 5.53 | 7.74 | 5.81 | 67.98 | 70.54 | 70.48 | 71.44
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 5.99 | 5.53 | 7.74 | 5.81 | 67.98 | 70.54 | 70.48 | 71.44 |'
- en: 4.3 Comparison with full-model QAT
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 与全模型 QAT 的比较
- en: Finally, before presenting our main set of results, we compare our method with
    a vanilla full-model QAT [[17](#bib.bib17)]. For full-model QAT, we follow the
    same training setup as for our method. We also tune the maximum value of the learning
    rate using the following search space $\left\{\text{1e-5, {5e-5}, 1e-4, 5e-4,
    1e-3}\right\}$ and select the best configuration based on Wikipedia validation
    perplexity. As we can see in Table [3](#S4.T3 "Table 3 ‣ Baselines ‣ 4 Experiments
    ‣ Low-Rank Quantization-Aware Training for LLMs"), training with our method leads
    to a better predictive performance at a significantly lower memory usage compared
    to vanilla QAT.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在展示我们主要结果集之前，我们将我们的方法与原始全模型 QAT [[17](#bib.bib17)] 进行了比较。对于全模型 QAT，我们遵循与我们的方法相同的训练设置。我们还使用以下搜索空间
    $\left\{\text{1e-5, {5e-5}, 1e-4, 5e-4, 1e-3}\right\}$ 调整学习率的最大值，并根据 Wikipedia
    验证困惑度选择最佳配置。正如我们在表[3](#S4.T3 "Table 3 ‣ Baselines ‣ 4 Experiments ‣ Low-Rank Quantization-Aware
    Training for LLMs")中看到的，使用我们的方法进行训练可以在显著降低内存使用的情况下实现更好的预测性能。
- en: 4.4 Main results
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 主要结果
- en: Weight-only quantization
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 仅权重量化
- en: We summarize our results in Table [4](#S4.T4 "Table 4 ‣ 4.2 The choice of the
    downcasting operator 𝜑⁢(⋅) and 𝑨, 𝑩 initialization ‣ 4 Experiments ‣ Low-Rank
    Quantization-Aware Training for LLMs"). As we can see, in almost most cases LR-QAT
    outperforms or is on par with prior weight-only quantization methods across various
    LLM families and quantization settings, including both per-channel and group-wise
    quantization.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[4](#S4.T4 "Table 4 ‣ 4.2 The choice of the downcasting operator 𝜑⁢(⋅) and
    𝑨, 𝑩 initialization ‣ 4 Experiments ‣ Low-Rank Quantization-Aware Training for
    LLMs")中总结了我们的结果。正如我们所见，在几乎所有情况下，LR-QAT 在各种 LLM 家族和量化设置中优于或持平于先前的仅权重量化方法，包括每通道和组级量化。
- en: In a few cases, especially in case of group-wise quantization our method did
    not outpeform OmniQuant. However, OmniQuant uses asymmetric quantization which
    provides extra degrees of freedom compared to symmetric quantization, which are
    very helpful in the case of low-bit quantization. In practice, however, symmetric
    weight quantization yields more efficient inference [[51](#bib.bib51)]. Additionally,
    techniques like OmniQuant and related techniques are orthogonal to our method
    and can be used as as initialization of LR-QAT.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，尤其是在分组量化的情况下，我们的方法未能超越 OmniQuant。然而，OmniQuant 使用非对称量化，这比对称量化提供了更多的自由度，这在低比特量化时非常有用。然而，在实践中，对称权重量化通常能实现更高效的推理[[51](#bib.bib51)]。此外，像
    OmniQuant 及相关技术与我们的方法是正交的，可以作为 LR-QAT 的初始化。
- en: Weight-activation quantization
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 权重-激活量化
- en: 'Table 5: Weight and activation quantization results for LLaMA-1 7B. We report
    WikiText-2 test set perplexity (lower is better) and zero-shot accuracy of 6 tasks
    (higher is better). Numbers marked in bold are SOTA. ^§Uses asymmetric weight
    quantization. ^*Uses a maximum sequence length of 1024 for evaluation.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：LLaMA-1 7B 的权重和激活量化结果。我们报告了 WikiText-2 测试集的困惑度（数值越低越好）以及 6 个任务的零-shot 准确率（数值越高越好）。用粗体标记的数字为最新的
    SOTA 结果。^§使用非对称权重量化。^*在评估时使用了最大序列长度 1024。
- en: '| # Bits (W-A-KV) | Method | WikiText-2 $\downarrow$ |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| # 比特（W-A-KV） | 方法 | WikiText-2 $\downarrow$ |'
- en: '| BoolQ | PIQA | Winogrande | ARC-e | ARC-c | HellaSwag | Avg. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | PIQA | Winogrande | ARC-e | ARC-c | HellaSwag | 平均 |'
- en: '| FP16 |  | 5.68 | 75.05 | 79.16 | 70.01 | 72.85 | 44.80 | 76.21 | 69.68 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| FP16 |  | 5.68 | 75.05 | 79.16 | 70.01 | 72.85 | 44.80 | 76.21 | 69.68 |'
- en: '| 4-8-8 | RTN | 23.18 | 60.55 | 67.14 | 53.43 | 54.08 | 33.11 | 50.77 | 53.18
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 4-8-8 | RTN | 23.18 | 60.55 | 67.14 | 53.43 | 54.08 | 33.11 | 50.77 | 53.18
    |'
- en: '| SmoothQuant | 13.7^* | 71.00 | 76.00 | 66.00 | 67.40 | 42.80 | 67.80 | 65.17
    |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 13.7^* | 71.00 | 76.00 | 66.00 | 67.40 | 42.80 | 67.80 | 65.17
    |'
- en: '| LLM-QAT | 11.2^* | 74.60 | 77.50 | 67.70 | 70.20 | 45.60 | 73.50 | 68.18
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QAT | 11.2^* | 74.60 | 77.50 | 67.70 | 70.20 | 45.60 | 73.50 | 68.18
    |'
- en: '| PEQA (our impl.) | 5.89 | 74.86 | 78.24 | 70.01 | 70.12 | 42.83 | 75.14 |
    68.53 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 5.89 | 74.86 | 78.24 | 70.01 | 70.12 | 42.83 | 75.14 | 68.53
    |'
- en: '|  | LR-QAT (ours) | 5.85 | 73.76 | 78.51 | 71.19 | 71.09 | 41.81 | 75.10 |
    68.58 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | LR-QAT（我们的实现） | 5.85 | 73.76 | 78.51 | 71.19 | 71.09 | 41.81 | 75.10 |
    68.58 |'
- en: '| 4-8-4 | RTN | 182.7 | 48.01 | 52.23 | 48.78 | 29.00 | 22.70 | 28.52 | 38.21
    |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 4-8-4 | RTN | 182.7 | 48.01 | 52.23 | 48.78 | 29.00 | 22.70 | 28.52 | 38.21
    |'
- en: '| SmoothQuant | 163.6^* | 54.70 | 55.40 | 51.50 | 43.90 | 27.70 | 38.90 | 45.35
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 163.6^* | 54.70 | 55.40 | 51.50 | 43.90 | 27.70 | 38.90 | 45.35
    |'
- en: '| LLM-QAT | 11.6^* | 69.50 | 75.40 | 64.60 | 66.00 | 43.80 | 69.20 | 64.75
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QAT | 11.6^* | 69.50 | 75.40 | 64.60 | 66.00 | 43.80 | 69.20 | 64.75
    |'
- en: '| PEQA (our impl.) | 6.15 | 72.97 | 77.80 | 67.72 | 67.13 | 40.27 | 73.35 |
    66.54 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 6.15 | 72.97 | 77.80 | 67.72 | 67.13 | 40.27 | 73.35 | 66.54
    |'
- en: '| LR-QAT (ours) | 6.07 | 73.64 | 77.91 | 67.56 | 69.28 | 41.30 | 73.25 | 67.16
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的实现） | 6.07 | 73.64 | 77.91 | 67.56 | 69.28 | 41.30 | 73.25 | 67.16
    |'
- en: '| 4-4-4 | RTN | 4.1e4 | 41.41 | 49.24 | 51.30 | 27.19 | 28.84 | 26.30 | 37.38
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 4-4-4 | RTN | 4.1e4 | 41.41 | 49.24 | 51.30 | 27.19 | 28.84 | 26.30 | 37.38
    |'
- en: '| SmoothQuant | 25.25 | 49.10 | 49.80 | 48.00 | 30.40 | 25.80 | 27.40 | 38.42
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 25.25 | 49.10 | 49.80 | 48.00 | 30.40 | 25.80 | 27.40 | 38.42
    |'
- en: '| LLM-QAT | - | 61.30 | 51.50 | 51.90 | 27.90 | 23.90 | 31.10 | 41.27 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QAT | - | 61.30 | 51.50 | 51.90 | 27.90 | 23.90 | 31.10 | 41.27 |'
- en: '| LLM-QAT + SQ | - | 62.40 | 55.90 | 50.60 | 35.50 | 26.40 | 47.80 | 46.43
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QAT + SQ | - | 62.40 | 55.90 | 50.60 | 35.50 | 26.40 | 47.80 | 46.43
    |'
- en: '| OS+ | - | 60.21 | 62.73 | 52.96 | 39.98 | 30.29 | 44.39 | 48.43 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| OS+ | - | 60.21 | 62.73 | 52.96 | 39.98 | 30.29 | 44.39 | 48.43 |'
- en: '| OmniQuant^§ | 11.26 | 63.51 | 66.15 | 53.43 | 45.20 | 31.14 | 56.44 | 52.65
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant^§ | 11.26 | 63.51 | 66.15 | 53.43 | 45.20 | 31.14 | 56.44 | 52.65
    |'
- en: '| PEQA (our impl.) | 8.60 | 65.69 | 72.31 | 59.83 | 56.52 | 34.22 | 61.79 |
    58.39 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 8.60 | 65.69 | 72.31 | 59.83 | 56.52 | 34.22 | 61.79 | 58.39
    |'
- en: '|  | LR-QAT (ours) | 8.47 | 67.16 | 71.76 | 59.59 | 58.42 | 34.73 | 62.34 |
    59.00 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | LR-QAT（我们的实现） | 8.47 | 67.16 | 71.76 | 59.59 | 58.42 | 34.73 | 62.34 |
    59.00 |'
- en: We present our results for weight-activation quantization applied to LLaMA-1
    7B in Table [5](#S4.T5 "Table 5 ‣ Weight-activation quantization ‣ 4.4 Main results
    ‣ 4 Experiments ‣ Low-Rank Quantization-Aware Training for LLMs"). LR-QAT demonstrates
    superior performance compared to all the PTQ and QAT baselines, consistently outperforming
    them across all the bitwidth settings. In addition to that, as we decrease the
    activation bitwidths, the improvement in model performance compared to prior work
    becomes more pronounced.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表 [5](#S4.T5 "Table 5 ‣ Weight-activation quantization ‣ 4.4 Main results
    ‣ 4 Experiments ‣ Low-Rank Quantization-Aware Training for LLMs") 中展示了应用于 LLaMA-1
    7B 的权重-激活量化的结果。LR-QAT 展现出优于所有 PTQ 和 QAT 基线的性能，在所有比特宽度设置下均表现优异。此外，随着我们减少激活比特宽度，相比于先前的工作，模型性能的提升变得更加明显。
- en: This indicates LR-QAT’s versatility, being readily applicable not only to weight-only
    quantization but also weight-activation quantization, a setting that allows for
    a very efficient inference using fixed-point arithmetic. Finally, our method can
    still be combined with most of the related PTQ methods including OmniQuant that
    shift the difficulty of activation quantization to weight quantization, and will
    likely lead to even better results.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明 LR-QAT 的多功能性，它不仅适用于仅权重量化，还适用于权重-激活量化，这种设置允许使用定点运算进行非常高效的推理。最后，我们的方法仍然可以与大多数相关的
    PTQ 方法结合，包括 OmniQuant，它将激活量化的难度转移到权重量化上，并可能带来更好的结果。
- en: 5 Discussion
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: Limitations
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 局限性
- en: A core assumption of LR-QAT is that a low-rank approximation can compensate
    the introduced quantization noise. While quantization noise follows a random uniform
    distribution and is theoretically not low-rank, our results and several prior
    works [[16](#bib.bib16), [66](#bib.bib66), [32](#bib.bib32)] suggest that in an
    end-to-end training setup, low-rank approaches can effectively compensate quantization
    noise. In our work, we demonstrated the effectiveness of LR-QAT for LLMs up to
    13B parameters. It is unclear how it scales to significantly larger LLMs, though
    we do not see any reason why our findings should not hold beyond 13B models. We
    evaluate LR-QAT as extended pretraining technique with several thousands of iterations.
    It is unclear how it would perform in case it is used during pretraining for millions
    of iterations.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: LR-QAT 的一个核心假设是低秩近似可以弥补引入的量化噪声。尽管量化噪声遵循随机均匀分布，并且在理论上不是低秩的，但我们的结果和一些先前的工作 [[16](#bib.bib16),
    [66](#bib.bib66), [32](#bib.bib32)] 表明，在端到端训练设置中，低秩方法可以有效地弥补量化噪声。在我们的工作中，我们展示了
    LR-QAT 在参数高达 13B 的 LLM 上的有效性。目前尚不清楚它如何扩展到更大规模的 LLM，但我们没有看到为什么我们的发现不能适用于超过 13B
    的模型。我们将 LR-QAT 作为扩展的预训练技术进行了几千次迭代的评估。尚不清楚在预训练过程中使用几百万次迭代时其表现如何。
- en: Impact
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 影响
- en: As LR-QAT improves memory and inference efficiency of LLMs, we expect mainly
    positive outcomes from our work. Efficiently deploying LLMs will help with reducing
    their high power consumption at inference time. It further helps to move inference
    from the cloud to edge devices which can overcome potential privacy concerns.
    In some cases, quantization might lead to biased predictions, see [[24](#bib.bib24)]
    for further discussion.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 LR-QAT 改善了 LLM 的内存和推理效率，我们期望从我们的工作中获得主要的积极结果。高效部署 LLM 将有助于减少其在推理时的高能耗。它还帮助将推理从云端迁移到边缘设备，从而克服潜在的隐私问题。在某些情况下，量化可能导致偏倚预测，详见 [[24](#bib.bib24)]
    进一步讨论。
- en: 6 Conclusions
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper we propose LR-QAT, a lightweight and memory-efficient QAT algorithm
    for LLMs which enables training a 7B LLM on a single consumer grade GPU with 24GB
    of memory. Inspired by PEFT methods, we introduce a low-rank reparameterization
    that is aware of the quantization grid. We further reduce the memory requirements
    by introducing a downcasting operator involving fixed-point or double-packed integers,
    and applying checkpointing. In almost all cases, our method outperforms common
    PTQ approaches and reaches the same model performance as full-model QAT at the
    fraction of its memory usage.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了 LR-QAT，这是一种轻量且内存高效的 QAT 算法，用于 LLM，使得在单个 24GB 内存的消费级 GPU 上训练 7B LLM
    成为可能。受到 PEFT 方法的启发，我们引入了一种低秩重新参数化，能够适应量化网格。我们进一步通过引入涉及定点或双打包整数的降位运算符，并应用检查点，减少了内存需求。在几乎所有情况下，我们的方法都优于常见的
    PTQ 方法，并在内存使用量的一小部分下达到了与全模型 QAT 相同的模型性能。
- en: Acknowledgements
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank Tijmen Blankevoort, Paul Whatmough, Jorn Peters, and
    Ties van Rozendaal for valuable discussions and support.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢 Tijmen Blankevoort, Paul Whatmough, Jorn Peters, 和 Ties van Rozendaal
    提供的宝贵讨论和支持。
- en: References
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: AI@Meta [2024] AI@Meta. Llama 3 model card, 2024. URL [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI@Meta [2024] AI@Meta. Llama 3 模型卡，2024。网址 [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)。
- en: Banner et al. [2018] Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry.
    Post-training 4-bit quantization of convolution networks for rapid-deployment.
    *arXiv preprint arXiv:1810.05723*, 2018.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banner et al. [2018] Ron Banner, Yury Nahshan, Elad Hoffer, 和 Daniel Soudry。卷积网络的后训练
    4 位量化以实现快速部署。*arXiv 预印本 arXiv:1810.05723*，2018 年。
- en: Bengio et al. [2013] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating
    or propagating gradients through stochastic neurons for conditional computation.
    *arXiv preprint arXiv:1308.3432*, 2013.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio et al. [2013] Yoshua Bengio, Nicholas Léonard, 和 Aaron Courville。通过随机神经元估计或传播梯度以进行条件计算。*arXiv
    预印本 arXiv:1308.3432*，2013 年。
- en: 'Bhalgat et al. [2020] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort,
    and Nojun Kwak. Lsq+: Improving low-bit quantization through learnable offsets
    and better initialization. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR) Workshops*, 2020.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bhalgat et al. [2020] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort,
    和 Nojun Kwak。Lsq+: 通过可学习的偏移量和更好的初始化改进低位量化。载于 *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR) Workshops*，2020 年。'
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439,
    2020.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi 等人。Piqa:
    在自然语言中推理物理常识。载于 *Proceedings of the AAAI Conference on Artificial Intelligence*，第
    34 卷，页码 7432–7439，2020 年。'
- en: 'Bondarenko et al. [2021] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
    Understanding and overcoming the challenges of efficient transformer quantization.
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 7947–7969, Online and Punta Cana, Dominican Republic, November
    2021\. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.627.
    URL [https://aclanthology.org/2021.emnlp-main.627](https://aclanthology.org/2021.emnlp-main.627).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bondarenko et al. [2021] Yelysei Bondarenko, Markus Nagel, 和 Tijmen Blankevoort。理解和克服高效变换器量化的挑战。载于
    *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*，页码
    7947–7969，在线和多米尼加共和国蓬塔卡纳，2021 年 11 月。计算语言学协会。doi: 10.18653/v1/2021.emnlp-main.627。网址
    [https://aclanthology.org/2021.emnlp-main.627](https://aclanthology.org/2021.emnlp-main.627)。'
- en: 'Bondarenko et al. [2024] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
    Quantizable transformers: Removing outliers by helping attention heads do nothing.
    *Advances in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bondarenko et al. [2024] Yelysei Bondarenko, Markus Nagel, 和 Tijmen Blankevoort。可量化变换器：通过帮助注意力头不做任何事情来去除异常值。*Advances
    in Neural Information Processing Systems*，第 36 卷，2024 年。
- en: 'Cai et al. [2020] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W
    Mahoney, and Kurt Keutzer. Zeroq: A novel zero shot quantization framework. In
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 13169–13178, 2020.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai et al. [2020] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael
    W Mahoney, 和 Kurt Keutzer。Zeroq: 一种新颖的零样本量化框架。载于 *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*，页码 13169–13178，2020 年。'
- en: 'Chee et al. [2024] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M
    De Sa. Quip: 2-bit quantization of large language models with guarantees. *Advances
    in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chee et al. [2024] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, 和 Christopher
    M De Sa。Quip: 具有保证的大型语言模型的 2 位量化。*Advances in Neural Information Processing Systems*，第
    36 卷，2024 年。'
- en: Chen et al. [2016] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
    Training deep nets with sublinear memory cost. *arXiv preprint arXiv:1604.06174*,
    2016.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2016] Tianqi Chen, Bing Xu, Chiyuan Zhang, 和 Carlos Guestrin。用次线性内存成本训练深度网络。*arXiv
    预印本 arXiv:1604.06174*，2016 年。
- en: Choukroun et al. [2019] Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev.
    Low-bit quantization of neural networks for efficient inference. In *ICCV Workshops*,
    pages 3009–3018, 2019.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choukroun et al. [2019] Yoni Choukroun, Eli Kravchik, Fan Yang, 和 Pavel Kisilev。神经网络的低位量化以实现高效推理。载于
    *ICCV Workshops*，页码 3009–3018，2019 年。
- en: 'Clark et al. [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty
    of natural yes/no questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark 等人 [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, 和 Kristina Toutanova. Boolq: 探索自然是/否问题的惊人难度。*arXiv 预印本 arXiv:1905.10044*，2019。'
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人 [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, 和 Oyvind Tafjord. 认为你已经解决了问答问题？尝试 arc，AI2 推理挑战。*arXiv
    预印本 arXiv:1803.05457*，2018。
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Gpt3\. int8 (): 8-bit matrix multiplication for transformers at scale. In *Advances
    in Neural Information Processing Systems*, 2022.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 [2022] Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    Gpt3.int8(): 大规模变换器的 8 位矩阵乘法。在*神经信息处理系统进展*中，2022。'
- en: 'Dettmers et al. [2023] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm
    weight compression. *arXiv preprint arXiv:2306.03078*, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 [2023] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis
    Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    和 Dan Alistarh. Spqr: 一种用于近乎无损 llm 权重压缩的稀疏量化表示。*arXiv 预印本 arXiv:2306.03078*，2023。'
- en: 'Dettmers et al. [2024] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *Advances in Neural
    Information Processing Systems*, 36, 2024.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 [2024] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer.
    Qlora: 高效的量化 llms 微调。*神经信息处理系统进展*，第 36 卷，2024。'
- en: Esser et al. [2020] Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani,
    Rathinakumar Appuswamy, and Dharmendra S. Modha. Learned step size quantization.
    In *International Conference on Learning Representations (ICLR)*, 2020.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esser 等人 [2020] Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar
    Appuswamy, 和 Dharmendra S. Modha. 学习的步长量化。在*国际学习表征会议 (ICLR)*，2020。
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等人 [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh.
    Gptq: 生成预训练变换器的准确后训练量化。*arXiv 预印本 arXiv:2210.17323*，2022。'
- en: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    et al. A framework for few-shot language model evaluation. *Version v0\. 0.1\.
    Sept*, page 8, 2021.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi,
    Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    等人. 一个用于少样本语言模型评估的框架。*版本 v0.0.1. 九月*，第 8 页，2021。
- en: 'Gugger et al. [2022] Sylvain Gugger, Lysandre Debu, Thomas Wolf, Philipp Schmid,
    Zachary Mueller, and Sourab Mangrulkar. Accelerate: Training and inference at
    scale made simple, efficient and adaptable. [https://github.com/huggingface/accelerate](https://github.com/huggingface/accelerate),
    2022.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gugger 等人 [2022] Sylvain Gugger, Lysandre Debu, Thomas Wolf, Philipp Schmid,
    Zachary Mueller, 和 Sourab Mangrulkar. Accelerate: 使大规模训练和推理变得简单、高效和适应性强。 [https://github.com/huggingface/accelerate](https://github.com/huggingface/accelerate)，2022。'
- en: 'Guo et al. [2023] Han Guo, Philip Greengard, Eric P Xing, and Yoon Kim. Lq-lora:
    Low-rank plus quantized matrix decomposition for efficient language model finetuning.
    *arXiv preprint arXiv:2311.12023*, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等人 [2023] Han Guo, Philip Greengard, Eric P Xing, 和 Yoon Kim. Lq-lora:
    低秩加量化矩阵分解用于高效的语言模型微调。*arXiv 预印本 arXiv:2311.12023*，2023。'
- en: Gupta et al. [2015] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and
    Pritish Narayanan. Deep learning with limited numerical precision. In *International
    conference on machine learning*, pages 1737–1746\. PMLR, 2015.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等人 [2015] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, 和 Pritish
    Narayanan. 有限数值精度下的深度学习。在*国际机器学习大会*上，页码 1737–1746。PMLR，2015。
- en: 'He et al. [2015] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving
    deep into rectifiers: Surpassing human-level performance on imagenet classification.
    In *Proceedings of the IEEE international conference on computer vision*, pages
    1026–1034, 2015.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2015] Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 深入研究激活函数：在
    ImageNet 分类中超越人类水平。在*IEEE 国际计算机视觉大会论文集*中，页码 1026–1034，2015。
- en: Hooker et al. [2020] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio,
    and Emily Denton. Characterising bias in compressed models. *arXiv preprint arXiv:2010.03058*,
    2020.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hooker et al. [2020] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio,
    和 Emily Denton。表征压缩模型中的偏差。*arXiv预印本 arXiv:2010.03058*，2020。
- en: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen。LoRA：大型语言模型的低秩适应。*arXiv预印本 arXiv:2106.09685*，2021。
- en: 'Hubara et al. [2017] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran
    El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks
    with low precision weights and activations. *The Journal of Machine Learning Research*,
    18(1):6869–6898, 2017.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara et al. [2017] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv,
    和 Yoshua Bengio。量化神经网络：使用低精度权重和激活值训练神经网络。*机器学习研究期刊*，18(1):6869–6898，2017。
- en: 'Hubara et al. [2020] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and
    Daniel Soudry. Improving post training neural quantization: Layer-wise calibration
    and integer programming. *arXiv preprint arXiv:2006.10518*, 2020.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara et al. [2020] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, 和 Daniel
    Soudry。改善后训练神经量化：逐层校准和整数编程。*arXiv预印本 arXiv:2006.10518*，2020。
- en: Jacob et al. [2018] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    pages 2704–2713, 2018.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob et al. [2018] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, 和 Dmitry Kalenichenko。量化和训练神经网络以实现高效的整数算术推理。发表于*IEEE计算机视觉与模式识别会议录*，页2704–2713，2018。
- en: 'Jeon et al. [2024] Hyesung Jeon, Yulhwa Kim, and Jae-joon Kim. L4q: Parameter
    efficient quantization-aware training on large language models via lora-wise lsq.
    *arXiv preprint arXiv:2402.04902*, 2024.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeon et al. [2024] Hyesung Jeon, Yulhwa Kim, 和 Jae-joon Kim。L4Q：通过LoRA-方式LSQ在大型语言模型上进行参数高效的量化感知训练。*arXiv预印本
    arXiv:2402.04902*，2024。
- en: Jeon et al. [2023] Yongkweon Jeon, Chungman Lee, Kyungphil Park, and Ho-young
    Kim. A frustratingly easy post-training quantization scheme for llms. In *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    14446–14461, 2023.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeon et al. [2023] Yongkweon Jeon, Chungman Lee, Kyungphil Park, 和 Ho-young
    Kim。一个令人沮丧的简单后训练量化方案。发表于*2023年自然语言处理实证方法会议*，页14446–14461，2023。
- en: Jiang et al. [2023] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*, 2023.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. [2023] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, 等等。Mistral 7b。*arXiv预印本 arXiv:2310.06825*，2023。
- en: Kim et al. [2024] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park,
    Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee. Memory-efficient fine-tuning of compressed
    large language models via sub-4-bit integer quantization. *Advances in Neural
    Information Processing Systems*, 36, 2024.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. [2024] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park,
    Kang Min Yoo, Se Jung Kwon, 和 Dongsoo Lee。通过子4位整数量化实现压缩大型语言模型的内存高效微调。*神经信息处理系统进展*，36，2024。
- en: 'Kovaleva et al. [2021] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and
    Anna Rumshisky. Bert busters: Outlier dimensions that disrupt transformers. In
    *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*,
    pages 3392–3405, 2021.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kovaleva et al. [2021] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, 和 Anna
    Rumshisky。BERT击碎者：扰乱变换器的异常维度。发表于*计算语言学协会发现：ACL-IJCNLP 2021*，页3392–3405，2021。
- en: 'Krishnamoorthi [2018] Raghuraman Krishnamoorthi. Quantizing deep convolutional
    networks for efficient inference: A whitepaper. *arXiv preprint arXiv:1806.08342*,
    2018.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishnamoorthi [2018] Raghuraman Krishnamoorthi。量化深度卷积网络以实现高效推理：一份白皮书。*arXiv预印本
    arXiv:1806.08342*，2018。
- en: 'Lee et al. [2024] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok
    Park. Owq: Outlier-aware weight quantization for efficient fine-tuning and inference
    of large language models. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, volume 38, pages 13355–13364, 2024.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. [2024] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, 和 Eunhyeok
    Park。OWQ：针对大型语言模型高效微调和推理的异常值感知权重量化。在*美国人工智能协会会议录*，第38卷，页13355–13364，2024。
- en: Lee et al. [2023a] Jangwhan Lee, Minsoo Kim, Seungcheol Baek, Seok Joong Hwang,
    Wonyong Sung, and Jungwook Choi. Enhancing computation efficiency in large language
    models through weight and activation quantization. *arXiv preprint arXiv:2311.05161*,
    2023a.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 [2023a] Jangwhan Lee, Minsoo Kim, Seungcheol Baek, Seok Joong Hwang, Wonyong
    Sung 和 Jungwook Choi. 通过权重和激活量化提高大规模语言模型的计算效率。*arXiv 预印本 arXiv:2311.05161*，2023a。
- en: 'Lee et al. [2023b] Jung Hyun Lee, Jeonghoon Kim, Se Jung Kwon, and Dongsoo
    Lee. Flexround: Learnable rounding based on element-wise division for post-training
    quantization. In *International Conference on Machine Learning*, pages 18913–18939\.
    PMLR, 2023b.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 [2023b] Jung Hyun Lee, Jeonghoon Kim, Se Jung Kwon 和 Dongsoo Lee. Flexround：基于逐元素除法的可学习舍入，用于后训练量化。发表于
    *国际机器学习会议*，页码 18913–18939。PMLR，2023b。
- en: 'Lhoest et al. [2021] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite,
    Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame,
    Julien Plu, Lewis Tunstall, Joe Davison, Mario $\mathbf{S}$ko, Gunjan Chhablani,
    Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas
    Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue,
    Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer,
    Victor Mustar, François Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A
    community library for natural language processing. In *Proceedings of the 2021
    Conference on Empirical Methods in Natural Language Processing: System Demonstrations*,
    pages 175–184, Online and Punta Cana, Dominican Republic, November 2021\. Association
    for Computational Linguistics. URL [https://aclanthology.org/2021.emnlp-demo.21](https://aclanthology.org/2021.emnlp-demo.21).'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lhoest 等 [2021] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite,
    Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame,
    Julien Plu, Lewis Tunstall, Joe Davison, Mario $\mathbf{S}$ko, Gunjan Chhablani,
    Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas
    Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue,
    Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer,
    Victor Mustar, François Lagunas, Alexander Rush 和 Thomas Wolf. 数据集：自然语言处理的社区库。发表于
    *2021年自然语言处理实证方法会议论文集：系统演示*，页码 175–184，在线和多米尼加共和国蓬塔卡纳，2021年11月。计算语言学协会。URL [https://aclanthology.org/2021.emnlp-demo.21](https://aclanthology.org/2021.emnlp-demo.21)。
- en: 'Li et al. [2023] Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis,
    Weizhu Chen, and Tuo Zhao. Loftq: Lora-fine-tuning-aware quantization for large
    language models. *arXiv preprint arXiv:2310.08659*, 2023.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2023] Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis,
    Weizhu Chen 和 Tuo Zhao. Loftq：Lora 微调感知量化用于大规模语言模型。*arXiv 预印本 arXiv:2310.08659*，2023。
- en: 'Li et al. [2021] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization
    by block reconstruction. *arXiv preprint arXiv:2102.05426*, 2021.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2021] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei
    Yu, Wei Wang 和 Shi Gu. Brecq：通过块重建推动后训练量化的极限。*arXiv 预印本 arXiv:2102.05426*，2021。
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang 和 Song
    Han. Awq：用于 LLM 压缩和加速的激活感知权重量化。*arXiv 预印本 arXiv:2306.00978*，2023。
- en: 'Lin et al. [2024] Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan
    Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design
    for efficient llm serving. *arXiv preprint arXiv:2405.04532*, 2024.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 [2024] Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao,
    Chuang Gan 和 Song Han. Qserve：W4a8kv4 量化与系统共同设计以提高 LLM 服务效率。*arXiv 预印本 arXiv:2405.04532*，2024。
- en: 'Liu et al. [2023a] Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei
    Cai, and Bohan Zhuang. Qllm: Accurate and efficient low-bitwidth quantization
    for large language models. *arXiv preprint arXiv:2310.08041*, 2023a.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2023a] Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai 和
    Bohan Zhuang. Qllm：大规模语言模型的精确高效低位宽量化。*arXiv 预印本 arXiv:2310.08041*，2023a。
- en: 'Liu et al. [2023b] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models. *arXiv
    preprint arXiv:2305.17888*, 2023b.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2023b] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi 和 Vikas Chandra.
    Llm-qat：针对大规模语言模型的数据无关量化感知训练。*arXiv 预印本 arXiv:2305.17888*，2023b。
- en: Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. *arXiv preprint arXiv:1711.05101*, 2017.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov 和 Hutter [2017] 伊利亚·洛希洛夫和弗兰克·赫特。解耦权重衰减正则化。*arXiv 预印本 arXiv:1711.05101*，2017年。
- en: Luo et al. [2023] Yan Luo, Yangcheng Gao, Zhao Zhang, Jicong Fan, Haijun Zhang,
    and Mingliang Xu. Long-range zero-shot generative deep network quantization. *Neural
    Networks*, 166:683–691, 2023.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人 [2023] 罗彦、杨成、张钊、范吉聪、张海军和徐铭亮。长范围零样本生成深度网络量化。*神经网络*，166：683–691，2023年。
- en: 'Meller et al. [2019] Eldad Meller, Alexander Finkelstein, Uri Almog, and Mark
    Grobman. Same, same but different: Recovering neural network quantization error
    through weight factorization. In *International Conference on Machine Learning*,
    pages 4486–4495\. PMLR, 2019.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meller 等人 [2019] 埃尔达德·梅勒、亚历山大·芬克尔斯坦、乌里·阿尔莫格和马克·格罗布曼。同样的，但不同的：通过权重分解恢复神经网络量化误差。见于
    *国际机器学习会议*，第4486–4495页。PMLR，2019年。
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等人 [2016] 斯蒂芬·梅里蒂、蔡名雄、詹姆斯·布拉德伯里和理查德·索彻。指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843*，2016年。
- en: Nagel et al. [2019] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max
    Welling. Data-free quantization through weight equalization and bias correction.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    pages 1325–1334, 2019.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel 等人 [2019] 马库斯·纳戈尔、马特·范·巴伦、蒂杰门·布兰克沃特和马克斯·威灵。通过权重均衡和偏差校正进行无数据量化。见于 *IEEE/CVF
    国际计算机视觉会议论文集*，第1325–1334页，2019年。
- en: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning (ICML)*, 2020.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel 等人 [2020] 马库斯·纳戈尔、拉纳·阿里·阿姆贾德、马特·范·巴伦、克里斯托斯·卢伊佐斯和蒂杰门·布兰克沃特。上还是下？后训练量化的自适应舍入。见于
    *国际机器学习会议（ICML）*，2020年。
- en: Nagel et al. [2021] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei
    Bondarenko, Mart van Baalen, and Blankevoort Tijmen. A white paper on neural network
    quantization. *arXiv preprint arXiv:2106.08295*, 2021.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel 等人 [2021] 马库斯·纳戈尔、马里奥斯·福尔纳拉基斯、拉纳·阿里·阿姆贾德、耶利塞·邦达连科、马特·范·巴伦和蒂杰门·布兰克沃特。关于神经网络量化的白皮书。*arXiv
    预印本 arXiv:2106.08295*，2021年。
- en: Oberstar [2007] Erick L Oberstar. Fixed-point representation & fractional math.
    *Oberstar Consulting*, 9:19, 2007.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oberstar [2007] 埃里克·L·奥伯斯塔。定点表示和分数数学。*奥伯斯塔咨询*，9：19，2007年。
- en: 'Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,
    Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
    Soumith Chintala. Pytorch: An imperative style, high-performance deep learning
    library. In *Neural Information Processing Systems (NeuRIPS)*. 2019.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke 等人 [2019] 亚当·帕兹克、萨姆·格罗斯、弗朗西斯科·马萨、亚当·莱勒、詹姆斯·布拉德伯里、格雷戈里·查南、特雷弗·基林、林泽明、娜塔利亚·吉梅尔谢恩、卢卡·安蒂加、阿尔班·德斯梅森、安德烈亚斯·科普夫、爱德华·杨、扎卡里·德维托、马丁·雷森、阿利汗·特贾尼、萨桑克·奇拉姆库尔蒂、布诺瓦·斯坦纳、卢芳、白俊杰和苏密特·辛塔拉。Pytorch：一种命令式风格的高性能深度学习库。见于
    *神经信息处理系统（NeuRIPS）*，2019年。
- en: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi 等人 [2021] 坂口圭介、罗南·勒·布拉斯、昌德拉·巴伽瓦图拉和叶进。Winogrande：大规模对抗性 Winograd 语法挑战。*计算机通信通讯*，64(9)：99–106，2021年。
- en: 'Shao et al. [2023] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. *arXiv preprint arXiv:2308.13137*,
    2023.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等人 [2023] 沈文奇、陈梦钊、张兆阳、徐鹏、赵丽睿、李志前、张凯鹏、高鹏、邱宇和罗平。Omniquant：大语言模型的全向校准量化。*arXiv
    预印本 arXiv:2308.13137*，2023年。
- en: 'Shen et al. [2024] Xuan Shen, Zhenglun Kong, Changdi Yang, Zhaoyang Han, Lei
    Lu, Peiyan Dong, Cheng Lyu, Chih-hsiang Li, Xuehang Guo, Zhihao Shu, et al. Edgeqat:
    Entropy and distribution guided quantization-aware training for the acceleration
    of lightweight llms on the edge. *arXiv preprint arXiv:2402.10787*, 2024.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人 [2024] 沈轩、孔正伦、杨昌迪、韩兆阳、卢磊、董佩妍、吕诚、李志翔、郭学航、舒智豪等人。Edgeqat：熵和分布引导的量化感知训练，用于加速边缘上的轻量级
    LLMs。*arXiv 预印本 arXiv:2402.10787*，2024年。
- en: 'Soboleva et al. [2023] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R
    Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated
    version of RedPajama. [https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama),
    June 2023. URL [https://huggingface.co/datasets/cerebras/SlimPajama-627B](https://huggingface.co/datasets/cerebras/SlimPajama-627B).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Soboleva等人[2023] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves,
    Joel Hestness和Nolan Dey。SlimPajama: 一个627B标记的清理和去重版RedPajama。[https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama)，2023年6月。网址[https://huggingface.co/datasets/cerebras/SlimPajama-627B](https://huggingface.co/datasets/cerebras/SlimPajama-627B)。'
- en: Sun et al. [2024] Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. Massive
    activations in large language models. *arXiv preprint arXiv:2402.17762*, 2024.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等人[2024] Mingjie Sun, Xinlei Chen, J Zico Kolter和Zhuang Liu。大型语言模型中的大规模激活。*arXiv预印本
    arXiv:2402.17762*，2024年。
- en: 'Tang et al. [2024] Hanlin Tang, Yifu Sun, Decheng Wu, Kai Liu, Jianchen Zhu,
    and Zhanhui Kang. Easyquant: An efficient data-free quantization algorithm for
    llms. *arXiv preprint arXiv:2403.02775*, 2024.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tang等人[2024] Hanlin Tang, Yifu Sun, Decheng Wu, Kai Liu, Jianchen Zhu和Zhanhui
    Kang。Easyquant: 一种高效的无数据量化算法用于LLMs。*arXiv预印本 arXiv:2403.02775*，2024年。'
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron等人[2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar等。Llama: 开放且高效的基础语言模型。*arXiv预印本 arXiv:2302.13971*，2023年。'
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron等人[2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale等。Llama 2: 开放基础和微调对话模型。*arXiv预印本 arXiv:2307.09288*，2023年。'
- en: 'Wei et al. [2022] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong,
    Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression:
    Pushing the limit of low-bit transformer language models. *arXiv preprint arXiv:2209.13325*,
    2022.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei等人[2022] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang
    Zhang, Qi Zhang, Fengwei Yu和Xianglong Liu。Outlier suppression: 推动低比特变换器语言模型的极限。*arXiv预印本
    arXiv:2209.13325*，2022年。'
- en: 'Wei et al. [2023] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao
    Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization
    of large language models by equivalent and optimal shifting and scaling. *arXiv
    preprint arXiv:2304.09145*, 2023.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei等人[2023] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong,
    Jinyang Guo和Xianglong Liu。Outlier suppression+: 通过等效和最优的移位与缩放实现对大型语言模型的准确量化。*arXiv预印本
    arXiv:2304.09145*，2023年。'
- en: 'Wolf et al. [2020] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. Transformers: State-of-the-art natural language processing. In *Proceedings
    of the 2020 conference on empirical methods in natural language processing: system
    demonstrations*, pages 38–45, 2020.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf等人[2020] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement
    Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz等。Transformers:
    先进的自然语言处理技术。在*2020年自然语言处理经验方法会议：系统演示*会议记录，第38–45页，2020年。'
- en: 'Xiao et al. [2023] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *International Conference on Machine Learning*, pages
    38087–38099\. PMLR, 2023.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao等人[2023] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth和Song
    Han。Smoothquant: 精确且高效的大型语言模型后训练量化。在*国际机器学习会议*，第38087–38099页。PMLR，2023年。'
- en: 'Xu et al. [2023] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng
    Zhang, Zhensu Chen, Xiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank
    adaptation of large language models. *arXiv preprint arXiv:2309.14717*, 2023.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu等人[2023] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng
    Zhang, Zhensu Chen, Xiaopeng Zhang和Qi Tian。Qa-lora: 大型语言模型的量化感知低秩自适应。*arXiv预印本
    arXiv:2309.14717*，2023年。'
- en: 'Yao et al. [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. In S. Koyejo, S. Mohamed, A. Agarwal,
    D. Belgrave, K. Cho, and A. Oh, editors, *Advances in Neural Information Processing
    Systems*, volume 35, pages 27168–27183\. Curran Associates, Inc., 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/adf7fa39d65e2983d724ff7da57f00ac-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/adf7fa39d65e2983d724ff7da57f00ac-Paper-Conference.pdf).'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人 [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu,
    Conglong Li 和 Yuxiong He. Zeroquant: 高效且经济的大规模变换器后训练量化。见 S. Koyejo, S. Mohamed,
    A. Agarwal, D. Belgrave, K. Cho 和 A. Oh 编，*神经信息处理系统进展*，第 35 卷，页 27168–27183。Curran
    Associates, Inc., 2022年。网址 [https://proceedings.neurips.cc/paper_files/paper/2022/file/adf7fa39d65e2983d724ff7da57f00ac-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/adf7fa39d65e2983d724ff7da57f00ac-Paper-Conference.pdf)。'
- en: 'Yuan et al. [2023] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang,
    Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based
    post-training quantization for large language models. *arXiv preprint arXiv:2304.01089*,
    2023.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan 等人 [2023] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang,
    Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu 和 Bingzhe Wu. Rptq: 基于重排序的后训练量化用于大规模语言模型。
    *arXiv 预印本 arXiv:2304.01089*，2023年。'
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers 等人 [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi 和
    Yejin Choi. Hellaswag: 机器真的能完成你的句子吗？ *arXiv 预印本 arXiv:1905.07830*，2019年。'
- en: 'Zhang et al. [2024] Cheng Zhang, Jianyi Cheng, George A Constantinides, and
    Yiren Zhao. Lqer: Low-rank quantization error reconstruction for llms. *arXiv
    preprint arXiv:2402.02446*, 2024.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2024] Cheng Zhang, Jianyi Cheng, George A Constantinides 和 Yiren
    Zhao. Lqer: 针对大型语言模型的低秩量化误差重构。 *arXiv 预印本 arXiv:2402.02446*，2024年。'
- en: Zhao et al. [2019] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru
    Zhang. Improving neural network quantization without retraining using outlier
    channel splitting. In *International conference on machine learning*, pages 7543–7552\.
    PMLR, 2019.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2019] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa 和 Zhiru Zhang.
    改进神经网络量化而不重新训练，利用异常通道分裂。见 *国际机器学习会议*，页 7543–7552。PMLR，2019年。
- en: 'Zhou et al. [2016] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and
    Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with
    low bitwidth gradients. *arXiv preprint arXiv:1606.06160*, 2016.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人 [2016] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen 和 Yuheng
    Zou. Dorefa-net: 训练低位宽卷积神经网络与低位宽梯度。 *arXiv 预印本 arXiv:1606.06160*，2016年。'
- en: Appendix A Extended results
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 扩展结果
- en: In this section, we provide extended results and present some additional ablation
    studies.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们提供了扩展结果，并展示了一些额外的消融研究。
- en: 'Table A1: LM-eval weight-only quantization results for LLaMA-2/3 and Mistral
    models. We report zero-shot accuracy of 6 tasks (higher is better).'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A1: LLaMA-2/3 和 Mistral 模型的 LM-eval 仅权重量化结果。我们报告了 6 项任务的零样本准确率（数值越高越好）。'
- en: '| Model | # Bits | Method | BoolQ | PIQA | Winogrande | ARC-e | ARC-c | HellaSwag
    | Avg. |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | # 位 | 方法 | BoolQ | PIQA | Winogrande | ARC-e | ARC-c | HellaSwag | 平均值
    |'
- en: '| LLaMA-2 7B | FP16 |  | 77.74 | 79.11 | 69.14 | 74.58 | 46.25 | 75.98 | 70.47
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2 7B | FP16 |  | 77.74 | 79.11 | 69.14 | 74.58 | 46.25 | 75.98 | 70.47
    |'
- en: '| W4pc | RTN | 76.36 | 78.07 | 68.19 | 71.21 | 44.80 | 74.65 | 68.88 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| W4pc | RTN | 76.36 | 78.07 | 68.19 | 71.21 | 44.80 | 74.65 | 68.88 |'
- en: '| PEQA (our impl.) | 77.49 | 78.24 | 69.61 | 70.96 | 43.52 | 75.54 | 69.23
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 77.49 | 78.24 | 69.61 | 70.96 | 43.52 | 75.54 | 69.23 |'
- en: '| LR-QAT (ours) | 77.43 | 78.45 | 69.69 | 73.15 | 45.48 | 75.51 | 69.95 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 77.43 | 78.45 | 69.69 | 73.15 | 45.48 | 75.51 | 69.95 |'
- en: '| W3pc | RTN | 46.27 | 60.28 | 54.85 | 38.05 | 23.29 | 40.47 | 43.87 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| W3pc | RTN | 46.27 | 60.28 | 54.85 | 38.05 | 23.29 | 40.47 | 43.87 |'
- en: '| PEQA (our impl.) | 71.62 | 76.82 | 66.14 | 65.66 | 39.76 | 72.63 | 65.44
    |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 71.62 | 76.82 | 66.14 | 65.66 | 39.76 | 72.63 | 65.44 |'
- en: '| LR-QAT (ours) | 74.43 | 77.15 | 68.03 | 69.95 | 43.09 | 73.29 | 67.66 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 74.43 | 77.15 | 68.03 | 69.95 | 43.09 | 73.29 | 67.66 |'
- en: '| W4g128 | RTN | 76.76 | 78.18 | 69.77 | 72.60 | 45.73 | 75.43 | 69.75 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| W4g128 | RTN | 76.76 | 78.18 | 69.77 | 72.60 | 45.73 | 75.43 | 69.75 |'
- en: '| PEQA (our impl.) | 76.88 | 78.89 | 69.85 | 72.18 | 44.11 | 75.95 | 69.64
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 76.88 | 78.89 | 69.85 | 72.18 | 44.11 | 75.95 | 69.64 |'
- en: '| LR-QAT (ours) | 76.73 | 78.62 | 70.48 | 72.85 | 44.97 | 75.62 | 69.88 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 76.73 | 78.62 | 70.48 | 72.85 | 44.97 | 75.62 | 69.88 |'
- en: '| W3g128 | RTN | 66.42 | 75.57 | 65.19 | 64.90 | 38.14 | 68.96 | 63.20 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| W3g128 | RTN | 66.42 | 75.57 | 65.19 | 64.90 | 38.14 | 68.96 | 63.20 |'
- en: '| PEQA (our impl.) | 75.38 | 77.97 | 68.59 | 70.62 | 42.32 | 73.74 | 68.10
    |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 75.38 | 77.97 | 68.59 | 70.62 | 42.32 | 73.74 | 68.10 |'
- en: '| LR-QAT (ours) | 73.30 | 78.07 | 67.72 | 71.46 | 43.77 | 73.53 | 67.98 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 73.30 | 78.07 | 67.72 | 71.46 | 43.77 | 73.53 | 67.98 |'
- en: '| LLaMA-2 13B | FP16 |  | 80.55 | 80.52 | 72.22 | 77.44 | 48.98 | 79.38 | 73.18
    |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2 13B | FP16 |  | 80.55 | 80.52 | 72.22 | 77.44 | 48.98 | 79.38 | 73.18
    |'
- en: '| W4pc | RTN | 79.30 | 79.71 | 70.01 | 75.51 | 48.89 | 76.96 | 71.73 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| W4pc | RTN | 79.30 | 79.71 | 70.01 | 75.51 | 48.89 | 76.96 | 71.73 |'
- en: '| PEQA (our impl.) | 78.99 | 80.14 | 71.27 | 76.43 | 48.98 | 79.24 | 72.51
    |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 78.99 | 80.14 | 71.27 | 76.43 | 48.98 | 79.24 | 72.51 |'
- en: '| LR-QAT (ours) | 79.91 | 80.25 | 71.03 | 77.48 | 50.68 | 79.45 | 73.13 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 79.91 | 80.25 | 71.03 | 77.48 | 50.68 | 79.45 | 73.13 |'
- en: '| W3pc | RTN | 55.05 | 71.06 | 54.22 | 56.19 | 32.25 | 61.27 | 55.01 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| W3pc | RTN | 55.05 | 71.06 | 54.22 | 56.19 | 32.25 | 61.27 | 55.01 |'
- en: '| PEQA (our impl.) | 74.28 | 78.67 | 69.06 | 74.87 | 45.99 | 76.00 | 69.81
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 74.28 | 78.67 | 69.06 | 74.87 | 45.99 | 76.00 | 69.81 |'
- en: '| LR-QAT (ours) | 78.62 | 79.49 | 72.61 | 73.99 | 45.56 | 77.05 | 71.22 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 78.62 | 79.49 | 72.61 | 73.99 | 45.56 | 77.05 | 71.22 |'
- en: '| W4g128 | RTN | 81.10 | 79.82 | 72.38 | 76.73 | 49.06 | 78.52 | 72.94 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| W4g128 | RTN | 81.10 | 79.82 | 72.38 | 76.73 | 49.06 | 78.52 | 72.94 |'
- en: '| PEQA (our impl.) | 80.28 | 80.63 | 71.74 | 76.14 | 48.38 | 79.62 | 72.80
    |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 80.28 | 80.63 | 71.74 | 76.14 | 48.38 | 79.62 | 72.80 |'
- en: '| LR-QAT (ours) | 80.73 | 79.92 | 71.98 | 76.60 | 48.81 | 79.31 | 72.89 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 80.73 | 79.92 | 71.98 | 76.60 | 48.81 | 79.31 | 72.89 |'
- en: '| W3g128 | RTN | 74.65 | 76.93 | 69.14 | 70.16 | 42.66 | 72.06 | 67.60 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| W3g128 | RTN | 74.65 | 76.93 | 69.14 | 70.16 | 42.66 | 72.06 | 67.60 |'
- en: '| PEQA (our impl.) | 78.56 | 78.73 | 69.85 | 73.61 | 44.28 | 76.69 | 70.29
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 78.56 | 78.73 | 69.85 | 73.61 | 44.28 | 76.69 | 70.29 |'
- en: '| LR-QAT (ours) | 79.69 | 78.78 | 68.98 | 73.99 | 44.88 | 76.89 | 70.54 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 79.69 | 78.78 | 68.98 | 73.99 | 44.88 | 76.89 | 70.54 |'
- en: '| LLaMA-3 8B | FP16 |  | 81.44 | 80.79 | 72.85 | 77.74 | 53.33 | 79.16 | 74.22
    |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3 8B | FP16 |  | 81.44 | 80.79 | 72.85 | 77.74 | 53.33 | 79.16 | 74.22
    |'
- en: '| W4pc | RTN | 79.02 | 78.56 | 72.85 | 75.97 | 49.32 | 77.44 | 72.19 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| W4pc | RTN | 79.02 | 78.56 | 72.85 | 75.97 | 49.32 | 77.44 | 72.19 |'
- en: '| PEQA (our impl.) | 79.57 | 78.67 | 72.93 | 77.19 | 51.11 | 77.25 | 72.79
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 79.57 | 78.67 | 72.93 | 77.19 | 51.11 | 77.25 | 72.79 |'
- en: '| LR-QAT (ours) | 80.09 | 80.41 | 73.88 | 76.89 | 50.51 | 78.30 | 73.35 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 80.09 | 80.41 | 73.88 | 76.89 | 50.51 | 78.30 | 73.35 |'
- en: '| W3pc | RTN | 58.65 | 61.75 | 56.04 | 39.60 | 23.81 | 44.91 | 47.46 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| W3pc | RTN | 58.65 | 61.75 | 56.04 | 39.60 | 23.81 | 44.91 | 47.46 |'
- en: '| PEQA (our impl.) | 62.63 | 63.66 | 56.59 | 42.34 | 27.13 | 49.35 | 50.28
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 62.63 | 63.66 | 56.59 | 42.34 | 27.13 | 49.35 | 50.28 |'
- en: '| LR-QAT (ours) | 77.46 | 78.51 | 69.85 | 74.83 | 47.35 | 74.73 | 70.46 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 77.46 | 78.51 | 69.85 | 74.83 | 47.35 | 74.73 | 70.46 |'
- en: '| W4g128 | RTN | 79.48 | 79.27 | 73.56 | 75.08 | 48.81 | 77.61 | 72.30 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| W4g128 | RTN | 79.48 | 79.27 | 73.56 | 75.08 | 48.81 | 77.61 | 72.30 |'
- en: '| PEQA (our impl.) | 80.98 | 80.14 | 72.61 | 76.18 | 49.57 | 78.45 | 72.99
    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 80.98 | 80.14 | 72.61 | 76.18 | 49.57 | 78.45 | 72.99 |'
- en: '| LR-QAT (ours) | 80.28 | 80.96 | 74.11 | 77.19 | 51.45 | 78.31 | 73.72 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 80.28 | 80.96 | 74.11 | 77.19 | 51.45 | 78.31 | 73.72 |'
- en: '| W3g128 | RTN | 65.47 | 68.39 | 65.19 | 54.00 | 33.45 | 59.96 | 57.74 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| W3g128 | RTN | 65.47 | 68.39 | 65.19 | 54.00 | 33.45 | 59.96 | 57.74 |'
- en: '| PEQA (our impl.) | 72.26 | 76.06 | 67.80 | 69.02 | 46.08 | 71.89 | 67.19
    |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 72.26 | 76.06 | 67.80 | 69.02 | 46.08 | 71.89 | 67.19 |'
- en: '| LR-QAT (ours) | 72.97 | 79.38 | 71.67 | 74.37 | 49.06 | 75.44 | 70.48 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 72.97 | 79.38 | 71.67 | 74.37 | 49.06 | 75.44 | 70.48 |'
- en: '| Mistral 7B | FP16 |  | 83.58 | 82.10 | 73.88 | 79.59 | 53.92 | 81.07 | 75.69
    |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| Mistral 7B | FP16 |  | 83.58 | 82.10 | 73.88 | 79.59 | 53.92 | 81.07 | 75.69
    |'
- en: '| W4pc | RTN | 81.22 | 80.63 | 72.53 | 76.77 | 50.09 | 79.41 | 73.44 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| W4pc | RTN | 81.22 | 80.63 | 72.53 | 76.77 | 50.09 | 79.41 | 73.44 |'
- en: '| PEQA (our impl.) | 81.80 | 81.12 | 72.61 | 77.23 | 50.17 | 79.43 | 73.73
    |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 81.80 | 81.12 | 72.61 | 77.23 | 50.17 | 79.43 | 73.73 |'
- en: '| LR-QAT (ours) | 80.12 | 81.83 | 73.40 | 76.73 | 49.66 | 80.25 | 73.67 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 80.12 | 81.83 | 73.40 | 76.73 | 49.66 | 80.25 | 73.67 |'
- en: '| W3pc | RTN | 68.13 | 77.64 | 63.93 | 63.93 | 41.13 | 72.73 | 64.58 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| W3pc | RTN | 68.13 | 77.64 | 63.93 | 63.93 | 41.13 | 72.73 | 64.58 |'
- en: '| PEQA (our impl.) | 80.03 | 80.09 | 69.93 | 72.90 | 45.82 | 77.32 | 71.02
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 80.03 | 80.09 | 69.93 | 72.90 | 45.82 | 77.32 | 71.02 |'
- en: '| LR-QAT (ours) | 81.62 | 80.09 | 70.96 | 74.75 | 46.08 | 77.71 | 71.87 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 81.62 | 80.09 | 70.96 | 74.75 | 46.08 | 77.71 | 71.87 |'
- en: '| W4g128 | RTN | 84.16 | 81.77 | 74.43 | 77.95 | 51.71 | 80.42 | 75.07 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| W4g128 | RTN | 84.16 | 81.77 | 74.43 | 77.95 | 51.71 | 80.42 | 75.07 |'
- en: '| PEQA (our impl.) | 80.89 | 81.72 | 73.80 | 75.42 | 48.46 | 79.76 | 73.34
    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 80.89 | 81.72 | 73.80 | 75.42 | 48.46 | 79.76 | 73.34 |'
- en: '| LR-QAT (ours) | 83.85 | 81.77 | 73.88 | 78.54 | 52.22 | 80.79 | 75.18 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT（我们的） | 83.85 | 81.77 | 73.88 | 78.54 | 52.22 | 80.79 | 75.18 |'
- en: '| W3g128 | RTN | 78.44 | 79.60 | 69.14 | 71.17 | 43.00 | 74.75 | 69.35 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| W3g128 | RTN | 78.44 | 79.60 | 69.14 | 71.17 | 43.00 | 74.75 | 69.35 |'
- en: '| PEQA (our impl.) | 81.99 | 81.18 | 69.61 | 74.92 | 47.18 | 78.37 | 72.21
    |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| PEQA（我们的实现） | 81.99 | 81.18 | 69.61 | 74.92 | 47.18 | 78.37 | 72.21 |'
- en: '| LR-QAT (ours) | 81.38 | 80.41 | 70.32 | 73.15 | 45.39 | 78.00 | 71.44 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| LR-QAT (我们的方法) | 81.38 | 80.41 | 70.32 | 73.15 | 45.39 | 78.00 | 71.44 |'
- en: 'Table A2: A comparison between min-max and the best range setting used for
    round-to-nearest (RTN) initialization for LLaMA-2/3 and Mistral models. We report
    WikiText-2 test set perplexity (lower is better) and average zero-shot accuracy
    (higher is better).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A2: min-max 和用于 LLaMA-2/3 和 Mistral 模型的最佳范围设置之间的比较。我们报告了 WikiText-2 测试集的困惑度（越低越好）和平均零样本准确率（越高越好）。'
- en: '| Model | # Bits | RTN init. | WikiText-2 $\downarrow$ |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | # 位 | RTN 初始化 | WikiText-2 $\downarrow$ |'
- en: '| LLaMA-2 7B | FP16 |  | 5.47 | 70.47 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2 7B | FP16 |  | 5.47 | 70.47 |'
- en: '| W4pc | min-max | 7.14 | 66.41 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| W4pc | min-max | 7.14 | 66.41 |'
- en: '| best ($L^{3.5}$) | 6.14 | 68.88 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| best ($L^{3.5}$) | 6.14 | 68.88 |'
- en: '| W3pc | min-max | 1.9e4 | 35.71 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| W3pc | min-max | 1.9e4 | 35.71 |'
- en: '| best ($L^{3.5}$) | 26.73 | 43.87 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| best ($L^{3.5}$) | 26.73 | 43.87 |'
- en: '| W4g128 | best = min-max | 5.78 | 69.75 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| W4g128 | best = min-max | 5.78 | 69.75 |'
- en: '| W3g128 | min-max | 8.22 | 64.07 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| W3g128 | min-max | 8.22 | 64.07 |'
- en: '| best ($L^{4}$) | 7.61 | 63.20 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| best ($L^{4}$) | 7.61 | 63.20 |'
- en: '| LLaMA-2 13B | FP16 |  | 4.88 | 73.18 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2 13B | FP16 |  | 4.88 | 73.18 |'
- en: '| W4pc | min-max | 5.40 | 72.19 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| W4pc | min-max | 5.40 | 72.19 |'
- en: '| best ($L^{3.5}$) | 5.21 | 71.73 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| best ($L^{3.5}$) | 5.21 | 71.73 |'
- en: '| W3pc | min-max | 2.3e3 | 37.85 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| W3pc | min-max | 2.3e3 | 37.85 |'
- en: '| best ($L^{5}$) | 8.71 | 55.01 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| best ($L^{5}$) | 8.71 | 55.01 |'
- en: '| W4g128 | best = min-max | 5.04 | 72.94 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| W4g128 | best = min-max | 5.04 | 72.94 |'
- en: '| W3g128 | min-max | 6.14 | 66.81 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| W3g128 | min-max | 6.14 | 66.81 |'
- en: '| best ($L^{5}$) | 6.20 | 67.60 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| best ($L^{5}$) | 6.20 | 67.60 |'
- en: '| LLaMA-3 8B | FP16 |  | 6.14 | 74.22 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3 8B | FP16 |  | 6.14 | 74.22 |'
- en: '| W4pc | min-max | 10.53 | 67.44 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| W4pc | min-max | 10.53 | 67.44 |'
- en: '| best ($L^{3.5}$) | 7.53 | 72.19 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| best ($L^{3.5}$) | 7.53 | 72.19 |'
- en: '| W3pc | min-max | 1.6e5 | 35.78 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| W3pc | min-max | 1.6e5 | 35.78 |'
- en: '| best ($L^{3.5}$) | 34.10 | 47.46 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| best ($L^{3.5}$) | 34.10 | 47.46 |'
- en: '| W4g128 | min-max | 6.99 | 72.95 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| W4g128 | min-max | 6.99 | 72.95 |'
- en: '| best ($L^{4}$) | 6.96 | 72.30 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| best ($L^{4}$) | 6.96 | 72.30 |'
- en: '| W3g128 | min-max | 29.38 | 54.54 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| W3g128 | min-max | 29.38 | 54.54 |'
- en: '| best ($L^{5}$) | 15.11 | 57.74 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| best ($L^{5}$) | 15.11 | 57.74 |'
- en: '| Mistral 7B | FP16 |  | 5.25 | 75.69 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| Mistral 7B | FP16 |  | 5.25 | 75.69 |'
- en: '| W4pc | min-max | 6.33 | 71.84 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| W4pc | min-max | 6.33 | 71.84 |'
- en: '| best ($L^{4}$) | 5.91 | 73.44 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| best ($L^{4}$) | 5.91 | 73.44 |'
- en: '| W3pc | min-max | 3.2e3 | 36.78 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| W3pc | min-max | 3.2e3 | 36.78 |'
- en: '| best ($L^{4}$) | 9.49 | 64.58 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| best ($L^{4}$) | 9.49 | 64.58 |'
- en: '| W4g128 | min-max | 5.51 | 74.98 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| W4g128 | min-max | 5.51 | 74.98 |'
- en: '| best ($L^{5}$) | 5.49 | 75.07 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| best ($L^{5}$) | 5.49 | 75.07 |'
- en: '| W3g128 | min-max | 7.22 | 68.35 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| W3g128 | min-max | 7.22 | 68.35 |'
- en: '| best ($L^{5}$) | 6.77 | 69.35 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| best ($L^{5}$) | 6.77 | 69.35 |'
- en: Appendix B Experimental details
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 实验细节
- en: In this section, we list the details related to hyperparameters and other settings
    used in our experiments. If not stated otherwise, the standard hyperparameters
    that we use are the one shown in Table [B1](#A2.T1 "Table B1 ‣ Appendix B Experimental
    details ‣ Low-Rank Quantization-Aware Training for LLMs").
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们列出了与超参数和其他设置相关的实验细节。如果没有特别说明，我们使用的标准超参数如表 [B1](#A2.T1 "表 B1 ‣ 附录 B 实验细节
    ‣ 低秩量化感知训练") 中所示。
- en: '| Hyperparameter | Value / Search space |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 / 搜索空间 |'
- en: '| Optimizer | AdamW |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | AdamW |'
- en: '| Learning rate for $\bm{A}$ |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| $\bm{A}$ 的学习率 |'
- en: '| Learning rate for quantization scale ($\mathbf{s}$ |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 量化尺度的学习率 ($\mathbf{s}$ |'
- en: '| Learning rate schedule for $\bm{A}$ | linear (with warmup) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| $\bm{A}$ 的学习率计划 | 线性（带预热） |'
- en: '| Learning rate schedule for quantization scale ($\mathbf{s}$) | linear (with
    warmup) |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 量化尺度的学习率计划 ($\mathbf{s}$) | 线性（带预热） |'
- en: '| Weight decay for $\bm{A}$ |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| $\bm{A}$ 的权重衰减 |'
- en: '| Weight decay for quantization scale ($\mathbf{s}$ |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 量化尺度的权重衰减 ($\mathbf{s}$ |'
- en: '| Adam $\beta_{1}$ |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| Adam $\beta_{1}$ |'
- en: '| Adam $\beta_{2}$ |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| Adam $\beta_{2}$ |'
- en: '| Training steps | $10^{4}$ |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 训练步骤 | $10^{4}$ |'
- en: '| Warmup steps | $10\%$ of Training steps |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 预热步骤 | $10\%$ 的训练步骤 |'
- en: '| Batch size | $32$ |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | $32$ |'
- en: '| Maximum sequence length (during training) | $1024$ |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 最大序列长度（训练期间） | $1024$ |'
- en: '| $L^{2}$ |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| $L^{2}$ |'
- en: '| $\alpha$ |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha$ |'
- en: 'Table B1: Common hyperparameters used for experiments. ^*Is equivalent to freezing
    the quantization scale obtained after initial range estimation ($\mathbf{s}=\mathbf{s_{0}}$).'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '表 B1: 实验中使用的常见超参数。^*相当于在初始范围估计后冻结量化尺度 ($\mathbf{s}=\mathbf{s_{0}}$)。'
- en: Quantization
  id: totrans-374
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化
- en: We experiment with both weight-only and weight-activation quantization. The
    default settings are INT4 / INT3 per-channel (denoted ‘pc’) and group-wise weight
    quantization with a group size of 128 (denoted ‘g128’). We always use symmetric
    uniform affine quantization. We quantize all linear layers, except the classification
    head. LayerNorm and Embedding layers are always kept at full precision. In weight-activation
    quantization, defaults are INT4 per-channel weight and per-token activation quantization [[14](#bib.bib14)].
    Following OmniQuant [[55](#bib.bib55)] we quantize all inputs to matmuls with
    exception of the softmax output and additionally quantize the KV-cache as in LLM-QAT [[44](#bib.bib44)].
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试了仅权重和权重-激活量化。默认设置为每通道 INT4 / INT3（标记为‘pc’）和组间权重量化，组大小为 128（标记为‘g128’）。我们始终使用对称均匀仿射量化。我们量化了所有线性层，但分类头除外。LayerNorm
    和嵌入层始终保持全精度。在权重-激活量化中，默认设置为每通道 INT4 权重和每令牌激活量化[[14](#bib.bib14)]。根据 OmniQuant[[55](#bib.bib55)]，我们量化了所有输入到
    matmuls，但软最大输出除外，并且还量化了 KV 缓存，类似于 LLM-QAT[[44](#bib.bib44)]。
- en: Libraries
  id: totrans-376
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 库
- en: We implement our method in PyTorch [[53](#bib.bib53)] and use training and evaluation
    pipelines from HuggingFace libraries [[20](#bib.bib20), [38](#bib.bib38), [64](#bib.bib64)].
    For zero-shot evaluation, we use the LM Evaluation Harness framework [[19](#bib.bib19)].
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 PyTorch 中实现了我们的方法[[53](#bib.bib53)]，并使用了来自 HuggingFace 库的训练和评估管道[[20](#bib.bib20),
    [38](#bib.bib38), [64](#bib.bib64)]。对于零-shot 评估，我们使用了 LM Evaluation Harness 框架[[19](#bib.bib19)]。
- en: Datasets and training
  id: totrans-378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集和训练
- en: To optimize the learnable parameters, we use AdamW optimizer [[45](#bib.bib45)]
    with weight decay set to zero, $(\beta_{1},\beta_{2})=(0.9,0.95)$ steps, following
    by a linear decay to zero by the end of training. We use a separate maximum learning
    rate for quantization scales and for low-rank adapters, which are tuned depending
    on the experiment.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化可学习参数，我们使用 AdamW 优化器[[45](#bib.bib45)]，权重衰减设置为零，$(\beta_{1},\beta_{2})=(0.9,0.95)$
    步，随后在训练结束时线性衰减到零。我们为量化尺度和低秩适配器使用了不同的最大学习率，这取决于实验的具体情况。
- en: We apply our methods to all linear layers in the attention blocks (both in self-attention
    and in the feed-forward network). We only train low-rank auxiliary matrices $\bm{A}$.
    Specifically, we freeze the token embeddings, the final classification heads and
    LayerNorm parameters.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将方法应用于注意力块中的所有线性层（包括自注意力和前馈网络）。我们仅训练低秩辅助矩阵 $\bm{A}$。具体来说，我们冻结了令牌嵌入、最终分类头和
    LayerNorm 参数。
- en: We train on a small subset of SlimPajama [[57](#bib.bib57)], which is a close
    open-source replica of the dataset used for pre-training LLaMA models. We select
    hyperparameters based on the perplexity of a small subset of Wikipedia validation
    set⁴⁴4Specifically, we use the English subset of Wiki40b, [https://huggingface.co/datasets/wiki40b](https://huggingface.co/datasets/wiki40b),
    that contains cleaned-up text of English Wikipedia and training/validation splits.
    (512 sequences). For weight-only and weight-activation quantization results, we
    train for $10^{4}$ steps. The rest of the hyperparameters and their search spaces
    are listed in Table [B1](#A2.T1 "Table B1 ‣ Appendix B Experimental details ‣
    Low-Rank Quantization-Aware Training for LLMs")
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 SlimPajama 的一个小子集上进行训练[[57](#bib.bib57)]，这是一个接近于用于预训练 LLaMA 模型的数据集的开源副本。我们基于维基百科验证集的困惑度选择超参数。具体来说，我们使用的是
    Wiki40b 的英文子集，[https://huggingface.co/datasets/wiki40b](https://huggingface.co/datasets/wiki40b)，该子集包含了英文维基百科的清理文本和训练/验证拆分（512
    序列）。对于仅权重和权重-激活量化结果，我们训练了 $10^{4}$ 步。其他超参数及其搜索空间列在表格 [B1](#A2.T1 "Table B1 ‣ Appendix
    B Experimental details ‣ Low-Rank Quantization-Aware Training for LLMs") 中。
- en: PTQ initialization
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PTQ 初始化
- en: We compare with vanilla round-to-nearest quantization (RTN), where we explore
    several choices of range setting and report the best one based on Wikipedia validation
    set perplexity, and also use that as initialization for our method. Specifically,
    we experimented with min-max range estimator and with $L^{p}$.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与原始的最近取整量化（RTN）进行了比较，其中我们探索了几种范围设置选项，并根据维基百科验证集的困惑度报告了最佳选项，并将其作为我们方法的初始化。具体来说，我们尝试了
    min-max 范围估计器和 $L^{p}$。
- en: Computational Resources
  id: totrans-384
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算资源
- en: Each experiment for which we reported results, was executed on a single NVidia
    A100 GPU equipped with 80GB of VRAM. LLaMA-2 7B and 13B experiments needed respectively
    around 3 and 5 days for 10k training steps experiments. Mistral 7B and LLaMA-3
    8B needed around 1.6 days for 5k training steps experiments. For obtaining all
    the results in the paper, including the ablations, we needed 107 GPU days (A100).
    Including preliminary experiments that did not make it in the final paper and
    hyperparameter turning we estimate the total compute costs of this research to
    approximately 500 GPU days.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告结果的每个实验都是在配备了80GB VRAM的单个NVidia A100 GPU上执行的。LLaMA-2 7B 和 13B 实验分别需要约3天和5天的时间来完成10k训练步骤的实验。Mistral
    7B 和 LLaMA-3 8B 实验需要约1.6天来完成5k训练步骤的实验。为了获得论文中的所有结果，包括消融实验，我们需要107个GPU天（A100）。包括未纳入最终论文的初步实验和超参数调整，我们估计这项研究的总计算成本约为500个GPU天。
