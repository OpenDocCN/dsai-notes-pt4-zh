- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:54:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:54:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Prompt-prompted Mixture of Experts for Efficient LLM Generation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于高效 LLM 生成的提示-提示混合专家
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.01365](https://ar5iv.labs.arxiv.org/html/2404.01365)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.01365](https://ar5iv.labs.arxiv.org/html/2404.01365)
- en: Harry Dong
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Harry Dong
- en: 'CMU Department of Electrical and Computer Engineering, Carnegie Mellon University,
    USA; Emails: {harryd,beidic,yuejiec}@andrew.cmu.edu.    Beidi Chen¹¹footnotemark:
    1'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'CMU 电气与计算机工程系，卡内基梅隆大学，美国；电子邮件：{harryd,beidic,yuejiec}@andrew.cmu.edu。    Beidi
    Chen¹¹footnotemark: 1'
- en: 'CMU    Yuejie Chi¹¹footnotemark: 1'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 'CMU    Yuejie Chi¹¹footnotemark: 1'
- en: CMU
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: CMU
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: With the development of transformer-based large language models (LLMs), they
    have been applied to many fields due to their remarkable utility, but this comes
    at a considerable computational cost at deployment. Fortunately, some methods
    such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity
    in transformer feedforward (FF) blocks to gain boosts in speed and reduction in
    memory requirements. However, these techniques can be very costly and inflexible
    in practice, as they often require training or are restricted to specific types
    of architectures. To address this, we introduce GRIFFIN, a novel training-free
    MoE that selects unique FF experts at the sequence level for efficient generation
    across a plethora of LLMs with different non-ReLU activation functions. This is
    possible due to a critical observation that many trained LLMs naturally produce
    highly structured FF activation patterns within a sequence, which we call flocking.
    Despite our method’s simplicity, we show with 50% of the FF parameters, GRIFFIN
    maintains the original model’s performance with little to no degradation on a
    variety of classification and generation tasks, all while improving latency (e.g.
    1.25$\times$ speed-up in Llama 2 13B on an NVIDIA L40). Code is available at [https://github.com/hdong920/GRIFFIN](https://github.com/hdong920/GRIFFIN).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于变换器的大型语言模型（LLMs）的发展，它们由于其显著的实用性已被应用于许多领域，但这在部署时需要付出相当大的计算成本。幸运的是，一些方法如剪枝或构建专家混合（MoE）旨在利用变换器前馈（FF）模块中的稀疏性，以获得速度提升和内存需求的减少。然而，这些技术在实践中可能非常昂贵且不灵活，因为它们通常需要训练或仅限于特定类型的架构。为了解决这个问题，我们介绍了GRIFFIN，这是一种新型的无训练
    MoE，它在序列级别选择独特的 FF 专家，以实现不同非 ReLU 激活函数的众多 LLM 的高效生成。这得益于一个关键观察：许多训练后的 LLM 自然在序列内产生高度结构化的
    FF 激活模式，我们称之为“群体化”。尽管我们的方法很简单，但我们展示了使用 50% 的 FF 参数，GRIFFIN 在各种分类和生成任务中保持了原始模型的性能，并且几乎没有性能下降，同时提高了延迟（例如，在
    NVIDIA L40 上 Llama 2 13B 提速 1.25$\times$）。代码可在 [https://github.com/hdong920/GRIFFIN](https://github.com/hdong920/GRIFFIN)
    获得。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Transformers [[VSP^+17](#bib.bibx50)] have demonstrated incredible capabilities
    across a plethora of domains [[LWLQ22](#bib.bibx28), [KNH^+22](#bib.bibx23), [NBZ^+23](#bib.bibx35)].
    Their large language model (LLM) successors [[TMS^+23](#bib.bibx49), [TAB^+23](#bib.bibx46),
    [JSM^+23](#bib.bibx21), [JSR^+24](#bib.bibx22), [TMH^+24](#bib.bibx48), [Ant24](#bib.bibx2)]
    have pushed the bar higher, but these behemoths have become performative at the
    price of enormous amounts of computation and memory demands. One significant contributor
    is the model size itself. Not only is storage an issue, model layers tend to be
    wide and plenty, slowing down inference. Moreover, given the existence of sparse
    structures in LLMs, especially in feedforward (FF) blocks [[GSBL20](#bib.bibx17),
    [DLBZ22](#bib.bibx11), [LYB^+22](#bib.bibx29), [LWD^+23](#bib.bibx27)], these
    models waste computation on intermediate features with little to no impact on
    the final result. For instance, it has been observed that in OPT-175B [[ZRG^+22](#bib.bibx59)],
    fewer than 5% of neurons in FF blocks have nonzero values per token [[LWD^+23](#bib.bibx27)],
    meaning 95% of the compute in each FF block is wasted. Usually consisting of around
    two-thirds of the parameters in an LLM, FF blocks can be serious memory and compute
    bottlenecks. These inefficiencies are highly problematic in latency-sensitive
    scenarios like in chatbots and autonomous vehicles.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers [[VSP^+17](#bib.bibx50)] 在众多领域展现了令人难以置信的能力 [[LWLQ22](#bib.bibx28),
    [KNH^+22](#bib.bibx23), [NBZ^+23](#bib.bibx35)]。它们的大型语言模型（LLM）继任者 [[TMS^+23](#bib.bibx49),
    [TAB^+23](#bib.bibx46), [JSM^+23](#bib.bibx21), [JSR^+24](#bib.bibx22), [TMH^+24](#bib.bibx48),
    [Ant24](#bib.bibx2)] 将标准提升到了更高的层次，但这些庞然大物在计算和内存需求上付出了巨大的代价。一个重要的因素是模型本身的大小。不仅存储是一个问题，模型层通常宽度很大，数量也很多，从而减慢了推理速度。此外，由于
    LLMs 中存在稀疏结构，特别是在前馈（FF）块中 [[GSBL20](#bib.bibx17), [DLBZ22](#bib.bibx11), [LYB^+22](#bib.bibx29),
    [LWD^+23](#bib.bibx27)]，这些模型在中间特征上的计算浪费很大，对最终结果几乎没有影响。例如，观察到在 OPT-175B [[ZRG^+22](#bib.bibx59)]
    中，FF 块中少于 5% 的神经元在每个 token 上具有非零值 [[LWD^+23](#bib.bibx27)]，这意味着每个 FF 块中 95% 的计算是浪费的。通常，FF
    块占据了 LLM 中大约三分之二的参数，它们可能成为严重的内存和计算瓶颈。这些低效在像聊天机器人和自动驾驶汽车这样的延迟敏感场景中尤为问题严重。
- en: 'There have been many methods to exploit sparsity in LLMs for efficiency gains,
    such as pruning model weights and constructing mixtures of experts (MoEs). Pruning
    removes low-impact pre-trained weights to reduce storage, yet this often does
    not translate to real speed-ups in practice, unless the pruning is done in a hardware-friendly
    manner which typically causes greater performance deterioration. MoEs better preserve
    the original performance by adaptively selecting subsets of the model to use per
    input but also come with drawbacks. Unless the model has been trained in this
    fashion [[FZS22](#bib.bibx15), [JSR^+24](#bib.bibx22)], it will need to learn
    a cheap yet effective gating function (expert selection mechanism) and sometimes
    even require full fine tuning. Perhaps an even bigger weakness of many of these
    methods is the inability to effectively carry over to pre-trained LLMs with non-ReLU
    activations. We seek to overcome these challenges with MoEs:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有很多方法利用 LLM 中的稀疏性来提高效率，例如修剪模型权重和构建混合专家（MoEs）。修剪通过移除影响较小的预训练权重来减少存储，但这往往不会在实际中转化为真正的速度提升，除非修剪是以硬件友好的方式进行的，这通常会导致更大的性能下降。MoEs
    通过对每个输入自适应地选择模型的子集，从而更好地保留了原始性能，但也有其缺点。除非模型以这种方式进行了训练 [[FZS22](#bib.bibx15), [JSR^+24](#bib.bibx22)]，否则它需要学习一个廉价而有效的门控函数（专家选择机制），有时甚至需要完全微调。或许这些方法的一个更大弱点是无法有效地应用于具有非
    ReLU 激活的预训练 LLMs。我们寻求通过 MoEs 克服这些挑战：
- en: '1.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Most current LLMs are not MoEs, so they must be adapted by training gating functions
    and/or fine tuning the whole model.
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目前大多数现有的大型语言模型（LLMs）并非混合专家模型（MoEs），因此必须通过训练门控函数和/或对整个模型进行微调来进行适配。
- en: '2.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Gating functions should be simple enough to not cost more than they save.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 门控函数应该足够简单，以免其成本高于节省的成本。
- en: '3.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Many current approaches work solely on FF blocks with ReLUs or depend on expensive
    conversions to ReLUs.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 许多当前的方法仅在使用 ReLU 的前馈（FF）块上进行操作，或依赖于将其转换为 ReLU 的昂贵过程。
- en: Daunting at first, these challenges become surmountable because of a simple
    observation of a phenomenon we call flocking, highly consistent sparse activations
    that persist throughout a sequence observed in many LLMs. Flocking emerges in
    FF activations (inputs into the FF down projection) when we focus on a sequence’s
    relative activation magnitudes instead of the absolute values. Examples from Llama
    2 7B and Gemma 7B are shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Prompt-prompted Mixture of Experts for Efficient LLM Generation"). The key takeaway
    is that neurons which produce high relative magnitudes are naturally shared across
    tokens within a sequence, as seen with the distinct vertical streaks. Increasingly
    bizarre, the two models have different architectures and different non-ReLU activation
    functions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 起初让人望而生畏的这些挑战，由于对我们称之为群集的现象的简单观察而变得可以克服，群集是一种在许多LLM中观察到的序列中持续存在的高度一致的稀疏激活现象。当我们关注序列的相对激活幅度而非绝对值时，群集在FF激活（FF下投影的输入）中出现。Llama
    2 7B和Gemma 7B的例子见图[1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 高效LLM生成的提示驱动专家组合")。关键在于，产生高相对幅度的神经元在序列中的各个token之间自然共享，正如明显的垂直条纹所示。更为奇怪的是，这两个模型具有不同的架构和不同的非ReLU激活函数。
- en: '![Refer to caption](img/48f38c689a89b5785de8828a52398190.png)![Refer to caption](img/f9b29e92abd2606d730649d930fb8869.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/48f38c689a89b5785de8828a52398190.png)![参见说明](img/f9b29e92abd2606d730649d930fb8869.png)'
- en: 'Figure 1: Relative FF activation magnitudes of the first 512 features and tokens
    across a sequence from PG-19 [[RPJ^+19](#bib.bibx41), [GBB^+20](#bib.bibx16)]
    in layer 10 of Llama 2 7B (left) and Gemma 7B (right). These heatmaps show flocking,
    where relative activation magnitudes are shared within a sequence. More examples
    in Appendix [B](#A2 "Appendix B More Flocking Examples ‣ Prompt-prompted Mixture
    of Experts for Efficient LLM Generation").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：Llama 2 7B（左）和Gemma 7B（右）第10层中PG-19 [[RPJ^+19](#bib.bibx41), [GBB^+20](#bib.bibx16)]的前512个特征和token的相对FF激活幅度。这些热图显示了群集，其中相对激活幅度在序列中共享。更多例子见附录[B](#A2
    "附录 B 更多群集示例 ‣ 高效LLM生成的提示驱动专家组合")。
- en: 'Unlike existing pruning or MoE methods, we exploit flocking in our design of
    GRIFFIN (Gating by Repetition In Feedforward Intermediate Neurons), a highly performative
    and efficient training-free method to turn an LLM into an MoE. GRIFFIN does this
    by using a sequence’s prompt to determine the experts to activate during generation,
    allowing it to overcome all of the aforementioned challenges:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有的剪枝或MoE方法不同，我们在GRIFFIN（由重复驱动的前馈中间神经元的门控）设计中利用了群集，这是一种高度高效且无需训练的方法，用于将LLM转变为MoE。GRIFFIN通过使用序列的提示来确定生成过程中需要激活的专家，从而克服了上述所有挑战。
- en: '1.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'No Preparation: Our no-cost method is completely training-free and requires
    no preparation. Moreover, the simple implementation of GRIFFIN means it can easily
    be dropped into any FF block.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无需准备：我们的方法完全免费且无需训练，也不需要任何准备。此外，GRIFFIN的简单实现意味着它可以轻松地嵌入到任何FF块中。
- en: '2.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Simple Expert Selection: Since flocking exists throughout a sequence, the prompt
    reveals the most relevant FF neurons for generation with little to no performance
    loss. The selection process is parameter-free and adds negligible overhead.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单的专家选择：由于群集存在于整个序列中，提示揭示了生成时最相关的FF神经元，几乎没有性能损失。选择过程无需参数，并且增加的开销微乎其微。
- en: '3.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Model & Activation Function Diversity: Thorough experimentation demonstrates
    the efficacy of GRIFFIN on numerous models, including Llama 2 [[TMS^+23](#bib.bibx49)],
    Gemma [[TMH^+24](#bib.bibx48)], Mistral [[JSM^+23](#bib.bibx21)], OPT [[ZRG^+22](#bib.bibx59)],
    and ReluLlama [[Tea23](#bib.bibx47)]. Together, the tested activation functions
    include ReLU, SwiGLU, GEGLU, and ReGLU [[Sha20](#bib.bibx42)].'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型与激活函数多样性：彻底的实验表明，GRIFFIN在众多模型上的有效性，包括Llama 2 [[TMS^+23](#bib.bibx49)]、Gemma
    [[TMH^+24](#bib.bibx48)]、Mistral [[JSM^+23](#bib.bibx21)]、OPT [[ZRG^+22](#bib.bibx59)]和ReluLlama
    [[Tea23](#bib.bibx47)]。所测试的激活函数包括ReLU、SwiGLU、GEGLU和ReGLU [[Sha20](#bib.bibx42)]。
- en: In this paper, we show GRIFFIN is a simple and strong method to turn any LLM
    into MoE because of flocking. In the next section (Section [2](#S2 "2 Background
    ‣ Prompt-prompted Mixture of Experts for Efficient LLM Generation")), we discuss
    some strengths and weaknesses of current methods that seek to improve FF efficiency.
    In more detail, we formalize the MoE problem and its motivation in Section [3](#S3
    "3 Problem Formulation ‣ Prompt-prompted Mixture of Experts for Efficient LLM
    Generation") and present our novel approach in Section [4.2](#S4.SS2 "4.2 GRIFFIN
    Algorithm ‣ 4 From Flocking to GRIFFIN ‣ Prompt-prompted Mixture of Experts for
    Efficient LLM Generation"), which requires a thorough examination of the surprising
    phenomenon of flocking shared by many LLMs in Section [4.1](#S4.SS1 "4.1 Observing
    Flocking ‣ 4 From Flocking to GRIFFIN ‣ Prompt-prompted Mixture of Experts for
    Efficient LLM Generation"). Our rigorous experiments demonstrate GRIFFIN preserves
    performance on classification and generation even after removing 50% of FF neurons
    (Section [5.1](#S5.SS1 "5.1 Performance ‣ 5 Experiments ‣ Prompt-prompted Mixture
    of Experts for Efficient LLM Generation")), all while having lower latency (Section [5.2](#S5.SS2
    "5.2 Efficiency ‣ 5 Experiments ‣ Prompt-prompted Mixture of Experts for Efficient
    LLM Generation")). For instance, GRIFFIN reduces the number of active parameters
    in Llama 2 13B from 13B to 8.8B during generation to improve latency by 1.25$\times$
    with almost no loss in performance. Finally, we show our method’s incredible scalability
    and robustness in several ablation studies (Section [5.3](#S5.SS3 "5.3 Ablations
    and Analysis ‣ 5 Experiments ‣ Prompt-prompted Mixture of Experts for Efficient
    LLM Generation")).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们展示了GRIFFIN是一种简单而有效的方法，通过集群将任何LLM转变为MoE。在下一节（第[2节](#S2 "2 背景 ‣ 以提示为驱动的专家混合体以实现高效LLM生成")），我们讨论了当前改进FF效率的方法的一些优缺点。更详细地说，我们在第[3节](#S3
    "3 问题定义 ‣ 以提示为驱动的专家混合体以实现高效LLM生成")中形式化了MoE问题及其动机，并在第[4.2节](#S4.SS2 "4.2 GRIFFIN算法
    ‣ 4 从集群到GRIFFIN ‣ 以提示为驱动的专家混合体以实现高效LLM生成")中提出了我们的新方法，这需要对第[4.1节](#S4.SS1 "4.1
    观察集群 ‣ 4 从集群到GRIFFIN ‣ 以提示为驱动的专家混合体以实现高效LLM生成")中许多LLMs共享的集群现象进行彻底的研究。我们的严格实验表明，GRIFFIN即使在移除50%的FF神经元后，仍能保持分类和生成的性能（第[5.1节](#S5.SS1
    "5.1 性能 ‣ 5 实验 ‣ 以提示为驱动的专家混合体以实现高效LLM生成")），同时具有更低的延迟（第[5.2节](#S5.SS2 "5.2 效率 ‣
    5 实验 ‣ 以提示为驱动的专家混合体以实现高效LLM生成")）。例如，GRIFFIN将Llama 2 13B生成时的活动参数数量从13B减少到8.8B，将延迟提高了1.25$\times$，几乎没有性能损失。最后，我们在几项消融研究中展示了我们方法的惊人可扩展性和鲁棒性（第[5.3节](#S5.SS3
    "5.3 消融和分析 ‣ 5 实验 ‣ 以提示为驱动的专家混合体以实现高效LLM生成")）。
- en: 2 Background
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: Our novel method and FF activation observations are inspired and motivated by
    ample amounts of previous research that sought to characterize FF sparsity and
    accelerate LLMs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新方法和FF激活观察受到大量先前研究的启发和推动，这些研究旨在表征FF稀疏性并加速LLMs。
- en: Feedforward Activation Sparsity.
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 前馈激活稀疏性。
- en: The observation that transformer FF blocks produce sparse activations is not
    new [[GSBL20](#bib.bibx17), [DLBZ22](#bib.bibx11), [LYB^+22](#bib.bibx29), [DCC23](#bib.bibx8),
    [LWD^+23](#bib.bibx27)]. In ReLU-based LLMs like OPT [[ZRG^+22](#bib.bibx59)],
    the activations can be exceptionally sparse and become more apparent for larger
    models [[LWD^+23](#bib.bibx27)]. As more models use non-sparse activation functions
    like GLU variants [[Sha20](#bib.bibx42)], it is difficult for neurons to have
    no contribution to the output since these functions do not have an interval that
    maps to zero. Without exact sparsity, the efficacy of these methods becomes limited.
    As such, this has ushered a wave of models that are either adapted from available
    models [[ZLL^+21](#bib.bibx58), [LLLC21](#bib.bibx26), [MAM^+23](#bib.bibx32),
    [ZBC^+24](#bib.bibx56), [JSR^+24](#bib.bibx22)] or trained from scratch [[FZS22](#bib.bibx15)]
    which can produce activations that are exactly zero with little to no performance
    loss. Even so, these methods require considerable amounts of computational resources.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到变压器 FF 块产生稀疏激活并不新鲜[[GSBL20](#bib.bibx17), [DLBZ22](#bib.bibx11), [LYB^+22](#bib.bibx29),
    [DCC23](#bib.bibx8), [LWD^+23](#bib.bibx27)]。在如 OPT [[ZRG^+22](#bib.bibx59)] 等基于
    ReLU 的大型语言模型中，激活可以非常稀疏，并且在更大的模型中变得更加明显[[LWD^+23](#bib.bibx27)]。随着更多模型使用如 GLU 变体
    [[Sha20](#bib.bibx42)] 这样的非稀疏激活函数，由于这些函数没有映射到零的区间，神经元很难对输出没有贡献。没有精确的稀疏性，这些方法的效果变得有限。因此，这引发了一波模型，它们要么是从现有模型
    [[ZLL^+21](#bib.bibx58), [LLLC21](#bib.bibx26), [MAM^+23](#bib.bibx32), [ZBC^+24](#bib.bibx56),
    [JSR^+24](#bib.bibx22)] 适配而来，要么是从头训练 [[FZS22](#bib.bibx15)]，可以产生精确为零的激活而几乎没有性能损失。即便如此，这些方法仍需大量计算资源。
- en: Pruning.
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 剪枝。
- en: Pruning [[LDS89](#bib.bibx24)] is another sparsity-guided way to tackle compute
    and memory bottlenecks of models. Previously, the common method would be some
    variation of iteratively rounding weights down to zero based on some score and
    retraining to recover any lost performance [[FC18](#bib.bibx13), [BGOFG20](#bib.bibx3),
    [LGW^+21](#bib.bibx25), [LZH^+24](#bib.bibx31)]. While this can result in most
    parameters being pruned, this method comes with a few issues. First, with the
    increasing scale of LLMs, retraining becomes impractical for most. Fortunately,
    cheap methods to effectively prune LLMs have been developed [[FA23](#bib.bibx12),
    [SLBK23](#bib.bibx43), [JLC^+23](#bib.bibx20), [DKK^+24](#bib.bibx9)]. The second
    issue is that unless pruning is done in a structured manner [[XZC22](#bib.bibx53),
    [SWSL23](#bib.bibx45), [MFW23](#bib.bibx33), [LYZ^+23](#bib.bibx30), [XGZC23](#bib.bibx52)],
    it is difficult to see real computational savings, yet structured pruning often
    leads to much more severe performance degradation. Third, pruning enforces sparsity
    to be static which can be strong assumption since FF blocks are widely believed
    to contain the model’s memory [[GSBL20](#bib.bibx17)].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝 [[LDS89](#bib.bibx24)] 是另一种以稀疏性为指导的解决模型计算和内存瓶颈的方法。以往常见的方法是基于某些评分将权重逐步减小至零并重新训练以恢复丢失的性能
    [[FC18](#bib.bibx13), [BGOFG20](#bib.bibx3), [LGW^+21](#bib.bibx25), [LZH^+24](#bib.bibx31)]。虽然这可以导致大部分参数被剪枝，但该方法存在一些问题。首先，随着大型语言模型规模的增加，对于大多数人来说，重新训练变得不切实际。幸运的是，已经开发出有效地剪枝大型语言模型的廉价方法
    [[FA23](#bib.bibx12), [SLBK23](#bib.bibx43), [JLC^+23](#bib.bibx20), [DKK^+24](#bib.bibx9)]。第二个问题是，除非剪枝以结构化方式进行
    [[XZC22](#bib.bibx53), [SWSL23](#bib.bibx45), [MFW23](#bib.bibx33), [LYZ^+23](#bib.bibx30),
    [XGZC23](#bib.bibx52)]，否则很难看到实际的计算节省，但结构化剪枝往往会导致性能下降更为严重。第三，剪枝使稀疏性保持静态，这可能是一个强假设，因为
    FF 块被广泛认为包含模型的记忆 [[GSBL20](#bib.bibx17)]。
- en: Mixture of Experts.
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 专家混合。
- en: 'Making sparsity more dynamic has motivated the design of mixture-of-experts
    (MoEs) [[JJNH91](#bib.bibx19)] to avoid computing low-impact features in trained
    models at varying granularities [[ZLL^+21](#bib.bibx58), [LWD^+23](#bib.bibx27),
    [PSBW23](#bib.bibx38), [CIS23](#bib.bibx6), [AMB^+23](#bib.bibx1), [YYB^+24](#bib.bibx55),
    [ZBC^+24](#bib.bibx56)]. The main idea of MoEs is to use a gating function to
    identify a small subset of neurons that will be used to compute the layer’s output,
    an adaptive form of pruning. In the ideal case, all active neurons are selected
    and inactive neurons are ignored for each input. However, current methods either
    require training or rely on ReLU activation functions. Our method has the best
    of both worlds: it is training-free and effective on a variety of activation functions.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使稀疏性更加动态的动机促使了混合专家（MoEs）的设计[[JJNH91](#bib.bibx19)]，以避免在不同粒度下计算训练模型中的低影响特征[[ZLL^+21](#bib.bibx58)，[LWD^+23](#bib.bibx27)，[PSBW23](#bib.bibx38)，[CIS23](#bib.bibx6)，[AMB^+23](#bib.bibx1)，[YYB^+24](#bib.bibx55)，[ZBC^+24](#bib.bibx56)]。MoEs的主要思想是使用门控函数来识别一个小的神经元子集，该子集将用于计算层的输出，这是一种自适应的剪枝形式。在理想情况下，所有活跃神经元都会被选择，而每个输入的非活动神经元则会被忽略。然而，目前的方法要么需要训练，要么依赖于ReLU激活函数。我们的方法兼具两者的优点：它无需训练，且在多种激活函数上都有效。
- en: 3 Problem Formulation
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 问题表述
- en: 'This section contains an overview of different components of the FF block followed
    by a more detailed introduction of the MoE problem which our method aims to tackle.
    Since FF blocks operate identically and independently for each token unlike attention,
    we begin with defining the FF block with a single column vector input $\bm{x}\in\mathbb{R}^{D}$:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含FF块不同组件的概述，随后是对我们方法旨在解决的MoE问题的更详细介绍。由于FF块对每个令牌的操作与注意力机制不同，都是相同且独立的，我们首先定义具有单列向量输入$\bm{x}\in\mathbb{R}^{D}$的FF块：
- en: '|  | $\displaystyle\text{FF}(\bm{x})=\text{FF}_{2}(\underbrace{\text{FF}_{1}(\bm{x})}_{\bm{z}})$
    |  | (1) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{FF}(\bm{x})=\text{FF}_{2}(\underbrace{\text{FF}_{1}(\bm{x})}_{\bm{z}})$
    |  | (1) |'
- en: where $\text{FF}_{2}(\bm{z})=\bm{W}_{2}\bm{z}+\bm{b}_{2}$. For instance, in
    OPT,
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\text{FF}_{2}(\bm{z})=\bm{W}_{2}\bm{z}+\bm{b}_{2}$。例如，在OPT中，
- en: '|  | $\displaystyle\text{FF}_{1}(\bm{x})$ |  | (2) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{FF}_{1}(\bm{x})$ |  | (2) |'
- en: For FF blocks with GLU variants such as in Llama 2 and Gemma,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有GLU变体的FF块，如在Llama 2和Gemma中，
- en: '|  | $\displaystyle\text{FF}_{1}(\bm{x})$ |  | (3) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{FF}_{1}(\bm{x})$ |  | (3) |'
- en: where $\odot$ such that when the FF block is reparameterized with these matrices,
    the output value is preserved. In other words, for
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\odot$，当FF块用这些矩阵重新参数化时，输出值被保留。换句话说，对于
- en: '|  | $\displaystyle\widehat{\bm{z}}=\widehat{\text{FF}}_{1}(\bm{x})$ |  | (4)
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{\bm{z}}=\widehat{\text{FF}}_{1}(\bm{x})$ |  | (4)
    |'
- en: '|  | $\displaystyle\widehat{\text{FF}}_{2}(\widehat{\bm{z}})$ |  | (5) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{\text{FF}}_{2}(\widehat{\bm{z}})$ |  | (5) |'
- en: $\text{FF}(\bm{x})\approx\widehat{\text{FF}}_{2}(\widehat{\text{FF}}_{1}(\bm{x}))$).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{FF}(\bm{x})\approx\widehat{\text{FF}}_{2}(\widehat{\text{FF}}_{1}(\bm{x}))$）。
- en: 4 From Flocking to GRIFFIN
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 从集群到GRIFFIN
- en: Here, we take deeper dive into the phenomenon of flocking and describe the intuitive
    algorithm of GRIFFIN which is directly inspired by it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们深入探讨了集群现象，并描述了直接受到其启发的GRIFFIN直观算法。
- en: '![Refer to caption](img/b6a8754eddae6b07611fe6a051a20176.png)![Refer to caption](img/d3950eebdcfb569f1e0f237030640c8a.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b6a8754eddae6b07611fe6a051a20176.png)![参见标题](img/d3950eebdcfb569f1e0f237030640c8a.png)'
- en: 'Figure 2: Average Jaccard similarity between WikiText samples’ top FF neuron
    activations in Llama 2 7B (left) and Gemma 7B (right). Higher values indicate
    greater similarity.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Llama 2 7B（左）和Gemma 7B（右）中WikiText样本的顶级FF神经元激活的平均Jaccard相似度。较高的值表示更大的相似性。
- en: 4.1 Observing Flocking
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 观察集群现象
- en: Flocking arises when we look at the relative impact of each neuron per token
    within a sequence. To see this, we normalize rows of $\bm{Z}$), the relative activations.
    We show example relative activation magnitudes for a sequence in Llama 2 7B and
    Gemma 7B in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Prompt-prompted Mixture
    of Experts for Efficient LLM Generation"). Since there are distinct vertical streaks,
    this intriguingly implies that activations that have relatively greater weight
    are common across all tokens in a sequence. Notably, Llama 2 7B and Gemma 7B use
    SwiGLU and GEGLU activations, respectively, along with other major architecture
    differences. We call this phenomenon flocking, like highly organized groups of
    birds, and we observe this in virtually all FF layers (see Appendix [B](#A2 "Appendix
    B More Flocking Examples ‣ Prompt-prompted Mixture of Experts for Efficient LLM
    Generation")).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 群体行为的出现源于我们观察到每个神经元在序列中的每个标记的相对影响。为了观察这一点，我们对$\bm{Z}$）的行进行标准化，即相对激活值。我们在图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Prompt-prompted Mixture of Experts for Efficient
    LLM Generation")中展示了Llama 2 7B和Gemma 7B中一个序列的示例相对激活幅度。由于存在明显的垂直条纹，这令人感兴趣地暗示，相对较大的激活权重在序列中的所有标记中是常见的。值得注意的是，Llama
    2 7B和Gemma 7B分别使用SwiGLU和GEGLU激活函数，此外还有其他主要的架构差异。我们将这种现象称为群体行为，就像高度组织的鸟群一样，我们在几乎所有的FF层中都观察到这种现象（参见附录[B](#A2
    "Appendix B More Flocking Examples ‣ Prompt-prompted Mixture of Experts for Efficient
    LLM Generation")）。
- en: While relative activations magnitudes are shared within a sequence, they are
    not generally shared between sequences. We show this by taking the $\ell_{2}$
    sets. From Figure [2](#S4.F2 "Figure 2 ‣ 4 From Flocking to GRIFFIN ‣ Prompt-prompted
    Mixture of Experts for Efficient LLM Generation") where we aggregate Jaccard similarities
    across WikiText [[MXBS16](#bib.bibx34)] samples, we observe a lack of inter-sample
    activation similarities for the vast majority of layers in Llama 2 7B and Gemma
    7B, unless the sets of selected neurons are large. This lack of consistency implies
    pruning entire FF neurons without retraining would be less effective than a more
    adaptive method.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然相对激活幅度在序列内是共享的，但它们在序列之间通常是不共享的。我们通过取$\ell_{2}$集来展示这一点。从图[2](#S4.F2 "Figure
    2 ‣ 4 From Flocking to GRIFFIN ‣ Prompt-prompted Mixture of Experts for Efficient
    LLM Generation")中，我们聚合了WikiText [[MXBS16](#bib.bibx34)]样本的Jaccard相似度，观察到在Llama
    2 7B和Gemma 7B的大多数层中，样本间的激活相似性缺乏，除非选择的神经元集合很大。这种不一致性暗示，完全修剪FF神经元而不重新训练将比更具适应性的方法效果差。
- en: 4.2 GRIFFIN Algorithm
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 GRIFFIN 算法
- en: '![Refer to caption](img/20caf01d5e982dc74ec63a6e6d8ebb15.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/20caf01d5e982dc74ec63a6e6d8ebb15.png)'
- en: 'Figure 3: GRIFFIN overview. Relative activations from the prompt determine
    expert neurons to use for generation.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：GRIFFIN概述。来自提示的相对激活值决定用于生成的专家神经元。
- en: 'Using our insight on flocking, we introduce GRIFFIN as a simple general purpose
    and training-free MoE method for efficient generation, captured in Figure [3](#S4.F3
    "Figure 3 ‣ 4.2 GRIFFIN Algorithm ‣ 4 From Flocking to GRIFFIN ‣ Prompt-prompted
    Mixture of Experts for Efficient LLM Generation"). In a nutshell, we select experts
    during the prompt phase of each sample which are then used for the entire duration
    of the generation phase. This effective approach is based on a key observation
    on flocking: since tokens within a sequence share activation patterns, the prompt
    and generated tokens will also share activation patterns.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 利用我们对群体行为的洞察，我们介绍了GRIFFIN，作为一种简单的通用且无需训练的MoE方法，用于高效生成，如图[3](#S4.F3 "Figure 3
    ‣ 4.2 GRIFFIN Algorithm ‣ 4 From Flocking to GRIFFIN ‣ Prompt-prompted Mixture
    of Experts for Efficient LLM Generation")所示。简而言之，我们在每个样本的提示阶段选择专家，然后在生成阶段的整个过程中使用这些专家。这种有效的方法基于对群体行为的关键观察：由于序列中的标记共享激活模式，因此提示和生成的标记也将共享激活模式。
- en: Prompt Phase Expert Selection.
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示阶段专家选择。
- en: 'Our experts or neurons are chosen at the sequence level, so we need to consider
    the dynamics of the entire input sequence rather than just a single token when
    choosing our experts. To select expert neurons, we need a statistic $\bm{s}\in\mathbb{R}^{D_{\text{FF}}}$
    along the token axis:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的专家或神经元是在序列级别上选择的，因此在选择专家时我们需要考虑整个输入序列的动态，而不仅仅是单个标记。为了选择专家神经元，我们需要沿标记轴的统计量$\bm{s}\in\mathbb{R}^{D_{\text{FF}}}$：
- en: '|  | $\displaystyle\bm{s}$ |  | (6) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{s}$ |  | (6) |'
- en: Taking the indices of the top-$k$ highlights neurons consistently activated
    at relatively high intensities.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 选择前$k$的索引突出显示了在相对较高强度下始终激活的神经元。
- en: Generation with Experts.
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 专家生成。
- en: When generating tokens, we directly use the pruned layers which contain the
    expert neurons, $\widehat{\text{FF}}_{1}$ for all future tokens. In Llama 2 13B
    and Gemma 7B, this reduces the active number of parameters from 13B to 8.8B and
    from 8.5B to 5.4B, respectively, during generation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 生成令牌时，我们直接使用包含专家神经元的剪枝层，$\widehat{\text{FF}}_{1}$用于所有未来的令牌。在Llama 2 13B和Gemma
    7B中，这将活动参数的数量从13B减少到8.8B，从8.5B减少到5.4B。
- en: 5 Experiments
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个实验
- en: We showcase the superb performance of GRIFFIN on numerous tasks and models (Section [5.1](#S5.SS1
    "5.1 Performance ‣ 5 Experiments ‣ Prompt-prompted Mixture of Experts for Efficient
    LLM Generation")) while achieving lower latency (Section [5.2](#S5.SS2 "5.2 Efficiency
    ‣ 5 Experiments ‣ Prompt-prompted Mixture of Experts for Efficient LLM Generation")),
    along with a study on several of its properties like sampling experts, sequence
    length scaling, and random inputs (Section [5.3](#S5.SS3 "5.3 Ablations and Analysis
    ‣ 5 Experiments ‣ Prompt-prompted Mixture of Experts for Efficient LLM Generation")).
    All experiments are run on NVIDIA L40 GPUs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了**GRIFFIN**在众多任务和模型上的卓越表现（第[5.1节](#S5.SS1 "5.1 Performance ‣ 5 Experiments
    ‣ Prompt-prompted Mixture of Experts for Efficient LLM Generation")），同时实现了更低的延迟（第[5.2节](#S5.SS2
    "5.2 Efficiency ‣ 5 Experiments ‣ Prompt-prompted Mixture of Experts for Efficient
    LLM Generation")），并对其多个属性如采样专家、序列长度缩放和随机输入进行了研究（第[5.3节](#S5.SS3 "5.3 Ablations
    and Analysis ‣ 5 Experiments ‣ Prompt-prompted Mixture of Experts for Efficient
    LLM Generation")）。所有实验都在NVIDIA L40 GPU上运行。
- en: 5.1 Performance
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 性能
- en: 'Table 1: Classification tasks (0-shot unnormalized accuracy) at 50% FF sparsity.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 在50% FF稀疏度下的分类任务（0-shot未经标准化准确率）。'
- en: '| Model | HellaSwag | PIQA | COPA | ARC-e | ARC-c | BoolQ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | HellaSwag | PIQA | COPA | ARC-e | ARC-c | BoolQ |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Llama 2 7B | 57.16 | 78.07 | 87.00 | 76.35 | 43.34 | 77.71 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 7B | 57.16 | 78.07 | 87.00 | 76.35 | 43.34 | 77.71 |'
- en: '| Magnitude | 57.12 | 77.31 | 84.00 | 70.33 | 40.27 | 66.54 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 57.12 | 77.31 | 84.00 | 70.33 | 40.27 | 66.54 |'
- en: '| GRIFFIN | 57.11 | 77.69 | 86.00 | 74.54 | 42.75 | 73.15 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| GRIFFIN | 57.11 | 77.69 | 86.00 | 74.54 | 42.75 | 73.15 |'
- en: '| Llama 2 13B | 60.06 | 79.05 | 90.00 | 79.46 | 48.46 | 80.61 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 13B | 60.06 | 79.05 | 90.00 | 79.46 | 48.46 | 80.61 |'
- en: '| Magnitude | 60.00 | 79.00 | 88.00 | 74.07 | 46.25 | 70.52 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 60.00 | 79.00 | 88.00 | 74.07 | 46.25 | 70.52 |'
- en: '| GRIFFIN | 60.10 | 79.11 | 89.00 | 77.19 | 46.84 | 78.50 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| GRIFFIN | 60.10 | 79.11 | 89.00 | 77.19 | 46.84 | 78.50 |'
- en: '| Gemma 7B | 60.61 | 80.30 | 88.00 | 82.74 | 50.09 | 83.49 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Gemma 7B | 60.61 | 80.30 | 88.00 | 82.74 | 50.09 | 83.49 |'
- en: '| Magnitude | 46.24 | 73.12 | 57.00 | 45.20 | 32.76 | 62.84 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 46.24 | 73.12 | 57.00 | 45.20 | 32.76 | 62.84 |'
- en: '| GRIFFIN | 60.62 | 79.98 | 88.00 | 81.65 | 50.09 | 81.90 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| GRIFFIN | 60.62 | 79.98 | 88.00 | 81.65 | 50.09 | 81.90 |'
- en: '| Mistral 7B | 61.21 | 80.58 | 92.00 | 80.89 | 50.43 | 83.61 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Mistral 7B | 61.21 | 80.58 | 92.00 | 80.89 | 50.43 | 83.61 |'
- en: '| Magnitude | 61.15 | 80.36 | 86.00 | 74.20 | 48.89 | 60.40 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 61.15 | 80.36 | 86.00 | 74.20 | 48.89 | 60.40 |'
- en: '| GRIFFIN | 61.18 | 80.52 | 91.00 | 79.25 | 50.00 | 80.06 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| GRIFFIN | 61.18 | 80.52 | 91.00 | 79.25 | 50.00 | 80.06 |'
- en: '| OPT 6.7B | 50.48 | 76.28 | 81.00 | 65.53 | 30.55 | 66.12 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| OPT 6.7B | 50.48 | 76.28 | 81.00 | 65.53 | 30.55 | 66.12 |'
- en: '| Magnitude | 49.21 | 72.63 | 79.00 | 47.60 | 27.13 | 40.15 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 49.21 | 72.63 | 79.00 | 47.60 | 27.13 | 40.15 |'
- en: '| GRIFFIN | 50.44 | 75.63 | 80.00 | 63.93 | 30.55 | 65.44 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| GRIFFIN | 50.44 | 75.63 | 80.00 | 63.93 | 30.55 | 65.44 |'
- en: '| OPT 13B | 52.46 | 75.90 | 86.00 | 67.05 | 32.94 | 65.81 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| OPT 13B | 52.46 | 75.90 | 86.00 | 67.05 | 32.94 | 65.81 |'
- en: '| Magnitude | 51.31 | 74.21 | 81.00 | 49.41 | 28.07 | 38.75 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 51.31 | 74.21 | 81.00 | 49.41 | 28.07 | 38.75 |'
- en: '| GRIFFIN | 52.42 | 76.17 | 86.00 | 66.92 | 33.19 | 67.65 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| GRIFFIN | 52.42 | 76.17 | 86.00 | 66.92 | 33.19 | 67.65 |'
- en: Using various models, we evaluate on several generation and classification tasks.
    For generation, we evaluate on XSum [[NCL18](#bib.bibx36)], CNN/DailyMail [[NZG^+16](#bib.bibx37)],
    COQA [[RCM19](#bib.bibx40)], and SCROLLS QASPER [[DLB^+21](#bib.bibx10), [SSI^+22](#bib.bibx44)].
    For classification, we evaluate on HellaSwag [[ZHB^+19](#bib.bibx57)], PIQA [[BZB^+20](#bib.bibx4)],
    COPA [[RBG11](#bib.bibx39)], ARC-Easy/Challenge [[CCE^+18](#bib.bibx5)], and BoolQ
    [[CLC^+19](#bib.bibx7)]. With the exception of XSum and CNN/DailyMail, we use
    LM Evaluation Harness for our experiments [[GTA^+23](#bib.bibx18)]. Aside from
    comparing with the original LLM, we also compare GRIFFIN with a static sequence-level
    MoE based on neuron magnitudes. Similar to neuron magnitude pruning, this baseline
    selects experts based on neuron magnitudes in $\bm{W}_{1}$ are elementwise multiplied
    to produce the pruning metric. As we will see, this straightforward baseline achieves
    great classification results but falters for generation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用各种模型，我们在多个生成和分类任务上进行了评估。对于生成任务，我们评估了 XSum [[NCL18](#bib.bibx36)]、CNN/DailyMail
    [[NZG^+16](#bib.bibx37)]、COQA [[RCM19](#bib.bibx40)] 和 SCROLLS QASPER [[DLB^+21](#bib.bibx10)、[SSI^+22](#bib.bibx44)]。对于分类任务，我们评估了
    HellaSwag [[ZHB^+19](#bib.bibx57)]、PIQA [[BZB^+20](#bib.bibx4)]、COPA [[RBG11](#bib.bibx39)]、ARC-Easy/Challenge
    [[CCE^+18](#bib.bibx5)] 和 BoolQ [[CLC^+19](#bib.bibx7)]。除了 XSum 和 CNN/DailyMail，我们还使用了
    LM Evaluation Harness 进行实验 [[GTA^+23](#bib.bibx18)]。除了与原始 LLM 进行比较外，我们还将 GRIFFIN
    与基于神经元幅度的静态序列级 MoE 进行比较。类似于神经元幅度剪枝，这一基线根据神经元幅度在 $\bm{W}_{1}$ 中逐元素相乘以生成剪枝指标。正如我们将看到的，这一简单基线在分类任务中取得了很好的结果，但在生成任务中却表现不佳。
- en: As our method is designed specifically for generation, we alter classification
    evaluations to simulate generation. In typical classification tasks, LLMs do not
    enter the generative phase since the final token output of the prompt phase indicates
    the class. Consequently, directly applying GRIFFIN for classification tasks trivially
    yields the exact performance of the original model. Therefore, we treat all tokens
    but the final input token as the prompt. Then, the model is forced to go into
    the generation phase for one step to produce the class.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的方法专门为生成任务设计，我们将分类评估调整为模拟生成任务。在典型的分类任务中，LLMs 不进入生成阶段，因为提示阶段的最终标记输出指示了类别。因此，直接将
    GRIFFIN 应用于分类任务会简单地产生原始模型的准确性能。因此，我们将所有标记（但最后一个输入标记除外）视为提示。然后，模型被迫进入生成阶段进行一步操作以生成类别。
- en: We start with a look into the relationship between the sparsity levels and performance
    degradation. This translates to varying $k$. To compare the performance degradation
    across multiple tasks, we plot the ratio of the final performance metrics between
    GRIFFIN and the full model in Figure [4](#S5.F4 "Figure 4 ‣ 5.1 Performance ‣
    5 Experiments ‣ Prompt-prompted Mixture of Experts for Efficient LLM Generation").
    We see most of the performance is preserved at 50% FF sparsity in Llama 2 7B,
    Gemma 7B, and Mistral 7B. Different tasks have different tipping points where
    performance sharply drops, which may be related to the difficulty of the task
    [[YJL^+24](#bib.bibx54)].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先探讨稀疏水平与性能下降之间的关系。这转化为不同的 $k$ 值。为了比较多个任务的性能下降，我们在图 [4](#S5.F4 "Figure 4 ‣
    5.1 Performance ‣ 5 Experiments ‣ Prompt-prompted Mixture of Experts for Efficient
    LLM Generation") 中绘制了 GRIFFIN 和完整模型的最终性能指标的比率。我们看到，在 Llama 2 7B、Gemma 7B 和 Mistral
    7B 中，大部分性能在 50% FF 稀疏率下得以保留。不同任务有不同的临界点，性能急剧下降，这可能与任务的难度有关 [[YJL^+24](#bib.bibx54)]。
- en: 'Table 2: Generation tasks XSum (1-shot), CNN/DailyMail (1-shot), CoQA (0-shot),
    SCROLLS QASPER (0-shot) at 50% FF sparsity. Magnitude neuron pruning fails in
    almost every case while GRIFFIN effectively preserves performance.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 50% FF 稀疏率下的生成任务 XSum (1-shot)、CNN/DailyMail (1-shot)、CoQA (0-shot)、SCROLLS
    QASPER (0-shot)。幅度神经元剪枝几乎在所有情况下都失败，而 GRIFFIN 有效地保持了性能。
- en: '| Model | XSum | CNN/DailyMail | CoQA | QASPER |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | XSum | CNN/DailyMail | CoQA | QASPER |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | (Rouge-1/2/L) | (Rouge-1/2/L) | (F1/EM) | (F1) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | (Rouge-1/2/L) | (Rouge-1/2/L) | (F1/EM) | (F1) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Llama 2 7B | 27.15/9.06/22.62 | 10.08/0.13/9.55 | 77.35/63.88 | 26.31 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 7B | 27.15/9.06/22.62 | 10.08/0.13/9.55 | 77.35/63.88 | 26.31 |'
- en: '| Magnitude | 9.71/1.31/8.59 | 9.66/0.63/9.32 | 56.59/39.93 | 12.93 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 规模 | 9.71/1.31/8.59 | 9.66/0.63/9.32 | 56.59/39.93 | 12.93 |'
- en: '| GRIFFIN | 24.75/7.41/20.55 | 10.97/0.66/10.37 | 77.18/63.58 | 25.76 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| GRIFFIN | 24.75/7.41/20.55 | 10.97/0.66/10.37 | 77.18/63.58 | 25.76 |'
- en: '| Llama 2 13B | 26.90/9.45/22.09 | 2.51/0.22/2.34 | 79.18/66.37 | 28.32 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 13B | 26.90/9.45/22.09 | 2.51/0.22/2.34 | 79.18/66.37 | 28.32 |'
- en: '| Magnitude | 5.72/0.78/5.06 | 0.02/0.00/0.02 | 65.69/47.87 | 15.55 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 规模 | 5.72/0.78/5.06 | 0.02/0.00/0.02 | 65.69/47.87 | 15.55 |'
- en: '| GRIFFIN | 25.69/7.85/20.89 | 3.31/0.78/3.07 | 79.22/66.62 | 27.91 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| GRIFFIN | 25.69/7.85/20.89 | 3.31/0.78/3.07 | 79.22/66.62 | 27.91 |'
- en: '| Gemma 7B | 26.86/9.15/22.03 | 17.45/4.15/15.94 | 79.04/65.25 | 30.78 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Gemma 7B | 26.86/9.15/22.03 | 17.45/4.15/15.94 | 79.04/65.25 | 30.78 |'
- en: '| Magnitude | 1.49/0.01/1.47 | 0.00/0.00/0.00 | 2.92/1.50 | 7.02 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | 1.49/0.01/1.47 | 0.00/0.00/0.00 | 2.92/1.50 | 7.02 |'
- en: '| GRIFFIN | 25.86/7.81/20.93 | 18.26/4.75/16.58 | 78.52/64.62 | 27.37 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GRIFFIN | 25.86/7.81/20.93 | 18.26/4.75/16.58 | 78.52/64.62 | 27.37 |'
- en: '| Mistral 7B | 28.67/10.21/23.64 | 0.28/0.01/0.28 | 80.70/67.30 | 24.56 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Mistral 7B | 28.67/10.21/23.64 | 0.28/0.01/0.28 | 80.70/67.30 | 24.56 |'
- en: '| Magnitude | 3.58/0.27/3.31 | 0.26/0.03/0.26 | 61.99/45.93 | 17.18 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | 3.58/0.27/3.31 | 0.26/0.03/0.26 | 61.99/45.93 | 17.18 |'
- en: '| GRIFFIN | 26.59/8.70/22.17 | 1.26/0.21/1.17 | 80.15/66.50 | 23.92 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| GRIFFIN | 26.59/8.70/22.17 | 1.26/0.21/1.17 | 80.15/66.50 | 23.92 |'
- en: '| OPT 6.7B | 23.60/7.04/19.46 | 13.85/1.54/13.04 | 68.70/54.98 | 18.53 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| OPT 6.7B | 23.60/7.04/19.46 | 13.85/1.54/13.04 | 68.70/54.98 | 18.53 |'
- en: '| Magnitude | 1.63/0.00/1.54 | 1.20/0.00/1.17 | 31.53/16.52 | 7.28 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | 1.63/0.00/1.54 | 1.20/0.00/1.17 | 31.53/16.52 | 7.28 |'
- en: '| GRIFFIN | 21.17/5.42/17.58 | 13.01/1.06/12.26 | 68.99/55.00 | 17.40 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| GRIFFIN | 21.17/5.42/17.58 | 13.01/1.06/12.26 | 68.99/55.00 | 17.40 |'
- en: '| OPT 13B | 25.14/7.93/20.80 | 13.22/1.18/12.46 | 69.51/55.67 | 20.58 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| OPT 13B | 25.14/7.93/20.80 | 13.22/1.18/12.46 | 69.51/55.67 | 20.58 |'
- en: '| Magnitude | 1.23/0.00/1.21 | 1.29/0.00/1.29 | 39.38/27.07 | 8.87 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | 1.23/0.00/1.21 | 1.29/0.00/1.29 | 39.38/27.07 | 8.87 |'
- en: '| GRIFFIN | 22.11/6.28/18.29 | 12.92/1.13/12.20 | 69.07/54.83 | 20.16 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| GRIFFIN | 22.11/6.28/18.29 | 12.92/1.13/12.20 | 69.07/54.83 | 20.16 |'
- en: '| ReluLlama 2 7B | 25.10/7.81/20.76 | 20.95/6.79/19.24 | 78.49/66.73 | 23.31
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| ReluLlama 2 7B | 25.10/7.81/20.76 | 20.95/6.79/19.24 | 78.49/66.73 | 23.31
    |'
- en: '| Magnitude | 9.09/0.22/8.20 | 8.50/0.14/8.17 | 19.43/6.48 | 7.21 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | 9.09/0.22/8.20 | 8.50/0.14/8.17 | 19.43/6.48 | 7.21 |'
- en: '| GRIFFIN | 21.83/5.88/18.09 | 16.85/4.96/14.69 | 78.35/67.10 | 22.29 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| GRIFFIN | 21.83/5.88/18.09 | 16.85/4.96/14.69 | 78.35/67.10 | 22.29 |'
- en: '![Refer to caption](img/640683068020076f981f79086f6f75da.png)![Refer to caption](img/61163a6292fd02733938dcf6f0a652f5.png)![Refer
    to caption](img/5084c5af7deb8924c0ead2741783a7da.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/640683068020076f981f79086f6f75da.png)![参见标题](img/61163a6292fd02733938dcf6f0a652f5.png)![参见标题](img/5084c5af7deb8924c0ead2741783a7da.png)'
- en: 'Figure 4: Relative performance of GRIFFIN for Llama 2 7B (left), Gemma 7B (center),
    and Mistral 7B (right) as we enforce varying degrees of sparsity per FF block.
    For all tasks, the original model’s performance for each task is normalized to
    1.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：GRIFFIN 对 Llama 2 7B（左）、Gemma 7B（中）和 Mistral 7B（右）的相对性能，我们对每个 FF 块强制施加不同程度的稀疏性。对于所有任务，原始模型在每个任务上的性能都被标准化为
    1。
- en: Fixing FF sparsity to be 50%, we evaluate on more tasks and models. Table [1](#S5.T1
    "Table 1 ‣ 5.1 Performance ‣ 5 Experiments ‣ Prompt-prompted Mixture of Experts
    for Efficient LLM Generation") and Table [2](#S5.T2 "Table 2 ‣ 5.1 Performance
    ‣ 5 Experiments ‣ Prompt-prompted Mixture of Experts for Efficient LLM Generation")
    show the performance of GRIFFIN on classification and generation, respectively.
    We see that magnitude neuron pruning achieves reasonable results for classification
    but completely annihilates the original performance in most generation settings.
    In contrast, GRIFFIN achieves not only better performance than the baseline in
    most scenarios, but also preserves most of or matches the original performance
    on all tasks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 将 FF 稀疏性固定为 50%，我们在更多任务和模型上进行了评估。表 [1](#S5.T1 "Table 1 ‣ 5.1 Performance ‣ 5
    Experiments ‣ Prompt-prompted Mixture of Experts for Efficient LLM Generation")
    和表 [2](#S5.T2 "Table 2 ‣ 5.1 Performance ‣ 5 Experiments ‣ Prompt-prompted Mixture
    of Experts for Efficient LLM Generation") 分别展示了 GRIFFIN 在分类和生成任务上的表现。我们发现，幅度神经元修剪在分类任务上取得了合理的结果，但在大多数生成任务中完全消灭了原始性能。相比之下，GRIFFIN
    在大多数场景中不仅表现优于基线，而且在所有任务上保留了大部分或匹配了原始性能。
- en: 5.2 Efficiency
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 效率
- en: We now present efficiency metrics of GRIFFIN. We collect synthetic datasets
    with samples having identical lengths and average results across samples. Like
    many other MoE methods, GRIFFIN is ideal for single sample inputs, such as in
    the case of personal devices, so we use batch size 1 for these experiments. We
    plan to extend our method to larger batch sizes in future work. Using Hugging
    Face implementations of Llama 2 13B and Gemma 7B at FP16 precision, we measure
    the latency in different scenarios on an NVIDIA L40 GPU.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在展示 GRIFFIN 的效率指标。我们收集了具有相同长度样本的合成数据集，并对样本的平均结果进行了统计。像许多其他 MoE 方法一样，GRIFFIN
    非常适合单个样本输入，例如在个人设备的情况下，因此我们在这些实验中使用了批量大小 1。我们计划在未来的工作中将我们的方法扩展到更大的批量大小。使用 Hugging
    Face 实现的 Llama 2 13B 和 Gemma 7B，在 FP16 精度下，我们测量了在 NVIDIA L40 GPU 上不同场景下的延迟。
- en: 'Table 3: Generation phase latency (s). We denote “$P+G$ prompt. When relevant,
    times are in the format 50% / 75% FF sparsity.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：生成阶段延迟（秒）。我们用“$P+G$”表示提示。在相关情况下，时间格式为 50% / 75% FF 稀疏性。
- en: '| Model | Setup | Prompt | Full | Magnitude | GRIFFIN |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 提示 | 完整 | Magnitude | GRIFFIN |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Llama 2 13B | 2048+128 | 0.5 | 6.8 | 5.4 / 5.0 | 5.4 / 5.1 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 13B | 2048+128 | 0.5 | 6.8 | 5.4 / 5.0 | 5.4 / 5.1 |'
- en: '|  | 2048+2048 | 0.5 | 119.1 | 95.0 / 83.4 | 94.9 / 82.8 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | 2048+2048 | 0.5 | 119.1 | 95.0 / 83.4 | 94.9 / 82.8 |'
- en: '| Gemma 7B | 2048+128 | 0.3 | 4.5 | 4.1 / 4.2 | 4.2 / 4.1 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Gemma 7B | 2048+128 | 0.3 | 4.5 | 4.1 / 4.2 | 4.2 / 4.1 |'
- en: '|  | 2048+2048 | 0.3 | 78.5 | 67.7 / 65.0 | 67.4 / 65.0 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | 2048+2048 | 0.3 | 78.5 | 67.7 / 65.0 | 67.4 / 65.0 |'
- en: Recalling that our magnitude selection baseline is essentially neuron pruning
    at generation, this has the best possible speed-up since there is no MoE overhead
    per sample. From Table [3](#S5.T3 "Table 3 ‣ 5.2 Efficiency ‣ 5 Experiments ‣
    Prompt-prompted Mixture of Experts for Efficient LLM Generation"), GRIFFIN matches
    the best case, producing up to a 1.16$\times$ improvement in latency for long
    generation at 50% FF sparsity in Gemma 7B and Llama 2 13B, respectively. This
    illustrates that our method is as fast as a static neuron pruned LLM during generation
    while being adaptive to preserve the accuracy of the full model. In offloading
    settings with large models, our method has the potential to further accelerate
    inference. For a prompt, GRIFFIN essentially performs structured pruning on the
    massive network, and if this pruned model can fit on a single device, it will
    avoid offloading for the entirety of generation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下我们的 magnitude 选择基准，本质上是在生成时进行神经元剪枝，这可以实现最佳的加速，因为每个样本没有 MoE 开销。从表 [3](#S5.T3
    "Table 3 ‣ 5.2 Efficiency ‣ 5 Experiments ‣ Prompt-prompted Mixture of Experts
    for Efficient LLM Generation") 中可以看出，GRIFFIN 达到了最佳情况，在 Gemma 7B 和 Llama 2 13B
    中，分别在 50% FF 稀疏性下提供了高达 1.16$\times$ 的延迟改进。这表明我们的方法在生成时的速度与静态神经元剪枝 LLM 相当，同时具有适应性，以保持完整模型的准确性。在大型模型的卸载设置中，我们的方法有潜力进一步加速推理。对于一个提示，GRIFFIN
    本质上对庞大的网络执行结构化剪枝，如果这个剪枝模型能够适应单一设备，它将避免整个生成过程中的卸载。
- en: 5.3 Ablations and Analysis
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 消融实验与分析
- en: Sampling-based Selection.
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于采样的选择。
- en: Here, we verify that given the statistic $\bm{s}$ selection for half of the
    experts followed by weighted sampling. Based on Table [4](#S5.T4 "Table 4 ‣ Sampling-based
    Selection. ‣ 5.3 Ablations and Analysis ‣ 5 Experiments ‣ Prompt-prompted Mixture
    of Experts for Efficient LLM Generation"), we can see that sampling generally
    degrades performance much more.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们验证了给定统计数据 $\bm{s}$ 选择一半的专家后进行加权采样。根据表 [4](#S5.T4 "Table 4 ‣ Sampling-based
    Selection. ‣ 5.3 Ablations and Analysis ‣ 5 Experiments ‣ Prompt-prompted Mixture
    of Experts for Efficient LLM Generation")，我们可以看到采样通常会显著降低性能。
- en: 'Table 4: Comparison between different expert selection methods at 50% FF sparsity.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 在 50% FF 稀疏性下不同专家选择方法的比较。'
- en: '| Selection Method | XSum | CNN/DailyMail | CoQA | QASPER |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 选择方法 | XSum | CNN/DailyMail | CoQA | QASPER |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | (Rouge-1/2/L) | (Rouge-1/2/L) | (F1/EM) | (F1) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | (Rouge-1/2/L) | (Rouge-1/2/L) | (F1/EM) | (F1) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Llama 2 7B |  |  |  |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 7B |  |  |  |  |'
- en: '| Full | 27.15/9.06/22.62 | 10.08/0.13/9.55 | 77.35/63.88 | 26.31 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 完整 | 27.15/9.06/22.62 | 10.08/0.13/9.55 | 77.35/63.88 | 26.31 |'
- en: '| Top-$k$ | 24.75/7.41/20.55 | 10.97/0.66/10.37 | 77.18/63.58 | 25.76 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Top-$k$ | 24.75/7.41/20.55 | 10.97/0.66/10.37 | 77.18/63.58 | 25.76 |'
- en: '| Sampling | 21.04/5.22/17.12 | 8.78/0.49/8.28 | 76.15/62.53 | 24.46 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 采样 | 21.04/5.22/17.12 | 8.78/0.49/8.28 | 76.15/62.53 | 24.46 |'
- en: '| Top-$k$ + Sampling | 24.35/7.08/20.07 | 10.45/0.48/9.88 | 77.12/64.17 | 25.22
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Top-$k$ + Sampling | 24.35/7.08/20.07 | 10.45/0.48/9.88 | 77.12/64.17 | 25.22
    |'
- en: '| Gemma 7B |  |  |  |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Gemma 7B |  |  |  |  |'
- en: '| Full | 26.86/9.15/22.03 | 17.45/4.15/15.94 | 79.04/65.25 | 30.78 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 完整 | 26.86/9.15/22.03 | 17.45/4.15/15.94 | 79.04/65.25 | 30.78 |'
- en: '| Top-$k$ | 25.86/7.81/20.93 | 18.26/4.75/16.58 | 78.52/64.62 | 27.37 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Top-$k$ | 25.86/7.81/20.93 | 18.26/4.75/16.58 | 78.52/64.62 | 27.37 |'
- en: '| Sampling | 20.25/5.16/16.79 | 8.34/1.71/7.72 | 75.02/59.93 | 24.97 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 采样 | 20.25/5.16/16.79 | 8.34/1.71/7.72 | 75.02/59.93 | 24.97 |'
- en: '| Top-$k$ + Sampling | 24.47/7.43/19.98 | 10.93/2.60/9.98 | 76.76/62.12 | 27.09
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Top-$k$ + Sampling | 24.47/7.43/19.98 | 10.93/2.60/9.98 | 76.76/62.12 | 27.09
    |'
- en: Prompt vs. Generation Length.
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示与生成长度。
- en: We find that GRIFFIN can potentially be made more robust for long generation
    by lengthening the prompt. To see this, we use language modeling on the concatenated
    version of WikiText to simulate generation. For a length $S$ and determine the
    experts. The prompt partition uses the full FF block while the generation partition
    only uses the selected experts. When comparing the original model with GRIFFIN,
    we only compute the perplexity of the outputs from the generation partition since
    the other outputs will be identical. Based on Figure [5](#S5.F5 "Figure 5 ‣ Prompt
    vs. Generation Length. ‣ 5.3 Ablations and Analysis ‣ 5 Experiments ‣ Prompt-prompted
    Mixture of Experts for Efficient LLM Generation"), GRIFFIN gets closer to the
    full model outputs when the prompt length increases and generation length decreases,
    meaning the difficulty with long generation can be suppressed with longer prompts.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，通过延长提示，GRIFFIN 可能在长生成任务中变得更加稳健。为此，我们使用 WikiText 的串联版本进行语言建模，以模拟生成过程。对于长度
    $S$，并确定专家。提示分区使用完整的 FF 块，而生成分区仅使用选择的专家。在比较原始模型和 GRIFFIN 时，我们只计算生成分区输出的困惑度，因为其他输出将是相同的。根据图 [5](#S5.F5
    "图 5 ‣ 提示 vs. 生成长度。 ‣ 5.3 消融研究和分析 ‣ 5 实验 ‣ 提示驱动的专家混合模型用于高效 LLM 生成")，当提示长度增加而生成长度减少时，GRIFFIN
    的输出更接近完整模型的输出，这意味着较长的提示可以抑制长生成任务中的困难。
- en: '![Refer to caption](img/3b20b93b610228fd59bfa0187111116f.png)![Refer to caption](img/a5e0c4f0c46f53978e2a5a23fafe6050.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/3b20b93b610228fd59bfa0187111116f.png)![参考标题](img/a5e0c4f0c46f53978e2a5a23fafe6050.png)'
- en: 'Figure 5: Prompt length vs. generation length for Llama 2 7B (left) and Gemma
    7B (right) as measured by increase in perplexity (PPL) from the full model on
    concatenated WikiText at 50% FF sparsity.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：Llama 2 7B（左）和 Gemma 7B（右）中提示长度与生成长度的关系，以串联 WikiText 上的完整模型的困惑度（PPL）增加为衡量标准，FF
    稀疏度为 50%。
- en: Sparsity in Random Sequences.
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机序列中的稀疏性。
- en: As further exploration into flocking, we investigate this phenomenon with random
    inputs. As input sequences, we use a sample from concatenated WikiText, a permuted
    version of that sample, and completely random sequence where tokens are uniformly
    sampled from the vocabulary. Seen in Figure [6](#S5.F6 "Figure 6 ‣ Sparsity in
    Random Sequences. ‣ 5.3 Ablations and Analysis ‣ 5 Experiments ‣ Prompt-prompted
    Mixture of Experts for Efficient LLM Generation"), this structure exists in permuted
    and random inputs, perhaps even more consistently than in unperturbed sequences.
    This suggests something within language actually diversifies the activations,
    the cause of which would be of interest for future work.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步探索集群现象时，我们使用随机输入进行研究。作为输入序列，我们使用从串联的 WikiText 中提取的样本、该样本的排列版本，以及完全随机的序列，其中标记从词汇表中均匀抽取。如图 [6](#S5.F6
    "图 6 ‣ 随机序列中的稀疏性。 ‣ 5.3 消融研究和分析 ‣ 5 实验 ‣ 提示驱动的专家混合模型用于高效 LLM 生成") 所示，这种结构在排列和随机输入中存在，甚至比在未扰动序列中更为一致。这表明语言中的某些因素实际上使激活多样化，这一原因将是未来研究的兴趣所在。
- en: '![Refer to caption](img/e6d0afc0d5350f0b9786272823c75cbf.png)![Refer to caption](img/11ab7af04bb3055f8c48eded34b8cfa6.png)![Refer
    to caption](img/0b6cd2feb494fdd9ba50eed992c9460f.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e6d0afc0d5350f0b9786272823c75cbf.png)![参考标题](img/11ab7af04bb3055f8c48eded34b8cfa6.png)![参考标题](img/0b6cd2feb494fdd9ba50eed992c9460f.png)'
- en: 'Figure 6: First 512 tokens and their relative FF activation magnitudes in layer
    18 of Gemma 7B when inputting the original WikiText sequence (left), permuted
    sequence (center), and random tokens (right). Best viewed zoomed in.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：输入原始 WikiText 序列（左）、排列序列（中）和随机标记（右）时，Gemma 7B 第 18 层的前 512 个标记及其相对 FF 激活幅度。最好放大查看。
- en: 6 Conclusion
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this work, we have shown a special form of sparsity in FF layers and a simple
    method to exploit it. Flocking is a curious phenomenon present in many LLMs where
    tokens within a sequence activate at similar intensities. This structure motivated
    the design of GRIFFIN, a learning-free MoE selection mechanism to remove FF neurons
    during inference at the sequence level which preserves the full model’s performance
    on a large collection of classification and generative tasks at 50% FF sparsity
    while achieving lower latency. Furthermore, its applicability extends beyond just
    ReLU-based LLMs, allowing MoE adaptation to be possible for many more models.
    With its straightforward algorithm and no-cost deployment, GRIFFIN expands the
    accessibility of numerous LLMs for generative inference.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们展示了一种FF层的特殊稀疏形式和利用它的简单方法。Flocking是一种存在于许多LLM中的奇特现象，其中序列中的标记以类似的强度激活。这种结构激发了GRIFFIN的设计，GRIFFIN是一种无学习的MoE选择机制，用于在推理过程中在序列级别去除FF神经元，保留了完整模型在大规模分类和生成任务上的性能，在50%
    FF稀疏性下同时实现了更低的延迟。此外，它的适用性超越了基于ReLU的LLM，使MoE适应更多模型成为可能。凭借其直接的算法和无成本的部署，GRIFFIN扩大了众多LLM在生成推理中的可及性。
- en: Acknowledgements
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Zixin Wen for insightful discussions. The work of H. Dong is supported
    in part by the Wei Shen and Xuehong Zhang Presidential Fellowship at Carnegie
    Mellon University. The work of Y. Chi is supported in part by the grants NSF DMS-2134080
    and ONR N00014-19-1-2404.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢Zixin Wen的深刻讨论。H. Dong的工作部分得到卡内基梅隆大学Wei Shen和Xuehong Zhang总统奖学金的资助。Y. Chi的工作部分得到NSF
    DMS-2134080和ONR N00014-19-1-2404资助。
- en: References
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[AMB^+23] K. Alizadeh, I. Mirzadeh, D. Belenko, K. Khatamifard, M. Cho, C. C.
    Del Mundo, M. Rastegari, and M. Farajtabar. Llm in a flash: Efficient large language
    model inference with limited memory. arXiv preprint arXiv:2312.11514, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AMB^+23] K. Alizadeh, I. Mirzadeh, D. Belenko, K. Khatamifard, M. Cho, C.
    C. Del Mundo, M. Rastegari, 和 M. Farajtabar. 闪电中的LLM：在有限内存下高效的大型语言模型推理。arXiv预印本
    arXiv:2312.11514, 2023。'
- en: '[Ant24] Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ant24] Anthropic. Claude 3模型系列：Opus, Sonnet, Haiku, 2024。'
- en: '[BGOFG20] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag. What
    is the state of neural network pruning? Proceedings of machine learning and systems,
    2:129–146, 2020.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BGOFG20] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, 和 J. Guttag. 神经网络剪枝的现状如何？机器学习与系统会议论文集,
    2:129–146, 2020。'
- en: '[BZB^+20] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. Piqa: Reasoning
    about physical commonsense in natural language. In Thirty-Fourth AAAI Conference
    on Artificial Intelligence, 2020.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BZB^+20] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, 和 Y. Choi. Piqa：关于自然语言中的物理常识的推理。发表于第34届AAAI人工智能会议,
    2020。'
- en: '[CCE^+18] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
    and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning
    challenge. arXiv:1803.05457v1, 2018.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CCE^+18] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
    和 O. Tafjord. 认为你已经解决了问答问题？试试arc，AI2推理挑战。arXiv:1803.05457v1, 2018。'
- en: '[CIS23] R. Csordás, K. Irie, and J. Schmidhuber. Approximating two-layer feedforward
    networks for efficient transformers. arXiv preprint arXiv:2310.10837, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CIS23] R. Csordás, K. Irie, 和 J. Schmidhuber. 近似两层前馈网络以实现高效变换器。arXiv预印本 arXiv:2310.10837,
    2023。'
- en: '[CLC^+19] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova.
    Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv
    preprint arXiv:1905.10044, 2019.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CLC^+19] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, 和 K. Toutanova.
    Boolq：探索自然是/否问题的意外难度。arXiv预印本 arXiv:1905.10044, 2019。'
- en: '[DCC23] H. Dong, B. Chen, and Y. Chi. Towards structured sparsity in transformers
    for efficient inference. In Workshop on Efficient Systems for Foundation Models@
    ICML2023, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DCC23] H. Dong, B. Chen, 和 Y. Chi. 朝着变换器中的结构稀疏性迈进，以实现高效推理。发表于ICML2023的基础模型高效系统研讨会,
    2023。'
- en: '[DKK^+24] L. Dery, S. Kolawole, J.-F. Kagey, V. Smith, G. Neubig, and A. Talwalkar.
    Everybody prune now: Structured pruning of llms with only forward passes. arXiv
    preprint arXiv:2402.05406, 2024.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DKK^+24] L. Dery, S. Kolawole, J.-F. Kagey, V. Smith, G. Neubig, 和 A. Talwalkar.
    大家现在都剪枝吧：仅通过前向传递对LLM的结构化剪枝。arXiv预印本 arXiv:2402.05406, 2024。'
- en: '[DLB^+21] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner.
    A dataset of information-seeking questions and answers anchored in research papers.
    arXiv preprint arXiv:2105.03011, 2021.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DLB^+21] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, 和 M. Gardner.
    一份基于研究论文的信息检索问题和答案的数据集。arXiv预印本 arXiv:2105.03011, 2021。'
- en: '[DLBZ22] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Llm. int8 ():
    8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339,
    2022.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DLBZ22] T. Dettmers, M. Lewis, Y. Belkada, 和 L. Zettlemoyer. Llm. int8 ():
    8 位矩阵乘法在规模上用于变换器。arXiv 预印本 arXiv:2208.07339，2022 年。'
- en: '[FA23] E. Frantar and D. Alistarh. Massive language models can be accurately
    pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FA23] E. Frantar 和 D. Alistarh. 大型语言模型可以通过一次性准确剪枝。arXiv 预印本 arXiv:2301.00774，2023
    年。'
- en: '[FC18] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse,
    trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FC18] J. Frankle 和 M. Carbin. 彩票票假设：寻找稀疏的、可训练的神经网络。arXiv 预印本 arXiv:1803.03635，2018
    年。'
- en: '[FSH04] K. Fatahalian, J. Sugerman, and P. Hanrahan. Understanding the efficiency
    of gpu algorithms for matrix-matrix multiplication. In Proceedings of the ACM
    SIGGRAPH/EUROGRAPHICS conference on Graphics hardware, pages 133–137, 2004.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FSH04] K. Fatahalian, J. Sugerman, 和 P. Hanrahan. 理解 GPU 算法在矩阵-矩阵乘法中的效率。发表于
    ACM SIGGRAPH/EUROGRAPHICS 图形硬件会议论文集，第 133–137 页，2004 年。'
- en: '[FZS22] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to
    trillion parameter models with simple and efficient sparsity. The Journal of Machine
    Learning Research, 23(1):5232–5270, 2022.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FZS22] W. Fedus, B. Zoph, 和 N. Shazeer. Switch transformers: 通过简单高效的稀疏性扩展到万亿参数模型。《机器学习研究期刊》，23(1):5232–5270，2022
    年。'
- en: '[GBB^+20] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang,
    H. He, A. Thite, N. Nabeshima, et al. The pile: An 800gb dataset of diverse text
    for language modeling. arXiv preprint arXiv:2101.00027, 2020.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GBB^+20] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J.
    Phang, H. He, A. Thite, N. Nabeshima, 等等. The pile: 一个 800GB 的多样文本数据集用于语言建模。arXiv
    预印本 arXiv:2101.00027，2020 年。'
- en: '[GSBL20] M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer feed-forward
    layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GSBL20] M. Geva, R. Schuster, J. Berant, 和 O. Levy. 变换器前馈层是键值记忆。arXiv 预印本
    arXiv:2012.14913，2020 年。'
- en: '[GTA^+23] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster,
    L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa,
    J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite,
    B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation,
    12 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GTA^+23] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster,
    L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa,
    J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite,
    B. Wang, K. Wang, 和 A. Zou. 一个少样本语言模型评估框架，2023 年 12 月。'
- en: '[JJNH91] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive
    mixtures of local experts. Neural computation, 3(1):79–87, 1991.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[JJNH91] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, 和 G. E. Hinton. 自适应局部专家混合。神经计算，3(1):79–87，1991
    年。'
- en: '[JLC^+23] A. K. Jaiswal, S. Liu, T. Chen, Y. Ding, and Z. Wang. Instant soup:
    Cheap pruning ensembles in a single pass can draw lottery tickets from large models.
    In International Conference on Machine Learning, pages 14691–14701\. PMLR, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[JLC^+23] A. K. Jaiswal, S. Liu, T. Chen, Y. Ding, 和 Z. Wang. 即时剪枝：在一次通过中用廉价的剪枝集成从大模型中抽取彩票。发表于国际机器学习会议论文集，第
    14691–14701 页。PMLR，2023 年。'
- en: '[JSM^+23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,
    D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral
    7b. arXiv preprint arXiv:2310.06825, 2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[JSM^+23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,
    D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, 等等. Mistral 7b.
    arXiv 预印本 arXiv:2310.06825，2023 年。'
- en: '[JSR^+24] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford,
    D. S. Chaplot, D. de las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour,
    G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux, P. Stock, S. Subramanian,
    S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, and
    W. E. Sayed. Mixtral of experts, 2024.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[JSR^+24] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford,
    D. S. Chaplot, D. de las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour,
    G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux, P. Stock, S. Subramanian,
    S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, 和
    W. E. Sayed. 专家混合模型，2024 年。'
- en: '[KNH^+22] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah.
    Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s):1–41,
    2022.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KNH^+22] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, 和 M. Shah.
    视觉中的变换器：一项调查。ACM 计算调查 (CSUR)，54(10s):1–41，2022 年。'
- en: '[LDS89] Y. LeCun, J. Denker, and S. Solla. Optimal brain damage. Advances in
    neural information processing systems, 2, 1989.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LDS89] Y. LeCun, J. Denker, 和 S. Solla. 最优脑损伤。神经信息处理系统进展，2，1989 年。'
- en: '[LGW^+21] T. Liang, J. Glossner, L. Wang, S. Shi, and X. Zhang. Pruning and
    quantization for deep neural network acceleration: A survey. Neurocomputing, 461:370–403,
    2021.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LGW^+21] T. Liang, J. Glossner, L. Wang, S. Shi 和 X. Zhang. 深度神经网络加速的剪枝与量化：综述。Neurocomputing,
    461:370–403, 2021 年。'
- en: '[LLLC21] Z. Liu, F. Li, G. Li, and J. Cheng. Ebert: Efficient bert inference
    with dynamic structured pruning. In Findings of the Association for Computational
    Linguistics: ACL-IJCNLP 2021, pages 4814–4823, 2021.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLLC21] Z. Liu, F. Li, G. Li 和 J. Cheng. Ebert: 使用动态结构化剪枝的高效 BERT 推断。收录于《计算语言学协会会议论文集：ACL-IJCNLP
    2021》，第 4814–4823 页，2021 年。'
- en: '[LWD^+23] Z. Liu, J. Wang, T. Dao, T. Zhou, B. Yuan, Z. Song, A. Shrivastava,
    C. Zhang, Y. Tian, C. Re, et al. Deja vu: Contextual sparsity for efficient llms
    at inference time. In International Conference on Machine Learning, pages 22137–22176\.
    PMLR, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LWD^+23] Z. Liu, J. Wang, T. Dao, T. Zhou, B. Yuan, Z. Song, A. Shrivastava,
    C. Zhang, Y. Tian, C. Re 等. Déjà vu: 推断时高效 LLMS 的上下文稀疏性。国际机器学习大会论文集，第 22137–22176
    页。PMLR，2023 年。'
- en: '[LWLQ22] T. Lin, Y. Wang, X. Liu, and X. Qiu. A survey of transformers. AI
    Open, 2022.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LWLQ22] T. Lin, Y. Wang, X. Liu 和 X. Qiu. 变换器的综述。AI Open，2022 年。'
- en: '[LYB^+22] Z. Li, C. You, S. Bhojanapalli, D. Li, A. S. Rawat, S. J. Reddi,
    K. Ye, F. Chern, F. Yu, R. Guo, et al. The lazy neuron phenomenon: On emergence
    of activation sparsity in transformers. In The Eleventh International Conference
    on Learning Representations, 2022.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LYB^+22] Z. Li, C. You, S. Bhojanapalli, D. Li, A. S. Rawat, S. J. Reddi,
    K. Ye, F. Chern, F. Yu, R. Guo 等. 懒惰神经元现象：变换器中激活稀疏性的出现。第十一届国际学习表示会议，2022 年。'
- en: '[LYZ^+23] Y. Li, Y. Yu, Q. Zhang, C. Liang, P. He, W. Chen, and T. Zhao. Losparse:
    Structured compression of large language models based on low-rank and sparse approximation.
    arXiv preprint arXiv:2306.11222, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LYZ^+23] Y. Li, Y. Yu, Q. Zhang, C. Liang, P. He, W. Chen 和 T. Zhao. Losparse:
    基于低秩和稀疏近似的大型语言模型的结构压缩。arXiv 预印本 arXiv:2306.11222，2023 年。'
- en: '[LZH^+24] B. Liu, Z. Zhang, P. He, Z. Wang, Y. Xiao, R. Ye, Y. Zhou, W.-S.
    Ku, and B. Hui. A survey of lottery ticket hypothesis. arXiv preprint arXiv:2403.04861,
    2024.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LZH^+24] B. Liu, Z. Zhang, P. He, Z. Wang, Y. Xiao, R. Ye, Y. Zhou, W.-S.
    Ku 和 B. Hui. 关于彩票票据假设的综述。arXiv 预印本 arXiv:2403.04861，2024 年。'
- en: '[MAM^+23] I. Mirzadeh, K. Alizadeh, S. Mehta, C. C. Del Mundo, O. Tuzel, G. Samei,
    M. Rastegari, and M. Farajtabar. Relu strikes back: Exploiting activation sparsity
    in large language models. arXiv preprint arXiv:2310.04564, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MAM^+23] I. Mirzadeh, K. Alizadeh, S. Mehta, C. C. Del Mundo, O. Tuzel, G.
    Samei, M. Rastegari 和 M. Farajtabar. ReLU 的反击：利用大型语言模型中的激活稀疏性。arXiv 预印本 arXiv:2310.04564，2023
    年。'
- en: '[MFW23] X. Ma, G. Fang, and X. Wang. Llm-pruner: On the structural pruning
    of large language models. Advances in neural information processing systems, 36:21702–21720,
    2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MFW23] X. Ma, G. Fang 和 X. Wang. LLM-pruner: 大型语言模型的结构剪枝。神经信息处理系统进展，36:21702–21720，2023
    年。'
- en: '[MXBS16] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel
    mixture models, 2016.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MXBS16] S. Merity, C. Xiong, J. Bradbury 和 R. Socher. 指针哨兵混合模型，2016 年。'
- en: '[NBZ^+23] S. Nerella, S. Bandyopadhyay, J. Zhang, M. Contreras, S. Siegel,
    A. Bumin, B. Silva, J. Sena, B. Shickel, A. Bihorac, et al. Transformers in healthcare:
    A survey. arXiv preprint arXiv:2307.00067, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NBZ^+23] S. Nerella, S. Bandyopadhyay, J. Zhang, M. Contreras, S. Siegel,
    A. Bumin, B. Silva, J. Sena, B. Shickel, A. Bihorac 等. 医疗保健中的变换器：综述。arXiv 预印本
    arXiv:2307.00067，2023 年。'
- en: '[NCL18] S. Narayan, S. B. Cohen, and M. Lapata. Don’t give me the details,
    just the summary! topic-aware convolutional neural networks for extreme summarization.
    arXiv preprint arXiv:1808.08745, 2018.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NCL18] S. Narayan, S. B. Cohen 和 M. Lapata. 不要给我细节，只要总结！针对极端摘要的主题感知卷积神经网络。arXiv
    预印本 arXiv:1808.08745，2018 年。'
- en: '[NZG^+16] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang, et al. Abstractive
    text summarization using sequence-to-sequence rnns and beyond. arXiv preprint
    arXiv:1602.06023, 2016.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NZG^+16] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang 等. 使用序列到序列 RNN 及其他方法的抽象文本摘要。arXiv
    预印本 arXiv:1602.06023，2016 年。'
- en: '[PSBW23] M. Piórczyński, F. Szatkowski, K. Bałazy, and B. Wójcik. Exploiting
    transformer activation sparsity with dynamic inference. arXiv preprint arXiv:2310.04361,
    2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PSBW23] M. Piórczyński, F. Szatkowski, K. Bałazy 和 B. Wójcik. 利用变换器激活稀疏性进行动态推断。arXiv
    预印本 arXiv:2310.04361，2023 年。'
- en: '[RBG11] M. Roemmele, C. A. Bejan, and A. S. Gordon. Choice of plausible alternatives:
    An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series,
    2011.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RBG11] M. Roemmele, C. A. Bejan 和 A. S. Gordon. 可能替代方案的选择：对常识因果推理的评估。2011
    年 AAAI 春季研讨会论文集，2011 年。'
- en: '[RCM19] S. Reddy, D. Chen, and C. D. Manning. Coqa: A conversational question
    answering challenge. Transactions of the Association for Computational Linguistics,
    7:249–266, 2019.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RCM19] S. Reddy, D. Chen 和 C. D. Manning. Coqa：一个对话式问答挑战。计算语言学协会会刊，7:249–266,
    2019。'
- en: '[RPJ^+19] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap.
    Compressive transformers for long-range sequence modelling. arXiv preprint, 2019.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RPJ^+19] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier 和 T. P. Lillicrap.
    用于长范围序列建模的压缩 Transformer。arXiv 预印本, 2019。'
- en: '[Sha20] N. Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202,
    2020.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sha20] N. Shazeer. GLU 变体改进了 Transformer。arXiv 预印本 arXiv:2002.05202, 2020。'
- en: '[SLBK23] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter. A simple and effective
    pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SLBK23] M. Sun, Z. Liu, A. Bair 和 J. Z. Kolter. 一种简单有效的大型语言模型剪枝方法。arXiv 预印本
    arXiv:2306.11695, 2023。'
- en: '[SSI^+22] U. Shaham, E. Segal, M. Ivgi, A. Efrat, O. Yoran, A. Haviv, A. Gupta,
    W. Xiong, M. Geva, J. Berant, et al. Scrolls: Standardized comparison over long
    language sequences. arXiv preprint arXiv:2201.03533, 2022.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SSI^+22] U. Shaham, E. Segal, M. Ivgi, A. Efrat, O. Yoran, A. Haviv, A. Gupta,
    W. Xiong, M. Geva, J. Berant 等人. Scrolls：对长语言序列的标准化比较。arXiv 预印本 arXiv:2201.03533,
    2022。'
- en: '[SWSL23] M. Santacroce, Z. Wen, Y. Shen, and Y. Li. What matters in the structured
    pruning of generative language models? arXiv preprint arXiv:2302.03773, 2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SWSL23] M. Santacroce, Z. Wen, Y. Shen 和 Y. Li. 在生成语言模型的结构化剪枝中什么是重要的？arXiv
    预印本 arXiv:2302.03773, 2023。'
- en: '[TAB^+23] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,
    J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal
    models. arXiv preprint arXiv:2312.11805, 2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TAB^+23] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,
    J. Schalkwyk, A. M. Dai, A. Hauth 等人. Gemini：一系列高度能力的多模态模型。arXiv 预印本 arXiv:2312.11805,
    2023。'
- en: '[Tea23] S. Team. Sparse large language models with relu activation, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tea23] S. Team. 带有 ReLU 激活的稀疏大型语言模型，2023。'
- en: '[TMH^+24] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak,
    L. Sifre, M. Rivière, M. S. Kale, J. Love, et al. Gemma: Open models based on
    gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TMH^+24] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak,
    L. Sifre, M. Rivière, M. S. Kale, J. Love 等人. Gemma：基于 Gemini 研究和技术的开放模型。arXiv
    预印本 arXiv:2403.08295, 2024。'
- en: '[TMS^+23] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,
    N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation
    and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TMS^+23] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,
    N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale 等人. Llama 2：开放基础和微调聊天模型。arXiv
    预印本 arXiv:2307.09288, 2023。'
- en: '[VSP^+17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
    Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural
    information processing systems, 30, 2017.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[VSP^+17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
    Gomez, Ł. Kaiser 和 I. Polosukhin. 注意力即你所需。神经信息处理系统进展，30, 2017。'
- en: '[WWB19] Y. E. Wang, G.-Y. Wei, and D. Brooks. Benchmarking tpu, gpu, and cpu
    platforms for deep learning. arXiv preprint arXiv:1907.10701, 2019.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WWB19] Y. E. Wang, G.-Y. Wei 和 D. Brooks. TPU、GPU 和 CPU 平台的深度学习基准测试。arXiv
    预印本 arXiv:1907.10701, 2019。'
- en: '[XGZC23] M. Xia, T. Gao, Z. Zeng, and D. Chen. Sheared llama: Accelerating
    language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694,
    2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGZC23] M. Xia, T. Gao, Z. Zeng 和 D. Chen. Sheared llama：通过结构化剪枝加速语言模型预训练。arXiv
    预印本 arXiv:2310.06694, 2023。'
- en: '[XZC22] M. Xia, Z. Zhong, and D. Chen. Structured pruning learns compact and
    accurate models. arXiv preprint arXiv:2204.00408, 2022.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XZC22] M. Xia, Z. Zhong 和 D. Chen. 结构化剪枝学习紧凑而准确的模型。arXiv 预印本 arXiv:2204.00408,
    2022。'
- en: '[YJL^+24] L. Yin, A. Jaiswal, S. Liu, S. Kundu, and Z. Wang. Pruning small
    pre-trained weights irreversibly and monotonically impairs "difficult" downstream
    tasks in llms, 2024.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[YJL^+24] L. Yin, A. Jaiswal, S. Liu, S. Kundu 和 Z. Wang. 剪枝小型预训练权重不可逆且单调地损害
    LLM 中的“困难”下游任务，2024。'
- en: '[YYB^+24] V. Yerram, C. You, S. Bhojanapalli, S. Kumar, P. Jain, P. Netrapalli,
    et al. Hire: High recall approximate top-$k$ estimation for efficient llm inference.
    arXiv preprint arXiv:2402.09360, 2024.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[YYB^+24] V. Yerram, C. You, S. Bhojanapalli, S. Kumar, P. Jain, P. Netrapalli
    等人. Hire：高召回率近似 top-$k$ 估计用于高效的 LLM 推理。arXiv 预印本 arXiv:2402.09360, 2024。'
- en: '[ZBC^+24] H. Zheng, X. Bai, B. Chen, F. Lai, and A. Prakash. Learn to be efficient:
    Build structured sparsity in large language models. arXiv preprint arXiv:2402.06126,
    2024.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZBC^+24] H. Zheng, X. Bai, B. Chen, F. Lai 和 A. Prakash. 学会高效：在大型语言模型中建立结构化稀疏性。arXiv
    预印本 arXiv:2402.06126, 2024。'
- en: '[ZHB^+19] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag:
    Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZHB^+19] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, 和 Y. Choi. Hellaswag:
    机器真的能完成你的句子吗？arXiv预印本 arXiv:1905.07830, 2019.'
- en: '[ZLL^+21] Z. Zhang, Y. Lin, Z. Liu, P. Li, M. Sun, and J. Zhou. Moefication:
    Transformer feed-forward layers are mixtures of experts. arXiv preprint arXiv:2110.01786,
    2021.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZLL^+21] Z. Zhang, Y. Lin, Z. Liu, P. Li, M. Sun, 和 J. Zhou. Moefication:
    变换器前馈层是专家的混合体。arXiv预印本 arXiv:2110.01786, 2021.'
- en: '[ZRG^+22] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig,
    P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer. Opt: Open pre-trained transformer
    language models, 2022.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ZRG^+22] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig,
    P. S. Koura, A. Sridhar, T. Wang, 和 L. Zettlemoyer. Opt: 开放预训练的变换器语言模型, 2022.'
- en: Appendix A Gating Metric
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 门控指标
- en: Here we present visualizations of our gating statistic $\bm{s}$ can capture
    heavily and frequently activated neurons.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了我们的门控统计量$\bm{s}$可以捕捉到高度和频繁激活的神经元。
- en: '![Refer to caption](img/37b126c969a8171f4c0464352ba492d3.png)![Refer to caption](img/7c55b38f77b75a80710009620dea5ebb.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/37b126c969a8171f4c0464352ba492d3.png)![参见说明](img/7c55b38f77b75a80710009620dea5ebb.png)'
- en: 'Figure 7: Sorted entries of $\bm{s}$ for each layer of Llama 2 7B (left) and
    Gemma 7B (right) with a sequence from PG-19 as the input. Each line is a layer.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：输入PG-19序列时，Llama 2 7B（左）和Gemma 7B（右）每层的$\bm{s}$排序条目。每行表示一个层。
- en: Appendix B More Flocking Examples
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 更多集群示例
- en: We provide more example of flocking across different layers of the LLM. Figure [8](#A2.F8
    "Figure 8 ‣ Appendix B More Flocking Examples ‣ Prompt-prompted Mixture of Experts
    for Efficient LLM Generation") and Figure [9](#A2.F9 "Figure 9 ‣ Appendix B More
    Flocking Examples ‣ Prompt-prompted Mixture of Experts for Efficient LLM Generation")
    show flocking in Gemma 7B. Figure [10](#A2.F10 "Figure 10 ‣ Appendix B More Flocking
    Examples ‣ Prompt-prompted Mixture of Experts for Efficient LLM Generation") and
    Figure [11](#A2.F11 "Figure 11 ‣ Appendix B More Flocking Examples ‣ Prompt-prompted
    Mixture of Experts for Efficient LLM Generation") show flocking in Llama2 7B.
    Flocking in Gemma 7B is more visually distinct while activations in Llama2 7B
    are more distributed.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了更多关于LLM不同层次的集群示例。图 [8](#A2.F8 "图 8 ‣ 附录 B 更多集群示例 ‣ 提示驱动的专家混合用于高效的LLM生成")和图 [9](#A2.F9
    "图 9 ‣ 附录 B 更多集群示例 ‣ 提示驱动的专家混合用于高效的LLM生成")展示了Gemma 7B中的集群。图 [10](#A2.F10 "图 10
    ‣ 附录 B 更多集群示例 ‣ 提示驱动的专家混合用于高效的LLM生成")和图 [11](#A2.F11 "图 11 ‣ 附录 B 更多集群示例 ‣ 提示驱动的专家混合用于高效的LLM生成")展示了Llama2
    7B中的集群。Gemma 7B中的集群视觉上更为明显，而Llama2 7B中的激活则更为分散。
- en: '![Refer to caption](img/e4eeea0f622bc5e0fcfbdc1ba5a5989c.png)![Refer to caption](img/e04d572439748929f7af8def3cfd6121.png)![Refer
    to caption](img/e47d149ef509053d7f0d7c2eafe1367a.png)![Refer to caption](img/92ae3a61f11c890cfd555478a04e66d9.png)![Refer
    to caption](img/9b0f2c832da1f1689cb78fc656977a77.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e4eeea0f622bc5e0fcfbdc1ba5a5989c.png)![参见说明](img/e04d572439748929f7af8def3cfd6121.png)![参见说明](img/e47d149ef509053d7f0d7c2eafe1367a.png)![参见说明](img/92ae3a61f11c890cfd555478a04e66d9.png)![参见说明](img/9b0f2c832da1f1689cb78fc656977a77.png)'
- en: 'Figure 8: First 512 tokens and their relative FF activation magnitudes in layers
    1 to 20 of Gemma 7B when inputting a sequence from PG-19.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：输入PG-19序列时，Gemma 7B中第1至20层的前512个标记及其相对FF激活幅度。
- en: '![Refer to caption](img/d11c1f7bffc4476e8d72403398e46461.png)![Refer to caption](img/ed8284587e876dab9f39a0f315d061f3.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d11c1f7bffc4476e8d72403398e46461.png)![参见说明](img/ed8284587e876dab9f39a0f315d061f3.png)'
- en: 'Figure 9: First 512 tokens and their relative FF activation magnitudes in layers
    21 to 28 of Gemma 7B when inputting a sequence from PG-19.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：输入PG-19序列时，Gemma 7B中第21至28层的前512个标记及其相对FF激活幅度。
- en: '![Refer to caption](img/22fc03a9cdcbc2b788e43de5fe4981a2.png)![Refer to caption](img/8c279f279aa6ce810e536b417af211c1.png)![Refer
    to caption](img/80bc7320c5400428bd4f82c3cb077564.png)![Refer to caption](img/1f727abb6741268e3997a33b620b9349.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/22fc03a9cdcbc2b788e43de5fe4981a2.png)![参见说明](img/8c279f279aa6ce810e536b417af211c1.png)![参见说明](img/80bc7320c5400428bd4f82c3cb077564.png)![参见说明](img/1f727abb6741268e3997a33b620b9349.png)'
- en: 'Figure 10: First 512 tokens and their relative FF activation magnitudes in
    layers 1 to 16 of Llama 2 7B when inputting a sequence from PG-19.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：输入PG-19序列时，Llama 2 7B中第1至16层的前512个标记及其相对FF激活幅度。
- en: '![Refer to caption](img/5adc775ed61901e8d3de0c453b97b15a.png)![Refer to caption](img/c8a6541d65e8491d63e9b45a96041426.png)![Refer
    to caption](img/611d8636363e219e99b2b38be241a95c.png)![Refer to caption](img/b26116b3c5e60153fcbc072119c876ec.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5adc775ed61901e8d3de0c453b97b15a.png)![参见说明](img/c8a6541d65e8491d63e9b45a96041426.png)![参见说明](img/611d8636363e219e99b2b38be241a95c.png)![参见说明](img/b26116b3c5e60153fcbc072119c876ec.png)'
- en: 'Figure 11: First 512 tokens and their relative FF activation magnitudes in
    layers 17 to 32 of Llama 2 7B when inputting a sequence from PG-19.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：在输入来自 PG-19 的序列时，Llama 2 7B 模型第 17 层到第 32 层中前 512 个标记及其相对 FF 激活幅度。
