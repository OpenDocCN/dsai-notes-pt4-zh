- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:05:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:05:30
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'BOND: Aligning LLMs with Best-of-N Distillation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'BOND: Aligning LLMs with Best-of-N Distillation'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.14622](https://ar5iv.labs.arxiv.org/html/2407.14622)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.14622](https://ar5iv.labs.arxiv.org/html/2407.14622)
- en: \pdftrailerid
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \pdftrailerid
- en: redacted \correspondingauthorpiergs@google.com
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: redacted \correspondingauthorpiergs@google.com
- en: Pier Giuseppe Sessa Google DeepMind Robert Dadashi Google DeepMind Léonard Hussenot
    Google DeepMind Johan Ferret Google DeepMind Nino Vieillard Google DeepMind Alexandre Ramé
    Google DeepMind Bobak Shariari Google DeepMind Sarah Perrin Google DeepMind Abe Friesen
    Google DeepMind Geoffrey Cideron Google DeepMind Sertan Girgin Google DeepMind
    Piotr Stanczyk Google DeepMind Andrea Michi Google DeepMind Danila Sinopalnikov
    Google DeepMind Sabela Ramos Google DeepMind Amélie Héliou Google DeepMind Aliaksei Severyn
    Google DeepMind Matt Hoffman Google DeepMind Nikola Momchev Google DeepMind Olivier Bachem
    Google DeepMind
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Pier Giuseppe Sessa Google DeepMind Robert Dadashi Google DeepMind Léonard Hussenot
    Google DeepMind Johan Ferret Google DeepMind Nino Vieillard Google DeepMind Alexandre Ramé
    Google DeepMind Bobak Shariari Google DeepMind Sarah Perrin Google DeepMind Abe Friesen
    Google DeepMind Geoffrey Cideron Google DeepMind Sertan Girgin Google DeepMind
    Piotr Stanczyk Google DeepMind Andrea Michi Google DeepMind Danila Sinopalnikov
    Google DeepMind Sabela Ramos Google DeepMind Amélie Héliou Google DeepMind Aliaksei Severyn
    Google DeepMind Matt Hoffman Google DeepMind Nikola Momchev Google DeepMind Olivier Bachem
    Google DeepMind
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Reinforcement learning from human feedback (RLHF) is a key driver of quality
    and safety in state-of-the-art large language models. Yet, a surprisingly simple
    and strong inference-time strategy is Best-of-N sampling that selects the best
    generation among $N$) to balance between mode-covering and mode-seeking behavior,
    and derive an iterative formulation that utilizes a moving anchor for efficiency.
    We demonstrate the effectiveness of our approach and several design choices through
    experiments on abstractive summarization and Gemma models. Aligning Gemma policies
    with BOND outperforms other RLHF algorithms by improving results on several benchmarks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类反馈中进行的强化学习（RLHF）是提升最先进的大型语言模型质量和安全性的关键驱动因素。然而，一种令人惊讶的简单且强大的推理时策略是Best-of-N采样，它在$N$中选择最佳生成结果，以平衡模式覆盖和模式寻求行为，并推导出利用移动锚点以提高效率的迭代公式。我们通过在抽象总结和Gemma模型上的实验展示了我们方法的有效性及若干设计选择。将Gemma策略与BOND对齐在多个基准测试中优于其他RLHF算法。
- en: 'keywords:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: LLM, Alignment, RLHF, Best-of-N
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LLM, 对齐, RLHF, Best-of-N
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: State-of-the-art large language models (LLMs) such as Gemini (Gemini Team, [2023](#bib.bib21);
    Reid et al., [2024](#bib.bib51)) and GPT-4 (OpenAI, [2023](#bib.bib39)) are generally
    trained in three stages. First, LLMs are pre-trained on large corpora of knowledge
    using next-token prediction (Radford et al., [2018](#bib.bib45), [2019](#bib.bib46)).
    Second, the pre-trained models are fine-tuned to follow instructions via supervised
    fine-tuning (SFT) (Raffel et al., [2020](#bib.bib48); Wei et al., [2022](#bib.bib61)).
    Lastly, reinforcement learning from human feedback (RLHF) (Christiano et al.,
    [2017](#bib.bib10); Ziegler et al., [2019](#bib.bib65); Stiennon et al., [2020](#bib.bib55))
    is used to further increase the quality of generations. The RLHF step generally
    consists of learning a reward model (RM) (Ouyang et al., [2022](#bib.bib40)) on
    human preferences and then optimizing the LLM to maximize predicted rewards using
    reinforcement learning algorithms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 先进的大型语言模型（LLMs），例如Gemini（Gemini Team，[2023](#bib.bib21)；Reid et al.，[2024](#bib.bib51)）和GPT-4（OpenAI，[2023](#bib.bib39)），通常经过三个阶段的训练。首先，LLMs在大量知识语料上进行预训练，使用下一个标记预测（Radford
    et al.，[2018](#bib.bib45)，[2019](#bib.bib46)）。其次，预训练模型通过监督微调（SFT）（Raffel et al.，[2020](#bib.bib48)；Wei
    et al.，[2022](#bib.bib61)）进行微调以遵循指令。最后，使用从人类反馈中进行的强化学习（RLHF）（Christiano et al.，[2017](#bib.bib10)；Ziegler
    et al.，[2019](#bib.bib65)；Stiennon et al.，[2020](#bib.bib55)）进一步提高生成质量。RLHF步骤通常包括学习一个基于人类偏好的奖励模型（RM）（Ouyang
    et al.，[2022](#bib.bib40)），然后使用强化学习算法优化LLM以最大化预测奖励。
- en: RLHF algorithms and their challenges. Fine-tuning LLMs with reinforcement learning
    (RL) is challenging (Casper et al., [2023](#bib.bib8)), notably since it can cause
    *forgetting* (French, [1992](#bib.bib18)) of pre-trained knowledge, and since
    loopholes in the RM (Clark and Amodei, [2016](#bib.bib11); Pan et al., [2022](#bib.bib42))
    can cause *reward hacking* (Askell et al., [2021](#bib.bib4); Skalse et al., [2022](#bib.bib54)).
    The standard strategy is to use policy-gradient methods (Williams, [1992](#bib.bib62))
    with $\mathrm{KL}$, to preserve the general capabilities of the original model
    and tackle the misalignment (Ngo et al., [2022](#bib.bib38)) concerns.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF算法及其挑战。使用强化学习（RL）对LLM进行微调具有挑战性（Casper等，[2023](#bib.bib8)），特别是因为它可能导致*遗忘*（French，[1992](#bib.bib18)）预训练知识，以及由于RM中的漏洞（Clark和Amodei，[2016](#bib.bib11)；Pan等，[2022](#bib.bib42)）可能导致*奖励劫持*（Askell等，[2021](#bib.bib4)；Skalse等，[2022](#bib.bib54)）。标准策略是使用策略梯度方法（Williams，[1992](#bib.bib62)）与$\mathrm{KL}$，以保持原始模型的整体能力并解决不一致性（Ngo等，[2022](#bib.bib38)）问题。
- en: 'Best-of-N sampling. In practice, a surprisingly simple inference-time approach
    is often used to improve the quality of generations: Best-of-N sampling (Stiennon
    et al., [2020](#bib.bib55)). It consists of drawing $N$.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳-N采样。在实践中，通常使用一种令人惊讶的简单推断时方法来提高生成质量：最佳-N采样（Stiennon等，[2020](#bib.bib55)）。它包括绘制$N$。
- en: 'BOND. In this paper, we propose BOND (Best-of-N Distillation), a novel RLHF
    algorithm that *learns* a policy that achieves the strong performance of Best-of-N
    sampling but, crucially, requires only a single sample at inference time, as depicted
    in [Figure 1](#S1.F1 "In 1 Introduction ‣ BOND: Aligning LLMs with Best-of-N Distillation").
    Our key idea is to cast the alignment of the policy as a distribution matching
    problem, where we fine-tune the policy to emulate the Best-of-N distribution.
    To achieve this, we first derive an analytical expression for the Best-of-N distribution.
    This allows us to consider and optimize different divergence metrics. We first
    show how to minimize the *forward* $\mathrm{KL}$, also known as *Jeffreys divergence*,
    which retains the best of both approaches. Furthermore, to optimize performance
    while keeping a reduced sample-complexity, we propose an *iterative* BOND approach
    which consists of iteratively distilling the Best-of-N of a moving anchor policy.
    Finally, based on the aforementioned ideas, we propose J-BOND (J for Jeffreys),
    a novel, stable, efficient and practical RLHF algorithm to align LLMs.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 'BOND。在本文中，我们提出了BOND（最佳-N蒸馏），这是一种新型RLHF算法，它*学习*一种策略，以实现最佳-N采样的强大性能，但关键是仅在推断时需要一个样本，如[图1](#S1.F1
    "In 1 Introduction ‣ BOND: Aligning LLMs with Best-of-N Distillation")所示。我们的关键思想是将策略的对齐视为一个分布匹配问题，我们微调策略以模拟最佳-N分布。为此，我们首先推导出最佳-N分布的解析表达式。这使我们能够考虑和优化不同的散度度量。我们首先展示了如何最小化*前向*
    $\mathrm{KL}$，也称为*Jeffreys散度*，这保留了两种方法的优点。此外，为了优化性能同时保持较低的样本复杂度，我们提出了一种*迭代* BOND方法，该方法包括迭代蒸馏一个移动锚策略的最佳-N。最后，基于上述思想，我们提出了J-BOND（J代表Jeffreys），一种新颖、稳定、高效且实用的RLHF算法，用于对齐LLMs。'
- en: 'Experiments. We first demonstrate the effectiveness of BOND and of our design
    choices on the abstractive summarization XSum (Narayan et al., [2018](#bib.bib37))
    task. Then, in [Section 6](#S6 "6 Experiments ‣ BOND: Aligning LLMs with Best-of-N
    Distillation"), we apply J-BOND to align Gemma (Gemma Team, [2024](#bib.bib22))
    policies. J-BOND not only improves the $\mathrm{KL}$-reward Pareto front compared
    to standard RL algorithms, but also enhances performance on academic benchmarks
    and side-by-side comparisons against open-source variants such as Mixtral (Jiang
    et al., [2024](#bib.bib29)).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '实验。我们首先展示了BOND及其设计选择在抽象摘要XSum（Narayan等，[2018](#bib.bib37)）任务上的有效性。然后，在[第6节](#S6
    "6 Experiments ‣ BOND: Aligning LLMs with Best-of-N Distillation")中，我们应用J-BOND来对齐Gemma（Gemma
    Team，[2024](#bib.bib22)）策略。与标准RL算法相比，J-BOND不仅改善了$\mathrm{KL}$-奖励Pareto前沿，而且在学术基准测试和与开源变体如Mixtral（Jiang等，[2024](#bib.bib29)）的逐项比较中提升了性能。'
- en: '![Refer to caption](img/5534e08bd1941c3c246fb046ff5878a9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5534e08bd1941c3c246fb046ff5878a9.png)'
- en: 'Figure 1: Best-of-N is an *inference-time* strategy that selects the best generation
    among $N$ times from the model). In contrast, the proposed BOND approach aims
    at obtaining a fine-tuned policy that can directly sample the Best-of-N generation.
    This would inherit the quality of Best-of-N sampling, while requiring a single
    sample at inference time. We achieve this by distilling the Best-of-N strategy
    into the policy via online *distribution matching*.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：Best-of-N 是一种*推理时*策略，它在模型中从 $N$ 次生成中选择最佳生成结果。相比之下，提出的 BOND 方法旨在获得一个经过微调的策略，可以直接采样
    Best-of-N 生成结果。这将继承 Best-of-N 采样的质量，同时在推理时只需要一个样本。我们通过将 Best-of-N 策略在线 *分布匹配*
    成为策略来实现这一点。
- en: 2 Problem Setup
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题设置
- en: We consider a LLM based on the transformer (Vaswani et al., [2017](#bib.bib59))
    architecture, defining a policy $\pi(x,\cdot)$, trained to reflect human preferences.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑基于 transformer (Vaswani et al., [2017](#bib.bib59)) 架构的 LLM，定义一个策略 $\pi(x,\cdot)$，训练以反映人类偏好。
- en: 'Standard RLHF. Most RL algorithms optimize a linear combination of the expected
    reward and a $\mathrm{KL}$ divergence between the current and reference policy:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 标准 RLHF。大多数 RL 算法优化期望奖励和当前策略与参考策略之间的 $\mathrm{KL}$ 散度的线性组合：
- en: '|  | $\pi_{\text{RL}}=\operatorname{argmax}_{\pi}\mathbb{E}_{\pi}\mathopen{}\mathclose{{}\left[r(y)}\right]-\beta_{\textrm{RL}}\cdot\operatorname{KL}\mathopen{}\mathclose{{}\left(\pi\mid\mid{\pi_{\text{ref}}}}\right),$
    |  | (1) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi_{\text{RL}}=\operatorname{argmax}_{\pi}\mathbb{E}_{\pi}\mathopen{}\mathclose{{}\left[r(y)}\right]-\beta_{\textrm{RL}}\cdot\operatorname{KL}\mathopen{}\mathclose{{}\left(\pi\mid\mid{\pi_{\text{ref}}}}\right),$
    |  | (1) |'
- en: 'with regularization strength $\beta_{\textrm{RL}}\geq 0$ (Geist et al., [2019](#bib.bib20);
    Lazaridou et al., [2020](#bib.bib33)), reducing forgetting (French, [1992](#bib.bib18))
    and reward hacking (Skalse et al., [2022](#bib.bib54)). [Equation 1](#S2.E1 "In
    2 Problem Setup ‣ BOND: Aligning LLMs with Best-of-N Distillation") is usually
    optimized with online algorithms, as they perform better than their offline counterparts
    (Tang et al., [2024](#bib.bib57)). Moreover, simple methods have demonstrated
    the best results, e.g., REINFORCE (Williams, [1992](#bib.bib62)) with a sampled
    baseline for variance reduction (Li et al., [2023](#bib.bib34); Ahmadian et al.,
    [2024](#bib.bib2)) outperform PPO (Schulman et al., [2017](#bib.bib53)).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '具有正则化强度 $\beta_{\textrm{RL}}\geq 0$ (Geist et al., [2019](#bib.bib20); Lazaridou
    et al., [2020](#bib.bib33))，减少遗忘 (French, [1992](#bib.bib18)) 和奖励破解 (Skalse et
    al., [2022](#bib.bib54))。 [方程 1](#S2.E1 "在 2 问题设置 ‣ BOND: 用 Best-of-N 蒸馏对齐 LLMs")
    通常通过在线算法进行优化，因为它们的表现优于离线算法 (Tang et al., [2024](#bib.bib57))。此外，简单的方法表现最佳，例如，REINFORCE
    (Williams, [1992](#bib.bib62)) 结合采样基准以减少方差 (Li et al., [2023](#bib.bib34); Ahmadian
    et al., [2024](#bib.bib2)) 优于 PPO (Schulman et al., [2017](#bib.bib53))。'
- en: Best-of-N. A complementary alignment strategy is Best-of-N, which is an inference-time
    strategy that involves sampling multiple times from ${\pi_{\text{ref}}}$ times
    more costly than sampling a single one.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Best-of-N。一个互补的对齐策略是 Best-of-N，它是一种推理时策略，涉及从 ${\pi_{\text{ref}}}$ 中多次采样，代价比单次采样要高得多。
- en: Motivated by the above considerations, we propose a novel alignment method which
    we name BOND for Best-of-N Distillation. The goal of BOND is to *distill the Best-of-N
    strategy into the policy*. This allows the policy to reach the strong performance
    of Best-of-N sampling, while requiring only *a single sample* at inference time.
    We outline our overall approach in the next section.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于上述考虑，我们提出了一种新颖的对齐方法，我们将其命名为 BOND，即 Best-of-N 蒸馏。BOND 的目标是 *将 Best-of-N 策略蒸馏到策略中*。这使得策略能够达到
    Best-of-N 采样的强性能，同时在推理时只需要 *一个样本*。我们将在下一节中概述我们的整体方法。
- en: 3 The BOND Approach
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 BOND 方法
- en: 'We formulate the BOND approach in two main steps. First, we derive an analytical
    expression for the Best-of-N distribution ([Section 3.1](#S3.SS1 "3.1 The Best-of-N
    distribution ‣ 3 The BOND Approach ‣ BOND: Aligning LLMs with Best-of-N Distillation")).
    Second, using the derived expression, we phrase the problem as a *distribution
    matching* problem ([Section 3.2](#S3.SS2 "3.2 The BOND objective ‣ 3 The BOND
    Approach ‣ BOND: Aligning LLMs with Best-of-N Distillation")), i.e., we want to
    steer the policy closer to the Best-of-N distribution. In [Section 3.3](#S3.SS3
    "3.3 Connection with standard RLHF ‣ 3 The BOND Approach ‣ BOND: Aligning LLMs
    with Best-of-N Distillation"), we draw insightful connections between BOND and
    standard RLHF.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 BOND 方法分为两个主要步骤。首先，我们推导出 Best-of-N 分布的解析表达式（[第3.1节](#S3.SS1 "3.1 Best-of-N
    分布 ‣ 3 BOND 方法 ‣ BOND：将 LLMs 与 Best-of-N 蒸馏对齐")）。其次，利用推导出的表达式，我们将问题表述为 *分布匹配*
    问题（[第3.2节](#S3.SS2 "3.2 BOND 目标 ‣ 3 BOND 方法 ‣ BOND：将 LLMs 与 Best-of-N 蒸馏对齐")），即，我们希望将策略引导得更接近于
    Best-of-N 分布。在 [第3.3节](#S3.SS3 "3.3 与标准 RLHF 的联系 ‣ 3 BOND 方法 ‣ BOND：将 LLMs 与 Best-of-N
    蒸馏对齐")，我们揭示了 BOND 与标准 RLHF 之间的有益联系。
- en: 3.1 The Best-of-N distribution
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 Best-of-N 分布
- en: 'In this section, we derive the exact analytical distribution of Best-of-N sampling
    and study its properties. For simplicity, we drop the context $x$¹¹1To distinguish
    between generations with the same reward, ties can be broken with any arbitrary
    strict ordering.. We can affirm the following main theorem (proof in [Section A.1](#A1.SS1
    "A.1 Proof of Theorem 1 ‣ Appendix A Supporting results and derivations ‣ BOND:
    Aligning LLMs with Best-of-N Distillation")).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们推导了 Best-of-N 采样的精确解析分布并研究其性质。为了简化，我们省略了上下文 $x$¹¹1为了区分具有相同奖励的生成，可以使用任何任意严格的排序来打破平局。我们可以确认以下主要定理（证明见
    [第A.1节](#A1.SS1 "A.1 定理 1 的证明 ‣ 附录 A 支持结果与推导 ‣ BOND：将 LLMs 与 Best-of-N 蒸馏对齐")）。
- en: Theorem 1.
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 1。
- en: For any generation $y$, let
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何生成的 $y$，设
- en: '|  | ${p_{<}}(y)=\mathbb{P}_{y^{\prime}\sim{\pi_{\text{ref}}}}\mathopen{}\mathclose{{}\left[r(y^{\prime})<r(y)}\right]$
    |  | (2) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | ${p_{<}}(y)=\mathbb{P}_{y^{\prime}\sim{\pi_{\text{ref}}}}\mathopen{}\mathclose{{}\left[r(y^{\prime})<r(y)}\right]$
    |  | (2) |'
- en: denote the probability that a random generation $y^{\prime}$ and let
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 表示随机生成 $y^{\prime}$ 的概率，并设
- en: '|  | ${p_{\leq}}(y)=\mathbb{P}_{y^{\prime}\sim{\pi_{\text{ref}}}}\mathopen{}\mathclose{{}\left[r(y^{\prime})\leq
    r(y)}\right],$ |  | (3) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | ${p_{\leq}}(y)=\mathbb{P}_{y^{\prime}\sim{\pi_{\text{ref}}}}\mathopen{}\mathclose{{}\left[r(y^{\prime})\leq
    r(y)}\right],$ |  | (3) |'
- en: the probability that $y^{\prime}$ is the output of Best-of-N sampling is given
    by
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: $y^{\prime}$ 是 Best-of-N 采样的输出的概率为
- en: '|  | $1$2 |  | (4) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: 'Interpretation. [Theorem 1](#Thmtheorem1 "Theorem 1\. ‣ 3.1 The Best-of-N distribution
    ‣ 3 The BOND Approach ‣ BOND: Aligning LLMs with Best-of-N Distillation") provides
    an intuitive explanation on the behavior of Best-of-N sampling: it essentially
    reweights the original sampling distribution ${\pi_{\text{ref}}}$.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 解释。[定理 1](#Thmtheorem1 "定理 1 ‣ 3.1 Best-of-N 分布 ‣ 3 BOND 方法 ‣ BOND：将 LLMs 与
    Best-of-N 蒸馏对齐") 提供了对 Best-of-N 采样行为的直观解释：它实质上重新加权了原始采样分布 ${\pi_{\text{ref}}}$。
- en: The term $\mathtt{(A)}$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 $\mathtt{(A)}$。
- en: 'The term $\mathtt{(B)}$:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 $\mathtt{(B)}$：
- en: '|  | $1$2 |  | (5) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: 'It achieves its minimum at $1$ (see [Section A.2](#A1.SS2 "A.2 Link to the
    continuous case ‣ Appendix A Supporting results and derivations ‣ BOND: Aligning
    LLMs with Best-of-N Distillation")).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 它在 $1$ 时达到最小值（参见 [第A.2节](#A1.SS2 "A.2 连续案例的链接 ‣ 附录 A 支持结果与推导 ‣ BOND：将 LLMs 与
    Best-of-N 蒸馏对齐")）。
- en: 3.2 The BOND objective
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 BOND 目标
- en: 'The analytical characterization of the Best-of-N distribution allows us to
    formulate BOND as a distribution matching problem. That is, we want to solve the
    objective:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Best-of-N 分布的解析表征使我们能够将 BOND 表述为分布匹配问题。即，我们希望解决以下目标：
- en: '|  | $\pi_{\texttt{BOND}}=\arg\min_{\pi\in\Pi}\,D(\pi\,\&#124;\,{\pi_{\text{BoN}}}),$
    |  | (6) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi_{\texttt{BOND}}=\arg\min_{\pi\in\Pi}\,D(\pi\,\&\#124;\,{\pi_{\text{BoN}}}),$
    |  | (6) |'
- en: 'where $D(\cdot\,\|\,\cdot)$ from online and offline samples. We defer the choice
    of suitable divergences and resulting BOND algorithms to [Section 4](#S4 "4 BOND
    Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D(\cdot\,\|\,\cdot)$ 来自在线和离线样本。我们将选择合适的散度和结果 BOND 算法的决策推迟到 [第4节](#S4 "4
    BOND 挑战与算法 ‣ BOND：将 LLMs 与 Best-of-N 蒸馏对齐")。
- en: 3.3 Connection with standard RLHF
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 与标准 RLHF 的联系
- en: 'In this section, we draw important connections between the two seemingly different
    objectives of standard RLHF ([Equation 1](#S2.E1 "In 2 Problem Setup ‣ BOND: Aligning
    LLMs with Best-of-N Distillation")) and BOND ([Equation 6](#S3.E6 "In 3.2 The
    BOND objective ‣ 3 The BOND Approach ‣ BOND: Aligning LLMs with Best-of-N Distillation")).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们描绘了标准 RLHF ([方程 1](#S2.E1 "在 2 问题设置 ‣ BOND: 使 LLM 与 Best-of-N 蒸馏对齐"))
    和 BOND ([方程 6](#S3.E6 "在 3.2 BOND 目标 ‣ 3 BOND 方法 ‣ BOND: 使 LLM 与 Best-of-N 蒸馏对齐"))
    这两个看似不同的目标之间的重要联系。'
- en: 'It is well known (see, e.g., Vieillard et al. ([2020](#bib.bib60)); Rafailov
    et al. ([2023](#bib.bib47))) that the policy maximizing the RLHF objective from
    [Equation 1](#S2.E1 "In 2 Problem Setup ‣ BOND: Aligning LLMs with Best-of-N Distillation")
    is:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '众所周知（见例如 Vieillard 等人 ([2020](#bib.bib60))；Rafailov 等人 ([2023](#bib.bib47)))，最大化[方程
    1](#S2.E1 "在 2 问题设置 ‣ BOND: 使 LLM 与 Best-of-N 蒸馏对齐") 中 RLHF 目标的策略是：'
- en: '|  | $\pi_{\text{RL}}(y)\propto{\pi_{\text{ref}}}(y)\exp\mathopen{}\mathclose{{}\left(\frac{1}{\beta}_{\textrm{RL}}r(y)}\right).$
    |  | (7) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi_{\text{RL}}(y)\propto{\pi_{\text{ref}}}(y)\exp\mathopen{}\mathclose{{}\left(\frac{1}{\beta}_{\textrm{RL}}r(y)}\right).$
    |  | (7) |'
- en: 'From the derived expression of ${\pi_{\text{BoN}}}$ in [Theorem 1](#Thmtheorem1
    "Theorem 1\. ‣ 3.1 The Best-of-N distribution ‣ 3 The BOND Approach ‣ BOND: Aligning
    LLMs with Best-of-N Distillation"), we see that the Best-of-N sampling distribution
    coincides with the optimal solution of standard RLHF when using the following
    specific BOND reward:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '从[定理 1](#Thmtheorem1 "定理 1. ‣ 3.1 最佳-N 分布 ‣ 3 BOND 方法 ‣ BOND: 使 LLM 与 Best-of-N
    蒸馏对齐")中推导出的 ${\pi_{\text{BoN}}}$ 表达式中，我们看到，当使用以下特定的 BOND 奖励时，Best-of-N 采样分布与标准
    RLHF 的最优解一致：'
- en: '|  | $1$2 |  | (8) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (8) |'
- en: 'and the specific regularization strength $\beta_{\text{BOND}}=\frac{1}{N-1}$.
    This provides two interesting insights for Best-of-N sampling:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 和具体的正则化强度 $\beta_{\text{BOND}}=\frac{1}{N-1}$。这为 Best-of-N 采样提供了两个有趣的见解：
- en: '1.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Best-of-N sampling corresponds to the solution of a standard $\mathrm{KL}$ determines
    the level of KL regularization.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Best-of-N 采样对应于标准 $\mathrm{KL}$ 解决方案，该解决方案决定了 KL 正则化的水平。
- en: '2.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Best-of-N sampling corresponds to optimizing the expected log reward quantile,
    i.e., the log likelihood that the generation has larger reward than a random sample
    from the reference distribution. Interestingly, due to the concavity of the logarithm,
    $r_{\texttt{BOND}}(y)$ more robust to reward hacking compared to standard RLHF.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Best-of-N 采样对应于优化期望对数奖励分位数，即生成具有比参考分布的随机样本更高奖励的对数似然性。有趣的是，由于对数的凹性，$r_{\texttt{BOND}}(y)$
    比标准 RLHF 对奖励作弊更具鲁棒性。
- en: 'The connection to RLHF also inspires the proposed approach in this manuscript:
    if we can compute the BOND reward or equivalently the Best-of-N distribution ${\pi_{\text{BoN}}}$,
    then we can steer the policy towards Best-of-N via distribution matching. In the
    next section we explore different algorithms to tackle the main underlying challenges.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与 RLHF 的联系也激发了本文中提出的方法：如果我们能计算 BOND 奖励或等效地计算 Best-of-N 分布 ${\pi_{\text{BoN}}}$，那么我们可以通过分布匹配将策略引导到
    Best-of-N。在下一节中，我们探索不同的算法来应对主要的潜在挑战。
- en: 4 BOND Challenges and Algorithms
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 BOND 挑战和算法
- en: 'Implementing the BOND approach induces the three following challenges: *(1)*
    how to estimate the reward quantiles, *(2)* which is the appropriate divergence
    metric to use, and *(3)* how to choose the hyperparameter $N$ representing the
    number of sampled generations in Best-of-N. We discuss and address these challenges
    in the next three subsections.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 实施 BOND 方法会带来以下三个挑战：*(1)* 如何估计奖励分位数，*(2)* 应使用哪种适当的散度度量，以及 *(3)* 如何选择表示 Best-of-N
    中采样生成数的超参数 $N$。我们在接下来的三个小节中讨论并解决这些挑战。
- en: 4.1 Monte-Carlo quantile estimation
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 蒙特卡洛分位数估计
- en: One key difficulty in estimating the ${\pi_{\text{BoN}}}$ distribution is that
    we need to estimate the quantile
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 估计 ${\pi_{\text{BoN}}}$ 分布的一个关键困难是我们需要估计分位数
- en: '|  | ${p_{\leq}}(y)=\mathbb{P}_{y^{\prime}\sim{\pi_{\text{ref}}}}\mathopen{}\mathclose{{}\left[r(y^{\prime})\leq
    r(y)}\right],$ |  | (9) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | ${p_{\leq}}(y)=\mathbb{P}_{y^{\prime}\sim{\pi_{\text{ref}}}}\mathopen{}\mathclose{{}\left[r(y^{\prime})\leq
    r(y)}\right],$ |  | (9) |'
- en: 'of a given generation $y$ and obtaining the following empirical estimate:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 给定生成 $y$ 并获得以下经验估计：
- en: '|  | $\hat{p}_{\leq}(y)=\frac{1}{k}\sum_{i=1}^{k}\mathbb{I}\{r(y_{i})\leq r(y)\}.$
    |  | (10) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{p}_{\leq}(y)=\frac{1}{k}\sum_{i=1}^{k}\mathbb{I}\{r(y_{i})\leq r(y)\}.$
    |  | (10) |'
- en: 'We found this to be a very effective in our experiments, even with a limited
    number of samples. In principle, though, one could also use alternative approaches,
    e.g., training a learned quantile model (as we explore in [Section B.1](#A2.SS1
    "B.1 Learned quantile models ‣ Appendix B Additional Experiments ‣ BOND: Aligning
    LLMs with Best-of-N Distillation")).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '我们发现这在我们的实验中非常有效，即使样本数量有限。然而，原则上，也可以使用替代方法，例如训练一个学习的分位数模型（正如我们在[第B.1节](#A2.SS1
    "B.1 Learned quantile models ‣ Appendix B Additional Experiments ‣ BOND: Aligning
    LLMs with Best-of-N Distillation")中探讨的）。'
- en: 4.2 Jeffreys divergence as a robust objective
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 Jeffreys散度作为一种稳健的目标
- en: 'The choice of the divergence metric used in BOND is of crucial importance:
    different divergences can steer the policy to very different solutions. Here,
    we propose the *Jeffreys divergence* as a robust distribution matching objective.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: BOND中使用的散度度量的选择至关重要：不同的散度可以将策略引导到非常不同的解决方案。在这里，我们提出*Jeffreys散度*作为一种稳健的分布匹配目标。
- en: 'The Jeffreys divergence (Jeffreys, [1946](#bib.bib28)) between two distributions
    is defined as:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Jeffreys散度（Jeffreys，[1946](#bib.bib28)）在两个分布之间定义为：
- en: '|  | $J_{\text{effreys}}^{\beta}(p\,\&#124;\,q):=(1-\beta)\cdot\underbrace{\text{KL}(q\,\&#124;\,p)}_{\text{Forward
    KL}}\,+\,\beta\cdot\underbrace{\text{KL}(p\,\&#124;\,q)}_{\text{Backward KL}}.$
    |  | (11) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $J_{\text{effreys}}^{\beta}(p\,\|\,q):=(1-\beta)\cdot\underbrace{\text{KL}(q\,\|\,p)}_{\text{Forward
    KL}}\,+\,\beta\cdot\underbrace{\text{KL}(p\,\|\,q)}_{\text{Backward KL}}.$ |  |
    (11) |'
- en: The (generalized) Jeffreys divergence is a weighted average (with weight $\beta\in[0,1]$
    can lead to policy and entropy collapses. Instead, we empirically show that the
    Jeffreys divergence inherits the best of both divergences, producing better aligned
    policies.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: （广义的）Jeffreys散度是加权平均（权重$\beta\in[0,1]$可能导致策略和熵崩溃。相反，我们通过实验证明Jeffreys散度继承了两种散度的优点，产生了更好对齐的策略。
- en: In the context of BOND, this translates into minimizing the divergence $J_{\text{effreys}}^{\beta}(\pi\,\|\,{\pi_{\text{BoN}}})$
    as follows.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在BOND的背景下，这转化为最小化散度$J_{\text{effreys}}^{\beta}(\pi\,\|\,{\pi_{\text{BoN}}})$，具体如下。
- en: Estimation of the forward KL. The forward KL defined as
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正向KL的估计。正向KL定义为
- en: '|  | $\text{KL}({\pi_{\text{BoN}}}\,\&#124;\,\pi)=\mathbb{E}_{y\sim{\pi_{\text{BoN}}}}\mathopen{}\mathclose{{}\left[\log{\pi_{\text{BoN}}}(y)-\log\pi(y)}\right]$
    |  | (12) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{KL}({\pi_{\text{BoN}}}\,\|\,\pi)=\mathbb{E}_{y\sim{\pi_{\text{BoN}}}}\mathopen{}\mathclose{{}\left[\log{\pi_{\text{BoN}}}(y)-\log\pi(y)}\right]$
    |  | (12) |'
- en: 'can be estimated directly drawing samples from the ${\pi_{\text{BoN}}}$ and
    selecting the best one) and can be seen as a supervised fine-tuning loss on the
    Best-of-N samples:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过直接从${\pi_{\text{BoN}}}$中抽样并选择最佳样本来估计，并且可以视为对Best-of-N样本的监督微调损失：
- en: '|  | $\nabla_{\pi}\text{KL}({\pi_{\text{BoN}}}\,\&#124;\,\pi)=-\mathbb{E}_{y\sim{\pi_{\text{BoN}}}}\nabla\log\pi(y).$
    |  | (13) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla_{\pi}\text{KL}({\pi_{\text{BoN}}}\,\|\,\pi)=-\mathbb{E}_{y\sim{\pi_{\text{BoN}}}}\nabla\log\pi(y).$
    |  | (13) |'
- en: Estimation of the backward KL. The backward KL defined as
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 反向KL的估计。反向KL定义为
- en: '|  | $\text{KL}(\pi\,\&#124;\,{\pi_{\text{BoN}}})=\mathbb{E}_{y\sim\pi}\mathopen{}\mathclose{{}\left[\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right]$
    |  | (14) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{KL}(\pi\,\|\,{\pi_{\text{BoN}}})=\mathbb{E}_{y\sim\pi}\mathopen{}\mathclose{{}\left[\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right]$
    |  | (14) |'
- en: 'can be estimated from the policy samples (note the expectation w.r.t. $\pi$.
    In particular, by the analogies drawn in [Section 3.3](#S3.SS3 "3.3 Connection
    with standard RLHF ‣ 3 The BOND Approach ‣ BOND: Aligning LLMs with Best-of-N
    Distillation"), we show (in [Section A.3](#A1.SS3 "A.3 Backward KL and policy
    gradient equivalence ‣ Appendix A Supporting results and derivations ‣ BOND: Aligning
    LLMs with Best-of-N Distillation")) that its gradient coincides with a policy
    gradient (e.g., used by REINFORCE (Williams, [1992](#bib.bib62)) in standard RLHF):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '可以从策略样本中估计（注意期望值相对于$\pi$。特别是，通过在[第3.3节](#S3.SS3 "3.3 Connection with standard
    RLHF ‣ 3 The BOND Approach ‣ BOND: Aligning LLMs with Best-of-N Distillation")中绘制的类比，我们展示（在[第A.3节](#A1.SS3
    "A.3 Backward KL and policy gradient equivalence ‣ Appendix A Supporting results
    and derivations ‣ BOND: Aligning LLMs with Best-of-N Distillation")）其梯度与策略梯度（例如，标准RLHF中由REINFORCE（Williams，[1992](#bib.bib62)）使用）一致：'
- en: '|  | $1$2 |  | (15) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (15) |'
- en: with the equivalent reward $r_{\texttt{BOND}}$. Moreover, to reduce the resulting
    variance, we use a policy gradient baseline (Sutton and Barto, [1998](#bib.bib56))
    which we compute as the average return for the other generations in the batch.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 具有等效的奖励$r_{\texttt{BOND}}$。此外，为了减少结果的方差，我们使用策略梯度基线（Sutton和Barto，[1998](#bib.bib56)），其计算方法是批次中其他生成的平均回报。
- en: Thus, the overall $J_{\text{effreys}}^{\beta}$ loss is a linear weighted combination
    between a supervised fine-tuning and a policy gradient loss.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，整体的 $J_{\text{effreys}}^{\beta}$ 损失是监督微调和策略梯度损失之间的线性加权组合。
- en: 'Experiments. In [Figure 2](#S4.F2 "In 4.2 Jeffreys divergence as a robust objective
    ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation"),
    we consider the abstractive summarization XSum task (Narayan et al., [2018](#bib.bib37))
    with ${\pi_{\text{ref}}}$) lags behind.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 实验。在 [图 2](#S4.F2 "在 4.2 Jeffreys 散度作为一个稳健的目标 ‣ 4 BOND 挑战和算法 ‣ BOND：与 Best-of-N
    蒸馏对齐 LLMs") 中，我们考虑了抽象总结 XSum 任务 (Narayan et al., [2018](#bib.bib37))，其中 ${\pi_{\text{ref}}}$)
    落后。
- en: '![Refer to caption](img/773f1ad3e6c325e9ee7a6641063840e1.png)![Refer to caption](img/fd7fa408947dbbdfcef6be2cab43ed28.png)![Refer
    to caption](img/02e2cd47fb765f1e3e281461d11c6870.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/773f1ad3e6c325e9ee7a6641063840e1.png)![参见图注](img/fd7fa408947dbbdfcef6be2cab43ed28.png)![参见图注](img/02e2cd47fb765f1e3e281461d11c6870.png)'
- en: 'Figure 2: BOND with $N=8$ are reported in [Section B.2](#A2.SS2 "B.2 Additional
    plots for BOND with Jeffreys divergence objective ‣ Appendix B Additional Experiments
    ‣ BOND: Aligning LLMs with Best-of-N Distillation").'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：BOND 与 $N=8$ 的结果见 [附录 B.2](#A2.SS2 "B.2 具有 Jeffreys 散度目标的 BOND 额外图 ‣ 附录
    B 额外实验 ‣ BOND：与 Best-of-N 蒸馏对齐 LLMs")。
- en: 4.3 Iterative BOND
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 迭代 BOND
- en: Finally, we discuss the choice of the parameter $N$.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了参数 $N$ 的选择。
- en: 'To address the above challenges, we propose the *iterative* BOND approach.
    The approach is inspired by the fact that Best-of-N sampling from a Best-of-N
    distribution, coincides with Best-of-N² sampling from the original distribution.
    More generally, by informally defining $\text{Bo$N$}(\cdot)$ as an operator that
    performs Best-of-N sampling from a base distribution, we have:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对上述挑战，我们提出了*迭代* BOND 方法。该方法的灵感来自于 Best-of-N 分布的 Best-of-N 采样与原始分布的 Best-of-N²
    采样一致这一事实。更一般地，通过非正式地定义 $\text{Bo$N$}(\cdot)$ 为一个从基本分布中执行 Best-of-N 采样的算子，我们有：
- en: '|  | $\underbrace{\text{Bo{N}}(\cdots\text{Bo{N}}(\text{Bo{N}}(}_{M\text{ times}}{\pi_{\text{ref}}})))\,\equiv\,\text{Bo{N}${}^{M}$}({\pi_{\text{ref}}}).$
    |  | (16) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underbrace{\text{Bo{N}}(\cdots\text{Bo{N}}(\text{Bo{N}}(}_{M\text{ 次}}{\pi_{\text{ref}}})))\,\equiv\,\text{Bo{N}${}^{M}$}({\pi_{\text{ref}}}).$
    |  | (16) |'
- en: 'This suggests the key idea behind iterative BOND: if we know how to distill
    the Best-of-N distribution (i.e., via BOND), then we can apply BOND recursively
    (say $M$. The overall approach is depicted in [Figure 3](#S4.F3 "In 4.3 Iterative
    BOND ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation")
    and summarized in [Algorithm 1](#alg1 "In 4.3 Iterative BOND ‣ 4 BOND Challenges
    and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明了迭代 BOND 的关键思想：如果我们知道如何蒸馏 Best-of-N 分布（即通过 BOND），那么我们可以递归地应用 BOND（例如 $M$）。整体方法在
    [图 3](#S4.F3 "在 4.3 迭代 BOND ‣ 4 BOND 挑战和算法 ‣ BOND：与 Best-of-N 蒸馏对齐 LLMs") 中描绘，并在
    [算法 1](#alg1 "在 4.3 迭代 BOND ‣ 4 BOND 挑战和算法 ‣ BOND：与 Best-of-N 蒸馏对齐 LLMs") 中总结。
- en: '![Refer to caption](img/5433d7e76346064327e61db61f21e541.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/5433d7e76346064327e61db61f21e541.png)'
- en: 'Figure 3: Iterative BOND approach. The policy $\pi_{t}$ is used at each distillation
    step.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：迭代 BOND 方法。在每个蒸馏步骤中使用策略 $\pi_{t}$。
- en: In a nutshell, iterative BOND allows exponential scaling to arbitrary large
    $N$ in advance) while keeping a reduced sample complexity and a stable optimization.
    The claim is validated in the results below.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，迭代 BOND 允许在提前对任意大的 $N$ 进行指数扩展，同时保持较低的样本复杂度和稳定的优化。该声明在下面的结果中得到了验证。
- en: 'Experiments. In [Figure 4](#S4.F4 "In 4.3 Iterative BOND ‣ 4 BOND Challenges
    and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation"), we consider
    the same experimental setup than in [Figure 2](#S4.F2 "In 4.2 Jeffreys divergence
    as a robust objective ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs
    with Best-of-N Distillation") from [Section 4.2](#S4.SS2 "4.2 Jeffreys divergence
    as a robust objective ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs
    with Best-of-N Distillation"), fix the BOND objective to $J_{\text{effreys}}^{0.5}$.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 实验。在 [图 4](#S4.F4 "在 4.3 迭代 BOND ‣ 4 BOND 挑战和算法 ‣ BOND：与 Best-of-N 蒸馏对齐 LLMs")
    中，我们考虑了与 [图 2](#S4.F2 "在 4.2 Jeffreys 散度作为一个稳健的目标 ‣ 4 BOND 挑战和算法 ‣ BOND：与 Best-of-N
    蒸馏对齐 LLMs") 中相同的实验设置，并将 BOND 目标固定为 $J_{\text{effreys}}^{0.5}$。
- en: Algorithm 1 Iterative BOND (meta algorithm)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 迭代 BOND（元算法）
- en: 'Inputs: ${\pi_{\text{ref}}}$![Refer to caption](img/d4ecdfae7ce47c9ff665e41fe3803ca2.png)![Refer
    to caption](img/23a62a9c9e0f69d8bb9960cd09a3186d.png)![Refer to caption](img/c22d19890adb1ee1a4f84a0c307732cc.png)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：${\pi_{\text{ref}}}$![参见说明](img/d4ecdfae7ce47c9ff665e41fe3803ca2.png)![参见说明](img/23a62a9c9e0f69d8bb9960cd09a3186d.png)![参见说明](img/c22d19890adb1ee1a4f84a0c307732cc.png)
- en: 'Figure 4: Iterative BOND (with $n=2$.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：迭代 BOND（$n=2$）。
- en: 5 The J-BOND Algorithm
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 5 节 J-BOND 算法
- en: 'In this section we present J-BOND, a concrete and practical BOND algorithm
    motivated by the results discussed in the previous sections. We describe its main
    components below, and summarize it in the pseudo-code of [Algorithm 2](#alg2 "In
    5 The J-BOND Algorithm ‣ BOND: Aligning LLMs with Best-of-N Distillation").'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了 J-BOND，这是一种具体而实用的 BOND 算法，受前面讨论结果的启发。我们在下面描述其主要组成部分，并在 [算法 2](#alg2
    "在第 5 节 J-BOND 算法 ‣ BOND：与最佳 N 提炼对齐的 LLM") 的伪代码中总结。
- en: 'J-BOND follows the template of iterative BOND ([Algorithm 1](#alg1 "In 4.3
    Iterative BOND ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N
    Distillation")) with $n=2$ as defined in [Section 4.2](#S4.SS2 "4.2 Jeffreys divergence
    as a robust objective ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs
    with Best-of-N Distillation").'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: J-BOND 遵循迭代 BOND 的模板（[算法 1](#alg1 "在第 4.3 节 迭代 BOND ‣ 4 BOND 挑战和算法 ‣ BOND：与最佳
    N 提炼对齐的 LLM")），$n=2$，如 [第 4.2 节](#S4.SS2 "4.2 Jeffreys 发散作为一种鲁棒目标 ‣ 4 BOND 挑战和算法
    ‣ BOND：与最佳 N 提炼对齐的 LLM") 中定义。
- en: 'Minimal sample complexity. Compared to the BOND algorithms tested in the previous
    section, J-BOND has a minimal sample complexity: for each prompt in the batch
    it generates $1$. While more anchor samples are generally useful for a better
    divergence estimation (in Section [4](#S4 "4 BOND Challenges and Algorithms ‣
    BOND: Aligning LLMs with Best-of-N Distillation") we used 16 MC samples), autoregressive
    sampling is the main bottleneck of online RLHF and we have thus opted for a practical
    approach working with a small number of samples.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最小样本复杂度。与前一节测试的 BOND 算法相比，J-BOND 具有最小的样本复杂度：对于批处理中的每个提示，它生成 $1$。虽然更多的锚点样本通常有助于更好的发散估计（在第
    [4](#S4 "4 BOND 挑战和算法 ‣ BOND：与最佳 N 提炼对齐的 LLM") 节中我们使用了 16 个 MC 样本），但自回归采样是在线 RLHF
    的主要瓶颈，因此我们选择了处理少量样本的实用方法。
- en: Algorithm 2 The J-BOND algorithm
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 J-BOND 算法
- en: 'Inputs: Prompt dataset $\mathcal{D}$ with the overall stochastic gradient:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：包含总体随机梯度的提示数据集 $\mathcal{D}$：
- en: '|  | $1$2 |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '*/ Update moving anchor */     Update anchor weights with EMA:  $\theta_{\text{anchor}}^{t+1}\leftarrow(1-\eta)\cdot\theta_{\text{anchor}}^{t}+\eta\cdot\theta_{t+1}$'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*/ 更新移动锚点 */      使用 EMA 更新锚点权重：  $\theta_{\text{anchor}}^{t+1}\leftarrow(1-\eta)\cdot\theta_{\text{anchor}}^{t}+\eta\cdot\theta_{t+1}$'
- en: Crude divergence estimate based on 2 anchor samples. The policy and anchor samples
    are used to obtain a crude estimate of the forward and backward KL components
    of $J_{\text{effreys}}^{\beta}(\pi\,\|\,\text{Best-of-2}(\pi_{\text{anchor}}^{t}))$
    as described next.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 2 个锚点样本的粗略发散估计。策略和锚点样本用于获得 $J_{\text{effreys}}^{\beta}(\pi\,\|\,\text{Best-of-2}(\pi_{\text{anchor}}^{t}))$
    的前向和后向 KL 组件的粗略估计，如下所述。
- en: 'We can minimize the *forward KL* as described in [Section 4.2](#S4.SS2 "4.2
    Jeffreys divergence as a robust objective ‣ 4 BOND Challenges and Algorithms ‣
    BOND: Aligning LLMs with Best-of-N Distillation"), by doing supervised fine-tuning
    on the best of the 2 anchor samples. To minimize the *backward KL*, we utilize
    the policy gradient-style loss of [Equation 15](#S4.E15 "In 4.2 Jeffreys divergence
    as a robust objective ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs
    with Best-of-N Distillation") replacing $r_{\texttt{{BOND}\@}}(y)$ as'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在 2 个锚点样本中的最佳样本上进行监督微调，来最小化 *前向 KL*，如 [第 4.2 节](#S4.SS2 "4.2 Jeffreys
    发散作为一种鲁棒目标 ‣ 4 BOND 挑战和算法 ‣ BOND：与最佳 N 提炼对齐的 LLM") 所述。为了最小化 *后向 KL*，我们利用 [方程 15](#S4.E15
    "在第 4.2 节 Jeffreys 发散作为一种鲁棒目标 ‣ 4 BOND 挑战和算法 ‣ BOND：与最佳 N 提炼对齐的 LLM") 中的策略梯度风格损失，将
    $r_{\texttt{{BOND}\@}}(y)$ 替换为
- en: '|  | $1$2 |  | (17) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (17) |'
- en: 'That is, generation $y$ reward otherwise. The above definition is motivated
    by the following two main reasons:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 即，生成 $y$ 奖励其他情况。上述定义的动机有两个主要原因：
- en: (i)
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (i)
- en: We negatively reward $y$.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对 $y$ 进行负奖励。
- en: (ii)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (ii)
- en: We choose value $-\log(16)$).
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们选择值 $-\log(16)$)。
- en: 'Exponential Moving Average (EMA) anchor. An important component of J-BOND,
    which refines the vanilla iterative BOND of [Section 4.3](#S4.SS3 "4.3 Iterative
    BOND ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation"),
    is the use of an *Exponential Moving Average (EMA)* anchor. That is, instead of
    using a periodically updated anchor, we update the anchor weights $\theta_{\text{anchor}}^{t}$:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 指数移动平均（EMA）锚点。J-BOND的一个重要组成部分，是使用*指数移动平均（EMA）*锚点来优化传统的迭代BOND（见第[4.3节](#S4.SS3
    "4.3 迭代BOND ‣ 4 BOND挑战与算法 ‣ BOND：将LLM与最佳之N蒸馏对齐")）。即，不使用定期更新的锚点，而是更新锚点权重$\theta_{\text{anchor}}^{t}$：
- en: '|  | $\theta_{\text{anchor}}^{t+1}\leftarrow(1-\eta)\cdot\theta_{\text{anchor}}^{t}+\eta\cdot\theta_{t+1}.$
    |  | (18) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta_{\text{anchor}}^{t+1}\leftarrow(1-\eta)\cdot\theta_{\text{anchor}}^{t}+\eta\cdot\theta_{t+1}.$
    |  | (18) |'
- en: 'Consistently with WARP (Ramé et al., [2024](#bib.bib50)), we observed that
    this weight averaging procedure has a positive effect on training stability by
    reducing variance, and can improve the overall reward/KL trade-off of J-BOND.
    We provide an ablation in [Section 6](#S6 "6 Experiments ‣ BOND: Aligning LLMs
    with Best-of-N Distillation").'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与WARP（Ramé等，[2024](#bib.bib50)）一致，我们观察到这种权重平均程序对训练稳定性有积极影响，通过减少方差来改善J-BOND的整体奖励/KL权衡。我们在[第6节](#S6
    "6 实验 ‣ BOND：将LLM与最佳之N蒸馏对齐")中提供了一个消融实验。
- en: 'Additional KL regularization. Finally, we further regularize the policy to
    stay closer to the moving anchor via an extra²²2Note that $\mathrm{KL}$. The scope
    is to further stabilize the policy updates, viewing the overall operator as a
    constrained optimization one:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的KL正则化。最后，我们通过额外的KL正则化进一步规范策略，使其更接近移动锚点。其目的是进一步稳定策略更新，将整体运算符视为受约束的优化问题：
- en: '|  | $1$2 |  | (19) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (19) |'
- en: 6 Experiments
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验
- en: 'We test J-BOND on relevant use cases with the following main goals. First,
    we ablate and showcase important aspects of J-BOND: the benefits of the EMA anchor,
    and the effects of the anchor speed and the additional KL regularization. Then,
    we compare J-BOND to classical RLHF baselines using REINFORCE, demonstrating its
    efficacy and better performance.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在相关用例上测试J-BOND，主要有以下目标。首先，我们分析并展示J-BOND的关键方面：EMA锚点的好处、锚点速度的影响以及额外的KL正则化。然后，我们将J-BOND与使用REINFORCE的经典RLHF基线进行比较，展示其有效性和更好的性能。
- en: '![Refer to caption](img/68fd781ee47aa10faf02150c5a645610.png)![Refer to caption](img/b1ce09808b8a400096f9203b5b3d3752.png)![Refer
    to caption](img/9a048be88a8e87cfb2f167bcdb1370ca.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/68fd781ee47aa10faf02150c5a645610.png)![参考说明](img/b1ce09808b8a400096f9203b5b3d3752.png)![参考说明](img/9a048be88a8e87cfb2f167bcdb1370ca.png)'
- en: 'Figure 5: J-BOND with periodic anchor updates (every 50 steps) vs. EMA anchor
    ($\eta=0.02$) on Gemma 7B. While attaining the same reward (left), using the EMA
    anchor displays a significantly lower KL than the reference policy (middle) and
    thus a better reward/KL trade-off (right).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：J-BOND与定期锚点更新（每50步）对比EMA锚点（$\eta=0.02$）在Gemma 7B上的表现。虽然获得相同的奖励（左），但使用EMA锚点显示出显著较低的KL值（中），从而实现更好的奖励/KL权衡（右）。
- en: Setup. We consider Gemma (2B and 7B) models (Gemma Team, [2024](#bib.bib22))
    which we aim to fine-tune into better conversational agents. For this task, we
    consider a set of conversational prompts $\mathcal{D}$.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 设置。我们考虑Gemma（2B和7B）模型（Gemma团队，[2024](#bib.bib22)），我们旨在将其微调为更好的对话代理。为此任务，我们考虑了一组对话提示$\mathcal{D}$。
- en: 'EMA vs. hard anchor updates. We ablate the benefits of using an EMA moving
    anchor ([Equation 18](#S5.E18 "In 5 The J-BOND Algorithm ‣ BOND: Aligning LLMs
    with Best-of-N Distillation")) compared to the periodically updated anchor used
    in Section [4.3](#S4.SS3 "4.3 Iterative BOND ‣ 4 BOND Challenges and Algorithms
    ‣ BOND: Aligning LLMs with Best-of-N Distillation"). For this, we run J-BOND with
    $\gamma=0$ roughly corresponds to an update period of 50 steps) but, crucially,
    J-BOND *with an EMA anchor displays a significantly lower KL increase* and, as
    a result, a better reward/KL trade-off.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: EMA与硬锚更新对比。我们分析了使用EMA移动锚点（[方程 18](#S5.E18 "在5节 J-BOND算法 ‣ BOND：将LLM与最佳之N蒸馏对齐")）与第[4.3节](#S4.SS3
    "4.3 迭代BOND ‣ 4 BOND挑战与算法 ‣ BOND：将LLM与最佳之N蒸馏对齐")中使用的定期更新锚点的好处。为此，我们运行J-BOND，其中$\gamma=0$大致对应于50步的更新周期），但关键是，J-BOND
    *使用EMA锚点显示出显著较低的KL增加*，从而实现了更好的奖励/KL权衡。
- en: '![Refer to caption](img/2e14f958053225cd8a3addde46573a3f.png)![Refer to caption](img/f5e7c07a7041a97d65c212e164603c07.png)![Refer
    to caption](img/0df4845367e7cbecf9eae60186145bd1.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2e14f958053225cd8a3addde46573a3f.png)![参考说明](img/f5e7c07a7041a97d65c212e164603c07.png)![参考说明](img/0df4845367e7cbecf9eae60186145bd1.png)'
- en: 'Figure 6: J-BOND: the role of the EMA mixing parameter $\eta$, improving the
    reward/KL trade-off.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：J-BOND：EMA 混合参数 $\eta$ 的作用，改进奖励/KL 权衡。
- en: '![Refer to caption](img/2ef9a1e67eb63efca503dc0c75109bbc.png)![Refer to caption](img/4c78846d0084cdf3a9f8a709f023b4b4.png)![Refer
    to caption](img/3fa889e61390181a44f630805cca2cb0.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2ef9a1e67eb63efca503dc0c75109bbc.png)![参见说明](img/4c78846d0084cdf3a9f8a709f023b4b4.png)![参见说明](img/3fa889e61390181a44f630805cca2cb0.png)'
- en: 'Figure 7: J-BOND ($\eta=0.02$ trade-off.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：J-BOND ($\eta=0.02$ 权衡。
- en: Anchor speed and $\mathrm{KL}$ trade-off.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 锚点速度和 $\mathrm{KL}$ 权衡。
- en: 'Comparison with standard RLHF. We compare J-BOND against standard RLHF algorithms
    that aim at maximizing the $\mathrm{KL}$). This highlight a key advantage of J-BOND:
    it does not require committing to a specific regularization level, but it continuously
    improves the reward displaying a stable and linear KL increase. Moreover, in the
    rightmost plot we plot the corresponding reward/KL trade-off showing that J-BOND
    produces a better reward/KL than all of the REINFORCE baselines.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准 RLHF 的比较。我们将 J-BOND 与旨在最大化 $\mathrm{KL}$ 的标准 RLHF 算法进行比较。这突显了 J-BOND 的一个关键优势：它不需要承诺特定的正则化水平，而是持续改进奖励，表现出稳定且线性的
    KL 增加。此外，在最右侧的图中，我们绘制了相应的奖励/KL 权衡，显示 J-BOND 在奖励/KL 上优于所有 REINFORCE 基线。
- en: J-BOND for Gemma open models. J-BOND was also the algorithm used to fine-tune
    open-weight models such as Gemma 1.1 2B and 7B (Gemma Team, [2024](#bib.bib22)),
    RecurrentGemma 2B and 9B (Botev et al., [2024](#bib.bib6)) as well as CodeGemma
    1.1 (CodeGemma Team, [2024](#bib.bib12)). This led to competitive performance.
    For example, the Gemma 1.1 IT 7B model outperforms Mistral 7B v0.2 Instruct in
    both safety and instruction following (see Gemma Team ([2024](#bib.bib22), Table
    5) for more details).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: J-BOND 用于 Gemma 开放模型。J-BOND 也是用于微调开放权重模型的算法，如 Gemma 1.1 2B 和 7B（Gemma 团队，[2024](#bib.bib22)）、RecurrentGemma
    2B 和 9B（Botev 等人，[2024](#bib.bib6)）以及 CodeGemma 1.1（CodeGemma 团队，[2024](#bib.bib12)）。这导致了具有竞争力的性能。例如，Gemma
    1.1 IT 7B 模型在安全性和指令遵循方面超越了 Mistral 7B v0.2 Instruct（有关更多细节，请参见 Gemma 团队 ([2024](#bib.bib22)，表
    5)）。
- en: 7 Related Work
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 相关工作
- en: Best-of-N was introduced in  Stiennon et al. ([2020](#bib.bib55)) as a straightforward
    but costly inference method to optimize language generation against a given reward
    function. Further works established and refined an analytical form for the KL
    divergence against the reference (i.e., Bo$1$-constrained RL (Yang et al., [2024](#bib.bib64))
    and provided scaling laws for Best-of-N alignment (Gao et al., [2023](#bib.bib19)).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Best-of-N 方法在 Stiennon 等人 ([2020](#bib.bib55)) 的研究中被引入，作为一种简单但代价高昂的推断方法，用于优化语言生成以符合给定的奖励函数。进一步的研究建立并细化了
    KL 散度相对于参考的分析形式（即 Bo$1$-约束 RL（Yang 等人，[2024](#bib.bib64)）），并提供了 Best-of-N 对齐的缩放规律（Gao
    等人，[2023](#bib.bib19)）。
- en: 'Matching Best-of-N for improved alignment is a strategy that was studied in
    different flavors in the literature. Dong et al. ([2023](#bib.bib16)) and Touvron
    et al. ([2023](#bib.bib58)) propose to fine-tune LLMs in a supervised fashion
    on Best-of-N data actually applying forward $\mathrm{KL}$ minimization. Concurrently
    to ours, Gui et al. ([2024](#bib.bib24)) proposes to mimic the Best-of-N policy
    by applying a combination of supervised fine-tuning on best responses and direct
    preference optimization on best-and-worst response pairs. The latter is similar
    to a common strategy in online preference optimization methods: Guo et al. ([2024](#bib.bib25))
    use pairwise AI feedback on online generations to obtain online preferences that
    are then optimized, while Calandriello et al. ([2024](#bib.bib7)) use a dedicated
    preference reward model instead. Concurrently and closest to our work, Amini et al.
    ([2024](#bib.bib3)) also apply distribution matching in order to get the benefits
    of Best-of-N sampling with amortized cost. While their formalization is identical,
    we opt for a different divergence (i.e., Jeffreys) than the one they use (i.e.,
    only backward KL), and propose an iterative procedure with dynamic anchor, which
    we show critical for optimal results. Best-of-N can also be used for self-improvement
    in reward modeling, as evidenced in Pace et al. ([2024](#bib.bib41)).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为提高对齐效果的 Best-of-N 匹配是一种在文献中研究过的策略。Dong 等（[2023](#bib.bib16)）和 Touvron 等（[2023](#bib.bib58)）建议在
    Best-of-N 数据上以监督方式微调 LLMs，实际上应用了前向 $\mathrm{KL}$ 最小化。与我们的工作同时，Gui 等（[2024](#bib.bib24)）建议通过在最佳响应上应用监督微调和在最佳与最差响应对上进行直接偏好优化来模仿
    Best-of-N 策略。后者类似于在线偏好优化方法中的常见策略：Guo 等（[2024](#bib.bib25)）使用对在线生成的成对 AI 反馈来获得在线偏好，然后对这些偏好进行优化，而
    Calandriello 等（[2024](#bib.bib7)）则使用专门的偏好奖励模型。与我们的工作最接近的是，Amini 等（[2024](#bib.bib3)）也应用了分布匹配，以获得
    Best-of-N 采样的好处并降低成本。尽管他们的形式化与我们相同，我们选择了与他们不同的发散度（即 Jeffreys），并提出了一种具有动态锚点的迭代过程，这对实现最佳结果至关重要。Best-of-N
    还可以用于奖励建模中的自我改进，正如 Pace 等（[2024](#bib.bib41)）所证明的那样。
- en: Using a contrastive advantage is an option of J-BOND studied in prior works
    as well, which replaced a value estimate by the average Monte Carlo return of
    other samples. This was applied in the context of REINFORCE (Kool et al., [2019](#bib.bib31);
    Pinto et al., [2023](#bib.bib43)), for online RLHF (Ahmadian et al., [2024](#bib.bib2)),
    offline RLHF (Flet-Berliac et al., [2024](#bib.bib17)) and preference optimization (Wu
    et al., [2024](#bib.bib63)).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对比优势是 J-BOND 中一个选项，在以往的研究中也有涉及，该选项通过其他样本的平均蒙特卡罗回报替代值估计。这在 REINFORCE 的背景下应用（Kool
    等，[2019](#bib.bib31)；Pinto 等，[2023](#bib.bib43)），在线 RLHF（Ahmadian 等，[2024](#bib.bib2)），离线
    RLHF（Flet-Berliac 等，[2024](#bib.bib17)）和偏好优化（Wu 等，[2024](#bib.bib63)）中得到了应用。
- en: Exponential moving average (EMA) of policy as reference in regularization, which
    we use in J-BOND, is an increasingly popular option. While most alignment approaches
    use a static anchor, dynamic anchors bring the benefit of improving the flexibility
    of the policy space being explored (Munos et al., [2023](#bib.bib35); Gorbatovski
    et al., [2024](#bib.bib23); Ramé et al., [2024](#bib.bib50)), with the caveat
    that too slow updates limit optimization and too fast updates hinder stability.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在正则化中作为参考的政策指数移动平均（EMA），我们在 J-BOND 中使用，它是一种越来越受欢迎的选择。虽然大多数对齐方法使用静态锚点，但动态锚点带来了提高探索政策空间灵活性的好处（Munos
    等，[2023](#bib.bib35)；Gorbatovski 等，[2024](#bib.bib23)；Ramé 等，[2024](#bib.bib50)），但需要注意的是，更新过慢会限制优化，而更新过快会影响稳定性。
- en: Scaling post-training and iterated amplification. BOND hinges on the idea of
    investing more resources during training to ensure that computational demands
    during inference remain low, a factor often overlooked in traditional scaling
    laws (Hoffmann et al., [2022](#bib.bib27)). Specifically, BOND incorporates the
    principles of iterated amplification (Christiano et al., [2018](#bib.bib9); Cotra,
    [2018](#bib.bib13)), where amplification in this context consists of producing
    multiple generations, comparing their rewards, and using these to iteratively
    improve the policy performance. In this regard, BOND is complementary to WARM
    (Ramé et al., [2024](#bib.bib49)) and WARP (Ramé et al., [2024](#bib.bib50)),
    which previously scaled post-training by training multiple reward models and policies,
    respectively.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练和迭代放大。BOND 基于在训练期间投入更多资源的理念，以确保推理时计算需求保持低，这是传统规模定律中常被忽视的因素 (Hoffmann 等，[2022](#bib.bib27))。具体来说，BOND
    融入了迭代放大的原则 (Christiano 等，[2018](#bib.bib9)；Cotra，[2018](#bib.bib13))，在这种情况下，放大包括生成多个版本，比较它们的奖励，并利用这些奖励来迭代改进策略表现。在这方面，BOND
    补充了 WARM (Ramé 等，[2024](#bib.bib49)) 和 WARP (Ramé 等，[2024](#bib.bib50))，它们之前通过训练多个奖励模型和策略来实现后训练的扩展。
- en: 8 Conclusion
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: We introduce BOND, a novel RLHF method that fine-tunes the policy via online
    distillation of the Best-of-N sampling distribution. We propose a concrete algorithm,
    J-BOND, that integrates multiple components to enhance its practicality and efficiency;
    Monte-Carlo quantile estimation, a combination between forward and backward $\mathrm{KL}$-reward
    Pareto front of solutions, and compares favorably against state-of-the-art baselines.
    We hope this work can help improve alignment of AI systems, making them safer
    and more reliable.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 BOND，一种新颖的 RLHF 方法，通过对 Best-of-N 采样分布进行在线蒸馏来微调策略。我们提出了一种具体的算法 J-BOND，整合了多个组件以增强其实用性和效率；蒙特卡罗分位数估计、前向和后向
    $\mathrm{KL}$-奖励帕累托前沿解的组合，并且在对比最先进的基线时表现良好。我们希望这项工作能帮助提高 AI 系统的对齐性，使其更安全、更可靠。
- en: Acknowledgements
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Daniele Calandriello for insightful comments, as well as Gil Shamir,
    Bilal Piot, and Remi Munos for helpful discussions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Daniele Calandriello 的有益评论，以及 Gil Shamir、Bilal Piot 和 Remi Munos 的有益讨论。
- en: References
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Agarwal et al. (2024) R. Agarwal, N. Vieillard, Y. Zhou, P. Stanczyk, S. R.
    Garea, M. Geist, and O. Bachem. On-policy distillation of language models: Learning
    from self-generated mistakes. In *ICLR*, 2024.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等 (2024) R. Agarwal, N. Vieillard, Y. Zhou, P. Stanczyk, S. R. Garea,
    M. Geist, 和 O. Bachem. 语言模型的在线蒸馏：从自生成的错误中学习。在 *ICLR*，2024。
- en: 'Ahmadian et al. (2024) A. Ahmadian, C. Cremer, M. Gallé, M. Fadaee, J. Kreutzer,
    A. Üstün, and S. Hooker. Back to basics: Revisiting REINFORCE style optimization
    for learning from human feedback in LLMs. *arXiv preprint*, 2024.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmadian 等 (2024) A. Ahmadian, C. Cremer, M. Gallé, M. Fadaee, J. Kreutzer,
    A. Üstün, 和 S. Hooker. 回归基础：重新审视 REINFORCE 风格的优化，以从人类反馈中学习 LLMs。*arXiv 预印本*，2024。
- en: Amini et al. (2024) A. Amini, T. Vieira, and R. Cotterell. Variational Best-of-N
    alignment. *arXiv preprint*, 2024.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amini 等 (2024) A. Amini, T. Vieira, 和 R. Cotterell. 变分 Best-of-N 对齐。*arXiv 预印本*，2024。
- en: Askell et al. (2021) A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan,
    A. Jones, N. Joseph, B. Mann, N. DasSarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez,
    J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish,
    C. Olah, and J. Kaplan. A general language assistant as a laboratory for alignment.
    *arXiv preprint*, 2021.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Askell 等 (2021) A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan,
    A. Jones, N. Joseph, B. Mann, N. DasSarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez,
    J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish,
    C. Olah, 和 J. Kaplan. 作为对齐实验室的一般语言助手。*arXiv 预印本*，2021。
- en: Beirami et al. (2024) A. Beirami, A. Agarwal, J. Berant, A. D’Amour, J. Eisenstein,
    C. Nagpal, and A. T. Suresh. Theoretical guarantees on the Best-of-N alignment
    policy. *arXiv preprint*, 2024.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beirami 等 (2024) A. Beirami, A. Agarwal, J. Berant, A. D’Amour, J. Eisenstein,
    C. Nagpal, 和 A. T. Suresh. 对 Best-of-N 对齐策略的理论保证。*arXiv 预印本*，2024。
- en: 'Botev et al. (2024) A. Botev, S. De, S. L. Smith, A. Fernando, G.-C. Muraru,
    R. Haroun, L. Berrada, R. Pascanu, P. G. Sessa, R. Dadashi, L. Hussenot, J. Ferret,
    S. Girgin, O. Bachem, A. Andreev, K. Kenealy, T. Mesnard, C. Hardin, S. Bhupatiraju,
    S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love, P. Tafti, A. Joulin, N. Fiedel,
    E. Senter, Y. Chen, S. Srinivasan, G. Desjardins, D. Budden, A. Doucet, S. Vikram,
    A. Paszke, T. Gale, S. Borgeaud, C. Chen, A. Brock, A. Paterson, J. Brennan, M. Risdal,
    R. Gundluru, N. Devanathan, P. Mooney, N. Chauhan, P. Culliton, L. G. Martins,
    E. Bandy, D. Huntsperger, G. Cameron, A. Zucker, T. Warkentin, L. Peran, M. Giang,
    Z. Ghahramani, C. Farabet, K. Kavukcuoglu, D. Hassabis, R. Hadsell, Y. W. Teh,
    and N. de Frietas. Recurrentgemma: Moving past transformers for efficient open
    language models. *arXiv preprint*, 2024.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Botev 等（2024）A. Botev, S. De, S. L. Smith, A. Fernando, G.-C. Muraru, R. Haroun,
    L. Berrada, R. Pascanu, P. G. Sessa, R. Dadashi, L. Hussenot, J. Ferret, S. Girgin,
    O. Bachem, A. Andreev, K. Kenealy, T. Mesnard, C. Hardin, S. Bhupatiraju, S. Pathak,
    L. Sifre, M. Rivière, M. S. Kale, J. Love, P. Tafti, A. Joulin, N. Fiedel, E.
    Senter, Y. Chen, S. Srinivasan, G. Desjardins, D. Budden, A. Doucet, S. Vikram,
    A. Paszke, T. Gale, S. Borgeaud, C. Chen, A. Brock, A. Paterson, J. Brennan, M.
    Risdal, R. Gundluru, N. Devanathan, P. Mooney, N. Chauhan, P. Culliton, L. G.
    Martins, E. Bandy, D. Huntsperger, G. Cameron, A. Zucker, T. Warkentin, L. Peran,
    M. Giang, Z. Ghahramani, C. Farabet, K. Kavukcuoglu, D. Hassabis, R. Hadsell,
    Y. W. Teh 和 N. de Frietas. Recurrentgemma: 超越变压器，实现高效开放语言模型。 *arXiv 预印本*，2024年。'
- en: Calandriello et al. (2024) D. Calandriello, D. Guo, R. Munos, M. Rowland, Y. Tang,
    B. A. Pires, P. H. Richemond, C. L. Lan, M. Valko, T. Liu, et al. Human alignment
    of large language models through online preference optimisation. In *ICML*, 2024.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calandriello 等（2024）D. Calandriello, D. Guo, R. Munos, M. Rowland, Y. Tang,
    B. A. Pires, P. H. Richemond, C. L. Lan, M. Valko, T. Liu 等. 通过在线偏好优化对大型语言模型进行人类对齐。
    在 *ICML*，2024年。
- en: Casper et al. (2023) S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer,
    J. Rando, R. Freedman, T. Korbak, D. Lindner, P. Freire, et al. Open problems
    and fundamental limitations of reinforcement learning from human feedback. *TMLR*,
    2023.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Casper 等（2023）S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando,
    R. Freedman, T. Korbak, D. Lindner, P. Freire 等. 人类反馈中的强化学习的开放问题和基本限制。 *TMLR*，2023年。
- en: Christiano et al. (2018) P. Christiano, B. Shlegeris, and D. Amodei. Supervising
    strong learners by amplifying weak experts. *arXiv preprint*, 2018.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano 等（2018）P. Christiano, B. Shlegeris 和 D. Amodei. 通过放大弱专家来监督强学习者。 *arXiv
    预印本*，2018年。
- en: Christiano et al. (2017) P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg,
    and D. Amodei. Deep reinforcement learning from human preferences. In *NeurIPS*,
    2017.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano 等（2017）P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg 和
    D. Amodei. 从人类偏好中学习的深度强化学习。 在 *NeurIPS*，2017年。
- en: Clark and Amodei (2016) J. Clark and D. Amodei. Faulty Reward Functions in the
    Wild. [https://openai.com/research/faulty-reward-functions](https://openai.com/research/faulty-reward-functions),
    2016.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 和 Amodei（2016）J. Clark 和 D. Amodei. 野外的错误奖励函数。 [https://openai.com/research/faulty-reward-functions](https://openai.com/research/faulty-reward-functions)，2016年。
- en: 'CodeGemma Team (2024) CodeGemma Team. Codegemma: Open code models based on
    gemma. *arXiv preprint*, 2024.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CodeGemma Team（2024）CodeGemma Team. Codegemma: 基于 gemma 的开放代码模型。 *arXiv 预印本*，2024年。'
- en: Cotra (2018) A. Cotra. Iterated distillation and amplification. [https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616),
    2018.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cotra（2018）A. Cotra. 迭代蒸馏与放大。 [https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616)，2018年。
- en: Cover (1999) T. M. Cover. *Elements of information theory*. 1999.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cover（1999）T. M. Cover. *信息理论元素*。1999年。
- en: Dabney et al. (2017) W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos. Distributional
    reinforcement learning with quantile regression. In *AAAI Conference on Artificial
    Intelligence*, 2017.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dabney 等（2017）W. Dabney, M. Rowland, M. G. Bellemare 和 R. Munos. 具有分位回归的分布式强化学习。
    在 *AAAI 人工智能会议*，2017年。
- en: 'Dong et al. (2023) H. Dong, W. Xiong, D. Goyal, Y. Zhang, W. Chow, R. Pan,
    S. Diao, J. Zhang, K. SHUM, and T. Zhang. RAFT: Reward ranked finetuning for generative
    foundation model alignment. *TMLR*, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong 等（2023）H. Dong, W. Xiong, D. Goyal, Y. Zhang, W. Chow, R. Pan, S. Diao,
    J. Zhang, K. SHUM 和 T. Zhang. RAFT: 基于奖励排名的生成基础模型对齐。 *TMLR*，2023年。'
- en: 'Flet-Berliac et al. (2024) Y. Flet-Berliac, N. Grinsztajn, F. Strub, E. Choi,
    C. Cremer, A. Ahmadian, Y. Chandak, M. G. Azar, O. Pietquin, and M. Geist. Contrastive
    policy gradient: Aligning llms on sequence-level scores in a supervised-friendly
    fashion. *arXiv preprint*, 2024.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Flet-Berliac 等（2024）Y. Flet-Berliac, N. Grinsztajn, F. Strub, E. Choi, C. Cremer,
    A. Ahmadian, Y. Chandak, M. G. Azar, O. Pietquin 和 M. Geist. 对比策略梯度: 以监督友好的方式在序列级别对齐
    LLM。 *arXiv 预印本*，2024年。'
- en: French (1992) R. M. French. Semi-distributed representations and catastrophic
    forgetting in connectionist networks. *Connection Science*, 1992.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: French (1992) R. M. French. 半分布式表示与连接主义网络中的灾难性遗忘。*连接科学*，1992年。
- en: Gao et al. (2023) L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward
    model overoptimization. In *ICML*, 2023.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2023) L. Gao, J. Schulman, and J. Hilton. 奖励模型过度优化的缩放规律。在 *ICML*，2023年。
- en: Geist et al. (2019) M. Geist, B. Scherrer, and O. Pietquin. A theory of regularized
    markov decision processes. In *ICML*, 2019.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geist et al. (2019) M. Geist, B. Scherrer, and O. Pietquin. 正则化马尔可夫决策过程的理论。在
    *ICML*，2019年。
- en: 'Gemini Team (2023) Gemini Team. Gemini: A family of highly capable multimodal
    models. 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gemini Team (2023) Gemini Team. Gemini：一系列高度能力的多模态模型。2023年。
- en: 'Gemma Team (2024) Gemma Team. Gemma: Open models based on gemini research and
    technology. *arXiv preprint*, 2024.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gemma Team (2024) Gemma Team. Gemma: 基于双子研究和技术的开放模型。*arXiv 预印本*，2024年。'
- en: Gorbatovski et al. (2024) A. Gorbatovski, B. Shaposhnikov, A. Malakhov, N. Surnachev,
    Y. Aksenov, I. Maksimov, N. Balagansky, and D. Gavrilov. Learn your reference
    model for real good alignment. *arXiv preprint*, 2024.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gorbatovski et al. (2024) A. Gorbatovski, B. Shaposhnikov, A. Malakhov, N. Surnachev,
    Y. Aksenov, I. Maksimov, N. Balagansky, and D. Gavrilov. 真实良好对齐的参考模型学习。*arXiv
    预印本*，2024年。
- en: Gui et al. (2024) L. Gui, C. Gârbacea, and V. Veitch. Bonbon alignment for large
    language models and the sweetness of Best-of-N sampling. *arXiv preprint*, 2024.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gui et al. (2024) L. Gui, C. Gârbacea, and V. Veitch. 大型语言模型的Bonbon对齐与最佳采样的甜美。*arXiv
    预印本*，2024年。
- en: Guo et al. (2024) S. Guo, B. Zhang, T. Liu, T. Liu, M. Khalman, F. Llinares,
    A. Rame, T. Mesnard, Y. Zhao, B. Piot, J. Ferret, and M. Blondel. Direct language
    model alignment from online ai feedback. *arXiv preprint*, 2024.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2024) S. Guo, B. Zhang, T. Liu, T. Liu, M. Khalman, F. Llinares,
    A. Rame, T. Mesnard, Y. Zhao, B. Piot, J. Ferret, and M. Blondel. 通过在线AI反馈进行直接语言模型对齐。*arXiv
    预印本*，2024年。
- en: Hilton (2023) J. Hilton. KL divergence of max-of-n, 2023.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hilton (2023) J. Hilton. 最大化的KL散度，2023年。
- en: Hoffmann et al. (2022) J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya,
    T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al.
    Training compute-optimal large language models. In *NeurIPS*, 2022.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann et al. (2022) J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya,
    T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et
    al. 训练计算最优的大型语言模型。在 *NeurIPS*，2022年。
- en: Jeffreys (1946) H. Jeffreys. An invariant form for the prior probability in
    estimation problems. *Proceedings of the Royal Society of London. Series A, Mathematical
    and Physical Sciences*, 186(1007):453–461, 1946.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeffreys (1946) H. Jeffreys. 估计问题中的先验概率的不变形式。*伦敦皇家学会A系列：数学与物理科学论文集*，186(1007):453–461，1946年。
- en: Jiang et al. (2024) A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary,
    C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mixtral
    of experts. *arXiv preprint arXiv:2401.04088*, 2024.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2024) A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary,
    C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. 专家混合模型。*arXiv
    预印本 arXiv:2401.04088*，2024年。
- en: 'Kingma and Ba (2015) D. P. Kingma and J. Ba. Adam: A method for stochastic
    optimization. In *ICLR*, 2015.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma and Ba (2015) D. P. Kingma and J. Ba. Adam：一种随机优化方法。在 *ICLR*，2015年。
- en: Kool et al. (2019) W. Kool, H. van Hoof, and M. Welling. Buy 4 REINFORCE samples,
    get a baseline for free! 2019.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kool et al. (2019) W. Kool, H. van Hoof, and M. Welling. 购买4个REINFORCE样本，免费获得一个基准！2019年。
- en: Kullback (1959) S. Kullback. *Information Theory and Statistics*. New York,
    1959.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kullback (1959) S. Kullback. *信息论与统计学*。纽约，1959年。
- en: 'Lazaridou et al. (2020) A. Lazaridou, A. Potapenko, and O. Tieleman. Multi-agent
    communication meets natural language: Synergies between functional and structural
    language learning. In *ACL*, 2020.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lazaridou et al. (2020) A. Lazaridou, A. Potapenko, and O. Tieleman. 多智能体通信与自然语言：功能与结构语言学习的协同作用。在
    *ACL*，2020年。
- en: 'Li et al. (2023) Z. Li, T. Xu, Y. Zhang, Y. Yu, R. Sun, and Z.-Q. Luo. Remax:
    A simple, effective, and efficient reinforcement learning method for aligning
    large language models. *arXiv preprint*, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Z. Li, T. Xu, Y. Zhang, Y. Yu, R. Sun, and Z.-Q. Luo. Remax：一种简单、有效且高效的强化学习方法，用于对齐大型语言模型。*arXiv
    预印本*，2023年。
- en: Munos et al. (2023) R. Munos, M. Valko, D. Calandriello, M. G. Azar, M. Rowland,
    Z. D. Guo, Y. Tang, M. Geist, T. Mesnard, A. Michi, et al. Nash learning from
    human feedback. *arXiv preprint*, 2023.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Munos et al. (2023) R. Munos, M. Valko, D. Calandriello, M. G. Azar, M. Rowland,
    Z. D. Guo, Y. Tang, M. Geist, T. Mesnard, A. Michi, et al. Nash 从人类反馈中学习。*arXiv
    预印本*，2023年。
- en: 'Nakano et al. (2021) R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,
    C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted question-answering
    with human feedback. *arXiv preprint*, 2021.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakano等人（2021）R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C.
    Hesse, S. Jain, V. Kosaraju, W. Saunders, 等人. WebGPT：浏览器辅助的带有人类反馈的问答。*arXiv preprint*，2021。
- en: Narayan et al. (2018) S. Narayan, S. B. Cohen, and M. Lapata. Don’t give me
    the details, just the summary! topic-aware convolutional neural networks for extreme
    summarization. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors,
    *Conference on Empirical Methods in Natural Language Processing*, 2018.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayan等人（2018）S. Narayan, S. B. Cohen, 和M. Lapata. 别给我细节，只要总结！针对极端摘要的主题感知卷积神经网络。在E.
    Riloff, D. Chiang, J. Hockenmaier, 和J. Tsujii主编的*自然语言处理经验方法会议*，2018。
- en: Ngo et al. (2022) R. Ngo, L. Chan, and S. Mindermann. The alignment problem
    from a deep learning perspective. *arXiv preprint*, 2022.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ngo等人（2022）R. Ngo, L. Chan, 和S. Mindermann. 从深度学习的角度看对齐问题。*arXiv preprint*，2022。
- en: OpenAI (2023) OpenAI. Gpt-4 technical report. 2023.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI. GPT-4技术报告。2023。
- en: Ouyang et al. (2022) L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright,
    P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models
    to follow instructions with human feedback. *NeurIPS*, 2022.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang等人（2022）L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,
    C. Zhang, S. Agarwal, K. Slama, A. Ray, 等人. 训练语言模型以跟随人类反馈的指令。*NeurIPS*，2022。
- en: 'Pace et al. (2024) A. Pace, J. Mallinson, E. Malmi, S. Krause, and A. Severyn.
    West-of-n: Synthetic preference generation for improved reward modeling. *arXiv
    preprint*, 2024.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pace等人（2024）A. Pace, J. Mallinson, E. Malmi, S. Krause, 和A. Severyn. West-of-n：用于改进奖励建模的合成偏好生成。*arXiv
    preprint*，2024。
- en: 'Pan et al. (2022) A. Pan, K. Bhatia, and J. Steinhardt. The effects of reward
    misspecification: Mapping and mitigating misaligned models. In *ICLR*, 2022.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan等人（2022）A. Pan, K. Bhatia, 和J. Steinhardt. 奖励误设的影响：映射和缓解不对齐模型。在*ICLR*，2022。
- en: Pinto et al. (2023) A. S. Pinto, A. Kolesnikov, Y. Shi, L. Beyer, and X. Zhai.
    Tuning computer vision models with task rewards. In *ICML*, 2023.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinto等人（2023）A. S. Pinto, A. Kolesnikov, Y. Shi, L. Beyer, 和X. Zhai. 使用任务奖励调整计算机视觉模型。在*ICML*，2023。
- en: Qiping Yang et al. (2024) J. Qiping Yang, S. Salamatian, Z. Sun, A. Theertha Suresh,
    and A. Beirami. Asymptotics of language model alignment. *arXiv*, 2024.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiping Yang等人（2024）J. Qiping Yang, S. Salamatian, Z. Sun, A. Theertha Suresh,
    和A. Beirami. 语言模型对齐的渐近分析。*arXiv*, 2024。
- en: Radford et al. (2018) A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.
    Improving language understanding by generative pre-training. 2018.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等人（2018）A. Radford, K. Narasimhan, T. Salimans, 和I. Sutskever. 通过生成预训练提升语言理解。2018。
- en: Radford et al. (2019) A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever.
    Language models are unsupervised multitask learners. 2019.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等人（2019）A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, 和I. Sutskever.
    语言模型是无监督的多任务学习者。2019。
- en: 'Rafailov et al. (2023) R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D.
    Manning, and C. Finn. Direct preference optimization: Your language model is secretly
    a reward model. *arXiv preprint*, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov等人（2023）R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning,
    和C. Finn. 直接偏好优化：你的语言模型实际上是一个奖励模型。*arXiv preprint*，2023。
- en: Raffel et al. (2020) C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
    Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with
    a unified text-to-text transformer. *JMLR*, 2020.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel等人（2020）C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
    Y. Zhou, W. Li, 和P. J. Liu. 通过统一的文本到文本转换器探索迁移学习的极限。*JMLR*，2020。
- en: 'Ramé et al. (2024) A. Ramé, N. Vieillard, L. Hussenot, R. Dadashi, G. Cideron,
    O. Bachem, and J. Ferret. WARM: On the benefits of weight averaged reward models.
    In *ICML*, 2024.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramé等人（2024）A. Ramé, N. Vieillard, L. Hussenot, R. Dadashi, G. Cideron, O. Bachem,
    和J. Ferret. WARM：权重平均奖励模型的好处。在*ICML*，2024。
- en: 'Ramé et al. (2024) A. Ramé, J. Ferret, N. Vieillard, R. Dadashi, L. Hussenot,
    P.-L. Cedoz, P. G. Sessa, S. Girgin, A. Douillard, and O. Bachem. WARP: On the
    benefits of weight averaged rewarded policies. *arXiv preprint*, 2024.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramé等人（2024）A. Ramé, J. Ferret, N. Vieillard, R. Dadashi, L. Hussenot, P.-L.
    Cedoz, P. G. Sessa, S. Girgin, A. Douillard, 和O. Bachem. WARP：权重平均奖励策略的好处。*arXiv
    preprint*，2024。
- en: 'Reid et al. (2024) M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap,
    J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini
    1.5: Unlocking multimodal understanding across millions of tokens of context.
    *arXiv preprint*, 2024.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reid等人（2024）M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b.
    Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, 等人. Gemini 1.5：解锁跨数百万个上下文令牌的多模态理解。*arXiv
    preprint*，2024。
- en: Roit et al. (2023) P. Roit, J. Ferret, L. Shani, R. Aharoni, G. Cideron, R. Dadashi,
    M. Geist, S. Girgin, L. Hussenot, O. Keller, et al. Factually consistent summarization
    via reinforcement learning with textual entailment feedback. In *ACL*, 2023.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roit 等人 (2023) P. Roit, J. Ferret, L. Shani, R. Aharoni, G. Cideron, R. Dadashi,
    M. Geist, S. Girgin, L. Hussenot, O. Keller, 等人. 通过文本蕴涵反馈的强化学习实现事实一致的摘要生成。发表于
    *ACL*，2023 年。
- en: Schulman et al. (2017) J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and
    O. Klimov. Proximal policy optimization algorithms. *arXiv preprint*, 2017.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等人 (2017) J. Schulman, F. Wolski, P. Dhariwal, A. Radford, 和 O. Klimov.
    近端策略优化算法。*arXiv 预印本*，2017 年。
- en: Skalse et al. (2022) J. M. V. Skalse, N. H. R. Howe, D. Krasheninnikov, and
    D. Krueger. Defining and characterizing reward gaming. In *NeurIPS*, 2022.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skalse 等人 (2022) J. M. V. Skalse, N. H. R. Howe, D. Krasheninnikov, 和 D. Krueger.
    定义和表征奖励游戏。发表于 *NeurIPS*，2022 年。
- en: Stiennon et al. (2020) N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss,
    A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human
    feedback. *NeurIPS*, 2020.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stiennon 等人 (2020) N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss,
    A. Radford, D. Amodei, 和 P. F. Christiano. 学习用人类反馈进行总结。发表于 *NeurIPS*，2020 年。
- en: 'Sutton and Barto (1998) R. S. Sutton and A. G. Barto. *Reinforcement Learning:
    An Introduction*. MIT Press, 1998.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto (1998) R. S. Sutton 和 A. G. Barto. *强化学习：导论*。麻省理工学院出版社，1998 年。
- en: Tang et al. (2024) Y. Tang, D. Z. Guo, Z. Zheng, D. Calandriello, Y. Cao, E. Tarassov,
    R. Munos, B. Ávila Pires, M. Valko, Y. Cheng, and W. Dabney. Understanding the
    performance gap between online and offline alignment algorithms. *arXiv preprint*,
    2024.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人 (2024) Y. Tang, D. Z. Guo, Z. Zheng, D. Calandriello, Y. Cao, E. Tarassov,
    R. Munos, B. Ávila Pires, M. Valko, Y. Cheng, 和 W. Dabney. 理解在线和离线对齐算法之间的性能差距。*arXiv
    预印本*，2024 年。
- en: 'Touvron et al. (2023) H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,
    Y. Babaei, N. Bashlykov, et al. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint*, 2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 (2023) H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,
    Y. Babaei, N. Bashlykov, 等人. Llama 2：开放基础和微调的聊天模型。*arXiv 预印本*，2023 年。
- en: Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In *NeurIPS*,
    2017.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人 (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, L. Kaiser, 和 I. Polosukhin. 注意力机制是你所需的一切。发表于 *NeurIPS*，2017 年。
- en: 'Vieillard et al. (2020) N. Vieillard, T. Kozuno, B. Scherrer, O. Pietquin,
    R. Munos, and M. Geist. Leverage the average: an analysis of kl regularization
    in reinforcement learning. *NeurIPS*, 2020.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vieillard 等人 (2020) N. Vieillard, T. Kozuno, B. Scherrer, O. Pietquin, R. Munos,
    和 M. Geist. 利用平均值：强化学习中 KL 正则化的分析。发表于 *NeurIPS*，2020 年。
- en: Wei et al. (2022) J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,
    A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. In
    *ICLR*, 2022.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 (2022) J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,
    A. M. Dai, 和 Q. V. Le. 微调语言模型是零样本学习者。发表于 *ICLR*，2022 年。
- en: Williams (1992) R. J. Williams. Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. *Reinforcement learning*, 1992.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams (1992) R. J. Williams. 用于连接主义强化学习的简单统计梯度跟踪算法。*强化学习*，1992 年。
- en: Wu et al. (2024) Y. Wu, Z. Sun, H. Yuan, K. Ji, Y. Yang, and Q. Gu. Self-play
    preference optimization for language model alignment. *arXiv preprint*, 2024.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 (2024) Y. Wu, Z. Sun, H. Yuan, K. Ji, Y. Yang, 和 Q. Gu. 自我博弈偏好优化用于语言模型对齐。*arXiv
    预印本*，2024 年。
- en: Yang et al. (2024) J. Q. Yang, S. Salamatian, Z. Sun, A. T. Suresh, and A. Beirami.
    Asymptotics of language model alignment. *arXiv preprint*, 2024.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 (2024) J. Q. Yang, S. Salamatian, Z. Sun, A. T. Suresh, 和 A. Beirami.
    语言模型对齐的渐近分析。*arXiv 预印本*，2024 年。
- en: Ziegler et al. (2019) D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford,
    D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human
    preferences. *arXiv preprint*, 2019.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler 等人 (2019) D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford,
    D. Amodei, P. Christiano, 和 G. Irving. 从人类偏好中微调语言模型。*arXiv 预印本*，2019 年。
- en: Appendix A Supporting results and derivations
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 支持结果和推导
- en: 'A.1 Proof of [Theorem 1](#Thmtheorem1 "Theorem 1\. ‣ 3.1 The Best-of-N distribution
    ‣ 3 The BOND Approach ‣ BOND: Aligning LLMs with Best-of-N Distillation")'
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'A.1 [定理 1](#Thmtheorem1 "定理 1\. ‣ 3.1 最佳-N 分布 ‣ 3 BOND 方法 ‣ BOND: 将 LLMs 与最佳-N
    蒸馏对齐") 的证明'
- en: Consider $N$ being selected by Best-of-N sampling.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑 $N$ 由最佳-N 采样选定。
- en: 'The event $A_{i}(y)$:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '事件 $A_{i}(y)$:'
- en: '|  | $\displaystyle\mathbb{P}\mathopen{}\mathclose{{}\left[A_{i}(y)}\right]$
    |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{P}\mathopen{}\mathclose{{}\left[A_{i}(y)}\right]$
    |  |'
- en: '|  |  | $\displaystyle={p_{<}}(y)^{i-1}\times{\pi_{\text{ref}}}(y)\times{p_{\leq}}(y)^{N-i-1}.$
    |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle={p_{<}}(y)^{i-1}\times{\pi_{\text{ref}}}(y)\times{p_{\leq}}(y)^{N-i-1}.$
    |  |'
- en: The likelihood that Best-of-N sampling selects the generation $y$ is then given
    by
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Best-of-N采样选择生成$y$的可能性由此给出
- en: '|  | $\displaystyle{\pi_{\text{BoN}}}(y)$ |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\pi_{\text{BoN}}}(y)$ |  |'
- en: '|  |  | $\displaystyle=\sum_{i=1}^{N}\mathopen{}\mathclose{{}\left[{p_{<}}(y)^{i-1}\times{\pi_{\text{ref}}}(y)\times{p_{\leq}}(y)^{N-i}}\right]$
    |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{i=1}^{N}\mathopen{}\mathclose{{}\left[{p_{<}}(y)^{i-1}\times{\pi_{\text{ref}}}(y)\times{p_{\leq}}(y)^{N-i}}\right]$
    |  |'
- en: '|  |  | $\displaystyle={\pi_{\text{ref}}}(y)\times\sum_{i=1}^{N}\mathopen{}\mathclose{{}\left[{p_{<}}(y)^{i-1}\times{p_{\leq}}(y)^{N-i}}\right]$
    |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle={\pi_{\text{ref}}}(y)\times\sum_{i=1}^{N}\mathopen{}\mathclose{{}\left[{p_{<}}(y)^{i-1}\times{p_{\leq}}(y)^{N-i}}\right]$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: A.2 Link to the continuous case
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 关联到连续情况
- en: A noteworthy observation is that we can relate the Best-of-N expression to the
    case of a continuous distribution, in which case the term $\mathtt{(B)}$ have
    the same value in this case).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值得注意的观察是，我们可以将Best-of-N表达式与连续分布的情况关联起来，此时$\mathtt{(B)}$项在这种情况下具有相同的值。
- en: Indeed, recall that the probability for a sequence $y$ to be drawn from the
    Best-of-N distribution is
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，回忆一下从Best-of-N分布中抽取序列$y$的概率是
- en: '|  | $1$2 |  | (20) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (20) |'
- en: Here, $y$ is the maximum length of a sequence.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$y$是序列的最大长度。
- en: 'Now, we show why [Equation 20](#A1.E20 "In A.2 Link to the continuous case
    ‣ Appendix A Supporting results and derivations ‣ BOND: Aligning LLMs with Best-of-N
    Distillation") matches the classic formula for the max of $N$ variables. Then,
    we have that'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，我们展示[方程20](#A1.E20 "在A.2 关联到连续情况 ‣ 附录A 支持结果与推导 ‣ BOND: 与Best-of-N蒸馏对齐LLMs")如何与$N$个变量的最大值的经典公式相匹配。然后，我们得到'
- en: '|  | $F_{X_{N}}(y)=\mathbb{P}(Y_{1}\leq y,\dots Y_{N}\leq y)=F_{X}(y)^{N},$
    |  | (21) |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{X_{N}}(y)=\mathbb{P}(Y_{1}\leq y,\dots Y_{N}\leq y)=F_{X}(y)^{N},$
    |  | (21) |'
- en: and thus
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: '|  | $f_{X_{N}}(y)=f_{X}(y)F_{X}(y)^{N-1}N.$ |  | (22) |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{X_{N}}(y)=f_{X}(y)F_{X}(y)^{N-1}N.$ |  | (22) |'
- en: 'In [Equation 22](#A1.E22 "In A.2 Link to the continuous case ‣ Appendix A Supporting
    results and derivations ‣ BOND: Aligning LLMs with Best-of-N Distillation"), we
    recognize the Best-of-N formula in the case where the correction factor $\mathtt{(B)}$
    in the discrete case.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '在[方程22](#A1.E22 "在A.2 关联到连续情况 ‣ 附录A 支持结果与推导 ‣ BOND: 与Best-of-N蒸馏对齐LLMs")中，我们识别出在离散情况下修正因子$\mathtt{(B)}$的Best-of-N公式。'
- en: A.3 Backward KL and policy gradient equivalence
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 反向KL和策略梯度等价性
- en: 'We formally show the analogy between the gradient of the backward KL divergence
    of [Equation 14](#S4.E14 "In 4.2 Jeffreys divergence as a robust objective ‣ 4
    BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation")
    and the standard (e.g., REINFORCE (Williams, [1992](#bib.bib62))) policy gradient
    of a KL-regularized RLHF problem with equivalent reward $r_{\texttt{BOND}}$.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '我们正式展示了[方程14](#S4.E14 "在4.2 Jeffreys散度作为一个鲁棒目标 ‣ 4 BOND挑战与算法 ‣ BOND: 与Best-of-N蒸馏对齐LLMs")的反向KL散度的梯度与标准（例如，REINFORCE
    (Williams, [1992](#bib.bib62))）的策略梯度之间的类比，这两者在KL正则化的RLHF问题中具有等效的奖励$r_{\texttt{BOND}}$。'
- en: 'The exact backward KL gradient can be derived as:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 精确的反向KL梯度可以推导为：
- en: '|  | $\displaystyle\nabla_{\pi}\operatorname{KL}\mathopen{}\mathclose{{}\left(\pi\mid\mid{\pi_{\text{BoN}}}}\right)$
    |  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{\pi}\operatorname{KL}\mathopen{}\mathclose{{}\left(\pi\mid\mid{\pi_{\text{BoN}}}}\right)$
    |  |'
- en: '|  |  | $\displaystyle=\nabla_{\pi}\sum_{y}\pi(y)\mathopen{}\mathclose{{}\left(\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right)$
    |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\nabla_{\pi}\sum_{y}\pi(y)\mathopen{}\mathclose{{}\left(\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right)$
    |  |'
- en: '|  |  | $\displaystyle=\sum_{y}\nabla_{\pi}\pi(y)\mathopen{}\mathclose{{}\left(\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right)+\pi(y)\nabla_{\pi}\log\pi(y)$
    |  |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{y}\nabla_{\pi}\pi(y)\mathopen{}\mathclose{{}\left(\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right)+\pi(y)\nabla_{\pi}\log\pi(y)$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{y\sim\pi}\mathopen{}\mathclose{{}\left[\nabla_{\pi}\log\pi(y)\mathopen{}\mathclose{{}\left(\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right)}\right].$
    |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{y\sim\pi}\mathopen{}\mathclose{{}\left[\nabla_{\pi}\log\pi(y)\mathopen{}\mathclose{{}\left(\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right)}\right].$
    |  |'
- en: Above, we have used the product rule of gradient, the rule $\nabla_{\pi}\pi(y)=\pi(y)\nabla_{\pi}\log\pi(y)$.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 上述公式中，我们使用了梯度的乘法规则，即规则$\nabla_{\pi}\pi(y)=\pi(y)\nabla_{\pi}\log\pi(y)$。
- en: '*Equivalence with Policy Gradient RL.* As anticipated, one can verify that
    descending the above gradient is equivalent – up to a constant scaling – to running
    the RL policy gradient REINFORCE algorithm on the RL objective of Equation [1](#S2.E1
    "Equation 1 ‣ 2 Problem Setup ‣ BOND: Aligning LLMs with Best-of-N Distillation")
    with $r=r_{\texttt{BOND}}$ to break down the above gradient into:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '*与策略梯度 RL 的等价性。* 正如预期的那样，可以验证下降上述梯度等同于——即使是常数缩放——在 RL 目标方程 [1](#S2.E1 "方程 1
    ‣ 2 问题设置 ‣ BOND：将 LLMs 与 Best-of-N 蒸馏对齐")上运行 RL 策略梯度 REINFORCE 算法，其中 $r=r_{\texttt{BOND}}$，将上述梯度分解为：'
- en: '|  | $\displaystyle\mathbb{E}_{y\sim\pi}\mathopen{}\mathclose{{}\left[\nabla_{\pi}\log\pi(y)\mathopen{}\mathclose{{}\left(\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right)}\right]$
    |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{y\sim\pi}\mathopen{}\mathclose{{}\left[\nabla_{\pi}\log\pi(y)\mathopen{}\mathclose{{}\left(\log\pi(y)-\log{\pi_{\text{BoN}}}(y)}\right)}\right]$
    |  |'
- en: '|  | $1$2 |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: A.4 Derivation of J-BOND reward
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 J-BOND 奖励的推导
- en: 'Here we provide a theoretical explanation behind the design of the J-BOND reward
    function discussed in [Section 5](#S5 "5 The J-BOND Algorithm ‣ BOND: Aligning
    LLMs with Best-of-N Distillation"):'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供了[第5节](#S5 "5 J-BOND 算法 ‣ BOND：将 LLMs 与 Best-of-N 蒸馏对齐")中讨论的 J-BOND
    奖励函数设计背后的理论解释：
- en: '|  | $1$2 |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Recall that $r_{\texttt{J-BOND}}(\cdot)$).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 记住 $r_{\texttt{J-BOND}}(\cdot)$。
- en: 'As mentioned in [Section 5](#S5 "5 The J-BOND Algorithm ‣ BOND: Aligning LLMs
    with Best-of-N Distillation"), we designed $r_{\texttt{J-BOND}}(\cdot)$ is motivated
    by the following main reason.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第5节](#S5 "5 J-BOND 算法 ‣ BOND：将 LLMs 与 Best-of-N 蒸馏对齐")中提到的，我们设计 $r_{\texttt{J-BOND}}(\cdot)$
    的主要原因如下。
- en: 'We want that, when sample $y$. For this purpose, let us consider the parametrized
    function:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望，当样本为 $y$ 时。为此，请考虑参数化的函数：
- en: '|  | $1$2 |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'Note that the stochasticity of $r_{\texttt{J-BOND}}^{\alpha}(y)$ and its expectation
    can be computed as:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 $r_{\texttt{J-BOND}}^{\alpha}(y)$ 的随机性及其期望可以计算为：
- en: '|  | $\displaystyle\mathbb{E}_{y_{1}^{\prime},y_{2}^{\prime}\sim\pi_{\text{anchor}}^{t}}\mathopen{}\mathclose{{}\left[r_{\texttt{J-BOND}}^{\alpha}(y)}\right]$
    |  | (23) |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{y_{1}^{\prime},y_{2}^{\prime}\sim\pi_{\text{anchor}}^{t}}\mathopen{}\mathclose{{}\left[r_{\texttt{J-BOND}}^{\alpha}(y)}\right]$
    |  | (23) |'
- en: '|  |  | $\displaystyle=\alpha\cdot(1-{p_{\leq}}(y))^{2},$ |  | (24) |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\alpha\cdot(1-{p_{\leq}}(y))^{2},$ |  | (24) |'
- en: 'where we have used the definition of ${p_{\leq}}(y)=\mathbb{P}_{y^{\prime}\sim\pi_{\text{anchor}}^{t}}\mathopen{}\mathclose{{}\left[r(y^{\prime})\leq
    r(y)}\right]$:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 ${p_{\leq}}(y)=\mathbb{P}_{y^{\prime}\sim\pi_{\text{anchor}}^{t}}\mathopen{}\mathclose{{}\left[r(y^{\prime})\leq
    r(y)}\right]$ 的定义：
- en: '|  | $\alpha\cdot(1-0.5)^{2}=\log(0.5)\quad\rightarrow\alpha=-\log(16).$ |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha\cdot(1-0.5)^{2}=\log(0.5)\quad\rightarrow\alpha=-\log(16).$ |  |'
- en: 'We illustrate this in [Figure 8](#A1.F8 "In A.4 Derivation of J-BOND reward
    ‣ Appendix A Supporting results and derivations ‣ BOND: Aligning LLMs with Best-of-N
    Distillation"), where we plot the expected $r_{\texttt{J-BOND}}(y)$.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[图8](#A1.F8 "在 A.4 J-BOND 奖励推导 ‣ 附录 A 支持结果和推导 ‣ BOND：将 LLMs 与 Best-of-N 蒸馏对齐")中进行了说明，在图中我们绘制了期望的
    $r_{\texttt{J-BOND}}(y)$。
- en: '![Refer to caption](img/5be2bfa6348b58f1ae049196ccf83537.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5be2bfa6348b58f1ae049196ccf83537.png)'
- en: 'Figure 8: Expected value of the J-BOND reward, i.e., $\mathbb{E}_{y_{1}^{\prime},y_{2}^{\prime}\sim\pi_{\text{anchor}}^{t}}\mathopen{}\mathclose{{}\left[r_{\texttt{J-BOND}}(y)}\right]$
    has median reward w.r.t. the anchor distribution.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：J-BOND 奖励的期望值，即 $\mathbb{E}_{y_{1}^{\prime},y_{2}^{\prime}\sim\pi_{\text{anchor}}^{t}}\mathopen{}\mathclose{{}\left[r_{\texttt{J-BOND}}(y)}\right]$
    相对于锚点分布具有中位奖励。
- en: Appendix B Additional Experiments
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 额外实验
- en: B.1 Learned quantile models
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 学习的分位数模型
- en: 'Monte-Carlo quantile estimation ([Section 4.1](#S4.SS1 "4.1 Monte-Carlo quantile
    estimation ‣ 4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N
    Distillation")) approximates the reward quantiles by sampling multiple times from
    the reference policy ${\pi_{\text{ref}}}$, although they may have very similar
    reward quantiles.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡罗分位数估计（[第4.1节](#S4.SS1 "4.1 蒙特卡罗分位数估计 ‣ 4 BOND 挑战和算法 ‣ BOND：将 LLMs 与 Best-of-N
    蒸馏对齐")）通过从参考策略 ${\pi_{\text{ref}}}$ 多次采样来近似奖励分位数，尽管它们可能具有非常相似的奖励分位数。
- en: 'Motivated by this, in this section we explore an alternative approach that
    aims at *learning* a context-dependent quantile estimator $\widehat{{p_{\leq}}}_{\theta}(\cdot)$
    as the output of a binary classifier and train it via maximum likelihood estimation
    using the standard binary cross-entropy loss (Cover, [1999](#bib.bib14)):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 受到此启发，本节我们探讨一种替代方法，旨在*学习*一个上下文依赖的分位数估计器 $\widehat{{p_{\leq}}}_{\theta}(\cdot)$
    作为二分类器的输出，并通过最大似然估计使用标准的二元交叉熵损失进行训练 (Cover, [1999](#bib.bib14))：
- en: '|  | $1$2 |  | (25) |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (25) |'
- en: 'We test such an approach in the abtractive summarization task considered in [Section 4](#S4
    "4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N Distillation").
    We parametrize $\widehat{{p_{\leq}}}_{\theta}(\cdot)$ can be re-used or learned
    offline with a fixed sample budget.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在[第4节](#S4 "4 BOND Challenges and Algorithms ‣ BOND: Aligning LLMs with Best-of-N
    Distillation")考虑的抽象总结任务中测试了这种方法。我们将 $\widehat{{p_{\leq}}}_{\theta}(\cdot)$ 参数化，使其可以在固定的样本预算下重新使用或离线学习。'
- en: '![Refer to caption](img/760ae9ef9571b49e9b39c61739eb3c4d.png)![Refer to caption](img/29b8808dc1035d0652ecda9ebfdc82ff.png)![Refer
    to caption](img/77b08d2eb66c475f743e7d87b5af20a4.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/760ae9ef9571b49e9b39c61739eb3c4d.png)![参见说明](img/29b8808dc1035d0652ecda9ebfdc82ff.png)![参见说明](img/77b08d2eb66c475f743e7d87b5af20a4.png)'
- en: 'Figure 9: BOND (with $N=8$) using MC quantile estimates vs. a Learned quantile
    model.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：使用MC分位数估计与学习的分位数模型的BOND（$N=8$）。
- en: Finally, we remark that the explored approach is quite naive, and alternative
    learned quantile models can definitely be derived, e.g., further enforcing ordering
    in the predicted quantiles, using quantile regression (Dabney et al., [2017](#bib.bib15)),
    or assuming a pre-specified (e.g., Gaussian) rewards’ distributions.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们指出所探讨的方法相当简单，确实可以推导出替代的学习分位数模型，例如，进一步强化预测分位数中的排序，使用分位数回归 (Dabney et al.,
    [2017](#bib.bib15))，或假设预先指定的（例如，高斯）奖励分布。
- en: B.2 Additional plots for BOND with Jeffreys divergence objective
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 使用Jeffreys散度目标的BOND的附加图
- en: 'We provide additional experiments that complement the ablation of [Section 4.2](#S4.SS2
    "4.2 Jeffreys divergence as a robust objective ‣ 4 BOND Challenges and Algorithms
    ‣ BOND: Aligning LLMs with Best-of-N Distillation") when running BOND with a Jeffreys
    divergence objective $J_{\text{effreys}}^{\beta}$.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提供了额外的实验，以补充在使用Jeffreys散度目标 $J_{\text{effreys}}^{\beta}$ 运行BOND时[第4.2节](#S4.SS2
    "4.2 Jeffreys divergence as a robust objective ‣ 4 BOND Challenges and Algorithms
    ‣ BOND: Aligning LLMs with Best-of-N Distillation")的消融实验。'
- en: '![Refer to caption](img/9420e33547fbd1ab84ceaad13e8a3890.png)![Refer to caption](img/bdbfb565e6f0d787dcb85042461f3e8c.png)![Refer
    to caption](img/edc2e57f0e97b3f42bdb78fa88420d6d.png)![Refer to caption](img/1d9b98519b18b9286860652024c27497.png)![Refer
    to caption](img/31c683a9c9a75f40fa11f45db707cc7b.png)![Refer to caption](img/41b099c60d0d8f856a7f4142c033201a.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9420e33547fbd1ab84ceaad13e8a3890.png)![参见说明](img/bdbfb565e6f0d787dcb85042461f3e8c.png)![参见说明](img/edc2e57f0e97b3f42bdb78fa88420d6d.png)![参见说明](img/1d9b98519b18b9286860652024c27497.png)![参见说明](img/31c683a9c9a75f40fa11f45db707cc7b.png)![参见说明](img/41b099c60d0d8f856a7f4142c033201a.png)'
- en: 'Figure 10: BOND with $N=4$.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：$N=4$的BOND。
