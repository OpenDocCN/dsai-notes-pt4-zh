- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:51:10'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:10
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双重粒度量化：高效的细粒度量化方法用于大型语言模型
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.04836](https://ar5iv.labs.arxiv.org/html/2310.04836)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.04836](https://ar5iv.labs.arxiv.org/html/2310.04836)
- en: '^†^†footnotetext: ^∗Corresponding author: Hong Zhou, zhouh@mail.bme.zju.edu.cn'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†脚注：^∗通讯作者：Hong Zhou，zhouh@mail.bme.zju.edu.cn
- en: ^† Equal ContributionLuoming Zhang^(†1), Wen Fei^(†2), Weijia Wu¹, Yefei He¹,
    Zhenyu Lou¹, Hong Zhou^(∗1)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ^† 相等贡献Luoming Zhang^(†1), Wen Fei^(†2), Weijia Wu¹, Yefei He¹, Zhenyu Lou¹,
    Hong Zhou^(∗1)
- en: ¹Zhejiang University ²Shanghai Jiao Tong University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹浙江大学 ²上海交通大学
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Models (LLMs) pose significant hardware challenges related to
    memory requirements and computational ability. There are two mainstream quantization
    schemes for LLMs: coarse-grained (e.g., channel-wise) quantization and fine-grained
    ( e.g., group-wise) quantization. Fine-grained quantization has smaller quantization
    loss, consequently achieving superior performance. However, when applied to weight-activation
    quantization, it disrupts continuous integer matrix multiplication, leading to
    inefficient inference. In this paper, we introduce Dual Grained Quantization (DGQ),
    a novel A8W4 quantization for LLM that maintains superior performance while ensuring
    fast inference speed. DSQ dequantizes the fine-grained INT4 weight into coarse-grained
    INT8 representation and preform matrix multiplication using INT8 kernels. Besides,
    we develop a two-phase grid search algorithm to simplify the determination of
    fine-grained and coarse-grained quantization scales. We also devise a percentile
    clipping schema for smoothing the activation outliers without the need for complex
    optimization techniques. Experimental results demonstrate that DGQ consistently
    outperforms prior methods across various LLM architectures and a wide range of
    tasks. Remarkably, by our implemented efficient CUTLASS kernel, we achieve 1.12
    $\times$ speed gains comparing A16W4 implementation. These advancements enable
    efficient deployment of A8W4 LLMs for real-world applications.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在内存需求和计算能力方面带来了重大硬件挑战。LLMs 主要有两种量化方案：粗粒度（例如，通道级）量化和细粒度（例如，组级）量化。细粒度量化具有较小的量化损失，从而实现更优的性能。然而，当应用于权重-激活量化时，它会破坏连续整数矩阵乘法，导致推理效率低下。在本文中，我们介绍了双重粒度量化（DGQ），这是一种新颖的
    A8W4 量化方法，旨在保持卓越性能的同时确保快速推理速度。DGQ 将细粒度的 INT4 权重解量化为粗粒度的 INT8 表示，并使用 INT8 内核进行矩阵乘法。此外，我们开发了一种两阶段网格搜索算法，以简化细粒度和粗粒度量化尺度的确定。我们还设计了一种百分位裁剪方案，用于平滑激活异常值，而无需复杂的优化技术。实验结果表明，DGQ
    在各种 LLM 架构和广泛任务中始终优于先前的方法。值得注意的是，通过我们实现的高效 CUTLASS 内核，我们实现了相较于 A16W4 实现 1.12 倍的速度提升。这些进展使得
    A8W4 LLMs 在现实世界应用中得到了高效部署。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The internet has generated vast amounts of text data, and with the increase
    in computing power, Large Language Models (LLMs) such as GPT-4 (Bubeck et al.,
    [2023](#bib.bib5)) have excelled in comprehending and generating natural language.
    However, these models have become much larger. For instance, GPT-3 (Brown et al.,
    [2020](#bib.bib4)) boasts over 175 billion parameters. Open-source models like
    OPT (Zhang et al., [2022](#bib.bib48)) and BLOOM (Scao et al., [2022](#bib.bib35)),
    built on the GPT architecture, often surpass GPT-3 in parameter count. More recently,
    models like Meta’s 65B (Touvron et al., [2023a](#bib.bib37)) have matched GPT-3’s
    language generation abilities. To put this in perspective, LLaMA-65B is approximately
    190 times larger than BERT-Large (Devlin et al., [2018](#bib.bib15)), necessitating
    around 130 GB of memory storage, which requires two A100 GPUs to accommodate.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网生成了大量的文本数据，随着计算能力的提升，像 GPT-4 (Bubeck et al., [2023](#bib.bib5)) 这样的**大型语言模型**（LLMs）在理解和生成自然语言方面表现优异。然而，这些模型变得越来越庞大。例如，GPT-3
    (Brown et al., [2020](#bib.bib4)) 拥有超过 1750 亿个参数。像 OPT (Zhang et al., [2022](#bib.bib48))
    和 BLOOM (Scao et al., [2022](#bib.bib35)) 这样的开源模型，基于 GPT 架构，通常在参数数量上超过 GPT-3。最近，像
    Meta 的 65B (Touvron et al., [2023a](#bib.bib37)) 这样的模型在语言生成能力上与 GPT-3 相匹敌。为了提供一个对比，LLaMA-65B
    的规模大约是 BERT-Large (Devlin et al., [2018](#bib.bib15)) 的 190 倍，需约 130 GB 的内存存储，需要两个
    A100 GPU 才能容纳。
- en: Model quantization, as discussed in Han et al. ([2015](#bib.bib19)), maps high-precision
    values to lower-precision representations (e.g., INT8, INT4, FP8, FP4). This technique
    serves to reduce memory requirements and enhance inference speed. In the context
    of Large Language Models (LLM), Post-training Quantization(PTQ)  (Nagel et al.,
    [2020](#bib.bib30); Hubara et al., [2020](#bib.bib21)) methods are preferred,
    primarily due to the extensive computational demands associated with fine-tuning
    for Quantization Aware Training (QAT) (Esser et al., [2019](#bib.bib16); Martinez
    et al., [2018](#bib.bib28)). To maintain precision when applying PTQ, we incorporate
    finer-grained quantization methods (Yao et al., [2022](#bib.bib44); Bondarenko
    et al., [2021](#bib.bib3)). Fine-grained quantization involves dividing a dimension
    into multiple parts and quantizing each smaller slice, while coarse-grained quantization (Xiao
    et al., [2023](#bib.bib43); Yuan et al., [2023](#bib.bib46)) typically quantizes
    the entire tensor or quantizes along a dimension. Fine-grained quantization is
    commonly used in weight-only quantization (Frantar et al., [2022](#bib.bib18);
    Lin et al., [2023](#bib.bib24); Dettmers et al., [2023b](#bib.bib14)), where only
    model weights are quantized.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 模型量化，如Han等（[2015](#bib.bib19)）讨论的，将高精度值映射到低精度表示（例如，INT8，INT4，FP8，FP4）。这一技术旨在减少内存需求并提高推理速度。在大型语言模型（LLM）的背景下，后训练量化（PTQ）（Nagel等，[2020](#bib.bib30)；Hubara等，[2020](#bib.bib21)）方法更受青睐，主要是由于量化感知训练（QAT）（Esser等，[2019](#bib.bib16)；Martinez等，[2018](#bib.bib28)）所涉及的高计算需求。为了在应用PTQ时保持精度，我们采用更精细的量化方法（Yao等，[2022](#bib.bib44)；Bondarenko等，[2021](#bib.bib3)）。精细量化涉及将一个维度划分为多个部分，并对每个较小的切片进行量化，而粗略量化（Xiao等，[2023](#bib.bib43)；Yuan等，[2023](#bib.bib46)）通常对整个张量或沿某个维度进行量化。精细量化通常用于仅量化权重（Frantar等，[2022](#bib.bib18)；Lin等，[2023](#bib.bib24)；Dettmers等，[2023b](#bib.bib14)），其中仅量化模型权重。
- en: '![Refer to caption](img/0a657b2089e34596fd56ee630ba056a7.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0a657b2089e34596fd56ee630ba056a7.png)'
- en: 'Figure 1: Comparison of four different quantization methods in terms of runtime
    and memory usage. Opt-30b with different sequence length from 512 to 2048 is used
    as the baseline. Our A8W4 implement maintain the comparable run time to A8W8 and
    FP16 while maintaining small memory usages. Besieds, our implement has a smaller
    memory usage than A16W4 implement.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：四种不同量化方法在运行时间和内存使用方面的比较。Opt-30b在从512到2048的不同序列长度下作为基线。我们的A8W4实现保持了与A8W8和FP16相当的运行时间，同时保持了较小的内存使用。此外，我们的实现比A16W4实现具有更小的内存使用。
- en: However, in weight-activation quantization (Xiao et al., [2023](#bib.bib43);
    Yuan et al., [2023](#bib.bib46); Wei et al., [2023](#bib.bib41)), which includes
    both weights and activations, a key challenge emerges due to varying quantization
    scales along the accumulation axis, typically represented by input channels in
    linear layers.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在权重-激活量化（Xiao等，[2023](#bib.bib43)；Yuan等，[2023](#bib.bib46)；Wei等，[2023](#bib.bib41)）中，包括了权重和激活，由于沿累积轴（通常由线性层中的输入通道表示）量化尺度的变化，出现了一个关键挑战。
- en: Addressing this challenge involves partitioning the integer General Matrix Multiplication
    (GEMM) operation into discrete segments and aggregating them through floating-point
    arithmetic. Notably, implementing such a scheme on existing hardware infrastructure
    proves to be notably inefficient.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这一挑战的方法是将整数通用矩阵乘法（GEMM）操作划分为离散的片段，并通过浮点运算进行聚合。值得注意的是，在现有硬件基础设施上实现这种方案被证明非常低效。
- en: 'To address this challenge, we introduce Dual Grained Quantization (DGQ) as
    an efficient deployment solution for LLMs. DGQ combines the performance benefits
    of fine-grained quantization with the computational efficiency of coarse-grained
    methods. In our weight quantization approach, we employ a two-step process. To
    avoid the need for segmenting General Matrix Multiplication (GEMM) operations,
    we dequantize INT4-weight back to INT8, instead of directly casting to INT8\.
    This results in INT8 weights with a coarser-grained scale. As a consequence, our
    coarser-grained INT8 activation-weight quantization can be efficiently accelerated
    by general-purpose hardware, eliminating the necessity for specialized hardware
    designs. DGQ introduces two quantized scales for weight tensors: a fine-grained
    INT8 quantization scale and a coarse-grained FP16 quantization scale. A notable
    challenge arises when directly quantizing the FP16 fine-grained scale, leading
    to a significant drop in accuracy, as illustrated in Table  ([1](#S3.T1 "Table
    1 ‣ 3.2 Dual Grained Quantization ‣ 3 Method ‣ Dual Grained Quantization: Efficient
    Fine-Grained Quantization for LLM")). To tackle this issue, we propose a two-phase
    search approach. In the first phase, we diligently search for the original fine-grained
    parameters. In the subsequent phase, we determine coarse-grained parameters using
    the original weights, rather than relying on previously derived fine-grained parameters.
    We then partition the fine-grained parameters using the determined coarse-grained
    parameters, preserving the performance of the original fine-grained quantized
    models.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '为了应对这一挑战，我们引入了 Dual Grained Quantization（DGQ）作为 LLMs 的高效部署解决方案。DGQ 结合了细粒度量化的性能优势和粗粒度方法的计算效率。在我们的权重量化方法中，我们采用了一个两步过程。为了避免对通用矩阵乘法（GEMM）操作进行分段处理，我们将
    INT4 权重量化回 INT8，而不是直接转换为 INT8。这导致了具有较粗粒度的 INT8 权重。因此，我们的粗粒度 INT8 激活权重量化可以通过通用硬件高效加速，消除了对专用硬件设计的需求。DGQ
    为权重张量引入了两种量化尺度：细粒度 INT8 量化尺度和粗粒度 FP16 量化尺度。一个显著的挑战是直接量化 FP16 细粒度尺度时，会导致准确度显著下降，如表（[1](#S3.T1
    "Table 1 ‣ 3.2 Dual Grained Quantization ‣ 3 Method ‣ Dual Grained Quantization:
    Efficient Fine-Grained Quantization for LLM")）所示。为解决这一问题，我们提出了一个两阶段搜索方法。在第一阶段，我们认真搜索原始细粒度参数。在随后的阶段，我们使用原始权重确定粗粒度参数，而不是依赖先前获得的细粒度参数。然后，我们使用确定的粗粒度参数对细粒度参数进行分区，从而保持原始细粒度量化模型的性能。'
- en: Recent studies (Xiao et al., [2023](#bib.bib43); Yuan et al., [2023](#bib.bib46);
    Wei et al., [2023](#bib.bib41)) have made significant strides in addressing the
    challenge of outliers in Large Language Models (LLMs). These efforts have resulted
    in INT8 models that match the accuracy levels of their full-precision counterparts.
    Drawing inspiration from LLM.int8() (Dettmers et al., [2022](#bib.bib12)) and
    AWQ (Lin et al., [2023](#bib.bib24)), we propose a novel percentile clipping smoothing
    strategy to further enhance quantization. One of the notable advantages of our
    method is its gradient-free nature, which ensures the preservation of the model’s
    generalization ability across diverse domains and modalities. Our optimization
    approach also demonstrates exceptional efficiency. For instance, we can achieve
    quantized A8W4 models for BLOOM-176B within just one GPU hour on the A100-80G.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究（Xiao 等，[2023](#bib.bib43)；Yuan 等，[2023](#bib.bib46)；Wei 等，[2023](#bib.bib41)）在解决大语言模型（LLMs）中的异常值挑战方面取得了显著进展。这些努力导致了
    INT8 模型，它们的准确度与全精度模型相匹配。受到 LLM.int8()（Dettmers 等，[2022](#bib.bib12)）和 AWQ（Lin
    等，[2023](#bib.bib24)）的启发，我们提出了一种新颖的百分位剪切平滑策略，以进一步提升量化效果。我们方法的一个显著优点是其无梯度的特性，这确保了模型在不同领域和模态下的泛化能力。我们的优化方法也表现出卓越的效率。例如，我们可以在
    A100-80G 上仅用一个 GPU 小时实现 BLOOM-176B 的量化 A8W4 模型。
- en: The experimental results highlight the effectiveness of DGQ across a wide range
    of tasks, model families, and sizes. In particular, DGQ demonstrates impressive
    performance on the WikiText-2 (Merity et al., [2016](#bib.bib29)) task, achieving
    a mere 0.3 perplexity loss for integer A8W4 models. This outperforms the floating-point
    A8W4 quantization scheme (Wu et al., [2023](#bib.bib42)) by approximately 0.3.
    To support DGQ inference, we have implemented efficient CUTLASS kernels. Leveraging
    these efficient kernels, DGQ achieves a remarkable up to 3x speedup compared to
    the A16W4 baseline while maintaining similar memory usage. Furthermore, DGQ excels
    in overhead management, enabling it to achieve similar inference speeds to A8W8
    models but with only half the memory usage.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果突显了DGQ在各种任务、模型家族和规模上的有效性。特别是，DGQ在WikiText-2 （Merity等，[2016](#bib.bib29)）任务中表现出色，实现了整数A8W4模型仅0.3的困惑度损失。这比浮点A8W4量化方案（Wu等，[2023](#bib.bib42)）提高了约0.3。为了支持DGQ推断，我们实现了高效的CUTLASS内核。利用这些高效内核，DGQ实现了最高可达3倍的加速，相比于A16W4基准线，同时保持了相似的内存使用。此外，DGQ在开销管理方面表现卓越，使其能够实现与A8W8模型相似的推断速度，但内存使用量仅为其一半。
- en: 2 Related Works
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Large Language Models
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 大型语言模型
- en: Pre-trained large language models (LLMs), such as GPT-4 (Bubeck et al., [2023](#bib.bib5)),
    LLaMA-2 (Touvron et al., [2023b](#bib.bib38)), and OPT (Zhang et al., [2022](#bib.bib48)),
    have demonstrated exceptional performance across a wide range of tasks and domains.
    Despite their impressive capabilities, these models come with significant memory
    and computational demands, presenting challenges in practical deployment. To address
    these challenges, a growing body of research has emerged, focusing on various
    techniques to optimize LLMs. These approaches encompass model compression methods (Frantar
    & Alistarh, [2023](#bib.bib17); Xiao et al., [2023](#bib.bib43)), distributed
    computing strategies (Aminabadi et al., [2022](#bib.bib1)), and computational
    graph optimizations (Dao et al., [2022](#bib.bib10); Dao, [2023](#bib.bib9)).
    In this study, our primary focus is on model quantization, a key component of
    model compression. We aim to explore and advance quantization techniques to enable
    efficient deployment of LLMs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的大型语言模型（LLMs），如GPT-4（Bubeck等，[2023](#bib.bib5)）、LLaMA-2（Touvron等，[2023b](#bib.bib38)）和OPT（Zhang等，[2022](#bib.bib48)），在广泛的任务和领域中表现出色。尽管这些模型具有令人印象深刻的能力，但它们伴随有显著的内存和计算需求，这在实际部署中带来了挑战。为了解决这些挑战，越来越多的研究关注于优化LLM的各种技术。这些方法包括模型压缩方法（Frantar
    & Alistarh，[2023](#bib.bib17)；Xiao等，[2023](#bib.bib43)）、分布式计算策略（Aminabadi等，[2022](#bib.bib1)）和计算图优化（Dao等，[2022](#bib.bib10)；Dao，[2023](#bib.bib9)）。在这项研究中，我们的主要关注点是模型量化，这是模型压缩的关键组成部分。我们旨在探索和推进量化技术，以实现LLM的高效部署。
- en: 2.2 Model Quantization
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 模型量化
- en: Quantization-Aware Training (QAT) (Esser et al., [2019](#bib.bib16); Martinez
    et al., [2018](#bib.bib28)) and Post-training Quantization (PTQ) (Choukroun et al.,
    [2019](#bib.bib6); Li et al., [2021](#bib.bib23); Wei et al., [2022](#bib.bib40)).
    QAT fine-tunes quantized models with the full dataset, preserving accuracy but
    involving complex computations, making it less suitable for LLMs. In contrast,
    PTQ directly quantizes models with little data and computation. Techniques like
    AdaRound (Nagel et al., [2020](#bib.bib30)), and Adaquant (Hubara et al., [2020](#bib.bib21))
    optimize quantization parameters and distill quantized models layer by layer.
    Some approaches (Qin et al., [2022](#bib.bib32); He et al., [2022](#bib.bib20))
    employ alternating optimization, and some push the boundaries by compressing transformations
    into binary values. These quantization techniques are crucial for enhancing the
    efficiency of deep learning models, especially in resource-constrained environments.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 量化感知训练（QAT）（Esser等，[2019](#bib.bib16)；Martinez等，[2018](#bib.bib28)）和后训练量化（PTQ）（Choukroun等，[2019](#bib.bib6)；Li等，[2021](#bib.bib23)；Wei等，[2022](#bib.bib40)）。QAT使用完整数据集微调量化模型，保持准确性，但涉及复杂计算，使其不太适合LLM。相比之下，PTQ直接用少量数据和计算对模型进行量化。像AdaRound（Nagel等，[2020](#bib.bib30)）和Adaquant（Hubara等，[2020](#bib.bib21)）这样的技术优化量化参数，并逐层提炼量化模型。一些方法（Qin等，[2022](#bib.bib32)；He等，[2022](#bib.bib20)）采用交替优化，一些则通过将变换压缩成二进制值推动边界。这些量化技术对于提高深度学习模型的效率至关重要，尤其是在资源受限的环境中。
- en: For LLMs quantization, fine-grained quantization is introduced to solve the
    significant accuracy drop. PEG-PTQ (Bondarenko et al., [2021](#bib.bib3)) proposed
    per-embedding-group quantization, which splits the activation into several groups
    and quantizes activation via each group. ZeroQuant (Yao et al., [2022](#bib.bib44))
    used token-wise activation quantization and group-wise weight quantization as
    quantization scheme. LLM.int8() (Dettmers et al., [2022](#bib.bib12)) finds that
    the outliers in activation have a significant contributor to poor quantization
    performance and splits the outliers and calculate the matrix multiple evolved
    outliers in FP16\. GPTQ (Frantar et al., [2022](#bib.bib18)) and SparseGPT (Frantar
    & Alistarh, [2023](#bib.bib17)) use second-order approximation to quantize and
    prune weights. GPTQ also introduces channel-reorder via hessian matrix to weight
    to maintain better accuracy. RPTQ (Yuan et al., [2023](#bib.bib46)) reorders channels
    and splits activation into three groups via activation ranges. It successfully
    quants LLMs into fine-grained A4W4 models.SmoothQuant (Xiao et al., [2023](#bib.bib43))
    proposes a mathematically equivalent per-channel scaling transformation makes
    activation easier to quantize. Outlier Suppression+ (Wei et al., [2023](#bib.bib41))
    introduce a fusible offset for activation quantization and convert the hard-to-quant
    asymmetric quantization into symmetric quantization. AWQ (Lin et al., [2023](#bib.bib24))
    finds that the sensitive weight is based on significant activation channels and
    introduces a scale to amplify the significant weight channels. OmniQuant (Shao
    et al., [2023](#bib.bib36)) follows the Quadapter (Park et al., [2022](#bib.bib31))
    try to only optimize the smooth scale during optimization.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLMs量化，引入了细粒度量化以解决显著的准确率下降问题。PEG-PTQ（Bondarenko等，[2021](#bib.bib3)）提出了每嵌入组量化方法，该方法将激活分为多个组，并通过每个组量化激活。ZeroQuant（Yao等，[2022](#bib.bib44)）采用了基于令牌的激活量化和基于组的权重量化作为量化方案。LLM.int8()（Dettmers等，[2022](#bib.bib12)）发现激活中的离群值对差的量化性能有显著影响，并将离群值分开，并计算矩阵中的多个演化离群值，使用FP16格式。GPTQ（Frantar等，[2022](#bib.bib18)）和SparseGPT（Frantar
    & Alistarh，[2023](#bib.bib17)）使用二阶近似来量化和修剪权重。GPTQ还通过赫西安矩阵引入通道重排序，以保持更好的准确性。RPTQ（Yuan等，[2023](#bib.bib46)）重新排序通道，并通过激活范围将激活分为三组。它成功将LLMs量化为细粒度的A4W4模型。SmoothQuant（Xiao等，[2023](#bib.bib43)）提出了一种数学上等效的每通道缩放变换，使得激活更易于量化。Outlier
    Suppression+（Wei等，[2023](#bib.bib41)）引入了一个可融合的偏移量用于激活量化，并将难以量化的非对称量化转换为对称量化。AWQ（Lin等，[2023](#bib.bib24)）发现敏感权重基于显著的激活通道，并引入了一种缩放因子来放大显著的权重通道。OmniQuant（Shao等，[2023](#bib.bib36)）遵循了Quadapter（Park等，[2022](#bib.bib31)），尝试在优化过程中仅优化平滑缩放。
- en: ZeroQuantv2 (Yao et al., [2023](#bib.bib45)) introduces the Low Rank Compensation
    (LoRC) technique to enhance the precision of A8W4 quantized models, resulting
    in improved accuracy. However, it’s essential to note that LoRC involves FP16
    computations, which add computational overhead. ZeroQuant-FP (Wu et al., [2023](#bib.bib42))
    explores the use of A8W4 precision within a floating-point (FP) context, showcasing
    improved accuracy. It introduces an efficient method for optimizing weight scales,
    especially relevant given the limited support for FP8, primarily on H100 hardware.
    However, it’s worth noting that floating-point calculations generally entail higher
    computational costs compared to integer-based computations. QLoRA (Dettmers et al.,
    [2023a](#bib.bib13)) introduces the NF4 double quantization technique, which enhances
    memory utilization by quantizing FP32 group-wise quantization scales to FP8 while
    using a channel-wise FP32 scale. This method prioritizes memory efficiency over
    computational efficiency and shows that direct quantization of quantization scales
    to FP8 does not lead to substantial accuracy loss. In our proposed methods, we
    tackle the dual quantization challenge for integer (INT) values by introducing
    a lossless compression solution. This innovative approach seamlessly combines
    computational efficiency with memory conservation, presenting a promising solution
    within this field.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ZeroQuantv2 （姚等，[2023](#bib.bib45)）引入了低秩补偿（LoRC）技术，以提高A8W4量化模型的精度，从而提高了准确性。然而，需要注意的是，LoRC涉及FP16计算，这会增加计算开销。ZeroQuant-FP （吴等，[2023](#bib.bib42)）探索了在浮点（FP）环境中使用A8W4精度，展示了提高的准确性。它引入了一种高效的优化权重尺度的方法，特别是在FP8支持有限的情况下，主要是在H100硬件上。然而，值得注意的是，浮点计算通常比基于整数的计算涉及更高的计算成本。QLoRA （Dettmers等，[2023a](#bib.bib13)）引入了NF4双重量化技术，通过将FP32组尺度量化到FP8，同时使用通道尺度FP32，增强了内存利用率。这种方法优先考虑内存效率而非计算效率，并且表明直接将量化尺度量化到FP8不会导致显著的准确性损失。在我们提出的方法中，我们通过引入无损压缩解决方案来应对整数（INT）值的双重量化挑战。这种创新的方法将计算效率与内存节约无缝结合，展示了在该领域内的有前景的解决方案。
- en: 3 Method
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 3.1 Preliminary
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 初步
- en: 'Quantization is a crucial process that transforms high-precision values into
    low-precision representations. In our work, we emphasize the use of uniform integer
    quantization, which offers better hardware support and computational efficiency.
    Asymmetric quantization can be expressed as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一个关键过程，它将高精度值转换为低精度表示。在我们的工作中，我们强调使用统一整数量化，它提供了更好的硬件支持和计算效率。非对称量化可以表示如下：
- en: '|  | $\displaystyle{\bf\widehat{X}}=$ |  | (1) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bf\widehat{X}}=$ |  | (1) |'
- en: '|  | $\displaystyle\mathbf{ZP}$ |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{ZP}$ |  |'
- en: Here, $\mathbf{X}$.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\mathbf{X}$。
- en: There are two distinct quantization methods for LLM quantization, weight-only
    quantization and weight-activation quantization. Weight-only quantization focuses
    on quantizing only the model weights into low-bit representations while preserving
    full precision during inference. This method reduces memory requirements and storage
    demands. In contrast, weight-activation quantization extends quantization to both
    the model weights and input activations. By utilizing lower-bit representations
    for both weights and activations, this approach accelerates inference. In our
    paper, we primarily concentrate on weight-activation quantization, exploring its
    advantages and optimizing it to enhance overall model efficiency.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLM量化有两种不同的量化方法：仅权重量化和权重-激活量化。仅权重量化专注于将模型权重量化为低位表示，同时在推理过程中保持全精度。这种方法减少了内存需求和存储要求。相比之下，权重-激活量化将量化扩展到模型权重和输入激活。通过对权重和激活都使用低位表示，这种方法加速了推理。在我们的论文中，我们主要集中于权重-激活量化，探索其优势并优化以提升整体模型效率。
- en: 3.2 Dual Grained Quantization
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 双粒度量化
- en: 'For weight quantization, we consider two levels of granularity, i.e. channel-
    and group-wise quantization, as illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.2
    Dual Grained Quantization ‣ 3 Method ‣ Dual Grained Quantization: Efficient Fine-Grained
    Quantization for LLM") (a) and (b). Channel-wise quantization involves assigning
    a specific quantization scale to each output channel, which is then applied to
    the result of the integer General Matrix Multiplication (GEMM) operation. On the
    other hand, group-wise quantization is a more refined approach that divides the
    output channels into multiple groups and assigns a quantization scale to each
    group. These quantization scales are collectively represented as a matrix denoted
    by $\mathbf{S}$. In general, group-wise quantization tends to yield smaller quantization
    errors compared to channel-wise quantization, resulting in higher accuracy.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '对于权重量化，我们考虑了两级粒度，即通道级和组级量化，如图[2](#S3.F2 "Figure 2 ‣ 3.2 Dual Grained Quantization
    ‣ 3 Method ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization for
    LLM") (a) 和 (b) 所示。通道级量化涉及为每个输出通道分配特定的量化尺度，然后将其应用于整数通用矩阵乘法（GEMM）操作的结果。另一方面，组级量化是一种更精细的方法，将输出通道划分为多个组，并为每个组分配量化尺度。这些量化尺度被集体表示为一个矩阵，记作
    $\mathbf{S}$。一般而言，组级量化比通道级量化产生更小的量化误差，从而提高了准确性。'
- en: '![Refer to caption](img/876d63c25b8e964235533e99491436f2.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/876d63c25b8e964235533e99491436f2.png)'
- en: 'Figure 2: Different grained quantization method. Unlike channel-wise and DGQ
    quantization, group-wise quantization tends to result in FP16 accumulation, which
    is conflict with typical hardware design.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不同粒度的量化方法。与通道级和DGQ量化不同，组级量化通常会导致FP16累加，这与典型的硬件设计相冲突。
- en: To achieve model compression with lower bit precision, it becomes essential
    to introduce fine-grained quantization (e.g., group-wise) methods into weight
    quantization. However, group-wise quantization methods present a notable challenge
    when it comes to their implementation on hardware platforms. In group-wise quantization,
    the reduction axis is divided into multiple groups, each with distinct quantization
    scales. These groups effectively operate within separate INT8 domains, which poses
    limitations on their ability to perform direct INT8 General Matrix Multiplication
    (GEMM) operations. To facilitate Group-Wise INT8 quantization, the matrix is divided
    into segments, allowing for separate GEMM computations on each segment. Following
    these computations, the results are combined through FP16 accumulation. It’s worth
    noting, however, that the adoption of FP16 accumulation introduces a delay in
    the INT8 kernel’s GEMM calculations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现低位精度的模型压缩，引入细粒度量化（例如组级量化）方法成为关键。然而，组级量化方法在硬件平台上的实现呈现出显著挑战。在组级量化中，减缩轴被划分为多个组，每个组有不同的量化尺度。这些组实际上在独立的INT8域中操作，这限制了它们进行直接INT8通用矩阵乘法（GEMM）操作的能力。为了促进组级INT8量化，矩阵被划分为多个段，允许对每个段进行独立的GEMM计算。计算完成后，结果通过FP16累加进行合并。然而，值得注意的是，FP16累加的采用会在INT8内核的GEMM计算中引入延迟。
- en: 'To enhance the hardware efficiency of group-wise quantization, we introduce
    dual-grained quantization (DGQ) as shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.2
    Dual Grained Quantization ‣ 3 Method ‣ Dual Grained Quantization: Efficient Fine-Grained
    Quantization for LLM") (c). This approach incorporates a crucial dequantization
    step before the GEMM operation, which involves converting INT4 weights into INT8
    weights, enabling subsequent INT8 GEMM operations. Our method leverages group-wise
    INT8 quantization scales and zero points to transform INT4 weights into INT8 weights.
    Following this, we employ channel-wise FP16 quantization scales to determine the
    dequantization factors for the output. Given the hidden states $\mathbf{X}_{s8}\in\mathbb{N}_{s8}^{b\times
    h}$, the quantization process is defined by the following equations:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提高组级量化的硬件效率，我们引入了双粒度量化（DGQ），如图[2](#S3.F2 "Figure 2 ‣ 3.2 Dual Grained Quantization
    ‣ 3 Method ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization for
    LLM") (c) 所示。该方法在GEMM操作之前引入了一个关键的去量化步骤，即将INT4权重转换为INT8权重，从而支持后续的INT8 GEMM操作。我们的方法利用组级INT8量化尺度和零点将INT4权重转换为INT8权重。随后，我们使用通道级FP16量化尺度来确定输出的去量化因子。给定隐藏状态
    $\mathbf{X}_{s8}\in\mathbb{N}_{s8}^{b\times h}$，量化过程由以下方程定义：'
- en: '|  | $\displaystyle\mathbf{s}_{f16}$ |  | (2) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{s}_{f16}$ |  | (2) |'
- en: '|  | $\displaystyle\mathbf{W}_{s8}$ |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{W}_{s8}$ |  |'
- en: '|  | $\displaystyle\mathbf{O}_{f16}$ |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{O}_{f16}$ |  |'
- en: Here, $\mathbf{S}^{(2)}_{s8}\in\mathbb{N}_{s8}^{g\times o}$ represents the input
    channels for the weight. When comparing A16W4 to A8W4 with DGQ, it becomes evident
    that the latter configuration significantly accelerates the calculation process.
    Moreover, in comparison to A8W8, A8W4 with DGQ exhibits reduced weight memory
    usage and facilitates faster memory transfers. Our method offers a significant
    improvement in calculation matrix efficiency when compared to weight-only quantization
    with the same weight bitwidth. Furthermore, it demonstrates superior memory efficiency
    compared to weight-activation quantization with lower bit-width weights.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\mathbf{S}^{(2)}_{s8}\in\mathbb{N}_{s8}^{g\times o}$表示权重的输入通道。当将A16W4与A8W4的DGQ进行比较时，很明显后者配置显著加速了计算过程。此外，与A8W8相比，A8W4与DGQ在权重内存使用上减少，并促进了更快的内存传输。与相同权重位宽的权重量化相比，我们的方法在计算矩阵效率上有显著改善。此外，与位宽较低的权重的权重-激活量化相比，它显示出优越的内存效率。
- en: '![Refer to caption](img/56a2015bc9aa816c27ae5091d964a5ac.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/56a2015bc9aa816c27ae5091d964a5ac.png)'
- en: 'Figure 3: DGQ inference schema and quantization schema with Two-Phase Search
    algorithm.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：DGQ推理模式和带有两阶段搜索算法的量化模式。
- en: 'Table 1: WikiText-2 Results for A8W4 LLaMA family: Original Group-Wise Quantization
    vs. Round to Nearst(RTN) Dual-Grained Quantization.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：A8W4 LLaMA系列的WikiText-2结果：原始组级量化与最近舍入（RTN）双粒度量化。
- en: '| LLaMA PPL $\downarrow$ | 1-7B | 1-13B | 1-30B | 2-7B | 2-13B |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA PPL $\downarrow$ | 1-7B | 1-13B | 1-30B | 2-7B | 2-13B |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Original | 6.03 | 5.39 | 4.43 | 5.87 | 5.23 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | 6.03 | 5.39 | 4.43 | 5.87 | 5.23 |'
- en: '| RTN | 6.93 | 6.00 | 4.83 | 2155.54 | 5.48 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 6.93 | 6.00 | 4.83 | 2155.54 | 5.48 |'
- en: 3.3 Two-phase Grid Search
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 两阶段网格搜索
- en: A significant quantization error is introduced when directly quantizing group-wise
    parameters $\mathbf{S^{\prime}}$. To address these two problem, we propose a novel
    two-phase search approach.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 直接对组参数$\mathbf{S^{\prime}}$进行量化会引入显著的量化误差。为了解决这两个问题，我们提出了一种新颖的两阶段搜索方法。
- en: 'To narrow down the expansive search space, our initial step involves quantizing
    the weights into group-wise INT4 representations. This approach capitalizes on
    the assumption of weight independence across in-channels. We divide both weights
    and activations into $\mathbf{n_{g}}$ as the parallelism axis, and our goal is
    to formulate the minimum granularity optimization problem as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缩小广泛的搜索空间，我们的初步步骤是将权重量化为组级INT4表示。这种方法利用了在通道间权重独立的假设。我们将权重和激活值分成$\mathbf{n_{g}}$作为并行轴，我们的目标是将最小粒度优化问题表述如下：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: Here, $k$.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是$k$。
- en: For next phase, we aim to decouple the group-wise FP16 scale $\mathbf{S}^{\prime}$.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一阶段，我们的目标是解耦组级FP16缩放$\mathbf{S}^{\prime}$。
- en: '|  | $\mathbf{W}_{u4}\in\mathbf{[0,15]},\mathbf{S^{(2)}}\in\mathbf{[-128,127]},\mathbf{W}_{s8}\in\mathbf{[-128,127]}$
    |  | (4) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{W}_{u4}\in\mathbf{[0,15]},\mathbf{S^{(2)}}\in\mathbf{[-128,127]},\mathbf{W}_{s8}\in\mathbf{[-128,127]}$
    |  | (4) |'
- en: 'Remarkably, we can consolidate the range for $\mathbf{W_{s8}}$. The proof of
    this consolidation can be found in Appendix B. This fusion leads us to the following
    constraint for our search space:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们可以合并$\mathbf{W_{s8}}$的范围。关于这一整合的证明可以在附录B中找到。这一融合导致我们对搜索空间提出了以下约束：
- en: '|  | $\mathbf{W}_{u4}\in\mathbf{[max(0,\lfloor\frac{-127}{S^{(2)}}\rceil+ZP),min(15,\lfloor\frac{127}{S^{(2)}}\rceil+ZP)]}\\
    $ |  | (5) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{W}_{u4}\in\mathbf{[max(0,\lfloor\frac{-127}{S^{(2)}}\rceil+ZP),min(15,\lfloor\frac{127}{S^{(2)}}\rceil+ZP)]}\\
    $ |  | (5) |'
- en: 'Similar to the prior phase, we also use the grid employing for the scaling
    parameter $\alpha$. The optimization problem for this phase is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于前一阶段，我们也使用网格来处理缩放参数$\alpha$。该阶段的优化问题如下：
- en: '|  | $\mathop{\arg\min}\limits_{\mathbf{{s}^{(1)}}}\&#124;\mathbf{X}\mathbf{W}-\widehat{\mathbf{X}}\mathcal{Q}_{W}(\mathbf{W},\mathbf{S},\mathbf{ZP})\&#124;^{2},\mathbf{S^{(2)}=\lfloor\frac{S^{\prime}}{s^{(1)}}\rceil},\mathbf{S=s^{(1)}\cdot
    S^{(2)}}$ |  | (6) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathop{\arg\min}\limits_{\mathbf{{s}^{(1)}}}\&#124;\mathbf{X}\mathbf{W}-\widehat{\mathbf{X}}\mathcal{Q}_{W}(\mathbf{W},\mathbf{S},\mathbf{ZP})\&#124;^{2},\mathbf{S^{(2)}=\lfloor\frac{S^{\prime}}{s^{(1)}}\rceil},\mathbf{S=s^{(1)}\cdot
    S^{(2)}}$ |  | (6) |'
- en: 'Here, we replace the max value and min value in Eq. [1](#S3.E1 "In 3.1 Preliminary
    ‣ 3 Method ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization for
    LLM") with Eq. [5](#S3.E5 "In 3.3 Two-phase Grid Search ‣ 3 Method ‣ Dual Grained
    Quantization: Efficient Fine-Grained Quantization for LLM") to prevent overflow
    or underflow in integer representations. It’s worth noting that this phase allows
    for efficient parallelization along the $\mathbf{o}$. This approach significantly
    reduces the search space compared to the one-step search. Experimental results
    demonstrate that our method introduces virtually no additional error.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们将方程式[1](#S3.E1 "In 3.1 Preliminary ‣ 3 Method ‣ Dual Grained Quantization:
    Efficient Fine-Grained Quantization for LLM")中的最大值和最小值替换为方程式[5](#S3.E5 "In 3.3
    Two-phase Grid Search ‣ 3 Method ‣ Dual Grained Quantization: Efficient Fine-Grained
    Quantization for LLM")，以防止整数表示中的溢出或下溢。值得注意的是，这一阶段允许沿着$\mathbf{o}$进行高效的并行处理。与一步搜索相比，这种方法显著减少了搜索空间。实验结果表明，我们的方法几乎没有引入额外的误差。'
- en: 3.4 percentile clipping smoothing
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 百分位裁剪平滑
- en: As discussed in prior works (Xiao et al., [2023](#bib.bib43)), outliers in activation
    have a large impact on model accuracy. A channel-wise smooth would help limit
    the influence of outliers for activation quantization. For weight quantization,
    AWQ (Lin et al., [2023](#bib.bib24)) proposed to use activation outliers to amplify
    the weight in outlier channels to decrease the quantization error of such channels.
    This is similar in principle to reordering in GPTQ (Frantar et al., [2022](#bib.bib18)).
    As the quantization-sensitivity can be approximated by the Hessian matrix of weights.
    Hessian matrix of weights is calculated by $\mathbf{H=XX^{T}}$ and is directly
    related to the absolute value of the input. Therefore, the mean value of the input
    channel for activation can approximately present the quantization sensitivity
    of weight.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如先前工作（Xiao 等，[2023](#bib.bib43)）中所讨论的，激活中的异常值对模型准确度有很大影响。逐通道平滑有助于限制异常值对激活量化的影响。对于权重量化，AWQ（Lin
    等，[2023](#bib.bib24)）建议利用激活异常值来放大异常通道中的权重，从而减少这些通道的量化误差。这在原理上类似于GPTQ（Frantar 等，[2022](#bib.bib18)）中的重新排序。由于量化敏感度可以通过权重的Hessian矩阵来近似，Hessian矩阵的计算公式为$\mathbf{H=XX^{T}}$，并且与输入的绝对值直接相关。因此，激活的输入通道的均值可以大致表示权重的量化敏感度。
- en: For weight-activation smooth scales, the quantization difficulty that moves
    from activation to weight could act as the quantization amplifier for weights,
    making it possible as a win-win for both weight and activation quantization. And
    from the observation from  Dettmers et al. ([2023b](#bib.bib14)), the outliers
    always appear at fixed channels, which means that the biggest maximum and mean
    channels are almost the same channels. We can conduct these two methods into one
    activation-to-weight smooth.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于权重-激活平滑尺度，激活到权重的量化困难可以作为权重的量化放大器，使得权重和激活的量化都能实现双赢。从Dettmers 等（[2023b](#bib.bib14)）的观察来看，异常值总是出现在固定的通道，这意味着最大的最大值和均值通道几乎是相同的通道。我们可以将这两种方法合并为一种激活到权重的平滑方法。
- en: LLM.int8() finds that keeping the activation channels with outliers unquantized
    will help the model maintain its performance. It’s interesting that AWQ also finds
    that keeping that 1% salient channels for weight would protect the performance.
    We can follow their conclusions for weight-activation quantization.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: LLM.int8()发现，保持带有异常值的激活通道不进行量化将有助于模型保持其性能。有趣的是，AWQ也发现，保持1%的显著通道的权重会保护性能。我们可以遵循他们的结论进行权重-激活量化。
- en: 'In our smooth strategy, we calculate the smooth scale $\mathbf{k}_{j}$ as:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的平滑策略中，我们计算平滑尺度$\mathbf{k}_{j}$如下：
- en: '|  | $\mathbf{z}_{j}=\mathbf{max}(&#124;\mathbf{X}_{[j,:]}&#124;),\ \mathbf{k}_{j}=\mathbf{clamp}(\mathbf{z}_{j}/\mathbf{max}_{0.5\%}(\mathbf{z}),\text{low}=1)$
    |  | (7) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{z}_{j}=\mathbf{max}(\|\mathbf{X}_{[j,:]}\|),\ \mathbf{k}_{j}=\mathbf{clamp}(\mathbf{z}_{j}/\mathbf{max}_{0.5\%}(\mathbf{z}),\text{low}=1)$
    |  | (7) |'
- en: Here, we use the top 0.5% of the largest values as the clipping threshold. We
    constrain outliers larger than 0.5% by setting their value to 0.5% and employ
    the resulting scale as an amplifier for quantization-sensitive channels.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用最大值的前0.5%作为裁剪阈值。我们通过将大于0.5%的异常值的值设置为0.5%来限制这些异常值，并使用结果的尺度作为量化敏感通道的放大器。
- en: 4 Experiments
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Experiments Setups
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Baseline. We compare with baselines in the A8W4 post-training quantization setting,
    ZeroQuantv2 (Yao et al., [2023](#bib.bib45)), SmoothQuant (Xiao et al., [2023](#bib.bib43)),
    RPTQ (Yuan et al., [2023](#bib.bib46)) and ZeroQuant-FP (Wu et al., [2023](#bib.bib42)).
    Since the quantization schemes vary from different quantization methods, we test
    one aligned quantization scheme per-token dynamic activation quantization and
    group-wise weight quantization. Additionally, we include results for static activation
    quantization to facilitate further comparisons. AWQ (Lin et al., [2023](#bib.bib24))
    and GPTQ (Frantar et al., [2022](#bib.bib18)), which is designed for weight-only
    quantization. As activation quantization is harder for activation, comparison
    to those methods in A8W4 quantization schemes is unfair. We try to compare the
    results of A8W4 to A16W3, since they are both the next step of A16W4.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 基线。我们在 A8W4 后训练量化设置中与基线进行比较，包括 ZeroQuantv2 (Yao et al., [2023](#bib.bib45))、SmoothQuant (Xiao
    et al., [2023](#bib.bib43))、RPTQ (Yuan et al., [2023](#bib.bib46)) 和 ZeroQuant-FP (Wu
    et al., [2023](#bib.bib42))。由于量化方案在不同的量化方法中有所不同，我们测试了一种对齐的量化方案，每个令牌的动态激活量化和组别权重量化。此外，我们还包括了静态激活量化的结果，以便进行进一步的比较。AWQ (Lin
    et al., [2023](#bib.bib24)) 和 GPTQ (Frantar et al., [2022](#bib.bib18))，后者设计用于仅权重量化。由于激活量化对于激活来说更为困难，与
    A8W4 量化方案中的这些方法进行比较是不公平的。我们尝试将 A8W4 的结果与 A16W3 进行比较，因为它们都是 A16W4 的下一步。
- en: 'Models and datasets. We choose OPT (Zhang et al., [2022](#bib.bib48)) and LLaMA (Touvron
    et al., [2023a](#bib.bib37); [b](#bib.bib38)) families to evaluate our quantization
    methods. As BLOOM (Scao et al., [2022](#bib.bib35)) has a similar structure as
    OPT and have close quantization performance. For Common Sense Question Answers
    evaluation, we use five zero-shot evaluation task: HellaSwag (Zellers et al.,
    [2019](#bib.bib47)), PIQA (Bisk et al., [2020](#bib.bib2)), Winogrande (Sakaguchi
    et al., [2021](#bib.bib34)), BoolQ (Clark et al., [2019](#bib.bib7)) and ARC (Clark
    et al., [2018](#bib.bib8)). Common Sense Question Answers benchmark is done with
    lm-eval (Lin & Chen, [2023](#bib.bib25)). Follows the evaluation proposed at GPTQ (Frantar
    et al., [2022](#bib.bib18)), we use WikiText-2 (Merity et al., [2016](#bib.bib29)),
    PTB (Marcus et al., [1994](#bib.bib27)) and C4 (Raffel et al., [2020](#bib.bib33))
    to compare the generation ability.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和数据集。我们选择了 OPT (Zhang et al., [2022](#bib.bib48)) 和 LLaMA (Touvron et al.,
    [2023a](#bib.bib37); [b](#bib.bib38)) 系列来评估我们的量化方法。由于 BLOOM (Scao et al., [2022](#bib.bib35))
    具有与 OPT 相似的结构，并且量化性能接近。对于常识问答的评估，我们使用了五个零样本评估任务：HellaSwag (Zellers et al., [2019](#bib.bib47))、PIQA (Bisk
    et al., [2020](#bib.bib2))、Winogrande (Sakaguchi et al., [2021](#bib.bib34))、BoolQ (Clark
    et al., [2019](#bib.bib7)) 和 ARC (Clark et al., [2018](#bib.bib8))。常识问答基准测试使用
    lm-eval (Lin & Chen, [2023](#bib.bib25)) 完成。按照 GPTQ (Frantar et al., [2022](#bib.bib18))
    提出的评估方法，我们使用了 WikiText-2 (Merity et al., [2016](#bib.bib29))、PTB (Marcus et al.,
    [1994](#bib.bib27)) 和 C4 (Raffel et al., [2020](#bib.bib33)) 来比较生成能力。
- en: Implementation. We implement our methods with Pytorch Huggingface for the proof
    of concept. We use the CUTLASS GEMM kernels to develop our two-grained quantization
    kernels, and we use the INT8 BMM kernels from torch-int.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 实现。我们使用 Pytorch Huggingface 实现了我们的方案以验证概念。我们使用 CUTLASS GEMM 内核开发了两种粒度的量化内核，并使用
    torch-int 提供的 INT8 BMM 内核。
- en: 'Table 2: Quantization Results on WikiText-2 with A16W3 and A8W4 LLaMA Models.
    C4 perplexity results can be found in Table [A1](#A3.T1 "Table A1 ‣ Appendix A3
    More accuracy results ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization
    for LLM") in Appendix [A3](#A3 "Appendix A3 More accuracy results ‣ Dual Grained
    Quantization: Efficient Fine-Grained Quantization for LLM"). $\dagger$ indicates
    static quantization for activation.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2：A16W3 和 A8W4 LLaMA 模型在 WikiText-2 上的量化结果。C4 困惑度结果可以在附录 [A1](#A3.T1 "Table
    A1 ‣ Appendix A3 More accuracy results ‣ Dual Grained Quantization: Efficient
    Fine-Grained Quantization for LLM") 的表格中找到，附录 [A3](#A3 "Appendix A3 More accuracy
    results ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM")。$\dagger$
    表示激活的静态量化。'
- en: '| LLaMA PPL $\downarrow$ | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B | 2-70B
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA PPL $\downarrow$ | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B | 2-70B
    |'
- en: '| FP16 | - | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 |'
- en: '| W3A16 g128 | RTN | 7.01 | 5.88 | 4.87 | 4.24 | 6.66 | 5.51 | 3.97 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| W3A16 g128 | RTN | 7.01 | 5.88 | 4.87 | 4.24 | 6.66 | 5.51 | 3.97 |'
- en: '| GPTQ | 6.55 | 5.62 | 4.80 | 4.17 | 6.29 | 5.42 | 3.85 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 6.55 | 5.62 | 4.80 | 4.17 | 6.29 | 5.42 | 3.85 |'
- en: '| AWQ | 6.46 | 5.51 | 4.63 | 3.99 | 6.24 | 5.32 | - |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 6.46 | 5.51 | 4.63 | 3.99 | 6.24 | 5.32 | - |'
- en: '| W4A8 g128 | RTN | 8.72 | 7.81 | 6.76 | 6.16 | 12.85 | 44.82 | 8.70 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| W4A8 g128 | RTN | 8.72 | 7.81 | 6.76 | 6.16 | 12.85 | 44.82 | 8.70 |'
- en: '| ZeroQuantv2 | 6.44 | 5.32 | 4.36 | - | - | - | - |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ZeroQuantv2 | 6.44 | 5.32 | 4.36 | - | - | - | - |'
- en: '| SmoothQuant | 6.04 | 5.36 | 4.48 | 3.98 | 5.97 | 5.23 | 3.65 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 6.04 | 5.36 | 4.48 | 3.98 | 5.97 | 5.23 | 3.65 |'
- en: '| ZeroQuant-FP | 6.32 | 5.26 | 4.26 | - | - | - | - |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ZeroQuant-FP | 6.32 | 5.26 | 4.26 | - | - | - | - |'
- en: '| Ours | 5.85 | 5.21 | 4.28 | 3.71 | 5.64 | 5.01 | 3.45 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 5.85 | 5.21 | 4.28 | 3.71 | 5.64 | 5.01 | 3.45 |'
- en: '| Ours^† | 6.04 | 5.39 | 4.45 | 3.89 | 5.87 | 5.23 | 3.74 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 我们的^† | 6.04 | 5.39 | 4.45 | 3.89 | 5.87 | 5.23 | 3.74 |'
- en: 4.2 Accuracy Evaluation
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 准确性评估
- en: Results on LLaMA Family. Our static activation quantization surpasses INT3 weight-only
    quantization, demonstrating that A8W4 quantization offers more sophisticated deployment
    options than A16W3\. Furthermore, our quantized models consistently outperform
    their half-size counterparts, demonstrating the practical feasibility of dual-grained
    quantization (DGQ). In addition, our quantization methods consistently produce
    results that are comparable to, or even superior to, those achieved by ZeroQuantFP,
    a method that employs FP8 for quantization. This advantage is especially notable
    when applied to smaller models with identical quantization settings. It’s important
    to note that the utilization of FP8, as reported by van Baalen et al. ([2023](#bib.bib39)),
    comes at the cost of increased chip area and extended inference times. In the
    realm of Common Sense Question Answering tasks, our dynamic quantization methods
    outperform other quantization approaches. Additionally, our static quantization
    schemes yield results that are on par with gradient-based methods (Liu et al.,
    [2023](#bib.bib26)). The application of GLU (Dauphin et al., [2017](#bib.bib11))
    in LLaMA amplify the outliers by element-wise multiplication, making quantization
    difficult. While dynamic activation quantization can mitigate this issue, it necessitates
    computing statistics for each token before passing through linear kernels, impacting
    processing time.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA 家族的结果。我们的静态激活量化超越了仅重量化的 INT3，展示了 A8W4 量化提供了比 A16W3 更复杂的部署选项。此外，我们的量化模型始终优于其半尺寸的对应物，展示了双粒度量化
    (DGQ) 的实际可行性。此外，我们的量化方法持续产生与 ZeroQuantFP（使用 FP8 进行量化）相当或更优的结果。特别是在应用于具有相同量化设置的小型模型时，这一优势尤为明显。值得注意的是，正如
    van Baalen 等人 ([2023](#bib.bib39)) 报告的，使用 FP8 的代价是增加了芯片面积和延长了推理时间。在常识问答任务中，我们的动态量化方法优于其他量化方法。此外，我们的静态量化方案在与基于梯度的方法（Liu
    等人，[2023](#bib.bib26)）相比时表现出色。LLaMA 中的 GLU (Dauphin 等人，[2017](#bib.bib11)) 通过逐元素乘法放大了异常值，使量化变得困难。虽然动态激活量化可以缓解这一问题，但它需要在通过线性内核之前计算每个
    token 的统计数据，从而影响处理时间。
- en: 'Table 3: CSQA Reuslts on Six zero-shot tasks with A8W4 LLaMA Models. Due to
    the unavailability of the identical model as LLM-QAT, we present FP16 accuracy
    data sourced from LLM-QAT. MMLU results can be found in Table [A4](#A3.T4 "Table
    A4 ‣ Appendix A3 More accuracy results ‣ Dual Grained Quantization: Efficient
    Fine-Grained Quantization for LLM") in Appendix [A3](#A3 "Appendix A3 More accuracy
    results ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM").
    $\dagger$ indicates static quantization for activation.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在六个零样本任务上的 CSQA 结果，使用 A8W4 LLaMA 模型。由于没有与 LLM-QAT 完全相同的模型，我们展示了来自 LLM-QAT
    的 FP16 准确性数据。MMLU 结果可以在附录 [A4](#A3.T4 "表 A4 ‣ 附录 A3 更多准确性结果 ‣ 双粒度量化：LLM 的高效细粒度量化")
    中找到。$\dagger$ 表示激活的静态量化。
- en: '| LLaMA / Acc$\uparrow$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA / 准确率$\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-1-7B | FP16 | 79.3 | 73.0 | 48.0 | 76.8 | 76.1 | 70.0 | 70.5 | 0.0
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-1-7B | FP16 | 79.3 | 73.0 | 48.0 | 76.8 | 76.1 | 70.0 | 70.5 | 0.0
    |'
- en: '| SmoothQunat | 76.0 | 67.4 | 42.8 | 71.0 | 67.8 | 66.0 | 65.2 | 5.3 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQunat | 76.0 | 67.4 | 42.8 | 71.0 | 67.8 | 66.0 | 65.2 | 5.3 |'
- en: '| LLM-QAT | 77.5 | 70.2 | 45.6 | 74.6 | 73.5 | 67.7 | 68.2 | 2.3 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QAT | 77.5 | 70.2 | 45.6 | 74.6 | 73.5 | 67.7 | 68.2 | 2.3 |'
- en: '| FP16 | 79.2 | 73.0 | 44.7 | 75.1 | 76.2 | 70.0 | 69.7 | 0.0 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 79.2 | 73.0 | 44.7 | 75.1 | 76.2 | 70.0 | 69.7 | 0.0 |'
- en: '| Ours | 78.8 | 72.4 | 43.9 | 74.7 | 74.9 | 70.2 | 69.2 | 0.5 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 78.8 | 72.4 | 43.9 | 74.7 | 74.9 | 70.2 | 69.2 | 0.5 |'
- en: '| Ours^† | 77.4 | 70.4 | 42.7 | 69.1 | 73.0 | 68.6 | 66.9 | 2.8 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 我们的^† | 77.4 | 70.4 | 42.7 | 69.1 | 73.0 | 68.6 | 66.9 | 2.8 |'
- en: '| LLaMA-1-13B | FP16 | 80.0 | 74.5 | 52.6 | 78.1 | 79.2 | 73.6 | 73.0 | 0.0
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-1-13B | FP16 | 80.0 | 74.5 | 52.6 | 78.1 | 79.2 | 73.6 | 73.0 | 0.0
    |'
- en: '| SmoothQunat | 77.1 | 67.4 | 43.4 | 72.5 | 74.3 | 69.5 | 67.4 | 5.6 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQunat | 77.1 | 67.4 | 43.4 | 72.5 | 74.3 | 69.5 | 67.4 | 5.6 |'
- en: '| LLM-QAT | 79.1 | 73.0 | 51.9 | 77.5 | 77.5 | 70.6 | 71.6 | 1.4 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QAT | 79.1 | 73.0 | 51.9 | 77.5 | 77.5 | 70.6 | 71.6 | 1.4 |'
- en: '| FP16 | 80.3 | 74.6 | 47.7 | 77.9 | 79.1 | 72.4 | 72.0 | 0.0 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 80.3 | 74.6 | 47.7 | 77.9 | 79.1 | 72.4 | 72.0 | 0.0 |'
- en: '| Ours | 80.1 | 73.7 | 46.8 | 77.8 | 78.6 | 71.7 | 71.5 | 0.5 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 我们 | 80.1 | 73.7 | 46.8 | 77.8 | 78.6 | 71.7 | 71.5 | 0.5 |'
- en: '| Ours^† | 79.2 | 72.5 | 47.2 | 73.9 | 77.3 | 70.6 | 70.1 | 1.9 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 我们^† | 79.2 | 72.5 | 47.2 | 73.9 | 77.3 | 70.6 | 70.1 | 1.9 |'
- en: '| LLaMA-1-30B | FP16 | 82.1 | 80.0 | 58.0 | 83.2 | 82.9 | 75.6 | 77.0 | 0.0
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-1-30B | FP16 | 82.1 | 80.0 | 58.0 | 83.2 | 82.9 | 75.6 | 77.0 | 0.0
    |'
- en: '| SmoothQunat | 79.5 | 76.5 | 54.5 | 74.9 | 76.9 | 70.6 | 72.2 | 4.8 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQunat | 79.5 | 76.5 | 54.5 | 74.9 | 76.9 | 70.6 | 72.2 | 4.8 |'
- en: '| LLM-QAT | 80.9 | 80.3 | 56.5 | 81.3 | 81.3 | 76.3 | 76.1 | 0.9 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QAT | 80.9 | 80.3 | 56.5 | 81.3 | 81.3 | 76.3 | 76.1 | 0.9 |'
- en: '| FP16 | 82.1 | 79.0 | 53.1 | 82.7 | 82.6 | 75.6 | 75.8 | 0.0 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 82.1 | 79.0 | 53.1 | 82.7 | 82.6 | 75.6 | 75.8 | 0.0 |'
- en: '| Ours | 81.6 | 78.5 | 53.6 | 83.1 | 82.1 | 74.6 | 75.6 | 0.2 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 我们 | 81.6 | 78.5 | 53.6 | 83.1 | 82.1 | 74.6 | 75.6 | 0.2 |'
- en: '| Ours^† | 79.7 | 78.0 | 51.8 | 80.2 | 79.8 | 74.0 | 73.9 | 1.9 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 我们^† | 79.7 | 78.0 | 51.8 | 80.2 | 79.8 | 74.0 | 73.9 | 1.9 |'
- en: Results on OPT Family. In our evaluation of OPT models spanning from 125M to
    66B parameters, we observe that the gap between static and dynamic activation
    quantization is relatively narrow. This phenomenon is attributed to the fact that
    the outliers in OPT models tend to be smaller compared to LLaMA models. Compared
    to RPTQ, which is also static activation quantization, our methods achieve superior
    results. As our coarse is finer than RPTQ. But the RPTQ needs to reorder the input
    channel at each layer norm, introducing extra inference cost. After all, our method
    outperforms both other quantization methods on OPT Family, demonstrating the generality
    of our method to different model families and model sizes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 结果在OPT家族上。在我们对范围从125M到66B参数的OPT模型进行的评估中，我们观察到静态和动态激活量化之间的差距相对较小。这一现象归因于OPT模型中的异常值通常比LLaMA模型中的要小。与同样是静态激活量化的RPTQ相比，我们的方法表现更优。因为我们的粗略量化比RPTQ更精细。但RPTQ需要在每层归一化时重新排序输入通道，增加了额外的推理成本。总之，我们的方法在OPT家族上优于其他量化方法，展示了我们方法对不同模型家族和模型规模的通用性。
- en: 'Table 4: Quantization Results on WikiText-2 with A16W3 and A8W4 OPT Models.
    C4 and PTB perplexity can be found in Table [A2](#A3.T2 "Table A2 ‣ Appendix A3
    More accuracy results ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization
    for LLM") and Table [A3](#A3.T3 "Table A3 ‣ Appendix A3 More accuracy results
    ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM") in
    Appendix [A3](#A3 "Appendix A3 More accuracy results ‣ Dual Grained Quantization:
    Efficient Fine-Grained Quantization for LLM"). $\dagger$ indicates static quantization
    for activation.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：WikiText-2上使用A16W3和A8W4 OPT模型的量化结果。C4和PTB困惑度可以在附录[A2](#A3.T2 "表A2 ‣ 附录A3
    更多准确性结果 ‣ 双粒度量化：高效细粒度量化用于LLM")和附录[A3](#A3.T3 "表A3 ‣ 附录A3 更多准确性结果 ‣ 双粒度量化：高效细粒度量化用于LLM")中的表中找到。$\dagger$表示激活的静态量化。
- en: '| OPT PPL $\downarrow$ | 125M | 1.3B | 2.7B | 6.7B | 13B | 30B | 66B |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| OPT PPL $\downarrow$ | 125M | 1.3B | 2.7B | 6.7B | 13B | 30B | 66B |'
- en: '| FP16 | - | 27.65 | 14.63 | 12.47 | 10.86 | 10.12 | 9.56 | 9.34 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 27.65 | 14.63 | 12.47 | 10.86 | 10.12 | 9.56 | 9.34 |'
- en: '| W3A16 g128 | RTN | 51.22 | 119.00 | 297.98 | 23.54 | 46.03 | 18.80 | 136.89
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| W3A16 g128 | RTN | 51.22 | 119.00 | 297.98 | 23.54 | 46.03 | 18.80 | 136.89
    |'
- en: '| GPTQ | 39.24 | 16.47 | 13.69 | 11.65 | 10.35 | 9.73 | 10.96 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 39.24 | 16.47 | 13.69 | 11.65 | 10.35 | 9.73 | 10.96 |'
- en: '| AWQ | 36.74 | 16.32 | 13.58 | 11.41 | 10.68 | 9.85 | 9.60 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 36.74 | 16.32 | 13.58 | 11.41 | 10.68 | 9.85 | 9.60 |'
- en: '| W4A8 g128 | RTN | 32.21 | 17.33 | 15.51 | 51.57 | 3978.101 | 2407.99 | 2832.57
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| W4A8 g128 | RTN | 32.21 | 17.33 | 15.51 | 51.57 | 3978.101 | 2407.99 | 2832.57
    |'
- en: '| ZeroQuantv2 | 31.69 | 15.53 | 13.02 | 11.29 | 10.43 | 9.86 | 9.62 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| ZeroQuantv2 | 31.69 | 15.53 | 13.02 | 11.29 | 10.43 | 9.86 | 9.62 |'
- en: '| SmoothQuant | 29.01 | 14.71 | 12.71 | 10.90 | 10.25 | 9.57 | 9.32 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 29.01 | 14.71 | 12.71 | 10.90 | 10.25 | 9.57 | 9.32 |'
- en: '| RPTQ | - | 15.39 | - | 11.21 | 10.90 | 10.22 | 9.46 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| RPTQ | - | 15.39 | - | 11.21 | 10.90 | 10.22 | 9.46 |'
- en: '| ZeroQuant-FP | - | 15.32 | - | 10.89 | 10.16 | 9.52 | - |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ZeroQuant-FP | - | 15.32 | - | 10.89 | 10.16 | 9.52 | - |'
- en: '| Ours | 29.25 | 14.78 | 12.67 | 10.93 | 10.29 | 9.53 | 9.31 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 我们 | 29.25 | 14.78 | 12.67 | 10.93 | 10.29 | 9.53 | 9.31 |'
- en: '| Ours$\dagger$ | 29.94 | 14.96 | 12.75 | 10.92 | 10.30 | 9.55 | 9.32 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 我们$\dagger$ | 29.94 | 14.96 | 12.75 | 10.92 | 10.30 | 9.55 | 9.32 |'
- en: 4.3 Efficiency Evaluation
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 效率评估
- en: 'In Figure [4](#S4.F4 "Figure 4 ‣ 4.3 Efficiency Evaluation ‣ 4 Experiments
    ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM"), we
    compare the end-to-end efficiency of OPT-30B and LLaMa-30B models using different
    quantization methods: SmoothQuant (A8W8), AWQ (A8W4), and our methods. As SmoothQuant
    and AWQ both give their implement code, we directly test the implementation time
    on a single 80G A100 GPU. For shorter sequences, A8W4 implementation takes more
    time than A8W8 and FP16 due to additional computation introduced by group-wise
    quantization. However, as sequences get longer, A8W4 outperforms A8W8 because
    it fuses dequantization and matrix multiplication efficiently. AWQ (A16W4) performs
    well but struggles with long sequences due to activation bottlenecks. Our method
    achieves A8W8-level inference times with half the memory usage, demonstrating
    its efficiency across various quantization methods.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [4](#S4.F4 "图 4 ‣ 4.3 效率评估 ‣ 4 实验 ‣ 双重粒度量化：高效的细粒度量化方法")中，我们比较了OPT-30B和LLaMa-30B模型在使用不同量化方法时的端到端效率：SmoothQuant（A8W8）、AWQ（A8W4）以及我们的方法。由于SmoothQuant和AWQ都提供了其实现代码，我们直接在单个80G
    A100 GPU上测试了实现时间。对于较短的序列，A8W4实现的时间比A8W8和FP16更长，因为分组量化引入了额外的计算。然而，随着序列变长，A8W4在融合去量化和矩阵乘法方面表现优于A8W8。AWQ（A16W4）表现良好，但由于激活瓶颈，在长序列上表现较差。我们的方法在内存使用量为一半的情况下实现了A8W8水平的推理时间，展示了其在各种量化方法中的效率。
- en: '![Refer to caption](img/4cd2976622d0d7b8fd93c98a00d44221.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4cd2976622d0d7b8fd93c98a00d44221.png)'
- en: 'Figure 4: Runtime and Memory Usage for LLaMA-30B and OPT-30B Across Varying
    Sequence Lengths on a Single A100-80G GPU.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: LLaMA-30B和OPT-30B在单个A100-80G GPU上对不同序列长度的运行时间和内存使用情况。'
- en: 4.4 Ablation Study
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 消融研究
- en: 'Different quantization scheme for A8W4 LLaMA models. In our experiments with
    LLaMA1-7b and LLaMA1-13b models, we explored different quantization schemes, including
    A8W4 quantization, as detailed in Table [5](#S4.T5 "Table 5 ‣ 4.4 Ablation Study
    ‣ 4 Experiments ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization
    for LLM"). The results indicate that fine-grained and coarse-grained quantization
    methods can lead to up to a 1 PPL (Perplexity) difference. Specifically, in fine-grained
    quantization, we observe that static activation quantization in combination with
    group-wise weight quantization outperforms dynamic activation quantization coupled
    with channel-wise weight quantization. Additionally, our dual-grained quantization
    approach demonstrates that it introduces minimal additional quantization errors
    compared to group-wise quantization.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 针对A8W4 LLaMA模型的不同量化方案。在我们对LLaMA1-7b和LLaMA1-13b模型的实验中，我们探讨了不同的量化方案，包括A8W4量化，详细信息见表格 [5](#S4.T5
    "表格 5 ‣ 4.4 消融研究 ‣ 4 实验 ‣ 双重粒度量化：高效的细粒度量化方法")。结果表明，细粒度和粗粒度量化方法可能导致最多1 PPL（困惑度）的差异。具体来说，在细粒度量化中，我们观察到静态激活量化结合分组权重量化优于动态激活量化配合通道权重量化。此外，我们的双重粒度量化方法显示出与分组量化相比，引入的额外量化误差最小。
- en: 'Table 5: Comparison for different quantization schemes for A8W4 LLaMA models.
    S means static tensor-wise quantization, D means dynamic token-wise quantization,
    CW means channel-wise quantization, GW means Group-wise quantization and DG means
    Dula-Grained quantization.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 5: 针对A8W4 LLaMA模型的不同量化方案的比较。S表示静态张量量化，D表示动态标记量化，CW表示通道量化，GW表示分组量化，DG表示双重粒度量化。'
- en: '| PPL $\downarrow$ | LLaMA1-7B | LLaMA1-13B |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| PPL $\downarrow$ | LLaMA1-7B | LLaMA1-13B |'
- en: '| --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| FP16 | S$+$DG |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | S$+$DG |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| WikiText-2 | 5.68 | 6.57 | 6.03 | 6.37 | 6.04 | 5.09 | 6.17 | 5.39 | 5.82
    | 5.39 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| WikiText-2 | 5.68 | 6.57 | 6.03 | 6.37 | 6.04 | 5.09 | 6.17 | 5.39 | 5.82
    | 5.39 |'
- en: '| C4 | 7.08 | 8.10 | 7.44 | 7.75 | 7.43 | 6.61 | 7.84 | 6.92 | 7.37 | 6.93
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| C4 | 7.08 | 8.10 | 7.44 | 7.75 | 7.43 | 6.61 | 7.84 | 6.92 | 7.37 | 6.93
    |'
- en: 'Effect of Percentile Clipping Smooth. In our analysis, we present the WikiText-2
    perplexity (PPL) results in both Table [2](#S4.T2 "Table 2 ‣ 4.1 Experiments Setups
    ‣ 4 Experiments ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization
    for LLM") and Table [4](#S4.T4 "Table 4 ‣ 4.2 Accuracy Evaluation ‣ 4 Experiments
    ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM"). The
    primary distinction between SmoothQuant (Xiao et al., [2023](#bib.bib43)) and
    our methods lies in the choice of the smoothing scale. We want to emphasize the
    effectiveness of our approach, termed Percentile Clipping Smoothing, as a straightforward
    yet powerful technique for LLaMA quantization. Notably, it’s worth mentioning
    that the presence of outliers in OPT models is less pronounced compared to LLaMA
    models. This difference explains why our methods achieve comparable performance
    to SmoothQuant, primarily on OPT models.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '百分位裁剪平滑的效果。在我们的分析中，我们展示了WikiText-2困惑度（PPL）结果，见表[2](#S4.T2 "Table 2 ‣ 4.1 Experiments
    Setups ‣ 4 Experiments ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization
    for LLM")和表[4](#S4.T4 "Table 4 ‣ 4.2 Accuracy Evaluation ‣ 4 Experiments ‣ Dual
    Grained Quantization: Efficient Fine-Grained Quantization for LLM")。SmoothQuant
    (Xiao等，[2023](#bib.bib43))和我们的方法之间的主要区别在于平滑尺度的选择。我们想强调我们的方法，即百分位裁剪平滑，作为一种直接而强大的LLaMA量化技术。值得一提的是，OPT模型中异常值的存在相比于LLaMA模型不那么明显。这一差异解释了为什么我们的方法在OPT模型上与SmoothQuant的性能相当。'
- en: 5 Conclusion
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we propose Dual Grained Quantization (DGQ), a promising and hardware-efficient
    scheme for mixed bit weight-activation quantization (A8W4) for LLM. DGQ is designed
    to compensate for the hardware-inefficient group-wise quantization by a fine-grained
    integer quantization scale and a coarse-grained full-precision scale. We also
    improve the search algorithm for quantization scales to adapt to our quantization
    scheme. We propose a percentile clipping smooth strategy, achieving a better smooth
    scales without search. Furthermore, we implement efficient kernels, achieving
    3 $\times$ memory usage over A8W8 with comparable run time.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了双粒度量化（DGQ），一种有前景且硬件高效的混合位权重-激活量化（A8W4）方案。DGQ旨在通过细粒度整数量化尺度和粗粒度全精度尺度来补偿硬件效率低的组级量化。我们还改进了量化尺度的搜索算法，以适应我们的量化方案。我们提出了一种百分位裁剪平滑策略，在没有搜索的情况下实现了更好的平滑尺度。此外，我们实现了高效的内核，在运行时间相当的情况下，实现了3倍的内存使用效率。
- en: References
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Aminabadi et al. (2022) Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad
    Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang,
    Jeff Rasley, et al. Deepspeed-inference: enabling efficient inference of transformer
    models at unprecedented scale. In *SC22: International Conference for High Performance
    Computing, Networking, Storage and Analysis*, pp.  1–15\. IEEE, 2022.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aminabadi等（2022）Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan,
    Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff
    Rasley等。Deepspeed-inference：在前所未有的规模下实现变换器模型的高效推理。在*SC22：国际高性能计算、网络、存储与分析会议*，第1–15页，IEEE，2022年。
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pp.  7432–7439,
    2020.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisk等（2020）Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi等。Piqa：在自然语言中推理物理常识。在*AAAI人工智能会议论文集*，第34卷，第7432–7439页，2020年。
- en: Bondarenko et al. (2021) Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
    Understanding and overcoming the challenges of efficient transformer quantization.
    *arXiv preprint arXiv:2109.12948*, 2021.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bondarenko等（2021）Yelysei Bondarenko, Markus Nagel, 和 Tijmen Blankevoort。理解和克服高效变换器量化的挑战。*arXiv预印本
    arXiv:2109.12948*，2021年。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell等。语言模型是少量学习者。*神经信息处理系统进展*，33:1877–1901，2020年。
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
    *arXiv preprint arXiv:2303.12712*, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 等（2023）塞巴斯蒂安·布贝克、瓦伦·钱德拉塞卡兰、罗嫩·埃尔丹、约翰内斯·格赫克、埃里克·霍维茨、埃切·卡马尔、彼得·李、尹·塔特·李、袁智·李、斯科特·伦德伯格等。人工通用智能的火花：对
    GPT-4 的早期实验。*arXiv 预印本 arXiv:2303.12712*，2023年。
- en: Choukroun et al. (2019) Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev.
    Low-bit quantization of neural networks for efficient inference. In *2019 IEEE/CVF
    International Conference on Computer Vision Workshop (ICCVW)*, pp.  3009–3018\.
    IEEE, 2019.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choukroun 等（2019）约尼·乔克伦、伊利·克拉夫奇克、范·杨和帕维尔·基西列夫。用于高效推理的神经网络低位量化。见 *2019 IEEE/CVF
    国际计算机视觉大会研讨会（ICCVW）*，第 3009–3018 页。IEEE，2019年。
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty
    of natural yes/no questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等（2019）克里斯托弗·克拉克、肯顿·李、明伟·张、汤姆·克维亚特科夫斯基、迈克尔·柯林斯和克里斯蒂娜·图塔诺娃。Boolq：探索自然是/否问题的惊人难度。*arXiv
    预印本 arXiv:1905.10044*，2019年。
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *ArXiv*, abs/1803.05457, 2018.
    URL [https://api.semanticscholar.org/CorpusID:3922816](https://api.semanticscholar.org/CorpusID:3922816).
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等（2018）彼得·克拉克、艾萨克·考赫、奥伦·埃提齐奥尼、图沙尔·霍特、阿希什·萨巴瓦尔、卡丽莎·肖尼克和欧文·塔福尔德。认为你已经解决了问答问题？试试
    ARC，即 AI2 推理挑战。*ArXiv*，abs/1803.05457，2018年。网址 [https://api.semanticscholar.org/CorpusID:3922816](https://api.semanticscholar.org/CorpusID:3922816)。
- en: 'Dao (2023) Tri Dao. Flashattention-2: Faster attention with better parallelism
    and work partitioning. *arXiv preprint arXiv:2307.08691*, 2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao（2023）Tri Dao。Flashattention-2：更快的注意力机制，提供更好的并行性和工作分区。*arXiv 预印本 arXiv:2307.08691*，2023年。
- en: 'Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    *Advances in Neural Information Processing Systems*, 35:16344–16359, 2022.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao 等（2022）Tri Dao、丹·傅、斯特凡诺·厄尔蒙、阿特里·鲁德拉和克里斯托弗·雷。Flashattention：快速且内存高效的精确注意力机制，具备
    IO 预知能力。*神经信息处理系统进展*，35：16344–16359，2022年。
- en: Dauphin et al. (2017) Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier.
    Language modeling with gated convolutional networks. In *International conference
    on machine learning*, pp. 933–941\. PMLR, 2017.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dauphin 等（2017）扬·N·多芬、安吉拉·范、迈克尔·奥利和大卫·格兰杰。使用门控卷积网络进行语言建模。见 *国际机器学习大会*，第 933–941
    页。PMLR，2017年。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv preprint
    arXiv:2208.07339*, 2022.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等（2022）蒂姆·德特默斯、迈克·刘易斯、尤尼斯·贝尔卡达和卢克·泽特尔摩耶。LLM.int8()：用于大规模变换器的 8 位矩阵乘法。*arXiv
    预印本 arXiv:2208.07339*，2022年。
- en: 'Dettmers et al. (2023a) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023a.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等（2023a）蒂姆·德特默斯、阿尔蒂多罗·帕尼奥尼、阿里·霍尔茨曼和卢克·泽特尔摩耶。QLoRA：量化 LLM 的高效微调。*arXiv
    预印本 arXiv:2305.14314*，2023年a。
- en: 'Dettmers et al. (2023b) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm
    weight compression. *arXiv preprint arXiv:2306.03078*, 2023b.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等（2023b）蒂姆·德特默斯、鲁斯兰·斯维尔切夫斯基、瓦基·埃吉亚扎里安、丹尼斯·库兹尼德列夫、埃利亚斯·弗兰塔、萨利赫·阿什克布斯、亚历山大·博尔祖诺夫、托尔斯滕·赫夫勒和丹·阿利斯塔赫。Spqr：用于近乎无损的
    LLM 权重压缩的稀疏量化表示。*arXiv 预印本 arXiv:2306.03078*，2023年b。
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*, 2018.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等（2018）雅各布·德夫林、明伟·张、肯顿·李和克里斯蒂娜·图塔诺娃。BERT：深度双向变换器的语言理解预训练。*arXiv 预印本 arXiv:1810.04805*，2018年。
- en: Esser et al. (2019) Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar
    Appuswamy, and Dharmendra S Modha. Learned step size quantization. *arXiv preprint
    arXiv:1902.08153*, 2019.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esser 等（2019）斯蒂文·K·埃瑟、杰弗里·L·麦金斯特里、迪皮卡·巴布拉尼、拉辛卡马尔·阿普苏瓦米和达尔梅德拉·S·莫达。学习步长量化。*arXiv
    预印本 arXiv:1902.08153*，2019年。
- en: Frantar & Alistarh (2023) Elias Frantar and Dan Alistarh. Massive language models
    can be accurately pruned in one-shot. *arXiv preprint arXiv:2301.00774*, 2023.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar & Alistarh（2023）埃利亚斯·弗兰塔和丹·阿利斯塔赫。大规模语言模型可以在一次性操作中准确修剪。*arXiv 预印本 arXiv:2301.00774*，2023年。
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等 (2022) Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh。Gptq：生成预训练变换器的准确训练后量化。*arXiv
    preprint arXiv:2210.17323*，2022年。
- en: 'Han et al. (2015) Song Han, Huizi Mao, and William J Dally. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. *arXiv preprint arXiv:1510.00149*, 2015.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2015) Song Han、Huizi Mao 和 William J Dally。深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。*arXiv
    preprint arXiv:1510.00149*，2015年。
- en: 'He et al. (2022) Yefei He, Zhenyu Lou, Luoming Zhang, Weijia Wu, Bohan Zhuang,
    and Hong Zhou. Bivit: Extremely compressed binary vision transformer. *arXiv preprint
    arXiv:2211.07091*, 2022.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 (2022) Yefei He、Zhenyu Lou、Luoming Zhang、Weijia Wu、Bohan Zhuang 和 Hong
    Zhou。Bivit：极度压缩的二进制视觉变换器。*arXiv preprint arXiv:2211.07091*，2022年。
- en: 'Hubara et al. (2020) Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and
    Daniel Soudry. Improving post training neural quantization: Layer-wise calibration
    and integer programming. *arXiv preprint arXiv:2006.10518*, 2020.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara 等 (2020) Itay Hubara、Yury Nahshan、Yair Hanani、Ron Banner 和 Daniel Soudry。改进训练后神经量化：逐层校准和整数规划。*arXiv
    preprint arXiv:2006.10518*，2020年。
- en: 'Li et al. (2023) Qingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang
    Chu, Yerui Sun, Li Du, and Yuchen Xie. Fptq: Fine-grained post-training quantization
    for large language models. *arXiv preprint arXiv:2308.15987*, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023) Qingyuan Li、Yifan Zhang、Liang Li、Peng Yao、Bo Zhang、Xiangxiang Chu、Yerui
    Sun、Li Du 和 Yuchen Xie。Fptq：大型语言模型的细粒度训练后量化。*arXiv preprint arXiv:2308.15987*，2023年。
- en: 'Li et al. (2021) Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization
    by block reconstruction. *arXiv preprint arXiv:2102.05426*, 2021.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2021) Yuhang Li、Ruihao Gong、Xu Tan、Yang Yang、Peng Hu、Qi Zhang、Fengwei
    Yu、Wei Wang 和 Shi Gu。Brecq：通过块重建推动训练后量化的极限。*arXiv preprint arXiv:2102.05426*，2021年。
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 (2023) Ji Lin、Jiaming Tang、Haotian Tang、Shang Yang、Xingyu Dang 和 Song
    Han。Awq：用于 llm 压缩和加速的激活感知权重量化。*arXiv preprint arXiv:2306.00978*，2023年。
- en: 'Lin & Chen (2023) Yen-Ting Lin and Yun-Nung Chen. Llm-eval: Unified multi-dimensional
    automatic evaluation for open-domain conversations with large language models.
    *arXiv preprint arXiv:2305.13711*, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin & Chen (2023) Yen-Ting Lin 和 Yun-Nung Chen。Llm-eval：用于开放域对话的统一多维自动评估，大型语言模型。*arXiv
    preprint arXiv:2305.13711*，2023年。
- en: 'Liu et al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models. *arXiv
    preprint arXiv:2305.17888*, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2023) Zechun Liu、Barlas Oguz、Changsheng Zhao、Ernie Chang、Pierre Stock、Yashar
    Mehdad、Yangyang Shi、Raghuraman Krishnamoorthi 和 Vikas Chandra。Llm-qat：针对大型语言模型的数据无关量化感知训练。*arXiv
    preprint arXiv:2305.17888*，2023年。
- en: 'Marcus et al. (1994) Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert
    MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn
    treebank: Annotating predicate argument structure. In *Human Language Technology:
    Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994*, 1994.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Marcus 等 (1994) Mitch Marcus、Grace Kim、Mary Ann Marcinkiewicz、Robert MacIntyre、Ann
    Bies、Mark Ferguson、Karen Katz 和 Britta Schasberger。Penn Treebank：注释谓词论元结构。见 *Human
    Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey,
    March 8-11, 1994*，1994年。'
- en: 'Martinez et al. (2018) Julieta Martinez, Shobhit Zakhmi, Holger H Hoos, and
    James J Little. Lsq++: Lower running time and higher recall in multi-codebook
    quantization. In *Proceedings of the European Conference on Computer Vision (ECCV)*,
    pp.  491–506, 2018.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martinez 等 (2018) Julieta Martinez、Shobhit Zakhmi、Holger H Hoos 和 James J Little。Lsq++：在多代码本量化中降低运行时间并提高召回率。见
    *Proceedings of the European Conference on Computer Vision (ECCV)*，第491–506页，2018年。
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等 (2016) Stephen Merity、Caiming Xiong、James Bradbury 和 Richard Socher。指针哨兵混合模型。*arXiv
    preprint arXiv:1609.07843*，2016年。
- en: Nagel et al. (2020) Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning*, pp. 7197–7206\.
    PMLR, 2020.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel 等 (2020) Markus Nagel、Rana Ali Amjad、Mart Van Baalen、Christos Louizos
    和 Tijmen Blankevoort。向上还是向下？用于训练后量化的自适应舍入。见 *International Conference on Machine
    Learning*，第7197–7206页。PMLR，2020年。
- en: 'Park et al. (2022) Minseop Park, Jaeseong You, Markus Nagel, and Simyung Chang.
    Quadapter: Adapter for gpt-2 quantization. *arXiv preprint arXiv:2211.16912*,
    2022.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等（2022） Minseop Park, Jaeseong You, Markus Nagel 和 Simyung Chang。《Quadapter：GPT-2
    量化的适配器》。*arXiv 预印本 arXiv:2211.16912*，2022。
- en: 'Qin et al. (2022) Haotong Qin, Yifu Ding, Mingyuan Zhang, Qinghua Yan, Aishan
    Liu, Qingqing Dang, Ziwei Liu, and Xianglong Liu. Bibert: Accurate fully binarized
    bert. *arXiv preprint arXiv:2203.06390*, 2022.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等（2022） Haotong Qin, Yifu Ding, Mingyuan Zhang, Qinghua Yan, Aishan Liu,
    Qingqing Dang, Ziwei Liu 和 Xianglong Liu。《Bibert：准确的完全二值化 BERT》。*arXiv 预印本 arXiv:2203.06390*，2022。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等（2020） Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li 和 Peter J Liu。《通过统一的文本到文本变换器探索迁移学习的极限》。*机器学习研究杂志*，21(1):5485–5551，2020。
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi 等（2021） Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula 和 Yejin
    Choi。《Winogrande：大规模对抗 Winograd 语法挑战》。*ACM 通讯*，64(9):99–106，2021。
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*, 2022.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scao 等（2022） Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana
    Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon,
    Matthias Gallé 等。《Bloom：一个1760亿参数的开放访问多语言模型》。*arXiv 预印本 arXiv:2211.05100*，2022。
- en: 'Shao et al. (2023) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. *arXiv preprint arXiv:2308.13137*,
    2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等（2023） Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao,
    Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao 和 Ping Luo。《Omniquant：面向大型语言模型的全方位校准量化》。*arXiv
    预印本 arXiv:2308.13137*，2023。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023a） Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar 等。《Llama：开放而高效的基础语言模型》。*arXiv 预印本 arXiv:2302.13971*，2023a。
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023b） Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等。《Llama 2：开放基础和微调聊天模型》。*arXiv 预印本 arXiv:2307.09288*，2023b。
- en: van Baalen et al. (2023) Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei
    Ren, Eric Mahurin, Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel,
    Joseph Soriaga, et al. Fp8 versus int8 for efficient deep learning inference.
    *arXiv preprint arXiv:2303.17951*, 2023.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Baalen 等（2023） Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren,
    Eric Mahurin, Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph
    Soriaga 等。《Fp8 与 Int8 在高效深度学习推理中的对比》。*arXiv 预印本 arXiv:2303.17951*，2023。
- en: 'Wei et al. (2022) Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei
    Yu. Qdrop: Randomly dropping quantization for extremely low-bit post-training
    quantization. *arXiv preprint arXiv:2203.05740*, 2022.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2022） Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu 和 Fengwei Yu。《Qdrop：用于极低比特后训练量化的随机丢弃量化》。*arXiv
    预印本 arXiv:2203.05740*，2022。
- en: 'Wei et al. (2023) Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao
    Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization
    of large language models by equivalent and optimal shifting and scaling. *arXiv
    preprint arXiv:2304.09145*, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2023） Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong,
    Jinyang Guo 和 Xianglong Liu。《Outlier suppression+：通过等效和最优的平移和缩放来准确量化大型语言模型》。*arXiv
    预印本 arXiv:2304.09145*，2023。
- en: 'Wu et al. (2023) Xiaoxia Wu, Zhewei Yao, and Yuxiong He. Zeroquant-fp: A leap
    forward in llms post-training w4a8 quantization using floating-point formats.
    *arXiv preprint arXiv:2307.09782*, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2023） Xiaoxia Wu, Zhewei Yao 和 Yuxiong He。《Zeroquant-fp：使用浮点格式在 LLMs 后训练
    W4A8 量化中的飞跃》。*arXiv 预印本 arXiv:2307.09782*，2023。
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *International Conference on Machine Learning*, pp. 38087–38099\.
    PMLR, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等（2023）Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth
    和 Song Han。Smoothquant: 针对大型语言模型的准确且高效的后训练量化。在*国际机器学习会议*，第 38087–38099 页。PMLR，2023年。'
- en: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. *Advances in Neural Information Processing
    Systems*, 35:27168–27183, 2022.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等（2022）Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li 和 Yuxiong He。Zeroquant: 高效且经济的后训练量化用于大规模变换器。*神经信息处理系统进展*，35:27168–27183，2022年。'
- en: Yao et al. (2023) Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong
    He. A comprehensive study on post-training quantization for large language models.
    *arXiv preprint arXiv:2303.08302*, 2023.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等（2023）Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn 和 Yuxiong He。针对大型语言模型的后训练量化的全面研究。*arXiv
    预印本 arXiv:2303.08302*，2023年。
- en: 'Yuan et al. (2023) Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang,
    Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based
    post-training quantization for large language models. *arXiv preprint arXiv:2304.01089*,
    2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan 等（2023）Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang
    Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu 和 Bingzhe Wu。Rptq: 基于重排的后训练量化方法，用于大型语言模型。*arXiv
    预印本 arXiv:2304.01089*，2023年。'
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers 等（2019）Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi 和 Yejin
    Choi。Hellaswag: 机器真的能完成你的句子吗？*arXiv 预印本 arXiv:1905.07830*，2019年。'
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2022）Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin 等。Opt:
    开放的预训练变换器语言模型。*arXiv 预印本 arXiv:2205.01068*，2022年。'
- en: Appendix A1 Limitation and Discussion
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A1 限制与讨论
- en: In this paper, we propose a simple yet highly efficient quantization method
    for fine-grained weight-activation quantization. This method makes it practical
    to implement large language models using A8W4 quantization. Our primary focus
    is on developing efficient solutions for fine-grained weight quantization. For
    activation, layer-wise activation quantization strategy (Li et al., [2023](#bib.bib22))
    would be a good solution, as the limitation of smooth scales.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种简单但高效的量化方法，用于细粒度权重激活量化。该方法使得使用 A8W4 量化实现大型语言模型变得切实可行。我们主要关注的是开发细粒度权重量化的高效解决方案。对于激活，逐层激活量化策略（Li
    等，[2023](#bib.bib22)）将是一个好的解决方案，因为其平滑尺度的限制。
- en: In our work, we specifically develop efficient kernels tailored for long-sequence
    inference. A16W4 kernels will introduce redundant dequantization operations. Self-decoding
    task is memory bound, A16W4 kernels can ease memory bound. Compared to A16W4,
    our method have same bit width for weight and the calculation progression is done
    with INT8 kernels. This theoretically positions our approach as more efficient
    than A16W4. One challenge we are currently addressing is the need for two separate
    operations in different situations, and we are actively working on a solution.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，我们特别开发了针对长序列推断的高效内核。A16W4 内核会引入冗余的反量化操作。自解码任务受内存限制，A16W4 内核可以缓解内存限制。与
    A16W4 相比，我们的方法在权重方面具有相同的位宽，并且计算过程使用 INT8 内核。这在理论上使我们的方法比 A16W4 更高效。我们目前面临的一个挑战是需要在不同情况下执行两个独立的操作，我们正在积极寻求解决方案。
- en: Furthermore, self-decoding tasks face a challenge with memory consumption, especially
    when dealing with large self-attention matrices. For example, with a sequence
    length of 32K, a single self-attention matrix can occupy about 20GB of memory
    in FP16\. To mitigate this issue, we are exploring quantization methods for activation,
    which we plan to incorporate into our future work.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，自解码任务在内存消耗方面面临挑战，特别是在处理大型自注意力矩阵时。例如，对于 32K 的序列长度，一个自注意力矩阵在 FP16 下可以占用约 20GB
    的内存。为了缓解这一问题，我们正在探索激活的量化方法，并计划将其纳入我们的未来工作中。
- en: Appendix A2 Proof for constraints relaxation and fusion for Grid Search.
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A2 约束放松和网格搜索融合的证明。
- en: 'The constraints are as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 约束条件如下：
- en: '|  | $\displaystyle\mathbf{W}_{u4}\in\mathbf{[0,15]},\mathbf{S^{(2)}}\in\mathbf{[0,15]},\mathbf{W}_{s8}\in\mathbf{[-127,127]}$
    |  | (A8) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{W}_{u4}\in\mathbf{[0,15]},\mathbf{S^{(2)}}\in\mathbf{[0,15]},\mathbf{W}_{s8}\in\mathbf{[-127,127]}$
    |  | (A8) |'
- en: 'The dequantization calculation is as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 反量化计算如下：
- en: '|  | $\displaystyle\mathbf{W}_{f16}=(\mathbf{W}_{u4}-\mathbf{ZP}_{u4})\mathbf{S}_{f16},\mathbf{S}_{f16}=\left\lfloor\mathbf{S^{\prime}/S^{(1)}}\right\rceil
    S^{(1)}$ |  | (A9) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{W}_{f16}=(\mathbf{W}_{u4}-\mathbf{ZP}_{u4})\mathbf{S}_{f16},\mathbf{S}_{f16}=\left\lfloor\mathbf{S^{\prime}/S^{(1)}}\right\rceil
    S^{(1)}$ |  | (A9) |'
- en: Due to the rounding operation $\lfloor\cdot\rceil$.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于取整操作 $\lfloor\cdot\rceil$。
- en: '|  | $\displaystyle\mathbf{W}_{u4}=\left\lfloor{\frac{\mathbf{W}_{s8}}{\mathbf{S^{(2)}}}}\right\rceil+\mathbf{ZP}\in[\left\lfloor\mathbf{\frac{-127}{S^{(2)}}}\right\rceil+\mathbf{ZP},\left\lfloor\mathbf{\frac{127}{S^{(2)}}}\right\rceil+\mathbf{ZP}]$
    |  | (A10) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{W}_{u4}=\left\lfloor{\frac{\mathbf{W}_{s8}}{\mathbf{S^{(2)}}}}\right\rceil+\mathbf{ZP}\in[\left\lfloor\mathbf{\frac{-127}{S^{(2)}}}\right\rceil+\mathbf{ZP},\left\lfloor\mathbf{\frac{127}{S^{(2)}}}\right\rceil+\mathbf{ZP}]$
    |  | (A10) |'
- en: 'We can merge the two intervals as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以合并两个区间如下：
- en: '|  | $\displaystyle\mathbf{W}_{u4}\in\mathbf{[max(0,\lfloor\frac{-127}{S^{(2)}}\rceil+ZP),min(15,\lfloor\frac{127}{S^{(2)}}\rceil+ZP)]}$
    |  | (A11) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{W}_{u4}\in\mathbf{[max(0,\lfloor\frac{-127}{S^{(2)}}\rceil+ZP),min(15,\lfloor\frac{127}{S^{(2)}}\rceil+ZP)]}$
    |  | (A11) |'
- en: Appendix A3 More accuracy results
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A3 更多准确度结果
- en: 'In this section, we provide a comprehensive presentation of our results across
    various datasets to complement the main paper. We also provide here a comparison
    with contemporaneous work Omniquant (Shao et al., [2023](#bib.bib36)) and FPTQ (Li
    et al., [2023](#bib.bib22)). Specifically, the results include:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们对各种数据集的结果进行了全面展示，以补充主要论文。我们还提供了与当代工作 Omniquant（Shao 等， [2023](#bib.bib36)）和
    FPTQ（Li 等， [2023](#bib.bib22)）的比较。具体结果包括：
- en: •
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'C4 perplexity in the LLaMA families (Table [A1](#A3.T1 "Table A1 ‣ Appendix
    A3 More accuracy results ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization
    for LLM"))'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLaMA 系列中的 C4 困惑度 (表 [A1](#A3.T1 "表 A1 ‣ 附录 A3 更多准确度结果 ‣ 双粒度量化：高效的细粒度量化方法"))
- en: •
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'C4 perplexity in OPT families (Table [A2](#A3.T2 "Table A2 ‣ Appendix A3 More
    accuracy results ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization
    for LLM")).'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C4 在 OPT 系列中的困惑度 (表 [A2](#A3.T2 "表 A2 ‣ 附录 A3 更多准确度结果 ‣ 双粒度量化：高效的细粒度量化方法"))。
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'PTB perplexity in OPT families (Table [A3](#A3.T3 "Table A3 ‣ Appendix A3 More
    accuracy results ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization
    for LLM")).'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PTB 在 OPT 系列中的困惑度 (表 [A3](#A3.T3 "表 A3 ‣ 附录 A3 更多准确度结果 ‣ 双粒度量化：高效的细粒度量化方法"))。
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MMLU in LLaMA families (Table [A4](#A3.T4 "Table A4 ‣ Appendix A3 More accuracy
    results ‣ Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM")).'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MMLU 在 LLaMA 系列中的表现 (表 [A4](#A3.T4 "表 A4 ‣ 附录 A3 更多准确度结果 ‣ 双粒度量化：高效的细粒度量化方法"))。
- en: 'Table A1: Quantization Results on c4 with A16W3 and A8W4 LLaMA Models. $\dagger$
    indicates static quantization for activation.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A1：在 c4 上使用 A16W3 和 A8W4 LLaMA 模型的量化结果。$\dagger$ 表示激活的静态量化。
- en: '| LLaMA PPL $\downarrow$ | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B | 2-70B
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA PPL $\downarrow$ | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B | 2-70B
    |'
- en: '| FP16 | - | 7.08 | 6.61 | 5.98 | 5.62 | 6.97 | 6.46 | 5.52 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 7.08 | 6.61 | 5.98 | 5.62 | 6.97 | 6.46 | 5.52 |'
- en: '| W3A16 g128 | RTN | 8.62 | 7.49 | 6.58 | 6.10 | 8.40 | 7.18 | 6.02 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| W3A16 g128 | RTN | 8.62 | 7.49 | 6.58 | 6.10 | 8.40 | 7.18 | 6.02 |'
- en: '| GPTQ | 7.85 | 7.10 | 6.47 | 6.00 | 7.89 | 7.00 | 5.85 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 7.85 | 7.10 | 6.47 | 6.00 | 7.89 | 7.00 | 5.85 |'
- en: '| AWQ | 7.92 | 7.07 | 6.37 | 5.94 | 7.84 | 6.94 | - |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 7.92 | 7.07 | 6.37 | 5.94 | 7.84 | 6.94 | - |'
- en: '| OmniQuant | 7.34 | 6.76 | 6.11 | 5.73 | 7.35 | 6.65 | 5.86 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 7.34 | 6.76 | 6.11 | 5.73 | 7.35 | 6.65 | 5.86 |'
- en: '| W4A8 g128 | RTN | 10.76 | 9.94 | 8.14 | 7.96 | 17.29 | 90.57 | 11.86 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| W4A8 g128 | RTN | 10.76 | 9.94 | 8.14 | 7.96 | 17.29 | 90.57 | 11.86 |'
- en: '| ZeroQuantv2 | 7.79 | 6.78 | 6.16 | - | - | - | - |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| ZeroQuantv2 | 7.79 | 6.78 | 6.16 | - | - | - | - |'
- en: '| SmoothQuant | 7.51 | 6.89 | 6.39 | 5.94 | 7.50 | 6.82 | 5.78 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 7.51 | 6.89 | 6.39 | 5.94 | 7.50 | 6.82 | 5.78 |'
- en: '| ZeroQuant-FP | 7.51 | 5.73 | 6.09 | - | - | - | - |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| ZeroQuant-FP | 7.51 | 5.73 | 6.09 | - | - | - | - |'
- en: '| Ours | 7.29 | 6.73 | 6.10 | 5.73 | 7.16 | 6.62 | 5.62 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 7.29 | 6.73 | 6.10 | 5.73 | 7.16 | 6.62 | 5.62 |'
- en: '| Ours^† | 7.43 | 6.93 | 6.31 | 5.97 | 7.44 | 6.82 | 5.89 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 我们的^† | 7.43 | 6.93 | 6.31 | 5.97 | 7.44 | 6.82 | 5.89 |'
- en: 'Table A2: Quantization Results on c4 with A16W3 and A8W4 OPT Models $\dagger$
    indicates static quantization for activation.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A2：在 c4 上使用 A16W3 和 A8W4 OPT 模型的量化结果 $\dagger$ 表示激活的静态量化。
- en: '| OPT PPL $\downarrow$ | 125M | 1.3B | 2.7B | 6.7B | 13B | 30B | 66B |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| OPT PPL $\downarrow$ | 125M | 1.3B | 2.7B | 6.7B | 13B | 30B | 66B |'
- en: '| FP16 | - | 24.61 | 14.73 | 13.17 | 11.75 | 11.21 | 10.69 | 10.28 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 24.61 | 14.73 | 13.17 | 11.75 | 11.21 | 10.69 | 10.28 |'
- en: '| W3A16 g128 | RTN | 40.13 | 126.47 | 372.23 | 32.56 | 44.12 | 25.70 | 286.87
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| W3A16 g128 | RTN | 40.13 | 126.47 | 372.23 | 32.56 | 44.12 | 25.70 | 286.87
    |'
- en: '| GPTQ | 30.08 | 16.47 | 14.54 | 12.48 | 11.58 | 10.91 | 11.35 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 30.08 | 16.47 | 14.54 | 12.48 | 11.58 | 10.91 | 11.35 |'
- en: '| AWQ | 30.39 | 16.27 | 14.19 | 12.30 | 11.61 | 10.96 | 10.53 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 30.39 | 16.27 | 14.19 | 12.30 | 11.61 | 10.96 | 10.53 |'
- en: '| OmniQuant | 29.34 | 16.11 | 14.15 | 12.31 | 11.63 | 10.98 | 10.51 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 29.34 | 16.11 | 14.15 | 12.31 | 11.63 | 10.98 | 10.51 |'
- en: '| W4A8 g128 | RTN | 27.93 | 17.52 | 16.33 | 98.34 | 3926.05 | 3557.30 | 2493.73
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| W4A8 g128 | RTN | 27.93 | 17.52 | 16.33 | 98.34 | 3926.05 | 3557.30 | 2493.73
    |'
- en: '| ZeroQuantv2 | 27.19 | 15.73 | 13.82 | 12.19 | 11.64 | 11.00 | 10.63 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| ZeroQuantv2 | 27.19 | 15.73 | 13.82 | 12.19 | 11.64 | 11.00 | 10.63 |'
- en: '| SmoothQuant | 25.99 | 15.16 | 13.46 | 11.88 | 11.39 | 10.75 | 10.32 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 25.99 | 15.16 | 13.46 | 11.88 | 11.39 | 10.75 | 10.32 |'
- en: '| RPTQ | - | 15.48 | - | 12.11 | 11.62 | 11.01 | 10.57 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| RPTQ | - | 15.48 | - | 12.11 | 11.62 | 11.01 | 10.57 |'
- en: '| ZeroQuant-FP | - | 15.32 | - | 11.95 | 11.30 | 10.75 | - |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| ZeroQuant-FP | - | 15.32 | - | 11.95 | 11.30 | 10.75 | - |'
- en: '| Ours | 26.03 | 15.10 | 13.42 | 11.87 | 11.40 | 10.74 | 10.33 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 26.03 | 15.10 | 13.42 | 11.87 | 11.40 | 10.74 | 10.33 |'
- en: '| Ours^† | 26.64 | 15.24 | 13.45 | 11.89 | 11.42 | 10.76 | 10.33 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| Ours^† | 26.64 | 15.24 | 13.45 | 11.89 | 11.42 | 10.76 | 10.33 |'
- en: 'Table A3: Quantization Results on ptb with A16W3 and A8W4 OPT Models $\dagger$
    indicates static quantization for activation.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table A3: 使用 A16W3 和 A8W4 OPT 模型的量化结果。$\dagger$ 表示激活的静态量化。'
- en: '| OPT PPL $\downarrow$ | 125M | 1.3B | 2.7B | 6.7B | 13B | 30B | 66B |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| OPT PPL $\downarrow$ | 125M | 1.3B | 2.7B | 6.7B | 13B | 30B | 66B |'
- en: '| FP16 | - | 32.55 | 16.97 | 15.11 | 13.09 | 12.34 | 11.84 | 11.36 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 32.55 | 16.97 | 15.11 | 13.09 | 12.34 | 11.84 | 11.36 |'
- en: '| W3A16 g128 | RTN | 64.67 | 222.13 | 337.75 | 39.90 | 65.33 | 34.27 | 309.69
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| W3A16 g128 | RTN | 64.67 | 222.13 | 337.75 | 39.90 | 65.33 | 34.27 | 309.69
    |'
- en: '| GPTQ | 45.17 | 19.90 | 17.06 | 14.24 | 12.84 | 12.54 | 13.27 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 45.17 | 19.90 | 17.06 | 14.24 | 12.84 | 12.54 | 13.27 |'
- en: '| AWQ | 44.07 | 19.59 | 16.52 | 13.98 | 12.87 | 66.68 | 3.4e3 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 44.07 | 19.59 | 16.52 | 13.98 | 12.87 | 66.68 | 3.4e3 |'
- en: '| OmniQuant | 45.29 | 20.42 | 17.08 | 14.23 | 13.49 | 12.54 | 12.06 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 45.29 | 20.42 | 17.08 | 14.23 | 13.49 | 12.54 | 12.06 |'
- en: '| W4A8 g128 | RTN | 38.31 | 20.84 | 19.75 | 65.86 | 3370.84 | 2972.69 | 2556.84
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| W4A8 g128 | RTN | 38.31 | 20.84 | 19.75 | 65.86 | 3370.84 | 2972.69 | 2556.84
    |'
- en: '| ZeroQuantv2 | 36.66 | 18.35 | 16.11 | 13.70 | 12.91 | 12.28 | 11.84 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| ZeroQuantv2 | 36.66 | 18.35 | 16.11 | 13.70 | 12.91 | 12.28 | 11.84 |'
- en: '| SmoothQuant | 34.32 | 17.37 | 15.27 | 13.27 | 12.55 | 11.93 | 11.42 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 34.32 | 17.37 | 15.27 | 13.27 | 12.55 | 11.93 | 11.42 |'
- en: '| RPTQ | - | 17.79 | - | 13.74 | 13.40 | 12.41 | 11.73 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| RPTQ | - | 17.79 | - | 13.74 | 13.40 | 12.41 | 11.73 |'
- en: '| ZeroQuant-FP | - | 18.19 | - | 13.44 | 12.55 | 11.90 | - |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| ZeroQuant-FP | - | 18.19 | - | 13.44 | 12.55 | 11.90 | - |'
- en: '| Ours | 34.29 | 17.48 | 15.31 | 13.26 | 12.61 | 11.93 | 11.42 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 34.29 | 17.48 | 15.31 | 13.26 | 12.61 | 11.93 | 11.42 |'
- en: '| Ours^† | 35.29 | 17.61 | 15.34 | 13.29 | 12.63 | 11.93 | 11.42 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Ours^† | 35.29 | 17.61 | 15.34 | 13.29 | 12.63 | 11.93 | 11.42 |'
- en: 'Table A4: MMLU Reuslts with A8W4 LLaMA Models. $\dagger$ indicates static quantization
    for activation.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table A4: MMLU 结果与 A8W4 LLaMA 模型。$\dagger$ 表示激活的静态量化。'
- en: '| LLaMA / Acc$\uparrow$ |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA / Acc$\uparrow$ |'
- en: '| LLaMA-1-7B | FP16 | A16W16 | 33.60 | 31.10 | 38.20 | 38.40 | 35.20 | 0.00
    |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-1-7B | FP16 | A16W16 | 33.60 | 31.10 | 38.20 | 38.40 | 35.20 | 0.00
    |'
- en: '| SmoothQunat | A8W8 | 33.80 | 30.32 | 37.63 | 39.08 | 35.14 | 0.06 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQunat | A8W8 | 33.80 | 30.32 | 37.63 | 39.08 | 35.14 | 0.06 |'
- en: '| GPTQ | A16W4 | 32.39 | 30.35 | 35.03 | 36.15 | 33.40 | 1.80 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | A16W4 | 32.39 | 30.35 | 35.03 | 36.15 | 33.40 | 1.80 |'
- en: '| FPTQ | A8W4 | 30.20 | 29.95 | 32.76 | 35.87 | 32.02 | 3.18 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| FPTQ | A8W4 | 30.20 | 29.95 | 32.76 | 35.87 | 32.02 | 3.18 |'
- en: '| FP16 | A16W16 | 31.27 | 30.53 | 36.79 | 35.90 | 33.50 | 0.00 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | A16W16 | 31.27 | 30.53 | 36.79 | 35.90 | 33.50 | 0.00 |'
- en: '| Ours | A8W4 | 31.08 | 30.53 | 37.09 | 34.56 | 33.78 | -0.28 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Ours | A8W4 | 31.08 | 30.53 | 37.09 | 34.56 | 33.78 | -0.28 |'
- en: '| Ours^† | A8W4 | 28.96 | 30.22 | 35.91 | 34.24 | 32.20 | 1.30 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| Ours^† | A8W4 | 28.96 | 30.22 | 35.91 | 34.24 | 32.20 | 1.30 |'
- en: '| LLaMA-1-13B | FP16 | A16W16 | 44.60 | 37.10 | 54.00 | 53.50 | 47.10 | 0.00
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-1-13B | FP16 | A16W16 | 44.60 | 37.10 | 54.00 | 53.50 | 47.10 | 0.00
    |'
- en: '| SmoothQunat | A8W8 | 44.14 | 36.51 | 54.05 | 52.65 | 46.64 | 0.54 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQunat | A8W8 | 44.14 | 36.51 | 54.05 | 52.65 | 46.64 | 0.54 |'
- en: '| GPTQ | W4A16 | 46.01 | 39.00 | 54.01 | 53.36 | 47.96 | -0.86 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | W4A16 | 46.01 | 39.00 | 54.01 | 53.36 | 47.96 | -0.86 |'
- en: '| FPTQ | W4A8 | 40.96 | 34.19 | 49.72 | 49.75 | 43.46 | 3.64 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| FPTQ | W4A8 | 40.96 | 34.19 | 49.72 | 49.75 | 43.46 | 3.64 |'
- en: '| FP16 | A16W16 | 40.73 | 38.31 | 54.60 | 54.04 | 47.06 | 0.00 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | A16W16 | 40.73 | 38.31 | 54.60 | 54.04 | 47.06 | 0.00 |'
- en: '| Ours | A8W4 | 40.93 | 37.69 | 50.44 | 51.48 | 45.83 | 1.23 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Ours | A8W4 | 40.93 | 37.69 | 50.44 | 51.48 | 45.83 | 1.23 |'
- en: '|  | Ours^† | A8W4 | 39.56 | 41.50 | 48.66 | 51.27 | 45.74 | 1.32 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours^† | A8W4 | 39.56 | 41.50 | 48.66 | 51.27 | 45.74 | 1.32 |'
- en: '| LLaMA-1-65B | FP16 | A16W16 | 61.80 | 52.00 | 73.30 | 67.60 | 63.50 | 0.00
    |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-1-65B | FP16 | A16W16 | 61.80 | 52.00 | 73.30 | 67.60 | 63.50 | 0.00
    |'
- en: '| SmoothQunat | A8W8 | 61.32 | 50.50 | 71.69 | 66.90 | 62.56 | 0.94 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQunat | A8W8 | 61.32 | 50.50 | 71.69 | 66.90 | 62.56 | 0.94 |'
- en: '| GPTQ | A16W4 | 60.23 | 52.09 | 72.15 | 66.75 | 62.60 | 0.90 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | A16W4 | 60.23 | 52.09 | 72.15 | 66.75 | 62.60 | 0.90 |'
- en: '| FPTQ | A8W4 | 59.85 | 49.24 | 71.50 | 65.89 | 61.52 | 1.98 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| FPTQ | A8W4 | 59.85 | 49.24 | 71.50 | 65.89 | 61.52 | 1.98 |'
- en: '| FP16 | A16W16 | 57.72 | 47.04 | 75.96 | 67.44 | 62.18 | 0.00 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | A16W16 | 57.72 | 47.04 | 75.96 | 67.44 | 62.18 | 0.00 |'
- en: '| Ours | A8W4 | 56.76 | 43.62 | 75.07 | 65.62 | 61.21 | 0.97 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 我们 | A8W4 | 56.76 | 43.62 | 75.07 | 65.62 | 61.21 | 0.97 |'
- en: '|  | Ours^† | A8W4 | 55.60 | 44.86 | 72.11 | 63.53 | 59.57 | 2.61 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们^† | A8W4 | 55.60 | 44.86 | 72.11 | 63.53 | 59.57 | 2.61 |'
