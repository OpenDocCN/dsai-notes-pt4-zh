- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:49:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:17
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'SpinQuant: LLM Quantization with Learned Rotations'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'SpinQuant: LLM量化与学习旋转'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16406](https://ar5iv.labs.arxiv.org/html/2405.16406)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16406](https://ar5iv.labs.arxiv.org/html/2405.16406)
- en: \DeclareDocumentCommand\pdd
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \DeclareDocumentCommand\pdd
- en: O O m ∂#2​#1∂#3#2
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: O O m ∂#2​#1∂#3#2
- en: Zechun Liu     Changsheng Zhao${}^{*}$    Igor Fedorov    Bilge Soran    Dhruv
    Choudhary
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Zechun Liu     Changsheng Zhao${}^{*}$    Igor Fedorov    Bilge Soran    Dhruv
    Choudhary
- en: Raghuraman Krishnamoorthi    Vikas Chandra    Yuandong Tian    Tijmen Blankevoort
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Raghuraman Krishnamoorthi    Vikas Chandra    Yuandong Tian    Tijmen Blankevoort
- en: 'Meta   Equal contribution. Correspondence to: Zechun Liu .'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Meta    等同贡献。联系：Zechun Liu 。
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Post-training quantization (PTQ) techniques applied to weights, activations,
    and the KV cache greatly reduce memory usage, latency, and power consumption of
    Large Language Models (LLMs), but may lead to large quantization errors when outliers
    are present. Recent findings suggest that rotating activation or weight matrices
    helps remove outliers and benefits quantization. In this work, we identify a collection
    of applicable rotation parameterizations that lead to identical outputs in full-precision
    Transformer architectures, and find that some random rotations lead to much better
    quantization than others, with an up to *13 points* difference in downstream zero-shot
    reasoning performance. As a result, we propose SpinQuant that *optimizes* (or
    *learns*) the rotation matrices with Cayley optimization on a small validation
    set. With 4-bit quantization of weight, activation, and KV-cache, SpinQuant narrows
    the accuracy gap on zero-shot reasoning tasks with full precision to merely 2.9
    points on the LLaMA-2 7B model, surpassing LLM-QAT by 19.1 points and SmoothQuant
    by 25.0 points. SpinQuant also outperforms concurrent work QuaRot, which applies
    random rotations to remove outliers. In particular, for LLaMA-2 7B/LLaMA-3 8B
    models that are hard to quantize, SpinQuant reduces the gap to full precision
    by 30.2%/34.1% relative to QuaRot.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于权重、激活和KV缓存的后训练量化（PTQ）技术大大减少了大型语言模型（LLMs）的内存使用、延迟和功耗，但在存在异常值时可能导致较大的量化误差。最近的研究表明，旋转激活或权重矩阵有助于去除异常值，并对量化有益。在这项工作中，我们确定了一组适用的旋转参数化，这些参数化在全精度Transformer架构中产生相同的输出，并发现一些随机旋转比其他旋转提供了更好的量化效果，下游零样本推理性能相差高达*13分*。因此，我们提出了SpinQuant，它通过在小型验证集上使用Cayley优化来*优化*（或*学习*）旋转矩阵。通过对权重、激活和KV缓存进行4位量化，SpinQuant将LLaMA-2
    7B模型在零样本推理任务上的精度差距缩小到仅2.9分，超越了LLM-QAT 19.1分和SmoothQuant 25.0分。SpinQuant还优于当前的工作QuaRot，后者通过应用随机旋转来去除异常值。特别是，对于难以量化的LLaMA-2
    7B/LLaMA-3 8B模型，SpinQuant相对于QuaRot将全精度差距减少了30.2%/34.1%。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language models (LLMs) have demonstrated impressive performance across
    many disciplines. SoTA open source models (e.g., LLaMA [[40](#bib.bib40)], Mistral
    [[17](#bib.bib17)], etc) and proprietary LLMs (e.g., GPT [[2](#bib.bib2)], Gemini[[37](#bib.bib37)],
    etc) have been used in general purpose chatting assistants, medical diagnosticians
    [[38](#bib.bib38)], computer game content generators [[10](#bib.bib10)], coding
    co-pilots [[32](#bib.bib32)], and much more.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在许多领域展示了令人印象深刻的性能。SoTA开源模型（如LLaMA [[40](#bib.bib40]、Mistral [[17](#bib.bib17]等）和专有LLMs（如GPT
    [[2](#bib.bib2]、Gemini[[37](#bib.bib37]等）已被用于通用聊天助手、医学诊断师[[38](#bib.bib38]、计算机游戏内容生成器[[10](#bib.bib10]、编码助手[[32](#bib.bib32]等。
- en: To serve such a high demand, the inference cost becomes a real issue. Many effective
    techniques have been developed. Post-training Quantization (PTQ), as one effective
    category of techniques, quantizes the weights (or activations) into low-precision
    and thus reduces the memory usage and may significantly improve latency. This
    is not only important for server-side inference, but also for on-device scenarios
    with small-sized LLMs [[26](#bib.bib26)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为满足如此高的需求，推理成本成为一个实际问题。已经开发了许多有效的技术。后训练量化（PTQ），作为一种有效的技术类别，将权重（或激活）量化为低精度，从而减少内存使用并可能显著改善延迟。这不仅对服务器端推理重要，也对小型LLM的设备端场景至关重要[[26](#bib.bib26)]。
- en: 'When applying quantization, outliers remain an open challenge because they
    stretch the quantization range, leaving fewer effective bits available for the
    majority of values. Prior research mitigates this challenge by trading quantization
    difficulty between weights and activations [[43](#bib.bib43), [23](#bib.bib23)]
    or employing mixed-precision to handle outliers [[47](#bib.bib47)]. Recent studies
    take a different angle by demonstrating an interesting property: Multiplying the
    weight matrix with a random rotation can effectively reduce outliers and enhance
    quantizability [[7](#bib.bib7), [41](#bib.bib41)]. Intuitively, due to the statistical
    property of random rotation, such a transform yields a outlier-less distribution
    of resulting weight or activation entries [[13](#bib.bib13)]. Since the rotation
    matrices can be constructed in pairs from identity mapping, and get integrated
    into nearby weights without changing the overall network outputs, a property known
    as *rotational invariance*[[4](#bib.bib4)], the transformed weights (or activations)
    can be quantized with lower reconstruction error, without additional inference
    overhead.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用量化时，异常值仍然是一个未解决的挑战，因为它们扩展了量化范围，使大多数值可用的有效位数减少。先前的研究通过在权重和激活之间权衡量化难度[[43](#bib.bib43),
    [23](#bib.bib23)]或采用混合精度来处理异常值[[47](#bib.bib47)]来缓解这一挑战。最近的研究从不同角度出发，展示了一个有趣的特性：将权重矩阵与随机旋转相乘可以有效减少异常值并增强量化能力[[7](#bib.bib7),
    [41](#bib.bib41)]。直观上，由于随机旋转的统计特性，这种变换产生的权重或激活条目分布没有异常值[[13](#bib.bib13)]。由于旋转矩阵可以成对构建从单位映射中，并且在不改变整体网络输出的情况下集成到相邻权重中，这被称为*旋转不变性*[[4](#bib.bib4)]，因此变换后的权重（或激活）可以以较低的重建误差进行量化，而不会增加额外的推理开销。
- en: While statistically any random rotation works well, in this paper, we find that
    the performance of quantized network *varies a lot* with different rotation matrices.
    For example, the downstream averaged accuracy on zero-shot reasoning tasks may
    change up to 13 points with different rotations. As a result, we propose SpinQuant
    that *optimizes* the rotation matrix to minimize the final loss of the quantized
    network, with fixed weight parameters, by employing the Cayley SGD[[21](#bib.bib21)],
    a proficient technique for optimizing orthonormal matrices. This optimization
    does not alter the full-precision network output but refines the intermediate
    activations and weights, making them more quantization-friendly.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从统计学角度来看，任何随机旋转都表现良好，但在本文中，我们发现量化网络的性能在不同的旋转矩阵下*变化很大*。例如，下游零样本推理任务的平均准确率可能会因不同旋转而变化多达13个百分点。因此，我们提出了SpinQuant，它通过使用Cayley
    SGD[[21](#bib.bib21)]，一种优化正交矩阵的高效技术，在固定权重参数的情况下*优化*旋转矩阵，以最小化量化网络的最终损失。这种优化不会改变全精度网络的输出，而是改进中间激活和权重，使它们更适合量化。
- en: 'In SpinQuant, we also extend the concept of outlier reduction for single matrices [[7](#bib.bib7),
    [41](#bib.bib41)] to a comprehensive network-level perspective. We identify four
    different places that are rotationally invariant in most prevalent LLMs (e.g.,
    OPT [[46](#bib.bib46)], LLaMA [[39](#bib.bib39)]), as depicted in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ ​SpinQuant: LLM Quantization with Learned Rotations​").
    This constitutes the optimization space of SpinQuant.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '在SpinQuant中，我们还将单矩阵的异常值减少概念[[7](#bib.bib7), [41](#bib.bib41)]扩展到全面的网络级别视角。我们识别了在大多数流行的LLM（例如OPT[[46](#bib.bib46)],
    LLaMA[[39](#bib.bib39)]）中旋转不变的四个不同位置，如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    ​SpinQuant: LLM Quantization with Learned Rotations​")所示。这构成了SpinQuant的优化空间。'
- en: Experimental results demonstrate that SpinQuant significantly improves accuracy
    compared to random rotation matrices. With 4-bit quantization of weights, activations,
    and KV cache, SpinQuant achieves an average accuracy of 64.0 on zero-shot commonsense
    reasoning tasks for LLaMA-2 7B. This marks a gap of just 2.9 points from the full-precision
    network, considerably better than previous LLM-QAT [[25](#bib.bib25)] with a gap
    as large as 22.0 points despite using the same precision. We also demonstrate
    results on LLaMA-3, showing 17.7 points accuracy improvement than SmoothQuant [[43](#bib.bib43)]
    on 70B model with 4-bit weights, activations, and KV cache quantization, narrowing
    the gap to full-precision to only 4.4 points. In terms of speed, it only takes
    100 iterations (and 1.3 hours) to optimize the rotation matrices on 800 WikiText2[[27](#bib.bib27)]
    calibration data for a LLaMA-2 7B model on a single A100 node.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，SpinQuant在精度上显著优于随机旋转矩阵。在对权重、激活值和KV缓存进行4位量化时，SpinQuant在LLaMA-2 7B的零样本常识推理任务中达到了64.0的平均准确率。这标志着与全精度网络之间仅有2.9点的差距，明显优于之前的LLM-QAT [[25](#bib.bib25)]，尽管使用相同精度，但差距高达22.0点。我们还在LLaMA-3上展示了结果，表明在70B模型中，使用4位权重、激活值和KV缓存量化，相比于SmoothQuant [[43](#bib.bib43)]，准确率提高了17.7点，将与全精度的差距缩小至仅4.4点。在速度方面，仅需100次迭代（1.3小时）即可优化LLaMA-2
    7B模型上的旋转矩阵，使用单个A100节点在800 WikiText2[[27](#bib.bib27)]校准数据上进行。
- en: '![Refer to caption](img/9c2b7103c351bb09ba8ca6fe6e4e4c79.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9c2b7103c351bb09ba8ca6fe6e4e4c79.png)'
- en: 'Figure 1: Overall diagram of rotation. (a) The residual stream can be rotated
    in the transformer network, resulting in numerically equivalent floating point
    networks before and after rotation. The rotated activations exhibit fewer outliers
    and are easier to quantize. (b) & (c) The rotation matrix can be integrated with
    the corresponding weight matrices and we further define $R_{2}$ for reducing outliers
    inside the block.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：旋转的整体示意图。（a）在变换器网络中，残差流可以进行旋转，从而使旋转前后的浮点网络数值等效。旋转后的激活值表现出更少的离群值，更易于量化。（b）&（c）旋转矩阵可以与相应的权重矩阵集成，我们进一步定义$R_{2}$以减少块内的离群值。
- en: 2 Motivation and Preliminaries
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 动机与初步研究
- en: '![Refer to caption](img/75cfad6c597e7cc540738199a4c3dbed.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/75cfad6c597e7cc540738199a4c3dbed.png)'
- en: 'Figure 2: Activation distribution in LLaMA-2 7B model before and after rotation.
    Outliers exist in particular channels before rotation. Since channel-wise quantization
    is not supported in most hardware, outlier removal using rotation enables accurate
    token-wise or tensor-wise quantization.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：LLaMA-2 7B模型在旋转前后的激活分布。在旋转前，特定通道存在离群值。由于大多数硬件不支持通道级量化，因此通过旋转去除离群值可以实现准确的令牌级或张量级量化。
- en: '![Refer to caption](img/76bbed934b175dad8a40b4388d492367.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/76bbed934b175dad8a40b4388d492367.png)'
- en: 'Figure 3: Outlier measurement and quantization error across input activation
    and weights in the five layers that take inputs from the residual (Q/K/V/Up/Gate-projection)
    of each block in the LLaMA-2 7B model. (a) After rotation, kurtosis of activation
    distributions is significantly reduced to approximately three across all layers.
    Quantization error is reduced after rotation in both (b) activations and (c) weights.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：LLaMA-2 7B模型中，五层从每个块的残差（Q/K/V/Up/Gate-projection）接收输入的输入激活和权重的离群值测量和量化误差。（a）旋转后，激活分布的峰度在所有层中显著降低至约3。旋转后（b）激活值和（c）权重的量化误差有所减少。
- en: 'Quantization reduces the precision of weights (and/or activations) in a neural
    network in order to save memory and lower the latency. For Large language models
    (LLMs), the presence of outliers extends the range of weight/activation values
    and increases the reconstruction errors for normal values [[11](#bib.bib11), [24](#bib.bib24),
    [44](#bib.bib44)] (Figures [2](#S2.F2 "Figure 2 ‣ 2 Motivation and Preliminaries
    ‣ ​SpinQuant: LLM Quantization with Learned Rotations​") (a)&(c)).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '量化通过减少神经网络中权重（和/或激活值）的精度，以节省内存并降低延迟。对于大型语言模型（LLMs），离群值的存在扩展了权重/激活值的范围，并增加了正常值的重建误差 [[11](#bib.bib11),
    [24](#bib.bib24), [44](#bib.bib44)]（图 [2](#S2.F2 "图2 ‣ 2 动机与初步研究 ‣ ​SpinQuant:
    LLM量化与学习旋转​")（a）&（c））。'
- en: 2.1 Outlier Reduction via Random Rotation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 通过随机旋转减少离群值
- en: There exist many ways to mitigate the effect of outliers [[43](#bib.bib43),
    [11](#bib.bib11)]. In this paper, we focus on the technique that uses random rotation
    to reduce outliers, which outperforms previous approaches. Intuitively, a random
    rotation matrix statistically blends large and small weights together into a well-behaved
    distribution with fewer outliers, and thus is easier to quantize. Theoretically,
    QuIP[[7](#bib.bib7), [41](#bib.bib41)] proves that multiplication of random orthonormal
    matrix leads to high incoherence (i.e., lower maximal entry of a matrix compared
    to its norm) with high probability and thus fewer outliers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多方法可以减轻离群值的影响[[43](#bib.bib43), [11](#bib.bib11)]。在本文中，我们专注于使用随机旋转来减少离群值的技术，这种方法优于之前的方法。直观上，随机旋转矩阵统计上将大和小权重混合成一个行为良好的分布，离群值较少，因此更容易量化。从理论上讲，QuIP[[7](#bib.bib7),
    [41](#bib.bib41)] 证明了随机正交矩阵的乘法在高概率下会导致高不一致性（即，矩阵的最大条目相对于其范数较低），从而减少离群值。
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ 2 Motivation and Preliminaries ‣ ​SpinQuant:
    LLM Quantization with Learned Rotations​") (a) illustrates the measurement of
    the Kurtosis $\kappa$ across all layers becomes approximately 3, indicating a
    more Gaussian-shaped distribution that is easier to quantize. This is corroborated
    by Figure [3](#S2.F3 "Figure 3 ‣ 2 Motivation and Preliminaries ‣ ​SpinQuant:
    LLM Quantization with Learned Rotations​") (b), where the quantization error of
    the activation tensor significantly decreases after rotation.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '图[3](#S2.F3 "Figure 3 ‣ 2 Motivation and Preliminaries ‣ ​SpinQuant: LLM Quantization
    with Learned Rotations​") (a) 显示了所有层的Kurtosis $\kappa$ 测量值接近 3，表明分布更接近高斯分布，便于量化。这一点在图[3](#S2.F3
    "Figure 3 ‣ 2 Motivation and Preliminaries ‣ ​SpinQuant: LLM Quantization with
    Learned Rotations​") (b) 中得到了验证，其中旋转后的激活张量的量化误差显著减少。'
- en: '![Refer to caption](img/b0e645b2c1d6440ce1c0763949888d0f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b0e645b2c1d6440ce1c0763949888d0f.png)'
- en: 'Figure 4: The performance distributions under different random rotations on
    LLaMA-2 7B, using network-level parameterization (Sec. [3.1](#S3.SS1 "3.1 Rotation
    parameterization ‣ 3 Method ‣ ​SpinQuant: LLM Quantization with Learned Rotations​")).
    We compare the distributions using random floating-point rotations, random Hadamard
    matrices, and optimized rotation matrices with Cayley optimization (Sec. [3.2](#S3.SS2
    "3.2 Cayley-optimized rotation ‣ 3 Method ‣ ​SpinQuant: LLM Quantization with
    Learned Rotations​")). Despite that Hadamard matrices mostly perform better than
    random rotations, both random groups demonstrate large variance. In contrast,
    by optimizing the rotation matrix with Cayley optimization (i.e., SpinQuant),
    the performance is improved significantly and the variance becomes much smaller.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4：在不同随机旋转下，LLaMA-2 7B 的性能分布，使用网络级参数化（Sec. [3.1](#S3.SS1 "3.1 Rotation parameterization
    ‣ 3 Method ‣ ​SpinQuant: LLM Quantization with Learned Rotations​")）。我们比较了使用随机浮点旋转、随机Hadamard矩阵和通过Cayley优化的优化旋转矩阵的分布（Sec.
    [3.2](#S3.SS2 "3.2 Cayley-optimized rotation ‣ 3 Method ‣ ​SpinQuant: LLM Quantization
    with Learned Rotations​")）。尽管Hadamard矩阵的表现通常优于随机旋转，但这两组随机旋转都表现出较大的方差。相比之下，通过Cayley优化（即SpinQuant）优化旋转矩阵，性能显著提高，方差大大减少。'
- en: 2.2 Random rotations produce large variance
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 随机旋转产生大范围方差
- en: 'Interestingly, while statistically random rotation leads to better quantization,
    not all random rotations give the same quantization outcome. To show this, we
    tested the zero-shot average accuracy of the rotated version of LLaMA-2 7B, quantized
    to 4-bit weight and 4-bit activation, under 100 randomized trials. As shown in
    Figure [4](#S2.F4 "Figure 4 ‣ 2.1 Outlier Reduction via Random Rotation ‣ 2 Motivation
    and Preliminaries ‣ ​SpinQuant: LLM Quantization with Learned Rotations​"), the
    performance variance is substantial, with the best random matrix outperforming
    the worst by 13 points. Random Hadamard matrices ¹¹1A Hadamard matrix $H$. outperform
    random matrices, in consistent with the findings in [[41](#bib.bib41)] that Hadamard
    matrices yield tighter bounds on weight quantization error. However, even random
    Hadamard rotation matrices exhibit a non-negligible variance in final performance,
    as large as 6 points.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '有趣的是，虽然统计上的随机旋转导致更好的量化，但并非所有随机旋转都会产生相同的量化结果。为了说明这一点，我们在100次随机试验中测试了旋转版本的LLaMA-2
    7B的零-shot平均准确率，量化为4位权重和4位激活。如图[4](#S2.F4 "Figure 4 ‣ 2.1 Outlier Reduction via
    Random Rotation ‣ 2 Motivation and Preliminaries ‣ ​SpinQuant: LLM Quantization
    with Learned Rotations​")所示，性能方差相当大，最好的随机矩阵比最差的高出13分。随机Hadamard矩阵¹¹1A Hadamard矩阵$H$表现优于随机矩阵，与[[41](#bib.bib41)]的发现一致，即Hadamard矩阵在权重量化误差上提供了更紧的界限。然而，即使是随机Hadamard旋转矩阵也表现出不可忽视的最终性能方差，高达6分。'
- en: 'Given the huge variance across multiple trials of rotations, a natural question
    arises:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于多个旋转试验之间的巨大方差，一个自然的问题出现了：
- en: Is it possible to *optimize* the rotation to maximize the benefit of quantization?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有可能*优化*旋转，以最大化量化的好处？
- en: As the main contribution of this work, we show that not only it is possible
    to do so, but also such an optimization procedure leads to a much better quantized
    network. For LLaMA-2 7B, it reduces the gap between the 4-bit quantization of
    weights, activations, and KV caches and its 16-bit precision version to merely
    2.9 points. It also works well for LLaMA-3 70B, whose performance deteriorates
    under existing quantization techniques [[15](#bib.bib15)], shrinking the 4-bit
    quantized network accuracy gap to full-precision from previous SoTA 9.4 points
    to 4.4 points.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本工作的主要贡献，我们展示了不仅有可能做到这一点，而且这种优化过程会导致一个量化网络性能大幅提升。对于LLaMA-2 7B，它将4位量化的权重、激活和KV缓存与其16位精度版本之间的差距减少到仅2.9分。它对LLaMA-3
    70B也表现良好，尽管在现有量化技术[[15](#bib.bib15)]下性能恶化，但将4位量化网络的准确率差距从先前的SoTA 9.4分缩小到4.4分。
- en: 3 Method
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: In this section, we introduce our rotation parameterization of popular LLM architectures,
    which covers a broad search space to optimize. Such a parameterization leads to
    identity network output without quantization. We then introduce Cayley optimization
    to optimize these rotations for better downstream performance under quantization.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们介绍了对流行LLM架构的旋转参数化，这涵盖了一个广泛的搜索空间以进行优化。这种参数化在没有量化的情况下导致身份网络输出。然后，我们介绍了凯利优化，以优化这些旋转，以在量化下获得更好的下游性能。
- en: 3.1 Rotation parameterization
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 旋转参数化
- en: 'Rotating activations in residual As shown in Figure [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ ​SpinQuant: LLM Quantization with Learned Rotations​")(a), we
    rotate the activations in the residual path by multiplying the embedding output
    $X$ freely without impacting the floating-point network’s accuracy or parameter
    count.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '残差中的激活旋转 如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ​SpinQuant: LLM Quantization
    with Learned Rotations​") (a)所示，我们通过自由地乘以嵌入输出$X$来旋转残差路径中的激活，而不会影响浮点网络的准确性或参数数量。'
- en: 'Rotating activations in the attention block As depicted in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ ​SpinQuant: LLM Quantization with Learned Rotations​")
    (b), in the attention block, we propose to rotate the value matrix by multiplying
    $R_{2}$) without introducing any new parameters in the network.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '注意力块中的激活旋转 如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ​SpinQuant: LLM Quantization
    with Learned Rotations​") (b)所示，在注意力块中，我们建议通过乘以$R_{2}$来旋转值矩阵，而不在网络中引入任何新参数。'
- en: '![Refer to caption](img/5838ca70b2286109dddc9691c1f24bcf.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5838ca70b2286109dddc9691c1f24bcf.png)'
- en: 'Figure 5: Rotation equivalence in Multi-Head Self-Attention.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 多头自注意力中的旋转等价性。'
- en: 'Additional unabsorbed rotations To further address the presence of outliers
    in KV-cache, we adopt a recent approach [[5](#bib.bib5)], which adds a Hadamard
    matrix ($R_{3}$ in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ​SpinQuant:
    LLM Quantization with Learned Rotations​")(c)) inside the feed-forward layer,
    reducing the outliers in the input to the down projection layer [[5](#bib.bib5)].'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '额外的未吸收旋转 为进一步解决 KV-cache 中存在的离群点问题，我们采用了最近的方法 [[5](#bib.bib5)]，该方法在前馈层中添加了一个
    Hadamard 矩阵（图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ​SpinQuant: LLM Quantization
    with Learned Rotations​")(c) 中的 $R_{3}$），从而减少了输入到下投影层中的离群点 [[5](#bib.bib5)]。'
- en: '![Refer to caption](img/50e631cca98a0983da7d801872a925c9.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/50e631cca98a0983da7d801872a925c9.png)'
- en: 'Figure 6: Application of rotation pair to (a) query and key (b) down project
    layer input.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：旋转对（a）查询和键（b）下投影层输入的应用。
- en: 'After inserting rotation into LLM, the difficulty of network quantization is
    significantly alleviated due to outlier elimination. As shown in Figure [3](#S2.F3
    "Figure 3 ‣ 2 Motivation and Preliminaries ‣ ​SpinQuant: LLM Quantization with
    Learned Rotations​") (b), activation quantization error drops sharply from as
    high as 0.7 to around 0.15\. In addition, weight quantization also becomes easier
    by absorbing the rotation matrix into the weight matrix, as shown in Figure [3](#S2.F3
    "Figure 3 ‣ 2 Motivation and Preliminaries ‣ ​SpinQuant: LLM Quantization with
    Learned Rotations​") (c), as depicted in Figure [3](#S2.F3 "Figure 3 ‣ 2 Motivation
    and Preliminaries ‣ ​SpinQuant: LLM Quantization with Learned Rotations​") (c),
    remarking rotation as a beneficial technique that simultaneously helps both weight
    and activation quantization with a single implementation.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '在将旋转应用到 LLM 中后，由于去除了离群点，网络量化的难度显著减轻。如图 [3](#S2.F3 "Figure 3 ‣ 2 Motivation
    and Preliminaries ‣ ​SpinQuant: LLM Quantization with Learned Rotations​") (b)
    所示，激活量化误差从高达 0.7 急剧下降到约 0.15。 此外，通过将旋转矩阵吸收到权重矩阵中，权重量化也变得更容易，如图 [3](#S2.F3 "Figure
    3 ‣ 2 Motivation and Preliminaries ‣ ​SpinQuant: LLM Quantization with Learned
    Rotations​") (c) 所示，这表明旋转作为一种有益技术，通过单一实现同时帮助了权重和激活量化。'
- en: 3.2 Cayley-optimized rotation
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 Cayley 优化旋转
- en: 'As illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ​SpinQuant:
    LLM Quantization with Learned Rotations​"), we have determined that the incorporation
    of four rotation matrices ($R_{1}$ that minimizes the final loss of the quantized
    network:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ​SpinQuant: LLM Quantization with
    Learned Rotations​") 所示，我们已确定，加入四个旋转矩阵（$R_{1}$）可以最小化量化网络的最终损失：'
- en: '|  | $\displaystyle\operatorname*{arg\,min}_{R\in\mathcal{M}}\mathcal{L}_{Q}(R_{1},R_{2}\mid
    W,X)$ |  | (1) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\operatorname*{arg\,min}_{R\in\mathcal{M}}\mathcal{L}_{Q}(R_{1},R_{2}\mid
    W,X)$ |  | (1) |'
- en: 'Here, $\mathcal{M}$ is parameterized as the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\mathcal{M}$ 的参数化如下：
- en: '|  | $R^{\prime}=\Delta R(Y)R:=\left(I-\frac{\alpha}{2}Y\right)^{-1}\left(I+\frac{\alpha}{2}Y\right)R$
    |  | (2) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $R^{\prime}=\Delta R(Y)R:=\left(I-\frac{\alpha}{2}Y\right)^{-1}\left(I+\frac{\alpha}{2}Y\right)R$
    |  | (2) |'
- en: 'where $\Delta R(Y):=(I-\frac{\alpha}{2}Y)^{-1}(I+\frac{\alpha}{2}Y)$ of the
    loss function:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Delta R(Y):=(I-\frac{\alpha}{2}Y)^{-1}(I+\frac{\alpha}{2}Y)$ 是损失函数的一部分：
- en: '|  | $Y=\hat{G}-\hat{G}^{\top},\quad\quad\hat{G}:=GR^{\top}-\frac{1}{2}RR^{\top}GR^{\top}$
    |  | (3) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $Y=\hat{G}-\hat{G}^{\top},\quad\quad\hat{G}:=GR^{\top}-\frac{1}{2}RR^{\top}GR^{\top}$
    |  | (3) |'
- en: It can be shown that $\Delta R(Y)$2 times the computation time per iteration
    compared to a naive SGD algorithm.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，相比于简单的 SGD 算法，$\Delta R(Y)$ 每次迭代的计算时间增加了 2 倍。
- en: 'We apply the Cayley SGD method to solve Eqn. ([1](#S3.E1 "Equation 1 ‣ 3.2
    Cayley-optimized rotation ‣ 3 Method ‣ ​SpinQuant: LLM Quantization with Learned
    Rotations​")) for $\{R_{1},R_{2}\}$ count for only 0.26% of the weight size and
    is constrained to be orthonormal. Consequently, the underlying floating-point
    network remains unchanged, and the rotation only influences the quantization performance.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '我们应用 Cayley SGD 方法来求解 Eqn. ([1](#S3.E1 "Equation 1 ‣ 3.2 Cayley-optimized rotation
    ‣ 3 Method ‣ ​SpinQuant: LLM Quantization with Learned Rotations​"))，其中 $\{R_{1},R_{2}\}$
    仅占权重大小的 0.26%，且被约束为正交归一的。因此，底层浮点网络保持不变，旋转仅影响量化性能。'
- en: 'By employing Cayley optimization to update the rotation for 100 iterations
    on an 800-sample WikiText2 calibration dataset, we obtain a rotation matrix that
    outperforms the best random matrix and random Hadamard matrix in 100 random seeds,
    shown in Figure [4](#S2.F4 "Figure 4 ‣ 2.1 Outlier Reduction via Random Rotation
    ‣ 2 Motivation and Preliminaries ‣ ​SpinQuant: LLM Quantization with Learned Rotations​").
    The Cayley-optimized rotation exhibits minimal variance when initiated from different
    random seeds. The rotation matrices are initialized with random Hadamard matrices
    for optimization in our experiments and our ablation study in Section [4.3.3](#S4.SS3.SSS3
    "4.3.3 Rotation type ‣ 4.3 Ablation studies ‣ 4.2 Main results ‣ 4 Experiments
    ‣ ​SpinQuant: LLM Quantization with Learned Rotations​") demonstrates that the
    optimized rotation is robust to random rotation initialization as well.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '通过使用 Cayley 优化在 800 样本的 WikiText2 校准数据集上进行 100 次迭代更新旋转，我们获得了一个比 100 个随机种子下的最佳随机矩阵和随机
    Hadamard 矩阵表现更好的旋转矩阵，如图 [4](#S2.F4 "Figure 4 ‣ 2.1 Outlier Reduction via Random
    Rotation ‣ 2 Motivation and Preliminaries ‣ ​SpinQuant: LLM Quantization with
    Learned Rotations​") 所示。Cayley 优化的旋转在不同随机种子下展现了最小的方差。在我们的实验中，旋转矩阵使用随机 Hadamard
    矩阵初始化，且在第 [4.3.3](#S4.SS3.SSS3 "4.3.3 Rotation type ‣ 4.3 Ablation studies ‣ 4.2
    Main results ‣ 4 Experiments ‣ ​SpinQuant: LLM Quantization with Learned Rotations​")
    节的消融研究中证明，优化后的旋转对于随机旋转初始化也具有鲁棒性。'
- en: 4 Experiments
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: We conduct experiments on the LLaMA-2[[40](#bib.bib40)] models (7B/13B/70B)
    and the LLaMA-3[[3](#bib.bib3)] models (8B/70B). Our evaluation of the proposed
    SpinQuant was carried out on eight zero-shot commonsense reasoning tasks. These
    tasks include BoolQ[[8](#bib.bib8)], PIQA[[6](#bib.bib6)], SIQA[[34](#bib.bib34)],
    HellaSwag[[45](#bib.bib45)], WinoGrande[[33](#bib.bib33)], ARC-easy and ARC-challenge[[9](#bib.bib9)],
    and OBQA [[28](#bib.bib28)]. Additionally, we also report the perplexity score
    on WikiText2 testset [[27](#bib.bib27)] for our evaluation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 LLaMA-2 [[40](#bib.bib40)] 模型（7B/13B/70B）和 LLaMA-3 [[3](#bib.bib3)] 模型（8B/70B）上进行实验。我们对提出的
    SpinQuant 的评估在八个零-shot 常识推理任务上进行。这些任务包括 BoolQ [[8](#bib.bib8)], PIQA [[6](#bib.bib6)],
    SIQA [[34](#bib.bib34)], HellaSwag [[45](#bib.bib45)], WinoGrande [[33](#bib.bib33)],
    ARC-easy 和 ARC-challenge [[9](#bib.bib9)]，以及 OBQA [[28](#bib.bib28)]。此外，我们还报告了
    WikiText2 测试集 [[27](#bib.bib27)] 上的困惑度分数。
- en: 4.1 Experimental settings
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: We employ Cayley SGD [[21](#bib.bib21)] to optimize the rotation matrix, $R_{1}$12
    hours for 70B models on 8 A100 GPUs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Cayley SGD [[21](#bib.bib21)] 来优化旋转矩阵，$R_{1}$，在 8 个 A100 GPU 上训练 70B 模型
    12 小时。
- en: The first results we show employ simple round-to-nearest (RTN) quantization
    for the weights. The activation and key-value cache use asymmetric min-max dynamic
    quantization, with per-token activation quantization and group size 128 for the
    key-value quantization. Weight quantization employs per-channel symmetric quantization,
    and we set the quantization ranges via a linear search of the mean-squared error
    loss between quantized full-precision weights, following common practice [[7](#bib.bib7),
    [30](#bib.bib30), [24](#bib.bib24)]. Besides rounding-to-nearest, we also show
    that our method is compatible with GPTQ [[14](#bib.bib14)], for which we adhere
    to the standard GPTQ settings by using 128 samples from WikiText-2 with a sequence
    length of 2048 as the calibration set for GPTQ quantization.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示的第一个结果使用了简单的四舍五入（RTN）量化来处理权重。激活和键值缓存使用了非对称的最小-最大动态量化，激活量化按每个令牌进行，而键值量化的组大小为
    128。权重量化使用每通道对称量化，我们通过对量化的全精度权重的均方误差损失进行线性搜索来设置量化范围，遵循常见实践 [[7](#bib.bib7), [30](#bib.bib30),
    [24](#bib.bib24)]。除了四舍五入，我们还展示了我们的方法与 GPTQ [[14](#bib.bib14)] 的兼容性，对于 GPTQ 量化，我们遵循标准
    GPTQ 设置，使用 128 个来自 WikiText-2 的样本，序列长度为 2048 作为 GPTQ 量化的校准集。
- en: 4.2 Main results
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主要结果
- en: 'Table 1: Comparison of the perplexity score on WikiText2 and averaged accuracy
    on Zero-shot Common Sense Reasoning tasks. 0-shot${}^{4}$ adds BoolQ, SIQA, HellaSwag,
    and OBQA tasks. Results for SmoothQuant[[43](#bib.bib43)], LLM-QAT[[25](#bib.bib25)],
    GPTQ [[14](#bib.bib14)], and QuaRot [[5](#bib.bib5)] were obtained using their
    publicly released codebase. While OmniQuant [[35](#bib.bib35)], AQLM [[12](#bib.bib12)],
    AWQ [[23](#bib.bib23)], QuIP [[7](#bib.bib7)], and QuIP# [[41](#bib.bib41)] results
    were quoted from their papers. SpinQuant* and QuaRot* represent using RTN quantization,
    while SpinQuant and QuaRot denote using GPTQ weight quantization. Mean scores
    for SpinQuant, GPTQ, and QuaRot are reported from six trials. Full results, including
    error bars, are in the Appendix.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在 WikiText2 上的困惑度评分与 Zero-shot Common Sense Reasoning 任务上的平均准确度比较。0-shot${}^{4}$
    增加了 BoolQ、SIQA、HellaSwag 和 OBQA 任务。SmoothQuant[[43](#bib.bib43)]、LLM-QAT[[25](#bib.bib25)]、GPTQ[[14](#bib.bib14)]
    和 QuaRot[[5](#bib.bib5)] 的结果是使用他们公开发布的代码库获得的。OmniQuant[[35](#bib.bib35)]、AQLM[[12](#bib.bib12)]、AWQ[[23](#bib.bib23)]、QuIP[[7](#bib.bib7)]
    和 QuIP#[[41](#bib.bib41)] 的结果则引用自他们的论文。SpinQuant* 和 QuaRot* 表示使用 RTN 量化，而 SpinQuant
    和 QuaRot 表示使用 GPTQ 权重量化。SpinQuant、GPTQ 和 QuaRot 的平均分数来自六次试验。完整结果，包括误差条，见附录。
- en: '|  |  | LLaMA-3 8B | LLaMA-3 70B | LLaMA-2 7B | LLaMA-2 13B | LLaMA-2 70B |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLaMA-3 8B | LLaMA-3 70B | LLaMA-2 7B | LLaMA-2 13B | LLaMA-2 70B |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| \cdashline3-15 #Bits | Method | 0-shot${}^{8}$ | Wiki |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline3-15 #Bits | 方法 | 0-shot${}^{8}$ | Wiki |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
- en: '| W-A-KV |  | Avg.($\uparrow$) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| W-A-KV |  | 平均（$\uparrow$） |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
- en: '| 16-16-16 | FloatingPoint | 69.6 | 6.1 | 74.5 | 2.8 | 68.6 | 66.9 | 5.5 |
    69.9 | 68.3 | 5.0 | 76.0 | 72.9 | 3.3 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 16-16-16 | 浮点数 | 69.6 | 6.1 | 74.5 | 2.8 | 68.6 | 66.9 | 5.5 | 69.9 | 68.3
    | 5.0 | 76.0 | 72.9 | 3.3 |'
- en: '| \hdashline 4-16-16 | RTN | 65.4 | 7.8 | 35.5 | 1e5 | 65.3 | 63.6 | 7.2 |
    59.9 | 57.9 | 6.4 | 72.3 | 69.2 | 4.6 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline 4-16-16 | RTN | 65.4 | 7.8 | 35.5 | 1e5 | 65.3 | 63.6 | 7.2 |
    59.9 | 57.9 | 6.4 | 72.3 | 69.2 | 4.6 |'
- en: '|  | SmoothQuant | 61.0 | 10.7 | 66.9 | 12.0 | 62.3 | 59.1 | 7.5 | 66.1 | 63.3
    | 6.1 | 73.6 | 70.2 | 4.1 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | SmoothQuant | 61.0 | 10.7 | 66.9 | 12.0 | 62.3 | 59.1 | 7.5 | 66.1 | 63.3
    | 6.1 | 73.6 | 70.2 | 4.1 |'
- en: '|  | LLM-QAT | 67.7 | 7.1 | – | – | 67.1 | 64.9 | 5.9 | – | – | – | – | – |
    – |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-QAT | 67.7 | 7.1 | – | – | 67.1 | 64.9 | 5.9 | – | – | – | – | – |
    – |'
- en: '|  | OmniQuant | – | – | – | – | 62.5 | – | 5.7 | 64.9 | – | 5.0 | 71.1 | –
    | 3.5 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | OmniQuant | – | – | – | – | 62.5 | – | 5.7 | 64.9 | – | 5.0 | 71.1 | –
    | 3.5 |'
- en: '|  | QuIP | – | – | – | – | – | – | – | 66.7 | – | – | 69.4 | – | – |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | QuIP | – | – | – | – | – | – | – | 66.7 | – | – | 69.4 | – | – |'
- en: '|  | AQLM | – | – | – | – | 63.6 | – | – | 66.3 | – | – | 71.9 | – | – |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | AQLM | – | – | – | – | 63.6 | – | – | 66.3 | – | – | 71.9 | – | – |'
- en: '|  | QuIP# | – | – | – | – | 63.9 | – | – | 67.1 | – | – | 71.8 | – | – |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | QuIP# | – | – | – | – | 63.9 | – | – | 67.1 | – | – | 71.8 | – | – |'
- en: '|  | AWQ | – | – | – | – | – | – | 6.2 | – | – | 5.1 | – | – | – |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | AWQ | – | – | – | – | – | – | 6.2 | – | – | 5.1 | – | – | – |'
- en: '|  | GPTQ | 66.5 | 7.2 | 35.7 | 1e5 | 66.6 | 64.5 | 11.3 | 67.0 | 64.7 | 5.6
    | 75.0 | 71.9 | 3.9 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 66.5 | 7.2 | 35.7 | 1e5 | 66.6 | 64.5 | 11.3 | 67.0 | 64.7 | 5.6
    | 75.0 | 71.9 | 3.9 |'
- en: '|  | QuaRot* | 66.2 | 7.5 | 57.2 | 41.6 | 64.5 | 62.4 | 6.9 | 69.3 | 67.1 |
    5.5 | 74.6 | 71.8 | 3.7 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | QuaRot* | 66.2 | 7.5 | 57.2 | 41.6 | 64.5 | 62.4 | 6.9 | 69.3 | 67.1 |
    5.5 | 74.6 | 71.8 | 3.7 |'
- en: '|  | QuaRot | 68.4 | 6.4 | 70.3 | 7.9 | 67.6 | 65.8 | 5.6 | 70.3 | 68.3 | 5.0
    | 75.2 | 72.2 | 3.5 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | QuaRot | 68.4 | 6.4 | 70.3 | 7.9 | 67.6 | 65.8 | 5.6 | 70.3 | 68.3 | 5.0
    | 75.2 | 72.2 | 3.5 |'
- en: '|  | SpinQuant* | 67.6 | 6.5 | 71.4 | 3.9 | 66.7 | 64.6 | 5.5 | 69.6 | 67.4
    | 4.9 | 74.9 | 72.2 | 3.5 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | SpinQuant* | 67.6 | 6.5 | 71.4 | 3.9 | 66.7 | 64.6 | 5.5 | 69.6 | 67.4
    | 4.9 | 74.9 | 72.2 | 3.5 |'
- en: '|  | SpinQuant | 68.5 | 6.4 | 71.6 | 4.8 | 67.7 | 65.9 | 5.6 | 70.5 | 68.5
    | 5.0 | 75.4 | 72.6 | 3.5 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | SpinQuant | 68.5 | 6.4 | 71.6 | 4.8 | 67.7 | 65.9 | 5.6 | 70.5 | 68.5
    | 5.0 | 75.4 | 72.6 | 3.5 |'
- en: '| \hdashline 4-4-16 | RTN | 38.5 | 9e2 | 35.6 | 1e5 | 37.3 | 35.6 | 2e3 | 37.9
    | 35.3 | 7e3 | 37.2 | 35.1 | 2e5 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline 4-4-16 | RTN | 38.5 | 9e2 | 35.6 | 1e5 | 37.3 | 35.6 | 2e3 | 37.9
    | 35.3 | 7e3 | 37.2 | 35.1 | 2e5 |'
- en: '|  | SmoothQuant | 40.3 | 8e2 | 55.3 | 18.0 | 44.2 | 41.8 | 2e2 | 46.4 | 44.9
    | 34.5 | 46.8 | 64.6 | 57.1 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | SmoothQuant | 40.3 | 8e2 | 55.3 | 18.0 | 44.2 | 41.8 | 2e2 | 46.4 | 44.9
    | 34.5 | 46.8 | 64.6 | 57.1 |'
- en: '|  | LLM-QAT | 44.9 | 42.9 | – | – | 48.8 | 47.8 | 12.9 | – | – | – | – | –
    | – |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-QAT | 44.9 | 42.9 | – | – | 48.8 | 47.8 | 12.9 | – | – | – | – | –
    | – |'
- en: '|  | OmniQuant | – | – | – | – | – | – | 14.3 | – | – | 12.3 | – | – | – |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | OmniQuant | – | – | – | – | – | – | 14.3 | – | – | 12.3 | – | – | – |'
- en: '|  | GPTQ | 37.0 | 9e2 | 35.3 | 1e5 | 38.3 | 36.8 | 8e3 | 37.7 | 35.3 | 5e3
    | 37.8 | 35.5 | 2e6 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 37.0 | 9e2 | 35.3 | 1e5 | 38.3 | 36.8 | 8e3 | 37.7 | 35.3 | 5e3
    | 37.8 | 35.5 | 2e6 |'
- en: '|  | QuaRot* | 59.5 | 10.4 | 41.5 | 91.2 | 60.9 | 59.0 | 8.2 | 66.9 | 64.8
    | 6.1 | 72.6 | 69.7 | 4.2 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | QuaRot* | 59.5 | 10.4 | 41.5 | 91.2 | 60.9 | 59.0 | 8.2 | 66.9 | 64.8
    | 6.1 | 72.6 | 69.7 | 4.2 |'
- en: '|  | QuaRot | 63.8 | 7.9 | 65.4 | 20.4 | 65.6 | 63.5 | 6.1 | 68.5 | 66.7 |
    5.4 | 72.9 | 70.4 | 3.9 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | QuaRot | 63.8 | 7.9 | 65.4 | 20.4 | 65.6 | 63.5 | 6.1 | 68.5 | 66.7 |
    5.4 | 72.9 | 70.4 | 3.9 |'
- en: '|  | SpinQuant* | 64.6 | 7.7 | 70.1 | 4.1 | 63.6 | 61.8 | 6.1 | 67.7 | 65.8
    | 5.4 | 73.5 | 71.1 | 3.9 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | SpinQuant* | 64.6 | 7.7 | 70.1 | 4.1 | 63.6 | 61.8 | 6.1 | 67.7 | 65.8
    | 5.4 | 73.5 | 71.1 | 3.9 |'
- en: '|  | SpinQuant | 65.8 | 7.1 | 69.5 | 5.5 | 65.9 | 64.1 | 5.9 | 69.3 | 67.2
    | 5.2 | 73.5 | 71.0 | 3.8 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | SpinQuant | 65.8 | 7.1 | 69.5 | 5.5 | 65.9 | 64.1 | 5.9 | 69.3 | 67.2
    | 5.2 | 73.5 | 71.0 | 3.8 |'
- en: '| \hdashline 4-4-4 | RTN | 38.2 | 1e3 | 35.2 | 1e5 | 38.2 | 37.1 | 2e3 | 37.2
    | 35.4 | 7e3 | 37.4 | 35.0 | 2e5 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline 4-4-4 | RTN | 38.2 | 1e3 | 35.2 | 1e5 | 38.2 | 37.1 | 2e3 | 37.2
    | 35.4 | 7e3 | 37.4 | 35.0 | 2e5 |'
- en: '|  | SmoothQuant | 38.7 | 1e3 | 52.4 | 22.1 | 40.1 | 39.0 | 6e2 | 42.7 | 40.5
    | 56.6 | 58.8 | 55.9 | 10.5 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | SmoothQuant | 38.7 | 1e3 | 52.4 | 22.1 | 40.1 | 39.0 | 6e2 | 42.7 | 40.5
    | 56.6 | 58.8 | 55.9 | 10.5 |'
- en: '|  | LLM-QAT | 43.2 | 52.5 | – | – | 45.5 | 44.9 | 14.9 | – | – | – | – | –
    | – |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-QAT | 43.2 | 52.5 | – | – | 45.5 | 44.9 | 14.9 | – | – | – | – | –
    | – |'
- en: '|  | GPTQ | 37.1 | 1e3 | 35.1 | 1e5 | 38.1 | 36.8 | 9e3 | 37.4 | 35.2 | 5e3
    | 37.8 | 35.6 | 1e6 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 37.1 | 1e3 | 35.1 | 1e5 | 38.1 | 36.8 | 9e3 | 37.4 | 35.2 | 5e3
    | 37.8 | 35.6 | 1e6 |'
- en: '|  | QuaRot* | 58.6 | 10.9 | 41.3 | 92.4 | 60.5 | 58.7 | 8.2 | 66.5 | 64.4
    | 6.2 | 72.2 | 69.5 | 4.2 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | QuaRot* | 58.6 | 10.9 | 41.3 | 92.4 | 60.5 | 58.7 | 8.2 | 66.5 | 64.4
    | 6.2 | 72.2 | 69.5 | 4.2 |'
- en: '|  | QuaRot | 63.3 | 8.0 | 65.1 | 20.2 | 64.4 | 62.5 | 6.4 | 68.1 | 66.2 |
    5.4 | 73.0 | 70.3 | 3.9 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | QuaRot | 63.3 | 8.0 | 65.1 | 20.2 | 64.4 | 62.5 | 6.4 | 68.1 | 66.2 |
    5.4 | 73.0 | 70.3 | 3.9 |'
- en: '|  | SpinQuant* | 64.1 | 7.8 | 70.1 | 4.1 | 63.2 | 61.5 | 6.2 | 67.7 | 65.5
    | 5.4 | 73.2 | 70.5 | 3.9 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | SpinQuant* | 64.1 | 7.8 | 70.1 | 4.1 | 63.2 | 61.5 | 6.2 | 67.7 | 65.5
    | 5.4 | 73.2 | 70.5 | 3.9 |'
- en: '|  | SpinQuant | 65.2 | 7.3 | 69.3 | 5.5 | 66.0 | 64.0 | 5.9 | 68.9 | 66.9
    | 5.3 | 73.7 | 71.2 | 3.8 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | SpinQuant | 65.2 | 7.3 | 69.3 | 5.5 | 66.0 | 64.0 | 5.9 | 68.9 | 66.9
    | 5.3 | 73.7 | 71.2 | 3.8 |'
- en: 'Our primary comparison metric is accuracy in challenging 4-bit quantization
    scenarios, which include weight quantization, activation quantization, and KV-cache
    quantization. We use RTN to map weights, activations, and KV-cache to integers
    for inference, after applying the Cayley-optimized $R$. We compare SpinQuant to
    state-of-the-art quantization methods on LLaMA-2 and LLaMA-3 in Table [4.2](#S4.SS2
    "4.2 Main results ‣ 4 Experiments ‣ ​SpinQuant: LLM Quantization with Learned
    Rotations​"), where we represent the number of bits for weight, activation, and
    KV-cache as W-A-KV. We compare our proposed method against other PTQ methods including,
    GPTQ [[14](#bib.bib14)], SmoothQuant [[43](#bib.bib43)], QuIP [[7](#bib.bib7)],
    QuIP# [[41](#bib.bib41)], OmniQuant [[35](#bib.bib35)], AQLM [[12](#bib.bib12)],
    as well as a quantization-aware training (QAT) method, LLM-QAT [[25](#bib.bib25)].
    We also compare our results to a concurrent work QuaRot [[5](#bib.bib5)].'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的主要比较指标是在具有挑战性的4位量化场景下的准确性，这些场景包括权重量化、激活量化和KV缓存量化。我们使用RTN将权重、激活和KV缓存映射为整数以进行推理，之后应用了Cayley优化的$R$。我们在表[4.2](#S4.SS2
    "4.2 主要结果 ‣ 4 实验 ‣ ​SpinQuant: 具有学习旋转的LLM量化​")中将SpinQuant与最新的量化方法进行比较，其中我们用W-A-KV表示权重、激活和KV缓存的位数。我们将我们提出的方法与其他PTQ方法进行比较，包括GPTQ [[14](#bib.bib14)]、SmoothQuant [[43](#bib.bib43)]、QuIP [[7](#bib.bib7)]、QuIP# [[41](#bib.bib41)]、OmniQuant [[35](#bib.bib35)]、AQLM [[12](#bib.bib12)]，以及量化感知训练（QAT）方法LLM-QAT [[25](#bib.bib25)]。我们还将我们的结果与一项并行工作QuaRot [[5](#bib.bib5)]进行比较。'
- en: In the most challenging 4-4-4 quantization setting, SpinQuant significantly
    surpasses LLM-QAT, by 19.1 points on the LLaMA-2 7B models, thereby reducing the
    gap to the corresponding full-precision network to a mere 2.9 points, respectively.
    This is measured in terms of the average accuracy across eight zero-shot tasks.
    This represents a significant advancement, given the difficulties previous work
    encountered in generating meaningful results in this context. Furthermore, SpinQuant
    also outperforms QuaRot by non-negligible 1.5 points on LLaMA-2 7B models. When
    applying SpinQuant approach to larger models, we can obtain 4-4-4 LLaMA-2 13B
    and LLaMA-2 70B models with only 1.4 and 1.7 points gap to the corresponding full-precision
    network, respectively.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在最具挑战性的4-4-4量化设置中，SpinQuant显著超越了LLM-QAT，在LLaMA-2 7B模型上超出19.1分，从而将与相应全精度网络的差距缩小至仅2.9分。这是以八个零样本任务的平均准确率来衡量的。这标志着一个重要进展，因为以前的工作在此背景下生成有意义的结果遇到了困难。此外，SpinQuant在LLaMA-2
    7B模型上也比QuaRot高出不容忽视的1.5分。当将SpinQuant方法应用于更大的模型时，我们可以获得4-4-4 LLaMA-2 13B和LLaMA-2
    70B模型，其与相应全精度网络的差距分别为1.4和1.7分。
- en: It is noteworthy that our LLaMA-3 quantization experiences a larger degradation
    compared to LLaMA-2\. This trend is also observed in a recent work [[15](#bib.bib15)],
    suggesting that LLaMA-3 is generally more difficult to quantize. The previously
    best 4-4-4 quantized model, obtained with QuaRot, had a 6.3 point gap to full
    precision on the 8B model and an even larger drop of 9.4 points on the 70B model.
    In contrast, SpinQuant improves the LLaMA-3 8B and 70B models by 1.9 points and
    5.0 points respectively, reducing the gap to approximately 4.4 points for both
    model sizes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们的 LLaMA-3 量化相比于 LLaMA-2 体验到的降级更大。最近的一项研究 [[15](#bib.bib15)] 也观察到了这一趋势，建议
    LLaMA-3 一般来说更难量化。之前最好的 4-4-4 量化模型，使用 QuaRot 获得的，在 8B 模型上与全精度相比有 6.3 点的差距，而在 70B
    模型上更大的下降为 9.4 点。相比之下，SpinQuant 在 LLaMA-3 8B 和 70B 模型上分别提高了 1.9 点和 5.0 点，将差距减少到大约
    4.4 点。
- en: In the 4-4-16 quantization setting, SpinQuant consistently outperforms previous
    work with a non-negligible margin. Notably, using SpinQuant for W4A4 quantization
    yields gaps to floating point networks as small as 2.8/1.1/1.8 points averaged
    across eight zero-shot tasks on LLaMA-2 7B/13B/70B models, respectively. In LLaMA-3
    models, SpinQuant further narrows the accuracy gap to full precision from the
    previous SoTA of 5.8 points to 3.8 points on the 8B model, and from 9.1 to 4.4
    points on the 70B model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在 4-4-16 量化设置下，SpinQuant 一贯超越了之前的工作，差距不容忽视。特别是，使用 SpinQuant 进行 W4A4 量化，LLaMA-2
    7B/13B/70B 模型在八个零-shot 任务中，浮点网络的差距分别小至 2.8/1.1/1.8 点。 在 LLaMA-3 模型中，SpinQuant
    进一步将精度差距从之前的 SoTA 5.8 点缩小到 3.8 点（8B 模型），以及从 9.1 点缩小到 4.4 点（70B 模型）。
- en: In the context of 4-bit weight-only quantization, SpinQuant continues to demonstrate
    its effectiveness in enhancing accuracy and minimizing the disparity between the
    quantized network and full-precision network to $\sim$1 point on the average zero-shot
    accuracy on LLaMA-3 8B and LLaMA-2 models, and shows significant improvement on
    the LLaMA-3 70B model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在 4 位权重量化的背景下，SpinQuant 继续展示其在提升精度和减少量化网络与全精度网络之间差异方面的有效性，将 LLaMA-3 8B 和 LLaMA-2
    模型的平均零-shot 精度差距缩小至 $\sim$1 点，并在 LLaMA-3 70B 模型上显示出显著改进。
- en: 4.3 Ablation studies
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: 4.3.1 Compatibility with GPTQ
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 与 GPTQ 的兼容性
- en: 'Table 2: Compatibility with GPTQ.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：与 GPTQ 的兼容性。
- en: '| #Bits${}_{\text{(W-A-KV)}}$ | Task | Cayley on 4-4-KV | Cayley on 16-4-KV
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| #Bits${}_{\text{(W-A-KV)}}$ | 任务 | 4-4-KV 上的 Cayley | 16-4-KV 上的 Cayley |'
- en: '| --- | --- | --- | --- |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 4-4-16 | 0-shot${}^{8}$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 4-4-16 | 0-shot${}^{8}$ |'
- en: '| Wiki | 6.7 ${}_{\pm 0.07}$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Wiki | 6.7 ${}_{\pm 0.07}$ |'
- en: '| 4-4-4 | 0-shot${}^{8}$ |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 4-4-4 | 0-shot${}^{8}$ |'
- en: '| Wiki | 6.8 ${}_{\pm 0.15}$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Wiki | 6.8 ${}_{\pm 0.15}$ |'
- en: 'In the context where both weights and activations are quantized, we observed
    that the Cayley-optimized rotations tend to adapt effectively to both weight and
    activation quantization. Given that GPTQ significantly helps mitigate the errors
    due to weight quantization, but leaves activation quantization untouched, we elect
    to optimize the rotation matrices with respect to a network where only activations
    are quantized. This approach allows the rotation to more efficiently manage the
    activation quantization error, while leaving the weight quantization error to
    be addressed by GPTQ. As shown in Table [2](#S4.T2 "Table 2 ‣ 4.3.1 Compatibility
    with GPTQ ‣ 4.3 Ablation studies ‣ 4.2 Main results ‣ 4 Experiments ‣ ​SpinQuant:
    LLM Quantization with Learned Rotations​"), this modification resulted in superior
    performance in both W4A4 and W4A4KV4 settings in the LLaMA-2 7B model, which is
    the configuration we have chosen to utilize throughout the rest of this paper.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '在权重和激活量化的背景下，我们观察到 Cayley 优化的旋转趋向于有效适应权重和激活量化。鉴于 GPTQ 在减少权重量化引起的误差方面显著有效，但对激活量化的处理尚未触及，我们选择对仅量化激活的网络优化旋转矩阵。这种方法允许旋转更有效地管理激活量化误差，同时将权重量化误差交由
    GPTQ 处理。如表 [2](#S4.T2 "表 2 ‣ 4.3.1 与 GPTQ 的兼容性 ‣ 4.3 消融研究 ‣ 4.2 主要结果 ‣ 4 实验 ‣
    ​SpinQuant: LLM 量化与学习的旋转​") 所示，这一修改在 LLaMA-2 7B 模型的 W4A4 和 W4A4KV4 设置中带来了更优的性能，这也是我们在本文其余部分选择使用的配置。'
- en: 4.3.2 Impact of each rotation
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 各个旋转的影响
- en: 'Table 3: Impact of individual rotation matrices ($R_{1}$ (down projection layer
    input rotation).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：单独旋转矩阵的影响（$R_{1}$（向下投影层输入旋转））。
- en: '|  | W4A16KV16 | W4A4KV16 | W4A4KV4 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | W4A16KV16 | W4A4KV16 | W4A4KV4 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | 0-shot${}^{8}$ | Wiki |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | 0-shot${}^{8}$ | Wiki |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Rotation | Avg.($\uparrow$) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 旋转 | 平均值($\uparrow$) |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| no rotation | 63.6 | 7.2 | 35.6 | 2167.2 | 37.1 | 2382.5 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 无旋转 | 63.6 | 7.2 | 35.6 | 2167.2 | 37.1 | 2382.5 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| $R_{1}$ |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| $R_{1}$ |'
- en: '| $R_{1}$ |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| $R_{1}$ |'
- en: '| $R_{1}$ |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| $R_{1}$ |'
- en: '| $R_{1}$ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| $R_{1}$ |'
- en: We performed an ablation study to assess the influence of each rotation matrix,
    employing round-to-nearest quantization for our evaluation. Cayley optimization
    was used to optimize $R_{1}$, effectively restoring the accuracy.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了消融研究，以评估每种旋转矩阵的影响，采用了最接近量化的方法进行评估。使用 Cayley 优化来优化 $R_{1}$，有效地恢复了准确性。
- en: 4.3.3 Rotation type
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 旋转类型
- en: 'Table 4: Floating-point(FP) rotation vs Hadamard rotation'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 浮点（FP）旋转与 Hadamard 旋转'
- en: '| #Bits |  | No Cayley + RTN | Cayley + RTN |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| #位数 |  | 无 Cayley + RTN | Cayley + RTN |'
- en: '| --- | --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| (W-A-KV) | Task | FP | Hadamard | FP init. | Hadamard init. |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| (W-A-KV) | 任务 | FP | Hadamard | FP 初始化 | Hadamard 初始化 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 4-16-16 | 0-shot${}^{8}$ |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 4-16-16 | 0-shot${}^{8}$ |'
- en: '| Wiki($\downarrow$ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 维基百科($\downarrow$ |'
- en: '| 4-4-16 | 0-shot${}^{8}$ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 4-4-16 | 0-shot${}^{8}$ |'
- en: '| Wiki($\downarrow$ |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 维基百科($\downarrow$ |'
- en: '| 4-4-4 | 0-shot${}^{8}$ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 4-4-4 | 0-shot${}^{8}$ |'
- en: '| Wiki($\downarrow$ |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 维基百科($\downarrow$ |'
- en: 'In Table [4](#S4.T4 "Table 4 ‣ 4.3.3 Rotation type ‣ 4.3 Ablation studies ‣
    4.2 Main results ‣ 4 Experiments ‣ ​SpinQuant: LLM Quantization with Learned Rotations​"),
    we evaluate the impact of random orthogonal floating-point rotation matrices and
    random Hadamard matrices on quantization accuracy, utilizing round-to-nearest
    quantization for our analysis. Prior to the application of Cayley optimization,
    the Hadamard matrices yield better quantized network performance compared to floating-point
    rotation matrices. However, after Cayley optimization, the initial choice of rotation,
    whether floating-point or Hadamard, becomes less significant. This is likely due
    to the Cayley optimization’s ability to locate an optimal local minima that effectively
    minimizes quantization error, thereby enhancing robustness to varying types of
    rotation initialization.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [4](#S4.T4 "表 4 ‣ 4.3.3 旋转类型 ‣ 4.3 消融研究 ‣ 4.2 主要结果 ‣ 4 实验 ‣ ​SpinQuant:
    使用学习的旋转进行 LLM 量化​") 中，我们评估了随机正交浮点旋转矩阵和随机 Hadamard 矩阵对量化准确性的影响，利用最接近量化方法进行分析。在应用
    Cayley 优化之前，Hadamard 矩阵相比于浮点旋转矩阵产生了更好的量化网络性能。然而，在 Cayley 优化之后，初始的旋转选择，无论是浮点还是
    Hadamard，都变得不那么重要。这可能是由于 Cayley 优化能够找到一个有效最小化量化误差的局部最优解，从而增强了对不同类型旋转初始化的鲁棒性。'
- en: 5 Related Work
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: 'Quantization Neural network quantization has been demonstrated as an effective
    tool for model size compression and storage reduction [[30](#bib.bib30), [19](#bib.bib19),
    [29](#bib.bib29), [22](#bib.bib22)]. However, in large language models (LLMs),
    quantization presents unique challenges due to the presence of numerous outliers.
    These outliers dominate the quantization range, leaving only a few effective bits
    for the majority of values. Various strategies have been proposed to address the
    difficulties in LLM quantization. These include separating outliers and using
    mixed precision [[11](#bib.bib11), [42](#bib.bib42), [18](#bib.bib18)], employing
    Hessian-based methods to mitigate quantization difficulty [[14](#bib.bib14)],
    trading outliers between weights and activations [[43](#bib.bib43), [23](#bib.bib23),
    [24](#bib.bib24)] utilizing weight equalization [[29](#bib.bib29)], and even suggesting
    architectural modifications to handle outliers during pre-training[[44](#bib.bib44)].
    Recently two QuIP papers [[7](#bib.bib7), [41](#bib.bib41)] introduce the incoherence
    processing using random rotation matrices and applying vector quantization on
    the weights for compression. This does introduce extra overhead and imposes some
    constraints on the devices the LLM is deployed to in the availability of vector
    quantization kernels. Ashkboos et al. [[5](#bib.bib5)] recently released their
    method QuaRot. Our work was done concurrently with theirs. The authors also introduce
    random rotation matrices in a similar fashion as our paper. However, there are
    some notable differences. (1) Instead of relying on random matrices, we learn
    the rotation matrices. As seen in section [2.2](#S2.SS2 "2.2 Random rotations
    produce large variance ‣ 2 Motivation and Preliminaries ‣ ​SpinQuant: LLM Quantization
    with Learned Rotations​"), if accidentally randomized to a poor random matrix
    choice could be quite detrimental to performance. (2) We introduce less extra
    parameters in our method with the rotation matrix inside of each block as in section
    [3.1](#S3.SS1 "3.1 Rotation parameterization ‣ 3 Method ‣ ​SpinQuant: LLM Quantization
    with Learned Rotations​"). (3) Our pipeline gives significantly better performance
    compared to QuaRot, as demonstrated in Section [4](#S4 "4 Experiments ‣ ​SpinQuant:
    LLM Quantization with Learned Rotations​").'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '量化 神经网络量化已被证明是模型大小压缩和存储减少的有效工具[[30](#bib.bib30), [19](#bib.bib19), [29](#bib.bib29),
    [22](#bib.bib22)]。然而，在大型语言模型（LLMs）中，由于存在大量异常值，量化面临独特的挑战。这些异常值主导了量化范围，仅为大多数值留下了少数有效位。为了解决LLM量化中的困难，提出了各种策略。这些策略包括分离异常值和使用混合精度[[11](#bib.bib11),
    [42](#bib.bib42), [18](#bib.bib18)]，采用基于Hessian的方法来减轻量化难度[[14](#bib.bib14)]，在权重和激活之间交换异常值[[43](#bib.bib43),
    [23](#bib.bib23), [24](#bib.bib24)]，利用权重量化[[29](#bib.bib29)]，甚至建议在预训练期间对异常值进行结构调整[[44](#bib.bib44)]。最近，两篇QuIP论文[[7](#bib.bib7),
    [41](#bib.bib41)]介绍了使用随机旋转矩阵进行不一致处理，并在权重上应用向量量化以进行压缩。这确实引入了额外的开销，并对LLM部署设备的向量量化内核的可用性施加了一些限制。Ashkboos等人[[5](#bib.bib5)]最近发布了他们的方法QuaRot。我们的工作与他们同时进行。作者还以类似于我们论文的方式引入了随机旋转矩阵。然而，也有一些显著的区别。(1)
    我们不是依赖随机矩阵，而是学习旋转矩阵。如第[2.2节](#S2.SS2 "2.2 Random rotations produce large variance
    ‣ 2 Motivation and Preliminaries ‣ ​SpinQuant: LLM Quantization with Learned Rotations​")所示，如果不小心随机选择了一个不好的随机矩阵，可能会对性能造成相当大的损害。(2)
    我们的方法在每个块内部引入的额外参数更少，如第[3.1节](#S3.SS1 "3.1 Rotation parameterization ‣ 3 Method
    ‣ ​SpinQuant: LLM Quantization with Learned Rotations​")所示。(3) 我们的流程相比QuaRot表现出显著更好的性能，如第[4节](#S4
    "4 Experiments ‣ ​SpinQuant: LLM Quantization with Learned Rotations​")所示。'
- en: Optimization in orthonormal space The optimization of rotation matrices is carried
    out within the Stiefel Manifold [[16](#bib.bib16)], which encompasses all orthonormal
    matrices. Optimization while staying on this manifold can be done by e.g., parameterizing
    a skew-symmetric matrix and applying the Cayley transformation on top of it [[31](#bib.bib31)],
    or using a matrix exponential [[1](#bib.bib1), [20](#bib.bib20)]. However, these
    methods rely on expensive inverse or matrix-exponential functions that are applied
    every iteration. Instead, we follow the more efficient method named Cayley SGD [[21](#bib.bib21)],
    which can be applied to optimize a rotation matrix $R$ for arbitrary loss functions
    efficiently. Cayley SGD relies on an iterative approximation of the Cayley Transform
    that is conducted solely with matrix multiplications.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正交空间中的优化 旋转矩阵的优化是在 Stiefel 流形 [[16](#bib.bib16)] 中进行的，该流形包含所有正交矩阵。在这个流形上进行优化可以通过例如，对一个反对称矩阵进行参数化，并在其上应用
    Cayley 变换 [[31](#bib.bib31)]，或使用矩阵指数 [[1](#bib.bib1), [20](#bib.bib20)]。然而，这些方法依赖于每次迭代都要应用的昂贵的逆矩阵或矩阵指数函数。相反，我们采用了名为
    Cayley SGD [[21](#bib.bib21)] 的更高效方法，该方法可以有效地优化任意损失函数的旋转矩阵 $R$。Cayley SGD 依赖于仅通过矩阵乘法进行的
    Cayley 变换的迭代逼近。
- en: 6 Conclusions
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we present SpinQuant, a novel quantization technique that effectively
    bridges the performance gap between full precision and 4-bit weight, activation,
    and kv-cache quantization, for the LLaMA-2 7B model to within a mere 2.9 points.
    At its core, SpinQuant leverages the rotation invariance property of LLM models
    to insert rotation matrices that diminish outliers in the weights and intermediate
    activations while maintaining the network’s full-precision output numerically
    identical. Additionally, SpinQuant incorporates Cayley SGD for optimizing rotation
    matrices to boost quantization performance further. Importantly, our method is
    compatible with more advanced weight quantization techniques (e.g., GPTQ) and
    demonstrates state-of-the-art performance.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了 SpinQuant，这是一种新颖的量化技术，能够有效缩小 LLaMA-2 7B 模型在全精度和 4 位权重、激活和 kv-cache
    量化之间的性能差距至仅 2.9 个点。SpinQuant 的核心利用了 LLM 模型的旋转不变性，通过插入旋转矩阵来减少权重和中间激活中的异常值，同时保持网络的全精度输出在数值上完全一致。此外，SpinQuant
    还结合了 Cayley SGD 来优化旋转矩阵，以进一步提升量化性能。重要的是，我们的方法与更先进的权重量化技术（如 GPTQ）兼容，并展现了最先进的性能。
- en: 7 Limitations and Broader Impacts
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制与更广泛的影响
- en: In this study, we introduce a rotation-based quantization technique designed
    for precise low-bit quantization of LLMs. This method has the potential to reduce
    energy consumption during LLM inference. We evaluated our model’s performance
    in an academic context. However, real-world tasks may involve different training
    data and activation distributions, suggesting that the model’s generalizability
    to real-world scenarios warrants further investigation.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们介绍了一种基于旋转的量化技术，旨在实现 LLMs 的精确低位量化。该方法有潜力在 LLM 推理过程中减少能源消耗。我们在学术环境中评估了我们模型的性能。然而，实际任务可能涉及不同的训练数据和激活分布，表明模型对实际场景的泛化能力需要进一步研究。
- en: References
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Absil and Malick [2012] P-A Absil and Jérôme Malick. Projection-like retractions
    on matrix manifolds. *SIAM Journal on Optimization*, 22(1):135–158, 2012.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Absil 和 Malick [2012] P-A Absil 和 Jérôme Malick。矩阵流形上的投影类回撤。*SIAM 优化杂志*，22(1):135–158，2012。
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等 [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等. Gpt-4 技术报告。*arXiv 预印本 arXiv:2303.08774*，2023。
- en: AI@Meta [2024] AI@Meta. Llama 3 model card. 2024. URL [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI@Meta [2024] AI@Meta。Llama 3 模型卡。2024。网址 [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)。
- en: 'Ashkboos et al. [2023a] Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari
    do Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language
    models by deleting rows and columns. In *The Twelfth International Conference
    on Learning Representations*, 2023a.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashkboos 等 [2023a] Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento,
    Torsten Hoefler 和 James Hensman。Slicegpt：通过删除行和列压缩大型语言模型。发表于 *第十二届国际学习表征会议*，2023a。
- en: 'Ashkboos et al. [2023b] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L.
    Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman.
    Quarot: Outlier-free 4-bit inference in rotated llms. 2023b.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ashkboos 等人 [2023b] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci,
    Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler 和 James Hensman. Quarot: 无异常值的
    4 位旋转 LLM 推断。2023b。'
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439,
    2020.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk 等人 [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi 等人. Piqa:
    用自然语言推理物理常识。见 *人工智能协会会议论文集*，第 34 卷，第 7432–7439 页，2020。'
- en: 'Chee et al. [2024] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M
    De Sa. Quip: 2-bit quantization of large language models with guarantees. *Advances
    in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chee 等人 [2024] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov 和 Christopher M De
    Sa. Quip: 大型语言模型的 2 位量化及其保证。*神经信息处理系统的进展*，36，2024。'
- en: 'Clark et al. [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty
    of natural yes/no questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark 等人 [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins 和 Kristina Toutanova. Boolq: 探索自然是/否问题的意外难度。*arXiv 预印本 arXiv:1905.10044*，2019。'
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人 [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick 和 Oyvind Tafjord. 认为你已经解决了问答问题？试试 arc，AI2 推理挑战。*arXiv
    预印本 arXiv:1803.05457*，2018。
- en: 'Cox and Ooi [2023] Samuel Rhys Cox and Wei Tsang Ooi. Conversational interactions
    with npcs in llm-driven gaming: Guidelines from a content analysis of player feedback.
    In *International Workshop on Chatbot Research and Design*, pages 167–184\. Springer,
    2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cox 和 Ooi [2023] Samuel Rhys Cox 和 Wei Tsang Ooi. 基于 LLM 驱动的游戏中的 NPC 对话互动：来自玩家反馈内容分析的指南。见
    *国际聊天机器人研究与设计研讨会*，第 167–184 页。Springer，2023。
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm.int8(): 8-bit matrix multiplication for transformers at scale. 2022.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 [2022] Tim Dettmers, Mike Lewis, Younes Belkada 和 Luke Zettlemoyer.
    Llm.int8(): 大规模变换器的 8 位矩阵乘法。2022。'
- en: Egiazarian et al. [2024] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev,
    Elias Frantar, Artem Babenko, and Dan Alistarh. Extreme compression of large language
    models via additive quantization. *arXiv preprint arXiv:2401.06118*, 2024.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Egiazarian 等人 [2024] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias
    Frantar, Artem Babenko 和 Dan Alistarh. 通过加法量化对大型语言模型进行极限压缩。*arXiv 预印本 arXiv:2401.06118*，2024。
- en: Elhage et al. [2023] Nelson Elhage, Robert Lasenby, and Christopher Olah. Privileged
    bases in the transformer residual stream. *Transformer Circuits Thread*, 2023.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elhage 等人 [2023] Nelson Elhage, Robert Lasenby 和 Christopher Olah. 变换器残差流中的特权基础。*变换器电路线程*，2023。
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等人 [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh.
    Gptq: 生成预训练变换器的准确后训练量化。*arXiv 预印本 arXiv:2210.17323*，2022。'
- en: Huang et al. [2024] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao
    Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. How good
    are low-bit quantized llama3 models? an empirical study. *arXiv preprint arXiv:2404.14047*,
    2024.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2024] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv,
    Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu 和 Michele Magno. 低位量化的 llama3 模型表现如何？一项实证研究。*arXiv
    预印本 arXiv:2404.14047*，2024。
- en: James [1976] Ioan Mackenzie James. *The topology of Stiefel manifolds*, volume 24.
    Cambridge University Press, 1976.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: James [1976] Ioan Mackenzie James. *Stiefel 流形的拓扑*，第 24 卷。剑桥大学出版社，1976。
- en: Jiang et al. [2023] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*, 2023.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 [2023] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier 等人. Mistral 7b。*arXiv 预印本 arXiv:2310.06825*，2023。
- en: 'Kim et al. [2023] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. [2023] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, 和 Kurt Keutzer. Squeezellm：密集与稀疏量化。*arXiv 预印本
    arXiv:2306.07629*，2023年。
- en: 'Krishnamoorthi [2018] Raghuraman Krishnamoorthi. Quantizing deep convolutional
    networks for efficient inference: A whitepaper. *arXiv preprint arXiv:1806.08342*,
    2018.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishnamoorthi [2018] Raghuraman Krishnamoorthi. 量化深度卷积网络以实现高效推理：一份白皮书。*arXiv
    预印本 arXiv:1806.08342*，2018年。
- en: 'Lezcano-Casado and Martinez-Rubio [2019] Mario Lezcano-Casado and David Martinez-Rubio.
    Cheap orthogonal constraints in neural networks: A simple parametrization of the
    orthogonal and unitary group. 2019.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lezcano-Casado 和 Martinez-Rubio [2019] Mario Lezcano-Casado 和 David Martinez-Rubio.
    神经网络中的廉价正交约束：正交和单位群的简单参数化。2019年。
- en: Li et al. [2020] Jun Li, Li Fuxin, and Sinisa Todorovic. Efficient riemannian
    optimization on the stiefel manifold via the cayley transform. *arXiv preprint
    arXiv:2002.01113*, 2020.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2020] Jun Li, Li Fuxin, 和 Sinisa Todorovic. 通过 Cayley 变换在 Stiefel
    流形上的高效黎曼优化。*arXiv 预印本 arXiv:2002.01113*，2020年。
- en: 'Li et al. [2021] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization
    by block reconstruction. In *International Conference on Learning Representations
    (ICLR)*, 2021.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2021] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, 和 Shi Gu. Brecq：通过块重建突破后训练量化的极限。发表于 *国际学习表征大会（ICLR）*，2021年。
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. Awq：针对 LLM 压缩和加速的激活感知权重量化。*arXiv 预印本 arXiv:2306.00978*，2023年。
- en: 'Liu et al. [2023a] Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong,
    and Kwang-Ting Cheng. Llm-fp4: 4-bit floating-point quantized transformers. *arXiv
    preprint arXiv:2310.16836*, 2023a.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023a] Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, 和
    Kwang-Ting Cheng. Llm-fp4：4位浮点量化变换器。*arXiv 预印本 arXiv:2310.16836*，2023年。
- en: 'Liu et al. [2023b] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models. *arXiv
    preprint arXiv:2305.17888*, 2023b.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023b] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra.
    Llm-qat：大语言模型的数据无关量化感知训练。*arXiv 预印本 arXiv:2305.17888*，2023年。
- en: 'Liu et al. [2024] Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong
    Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi,
    et al. Mobilellm: Optimizing sub-billion parameter language models for on-device
    use cases. *arXiv preprint arXiv:2402.14905*, 2024.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2024] Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong
    Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi
    等人。Mobilellm：针对设备使用场景优化小于十亿参数的语言模型。*arXiv 预印本 arXiv:2402.14905*，2024年。
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher. 指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843*，2016年。
- en: Mihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. *arXiv preprint arXiv:1809.02789*, 2018.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, 和 Ashish Sabharwal.
    一套盔甲能导电吗？一个用于开放书籍问答的新数据集。*arXiv 预印本 arXiv:1809.02789*，2018年。
- en: Nagel et al. [2019] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max
    Welling. Data-free quantization through weight equalization and bias correction.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    pages 1325–1334, 2019.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel et al. [2019] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, 和 Max
    Welling. 通过权重均衡和偏置修正实现无数据量化。发表于 *IEEE/CVF 国际计算机视觉大会论文集*，第1325–1334页，2019年。
- en: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning (ICML)*, 2020.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, 和 Tijmen Blankevoort. 上还是下？后训练量化的自适应舍入。发表于 *国际机器学习大会（ICML）*，2020年。
- en: Nishimori and Akaho [2005] Yasunori Nishimori and Shotaro Akaho. Learning algorithms
    utilizing quasi-geodesic flows on the stiefel manifold. *Neurocomputing*, 67:106–135,
    2005.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nishimori and Akaho [2005] Yasunori Nishimori 和 Shotaro Akaho. 利用 Stiefel 流形上的准测地流的学习算法。*Neurocomputing*，67:106–135，2005年。
- en: 'Roziere et al. [2023] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. Code llama: Open foundation models for code. *arXiv preprint arXiv:2308.12950*,
    2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roziere et al. [2023] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, 等等。Code llama: 开放的代码基础模型。*arXiv 预印本 arXiv:2308.12950*，2023年。'
- en: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    和 Yejin Choi. Winogrande: 大规模对抗性 Winograd 模式挑战。*Communications of the ACM*，64(9):99–106，2021年。'
- en: 'Sap et al. [2019] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and
    Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. *arXiv
    preprint arXiv:1904.09728*, 2019.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sap et al. [2019] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, 和
    Yejin Choi. Socialiqa: 关于社会互动的常识推理。*arXiv 预印本 arXiv:1904.09728*，2019年。'
- en: 'Shao et al. [2023] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. *arXiv preprint arXiv:2308.13137*,
    2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shao et al. [2023] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, 和 Ping Luo. Omniquant: 面向大型语言模型的全方向校准量化。*arXiv
    预印本 arXiv:2308.13137*，2023年。'
- en: 'Su et al. [2021] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu.
    Roformer: Enhanced transformer with rotary position embedding, 2021.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Su et al. [2021] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, 和 Yunfeng Liu. Roformer:
    使用旋转位置嵌入的增强型 Transformer，2021年。'
- en: 'Team et al. [2023] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv
    preprint arXiv:2312.11805*, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team et al. [2023] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, 等等。Gemini: 一系列高性能的多模态模型。*arXiv 预印本 arXiv:2312.11805*，2023年。'
- en: Thirunavukarasu et al. [2023] Arun James Thirunavukarasu, Darren Shu Jeng Ting,
    Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large
    language models in medicine. *Nature medicine*, 29(8):1930–1940, 2023.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thirunavukarasu et al. [2023] Arun James Thirunavukarasu, Darren Shu Jeng Ting,
    Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, 和 Daniel Shu Wei Ting. 医学中的大型语言模型。*Nature
    medicine*，29(8):1930–1940，2023年。
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等等。Llama: 开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023a年。'
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等等。Llama 2: 开放的基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023b年。'
- en: 'Tseng et al. [2024] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov,
    and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence
    and lattice codebooks. *arXiv preprint arXiv:2402.04396*, 2024.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tseng et al. [2024] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov,
    和 Christopher De Sa. Quip#: 更优的 LLM 量化方法，利用 Hadamard 不连贯性和格子代码簿。*arXiv 预印本 arXiv:2402.04396*，2024年。'
- en: 'van Baalen et al. [2023] Mart van Baalen, Markus Nagel Andrey Kuzmin, Peter
    Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, and Paul Whatmough.
    Gptvq: The blessing of dimensionality for llm quantization. 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'van Baalen et al. [2023] Mart van Baalen, Markus Nagel, Andrey Kuzmin, Peter
    Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, 和 Paul Whatmough.
    Gptvq: 维度的祝福用于 LLM 量化。2023年。'
- en: 'Xiao et al. [2022] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *CVPR*, 2022.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. [2022] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    和 Song Han. Smoothquant: 大型语言模型的准确且高效的后训练量化。发表于 *CVPR*，2022年。'
- en: 'Yelysei Bondarenko [2023] Tijmen Blankevoort Yelysei Bondarenko, Markus Nagel.
    Quantizable transformers: Removing outliers by helping attention heads do nothing.
    2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yelysei Bondarenko [2023] Tijmen Blankevoort Yelysei Bondarenko, Markus Nagel。可量化的变换器：通过帮助注意力头无所作为来去除离群值。2023年。
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi
    和 Yejin Choi。Hellaswag: 机器真的能完成你的句子吗？*arXiv 预印本 arXiv:1905.07830*，2019年。'
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin
    等。Opt: 开放的预训练变换器语言模型。*arXiv 预印本 arXiv:2205.01068*，2022年。'
- en: 'Zhao et al. [2023] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen,
    Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom:
    Low-bit quantization for efficient and accurate llm serving. *arXiv preprint arXiv:2310.19102*,
    2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. [2023] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen,
    Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen 和 Baris Kasikci。Atom:
    低位量化以实现高效和准确的 LLM 服务。*arXiv 预印本 arXiv:2310.19102*，2023年。'
- en: Appendix A Appendix / supplemental material
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录 / 补充材料
- en: A.1 Complete results of main result table
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 主结果表的完整结果
- en: 'In Tables [A.1](#A1.SS1 "A.1 Complete results of main result table ‣ Appendix
    A Appendix / supplemental material ‣ 7 Limitations and Broader Impacts ‣ 6 Conclusions
    ‣ 5 Related Work ‣ 4.3.3 Rotation type ‣ 4.3 Ablation studies ‣ 4.2 Main results
    ‣ 4 Experiments ‣ ​SpinQuant: LLM Quantization with Learned Rotations​") and  [A.1](#A1.SS1
    "A.1 Complete results of main result table ‣ Appendix A Appendix / supplemental
    material ‣ 7 Limitations and Broader Impacts ‣ 6 Conclusions ‣ 5 Related Work
    ‣ 4.3.3 Rotation type ‣ 4.3 Ablation studies ‣ 4.2 Main results ‣ 4 Experiments
    ‣ ​SpinQuant: LLM Quantization with Learned Rotations​"), we show the complete
    results of Table [4.2](#S4.SS2 "4.2 Main results ‣ 4 Experiments ‣ ​SpinQuant:
    LLM Quantization with Learned Rotations​"). We compare the accuracy on eight zero-shot
    commonsense reasoning tasks including ARC-easy, ARC-challenge [[9](#bib.bib9)],
    BoolQ [[8](#bib.bib8)], PIQA [[6](#bib.bib6)], SIQA [[34](#bib.bib34)], HellaSwag [[45](#bib.bib45)],
    OBQA [[28](#bib.bib28)], and WinoGrande [[33](#bib.bib33)]. We compare our results
    with previous works including SmoothQuant[[43](#bib.bib43)], LLM-QAT[[25](#bib.bib25)],
    GPTQ [[14](#bib.bib14)], OmniQuant [[35](#bib.bib35)], AQLM [[12](#bib.bib12)],
    ATOM [[47](#bib.bib47)], AWQ [[23](#bib.bib23)], QuIP [[7](#bib.bib7)], QuIP# [[41](#bib.bib41)],
    and QuaRot [[5](#bib.bib5)].'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [A.1](#A1.SS1 "A.1 主结果表的完整结果 ‣ 附录 A 附录 / 补充材料 ‣ 7 限制与更广泛影响 ‣ 6 结论 ‣ 5 相关工作
    ‣ 4.3.3 旋转类型 ‣ 4.3 消融研究 ‣ 4.2 主要结果 ‣ 4 实验 ‣ ​SpinQuant: 基于学习的 LLM 量化​") 和  [A.1](#A1.SS1
    "A.1 主结果表的完整结果 ‣ 附录 A 附录 / 补充材料 ‣ 7 限制与更广泛影响 ‣ 6 结论 ‣ 5 相关工作 ‣ 4.3.3 旋转类型 ‣ 4.3
    消融研究 ‣ 4.2 主要结果 ‣ 4 实验 ‣ ​SpinQuant: 基于学习的 LLM 量化​") 中，我们展示了表格 [4.2](#S4.SS2 "4.2
    主要结果 ‣ 4 实验 ‣ ​SpinQuant: 基于学习的 LLM 量化​") 的完整结果。我们比较了八个零样本常识推理任务的准确性，包括 ARC-easy、ARC-challenge [[9](#bib.bib9)]、BoolQ [[8](#bib.bib8)]、PIQA [[6](#bib.bib6)]、SIQA [[34](#bib.bib34)]、HellaSwag [[45](#bib.bib45)]、OBQA [[28](#bib.bib28)]
    和 WinoGrande [[33](#bib.bib33)]。我们将我们的结果与包括 SmoothQuant[[43](#bib.bib43)]、LLM-QAT[[25](#bib.bib25)]、GPTQ [[14](#bib.bib14)]、OmniQuant [[35](#bib.bib35)]、AQLM [[12](#bib.bib12)]、ATOM [[47](#bib.bib47)]、AWQ [[23](#bib.bib23)]、QuIP [[7](#bib.bib7)]、QuIP# [[41](#bib.bib41)]
    和 QuaRot [[5](#bib.bib5)] 等先前的研究进行比较。'
- en: 'Table 5: Complete omparison of the perplexity score on WikiText2 and averaged
    accuracy on Zero-shot Common Sense Reasoning tasks on LLaMA-3. We reported the
    mean and standard deviation across six trails for SpinQuant as well as our reproduced
    results of GPTQ and QuaRot.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：在 WikiText2 上的困惑度得分和 LLaMA-3 上的零样本常识推理任务的平均准确度的完整比较。我们报告了 SpinQuant 的六次试验的均值和标准差，以及我们复现的
    GPTQ 和 QuaRot 的结果。
- en: '| Model | #Bits | Method | ARC-e | ARC-c | BoolQ | PIQA | SIQA | HellaS. |
    OBQA | WinoG. | Avg. | Wiki2 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 位数 | 方法 | ARC-e | ARC-c | BoolQ | PIQA | SIQA | HellaS. | OBQA | WinoG.
    | 平均 | Wiki2 |'
- en: '| W-A-KV | ($\uparrow$) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| W-A-KV | ($\uparrow$) |'
- en: '| 8B | 16-16-16 | Full Precision | 77.6 | 57.7 | 83.3 | 80.7 | 48.7 | 79.6
    | 55.8 | 73.7 | 69.6 | 6.1 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 8B | 16-16-16 | 全精度 | 77.6 | 57.7 | 83.3 | 80.7 | 48.7 | 79.6 | 55.8 | 73.7
    | 69.6 | 6.1 |'
- en: '| \cdashline2-13 | 4-16-16 | RTN | 74.7 | 49.0 | 73.0 | 77.0 | 47.6 | 76.6
    | 53.4 | 71.4 | 65.4 | 7.8 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-16-16 | RTN | 74.7 | 49.0 | 73.0 | 77.0 | 47.6 | 76.6
    | 53.4 | 71.4 | 65.4 | 7.8 |'
- en: '|  |  | SmoothQuant | 67.6 | 41.3 | 72.6 | 74.4 | 46.7 | 70.6 | 48.0 | 67.0
    | 61.0 | 10.7 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 67.6 | 41.3 | 72.6 | 74.4 | 46.7 | 70.6 | 48.0 | 67.0
    | 61.0 | 10.7 |'
- en: '|  |  | LLM-QAT | 77.1 | 53.0 | 82.4 | 79.0 | 48.1 | 76.6 | 54.4 | 71.3 | 67.7
    | 7.1 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLM-QAT | 77.1 | 53.0 | 82.4 | 79.0 | 48.1 | 76.6 | 54.4 | 71.3 | 67.7
    | 7.1 |'
- en: '|  |  | GPTQ | 73.5 ${}_{\pm 1.9}$ |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 73.5 ${}_{\pm 1.9}$ |'
- en: '|  |  | QuaRot* | 73.8 ${}_{\pm 1.4}$ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 73.8 ${}_{\pm 1.4}$ |'
- en: '|  |  | QuaRot | 77.0 ${}_{\pm 0.8}$ |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 77.0 ${}_{\pm 0.8}$ |'
- en: '|  |  | SpinQuant* | 76.5 ${}_{\pm 1.1}$ |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 76.5 ${}_{\pm 1.1}$ |'
- en: '|  |  | SpinQuant | 77.6 ${}_{\pm 0.7}$ |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 77.6 ${}_{\pm 0.7}$ |'
- en: '| \cdashline2-13 | 4-4-16 | RTN | 31.8 | 27.6 | 47.2 | 53.8 | 39.7 | 30.8 |
    28.2 | 48.9 | 38.5 | 923.9 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-4-16 | RTN | 31.8 | 27.6 | 47.2 | 53.8 | 39.7 | 30.8 |
    28.2 | 48.9 | 38.5 | 923.9 |'
- en: '|  |  | SmoothQuant | 36.3 | 26.3 | 50.6 | 54.1 | 40.3 | 31.4 | 30.6 | 52.9
    | 40.3 | 867.5 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 36.3 | 26.3 | 50.6 | 54.1 | 40.3 | 31.4 | 30.6 | 52.9
    | 40.3 | 867.5 |'
- en: '|  |  | LLM-QAT | 44.1 | 29.7 | 58.0 | 61.5 | 42.1 | 39.9 | 33.0 | 51.3 | 44.9
    | 42.9 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLM-QAT | 44.1 | 29.7 | 58.0 | 61.5 | 42.1 | 39.9 | 33.0 | 51.3 | 44.9
    | 42.9 |'
- en: '|  |  | GPTQ | 31.4 ${}_{\pm 0.9}$ | 955.9 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 31.4 ${}_{\pm 0.9}$ | 955.9 |'
- en: '|  |  | QuaRot* | 66.0 ${}_{\pm 1.2}$ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 66.0 ${}_{\pm 1.2}$ |'
- en: '|  |  | QuaRot | 72.4 ${}_{\pm 1.1}$ |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 72.4 ${}_{\pm 1.1}$ |'
- en: '|  |  | SpinQuant* | 74.1 ${}_{\pm 1.6}$ |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 74.1 ${}_{\pm 1.6}$ |'
- en: '|  |  | SpinQuant | 75.0 ${}_{\pm 1.0}$ |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 75.0 ${}_{\pm 1.0}$ |'
- en: '| \cdashline2-13 | 4-4-4 | RTN | 31.9 | 26.1 | 46.2 | 52.3 | 39.9 | 29.9 |
    28.6 | 51.0 | 38.2 | 1,118.5 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-4-4 | RTN | 31.9 | 26.1 | 46.2 | 52.3 | 39.9 | 29.9 |
    28.6 | 51.0 | 38.2 | 1,118.5 |'
- en: '|  |  | SmoothQuant | 33.5 | 25.1 | 49.6 | 53.1 | 40.3 | 28.8 | 29.6 | 49.6
    | 38.7 | 1,530.5 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 33.5 | 25.1 | 49.6 | 53.1 | 40.3 | 28.8 | 29.6 | 49.6
    | 38.7 | 1,530.5 |'
- en: '|  |  | LLM-QAT | 40.5 | 26.6 | 52.7 | 59.9 | 42.3 | 37.5 | 33.6 | 52.7 | 43.2
    | 52.5 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLM-QAT | 40.5 | 26.6 | 52.7 | 59.9 | 42.3 | 37.5 | 33.6 | 52.7 | 43.2
    | 52.5 |'
- en: '|  |  | GPTQ | 31.0 ${}_{\pm 0.9}$ | 1,071.7 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 31.0 ${}_{\pm 0.9}$ | 1,071.7 |'
- en: '|  |  | QuaRot* | 65.9 ${}_{\pm 3.0}$ |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 65.9 ${}_{\pm 3.0}$ |'
- en: '|  |  | QuaRot | 71.6 ${}_{\pm 0.9}$ |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 71.6 ${}_{\pm 0.9}$ |'
- en: '|  |  | SpinQuant* | 72.6 ${}_{\pm 1.4}$ |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 72.6 ${}_{\pm 1.4}$ |'
- en: '|  |  | SpinQuant | 74.4 ${}_{\pm 1.3}$ |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 74.4 ${}_{\pm 1.3}$ |'
- en: '| 70B | 16-16-16 | Full Precision | 80.6 | 64.5 | 87.4 | 83.7 | 51.7 | 85.3
    | 62.0 | 80.5 | 74.5 | 2.8 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 70B | 16-16-16 | Full Precision | 80.6 | 64.5 | 87.4 | 83.7 | 51.7 | 85.3
    | 62.0 | 80.5 | 74.5 | 2.8 |'
- en: '| \cdashline2-13 | 4-16-16 | RTN | 27.3 | 27.2 | 37.8 | 51.0 | 39.1 | 25.6
    | 26.2 | 49.8 | 35.5 | 1e5 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-16-16 | RTN | 27.3 | 27.2 | 37.8 | 51.0 | 39.1 | 25.6
    | 26.2 | 49.8 | 35.5 | 1e5 |'
- en: '|  |  | SmoothQuant | 76.7 | 45.5 | 80.6 | 81.2 | 48.7 | 81.1 | 46.2 | 75.5
    | 66.9 | 12.0 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 76.7 | 45.5 | 80.6 | 81.2 | 48.7 | 81.1 | 46.2 | 75.5
    | 66.9 | 12.0 |'
- en: '|  |  | GPTQ | 28.2 ± 1.3 | 24.7 ± 1.6 | 37.9 ± 0.1 | 50.7 ± 0.8 | 39.0 ± 0.6
    | 26.8 ± 1.7 | 27.7 ± 3.6 | 50.5 ± 0.6 | 35.7 ± 0.6 | 1e5 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 28.2 ± 1.3 | 24.7 ± 1.6 | 37.9 ± 0.1 | 50.7 ± 0.8 | 39.0 ± 0.6
    | 26.8 ± 1.7 | 27.7 ± 3.6 | 50.5 ± 0.6 | 35.7 ± 0.6 | 1e5 |'
- en: '|  |  | QuaRot* | 65.9 ± 1.4 | 44.3 ± 2.7 | 67.0 ± 4.9 | 75.2 ± 2.0 | 44.1
    ± 2.1 | 59.8 ± 8.6 | 42.5 ± 3.8 | 58.5 ± 2.9 | 57.2 ± 2.7 | 41.6 ± 16.76 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 65.9 ± 1.4 | 44.3 ± 2.7 | 67.0 ± 4.9 | 75.2 ± 2.0 | 44.1
    ± 2.1 | 59.8 ± 8.6 | 42.5 ± 3.8 | 58.5 ± 2.9 | 57.2 ± 2.7 | 41.6 ± 16.76 |'
- en: '|  |  | QuaRot | 74.4 ± 0.7 | 58.6 ± 2.6 | 86.4 ± 0.5 | 83.8 ± 0.4 | 51.9 ±
    0.4 | 83.7 ± 0.1 | 47.7 ± 2.2 | 76.1 ± 0.6 | 70.3 ± 0.7 | 7.9 ± 0.21 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 74.4 ± 0.7 | 58.6 ± 2.6 | 86.4 ± 0.5 | 83.8 ± 0.4 | 51.9 ±
    0.4 | 83.7 ± 0.1 | 47.7 ± 2.2 | 76.1 ± 0.6 | 70.3 ± 0.7 | 7.9 ± 0.21 |'
- en: '|  |  | SpinQuant* | 78.5 ± 2.2 | 57.9 ± 1.5 | 84.5 ± 1.0 | 82.3 ± 0.6 | 50.3
    ± 0.6 | 82.6 ± 0.4 | 57.4 ± 5.9 | 77.5 ± 1.9 | 71.4 ± 1.5 | 3.9 ± 0.47 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 78.5 ± 2.2 | 57.9 ± 1.5 | 84.5 ± 1.0 | 82.3 ± 0.6 | 50.3
    ± 0.6 | 82.6 ± 0.4 | 57.4 ± 5.9 | 77.5 ± 1.9 | 71.4 ± 1.5 | 3.9 ± 0.47 |'
- en: '|  |  | SpinQuant | 78.0 ± 2.7 | 59.1 ± 0.9 | 85.2 ± 1.1 | 82.8 ± 1.0 | 50.6
    ± 0.6 | 83.5 ± 0.2 | 54.8 ± 6.6 | 78.5 ± 1.6 | 71.6 ± 1.1 | 4.8 ± 1.97 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 78.0 ± 2.7 | 59.1 ± 0.9 | 85.2 ± 1.1 | 82.8 ± 1.0 | 50.6
    ± 0.6 | 83.5 ± 0.2 | 54.8 ± 6.6 | 78.5 ± 1.6 | 71.6 ± 1.1 | 4.8 ± 1.97 |'
- en: '| \cdashline2-13 | 4-4-16 | RTN | 27.6 | 27.0 | 38.2 | 50.1 | 38.5 | 26.0 |
    28.4 | 49.1 | 35.6 | 1e5 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-4-16 | RTN | 27.6 | 27.0 | 38.2 | 50.1 | 38.5 | 26.0 |
    28.4 | 49.1 | 35.6 | 1e5 |'
- en: '|  |  | SmoothQuant | 59.5 | 35.7 | 62.4 | 70.3 | 44.3 | 61.7 | 44.2 | 63.9
    | 55.3 | 18.0 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 59.5 | 35.7 | 62.4 | 70.3 | 44.3 | 61.7 | 44.2 | 63.9
    | 55.3 | 18.0 |'
- en: '|  |  | GPTQ | 27.0 ± 0.4 | 26.1 ± 1.5 | 39.1 ± 1.2 | 50.4 ± 0.4 | 38.9 ± 0.4
    | 25.7 ± 0.2 | 25.7 ± 1.7 | 49.6 ± 0.9 | 35.3 ± 0.2 | 1e5 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 27.0 ± 0.4 | 26.1 ± 1.5 | 39.1 ± 1.2 | 50.4 ± 0.4 | 38.9 ± 0.4
    | 25.7 ± 0.2 | 25.7 ± 1.7 | 49.6 ± 0.9 | 35.3 ± 0.2 | 1e5 |'
- en: '|  |  | QuaRot* | 40.8 ± 4.8 | 26.2 ± 2.8 | 52.4 ± 3.8 | 58.4 ± 3.5 | 40.0
    ± 0.9 | 33.8 ± 3.9 | 30.0 ± 3.6 | 50.2 ± 1.7 | 41.5 ± 2.5 | 91.2 ± 24.05 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 40.8 ± 4.8 | 26.2 ± 2.8 | 52.4 ± 3.8 | 58.4 ± 3.5 | 40.0
    ± 0.9 | 33.8 ± 3.9 | 30.0 ± 3.6 | 50.2 ± 1.7 | 41.5 ± 2.5 | 91.2 ± 24.05 |'
- en: '|  |  | QuaRot | 72.4 ± 1.5 | 52.2 ± 1.6 | 78.5 ± 2.4 | 78.9 ± 0.8 | 49.0 ±
    1.1 | 78.5 ± 0.9 | 45.2 ± 1.9 | 68.2 ± 3.0 | 65.4 ± 1.3 | 20.4 ± 3.23 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 72.4 ± 1.5 | 52.2 ± 1.6 | 78.5 ± 2.4 | 78.9 ± 0.8 | 49.0 ±
    1.1 | 78.5 ± 0.9 | 45.2 ± 1.9 | 68.2 ± 3.0 | 65.4 ± 1.3 | 20.4 ± 3.23 |'
- en: '|  |  | SpinQuant* | 77.2 ± 0.9 | 55.9 ± 1.1 | 81.7 ± 1.7 | 80.9 ± 0.6 | 49.0
    ± 0.5 | 80.9 ± 0.4 | 58.7 ± 1.9 | 76.2 ± 0.8 | 70.1 ± 0.4 | 4.1 ± 0.02 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 77.2 ± 0.9 | 55.9 ± 1.1 | 81.7 ± 1.7 | 80.9 ± 0.6 | 49.0
    ± 0.5 | 80.9 ± 0.4 | 58.7 ± 1.9 | 76.2 ± 0.8 | 70.1 ± 0.4 | 4.1 ± 0.02 |'
- en: '|  |  | SpinQuant | 76.7 ± 1.9 | 55.6 ± 2.1 | 82.3 ± 1.3 | 80.6 ± 1.1 | 49.8
    ± 0.9 | 81.0 ± 1.6 | 55.4 ± 6.3 | 74.5 ± 3.9 | 69.5 ± 2.1 | 5.5 ± 2.56 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 76.7 ± 1.9 | 55.6 ± 2.1 | 82.3 ± 1.3 | 80.6 ± 1.1 | 49.8
    ± 0.9 | 81.0 ± 1.6 | 55.4 ± 6.3 | 74.5 ± 3.9 | 69.5 ± 2.1 | 5.5 ± 2.56 |'
- en: '| \cdashline2-13 | 4-4-4 | RTN | 27.0 | 24.1 | 38.5 | 50.4 | 38.8 | 25.8 |
    25.2 | 51.8 | 35.2 | 1e5 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-4-4 | RTN | 27.0 | 24.1 | 38.5 | 50.4 | 38.8 | 25.8 |
    25.2 | 51.8 | 35.2 | 1e5 |'
- en: '|  |  | SmoothQuant | 55.0 | 34.9 | 62.2 | 66.8 | 43.1 | 59.4 | 39.8 | 58.0
    | 52.4 | 22.1 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 55.0 | 34.9 | 62.2 | 66.8 | 43.1 | 59.4 | 39.8 | 58.0
    | 52.4 | 22.1 |'
- en: '|  |  | GPTQ | 27.1 ± 0.3 | 24.8 ± 1.7 | 38.8 ± 0.7 | 50.8 ± 0.6 | 39.0 ± 0.4
    | 25.5 ± 0.2 | 24.8 ± 2.9 | 49.8 ± 0.6 | 35.1 ± 0.2 | 1e5 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 27.1 ± 0.3 | 24.8 ± 1.7 | 38.8 ± 0.7 | 50.8 ± 0.6 | 39.0 ± 0.4
    | 25.5 ± 0.2 | 24.8 ± 2.9 | 49.8 ± 0.6 | 35.1 ± 0.2 | 1e5 |'
- en: '|  |  | QuaRot* | 40.5 ± 4.9 | 25.7 ± 3.7 | 50.9 ± 4.5 | 57.7 ± 3.3 | 39.9
    ± 0.8 | 33.9 ± 3.9 | 31.0 ± 2.6 | 51.0 ± 1.6 | 41.3 ± 2.6 | 92.4 ± 24.18 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 40.5 ± 4.9 | 25.7 ± 3.7 | 50.9 ± 4.5 | 57.7 ± 3.3 | 39.9
    ± 0.8 | 33.9 ± 3.9 | 31.0 ± 2.6 | 51.0 ± 1.6 | 41.3 ± 2.6 | 92.4 ± 24.18 |'
- en: '|  |  | QuaRot | 72.3 ± 1.9 | 51.6 ± 1.0 | 77.5 ± 2.0 | 78.9 ± 1.1 | 49.0 ±
    1.0 | 78.2 ± 0.9 | 45.5 ± 1.7 | 67.8 ± 2.6 | 65.1 ± 1.1 | 20.2 ± 3.12 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 72.3 ± 1.9 | 51.6 ± 1.0 | 77.5 ± 2.0 | 78.9 ± 1.1 | 49.0 ±
    1.0 | 78.2 ± 0.9 | 45.5 ± 1.7 | 67.8 ± 2.6 | 65.1 ± 1.1 | 20.2 ± 3.12 |'
- en: '|  |  | SpinQuant* | 77.3 ± 1.0 | 56.0 ± 1.1 | 81.8 ± 1.7 | 80.8 ± 0.4 | 49.3
    ± 0.5 | 80.9 ± 0.3 | 58.7 ± 0.7 | 76.4 ± 0.3 | 70.1 ± 0.5 | 4.1 ± 0.01 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 77.3 ± 1.0 | 56.0 ± 1.1 | 81.8 ± 1.7 | 80.8 ± 0.4 | 49.3
    ± 0.5 | 80.9 ± 0.3 | 58.7 ± 0.7 | 76.4 ± 0.3 | 70.1 ± 0.5 | 4.1 ± 0.01 |'
- en: '|  |  | SpinQuant | 76.8 ± 2.1 | 55.8 ± 2.6 | 82.2 ± 1.7 | 81.0 ± 0.8 | 49.5
    ± 1.0 | 81.2 ± 1.3 | 53.8 ± 6.3 | 74.2 ± 3.8 | 69.3 ± 2.2 | 5.5 ± 2.59 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 76.8 ± 2.1 | 55.8 ± 2.6 | 82.2 ± 1.7 | 81.0 ± 0.8 | 49.5
    ± 1.0 | 81.2 ± 1.3 | 53.8 ± 6.3 | 74.2 ± 3.8 | 69.3 ± 2.2 | 5.5 ± 2.59 |'
- en: 'Table 6: Complete omparison of the perplexity score on WikiText2 and averaged
    accuracy on Zero-shot Common Sense Reasoning tasks on LLaMA-2. We reported the
    mean and standard deviation across six trails for SpinQuant as well as our reproduced
    results of GPTQ and QuaRot.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 6: 对LLaMA-2上WikiText2的困惑度评分和零-shot常识推理任务的平均准确率进行了完整比较。我们报告了SpinQuant的六次试验的平均值和标准差，以及我们重现的GPTQ和QuaRot的结果。'
- en: '| Model | #Bits | Method | ARC-e | ARC-c | BoolQ | PIQA | SIQA | HellaS. |
    OBQA | WinoG. | Avg. | Wiki2 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Model | #Bits | Method | ARC-e | ARC-c | BoolQ | PIQA | SIQA | HellaS. |
    OBQA | WinoG. | Avg. | Wiki2 |'
- en: '| W-A-KV | ($\uparrow$) |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| W-A-KV | ($\uparrow$) |'
- en: '| 7B | 16-16-16 | Full Precision | 75.0 | 50.8 | 77.3 | 78.9 | 48.5 | 76.0
    | 59.3 | 69.5 | 66.9 | 5.5 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 7B | 16-16-16 | Full Precision | 75.0 | 50.8 | 77.3 | 78.9 | 48.5 | 76.0
    | 59.3 | 69.5 | 66.9 | 5.5 |'
- en: '| \cdashline2-13 | 4-16-16 | RTN | 71.3 | 46.0 | 73.5 | 76.9 | 47.2 | 72.5
    | 54.2 | 66.9 | 63.6 | 7.2 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-16-16 | RTN | 71.3 | 46.0 | 73.5 | 76.9 | 47.2 | 72.5
    | 54.2 | 66.9 | 63.6 | 7.2 |'
- en: '|  |  | SmoothQuant | 66.2 | 42.5 | 67.4 | 75.8 | 44.1 | 67.2 | 44.6 | 64.6
    | 59.1 | 7.5 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 66.2 | 42.5 | 67.4 | 75.8 | 44.1 | 67.2 | 44.6 | 64.6
    | 59.1 | 7.5 |'
- en: '|  |  | LLM-QAT | 73.3 | 48.6 | 73.2 | 78.2 | 48.8 | 73.6 | 55.0 | 68.4 | 64.9
    | 5.9 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLM-QAT | 73.3 | 48.6 | 73.2 | 78.2 | 48.8 | 73.6 | 55.0 | 68.4 | 64.9
    | 5.9 |'
- en: '|  |  | OmniQuant | 67.8 | 37.9 | – | 77.1 | – | – | – | 67 | – | 5.7 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  |  | OmniQuant | 67.8 | 37.9 | – | 77.1 | – | – | – | 67 | – | 5.7 |'
- en: '|  |  | AQLM | 68.9 | 40.3 | – | 77.7 | – | – | – | 67.3 | – | – |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  |  | AQLM | 68.9 | 40.3 | – | 77.7 | – | – | – | 67.3 | – | – |'
- en: '|  |  | QuIP# | 69.1 | 40.5 | – | 78.4 | – | – | – | 67.6 | – | – |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuIP# | 69.1 | 40.5 | – | 78.4 | – | – | – | 67.6 | – | – |'
- en: '|  |  | GPTQ | 72.6 ${}_{\pm 0.3}$ |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 72.6 ${}_{\pm 0.3}$ |'
- en: '|  |  | QuaRot* | 69.5 ${}_{\pm 1.9}$ |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 69.5 ${}_{\pm 1.9}$ |'
- en: '|  |  | QuaRot | 74.2 ${}_{\pm 0.4}$ |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 74.2 ${}_{\pm 0.4}$ |'
- en: '|  |  | SpinQuant* | 72.2 ${}_{\pm 0.9}$ |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 72.2 ${}_{\pm 0.9}$ |'
- en: '|  |  | SpinQuant | 73.8 ${}_{\pm 0.4}$ |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 73.8 ${}_{\pm 0.4}$ |'
- en: '| \cdashline2-13 | 4-4-16 | RTN | 26.6 | 22.1 | 44.3 | 50.9 | 38.9 | 26.2 |
    26.6 | 49.4 | 35.6 | 2,167.2 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-4-16 | RTN | 26.6 | 22.1 | 44.3 | 50.9 | 38.9 | 26.2 |
    26.6 | 49.4 | 35.6 | 2,167.2 |'
- en: '|  |  | SmoothQuant | 37.8 | 27.1 | 51.9 | 59.4 | 40.2 | 34.3 | 31.6 | 52.4
    | 41.8 | 254.5 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 37.8 | 27.1 | 51.9 | 59.4 | 40.2 | 34.3 | 31.6 | 52.4
    | 41.8 | 254.5 |'
- en: '|  |  | LLM-QAT | 46.2 | 32.4 | 61.8 | 62.0 | 41.3 | 47.6 | 36.1 | 54.7 | 47.8
    | 12.9 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLM-QAT | 46.2 | 32.4 | 61.8 | 62.0 | 41.3 | 47.6 | 36.1 | 54.7 | 47.8
    | 12.9 |'
- en: '|  |  | GPTQ | 27.6 ${}_{\pm 1.0}$ | 8,949.0 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 27.6 ${}_{\pm 1.0}$ | 8,949.0 |'
- en: '|  |  | QuaRot* | 65.3 ${}_{\pm 2.1}$ |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 65.3 ${}_{\pm 2.1}$ |'
- en: '|  |  | QuaRot | 71.8 ${}_{\pm 1.2}$ |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 71.8 ${}_{\pm 1.2}$ |'
- en: '|  |  | SpinQuant* | 67.7 ${}_{\pm 0.5}$ |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 67.7 ${}_{\pm 0.5}$ |'
- en: '|  |  | SpinQuant | 72.1 ${}_{\pm 0.9}$ |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 72.1 ${}_{\pm 0.9}$ |'
- en: '| \cdashline2-13 | 4-4-4 | RTN | 27.1 | 24.4 | 44.8 | 51.4 | 39.4 | 26.7 |
    33.0 | 50.0 | 37.1 | 2,382.5 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-4-4 | RTN | 27.1 | 24.4 | 44.8 | 51.4 | 39.4 | 26.7 |
    33.0 | 50.0 | 37.1 | 2,382.5 |'
- en: '|  |  | SmoothQuant | 31.4 | 24.8 | 51.4 | 54.1 | 39.4 | 29.1 | 31.9 | 50.0
    | 39.0 | 698.7 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 31.4 | 24.8 | 51.4 | 54.1 | 39.4 | 29.1 | 31.9 | 50.0
    | 39.0 | 698.7 |'
- en: '|  |  | LLM-QAT | 42.0 | 27.7 | 59.5 | 58.9 | 41.0 | 43.1 | 33.5 | 53.3 | 44.9
    | 14.9 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLM-QAT | 42.0 | 27.7 | 59.5 | 58.9 | 41.0 | 43.1 | 33.5 | 53.3 | 44.9
    | 14.9 |'
- en: '|  |  | GPTQ | 27.6 ${}_{\pm 1.1}$ | 9,253.1 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 27.6 ${}_{\pm 1.1}$ | 9,253.1 |'
- en: '|  |  | QuaRot* | 65.3 ${}_{\pm 1.6}$ |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 65.3 ${}_{\pm 1.6}$ |'
- en: '|  |  | QuaRot | 70.1 ${}_{\pm 0.8}$ |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 70.1 ${}_{\pm 0.8}$ |'
- en: '|  |  | SpinQuant* | 68.1 ${}_{\pm 0.9}$ |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 68.1 ${}_{\pm 0.9}$ |'
- en: '|  |  | SpinQuant | 72.6 ${}_{\pm 0.9}$ |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 72.6 ${}_{\pm 0.9}$ |'
- en: '| 13B | 16-16-16 | Full Precision | 75.3 | 51.4 | 79.8 | 80.4 | 50.5 | 79.8
    | 56.8 | 72.5 | 68.3 | 5.0 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 13B | 16-16-16 | Full Precision | 75.3 | 51.4 | 79.8 | 80.4 | 50.5 | 79.8
    | 56.8 | 72.5 | 68.3 | 5.0 |'
- en: '| \cdashline2-13 | 4-16-16 | RTN | 63.7 | 40.3 | 69.5 | 74.0 | 46.5 | 60.4
    | 47.0 | 61.4 | 57.9 | 6.4 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-16-16 | RTN | 63.7 | 40.3 | 69.5 | 74.0 | 46.5 | 60.4
    | 47.0 | 61.4 | 57.9 | 6.4 |'
- en: '|  |  | SmoothQuant | 72.0 | 45.6 | 71.4 | 78.4 | 46.8 | 72.9 | 51.0 | 68.4
    | 63.3 | 6.1 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 72.0 | 45.6 | 71.4 | 78.4 | 46.8 | 72.9 | 51.0 | 68.4
    | 63.3 | 6.1 |'
- en: '|  |  | OmniQuant | 70.2 | 43.1 | – | 78.4 | – | – | – | 67.8 | – | – |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  |  | OmniQuant | 70.2 | 43.1 | – | 78.4 | – | – | – | 67.8 | – | – |'
- en: '|  |  | QuIP | 73.3 | 44.9 | – | 79 | – | – | – | 69.7 | – | – |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuIP | 73.3 | 44.9 | – | 79 | – | – | – | 69.7 | – | – |'
- en: '|  |  | AQLM | 72.2 | 43.9 | – | 78.6 | – | – | – | 70.4 | – | – |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  |  | AQLM | 72.2 | 43.9 | – | 78.6 | – | – | – | 70.4 | – | – |'
- en: '|  |  | QuIP# | 73.9 | 45.5 | – | 78.9 | – | – | – | 69.9 | – | – |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuIP# | 73.9 | 45.5 | – | 78.9 | – | – | – | 69.9 | – | – |'
- en: '|  |  | GPTQ | 73.2 ${}_{\pm 1.4}$ |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 73.2 ${}_{\pm 1.4}$ |'
- en: '|  |  | QuaRot* | 75.3 ${}_{\pm 1.3}$ |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 75.3 ${}_{\pm 1.3}$ |'
- en: '|  |  | QuaRot | 76.3 ${}_{\pm 0.6}$ |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 76.3 ${}_{\pm 0.6}$ |'
- en: '|  |  | SpinQuant* | 76.3 ${}_{\pm 0.8}$ |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 76.3 ${}_{\pm 0.8}$ |'
- en: '|  |  | SpinQuant | 77.0 ${}_{\pm 0.5}$ |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 77.0 ${}_{\pm 0.5}$ |'
- en: '| \cdashline2-13 | 4-4-16 | RTN | 26.0 | 26.0 | 40.6 | 49.7 | 38.7 | 26.0 |
    25.4 | 49.9 | 35.3 | 7,216.7 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-4-16 | RTN | 26.0 | 26.0 | 40.6 | 49.7 | 38.7 | 26.0 |
    25.4 | 49.9 | 35.3 | 7,216.7 |'
- en: '|  |  | SmoothQuant | 45.2 | 27.1 | 55.4 | 62.5 | 40.5 | 44.3 | 33.4 | 50.8
    | 44.9 | 34.5 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 45.2 | 27.1 | 55.4 | 62.5 | 40.5 | 44.3 | 33.4 | 50.8
    | 44.9 | 34.5 |'
- en: '|  |  | GPTQ | 26.6 ${}_{\pm 0.5}$ | 5,245.3 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 26.6 ${}_{\pm 0.5}$ | 5,245.3 |'
- en: '|  |  | QuaRot* | 72.5 ${}_{\pm 1.3}$ |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 72.5 ${}_{\pm 1.3}$ |'
- en: '|  |  | QuaRot | 74.4 ${}_{\pm 1.1}$ |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 74.4 ${}_{\pm 1.1}$ |'
- en: '|  |  | SpinQuant* | 73.7 ${}_{\pm 1.1}$ |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 73.7 ${}_{\pm 1.1}$ |'
- en: '|  |  | SpinQuant | 75.9 ${}_{\pm 0.8}$ |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 75.9 ${}_{\pm 0.8}$ |'
- en: '| \cdashline2-13 | 4-4-4 | RTN | 26.1 | 24.3 | 40.3 | 48.7 | 39.6 | 25.8 |
    29.2 | 49.6 | 35.4 | 7,428.8 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-4-4 | RTN | 26.1 | 24.3 | 40.3 | 48.7 | 39.6 | 25.8 |
    29.2 | 49.6 | 35.4 | 7,428.8 |'
- en: '|  |  | SmoothQuant | 36.9 | 24.8 | 49.4 | 57.2 | 39.6 | 33.3 | 31.2 | 51.7
    | 40.5 | 56.6 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 36.9 | 24.8 | 49.4 | 57.2 | 39.6 | 33.3 | 31.2 | 51.7
    | 40.5 | 56.6 |'
- en: '|  |  | GPTQ | 26.6 ${}_{\pm 0.5}$ | 5,237.1 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 26.6 ${}_{\pm 0.5}$ | 5,237.1 |'
- en: '|  |  | QuaRot* | 72.4 ${}_{\pm 1.7}$ |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 72.4 ${}_{\pm 1.7}$ |'
- en: '|  |  | QuaRot | 74.0 ${}_{\pm 0.7}$ |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 74.0 ${}_{\pm 0.7}$ |'
- en: '|  |  | SpinQuant* | 73.8 ${}_{\pm 1.4}$ |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 73.8 ${}_{\pm 1.4}$ |'
- en: '|  |  | SpinQuant | 75.7 ${}_{\pm 1.0}$ |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 75.7 ${}_{\pm 1.0}$ |'
- en: '| 70B | 16-16-16 | Full Precision | 80.2 | 60.5 | 85.1 | 82.8 | 50.8 | 84.3
    | 59.0 | 80.6 | 72.9 | 3.3 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 70B | 16-16-16 | Full Precision | 80.2 | 60.5 | 85.1 | 82.8 | 50.8 | 84.3
    | 59.0 | 80.6 | 72.9 | 3.3 |'
- en: '| \cdashline2-13 | 4-16-16 | RTN | 77.7 | 54.6 | 82.7 | 81.5 | 47.7 | 78.4
    | 56.2 | 75.2 | 69.2 | 4.6 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-16-16 | RTN | 77.7 | 54.6 | 82.7 | 81.5 | 47.7 | 78.4
    | 56.2 | 75.2 | 69.2 | 4.6 |'
- en: '|  |  | SmoothQuant | 79.7 | 56.7 | 81.3 | 81.4 | 50.2 | 81.4 | 54.8 | 76.4
    | 70.2 | 4.1 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 79.7 | 56.7 | 81.3 | 81.4 | 50.2 | 81.4 | 54.8 | 76.4
    | 70.2 | 4.1 |'
- en: '|  |  | OMNIQ | 77.9 | 49.8 | – | 80.7 | – | – | – | 75.8 | – | – |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|  |  | OMNIQ | 77.9 | 49.8 | – | 80.7 | – | – | – | 75.8 | – | – |'
- en: '|  |  | QuIP | 74.3 | 47 | – | 80.3 | – | – | – | 76 | – | – |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuIP | 74.3 | 47 | – | 80.3 | – | – | – | 76 | – | – |'
- en: '|  |  | AQLM | 78.1 | 51 | – | 81.4 | – | – | – | 76.9 | – | – |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  |  | AQLM | 78.1 | 51 | – | 81.4 | – | – | – | 76.9 | – | – |'
- en: '|  |  | QuIP# | 78.1 | 50.6 | – | 81.4 | – | – | – | 77.1 | – | – |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuIP# | 78.1 | 50.6 | – | 81.4 | – | – | – | 77.1 | – | – |'
- en: '|  |  | GPTQ | 80.1 ${}_{\pm 0.2}$ |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 80.1 ${}_{\pm 0.2}$ |'
- en: '|  |  | QuaRot* | 79.5 ${}_{\pm 0.7}$ |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 79.5 ${}_{\pm 0.7}$ |'
- en: '|  |  | QuaRot | 79.4 ${}_{\pm 0.7}$ |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 79.4 ${}_{\pm 0.7}$ |'
- en: '|  |  | SpinQuant* | 79.8 ${}_{\pm 0.6}$ |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 79.8 ${}_{\pm 0.6}$ |'
- en: '|  |  | SpinQuant | 79.7 ${}_{\pm 0.6}$ |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 79.7 ${}_{\pm 0.6}$ |'
- en: '| \cdashline2-13 | 4-4-16 | RTN | 26.0 | 23.2 | 43.5 | 48.9 | 37.0 | 26.0 |
    25.6 | 50.5 | 35.1 | 2e5 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-4-16 | RTN | 26.0 | 23.2 | 43.5 | 48.9 | 37.0 | 26.0 |
    25.6 | 50.5 | 35.1 | 2e5 |'
- en: '|  |  | SmoothQuant | 9.5 | 71.7 | 29.0 | 66.6 | 73.1 | 45.1 | 67.4 | 39.4
    | 64.6 | 57.1 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 9.5 | 71.7 | 29.0 | 66.6 | 73.1 | 45.1 | 67.4 | 39.4
    | 64.6 | 57.1 |'
- en: '|  |  | GPTQ | 25.3 ${}_{\pm 0.5}$ | 2e6 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 25.3 ${}_{\pm 0.5}$ | 2e6 |'
- en: '|  |  | QuaRot* | 77.9 ${}_{\pm 0.8}$ |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 77.9 ${}_{\pm 0.8}$ |'
- en: '|  |  | QuaRot | 78.1 ${}_{\pm 0.6}$ |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 78.1 ${}_{\pm 0.6}$ |'
- en: '|  |  | SpinQuant* | 79.2 ${}_{\pm 0.8}$ |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 79.2 ${}_{\pm 0.8}$ |'
- en: '|  |  | SpinQuant | 78.4 ${}_{\pm 0.4}$ |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 78.4 ${}_{\pm 0.4}$ |'
- en: '| \cdashline2-13 | 4-4-4 | RTN | 25.5 | 24.5 | 43.2 | 50.2 | 36.7 | 26.6 |
    24.2 | 49.3 | 35.0 | 2e5 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-13 | 4-4-4 | RTN | 25.5 | 24.5 | 43.2 | 50.2 | 36.7 | 26.6 |
    24.2 | 49.3 | 35.0 | 2e5 |'
- en: '|  |  | SmoothQuant | 68.1 | 31.9 | 65.8 | 72.0 | 43.5 | 64.2 | 38.2 | 63.1
    | 55.9 | 10.5 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SmoothQuant | 68.1 | 31.9 | 65.8 | 72.0 | 43.5 | 64.2 | 38.2 | 63.1
    | 55.9 | 10.5 |'
- en: '|  |  | GPTQ | 26.1 ${}_{\pm 1.0}$ | 1e6 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPTQ | 26.1 ${}_{\pm 1.0}$ | 1e6 |'
- en: '|  |  | QuaRot* | 77.8 ${}_{\pm 0.6}$ |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot* | 77.8 ${}_{\pm 0.6}$ |'
- en: '|  |  | QuaRot | 78.4 ${}_{\pm 1.0}$ |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QuaRot | 78.4 ${}_{\pm 1.0}$ |'
- en: '|  |  | SpinQuant* | 78.7 ${}_{\pm 0.8}$ |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant* | 78.7 ${}_{\pm 0.8}$ |'
- en: '|  |  | SpinQuant | 78.3 ${}_{\pm 0.3}$ |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SpinQuant | 78.3 ${}_{\pm 0.3}$ |'
- en: A.2 Cayley optimization choice
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 Cayley 优化选择
- en: 'Table 7: Ablation study on Number of training samples and iterations in Cayley
    SGD optimization.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 关于训练样本数量和迭代次数在 Cayley SGD 优化中的消融研究。'
- en: '| #Bits | Task | # Training sample | # Training iterations |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| #Bits | 任务 | # 训练样本 | # 训练迭代次数 |'
- en: '| (W-A-KV) | 128 | 800 | 10 | 25 | 50 | 100 | 200 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| (W-A-KV) | 128 | 800 | 10 | 25 | 50 | 100 | 200 |'
- en: '| 4-4-4 | Wiki ($\downarrow$ |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 4-4-4 | Wiki ($\downarrow$ |'
- en: 'In Table [7](#A1.T7 "Table 7 ‣ A.2 Cayley optimization choice ‣ A.1 Complete
    results of main result table ‣ Appendix A Appendix / supplemental material ‣ 7
    Limitations and Broader Impacts ‣ 6 Conclusions ‣ 5 Related Work ‣ 4.3.3 Rotation
    type ‣ 4.3 Ablation studies ‣ 4.2 Main results ‣ 4 Experiments ‣ ​SpinQuant: LLM
    Quantization with Learned Rotations​"), we evaluate the impact of varying the
    number of samples and iterations used in Cayley optimization. Given the relatively
    small number of trainable parameters in the rotation matrix compared to the original
    weight parameters, and considering it as a constraint optimization, we only need
    a minimal amount of calibration data and iterations to enhance the rotation for
    improved quantization. The findings indicate that rotation optimization is resilient
    to modifications in the number of samples. Even though we used 800 samples in
    our experiments, reducing this to 128 samples does not lead to a significant change
    in the perplexity. Furthermore, we examined the optimal number of iterations and
    found that the wiki perplexity ceases to decrease and stabilizes at 100 iterations.
    Consequently, we chose to use 100 iterations in all our experiments.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [7](#A1.T7 "表 7 ‣ A.2 Cayley 优化选择 ‣ A.1 主要结果表完整结果 ‣ 附录 A 附录 / 补充材料 ‣ 7 局限性和广泛影响
    ‣ 6 结论 ‣ 5 相关工作 ‣ 4.3.3 旋转类型 ‣ 4.3 消融研究 ‣ 4.2 主要结果 ‣ 4 实验 ‣ ​SpinQuant: LLM 定量化与学习旋转​")中，我们评估了在
    Cayley 优化中样本数量和迭代次数的变化对结果的影响。鉴于旋转矩阵中的可训练参数数量相对较少，相较于原始权重参数，并且将其视为约束优化，我们只需要最少量的校准数据和迭代次数来提升旋转效果以改善定量化。结果表明，旋转优化对样本数量的变化具有一定的弹性。尽管我们的实验使用了
    800 个样本，但将样本减少到 128 个不会显著改变困惑度。此外，我们检查了最优的迭代次数，发现 wiki 困惑度在 100 次迭代后停止下降并稳定。因此，我们选择在所有实验中使用
    100 次迭代。'
