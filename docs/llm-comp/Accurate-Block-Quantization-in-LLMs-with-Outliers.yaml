- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:49:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Accurate Block Quantization in LLMs with Outliers
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 中带有离群值的准确块量化
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.20137](https://ar5iv.labs.arxiv.org/html/2403.20137)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.20137](https://ar5iv.labs.arxiv.org/html/2403.20137)
- en: Nikita Trukhanov d-Matrix Santa Clara, CA, USA
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Nikita Trukhanov d-Matrix 圣克拉拉，加州，美国
- en: ntrukhanov@d-matrix.ai    Ilya Soloveychik d-Matrix Santa Clara, CA, USA
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ntrukhanov@d-matrix.ai    Ilya Soloveychik d-Matrix 圣克拉拉，加州，美国
- en: ilyas@d-matrix.ai
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ilyas@d-matrix.ai
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The demand for inference on extremely large scale LLMs has seen enormous growth
    in the recent months. It made evident the colossal shortage of dedicated hardware
    capable of efficient and fast processing of the involved compute and memory movement.
    The problem is aggravated by the exploding raise in the lengths of the sequences
    being processed, since those require efficient on-chip storage of the KV-cache
    of size proportional to the sequence length. To make the required compute feasible
    and fit the involved data into available memory, numerous quantization techniques
    have been proposed that allow accurate quantization for both weights and activations.
    One of the main recent breakthroughs in this direction was introduction of the
    family of Block Floating Point (BFP) formats characterized by a block of mantissas
    with a shared scale factor. These enable memory- power-, and compute- efficient
    hardware support of the tensor operations and provide extremely good quantization
    accuracy. The main issues preventing widespread application of block formats is
    caused by the presence of outliers in weights and activations since those affect
    the accuracy of the other values in the same block. In this paper, we focus on
    the most critical problem of limited KV-cache storage. We propose a novel approach
    enabling usage of low precision BFP formats without compromising the resulting
    model accuracy. We exploit the common channel-wise patterns exhibited by the outliers
    to rearrange them in such a way, that their quantization quality is significantly
    improved. The methodology yields 2x savings in the memory footprint without significant
    degradation of the model’s accuracy. Importantly, the rearrangement of channels
    happens at the compile time and thus has no impact on the inference latency.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几个月，对极大规模 LLM 的推断需求急剧增长。这显著暴露出专用硬件在处理相关计算和内存移动方面的巨大短缺。问题由于处理序列长度的激增而加剧，因为这些序列需要按序列长度比例存储
    KV-cache 的高效片上存储。为了使所需的计算成为可能并将相关数据适配到可用内存中，已经提出了许多量化技术，这些技术允许对权重和激活进行准确量化。最近在这方面的主要突破之一是引入了一系列块浮点（BFP）格式，这些格式以具有共享尺度因子的尾数块为特征。这些格式使得硬件支持张量操作的内存、功耗和计算效率得到了极大的提升，并提供了极好的量化精度。阻碍块格式广泛应用的主要问题是权重和激活中的离群值，因为它们会影响同一块中其他值的精度。在本文中，我们关注于有限的
    KV-cache 存储的最关键问题。我们提出了一种新颖的方法，使得在不影响模型精度的情况下使用低精度 BFP 格式。我们利用离群值表现出的常见通道级模式，通过重新排列这些离群值，显著提高它们的量化质量。这一方法在不显著降低模型精度的情况下实现了内存占用的
    2 倍节省。重要的是，通道的重新排列发生在编译时，因此对推断延迟没有影响。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: LLM inference; block formats, outliers, cache.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 推断；块格式，离群值，缓存。
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Pretrained Large Language Models (LLMs) have become enormously popular in the
    recent years [[1](#bib.bibx1), [2](#bib.bibx2), [3](#bib.bibx3), [4](#bib.bibx4),
    [5](#bib.bibx5)]. Such popularity has mostly been gained due to the extremely
    high quality of the text generated by the state-of-the-art models. However, such
    improvements often come at the cost of increased model sizes which makes training
    of these large models and using them for inference highly challenging in terms
    of storage capacity, memory transfer, and compute. The architecture of the modern
    LLMs is typically based on the decoder part of a transformer [[6](#bib.bibx6)].
    While the LLM training process can fully exploit parallelization across the input
    tokens, the inference must be performed sequentially. The generation process produces
    one token on every pass over the network given the prompt and all previously generated
    tokens. The core building block of the transformer architecture – the attention
    mechanism – requires computation of the so called keys ${\bm{K}}$ matrices become
    prohibitively resource greedy. To avoid those redundant operations, one could
    exploit the fact that the keys and values of the already appended tokens never
    change and can therefore be cached on chip.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的大型语言模型（LLMs）在近年来变得极为流行 [[1](#bib.bibx1), [2](#bib.bibx2), [3](#bib.bibx3),
    [4](#bib.bibx4), [5](#bib.bibx5)]。这种流行主要是由于最先进模型生成的文本质量极高。然而，这种改进通常以增加模型规模为代价，这使得这些大型模型的训练和推理在存储容量、内存传输和计算方面变得极具挑战性。现代LLMs的架构通常基于transformer的解码器部分
    [[6](#bib.bibx6)]。虽然LLM的训练过程可以充分利用输入标记的并行化，但推理必须按顺序进行。生成过程在每次经过网络时生成一个标记，给定提示和所有先前生成的标记。transformer架构的核心构建块——注意力机制——需要计算所谓的键
    ${\bm{K}}$ 矩阵，这使得计算变得极其资源密集。为了避免这些冗余操作，可以利用已附加标记的键和值永远不变的事实，因此可以在芯片上进行缓存。
- en: Caching ${\bm{K}}$ matrices is extremely helpful if the on-chip storage allows
    it. However, the ever growing demand for generation of longer sequence dwarfs
    any amount of on-chip storage [[7](#bib.bibx7), [8](#bib.bibx8)]. Hence, every
    possible technique must be exploited to reduce the memory footprint of the cached
    tensors. The most promising approach consists in efficient quantization of keys
    and values. To this end such algorithms as GPTQ [[9](#bib.bibx9)], SmoothQuant
    [[10](#bib.bibx10)], and many others have been proposed. For example, the GPTQ
    technique prescribes successive quantization of the weight columns in such a way
    that the rounding of every next column carefully takes into account the accumulated
    error of the previously quantized columns. The error is calculated on a small
    representative batch of data. In contrast, SmoothQuant is targeted to better quantization
    of activations. The authors notice that in the activations they were observing,
    a few channels had consistently higher values on various tokens. They introduced
    per-channel scaling factors to carry the dynamic range of activations over into
    weights. This way they transferred part of quantization burden from harder-to-quantize
    activations to easier-to-quantize weights. In all quantization approaches, the
    goal is always to enable a low-bit, e.g. 4 bits per element, storage for the tensors,
    with a common scaling vector, and, in some cases, bias vectors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存 ${\bm{K}}$ 矩阵在芯片存储允许的情况下极为有用。然而，对更长序列生成的不断增长的需求使得任何芯片上的存储量都显得微不足道 [[7](#bib.bibx7),
    [8](#bib.bibx8)]。因此，必须充分利用所有可能的技术来减少缓存张量的内存占用。最有前途的方法是对键和值进行高效的量化。为此，已经提出了如GPTQ
    [[9](#bib.bibx9)]、SmoothQuant [[10](#bib.bibx10)]等算法。例如，GPTQ技术规定了权重列的连续量化，使得每个下一列的四舍五入都仔细考虑了之前量化列的累计误差。误差是在一个小的代表性数据批次上计算的。相比之下，SmoothQuant旨在更好地量化激活。作者注意到，在他们观察的激活中，几个通道在各种标记上
    consistently具有更高的值。他们引入了每通道的缩放因子，将激活的动态范围传递到权重中。通过这种方式，他们将部分量化负担从更难量化的激活转移到更容易量化的权重上。在所有量化方法中，目标始终是实现低比特存储，例如每元素4比特，使用共同的缩放向量，并在某些情况下使用偏置向量。
- en: Further refinements of the algorithmic and software solutions have only limited
    impact on the overall efficiency if not supported by hardware. Most of the modern
    LLM models are designed and run on Graphics Processing Unit (GPUs) which exploit
    floating-point arithmetic [[11](#bib.bibx11), [12](#bib.bibx12)]. As mentioned
    earlier, the computational load required by modern transformers has reached such
    enormous volumes that traditional GPUs cannot fully meet the growing demand, pushing
    both accelerators and high performance GPUs towards narrow arithmetic. As a consequence,
    unmatched research efforts have been applied by the engineering community to replace
    narrow floating-point with even denser fixed-point representations [[13](#bib.bibx13),
    [14](#bib.bibx14), [15](#bib.bibx15), [16](#bib.bibx16)]. Despite the excellent
    gains in both speed and computational density achieved by fixed-point arithmetic,
    training using it or even half-precision floating-point arithmetic has not provided
    clear evidence in its favor due to the limited dynamic range inherent in such
    formats [[17](#bib.bibx17)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有硬件支持，算法和软件解决方案的进一步改进对整体效率的影响有限。大多数现代LLM模型设计并运行在利用浮点运算的图形处理单元（GPUs）上[[11](#bib.bibx11),
    [12](#bib.bibx12)]。如前所述，现代变压器所需的计算负载已经达到如此巨大的体积，以至于传统的GPU无法完全满足不断增长的需求，这推动了加速器和高性能GPU向狭窄的算术方向发展。因此，工程界付出了巨大研究努力，试图用更密集的定点表示来取代狭窄的浮点表示[[13](#bib.bibx13),
    [14](#bib.bibx14), [15](#bib.bibx15), [16](#bib.bibx16)]。尽管定点运算在速度和计算密度上都取得了优秀的提升，但由于这种格式固有的有限动态范围，使用它或半精度浮点运算进行训练并没有提供明确的证据[[17](#bib.bibx17)]。
- en: Block Floating Point (BFP) numerical formats have received renewed interest
    recently for LLM inference applications due to their combination of wide dynamic
    range, numerical accuracy, and efficient hardware implementation of inner products
    using simple integer arithmetic [[18](#bib.bibx18), [19](#bib.bibx19), [20](#bib.bibx20),
    [21](#bib.bibx21)]. BFP formats are characterized by a block of mantissas with
    a shared scale factor. The simplest implementation has the scale factor as a power
    of two, the so-called exponent, in which case the inner product between two blocks
    involves multiplying the integer mantissas and adding the two block exponents.
    The industry has thus far mainly exploited BFP12 (with $4$ elements [[19](#bib.bibx19),
    [20](#bib.bibx20), [18](#bib.bibx18), [21](#bib.bibx21)]. Alternative formats,
    using low-bit floating point elements, with a wider range common exponent, are
    also considered [[22](#bib.bibx22)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其广泛的动态范围、数值精度和使用简单整数运算高效实现内积，Block Floating Point（BFP）数值格式最近在LLM推理应用中受到了重新关注[[18](#bib.bibx18),
    [19](#bib.bibx19), [20](#bib.bibx20), [21](#bib.bibx21)]。BFP格式的特点是具有共享尺度因子的尾数块。最简单的实现方式是将尺度因子设为二的幂，即所谓的指数，这种情况下，两个块之间的内积涉及到乘法整数尾数和加法两个块的指数。因此，业内迄今为止主要利用BFP12（具有$4$个元素[[19](#bib.bibx19),
    [20](#bib.bibx20), [18](#bib.bibx18), [21](#bib.bibx21)]）。还考虑了使用低比特浮点元素的替代格式，这些格式具有更宽范围的共同指数[[22](#bib.bibx22)]。
- en: One of the main numerical issues faced by the ML engineers dealing with LLMs
    both from theoretical and practical perspectives is the sporadic emergence of
    so-called outliers in weights and activations of the modern large-scale transformers
    [[10](#bib.bibx10), [7](#bib.bibx7)]. Existence of outliers becomes especially
    challenging when it comes to block formats, since presence of even a single element
    with an extremely large magnitude in a block can completely ruin the quantization
    accuracy of all the other elements in that same block.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论和实践的角度来看，处理LLM的ML工程师面临的主要数值问题之一是现代大规模变压器权重和激活中所谓的异常值的偶发出现[[10](#bib.bibx10),
    [7](#bib.bibx7)]。当涉及到块格式时，异常值的存在变得尤为具有挑战性，因为块中即使只有一个具有极大幅度的元素也会完全破坏该块中所有其他元素的量化精度。
- en: Below, we address this problem. We demonstrate how the advantages of the BFP
    quantization can be maintained when weights or activations contain numerous outliers.
    The key observation behind our approach consists in the fact that the inner product
    is invariant to synchronized reshuffling of the tensors being multiplied. For
    instance, if we focus on the ${\bm{q}}{\bm{K}}^{\top}$ happens at the compile
    time. It requires no calibration data and has no effect on the inference latency.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们解决了这个问题。我们展示了当权重或激活包含大量异常值时，BFP量化的优势如何得以保持。我们方法背后的关键观察是内积对被乘张量的同步重新排列是不变的。例如，如果我们关注${\bm{q}}{\bm{K}}^{\top}$发生在编译时。这不需要校准数据，对推理延迟没有影响。
- en: The rest of the paper is organized as follows. In section [II](#S2 "II Inference
    in LLMs ‣ Accurate Block Quantization in LLMs with Outliers") we describe the
    setup in more detail and define the block formats. Section [III](#S3 "III K-sort
    Algorithm ‣ Accurate Block Quantization in LLMs with Outliers") features our novel
    ${\bm{K}}$ cache containing outliers. Supporting empirical data is provided in
    Section [IV](#S4 "IV Experiments ‣ Accurate Block Quantization in LLMs with Outliers").
    We summarize our findings in Section [V](#S5 "V Conclusion ‣ Accurate Block Quantization
    in LLMs with Outliers").
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下。在[II](#S2 "II Inference in LLMs ‣ Accurate Block Quantization in
    LLMs with Outliers")节中，我们详细描述了设置并定义了块格式。[III](#S3 "III K-sort Algorithm ‣ Accurate
    Block Quantization in LLMs with Outliers")节介绍了我们新颖的包含异常值的${\bm{K}}$缓存。支持的实证数据在[IV](#S4
    "IV Experiments ‣ Accurate Block Quantization in LLMs with Outliers")节中提供。我们在[V](#S5
    "V Conclusion ‣ Accurate Block Quantization in LLMs with Outliers")节中总结了我们的发现。
- en: II Inference in LLMs
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II LLMs中的推理
- en: In this paper, we focus on the problem of inference in LLMs. The sizes of the
    up-to-date models have become so large and the amount of compute involved became
    so enormous that efficient processing requires dedicated hardware and specialized
    algorithms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们专注于LLMs中的推理问题。当前模型的规模已经变得如此庞大，涉及的计算量也变得如此巨大，以至于高效处理需要专用硬件和专业算法。
- en: II-A KV-cache
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A KV缓存
- en: Inference on modern transformers essentially means sequential generation of
    tokens one by one given the initial prompt. After every pass through the model’s
    stack of decoders, the newly generated token is appended to the growing sequence
    and the process repeats with the updated context. The very nature of the attention
    mechanism requires calculation of the keys and values for the entire sequence
    generated up until current iteration. This leads to a lot of duplicated compute
    since every head inside every decoder block will repeatedly calculate the entire
    ${\bm{K}}$ contains numerous outliers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现代变换器上的推理实质上意味着根据初始提示逐个生成令牌。在每次通过模型的解码器堆栈后，新生成的令牌会附加到增长的序列中，过程会在更新的上下文中重复。注意机制的本质要求计算整个生成序列的键和值，直到当前迭代。这导致了大量重复计算，因为每个解码器块中的每个头将反复计算整个${\bm{K}}$，其中包含大量异常值。
- en: II-B Block Floating Point Formats
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 块浮点格式
- en: The unprecedented and ever growing amount of compute and storage required by
    the modern LLMs has lead to the development of numerous new data formats and novel
    directions and techniques involving quantization of weights and activations. New
    data formats are announced every few months both by the computer science community
    training the models [[23](#bib.bibx23), [24](#bib.bibx24)] and by the manufacturers
    of hardware [[18](#bib.bibx18), [22](#bib.bibx22), [25](#bib.bibx25), [21](#bib.bibx21)].
    Different techniques are proposed separately for storage and for compute [[7](#bib.bibx7),
    [24](#bib.bibx24), [23](#bib.bibx23)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现代LLMs所需的前所未有且不断增长的计算和存储量导致了许多新数据格式以及涉及权重和激活量化的新方向和技术的发展。计算机科学社区在训练模型时[[23](#bib.bibx23),
    [24](#bib.bibx24)]和硬件制造商[[18](#bib.bibx18), [22](#bib.bibx22), [25](#bib.bibx25),
    [21](#bib.bibx21)]每几个月就会宣布新的数据格式。不同的技术分别用于存储和计算[[7](#bib.bibx7), [24](#bib.bibx24),
    [23](#bib.bibx23)]。
- en: In this work, we focus on an extremely promising Block Floating Point family
    of formats that has become very popular in the recent months [[18](#bib.bibx18),
    [19](#bib.bibx19), [20](#bib.bibx20), [21](#bib.bibx21)]. The idea is based on
    the observation that quite often the elements of involved tensors have comparable
    amplitudes and thus can share the same or close exponent value when written in
    floating-point notation. As a consequence, we can store entire blocks of elements
    using shared exponent and individual integer mantissas. Numerous companies design
    there hardware specifically to support this family of formats [[18](#bib.bibx18),
    [19](#bib.bibx19), [20](#bib.bibx20)]. The main advantage enjoyed by the chips
    designed to support BFP formats consists in very significant reduction of required
    storage and effectively integer matrix multiplication, see [[19](#bib.bibx19),
    [20](#bib.bibx20)] for more details. This further leads to a huge reduction in
    consumed power and energy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们关注的是一种非常有前景的块浮点格式，它在最近几个月变得非常流行 [[18](#bib.bibx18), [19](#bib.bibx19),
    [20](#bib.bibx20), [21](#bib.bibx21)]。这个想法基于一个观察：涉及的张量的元素往往具有可比的幅度，因此在浮点表示中可以共享相同或接近的指数值。因此，我们可以使用共享的指数和各个整数尾数来存储整个块的元素。许多公司专门设计硬件以支持这种格式
    [[18](#bib.bibx18), [19](#bib.bibx19), [20](#bib.bibx20)]。支持 BFP 格式的芯片的主要优点在于显著减少所需存储量并有效进行整数矩阵乘法，更多细节见
    [[19](#bib.bibx19), [20](#bib.bibx20)]。这进一步导致了消耗的功率和能量的巨大减少。
- en: More specifically, a Block Floating Point format is characterized by the block
    size $n\in\mathbb{N}$, and their values are computed as
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，块浮点格式的特征是块大小 $n\in\mathbb{N}$，它们的值计算为
- en: '|  | $\{2^{e}\cdot M_{1},\dots,2^{e}\cdot M_{n}\},$ |  | (1) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $\{2^{e}\cdot M_{1},\dots,2^{e}\cdot M_{n}\},$ |  | (1) |'
- en: where $e$-bit integer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $e$ 位整数。
- en: Blocks formats are extremely efficient for matrix operations, since dot product
    using this family of formats effective turns into integer matrix multiplication
    and simple addition of the corresponding block exponents [[20](#bib.bibx20), [18](#bib.bibx18),
    [21](#bib.bibx21)]. The typical values of $p$.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 块格式对于矩阵操作非常高效，因为使用这类格式的点积实际上转化为整数矩阵乘法和相应块指数的简单加法 [[20](#bib.bibx20), [18](#bib.bibx18),
    [21](#bib.bibx21)]。$p$ 的典型值。
- en: II-C Sorting Channels of ${\bm{W}}_{\bm{k}}$
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C ${\bm{W}}_{\bm{k}}$ 的排序通道
- en: To efficiently store matrix ${\bm{K}}$ faster, we propose to quantize the former
    into a low-precision block format. The definition of the BFP format, says that
    the quantization range of a block is determined by its largest (in absolute value)
    element. If some blocks contain outliers, their overall quantization accuracy
    will be poor because the smallest elements might be rounded to zero. Next we show
    how to resolve this problem.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更高效地存储矩阵 ${\bm{K}}$，我们建议将其量化为低精度块格式。BFP 格式的定义表示，一个块的量化范围由其最大（绝对值）元素决定。如果某些块包含异常值，它们的总体量化精度会很差，因为最小的元素可能被舍入为零。接下来我们展示如何解决这个问题。
- en: 'The natural approach would be to sort the elements of the tensor by their absolute
    values before quantization. In that case, each block will only contain elements
    of comparable magnitudes: there will be blocks with larger elements and blocks
    with smaller elements, but we will avoid the undesirable scenario of having numerous
    blocks containing mixtures of elements of wide dynamic range. However, we must
    note that sorting tensors on the fly would be prohibitively expensive. Also, if
    we need to keep the sorting order to restore the original one for every token
    for every attention layer, it would outweigh any memory savings. Therefore, the
    brute-force sorting of elements will not work and we need a finer approach.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 自然的做法是在量化之前按绝对值对张量的元素进行排序。在这种情况下，每个块只会包含具有可比大小的元素：会有包含较大元素的块和包含较小元素的块，但我们会避免有许多包含广泛动态范围元素混合的块。然而，我们必须注意到，动态排序张量将是极其昂贵的。此外，如果我们需要保持排序顺序以便在每个注意力层恢复原始顺序，这将超过任何内存节省。因此，元素的蛮力排序将不起作用，我们需要更细致的方法。
- en: '![Refer to caption](img/8818003d88b76896579d4cef2b6f2c06.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8818003d88b76896579d4cef2b6f2c06.png)'
- en: 'Figure 1: Left: original ${\bm{W}}_{\bm{k}}$ since the entries of the former
    ending up in same blocks are closer in their absolute values.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：左侧：原始 ${\bm{W}}_{\bm{k}}$，因为前者的条目最终落在相同的块中，它们的绝对值更接近。
- en: As noted in [[7](#bib.bibx7)], the keys tend to exhibit certain outlier patterns.
    Namely, the outliers often concentrate in particular channels, which are quite
    consistent both across tokens in input sequences, and across different input sequences.
    Such behavior is usually caused by higher norms of the corresponding rows of ${\bm{W}}_{\bm{k}}$,
    then due to the linearity of inner product we can say that
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [[7](#bib.bibx7)] 所述，键往往表现出某些异常值模式。即，异常值通常集中在特定通道，这在输入序列的标记之间以及不同的输入序列之间非常一致。这种行为通常是由于
    ${\bm{W}}_{\bm{k}}$ 的相应行的范数较高，然后由于内积的线性性，我们可以说
- en: '|  | ${\bm{W}}_{q}^{\top}\cdot{\bm{W}}_{\bm{k}}=[\pi({\bm{W}}_{\bm{q}})]^{\top}\cdot\pi({\bm{W}}_{\bm{k}}).$
    |  | (2) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{W}}_{q}^{\top}\cdot{\bm{W}}_{\bm{k}}=[\pi({\bm{W}}_{\bm{q}})]^{\top}\cdot\pi({\bm{W}}_{\bm{k}}).$
    |  | (2) |'
- en: As a consequence, we have
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有
- en: '|  | $1$2 |  | (3) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: Now that we have applied permutation $\pi$. The idea is illustrated by Figure
    [1](#S2.F1 "Figure 1 ‣ II-C Sorting Channels of 𝐖_𝐤 ‣ II Inference in LLMs ‣ Accurate
    Block Quantization in LLMs with Outliers"). The colors of the heat-map reflect
    the absolute values of the elements, from lower (green) to larger (red). It is
    important to note that we do not store the queries in the cache and they can therefore
    be cast to a higher precision format. To enable application of this technique
    to any transformer, we need to show how it works when rotary embeddings are applied
    to keys and queries.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经应用了置换 $\pi$。这个思想由图 [1](#S2.F1 "图 1 ‣ II-C 𝐖_𝐤 的排序 ‣ II 在大型语言模型中的推理 ‣ 带异常值的LLMs的精确块量化")
    说明。热图的颜色反映了元素的绝对值，从较低（绿色）到较高（红色）。需要注意的是，我们不会在缓存中存储查询，因此它们可以被转换为更高精度的格式。为了使这种技术能够应用于任何变压器，我们需要展示在应用旋转嵌入于键和值时，它是如何工作的。
- en: II-D Rotary Embeddings
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 旋转嵌入
- en: Many modern LLMs use rotary positional embeddings (RoPE) [[26](#bib.bibx26)]
    to encode information about the order of tokens in the input sequence. Rotary
    embeddings are linear transformations applied to keys and queries defined as
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现代大型语言模型使用旋转位置嵌入（RoPE）[[26](#bib.bibx26)]来编码输入序列中标记的顺序信息。旋转嵌入是应用于键和值的线性变换，定义为
- en: '|  | $${\bm{R}}_{\Theta,m}^{d_{h}}=\begin{pmatrix}\cos{m\theta_{1}}&amp;-\sin{m\theta_{1}}&amp;\cdots&amp;0\\
    \sin{m\theta_{1}}&amp;\cos{m\theta_{1}}&amp;\cdots&amp;0\\'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $${\bm{R}}_{\Theta,m}^{d_{h}}=\begin{pmatrix}\cos{m\theta_{1}}&amp;-\sin{m\theta_{1}}&amp;\cdots&amp;0\\
    \sin{m\theta_{1}}&amp;\cos{m\theta_{1}}&amp;\cdots&amp;0\\'
- en: \vdots&amp;\ddots&amp;\ddots&amp;\vdots\\
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&amp;\ddots&amp;\ddots&amp;\vdots\\
- en: 0&amp;\cdots&amp;\cos{m\theta_{d_{h}/2}}&amp;-\sin{m\theta_{d_{h}/2}}\\
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 0&amp;\cdots&amp;\cos{m\theta_{d_{h}/2}}&amp;-\sin{m\theta_{d_{h}/2}}\\
- en: 0&amp;\cdots&amp;\sin{m\theta_{d_{h}/2}}&amp;\cos{m\theta_{d_{h}/2}}\end{pmatrix},$$
    |  |
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 0&amp;\cdots&amp;\sin{m\theta_{d_{h}/2}}&amp;\cos{m\theta_{d_{h}/2}}\end{pmatrix},$$
    |  |
- en: where $m$,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m$，
- en: '|  | $${\bm{R}}^{d_{h}}_{\Theta,m}{\bm{x}}=\begin{pmatrix}x_{1}\\ x_{2}\\'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $${\bm{R}}^{d_{h}}_{\Theta,m}{\bm{x}}=\begin{pmatrix}x_{1}\\ x_{2}\\'
- en: x_{3}\\
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: x_{3}\\
- en: x_{4}\\
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: x_{4}\\
- en: \vdots\\
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: x_{d_{h}-1}\\
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: x_{d_{h}-1}\\
- en: x_{d_{h}}\end{pmatrix}\otimes\begin{pmatrix}\cos{m\theta_{1}}\\
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: x_{d_{h}}\end{pmatrix}\otimes\begin{pmatrix}\cos{m\theta_{1}}\\
- en: \cos{m\theta_{1}}\\
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{1}}\\
- en: \cos{m\theta_{2}}\\
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{2}}\\
- en: \cos{m\theta_{2}}\\
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{2}}\\
- en: \vdots\\
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: \cos{m\theta_{d_{h}/2}}\\
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{d_{h}/2}}\\
- en: \cos{m\theta_{d_{h}/2}}\end{pmatrix}\\
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{d_{h}/2}}\end{pmatrix}\\
- en: +\begin{pmatrix}-x_{2}\\
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: +\begin{pmatrix}-x_{2}\\
- en: x_{1}\\
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: x_{1}\\
- en: -x_{4}\\
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: -x_{4}\\
- en: x_{3}\\
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: x_{3}\\
- en: \vdots\\
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: -x_{d_{h}}\\
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: -x_{d_{h}}\\
- en: x_{d_{h}-1}\end{pmatrix}\otimes\begin{pmatrix}\sin{m\theta_{1}}\\
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: x_{d_{h}-1}\end{pmatrix}\otimes\begin{pmatrix}\sin{m\theta_{1}}\\
- en: \sin{m\theta_{1}}\\
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{1}}\\
- en: \sin{m\theta_{2}}\\
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{2}}\\
- en: \sin{m\theta_{2}}\\
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{2}}\\
- en: \vdots\\
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: \sin{m\theta_{d_{h}/2}}\\
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{d_{h}/2}}\\
- en: \sin{m\theta_{d_{h}/2}}\end{pmatrix},$$ |  | (4) |
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{d_{h}/2}}\end{pmatrix},$$ |  | (4) |
- en: where $\otimes$ is the element-wise product. Next we provide a general version
    of our sorting algorithm that works well when rotary embeddings are used.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\otimes$ 是逐元素乘积。接下来，我们提供了一个适用于旋转嵌入的通用排序算法版本。
- en: III K-sort Algorithm
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III K-排序算法
- en: The main idea of our ${\bm{K}}$ is known at the compile time, all the necessary
    permutations of the frequencies and signs needed for correct application of RoPE
    can be done then as well - this does not delay the inference.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 ${\bm{K}}$ 的主要思想在编译时已知，因此所有必要的频率和符号的置换可以在此时完成——这不会延迟推理过程。
- en: Algorithm 1 ${\bm{K}}$-sort algorithm for a head
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 ${\bm{K}}$-排序算法
- en: '1: ${\bm{N}}_{i}\leftarrow||{\bm{W}}_{\bm{k}}[i,:]||,\;\forall i\in\{1,\dots,d_{h}\}$'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '1: ${\bm{N}}_{i}\leftarrow||{\bm{W}}_{\bm{k}}[i,:]||,\;\forall i\in\{1,\dots,d_{h}\}$'
- en: In practice, we propose to use ${\bm{K}}$-bits per element mantissa without
    any significant effect on the performance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们建议使用 ${\bm{K}}$-位每元素尾数，而不会对性能产生重大影响。
- en: While present work concentrates on the keys ${\bm{K}}$ stored in the cache without
    any run-time overhead. For the lack of space, we postpone the details for further
    publications.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当前工作集中于缓存中存储的 ${\bm{K}}$ 键，没有任何运行时开销。由于篇幅限制，我们将细节推迟到后续出版物中。
- en: IV Experiments
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验
- en: Numerous recent publications have reported the issues of outliers in K-cache
    and their significant impact on the accuracy and storage requirements [[10](#bib.bibx10),
    [7](#bib.bibx7), [27](#bib.bibx27)]. For the lack space, in this short contribution
    we focus on one of such popular LLMs, Llama2-7B-hf model [[2](#bib.bibx2)]. As
    shown in [[7](#bib.bibx7)], this network and its many relatives and variations
    exhibit the K-outliers phenomenon very clearly. In this section, we demonstrate
    the advantages of our ${\bm{K}}$-sort. This implies that the gain on larger models
    will be even more remarkable.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 许多近期出版物报告了 K-cache 中离群值的问题及其对准确性和存储需求的显著影响 [[10](#bib.bibx10), [7](#bib.bibx7),
    [27](#bib.bibx27)]。由于篇幅有限，我们在这篇简短的贡献中专注于一种流行的 LLM，即 Llama2-7B-hf 模型 [[2](#bib.bibx2)]。如
    [[7](#bib.bibx7)] 所示，这个网络及其许多亲属和变体非常明显地展示了 K-outliers 现象。在本节中，我们展示了我们的 ${\bm{K}}$-排序的优势。这意味着在更大的模型上的收益将更加显著。
- en: The experiments were carried out using the default Hugging Face checkpoint without
    extra fine-tuning. The baseline perplexity of the model with FP16 weights on wikitext-2
    [[28](#bib.bibx28)] dataset is $9.4881$-s are not stored in the cache so their
    compression is not required. For fair comparison, the rest of the operations were
    performed exactly as in the baseline model - in FP16 format.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 实验使用了默认的 Hugging Face 检查点，没有额外的微调。具有 FP16 权重的模型在 wikitext-2 [[28](#bib.bibx28)]
    数据集上的基线困惑度为 $9.4881$，不在缓存中存储，因此不需要压缩。为了公平比较，其余操作与基线模型完全一致 - 使用 FP16 格式。
- en: Table [I](#S4.T1 "TABLE I ‣ IV Experiments ‣ Accurate Block Quantization in
    LLMs with Outliers") demonstrates the obtained results. As a sanity check, we
    see that for the block size of $128$.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [I](#S4.T1 "表 I ‣ IV 实验 ‣ LLM 中的准确块量化与离群值") 展示了获得的结果。作为一个理智检查，我们看到对于块大小为 $128$。
- en: 'TABLE I: LLama2-7B perplexity on wikitext-2'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: LLama2-7B 在 wikitext-2 上的困惑度'
- en: '| format | algorithm |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | 算法 |'
- en: '| --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Q | K | original | ${\bm{K}}$-sorted |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Q | K | 原始 | ${\bm{K}}$-排序 |'
- en: '| FP16 | FP16 | 9.4881 | 9.4881 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | FP16 | 9.4881 | 9.4881 |'
- en: '| BFP16_128 | BFP12_128 | 10.0861 | 10.0861 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| BFP16_128 | BFP12_128 | 10.0861 | 10.0861 |'
- en: '| BFP16_64 | BFP12_64 | 9.9999 | 9.6061 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| BFP16_64 | BFP12_64 | 9.9999 | 9.6061 |'
- en: '| BFP16_32 | BFP12_32 | 9.8300 | 9.5196 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| BFP16_32 | BFP12_32 | 9.8300 | 9.5196 |'
- en: V Conclusion
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: In this paper, we demonstrate that simple reshuffling of the static weights
    in popular LLMs can make their quantization quality much better. Specifically,
    we advocate for the use of Block Floating Point formats and show that BFP12 format
    with $4$-cache and therefore allows generation of much longer sequences on the
    same hardware.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们展示了流行 LLM 中静态权重的简单重新排列可以显著提高其量化质量。具体来说，我们主张使用块浮点格式，并展示了 BFP12 格式与 $4$-缓存的结合，可以在相同硬件上生成更长的序列。
- en: References
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Susan Zhang et al. “OPT: open pre-trained transformer language models”
    In *arXiv preprint arXiv:2205.01068*, 2022'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Susan Zhang 等. “OPT: 开放预训练变换器语言模型” 见 *arXiv 预印本 arXiv:2205.01068*，2022'
- en: '[2] Hugo Touvron et al. “Llama 2: open Foundation and Fine-Tuned Chat Models”
    In *arXiv preprint arXiv:2307.09288*, 2023'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Hugo Touvron 等. “Llama 2: 开放的基础和微调聊天模型” 见 *arXiv 预印本 arXiv:2307.09288*，2023'
- en: '[3] OpenAI “GPT-4 Technical Report” In *arXiv preprint arXiv:2303.08774*, 2024'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] OpenAI “GPT-4 技术报告” 见 *arXiv 预印本 arXiv:2303.08774*，2024'
- en: '[4] Albert Q. Jiang et al. “Mixtral of Experts” In *arXiv preprint arXiv:2401.04088*,
    2024'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Albert Q. Jiang 等. “专家的混合” 见 *arXiv 预印本 arXiv:2401.04088*，2024'
- en: '[5] Gemma Team et al. “Gemma: open Models Based on Gemini Research and Technology”
    In *arXiv preprint arXiv:2403.08295*, 2024'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Gemma Team 等. “Gemma: 基于 Gemini 研究和技术的开放模型” 见 *arXiv 预印本 arXiv:2403.08295*，2024'
- en: '[6] A. Vaswani et al. “Attention is all you need” In *Advances in Neural Information
    Processing Systems* 30, 2017'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Vaswani 等. “注意力即你所需” 见 *神经信息处理系统进展* 30, 2017'
- en: '[7] Coleman Hooper et al. “KVQuant: towards 10 Million Context Length LLM Inference
    with KV Cache Quantization” In *arXiv preprint arXiv:2401.18079*, 2024'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Coleman Hooper 等. “KVQuant: 朝着 1000 万上下文长度 LLM 推理与 KV 缓存量化” 见 *arXiv 预印本
    arXiv:2401.18079*，2024'
- en: '[8] Yiran Ding et al. “LongRoPE: Extending LLM Context Window Beyond 2 Million
    Tokens” In *arXiv preprint arXiv:2402.13753*, 2024'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Yiran Ding 等 “LongRoPE: 扩展 LLM 上下文窗口至 200 万个标记” 见于 *arXiv 预印本 arXiv:2402.13753*，2024
    年'
- en: '[9] Elias Frantar, Saleh Ashkboos, Torsten Hoefler and Dan Alistarh “GPTQ:
    accurate Post-Training Quantization for Generative Pre-trained Transformers” In
    *arXiv preprint arXiv:2210.17323*, 2023'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh “GPTQ: 精确的后训练量化生成预训练变换器”
    见于 *arXiv 预印本 arXiv:2210.17323*，2023 年'
- en: '[10] Guangxuan Xiao et al. “SmoothQuant: accurate and Efficient Post-Training
    Quantization for Large Language Models” In *arXiv preprint arXiv:2211.10438*,
    2023'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Guangxuan Xiao 等 “SmoothQuant: 大型语言模型的准确和高效的后训练量化” 见于 *arXiv 预印本 arXiv:2211.10438*，2023
    年'
- en: '[11] Y.. Wang, G.-Y. Wei and D. Brooks “Benchmarking TPU, GPU, and CPU platforms
    for deep learning” In *arXiv preprint arXiv:1907.10701*, 2019'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Y.. Wang, G.-Y. Wei 和 D. Brooks “TPU、GPU 和 CPU 平台的深度学习基准测试” 见于 *arXiv
    预印本 arXiv:1907.10701*，2019 年'
- en: '[12] A. Srinivas et al. “Bottleneck transformers for visual recognition” In
    *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 16519–16529'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Srinivas 等 “用于视觉识别的瓶颈变换器” 见于 *IEEE/CVF 计算机视觉与模式识别会议*，2021 年，第 16519–16529
    页'
- en: '[13] A.. Zadeh, I. Edo, O.. Awad and A. Moshovos “GOBO: quantizing attention-based
    NLP models for low latency and energy efficient inference” In *IEEE/ACM International
    Symposium on Microarchitecture*, 2020, pp. 811–824 IEEE'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] A.. Zadeh, I. Edo, O.. Awad 和 A. Moshovos “GOBO: 量化基于注意力的 NLP 模型以实现低延迟和节能推理”
    见于 *IEEE/ACM 微架构国际研讨会*，2020 年，第 811–824 页 IEEE'
- en: '[14] O. Zafrir, G. Boudoukh, P. Izsak and M. Wasserblat “Q8BERT: Quantized
    8bit BERT” In *Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS
    Edition*, 2019, pp. 36–39 IEEE'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] O. Zafrir, G. Boudoukh, P. Izsak 和 M. Wasserblat “Q8BERT: 量化的 8 位 BERT”
    见于 *节能机器学习与认知计算研讨会- NeurIPS 版*，2019 年，第 36–39 页 IEEE'
- en: '[15] S. Shen et al. “Q-BERT: hessian based ultra low precision quantization
    of BERT” In *Proceedings of the AAAI Conference on Artificial Intelligence* 34.05,
    2020, pp. 8815–8821'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. Shen 等 “Q-BERT: 基于 Hessian 的超低精度 BERT 量化” 见于 *AAAI 人工智能会议论文集* 34.05，2020
    年，第 8815–8821 页'
- en: '[16] W. Zhang et al. “TernaryBERT: distillation-aware ultra-low bit BERT” In
    *arXiv preprint arXiv:2009.12812*, 2020'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] W. Zhang 等 “TernaryBERT: 注重蒸馏的超低位 BERT” 见于 *arXiv 预印本 arXiv:2009.12812*，2020
    年'
- en: '[17] P. Micikevicius et al. “Mixed precision training” In *arXiv preprint arXiv:1710.03740*,
    2017'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] P. Micikevicius 等 “混合精度训练” 见于 *arXiv 预印本 arXiv:1710.03740*，2017 年'
- en: '[18] Bita Darvish Rouhani et al. “Pushing the limits of narrow precision inferencing
    at cloud scale with microsoft floating point” In *Advances in neural information
    processing systems* 33, 2020, pp. 10271–10281'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Bita Darvish Rouhani 等 “利用 Microsoft 浮点在云规模上推动狭窄精度推理的极限” 见于 *神经信息处理系统进展*
    33，2020 年，第 10271–10281 页'
- en: '[19] I. Lyubomirsky and X. Wang “Block Floating Point (BFP) for Efficient Deep
    Neural Net Inference” In *IEEE P3109 Working Group, June 6*, 2022'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] I. Lyubomirsky 和 X. Wang “块浮点（BFP）用于高效深度神经网络推理” 见于 *IEEE P3109 工作组，6 月
    6 日*，2022 年'
- en: '[20] I. Soloveychik, I. Lyubomirsky, X. Wang and S. Bhoja “Block Format Error
    Bounds and Optimal Block Size Selection” In *arXiv preprint arXiv:2210.05470*,
    2022'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] I. Soloveychik, I. Lyubomirsky, X. Wang 和 S. Bhoja “块格式误差界限和最优块大小选择” 见于
    *arXiv 预印本 arXiv:2210.05470*，2022 年'
- en: '[21] Microsoft “MX Pytorch Emulation Library” In *https://github.com/microsoft/microxcaling*,
    2023'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Microsoft “MX Pytorch 模拟库” 见于 *https://github.com/microsoft/microxcaling*，2023
    年'
- en: '[22] Bita Darvish Rouhani et al. “Microscaling Data Formats for Deep Learning”
    In *arXiv preprint arXiv:2310.10537*, 2023'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Bita Darvish Rouhani 等 “深度学习的微缩数据格式” 见于 *arXiv 预印本 arXiv:2310.10537*，2023
    年'
- en: '[23] Shuming Ma et al. “The Era of 1-bit LLMs: all large language lodels are
    in 1.58 bits” In *arXiv preprint arXiv:2402.17764*, 2024'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Shuming Ma 等 “1 位 LLM 时代：所有大型语言模型都在 1.58 位” 见于 *arXiv 预印本 arXiv:2402.17764*，2024
    年'
- en: '[24] Houwen Peng et al. “FP8-lm: Training FP8 large language models” In *arXiv
    preprint arXiv:2310.18313*, 2023'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Houwen Peng 等 “FP8-lm: 训练 FP8 大型语言模型” 见于 *arXiv 预印本 arXiv:2310.18313*，2023
    年'
- en: '[25] Paulius Micikevicius et al. “FP8 formats for deep learning” In *arXiv
    preprint arXiv:2209.05433*, 2022'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Paulius Micikevicius 等 “用于深度学习的 FP8 格式” 见于 *arXiv 预印本 arXiv:2209.05433*，2022
    年'
- en: '[26] Jianlin Su et al. “RoFormer: enhanced transformer with rotary position
    embedding” In *Neurocomputing* 568 Elsevier, 2024, pp. 127063'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Jianlin Su 等 “RoFormer: 增强的带旋转位置嵌入的变换器” 见于 *神经计算* 568 Elsevier，2024 年，第
    127063 页'
- en: '[27] Tim Dettmers, Mike Lewis, Younes Belkada and Luke Zettlemoyer “LLM.int8():
    8-bit Matrix Multiplication for Transformers at Scale” In *arXiv preprint arXiv:2208.07339*,
    2022'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Tim Dettmers、Mike Lewis、Younes Belkada 和 Luke Zettlemoyer “LLM.int8():
    8-bit Matrix Multiplication for Transformers at Scale” 见于 *arXiv 预印本 arXiv:2208.07339*，2022
    年'
- en: '[28] Stephen Merity, Caiming Xiong, James Bradbury and Richard Socher “Pointer
    sentinel mixture models” In *arXiv preprint arXiv:1609.07843*, 2016'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Stephen Merity、Caiming Xiong、James Bradbury 和 Richard Socher “Pointer
    sentinel mixture models” 见于 *arXiv 预印本 arXiv:1609.07843*，2016 年'
