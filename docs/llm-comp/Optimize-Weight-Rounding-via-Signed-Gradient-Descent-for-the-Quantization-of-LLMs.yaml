- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:51:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:51:18'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Optimize Weight Rounding via Signed Gradient Descent for the Quantization of
    LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过有符号梯度下降优化权重舍入以实现 LLMs 的量化
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.05516](https://ar5iv.labs.arxiv.org/html/2309.05516)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.05516](https://ar5iv.labs.arxiv.org/html/2309.05516)
- en: Wenhua Cheng  , Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He & Kaokao Lv
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 程文化, 张伟伟, 沈海浩, 蔡奕扬, 贺欣 & 吕考考
- en: Intel Corresponding author wenhua.cheng@intel.com
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Intel 通讯作者 wenhua.cheng@intel.com
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have proven their exceptional capabilities in performing
    language-related tasks. However, their deployment poses significant challenges
    due to their considerable memory and storage requirements. In response to this
    issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization,
    has emerged as one of the most viable solutions. As the number of bits decreases,
    the quantization grid broadens, thus emphasizing the importance of up and down
    rounding. While previous studies have demonstrated that fine-tuning up and down
    rounding with the addition of perturbations can enhance accuracy in some scenarios,
    our study is driven by the precise and limited boundary of these perturbations,
    where only the threshold for altering the rounding value is of significance. Consequently,
    we propose a concise and highly effective approach for optimizing the weight rounding
    task. Our method, named SignRound, involves lightweight block-wise tuning using
    signed gradient descent, enabling us to achieve outstanding results within 400
    steps. SignRound competes impressively against recent methods without introducing
    additional inference overhead. The source code will be publicly available at [https://github.com/intel/neural-compressor](https://github.com/intel/neural-compressor)
    soon.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经证明了其在执行语言相关任务中的卓越能力。然而，由于其巨大的内存和存储需求，它们的部署面临显著挑战。为应对这一问题，权重量化，特别是3位和4位权重量化，已成为最具可行性的解决方案之一。随着位数的减少，量化网格变得更广泛，从而强调了上下舍入的重要性。虽然先前的研究表明，通过增加扰动来微调上下舍入可以在某些情况下提高准确性，但我们的研究受限于这些扰动的精确且有限的边界，其中仅阈值的改变对舍入值有意义。因此，我们提出了一种简洁且高效的优化权重舍入任务的方法。我们的方法，称为
    SignRound，涉及使用有符号梯度下降进行轻量级的块级调优，使我们能够在400步内取得出色的结果。SignRound 在没有引入额外推理开销的情况下，表现出色地与最近的方法竞争。源代码将很快公开在
    [https://github.com/intel/neural-compressor](https://github.com/intel/neural-compressor)
    上。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have demonstrated exceptional proficiency on language-related
    tasks([OpenAI,](#bib.bib24) ; Touvron et al., [2023a](#bib.bib32)). Nevertheless,
    the deployment of LLMs presents notable hurdles due to their extensive memory
    and storage needs. Moreover, the computational demands of these models leads to
    the challenges for real-time applications. Consequently, it becomes imperative
    to explore techniques like quantization to facilitate the efficient deployment
    of LLMs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在语言相关任务上展示了卓越的能力（[OpenAI](#bib.bib24)；Touvron 等，[2023a](#bib.bib32)）。然而，LLMs
    的部署由于其庞大的内存和存储需求而面临显著挑战。此外，这些模型的计算需求给实时应用带来了困难。因此，探索如量化等技术以促进 LLMs 高效部署显得尤为重要。
- en: 'Quantization techniques can be broadly classified into two categories: quantization-aware
    training (QAT) (Esser et al., [2019](#bib.bib6); Zhuang et al., [2021](#bib.bib46);
    Lee et al., [2021](#bib.bib13); Liu et al., [2023](#bib.bib19)) and post-training
    quantization (PTQ) (Nagel et al., [2019](#bib.bib22); Xiao et al., [2022](#bib.bib38);
    Frantar et al., [2022](#bib.bib8); Nagel et al., [2020](#bib.bib23)). QAT involves
    training the model with quantization in mind. During QAT, the model is trained
    using simulated lower-precision representations, allowing it to learn and adapt
    to the effects of quantization. This approach often yields better accuracy compared
    to PTQ. However, QAT comes with certain drawbacks, including increased training
    complexity, longer training times, and the need to tune hyperparameters. Applying
    QAT to LLMs can be particularly costly, despite recent efforts (Hu et al., [2021](#bib.bib11);
    Dettmers et al., [2023](#bib.bib5)) to improve the efficiency of fine-tuning LLMs.
    In contrast, PTQ directly quantizes the model without any simulated training or
    fine-tuning. While PTQ is a concise approach, it is susceptible to significant
    accuracy drops. This highlights the need for further advancements in PTQ methods
    to enhance their accuracy preservation capabilities.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 量化技术可以大致分为两类：量化感知训练（QAT）（Esser et al.，[2019](#bib.bib6)；Zhuang et al.，[2021](#bib.bib46)；Lee
    et al.，[2021](#bib.bib13)；Liu et al.，[2023](#bib.bib19)）和训练后量化（PTQ）（Nagel et al.，[2019](#bib.bib22)；Xiao
    et al.，[2022](#bib.bib38)；Frantar et al.，[2022](#bib.bib8)；Nagel et al.，[2020](#bib.bib23)）。QAT涉及以量化为目的对模型进行训练。在QAT过程中，模型使用模拟的低精度表示进行训练，从而使其学习和适应量化的效果。这种方法通常比PTQ具有更好的准确性。然而，QAT也有一定的缺点，包括增加了训练复杂性、延长了训练时间以及需要调整超参数。尽管最近有努力（Hu
    et al.，[2021](#bib.bib11)；Dettmers et al.，[2023](#bib.bib5)）以提高LLMs微调的效率，但应用QAT于LLMs仍然特别昂贵。相比之下，PTQ直接对模型进行量化，无需任何模拟训练或微调。虽然PTQ是一种简洁的方法，但容易导致显著的准确性下降。这突显了在PTQ方法上进一步提升准确性保留能力的必要性。
- en: 'Two types of tensors could be quantized: activations and weights. Weight-only
    quantization has gained prominence in recent times as it offers a favorable tradeoff
    for LLMs. Quantizing activations for LLMs can be challenging (Wei et al., [2022b](#bib.bib36);
    Xiao et al., [2023](#bib.bib39); Bondarenko et al., [2023](#bib.bib3)), making
    weight-only quantization a more practical choice. Additionally, the primary bottleneck
    in generating new tokens for LLMs often lies in memory bandwidth (Kim et al.,
    [2023](#bib.bib12)), further emphasizing the significance of weight-only quantization.
    In this work, we only focus on weight only quantization.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 两种类型的张量可以被量化：激活和权重。仅对权重进行量化近年来获得了关注，因为它为LLMs提供了有利的权衡。对LLMs进行激活量化可能具有挑战性（Wei
    et al.，[2022b](#bib.bib36)；Xiao et al.，[2023](#bib.bib39)；Bondarenko et al.，[2023](#bib.bib3)），使得仅对权重进行量化成为更实际的选择。此外，生成新令牌的主要瓶颈通常在于内存带宽（Kim
    et al.，[2023](#bib.bib12)），进一步强调了仅对权重进行量化的重要性。在这项工作中，我们只关注仅对权重进行量化。
- en: In order to quantize the weights, a rounding operation is necessary, with rounding-to-nearest
    (RTN) being the predominant method. RTN quantizes each element independently by
    simply rounding it to the nearest integer. However, RTN fails to consider the
    relationships between weights and weights, as well as weights and activations.
    The potential of an advanced rounding strategy to improve accuracy has been initially
    demonstrated by Nagel et al. (Nagel et al., [2020](#bib.bib23)). They addressed
    the rounding task by formulating it as a quadratic unconstrained binary optimization
    problem and approximated the task loss by employing a Taylor series expansion.
    However, relying exclusively on the second-order term may not produce accurate
    results. This is because rounding can introduce considerable weight modifications
    that may make other order terms significant and non-negligible.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化权重，必须进行舍入操作，其中舍入到最近整数（RTN）是主要方法。RTN通过简单地将每个元素舍入到最接近的整数来独立地量化每个元素。然而，RTN未能考虑权重与权重之间以及权重与激活之间的关系。Nagel
    et al.（Nagel et al.，[2020](#bib.bib23)）初步展示了先进舍入策略提升准确性的潜力。他们通过将舍入任务表述为一个二次无约束二进制优化问题，并使用泰勒级数展开来近似任务损失来解决这一任务。然而，单独依赖二阶项可能不会产生准确的结果。这是因为舍入可能会引入相当大的权重修改，使得其他阶的项变得重要且不可忽视。
- en: 'We prefer the signed gradient descent method to effectively tackle the issue
    of sub-optimal rounding solutions. This approach is inspired by the well-defined
    boundaries of the solution space, which are confined to the range of [-0.5, 0.5],
    where only the threshold for altering the rounding value is of significance. Figure
    [1](#S3.F1 "Figure 1 ‣ 3.1 SignRound ‣ 3 Methodology ‣ Optimize Weight Rounding
    via Signed Gradient Descent for the Quantization of LLMs") provides an overview
    of our method, SignRound. It utilizes signed gradient descent to fine-tune the
    up and down rounding through block-wise output reconstruction, resulting in enhanced
    flexibility and faster convergence. Our contributions are primarily threefold:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更倾向于使用符号梯度下降法来有效解决次优舍入解决方案的问题。该方法受到解决空间明确边界的启发，这些边界被限制在[-0.5, 0.5]的范围内，在这个范围内，只有改变舍入值的阈值是重要的。图[1](#S3.F1
    "图 1 ‣ 3.1 SignRound ‣ 3 方法 ‣ 通过符号梯度下降优化LLMs的权重舍入")提供了我们方法SignRound的概述。它利用符号梯度下降通过块状输出重建来微调向上和向下的舍入，从而实现了更大的灵活性和更快的收敛。我们的贡献主要有三方面：
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a succinct and potent method for optimizing the weight-rounding
    task. Our approach utilizes a minimal amount of unlabeled data and executes quantization
    in a block-wise fashion. Moreover, it is worth noting that our method does not
    introduce any additional overhead during inference, further enhancing its general
    practicality.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了一种简洁而有效的方法来优化权重舍入任务。我们的方法利用了最少量的未标记数据，并以块状方式执行量化。此外，值得注意的是，我们的方法在推理过程中不会引入额外的开销，从而进一步提升了其实际应用性。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our findings demonstrate that a mere alteration of approximately 5% of the rounding
    values can significantly enhance the performance of some quantization models.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的发现表明，仅仅改变约5%的舍入值就可以显著提高某些量化模型的性能。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our empirical results exhibit substantial performance enhancements over the
    established baseline of RTN, and our method contends favorably against recent
    techniques.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的实证结果显示，相比于RTN的既定基准，我们的方法在性能上有显著提升，并且在与近期技术的比较中表现良好。
- en: 2 Related Work
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Quantization Aware Training.
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化感知训练。
- en: QAT methods have gained widespread popularity in model compression, as they
    enable the fine-tuning process, often leading to superior accuracy compared to
    the PTQ method. In their work, (Esser et al., [2019](#bib.bib6)) proposed a novel
    approach that estimates and scales the task loss gradient at each weight and activation
    layer’s quantizer step size, allowing for joint learning with other network parameters. (Zhuang
    et al., [2021](#bib.bib46)) put forward a progressive quantization scheme that
    involves quantizing activations after weights. Additionally, CPQ (Lee et al.,
    [2021](#bib.bib13)) effectively identified the optimal quantization grids while
    naturally encouraging the underlying full-precision weights to gather around those
    quantization grids cohesively during training. While QAT methods are popular in
    relatively small-scale models, their application in LLMs is limited due to the
    high computational cost associated with training or fine-tuning.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: QAT方法在模型压缩中获得了广泛的关注，因为它们能够进行微调，通常会带来比PTQ方法更好的准确性。在他们的研究中，（Esser等， [2019](#bib.bib6)）提出了一种新颖的方法，通过估计并调整每个权重和激活层量化器步骤的任务损失梯度，实现与其他网络参数的联合学习。（Zhuang等，[2021](#bib.bib46)）提出了一种渐进量化方案，涉及在权重量化后对激活进行量化。此外，CPQ（Lee等，[2021](#bib.bib13)）有效地识别了最佳量化网格，同时在训练过程中自然地促使底层全精度权重围绕这些量化网格聚集。虽然QAT方法在相对小规模模型中非常流行，但由于训练或微调相关的高计算成本，其在LLM中的应用受到限制。
- en: Post-training Quantization (PTQ).
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练后量化（PTQ）。
- en: PTQ methods simplify the quantization process without the needs of additional
    training. (Nagel et al., [2019](#bib.bib22)) focused on minimizing quantization
    error through weight equalization and bias correction techniques. (Liu et al.,
    [2021](#bib.bib20)) specifically addressed the quantization of vision transformers,
    introducing a ranking loss to preserve the relative order of self-attention results
    after quantization and exploring a mixed-precision quantization scheme. (Frantar
    & Alistarh, [2022](#bib.bib7)) leveraged Optimal Brain Surgeon (Hassibi et al.,
    [1993](#bib.bib10)) to tune weights during model compression. Both Hawq (Yao et al.,
    [2021](#bib.bib40)) and HAQ (Wang et al., [2019](#bib.bib34)) aimed to identify
    important layers and maintain higher precision for them. Given its low resource
    requirement, PTQ is particularly suitable for the quantization of Large Language
    Models (LLMs). We will next focus on the quantization methods designed for LLMs,
    most of which fall under the category of PTQ.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: PTQ方法简化了量化过程，无需额外的训练。（Nagel等，[2019](#bib.bib22)）专注于通过权重均衡和偏差校正技术来最小化量化误差。（Liu等，[2021](#bib.bib20)）特别解决了视觉变换器的量化问题，引入了排名损失来保留量化后自注意力结果的相对顺序，并探索了混合精度量化方案。（Frantar
    & Alistarh，[2022](#bib.bib7)）利用Optimal Brain Surgeon（Hassibi等，[1993](#bib.bib10)）在模型压缩过程中调整权重。Hawq（Yao等，[2021](#bib.bib40)）和HAQ（Wang等，[2019](#bib.bib34)）都旨在识别重要层并保持较高的精度。鉴于其低资源需求，PTQ特别适合用于大型语言模型（LLMs）的量化。接下来我们将重点关注为LLMs设计的量化方法，这些方法大多数属于PTQ类别。
- en: Large Language Models Quantization.
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大型语言模型量化。
- en: Significant advancements have been made in addressing the pressing demand for
    quantizing large language models (LLMs). LLM.int8() (Dettmers et al., [2022](#bib.bib4))
    introduced a mixed precision approach to preserve essential channels in high precision.
    ZeroQuantV2 (Yao et al., [2023](#bib.bib41)) employed low-rank matrices to enhance
    model quality recovery. RPTQ (Yuan et al., [2023](#bib.bib42)) mitigated the impact
    of range differences between channel by rearranging the channels and quantizing
    them in clusters. Other methods, such as SPIQ (Yvinec et al., [2023](#bib.bib43)),
    SmoothQuant (Xiao et al., [2022](#bib.bib38)), Outlier Suppression+ (Wei et al.,
    [2023](#bib.bib37)), utilized handcrafted equivalent transformations to mitigate
    quantization errors. While these approaches are effective, their applicability
    is limited due to the performance overhead involved during inference, because
    there is no chance to fuse the transformation scale to the model itself on certain
    model architectures. LLM-QAT (Liu et al., [2023](#bib.bib19)) employs QAT to enhance
    the performance of W4A8. In the context of weight-only quantization, GPTQ (Frantar
    et al., [2022](#bib.bib8)) optimized weights using the Optimal Brain Surgeon (Hassibi
    et al., [1993](#bib.bib10)) technique, achieving low-bit quantization on LLMs
    with minimal computational overhead. AWQ (Lin et al., [2023](#bib.bib18)) followed
    the equivalent transformation approach with additional tuning in a constrained
    space, and has the similar limitations as SmoothQuant (Xiao et al., [2022](#bib.bib38)).
    SqueezeLLM (Kim et al., [2023](#bib.bib12)) employed sensitivity-based non-uniform
    quantization and dense-and-sparse decomposition to achieve lossless compression
    to ultra-low precision. While recent advancements in LLM quantization have made
    significant progress, there is still room for improvement in achieving minimal
    quantization loss without introducing inference overhead.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决对大规模语言模型（LLMs）量化的迫切需求方面，已取得了显著进展。LLM.int8()（Dettmers 等人，[2022](#bib.bib4)）引入了一种混合精度方法，以在高精度下保留关键通道。ZeroQuantV2（Yao
    等人，[2023](#bib.bib41)）采用了低秩矩阵来增强模型质量的恢复。RPTQ（Yuan 等人，[2023](#bib.bib42)）通过重新排列通道并将其分组量化来减轻通道之间范围差异的影响。其他方法，如
    SPIQ（Yvinec 等人，[2023](#bib.bib43)），SmoothQuant（Xiao 等人，[2022](#bib.bib38)），Outlier
    Suppression+（Wei 等人，[2023](#bib.bib37)），利用手工设计的等效变换来减轻量化误差。虽然这些方法有效，但由于推理过程中涉及的性能开销，它们的适用性受到限制，因为在某些模型架构上，无法将变换尺度融合到模型本身。LLM-QAT（Liu
    等人，[2023](#bib.bib19)）采用 QAT 来提升 W4A8 的性能。在仅对权重进行量化的背景下，GPTQ（Frantar 等人，[2022](#bib.bib8)）使用
    Optimal Brain Surgeon（Hassibi 等人，[1993](#bib.bib10)）技术来优化权重，实现了在 LLMs 上进行低位量化，计算开销最小。AWQ（Lin
    等人，[2023](#bib.bib18)）在等效变换方法的基础上进行了额外调优，并在受限空间中具有与 SmoothQuant（Xiao 等人，[2022](#bib.bib38)）类似的局限性。SqueezeLLM（Kim
    等人，[2023](#bib.bib12)）采用基于敏感性的非均匀量化和密集与稀疏分解，实现了超低精度下的无损压缩。尽管最近在 LLM 量化方面取得了显著进展，但在实现最小量化损失而不引入推理开销方面仍有改进空间。
- en: Rounding Methods.
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 四舍五入方法。
- en: Adaptive Rounding (Nagel et al., [2020](#bib.bib23)) has already showcased the
    potential of an advanced rounding strategy to enhance accuracy (Li et al., [2021](#bib.bib16);
    Wei et al., [2022a](#bib.bib35)). They used the rounding task as a quadratic unconstrained
    binary optimization problem by approximating the task loss through a Taylor series
    expansion. However, considering only the second-order term may not yield accurate
    results. This is because the rounding value gets multiplied by a scaling coefficient
    during de-quantization, potentially introducing significant weight changes that
    make other order terms non-negligible. FlexRound (Lee et al., [2023](#bib.bib14))
    introduces a more flexible approach to rounding by incorporating element-wise
    division. This allows for simultaneous learning of a shared quantization grid
    size and individual scales for each pre-trained weight. However, it’s not easily
    scalable to apply to LLMs due to the needs of specialized hyperparameters for
    each specific model and task. AQuant (Li et al., [2022](#bib.bib17)) introduced
    a dynamic approach where the border becomes a function dependent on the activation
    value to reduce the quantization error of activation. We specifically concentrate
    on the up and down rounding task for weight quantization in this work.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应舍入（Nagel et al., [2020](#bib.bib23)）已经展示了高级舍入策略在提高准确性方面的潜力（Li et al., [2021](#bib.bib16);
    Wei et al., [2022a](#bib.bib35)）。他们将舍入任务作为一个二次无约束的二进制优化问题，通过泰勒级数展开来近似任务损失。然而，仅考虑二阶项可能无法得到准确结果。这是因为在去量化过程中，舍入值会被一个缩放系数乘以，这可能引入显著的权重变化，使其他阶的项变得不可忽视。FlexRound（Lee
    et al., [2023](#bib.bib14)）通过引入逐元素除法来提出一种更灵活的舍入方法。这允许同时学习共享的量化网格大小和每个预训练权重的单独缩放。然而，由于每个特定模型和任务都需要专门的超参数，这种方法不容易扩展到大型语言模型（LLMs）。AQuant（Li
    et al., [2022](#bib.bib17)）引入了一种动态方法，其中边界成为一个依赖于激活值的函数，以减少激活的量化误差。在这项工作中，我们专注于权重量化的上舍入和下舍入任务。
- en: Signed Gradient Descent.
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 签名梯度下降。
- en: Signed gradient descent is not commonly utilized and is typically applied in
    specific scenarios, such as reducing communication costs. This is because signed
    gradient carries significantly less information compared to original gradient.
    Recent studies have shed light on the advantages of sign-based methods over gradient
    descent in certain conditions. Safaryan et al. (Safaryan & Richtárik, [2021](#bib.bib29))
    found that sign-based methods are preferable when the Hessian matrix is concentrated
    on its diagonal and the maximal eigenvalue is much larger than the average eigenvalue.
    Li et al. (Li et al., [2023](#bib.bib15)) investigated a variant of sign-based
    gradient descent that exhibits faster convergence. Additionally, Safaryan et al.
    (Safaryan & Richtárik, [2021](#bib.bib29)) proposed a stochastic sign descent
    with momentum, which converges under the standard bounded variance assumption
    with the optimal asymptotic rate. These findings contribute to a better understanding
    of the potential benefits and applications of signed gradient descent methods.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 签名梯度下降不常被使用，通常应用于特定场景，例如降低通信成本。这是因为签名梯度相比于原始梯度携带的信息显著更少。最近的研究揭示了在某些条件下，基于签名的方法相较于梯度下降的优势。Safaryan
    et al.（Safaryan & Richtárik, [2021](#bib.bib29)）发现，当海森矩阵集中在其对角线上且最大特征值远大于平均特征值时，基于签名的方法更为优越。Li
    et al.（Li et al., [2023](#bib.bib15)）研究了一种签名梯度下降的变体，该变体表现出更快的收敛速度。此外，Safaryan
    et al.（Safaryan & Richtárik, [2021](#bib.bib29)）提出了一种带有动量的随机签名下降，在标准有界方差假设下以最优渐近速度收敛。这些发现有助于更好地理解签名梯度下降方法的潜在好处和应用。
- en: 3 Methodology
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: We provide an overview of quantization before diving into the details of our
    approach. To quantize and de-quantize the weights, the following operation as
    shown in Eq.[1](#S3.E1 "In 3 Methodology ‣ Optimize Weight Rounding via Signed
    Gradient Descent for the Quantization of LLMs") is used (disregarding zero point
    for simplicity).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入我们的方法细节之前，我们先提供一个量化的概述。为了对权重进行量化和去量化，如 Eq.[1](#S3.E1 "In 3 Methodology ‣
    Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs")
    所示的操作（为简化起见，忽略零点）被使用。
- en: '|  | $\widetilde{W}=s*clip(\left\lfloor\frac{W}{s}\right\rceil,n,m),n,m\in\mathbb{N}$
    |  | (1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widetilde{W}=s*clip(\left\lfloor\frac{W}{s}\right\rceil,n,m\in\mathbb{N}$
    |  | (1) |'
- en: where $s$ is typically performed using the RTN method. While RTN is a concise
    approach, it quantizes each element independently, thereby losing the ability
    to model the correlation among different weights or activations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s$ 通常使用 RTN 方法来完成。虽然 RTN 是一种简洁的方法，但它独立地量化每个元素，因此失去了建模不同权重或激活之间相关性的能力。
- en: To introduce more flexibility into the rounding operation, a tensor $V$ is set
    to 0.5 in all of experiments to ensure that the changes made only impact the rounding
    value.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在舍入操作中引入更多灵活性，在所有实验中，将张量 $V$ 设置为 0.5，以确保所做的更改仅影响舍入值。
- en: '|  | $\widetilde{W}=s*clip(\left\lfloor\frac{W}{s}+V\right\rceil,n,m),n,m\in\mathbb{N}$
    |  | (2) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widetilde{W}=s*clip(\left\lfloor\frac{W}{s}+V\right\rceil,n,m\in\mathbb{N}$
    |  | (2) |'
- en: This adjustment allows for a more adaptable and context-aware quantization process.
    If we try to reconstruct the output of layers, the loss could be formulated as
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种调整使量化过程更加灵活和具有上下文感知能力。如果我们尝试重建层的输出，则损失可以表述为
- en: '|  | $L=&#124;&#124;WX-\widetilde{W}X&#124;&#124;_{F}^{2}$ |  | (3) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $L=&#124;&#124;WX-\widetilde{W}X&#124;&#124;_{F}^{2}$ |  | (3) |'
- en: where $X$ denotes the Frobenius norm. Then the final optimization task is described
    as the following
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $X$ 表示 Frobenius 范数。然后最终的优化任务描述如下
- en: '|  | $\mathop{\arg min}\limits_{V}&#124;&#124;WX-\widetilde{W}X&#124;&#124;_{F}^{2}$
    |  | (4) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathop{\arg min}\limits_{V}&#124;&#124;WX-\widetilde{W}X&#124;&#124;_{F}^{2}$
    |  | (4) |'
- en: 3.1 SignRound
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 SignRound
- en: '![Refer to caption](img/f5d247629e27eb1e96951492c886b1fd.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f5d247629e27eb1e96951492c886b1fd.png)'
- en: 'Figure 1: An illustration of SignRound. Unlike the direct rounding in RTN,
    SignRound performs signed gradient descent to fine-tune the up and down rounding
    through block-wise output reconstruction. After lightweight forward and backward
    steps, WINT4 has been well optimized towards the minimal loss, therefore ready
    for the final inference deployment. Note that Quant and Dequant are two standard
    operations for quantization and dequantization respectively.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：SignRound 的示意图。与 RTN 中的直接舍入不同，SignRound 通过块级输出重建来执行带符号的梯度下降，以微调上舍入和下舍入。在轻量级的前向和反向步骤之后，WINT4
    已经针对最小损失进行了良好的优化，因此可以进行最终的推理部署。请注意，Quant 和 Dequant 分别是量化和反量化的两个标准操作。
- en: Since $V$, and only the threshold for altering the rounding value is of significance,
    we prefer scaled signed gradient descent instead of normal gradient descent to
    optimize this task. Figure [1](#S3.F1 "Figure 1 ‣ 3.1 SignRound ‣ 3 Methodology
    ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization of
    LLMs") shows an illustration of our method. More precisely, we follow the below
    optimization to approach the sub-optimal solution of Eq. [4](#S3.E4 "In 3 Methodology
    ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization of
    LLMs").
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $V$，以及仅改变舍入值的阈值才是重要的，我们更倾向于使用缩放的带符号梯度下降而不是普通的梯度下降来优化此任务。图 [1](#S3.F1 "图 1
    ‣ 3.1 SignRound ‣ 3 方法 ‣ 通过带符号的梯度下降优化权重舍入以进行 LLMs 的量化") 显示了我们方法的示意图。更准确地说，我们遵循以下优化来接近
    Eq. [4](#S3.E4 "在 3 方法 ‣ 通过带符号的梯度下降优化权重舍入以进行 LLMs 的量化") 的次优解。
- en: '|  |  | $\displaystyle V_{t+1}=V_{t}-lr*sign(\frac{\partial L}{\partial V})$
    |  | (5) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle V_{t+1}=V_{t}-lr*sign(\frac{\partial L}{\partial V})$
    |  | (5) |'
- en: '|  |  | $\displaystyle\mathrm{s.t.}&#124;\sum_{t}lr*sign(\frac{\partial L}{\partial
    V})&#124;\leq B$ |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathrm{s.t.}&#124;\sum_{t}lr*sign(\frac{\partial L}{\partial
    V})&#124;\leq B$ |  |'
- en: where $t$ is the absolute operation and B is the boundary we use, which is set
    to 0.5 in all our experiments.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t$ 是绝对操作，B 是我们使用的边界，在所有实验中设置为 0.5。
- en: Further, by employing straight-through estimator (STE) (Bengio et al., [2013](#bib.bib1)),
    it can be easily demonstrated that $sign(\frac{\partial L}{\partial V})=sign(\frac{\partial
    L}{\partial W})$ are all positive.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过使用直通估计器 (STE) (Bengio 等人，[2013](#bib.bib1))，可以很容易地证明 $sign(\frac{\partial
    L}{\partial V})=sign(\frac{\partial L}{\partial W})$ 都是正的。
- en: '|  | $\frac{\partial L}{\partial W}=-2(WX-\widetilde{W}X)X^{T}$ |  | (6) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial L}{\partial W}=-2(WX-\widetilde{W}X)X^{T}$ |  | (6) |'
- en: '|  | $\frac{\partial L}{\partial V}=-2s(WX-\widetilde{W}X)X^{T}$ |  | (7) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial L}{\partial V}=-2s(WX-\widetilde{W}X)X^{T}$ |  | (7) |'
- en: So our optimization could be simplified as
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的优化可以简化为
- en: '|  |  | $\displaystyle V_{t+1}=V_{t}-lr*sign(\frac{\partial L}{\partial W})$
    |  | (8) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle V_{t+1}=V_{t}-lr*sign(\frac{\partial L}{\partial W})$
    |  | (8) |'
- en: '|  |  | $\displaystyle\mathrm{s.t.}&#124;\sum_{t}lr*sign(\frac{\partial L}{\partial
    W})&#124;\leq B$ |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathrm{s.t.}&#124;\sum_{t}lr*sign(\frac{\partial L}{\partial
    W})&#124;\leq B$ |  |'
- en: Moreover, as Eq [3](#S3.E3 "In 3 Methodology ‣ Optimize Weight Rounding via
    Signed Gradient Descent for the Quantization of LLMs") averages the loss of each
    element, which presumes that each one contributes equally to the network, that
    basically is not true. To alleviate this issue, we optimize the rounding task
    blockwise. To clarify, in our context, we use the term ’layer’ to refer to a linear/convolution
    layer, while ’block’ denotes a transformer block that typically consists of several
    linear layers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于公式[3](#S3.E3 "在 3 方法 ‣ 通过签名梯度下降优化 LLM 量化的权重舍入")平均每个元素的损失，这假设每个元素对网络的贡献是相等的，但这基本上并不正确。为了解决这个问题，我们在块级别优化舍入任务。为了澄清，在我们的上下文中，我们使用“层”一词来指代线性/卷积层，而“块”表示一个变换器块，该块通常由几个线性层组成。
- en: The above pseudocode [1](#alg1 "Algorithm 1 ‣ 3.1 SignRound ‣ 3 Methodology
    ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization of
    LLMs") presents more details of SignRound.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 上述伪代码[1](#alg1 "算法 1 ‣ 3.1 SignRound ‣ 3 方法 ‣ 通过签名梯度下降优化 LLM 量化的权重舍入")提供了更多关于
    SignRound 的细节。
- en: Algorithm 1 SignRound
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 SignRound
- en: 'Input: Calibration Data $\mathcal{D}$'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：校准数据 $\mathcal{D}$
- en: 'Output: $best\_V$'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：$best\_V$
- en: 1:$V\leftarrow 0$14:     update V via Eq. [8](#S3.E8 "In 3.1 SignRound ‣ 3 Methodology
    ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization of
    LLMs")15:end for
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 1:$V\leftarrow 0$14:     通过公式[8](#S3.E8 "在 3.1 SignRound ‣ 3 方法 ‣ 通过签名梯度下降优化
    LLM 量化的权重舍入")更新 V15:结束 for
- en: 4 Experiments
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this section, we conduct a comprehensive evaluation of SignRound from various
    perspectives. Firstly, we provide a brief overview of the LLM architectures and
    tasks that are included in our evaluation. Secondly, we present a detailed comparison
    between our method and some other existing approaches, highlighting the unique
    features and advantages of SignRound. Thirdly, we conduct additional experiments
    to further demonstrate the validity of our choices, assess the sensitivity of
    hyperparameters, and explore other relevant factors. Finally, the runtime is reported
    in Appendix [D](#A4 "Appendix D Runtime ‣ Optimize Weight Rounding via Signed
    Gradient Descent for the Quantization of LLMs") for reference.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从多个角度对 SignRound 进行全面评估。首先，我们简要概述了包含在我们评估中的 LLM 架构和任务。其次，我们详细比较了我们的方法与一些其他现有方法，突出了
    SignRound 的独特特点和优势。第三，我们进行了额外实验，以进一步验证我们的选择的有效性，评估超参数的敏感性，并探索其他相关因素。最后，运行时间在附录[D](#A4
    "附录 D 运行时间 ‣ 通过签名梯度下降优化 LLM 量化的权重舍入")中报告以供参考。
- en: 4.1 Experimental Settings
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Evaluation and Datasets.
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估和数据集。
- en: We make assessments on several language tasks to satisfy the task-agnostic setting.
    Specifically, we report average accuracy results on four common sense reasoning
    tasks including HellaSwag (Zellers et al., [2019](#bib.bib44)), WinoGrande (Sakaguchi
    et al., [2021](#bib.bib30)), PIQA (Bisk et al., [2020](#bib.bib2)) and LAMBADA
    (Paperno et al., [2016](#bib.bib25)).Additionally, we benchmarked our models on
    MMLU (hendrycks2020measuring), which encompasses 57 tasks spanning STEM, humanities,
    social science, and more. Evaluation for all these tasks was performed using the
    lm-eval-harness (Gao et al., [2021](#bib.bib9)). Furthermore, we complement our
    evaluation with perplexity (ppl) analysis on Wikitext2 (Merity et al., [2016](#bib.bib21))
    and C4 (Raffel et al., [2020](#bib.bib27)), by following the source code ¹¹1https://github.com/IST-DASLab/gptq
    of GPTQ.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在多个语言任务上进行评估，以满足任务无关的设置。具体而言，我们报告了四个常识推理任务的平均准确率结果，包括 HellaSwag (Zellers et
    al., [2019](#bib.bib44))、WinoGrande (Sakaguchi et al., [2021](#bib.bib30))、PIQA
    (Bisk et al., [2020](#bib.bib2)) 和 LAMBADA (Paperno et al., [2016](#bib.bib25))。此外，我们在
    MMLU (hendrycks2020measuring) 上对我们的模型进行了基准测试，该数据集涵盖了 STEM、人文学科、社会科学等 57 个任务。所有这些任务的评估使用了
    lm-eval-harness (Gao et al., [2021](#bib.bib9))。此外，我们通过遵循源代码¹¹1https://github.com/IST-DASLab/gptq
    对 Wikitext2 (Merity et al., [2016](#bib.bib21)) 和 C4 (Raffel et al., [2020](#bib.bib27))
    进行了困惑度 (ppl) 分析，以补充我们的评估。
- en: Quantization Configurations.
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化配置。
- en: In line with the approach taken in GPTQ (Frantar et al., [2022](#bib.bib8)),
    we specifically concentrate on weight-only quantization, targeting the linear
    layers within transformer blocks. Other layers, such as the embedding layer and
    typically the last layer like lm-head, are excluded from the quantization process.
    We initially intended to utilize the pile (gao2020pile) dataset for calibration,
    following AWQ (Lin et al., [2023](#bib.bib18)) and SmoothQuant (Xiao et al., [2022](#bib.bib38)).
    However, due to its large size, we have opted to use the readily available pile-10k
    dataset ²²2https://huggingface.co/datasets/NeelNanda/pile-10k, which consists
    of the first 10k samples from pile, for both GPTQ and our method. We employ standard
    uniform per-row asymmetric quantization on the min-max grid. Our evaluation primarily
    focuses on W4, W4G128, and W3G128, where W4 indicates quantizing weights with
    4 bits and G represents finer-granularity grouping as described in (Park et al.,
    [2022](#bib.bib26); Frantar et al., [2022](#bib.bib8)).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与GPTQ（Frantar et al., [2022](#bib.bib8)）的方法一致，我们特别关注仅对权重进行量化，目标是变换器块中的线性层。其他层，如嵌入层以及通常的最后一层，如lm-head，都被排除在量化过程之外。我们最初打算使用pile
    (gao2020pile) 数据集进行校准，遵循AWQ（Lin et al., [2023](#bib.bib18)）和SmoothQuant（Xiao et
    al., [2022](#bib.bib38)）。然而，由于其规模庞大，我们选择使用现成的pile-10k数据集²²2https://huggingface.co/datasets/NeelNanda/pile-10k，该数据集包含pile中的前10k样本，用于GPTQ和我们的方法。我们在最小-最大网格上采用标准的均匀每行不对称量化。我们的评估主要集中在W4、W4G128和W3G128，其中W4表示使用4位对权重进行量化，G代表更细粒度的分组，如（Park
    et al., [2022](#bib.bib26); Frantar et al., [2022](#bib.bib8)）中所描述。
- en: Large Language Models.
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大型语言模型。
- en: Our experimental evaluation encompasses a range of widely adopted LLM architectures,
    such as LLaMAs (Touvron et al., [2023a](#bib.bib32)), LLaMAs v2 (Touvron et al.,
    [2023b](#bib.bib33)), BLOOMs (Scao et al., [2022](#bib.bib31)), and OPTs (Zhang
    et al., [2022](#bib.bib45)). We cover a wide range of LLM parameters, ranging
    from millions to billions, to ensure comprehensive coverage and analysis.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验评估涵盖了多种广泛采用的LLM架构，如LLaMAs（Touvron et al., [2023a](#bib.bib32)）、LLaMAs v2（Touvron
    et al., [2023b](#bib.bib33)）、BLOOMs（Scao et al., [2022](#bib.bib31)）和OPTs（Zhang
    et al., [2022](#bib.bib45)）。我们涵盖了从百万到十亿不等的LLM参数，以确保全面覆盖和分析。
- en: SignRound Hyperparameters.
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SignRound 超参数。
- en: We selected 512 samples randomly from pile-10k and truncated each sample to
    a sequence length of 512\. The tuning process involves adjusting each block for
    400 steps using a learning rate of 2.5e-3, a batch size of 8, and employing a
    linear learning rate decay. We set the value of B in Eq. [8](#S3.E8 "In 3.1 SignRound
    ‣ 3 Methodology ‣ Optimize Weight Rounding via Signed Gradient Descent for the
    Quantization of LLMs") to 0.5\. Besides, we adopted automatic mixed precision(AMP)
    to accelerate the tuning. It’s worth noting that adjusting the sequence length
    to 2048 yielded improvements in numerous scenarios. However, we did not adopt
    this as the default setting due to the associated runtime overhead. For models
    $\geq$ 30B, we made configuration adjustments to strike a balance between runtime
    and performance. Specifically, we reduced the sample count to 256, shorted the
    sequence length to 256, and disabled AMP.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从pile-10k中随机选择了512个样本，并将每个样本截断为长度为512的序列。调优过程涉及使用2.5e-3的学习率、8的批量大小，并采用线性学习率衰减来调整每个块400步。我们将公式[8](#S3.E8
    "在3.1 SignRound ‣ 3 方法论 ‣ 通过带符号梯度下降优化LLM的权重量化")中的B值设置为0.5。此外，我们采用了自动混合精度（AMP）来加速调优。值得注意的是，将序列长度调整为2048在许多场景中带来了改进。然而，由于相关的运行时间开销，我们没有将其作为默认设置。对于模型$\geq$
    30B，我们进行了配置调整，以在运行时间和性能之间取得平衡。具体而言，我们将样本数量减少到256，将序列长度缩短到256，并禁用了AMP。
- en: 'Table 1: Average % accuracy($\uparrow$) of HellaSwag, WinoGrand, PIQA and LAMBADA
    for LLaMA & OPT.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：LLaMA & OPT在HellaSwag、WinoGrand、PIQA和LAMBADA上的平均准确率（%）($\uparrow$)。
- en: '| nbits | methods | LLaMA | OPT |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| nbits | methods | LLaMA | OPT |'
- en: '| --- | --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 7b | 13b | 7bv2 | 13bv2 | 125m | 1.3b | 2.7b | 6.7b | 13b |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 7b | 13b | 7bv2 | 13bv2 | 125m | 1.3b | 2.7b | 6.7b | 13b |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 16 | FP16 | 68.80 | 71.14 | 69.02 | 71.20 | 45.09 | 57.66 | 61.04 | 64.92
    | 65.49 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 16 | FP16 | 68.80 | 71.14 | 69.02 | 71.20 | 45.09 | 57.66 | 61.04 | 64.92
    | 65.49 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| W4 | RTN | 67.38 | 68.82 | 66.98 | 70.17 | 39.41 | 47.22 | 58.61 | 62.99
    | 64.08 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| W4 | RTN | 67.38 | 68.82 | 66.98 | 70.17 | 39.41 | 47.22 | 58.61 | 62.99
    | 64.08 |'
- en: '| GPTQ | 64.70 | 70.00 | 66.89 | 69.24 | 43.58 | 56.15 | 59.92 | 63.09 | 64.83
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 64.70 | 70.00 | 66.89 | 69.24 | 43.58 | 56.15 | 59.92 | 63.09 | 64.83
    |'
- en: '| Ours | 68.05 | 70.58 | 67.74 | 70.03 | 44.13 | 56.17 | 60.58 | 64.34 | 65.05
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 68.05 | 70.58 | 67.74 | 70.03 | 44.13 | 56.17 | 60.58 | 64.34 | 65.05
    |'
- en: '| W4G128 | RTN | 67.85 | 70.84 | 68.32 | 70.72 | 45.27 | 56.47 | 60.70 | 64.03
    | 64.84 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| W4G128 | RTN | 67.85 | 70.84 | 68.32 | 70.72 | 45.27 | 56.47 | 60.70 | 64.03
    | 64.84 |'
- en: '| GPTQ | 66.32 | 70.92 | 68.90 | 70.68 | 42.88 | 56.99 | 61.23 | 64.75 | 65.37
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 66.32 | 70.92 | 68.90 | 70.68 | 42.88 | 56.99 | 61.23 | 64.75 | 65.37
    |'
- en: '| Ours | 68.09 | 71.43 | 68.65 | 70.81 | 44.23 | 57.30 | 60.86 | 64.76 | 65.67
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 68.09 | 71.43 | 68.65 | 70.81 | 44.23 | 57.30 | 60.86 | 64.76 | 65.67
    |'
- en: '| W3G128 | RTN | 64.94 | 67.70 | 65.92 | 68.70 | 39.11 | 42.61 | 36.99 | 56.09
    | 49.56 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| W3G128 | RTN | 64.94 | 67.70 | 65.92 | 68.70 | 39.11 | 42.61 | 36.99 | 56.09
    | 49.56 |'
- en: '| GPTQ | 58.29 | 68.73 | 65.51 | 68.73 | 39.78 | 54.43 | 58.47 | 62.98 | 64.68
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 58.29 | 68.73 | 65.51 | 68.73 | 39.78 | 54.43 | 58.47 | 62.98 | 64.68
    |'
- en: '| Ours | 66.62 | 69.59 | 66.88 | 69.70 | 43.31 | 55.46 | 59.12 | 53.42 | 63.61
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 66.62 | 69.59 | 66.88 | 69.70 | 43.31 | 55.46 | 59.12 | 53.42 | 63.61
    |'
- en: 'Table 2: Average % accuracy($\uparrow$) of HellaSwag, WinoGrand, PIQA and LAMBADA
    for BLOOM.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: BLOOM 对 HellaSwag、WinoGrand、PIQA 和 LAMBADA 的平均 % 准确率（$\uparrow$）。'
- en: '|  | W4 | W4G128 | W3G128 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | W4G128 | W3G128 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Size | 560m | 1b7 | 3b | 7b1 | 560m | 1b7 | 3b | 7b1 | 560m | 1b7 | 3b |
    7b1 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Size | 560m | 1b7 | 3b | 7b1 | 560m | 1b7 | 3b | 7b1 | 560m | 1b7 | 3b |
    7b1 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| FP16 | 45.50 | 52.31 | 55.48 | 60.22 | 45.50 | 52.31 | 55.48 | 60.22 | 45.50
    | 52.31 | 55.48 | 60.22 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 45.50 | 52.31 | 55.48 | 60.22 | 45.50 | 52.31 | 55.48 | 60.22 | 45.50
    | 52.31 | 55.48 | 60.22 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| RTN | 43.10 | 49.97 | 53.16 | 57.73 | 44.28 | 52.08 | 54.86 | 59.31 | 40.83
    | 47.98 | 52.51 | 57.59 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 43.10 | 49.97 | 53.16 | 57.73 | 44.28 | 52.08 | 54.86 | 59.31 | 40.83
    | 47.98 | 52.51 | 57.59 |'
- en: '| GPTQ | 43.95 | 50.91 | 54.65 | 58.27 | 44.79 | 52.08 | 55.68 | 59.59 | 42.74
    | 48.81 | 53.41 | 58.12 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 43.95 | 50.91 | 54.65 | 58.27 | 44.79 | 52.08 | 55.68 | 59.59 | 42.74
    | 48.81 | 53.41 | 58.12 |'
- en: '| Ours | 45.00 | 51.47 | 54.63 | 59.52 | 45.40 | 51.85 | 55.40 | 59.83 | 44.08
    | 50.52 | 53.64 | 58.69 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 45.00 | 51.47 | 54.63 | 59.52 | 45.40 | 51.85 | 55.40 | 59.83 | 44.08
    | 50.52 | 53.64 | 58.69 |'
- en: 'Table 3: C4 ppl ( $\downarrow$) at W4.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: C4 ppl ( $\downarrow$) 在 W4 上。'
- en: '|  | LLaMA | OPT | BLOOM |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA | OPT | BLOOM |'
- en: '| --- | --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Size | 7b | 13b | 7bv2 | 13bv2 | 1.3b | 2.7b | 6.7b | 13b | 560m | 1b7 |
    3b | 7b1 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Size | 7b | 13b | 7bv2 | 13bv2 | 1.3b | 2.7b | 6.7b | 13b | 560m | 1b7 |
    3b | 7b1 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| FP16 | 7.34 | 6.80 | 7.26 | 6.73 | 16.07 | 14.34 | 12.71 | 12.06 | 26.59
    | 19.49 | 17.48 | 15.20 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 7.34 | 6.80 | 7.26 | 6.73 | 16.07 | 14.34 | 12.71 | 12.06 | 26.59
    | 19.49 | 17.48 | 15.20 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| RTN | 8.12 | 7.23 | 8.16 | 7.14 | 27.49 | 18.83 | 14.37 | 13.32 | 29.87 |
    21.25 | 18.76 | 16.05 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 8.12 | 7.23 | 8.16 | 7.14 | 27.49 | 18.83 | 14.37 | 13.32 | 29.87 |
    21.25 | 18.76 | 16.05 |'
- en: '| GPTQ | 8.64 | 7.13 | 7.90 | 6.87 | 17.04 | 15.06 | 13.39 | 12.29 | 28.15
    | 20.71 | 18.18 | 15.67 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 8.64 | 7.13 | 7.90 | 6.87 | 17.04 | 15.06 | 13.39 | 12.29 | 28.15
    | 20.71 | 18.18 | 15.67 |'
- en: '| Ours | 7.84 | 7.05 | 11.20 | 7.72 | 16.92 | 14.97 | 13.08 | 12.48 | 28.12
    | 20.41 | 18.18 | 15.67 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 7.84 | 7.05 | 11.20 | 7.72 | 16.92 | 14.97 | 13.08 | 12.48 | 28.12
    | 20.41 | 18.18 | 15.67 |'
- en: 4.2 Comparing With Other Methods
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 与其他方法的比较
- en: We conducted a comprehensive benchmarking of our results against RTN and GPTQ (Frantar
    et al., [2022](#bib.bib8)). However, it is important to highlight that act-order
    was not enabled in GPTQ due to the kernel overhead it introduces (Lin et al.,
    [2023](#bib.bib18)), although it has the potential to improve accuracy for certain
    models. When evaluating perplexity (ppl), we prioritize reporting the ppl on C4
    dataset as our primary focus, taking into consideration the potential occurrence
    of NaN values when assessing perplexity for Wikitext2 and ptb datasets, both for
    SignRound and GPTQ. Furthermore, we conducted a limited and non-rigorous comparison
    between our approach and AWQ Lin et al. ([2023](#bib.bib18)) in Appendix [A.1](#A1.SS1
    "A.1 Non-rigorous comparison with AWQ ‣ Appendix A More results ‣ Optimize Weight
    Rounding via Signed Gradient Descent for the Quantization of LLMs").
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对我们的结果与RTN和GPTQ（Frantar等，[2022](#bib.bib8)）进行了全面的基准测试。然而，值得指出的是，由于引入的内核开销，GPTQ中未启用act-order（Lin等，[2023](#bib.bib18)），尽管它有潜力提高某些模型的准确性。在评估困惑度（ppl）时，我们优先报告C4数据集上的ppl，考虑到在评估Wikitext2和ptb数据集的困惑度时，可能会出现NaN值，无论是对于SignRound还是GPTQ。此外，我们在附录[A.1](#A1.SS1
    "A.1 Non-rigorous comparison with AWQ ‣ Appendix A More results ‣ Optimize Weight
    Rounding via Signed Gradient Descent for the Quantization of LLMs")中进行了有限且非严格的比较，比较了我们的方法与AWQ
    Lin等（[2023](#bib.bib18)）。
- en: We begin by presenting the average accuracy results for the HellaSwag, WinoGrand,
    PIQA, and LAMBADA tasks across LLaMA, OPT, and BLOOM models with a size below
    13B. These results are shown in Table [1](#S4.T1 "Table 1 ‣ SignRound Hyperparameters.
    ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ Optimize Weight Rounding via Signed
    Gradient Descent for the Quantization of LLMs") and [2](#S4.T2 "Table 2 ‣ SignRound
    Hyperparameters. ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ Optimize Weight
    Rounding via Signed Gradient Descent for the Quantization of LLMs"). In conclusion,
    our method outperforms RTN in 36 out of 39 scenarios, showcasing its effectiveness.
    Additionally, when comparing our approach to GPTQ, we surpass it in 32 out of
    39 scenarios, further highlighting the strengths of our method. While our method
    showcases overall effectiveness, it is important to acknowledge the presence of
    outliers, such as OPT6.7B at W3G128\. Although the root cause for this has not
    been identified yet, it could be mitigated by fine-tuning hyperparameters, as
    discussed in the following sections. For detailed results of LLaMA7B, LLaMA13B,
    LLAMA7B-V2, and LLAMA13B-V2, please refer to Appendix [E](#A5 "Appendix E Detailed
    Results of some LLaMa models ‣ Optimize Weight Rounding via Signed Gradient Descent
    for the Quantization of LLMs"). The results in Appendix [E](#A5 "Appendix E Detailed
    Results of some LLaMa models ‣ Optimize Weight Rounding via Signed Gradient Descent
    for the Quantization of LLMs") also highlight that changing the sequence length
    to 2048 could bring noticeable improvement in many scenarios.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先展示了HellaSwag、WinoGrand、PIQA和LAMBADA任务在LLaMA、OPT和BLOOM模型（大小均低于13B）上的平均准确率结果。这些结果见表格[1](#S4.T1
    "Table 1 ‣ SignRound Hyperparameters. ‣ 4.1 Experimental Settings ‣ 4 Experiments
    ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization of
    LLMs")和[2](#S4.T2 "Table 2 ‣ SignRound Hyperparameters. ‣ 4.1 Experimental Settings
    ‣ 4 Experiments ‣ Optimize Weight Rounding via Signed Gradient Descent for the
    Quantization of LLMs")。总的来说，我们的方法在39个场景中的36个场景中超越了RTN，展示了其有效性。此外，与GPTQ的比较中，我们在39个场景中的32个场景中超过了它，进一步突显了我们方法的优势。虽然我们的方法总体上展示了有效性，但需要承认存在异常值，例如W3G128的OPT6.7B。虽然尚未确定其根本原因，但可以通过微调超参数来缓解，如以下章节所讨论。有关LLaMA7B、LLaMA13B、LLAMA7B-V2和LLAMA13B-V2的详细结果，请参阅附录[E](#A5
    "Appendix E Detailed Results of some LLaMa models ‣ Optimize Weight Rounding via
    Signed Gradient Descent for the Quantization of LLMs")。附录[E](#A5 "Appendix E Detailed
    Results of some LLaMa models ‣ Optimize Weight Rounding via Signed Gradient Descent
    for the Quantization of LLMs")中的结果还突出显示，将序列长度更改为2048可能在许多场景中带来显著的改进。
- en: We then present the perplexity (ppl) results for C4 in Table [3](#S4.T3 "Table
    3 ‣ SignRound Hyperparameters. ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ Optimize
    Weight Rounding via Signed Gradient Descent for the Quantization of LLMs"), along
    with the detailed results for Wikitext2 in Appendix [A.2](#A1.SS2 "A.2 Results
    of Wikitext2 ppl at W4 ‣ Appendix A More results ‣ Optimize Weight Rounding via
    Signed Gradient Descent for the Quantization of LLMs"). In conclusion, we achieve
    better or comparable performance in 9 out of 12 models. In certain cases where
    the results may not be optimal, we can still fine-tune the hyperparameters to
    achieve better results, as demonstrated in the subsequent sections.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来展示了C4的困惑度（ppl）结果，如表[3](#S4.T3 "表 3 ‣ SignRound 超参数 ‣ 4.1 实验设置 ‣ 4 实验 ‣
    通过有符号梯度下降优化权重量化")所示，以及Wikitext2的详细结果见附录[A.2](#A1.SS2 "A.2 W4上的Wikitext2 ppl 结果
    ‣ 附录 A 更多结果 ‣ 通过有符号梯度下降优化权重量化")。总的来说，我们在12个模型中的9个中实现了更好或相当的性能。在某些结果可能不是最优的情况下，我们仍然可以通过微调超参数来取得更好的结果，后续部分将进行详细说明。
- en: Next, we present a comprehensive breakdown of the accuracies achieved by MMLU
    for LLaMA-7B and LLaMa-7B-V2 in Table [4](#S4.T4 "Table 4 ‣ 4.2 Comparing With
    Other Methods ‣ 4 Experiments ‣ Optimize Weight Rounding via Signed Gradient Descent
    for the Quantization of LLMs"). By analyzing the average accuracies, we observe
    that SingRound outperforms RTN and GPTQ in 4 out of the 6 scenarios when the best
    model-wise setting is applied.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在表[4](#S4.T4 "表 4 ‣ 4.2 与其他方法的比较 ‣ 4 实验 ‣ 通过有符号梯度下降优化权重量化")中展示了MMLU对LLaMA-7B和LLaMa-7B-V2的准确性详细数据。通过分析平均准确率，我们观察到SingRound在6种场景中的4种情况下优于RTN和GPTQ，前提是应用了最佳模型设置。
- en: 'Table 4: Accuracies($\uparrow$) of MMLU(5-shot) for LLaMA-7B & LLaMA-7B-V2\.
    ”Ours-2048” indicates that we have modified the sequence length of the calibration
    dataset from 512 to 2048.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：MMLU(5-shot)对LLaMA-7B和LLaMA-7B-V2的准确率（$\uparrow$）。“Ours-2048”表示我们将校准数据集的序列长度从512修改为2048。
- en: '|  |  | LLaMA-7B | LLaMA-7B-V2 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLaMA-7B | LLaMA-7B-V2 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | Hums. | STEM | Social | Other | Avg. | Hums. | STEM | Social | Other |
    Avg. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | Hums. | STEM | Social | Other | Avg. | Hums. | STEM | Social | Other |
    Avg. |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | FP16 | 38.32 | 31.17 | 38.05 | 36.85 | 35.64 | 51.40 | 37.00 | 52.23 |
    49.51 | 46.56 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 38.32 | 31.17 | 38.05 | 36.85 | 35.64 | 51.40 | 37.00 | 52.23 |
    49.51 | 46.56 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| W4G-1 | RTN | 34.84 | 29.53 | 32.87 | 36.28 | 33.10 | 44.03 | 32.83 | 44.97
    | 42.19 | 40.24 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| W4G-1 | RTN | 34.84 | 29.53 | 32.87 | 36.28 | 33.10 | 44.03 | 32.83 | 44.97
    | 42.19 | 40.24 |'
- en: '| GPTQ | 33.31 | 26.29 | 29.86 | 33.11 | 30.32 | 46.21 | 34.29 | 46.68 | 44.85
    | 42.21 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 33.31 | 26.29 | 29.86 | 33.11 | 30.32 | 46.21 | 34.29 | 46.68 | 44.85
    | 42.21 |'
- en: '| Ours | 34.30 | 31.05 | 34.74 | 36.66 | 33.95 | 47.28 | 33.14 | 46.90 | 44.70
    | 42.10 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 34.30 | 31.05 | 34.74 | 36.66 | 33.95 | 47.28 | 33.14 | 46.90 | 44.70
    | 42.10 |'
- en: '| Ours2048 | 35.10 | 30.69 | 36.43 | 36.85 | 34.42 | 47.40 | 33.92 | 49.61
    | 44.91 | 43.00 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Ours2048 | 35.10 | 30.69 | 36.43 | 36.85 | 34.42 | 47.40 | 33.92 | 49.61
    | 44.91 | 43.00 |'
- en: '| W4G128 | RTN | 36.30 | 31.67 | 37.40 | 37.99 | 35.48 | 49.54 | 36.50 | 50.95
    | 47.87 | 45.31 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| W4G128 | RTN | 36.30 | 31.67 | 37.40 | 37.99 | 35.48 | 49.54 | 36.50 | 50.95
    | 47.87 | 45.31 |'
- en: '| GPTQ | 37.77 | 29.64 | 36.38 | 37.45 | 34.83 | 50.30 | 36.51 | 50.91 | 47.69
    | 45.43 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 37.77 | 29.64 | 36.38 | 37.45 | 34.83 | 50.30 | 36.51 | 50.91 | 47.69
    | 45.43 |'
- en: '| Ours | 36.06 | 30.86 | 35.99 | 36.21 | 34.44 | 51.39 | 37.87 | 52.56 | 49.69
    | 46.95 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 36.06 | 30.86 | 35.99 | 36.21 | 34.44 | 51.39 | 37.87 | 52.56 | 49.69
    | 46.95 |'
- en: '| Ours2048 | 35.66 | 30.05 | 36.16 | 37.57 | 34.46 | 50.12 | 36.70 | 51.44
    | 48.20 | 45.69 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Ours2048 | 35.66 | 30.05 | 36.16 | 37.57 | 34.46 | 50.12 | 36.70 | 51.44
    | 48.20 | 45.69 |'
- en: '| W3G128 | RTN | 32.97 | 30.28 | 33.66 | 32.60 | 32.17 | 41.14 | 33.06 | 40.98
    | 40.94 | 38.51 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| W3G128 | RTN | 32.97 | 30.28 | 33.66 | 32.60 | 32.17 | 41.14 | 33.06 | 40.98
    | 40.94 | 38.51 |'
- en: '| GPTQ | 30.77 | 28.29 | 30.73 | 31.33 | 30.12 | 44.66 | 37.55 | 46.36 | 43.47
    | 42.48 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 30.77 | 28.29 | 30.73 | 31.33 | 30.12 | 44.66 | 37.55 | 46.36 | 43.47
    | 42.48 |'
- en: '| Ours | 30.12 | 28.21 | 30.64 | 30.34 | 29.68 | 44.53 | 33.53 | 44.60 | 43.52
    | 40.82 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 30.12 | 28.21 | 30.64 | 30.34 | 29.68 | 44.53 | 33.53 | 44.60 | 43.52
    | 40.82 |'
- en: '| Ours2048 | 32.43 | 28.62 | 31.03 | 32.10 | 30.85 | 42.75 | 32.98 | 42.88
    | 41.30 | 39.34 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Ours2048 | 32.43 | 28.62 | 31.03 | 32.10 | 30.85 | 42.75 | 32.98 | 42.88
    | 41.30 | 39.34 |'
- en: We also provide the results for models with a capacity of 30B or greater at
    W3G128 in Table [5](#S4.T5 "Table 5 ‣ 4.2 Comparing With Other Methods ‣ 4 Experiments
    ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization of
    LLMs") and W4 in Appendix [A.3](#A1.SS3 "A.3 Other results for large models ‣
    Appendix A More results ‣ Optimize Weight Rounding via Signed Gradient Descent
    for the Quantization of LLMs"). Additionally, we discovered that recovering the
    sequence length to 512 of the calibration dataset yielded improvements in certain
    scenarios, and thus we include these results. In summary, our approach achieves
    comparable performance to GPTQ for the given accuracy task. However, we slightly
    lag behind GPTQ in terms of ppl tasks.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了在 W3G128 的 30B 或更大容量模型的结果，见表[5](#S4.T5 "表 5 ‣ 4.2 与其他方法的比较 ‣ 4 实验 ‣ 通过签名梯度下降优化权重量化")，以及附录[A.3](#A1.SS3
    "A.3 大型模型的其他结果 ‣ 附录 A 更多结果 ‣ 通过签名梯度下降优化权重量化")中的 W4 结果。此外，我们发现将校准数据集的序列长度恢复到 512
    在某些情况下提高了性能，因此我们包含了这些结果。总的来说，我们的方法在给定准确率任务中表现与 GPTQ 相当。然而，在 PPL 任务中我们稍微落后于 GPTQ。
- en: 'Table 5: Average % accuracy($\uparrow$ 30B at W3G128\. ”Ours-seq512” indicates
    that we have modified the sequence length of the calibration dataset from 256
    to 512.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 平均 % 准确率（$\uparrow$ 30B 在 W3G128\. “Ours-seq512” 表示我们已将校准数据集的序列长度从 256
    修改为 512。'
- en: '|  | Accuracy | PPL on C4 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | 准确率 | C4 上的 PPL |'
- en: '| Type | LLaMA | OPT | LLaMA | OPT |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | LLaMA | OPT | LLaMA | OPT |'
- en: '| Size | 30b | 65b | 30b | 66b | 30b | 65b | 30b | 66b |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 30b | 65b | 30b | 66b | 30b | 65b | 30b | 66b |'
- en: '| FP16 | 73.46 | 75.48 | 67.87 | 69.54 | 6.13 | 5.98 | 11.46 | 10.99 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 73.46 | 75.48 | 67.87 | 69.54 | 6.13 | 5.98 | 11.46 | 10.99 |'
- en: '| RTN | 72.17 | 73.69 | 62.83 | 38.00 | 6.85 | 6.52 | 30.81 | 285.41 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 72.17 | 73.69 | 62.83 | 38.00 | 6.85 | 6.52 | 30.81 | 285.41 |'
- en: '| GPTQ | 72.09 | 73.97 | 66.76 | 67.87 | 6.80 | 6.52 | 11.74 | 11.87 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 72.09 | 73.97 | 66.76 | 67.87 | 6.80 | 6.52 | 11.74 | 11.87 |'
- en: '| Ours-seq256 | 72.45 | 73.71 | 66.51 | 68.00 | 6.83 | 6.52 | 13.00 | 13.34
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Ours-seq256 | 72.45 | 73.71 | 66.51 | 68.00 | 6.83 | 6.52 | 13.00 | 13.34
    |'
- en: '| Ours-seq512 | 71.95 | 73.78 | 66.70 | 67.26 | 6.79 | 6.53 | 12.50 | 13.97
    |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Ours-seq512 | 71.95 | 73.78 | 66.70 | 67.26 | 6.79 | 6.53 | 12.50 | 13.97
    |'
- en: 4.3 Block-wise versus Layer-wise
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 按块调整与按层调整
- en: We examined the effects of layer-wise and block-wise tuning. As explained in
    Section [3.1](#S3.SS1 "3.1 SignRound ‣ 3 Methodology ‣ Optimize Weight Rounding
    via Signed Gradient Descent for the Quantization of LLMs"), the term ”layer” refers
    to a linear/convolution layer, while ”block” specifically denotes a transformer
    block consisting of multiple linear layers. To simplify this evaluation, we set
    the sequence length to 256 and disable AMP. Based on the below results, block-wise
    tuning outperformed layer-wise tuning in the majority of scenarios.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查了按层调整和按块调整的效果。如[3.1节](#S3.SS1 "3.1 SignRound ‣ 3 方法 ‣ 通过签名梯度下降优化权重量化")所述，“层”指的是线性/卷积层，而“块”特指由多个线性层组成的变换器块。为了简化评估，我们将序列长度设置为
    256，并禁用了 AMP。根据以下结果，按块调整在大多数情况下优于按层调整。
- en: 'Table 6: Comparing block-wise and layer-wise tuning for around 7B models, the
    models LLaMA7b, LLaMA7bv2, OPT6.7b, and BLOOM7b1 are denoted by 7b, 7bv2, 6.7b,
    and 7b1 respectively. The accuracy is the % average accuracy($\uparrow$) is evaluated
    using the C4 dataset.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 比较约 7B 模型的按块调整和按层调整，模型 LLaMA7b、LLaMA7bv2、OPT6.7b 和 BLOOM7b1 分别表示为 7b、7bv2、6.7b
    和 7b1。准确率的 % 平均准确率（$\uparrow$）使用 C4 数据集进行评估。'
- en: '|  | W4 | W3G128 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | W3G128 |'
- en: '| --- | --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Size | 7b | 7bv2 | 6.7b | 7b1 | 7b | 7bv2 | 6.7b | 7b1 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 7b | 7bv2 | 6.7b | 7b1 | 7b | 7bv2 | 6.7b | 7b1 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| layer-acc-seq256 | 67.50 | 67.78 | 63.46 | 58.72 | 65.96 | 66.09 | 61.60
    | 58.24 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| layer-acc-seq256 | 67.50 | 67.78 | 63.46 | 58.72 | 65.96 | 66.09 | 61.60
    | 58.24 |'
- en: '| block-acc-seq256 | 67.64 | 67.96 | 64.55 | 59.08 | 66.31 | 66.63 | 57.76
    | 58.34 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| block-acc-seq256 | 67.64 | 67.96 | 64.55 | 59.08 | 66.31 | 66.63 | 57.76
    | 58.34 |'
- en: '| layer-c4-ppl-seq256 | 8.02 | 7.92 | 13.44 | 15.73 | 8.81 | 8.69 | 16.83 |
    16.15 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| layer-c4-ppl-seq256 | 8.02 | 7.92 | 13.44 | 15.73 | 8.81 | 8.69 | 16.83 |
    16.15 |'
- en: '| block-c4-ppl-seq256 | 7.81 | 8.19 | 13.10 | 15.71 | 8.34 | 10.84 | 25.44
    | 16.05 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| block-c4-ppl-seq256 | 7.81 | 8.19 | 13.10 | 15.71 | 8.34 | 10.84 | 25.44
    | 16.05 |'
- en: 4.4 The analysis of hyperparameters sensitivity
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 超参数敏感性分析
- en: We conducted a hyperparameters sensitivity analysis, the results of which are
    summarized in Table [7](#S4.T7 "Table 7 ‣ 4.4 The analysis of hyperparameters
    sensitivity ‣ 4 Experiments ‣ Optimize Weight Rounding via Signed Gradient Descent
    for the Quantization of LLMs"). In the ”steps100” configuration, we used 100 steps,
    and a learning rate of 1e-2. In the ”lr4e-3” configuration, we set the learning
    rate to 4e-3\. We also changed the sequence length of the calibration dataset
    from 512 to 2048, denoted by ”seq2048”. Please note that all other hyperparameters
    not mentioned in each configuration were kept the same as the default configurations,
    as detailed in Section [4.1](#S4.SS1 "4.1 Experimental Settings ‣ 4 Experiments
    ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization of
    LLMs"). Overall, our method exhibits robustness to hyperparameters in common sense
    reasoning tasks, with the exception of the perplexity of LLaMA-7b-v2\. However,
    we did discover that certain hyperparameters, such as the sequence length of the
    calibration dataset, can significantly impact performance in some scenarios, as
    demonstrated in Table [4](#S4.T4 "Table 4 ‣ 4.2 Comparing With Other Methods ‣
    4 Experiments ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization
    of LLMs") and [5](#S4.T5 "Table 5 ‣ 4.2 Comparing With Other Methods ‣ 4 Experiments
    ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization of
    LLMs").
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了超参数灵敏度分析，结果总结在表[7](#S4.T7 "表7 ‣ 4.4 超参数灵敏度分析 ‣ 4 实验 ‣ 通过带符号梯度下降优化LLMs的权重舍入")中。在“steps100”配置中，我们使用了100步和1e-2的学习率。在“lr4e-3”配置中，我们将学习率设置为4e-3。我们还将校准数据集的序列长度从512更改为2048，表示为“seq2048”。请注意，每个配置中未提及的其他超参数保持与默认配置相同，如[4.1节](#S4.SS1
    "4.1 实验设置 ‣ 4 实验 ‣ 通过带符号梯度下降优化LLMs的权重舍入")所述。总体而言，我们的方法在常识推理任务中对超参数表现出鲁棒性，除了LLaMA-7b-v2的困惑度。然而，我们确实发现，某些超参数（如校准数据集的序列长度）在某些情况下对性能有显著影响，如表[4](#S4.T4
    "表4 ‣ 4.2 与其他方法比较 ‣ 4 实验 ‣ 通过带符号梯度下降优化LLMs的权重舍入")和[5](#S4.T5 "表5 ‣ 4.2 与其他方法比较
    ‣ 4 实验 ‣ 通过带符号梯度下降优化LLMs的权重舍入")所示。
- en: 'Table 7: Hyperparameter sensitivity analysis, the models LLaMA7b, LLaMA7bv2,
    OPT6.7b, and BLOOM7b1 are denoted by 7b, 7bv2, 6.7b, and 7b1 respectively. The
    accuracy is the % average accuracy($\uparrow$) is evaluated using the C4 dataset.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：超参数灵敏度分析，模型LLaMA7b、LLaMA7bv2、OPT6.7b和BLOOM7b1分别用7b、7bv2、6.7b和7b1表示。准确度是使用C4数据集评估的%平均准确度($\uparrow$)。
- en: '|  | Accuracy | PPL on C4 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | 准确度 | C4上的PPL |'
- en: '| --- | --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Size | 7b | 7bv2 | 6.7b | 7b1 | 7b | 7bv2 | 6.7b | 7b1 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 7b | 7bv2 | 6.7b | 7b1 | 7b | 7bv2 | 6.7b | 7b1 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| steps100 | 67.53 | 67.76 | 64.64 | 58.76 | 7.93 | 7.83 | 13.12 | 15.71 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| steps100 | 67.53 | 67.76 | 64.64 | 58.76 | 7.93 | 7.83 | 13.12 | 15.71 |'
- en: '| lr4e-3 | 68.01 | 67.57 | 64.57 | 59.47 | 7.81 | 10.29 | 13.09 | 15.66 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| lr4e-3 | 68.01 | 67.57 | 64.57 | 59.47 | 7.81 | 10.29 | 13.09 | 15.66 |'
- en: '| seq2048 | 68.11 | 67.79 | 64.32 | 59.39 | 7.76 | 9.97 | 13.06 | 15.66 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| seq2048 | 68.11 | 67.79 | 64.32 | 59.39 | 7.76 | 9.97 | 13.06 | 15.66 |'
- en: '| default | 68.05 | 67.74 | 64.34 | 59.52 | 7.84 | 11.20 | 13.08 | 15.67 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 默认 | 68.05 | 67.74 | 64.34 | 59.52 | 7.84 | 11.20 | 13.08 | 15.67 |'
- en: 4.5 The Analysis of Gradients and Their Effects on Rounding
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 梯度及其对舍入的影响分析
- en: '![Refer to caption](img/53cf38997dfbdfb90112de5ab62478f3.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/53cf38997dfbdfb90112de5ab62478f3.png)'
- en: LLaMA-7B
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-7B
- en: '![Refer to caption](img/1e88fba19cd10eb22ceca1f14bc1b01d.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1e88fba19cd10eb22ceca1f14bc1b01d.png)'
- en: LLaMA-7B-V2
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-7B-V2
- en: '![Refer to caption](img/5025eeb632f37032765f3dba2e857179.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5025eeb632f37032765f3dba2e857179.png)'
- en: OPT-6.7B
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: OPT-6.7B
- en: '![Refer to caption](img/105f227d3d5f3c7f2d808af34cde570e.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/105f227d3d5f3c7f2d808af34cde570e.png)'
- en: BLOOM-7B1
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: BLOOM-7B1
- en: 'Figure 2: The impact of the rounding value introduced by the $V$ in Eq. [2](#S3.E2
    "In 3 Methodology ‣ Optimize Weight Rounding via Signed Gradient Descent for the
    Quantization of LLMs")'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：公式[2](#S3.E2 "在3 方法论 ‣ 通过带符号梯度下降优化LLMs的权重舍入")中引入的$V$对舍入值的影响
- en: In this analysis, we dive into the distribution of the magnitude of $V$ in Eq.
    [2](#S3.E2 "In 3 Methodology ‣ Optimize Weight Rounding via Signed Gradient Descent
    for the Quantization of LLMs") and its impact on rounding values across approximately
    7 billion models at W4\. The visual representations of these distributions are
    provided in Appendix [B](#A2 "Appendix B Visualization of V ‣ Optimize Weight
    Rounding via Signed Gradient Descent for the Quantization of LLMs"). Our investigation
    reveals that the majority of V values are concentrated within the range of [-0.3,
    0.3]. Additionally, we observe an interesting pattern in the distribution of V
    across different layers. The middle layers exhibit a more tightly clustered distribution
    compared to the other layers. This observation aligns with the common understanding
    that the head and tail layers tend to be more sensitive to compression, while
    the middle layers are relatively more robust.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项分析中，我们深入探讨了方程[$V$](#S3.E2 "在3方法论 ‣ 通过有符号梯度下降优化权重量化")的幅度分布及其对约70亿模型在W4的舍入值的影响。这些分布的可视化结果见附录[B](#A2
    "附录B：V的可视化 ‣ 通过有符号梯度下降优化权重量化")。我们的研究表明，大多数V值集中在[-0.3, 0.3]范围内。此外，我们还观察到V在不同层次的分布中存在有趣的模式。中间层的分布较其他层更为集中。这一观察结果符合常见的理解，即头部和尾部层对压缩的敏感性较高，而中间层则相对更具鲁棒性。
- en: Figure [2](#S4.F2 "Figure 2 ‣ 4.5 The Analysis of Gradients and Their Effects
    on Rounding ‣ 4 Experiments ‣ Optimize Weight Rounding via Signed Gradient Descent
    for the Quantization of LLMs") illustrates the impact of the rounding value introduced
    by the $V$ in Eq. [2](#S3.E2 "In 3 Methodology ‣ Optimize Weight Rounding via
    Signed Gradient Descent for the Quantization of LLMs") for models around 7B at
    W4\. The red line represents ”up rounding”, indicating that while RTN rounds the
    value to the floor, SignRound changes it to the ceiling. Conversely, the green
    line represents ”down rounding” indicating that while RTN rounds the value to
    the ceiling, SignRound changes it to the floor. It is worth noting that SignRound
    modifies only a small percentage of weight rounding values for each of the four
    models, namely 5.27%, 5.29%, 4.14%, and 4.10%.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S4.F2 "图2 ‣ 4.5 梯度分析及其对舍入的影响 ‣ 4 实验 ‣ 通过有符号梯度下降优化权重量化")展示了方程[$V$](#S3.E2
    "在3方法论 ‣ 通过有符号梯度下降优化权重量化")中引入的舍入值对7B左右模型的影响。红线代表“上舍入”，表明RTN将值舍入至下限，而SignRound将其舍入至上限。相反，绿线代表“下舍入”，表明RTN将值舍入至上限，而SignRound将其舍入至下限。值得注意的是，SignRound仅修改了四个模型中较小比例的权重舍入值，分别为5.27%、5.29%、4.14%和4.10%。
- en: We were also intrigued by the possible correlation between rounding and activation,
    as previous research has shown that keeping only 0.1%-1% of the channels corresponding
    to larger activation can significantly improve the quantized performance in AWQ
    (Lin et al., [2023](#bib.bib18)). We shown the result in Appendix [C](#A3 "Appendix
    C Correction between SignRound and salient activation channels ‣ Optimize Weight
    Rounding via Signed Gradient Descent for the Quantization of LLMs").
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也对**舍入**与**激活**之间可能的相关性感到兴趣，因为以往研究表明，仅保留对应较大激活的0.1%-1%的通道，可以显著提高AWQ中的量化性能（Lin
    et al., [2023](#bib.bib18)）。我们在附录[C](#A3 "附录C：SignRound与显著激活通道的关系 ‣ 通过有符号梯度下降优化权重量化")中展示了结果。
- en: 5 Conclusions and Limitations
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与局限性
- en: In this paper, we present a highly effective and concise approach to optimize
    the weight rounding task. Our method, SignRound, leverages lightweight block-wise
    tuning using signed gradient descent, achieving remarkable results within a mere
    400 steps. Extensive experiments demonstrate the superior performance of our approach.
    As part of our future work, we plan to apply our approach to more diverse LLM
    models (e.g., Code LLaMA (Rozière et al., [2023](#bib.bib28)), LLaMA v2 Chat (Touvron
    et al., [2023b](#bib.bib33))), and contribute our recipes and implementations
    to the open source community. On the other hand, although our method is generally
    effective, there are a few outliers in certain scenarios, where we plan to mitigate
    the issue by fine-tuning the hyperparameters.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们提出了一种极其有效且简洁的方法来优化权重舍入任务。我们的方法，SignRound，利用了使用带符号的梯度下降进行的轻量级块级调整，在短短
    400 步内取得了显著结果。大量实验展示了我们方法的优越性能。作为我们未来的工作的一部分，我们计划将我们的方法应用于更多的 LLM 模型（例如，Code LLaMA（Rozière
    等，[2023](#bib.bib28)），LLaMA v2 Chat（Touvron 等，[2023b](#bib.bib33)）），并将我们的配方和实现贡献给开源社区。另一方面，尽管我们的方法通常有效，但在某些情况下存在少数异常值，我们计划通过微调超参数来缓解这一问题。
- en: References
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bengio et al. (2013) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating
    or propagating gradients through stochastic neurons for conditional computation.
    *arXiv preprint arXiv:1308.3432*, 2013.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等（2013）Yoshua Bengio, Nicholas Léonard, 和 Aaron Courville. 通过随机神经元估计或传播梯度以进行条件计算。*arXiv
    预印本 arXiv:1308.3432*，2013年。
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pp.  7432–7439,
    2020.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisk 等（2020）Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi 等. Piqa：在自然语言中推理物理常识。发表于
    *AAAI 人工智能会议论文集*，第 34 卷，第 7432–7439 页，2020年。
- en: 'Bondarenko et al. (2023) Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
    Quantizable transformers: Removing outliers by helping attention heads do nothing.
    *arXiv preprint arXiv:2306.12929*, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bondarenko 等（2023）Yelysei Bondarenko, Markus Nagel, 和 Tijmen Blankevoort. 可量化变换器：通过帮助注意力头无所作为来去除异常值。*arXiv
    预印本 arXiv:2306.12929*，2023年。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv preprint
    arXiv:2208.07339*, 2022.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2022）Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    Llm. int8（）: 变换器的大规模 8 位矩阵乘法。*arXiv 预印本 arXiv:2208.07339*，2022年。'
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等（2023）Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer.
    Qlora：量化语言模型的高效微调。*arXiv 预印本 arXiv:2305.14314*，2023年。
- en: Esser et al. (2019) Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar
    Appuswamy, and Dharmendra S Modha. Learned step size quantization. *arXiv preprint
    arXiv:1902.08153*, 2019.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esser 等（2019）Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar
    Appuswamy, 和 Dharmendra S Modha. 学习步长量化。*arXiv 预印本 arXiv:1902.08153*，2019年。
- en: 'Frantar & Alistarh (2022) Elias Frantar and Dan Alistarh. Optimal brain compression:
    A framework for accurate post-training quantization and pruning. *arXiv preprint
    arXiv:2208.11580*, 2022.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar & Alistarh（2022）Elias Frantar 和 Dan Alistarh. 最优脑压缩：准确后训练量化和剪枝的框架。*arXiv
    预印本 arXiv:2208.11580*，2022年。
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等（2022）Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh.
    Gptq：生成预训练变换器的准确后训练量化。*arXiv 预印本 arXiv:2210.17323*，2022年。
- en: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. A framework for few-shot language model evaluation, September 2021.
    URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2021）Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi,
    Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, 和 Andy
    Zou. 少样本语言模型评估框架，2021年9月。网址 [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628)。
- en: Hassibi et al. (1993) Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal
    brain surgeon and general network pruning. In *IEEE international conference on
    neural networks*, pp. 293–299\. IEEE, 1993.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi 等人 (1993) Babak Hassibi, David G Stork, 和 Gregory J Wolff. 最优大脑外科医生和通用网络修剪。见
    *IEEE 国际神经网络会议*，第 293–299 页。IEEE，1993。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等人 (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang, 和 Weizhu Chen. Lora: 大型语言模型的低秩适应。*arXiv 预印本 arXiv:2106.09685*，2021。'
- en: 'Kim et al. (2023) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等人 (2023) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li,
    Sheng Shen, Michael W Mahoney, 和 Kurt Keutzer. Squeezellm: 密集与稀疏量化。*arXiv 预印本
    arXiv:2306.07629*，2023。'
- en: Lee et al. (2021) Jung Hyun Lee, Jihun Yun, Sung Ju Hwang, and Eunho Yang. Cluster-promoting
    quantization with bit-drop for minimizing network quantization loss. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, pp.  5370–5379,
    2021.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 (2021) Jung Hyun Lee, Jihun Yun, Sung Ju Hwang, 和 Eunho Yang. 使用位丢弃的聚类促进量化以最小化网络量化损失。见
    *IEEE/CVF 国际计算机视觉会议论文集*，第 5370–5379 页，2021。
- en: 'Lee et al. (2023) Jung Hyun Lee, Jeonghoon Kim, Se Jung Kwon, and Dongsoo Lee.
    Flexround: Learnable rounding based on element-wise division for post-training
    quantization. *arXiv preprint arXiv:2306.00317*, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee 等人 (2023) Jung Hyun Lee, Jeonghoon Kim, Se Jung Kwon, 和 Dongsoo Lee. Flexround:
    基于逐元素除法的可学习舍入用于后训练量化。*arXiv 预印本 arXiv:2306.00317*，2023。'
- en: Li et al. (2023) Xiuxian Li, Kuo-Yi Lin, Li Li, Yiguang Hong, and Jie Chen.
    On faster convergence of scaled sign gradient descent. *IEEE Transactions on Industrial
    Informatics*, 2023.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2023) Xiuxian Li, Kuo-Yi Lin, Li Li, Yiguang Hong, 和 Jie Chen. 关于标量符号梯度下降的更快收敛。*IEEE
    工业信息学交易期刊*，2023。
- en: 'Li et al. (2021) Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization
    by block reconstruction. *arXiv preprint arXiv:2102.05426*, 2021.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2021) Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, 和 Shi Gu. Brecq: 通过块重建突破后训练量化的极限。*arXiv 预印本 arXiv:2102.05426*，2021。'
- en: Li et al. (2022) Zhengyi Li, Cong Guo, Zhanda Zhu, Yangjie Zhou, Yuxian Qiu,
    Xiaotian Gao, Jingwen Leng, and Minyi Guo. Efficient activation quantization via
    adaptive rounding border for post-training quantization. *arXiv preprint arXiv:2208.11945*,
    2022.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2022) Zhengyi Li, Cong Guo, Zhanda Zhu, Yangjie Zhou, Yuxian Qiu, Xiaotian
    Gao, Jingwen Leng, 和 Minyi Guo. 通过自适应舍入边界实现高效激活量化。*arXiv 预印本 arXiv:2208.11945*，2022。
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. Awq: 面向 LLM 压缩和加速的激活感知权重量化。*arXiv 预印本 arXiv:2306.00978*，2023。'
- en: 'Liu et al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models. *arXiv
    preprint arXiv:2305.17888*, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra.
    Llm-qat: 大型语言模型的数据无关量化感知训练。*arXiv 预印本 arXiv:2305.17888*，2023。'
- en: Liu et al. (2021) Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and
    Wen Gao. Post-training quantization for vision transformer. *Advances in Neural
    Information Processing Systems*, 34:28092–28103, 2021.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2021) Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, 和 Wen Gao.
    视觉变换器的后训练量化。*神经信息处理系统进展*，34:28092–28103，2021。
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等人 (2016) Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard Socher.
    指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843*，2016。
- en: Nagel et al. (2019) Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max
    Welling. Data-free quantization through weight equalization and bias correction.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    pp.  1325–1334, 2019.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel 等人 (2019) Markus Nagel, Mart van Baalen, Tijmen Blankevoort, 和 Max Welling.
    通过权重均衡和偏置修正进行无数据量化。见 *IEEE/CVF 国际计算机视觉会议论文集*，第 1325–1334 页，2019。
- en: Nagel et al. (2020) Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning*, pp. 7197–7206\.
    PMLR, 2020.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel 等人 (2020) Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos
    和 Tijmen Blankevoort. 向上还是向下？用于后训练量化的自适应舍入。发表于 *国际机器学习会议*，第 7197–7206 页。PMLR，2020年。
- en: '(24) OpenAI. Openai: Chatgpt. URL [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(24) OpenAI. Openai: Chatgpt. URL [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)。'
- en: 'Paperno et al. (2016) Denis Paperno, Germán Kruszewski, Angeliki Lazaridou,
    Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda,
    and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse
    context. *arXiv preprint arXiv:1606.06031*, 2016.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paperno 等人 (2016) Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan
    Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda 和 Raquel
    Fernández. lambada 数据集：需要广泛话语上下文的词预测。*arXiv 预印本 arXiv:1606.06031*，2016年。
- en: 'Park et al. (2022) Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim,
    Youngjoo Lee, and Dongsoo Lee. nuqmm: Quantized matmul for efficient inference
    of large-scale generative language models. *arXiv preprint arXiv:2206.09557*,
    2022.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park 等人 (2022) Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo
    Lee 和 Dongsoo Lee. nuqmm: 量化矩阵乘法用于大规模生成语言模型的高效推理。*arXiv 预印本 arXiv:2206.09557*，2022年。'
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li 和 Peter J Liu. 利用统一的文本到文本变换器探索迁移学习的极限。*机器学习研究期刊*，21(1):5485–5551，2020年。
- en: 'Rozière et al. (2023) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. Code llama: Open foundation models for code. *arXiv preprint arXiv:2308.12950*,
    2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rozière 等人 (2023) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla,
    Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin 等.
    Code llama: 用于代码的开放基础模型。*arXiv 预印本 arXiv:2308.12950*，2023年。'
- en: 'Safaryan & Richtárik (2021) Mher Safaryan and Peter Richtárik. Stochastic sign
    descent methods: New algorithms and better theory. In *International Conference
    on Machine Learning*, pp. 9224–9234\. PMLR, 2021.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Safaryan 和 Richtárik (2021) Mher Safaryan 和 Peter Richtárik. 随机符号下降方法：新算法和更好的理论。发表于
    *国际机器学习会议*，第 9224–9234 页。PMLR，2021年。
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi 等人 (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula 和
    Yejin Choi. Winogrande: 大规模对抗性 Winograd 模式挑战。*ACM 通讯*，64(9):99–106，2021年。'
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*, 2022.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Scao 等人 (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé 等. Bloom: 一个 176b 参数的开放访问多语言模型。*arXiv 预印本 arXiv:2211.05100*，2022年。'
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar 等. Llama: 开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023年。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等. Llama 2: 开放基础和微调对话模型。*arXiv 预印本 arXiv:2307.09288*，2023年。'
- en: 'Wang et al. (2019) Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han.
    Haq: Hardware-aware automated quantization with mixed precision. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pp.  8612–8620,
    2019.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 (2019) Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin 和 Song Han. Haq: 硬件感知的自动量化与混合精度。发表于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，第 8612–8620 页，2019年。'
- en: 'Wei et al. (2022a) Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and
    Fengwei Yu. Qdrop: randomly dropping quantization for extremely low-bit post-training
    quantization. *arXiv preprint arXiv:2203.05740*, 2022a.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei et al. (2022a) Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, 和 Fengwei
    Yu. Qdrop: 随机丢弃量化以实现极低位后训练量化。*arXiv 预印本 arXiv:2203.05740*，2022年。'
- en: 'Wei et al. (2022b) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong,
    Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression:
    Pushing the limit of low-bit transformer language models. *Advances in Neural
    Information Processing Systems*, 35:17402–17414, 2022b.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei et al. (2022b) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong,
    Shanghang Zhang, Qi Zhang, Fengwei Yu, 和 Xianglong Liu. 异常值抑制: 推动低位变换器语言模型的极限。*神经信息处理系统进展*，35:17402–17414，2022年。'
- en: 'Wei et al. (2023) Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao
    Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization
    of large language models by equivalent and optimal shifting and scaling. *arXiv
    preprint arXiv:2304.09145*, 2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei et al. (2023) Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao
    Gong, Jinyang Guo, 和 Xianglong Liu. 异常值抑制+: 通过等效和优化的平移与缩放实现大型语言模型的准确量化。*arXiv
    预印本 arXiv:2304.09145*，2023年。'
- en: 'Xiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. *arXiv preprint arXiv:2211.10438*, 2022.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth,
    和 Song Han. Smoothquant: 精确而高效的大型语言模型后训练量化。*arXiv 预印本 arXiv:2211.10438*，2022年。'
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *International Conference on Machine Learning*, pp. 38087–38099\.
    PMLR, 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    和 Song Han. Smoothquant: 精确而高效的大型语言模型后训练量化。在*国际机器学习会议*上，pp. 38087–38099。PMLR，2023年。'
- en: 'Yao et al. (2021) Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali
    Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael Mahoney, et al. Hawq-v3:
    Dyadic neural network quantization. In *International Conference on Machine Learning*,
    pp. 11875–11886\. PMLR, 2021.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2021) Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali
    Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael Mahoney, 等人。Hawq-v3:
    双向神经网络量化。在*国际机器学习会议*上，pp. 11875–11886。PMLR，2021年。'
- en: 'Yao et al. (2023) Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong
    He. Zeroquant-v2: Exploring post-training quantization in llms from comprehensive
    study to low rank compensation. *arXiv:2303.08302*, 2023.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2023) Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, 和 Yuxiong
    He. Zeroquant-v2: 从综合研究到低秩补偿探索LLMs中的后训练量化。*arXiv:2303.08302*，2023年。'
- en: 'Yuan et al. (2023) Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang,
    Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based
    post-training quantization for large language models. *arXiv preprint arXiv:2304.01089*,
    2023.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan et al. (2023) Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang,
    Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, 和 Bingzhe Wu. Rptq: 基于重排序的大型语言模型后训练量化。*arXiv
    预印本 arXiv:2304.01089*，2023年。'
- en: 'Yvinec et al. (2023) Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin
    Bailly. Spiq: Data-free per-channel static input quantization. In *Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision*, pp.  3869–3878,
    2023.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yvinec et al. (2023) Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, 和 Kevin
    Bailly. Spiq: 无数据的每通道静态输入量化。在*IEEE/CVF计算机视觉应用冬季会议论文集*，pp. 3869–3878，2023年。'
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    和 Yejin Choi. Hellaswag: 机器真的能完成你的句子吗？*arXiv 预印本 arXiv:1905.07830*，2019年。'
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    等人。Opt: 开放的预训练变换器语言模型。*arXiv 预印本 arXiv:2205.01068*，2022年。'
- en: Zhuang et al. (2021) Bohan Zhuang, Mingkui Tan, Jing Liu, Lingqiao Liu, Ian
    Reid, and Chunhua Shen. Effective training of convolutional neural networks with
    low-bitwidth weights and activations. *IEEE Transactions on Pattern Analysis and
    Machine Intelligence*, 44(10):6140–6152, 2021.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang等人（2021）Bohan Zhuang, Mingkui Tan, Jing Liu, Lingqiao Liu, Ian Reid, 和
    Chunhua Shen. 使用低位宽权重和激活进行卷积神经网络的有效训练。*IEEE Transactions on Pattern Analysis and
    Machine Intelligence*，44（10）：6140–6152，2021。
- en: Appendix A More results
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 更多结果
- en: A.1 Non-rigorous comparison with AWQ
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 与AWQ的非严格比较
- en: We conducted a limited comparison between our approach and AWQ Lin et al. ([2023](#bib.bib18)),
    considering that our evaluation methodology closely follows that of GPTQ and we
    only share a few common tasks with AWQ. It is important to acknowledge that this
    comparison inherently lacks rigor due to our reliance on referencing AWQ’s data
    alone. Consequently, this approach introduces the possibility of unfairness in
    the evaluation process, primarily stemming from the utilization of different calibration
    datasets and other potential factors that may influence the obtained results.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在评估方法论方面紧跟GPTQ，对我们的方案与AWQ Lin等人（[2023](#bib.bib18)）进行了有限的比较。需要指出的是，由于我们仅参考了AWQ的数据，这一比较在本质上缺乏严格性。因此，这种方法可能会引入评估过程中的不公平性，主要源于使用了不同的校准数据集和其他可能影响结果的因素。
- en: We present the results of our common tasks alongside AWQ in table [8](#A1.T8
    "Table 8 ‣ A.1 Non-rigorous comparison with AWQ ‣ Appendix A More results ‣ Optimize
    Weight Rounding via Signed Gradient Descent for the Quantization of LLMs") and
    all the results of AWQ are from their paper. While we both report MMLU results,
    it is important to note that there was a bug fix ³³3https://github.com/EleutherAI/lm-evaluation-harness/pull/497
    in lm-eval, resulting in significant changes to the baseline. So we have not included
    them in this report.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[8](#A1.T8 "Table 8 ‣ A.1 Non-rigorous comparison with AWQ ‣ Appendix A
    More results ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization
    of LLMs")中展示了与AWQ的共同任务结果，所有AWQ的结果均来自他们的论文。虽然我们都报告了MMLU结果，但需要注意的是，lm-eval中出现了一个错误修复³³3https://github.com/EleutherAI/lm-evaluation-harness/pull/497，这导致了基线的显著变化。因此我们没有在此报告中包含这些结果。
- en: 'Table 8: Reported results of AWQ and Ours'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：AWQ与我们的报告结果
- en: '| LLaMA-7B | AWQ | Ours |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | AWQ | 我们的 |'
- en: '| nbits | Method | PIQA | Hella. | Wino. | PIQA | Hella. | Wino. |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| nbits | 方法 | PIQA | Hella. | Wino. | PIQA | Hella. | Wino. |'
- en: '| 16 | FP16 | 78.35 | 56.44 | 67.09 | 78.35 | 56.42 | 66.85 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 16 | FP16 | 78.35 | 56.44 | 67.09 | 78.35 | 56.42 | 66.85 |'
- en: '| W3G128 | RTN | 75.84 | 53.10 | 63.22 | 75.73 | 53.17 | 63.14 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| W3G128 | RTN | 75.84 | 53.10 | 63.22 | 75.73 | 53.17 | 63.14 |'
- en: '| GPTQ | 70.89 | 46.77 | 60.93 | 72.58 | 47.10 | 59.91 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 70.89 | 46.77 | 60.93 | 72.58 | 47.10 | 59.91 |'
- en: '| Proposed | 76.66 | 53.63 | 66.14 | 76.61 | 53.98 | 66.06 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 提议的 | 76.66 | 53.63 | 66.14 | 76.61 | 53.98 | 66.06 |'
- en: '| W4G128 | RTN | 77.86 | 55.81 | 65.59 | 77.58 | 55.86 | 65.75 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| W4G128 | RTN | 77.86 | 55.81 | 65.59 | 77.58 | 55.86 | 65.75 |'
- en: '| GPTQ | 77.20 | 53.98 | 65.67 | 77.26 | 54.09 | 64.09 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 77.20 | 53.98 | 65.67 | 77.26 | 54.09 | 64.09 |'
- en: '| Proposed | 78.07 | 55.76 | 65.82 | 78.07 | 55.92 | 66.30 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 提议的 | 78.07 | 55.76 | 65.82 | 78.07 | 55.92 | 66.30 |'
- en: A.2 Results of Wikitext2 ppl at W4
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 Wikitext2 ppl在W4的结果
- en: The perplexity results for Wikitext2 at W4 are shown in Table [9](#A1.T9 "Table
    9 ‣ A.2 Results of Wikitext2 ppl at W4 ‣ Appendix A More results ‣ Optimize Weight
    Rounding via Signed Gradient Descent for the Quantization of LLMs"). In conclusion,
    our performance is comparable to that of GPTQ.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Wikitext2在W4的困惑度结果见表[9](#A1.T9 "Table 9 ‣ A.2 Results of Wikitext2 ppl at W4
    ‣ Appendix A More results ‣ Optimize Weight Rounding via Signed Gradient Descent
    for the Quantization of LLMs")。总之，我们的表现与GPTQ相当。
- en: 'Table 9: Wikitext2 ppl ( $\downarrow$) at W4'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：Wikitext2 ppl（$\downarrow$）在W4
- en: '|  | LLaMA | OPT | BLOOM |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA | OPT | BLOOM |'
- en: '| --- | --- | --- | --- |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Size | 7b | 13b | 7bv2 | 13bv2 | 1.3b | 2.7b | 6.7b | 13b | 560m | 1b7 |
    3b | 7b1 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 7b | 13b | 7bv2 | 13bv2 | 1.3b | 2.7b | 6.7b | 13b | 560m | 1b7 | 3b
    | 7b1 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| FP16 | 5.67 | 5.09 | 5.47 | 4.88 | 14.62 | 12.47 | 10.86 | 10.13 | 22.41
    | 15.39 | 13.48 | 11.37 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 5.67 | 5.09 | 5.47 | 4.88 | 14.62 | 12.47 | 10.86 | 10.13 | 22.41
    | 15.39 | 13.48 | 11.37 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| RTN | 6.29 | 5.53 | 6.12 | 5.20 | 48.20 | 16.92 | 12.10 | 11.32 | 25.88 |
    16.97 | 14.75 | 12.10 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 6.29 | 5.53 | 6.12 | 5.20 | 48.20 | 16.92 | 12.10 | 11.32 | 25.88 |
    16.97 | 14.75 | 12.10 |'
- en: '| GPTQ | 6.59 | 5.33 | 6.09 | 5.16 | 15.67 | 13.30 | 11.59 | 10.33 | 23.95
    | 16.37 | 14.10 | 11.73 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 6.59 | 5.33 | 6.09 | 5.16 | 15.67 | 13.30 | 11.59 | 10.33 | 23.95
    | 16.37 | 14.10 | 11.73 |'
- en: '|  Ours | 6.12 | 5.32 | 298.42 | 9.15 | 15.65 | 13.05 | 11.18 | 10.66 | 23.80
    | 16.22 | 14.13 | 11.80 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  我们的 | 6.12 | 5.32 | 298.42 | 9.15 | 15.65 | 13.05 | 11.18 | 10.66 | 23.80
    | 16.22 | 14.13 | 11.80 |'
- en: A.3 Other results for large models
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 大模型的其他结果
- en: We present the results for models with a capacity of 30B or higher at W4 in
    Table [10](#A1.T10 "Table 10 ‣ A.3 Other results for large models ‣ Appendix A
    More results ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization
    of LLMs") and PPL on Wikitext2 in Table [11](#A1.T11 "Table 11 ‣ A.3 Other results
    for large models ‣ Appendix A More results ‣ Optimize Weight Rounding via Signed
    Gradient Descent for the Quantization of LLMs"). Furthermore, we observed that
    adjusting the sequence length of the calibration dataset led to improvements in
    specific scenarios, and we include these findings in our analysis. Overall, our
    approach demonstrates comparable accuracy performance to GPTQ for the given task.
    However, it is worth noting that we slightly fall behind GPTQ in terms of PPL
    tasks.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表 [10](#A1.T10 "表 10 ‣ A.3 大模型的其他结果 ‣ 附录 A 更多结果 ‣ 通过有符号梯度下降优化权重舍入以量化LLMs")
    中展示了 W4 下 30B 或更高容量模型的结果，并在表 [11](#A1.T11 "表 11 ‣ A.3 大模型的其他结果 ‣ 附录 A 更多结果 ‣ 通过有符号梯度下降优化权重舍入以量化LLMs")
    中展示了 Wikitext2 的 PPL。此外，我们观察到调整校准数据集的序列长度在特定场景下带来了改进，我们将这些发现纳入分析中。总体而言，我们的方法在给定任务上的准确性表现与
    GPTQ 相当。然而，值得注意的是，在 PPL 任务上我们略微落后于 GPTQ。
- en: 'Table 10: Average % accuracy($\uparrow$ 30B at W4\. ”Ours-seq512” indicates
    that we have modified the sequence length of the calibration dataset from 256
    to 512.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：平均百分比准确率（$\uparrow$ 30B 在 W4 中。”Ours-seq512” 表示我们将校准数据集的序列长度从 256 修改为 512。
- en: '|  | Accuracy | PPL on C4 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | 准确性 | C4 上的 PPL |'
- en: '| Type | LLaMA | OPT | LLaMA | OPT |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | LLaMA | OPT | LLaMA | OPT |'
- en: '| Size | 30b | 65b | 30b | 66b | 30b | 65b | 30b | 66b |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 30b | 65b | 30b | 66b | 30b | 65b | 30b | 66b |'
- en: '| FP16 | 73.46 | 75.48 | 67.87 | 69.54 | 6.13 | 5.98 | 11.45 | 10.99 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 73.46 | 75.48 | 67.87 | 69.54 | 6.13 | 5.98 | 11.45 | 10.99 |'
- en: '| RTN | 72.33 | 73.91 | 65.94 | 37.12 | 6.54 | 6.46 | 13.56 | 305.73 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 72.33 | 73.91 | 65.94 | 37.12 | 6.54 | 6.46 | 13.56 | 305.73 |'
- en: '| GPTQ | 72.85 | 74.45 | 67.55 | 68.23 | 6.42 | 6.23 | 11.59 | 11.24 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 72.85 | 74.45 | 67.55 | 68.23 | 6.42 | 6.23 | 11.59 | 11.24 |'
- en: '| Ours-seq256 | 72.69 | 74.03 | 66.74 | 68.80 | 6.47 | 6.31 | 11.84 | 11.42
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| Ours-seq256 | 72.69 | 74.03 | 66.74 | 68.80 | 6.47 | 6.31 | 11.84 | 11.42
    |'
- en: '| Ours-seq512 | 72.86 | 73.91 | 67.40 | 69.22 | 6.47 | 6.34 | 11.77 | 11.45
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Ours-seq512 | 72.86 | 73.91 | 67.40 | 69.22 | 6.47 | 6.34 | 11.77 | 11.45
    |'
- en: 'Table 11: Wikitext ppl($\downarrow$ 30B. ”Ours-seq512” indicates that we have
    modified the sequence length of the calibration dataset from 256 to 512.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：Wikitext 的 PPL（$\downarrow$ 30B。”Ours-seq512” 表示我们将校准数据集的序列长度从 256 修改为
    512。
- en: '|  | W4 | W3G128 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | W3G128 |'
- en: '| Type | LLaMA | OPT | LLaMA | OPT |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | LLaMA | OPT | LLaMA | OPT |'
- en: '| Size | 30b | 65b | 30b | 66b | 30b | 65b | 30b | 66b |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 30b | 65b | 30b | 66b | 30b | 65b | 30b | 66b |'
- en: '| FP16 | 4.10 | 3.56 | 9.56 | 9.34 | 4.10 | 3.56 | 9.56 | 9.34 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 4.10 | 3.56 | 9.56 | 9.34 | 4.10 | 3.56 | 9.56 | 9.34 |'
- en: '| RTN | 4.54 | 3.99 | 10.98 | 110.43 | 4.87 | 4.44 | 23.05 | 126.92 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4.54 | 3.99 | 10.98 | 110.43 | 4.87 | 4.44 | 23.05 | 126.92 |'
- en: '| GPTQ | 4.45 | 4.16 | 9.66 | 9.66 | 4.84 | 4.17 | 9.75 | 10.58 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4.45 | 4.16 | 9.66 | 9.66 | 4.84 | 4.17 | 9.75 | 10.58 |'
- en: '| Ours-seq256 | 4.51 | 3.91 | 9.88 | 9.56 | 4.85 | 4.15 | 11.07 | 11.40 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Ours-seq256 | 4.51 | 3.91 | 9.88 | 9.56 | 4.85 | 4.15 | 11.07 | 11.40 |'
- en: '| Ours-seq512 | 4.52 | 3.90 | 9.88 | 9.70 | 4.81 | 4.17 | 10.54 | 10.87 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| Ours-seq512 | 4.52 | 3.90 | 9.88 | 9.70 | 4.81 | 4.17 | 10.54 | 10.87 |'
- en: Appendix B Visualization of V
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B V 的可视化
- en: We provide an analysis of the magnitude distribution of V in Eq. [2](#S3.E2
    "In 3 Methodology ‣ Optimize Weight Rounding via Signed Gradient Descent for the
    Quantization of LLMs") for approximately 7B models at W4 in Figure [3](#A2.F3
    "Figure 3 ‣ Appendix B Visualization of V ‣ Optimize Weight Rounding via Signed
    Gradient Descent for the Quantization of LLMs"). The findings reveal that the
    majority of V values are concentrated within the range of [-0.3, 0.3]. Notably,
    the middle layers demonstrate a narrower distribution in comparison to the other
    layers. This observation suggests that the head or tail layers may be more susceptible
    to the compression.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了关于图[3](#A2.F3 "Figure 3 ‣ Appendix B Visualization of V ‣ Optimize Weight
    Rounding via Signed Gradient Descent for the Quantization of LLMs") 中 W4 下约 7B
    模型的 Eq. [2](#S3.E2 "In 3 Methodology ‣ Optimize Weight Rounding via Signed Gradient
    Descent for the Quantization of LLMs") 中 V 的幅度分布分析。结果表明，大多数 V 值集中在 [-0.3, 0.3]
    范围内。值得注意的是，中间层的分布比其他层更窄。这一观察表明，头部或尾部层可能更容易受到压缩的影响。
- en: '![Refer to caption](img/0a11b7c16293b6a80900b2719283c38b.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0a11b7c16293b6a80900b2719283c38b.png)'
- en: LLaMA-7B
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-7B
- en: '![Refer to caption](img/5dcc140a60a30a111aac15c3554ac35f.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5dcc140a60a30a111aac15c3554ac35f.png)'
- en: LLaMA-7B-V2
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-7B-V2
- en: '![Refer to caption](img/659ef93dd082083a883d82b997800f8f.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/659ef93dd082083a883d82b997800f8f.png)'
- en: OPT-6.7B
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: OPT-6.7B
- en: '![Refer to caption](img/ba27e2512611134a6f898bb0da000695.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ba27e2512611134a6f898bb0da000695.png)'
- en: BLOOM-7B1
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: BLOOM-7B1
- en: 'Figure 3: The distribution of the magnitude of V in Eq. [2](#S3.E2 "In 3 Methodology
    ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization of
    LLMs") for different models, namely LLaMA-7B, LLaMA-7B-V2, OPT-6.7B, and BLOOM-7B1
    at W4\. Each color in the distribution represents a specific layer index in the
    models, with blue indicating shallow layers closer to the data layer, and red
    representing deeper layers.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：不同模型（即 LLaMA-7B、LLaMA-7B-V2、OPT-6.7B 和 BLOOM-7B1）在 W4\ 下的 Eq. [2](#S3.E2
    "In 3 Methodology ‣ Optimize Weight Rounding via Signed Gradient Descent for the
    Quantization of LLMs") 中 V 的幅度分布。分布中的每种颜色表示模型中的特定层索引，蓝色表示接近数据层的浅层，红色表示较深的层。
- en: Appendix C Correction between SignRound and salient activation channels
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C SignRound 与显著激活通道的校正
- en: We were also intrigued by the possible correlation between rounding and activation,
    as previous research has shown that keeping only 0.1%-1% of the channels corresponding
    to larger activation can significantly improve the quantized performance in AWQ
    (Lin et al., [2023](#bib.bib18)). Therefore, we investigated whether the altered
    rounding values tend to fall more frequently in these salient channels. The results
    of our analysis, presented in Figure [4](#A3.F4 "Figure 4 ‣ Appendix C Correction
    between SignRound and salient activation channels ‣ Optimize Weight Rounding via
    Signed Gradient Descent for the Quantization of LLMs"), reveal an interesting
    finding. The ratio, representing the percentage of altered rounding values falling
    within the top 1% salient activation channels out of all altered rounding values,
    is typically around 1%. This suggests that there is no strong correlation between
    rounding and activation. It is possible that rounding values of less significant
    channels need to be changed to compensate for the quantization error introduced
    by these salient channels.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也对舍入与激活之间的可能相关性感到好奇，因为先前的研究表明，仅保留0.1%-1%与较大激活相关的通道可以显著提高AWQ中的量化性能（Lin et al.,
    [2023](#bib.bib18)）。因此，我们调查了是否改变的舍入值更倾向于落在这些显著通道中。我们分析的结果，展示在图[4](#A3.F4 "Figure
    4 ‣ Appendix C Correction between SignRound and salient activation channels ‣
    Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs")中，揭示了一个有趣的发现。该比例表示落在前1%显著激活通道中的改变舍入值的百分比，通常约为1%。这表明舍入与激活之间没有强相关性。可能是由于显著通道引入的量化误差，需要改变不太显著通道的舍入值来补偿。
- en: '![Refer to caption](img/f79169c914a4b1488548fa55106ba8cb.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f79169c914a4b1488548fa55106ba8cb.png)'
- en: LLaMA-7B
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-7B
- en: '![Refer to caption](img/a6117018d509032e216106f6e823f031.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a6117018d509032e216106f6e823f031.png)'
- en: LLaMA-7B-V2
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-7B-V2
- en: '![Refer to caption](img/210ad7b3d2e9ea047de69ac6eddf3bdc.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/210ad7b3d2e9ea047de69ac6eddf3bdc.png)'
- en: OPT-6.7B
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: OPT-6.7B
- en: '![Refer to caption](img/d2e7a03de109e75243e719ca92b457e3.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d2e7a03de109e75243e719ca92b457e3.png)'
- en: BLOOM-7B1
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: BLOOM-7B1
- en: 'Figure 4: The correction between SignRound and salient activation channels'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：SignRound 与显著激活通道之间的校正
- en: Appendix D Runtime
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 运行时
- en: Table [12](#A4.T12 "Table 12 ‣ Appendix D Runtime ‣ Optimize Weight Rounding
    via Signed Gradient Descent for the Quantization of LLMs") provides a runtime
    comparison between GPTQ and our method. All measurements were conducted on a single
    NVIDIA A100 card with 80GB of memory. Although our method demonstrates slightly
    slower performance compared to GPTQ, it remains well within acceptable limits
    for real-world deployment.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [12](#A4.T12 "表 12 ‣ 附录 D 运行时间 ‣ 通过带符号梯度下降优化权重舍入以量化 LLMs") 提供了 GPTQ 和我们方法之间的运行时间比较。所有测量都在一张具有
    80GB 内存的 NVIDIA A100 显卡上进行。虽然我们的方法在性能上略逊于 GPTQ，但仍在实际部署的可接受范围内。
- en: 'Table 12: Runtime in seconds at W4'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：W4 下的运行时间（秒）
- en: '| Type | LLaMA | OPT | BLOOM |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | LLaMA | OPT | BLOOM |'
- en: '| --- | --- | --- | --- |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | 7B | 13B | 6.7B | 13B | 3B | 7B1 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | 7B | 13B | 6.7B | 13B | 3B | 7B1 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPTQ | 712 | 1240 | 841 | 1523 | 345 | 661 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 712 | 1240 | 841 | 1523 | 345 | 661 |'
- en: '| Ours | 899 | 1590 | 819 | 1429 | 467 | 843 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 899 | 1590 | 819 | 1429 | 467 | 843 |'
- en: Appendix E Detailed Results of some LLaMa models
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 一些 LLaMa 模型的详细结果
- en: Detailed results of LLaMA7B, LLaMA13B, LLAMA7B-V2, and LLAMA13B-V2 can be found
    in Table [13](#A5.T13 "Table 13 ‣ Appendix E Detailed Results of some LLaMa models
    ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization of
    LLMs"), Table [14](#A5.T14 "Table 14 ‣ Appendix E Detailed Results of some LLaMa
    models ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization
    of LLMs"), Table [15](#A5.T15 "Table 15 ‣ Appendix E Detailed Results of some
    LLaMa models ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization
    of LLMs"), and Table [16](#A5.T16 "Table 16 ‣ Appendix E Detailed Results of some
    LLaMa models ‣ Optimize Weight Rounding via Signed Gradient Descent for the Quantization
    of LLMs") respectively.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA7B、LLaMA13B、LLAMA7B-V2 和 LLAMA13B-V2 的详细结果可以在表 [13](#A5.T13 "表 13 ‣ 附录
    E 一些 LLaMa 模型的详细结果 ‣ 通过带符号梯度下降优化权重舍入以量化 LLMs")、表 [14](#A5.T14 "表 14 ‣ 附录 E 一些
    LLaMa 模型的详细结果 ‣ 通过带符号梯度下降优化权重舍入以量化 LLMs")、表 [15](#A5.T15 "表 15 ‣ 附录 E 一些 LLaMa
    模型的详细结果 ‣ 通过带符号梯度下降优化权重舍入以量化 LLMs") 和表 [16](#A5.T16 "表 16 ‣ 附录 E 一些 LLaMa 模型的详细结果
    ‣ 通过带符号梯度下降优化权重舍入以量化 LLMs") 找到。
- en: 'Table 13: Accuracies($\uparrow$) of WikiText, PTB, C4 for LLaMA-7B, ”Ours-2048”
    indicates that we have modified the sequence length of the calibration dataset
    from 512 to 2048.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：LLaMA-7B 在 WikiText、PTB、C4 上的准确度（$\uparrow$），”Ours-2048” 表示我们将校准数据集的序列长度从
    512 修改为 2048。
- en: '|  |  | Hella. | Wino. | PIQA | Lamb. | Avg. | Wiki. | PTB | C4 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Hella. | Wino. | PIQA | Lamb. | 平均 | Wiki. | PTB | C4 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | FP16 | 56.42 | 66.85 | 78.35 | 73.57 | 68.80 | 5.68 | 10.12 | 7.34 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 56.42 | 66.85 | 78.35 | 73.57 | 68.80 | 5.68 | 10.12 | 7.34 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| W4G-1 | RTN | 54.96 | 67.25 | 77.31 | 70.00 | 67.38 | 6.29 | 11.25 | 8.12
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| W4G-1 | RTN | 54.96 | 67.25 | 77.31 | 70.00 | 67.38 | 6.29 | 11.25 | 8.12
    |'
- en: '| GPTQ | 52.25 | 63.85 | 74.59 | 68.12 | 64.70 | 6.59 | 12.02 | 8.64 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 52.25 | 63.85 | 74.59 | 68.12 | 64.70 | 6.59 | 12.02 | 8.64 |'
- en: '| Ours | 55.28 | 66.14 | 77.64 | 73.14 | 68.05 | 6.12 | 10.88 | 7.84 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 55.28 | 66.14 | 77.64 | 73.14 | 68.05 | 6.12 | 10.88 | 7.84 |'
- en: '| Ours-2048 | 54.96 | 67.56 | 77.80 | 72.13 | 68.11 | 6.05 | 10.87 | 7.76 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| Ours-2048 | 54.96 | 67.56 | 77.80 | 72.13 | 68.11 | 6.05 | 10.87 | 7.76 |'
- en: '| W4G128 | RTN | 55.86 | 65.75 | 77.58 | 72.25 | 67.86 | 5.96 | 10.54 | 7.70
    |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| W4G128 | RTN | 55.86 | 65.75 | 77.58 | 72.25 | 67.86 | 5.96 | 10.54 | 7.70
    |'
- en: '| GPTQ | 54.09 | 64.09 | 77.26 | 69.86 | 66.33 | 6.29 | 11.11 | 8.02 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 54.09 | 64.09 | 77.26 | 69.86 | 66.33 | 6.29 | 11.11 | 8.02 |'
- en: '| Ours | 55.92 | 66.30 | 78.07 | 72.07 | 68.09 | 5.86 | 10.49 | 7.56 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 55.92 | 66.30 | 78.07 | 72.07 | 68.09 | 5.86 | 10.49 | 7.56 |'
- en: '| Ours-2048 | 55.98 | 66.77 | 78.29 | 71.78 | 68.21 | 5.88 | 10.52 | 7.58 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| Ours-2048 | 55.98 | 66.77 | 78.29 | 71.78 | 68.21 | 5.88 | 10.52 | 7.58 |'
- en: '| W3G128 | RTN | 53.17 | 63.14 | 75.73 | 67.71 | 64.94 | 7.01 | 12.83 | 9.18
    |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| W3G128 | RTN | 53.17 | 63.14 | 75.73 | 67.71 | 64.94 | 7.01 | 12.83 | 9.18
    |'
- en: '| GPTQ | 47.10 | 59.91 | 72.58 | 53.58 | 58.29 | 8.28 | 16.84 | 10.45 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 47.10 | 59.91 | 72.58 | 53.58 | 58.29 | 8.28 | 16.84 | 10.45 |'
- en: '| Ours | 53.98 | 66.06 | 76.61 | 69.82 | 66.62 | 6.93 | 11.67 | 8.30 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 53.98 | 66.06 | 76.61 | 69.82 | 66.62 | 6.93 | 11.67 | 8.30 |'
- en: '| Ours-2048 | 53.45 | 65.67 | 76.55 | 71.08 | 66.69 | 6.52 | 11.60 | 8.26 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| Ours-2048 | 53.45 | 65.67 | 76.55 | 71.08 | 66.69 | 6.52 | 11.60 | 8.26 |'
- en: 'Table 14: Accuracies($\uparrow$) of WikiText, PTB, C4 for LLaMA-13B, ”Ours-2048”
    indicates that we have modified the sequence length of the calibration dataset
    from 512 to 2048.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '表 14: LLaMA-13B 在 WikiText、PTB、C4 上的准确率（$\uparrow$），"Ours-2048" 表示我们已将校准数据集的序列长度从
    512 修改为 2048。'
- en: '|  |  | Hella. | Wino. | PIQA | Lamb. | Avg. | Wiki. | PTB | C4 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Hella. | Wino. | PIQA | Lamb. | Avg. | Wiki. | PTB | C4 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | FP16 | 59.13 | 70.32 | 78.94 | 76.17 | 71.14 | 5.09 | 9.08 | 6.80 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 59.13 | 70.32 | 78.94 | 76.17 | 71.14 | 5.09 | 9.08 | 6.80 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| W4G-1 | RTN | 57.96 | 68.19 | 78.18 | 70.95 | 68.82 | 5.53 | 9.78 | 7.23
    |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| W4G-1 | RTN | 57.96 | 68.19 | 78.18 | 70.95 | 68.82 | 5.53 | 9.78 | 7.23
    |'
- en: '| GPTQ | 57.96 | 70.24 | 77.97 | 73.84 | 70.00 | 5.33 | 9.48 | 7.13 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 57.96 | 70.24 | 77.97 | 73.84 | 70.00 | 5.33 | 9.48 | 7.13 |'
- en: '| Ours | 58.02 | 69.61 | 78.94 | 75.74 | 70.58 | 5.32 | 9.37 | 7.05 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 58.02 | 69.61 | 78.94 | 75.74 | 70.58 | 5.32 | 9.37 | 7.05 |'
- en: '| Ours-2048 | 58.13 | 69.69 | 78.67 | 74.95 | 70.36 | 5.34 | 9.49 | 7.05 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Ours-2048 | 58.13 | 69.69 | 78.67 | 74.95 | 70.36 | 5.34 | 9.49 | 7.05 |'
- en: '| W4G128 | RTN | 58.43 | 70.32 | 79.33 | 75.32 | 70.85 | 5.26 | 9.29 | 6.94
    |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| W4G128 | RTN | 58.43 | 70.32 | 79.33 | 75.32 | 70.85 | 5.26 | 9.29 | 6.94
    |'
- en: '| GPTQ | 58.79 | 70.56 | 79.33 | 75.00 | 70.92 | 5.21 | 9.28 | 6.92 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 58.79 | 70.56 | 79.33 | 75.00 | 70.92 | 5.21 | 9.28 | 6.92 |'
- en: '| Ours | 58.62 | 71.35 | 79.76 | 75.98 | 71.43 | 5.19 | 9.18 | 6.90 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 58.62 | 71.35 | 79.76 | 75.98 | 71.43 | 5.19 | 9.18 | 6.90 |'
- en: '| Ours-2048 | 58.47 | 70.56 | 79.22 | 76.23 | 71.12 | 5.19 | 9.19 | 6.90 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| Ours-2048 | 58.47 | 70.56 | 79.22 | 76.23 | 71.12 | 5.19 | 9.19 | 6.90 |'
- en: '| W3G128 | RTN | 56.39 | 67.56 | 77.20 | 69.63 | 67.70 | 5.88 | 10.58 | 7.86
    |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| W3G128 | RTN | 56.39 | 67.56 | 77.20 | 69.63 | 67.70 | 5.88 | 10.58 | 7.86
    |'
- en: '| GPTQ | 56.58 | 67.96 | 78.07 | 72.31 | 68.73 | 5.64 | 9.95 | 7.54 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 56.58 | 67.96 | 78.07 | 72.31 | 68.73 | 5.64 | 9.95 | 7.54 |'
- en: '| Ours | 57.04 | 69.14 | 77.86 | 74.33 | 69.59 | 5.53 | 9.81 | 7.39 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 57.04 | 69.14 | 77.86 | 74.33 | 69.59 | 5.53 | 9.81 | 7.39 |'
- en: '| Ours-2048 | 56.62 | 68.82 | 78.13 | 74.42 | 69.50 | 5.57 | 9.76 | 7.37 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Ours-2048 | 56.62 | 68.82 | 78.13 | 74.42 | 69.50 | 5.57 | 9.76 | 7.37 |'
- en: 'Table 15: Accuracies($\uparrow$) of WikiText, PTB, C4 for LLaMA-7B-V2, ”Ours-2048”
    indicates that we have modified the sequence length of the calibration dataset
    from 512 to 2048.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '表 15: LLaMA-7B-V2 在 WikiText、PTB、C4 上的准确率（$\uparrow$），"Ours-2048" 表示我们已将校准数据集的序列长度从
    512 修改为 2048。'
- en: '|  |  | Hella. | Wino. | PIQA | Lamb. | Avg. | Wiki. | PTB | C4 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Hella. | Wino. | PIQA | Lamb. | Avg. | Wiki. | PTB | C4 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | FP16 | 56.69 | 67.17 | 78.35 | 73.88 | 69.02 | 5.47 | 32.91 | 7.26 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 56.69 | 67.17 | 78.35 | 73.88 | 69.02 | 5.47 | 32.91 | 7.26 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| W4G-1 | RTN | 55.51 | 66.77 | 77.58 | 68.08 | 66.98 | 6.12 | 61.61 | 8.16
    |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| W4G-1 | RTN | 55.51 | 66.77 | 77.58 | 68.08 | 66.98 | 6.12 | 61.61 | 8.16
    |'
- en: '| GPTQ | 54.74 | 66.93 | 76.17 | 69.73 | 66.89 | 6.09 | NAN | 7.90 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 54.74 | 66.93 | 76.17 | 69.73 | 66.89 | 6.09 | NAN | 7.90 |'
- en: '| Ours | 55.53 | 67.09 | 77.53 | 70.81 | 67.74 | 298.4 | 2677 | 11.20 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 55.53 | 67.09 | 77.53 | 70.81 | 67.74 | 298.4 | 2677 | 11.20 |'
- en: '| Ours-2048 | 55.63 | 67.96 | 77.64 | 69.92 | 67.79 | 196.7 | 2622 | 9.97 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| Ours-2048 | 55.63 | 67.96 | 77.64 | 69.92 | 67.79 | 196.7 | 2622 | 9.97 |'
- en: '| W4G128 | RTN | 56.55 | 66.93 | 77.37 | 72.44 | 68.32 | 5.72 | 50.25 | 7.58
    |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| W4G128 | RTN | 56.55 | 66.93 | 77.37 | 72.44 | 68.32 | 5.72 | 50.25 | 7.58
    |'
- en: '| GPTQ | 56.16 | 68.03 | 78.56 | 72.83 | 68.90 | 5.73 | NAN | 7.53 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 56.16 | 68.03 | 78.56 | 72.83 | 68.90 | 5.73 | NAN | 7.53 |'
- en: '| Ours | 56.21 | 67.56 | 77.64 | 73.20 | 68.65 | 60.03 | 1786 | 8.16 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 56.21 | 67.56 | 77.64 | 73.20 | 68.65 | 60.03 | 1786 | 8.16 |'
- en: '| Ours-2048 | 55.97 | 67.09 | 77.15 | 73.57 | 68.45 | 48.91 | 1872 | 8.05 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Ours-2048 | 55.97 | 67.09 | 77.15 | 73.57 | 68.45 | 48.91 | 1872 | 8.05 |'
- en: '| W3G128 | RTN | 54.65 | 67.17 | 75.90 | 65.98 | 65.92 | 6.66 | 44.89 | 8.98
    |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| W3G128 | RTN | 54.65 | 67.17 | 75.90 | 65.98 | 65.92 | 6.66 | 44.89 | 8.98
    |'
- en: '| GPTQ | 52.93 | 65.19 | 76.44 | 67.49 | 65.51 | 6.57 | NAN | 8.61 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 52.93 | 65.19 | 76.44 | 67.49 | 65.51 | 6.57 | NAN | 8.61 |'
- en: '| Ours | 53.65 | 66.14 | 77.09 | 70.64 | 66.88 | NAN | 1159 | 9.88 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 53.65 | 66.14 | 77.09 | 70.64 | 66.88 | NAN | 1159 | 9.88 |'
- en: '| Ours-2048 | 53.91 | 67.32 | 76.33 | 71.12 | 67.17 | NAN | 1739 | 10.11 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| Ours-2048 | 53.91 | 67.32 | 76.33 | 71.12 | 67.17 | NAN | 1739 | 10.11 |'
- en: 'Table 16: Accuracies($\uparrow$) of WikiText, PTB, C4 for LLaMA-13B-V2, ”Ours-2048”
    indicates that we have modified the sequence length of the calibration dataset
    from 512 to 2048.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '表 16: LLaMA-13B-V2 在 WikiText、PTB、C4 上的准确率（$\uparrow$），"Ours-2048" 表示我们已将校准数据集的序列长度从
    512 修改为 2048。'
- en: '|  |  | Hella. | Wino. | PIQA | Lamb. | Avg. | Wiki. | PTB | C4 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Hella. | Wino. | PIQA | Lamb. | Avg. | Wiki. | PTB | C4 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | FP16 | 59.71 | 69.61 | 78.78 | 76.71 | 71.20 | 4.88 | 48.82 | 6.73 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 59.71 | 69.61 | 78.78 | 76.71 | 71.20 | 4.88 | 48.82 | 6.73 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| W4G-1 | RTN | 58.56 | 69.30 | 78.45 | 74.36 | 70.17 | 5.20 | 58.57 | 7.14
    |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| W4G-1 | RTN | 58.56 | 69.30 | 78.45 | 74.36 | 70.17 | 5.20 | 58.57 | 7.14
    |'
- en: '| GPTQ | 57.81 | 67.48 | 77.86 | 73.84 | 69.25 | 5.16 | 52.46 | 6.87 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 57.81 | 67.48 | 77.86 | 73.84 | 69.25 | 5.16 | 52.46 | 6.87 |'
- en: '| Ours | 58.63 | 69.61 | 77.91 | 73.98 | 70.03 | 9.15 | 66.80 | 7.72 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 58.63 | 69.61 | 77.91 | 73.98 | 70.03 | 9.15 | 66.80 | 7.72 |'
- en: '| Ours-2048 | 58.87 | 68.67 | 78.07 | 75.90 | 70.38 | 6.51 | 60.35 | 7.28 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 我们的-2048 | 58.87 | 68.67 | 78.07 | 75.90 | 70.38 | 6.51 | 60.35 | 7.28 |'
- en: '| W4G128 | RTN | 59.12 | 69.46 | 78.02 | 76.29 | 70.72 | 4.98 | 52.22 | 6.87
    |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| W4G128 | RTN | 59.12 | 69.46 | 78.02 | 76.29 | 70.72 | 4.98 | 52.22 | 6.87
    |'
- en: '| GPTQ | 59.22 | 68.51 | 78.84 | 76.13 | 70.68 | 4.99 | 51.59 | 6.87 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 59.22 | 68.51 | 78.84 | 76.13 | 70.68 | 4.99 | 51.59 | 6.87 |'
- en: '| Ours | 59.20 | 69.14 | 78.35 | 76.56 | 70.81 | 5.80 | 51.92 | 6.84 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 59.20 | 69.14 | 78.35 | 76.56 | 70.81 | 5.80 | 51.92 | 6.84 |'
- en: '| Ours-2048 | 59.25 | 70.48 | 78.29 | 76.81 | 71.21 | 5.00 | 51.78 | 6.84 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 我们的-2048 | 59.25 | 70.48 | 78.29 | 76.81 | 71.21 | 5.00 | 51.78 | 6.84 |'
- en: '| W3G128 | RTN | 57.03 | 67.56 | 77.86 | 72.37 | 68.70 | 5.52 | 62.33 | 7.58
    |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| W3G128 | RTN | 57.03 | 67.56 | 77.86 | 72.37 | 68.70 | 5.52 | 62.33 | 7.58
    |'
- en: '| GPTQ | 56.99 | 66.69 | 78.40 | 72.85 | 68.73 | 5.45 | 55.09 | 7.54 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 56.99 | 66.69 | 78.40 | 72.85 | 68.73 | 5.45 | 55.09 | 7.54 |'
- en: '| Ours | 57.29 | 68.90 | 77.37 | 75.22 | 69.70 | 5.35 | 59.57 | 7.35 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 57.29 | 68.90 | 77.37 | 75.22 | 69.70 | 5.35 | 59.57 | 7.35 |'
- en: '| Ours-2048 | 57.20 | 70.88 | 78.13 | 75.35 | 70.39 | 10.38 | 66.22 | 7.92
    |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 我们的-2048 | 57.20 | 70.88 | 78.13 | 75.35 | 70.39 | 10.38 | 66.22 | 7.92 |'
