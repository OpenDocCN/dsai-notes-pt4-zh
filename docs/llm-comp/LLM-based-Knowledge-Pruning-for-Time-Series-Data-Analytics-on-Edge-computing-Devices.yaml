- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:04:43'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:43
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing
    Devices
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的时间序列数据分析知识剪枝在边缘计算设备上的应用
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.08765](https://ar5iv.labs.arxiv.org/html/2406.08765)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.08765](https://ar5iv.labs.arxiv.org/html/2406.08765)
- en: Ruibing Jin
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ruibing Jin
- en: '&Qing Xu'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '&Qing Xu'
- en: Min Wu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Min Wu
- en: '&Yuecong Xu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&Yuecong Xu'
- en: '&Dan Li'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&Dan Li'
- en: '&Xiaoli Li'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '&Xiaoli Li'
- en: '&Zhenghua Chen'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Zhenghua Chen'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Limited by the scale and diversity of time series data, the neural networks
    trained on time series data often overfit and show unsatisfacotry performances.
    In comparison, large language models (LLMs) recently exhibit impressive generalization
    in diverse fields. Although massive LLM based approaches are proposed for time
    series tasks, these methods require to load the whole LLM in both training and
    reference. This high computational demands limit practical applications in resource-constrained
    settings, like edge-computing and IoT devices. To address this issue, we propose
    Knowledge Pruning (KP), a novel paradigm for time series learning in this paper.
    For a specific downstream task, we argue that the world knowledge learned by LLMs
    is much redundant and only the related knowledge termed as "pertinent knowledge"
    is useful. Unlike other methods, our KP targets to prune the redundant knowledge
    and only distill the pertinent knowledge into the target model. This reduces model
    size and computational costs significantly. Additionally, different from existing
    LLM based approaches, our KP does not require to load the LLM in the process of
    training and testing, further easing computational burdens. With our proposed
    KP, a lightweight network can effectively learn the pertinent knowledge, achieving
    satisfactory performances with a low computation cost. To verify the effectiveness
    of our KP, two fundamental tasks on edge-computing devices are investigated in
    our experiments, where eight diverse environments or benchmarks with different
    networks are used to verify the generalization of our KP. Through experiments,
    our KP demonstrates effective learning of pertinent knowledge, achieving notable
    performance improvements in regression (19.7% on average) and classification (up
    to 13.7%) tasks, showcasing state-of-the-art results.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于时间序列数据的规模和多样性限制，基于时间序列数据训练的神经网络往往出现过拟合现象，并且表现不尽如人意。相比之下，最近的大型语言模型（LLMs）在多个领域展现了令人印象深刻的泛化能力。尽管已经提出了大量基于LLM的方法来处理时间序列任务，这些方法在训练和引用过程中都需要加载整个LLM。这种高计算需求限制了在资源受限的环境下的实际应用，例如边缘计算和物联网设备。为了解决这个问题，我们在本文中提出了知识剪枝（KP），这是一种用于时间序列学习的新范式。对于特定的下游任务，我们认为LLM所学习的世界知识中有很大一部分是冗余的，只有被称为“相关知识”的部分才是有用的。与其他方法不同，我们的KP旨在剪枝冗余知识，仅将相关知识提炼到目标模型中。这显著减少了模型的大小和计算成本。此外，与现有的基于LLM的方法不同，我们的KP在训练和测试过程中不需要加载LLM，进一步减轻了计算负担。通过我们提出的KP，一个轻量级网络可以有效学习相关知识，实现令人满意的性能且计算成本低。为了验证我们KP的有效性，我们在实验中调查了两个基础任务，使用了八种不同环境或基准与不同的网络来验证我们KP的泛化能力。通过实验，我们的KP展示了有效的相关知识学习，在回归（平均提高19.7%）和分类（最高提升13.7%）任务中取得了显著的性能提升，展现了最先进的结果。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: With the advancement of deep learning, massive methods are proposed for time
    series learning across different fields such as healthcare Zhao et al. ([2019](#bib.bib1));
    Chen et al. ([2018](#bib.bib2)), transportation Jin et al. ([2023a](#bib.bib3)),
    energy Zhu et al. ([2023](#bib.bib4)) and industry Chen et al. ([2020](#bib.bib5)).
    Although these approaches show significant improvements on some benchmarks, it
    is still challenging to generalize these methods to complex scenarios Jin et al.
    ([2023b](#bib.bib6)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的进步，提出了大量方法用于不同领域的时间序列学习，例如医疗保健赵等（[2019](#bib.bib1)）；陈等（[2018](#bib.bib2)）、交通金等（[2023a](#bib.bib3)）、能源朱等（[2023](#bib.bib4)）和工业陈等（[2020](#bib.bib5)）。尽管这些方法在一些基准上显示了显著的改进，但在复杂场景下泛化这些方法仍然具有挑战性金等（[2023b](#bib.bib6)）。
- en: The main issue which limits the generalization of existing time series approaches,
    is that different measurements are applied in the process of time series data
    collection. Unlike computer vision and language, it is difficult to combine these
    time series datasets collected from different measurements into a large scale
    dataset. Limited by the scale and diversity of a single time series dataset, the
    generalization of trained neural network on time series data is not satisfacotry.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 限制现有时间序列方法泛化能力的主要问题是，时间序列数据采集过程中应用了不同的测量方式。与计算机视觉和语言不同，将这些从不同测量方式中采集的时间序列数据集整合成一个大规模数据集是困难的。由于单一时间序列数据集的规模和多样性有限，基于时间序列数据训练的神经网络的泛化能力不令人满意。
- en: '![Refer to caption](img/e340f150b5dfa13182a38e0b215dab4d.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e340f150b5dfa13182a38e0b215dab4d.png)'
- en: 'Figure 1: The world knowledge learned by LLMs. Different parts of the knowledge
    of LLMs may contribute differently on diverse tasks. For a specific task, the
    world knowledge in LLMs is actually redundant and only the related knowledge termed
    as pertinent knowledge is useful. Our proposed Knowledge Pruning (KP) aims to
    prune the redundant knowledge and effectively transfer the pertinent knowledge
    to the target model, significantly reducing computation cost while retaining satisfactory
    performances.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLM学到的世界知识。LLM的不同知识部分可能在不同任务中有不同的贡献。对于特定任务，LLM中的世界知识实际上是冗余的，只有被称为相关知识的知识才是有用的。我们提出的知识修剪（KP）旨在修剪冗余知识，并有效地将相关知识传递到目标模型中，从而显著降低计算成本，同时保持令人满意的性能。
- en: Recently, large language models (LLM) with tens of billions of parameters, demonstrate
    remarkable generalization capabilities in different tasks Touvron et al. ([2023](#bib.bib7));
    Peng et al. ([2023](#bib.bib8)). Pre-trained on massive corpus of self-supervised
    data, these foundation models implicitly capture knowledge understanding on the
    world, which enables them to be zero-shot transferable on downstream tasks. To
    alleviate the issues in time series learning, some methods Xue and Salim ([2023](#bib.bib9));
    Chang et al. ([2023](#bib.bib10)); Zhou et al. ([2023](#bib.bib11)); Gruver et al.
    ([2023](#bib.bib12)) are proposed to integrate the knowledge from LLMs into their
    frameworks. Nevertheless, there are two issues in these LLM based time series
    methods.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，具有数十亿参数的大型语言模型（LLM）在不同任务中展示了显著的泛化能力 Touvron et al. ([2023](#bib.bib7)); Peng
    et al. ([2023](#bib.bib8))。这些基础模型在大规模自监督数据语料上进行预训练，隐式捕捉了对世界的知识理解，使其在下游任务中能够零-shot
    迁移。为了缓解时间序列学习中的问题，一些方法 Xue and Salim ([2023](#bib.bib9)); Chang et al. ([2023](#bib.bib10));
    Zhou et al. ([2023](#bib.bib11)); Gruver et al. ([2023](#bib.bib12)) 被提出，将LLM中的知识整合到其框架中。然而，这些基于LLM的时间序列方法仍存在两个问题。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: These approaches often require to load the whole LLM during training and inference,
    which is computationally expensive and time-consuming.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些方法通常要求在训练和推理过程中加载整个LLM，这在计算上是昂贵且耗时的。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: These methods are generally based on a pre-trained and fixed LLM, which largely
    limits the flexible of these methods.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些方法通常基于一个预训练且固定的LLM，这在很大程度上限制了这些方法的灵活性。
- en: Limited by these issues above, It is challenging for existing LLM based methods
    to flexibly design models with different scales according to the requirements
    of tasks, especially for some computation constrained scenarios.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 受到上述问题的限制，现有基于LLM的方法很难根据任务的需求灵活设计不同规模的模型，特别是在一些计算受限的场景中。
- en: 'To address this problem, we re-evaluate the impact of the world knowledge acquired
    by LLMs on downstream tasks. We argue that for a specific downstream task, it
    is not necessary to transfer th entire knowledge of a pre-trained LLM into a target
    model. Instead, as illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices"),
    we contend that this world knowledge actually can be divided into two parts: related
    knowledge and redundant knowledge for a specific task. Only the related knowledge
    termed as "pertinent knowledge" is what we need to transfer to the target model.
    Motivated by this discernment, we propose a novel compression paradigm called
    Knowledge Pruning (KP) for LLMs, which is able to identify the pertinent knowledge,
    prune the redundant knowledge and effectively distill the pertinent knowledge
    to our target model.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们重新评估了LLMs获得的世界知识对下游任务的影响。我们认为，对于特定的下游任务，并不需要将预训练LLM的全部知识转移到目标模型中。相反，如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM-based Knowledge Pruning for Time Series
    Data Analytics on Edge-computing Devices") 所示，我们认为这些世界知识实际上可以分为两部分：与特定任务相关的知识和冗余知识。只有被称为“相关知识”的部分才是我们需要转移到目标模型中的。受此洞察的启发，我们提出了一种新型的压缩范式，称为知识剪枝（KP），它能够识别相关知识，剪枝冗余知识，并有效地将相关知识提炼到我们的目标模型中。
- en: Knowledge is implicitly stored in a neural network. It is generally difficult
    to directly obtain a specified part of knowledge from a network. However, unlike
    traditional networks, LLMs can produce related knowledge description via prompts
    based on a dialogue scheme. According to this scheme, our proposed KP firstly
    generates a knowledge prompt set (KPS) for a specific task, where these prompts
    are forwarded to a pre-trained LLM to produce corresponding embeddings. In our
    proposed KP, these embeddings are called knowledge anchor points (KAPs). Although
    the latent space of the pertinent knowledge is a continuous space, these KAPs
    can be used to roughly represent this latent space. After that, a metric learning
    is applied to learn this prior knowledge via knowledge distillation and transfer
    this pertinent knowledge to our target model. Additionally, the regression task
    requires a network to learn the continuous domain of the task and predict arbitrary
    value. To fulfill this requirement, an anchor voting scheme (AVS) is proposed,
    where the confidence distribution among different anchor points is generated to
    predict the expected output.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 知识隐式地存储在神经网络中。通常很难直接从网络中获取特定部分的知识。然而，与传统网络不同，LLMs可以通过基于对话方案的提示生成相关的知识描述。根据这个方案，我们提出的KP首先为特定任务生成一个知识提示集（KPS），这些提示被转发到预训练LLM以产生相应的嵌入。在我们提出的KP中，这些嵌入被称为知识锚点（KAPs）。尽管相关知识的潜在空间是一个连续的空间，但这些KAPs可以用来粗略地表示这个潜在空间。之后，应用度量学习通过知识蒸馏学习这些先验知识，并将相关知识转移到我们的目标模型中。此外，回归任务要求网络学习任务的连续域并预测任意值。为了满足这一要求，提出了一种锚点投票方案（AVS），生成不同锚点之间的置信度分布以预测预期输出。
- en: 'To verify the effectiveness of our proposed KP, massive experiments are conducted
    on two fundamental tasks on edge-computing devices, where different network architectures
    are investigated on two different task categories: classification and regression
    in time series learning. In classification, we evaluate the performances of our
    KP on four different benchmarks of human activity recognition, where our approach
    effectively improve the performances by up to 13.7%. In regression, we investigate
    the performance of our KP on the remaining useful life prediction under four different
    scenarios. Through experiments, our proposed KP significantly improves the accuracy
    by 19.7% on average. Our proposed KP achieves state-of-the-art performances on
    both tasks. Overall, our contributions are summarized as below:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为验证我们提出的KP的有效性，在边缘计算设备上进行了大量实验，研究了两种不同任务类别（分类和时间序列学习中的回归）的不同网络架构。在分类任务中，我们评估了KP在四个不同的人体活动识别基准上的表现，我们的方法有效地提高了最多13.7%的性能。在回归任务中，我们研究了KP在四种不同场景下的剩余使用寿命预测中的表现。通过实验，我们提出的KP平均提高了19.7%的准确率。我们的KP在这两项任务上均实现了最先进的性能。总体而言，我们的贡献总结如下：
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discover that the knowledge in LLMs is much redundant for a specific downstream
    task. In stead of the entire knowledge, only the pertinent knowledge needs to
    be transferred to the target model.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们发现LLMs中的知识对于特定的下游任务来说是多余的。与其传递整个知识，不如仅传递相关知识到目标模型中。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A novel compression paradigm, Knowledge Pruning (KP) is proposed to effectively
    distill the pertinent knowledge into the target model, which achieves satisfactory
    performances, while remaining low computation cost.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出了一个新颖的压缩范式，知识剪枝（KP），它能够有效地将相关知识提炼到目标模型中，实现了令人满意的性能，同时保持低计算成本。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An anchor voting scheme (AVS) is proposed based on the scores of knowledge anchor
    points to predict arbitrary value for the regression task.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出了一个基于知识锚点分数的锚点投票方案（AVS），用于预测回归任务的任意值。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Experiments are extensively conducted on two fundamental tasks: classification
    and regression in time series learning, where different networks are employed
    among 8 different scenarios or benchmarks. For the regression task, our KP significantly
    improves the accuracy by 19.7% on average. For the classification task, the performances
    are largely improved by up to 13.7%. With our KP, state-of-the-art performances
    are achieved on both two tasks'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在时间序列学习的两个基础任务：分类和回归上进行了广泛的实验，其中在8种不同的场景或基准中使用了不同的网络。对于回归任务，我们的KP显著提高了19.7%的准确性。对于分类任务，性能提高了最高13.7%。使用我们的KP，在这两个任务上都达到了最先进的性能。
- en: 2 Related Work
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Large language models (LLMs) recently witness significant progress and show
    impressive performances among a multitude of fields including natural language
    processing (NLP) Zhao et al. ([2023](#bib.bib13)) and computer vision (CV) Awais
    et al. ([2023](#bib.bib14)). To integrate the knowledge representations of LLMs
    into time series analytics, many approaches are proposed. PromptCast Xue and Salim
    ([2023](#bib.bib9)) firstly attempt to utilize LLMs for time series forecasting,
    where the time series data is converted into prompts. OFA Zhou et al. ([2023](#bib.bib11))
    proposes to fine-tune a pre-trained LLM for downstream tasks in time series analytics.
    Time-LLM Jin et al. ([2023c](#bib.bib15)) and LLM4TS Chang et al. ([2023](#bib.bib10))
    aim to repurpose a pre-trained LLM by aligning the time series domain to that
    of language for time series tasks. TEST Sun et al. ([2023](#bib.bib16)) combines
    the text prompts with time series encoding for better aligning time series data
    to the language. To fully utilize the generalization capability of LLMs, TEMPO
    Cao et al. ([2023](#bib.bib17)) augment the raw time series data by data decomposition
    and fine-tune a Pre-trained LLM on these augmented time series data. Although
    these LLM based approaches achieve significant performances on time series tasks,
    they are proposed based on a pre-trained and fixed LLM and require to load the
    whole LLM during training and inference. These drawbacks limit their flexibility
    and their applications on some scenarios with limited computation resources. To
    address this issue, we propose the knowledge pruning (KP), which is able to prune
    the redundant knowledge and effectively transfer the pertinent knowledge to a
    target model without the retaining process of LLMs, significantly reducing the
    computation cost and maintaining satisfactory performances.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）在自然语言处理（NLP）赵等人（[2023](#bib.bib13)）和计算机视觉（CV）阿瓦伊斯等人（[2023](#bib.bib14)）等众多领域取得了显著进展，展现了令人印象深刻的表现。为了将LLMs的知识表示整合到时间序列分析中，提出了许多方法。PromptCast
    薛和萨利姆（[2023](#bib.bib9)）首次尝试利用LLMs进行时间序列预测，其中时间序列数据被转换为提示。OFA 周等人（[2023](#bib.bib11)）提出对预训练的LLM进行微调以用于时间序列分析中的下游任务。Time-LLM
    金等人（[2023c](#bib.bib15)）和LLM4TS 常等人（[2023](#bib.bib10)）旨在通过将时间序列领域与语言领域对齐来重新利用预训练的LLM以用于时间序列任务。TEST
    孙等人（[2023](#bib.bib16)）结合文本提示和时间序列编码，以更好地将时间序列数据与语言对齐。为了充分利用LLMs的泛化能力，TEMPO 曹等人（[2023](#bib.bib17)）通过数据分解来增强原始时间序列数据，并在这些增强的时间序列数据上微调预训练的LLM。尽管这些基于LLM的方法在时间序列任务上取得了显著的表现，但它们是基于预训练和固定的LLM提出的，并且需要在训练和推理过程中加载整个LLM。这些缺点限制了它们的灵活性以及在计算资源有限的情况下的应用。为了解决这个问题，我们提出了知识剪枝（KP），它能够剪除冗余知识并有效地将相关知识转移到目标模型中，而无需保留LLMs，从而显著降低计算成本并保持令人满意的性能。
- en: 3 Main Work
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 主要工作
- en: Large language models (LLMs) have high computational demands and the knowledge
    store in them is much redundant for a specific task. To alleviate these drawbacks
    and facilitate the application of LLMs on computational constrained scenarios,
    we propose a new compression paradigm, Knowledge Pruning (KP) in this section.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）具有高计算需求，而且它们存储的知识对于特定任务来说是多余的。为了缓解这些缺点并促进LLMs在计算受限场景下的应用，我们在本节中提出了一种新的压缩范式——知识剪枝（KP）。
- en: '![Refer to caption](img/afe71cfbe3d14881fb6b46f5593a0051.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/afe71cfbe3d14881fb6b46f5593a0051.png)'
- en: 'Figure 2: The pipeline of our knowledge pruning (KP). Our KP coonsists of two
    stages: pre-processing and training stage. For a specific task, a knowledge prompt
    set is firstly produced, where these prompts are forwarded into a pre-trained
    LLM to obtain the corresponding language embeddings. Then, these embeddings are
    used as knowledge anchor points to estimate the pertinent knowledge and prune
    redundant knowledge of the LLM. After that, in the training stage, these knowledge
    anchor points are regarded as prior knowledge. The prior knowledges are transfered
    to the target mode via knowledge distillation.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：我们的知识剪枝（KP）流程。我们的KP由两个阶段组成：预处理阶段和训练阶段。对于特定任务，首先生成一个知识提示集，这些提示被传递到一个预训练的LLM中以获取相应的语言嵌入。然后，这些嵌入被用作知识锚点来估计相关知识并剪除LLM的冗余知识。之后，在训练阶段，这些知识锚点被视为先验知识。先验知识通过知识提炼转移到目标模型中。
- en: 'Knowledge is implicitly stored in neural networks. It is difficult to directly
    obtain the specified network in general. To address this problem, we alleviate
    the dialogue scheme of LLMs to generate a series of language embeddings based
    on prompts. The pipeline of our KP is shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3 Main
    Work ‣ LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing
    Devices"), where our KP is composed of two stages: pre-processing and training
    stage. Given a specific downstream task, a knowledge prompt set (KPS) is firstly
    generated. After that, the prompts in KPS are forwarded into a pre-trained LLM
    to produce corresponding embeddings, which serve as knowledge anchor points (KAPs)
    and are used to represent the pertinent knowledge. We regard this pertinent knowledge
    as prior knowledge for the target model. After that, at the training stage, the
    metric learning and knowledge distillation are leveraged to transfer this prior
    knowledge into the target model. Additionally, the output based on metric learning
    is generally discrete. To extend the application of our KP to the tasks with continuous
    output, an anchor voting scheme (AVS) is proposed, which enables our KP to produce
    arbitrary value, achieving significant improvements on both classification and
    regression tasks.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 知识隐式地存储在神经网络中。一般来说，难以直接获得指定的网络。为了解决这个问题，我们通过对话方案生成一系列基于提示的语言嵌入。我们的KP流程如图[2](#S3.F2
    "Figure 2 ‣ 3 Main Work ‣ LLM-based Knowledge Pruning for Time Series Data Analytics
    on Edge-computing Devices")所示，其中我们的KP由两个阶段组成：预处理阶段和训练阶段。给定一个特定的下游任务，首先生成一个知识提示集（KPS）。随后，将KPS中的提示传递到一个预训练的LLM中以生成相应的嵌入，这些嵌入作为知识锚点（KAPs）并用于表示相关知识。我们将这些相关知识视为目标模型的先验知识。之后，在训练阶段，利用度量学习和知识提炼将这些先验知识转移到目标模型中。此外，基于度量学习的输出通常是离散的。为了将我们的KP扩展到连续输出的任务中，提出了一种锚点投票方案（AVS），使我们的KP能够产生任意值，在分类和回归任务中取得显著改善。
- en: 3.1 Knowledge Pruning
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 知识剪枝
- en: 'Our knowledge pruning consists of three steps: knowledge prompt set generation,
    knowledge anchor point production and pertinent knowledge distillation.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的知识剪枝包括三个步骤：知识提示集生成、知识锚点生成和相关知识提炼。
- en: 'Knowledge Prompt Set Generation Knowledge prompt set (KPS) contains the prompts
    which are forwarded into a pre-trained LLM for getting the knoweldge anchor points
    (KAPs). In this paper, these prompts in KPS indicate the description of corresponding
    data. Since we devise to apply our proposed KP to two fundumental tasks: regression
    and classification, two different prompt templates are proposed. In regression,
    the remaining sueful prediction is used to evaluate the performance of our KP,
    and the prompt template is “The remaining useful life is {num}.”, where num indicates
    the correspoding groud truth value and ranges from [$y_{min}$] In classification,
    we apply our KP on the human activity recognition, and the prompt template is
    “The subject is {action}.”, where action means the name of the corresponding activity.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 知识提示集生成 知识提示集（KPS）包含将提示传递给预训练 LLM 以获取知识锚点（KAP）的提示。在本文中，这些 KPS 中的提示表示对应数据的描述。由于我们设计了将所提出的
    KP 应用于两个基础任务：回归和分类，因此提出了两种不同的提示模板。在回归中，使用剩余有用预测来评估我们的 KP 的性能，提示模板为“剩余使用寿命为 {num}。”，其中
    num 表示相应的真实值，并在 [$y_{min}$] 范围内。在分类中，我们将 KP 应用于人类活动识别，提示模板为“受试者是 {action}。”，其中
    action 表示相应活动的名称。
- en: Knowledge Anchor Point Production After obtaining the KPS, we forward these
    prompts in the KPS to a pre-trained LLM to obtain the language embeddings, which
    can be formulated as following.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 知识锚点生成 在获得 KPS 后，我们将这些提示传递给预训练的 LLM，以获得语言嵌入，可以表示如下。
- en: '|  | $z_{i}=F_{l}(\mathcal{P}_{i}),$ |  | (1) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $z_{i}=F_{l}(\mathcal{P}_{i}),$ |  | (1) |'
- en: where $\mathcal{P}_{i}$ is the produced language embedding termed as a knowledge
    anchor point (KAP). These KAPs are used to represent the space of pertinent knowledge.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{P}_{i}$ 是作为知识锚点（KAP）的生成语言嵌入。这些 KAP 用于表示相关知识的空间。
- en: Pertinent Knowledge Distillation Without transfering the entire knowledge of
    a LLM, our KP only transfer the pertinent knowledge which is indicated by KAPs.
    However, there is domain gap between the knowledge learned by LLM and the knowledge
    of downstream tasks. To alleviate this issue, an alignment module consisting of
    2 fully connected layers are used to project these KAPs into tha latent space
    of the downstream task. This process is computed as below,
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 相关知识蒸馏 在不转移整个 LLM 知识的情况下，我们的 KP 仅转移由 KAP 指示的相关知识。然而，LLM 学习的知识与下游任务的知识之间存在领域差距。为了解决这个问题，使用了一个由两个全连接层组成的对齐模块，将这些
    KAP 投影到下游任务的潜在空间中。这个过程计算如下：
- en: '|  | $k_{i}=\phi(z_{i}),$ |  | (2) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $k_{i}=\phi(z_{i}),$ |  | (2) |'
- en: where $\phi$, the metric learning is leveraged.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi$，即度量学习被应用。
- en: 'Moreover, to optimze the target model and the alignment simultaneously, based
    on the unidirectional metric learning in Prototypical Networks Snell et al. ([2017](#bib.bib18)),
    we develope a bi-directional metric learning. For optimizing the target model,
    the process is computed as:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了同时优化目标模型和对齐，基于 Prototypical Networks Snell 等人 ([2017](#bib.bib18)) 中的单向度量学习，我们开发了一种双向度量学习。为了优化目标模型，过程计算如下：
- en: '|  | $p_{t}(i)=\frac{\exp(-d(k_{i},x_{i}))}{\sum_{t=1}^{&#124;\mathcal{Z}&#124;}\exp(-d(k_{t},x_{i}))},$
    |  | (3) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{t}(i)=\frac{\exp(-d(k_{i},x_{i}))}{\sum_{t=1}^{&#124;\mathcal{Z}&#124;}\exp(-d(k_{t},x_{i}))},$
    |  | (3) |'
- en: 'where $d$ denotes the distance function. To improve the numerical stability
    and computational efficiency, we further improve this computation progress and
    compute the prediction as below:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d$ 表示距离函数。为了提高数值稳定性和计算效率，我们进一步改进了这一计算过程，并计算预测结果如下：
- en: '|  | $p_{t}(i)=\log(\frac{\exp({\rm simi}(k_{i},f_{i}))}{\sum_{t=1}^{&#124;\mathcal{Z}&#124;}\exp({\rm
    simi}(k_{t},f_{i}))}),$ |  | (4) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{t}(i)=\log(\frac{\exp({\rm simi}(k_{i},f_{i}))}{\sum_{t=1}^{&#124;\mathcal{Z}&#124;}\exp({\rm
    simi}(k_{t},f_{i}))}),$ |  | (4) |'
- en: 'where simi is the cosine similarity. For the alignment optimzation part, the
    process can be formulated as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 simi 是余弦相似度。对于对齐优化部分，过程可以表示为：
- en: '|  | $p_{l}(i)=\log(\frac{\exp({\rm simi}(k_{i},f_{i}))}{\sum_{t=1}^{&#124;\mathcal{B}&#124;}\exp({\rm
    simi}(k_{i},f_{t}))}),$ |  | (5) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{l}(i)=\log(\frac{\exp({\rm simi}(k_{i},f_{i}))}{\sum_{t=1}^{&#124;\mathcal{B}&#124;}\exp({\rm
    simi}(k_{i},f_{t}))}),$ |  | (5) |'
- en: 'where $|B|$ denotes the batch number. Finally, to distill the pertinent knowledge
    to the target model, the Kullback–Leibler divergence (KL-div) is used and the
    final loss is:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $|B|$ 表示批次数量。最后，为了将相关知识提炼到目标模型中，使用了 Kullback–Leibler 散度（KL-div），最终的损失为：
- en: '|  | $L=0.5*D_{KL}(p_{t},p_{g})+0.5*D_{KL}(p_{l},p_{g}^{T}),$ |  | (6) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $L=0.5*D_{KL}(p_{t},p_{g})+0.5*D_{KL}(p_{l},p_{g}^{T}),$ |  | (6) |'
- en: where $p_{g}$ is the ground truth distribution and is defined as following,
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{g}$ 是真实分布，定义如下，
- en: '|  | $p_{g}(i)=\frac{\exp(g_{i}*\tau)}{\sum_{t=1}^{&#124;\mathcal{B}&#124;}\exp(g_{t}*\tau))},$
    |  | (7) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{g}(i)=\frac{\exp(g_{i}*\tau)}{\sum_{t=1}^{\vert \mathcal{B} \vert}\exp(g_{t}*\tau))},$
    |  | (7) |'
- en: where $g_{i}$ is a temperature hyper-parameter.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $g_{i}$ 是一个温度超参数。
- en: 3.2 Anchor Voting Scheme
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 锚点投票方案
- en: Since our KP is based on metric learning, the prediction of the target model
    is discrete. To extend our KP to the task with continuous output like regression,
    an anchor voting scheme (AVS) is proposed.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的 KP 基于度量学习，目标模型的预测是离散的。为了将我们的 KP 扩展到具有连续输出的任务，如回归，提出了一种锚点投票方案（AVS）。
- en: 'Given the prediction distribution $\mathcal{S}=\{p_{t}(i)|i=1,\dots,|\mathcal{Z}|\}$,
    we firstly sort these scores in a descending order as below:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 给定预测分布 $\mathcal{S}=\{p_{t}(i)|i=1,\dots,|\mathcal{Z}|\}$，我们首先将这些分数按降序排序如下：
- en: '|  | $\hat{\mathcal{S}}={\rm sort}(\mathcal{S}).$ |  | (8) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathcal{S}}={\rm sort}(\mathcal{S}).$ |  | (8) |'
- en: After that, these scores are cumulated according to Eq. [9](#S3.E9 "In 3.2 Anchor
    Voting Scheme ‣ 3 Main Work ‣ LLM-based Knowledge Pruning for Time Series Data
    Analytics on Edge-computing Devices").
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，这些分数根据公式 [9](#S3.E9 "在 3.2 锚点投票方案 ‣ 3 主要工作 ‣ 基于 LLM 的时间序列数据分析的知识剪枝") 进行累积。
- en: '|  | $\mathcal{S}_{a}={\rm cumsum}(\hat{\mathcal{S}})$ |  | (9) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{S}_{a}={\rm cumsum}(\hat{\mathcal{S}})$ |  | (9) |'
- en: Then, the cumlated scores which are larger than $\theta$. The final prediciton
    is generated as following,
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，累积大于 $\theta$ 的分数。最终预测生成如下，
- en: '|  | $o=\frac{\sum_{i=1}^{&#124;\mathcal{V}&#124;}v_{i}*n_{i}}{\sum_{i=1}^{&#124;\mathcal{V}&#124;}v_{i}},$
    |  | (10) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $o=\frac{\sum_{i=1}^{\vert \mathcal{V} \vert}v_{i}*n_{i}}{\sum_{i=1}^{\vert
    \mathcal{V} \vert}v_{i}},$ |  | (10) |'
- en: where $n_{i}$. With our proposed AVS, our proposed KP is effectively entended
    to the regression task, achieving significant performances.Chen et al. ([2020](#bib.bib5))
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $n_{i}$。通过我们提出的 AVS，我们的 KP 有效地扩展到了回归任务中，取得了显著的性能提升。Chen 等 ([2020](#bib.bib5))
- en: 4 Experiments
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: To verify the effectiveness of our Knowledge Pruning (KP), extensive expriments
    are conducted in this section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为验证我们的知识剪枝（KP）的有效性，本节进行了广泛的实验。
- en: 'Table 1: Comparison with other methods in regression. Compared with other methods,
    our KP performs much better, achieving the best performances on nearly all subsets.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：与其他方法在回归中的比较。与其他方法相比，我们的 KP 表现更好，在几乎所有子集上都取得了最佳性能。
- en: '| Dataset | FD001 | FD002 | FD003 | FD004 | AVG |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | FD001 | FD002 | FD003 | FD004 | 平均 |'
- en: '| Evaluation | RMSE | Score | RMSE | Score | RMSE | Score | RMSE | Score |
    RMSE | Score |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 评估 | RMSE | 分数 | RMSE | 分数 | RMSE | 分数 | RMSE | 分数 | RMSE | 分数 |'
- en: '| Li et al. | 12.61 | 273.70 | 22.36 | 10412.00 | 12.64 | 284.10 | 23.31 |
    12466.00 | 17.73 | 5858.95 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Li 等 | 12.61 | 273.70 | 22.36 | 10412.00 | 12.64 | 284.10 | 23.31 | 12466.00
    | 17.73 | 5858.95 |'
- en: '| BLCNN | 13.18 | 302.27 | 19.09 | 1558.00 | 13.75 | 381.37 | 20.97 | 3859.00
    | 16.75 | 1525.16 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| BLCNN | 13.18 | 302.27 | 19.09 | 1558.00 | 13.75 | 381.37 | 20.97 | 3859.00
    | 16.75 | 1525.16 |'
- en: '| PE-Net | 13.98 | 280.87 | 14.69 | 881.73 | 12.33 | 272.85 | 15.40 | 1103.18
    | 14.10 | 634.66 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| PE-Net | 13.98 | 280.87 | 14.69 | 881.73 | 12.33 | 272.85 | 15.40 | 1103.18
    | 14.10 | 634.66 |'
- en: '| DGRU | 18.54 | 1467.00 | 20.06 | 4085.00 | 19.28 | 1488.00 | 20.88 | 3872.00
    | 19.69 | 2728.00 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| DGRU | 18.54 | 1467.00 | 20.06 | 4085.00 | 19.28 | 1488.00 | 20.88 | 3872.00
    | 19.69 | 2728.00 |'
- en: '| AdaNet | 13.12 | 248.45 | 15.20 | 890.71 | 12.41 | 231.06 | 15.02 | 883.21
    | 13.94 | 563.36 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| AdaNet | 13.12 | 248.45 | 15.20 | 890.71 | 12.41 | 231.06 | 15.02 | 883.21
    | 13.94 | 563.36 |'
- en: '| Jang et al. | 12.47 | 253.00 | 18.18 | 1618.00 | 11.88 | 270.00 | 22.11 |
    2797.00 | 16.16 | 1234.5 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Jang 等 | 12.47 | 253.00 | 18.18 | 1618.00 | 11.88 | 270.00 | 22.11 | 2797.00
    | 16.16 | 1234.5 |'
- en: '| KDnet | 13.68 | 362.08 | 14.47 | 929.20 | 12.95 | 327.27 | 15.96 | 1303.19
    | 14.27 | 730.44 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| KDnet | 13.68 | 362.08 | 14.47 | 929.20 | 12.95 | 327.27 | 15.96 | 1303.19
    | 14.27 | 730.44 |'
- en: '| Two-Stream BiLSTM | 12.07 | 208.11 | 14.97 | 847.98 | 11.84 | 211.80 | 14.94
    | 906.61 | 13.45 | 543.63 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 双流 BiLSTM | 12.07 | 208.11 | 14.97 | 847.98 | 11.84 | 211.80 | 14.94 | 906.61
    | 13.45 | 543.63 |'
- en: '| KP (ours) | 12.42 | 197.05 | 12.86 | 584.56 | 11.29 | 175.50 | 14.09 | 788.75
    | 12.66 | 436.47 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| KP（我们的） | 12.42 | 197.05 | 12.86 | 584.56 | 11.29 | 175.50 | 14.09 | 788.75
    | 12.66 | 436.47 |'
- en: 4.1 Datasets and Experimental Setup
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集和实验设置
- en: 'Datasets To comprehensively investigate the performances of our KP, two fundamental
    tasks on edge-computing devices: classification and regression, are evaluated
    in this paper. In classificaiton, the human activity recognition (HAR) task is
    studied and four different benchmarks: UCI_HAR Anguita et al. ([2013](#bib.bib19)),
    Opportunity Roggen et al. ([2010](#bib.bib20)), PAMAP2 Reiss and Stricker ([2012](#bib.bib21)),
    and WISDM Kwapisz et al. ([2011](#bib.bib22)) are used. These benchmarks contain
    different number of activity categories ranging from 6 to 17 with different scales
    between 3k and 29k samples. In regression, the remainning useful life (RUL) prediciton
    is alleviated for evaluation, where the C-MAPSS Saxena et al. ([2008](#bib.bib23))
    dataset is used. C-MAPSS contains four different subsets: FD001, FD002, FD003
    and FD004 with different scenarioes.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 为了全面调查我们KP的性能，本文评估了两个基本任务在边缘计算设备上的表现：分类和回归。在分类任务中，研究了人类活动识别（HAR）任务，并使用了四个不同的基准：UCI_HAR
    Anguita et al. ([2013](#bib.bib19))、Opportunity Roggen et al. ([2010](#bib.bib20))、PAMAP2
    Reiss 和 Stricker ([2012](#bib.bib21)) 和 WISDM Kwapisz et al. ([2011](#bib.bib22))。这些基准包含不同数量的活动类别，从6到17不等，样本量从3k到29k不等。在回归任务中，评估了剩余使用寿命（RUL）预测，其中使用了C-MAPSS
    Saxena et al. ([2008](#bib.bib23)) 数据集。C-MAPSS包含四个不同的子集：FD001、FD002、FD003 和 FD004，场景各异。
- en: Experimental Setup In classifiction, for consistency and meaning fulcomparison,
    the training and inference process on UCI_HAR, Opportunity and PAMAP2 are conducted
    according to the protocol of iSPLInception Ronald et al. ([2021](#bib.bib24)).
    Since the experiments in iSPLInception Ronald et al. ([2021](#bib.bib24)) do not
    include WISDM benchmark, the expriments on WISDM follow the setting in Multi CNN-BiLSTM
    Challa et al. ([2022](#bib.bib25)). For fair comparion, other compared methods
    are re-implemented under the same setting. According to approches Ronald et al.
    ([2021](#bib.bib24)); Challa et al. ([2022](#bib.bib25)), F1-Score is used for
    evaluation in HAR tasks. In regression, some methods are also re-implemented under
    the same conditions. The training and inference processes are conducted according
    to classic RUL methods Jin et al. ([2022a](#bib.bib26)); Chen et al. ([2020](#bib.bib5)).
    RMSE and scoring fuction are used as evaluation metrics. Two hyper-parameters
    $\tau$ are set as 10 and 0.9, respectively for all experiments. The pre-trained
    text encoder in CLIP Radford et al. ([2021](#bib.bib27)) is used as the pre-trained
    LLM in experiments. Experiments are conducted on a workstation with a GeForce
    RTX 4080 GPU and 128 GB memory from 1 to 4 hours.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置 在分类任务中，为了保持一致性和有意义的比较，对UCI_HAR、Opportunity和PAMAP2的数据进行了iSPLInception Ronald
    et al. ([2021](#bib.bib24))协议的训练和推断。由于iSPLInception Ronald et al. ([2021](#bib.bib24))的实验未包括WISDM基准，因此WISDM上的实验遵循Multi
    CNN-BiLSTM Challa et al. ([2022](#bib.bib25))中的设置。为了公平比较，其他比较方法在相同设置下重新实现。根据Ronald
    et al. ([2021](#bib.bib24)); Challa et al. ([2022](#bib.bib25))的方法，HAR任务中使用F1-Score进行评估。在回归任务中，一些方法也在相同条件下重新实现。训练和推断过程遵循经典的RUL方法Jin
    et al. ([2022a](#bib.bib26)); Chen et al. ([2020](#bib.bib5))。RMSE和评分函数被用作评估指标。所有实验中两个超参数$\tau$分别设置为10和0.9。实验中使用CLIP
    Radford et al. ([2021](#bib.bib27))中的预训练文本编码器作为预训练LLM。实验在配备GeForce RTX 4080 GPU和128
    GB内存的工作站上进行，持续时间为1到4小时。
- en: 4.2 Comparison with other methods
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 与其他方法的比较
- en: To evaluate the performances of our KP, we compare our approach with orther
    state-of-the-art (SOTA) methods. The exprimental results in regression and classificationare
    listed in Table. [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ LLM-based Knowledge Pruning
    for Time Series Data Analytics on Edge-computing Devices") and Table. [2](#S4.T2
    "Table 2 ‣ 4.2 Comparison with other methods ‣ 4 Experiments ‣ LLM-based Knowledge
    Pruning for Time Series Data Analytics on Edge-computing Devices"), respectively.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们KP的性能，我们将我们的方法与其他最先进（SOTA）的方法进行了比较。回归和分类的实验结果列在表 [1](#S4.T1 "Table 1 ‣
    4 Experiments ‣ LLM-based Knowledge Pruning for Time Series Data Analytics on
    Edge-computing Devices") 和表 [2](#S4.T2 "Table 2 ‣ 4.2 Comparison with other methods
    ‣ 4 Experiments ‣ LLM-based Knowledge Pruning for Time Series Data Analytics on
    Edge-computing Devices") 中。
- en: 'In the RUL task, serveral SOTA approaches: Li et al. Li et al. ([2018](#bib.bib28)),
    BLCNN Liu et al. ([2019](#bib.bib29)), PE-Net Jin et al. ([2022b](#bib.bib30)),
    DGRU Behera and Misra ([2021](#bib.bib31)), AdaNet Jin et al. ([2023d](#bib.bib32)),
    Jang et al. Jang and Kim ([2021](#bib.bib33)) and KDnet Xu et al. ([2021](#bib.bib34))
    , are compared with our method. Among these methods, Li et al. proposes a CNN
    based network to predict the RUL. PE-Net integrates position encoding scheme with
    an optimzed CNN architecture for the RUL task. AdaNet introduces the deformable
    convolution into the RUL task. BLCNN devises a hybrid network which combines RNN
    and CNN together to improve the prediction accuracy. DGRU apply the adversarial
    learning on the RUL task. A self-supervised learning approch is proposed in Jang
    et al. KDnet utilize knowledge distillation to transfer the knowledge in RNN to
    a CNN model. Benefitted from the learned pertinent knowledge from a pre-trained
    LLM, our KP performs much better than them and achieve the best performances.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RUL 任务中，与我们的方法相比，几个 SOTA 方法进行了比较：Li et al. Li et al. ([2018](#bib.bib28))，BLCNN
    Liu et al. ([2019](#bib.bib29))，PE-Net Jin et al. ([2022b](#bib.bib30))，DGRU Behera
    和 Misra ([2021](#bib.bib31))，AdaNet Jin et al. ([2023d](#bib.bib32))，Jang et al.
    Jang 和 Kim ([2021](#bib.bib33)) 和 KDnet Xu et al. ([2021](#bib.bib34))。这些方法中，Li
    et al. 提出了基于 CNN 的网络来预测 RUL。PE-Net 将位置编码方案与优化的 CNN 架构集成用于 RUL 任务。AdaNet 将可变形卷积引入
    RUL 任务。BLCNN 设计了一个将 RNN 和 CNN 结合在一起的混合网络，以提高预测准确性。DGRU 在 RUL 任务中应用对抗学习。Jang et
    al. 提出了自监督学习方法。KDnet 利用知识蒸馏将 RNN 中的知识迁移到 CNN 模型中。得益于从预训练 LLM 学到的相关知识，我们的 KP 表现远超这些方法，取得了最佳表现。
- en: 'Table 2: Comparison with other methods in classification. Compared with other
    methods, our proposed KP achieves the best performances in F1-Score among four
    different HAR benchmarks.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：与其他分类方法的比较。与其他方法相比，我们提出的 KP 在四种不同的 HAR 基准中在 F1-Score 上表现最佳。
- en: '| Methods | UCI_HAR | Oppotunity | PAMAP2 | WISDM |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | UCI_HAR | Oppotunity | PAMAP2 | WISDM |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| LSTM-CNN | 93.14 | 78.19 | 72.22 | 96.02 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LSTM-CNN | 93.14 | 78.19 | 72.22 | 96.02 |'
- en: '| CNN | 93.21 | 79.73 | 62.29 | 95.51 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| CNN | 93.21 | 79.73 | 62.29 | 95.51 |'
- en: '| Multi CNN-GRU | 94.05 | 83.92 | 68.13 | 96.15 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 多重 CNN-GRU | 94.05 | 83.92 | 68.13 | 96.15 |'
- en: '| Multi CNN-BiLSTM | 93.60 | 84.53 | 70.23 | 95.8 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 多重 CNN-BiLSTM | 93.60 | 84.53 | 70.23 | 95.8 |'
- en: '| GRU_INC | 93.67 | 72.58 | 75.88 | 83.93 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| GRU_INC | 93.67 | 72.58 | 75.88 | 83.93 |'
- en: '| DTL | 93.11 | 74.03 | 82.16 | 96.42 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| DTL | 93.11 | 74.03 | 82.16 | 96.42 |'
- en: '| iSPLInception | 93.08 | 84.45 | 80.01 | 96.14 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| iSPLInception | 93.08 | 84.45 | 80.01 | 96.14 |'
- en: '| KP (ours) | 96.63 | 86.74 | 85.28 | 98.25 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| KP（我们的方法） | 96.63 | 86.74 | 85.28 | 98.25 |'
- en: 'In the HAR task, we compare our KP with seven SOTA methods: LSTM-CNN Xia et al.
    ([2020](#bib.bib35)), CNN Van Kuppevelt et al. ([2020](#bib.bib36)), Multi CNN-GRU
    Dua et al. ([2021](#bib.bib37)), Multi CNN-BiLSTM Challa et al. ([2022](#bib.bib25)),
    GRU_INC Mim et al. ([2023](#bib.bib38)), DTL Ige and Noor ([2023](#bib.bib39))
    and iSPLInception Ronald et al. ([2021](#bib.bib24)). These compared approaches
    employ different network architectures like RNN, CNN and even hybrid networks.
    As a novel comdel compression paradigm, our proposed KP is fundamentally orthogonal
    to existing HAR methods and can be applied to any existing HAR approaches. We
    apply our KP to two different methods: DTL and iSPLInception, and list the best
    performances we achieved in Table [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ LLM-based
    Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices").
    Through experiments, it demonstrates that our KP effectively transfer the pertinent
    knowledge of a pre-trained LLM to the target model, which achieves the best performances
    among other SOTA methods.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HAR 任务中，我们将我们的 KP 与七种 SOTA 方法进行比较：LSTM-CNN Xia et al. ([2020](#bib.bib35))，CNN
    Van Kuppevelt et al. ([2020](#bib.bib36))，多重 CNN-GRU Dua et al. ([2021](#bib.bib37))，多重
    CNN-BiLSTM Challa et al. ([2022](#bib.bib25))，GRU_INC Mim et al. ([2023](#bib.bib38))，DTL
    Ige 和 Noor ([2023](#bib.bib39)) 以及 iSPLInception Ronald et al. ([2021](#bib.bib24))。这些比较方法使用了不同的网络架构，如
    RNN、CNN 甚至混合网络。作为一种新颖的压缩范式，我们提出的 KP 从根本上与现有的 HAR 方法正交，并且可以应用于任何现有的 HAR 方法。我们将我们的
    KP 应用于两种不同的方法：DTL 和 iSPLInception，并在表 [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ LLM-based
    Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices") 中列出了我们取得的最佳性能。实验表明，我们的
    KP 能有效地将预训练 LLM 的相关知识迁移到目标模型中，达到了在其他 SOTA 方法中的最佳表现。
- en: 'Table 3: Ablation study in regression. Baseline1 indicates the Bi-LSTM, baseline2
    is the PE-Net, and baseline3 represents the Two-Stream BiLSTM. With our proposed
    KP, the performances on three different baselines are improved by a large marge.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：回归任务中的消融研究。基线1指的是Bi-LSTM，基线2是PE-Net，基线3代表Two-Stream BiLSTM。通过我们提出的KP，三个不同基线上的表现都有大幅提升。
- en: '| Dataset | FD001 | FD002 | FD003 | FD004 | AVG |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | FD001 | FD002 | FD003 | FD004 | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Evaluation | RMSE | Score | RMSE | Score | RMSE | Score | RMSE | Score |
    RMSE | Score |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 评估 | RMSE | 得分 | RMSE | 得分 | RMSE | 得分 | RMSE | 得分 | RMSE | 得分 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Baseline1 | 13.09 | 260.67 | 15.85 | 871.62 | 13.23 | 266.12 | 15.81 | 1065.47
    | 14.50 | 615.97 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 基线1 | 13.09 | 260.67 | 15.85 | 871.62 | 13.23 | 266.12 | 15.81 | 1065.47
    | 14.50 | 615.97 |'
- en: '| Baseline1+KP (ours) | 12.82 | 201.85 | 14.09 | 716.48 | 11.59 | 190.94 |
    15.17 | 953.94 | 13.42 | 515.80 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 基线1+KP（我们的） | 12.82 | 201.85 | 14.09 | 716.48 | 11.59 | 190.94 | 15.17 |
    953.94 | 13.42 | 515.80 |'
- en: '| Baseline2 | 13.98 | 280.87 | 14.69 | 881.73 | 12.33 | 272.85 | 15.40 | 1103.18
    | 14.10 | 634.66 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 基线2 | 13.98 | 280.87 | 14.69 | 881.73 | 12.33 | 272.85 | 15.40 | 1103.18
    | 14.10 | 634.66 |'
- en: '| Baseline2 + KP(ours) | 13.63 | 251.64 | 14.11 | 721.38 | 12.44 | 197.96 |
    15.52 | 938.36 | 13.92 | 527.34 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 基线2 + KP（我们的） | 13.63 | 251.64 | 14.11 | 721.38 | 12.44 | 197.96 | 15.52
    | 938.36 | 13.92 | 527.34 |'
- en: '| Baseline3 | 12.07 | 208.11 | 14.97 | 847.98 | 11.84 | 211.80 | 14.94 | 906.61
    | 13.45 | 543.63 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 基线3 | 12.07 | 208.11 | 14.97 | 847.98 | 11.84 | 211.80 | 14.94 | 906.61 |
    13.45 | 543.63 |'
- en: '| Baseline3 + KP (ours) | 12.42 | 197.05 | 12.86 | 584.56 | 11.29 | 175.50
    | 14.09 | 788.75 | 12.66 | 436.47 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 基线3 + KP（我们的） | 12.42 | 197.05 | 12.86 | 584.56 | 11.29 | 175.50 | 14.09
    | 788.75 | 12.66 | 436.47 |'
- en: 'Table 4: Ablation study for AVS. Baseline1 indicates the Bi-LSTM. Through experiments,
    it shows that without our proposed AVS, the performances of KP on the regression
    task are limited. After applying our AVS, the performances on the regression task
    are significantly improved.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：AVS的消融研究。基线1指的是Bi-LSTM。通过实验，表明没有我们提出的AVS时，KP在回归任务上的表现受限。应用我们的AVS后，回归任务的表现显著提升。
- en: '| Dataset | FD001 | FD002 | FD003 | FD004 | AVG |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | FD001 | FD002 | FD003 | FD004 | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Evaluation | RMSE | Score | RMSE | Score | RMSE | Score | RMSE | Score |
    RMSE | Score |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 评估 | RMSE | 得分 | RMSE | 得分 | RMSE | 得分 | RMSE | 得分 | RMSE | 得分 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Baseline | 13.09 | 260.67 | 15.85 | 871.62 | 13.23 | 266.12 | 15.81 | 1065.47
    | 14.50 | 615.97 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 13.09 | 260.67 | 15.85 | 871.62 | 13.23 | 266.12 | 15.81 | 1065.47 |
    14.50 | 615.97 |'
- en: '| Baseline+KP w/o AVS (ours) | 16.07 | 339.03 | 14.27 | 963.37 | 13.35 | 219.47
    | 17.29 | 1340.89 | 15.25 | 715.69 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 基线+KP 无AVS（我们的） | 16.07 | 339.03 | 14.27 | 963.37 | 13.35 | 219.47 | 17.29
    | 1340.89 | 15.25 | 715.69 |'
- en: '| Baseline+KP (ours) | 12.82 | 201.85 | 14.09 | 716.48 | 11.59 | 190.94 | 15.17
    | 953.94 | 13.42 | 515.80 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 基线+KP（我们的） | 12.82 | 201.85 | 14.09 | 716.48 | 11.59 | 190.94 | 15.17 | 953.94
    | 13.42 | 515.80 |'
- en: 4.3 Ablation Study
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: To verify the effectiveness, experiments on ablation study are presented. Our
    KP is orthogonal to approaches for time series analytics and can be directly applied
    to these methods. To show the generalization of our KP, we apply our KP on serveral
    different networks and show the performance improvements. Experimental results
    on the regression task, RUL, and classification task, HAR are listed in Table.
    [3](#S4.T3 "Table 3 ‣ 4.2 Comparison with other methods ‣ 4 Experiments ‣ LLM-based
    Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices") and
    Table. [5](#S4.T5 "Table 5 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ LLM-based Knowledge
    Pruning for Time Series Data Analytics on Edge-computing Devices"), respectively.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为验证效果，展示了消融研究实验。我们的KP与时间序列分析方法正交，可以直接应用于这些方法。为了展示我们KP的普适性，我们将其应用于多个不同的网络，并展示性能提升。回归任务RUL和分类任务HAR的实验结果分别列在表[3](#S4.T3
    "Table 3 ‣ 4.2 Comparison with other methods ‣ 4 Experiments ‣ LLM-based Knowledge
    Pruning for Time Series Data Analytics on Edge-computing Devices")和表[5](#S4.T5
    "Table 5 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ LLM-based Knowledge Pruning for
    Time Series Data Analytics on Edge-computing Devices")中。
- en: 'In the RUL task, three different approaches: Bi-LSTM, Two-Stream BiLSTM Jin
    et al. ([2022a](#bib.bib26)) and PE-Net Jin et al. ([2022b](#bib.bib30)) are used
    as our baselines. Bi-LSTM is a shallow network, which consists of two bi-directional
    LSTM. Two-Stream BiLSTM integrates the handcrafted feature flow Jin et al. ([2022a](#bib.bib26))
    into the raw time series data via a Bi-LSTM based network. PE-Net designs a CNN
    with a position encoding scheme to predict the RUL. In Table. [5](#S4.T5 "Table
    5 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ LLM-based Knowledge Pruning for Time
    Series Data Analytics on Edge-computing Devices"), Bi-LSTM is used as baseline1,
    PE-Net is used as baseline2 and Two-Stream Bi-LSTM is used as baseline3\. With
    our proosed KP, all these three methods are remarkablely improved among four different
    scenarioes. Compared with RMSE, Score is generally regarded as a more important
    evaluation metric, since it give more penalty on the late prediciton, which is
    similar to the practical setting. Among these three baselines, baseline3 acheves
    the best performances on average. After applying our KP, its performances are
    further improved by 19.7% in Score.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RUL 任务中，使用了三种不同的方法：Bi-LSTM、Two-Stream BiLSTM Jin 等 ([2022a](#bib.bib26)) 和
    PE-Net Jin 等 ([2022b](#bib.bib30)) 作为我们的基准。Bi-LSTM 是一个浅层网络，由两个双向 LSTM 组成。Two-Stream
    BiLSTM 将手工特征流 Jin 等 ([2022a](#bib.bib26)) 集成到原始时间序列数据中，通过基于 Bi-LSTM 的网络。PE-Net
    设计了一个具有位置编码方案的 CNN 来预测 RUL。在表[5](#S4.T5 "Table 5 ‣ 4.3 Ablation Study ‣ 4 Experiments
    ‣ LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing
    Devices")中，Bi-LSTM 被用作 baseline1，PE-Net 被用作 baseline2，而 Two-Stream Bi-LSTM 被用作
    baseline3。通过我们提出的 KP，这三种方法在四种不同的场景中都有显著提高。与 RMSE 相比，Score 通常被认为是更重要的评价指标，因为它对晚期预测给予更多惩罚，这类似于实际设置。在这三种基准中，baseline3
    在平均性能上表现最佳。应用我们的 KP 后，其性能在 Score 上进一步提高了 19.7%。
- en: 'In the HAR task, two different methods: DTL Ige and Noor ([2023](#bib.bib39))
    and iSPLInception Ronald et al. ([2021](#bib.bib24)), are used as our baselines.
    DTL is a hybrid network, which combines CNN and RNN togethor to capture the temporal
    features. In comparison, iSPLInception proposes to utilize the inception based
    CNN network to classify the human activaities.In Table. [3](#S4.T3 "Table 3 ‣
    4.2 Comparison with other methods ‣ 4 Experiments ‣ LLM-based Knowledge Pruning
    for Time Series Data Analytics on Edge-computing Devices"), baseline1 indicates
    the DTL, and basleine2 represents the iSPLInception. According to the experimental
    results, our KP is able to effectively improve the performances on these two baselines
    among all four benchmarks. The improvements on these methods range from 0.8% to
    13.7%.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HAR 任务中，使用了两种不同的方法：DTL Ige 和 Noor ([2023](#bib.bib39)) 和 iSPLInception Ronald
    等 ([2021](#bib.bib24))，作为我们的基准。DTL 是一个混合网络，它将 CNN 和 RNN 结合在一起，以捕捉时间特征。相比之下，iSPLInception
    提出了利用基于 inception 的 CNN 网络来分类人类活动。在表[3](#S4.T3 "Table 3 ‣ 4.2 Comparison with
    other methods ‣ 4 Experiments ‣ LLM-based Knowledge Pruning for Time Series Data
    Analytics on Edge-computing Devices")中，baseline1 表示 DTL，baseline2 表示 iSPLInception。根据实验结果，我们的
    KP 能够在所有四个基准中有效提高这两种基准的方法的性能。这些方法的改进范围从 0.8% 到 13.7%。
- en: 'Table 5: Ablation study in classification. Baseline1 indicates the DTL method,
    and baseline2 represents the iSPLInception method. Our proposed KP is able to
    effectively improve the performances on two different network architectures among
    four different HAR benchmarks.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：分类中的消融研究。Baseline1 表示 DTL 方法，baseline2 表示 iSPLInception 方法。我们提出的 KP 能够在四个不同的
    HAR 基准测试中，在两种不同的网络架构上有效提高性能。
- en: '| Methods | UCI_HAR | Oppotunity | PAMAP2 | WISDM |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | UCI_HAR | Oppotunity | PAMAP2 | WISDM |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Baseline1 | 93.11 | 74.03 | 82.16 | 96.42 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Baseline1 | 93.11 | 74.03 | 82.16 | 96.42 |'
- en: '| Baseline1+KP (ours) | 96.63 | 84.14 | 85.28 | 97.18 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Baseline1+KP（我们的方法） | 96.63 | 84.14 | 85.28 | 97.18 |'
- en: '| Baseline2) | 93.08 | 84.45 | 80.01 | 96.14 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Baseline2 | 93.08 | 84.45 | 80.01 | 96.14 |'
- en: '| Baseline2 + KP (ours) | 94.75 | 86.47 | 83.99 | 98.25 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Baseline2 + KP（我们的方法） | 94.75 | 86.47 | 83.99 | 98.25 |'
- en: These expriments above show that our propsoed KP effectively identify the pertinent
    knowledge and transfer it to the target model. With our KP, all these five baselines
    are improved by a large marge.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 上述实验表明，我们提出的 KP 能够有效识别相关知识并将其转移到目标模型中。通过我们的 KP，所有这五个基准都得到了大幅提升。
- en: Effectiveness of AVS AVS is proposed to enable the metric learning based network
    to predict continuous value for the regression task, like RUL. Experiments are
    designed to show the effectiveness of our proposed AVS, which are listed in Table
    [4](#S4.T4 "Table 4 ‣ 4.2 Comparison with other methods ‣ 4 Experiments ‣ LLM-based
    Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices").
    The Bi-LSTM is used as the baseline. The experimental reults indicates that without
    our propsoed AVS, the performances of KP on the regrression task are not satisfactory.
    After applying our proposed AVS, our KP can effectively improve the accuracy on
    the RUL prediction by a large marge.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: AVS 的有效性。AVS 被提出用于使基于度量学习的网络能够预测回归任务的连续值，如 RUL。实验旨在展示我们提出的 AVS 的有效性，结果列在表 [4](#S4.T4
    "表 4 ‣ 4.2 与其他方法的比较 ‣ 4 实验 ‣ 基于 LLM 的时间序列数据分析的知识剪枝在边缘计算设备上") 中。Bi-LSTM 被用作基准。实验结果表明，在没有我们提出的
    AVS 的情况下，KP 在回归任务中的表现不尽如人意。应用我们提出的 AVS 后，KP 可以大幅度提高 RUL 预测的准确性。
- en: Based on the experiments above, it can be found that our proposed KP can consistently
    improve the performances across different tasks and benchmarks. Since the improvemment
    by KP ranges from 0.8% to 13.7%, the effectiveness of our KP may be affected by
    the specific data distribution and the neural network architecture.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述实验，可以发现我们提出的 KP 可以在不同任务和基准测试中一致地提高性能。由于 KP 的改进范围从 0.8% 到 13.7%，KP 的有效性可能会受到特定数据分布和神经网络架构的影响。
- en: 4.4 Computation Efficiency
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 计算效率
- en: Our KP is proposed to alleviate the issue of the computation cost in LLMs. The
    experiments on computation efficiency is carries out and listed in Table. [6](#S4.T6
    "Table 6 ‣ 4.4 Computation Efficiency ‣ 4 Experiments ‣ LLM-based Knowledge Pruning
    for Time Series Data Analytics on Edge-computing Devices"), where the FLOPs and
    Params are listed to compare the computation efficiency.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的 KP 旨在缓解 LLM 中的计算成本问题。计算效率的实验已完成，并列在表中 [6](#S4.T6 "表 6 ‣ 4.4 计算效率 ‣ 4 实验
    ‣ 基于 LLM 的时间序列数据分析的知识剪枝在边缘计算设备上")，其中列出了 FLOPs 和参数，以比较计算效率。
- en: 'Table 6: Experiments on computation efficiency.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 计算效率实验。'
- en: '| Methods | FLOPs (G) | Params (M) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | FLOPs (G) | 参数 (M) |'
- en: '| --- | --- | --- |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| LLM in CLIP | 5.96 | 63.43 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| LLM 在 CLIP | 5.96 | 63.43 |'
- en: '| Two-Stream BiLSTM + KP (ours) | 0.002 | 0.042 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 双流 BiLSTM + KP（我们的方法） | 0.002 | 0.042 |'
- en: '| DTL+ KP (ours) | 0.024 | 1.12 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| DTL+ KP（我们的方法） | 0.024 | 1.12 |'
- en: 'As listed in Table. [6](#S4.T6 "Table 6 ‣ 4.4 Computation Efficiency ‣ 4 Experiments
    ‣ LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing
    Devices"), we apply our KP to two networks: Two-Stream BiLSTM and DTL for the
    task RUL and HAR, respectively. Since DTL applies a hybrid network, which is composed
    of CNN and RNN and is more complex than the Two-Stream BiLSTM, the computation
    complexity of DTL is higher than that of Two-Stream BiLSTM. Nevertheless, the
    computation demands on these two approaches are much lower than that of the LLM
    in CLIP. According to the experimental results, our proposed KP is able to effectively
    prune the redundant knowedlge of LLM. The computation issue of LLMs is well alleviated,
    and the performances of the target model are improved.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [6](#S4.T6 "表 6 ‣ 4.4 计算效率 ‣ 4 实验 ‣ 基于 LLM 的时间序列数据分析的知识剪枝在边缘计算设备上") 所列，我们将
    KP 应用于两个网络：双流 BiLSTM 和 DTL，分别用于 RUL 和 HAR 任务。由于 DTL 采用了一个由 CNN 和 RNN 组成的混合网络，比双流
    BiLSTM 更复杂，因此 DTL 的计算复杂度高于双流 BiLSTM。然而，这两种方法的计算需求远低于 CLIP 中的 LLM。根据实验结果，我们提出的
    KP 能够有效地剪枝 LLM 的冗余知识。LLM 的计算问题得到了有效缓解，目标模型的性能得到了改善。
- en: '![Refer to caption](img/baa645e766dbf326fdc68345454dd3c0.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/baa645e766dbf326fdc68345454dd3c0.png)'
- en: 'Figure 3: Expriements on RUL task with different $\tau$ values on FD004.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 在 FD004 上使用不同的 $\tau$ 值进行的 RUL 任务实验。'
- en: 4.5 Sensitivity Analysis
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 敏感性分析
- en: 'Our proposed KP involves two hyper-parameters: $\tau$, we graduately increase
    its values and carry out experiments on HAR and RUL tasks, respectively. These
    experimental results on RUL and HAR are illurstrated in Fig. [3](#S4.F3 "Figure
    3 ‣ 4.4 Computation Efficiency ‣ 4 Experiments ‣ LLM-based Knowledge Pruning for
    Time Series Data Analytics on Edge-computing Devices") and Fig. [4](#S4.F4 "Figure
    4 ‣ 4.5 Sensitivity Analysis ‣ 4 Experiments ‣ LLM-based Knowledge Pruning for
    Time Series Data Analytics on Edge-computing Devices"), respectively.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的KP涉及两个超参数：$\tau$，我们逐步增加其值，并分别在HAR和RUL任务上进行实验。这些在RUL和HAR上的实验结果分别在图[3](#S4.F3
    "Figure 3 ‣ 4.4 Computation Efficiency ‣ 4 Experiments ‣ LLM-based Knowledge Pruning
    for Time Series Data Analytics on Edge-computing Devices")和图[4](#S4.F4 "Figure
    4 ‣ 4.5 Sensitivity Analysis ‣ 4 Experiments ‣ LLM-based Knowledge Pruning for
    Time Series Data Analytics on Edge-computing Devices")中展示。
- en: In Fig. [3](#S4.F3 "Figure 3 ‣ 4.4 Computation Efficiency ‣ 4 Experiments ‣
    LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices"),
    our KP is applied on Two-Stream BiLSTM with different $\tau$.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[3](#S4.F3 "Figure 3 ‣ 4.4 Computation Efficiency ‣ 4 Experiments ‣ LLM-based
    Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices")中，我们将KP应用于具有不同$\tau$的双流BiLSTM。
- en: '![Refer to caption](img/23c09998ea4079ce84af1b8deb788755.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/23c09998ea4079ce84af1b8deb788755.png)'
- en: 'Figure 4: Expriements on HAR task with different $\tau$ values.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：在不同$\tau$值下对HAR任务的实验。
- en: 'In Fig. [4](#S4.F4 "Figure 4 ‣ 4.5 Sensitivity Analysis ‣ 4 Experiments ‣ LLM-based
    Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices"),
    we apply our KP on the DTL in two different datasets: UCI_HAR and WISDM benchmarks.
    It shows that our KP can improves the performances of DTL under different $\tau$
    values.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[4](#S4.F4 "Figure 4 ‣ 4.5 Sensitivity Analysis ‣ 4 Experiments ‣ LLM-based
    Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices")中，我们将KP应用于两个不同的数据集：UCI_HAR和WISDM基准。结果表明，我们的KP可以在不同$\tau$值下提高DTL的性能。
- en: AVS is proposed to regression tasks, which enables our network to predict continuous
    value for the RUL task. The hyper-parameter $\theta$, which are presented in Table.
    [7](#S4.T7 "Table 7 ‣ 4.5 Sensitivity Analysis ‣ 4 Experiments ‣ LLM-based Knowledge
    Pruning for Time Series Data Analytics on Edge-computing Devices").
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: AVS被提出用于回归任务，使我们的网络能够预测RUL任务的连续值。超参数$\theta$如表[7](#S4.T7 "Table 7 ‣ 4.5 Sensitivity
    Analysis ‣ 4 Experiments ‣ LLM-based Knowledge Pruning for Time Series Data Analytics
    on Edge-computing Devices")所示。
- en: 'Table 7: Experiments for AVS with different $\theta$ value on FD004.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：在FD004上进行的不同$\theta$值的AVS实验。
- en: '| Metric | baseline | 0.9 | 0.8 | 0.7 | 0.6 | 0.5 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Metric | baseline | 0.9 | 0.8 | 0.7 | 0.6 | 0.5 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| RMSE | 14.94 | 14.09 | 13.92 | 13.91 | 14.11 | 14.30 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| RMSE | 14.94 | 14.09 | 13.92 | 13.91 | 14.11 | 14.30 |'
- en: '| Score | 906.61 | 788.75 | 810.19 | 824.61 | 853.73 | 891.58 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Score | 906.61 | 788.75 | 810.19 | 824.61 | 853.73 | 891.58 |'
- en: As listed in Table [7](#S4.T7 "Table 7 ‣ 4.5 Sensitivity Analysis ‣ 4 Experiments
    ‣ LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing
    Devices"), our AVS with different $\theta$.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[7](#S4.T7 "Table 7 ‣ 4.5 Sensitivity Analysis ‣ 4 Experiments ‣ LLM-based
    Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices")所列，我们的AVS与不同的$\theta$。
- en: 5 Conclusions
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: 'In this paper, we have proposed a new model compression paradigm, Knowledge
    Pruning (KP). Our KP consists of three steps: knowledge prompt set generation,
    knowledge anchor point production and pertinent knowledge distillation. Furethurmore,
    since our KP is based on metric learning, the performances on the regresison tasks
    may be limited. To extend our KP to the regression task, a anchor voting scheme
    has been proposed. Through experiments, our KP has effectively pruned the redundant
    knoweldge of LLMs for a specific downstream task and accurately transfer the pertinent
    knowledge to the target model. With our KP, the computation cost introduced by
    LLMs is largely reduced, and satisfacotry performances are achieved. Our KP shown
    siginificant improvement on both classification task, HAR and regression task,
    RUL, achieving state-of-the-art performances.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新的模型压缩范式——知识剪枝（KP）。我们的KP包括三个步骤：知识提示集生成、知识锚点生成和相关知识蒸馏。此外，由于我们的KP基于度量学习，因此在回归任务上的表现可能有限。为将KP扩展到回归任务，提出了一种锚点投票方案。通过实验，我们的KP有效地剪枝了LLMs的冗余知识，并准确地将相关知识转移到目标模型。通过我们的KP，LLMs引入的计算成本大幅降低，并且取得了令人满意的性能。我们的KP在分类任务HAR和回归任务RUL上表现出显著的改进，达到了最先进的性能。
- en: References
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Zhao et al. [2019] Rui Zhao, Ruqiang Yan, Zhenghua Chen, Kezhi Mao, Peng Wang,
    and Robert X Gao. Deep learning and its applications to machine health monitoring.
    *Mechanical Systems and Signal Processing*, 115:213–237, 2019.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等人 [2019] 包括**瑞·赵**、**如强·阎**、**郑华·陈**、**克志·毛**、**彭·王** 和 **罗伯特·X·高**。深度学习及其在机器健康监测中的应用。*机械系统与信号处理*，115:213–237，2019年。
- en: Chen et al. [2018] Zhenghua Chen, Le Zhang, Chaoyang Jiang, Zhiguang Cao, and
    Wei Cui. Wifi csi based passive human activity recognition using attention based
    blstm. *IEEE Transactions on Mobile Computing*, 18(11):2714–2724, 2018.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2018] 包括**郑华·陈**、**乐·张**、**朝阳·江**、**志光·曹** 和 **伟·崔**。基于WiFi CSI的被动人体活动识别，使用基于注意力的BLSTM。*IEEE
    移动计算学报*，18(11):2714–2724，2018年。
- en: 'Jin et al. [2023a] Guangyin Jin, Yuxuan Liang, Yuchen Fang, Zezhi Shao, Jincai
    Huang, Junbo Zhang, and Yu Zheng. Spatio-temporal graph neural networks for predictive
    learning in urban computing: A survey. *IEEE Transactions on Knowledge and Data
    Engineering*, 2023a.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '金等人 [2023a] 包括**光银·金**、**玉轩·梁**、**宇晨·方**、**泽志·邵**、**锦才·黄**、**军博·张** 和 **宇·郑**。用于城市计算的时空图神经网络预测学习:
    一项综述。*IEEE 知识与数据工程学报*，2023a年。'
- en: 'Zhu et al. [2023] Zhaoyang Zhu, Weiqi Chen, Rui Xia, Tian Zhou, Peisong Niu,
    Bingqing Peng, Wenwei Wang, Hengbo Liu, Ziqing Ma, Qingsong Wen, et al. eforecaster:
    unifying electricity forecasting with robust, flexible, and explainable machine
    learning algorithms. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 37, pages 15630–15638, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '朱等人 [2023] 包括**赵阳·朱**、**卫祺·陈**、**瑞·夏**、**天·周**、**佩松·牛**、**冰清·彭**、**文伟·王**、**恒博·刘**、**子青·马**、**青松·温**
    等。eforecaster: 通过强健、灵活且可解释的机器学习算法统一电力预测。发表于 *AAAI 人工智能会议论文集*，第37卷，第15630–15638页，2023年。'
- en: Chen et al. [2020] Zhenghua Chen, Min Wu, Rui Zhao, Feri Guretno, Ruqiang Yan,
    and Xiaoli Li. Machine remaining useful life prediction via an attention-based
    deep learning approach. *IEEE Transactions on Industrial Electronics*, 68(3):2521–2531,
    2020.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2020] 包括**郑华·陈**、**敏·吴**、**瑞·赵**、**费里·古尔特诺**、**如强·阎** 和 **晓莉·李**。基于注意力的深度学习方法进行机器剩余使用寿命预测。*IEEE
    工业电子学报*，68(3):2521–2531，2020年。
- en: 'Jin et al. [2023b] Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao
    Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, et al. Large models
    for time series and spatio-temporal data: A survey and outlook. *arXiv preprint
    arXiv:2310.10196*, 2023b.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '金等人 [2023b] 包括**明·金**、**青松·温**、**玉轩·梁**、**朝利·张**、**思乔·薛**、**雪·王**、**詹姆斯·张**、**毅·王**、**海峰·陈**、**晓莉·李**
    等。时间序列和时空数据的大型模型: 综述与展望。*arXiv 预印本 arXiv:2310.10196*，2023b年。'
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '图夫朗等人 [2023] 包括**雨果·图夫朗**、**路易斯·马丁**、**凯文·斯通**、**彼得·阿尔伯特**、**阿姆贾德·阿尔迈赫里**、**雅斯敏·巴巴伊**、**尼古拉·巴什利科夫**、**苏姆亚·巴特拉**、**普拉吉瓦尔·巴尔戈瓦**、**舒尔蒂·博萨尔**
    等。Llama 2: 开放的基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023年。'
- en: Peng et al. [2023] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and
    Jianfeng Gao. Instruction tuning with gpt-4. *arXiv preprint arXiv:2304.03277*,
    2023.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彭等人 [2023] 包括**鲍林·彭**、**春元·李**、**彭城·何**、**米歇尔·加利** 和 **剑锋·高**。基于 GPT-4 的指令调优。*arXiv
    预印本 arXiv:2304.03277*，2023年。
- en: 'Xue and Salim [2023] Hao Xue and Flora D Salim. Promptcast: A new prompt-based
    learning paradigm for time series forecasting. *IEEE Transactions on Knowledge
    and Data Engineering*, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '薛和萨利姆 [2023] 包括**浩·薛** 和 **弗洛拉·D·萨利姆**。Promptcast: 一种新的基于提示的时间序列预测学习范式。*IEEE
    知识与数据工程学报*，2023年。'
- en: 'Chang et al. [2023] Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. Llm4ts: Two-stage
    fine-tuning for time-series forecasting with pre-trained llms. *arXiv preprint
    arXiv:2308.08469*, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '常等人 [2023] 包括**青常**、**文志·彭** 和 **天傅·陈**。LLM4TS: 预训练LLM进行时间序列预测的两阶段微调。*arXiv
    预印本 arXiv:2308.08469*，2023年。'
- en: 'Zhou et al. [2023] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin.
    One fits all: Power general time series analysis by pretrained lm. *arXiv preprint
    arXiv:2302.11939*, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '周等人 [2023] 包括**天·周**、**佩松·牛**、**雪·王**、**梁·孙** 和 **荣·金**。一网打尽: 通过预训练的语言模型进行通用时间序列分析。*arXiv
    预印本 arXiv:2302.11939*，2023年。'
- en: Gruver et al. [2023] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon
    Wilson. Large language models are zero-shot time series forecasters. *arXiv preprint
    arXiv:2310.07820*, 2023.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格鲁弗等人 [2023] 包括**内特·格鲁弗**、**马克·芬齐**、**时凯·丘** 和 **安德鲁·戈登·威尔逊**。大型语言模型是零样本时间序列预测器。*arXiv
    预印本 arXiv:2310.07820*，2023年。
- en: Zhao et al. [2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    A survey of large language models. *arXiv preprint arXiv:2303.18223*, 2023.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2023] Wayne Xin Zhao、Kun Zhou、Junyi Li、Tianyi Tang、Xiaolei Wang、Yupeng
    Hou、Yingqian Min、Beichen Zhang、Junjie Zhang、Zican Dong 等. 大型语言模型的调查。*arXiv 预印本
    arXiv:2303.18223*, 2023.
- en: 'Awais et al. [2023] Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad
    Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, and Fahad Shahbaz Khan.
    Foundational models defining a new era in vision: A survey and outlook. *arXiv
    preprint arXiv:2307.13721*, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Awais 等人 [2023] Muhammad Awais、Muzammal Naseer、Salman Khan、Rao Muhammad Anwer、Hisham
    Cholakkal、Mubarak Shah、Ming-Hsuan Yang 和 Fahad Shahbaz Khan. 定义视觉新时代的基础模型：调查与展望。*arXiv
    预印本 arXiv:2307.13721*, 2023.
- en: 'Jin et al. [2023c] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang,
    Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm:
    Time series forecasting by reprogramming large language models. *arXiv preprint
    arXiv:2310.01728*, 2023c.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin 等人 [2023c] Ming Jin、Shiyu Wang、Lintao Ma、Zhixuan Chu、James Y Zhang、Xiaoming
    Shi、Pin-Yu Chen、Yuxuan Liang、Yuan-Fang Li、Shirui Pan 等. Time-llm: 通过重新编程大型语言模型进行时间序列预测。*arXiv
    预印本 arXiv:2310.01728*, 2023c.'
- en: 'Sun et al. [2023] Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. Test:
    Text prototype aligned embedding to activate llm’s ability for time series. *arXiv
    preprint arXiv:2308.08241*, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等人 [2023] Chenxi Sun、Yaliang Li、Hongyan Li 和 Shenda Hong. Test: 文本原型对齐嵌入以激活大型语言模型的时间序列能力。*arXiv
    预印本 arXiv:2308.08241*, 2023.'
- en: 'Cao et al. [2023] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang
    Zheng, Wen Ye, and Yan Liu. Tempo: Prompt-based generative pre-trained transformer
    for time series forecasting. *arXiv preprint arXiv:2310.04948*, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cao 等人 [2023] Defu Cao、Furong Jia、Sercan O Arik、Tomas Pfister、Yixiang Zheng、Wen
    Ye 和 Yan Liu. Tempo: 基于提示的生成预训练变换器用于时间序列预测。*arXiv 预印本 arXiv:2310.04948*, 2023.'
- en: Snell et al. [2017] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical
    networks for few-shot learning. *Advances in neural information processing systems*,
    30, 2017.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snell 等人 [2017] Jake Snell、Kevin Swersky 和 Richard Zemel. 用于少样本学习的原型网络。*神经信息处理系统进展*,
    30, 2017.
- en: Anguita et al. [2013] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra,
    Jorge Luis Reyes-Ortiz, et al. A public domain dataset for human activity recognition
    using smartphones. In *Esann*, volume 3, page 3, 2013.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anguita 等人 [2013] Davide Anguita、Alessandro Ghio、Luca Oneto、Xavier Parra、Jorge
    Luis Reyes-Ortiz 等. 用于人类活动识别的公共数据集，使用智能手机。在 *Esann*, 第 3 卷，第 3 页，2013。
- en: Roggen et al. [2010] Daniel Roggen, Alberto Calatroni, Mirco Rossi, Thomas Holleczek,
    Kilian Förster, Gerhard Tröster, Paul Lukowicz, David Bannach, Gerald Pirkl, Alois
    Ferscha, et al. Collecting complex activity datasets in highly rich networked
    sensor environments. In *2010 Seventh international conference on networked sensing
    systems (INSS)*, pages 233–240\. IEEE, 2010.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roggen 等人 [2010] Daniel Roggen、Alberto Calatroni、Mirco Rossi、Thomas Holleczek、Kilian
    Förster、Gerhard Tröster、Paul Lukowicz、David Bannach、Gerald Pirkl、Alois Ferscha
    等. 在高度丰富的网络传感器环境中收集复杂的活动数据集。在 *2010 第七届国际网络传感系统会议 (INSS)* 中，第 233–240 页。IEEE,
    2010.
- en: Reiss and Stricker [2012] Attila Reiss and Didier Stricker. Introducing a new
    benchmarked dataset for activity monitoring. In *2012 16th international symposium
    on wearable computers*, pages 108–109\. IEEE, 2012.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reiss 和 Stricker [2012] Attila Reiss 和 Didier Stricker. 引入一个新的基准数据集用于活动监测。在
    *2012 第十六届可穿戴计算国际研讨会* 中，第 108–109 页。IEEE, 2012.
- en: Kwapisz et al. [2011] Jennifer R Kwapisz, Gary M Weiss, and Samuel A Moore.
    Activity recognition using cell phone accelerometers. *ACM SigKDD Explorations
    Newsletter*, 12(2):74–82, 2011.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwapisz 等人 [2011] Jennifer R Kwapisz、Gary M Weiss 和 Samuel A Moore. 使用手机加速度计进行活动识别。*ACM
    SigKDD Explorations Newsletter*, 12(2):74–82, 2011.
- en: Saxena et al. [2008] Abhinav Saxena, Kai Goebel, Don Simon, and Neil Eklund.
    Damage propagation modeling for aircraft engine run-to-failure simulation. In
    *2008 international conference on prognostics and health management*, pages 1–9\.
    IEEE, 2008.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saxena 等人 [2008] Abhinav Saxena、Kai Goebel、Don Simon 和 Neil Eklund. 用于飞机发动机故障模拟的损伤传播建模。在
    *2008 国际预测与健康管理会议* 中，第 1–9 页。IEEE, 2008.
- en: 'Ronald et al. [2021] Mutegeki Ronald, Alwin Poulose, and Dong Seog Han. isplinception:
    An inception-resnet deep learning architecture for human activity recognition.
    *IEEE Access*, 9:68985–69001, 2021.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ronald 等人 [2021] Mutegeki Ronald、Alwin Poulose 和 Dong Seog Han. isplinception:
    一种用于人类活动识别的 inception-resnet 深度学习架构。*IEEE Access*, 9:68985–69001, 2021.'
- en: Challa et al. [2022] Sravan Kumar Challa, Akhilesh Kumar, and Vijay Bhaskar
    Semwal. A multibranch cnn-bilstm model for human activity recognition using wearable
    sensor data. *The Visual Computer*, 38(12):4095–4109, 2022.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查拉等人 [2022] 斯拉万·库马尔·查拉、阿基莱什·库马尔和维贾伊·布哈斯卡尔·森瓦尔。用于人类活动识别的多分支CNN-BILSTM模型，基于可穿戴传感器数据。*视觉计算*，38(12):4095–4109，2022年。
- en: Jin et al. [2022a] Ruibing Jin, Zhenghua Chen, Keyu Wu, Min Wu, Xiaoli Li, and
    Ruqiang Yan. Bi-lstm-based two-stream network for machine remaining useful life
    prediction. *IEEE Transactions on Instrumentation and Measurement*, 71:1–10, 2022a.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金等人 [2022a] 瑞冰金、正华陈、可优吴、敏吴、晓莉李和瑞强阎。基于Bi-LSTM的双流网络用于机器剩余使用寿命预测。*IEEE仪器与测量汇刊*，71:1–10，2022a年。
- en: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pages 8748–8763\. PMLR, 2021.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉德福德等人 [2021] 亚历克·拉德福德、钟沃克·金、克里斯·哈拉西、阿迪蒂亚·拉梅什、加布里埃尔·戈、桑迪尼·阿格瓦尔、吉里什·萨斯特里、阿曼达·阿斯克尔、帕梅拉·米什金、杰克·克拉克等。通过自然语言监督学习可迁移的视觉模型。发表于
    *国际机器学习会议*，第8748–8763页。PMLR，2021年。
- en: Li et al. [2018] Xiang Li, Qian Ding, and Jian-Qiao Sun. Remaining useful life
    estimation in prognostics using deep convolution neural networks. *Reliability
    Engineering & System Safety*, 172:1–11, 2018.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 [2018] 相李、倩丁和建桥孙。使用深度卷积神经网络进行预测中的剩余使用寿命估计。*可靠性工程与系统安全*，172:1–11，2018年。
- en: Liu et al. [2019] Hui Liu, Zhenyu Liu, Weiqiang Jia, and Xianke Lin. A novel
    deep learning-based encoder-decoder model for remaining useful life prediction.
    In *2019 International Joint Conference on Neural Networks (IJCNN)*, pages 1–8\.
    IEEE, 2019.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2019] 惠刘、振宇刘、伟强贾和贤科林。用于剩余使用寿命预测的全新深度学习编码器-解码器模型。发表于 *2019 国际联合神经网络会议 (IJCNN)*，第1–8页。IEEE，2019年。
- en: Jin et al. [2022b] Ruibing Jin, Min Wu, Keyu Wu, Kaizhou Gao, Zhenghua Chen,
    and Xiaoli Li. Position encoding based convolutional neural networks for machine
    remaining useful life prediction. *IEEE/CAA Journal of Automatica Sinica*, 9(8):1427–1439,
    2022b.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金等人 [2022b] 瑞冰金、敏吴、可优吴、开洲高、正华陈和晓莉李。基于位置编码的卷积神经网络用于机器剩余使用寿命预测。*IEEE/CAA自动化学报*，9(8):1427–1439，2022b年。
- en: Behera and Misra [2021] Sourajit Behera and Rajiv Misra. Generative adversarial
    networks based remaining useful life estimation for iiot. *Computers & Electrical
    Engineering*, 92:107195, 2021.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝赫拉和米斯拉 [2021] 苏拉吉特·贝赫拉和拉吉夫·米斯拉。基于生成对抗网络的工业互联网剩余使用寿命估计。*计算机与电气工程*，92:107195，2021年。
- en: Jin et al. [2023d] Ruibing Jin, Duo Zhou, Min Wu, Xiaoli Li, and Zhenghua Chen.
    An adaptive and dynamical neural network for machine remaining useful life prediction.
    *IEEE Transactions on Industrial Informatics*, 2023d.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金等人 [2023d] 瑞冰金、多周、敏吴、晓莉李和正华陈。一种自适应和动态神经网络用于机器剩余使用寿命预测。*IEEE工业信息学汇刊*，2023d年。
- en: Jang and Kim [2021] Jaeyeon Jang and Chang Ouk Kim. Siamese network-based health
    representation learning and robust reference-based remaining useful life prediction.
    *IEEE Transactions on Industrial Informatics*, 18(8):5264–5274, 2021.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张和金 [2021] 照贤张和昌玉金。基于Siamese网络的健康表示学习和鲁棒参考剩余使用寿命预测。*IEEE工业信息学汇刊*，18(8):5264–5274，2021年。
- en: 'Xu et al. [2021] Qing Xu, Zhenghua Chen, Keyu Wu, Chao Wang, Min Wu, and Xiaoli
    Li. Kdnet-rul: A knowledge distillation framework to compress deep neural networks
    for machine remaining useful life prediction. *IEEE Transactions on Industrial
    Electronics*, 2021.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 徐等人 [2021] 庆徐、正华陈、可优吴、超王、敏吴和晓莉李。KDNet-RUL：一种知识蒸馏框架，用于压缩深度神经网络以预测机器剩余使用寿命。*IEEE工业电子汇刊*，2021年。
- en: Xia et al. [2020] Kun Xia, Jianguang Huang, and Hanyu Wang. Lstm-cnn architecture
    for human activity recognition. *IEEE Access*, 8:56855–56866, 2020.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 夏等人 [2020] 昆夏、建光黄和汉宇王。LSTM-CNN架构用于人类活动识别。*IEEE Access*，8:56855–56866，2020年。
- en: 'Van Kuppevelt et al. [2020] D Van Kuppevelt, C Meijer, F Huber, A van der Ploeg,
    S Georgievska, and Vincent T van Hees. Mcfly: Automated deep learning on time
    series. *SoftwareX*, 12:100548, 2020.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 范·库佩维尔特等人 [2020] D·范·库佩维尔特、C·梅耶、F·休伯、A·范德·普洛格、S·乔治耶夫斯卡和文森特·T·范·赫斯。McFly：时间序列上的自动深度学习。*SoftwareX*，12:100548，2020年。
- en: Dua et al. [2021] Nidhi Dua, Shiva Nand Singh, and Vijay Bhaskar Semwal. Multi-input
    cnn-gru based human activity recognition using wearable sensors. *Computing*,
    103:1461–1478, 2021.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杜等人 [2021] 尼迪·杜、希瓦·南德·辛格和维贾伊·布哈斯卡尔·森瓦尔。基于多输入CNN-GRU的可穿戴传感器人类活动识别。*计算*，103:1461–1478，2021年。
- en: 'Mim et al. [2023] Taima Rahman Mim, Maliha Amatullah, Sadia Afreen, Mohammad Abu
    Yousuf, Shahadat Uddin, Salem A Alyami, Khondokar Fida Hasan, and Mohammad Ali
    Moni. Gru-inc: An inception-attention based approach using gru for human activity
    recognition. *Expert Systems with Applications*, 216:119419, 2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mim 等人 [2023] Taima Rahman Mim, Maliha Amatullah, Sadia Afreen, Mohammad Abu
    Yousuf, Shahadat Uddin, Salem A Alyami, Khondokar Fida Hasan 和 Mohammad Ali Moni。Gru-inc:
    一种基于 inception-attention 的 GRU 方法用于人类活动识别。*Expert Systems with Applications*，216:119419，2023年。'
- en: Ige and Noor [2023] Ayokunle Olalekan Ige and Mohd Halim Mohd Noor. A deep local-temporal
    architecture with attention for lightweight human activity recognition. *Applied
    Soft Computing*, 149:110954, 2023.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ige 和 Noor [2023] Ayokunle Olalekan Ige 和 Mohd Halim Mohd Noor。一种具有注意力机制的深度局部-时间架构用于轻量级人类活动识别。*Applied
    Soft Computing*，149:110954，2023年。
