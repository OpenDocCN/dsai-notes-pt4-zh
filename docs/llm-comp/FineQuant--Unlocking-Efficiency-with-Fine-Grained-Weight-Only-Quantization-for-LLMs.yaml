- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:51:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:51:21'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization
    for LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'FineQuant: 通过细粒度权重量化解锁LLM的效率'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.09723](https://ar5iv.labs.arxiv.org/html/2308.09723)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2308.09723](https://ar5iv.labs.arxiv.org/html/2308.09723)
- en: Young Jin Kim
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Young Jin Kim
- en: Microsoft
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft
- en: youki@microsoft.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: youki@microsoft.com
- en: '&Rawn Henry¹¹footnotemark: 1'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&Rawn Henry¹¹脚注标记: 1'
- en: NVIDIA
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA
- en: rhenry@nvidia.com
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: rhenry@nvidia.com
- en: Raffy Fahim
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Raffy Fahim
- en: Microsoft
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft
- en: raffybekheit@microsoft.com
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: raffybekheit@microsoft.com
- en: '&Hany Hassan Awadalla'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '&Hany Hassan Awadalla'
- en: Microsoft
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft
- en: hanyh@microsoft.com Equal contribution.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: hanyh@microsoft.com 等贡献。
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have achieved state-of-the-art performance across
    various language tasks but pose challenges for practical deployment due to their
    substantial memory requirements. Furthermore, the latest generative models suffer
    from high inference costs caused by the memory bandwidth bottleneck in the auto-regressive
    decoding process. To address these issues, we propose an efficient weight-only
    quantization method that reduces memory consumption and accelerates inference
    for LLMs. To ensure minimal quality degradation, we introduce a simple and effective
    heuristic approach that utilizes only the model weights of a pre-trained model.
    This approach is applicable to both Mixture-of-Experts (MoE) and dense models
    without requiring additional fine-tuning. To demonstrate the effectiveness of
    our proposed method, we first analyze the challenges and issues associated with
    LLM quantization. Subsequently, we present our heuristic approach, which adaptively
    finds the granularity of quantization, effectively addressing these problems.
    Furthermore, we implement highly efficient GPU GEMMs that perform on-the-fly matrix
    multiplication and dequantization, supporting the multiplication of fp16 or bf16
    activations with int8 or int4 weights. We evaluate our approach on large-scale
    open source models such as OPT-175B and internal MoE models, showcasing minimal
    accuracy loss while achieving up to 3.65 times higher throughput on the same number
    of GPUs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种语言任务中已实现了最先进的性能，但由于其巨大的内存需求，在实际部署中存在挑战。此外，最新的生成模型由于自回归解码过程中的内存带宽瓶颈而面临高推理成本。为了解决这些问题，我们提出了一种高效的仅权重量化方法，能够减少内存消耗并加速LLM的推理。为了确保最小的质量下降，我们引入了一种简单有效的启发式方法，该方法仅利用预训练模型的权重。这种方法适用于Mixture-of-Experts（MoE）和密集模型，无需额外的微调。为了展示我们提出的方法的有效性，我们首先分析了LLM量化的挑战和问题。随后，我们展示了我们的启发式方法，它自适应地确定量化的粒度，有效地解决了这些问题。此外，我们实现了高效的GPU
    GEMMs，能够实时进行矩阵乘法和去量化，支持fp16或bf16激活值与int8或int4权重的乘法。我们在大规模开源模型如OPT-175B和内部MoE模型上评估了我们的方法，展示了在相同数量的GPU上，准确度损失最小的情况下，吞吐量提高了最多3.65倍。
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have proven their efficacy in various language
    tasks by increasing the number of trainable parameters and pre-training models
    on large-scale data to be used in different downstream tasks (Devlin et al., [2018](#bib.bib7);
    Radford et al., [2018](#bib.bib24); Liu et al., [2019](#bib.bib22); Raffel et al.,
    [2020](#bib.bib26)). With the advancement of distributed large-scale training
    methods (Shazeer et al., [2018](#bib.bib30); Rasley et al., [2020](#bib.bib27);
    Ren et al., [2021](#bib.bib28); Baines et al., [2021](#bib.bib2)) and large-scale
    data collection (Raffel et al., [2020](#bib.bib26); Hoffmann et al., [2022](#bib.bib13)),
    models have grown even larger and achieved state-of-the-art performance with increased
    capacity to learn, demonstrating the capability for in-context learning (Brown
    et al., [2020](#bib.bib3); Zhang et al., [2022](#bib.bib36); Chowdhery et al.,
    [2022](#bib.bib5)) that can be used for various language tasks even without updating
    parameters for specific tasks. Zhang et al. ([2022](#bib.bib36))
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）通过增加可训练参数的数量和在大规模数据上进行预训练，以用于不同的下游任务，已经证明了它们在各种语言任务中的有效性（Devlin
    et al., [2018](#bib.bib7); Radford et al., [2018](#bib.bib24); Liu et al., [2019](#bib.bib22);
    Raffel et al., [2020](#bib.bib26)）。随着分布式大规模训练方法（Shazeer et al., [2018](#bib.bib30);
    Rasley et al., [2020](#bib.bib27); Ren et al., [2021](#bib.bib28); Baines et al.,
    [2021](#bib.bib2)）和大规模数据收集（Raffel et al., [2020](#bib.bib26); Hoffmann et al.,
    [2022](#bib.bib13)）的进步，模型变得更大，并且在增加学习能力的同时实现了最先进的性能，展示了上下文学习的能力（Brown et al.,
    [2020](#bib.bib3); Zhang et al., [2022](#bib.bib36); Chowdhery et al., [2022](#bib.bib5)），即使在不更新特定任务的参数的情况下，也可以用于各种语言任务。Zhang
    et al. ([2022](#bib.bib36))
- en: However, deploying such large models comes with a significant cost which increases
    proportionally with the model size. Model size growth has increased several orders
    of magnitude over the last few years (1,588 times larger from BERT large - 340
    million to PaLM 540 billion)(Devlin et al., [2018](#bib.bib7); Chowdhery et al.,
    [2022](#bib.bib5)), and without improving inference efficiency, inference cost
    and latency will rise dramatically.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，部署如此大型的模型伴随着显著的成本，这些成本与模型的大小成正比。模型规模的增长在过去几年中增加了几个数量级（从BERT large - 3.4亿到PaLM
    5400亿增长了1,588倍）（Devlin et al., [2018](#bib.bib7); Chowdhery et al., [2022](#bib.bib5)），而且如果不提高推理效率，推理成本和延迟将会大幅上升。
- en: Quantization is a compression technique that reduces model size and speeds up
    inference by approximating floating-point numbers with smaller precision numbers.
    Numerous studies have demonstrated the effectiveness of quantization in accelerating
    neural network model inference (Rodriguez et al., [2018](#bib.bib29); Stock et al.,
    [2019](#bib.bib31); Choukroun et al., [2019](#bib.bib4); Gholami et al., [2022](#bib.bib12)),
    particularly in natural language generation, such as machine translation (Kim
    et al., [2019](#bib.bib19); Aji and Heafield, [2020](#bib.bib1); Fan et al., [2021](#bib.bib8);
    Park et al., [2022](#bib.bib23); Kim et al., [2022](#bib.bib18)) and natural language
    understanding tasks (Kim and Awadalla, [2020](#bib.bib16)). However, it is still
    under-explored how weight-only quantization can be effectively utilized in the
    context of large language models. Also, the existing methods introduce complex
    and costly procedures such as additional Quantization Aware Training (QAT) and/or
    calibration on additional data. Otherwise, they compromise either speed or accuracy.
    To more effectively solve the challenge, we focus on simple weight-only quantization
    method that requires no additional training in this study because it has multiple
    advantages - (i) the accuracy could be maintained well because its underlying
    numerical computation is done in floating-point precision which is more accurate.
    As a result, we can effectively push the precision to very low bit-ranges. (ii)
    it could be used for various hardware and GPU architectures without needing specific
    hardware instructions dealing with low-bit multiplications. (iii) it can avoid
    expensive additional training steps. Then, the key research questions are how
    to effectively exploit this low-bit quantization without losing accuracy and how
    to efficiently implement a GEMM which accepts different types on modern GPUs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '量化是一种压缩技术，通过使用更小精度的数字来近似浮点数，从而减少模型大小并加速推断。大量研究已经证明了量化在加速神经网络模型推断中的有效性（Rodriguez
    等，[2018](#bib.bib29)；Stock 等，[2019](#bib.bib31)；Choukroun 等，[2019](#bib.bib4)；Gholami
    等，[2022](#bib.bib12)），特别是在自然语言生成任务中，如机器翻译（Kim 等，[2019](#bib.bib19)；Aji 和 Heafield，[2020](#bib.bib1)；Fan
    等，[2021](#bib.bib8)；Park 等，[2022](#bib.bib23)；Kim 等，[2022](#bib.bib18)）和自然语言理解任务（Kim
    和 Awadalla，[2020](#bib.bib16)）。然而，在大型语言模型的背景下，权重仅量化如何有效利用仍然尚未充分探讨。此外，现有方法引入了复杂且昂贵的程序，如额外的量化感知训练（QAT）和/或在额外数据上的校准。否则，它们会在速度或准确性之间做出妥协。为了更有效地解决这一挑战，我们在本研究中专注于无需额外训练的简单权重仅量化方法，因为它具有多个优点——（i）准确性可以很好地保持，因为其底层数值计算是在浮点精度下进行的，这种精度更高。因此，我们可以有效地将精度压缩到非常低的位范围。（ii）它可以用于各种硬件和
    GPU 架构，无需特定的硬件指令来处理低位乘法。（iii）它可以避免昂贵的额外训练步骤。然后，关键的研究问题是如何有效利用这种低位量化而不失去准确性，以及如何在现代
    GPU 上高效实现接受不同类型的 GEMM。 '
- en: 'In this paper, we make the following contributions:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们做出了以下贡献：
- en: '1.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Extensive Analyses of Quantization Behaviors: We provide comprehensive analyses
    of the quantization behaviors on Language Model Models (LLMs). We investigate
    the impact of applying low-bit quantization (down to 3-bits) on LLM accuracy.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化行为的广泛分析：我们提供了关于语言模型（LLMs）量化行为的全面分析。我们研究了低位量化（最低可达 3 位）对 LLM 准确性的影响。
- en: '2.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Fine-Grained Quantization Algorithm: We propose a fine-grained quantization
    algorithm that incorporates group-wise quantization and adaptive selection of
    granularity. This approach helps preserve the original floating-point precision
    accuracy even when there is loss due to quantization.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精细化量化算法：我们提出了一种精细化量化算法，该算法结合了组级量化和自适应粒度选择。这种方法即使在量化导致损失的情况下，也有助于保持原始浮点精度的准确性。
- en: '3.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Highly Efficient GPU Kernels: We implement highly efficient GPU kernels and
    conduct a thorough performance analysis, considering different batch sizes and
    context lengths. This analysis allows us to identify the optimal utilization of
    the proposed approach on real GPUs.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高效的 GPU 核心：我们实现了高效的 GPU 核心，并进行了全面的性能分析，考虑了不同的批处理大小和上下文长度。这项分析使我们能够识别提出的方法在实际
    GPU 上的最佳利用方式。
- en: '4.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Accelerated Inference with Large-Scale Models: We demonstrate the effectiveness
    of the proposed method by applying it to a large-scale open-source dense transformer
    model called OPT. With its 175 billion parameters and internal MoE models utilizing
    optimized GPU kernels, our method enables deployment of the 175 billion parameter
    model on only 2 GPUs, resulting in a significant reduction of overhead and cost
    by 64%. Moreover, our method achieves 3.65 times higher throughput on the same
    number of GPUs.'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大规模模型的加速推理：我们通过将提出的方法应用于一个名为OPT的大规模开源稠密变换器模型，展示了其有效性。凭借其1750亿个参数和利用优化GPU内核的内部MoE模型，我们的方法使得在仅使用2个GPU的情况下部署1750亿参数的模型成为可能，从而显著减少了64%的开销和成本。此外，我们的方法在相同数量的GPU上实现了3.65倍的吞吐量提升。
- en: These contributions collectively advance the understanding of quantization behaviors
    in LLMs, propose an effective quantization algorithm, optimize GPU implementation,
    and demonstrate the practical benefits in terms of reduced resource requirements
    and improved inference throughput.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些贡献共同推进了对LLMs中量化行为的理解，提出了一种有效的量化算法，优化了GPU实现，并展示了在减少资源需求和提高推理吞吐量方面的实际好处。
- en: 2 Background - Challenges of Quantizing LLMs
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景 - 量化LLMs的挑战
- en: 2.1 Fundamental challenges of inferencing generative LLMs
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 生成型LLMs推理的基本挑战
- en: Increased communication overhead. We must issue an all reduce after each attention
    and FFN block when doing inference with tensor parallelism. While technologies
    such as NVLink and NCCL greatly accelerate GPU to GPU communication, it is desirable
    to use as few GPUs as possible to minimize this overhead.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通信开销增加。在进行张量并行推理时，我们必须在每个注意力和FFN块之后进行全归约。虽然NVLink和NCCL等技术极大地加速了GPU到GPU的通信，但仍然希望使用尽可能少的GPU来最小化这一开销。
- en: Large weights with small activations. The increase in the model size causes
    the matrix multiplies in the decoding phase of LLMs to be bottlenecked by memory
    bandwidth. The weights typically dominate the memory traffic as the activations
    tend to only have a few tokens once the context has been used to generate the
    KV attention caches. As the number of parameters increase, the amount of data
    that must be moved from HBM to the GPU cores increases which places even more
    pressure on the memory subsystem. In modern processors, compute is much faster
    than memory so it is desirable to reduce the memory bottleneck.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 大量权重和小激活。模型大小的增加导致LLMs在解码阶段的矩阵乘法受到内存带宽的瓶颈。权重通常主导内存流量，因为激活在使用上下文生成KV注意力缓存后往往只有少量标记。随着参数数量的增加，从HBM到GPU核心的数据量也增加，这对内存子系统施加了更大的压力。在现代处理器中，计算速度远快于内存，因此希望减少内存瓶颈。
- en: Given those observations, it is critical to reduce the memory footprint.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些观察结果，减少内存占用至关重要。
- en: 2.2 Quantization challenges
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 量化挑战
- en: Quantization is an active research topic to accelerate inference and reduce
    the memory footprint of LLMs. However, there are still many challenges remaining,
    and especially there is no single method which can maintain the accuracy and improve
    the efficiency at the same time without introducing complex procedures to convert
    and execute an inference.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一个积极的研究课题，旨在加速推理并减少大型语言模型（LLMs）的内存占用。然而，仍然存在许多挑战，尤其是没有单一的方法可以在不引入复杂程序的情况下同时保持准确性和提高效率来转换和执行推理。
- en: It is hard to maintain good accuracy when applying quantizaiton on LLMs. It
    is known that naive quantization methods could significantly degrade the accuracy
    compared to the original models’ (Frantar et al., [2022](#bib.bib10)). One reason
    for this is outliers in the activation based on the previous studies (Dettmers
    et al., [2022](#bib.bib6); Xiao et al., [2022](#bib.bib33)). Dettmers et al. ([2022](#bib.bib6));
    Xiao et al. ([2022](#bib.bib33)) proposed methods to mitigate this issue by handling
    the outliers separately in floating-point arithmetic or by shifting the multiplier
    to the model weights from the activations.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在对LLMs应用量化时，保持良好的准确性是困难的。已知天真的量化方法可能会显著降低准确性，相比于原始模型（Frantar等，[2022](#bib.bib10)）。这其中一个原因是基于以往研究的激活中的异常值（Dettmers等，[2022](#bib.bib6)；Xiao等，[2022](#bib.bib33)）。Dettmers等（[2022](#bib.bib6)）；Xiao等（[2022](#bib.bib33)）提出了通过在浮点运算中单独处理异常值或将乘法器从激活迁移到模型权重来缓解这一问题的方法。
- en: It is difficult to achieve high efficiency. Even if some algorithms could maintain
    the accuracy of the original floating-point models, it is also non-trivial to
    get efficient implementation of the proposed method in reality. This requires
    special kernel implementations on GPUs. For example, Dettmers et al. ([2022](#bib.bib6))
    could achieve a good accuracy with quantization, but the efficiency improvement
    was marginal. Also, OPTQ Frantar et al. ([2022](#bib.bib10)) does not provide
    efficient inference kernels other than batch size 1\. However, we note that our
    efficient GPU kernels can be used with weights quantized by OPTQ., allowing one
    to benefit from the speed of our kernels and the accuracy of OPTQ.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 实现高效性很困难。即使一些算法能够保持原始浮点模型的准确性，但在现实中实现这些方法的高效性也并不简单。这需要在 GPU 上进行特殊的内核实现。例如，Dettmers
    等人 ([2022](#bib.bib6)) 能够通过量化实现良好的准确度，但效率提升却微不足道。此外，OPTQ Frantar 等人 ([2022](#bib.bib10))
    除了批量大小为 1 的情况外，不提供高效的推理内核。然而，我们注意到我们的高效 GPU 内核可以与 OPTQ 量化的权重一起使用，从而使人们能够同时受益于我们内核的速度和
    OPTQ 的准确性。
- en: Added complexity to solve the problem. To overcome the issues of accuracy drop
    and inefficiency of runtimes, there have been several studies proposed. Those
    approaches require expensive and complex procedures to achieve the goal, especially
    with target task specific dataset for the calibration. Yao et al. ([2022](#bib.bib35))
    uses additional knowledge distillation steps to recover the accuracy drop from
    the quantization. Park et al. ([2022](#bib.bib23)) uses binary coding quantization
    and it performs iterative numerical optimization to find the best binary coding
    scheme for a given model and a task which is non-trivial. Frantar et al. ([2022](#bib.bib10))
    uses Optimal Brain Quantization (OBQ) to maintain the accuracy of the original
    floating-point model which shuffles the model weights based on the approximated
    second-order Hessian matrix information. All of those approaches have introduced
    non-trivial and dataset specific algorithmic procedures. Especially, the cost
    of those algorithms grows together with the size of the base models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 解决问题的复杂性。为了克服准确度下降和运行效率低的问题，已经提出了几种研究。这些方法需要昂贵且复杂的程序来实现目标，尤其是针对特定任务的数据集用于校准。Yao
    等人 ([2022](#bib.bib35)) 使用了额外的知识蒸馏步骤来恢复量化带来的准确度下降。Park 等人 ([2022](#bib.bib23))
    使用二进制编码量化，并执行迭代数值优化以找到适合特定模型和任务的最佳二进制编码方案，这并不简单。Frantar 等人 ([2022](#bib.bib10))
    使用最优脑量化（OBQ）来维持原始浮点模型的准确度，该方法基于近似的二阶 Hessian 矩阵信息对模型权重进行重新排序。所有这些方法都引入了复杂且特定数据集的算法程序。特别是，这些算法的成本随着基础模型的规模而增长。
- en: In this work, our goal is to find a scalable, accurate and efficient quantization
    method without introducing additional cost of model conversion.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们的目标是找到一种可扩展、准确且高效的量化方法，而不引入额外的模型转换成本。
- en: 3 Designing Quantization Methods for LLMs - Adaptive Fine-grained Quantization
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 设计 LLMs 的量化方法 - 自适应细粒度量化
- en: This section delves into the phenomenon observed in LLM quantization, specifically
    focusing on potential issues that can lead to quality degradation, particularly
    in relation to the quantization range. We thoroughly examine these issues and
    explore potential strategies to mitigate them while ensuring effective control
    over the quantization range. Building on our analysis, we propose a heuristic
    algorithm designed to automatically determine the appropriate quantization range.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本节**深入探讨**了在 LLM 量化中观察到的现象，特别关注可能导致质量下降的问题，尤其是与量化范围相关的问题。我们彻底审查了这些问题，并探索了潜在的策略以减轻这些问题，同时确保有效控制量化范围。基于我们的分析，我们提出了一种启发式算法，旨在自动确定适当的量化范围。
- en: '3.1 Quantization methodology: basic settings'
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 量化方法论：基本设置
- en: Uniformity of quantization
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 量化的一致性
- en: We conducted experiments involving two quantization techniques that focus on
    the uniformity of the quantized range. Firstly, we employed linear quantization,
    which uniformly maps quantized integer values to their corresponding original
    float values. Secondly, we explored log-based quantization, inspired by Aji and
    Heafield ([2020](#bib.bib1)), where both integer and float ranges are mapped in
    a logarithmic scale. In both cases, we applied column-wise quantization to assess
    the impact of quantization uniformity on model accuracy. Detailed formulations
    for those two techniques are described in Appendix A.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了涉及两种量化技术的实验，这些技术关注于量化范围的均匀性。首先，我们采用了线性量化，将量化的整数值均匀地映射到其对应的原始浮点值。其次，我们探索了基于对数的量化，受到Aji和Heafield（[2020](#bib.bib1)）的启发，其中整数和浮点范围都在对数尺度上进行映射。在这两种情况下，我们应用了按列量化，以评估量化均匀性对模型准确度的影响。这两种技术的详细公式描述见附录A。
- en: 'Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Quantization methodology: basic settings
    ‣ 3 Designing Quantization Methods for LLMs - Adaptive Fine-grained Quantization
    ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for
    LLMs") illustrates the performance comparison between two quantization techniques
    applied to FFN layers using low bits. For 3 and 4 bits, both techniques exhibit
    similar performance. However, with 2-bit quantization, log-scale quantization
    shows a significant decrease in accuracy. Considering these observations and the
    computational simplicity, we opt to use uniform quantization for all subsequent
    experiments.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '图[1](#S3.F1 "Figure 1 ‣ 3.1 Quantization methodology: basic settings ‣ 3 Designing
    Quantization Methods for LLMs - Adaptive Fine-grained Quantization ‣ FineQuant:
    Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs")展示了在FFN层中应用低位数的两种量化技术的性能比较。对于3位和4位，两种技术的表现相似。然而，对于2位量化，log尺度量化显示了显著的准确度下降。考虑到这些观察结果和计算简便性，我们选择在所有后续实验中使用均匀量化。'
- en: '![Refer to caption](img/9bbb869a0567daf9794cf8091338eeab.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9bbb869a0567daf9794cf8091338eeab.png)'
- en: 'Figure 1: A comparison of how the quality of the model, as measured by BLEU,
    changes when quantizing with different precisions using different quantization
    methods.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：当使用不同精度和不同量化方法进行量化时，模型质量（以BLEU衡量）的变化比较。
- en: Symmetricity - numerical distribution of model weights
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对称性 - 模型权重的数值分布
- en: 'In order to determine the most appropriate quantization approach, we have conducted
    further analysis on the weight parameter distribution across various layers. Figure [2](#S3.F2
    "Figure 2 ‣ 3.1 Quantization methodology: basic settings ‣ 3 Designing Quantization
    Methods for LLMs - Adaptive Fine-grained Quantization ‣ FineQuant: Unlocking Efficiency
    with Fine-Grained Weight-Only Quantization for LLMs") presents example distributions
    of model weights, which generally exhibit a normal distribution centered around
    zero. However, in some cases, outliers can distort the weight distribution, potentially
    leading to an inaccurate quantization range. Based on our observations and considering
    implementation efficiency, we choose to employ symmetric quantization around zero.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '为了确定最合适的量化方法，我们对不同层的权重参数分布进行了进一步分析。图[2](#S3.F2 "Figure 2 ‣ 3.1 Quantization
    methodology: basic settings ‣ 3 Designing Quantization Methods for LLMs - Adaptive
    Fine-grained Quantization ‣ FineQuant: Unlocking Efficiency with Fine-Grained
    Weight-Only Quantization for LLMs")展示了模型权重的示例分布，这些分布通常呈现以零为中心的正态分布。然而，在某些情况下，离群值可能会扭曲权重分布，从而导致不准确的量化范围。基于我们的观察和实施效率，我们选择使用围绕零对称的量化。'
- en: '![Refer to caption](img/8cbbc991650f5a510b5b3a5e6258c251.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8cbbc991650f5a510b5b3a5e6258c251.png)'
- en: (a) Example expert weight distribution                                                                                                                                                 (layer
    6, FFN 2, expert 15)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 示例专家权重分布（第6层，FFN 2，专家15）
- en: '![Refer to caption](img/1f27ca2e2938d31141325a67a48558db.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1f27ca2e2938d31141325a67a48558db.png)'
- en: (b) Example FFN weight distribution                                                                                                                                                 (layer
    7, FFN 2)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 示例FFN权重分布（第7层，FFN 2）
- en: 'Figure 2: A comparison of example weight distributions from MoE and dense FFN
    layers.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：MoE和稠密FFN层的示例权重分布比较。
- en: 3.2 Granularity of quantization
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 量化的粒度
- en: Considering the design choices made earlier in this section, the granularity
    of quantization emerges as the most crucial component of the quantization algorithm.
    For the sake of efficient computation and reduced memory consumption, it is typical
    to have 1 quantization scale per tensor or 1 quantization scale for each column
    in the tensor. However, to maintain a close approximation of the original numerical
    values with the quantized values, it is desirable to have smaller groups of parameters
    sharing scales. This is necessary because outliers in the distribution have the
    potential to significantly skew the data, leading to decreased quantization precision,
    especially for smaller numerical values.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到本节早些时候做出的设计选择，量化的粒度成为量化算法中最关键的组成部分。为了实现高效的计算和减少内存消耗，通常会为每个张量使用 1 个量化尺度，或为张量中的每一列使用
    1 个量化尺度。然而，为了使量化值尽可能接近原始数值，理想的情况是让较小的参数组共享尺度。这是必要的，因为分布中的离群点可能会显著扭曲数据，从而导致量化精度下降，特别是对于较小的数值。
- en: 3.2.1 Catastrophic collapse of model performance
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 模型性能的灾难性崩溃
- en: 'Throughout our observations, we have noted a significant decline in performance
    when employing matrix-wise quantization compared to column-wise quantization across
    various layers, as demonstrated in Appendix B. Consequently, column-wise quantization
    serves as the baseline for our experiments. However, even with column-wise quantization,
    we have encountered instances of catastrophic collapse in LLM performance, particularly
    when certain outliers exist in the model weights. Figure [3(a)](#S3.F3.sf1 "In
    Figure 3 ‣ 3.2.1 Catastrophic collapse of model performance ‣ 3.2 Granularity
    of quantization ‣ 3 Designing Quantization Methods for LLMs - Adaptive Fine-grained
    Quantization ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization
    for LLMs") depicts the relationship between the Mean Squared Error (MSE) of quantized
    values and the translation BLEU scores as we modify the group size in the OPT
    30B model. While increasing granularity leads to a gradual rise in MSE values,
    the model quickly loses its capability in terms of task BLEU score beyond a certain
    point. Consequently, it is crucial to determine the optimal granularity for each
    matrix to preserve the task performance while maximizing the size of the parameter
    groups which share scales.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的观察，与列-wise 量化相比，矩阵-wise 量化在不同层次上表现出显著的性能下降，如附录 B 所示。因此，列-wise 量化作为我们实验的基线。然而，即使在列-wise
    量化的情况下，当模型权重中存在某些离群点时，我们也遇到了 LLM 性能的灾难性崩溃实例。图 [3(a)](#S3.F3.sf1 "图 3 ‣ 3.2.1 模型性能的灾难性崩溃
    ‣ 3.2 量化的粒度 ‣ 3 设计 LLM 的量化方法 - 自适应细粒度量化 ‣ FineQuant：利用细粒度权重量化提升 LLM 的效率") 展示了在
    OPT 30B 模型中，量化值的均方误差 (MSE) 与翻译 BLEU 分数之间的关系，随着我们修改组大小的情况。虽然增加粒度会导致 MSE 值逐渐上升，但模型的任务
    BLEU 分数在超过某个点后迅速下降。因此，确定每个矩阵的最佳粒度以在最大化参数组大小的同时保持任务性能是至关重要的。
- en: '![Refer to caption](img/a2c56fca832d18ddcdeb0ee1eeec6fd8.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a2c56fca832d18ddcdeb0ee1eeec6fd8.png)'
- en: (a) MSE and BLEU changes with quantization group sizes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MSE 和 BLEU 随量化分组大小的变化情况。
- en: '![Refer to caption](img/53ce0d7d64419db7b8041f2039d7111f.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/53ce0d7d64419db7b8041f2039d7111f.png)'
- en: (b) BLEU score and model size comparison with adaptive group quantization with
    reference lines of fp16 and fixed group size (64). X-axis represents threshold
    value $\alpha$ of adaptive fine-grained quantization.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 自适应分组量化与参考线 fp16 和固定分组大小 (64) 的 BLEU 分数和模型大小比较。X 轴表示自适应细粒度量化的阈值 $\alpha$。
- en: 'Figure 3: Impact analyses of quantization granularity on translation accuracy
    of OPT-30B.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 量化粒度对 OPT-30B 翻译准确性的影响分析。'
- en: 3.3 Adaptive fine-grained quantization
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 自适应细粒度量化
- en: Upon further investigation into the catastrophic failure of a quantized model,
    we have discovered that the failure could be rectified by adjusting the granularity
    of four specific matrices out of the 288 quantized matrices. Merely increasing
    the granularity of these four matrices by a factor of two allowed for the recovery
    of over 94% of the lost accuracy. Based on this observation, we have developed
    a simple heuristic-based method to assign varying granularity to different model
    weight matrices.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 经过对量化模型灾难性失败的进一步调查，我们发现，通过调整288个量化矩阵中的四个特定矩阵的粒度，可以纠正这种失败。仅仅将这四个矩阵的粒度提高一倍，就能够恢复超过94%的丢失准确性。基于这一观察，我们开发了一种基于启发式的方法，以为不同模型权重矩阵分配不同的粒度。
- en: 'In the process of quantizing a matrix, we start from the column-wise quantization
    and compute the range of the values that must be quantized. We then halve the
    quantization group size and compute the range of each group. If for any group,
    .
    Figure [3(b)](#S3.F3.sf2 "In Figure 3 ‣ 3.2.1 Catastrophic collapse of model performance
    ‣ 3.2 Granularity of quantization ‣ 3 Designing Quantization Methods for LLMs
    - Adaptive Fine-grained Quantization ‣ FineQuant: Unlocking Efficiency with Fine-Grained
    Weight-Only Quantization for LLMs") illustrates the impact of adaptive group size
    on BLEU scores and model sizes in gigabytes (GB). With the adaptive fine-grained
    quantization approach, there is only a marginal 0.1% difference in BLEU score,
    while the model size is reduced to a mere 26% of the original FP16 model size.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '在量化矩阵的过程中，我们从按列量化开始，计算必须量化的值的范围。然后，我们将量化组大小减半，并计算每个组的范围。如果某组的范围变化 。图 [3(b)](#S3.F3.sf2
    "图3 ‣ 3.2.1 模型性能的灾难性崩溃 ‣ 3.2 量化粒度 ‣ 3 为LLMs设计量化方法 - 自适应细粒度量化 ‣ FineQuant: 通过细粒度权重量化解锁LLMs的效率")
    展示了自适应组大小对BLEU分数和以GB为单位的模型大小的影响。采用自适应细粒度量化方法，BLEU分数仅有0.1%的微小差异，而模型大小减少到原FP16模型大小的26%。'
- en: 4 Experiments
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Experimental setup
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Our latency and throughput experiments are conducted using NVIDIA A100 SXM4
    GPUs inside a Docker container running Ubuntu 20.04 and CUDA 11.8\. All code is
    compiled using nvcc 11.8.89 and gcc/g++ 9.3\. To carry out the experiments, we
    use a modified version of FasterTransformer ¹¹1[https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
    v5.3\. The weight-only quantization kernels for per-column quantization are already
    open source.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的延迟和吞吐量实验是在运行Ubuntu 20.04和CUDA 11.8的Docker容器中，使用NVIDIA A100 SXM4 GPU进行的。所有代码都使用nvcc
    11.8.89和gcc/g++ 9.3编译。为了进行实验，我们使用了修改版的FasterTransformer¹¹1[https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
    v5.3。仅权重量化的内核已经开源。
- en: Task and datasets. For the dense models, we utilize various open-source language
    tasks, including LAMBADA, HellaSwag, PiQA, WinoGrande, OpenBookQA, RTE, COPA from
    the lm-evaluation harness (Gao et al., [2021](#bib.bib11)), as well as WMT machine
    translation task (WMT16 German and English)²²2[https://statmt.org/wmt16/](https://statmt.org/wmt16/).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 任务和数据集。对于密集模型，我们利用各种开源语言任务，包括LAMBADA、HellaSwag、PiQA、WinoGrande、OpenBookQA、RTE、COPA（来自lm-evaluation
    harness（Gao等人，[2021](#bib.bib11)）），以及WMT机器翻译任务（WMT16德语和英语）²²2[https://statmt.org/wmt16/](https://statmt.org/wmt16/)。
- en: For the MoE models, we use a multilingual machine translation task that covers
    10 language translation directions from and into English covering German (de),
    French (fr), Italian (it), Spanish (es), Dutch (nl), and English (en). We use
    a 128,000 sub-word vocabulary, built with the sentencepiece library³³3[https://github.com/google/sentencepiece](https://github.com/google/sentencepiece).
    The number of training sentences is included in Appendix E. To measure the accuracy
    of the models, we utilized sacrebleu  ⁴⁴4[https://github.com/mjpost/sacrebleu](https://github.com/mjpost/sacrebleu)
    on the detokenized output.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MoE模型，我们使用一个多语言机器翻译任务，涵盖从和到英语的10种语言翻译方向，包括德语（de）、法语（fr）、意大利语（it）、西班牙语（es）、荷兰语（nl）和英语（en）。我们使用一个包含128,000个子词的词汇表，该词汇表是使用sentencepiece库³³3[https://github.com/google/sentencepiece](https://github.com/google/sentencepiece)构建的。训练句子的数量包含在附录E中。为了衡量模型的准确性，我们在去标记化输出上使用了sacrebleu
     ⁴⁴4[https://github.com/mjpost/sacrebleu](https://github.com/mjpost/sacrebleu)。
- en: Dense model architecture. For the dense model experiments, we utilize various
    open-source large language models that share a similar architecture, which consists
    of decoder-only with multiple transformer layers. To evaluate the accuracy of
    these models, we include GPT-2-XL (1.5B) (Radford et al., [2019](#bib.bib25)),
    OPT (13B and 30B) (Zhang et al., [2022](#bib.bib36)), and OPT-IML (Max 30B and
    Max 175B) (Iyer et al., [2022](#bib.bib14)). The number of model parameters ranges
    from 1.5 billion to 175 billion. The detailed number of layers and hidden dimensions
    can be found in the original papers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 稠密模型架构。对于稠密模型实验，我们使用了多种开源大型语言模型，这些模型共享类似的架构，均为仅解码器且具有多个 transformer 层。为了评估这些模型的准确性，我们包括了
    GPT-2-XL (1.5B) (Radford et al., [2019](#bib.bib25))、OPT (13B 和 30B) (Zhang et
    al., [2022](#bib.bib36)) 和 OPT-IML (Max 30B 和 Max 175B) (Iyer et al., [2022](#bib.bib14))。模型参数数量从
    15 亿到 1750 亿不等。详细的层数和隐藏维度可以在原始论文中找到。
- en: 'MoE model architecture. For our MoE model experiments, we utilize internal
    pre-trained MoE models (5.3B) with a few modifications to the transformer model
    architecture (Vaswani et al., [2017](#bib.bib32)). These modifications encompass
    the following: (i) a deep encoder consisting of 24 transformer layers and a shallow
    decoder comprising 12 transformer layers, (ii) adoption of Transformer with Untied
    Positional Encoding (TUPE) proposed in Ke et al. ([2021](#bib.bib15)) instead
    of the conventional sinusoidal positional embedding, and (iii) implementation
    of pre-layer normalization from Xiong et al. ([2020](#bib.bib34)). For the MoE
    models, we employ top-1 learned gating from Fedus et al. ([2021](#bib.bib9)) and
    an MoE layer with 32 experts at every other layer, specifically the even-numbered
    layers, as utilized in Lepikhin et al. ([2020](#bib.bib20)); Fedus et al. ([2021](#bib.bib9));
    Kim et al. ([2021](#bib.bib17)). Additionally, we apply jittering noise, balancing
    loss (ratio of 0.01) (Lepikhin et al., [2020](#bib.bib20); Fedus et al., [2021](#bib.bib9))
    to more uniformly distribute expert utilization and gating dropout (0.2) (Liu
    et al., [2022](#bib.bib21)) to prevent overfitting and improve regularization.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: MoE 模型架构。对于我们的 MoE 模型实验，我们使用了内部预训练的 MoE 模型 (5.3B)，并对 transformer 模型架构 (Vaswani
    et al., [2017](#bib.bib32)) 进行了少量修改。这些修改包括： (i) 由 24 层 transformer 组成的深层编码器和由
    12 层 transformer 组成的浅层解码器， (ii) 采用 Ke 等人 ([2021](#bib.bib15)) 提出的 Transformer
    with Untied Positional Encoding (TUPE) 代替传统的正弦位置嵌入， (iii) 实现 Xiong 等人 ([2020](#bib.bib34))
    提出的预层归一化。对于 MoE 模型，我们采用 Fedus 等人 ([2021](#bib.bib9)) 提出的 top-1 学习门控，并在每隔一层的 MoE
    层中使用 32 个专家，特别是在偶数层中，如 Lepikhin 等人 ([2020](#bib.bib20)); Fedus 等人 ([2021](#bib.bib9));
    Kim 等人 ([2021](#bib.bib17)) 所使用的那样。此外，我们应用了抖动噪声、平衡损失（比率为 0.01）(Lepikhin et al.,
    [2020](#bib.bib20); Fedus et al., [2021](#bib.bib9)) 以更均匀地分布专家利用情况，以及门控丢弃（0.2）(Liu
    et al., [2022](#bib.bib21)) 以防止过拟合并改善正则化。
- en: GPU kernel implementations. We utilized the kernel implementations developed
    by Kim et al. ([2022](#bib.bib18)), which rely on CUTLASS to create efficient
    kernels for fused dequantization and matrix multiplication. These kernels can
    process either FP16 or BF16 activations, a vector of scales of the same data type
    as the activation, and int8 or int4 weights. The kernels dequantize the weights
    to match the data type of the activation and perform floating-point tensor core
    math. The final output of the kernel is also of the same data type as the input
    activation. These kernels are available as open source code in FasterTransformer.
    To support multiple scaling factors for each column, we extended these kernels
    to process a matrix of scales, enabling us to implement int4 block quantization
    kernels. We set the block size to 64 for all performance analyses below, since
    it matches the K tile size of our fused gemm + dequantize kernels.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 内核实现。我们使用了 Kim 等人开发的内核实现 ([2022](#bib.bib18))，这些实现依赖于 CUTLASS 来创建用于融合去量化和矩阵乘法的高效内核。这些内核可以处理
    FP16 或 BF16 激活值、与激活值相同数据类型的尺度向量，以及 int8 或 int4 权重。这些内核对权重进行去量化，以匹配激活值的数据类型，并执行浮点张量核心计算。内核的最终输出也与输入激活值的数据类型相同。这些内核作为开源代码在
    FasterTransformer 中提供。为了支持每列多个缩放因子，我们扩展了这些内核以处理尺度矩阵，使我们能够实现 int4 块量化内核。我们将块大小设置为
    64，以便于下面的所有性能分析，因为它与我们融合的 gemm + 去量化内核的 K 瓷砖大小相匹配。
- en: 'In compute-bound cases such as an encoder or the context creation phase of
    GPT, the conversions from integer to float bottlenecks our kernels, rather than
    tensor core math. As a result, our weight-only quantization GEMMs slower than
    equivalent FP16xFP16 GEMMs in compute bound cases but offer significant speedup
    in memory bound cases as seen in Figure [4](#S4.F4 "Figure 4 ‣ 4.2.3 End to End
    Benchmarks ‣ 4.2 Dense model performance results ‣ 4 Experiments ‣ FineQuant:
    Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs"). We
    argue that this kernel is useful because:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '在计算受限的情况下，如编码器或GPT的上下文创建阶段，从整数到浮点的转换瓶颈了我们的内核，而不是张量核心数学。因此，我们的仅权重量化GEMMs在计算受限情况下比等效的FP16xFP16
    GEMMs慢，但在内存受限情况下提供了显著的加速，如图[4](#S4.F4 "图 4 ‣ 4.2.3 端到端基准 ‣ 4.2 密集模型性能结果 ‣ 4 实验
    ‣ FineQuant: 通过精细粒度权重量化解锁 LLM 的效率")所示。我们认为这个内核是有用的，因为：'
- en: '1.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Large language models (LLMs) usually spend a lot more time in the memory-bound
    decoding phase than in the compute-bound context creation phase, especially when
    the output sequence length is long.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）通常在内存受限的解码阶段比在计算受限的上下文创建阶段花费更多时间，尤其是当输出序列长度较长时。
- en: '2.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'LLMs are typically served with small batch sizes in most practical cases, which
    puts significant pressure on the memory system during matrix multiplication as
    the weights need to be read from the GPU’s HBM. However, our kernel utilizes int4
    compression, which reduces the number of bytes needed to load the weights by up
    to 4X. The overhead of loading the scales is small, even for block quantization
    with block size 64 as shown in Figure [4](#S4.F4 "Figure 4 ‣ 4.2.3 End to End
    Benchmarks ‣ 4.2 Dense model performance results ‣ 4 Experiments ‣ FineQuant:
    Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs").'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'LLMs在大多数实际情况下通常以小批量大小提供，这在矩阵乘法过程中对内存系统造成了很大压力，因为权重需要从GPU的HBM中读取。然而，我们的内核利用了int4压缩，将加载权重所需的字节数减少了最多4倍。即使是块量化（块大小为64）情况下，加载缩放因子的开销也很小，如图[4](#S4.F4
    "图 4 ‣ 4.2.3 端到端基准 ‣ 4.2 密集模型性能结果 ‣ 4 实验 ‣ FineQuant: 通过精细粒度权重量化解锁 LLM 的效率")所示。'
- en: Quantization Method. All quantization experiments have one scaling factor for
    each column of the weight matrix, unless a block size $B$ elements in a given
    column has its own scaling factor. This means we have multiple scaling factors
    per column.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 量化方法。所有量化实验对权重矩阵的每一列都有一个缩放因子，除非给定列中的一个块大小$B$元素有其自己的缩放因子。这意味着我们每列有多个缩放因子。
- en: 4.2 Dense model performance results
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 密集模型性能结果
- en: 4.2.1 Accuracy
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 准确性
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4.2.3 End to End Benchmarks ‣ 4.2 Dense model performance
    results ‣ 4 Experiments ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs") presents the impact of quantization on various natural
    language tasks using different models. The results show that, in general, 8-bit
    weight-only quantization does not significantly affect the accuracy compared to
    fp16\. This is observed across different language tasks, indicating that the models
    produce similar outputs. However, 4-bit quantization with column-wise granularity
    leads to some degradation in accuracy due to outliers in the weight distribution,
    as discussed in Section [3.2](#S3.SS2 "3.2 Granularity of quantization ‣ 3 Designing
    Quantization Methods for LLMs - Adaptive Fine-grained Quantization ‣ FineQuant:
    Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs"). To
    recover the accuracy, we adopt a group-wise quantization strategy, which shows
    similar accuracy to the original fp16.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '表[1](#S4.T1 "表 1 ‣ 4.2.3 端到端基准 ‣ 4.2 密集模型性能结果 ‣ 4 实验 ‣ FineQuant: 通过精细粒度权重量化解锁
    LLM 的效率")展示了量化对不同自然语言任务的影响，使用了不同的模型。结果显示，总体而言，与fp16相比，8位权重量化对准确性没有显著影响。这在不同的语言任务中均有观察，表明模型生成了类似的输出。然而，4位按列粒度量化由于权重分布中的异常值会导致准确性有所下降，如第[3.2](#S3.SS2
    "3.2 量化粒度 ‣ 3 为 LLM 设计量化方法 - 自适应精细粒度量化 ‣ FineQuant: 通过精细粒度权重量化解锁 LLM 的效率")节中讨论的那样。为了恢复准确性，我们采用了组量化策略，其准确性与原始fp16相似。'
- en: 'We also show similar experiments for OPT-IML for machine translation. Table [5](#S4.T5
    "Table 5 ‣ 4.2.3 End to End Benchmarks ‣ 4.2 Dense model performance results ‣
    4 Experiments ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs") shows the accuracy numbers with different bit quantization
    on OPT-IML 30B and 175B models. With a group-wise quantization approach, the models
    could preserve the accuracy while quantizing down to 4-bit and 3-bit for some
    parts.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还展示了针对机器翻译的OPT-IML的类似实验。表[5](#S4.T5 "Table 5 ‣ 4.2.3 End to End Benchmarks
    ‣ 4.2 Dense model performance results ‣ 4 Experiments ‣ FineQuant: Unlocking Efficiency
    with Fine-Grained Weight-Only Quantization for LLMs")显示了在OPT-IML 30B和175B模型上不同位数量化的准确性数值。通过分组量化方法，这些模型能够在将部分量化至4位和3位的同时保持准确性。'
- en: 4.2.2 Microbenchmarks
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 微基准测试
- en: 'To understand how our weight-only quantization accelerates the matrix multiplies,
    we collect micro-benchmarks from OPT-13B and OPT-30b and present the results in
    Figure [4](#S4.F4 "Figure 4 ‣ 4.2.3 End to End Benchmarks ‣ 4.2 Dense model performance
    results ‣ 4 Experiments ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs"). We find that the matrix multiplies can be accelerated
    by up to 2.5X for those models when the number of tokens in the activation is
    small. This is typically the case for the auto-regressive part of LLMs which tends
    to dominate the overall run-time.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '为了理解我们的权重仅量化如何加速矩阵乘法，我们从OPT-13B和OPT-30b中收集了微基准测试，并在图[4](#S4.F4 "Figure 4 ‣
    4.2.3 End to End Benchmarks ‣ 4.2 Dense model performance results ‣ 4 Experiments
    ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for
    LLMs")中展示了结果。我们发现，当激活中的令牌数量较少时，这些模型的矩阵乘法可以加速高达2.5倍。这通常适用于自回归部分的LLMs，这部分往往主导了总体运行时间。'
- en: 4.2.3 End to End Benchmarks
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 端到端基准测试
- en: 'We construct Table [3](#S4.T3 "Table 3 ‣ 4.2.3 End to End Benchmarks ‣ 4.2
    Dense model performance results ‣ 4 Experiments ‣ FineQuant: Unlocking Efficiency
    with Fine-Grained Weight-Only Quantization for LLMs") as a reference to compute
    end to end times for different input and output lengths for OPT-175B on 8, 4 and
    2 GPUs. Our table shows that the context phase slows done which is primarily due
    to running on fewer GPUs. Additionally, our weight-only quantization kernels have
    some slowdown for compute bound cases. However, we show that the time per decoder
    step is typically within 20 % of FP16 despite using 2X or 4x fewer GPUs. The per-token
    latency does not scale with the number of GPUs since fewer GPUs need to communicate
    and our kernels provide significant acceleration (as shown in Table [4](#S4.F4
    "Figure 4 ‣ 4.2.3 End to End Benchmarks ‣ 4.2 Dense model performance results
    ‣ 4 Experiments ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs")) in the decoder phase.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '我们构建了表[3](#S4.T3 "Table 3 ‣ 4.2.3 End to End Benchmarks ‣ 4.2 Dense model performance
    results ‣ 4 Experiments ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs")，作为参考，以计算OPT-175B在8个、4个和2个GPU上的不同输入和输出长度的端到端时间。我们的表格显示，背景阶段变慢，主要是由于在更少的GPU上运行。此外，我们的权重仅量化内核在计算密集型情况下有一些减速。然而，我们展示了每个解码器步骤的时间通常在FP16的20%以内，尽管使用了2倍或4倍更少的GPU。每个令牌的延迟与GPU的数量无关，因为更少的GPU需要进行通信，而我们的内核在解码阶段提供了显著的加速（如表[4](#S4.F4
    "Figure 4 ‣ 4.2.3 End to End Benchmarks ‣ 4.2 Dense model performance results
    ‣ 4 Experiments ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs")所示）。'
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.2.3 End to End Benchmarks ‣ 4.2 Dense model performance
    results ‣ 4 Experiments ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs") shows end to end times (constructed from Table [3](#S4.T3
    "Table 3 ‣ 4.2.3 End to End Benchmarks ‣ 4.2 Dense model performance results ‣
    4 Experiments ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs")) and associated throughput increases. To calculate the
    throughput increase, we assume the original FP16 model was sharded across 8-GPUs
    within a single node and that same node is used to serve INT8 or INT4 models.
    We measure the throughput per node by assuming that the model is replicated twice
    on the node for INT8 and 4 times for INT4 (64) and that requests are served to
    the independent model instances concurrently. We highlight that our compression
    technique allows serving 4 instances of OPT-175B on a single A100 node with 8
    GPUs.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [4](#S4.T4 "Table 4 ‣ 4.2.3 End to End Benchmarks ‣ 4.2 Dense model performance
    results ‣ 4 Experiments ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs") 显示了端到端时间（从表 [3](#S4.T3 "Table 3 ‣ 4.2.3 End to End Benchmarks
    ‣ 4.2 Dense model performance results ‣ 4 Experiments ‣ FineQuant: Unlocking Efficiency
    with Fine-Grained Weight-Only Quantization for LLMs") 构建而来）和相关的吞吐量提升。为了计算吞吐量的提升，我们假设原始的FP16模型在单个节点上分布于8个GPU，并且该节点用于服务INT8或INT4模型。我们通过假设模型在节点上为INT8复制两次，为INT4
    (64)复制四次，并且请求同时服务于独立的模型实例来测量每个节点的吞吐量。我们强调，我们的压缩技术允许在单个A100节点上使用8个GPU服务4个OPT-175B实例。'
- en: 'Table 1: Accuracy of various models with low-bit weight only quantization on
    different natural language tasks. We also include the perplexity on the wikitext
    dataset for each model. We note that with int4 per-column for OPT-30B actually
    performs worse than FP16 for OPT-13B. Using block quantization (with block size
    64) improves the accuracy by 2.3 % over just using per-column'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同自然语言任务上低比特权重量化的各种模型的准确度。我们还包括了每个模型在wikitext数据集上的困惑度。我们注意到，使用每列int4进行OPT-30B的实际效果比使用FP16的OPT-13B要差。使用块量化（块大小为64）比仅使用每列量化提高了2.3%的准确度。
- en: '| Model type | Precision | LAMBADA | HellaSwag | PiQA | WinoGrande | OBQA |
    RTE | COPA | Average $\uparrow$ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 模型类型 | 精度 | LAMBADA | HellaSwag | PiQA | WinoGrande | OBQA | RTE | COPA |
    平均值 $\uparrow$ |'
- en: '| GPT2-XL | fp16 | 51.1% | 40.0% | 70.7% | 58.2% | 22.4% | 52.3% | 73.0% |
    52.5% | 20.4 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| GPT2-XL | fp16 | 51.1% | 40.0% | 70.7% | 58.2% | 22.4% | 52.3% | 73.0% |
    52.5% | 20.4 |'
- en: '| int8 | 51.1% | 40.0% | 70.7% | 58.3% | 22.6% | 52.7% | 73.0% | 52.6% | 20.4
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| int8 | 51.1% | 40.0% | 70.7% | 58.3% | 22.6% | 52.7% | 73.0% | 52.6% | 20.4
    |'
- en: '| int4 (64) | 49.3% | 39.6% | 70.7% | 58.4% | 20.6% | 50.9% | 74.0% | 51.9%
    | 20.9 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| int4 (64) | 49.3% | 39.6% | 70.7% | 58.4% | 20.6% | 50.9% | 74.0% | 51.9%
    | 20.9 |'
- en: '| int4 | 47.5% | 37.4% | 69.4% | 57.1% | 19.4% | 51.9% | 73.0 % | 50.8% | 21.7
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| int4 | 47.5% | 37.4% | 69.4% | 57.1% | 19.4% | 51.9% | 73.0 % | 50.8% | 21.7
    |'
- en: '| OPT-13B | fp16 | 68.6% | 52.5% | 75.9% | 65.0% | 26.6% | 58.1% | 86.0% |
    61.8% | 11.5 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13B | fp16 | 68.6% | 52.5% | 75.9% | 65.0% | 26.6% | 58.1% | 86.0% |
    61.8% | 11.5 |'
- en: '| int8 | 68.5% | 52.4% | 76.0% | 65.4% | 27.%2 | 57.0% | 86.0% | 61.8% | 11.5
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| int8 | 68.5% | 52.4% | 76.0% | 65.4% | 27.%2 | 57.0% | 86.0% | 61.8% | 11.5
    |'
- en: '| int4 (64) | 67.4% | 50.7% | 75.6% | 65.4% | 25.8% | 59.2% | 84.0% | 61.2%
    | 12.0 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| int4 (64) | 67.4% | 50.7% | 75.6% | 65.4% | 25.8% | 59.2% | 84.0% | 61.2%
    | 12.0 |'
- en: '| int4 | 65.5% | 50.2% | 75.5% | 64.8% | 26.4% | 56.0% | 85.0% | 60.5% | 12.8
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| int4 | 65.5% | 50.2% | 75.5% | 64.8% | 26.4% | 56.0% | 85.0% | 60.5% | 12.8
    |'
- en: '| OPT-30B | fp16 | 71.5% | 54.3% | 77.6% | 68.2% | 30.2% | 57.4% | 82.0% |
    63.0% | 10.7 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30B | fp16 | 71.5% | 54.3% | 77.6% | 68.2% | 30.2% | 57.4% | 82.0% |
    63.0% | 10.7 |'
- en: '| int8 | 71.4% | 54.3% | 77.6% | 67.9% | 30.2% | 58.1% | 82.0% | 63.0% | 10.7
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| int8 | 71.4% | 54.3% | 77.6% | 67.9% | 30.2% | 58.1% | 82.0% | 63.0% | 10.7
    |'
- en: '| int4 (64) | 69.9% | 53.4% | 77.5% | 67.3% | 30.0% | 56.0% | 83.0% | 62.4%
    | 11.1 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| int4 (64) | 69.9% | 53.4% | 77.5% | 67.3% | 30.0% | 56.0% | 83.0% | 62.4%
    | 11.1 |'
- en: '| int4 | 69.5% | 51.9% | 75.8% | 66.3% | 26.8% | 54.9% | 79.0% | 60.1% | 11.6
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| int4 | 69.5% | 51.9% | 75.8% | 66.3% | 26.8% | 54.9% | 79.0% | 60.1% | 11.6
    |'
- en: 'Table 2: Perplexity using LM Eval Harness and FasterTransformer. OPT 66B suffers
    from catastrophic collapse with INT4 per column quantization, but recovers with
    block quantization with a size of 64.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：使用LM Eval Harness和FasterTransformer的困惑度。OPT 66B在INT4每列量化下遭遇灾难性崩溃，但在块量化（大小为64）下恢复。
- en: '| Dataset | OPT 66B | OPT 175B |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | OPT 66B | OPT 175B |'
- en: '| FP16 | INT8 per col | INT4 per col | INT4 (64) | FP16 | INT8 per col | INT4
    per col | INT4 (64) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | INT8 per col | INT4 per col | INT4 (64) | FP16 | INT8 per col | INT4
    per col | INT4 (64) |'
- en: '| Wikitext | 10.15 | 10.15 | 143.16 | 10.66 | 9.08 | 9.08 | 11.08 | 9.84 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Wikitext | 10.15 | 10.15 | 143.16 | 10.66 | 9.08 | 9.08 | 11.08 | 9.84 |'
- en: 'Table 3: We show the time taken to construct the context and the time per decoder
    step for OPT-175B on 8, 4 and 2 GPUs using our different weight-only quantization
    schemes. The numbers for int4 per-column quantization are similar to int4 (64)
    so they are omitted. The compute bound context creation phase is up to 3.5X slower
    when using INT4 block quantization, but running on 4x fewer GPUs. For INT8, it
    is up to 1.9X slower but runs on 2X fewer GPUs. In addition, the time per decoder
    step is typically within 20% of FP16 despite using 2X for 4X fewer GPUs with weight-only
    quantization. End to end times for different numbers of generated tokens can be
    estimated from this table by identifying the batch size and input length of interest
    and computing: context_time + num_generated_tokens * time_per_decoder_step. The
    batch sizes and sequence lengths shown are the maximum sizes that could fit in
    GPU memory.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：我们展示了使用我们不同的仅权重量化方案在 8、4 和 2 个 GPU 上构建上下文所需的时间和每个解码步骤的时间。每列量化的 int4 数字类似于
    int4 (64)，因此被省略。使用 INT4 块量化时，计算限制的上下文创建阶段最多比 FP16 慢 3.5 倍，但 GPU 数量减少 4 倍。对于 INT8，它最多慢
    1.9 倍，但 GPU 数量减少 2 倍。此外，尽管使用了仅权重量化的 2X 到 4X 更少的 GPU，每个解码步骤的时间通常在 FP16 的 20% 以内。可以通过识别感兴趣的批量大小和输入长度，并计算：context_time
    + num_generated_tokens * time_per_decoder_step 来估算生成不同数量令牌的端到端时间。所示的批量大小和序列长度是可以适配到
    GPU 内存的最大尺寸。
- en: '| Batch Size | Input length | FP16 (8 GPUs) | INT8 (4 GPUs) | INT4 (64) (2
    GPUs) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 输入长度 | FP16 (8 GPUs) | INT8 (4 GPUs) | INT4 (64) (2 GPUs) |'
- en: '| Context time (ms) | Avg time per decoder step (ms) | Context time (ms) |
    Avg time per decoder step (ms) | Context time (ms) | Avg time per decoder step
    (ms) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 上下文时间 (ms) | 每个解码步骤的平均时间 (ms) | 上下文时间 (ms) | 每个解码步骤的平均时间 (ms) | 上下文时间 (ms)
    | 每个解码步骤的平均时间 (ms) |'
- en: '| 1 | 128 | 60 | 40 | 76 | 38 | 121 | 43 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 128 | 60 | 40 | 76 | 38 | 121 | 43 |'
- en: '| 2 | 128 | 82 | 41 | 134 | 38 | 226 | 42 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 128 | 82 | 41 | 134 | 38 | 226 | 42 |'
- en: '| 4 | 128 | 148 | 41 | 283 | 38 | 431 | 43 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 128 | 148 | 41 | 283 | 38 | 431 | 43 |'
- en: '| 8 | 128 | 272 | 41 | 468 | 40 | 835 | 45 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 128 | 272 | 41 | 468 | 40 | 835 | 45 |'
- en: '| 12 | 128 | 372 | 42 | 743 | 41 | 1173 | 48 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 128 | 372 | 42 | 743 | 41 | 1173 | 48 |'
- en: '| 16 | 128 | 491 | 42 | 890 | 42 | 1627 | 49 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 128 | 491 | 42 | 890 | 42 | 1627 | 49 |'
- en: '| 32 | 128 | 935 | 44 | 1776 | 47 | 3261 | 58 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 128 | 935 | 44 | 1776 | 47 | 3261 | 58 |'
- en: '| 1 | 512 | 148 | 42 | 280 | 40 | 427 | 44 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 512 | 148 | 42 | 280 | 40 | 427 | 44 |'
- en: '| 2 | 512 | 273 | 43 | 470 | 40 | 838 | 45 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 512 | 273 | 43 | 470 | 40 | 838 | 45 |'
- en: '| 4 | 512 | 493 | 43 | 892 | 41 | 1637 | 46 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 512 | 493 | 43 | 892 | 41 | 1637 | 46 |'
- en: '| 8 | 512 | 939 | 43 | 1784 | 43 | 3291 | 49 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 512 | 939 | 43 | 1784 | 43 | 3291 | 49 |'
- en: '| 1 | 1024 | 271 | 41 | 465 | 39 | 829 | 44 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1024 | 271 | 41 | 465 | 39 | 829 | 44 |'
- en: '| 2 | 1024 | 498 | 42 | 899 | 39 | 1648 | 46 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1024 | 498 | 42 | 899 | 39 | 1648 | 46 |'
- en: '| 4 | 1024 | 945 | 42 | 1795 | 42 | 3307 | 50 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1024 | 945 | 42 | 1795 | 42 | 3307 | 50 |'
- en: 'Table 4: Shows the throughput improvement for batch 1 on a 8-GPU node for different
    input and output lengths. We assume that the model is replicated twice on the
    node for INT8 weight-only quantization and 4 times for INT4 (64) weight-only quantization.
    We show the throughput increase relative to FP16 in parentheses next to through-puts
    for INT8 and INT4\. The table is constructed using data from Table [3](#S4.T3
    "Table 3 ‣ 4.2.3 End to End Benchmarks ‣ 4.2 Dense model performance results ‣
    4 Experiments ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs").'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4：显示了在 8-GPU 节点上针对不同输入和输出长度的批量 1 的吞吐量改进。我们假设在 INT8 仅权重量化情况下，模型在节点上复制了两次，而在
    INT4 (64) 仅权重量化情况下复制了四次。我们在 INT8 和 INT4 的吞吐量旁边用括号显示了相对于 FP16 的吞吐量增益。该表格的数据来自表 [3](#S4.T3
    "表 3 ‣ 4.2.3 端到端基准测试 ‣ 4.2 密集模型性能结果 ‣ 4 实验 ‣ FineQuant: 通过细粒度权重量化解锁 LLM 的效率")。'
- en: '| Input Length | Output Length | FP16 throughput per 8 GPU node (generated
    tokens per sec) | INT8 throughput per 8 GPU node (generated tokens per sec) |
    INT4 (64) throughput per 8 GPU node (generated tokens per sec) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 输入长度 | 输出长度 | FP16 每 8 GPU 节点吞吐量（生成令牌每秒） | INT8 每 8 GPU 节点吞吐量（生成令牌每秒） | INT4
    (64) 每 8 GPU 节点吞吐量（生成令牌每秒） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 128 | 32 | 24 | 49 (2.04$\times$) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 32 | 24 | 49 (2.04$\times$) |'
- en: '| 128 | 128 | 25 | 52 (2.08$\times$) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 128 | 25 | 52 (2.08$\times$) |'
- en: '| 512 | 32 | 21 | 41 (1.95$\times$) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 512 | 32 | 21 | 41 (1.95$\times$) |'
- en: '| 512 | 128 | 23 | 47 (2.04$\times$) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 512 | 128 | 23 | 47 (2.04$\times$) |'
- en: '| 1024 | 32 | 20 | 37 (1.85$\times$) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 1024 | 32 | 20 | 37 (1.85$\times$) |'
- en: '| 1024 | 128 | 23 | 47 (2.04$\times$) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 1024 | 128 | 23 | 47 (2.04$\times$) |'
- en: 183264128256204816384$0$Number
    of Rows in ActivationSpeedup over FP16INT8INT4INT4
    (64)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 183264128256204816384$0$Number
    of Rows in ActivationSpeedup over FP16INT8INT4INT4
    (64)
- en: 'Figure 4: Demonstrates the speed up over FP16 on only the matrix multiplies
    for OPT-13B (left) and OPT-30B (right) on a single GPU. We measure the performance
    of the QKV Projection, Attention Output, FFN1 and FFN2 matrix multiplies and compare
    our CUTLASS FP16 x INT GEMM against cuBLAS performing FP16 x FP16 GEMM. We report
    the geometric mean of the speedups across those 4 GEMMs while varying the number
    of rows in the activation (which represents batch_size $\times$ sequence_length).
    We highlight that when the number of rows is small (such as the decoding phase
    of GPT), we achieve up to 2.5X GEMM speedup when doing int4 quantization with
    block size 64.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：演示了在单个 GPU 上 OPT-13B（左）和 OPT-30B（右）仅对矩阵乘法的 FP16 加速。我们测量了 QKV 投影、注意力输出、FFN1
    和 FFN2 矩阵乘法的性能，并将我们的 CUTLASS FP16 x INT GEMM 与执行 FP16 x FP16 GEMM 的 cuBLAS 进行比较。我们报告了在激活的行数变化下（表示
    batch_size × sequence_length），这 4 个 GEMM 的加速几何平均数。我们强调，当行数较小时（例如 GPT 的解码阶段），在进行
    int4 量化且块大小为 64 时，我们实现了最高 2.5X 的 GEMM 加速。
- en: 'Table 5: Accuracy of OPT-IML models (30B and 175B) with various weight only
    quantization settings on machine translation tasks. The BLEU score is used as
    a metric and higher number represent a better result. The group size for the group-wise
    quantization is specified together with quantization bits.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：不同权重量化设置下 OPT-IML 模型（30B 和 175B）在机器翻译任务中的准确度。使用 BLEU 分数作为度量指标，较高的数值代表更好的结果。按组量化的组大小与量化位数一起指定。
- en: '| Model type | Attention (group) | Others (group) | WMT 2016 German to English
    | Model Footprint (GB) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 模型类型 | 注意力（组） | 其他（组） | WMT 2016 德语到英语 | 模型占用（GB） |'
- en: '|  | fp16 | fp16 | 38.20 | 55.21 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | fp16 | fp16 | 38.20 | 55.21 |'
- en: '|  | int8 (7,168) | int8 (7,168) | 38.20 | 27.66 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | int8 (7,168) | int8 (7,168) | 38.20 | 27.66 |'
- en: '|  | int4 (16) | int4 (16) | 38.10 | 17.32 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | int4 (16) | int4 (16) | 38.10 | 17.32 |'
- en: '|  | int4 (64) | int4 (64) | 37.96 | 14.73 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | int4 (64) | int4 (64) | 37.96 | 14.73 |'
- en: '| OPT-IML Max 30B | int4 (7,168) | int4 (7,168) | 16.86 | 13.88 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| OPT-IML Max 30B | int4 (7,168) | int4 (7,168) | 16.86 | 13.88 |'
- en: '| int4 (adaptive) | int4 (adaptive) | 38.12 | 14.62 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| int4 (自适应) | int4 (自适应) | 38.12 | 14.62 |'
- en: '| int3 (64) | int4 (64) | 37.75 | 13.87 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| int3 (64) | int4 (64) | 37.75 | 13.87 |'
- en: '|  | int4 (64) | int3 (64) | 37.06 | 12.15 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | int4 (64) | int3 (64) | 37.06 | 12.15 |'
- en: '|  | int3 (16) | int3 (16) | 37.57 | 13.87 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | int3 (16) | int3 (16) | 37.57 | 13.87 |'
- en: '|  | int3 (64) | int3 (64) | 36.95 | 11.29 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | int3 (64) | int3 (64) | 36.95 | 11.29 |'
- en: '|  | int3 (7,168) | int3 (7,168) | 0.00 (degenerate) | 10.43 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | int3 (7,168) | int3 (7,168) | 0.00 (退化) | 10.43 |'
- en: '|  | fp16 | fp16 | 41.14 | 324.16 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | fp16 | fp16 | 41.14 | 324.16 |'
- en: '|  | int8 (12,288) | int8 (12,288) | 41.18 | 162.18 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | int8 (12,288) | int8 (12,288) | 41.18 | 162.18 |'
- en: '| OPT-IML Max 175B | int4 (64) | int4 (64) | 40.86 | 86.23 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| OPT-IML Max 175B | int4 (64) | int4 (64) | 40.86 | 86.23 |'
- en: '| int4 (12,288) | int4 (12,288) | 0.00 (degenerate) | 81.19 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| int4 (12,288) | int4 (12,288) | 0.00 (退化) | 81.19 |'
- en: '|  | int3 (64) | int4 (64) | 40.93 | 81.16 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | int3 (64) | int4 (64) | 40.93 | 81.16 |'
- en: '|  | int4 (64) | int3 (64) | 37.02 | 71.04 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | int4 (64) | int3 (64) | 37.02 | 71.04 |'
- en: 4.3 MoE model performance results
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 MoE 模型性能结果
- en: 'We evaluate the performance of our weight-only quantization method on an MoE
    model and report the results in Table [6](#S4.T6 "Table 6 ‣ 4.3 MoE model performance
    results ‣ 4 Experiments ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs"). We investigate the impact of different quantization precisions,
    ranging from 8-bit to 3-bit. Due to the robustness of the MoE FFN layers, the
    model’s accuracy is preserved quite well even with 3-bit and 4-bit precision,
    when compared to the original fp16 accuracy.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了我们权重量化方法在 MoE 模型上的性能，并在表 [6](#S4.T6 "表 6 ‣ 4.3 MoE 模型性能结果 ‣ 4 实验 ‣ FineQuant:
    通过细粒度权重量化解锁 LLMs 的效率") 中报告了结果。我们研究了不同量化精度的影响，范围从 8 位到 3 位。由于 MoE FFN 层的鲁棒性，与原始
    fp16 精度相比，即使在 3 位和 4 位精度下，模型的准确度也得到了较好的保留。'
- en: 'Table 6: Accuracy of MoE models with quantization. Speed-up comparison is presented
    in Figure [5](#S4.F5 "Figure 5 ‣ 4.3 MoE model performance results ‣ 4 Experiments
    ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for
    LLMs"). The optimized kernels are implemented for 8-bit and 4-bit precisions.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6：MoE 模型量化的准确度。加速比较见图 [5](#S4.F5 "图 5 ‣ 4.3 MoE 模型性能结果 ‣ 4 实验 ‣ FineQuant:
    通过细粒度权重量化解锁 LLMs 的效率")。优化的内核已针对 8 位和 4 位精度实现。'
- en: '| Model type | Precision | BLEU ($\Delta$ BLEU compared to fp16) | Size (X
    times compared to fp16) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 模型类型 | 精度 | BLEU（$\Delta$ BLEU 与 fp16 的比较） | 尺寸（与 fp16 比较的倍数） |'
- en: '| MoE 5.3B | fp16 | 46.35 (0.0) | 1.00X |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| MoE 5.3B | fp16 | 46.35 (0.0) | 1.00X |'
- en: '| int8 | 46.34 (-0.01) | 0.55X |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| int8 | 46.34 (-0.01) | 0.55X |'
- en: '| int4 | 46.18 (-0.17) | 0.32X |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| int4 | 46.18 (-0.17) | 0.32X |'
- en: '| int3 | 46.01 (-0.34) | 0.26X |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| int3 | 46.01 (-0.34) | 0.26X |'
- en: 'Figure [5](#S4.F5 "Figure 5 ‣ 4.3 MoE model performance results ‣ 4 Experiments
    ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for
    LLMs") shows the end-to-end speed improvements with various batch size with 8-bit
    and 4-bit quantization.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#S4.F5 "Figure 5 ‣ 4.3 MoE model performance results ‣ 4 Experiments ‣
    FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for
    LLMs") 显示了在不同批量大小下，8位和4位量化的端到端速度改进。'
- en: (1,1)(1,2)(8,1)(8,2)(20,1)(20,2)(32,1)(32,2)(64,1)(64,2)(96,1)(96,2)$1$(Batch
    Size, Beam Width)Speedup over FP16INT8INT4
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: (1,1)(1,2)(8,1)(8,2)(20,1)(20,2)(32,1)(32,2)(64,1)(64,2)(96,1)(96,2)$1$(Batch
    Size, Beam Width)Speedup over FP16INT8INT4
- en: 'Figure 5: MoE model speed-up with quantization methods.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：MoE 模型在量化方法下的加速。
- en: 5 Conclusions and Limitations
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与局限性
- en: This paper presents a method for accelerating large language models through
    the use of low-bit quantization. The proposed weight-only quantization technique
    demonstrates promising results in compressing very large models with up to 175
    billion parameters, while still maintaining accuracy. To address the issue of
    outliers affecting the quantized weight distribution, fine-grained quantization
    is employed.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种通过低位量化加速大型语言模型的方法。所提的仅权重量化技术在压缩高达1750亿参数的非常大型模型时显示出有前景的结果，同时保持了准确性。为了应对离群值影响量化权重分布的问题，采用了细粒度量化。
- en: Despite its strengths, the study does have a few limitations. Firstly, optimized
    GPU kernels are only implemented for group size 64\. However, we plan to expand
    support for any power of 2 group size greater than 16\. Secondly, the performance
    benchmarking is conducted solely on A100 GPUs, so the speed improvements may vary
    on different GPU architectures. Lastly, the proposed method does not leverage
    integer instructions even when they are available. These limitations suggest potential
    directions for future research.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管研究具有优势，但仍存在一些局限性。首先，优化的 GPU 内核仅对组大小 64 实现。然而，我们计划扩展对任何大于 16 的 2 的幂组大小的支持。其次，性能基准测试仅在
    A100 GPU 上进行，因此在不同的 GPU 架构上，速度改进可能会有所不同。最后，所提方法即使在有整数指令可用的情况下也未利用这些指令。这些局限性提示了未来研究的潜在方向。
- en: One particularly promising avenue for future work involves exploring the accuracy
    and efficiency of using int8 activations and int4 weights with integer scales
    for fine-grained quantization. This approach has the potential to further enhance
    the efficiency of the models.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一条特别有前景的未来工作方向是探索使用 int8 激活和 int4 权重与整数尺度进行细粒度量化的准确性和效率。这种方法有潜力进一步提高模型的效率。
- en: References
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Aji and Heafield (2020) Alham Fikri Aji and Kenneth Heafield. 2020. Compressing
    neural machine translation models with 4-bit precision. In *NGT*.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aji 和 Heafield（2020）Alham Fikri Aji 和 Kenneth Heafield。2020。用4位精度压缩神经机器翻译模型。在
    *NGT*。
- en: 'Baines et al. (2021) Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Naman
    Goyal, Siddharth Goyal, Myle Ott, Benjamin Lefaudeux, Vitaliy Liptchinsky, Mike
    Rabbat, Sam Sheiffer, et al. 2021. Fairscale: A general purpose modular pytorch
    library for high performance and large scale training.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baines 等人（2021）Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Naman Goyal,
    Siddharth Goyal, Myle Ott, Benjamin Lefaudeux, Vitaliy Liptchinsky, Mike Rabbat,
    Sam Sheiffer 等人。2021。Fairscale：一个用于高性能和大规模训练的通用模块化 PyTorch 库。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等人。2020。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901。
- en: Choukroun et al. (2019) Yoni Choukroun, Eli Kravchik, and Pavel Kisilev. 2019.
    Low-bit quantization of neural networks for efficient inference. *2019 IEEE/CVF
    International Conference on Computer Vision Workshop (ICCVW)*, pages 3009–3018.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choukroun 等人（2019）Yoni Choukroun, Eli Kravchik, 和 Pavel Kisilev。2019。神经网络的低位量化用于高效推理。*2019
    IEEE/CVF 国际计算机视觉会议研讨会（ICCVW）*，页码 3009–3018。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery 等人（2022）Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann 等人。2022。Palm：通过路径扩展语言建模。*arXiv 预印本 arXiv:2204.02311*。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Llm.int8(): 8-bit matrix multiplication for transformers at scale. *ArXiv*,
    abs/2208.07339.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2022）Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    2022. Llm.int8(): 8-bit matrix multiplication for transformers at scale. *ArXiv*,
    abs/2208.07339。'
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等（2018）Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2018. Bert: Pre-training of deep bidirectional transformers for language understanding.
    *arXiv 预印本 arXiv:1810.04805*。'
- en: Fan et al. (2021) Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave,
    Rémi Gribonval, Hervé Jégou, and Armand Joulin. 2021. Training with quantization
    noise for extreme model compression. *ArXiv*, abs/2004.07320.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等（2021）Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi Gribonval,
    Hervé Jégou, 和 Armand Joulin. 2021. Training with quantization noise for extreme
    model compression. *ArXiv*, abs/2004.07320。
- en: 'Fedus et al. (2021) William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch
    transformers: Scaling to trillion parameter models with simple and efficient sparsity.
    *arXiv preprint arXiv:2101.03961*.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fedus 等（2021）William Fedus, Barret Zoph, 和 Noam Shazeer. 2021. Switch transformers:
    Scaling to trillion parameter models with simple and efficient sparsity. *arXiv
    预印本 arXiv:2101.03961*。'
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等（2022）Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh.
    2022. Gptq: Accurate post-training quantization for generative pre-trained transformers.
    *arXiv 预印本 arXiv:2210.17323*。'
- en: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. 2021. [A framework for few-shot language model evaluation](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2021）Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi,
    Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, 和 Andy
    Zou. 2021. [一个用于少量样本语言模型评估的框架](https://doi.org/10.5281/zenodo.5371628)。
- en: Gholami et al. (2022) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W.
    Mahoney, and Kurt Keutzer. 2022. A survey of quantization methods for efficient
    neural network inference. *ArXiv*, abs/2103.13630.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gholami 等（2022）Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney,
    和 Kurt Keutzer. 2022. A survey of quantization methods for efficient neural network
    inference. *ArXiv*, abs/2103.13630。
- en: Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language
    models. *arXiv preprint arXiv:2203.15556*.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann 等（2022）Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
    Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
    Welbl, Aidan Clark, 等. 2022. Training compute-optimal large language models. *arXiv
    预印本 arXiv:2203.15556*。
- en: 'Iyer et al. (2022) Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor
    Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh
    Koura, et al. 2022. Opt-iml: Scaling language model instruction meta learning
    through the lens of generalization. *arXiv preprint arXiv:2212.12017*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Iyer 等（2022）Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov,
    Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura,
    等. 2022. Opt-iml: Scaling language model instruction meta learning through the
    lens of generalization. *arXiv 预印本 arXiv:2212.12017*。'
- en: Ke et al. (2021) Guolin Ke, Di He, and Tie-Yan Liu. 2021. Rethinking positional
    encoding in language pre-training. *ArXiv*, abs/2006.15595.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke 等（2021）Guolin Ke, Di He, 和 Tie-Yan Liu. 2021. Rethinking positional encoding
    in language pre-training. *ArXiv*, abs/2006.15595。
- en: 'Kim and Awadalla (2020) Young Jin Kim and Hany Hassan Awadalla. 2020. Fastformers:
    Highly efficient transformer models for natural language understanding. *arXiv
    preprint arXiv:2010.13382*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 和 Awadalla（2020）Young Jin Kim 和 Hany Hassan Awadalla. 2020. Fastformers:
    Highly efficient transformer models for natural language understanding. *arXiv
    预印本 arXiv:2010.13382*。'
- en: Kim et al. (2021) Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz
    Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan
    Awadalla. 2021. Scalable and efficient moe training for multitask multilingual
    models. *arXiv preprint arXiv:2109.10465*.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2021）Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz
    Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, 和 Hany Hassan Awadalla.
    2021. Scalable and efficient moe training for multitask multilingual models. *arXiv
    预印本 arXiv:2109.10465*。
- en: 'Kim et al. (2022) Young Jin Kim, Rawn Henry, Raffy Fahim, and Hany Hassan Awadalla.
    2022. Who says elephants can’t run: Bringing large scale moe models into cloud
    scale production. *arXiv preprint arXiv:2211.10017*.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 (2022) Young Jin Kim, Rawn Henry, Raffy Fahim, 和 Hany Hassan Awadalla.
    2022. 谁说大象不能跑：将大规模 Moe 模型引入云规模生产。*arXiv 预印本 arXiv:2211.10017*。
- en: 'Kim et al. (2019) Young Jin Kim, Marcin Junczys-Dowmunt, Hany Hassan, Alham Fikri
    Aji, Kenneth Heafield, Roman Grundkiewicz, and Nikolay Bogoychev. 2019. From research
    to production and back: Ludicrously fast neural machine translation. In *Proceedings
    of the 3rd Workshop on Neural Generation and Translation*, pages 280–288.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 (2019) Young Jin Kim, Marcin Junczys-Dowmunt, Hany Hassan, Alham Fikri
    Aji, Kenneth Heafield, Roman Grundkiewicz, 和 Nikolay Bogoychev. 2019. 从研究到生产再到回归：极其快速的神经机器翻译。在
    *第3届神经生成与翻译研讨会论文集*，第 280–288 页。
- en: 'Lepikhin et al. (2020) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao
    Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
    2020. Gshard: Scaling giant models with conditional computation and automatic
    sharding. *arXiv preprint arXiv:2006.16668*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lepikhin 等 (2020) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen,
    Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, 和 Zhifeng Chen. 2020.
    Gshard: 通过条件计算和自动分片扩展巨型模型。*arXiv 预印本 arXiv:2006.16668*。'
- en: 'Liu et al. (2022) Rui Liu, Young Jin Kim, Alexandre Muzio, and Hany Hassan.
    2022. Gating dropout: Communication-efficient regularization for sparsely activated
    transformers. In *International Conference on Machine Learning*, pages 13782–13792\.
    PMLR.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2022) Rui Liu, Young Jin Kim, Alexandre Muzio, 和 Hany Hassan. 2022.
    Gating dropout: 通信高效的稀疏激活变换器正则化。在 *国际机器学习会议*，第 13782–13792 页。PMLR。'
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
    Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi
    Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, 和 Veselin Stoyanov. 2019. Roberta:
    一种稳健优化的 BERT 预训练方法。*arXiv 预印本 arXiv:1907.11692*。'
- en: 'Park et al. (2022) Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim,
    Youngjoo Lee, and Dongsoo Lee. 2022. nuqmm: Quantized matmul for efficient inference
    of large-scale generative language models. *arXiv preprint arXiv:2206.09557*.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park 等 (2022) Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo
    Lee, 和 Dongsoo Lee. 2022. nuqmm: 量化矩阵乘法用于大规模生成语言模型的高效推理。*arXiv 预印本 arXiv:2206.09557*。'
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. 2018. Improving language understanding by generative pre-training.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever
    等. 2018. 通过生成预训练提高语言理解能力。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever 等. 2019. 语言模型是无监督的多任务学习者。*OpenAI 博客*, 1(8):9。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *J. Mach.
    Learn. Res.*, 21(140):1–67.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等 (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu 等. 2020. 通过统一的文本到文本变换器探索迁移学习的极限。*J.
    Mach. Learn. Res.*, 21(140):1–67。
- en: 'Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
    Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning
    models with over 100 billion parameters. *Proceedings of the 26th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rasley 等 (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, 和 Yuxiong
    He. 2020. Deepspeed: 系统优化使得训练超过 1000 亿参数的深度学习模型成为可能。*第26届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*。'
- en: 'Ren et al. (2021) Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji
    Ruwase, Shuangyang Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. Zero-offload:
    Democratizing billion-scale model training. In *USENIX Annual Technical Conference*.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren 等 (2021) Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji
    Ruwase, Shuangyang Yang, Minjia Zhang, Dong Li, 和 Yuxiong He. 2021. Zero-offload:
    民主化亿级模型训练。在 *USENIX 年度技术会议*。'
- en: Rodriguez et al. (2018) Andres Rodriguez, Eden Segal, Etay Meiri, Evarist Fomenko,
    Young Jin Kim, Haihao Shen, and Barukh Ziv. 2018. Lower numerical precision deep
    learning inference and training.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rodriguez 等 (2018) Andres Rodriguez, Eden Segal, Etay Meiri, Evarist Fomenko,
    Young Jin Kim, Haihao Shen, 和 Barukh Ziv. 2018. 较低数值精度的深度学习推理和训练。
- en: 'Shazeer et al. (2018) Noam M. Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran,
    Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng
    Hong, Cliff Young, Ryan Sepassi, and Blake A. Hechtman. 2018. Mesh-tensorflow:
    Deep learning for supercomputers. *ArXiv*, abs/1811.02084.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shazeer 等（2018）Noam M. Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish
    Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong,
    Cliff Young, Ryan Sepassi 和 Blake A. Hechtman. 2018. Mesh-tensorflow: 超级计算机的深度学习。*ArXiv*,
    abs/1811.02084。'
- en: 'Stock et al. (2019) Pierre Stock, Armand Joulin, Rémi Gribonval, Benjamin Graham,
    and Hervé Jégou. 2019. And the bit goes down: Revisiting the quantization of neural
    networks. *arXiv preprint arXiv:1907.05686*.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stock 等（2019）Pierre Stock, Armand Joulin, Rémi Gribonval, Benjamin Graham 和
    Hervé Jégou. 2019. 位下降：重新审视神经网络的量化。*arXiv 预印本 arXiv:1907.05686*。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *NIPS*.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017）Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser 和 Illia Polosukhin. 2017. 注意力机制就是你所需要的。发表于
    *NIPS*。
- en: 'Xiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth,
    and Song Han. 2022. Smoothquant: Accurate and efficient post-training quantization
    for large language models. *arXiv preprint arXiv:2211.10438*.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等（2022）Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth 和 Song Han.
    2022. Smoothquant: 准确且高效的大型语言模型后训练量化。*arXiv 预印本 arXiv:2211.10438*。'
- en: Xiong et al. (2020) Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng,
    Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020. On layer
    normalization in the transformer architecture. In *ICML*.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong 等（2020）Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen
    Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang 和 Tie-Yan Liu. 2020. 变换器架构中的层归一化。发表于
    *ICML*。
- en: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. 2022. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. *ArXiv*, abs/2206.01861.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等（2022）Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li 和 Yuxiong He. 2022. Zeroquant: 高效且经济的大规模变换器后训练量化。*ArXiv*, abs/2206.01861。'
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. 2022. Opt: Open pre-trained transformer language models. *arXiv preprint
    arXiv:2205.01068*.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2022）Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin 等。2022.
    Opt: 开放的预训练变换器语言模型。*arXiv 预印本 arXiv:2205.01068*。'
- en: Appendix A Quantization method formulation
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 量化方法公式
- en: 'Linear quantization with absolute maximum. We used linear quantization with
    absolute maximum as the main method. Given a matrix ${\bm{A}}$ as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对最大线性量化。我们将绝对最大线性量化作为主要方法。给定一个矩阵 ${\bm{A}}$ 如下：
- en: $\displaystyle{\bm{s}}_{j}=\frac{2\times\max(|{\bm{A}}_{:,j}|)}{2^{b}-1}$
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: $\displaystyle{\bm{s}}_{j}=\frac{2\times\max(|{\bm{A}}_{:,j}|)}{2^{b}-1}$
- en: $\displaystyle{\bm{Q}}_{:,j}=\operatorname{int}(\frac{{\bm{A}}_{:,j}}{{\bm{s}}_{j}})$
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: $\displaystyle{\bm{Q}}_{:,j}=\operatorname{int}(\frac{{\bm{A}}_{:,j}}{{\bm{s}}_{j}})$
- en: 'Here, $s$ as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$s$ 如下：
- en: $\displaystyle{\bm{A}}^{{}^{\prime}}{:,j}={\bm{Q}}{:,j}\times{\bm{s}}_{j}$
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: $\displaystyle{\bm{A}}^{{}^{\prime}}{:,j}={\bm{Q}}{:,j}\times{\bm{s}}_{j}$
- en: 'Log-scale quantization. Another quantization method we experimented is log-scale
    quantization where $1$, the quantization formula is as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对数尺度量化。我们实验的另一种量化方法是对数尺度量化，其中 $1$，量化公式如下：
- en: $\displaystyle{\bm{P}}=sign({{\bm{A}}})$
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: $\displaystyle{\bm{P}}=sign({{\bm{A}}})$
- en: $\displaystyle{\bm{T}}=clip(\frac{|{\bm{A}}|}{s},1,2^{1-2^{b-1})}$
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: $\displaystyle{\bm{T}}=clip(\frac{|{\bm{A}}|}{s},1,2^{1-2^{b-1}})$
- en: $\displaystyle{\bm{Q}}=\lceil log_{2}(\frac{2}{3}{\bm{T}})\rceil$
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: $\displaystyle{\bm{Q}}=\lceil \log_{2}(\frac{2}{3}{\bm{T}})\rceil$
- en: 'where $s$ can be chosen in two ways, either (i) the absolute maximum or (ii)
    the optimal value to minimize the mean squared error (MSE) between the quantized
    and original values which is described in Aji and Heafield ([2020](#bib.bib1)).
    We use the second algorithm which we observe a better accuracy with the quantization.
    At inference time, the quantized weight values are dequantized based on the formula
    as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s$ 可以通过两种方式选择，(i) 绝对最大值或 (ii) 最优值以最小化量化值和原始值之间的均方误差（MSE），这在 Aji 和 Heafield
    ([2020](#bib.bib1)) 中有所描述。我们使用第二种算法，这种算法在量化中观察到更好的准确性。在推理时，量化的权重值将根据如下公式反量化：
- en: $\displaystyle{\bm{A}}^{{}^{\prime}}={\bm{P}}\times s\times 2^{\bm{Q}}$
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: $\displaystyle{\bm{A}}^{{}^{\prime}}={\bm{P}}\times s\times 2^{\bm{Q}}$
- en: 'Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Quantization methodology: basic settings
    ‣ 3 Designing Quantization Methods for LLMs - Adaptive Fine-grained Quantization
    ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for
    LLMs") shows the performance comparison of two quantization methods.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1](#S3.F1 "图 1 ‣ 3.1 量化方法论：基本设置 ‣ 3 设计LLM的量化方法 - 自适应细粒度量化 ‣ FineQuant：通过细粒度权重唯一量化解锁LLM的效率")展示了两种量化方法的性能比较。
- en: Appendix B Channel-wise vs matrix-wise quantization
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 通道级与矩阵级量化
- en: 'Scaling factors are calculated by the quantization algorithm and stored in
    half precision floating-point (fp16) numbers to dequantize the matrices with.
    These factors can be chosen on the channel scale or the whole matrix scale. As
    shown in figure [6](#A2.F6 "Figure 6 ‣ Appendix B Channel-wise vs matrix-wise
    quantization ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization
    for LLMs"), channel-wise quantization gives quite higher scores than tensor-wise
    especially for low precision. Additional parameters to store channel-wise scaling
    factors is small, because only one value is needed for a channel and less than
    1% of total parameters in a matrix. Therefore, we use channel-wise quantization
    for all the quantization experiments.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放因子由量化算法计算，并以半精度浮点数（fp16）存储，用于解量化矩阵。这些因子可以选择在通道尺度或整个矩阵尺度上。如图[6](#A2.F6 "图 6
    ‣ 附录B 通道级与矩阵级量化 ‣ FineQuant：通过细粒度权重唯一量化解锁LLM的效率")所示，通道级量化特别是在低精度下比张量级量化给出更高的分数。存储通道级缩放因子的额外参数很小，因为每个通道只需一个值，占矩阵总参数的不到1%。因此，我们在所有量化实验中使用通道级量化。
- en: '![Refer to caption](img/1f17911dd007ac2610cdf87575ca22d0.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1f17911dd007ac2610cdf87575ca22d0.png)'
- en: 'Figure 6: Linear quantization of expert FFNs with channel-wise and matrix-wise
    scaling factors.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：具有通道级和矩阵级缩放因子的专家FFNs的线性量化。
- en: Appendix C Quantization of different layers in a dense model
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 密集模型中不同层的量化
- en: 'For the comparison with MoE models which alternate different block types which
    are an expert block and a dense block, we consider quantizing only half of the
    dense transformer blocks’ FFNs, because we quantize expert weights only on MoE
    models which exist only in every other block (even numbered). We compare three
    different configurations - (1) quantizing even numbered blocks’ FFNs only, (2)
    quantizing odd numbered blocks’ FFNs only and (3) quantizing all FFN layers. As
    can be seen in Figure [7](#A3.F7 "Figure 7 ‣ Appendix C Quantization of different
    layers in a dense model ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs"), quantizing even numbered blocks’ FFNs affects the accuracy
    the least, and quantizing all FFN layers give the worst result. Based on this
    experiment, we quantize only even numbered transformer blocks’ FFNs for the dense
    model in all the experiments and comparisons.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与交替不同块类型的MoE模型进行比较，我们考虑只量化一半密集Transformer块的FFNs，因为我们只在每隔一个块（偶数块）中量化专家权重。我们比较了三种不同的配置
    - （1）仅量化偶数块的FFNs，（2）仅量化奇数块的FFNs，以及（3）量化所有FFN层。如图[7](#A3.F7 "图 7 ‣ 附录C 密集模型中不同层的量化
    ‣ FineQuant：通过细粒度权重唯一量化解锁LLM的效率")所示，量化偶数块的FFNs对准确性的影响最小，而量化所有FFN层给出的结果最差。根据这个实验，我们在所有实验和比较中仅量化偶数块的Transformer
    FFNs。
- en: '![Refer to caption](img/a34a1a99b55626fdaa72b75d284b6e45.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a34a1a99b55626fdaa72b75d284b6e45.png)'
- en: 'Figure 7: Quantization impact of different layers in a dense model.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：密集模型中不同层的量化影响。
- en: Appendix D Skewness of weight matrices in MoE and dense models
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D MoE和密集模型中权重矩阵的偏斜度
- en: 'In the analysis of model weight distribution in Section [3](#S3 "3 Designing
    Quantization Methods for LLMs - Adaptive Fine-grained Quantization ‣ FineQuant:
    Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs"), we
    observe that dense models’ FFN layers tend to have more outliers than MoEs’ expert
    FFN layers. We measure the skewness of weight distribution of those in Table [7](#A4.T7
    "Table 7 ‣ Appendix D Skewness of weight matrices in MoE and dense models ‣ FineQuant:
    Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs").'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [3](#S3 "3 设计 LLM 的量化方法 - 自适应细粒度量化 ‣ FineQuant：通过细粒度权重量化解锁 LLM 的效率") 节对模型权重分布的分析中，我们观察到密集模型的
    FFN 层往往比 MoE 的专家 FFN 层有更多的异常值。我们在表 [7](#A4.T7 "表 7 ‣ 附录 D MoE 和密集模型中权重矩阵的偏斜度 ‣
    FineQuant：通过细粒度权重量化解锁 LLM 的效率") 中测量了这些层的权重分布偏斜度。
- en: 'Table 7: Expert vs non-expert FFN layers parameters distribution skewness'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：专家与非专家 FFN 层参数分布偏斜度
- en: '| Parameter | skew |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 偏斜 |'
- en: '| encoder expert 15 FFN fc1 layer 0 | -0.002 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 编码器专家 15 FFN fc1 层 0 | -0.002 |'
- en: '| encoder expert 15 FFN fc2 layer 0 | -0.190 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 编码器专家 15 FFN fc2 层 0 | -0.190 |'
- en: '| encoder expert 15 FFN fc1 layer 6 | -0.002 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 编码器专家 15 FFN fc1 层 6 | -0.002 |'
- en: '| encoder expert 15 FFN fc2 layer 6 | -0.002 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 编码器专家 15 FFN fc2 层 6 | -0.002 |'
- en: '| encoder non-expert FFN fc1 layer 1 | -0.019 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 编码器非专家 FFN fc1 层 1 | -0.019 |'
- en: '| encoder non-expert FFN fc2 layer 1 | -10.729 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 编码器非专家 FFN fc2 层 1 | -10.729 |'
- en: '| encoder non-expert FFN fc1 layer 7 | 0.003 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 编码器非专家 FFN fc1 层 7 | 0.003 |'
- en: '| encoder non-expert FFN fc2 layer 7 | -1.574 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 编码器非专家 FFN fc2 层 7 | -1.574 |'
- en: '| encoder expert FFN fc1 mean | 0.00 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 编码器专家 FFN fc1 平均值 | 0.00 |'
- en: '| encoder expert FFN fc2 mean | -0.63 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 编码器专家 FFN fc2 平均值 | -0.63 |'
- en: '| decoder expert FFN fc1 mean | 0.00 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 解码器专家 FFN fc1 平均值 | 0.00 |'
- en: '| decoder expert FFN fc2 mean | 0.48 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 解码器专家 FFN fc2 平均值 | 0.48 |'
- en: '| encoder non-expert FFN fc1 mean | 0.00 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 编码器非专家 FFN fc1 平均值 | 0.00 |'
- en: '| encoder non-expert FFN fc2 mean | -1.84 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 编码器非专家 FFN fc2 平均值 | -1.84 |'
- en: '| decoder non-expert FFN fc1 mean | 0.00 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 解码器非专家 FFN fc1 平均值 | 0.00 |'
- en: '| decoder non-expert FFN fc2 mean | -0.09 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 解码器非专家 FFN fc2 平均值 | -0.09 |'
- en: Appendix E Machine translation dataset summary
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 机器翻译数据集总结
- en: 'Table [8](#A5.T8 "Table 8 ‣ Appendix E Machine translation dataset summary
    ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for
    LLMs") shows the number of parallel sentences used to train dense and MoE models.
    All languages have at least 300 million sentences and the differences in the number
    among languages are less than two times.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [8](#A5.T8 "表 8 ‣ 附录 E 机器翻译数据集总结 ‣ FineQuant：通过细粒度权重量化解锁 LLM 的效率") 显示了用于训练密集和
    MoE 模型的平行句子的数量。所有语言至少有 3 亿个句子，各语言之间的差异不到两倍。
- en: 'Table 8: The number of parallel sentences including backtranslation data.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：包括回译数据的平行句子数量。
- en: '| Language | Number of parallel sentences (million) |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 语言 | 平行句子数量（百万） |'
- en: '| xx $\rightarrow$ xx |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| xx $\rightarrow$ xx |'
- en: '| DE (German) | 505 | 411 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| DE（德语） | 505 | 411 |'
- en: '| ES (Spanish) | 448 | 407 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| ES（西班牙语） | 448 | 407 |'
- en: '| FR (French) | 448 | 376 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| FR（法语） | 448 | 376 |'
- en: '| IT (Italian) | 447 | 303 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| IT（意大利语） | 447 | 303 |'
- en: '| NL (Dutch) | 302 | 378 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| NL（荷兰语） | 302 | 378 |'
- en: Appendix F Robustness comparison between MoE and dense models
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F MoE 与密集模型之间的鲁棒性比较
- en: 'We compared robustness against low-bit quantization between MoE and dense models
    using the post-training quantization without any QAT. For the dense model, quantization
    with different bits was applied to the even numbered FFN layers. Appendix [C](#A3
    "Appendix C Quantization of different layers in a dense model ‣ FineQuant: Unlocking
    Efficiency with Fine-Grained Weight-Only Quantization for LLMs") shows this is
    the best layer selection for the dense model. We used two different datasets to
    verify the proposed quantization method works in different model settings.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用后训练量化（没有任何 QAT）比较了 MoE 和密集模型在低位量化下的鲁棒性。对于密集模型，应用了不同位数的量化到偶数编号的 FFN 层。附录
    [C](#A3 "附录 C 密集模型中不同层的量化 ‣ FineQuant：通过细粒度权重量化解锁 LLM 的效率") 显示了这是密集模型的最佳层选择。我们使用了两个不同的数据集来验证提出的量化方法在不同模型设置中的效果。
- en: '![Refer to caption](img/724af0ee50c5821cc22eae98913f1149.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/724af0ee50c5821cc22eae98913f1149.png)'
- en: 'Figure 8: Quantization performance comparison between MoE and dense models.
    10 different language pair scores are averaged.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：MoE 与密集模型之间量化性能比较。10 种不同语言对的得分进行了平均。
- en: 'Figure [8](#A6.F8 "Figure 8 ‣ Appendix F Robustness comparison between MoE
    and dense models ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs") presents the experiment with the model trained with the
    larger dataset. It shows the average BLEU scores with different quantization precision
    for both MoE and dense models. The MoE model can maintain accuracy within -0.3
    down to 3-bit and -1.82 for 2-bit. On the other hand, the dense model can preserve
    the accuracy only down to 4-bit, but starts to lose significant accuracy more
    than 2 BLEU scores when it goes down to 3-bits. In case of 2-bits, dense model
    loses most of capability by -42.96 BLEU scores.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图[8](#A6.F8 "图 8 ‣ 附录 F MoE 和密集模型的鲁棒性比较 ‣ FineQuant：通过细粒度权重量化解锁 LLMs 的效率") 展示了使用更大数据集训练的模型的实验结果。它显示了不同量化精度下
    MoE 和密集模型的平均 BLEU 分数。MoE 模型在 3-bit 下能保持精度在 -0.3 以内，在 2-bit 下为 -1.82。另一方面，密集模型仅能保持到
    4-bit 的精度，但当降到 3-bit 时，准确度显著下降超过 2 个 BLEU 分数。在 2-bit 的情况下，密集模型的能力几乎完全丧失，BLEU 分数下降了
    -42.96。
- en: 'Figure [9](#A6.F9 "Figure 9 ‣ Appendix F Robustness comparison between MoE
    and dense models ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs") presents the experiment with the model trained with the
    smaller dataset. In this setting, each individual expert is smaller, but there
    are 4 times more experts in one MoE layer. And, they are trained with smaller
    dataset, so they do not have equivalent knowledge as the previous model trained
    on the larger dataset. As can be seen in the Figure, the quantization performance
    shows a similar pattern. The MoE model preserves accuracy even when it is quantized
    to 2 or 3 bits. However, dense model quickly loses the performance when it is
    quantized down to lower than 4-bit. Again, the MoE model is much more robust to
    quantization than the dense model.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](#A6.F9 "图 9 ‣ 附录 F MoE 和密集模型的鲁棒性比较 ‣ FineQuant：通过细粒度权重量化解锁 LLMs 的效率") 展示了使用较小数据集训练的模型的实验结果。在这种情况下，每个专家更小，但一个
    MoE 层中的专家数量增加了 4 倍。而且，它们在较小的数据集上进行训练，因此没有与先前在更大数据集上训练的模型相当的知识。从图中可以看出，量化性能表现出类似的模式。即使在
    2 或 3 bit 的量化下，MoE 模型也能保持准确性。然而，当密集模型量化到低于 4-bit 时，性能迅速下降。再次证明，MoE 模型比密集模型对量化的鲁棒性更强。
- en: '![Refer to caption](img/fe952d67113693032b50d4f56ec8fa10.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fe952d67113693032b50d4f56ec8fa10.png)'
- en: 'Figure 9: Quantization performance comparison between MoE and dense models.
    20 different WMT language pairs are averaged.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：MoE 和密集模型的量化性能比较。20 对不同的 WMT 语言对进行了平均。
- en: F.1 Robustness of MoE FFN layers to quantization
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 MoE FFN 层对量化的鲁棒性
- en: 'For MoE models, we also conducted a set of experiments with various quantization
    bits. We divide an MoE model into four parts: (i) expert FFNs, (ii) dense FFN
    layers, (iii) self-attention layers and (iv) cross-attention layers.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MoE 模型，我们还进行了各种量化位的实验。我们将 MoE 模型划分为四个部分：(i) 专家 FFNs，(ii) 密集 FFN 层，(iii) 自注意力层和
    (iv) 交叉注意力层。
- en: 'Figure [10](#A6.F10 "Figure 10 ‣ F.1 Robustness of MoE FFN layers to quantization
    ‣ Appendix F Robustness comparison between MoE and dense models ‣ FineQuant: Unlocking
    Efficiency with Fine-Grained Weight-Only Quantization for LLMs") shows the evaluation
    BLEU scores when different parts of the MoE model are quantized. It is observed
    that quantizing expert FFN layers to 2-bit does not significantly impact the overall
    model quality. However, quantizing other parts of the model into 2-bit significantly
    hurts the output quality. Quantized cross-attention and self-attention blocks
    can still maintain the quality with 3-bit quantization, but their performance
    gets impacted with 2-bit quantization. On the other hand, dense FFN layers are
    significantly impacted by lower bit quantization of 2-bit and 3-bit. With 3-bit
    quantization, the model score drops by 23 % of the original score, and 2-bit quantization
    on dense FFN layers gives almost zero score. The same study is also included on
    a dense model in Appendix [C](#A3 "Appendix C Quantization of different layers
    in a dense model ‣ FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only
    Quantization for LLMs"), and a similar pattern with 2 and 3 bit quantization is
    observed.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '图[10](#A6.F10 "图 10 ‣ F.1 MoE FFN 层对量化的鲁棒性 ‣ 附录 F MoE 和密集模型的鲁棒性比较 ‣ FineQuant:
    利用细粒度权重量化解锁 LLM 的效率") 显示了在对 MoE 模型的不同部分进行量化时的 BLEU 评分评估。观察到将专家 FFN 层量化为 2 位不会显著影响整体模型质量。然而，将模型的其他部分量化为
    2 位会显著降低输出质量。量化的交叉注意力和自注意力模块在 3 位量化下仍能保持质量，但在 2 位量化下其性能受到影响。另一方面，密集 FFN 层在 2 位和
    3 位的低位量化下会受到显著影响。3 位量化时，模型评分下降至原始评分的 23%，而对密集 FFN 层进行 2 位量化几乎会得到零评分。附录[C](#A3
    "附录 C 密集模型中不同层的量化 ‣ FineQuant: 利用细粒度权重量化解锁 LLM 的效率")中也包含了对密集模型的相同研究，观察到与 2 位和
    3 位量化相似的模式。'
- en: '![Refer to caption](img/9a1c433a895f1dddf604146ccdaedddb.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9a1c433a895f1dddf604146ccdaedddb.png)'
- en: 'Figure 10: Quantization impact on different MoE model parts (channel-wise linear
    quantiztation without any additional training).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：不同 MoE 模型部分的量化影响（通道级线性量化，无需额外训练）。
