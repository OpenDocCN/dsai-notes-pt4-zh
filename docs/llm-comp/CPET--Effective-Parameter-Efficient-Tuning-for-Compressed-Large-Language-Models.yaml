- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:03:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:03:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2307.07705](https://ar5iv.labs.arxiv.org/html/2307.07705)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2307.07705](https://ar5iv.labs.arxiv.org/html/2307.07705)
- en: Weilin Zhao  , Yuxiang Huang^∗, Xu Han, Zhiyuan Liu
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Weilin Zhao  , Yuxiang Huang^∗, Xu Han, Zhiyuan Liu
- en: Zhengyan Zhang, Maosong Sun NLP Group, DCST, IAI, BNRIST, Tsinghua University,
    Beijing
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zhengyan Zhang，Maosong Sun NLP组，DCST，IAI，BNRIST，清华大学，北京
- en: '{zwl23,huang-yx21,zy-z19}@mails.tsinghua.edu.cn'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{zwl23,huang-yx21,zy-z19}@mails.tsinghua.edu.cn'
- en: '{hanxu2022,liuzy,sms}@tsinghua.edu.cn  indicates equal contribution.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{hanxu2022,liuzy,sms}@tsinghua.edu.cn  indicates equal contribution.'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Abstract
- en: Parameter-efficient tuning (PET) has been widely explored in recent years because
    it tunes much fewer parameters than full-parameter fine-tuning (FT) while still
    stimulating sufficient knowledge from large language models (LLMs) for downstream
    tasks. Moreover, when PET is employed to serve multiple tasks, different task-specific
    PET modules can be built on a frozen LLM, avoiding redundant LLM deployments.
    Although PET significantly reduces the cost of tuning and deploying LLMs, its
    inference still suffers from the computational bottleneck of LLMs. To address
    the above issue, we propose an effective PET framework based on compressed LLMs,
    named “CPET”. In CPET, we evaluate the impact of mainstream LLM compression techniques
    and then introduce knowledge inheritance and recovery strategies to restore the
    knowledge loss caused by these compression techniques. Our experimental results
    demonstrate that, owing to the restoring strategies of CPET, collaborating task-specific
    PET modules with a compressed LLM can achieve comparable performance to collaborating
    PET modules with the non-compressed LLM and outperform directly applying vanilla
    PET methods to the compressed LLM.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效调优（PET）近年来得到广泛探索，因为它调优的参数比全参数微调（FT）少得多，同时仍能从大型语言模型（LLMs）中提取足够的知识用于下游任务。此外，当PET用于服务多个任务时，可以在冻结的LLM上构建不同任务特定的PET模块，从而避免冗余的LLM部署。尽管PET显著减少了调优和部署LLMs的成本，但其推理仍然受到LLMs计算瓶颈的影响。为了解决上述问题，我们提出了一种基于压缩LLMs的有效PET框架，名为“CPET”。在CPET中，我们评估了主流LLM压缩技术的影响，然后引入知识继承和恢复策略以恢复这些压缩技术造成的知识损失。我们的实验结果表明，由于CPET的恢复策略，与压缩LLM协作的任务特定PET模块能够实现与与非压缩LLM协作的PET模块相媲美的性能，并且优于直接将原始PET方法应用于压缩LLM。
- en: 'CPET: Effective Parameter-Efficient Tuning for'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 'CPET: Effective Parameter-Efficient Tuning for'
- en: Compressed Large Language Models
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Compressed Large Language Models
- en: 'Weilin Zhao^†^†thanks: indicatesequalcontribution.  , Yuxiang Huang^∗, Xu Han,
    Zhiyuan Liu Zhengyan Zhang, Maosong Sun NLP Group, DCST, IAI, BNRIST, Tsinghua
    University, Beijing {zwl23,huang-yx21,zy-z19}@mails.tsinghua.edu.cn {hanxu2022,liuzy,sms}@tsinghua.edu.cn'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'Weilin Zhao^†^†thanks: indicatesequalcontribution.  , Yuxiang Huang^∗, Xu Han,
    Zhiyuan Liu Zhengyan Zhang, Maosong Sun NLP Group, DCST, IAI, BNRIST, Tsinghua
    University, Beijing {zwl23,huang-yx21,zy-z19}@mails.tsinghua.edu.cn {hanxu2022,liuzy,sms}@tsinghua.edu.cn'
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 Introduction
- en: In recent years, the rise in data scale and computing power has boosted the
    growth of the parameter size of language models. While some small and medium language
    models with millions of parameters have shown proficiency in capturing rich knowledge Jawahar
    et al. ([2019](#bib.bib28)); Yenicelik et al. ([2020](#bib.bib63)), large language
    models (LLMs) with billions of parameters Brown et al. ([2020](#bib.bib6)); Black
    et al. ([2022](#bib.bib4)); Chowdhery et al. ([2022](#bib.bib9)) exhibit more
    powerful and comprehensive abilities, especially in terms of cognition and embodiment Lewkowycz
    et al. ([2022](#bib.bib32)); Nakano et al. ([2021](#bib.bib38)); Driess et al.
    ([2023](#bib.bib17)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，数据规模和计算能力的提升推动了语言模型参数规模的增长。虽然一些拥有百万级参数的小型和中型语言模型已展示出丰富的知识捕捉能力 Jawahar et
    al. ([2019](#bib.bib28)); Yenicelik et al. ([2020](#bib.bib63))，但拥有数十亿参数的大型语言模型（LLMs）
    Brown et al. ([2020](#bib.bib6)); Black et al. ([2022](#bib.bib4)); Chowdhery
    et al. ([2022](#bib.bib9)) 展现出更强大和全面的能力，尤其在认知和体现方面 Lewkowycz et al. ([2022](#bib.bib32));
    Nakano et al. ([2021](#bib.bib38)); Driess et al. ([2023](#bib.bib17))。
- en: Despite the success of LLMs, how to apply LLMs to serve real-world scenarios
    is an important issue. As most users cannot afford the enormous cost of running
    LLMs, the prevailing solution is to provide LLM services, with service providers OpenAI
    ([2022](#bib.bib39)); Google ([2023](#bib.bib21)) adapting LLMs for specific tasks
    and then providing users with interfaces to infer the task-specific LLMs. To extend
    LLM services to multi-task scenarios, parameter-efficient tuning (PET) Houlsby
    et al. ([2019](#bib.bib26)); Hu et al. ([2021](#bib.bib27)) has been widely used
    for the task adaptation of LLMs, where a unified LLM is frozen as a backbone among
    different tasks and then tiny tunable PET modules are injected into the backbone
    to stimulate task-specific knowledge. Compared to conventional full-parameter
    fine-tuning (FT), where a single LLM is tuned into multiple task-specific LLM
    copies, PET tunes much fewer parameters and has lower memory overhead in multi-task
    serving while achieving comparable performance Ding et al. ([2023](#bib.bib15));
    Zhou et al. ([2022](#bib.bib69)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LLMs（大型语言模型）取得了成功，如何将 LLMs 应用于现实场景仍是一个重要问题。由于大多数用户无法负担运行 LLMs 的巨大成本，目前的解决方案是提供
    LLM 服务，服务提供者 OpenAI ([2022](#bib.bib39))；Google ([2023](#bib.bib21)) 适配 LLMs 以执行特定任务，然后向用户提供推理任务特定
    LLMs 的接口。为了将 LLM 服务扩展到多任务场景，参数高效调优（PET） Houlsby et al. ([2019](#bib.bib26))；Hu
    et al. ([2021](#bib.bib27)) 已被广泛用于 LLMs 的任务适配，其中统一的 LLM 被冻结为不同任务之间的主干，然后将微小的可调
    PET 模块注入到主干中以激发任务特定知识。与传统的全参数微调（FT）相比，PET 调整的参数更少，在多任务服务中具有更低的内存开销，同时实现了可比的性能
    Ding et al. ([2023](#bib.bib15))；Zhou et al. ([2022](#bib.bib69))。
- en: Although PET has shown potential in reducing the cost of tuning and deploying
    LLMs for LLM services, the computation of the shared backbone LLM is inevitable,
    i.e., the inference of the combination of the backbone LLM and PET modules is
    still computation-intensive and latency-high. Empirically, adopting model compression
    techniques Hinton et al. ([2015](#bib.bib25)); Bai et al. ([2021](#bib.bib1));
    Liang et al. ([2021](#bib.bib34)) to compress LLMs into smaller versions is a
    solution to cope with the different latency requirements of inferring LLMs, yet
    whether PET modules can work well with compressed LLMs is still an open problem,
    especially considering that model compression techniques may introduce knowledge
    loss and performance degradation to the compressed LLMs. In this paper, we build
    an effective PET framework based on compressed LLMs, named “CPET”.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 PET 在减少调优和部署 LLMs 成本方面显示出了潜力，共享主干 LLM 的计算仍然不可避免，即主干 LLM 和 PET 模块的组合推理仍然计算密集且延迟高。经验上，采用模型压缩技术
    Hinton et al. ([2015](#bib.bib25))；Bai et al. ([2021](#bib.bib1))；Liang et al.
    ([2021](#bib.bib34)) 将 LLMs 压缩成更小版本是应对 LLMs 推理不同延迟要求的解决方案，但 PET 模块是否能与压缩 LLMs
    良好配合仍是一个悬而未决的问题，特别是考虑到模型压缩技术可能会给压缩 LLMs 引入知识丢失和性能退化。在本文中，我们基于压缩 LLMs 构建了一个有效的
    PET 框架，称为“CPET”。
- en: 'To restore the knowledge loss caused by the compression process, CPET introduce
    the following two mechanisms:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了恢复压缩过程导致的知识丢失，CPET 引入了以下两个机制：
- en: (1) PET Knowledge Inheritance. A stronger LLM can make learning PET modules
    easier. Meanwhile, the PET modules based on the stronger LLM can also better grasp
    how to stimulate task-specific knowledge distributed in the LLM. Therefore, we
    propose to adopt the PET modules learned on the non-compressed LLM as the initialization
    to learn the PET modules for the compressed LLM. In this way, the task-related
    knowledge of PET modules learned with the help of the non-compressed LLM can be
    inherited to obtain more effective PET modules for the compressed LLM.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (1) PET 知识继承。更强的 LLM 可以使学习 PET 模块变得更容易。同时，基于更强 LLM 的 PET 模块也可以更好地把握如何激发分布在 LLM
    中的任务特定知识。因此，我们建议采用在未压缩 LLM 上学习的 PET 模块作为初始化，以学习压缩 LLM 的 PET 模块。通过这种方式，借助未压缩 LLM
    学习的 PET 模块的任务相关知识可以被继承，从而获得更有效的压缩 LLM 的 PET 模块。
- en: (2) Model Knowledge Recovery. In addition to the knowledge of PET modules, the
    knowledge of the LLM is also important to perform well on downstream tasks. Since
    compression techniques may result in losing some task-related knowledge within
    the LLM, we add extra knowledge recovery modules into the compressed LLM to bridge
    the knowledge gap that arises from the compression process. We point out that
    compression techniques may weaken multiple capabilities of the LLM while restoring
    only a part of the lost capabilities requires only a small number of parameters.
    Through the supervision of task data, we can recover most of the lost capabilities
    related to specific tasks through some tiny recovery modules.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 模型知识恢复。除了PET模块的知识外，LLM的知识对于在下游任务中表现良好也很重要。由于压缩技术可能导致LLM中某些任务相关知识的丢失，我们在压缩后的LLM中添加了额外的知识恢复模块，以弥补压缩过程中的知识缺失。我们指出，压缩技术可能会削弱LLM的多种能力，而恢复丧失的部分能力只需要少量参数。通过任务数据的监督，我们可以通过一些微小的恢复模块恢复大部分与特定任务相关的丧失能力。
- en: In experiments, we conduct a comprehensive evaluation of the performance impact
    brought by various compression methods. The results show that compression results
    in a significant performance drop without using any knowledge recovery mechanisms.
    Based on the above observation, we apply CPET for performance recovery, and the
    experimental results indicate that CPET can restore the performance to the level
    before model compression. Moreover, computing the compressed LLM requires much
    lower resources than computing the non-compressed LLM, making CPET finally an
    effective and efficient PET framework.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们对各种压缩方法带来的性能影响进行了全面评估。结果显示，未经任何知识恢复机制的压缩会导致显著的性能下降。基于上述观察，我们应用CPET进行性能恢复，实验结果表明CPET能够将性能恢复到模型压缩前的水平。此外，计算压缩后的LLM所需的资源远低于计算未压缩LLM的资源，使得CPET最终成为一个有效且高效的PET框架。
- en: 2 Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: This work is related to LLMs, PET, and model compression. We mainly introduce
    PET and model compression methods in this paper. More details on LLMs can refer
    to the survey for more details Qiu et al. ([2020](#bib.bib43)); Han et al. ([2021](#bib.bib23));
    Bommasani et al. ([2021](#bib.bib5)); Zhao et al. ([2023](#bib.bib68)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作涉及LLMs、PET和模型压缩。本文主要介绍了PET和模型压缩方法。有关LLMs的更多细节，可以参考Qiu等人的调查（[2020](#bib.bib43)）；Han等人（[2021](#bib.bib23)）；Bommasani等人（[2021](#bib.bib5)）；Zhao等人（[2023](#bib.bib68)）。
- en: 2.1 Parameter-Efficient Tuning
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 参数高效调优
- en: Although an LLM can acquire rich knowledge from massive pre-training data to
    handle complex tasks in a zero-shot or few-shot manner Brown et al. ([2020](#bib.bib6));
    Black et al. ([2022](#bib.bib4)), to better stimulate the knowledge stored in
    the LLM to serve downstream tasks, there is still a need for adapting the LLM
    to various scenarios. For traditional PLMs, fine-tuning all parameters of PLMs
    is the mainstream way to adapt them Church et al. ([2021](#bib.bib10)), yet its
    parameter inefficiency makes this way costly to adapt LLMs Ding et al. ([2023](#bib.bib15)).
    Moreover, maintaining task-specific versions of LLM in the storage is unacceptably
    resource-intensive Zhou et al. ([2022](#bib.bib69)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM可以通过大量预训练数据获取丰富的知识，从而以零样本或少样本的方式处理复杂任务（Brown等人（[2020](#bib.bib6)）；Black等人（[2022](#bib.bib4)）），但为了更好地激发LLM中存储的知识以服务于下游任务，仍然需要对LLM进行各种场景的适配。对于传统的PLMs，微调PLMs的所有参数是适配它们的主流方式（Church等人（[2021](#bib.bib10)）），但这种方法的参数低效使得适配LLMs成本高（Ding等人（[2023](#bib.bib15)））。此外，维护任务特定的LLM版本在存储中是不可接受的资源密集型操作（Zhou等人（[2022](#bib.bib69)））。
- en: To adapt LLMs to multi-task scenarios in a more efficient manner, various PET
    methods Lester et al. ([2021](#bib.bib31)); Houlsby et al. ([2019](#bib.bib26));
    Hu et al. ([2021](#bib.bib27)); Li and Liang ([2021](#bib.bib33)); Ben Zaken et al.
    ([2022](#bib.bib2)) have been proposed, where LLMs are frozen and some model-independent
    tunable modules are injected into the transformer architecture of LLMs to help
    the adaptation process. PET modules are usually tiny, which can significantly
    reduce the cost of adapting LLMs. PET modules can be inserted into different locations
    within the transformer architecture. For instance, prompt tuning Lester et al.
    ([2021](#bib.bib31)) and prefix tuning Li and Liang ([2021](#bib.bib33)) are two
    methods that prepend tunable embeddings to the input and hidden states, respectively.
    Adapter tuning Houlsby et al. ([2019](#bib.bib26)) applies tunable transformation
    between adjacent modules. BitFit Ben Zaken et al. ([2022](#bib.bib2)) and LoRA Hu
    et al. ([2021](#bib.bib27)) make minor internal modifications to the modules of
    the transformer architecture.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更高效地将 LLM 适配到多任务场景中，已经提出了各种 PET 方法 Lester et al. ([2021](#bib.bib31)); Houlsby
    et al. ([2019](#bib.bib26)); Hu et al. ([2021](#bib.bib27)); Li and Liang ([2021](#bib.bib33));
    Ben Zaken et al. ([2022](#bib.bib2))，其中 LLM 被冻结，并且一些与模型无关的可调模块被注入到 LLM 的 Transformer
    架构中，以帮助适配过程。PET 模块通常很小，可以显著降低适配 LLM 的成本。PET 模块可以插入到 Transformer 架构中的不同位置。例如，prompt
    tuning Lester et al. ([2021](#bib.bib31)) 和 prefix tuning Li and Liang ([2021](#bib.bib33))
    是两种将可调嵌入添加到输入和隐藏状态中的方法。Adapter tuning Houlsby et al. ([2019](#bib.bib26)) 在相邻模块之间应用可调变换。BitFit Ben Zaken
    et al. ([2022](#bib.bib2)) 和 LoRA Hu et al. ([2021](#bib.bib27)) 对 Transformer
    架构的模块进行细微的内部修改。
- en: As mentioned before, LLMs have acquired rich capabilities and just need an efficient
    way to stimulate these capabilities. The role of tunable PET modules is to learn
    task features and serve as triggers to stimulate task-specific capabilities in
    LLMs Ding et al. ([2023](#bib.bib15)). Sufficient experiments show that collaborating
    task-specific PET modules and a frozen LLM can reach comparable performance to
    fine-tuning all parameters of the LLM. Furthermore, since different task-specific
    PET modules can share a unified frozen LLM as their backbone, this also leads
    to lower computation and storage overhead in multi-task serving and switching Zhou
    et al. ([2022](#bib.bib69)). In general, the emergence of PET methods significantly
    reduces the cost of tuning and deploying LLMs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LLM 已经获得了丰富的能力，只需要一种高效的方法来激发这些能力。可调 PET 模块的作用是学习任务特征，并作为触发器来激发 LLM 的任务特定能力 Ding
    et al. ([2023](#bib.bib15))。充分的实验表明，协作的任务特定 PET 模块与冻结的 LLM 可以达到与微调 LLM 所有参数相媲美的性能。此外，由于不同的任务特定
    PET 模块可以共享一个统一的冻结 LLM 作为它们的骨干，这也导致了在多任务服务和切换中的计算和存储开销降低 Zhou et al. ([2022](#bib.bib69))。总体而言，PET
    方法的出现显著降低了调整和部署 LLM 的成本。
- en: 2.2 Model Compression
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 模型压缩
- en: Although PET methods can reduce the storage cost for deploying LLMs, the computation
    bottleneck of the LLM itself still exists. Therefore, to further improve efficiency
    for model serving, it is crucial to speed up the computation of LLMs, and model
    compression is a commonly used solution. Considering that the PET modules of different
    tasks usually work together on a unified LLM, here we mainly introduce task-agnostic
    model compression Sanh et al. ([2019](#bib.bib48)) rather than task-specific compression Sun
    et al. ([2019](#bib.bib52)) for LLMs, including quantization, pruning, and MoEfication.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 PET 方法可以降低部署 LLM 的存储成本，但 LLM 本身的计算瓶颈仍然存在。因此，为了进一步提高模型服务的效率，至关重要的是加速 LLM 的计算，而模型压缩是一种常用的解决方案。考虑到不同任务的
    PET 模块通常在统一的 LLM 上协同工作，这里我们主要介绍与任务无关的模型压缩方法 Sanh et al. ([2019](#bib.bib48))，而不是针对特定任务的压缩方法 Sun
    et al. ([2019](#bib.bib52))，包括量化、剪枝和 MoEfication。
- en: In traditional PLMs, 32-bit floating-point numbers are mainly used to represent
    models. As the model size gradually increases, representing LLMs in a 32-bit format
    consumes too much GPU memory and computational time. To address this issue, mixed-precision
    training Micikevicius et al. ([2017](#bib.bib37)) is adopted to represent LLMs
    with 16-bit floating-point numbers. To further reduce the memory overhead and
    improve the model speed, quantization methods are applied to represent models
    with fixed-point numbers, from 8-bit Zafrir et al. ([2019](#bib.bib64)), 4-bit Frantar
    et al. ([2023](#bib.bib19)) to 1-bit Bai et al. ([2021](#bib.bib1)). To avoid
    the performance degradation caused by quantization, quantization-aware training
    (QAT) Stock et al. ([2021](#bib.bib50)) has also been proposed to use a small
    amount of data to adjust the distribution of model parameters for quantization.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的预训练语言模型（PLMs）中，主要使用32位浮点数来表示模型。随着模型大小的逐渐增加，使用32位格式表示大规模语言模型（LLMs）会消耗过多的GPU内存和计算时间。为了解决这个问题，采用了混合精度训练Micikevicius
    et al. ([2017](#bib.bib37))，用16位浮点数表示LLMs。为了进一步减少内存开销并提高模型速度，应用了量化方法来用定点数表示模型，从8位Zafrir
    et al. ([2019](#bib.bib64))、4位Frantar et al. ([2023](#bib.bib19))到1位Bai et al.
    ([2021](#bib.bib1))。为了避免量化引起的性能下降，还提出了量化感知训练（QAT）Stock et al. ([2021](#bib.bib50))，使用少量数据来调整模型参数的分布以适应量化。
- en: Different from quantization methods that compress the representation of each
    parameter, pruning methods directly discard some parameters. Commonly used pruning
    methods include structured pruning Fan et al. ([2020](#bib.bib18)); Wang et al.
    ([2020](#bib.bib58)); Zhang et al. ([2021](#bib.bib67)); Xia et al. ([2022](#bib.bib60))
    and unstructured pruning Han et al. ([2015](#bib.bib22)); Chen et al. ([2020](#bib.bib7));
    Xu et al. ([2021](#bib.bib62)). Structured pruning aims to find useless modules
    and remove them completely, such as erasing all parameters in a linear layer.
    Unstructured pruning only removes individual parameters, such as deleting some
    parameters to form a sparse matrix.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与压缩每个参数表示的量化方法不同，剪枝方法直接丢弃一些参数。常用的剪枝方法包括结构化剪枝Fan et al. ([2020](#bib.bib18));
    Wang et al. ([2020](#bib.bib58)); Zhang et al. ([2021](#bib.bib67)); Xia et al.
    ([2022](#bib.bib60))和非结构化剪枝Han et al. ([2015](#bib.bib22)); Chen et al. ([2020](#bib.bib7));
    Xu et al. ([2021](#bib.bib62))。结构化剪枝旨在找到无用的模块并完全移除它们，例如擦除线性层中的所有参数。非结构化剪枝仅删除单个参数，例如删除一些参数以形成稀疏矩阵。
- en: '![Refer to caption](img/1edae9477e5148bdb28dea4d3b29043a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1edae9477e5148bdb28dea4d3b29043a.png)'
- en: 'Figure 1: The overall design of our CPET. We use LoRA Hu et al. ([2021](#bib.bib27))
    as an example of PET methods.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们CPET的总体设计。我们使用LoRA Hu et al. ([2021](#bib.bib27))作为PET方法的示例。
- en: MoEfication Zhang et al. ([2022b](#bib.bib66)), inspired by the mixture-of-experts
    (MoE) transformer Lepikhin et al. ([2021](#bib.bib30)), aims to divide the parameters
    of LLMs into multiple partitions, and each time only a few partitions are used
    to compute the final results. Although most of the currently popular LLMs are
    dense models, studies have shown that dense LLMs are activated sparsely, and different
    parameter areas are activated by different data to form some skill partitions Wang
    et al. ([2022](#bib.bib57)); Dai et al. ([2021](#bib.bib12)); Suau et al. ([2020](#bib.bib51));
    Panigrahi et al. ([2023](#bib.bib40)). Specifically, by analyzing the sparse pattern
    of activation states in LLMs, the linear layers of LLMs are sliced to MoE, and
    an expert router is trained to select experts. During the computation process,
    a certain proportion of relevant experts is dynamically activated according to
    the input data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: MoEfication Zhang et al. ([2022b](#bib.bib66))，受混合专家（MoE）变换器Lepikhin et al.
    ([2021](#bib.bib30))的启发，旨在将LLMs的参数划分为多个部分，每次仅使用少数几个部分来计算最终结果。尽管目前流行的大多数LLMs是密集模型，但研究表明，密集LLMs是稀疏激活的，不同的数据激活不同的参数区域，形成一些技能分区Wang
    et al. ([2022](#bib.bib57)); Dai et al. ([2021](#bib.bib12)); Suau et al. ([2020](#bib.bib51));
    Panigrahi et al. ([2023](#bib.bib40))。具体来说，通过分析LLMs中的激活状态稀疏模式，将LLMs的线性层切分为MoE，并训练一个专家路由器来选择专家。在计算过程中，根据输入数据动态激活一定比例的相关专家。
- en: Typically, to make a compressed LLM behave the same as its original version,
    distillation objectives are often used to align the pre-compression and post-compression
    models, including aligning both output and intermediate states Hinton et al. ([2015](#bib.bib25));
    Sun et al. ([2019](#bib.bib52)); Jiao et al. ([2020](#bib.bib29)); Liu et al.
    ([2022](#bib.bib35)); Park et al. ([2021](#bib.bib41)). Due to space limitations,
    more compression details can refer to the survey Liang et al. ([2021](#bib.bib34));
    Xu and McAuley ([2022](#bib.bib61)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，为了使压缩后的 LLM 行为与其原始版本相同，常常使用蒸馏目标来对齐压缩前后的模型，包括对齐输出和中间状态 Hinton 等人 ([2015](#bib.bib25));
    Sun 等人 ([2019](#bib.bib52)); Jiao 等人 ([2020](#bib.bib29)); Liu 等人 ([2022](#bib.bib35));
    Park 等人 ([2021](#bib.bib41))。由于空间限制，更多的压缩细节可以参考 Liang 等人 ([2021](#bib.bib34));
    Xu 和 McAuley ([2022](#bib.bib61)) 的综述。
- en: Currently, combining PET with model compression is preliminary, and only some
    works attempt to combine PET with model quantization Dettmers et al. ([2023](#bib.bib14));
    Liu et al. ([2023](#bib.bib36)). Recent work Chen et al. ([2023](#bib.bib8)) also
    attempts to add modules to recover the knowledge loss caused by model compression,
    but it has not been fully verified on various downstream tasks. Combining PET
    with other compression methods to improve the inference speed is still an open
    issue for further exploration.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，将 PET 与模型压缩结合的研究仍处于初步阶段，仅有一些研究尝试将 PET 与模型量化结合 Dettmers 等人 ([2023](#bib.bib14));
    Liu 等人 ([2023](#bib.bib36))。近期的研究 Chen 等人 ([2023](#bib.bib8)) 也尝试添加模块以恢复因模型压缩造成的知识丧失，但在各种下游任务中尚未得到充分验证。将
    PET 与其他压缩方法结合以提高推理速度仍是一个待进一步探索的开放问题。
- en: 3 Methodology
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: In this section, we will introduce how to build an effective PET framework CPET based
    on compressed LLMs. Before introducing CPET, we first explain some essential preliminaries.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将介绍如何基于压缩后的 LLM 构建有效的 PET 框架 CPET。在介绍 CPET 之前，我们首先解释一些基本的预备知识。
- en: 3.1 Preliminary
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 预备知识
- en: For simplicity, we denote a LLM $\mathcal{M}$) are tuned as follows
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们将一个 LLM $\mathcal{M}$) 的调整方法表示如下。
- en: '|  | $\mathbf{\theta}_{\mathcal{M}}^{t}=\arg\min_{\mathbf{\theta}_{\mathcal{M}}}\mathcal{L}(f(\mathbf{X}^{t};\mathbf{\theta}_{\mathcal{M}}),\mathbf{Y}^{t}),$
    |  | (1) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\theta}_{\mathcal{M}}^{t}=\arg\min_{\mathbf{\theta}_{\mathcal{M}}}\mathcal{L}(f(\mathbf{X}^{t};\mathbf{\theta}_{\mathcal{M}}),\mathbf{Y}^{t}),$
    |  | (1) |'
- en: where $\mathbf{X}^{t},\mathbf{Y}^{t}$.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{X}^{t},\mathbf{Y}^{t}$。
- en: In the PET setting, $\mathcal{M}$. The tuning process is formalized as
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PET 设置中，$\mathcal{M}$。调整过程形式化为
- en: '|  | $1$2 |  | (2) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $\mathbf{\theta}_{\mathcal{P}(\mathcal{M})}^{t}$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{\theta}_{\mathcal{P}(\mathcal{M})}^{t}$。
- en: This paper aims to obtain PET modules based on a compressed LLM. To this end,
    after applying compression algorithms to compress the LLM $\mathcal{M}$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在基于压缩后的 LLM 获得 PET 模块。为此，在对 LLM $\mathcal{M}$ 应用压缩算法进行压缩后。
- en: 3.2 Framework
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 框架
- en: Since PET methods do not change LLMs, adopting PET methods is thus orthogonal
    to compressing LLMs. Therefore, we propose a more efficient PET framework CPET,
    by first compressing a LLM using task-agnostic model compression methods and then
    applying PET methods to the compressed LLM. Formally, CPET can be formalized as
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 PET 方法不会改变 LLM，因此采用 PET 方法与压缩 LLM 是正交的。因此，我们提出了一种更高效的 PET 框架 CPET，通过首先使用任务无关的模型压缩方法来压缩
    LLM，然后将 PET 方法应用于压缩后的 LLM。形式上，CPET 可以表述为
- en: '|  | $1$2 |  | (3) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: where $\theta_{\mathcal{P}(\mathcal{C})}$.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta_{\mathcal{P}(\mathcal{C})}$。
- en: By combining model compression and PET, on the one hand, we can take advantage
    of PET to deploy a unified LLM to serve multiple downstream tasks, while only
    maintaining tiny task-specific PET modules for each downstream task. On the other
    hand, by adopting a compressed LLM instead of a non-compressed LLM, the inference
    time and resource requirements of the LLM can be significantly reduced. It is
    worth noting that this acceleration is not free. It is not difficult to imagine
    that adopting task-agnostic compression methods may weaken the LLM, which will
    inevitably affect the search for the optimal parameters $\mathbf{\theta}_{\mathcal{P}(\mathcal{C})}^{t}$.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将模型压缩与 PET 结合，一方面，我们可以利用 PET 部署一个统一的 LLM 来服务多个下游任务，同时仅维护少量任务特定的 PET 模块。另一方面，通过采用压缩后的
    LLM 而非未压缩的 LLM，可以显著减少 LLM 的推理时间和资源需求。值得注意的是，这种加速不是免费的。可以想象，采用任务无关的压缩方法可能会削弱 LLM，这不可避免地会影响对最佳参数
    $\mathbf{\theta}_{\mathcal{P}(\mathcal{C})}^{t}$ 的搜索。
- en: 'Inspired by the fact that model compression preserves those capabilities of
    LLMs that smaller models cannot master, we suppose that the PET modules trained
    on the non-compressed LLM would contain certain task knowledge that the PET modules
    can hardly learn solely on the compressed model. As shown in Figure [1](#S2.F1
    "Figure 1 ‣ 2.2 Model Compression ‣ 2 Related Work ‣ CPET: Effective Parameter-Efficient
    Tuning for Compressed Large Language Models"), to better learn PET modules for
    the compressed LLM, we adopt the method of inheriting the PET knowledge from those
    modules trained on the non-compressed LLM. To restore the knowledge loss caused
    by the compressing process, in addition to the PET modules $\mathcal{P}$, and
    Eq. ([3](#S3.E3 "In 3.2 Framework ‣ 3 Methodology ‣ CPET: Effective Parameter-Efficient
    Tuning for Compressed Large Language Models")) is modified to'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 受模型压缩保留了LLM的那些小模型无法掌握的能力的事实启发，我们认为在非压缩LLM上训练的PET模块将包含压缩模型上难以仅凭自身学习到的任务知识。如图 [1](#S2.F1
    "图 1 ‣ 2.2 模型压缩 ‣ 2 相关工作 ‣ CPET：压缩大语言模型的有效参数高效调优") 所示，为了更好地学习压缩LLM的PET模块，我们采用了从那些在非压缩LLM上训练的模块继承PET知识的方法。为了恢复压缩过程造成的知识损失，除了PET模块
    $\mathcal{P}$ 之外，方程 ([3](#S3.E3 "在 3.2 框架 ‣ 3 方法 ‣ CPET：压缩大语言模型的有效参数高效调优")) 被修改为
- en: '|  | $\displaystyle\mathbf{\theta}_{\mathcal{P}(\mathcal{C})}^{t},\mathbf{\theta}_{\mathcal{R}}^{t}=$
    |  |  | (4) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{\theta}_{\mathcal{P}(\mathcal{C})}^{t},\mathbf{\theta}_{\mathcal{R}}^{t}=$
    |  |  | (4) |'
- en: '|  | $\displaystyle\arg\min_{\mathbf{\theta}_{\mathcal{P}(\mathcal{C})},\mathbf{\theta}_{\mathcal{R}}}$
    |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\arg\min_{\mathbf{\theta}_{\mathcal{P}(\mathcal{C})},\mathbf{\theta}_{\mathcal{R}}}$
    |  |'
- en: '|  |  | $\displaystyle+\alpha\mathcal{L}_{\text{DIST}}(\mathbf{X}^{t};\theta_{\mathcal{M}},\theta_{\mathcal{C}},\theta_{\mathcal{P}(\mathcal{C})},\mathbf{\theta}_{\mathcal{R}})\big{]},$
    |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\alpha\mathcal{L}_{\text{DIST}}(\mathbf{X}^{t};\theta_{\mathcal{M}},\theta_{\mathcal{C}},\theta_{\mathcal{P}(\mathcal{C})},\mathbf{\theta}_{\mathcal{R}})\big{]},$
    |  |'
- en: where $\mathbf{\theta}_{\mathcal{R}}^{t}$ is the distillation loss function
    for model knowledge recovery, which will be introduced later.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{\theta}_{\mathcal{R}}^{t}$ 是用于模型知识恢复的蒸馏损失函数，这将在后面介绍。
- en: In subsequent sections, we will elaborate on how to conduct PET knowledge inheritance
    and model knowledge recovery. It is worth noting that although our methods require
    the non-compressed model to participate in the training process, resulting in
    extra training time, in the context of model serving, the service period after
    training for typical tasks is much longer than the training time. Therefore, we
    sacrifice some training costs for better inference efficiency and effectiveness
    of the final model obtained.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续章节中，我们将详细阐述如何进行PET知识继承和模型知识恢复。值得注意的是，虽然我们的方法需要非压缩模型参与训练过程，从而增加了额外的训练时间，但在模型服务的背景下，典型任务的服务周期远远长于训练时间。因此，我们为了最终模型的更好推理效率和效果，牺牲了一些训练成本。
- en: 3.3 PET Knowledge Inheritance
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 PET知识继承
- en: Instead of training PET modules based on the compressed LLM from scratch, we
    propose training PET modules based on the original non-compressed LLM first, then
    adapting the learned PET modules to the compressed LLM. The adaption from the
    non-compressed LLM to the compressed LLM can lead to learning better PET modules
    on the compressed LLM. Intuitively, it is more effective for a teacher to teach
    students the fundamentals of a discipline and then let students adapt their comprehension
    based on their circumstances rather than letting students learn from scratch.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的方案不是从头训练基于压缩LLM的PET模块，而是首先基于原始非压缩LLM训练PET模块，然后将学习到的PET模块调整到压缩LLM上。从非压缩LLM到压缩LLM的调整可以导致在压缩LLM上学习到更好的PET模块。从直观上看，让教师教授学生学科的基础知识，然后让学生根据自己的情况调整理解，比让学生从头开始学习更有效。
- en: 'Formally, we first use Eq. ([2](#S3.E2 "In 3.1 Preliminary ‣ 3 Methodology
    ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models"))
    to obtain the parameters of the task-specific PET modules $\theta_{\mathcal{P}(\mathcal{M})}^{t}$
    by using Eq. ([3](#S3.E3 "In 3.2 Framework ‣ 3 Methodology ‣ CPET: Effective Parameter-Efficient
    Tuning for Compressed Large Language Models")) or Eq. ([4](#S3.E4 "In 3.2 Framework
    ‣ 3 Methodology ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large
    Language Models")).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '正式地，我们首先使用公式 ([2](#S3.E2 "In 3.1 Preliminary ‣ 3 Methodology ‣ CPET: Effective
    Parameter-Efficient Tuning for Compressed Large Language Models")) 通过使用公式 ([3](#S3.E3
    "In 3.2 Framework ‣ 3 Methodology ‣ CPET: Effective Parameter-Efficient Tuning
    for Compressed Large Language Models")) 或公式 ([4](#S3.E4 "In 3.2 Framework ‣ 3
    Methodology ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large
    Language Models")) 来获得任务特定PET模块的参数 $\theta_{\mathcal{P}(\mathcal{M})}^{t}$。'
- en: 3.4 Model Knowledge Recovery
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 模型知识恢复
- en: Since the reduction of parameters orienting to the compressed LLM $\mathcal{C}$.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于参数减少指向压缩后的LLM $\mathcal{C}$。
- en: 'To help obtain $\theta_{\mathcal{R}}^{t}$, we design a distillation objective.
    Specifically, we first select the PET modules trained with Eq. ([2](#S3.E2 "In
    3.1 Preliminary ‣ 3 Methodology ‣ CPET: Effective Parameter-Efficient Tuning for
    Compressed Large Language Models")) as the teacher, and then select the PET modules
    and recovery modules in Eq. ([4](#S3.E4 "In 3.2 Framework ‣ 3 Methodology ‣ CPET:
    Effective Parameter-Efficient Tuning for Compressed Large Language Models")) as
    the student, and the whole distillation loss is given as'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '为了帮助获得 $\theta_{\mathcal{R}}^{t}$，我们设计了一个蒸馏目标。具体来说，我们首先选择用公式 ([2](#S3.E2 "In
    3.1 Preliminary ‣ 3 Methodology ‣ CPET: Effective Parameter-Efficient Tuning for
    Compressed Large Language Models")) 训练的PET模块作为教师，然后选择公式 ([4](#S3.E4 "In 3.2 Framework
    ‣ 3 Methodology ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large
    Language Models")) 中的PET模块和恢复模块作为学生，整个蒸馏损失定义为'
- en: '|  | $\displaystyle\mathcal{L}_{\text{DIST}}$ |  | (5) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{DIST}}$ |  | (5) |'
- en: '|  | $\displaystyle\frac{1}{&#124;\mathbf{X}^{t}&#124;}\big{\&#124;}f_{\text{PET}}$
    |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{1}{\lvert\mathbf{X}^{t}\rvert}\big{\lvert}f_{\text{PET}}$
    |  |'
- en: 'where $\mathbf{X}^{t}$. As shown in Eq. ([4](#S3.E4 "In 3.2 Framework ‣ 3 Methodology
    ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models")),
    instead of first learning the recovery modules and then adding the inherited PET
    modules for further adaptation, we simultaneously conduct knowledge recovery and
    tune PET modules.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '在 $\mathbf{X}^{t}$ 处。如在公式 ([4](#S3.E4 "In 3.2 Framework ‣ 3 Methodology ‣ CPET:
    Effective Parameter-Efficient Tuning for Compressed Large Language Models")) 所示，我们不是先学习恢复模块再添加继承的PET模块进行进一步调整，而是同时进行知识恢复和调整PET模块。'
- en: 4 Experiments and Analyses
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验与分析
- en: In this section, we will present experimental results and analyses in detail.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将详细呈现实验结果和分析。
- en: 4.1 Datasets
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集
- en: We evaluate CPET on 11 datasets, covering typical NLP tasks, including BoolQ Clark
    et al. ([2019](#bib.bib11)), CB De Marneffe et al. ([2019](#bib.bib13)), RTE Bentivogli
    et al. ([2009](#bib.bib3)); Wang et al. ([2019](#bib.bib55)), COPA Roemmele et al.
    ([2011](#bib.bib47)), WiC Pilehvar and Camacho-Collados ([2018](#bib.bib42)),
    SST-2 Socher et al. ([2013](#bib.bib49)), MRPC Dolan and Brockett ([2005](#bib.bib16)),
    QQP Wang et al. ([2018](#bib.bib56)), MNLI Williams et al. ([2017](#bib.bib59)),
    QNLI Rajpurkar et al. ([2016](#bib.bib45)), and SQuAD Rajpurkar et al. ([2016](#bib.bib45)).
    For all these datasets, we use their validation sets for testing and parts of
    their training sets for validation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 11 个数据集上评估 CPET，涵盖了典型的 NLP 任务，包括 BoolQ Clark et al. ([2019](#bib.bib11))、CB
    De Marneffe et al. ([2019](#bib.bib13))、RTE Bentivogli et al. ([2009](#bib.bib3))；Wang
    et al. ([2019](#bib.bib55))、COPA Roemmele et al. ([2011](#bib.bib47))、WiC Pilehvar
    和 Camacho-Collados ([2018](#bib.bib42))、SST-2 Socher et al. ([2013](#bib.bib49))、MRPC
    Dolan 和 Brockett ([2005](#bib.bib16))、QQP Wang et al. ([2018](#bib.bib56))、MNLI
    Williams et al. ([2017](#bib.bib59))、QNLI Rajpurkar et al. ([2016](#bib.bib45))
    和 SQuAD Rajpurkar et al. ([2016](#bib.bib45))。对于所有这些数据集，我们使用它们的验证集进行测试，部分训练集用于验证。
- en: '![Refer to caption](img/2b4b49e035cc611966a5f768a8fa4f94.png)![Refer to caption](img/71b47096ceb02f993d64b837538c2b93.png)![Refer
    to caption](img/acd0e11e00ad6eb981d85c9210ea1f3b.png)![Refer to caption](img/14eac2964b6b5f9d7285657a33eabaa2.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2b4b49e035cc611966a5f768a8fa4f94.png)![参见说明](img/71b47096ceb02f993d64b837538c2b93.png)![参见说明](img/acd0e11e00ad6eb981d85c9210ea1f3b.png)![参见说明](img/14eac2964b6b5f9d7285657a33eabaa2.png)'
- en: 'Figure 2: Performance on various compression methods.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：各种压缩方法的性能。
- en: 4.2 Baselines and Implementation Details
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基准和实施细节
- en: 'We select T5-3b Raffel et al. ([2020](#bib.bib44)) as the backbone LLM in our
    experiments. For the compressed models, we use the compressed versions of T5-3b
    released by Zhang et al. ([2022a](#bib.bib65)). The compression methods used in
    our experiments include 8-bit quantization, structured pruning, unstructured pruning,
    and MoEfication. Table [1](#S4.T1 "Table 1 ‣ 4.3 The Overall Performance of CPET
    ‣ 4 Experiments and Analyses ‣ CPET: Effective Parameter-Efficient Tuning for
    Compressed Large Language Models") shows the models used in our experiments and
    their ideal inference speedup compared to T5-3b Zhang et al. ([2022a](#bib.bib65)).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '我们选择了T5-3b Raffel et al. ([2020](#bib.bib44))作为实验中的骨干LLM。对于压缩模型，我们使用了Zhang
    et al. ([2022a](#bib.bib65))发布的T5-3b压缩版本。我们实验中使用的压缩方法包括8位量化、结构化剪枝、非结构化剪枝和MoEfication。表[1](#S4.T1
    "Table 1 ‣ 4.3 The Overall Performance of CPET ‣ 4 Experiments and Analyses ‣
    CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models")显示了我们实验中使用的模型及其与T5-3b
    Zhang et al. ([2022a](#bib.bib65))的理想推理加速比。'
- en: 'To evaluate CPET, we adopt 4 paradigms:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估CPET，我们采用了4种模式：
- en: '(1) T5-3b + PET: PET modules are attached to the original T5-3b, and only the
    parameters of PET modules are tunable while the parameters of the LLM are frozen.
    (2) T5-base + PET: Tunable PET modules are attached to the frozen T5-base model.
    (3) CLM + PET: PET modules are attached to the compressed versions of T5-3b, and
    then these compressed LLMs (CLMs) are frozen and PET modules are tuned on task-specific
    data. (4) CLM + CPET: CPET is applied to the compressed versions of T5-3b, and
    only PET and recovery modules are tuned on task-specific data.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: (1) T5-3b + PET：将PET模块附加到原始的T5-3b上，仅PET模块的参数可调，而LLM的参数保持不变。(2) T5-base + PET：将可调的PET模块附加到冻结的T5-base模型上。(3)
    CLM + PET：将PET模块附加到T5-3b的压缩版本上，然后这些压缩LLM（CLM）被冻结，PET模块在任务特定数据上进行调优。(4) CLM + CPET：将CPET应用于T5-3b的压缩版本，仅在任务特定数据上调优PET和恢复模块。
- en: All the above paradigms are implemented with the open-source toolkit OpenDelta Ding
    et al. ([2023](#bib.bib15)). For a fair comparison, we use LoRA Hu et al. ([2021](#bib.bib27))
    as a representative PET method for all paradigms. We set the bottleneck dimension
    of the LoRA modules to $32$.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所有上述模式均使用开源工具包OpenDelta Ding et al. ([2023](#bib.bib15))实现。为了公平比较，我们使用LoRA Hu
    et al. ([2021](#bib.bib27))作为所有模式的代表性PET方法。我们将LoRA模块的瓶颈维度设置为$32$。
- en: 4.3 The Overall Performance of CPET
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 CPET 的整体性能
- en: 'Figure [2](#S4.F2 "Figure 2 ‣ 4.1 Datasets ‣ 4 Experiments and Analyses ‣ CPET:
    Effective Parameter-Efficient Tuning for Compressed Large Language Models") shows
    the performance improvement of CPET compared to PET. From the figure, we can find
    that:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '图[2](#S4.F2 "Figure 2 ‣ 4.1 Datasets ‣ 4 Experiments and Analyses ‣ CPET: Effective
    Parameter-Efficient Tuning for Compressed Large Language Models")显示了CPET与PET相比的性能提升。从图中可以发现：'
- en: (1) Comparing the original LLM and its compressed versions, the results show
    that the compressed LLMs cannot perform as well as the original LLM. It suggests
    that task-agnostic compression methods lead to losing some knowledge related to
    downstream tasks. That is to say, to improve the inference speed, the performance
    of the compressed model may decrease due to the acceleration process. If there
    is a mechanism to make up the performance gap without affecting the inference
    speed, applying compression methods will be more reasonable.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 比较原始LLM及其压缩版本，结果表明压缩LLM的表现不如原始LLM。这表明任务无关的压缩方法会丢失与下游任务相关的一些知识。也就是说，为了提高推理速度，压缩模型的性能可能会因为加速过程而降低。如果有一种机制可以弥补性能差距而不影响推理速度，那么应用压缩方法将更为合理。
- en: '| Model | Model Size | Ideal Speedup |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 模型大小 | 理想加速比 |'
- en: '| T5-3b (bf16) | 5.61 GB | 100% |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| T5-3b (bf16) | 5.61 GB | 100% |'
- en: '| T5-3b (M) | 3.74 GB | 150% |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| T5-3b (M) | 3.74 GB | 150% |'
- en: '| T5-3b (UP) | 2.81 GB | 200% |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| T5-3b (UP) | 2.81 GB | 200% |'
- en: '| T5-3b (SP) | 2.81 GB | 200% |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| T5-3b (SP) | 2.81 GB | 200% |'
- en: '| T5-3b (Q) | 2.81 GB | 200% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| T5-3b (Q) | 2.81 GB | 200% |'
- en: '| T5-3b (Q+UP+M) | 0.94 GB | 600% |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| T5-3b (Q+UP+M) | 0.94 GB | 600% |'
- en: '| T5-base (bf16) | 0.44 GB | 1400% |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| T5-base (bf16) | 0.44 GB | 1400% |'
- en: 'Table 1: The models used in the experiments. The notation “T5-3b (X)” represents
    the T5-3b model with the setting “X”. “M”, “UP”, “SP”, and “Q” represent the model
    is compressed with MoEfication, unstructured pruning, structured pruning, and
    8-bit quantization, respectively. “Q+UP+M“ means “Q”, “UP” and “M” are combined
    together to achieve higher compression ratios. “bf16” indicates the model is represented
    in bfloat16 floating-point rather than 32-bit floating-point format.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1：实验中使用的模型。标记 “T5-3b (X)” 表示具有设置 “X” 的 T5-3b 模型。“M”、“UP”、“SP”和“Q” 分别表示模型使用
    MoEfication、无结构剪枝、结构剪枝和 8 位量化进行压缩。“Q+UP+M” 意味着 “Q”、“UP”和“M” 结合在一起以实现更高的压缩比。“bf16”
    表示模型以 bfloat16 浮点格式而非 32 位浮点格式表示。
- en: '|  | Model | BoolQ | CB | RTE | COPA | WiC | SST2 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型 | BoolQ | CB | RTE | COPA | WiC | SST2 |'
- en: '| Method | Size(GB) | Acc(%) | Acc(%) | Acc(%) | Acc(%) | Acc(%) | Acc(%) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 大小(GB) | 准确率(%) | 准确率(%) | 准确率(%) | 准确率(%) | 准确率(%) | 准确率(%) |'
- en: '| T5-3b + PET | 5.61 | 88.4 | 98.2 | 89.6 | 87.0 | 75.4 | 96.1 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| T5-3b + PET | 5.61 | 88.4 | 98.2 | 89.6 | 87.0 | 75.4 | 96.1 |'
- en: '| T5-base + PET | 0.44 | 79.5 | 91.1 | 80.7 | 71.0 | 69.9 | 93.5 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| T5-base + PET | 0.44 | 79.5 | 91.1 | 80.7 | 71.0 | 69.9 | 93.5 |'
- en: '| CLM + PET | 0.94 | 86.0 | 94.6 | 81.8 | 79.0 | 73.6 | 94.8 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| CLM + PET | 0.94 | 86.0 | 94.6 | 81.8 | 79.0 | 73.6 | 94.8 |'
- en: '| CLM + CPET | 0.94 | 86.7 | 100.0 | 86.1 | 85.0 | 75.3 | 96.2 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| CLM + CPET | 0.94 | 86.7 | 100.0 | 86.1 | 85.0 | 75.3 | 96.2 |'
- en: '|  | MNLI-m | QQP | QQP | MRPC | QNLI | SQuAD | SQuAD |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | MNLI-m | QQP | QQP | MRPC | QNLI | SQuAD | SQuAD |'
- en: '| Method | Acc(%) | Acc(%) | F1(%) | Acc(%) | Acc(%) | EM(%) | F1(%) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 准确率(%) | 准确率(%) | F1(%) | 准确率(%) | 准确率(%) | EM(%) | F1(%) |'
- en: '| T5-3b + PET | 90.6 | 91.3 | 90.7 | 89.5 | 95.4 | 84.2 | 92.5 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| T5-3b + PET | 90.6 | 91.3 | 90.7 | 89.5 | 95.4 | 84.2 | 92.5 |'
- en: '| T5-base + PET | 84.8 | 90.6 | 89.9 | 86.5 | 93.1 | 79.0 | 87.8 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| T5-base + PET | 84.8 | 90.6 | 89.9 | 86.5 | 93.1 | 79.0 | 87.8 |'
- en: '| CLM + PET | 89.0 | 90.6 | 89.9 | 89.7 | 94.7 | 79.9 | 90.6 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| CLM + PET | 89.0 | 90.6 | 89.9 | 89.7 | 94.7 | 79.9 | 90.6 |'
- en: '| CLM + CPET | 89.9 | 91.5 | 90.9 | 89.5 | 94.7 | 81.3 | 90.5 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| CLM + CPET | 89.9 | 91.5 | 90.9 | 89.5 | 94.7 | 81.3 | 90.5 |'
- en: 'Table 2: The results of applying CPET on the mixture of multiple compression
    methods (%).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：应用 CPET 在多种压缩方法混合上的结果（%）。
- en: (2) Within the compressed model, CPET consistently outperforms vanilla PET methods.
    Such results indicate that task capabilities are effectively migrated through
    the mechanisms of knowledge inheritance and knowledge recovery.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 在压缩模型中，CPET 一直优于普通 PET 方法。这些结果表明，通过知识继承和知识恢复机制，任务能力得到了有效迁移。
- en: (3) Through cross-comparisons between different compression models, we find
    that quantization and MoEfication have relatively little loss on the model performance,
    and the loss can be completely restored using our CPET method. However, the pruning
    methods cause more performance loss, especially the structured pruning method.
    Even though, CPET can recover most of the performance loss caused by these pruning
    methods.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 通过不同压缩模型之间的交叉比较，我们发现量化和 MoEfication 对模型性能的损失相对较小，并且损失可以通过我们的 CPET 方法完全恢复。然而，剪枝方法会导致更多的性能损失，尤其是结构剪枝方法。即便如此，CPET
    也能恢复这些剪枝方法造成的大部分性能损失。
- en: '![Refer to caption](img/bbe6645e7aba9246d6f5a1ec21616736.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bbe6645e7aba9246d6f5a1ec21616736.png)'
- en: 'Figure 3: The convergence of vanilla PET, inherited PET (CPET without recovery
    modules), and CPET.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：普通 PET、继承 PET（无恢复模块的 CPET）和 CPET 的收敛性。
- en: 'To further evaluate CPET on a higher compression ratio, we combine quantization,
    MoEfication, and unstructured pruning to obtain a compressed T5-3b that has a
    close size to T5-base. We compare the four paradigms based on this compressed
    model. Table [2](#S4.T2 "Table 2 ‣ 4.3 The Overall Performance of CPET ‣ 4 Experiments
    and Analyses ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large
    Language Models") shows the experimental results. Results show that CPET is compatible
    and can be easily applied to a highly compressed model that uses multiple compression
    methods. Meanwhile, CPET demonstrates better performance on compressed models
    than training a small model with a close size from scratch.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步评估 CPET 在更高压缩比下的表现，我们结合量化、MoEfication 和无结构剪枝，获得了一个与 T5-base 大小接近的压缩 T5-3b
    模型。我们基于这个压缩模型比较了四种范式。表格 [2](#S4.T2 "表格 2 ‣ 4.3 CPET 的整体性能 ‣ 4 实验与分析 ‣ CPET：有效的参数高效调优方法")
    显示了实验结果。结果表明，CPET 兼容并且可以轻松应用于使用多种压缩方法的高度压缩模型。同时，CPET 在压缩模型上的表现优于从头训练一个大小相近的小模型。
- en: 4.4 Ablation Studies
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 消融研究
- en: To further discuss the effectiveness of CPET design, it is vital to understand
    how different mechanisms in CPET help restore the knowledge loss caused by model
    compression, whether the number of parameters the main cause of performance enhancement
    and what is the speed effect of applying CPET. Therefore, we focus on answering
    the following questions to illustrate the benefits of CPET.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步讨论 CPET 设计的有效性，理解 CPET 中的不同机制如何帮助恢复模型压缩导致的知识丢失、参数数量是否是性能提升的主要原因以及应用 CPET
    的速度效应至关重要。因此，我们专注于回答以下问题以说明 CPET 的优势。
- en: To more clearly demonstrate the inner mechanisms of CPET, we conduct ablation
    studies based on the compressed model using the mixture of compression methods
    with different settings, and adopt RTE for evaluation. In the settings of CPET,
    the intermediate ranks of LoRA and Recovery modules are 8\. LoRA modules are injected
    into linear transformations of $\mathbf{W^{Q}}$ in attention layers. Recovery
    modules are injected into all linear transformations in both attention and feed-forward
    layers.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地展示 CPET 的内在机制，我们基于使用不同设置的压缩方法混合的压缩模型进行消融研究，并采用 RTE 进行评估。在 CPET 的设置中，LoRA
    和恢复模块的中间排名为 8。LoRA 模块被注入到注意力层中 $\mathbf{W^{Q}}$ 的线性变换中。恢复模块被注入到注意力层和前馈层中的所有线性变换中。
- en: How do different mechanisms in CPET help restore the knowledge loss caused by
    model compression methods?
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: CPET 中的不同机制如何帮助恢复模型压缩方法导致的知识丢失？
- en: 'To evaluate the effectiveness of each mechanism used in CPET, we test all possible
    combinations of inheritance, recovery, and distillation. We can find that from
    Table [3](#S4.T3 "Table 3 ‣ 4.4 Ablation Studies ‣ 4 Experiments and Analyses
    ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models"):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估 CPET 中使用的每种机制的有效性，我们测试了继承、恢复和蒸馏的所有可能组合。我们可以从表 [3](#S4.T3 "Table 3 ‣ 4.4
    Ablation Studies ‣ 4 Experiments and Analyses ‣ CPET: Effective Parameter-Efficient
    Tuning for Compressed Large Language Models") 中发现：'
- en: (1) PET knowledge inheritance is effective. By initializing the tunable parameters
    with the PET modules trained on the non-compressed LLM, the performance of combining
    the final PET modules and the compressed LLM has been significantly improved.
    This indicates that in the optimization space of the compressed LLM, it is difficult
    to obtain the optimal task-specific parameters of PET modules based on random
    initialization, but more optimal PET modules can be achieved more easily using
    PET knowledge inheritance.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: (1) PET 知识继承是有效的。通过将可调节的参数初始化为在未压缩 LLM 上训练的 PET 模块，结合最终的 PET 模块和压缩 LLM 的性能得到了显著提升。这表明在压缩
    LLM 的优化空间中，基于随机初始化很难获得 PET 模块的最佳任务特定参数，但通过 PET 知识继承可以更容易地获得更优化的 PET 模块。
- en: (2) Simply adding recovery modules brings a certain level of performance improvements
    in some circumstances. However, it can achieve further performance improvements
    by adopting our distillation strategy. This suggests combining recovery modules
    with the knowledge distillation strategy to enhance PET modules is necessary.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 简单地添加恢复模块在某些情况下带来了一定的性能提升。然而，通过采用我们的蒸馏策略可以实现进一步的性能改进。这表明，将恢复模块与知识蒸馏策略结合以增强
    PET 模块是必要的。
- en: Is the improvement only caused by increased parameters?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这种改进是否仅仅是由于参数增加所导致的？
- en: 'To answer this question, we carry out the study in Table [4](#S4.T4 "Table
    4 ‣ 4.4 Ablation Studies ‣ 4 Experiments and Analyses ‣ CPET: Effective Parameter-Efficient
    Tuning for Compressed Large Language Models"). Each setting in this study maintains
    the same number of parameters as CPET. In CPET, the tunable parameters consist
    of PET modules and recovery modules. We introduce new settings of LoRA+LoRA and
    Large LoRA, which trivially increase the tunable parameters by adding more LoRA
    modules or using larger LoRA modules. Considering that recovery modules can be
    regarded as functionally specialized PET modules, we further test Rec+Rec (use
    recovery modules instead of LoRA) and Large Rec (Larger recovery modules with
    the same parameter number). More specifically, in LoRA+LoRA settings, we first
    inject LoRA modules to $\mathbf{W^{Q}}$ to larger modules whose intermediate rank
    is 16. Rec+Rec settings are similar to LoRA+LoRA settings, except for replacing
    all LoRA modules with recovery modules (intermediate ranks are 8). Large Rec settings
    are similar to Large LoRA settings.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '为了回答这个问题，我们在表格 [4](#S4.T4 "Table 4 ‣ 4.4 Ablation Studies ‣ 4 Experiments and
    Analyses ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large Language
    Models")中进行了研究。该研究中的每个设置都保持与CPET相同数量的参数。在CPET中，可调参数包括PET模块和恢复模块。我们引入了LoRA+LoRA和Large
    LoRA的新设置，这些设置通过添加更多的LoRA模块或使用更大的LoRA模块来增加可调参数。考虑到恢复模块可以被视为功能上专门化的PET模块，我们进一步测试了Rec+Rec（使用恢复模块代替LoRA）和Large
    Rec（参数数量相同的更大恢复模块）。更具体来说，在LoRA+LoRA设置中，我们首先将LoRA模块注入到$\mathbf{W^{Q}}$中，目标是较大的模块，其中间秩为16。Rec+Rec设置与LoRA+LoRA设置类似，只是将所有LoRA模块替换为恢复模块（中间秩为8）。Large
    Rec设置类似于Large LoRA设置。'
- en: 'From Table [4](#S4.T4 "Table 4 ‣ 4.4 Ablation Studies ‣ 4 Experiments and Analyses
    ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models"),
    we find that adding more parameters makes marginal improvements, showing that
    only adding more tunable parameters cannot bridge the knowledge gap caused by
    model compression.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '从表格 [4](#S4.T4 "Table 4 ‣ 4.4 Ablation Studies ‣ 4 Experiments and Analyses
    ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models")中，我们发现增加更多参数只带来边际改进，这表明仅仅增加更多的可调参数不能弥补模型压缩带来的知识缺口。'
- en: Does adding more parameters slow down model inference?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 增加更多的参数会减慢模型推理速度吗？
- en: 'Since CPET introduces a little more parameters than conventional PET methods,
    it has a potential cause of slower inference. Given that the inference speed of
    compressed models are highly related to implementation, we evaluate CPET and conventional
    PET methods with the same backbone on the same platform to avoid uncertainties
    caused by backbone model implementation. We report the average time and corresponding
    standard deviation of each model call. Table [5](#S4.T5 "Table 5 ‣ 4.4 Ablation
    Studies ‣ 4 Experiments and Analyses ‣ CPET: Effective Parameter-Efficient Tuning
    for Compressed Large Language Models") shows that the inference time of CPET and
    PET methods are almost the same, proving that the time bottleneck of inference
    lies in the model itself, while the extra parameters added due to CPET do not
    slow down model inference.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '由于CPET引入了比传统PET方法略多的参数，它可能会导致推理速度变慢。鉴于压缩模型的推理速度与实现高度相关，我们在相同平台上使用相同的主干模型来评估CPET和传统PET方法，以避免由于主干模型实现造成的不确定性。我们报告了每个模型调用的平均时间及其相应的标准偏差。表格 [5](#S4.T5
    "Table 5 ‣ 4.4 Ablation Studies ‣ 4 Experiments and Analyses ‣ CPET: Effective
    Parameter-Efficient Tuning for Compressed Large Language Models")显示CPET和PET方法的推理时间几乎相同，证明了推理的时间瓶颈在于模型本身，而由于CPET而增加的额外参数不会减慢模型推理。'
- en: From the ablations above, we prove that each mechanism introduced in CPET is
    helpful to recover performance degradation of model compression without retarding
    model inference. The structural design of CPET is reasonable, which is beneficial
    to knowledge inheritance and recovery.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述消融实验中，我们证明了CPET引入的每个机制都有助于恢复模型压缩带来的性能下降，而不会减缓模型推理。CPET的结构设计是合理的，有利于知识的继承和恢复。
- en: '| Inherit | Recover | Distill | RTE Acc(%) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 继承 | 恢复 | 提炼 | RTE准确率(%) |'
- en: '|  |  |  | 83.6 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 83.6 |'
- en: '| ✓ |  |  | 85.4 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ✓ |  |  | 85.4 |'
- en: '|  | ✓ |  | 82.5 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | ✓ |  | 82.5 |'
- en: '|  |  | ✓ | 82.5 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ✓ | 82.5 |'
- en: '| ✓ | ✓ |  | 87.5 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ |  | 87.5 |'
- en: '| ✓ |  | ✓ | 85.7 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| ✓ |  | ✓ | 85.7 |'
- en: '|  | ✓ | ✓ | 86.1 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | ✓ | ✓ | 86.1 |'
- en: '| ✓ | ✓ | ✓ | 88.6 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✓ | 88.6 |'
- en: 'Table 3: The ablation studies on RTE (%). When eliminating inheritance, we
    keep the LoRA modules injected and train these modules from scratch. When eliminating
    recovery, we simply remove the recovery modules. When eliminating distillation,
    the training loss function only consists of the task loss function without the
    distillation loss function. These studies use the mixutre of multiple compression
    methods same as [4.3](#S4.SS3 "4.3 The Overall Performance of CPET ‣ 4 Experiments
    and Analyses ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large
    Language Models") to compress the LLM.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：RTE (%) 的消融研究。去除继承时，我们保留注入的 LoRA 模块，并从头开始训练这些模块。去除恢复时，我们简单地去除恢复模块。去除蒸馏时，训练损失函数仅包含任务损失函数，而不包含蒸馏损失函数。这些研究使用了与[4.3](#S4.SS3
    "4.3 CPET 的总体性能 ‣ 4 实验和分析 ‣ CPET：针对压缩大语言模型的有效参数高效调整")相同的多种压缩方法来压缩 LLM。
- en: '| Setting | RTE Acc(%) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | RTE 准确率 (%) |'
- en: '| LoRA | 83.6 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 83.6 |'
- en: '| LoRA+LoRA | 83.9 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| LoRA+LoRA | 83.9 |'
- en: '| Large LoRA | 84.6 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 大型 LoRA | 84.6 |'
- en: '| Rec+Rec | 85.0 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Rec+Rec | 85.0 |'
- en: '| Large Rec | 84.3 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 大型 Rec | 84.3 |'
- en: '| CPET | 88.6 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| CPET | 88.6 |'
- en: 'Table 4: The ablation studies with parameter quantity controlled. Each setting
    has the same amount of tunable parameters. “LoRA” represents the conventional
    LoRA module with the same size in CPET. “Rec” represents the recovery module with
    the same size in CPET. “Large” prefix means doubling the size, which equals to
    the size of the whole CPET. These studies use the mixutre of multiple compression
    methods same as [4.3](#S4.SS3 "4.3 The Overall Performance of CPET ‣ 4 Experiments
    and Analyses ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large
    Language Models") to compress the LLM.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：参数数量控制下的消融研究。每种设置具有相同数量的可调参数。“LoRA”代表 CPET 中的传统 LoRA 模块，大小相同。“Rec”代表 CPET
    中的恢复模块，大小相同。“Large”前缀表示尺寸翻倍，相当于整个 CPET 的大小。这些研究使用了与[4.3](#S4.SS3 "4.3 CPET 的总体性能
    ‣ 4 实验和分析 ‣ CPET：针对压缩大语言模型的有效参数高效调整")相同的多种压缩方法来压缩 LLM。
- en: '| Method | # Param | Avg. Time (ms) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | # Param | 平均时间 (毫秒) |'
- en: '| CLM+PET | 10M | (9761$\pm$17) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| CLM+PET | 10M | (9761$\pm$17) |'
- en: '| CLM+CPET | 60M | (9526$\pm$70) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| CLM+CPET | 60M | (9526$\pm$70) |'
- en: 'Table 5: The average inference time. “CLM” here represents the compressed T5-3b
    using mixture of compression methods. “# Param” represents the additional parameters
    compared with the backbone model. The average time and corresponding standard
    deviations represent the time taken for each model call.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：平均推理时间。“CLM”在这里代表使用多种压缩方法的压缩 T5-3b。“# Param”代表与基础模型相比的额外参数。平均时间和对应的标准差表示每次模型调用所需的时间。
- en: 4.5 The Convergence of CPET
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 CPET 的收敛性
- en: Although we are primarily concerned with the final inference speed and performance
    after training, the method usability may be compromised if the training process
    spends too much time. Therefore, based on four compressed LLMs, we compare the
    convergence speed of tuning PET modules to handle the BoolQ dataset.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们主要关注训练后的最终推理速度和性能，但如果训练过程花费过多时间，方法的可用性可能会受到影响。因此，基于四种压缩 LLM，我们比较了调整 PET
    模块以处理 BoolQ 数据集的收敛速度。
- en: 'From Figure [3](#S4.F3 "Figure 3 ‣ 4.3 The Overall Performance of CPET ‣ 4
    Experiments and Analyses ‣ CPET: Effective Parameter-Efficient Tuning for Compressed
    Large Language Models"), we can find that due to our PET knowledge inheritance
    mechanism, CPET is superior to the vanilla PET methods in terms of convergence
    speed and final results. Adding the recovery module will not affect the convergence
    speed. Furthermore, when quantization, unstructured pruning, or MoEfication is
    used, the inheritance mechanism gives a better starting point for tuning PET modules.
    While in the case of structured pruning, even though the initial point of CPET does
    not work well on tasks, it is closer to the optimal point in the optimization
    space and converges faster.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 [3](#S4.F3 "图 3 ‣ 4.3 CPET 的总体性能 ‣ 4 实验和分析 ‣ CPET：针对压缩大语言模型的有效参数高效调整") 中，我们可以发现，由于我们的
    PET 知识继承机制，CPET 在收敛速度和最终结果方面优于传统 PET 方法。添加恢复模块不会影响收敛速度。此外，当使用量化、无结构修剪或 MoEfication
    时，继承机制为调整 PET 模块提供了更好的起点。而在结构化修剪的情况下，尽管 CPET 的初始点在任务上效果不佳，但它在优化空间中更接近最优点，并且收敛更快。
- en: 'Moreover, considering the existence of numerous downstream tasks, the PET modules
    based on a unified LLM may be trained by community users on their own private
    data and then uploaded to the Internet. When adapting these PET modules to a compressed
    LLM, there may not be any task-specific data available for the adaptation process.
    Intuitively, applying PET parameters trained on one model to another requires
    adaptation using additional data. Surprisingly, from Figure [3](#S4.F3 "Figure
    3 ‣ 4.3 The Overall Performance of CPET ‣ 4 Experiments and Analyses ‣ CPET: Effective
    Parameter-Efficient Tuning for Compressed Large Language Models"), we can find
    that when quantization or MoEfication is used, we can achieve ideal results without
    using any data for adaptation, by only using the PET inheritance mechanism we
    proposed.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，考虑到存在大量的下游任务，基于统一 LLM 的 PET 模块可以由社区用户在其私人数据上进行训练，然后上传到互联网。当将这些 PET 模块适配到压缩
    LLM 时，可能没有任何特定任务的数据可用于适配过程。直观地说，将在一个模型上训练的 PET 参数应用到另一个模型上需要使用额外的数据进行适配。令人惊讶的是，从图
    [3](#S4.F3 "Figure 3 ‣ 4.3 The Overall Performance of CPET ‣ 4 Experiments and
    Analyses ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large Language
    Models") 中可以发现，当使用量化或 MoEfication 时，我们仅通过使用我们提出的 PET 继承机制，就能在不使用任何数据进行适配的情况下实现理想的结果。'
- en: 4.6 Instruction Tuning with CPET
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 CPET 的指令微调
- en: 'The above experimental results have proven the strength of CPET for specific
    downstream tasks. In this section, we further investigate the effectiveness of
    CPET when applied CPET to a more general scenario — instruction tuning. We select
    Alpaca Taori et al. ([2023](#bib.bib53)) as the instruction tuning dataset and
    use 5-shot MMLU (containing 57 subtasks) Hendrycks et al. ([2020](#bib.bib24))
    as the evaluation benchmark. We compress LLaMA-13b Touvron et al. ([2023](#bib.bib54))
    into 8-bit quantization version. Since LLaMA use SwiGLU Ramachandran et al. ([2017](#bib.bib46))
    as its activation function, which makes LLaMA not sparse enough to adopt MoEfication
    to compress itself, we thus do not adopt MoEfication to compress LLaMA-13b. More
    details of the LLaMA experiments can be found in Appendix [A](#A1 "Appendix A
    The Experimental Settings of LLaMA ‣ CPET: Effective Parameter-Efficient Tuning
    for Compressed Large Language Models"). From Table [6](#S4.T6 "Table 6 ‣ 4.6 Instruction
    Tuning with CPET ‣ 4 Experiments and Analyses ‣ CPET: Effective Parameter-Efficient
    Tuning for Compressed Large Language Models"), we can find that CPET combined
    with quantization can still exhibit strong performance while achieving faster
    inference speed. In future work, we will explore better recovery solutions orienting
    pruning methods to make CPET more effective on those general tasks like instruction
    tuning. As compared with the current competitive methods that combine LoRA and
    quantized LLaMA models, CPET achieves better performance and exhibits versatility
    on different compression models.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '上述实验结果已证明了 CPET 在特定下游任务中的强大性能。在本节中，我们进一步探讨了将 CPET 应用于更通用场景——指令微调时的有效性。我们选择了
    Alpaca Taori 等人 ([2023](#bib.bib53)) 作为指令微调数据集，并使用 5-shot MMLU（包含 57 个子任务）Hendrycks
    等人 ([2020](#bib.bib24)) 作为评估基准。我们将 LLaMA-13b Touvron 等人 ([2023](#bib.bib54)) 压缩为
    8-bit 量化版本。由于 LLaMA 使用 SwiGLU Ramachandran 等人 ([2017](#bib.bib46)) 作为其激活函数，这使得
    LLaMA 不够稀疏而无法采用 MoEfication 进行压缩，因此我们没有采用 MoEfication 来压缩 LLaMA-13b。有关 LLaMA 实验的更多细节请参见附录
    [A](#A1 "Appendix A The Experimental Settings of LLaMA ‣ CPET: Effective Parameter-Efficient
    Tuning for Compressed Large Language Models")。从表 [6](#S4.T6 "Table 6 ‣ 4.6 Instruction
    Tuning with CPET ‣ 4 Experiments and Analyses ‣ CPET: Effective Parameter-Efficient
    Tuning for Compressed Large Language Models") 中可以看出，结合量化的 CPET 仍然能表现出强大的性能，同时实现更快的推理速度。在未来的工作中，我们将探索更好的恢复解决方案，针对剪枝方法，使
    CPET 在像指令微调这样的通用任务上更加有效。与当前将 LoRA 和量化的 LLaMA 模型相结合的竞争方法相比，CPET 实现了更好的性能，并在不同的压缩模型上展示了多样性。'
- en: '| Model | MMLU(5-shot) | Ideal Speedup |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | MMLU(5-shot) | 理想加速比 |'
- en: '| LLaMA-13b^∗ | 46.9 | 100% |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-13b^∗ | 46.9 | 100% |'
- en: '| Q^∗ | 46.6 | 200% |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Q^∗ | 46.6 | 200% |'
- en: '| Dettmers et al. ([2023](#bib.bib14))$\dagger$ | 47.5 | 100% |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Dettmers 等人 ([2023](#bib.bib14))$\dagger$ | 47.5 | 100% |'
- en: '| Liu et al. ([2023](#bib.bib36))$\dagger$ | 46.7 | 200% |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 ([2023](#bib.bib36))$\dagger$ | 46.7 | 200% |'
- en: '| Q + CPET | 48.3 | 200% |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Q + CPET | 48.3 | 200% |'
- en: 'Table 6: The 5-shot MMLU performance based on different compressed LLaMA models
    (%). “Q” represents 8-bit quantization. “^∗” represents the MMLU performance without
    training on Alpaca. “$\dagger$” are the current competitive methods that train
    LoRA on the quantized LLaMA-13b model.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：基于不同压缩 LLaMA 模型的 5-shot MMLU 性能（%）。“Q”表示 8-bit 量化。“^∗”表示未在 Alpaca 上训练的 MMLU
    性能。“$\dagger$”为当前在量化的 LLaMA-13b 模型上训练 LoRA 的竞争方法。
- en: 5 Conclusion
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we propose an effective PET framework based on the compressed
    LLM (named CPET) to further reduce the resource requirements and inference time
    when deploying LLM and PET modules to serve downstream tasks. Considering task-agnostic
    compression methods may cause losing some task-specific knowledge, we introduce
    PET knowledge inheritance and model knowledge recovery to restore the lost knowledge.
    By inheriting the prior task knowledge of the PET modules learned on the non-compressed
    LLM, searching for the optimal PET modules for the compressed LLM becomes easier.
    Moreover, by introducing knowledge recovery modules to recover task-specific capabilities
    lost in the compression phase, collaborating PET modules with the compressed LLM
    can achieve comparable performance to those PET modules based on the non-compressed
    LLM. The experimental results show that CPET can outperform baselines based on
    the compressed LLM, and meanwhile, CPET maintains the advantages of PET methods
    for multi-task serving. This paper mainly accelerates the inference of PET methods
    and LLMs. We leave the computation bottleneck of LLMs in the tuning process as
    future work.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种基于压缩LLM的有效PET框架（称为CPET），以进一步减少部署LLM和PET模块服务下游任务时的资源需求和推理时间。考虑到任务无关的压缩方法可能会导致丢失一些任务特定的知识，我们引入了PET知识继承和模型知识恢复以恢复丢失的知识。通过继承在非压缩LLM上学习的PET模块的先前任务知识，寻找压缩LLM的最优PET模块变得更加容易。此外，通过引入知识恢复模块以恢复在压缩阶段丢失的任务特定能力，协同PET模块与压缩LLM可以实现与基于非压缩LLM的PET模块相媲美的性能。实验结果表明，CPET可以超越基于压缩LLM的基线，同时CPET保持了PET方法在多任务服务中的优势。本文主要加速了PET方法和LLMs的推理。我们将LLMs在调整过程中的计算瓶颈留作未来工作。
- en: 6 Limitations
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制
- en: Our work focuses on the efficiency and effectiveness of model serving, but it
    requires a small amount of extra training time. In this paper, we only choose
    LoRA as a representative of PET methods. In fact, our framework can be applied
    to any PET method. The compression method adopted in this paper does not change
    the number of layers of the LLM. However, for those compression methods that change
    the hidden dimensions of the model, how to transfer the knowledge of PET modules
    on the non-compressed LLM remains an open problem for our future work.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作集中在模型服务的效率和有效性上，但它需要少量额外的训练时间。在本文中，我们只选择了LoRA作为PET方法的代表。实际上，我们的框架可以应用于任何PET方法。本文采用的压缩方法不改变LLM的层数。然而，对于那些改变模型隐藏维度的压缩方法，如何转移非压缩LLM上PET模块的知识仍然是我们未来工作中的一个未解问题。
- en: References
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bai et al. (2021) Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin
    Jiang, Qun Liu, Michael Lyu, and Irwin King. 2021. [Binarybert: Pushing the limit
    of bert quantization](https://doi.org/10.18653/v1/2021.acl-long.334). In *Proceedings
    of ACL/IJCNLP*, pages 4334–4348.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai et al. (2021) Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin
    Jiang, Qun Liu, Michael Lyu, 和 Irwin King. 2021. [Binarybert: 推动BERT量化的极限](https://doi.org/10.18653/v1/2021.acl-long.334).
    收录于 *ACL/IJCNLP 会议论文集*，第4334–4348页。'
- en: 'Ben Zaken et al. (2022) Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.
    2022. [BitFit: Simple parameter-efficient fine-tuning for transformer-based masked
    language-models](https://aclanthology.org/2022.acl-short.1/). In *Proceedings
    of ACL*, pages 1–9.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ben Zaken et al. (2022) Elad Ben Zaken, Yoav Goldberg, 和 Shauli Ravfogel. 2022.
    [BitFit: 简单的参数高效微调用于基于变换器的掩码语言模型](https://aclanthology.org/2022.acl-short.1/).
    收录于 *ACL 会议论文集*，第1–9页。'
- en: Bentivogli et al. (2009) Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang
    Dang, and Danilo Giampiccolo. 2009. [The fifth PASCAL recognizing textual entailment
    challenge](https://tac.nist.gov/publications/2009/additional.papers/RTE5_overview.proceedings.pdf).
    In *Proceedings of TAC*.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bentivogli et al. (2009) Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa
    Trang Dang, 和 Danilo Giampiccolo. 2009. [第五届PASCAL文本蕴含识别挑战](https://tac.nist.gov/publications/2009/additional.papers/RTE5_overview.proceedings.pdf).
    收录于 *TAC 会议论文集*。
- en: 'Black et al. (2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony,
    Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang,
    et al. 2022. [Gpt-neox-20b: An open-source autoregressive language model](https://arxiv.org/abs/2204.06745).
    *arXiv preprint arXiv:2204.06745*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Black et al. (2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony,
    Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang,
    等. 2022. [Gpt-neox-20b: 一种开源自回归语言模型](https://arxiv.org/abs/2204.06745). *arXiv
    预印本 arXiv:2204.06745*。'
- en: Bommasani et al. (2021) Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman,
    Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut,
    Emma Brunskill, et al. 2021. [On the opportunities and risks of foundation models](https://arxiv.org/abs/2108.07258).
    *arXiv preprint arXiv:2108.07258*.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bommasani et al. (2021) Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman,
    Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut,
    Emma Brunskill, 等. 2021. [关于基础模型的机会与风险](https://arxiv.org/abs/2108.07258)。*arXiv
    预印本 arXiv:2108.07258*。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. [Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).
    In *Proceedings of NeurIPS*, volume 33, pages 1877–1901.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等. 2020. [语言模型是少量学习者](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)。在
    *NeurIPS 论文集*，第 33 卷，第 1877–1901 页。
- en: Chen et al. (2020) Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,
    Yang Zhang, Zhangyang Wang, and Michael Carbin. 2020. [The lottery ticket hypothesis
    for pre-trained bert networks](https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf).
    In *Proceedings of NeurIPS*, volume 33, pages 15834–15846.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2020) Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,
    Yang Zhang, Zhangyang Wang, 和 Michael Carbin. 2020. [预训练 BERT 网络的乐透票假设](https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf)。在
    *NeurIPS 论文集*，第 33 卷，第 15834–15846 页。
- en: 'Chen et al. (2023) Tianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, and
    Luming Liang. 2023. [Lorashear: Efficient large language model structured pruning
    and knowledge recovery](https://arxiv.org/abs/2310.18356). *arXiv preprint arXiv:2310.18356*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) Tianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, 和 Luming
    Liang. 2023. [Lorashear：高效的大型语言模型结构修剪与知识恢复](https://arxiv.org/abs/2310.18356)。*arXiv
    预印本 arXiv:2310.18356*。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. [Palm: Scaling language modeling with pathways](https://arxiv.org/abs/2204.02311).
    *arXiv preprint arXiv:2204.02311*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, 等. 2022. [Palm：通过路径扩展语言建模](https://arxiv.org/abs/2204.02311)。*arXiv
    预印本 arXiv:2204.02311*。
- en: 'Church et al. (2021) Kenneth Ward Church, Zeyu Chen, and Yanjun Ma. 2021. [Emerging
    trends: A gentle introduction to fine-tuning](https://www.cambridge.org/core/journals/natural-language-engineering/article/emerging-trends-a-gentle-introduction-to-finetuning/C31D429D0928351D6A6692F8ECD1E7ED).
    *Natural Language Engineering*, 27(6):763–778.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Church et al. (2021) Kenneth Ward Church, Zeyu Chen, 和 Yanjun Ma. 2021. [新兴趋势：细调的温和介绍](https://www.cambridge.org/core/journals/natural-language-engineering/article/emerging-trends-a-gentle-introduction-to-finetuning/C31D429D0928351D6A6692F8ECD1E7ED)。*自然语言工程*，27(6):763–778。
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. 2019. [Boolq: Exploring the surprising
    difficulty of natural yes/no questions](https://aclanthology.org/N19-1300/). In
    *Proceedings of NAACL*.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, 和 Kristina Toutanova. 2019. [Boolq：探索自然是/否问题的惊人难度](https://aclanthology.org/N19-1300/)。在
    *NAACL 论文集*。
- en: Dai et al. (2021) Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2021.
    [Knowledge neurons in pretrained transformers](https://arxiv.org/abs/2104.08696).
    *arXiv preprint arXiv:2104.08696*.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai et al. (2021) Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, 和 Furu Wei. 2021.
    [预训练变换器中的知识神经元](https://arxiv.org/abs/2104.08696)。*arXiv 预印本 arXiv:2104.08696*。
- en: 'De Marneffe et al. (2019) Marie-Catherine De Marneffe, Mandy Simons, and Judith
    Tonhauser. 2019. [The commitmentbank: Investigating projection in naturally occurring
    discourse](https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601).
    In *Proceedings of Sinn und Bedeutung*, volume 23, pages 107–124.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Marneffe et al. (2019) Marie-Catherine De Marneffe, Mandy Simons, 和 Judith
    Tonhauser. 2019. [承诺银行：调查自然发生话语中的投射](https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601)。在
    *Sinn und Bedeutung 论文集*，第 23 卷，第 107–124 页。
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. [Qlora: Efficient finetuning of quantized llms](https://arxiv.org/abs/2305.14314).
    *arXiv preprint arXiv:2305.14314*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke
    Zettlemoyer. 2023. [Qlora：高效微调量化的大型语言模型](https://arxiv.org/abs/2305.14314)。*arXiv
    预印本 arXiv:2305.14314*。
- en: Ding et al. (2023) Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang,
    Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023. [Parameter-efficient
    fine-tuning of large-scale pre-trained language models](https://www.nature.com/articles/s42256-023-00626-4).
    *Nature Machine Intelligence*, pages 1–16.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等人（2023）Ning Ding、Yujia Qin、Guang Yang、Fuchao Wei、Zonghan Yang、Yusheng
    Su、Shengding Hu、Yulin Chen、Chi-Min Chan、Weize Chen 等。2023年。[大规模预训练语言模型的参数高效微调](https://www.nature.com/articles/s42256-023-00626-4)。*自然机器智能*，第1–16页。
- en: Dolan and Brockett (2005) William B. Dolan and Chris Brockett. 2005. [Automatically
    constructing a corpus of sentential paraphrases](https://aclanthology.org/I05-5002).
    In *Proceedings of the Third International Workshop on Paraphrasing (IWP2005)*.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dolan 和 Brockett（2005）William B. Dolan 和 Chris Brockett。2005年。[自动构建句子释义语料库](https://aclanthology.org/I05-5002)。在
    *第三届国际释义研讨会（IWP2005）论文集* 中。
- en: 'Driess et al. (2023) Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, et al. 2023. [Palm-e: An embodied multimodal language model](https://arxiv.org/abs/2303.03378).
    *arXiv preprint arXiv:2303.03378*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Driess 等人（2023）Danny Driess、Fei Xia、Mehdi SM Sajjadi、Corey Lynch、Aakanksha Chowdhery、Brian
    Ichter、Ayzaan Wahid、Jonathan Tompson、Quan Vuong、Tianhe Yu 等。2023年。[Palm-e：一种具身的多模态语言模型](https://arxiv.org/abs/2303.03378)。*arXiv
    预印本 arXiv:2303.03378*。
- en: Fan et al. (2020) Angela Fan, Edouard Grave, and Armand Joulin. 2020. [Reducing
    transformer depth on demand with structured dropout](https://openreview.net/forum?id=SylO2yStDr).
    In *Proceedings of ICLR*.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人（2020）Angela Fan、Edouard Grave 和 Armand Joulin。2020年。[按需减少变换器深度的结构性丢弃](https://openreview.net/forum?id=SylO2yStDr)。在
    *ICLR 会议论文集* 中。
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2023. [Gptq: Accurate quantization for generative pre-trained transformers](https://arxiv.org/abs/2210.17323).
    In *Proceedings of ICLR*.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等人（2023）Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh。2023年。[Gptq：生成预训练变换器的精确量化](https://arxiv.org/abs/2210.17323)。在
    *ICLR 会议论文集* 中。
- en: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. 2021. [A framework for few-shot language model evaluation](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人（2021）Leo Gao、Jonathan Tow、Stella Biderman、Sid Black、Anthony DiPofi、Charles
    Foster、Laurence Golding、Jeffrey Hsu、Kyle McDonell、Niklas Muennighoff、Jason Phang、Laria
    Reynolds、Eric Tang、Anish Thite、Ben Wang、Kevin Wang 和 Andy Zou。2021年。[少样本语言模型评估框架](https://doi.org/10.5281/zenodo.5371628)。
- en: Google (2023) Google. 2023. [Google ai palm 2](https://ai.google/discover/palm2).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google（2023）Google。2023年。[Google ai palm 2](https://ai.google/discover/palm2)。
- en: Han et al. (2015) Song Han, Jeff Pool, John Tran, and William J. Dally. 2015.
    [Learning both weights and connections for efficient neural network](https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html).
    In *Proceedings of NeurIPS*, pages 1135–1143.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等人（2015）Song Han、Jeff Pool、John Tran 和 William J. Dally。2015年。[学习权重和连接以提高神经网络效率](https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html)。在
    *NeurIPS 会议论文集* 中，第1135–1143页。
- en: 'Han et al. (2021) Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi
    Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang, Liang Zhang, et al. 2021. [Pre-trained
    models: Past, present and future](https://www.sciencedirect.com/science/article/pii/S2666651021000231).
    *AI Open*, 2:225–250.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等人（2021）Xu Han、Zhengyan Zhang、Ning Ding、Yuxian Gu、Xiao Liu、Yuqi Huo、Jiezhong
    Qiu、Yuan Yao、Ao Zhang、Liang Zhang 等。2021年。[预训练模型：过去、现在和未来](https://www.sciencedirect.com/science/article/pii/S2666651021000231)。*AI
    Open*，2:225–250。
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. [Measuring massive multitask
    language understanding](https://arxiv.org/abs/2009.03300). *arXiv preprint arXiv:2009.03300*.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人（2020）Dan Hendrycks、Collin Burns、Steven Basart、Andy Zou、Mantas Mazeika、Dawn
    Song 和 Jacob Steinhardt。2020年。[测量大规模多任务语言理解](https://arxiv.org/abs/2009.03300)。*arXiv
    预印本 arXiv:2009.03300*。
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. [Distilling
    the knowledge in a neural network](https://arxiv.org/abs/1503.02531). *arXiv preprint
    arXiv:1503.02531*.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等人（2015）Geoffrey Hinton、Oriol Vinyals 和 Jeff Dean。2015年。[提取神经网络中的知识](https://arxiv.org/abs/1503.02531)。*arXiv
    预印本 arXiv:1503.02531*。
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. 2019. [Parameter-efficient transfer learning for nlp](http://proceedings.mlr.press/v97/houlsby19a.html).
    In *Proceedings of ICML*, pages 2790–2799\. PMLR.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby 等（2019）Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone,
    Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan 和 Sylvain Gelly. 2019.
    [针对自然语言处理的参数高效迁移学习](http://proceedings.mlr.press/v97/houlsby19a.html). 见于 *ICML
    会议论文集*，第 2790–2799 页。PMLR。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. [Lora: Low-rank adaptation
    of large language models](https://arxiv.org/abs/2106.09685). *arXiv preprint arXiv:2106.09685*.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2021）Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang 和 Weizhu Chen. 2021. [Lora：大规模语言模型的低秩适配](https://arxiv.org/abs/2106.09685).
    *arXiv 预印本 arXiv:2106.09685*。
- en: Jawahar et al. (2019) Ganesh Jawahar, Benoît Sagot, and Djamé Seddah. 2019.
    [What does bert learn about the structure of language?](https://www.aclweb.org/anthology/P19-1356)
    In *Proceedings of ACL*, pages 3651–3657.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jawahar 等（2019）Ganesh Jawahar, Benoît Sagot 和 Djamé Seddah. 2019. [BERT 对语言结构的学习是什么？](https://www.aclweb.org/anthology/P19-1356)
    见于 *ACL 会议论文集*，第 3651–3657 页。
- en: 'Jiao et al. (2020) Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
    Linlin Li, Fang Wang, and Qun Liu. 2020. [TinyBERT: Distilling BERT for natural
    language understanding](https://aclanthology.org/2020.findings-emnlp.372). In
    *Findings of EMNLP*, pages 4163–4174.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiao 等（2020）Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin
    Li, Fang Wang 和 Qun Liu. 2020. [TinyBERT：为自然语言理解提炼 BERT](https://aclanthology.org/2020.findings-emnlp.372).
    见于 *EMNLP 会议论文集*，第 4163–4174 页。
- en: 'Lepikhin et al. (2021) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao
    Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam M. Shazeer, and Z. Chen.
    2021. [Gshard: Scaling giant models with conditional computation and automatic
    sharding](https://openreview.net/forum?id=qrwe7XHTmYb). In *Proceedings of ICLR*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lepikhin 等（2021）Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan
    Firat, Yanping Huang, Maxim Krikun, Noam M. Shazeer 和 Z. Chen. 2021. [Gshard：通过条件计算和自动分片扩展巨型模型](https://openreview.net/forum?id=qrwe7XHTmYb).
    见于 *ICLR 会议论文集*。
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. [The
    power of scale for parameter-efficient prompt tuning](https://arxiv.org/abs/2104.08691).
    *arXiv preprint arXiv:2104.08691*.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester 等（2021）Brian Lester, Rami Al-Rfou 和 Noah Constant. 2021. [参数高效提示调优的规模效应](https://arxiv.org/abs/2104.08691).
    *arXiv 预印本 arXiv:2104.08691*。
- en: Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan
    Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,
    Theo Gutman-Solo, et al. 2022. [Solving quantitative reasoning problems with language
    models](https://arxiv.org/abs/2206.14858). *arXiv preprint arXiv:2206.14858*.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewkowycz 等（2022）Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer,
    Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo
    Gutman-Solo 等. 2022. [使用语言模型解决定量推理问题](https://arxiv.org/abs/2206.14858). *arXiv
    预印本 arXiv:2206.14858*。
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. [Prefix-tuning: Optimizing
    continuous prompts for generation](https://aclanthology.org/2021.acl-long.353/).
    In *Proceedings of ACL-IJCNLP*, pages 4582–4597.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Liang（2021）Xiang Lisa Li 和 Percy Liang. 2021. [前缀调优：优化生成的连续提示](https://aclanthology.org/2021.acl-long.353/).
    见于 *ACL-IJCNLP 会议论文集*，第 4582–4597 页。
- en: 'Liang et al. (2021) Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and
    Xiaotong Zhang. 2021. [Pruning and quantization for deep neural network acceleration:
    A survey](https://www.sciencedirect.com/science/article/abs/pii/S0925231221010894).
    *Neurocomputing*, 461:370–403.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等（2021）Tailin Liang, John Glossner, Lei Wang, Shaobo Shi 和 Xiaotong Zhang.
    2021. [深度神经网络加速的剪枝与量化：综述](https://www.sciencedirect.com/science/article/abs/pii/S0925231221010894).
    *神经计算*，461：370–403。
- en: Liu et al. (2022) Chang Liu, Chongyang Tao, Jiazhan Feng, and Dongyan Zhao.
    2022. [Multi-granularity structural knowledge distillation for language model
    compression](https://aclanthology.org/2022.acl-long.71). In *Proceedings of ACL*,
    pages 1001–1011.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2022）Chang Liu, Chongyang Tao, Jiazhan Feng 和 Dongyan Zhao. 2022. [用于语言模型压缩的多粒度结构知识蒸馏](https://aclanthology.org/2022.acl-long.71).
    见于 *ACL 会议论文集*，第 1001–1011 页。
- en: 'Liu et al. (2023) Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin
    Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2023. [Do emergent abilities exist
    in quantized large language models: An empirical study](https://arxiv.org/abs/2307.08072).
    *arXiv preprint arXiv:2307.08072*.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023）Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang
    Li, Bolin Ding 和 Ji-Rong Wen. 2023. [量化大规模语言模型是否存在突现能力：一项实证研究](https://arxiv.org/abs/2307.08072).
    *arXiv 预印本 arXiv:2307.08072*。
- en: Micikevicius et al. (2017) Paulius Micikevicius, Sharan Narang, Jonah Alben,
    Gregory Frederick Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston,
    Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2017. [Mixed precision training](https://arxiv.org/abs/1710.03740).
    *arXiv preprint arXiv:1710.03740*.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micikevicius 等 (2017) Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory
    Frederick Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston,
    Oleksii Kuchaiev, Ganesh Venkatesh, 和 Hao Wu. 2017. [混合精度训练](https://arxiv.org/abs/1710.03740)。*arXiv
    预印本 arXiv:1710.03740*。
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, et al. 2021. [Webgpt: Browser-assisted question-answering with
    human feedback](https://arxiv.org/abs/2112.09332). *arXiv preprint arXiv:2112.09332*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nakano 等 (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long
    Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William
    Saunders 等. 2021. [Webgpt: 浏览器辅助的问答系统与人类反馈](https://arxiv.org/abs/2112.09332)。*arXiv
    预印本 arXiv:2112.09332*。'
- en: 'OpenAI (2022) TB OpenAI. 2022. Chatgpt: Optimizing language models for dialogue.
    *OpenAI*.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI (2022) TB OpenAI. 2022. Chatgpt: 为对话优化语言模型。*OpenAI*。'
- en: Panigrahi et al. (2023) Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and
    Sanjeev Arora. 2023. [Task-specific skill localization in fine-tuned language
    models](https://arxiv.org/abs/2302.06600). *arXiv preprint arXiv:2302.06600*.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Panigrahi 等 (2023) Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, 和 Sanjeev
    Arora. 2023. [在微调语言模型中进行任务特定技能定位](https://arxiv.org/abs/2302.06600)。*arXiv 预印本
    arXiv:2302.06600*。
- en: Park et al. (2021) Geondo Park, Gyeongman Kim, and Eunho Yang. 2021. [Distilling
    linguistic context for language model compression](https://aclanthology.org/2021.emnlp-main.30).
    In *Proceedings of EMNLP*, pages 364–378.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 (2021) Geondo Park, Gyeongman Kim, 和 Eunho Yang. 2021. [为语言模型压缩提炼语言上下文](https://aclanthology.org/2021.emnlp-main.30)。见
    *EMNLP 会议论文集*，页码 364–378。
- en: 'Pilehvar and Camacho-Collados (2018) Mohammad Taher Pilehvar and Jose Camacho-Collados.
    2018. [Wic: the word-in-context dataset for evaluating context-sensitive meaning
    representations](https://arxiv.org/abs/1808.09121). *arXiv preprint arXiv:1808.09121*.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pilehvar 和 Camacho-Collados (2018) Mohammad Taher Pilehvar 和 Jose Camacho-Collados.
    2018. [Wic: 评估上下文敏感语义表示的词汇上下文数据集](https://arxiv.org/abs/1808.09121)。*arXiv 预印本
    arXiv:1808.09121*。'
- en: 'Qiu et al. (2020) Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai,
    and Xuanjing Huang. 2020. [Pre-trained models for natural language processing:
    A survey](https://link.springer.com/article/10.1007/s11431-020-1647-3). *Science
    China Technological Sciences*, 63(10):1872–1897.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu 等 (2020) Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, 和 Xuanjing
    Huang. 2020. [预训练模型在自然语言处理中的应用综述](https://link.springer.com/article/10.1007/s11431-020-1647-3)。*科学中国技术科学*，63(10):1872–1897。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf).
    *JMLR*, 21:1–67.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等 (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu. 2020. [探索统一的文本到文本转换器的迁移学习极限](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)。*JMLR*，21:1–67。
- en: 'Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
    Percy Liang. 2016. [Squad: 100,000+ questions for machine comprehension of text](https://arxiv.org/abs/1606.05250).
    *arXiv preprint arXiv:1606.05250*.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajpurkar 等 (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, 和 Percy
    Liang. 2016. [Squad: 100,000+ 个问题用于机器理解文本](https://arxiv.org/abs/1606.05250)。*arXiv
    预印本 arXiv:1606.05250*。'
- en: Ramachandran et al. (2017) Prajit Ramachandran, Barret Zoph, and Quoc V Le.
    2017. [Searching for activation functions](https://arxiv.org/abs/1710.05941).
    *arXiv preprint arXiv:1710.05941*.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramachandran 等 (2017) Prajit Ramachandran, Barret Zoph, 和 Quoc V Le. 2017. [寻找激活函数](https://arxiv.org/abs/1710.05941)。*arXiv
    预印本 arXiv:1710.05941*。
- en: 'Roemmele et al. (2011) Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S
    Gordon. 2011. [Choice of plausible alternatives: An evaluation of commonsense
    causal reasoning](https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF).
    In *Proceedings of AAAI*, pages 90–95.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roemmele 等 (2011) Melissa Roemmele, Cosmin Adrian Bejan, 和 Andrew S Gordon.
    2011. [选择合理的替代方案：对常识因果推理的评估](https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF)。见
    *AAAI 会议论文集*，页码 90–95。
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. [Distilbert, a distilled version of bert: smaller, faster, cheaper
    and lighter](https://arxiv.org/abs/1910.01108). *arXiv preprint arXiv:1910.01108*.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等 (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, 和 Thomas Wolf. 2019.
    [Distilbert, bert 的蒸馏版本：更小、更快、更便宜、更轻便](https://arxiv.org/abs/1910.01108)。*arXiv
    预印本 arXiv:1910.01108*。
- en: Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. [Recursive deep
    models for semantic compositionality over a sentiment treebank](https://aclanthology.org/D13-1170).
    In *Proceedings of EMNLP*, pages 1631–1642.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher et al. (2013) Richard Socher、Alex Perelygin、Jean Wu、Jason Chuang、Christopher
    D. Manning、Andrew Ng 和 Christopher Potts. 2013. [用于情感树库的递归深度模型进行语义组合性分析](https://aclanthology.org/D13-1170)。在
    *EMNLP 会议记录* 中，第 1631–1642 页。
- en: Stock et al. (2021) Pierre Stock, Angela Fan, Benjamin Graham, Edouard Grave,
    Rémi Gribonval, Herve Jegou, and Armand Joulin. 2021. [Training with quantization
    noise for extreme model compression](https://openreview.net/forum?id=dV19Yyi1fS3).
    In *Proceedings of ICLR*.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stock et al. (2021) Pierre Stock、Angela Fan、Benjamin Graham、Edouard Grave、Rémi
    Gribonval、Herve Jegou 和 Armand Joulin. 2021. [通过量化噪声训练以实现极端模型压缩](https://openreview.net/forum?id=dV19Yyi1fS3)。在
    *ICLR 会议记录* 中。
- en: Suau et al. (2020) Xavier Suau, Luca Zappella, and Nicholas Apostoloff. 2020.
    [Finding experts in transformer models](https://arxiv.org/abs/2005.07647). *arXiv
    preprint arXiv:2005.07647*.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suau et al. (2020) Xavier Suau、Luca Zappella 和 Nicholas Apostoloff. 2020. [在变换器模型中寻找专家](https://arxiv.org/abs/2005.07647)。*arXiv
    预印本 arXiv:2005.07647*。
- en: Sun et al. (2019) Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. [Patient
    knowledge distillation for BERT model compression](https://aclanthology.org/D19-1441).
    In *Proceedings EMNLP-IJCNLP*, pages 4323–4332.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2019) Siqi Sun、Yu Cheng、Zhe Gan 和 Jingjing Liu. 2019. [用于 BERT 模型压缩的患者知识蒸馏](https://aclanthology.org/D19-1441)。在
    *EMNLP-IJCNLP 会议记录* 中，第 4323–4332 页。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori et al. (2023) Rohan Taori、Ishaan Gulrajani、Tianyi Zhang、Yann Dubois、Xuechen
    Li、Carlos Guestrin、Percy Liang 和 Tatsunori B. Hashimoto. 2023. Stanford alpaca:
    一种跟随指令的 llama 模型。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. [Llama: Open and efficient foundation
    language models](https://arxiv.org/abs/2302.13971). *arXiv preprint arXiv:2302.13971*.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等. 2023. [Llama：开放且高效的基础语言模型](https://arxiv.org/abs/2302.13971)。*arXiv 预印本 arXiv:2302.13971*。
- en: 'Wang et al. (2019) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet
    Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. [Superglue:
    A stickier benchmark for general-purpose language understanding systems](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html).
    In *Proceedings of NeurIPS*, volume 32.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019) Alex Wang、Yada Pruksachatkun、Nikita Nangia、Amanpreet Singh、Julian
    Michael、Felix Hill、Omer Levy 和 Samuel Bowman. 2019. [Superglue：用于通用语言理解系统的更具挑战性的基准](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html)。在
    *NeurIPS 会议记录* 中，第 32 卷。
- en: 'Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R Bowman. 2018. [Glue: A multi-task benchmark and analysis
    platform for natural language understanding](https://arxiv.org/abs/1804.07461).
    *arXiv preprint arXiv:1804.07461*.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2018) Alex Wang、Amanpreet Singh、Julian Michael、Felix Hill、Omer
    Levy 和 Samuel R Bowman. 2018. [Glue：用于自然语言理解的多任务基准和分析平台](https://arxiv.org/abs/1804.07461)。*arXiv
    预印本 arXiv:1804.07461*。
- en: Wang et al. (2022) Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan
    Liu, and Juanzi Li. 2022. [Finding skill neurons in pre-trained transformer-based
    language models](https://aclanthology.org/2022.emnlp-main.765). In *Proceedings
    of EMNLP*, pages 11132–11152.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022) Xiaozhi Wang、Kaiyue Wen、Zhengyan Zhang、Lei Hou、Zhiyuan Liu
    和 Juanzi Li. 2022. [在预训练的基于变换器的语言模型中找到技能神经元](https://aclanthology.org/2022.emnlp-main.765)。在
    *EMNLP 会议记录* 中，第 11132–11152 页。
- en: Wang et al. (2020) Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2020. [Structured
    pruning of large language models](https://aclanthology.org/2020.emnlp-main.496).
    In *Proceedings of EMNLP*, pages 6151–6162.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020) Ziheng Wang、Jeremy Wohlwend 和 Tao Lei. 2020. [大型语言模型的结构化剪枝](https://aclanthology.org/2020.emnlp-main.496)。在
    *EMNLP 会议记录* 中，第 6151–6162 页。
- en: Williams et al. (2017) Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017.
    [A broad-coverage challenge corpus for sentence understanding through inference](https://arxiv.org/abs/1704.05426).
    *arXiv preprint arXiv:1704.05426*.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams et al. (2017) Adina Williams、Nikita Nangia 和 Samuel R Bowman. 2017.
    [用于通过推理进行句子理解的广覆盖挑战语料库](https://arxiv.org/abs/1704.05426)。*arXiv 预印本 arXiv:1704.05426*。
- en: Xia et al. (2022) Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022. [Structured
    pruning learns compact and accurate models](https://aclanthology.org/2022.acl-long.107).
    In *Proceedings of ACL*, pages 1513–1528.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia等人（2022）夏梦洲、钟泽轩和陈丹奇。2022。 [Structured pruning learns compact and accurate
    models](https://aclanthology.org/2022.acl-long.107)。发表于*Proceedings of ACL*，第1513–1528页。
- en: Xu and McAuley (2022) Canwen Xu and Julian McAuley. 2022. [A survey on model
    compression for natural language processing](https://arxiv.org/abs/2202.07105).
    *arXiv preprint arXiv:2202.07105*.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu和McAuley（2022）徐灿文和朱利安·麦考利。2022。 [A survey on model compression for natural
    language processing](https://arxiv.org/abs/2202.07105)。*arXiv预印本arXiv:2202.07105*。
- en: Xu et al. (2021) Dongkuan Xu, Ian En-Hsu Yen, Jinxi Zhao, and Zhibin Xiao. 2021.
    [Rethinking network pruning – under the pre-train and fine-tune paradigm](https://aclanthology.org/2021.naacl-main.188).
    In *Proceedings of NAACL*, pages 2376–2382.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等人（2021）徐东宽、伊恩·恩修·阎、赵金熙和萧志斌。2021。 [Rethinking network pruning – under the
    pre-train and fine-tune paradigm](https://aclanthology.org/2021.naacl-main.188)。发表于*Proceedings
    of NAACL*，第2376–2382页。
- en: Yenicelik et al. (2020) David Yenicelik, Florian Schmidt, and Yannic Kilcher.
    2020. [How does bert capture semantics? a closer look at polysemous words](https://www.aclweb.org/anthology/2020.blackboxnlp-1.15).
    In *Proceedings of BlackboxNLP*, pages 156–162.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yenicelik等人（2020）大卫·耶尼切利克、弗洛里安·施密特和扬尼克·基尔彻。2020。 [How does bert capture semantics?
    a closer look at polysemous words](https://www.aclweb.org/anthology/2020.blackboxnlp-1.15)。发表于*Proceedings
    of BlackboxNLP*，第156–162页。
- en: 'Zafrir et al. (2019) Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
    2019. [Q8bert: Quantized 8bit bert](https://ieeexplore.ieee.org/abstract/document/9463531).
    In *Proceedings of EMC2-NIPS*, pages 36–39\. IEEE.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zafrir等人（2019）奥菲尔·扎夫里尔、盖·布杜克、彼得·伊扎克和摩西·瓦瑟布拉特。2019。 [Q8bert: Quantized 8bit
    bert](https://ieeexplore.ieee.org/abstract/document/9463531)。发表于*Proceedings of
    EMC2-NIPS*，第36–39页。IEEE。'
- en: 'Zhang et al. (2022a) Zhengyan Zhang, Baitao Gong, Yingfa Chen, Xu Han, Guoyang
    Zeng, Weilin Zhao, Yanxu Chen, Zhiyuan Liu, and Maosong Sun. 2022a. [Bmcook: A
    task-agnostic compression toolkit for big models](BMCook:%20A%20Task-agnostic%20Compression%20Toolkit%20for%20Big%20Models).
    In *Proceedings of EMNLP Demonstration*, pages 396–405.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等人（2022a）郑延张、龚柏涛、陈颖发、韩旭、曾国扬、赵伟林、陈彦旭、刘志远和孙茂松。2022a。 [Bmcook: A task-agnostic
    compression toolkit for big models](BMCook:%20A%20Task-agnostic%20Compression%20Toolkit%20for%20Big%20Models)。发表于*Proceedings
    of EMNLP Demonstration*，第396–405页。'
- en: 'Zhang et al. (2022b) Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong
    Sun, and Jie Zhou. 2022b. [MoEfication: Transformer feed-forward layers are mixtures
    of experts](https://aclanthology.org/2022.findings-acl.71). In *Findings of ACL*,
    pages 877–890.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等人（2022b）郑延张、林彦凯、刘志远、李鹏、孙茂松和周杰。2022b。 [MoEfication: Transformer feed-forward
    layers are mixtures of experts](https://aclanthology.org/2022.findings-acl.71)。发表于*Findings
    of ACL*，第877–890页。'
- en: 'Zhang et al. (2021) Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Qun Liu, and Maosong
    Sun. 2021. [Know what you don’t need: Single-shot meta-pruning for attention heads](https://www.sciencedirect.com/science/article/pii/S2666651021000140).
    *AI Open*, 2:36–42.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等人（2021）郑延张、齐范超、刘志远、刘群和孙茂松。2021。 [Know what you don’t need: Single-shot
    meta-pruning for attention heads](https://www.sciencedirect.com/science/article/pii/S2666651021000140)。*AI
    Open*，2:36–42。'
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    2023. [A survey of large language models](https://arxiv.org/abs/2303.18223). *arXiv
    preprint arXiv:2303.18223*.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等人（2023）赵伟新、周坤、李俊毅、唐天逸、王小雷、侯玉鹏、闵颖茜、张碧晨、张俊杰、董子灿等。2023。 [A survey of large
    language models](https://arxiv.org/abs/2303.18223)。*arXiv预印本arXiv:2303.18223*。
- en: 'Zhou et al. (2022) Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. 2022.
    [PetS: A unified framework for Parameter-Efficient transformers serving](https://www.usenix.org/conference/atc22/presentation/zhou-zhe).
    In *Proceedings of ATC*, pages 489–504.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou等人（2022）周哲、魏学超、张洁静和孙光宇。2022。 [PetS: A unified framework for Parameter-Efficient
    transformers serving](https://www.usenix.org/conference/atc22/presentation/zhou-zhe)。发表于*Proceedings
    of ATC*，第489–504页。'
- en: Appendix A The Experimental Settings of LLaMA
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A LLaMA的实验设置
- en: 'When applying CPET on LLaMA, we add LoRA on all linear modules of the attention
    layer and add the recovery module on all linear modules in transformers. LoRA
    and recovery modules’ ranks are set to 16 with a dropout rate of $p=0.05$. In
    Table [6](#S4.T6 "Table 6 ‣ 4.6 Instruction Tuning with CPET ‣ 4 Experiments and
    Analyses ‣ CPET: Effective Parameter-Efficient Tuning for Compressed Large Language
    Models"), method Dettmers et al. ([2023](#bib.bib14)) is in the setting of “NFloat4+DQ”,
    method Liu et al. ([2023](#bib.bib36)) is in the setting of LoRA with 4-bit quantized
    model. All results are evaluated with the framework LM-Eval Gao et al. ([2021](#bib.bib20)).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '在对LLaMA应用CPET时，我们在注意力层的所有线性模块上添加了LoRA，并在变换器的所有线性模块上添加了恢复模块。LoRA和恢复模块的秩设置为16，丢弃率为$p=0.05$。在表格[6](#S4.T6
    "表格 6 ‣ 4.6 使用CPET进行指令调优 ‣ 4 实验和分析 ‣ CPET: 有效的参数高效调优方法用于压缩的大型语言模型")中，Dettmers等人（[2023](#bib.bib14)）的方法属于“NFloat4+DQ”设置，Liu等人（[2023](#bib.bib36)）的方法属于4位量化模型的LoRA设置。所有结果均使用框架LM-Eval
    Gao等人（[2021](#bib.bib20)）进行评估。'
