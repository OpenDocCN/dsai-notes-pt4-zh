- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 19:04:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:24
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LazyLLM：高效长上下文LLM推理的动态标记修剪
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.14057](https://ar5iv.labs.arxiv.org/html/2407.14057)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.14057](https://ar5iv.labs.arxiv.org/html/2407.14057)
- en: Qichen Fu Apple Minsik Cho Apple Thomas Merth Apple Sachin Mehta Apple Mohammad
    Rastegari Mahyar Najibi Apple
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Qichen Fu Apple Minsik Cho Apple Thomas Merth Apple Sachin Mehta Apple Mohammad
    Rastegari Mahyar Najibi Apple
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The inference of transformer-based large language models consists of two sequential
    stages: 1) a *prefilling* stage to compute the KV cache of prompts and generate
    the first token, and 2) a *decoding* stage to generate subsequent tokens. For
    long prompts, the KV cache must be computed for all tokens during the *prefilling*
    stage, which can significantly increase the time needed to generate the first
    token. Consequently, the *prefilling* stage may become a bottleneck in the generation
    process. An open question remains whether all prompt tokens are essential for
    generating the first token. To answer this, we introduce a novel method, *LazyLLM*,
    that selectively computes the KV for tokens important for the next token prediction
    in both the *prefilling* and *decoding* stages. Contrary to static pruning approaches
    that prune the prompt at once, *LazyLLM* allows language models to dynamically
    select different subsets of tokens from the context in different generation steps,
    even though they might be pruned in previous steps. Extensive experiments on standard
    datasets across various tasks demonstrate that *LazyLLM* is a generic method that
    can be seamlessly integrated with existing language models to significantly accelerate
    the generation without fine-tuning. For instance, in the multi-document question-answering
    task, *LazyLLM* accelerates the *prefilling* stage of the LLama 2 7B model by
    $2.34\times$ while maintaining accuracy.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变换器的大型语言模型的推理包括两个顺序阶段：1）*预填充*阶段，用于计算提示的KV缓存并生成第一个标记，以及2）*解码*阶段，用于生成后续标记。对于长提示，在*预填充*阶段必须计算所有标记的KV缓存，这可能显著增加生成第一个标记所需的时间。因此，*预填充*阶段可能成为生成过程中的瓶颈。一个未解的问题是，所有提示标记是否对生成第一个标记都是必要的。为了解答这个问题，我们引入了一种新方法，*LazyLLM*，它在*预填充*和*解码*阶段选择性地计算对下一个标记预测重要的KV。与一次性修剪提示的静态修剪方法相反，*LazyLLM*允许语言模型在不同的生成步骤中动态选择上下文中的不同标记子集，即使这些标记可能在先前的步骤中被修剪。对各种任务的标准数据集进行的广泛实验表明，*LazyLLM*是一种通用方法，可以无缝集成到现有语言模型中，以显著加速生成过程而无需微调。例如，在多文档问答任务中，*LazyLLM*将LLama
    2 7B模型的*预填充*阶段加速了$2.34\times$，同时保持准确性。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Standard prompt-based LLM inference has two sequential stages: *prefilling*
    and *decoding*, as shown in [Figure 1](#S1.F1 "In 1 Introduction ‣ LazyLLM: Dynamic
    Token Pruning for Efficient Long Context LLM Inference"). During the *prefilling*
    stage, the model computes and saves the KV cache of each token from the prompt,
    and predicts the first token. We refer to the time taken during *prefilling* stage
    as “time-to-first-token” (*TTFT*). Following the *prefilling* stage is the *decoding*
    stage, where the model reuses cached KVs to decode the next token iteratively
    until the stop criteria are met.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 标准基于提示的LLM推理有两个顺序阶段：*预填充*和*解码*，如[图1](#S1.F1 "在1引言 ‣ LazyLLM：高效长上下文LLM推理的动态标记修剪")所示。在*预填充*阶段，模型计算并保存每个提示标记的KV缓存，并预测第一个标记。我们将*预填充*阶段所需的时间称为“时间到第一个标记”（*TTFT*）。*预填充*阶段之后是*解码*阶段，其中模型重用缓存的KV，迭代解码下一个标记，直到满足停止标准。
- en: During the *prefilling* stage, all tokens from the prompt are used by all transformer
    layers. For long prompts, *TTFT* could be slow because state-of-the-art transformer-based
    LLMs are both deep and wide (Pope et al., [2023](#bib.bib26); Kim et al., [2023](#bib.bib16);
    Aminabadi et al., [2022](#bib.bib2)), and the cost of computing attention increases
    quadratically with the number of tokens in the prompts. For instance, Llama 2
    (Touvron et al., [2023](#bib.bib28)), with 7 billion parameters, stacks 32 transformer
    layers with a model dimension of 4096\. In this scenario, *TTFT* requires $21\times$
    tokens. (Bai et al., [2023](#bib.bib4)). Therefore, optimizing *TTFT* is a critical
    path toward efficient LLM inference (NVIDIA, [2024](#bib.bib25)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在*预填充*阶段，提示中的所有标记都被所有变换器层使用。对于长提示，*TTFT*可能较慢，因为最先进的变换器基础LLMs既深又宽（Pope等，[2023](#bib.bib26)；Kim等，[2023](#bib.bib16)；Aminabadi等，[2022](#bib.bib2)），并且计算注意力的成本随着提示中标记数量的增加呈平方关系。例如，Llama
    2（Touvron等，[2023](#bib.bib28)），具有70亿个参数，堆叠了32层变换器，模型维度为4096。在这种情况下，*TTFT*需要$21\times$的标记（Bai等，[2023](#bib.bib4)）。因此，优化*TTFT*是实现高效LLM推理的关键路径（NVIDIA，[2024](#bib.bib25)）。
- en: 'While optimizing LLM inference is an active area of research, many methods
    (Leviathan et al., [2023](#bib.bib18); Cai et al., [2024](#bib.bib7); Zhang et al.,
    [2024](#bib.bib31); Bhendawade et al., [2024](#bib.bib6); Li et al., [2024](#bib.bib20))
    have focused on improving inference speed during the *decoding* stage. Yet, there
    is little attention given to improving *TTFT*. We note that some compression-based
    works implicitly improve the *TTFT* by reducing the size of LLMs (Frantar et al.,
    [2022](#bib.bib12); Sun et al., [2023](#bib.bib27); Ma et al., [2023](#bib.bib21)).
    However, an orthogonal line of research(Li et al., [2023](#bib.bib19); Jiang et al.,
    [2023](#bib.bib14); Dao et al., [2022](#bib.bib9)) investigates how *TTFT* can
    be improved given a static transformer architecture. Within this line of research,
    a natural question arises: Are all prompt tokens essential for generating the
    first token?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管优化LLM推理是一个活跃的研究领域，但许多方法（Leviathan等，[2023](#bib.bib18)；Cai等，[2024](#bib.bib7)；Zhang等，[2024](#bib.bib31)；Bhendawade等，[2024](#bib.bib6)；Li等，[2024](#bib.bib20)）已专注于改进*解码*阶段的推理速度。然而，对*TTFT*的改进关注较少。我们注意到一些基于压缩的研究通过减小LLMs的规模（Frantar等，[2022](#bib.bib12)；Sun等，[2023](#bib.bib27)；Ma等，[2023](#bib.bib21)）间接提高了*TTFT*。然而，另一类研究（Li等，[2023](#bib.bib19)；Jiang等，[2023](#bib.bib14)；Dao等，[2022](#bib.bib9)）探讨了在静态变换器架构下如何改进*TTFT*。在这一研究方向上，出现了一个自然的问题：生成第一个标记时，所有提示标记是否都必不可少？
- en: '![Refer to caption](img/c8265491916632562842c341b75efe84.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c8265491916632562842c341b75efe84.png)'
- en: 'Figure 1: Prompt-based LLM inference can be divided into two sequential stages:
    *prefilling* and *decoding*. For long prompts, the first token generation during
    *prefilling* stage could be slow. As an example, for Llama 2 7B model (Touvron
    et al., [2023](#bib.bib28)), on average, the time to generate the first token
    requires $21\times$ of the total generation time in the LongBench benchmark.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于提示的LLM推理可以分为两个连续阶段：*预填充*和*解码*。对于长提示，*预填充*阶段的第一个标记生成可能较慢。例如，对于Llama 2 7B模型（Touvron等，[2023](#bib.bib28)），在LongBench基准测试中，生成第一个标记的时间平均需要$21\times$的总生成时间。
- en: 'LLM profiling on the LongBench benchmark (Bai et al., [2023](#bib.bib4)) in
    [Figure 2](#S1.F2 "In 1 Introduction ‣ LazyLLM: Dynamic Token Pruning for Efficient
    Long Context LLM Inference") reveals that the attention scores of input tokens
    w.r.t. to the first generated token are very sparse, indicating that many tokens
    in the input prompt are redundant and can be removed without affecting the next
    token prediction. To this end, we propose *LazyLLM*, a novel, simple, yet effective
    technique tailored for speeding up *prefilling*. As depicted in [Figure 3](#S3.F3
    "In 3 LazyLLM ‣ LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM
    Inference"), in each generation step, *LazyLLM* selectively computes the KV for
    tokens important for the next token prediction and “lazily” defers the computation
    of remaining tokens to later steps when they become relevant. We propose using
    the attention score of the prior transformer layer to measure the importance of
    tokens and progressively prune tokens along the depth of the transformer. In contrast
    to prompt compression works (Li et al., [2023](#bib.bib19); Jiang et al., [2023](#bib.bib14);
    Xu et al., [2023](#bib.bib29)), which permanently reduce the prompt for all the
    following generation steps, our method allows the model to revive previously pruned
    tokens, which we found crucial to retain accuracy. Extending progressive token
    pruning to all generation steps is non-trivial. Specifically, if a token is pruned
    at generation step $t$. To avoid such repetitive computation, we employ an additional
    caching mechanism, *Aux Cache*, to cache the hidden states of pruned tokens. This
    enables a computationally efficient pathway to revive pruned tokens, and ensures
    that the worst runtime of *LazyLLM* is never slower than the baseline.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LongBench 基准测试（Bai 等，[2023](#bib.bib4)）上进行的 LLM 分析（见 [图 2](#S1.F2 "在 1 引言
    ‣ LazyLLM：用于高效长上下文 LLM 推理的动态令牌剪枝")）揭示了输入令牌相对于第一个生成令牌的注意力分数非常稀疏，表明输入提示中的许多令牌是多余的，可以在不影响下一个令牌预测的情况下被移除。为此，我们提出了
    *LazyLLM*，这是一种新颖、简单但有效的技术，旨在加速 *预填充*。如 [图 3](#S3.F3 "在 3 LazyLLM ‣ LazyLLM：用于高效长上下文
    LLM 推理的动态令牌剪枝") 所示，在每个生成步骤中，*LazyLLM* 会选择性地计算对下一个令牌预测重要的 KV，并“懒惰地”将剩余令牌的计算推迟到它们变得相关的后续步骤。我们提出使用先前
    Transformer 层的注意力分数来衡量令牌的重要性，并沿着 Transformer 的深度逐步剪枝令牌。与提示压缩方法（Li 等，[2023](#bib.bib19)；Jiang
    等，[2023](#bib.bib14)；Xu 等，[2023](#bib.bib29)）相比，这些方法会永久性地减少所有后续生成步骤的提示，我们的方法允许模型恢复先前剪枝的令牌，我们发现这对于保持准确性至关重要。将渐进的令牌剪枝扩展到所有生成步骤并非易事。具体来说，如果一个令牌在生成步骤
    $t$ 时被剪枝。为了避免重复计算，我们采用了额外的缓存机制 *Aux Cache*，以缓存剪枝令牌的隐藏状态。这使得恢复剪枝令牌成为计算上高效的过程，并确保
    *LazyLLM* 的最差运行时间从未比基线慢。
- en: '![Refer to caption](img/54450a998b61941f2e6a2a8eea419e26.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/54450a998b61941f2e6a2a8eea419e26.png)'
- en: 'Figure 2: We visualize the attention scores of input tokens in the prompt w.r.t.
    to the next token for each layer of Llama 2 7BTouvron et al. ([2023](#bib.bib28)).
    We also plot the distribution of the average attention score across all transformer
    layers. Result reveals that the attention scores of input tokens w.r.t. to the
    next token are very sparse, indicating that many tokens in the input prompt are
    redundant and can be safely removed without affecting the next token prediction.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们可视化了 Llama 2 7B（Touvron 等，[2023](#bib.bib28)）中每一层的输入令牌相对于下一个令牌的注意力分数。我们还绘制了所有
    Transformer 层平均注意力分数的分布。结果表明，输入令牌相对于下一个令牌的注意力分数非常稀疏，表明输入提示中的许多令牌是多余的，可以安全地移除而不会影响下一个令牌的预测。
- en: 'In summary, the advantages of *LazyLLM* are: (1) Universal: *LazyLLM* can be
    seamlessly integrated with any existing transformer-based LLM to improve inference
    speed, (2) Training-free: *LazyLLM* doesn’t require any finetuning and can be
    directly integrated without any parameter modification, (3) Effective: Empirical
    results on 16 standard datasets across 6 different language tasks shows *LazyLLM*
    can improve the inference speed of the LLM during both *prefilling* and *decoding*
    stages.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，*LazyLLM* 的优点包括：(1) 通用性：*LazyLLM* 可以无缝集成到任何现有的基于 Transformer 的 LLM 中，以提高推理速度，(2)
    无需训练：*LazyLLM* 不需要任何微调，可以直接集成而无需修改任何参数，(3) 高效：在 16 个标准数据集和 6 种不同语言任务上的实证结果表明，*LazyLLM*
    能够提高 LLM 在 *预填充* 和 *解码* 阶段的推理速度。
- en: 2 Related Work
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'The increase in the scale of large language models (LLMs) has greatly enhanced
    their performance but also introduced challenges with respect to their inference
    efficiency. The inference of generative LLMs consists of two distinct stages as
    depicted in [Figure 1](#S1.F1 "In 1 Introduction ‣ LazyLLM: Dynamic Token Pruning
    for Efficient Long Context LLM Inference"). In particular, extensive computation
    is needed under long context scenarios to calculate the full KV cache during the
    *prefilling* stage, resulting in a long time-to-first-token (*TTFT*). This delay
    causes users to wait several seconds after submitting a prompt before receiving
    any response from the agent, leading to a poor user experience.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '大型语言模型（LLMs）的规模增加极大地提升了它们的性能，但也带来了推理效率方面的挑战。生成型LLMs的推理由两个不同阶段组成，如[图1](#S1.F1
    "在 1 引言 ‣ LazyLLM: 动态令牌修剪以实现高效的长上下文LLM推理")所示。特别是在长上下文场景下，需要大量计算来计算完整的KV缓存，这导致了较长的首次令牌时间（*TTFT*）。这一延迟使得用户在提交提示后需等待几秒钟才能收到代理的响应，从而导致用户体验不佳。'
- en: Efficient Long Context Inference. Extensive work (Merth et al., [2024](#bib.bib22);
    Chen et al., [2023](#bib.bib8); Beltagy et al., [2020](#bib.bib5); Kitaev et al.,
    [2020](#bib.bib17)) has been proposed to improve inference efficiency for long
    context applications by reducing the memory footprint and total computations.
    Some works have focused on tailoring the architecture of the transformer for long
    context input. For instance, (Beltagy et al., [2020](#bib.bib5)) introduces a
    drop-in replacement for standard self-attention and combines local windowed attention
    with task-motivated global attention. In parallel, Reformer (Kitaev et al., [2020](#bib.bib17))
    replaces dot-product attention by one that uses locality-sensitive hashing to
    reduce its computational complexity. Though the above methods can speed up long
    context inference, they require significant model architecture change and re-training.
    This drawback makes them impractical to be applied to existing pre-trained LLMs.
    Closer to our work are efficient techniques that optimize the KV cache (Zhang
    et al., [2024](#bib.bib31); Li et al., [2024](#bib.bib20); Anagnostidis et al.,
    [2024](#bib.bib3); Nawrot et al., [2024](#bib.bib23)) by minimizing the KV cache
    size and data transfer. However, these works only focus on accelerating decoding
    steps, which are not applicable to reducing *TTFT*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的长上下文推理。已有大量工作（Merth等，[2024](#bib.bib22)；Chen等，[2023](#bib.bib8)；Beltagy等，[2020](#bib.bib5)；Kitaev等，[2020](#bib.bib17)）旨在通过减少内存占用和总计算量来提高长上下文应用的推理效率。一些工作集中于为长上下文输入量身定制变换器架构。例如，（Beltagy等，[2020](#bib.bib5)）引入了一种标准自注意力的替代方案，将局部窗口注意力与任务驱动的全局注意力相结合。与此同时，Reformer（Kitaev等，[2020](#bib.bib17)）用局部敏感哈希替代了点积注意力，以降低其计算复杂性。尽管上述方法可以加快长上下文推理，但它们需要显著的模型架构更改和重新训练。这一缺点使得它们难以应用于现有的预训练LLMs。与我们的工作更为接近的是通过最小化KV缓存大小和数据传输来优化KV缓存的高效技术（Zhang等，[2024](#bib.bib31)；Li等，[2024](#bib.bib20)；Anagnostidis等，[2024](#bib.bib3)；Nawrot等，[2024](#bib.bib23)）。然而，这些工作仅专注于加速解码步骤，不适用于减少*TTFT*。
- en: Token Pruning. Previous studies on the sentence classification task (Kim et al.,
    [2022](#bib.bib15); Anagnostidis et al., [2024](#bib.bib3); He et al., [2021](#bib.bib13))
    has shown that not all tokens (*i.e*. words) in an input sequence are necessary
    to make a successful prediction. This provides several possibilities for token
    pruning, which minimizes computational demands by selectively removing less important
    tokens during inference. For example, (Kim et al., [2022](#bib.bib15)) presents
    Learned Token Pruning which adaptively removes unimportant tokens as an input
    sequence passes through transformer layers. In parallel, (He et al., [2021](#bib.bib13))
    proposes to reduce width-wise computation via token pruning for transformer-based
    models such as BERT (Devlin et al., [2018](#bib.bib10)). These aforementioned
    approaches were designed for tasks requiring only a single iteration of processing,
    such as text classification. In this work, we extend the idea of token pruning
    to generative LLMs. Specifically, our method allows the model to dynamically choose
    different sets of tokens at each generation step, which is crucial to retaining
    the performance. Furthermore, we also introduce *Aux Cache* to ensure that each
    token is computed at most once along the whole generation, and ensure the worst
    runtime of our method is not slower than the baseline.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌修剪。先前在句子分类任务上的研究（Kim等，[2022](#bib.bib15)；Anagnostidis等，[2024](#bib.bib3)；He等，[2021](#bib.bib13)）表明，并非所有输入序列中的令牌（*即*
    单词）都是成功预测所必需的。这为令牌修剪提供了若干可能性，通过选择性地去除不重要的令牌来减少计算需求。例如，（Kim等，[2022](#bib.bib15)）提出了学习型令牌修剪，该方法在输入序列通过变换器层时自适应地去除不重要的令牌。与此同时，（He等，[2021](#bib.bib13)）提出通过令牌修剪减少宽度方向的计算，适用于如BERT（Devlin等，[2018](#bib.bib10)）等基于变换器的模型。这些上述方法是为只需要单次处理的任务（如文本分类）设计的。在这项工作中，我们将令牌修剪的思想扩展到生成式LLM。具体来说，我们的方法允许模型在每个生成步骤动态选择不同的令牌集，这对保持性能至关重要。此外，我们还引入了*Aux
    Cache*，以确保每个令牌在整个生成过程中最多计算一次，并确保我们方法的最坏运行时间不慢于基准。
- en: 3 *LazyLLM*
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 *LazyLLM*
- en: '![Refer to caption](img/cf40bab5072e97eb7576d52bb932d7df.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cf40bab5072e97eb7576d52bb932d7df.png)'
- en: 'Figure 3: Comparison between standard LLM and *LazyLLM*. Instead of computing
    the KV cache of all input tokens at the *prefilling* stage, *LazyLLM* only selectively
    computes the tokens that are important to the next token prediction, deferring
    the computation of remaining tokens to later steps. *LazyLLM* significantly optimizes
    *TTFT* by reducing the amount of computation during *prefilling*. Moreover, as
    some tokens in the prompt are never selected by *LazyLLM* during the whole generation
    process (even though theoretically the model could use all tokens in the prompt),
    *LazyLLM* also reduces the total amount of computation and accelerates the overall
    generation.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：标准LLM与*LazyLLM*的比较。在*预填充*阶段，*LazyLLM*不是计算所有输入令牌的KV缓存，而是仅选择性地计算对下一个令牌预测重要的令牌，将剩余令牌的计算推迟到后续步骤。*LazyLLM*通过减少*预填充*阶段的计算量，显著优化了*TTFT*。此外，由于在整个生成过程中，某些提示中的令牌从未被*LazyLLM*选择（尽管理论上模型可以使用提示中的所有令牌），*LazyLLM*还减少了总体计算量，并加速了整体生成。
- en: 3.1 Background on LLM Inference
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 LLM推理背景
- en: 'Generative LLM inference consists of two stages: *prefilling* and *decoding*
    (see [Figure 1](#S1.F1 "In 1 Introduction ‣ LazyLLM: Dynamic Token Pruning for
    Efficient Long Context LLM Inference")). In the *prefilling* stage, the model
    receives the prompt (a sequence of tokens) $\mathcal{T}=\{t_{i}\}_{i=1}^{N}$ to
    the input, and subsequently decodes the following token. The *decoding* step is
    repeatedly performed until the stop criteria are met. While the formula of each
    decoding step is similar to *prefilling*, the amount of its computation is significantly
    lower thanks to the KV cache. Specifically, with saved KV cache from *prefilling*,
    all the previous tokens do not need to pass any linear layers in the model.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '生成式LLM推理包括两个阶段：*预填充*和*解码*（见[图1](#S1.F1 "在1 引言 ‣ LazyLLM: 动态令牌修剪以实现高效长上下文LLM推理")）。在*预填充*阶段，模型接收输入的提示（一个令牌序列）$\mathcal{T}=\{t_{i}\}_{i=1}^{N}$，然后解码下一个令牌。*解码*步骤重复进行，直到满足停止标准。虽然每个解码步骤的公式类似于*预填充*，但由于KV缓存，其计算量显著减少。具体来说，通过保存*预填充*阶段的KV缓存，所有先前的令牌不需要通过模型中的任何线性层。'
- en: 3.2 Inference with *LazyLLM*
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 使用*LazyLLM*进行推理
- en: 'The overview of the proposed *LazyLLM* framework is illustrated in [Figure 4](#S3.F4
    "In 3.2 Inference with LazyLLM ‣ 3 LazyLLM ‣ LazyLLM: Dynamic Token Pruning for
    Efficient Long Context LLM Inference"). *LazyLLM* starts with the full context
    and progressively prunes tokens to gradually reduce the number of computations
    towards the end of the model. Note, *LazyLLM* allows the model to select different
    subsets of tokens from the context in different generation steps, even though
    some of them may be pruned in previous steps. Compared to static pruning which
    prunes all the tokens at once, dynamic pruning optimizes the next token prediction
    in each generation step, which is crucial to retaining the performance.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '提议的*LazyLLM*框架的概述见[图4](#S3.F4 "在3.2 LazyLLM推理 ‣ 3 LazyLLM ‣ LazyLLM: 动态令牌修剪以实现高效的长上下文LLM推理")。*LazyLLM*从完整的上下文开始，逐渐修剪令牌，以在模型末端逐步减少计算量。请注意，*LazyLLM*允许模型在不同的生成步骤中从上下文中选择不同的令牌子集，即使其中一些可能在之前的步骤中已被修剪。与一次性修剪所有令牌的静态修剪相比，动态修剪在每个生成步骤中优化下一个令牌的预测，这对保持性能至关重要。'
- en: Progressive Token Pruning. Prior to this work, token pruning has been successfully
    applied to optimize LLM inference (Zhang et al., [2024](#bib.bib31); Li et al.,
    [2024](#bib.bib20); Adnan et al., [2024](#bib.bib1); Nawrot et al., [2024](#bib.bib23)).
    However, these approaches require accumulating the full attention maps of predicting
    the first few tokens to profile the importance of prompt tokens before starting
    pruning. Consequently, they are not applicable to reduce *TTFT* as they still
    require computing all the KV cache at the *prefilling* stage.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 进阶令牌修剪。在此之前的工作中，令牌修剪已成功应用于优化LLM推理（Zhang等，[2024](#bib.bib31)；Li等，[2024](#bib.bib20)；Adnan等，[2024](#bib.bib1)；Nawrot等，[2024](#bib.bib23)）。然而，这些方法需要累积完整的注意力图以预测前几个令牌，从而在开始修剪之前分析提示令牌的重要性。因此，它们不适用于减少*TTFT*，因为它们仍需在*预填充*阶段计算所有KV缓存。
- en: In contrast, *LazyLLM* only “lazily” computes the tokens that are important
    to predict the next token by starting from the first iteration of the inference
    (the *prefilling* step). A key challenge to pruning tokens in the first iteration
    is determining their importance. Inspired by the early exiting work (Elhoushi
    et al., [2024](#bib.bib11)) which shows the token hidden states gradually evolve
    through the transformer layers, we apply layer-wise token pruning in each generation
    step. Specifically, we use the attention map of the layer $A^{l}\in\mathcal{R}^{H\times
    N\times N}$ w.r.t. the next token to be predicted as
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，*LazyLLM*仅在推理的第一次迭代（*预填充*步骤）中“懒惰地”计算对预测下一个令牌重要的令牌。修剪第一次迭代中的令牌的一个关键挑战是确定它们的重要性。受到早期退出工作（Elhoushi等，[2024](#bib.bib11)）的启发，该工作显示令牌隐藏状态通过变换器层逐渐演变，我们在每个生成步骤中应用逐层令牌修剪。具体而言，我们使用层$A^{l}\in\mathcal{R}^{H\times
    N\times N}$的注意力图来预测下一个令牌。
- en: '|  | $s_{i}^{l}=\frac{1}{H}\sum_{h=1}^{H}A^{l}_{h,i,N}$ |  | (1) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{i}^{l}=\frac{1}{H}\sum_{h=1}^{H}A^{l}_{h,i,N}$ |  | (1) |'
- en: where $H$ head.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$H$为头数。
- en: After computing the confidence scores of tokens, it is challenging to determine
    the threshold value to prune the token. Concretely, the threshold can change as
    the distribution of the attention scores varies between different layers and different
    tasks. We address this challenge by using the top-$k$th percentile among the input
    tokens. Once the token is pruned, it is excluded from the computation of all successive
    layers. In other words, the tokens used in the later layers will be a subset of
    previous layers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算令牌的置信度分数后，确定修剪令牌的阈值具有挑战性。具体而言，阈值可能会随着注意力分数在不同层和不同任务之间的分布变化而变化。我们通过使用输入令牌中的前$k$百分位来解决这个问题。一旦令牌被修剪，它将被排除在所有后续层的计算之外。换句话说，后层使用的令牌将是前层的子集。
- en: 'Our study in [Section 5.4](#S5.SS4 "5.4 Drop Rate in Different Layers ‣ 5 Experiments
    ‣ LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference") shows
    the performance changes with different locations of pruning layers and the number
    of tokens pruned. In particular, when pruning at the same transformer layer, the
    model’s performance gradually decreases as fewer tokens are kept. We also found
    pruning at later transformer layers consistently has better performance than pruning
    at earlier layers, suggesting that later layers are less sensitive to token pruning.
    To achieve a better balance of speedup and accuracy, as shown in [Figure 4](#S3.F4
    "In 3.2 Inference with LazyLLM ‣ 3 LazyLLM ‣ LazyLLM: Dynamic Token Pruning for
    Efficient Long Context LLM Inference"), we apply progressive pruning that keeps
    more tokens at earlier transformer layers and gradually reduces the number of
    tokens towards the end of the transformer.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第5.4节](#S5.SS4 "5.4 不同层中的丢弃率 ‣ 5 实验 ‣ LazyLLM：动态令牌修剪以提高长上下文LLM推理的效率")中的研究显示了在不同修剪层的位置和修剪的令牌数量下性能的变化。特别是，当在相同的变换器层进行修剪时，模型的性能随着保留的令牌数量减少而逐渐下降。我们还发现，在后面的变换器层进行修剪的性能一致优于在早期层进行修剪，这表明后层对令牌修剪的敏感度较低。为了实现速度和准确性的更好平衡，如[图4](#S3.F4
    "在3.2 LazyLLM推理 ‣ 3 LazyLLM ‣ LazyLLM：动态令牌修剪以提高长上下文LLM推理的效率")所示，我们应用了渐进式修剪，即在早期的变换器层保留更多的令牌，并逐渐减少到变换器末端的令牌数量。
- en: '![Refer to caption](img/7fca38505a7abefe072633fe28c6f984.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7fca38505a7abefe072633fe28c6f984.png)'
- en: 'Figure 4: Overview of the *LazyLLM* framework. *LazyLLM* starts with the full
    context and progressively prunes tokens to gradually reduce the number of computations
    towards the end of the model. *LazyLLM* allows the model to select different subsets
    of tokens from the context in different generation steps, which is crucial to
    retaining the performance.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：*LazyLLM*框架概述。*LazyLLM*从完整上下文开始，逐步修剪令牌，以逐渐减少模型末端的计算量。*LazyLLM*允许模型在不同生成步骤中选择上下文中的不同令牌子集，这对于保持性能至关重要。
- en: Aux Cache. In the prefilling stage, there is no KV cache and every token is
    represented by hidden states. Thus, progressive token pruning can be implemented
    by removing pruned tokens’ hidden states. However, extending the progressive token
    pruning to the following *decoding* steps is non-trivial. This is because each
    *decoding* step leverages the KV cache computed in the *prefilling* to compute
    attention. As the *LazyLLM* performs progressive token pruning at the *prefilling*
    stage, the KV of tokens pruned at layer $l$ may be re-selected to compute attention.
    In such cases, the model can not retrieve the KV cache of these tokens. An intuitive
    solution is to pass those tokens again from the beginning of the transformer.
    However, that would cause repetitive computation for the same token, and eventually
    slow down the whole generation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Aux Cache。在预填充阶段，没有KV缓存，每个令牌都由隐藏状态表示。因此，渐进式令牌修剪可以通过移除修剪令牌的隐藏状态来实现。然而，将渐进式令牌修剪扩展到随后的*解码*步骤并非易事。这是因为每个*解码*步骤利用在*预填充*阶段计算的KV缓存来计算注意力。由于*LazyLLM*在*预填充*阶段执行渐进式令牌修剪，层
    $l$ 中被修剪的令牌的KV可能会被重新选择来计算注意力。在这种情况下，模型无法检索这些令牌的KV缓存。一个直观的解决方案是将这些令牌从变换器的开始重新传递。然而，这将导致对相同令牌的重复计算，并最终使整个生成过程变慢。
- en: To tackle this challenge, we introduce *Aux Cache* in addition to the original
    KV cache, which stores the hidden states of those pruned tokens (*e.g*. $T4$),
    we could retrieve their hidden states from the *Aux Cache* of its previous layer
    directly instead of passing through previous layers again. The introduction of
    *Aux Cache* ensures that each token is computed at most once in every transformer
    layer, and ensures the worst runtime of *LazyLLM* is not slower than the baseline.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个挑战，我们在原有的KV缓存之外引入了*Aux Cache*，它存储那些被修剪令牌的隐藏状态（*例如* $T4$），我们可以直接从其前一层的*Aux
    Cache*中检索这些令牌的隐藏状态，而无需再次通过前面的层。引入*Aux Cache*确保每个令牌在每个变换器层中最多计算一次，并确保*LazyLLM*的最差运行时间不慢于基线。
- en: 4 Implementations Details
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实现细节
- en: We implement *LazyLLM* on Llama 2 (Touvron et al., [2023](#bib.bib28)) and XGen
    (Nijkamp et al., [2023](#bib.bib24)) and evaluate it on the LongBench (Bai et al.,
    [2023](#bib.bib4)) using HuggingFace²²2[https://github.com/huggingface/transformers/](https://github.com/huggingface/transformers/).
    We follow the official GitHub repository³³3[https://github.com/THUDM/LongBench](https://github.com/THUDM/LongBench)
    of LongBench for data preprocessing and prompting in all experiments. The LongBench
    benchmark consists of multiple datasets in different tasks, where each task may
    have different metrics, including ROUGE-L, F1, Accuracy, and Edit Sim. Following
    the official evaluation pipeline, we categorize all results over major task categories
    by computing the macro-average score.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Llama 2 (Touvron et al., [2023](#bib.bib28)) 和 XGen (Nijkamp et al., [2023](#bib.bib24))
    上实现了 *LazyLLM* 并在 LongBench (Bai et al., [2023](#bib.bib4)) 上进行评估，使用 HuggingFace²²2[https://github.com/huggingface/transformers/](https://github.com/huggingface/transformers/)。我们遵循
    LongBench 的官方 GitHub 仓库³³3[https://github.com/THUDM/LongBench](https://github.com/THUDM/LongBench)
    进行所有实验的数据预处理和提示。LongBench 基准包括不同任务中的多个数据集，每个任务可能具有不同的度量标准，包括 ROUGE-L、F1、准确率和编辑相似度。根据官方评估流程，我们通过计算宏平均分数对所有主要任务类别的结果进行分类。
- en: As previously noted, the proposed *LazyLLM* doesn’t require any training. Thus,
    *LazyLLM* uses the exact same existing checkpoints as the baseline, for all models.
    For inference, we conduct all experiments on NVIDIA A100 GPUs. We measure and
    report the speedup based on the empirical walltime improvement. Specifically,
    for *TTFT Speedup*, we measure the empirical walltime between when the prompt
    is fed to the model, and when the model generates the first token. For *Generation
    Speedup*, we measure the empirical walltime between when the prompt is fed to
    the model, and when the model finished generating all output tokens. We add 5
    warmup runs for each experiment before starting the time measurement to remove
    the noise such as loading model parameters.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，提出的 *LazyLLM* 不需要任何训练。因此，*LazyLLM* 对所有模型使用与基线完全相同的现有检查点。对于推理，我们在 NVIDIA
    A100 GPU 上进行所有实验。我们测量并报告基于实测墙时间改进的加速比。具体而言，对于 *TTFT 加速比*，我们测量提示输入模型时和模型生成第一个 token
    时之间的实测墙时间。对于 *生成加速比*，我们测量提示输入模型时和模型完成生成所有输出 token 时之间的实测墙时间。在开始时间测量之前，我们为每个实验添加
    5 次预热运行，以消除诸如加载模型参数等噪声。
- en: 5 Experiments
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'We examine our method using two large language models: Llama 2 7B and XGen
    7B. We compare our method with baselines using the same publicly released pretrained
    checkpoints, without employing any additional training. We perform experiments
    using LongBench, a multi-task benchmark for long content understanding. The LongBench
    comprises 16 datasets and covers 6 tasks including single-doc QA, multi-doc QA,
    summarization, few-shot learning, synthetic tasks, and code completion.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两种大型语言模型来检验我们的方法：Llama 2 7B 和 XGen 7B。我们在不进行额外训练的情况下，使用相同的公开发布的预训练检查点，将我们的方法与基线进行比较。我们使用
    LongBench 进行实验，这是一种用于长内容理解的多任务基准。LongBench 包含 16 个数据集，涵盖 6 个任务，包括单文档问答、多文档问答、摘要生成、少量样本学习、合成任务和代码补全。
- en: For the metrics, we primarily evaluate the effectiveness and efficiency of each
    method in the *TTFT* speedup *vs*. accuracy trade-off. Following LongBench, the
    accuracy (*score*) denotes the macro-averaged scores across datasets in each task.
    The *TTFT* speedup measures the wall time improvement w.r.t. to the baseline for
    generating the first token. In analysis, we also assess the impact of our method
    on *$\%$ of Prompt Token Computed* measures the accumulated percent of prompt
    tokens computed at the end of the generation, which indicates the save of total
    computation. The *Generation* speedup measures the walltime change w.r.t. to the
    baseline for completing the entire generation process.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于度量标准，我们主要评估每种方法在 *TTFT* 加速与准确率权衡中的有效性和效率。根据 LongBench，准确率（*分数*）表示每个任务中跨数据集的宏平均分数。*TTFT*
    加速比测量生成第一个 token 时相对于基线的墙时间改进。在分析中，我们还评估我们的方法对 *$\%$ 的提示 token 计算* 的影响，这度量了生成结束时计算的提示
    token 的累计百分比，表示节省的总计算量。*生成* 加速比测量完成整个生成过程时相对于基线的墙时间变化。
- en: '| Tasks | Method | Llama 2 |  | XGen |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 方法 | Llama 2 |  | XGen |'
- en: '| Score | TTFT Speedup ($\times$) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 分数 | TTFT 加速比 ($\times$) |'
- en: '| Single-Document QA | Baseline | $\mathbf{25.79}$ |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 单文档问答 | 基线 | $\mathbf{25.79}$ |'
- en: '| Random Token Drop | $20.05$ |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 随机 token 丢弃 | $20.05$ |'
- en: '| Static Token Pruning | $21.89$ |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 静态 token 剪枝 | $21.89$ |'
- en: '| Prompt Compression | $22.88$ |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 提示压缩 | $22.88$ |'
- en: '| *LazyLLM (Ours)* | $25.59$ |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| *LazyLLM（我们的方法）* | $25.59$ |'
- en: '| Multi-Document QA | Baseline | $\mathbf{22.43}$ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 多文档问答 | 基线 | $\mathbf{22.43}$ |'
- en: '| Random Token Drop | $16.77$ |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 随机令牌丢弃 | $16.77$ |'
- en: '| Static Token Pruning | $19.93$ |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 静态令牌修剪 | $19.93$ |'
- en: '| Prompt Compression | $8.42$ |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 提示压缩 | $8.42$ |'
- en: '| *LazyLLM (Ours)* | $22.31$ |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| *LazyLLM（我们的）* | $22.31$ |'
- en: '| Summarization | Baseline | $24.65$ |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 总结 | 基线 | $24.65$ |'
- en: '| Random Token Drop | $24.39$ |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 随机令牌丢弃 | $24.39$ |'
- en: '| Static Token Pruning | $24.59$ |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 静态令牌修剪 | $24.59$ |'
- en: '| Prompt Compression | $25.16$ |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 提示压缩 | $25.16$ |'
- en: '| *LazyLLM (Ours)* | $\mathbf{24.75}$ |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| *LazyLLM（我们的）* | $\mathbf{24.75}$ |'
- en: '| Few-shot Learning | Baseline | $\mathbf{62.90}$ |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 少样本学习 | 基线 | $\mathbf{62.90}$ |'
- en: '| Random Token Drop | $53.93$ |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 随机令牌丢弃 | $53.93$ |'
- en: '| Static Token Pruning | $56.54$ |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 静态令牌修剪 | $56.54$ |'
- en: '| Prompt Compression | $24.18$ |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 提示压缩 | $24.18$ |'
- en: '| *LazyLLM (Ours)* | $62.81$ |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| *LazyLLM（我们的）* | $62.81$ |'
- en: '| Synthetic | Baseline | $4.97$ |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | 基线 | $4.97$ |'
- en: '| Random Token Drop | $3.57$ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 随机令牌丢弃 | $3.57$ |'
- en: '| Static Token Pruning | $2.81$ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 静态令牌修剪 | $2.81$ |'
- en: '| Prompt Compression | $3.20$ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 提示压缩 | $3.20$ |'
- en: '| *LazyLLM (Ours)* | $\mathbf{4.98}$ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| *LazyLLM（我们的）* | $\mathbf{4.98}$ |'
- en: '| Code Completion | Baseline | $\mathbf{55.18}$ |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 代码补全 | 基线 | $\mathbf{55.18}$ |'
- en: '| Random Token Drop | $44.92$ |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 随机令牌丢弃 | $44.92$ |'
- en: '| Static Token Pruning | $37.51$ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 静态令牌修剪 | $37.51$ |'
- en: '| Prompt Compression | $17.45$ |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 提示压缩 | $17.45$ |'
- en: '| *LazyLLM (Ours)* | $53.30$ |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| *LazyLLM（我们的）* | $53.30$ |'
- en: 'Table 1: Comparisons of *TTFT* speedup *vs*. accuracy on various tasks. Without
    requiring any training/finetuning, *LazyLLM* consistently achieves better *TTFT*
    speedup with negligible accuracy drop. Note that the prompt compression approach
    fails at improving *TTFT* because the overhead of running LLMs to compress the
    prompt is very computationally expensive.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：*TTFT* 加速与各种任务上的准确度比较。无需任何训练/微调，*LazyLLM* 一贯实现了更好的 *TTFT* 加速，且准确度下降极小。注意，提示压缩方法在提升
    *TTFT* 方面效果不佳，因为运行 LLMs 以压缩提示的开销非常大。
- en: 5.1 Results
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 结果
- en: '[Table 1](#S5.T1 "In 5 Experiments ‣ LazyLLM: Dynamic Token Pruning for Efficient
    Long Context LLM Inference") presents the *TTFT* speedup *vs*. accuracy comparisons
    between *LazyLLM*, standard LLM, and other baselines. In the table, the “baseline”
    refers to the standard LLM inference. The “random token drop” baseline is based
    on (Yao et al., [2022](#bib.bib30)) that randomly prunes the prompt tokens before
    feeding them to the LLMs. We report the average metrics across 5 runs for the
    “random token drop” baseline. Our “static token pruning” baseline prunes input
    tokens at once based on their attention score of the first few transformer layers
    during the *prefilling* stage. We also compare with the prompt compression method
    (Li et al., [2023](#bib.bib19)) which pruning redundancy in the input context
    using LLMs. [Table 1](#S5.T1 "In 5 Experiments ‣ LazyLLM: Dynamic Token Pruning
    for Efficient Long Context LLM Inference") shows *LazyLLM* consistently achieves
    better *TTFT* speedup with negligible accuracy drop across multiple tasks. It
    is worth noting that the overhead of running LLMs to compress the prompt is very
    computationally expensive. Even though the inference on the reduced prompt is
    faster, the actual *TTFT* of the “prompt compression” baseline is longer than
    the baseline.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 1](#S5.T1 "在 5 次实验中 ‣ LazyLLM：高效长上下文 LLM 推理的动态令牌修剪") 展示了 *LazyLLM*、标准 LLM
    和其他基线之间的 *TTFT* 加速与准确度比较。在表格中，“基线”指的是标准 LLM 推理。“随机令牌丢弃”基线基于 (Yao et al., [2022](#bib.bib30))，在将提示输入
    LLMs 之前随机修剪提示令牌。我们报告了“随机令牌丢弃”基线的 5 次运行的平均指标。我们的“静态令牌修剪”基线在 *预填充* 阶段基于前几层变换器的注意力得分一次性修剪输入令牌。我们还与提示压缩方法
    (Li et al., [2023](#bib.bib19)) 进行了比较，该方法使用 LLMs 修剪输入上下文中的冗余。[表 1](#S5.T1 "在 5
    次实验中 ‣ LazyLLM：高效长上下文 LLM 推理的动态令牌修剪") 显示 *LazyLLM* 在多个任务中一贯实现了更好的 *TTFT* 加速，且准确度下降微乎其微。值得注意的是，运行
    LLMs 以压缩提示的开销非常大。尽管在减少的提示上的推理更快，但“提示压缩”基线的实际 *TTFT* 时间长于基线。'
- en: 5.2 *TTFT* Speedup *vs*. Accuracy
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 *TTFT* 加速与准确度
- en: 'The inference efficiency of *LazyLLM* is controlled using three parameters:
    1) the number of pruning layers, 2) the locations of these pruning layers, and
    3) the number of tokens pruned within these layers. Increasing the number of pruning
    layers and pruning more tokens optimize computation by processing fewer tokens,
    and pruning tokens at earlier layers can save the computations for the successive
    layers. Prompting these factors will give more overall computation reduction,
    and offer better *TTFT* speedup. As a side effect, excessively pruning tokens
    may cause information loss and eventually lead to performance degradation. Similarly,
    the *TTFT* speedup and accuracy of baselines can vary with different hyperparameters.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*LazyLLM* 的推理效率通过三个参数进行控制：1) 剪枝层的数量，2) 这些剪枝层的位置，以及 3) 在这些层内剪枝的 token 数量。增加剪枝层的数量并剪枝更多的
    tokens 通过处理更少的 tokens 来优化计算，而在早期层剪枝 tokens 可以节省后续层的计算。调整这些因素将带来更多的整体计算减少，并提供更好的
    *TTFT* 加速。作为副作用，过度剪枝可能导致信息丢失，最终导致性能下降。同样，基线的 *TTFT* 加速和准确性可能会因不同的超参数而有所变化。'
- en: 'We compare *TTFT* speedup *vs*. accuracy in [Figure 5](#S5.F5 "In 5.3 Impact
    on Overall Generation Speed ‣ 5 Experiments ‣ LazyLLM: Dynamic Token Pruning for
    Efficient Long Context LLM Inference") with different hyperparameters. The visualization
    shows that, without any training, the proposed *LazyLLM* retains the accuracy
    better than baselines under the same *TTFT* speedup. For example, our method can
    offer $2.34\times$ degradation in accuracy. On the other hand, baseline methods
    accuracy degrades significantly for similar *TTFT* speed-up. Note that the prompt
    compression approaches fail at improving *TTFT* because of the compression overhead.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 [图 5](#S5.F5 "在 5.3 对整体生成速度的影响 ‣ 5 实验 ‣ LazyLLM: 动态 Token 剪枝以提高长上下文 LLM
    推理效率") 中比较了 *TTFT* 加速 *与* 准确性，使用了不同的超参数。可视化结果显示，在没有任何训练的情况下，所提出的 *LazyLLM* 在相同的
    *TTFT* 加速下比基线保持了更好的准确性。例如，我们的方法可以提供 $2.34\times$ 的准确性降级。另一方面，基线方法在类似的 *TTFT* 加速下准确性显著下降。请注意，提示压缩方法由于压缩开销未能提高
    *TTFT*。'
- en: 5.3 Impact on Overall Generation Speed
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 对整体生成速度的影响
- en: To evaluate the impact of the proposed method on the overall generation process,
    we also profile the *$\%$ of Token Computed* indicates *LazyLLM* reduces the total
    computation, consequently offering additional speedup to the overall generation
    process across diverse tasks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估所提方法对整体生成过程的影响，我们还分析了 *计算的 Token 百分比*，这表明 *LazyLLM* 减少了总计算量，从而在各种任务中为整体生成过程提供了额外的加速。
- en: '![Refer to caption](img/d204b790048ec6b030af8ac2d35df6b6.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d204b790048ec6b030af8ac2d35df6b6.png)'
- en: 'Figure 5: *TTFT* speedup *vs*. accuracy comparison for Llama 2 7B across different
    tasks.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: *TTFT* 加速 *与* 准确性比较，针对 Llama 2 7B 在不同任务中。'
- en: '| Tasks | $\%$ of Prompt Token Computed |  | Overall Generation Speedup |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 提示 Token 计算的百分比 |  | 整体生成加速 |'
- en: '| Llama 2 | XGen |  | Llama 2 | XGen |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 | XGen |  | Llama 2 | XGen |'
- en: '| Single-Document QA | $87.31$ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 单文档问答 | $87.31$ |'
- en: '| Multi-Document QA | $63.94$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 多文档问答 | $63.94$ |'
- en: '| Summarization | $99.59$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 摘要生成 | $99.59$ |'
- en: '| Few-shot Learning | $69.98$ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 少样本学习 | $69.98$ |'
- en: '| Synthetic | $63.73$ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | $63.73$ |'
- en: '| Code Completion | $68.57$ |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 代码完成 | $68.57$ |'
- en: 'Table 2: The *$\%$ of Token Computed* indicates *LazyLLM* reduces the total
    computation, consequently offering additional speedup to the overall generation
    process across diverse tasks.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: *计算的 Token 百分比* 表明 *LazyLLM* 减少了总计算量，从而在各种任务中为整体生成过程提供了额外的加速。'
- en: 5.4 Drop Rate in Different Layers
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 不同层的丢弃率
- en: 'In this section, we analyze the effect of the locations of pruning layers,
    and the number of tokens pruned. In particular, we report a series of experiments
    using a simplified version of *LazyLLM* that prunes tokens just once within the
    transformer. For each trial, we position the pruning layer at various levels of
    the transformer stack and apply different pruning ratios. We perform the experiments
    for both Llama 2 and XGen, and visualize the results in [Figure 6](#S5.F6 "In
    5.4 Drop Rate in Different Layers ‣ 5 Experiments ‣ LazyLLM: Dynamic Token Pruning
    for Efficient Long Context LLM Inference").'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们分析了剪枝层的位置和剪枝的 token 数量的影响。特别是，我们报告了一系列使用 *LazyLLM* 简化版的实验，该版在 transformer
    内仅剪枝一次。在每次实验中，我们将剪枝层放置在 transformer 堆栈的不同层级，并应用不同的剪枝比率。我们对 Llama 2 和 XGen 进行了实验，并在
    [图 6](#S5.F6 "在 5.4 不同层的丢弃率 ‣ 5 实验 ‣ LazyLLM: 动态 Token 剪枝以提高长上下文 LLM 推理效率") 中可视化了结果。'
- en: 'The results show both models share a similar trend. As expected, when pruning
    at the same transformer layer, the model’s performance gradually decreases as
    fewer tokens are kept. Furthermore, pruning at later transformer layers consistently
    yields better performance compared to pruning at earlier layers, suggesting that
    later layers are less sensitive to token pruning. Based on these observations,
    we propose progressive token pruning in [Section 3.2](#S3.SS2 "3.2 Inference with
    LazyLLM ‣ 3 LazyLLM ‣ LazyLLM: Dynamic Token Pruning for Efficient Long Context
    LLM Inference"), which strategically prunes more tokens in later layers while
    preserving more in the earlier layers, optimizing the balance between efficiency
    and performance retention.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，两种模型具有相似的趋势。正如预期的那样，在相同的变换器层进行修剪时，模型的性能随着保留的令牌减少而逐渐下降。此外，与在早期层进行修剪相比，在后续变换器层进行修剪通常会产生更好的性能，这表明后续层对令牌修剪的敏感性较低。基于这些观察结果，我们在[第
    3.2 节](#S3.SS2 "3.2 使用 LazyLLM 进行推理 ‣ 3 LazyLLM ‣ LazyLLM：高效长上下文 LLM 推理的动态令牌修剪")
    中提出了递进式令牌修剪，这一策略在后续层修剪更多令牌，同时在早期层保留更多令牌，从而优化效率与性能保持之间的平衡。
- en: '![Refer to caption](img/a4f0a9266dac4b1b2f64b8b830a0b9bd.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a4f0a9266dac4b1b2f64b8b830a0b9bd.png)'
- en: 'Figure 6: Effect of the locations of pruning layers, and the number of tokens
    pruned. The results of both Llama 2 7B Touvron et al. ([2023](#bib.bib28)) and
    XGen 7B Nijkamp et al. ([2023](#bib.bib24)) share a similar trend: 1) when pruning
    at the same transformer layer, the model’s performance gradually decreases as
    fewer tokens are kept, and 2) Pruning at later transformer layers consistently
    has better performance than pruning at earlier layers, suggesting that later layers
    are less sensitive to token pruning.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：修剪层的位置及修剪的令牌数量的影响。Llama 2 7B Touvron 等人 ([2023](#bib.bib28)) 和 XGen 7B Nijkamp
    等人 ([2023](#bib.bib24)) 的结果显示出类似的趋势：1) 在相同的变换器层进行修剪时，随着保留的令牌减少，模型的性能逐渐下降；2) 在后续变换器层进行修剪通常比在早期层进行修剪表现更好，表明后续层对令牌修剪的敏感性较低。
- en: '![Refer to caption](img/724f5f3ad87c2a64772e8d31a2c779bb.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/724f5f3ad87c2a64772e8d31a2c779bb.png)'
- en: 'Figure 7: Statistics on number of tokens processed during generation using
    our LazyLLM technique with Llama 2 7B (Touvron et al., [2023](#bib.bib28)). We
    visualize the statistics of 1000 samples randomly sampled from LongBench. The
    $x$-axis represents the number of prompt tokens processed at that time step (normalized
    by the prompt size). We visualize these statistics for various stages within the
    network. Note that cumulative token usage is upper-bounded by the baseline (evident
    with early layers).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：使用我们LazyLLM技术与Llama 2 7B (Touvron 等人，[2023](#bib.bib28)) 生成过程中的处理令牌数量统计。我们可视化了从
    LongBench 随机抽取的 1000 个样本的统计数据。$x$轴表示在该时间步骤处理的提示令牌数量（按提示大小标准化）。我们为网络内的各个阶段可视化这些统计数据。请注意，累积令牌使用量受到基线的上限（在早期层明显）限制。
- en: 5.5 Progressive KV Growth
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 递进式 KV 增长
- en: 'In this section, we characterize the internals of the model with the token
    pruning logic. Specifically, we seek to understand what fractions of prompt tokens
    are cumulatively used and, inversely, not used. This “cumulative token usage”
    can be equivalently defined as the KV cache size at each given step. [Figure 7](#S5.F7
    "In 5.4 Drop Rate in Different Layers ‣ 5 Experiments ‣ LazyLLM: Dynamic Token
    Pruning for Efficient Long Context LLM Inference") presents these cumulative prompt
    token usage numbers for each of the stages of the LazyLLM.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过令牌修剪逻辑来描述模型的内部特性。具体而言，我们旨在理解提示令牌的哪些部分被累积使用，以及反向地，哪些未被使用。这种“累积令牌使用”可以等同于每个给定步骤的
    KV 缓存大小。[图 7](#S5.F7 "在 5.4 不同层的丢弃率 ‣ 5 实验 ‣ LazyLLM：高效长上下文 LLM 推理的动态令牌修剪") 展示了
    LazyLLM 各个阶段的累积提示令牌使用量。
- en: Our analysis supports the hypothesis that many tokens are never selected by
    the model (even though theoretically the model could use all tokens in the prompt).
    Since this model retains accuracy on the task(s), we can conclude that the model
    effectively drops the tokens which do not affect the output quality.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析支持这样一个假设：许多令牌从未被模型选择（尽管理论上模型可以使用提示中的所有令牌）。由于该模型在任务上保持了准确性，我们可以得出结论：模型有效地丢弃了不会影响输出质量的令牌。
- en: 6 Conclusion
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this work, we proposed a novel *LazyLLM* technique for efficient LLM inference,
    in particular under long context scenarios. *LazyLLM* selectively computes the
    KV for tokens important for the next token prediction and “lazily” defers the
    computation of remaining tokens to later steps, when they become relevant. We
    carefully examine *LazyLLM* on various tasks, where we observed the proposed method
    effectively reduces *TTFT* with negligible performance loss. It is worth noting
    that our method can be seamlessly integrated with existing transformer-based LLMs
    to improve their inference speed without requiring any fine-tuning.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种新颖的 *LazyLLM* 技术，用于高效的 LLM 推理，特别是在长上下文场景下。*LazyLLM* 选择性地计算对下一个令牌预测重要的
    KV，并“懒惰地”将其余令牌的计算推迟到后续步骤，当它们变得相关时。我们仔细检查了 *LazyLLM* 在各种任务中的表现，观察到所提出的方法有效减少了 *TTFT*，且几乎没有性能损失。值得注意的是，我们的方法可以无缝集成到现有的基于转换器的
    LLM 中，以提高推理速度，而无需任何微调。
- en: References
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Adnan et al. (2024) Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J
    Nair, Ilya Soloveychik, and Purushotham Kamath. Keyformer: Kv cache reduction
    through key tokens selection for efficient generative inference. *arXiv preprint
    arXiv:2403.09054*, 2024.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Adnan 等 (2024) Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J Nair,
    Ilya Soloveychik, 和 Purushotham Kamath. Keyformer: 通过关键令牌选择减少 KV 缓存，以实现高效的生成推理。*arXiv
    预印本 arXiv:2403.09054*，2024年。'
- en: 'Aminabadi et al. (2022) Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad
    Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang,
    Jeff Rasley, et al. Deepspeed-inference: enabling efficient inference of transformer
    models at unprecedented scale. In *SC22: International Conference for High Performance
    Computing, Networking, Storage and Analysis*, pp.  1–15\. IEEE, 2022.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Aminabadi 等 (2022) Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad
    Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang,
    Jeff Rasley 等。Deepspeed-inference: 实现前所未有规模的转换器模型高效推理。在 *SC22: 国际高性能计算、网络、存储与分析会议*，第1–15页。IEEE，2022年。'
- en: Anagnostidis et al. (2024) Sotiris Anagnostidis, Dario Pavllo, Luca Biggio,
    Lorenzo Noci, Aurelien Lucchi, and Thomas Hofmann. Dynamic context pruning for
    efficient and interpretable autoregressive transformers. *Advances in Neural Information
    Processing Systems*, 36, 2024.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anagnostidis 等 (2024) Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo
    Noci, Aurelien Lucchi, 和 Thomas Hofmann. 动态上下文剪枝用于高效且可解释的自回归转换器。*神经信息处理系统进展*，36，2024年。
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench:
    A bilingual, multitask benchmark for long context understanding. *arXiv preprint
    arXiv:2308.14508*, 2023.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai 等 (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou 等。Longbench: 一个双语、多任务基准，用于长上下文理解。*arXiv
    预印本 arXiv:2308.14508*，2023年。'
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer:
    The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy 等 (2020) Iz Beltagy, Matthew E Peters, 和 Arman Cohan. Longformer: 长文档转换器。*arXiv
    预印本 arXiv:2004.05150*，2020年。'
- en: 'Bhendawade et al. (2024) Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry
    Mason, Mohammad Rastegari, and Mahyar Najibi. Speculative streaming: Fast llm
    inference without auxiliary models. *arXiv preprint arXiv:2402.11131*, 2024.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bhendawade 等 (2024) Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason,
    Mohammad Rastegari, 和 Mahyar Najibi. 推测流: 无需辅助模型的快速 LLM 推理。*arXiv 预印本 arXiv:2402.11131*，2024年。'
- en: 'Cai et al. (2024) Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D
    Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework
    with multiple decoding heads. *arXiv preprint arXiv:2401.10774*, 2024.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai 等 (2024) Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee,
    Deming Chen, 和 Tri Dao. Medusa: 简单的 LLM 推理加速框架，具有多个解码头。*arXiv 预印本 arXiv:2401.10774*，2024年。'
- en: 'Chen et al. (2023) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
    Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context
    large language models. *arXiv preprint arXiv:2309.12307*, 2023.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 (2023) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu,
    Song Han, 和 Jiaya Jia. Longlora: 高效的长上下文大型语言模型微调。*arXiv 预印本 arXiv:2309.12307*，2023年。'
- en: 'Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    *Advances in Neural Information Processing Systems*, 35:16344–16359, 2022.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao 等 (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, 和 Christopher Ré.
    Flashattention: 快速且内存高效的精确注意力，具有 IO 关注。*神经信息处理系统进展*，35:16344–16359，2022年。'
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*, 2018.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    Bert: 深度双向变换器的预训练用于语言理解。*arXiv 预印本 arXiv:1810.04805*，2018。'
- en: 'Elhoushi et al. (2024) Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich,
    Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal,
    Ahmed Roman, et al. Layer skip: Enabling early exit inference and self-speculative
    decoding. *arXiv preprint arXiv:2404.16710*, 2024.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elhoushi et al. (2024) Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich,
    Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal,
    Ahmed Roman 等. 层跳过：实现早期退出推理和自我推测解码。*arXiv 预印本 arXiv:2404.16710*，2024。
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan
    Alistarh. Gptq: 生成预训练变换器的准确后训练量化。*arXiv 预印本 arXiv:2210.17323*，2022。'
- en: 'He et al. (2021) Xuanli He, Iman Keivanloo, Yi Xu, Xiang He, Belinda Zeng,
    Santosh Rajagopalan, and Trishul Chilimbi. Magic pyramid: Accelerating inference
    with early exiting and token pruning. *arXiv preprint arXiv:2111.00230*, 2021.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2021) Xuanli He, Iman Keivanloo, Yi Xu, Xiang He, Belinda Zeng, Santosh
    Rajagopalan, 和 Trishul Chilimbi. 魔法金字塔：通过早期退出和标记剪枝加速推理。*arXiv 预印本 arXiv:2111.00230*，2021。
- en: 'Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,
    and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large
    language models. *arXiv preprint arXiv:2310.05736*, 2023.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,
    和 Lili Qiu. Llmlingua: 压缩提示以加速大型语言模型的推理。*arXiv 预印本 arXiv:2310.05736*，2023。'
- en: Kim et al. (2022) Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk
    Kwon, Joseph Hassoun, and Kurt Keutzer. Learned token pruning for transformers.
    In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining*, pp.  784–794, 2022.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2022) Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk
    Kwon, Joseph Hassoun, 和 Kurt Keutzer. 变换器的学习型标记剪枝。在*第28届 ACM SIGKDD 知识发现与数据挖掘大会论文集*，第784–794页，2022。
- en: 'Kim et al. (2023) Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo
    Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W
    Mahoney, et al. Full stack optimization of transformer inference: a survey. *arXiv
    preprint arXiv:2302.14017*, 2023.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2023) Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang,
    Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney
    等. 变换器推理的全栈优化：综述。*arXiv 预印本 arXiv:2302.14017*，2023。
- en: 'Kitaev et al. (2020) Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer:
    The efficient transformer. *arXiv preprint arXiv:2001.04451*, 2020.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kitaev et al. (2020) Nikita Kitaev, Łukasz Kaiser, 和 Anselm Levskaya. Reformer:
    高效的变换器。*arXiv 预印本 arXiv:2001.04451*，2020。'
- en: Leviathan et al. (2023) Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast
    inference from transformers via speculative decoding. In *International Conference
    on Machine Learning*, pp.  19274–19286\. PMLR, 2023.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leviathan et al. (2023) Yaniv Leviathan, Matan Kalman, 和 Yossi Matias. 通过推测解码实现变换器的快速推理。在*国际机器学习会议*，第19274–19286页。PMLR，2023。
- en: Li et al. (2023) Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing
    context to enhance inference efficiency of large language models. *arXiv preprint
    arXiv:2310.06201*, 2023.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Yucheng Li, Bo Dong, Chenghua Lin, 和 Frank Guerin. 压缩上下文以增强大语言模型的推理效率。*arXiv
    预印本 arXiv:2310.06201*，2023。
- en: 'Li et al. (2024) Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr
    Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm
    knows what you are looking for before generation. *arXiv preprint arXiv:2404.14469*,
    2024.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2024) Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr
    Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, 和 Deming Chen. Snapkv: Llm 知道你在生成之前正在寻找什么。*arXiv
    预印本 arXiv:2404.14469*，2024。'
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. *Advances in neural information
    processing systems*, 36:21702–21720, 2023.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, 和 Xinchao Wang. Llm-pruner: 关于大型语言模型的结构剪枝。*神经信息处理系统进展*，36:21702–21720，2023。'
- en: 'Merth et al. (2024) Thomas Merth, Qichen Fu, Mohammad Rastegari, and Mahyar
    Najibi. Superposition prompting: Improving and accelerating retrieval-augmented
    generation. 2024. URL [https://api.semanticscholar.org/CorpusID:269033436](https://api.semanticscholar.org/CorpusID:269033436).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merth et al. (2024) Thomas Merth, Qichen Fu, Mohammad Rastegari, 和 Mahyar Najibi.
    超级位置提示：改进和加速检索增强生成。2024。网址 [https://api.semanticscholar.org/CorpusID:269033436](https://api.semanticscholar.org/CorpusID:269033436)。
- en: 'Nawrot et al. (2024) Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David
    Tarjan, and Edoardo M Ponti. Dynamic memory compression: Retrofitting llms for
    accelerated inference. *arXiv preprint arXiv:2403.09636*, 2024.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nawrot 等 (2024) 皮奥特·纳沃特、阿德里安·兰茨基、马尔钦·乔霍夫斯基、戴维·塔尔詹、和爱德华多·M·庞蒂。动态内存压缩：为加速推理改造LLMs。*arXiv
    预印本 arXiv:2403.09636*，2024年。
- en: Nijkamp et al. (2023) Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying
    Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, et al. Xgen-7b
    technical report. *arXiv preprint arXiv:2309.03450*, 2023.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nijkamp 等 (2023) 埃里克·尼克坎普、田·谢、广桥·林、波·庞、从英·夏、陈·邢、杰西·维格、塞米赫·亚武兹、菲利普·拉班、本·克劳斯，等。Xgen-7b
    技术报告。*arXiv 预印本 arXiv:2309.03450*，2023年。
- en: 'NVIDIA (2024) NVIDIA. NVIDIA L40S: Unparalleled AI and graphics performance
    for the data center. [https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413](https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413),
    2024. [Online; accessed 31-May-2024].'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'NVIDIA (2024) NVIDIA。NVIDIA L40S: 数据中心无与伦比的AI和图形性能。 [https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413](https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413)，2024年。[在线；访问日期
    2024年5月31日]。'
- en: Pope et al. (2023) Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin,
    James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently
    scaling transformer inference. *Proceedings of Machine Learning and Systems*,
    5, 2023.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pope 等 (2023) 莱纳·教皇、肖尔托·道格拉斯、阿坎克莎·乔杜赫里、雅各布·德夫林、詹姆斯·布拉德伯里、乔纳森·赫克、科凡·肖、希瓦尼·阿格拉瓦尔、和杰夫·迪安。高效扩展变换器推理。*机器学习与系统会议录*，5，2023年。
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple
    and effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 (2023) 孙明杰、庄刘、安娜·贝尔、和J·齐科·科尔特。大型语言模型的简单有效剪枝方法。*arXiv 预印本 arXiv:2306.11695*，2023年。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 (2023) 雨果·图夫龙、路易斯·马丁、凯文·斯通、彼得·阿尔伯特、阿姆贾德·阿尔马黑里、雅斯敏·巴巴伊、尼古拉·巴什利科夫、索姆亚·巴特拉、普拉杰瓦尔·巴尔加瓦、舒丽提·博萨尔，等。Llama
    2: 开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023年。'
- en: 'Xu et al. (2023) Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang,
    Kaixiong Zhou, Xia Hu, and Anshumali Shrivastava. Compress, then prompt: Improving
    accuracy-efficiency trade-off of llm inference with transferable prompt. *arXiv
    preprint arXiv:2305.11186*, 2023.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2023) 赵卓·徐、紫睿·刘、贝迪·陈、宇欣·唐、觉·王、凯雄·周、夏·胡、和安舒玛利·施里瓦斯塔瓦。压缩，然后提示：通过可转移提示提高LLM推理的准确性效率权衡。*arXiv
    预印本 arXiv:2305.11186*，2023年。
- en: 'Yao et al. (2022) Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia
    Zhang, Cheng Li, and Yuxiong He. Random-ltd: Random and layerwise token dropping
    brings efficient training for large-scale transformers. *arXiv preprint arXiv:2211.11586*,
    2022.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等 (2022) 赵威·姚、萧霞·吴、丛龙·李、康纳·霍尔姆斯、敏佳·张、程李、和宇雄·赫。Random-ltd: 随机和逐层令牌丢弃为大规模变换器带来高效训练。*arXiv
    预印本 arXiv:2211.11586*，2022年。'
- en: 'Zhang et al. (2024) Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al.
    H2o: Heavy-hitter oracle for efficient generative inference of large language
    models. *Advances in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 (2024) 张振宇、英 Sheng、田毅·周、田龙·陈、连敏·郑、瑞斯·蔡、赵松、袁东·田、克里斯托弗·Ré、克拉克·巴雷特，等。H2o:
    针对大型语言模型的高效生成推理的重击者预言器。*神经信息处理系统进展*，36，2024年。'
