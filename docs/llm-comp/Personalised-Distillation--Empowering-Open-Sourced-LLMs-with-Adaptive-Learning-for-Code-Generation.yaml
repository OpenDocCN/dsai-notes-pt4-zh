- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:59:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:59:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning
    for Code Generation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 个性化提炼：通过自适应学习赋能开源LLMs的代码生成
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.18628](https://ar5iv.labs.arxiv.org/html/2310.18628)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.18628](https://ar5iv.labs.arxiv.org/html/2310.18628)
- en: Hailin Chen^∗^♣^♠, Amrita Saha^∗^♠, Steven HOI^♠, Shafiq Joty^♣^♠
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Hailin Chen^∗^♣^♠, Amrita Saha^∗^♠, Steven HOI^♠, Shafiq Joty^♣^♠
- en: ^♣ Nanyang Technological University, Singapore
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ^♣ 南洋理工大学，新加坡
- en: ^♠ Salesforce Research
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^♠ Salesforce Research
- en: '{hailin001, srjoty}@ntu.edu.sg'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{hailin001, srjoty}@ntu.edu.sg'
- en: '{amrita.saha, shoi}@salesforce.com'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{amrita.saha, shoi}@salesforce.com'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing
    interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced
    LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of
    instructions and answers, for the student model to learn. However, such standard
    distillation approach neglects the merits and conditions of the student model.
    Inspired by modern teaching principles, we design a personalised distillation
    process, in which the student attempts to solve a task first, then the teacher
    provides an adaptive refinement for the student to improve. Instead of feeding
    the student with teacher’s prior, personalised distillation enables personalised
    learning for the student model, as it only learns on examples it makes mistakes
    upon and learns to improve its own solution. On code generation, personalised
    distillation consistently outperforms standard distillation with only one third
    of the data. With only 2.5-3K personalised examples that incur a data-collection
    cost of 4-6$, we boost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder
    by 12.2% to achieve 45.8% pass@1 on HumanEval.¹¹1Our codes will be available at
    [https://github.com/salesforce/PersDistill](https://github.com/salesforce/PersDistill)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着强大的闭源LLMs（ChatGPT、GPT-4）的崛起，人们越来越关注如何将闭源LLMs的能力提炼到更小的开源LLMs中。以往的提炼方法通常要求ChatGPT生成一组指令和答案，供学生模型学习。然而，这种标准提炼方法忽视了学生模型的优点和条件。受到现代教学原则的启发，我们设计了一个个性化的提炼过程，在此过程中，学生首先尝试解决一个任务，然后教师为学生提供适应性的改进。与其将教师的先前知识喂给学生，个性化提炼使学生模型能够进行个性化学习，因为它只学习那些出错的例子，并学习改进自己的解决方案。在代码生成方面，个性化提炼在仅使用三分之一的数据的情况下，始终优于标准提炼。仅用2.5-3K个个性化示例，数据收集成本为4-6美元，我们将CodeGen-mono-16B的表现提高了7%，达到了36.4%的pass@1，并将StarCoder的表现提高了12.2%，达到了45.8%的pass@1。¹¹1我们的代码将发布在[https://github.com/salesforce/PersDistill](https://github.com/salesforce/PersDistill)
- en: '^*^*footnotetext: These authors contributed equally to this work'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ^*^*脚注：这些作者对本工作作出了同等贡献
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: 'Recently, powerful close-sourced large langauge models (LLMs) including ChatGPT,
    GPT-4 have become predominant, accumulating over 170 million users within 5 month
    of its launch. Such close-sourced LLMs demonstrate strong performance in a wide
    range of tasks, from improving writing proficiency to code generation. However,
    due to their closed-source nature, concerns have been raised regarding factors
    such as the availability of these services, high associated costs, concerns on
    ethics and safety, and potential data privacy implications, all of which limit
    their seamless integration into real-world applications. In light of these concerns,
    a natural question arises: Can we distill the remarkable abilities exhibited by
    closed-source LLMs into smaller open-source LLMs?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，强大的闭源大型语言模型（LLMs），如ChatGPT、GPT-4，已经成为主流，在发布后的5个月内积累了超过1.7亿用户。这些闭源LLMs在多种任务中表现出色，从提高写作能力到代码生成。然而，由于其闭源性质，出现了一些问题，如服务的可用性、高昂的成本、伦理和安全性问题以及潜在的数据隐私影响，这些都限制了它们在实际应用中的无缝整合。面对这些问题，一个自然的问题出现了：我们能否将闭源LLMs展示的卓越能力提炼到更小的开源LLMs中？
- en: Researchers have explored such distillation idea Taori et al. ([2023](#bib.bib20));
    Wang et al. ([2022](#bib.bib21)); Xu et al. ([2023b](#bib.bib25)), by querying
    ChatGPT to generate task instruction and solution pairs, and using the collected
    data to finetune a student model. However, this standard distillation approach
    fits different student models to the same data distribution (teacher’s prior),
    disregarding their unique abilities and capacity. In education domain, personalised
    learning which provides customized learning experience that adapts to student’s
    learning progress and capacity, has proven highly effective and widely adopted
    Roberts-Mahoney et al. ([2016](#bib.bib17)); Shemshack and Spector ([2020](#bib.bib18)).
    Inspired by such finding, we hypothesize that personalised learning is also beneficial
    for model distillation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员探索了这样的蒸馏思想 Taori et al. ([2023](#bib.bib20)); Wang et al. ([2022](#bib.bib21));
    Xu et al. ([2023b](#bib.bib25))，通过查询ChatGPT生成任务指令和解决方案对，并使用收集的数据来微调学生模型。然而，这种标准蒸馏方法将不同的学生模型适配到相同的数据分布（教师的先验知识），忽略了它们的独特能力和容量。在教育领域，个性化学习提供了适应学生学习进度和能力的定制化学习体验，已被证明极为有效并广泛应用
    Roberts-Mahoney et al. ([2016](#bib.bib17)); Shemshack and Spector ([2020](#bib.bib18))。受到这一发现的启发，我们假设个性化学习对模型蒸馏也有益。
- en: 'In this work, we propose personalised distillation and empirically evaluate
    its effectiveness in the domain of code generation. Similar to standard distillation,
    we first employ ChatGPT to generate task instructions accompanied by unit test
    cases. Then we follow three steps for personalized distillation as shown in Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Personalised Distillation: Empowering
    Open-Sourced LLMs with Adaptive Learning for Code Generation"). First, we let
    the student model attempt to solve the task. Then, we evaluate the student’s attempt
    with unit test cases and get execution feedback. If the execution feedback contains
    errors, in the final step we prompt the teacher model (ChatGPT) to refine the
    student’s attempt.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们提出了个性化蒸馏方法，并在代码生成领域对其有效性进行了实证评估。类似于标准蒸馏方法，我们首先使用ChatGPT生成带有单元测试用例的任务指令。然后，我们按照图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Personalised Distillation: Empowering Open-Sourced
    LLMs with Adaptive Learning for Code Generation")所示的三个步骤进行个性化蒸馏。首先，我们让学生模型尝试解决任务。然后，我们用单元测试用例评估学生的尝试并获得执行反馈。如果执行反馈包含错误，在最后一步中，我们提示教师模型（ChatGPT）来完善学生的尝试。'
- en: Such data collection process makes the learning experience both interactive
    — as the student participates to make attempts, and personalised — both the input
    (tasks) and output (refinement data) are customised to the student. Essentially,
    personalised labeled data help the student to refine its own policy, rather than
    adopting a new prior of the teacher.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的数据收集过程使学习体验既具有互动性——因为学生参与了尝试，又具有个性化——输入（任务）和输出（改进数据）都针对学生进行了定制。实质上，个性化标记数据帮助学生改进自身的策略，而不是采用教师的新的先验知识。
- en: '![Refer to caption](img/1b7284092ed3cd1e81dfa969ed9df1b0.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1b7284092ed3cd1e81dfa969ed9df1b0.png)'
- en: 'Figure 1: Overview of our framework. Left: standard distillation.1Teacher
    generates standard answer to a given problem for the student to learn Right: personalised
    distillation.1Student
    first generates its own attempt to solve the task.2Executor evaluates
    generated code with unit test cases.3Teacher provides
    adaptive refinement given student’s attempt and its execution feedback.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们框架的概述。左：标准蒸馏。1老师为学生提供给定问题的标准答案供其学习
    右：个性化蒸馏。1学生首先生成自己的任务解决尝试。2执行器使用单元测试用例评估生成的代码。3老师根据学生的尝试及其执行反馈提供自适应精炼。
- en: With the personalized code data as target output, we construct three variants
    of finetuning data (i) PERsD data which formats it as a typical text-to-code generation
    task, (ii) PERsD-refine which treats it as a code-refinement task, given a task
    instruction, incorrect code and execution error feedback (ii) PERsD-combine which
    simply combines PERsD and PERsD-refine finetuning data, i.e. code generation and
    refinement tasks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以个性化代码数据为目标输出，我们构建了三种微调数据的变体：（i）PERsD数据，将其格式化为典型的文本到代码生成任务，（ii）PERsD-refine，将其视为代码精炼任务，给出任务指令、错误代码和执行错误反馈，（iii）PERsD-combine，简单地将PERsD和PERsD-refine微调数据合并，即代码生成和精炼任务。
- en: We collect 10K standard distillation examples and around 2.5-3K personalised
    examples for pretraining. Through zero-shot evaluation on HumanEval Chen et al.
    ([2021](#bib.bib4)) and MBPP Austin et al. ([2021](#bib.bib1)), we observe that
    all PERsD variants consistently outperform their counterparts which use standard
    distillation. This compelling result strongly validates our hypothesis regarding
    the advantages of personalized distillation. Ablation studies further reinforce
    our hypothesis, uncovering intriguing properties such as the benefits of multi-round
    personalized distillation and the ability of our models to leverage execution
    feedback for self-correction. Notably, personalised distillation boosts the state-of-the-art
    open-sourced pretrain model StarCoder Li et al. ([2023a](#bib.bib8)) significantly
    — by 12.2% to achieve 45.8 in pass@1 and 82.3 in pass@100 on HumanEval.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收集了 10K 标准蒸馏示例和约 2.5-3K 个性化示例用于预训练。通过在 HumanEval Chen 等 ([2021](#bib.bib4))
    和 MBPP Austin 等 ([2021](#bib.bib1)) 上的零样本评估，我们观察到所有 PERsD 变体一致优于使用标准蒸馏的对应方法。这一引人注目的结果强有力地验证了我们关于个性化蒸馏优势的假设。消融研究进一步加强了我们的假设，揭示了如多轮个性化蒸馏的好处以及我们的模型利用执行反馈进行自我修正的能力等有趣的属性。值得注意的是，个性化蒸馏显著提升了最先进的开源预训练模型
    StarCoder Li 等 ([2023a](#bib.bib8)) — 提升了 12.2%，在 HumanEval 上达到 pass@1 45.8 和
    pass@100 82.3。
- en: 2 Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Distillation from ChatGPT
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 从 ChatGPT 的蒸馏
- en: Previous works have explored distillation from ChatGPT including AlpacaTaori
    et al. ([2023](#bib.bib20)), VicunaChiang et al. ([2023](#bib.bib6)) and BaizeXu
    et al. ([2023b](#bib.bib25)). However, these works can all be considered as standard
    distillation as they do not consider the conditions and capacity of student model.
    WizardLMXu et al. ([2023a](#bib.bib24)) and WizardCoderLuo et al. ([2023](#bib.bib11))
    iteratively prompts teacher model to generate more complex instructions. Their
    approach can be seen as an orthogonal advancement that can potentially be combined
    with personalised distillation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的工作探讨了从 ChatGPT 进行蒸馏，包括 AlpacaTaori 等 ([2023](#bib.bib20))、VicunaChiang 等
    ([2023](#bib.bib6)) 和 BaizeXu 等 ([2023b](#bib.bib25))。然而，这些工作都可以被视为标准蒸馏，因为它们没有考虑学生模型的条件和能力。WizardLMXu
    等 ([2023a](#bib.bib24)) 和 WizardCoderLuo 等 ([2023](#bib.bib11)) 通过迭代提示教师模型生成更复杂的指令。它们的方法可以被视为一种正交的进展，可能与个性化蒸馏相结合。
- en: Lion Jiang et al. ([2023](#bib.bib7)) proposes to incorporate student model’s
    answer and sample more hard tasks for which the student failed to solve. Thus,
    Lion can be considered as input personalised distillation as only the input tasks
    are customised for different student. Our approach differs as we provide customization
    both on input and output, and we empirically show that personalising labels is
    critically beneficial.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Lion Jiang 等 ([2023](#bib.bib7)) 提出了结合学生模型的答案并为学生未能解决的更困难任务采样。因此，Lion 可以被视为输入个性化蒸馏，因为仅输入任务会根据不同的学生进行定制。我们的方法有所不同，因为我们在输入和输出上都提供定制，并且我们通过实证研究表明，个性化标签是至关重要的。
- en: '|   Methods | Personalised | Interactive | Code-related |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|   方法 | 个性化 | 互动 | 与代码相关 |'
- en: '| Alpaca | ✗ | ✗ | ✗ |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca | ✗ | ✗ | ✗ |'
- en: '| Vicuna | ✗ | ✗ | ✗ |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna | ✗ | ✗ | ✗ |'
- en: '| Baize | ✗ | ✗ | ✗ |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| Baize | ✗ | ✗ | ✗ |'
- en: '| WizardLM | ✗ | ✗ | ✗ |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| WizardLM | ✗ | ✗ | ✗ |'
- en: '| WizardCoder | ✗ | ✗ | ✓ |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| WizardCoder | ✗ | ✗ | ✓ |'
- en: '| Lion | Input | ✓ | ✗ |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Lion | 输入 | ✓ | ✗ |'
- en: '| \hdashlinePERsD | Input + Output | ✓ | ✓ |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlinePERsD | 输入 + 输出 | ✓ | ✓ |'
- en: '|   |  |  |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |'
- en: 'Table 1: Related work on distillation from ChatGPT'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：来自 ChatGPT 的蒸馏相关工作
- en: 2.2 Code Generation with Feedback
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 反馈下的代码生成
- en: Recently, there has been an increasing amount of research on exploring on how
    to use feedback for an iterative and improved code generation through code-refinement.
    Self-refineMadaan et al. ([2023](#bib.bib12)), Self-debugChen et al. ([2023b](#bib.bib5))
    and Reflexion Shinn et al. ([2023](#bib.bib19)) are inference-time methods which
    use powerful close-sourced LLMs to generate better code from internal or external
    feedback. Although they show high performance, these methods are limited as they
    require access to close-sourced LLMs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，关于如何利用反馈进行迭代和改进代码生成的研究越来越多，自我修正Madaan 等 ([2023](#bib.bib12))、自我调试Chen 等 ([2023b](#bib.bib5))
    和 Reflexion Shinn 等 ([2023](#bib.bib19)) 是利用强大的闭源 LLMs 从内部或外部反馈中生成更好代码的推理时间方法。尽管它们显示出高性能，但这些方法的局限性在于它们需要访问闭源
    LLMs。
- en: 'Self-edit Zhang et al. ([2023](#bib.bib26)) trains a separate code editor to
    rectify generated code from a base LLM. The training label is from original gold
    answer, thus not label-personalised. Similarly, Self-correct Welleck et al. ([2022](#bib.bib22))
    trains a separate corrector model to rectify the output from a fixed generator
    model. However, the training label is from self-exploration of the corrector model:
    sampling multiple refinements and choosing the one leading to higher reward. Finally,
    ILF Chen et al. ([2023a](#bib.bib3)) collects human-annotated code refinement
    data to train a separate refinement model on it. Fhe refinement model is used
    to generate text-to-code data for finetuning the code-generation LLM. Unlike ILF,
    our approach is more scalable as we do not require human annotation and our personalized
    data proves significantly more effective than ILF as we empirically investigate
    in [Section 5](#S5 "5 Experimental Results ‣ Personalised Distillation: Empowering
    Open-Sourced LLMs with Adaptive Learning for Code Generation").'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '自我编辑 Zhang et al. ([2023](#bib.bib26)) 训练了一个独立的代码编辑器来修正来自基础 LLM 的生成代码。训练标签来自原始的金标准答案，因此不是个性化的标签。类似地，自我纠正
    Welleck et al. ([2022](#bib.bib22)) 训练了一个独立的修正模型来修正来自固定生成模型的输出。然而，训练标签来自修正模型的自我探索：采样多个改进方案并选择奖励更高的方案。最后，ILF
    Chen et al. ([2023a](#bib.bib3)) 收集了人工标注的代码改进数据，用于训练一个独立的改进模型。该改进模型用于生成文本到代码的数据，以微调代码生成
    LLM。与 ILF 不同，我们的方法更具可扩展性，因为我们不需要人工标注，我们的个性化数据在 [第 5 节](#S5 "5 Experimental Results
    ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning
    for Code Generation") 中的实证研究表明，比 ILF 更有效。'
- en: '|   Methods | Training | Inference |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|   方法 | 训练 | 推断 |'
- en: '| \cdashline2-6 | Single | Data Source | Personalised | w/ execution | w/o
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-6 | 单一 | 数据来源 | 个性化 | 带执行 | 不带 |'
- en: '|  | Model |  |  | feedback | ChatGPT |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型 |  |  | 反馈 | ChatGPT |'
- en: '| Self-refine | ✓ | No Training | ✗ | ✗ | ✗ |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 自我改进 | ✓ | 无训练 | ✗ | ✗ | ✗ |'
- en: '| Self-debug | ✓ | No Training | ✗ | ✓ | ✗ |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 自我调试 | ✓ | 无训练 | ✗ | ✓ | ✗ |'
- en: '| Reflexion | ✓ | No Training | ✗ | ✓ | ✗ |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Reflexion | ✓ | 无训练 | ✗ | ✓ | ✗ |'
- en: '| Self-edit | ✗ | Standard GT | ✗ | ✓ | ✓ |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 自我编辑 | ✗ | 标准 GT | ✗ | ✓ | ✓ |'
- en: '| Self-correct | ✗ | Self-exploration | ✓ | ✓ | ✓ |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 自我纠正 | ✗ | 自我探索 | ✓ | ✓ | ✓ |'
- en: '| ILF | ✗ | Human labeled | ✓ | ✓ | ✓ |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| ILF | ✗ | 人工标注 | ✓ | ✓ | ✓ |'
- en: '| \hdashlinePERsD-refine | ✓ | ChatGPT | ✓ | ✓ | ✓ |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlinePERsD-refine | ✓ | ChatGPT | ✓ | ✓ | ✓ |'
- en: '|   |  |  |  |  |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |'
- en: 'Table 2: Related work on Code Generation w/ feedback'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：带反馈的代码生成相关工作
- en: 2.3 Reinforcement Learning from (Human) Feedback
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 从（人类）反馈中强化学习
- en: After the launch of ChatGPT, aligning LLMs to human preference has drawn tremendous
    attention to research communities. As one of the most influential approaches in
    this direction, reinforcement learning from human feedback (RLHF) Ouyang et al.
    ([2022](#bib.bib14)); Li et al. ([2023b](#bib.bib9)), adopts an actor-critic framework,
    where the student model is optimized to generate responses to receive higher reward
    from the critic model. In InstructGPT Ouyang et al. ([2022](#bib.bib14)), the
    critic (reward model) is trained from human annotation. Direct Preference Optimization
    (DPO) Rafailov et al. ([2023](#bib.bib15)) drops the need of training a reward
    model, by using a reference LLM and offline trajectories to estimate the reward.
    Chain-of-Hindsight Liu et al. ([2023](#bib.bib10)) converts human preference annotations
    into simple natural language feedback, and thus turns RL optimization to conditional
    generation. In above methods, the assumption is that there are no ground truth
    targets and thus they try to improve the LLM based on the assessment (critic)
    of multiple generated outputs. However, such RL-style training will be less effective
    and efficient to supervised finetuning, especially for challenging tasks with
    sparse rewards – e.g. sovling math puzzles or coding tasks. Unlike these methods,
    our approach can acquire "ground truth" outputs from a personalised teacher, thus
    supervised finetuning can be applied which makes the learning effective and efficient,
    even for challenging tasks like solving coding problems.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ChatGPT 发布之后，将大语言模型（LLMs）对齐到人类偏好引起了研究界的极大关注。作为这一方向上最具影响力的方法之一，从人类反馈中进行强化学习（RLHF）Ouyang
    et al. ([2022](#bib.bib14)); Li et al. ([2023b](#bib.bib9))，采用了一个演员-评论家框架，其中学生模型被优化以生成回应，从而从评论家模型那里获得更高的奖励。在
    InstructGPT Ouyang et al. ([2022](#bib.bib14)) 中，评论家（奖励模型）是通过人类注释进行训练的。直接偏好优化（DPO）Rafailov
    et al. ([2023](#bib.bib15)) 通过使用参考 LLM 和离线轨迹来估计奖励，从而省去了训练奖励模型的需要。回顾链 Liu et al.
    ([2023](#bib.bib10)) 将人类偏好注释转换为简单的自然语言反馈，从而将强化学习优化转变为条件生成。在上述方法中，假设没有真实目标，因此他们试图根据多个生成输出的评估（评论家）来改进
    LLM。然而，这种 RL 风格的训练对于监督微调的效果和效率较差，尤其是在奖励稀疏的挑战性任务中——例如解决数学难题或编码任务。与这些方法不同，我们的方法可以从个性化教师那里获取“真实目标”输出，因此可以应用监督微调，使学习更有效和高效，即使在解决编码问题等挑战性任务中也是如此。
- en: 3 Method
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 3.1 Standard Distillation
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 标准蒸馏
- en: Assume a dataset of code generation tasks $\mathcal{D}=\{(t,u)\}$.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个代码生成任务的数据集 $\mathcal{D}=\{(t,u)\}$。
- en: We then finetune the student model $\pi_{\theta}$. We name this approach StanD.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对学生模型 $\pi_{\theta}$ 进行微调。我们将这种方法命名为 StanD。
- en: Algorithm 1 personalised distillation for code generation (PERsD-combined).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 个性化蒸馏用于代码生成（PERsD-综合）。
- en: '1:Input: Dataset $\mathcal{D_{\textsc{StanD}}}$'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入：数据集 $\mathcal{D_{\textsc{StanD}}}$'
- en: 3.2 Personalised Distillation
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 个性化蒸馏
- en: 'The StanD approach simply samples training examples (instructions and labels)
    from the prior distribution of the teacher model and feeds it to the student without
    considering the conditions of the student model. Inspired by modern education
    principles which advocates interactive and personalised learning experience, we
    propose personalised distillation: adapting teaching materials to student’s current
    knowledge and capacity. We propose three variants:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: StanD 方法仅从教师模型的先验分布中采样训练示例（指令和标签），然后将其输入学生模型，而不考虑学生模型的条件。受现代教育原则的启发，现代教育提倡互动和个性化学习体验，我们提出了个性化蒸馏：将教学材料调整为学生当前的知识和能力。我们提出了三种变体：
- en: 'PERsD-combined  Algorithm [1](#alg1 "Algorithm 1 ‣ 3.1 Standard Distillation
    ‣ 3 Method ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive
    Learning for Code Generation") shows detailed steps for PERsD-combined. This method
    takes the standard distillation dataset $\mathcal{D}_{\textsc{StanD}}$ from [Section 3.1](#S3.SS1
    "3.1 Standard Distillation ‣ 3 Method ‣ Personalised Distillation: Empowering
    Open-Sourced LLMs with Adaptive Learning for Code Generation") and first lets
    the student generate solutions for each task. Then it filters out the tasks where
    the student model can already solve correctly. For the remaining tasks, it obtains
    the teacher’s personalised refinement conditioned on the student’s attempt and
    its execution error feedback, and only keeps the tasks where the teacher’s refinement
    is valid (i.e., passes all the unit test cases). Figure [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Personalised Distillation: Empowering Open-Sourced LLMs with
    Adaptive Learning for Code Generation") visualizes these three steps.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PERsD-combined 算法 [1](#alg1 "算法 1 ‣ 3.1 标准蒸馏 ‣ 3 方法 ‣ 个性化蒸馏：通过自适应学习赋能开源 LLMs
    进行代码生成") 显示了 PERsD-combined 的详细步骤。这种方法从 [第 3.1 节](#S3.SS1 "3.1 标准蒸馏 ‣ 3 方法 ‣ 个性化蒸馏：通过自适应学习赋能开源
    LLMs 进行代码生成") 获取标准蒸馏数据集 $\mathcal{D}_{\textsc{StanD}}$，并首先让学生为每个任务生成解决方案。然后过滤出学生模型已经能够正确解决的任务。对于剩余的任务，它获得教师的个性化精炼，基于学生的尝试及其执行错误反馈，并仅保留教师精炼有效的任务（即通过所有单元测试用例）。图
    [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 个性化蒸馏：通过自适应学习赋能开源 LLMs 进行代码生成") 形象展示了这三个步骤。
- en: 'For this final task-set, we create two datasets: i) $\mathcal{D}_{\text{code}}$.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个最终任务集，我们创建了两个数据集：i) $\mathcal{D}_{\text{code}}$。
- en: 'PERsD-refine Similar to PERsD-combined, this variant follows line 1-15 of Algorithm
    [1](#alg1 "Algorithm 1 ‣ 3.1 Standard Distillation ‣ 3 Method ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation") to collect
    refinement data $\mathcal{D}_{\text{refine}}$ to finetune the student model.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: PERsD-refine 类似于 PERsD-combined，这个变体遵循算法 [1](#alg1 "算法 1 ‣ 3.1 标准蒸馏 ‣ 3 方法 ‣
    个性化蒸馏：通过自适应学习赋能开源 LLMs 进行代码生成") 的第 1-15 行来收集精炼数据 $\mathcal{D}_{\text{refine}}$，以微调学生模型。
- en: PERsD This variant takes the training data $\mathcal{D}_{\text{refine}}$ from
    PERsD-refine and replace the input of each data point from code refinement prompt
    to original task instruction. It thus trains the student model with personalised
    labels on code generation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: PERsD 这个变体从 PERsD-refine 中获取训练数据 $\mathcal{D}_{\text{refine}}$，并将每个数据点的输入从代码精炼提示替换为原始任务指令。它因此使用个性化标签对学生模型进行训练，进行代码生成。
- en: 'To illustrate the difference between personalised refinement and teacher’s
    direct solution, we show a real example in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Personalised
    Distillation ‣ 3 Method ‣ Personalised Distillation: Empowering Open-Sourced LLMs
    with Adaptive Learning for Code Generation"). The top shows the personalised refinement
    for the given task, while the bottom section shows the direct teacher’s generation
    for the same task. Note how the teacher’s direct generation is significantly different
    from the student model’s attempt, while the teacher’s refinement follows the student’s
    attempt and improves upon it. We hypothesize that such adaptive refinement where
    the teacher aligns to student’s generation, helps the student to learn more efficiently
    and effectively, similar to how humans benefit from personalised learning.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明个性化精炼与教师直接解决方案之间的区别，我们在图 [2](#S3.F2 "图 2 ‣ 3.2 个性化蒸馏 ‣ 3 方法 ‣ 个性化蒸馏：通过自适应学习赋能开源
    LLMs 进行代码生成") 中展示了一个实际例子。顶部显示了给定任务的个性化精炼，而底部则展示了教师针对相同任务生成的直接解决方案。请注意，教师的直接生成与学生模型的尝试有显著不同，而教师的精炼则跟随学生的尝试并加以改进。我们假设这种适应性精炼方式，教师对齐到学生生成的内容，有助于学生更高效、更有效地学习，类似于人类从个性化学习中受益。
- en: '![Refer to caption](img/902ea1e55153067ece0c879843a27bd9.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/902ea1e55153067ece0c879843a27bd9.png)'
- en: 'Figure 2: Example: (Top) Personalised refinement from student’s attempt and
    execution feedback; (Bottom) Direct solution generated by teacher conditioned
    on task.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：示例：（顶部）来自学生尝试和执行反馈的个性化精炼；（底部）教师根据任务生成的直接解决方案。
- en: 3.3 Iterative Inference
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 迭代推断
- en: Let $\mathcal{D}_{\text{test}}=\{(t,u)\}$ passed.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让 $\mathcal{D}_{\text{test}}=\{(t,u)\}$ 通过。
- en: Multi-step inference If the model $\pi_{\theta}$. We then compute pass@k over
    the 2-step generations similar to 1-step inference.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 多步骤推理 如果模型 $\pi_{\theta}$。然后，我们计算 2 步生成的 pass@k，类似于 1 步推理。
- en: 4 Experimental Setup
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验设置
- en: 4.1 Baselines
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基线
- en: 'The first baseline is StanD, the standard distillation approach mentioned in
    [Section 3.1](#S3.SS1 "3.1 Standard Distillation ‣ 3 Method ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation").'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '第一个基线是 StanD，标准蒸馏方法，详见[第 3.1 节](#S3.SS1 "3.1 Standard Distillation ‣ 3 Method
    ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning
    for Code Generation")。'
- en: 'To measure the effectiveness of personalised labels quantitatively, we also
    compare with Input-personalised distillation baselines as well, where only the
    input tasks are selected in a manner customized to the student’s abilities. However,
    the output labels are not personalised, as they are taken from teacher’s direction
    generation $c$ from PERsD-combined and have three variants:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定量测量个性化标签的有效性，我们还与 Input-personalised 蒸馏基线进行了比较，在这些基线中，只有输入任务是根据学生的能力进行定制的。然而，输出标签并没有个性化，因为它们来自老师的方向生成
    $c$，并有三种变体。
- en: InpD We finetune the student model $\pi_{\theta}$, where the input is a task
    instruction and the output is a code solution. This variant is more customized
    than StanD as it filters out the tasks which the student can already solve correctly.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: InpD 我们对学生模型 $\pi_{\theta}$ 进行微调，其中输入是任务说明，输出是代码解决方案。这个变体比 StanD 更具定制化，因为它过滤掉了学生已经可以正确解决的任务。
- en: InpD-refine Similar to PERsD-refine, InpD-refine trains the student model to
    rectify its wrong attempt. The difference is in InpD-refine, the refined code
    is from teacher’s direct solution $c$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: InpD-refine 类似于 PERsD-refine，InpD-refine 训练学生模型纠正其错误尝试。不同之处在于，在 InpD-refine
    中，优化后的代码来自老师的直接解决方案 $c$。
- en: InpD-combined Similar to PERsD-combined, InpD-combined trains the student on
    rectifying its answers as well as directly solving the task. The difference is
    that in InpD-combined, the labels for both code refinement and code generation
    are taken from teacher’s direct solution $c$.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: InpD-combined 类似于 PERsD-combined，InpD-combined 训练学生纠正其答案以及直接解决任务。不同之处在于，在 InpD-combined
    中，代码优化和代码生成的标签均来自老师的直接解决方案 $c$。
- en: 4.2 Pretraining Data Construction
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 预训练数据构建
- en: To construct our pretraining data, we adopted the data collection process in
    code-alpacaChaudhary ([2023](#bib.bib2)) and used a set of 374 seed tasks from
    MBPP (task-ids 601-974) as in-context prompt to query ChatGPT for novel code generation
    tasks. This seed-set increases the likelihood of ChatGPT generating python codes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的预训练数据，我们采用了 code-alpacaChaudhary ([2023](#bib.bib2)) 中的数据收集过程，并使用了一组来自
    MBPP 的 374 个种子任务（任务 ID 601-974）作为上下文提示来查询 ChatGPT 生成新的代码生成任务。这个种子集增加了 ChatGPT
    生成 Python 代码的可能性。
- en: Through this process, we obtained a corpus of 20K code generation tasks from
    ChatGPT each comprising a task instruction and the corresponding generated code,
    which is typically a single python function. Next we show each generated instance
    to ChatGPT again and prompt it to generate 5 unique test-case inputs (i.e. input
    argument values) for the python function. We then parse and format the generated
    test-case input and execute the generated code on it obtain an output. Thus, out
    of 20K, for 14880 instances we could successfully generate and parse 5 unit test
    case inputs and for 10172 instances we were able to successfully execute the generated
    code and obtain outputs on all 5 inputs. This final corpus of 10K code generation
    tasks, each comprising a task instruction and the corresponding generated code
    along with 5 unit test input and outputs forms our standard distillation dataset
    $\mathcal{D}_{\textsc{StanD}}$.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个过程，我们从 ChatGPT 获得了一个包含 20K 代码生成任务的语料库，每个任务包括一个任务说明和相应生成的代码，这通常是一个单一的 Python
    函数。接下来，我们将每个生成的实例再次展示给 ChatGPT，并提示其为 Python 函数生成 5 个独特的测试用例输入（即输入参数值）。然后，我们解析并格式化生成的测试用例输入，并在其上执行生成的代码以获得输出。因此，在
    20K 个实例中，对于 14880 个实例，我们成功生成并解析了 5 个单元测试用例输入，对于 10172 个实例，我们能够成功执行生成的代码，并在所有 5
    个输入上获得输出。这个最终的 10K 代码生成任务语料库，每个任务包括一个任务说明和相应生成的代码以及 5 个单元测试输入和输出，形成了我们的标准蒸馏数据集
    $\mathcal{D}_{\textsc{StanD}}$。
- en: 'To collect personalised distillation data, we follow [Section 3.2](#S3.SS2
    "3.2 Personalised Distillation ‣ 3 Method ‣ Personalised Distillation: Empowering
    Open-Sourced LLMs with Adaptive Learning for Code Generation") to first ask the
    student model to generate 1 output code per task, setting sampling temperature
    to 0.3\. We then evaluate the student’s attempt and only keep the tasks with the
    wrong generations (i.e. the ones which failed any of the unit test-case). We use
    this to query ChatGPT for personalised refinements and only retain the valid refinements
    which passed all unit tests. Our prompt to ChatGPT contains the original task
    instruction and code from $\mathcal{D}_{\textsc{StanD}}$ along with the student
    model’s generated code and execution feedback (compiler errors or unit test failures).
    Our instruction to ChatGPT is to generate a correct solution that rectifies the
    errors and is closest in semantics to the student’s code (More details in [Appendix B](#A2
    "Appendix B ChatGPT Prompt Template for Personalised Distillation ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation")).
    Table [3](#S4.T3 "Table 3 ‣ 4.2 Pretraining Data Construction ‣ 4 Experimental
    Setup ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive
    Learning for Code Generation") shows the statistics of personalised data construction
    process.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了收集个性化蒸馏数据，我们按照 [第 3.2 节](#S3.SS2 "3.2 个性化蒸馏 ‣ 3 方法 ‣ 个性化蒸馏：通过自适应学习增强开源 LLM
    的代码生成能力")，首先要求学生模型每个任务生成 1 个输出代码，设置采样温度为 0.3。然后，我们评估学生的尝试，只保留那些生成错误的任务（即未通过任何单元测试的任务）。我们使用这些任务查询
    ChatGPT 进行个性化的修正，并只保留所有单元测试通过的有效修正。我们给 ChatGPT 的提示包含原始任务指令和来自 $\mathcal{D}_{\textsc{StanD}}$
    的代码以及学生模型生成的代码和执行反馈（编译错误或单元测试失败）。我们对 ChatGPT 的指示是生成一个正确的解决方案，纠正错误，并在语义上最接近学生的代码（更多细节见
    [附录 B](#A2 "附录 B ChatGPT 提示模板用于个性化蒸馏 ‣ 个性化蒸馏：通过自适应学习增强开源 LLM 的代码生成能力")）。表 [3](#S4.T3
    "表 3 ‣ 4.2 预训练数据构建 ‣ 4 实验设置 ‣ 个性化蒸馏：通过自适应学习增强开源 LLM 的代码生成能力") 显示了个性化数据构建过程的统计信息。
- en: '|   Student Model | # Wrong Attempt | # Validated Per- | Data |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|   学生模型 | 错误尝试数量 | 验证过的每个 | 数据 |'
- en: '|  | by Student | sonalised Tasks | Cost |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | 学生 | 个性化任务 | 成本 |'
- en: '| CodeGen-mono-6B Nijkamp et al. ([2023](#bib.bib13)) | 6.5K | 3.25K | 5.5$
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono-6B Nijkamp 等 ([2023](#bib.bib13)) | 6.5K | 3.25K | 5.5$ |'
- en: '| CodeGen-mono-6B (round2) | 4K | 1.4K | 4.4$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono-6B (round2) | 4K | 1.4K | 4.4$ |'
- en: '| CodeGen-mono-16B | 6.2K | 2.8K | 6.5$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono-16B | 6.2K | 2.8K | 6.5$ |'
- en: '| StarCoder Li et al. ([2023a](#bib.bib8)) | 4.3K | 2.5K | 4.3$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| StarCoder Li 等 ([2023a](#bib.bib8)) | 4.3K | 2.5K | 4.3$ |'
- en: '|   |  |  |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |'
- en: 'Table 3: Statistics of Personalised Data Construction'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：个性化数据构建统计
- en: 4.3 Model Evaluation
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 模型评估
- en: 'We evaluate our models on two datasets: HumanEvalChen et al. ([2021](#bib.bib4)),
    which contains 164 Python problems, and the subset MBPPAustin et al. ([2021](#bib.bib1))
    sanitized set that has no overlap with our MBPP seed tasks for pretraining data
    collection. This corresponds to test+validation+prompt splits of MBPP-sanitized
    and consists of 306 Python problems. We use nucleus sampling with temperature
    0.2 to generate 20 candidates per task for estimating pass@1, and with temperature
    0.8, 100 candidates per task for estimating pass@5/10/20/50/100.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个数据集上评估我们的模型：HumanEvalChen 等 ([2021](#bib.bib4))，该数据集包含 164 个 Python 问题，以及
    MBPPAustin 等 ([2021](#bib.bib1)) 清理后的子集，与我们的 MBPP 种子任务没有重叠，用于预训练数据收集。这对应于 MBPP-清理数据的测试+验证+提示分割，共有
    306 个 Python 问题。我们使用温度为 0.2 的核采样生成每个任务的 20 个候选项，以估计 pass@1，并使用温度为 0.8 的核采样生成每个任务的
    100 个候选项，以估计 pass@5/10/20/50/100。
- en: 'For multi-step inference, we first extract the “seen” unit test-cases from
    the doc-string of the task instruction (More details in [Appendix A](#A1 "Appendix
    A Details in Multi-step Model Evaluation ‣ Personalised Distillation: Empowering
    Open-Sourced LLMs with Adaptive Learning for Code Generation")). Next, we generate
    output samples in the usual code-generation style forming the set of 1-step generations
    for each instance. Each of these candidate generations are then executed on the
    extracted “seen” unit test cases to obtain a refined code, thus forming the set
    of 2-step generations.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多步骤推理，我们首先从任务指令的文档字符串中提取“已见”的单元测试用例（更多细节见 [附录 A](#A1 "附录 A 多步骤模型评估的细节 ‣ 个性化蒸馏：通过自适应学习增强开源
    LLM 的代码生成能力")）。接下来，我们生成以往代码生成风格的输出样本，形成每个实例的 1 步生成集。然后，将这些候选生成的代码在提取的“已见”单元测试用例上执行，以获得精炼的代码，从而形成
    2 步生成集。
- en: 4.4 Pretraining Setup
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 预训练设置
- en: For all experiments with CodeGen-mono-6B backbone, we use effective batch size
    of 1024 and pretrain for 20 epochs. For backbone as CodeGen-mono-16B, we use effective
    batch size of 1024 and pretrain for 3 epochs, as the training converges much faster
    than CodeGen-mono-6B. For PERsD-combine with StarCoder model, we use effective
    batch size of 1024 and pretrain for 8 epochs, which results in similar training
    loss as CodeGen-mono-16B. We implement using HuggingFace transformersWolf et al.
    ([2020](#bib.bib23)) and DeepSpeed Zero Rajbhandari et al. ([2020](#bib.bib16)).
    All experiments are conducted on a cluster of 8 A100-40GB GPUs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有使用 CodeGen-mono-6B 主干的实验，我们使用有效批量大小 1024 并训练 20 个周期。对于 CodeGen-mono-16B
    主干，我们使用有效批量大小 1024 并训练 3 个周期，因为训练收敛速度远快于 CodeGen-mono-6B。对于与 StarCoder 模型配合的 PERsD-combine，我们使用有效批量大小
    1024 并训练 8 个周期，结果与 CodeGen-mono-16B 的训练损失相似。我们使用 HuggingFace transformersWolf
    等（[2020](#bib.bib23)）和 DeepSpeed Zero Rajbhandari 等（[2020](#bib.bib16)）进行实现。所有实验均在
    8 张 A100-40GB GPU 的集群上进行。
- en: 5 Experimental Results
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验结果
- en: 5.1 Main Results
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 主要结果
- en: 'We empirically test the hypothesis that personalised distillation helps student
    model learn more effectively, by comparing PERsD models with baseline distillation
    methods (InpD, StanD) in Table [4](#S5.T4 "Table 4 ‣ Multi-step inference consistently
    improves answer quality ‣ 5.1 Main Results ‣ 5 Experimental Results ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation").'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过比较 PERsD 模型与基线蒸馏方法（InpD，StanD）（见表格[4](#S5.T4 "表 4 ‣ 多步骤推理始终改善答案质量 ‣ 5.1
    主要结果 ‣ 5 实验结果 ‣ 个性化蒸馏：通过适应性学习提升开源 LLM 的代码生成能力")）来实证检验个性化蒸馏是否有助于学生模型更有效地学习。
- en: Personalised labeled-data is generally better than standard data
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 个性化标签数据通常优于标准数据
- en: Comparing PERsD-combine to InpD-combine, we find PERsD-combine outperforms InpD-combine
    in all settings, often with a significant margin (two backbones, two datasets,
    two inference steps, 4 pass@k metric). Similar observation holds true when comparing
    PERsD-refine to InpD-refine (except for 2/32 settings), and PERsD to InpD. Thus,
    we conclude that PERsD-variants are generally significantly better than their
    InpD counterparts, providing strong evidence that personalised labels are more
    effective for the student model to learn than standard labels.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 比较 PERsD-combine 和 InpD-combine，我们发现 PERsD-combine 在所有设置下都优于 InpD-combine，通常具有显著的优势（两个主干，两个数据集，两个推理步骤，4
    pass@k 评分）。类似的观察也适用于 PERsD-refine 与 InpD-refine 的比较（除了 2/32 的设置外），以及 PERsD 与 InpD
    的比较。因此，我们得出结论，PERsD 变体通常比其 InpD 对应物显著更好，这提供了强有力的证据表明个性化标签对学生模型的学习效果优于标准标签。
- en: PERsD outperforms StanD with less than one-third of its data
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PERsD 在数据量不到三分之一的情况下优于 StanD
- en: 'We observe that PERsD outperforms StanD for every pass@k on both 16B and 6B
    CodeGen-mono backbone across both HumanEval and MBPP, even though StanD has 10K
    data and PERsD has only 3.3K and 2.8K examples for CodeGen-mono-6B and 16B. The
    only exception is in the setting CodeGen-mono-16B, MBPP, pass@1, where StanD edges
    out PERsD by 1.2 points. Given that our pretraining data is constructed from seed
    tasks taken from MBPP, we hypothesize that StanD might enjoy an unfair advantage
    due to its having three times more data, making it more susceptible to data leakage.
    We verify such hypothesis further in [Section 5.2](#S5.SS2 "5.2 Train-Test overlap
    analysis ‣ 5 Experimental Results ‣ Personalised Distillation: Empowering Open-Sourced
    LLMs with Adaptive Learning for Code Generation"). In summary, with PERsD outperforming
    StanD in 15 out of 16 settings while having less than a third of the data, it’s
    evident that personalized labeled data makes the learning more efficient.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，PERsD 在 16B 和 6B CodeGen-mono 主干上对 HumanEval 和 MBPP 的每个 pass@k 都优于 StanD，即使
    StanD 有 10K 数据，而 PERsD 仅有 3.3K 和 2.8K 示例用于 CodeGen-mono-6B 和 16B。唯一的例外是在设置 CodeGen-mono-16B，MBPP，pass@1，其中
    StanD 比 PERsD 高出 1.2 分。考虑到我们的预训练数据是从 MBPP 中获取的种子任务构建的，我们推测 StanD 可能由于数据量是 PERsD
    的三倍而享有不公平的优势，使其更容易受到数据泄漏的影响。我们在[第 5.2 节](#S5.SS2 "5.2 训练-测试重叠分析 ‣ 5 实验结果 ‣ 个性化蒸馏：通过适应性学习提升开源
    LLM 的代码生成能力")中进一步验证了这一假设。总之，PERsD 在 16 个设置中有 15 个超越 StanD，同时数据量不到其三分之一，显然个性化标签数据使学习更为高效。
- en: Multi-step inference consistently improves answer quality
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多步骤推理始终改善答案质量
- en: For PERsD-refine and PERsD-combine models, we find that 2 step inference consistently
    improves performance on HumanEval and MBPP. This shows the models successfully
    learn how to rectify its solution based on execution error feedback. Note that
    InpD-refine yields worse accuracy with 2 step inference on HumanEval pass@10/20,
    strengthening the advantage of personalised labeled data over standard labeled
    data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PERsD-refine和PERsD-combine模型，我们发现2步推理在HumanEval和MBPP上的性能始终有所提升。这表明模型成功学习了如何根据执行错误反馈来修正其解决方案。需要注意的是，InpD-refine在HumanEval
    pass@10/20上的准确性较低，进一步增强了个性化标注数据相对于标准标注数据的优势。
- en: (a) Backbone as CodeGen-mono-6B
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Backbone as CodeGen-mono-6B
- en: '|   Methods | #Data | Pass@1 | Pass@5 | Pass@10 | Pass@20 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|   Methods | #Data | Pass@1 | Pass@5 | Pass@10 | Pass@20 |'
- en: '| \cdashline3-10 | step=1 | step=2 | step=1 | step=2 | step=1 | step=2 | step=1
    | step=2 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline3-10 | step=1 | step=2 | step=1 | step=2 | step=1 | step=2 | step=1
    | step=2 |'
- en: '|  | HumanEval |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | HumanEval |  |'
- en: '| StanD | 10K | 32.41 | - | 41.79 | - | 45.67 | - | 49.26 | - |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| StanD | 10K | 32.41 | - | 41.79 | - | 45.67 | - | 49.26 | - |'
- en: '| \hdashlineInpD | 3.3K | 31.65 | - | 44.55 | - | 50.72 | - | 56.76 | - |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineInpD | 3.3K | 31.65 | - | 44.55 | - | 50.72 | - | 56.76 | - |'
- en: '| -refine | 3.3K | 29.70 | 29.70 | 43.82 | 41.99 | 51.28 | 47.89 | 58.29 |
    53.51 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| -refine | 3.3K | 29.70 | 29.70 | 43.82 | 41.99 | 51.28 | 47.89 | 58.29 |
    53.51 |'
- en: '| -combined | 6.5K | 30.15 | 32.30 | 42.94 | 45.27 | 47.91 | 50.50 | 52.54
    | 55.46 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| -combined | 6.5K | 30.15 | 32.30 | 42.94 | 45.27 | 47.91 | 50.50 | 52.54
    | 55.46 |'
- en: '| \hdashlinePERsD | 3.3K | 34.63 | - | 49.34 | - | 55.34 | - | 60.41 | - |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlinePERsD | 3.3K | 34.63 | - | 49.34 | - | 55.34 | - | 60.41 | - |'
- en: '| -refine | 3.3K | 32.35 | 33.35 | 48.69 | 49.35 | 56.07 | 56.87 | 63.60 |
    64.76 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| -refine | 3.3K | 32.35 | 33.35 | 48.69 | 49.35 | 56.07 | 56.87 | 63.60 |
    64.76 |'
- en: '| -combined | 6.5K | 33.81 | 35.53 | 44.64 | 49.67 | 49.96 | 55.67 | 55.23
    | 61.21 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| -combined | 6.5K | 33.81 | 35.53 | 44.64 | 49.67 | 49.96 | 55.67 | 55.23
    | 61.21 |'
- en: '|  | MBPP |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | MBPP |  |'
- en: '| StanD | 10K | 43.11 | - | 55.24 | - | 59.07 | - | 62.51 | - |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| StanD | 10K | 43.11 | - | 55.24 | - | 59.07 | - | 62.51 | - |'
- en: '| \hdashlineInpD | 3.3K | 43.59 | - | 55.83 | - | 63.13 | - | 67.34 | - |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineInpD | 3.3K | 43.59 | - | 55.83 | - | 63.13 | - | 67.34 | - |'
- en: '| -refine | 3.3K | 44.44 | 47.81 | 62.25 | 66.43 | 67.61 | 71.44 | 71.68 |
    75.22 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| -refine | 3.3K | 44.44 | 47.81 | 62.25 | 66.43 | 67.61 | 71.44 | 71.68 |
    75.22 |'
- en: '| -combined | 6.5K | 42.69 | 47.25 | 56.70 | 62.17 | 61.39 | 66.49 | 65.46
    | 70.22 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| -combined | 6.5K | 42.69 | 47.25 | 56.70 | 62.17 | 61.39 | 66.49 | 65.46
    | 70.22 |'
- en: '| \hdashlinePERsD | 3.3K | 45.47 | - | 59.90 | - | 64.85 | - | 69.73 | - |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlinePERsD | 3.3K | 45.47 | - | 59.90 | - | 64.85 | - | 69.73 | - |'
- en: '| -refine | 3.3K | 48.24 | 52.65 | 63.65 | 68.49 | 69.00 | 73.34 | 73.16 |
    77.62 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| -refine | 3.3K | 48.24 | 52.65 | 63.65 | 68.49 | 69.00 | 73.34 | 73.16 |
    77.62 |'
- en: '| -combined | 6.5K | 42.77 | 48.92 | 56.91 | 62.29 | 61.43 | 66.89 | 65.22
    | 70.96 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| -combined | 6.5K | 42.77 | 48.92 | 56.91 | 62.29 | 61.43 | 66.89 | 65.22
    | 70.96 |'
- en: '|   |  |  |  |  |  |  |  |  |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |'
- en: (b) Backbone as CodeGen-mono-16B
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Backbone as CodeGen-mono-16B
- en: '|   Methods | #Data | Pass@1 | Pass@5 | Pass@10 | Pass@20 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|   Methods | #Data | Pass@1 | Pass@5 | Pass@10 | Pass@20 |'
- en: '| \cdashline3-10 | step=1 | step=2 | step=1 | step=2 | step=1 | step=2 | step=1
    | step=2 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline3-10 | step=1 | step=2 | step=1 | step=2 | step=1 | step=2 | step=1
    | step=2 |'
- en: '|  | HumanEval |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | HumanEval |  |'
- en: '| StanD | 10K | 33.96 | - | 50.56 | - | 57.69 | - | 63.82 | - |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| StanD | 10K | 33.96 | - | 50.56 | - | 57.69 | - | 63.82 | - |'
- en: '| \hdashlineInpD | 2.8K | 36.68 | - | 49.51 | - | 53.85 | - | 57.47 | - |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineInpD | 2.8K | 36.68 | - | 49.51 | - | 53.85 | - | 57.47 | - |'
- en: '| -refine | 2.8K | 30.55 | 31.28 | 48.40 | 48.13 | 55.00 | 54.52 | 61.31 |
    60.62 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| -refine | 2.8K | 30.55 | 31.28 | 48.40 | 48.13 | 55.00 | 54.52 | 61.31 |
    60.62 |'
- en: '| -combined | 5.6K | 34.66 | 36.49 | 50.65 | 53.89 | 56.75 | 60.07 | 62.78
    | 65.85 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| -combined | 5.6K | 34.66 | 36.49 | 50.65 | 53.89 | 56.75 | 60.07 | 62.78
    | 65.85 |'
- en: '| \hdashlinePERsD | 2.8K | 37.74 | - | 56.57 | - | 63.92 | - | 69.97 | - |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlinePERsD | 2.8K | 37.74 | - | 56.57 | - | 63.92 | - | 69.97 | - |'
- en: '| -refine | 2.8K | 36.77 | 37.99 | 51.86 | 54.23 | 58.07 | 60.92 | 63.17 |
    67.13 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| -refine | 2.8K | 36.77 | 37.99 | 51.86 | 54.23 | 58.07 | 60.92 | 63.17 |
    67.13 |'
- en: '| -combined | 5.6K | 36.40 | 37.74 | 53.57 | 55.80 | 60.81 | 63.37 | 67.3 |
    70.50 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| -combined | 5.6K | 36.40 | 37.74 | 53.57 | 55.80 | 60.81 | 63.37 | 67.3 |
    70.50 |'
- en: '|  | MBPP |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | MBPP |  |'
- en: '| StanD | 10K | 48.90 | - | 62.21 | - | 66.91 | - | 71.33 | - |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| StanD | 10K | 48.90 | - | 62.21 | - | 66.91 | - | 71.33 | - |'
- en: '| \hdashlineInpD | 2.8K | 46.27 | - | 58.45 | - | 62.61 | - | 66.43 | - |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineInpD | 2.8K | 46.27 | - | 58.45 | - | 62.61 | - | 66.43 | - |'
- en: '| -refine | 2.8K | 48.79 | 54.87 | 66.89 | 71.32 | 72.24 | 75.71 | 75.82 |
    78.84 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| -refine | 2.8K | 48.79 | 54.87 | 66.89 | 71.32 | 72.24 | 75.71 | 75.82 |
    78.84 |'
- en: '| -combined | 5.6K | 47.39 | 53.59 | 59.14 | 66.38 | 63.48 | 70.76 | 67.10
    | 74.35 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| -combined | 5.6K | 47.39 | 53.59 | 59.14 | 66.38 | 63.48 | 70.76 | 67.10
    | 74.35 |'
- en: '| \hdashlinePERsD | 2.8K | 47.68 | - | 65.80 | - | 71.56 | - | 76.02 | - |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlinePERsD | 2.8K | 47.68 | - | 65.80 | - | 71.56 | - | 76.02 | - |'
- en: '| -refine | 2.8K | 51.50 | 56.21 | 66.82 | 71.86 | 72.06 | 76.78 | 76.03 |
    80.42 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| -refine | 2.8K | 51.50 | 56.21 | 66.82 | 71.86 | 72.06 | 76.78 | 76.03 |
    80.42 |'
- en: '| -combined | 5.6K | 51.44 | 56.44 | 66.45 | 71.31 | 71.64 | 76.43 | 76.04
    | 80.20 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| -combined | 5.6K | 51.44 | 56.44 | 66.45 | 71.31 | 71.64 | 76.43 | 76.04
    | 80.20 |'
- en: '|   |  |  |  |  |  |  |  |  |  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |'
- en: 'Table 4: Comparing PERsD models to StanD & InpD'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 4: 比较 PERsD 模型与 StanD 和 InpD'
- en: 5.2 Train-Test overlap analysis
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 训练-测试重叠分析
- en: 'As observed in Table [4](#S5.T4 "Table 4 ‣ Multi-step inference consistently
    improves answer quality ‣ 5.1 Main Results ‣ 5 Experimental Results ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation"),
    PersD-variants enjoy higher average improvements over their InpD counterparts,
    on HumanEvan than on MBPP. To delve deeper, we conduct a data overlap analysis.
    For each test task, we extract the most similar training task and use GPT-3.5-turbo
    to score their semantic similarity, with 0 indicating no relation and 1 indicating
    complete semantic overlap (further details in [Appendix D](#A4 "Appendix D Details
    in Data Overlap Analysis ‣ Personalised Distillation: Empowering Open-Sourced
    LLMs with Adaptive Learning for Code Generation")). Table [5](#S5.T5 "Table 5
    ‣ 5.2 Train-Test overlap analysis ‣ 5 Experimental Results ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation") reveals
    more overlap in MBPP than HumanEval, and more overlap for StanD compared to PERsD.
    This overlap could be why StanD surpasses PERsD in the 1/16 setting (CodeGen-mono-16B,
    MBPP, pass@1), as StanD has an unfair advantage of having significantly more data
    leakage. In addition, if we test our methods on clean-MBPP where the leaked data
    points are removed, then PERsD becomes almost on-par with StanD in this specific
    setting while having larger margin over StanD on the rest 15/16 settings (from
    4.8 points average margin to 5.9 points, more details at [Appendix E](#A5 "Appendix
    E Results in MBPP-Cleaned ‣ Personalised Distillation: Empowering Open-Sourced
    LLMs with Adaptive Learning for Code Generation")). Altogether, this overlap analysis,
    coupled with results from cleaned MBPP, further underscores the advantages of
    personalized distillation.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 正如表格[4](#S5.T4 "表格 4 ‣ 多步骤推理一致提升回答质量 ‣ 5.1 主要结果 ‣ 5 实验结果 ‣ 个性化蒸馏：通过自适应学习提升开源
    LLMs 的代码生成能力")中所观察到的，PersD-变体在 HumanEvan 上的平均改进高于它们的 InpD 对应物，而在 MBPP 上则较低。为了深入探讨，我们进行了一项数据重叠分析。对于每个测试任务，我们提取最相似的训练任务，并使用
    GPT-3.5-turbo 评估它们的语义相似度，0 表示没有关系，1 表示完全语义重叠（更多细节见 [附录 D](#A4 "附录 D 数据重叠分析细节 ‣
    个性化蒸馏：通过自适应学习提升开源 LLMs 的代码生成能力")）。表格[5](#S5.T5 "表格 5 ‣ 5.2 训练-测试重叠分析 ‣ 5 实验结果
    ‣ 个性化蒸馏：通过自适应学习提升开源 LLMs 的代码生成能力")显示 MBPP 中的重叠程度高于 HumanEval，并且 StanD 相较于 PERsD
    的重叠更多。这种重叠可能是为什么 StanD 在 1/16 设置下（CodeGen-mono-16B，MBPP，pass@1）超越 PERsD 的原因，因为
    StanD 由于数据泄漏显著更多而具有不公平的优势。此外，如果我们在清理后的 MBPP 上测试我们的方法，移除泄漏的数据点，那么 PERsD 在这个特定设置下几乎与
    StanD 持平，同时在其余的 15/16 设置中相较于 StanD 具有更大的优势（从 4.8 分的平均差距增加到 5.9 分，更多细节见 [附录 E](#A5
    "附录 E 清理后的 MBPP 结果 ‣ 个性化蒸馏：通过自适应学习提升开源 LLMs 的代码生成能力")）。总体而言，这项重叠分析，加上清理后的 MBPP
    结果，进一步突显了个性化蒸馏的优势。
- en: '|   Method | Backbone | %("leak") | Similarity |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|   方法 | 骨干网络 | %("leak") | 相似度 |'
- en: '|  |  | HumanEval |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  |  | HumanEval |'
- en: '| StanD | 6B,16B | 6.1% | 0.22 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| StanD | 6B,16B | 6.1% | 0.22 |'
- en: '| PERsD | 6B | 3.6% | 0.18 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| PERsD | 6B | 3.6% | 0.18 |'
- en: '| PERsD | 16B | 3.05% | 0.22 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| PERsD | 16B | 3.05% | 0.22 |'
- en: '|  |  | MBPP |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MBPP |'
- en: '| StanD | 6B,16B | 18.24% | 0.40 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| StanD | 6B,16B | 18.24% | 0.40 |'
- en: '| PERsD | 6B | 8.47% | 0.30 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| PERsD | 6B | 8.47% | 0.30 |'
- en: '| PERsD | 16B | 7.49% | 0.30 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| PERsD | 16B | 7.49% | 0.30 |'
- en: '|   |  |  |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |'
- en: 'Table 5: Train-Test Overlap Analysis. 6B/16B denotes CodeGen-mono-{6/16}B backbones.
    %("leak") denotes the percentage of test data that are semantically leaked in
    training data. ’Similarity’ represents the average similarity score (range: 0
    to 1; higher values indicate greater similarity)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 5: 训练-测试重叠分析。6B/16B 表示 CodeGen-mono-{6/16}B 骨干网络。%("leak") 表示测试数据在训练数据中语义泄漏的百分比。''Similarity''
    代表平均相似度分数（范围: 0 到 1；值越高表示相似度越大）'
- en: 5.3 Effect of mixing StanD and InpD data
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 StanD 和 InpD 数据混合的效果
- en: 'Table [6](#S5.T6 "Table 6 ‣ 5.3 Effect of mixing StanD and InpD data ‣ 5 Experimental
    Results ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive
    Learning for Code Generation") shows the ablation study on mixing standard distillation
    data to PERsD-refine and InpD-refine: while mixing standard data to InpD-refine
    improves its 1-step performance on MBPP and roughly maintains its performance
    on other settings, mixing StanD data to PERsD-refine significantly deteriorate
    its performance (except pass@1 inf-step=2 on HumanEval). We conjecture that as
    StanD has much larger data volume than PERsD-refine, it overwhelms the student
    training on standard distillation. However, combining with a balanced input-personalised
    data can be beneficial, as we observe from the good performance of PERsD-combined
    in Table [4](#S5.T4 "Table 4 ‣ Multi-step inference consistently improves answer
    quality ‣ 5.1 Main Results ‣ 5 Experimental Results ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation") on CodeGen-mono-16B.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6](#S5.T6 "表 6 ‣ 5.3 混合 StanD 和 InpD 数据的效果 ‣ 5 实验结果 ‣ 个性化蒸馏：赋能开源 LLM 的自适应学习以进行代码生成")
    显示了对将标准蒸馏数据混合到 PERsD-refine 和 InpD-refine 的消融研究：将标准数据混合到 InpD-refine 提高了其在 MBPP
    上的一步性能，并大致保持了其在其他设置上的性能，而将 StanD 数据混合到 PERsD-refine 显著恶化了其性能（除了 HumanEval 上的 pass@1
    inf-step=2）。我们推测，由于 StanD 的数据量远大于 PERsD-refine，它压倒了标准蒸馏中的学生训练。然而，与平衡的输入个性化数据相结合可能会有益，因为我们从表
    [4](#S5.T4 "表 4 ‣ 多步骤推断一致提高答案质量 ‣ 5.1 主要结果 ‣ 5 实验结果 ‣ 个性化蒸馏：赋能开源 LLM 的自适应学习以进行代码生成")
    中 PERsD-combined 在 CodeGen-mono-16B 上的良好表现中观察到了这一点。
- en: '|   Methods | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|   方法 | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
- en: '|  | Step | HumanEval |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | 步骤 | HumanEval |'
- en: '| StanD + InpD-refine | 1 | 30.59 | 40.04 | 44.20 | 54.23 | 58.54 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| StanD + InpD-refine | 1 | 30.59 | 40.04 | 44.20 | 54.23 | 58.54 |'
- en: '| StanD + InpD-refine* | 29.45 | 39.83 | 44.07 | 54.55 | 59.76 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| StanD + InpD-refine* | 29.45 | 39.83 | 44.07 | 54.55 | 59.76 |'
- en: '| \cdashline1-1\cdashline3-7 StanD + PERsD-refine | 32.13 | 43.82 | 48.66 |
    59.55 | 64.02 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-1\cdashline3-7 StanD + PERsD-refine | 32.13 | 43.82 | 48.66 |
    59.55 | 64.02 |'
- en: '| PERsD-refine | 32.35 | 48.69 | 56.07 | 72.10 | 77.44 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| PERsD-refine | 32.35 | 48.69 | 56.07 | 72.10 | 77.44 |'
- en: '| StanD + InpD-refine | 2 | 30.87 | 42.88 | 47.90 | 58.21 | 60.98 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| StanD + InpD-refine | 2 | 30.87 | 42.88 | 47.90 | 58.21 | 60.98 |'
- en: '| StanD + InpD-refine* | 30.12 | 42.71 | 47.42 | 58.69 | 64.02 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| StanD + InpD-refine* | 30.12 | 42.71 | 47.42 | 58.69 | 64.02 |'
- en: '| \cdashline1-1\cdashline3-7 StanD + PERsD-refine | 35.00 | 47.89 | 52.96 |
    64.36 | 69.51 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-1\cdashline3-7 StanD + PERsD-refine | 35.00 | 47.89 | 52.96 |
    64.36 | 69.51 |'
- en: '| PERsD-refine | 33.35 | 49.35 | 56.87 | 74.13 | 79.88 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| PERsD-refine | 33.35 | 49.35 | 56.87 | 74.13 | 79.88 |'
- en: '|  |  | MBPP |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MBPP |'
- en: '| StanD + InpD-refine | 1 | 42.60 | 53.18 | 56.49 | 62.11 | 63.07 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| StanD + InpD-refine | 1 | 42.60 | 53.18 | 56.49 | 62.11 | 63.07 |'
- en: '| StanD + InpD-refine* | 44.08 | 54.12 | 57.82 | 64.96 | 66.34 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| StanD + InpD-refine* | 44.08 | 54.12 | 57.82 | 64.96 | 66.34 |'
- en: '| \cdashline1-1\cdashline3-7 StanD + PERsD-refine | 45.63 | 53.20 | 56.38 |
    63.02 | 65.36 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-1\cdashline3-7 StanD + PERsD-refine | 45.63 | 53.20 | 56.38 |
    63.02 | 65.36 |'
- en: '| PERsD-refine | 48.24 | 63.65 | 69.00 | 78.16 | 81.70 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| PERsD-refine | 48.24 | 63.65 | 69.00 | 78.16 | 81.70 |'
- en: '| StanD + InpD-refine | 2 | 46.32 | 58.84 | 62.80 | 69.80 | 71.23 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| StanD + InpD-refine | 2 | 46.32 | 58.84 | 62.80 | 69.80 | 71.23 |'
- en: '| StanD + InpD-refine* | 46.92 | 58.18 | 62.03 | 68.82 | 68.95 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| StanD + InpD-refine* | 46.92 | 58.18 | 62.03 | 68.82 | 68.95 |'
- en: '| \cdashline1-1\cdashline3-7 StanD + PERsD-refine | 48.44 | 58.37 | 62.47 |
    70.64 | 73.20 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-1\cdashline3-7 StanD + PERsD-refine | 48.44 | 58.37 | 62.47 |
    70.64 | 73.20 |'
- en: '| PERsD-refine | 52.65 | 68.49 | 73.34 | 82.72 | 85.62 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| PERsD-refine | 52.65 | 68.49 | 73.34 | 82.72 | 85.62 |'
- en: '|   |  |  |  |  |  |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |'
- en: 'Table 6: Ablation on mixing StanD, with Backbone as CodeGen-mono 6B. InpD-refine*
    denotes using all 6.5K tasks where the student model made mistakes, which covers
    around 3K more tasks than InpD-refine.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 混合 StanD 的消融研究，基础模型为 CodeGen-mono 6B。InpD-refine* 表示使用所有 6.5K 任务，其中学生模型出现了错误，这覆盖了比
    InpD-refine 多约 3K 的任务。'
- en: 'Similarly, in Table [7](#S5.T7 "Table 7 ‣ 5.3 Effect of mixing StanD and InpD
    data ‣ 5 Experimental Results ‣ Personalised Distillation: Empowering Open-Sourced
    LLMs with Adaptive Learning for Code Generation") we show another ablation: that
    mixing InpD data with PERsD roughly maintains the performance on HumanEval but
    degrades on MBPP. This shows personalised labels are of higher quality and mixing
    non personalised labels for the same task generally hurts performance.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在表格 [7](#S5.T7 "表 7 ‣ 5.3 混合 StanD 和 InpD 数据的效果 ‣ 5 实验结果 ‣ 个性化蒸馏：赋能开源 LLM
    的自适应学习用于代码生成") 中，我们展示了另一个消融实验：将 InpD 数据与 PERsD 混合，大致保持了 HumanEval 上的表现，但在 MBPP
    上表现下降。这表明个性化标签的质量更高，而混合非个性化标签的效果通常会降低表现。
- en: '|   Methods | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|   方法 | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
- en: '|  | HumanEval |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | HumanEval |'
- en: '| PERsD | 34.63 | 49.34 | 55.34 | 65.56 | 67.93 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| PERsD | 34.63 | 49.34 | 55.34 | 65.56 | 67.93 |'
- en: '| PERsD + InpD | 34.88 | 48.35 | 54.06 | 64.88 | 68.90 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| PERsD + InpD | 34.88 | 48.35 | 54.06 | 64.88 | 68.90 |'
- en: '|  | MBPP |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | MBPP |'
- en: '| PERsD | 45.47 | 59.90 | 64.85 | 76.05 | 80.07 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| PERsD | 45.47 | 59.90 | 64.85 | 76.05 | 80.07 |'
- en: '| PERsD + InpD | 43.84 | 59.02 | 63.77 | 71.69 | 74.84 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| PERsD + InpD | 43.84 | 59.02 | 63.77 | 71.69 | 74.84 |'
- en: '|   |  |  |  |  |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |'
- en: 'Table 7: Ablation on PERsD mixing InpD with CodeGen-mono 6B as backbone'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：在 PERsD 上进行消融实验，将 InpD 与 CodeGen-mono 6B 作为基础进行混合
- en: 5.4 Multi-round Distillation
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 多轮蒸馏
- en: '|   Round | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|   轮次 | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
- en: '|  | Step | HumanEval |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | 步骤 | HumanEval |'
- en: '| 1 | 1 | 33.81 | 44.64 | 49.96 | 61.75 | 70.73 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 33.81 | 44.64 | 49.96 | 61.75 | 70.73 |'
- en: '| 2 | 32.74 | 45.50 | 51.52 | 66.14 | 71.95 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 32.74 | 45.50 | 51.52 | 66.14 | 71.95 |'
- en: '| \hdashline1 | 2 | 35.53 | 49.67 | 55.67 | 68.16 | 77.44 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline1 | 2 | 35.53 | 49.67 | 55.67 | 68.16 | 77.44 |'
- en: '| 2 | 36.75 | 49.71 | 56.13 | 70.24 | 75.00 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 36.75 | 49.71 | 56.13 | 70.24 | 75.00 |'
- en: '|  |  | MBPP |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MBPP |'
- en: '| 1 | 1 | 42.77 | 56.91 | 61.43 | 68.84 | 70.67 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 42.77 | 56.91 | 61.43 | 68.84 | 70.67 |'
- en: '| 2 | 45.07 | 57.75 | 62.27 | 70.49 | 72.55 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 45.07 | 57.75 | 62.27 | 70.49 | 72.55 |'
- en: '| \hdashline1 | 2 | 48.92 | 62.29 | 66.89 | 75.09 | 77.25 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline1 | 2 | 48.92 | 62.29 | 66.89 | 75.09 | 77.25 |'
- en: '| 2 | 49.59 | 63.43 | 68.30 | 76.00 | 78.10 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 49.59 | 63.43 | 68.30 | 76.00 | 78.10 |'
- en: '|   |  |  |  |  |  |  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |'
- en: 'Table 8: Ablation on multi-round distillation on PERsD-combined with CodeGen-mono
    6B as backbone'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：在 PERsD 结合与 CodeGen-mono 6B 作为基础的多轮蒸馏消融实验
- en: 'After finetuning the student model with the personalised distillation data,
    can we perform another round of personalised distillation, on the new model? We
    show such an ablation study in Table [8](#S5.T8 "Table 8 ‣ 5.4 Multi-round Distillation
    ‣ 5 Experimental Results ‣ Personalised Distillation: Empowering Open-Sourced
    LLMs with Adaptive Learning for Code Generation"). Encouragingly, we find PERsD-combined
    round-2 generally outperforms PERsD-combined round-1 by a modest margin. This
    improvement provides further evidence of the benefits of personalized learning,
    even when applied to models trained with personalized distillation. These findings
    suggest the intriguing possibility of an online or active version of personalized
    distillation, where data collection and model training occur simultaneously to
    ensure each batch is fully personalized and has higher sample efficiency. However,
    we will leave such intriguing exploration for future work.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在用个性化蒸馏数据微调学生模型后，我们能否在新模型上进行另一轮个性化蒸馏？我们在表格 [8](#S5.T8 "表 8 ‣ 5.4 多轮蒸馏 ‣ 5 实验结果
    ‣ 个性化蒸馏：赋能开源 LLM 的自适应学习用于代码生成") 中展示了这样的消融研究。令人鼓舞的是，我们发现 PERsD 结合的第二轮通常比 PERsD
    结合的第一轮表现略好。这一改进进一步证明了个性化学习的好处，即使在应用于经过个性化蒸馏训练的模型时也是如此。这些发现暗示了一个有趣的可能性，即个性化蒸馏的在线或主动版本，其中数据收集和模型训练同时进行，以确保每一批数据都完全个性化并具有更高的样本效率。然而，我们将把这种有趣的探索留待未来工作。
- en: 5.5 Utilizing feedback for multi-step Inference
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 利用反馈进行多步骤推理
- en: 'To better understand the role of execution feedback during training and multi-step
    inference, we show an ablation study in Table [9](#S5.T9 "Table 9 ‣ 5.5 Utilizing
    feedback for multi-step Inference ‣ 5 Experimental Results ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation"), where
    we compare PERsD-combine with a specific variant (PERsD-combine*) that excludes
    feedback during both training and inference. we observed that PERsD-combine* performs
    comparably to PERsD-combine on HumanEval and slightly better on MBPP for 1-step
    inference. However, for 2-step inference, PERsD-combine* consistently underperforms
    PERsD-combine. This result aligns well with our expectations that code-rectification
    needs the execution feedback to guide the refinement.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '为了更好地理解执行反馈在训练和多步骤推理中的作用，我们在表 [9](#S5.T9 "Table 9 ‣ 5.5 Utilizing feedback
    for multi-step Inference ‣ 5 Experimental Results ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation") 中展示了一项消融研究，比较了
    PERsD-combine 和一个特定变体 (PERsD-combine*)，该变体在训练和推理过程中都不包含反馈。我们观察到，PERsD-combine*
    在 HumanEval 上的表现与 PERsD-combine 相当，在 MBPP 上对于 1 步推理稍微好一些。然而，对于 2 步推理，PERsD-combine*
    一直表现不如 PERsD-combine。这一结果与我们的预期一致，即代码修正需要执行反馈来指导细化。'
- en: '|   Methods | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|   方法 | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
- en: '|  | Step | HumanEval |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | 步骤 | HumanEval |'
- en: '| PERsD-combine | 1 | 33.81 | 44.64 | 49.96 | 61.75 | 70.73 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| PERsD-combine | 1 | 33.81 | 44.64 | 49.96 | 61.75 | 70.73 |'
- en: '| PERsD-combine* | 33.29 | 45.47 | 50.90 | 62.87 | 68.29 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| PERsD-combine* | 33.29 | 45.47 | 50.90 | 62.87 | 68.29 |'
- en: '| PERsD-combine | 2 | 35.53 | 49.67 | 55.67 | 68.16 | 77.44 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| PERsD-combine | 2 | 35.53 | 49.67 | 55.67 | 68.16 | 77.44 |'
- en: '| PERsD-combine* | 34.59 | 49.54 | 55.59 | 67.27 | 71.95 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| PERsD-combine* | 34.59 | 49.54 | 55.59 | 67.27 | 71.95 |'
- en: '|  |  | MBPP |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MBPP |'
- en: '| PERsD-combine | 1 | 42.77 | 56.91 | 61.43 | 68.84 | 70.67 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| PERsD-combine | 1 | 42.77 | 56.91 | 61.43 | 68.84 | 70.67 |'
- en: '| PERsD-combine* | 44.76 | 56.95 | 60.85 | 68.67 | 71.57 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| PERsD-combine* | 44.76 | 56.95 | 60.85 | 68.67 | 71.57 |'
- en: '| PERsD-combine | 2 | 48.92 | 62.29 | 66.89 | 75.09 | 77.25 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| PERsD-combine | 2 | 48.92 | 62.29 | 66.89 | 75.09 | 77.25 |'
- en: '| PERsD-combine* | 47.83 | 61.28 | 65.54 | 73.03 | 75.49 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| PERsD-combine* | 47.83 | 61.28 | 65.54 | 73.03 | 75.49 |'
- en: '|   |  |  |  |  |  |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |'
- en: 'Table 9: Ablation on removing execution feedback with CodeGen-mono 6B as backbone.
    PERsD-combine* denotes combined personalised distillation without execution feedback
    in input prompt.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：去除执行反馈的消融实验，以 CodeGen-mono 6B 作为骨干。PERsD-combine* 表示在输入提示中没有执行反馈的组合个性化蒸馏。
- en: 5.6 Cross-Model Personalised Distillation
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 跨模型个性化蒸馏
- en: 'To investigate whether personalised distillation data of one model can be benefical
    to another, we conduct an ablation in Table [10](#S5.T10 "Table 10 ‣ 5.6 Cross-Model
    Personalised Distillation ‣ 5 Experimental Results ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation") by using
    PERsD-combined data of CodeGen-mono-6B to train CodeGen-mono-16B. The results
    show that such cross-model persionalised data do not perform as well as real personalised
    data: leading to a consistent performance drop by a large margin. This finding
    reinforces our notion that learning data should be tailored to the specific student
    model, as personalized data suitable for one model may not necessarily benefit
    others.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '为了调查一个模型的个性化蒸馏数据是否对另一个模型有利，我们在表 [10](#S5.T10 "Table 10 ‣ 5.6 Cross-Model Personalised
    Distillation ‣ 5 Experimental Results ‣ Personalised Distillation: Empowering
    Open-Sourced LLMs with Adaptive Learning for Code Generation") 中进行了一项消融实验，使用 CodeGen-mono-6B
    的 PERsD-combined 数据来训练 CodeGen-mono-16B。结果显示，这种跨模型的个性化数据表现不如真实的个性化数据，导致性能一致性大幅下降。这个发现加强了我们的观点，即学习数据应针对特定的学生模型量身定制，因为适用于一个模型的个性化数据可能不一定对其他模型有益。'
- en: '|   Model | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|   模型 | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100 |'
- en: '|  | Step | HumanEval |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | 步骤 | HumanEval |'
- en: '| CodeGen-mono-6B | 1 | 33.81 | 44.64 | 49.96 | 61.75 | 70.73 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono-6B | 1 | 33.81 | 44.64 | 49.96 | 61.75 | 70.73 |'
- en: '| CodeGen-mono-16B* | 32.99 | 47.81 | 54.58 | 69.31 | 73.98 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono-16B* | 32.99 | 47.81 | 54.58 | 69.31 | 73.98 |'
- en: '| CodeGen-mono-16B | 36.40 | 53.57 | 60.81 | 74.64 | 79.88 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono-16B | 36.40 | 53.57 | 60.81 | 74.64 | 79.88 |'
- en: '| \hdashlineCodeGen-mono-6B | 2 | 35.53 | 49.67 | 55.67 | 68.16 | 77.44 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineCodeGen-mono-6B | 2 | 35.53 | 49.67 | 55.67 | 68.16 | 77.44 |'
- en: '| CodeGen-mono-16B* | 35.85 | 51.31 | 58.23 | 74.02 | 76.60 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono-16B* | 35.85 | 51.31 | 58.23 | 74.02 | 76.60 |'
- en: '| CodeGen-mono-16B | 37.74 | 55.80 | 63.37 | 77.14 | 81.10 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono-16B | 37.74 | 55.80 | 63.37 | 77.14 | 81.10 |'
- en: '|  |  | MBPP |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MBPP |'
- en: '| CodeGen-mono-6B | 1 | 42.77 | 56.91 | 61.43 | 68.84 | 70.67 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono-6B | 1 | 42.77 | 56.91 | 61.43 | 68.84 | 70.67 |'
- en: '| CodeGen-mono-16B* | 43.24 | 60.14 | 65.19 | 72.31 | 74.19 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono-16B* | 43.24 | 60.14 | 65.19 | 72.31 | 74.19 |'
- en: '| CodeGen-mono-16B | 51.44 | 66.45 | 71.64 | 80.62 | 82.93 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono-16B | 51.44 | 66.45 | 71.64 | 80.62 | 82.93 |'
- en: '| \hdashlineCodeGen-mono-6B | 2 | 48.92 | 62.29 | 66.89 | 75.09 | 77.25 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineCodeGen-mono-6B | 2 | 48.92 | 62.29 | 66.89 | 75.09 | 77.25 |'
- en: '| CodeGen-mono-16B* | 48.12 | 65.31 | 70.02 | 76.60 | 78.70 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono-16B* | 48.12 | 65.31 | 70.02 | 76.60 | 78.70 |'
- en: '| CodeGen-mono-16B | 56.44 | 71.31 | 76.43 | 84.39 | 86.76 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono-16B | 56.44 | 71.31 | 76.43 | 84.39 | 86.76 |'
- en: '|   |  |  |  |  |  |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |'
- en: 'Table 10: Ablation on cross-model personalised distillation with PERsD-combined.
    CodeGen-mono-16B* means distillation data is from CodeGen-mono-6B.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: 关于 PERsD-combined 的跨模型个性化蒸馏的消融研究。CodeGen-mono-16B* 表示蒸馏数据来自 CodeGen-mono-6B。'
- en: 5.7 Comparison with other Feedback-based Code Generation Models
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7 与其他基于反馈的代码生成模型的比较
- en: 'Comparison with ILF Chen et al. ([2023a](#bib.bib3)): In order to compare with
    ILF, one of our closest related work, we experiment on a separate setting: starting
    with full MBPP dataset (974 tasks) and use Task-Ids 11-111 as test split and remaining
    863 as training data. On the training set, our student model CodeGen-6B (same
    as ILF) generated wrong attempts on 562 tasks, which were shown to ChatGPT along
    with the task instruction and execution error feedback to eventually collect 288
    valid personalized code rectification labels.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ILF Chen 等 ([2023a](#bib.bib3)) 的比较：为了与 ILF 比较，我们实验了一个不同的设置：从完整的 MBPP 数据集
    (974 个任务) 开始，使用任务 ID 11-111 作为测试集，其余 863 个作为训练数据。在训练集上，我们的学生模型 CodeGen-6B (与 ILF
    相同) 对 562 个任务产生了错误尝试，这些尝试连同任务指令和执行错误反馈一起展示给 ChatGPT，最终收集了 288 个有效的个性化代码纠正标签。
- en: The original MBPP text-to-code data and this collected personalized code-refinement
    data for the 288 tasks
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 MBPP 文本到代码数据和这次收集的个性化代码改进数据用于 288 个任务
- en: '|   | MBPP Test Set |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|   | MBPP 测试集 |'
- en: '| Method | Cost | Pass@1 | Pass@10 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 成本 | Pass@1 | Pass@10 |'
- en: '| ILF | >4K$ | 36 | 68 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| ILF | >4K$ | 36 | 68 |'
- en: '| PERsD | 0.65$ | 46.8 | 67.4 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| PERsD | 0.65$ | 46.8 | 67.4 |'
- en: '| -refine | 0.65$ | 41.8 | 66.8 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| -refine | 0.65$ | 41.8 | 66.8 |'
- en: '| -combined | 0.65$ | 47.8 | 64.8 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| -combined | 0.65$ | 47.8 | 64.8 |'
- en: '|   |  |  |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |'
- en: 'Table 11: Comparison with ILF'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: 与 ILF 的比较'
- en: 'respectively form the finetuning data $\mathcal{D}_{\text{code}}$ to train
    PERsD-combined. Our experimental results in Table [11](#S5.T11 "Table 11 ‣ 5.7
    Comparison with other Feedback-based Code Generation Models ‣ 5 Experimental Results
    ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning
    for Code Generation") show that all PERsD-variants significantly outperform ILF
    by 11.8% at pass@1 at a cost 1e-4 times lower than ILF, thus showcasing the lack
    of scalability of ILF-style models.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 分别形成 finetuning 数据 $\mathcal{D}_{\text{code}}$ 以训练 PERsD-combined。我们在表 [11](#S5.T11
    "表 11 ‣ 5.7 与其他基于反馈的代码生成模型的比较 ‣ 5 实验结果 ‣ 个性化蒸馏：为开源 LLM 提供适应性学习以进行代码生成") 中的实验结果表明，所有
    PERsD 变体在 pass@1 上显著优于 ILF，提升了 11.8%，且成本比 ILF 低 1e-4 倍，从而展示了 ILF 风格模型的扩展性不足。
- en: 'Comparison with Self-Edit: Since Self-Edit Zhang et al. ([2023](#bib.bib26))
    uses a trainable CodeGen-350M code editor model and a frozen code-generation model,
    our experimental setup is not directly comparable with theirs. However, our InpD-refine
    and InpD-combined models can actually be considered as very close counterparts
    to a version of Self-Edit with shared a code-generation and code-refinement model
    and CodeGen-6B backbone. The consistent performance improvement of the personalized
    distillation models over the input-distilled ones across the board, alludes towards
    the prospect that PERsD-models are indeed more effective than Self-Edit style
    models.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Self-Edit 的比较：由于 Self-Edit Zhang 等 ([2023](#bib.bib26)) 使用了一个可训练的 CodeGen-350M
    代码编辑器模型和一个冻结的代码生成模型，我们的实验设置不能直接与其进行比较。然而，我们的 InpD-refine 和 InpD-combined 模型实际上可以视为与
    Self-Edit 版本非常接近的对等模型，该版本共享一个代码生成和代码改进模型及 CodeGen-6B 主干。个性化蒸馏模型在各个方面相较于输入蒸馏模型的一致性能提升，暗示了
    PERsD 模型确实比 Self-Edit 风格的模型更有效。
- en: 5.8 Comparison with SOTA Models
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.8 与 SOTA 模型的比较
- en: 'Fianlly, we compare PERsD-combine models with open-source and close-sourced
    state-of-the-art models on HumanEval in Table [12](#S5.T12 "Table 12 ‣ 5.8 Comparison
    with SOTA Models ‣ 5 Experimental Results ‣ Personalised Distillation: Empowering
    Open-Sourced LLMs with Adaptive Learning for Code Generation").We find that PERsD-combine
    methods can significantly improve the backbone model, with a performance gain
    of 6.2 points for CodeGen-mono 6B (8.4% error reduction), 5.9 points for CodeGen-mono
    16B (8.3% error reduction) and 12.2 points for StarCoder (18.4% error reduction).
    Moreover, StarCoder with PERsD-combined, outperforms other open-sourced models
    except WizardCoder. Note that our model ues 5K data examples while WizardCoder
    uses 78K. As mentioned in [Section 2.1](#S2.SS1 "2.1 Distillation from ChatGPT
    ‣ 2 Related Work ‣ Personalised Distillation: Empowering Open-Sourced LLMs with
    Adaptive Learning for Code Generation"), WizardCoder is an orthogonal approach
    that can be integrated into personalised distillation.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在表格 [12](#S5.T12 "表 12 ‣ 5.8 与SOTA模型的比较 ‣ 5 实验结果 ‣ 个性化蒸馏：通过自适应学习增强开源LLM的代码生成能力")
    中比较了 PERsD-combine 模型与开源和闭源的最先进模型在 HumanEval 上的表现。我们发现，PERsD-combine 方法可以显著提升基础模型的性能，其中
    CodeGen-mono 6B 提高了 6.2 分（错误率降低 8.4%），CodeGen-mono 16B 提高了 5.9 分（错误率降低 8.3%），StarCoder
    提高了 12.2 分（错误率降低 18.4%）。此外，带有 PERsD-combined 的 StarCoder 超越了其他开源模型，仅 WizardCoder
    除外。值得注意的是，我们的模型使用了 5K 数据样本，而 WizardCoder 使用了 78K。如在 [第2.1节](#S2.SS1 "2.1 来自 ChatGPT
    的蒸馏 ‣ 2 相关工作 ‣ 个性化蒸馏：通过自适应学习增强开源LLM的代码生成能力") 中提到的，WizardCoder 是一种可以集成到个性化蒸馏中的正交方法。
- en: '| Model | Model size | Pass@1 | Pass@10 | Pass@100 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 模型大小 | Pass@1 | Pass@10 | Pass@100 |'
- en: '| Closed-source models |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 闭源模型 |'
- en: '| LaMDA | 137B | 14.0 | - | 47.3 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| LaMDA | 137B | 14.0 | - | 47.3 |'
- en: '| PaLM | 540B | 26.2 | - | 76.2 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| PaLM | 540B | 26.2 | - | 76.2 |'
- en: '| Codex | 12B | 28.8 | 46.8 | 72.3 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Codex | 12B | 28.8 | 46.8 | 72.3 |'
- en: '| code-cushman-001 | - | 33.5 | 54.3 | 77.4 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| code-cushman-001 | - | 33.5 | 54.3 | 77.4 |'
- en: '| code-davinci-002 | - | 47.0 | 74.9 | 92.1 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| code-davinci-002 | - | 47.0 | 74.9 | 92.1 |'
- en: '| GPT-3.5 | - | 48.1 | - | - |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | - | 48.1 | - | - |'
- en: '| phi-1 | 1.3B | 50.6 | - | - |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| phi-1 | 1.3B | 50.6 | - | - |'
- en: '| GPT-4 | - | 67.0 | - | - |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | - | 67.0 | - | - |'
- en: '| Open-source models |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 开源模型 |'
- en: '| CodeGeeX | 13B | 22.9 | 39.6 | 60.9 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| CodeGeeX | 13B | 22.9 | 39.6 | 60.9 |'
- en: '| LLaMA | 65B | 23.7 | - | 79.3 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA | 65B | 23.7 | - | 79.3 |'
- en: '| StarCoder | 15B | 33.6 | - | - |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| StarCoder | 15B | 33.6 | - | - |'
- en: '| CodeGen-mono | 6B | 26.1 | 42.3 | 65.8 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono | 6B | 26.1 | 42.3 | 65.8 |'
- en: '| CodeGen-mono | 16B | 29.3 | 49.9 | 75.0 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono | 16B | 29.3 | 49.9 | 75.0 |'
- en: '| InstructCodeT5+ | 16B | 35.0 | 54.5 | 77.9 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| InstructCodeT5+ | 16B | 35.0 | 54.5 | 77.9 |'
- en: '| WizardCoder | 15B | 57.3 | - | - |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| WizardCoder | 15B | 57.3 | - | - |'
- en: '| \hdashlineCodeGen-mono (PERsD-combined) | 6B | 33.8 | 50.0 | 70.7 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineCodeGen-mono (PERsD-combined) | 6B | 33.8 | 50.0 | 70.7 |'
- en: '| CodeGen-mono (PERsD-combined) | 16B | 36.4 | 60.8 | 79.9 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen-mono (PERsD-combined) | 16B | 36.4 | 60.8 | 79.9 |'
- en: '| StarCoder (PERsD-combined) | 15B | 45.8 | 68.3 | 82.3 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| StarCoder (PERsD-combined) | 15B | 45.8 | 68.3 | 82.3 |'
- en: 'Table 12: Results of *pass@k*(%) on HumanEval'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：*pass@k*(%) 在 HumanEval 上的结果
- en: 6 Conclusion
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we introduced personalized distillation as a method for collecting
    customized labeled data that adapts to the capacity of student models, resulting
    in more effective learning. We demonstrated the advantages of personalized distillation
    over standard distillation in the field of code generation, achieving superior
    performance on both the HumanEval and MBPP datasets. Through comprehensive ablation
    studies, we confirmed that personalized distillation leads to higher data quality,
    benefits from multi-round distillation, and enables models to leverage execution
    feedback for self-rectification. We believe personalized distillation represents
    an exciting step towards better distillation of closed-source LLMs to open-source
    models.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了个性化蒸馏作为一种收集适应学生模型容量的定制标注数据的方法，从而实现更有效的学习。我们展示了个性化蒸馏在代码生成领域相对于标准蒸馏的优势，在
    HumanEval 和 MBPP 数据集上都取得了卓越的表现。通过全面的消融研究，我们确认个性化蒸馏带来了更高的数据质量，受益于多轮蒸馏，并使模型能够利用执行反馈进行自我修正。我们相信，个性化蒸馏代表了将闭源
    LLM 更好地蒸馏到开源模型的一个令人兴奋的步骤。
- en: Limitations
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: 'In this section, we discuss some limitations of this paper and future directions
    to make it more valuable:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们讨论了本文的一些局限性及未来的研究方向，以使其更具价值：
- en: On Data Scale
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据规模
- en: 'For a fair comparison, we have conducted all experiments based on the same
    10K $\mathcal{D}_{\textsc{StanD}}$ are of size 2-3K as shown in Table [3](#S4.T3
    "Table 3 ‣ 4.2 Pretraining Data Construction ‣ 4 Experimental Setup ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation").
    However, as we have proven personalised distillation supports more effective and
    efficient learning, it is intriguing to investigate how well does personalised
    distillation scale with the data size. For example, if we scale personalised distillation
    data to 50K, how much more performance gain will PERsD methods receive compared
    to InpD and StanD with the scaling of data size.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进行公平比较，我们基于相同的10K $\mathcal{D}_{\textsc{StanD}}$进行所有实验，数据规模为2-3K，如表[3](#S4.T3
    "Table 3 ‣ 4.2 Pretraining Data Construction ‣ 4 Experimental Setup ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation")所示。然而，正如我们所证明的，个性化蒸馏支持更有效的学习，因此，研究个性化蒸馏如何随着数据规模的扩大而表现如何是很有趣的。例如，如果我们将个性化蒸馏的数据规模扩大到50K，PERsD方法相较于InpD和StanD在数据规模扩大的情况下能获得多大的性能提升。'
- en: Online Personalised Distillation
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在线个性化蒸馏
- en: 'As discussed in [Section 5.4](#S5.SS4 "5.4 Multi-round Distillation ‣ 5 Experimental
    Results ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive
    Learning for Code Generation"), conducting a second round personalised distillation
    continues to improve a student model that is already trained with PERsD-combine.
    Such observation suggests the potential of an online version of personalised distillation,
    which collects a batch of personalised data on-the-fly with the teacher model,
    after each optimization step during finetuning. As we have proven that true personalised
    data is more beneficial than standard data or cross-model personalised data ([Section 5.6](#S5.SS6
    "5.6 Cross-Model Personalised Distillation ‣ 5 Experimental Results ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation")),
    such online personalised distillation will in-principle maximally benefit from
    personalised distillation, as each batch of training data is fully tailored to
    the student model.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '如[第5.4节](#S5.SS4 "5.4 Multi-round Distillation ‣ 5 Experimental Results ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation")所讨论，进行第二轮个性化蒸馏可以继续改进已经使用PERsD-combine训练过的学生模型。这种观察表明了在线个性化蒸馏的潜力，即在每次微调的优化步骤后，利用教师模型实时收集一批个性化数据。正如我们所证明的，真实的个性化数据比标准数据或跨模型个性化数据更有益（[第5.6节](#S5.SS6
    "5.6 Cross-Model Personalised Distillation ‣ 5 Experimental Results ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation")），因此这种在线个性化蒸馏原则上将最大限度地受益于个性化蒸馏，因为每批训练数据都完全为学生模型量身定制。'
- en: References
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V.
    Le, and Charles Sutton. 2021. Program synthesis with large language models. *CoRR*,
    abs/2108.07732.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc
    V. Le, and Charles Sutton. 2021. 使用大型语言模型进行程序合成。 *CoRR*, abs/2108.07732.
- en: 'Chaudhary (2023) Sahil Chaudhary. 2023. Code alpaca: An instruction-following
    llama model for code generation. [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca).'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chaudhary (2023) Sahil Chaudhary. 2023. Code alpaca: 一种用于代码生成的遵循指令的llama模型。
    [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca).'
- en: Chen et al. (2023a) Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander
    Campos, Jun Shern Chan, Samuel R. Bowman, Kyunghyun Cho, and Ethan Perez. 2023a.
    [Improving code generation by training with natural language feedback](http://arxiv.org/abs/arXiv%20preprint%20arXiv:2303.16749).
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023a) Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander
    Campos, Jun Shern Chan, Samuel R. Bowman, Kyunghyun Cho, and Ethan Perez. 2023a.
    [通过自然语言反馈提高代码生成](http://arxiv.org/abs/arXiv%20preprint%20arXiv:2303.16749)。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé
    de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. 2021. Evaluating large language models trained on code. *CoRR*,
    abs/2107.03374.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas
    Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,
    Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
    Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,
    Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios
    Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol,
    Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu
    Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam,
    Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira
    Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,
    Ilya Sutskever, 和 Wojciech Zaremba. 2021. 评估以代码训练的大语言模型。*CoRR*, abs/2107.03374.
- en: Chen et al. (2023b) Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
    2023b. Teaching large language models to self-debug. *CoRR*, abs/2304.05128.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023b) Xinyun Chen, Maxwell Lin, Nathanael Schärli, 和 Denny Zhou.
    2023b. 教授大语言模型自我调试。*CoRR*, abs/2304.05128.
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. [Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, 和 Eric P. Xing. 2023. [Vicuna: 一款开源聊天机器人以90%* chatgpt质量* 令GPT-4印象深刻](https://lmsys.org/blog/2023-03-30-vicuna/)。'
- en: 'Jiang et al. (2023) Yuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang.
    2023. Lion: Adversarial distillation of closed-source large language model. *CoRR*,
    abs/2305.12870.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2023) Yuxin Jiang, Chunkit Chan, Mingyang Chen, 和 Wei Wang. 2023.
    Lion: 封闭源大语言模型的对抗性蒸馏。*CoRR*, abs/2305.12870.'
- en: 'Li et al. (2023a) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene,
    Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier,
    Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin
    Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp
    Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi
    Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim
    Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire
    Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer
    Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva
    Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis,
    Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023a.
    Starcoder: may the source be with you! *CoRR*, abs/2305.06161.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023a) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene,
    Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier,
    Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin
    Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva
    Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy,
    Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas,
    Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding,
    Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex
    Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor,
    Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis,
    Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, 和 Harm de Vries. 2023a.
    Starcoder: 愿源代码与你同在！*CoRR*, abs/2305.06161.'
- en: 'Li et al. (2023b) Zihao Li, Zhuoran Yang, and Mengdi Wang. 2023b. Reinforcement
    learning with human feedback: Learning dynamic choices via pessimism. *CoRR*,
    abs/2305.18438.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023b) Zihao Li, Zhuoran Yang, 和 Mengdi Wang. 2023b. 人类反馈的强化学习：通过悲观主义学习动态选择。*CoRR*,
    abs/2305.18438.
- en: Liu et al. (2023) Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023. Chain
    of hindsight aligns language models with feedback. *CoRR*, abs/2302.02676.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023) **浩·刘**、**卡梅洛·斯费拉扎** 和 **彼得·阿贝尔**。2023。追溯链将语言模型与反馈对齐。*CoRR*，abs/2302.02676。
- en: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering
    code large language models with evol-instruct. *CoRR*, abs/2306.08568.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人 (2023) **紫阳·罗**、**灿·徐**、**蒲·赵**、**清风·孙**、**修波·耿**、**文翔·胡**、**崇阳·陶**、**晶·马**、**清伟·林**
    和 **达新·姜**。2023。Wizardcoder：通过 evol-instruct 赋能代码大型语言模型。*CoRR*，abs/2306.08568。
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh,
    and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. *CoRR*,
    abs/2303.17651.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madaan 等人 (2023) **阿曼·马丹**、**尼克特·坦东**、**普拉卡尔·古普塔**、**斯凯勒·哈利南**、**吕宇·高**、**莎拉·维格雷夫**、**乌里·阿隆**、**诺哈·兹里**、**施瑞迈·普拉布默**、**易鸣·杨**、**肖恩·维莱克**、**博地萨特瓦·普拉萨德·马贾姆德**、**沙尚克·古普塔**、**阿米尔·亚兹丹巴赫**
    和 **彼得·克拉克**。2023。Self-refine：通过自我反馈进行迭代优化。*CoRR*，abs/2303.17651。
- en: 'Nijkamp et al. (2023) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. Codegen: An open
    large language model for code with multi-turn program synthesis. *ICLR*.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nijkamp 等人 (2023) **埃里克·奈坎普**、**博·庞**、**广崎·林**、**李富·图**、**华·王**、**英博·周**、**西尔维奥·萨瓦雷斯**
    和 **蔡明·熊**。2023。Codegen：用于多轮程序合成的开源大型代码语言模型。*ICLR*。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training
    language models to follow instructions with human feedback. In *NeurIPS*.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 (2022) **龙·欧阳**、**杰弗里·吴**、**徐·姜**、**迪奥戈·阿尔梅达**、**卡罗尔·L·温赖特**、**帕梅拉·米什金**、**崇·张**、**桑迪尼·阿加瓦尔**、**卡塔里娜·斯拉马**、**亚历克斯·雷**、**约翰·舒尔曼**、**雅各布·希尔顿**、**弗雷泽·凯尔顿**、**卢克·米勒**、**麦迪·西门斯**、**阿曼达·阿斯克尔**、**彼得·韦林德**、**保罗·F·克里斯蒂亚诺**、**简·莱克**
    和 **瑞安·洛**。2022。训练语言模型以遵循指令和人类反馈。在 *NeurIPS* 上。
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct preference optimization:
    Your language model is secretly a reward model. *CoRR*, abs/2305.18290.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov 等人 (2023) **拉法伊尔·拉法伊洛夫**、**阿基特·香玛**、**埃里克·米切尔**、**斯特凡诺·厄尔蒙**、**克里斯托弗·D·曼宁**
    和 **切尔西·芬恩**。2023。直接偏好优化：你的语言模型实际上是一个奖励模型。*CoRR*，abs/2305.18290。
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. 2020. Zero: memory optimizations toward training trillion parameter
    models. In *SC*, page 20\. IEEE/ACM.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajbhandari 等人 (2020) **萨米亚姆·拉杰班达里**、**杰夫·拉斯利**、**奥拉图吉·鲁瓦斯** 和 **余雄·赫**。2020。Zero：训练万亿参数模型的内存优化。在
    *SC*，第20页。IEEE/ACM。
- en: 'Roberts-Mahoney et al. (2016) Heather Roberts-Mahoney, Alexander J. Means,
    and Mark J. Garrison. 2016. [Netflixing human capital development: personalized
    learning technology and the corporatization of k-12 education](https://doi.org/10.1080/02680939.2015.1132774).
    *Journal of Education Policy*, 31(4):405–420.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roberts-Mahoney 等人 (2016) **希瑟·罗伯茨-马赫尼**、**亚历山大·J·米恩斯** 和 **马克·J·加里森**。2016。
    [Netflixing human capital development: personalized learning technology and the
    corporatization of k-12 education](https://doi.org/10.1080/02680939.2015.1132774)。*Journal
    of Education Policy*，31(4)：405–420。'
- en: Shemshack and Spector (2020) Atikah Shemshack and Jonathan Michael Spector.
    2020. A systematic literature review of personalized learning terms. *Smart Learning
    Environments*, 7(1):1–20.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shemshack 和 Spector (2020) **阿蒂卡·谢姆沙克** 和 **乔纳森·迈克尔·斯佩克特**。2020。个性化学习术语的系统文献综述。*Smart
    Learning Environments*，7(1)：1–20。
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath,
    Karthik Narasimhan, and Shunyu Yao. 2023. [Reflexion: Language agents with verbal
    reinforcement learning](http://arxiv.org/abs/2303.11366).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn 等人 (2023) **诺亚·申**、**费德里科·卡萨诺**、**贝克·拉巴什**、**阿什温·戈皮纳特**、**卡尔提克·纳拉辛汉**
    和 **顺宇·姚**。2023。 [Reflexion: Language agents with verbal reinforcement learning](http://arxiv.org/abs/2303.11366)。'
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori 等人 (2023) **罗汉·陶里**、**伊尚·古尔拉贾尼**、**田一·张**、**扬·杜布瓦**、**薛晨·李**、**卡洛斯·古斯特林**、**帕西·梁**
    和 **辰里·B·哈希莫托**。2023。斯坦福 Alpaca：一个遵循指令的 Llama 模型。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。
- en: 'Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct:
    Aligning language model with self generated instructions.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2022）Yizhong Wang、Yeganeh Kordi、Swaroop Mishra、Alisa Liu、Noah A. Smith、Daniel
    Khashabi 和 Hannaneh Hajishirzi。2022。Self-instruct：使语言模型与自生成的指令对齐。
- en: Welleck et al. (2022) Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao
    Shen, Daniel Khashabi, and Yejin Choi. 2022. Generating sequences by learning
    to self-correct. *CoRR*, abs/2211.00053.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Welleck 等人（2022）Sean Welleck、Ximing Lu、Peter West、Faeze Brahman、Tianxiao Shen、Daniel
    Khashabi 和 Yejin Choi。2022。通过学习自我修正生成序列。*CoRR*，abs/2211.00053。
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. 2020. [Transformers: State-of-the-art natural language
    processing](https://www.aclweb.org/anthology/2020.emnlp-demos.6). In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations*, pages 38–45, Online. Association for Computational Linguistics.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf 等人（2020）Thomas Wolf、Lysandre Debut、Victor Sanh、Julien Chaumond、Clement
    Delangue、Anthony Moi、Pierric Cistac、Tim Rault、Rémi Louf、Morgan Funtowicz、Joe Davison、Sam
    Shleifer、Patrick von Platen、Clara Ma、Yacine Jernite、Julien Plu、Canwen Xu、Teven
    Le Scao、Sylvain Gugger、Mariama Drame、Quentin Lhoest 和 Alexander M. Rush。2020。
    [Transformers: State-of-the-art natural language processing](https://www.aclweb.org/anthology/2020.emnlp-demos.6)。在
    *2020年自然语言处理领域方法会议：系统演示* 中，第 38–45 页，在线。计算语言学协会。'
- en: 'Xu et al. (2023a) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan
    Feng, Chongyang Tao, and Daxin Jiang. 2023a. Wizardlm: Empowering large language
    models to follow complex instructions. *CoRR*, abs/2304.12244.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 徐等人（2023a）Can Xu、Qingfeng Sun、Kai Zheng、Xiubo Geng、Pu Zhao、Jiazhan Feng、Chongyang
    Tao 和 Daxin Jiang。2023a。Wizardlm：赋能大型语言模型以遵循复杂指令。*CoRR*，abs/2304.12244。
- en: 'Xu et al. (2023b) Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023b.
    Baize: An open-source chat model with parameter-efficient tuning on self-chat
    data. *arXiv preprint arXiv:2304.01196*.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 徐等人（2023b）Canwen Xu、Daya Guo、Nan Duan 和 Julian McAuley。2023b。Baize：一个开源聊天模型，具有在自我聊天数据上的参数高效调优。*arXiv
    预印本 arXiv:2304.01196*。
- en: 'Zhang et al. (2023) Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. 2023.
    Self-edit: Fault-aware code editor for code generation. *CoRR*, abs/2305.04087.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2023）Kechi Zhang、Zhuo Li、Jia Li、Ge Li 和 Zhi Jin。2023。Self-edit：一种面向代码生成的故障感知代码编辑器。*CoRR*，abs/2305.04087。
- en: Appendix A Details in Multi-step Model Evaluation
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 多步骤模型评估的细节
- en: As the docstrings are ill-formated in HumanEval, we write a simple rule-based
    parsing code snippet to extract its seen unit test cases. On average per task,
    there is 2 seen unit test cases and 4.2 unseen unit test cases. The overlap between
    seen and unseen tests is 11.33%. For MBPP, since conventionally the instruction
    prompt is constructed by taking the task description and example usages (from
    the unit test cases) as part of the doc-string, we consider all the unit test
    cases to be "seen" and use all of them for multi-step inference.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 HumanEval 中的文档字符串格式不规范，我们编写了一个简单的基于规则的解析代码片段来提取其已见单元测试用例。平均每个任务中，有 2 个已见单元测试用例和
    4.2 个未见单元测试用例。已见测试和未见测试之间的重叠率为 11.33%。对于 MBPP，由于传统上指令提示是通过将任务描述和示例用法（来自单元测试用例）作为文档字符串的一部分来构造的，我们将所有单元测试用例视为“已见”，并使用所有这些用例进行多步骤推理。
- en: Appendix B ChatGPT Prompt Template for Personalised Distillation
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 针对个性化蒸馏的 ChatGPT 提示模板
- en: 'In Figure [3](#A2.F3 "Figure 3 ‣ Appendix B ChatGPT Prompt Template for Personalised
    Distillation ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive
    Learning for Code Generation"), we show the prompt template we use to query ChatGPT
    for personalised refinement. For each task example, with task instruction $t$,
    we query ChatGPT API with two turn conversation history.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [3](#A2.F3 "图 3 ‣ 附录 B ChatGPT 提示模板用于个性化蒸馏 ‣ 个性化蒸馏：通过自适应学习赋能开源 LLM 以生成代码")
    中，我们展示了用于查询 ChatGPT 进行个性化改进的提示模板。对于每个任务示例，使用任务指令 $t$，我们用两轮对话历史查询 ChatGPT API。
- en: 'For the first turn, we use the template in Figure [3(a)](#A2.F3.sf1 "Figure
    3(a) ‣ Figure 3 ‣ Appendix B ChatGPT Prompt Template for Personalised Distillation
    ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning
    for Code Generation") and replace <>, <> with the actual task instruction
    $t$ is included as first turn’s assistant output. For the second turn, we use
    the template in Figure [3(b)](#A2.F3.sf2 "Figure 3(b) ‣ Figure 3 ‣ Appendix B
    ChatGPT Prompt Template for Personalised Distillation ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation") and
    replace <>, <> with the student model’s attempt and its execution
    feedback. This is added to second turn’s user input and we query ChatGPT with
    the constructed converstaion history to get second turn’s assistant output as
    personalised code refinement.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一次对话，我们使用图 [3(a)](#A2.F3.sf1 "图 3(a) ‣ 图 3 ‣ 附录 B ChatGPT 个性化蒸馏提示模板 ‣ 个性化蒸馏：通过自适应学习赋能开源
    LLM 进行代码生成") 中的模板，并将 <>、<> 替换为实际任务指令 $t$，作为第一次对话的助理输出。对于第二次对话，我们使用图
    [3(b)](#A2.F3.sf2 "图 3(b) ‣ 图 3 ‣ 附录 B ChatGPT 个性化蒸馏提示模板 ‣ 个性化蒸馏：通过自适应学习赋能开源 LLM
    进行代码生成") 中的模板，将 <>、<> 替换为学生模型的尝试及其执行反馈。这将添加到第二次对话的用户输入中，我们使用构建的对话历史查询
    ChatGPT，以获得第二次对话的助理输出作为个性化代码精炼。
- en: '![Refer to caption](img/7126bd002ce9d8e563740bcc7dca117e.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7126bd002ce9d8e563740bcc7dca117e.png)'
- en: (a) Turn-1 Prompt Template
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 第一次提示模板
- en: '![Refer to caption](img/83572f4ce8a60c494c00032ea6824120.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/83572f4ce8a60c494c00032ea6824120.png)'
- en: (b) Turn-2 Prompt Template
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 第二轮提示模板
- en: 'Figure 3: Prompt templates to query personalised refinement. Top(a): prompt
    template for first turn conversation, Botton(b): prompt template for second turn
    conversation.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 查询个性化精炼的提示模板。顶部(a): 第一次对话的提示模板，底部(b): 第二次对话的提示模板。'
- en: Appendix C Prompt Template for Code Refinement Finetuning
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 代码精炼微调的提示模板
- en: 'Figure [4](#A3.F4 "Figure 4 ‣ Appendix C Prompt Template for Code Refinement
    Finetuning ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive
    Learning for Code Generation") shows the refinement template $T_{\text{refine}}$
    introduced in [Section 3.2](#S3.SS2 "3.2 Personalised Distillation ‣ 3 Method
    ‣ Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning
    for Code Generation")), which is used to construct input prompt for code refinement
    finetuning. we replace <> with task instruction, <> with the initial
    wrong attempt from student, <> with the execution feedback, and <>
    with function header extracted from task instruciton.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#A3.F4 "图 4 ‣ 附录 C 代码精炼微调的提示模板 ‣ 个性化蒸馏：通过自适应学习赋能开源 LLM 进行代码生成") 显示了在 [第
    3.2 节](#S3.SS2 "3.2 个性化蒸馏 ‣ 3 方法 ‣ 个性化蒸馏：通过自适应学习赋能开源 LLM 进行代码生成") 引入的精炼模板 $T_{\text{refine}}$，用于构建代码精炼微调的输入提示。我们用任务指令替换
    <>，用学生的初步错误尝试替换 <>，用执行反馈替换 <>，用从任务指令中提取的函数头替换 <>。
- en: '![Refer to caption](img/f3572377187693d1a1db3580522a37ca.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f3572377187693d1a1db3580522a37ca.png)'
- en: 'Figure 4: Prompt template for code refinement finetuning.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 代码精炼微调的提示模板。'
- en: Appendix D Details in Data Overlap Analysis
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 数据重叠分析详细信息
- en: This section describes the detailed procedures to conduct train-test data overlap
    analysis. The objective is to assess the extent of data leakage in the test datasets
    originating from our self-constructed pretraining corpus.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了进行训练-测试数据重叠分析的详细过程。其目标是评估来自我们自构建的预训练语料库的测试数据集中数据泄漏的程度。
- en: Firstly, we have performed exact string match and found no data leakage in any
    test data (HumanEval/MBPP).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们进行了精确字符串匹配，并在任何测试数据 (HumanEval/MBPP) 中未发现数据泄漏。
- en: 'To measure the semantic similarity between training/test tasks, we did the
    following:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量训练/测试任务之间的语义相似度，我们进行了以下操作：
- en: '1.'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: For each task in the test (MBPP/HumanEval) we retrieve two closest training
    tasks (based on cosine similarity of starcoder embedding & tf-idf vectors of task
    description).
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于测试中的每个任务 (MBPP/HumanEval)，我们检索两个最接近的训练任务（基于星编码嵌入和任务描述的 tf-idf 向量的余弦相似度）。
- en: '2.'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We use gpt-3.5-turbo-16k to identify whether there is a data leak between a
    train and test instance by classifying the pair into (“leak”, “somewhat similar”,
    “somewhat not similar”, “not related”). We use a prompt with instructions and
    manually created few-shot examples and ask gpt-3.5 to generate the reasoning and
    categorization. We manually examined several examples per category to ensure the
    reasoning and judgment is done correctly and consistently.
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用gpt-3.5-turbo-16k来识别训练和测试实例之间是否存在数据泄漏，通过将对的配对分类为（“泄漏”，“有些相似”，“有些不相似”，“不相关”）。我们使用包含指令的提示和手动创建的少量示例，并请求gpt-3.5生成推理和分类。我们手动检查了每个类别的几个示例，以确保推理和判断是正确且一致的。
- en: '3.'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Map the similarity categories to 0-1 similarity-score (“leak” -> 1, “somewhat
    similar” -> 0.75, “somewhat not similar” -> 0.25, “not related” -> 0) and show
    the mean score and % of cases classified as “leak”. Note that StanD & PERsD have
    10K & 3K training data respectively so their scores are different.
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将相似度类别映射到0-1相似度分数（“泄漏” -> 1，“有些相似” -> 0.75，“有些不相似” -> 0.25，“不相关” -> 0），并显示平均分数和分类为“泄漏”的案例百分比。注意，StanD
    和 PERsD 分别有10K和3K的训练数据，因此它们的分数不同。
- en: Appendix E Results in MBPP-Cleaned
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E MBPP-Cleaned结果
- en: 'In [Appendix D](#A4 "Appendix D Details in Data Overlap Analysis ‣ Personalised
    Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation"),
    we find 55 data instances that are potentially leaked (with similarity score =
    1) in MBPP test data. In this section, we construct a new MBPP-Cleaned dataset,
    where the leaked data points are removed (originally 306 problems → 251 problems
    after filtering). The results on this new MBPP-Cleaned dataset is shown in Table
    [13](#A5.T13 "Table 13 ‣ Appendix E Results in MBPP-Cleaned ‣ Personalised Distillation:
    Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation"). From
    the results, we can see for setting CodeGen-mono-16B, pass@1, PERsD becomes almost
    on-par with StanD (from a gap of -1.21 to -0.17). For the rest of 15/16 settings
    on PERsD comparing with StanD, its average margin is increased from 4.8 points
    to 5.9 points. Besides, PERsD-refine on MBPP-Cleaned shows more consistent and
    sizable improvements over InpD-refine, with an average edge of +0.86 for 1 step
    inference, and +1.91 for two step inference. Overall, with overlapped test data
    removed, PERsD methods show even larger advantages compared to StanD or InpD methods.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在[附录D](#A4 "附录 D 数据重叠分析 ‣ 个性化蒸馏：为开源LLMs赋能，进行代码生成的自适应学习")中，我们发现MBPP测试数据中有55个数据实例可能泄漏（相似度分数
    = 1）。在这一部分，我们构建了一个新的MBPP-Cleaned数据集，其中移除了泄漏的数据点（原本306个问题 → 过滤后251个问题）。新MBPP-Cleaned数据集上的结果显示在表[13](#A5.T13
    "表13 ‣ 附录E MBPP-Cleaned结果 ‣ 个性化蒸馏：为开源LLMs赋能，进行代码生成的自适应学习")中。从结果可以看出，对于CodeGen-mono-16B设置，pass@1，PERsD几乎与StanD持平（从-1.21的差距缩小到-0.17）。在与StanD比较的其余15/16个设置中，PERsD的平均差距从4.8分增加到5.9分。此外，PERsD-refine在MBPP-Cleaned数据集上显示出比InpD-refine更一致和显著的改进，1步推理的平均优势为+0.86，两步推理为+1.91。总体而言，去除重叠测试数据后，PERsD方法相比StanD或InpD方法显示出更大的优势。
- en: (a) Backbone as CodeGen-mono-6B
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Backbone as CodeGen-mono-6B
- en: '|   Methods | #Data | Pass@1 | Pass@5 | Pass@10 | Pass@20 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|   方法 | #数据 | Pass@1 | Pass@5 | Pass@10 | Pass@20 |'
- en: '| \cdashline3-10 | step=1 | step=2 | step=1 | step=2 | step=1 | step=2 | step=1
    | step=2 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline3-10 | step=1 | step=2 | step=1 | step=2 | step=1 | step=2 | step=1
    | step=2 |'
- en: '|  | MBPP-Cleaned |  |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  | MBPP-Cleaned |  |'
- en: '| StanD | 10K | 37.51 | - | 50.89 | - | 55.15 | - | 58.87 | - |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| StanD | 10K | 37.51 | - | 50.89 | - | 55.15 | - | 58.87 | - |'
- en: '| \hdashlineInpD | 3.3K | 38.80 | - | 53.91 | - | 58.47 | - | 62.73 | - |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineInpD | 3.3K | 38.80 | - | 53.91 | - | 58.47 | - | 62.73 | - |'
- en: '| -refine | 3.3K | 37.58 | 42.95 | 57.65 | 62.29 | 63.52 | 67.79 | 67.92 |
    71.96 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| -refine | 3.3K | 37.58 | 42.95 | 57.65 | 62.29 | 63.52 | 67.79 | 67.92 |
    71.96 |'
- en: '| -combined | 6.5K | 38.11 | 43.01 | 52.69 | 58.32 | 57.36 | 62.75 | 61.19
    | 66.18 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| -combined | 6.5K | 38.11 | 43.01 | 52.69 | 58.32 | 57.36 | 62.75 | 61.19
    | 66.18 |'
- en: '| \hdashlinePERsD | 3.3K | 41.30 | - | 56.20 | - | 61.86 | - | 67.53 | - |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlinePERsD | 3.3K | 41.30 | - | 56.20 | - | 61.86 | - | 67.53 | - |'
- en: '| -refine | 3.3K | 43.86 | 47.73 | 59.33 | 64.41 | 65.19 | 69.95 | 69.62 |
    74.33 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| -refine | 3.3K | 43.86 | 47.73 | 59.33 | 64.41 | 65.19 | 69.95 | 69.62 |
    74.33 |'
- en: '| -combined | 6.5K | 38.86 | 43.75 | 52.78 | 57.04 | 57.35 | 61.78 | 61.52
    | 66.19 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| -combined | 6.5K | 38.86 | 43.75 | 52.78 | 57.04 | 57.35 | 61.78 | 61.52
    | 66.19 |'
- en: '|   |  |  |  |  |  |  |  |  |  |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |'
- en: (b) Backbone as CodeGen-mono-16B
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Backbone as CodeGen-mono-16B
- en: '|   Methods | #Data | Pass@1 | Pass@5 | Pass@10 | Pass@20 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|   方法 | #数据 | Pass@1 | Pass@5 | Pass@10 | Pass@20 |'
- en: '| \cdashline3-10 | step=1 | step=2 | step=1 | step=2 | step=1 | step=2 | step=1
    | step=2 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline3-10 | step=1 | step=2 | step=1 | step=2 | step=1 | step=2 | step=1
    | step=2 |'
- en: '|  | MBPP-Cleaned |  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  | MBPP-Cleaned |  |'
- en: '| StanD | 10K | 43.10 | - | 57.53 | - | 62.92 | - | 68.12 | - |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| StanD | 10K | 43.10 | - | 57.53 | - | 62.92 | - | 68.12 | - |'
- en: '| \hdashlineInpD | 2.8K | 40.64 | - | 53.88 | - | 58.82 | - | 62.88 | - |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineInpD | 2.8K | 40.64 | - | 53.88 | - | 58.82 | - | 62.88 | - |'
- en: '| -refine | 2.8K | 43.67 | 49.60 | 63.14 | 68.21 | 69.27 | 73.28 | 73.36 |
    76.85 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| -refine | 2.8K | 43.67 | 49.60 | 63.14 | 68.21 | 69.27 | 73.28 | 73.36 |
    76.85 |'
- en: '| -combined | 5.6K | 41.63 | 47.77 | 54.74 | 62.24 | 59.67 | 67.33 | 63.75
    | 71.57 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| -combined | 5.6K | 41.63 | 47.77 | 54.74 | 62.24 | 59.67 | 67.33 | 63.75
    | 71.57 |'
- en: '| \hdashlinePERsD | 2.8K | 42.93 | - | 62.40 | - | 68.90 | - | 74.10 | - |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlinePERsD | 2.8K | 42.93 | - | 62.40 | - | 68.90 | - | 74.10 | - |'
- en: '| -refine | 2.8K | 47.73 | 52.63 | 63.62 | 69.21 | 69.84 | 75.17 | 74.90 |
    79.69 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| -refine | 2.8K | 47.73 | 52.63 | 63.62 | 69.21 | 69.84 | 75.17 | 74.90 |
    79.69 |'
- en: '| -combined | 5.6K | 46.33 | 51.67 | 63.46 | 68.65 | 69.49 | 74.26 | 74.53
    | 78.83 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| -combined | 5.6K | 46.33 | 51.67 | 63.46 | 68.65 | 69.49 | 74.26 | 74.53
    | 78.83 |'
- en: '|   |  |  |  |  |  |  |  |  |  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |'
- en: 'Table 13: Comparing performance of PERsD models to StanD & InpD on MBPP-Cleaned'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '表 13: 比较 PERsD 模型与 StanD 和 InpD 在 MBPP-Cleaned 上的表现'
