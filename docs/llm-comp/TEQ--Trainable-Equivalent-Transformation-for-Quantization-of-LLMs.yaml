- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:51:05'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:05
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'TEQ: Trainable Equivalent Transformation for Quantization of LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TEQ：用于LLMs量化的可训练等效变换
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.10944](https://ar5iv.labs.arxiv.org/html/2310.10944)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.10944](https://ar5iv.labs.arxiv.org/html/2310.10944)
- en: Wenhua Cheng    Yiyang Cai    Kaokao Lv    Haihao Shen
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 温华·程    义阳·蔡    考考·吕    海浩·申
- en: Intel
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Intel
- en: '{wenhua.cheng, yiyang.cai, kaokao.lv, haihao.shen}@intel.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{wenhua.cheng, yiyang.cai, kaokao.lv, haihao.shen}@intel.com'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: As large language models (LLMs) become more prevalent, there is a growing need
    for new and improved quantization methods that can meet the computationalast layer
    demands of these modern architectures while maintaining the accuracy. In this
    paper, we present TEQ, a trainable equivalent transformation that preserves the
    FP32 precision of the model output while taking advantage of low-precision quantization,
    especially 3 and 4 bits weight-only quantization. The training process is lightweight,
    requiring only 1K steps and less than $1\text{\textperthousand}$ of the original
    model’s trainable parameters. Furthermore, the transformation does not add any
    computational overhead during inference. Our results are on-par with the state-of-the-art
    (SOTA) methods on typical LLMs. Our approach can be combined with other methods
    to achieve even better performance. The code is available at https://github.com/intel/neural-compressor.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）变得越来越普遍，对新的改进的量化方法的需求也在增长，这些方法需要满足这些现代架构的计算需求，同时保持准确性。在本文中，我们提出了TEQ，一种可训练的等效变换，它在利用低精度量化（特别是3位和4位权重量化）的同时，保留了模型输出的FP32精度。训练过程非常轻量，仅需1K步和不到原始模型训练参数的$1\text{\textperthousand}$。此外，该变换在推理过程中不会增加任何计算开销。我们的结果与典型LLMs的最先进（SOTA）方法相当。我们的方法可以与其他方法结合，以实现更好的性能。代码可在
    [https://github.com/intel/neural-compressor](https://github.com/intel/neural-compressor)
    获得。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Large language models (LLMs) have not only shown breakthrough performance in
    a wide range of benchmarks and tasks but played an increasingly important role
    in daily life, e.g., ChatGPT ([OpenAI,](#bib.bib25) ) in information retrieval
    and Copilot ([Github,](#bib.bib10) ) in programming. However, as LLMs’ model size
    keeps growing dramatically, their significant memory footprint and heavy computation
    requirements become a major bottleneck of their usage.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）不仅在广泛的基准和任务中表现出了突破性的性能，而且在日常生活中扮演着越来越重要的角色，例如，ChatGPT ([OpenAI,](#bib.bib25)
    ) 在信息检索中，Copilot ([Github,](#bib.bib10) ) 在编程中。然而，随着LLMs模型规模的急剧增长，其显著的内存占用和巨大的计算需求成为其使用的主要瓶颈。
- en: One of the most promising ways to alleviate this challenge is quantization,
    which can reduce storage and computational overhead. Quantization converts high-bit
    floating-point data to lower-bit representations, and it has become an effective
    model compression technique.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解这一挑战的最有前景的方法之一是量化，它可以减少存储和计算开销。量化将高位浮点数据转换为低位表示，它已成为一种有效的模型压缩技术。
- en: 'Quantization methods can generally be divided into two categories: quantization
    aware training (QAT) (Shen et al., [2021](#bib.bib30); Zhuang et al., [2021](#bib.bib38);
    Gong et al., [2019](#bib.bib11); Esser et al., [2019](#bib.bib6); Louizos et al.,
    [2018](#bib.bib21)) and post-training quantization (PTQ) (Frantar et al., [2022](#bib.bib8);
    Li et al., [2022](#bib.bib19); Xiao et al., [2022](#bib.bib33); Wei et al., [2022](#bib.bib32);
    Frantar and Alistarh, [2022](#bib.bib7); Hubara et al., [2021](#bib.bib15); Nagel
    et al., [2020](#bib.bib24); Hassibi et al., [1993](#bib.bib12); LeCun et al.,
    [1989](#bib.bib18)). Their effectiveness has been validated for a wide range of
    models. However, several issues still need to be addressed, especially for LLMs.
    QAT simulates the quantization behavior in the training/finetuning phase, but
    such a process is very costly for LLMs due to their unprecedented parameter scale.
    In contrast, PTQ requires no training and thus has drawn rising attention. However,
    PTQ is prone to large accuracy drops, especially for extreme low-bit quantization.
    This provides LLMs’ PTQ methods with great opportunities for improvement.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 量化方法通常可以分为两类：量化感知训练（QAT）（Shen et al., [2021](#bib.bib30); Zhuang et al., [2021](#bib.bib38);
    Gong et al., [2019](#bib.bib11); Esser et al., [2019](#bib.bib6); Louizos et al.,
    [2018](#bib.bib21)）和训练后量化（PTQ）（Frantar et al., [2022](#bib.bib8); Li et al., [2022](#bib.bib19);
    Xiao et al., [2022](#bib.bib33); Wei et al., [2022](#bib.bib32); Frantar 和 Alistarh,
    [2022](#bib.bib7); Hubara et al., [2021](#bib.bib15); Nagel et al., [2020](#bib.bib24);
    Hassibi et al., [1993](#bib.bib12); LeCun et al., [1989](#bib.bib18)）。这两类方法的有效性已在广泛的模型上得到了验证。然而，仍有若干问题需要解决，尤其是对于
    LLM。QAT 在训练/微调阶段模拟量化行为，但由于 LLM 的前所未有的参数规模，这一过程对 LLM 的成本非常高。相反，PTQ 不需要训练，因此引起了越来越多的关注。然而，PTQ
    容易出现大幅准确度下降，特别是在极端低比特量化的情况下。这为 LLM 的 PTQ 方法提供了很大的改进机会。
- en: Lower-bit quantization (e.g., Int4, W4) has recently been widely discussed since
    memory bandwidth is becoming the main bottleneck of LLMs. However, most existing
    works focus on computer vision models (He et al., [2016](#bib.bib13); Howard et al.,
    [2017](#bib.bib14)) that are much smaller than current popular LLMs such as BLOOM-176B(Scao
    et al., [2022](#bib.bib29)), OPT-175B(Zhang et al., [2022](#bib.bib36)). Other
    extreme quantization methods (Bai et al., [2020](#bib.bib2); Zhang et al., [2020](#bib.bib37))
    rely on the knowledge distillation technique, introducing extra overhead. GPTQ(Frantar
    et al., [2022](#bib.bib8)) tunes the weights based on optimal brain surgeon(Hassibi
    et al., [1993](#bib.bib12)) and successfully achieves low-bit quantization on
    LLMs with low computation overhead.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内存带宽正成为 LLM 的主要瓶颈，低比特量化（例如，Int4，W4）最近得到了广泛讨论。然而，大多数现有工作关注的是计算机视觉模型（He et al.,
    [2016](#bib.bib13); Howard et al., [2017](#bib.bib14)），这些模型远小于当前流行的 LLM，如 BLOOM-176B（Scao
    et al., [2022](#bib.bib29)），OPT-175B（Zhang et al., [2022](#bib.bib36)）。其他极端量化方法（Bai
    et al., [2020](#bib.bib2); Zhang et al., [2020](#bib.bib37)）依赖知识蒸馏技术，带来额外的开销。GPTQ（Frantar
    et al., [2022](#bib.bib8)）基于优化脑外科医生（Hassibi et al., [1993](#bib.bib12)）调整权重，并成功实现了低计算开销的
    LLM 低比特量化。
- en: 'Our proposed method reduces the compression error by introducing a trainable
    equivalent transformation (Fig. [1](#S3.F1 "Figure 1 ‣ 3 Methodology ‣ TEQ: Trainable
    Equivalent Transformation for Quantization of LLMs")), which keeps the mathematical
    equivalency of model output at FP32 precision. Moreover, the training cost is
    significantly low, only 1k steps of batch size 1 with around less than one-thousandth
    trainable parameters of the original models. Also, our method is orthogonal to
    current popular LLMs quantization methods, and better accuracy results could be
    achieved by combining ours with them.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提出的方法通过引入可训练的等效变换（见图 [1](#S3.F1 "Figure 1 ‣ 3 Methodology ‣ TEQ: Trainable
    Equivalent Transformation for Quantization of LLMs")），降低了压缩误差，同时保持模型输出在 FP32 精度下的数学等价性。此外，训练成本显著较低，仅需
    1k 步的批次大小为 1，相较于原始模型的可训练参数少于千分之一。我们的这种方法与当前流行的 LLM 量化方法是正交的，通过将我们的方法与它们结合，可以获得更好的准确性结果。'
- en: 'In summary, the contribution of this paper is threefold:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本文的贡献有三个方面：
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a trainable equivalent transformation for the quantization of LLMs,
    which keeps the model output unchanged at FP32 precision. Besides, the training
    is quite lightweight.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了一种用于 LLM 量化的可训练等效变换，这种变换在 FP32 精度下保持模型输出不变。此外，训练过程相当轻量。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Experimental results show our method could achieve results on par with or better
    than the SOTA methods.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验结果显示，我们的方法能够达到与当前最先进技术（SOTA）相当或更好的效果。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We also show that our method could be combined to get the new SOTA performance.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还展示了我们的方法可以与其他方法结合，从而获得新的 SOTA 性能。
- en: In the following, we first briefly introduce the work related to ours in Section
    2\. We then present the trainable equivalent transformation in Section 3\. Experiments
    and conclusion are described in Sections 4 and 5 respectively.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们首先在第 2 节简要介绍与我们工作相关的内容。然后在第 3 节中介绍可训练的等效变换。实验和结论分别在第 4 和第 5 节中描述。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Quantization-aware Training.
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化感知训练。
- en: QAT methods are widely used in model compression. By enabling finetuning process,
    quantized models’ accuracy can often be on par with or even better than those
    of original models. (Louizos et al., [2018](#bib.bib21)) introduce a differentiable
    quantization procedure by converting original weights and activations’ distribution
    to categorical distributions. OQAT (Shen et al., [2021](#bib.bib30)) proposes
    a combined training scheme of architecture and quantization to acquire many quantized
    models. Afterward, they are converted to lower-bit models and optimized. (Zhuang
    et al., [2021](#bib.bib38)) propose a progressive quantization scheme by quantizing
    activations after weights. Indeed, QAT methods are popular in relatively small-scale
    models, but their application in LLMs is limited due to the expensive training
    or even fine-tuning costs as mentioned in Section 1.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: QAT 方法广泛用于模型压缩。通过启用微调过程，量化模型的准确性通常可以与原始模型相当甚至更好。（Louizos 等，[2018](#bib.bib21)）通过将原始权重和激活的分布转换为分类分布来引入可微分的量化过程。**OQAT**（Shen
    等，[2021](#bib.bib30)）提出了一种架构和量化的联合训练方案，以获得多个量化模型。随后，这些模型被转换为更低位数的模型并进行优化。（Zhuang
    等，[2021](#bib.bib38)）提出了一种渐进量化方案，即在量化权重后量化激活。确实，QAT 方法在相对小规模模型中很受欢迎，但在大规模语言模型中的应用受限，因为如第
    1 节所述的训练甚至微调成本昂贵。
- en: Post-training Quantization.
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 后训练量化。
- en: A large number of post-training methods quantize weights step by step and modify
    unquantized weights to compensate for errors produced by previously quantized
    weights. Optimal Brain Damage (OBD) (LeCun et al., [1989](#bib.bib18)) uses second-derivative
    information (Hessian-based estimation) to predict the effect of weights’ perturbation
    analytically. Optimal Brain Surgeon (OBS) (Hassibi et al., [1993](#bib.bib12))
    applies such an idea by devising a second-order framework for weight pruning.
    Afterward, Optimal Brain Quantization (OBQ) migrate OBS’s pruning framework to
    quantization since pruning and quantization share the common idea of introducing
    perturbation in original models. Finally, GPTQ (Frantar et al., [2022](#bib.bib8))
    improves the original framework’s efficiency by fixing the quantization order
    within the layer and calculating the Hessian matrix’s Cholesky decomposition before
    quantization. Other PTQ methods use a better rounding scheme than commonly used
    rounding-to-nearest (RTN). AdaRound (Nagel et al., [2020](#bib.bib24)) learns
    a rounding scheme using mean squared error (MSE) for layer-wise activation. AQuant
    (Li et al., [2022](#bib.bib19)) adds a learnable border function for activation
    quantization.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 许多后训练方法通过逐步量化权重并修改未量化的权重来补偿之前量化权重产生的误差。**最优脑损伤**（OBD）（LeCun 等，[1989](#bib.bib18)）使用二阶导数信息（基于
    Hessian 的估计）来解析预测权重扰动的效果。**最优脑外科医生**（OBS）（Hassibi 等，[1993](#bib.bib12)）通过设计一个二阶框架来进行权重剪枝。之后，**最优脑量化**（OBQ）将
    OBS 的剪枝框架迁移到量化上，因为剪枝和量化都共享在原始模型中引入扰动的共同理念。最后，**GPTQ**（Frantar 等，[2022](#bib.bib8)）通过固定层内的量化顺序并在量化前计算
    Hessian 矩阵的 Cholesky 分解来提高原始框架的效率。其他 PTQ 方法使用比常用的四舍五入到最近（RTN）更好的四舍五入方案。**AdaRound**（Nagel
    等，[2020](#bib.bib24)）使用均方误差（MSE）为逐层激活学习四舍五入方案。**AQuant**（Li 等，[2022](#bib.bib19)）为激活量化添加了一个可学习的边界函数。
- en: Large Language Model Quantization.
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大型语言模型量化。
- en: Researchers are devoting efforts to compression methods particularly designed
    for LLMs as more open-source releases are available. LLM.int8() (Dettmers et al.,
    [2022](#bib.bib5)) discovers peak values in activation outliers’ particular channels.
    It proposes methods to ensure that these channels are kept in higher precision.
    SmoothQuant (Xiao et al., [2022](#bib.bib33)) addresses the issues mentioned above
    by migrating difficulties from activation to weights with a handcrafted equivalent
    transformation. ZeroQuant (Yao et al., [2022](#bib.bib34)) devises an end-to-end
    quantization and inference pipeline with a novel layer-wise knowledge distillation
    algorithm. However, the largest model it has quantized has only 1.3B parameters.
    GPTQ (Frantar et al., [2022](#bib.bib8)) tunes the weights based on optimal brain
    surgeon (Hassibi et al., [1993](#bib.bib12)) and successfully achieves low-bit
    quantization on LLMs with low computation overhead. More recent, AWQ (Lin et al.,
    [2023](#bib.bib20)) propose to search the optimal scales to protect parts of weights,
    since they can significantly reduce the error caused by quantization.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随着更多开源版本的发布，研究人员正在致力于特别为LLMs（大型语言模型）设计的压缩方法。**LLM.int8()**（Dettmers 等，[2022](#bib.bib5)）发现了激活异常值中特定通道的峰值，并提出了确保这些通道保持较高精度的方法。**SmoothQuant**（Xiao
    等，[2022](#bib.bib33)）通过将困难从激活迁移到权重，并使用手工设计的等效变换来解决上述问题。**ZeroQuant**（Yao 等，[2022](#bib.bib34)）设计了一个端到端量化和推理管道，采用了新颖的逐层知识蒸馏算法。然而，它所量化的最大模型只有1.3B参数。**GPTQ**（Frantar
    等，[2022](#bib.bib8)）基于**最佳脑外科医生**（Hassibi 等，[1993](#bib.bib12)）调整权重，并成功实现了在LLMs上的低位量化，计算开销低。最近，**AWQ**（Lin
    等，[2023](#bib.bib20)）提出了搜索最佳尺度来保护部分权重，因为它们可以显著减少量化引起的误差。
- en: 3 Methodology
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'Figure [1](#S3.F1 "Figure 1 ‣ 3 Methodology ‣ TEQ: Trainable Equivalent Transformation
    for Quantization of LLMs") presents a schematic illustration of equivalent transformation.
    In the following, we introduce the quantization process first. Consider a feed-forward
    neural network comprised of $L$ is the corresponding output. To quantize a tensor,
    a quantization op presented below could be applied.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S3.F1 "图 1 ‣ 3 方法论 ‣ TEQ：用于LLMs量化的可训练等效变换") 展示了等效变换的示意图。接下来，我们首先介绍量化过程。考虑一个由
    $L$ 层组成的前馈神经网络，其对应的输出。要量化一个张量，可以应用如下所示的量化操作。
- en: '|  | $Q(v)=clip(\left[\frac{v}{s}\right],-n,n),n\in\mathbb{N}$ |  | (1) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(v)=clip(\left[\frac{v}{s}\right],-n,n),n\in\mathbb{N}$ |  | (1) |'
- en: 'where $s$’s output after normal quantization is converted to:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s$ 在正常量化后的输出被转换为：
- en: '|  | $\hat{y_{l}}=Q^{-1}(Q(w_{l})\cdot Q(x_{l}))$ |  | (2) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y_{l}}=Q^{-1}(Q(w_{l})\cdot Q(x_{l}))$ |  | (2) |'
- en: where $\hat{y_{l}}$ is usually named as quantization loss.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{y_{l}}$ 通常被称为量化损失。
- en: '![Refer to caption](img/4e13a368922724ee01c05afb556f5298.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4e13a368922724ee01c05afb556f5298.png)'
- en: 'Figure 1: A schematic illustration of TEQ, where $s_{w1}$ are trainable parameters.
    A per-channel scale is multiplied at activations while an inverse scale is multiplied
    at weights, which could keep the output equivalent.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：**TEQ** 的示意图，其中 $s_{w1}$ 是可训练的参数。每个通道的尺度乘以激活，而反向尺度乘以权重，这样可以保持输出等效。
- en: 3.1 Trainable Equivalent Transformation
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 可训练等效变换
- en: PTQ tends to cause a noticeable accuracy drop as mentioned before. SmoothQuant
    (Xiao et al., [2022](#bib.bib33)) and AWQ (Lin et al., [2023](#bib.bib20)) rely
    on handcrafted rules to migrating quantization difficulties of weights and activations.
    However, these rules often fall into sub-optimal solutions, which cannot minimize
    error caused by quantization. To alleviate this issue, we introduce a trainable
    equivalent transformation that enforces the Fp32 output as the same but greatly
    improves the quantization robustness. To be more specific, suppose the shape of
    $w_{l}$ for weights and append a corresponding inverse scale vector for activation.
    Mathematically, this can be restated as
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，**PTQ**（后训练量化）往往会导致明显的准确性下降。**SmoothQuant**（Xiao 等，[2022](#bib.bib33)）和**AWQ**（Lin
    等，[2023](#bib.bib20)）依赖于手工规则来迁移权重和激活的量化困难。然而，这些规则往往陷入次优解，无法最小化量化引起的误差。为了解决这个问题，我们引入了一种可训练的等效变换，它强制**Fp32**输出保持不变，但大大提高了量化的鲁棒性。具体来说，假设权重的形状是
    $w_{l}$ 并附加一个对应的反向尺度向量用于激活。在数学上，这可以重新表述为
- en: '|  | $y_{l}=w_{l}\cdot diag(s_{l})\cdot diag(s_{l})^{-1}\cdot x_{l}$ |  | (3)
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{l}=w_{l}\cdot diag(s_{l})\cdot diag(s_{l})^{-1}\cdot x_{l}$ |  | (3)
    |'
- en: operator $diag(\cdot)$ denotes converting a column/row vector to a diagonal
    matrix whose eigenvalues are identical to the original vector’s elements.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 运算符 $diag(\cdot)$ 表示将列/行向量转换为对角矩阵，其特征值与原向量的元素相同。
- en: '|  | $$diag\left(\begin{bmatrix}s_{1}\\ s_{2}\\'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$diag\left(\begin{bmatrix}s_{1}\\ s_{2}\\'
- en: \vdots\\
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: s_{n}\end{bmatrix}\right)=\begin{bmatrix}s_{1}&amp;&amp;&amp;\\
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: s_{n}\end{bmatrix}\right)=\begin{bmatrix}s_{1}&amp;&amp;&amp;\\
- en: '&amp;s_{2}&amp;&amp;\\'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;s_{2}&amp;&amp;\\'
- en: '&amp;&amp;\ddots&amp;\\'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;&amp;\ddots&amp;\\'
- en: '&amp;&amp;&amp;s_{n}\end{bmatrix}$$ |  | (4) |'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;&amp;&amp;s_{n}\end{bmatrix}$$ |  | (4) |'
- en: Our observation shows the optimal $s_{w}$ is useful to reduce the quantization
    loss. Therefore, we quantize the transformed model rather than the original one.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的观察表明，最优的 $s_{w}$ 对减少量化损失是有效的。因此，我们对转化后的模型进行量化，而不是原始模型。
- en: '| n_bits | Methods | OPT-6.7B | OPT-13B | BLOOM-3B | BLOOM-7B1 | LLAMA-7B |
    LLAMA-13B |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| n_bits | 方法 | OPT-6.7B | OPT-13B | BLOOM-3B | BLOOM-7B1 | LLAMA-7B | LLAMA-13B
    |'
- en: '| 32 | FP32 | 64.97 | 65.54 | 55.65 | 60.29 | 68.87 | 71.06 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 32 | FP32 | 64.97 | 65.54 | 55.65 | 60.29 | 68.87 | 71.06 |'
- en: '| 4 | RTN | 62.99 | 64.17 | 53.17 | 57.80 | 67.41 | 68.86 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 4 | RTN | 62.99 | 64.17 | 53.17 | 57.80 | 67.41 | 68.86 |'
- en: '| GPTQ | 63.09 | 64.83 | 54.65 | 58.26 | 64.70 | 70.00 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 63.09 | 64.83 | 54.65 | 58.26 | 64.70 | 70.00 |'
- en: '| Ours | 63.30 | 64.91 | 53.83 | 58.93 | 67.71 | 69.55 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 63.30 | 64.91 | 53.83 | 58.93 | 67.71 | 69.55 |'
- en: '| Ours+GPTQ | 63.94 | 65.03 | 54.42 | 59.62 | 65.27 | 69.73 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Ours+GPTQ | 63.94 | 65.03 | 54.42 | 59.62 | 65.27 | 69.73 |'
- en: '| 4_g128 | RTN | 64.04 | 64.88 | 54.91 | 59.32 | 67.87 | 70.88 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 4_g128 | RTN | 64.04 | 64.88 | 54.91 | 59.32 | 67.87 | 70.88 |'
- en: '| GPTQ | 64.76 | 65.37 | 55.68 | 59.59 | 66.33 | 70.92 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 64.76 | 65.37 | 55.68 | 59.59 | 66.33 | 70.92 |'
- en: '| Ours | 64.11 | 64.87 | 54.98 | 59.35 | 68.10 | 71.00 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 64.11 | 64.87 | 54.98 | 59.35 | 68.10 | 71.00 |'
- en: '| Ours+GPTQ | 64.77 | 65.20 | 55.49 | 59.60 | 66.56 | 70.96 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Ours+GPTQ | 64.77 | 65.20 | 55.49 | 59.60 | 66.56 | 70.96 |'
- en: 'Table 1: The w4 average accuracy($\uparrow$) of four tasks, e.g., HellaSwag,
    WinoGrande, PIQA, and LAMBADA, in LM-eval. g denotes group size. "Ours+GPTQ" means
    we apply TEQ first and then apply GPTQ afterward. For LLAMA-7B, the result of
    GPTQ is w/o act-order. Results of act-order are shown in Appendix [A.2](#A1.SS2
    "A.2 Additional comparison with GPTQ act-order ‣ Appendix A Appendix ‣ TEQ: Trainable
    Equivalent Transformation for Quantization of LLMs").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1：LM-eval 中四个任务（例如 HellaSwag、WinoGrande、PIQA 和 LAMBADA）的 w4 平均准确率($\uparrow$)。g
    表示组大小。"Ours+GPTQ" 表示我们首先应用 TEQ，然后再应用 GPTQ。对于 LLAMA-7B，GPTQ 的结果为无 act-order。act-order
    的结果见附录 [A.2](#A1.SS2 "A.2 Additional comparison with GPTQ act-order ‣ Appendix
    A Appendix ‣ TEQ: Trainable Equivalent Transformation for Quantization of LLMs")。'
- en: '| n_bits | Methods | OPT-6.7B | OPT-13B | BLOOM-3B | BLOOM-7B1 | LLAMA-7B |
    LLAMA-13B |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| n_bits | 方法 | OPT-6.7B | OPT-13B | BLOOM-3B | BLOOM-7B1 | LLAMA-7B | LLAMA-13B
    |'
- en: '| 32 | FP32 | 10.86 | 10.12 | 13.48 | 11.36 | 5.68 | 5.09 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 32 | FP32 | 10.86 | 10.12 | 13.48 | 11.36 | 5.68 | 5.09 |'
- en: '| 4 | RTN | 12.10 | 11.32 | 14.75 | 12.09 | 6.29 | 5.53 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 4 | RTN | 12.10 | 11.32 | 14.75 | 12.09 | 6.29 | 5.53 |'
- en: '| GPTQ | 11.59 | 10.33 | 14.10 | 11.73 | 6.59 | 5.33 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 11.59 | 10.33 | 14.10 | 11.73 | 6.59 | 5.33 |'
- en: '| Ours | 11.68 | 10.59 | 14.72 | 12.21 | 6.30 | 5.50 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 11.68 | 10.59 | 14.72 | 12.21 | 6.30 | 5.50 |'
- en: '| Ours+GPTQ | 11.29 | 10.36 | 14.03 | 11.74 | 6.76 | 5.35 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Ours+GPTQ | 11.29 | 10.36 | 14.03 | 11.74 | 6.76 | 5.35 |'
- en: '| 4_g128 | RTN | 11.16 | 10.32 | 13.85 | 11.60 | 5.97 | 5.26 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 4_g128 | RTN | 11.16 | 10.32 | 13.85 | 11.60 | 5.97 | 5.26 |'
- en: '| GPTQ | 10.98 | 10.20 | 13.69 | 11.48 | 6.29 | 5.21 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 10.98 | 10.20 | 13.69 | 11.48 | 6.29 | 5.21 |'
- en: '| Ours | 11.11 | 10.28 | 13.82 | 11.58 | 5.97 | 5.26 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 11.11 | 10.28 | 13.82 | 11.58 | 5.97 | 5.26 |'
- en: '| Ours+GPTQ | 11.02 | 10.21 | 13.69 | 11.48 | 6.28 | 5.21 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Ours+GPTQ | 11.02 | 10.21 | 13.69 | 11.48 | 6.28 | 5.21 |'
- en: 'Table 2: The w4 perplexity($\downarrow$) on WikiText-2\. For LLAMA-7B, the
    result of GPTQ is w/o act-order. Results of act-order are shown in Appendix [A.2](#A1.SS2
    "A.2 Additional comparison with GPTQ act-order ‣ Appendix A Appendix ‣ TEQ: Trainable
    Equivalent Transformation for Quantization of LLMs").'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2：WikiText-2 上的 w4 困惑度($\downarrow$)。对于 LLAMA-7B，GPTQ 的结果为无 act-order。act-order
    的结果见附录 [A.2](#A1.SS2 "A.2 Additional comparison with GPTQ act-order ‣ Appendix
    A Appendix ‣ TEQ: Trainable Equivalent Transformation for Quantization of LLMs")。'
- en: The transformation has two per-channel scale operations, which will introduce
    computation overhead. We fuse the weight scale to the weight itself. For the activation
    scale, following (Xiao et al., [2022](#bib.bib33)), we fuse it to the previous
    layers, such as layernorm(Ba et al., [2016](#bib.bib1)), batchnorm(Ioffe and Szegedy,
    [2015](#bib.bib16)) and etc. In all our experiments, we only apply the transformation
    to the layer whose scales could be fused, which introduces no extra overhead at
    deployment.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 转换具有两个每通道缩放操作，这将引入计算开销。我们将权重缩放融合到权重本身。对于激活缩放，按照（Xiao 等，[2022](#bib.bib33)），我们将其融合到前面的层，如
    layernorm（Ba 等，[2016](#bib.bib1)），batchnorm（Ioffe 和 Szegedy，[2015](#bib.bib16)）等。在所有实验中，我们仅将转换应用于那些缩放可以融合的层，这样在部署时不会引入额外的开销。
- en: 3.2 Training Details
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 训练细节
- en: We train the scales $s_{l}$ because there is little knowledge of the best equivalent
    transformation due to various models and quantization configurations. It’s worth
    mentioning that the count of trainable scales is much less than the model’s parameters,
    and the model weights are frozen.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练缩放 $s_{l}$ 是因为由于各种模型和量化配置，最佳等效转换的知识较少。值得一提的是，可训练缩放的数量远低于模型参数数量，并且模型权重保持冻结状态。
- en: To train the transformation scales, we follow the basic QAT to simulate the
    quantization behavior, which could be denoted as
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练转换缩放，我们遵循基本的 QAT 来模拟量化行为，表示为
- en: '|  | $y_{l_{q}}=(Q^{-1}Q(w_{l}))(Q^{-1}Q(x_{l}))$ |  | (5) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{l_{q}}=(Q^{-1}Q(w_{l}))(Q^{-1}Q(x_{l}))$ |  | (5) |'
- en: For weight-only quantization, activation quantization will be ignored. We adopt
    straight-through estimator (STE) (Bengio et al., [2013](#bib.bib3)) to backward
    the gradients.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于仅权重量化，激活量化将被忽略。我们采用直通估计器（STE）（Bengio 等，[2013](#bib.bib3)）来反向传播梯度。
- en: We use Adam(Kingma and Ba, [2014](#bib.bib17)) optimizer, betas [0.9, 0.9],
    and weight decay 0\. The learning rate is 1e-3 unless explicitly stated and the
    decay type is linear. We only train 1000 steps. We use the same loss function
    as the original one in the training phase. For example, CrossEntorpy loss is adopted
    for LLMs. The $s_{l}$ leads to better results, so we pick the better one in our
    experiments.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Adam（Kingma 和 Ba，[2014](#bib.bib17)）优化器，betas [0.9, 0.9]，权重衰减 0。学习率为 1e-3，除非另有说明，衰减类型为线性。我们只训练
    1000 步。在训练阶段我们使用与原始方法相同的损失函数。例如，LLMs 采用 CrossEntorpy 损失。$s_{l}$ 导致了更好的结果，因此我们在实验中选择了更好的那个。
- en: 4 Experiments
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this section, we evaluate our proposed TEQ’s in different aspects. Initially,
    we briefly introduce LLM architectures and tasks included in our evaluation. Secondly,
    we illustrate a detailed comparison of our method and other state-of-the-art (SOTA)
    methods, and both quantization accuracy and time are considered.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们从不同方面评估我们提出的 TEQ。首先，我们简要介绍了包含在评估中的 LLM 架构和任务。其次，我们详细比较了我们的方法和其他最先进（SOTA）方法，考虑了量化精度和时间。
- en: 4.1 Experimental Settings
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Large Language Models.
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大型语言模型。
- en: We conduct our experiments on the most popular LLM architectures, including
    LLaMAs (Touvron et al., [2023](#bib.bib31)), BLOOMs (Scao et al., [2022](#bib.bib29)),
    and OPTs (Zhang et al., [2022](#bib.bib36)). Parameter scalings ranging from million
    to billion are all included.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在最受欢迎的 LLM 架构上进行实验，包括 LLaMAs（Touvron 等，[2023](#bib.bib31)），BLOOMs（Scao 等，[2022](#bib.bib29)），和
    OPTs（Zhang 等，[2022](#bib.bib36)）。包括从百万到十亿的参数缩放。
- en: '| n_bits | Methods | OPT-6.7B | OPT-13B | BLOOM-3B | BLOOM-7B1 | LLAMA-7B |
    LLAMA-13B |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| n_bits | 方法 | OPT-6.7B | OPT-13B | BLOOM-3B | BLOOM-7B1 | LLAMA-7B | LLAMA-13B
    |'
- en: '| 32 | FP32 | 64.97 | 65.54 | 55.65 | 60.29 | 68.87 | 71.06 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 32 | FP32 | 64.97 | 65.54 | 55.65 | 60.29 | 68.87 | 71.06 |'
- en: '| 3_g128 | RTN | 56.03 | 49.59 | 52.54 | 57.53 | 64.92 | 67.68 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 3_g128 | RTN | 56.03 | 49.59 | 52.54 | 57.53 | 64.92 | 67.68 |'
- en: '| GPTQ | 62.98 | 64.68 | 53.41 | 58.12 | 58.29 | 68.73 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 62.98 | 64.68 | 53.41 | 58.12 | 58.29 | 68.73 |'
- en: '| Ours | 61.41 | 63.27 | 52.69 | 57.79 | 65.25 | 68.32 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 61.41 | 63.27 | 52.69 | 57.79 | 65.25 | 68.32 |'
- en: '| Ours+GPTQ | 63.16 | 64.60 | 53.71 | 58.00 | 59.27 | 69.15 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Ours+GPTQ | 63.16 | 64.60 | 53.71 | 58.00 | 59.27 | 69.15 |'
- en: 'Table 3: The 3 bits with group size 128 average accuracy($\uparrow$) of four
    tasks,e.g., HellaSwag, WinoGrande, PIQA, and LAMBADA, in LM-eval. g denotes group
    size. For LLAMA-7B, the result of GPTQ is w/o act-order. Results of act-order
    are shown in Appendix [A.2](#A1.SS2 "A.2 Additional comparison with GPTQ act-order
    ‣ Appendix A Appendix ‣ TEQ: Trainable Equivalent Transformation for Quantization
    of LLMs").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3：在 LM-eval 中，使用组大小 128 的 3 位量化的四个任务（例如 HellaSwag、WinoGrande、PIQA 和 LAMBADA）的平均准确率（$\uparrow$）。g
    表示组大小。对于 LLAMA-7B，GPTQ 的结果未包含 act-order。act-order 的结果见附录 [A.2](#A1.SS2 "A.2 Additional
    comparison with GPTQ act-order ‣ Appendix A Appendix ‣ TEQ: Trainable Equivalent
    Transformation for Quantization of LLMs")。'
- en: '| n_bits | Methods | OPT-6.7B | OPT-13B | BLOOM-3B | BLOOM-7B1 | LLAMA-7B |
    LLAMA-13B |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| n_bits | 方法 | OPT-6.7B | OPT-13B | BLOOM-3B | BLOOM-7B1 | LLAMA-7B | LLAMA-13B
    |'
- en: '| 32 | FP32 | 10.86 | 10.12 | 13.48 | 11.36 | 5.68 | 5.09 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 32 | FP32 | 10.86 | 10.12 | 13.48 | 11.36 | 5.68 | 5.09 |'
- en: '| 3_g128 | RTN | 22.37 | 40.50 | 15.68 | 12.47 | 7.01 | 5.88 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 3_g128 | RTN | 22.37 | 40.50 | 15.68 | 12.47 | 7.01 | 5.88 |'
- en: '| GPTQ | 11.42 | 10.51 | 14.67 | 11.99 | 8.28 | 5.64 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 11.42 | 10.51 | 14.67 | 11.99 | 8.28 | 5.64 |'
- en: '| Ours | 12.03 | 11.83 | 15.48 | 12.40 | 6.89 | 5.81 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 12.03 | 11.83 | 15.48 | 12.40 | 6.89 | 5.81 |'
- en: '| Ours+GPTQ | 11.40 | 10.52 | 14.64 | 11.98 | 7.71 | 5.64 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Ours+GPTQ | 11.40 | 10.52 | 14.64 | 11.98 | 7.71 | 5.64 |'
- en: 'Table 4: WikiText-2 perplexity($\downarrow$) of 3 bits with group size 128\.
    For LLAMA-7B, the result of GPTQ is w/o act-order. Results of act-order are shown
    in Appendix [A.2](#A1.SS2 "A.2 Additional comparison with GPTQ act-order ‣ Appendix
    A Appendix ‣ TEQ: Trainable Equivalent Transformation for Quantization of LLMs").'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4：使用 128 的组大小的 3 位量化的 WikiText-2 困惑度（$\downarrow$）。对于 LLAMA-7B，GPTQ 的结果未包含
    act-order。act-order 的结果见附录 [A.2](#A1.SS2 "A.2 Additional comparison with GPTQ
    act-order ‣ Appendix A Appendix ‣ TEQ: Trainable Equivalent Transformation for
    Quantization of LLMs")。'
- en: Evaluation and Datasets.
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估与数据集。
- en: We make assessments on several language tasks to satisfy the task-agnostic setting.
    Specifically, we report average accuracy result on four common sense reasoning
    tasks by leveraging lm-eval-harness(Gao et al., [2021](#bib.bib9)), including
    HellaSwag (Zellers et al., [2019](#bib.bib35)), WinoGrande (Sakaguchi et al.,
    [2021](#bib.bib28)), PIQA (Bisk et al., [2020](#bib.bib4)) and LAMBADA (Paperno
    et al., [2016](#bib.bib26)). Furthermore, we complement our evaluation with perplexity
    (PPL) analysis on WikiText2 (Merity et al., [2016](#bib.bib23)), PTB (Marcus et al.,
    [1994](#bib.bib22)) as well as C4 (Raffel et al., [2020](#bib.bib27)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对几个语言任务进行了评估，以满足任务无关的设置。具体而言，我们利用 lm-eval-harness（Gao 等，[2021](#bib.bib9)）报告了四个常识推理任务的平均准确率结果，包括
    HellaSwag（Zellers 等，[2019](#bib.bib35)）、WinoGrande（Sakaguchi 等，[2021](#bib.bib28)）、PIQA（Bisk
    等，[2020](#bib.bib4)）和 LAMBADA（Paperno 等，[2016](#bib.bib26)）。此外，我们通过对 WikiText2（Merity
    等，[2016](#bib.bib23)）、PTB（Marcus 等，[1994](#bib.bib22)）以及 C4（Raffel 等，[2020](#bib.bib27)）的困惑度（PPL）分析来补充我们的评估。
- en: Implementation Details.
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现细节。
- en: Following GPTQ (Frantar et al., [2022](#bib.bib8)), we focus on weight-only
    quantization and exclude the last layer When quantifying. We used a single HW
    accelerator to quantize models with a scale of around ten billion parameters.
    We use the same calibration dataset pile-10k¹¹1https://huggingface.co/datasets/NeelNanda/pile-10k
    for a fair comparison.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 GPTQ（Frantar 等，[2022](#bib.bib8)），我们专注于仅对权重进行量化，并在量化时排除最后一层。我们使用了一个 HW 加速器来量化约十亿参数规模的模型。为了公平比较，我们使用了相同的校准数据集
    pile-10k¹¹1https://huggingface.co/datasets/NeelNanda/pile-10k。
- en: Baseline.
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线。
- en: Our primary baseline is vanilla round-to-nearest quantization (RTN) which has
    a remarkable result at 4bits using a small group size of 128\. We also compare
    with a state-of-the-art method GPTQ (Frantar et al., [2022](#bib.bib8)).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要基线是普通的四舍五入到最近量化（RTN），它在使用 128 的小组大小时，4 位精度的结果显著。我们还与最先进的方法 GPTQ（Frantar
    等，[2022](#bib.bib8)）进行了比较。
- en: 4.2 Results
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果
- en: As mentioned above, we compare our results with RTN and the SOTA GTPQ(Frantar
    et al., [2022](#bib.bib8)). Also, since our method is orthogonal to GPTQ, we report
    Ours+GPTQ as well, which applies TEQ first and then runs GPTQ official code²²2https://github.com/IST-DASLab/gptq
    afterward. We mainly focus on the models around 10B which is commonly used.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，我们将我们的结果与 RTN 和最先进的 GPTQ（Frantar 等，[2022](#bib.bib8)）进行了比较。此外，由于我们的方法与
    GPTQ 正交，我们还报告了 Ours+GPTQ，这适用于首先应用 TEQ，然后运行 GPTQ 官方代码²²2https://github.com/IST-DASLab/gptq。我们主要关注于常用的约
    10B 的模型。
- en: W4 Quantization.
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: W4 量化。
- en: 'We first evaluate TEQ on popular 4 bits quantization. Table [1](#S3.T1 "Table
    1 ‣ 3.1 Trainable Equivalent Transformation ‣ 3 Methodology ‣ TEQ: Trainable Equivalent
    Transformation for Quantization of LLMs") shows the lm-eval results of different
    LLM model architectures and parameter sizes. TEQ outperforms RTN in all cases
    except one. Comparing with GPTQ, TEQ shows better results in 6 out of 12 scenarios.
    After combining GPTQ, new state-of-the-art results could be achieved in 5 scenarios.
    In summary, TEQ could be helpful in 8 out of 12 scenarios. Table [8](#A1.T8 "Table
    8 ‣ A.3 Special hyperparameters and settings ‣ Appendix A Appendix ‣ TEQ: Trainable
    Equivalent Transformation for Quantization of LLMs") shows the hyper-parameters
    that we used in the experiements.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先在流行的4位量化中评估TEQ。表格[1](#S3.T1 "Table 1 ‣ 3.1 Trainable Equivalent Transformation
    ‣ 3 Methodology ‣ TEQ: Trainable Equivalent Transformation for Quantization of
    LLMs")展示了不同LLM模型架构和参数大小的lm-eval结果。除了一个情况外，TEQ在所有情况下都优于RTN。与GPTQ相比，TEQ在12种场景中的6种场景中表现更好。通过结合GPTQ，可以在5种场景中取得新的最先进结果。总之，TEQ在12种场景中的8种场景中可能会有所帮助。表格[8](#A1.T8
    "Table 8 ‣ A.3 Special hyperparameters and settings ‣ Appendix A Appendix ‣ TEQ:
    Trainable Equivalent Transformation for Quantization of LLMs")显示了我们在实验中使用的超参数。'
- en: 'We also evaluate WikiText2 ppl in table [2](#S3.T2 "Table 2 ‣ 3.1 Trainable
    Equivalent Transformation ‣ 3 Methodology ‣ TEQ: Trainable Equivalent Transformation
    for Quantization of LLMs") w/o group size and group size 128\. TEQ is better or
    on par with RTN. Similarly, the combined approach (Ours and GPTQ) shows comparable
    or better results than standalone GPTQ.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还在表格[2](#S3.T2 "Table 2 ‣ 3.1 Trainable Equivalent Transformation ‣ 3 Methodology
    ‣ TEQ: Trainable Equivalent Transformation for Quantization of LLMs")中评估了WikiText2
    ppl，分别为无组大小和组大小128。TEQ的表现与RTN相当或更好。类似地，组合方法（我们的和GPTQ）显示出与独立GPTQ相当或更好的结果。'
- en: W3 Quantization.
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: W3量化。
- en: 'We also evaluate TEQ at weight with 3 bits. We only consider group size 128,
    because the performance drops a lot without group size and usually could not be
    deployed in practice. Similar to 4 bits evaluation, we report the lm-eval result
    and wikitext2 ppl result in table [3](#S4.T3 "Table 3 ‣ Large Language Models.
    ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ TEQ: Trainable Equivalent Transformation
    for Quantization of LLMs") and [4](#S4.T4 "Table 4 ‣ Large Language Models. ‣
    4.1 Experimental Settings ‣ 4 Experiments ‣ TEQ: Trainable Equivalent Transformation
    for Quantization of LLMs") respectively. TEQ outperforms RTN in all scenarios
    and is inferior to GPTQ on certain models. However, TEQ could bring improvement
    for 8 out of 12 scenarios if taking Ours+GPTQ into account.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还在3位权重量化下评估了TEQ。我们仅考虑组大小128，因为没有组大小时性能大幅下降，通常无法在实践中部署。类似于4位评估，我们在表格[3](#S4.T3
    "Table 3 ‣ Large Language Models. ‣ 4.1 Experimental Settings ‣ 4 Experiments
    ‣ TEQ: Trainable Equivalent Transformation for Quantization of LLMs")和[4](#S4.T4
    "Table 4 ‣ Large Language Models. ‣ 4.1 Experimental Settings ‣ 4 Experiments
    ‣ TEQ: Trainable Equivalent Transformation for Quantization of LLMs")中分别报告了lm-eval结果和wikitext2
    ppl结果。TEQ在所有场景中优于RTN，在某些模型上不如GPTQ。然而，如果考虑Ours+GPTQ，TEQ可以在12种场景中的8种场景中带来改进。'
- en: Quantization Time.
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化时间。
- en: 'We report the quantization time in Table [5](#S4.T5 "Table 5 ‣ Quantization
    Time. ‣ 4.2 Results ‣ 4 Experiments ‣ TEQ: Trainable Equivalent Transformation
    for Quantization of LLMs"). We adopt Deepspeed³³3https://github.com/microsoft/DeepSpeed
    for 10B+ models due to the potential out-of-memory (OOM) issue. As TEQ needs training,
    our time cost is reasonably higher than GPTQ, especially when the model does not
    fit into the device memory. It’s possible to reduce the time further by using
    more resources or optimizing the code, while it’s out of scope.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格[5](#S4.T5 "Table 5 ‣ Quantization Time. ‣ 4.2 Results ‣ 4 Experiments
    ‣ TEQ: Trainable Equivalent Transformation for Quantization of LLMs")中报告了量化时间。由于可能的内存不足（OOM）问题，我们对10B+模型采用了Deepspeed³³3https://github.com/microsoft/DeepSpeed。由于TEQ需要训练，我们的时间成本比GPTQ高，特别是在模型无法适配设备内存时。通过使用更多资源或优化代码，可以进一步减少时间，但这超出了讨论范围。'
- en: '| Models | GPTQ | Ours |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | GPTQ | 我们的方法 |'
- en: '| OPT-6.7B | 841 | 1239 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7B | 841 | 1239 |'
- en: '| OPT-13B | 1523 | 8737* |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13B | 1523 | 8737* |'
- en: '| BLOOM-3B | 345 | 506 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3B | 345 | 506 |'
- en: '| BLOOM-7B1 | 661 | 1148 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7B1 | 661 | 1148 |'
- en: '| LLAMA-7B | 712 | 1249 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| LLAMA-7B | 712 | 1249 |'
- en: '| LLAMA-13B | 1240 | 9501* |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| LLAMA-13B | 1240 | 9501* |'
- en: 'Table 5: Quantization time in seconds for 4-bit weight quantization. * denotes
    DeepSpeed is adopted in training for 10B+ models.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表格5：4位权重量化的秒级量化时间。*表示对10B+模型采用了DeepSpeed进行训练。
- en: Analysis of Scales in TEQ.
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TEQ中的尺度分析。
- en: 'We visualize the magnitude distribution histograms of $s_{l}$ in Appendix [A.5](#A1.SS5
    "A.5 Counts of trainable parameters introduced by TEQ ‣ Appendix A Appendix ‣
    TEQ: Trainable Equivalent Transformation for Quantization of LLMs").'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在附录 [A.5](#A1.SS5 "A.5 TEQ 引入的可训练参数的计数 ‣ 附录 A 附录 ‣ TEQ: 用于 LLMs 量化的可训练等效转换")
    中可视化了 $s_{l}$ 的幅度分布直方图。'
- en: '![Refer to caption](img/3444ccdd4727df09869018c3e6bc1265.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3444ccdd4727df09869018c3e6bc1265.png)'
- en: 'Figure 2: The magnitude distributions of scales in TEQ for BLOOM-3B, BLOOM-7.1B,
    OPT-6.7B, LLAMA-7B. The quantization configurations are w3_g128, w4_g128, w4,
    and w4 respectively. Different colors refer to layer indices in models (blue stands
    for shallow layers which are close to the data layer, while red stands for deeper
    layers).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: TEQ 对 BLOOM-3B、BLOOM-7.1B、OPT-6.7B、LLAMA-7B 的尺度的幅度分布。量化配置分别为 w3_g128、w4_g128、w4
    和 w4。不同的颜色表示模型中的层索引（蓝色代表接近数据层的浅层，红色代表更深的层）。'
- en: 5 Conclusion
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we propose TEQ, a trainable equivalent transformation that preserves
    the FP32 precision of the model output while also taking advantage of low-precision
    quantization, and its training process is lightweight. Plus, TEQ is regarded as
    orthogonal support for other quantization methods to improve their performance.
    Our task-agnostic experiments and comparison with other methods show that TEQ
    or its combination with other methods can obtain comparable or better results.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了 TEQ，一种可训练的等效转换，既保留了模型输出的 FP32 精度，又利用了低精度量化，其训练过程轻量级。此外，TEQ 被认为是其他量化方法的正交支持，以提升其性能。我们的任务无关实验和与其他方法的比较表明，TEQ
    或其与其他方法的结合可以获得可比或更好的结果。
- en: 5.1 Limitations
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 局限性
- en: We find that the required memory during training is still high, though the number
    of training parameters remains low. Moreover, since we enforce the transformation
    to be equivalent and keep the architecture and FP32 output unchanged, our results
    in some scenarios are inferior to the SOTA methods, which could be fixed by combining
    the SOTA methods.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现训练期间所需的内存仍然很高，尽管训练参数的数量保持较低。此外，由于我们强制转换保持等效，并保持架构和 FP32 输出不变，因此在某些场景下，我们的结果不如
    SOTA 方法，这可以通过结合 SOTA 方法来修复。
- en: 5.2 Ethics Statement
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 伦理声明
- en: We propose TEQ for LLMs quantization. The method can be either used individually
    or combined with other quantization methods. Since TEQ only requires a few steps
    of finetuning on original models. Thus, it is safe to say that TEQ’s technical
    details have no significant ethical implications. Our work provides an exploration
    of large language model quantization through simple finetuning, making their application
    easier. We believe increasingly more work like this will emerge, making LLMs’
    quantization more powerful.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了用于 LLMs 量化的 TEQ 方法。该方法可以单独使用，也可以与其他量化方法结合使用。由于 TEQ 只需对原始模型进行几步微调，因此可以说
    TEQ 的技术细节没有显著的伦理影响。我们的工作通过简单的微调探索了大语言模型的量化，使其应用变得更加容易。我们相信，将会有越来越多类似的工作出现，使 LLMs
    的量化变得更加强大。
- en: References
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ba et al. (2016) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016.
    Layer normalization. *arXiv preprint arXiv:1607.06450*.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ba 等（2016）Jimmy Lei Ba, Jamie Ryan Kiros, 和 Geoffrey E Hinton. 2016. 层归一化。*arXiv
    预印本 arXiv:1607.06450*。
- en: 'Bai et al. (2020) Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin
    Jiang, Qun Liu, Michael Lyu, and Irwin King. 2020. Binarybert: Pushing the limit
    of bert quantization. *arXiv preprint arXiv:2012.15701*.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai 等（2020）Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang,
    Qun Liu, Michael Lyu, 和 Irwin King. 2020. Binarybert: 推动 BERT 量化的极限。*arXiv 预印本
    arXiv:2012.15701*。'
- en: Bengio et al. (2013) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013.
    Estimating or propagating gradients through stochastic neurons for conditional
    computation. *arXiv preprint arXiv:1308.3432*.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等（2013）Yoshua Bengio, Nicholas Léonard, 和 Aaron Courville. 2013. 通过随机神经元估计或传播梯度以进行条件计算。*arXiv
    预印本 arXiv:1308.3432*。
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    2020. Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk 等（2020）Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi 等. 2020.
    Piqa: 在自然语言中推理物理常识。*AAAI 人工智能会议论文集*，第34卷，第7432–7439页。'
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada 和 Luke Zettlemoyer.
    2022. Llm. int8 (): 适用于大规模变换器的8位矩阵乘法。*arXiv预印本 arXiv:2208.07339*。'
- en: Esser et al. (2019) Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar
    Appuswamy, and Dharmendra S Modha. 2019. Learned step size quantization. *arXiv
    preprint arXiv:1902.08153*.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esser et al. (2019) Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar
    Appuswamy 和 Dharmendra S Modha. 2019. 学习步长量化。*arXiv预印本 arXiv:1902.08153*。
- en: 'Frantar and Alistarh (2022) Elias Frantar and Dan Alistarh. 2022. Optimal brain
    compression: A framework for accurate post-training quantization and pruning.
    *arXiv preprint arXiv:2208.11580*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar and Alistarh (2022) Elias Frantar 和 Dan Alistarh. 2022. 最优脑压缩：准确后训练量化和剪枝的框架。*arXiv预印本
    arXiv:2208.11580*。
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh.
    2022. Gptq：生成预训练变换器的准确后训练量化。*arXiv预印本 arXiv:2210.17323*。
- en: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. 2021. [A framework for few-shot language model evaluation](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang 和 Andy
    Zou. 2021. [一个用于少样本语言模型评估的框架](https://doi.org/10.5281/zenodo.5371628)。
- en: '(10) Github. [Github: Copilot](https://github.com/features/copilot/).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(10) Github. [Github: Copilot](https://github.com/features/copilot/).'
- en: 'Gong et al. (2019) Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li,
    Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. 2019. Differentiable soft quantization:
    Bridging full-precision and low-bit neural networks. In *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, pages 4852–4861.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong et al. (2019) Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li,
    Peng Hu, Jiazhen Lin, Fengwei Yu 和 Junjie Yan. 2019. 可微分软量化：桥接全精度与低位神经网络。在 *IEEE/CVF国际计算机视觉会议*
    上，页面 4852–4861。
- en: Hassibi et al. (1993) Babak Hassibi, David G Stork, and Gregory J Wolff. 1993.
    Optimal brain surgeon and general network pruning. In *IEEE international conference
    on neural networks*, pages 293–299\. IEEE.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi et al. (1993) Babak Hassibi, David G Stork 和 Gregory J Wolff. 1993.
    最优脑外科医生与通用网络剪枝。在 *IEEE国际神经网络会议* 上，页面 293–299。IEEE。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pages 770–778.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren 和 Jian Sun. 2016. 图像识别的深度残差学习。在
    *IEEE计算机视觉与模式识别会议* 上，页面 770–778。
- en: 'Howard et al. (2017) Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko,
    Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets:
    Efficient convolutional neural networks for mobile vision applications. *arXiv
    preprint arXiv:1704.04861*.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard et al. (2017) Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko,
    Weijun Wang, Tobias Weyand, Marco Andreetto 和 Hartwig Adam. 2017. Mobilenets：用于移动视觉应用的高效卷积神经网络。*arXiv预印本
    arXiv:1704.04861*。
- en: Hubara et al. (2021) Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and
    Daniel Soudry. 2021. Accurate post training quantization with small calibration
    sets. In *International Conference on Machine Learning*, pages 4466–4475\. PMLR.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara et al. (2021) Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner 和 Daniel
    Soudry. 2021. 使用小型校准集的准确后训练量化。在 *国际机器学习会议* 上，页面 4466–4475。PMLR。
- en: 'Ioffe and Szegedy (2015) Sergey Ioffe and Christian Szegedy. 2015. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. In *International
    conference on machine learning*, pages 448–456\. pmlr.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe and Szegedy (2015) Sergey Ioffe 和 Christian Szegedy. 2015. 批量归一化：通过减少内部协变量偏移加速深度网络训练。在
    *国际机器学习会议* 上，页面 448–456。pmlr。
- en: 'Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for
    stochastic optimization. *arXiv preprint arXiv:1412.6980*.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma and Ba (2014) Diederik P Kingma 和 Jimmy Ba. 2014. Adam：一种随机优化方法。*arXiv预印本
    arXiv:1412.6980*。
- en: LeCun et al. (1989) Yann LeCun, John Denker, and Sara Solla. 1989. Optimal brain
    damage. *Advances in neural information processing systems*, 2.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1989) Yann LeCun, John Denker 和 Sara Solla. 1989. 最优脑损伤。*神经信息处理系统进展*，2。
- en: Li et al. (2022) Zhengyi Li, Cong Guo, Zhanda Zhu, Yangjie Zhou, Yuxian Qiu,
    Xiaotian Gao, Jingwen Leng, and Minyi Guo. 2022. Efficient activation quantization
    via adaptive rounding border for post-training quantization. *arXiv preprint arXiv:2208.11945*.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022) Zhengyi Li, Cong Guo, Zhanda Zhu, Yangjie Zhou, Yuxian Qiu,
    Xiaotian Gao, Jingwen Leng, and Minyi Guo. 2022. 通过自适应舍入边界实现高效激活量化。*arXiv预印本 arXiv:2208.11945*。
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. 2023. Awq: Activation-aware weight quantization for llm compression
    and acceleration. *arXiv*.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. 2023. Awq: 激活感知权重量化用于大语言模型压缩和加速。*arXiv*。'
- en: Louizos et al. (2018) Christos Louizos, Matthias Reisser, Tijmen Blankevoort,
    Efstratios Gavves, and Max Welling. 2018. Relaxed quantization for discretized
    neural networks. *arXiv preprint arXiv:1810.01875*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Louizos et al. (2018) Christos Louizos, Matthias Reisser, Tijmen Blankevoort,
    Efstratios Gavves, and Max Welling. 2018. 放松量化用于离散神经网络。*arXiv预印本 arXiv:1810.01875*。
- en: 'Marcus et al. (1994) Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert
    MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994.
    The penn treebank: Annotating predicate argument structure. In *Human Language
    Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11,
    1994*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Marcus et al. (1994) Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert
    MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994.
    Penn Treebank: 注释谓词论元结构。见于*人类语言技术：在新泽西州Plainsboro于1994年3月8-11日举行的研讨会论文集*。'
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. 2016. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. 2016. 指针哨兵混合模型。*arXiv预印本 arXiv:1609.07843*。
- en: Nagel et al. (2020) Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. 2020. Up or down? adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning*, pages 7197–7206\.
    PMLR.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel et al. (2020) Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. 2020. 向上还是向下？后训练量化的自适应舍入。见于*国际机器学习会议*，页码7197–7206。PMLR。
- en: '(25) OpenAI. [Openai: Chatgpt](https://openai.com/blog/chatgpt).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(25) OpenAI. [Openai: Chatgpt](https://openai.com/blog/chatgpt)。'
- en: 'Paperno et al. (2016) Denis Paperno, Germán Kruszewski, Angeliki Lazaridou,
    Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda,
    and Raquel Fernández. 2016. The lambada dataset: Word prediction requiring a broad
    discourse context. *arXiv preprint arXiv:1606.06031*.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paperno et al. (2016) Denis Paperno, Germán Kruszewski, Angeliki Lazaridou,
    Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda,
    and Raquel Fernández. 2016. LAMBADA数据集：需要广泛语境的词预测。*arXiv预印本 arXiv:1606.06031*。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. 探索统一文本到文本变换器的迁移学习极限。*机器学习研究杂志*，21(1):5485–5551。
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at
    scale. *Communications of the ACM*, 64(9):99–106.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2021. Winogrande: 大规模对抗性Winograd Schema挑战。*ACM通讯*，64(9):99–106。'
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. 2022. Bloom: 一个176b参数的开放访问多语言模型。*arXiv预印本 arXiv:2211.05100*。'
- en: 'Shen et al. (2021) Mingzhu Shen, Feng Liang, Ruihao Gong, Yuhang Li, Chuming
    Li, Chen Lin, Fengwei Yu, Junjie Yan, and Wanli Ouyang. 2021. Once quantization-aware
    training: High performance extremely low-bit architecture search. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, pages 5340–5349.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen et al. (2021) Mingzhu Shen, Feng Liang, Ruihao Gong, Yuhang Li, Chuming
    Li, Chen Lin, Fengwei Yu, Junjie Yan, and Wanli Ouyang. 2021. Once quantization-aware
    training: 高性能极低比特架构搜索。见于*IEEE/CVF国际计算机视觉会议论文集*，页码5340–5349。'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等. 2023. Llama: Open and efficient foundation language
    models. *arXiv 预印本 arXiv:2302.13971*。'
- en: 'Wei et al. (2022) Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei
    Yu. 2022. Qdrop: randomly dropping quantization for extremely low-bit post-training
    quantization. *arXiv preprint arXiv:2203.05740*.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei et al. (2022) Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, 和 Fengwei
    Yu. 2022. Qdrop: randomly dropping quantization for extremely low-bit post-training
    quantization. *arXiv 预印本 arXiv:2203.05740*。'
- en: 'Xiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth,
    and Song Han. 2022. Smoothquant: Accurate and efficient post-training quantization
    for large language models. *arXiv preprint arXiv:2211.10438*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth,
    和 Song Han. 2022. Smoothquant: Accurate and efficient post-training quantization
    for large language models. *arXiv 预印本 arXiv:2211.10438*。'
- en: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. 2022. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. *Advances in Neural Information Processing
    Systems*, 35:27168–27183.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, 和 Yuxiong He. 2022. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. *神经信息处理系统进展*, 35:27168–27183。'
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? *arXiv
    preprint arXiv:1905.07830*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    和 Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? *arXiv
    预印本 arXiv:1905.07830*。'
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. 2022. Opt: Open pre-trained transformer language models. *arXiv preprint
    arXiv:2205.01068*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    等. 2022. Opt: Open pre-trained transformer language models. *arXiv 预印本 arXiv:2205.01068*。'
- en: 'Zhang et al. (2020) Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,
    Xin Jiang, and Qun Liu. 2020. Ternarybert: Distillation-aware ultra-low bit bert.
    *arXiv preprint arXiv:2009.12812*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2020) Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,
    Xin Jiang, 和 Qun Liu. 2020. Ternarybert: Distillation-aware ultra-low bit bert.
    *arXiv 预印本 arXiv:2009.12812*。'
- en: Zhuang et al. (2021) Bohan Zhuang, Mingkui Tan, Jing Liu, Lingqiao Liu, Ian
    Reid, and Chunhua Shen. 2021. Effective training of convolutional neural networks
    with low-bitwidth weights and activations. *IEEE Transactions on Pattern Analysis
    and Machine Intelligence*, 44(10):6140–6152.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang et al. (2021) Bohan Zhuang, Mingkui Tan, Jing Liu, Lingqiao Liu, Ian
    Reid, 和 Chunhua Shen. 2021. Effective training of convolutional neural networks
    with low-bitwidth weights and activations. *IEEE 模式分析与机器智能汇刊*, 44(10):6140–6152。
- en: Appendix A Appendix
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Additional comparison with AWQ
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 与 AWQ 的附加比较
- en: Although both AWQ and TEQ use a small calibration set from Pile, TEQ’s evaluation
    methodology closely follows that of GPTQ and only shares a few common tasks with
    AWQ. It is important to acknowledge that this comparison inherently lacks rigor
    due to our reliance on referencing AWQ’s data alone. Consequently, this approach
    introduces the potential unfairness in the evaluation process, primarily stemming
    from the utilization of different calibration datasets.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 AWQ 和 TEQ 都使用了来自 Pile 的小型校准集，但 TEQ 的评估方法与 GPTQ 密切相关，只与 AWQ 共享少量共同任务。需要指出的是，由于仅参考
    AWQ 的数据，这种比较本质上缺乏严格性。因此，这种方法引入了评估过程中的潜在不公平，主要源于使用不同的校准数据集。
- en: '| LLaMA-7B | AWQ | Ours |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | AWQ | 我们的方法 |'
- en: '| nbits | Method | PIQA | Hella. | Wino. | PIQA | Hella. | Wino. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| nbits | 方法 | PIQA | Hella. | Wino. | PIQA | Hella. | Wino. |'
- en: '| 16 | FP16 | 78.35 | 56.44 | 67.09 | 78.35 | 56.42 | 66.85 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 16 | FP16 | 78.35 | 56.44 | 67.09 | 78.35 | 56.42 | 66.85 |'
- en: '| W3G128 | RTN | 75.84 | 53.10 | 63.22 | 75.68 | 53.18 | 63.06 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| W3G128 | RTN | 75.84 | 53.10 | 63.22 | 75.68 | 53.18 | 63.06 |'
- en: '| GPTQ | 70.89 | 46.77 | 60.93 | 72.58 | 47.10 | 59.91 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 70.89 | 46.77 | 60.93 | 72.58 | 47.10 | 59.91 |'
- en: '| Proposed | 76.66 | 53.63 | 66.14 | 76.01 | 53.30 | 63.06 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Proposed | 76.66 | 53.63 | 66.14 | 76.01 | 53.30 | 63.06 |'
- en: '| W4G128 | RTN | 77.86 | 55.81 | 65.59 | 77.58 | 55.91 | 65.59 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| W4G128 | RTN | 77.86 | 55.81 | 65.59 | 77.58 | 55.91 | 65.59 |'
- en: '| GPTQ | 77.20 | 53.98 | 65.67 | 77.58 | 55.83 | 66.54 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 77.20 | 53.98 | 65.67 | 77.58 | 55.83 | 66.54 |'
- en: '| Proposed | 78.07 | 55.76 | 65.82 | 78.02 | 55.76 | 66.54 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Proposed | 78.07 | 55.76 | 65.82 | 78.02 | 55.76 | 66.54 |'
- en: 'Table 6: Reported results of AWQ and Ours'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: AWQ 和我们的方法的报告结果'
- en: Table 6 presents the LLaMA-7B’s results of our common tasks alongside AWQ in
    table below and all the results of AWQ are from their paper.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6 展示了 LLaMA-7B 在我们的常见任务中的结果，与 AWQ 进行比较，所有 AWQ 的结果均来自他们的论文。
- en: A.2 Additional comparison with GPTQ act-order
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 与 GPTQ act-order 的额外比较
- en: 'We show the results in Table [7](#A1.T7 "Table 7 ‣ A.2 Additional comparison
    with GPTQ act-order ‣ Appendix A Appendix ‣ TEQ: Trainable Equivalent Transformation
    for Quantization of LLMs"). TEQ still outperforms GPTQ in most cases.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表 [7](#A1.T7 "Table 7 ‣ A.2 Additional comparison with GPTQ act-order ‣
    Appendix A Appendix ‣ TEQ: Trainable Equivalent Transformation for Quantization
    of LLMs") 中展示了结果。TEQ 在大多数情况下仍优于 GPTQ。'
- en: '| nbits / gs | Methods | lm-eval ($\uparrow$) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| nbits / gs | 方法 | lm-eval ($\uparrow$) |'
- en: '| 4 / -1 | GPTQ-AO | 0.6713 | 6.06 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 4 / -1 | GPTQ-AO | 0.6713 | 6.06 |'
- en: '| Ours | 0.6771 | 6.30 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 0.6771 | 6.30 |'
- en: '| Ours+GPTQ-AO | 0.6736 | 6.03 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 我们+GPTQ-AO | 0.6736 | 6.03 |'
- en: '| 4 / 128 | GPTQ-AO | 0.6809 | 5.82 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 4 / 128 | GPTQ-AO | 0.6809 | 5.82 |'
- en: '| Ours | 0.6813 | 5.97 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 0.6813 | 5.97 |'
- en: '| Ours+GPTQ-AO | 0.6811 | 5.82 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 我们+GPTQ-AO | 0.6811 | 5.82 |'
- en: '| 3 / 128 | GPTQ-AO | 0.6042 | 8.29 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 3 / 128 | GPTQ-AO | 0.6042 | 8.29 |'
- en: '| Ours | 0.6521 | 6.89 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 0.6521 | 6.89 |'
- en: '| Ours+GPTQ-AO | 0.6647 | 6.61 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 我们+GPTQ-AO | 0.6647 | 6.61 |'
- en: 'Table 7: Comparing results of Llama-7B for GPTQ with act-order. AO denotes
    act-order. TEQ still outperforms GPTQ in most cases.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：Llama-7B 对 GPTQ 的 act-order 结果比较。AO 表示 act-order。TEQ 在大多数情况下仍优于 GPTQ。
- en: A.3 Special hyperparameters and settings
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 特殊超参数和设置
- en: 'Usually, we adopt the same hyperparameters mentioned in section [3.2](#S3.SS2
    "3.2 Training Details ‣ 3 Methodology ‣ TEQ: Trainable Equivalent Transformation
    for Quantization of LLMs"). So we only list all the particular settings in Table
    [8](#A1.T8 "Table 8 ‣ A.3 Special hyperparameters and settings ‣ Appendix A Appendix
    ‣ TEQ: Trainable Equivalent Transformation for Quantization of LLMs").'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，我们采用第 [3.2](#S3.SS2 "3.2 Training Details ‣ 3 Methodology ‣ TEQ: Trainable
    Equivalent Transformation for Quantization of LLMs") 节中提到的相同超参数。因此，我们仅列出表 [8](#A1.T8
    "Table 8 ‣ A.3 Special hyperparameters and settings ‣ Appendix A Appendix ‣ TEQ:
    Trainable Equivalent Transformation for Quantization of LLMs") 中的所有特定设置。'
- en: '| lr | initialization | Models |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| lr | 初始化 | 模型 |'
- en: '| default | $1.0/sqrt(w_{cin})$ |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 默认 | $1.0/sqrt(w_{cin})$ |'
- en: '&#124; opt13b_w4; bloom3b_w4; &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; opt13b_w4; bloom3b_w4; &#124;'
- en: '&#124; bloom7b_w4; &#124;'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; bloom7b_w4; &#124;'
- en: '&#124; opt6.7b_w4_g128; &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; opt6.7b_w4_g128; &#124;'
- en: '&#124; opt13b_w3_g128 &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; opt13b_w3_g128 &#124;'
- en: '|'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 4e-4 | default | llama7b_w4; |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 4e-4 | 默认 | llama7b_w4; |'
- en: '| 2e-4 | default | llama13b_w4_g128; |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 2e-4 | 默认 | llama13b_w4_g128; |'
- en: 'Table 8: Special hyperparameters and settings. g denotes group size'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：特殊超参数和设置。g 表示组大小
- en: '![Refer to caption](img/bc8b00a5592ccba4340384a3d5d1ee78.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bc8b00a5592ccba4340384a3d5d1ee78.png)'
- en: 'Figure 3: TEQ’s trained transformation parameters’ magnitude distributions,
    using maximum’s square root value for initialization. From top to down are BLOOM-3B,
    BLOOM-7.1B, OPT-6.7B and LLAMA-7B respectively.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：TEQ 训练的转换参数幅度分布，使用最大值的平方根进行初始化。从上到下分别是 BLOOM-3B、BLOOM-7.1B、OPT-6.7B 和 LLAMA-7B。
- en: A.4 More visualization results for TEQ’s trained parameters.
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 TEQ 训练参数的更多可视化结果。
- en: 'Figure [3](#A1.F3 "Figure 3 ‣ A.3 Special hyperparameters and settings ‣ Appendix
    A Appendix ‣ TEQ: Trainable Equivalent Transformation for Quantization of LLMs")
    shows the magnitude distribution of scales initialized with $1.0/sqrt(w_{cin})$.
    Since the initial value is related to channel-wise maximum values, it’s more challenging
    to analyze. However, some outliers could be still observed.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [3](#A1.F3 "Figure 3 ‣ A.3 Special hyperparameters and settings ‣ Appendix
    A Appendix ‣ TEQ: Trainable Equivalent Transformation for Quantization of LLMs")
    显示了以 $1.0/sqrt(w_{cin})$ 初始化的尺度的幅度分布。由于初始值与通道方向的最大值相关，因此分析起来更具挑战性。然而，仍可以观察到一些离群值。'
- en: A.5 Counts of trainable parameters introduced by TEQ
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 TEQ 引入的可训练参数计数
- en: We provide more details about counts of trainable parameters introduced by TEQ
    in Table 9\. table presented below offers details regarding the applicable layers
    of TEQ in several models. We handle linear layers that possess transformation
    scales that can be assimilated by their preceding layers, such as Layer Normalization,
    among others.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表 9 中提供了关于 TEQ 引入的可训练参数计数的更多细节。下面的表格提供了 TEQ 在多个模型中的适用层的详细信息。我们处理具有可以被其前置层同化的转换尺度的线性层，如层归一化等。
- en: As an illustration, within a single transformer block of OPT-6.7B, the QKV layers
    have the same preceding layers and therefore utilize the same set of trainable
    parameters. Based on the statistics, we have observed that TEQ’s training only
    requires a minimal number of parameters (around the order from 1e-5 to 1e-4),
    thereby making our approach light-weighted enough.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，在OPT-6.7B的单个变换器块中，QKV层具有相同的前置层，因此使用相同的一组可训练参数。根据统计数据，我们观察到TEQ的训练仅需极少量的参数（大约在1e-5到1e-4的范围内），因此使得我们的方法足够轻量。
- en: '| Models | Blocks | TEQ Applicable Linear Layers | Total Linear Layers | TEQ
    Parameter Groups | TEQ Parameters Counts | Total Parameters Counts | Ratio TEQ
    params and Total Params |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 块数 | TEQ适用线性层 | 总线性层 | TEQ参数组 | TEQ参数计数 | 总参数计数 | TEQ参数与总参数比例 |'
- en: '| Bloom 3B | 30 | 60 | 121 | 60 | 153600 | 3644810240 | 0.00421% |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| Bloom 3B | 30 | 60 | 121 | 60 | 153600 | 3644810240 | 0.00421% |'
- en: '| Bloom 7B1 | 30 | 60 | 121 | 60 | 245760 | 8096620544 | 0.00304% |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Bloom 7B1 | 30 | 60 | 121 | 60 | 245760 | 8096620544 | 0.00304% |'
- en: '| OPT 6.7B | 32 | 160 | 193 | 72 | 786432 | 6864388096 | 0.01146% |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| OPT 6.7B | 32 | 160 | 193 | 72 | 786432 | 6864388096 | 0.01146% |'
- en: '| OPT 13B | 40 | 200 | 241 | 96 | 1228800 | 13110865920 | 0.00937% |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| OPT 13B | 40 | 200 | 241 | 96 | 1228800 | 13110865920 | 0.00937% |'
- en: '| Llama 7B | 32 | 160 | 225 | 64 | 262144 | 6738415616 | 0.00389% |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| Llama 7B | 32 | 160 | 225 | 64 | 262144 | 6738415616 | 0.00389% |'
- en: '| Llama 13B | 40 | 200 | 281 | 80 | 409600 | 13015864320 | 0.00315% |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| Llama 13B | 40 | 200 | 281 | 80 | 409600 | 13015864320 | 0.00315% |'
- en: 'Table 9: Analysis of TEQ Parameters. TEQ only require a minimal ratio of original
    models’ parameters (around the order from 1e-5 to 1e-4).'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：TEQ参数分析。TEQ仅需原始模型参数的极少比例（大约在1e-5到1e-4的范围内）。
