- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:47:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: How Does Quantization Affect Multilingual LLMs?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化如何影响多语言大型语言模型？
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.03211](https://ar5iv.labs.arxiv.org/html/2407.03211)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.03211](https://ar5iv.labs.arxiv.org/html/2407.03211)
- en: Kelly Marchisio¹, Saurabh Dash², Hongyu Chen¹, Dennis Aumiller¹,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kelly Marchisio¹, Saurabh Dash², Hongyu Chen¹, Dennis Aumiller¹,
- en: Ahmet Üstün², Sara Hooker², Sebastian Ruder¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Ahmet Üstün², Sara Hooker², Sebastian Ruder¹
- en: ¹Cohere ²Cohere For AI
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹Cohere ²Cohere For AI
- en: 'Correspondence: kelly@cohere.com'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 联系方式：kelly@cohere.com
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Quantization techniques are widely used to improve inference speed and deployment
    of large language models. While a wide body of work examines the impact of quantized
    LLMs on English tasks, none have examined the effect of quantization across languages.
    We conduct a thorough analysis of quantized multilingual LLMs, focusing on their
    performance across languages and at varying scales. We use automatic benchmarks,
    LLM-as-a-Judge methods, and human evaluation, finding that (1) harmful effects
    of quantization are apparent in human evaluation, and automatic metrics severely
    underestimate the detriment: a 1.7% average drop in Japanese across automatic
    tasks corresponds to a 16.0% drop reported by human evaluators on realistic prompts;
    (2) languages are disparately affected by quantization, with non-Latin script
    languages impacted worst; and (3) challenging tasks such as mathematical reasoning
    degrade fastest. As the ability to serve low-compute models is critical for wide
    global adoption of NLP technologies, our results urge consideration of multilingual
    performance as a key evaluation criterion for efficient models.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 量化技术被广泛用于提高大型语言模型的推理速度和部署效率。虽然大量研究探讨了量化大型语言模型对英语任务的影响，但尚未研究量化对跨语言的影响。我们对量化的多语言大型语言模型进行了深入分析，重点关注其在不同语言和不同规模下的表现。我们使用了自动基准测试、LLM作为评判者的方法和人工评估，发现（1）量化的有害效果在人工评估中显著，而自动指标严重低估了这种不利影响：自动任务中日语的平均下降1.7%对应于人工评估者在实际提示中报告的16.0%下降；（2）不同语言对量化的影响存在差异，非拉丁文字语言受影响最严重；（3）如数学推理等挑战性任务降级最快。由于为低计算模型提供服务的能力对自然语言处理技术的广泛全球应用至关重要，我们的结果促使考虑多语言性能作为高效模型的关键评估标准。
- en: How Does Quantization Affect Multilingual LLMs?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 量化如何影响多语言大型语言模型？
- en: 'Kelly Marchisio¹, Saurabh Dash², Hongyu Chen¹, Dennis Aumiller¹, Ahmet Üstün²,
    Sara Hooker², Sebastian Ruder¹ ¹Cohere ²Cohere For AI Correspondence: kelly@cohere.com'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Kelly Marchisio¹, Saurabh Dash², Hongyu Chen¹, Dennis Aumiller¹, Ahmet Üstün²,
    Sara Hooker², Sebastian Ruder¹ ¹Cohere ²Cohere For AI 联系方式：kelly@cohere.com
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Multilingual large language models (LLMs) have the power to bring modern language
    technology to the world, but only if they are cheap and reliable. Known as the
    low-resource double bind, underserved languages and severe compute constraints
    often geographically co-occur (Ahia et al., [2021](#bib.bib2)), meaning that for
    wide adoption, multilingual LLMs must be highly-performant and lightweight.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 多语言大型语言模型（LLMs）有能力将现代语言技术带到世界各地，但前提是它们必须便宜且可靠。被称为低资源困境，资源不足的语言和严重的计算限制通常在地理上同时存在（Ahia等，
    [2021](#bib.bib2)），这意味着为了广泛采用，多语言LLMs必须具备高性能和轻量化的特点。
- en: With the shift towards large models, quantization is a widely adopted technique
    to reduce cost, improve inference speed, and enable wider deployment of LLMs.
    Work on quantization, however, is by-and-large evaluated in English only (e.g.
    Xiao et al., [2023](#bib.bib56); Ahmadian et al., [2024](#bib.bib4); Frantar et al.,
    [2022](#bib.bib19)). No works to our knowledge have characterized the impact of
    quantization on the multilingual generation capabilities expected from modern
    LLMs. Ubiquitous use of compression techniques in the real world drives urgency
    to the question how are multilingual models impacted?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着向大型模型的转变，量化是一种广泛采用的技术，用于降低成本、提高推理速度，并实现LLMs的更广泛部署。然而，关于量化的研究大多只评估了英语（例如 Xiao等，
    [2023](#bib.bib56)；Ahmadian等， [2024](#bib.bib4)；Frantar等， [2022](#bib.bib19)）。据我们所知，还没有研究描绘量化对现代LLMs期望的多语言生成能力的影响。现实世界中压缩技术的普遍使用使得多语言模型受影响的紧迫性问题更加突出。
- en: 'Our question is timely, given recent work showing that compression techniques
    such as quantization and sparsity amplify disparate treatment of long-tail features,
    which may have implications for under-represented languages in multilingual LLMs
    (Hooker et al., [2019](#bib.bib24), [2020](#bib.bib25); Ahia et al., [2021](#bib.bib2);
    Ogueji et al., [2022](#bib.bib44)). Indeed, many model designs choices implicitly
    overfit to a handful of resource rich languages: from tokenizer choice, to weighting
    of training data, and to widely-used quantization techniques. Focusing on a small
    subset of high-resource languages in design degrades model performance for overlooked
    languages (Schwartz et al., [2022](#bib.bib51); Kotek et al., [2023](#bib.bib31);
    Khandelwal et al., [2023](#bib.bib27); Vashishtha et al., [2023](#bib.bib54);
    Khondaker et al., [2023](#bib.bib29); Pozzobon et al., [2024](#bib.bib49)), introduces
    security vulnerabilities (Yong et al., [2023](#bib.bib57); Nasr et al., [2023](#bib.bib41);
    Li et al., [2023a](#bib.bib32); Lukas et al., [2023](#bib.bib38); Deng et al.,
    [2023](#bib.bib14)), and unfairly passes high costs to non-English users faced
    with high latency (Held et al., [2023](#bib.bib22); Durmus et al., [2023](#bib.bib18);
    Nicholas and Bhatia, [2023](#bib.bib43); Ojo et al., [2023](#bib.bib45); Ahia
    et al., [2023](#bib.bib3)).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的问题很及时，因为最近的研究显示，压缩技术如量化和稀疏性加剧了对长尾特征的不同对待，这可能对多语种大型语言模型中的少数语言有影响（Hooker等人，[2019](#bib.bib24)，[2020](#bib.bib25)；Ahia等人，[2021](#bib.bib2)；Ogueji等人，[2022](#bib.bib44)）。确实，许多模型设计选择隐含地过度拟合少数资源丰富的语言：从分词器选择，到训练数据的加权，再到广泛使用的量化技术。在设计中专注于资源丰富语言的一个小子集会降低被忽视语言的模型性能（Schwartz等人，[2022](#bib.bib51)；Kotek等人，[2023](#bib.bib31)；Khandelwal等人，[2023](#bib.bib27)；Vashishtha等人，[2023](#bib.bib54)；Khondaker等人，[2023](#bib.bib29)；Pozzobon等人，[2024](#bib.bib49)），引入安全漏洞（Yong等人，[2023](#bib.bib57)；Nasr等人，[2023](#bib.bib41)；Li等人，[2023a](#bib.bib32)；Lukas等人，[2023](#bib.bib38)；Deng等人，[2023](#bib.bib14)），并不公平地将高成本传递给面对高延迟的非英语用户（Held等人，[2023](#bib.bib22)；Durmus等人，[2023](#bib.bib18)；Nicholas和Bhatia，[2023](#bib.bib43)；Ojo等人，[2023](#bib.bib45)；Ahia等人，[2023](#bib.bib3)）。
- en: 'We analyze four state-of-the-art multilingual LLMs across 3 different sizes
    ranging from 8 to 103 billion parameters and covering up to 23 languages, under
    various quantization techniques. Critically, it is vital that we move beyond automatic
    evaluation and gather real human feedback on performance cost. We thus perform
    multilingual human evaluation on challenging real-world prompts in addition to
    LLM-as-a-Judge and evaluation on standard automatic benchmarks such as multilingual
    MMLU Hendrycks et al. ([2020](#bib.bib23)), MGSM Shi et al. ([2023](#bib.bib52)),
    and FLORES-200 (Costa-jussà et al., [2022a](#bib.bib11)). Across experimental
    set-ups we find that:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析了四个最先进的多语言大型语言模型，涵盖8到103亿参数的3种不同规模，支持最多23种语言，应用了各种量化技术。至关重要的是，我们需要超越自动评估，获取有关性能成本的真实人类反馈。因此，我们在具有挑战性的实际提示上进行多语言人工评估，此外，还对LLM-as-a-Judge和标准自动基准（如多语言MMLU
    Hendrycks等人，[2020](#bib.bib23)），MGSM Shi等人（[2023](#bib.bib52)），以及FLORES-200（Costa-jussà等人，[2022a](#bib.bib11)）进行评估。在实验设置中，我们发现：
- en: '1.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Automatic metrics severely underestimate damage from quantization. Automatic
    evaluations estimate performance deterioration relative to FP16 across tasks at
    $-0.3\%$ reported by human evaluators.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动化指标严重低估了量化带来的损害。自动评估相对于FP16的性能下降在各项任务中估计为$-0.3\%$，由人工评估者报告。
- en: '2.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Quantization affects languages differently. Non-Latin script languages are more
    greatly harmed on average. Across tasks, Latin-script languages scored $-0.7\%$.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化对语言的影响各不相同。非拉丁文字脚本语言通常受到更大的损害。在各项任务中，拉丁文字脚本语言的得分下降了$-0.7\%$。
- en: '3.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Challenging tasks degrade fastest. Mathematical reasoning ($-13.1\%$) results
    are severely degraded.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 挑战性任务的退化最快。数学推理（$-13.1\%$）的结果严重退化。
- en: '4.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Occasionally, quantization brings benefits. Similar to Ahia et al. ([2021](#bib.bib2))
    and Ogueji et al. ([2022](#bib.bib44)) on sparsity, we find that quantization
    benefits model performance in some cases: e.g., an average 1.3% boost across tasks
    for a 35B model quantized with W8A8.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 偶尔，量化带来好处。类似于Ahia等人（[2021](#bib.bib2)）和Ogueji等人（[2022](#bib.bib44)）关于稀疏性的研究，我们发现量化在某些情况下对模型性能有益：例如，对一个35B模型使用W8A8量化在各项任务中平均提升1.3%。
- en: As the first to broadly study the impact of quantization on multilingual LLMs,
    our work is part of a wider body of literature that considers the impact of model
    design choices on downstream performance. Our results urge attention to multilingual
    performance at all stages of system design.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作为首次广泛研究量化对多语言LLMs影响的工作，我们的研究是一个更广泛文献中的一部分，这些文献考虑了模型设计选择对下游性能的影响。我们的结果呼吁在系统设计的所有阶段关注多语言性能。
- en: 2 Background
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 'Quantization compresses the weights and potentially activations of a neural
    network to lower-bit representations. Compression can be done by training the
    model at lower precision, known as Quantization Aware Training (QAT), or performed
    on the final model weights, known as Post Training Quantization (PTQ). Given the
    difficulties in training LLMs especially at precision lower than 16-bits floating
    point, PTQ methods which perform the quantization single-shot without needing
    gradient updates are highly desirable. Training is completed at higher precision,
    then weights/activations are quantized without further training. In this work,
    we focus on post-training quantization because of its simplicity and applicability
    at scale. PTQ of LLMs can be further categorized into:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 量化将神经网络的权重和可能的激活压缩为低位表示。压缩可以通过在较低精度下训练模型来完成，这被称为量化感知训练（QAT），或者在最终模型权重上执行，这被称为后训练量化（PTQ）。考虑到训练大型语言模型（LLMs）时特别是在低于16位浮点数精度的困难，PTQ方法由于能够一次性完成量化而不需要梯度更新，非常受欢迎。训练在较高精度下完成，然后权重/激活被量化而无需进一步训练。在这项工作中，我们关注于后训练量化，因为它的简单性和大规模适用性。LLMs的PTQ还可以进一步分类为：
- en: Weight-Only Quantization
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 仅权重量化
- en: Weight matrices are quantized offline and the compressed matrices are loaded
    from memory during inference. Quantized weight matrices have a smaller memory
    footprint compared to FP16 ($2\times$ smaller for 4-bit), enabling inference with
    less compute. In memory-bound scenarios, it also enables faster inference due
    to fewer bytes transferred from GPU memory to the compute units.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 权重矩阵在离线量化后，压缩的矩阵在推理过程中从内存加载。与FP16相比，量化后的权重矩阵具有更小的内存占用（4位的内存占用减少$2\times$），这使得推理计算量减少。在内存受限的场景下，它也因从GPU内存到计算单元传输的字节更少而实现更快的推理。
- en: 'For a weight matrix $\mathbf{W}\in\mathbb{R}^{d_{in}\times d_{out}}$, if only
    a single scaling factor is used for naive quantization (per-tensor), then the
    quantized weights are given by:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于权重矩阵$\mathbf{W}\in\mathbb{R}^{d_{in}\times d_{out}}$，如果仅使用一个单一的缩放因子进行简单量化（每个张量），则量化后的权重由以下公式给出：
- en: '|  | $\mathbf{W}_{Q}=\Delta\cdot\left\lfloor\frac{\mathbf{W}}{\Delta}\right\rceil,\quad\Delta=\frac{\max(&#124;\mathbf{W}&#124;)}{2^{N-1}}$
    |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{W}_{Q}=\Delta\cdot\left\lfloor\frac{\mathbf{W}}{\Delta}\right\rceil,\quad\Delta=\frac{\max(&#124;\mathbf{W}&#124;)}{2^{N-1}}$
    |  | (1) |'
- en: where $\Delta\in\mathbb{R}$ rounding to the nearest integer.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\Delta\in\mathbb{R}$舍入到最接近的整数。
- en: A single scaling factor might not be enough if the distribution of parameters
    in the weight matrix has high variance; thus one could increase the granularity
    of quantization by using a scale for each output dimension (per-column), i.e.,
    $\Delta\in\mathbb{R}^{d_{out}}$. A commonly used group size is 128.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果权重矩阵中参数的分布具有较高的方差，单一的缩放因子可能不够用；因此可以通过对每个输出维度使用一个缩放因子（每列）来增加量化的粒度，即$\Delta\in\mathbb{R}^{d_{out}}$。一个常用的组大小是128。
- en: Equation [1](#S2.E1 "In Weight-Only Quantization ‣ 2 Background ‣ How Does Quantization
    Affect Multilingual LLMs?") gives the simplest way to quantize the weights. For
    $N\leq 4$ bits, using more advanced Weight-Only Quantization methods like GPTQ
    (Frantar et al., [2022](#bib.bib19)) or AWQ (Lin et al., [2024](#bib.bib35)) leads
    to better downstream performance.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 方程[1](#S2.E1 "在仅权重量化 ‣ 2 背景 ‣ 量化如何影响多语言LLMs？")给出了量化权重的最简单方法。对于$N\leq 4$位，使用更先进的仅权重量化方法，如GPTQ（Frantar等，[2022](#bib.bib19)）或AWQ（Lin等，[2024](#bib.bib35)），可以带来更好的下游性能。
- en: Weight-and-Activation Quantization
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 权重和激活量化
- en: 'As the name suggests, Weight-and-Activation Quantization quantizes the model
    activations alongside the weights. Unlike Weight-Only Quantization where weights
    can be quantized offline, quantization of activations happens at runtime. One
    could compute the quantization scales for various activations by using a small
    slice of training or validation data (static scaling) but this method typically
    has large degradation (Xiao et al., [2023](#bib.bib56)). For minimal degradation,
    it is preferred to calculate the quantization scaling factor dynamically (dynamic
    scaling) for each input on-the-fly. While quantizing activations is more difficult,
    reducing the precision of the activations alongside the weights enables the usage
    of specialized low-precision matrix multiplication hardware in modern GPUs leading
    to up to $2\times$ by:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名，权重和激活量化同时对模型激活进行量化。与权重仅量化不同，权重可以离线量化，而激活的量化发生在运行时。可以通过使用小部分训练或验证数据（静态缩放）计算各种激活的量化缩放因子，但这种方法通常会有较大的性能下降（Xiao等，[2023](#bib.bib56)）。为了最小化降级，推荐动态计算每个输入的量化缩放因子（动态缩放）。虽然量化激活较为困难，但在现代GPU中将激活与权重一起降低精度可以使用专用的低精度矩阵乘法硬件，从而实现最高$2\times$的加速：
- en: '|  | $\displaystyle{\mathbf{W}_{Q}}_{:,j}=\left\lfloor\frac{\mathbf{W}_{:,j}}{\Delta^{W}_{:,j}}\right\rceil,\Delta^{W}_{:,j}=\frac{\max(&#124;\mathbf{W}_{:,j}&#124;)}{2^{N-1}}$
    |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathbf{W}_{Q}}_{:,j}=\left\lfloor\frac{\mathbf{W}_{:,j}}{\Delta^{W}_{:,j}}\right\rceil,\Delta^{W}_{:,j}=\frac{\max(&#124;\mathbf{W}_{:,j}&#124;)}{2^{N-1}}$
    |  | (2) |'
- en: '|  | $\displaystyle{\mathbf{X}_{Q}}_{i,:}=\left\lfloor\frac{\mathbf{X}_{i,:}}{\Delta^{X}_{i,:}}\right\rceil,\Delta^{X}_{i,:}=\frac{\max(&#124;\mathbf{X}_{i,:}&#124;)}{2^{N-1}}$
    |  | (3) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathbf{X}_{Q}}_{i,:}=\left\lfloor\frac{\mathbf{X}_{i,:}}{\Delta^{X}_{i,:}}\right\rceil,\Delta^{X}_{i,:}=\frac{\max(&#124;\mathbf{X}_{i,:}&#124;)}{2^{N-1}}$
    |  | (3) |'
- en: '|  | $\displaystyle Y=\Delta^{X}\odot(\mathbf{X}_{Q}\mathbf{W}_{Q})\odot\Delta^{W}$
    |  | (4) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Y=\Delta^{X}\odot(\mathbf{X}_{Q}\mathbf{W}_{Q})\odot\Delta^{W}$
    |  | (4) |'
- en: where, $\Delta^{W}\in\mathbb{R}^{d_{out}}$ denotes element-wise multiplication
    by broadcasting the elements to match the shape of the operands.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\Delta^{W}\in\mathbb{R}^{d_{out}}$ 表示通过广播元素以匹配操作数的形状的逐元素乘法。
- en: 3 Experiment Set-up
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置
- en: Models
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型
- en: We evaluate Command R+¹¹1[https://docs.cohere.com/docs/command-r-plus](https://docs.cohere.com/docs/command-r-plus),
    Command R²²2[https://docs.cohere.com/docs/command-r](https://docs.cohere.com/docs/command-r),
    and Aya 23 models (Aryabumi et al., [2024](#bib.bib5)) as representatives of state-of-the-art
    multilingual LLMs. Command models are 103 and 35 billion parameters, and Aya 23
    models are 35 and 8 billion parameters. We quantize the models using the weights
    available on HuggingFace.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了作为最先进多语言LLM代表的Command R+¹¹1[https://docs.cohere.com/docs/command-r-plus](https://docs.cohere.com/docs/command-r-plus)、Command
    R²²2[https://docs.cohere.com/docs/command-r](https://docs.cohere.com/docs/command-r)和Aya
    23模型（Aryabumi等，[2024](#bib.bib5)）。Command模型有103亿和35亿参数，而Aya 23模型有35亿和8亿参数。我们使用在HuggingFace上可用的权重对这些模型进行量化。
- en: Quantization
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化
- en: For Command R and R+, we evaluate both weight-only quantization at 8-bit (W8
    with per-column scaling) and 4-bit (W4-g with group-wise scaling using GPTQ (Frantar
    et al., [2022](#bib.bib19))), as well as weight-and-activation quantization at
    8-bit (W8A8 with per-column scaling for weights and per-token scaling for activations).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Command R和R+，我们评估了仅权重量化的8位（W8，按列缩放）和4位（W4-g，使用GPTQ（Frantar等，[2022](#bib.bib19)）进行组级缩放），以及8位的权重和激活量化（W8A8，权重按列缩放，激活按令牌缩放）。
- en: Ahmadian et al. ([2024](#bib.bib4)) show that if the model is trained with the
    right hyper-parameters, naive Weight-and-Activation Quantization has minimal degradation.
    Otherwise, one may leverage SmoothQuant (Xiao et al., [2023](#bib.bib56)) to smoothen
    the distribution of activations making them more amenable to quantization. We
    therefore also explore W8A8-SmoothQuant (a W8A8 variant with SmoothQuant) for
    Command R+ as well as a 4-bit weight-only quantized variant with column-wise scaling
    (W4) to understand the impact of scaling granularity at extremely low-bit precision.
    Following (Frantar et al., [2022](#bib.bib19); Xiao et al., [2023](#bib.bib56)),
    we use 128 English samples for calibration for SmoothQuant and GPTQ.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Ahmadian 等 ([2024](#bib.bib4)) 表明，如果模型使用正确的超参数进行训练，那么简单的权重和激活量化会有最小的退化。否则，可以利用
    SmoothQuant (Xiao 等, [2023](#bib.bib56)) 来平滑激活的分布，使其更易于量化。因此，我们还探索了 W8A8-SmoothQuant（一种带有
    SmoothQuant 的 W8A8 变体）用于 Command R+，以及带有列尺度的 4 位仅权重量化变体（W4），以理解在极低位精度下的尺度粒度的影响。根据
    (Frantar 等, [2022](#bib.bib19); Xiao 等, [2023](#bib.bib56))，我们使用 128 个英文样本对 SmoothQuant
    和 GPTQ 进行校准。
- en: For Aya 23 8B and 35B, we use bitsandbytes³³3[https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
    to obtain 8-bit and 4-bit quantized models. Bitsandbytes uses LLM.int8() (Dettmers
    et al., [2022](#bib.bib15))—similar to W8A8 described above except that it performs
    certain computations in FP16\. Bitsandbytes 4-bit uses the NF4 datatype (Dettmers
    et al., [2023](#bib.bib16)) to perform Quantile Quantization which limits degradation
    at the expense of inference speedups.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Aya 23 8B 和 35B，我们使用 bitsandbytes³³3[https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
    来获得 8 位和 4 位量化模型。Bitsandbytes 使用 LLM.int8() (Dettmers 等, [2022](#bib.bib15))——类似于上面描述的
    W8A8，只是它在 FP16 中执行某些计算。Bitsandbytes 4 位使用 NF4 数据类型 (Dettmers 等, [2023](#bib.bib16))
    进行分位量化，这在牺牲推理速度的情况下限制了退化。
- en: 3.1 Automatic Evaluation
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 自动评估
- en: 'We evaluate in 10 primary languages: Arabic, French, German, English, Spanish,
    Italian, Portuguese, Korean, Japanese, and Chinese. Quantized models are compared
    to the original FP16 versions, and we primarily report results as relative degradation
    compared to this FP16 baseline:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 10 种主要语言中进行评估：阿拉伯语、法语、德语、英语、西班牙语、意大利语、葡萄牙语、韩语、日语和中文。将量化模型与原始的 FP16 版本进行比较，我们主要报告相对于
    FP16 基准的相对退化结果：
- en: '|  | $\%\Delta=\frac{\text{score}_{\text{quantized}}-\text{score}_{\text{FP16}}}{\text{score}_{\text{FP16}}}*100$
    |  | (5) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\%\Delta=\frac{\text{score}_{\text{quantized}}-\text{score}_{\text{FP16}}}{\text{score}_{\text{FP16}}}*100$
    |  | (5) |'
- en: Raw numeric results are in the Appendix. Results are averaged over 5 runs.⁴⁴4k=0,
    p=0.75, temp=0.3, except mMMLU, which, as a QA eval, is run deterministically
    with temp=0.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数值结果在附录中。结果在 5 次运行中取平均。⁴⁴4k=0, p=0.75, temp=0.3，除 mMMLU 外，作为 QA 评估的 mMMLU
    是以 temp=0 确定性运行的。
- en: Multilingual MMLU (mMMLU)
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多语言 MMLU (mMMLU)
- en: This multi-domain question answering task consists of 14,000+ multiple-choice
    questions. We translate MMLU (Hendrycks et al., [2020](#bib.bib23)) to 9 languages
    with Google Translate and refer to this version as mMMLU. We measure accuracy
    in a 5-shot setting. An example is in Table [A1](#A1.T1 "Table A1 ‣ A.1 Prompts
    for mMMLU and LLM-as-a-Judge ‣ Appendix A Appendix ‣ How Does Quantization Affect
    Multilingual LLMs?").
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个多领域问答任务包含 14,000 多个选择题。我们使用 Google Translate 将 MMLU (Hendrycks 等, [2020](#bib.bib23))
    翻译成 9 种语言，并将此版本称为 mMMLU。我们在 5-shot 设置下测量准确率。一个示例见表 [A1](#A1.T1 "Table A1 ‣ A.1
    Prompts for mMMLU and LLM-as-a-Judge ‣ Appendix A Appendix ‣ How Does Quantization
    Affect Multilingual LLMs?")。
- en: MGSM (Shi et al., [2023](#bib.bib52))
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MGSM (Shi 等, [2023](#bib.bib52))
- en: MGSM is a generative mathematics evaluation set manually translated from GSM8K
    (Cobbe et al., [2021](#bib.bib9)). Of our target languages, it is available for
    German, Spanish, French, Japanese, and Chinese. We report accuracy over the 250-item
    test set for each language.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: MGSM 是一个生成性数学评估集，手动翻译自 GSM8K (Cobbe 等, [2021](#bib.bib9))。在我们的目标语言中，它可用于德语、西班牙语、法语、日语和中文。我们报告每种语言的
    250 项测试集上的准确率。
- en: FLORES-200 (Costa-jussà et al., [2022b](#bib.bib12))
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: FLORES-200 (Costa-jussà 等, [2022b](#bib.bib12))
- en: This well-known multi-way parallel test set evaluates translation capabilities.
    We translate into and out of English, and report SacreBLEU Post ([2018](#bib.bib48)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个著名的多通道平行测试集评估翻译能力。我们进行英中翻译并报告 SacreBLEU Post ([2018](#bib.bib48))。
- en: Language Confusion (Marchisio et al., [2024](#bib.bib39))
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语言混淆 (Marchisio 等, [2024](#bib.bib39))
- en: 'These test sets assess a model’s ability to respond in a user’s desired language.
    In the monolingual setting, prompts are in language $l$.⁵⁵5An example from the
    Okapi subsection of the evaluation is: “Reply in Spanish. Explain a common misconception
    about your topic. Topic: Using AI to Augment Human Capabilities” fastText Joulin
    et al. ([2016](#bib.bib26)) language identification is run over the output. We
    report line-level pass rate (LPR), i.e., the percentage of responses for which
    all lines in the response are in the user’s desired language.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些测试集评估模型在用户期望语言中的响应能力。在单语环境下，提示语言为 $l$。⁵⁵5 评估中 Okapi 子部分的一个示例是：“用西班牙语回答。解释一下你主题的一个常见误解。主题：使用
    AI 增强人类能力” fastText Joulin et al. ([2016](#bib.bib26)) 语言识别会在输出上运行。我们报告行级通过率 (LPR)，即所有回答行都用用户期望语言的响应比例。
- en: Aya Evaluation
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Aya 评估
- en: 'Aya 23 models are evaluated using an extended version of the Aya evaluation
    setup (Aryabumi et al., [2024](#bib.bib5)) using the unseen discriminative tasks
    (XWinograd (Muennighoff et al., [2023](#bib.bib40)), XCOPA (Ponti et al., [2020](#bib.bib47)),
    XStoryCloze (Lin et al., [2022](#bib.bib36))), mMMLU (Okapi; Dac Lai et al., [2023](#bib.bib13)),
    MGSM, and Belebele (Bandarkar et al., [2023](#bib.bib7)) from eval-harness (Gao
    et al., [2023](#bib.bib20)).⁶⁶6We follow the setup used by Üstün et al. ([2024](#bib.bib60)):
    each evaluation is run once, and for FLORES, no sampling is used and metric is
    spBLEU. We evaluate models on languages included in the covered 23 languages,
    except for the unseen tasks where we use all available languages.⁷⁷7mMMLU: ar,
    de, es, fr, hi, id, it, nl, pt, ro, ru, uk, vi, zh. MGSM: de, es, fr, ja, ru,
    zh. Belebele: {mMMLU} + cs, fa, el, ja, ko, pl, tr. FLORES: {Belebele} + he. Aya
    evaluations allow us to add: Czech, Greek, Hebrew, Hindi, Indonesian, Dutch, Persian,
    Polish, Romanian, Russian, Turkish, Ukrainian, Vietnamese.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 'Aya 23 模型使用扩展版的 Aya 评估设置（Aryabumi et al., [2024](#bib.bib5)）进行评估，涉及未见的区分任务（XWinograd
    (Muennighoff et al., [2023](#bib.bib40))，XCOPA (Ponti et al., [2020](#bib.bib47))，XStoryCloze
    (Lin et al., [2022](#bib.bib36)))，mMMLU (Okapi; Dac Lai et al., [2023](#bib.bib13))，MGSM
    和 Belebele (Bandarkar et al., [2023](#bib.bib7))，这些任务来自 eval-harness (Gao et al.,
    [2023](#bib.bib20))。⁶⁶6 我们遵循 Üstün et al. ([2024](#bib.bib60)) 使用的设置：每项评估仅运行一次，对于
    FLORES，不使用采样，指标为 spBLEU。我们评估包含在 23 种语言中的语言，除了未见的任务，我们使用所有可用语言。⁷⁷7 mMMLU: ar, de,
    es, fr, hi, id, it, nl, pt, ro, ru, uk, vi, zh。MGSM: de, es, fr, ja, ru, zh。Belebele:
    {mMMLU} + cs, fa, el, ja, ko, pl, tr。FLORES: {Belebele} + he。Aya 评估允许我们增加：捷克语、希腊语、希伯来语、印地语、印尼语、荷兰语、波斯语、波兰语、罗马尼亚语、俄语、土耳其语、乌克兰语、越南语。'
- en: 3.2 Human Evaluation
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 人工评估
- en: We run human evaluation in Spanish, French, Korean, and Japanese.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在西班牙语、法语、韩语和日语中进行人工评估。
- en: Internal Evaluation Suite
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内部评估套件
- en: 150 diverse prompts designed to be more complex than public evaluation benchmarks.
    As such, we expect greater degradation with increased quantization given the difficulty
    of the samples. Prompts for all four languages are translated by humans from an
    English seed prompt, ensuring that respective language-specific subsets share
    the same prompts.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 150 个多样化的提示，设计得比公共评估基准更复杂。因此，我们预计样本的难度会导致量化增加时的退化更严重。所有四种语言的提示均由人工从英语原始提示翻译而来，确保各语言特定子集共享相同提示。
- en: Aya Dolly-200  (Singh et al., [2024](#bib.bib53))
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Aya Dolly-200 (Singh et al., [2024](#bib.bib53))
- en: We use multilingual data from the Aya Evaluation Suite to assess open-ended
    generation capabilities. For Korean and Japanese, we use prompts from the Aya
    Dolly-200 test set (dolly-machine-translated), which are automatically translated
    from English Dolly-15k (Conover et al., [2023](#bib.bib10)) then human-curated
    to avoid references requiring specific cultural or geographic knowledge. For French
    and Spanish, we use dolly-human-edited, a human post-edited version of dolly-machine-translated.
    For each language, we evaluate using the first 150 prompts.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用来自 Aya 评估套件的多语言数据来评估开放式生成能力。对于韩语和日语，我们使用 Aya Dolly-200 测试集（dolly-machine-translated）中的提示，这些提示从英语
    Dolly-15k (Conover et al., [2023](#bib.bib10)) 自动翻译而来，然后由人工进行编辑以避免需要特定文化或地理知识的参考。对于法语和西班牙语，我们使用
    dolly-human-edited，这是 dolly-machine-translated 的人工后编辑版本。每种语言我们使用前 150 个提示进行评估。
- en: Annotator Statistics
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注释统计
- en: Annotations and translations were completed by native-level speakers of the
    respective languages, each of whom is also fluent in English. Annotators were
    paid by the hour, with compensation above the federal minimum wage of the country
    of employment.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注释和翻译由相应语言的母语者完成，这些人也流利使用英语。注释者按小时支付，报酬高于雇佣国的联邦最低工资。
- en: Annotation Interface
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注释接口
- en: We use a pairwise evaluation setup. Annotators see a prompt and two (shuffled)
    completions of the FP16 model and a quantized variant. They rate each response
    on a 5-point Likert scale, then express a preference between the two model outputs
    (tie, weak preference, strong preference). We encourage annotators to avoid tied
    rankings. Win rates are based on the ranking preferences alone.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用成对评估的设置。标注员会看到一个提示和两个（混合的）FP16模型的完成结果及一个量化变体。他们会在5分制的李克特量表上对每个响应进行评分，然后在两个模型输出之间表达偏好（平局、弱偏好、强偏好）。我们鼓励标注员避免平局排名。胜率仅基于排名偏好。
- en: 3.3 LLM/RM-as-a-Judge
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 LLM/RM作为评判者
- en: Because human evaluation is costly and time-intensive, it is common to use an
    “LLM-as-a-Judge” to rate model completions (e.g. Li et al., [2023b](#bib.bib34);
    Zheng et al., [2023](#bib.bib58)). Reward models (RMs) can also simulate human
    preference. An RM scores multiple completions given the same prompt, and the prompt-completion
    pair with the higher score is deemed preferred. We call this *RM-as-a-Judge*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因为人工评估成本高且耗时，常常使用“LLM作为评判者”来评估模型的完成情况（例如 Li et al., [2023b](#bib.bib34); Zheng
    et al., [2023](#bib.bib58)）。奖励模型（RMs）也可以模拟人类的偏好。一个RM会对给定相同提示的多个完成结果进行评分，得分更高的提示-完成对被认为是更受欢迎的。我们称之为*RM作为评判者*。
- en: We assess quantized model outputs using LLM- and RM-as-a-Judge. In the former,
    an LLM selects a preferred response from a $<$ pairs for each model output, over
    which we calculate win-rate. We report win-rates of quantized models versus the
    FP16 baseline.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用LLM-和RM作为评判者来评估量化模型的输出。在前者中，一个LLM从每个模型输出中选择一个首选响应，我们计算胜率。我们报告量化模型与FP16基线的胜率。
- en: We assess the outputs of quantized models over the Internal Evaluation Suite
    and Aya Dolly-200 described in Section [3.2](#S3.SS2 "3.2 Human Evaluation ‣ 3
    Experiment Set-up ‣ How Does Quantization Affect Multilingual LLMs?"). We use
    the same prompt and completion pairs as in human evaluation, which provides the
    ability to relate LLM/RM-as-a-Judge performance with human evaluation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在内部评估套件和第[3.2节](#S3.SS2 "3.2 人工评估 ‣ 3 实验设置 ‣ 量化如何影响多语言LLMs？")中描述的Aya Dolly-200上评估量化模型的输出。我们使用与人工评估相同的提示和完成对，这使我们能够将LLM/RM作为评判者的表现与人工评估进行关联。
- en: 4 Results
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果
- en: '|  |  | Avg. |  |  |  |  | FLORES | Language Confusion |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 平均值 |  |  |  |  | FLORES | 语言混淆 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  |  | Rel. $\%\Delta$En | Monolingual | Cross-lingual |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 相对 $\%\Delta$En | 单语 | 跨语言 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | FP16 | - | 66.7 | - | 70.6 | - | 37.7 | - | 39.6 | - | 99.2 | - | 91.5
    | - |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | - | 66.7 | - | 70.6 | - | 37.7 | - | 39.6 | - | 99.2 | - | 91.5
    | - |'
- en: '|  | W8 | -0.2% | 66.7 | 0.0% | 69.9 | -1.0% | 37.7 | 0.0% | 39.6 | 0.0% |
    99.2 | 0.0% | 91.2 | -0.3% |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -0.2% | 66.7 | 0.0% | 69.9 | -1.0% | 37.7 | 0.0% | 39.6 | 0.0% |
    99.2 | 0.0% | 91.2 | -0.3% |'
- en: '|  | W8A8-sq | -0.5% | 66.3 | -0.5% | 69.5 | -1.6% | 37.8 | 0.2% | 39.1 | -1.3%
    | 99.2 | 0.0% | 91.5 | 0.1% |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | -0.5% | 66.3 | -0.5% | 69.5 | -1.6% | 37.8 | 0.2% | 39.1 | -1.3%
    | 99.2 | 0.0% | 91.5 | 0.1% |'
- en: '|  | W8A8 | -0.8% | 65.6 | -1.7% | 69.8 | -1.1% | 37.7 | 0.0% | 39.1 | -1.2%
    | 99.4 | 0.2% | 90.4 | -1.2% |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | -0.8% | 65.6 | -1.7% | 69.8 | -1.1% | 37.7 | 0.0% | 39.1 | -1.2%
    | 99.4 | 0.2% | 90.4 | -1.2% |'
- en: '|  | W4-g | -0.9% | 65.7 | -1.4% | 68.6 | -2.9% | 37.8 | 0.4% | 39.4 | -0.5%
    | 99.2 | 0.0% | 90.5 | -1.1% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | W4-g | -0.9% | 65.7 | -1.4% | 68.6 | -2.9% | 37.8 | 0.4% | 39.4 | -0.5%
    | 99.2 | 0.0% | 90.5 | -1.1% |'
- en: '| 103B | W4 | -2.5% | 63.8 | -4.3% | 64.4 | -8.8% | 37.1 | -1.6% | 39.0 | -1.6%
    | 99.3 | 0.1% | 92.8 | 1.4% |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 103B | W4 | -2.5% | 63.8 | -4.3% | 64.4 | -8.8% | 37.1 | -1.6% | 39.0 | -1.6%
    | 99.3 | 0.1% | 92.8 | 1.4% |'
- en: '|  | FP16 | - | 59.4 | - | 49.8 | - | 32.4 | - | 35.5 | - | 98.7 | - | 66.5
    | - |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | - | 59.4 | - | 49.8 | - | 32.4 | - | 35.5 | - | 98.7 | - | 66.5
    | - |'
- en: '|  | W8 | -0.2% | 59.3 | -0.1% | 49.4 | -0.7% | 32.3 | -0.2% | 35.4 | -0.2%
    | 98.8 | 0.1% | 66.3 | -0.2% |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -0.2% | 59.3 | -0.1% | 49.4 | -0.7% | 32.3 | -0.2% | 35.4 | -0.2%
    | 98.8 | 0.1% | 66.3 | -0.2% |'
- en: '|  | W8A8 | 0.2% | 59.3 | -0.2% | 47.1 | -5.5% | 32.9 | 1.6% | 35.8 | 0.9%
    | 99.0 | 0.3% | 68.9 | 3.7% |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | 0.2% | 59.3 | -0.2% | 47.1 | -5.5% | 32.9 | 1.6% | 35.8 | 0.9%
    | 99.0 | 0.3% | 68.9 | 3.7% |'
- en: '| 35B | W4-g | -2.8% | 58.2 | -2.0% | 43.3 | -13.1% | 31.7 | -1.9% | 35.3 |
    -0.7% | 98.3 | -0.4% | 67.1 | 1.0% |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 35B | W4-g | -2.8% | 58.2 | -2.0% | 43.3 | -13.1% | 31.7 | -1.9% | 35.3 |
    -0.7% | 98.3 | -0.4% | 67.1 | 1.0% |'
- en: 'Table 1: Per-dataset average performance across non-English languages for 103B
    and 35B Command models at varying levels of quantization. %$\Delta$ were calculated
    at full precision.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：103B和35B命令模型在不同量化水平下的非英语语言数据集的平均表现。%$\Delta$是按全精度计算的。
- en: '|  |  | Avg. |  |  |  |  | FLORES |  |  | Unseen |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 平均值 |  |  |  |  | FLORES |  |  | 未见 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  |  | Rel. $\%\Delta$En | Belebele | Tasks |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 相对 $\%\Delta$En | Belebele | 任务 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | FP16 | - | 58.2 | - | 51.2 | - | 37.8 | - | 42.9 | - | 77.6 | - | 70.8
    | - |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | - | 58.2 | - | 51.2 | - | 37.8 | - | 42.9 | - | 77.6 | - | 70.8
    | - |'
- en: '|  | W8 | 0.1% | 57.9 | -0.5% | 52.1 | 1.8% | 37.9 | 0.3% | 43.0 | 0.1% | 77.1
    | -0.6% | 70.6 | -0.2% |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 0.1% | 57.9 | -0.5% | 52.1 | 1.8% | 37.9 | 0.3% | 43.0 | 0.1% | 77.1
    | -0.6% | 70.6 | -0.2% |'
- en: '| Aya 35B | W4 | -2.9% | 56.6 | -2.7% | 48.1 | -6.0% | 37.2 | -1.4% | 42.4
    | -1.2% | 73.0 | -5.9% | 70.5 | -0.3% |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Aya 35B | W4 | -2.9% | 56.6 | -2.7% | 48.1 | -6.0% | 37.2 | -1.4% | 42.4
    | -1.2% | 73.0 | -5.9% | 70.5 | -0.3% |'
- en: '|  | FP16 | - | 48.2 | - | 34.7 | - | 34.8 | - | 39.5 | - | 64.8 | - | 67.6
    | - |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | - | 48.2 | - | 34.7 | - | 34.8 | - | 39.5 | - | 64.8 | - | 67.6
    | - |'
- en: '|  | W8 | 0.3% | 47.8 | -0.9% | 35.4 | 2.1% | 34.8 | 0.2% | 39.7 | 0.5% | 64.6
    | -0.3% | 67.6 | 0.1% |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 0.3% | 47.8 | -0.9% | 35.4 | 2.1% | 34.8 | 0.2% | 39.7 | 0.5% | 64.6
    | -0.3% | 67.6 | 0.1% |'
- en: '| Aya 8B | W4 | -3.7% | 46.7 | -3.2% | 32.1 | -7.5% | 34.1 | -1.8% | 39.1 |
    -1.0% | 59.3 | -8.5% | 67.5 | -0.2% |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Aya 8B | W4 | -3.7% | 46.7 | -3.2% | 32.1 | -7.5% | 34.1 | -1.8% | 39.1 |
    -1.0% | 59.3 | -8.5% | 67.5 | -0.2% |'
- en: 'Table 2: Per-dataset average performance across non-English languages for 35B
    and 8B Aya 23 models at varying levels of quantization. %$\Delta$ is relative
    performance vs. FP16\. We follow the evaluation setup of Aryabumi et al. ([2024](#bib.bib5))
    and evaluate on languages in the 23 languages list. On “Unseen Tasks” (XWinograd,
    XCOPA, XStoryCloze), we use all the available languages. See Section [3.1](#S3.SS1
    "3.1 Automatic Evaluation ‣ 3 Experiment Set-up ‣ How Does Quantization Affect
    Multilingual LLMs?") for details and language list.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同量化级别下，35B 和 8B Aya 23 模型在非英语语言数据集上的平均性能。%$\Delta$ 是与 FP16 的相对性能。我们遵循 Aryabumi
    等人（[2024](#bib.bib5)）的评估设置，并在23种语言列表中的语言上进行评估。在“未见任务”（XWinograd, XCOPA, XStoryCloze）中，我们使用所有可用语言。详情和语言列表见第
    [3.1](#S3.SS1 "3.1 Automatic Evaluation ‣ 3 Experiment Set-up ‣ How Does Quantization
    Affect Multilingual LLMs?") 节。
- en: To clearly see the many-faceted impact of quantization, we discuss our results
    by quantization level (§[4.1](#S4.SS1 "4.1 By Quantization Level ‣ 4 Results ‣
    How Does Quantization Affect Multilingual LLMs?")), by task (§[4.2](#S4.SS2 "4.2
    By Task ‣ 4 Results ‣ How Does Quantization Affect Multilingual LLMs?")), by language
    (§[4.3](#S4.SS3 "4.3 By Language ‣ 4 Results ‣ How Does Quantization Affect Multilingual
    LLMs?")), by model size (§[4.4](#S4.SS4 "4.4 By Model Size ‣ 4 Results ‣ How Does
    Quantization Affect Multilingual LLMs?")), and by quantization strategy (§[4.5](#S4.SS5
    "4.5 By Quantization Strategy ‣ 4 Results ‣ How Does Quantization Affect Multilingual
    LLMs?")). We then report LLM-as-a-Judge and RM-as-a-Judge (§[4.6](#S4.SS6 "4.6
    LLM/RM-as-a-Judge ‣ 4 Results ‣ How Does Quantization Affect Multilingual LLMs?"))
    and human evaluation results (§[4.7](#S4.SS7 "4.7 Human Evaluation ‣ 4 Results
    ‣ How Does Quantization Affect Multilingual LLMs?")).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰地看到量化的多方面影响，我们按量化级别（§[4.1](#S4.SS1 "4.1 By Quantization Level ‣ 4 Results
    ‣ How Does Quantization Affect Multilingual LLMs?")）、任务（§[4.2](#S4.SS2 "4.2 By
    Task ‣ 4 Results ‣ How Does Quantization Affect Multilingual LLMs?")）、语言（§[4.3](#S4.SS3
    "4.3 By Language ‣ 4 Results ‣ How Does Quantization Affect Multilingual LLMs?")）、模型大小（§[4.4](#S4.SS4
    "4.4 By Model Size ‣ 4 Results ‣ How Does Quantization Affect Multilingual LLMs?")）和量化策略（§[4.5](#S4.SS5
    "4.5 By Quantization Strategy ‣ 4 Results ‣ How Does Quantization Affect Multilingual
    LLMs?")）来讨论我们的结果。随后，我们报告LLM作为裁判和RM作为裁判（§[4.6](#S4.SS6 "4.6 LLM/RM-as-a-Judge ‣
    4 Results ‣ How Does Quantization Affect Multilingual LLMs?")）以及人类评估结果（§[4.7](#S4.SS7
    "4.7 Human Evaluation ‣ 4 Results ‣ How Does Quantization Affect Multilingual
    LLMs?")）。
- en: 4.1 By Quantization Level
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 按量化级别
- en: How do different levels of quantization affect downstream performance?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 不同级别的量化如何影响下游性能？
- en: Command R and R+
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 命令 R 和 R+
- en: 'In Table [1](#S4.T1 "Table 1 ‣ 4 Results ‣ How Does Quantization Affect Multilingual
    LLMs?"), we aggregate results of each metric for each level of quantization. We
    average scores across languages, then calculate the relative percentage drop from
    FP16.⁹⁹9Ex. For 103B W4-g MGSM, scores were: {de: 71.2, es: 75.7, fr: 69.0, ja:
    58.0, zh: 68.9}, thus the average score was 68.6—a 2.9% drop from FP16 ($\frac{68.6-70.6}{70.6}=-0.029$
    overall for the 103B model. An exception is W8A8 for the 35B, which experiences
    a slight boost overall due to higher performance on translation and language confusion
    evaluations.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[1](#S4.T1 "表 1 ‣ 4 结果 ‣ 量化如何影响多语言大模型？")中，我们汇总了每种量化级别下的每个指标结果。我们计算跨语言的平均分数，然后计算相对于
    FP16 的百分比下降。⁹⁹9Ex。例如，对于103B W4-g MGSM，分数为：{de: 71.2, es: 75.7, fr: 69.0, ja: 58.0,
    zh: 68.9}，因此平均分数为68.6，比 FP16 下降了2.9%（$ \frac{68.6-70.6}{70.6}=-0.029 $，103B模型的总体下降）。例外的是35B的W8A8，由于在翻译和语言混淆评估中的表现更高，整体上有所提升。'
- en: Aya 23 Models
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Aya 23 模型
- en: Table [2](#S4.T2 "Table 2 ‣ 4 Results ‣ How Does Quantization Affect Multilingual
    LLMs?") shows the aggregated results for Aya 23 models on the extended Aya evaluations
    at W8, and W4 quantization. We find a similar trend with Command models where
    W4 often leads to a larger drop compared to W8, consistent across tasks and languages.
    W8, however, does not substantially drop performance in any task.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表[2](#S4.T2 "表 2 ‣ 4 结果 ‣ 量化如何影响多语言大模型？")展示了 Aya 23 模型在扩展的 Aya 评估中，W8 和 W4 量化的汇总结果。我们发现
    Command 模型具有类似的趋势，其中 W4 通常比 W8 导致更大的下降，这在任务和语言之间是一致的。然而，W8 在任何任务中的性能都没有显著下降。
- en: 4.2 By Task
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 按任务分类
- en: Are tasks differently affected by quantization?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是否受到量化的不同影响？
- en: Results here reference Tables [1](#S4.T1 "Table 1 ‣ 4 Results ‣ How Does Quantization
    Affect Multilingual LLMs?") and [2](#S4.T2 "Table 2 ‣ 4 Results ‣ How Does Quantization
    Affect Multilingual LLMs?"), with full raw and relative results in Appendix [A.2](#A1.SS2
    "A.2 Automatic Tasks - Full Results ‣ Appendix A Appendix ‣ How Does Quantization
    Affect Multilingual LLMs?"). Mathematical reasoning as measured by MGSM is strikingly
    affected by quantization. Relative performance of the 35B W4-g model is a dismal
    $-13.1\%$ on the 8B model. mMMLU is the next most greatly degraded task.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 结果参考表格[1](#S4.T1 "表 1 ‣ 4 结果 ‣ 量化如何影响多语言大模型？")和[2](#S4.T2 "表 2 ‣ 4 结果 ‣ 量化如何影响多语言大模型？")，详细的原始数据和相关结果见附录[A.2](#A1.SS2
    "A.2 自动任务 - 完整结果 ‣ 附录 A 附录 ‣ 量化如何影响多语言大模型？")。数学推理的量化影响显著。35B W4-g 模型的相对性能在8B模型上下降了$-13.1\%$。mMMLU是下一个受损最严重的任务。
- en: '|  |  | ar | de | es | fr | it | ja | ko | pt | zh | Avg | Ltn/IE | $\neg$
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ar | de | es | fr | it | ja | ko | pt | zh | 平均 | Ltn/IE | $\neg$ |'
- en: '|  | W8 | 0.0% | 0.1% | 0.0% | 0.0% | 0.0% | 0.1% | 0.0% | -0.4% | -0.2% |
    -0.1% | -0.1% | -0.1% |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 0.0% | 0.1% | 0.0% | 0.0% | 0.0% | 0.1% | 0.0% | -0.4% | -0.2% |
    -0.1% | -0.1% | -0.1% |'
- en: '|  | W8A8-sq | -0.6% | 0.2% | -0.3% | 0.1% | -0.6% | -0.3% | -0.1% | -0.7%
    | -0.8% | -0.3% | -0.3% | -0.4% |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | -0.6% | 0.2% | -0.3% | 0.1% | -0.6% | -0.3% | -0.1% | -0.7%
    | -0.8% | -0.3% | -0.3% | -0.4% |'
- en: '|  | W8A8 | -1.3% | -0.9% | -0.5% | -0.5% | -0.8% | -0.3% | -1.3% | -0.8% |
    -0.9% | -0.8% | -0.7% | -0.9% |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | -1.3% | -0.9% | -0.5% | -0.5% | -0.8% | -0.3% | -1.3% | -0.8% |
    -0.9% | -0.8% | -0.7% | -0.9% |'
- en: '|  | W4-g | -0.8% | -0.2% | -0.4% | 0.1% | -0.4% | -0.4% | -0.6% | -1.2% |
    -0.9% | -0.5% | -0.4% | -0.7% |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | W4-g | -0.8% | -0.2% | -0.4% | 0.1% | -0.4% | -0.4% | -0.6% | -1.2% |
    -0.9% | -0.5% | -0.4% | -0.7% |'
- en: '| 103B | W4 | -1.0% | -0.6% | 0.1% | -0.8% | -1.2% | -1.4% | -2.9% | -0.8%
    | -2.3% | -1.2% | -0.7% | -1.9% |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 103B | W4 | -1.0% | -0.6% | 0.1% | -0.8% | -1.2% | -1.4% | -2.9% | -0.8%
    | -2.3% | -1.2% | -0.7% | -1.9% |'
- en: '|  | W8 | 0.3% | -0.5% | -0.1% | -0.2% | -0.4% | 0.3% | -0.1% | 0.1% | -0.3%
    | -0.1% | -0.2% | 0.0% |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 0.3% | -0.5% | -0.1% | -0.2% | -0.4% | 0.3% | -0.1% | 0.1% | -0.3%
    | -0.1% | -0.2% | 0.0% |'
- en: '|  | W8A8 | 2.0% | 2.5% | 0.7% | 1.0% | 1.2% | 1.1% | 0.9% | 1.4% | 1.0% |
    1.3% | 1.3% | 1.3% |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | 2.0% | 2.5% | 0.7% | 1.0% | 1.2% | 1.1% | 0.9% | 1.4% | 1.0% |
    1.3% | 1.3% | 1.3% |'
- en: '| 35B | W4-g | -1.1% | -1.1% | 0.1% | -0.3% | -0.1% | -2.3% | -1.4% | -0.6%
    | -1.3% | -0.9% | -0.4% | -1.5% |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 35B | W4-g | -1.1% | -1.1% | 0.1% | -0.3% | -0.1% | -2.3% | -1.4% | -0.6%
    | -1.3% | -0.9% | -0.4% | -1.5% |'
- en: 'Table 3: Per-language relative performance (%$\Delta$ are the rest: ar, ja,
    ko, zh.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：按语言分类的相对性能（%$\Delta$ 是其余的：ar, ja, ko, zh）。
- en: '|  |  | de | es | fr | ja | zh | Avg | Ltn/IE | $\neg$ |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  |  | de | es | fr | ja | zh | 平均 | Ltn/IE | $\neg$ |'
- en: '|  | W8 | 0.1% | -0.1% | -0.3% | -0.4% | -0.2% | -0.2% | -0.1% | -0.3% |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 0.1% | -0.1% | -0.3% | -0.4% | -0.2% | -0.2% | -0.1% | -0.3% |'
- en: '|  | W8A8-sq | 0.4% | -0.9% | -0.1% | -0.3% | -1.2% | -0.4% | -0.2% | -0.8%
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | 0.4% | -0.9% | -0.1% | -0.3% | -1.2% | -0.4% | -0.2% | -0.8%
    |'
- en: '|  | W8A8 | -0.4% | -1.0% | -0.6% | -0.1% | -1.3% | -0.7% | -0.6% | -0.7% |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | -0.4% | -1.0% | -0.6% | -0.1% | -1.3% | -0.7% | -0.6% | -0.7% |'
- en: '|  | W4-g | -0.5% | -0.5% | -0.3% | -1.7% | -1.1% | -0.8% | -0.4% | -1.4% |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | W4-g | -0.5% | -0.5% | -0.3% | -1.7% | -1.1% | -0.8% | -0.4% | -1.4% |'
- en: '| 103B | W4 | -2.3% | -1.1% | -1.7% | -3.0% | -3.5% | -2.3% | -1.7% | -3.3%
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 103B | W4 | -2.3% | -1.1% | -1.7% | -3.0% | -3.5% | -2.3% | -1.7% | -3.3%
    |'
- en: '|  | W8 | -0.6% | -0.3% | -0.1% | -0.4% | 0.0% | -0.2% | -0.3% | -0.2% |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -0.6% | -0.3% | -0.1% | -0.4% | 0.0% | -0.2% | -0.3% | -0.2% |'
- en: '|  | W8A8 | 1.3% | -0.6% | 0.3% | -0.3% | 0.0% | 0.1% | 0.3% | -0.2% |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | 1.3% | -0.6% | 0.3% | -0.3% | 0.0% | 0.1% | 0.3% | -0.2% |'
- en: '| 35B | W4-g | -3.7% | -1.8% | -1.7% | -3.8% | -4.0% | -3.0% | -2.4% | -3.9%
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 35B | W4-g | -3.7% | -1.8% | -1.7% | -3.8% | -4.0% | -3.0% | -2.4% | -3.9%
    |'
- en: 'Table 4: Per-language relative performance (%$\Delta$ are the rest: ja, zh.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：按语言分类的相对表现（%$\Delta$为其余：ja, zh）。
- en: 'On FLORES, the 103B Command model is more sensitive to quantization for the
    L2$\rightarrow$En, though we see the opposite for the smaller 35B and Aya 23 models
    at W4. Quantization does not noticeably impact unseen discriminative tasks (XWinograd,
    XCOPA, XStoryCloze: Table [A17](#A1.T17 "Table A17 ‣ A.2 Automatic Tasks - Full
    Results ‣ Appendix A Appendix ‣ How Does Quantization Affect Multilingual LLMs?")).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '在FLORES上，103B Command模型对L2$\rightarrow$En的量化更为敏感，不过我们在较小的35B和Aya 23模型在W4上观察到相反的情况。量化对未见的区分任务（XWinograd,
    XCOPA, XStoryCloze: 表格 [A17](#A1.T17 "Table A17 ‣ A.2 Automatic Tasks - Full Results
    ‣ Appendix A Appendix ‣ How Does Quantization Affect Multilingual LLMs?")）没有明显影响。'
- en: 'Curiously, there are some fleeting performance boosts: an increase of 1.8–2.1%
    on MGSM and mild improvements on FLORES with W8 on Aya models, and a similar translation
    boost of the 35B Command model at W8A8. Quantization generally has no effect or
    causes mild improvement on the monolingual language confusion task, and cross-lingual
    language confusion performance is boosted with greater quantization in some cases.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，有一些短暂的性能提升：在MGSM上提升了1.8–2.1%，以及在Aya模型的W8上FLORES有轻微改进，还有35B Command模型在W8A8上有类似的翻译提升。量化通常对单语语言混淆任务没有影响或导致轻微改进，而在某些情况下，跨语言语言混淆的性能在量化增加时得到了提升。
- en: 4.3 By Language
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 按语言分类
- en: Are languages differently affected by quantization?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 语言是否受到量化的影响不同？
- en: Table [3](#S4.T3 "Table 3 ‣ 4.2 By Task ‣ 4 Results ‣ How Does Quantization
    Affect Multilingual LLMs?") shows performance averaged over mMMLU, FLORES, and
    Language Confusion tasks, with Table [4](#S4.T4 "Table 4 ‣ 4.2 By Task ‣ 4 Results
    ‣ How Does Quantization Affect Multilingual LLMs?") further including MGSM for
    supported languages. Metrics are on different scales, so we average relative change
    (%$\Delta$Ltn/IE).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [3](#S4.T3 "Table 3 ‣ 4.2 By Task ‣ 4 Results ‣ How Does Quantization Affect
    Multilingual LLMs?") 显示了在mMMLU、FLORES和语言混淆任务上的平均表现，表 [4](#S4.T4 "Table 4 ‣ 4.2
    By Task ‣ 4 Results ‣ How Does Quantization Affect Multilingual LLMs?") 进一步包括了支持语言的MGSM。度量标准在不同的量级上，因此我们对相对变化（%$\Delta$Ltn/IE）进行平均。
- en: 'W4-g causes considerable degradation across languages for the 35B Command model.
    A relationship between language and performance is apparent: $\neg$Ltn/IE languages
    typically degrade more. Chinese, Japanese, and Korean are particularly harmed
    by W4 quantization of the 103B. The effect is seen consistently across all automatic
    metrics for Command, with limited exception. Table [5](#S4.T5 "Table 5 ‣ 4.5 By
    Quantization Strategy ‣ 4 Results ‣ How Does Quantization Affect Multilingual
    LLMs?") is discussed more thoroughly in Section [4.5](#S4.SS5 "4.5 By Quantization
    Strategy ‣ 4 Results ‣ How Does Quantization Affect Multilingual LLMs?"), but
    also shows this discrepancy. In the Appendix, we see the same for Aya 23 models
    at W4.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: W4-g在35B Command模型中导致了语言间的显著退化。语言与表现之间的关系是明显的：$\neg$Ltn/IE语言通常退化更严重。中文、日语和韩语在103B的W4量化中受到特别大的影响。这个效应在所有Command模型的自动化指标中一致存在，几乎没有例外。表
    [5](#S4.T5 "Table 5 ‣ 4.5 By Quantization Strategy ‣ 4 Results ‣ How Does Quantization
    Affect Multilingual LLMs?") 在第 [4.5](#S4.SS5 "4.5 By Quantization Strategy ‣ 4
    Results ‣ How Does Quantization Affect Multilingual LLMs?")节中有更详细的讨论，但也显示了这种差异。在附录中，我们在W4的Aya
    23模型中看到相同的现象。
- en: Interestingly, W8A8 of the 35B Command model helps on average across all languages.
    The magnitude is primarily due to an increase on cross-lingual language confusion.
    W8 also aids Aya 23 on MGSM (Table [A6](#A1.T6 "Table A6 ‣ A.2 Automatic Tasks
    - Full Results ‣ Appendix A Appendix ‣ How Does Quantization Affect Multilingual
    LLMs?")) for $\neg$Ltn/IE languages, and across languages on FLORES (Table [A16](#A1.T16
    "Table A16 ‣ A.2 Automatic Tasks - Full Results ‣ Appendix A Appendix ‣ How Does
    Quantization Affect Multilingual LLMs?")).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，35B Command模型的W8A8在所有语言中平均都有帮助。这个幅度主要是由于跨语言语言混淆的增加。W8还在MGSM中帮助了Aya 23（表
    [A6](#A1.T6 "Table A6 ‣ A.2 Automatic Tasks - Full Results ‣ Appendix A Appendix
    ‣ How Does Quantization Affect Multilingual LLMs?")）对于$\neg$Ltn/IE语言，以及在FLORES（表
    [A16](#A1.T16 "Table A16 ‣ A.2 Automatic Tasks - Full Results ‣ Appendix A Appendix
    ‣ How Does Quantization Affect Multilingual LLMs?")）中跨语言的表现。
- en: 4.4 By Model Size
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 通过模型规模
- en: How do model size and quantization level interact?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 模型规模和量化水平如何相互作用？
- en: 'Across evaluations at the most extreme quantization (W4/W4-g), smaller models
    are more sensitive: W4-g variants of 103B and 35B Command record $-0.9\%$). (Refer
    back to Tables [1](#S4.T1 "Table 1 ‣ 4 Results ‣ How Does Quantization Affect
    Multilingual LLMs?") and [2](#S4.T2 "Table 2 ‣ 4 Results ‣ How Does Quantization
    Affect Multilingual LLMs?").)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在最极端的量化（W4/W4-g）评估中，更小的模型更为敏感：103B和35B Command的W4-g变体记录了$-0.9\%$。 （请参见表格 [1](#S4.T1
    "Table 1 ‣ 4 Results ‣ How Does Quantization Affect Multilingual LLMs?") 和 [2](#S4.T2
    "Table 2 ‣ 4 Results ‣ How Does Quantization Affect Multilingual LLMs?")。）
- en: 4.5 By Quantization Strategy
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 通过量化策略
- en: How do techniques like SmoothQuant and group-wise scaling affect downstream
    performance?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 像SmoothQuant和分组缩放这样的技术如何影响下游性能？
- en: '|  |  |  |  |  |  |  | FLORES | Language Confusion |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  | FLORES | 语言混淆 |'
- en: '|  | Avg. Rel. % | mMMLU | MGSM | En $\rightarrow$ En | Monolingual | Cross-lingual
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | Avg. Rel. % | mMMLU | MGSM | En $\rightarrow$ En | 单语 | 跨语言 |'
- en: '|  | Ltn/IE | $\neg$ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | Ltn/IE | $\neg$ |'
- en: '| W8A8 | -0.7% | -1.0% | -1.3% | -2.1% | -0.9% | -1.3% | -0.1% | 0.1% | -1.0%
    | -1.6% | 0.0% | 0.4% | -0.9% | -1.6% |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | -0.7% | -1.0% | -1.3% | -2.1% | -0.9% | -1.3% | -0.1% | 0.1% | -1.0%
    | -1.6% | 0.0% | 0.4% | -0.9% | -1.6% |'
- en: '| W8A8-sq | -0.4% | -0.7% | -0.4% | -0.8% | -1.3% | -1.9% | 0.2% | 0.0% | -1.1%
    | -1.6% | -0.1% | 0.1% | 0.1% | 0.0% |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| W8A8-sq | -0.4% | -0.7% | -0.4% | -0.8% | -1.3% | -1.9% | 0.2% | 0.0% | -1.1%
    | -1.6% | -0.1% | 0.1% | 0.1% | 0.0% |'
- en: '| W4 | -1.9% | -3.3% | -3.9% | -4.9% | -8.0% | -10.2% | -1.3% | -2.0% | -1.1%
    | -2.3% | 0.1% | 0.1% | 2.9% | -0.4% |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| W4 | -1.9% | -3.3% | -3.9% | -4.9% | -8.0% | -10.2% | -1.3% | -2.0% | -1.1%
    | -2.3% | 0.1% | 0.1% | 2.9% | -0.4% |'
- en: '| W4-g | -0.6% | -1.4% | -1.1% | -1.9% | -1.8% | -4.9% | 0.2% | 0.7% | -0.3%
    | -0.8% | 0.1% | -0.1% | -0.9% | -1.3% |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| W4-g | -0.6% | -1.4% | -1.1% | -1.9% | -1.8% | -4.9% | 0.2% | 0.7% | -0.3%
    | -0.8% | 0.1% | -0.1% | -0.9% | -1.3% |'
- en: 'Table 5: Effect of mitigation strategies on W8A8 and W4 quantization on the
    103B model. Percentage points off FP16 baseline for W8A8-sq vs. naive W8A8 and
    W4-g vs. W4, broken down by Latin-script/Indo-European languages (Ltn/IE) versus
    others ($\neg$). Avg. Rel. % reports averaged performance all datasets.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：W8A8和W4量化对103B模型的影响。W8A8-sq与朴素W8A8及W4-g与W4的FP16基线百分比差异，按拉丁文字母/印欧语言（Ltn/IE）与其他语言（$\neg$）分类。Avg.
    Rel. % 报告了所有数据集的平均性能。
- en: '|  |  | fr | es | ja | ko | Avg | Ltn/IE | $\neg$ |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  |  | fr | es | ja | ko | Avg | Ltn/IE | $\neg$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  |  | LLM | RM | LLM | RM | LLM | RM | LLM | RM | LLM | RM | LLM | RM | LLM
    | RM |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLM | RM | LLM | RM | LLM | RM | LLM | RM | LLM | RM | LLM | RM | LLM
    | RM |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
- en: '|  | W8 | 1.0% | -0.7% | -10.2% | 7.5% | -5.4% | 5.4% | 7.5% | -5.8% | -1.8%
    | 1.6% | -4.6% | 3.4% | 1.0% | -0.2% |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 1.0% | -0.7% | -10.2% | 7.5% | -5.4% | 5.4% | 7.5% | -5.8% | -1.8%
    | 1.6% | -4.6% | 3.4% | 1.0% | -0.2% |'
- en: '|  | W8A8-sq | -18.4% | -5.1% | -3.7% | 4.1% | 2.0% | 4.7% | 3.7% | -5.1% |
    -4.1% | -0.3% | -11.0% | -0.5% | 2.9% | -0.2% |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | -18.4% | -5.1% | -3.7% | 4.1% | 2.0% | 4.7% | 3.7% | -5.1% |
    -4.1% | -0.3% | -11.0% | -0.5% | 2.9% | -0.2% |'
- en: '|  | W4-g | -10.5% | -17.0% | -16.6% | 2.0% | -15.3% | 0.0% | -5.8% | -15.6%
    | -12.1% | -7.7% | -13.6% | -7.5% | -10.5% | -7.8% |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | W4-g | -10.5% | -17.0% | -16.6% | 2.0% | -15.3% | 0.0% | -5.8% | -15.6%
    | -12.1% | -7.7% | -13.6% | -7.5% | -10.5% | -7.8% |'
- en: '| Internal | W4 | -30.2% | -20.4% | -33.0% | -17.0% | -21.7% | -20.0% | -18.6%
    | -27.6% | -25.9% | -21.2% | -31.6% | -18.7% | -20.2% | -23.8% |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| Internal | W4 | -30.2% | -20.4% | -33.0% | -17.0% | -21.7% | -20.0% | -18.6%
    | -27.6% | -25.9% | -21.2% | -31.6% | -18.7% | -20.2% | -23.8% |'
- en: '|  | W8 | -1.3% | 2.0% | 7.3% | -4.0% | -6.0% | -5.3% | 2.7% | 2.0% | 0.7%
    | -1.3% | 3.0% | -1.0% | -1.7% | -1.7% |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -1.3% | 2.0% | 7.3% | -4.0% | -6.0% | -5.3% | 2.7% | 2.0% | 0.7%
    | -1.3% | 3.0% | -1.0% | -1.7% | -1.7% |'
- en: '|  | W8A8-sq | -15.3% | -8.7% | 8.7% | -8.0% | -1.3% | 1.3% | -8.0% | -4.7%
    | -4.0% | -5.0% | -3.3% | -8.3% | -4.7% | -1.7% |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | -15.3% | -8.7% | 8.7% | -8.0% | -1.3% | 1.3% | -8.0% | -4.7%
    | -4.0% | -5.0% | -3.3% | -8.3% | -4.7% | -1.7% |'
- en: '|  | W8A8 | -3.4% | 2.7% | 13.3% | -3.3% | 2.7% | -1.3% | 5.3% | -3.3% | 4.5%
    | -1.3% | 5.0% | -0.3% | 4.0% | -2.3% |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | -3.4% | 2.7% | 13.3% | -3.3% | 2.7% | -1.3% | 5.3% | -3.3% | 4.5%
    | -1.3% | 5.0% | -0.3% | 4.0% | -2.3% |'
- en: '| Dolly | W4-g | -7.4% | -2.7% | -4.0% | 4.7% | -15.3% | -15.3% | -11.3% |
    -5.3% | -9.5% | -4.7% | -5.7% | 1.0% | -13.3% | -10.3% |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Dolly | W4-g | -7.4% | -2.7% | -4.0% | 4.7% | -15.3% | -15.3% | -11.3% |
    -5.3% | -9.5% | -4.7% | -5.7% | 1.0% | -13.3% | -10.3% |'
- en: 'Table 6: Relative performance vs. FP16 of 103B quantized models according to
    LLM/RM-as-a-Judge over Internal and Aya Dolly subsampled test sets. Raw win-rates
    in Table [A20](#A1.T20 "Table A20 ‣ A.3 RM/LLM-as-a-Judge and Human Evaluation
    - Full Results ‣ Appendix A Appendix ‣ How Does Quantization Affect Multilingual
    LLMs?").'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：103B 量化模型相对于 FP16 的相对性能，根据 LLM/RM-as-a-Judge 对内部和 Aya Dolly 子样本测试集的评估。原始胜率见表
    [A20](#A1.T20 "Table A20 ‣ A.3 RM/LLM-as-a-Judge 和人工评估 - 完整结果 ‣ Appendix A Appendix
    ‣ Quantization 对多语言 LLM 的影响？")。
- en: Table [5](#S4.T5 "Table 5 ‣ 4.5 By Quantization Strategy ‣ 4 Results ‣ How Does
    Quantization Affect Multilingual LLMs?") shows the effect of using SmoothQuant
    and Group-Wise scaling strategies. We evaluate variants of the 103B Command model
    with SmoothQuant (W8A8-sq), and a more naive W4 variant using per-column quantization
    instead of group-wise scaling. We compare W8A8-sq to W8A8, and W4-g to W4.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [5](#S4.T5 "Table 5 ‣ 4.5 By Quantization Strategy ‣ 4 Results ‣ Quantization
    对多语言 LLM 的影响？") 显示了使用 SmoothQuant 和 Group-Wise scaling 策略的效果。我们评估了 103B Command
    模型的 SmoothQuant (W8A8-sq) 变体，以及使用按列量化而非组式缩放的更天真的 W4 变体。我们将 W8A8-sq 与 W8A8 进行比较，将
    W4-g 与 W4 进行比较。
- en: On average and across mMMLU, MGSM, and FLORES, Group-Wise scaling greatly improves
    over column-wise W4, recovering over 6 percentage points lost on MGSM for Ltn/IE
    languages. SmoothQuant has a similar effect on average and for mMMLU, though to
    a lesser degree. That said, SmoothQuant harms MGSM scores slightly, and Group-Wise
    scaling degrades cross-lingual language confusion. We again observe that $\neg\text{Ltn/IE}$
    languages suffer more in nearly all cases.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 平均来看，在 mMMLU、MGSM 和 FLORES 上，Group-Wise scaling 显著改善了列式 W4，恢复了在 MGSM 上 Ltn/IE
    语言丢失的超过 6 个百分点。SmoothQuant 对平均值和 mMMLU 有类似的效果，但程度较小。也就是说，SmoothQuant 对 MGSM 分数有轻微的负面影响，而
    Group-Wise scaling 加剧了跨语言混淆。我们再次观察到，$\neg\text{Ltn/IE}$ 语言在几乎所有情况下都遭受更多损害。
- en: 'On cross-lingual language confusion, strategies aimed to retain performance
    have different effects: SmoothQuant recovers all lost from naive W8A8, but Group-Wise
    scaling is actively damaging. In contrast, W4 benefits Ltn/IE and Arabic on cross-lingual
    language confusion, but worsens the rest.^(11)^(11)11Full results in are Table
    [A19](#A1.T19 "Table A19 ‣ A.2 Automatic Tasks - Full Results ‣ Appendix A Appendix
    ‣ How Does Quantization Affect Multilingual LLMs?").'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在跨语言混淆方面，旨在保持性能的策略效果各异：SmoothQuant 恢复了来自天真的 W8A8 的所有损失，但 Group-Wise scaling
    则积极造成损害。相比之下，W4 对 Ltn/IE 和阿拉伯语的跨语言混淆有益，但对其他语言则有所恶化。^(11)^(11)11完整结果见表 [A19](#A1.T19
    "Table A19 ‣ A.2 Automatic Tasks - Full Results ‣ Appendix A Appendix ‣ Quantization
    对多语言 LLM 的影响？")。
- en: Thus, while the quantization strategies tend to aid performance overall, there
    may be adverse effects on specific tasks. More research is needed to understand
    this, but it is intriguing to consider the effect that lower-precision might have
    on the ability to produce output in a desired language, and maintain that language
    once decoding begins.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管量化策略总体上有助于性能，但可能对特定任务产生不利影响。需要更多研究来理解这一点，但考虑到低精度可能对生成期望语言的能力以及解码开始后的语言保持产生的影响是很有趣的。
- en: 4.6 LLM/RM-as-a-Judge
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 LLM/RM-as-a-Judge
- en: 'Table [6](#S4.T6 "Table 6 ‣ 4.5 By Quantization Strategy ‣ 4 Results ‣ How
    Does Quantization Affect Multilingual LLMs?") shows relative performance of quantized
    variants of the 103B Command model evaluated with LLM- and RM-as-a-Judge.^(12)^(12)12Calculation:
    $\frac{\text{Quantized Win Rate}-50}{50}$Ltn/IE languages on Dolly with W4-g,
    and French with W8A8-sq. On average across languages, the LLM and RM agree on
    the ranking of model quality over Internal. Results on the easier Dolly test set
    are less clear-cut: The LLM reports greater degradation for *Internal* than Dolly
    overall, but the RM disagrees for W8 and W8A8-sq. Perhaps Dolly prompts are easy
    enough that models output similar responses, creating more noise in the judgments;
    future work could examine this hypothesis. Furthermore, on multiple instances,
    the LLM and RM disagree on whether performance improves or worsens, given the
    same setting. Comparisons between the two differing methods of automated evaluation
    are worthy of further study.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6](#S4.T6 "Table 6 ‣ 4.5 By Quantization Strategy ‣ 4 Results ‣ How Does
    Quantization Affect Multilingual LLMs?") 显示了用 LLM 和 RM 作为裁判对 103B Command 模型的量化变体的相对性能。^(12)^(12)12
    计算方法：$\frac{\text{Quantized Win Rate}-50}{50}$ Ltn/IE 语言在 W4-g 上的 Dolly 上，法语在
    W8A8-sq 上。跨语言的平均值，LLM 和 RM 对内部模型质量的排名一致。在更简单的 Dolly 测试集上的结果不那么明确：LLM 报告称*内部*的退化比
    Dolly 更严重，但 RM 对 W8 和 W8A8-sq 持不同意见。也许 Dolly 提示足够简单，以至于模型输出类似的响应，从而在判断中产生更多噪声；未来的工作可以检验这一假设。此外，在多个实例中，LLM
    和 RM 对于在相同设置下性能是否改善或恶化存在分歧。不同自动评估方法之间的比较值得进一步研究。
- en: 4.7 Human Evaluation
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 人类评估
- en: '|  |  | fr | es | ja | ko | Avg | Ltn/IE | $\neg$ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  |  | fr | es | ja | ko | Avg | Ltn/IE | $\neg$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | W8 | -7.4% | 0.6% | 7.4% | -12.0% | -2.8% | -3.4% | -2.3% |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -7.4% | 0.6% | 7.4% | -12.0% | -2.8% | -3.4% | -2.3% |'
- en: '|  | W8A8-sq | -9.4% | -7.4% | -2.0% | 4.0% | -3.7% | -8.4% | 1.0% |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | -9.4% | -7.4% | -2.0% | 4.0% | -3.7% | -8.4% | 1.0% |'
- en: '| Internal | W4-g | -16.6% | -4.6% | -16.0% | -4.6% | -10.5% | -10.6% | -10.3%
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Internal | W4-g | -16.6% | -4.6% | -16.0% | -4.6% | -10.5% | -10.6% | -10.3%
    |'
- en: '|  | W8 | 0.6% | -5.4% | 12.0% | 0.0% | 1.8% | -2.4% | 6.0% |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 0.6% | -5.4% | 12.0% | 0.0% | 1.8% | -2.4% | 6.0% |'
- en: '|  | W8A8-sq | -7.4% | -8.6% | 0.0% | -3.4% | -4.8% | -8.0% | -1.7% |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | -7.4% | -8.6% | 0.0% | -3.4% | -4.8% | -8.0% | -1.7% |'
- en: '| Dolly | W4-g | -9.4% | -1.4% | 2.6% | -8.0% | -4.1% | -5.4% | -2.7% |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Dolly | W4-g | -9.4% | -1.4% | 2.6% | -8.0% | -4.1% | -5.4% | -2.7% |'
- en: 'Table 7: Relative performance vs. FP16 of 103B quantized models according to
    human evaluators over Internal and Aya Dolly subsampled test sets.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：根据人类评估者对内部和 Aya Dolly 子样本测试集的评估，103B 量化模型相对于 FP16 的相对性能。
- en: Human evaluation paints a similar picture in Table [7](#S4.T7 "Table 7 ‣ 4.7
    Human Evaluation ‣ 4 Results ‣ How Does Quantization Affect Multilingual LLMs?"),
    with some outliers. Average performance drops steadily across evaluated languages
    on the *Internal* test set, which has more difficult prompts. The sharpest decline
    is in French, with $-16.6\%$ with more extreme quantization. Interestingly, human
    annotators generally prefer outputs of quantized models on Dolly prompts in Japanese,
    too, but disprefer those in other languages. We see more pronounced degradation
    on *Internal* overall, with an average relative drop of 5.7% versus 2.4% for Dolly.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 人类评估在表 [7](#S4.T7 "Table 7 ‣ 4.7 Human Evaluation ‣ 4 Results ‣ How Does Quantization
    Affect Multilingual LLMs?") 中描绘了类似的情况，但有一些异常值。被评估语言的*内部*测试集的平均性能在逐步下降，这些测试集有更难的提示。最显著的下降发生在法语中，量化程度更极端，下降幅度为$-16.6\%$。有趣的是，人类标注者通常更喜欢量化模型在
    Dolly 提示中的日语输出，但不喜欢其他语言的输出。我们看到*内部*的退化更为明显，整体平均相对下降为 5.7% 对比 Dolly 的 2.4%。
- en: 5 Related Work
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Impact of Compression on Multilingual Tasks
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 压缩对多语言任务的影响
- en: There is a scarcity of research examining the impact of compression and quantization
    on multilingual tasks. Paglieri et al. ([2024](#bib.bib46)) study the impact of
    multilingual calibration sets on quantization, but their evaluation is English-only.
    Ramesh et al. ([2023](#bib.bib50)) study the effect of compression on multilingual
    model fairness in terms of classification accuracy, showing that while monolingual
    evaluation indicates a negative impact, multilingual evaluation differs across
    languages and dimensions. Kharazmi et al. ([2023](#bib.bib28)) show that recovering
    compression-caused performance loss of LSTMs is harder in a multilingual setting
    than monolingually. In machine translation, Diddee et al. ([2022](#bib.bib17))
    show that distillation has a varied effect by language due to dependence on priors
    such as amount of synthetic data used and confidence of the teacher models, while
    quantization exhibits more consistent performance trends across languages. Our
    work is the first, to our knowledge, to study the effect of quantization on LLMs
    and for open-ended generation.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 目前对压缩和量化对多语言任务影响的研究较少。Paglieri et al. ([2024](#bib.bib46))研究了多语言校准集对量化的影响，但他们的评估仅限于英语。Ramesh
    et al. ([2023](#bib.bib50))研究了压缩对多语言模型公平性的影响，显示尽管单语言评估表明负面影响，但多语言评估在不同语言和维度上存在差异。Kharazmi
    et al. ([2023](#bib.bib28))展示了在多语言环境中恢复压缩导致的LSTM性能损失比单语言环境更为困难。在机器翻译中，Diddee et
    al. ([2022](#bib.bib17))表明，蒸馏对语言的效果因依赖于合成数据量和教师模型的信心而有所不同，而量化则在不同语言间表现出更一致的性能趋势。据我们所知，我们的工作首次研究了量化对LLMs的影响以及对开放式生成的影响。
- en: More broadly, multilingual data is an example of long tail data. Prior work
    shows that compression techniques like quantization and sparsity amplify disparate
    treatment of long-tail rare features (Hooker et al., [2019](#bib.bib24); Ahia
    et al., [2021](#bib.bib2); Ogueji et al., [2022](#bib.bib44); Hooker et al., [2020](#bib.bib25)).
    Ogueji et al. ([2022](#bib.bib44)) show that depending on how out of distribution
    the task data is, sparsity-based compression can sometimes avoid overfitting to
    the training data, making a model better suited to the downstream task. Ahia et al.
    ([2021](#bib.bib2)) find that sparsity preserves machine translation performance
    on frequent sentences, but disparately impacts infrequent sentences.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛地说，多语言数据是长尾数据的一个例子。以往的研究表明，像量化和稀疏性这样的压缩技术会放大对长尾稀有特征的不均衡处理（Hooker et al.,
    [2019](#bib.bib24); Ahia et al., [2021](#bib.bib2); Ogueji et al., [2022](#bib.bib44);
    Hooker et al., [2020](#bib.bib25)）。Ogueji et al. ([2022](#bib.bib44))表明，根据任务数据的分布情况，基于稀疏性的压缩有时可以避免对训练数据的过拟合，从而使模型更适合下游任务。Ahia
    et al. ([2021](#bib.bib2))发现稀疏性在常见句子的机器翻译性能上有保留，但对不常见句子的影响则不均衡。
- en: Quantization of LLMs
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs的量化
- en: A recent line of work has emerged on techniques to improve performance of quantized
    LLMs, with the sole focus on English models and data for tuning and evaluation
    (Ahmadian et al., [2024](#bib.bib4); Dettmers et al., [2022](#bib.bib15); Xiao
    et al., [2023](#bib.bib56); Bondarenko et al., [2024](#bib.bib8); Gong et al.,
    [2024](#bib.bib21)). Even the most recent (Li et al., [2024](#bib.bib33); Liu
    et al., [2024](#bib.bib37)) omit the multilingual dimension without acknowledging
    the limitation. Multilinguality and compression are both integral parts of LLMs,
    and our work explores this new territory.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最近出现了一系列研究，专注于提高量化大语言模型（LLMs）性能的技术，主要关注英语模型及其调优和评估数据（Ahmadian et al., [2024](#bib.bib4);
    Dettmers et al., [2022](#bib.bib15); Xiao et al., [2023](#bib.bib56); Bondarenko
    et al., [2024](#bib.bib8); Gong et al., [2024](#bib.bib21)）。即使是最新的研究（Li et al.,
    [2024](#bib.bib33); Liu et al., [2024](#bib.bib37)）也忽略了多语言维度而没有承认这一限制。多语言性和压缩都是LLMs的不可或缺的部分，我们的工作探索了这一新领域。
- en: Model design choices
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型设计选择
- en: We consider how design choices such as quantization impact performance for users
    of different languages. A wider body of work examines how design choices impact
    performance on underrepresented features or subgroups. Zhuang et al. ([2021](#bib.bib59))
    and Nelaturu et al. ([2023](#bib.bib42)) find that hardware choice incurs disparate
    impact on underrepresented features. Wang et al. ([2022](#bib.bib55)) establish
    that distillation imposes similar trade-offs, but the disproportionate harm to
    the long-tail could be mitigated by modifying the student-teacher objective. Ko
    et al. ([2023](#bib.bib30)) evaluate the positive role of ensembling disproportionately
    favoring underrepresented attributes. Bagdasaryan and Shmatikov ([2019](#bib.bib6))
    show that differential privacy techniques like gradient clipping and noise injection
    disproportionately impact underrepresented features.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了设计选择（如量化）如何影响不同语言用户的性能。更广泛的研究考察了设计选择对代表性不足特征或子群体的性能影响。Zhuang等（[2021](#bib.bib59)）和Nelaturu等（[2023](#bib.bib42)）发现，硬件选择对代表性不足特征产生了不同的影响。Wang等（[2022](#bib.bib55)）确定了蒸馏带来的类似权衡，但通过修改学生-教师目标可以减轻对长尾特征的过度伤害。Ko等（[2023](#bib.bib30)）评估了集合方法在偏向代表性不足属性方面的积极作用。Bagdasaryan和Shmatikov（[2019](#bib.bib6)）展示了差分隐私技术如梯度裁剪和噪声注入对代表性不足特征产生了不同的影响。
- en: 6 Conclusion & Future Work
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: 'We examine widely adopted quantization techniques for model compression and
    ask, How does quantization impact different languages? We perform an extensive
    study of quantization in state-of-the-art multilingual LLMs—from 8 billion to
    103 billion parameters—in 20+ languages using automatic metrics, LLM-as-a-Judge,
    RM-as-a-Judge, and human evaluation. We find that: (1) Damage from quantization
    is much worse than appears from automatic metrics: even when not observed automatically,
    human evaluators notice it. (2) Quantization affects languages to varying degrees,
    with non-Latin script languages more severely affected on automatic benchmarks.
    (3) Challenging tasks degrade fast and severely: math performance is strikingly
    reduced, as are responses on realistic challenging prompts judged by humans. On
    a bright note, quantization occasionally brings performance benefits.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们审视了广泛采用的模型压缩量化技术，并探讨了量化如何影响不同语言。我们对最先进的多语言LLM进行了一项广泛的研究，涵盖了从80亿到1030亿参数的模型，在20多种语言中使用自动化指标、LLM作为裁判、RM作为裁判和人工评估。我们发现：(1)
    量化造成的损害比自动化指标显示的要严重得多：即使自动化没有察觉，人类评估者也会注意到。(2) 量化对语言的影响程度不同，非拉丁文字的语言在自动化基准测试中受到的影响更为严重。(3)
    挑战性任务的性能迅速而严重地下降：数学表现显著降低，人类判断下的实际挑战性提示的反应也有所下降。值得欣慰的是，量化偶尔带来性能提升。
- en: Our results urge attention to multilingual performance at all stages of system
    design. Researchers might extend our work to consider the impact of other decisions
    on multilingual performance, including on languages excluded from training and
    out-of-distribution tasks. By being mindful of the impact on long-tail features,
    we’ll build better systems to serve the world.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果提醒大家在系统设计的各个阶段关注多语言性能。研究人员可以扩展我们的工作，考虑其他决策对多语言性能的影响，包括对训练中排除的语言和超出分布任务的影响。通过关注对长尾特征的影响，我们将构建更好的系统来服务全球。
- en: 7 Limitations
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制
- en: Generality of findings
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 研究结果的普遍性
- en: Due to the number of methods, languages, and benchmarks we examine, we focus
    our evaluation on models from two families (Command R and Aya). As we observe
    similar trends across these models, our findings are likely to generalize to other
    LLMs. Nevertheless, models that have been optimized differently or trained with
    a focus on specific tasks such as code or mathematical reasoning may behave differently.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们检查了许多方法、语言和基准，我们将评估重点放在了两个模型系列（Command R和Aya）上。由于我们在这些模型中观察到了类似的趋势，我们的发现可能对其他LLM也具有普遍性。然而，经过不同优化或专注于特定任务（如代码或数学推理）的模型可能会表现不同。
- en: Under-represented languages
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代表性不足的语言
- en: For our study, we focused on languages that were supported by the models we
    evaluated. Performance deterioration is likely even larger for languages that
    are not or severely under-represented in the pre-training data. For such languages,
    evaluation is also more challenging due to poor availability of benchmark data
    and human annotators.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，我们关注了模型支持的语言。对于那些在预训练数据中没有或严重不足的语言，性能恶化可能更为严重。对于这些语言，由于基准数据和人工标注者的可用性差，评估也更具挑战性。
- en: References
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等. 2023. Gpt-4 技术报告。*arXiv 预印本 arXiv:2303.08774*。
- en: 'Ahia et al. (2021) Orevaoghene Ahia, Julia Kreutzer, and Sara Hooker. 2021.
    [The low-resource double bind: An empirical study of pruning for low-resource
    machine translation](https://arxiv.org/abs/2110.03036). *Preprint*, arXiv:2110.03036.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahia 等（2021）Orevaoghene Ahia, Julia Kreutzer, 和 Sara Hooker. 2021. [低资源双重困境：低资源机器翻译的修剪实证研究](https://arxiv.org/abs/2110.03036)。*预印本*，arXiv:2110.03036。
- en: Ahia et al. (2023) Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai,
    David R. Mortensen, Noah A. Smith, and Yulia Tsvetkov. 2023. [Do all languages
    cost the same? tokenization in the era of commercial language models](https://arxiv.org/abs/2305.13707).
    *Preprint*, arXiv:2305.13707.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahia 等（2023）Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R.
    Mortensen, Noah A. Smith, 和 Yulia Tsvetkov. 2023. [所有语言的代价相同吗？商业语言模型时代的标记化](https://arxiv.org/abs/2305.13707)。*预印本*，arXiv:2305.13707。
- en: Ahmadian et al. (2024) Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh,
    Stephen Gou, Phil Blunsom, Ahmet Üstün, and Sara Hooker. 2024. Intriguing properties
    of quantization at scale. In *Proceedings of the 37th International Conference
    on Neural Information Processing Systems*, NIPS ’23, Red Hook, NY, USA. Curran
    Associates Inc.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmadian 等（2024）Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh,
    Stephen Gou, Phil Blunsom, Ahmet Üstün, 和 Sara Hooker. 2024. 大规模量化的有趣属性。收录于 *第37届国际神经信息处理系统会议论文集*，NIPS
    ’23，纽约州雷德胡克，美国。Curran Associates Inc.
- en: 'Aryabumi et al. (2024) Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh
    Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos,
    Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia
    Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet Üstün,
    and Sara Hooker. 2024. [Aya 23: Open weight releases to further multilingual progress](https://arxiv.org/abs/2405.15032).
    *Preprint*, arXiv:2405.15032.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aryabumi 等（2024）Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David
    Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern
    Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer,
    Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet Üstün, 和 Sara Hooker.
    2024. [Aya 23：开放权重发布以进一步推进多语言进展](https://arxiv.org/abs/2405.15032)。*预印本*，arXiv:2405.15032。
- en: Bagdasaryan and Shmatikov (2019) Eugene Bagdasaryan and Vitaly Shmatikov. 2019.
    [Differential privacy has disparate impact on model accuracy](https://arxiv.org/abs/1905.12101).
    *Preprint*, arXiv:1905.12101.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagdasaryan 和 Shmatikov（2019）Eugene Bagdasaryan 和 Vitaly Shmatikov. 2019. [差分隐私对模型准确性的影响不同](https://arxiv.org/abs/1905.12101)。*预印本*，arXiv:1905.12101。
- en: 'Bandarkar et al. (2023) Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel
    Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan,
    Luke Zettlemoyer, and Madian Khabsa. 2023. [The belebele benchmark: a parallel
    reading comprehension dataset in 122 language variants](https://arxiv.org/abs/2308.16884).
    *Preprint*, arXiv:2308.16884.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bandarkar 等（2023）Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe,
    Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer,
    和 Madian Khabsa. 2023. [Belebele 基准：122种语言变体的平行阅读理解数据集](https://arxiv.org/abs/2308.16884)。*预印本*，arXiv:2308.16884。
- en: 'Bondarenko et al. (2024) Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
    2024. Quantizable transformers: Removing outliers by helping attention heads do
    nothing. *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bondarenko 等（2024）Yelysei Bondarenko, Markus Nagel, 和 Tijmen Blankevoort. 2024.
    可量化的变压器：通过帮助注意力头不作为来去除异常值。*神经信息处理系统进展*，36。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等（2021）Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo
    Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano
    等. 2021. 训练验证器以解决数学词汇问题。*arXiv 预印本 arXiv:2110.14168*。
- en: 'Conover et al. (2023) Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,
    Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.
    2023. [Free dolly: Introducing the world’s first truly open instruction-tuned
    llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conover 等（2023）Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan,
    Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, 和 Reynold Xin. 2023. [Free
    dolly：介绍全球首个真正开放的指令调优 LLM](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)。
- en: 'Costa-jussà et al. (2022a) Marta R Costa-jussà, James Cross, Onur Çelebi, Maha
    Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel
    Licht, Jean Maillard, et al. 2022a. No language left behind: Scaling human-centered
    machine translation. *arXiv preprint arXiv:2207.04672*.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Costa-jussà et al. (2022a) Marta R Costa-jussà, James Cross, Onur Çelebi, Maha
    Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel
    Licht, Jean Maillard, 等。2022a年。无语言被落下：扩展以人为中心的机器翻译。*arXiv预印本 arXiv:2207.04672*。
- en: 'Costa-jussà et al. (2022b) Marta R Costa-jussà, James Cross, Onur Çelebi, Maha
    Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel
    Licht, Jean Maillard, et al. 2022b. No language left behind: Scaling human-centered
    machine translation. *arXiv preprint arXiv:2207.04672*.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Costa-jussà et al. (2022b) Marta R Costa-jussà, James Cross, Onur Çelebi, Maha
    Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel
    Licht, Jean Maillard, 等。2022b年。无语言被落下：扩展以人为中心的机器翻译。*arXiv预印本 arXiv:2207.04672*。
- en: 'Dac Lai et al. (2023) Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat
    Nguyen, Franck Dernoncourt, Ryan A Rossi, and Thien Huu Nguyen. 2023. Okapi: Instruction-tuned
    large language models in multiple languages with reinforcement learning from human
    feedback. *arXiv e-prints*, pages arXiv–2307.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dac Lai et al. (2023) Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat
    Nguyen, Franck Dernoncourt, Ryan A Rossi, 和 Thien Huu Nguyen。2023年。Okapi：多语言的指令微调大语言模型，结合了来自人类反馈的强化学习。*arXiv电子印刷本*,
    页码 arXiv–2307。
- en: Deng et al. (2023) Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing.
    2023. Multilingual jailbreak challenges in large language models. *arXiv preprint
    arXiv:2310.06474*.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng et al. (2023) Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, 和 Lidong Bing。2023年。大语言模型中的多语言破解挑战。*arXiv预印本
    arXiv:2310.06474*。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer。2022年。Llm.
    int8 (): 用于大规模变换器的8位矩阵乘法。*arXiv预印本 arXiv:2208.07339*。'
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. [Qlora: Efficient finetuning of quantized llms](https://arxiv.org/abs/2305.14314).
    *Preprint*, arXiv:2305.14314.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke
    Zettlemoyer。2023年。[Qlora: 高效微调量化大语言模型](https://arxiv.org/abs/2305.14314)。*预印本*,
    arXiv:2305.14314。'
- en: 'Diddee et al. (2022) Harshita Diddee, Sandipan Dandapat, Monojit Choudhury,
    Tanuja Ganu, and Kalika Bali. 2022. [Too brittle to touch: Comparing the stability
    of quantization and distillation towards developing low-resource MT models](https://aclanthology.org/2022.wmt-1.80).
    In *Proceedings of the Seventh Conference on Machine Translation (WMT)*, pages
    870–885, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational
    Linguistics.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diddee et al. (2022) Harshita Diddee, Sandipan Dandapat, Monojit Choudhury,
    Tanuja Ganu, 和 Kalika Bali。2022年。[过于脆弱：比较量化和蒸馏的稳定性以开发低资源机器翻译模型](https://aclanthology.org/2022.wmt-1.80)。在
    *第七届机器翻译会议（WMT）* 会议录中，第870–885页，阿布扎比，阿联酋（混合模式）。计算语言学协会。
- en: Durmus et al. (2023) Esin Durmus, Karina Nyugen, Thomas I. Liao, Nicholas Schiefer,
    Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez,
    Nicholas Joseph, Liane Lovitt, Sam McCandlish, Orowa Sikder, Alex Tamkin, Janel
    Thamkul, Jared Kaplan, Jack Clark, and Deep Ganguli. 2023. Towards measuring the
    representation of subjective global opinions in language models. *arXiv*, abs/2306.16388.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Durmus et al. (2023) Esin Durmus, Karina Nyugen, Thomas I. Liao, Nicholas Schiefer,
    Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez,
    Nicholas Joseph, Liane Lovitt, Sam McCandlish, Orowa Sikder, Alex Tamkin, Janel
    Thamkul, Jared Kaplan, Jack Clark, 和 Deep Ganguli。2023年。迈向测量语言模型中的主观全球意见表示。*arXiv*,
    abs/2306.16388。
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2022. GPTQ: Accurate post-training compression for generative pretrained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan
    Alistarh。2022年。GPTQ：生成预训练变换器的精确后训练压缩。*arXiv预印本 arXiv:2210.17323*。
- en: Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid
    Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. [A framework for few-shot language
    model evaluation](https://doi.org/10.5281/zenodo.10256836).
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black,
    Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, 和 Andy Zou. 2023. [一个用于少样本语言模型评估的框架](https://doi.org/10.5281/zenodo.10256836)。
- en: Gong et al. (2024) Zhuocheng Gong, Jiahao Liu, Jingang Wang, Xunliang Cai, Dongyan
    Zhao, and Rui Yan. 2024. What makes quantization for large language model hard?
    an empirical study from the lens of perturbation. In *Proceedings of the AAAI
    Conference on Artificial Intelligence*, volume 38, pages 18082–18089.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong 等人 (2024) Zhuocheng Gong, Jiahao Liu, Jingang Wang, Xunliang Cai, Dongyan
    Zhao, 和 Rui Yan. 2024. 量化大型语言模型为何困难？从扰动角度的实证研究。发表于 *AAAI 人工智能会议论文集*，第38卷，第18082–18089页。
- en: Held et al. (2023) William Held, Camille Harris, Michael Best, and Diyi Yang.
    2023. A material lens on coloniality in nlp. *arXiv*, abs/2311.08391.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Held 等人 (2023) William Held, Camille Harris, Michael Best, 和 Diyi Yang. 2023.
    从材料视角看 NLP 中的殖民性。*arXiv*，abs/2311.08391。
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. *arXiv preprint arXiv:2009.03300*.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人 (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song, 和 Jacob Steinhardt. 2020. 测量大规模多任务语言理解。 *arXiv 预印本 arXiv:2009.03300*。
- en: 'Hooker et al. (2019) Sara Hooker, Aaron C. Courville, Gregory Clark, Yann Dauphin,
    and Andrea Frome. 2019. [What do compressed deep neural networks forget](https://api.semanticscholar.org/CorpusID:226812844).
    *arXiv: Learning*.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hooker 等人 (2019) Sara Hooker, Aaron C. Courville, Gregory Clark, Yann Dauphin,
    和 Andrea Frome. 2019. [压缩深度神经网络遗忘了什么](https://api.semanticscholar.org/CorpusID:226812844)。
    *arXiv: Learning*。'
- en: Hooker et al. (2020) Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio,
    and Emily Denton. 2020. [Characterising bias in compressed models](https://arxiv.org/abs/2010.03058).
    *Preprint*, arXiv:2010.03058.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hooker 等人 (2020) Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio,
    和 Emily Denton. 2020. [压缩模型中的偏差特征](https://arxiv.org/abs/2010.03058)。 *Preprint*，arXiv:2010.03058。
- en: Joulin et al. (2016) Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas
    Mikolov. 2016. Bag of tricks for efficient text classification. *arXiv preprint
    arXiv:1607.01759*.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joulin 等人 (2016) Armand Joulin, Edouard Grave, Piotr Bojanowski, 和 Tomas Mikolov.
    2016. 高效文本分类的技巧袋。 *arXiv 预印本 arXiv:1607.01759*。
- en: Khandelwal et al. (2023) Khyati Khandelwal, Manuel Tonneau, Andrew M. Bean,
    Hannah Rose Kirk, and Scott A. Hale. 2023. [Casteist but not racist? quantifying
    disparities in large language model bias between india and the west](https://api.semanticscholar.org/CorpusID:262013517).
    *ArXiv*, abs/2309.08573.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khandelwal 等人 (2023) Khyati Khandelwal, Manuel Tonneau, Andrew M. Bean, Hannah
    Rose Kirk, 和 Scott A. Hale. 2023. [种姓歧视但非种族歧视？量化印度与西方之间的大型语言模型偏差差异](https://api.semanticscholar.org/CorpusID:262013517)。
    *ArXiv*，abs/2309.08573。
- en: Kharazmi et al. (2023) Pegah Kharazmi, Zhewei Zhao, Clement Chung, and Samridhi
    Choudhary. 2023. Distill-quantize-tune-leveraging large teachers for low-footprint
    efficient multilingual nlu on edge. In *ICASSP 2023-2023 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, pages 1–5\. IEEE.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kharazmi 等人 (2023) Pegah Kharazmi, Zhewei Zhao, Clement Chung, 和 Samridhi Choudhary.
    2023. Distill-quantize-tune——利用大型教师进行低足迹高效的多语言自然语言理解。发表于 *ICASSP 2023-2023 IEEE国际声学、语音与信号处理会议
    (ICASSP)*，第1–5页。IEEE。
- en: 'Khondaker et al. (2023) Md Tawkat Islam Khondaker, Abdul Waheed, El Moatez Billah
    Nagoudi, and Muhammad Abdul-Mageed. 2023. Gptaraeval: A comprehensive evaluation
    of chatgpt on arabic nlp. *arXiv*, abs/2305.14976.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khondaker 等人 (2023) Md Tawkat Islam Khondaker, Abdul Waheed, El Moatez Billah
    Nagoudi, 和 Muhammad Abdul-Mageed. 2023. Gptaraeval：对 ChatGPT 在阿拉伯语 NLP 上的全面评估。
    *arXiv*，abs/2305.14976。
- en: 'Ko et al. (2023) Wei-Yin Ko, Daniel D’souza, Karina Nguyen, Randall Balestriero,
    and Sara Hooker. 2023. [Fair-ensemble: When fairness naturally emerges from deep
    ensembling](https://arxiv.org/abs/2303.00586). *Preprint*, arXiv:2303.00586.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ko 等人 (2023) Wei-Yin Ko, Daniel D’souza, Karina Nguyen, Randall Balestriero,
    和 Sara Hooker. 2023. [公平集成：当公平性自然从深度集成中出现](https://arxiv.org/abs/2303.00586)。
    *Preprint*，arXiv:2303.00586。
- en: Kotek et al. (2023) Hadas Kotek, Rikker Dockum, and David Q. Sun. 2023. [Gender
    bias and stereotypes in large language models](https://api.semanticscholar.org/CorpusID:261276445).
    *Proceedings of The ACM Collective Intelligence Conference*.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kotek et al. (2023) Hadas Kotek, Rikker Dockum, 和 David Q. Sun. 2023. [大型语言模型中的性别偏见与刻板印象](https://api.semanticscholar.org/CorpusID:261276445)。*ACM
    集体智能会议论文集*。
- en: 'Li et al. (2023a) Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang,
    Qi Hu, Chunkit Chan, and Yangqiu Song. 2023a. [Privacy in large language models:
    Attacks, defenses and future directions](https://api.semanticscholar.org/CorpusID:264145758).
    *ArXiv*, abs/2310.10383.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023a) Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang,
    Qi Hu, Chunkit Chan, 和 Yangqiu Song. 2023a. [大型语言模型中的隐私：攻击、防御与未来方向](https://api.semanticscholar.org/CorpusID:264145758)。*ArXiv*，abs/2310.10383。
- en: Li et al. (2024) Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng
    Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. 2024. Evaluating quantized
    large language models. *arXiv preprint arXiv:2402.18158*.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2024) Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng
    Shi, Shengen Yan, Guohao Dai, Huazhong Yang, 和 Yu Wang. 2024. 评估量化的大型语言模型。*arXiv
    预印本 arXiv:2402.18158*。
- en: 'Li et al. (2023b) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023b. Alpacaeval:
    An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval).'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023b) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. 2023b. Alpacaeval:
    一种自动评估指令跟随模型的工具。 [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)。'
- en: 'Lin et al. (2024) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming
    Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024.
    Awq: Activation-aware weight quantization for llm compression and acceleration.
    In *MLSys*.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2024) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming
    Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, 和 Song Han. 2024.
    Awq: 激活感知的权重量化用于 LLM 压缩与加速。发表于*MLSys*。'
- en: Lin et al. (2022) Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang,
    Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du,
    Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian
    O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov,
    and Xian Li. 2022. [Few-shot learning with multilingual generative language models](https://doi.org/10.18653/v1/2022.emnlp-main.616).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 9019–9052, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2022) Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang,
    Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du,
    Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian
    O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov,
    和 Xian Li. 2022. [多语言生成语言模型的少样本学习](https://doi.org/10.18653/v1/2022.emnlp-main.616)。发表于*2022年自然语言处理实证方法会议论文集*，页码
    9019–9052，阿布扎比，阿拉伯联合酋长国。计算语言学协会。
- en: 'Liu et al. (2024) Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin
    Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2024. [Do emergent abilities exist
    in quantized large language models: An empirical study](https://aclanthology.org/2024.lrec-main.461).
    In *Proceedings of the 2024 Joint International Conference on Computational Linguistics,
    Language Resources and Evaluation (LREC-COLING 2024)*, pages 5174–5190, Torino,
    Italia. ELRA and ICCL.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024) Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao,
    Yaliang Li, Bolin Ding, 和 Ji-Rong Wen. 2024. [量化大型语言模型中是否存在突现能力：一项实证研究](https://aclanthology.org/2024.lrec-main.461)。发表于*2024年国际计算语言学、语言资源与评估联合会议
    (LREC-COLING 2024) 论文集*，页码 5174–5190，意大利都灵。ELRA 和 ICCL。
- en: Lukas et al. (2023) Nils Lukas, A. Salem, Robert Sim, Shruti Tople, Lukas Wutschitz,
    and Santiago Zanella-B’eguelin. 2023. [Analyzing leakage of personally identifiable
    information in language models](https://api.semanticscholar.org/CorpusID:256459554).
    *2023 IEEE Symposium on Security and Privacy (SP)*, pages 346–363.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lukas et al. (2023) Nils Lukas, A. Salem, Robert Sim, Shruti Tople, Lukas Wutschitz,
    和 Santiago Zanella-B’eguelin. 2023. [分析语言模型中的个人身份信息泄漏](https://api.semanticscholar.org/CorpusID:256459554)。*2023年IEEE安全与隐私研讨会
    (SP)*，页码 346–363。
- en: Marchisio et al. (2024) Kelly Marchisio, Wei-Yin Ko, Alexandre Bérard, Théo
    Dehaze, and Sebastian Ruder. 2024. [Understanding and mitigating language confusion
    in llms](https://arxiv.org/abs/2406.20052). *Preprint*, arXiv:2406.20052.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marchisio et al. (2024) Kelly Marchisio, Wei-Yin Ko, Alexandre Bérard, Théo
    Dehaze, 和 Sebastian Ruder. 2024. [理解和缓解 LLM 中的语言混淆](https://arxiv.org/abs/2406.20052)。*预印本*，arXiv:2406.20052。
- en: Muennighoff et al. (2023) Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
    Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin
    Yong, Hailey Schoelkopf, et al. 2023. Crosslingual generalization through multitask
    finetuning. In *The 61st Annual Meeting Of The Association For Computational Linguistics*.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muennighoff 等 (2023) Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam
    Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin
    Yong, Hailey Schoelkopf 等. 2023. 通过多任务微调实现跨语言泛化。在 *第61届计算语言学协会年会*。
- en: Nasr et al. (2023) Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski,
    A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace,
    Florian Tramèr, and Katherine Lee. 2023. Scalable extraction of training data
    from (production) language models. *arXiv*, abs/2311.17035.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nasr 等 (2023) Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski,
    A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace,
    Florian Tramèr, 和 Katherine Lee. 2023. 从（生产）语言模型中可扩展地提取训练数据。*arXiv*，abs/2311.17035。
- en: Nelaturu et al. (2023) Sree Harsha Nelaturu, Nishaanth Kanna Ravichandran, Cuong
    Tran, Sara Hooker, and Ferdinando Fioretto. 2023. [On the fairness impacts of
    hardware selection in machine learning](https://arxiv.org/abs/2312.03886). *Preprint*,
    arXiv:2312.03886.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nelaturu 等 (2023) Sree Harsha Nelaturu, Nishaanth Kanna Ravichandran, Cuong
    Tran, Sara Hooker, 和 Ferdinando Fioretto. 2023. [机器学习中硬件选择的公平性影响](https://arxiv.org/abs/2312.03886)。*预印本*，arXiv:2312.03886。
- en: 'Nicholas and Bhatia (2023) Gabriel Nicholas and Aliya Bhatia. 2023. Lost in
    translation: Large language models in non-english content analysis. *arXiv*, abs/2306.07377.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nicholas 和 Bhatia (2023) Gabriel Nicholas 和 Aliya Bhatia. 2023. 迷失在翻译中：大型语言模型在非英语内容分析中的应用。*arXiv*，abs/2306.07377。
- en: Ogueji et al. (2022) Kelechi Ogueji, Orevaoghene Ahia, Gbemileke Onilude, Sebastian
    Gehrmann, Sara Hooker, and Julia Kreutzer. 2022. [Intriguing properties of compression
    on multilingual models](https://doi.org/10.18653/v1/2022.emnlp-main.619). In *Proceedings
    of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages
    9092–9110, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ogueji 等 (2022) Kelechi Ogueji, Orevaoghene Ahia, Gbemileke Onilude, Sebastian
    Gehrmann, Sara Hooker, 和 Julia Kreutzer. 2022. [多语言模型压缩的引人入胜的特性](https://doi.org/10.18653/v1/2022.emnlp-main.619)。在
    *2022年自然语言处理经验方法会议论文集*，第 9092–9110 页，阿布扎比，阿联酋。计算语言学协会。
- en: Ojo et al. (2023) Jessica Ojo, Kelechi Ogueji, Pontus Stenetorp, and David I.
    Adelani. 2023. How good are large language models on african languages? *arXiv*,
    abs/2311.07978.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ojo 等 (2023) Jessica Ojo, Kelechi Ogueji, Pontus Stenetorp, 和 David I. Adelani.
    2023. 大型语言模型在非洲语言上的表现如何？*arXiv*，abs/2311.07978。
- en: Paglieri et al. (2024) Davide Paglieri, Saurabh Dash, Tim Rocktäschel, and Jack
    Parker-Holder. 2024. [Outliers and calibration sets have diminishing effect on
    quantization of modern llms](https://arxiv.org/abs/2405.20835). *Preprint*, arXiv:2405.20835.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paglieri 等 (2024) Davide Paglieri, Saurabh Dash, Tim Rocktäschel, 和 Jack Parker-Holder.
    2024. [异常值和校准集对现代语言模型量化的影响递减](https://arxiv.org/abs/2405.20835)。*预印本*，arXiv:2405.20835。
- en: 'Ponti et al. (2020) Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu
    Liu, Ivan Vulić, and Anna Korhonen. 2020. [Xcopa: A multilingual dataset for causal
    commonsense reasoning](https://doi.org/10.18653/v1/2020.emnlp-main.185). pages
    2362–2376.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ponti 等 (2020) Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu,
    Ivan Vulić, 和 Anna Korhonen. 2020. [Xcopa：用于因果常识推理的多语言数据集](https://doi.org/10.18653/v1/2020.emnlp-main.185)。第
    2362–2376 页。
- en: 'Post (2018) Matt Post. 2018. [A call for clarity in reporting BLEU scores](https://doi.org/10.18653/v1/W18-6319).
    In *Proceedings of the Third Conference on Machine Translation: Research Papers*,
    pages 186–191, Brussels, Belgium. Association for Computational Linguistics.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Post (2018) Matt Post. 2018. [对 BLEU 分数报告的明确呼吁](https://doi.org/10.18653/v1/W18-6319)。在
    *第三届机器翻译会议：研究论文集*，第 186–191 页，布鲁塞尔，比利时。计算语言学协会。
- en: 'Pozzobon et al. (2024) Luiza Pozzobon, Patrick Lewis, Sara Hooker, and Beyza
    Ermis. 2024. [From one to many: Expanding the scope of toxicity mitigation in
    language models](https://arxiv.org/abs/2403.03893). *Preprint*, arXiv:2403.03893.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pozzobon 等 (2024) Luiza Pozzobon, Patrick Lewis, Sara Hooker, 和 Beyza Ermis.
    2024. [从一个到多个：扩大语言模型中的毒性缓解范围](https://arxiv.org/abs/2403.03893)。*预印本*，arXiv:2403.03893。
- en: 'Ramesh et al. (2023) Krithika Ramesh, Arnav Chavan, Shrey Pandit, and Sunayana
    Sitaram. 2023. [A comparative study on the impact of model compression techniques
    on fairness in language models](https://doi.org/10.18653/v1/2023.acl-long.878).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 15762–15782, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh et al. (2023) Krithika Ramesh, Arnav Chavan, Shrey Pandit, 和 Sunayana
    Sitaram. 2023. [模型压缩技术对语言模型公平性的影响的比较研究](https://doi.org/10.18653/v1/2023.acl-long.878)。在*第61届计算语言学协会年会（第1卷：长篇论文）*上，页码
    15762–15782，多伦多，加拿大。计算语言学协会。
- en: Schwartz et al. (2022) Reva Schwartz, Apostol Vassilev, Kristen Greene, Lori
    Perine, Andrew Burt, Patrick Hall, et al. 2022. Towards a standard for identifying
    and managing bias in artificial intelligence. *NIST special publication*, 1270(10.6028).
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwartz et al. (2022) Reva Schwartz, Apostol Vassilev, Kristen Greene, Lori
    Perine, Andrew Burt, Patrick Hall 等. 2022. 迈向识别和管理人工智能偏见的标准。*NIST 特别出版物*，1270(10.6028)。
- en: Shi et al. (2023) Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj
    Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou,
    Dipanjan Das, and Jason Wei. 2023. [Language models are multilingual chain-of-thought
    reasoners](https://openreview.net/forum?id=fR3wGCk-IXp). In *The Eleventh International
    Conference on Learning Representations*.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2023) Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj
    Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou,
    Dipanjan Das, 和 Jason Wei. 2023. [语言模型是多语言链式推理者](https://openreview.net/forum?id=fR3wGCk-IXp)。在*第十一届国际学习表征会议*上。
- en: 'Singh et al. (2024) Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F.
    Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas
    Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina
    Machado, Luisa Souza Moura, Dominik Krzemiński, Hakimeh Fadaei, Irem Ergün, Ifeoma
    Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien, Sebastian
    Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff,
    Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, and Sara Hooker. 2024.
    Aya dataset: An open-access collection for multilingual instruction tuning. *arXiv
    preprint arXiv:2402.06619*.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh et al. (2024) Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F.
    Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas
    Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina
    Machado, Luisa Souza Moura, Dominik Krzemiński, Hakimeh Fadaei, Irem Ergün, Ifeoma
    Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien, Sebastian
    Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff,
    Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, 和 Sara Hooker. 2024.
    Aya 数据集：一个用于多语言指令调优的开放访问集合。*arXiv 预印本 arXiv:2402.06619*。
- en: Vashishtha et al. (2023) Aniket Vashishtha, Kabir Ahuja, and Sunayana Sitaram.
    2023. On evaluating and mitigating gender biases in multilingual settings. *arXiv*,
    abs/2307.01503.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vashishtha et al. (2023) Aniket Vashishtha, Kabir Ahuja, 和 Sunayana Sitaram.
    2023. 在多语言环境中评估和减轻性别偏见。*arXiv*，abs/2307.01503。
- en: Wang et al. (2022) Serena Wang, Harikrishna Narasimhan, Yichen Zhou, Sara Hooker,
    Michal Lukasik, and Aditya Krishna Menon. 2022. [Robust distillation for worst-class
    performance](https://arxiv.org/abs/2206.06479). *Preprint*, arXiv:2206.06479.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022) Serena Wang, Harikrishna Narasimhan, Yichen Zhou, Sara Hooker,
    Michal Lukasik, 和 Aditya Krishna Menon. 2022. [针对最差类别性能的鲁棒蒸馏](https://arxiv.org/abs/2206.06479)。*预印本*，arXiv:2206.06479。
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. 2023. SmoothQuant: Accurate and efficient post-training quantization
    for large language models. In *Proceedings of the 40th International Conference
    on Machine Learning*.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    和 Song Han. 2023. SmoothQuant: 对大型语言模型进行准确且高效的后训练量化。发表于*第40届国际机器学习会议论文集*。'
- en: Yong et al. (2023) Zheng-Xin Yong, Cristina Menghini, and Stephen H. Bach. 2023.
    Low-resource languages jailbreak GPT-4. *arXiv*, abs/2310.02446.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yong et al. (2023) Zheng-Xin Yong, Cristina Menghini, 和 Stephen H. Bach. 2023.
    低资源语言破解 GPT-4。*arXiv*，abs/2310.02446。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. *Advances in Neural
    Information Processing Systems*, 36.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing 等. 2023.
    使用 MT-Bench 和 Chatbot Arena 评估 LLM 作为裁判的表现。*神经信息处理系统进展*，36。
- en: 'Zhuang et al. (2021) Donglin Zhuang, Xingyao Zhang, Shuaiwen Leon Song, and
    Sara Hooker. 2021. [Randomness in neural network training: Characterizing the
    impact of tooling](https://arxiv.org/abs/2106.11872). *Preprint*, arXiv:2106.11872.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhuang 等人（2021）Donglin Zhuang, Xingyao Zhang, Shuaiwen Leon Song, 和 Sara Hooker。2021年。
    [Randomness in neural network training: Characterizing the impact of tooling](https://arxiv.org/abs/2106.11872)。*预印本*，arXiv:2106.11872。'
- en: 'Üstün et al. (2024) Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko,
    Daniel D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi,
    Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh
    Fadaee, Julia Kreutzer, and Sara Hooker. 2024. [Aya model: An instruction finetuned
    open-access multilingual language model](https://arxiv.org/abs/2402.07827). *Preprint*,
    arXiv:2402.07827.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Üstün 等人（2024）Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel
    D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid,
    Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee,
    Julia Kreutzer, 和 Sara Hooker。2024年。 [Aya model: An instruction finetuned open-access
    multilingual language model](https://arxiv.org/abs/2402.07827)。*预印本*，arXiv:2402.07827。'
- en: Appendix A Appendix
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Prompts for mMMLU and LLM-as-a-Judge
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 mMMLU 和 LLM-as-a-Judge 的提示
- en: '|'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The following are multiple choice questions (with answers) &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以下是选择题（带答案） &#124;'
- en: '&#124; about clinical knowledge. &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关于临床知识。 &#124;'
- en: '&#124; 다음 중 파제트병에 대한 설명으로 옳은 것은 무엇입니까? &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以下哪项对帕杰特病的描述是正确的？ &#124;'
- en: '&#124; A. 긴 뼈가 휘어지는 것이 특징 &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A. 长骨弯曲是特征 &#124;'
- en: '&#124; B. 척수압박은 흔한 합병증이다 &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; B. 脊髓压迫是一种常见的并发症 &#124;'
- en: '&#124; C. 심부전은 알려진 합병증이 아니다 &#124;'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; C. 心力衰竭并不是已知的并发症 &#124;'
- en: '&#124; D. 병적 골절은 특징이 아닙니다. &#124;'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; D. 病理性骨折不是特征。 &#124;'
- en: '&#124; Answer: B &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 答案：B &#124;'
- en: '&#124; … &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; … &#124;'
- en: '|'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table A1: mMMLU prompt. Following Achiam et al. ([2023](#bib.bib1)), letter
    choices and “Answer” are kept in English.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A1: mMMLU 提示。参照 Achiam 等人（[2023](#bib.bib1)），字母选择和“答案”保持英文。'
- en: '| Example Prompt |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 示例提示 |'
- en: '|'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; I want you to create a leaderboard of different large-language models.
    To do so, I will give you the conversations (prompts) given to the models, and
    the responses of two models. Please rank the models based on which responses would
    be preferred by humans. All inputs and outputs should be python dictionaries.
    &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我希望你创建一个不同的大型语言模型的排行榜。为此，我会给你模型的对话（提示）和两个模型的响应。请根据哪些响应更受人类欢迎来对模型进行排名。所有输入和输出应为
    Python 字典。 &#124;'
- en: '&#124; Here is the prompt: &#124;'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这里是提示： &#124;'
- en: '&#124; { &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; { &#124;'
- en: '&#124;        "conversation": """User: La tomate est-elle un fruit ou un légume?""",
    &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;        "conversation": """用户：西红柿是水果还是蔬菜？""", &#124;'
- en: '&#124; } &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; } &#124;'
- en: '&#124; Here are the outputs of the models: &#124;'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这是模型的输出： &#124;'
- en: '&#124; [ &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [ &#124;'
- en: '&#124;        { &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;        { &#124;'
- en: '&#124;         "model": "model_1", &#124;'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;         "model": "model_1", &#124;'
- en: '&#124;         "answer": """La tomate est un fruit. Plus précisément, il s’agit
    d’un fruit charnu, issu de la transformation de l’ovaire de la fleur du plant
    de tomate.""" &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;         "answer": """西红柿是水果。从植物学角度来看，它是一种肉质果实，源于西红柿花的子房的转变。尽管如此，在烹饪中，西红柿常常被视为一种蔬菜，因为它常用于咸味菜肴，并且其味道比其他水果更不甜。"""
    &#124;'
- en: '&#124;        }, &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;        }, &#124;'
- en: '&#124;        { &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;        { &#124;'
- en: '&#124;         "model": "model_2", &#124;'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;         "model": "model_2", &#124;'
- en: '&#124;         "answer": """La tomate est un fruit du point de vue botanique,
    car elle contient des graines et se développe à partir de la fleur d’une plante.
    Cependant, en cuisine, on considère souvent la tomate comme un légume en raison
    de son utilisation dans des plats salés et de sa saveur moins sucrée par rapport
    à d’autres fruits.""" &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;         "answer": """西红柿是水果。从植物学角度来看，它含有种子并且发育自植物的花。然而，在烹饪中，西红柿通常被视为一种蔬菜，因为它常用于咸味菜肴，并且与其他水果相比，其味道不那么甜。"""
    &#124;'
- en: '&#124;        } &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;        } &#124;'
- en: '&#124; ] &#124;'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ] &#124;'
- en: '&#124; Now please rank the models by the quality of their answers, so that
    the model with rank 1 has the best output. Then return a list of the model names
    and ranks, i.e., produce the following output: &#124;'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 现在请根据答案的质量对模型进行排名，使得排名第一的模型具有最佳输出。然后返回模型名称和排名的列表，即生成以下输出： &#124;'
- en: '&#124; [ &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [ &#124;'
- en: '&#124;        {’model’: , ’rank’: }, &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;        {’model’: , ’rank’: }, &#124;'
- en: '&#124;        {’model’: , ’rank’: } &#124;'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;        {’model’: , ’rank’: } &#124;'
- en: '&#124; ] &#124;'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ] &#124;'
- en: '&#124; Your response must be a valid Python dictionary and should contain nothing
    else because we will directly execute it in Python. Please provide the ranking
    that the majority of humans would give. &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你的回应必须是有效的 Python 字典，并且只包含这些内容，因为我们将直接在 Python 中执行它。请提供大多数人会给出的排名。 &#124;'
- en: '|'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table A2: Example Input for LLM-as-a-Judge. Template derived from Li et al.
    ([2023b](#bib.bib34)): [https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_configs/gpt-3.5-turbo-1106_ranking/ranking_prompt.txt](https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_configs/gpt-3.5-turbo-1106_ranking/ranking_prompt.txt)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A2：LLM-as-a-Judge 的示例输入。模板来源于 Li 等人 ([2023b](#bib.bib34))：[https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_configs/gpt-3.5-turbo-1106_ranking/ranking_prompt.txt](https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_configs/gpt-3.5-turbo-1106_ranking/ranking_prompt.txt)
- en: A.2 Automatic Tasks - Full Results
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 自动任务 - 完整结果
- en: '|  |  | de | es | fr | ja | zh | Avg |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  |  | de | es | fr | ja | zh | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 103B | FP16 | 72.6 | 76.6 | 70.6 | 63.0 | 70.2 | 70.6 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 103B | FP16 | 72.6 | 76.6 | 70.6 | 63.0 | 70.2 | 70.6 |'
- en: '| W8 | 72.8 | 75.9 | 69.5 | 61.3 | 70.2 | 69.9 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| W8 | 72.8 | 75.9 | 69.5 | 61.3 | 70.2 | 69.9 |'
- en: '| W8A8-sq | 73.4 | 73.8 | 69.6 | 62.9 | 67.7 | 69.5 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| W8A8-sq | 73.4 | 73.8 | 69.6 | 62.9 | 67.7 | 69.5 |'
- en: '| W8A8 | 74.1 | 73.9 | 69.8 | 63.4 | 68.0 | 69.8 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 74.1 | 73.9 | 69.8 | 63.4 | 68.0 | 69.8 |'
- en: '| W4-g | 71.2 | 75.7 | 69.0 | 58.0 | 68.9 | 68.6 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| W4-g | 71.2 | 75.7 | 69.0 | 58.0 | 68.9 | 68.6 |'
- en: '| W4 | 64.6 | 71.3 | 66.5 | 56.1 | 63.5 | 64.4 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| W4 | 64.6 | 71.3 | 66.5 | 56.1 | 63.5 | 64.4 |'
- en: '| 35B | FP16 | 56.6 | 57.3 | 51.8 | 38.8 | 44.4 | 49.8 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 35B | FP16 | 56.6 | 57.3 | 51.8 | 38.8 | 44.4 | 49.8 |'
- en: '| W8 | 55.9 | 56.6 | 52.1 | 37.4 | 45.1 | 49.4 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| W8 | 55.9 | 56.6 | 52.1 | 37.4 | 45.1 | 49.4 |'
- en: '| W8A8 | 54.2 | 53.4 | 49.9 | 35.8 | 42.0 | 47.1 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 54.2 | 53.4 | 49.9 | 35.8 | 42.0 | 47.1 |'
- en: '| W4-g | 47.2 | 51.0 | 47.1 | 34.3 | 36.7 | 43.3 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| W4-g | 47.2 | 51.0 | 47.1 | 34.3 | 36.7 | 43.3 |'
- en: 'Table A3: Command model MGSM results. (Acc.)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A3：命令模型 MGSM 结果。（准确率）
- en: '|  |  | de | es | fr | ja | zh | Avg | Ltn/IE | $\neg$ |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  |  | de | es | fr | ja | zh | 平均 | Ltn/IE | $\neg$ |'
- en: '|  | W8 | 0.3% | -0.9% | -1.6% | -2.7% | -0.1% | -1.0% | -0.7% | -1.4% |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 0.3% | -0.9% | -1.6% | -2.7% | -0.1% | -1.0% | -0.7% | -1.4% |'
- en: '|  | W8A8-sq | 1.1% | -3.7% | -1.5% | -0.1% | -3.6% | -1.6% | -1.3% | -1.9%
    |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | 1.1% | -3.7% | -1.5% | -0.1% | -3.6% | -1.6% | -1.3% | -1.9%
    |'
- en: '|  | W8A8 | 2.1% | -3.5% | -1.1% | 0.6% | -3.2% | -1.0% | -0.9% | -1.3% |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | 2.1% | -3.5% | -1.1% | 0.6% | -3.2% | -1.0% | -0.9% | -1.3%'
- en: '|  | W4-g | -1.9% | -1.3% | -2.3% | -7.9% | -1.9% | -3.0% | -1.8% | -4.9% |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  | W4-g | -1.9% | -1.3% | -2.3% | -7.9% | -1.9% | -3.0% | -1.8% | -4.9% |'
- en: '| 103B | W4 | -11.0% | -7.0% | -5.9% | -10.9% | -9.6% | -8.8% | -8.0% | -10.2%
    |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 103B | W4 | -11.0% | -7.0% | -5.9% | -10.9% | -9.6% | -8.8% | -8.0% | -10.2%
    |'
- en: '|  | W8 | -1.3% | -1.1% | 0.6% | -3.7% | 1.6% | -0.8% | -0.6% | -1.0% |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -1.3% | -1.1% | 0.6% | -3.7% | 1.6% | -0.8% | -0.6% | -1.0% |'
- en: '|  | W8A8 | -4.4% | -6.8% | -3.6% | -7.6% | -5.4% | -5.6% | -4.9% | -6.5% |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | -4.4% | -6.8% | -3.6% | -7.6% | -5.4% | -5.6% | -4.9% | -6.5% |'
- en: '| 35B | W4-g | -16.7% | -10.9% | -9.0% | -11.5% | -17.3% | -13.1% | -12.2%
    | -14.4% |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 35B | W4-g | -16.7% | -10.9% | -9.0% | -11.5% | -17.3% | -13.1% | -12.2%
    | -14.4% |'
- en: 'Table A4: Relative performance (%$\Delta$: ja, zh.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A4：相对性能（%$\Delta$：ja, zh）。
- en: '|  |  | Avg (-en) | Avg | de | en | es | fr | ja | ru | zh |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 平均（-en） | 平均 | de | en | es | fr | ja | ru | zh |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | FP16 | 51.2 | 53.7 | 61.6 | 68.4 | 58.4 | 55.6 | 22.8 | 58.0 | 50.8 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 51.2 | 53.7 | 61.6 | 68.4 | 58.4 | 55.6 | 22.8 | 58.0 | 50.8 |'
- en: '| Aya-23-35b | W8 | 52.1 | 54.2 | 54.4 | 66.4 | 61.2 | 60.4 | 24.4 | 57.2 |
    55.2 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-35b | W8 | 52.1 | 54.2 | 54.4 | 66.4 | 61.2 | 60.4 | 24.4 | 57.2 |
    55.2 |'
- en: '|  | W4 | 48.1 | 50.7 | 58.8 | 66.0 | 54.8 | 54.8 | 18.4 | 53.6 | 48.4 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | 48.1 | 50.7 | 58.8 | 66.0 | 54.8 | 54.8 | 18.4 | 53.6 | 48.4 |'
- en: '|  | FP16 | 34.7 | 36.6 | 40.4 | 48.0 | 45.2 | 38.8 | 12.8 | 38.0 | 32.8 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 34.7 | 36.6 | 40.4 | 48.0 | 45.2 | 38.8 | 12.8 | 38.0 | 32.8 |'
- en: '| Aya-23-8b | W8 | 35.4 | 36.9 | 39.6 | 45.6 | 45.6 | 38.8 | 13.6 | 38.8 |
    36.0 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-8b | W8 | 35.4 | 36.9 | 39.6 | 45.6 | 45.6 | 38.8 | 13.6 | 38.8 |
    36.0 |'
- en: '|  | W4 | 32.1 | 33.5 | 39.6 | 42.4 | 42.0 | 34.0 | 7.2 | 33.6 | 36.0 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | 32.1 | 33.5 | 39.6 | 42.4 | 42.0 | 34.0 | 7.2 | 33.6 | 36.0 |'
- en: 'Table A5: Aya 23 language-specific results for MGSM (5-shot).'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A5：Aya 23 语言特定的 MGSM（5-shot）结果。
- en: '|  |  | de | en | es | fr | ja | ru | zh | Avg | Avg (-en) | Ltn/IE | $\neg$
    |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  |  | de | en | es | fr | ja | ru | zh | 平均 | 平均（-en） | Ltn/IE | $\neg$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '|  | W8 | -11.7% | -2.9% | 4.8% | 8.6% | 7.0% | -1.4% | 8.7% | 1.9% | 2.7%
    | -0.3% | 4.8% |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -11.7% | -2.9% | 4.8% | 8.6% | 7.0% | -1.4% | 8.7% | 1.9% | 2.7%
    | -0.3% | 4.8% |'
- en: '| Aya-23-35b | W4 | -4.5% | -3.5% | -6.2% | -1.4% | -19.3% | -7.6% | -4.7%
    | -6.8% | -7.3% | -3.9% | -10.5% |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-35b | W4 | -4.5% | -3.5% | -6.2% | -1.4% | -19.3% | -7.6% | -4.7%
    | -6.8% | -7.3% | -3.9% | -10.5% |'
- en: '|  | W8 | -2.0% | -5.0% | 0.9% | 0.0% | 6.2% | 2.1% | 9.8% | 1.7% | 2.8% |
    -1.5% | 6.0% |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -2.0% | -5.0% | 0.9% | 0.0% | 6.2% | 2.1% | 9.8% | 1.7% | 2.8% |
    -1.5% | 6.0% |'
- en: '| Aya-23-8b | W4 | -2.0% | -11.7% | -7.1% | -12.4% | -43.8% | -11.6% | 9.8%
    | -11.2% | -11.2% | -8.3% | -15.2% |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-8b | W4 | -2.0% | -11.7% | -7.1% | -12.4% | -43.8% | -11.6% | 9.8%
    | -11.2% | -11.2% | -8.3% | -15.2% |'
- en: 'Table A6: Relative performance ($\%\Delta$ are the rest: ja, ru, zh.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A6：相对性能（%$\Delta$为剩余：ja、ru、zh）。
- en: '|  |  | Avg (-en) | Avg | ar | cs | de | el | en | es | fa | fr | hi | id |
    it | ja | ko | nl | pl | pt | ro | ru | tr | uk | vi | zh |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 平均值（-en） | 平均值 | ar | cs | de | el | en | es | fa | fr | hi | id |
    it | ja | ko | nl | pl | pt | ro | ru | tr | uk | vi | zh |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| 35b | FP16 | 77.6 | 77.9 | 78.9 | 78.2 | 77.1 | 76.4 | 84.7 | 81.0 | 75.8
    | 81.9 | 65.6 | 77.8 | 79.8 | 75.9 | 73.3 | 77.7 | 75.8 | 83.8 | 78.9 | 79.6 |
    74.1 | 77.6 | 78.3 | 81.2 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 35b | FP16 | 77.6 | 77.9 | 78.9 | 78.2 | 77.1 | 76.4 | 84.7 | 81.0 | 75.8
    | 81.9 | 65.6 | 77.8 | 79.8 | 75.9 | 73.3 | 77.7 | 75.8 | 83.8 | 78.9 | 79.6 |
    74.1 | 77.6 | 78.3 | 81.2 |'
- en: '| W8 | 77.1 | 77.4 | 77.3 | 78.8 | 77.2 | 76.6 | 84.6 | 80.8 | 74.9 | 82.4
    | 65.6 | 77.6 | 80.8 | 74.8 | 73.7 | 77.6 | 74.8 | 82.9 | 77.1 | 78.9 | 72.0 |
    77.2 | 77.0 | 80.3 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| W8 | 77.1 | 77.4 | 77.3 | 78.8 | 77.2 | 76.6 | 84.6 | 80.8 | 74.9 | 82.4
    | 65.6 | 77.6 | 80.8 | 74.8 | 73.7 | 77.6 | 74.8 | 82.9 | 77.1 | 78.9 | 72.0 |
    77.2 | 77.0 | 80.3 |'
- en: '| W4 | 73.0 | 73.4 | 73.8 | 74.9 | 73.2 | 70.8 | 83.2 | 77.0 | 71.4 | 78.1
    | 61.0 | 73.9 | 76.2 | 71.7 | 67.4 | 73.0 | 70.4 | 80.1 | 74.3 | 73.3 | 68.2 |
    73.0 | 71.4 | 78.8 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| W4 | 73.0 | 73.4 | 73.8 | 74.9 | 73.2 | 70.8 | 83.2 | 77.0 | 71.4 | 78.1
    | 61.0 | 73.9 | 76.2 | 71.7 | 67.4 | 73.0 | 70.4 | 80.1 | 74.3 | 73.3 | 68.2 |
    73.0 | 71.4 | 78.8 |'
- en: '| 8b | FP16 | 64.8 | 65.3 | 65.6 | 61.9 | 65.6 | 64.0 | 77.0 | 67.0 | 63.6
    | 69.6 | 54.3 | 67.4 | 65.7 | 65.2 | 61.7 | 63.8 | 61.3 | 69.1 | 65.7 | 69.7 |
    58.1 | 66.8 | 62.3 | 72.2 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 8b | FP16 | 64.8 | 65.3 | 65.6 | 61.9 | 65.6 | 64.0 | 77.0 | 67.0 | 63.6
    | 69.6 | 54.3 | 67.4 | 65.7 | 65.2 | 61.7 | 63.8 | 61.3 | 69.1 | 65.7 | 69.7 |
    58.1 | 66.8 | 62.3 | 72.2 |'
- en: '| W8 | 64.6 | 65.1 | 64.3 | 61.8 | 64.8 | 63.0 | 76.1 | 67.4 | 63.9 | 70.4
    | 54.2 | 67.4 | 64.6 | 65.4 | 61.4 | 64.3 | 59.8 | 68.7 | 65.4 | 68.7 | 58.1 |
    67.0 | 63.7 | 71.8 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| W8 | 64.6 | 65.1 | 64.3 | 61.8 | 64.8 | 63.0 | 76.1 | 67.4 | 63.9 | 70.4
    | 54.2 | 67.4 | 64.6 | 65.4 | 61.4 | 64.3 | 59.8 | 68.7 | 65.4 | 68.7 | 58.1 |
    67.0 | 63.7 | 71.8 |'
- en: '| W4 | 59.3 | 59.9 | 61.9 | 57.0 | 61.6 | 57.7 | 73.8 | 61.1 | 58.2 | 65.7
    | 49.8 | 64.7 | 58.3 | 60.7 | 51.1 | 60.7 | 54.9 | 62.0 | 59.8 | 63.9 | 50.1 |
    61.0 | 58.8 | 66.2 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| W4 | 59.3 | 59.9 | 61.9 | 57.0 | 61.6 | 57.7 | 73.8 | 61.1 | 58.2 | 65.7
    | 49.8 | 64.7 | 58.3 | 60.7 | 51.1 | 60.7 | 54.9 | 62.0 | 59.8 | 63.9 | 50.1 |
    61.0 | 58.8 | 66.2 |'
- en: 'Table A7: Aya 23 language-specific results for Belebele. (Accuracy)'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A7：Aya 23 语言特定的 Belebele 结果（准确率）
- en: '|  |  | ar | cs | de | el | en | es | fa | fr | hi | id | it | ja | ko | nl
    | pl | pt | ro | ru | tr | uk | vi | zh | Avg | Avg (-en) | Ltn | $\neg$ |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ar | cs | de | el | en | es | fa | fr | hi | id | it | ja | ko | nl
    | pl | pt | ro | ru | tr | uk | vi | zh | 平均值 | 平均值（-en） | Ltn | $\neg$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
- en: '|  | W8 | -2.0% | 0.7% | 0.1% | 0.2% | -0.1% | -0.3% | -1.2% | 0.7% | 0.0%
    | -0.3% | 1.3% | -1.5% | 0.5% | -0.1% | -1.3% | -1.1% | -2.3% | -0.8% | -2.8%
    | -0.4% | -1.7% | -1.1% | -0.6% | -0.6% | -0.6% | -0.7% |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -2.0% | 0.7% | 0.1% | 0.2% | -0.1% | -0.3% | -1.2% | 0.7% | 0.0%
    | -0.3% | 1.3% | -1.5% | 0.5% | -0.1% | -1.3% | -1.1% | -2.3% | -0.8% | -2.8%
    | -0.4% | -1.7% | -1.1% | -0.6% | -0.6% | -0.6% | -0.7% |'
- en: '| 35b | W4 | -6.5% | -4.3% | -5.0% | -7.4% | -1.7% | -4.9% | -5.7% | -4.6%
    | -7.0% | -5.0% | -4.5% | -5.6% | -8.0% | -6.0% | -7.0% | -4.4% | -5.8% | -7.8%
    | -7.9% | -5.9% | -8.8% | -3.0% | -5.8% | -6.0% | -5.4% | -6.3% |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 35b | W4 | -6.5% | -4.3% | -5.0% | -7.4% | -1.7% | -4.9% | -5.7% | -4.6%
    | -7.0% | -5.0% | -4.5% | -5.6% | -8.0% | -6.0% | -7.0% | -4.4% | -5.8% | -7.8%
    | -7.9% | -5.9% | -8.8% | -3.0% | -5.8% | -6.0% | -5.4% | -6.3% |'
- en: '|  | W8 | -1.9% | -0.2% | -1.2% | -1.6% | -1.2% | 0.7% | 0.5% | 1.3% | -0.2%
    | 0.0% | -1.7% | 0.3% | -0.4% | 0.9% | -2.5% | -0.6% | -0.4% | -1.4% | 0.0% |
    0.3% | 2.1% | -0.6% | -0.3% | -0.3% | -0.2% | -0.5% |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -1.9% | -0.2% | -1.2% | -1.6% | -1.2% | 0.7% | 0.5% | 1.3% | -0.2%
    | 0.0% | -1.7% | 0.3% | -0.4% | 0.9% | -2.5% | -0.6% | -0.4% | -1.4% | 0.0% |
    0.3% | 2.1% | -0.6% | -0.3% | -0.3% | -0.2% | -0.5% |'
- en: '| 8b | W4 | -5.6% | -7.9% | -6.1% | -9.9% | -4.2% | -8.8% | -8.4% | -5.6% |
    -8.4% | -4.1% | -11.2% | -7.0% | -17.1% | -4.9% | -10.5% | -10.3% | -9.0% | -8.3%
    | -13.8% | -8.7% | -5.7% | -8.3% | -8.3% | -8.5% | -7.8% | -9.1% |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 8b | W4 | -5.6% | -7.9% | -6.1% | -9.9% | -4.2% | -8.8% | -8.4% | -5.6% |
    -8.4% | -4.1% | -11.2% | -7.0% | -17.1% | -4.9% | -10.5% | -10.3% | -9.0% | -8.3%
    | -13.8% | -8.7% | -5.7% | -8.3% | -8.3% | -8.5% | -7.8% | -9.1% |'
- en: 'Table A8: Relative performance ($\%\Delta$ are the rest.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A8：相对性能（%$\Delta$为剩余）。
- en: '|  |  | ar | de | es | fr | it | ja | ko | pt | zh | Avg |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ar | de | es | fr | it | ja | ko | pt | zh | 平均值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 103B | FP16 | 64.0 | 68.3 | 68.7 | 68.0 | 69.3 | 64.4 | 62.3 | 70.0 | 65.0
    | 66.7 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 103B | FP16 | 64.0 | 68.3 | 68.7 | 68.0 | 69.3 | 64.4 | 62.3 | 70.0 | 65.0
    | 66.7 |'
- en: '| W8 | 64.1 | 68.3 | 68.7 | 68.1 | 69.4 | 64.3 | 62.3 | 69.9 | 65.0 | 66.7
    |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| W8 | 64.1 | 68.3 | 68.7 | 68.1 | 69.4 | 64.3 | 62.3 | 69.9 | 65.0 | 66.7
    |'
- en: '| W8A8-sq | 63.5 | 67.9 | 68.8 | 68.0 | 69.1 | 63.6 | 61.8 | 69.2 | 64.9 |
    66.3 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| W8A8-sq | 63.5 | 67.9 | 68.8 | 68.0 | 69.1 | 63.6 | 61.8 | 69.2 | 64.9 |
    66.3 |'
- en: '| W8A8 | 62.6 | 67.1 | 68.2 | 67.4 | 68.3 | 62.9 | 60.8 | 68.7 | 64.1 | 65.6
    |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 62.6 | 67.1 | 68.2 | 67.4 | 68.3 | 62.9 | 60.8 | 68.7 | 64.1 | 65.6
    |'
- en: '| W4-g | 62.9 | 67.5 | 68.2 | 67.6 | 68.6 | 62.8 | 61.1 | 68.6 | 64.0 | 65.7
    |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| W4-g | 62.9 | 67.5 | 68.2 | 67.6 | 68.6 | 62.8 | 61.1 | 68.6 | 64.0 | 65.7
    |'
- en: '| W4 | 60.5 | 65.7 | 66.5 | 65.4 | 66.6 | 61.1 | 59.3 | 66.7 | 62.1 | 63.8
    |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| W4 | 60.5 | 65.7 | 66.5 | 65.4 | 66.6 | 61.1 | 59.3 | 66.7 | 62.1 | 63.8
    |'
- en: '| 35B | FP16 | 56.5 | 60.7 | 62.3 | 61.8 | 62.0 | 56.4 | 54.8 | 62.0 | 57.9
    | 59.4 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 35B | FP16 | 56.5 | 60.7 | 62.3 | 61.8 | 62.0 | 56.4 | 54.8 | 62.0 | 57.9
    | 59.4 |'
- en: '| W8 | 56.5 | 60.6 | 62.2 | 61.8 | 61.9 | 56.4 | 54.7 | 62.1 | 57.9 | 59.3
    |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| W8 | 56.5 | 60.6 | 62.2 | 61.8 | 61.9 | 56.4 | 54.7 | 62.1 | 57.9 | 59.3
    |'
- en: '| W8A8 | 56.4 | 60.5 | 62.5 | 61.9 | 62.0 | 55.8 | 54.5 | 61.8 | 58.1 | 59.3
    |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 56.4 | 60.5 | 62.5 | 61.9 | 62.0 | 55.8 | 54.5 | 61.8 | 58.1 | 59.3
    |'
- en: '| W4-g | 55.4 | 59.7 | 62.0 | 61.0 | 60.7 | 54.4 | 53.2 | 60.8 | 56.6 | 58.2
    |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| W4-g | 55.4 | 59.7 | 62.0 | 61.0 | 60.7 | 54.4 | 53.2 | 60.8 | 56.6 | 58.2
    |'
- en: 'Table A9: mMMLU scores for Command Models. (Accuracy)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A9：Command Models 的 mMMLU 分数。（准确率）
- en: '|  |  | ar | de | es | fr | it | ja | ko | pt | zh | Avg | Ltn/IE | $\neg$
    |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ar | de | es | fr | it | ja | ko | pt | zh | 平均 | Ltn/IE | $\neg$ |'
- en: '|  | W8 | 0.2% | 0.0% | 0.0% | 0.1% | 0.1% | -0.2% | 0.0% | -0.1% | 0.0% |
    0.0% | 0.0% | 0.0% |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 0.2% | 0.0% | 0.0% | 0.1% | 0.1% | -0.2% | 0.0% | -0.1% | 0.0% |
    0.0% | 0.0% | 0.0% |'
- en: '|  | W8A8-sq | -0.8% | -0.6% | 0.1% | 0.1% | -0.3% | -1.3% | -0.8% | -1.1%
    | -0.2% | -0.5% | -0.4% | -0.8% |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | -0.8% | -0.6% | 0.1% | 0.1% | -0.3% | -1.3% | -0.8% | -1.1%
    | -0.2% | -0.5% | -0.4% | -0.8% |'
- en: '|  | W8A8 | -2.2% | -1.8% | -0.7% | -1.0% | -1.5% | -2.3% | -2.4% | -1.8% |
    -1.4% | -1.7% | -1.3% | -2.1% |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | -2.2% | -1.8% | -0.7% | -1.0% | -1.5% | -2.3% | -2.4% | -1.8% |
    -1.4% | -1.7% | -1.3% | -2.1% |'
- en: '|  | W4-g | -1.7% | -1.2% | -0.7% | -0.6% | -1.0% | -2.5% | -1.9% | -2.0% |
    -1.5% | -1.5% | -1.1% | -1.9% |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '|  | W4-g | -1.7% | -1.2% | -0.7% | -0.6% | -1.0% | -2.5% | -1.9% | -2.0% |
    -1.5% | -1.5% | -1.1% | -1.9% |'
- en: '| 103B | W4 | -5.5% | -3.8% | -3.1% | -3.9% | -3.8% | -5.1% | -4.8% | -4.8%
    | -4.4% | -4.4% | -3.9% | -4.9% |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 103B | W4 | -5.5% | -3.8% | -3.1% | -3.9% | -3.8% | -5.1% | -4.8% | -4.8%
    | -4.4% | -4.4% | -3.9% | -4.9% |'
- en: '|  | W8 | 0.0% | -0.2% | -0.2% | 0.0% | -0.2% | 0.0% | -0.2% | 0.2% | 0.0%
    | -0.1% | -0.1% | 0.0% |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 0.0% | -0.2% | -0.2% | 0.0% | -0.2% | 0.0% | -0.2% | 0.2% | 0.0%
    | -0.1% | -0.1% | 0.0% |'
- en: '|  | W8A8 | -0.2% | -0.3% | 0.3% | 0.2% | 0.0% | -1.1% | -0.5% | -0.3% | 0.3%
    | -0.2% | 0.0% | -0.4% |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | -0.2% | -0.3% | 0.3% | 0.2% | 0.0% | -1.1% | -0.5% | -0.3% | 0.3%
    | -0.2% | 0.0% | -0.4% |'
- en: '| 35B | W4-g | -1.9% | -1.6% | -0.5% | -1.3% | -2.1% | -3.5% | -2.9% | -1.9%
    | -2.2% | -2.0% | -1.5% | -2.7% |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 35B | W4-g | -1.9% | -1.6% | -0.5% | -1.3% | -2.1% | -3.5% | -2.9% | -1.9%
    | -2.2% | -2.0% | -1.5% | -2.7% |'
- en: 'Table A10: Relative performance (%$\Delta$ are the rest: ar, ja, ko, zh.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A10：相对性能（%$\Delta$为其余：ar, ja, ko, zh）。
- en: '|  |  | Avg (-en) | Avg | ar | de | en | es | fr | hi | id | it | nl | pt |
    ro | ru | uk | vi | zh |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 平均（-en） | 平均 | ar | de | en | es | fr | hi | id | it | nl | pt | ro
    | ru | uk | vi | zh |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- |'
- en: '|  | FP16 | 58.2 | 58.8 | 53.9 | 60.4 | 66.7 | 61.6 | 62.0 | 47.8 | 58.9 |
    61.5 | 60.3 | 62.0 | 59.7 | 57.8 | 56.3 | 55.3 | 57.5 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 58.2 | 58.8 | 53.9 | 60.4 | 66.7 | 61.6 | 62.0 | 47.8 | 58.9 |
    61.5 | 60.3 | 62.0 | 59.7 | 57.8 | 56.3 | 55.3 | 57.5 |'
- en: '| Aya-23-35b | W8 | 57.9 | 58.5 | 53.8 | 60.0 | 66.2 | 61.7 | 61.7 | 47.4 |
    58.7 | 61.1 | 60.0 | 61.6 | 59.1 | 57.5 | 56.1 | 54.9 | 57.5 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-35b | W8 | 57.9 | 58.5 | 53.8 | 60.0 | 66.2 | 61.7 | 61.7 | 47.4 |
    58.7 | 61.1 | 60.0 | 61.6 | 59.1 | 57.5 | 56.1 | 54.9 | 57.5 |'
- en: '|  | W4 | 56.6 | 57.2 | 52.3 | 58.7 | 65.2 | 60.3 | 60.4 | 45.7 | 57.4 | 59.8
    | 58.6 | 60.5 | 57.7 | 56.5 | 55.0 | 53.8 | 56.1 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | 56.6 | 57.2 | 52.3 | 58.7 | 65.2 | 60.3 | 60.4 | 45.7 | 57.4 | 59.8
    | 58.6 | 60.5 | 57.7 | 56.5 | 55.0 | 53.8 | 56.1 |'
- en: '|  | FP16 | 48.2 | 48.6 | 45.1 | 50.0 | 54.6 | 50.9 | 51.0 | 39.7 | 48.8 |
    50.7 | 49.7 | 50.8 | 49.9 | 47.8 | 46.8 | 46.5 | 47.1 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 48.2 | 48.6 | 45.1 | 50.0 | 54.6 | 50.9 | 51.0 | 39.7 | 48.8 |
    50.7 | 49.7 | 50.8 | 49.9 | 47.8 | 46.8 | 46.5 | 47.1 |'
- en: '| Aya-23-8b | W8 | 47.8 | 48.2 | 44.9 | 49.9 | 54.2 | 50.5 | 50.6 | 39.4 |
    48.5 | 50.2 | 49.4 | 50.6 | 49.2 | 47.4 | 46.3 | 45.7 | 46.4 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-8b | W8 | 47.8 | 48.2 | 44.9 | 49.9 | 54.2 | 50.5 | 50.6 | 39.4 |
    48.5 | 50.2 | 49.4 | 50.6 | 49.2 | 47.4 | 46.3 | 45.7 | 46.4 |'
- en: '|  | W4 | 46.7 | 47.1 | 43.9 | 48.4 | 53.4 | 49.4 | 49.0 | 38.4 | 47.5 | 49.1
    | 47.9 | 49.1 | 48.0 | 46.2 | 45.6 | 44.9 | 46.1 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | 46.7 | 47.1 | 43.9 | 48.4 | 53.4 | 49.4 | 49.0 | 38.4 | 47.5 | 49.1
    | 47.9 | 49.1 | 48.0 | 46.2 | 45.6 | 44.9 | 46.1 |'
- en: 'Table A11: Aya 23 language-specific results for mMMLU (Okapi). (Accuracy)'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A11: Aya 23 语言特定的 mMMLU (Okapi) 结果。 (准确率)'
- en: '|  |  | ar | de | en | es | fr | hi | id | it | nl | pt | ro | ru | uk | vi
    | zh | Avg | Avg (-en) | Ltn | $\neg$ |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ar | de | en | es | fr | hi | id | it | nl | pt | ro | ru | uk | vi
    | zh | 平均 | 平均 (-en) | Ltn | $\neg$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | W8 | -0.2% | -0.7% | -0.6% | 0.2% | -0.4% | -0.7% | -0.3% | -0.6% | -0.5%
    | -0.6% | -1.1% | -0.6% | -0.3% | -0.8% | -0.1% | -0.5% | -0.5% | -0.5% | -0.4%
    |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -0.2% | -0.7% | -0.6% | 0.2% | -0.4% | -0.7% | -0.3% | -0.6% | -0.5%
    | -0.6% | -1.1% | -0.6% | -0.3% | -0.8% | -0.1% | -0.5% | -0.5% | -0.5% | -0.4%
    |'
- en: '| Aya-23-35b | W4 | -3.1% | -2.8% | -2.2% | -2.0% | -2.5% | -4.4% | -2.5% |
    -2.7% | -2.8% | -2.5% | -3.3% | -2.2% | -2.2% | -2.7% | -2.6% | -2.7% | -2.7%
    | -2.6% | -2.9% |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-35b | W4 | -3.1% | -2.8% | -2.2% | -2.0% | -2.5% | -4.4% | -2.5% |
    -2.7% | -2.8% | -2.5% | -3.3% | -2.2% | -2.2% | -2.7% | -2.6% | -2.7% | -2.7%
    | -2.6% | -2.9% |'
- en: '|  | W8 | -0.5% | -0.3% | -0.8% | -0.8% | -0.8% | -0.9% | -0.6% | -1.0% | -0.7%
    | -0.4% | -1.4% | -0.9% | -1.1% | -1.7% | -1.5% | -0.9% | -0.9% | -0.8% | -1.0%
    |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -0.5% | -0.3% | -0.8% | -0.8% | -0.8% | -0.9% | -0.6% | -1.0% | -0.7%
    | -0.4% | -1.4% | -0.9% | -1.1% | -1.7% | -1.5% | -0.9% | -0.9% | -0.8% | -1.0%
    |'
- en: '| Aya-23-8b | W4 | -2.7% | -3.3% | -2.1% | -2.9% | -3.9% | -3.4% | -2.6% |
    -3.2% | -3.7% | -3.3% | -3.8% | -3.4% | -2.7% | -3.5% | -2.2% | -3.1% | -3.2%
    | -3.2% | -2.9% |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-8b | W4 | -2.7% | -3.3% | -2.1% | -2.9% | -3.9% | -3.4% | -2.6% |
    -3.2% | -3.7% | -3.3% | -3.8% | -3.4% | -2.7% | -3.5% | -2.2% | -3.1% | -3.2%
    | -3.2% | -2.9% |'
- en: 'Table A12: Relative performance ($\%\Delta$ are the rest.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A12: 相对性能（$\%\Delta$ 其余部分）。'
- en: '|  |  | English $\rightarrow$ English |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 英语 $\rightarrow$ 英语 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |  | ar | de | es | fr | it | ja | ko | pt | zh | Avg | ar | de | es | fr
    | it | ja | ko | pt | zh | Avg |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ar | de | es | fr | it | ja | ko | pt | zh | 平均 | ar | de | es | fr
    | it | ja | ko | pt | zh | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 103B | FP16 | 27.1 | 40.0 | 30.1 | 50.6 | 33.1 | 33.1 | 29.1 | 51.0 | 45.1
    | 37.7 | 45.0 | 46.3 | 33.4 | 48.6 | 36.5 | 29.5 | 33.0 | 52.2 | 32.1 | 39.6 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 103B | FP16 | 27.1 | 40.0 | 30.1 | 50.6 | 33.1 | 33.1 | 29.1 | 51.0 | 45.1
    | 37.7 | 45.0 | 46.3 | 33.4 | 48.6 | 36.5 | 29.5 | 33.0 | 52.2 | 32.1 | 39.6 |'
- en: '| W8 | 27.2 | 40.0 | 30.0 | 50.7 | 33.1 | 33.2 | 29.1 | 50.9 | 45.1 | 37.7
    | 45.2 | 46.3 | 33.4 | 48.5 | 36.5 | 29.5 | 33.0 | 52.1 | 32.0 | 39.6 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| W8 | 27.2 | 40.0 | 30.0 | 50.7 | 33.1 | 33.2 | 29.1 | 50.9 | 45.1 | 37.7
    | 45.2 | 46.3 | 33.4 | 48.5 | 36.5 | 29.5 | 33.0 | 52.1 | 32.0 | 39.6 |'
- en: '| W8A8-sq | 26.8 | 40.3 | 30.0 | 51.0 | 33.0 | 33.1 | 29.3 | 51.2 | 45.1 |
    37.8 | 44.5 | 46.2 | 32.9 | 48.1 | 35.9 | 29.3 | 32.5 | 51.6 | 31.2 | 39.1 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| W8A8-sq | 26.8 | 40.3 | 30.0 | 51.0 | 33.0 | 33.1 | 29.3 | 51.2 | 45.1 |
    37.8 | 44.5 | 46.2 | 32.9 | 48.1 | 35.9 | 29.3 | 32.5 | 51.6 | 31.2 | 39.1 |'
- en: '| W8A8 | 26.9 | 39.8 | 30.0 | 50.9 | 33.0 | 33.7 | 29.0 | 51.1 | 45.1 | 37.7
    | 44.4 | 45.9 | 33.1 | 47.9 | 36.2 | 29.2 | 32.5 | 51.8 | 31.4 | 39.1 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 26.9 | 39.8 | 30.0 | 50.9 | 33.0 | 33.7 | 29.0 | 51.1 | 45.1 | 37.7
    | 44.4 | 45.9 | 33.1 | 47.9 | 36.2 | 29.2 | 32.5 | 51.8 | 31.4 | 39.1 |'
- en: '| W4-g | 27.3 | 40.4 | 30.1 | 51.0 | 33.0 | 33.9 | 29.3 | 50.9 | 44.7 | 37.8
    | 44.9 | 46.4 | 33.2 | 48.4 | 36.3 | 29.3 | 32.7 | 52.0 | 31.6 | 39.4 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| W4-g | 27.3 | 40.4 | 30.1 | 51.0 | 33.0 | 33.9 | 29.3 | 50.9 | 44.7 | 37.8
    | 44.9 | 46.4 | 33.2 | 48.4 | 36.3 | 29.3 | 32.7 | 52.0 | 31.6 | 39.4 |'
- en: '| W4 | 26.9 | 39.1 | 29.9 | 50.0 | 32.8 | 32.8 | 27.9 | 50.3 | 44.0 | 37.1
    | 44.2 | 45.8 | 33.1 | 47.9 | 36.0 | 29.0 | 32.3 | 51.8 | 30.9 | 39.0 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| W4 | 26.9 | 39.1 | 29.9 | 50.0 | 32.8 | 32.8 | 27.9 | 50.3 | 44.0 | 37.1
    | 44.2 | 45.8 | 33.1 | 47.9 | 36.0 | 29.0 | 32.3 | 51.8 | 30.9 | 39.0 |'
- en: '| 35B | FP16 | 20.1 | 33.5 | 27.8 | 44.5 | 29.7 | 27.0 | 22.7 | 45.5 | 40.4
    | 32.4 | 38.4 | 41.2 | 31.8 | 43.1 | 34.0 | 26.2 | 28.4 | 48.1 | 28.4 | 35.5 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 35B | FP16 | 20.1 | 33.5 | 27.8 | 44.5 | 29.7 | 27.0 | 22.7 | 45.5 | 40.4
    | 32.4 | 38.4 | 41.2 | 31.8 | 43.1 | 34.0 | 26.2 | 28.4 | 48.1 | 28.4 | 35.5 |'
- en: '| W8 | 20.0 | 33.4 | 27.8 | 44.5 | 29.7 | 26.9 | 22.9 | 45.3 | 40.3 | 32.3
    | 38.3 | 41.1 | 31.7 | 43.0 | 34.0 | 26.4 | 28.2 | 48.0 | 28.2 | 35.4 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| W8 | 20.0 | 33.4 | 27.8 | 44.5 | 29.7 | 26.9 | 22.9 | 45.3 | 40.3 | 32.3
    | 38.3 | 41.1 | 31.7 | 43.0 | 34.0 | 26.4 | 28.2 | 48.0 | 28.2 | 35.4 |'
- en: '| W8A8 | 21.2 | 34.1 | 27.8 | 45.1 | 30.0 | 27.6 | 23.1 | 46.1 | 40.8 | 32.9
    | 38.5 | 42.2 | 31.7 | 43.5 | 34.2 | 26.5 | 28.6 | 48.6 | 28.7 | 35.8 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 21.2 | 34.1 | 27.8 | 45.1 | 30.0 | 27.6 | 23.1 | 46.1 | 40.8 | 32.9
    | 38.5 | 42.2 | 31.7 | 43.5 | 34.2 | 26.5 | 28.6 | 48.6 | 28.7 | 35.8 |'
- en: '| W4-g | 18.8 | 32.9 | 27.7 | 43.9 | 29.6 | 26.0 | 22.1 | 45.1 | 39.7 | 31.7
    | 38.3 | 41.4 | 31.0 | 43.1 | 34.0 | 25.5 | 28.1 | 48.0 | 28.0 | 35.3 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| W4-g | 18.8 | 32.9 | 27.7 | 43.9 | 29.6 | 26.0 | 22.1 | 45.1 | 39.7 | 31.7
    | 38.3 | 41.4 | 31.0 | 43.1 | 34.0 | 25.5 | 28.1 | 48.0 | 28.0 | 35.3 |'
- en: 'Table A13: Full results on FLORES for Command Models. (SacreBLEU)'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A13: Command 模型在 FLORES 上的完整结果。 (SacreBLEU)'
- en: '|  |  | English $\rightarrow$ English |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 英语 $\rightarrow$ 英语 |'
- en: '|  |  | ar | de | es | fr | it | ja | ko | pt | zh | Avg | Ltn/IE | $\neg$
    |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ar | de | es | fr | it | ja | ko | pt | zh | 平均 | Ltn/IE | $\neg$ |'
- en: '|  | W8 | 0.1% | 0.1% | -0.4% | 0.2% | 0.1% | 0.3% | -0.2% | -0.3% | 0.1% |
    0.0% | -0.1% | 0.1% | 0.4% | -0.1% | -0.1% | -0.1% | -0.1% | -0.1% | 0.0% | -0.1%
    | -0.1% | 0.0% | -0.1% | 0.1% |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 0.1% | 0.1% | -0.4% | 0.2% | 0.1% | 0.3% | -0.2% | -0.3% | 0.1% |
    0.0% | -0.1% | 0.1% | 0.4% | -0.1% | -0.1% | -0.1% | -0.1% | -0.1% | 0.0% | -0.1%
    | -0.1% | 0.0% | -0.1% | 0.1% |'
- en: '|  | W8A8-sq | -1.1% | 0.8% | -0.4% | 0.7% | -0.2% | 0.2% | 0.7% | 0.3% | 0.1%
    | 0.1% | 0.2% | 0.0% | -1.2% | -0.1% | -1.4% | -1.0% | -1.8% | -0.7% | -1.6% |
    -1.1% | -2.9% | -1.3% | -1.1% | -1.6% |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | -1.1% | 0.8% | -0.4% | 0.7% | -0.2% | 0.2% | 0.7% | 0.3% | 0.1%
    | 0.1% | 0.2% | 0.0% | -1.2% | -0.1% | -1.4% | -1.0% | -1.8% | -0.7% | -1.6% |
    -1.1% | -2.9% | -1.3% | -1.1% | -1.6% |'
- en: '|  | W8A8 | -1.0% | -0.4% | -0.5% | 0.5% | -0.4% | 1.8% | -0.4% | 0.1% | 0.0%
    | 0.0% | -0.1% | 0.1% | -1.3% | -0.8% | -1.1% | -1.4% | -1.0% | -1.2% | -1.7%
    | -0.8% | -2.1% | -1.3% | -1.0% | -1.6% |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | -1.0% | -0.4% | -0.5% | 0.5% | -0.4% | 1.8% | -0.4% | 0.1% | 0.0%
    | 0.0% | -0.1% | 0.1% | -1.3% | -0.8% | -1.1% | -1.4% | -1.0% | -1.2% | -1.7%
    | -0.8% | -2.1% | -1.3% | -1.0% | -1.6% |'
- en: '|  | W4-g | 0.7% | 1.0% | -0.3% | 0.8% | -0.3% | 2.6% | 0.5% | -0.3% | -0.8%
    | 0.4% | 0.2% | 0.7% | -0.3% | 0.2% | -0.6% | -0.3% | -0.6% | -0.7% | -0.9% |
    -0.3% | -1.4% | -0.6% | -0.3% | -0.8% |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|  | W4-g | 0.7% | 1.0% | -0.3% | 0.8% | -0.3% | 2.6% | 0.5% | -0.3% | -0.8%
    | 0.4% | 0.2% | 0.7% | -0.3% | 0.2% | -0.6% | -0.3% | -0.6% | -0.7% | -0.9% |
    -0.3% | -1.4% | -0.6% | -0.3% | -0.8% |'
- en: '| 103B | W4 | -0.8% | -2.2% | -0.7% | -1.3% | -0.9% | -0.8% | -4.3% | -1.5%
    | -2.3% | -1.6% | -1.3% | -2.0% | -1.8% | -1.1% | -0.8% | -1.4% | -1.6% | -1.7%
    | -2.2% | -0.7% | -3.6% | -1.7% | -1.1% | -2.3% |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 103B | W4 | -0.8% | -2.2% | -0.7% | -1.3% | -0.9% | -0.8% | -4.3% | -1.5%
    | -2.3% | -1.6% | -1.3% | -2.0% | -1.8% | -1.1% | -0.8% | -1.4% | -1.6% | -1.7%
    | -2.2% | -0.7% | -3.6% | -1.7% | -1.1% | -2.3% |'
- en: '|  | W8 | -0.7% | -0.4% | -0.1% | 0.0% | 0.0% | -0.2% | 0.7% | -0.4% | -0.2%
    | -0.1% | -0.2% | -0.1% | -0.2% | -0.2% | -0.5% | -0.3% | 0.0% | 0.8% | -0.5%
    | -0.1% | -0.6% | -0.2% | -0.2% | -0.1% |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -0.7% | -0.4% | -0.1% | 0.0% | 0.0% | -0.2% | 0.7% | -0.4% | -0.2%
    | -0.1% | -0.2% | -0.1'
- en: '|  | W8A8 | 5.5% | 1.9% | 0.1% | 1.4% | 0.9% | 2.1% | 1.9% | 1.4% | 0.9% |
    1.8% | 1.1% | 2.6% | 0.5% | 2.5% | -0.6% | 0.9% | 0.5% | 1.1% | 0.8% | 1.1% |
    1.0% | 0.9% | 0.9% | 0.8% |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | 5.5% | 1.9% | 0.1% | 1.4% | 0.9% | 2.1% | 1.9% | 1.4% | 0.9% |
    1.8% | 1.1% | 2.6% | 0.5% | 2.5% | -0.6% | 0.9% | 0.5% | 1.1% | 0.8% | 1.1% |
    1.0% | 0.9% | 0.9% | 0.8% |'
- en: '| 35B | W4-g | -6.7% | -1.9% | -0.4% | -1.3% | -0.4% | -3.9% | -2.8% | -0.7%
    | -1.7% | -2.2% | -1.0% | -3.8% | -0.1% | 0.6% | -2.5% | 0.0% | -0.1% | -2.8%
    | -1.1% | -0.2% | -1.4% | -0.8% | -0.5% | -1.3% |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 35B | W4-g | -6.7% | -1.9% | -0.4% | -1.3% | -0.4% | -3.9% | -2.8% | -0.7%
    | -1.7% | -2.2% | -1.0% | -3.8% | -0.1% | 0.6% | -2.5% | 0.0% | -0.1% | -2.8%
    | -1.1% | -0.2% | -1.4% | -0.8% | -0.5% | -1.3% |'
- en: 'Table A14: Relative performance (%$\Delta$ are the rest: ar, ja, ko, zh.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A14：相对表现（%$\Delta$ 其余为：ar, ja, ko, zh）。
- en: '|  | English$\rightarrow$L2 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  | 英文$\rightarrow$L2 |'
- en: '|  |  | ar | cs | de | el | es | fa | fr | he | hi | id | it | ja | ko | nl
    | pl | pt | ro | ru | tr | uk | vi | zh | Avg |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ar | cs | de | el | es | fa | fr | he | hi | id | it | ja | ko | nl
    | pl | pt | ro | ru | tr | uk | vi | zh | 平均 |'
- en: '|  | FP16 | 40.0 | 39.1 | 42.5 | 36.3 | 32.1 | 33.4 | 54.1 | 39.5 | 31.9 |
    44.7 | 36.6 | 28.7 | 25.5 | 33.4 | 30.7 | 53.1 | 43.3 | 38.9 | 33.8 | 38.2 | 41.0
    | 34.0 | 37.8 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 40.0 | 39.1 | 42.5 | 36.3 | 32.1 | 33.4 | 54.1 | 39.5 | 31.9 |
    44.7 | 36.6 | 28.7 | 25.5 | 33.4 | 30.7 | 53.1 | 43.3 | 38.9 | 33.8 | 38.2 | 41.0
    | 34.0 | 37.8 |'
- en: '| Aya-23-35b | W8 | 40.0 | 39.0 | 42.9 | 36.2 | 32.2 | 33.7 | 53.9 | 40.0 |
    32.3 | 44.8 | 36.5 | 28.9 | 25.5 | 33.6 | 30.9 | 53.2 | 43.4 | 38.7 | 33.8 | 38.3
    | 41.4 | 33.8 | 37.9 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-35b | W8 | 40.0 | 39.0 | 42.9 | 36.2 | 32.2 | 33.7 | 53.9 | 40.0 |
    32.3 | 44.8 | 36.5 | 28.9 | 25.5 | 33.6 | 30.9 | 53.2 | 43.4 | 38.7 | 33.8 | 38.3
    | 41.4 | 33.8 | 37.9 |'
- en: '|  | W4 | 39.3 | 38.0 | 42.5 | 36.0 | 32.0 | 32.6 | 53.3 | 39.1 | 31.2 | 44.6
    | 36.1 | 28.2 | 25.1 | 32.9 | 30.0 | 52.8 | 42.6 | 38.3 | 33.2 | 37.8 | 40.8 |
    33.1 | 37.3 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | 39.3 | 38.0 | 42.5 | 36.0 | 32.0 | 32.6 | 53.3 | 39.1 | 31.2 | 44.6
    | 36.1 | 28.2 | 25.1 | 32.9 | 30.0 | 52.8 | 42.6 | 38.3 | 33.2 | 37.8 | 40.8 |
    33.1 | 37.3 |'
- en: '|  | FP16 | 36.3 | 35.7 | 39.3 | 34.0 | 31.5 | 30.0 | 51.0 | 35.0 | 27.2 |
    43.4 | 34.7 | 24.9 | 22.0 | 32.2 | 28.4 | 50.2 | 41.6 | 35.0 | 29.1 | 34.2 | 39.0
    | 30.1 | 34.8 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 36.3 | 35.7 | 39.3 | 34.0 | 31.5 | 30.0 | 51.0 | 35.0 | 27.2 |
    43.4 | 34.7 | 24.9 | 22.0 | 32.2 | 28.4 | 50.2 | 41.6 | 35.0 | 29.1 | 34.2 | 39.0
    | 30.1 | 34.8 |'
- en: '| Aya-23-8b | W8 | 36.5 | 36.1 | 39.5 | 33.9 | 31.4 | 30.4 | 51.4 | 35.0 |
    27.0 | 43.2 | 34.8 | 24.8 | 22.2 | 32.1 | 28.5 | 50.0 | 42.0 | 34.9 | 28.9 | 34.3
    | 39.0 | 30.6 | 34.8 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-8b | W8 | 36.5 | 36.1 | 39.5 | 33.9 | 31.4 | 30.4 | 51.4 | 35.0 |
    27.0 | 43.2 | 34.8 | 24.8 | 22.2 | 32.1 | 28.5 | 50.0 | 42.0 | 34.9 | 28.9 | 34.3
    | 39.0 | 30.6 | 34.8 |'
- en: '|  | W4 | 35.4 | 35.0 | 39.2 | 33.4 | 31.2 | 29.6 | 50.2 | 33.3 | 26.4 | 42.8
    | 34.3 | 24.3 | 21.5 | 31.8 | 28.0 | 49.8 | 40.9 | 34.2 | 28.1 | 33.7 | 38.5 |
    29.4 | 34.1 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | 35.4 | 35.0 | 39.2 | 33.4 | 31.2 | 29.6 | 50.2 | 33.3 | 26.4 | 42.8
    | 34.3 | 24.3 | 21.5 | 31.8 | 28.0 | 49.8 | 40.9 | 34.2 | 28.1 | 33.7 | 38.5 |
    29.4 | 34.1 |'
- en: '|  | L2$\rightarrow$English |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '|  | L2$\rightarrow$英文 |'
- en: '|  | FP16 | 46.4 | 45.3 | 48.9 | 42.4 | 37.7 | 41.3 | 50.6 | 48.3 | 42.7 |
    48.5 | 40.5 | 33.7 | 35.3 | 37.7 | 36.4 | 54.8 | 49.5 | 41.6 | 42.2 | 44.8 | 41.4
    | 34.8 | 42.9 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 46.4 | 45.3 | 48.9 | 42.4 | 37.7 | 41.3 | 50.6 | 48.3 | 42.7 |
    48.5 | 40.5 | 33.7 | 35.3 | 37.7 | 36.4 | 54.8 | 49.5 | 41.6 | 42.2 | 44.8 | 41.4
    | 34.8 | 42.9 |'
- en: '| Aya-23-35b | W8 | 46.4 | 45.4 | 49.0 | 42.2 | 37.3 | 41.4 | 50.7 | 48.6 |
    42.9 | 48.7 | 40.5 | 34.0 | 35.1 | 37.5 | 36.4 | 54.8 | 49.5 | 41.7 | 42.2 | 45.0
    | 41.5 | 34.9 | 43.0 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-35b | W8 | 46.4 | 45.4 | 49.0 | 42.2 | 37.3 | 41.4 | 50.7 | 48.6 |
    42.9 | 48.7 | 40.5 | 34.0 | 35.1 | 37.5 | 36.4 | 54.8 | 49.5 | 41.7 | 42.2 | 45.0
    | 41.5 | 34.9 | 43.0 |'
- en: '|  | W4 | 45.7 | 44.9 | 48.5 | 41.8 | 37.5 | 40.5 | 50.4 | 47.3 | 41.8 | 48.0
    | 40.8 | 33.1 | 34.4 | 37.2 | 35.8 | 54.3 | 49.2 | 41.6 | 41.4 | 44.2 | 40.9 |
    34.2 | 42.4 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | 45.7 | 44.9 | 48.5 | 41.8 | 37.5 | 40.5 | 50.4 | 47.3 | 41.8 | 48.0
    | 40.8 | 33.1 | 34.4 | 37.2 | 35.8 | 54.3 | 49.2 | 41.6 | 41.4 | 44.2 | 40.9 |
    34.2 | 42.4 |'
- en: '|  | FP16 | 42.4 | 42.0 | 46.5 | 38.7 | 35.4 | 36.5 | 48.1 | 43.7 | 37.4 |
    45.5 | 37.9 | 29.9 | 30.9 | 35.8 | 33.6 | 51.7 | 46.7 | 38.6 | 36.9 | 41.2 | 38.2
    | 31.6 | 39.5 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 42.4 | 42.0 | 46.5 | 38.7 | 35.4 | 36.5 | 48.1 | 43.7 | 37.4 |
    45.5 | 37.9 | 29.9 | 30.9 | 35.8 | 33.6 | 51.7 | 46.7 | 38.6 | 36.9 | 41.2 | 38.2
    | 31.6 | 39.5 |'
- en: '| Aya-23-8b | W8 | 42.1 | 42.5 | 46.7 | 39.2 | 35.5 | 36.8 | 48.1 | 44.2 |
    37.7 | 45.5 | 38.2 | 30.0 | 31.3 | 35.6 | 33.7 | 52.0 | 46.6 | 38.5 | 37.0 | 41.6
    | 38.4 | 31.9 | 39.7 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-8b | W8 | 42.1 | 42.5 | 46.7 | 39.2 | 35.5 | 36.8 | 48.1 | 44.2 |
    37.7 | 45.5 | 38.2 | 30.0 | 31.3 | 35.6 | 33.7 | 52.0 | 46.6 | 38.5 | 37.0 | 41.6
    | 38.4 | 31.9 | 39.7 |'
- en: '|  | W4 | 41.4 | 42.2 | 46.2 | 38.1 | 35.8 | 36.7 | 47.4 | 42.8 | 36.2 | 44.7
    | 38.5 | 29.7 | 30.1 | 35.7 | 33.1 | 51.9 | 46.1 | 38.3 | 35.7 | 40.7 | 37.6 |
    31.6 | 39.1 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | 41.4 | 42.2 | 46.2 | 38.1 | 35.8 | 36.7 | 47.4 | 42.8 | 36.2 | 44.7
    | 38.5 | 29.7 | 30.1 | 35.7 | 33.1 | 51.9 | 46.1 | 38.3 | 35.7 | 40.7 | 37.6 |
    31.6 | 39.1 |'
- en: 'Table A15: Aya 23 language-specific results for FLORES. spBLEU with FLORES200
    tokenizer.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A15: Aya 23 针对 FLORES 的语言特定结果。使用 FLORES200 分词器的 spBLEU。'
- en: '|  | English$\rightarrow$L2 |  |  |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '|  | English$\rightarrow$L2 |  |  |'
- en: '|  |  | ar | cs | de | el | es | fa | fr | he | hi | id | it | ja | ko | nl
    | pl | pt | ro | ru | tr | uk | vi | zh | Avg | Ltn | $\neg$ |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ar | cs | de | el | es | fa | fr | he | hi | id | it | ja | ko | nl
    | pl | pt | ro | ru | tr | uk | vi | zh | Avg | Ltn | $\neg$ |'
- en: '|  | W8 | -0.1% | -0.2% | 1.1% | -0.1% | 0.2% | 0.7% | -0.4% | 1.2% | 1.3%
    | 0.3% | -0.4% | 0.6% | 0.0% | 0.6% | 0.7% | 0.3% | 0.4% | -0.6% | 0.1% | 0.2%
    | 0.9% | -0.8% | 0.3% | 0.3% | 0.3% |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -0.1% | -0.2% | 1.1% | -0.1% | 0.2% | 0.7% | -0.4% | 1.2% | 1.3%
    | 0.3% | -0.4% | 0.6% | 0.0% | 0.6% | 0.7% | 0.3% | 0.4% | -0.6% | 0.1% | 0.2%
    | 0.9% | -0.8% | 0.3% | 0.3% | 0.3% |'
- en: '| Aya-23-35b | W4 | -1.7% | -2.7% | 0.0% | -0.7% | -0.4% | -2.4% | -1.4% |
    -1.0% | -2.0% | -0.2% | -1.5% | -2.0% | -1.5% | -1.4% | -2.2% | -0.5% | -1.6%
    | -1.6% | -1.8% | -1.1% | -0.5% | -2.7% | -1.4% | -1.2% | -1.7% |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-35b | W4 | -1.7% | -2.7% | 0.0% | -0.7% | -0.4% | -2.4% | -1.4% |
    -1.0% | -2.0% | -0.2% | -1.5% | -2.0% | -1.5% | -1.4% | -2.2% | -0.5% | -1.6%
    | -1.6% | -1.8% | -1.1% | -0.5% | -2.7% | -1.4% | -1.2% | -1.7% |'
- en: '|  | W8 | 0.5% | 1.1% | 0.6% | -0.3% | -0.4% | 1.3% | 0.8% | -0.1% | -0.9%
    | -0.3% | 0.2% | -0.2% | 1.0% | -0.1% | 0.2% | -0.5% | 0.9% | -0.1% | -0.7% |
    0.0% | 0.0% | 1.7% | 0.2% | 0.2% | 0.3% |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 0.5% | 1.1% | 0.6% | -0.3% | -0.4% | 1.3% | 0.8% | -0.1% | -0.9%
    | -0.3% | 0.2% | -0.2% | 1.0% | -0.1% | 0.2% | -0.5% | 0.9% | -0.1% | -0.7% |
    0.0% | 0.0% | 1.7% | 0.2% | 0.2% | 0.3% |'
- en: '| Aya-23-8b | W4 | -2.5% | -2.0% | -0.3% | -1.7% | -1.0% | -1.4% | -1.6% |
    -5.1% | -3.1% | -1.2% | -1.3% | -2.1% | -2.0% | -1.1% | -1.3% | -0.9% | -1.8%
    | -2.1% | -3.6% | -1.7% | -1.1% | -2.2% | -1.9% | -1.4% | -2.4% |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-8b | W4 | -2.5% | -2.0% | -0.3% | -1.7% | -1.0% | -1.4% | -1.6% |
    -5.1% | -3.1% | -1.2% | -1.3% | -2.1% | -2.0% | -1.1% | -1.3% | -0.9% | -1.8%
    | -2.1% | -3.6% | -1.7% | -1.1% | -2.2% | -1.9% | -1.4% | -2.4% |'
- en: '|  | L2$\rightarrow$English |  |  |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '|  | L2$\rightarrow$English |  |  |'
- en: '|  | W8 | 0.0% | 0.3% | 0.2% | -0.5% | -1.1% | 0.3% | 0.3% | 0.7% | 0.4% |
    0.3% | 0.0% | 0.8% | -0.7% | -0.5% | 0.1% | -0.1% | -0.1% | 0.3% | -0.1% | 0.4%
    | 0.2% | 0.2% | 0.1% | 0.0% | 0.2% |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 0.0% | 0.3% | 0.2% | -0.5% | -1.1% | 0.3% | 0.3% | 0.7% | 0.4% |
    0.3% | 0.0% | 0.8% | -0.7% | -0.5% | 0.1% | -0.1% | -0.1% | 0.3% | -0.1% | 0.4%
    | 0.2% | 0.2% | 0.1% | 0.0% | 0.2% |'
- en: '| Aya-23-35b | W4 | -1.6% | -0.9% | -1.0% | -1.4% | -0.4% | -2.0% | -0.4% |
    -2.0% | -2.2% | -1.1% | 0.8% | -1.6% | -2.7% | -1.5% | -1.5% | -0.9% | -0.5% |
    0.1% | -2.0% | -1.5% | -1.4% | -2.0% | -1.2% | -0.9% | -1.7% |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-35b | W4 | -1.6% | -0.9% | -1.0% | -1.4% | -0.4% | -2.0% | -0.4% |
    -2.0% | -2.2% | -1.1% | 0.8% | -1.6% | -2.7% | -1.5% | -1.5% | -0.9% | -0.5% |
    0.1% | -2.0% | -1.5% | -1.4% | -2.0% | -1.2% | -0.9% | -1.7% |'
- en: '|  | W8 | -0.6% | 1.3% | 0.4% | 1.3% | 0.3% | 0.8% | 0.1% | 1.1% | 0.6% | 0.0%
    | 0.7% | 0.3% | 1.4% | -0.4% | 0.4% | 0.6% | -0.1% | -0.3% | 0.3% | 0.8% | 0.6%
    | 0.8% | 0.5% | 0.4% | 0.6% |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | -0.6% | 1.3% | 0.4% | 1.3% | 0.3% | 0.8% | 0.1%'
- en: '| Aya-23-8b | W4 | -2.2% | 0.6% | -0.6% | -1.6% | 1.2% | 0.4% | -1.3% | -2.1%
    | -3.2% | -1.8% | 1.5% | -0.6% | -2.7% | -0.3% | -1.3% | 0.5% | -1.2% | -0.9%
    | -3.2% | -1.4% | -1.6% | -0.1% | -1.0% | -0.6% | -1.4% |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-8b | W4 | -2.2% | 0.6% | -0.6% | -1.6% | 1.2% | 0.4% | -1.3% | -2.1%
    | -3.2% | -1.8% | 1.5% | -0.6% | -2.7% | -0.3% | -1.3% | 0.5% | -1.2% | -0.9%
    | -3.2% | -1.4% | -1.6% | -0.1% | -1.0% | -0.6% | -1.4% |'
- en: 'Table A16: Relative performance ($\%\Delta$ are the rest.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A16: 相对性能（$\%\Delta$ 是其余部分）。'
- en: '|  |  | Avg | XSC | XCOPA | XWNG |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 平均 | XSC | XCOPA | XWNG |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  | FP16 | 70.8 | 65.1 | 62.8 | 84.4 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 70.8 | 65.1 | 62.8 | 84.4 |'
- en: '| Aya-23-35b | W8 | 70.6 | 65.0 | 62.9 | 83.9 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-35b | W8 | 70.6 | 65.0 | 62.9 | 83.9 |'
- en: '|  | W4 | 70.5 | 64.8 | 62.3 | 84.5 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | 70.5 | 64.8 | 62.3 | 84.5 |'
- en: '|  | FP16 | 67.6 | 62.3 | 59.8 | 80.7 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 67.6 | 62.3 | 59.8 | 80.7 |'
- en: '| Aya-23-8b | W8 | 67.6 | 62.4 | 60.0 | 80.6 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| Aya-23-8b | W8 | 67.6 | 62.4 | 60.0 | 80.6 |'
- en: '|  | W4 | 67.5 | 62.3 | 59.6 | 80.6 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | 67.5 | 62.3 | 59.6 | 80.6 |'
- en: 'Table A17: Performance of quantized Aya 23 models on unseen discriminative
    tasks. XStoryCloze (XSC), XCOPA, and XWinograd (XWNG).'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A17: 量化 Aya 23 模型在未见区分任务上的表现。 XStoryCloze (XSC)、XCOPA 和 XWinograd (XWNG)。'
- en: '|  |  | Monolingual | Cross-Lingual |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 单语 | 跨语言 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |  | ar | de | es | fr | it | ja | ko | pt | zh | Avg | ar | de | es | fr
    | it | ja | ko | pt | zh | Avg |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ar | de | es | fr | it | ja | ko | pt | zh | 平均 | ar | de | es | fr
    | it | ja | ko | pt | zh | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 103B | FP16 | 99.3 | 100.0 | 99.3 | 99.6 | 100.0 | 98.6 | 100.0 | 98.3 |
    97.9 | 99.2 | 93.0 | 90.6 | 91.2 | 91.6 | 93.0 | 93.1 | 91.1 | 88.3 | 91.3 | 91.5
    |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| 103B | FP16 | 99.3 | 100.0 | 99.3 | 99.6 | 100.0 | 98.6 | 100.0 | 98.3 |
    97.9 | 99.2 | 93.0 | 90.6 | 91.2 | 91.6 | 93.0 | 93.1 | 91.1 | 88.3 | 91.3 | 91.5
    |'
- en: '| W8 | 99.0 | 100.0 | 99.5 | 99.4 | 99.8 | 99.2 | 99.8 | 97.8 | 98.5 | 99.2
    | 92.6 | 91.1 | 91.7 | 91.4 | 92.9 | 92.8 | 91.3 | 87.4 | 89.7 | 91.2 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| W8 | 99.0 | 100.0 | 99.5 | 99.4 | 99.8 | 99.2 | 99.8 | 97.8 | 98.5 | 99.2
    | 92.6 | 91.1 | 91.7 | 91.4 | 92.9 | 92.8 | 91.3 | 87.4 | 89.7 | 91.2 |'
- en: '| W8A8-sq | 99.4 | 100.0 | 99.3 | 99.6 | 100.0 | 98.6 | 100.0 | 97.7 | 98.4
    | 99.2 | 93.3 | 91.5 | 91.4 | 92.4 | 92.1 | 93.3 | 92.1 | 87.6 | 90.0 | 91.5 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| W8A8-sq | 99.4 | 100.0 | 99.3 | 99.6 | 100.0 | 98.6 | 100.0 | 97.7 | 98.4
    | 99.2 | 93.3 | 91.5 | 91.4 | 92.4 | 92.1 | 93.3 | 92.1 | 87.6 | 90.0 | 91.5 |'
- en: '| W8A8 | 99.3 | 100.0 | 99.5 | 99.8 | 100.0 | 99.0 | 99.8 | 98.1 | 99.1 | 99.4
    | 91.3 | 89.3 | 91.0 | 91.1 | 91.8 | 93.0 | 89.3 | 87.3 | 89.2 | 90.4 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 99.3 | 100.0 | 99.5 | 99.8 | 100.0 | 99.0 | 99.8 | 98.1 | 99.1 | 99.4
    | 91.3 | 89.3 | 91.0 | 91.1 | 91.8 | 93.0 | 89.3 | 87.3 | 89.2 | 90.4 |'
- en: '| W4-g | 99.1 | 100.0 | 99.6 | 99.9 | 100.0 | 97.4 | 100.0 | 98.1 | 98.9 |
    99.2 | 90.6 | 89.9 | 90.7 | 91.7 | 93.1 | 92.8 | 90.6 | 85.4 | 89.6 | 90.5 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| W4-g | 99.1 | 100.0 | 99.6 | 99.9 | 100.0 | 97.4 | 100.0 | 98.1 | 98.9 |
    99.2 | 90.6 | 89.9 | 90.7 | 91.7 | 93.1 | 92.8 | 90.6 | 85.4 | 89.6 | 90.5 |'
- en: '| W4 | 99.4 | 100.0 | 99.4 | 99.7 | 99.8 | 99.6 | 99.0 | 98.9 | 98.4 | 99.3
    | 95.8 | 94.3 | 95.9 | 93.8 | 93.6 | 92.6 | 88.9 | 90.5 | 89.7 | 92.8 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| W4 | 99.4 | 100.0 | 99.4 | 99.7 | 99.8 | 99.6 | 99.0 | 98.9 | 98.4 | 99.3
    | 95.8 | 94.3 | 95.9 | 93.8 | 93.6 | 92.6 | 88.9 | 90.5 | 89.7 | 92.8 |'
- en: '| 35B | FP16 | 99.2 | 97.0 | 98.1 | 99.2 | 99.6 | 99.6 | 99.0 | 99.0 | 97.7
    | 98.7 | 58.8 | 59.6 | 69.0 | 73.0 | 63.6 | 66.3 | 69.2 | 64.2 | 74.6 | 66.5 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 35B | FP16 | 99.2 | 97.0 | 98.1 | 99.2 | 99.6 | 99.6 | 99.0 | 99.0 | 97.7
    | 98.7 | 58.8 | 59.6 | 69.0 | 73.0 | 63.6 | 66.3 | 69.2 | 64.2 | 74.6 | 66.5 |'
- en: '| W8 | 99.7 | 97.0 | 98.1 | 98.9 | 100.0 | 99.8 | 99.0 | 99.3 | 97.4 | 98.8
    | 59.8 | 58.6 | 69.2 | 72.8 | 62.3 | 66.8 | 68.7 | 64.3 | 74.5 | 66.3 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| W8 | 99.7 | 97.0 | 98.1 | 98.9 | 100.0 | 99.8 | 99.0 | 99.3 | 97.4 | 98.8
    | 59.8 | 58.6 | 69.2 | 72.8 | 62.3 | 66.8 | 68.7 | 64.3 | 74.5 | 66.3 |'
- en: '| W8A8 | 99.9 | 98.0 | 97.1 | 98.9 | 100.0 | 100.0 | 100.0 | 99.0 | 98.4 |
    99.0 | 61.0 | 63.9 | 72.1 | 75.1 | 66.2 | 68.3 | 70.1 | 67.4 | 76.3 | 68.9 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 99.9 | 98.0 | 97.1 | 98.9 | 100.0 | 100.0 | 100.0 | 99.0 | 98.4 |
    99.0 | 61.0 | 63.9 | 72.1 | 75.1 | 66.2 | 68.3 | 70.1 | 67.4 | 76.3 | 68.9 |'
- en: '| W4-g | 99.4 | 95.0 | 96.5 | 99.9 | 100.0 | 99.8 | 97.0 | 98.3 | 98.6 | 98.3
    | 60.4 | 59.3 | 72.8 | 73.3 | 64.8 | 65.4 | 70.4 | 64.6 | 73.0 | 67.1 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| W4-g | 99.4 | 95.0 | 96.5 | 99.9 | 100.0 | 99.8 | 97.0 | 98.3 | 98.6 | 98.3
    | 60.4 | 59.3 | 72.8 | 73.3 | 64.8 | 65.4 | 70.4 | 64.6 | 73.0 | 67.1 |'
- en: 'Table A18: Language Confusion scores for Command Models. (Line-level pass rate
    (LPR))'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A18: 命令模型的语言混淆分数。 （行级通过率 (LPR)）'
- en: '|  |  | Monolingual |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 单语 |'
- en: '|  |  | ar | de | es | fr | it | ja | ko | pt | zh | Avg | Ltn/IE | $\neg$
    |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ar | de | es | fr | it | ja | ko | pt | zh | 平均 | Ltn/IE | $\neg$ |'
- en: '| 103B | W8 | -0.3% | 0.0% | 0.2% | -0.2% | -0.2% | 0.6% | -0.2% | -0.5% |
    0.6% | 0.0% | -0.1% | 0.2% |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| 103B | W8 | -0.3% | 0.0% | 0.2% | -0.2% | -0.2% | 0.6% | -0.2% | -0.5% |
    0.6% | 0.0% | -0.1% | 0.2% |'
- en: '|  | W8A8-sq | 0.1% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | -0.6% | 0.5%
    | 0.0% | -0.1% | 0.1% |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | 0.1% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | -0.6% | 0.5%
    | 0.0% | -0.1% | 0.1% |'
- en: '|  | W8A8 | 0.0% | 0.0% | 0.2% | 0.2% | 0.0% | 0.4% | -0.2% | -0.2% | 1.2%
    | 0.2% | 0.0% | 0.4% |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | 0.0% | 0.0% | 0.2% | 0.2% | 0.0% | 0.4% | -0.2% | -0.2% | 1.2%
    | 0.2% | 0.0% | 0.4% |'
- en: '|  | W4-g | -0.2% | 0.0% | 0.3% | 0.4% | 0.0% | -1.2% | 0.0% | -0.2% | 1.0%
    | 0.0% | 0.1% | -0.1% |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '|  | W4-g | -0.2% | 0.0% | 0.3% | 0.4% | 0.0% | -1.2% | 0.0% | -0.2% | 1.0%
    | 0.0% | 0.1% | -0.1% |'
- en: '|  | W4 | 0.0% | 0.0% | 0.1% | 0.1% | -0.2% | 1.0% | -1.0% | 0.6% | 0.5% |
    0.1% | 0.1% | 0.1% |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | 0.0% | 0.0% | 0.1% | 0.1% | -0.2% | 1.0% | -1.0% | 0.6% | 0.5% |
    0.1% | 0.1% | 0.1% |'
- en: '| 35B | W8 | 0.5% | 0.0% | 0.0% | -0.3% | 0.4% | 0.2% | 0.0% | 0.3% | -0.3%
    | 0.1% | 0.1% | 0.1% |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| 35B | W8 | 0.5% | 0.0% | 0.0% | -0.3% | 0.4% | 0.2% | 0.0% | 0.3% | -0.3%
    | 0.1% | 0.1% | 0.1% |'
- en: '|  | W8A8 | 0.7% | 1.0% | -0.9% | -0.3% | 0.4% | 0.4% | 1.0% | 0.0% | 0.7%
    | 0.3% | 0.0% | 0.7% |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | 0.7% | 1.0% | -0.9% | -0.3% | 0.4% | 0.4% | 1.0% | 0.0% | 0.7%
    | 0.3% | 0.0% | 0.7% |'
- en: '|  | W4-g | 0.2% | -2.1% | -1.6% | 0.7% | 0.4% | 0.2% | -2.0% | -0.7% | 0.9%
    | -0.4% | -0.6% | -0.2% |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '|  | W4-g | 0.2% | -2.1% | -1.6% | 0.7% | 0.4% | 0.2% | -2.0% | -0.7% | 0.9%
    | -0.4% | -0.6% | -0.2% |'
- en: '|  |  | Cross-lingual |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 跨语言 |'
- en: '| 103B | W8 | -0.5% | 0.5% | 0.5% | -0.2% | -0.1% | -0.3% | 0.3% | -1.0% |
    -1.8% | -0.3% | 0.0% | -0.6% |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| 103B | W8 | -0.5% | 0.5% | 0.5% | -0.2% | -0.1% | -0.3% | 0.3% | -1.0% |
    -1.8% | -0.3% | 0.0% | -0.6% |'
- en: '|  | W8A8-sq | 0.3% | 1.0% | 0.2% | 0.9% | -0.9% | 0.2% | 1.1% | -0.8% | -1.4%
    | 0.1% | 0.1% | 0.0% |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | 0.3% | 1.0% | 0.2% | 0.9% | -0.9% | 0.2% | 1.1% | -0.8% | -1.4%
    | 0.1% | 0.1% | 0.0% |'
- en: '|  | W8A8 | -1.8% | -1.5% | -0.2% | -0.6% | -1.2% | -0.2% | -1.9% | -1.1% |
    -2.3% | -1.2% | -0.9% | -1.6% |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | -1.8% | -1.5% | -0.2% | -0.6% | -1.2% | -0.2% | -1.9% | -1.1% |
    -2.3% | -1.2% | -0.9% | -1.6% |'
- en: '|  | W4-g | -2.6% | -0.8% | -0.6% | 0.1% | 0.1% | -0.3% | -0.5% | -3.3% | -1.8%
    | -1.1% | -0.9% | -1.3% |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '|  | W4-g | -2.6% | -0.8% | -0.6% | 0.1% | 0.1% | -0.3% | -0.5% | -3.3% | -1.8%
    | -1.1% | -0.9% | -1.3% |'
- en: '|  | W4 | 3.0% | 4.0% | 5.1% | 2.3% | 0.6% | -0.5% | -2.4% | 2.5% | -1.8% |
    1.4% | 2.9% | -0.4% |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '|  | W4 | 3.0% | 4.0% | 5.1% | 2.3% | 0.6% | -0.5% | -2.4% | 2.5% | -1.8% |
    1.4% | 2.9% | -0.4% |'
- en: '| 35B | W8 | 1.7% | -1.6% | 0.2% | -0.3% | -2.0% | 0.8% | -0.8% | 0.3% | -0.2%
    | -0.2% | -0.7% | 0.4% |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| 35B | W8 | 1.7% | -1.6% | 0.2% | -0.3% | -2.0% | 0.8% | -0.8% | 0.3% | -0.2%
    | -0.2% | -0.7% | 0.4% |'
- en: '|  | W8A8 | 3.8% | 7.2% | 4.4% | 2.9% | 4.1% | 3.1% | 1.2% | 5.0% | 2.3% |
    3.8% | 4.7% | 2.6% |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | 3.8% | 7.2% | 4.4% | 2.9% | 4.1% | 3.1% | 1.2% | 5.0% | 2.3% |
    3.8% | 4.7% | 2.6% |'
- en: '|  | W4-g | 2.8% | -0.5% | 5.4% | 0.5% | 1.9% | -1.3% | 1.7% | 0.6% | -2.2%
    | 1.0% | 1.6% | 0.3% |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '|  | W4-g | 2.8% | -0.5% | 5.4% | 0.5% | 1.9% | -1.3% | 1.7% | 0.6% | -2.2%
    | 1.0% | 1.6% | 0.3% |'
- en: 'Table A19: Relative performance (%$\Delta$ are the rest: ar, ja, ko, zh.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A19：相对性能（%$\Delta$其余：ar, ja, ko, zh）。
- en: A.3 RM/LLM-as-a-Judge and Human Evaluation - Full Results
  id: totrans-495
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 RM/LLM作为裁判与人工评估 - 完整结果
- en: '|  |  | fr | es | ja | ko |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '|  |  | fr | es | ja | ko |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  |  | LLM | RM | LLM | RM | LLM | RM | LLM | RM |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLM | RM | LLM | RM | LLM | RM | LLM | RM |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | W8 | 50.5 | 49.7 | 44.9 | 53.7 | 47.3 | 52.7 | 53.7 | 47.1 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 50.5 | 49.7 | 44.9 | 53.7 | 47.3 | 52.7 | 53.7 | 47.1 |'
- en: '|  | W8A8-sq | 40.8 | 47.5 | 48.1 | 52.0 | 51.0 | 52.4 | 51.9 | 47.5 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | 40.8 | 47.5 | 48.1 | 52.0 | 51.0 | 52.4 | 51.9 | 47.5 |'
- en: '|  | W4-g | 44.8 | 41.5 | 41.7 | 51.0 | 42.4 | 50.0 | 47.1 | 42.2 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '|  | W4-g | 44.8 | 41.5 | 41.7 | 51.0 | 42.4 | 50.0 | 47.1 | 42.2 |'
- en: '| Internal | W4 | 34.9 | 39.8 | 33.5 | 41.5 | 39.2 | 40.0 | 40.7 | 36.2 |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| 内部 | W4 | 34.9 | 39.8 | 33.5 | 41.5 | 39.2 | 40.0 | 40.7 | 36.2 |'
- en: '|  | W8 | 49.3 | 51.0 | 53.7 | 48.0 | 47.0 | 47.3 | 51.3 | 51.0 |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 49.3 | 51.0 | 53.7 | 48.0 | 47.0 | 47.3 | 51.3 | 51.0 |'
- en: '|  | W8A8-sq | 42.3 | 45.7 | 54.3 | 46.0 | 49.3 | 50.7 | 46.0 | 47.7 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | 42.3 | 45.7 | 54.3 | 46.0 | 49.3 | 50.7 | 46.0 | 47.7 |'
- en: '|  | W8A8 | 48.3 | 51.3 | 56.7 | 48.3 | 51.3 | 49.3 | 52.7 | 48.3 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8 | 48.3 | 51.3 | 56.7 | 48.3 | 51.3 | 49.3 | 52.7 | 48.3 |'
- en: '| Dolly | W4-g | 46.3 | 48.7 | 48.0 | 52.3 | 42.3 | 42.3 | 44.3 | 47.3 |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| Dolly | W4-g | 46.3 | 48.7 | 48.0 | 52.3 | 42.3 | 42.3 | 44.3 | 47.3 |'
- en: 'Table A20: LLM/RM-as-a-Judge Raw win-rates of 103B quantized models vs. FP16
    over Internal and Aya Dolly subsampled test sets.'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A20：LLM/RM作为裁判 103B量化模型与FP16在内部和Aya Dolly子采样测试集上的原始胜率。
- en: '|  |  | fr | es | ja | ko |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '|  |  | fr | es | ja | ko |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  | W8 | 46.3 | 50.3 | 53.7 | 44.0 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 46.3 | 50.3 | 53.7 | 44.0 |'
- en: '|  | W8A8-sq | 45.3 | 46.3 | 49.0 | 52.0 |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | 45.3 | 46.3 | 49.0 | 52.0 |'
- en: '| Internal | W4-g | 41.7 | 47.7 | 42.0 | 47.7 |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| 内部 | W4-g | 41.7 | 47.7 | 42.0 | 47.7 |'
- en: '|  | W8 | 50.3 | 47.3 | 56.0 | 50.0 |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '|  | W8 | 50.3 | 47.3 | 56.0 | 50.0 |'
- en: '|  | W8A8-sq | 46.3 | 45.7 | 50.0 | 48.3 |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|  | W8A8-sq | 46.3 | 45.7 | 50.0 | 48.3 |'
- en: '| Dolly | W4-g | 45.3 | 49.3 | 51.3 | 46.0 |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| Dolly | W4-g | 45.3 | 49.3 | 51.3 | 46.0 |'
- en: 'Table A21: Human evaluation raw win-rates of 103B quantized models vs. FP16
    over *Internal* and *Aya Dolly* subsampled test sets.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A21：人工评估 103B量化模型与FP16在*内部*和*Aya Dolly*子采样测试集上的原始胜率。
