- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:58:49'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:58:49
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Efficiently Distilling LLMs for Edge Applications
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效提炼LLMs以适用于边缘应用
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.01353](https://ar5iv.labs.arxiv.org/html/2404.01353)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.01353](https://ar5iv.labs.arxiv.org/html/2404.01353)
- en: \NewDocumentCommand\LeftComment
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \NewDocumentCommand\LeftComment
- en: 's m\IfBooleanF#1$\triangleright$ #2'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 's m\IfBooleanF#1$\triangleright$ #2'
- en: Achintya Kundu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Achintya Kundu
- en: IBM Research
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: IBM 研究
- en: achintya.k@ibm.com
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: achintya.k@ibm.com
- en: '&Fabian Lim'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '&Fabian Lim'
- en: IBM Research
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: IBM 研究
- en: flim@sg.ibm.com
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: flim@sg.ibm.com
- en: '&Aaron Chew'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '&Aaron Chew'
- en: IBM Research
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: IBM 研究
- en: aaron.chew1@ibm.com
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: aaron.chew1@ibm.com
- en: Laura Wynter
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Laura Wynter
- en: IBM Research
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: IBM 研究
- en: lwynter@sg.ibm.com
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: lwynter@sg.ibm.com
- en: '&Penny Chong'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '&Penny Chong'
- en: IBM Research
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: IBM 研究
- en: penny.chong@ibm.com
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: penny.chong@ibm.com
- en: '&Rhui Dih Lee'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '&Rhui Dih Lee'
- en: IBM Research
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: IBM 研究
- en: rhui.dih.lee@ibm.com
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: rhui.dih.lee@ibm.com
- en: Abstract
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Supernet training of LLMs is of great interest in industrial applications as
    it confers the ability to produce a palette of smaller models at constant cost,
    regardless of the number of models (of different size / latency) produced. We
    propose a new method called Multistage Low-rank Fine-tuning of Super-transformers
    (MLFS) for parameter-efficient supernet training. We show that it is possible
    to obtain high-quality encoder models that are suitable for commercial edge applications,
    and that while decoder-only models are resistant to a comparable degree of compression,
    decoders can be effectively sliced for a significant reduction in training time.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 超网络训练LLMs在工业应用中非常受关注，因为它使得在固定成本下能够生成一系列较小的模型，无论生成多少个（不同大小/延迟）模型。我们提出了一种新的方法，称为多阶段低秩超变换器微调（MLFS），用于参数高效的超网络训练。我们展示了能够获得适合商业边缘应用的高质量编码器模型，并且虽然仅解码器模型对压缩具有较强的抵抗力，但解码器可以有效地切片，从而显著减少训练时间。
- en: 1 Introduction
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: 'Given their sizes up to billions of parameters, [[27](#bib.bib27), [4](#bib.bib4)],
    it is challenging for enterprises to fine-tune Large Language Models (LLMs), and
    furthermore they are not suitable for deployment on edge devices with limited
    memory and computational power. We wish to enable LLMs on edge environments for
    enterprise use cases. This requires the following two capabilities. (1) Accommodating
    a variety of edge device hardware: A single fine-tuned model is not optimal across
    the spectrum of devices. For industrial applications, a palette of fine-tuned
    LLMs is required for different hardware. (2) Dynamically changing resource levels:
    At run-time, the available resources on edge devices evolve over time, and appropriate
    model should be dynamically selected based on the available resources of each
    device.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其参数规模可达到数十亿，[[27](#bib.bib27), [4](#bib.bib4)]，企业对大型语言模型（LLMs）的微调面临挑战，并且这些模型不适合在内存和计算能力有限的边缘设备上部署。我们希望在边缘环境中为企业用例提供LLMs。这需要以下两个能力：（1）适应各种边缘设备硬件：单一的微调模型在不同设备中并不最佳。对于工业应用，需要针对不同硬件的微调LLMs系列。（2）动态调整资源水平：在运行时，边缘设备上的可用资源会随时间变化，应该根据每个设备的可用资源动态选择合适的模型。
- en: 'A considerable amount of research has focused on compressing LLMs [[45](#bib.bib45),
    [31](#bib.bib31), [25](#bib.bib25), [26](#bib.bib26), [18](#bib.bib18), [15](#bib.bib15)].
    Methods that train a single small model guided by a large teacher model such as
    DistilBERT [[31](#bib.bib31)] and BERT-PKD [[33](#bib.bib33)], either achieve
    limited compression or do not scale to a large number of deployment devices. Supernet
    training methods [[14](#bib.bib14), [39](#bib.bib39), [6](#bib.bib6), [20](#bib.bib20),
    [22](#bib.bib22), [17](#bib.bib17)] were introduced to address these limitations:
    multiple smaller subnets within the supernet are trained simultaneously with weight-sharing.
    This one-time training approach produces a palette of smaller models, helping
    mitigate the computational cost of fine-tuning a model for each deployment scenario.
    However, the full-parameter supernet training approach is impractical when fine-tuning
    of an LLM is required for multiple deployment scenarios, limiting its utility
    for enterprises.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 相当多的研究集中在压缩 LLMs 上 [[45](#bib.bib45), [31](#bib.bib31), [25](#bib.bib25), [26](#bib.bib26),
    [18](#bib.bib18), [15](#bib.bib15)]。例如 DistilBERT [[31](#bib.bib31)] 和 BERT-PKD
    [[33](#bib.bib33)] 这样的单一小模型训练方法，虽然通过大教师模型指导，但要么实现了有限的压缩，要么无法扩展到大量的部署设备。超网训练方法
    [[14](#bib.bib14), [39](#bib.bib39), [6](#bib.bib6), [20](#bib.bib20), [22](#bib.bib22),
    [17](#bib.bib17)] 被引入以解决这些限制：超网中的多个较小子网通过权重共享同时训练。这种一次性训练的方法产生了一系列较小的模型，有助于减轻为每个部署场景微调模型的计算成本。然而，当需要为多个部署场景微调
    LLM 时，完整参数超网训练方法是不切实际的，这限制了其在企业中的实用性。
- en: 'Parameter-efficient fine-tuning (PEFT) methods such as Low-Rank Adaptation
    (LoRA) reduces the number of trainable parameters by allowing only rank-decomposition
    matrices to be trained while freezing the pre-trained weights of the model. PEFT
    methods, however, are not applicable to supernet training due to the implications
    on the weight-shared subnetworks. Our work bridges this gap to enable efficient
    fine-tuning of LLMs for edge devices. Our contributions are:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效的微调（PEFT）方法，如低秩适配（LoRA），通过仅训练秩分解矩阵而冻结模型的预训练权重，从而减少了可训练参数的数量。然而，由于对权重共享子网络的影响，PEFT
    方法不适用于超网训练。我们的工作弥补了这一空白，以实现 LLM 在边缘设备上的高效微调。我们的贡献是：
- en: '1.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We propose a parameter-efficient, distillation-based approach for supernet training
    of LLMs.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种基于知识蒸馏的参数高效超网训练方法。
- en: '2.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We devise a gradient scaling scheme to improve convergence speed of any form
    of supernet training.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了一种梯度缩放方案，以提高任何形式超网训练的收敛速度。
- en: '3.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We demonstrate significant compression of encoder models for edge. We highlight
    the limits of comparable compression for decoder models, while demonstrating a
    huge reduction in the steps needed for convergence.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了对边缘设备的编码器模型进行显著压缩的技术。我们突出了与解码器模型相比的压缩极限，同时展示了所需的收敛步骤的大幅减少。
- en: 2 Related Work
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Classical compression methods have been used for LLMs including pruning [[24](#bib.bib24),
    [34](#bib.bib34)], low rank approximation [[23](#bib.bib23), [21](#bib.bib21)],
    and quantization [[32](#bib.bib32), [41](#bib.bib41), [3](#bib.bib3)]. Knowledge
    distillation (KD) is adopted in BERT-PKD [[33](#bib.bib33)], tinyBERT [[18](#bib.bib18)],
    and distilBERT [[31](#bib.bib31)] and [[12](#bib.bib12)] in MiniLLM to distill
    knowledge from the layers of a large transformer model to a smaller one. See also
    the survey [[45](#bib.bib45)]. All these existing methods produce a single compressed
    model, unsuitable for edge scenarios with multiple deployment devices having varying
    computational capability.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的压缩方法已被用于大语言模型（LLMs），包括剪枝 [[24](#bib.bib24), [34](#bib.bib34)]、低秩近似 [[23](#bib.bib23),
    [21](#bib.bib21)] 和量化 [[32](#bib.bib32), [41](#bib.bib41), [3](#bib.bib3)]。知识蒸馏（KD）被应用于
    BERT-PKD [[33](#bib.bib33)]、tinyBERT [[18](#bib.bib18)] 和 distilBERT [[31](#bib.bib31)]
    以及 MiniLLM 中的 [[12](#bib.bib12)]，以将大型变换器模型的层中的知识蒸馏到较小的模型中。另见综述 [[45](#bib.bib45)]。所有这些现有方法产生一个单一的压缩模型，不适合具有不同计算能力的多个部署设备的边缘场景。
- en: 'Neural architecture search (NAS) based on reinforcement learning [[46](#bib.bib46)]
    and evolutionary algorithms [[28](#bib.bib28), [44](#bib.bib44)] trains every
    possible architecture and is very slow. Weight-sharing NAS was thus developed:
    in Guo et al. [[13](#bib.bib13)], Cai et al. [[5](#bib.bib5)], the building blocks
    in the same layer are isolated as all architectures are single paths. Weight-sharing
    NAS does not scale well to large architecture search spaces, hence, weight-entangled
    NAS, where subnets with common parts share weights, was introduced.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 基于强化学习 [[46](#bib.bib46)] 和进化算法 [[28](#bib.bib28), [44](#bib.bib44)] 的神经架构搜索（NAS）训练每一个可能的架构，速度非常慢。因此，开发了权重共享
    NAS：在 Guo 等人 [[13](#bib.bib13)]、Cai 等人 [[5](#bib.bib5)] 的方法中，同一层中的构建块被隔离，因为所有架构都是单路径的。权重共享
    NAS 在大型架构搜索空间中扩展性差，因此，引入了权重纠缠 NAS，其中具有共同部分的子网络共享权重。
- en: 'For resource-constrained edge deployment, supernet training [[6](#bib.bib6),
    [20](#bib.bib20), [8](#bib.bib8), [39](#bib.bib39), [11](#bib.bib11), [10](#bib.bib10)]
    was developed as a mode of jointly training multiple subnetworks (subnets) with
    entangled weights: one trains the supernet only once for all deployment scenarios.
    Cai et al. [[6](#bib.bib6)] introduced an elastic convolutional neural network
    with "progressive shrinkage", where larger subnets are trained first. Recent works
    have improved sampling strategies, e.g. the sandwich rule with in-place distillation
    [[40](#bib.bib40)], attentive sampling [[36](#bib.bib36)], stochastic nature gradient
    [[43](#bib.bib43)], or post-training sampling [[22](#bib.bib22)]. Our work is
    related to supernet training for transformer models [[14](#bib.bib14), [43](#bib.bib43),
    [38](#bib.bib38), [37](#bib.bib37), [8](#bib.bib8)]. This gradient scaling technique
    can be used with any of the above supernet methods.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于资源受限的边缘部署，超级网络训练 [[6](#bib.bib6), [20](#bib.bib20), [8](#bib.bib8), [39](#bib.bib39),
    [11](#bib.bib11), [10](#bib.bib10)] 被开发为一种联合训练多个子网络（subnets）并具有纠缠权重的模式：即一次训练超级网络即可适用于所有部署场景。Cai
    等人 [[6](#bib.bib6)] 提出了一个具有“渐进收缩”的弹性卷积神经网络，其中较大的子网络先进行训练。最近的研究改进了采样策略，例如，带有原地蒸馏的三明治规则
    [[40](#bib.bib40)]、注意力采样 [[36](#bib.bib36)]、随机自然梯度 [[43](#bib.bib43)]，或后训练采样 [[22](#bib.bib22)]。我们的工作与用于变换器模型的超级网络训练相关
    [[14](#bib.bib14), [43](#bib.bib43), [38](#bib.bib38), [37](#bib.bib37), [8](#bib.bib8)]。这种梯度缩放技术可以与上述任何超级网络方法一起使用。
- en: Parameter-efficient fine-tuning (PEFT) has been of great benefit in fine tuning
    LLMs. BitFit [[2](#bib.bib2)] updates the bias terms in pre-trained models while
    freezing the remaining parameters. LoRA [[16](#bib.bib16)] decomposes attention
    weight gradients into low-rank matrices to reduce the number of trainable parameters.
    AdaLoRA [[42](#bib.bib42)] and QLoRA [[9](#bib.bib9)] further improve LoRA [[16](#bib.bib16)].
    Note that PEFT allows fine-tuning a base model on a single GPU but does not produce
    smaller models. None of the PEFT methods can be used for weight-entangled supernet
    training.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）在微调大型语言模型（LLMs）方面带来了极大的好处。BitFit [[2](#bib.bib2)] 在冻结其余参数的同时更新预训练模型中的偏置项。LoRA
    [[16](#bib.bib16)] 将注意力权重梯度分解为低秩矩阵，以减少可训练参数的数量。AdaLoRA [[42](#bib.bib42)] 和 QLoRA
    [[9](#bib.bib9)] 进一步改进了 LoRA [[16](#bib.bib16)]。注意，PEFT 允许在单个 GPU 上对基础模型进行微调，但不会生成更小的模型。没有任何
    PEFT 方法可以用于权重纠缠的超级网络训练。
- en: 3 Solution Design
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 解决方案设计
- en: For use in enterprise settings, the solution must allow fine-tuning of models
    on a small GPU footprint. In addition, inference cost in terms of storage must
    be minimised. We therefore design a solution which does not store the full size
    model checkpoint for every downstream task but only the frozen weights of the
    pre-trained base model and the low rank matrices. For inference in commercial
    edge use cases, we wish to enable storing the desired models locally for a wide
    variety of edge device resource requirements. We thus develop an approach where
    storage is minimised, storing only one base model and as many low rank adapter
    matrices as there are target model size variations, where low-rank adapters are
    very small. If the model is stored locally on an edge device, our proposed slicing
    operation takes place where the supernet fine-tuning is performed and the desired
    model is downloaded for inference. The slicing operation takes place for each
    model size-task combination and each resulting subnet can be cached for inference.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于企业环境中的使用，该解决方案必须允许在小型 GPU 足迹上对模型进行微调。此外，必须最小化存储方面的推理成本。因此，我们设计了一种解决方案，该解决方案不为每个下游任务存储全尺寸模型检查点，而仅存储预训练基础模型的冻结权重和低秩矩阵。对于商业边缘用例的推理，我们希望能够根据各种边缘设备资源要求在本地存储所需的模型。因此，我们开发了一种方法，将存储最小化，只存储一个基础模型和与目标模型大小变化相匹配的低秩适配器矩阵，低秩适配器非常小。如果模型存储在边缘设备上，我们提出的切片操作会在超级网络微调时进行，并下载所需的模型进行推理。切片操作针对每个模型大小-任务组合进行，每个结果子网络可以缓存以进行推理。
- en: 4 Problem Formulation
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 问题表述
- en: 'First, we provide notation. Given a transformer model with architectural configuration
    $\Phi$ are learnt by minimizing training loss:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们提供符号表示。给定一个具有架构配置 $\Phi$ 的变换器模型，通过最小化训练损失来学习：
- en: '|  | $\displaystyle\mathop{\mathrm{argmin}}_{\mathrm{W}}\Big{[}\,\mathcal{L}_{\Phi}(\mathrm{W})\,:=\,\mathbb{E}\!\big{[}\,\ell\!\left[\,\mathrm{f}_{\Phi}\!\left(x;\mathrm{W}\right),y\,\right]\,\big{]}\,\Big{]},$
    |  | (1) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathop{\mathrm{argmin}}_{\mathrm{W}}\Big{[}\,\mathcal{L}_{\Phi}(\mathrm{W})\,:=\,\mathbb{E}\!\big{[}\,\ell\!\left[\,\mathrm{f}_{\Phi}\!\left(x;\mathrm{W}\right),y\,\right]\,\big{]}\,\Big{]},$
    |  | (1) |'
- en: where $\mathbb{E}$) for classification or causal language modeling loss for
    generative models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbb{E}$ 用于分类或生成模型的因果语言建模损失。
- en: Next, we introduce the super-transformer and related terminologies. We define
    three types of networks - Teacher network, Super-transformer (supernet) and Sub-transformer
    (subnet). The teacher is a fixed network with the same configuration as the pre-trained
    transformer. A super-transformer is a dynamic model whose architectural dimensions
    (embedding dimension, number of heads, number of layers, etc.) are configurable
    at run time. The maxnet (resp. minnet) is the largest (resp. smallest) network
    in the super-transformer’s architecture space. Weight entanglement (weight-sharing)
    allows super-transformer weights to be used across sub-transformers, which are
    subsets of the super-transformer. Pre-trained transformer weights initialise the
    super-transformer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍超级变换器和相关术语。我们定义三种类型的网络 - 教师网络、超级变换器（超级网络）和子变换器（子网络）。教师是具有与预训练变换器相同配置的固定网络。超级变换器是一个动态模型，其架构维度（嵌入维度、头数、层数等）在运行时可以配置。maxnet（即
    minnet）是超级变换器架构空间中的最大（即最小）网络。权重纠缠（权重共享）允许超级变换器的权重在子变换器中使用，子变换器是超级变换器的子集。预训练变换器的权重初始化超级变换器。
- en: 'The dynamic nature of a super-transformer is explicitly specified via a set
    $\mathcal{A}$ into weights of a sub-transformer model:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 超级变换器的动态性质通过一组 $\mathcal{A}$ 明确指定到子变换器模型的权重：
- en: '|  | $\displaystyle\mathrm{W}_{\Phi}:=\boldsymbol{\Pi}_{\Phi}\!\left(\mathrm{W}_{\!\texttt{Sup}}\right),\,\forall\Phi\in\mathcal{A}.$
    |  | (2) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{W}_{\Phi}:=\boldsymbol{\Pi}_{\Phi}\!\left(\mathrm{W}_{\!\texttt{Sup}}\right),\,\forall\Phi\in\mathcal{A}.$
    |  | (2) |'
- en: 'The aim of a weight-sharing super-transformer is to simultaneously train all
    the transformer models $\{\mathrm{f}_{\Phi}(\cdot;\boldsymbol{\Pi}_{\Phi}(\mathrm{W})):\mathcal{X}\to\mathcal{Y}\,|\,\Phi\in\mathcal{A}\}$:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 权重共享超级变换器的目标是同时训练所有变换器模型 $\{\mathrm{f}_{\Phi}(\cdot;\boldsymbol{\Pi}_{\Phi}(\mathrm{W})):\mathcal{X}\to\mathcal{Y}\,|\,\Phi\in\mathcal{A}\}$：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: where $\mathbb{E}$ estimated as
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbb{E}$ 估计为
- en: '|  | $1$2 |  | (4) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: '|  | $1$2 |  | (5) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: where $\{\Phi_{1},\cdots,\Phi_{K}\}$ to approximate the expectation in ([1](#S4.E1
    "Equation 1 ‣ 4 Problem Formulation ‣ Efficiently Distilling LLMs for Edge Applications")).
    Fine-tuning LLM super-transformers is computationally challenging in enterprise
    use cases as it involves computing gradients of sub-transformers’ loss functions
    with respect to a huge number of parameters.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\{\Phi_{1},\cdots,\Phi_{K}\}$ 近似 ([1](#S4.E1 "Equation 1 ‣ 4 Problem Formulation
    ‣ Efficiently Distilling LLMs for Edge Applications")) 中的期望。微调 LLM 超变换器在企业应用中计算上具有挑战性，因为它涉及到计算子变换器损失函数相对于大量参数的梯度。
- en: 5 Multistage Low-rank Fine-tuning of Super-transformers
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 超变换器的多阶段低秩微调
- en: 'We therefore developed Multistage Low-rank Fine-tuning of Super-transformers
    (MLFS). Given a teacher model with configuration $\Phi_{\texttt{Tch}}$:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们开发了超变换器的多阶段低秩微调 (MLFS)。给定配置为 $\Phi_{\texttt{Tch}}$ 的教师模型：
- en: '|  | $\displaystyle\mathrm{W}_{\texttt{Tch}}:=\mathrm{W}^{\mathrm{pretrain}}_{\texttt{Tch}}+A_{0}*B_{0},$
    |  | (6) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{W}_{\texttt{Tch}}:=\mathrm{W}^{\mathrm{pretrain}}_{\texttt{Tch}}+A_{0}*B_{0},$
    |  | (6) |'
- en: 'where $A_{0},B_{0}$:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A_{0},B_{0}$：
- en: '|  | $1$2 |  | (9) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: Stage-$s$, at every iteration. We call this Multistage Low-rank Fine-tuning
    of Super-transformers (MLFS) and present it in the Appendix in Algorithm [1](#alg1
    "Algorithm 1 ‣ 5 Multistage Low-rank Fine-tuning of Super-transformers ‣ Efficiently
    Distilling LLMs for Edge Applications").
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段-$s$，在每次迭代中。我们称之为超变换器的多阶段低秩微调 (MLFS)，并在附录中展示算法 [1](#alg1 "Algorithm 1 ‣ 5
    Multistage Low-rank Fine-tuning of Super-transformers ‣ Efficiently Distilling
    LLMs for Edge Applications")。
- en: Proposition 1
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 1
- en: 'Let the individually fine-tuned weights of a subnet, $\Phi$:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 设子网络的单独微调权重为 $\Phi$：
- en: '|  | $\displaystyle\begin{array}[]{l}\Delta\mathrm{W}_{\Phi}=\boldsymbol{\Pi}_{\Phi}\left(\sum_{s=0}^{2}A_{s}*B_{s}\right),~{}\forall\Phi\in\mathcal{A},\end{array}$
    |  | (11) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\begin{array}[]{l}\Delta\mathrm{W}_{\Phi}=\boldsymbol{\Pi}_{\Phi}\left(\sum_{s=0}^{2}A_{s}*B_{s}\right),~{}\forall\Phi\in\mathcal{A},\end{array}$
    |  | (11) |'
- en: where $\{A_{s},B_{s}\}_{s=0,1,2}$.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\{A_{s},B_{s}\}_{s=0,1,2}$。
- en: To illustrate the computational savings, recall $\mathrm{W}^{\mathrm{pretrain}}_{\texttt{Tch}}\in\mathbb{R}^{d\times
    d}$ parameters at every iteration.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明计算节省，回顾每次迭代中的 $\mathrm{W}^{\mathrm{pretrain}}_{\texttt{Tch}}\in\mathbb{R}^{d\times
    d}$ 参数。
- en: Algorithm 1 Multistage Low-rank Fine-tuning of Super-transformers (MLFS)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 超变换器的多阶段低秩微调 (MLFS)
- en: 'Input: Transformer model (teacher) with configuration $\Phi_{\texttt{Tch}}$.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：配置为 $\Phi_{\texttt{Tch}}$ 的变换器模型（教师）。
- en: 'Loss functions: Target task loss $\ell_{\texttt{task}}$.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数：目标任务损失 $\ell_{\texttt{task}}$。
- en: 'Multistage Training:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 多阶段训练：
- en: '1:for stage $s=0,1,2$ of the super-transformer’s loss:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 对阶段 $s=0,1,2$ 的超变换器损失进行处理：'
- en: '|  | $1$2 |  | (12) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (12) |'
- en: 16:     end for17:end for
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 16:     end for17:end for
- en: 'Output: $\{A_{s}$.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：$\{A_{s}$。
- en: Gradient Scaling
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度缩放
- en: For faster convergence of the smaller sub-transformers within a super- transformer,
    we propose a novel weighted-combination of the gradients of the sampled sub-transformers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速较小子变换器在超变换器中的收敛，我们提出了一种新颖的加权梯度组合方法。
- en: Proposition 2
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 2
- en: Let $1^{st}$, in Algorithm 1 is given by
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法 1 中 $1^{st}$ 由以下公式给出
- en: '|  | $\displaystyle\begin{array}[]{l}\sum_{j=1}^{K}(n_{1}/n_{j})^{\gamma}\,\nabla_{\mathrm{W}}\mathcal{L}_{\Phi_{j}},\end{array}$
    |  | (14) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\begin{array}[]{l}\sum_{j=1}^{K}(n_{1}/n_{j})^{\gamma}\,\nabla_{\mathrm{W}}\mathcal{L}_{\Phi_{j}},\end{array}$
    |  | (14) |'
- en: where $\nabla_{\mathrm{W}}$ is a hyper-parameter.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\nabla_{\mathrm{W}}$ 是一个超参数。
- en: 'Proof:'
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 证明：
- en: 'Each sub-transformer gradient in ([14](#S5.E14 "Equation 14 ‣ Proposition 2
    ‣ Gradient Scaling ‣ 5 Multistage Low-rank Fine-tuning of Super-transformers ‣
    Efficiently Distilling LLMs for Edge Applications")), $\texttt{grad}^{j}$-th sub-transformer’s
    loss. Using first-order Taylor expansion, we get:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 每个子变换器的梯度在 ([14](#S5.E14 "Equation 14 ‣ Proposition 2 ‣ Gradient Scaling ‣ 5
    Multistage Low-rank Fine-tuning of Super-transformers ‣ Efficiently Distilling
    LLMs for Edge Applications")) 中为 $\texttt{grad}^{j}$-th 子变换器的损失。使用一阶泰勒展开，我们得到：
- en: '|  | $\displaystyle\begin{array}[]{l}\mathcal{L}_{\Phi_{j}}(\mathrm{W}+\boldsymbol{\delta})\approx\mathcal{L}_{\Phi_{j}}(\mathrm{W})+\langle\nabla_{\mathrm{W}}\mathcal{L}_{\Phi_{j}}(\mathrm{W}),\boldsymbol{\delta}\rangle,\end{array}$
    |  | (16) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\begin{array}[]{l}\mathcal{L}_{\Phi_{j}}(\mathrm{W}+\boldsymbol{\delta})\approx\mathcal{L}_{\Phi_{j}}(\mathrm{W})+\langle\nabla_{\mathrm{W}}\mathcal{L}_{\Phi_{j}}(\mathrm{W}),\boldsymbol{\delta}\rangle,\end{array}$
    |  | (16) |'
- en: 'where $\langle\cdot,\cdot\rangle$ can be approximated as:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\langle\cdot,\cdot\rangle$ 可以近似为：
- en: '|  | $1$2 |  | (18) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (18) |'
- en: where we approximate the $\|\cdot\|_{1}$
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们近似 $\|\cdot\|_{1}$
- en: 'Distillation Loss for Super-transformers:'
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超变换器的蒸馏损失：
- en: 'Knowledge distillation is straightforward in a fixed-network fine-tuning setting.
    However, it is less so when fine-tuning a supernet, and in particular, fine-tuning
    a supernet using the proposed multistage LoRA based approach. Specifically, the
    subnets receive two types of knowledge distillation (KD) from the teacher: (a)
    the usual KD loss that utilizes the output logits of the teacher and (b) distillation
    of features from transformer layers [[18](#bib.bib18)] of the teacher.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在固定网络微调设置下，知识蒸馏是直接的。然而，当微调超网，尤其是使用提出的多阶段LoRA方法微调超网时，这就不那么直接了。具体而言，子网络从教师那里获得两种类型的知识蒸馏（KD）：（a）利用教师输出logits的常规KD损失和（b）从教师的变换器层[[18](#bib.bib18)]中蒸馏特征。
- en: To define the distillation based losses precisely, let the forward-pass mapping
    of an input training sample $x^{i}$, acts as the teacher and knowledge distillation
    loss for all other sub-transformers w.r.t the teacher is defined as
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确地定义基于蒸馏的损失，令输入训练样本$x^{i}$的前向传递映射作为教师，所有其他子变换器相对于教师的知识蒸馏损失定义为
- en: '|  | $1$2 |  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'where $\texttt{KL}[\cdot,\cdot]$:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\texttt{KL}[\cdot,\cdot]$：
- en: '|  | $1$2 |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'where $g_{j}$:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$g_{j}$：
- en: '|  | $\displaystyle\mathbf{U}^{l}_{j}~{}:=~{}[\,\mathbf{U}^{g_{j}(l)}_{1}\,]_{\Phi_{j}}\in\mathbb{R}^{d_{\mathrm{low}}\times
    d_{j}},$ |  | (19) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{U}^{l}_{j}~{}:=~{}[\,\mathbf{U}^{g_{j}(l)}_{1}\,]_{\Phi_{j}}\in\mathbb{R}^{d_{\mathrm{low}}\times
    d_{j}},$ |  | (19) |'
- en: where the operation $[~{}]_{\Phi_{j}}$.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中操作$[~{}]_{\Phi_{j}}$。
- en: 6 Low-rank approach for Distilling an LLM onto a Pre-trained student
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 低秩方法将大型语言模型（LLM）蒸馏到预训练学生模型上
- en: 'In this section, we considerthe standard single-stage distillation approach
    for training a smaller (student) model on a target task with help from a larger
    fine-tuned (teacher) model. Here, we assume availability of the following: (i)
    target data set $\mathcal{D}_{train}$ is chosen between 4 and 16).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们考虑了标准的单阶段蒸馏方法，用于在目标任务上训练较小的（学生）模型，借助于较大且经过微调的（教师）模型。在这里，我们假设可用以下内容：（i）目标数据集$\mathcal{D}_{train}$的选择范围在4到16之间。
- en: Algorithm 2 Low-rank approach for Distilling an LLM onto a Pre-trained Student
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 低秩方法将大型语言模型（LLM）蒸馏到预训练学生模型上
- en: 'Input: Larger transformer model (teacher) with configuration $\Phi_{\texttt{Tch}}$.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：配置为$\Phi_{\texttt{Tch}}$的大型变换模型（教师）。
- en: 'Loss functions: Target task loss $\ell_{\texttt{task}}$.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数：目标任务损失$\ell_{\texttt{task}}$。
- en: 1:Initialize the low-rank matrices $\{A_{1},B_{1}\}$.9:end for
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 1:初始化低秩矩阵$\{A_{1},B_{1}\}$。9:结束
- en: 'Output: $\{A_{1},B_{1}\}$.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：$\{A_{1},B_{1}\}$。
- en: Given an input sample $x^{i}$ is given by
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入样本$x^{i}$如下：
- en: '|  | $1$2 |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $\sigma(\cdot)$ is called the KD temperature.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\sigma(\cdot)$被称为KD温度。
- en: Given input $x^{i}$-th layer. Now, we define the feature based distillation
    loss as
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入$x^{i}$-层。现在，我们定义基于特征的蒸馏损失为
- en: '|  | $1$2 |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where the summation $l$ of the projected space is chosen to be small, 128 in
    our experiments.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，所选投影空间的总和$l$被设置为较小值，即128。
- en: 7 Results on Encoder and Decoder LLMs
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 编码器和解码器LLMs的结果
- en: We report performance on encoder tasks using GLUE [[35](#bib.bib35)] with BERT[base]
    as the teacher model $\Phi_{\texttt{Tch}}$. On GLUE, we use the train set for
    fine-tuning and the dev set for accuracy evaluation. For santacoder, we evaluate
    performance using HumanEval [[7](#bib.bib7)] and report pass@1 scores. All experiments
    were conducted using PyTorch on a single Nvidia A100 (40GB) GPU.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在使用BERT[base]作为教师模型$\Phi_{\texttt{Tch}}$的GLUE [[35](#bib.bib35)]任务上报告性能。在GLUE上，我们使用训练集进行微调，使用开发集进行准确性评估。对于santacoder，我们使用HumanEval
    [[7](#bib.bib7)]评估性能，并报告pass@1分数。所有实验都使用PyTorch在单个Nvidia A100（40GB）GPU上进行。
- en: In MLFS, the Low rank matrices are added on the QKV vectors and the intermediate
    size of feed-forward network (FFN) layers. We set $\beta_{l}=0.1\,\forall l$ for
    all other data sets. Following [[16](#bib.bib16)], we use the fine-tuned MNLI
    checkpoint to initialize the model weights for experiments on small data sets
    such as RTE and MRPC.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLFS中，低秩矩阵被添加到QKV向量和前馈网络（FFN）层的中间大小上。我们对所有其他数据集设置$\beta_{l}=0.1\,\forall l$。按照[[16](#bib.bib16)]，我们使用微调后的MNLI检查点来初始化模型权重，以便在RTE和MRPC等小数据集上进行实验。
- en: 7.1 Performance of Encoder Models
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 编码器模型的性能
- en: 'We compare performance of encoder models obtained with the MLFS approach against
    a static, fixed model (BERT base) from [[43](#bib.bib43), [14](#bib.bib14)], two
    popular distilled variants of the fixed model: TinyBERT [[18](#bib.bib18)] and
    DistilBERT [[31](#bib.bib31)], and models trained using existing super-transformer
    methods (DynaBERT [[14](#bib.bib14)]. Figure [1](#S7.F1 "Figure 1 ‣ 7.1 Performance
    of Encoder Models ‣ 7 Results on Encoder and Decoder LLMs ‣ Efficiently Distilling
    LLMs for Edge Applications") shows the performance of the palette of models, from
    a 45M param. minnet to full-size 110M maxnet. Model performance against forward-pass
    latency is plotted in Figure [2](#S7.F2 "Figure 2 ‣ 7.1 Performance of Encoder
    Models ‣ 7 Results on Encoder and Decoder LLMs ‣ Efficiently Distilling LLMs for
    Edge Applications"). Encoder models produced by MLFS are at par or better than
    much costlier methods. Results of PD-BERT, BERT-PKD are from [[43](#bib.bib43)],
    static BERT from [[43](#bib.bib43)] for all except MRPC for which we use [[14](#bib.bib14)].
    Note that TinyBERT performs data augmentation leading to higher accuracy but much
    longer computation time. We do not perform data augmentation for fairness of the
    comparison to the other methods. The main observation is that MLFS provides accurate,
    smaller encoder models at 1/4 the size of the teacher and 1/3 its runtime latency
    on a single GPU.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用MLFS方法获得的编码器模型性能与来自[[43](#bib.bib43)、[14](#bib.bib14)]的静态固定模型（BERT base）、两种流行的固定模型提炼变体：TinyBERT
    [[18](#bib.bib18)]和DistilBERT [[31](#bib.bib31)]、以及使用现有超变换器方法训练的模型（DynaBERT [[14](#bib.bib14)]）进行比较。图[1](#S7.F1
    "图 1 ‣ 7.1 编码器模型性能 ‣ 7 编码器和解码器LLMs的结果 ‣ 高效提炼LLMs用于边缘应用")显示了从45M参数的minnet到完整大小110M的maxnet模型的性能。图[2](#S7.F2
    "图 2 ‣ 7.1 编码器模型性能 ‣ 7 编码器和解码器LLMs的结果 ‣ 高效提炼LLMs用于边缘应用")绘制了模型性能与前向传递延迟的关系。MLFS生成的编码器模型与成本更高的方法持平或更优。PD-BERT、BERT-PKD的结果来自[[43](#bib.bib43)]，静态BERT的结果来自[[43](#bib.bib43)]，除了MRPC，我们使用[[14](#bib.bib14)]。注意，TinyBERT执行数据增强导致更高的准确性但计算时间更长。为了公平比较，我们没有执行数据增强。主要观察结果是，MLFS提供了准确的、更小的编码器模型，大小为教师模型的1/4，单GPU上的运行时延迟为其1/3。
- en: '![Refer to caption](img/ed9264d870176d14ccffdf71042904cf.png)![Refer to caption](img/28f5bd796c074ab61fd6497e938d8796.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ed9264d870176d14ccffdf71042904cf.png)![参见说明](img/28f5bd796c074ab61fd6497e938d8796.png)'
- en: 'Figure 1: Model size vs performance trade-off for task-specific BERT models
    produced by MLFS against other methods on 6 GLUE data sets.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：MLFS生成的任务特定BERT模型在6个GLUE数据集上的模型大小与性能权衡，与其他方法进行比较。
- en: '![Refer to caption](img/c164e298b2274965c90fcff24c7b658e.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c164e298b2274965c90fcff24c7b658e.png)'
- en: 'Figure 2: Latency vs performance trade-off for task-specific BERT models produced
    by MLFS against other methods on 6 GLUE data sets.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：MLFS生成的任务特定BERT模型在6个GLUE数据集上的延迟与性能权衡，与其他方法进行比较。
- en: '![Refer to caption](img/18c6922851fd88d9e0e4d6739b153ed2.png)![Refer to caption](img/dbf0dcb978fdd6ca5c75cd33d02331f1.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/18c6922851fd88d9e0e4d6739b153ed2.png)![参见说明](img/dbf0dcb978fdd6ca5c75cd33d02331f1.png)'
- en: 'Figure 3: Ablation study on gradient scaling: MLFS minnet convergence is improved
    using gradient scaling.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：关于梯度缩放的消融研究：使用梯度缩放可以改善MLFS minnet的收敛性。
- en: '![Refer to caption](img/d6628b9324a56c94385575b9b2babdc8.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d6628b9324a56c94385575b9b2babdc8.png)'
- en: 'Figure 4: Ablation study on MLFS rank of $A,B$ is optimal for small and medium
    subnets.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：关于MLFS中$A,B$的消融研究，发现对于小型和中型子网来说，排名是最优的。
- en: Ablation Study on Gradient Scaling
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关于梯度缩放的消融研究
- en: In supernet training, the weights of maxnet and subnets are shared and trained
    simultaneously. The maxnet tends to converge and overfit earlier than smaller
    subnets. The different convergence rates renders selecting a single supernet checkpoint
    for all networks difficult. Gradient scaling solves this by speeding up convergence
    of the smaller subnets to match that of the larger subnets or the maxnet. Fig.
    [3](#S7.F3 "Figure 3 ‣ 7.1 Performance of Encoder Models ‣ 7 Results on Encoder
    and Decoder LLMs ‣ Efficiently Distilling LLMs for Edge Applications") shows that
    gradient scaling improves minnet convergence, indicated by lower minnet loss.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在超网训练中，maxnet和子网的权重被共享并同时训练。由于maxnet趋于比较小的子网更早收敛并过拟合，不同的收敛速率使得选择一个单一的超网检查点用于所有网络变得困难。梯度缩放通过加快较小子网的收敛速度，使其匹配较大子网或maxnet的收敛速度，解决了这个问题。图[3](#S7.F3
    "图 3 ‣ 7.1 编码器模型性能 ‣ 7 编码器和解码器LLMs的结果 ‣ 高效提炼LLMs用于边缘应用")显示了梯度缩放改善了minnet的收敛性，表现为较低的minnet损失。
- en: Ablation Study on Rank of $A,B$
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关于$A,B$的消融研究
- en: In Fig. [4](#S7.F4 "Figure 4 ‣ 7.1 Performance of Encoder Models ‣ 7 Results
    on Encoder and Decoder LLMs ‣ Efficiently Distilling LLMs for Edge Applications"),
    we examines the impact of rank $r$ for all other MLFS experiments. From the scale
    of the y-axis in [4](#S7.F4 "Figure 4 ‣ 7.1 Performance of Encoder Models ‣ 7
    Results on Encoder and Decoder LLMs ‣ Efficiently Distilling LLMs for Edge Applications"),
    observe that MLFS is not overly sensitive to the chosen rank.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [4](#S7.F4 "图 4 ‣ 7.1 编码器模型的性能 ‣ 7 编码器和解码器 LLMs 的结果 ‣ 高效提炼 LLMs 以适应边缘应用")
    中，我们检查了所有其他 MLFS 实验中等级 $r$ 的影响。从 [4](#S7.F4 "图 4 ‣ 7.1 编码器模型的性能 ‣ 7 编码器和解码器 LLMs
    的结果 ‣ 高效提炼 LLMs 以适应边缘应用") 中 y 轴的规模观察到，MLFS 对所选等级并不特别敏感。
- en: 7.2 Performance of Decoder Models
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 解码器模型的性能
- en: Turning now to decoder models, we consider two code-pre-trained LLMs, Santacoder
    [[1](#bib.bib1)] and Codellama7B [[29](#bib.bib29)]. We evaluate a custom 0.7B
    parameter Santacoder model obtained from the 1.1B teacher. Due to an inability
    to fine-tune on the full 24M coding examples, we use up to 1.2M. Fig. [5](#S7.F5
    "Figure 5 ‣ 7.2 Performance of Decoder Models ‣ 7 Results on Encoder and Decoder
    LLMs ‣ Efficiently Distilling LLMs for Edge Applications") shows that MLFS pass@1
    improves rapidly as number of tokens increases from a low 10k to 400k to 1.2M
    examples, only 5% of the 24M examples. Table [1](#S7.T1 "Table 1 ‣ 7.2 Performance
    of Decoder Models ‣ 7 Results on Encoder and Decoder LLMs ‣ Efficiently Distilling
    LLMs for Edge Applications") shows analogous results with 3 small MLFS models.
    The improvement in pass@1 indicates that the smaller models retain the ability
    to learn from the larger teacher.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在转向解码器模型，我们考虑两个经过代码预训练的 LLMs，Santacoder [[1](#bib.bib1)] 和 Codellama7B [[29](#bib.bib29)]。我们评估了从
    1.1B 教师模型获得的定制 0.7B 参数的 Santacoder 模型。由于无法在完整的 24M 编码示例上进行微调，我们使用了最多 1.2M 示例。图
    [5](#S7.F5 "图 5 ‣ 7.2 解码器模型的性能 ‣ 7 编码器和解码器 LLMs 的结果 ‣ 高效提炼 LLMs 以适应边缘应用") 显示，随着标记数量从低
    10k 增加到 400k 再到 1.2M 示例（仅占 24M 示例的 5%），MLFS 的 pass@1 性能迅速提高。表 [1](#S7.T1 "表 1
    ‣ 7.2 解码器模型的性能 ‣ 7 编码器和解码器 LLMs 的结果 ‣ 高效提炼 LLMs 以适应边缘应用") 显示了 3 个小型 MLFS 模型的类似结果。pass@1
    的提升表明，小型模型保留了从大型教师模型中学习的能力。
- en: '![Refer to caption](img/65650942696b998a69ff0b3db0536e35.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/65650942696b998a69ff0b3db0536e35.png)'
- en: 'Figure 5: Performance of MLFS on a custom Santacoder 0.7B model using 10K/400K/1.2M
    training examples.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 使用 10K/400K/1.2M 训练样本的定制 Santacoder 0.7B 模型的 MLFS 性能。'
- en: '| Data set size | Model size |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 数据集大小 | 模型大小 |'
- en: '| --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0.5B | 0.7B | 0.9B |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 0.5B | 0.7B | 0.9B |'
- en: '| --- | --- | --- |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 10K | 4.5 | 8.6 | 13.4 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 10K | 4.5 | 8.6 | 13.4 |'
- en: '| 400K | 4.7 | 9.5 | 13.5 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 400K | 4.7 | 9.5 | 13.5 |'
- en: 'Table 1: HumanEval pass@1 (%) performance of 3 small models produced by MLFS
    from Santacoder 1.1B.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 由 MLFS 从 Santacoder 1.1B 生成的 3 个小型模型在 HumanEval pass@1 (%) 性能表现。'
- en: Contrary to encoder models, the compression levels that retain sufficient performance
    of the teacher with decoders is less. While MLFS retains accuracy performance
    of encoder models at 1/4 the size of the teacher, the decoder models are reduced
    to at most 2/3 the teacher’s size.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 与编码器模型不同，保持足够性能的解码器的压缩水平较低。虽然 MLFS 在模型大小为教师模型的 1/4 时保持了编码器模型的准确性性能，但解码器模型最多只能缩减到教师模型的
    2/3。
- en: MLFS slicing of the teacher model can, however, benefit decoder models by reducing
    substantially the training/fine-tuning time needed compared to a randomly-initialised
    model, as shown in Fig. [6](#S7.F6 "Figure 6 ‣ 7.2 Performance of Decoder Models
    ‣ 7 Results on Encoder and Decoder LLMs ‣ Efficiently Distilling LLMs for Edge
    Applications") on Santacoder sliced from 1.1B to 0.7B. In other words, when a
    smaller model is required for edge inference, one can train it from a random initialisation,
    or slice from a teacher as does MLFS, and train starting from the sliced weights.
    The latter significantly reduces training time as seen in the validation loss
    curves. We see the same benefit on Codellama, as shown below. See [[30](#bib.bib30)]
    for a similar observation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，MLFS 对教师模型的切片可以通过显著减少训练/微调时间来使解码器模型受益，相比于随机初始化模型，如图 [6](#S7.F6 "图 6 ‣ 7.2
    解码器模型的性能 ‣ 7 编码器和解码器 LLMs 的结果 ‣ 高效提炼 LLMs 以适应边缘应用") 中所示，从 1.1B 切片到 0.7B 的 Santacoder
    模型。换句话说，当需要一个更小的模型用于边缘推理时，可以从随机初始化开始训练，或者像 MLFS 那样从教师模型中切片，然后从切片权重开始训练。后者显著减少了训练时间，如验证损失曲线所示。我们在
    Codellama 上也观察到同样的好处，见下文。有关类似观察的详细信息，请参见 [[30](#bib.bib30)]。
- en: '![Refer to caption](img/b544bbb64d4c715b1a68c830da927c04.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b544bbb64d4c715b1a68c830da927c04.png)'
- en: 'Figure 6: Convergence comparison of validation loss while fine-tuning a custom
    model from random vs using MLFS. MLFS achieves low validation loss much faster.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在对自定义模型进行微调时，从随机初始化 vs 使用 MLFS 的验证损失收敛比较。MLFS 实现了更快的低验证损失。
- en: Results on CodeLlama-7B-Python
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CodeLlama-7B-Python 上的结果
- en: Here, we experiment on fine-tuning custom smaller models of different sizes
    using the *CodeLlama-7B-Python* model as the teacher. We apply 1 epoch of MLFS
    on 2 different subsets of the *bigcode/the-stack* data set and produce 3 smaller
    variants having 4.5B, 5.3B & 6B model parameters in each case. Table [2](#S7.T2
    "Table 2 ‣ Results on CodeLlama-7B-Python ‣ 7.2 Performance of Decoder Models
    ‣ 7 Results on Encoder and Decoder LLMs ‣ Efficiently Distilling LLMs for Edge
    Applications") shows HumanEval performance of these models as the number of examples
    is increased from 200K to 400K. Again, we see that the sliced CodeLlama retain
    their ability to learn and improve quickly as the number of examples increases.
    Note that the full data set includes 24M examples; MLFS achieves nearly 75% of
    the performance of fullsize CodeLlama after less than 2% of the examples.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 *CodeLlama-7B-Python* 模型作为教师，对不同大小的自定义小模型进行微调实验。我们对 *bigcode/the-stack*
    数据集的 2 个不同子集应用了 1 轮 MLFS，并生成了 3 个具有 4.5B、5.3B 和 6B 模型参数的小变体。表 [2](#S7.T2 "表 2
    ‣ CodeLlama-7B-Python 上的结果 ‣ 7.2 解码器模型的性能 ‣ 7 编码器和解码器 LLM 的结果 ‣ 高效蒸馏 LLM 用于边缘应用")
    显示了这些模型在示例数量从 200K 增加到 400K 时的人类评估性能。我们再次看到切片的 CodeLlama 能够保留其学习能力，并随着示例数量的增加而迅速改进。请注意，完整数据集包含
    24M 示例；MLFS 在不到 2% 示例的情况下，达到了全尺寸 CodeLlama 性能的近 75%。
- en: '| Data set size | Model size |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 数据集大小 | 模型大小 |'
- en: '| --- | --- |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 4.5B | 5.3B | 6B |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 4.5B | 5.3B | 6B |'
- en: '| --- | --- | --- |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 200K | 11.0 | 19.5 | 23.2 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 200K | 11.0 | 19.5 | 23.2 |'
- en: '| 400K | 14.0 | 28.1 | 30.5 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 400K | 14.0 | 28.1 | 30.5 |'
- en: 'Table 2: HumanEval pass@1 (%) performance of 3 small models produced by MLFS
    from CodeLlama-7B-Python'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：由 MLFS 从 CodeLlama-7B-Python 生成的 3 个小模型的人类评估 pass@1 (%) 性能
- en: Ablation Study on Santacoder
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对 Santacoder 的消融研究
- en: In Fig. [7](#S7.F7 "Figure 7 ‣ Ablation Study on Santacoder ‣ 7.2 Performance
    of Decoder Models ‣ 7 Results on Encoder and Decoder LLMs ‣ Efficiently Distilling
    LLMs for Edge Applications"), we compare HumanEval performance of a 0.7B Santacoder
    model fine-tuned through full fine-tuning (FT) from random initialisation vs.
    full-rank (non-LoRA) MLFS with ($\alpha=0.9$) distillation. The improvement in
    the evaluation numbers is remarkable even after fine-tuning on up to only 5% of
    the examples.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [7](#S7.F7 "图 7 ‣ 对 Santacoder 的消融研究 ‣ 7.2 解码器模型的性能 ‣ 7 编码器和解码器 LLM 的结果 ‣
    高效蒸馏 LLM 用于边缘应用") 中，我们比较了一个 0.7B Santacoder 模型通过全量微调 (FT) 从随机初始化 vs. 全秩 (非 LoRA)
    MLFS 的 HumanEval 性能。即使仅在 5% 的示例上进行微调，评估数字的改善也非常显著。
- en: '![Refer to caption](img/de3c2ea698a0382f4c6b09cf18b9c47e.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/de3c2ea698a0382f4c6b09cf18b9c47e.png)'
- en: 'Figure 7: Superior performance of supernet training compared to other full
    fine-tuning based approaches on three data sets with 10K/400K/1.2M examples.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：与其他基于全量微调的方法相比，supernet 训练在三个数据集（10K/400K/1.2M 示例）上的优越性能。
- en: In Fig. [8](#S7.F8 "Figure 8 ‣ Ablation Study on Santacoder ‣ 7.2 Performance
    of Decoder Models ‣ 7 Results on Encoder and Decoder LLMs ‣ Efficiently Distilling
    LLMs for Edge Applications"), we see better convergence of validation loss on
    the Santacoder 0.7B for MLFS with distillation loss (). This demonstrates the benefit of MLFS
    distillation as compared to full MLFS fine tuning of the sliced model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [8](#S7.F8 "图 8 ‣ 对 Santacoder 的消融研究 ‣ 7.2 解码器模型的性能 ‣ 7 编码器和解码器 LLM 的结果 ‣
    高效蒸馏 LLM 用于边缘应用") 中，我们看到带有蒸馏损失的 Santacoder 0.7B 在 MLFS 中验证损失的收敛性更好 ()。这展示了与完整 MLFS 微调切片模型相比，MLFS 蒸馏的好处。
- en: '![Refer to caption](img/e5c8f0038b90c08c83eca294796feded.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e5c8f0038b90c08c83eca294796feded.png)'
- en: 'Figure 8: Convergence comparison of validation loss while fine-tuning a custom
    model using MLFS with/without distillation.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：在使用 MLFS 对自定义模型进行微调时，带有/不带有蒸馏的验证损失收敛比较。
- en: 8 Perspectives
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 个视角
- en: Enterprise users require an efficient way to fine-tune LLMs for inference on
    edge devices of many sizes. We developed MLFS for such edge deployment scenarios.
    We demonstrate its benefits on encoder LLMs. We show the limitation of compressing
    decoder LLMs to a comparable degree; however, MLFS offers significant gains for
    smaller decoder training/fine-tuning by slicing from a larger pre-trained teacher.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 企业用户需要一种高效的方法来对许多尺寸的边缘设备上的LLM进行微调。我们为这些边缘部署场景开发了MLFS。我们展示了其在编码器LLM上的优势。我们展示了将解码器LLM压缩到类似程度的限制；然而，MLFS通过从更大的预训练教师模型中切片，提供了在较小的解码器训练/微调方面的显著收益。
- en: References
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Allal et al. [2023] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao
    Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra,
    Alex Gu, Manan Dey, et al. Santacoder: don’t reach for the stars! *arXiv preprint
    arXiv:2301.03988*, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Allal 等 [2023] Loubna Ben Allal、Raymond Li、Denis Kocetkov、Chenghao Mou、Christopher
    Akiki、Carlos Munoz Ferrandis、Niklas Muennighoff、Mayank Mishra、Alex Gu、Manan Dey
    等. Santacoder: 不要伸手去摘星星！ *arXiv 预印本 arXiv:2301.03988*，2023年。'
- en: 'Ben Zaken et al. [2022] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.
    BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 2: Short Papers)*, pages 1–9, Dublin, Ireland, May 2022\.
    Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.1.
    URL [https://aclanthology.org/2022.acl-short.1](https://aclanthology.org/2022.acl-short.1).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ben Zaken 等 [2022] Elad Ben Zaken、Yoav Goldberg 和 Shauli Ravfogel. BitFit:
    简单的参数高效微调方法用于基于变换器的掩蔽语言模型. 见 *第60届计算语言学协会年会论文集（第2卷：短篇论文）*，第1–9页，爱尔兰都柏林，2022年5月\.
    计算语言学协会。doi: 10.18653/v1/2022.acl-short.1. URL [https://aclanthology.org/2022.acl-short.1](https://aclanthology.org/2022.acl-short.1)。'
- en: Bhandare et al. [2019] Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada,
    Vivek Menon, Sun Choi, Kushal Datta, and Vikram Saletore. Efficient 8-bit quantization
    of transformer neural machine language translation model. *arXiv preprint arXiv:1906.00532*,
    2019.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhandare 等 [2019] Aishwarya Bhandare、Vamsi Sripathi、Deepthi Karkada、Vivek Menon、Sun
    Choi、Kushal Datta 和 Vikram Saletore. 变换器神经机器语言翻译模型的高效8位量化. *arXiv 预印本 arXiv:1906.00532*，2019年。
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 [2020] Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell 等. 语言模型是少样本学习者.
    *神经信息处理系统进展*，33:1877–1901，2020年。
- en: 'Cai et al. [2018] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural
    architecture search on target task and hardware. *arXiv preprint arXiv:1812.00332*,
    2018.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai 等 [2018] Han Cai、Ligeng Zhu 和 Song Han. Proxylessnas: 直接在目标任务和硬件上进行神经架构搜索.
    *arXiv 预印本 arXiv:1812.00332*，2018年。'
- en: 'Cai et al. [2019] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song
    Han. Once-for-all: Train one network and specialize it for efficient deployment.
    *arXiv preprint arXiv:1908.09791*, 2019.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai 等 [2019] Han Cai、Chuang Gan、Tianzhe Wang、Zhekai Zhang 和 Song Han. 一次性完成:
    训练一个网络并使其适应高效部署. *arXiv 预印本 arXiv:1908.09791*，2019年。'
- en: Chen et al. [2021a] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. Evaluating large language models trained on code. *arXiv preprint
    arXiv:2107.03374*, 2021a.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2021a] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, 和 Wojciech
    Zaremba. 评估基于代码训练的大型语言模型。*arXiv 预印本 arXiv:2107.03374*，2021年。
- en: 'Chen et al. [2021b] Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling.
    Autoformer: Searching transformers for visual recognition. In *Proceedings of
    the IEEE/CVF international conference on computer vision*, pages 12270–12280,
    2021b.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2021b] Minghao Chen, Houwen Peng, Jianlong Fu, 和 Haibin Ling. Autoformer：为视觉识别搜索变换器。收录于*IEEE/CVF
    国际计算机视觉会议论文集*，第12270–12280页，2021年。
- en: 'Dettmers et al. [2023] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. [2023] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke
    Zettlemoyer. Qlora：高效的量化 LLM 微调。*arXiv 预印本 arXiv:2305.14314*，2023年。
- en: Dong et al. [2022] Peijie Dong, Xin Niu, Lujun Li, Linzhen Xie, Wenbin Zou,
    Tian Ye, Zimian Wei, and Hengyue Pan. Prior-guided one-shot neural architecture
    search. *arXiv preprint arXiv:2206.13329*, 2022.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. [2022] Peijie Dong, Xin Niu, Lujun Li, Linzhen Xie, Wenbin Zou,
    Tian Ye, Zimian Wei, 和 Hengyue Pan. 基于先验的单次神经架构搜索。*arXiv 预印本 arXiv:2206.13329*，2022年。
- en: 'Gao et al. [2022] Jiahui Gao, Hang Xu, Han Shi, Xiaozhe Ren, LH Philip, Xiaodan
    Liang, Xin Jiang, and Zhenguo Li. Autobert-zero: Evolving bert backbone from scratch.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 36-10,
    pages 10663–10671, 2022.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. [2022] Jiahui Gao, Hang Xu, Han Shi, Xiaozhe Ren, LH Philip, Xiaodan
    Liang, Xin Jiang, 和 Zhenguo Li. Autobert-zero：从头开始演变 BERT 主干。收录于*AAAI 人工智能会议论文集*，第36卷第10期，第10663–10671页，2022年。
- en: Gu et al. [2023] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Knowledge distillation
    of large language models. *arXiv preprint arXiv:2306.08543*, 2023.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. [2023] Yuxian Gu, Li Dong, Furu Wei, 和 Minlie Huang. 大型语言模型的知识蒸馏。*arXiv
    预印本 arXiv:2306.08543*，2023年。
- en: 'Guo et al. [2020] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu,
    Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with
    uniform sampling. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
    UK, August 23–28, 2020, Proceedings, Part XVI 16*, pages 544–560\. Springer, 2020.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. [2020] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu,
    Yichen Wei, 和 Jian Sun. 单路径一次性神经架构搜索与均匀采样。收录于*计算机视觉–ECCV 2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第XVI部分*，第544–560页。Springer，2020年。
- en: 'Hou et al. [2020] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen,
    and Qun Liu. Dynabert: Dynamic bert with adaptive width and depth. *Advances in
    Neural Information Processing Systems*, 33:9782–9793, 2020.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou et al. [2020] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, 和
    Qun Liu. Dynabert：具有自适应宽度和深度的动态 BERT。*神经信息处理系统进展*，33：9782–9793，2020年。
- en: Hsieh et al. [2023] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    Distilling step-by-step! outperforming larger language models with less training
    data and smaller model sizes. *arXiv preprint arXiv:2305.02301*, 2023.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh et al. [2023] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, 和 Tomas Pfister.
    **逐步提炼**！用更少的训练数据和更小的模型尺寸超越大型语言模型。*arXiv 预印本 arXiv:2305.02301*，2023年。
- en: 'Hu et al. [2022] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of
    large language models. In *International Conference on Learning Representations*,
    2022. URL [https://openreview.net/forum?id=nZeVKeeFYf9](https://openreview.net/forum?id=nZeVKeeFYf9).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等 [2022] Edward J Hu、Yelong Shen、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi
    Li、Shean Wang、Lu Wang 和 Weizhu Chen。LoRA: 大型语言模型的低秩适应。发表于 *国际学习表征会议*，2022 年。网址
    [https://openreview.net/forum?id=nZeVKeeFYf9](https://openreview.net/forum?id=nZeVKeeFYf9)。'
- en: 'Jawahar et al. [2023] Ganesh Jawahar, Haichuan Yang, Yunyang Xiong, Zechun
    Liu, Dilin Wang, Fei Sun, Meng Li, Aasish Pappu, Barlas Oguz, Muhammad Abdul-Mageed,
    Laks V. S. Lakshmanan, Raghuraman Krishnamoorthi, and Vikas Chandra. Mixture-of-supernets:
    Improving weight-sharing supernet training with architecture-routed mixture-of-experts.
    *arXiv preprint arXiv:2306.04845*, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jawahar 等 [2023] Ganesh Jawahar、Haichuan Yang、Yunyang Xiong、Zechun Liu、Dilin
    Wang、Fei Sun、Meng Li、Aasish Pappu、Barlas Oguz、Muhammad Abdul-Mageed、Laks V. S.
    Lakshmanan、Raghuraman Krishnamoorthi 和 Vikas Chandra。Mixture-of-supernets: 通过架构路由的专家混合改进权重共享超网训练。*arXiv
    预印本 arXiv:2306.04845*，2023 年。'
- en: 'Jiao et al. [2020] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
    Linlin Li, Fang Wang, and Qun Liu. TinyBERT: Distilling BERT for natural language
    understanding. In *Findings of the Association for Computational Linguistics:
    EMNLP 2020*, pages 4163–4174, Online, November 2020\. Association for Computational
    Linguistics. doi: 10.18653/v1/2020.findings-emnlp.372. URL [https://aclanthology.org/2020.findings-emnlp.372](https://aclanthology.org/2020.findings-emnlp.372).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiao 等 [2020] Xiaoqi Jiao、Yichun Yin、Lifeng Shang、Xin Jiang、Xiao Chen、Linlin
    Li、Fang Wang 和 Qun Liu。TinyBERT: 精简 BERT 以用于自然语言理解。发表于 *计算语言学协会年会论文集：EMNLP 2020*，第
    4163–4174 页，在线，2020 年 11 月。计算语言学协会。doi: 10.18653/v1/2020.findings-emnlp.372。网址
    [https://aclanthology.org/2020.findings-emnlp.372](https://aclanthology.org/2020.findings-emnlp.372)。'
- en: 'Kocetkov et al. [2022] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,
    Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean
    Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The
    stack: 3 tb of permissively licensed source code. *arXiv preprint arXiv:2211.15533*,
    2022.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kocetkov 等 [2022] Denis Kocetkov、Raymond Li、Loubna Ben Allal、Jia Li、Chenghao
    Mou、Carlos Muñoz Ferrandis、Yacine Jernite、Margaret Mitchell、Sean Hughes、Thomas
    Wolf、Dzmitry Bahdanau、Leandro von Werra 和 Harm de Vries。The stack: 3 TB 的宽松授权源代码。*arXiv
    预印本 arXiv:2211.15533*，2022 年。'
- en: 'Kundu et al. [2023] Achintya Kundu, Laura Wynter, Rhui Dih Lee, and Luis Angel D.
    Bathen. Transfer-once-for-all: AI model optimization for edge. In *IEEE International
    Conference on Edge Computing and Communications, EDGE 2023, Chicago, IL, USA,
    July 2-8, 2023*, pages 26–35\. IEEE, 2023. URL [https://doi.org/10.1109/EDGE60047.2023.00017](https://doi.org/10.1109/EDGE60047.2023.00017).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kundu 等 [2023] Achintya Kundu、Laura Wynter、Rhui Dih Lee 和 Luis Angel D. Bathen。Transfer-once-for-all:
    边缘 AI 模型优化。发表于 *IEEE 边缘计算与通信国际会议，EDGE 2023，芝加哥，美国，2023 年 7 月 2-8 日*，第 26–35 页。IEEE，2023
    年。网址 [https://doi.org/10.1109/EDGE60047.2023.00017](https://doi.org/10.1109/EDGE60047.2023.00017)。'
- en: 'Lan et al. [2019] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning
    of language representations. *arXiv preprint arXiv:1909.11942*, 2019.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lan 等 [2019] Zhenzhong Lan、Mingda Chen、Sebastian Goodman、Kevin Gimpel、Piyush
    Sharma 和 Radu Soricut。Albert: 用于自监督语言表示学习的轻量级 BERT。*arXiv 预印本 arXiv:1909.11942*，2019
    年。'
- en: 'Lou et al. [2021] Wei Lou, Lei Xun, Amin Sabet, Jia Bi, Jonathon Hare, and
    Geoff V Merrett. Dynamic-ofa: Runtime dnn architecture switching for performance
    scaling on heterogeneous embedded platforms. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 3110–3118, 2021.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lou 等 [2021] Wei Lou、Lei Xun、Amin Sabet、Jia Bi、Jonathon Hare 和 Geoff V Merrett。Dynamic-ofa:
    在异构嵌入式平台上进行运行时 DNN 架构切换以实现性能扩展。发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第 3110–3118 页，2021
    年。'
- en: Ma et al. [2019] Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou,
    Ming Zhou, and Dawei Song. A tensorized transformer for language modeling. *Advances
    in neural information processing systems*, 32, 2019.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等 [2019] Xindian Ma、Peng Zhang、Shuai Zhang、Nan Duan、Yuexian Hou、Ming Zhou
    和 Dawei Song。用于语言建模的张量化 Transformer。*神经信息处理系统进展*，第 32 卷，2019 年。
- en: McCarley et al. [2019] JS McCarley, Rishav Chakravarti, and Avirup Sil. Structured
    pruning of a bert-based question answering model. *arXiv preprint arXiv:1910.06360*,
    2019.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCarley 等 [2019] JS McCarley、Rishav Chakravarti 和 Avirup Sil。基于 BERT 的问答模型的结构化剪枝。*arXiv
    预印本 arXiv:1910.06360*，2019 年。
- en: 'Mukherjee and Awadallah [2020] Subhabrata Mukherjee and Ahmed Awadallah. Xtremedistil:
    Multi-stage distillation for massive multilingual models. *arXiv preprint arXiv:2004.05686*,
    2020.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mukherjee 和 Awadallah [2020] Subhabrata Mukherjee 和 Ahmed Awadallah. Xtremedistil:
    大规模多语言模型的多阶段蒸馏。*arXiv 预印本 arXiv:2004.05686*, 2020。'
- en: 'Mukherjee et al. [2021] Subhabrata Mukherjee, Ahmed Hassan Awadallah, and Jianfeng
    Gao. Xtremedistiltransformers: Task transfer for task-agnostic distillation. *arXiv
    preprint arXiv:2106.04563*, 2021.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mukherjee 等 [2021] Subhabrata Mukherjee, Ahmed Hassan Awadallah 和 Jianfeng
    Gao. Xtremedistiltransformers: 任务无关蒸馏的任务转移。*arXiv 预印本 arXiv:2106.04563*, 2021。'
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等 [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li 和 Peter J Liu. 探索统一的文本到文本变换器在迁移学习中的极限。*机器学习研究期刊*，21(1):5485–5551，2020。
- en: Real et al. [2019] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le.
    Regularized evolution for image classifier architecture search. In *Proceedings
    of the aaai conference on artificial intelligence*, volume 33-01, pages 4780–4789,
    2019.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Real 等 [2019] Esteban Real, Alok Aggarwal, Yanping Huang 和 Quoc V Le. 用于图像分类器架构搜索的正则化进化。在
    *AAAI 人工智能会议论文集*，第 33 卷第 01 期，第 4780–4789 页，2019。
- en: 'Rozière et al. [2023] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton
    Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal
    Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel
    Synnaeve. Code llama: Open foundation models for code, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rozière 等 [2023] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla,
    Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin,
    Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton
    Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal
    Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom 和 Gabriel Synnaeve.
    代码大牛: 开放的代码基础模型，2023。'
- en: 'Samragh et al. [2023] Mohammad Samragh, Mehrdad Farajtabar, Sachin Mehta, Raviteja
    Vemulapalli, Fartash Faghri, Devang Naik, Oncel Tuzel, and Mohammad Rastegari.
    Weight subcloning: direct initialization of transformers using larger pretrained
    ones, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Samragh 等 [2023] Mohammad Samragh, Mehrdad Farajtabar, Sachin Mehta, Raviteja
    Vemulapalli, Fartash Faghri, Devang Naik, Oncel Tuzel 和 Mohammad Rastegari. 权重子克隆:
    使用更大预训练模型直接初始化变换器，2023。'
- en: 'Sanh et al. [2019] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.
    *arXiv preprint arXiv:1910.01108*, 2019.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sanh 等 [2019] Victor Sanh, Lysandre Debut, Julien Chaumond 和 Thomas Wolf. Distilbert,
    BERT 的蒸馏版本: 更小、更快、更便宜、更轻便。*arXiv 预印本 arXiv:1910.01108*, 2019。'
- en: 'Shen et al. [2020] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra
    low precision quantization of bert. In *Proceedings of the AAAI Conference on
    Artificial Intelligence*, volume 34-05, pages 8815–8821, 2020.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen 等 [2020] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir
    Gholami, Michael W Mahoney 和 Kurt Keutzer. Q-bert: 基于 Hessian 的极低精度 BERT 量化。在
    *AAAI 人工智能会议论文集*，第 34 卷第 05 期，第 8815–8821 页，2020。'
- en: Sun et al. [2019] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge
    distillation for bert model compression. *arXiv preprint arXiv:1908.09355*, 2019.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 [2019] Siqi Sun, Yu Cheng, Zhe Gan 和 Jingjing Liu. 面向 BERT 模型压缩的患者知识蒸馏。*arXiv
    预印本 arXiv:1908.09355*, 2019。
- en: 'Voita et al. [2019] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the
    heavy lifting, the rest can be pruned. *arXiv preprint arXiv:1905.09418*, 2019.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Voita 等 [2019] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich 和 Ivan
    Titov. 分析多头自注意力: 专门的头部完成了繁重的工作，其余的可以被剪枝。*arXiv 预印本 arXiv:1905.09418*, 2019。'
- en: 'Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform
    for natural language understanding. In *Proceedings of the 2018 EMNLP Workshop
    BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 353–355,
    Brussels, Belgium, November 2018\. Association for Computational Linguistics.
    doi: 10.18653/v1/W18-5446. URL [https://aclanthology.org/W18-5446](https://aclanthology.org/W18-5446).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer
    Levy, 和 Samuel Bowman. GLUE: 自然语言理解的多任务基准与分析平台。在*2018 EMNLP研讨会 BlackboxNLP: 分析与解释NLP中的神经网络*，第353–355页，比利时布鲁塞尔，2018年11月。计算语言学协会。doi:
    10.18653/v1/W18-5446。网址 [https://aclanthology.org/W18-5446](https://aclanthology.org/W18-5446)。'
- en: 'Wang et al. [2021] Dilin Wang, Meng Li, Chengyue Gong, and Vikas Chandra. Attentivenas:
    Improving neural architecture search via attentive sampling. In *Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition*, pages 6418–6427,
    2021.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 [2021] Dilin Wang, Meng Li, Chengyue Gong, 和 Vikas Chandra. Attentivenas:
    通过关注采样改进神经架构搜索。在*IEEE/CVF计算机视觉与模式识别会议论文集*，第6418–6427页，2021年。'
- en: 'Wang et al. [2020] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu,
    Chuang Gan, and Song Han. Hat: Hardware-aware transformers for efficient natural
    language processing. *arXiv preprint arXiv:2005.14187*, 2020.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 [2020] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang
    Gan, 和 Song Han. Hat: 面向高效自然语言处理的硬件感知变换器。*arXiv预印本 arXiv:2005.14187*，2020年。'
- en: 'Wang et al. [2022] Rui Wang, Qibing Bai, Junyi Ao, Long Zhou, Zhixiang Xiong,
    Zhihua Wei, Yu Zhang, Tom Ko, and Haizhou Li. Lighthubert: Lightweight and configurable
    speech representation learning with once-for-all hidden-unit bert. *arXiv preprint
    arXiv:2203.15610*, 2022.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 [2022] Rui Wang, Qibing Bai, Junyi Ao, Long Zhou, Zhixiang Xiong, Zhihua
    Wei, Yu Zhang, Tom Ko, 和 Haizhou Li. Lighthubert: 具有一次性隐藏单元bert的轻量级和可配置的语音表示学习。*arXiv预印本
    arXiv:2203.15610*，2022年。'
- en: 'Xu et al. [2021] Jin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin,
    and Tie-Yan Liu. Nas-bert: task-agnostic and adaptive-size bert compression with
    neural architecture search. In *Proceedings of the 27th ACM SIGKDD Conference
    on Knowledge Discovery & Data Mining*, pages 1933–1943, 2021.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等 [2021] Jin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, 和 Tie-Yan
    Liu. Nas-bert: 通过神经架构搜索实现任务无关和自适应大小的bert压缩。在*第27届ACM SIGKDD知识发现与数据挖掘大会论文集*，第1933–1943页，2021年。'
- en: 'Yu et al. [2020] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan
    Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, and Quoc Le.
    Bignas: Scaling up neural architecture search with big single-stage models. In
    *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part VII 16*, pages 702–717\. Springer, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等 [2020] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan
    Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, 和 Quoc Le.
    Bignas: 使用大型单阶段模型扩展神经架构搜索。在*计算机视觉–ECCV 2020: 第16届欧洲会议，英国格拉斯哥，2020年8月23日–28日，论文集，第七部分16*，第702–717页。Springer，2020年。'
- en: 'Zafrir et al. [2019] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
    Q8bert: Quantized 8bit bert. In *2019 Fifth Workshop on Energy Efficient Machine
    Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)*, pages 36–39\. IEEE,
    2019.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zafrir 等 [2019] Ofir Zafrir, Guy Boudoukh, Peter Izsak, 和 Moshe Wasserblat.
    Q8bert: 量化的8位bert。在*2019年第五届能源高效机器学习与认知计算研讨会–NeurIPS版 (EMC2-NIPS)*，第36–39页。IEEE，2019年。'
- en: Zhang et al. [2023] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng
    He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient
    fine-tuning. *arXiv preprint arXiv:2303.10512*, 2023.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 [2023] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He,
    Yu Cheng, Weizhu Chen, 和 Tuo Zhao. 参数高效微调的自适应预算分配。*arXiv预印本 arXiv:2303.10512*，2023年。
- en: 'Zhang et al. [2021] Shaokun Zhang, Xiawu Zheng, Chenyi Yang, Yuchao Li, Yan
    Wang, Fei Chao, Mengdi Wang, Shen Li, Jun Yang, and Rongrong Ji. You only compress
    once: Towards effective and elastic bert compression via exploit-explore stochastic
    nature gradient. *arXiv preprint arXiv:2106.02435*, 2021.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 [2021] Shaokun Zhang, Xiawu Zheng, Chenyi Yang, Yuchao Li, Yan Wang,
    Fei Chao, Mengdi Wang, Shen Li, Jun Yang, 和 Rongrong Ji. 你只需压缩一次: 通过利用-探索随机梯度实现有效且弹性的bert压缩。*arXiv预印本
    arXiv:2106.02435*，2021年。'
- en: 'Zhu et al. [2019] Hui Zhu, Zhulin An, Chuanguang Yang, Kaiqiang Xu, Erhu Zhao,
    and Yongjun Xu. Eena: efficient evolution of neural architecture. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision Workshops*, pages
    0–0, 2019.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等 [2019] Hui Zhu, Zhulin An, Chuanguang Yang, Kaiqiang Xu, Erhu Zhao, 和
    Yongjun Xu. Eena: 神经架构的高效进化。在*IEEE/CVF国际计算机视觉研讨会论文集*，第0–0页，2019年。'
- en: Zhu et al. [2023] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A
    survey on model compression for large language models. *arXiv preprint arXiv:2308.07633*,
    2023.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等 [2023] Xunyu Zhu, Jian Li, Yong Liu, Can Ma 和 Weiping Wang. 关于大型语言模型的模型压缩综述。*arXiv
    预印本 arXiv:2308.07633*，2023年。
- en: Zoph and Le [2016] Barret Zoph and Quoc V Le. Neural architecture search with
    reinforcement learning. *arXiv preprint arXiv:1611.01578*, 2016.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zoph 和 Le [2016] Barret Zoph 和 Quoc V Le. 使用强化学习进行神经网络架构搜索。*arXiv 预印本 arXiv:1611.01578*，2016年。
