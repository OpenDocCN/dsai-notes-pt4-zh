- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:52:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:52:51'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ReALLM: A general framework for LLM compression and fine-tuning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ReALLM: 一种通用的 LLM 压缩和微调框架'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13155](https://ar5iv.labs.arxiv.org/html/2405.13155)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13155](https://ar5iv.labs.arxiv.org/html/2405.13155)
- en: \newaliascnt
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \newaliascnt
- en: lemmatheorem \aliascntresetthelemma \newaliascntcorollarytheorem \aliascntresetthecorollary
    \newaliascntpropositiontheorem \aliascntresettheproposition \newaliascntdefinitiontheorem
    \aliascntresetthedefinition \newaliascntremarktheorem \aliascntresettheremark
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: lemmatheorem \aliascntresetthelemma \newaliascntcorollarytheorem \aliascntresetthecorollary
    \newaliascntpropositiontheorem \aliascntresettheproposition \newaliascntdefinitiontheorem
    \aliascntresetthedefinition \newaliascntremarktheorem \aliascntresettheremark
- en: Louis Leconte
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Louis Leconte
- en: Lisite, Isep, Sorbonne University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Lisite, Isep, 索邦大学
- en: Math. and Algo. Sciences Lab, Huawei Tech
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数学与算法科学实验室, 华为技术
- en: louis.leconte@ens-paris-saclay.fr
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: louis.leconte@ens-paris-saclay.fr
- en: '&Lisa Bedin^∗'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Lisa Bedin^∗'
- en: CMAP, Ecole Polytechnique, France
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CMAP, 巴黎高科大学, 法国
- en: lisa.bedin@polytechnique.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: lisa.bedin@polytechnique.edu
- en: Van Minh Nguyen
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Van Minh Nguyen
- en: Math. and Algo. Sciences Lab, Huawei Tech.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数学与算法科学实验室, 华为技术
- en: '&Eric Moulines'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '&Eric Moulines'
- en: CMAP, Ecole Polytechnique, France equal contribution
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CMAP, 巴黎高科大学, 法国 同等贡献
- en: Abstract
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We introduce ReALLM, a novel approach for compression and memory-efficient adaptation
    of pre-trained language models that encompasses most of the post-training quantization
    and fine-tuning methods for a budget of $<4$ bits, ReALLM achieves state-of-the
    art performance after fine-tuning on a small calibration dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 ReALLM，这是一种用于压缩和内存高效适配预训练语言模型的新方法，它涵盖了大多数后训练量化和微调方法，对于预算为 $<4$ 位的情况，ReALLM
    在对小型校准数据集进行微调后，达到了最先进的性能。
- en: 1 Introduction
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Large Language Models (LLMs) based on transformer architectures (Vaswani et al.,,
    [2017](#bib.bib48)) have attracted increasing interest, especially with the availability
    of high-quality, open-source LLMs such as LLaMA (Touvron et al.,, [2023](#bib.bib44)),
    Falcon (Almazrouei et al.,, [2023](#bib.bib1)) and Gemma (Team et al.,, [2024](#bib.bib43)).
    These open models offer the advantage that they can be used by end users for inference
    or local fine-tuning, provided their hardware has sufficient memory for the size
    of the models. However, “full fine-tuning” — a process that involves updating
    all previously trained parameters — is still prohibitively expensive for large
    models. For example, the standard 16-bits fine-tuning of the LLaMA-$65$ GB of
    GPU memory ([Dettmers et al., 2023a,](#bib.bib11) ). This high requirement is
    due to the need to store both the weights of the model and the states of the optimizer
    in GPU memory, a need that increases as the size of the LLMs increases.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变换器架构的**大型语言模型 (LLMs)**（Vaswani 等，[2017](#bib.bib48)）引起了越来越多的关注，特别是随着高质量开源
    LLMs 的出现，如 LLaMA（Touvron 等，[2023](#bib.bib44)），Falcon（Almazrouei 等，[2023](#bib.bib1)）和
    Gemma（Team 等，[2024](#bib.bib43)）。这些开放模型的优点在于，只要其硬件有足够的内存来容纳模型的大小，最终用户就可以用于推理或本地微调。然而，“全面微调”——一种更新所有先前训练参数的过程——对于大型模型来说仍然过于昂贵。例如，LLaMA-$65$
    GB 的标准 16 位微调需要 GPU 内存 ([Dettmers 等, 2023a,](#bib.bib11)）。这一高要求是由于需要在 GPU 内存中存储模型的权重和优化器的状态，而这种需求随着
    LLM 的规模增加而增加。
- en: A common method to mitigate memory constraints is to quantize the model weights,
    activations, and gradients — to a lower bit precision. Quantization-Aware Training
    (QAT) is often used in computer vision; see Courbariaux et al., ([2015](#bib.bib8));
    Liu et al., ([2020](#bib.bib33)); Gholami et al., ([2022](#bib.bib16)). However,
    training large language models (LLMs) from scratch is impractical due to high
    computational cost. Post-training quantization (PTQ) is an efficient compromise
    (Dettmers et al.,, [2022](#bib.bib10); Frantar et al.,, [2022](#bib.bib14)), which
    has recently attracted much attention ([Kim et al., 2023b,](#bib.bib24) ; [Dettmers
    et al., 2023b,](#bib.bib12) ; [Kim et al., 2023a,](#bib.bib23) ; Shao et al.,,
    [2023](#bib.bib39)). Although most research focuses on scalar quantization (SQ),
    a few studies investigate LLM compression using vector quantization (VQ) (Tseng
    et al.,, [2024](#bib.bib46); Egiazarian et al.,, [2024](#bib.bib13)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解内存限制的常见方法是将模型权重、激活和梯度量化为较低的位精度。量化感知训练（QAT）通常用于计算机视觉；参见Courbariaux et al.,（[2015](#bib.bib8)）；Liu
    et al.,（[2020](#bib.bib33)）；Gholami et al.,（[2022](#bib.bib16)）。然而，由于计算成本高，从头训练大型语言模型（LLMs）是不切实际的。后训练量化（PTQ）是一种高效的折中方案（Dettmers
    et al., [2022](#bib.bib10)；Frantar et al., [2022](#bib.bib14)），最近引起了广泛关注（[Kim
    et al., 2023b](#bib.bib24)；[Dettmers et al., 2023b](#bib.bib12)；[Kim et al., 2023a](#bib.bib23)；Shao
    et al., [2023](#bib.bib39)）。虽然大多数研究集中在标量量化（SQ）上，但也有一些研究探讨了使用向量量化（VQ）进行LLM压缩（Tseng
    et al., [2024](#bib.bib46)；Egiazarian et al., [2024](#bib.bib13)）。
- en: In [Dettmers et al., 2023a](#bib.bib11) , quantization is effectively combined
    with the Parameter Efficient Fine-Tuning (PEFT) method, LoRA (Hu et al.,, [2021](#bib.bib21)),
    to improve efficiency and practicality in memory-constrained environments. Post-Training
    Quantization (PTQ) has the potential to be further improved to achieve sub-$3$
    bit quantization (Li et al.,, [2023](#bib.bib27); Guo et al.,, [2023](#bib.bib18)).
    However, it was found that the weights of the LLM often contain outliers — weights
    with significantly higher values than others ([Kim et al., 2023b,](#bib.bib24)
    ; [Dettmers et al., 2023b,](#bib.bib12) ). These outliers pose a considerable
    challenge for model compression with PTQ and lead to significant quantization
    errors.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Dettmers et al., 2023a](#bib.bib11)中，量化与参数高效微调（PEFT）方法LoRA（Hu et al., [2021](#bib.bib21)）有效结合，以提高在内存受限环境中的效率和实用性。后训练量化（PTQ）有可能进一步改进，以实现低于$3$位的量化（Li
    et al., [2023](#bib.bib27)；Guo et al., [2023](#bib.bib18)）。然而，发现LLM的权重通常包含离群值——即显著高于其他权重的值（[Kim
    et al., 2023b](#bib.bib24)；[Dettmers et al., 2023b](#bib.bib12)）。这些离群值对PTQ的模型压缩构成了相当大的挑战，并导致了显著的量化误差。
- en: 'In this paper we present ReALLM - for Residual Autoencoder LLM - a general
    approach for LLM PTQ and fine-tuning. Pre-trained LLM matrices are decomposed
    into a $16$-bit remainder (low rank, sparse outliers, etc.) and a compressed part,
    which is fed into a VQ autoencoder (Van Den Oord et al.,, [2017](#bib.bib47)).
    In our experiments, we implement a low-rank and quantized decomposition of pre-trained
    LLM matrices. In this approach, only the low-rank components are fine-tuned (block-wise
    and end-to-end) while the quantized elements remain static. Our quantization strategy
    (i.e. the shape of the autoencoder) adapts to the matrix patterns: Our results
    suggest that some pre-trained LLM matrices exhibit “spatial” patterns (see [Figure 1](#S1.F1
    "In 1 Introduction ‣ ReALLM: A general framework for LLM compression and fine-tuning");
    left) that bear similarities to those in images/videos and allow for highly effective
    compression (see [Figure 3](#S3.F3 "In ReALLM: a new LLM format. ‣ 3 Method ‣
    ReALLM: A general framework for LLM compression and fine-tuning")).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了ReALLM - Residual Autoencoder LLM的缩写 - 一种通用的LLM PTQ和微调方法。预训练的LLM矩阵被分解为$16$位的余数（低秩、稀疏离群值等）和压缩部分，然后输入到VQ自编码器中（Van
    Den Oord et al., [2017](#bib.bib47)）。在我们的实验中，我们实现了对预训练LLM矩阵的低秩和量化分解。在这种方法中，仅对低秩组件进行微调（按块和端到端），而量化元素保持静态。我们的量化策略（即自编码器的形状）适应矩阵模式：我们的结果表明，一些预训练的LLM矩阵显示出“空间”模式（见[图1](#S1.F1
    "在引言中 ‣ ReALLM：LLM压缩和微调的一般框架")；左侧），这些模式与图像/视频中的模式相似，并允许高度有效的压缩（见[图3](#S3.F3 "在ReALLM中：一种新的LLM格式。
    ‣ 3 方法 ‣ ReALLM：LLM压缩和微调的一般框架")）。
- en: '![Refer to caption](img/69769f0dcf311395c643505d6b507101.png)![Refer to caption](img/a5078b6b39a9315e48bf083e47a52b0b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/69769f0dcf311395c643505d6b507101.png)![参见标题](img/a5078b6b39a9315e48bf083e47a52b0b.png)'
- en: (a) Mistral-7B (Jiang et al.,, [2023](#bib.bib22))
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Mistral-7B（Jiang et al., [2023](#bib.bib22)）
- en: '![Refer to caption](img/c322daf7100a0c194d410694fbf1811e.png)![Refer to caption](img/1a3fe2b2cd3f9062207e0f1903a5d246.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c322daf7100a0c194d410694fbf1811e.png)![参考说明](img/1a3fe2b2cd3f9062207e0f1903a5d246.png)'
- en: (b) Gemma-2B (Team et al.,, [2024](#bib.bib43))
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Gemma-2B（Team等，[2024](#bib.bib43)）
- en: 'Figure 1: Pre-trained matrix from the first block (left; with “structures”),
    and pre-trained matrix from the last block (right) for two different models. Stronger
    vertical patterns appear in the first blocks.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：来自第一个块（左；带有“结构”）和最后一个块（右）的预训练矩阵，针对两个不同的模型。第一个块中出现了更强的垂直模式。
- en: 'Contributions:'
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贡献：
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present ReALLM, a method that uses a novel autoencoder and a residual pipeline
    to efficiently compress pre-trained LLM matrices;
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了ReALLM，这是一种使用新型自编码器和残差管道来高效压缩预训练LLM矩阵的方法；
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We show that state-of-the-art PTQ approaches (Lin et al.,, [2023](#bib.bib29);
    Shao et al.,, [2023](#bib.bib39); Tseng et al.,, [2024](#bib.bib46); Egiazarian
    et al.,, [2024](#bib.bib13)) and fine-tuning methods (Hu et al.,, [2021](#bib.bib21);
    [Dettmers et al., 2023a,](#bib.bib11) ; Guo et al.,, [2023](#bib.bib18); Li et al.,,
    [2023](#bib.bib27); Liao and Monz,, [2024](#bib.bib28)) are all special cases
    of ReALLM;
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了最先进的PTQ方法（Lin等，[2023](#bib.bib29)；Shao等，[2023](#bib.bib39)；Tseng等，[2024](#bib.bib46)；Egiazarian等，[2024](#bib.bib13)）和微调方法（Hu等，[2021](#bib.bib21)；[Dettmers
    et al., 2023a,](#bib.bib11)；Guo等，[2023](#bib.bib18)；Li等，[2023](#bib.bib27)；Liao和Monz，[2024](#bib.bib28)）都是ReALLM的特殊情况；
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a preprocessing step that includes scaling and column permutations
    of matrices to mitigate the quantization errors associated with outliers; We also
    propose to adapt the general autoencoder scheme to the type of pre-trained matrix
    patterns.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种预处理步骤，包括矩阵的缩放和列置换，以减轻与异常值相关的量化误差；我们还建议将通用自编码器方案调整为预训练矩阵模式的类型。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our approach demonstrates that fine-tuning end-to-end with block-wise error
    reduction leads to the best results reported in the literature for 3 and 2-bit
    Post-Training Quantization (PTQ).
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的方法展示了通过块级错误减少的端到端微调在3位和2位后训练量化（PTQ）中取得的最佳文献结果。
- en: 2 Related works
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLMs adapters.
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs 适配器。
- en: After the introduction of high-performance open-source LLMs and due to the impracticality
    of “full fine-tuning”, several methods of parameter-efficient fine-tuning (PEFT)
    have emerged, including prefix tuning (Li and Liang,, [2021](#bib.bib26)), selective
    fine-tuning (Guo et al.,, [2021](#bib.bib17)) and Low Rank Adapter (LoRA). LoRA,
    introduced in Hu et al., ([2021](#bib.bib21)), is a simple but effective fine-tuning
    method that retains the pre-trained matrices but adds a low-rank component. For
    a typical pre-trained matrix $W$ denotes the Euclidean norm of a matrix over each
    column. DoRA with the trainable size vector requires little computational effort,
    but can lead to significant performance improvements (Liu et al.,, [2024](#bib.bib31)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在高性能开源LLM引入后，由于“完全微调”的不切实际，出现了几种参数高效微调（PEFT）方法，包括前缀调整（Li和Liang，[2021](#bib.bib26)）、选择性微调（Guo等，[2021](#bib.bib17)）和低秩适配器（LoRA）。LoRA，由Hu等（[2021](#bib.bib21)）提出，是一种简单但有效的微调方法，它保留了预训练矩阵，但添加了一个低秩组件。对于典型的预训练矩阵
    $W$，表示每列上的欧几里得范数。带有可训练大小向量的DoRA需要很少的计算努力，但可以显著提高性能（Liu等，[2024](#bib.bib31)）。
- en: Quantization.
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化。
- en: Current methods for compressing LLMs predominantly use quantization techniques.
    Early strategies, such as ZeroQuant (Yao et al.,, [2022](#bib.bib52)) and nuQmm
    (Park et al.,, [2022](#bib.bib36)), relied primarily on direct rounding of weights
    to the nearest quantization level. Later developments improved this approach by
    handling outliers through quantization to higher bitwidths (Xiao et al.,, [2023](#bib.bib51);
    Dettmers et al.,, [2022](#bib.bib10); [Kim et al., 2023b,](#bib.bib24) ; [Dettmers
    et al., 2023b,](#bib.bib12) ). Methods similar to ReALLM include those that combine
    quantization with a low-rank decomposition; see e.g. [Dettmers et al., 2023a](#bib.bib11)
    ; Guo et al., ([2023](#bib.bib18)); Li et al., ([2023](#bib.bib27)); Liao and
    Monz, ([2024](#bib.bib28)). QLoRA ([Dettmers et al., 2023a,](#bib.bib11) ) combined
    Parameter Efficient Fine-Tuning (PEFT) and quantization, but added zero-initialised
    low-rank adapters after quantization. In contrast, Loftq (Li et al.,, [2023](#bib.bib27))
    and LQ-LoRA (Guo et al.,, [2023](#bib.bib18)) propose to minimize quantization
    errors by initializing LoRA components with an SVD of the pre-trained weights.
    As part of this integration, ApiQ (Liao and Monz,, [2024](#bib.bib28)) uses gradient
    descent to optimize both the LoRA components and the quantization parameters for
    the entire model rather than for each individual layer. Quantization of pre-trained
    weights facilitates efficient inference on devices with limited memory. To achieve
    significant computational and energy efficiency, recent studies have combined
    quantization of weights with activation quantization (Liu et al.,, [2023](#bib.bib30);
    Nrusimha et al.,, [2024](#bib.bib35)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当前压缩LLMs的方法主要使用量化技术。早期策略，如 ZeroQuant (Yao 等人，[2022](#bib.bib52)) 和 nuQmm (Park
    等人，[2022](#bib.bib36))，主要依赖于将权重直接舍入到最接近的量化水平。后来的发展通过将异常值量化到更高位宽来改进这种方法（Xiao 等人，[2023](#bib.bib51);
    Dettmers 等人，[2022](#bib.bib10); [Kim 等人，2023b，](#bib.bib24) ; [Dettmers 等人，2023b，](#bib.bib12)）。类似
    ReALLM 的方法包括那些将量化与低秩分解结合起来的；参见例如 [Dettmers 等人，2023a](#bib.bib11)；Guo 等人（[2023](#bib.bib18)）；Li
    等人（[2023](#bib.bib27)）；Liao 和 Monz（[2024](#bib.bib28)）。QLoRA ([Dettmers 等人，2023a，](#bib.bib11))
    结合了参数高效微调（PEFT）和量化，但在量化后添加了零初始化的低秩适配器。相比之下，Loftq (Li 等人，[2023](#bib.bib27)) 和
    LQ-LoRA (Guo 等人，[2023](#bib.bib18)) 提出了通过用预训练权重的SVD初始化LoRA组件来最小化量化误差。在这种整合的一部分中，ApiQ
    (Liao 和 Monz，[2024](#bib.bib28)) 使用梯度下降来优化整个模型的 LoRA 组件和量化参数，而不是单独优化每一层。对预训练权重的量化有助于在内存有限的设备上实现高效推理。为了实现显著的计算和能源效率，最近的研究将权重量化与激活量化结合起来（Liu
    等人，[2023](#bib.bib30)；Nrusimha 等人，[2024](#bib.bib35)）。
- en: Block/Layer-Wise Tuning.
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 块/层级调整。
- en: GPTQ (Frantar et al.,, [2022](#bib.bib14)) introduced a higher accuracy strategy
    using an approximate large-scale solver to minimize the layer-wise quadratic error,
    which is crucial for low bit-width quantization, as highlighted in Tseng et al.,
    ([2024](#bib.bib46)); Egiazarian et al., ([2024](#bib.bib13)). Quip# (Tseng et al.,,
    [2024](#bib.bib46)) applies random rotations to the pre-trained matrices, segments
    the resulting matrix into vectors of dimension $d=8$ bits per parameter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ (Frantar 等人，[2022](#bib.bib14)) 介绍了一种更高精度的策略，使用近似的大规模求解器来最小化层级的二次误差，这对低位宽量化至关重要，如
    Tseng 等人（[2024](#bib.bib46)）和 Egiazarian 等人（[2024](#bib.bib13)）所强调。Quip# (Tseng
    等人，[2024](#bib.bib46)) 对预训练矩阵应用随机旋转，将结果矩阵分割为每个参数 $d=8$ 位的向量。
- en: 3 Method
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: '![Refer to caption](img/1cdadff5c11726b871db407ed222c818.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1cdadff5c11726b871db407ed222c818.png)'
- en: 'Figure 2: ReALLM; during the fine-tuning step only low-rank and scales are
    updated'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: ReALLM; 在微调步骤中仅更新低秩和缩放'
- en: Low-rank/sparse decomposition.
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 低秩/稀疏分解。
- en: 'Starting from a pre-trained LLM matrix $W\in\mathbb{R}^{p\times q}$ that (approximately)
    solve the following problem:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个预训练的LLM矩阵 $W\in\mathbb{R}^{p\times q}$ 开始，该矩阵（大致上）解决以下问题：
- en: '|  | $\min_{Q,L_{1},L_{2}}\&#124;W-(Q+L_{1}(L_{2})^{t})\&#124;.$ |  | (1) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{Q,L_{1},L_{2}}\&#124;W-(Q+L_{1}(L_{2})^{t})\&#124;.$ |  | (1) |'
- en: QLoRA [Dettmers et al., 2023a](#bib.bib11) provides a suboptimal solution for
    the previously described optimization problem by setting $L_{1}=0$. There is no
    guarantee that the initialization of the low-rank part to zero is optimal. It
    has been reported that QLoRA, Apiq and Loftq perform better than QLoRA in several
    language generation benchmarks (Guo et al.,, [2023](#bib.bib18); Liao and Monz,,
    [2024](#bib.bib28); Li et al.,, [2023](#bib.bib27)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA [Dettmers et al., 2023a](#bib.bib11) 通过将 $L_{1}=0$ 提供了对前述优化问题的次优解。没有保证低秩部分初始化为零是最优的。已有报告指出，QLoRA、ApiQ
    和 Loftq 在多个语言生成基准测试中表现优于 QLoRA（Guo 等，[2023](#bib.bib18)；Liao 和 Monz，[2024](#bib.bib28)；Li
    等，[2023](#bib.bib27)）。
- en: Mixed-autoencoder configuration.
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 混合自编码器配置。
- en: An autoencoder is the composition of an encoding function $\mathcal{E}$. LQ-LoRA
    (Guo et al.,, [2023](#bib.bib18)), Loftq (Li et al.,, [2023](#bib.bib27)), and
    ApiQ (Liao and Monz,, [2024](#bib.bib28)) are special cases of ReALLM where the
    encoder and the decoder are defined as the identity matrix.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是编码函数 $\mathcal{E}$ 的组合。LQ-LoRA（Guo 等，[2023](#bib.bib18)）、Loftq（Li 等，[2023](#bib.bib27)）和
    ApiQ（Liao 和 Monz，[2024](#bib.bib28)）是 ReALLM 的特殊情况，其中编码器和解码器被定义为单位矩阵。
- en: 'The approach may not be optimal as some matrices are more challenging to quantize
    than others (Guo et al.,, [2023](#bib.bib18)). Specifically, [Figure 1](#S1.F1
    "In 1 Introduction ‣ ReALLM: A general framework for LLM compression and fine-tuning")
    shows that pre-trained LLM matrices can display very different “spatial” patterns.
    ReALLM adapts the autoencoder to the type and shape of the matrix. When quantizing
    pre-trained matrices with strong coefficient dependencies, ReALLM is akin to image
    and video compression techniques that use the implicit neural representation (Chen
    et al.,, [2023](#bib.bib6); Kwan et al.,, [2024](#bib.bib25)). ReALLM extracts
    latent representations $\mathcal{E}_{\psi}(W)$ consisting of standard 2D convolutions,
    and a decoder combining 2D-convNeXt (Liu et al.,, [2022](#bib.bib32)) and PixelShuffle
    (Shi et al.,, [2016](#bib.bib40)).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '这种方法可能并非最优，因为有些矩阵比其他矩阵更难以量化（Guo 等，[2023](#bib.bib18)）。具体而言，[图 1](#S1.F1 "在
    1 引言 ‣ ReALLM: 用于 LLM 压缩和微调的一般框架") 显示了预训练 LLM 矩阵可能表现出非常不同的“空间”模式。ReALLM 使自编码器适应矩阵的类型和形状。在量化具有强系数依赖性的预训练矩阵时，ReALLM
    类似于使用隐式神经表示的图像和视频压缩技术（Chen 等，[2023](#bib.bib6)；Kwan 等，[2024](#bib.bib25)）。ReALLM
    提取由标准 2D 卷积组成的潜在表示 $\mathcal{E}_{\psi}(W)$，以及一个结合了 2D-convNeXt（Liu 等，[2022](#bib.bib32)）和
    PixelShuffle（Shi 等，[2016](#bib.bib40)）的解码器。'
- en: The decoding process is fast, as HNeRV requires only one network forward operation
    for decoding. ReALLM compression is a combination of a small (w.r.t. input signals)
    neural decoder model $\mathcal{D}_{\phi}$ bits per coordinate.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 解码过程非常快速，因为 HNeRV 只需要进行一次网络前向操作来完成解码。ReALLM 压缩是一个小型（相对于输入信号）神经解码器模型 $\mathcal{D}_{\phi}$
    每个坐标的比特数的组合。
- en: Vector Quantization (VQ).
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量量化 (VQ)。
- en: An efficient way to store the embedding $\mathcal{E}_{\psi}(W)$ bits). It should
    be noted that no separate gradient is defined for the quantization operator with
    the closest element (Van Den Oord et al.,, [2017](#bib.bib47)). Therefore, during
    the backward pass, we approximate the gradient similarly to the straight-through
    estimator (Bengio,, [2013](#bib.bib5)) and simply copy the gradients from the
    decoder input to the encoder output.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 存储嵌入 $\mathcal{E}_{\psi}(W)$ 比特的高效方法。需要注意的是，量化操作符的最近元素没有定义单独的梯度（Van Den Oord
    等，[2017](#bib.bib47)）。因此，在反向传播过程中，我们将梯度近似为类似于直通估计器的方式（Bengio，[2013](#bib.bib5)），并将梯度从解码器输入简单地复制到编码器输出。
- en: Quantization pre-processing.
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化预处理。
- en: 'Before using a tensor quantization method, it is important to perform an appropriate
    scaling. Several parameters (number of blocks, quantile bins, etc.) are chosen
    to correspond to a given compression ratio. But the presence of outliers ([Kim
    et al., 2023b,](#bib.bib24) ; [Dettmers et al., 2023b,](#bib.bib12) ) forces the
    scaling and quantization methods to have a poor compression ratio (Lin et al.,,
    [2023](#bib.bib29); Tseng et al.,, [2024](#bib.bib46); Ashkboos et al.,, [2024](#bib.bib4)).
    Incoherence processing uses random rotations as a pre-processing step. Although
    the main purpose of incoherence processing is to reduce the effects of outliers
    (Tseng et al.,, [2024](#bib.bib46); Ashkboos et al.,, [2024](#bib.bib4)), this
    technique has a detrimental effect on the structure of the pre-trained matrices
    within the initial blocks of the LLM (see [Figures 1](#S1.F1 "In 1 Introduction
    ‣ ReALLM: A general framework for LLM compression and fine-tuning") and [3](#S3.F3
    "Figure 3 ‣ ReALLM: a new LLM format. ‣ 3 Method ‣ ReALLM: A general framework
    for LLM compression and fine-tuning")). This is a serious bottleneck as quantization
    errors in these initial blocks can propagate throughout the model. As shown in
    [Figure 1](#S1.F1 "In 1 Introduction ‣ ReALLM: A general framework for LLM compression
    and fine-tuning"), some matrices have no specific patterns and resemble random
    Gaussian noise interspersed with randomly positioned outliers. To deal with outliers
    in the latent representation, we suggest rearranging the columns to create some
    spatial regularity. This strategy aims to find the most effective permutations
    that cluster outliers. Trukhanov and Soloveychik, ([2024](#bib.bib45)) has recently
    elaborated a row/column permutation strategy that summarizes vectors (i.e. sets
    of rows or columns) with similar norms. In contrast, for ReALLM we propose to
    permute columns such that neighboring columns are “similar” and not just on the
    same hypersphere. We develop a basic, yet efficient method for this: first we
    select a block of size $128\times q$ bits per coordinate.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '在使用张量量化方法之前，重要的是要进行适当的缩放。几个参数（块数、分位数箱等）被选择以对应于给定的压缩比。然而，异常值的存在（[Kim et al.,
    2023b,](#bib.bib24); [Dettmers et al., 2023b,](#bib.bib12)）使得缩放和量化方法的压缩比较差（Lin
    et al., [2023](#bib.bib29); Tseng et al., [2024](#bib.bib46); Ashkboos et al.,
    [2024](#bib.bib4)）。不一致性处理使用随机旋转作为预处理步骤。尽管不一致性处理的主要目的是减少异常值的影响（Tseng et al., [2024](#bib.bib46);
    Ashkboos et al., [2024](#bib.bib4)），但这一技术对 LLM 初始块内的预训练矩阵结构有不利影响（见 [Figures 1](#S1.F1
    "在 1 介绍 ‣ ReALLM: LLM 压缩与微调的通用框架") 和 [3](#S3.F3 "图 3 ‣ ReALLM: 一种新的 LLM 格式。 ‣
    3 方法 ‣ ReALLM: LLM 压缩与微调的通用框架")）。这是一个严重的瓶颈，因为这些初始块中的量化误差可能会在整个模型中传播。如 [Figure
    1](#S1.F1 "在 1 介绍 ‣ ReALLM: LLM 压缩与微调的通用框架") 所示，一些矩阵没有特定的模式，类似于随机高斯噪声夹杂着随机位置的异常值。为了解决潜在表示中的异常值问题，我们建议重新排列列以创建一些空间规律性。这一策略旨在寻找最有效的排列，从而将异常值聚集在一起。Trukhanov
    和 Soloveychik（[2024](#bib.bib45)）最近详细阐述了一种行/列排列策略，该策略总结了具有相似范数的向量（即行或列的集合）。相比之下，对于
    ReALLM，我们建议对列进行排列，使相邻的列是“相似”的，而不仅仅是在同一个超球面上。我们为此开发了一种基本而有效的方法：首先选择一个大小为 $128\times
    q$ 的块，每个坐标的位数。'
- en: Input : Matrix $w$
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：矩阵 $w$
- en: Algorithm 1 permutation function
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 排列函数
- en: 'ReALLM: a new LLM format.'
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ReALLM：一种新的 LLM 格式。
- en: LLM standard formats represent LLM weights as a set of matrices encoded on $16$.
    This speeds up the decoding step compared to diffusion-based approaches (Wang
    et al.,, [2024](#bib.bib50); Soro et al.,, [2024](#bib.bib41)). Note that for
    ReALLM a decoder model has to be trained on LLM matrices, but this learning step
    is done once and for all. Additionally, the more we train and overfit, the better
    ReALLM becomes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 标准格式将 LLM 权重表示为一组编码为 $16$ 的矩阵。这比基于扩散的方法（Wang et al., [2024](#bib.bib50);
    Soro et al., [2024](#bib.bib41)）加快了解码步骤。注意，对于 ReALLM，必须在 LLM 矩阵上训练一个解码器模型，但这个学习步骤只需进行一次。此外，我们训练和过拟合得越多，ReALLM
    的性能就越好。
- en: 'The set of hyper-parameters for ReALLM are: $r$ patches extracted from pre-trained
    LLM matrices, and we use the HNeRV Chen et al., ([2023](#bib.bib6)) autoencoder
    model. For more details on the practical aspect of decoder training, see [Section A.2](#A1.SS2
    "A.2 Autoencoder computational limitations ‣ Appendix A Appendix / supplemental
    material ‣ ReALLM: A general framework for LLM compression and fine-tuning").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'ReALLM 的超参数集合为：从预训练的 LLM 矩阵中提取的 $r$ 个块，我们使用 HNeRV Chen et al., ([2023](#bib.bib6))
    自动编码器模型。有关解码器训练实际方面的更多细节，请参见 [Section A.2](#A1.SS2 "A.2 自动编码器计算限制 ‣ 附录 A 附录/补充材料
    ‣ ReALLM: LLM 压缩与微调的通用框架")。'
- en: We have experimentally discovered two sets of optimal combinations of hyperparameters
    that depend on the type and shape of the pre-trained matrix. Some pre-trained
    matrices, especially those closer to the input tokens, compress better with small
    latent representations ($e_{0}e_{1}e_{2}<1024$).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验发现了两组最佳的超参数组合，这些组合取决于预训练矩阵的类型和形状。一些预训练矩阵，特别是那些接近输入标记的矩阵，用小的潜在表示（$e_{0}e_{1}e_{2}<1024$）压缩效果更好。
- en: '![Refer to caption](img/2a3b8c73103df94f28a4dac4b780901b.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2a3b8c73103df94f28a4dac4b780901b.png)'
- en: (a) Mistral-7B (Jiang et al.,, [2023](#bib.bib22))
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Mistral-7B (Jiang et al.,, [2023](#bib.bib22))
- en: '![Refer to caption](img/56a4889e163c5d185e777e5271277b5e.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/56a4889e163c5d185e777e5271277b5e.png)'
- en: (b) Llama2-7B (Touvron et al.,, [2023](#bib.bib44))
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Llama2-7B (Touvron et al.,, [2023](#bib.bib44))
- en: 'Figure 3: Reconstruction (Frobenius norm) error for layer of type “Q” for all
    blocks. Quip# (Tseng et al.,, [2024](#bib.bib46)) does not take advantage of the
    structures in the first blocks.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 所有块中“Q”类型层的重建（Frobenius 范数）误差。Quip# (Tseng et al.,, [2024](#bib.bib46))
    并未利用前几个块中的结构。'
- en: 'In [Figure 3](#S3.F3 "In ReALLM: a new LLM format. ‣ 3 Method ‣ ReALLM: A general
    framework for LLM compression and fine-tuning") ReALLM achieves the lowest Frobenius
    norm quantization error. We perform ablation experiments with this metric to decouple
    the effects of VQ and permutation preprocessing of ReALLM on the final performance.
    For example, in block $8$. Quip# rotates the matrices randomly, causing all patterns
    in the initial blocks to be lost.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 3](#S3.F3 "在 ReALLM 中：一种新的 LLM 格式。 ‣ 3 方法 ‣ ReALLM：一种通用的 LLM 压缩和微调框架")
    中，ReALLM 达到了最低的 Frobenius 范数量化误差。我们使用这个度量进行消融实验，以解耦 VQ 和 ReALLM 的置换预处理对最终性能的影响。例如，在块
    $8$ 中，Quip# 随机旋转矩阵，导致初始块中的所有模式丢失。
- en: Input : Number of end-to-end fine-tuning steps $T$ with gradient descent ;26      27
    end for
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '输入: 端到端微调步骤 $T$ 的数量与梯度下降 ;26      27 结束'
- en: Algorithm 2 Pseudo-code for ReALLM with block-wise and end-to-end fine-tuning
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 ReALLM 的伪代码，包含块级和端到端的微调。
- en: 4 Experimental validation
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验验证
- en: We test ReALLM on the LLaMA-2 (Touvron et al.,, [2023](#bib.bib44)) family models
    (with $7$ hours for a LLaMA2-7B model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 LLaMA-2 (Touvron et al.,, [2023](#bib.bib44)) 系列模型上测试 ReALLM（LLaMA2-7B 模型需
    $7$ 小时）。
- en: Language Generation Tasks.
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语言生成任务。
- en: For continual language modeling, we train on a single partition of the C4 (Raffel
    et al.,, [2020](#bib.bib37)) dataset for half an epoch and use a sequence length
    of $4096$ for both WikiText-2 (Merity et al.,, [2016](#bib.bib34)) and C4 evaluation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续语言建模，我们在 C4 (Raffel et al.,, [2020](#bib.bib37)) 数据集的一个分区上训练半个周期，并使用 $4096$
    的序列长度用于 WikiText-2 (Merity et al.,, [2016](#bib.bib34)) 和 C4 评估。
- en: 'Our main baselines are LQ-LoRA (Guo et al.,, [2023](#bib.bib18)), Quip# (Tseng
    et al.,, [2024](#bib.bib46)), and AQLM (Egiazarian et al.,, [2024](#bib.bib13)).
    However, we also report the performance of popular quantization approaches GPTQ
    (Frantar et al.,, [2022](#bib.bib14)), AWQ (Lin et al.,, [2023](#bib.bib29)),
    Omniquant (Shao et al.,, [2023](#bib.bib39)), as well as the performance of recent
    work ApiQ (Liao and Monz,, [2024](#bib.bib28)) and QuaRot (Ashkboos et al.,, [2024](#bib.bib4)).
    In the results below, we present the target bits per parameter that takes into
    account quantized weights and include parameters kept in high precision (head
    layer, scales, codebooks, permutations in $16$ bits precision) similarly to the
    related work. The exact bit budget is detailed in [Table 5](#A1.T5 "In A.2 Autoencoder
    computational limitations ‣ Appendix A Appendix / supplemental material ‣ ReALLM:
    A general framework for LLM compression and fine-tuning") in the Appendix.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要基准是 LQ-LoRA (Guo et al.,, [2023](#bib.bib18))、Quip# (Tseng et al.,, [2024](#bib.bib46))
    和 AQLM (Egiazarian et al.,, [2024](#bib.bib13))。然而，我们还报告了流行的量化方法 GPTQ (Frantar
    et al.,, [2022](#bib.bib14))、AWQ (Lin et al.,, [2023](#bib.bib29))、Omniquant (Shao
    et al.,, [2023](#bib.bib39))，以及最近的 ApiQ (Liao 和 Monz,, [2024](#bib.bib28)) 和 QuaRot
    (Ashkboos et al.,, [2024](#bib.bib4)) 的性能。在以下结果中，我们展示了每个参数的目标位数，这些位数考虑了量化权重，并包括以高精度（头层、尺度、代码本、$16$
    位精度的置换）保留的参数，类似于相关工作。具体的位预算详见附录中的 [表 5](#A1.T5 "在 A.2 自编码器计算限制 ‣ 附录 A 附录 / 补充材料
    ‣ ReALLM：一种通用的 LLM 压缩和微调框架")。
- en: In our experiments, following [Dettmers et al., 2023a](#bib.bib11) ; Guo et al.,
    ([2023](#bib.bib18)), we take a DoRA (Liu et al.,, [2024](#bib.bib31)) rank of
    $r=64$. As far as we know, we have also developed the first VQ code (available
    in the supplementary material) that makes efficient use of PyTorch’s “torch dispatch”
    functionality (Ansel et al.,, [2024](#bib.bib2)), which is known to be as fast
    as dedicated CUDA kernels (Guo et al.,, [2023](#bib.bib18)). This allows us to
    overload PyTorch operations to perform just-in-time dequantization.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，按照 [Dettmers 等人, 2023a](#bib.bib11)；Guo 等人 ([2023](#bib.bib18))，我们采用
    DoRA (Liu 等人，[2024](#bib.bib31)) 排名 $r=64$。据我们了解，我们还开发了第一个 VQ 代码（可在补充材料中找到），它高效利用了
    PyTorch 的“torch dispatch”功能（Ansel 等人，[2024](#bib.bib2)），已知其速度与专用 CUDA 内核相当（Guo
    等人，[2023](#bib.bib18)）。这使我们能够重载 PyTorch 操作以执行即时去量化。
- en: 'In [Tables 1](#S4.T1 "In Language Generation Tasks. ‣ 4 Experimental validation
    ‣ ReALLM: A general framework for LLM compression and fine-tuning") and [2](#S4.T2
    "Table 2 ‣ Language Generation Tasks. ‣ 4 Experimental validation ‣ ReALLM: A
    general framework for LLM compression and fine-tuning") we evaluate the perplexity
    of ReALLM on the respective validation datasets of C4 and WikiText-2 for a single
    run. During fine-tuning (on a single partition of the C4 dataset), we only update
    the DoRA components (scales and low-rank matrices). For each dataset, we provide
    three sets of results in [Table 1](#S4.T1 "In Language Generation Tasks. ‣ 4 Experimental
    validation ‣ ReALLM: A general framework for LLM compression and fine-tuning"):
    Perplexity without any fine-tuning (only low-rank and VQ autoencoder decomposition),
    perplexity with only block-wise fine-tuning, and perplexities with end-to-end
    fine-tuning (in addition to the block-wise fine-tuning process).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [表1](#S4.T1 "语言生成任务。 ‣ 4 实验验证 ‣ ReALLM: LLM压缩和微调的通用框架") 和 [2](#S4.T2 "表2
    ‣ 语言生成任务。 ‣ 4 实验验证 ‣ ReALLM: LLM压缩和微调的通用框架") 中，我们评估了 ReALLM 在 C4 和 WikiText-2
    的各自验证数据集上的困惑度（单次运行）。在微调过程中（在 C4 数据集的一个分区上），我们仅更新 DoRA 组件（尺度和低秩矩阵）。对于每个数据集，我们在
    [表1](#S4.T1 "语言生成任务。 ‣ 4 实验验证 ‣ ReALLM: LLM压缩和微调的通用框架") 中提供了三组结果：无微调的困惑度（仅低秩和
    VQ 自编码器分解），仅按块微调的困惑度，以及端到端微调（除了按块微调过程）的困惑度。'
- en: 'Table 1: Perplexity $(\downarrow)$'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：困惑度 $(\downarrow)$
- en: '| Method | #bits | rank $r$ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | 排名 $r$ |'
- en: '| ReALLM (no fine-tuning) | $3$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (无微调) | $3$ |'
- en: '| ReALLM (block-wise) | $3$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (按块) | $3$ |'
- en: '| ReALLM (40% training) | $3$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (40% 训练) | $3$ |'
- en: '| ReALLM (full training) | $3$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (全训练) | $3$ |'
- en: '| ReALLM (no fine-tuning) | $3$ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (无微调) | $3$ |'
- en: '| ReALLM (block-wise) | $3$ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (按块) | $3$ |'
- en: '| ReALLM (40% training) | $3$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (40% 训练) | $3$ |'
- en: '| ReALLM (full training) | $3$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (全训练) | $3$ |'
- en: '| ReALLM (no fine-tuning) | $2$ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (无微调) | $2$ |'
- en: '| ReALLM (block-wise 50 epochs) | $2$ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (按块微调 50 个周期) | $2$ |'
- en: '| ReALLM (block-wise 200 epochs) | $2$ |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (按块微调 200 个周期) | $2$ |'
- en: '| ReALLM (40% training) | $2$ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (40% 训练) | $2$ |'
- en: '| ReALLM (full training) | $2$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (全训练) | $2$ |'
- en: '| ReALLM (no fine-tuning) | $2$ |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (无微调) | $2$ |'
- en: '| ReALLM (block-wise 50 epochs) | $2$ | 15.74 | 12.08 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (按块微调 50 个周期) | $2$ | 15.74 | 12.08 |'
- en: '| ReALLM (40% training) | $2$ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (40% 训练) | $2$ |'
- en: '| ReALLM (full training) | $2$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (全训练) | $2$ |'
- en: 'Our *data-free* version of ReALLM (no fine-tuning; see [Table 1](#S4.T1 "In
    Language Generation Tasks. ‣ 4 Experimental validation ‣ ReALLM: A general framework
    for LLM compression and fine-tuning")) achieves state-of-the-art metrics for $3$
    bits are needed to store the codebook). Additional results for other models are
    available in the Appendix.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的*无数据*版本 ReALLM（无微调；见 [表1](#S4.T1 "语言生成任务。 ‣ 4 实验验证 ‣ ReALLM: LLM压缩和微调的通用框架")）在需要
    $3$ 位来存储代码本的情况下达到了最先进的指标。其他模型的额外结果见附录。'
- en: 'Table 2: Perplexity $(\downarrow)$'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：困惑度 $(\downarrow)$
- en: '| Method | Number of bits | C4 $(\downarrow)$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | C4 $(\downarrow)$ |'
- en: '|  |  | 7B | 13B | 7B | 13B |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 7B | 13B | 7B | 13B |'
- en: '| LLaMA2 (Touvron et al.,, [2023](#bib.bib44)) | $16$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2 (Touvron 等人，[2023](#bib.bib44)) | $16$ |'
- en: '| GPTQ (Frantar et al.,, [2022](#bib.bib14)) | $3$ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ (Frantar 等人，[2022](#bib.bib14)) | $3$ |'
- en: '| AWQ (Lin et al.,, [2023](#bib.bib29)) | $3$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| AWQ (Lin 等人，[2023](#bib.bib29)) | $3$ |'
- en: '| Omniquant (Shao et al.,, [2023](#bib.bib39)) | $3$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant (Shao 等人，[2023](#bib.bib39)) | $3$ |'
- en: '| LQ-LoRA (Guo et al.,, [2023](#bib.bib18)) | $3$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LQ-LoRA (Guo 等人，[2023](#bib.bib18)) | $3$ |'
- en: '| LoftQ (Li et al.,, [2023](#bib.bib27)) | $3$ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| LoftQ (Li 等人，[2023](#bib.bib27)) | $3$ |'
- en: '| ApiQ[PTQ] (Liao and Monz,, [2024](#bib.bib28)) | $3$ |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ApiQ[PTQ] (Liao and Monz,, [2024](#bib.bib28)) | $3$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | $3$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | $3$ |'
- en: '| QuaRot[A16W3] (Ashkboos et al.,, [2024](#bib.bib4)) | $3$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| QuaRot[A16W3] (Ashkboos et al.,, [2024](#bib.bib4)) | $3$ |'
- en: '| ReALLM | $3$ | 7.27 | 6.69 | 5.77 | 5.14 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | $3$ | 7.27 | 6.69 | 5.77 | 5.14 |'
- en: '| LoftQ (Li et al.,, [2023](#bib.bib27)) | $2$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| LoftQ (Li et al.,, [2023](#bib.bib27)) | $2$ |'
- en: '| ApiQ (Liao and Monz,, [2024](#bib.bib28)) | $2$ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ApiQ (Liao and Monz,, [2024](#bib.bib28)) | $2$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | $2$ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | $2$ |'
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | $2$ | 6.64 | 5.65 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | $2$ | 6.64 | 5.65 |'
- en: '| ReALLM | $2$ | 8.28 | 7.50 | 6.69 | 5.72 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | $2$ | 8.28 | 7.50 | 6.69 | 5.72 |'
- en: 'In [Table 2](#S4.T2 "In Language Generation Tasks. ‣ 4 Experimental validation
    ‣ ReALLM: A general framework for LLM compression and fine-tuning") we compare
    ReALLM with end-to-end fine-tuning, and the best performing PTQ approaches. All
    the methods cited in [Table 2](#S4.T2 "In Language Generation Tasks. ‣ 4 Experimental
    validation ‣ ReALLM: A general framework for LLM compression and fine-tuning")
    also uses a calibration dataset. It is interesting to note that ReALLM with $2$
    bits precision) during the layer-wise fine-tuning. This does not only slow down
    the PTQ process (as gradients must be store for all weights in the given block),
    but it also means Quip# has to store learnable vectors and also quantized weights
    for *each* fine-tuning task.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表2](#S4.T2 "语言生成任务中的实验验证 ‣ ReALLM：LLM压缩和微调的一般框架")中，我们将ReALLM与端到端微调以及最佳PTQ方法进行了比较。表2中引用的所有方法也使用了校准数据集。有趣的是，在层级微调过程中，ReALLM以$2$
    bits精度运行。这不仅会减慢PTQ过程（因为梯度必须存储在给定块中的所有权重中），而且意味着Quip#必须存储可学习的向量以及*每个*微调任务的量化权重。
- en: 'Table 3: Accuracy $(\uparrow)$ in LM Eval (acc, not acc_norm).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在LM Eval中的准确率 $(\uparrow)$（准确率，而非规范化准确率）。
- en: '| Method | Size | #bits | ARC-challenge | ARC-easy | PiQA | Winogrande | Average
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 大小 | #bits | ARC-challenge | ARC-easy | PiQA | Winogrande | 平均值 |'
- en: '| LLaMA-2 | 7B | $16$ |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2 | 7B | $16$ |'
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | 7B | $2$ |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | 7B | $2$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | 7B | $2$ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | 7B | $2$ |'
- en: '| ReALLM | 7B | $2$ | 35.15 | 68.56 | 75.73 | 66.46 | 61.47 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | 7B | $2$ | 35.15 | 68.56 | 75.73 | 66.46 | 61.47 |'
- en: '| LLaMA-2 | 13B | $16$ |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2 | 13B | $16$ |'
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | 13B | $3$ |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | 13B | $3$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | 13B | $3$ |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | 13B | $3$ |'
- en: '| ReALLM | 13B | $3$ | 47.01 | 75.96 | 78.67 | 70.96 | 68.15 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | 13B | $3$ | 47.01 | 75.96 | 78.67 | 70.96 | 68.15 |'
- en: Zero-Shot Tasks.
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 零-shot任务。
- en: 'Following HuggingFace’s Open LLM Leaderboard⁴⁴4https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard,
    and the literature (Frantar et al.,, [2022](#bib.bib14); Guo et al.,, [2023](#bib.bib18)),
    we also measure zero-shot accuracy on ARC (Clark et al.,, [2018](#bib.bib7)),
    PiQA (Tata and Patel,, [2003](#bib.bib42)), and Winogrande (Sakaguchi et al.,,
    [2021](#bib.bib38)), via the LM Evalaluation Harness (Gao et al.,, [2021](#bib.bib15)).
    We report results in [Table 3](#S4.T3 "In Language Generation Tasks. ‣ 4 Experimental
    validation ‣ ReALLM: A general framework for LLM compression and fine-tuning"),
    and compute the average on the 4 mentioned tasks. For all LLM sizes, ReALLM provides
    a notable advantage (between $0.5$ bits) on the zero-shot tasks.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 根据HuggingFace的Open LLM Leaderboard⁴⁴4https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard，以及文献（Frantar
    et al.,, [2022](#bib.bib14); Guo et al.,, [2023](#bib.bib18)），我们还测量了ARC（Clark
    et al.,, [2018](#bib.bib7)）、PiQA（Tata and Patel,, [2003](#bib.bib42)）和Winogrande（Sakaguchi
    et al.,, [2021](#bib.bib38)）的零-shot准确率，使用LM Evalaluation Harness（Gao et al.,,
    [2021](#bib.bib15)）。我们在[表3](#S4.T3 "语言生成任务中的实验验证 ‣ ReALLM：LLM压缩和微调的一般框架")中报告结果，并计算了4个提到的任务的平均值。对于所有LLM规模，ReALLM在零-shot任务中提供了显著的优势（在$0.5$
    bits之间）。
- en: 5 Conclusion
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We present ReALLM, a weight-only PTQ method that achieves state-of-the-art results
    on LLMs at $2$ GB of RAM.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了ReALLM，一种仅基于权重的PTQ方法，在$2$ GB RAM的情况下实现了LLMs的最先进结果。
- en: Large context sequence lengths result in large $KV$-cache quantization, and
    how to combine it with activation quantization.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 大上下文序列长度导致大规模$KV$-cache量化，以及如何将其与激活量化结合。
- en: 6 Societal impact
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 社会影响
- en: This paper presents work whose goal is to advance the field of LLM compression
    and fine-tuning. There are many potential societal consequences of our work, in
    particular malicious usage of LLMs for spams or language generation on edge devices.
    However, this negative societal impact is not limited to ReALLM, but to the field
    of LLM in general.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍的工作旨在推动 LLM 压缩和微调领域的发展。我们的工作有许多潜在的社会影响，特别是 LLM 在垃圾邮件或边缘设备语言生成中的恶意使用。然而，这种负面的社会影响不仅限于
    ReALLM，而是整个 LLM 领域。
- en: References
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Almazrouei et al., (2023) Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli,
    A., Cojocaru, R., Debbah, M., Goffinet, É., Hesslow, D., Launay, J., Malartic,
    Q., et al. (2023). The falcon series of open language models. arXiv preprint arXiv:2311.16867.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Almazrouei et al., (2023) Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli,
    A., Cojocaru, R., Debbah, M., Goffinet, É., Hesslow, D., Launay, J., Malartic,
    Q., 等 (2023). Falcon 系列开放语言模型。arXiv 预印本 arXiv:2311.16867。
- en: 'Ansel et al., (2024) Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A.,
    Voznesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia,
    A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J.,
    Gschwind, M., Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano,
    M., Liang, Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso,
    M., Saroufim, M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang,
    X., Wen, W., Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G.,
    Wu, P., and Chintala, S. (2024). PyTorch 2: Faster Machine Learning Through Dynamic
    Python Bytecode Transformation and Graph Compilation. In 29th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems,
    Volume 2 (ASPLOS ’24). ACM.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ansel et al., (2024) Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A.,
    Voznesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia,
    A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J.,
    Gschwind, M., Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano,
    M., Liang, Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso,
    M., Saroufim, M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang,
    X., Wen, W., Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G.,
    Wu, P., 和 Chintala, S. (2024). PyTorch 2: 通过动态 Python 字节码转换和图形编译加速机器学习。发表于第29届
    ACM 编程语言与操作系统架构支持国际会议，第2卷 (ASPLOS ’24)。ACM。'
- en: 'Arthur et al., (2007) Arthur, D., Vassilvitskii, S., et al. (2007). k-means++:
    The advantages of careful seeding. In Soda, volume 7, pages 1027–1035.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arthur et al., (2007) Arthur, D., Vassilvitskii, S., 等 (2007). k-means++: 精心初始化的优势。发表于
    Soda，第7卷，页码 1027–1035。'
- en: 'Ashkboos et al., (2024) Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B.,
    Jaggi, M., Alistarh, D., Hoefler, T., and Hensman, J. (2024). Quarot: Outlier-free
    4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ashkboos et al., (2024) Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B.,
    Jaggi, M., Alistarh, D., Hoefler, T., 和 Hensman, J. (2024). Quarot: 旋转的 llms 中无异常值的
    4 位推断。arXiv 预印本 arXiv:2404.00456。'
- en: Bengio, (2013) Bengio, Y. (2013). Estimating or propagating gradients through
    stochastic neurons. arXiv preprint arXiv:1305.2982.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio, (2013) Bengio, Y. (2013). 通过随机神经元估计或传播梯度。arXiv 预印本 arXiv:1305.2982。
- en: 'Chen et al., (2023) Chen, H., Gwilliam, M., Lim, S.-N., and Shrivastava, A.
    (2023). Hnerv: A hybrid neural representation for videos. In Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10270–10279.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al., (2023) Chen, H., Gwilliam, M., Lim, S.-N., 和 Shrivastava, A. (2023).
    Hnerv: 一种混合神经表示的视频。发表于 IEEE/CVF 计算机视觉与模式识别会议论文集，页码 10270–10279。'
- en: Clark et al., (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. (2018). Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al., (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., 和 Tafjord, O. (2018). 认为你已经解决了问答问题？尝试 arc，ai2 推理挑战。arXiv 预印本
    arXiv:1803.05457。
- en: 'Courbariaux et al., (2015) Courbariaux, M., Bengio, Y., and David, J.-P. (2015).
    Binaryconnect: Training deep neural networks with binary weights during propagations.
    Advances in neural information processing systems, 28.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Courbariaux et al., (2015) Courbariaux, M., Bengio, Y., 和 David, J.-P. (2015).
    Binaryconnect: 在传播过程中使用二进制权重训练深度神经网络。神经信息处理系统进展，28。'
- en: Dao et al., (2019) Dao, T., Gu, A., Eichhorn, M., Rudra, A., and Ré, C. (2019).
    Learning fast algorithms for linear transforms using butterfly factorizations.
    In International conference on machine learning, pages 1517–1527\. PMLR.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao et al., (2019) Dao, T., Gu, A., Eichhorn, M., Rudra, A., 和 Ré, C. (2019).
    使用蝴蝶分解学习快速算法进行线性变换。发表于国际机器学习会议，页码 1517–1527。PMLR。
- en: 'Dettmers et al., (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,
    L. (2022). Gpt3\. int8 (): 8-bit matrix multiplication for transformers at scale.
    Advances in Neural Information Processing Systems, 35:30318–30332.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等人（2022）Dettmers, T., Lewis, M., Belkada, Y., 和 Zettlemoyer, L.（2022）。Gpt3.int8（）：用于大规模变换器的
    8 位矩阵乘法。神经信息处理系统进展，35:30318–30332。
- en: '(11) Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. (2023a).
    Qlora: Efficient finetuning of quantized llms. Advances in Neural Information
    Processing Systems, 36.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (11) Dettmers, T., Pagnoni, A., Holtzman, A., 和 Zettlemoyer, L.（2023a）。Qlora：量化
    LLM 的高效微调。神经信息处理系统进展，36。
- en: '(12) Dettmers, T., Svirschevski, R. A., Egiazarian, V., Kuznedelev, D., Frantar,
    E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. (2023b). Spqr: A
    sparse-quantized representation for near-lossless llm weight compression. In The
    Twelfth International Conference on Learning Representations.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (12) Dettmers, T., Svirschevski, R. A., Egiazarian, V., Kuznedelev, D., Frantar,
    E., Ashkboos, S., Borzunov, A., Hoefler, T., 和 Alistarh, D.（2023b）。Spqr：一种稀疏量化表示，用于接近无损的
    LLM 权重压缩。在第十二届国际学习表示会议上。
- en: Egiazarian et al., (2024) Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar,
    E., Babenko, A., and Alistarh, D. (2024). Extreme compression of large language
    models via additive quantization. arXiv preprint arXiv:2401.06118.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Egiazarian 等人（2024）Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar, E.,
    Babenko, A., 和 Alistarh, D.（2024）。通过加性量化对大语言模型进行极限压缩。arXiv 预印本 arXiv:2401.06118。
- en: 'Frantar et al., (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. (2022). Gptq: Accurate post-training quantization for generative pre-trained
    transformers. arXiv preprint arXiv:2210.17323.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等人（2022）Frantar, E., Ashkboos, S., Hoefler, T., 和 Alistarh, D.（2022）。Gptq：生成预训练变换器的准确后训练量化。arXiv
    预印本 arXiv:2210.17323。
- en: Gao et al., (2021) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
    A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. (2021). A framework for
    few-shot language model evaluation.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人（2021）Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A.,
    Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., 和 Zou, A.（2021）。少样本语言模型评估框架。
- en: Gholami et al., (2022) Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W.,
    and Keutzer, K. (2022). A survey of quantization methods for efficient neural
    network inference. In Low-Power Computer Vision, pages 291–326\. Chapman and Hall/CRC.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gholami 等人（2022）Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., 和 Keutzer,
    K.（2022）。神经网络推理的量化方法综述。在《低功耗计算机视觉》一书中，第 291–326 页。Chapman and Hall/CRC。
- en: 'Guo et al., (2021) Guo, D., Rush, A. M., and Kim, Y. (2021). Parameter-efficient
    transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers), pages 4884–4896.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人（2021）Guo, D., Rush, A. M., 和 Kim, Y.（2021）。通过差异修剪进行参数高效的迁移学习。在第 59 届计算语言学协会年会上及第
    11 届国际自然语言处理联合会议（卷 1：长篇论文）上，页面 4884–4896。
- en: 'Guo et al., (2023) Guo, H., Greengard, P., Xing, E., and Kim, Y. (2023). Lq-lora:
    Low-rank plus quantized matrix decomposition for efficient language model finetuning.
    In The Twelfth International Conference on Learning Representations.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人（2023）Guo, H., Greengard, P., Xing, E., 和 Kim, Y.（2023）。Lq-lora：用于高效语言模型微调的低秩加量化矩阵分解。在第十二届国际学习表示会议上。
- en: Han et al., (2015) Han, S., Pool, J., Tran, J., and Dally, W. (2015). Learning
    both weights and connections for efficient neural network. Advances in neural
    information processing systems, 28.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等人（2015）Han, S., Pool, J., Tran, J., 和 Dally, W.（2015）。学习权重和连接以提高神经网络的效率。神经信息处理系统进展，28。
- en: 'Hooper et al., (2024) Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W.,
    Shao, Y. S., Keutzer, K., and Gholami, A. (2024). Kvquant: Towards 10 million
    context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hooper 等人（2024）Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W., Shao,
    Y. S., Keutzer, K., 和 Gholami, A.（2024）。Kvquant：通过 kv 缓存量化实现 1000 万上下文长度的 LLM
    推理。arXiv 预印本 arXiv:2401.18079。
- en: 'Hu et al., (2021) Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., Chen, W., et al. (2021). Lora: Low-rank adaptation of large language models.
    In International Conference on Learning Representations.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2021）Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,
    Chen, W., 等人。（2021）。Lora：大语言模型的低秩适应。在国际学习表示会议上。
- en: Jiang et al., (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. (2023). Mistral 7b. arXiv preprint arXiv:2310.06825.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人, (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot,
    D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., 等人.
    (2023). Mistral 7b。arXiv 预印本 arXiv:2310.06825。
- en: (23) Kim, J., Lee, J. H., Kim, S., Park, J., Yoo, K. M., Kwon, S. J., and Lee,
    D. (2023a). Memory-efficient fine-tuning of compressed large language models via
    sub-4-bit integer quantization. arXiv preprint arXiv:2305.14152.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (23) Kim, J., Lee, J. H., Kim, S., Park, J., Yoo, K. M., Kwon, S. J., 和 Lee,
    D. (2023a). 通过 sub-4-bit 整数量化实现压缩大型语言模型的内存高效微调。arXiv 预印本 arXiv:2305.14152。
- en: '(24) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney,
    M. W., and Keutzer, K. (2023b). Squeezellm: Dense-and-sparse quantization. arXiv
    preprint arXiv:2306.07629.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(24) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney,
    M. W., 和 Keutzer, K. (2023b). Squeezellm: 稠密与稀疏量化。arXiv 预印本 arXiv:2306.07629。'
- en: 'Kwan et al., (2024) Kwan, H. M., Gao, G., Zhang, F., Gower, A., and Bull, D.
    (2024). Hinerv: Video compression with hierarchical encoding-based neural representation.
    Advances in Neural Information Processing Systems, 36.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kwan 等人, (2024) Kwan, H. M., Gao, G., Zhang, F., Gower, A., 和 Bull, D. (2024).
    Hinerv: 基于层次编码的神经表示视频压缩。神经信息处理系统进展, 36。'
- en: 'Li and Liang, (2021) Li, X. L. and Liang, P. (2021). Prefix-tuning: Optimizing
    continuous prompts for generation. In Proceedings of the 59th Annual Meeting of
    the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 和 Liang, (2021) Li, X. L. 和 Liang, P. (2021). Prefix-tuning: 为生成优化连续提示。发表于第59届计算语言学协会年会及第11届国际自然语言处理联合会议论文集（第1卷:
    长篇论文）, 页码 4582–4597。'
- en: 'Li et al., (2023) Li, Y., Yu, Y., Liang, C., Karampatziakis, N., He, P., Chen,
    W., and Zhao, T. (2023). Loftq: Lora-fine-tuning-aware quantization for large
    language models. In The Twelfth International Conference on Learning Representations.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人, (2023) Li, Y., Yu, Y., Liang, C., Karampatziakis, N., He, P., Chen,
    W., 和 Zhao, T. (2023). Loftq: 关注 Lora 微调的大型语言模型量化。发表于第十二届国际学习表征会议。'
- en: 'Liao and Monz, (2024) Liao, B. and Monz, C. (2024). Apiq: Finetuning of 2-bit
    quantized large language model. arXiv preprint arXiv:2402.05147.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liao 和 Monz, (2024) Liao, B. 和 Monz, C. (2024). Apiq: 2-bit 量化大型语言模型的微调。arXiv
    预印本 arXiv:2402.05147。'
- en: 'Lin et al., (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han,
    S. (2023). Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人, (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., 和 Han, S. (2023).
    Awq: 激活感知的权重量化用于 LLM 压缩和加速。arXiv 预印本 arXiv:2306.00978。'
- en: 'Liu et al., (2023) Liu, J., Gong, R., Wei, X., Dong, Z., Cai, J., and Zhuang,
    B. (2023). Qllm: Accurate and efficient low-bitwidth quantization for large language
    models. In The Twelfth International Conference on Learning Representations.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人, (2023) Liu, J., Gong, R., Wei, X., Dong, Z., Cai, J., 和 Zhuang, B.
    (2023). Qllm: 准确且高效的大型语言模型低位宽量化。发表于第十二届国际学习表征会议。'
- en: 'Liu et al., (2024) Liu, S.-Y., Wang, C.-Y., Yin, H., Molchanov, P., Wang, Y.-C. F.,
    Cheng, K.-T., and Chen, M.-H. (2024). Dora: Weight-decomposed low-rank adaptation.
    arXiv preprint arXiv:2402.09353.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人, (2024) Liu, S.-Y., Wang, C.-Y., Yin, H., Molchanov, P., Wang, Y.-C. F.,
    Cheng, K.-T., 和 Chen, M.-H. (2024). Dora: 权重分解的低秩适应。arXiv 预印本 arXiv:2402.09353。'
- en: Liu et al., (2022) Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell,
    T., and Xie, S. (2022). A convnet for the 2020s. In Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, pages 11976–11986.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人, (2022) Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T.,
    和 Xie, S. (2022). 2020 年代的卷积网络。发表于 IEEE/CVF 计算机视觉与模式识别会议论文集, 页码 11976–11986。
- en: 'Liu et al., (2020) Liu, Z., Shen, Z., Savvides, M., and Cheng, K.-T. (2020).
    Reactnet: Towards precise binary neural network with generalized activation functions.
    In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part XIV 16, pages 143–159\. Springer.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人, (2020) Liu, Z., Shen, Z., Savvides, M., 和 Cheng, K.-T. (2020). Reactnet:
    朝着具有广义激活函数的精确二进制神经网络迈进。发表于计算机视觉–ECCV 2020: 第十六届欧洲会议, 英国格拉斯哥, 2020年8月23–28日, 论文集,
    第十四部分 16, 页码 143–159。Springer。'
- en: Merity et al., (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. (2016).
    Pointer sentinel mixture models. In International Conference on Learning Representations.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等人, (2016) Merity, S., Xiong, C., Bradbury, J., 和 Socher, R. (2016).
    Pointer sentinel 混合模型。发表于国际学习表征会议。
- en: Nrusimha et al., (2024) Nrusimha, A., Mishra, M., Wang, N., Alistarh, D., Panda,
    R., and Kim, Y. (2024). Mitigating the impact of outlier channels for language
    model quantization with activation regularization. arXiv preprint arXiv:2404.03605.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nrusimha et al., (2024) Nrusimha, A., Mishra, M., Wang, N., Alistarh, D., Panda,
    R., and Kim, Y. (2024). 通过激活正则化减轻语言模型量化的异常通道影响。arXiv 预印本 arXiv:2404.03605。
- en: 'Park et al., (2022) Park, G., Park, B., Kim, M., Lee, S., Kim, J., Kwon, B.,
    Kwon, S. J., Kim, B., Lee, Y., and Lee, D. (2022). Lut-gemm: Quantized matrix
    multiplication based on luts for efficient inference in large-scale generative
    language models. arXiv preprint arXiv:2206.09557.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park et al., (2022) Park, G., Park, B., Kim, M., Lee, S., Kim, J., Kwon, B.,
    Kwon, S. J., Kim, B., Lee, Y., and Lee, D. (2022). Lut-gemm: 基于 LUTs 的量化矩阵乘法，用于大规模生成语言模型的高效推理。arXiv
    预印本 arXiv:2206.09557。'
- en: Raffel et al., (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits
    of transfer learning with a unified text-to-text transformer. Journal of machine
    learning research, 21(140):1–67.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al., (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). 使用统一的文本到文本变换器探索迁移学习的极限。机器学习研究杂志，21(140):1–67。
- en: 'Sakaguchi et al., (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. (2021). Winogrande: An adversarial winograd schema challenge at scale. Communications
    of the ACM, 64(9):99–106.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi et al., (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. (2021). Winogrande: 大规模对抗 Winograd 语法挑战。ACM 通讯，64(9):99–106。'
- en: 'Shao et al., (2023) Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z.,
    Zhang, K., Gao, P., Qiao, Y., and Luo, P. (2023). Omniquant: Omnidirectionally
    calibrated quantization for large language models. In The Twelfth International
    Conference on Learning Representations.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shao et al., (2023) Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z.,
    Zhang, K., Gao, P., Qiao, Y., and Luo, P. (2023). Omniquant: 针对大规模语言模型的全方向校准量化。发表于第十二届国际学习表征会议。'
- en: Shi et al., (2016) Shi, W., Caballero, J., Huszár, F., Totz, J., Aitken, A. P.,
    Bishop, R., Rueckert, D., and Wang, Z. (2016). Real-time single image and video
    super-resolution using an efficient sub-pixel convolutional neural network. In
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 1874–1883.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al., (2016) Shi, W., Caballero, J., Huszár, F., Totz, J., Aitken, A.
    P., Bishop, R., Rueckert, D., and Wang, Z. (2016). 使用高效的子像素卷积神经网络进行实时单图像和视频超分辨率。发表于
    IEEE 计算机视觉与模式识别会议论文集，页码 1874–1883。
- en: Soro et al., (2024) Soro, B., Andreis, B., Lee, H., Chong, S., Hutter, F., and
    Hwang, S. J. (2024). Diffusion-based neural network weights generation. arXiv
    preprint arXiv:2402.18153.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Soro et al., (2024) Soro, B., Andreis, B., Lee, H., Chong, S., Hutter, F., and
    Hwang, S. J. (2024). 基于扩散的神经网络权重生成。arXiv 预印本 arXiv:2402.18153。
- en: 'Tata and Patel, (2003) Tata, S. and Patel, J. M. (2003). Piqa: An algebra for
    querying protein data sets. In 15th International Conference on Scientific and
    Statistical Database Management, 2003., pages 141–150\. IEEE.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tata and Patel, (2003) Tata, S. and Patel, J. M. (2003). Piqa: 用于查询蛋白质数据集的代数。发表于第十五届国际科学与统计数据库管理会议，2003
    年，页码 141–150。IEEE。'
- en: 'Team et al., (2024) Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju,
    S., Pathak, S., Sifre, L., Rivière, M., Kale, M. S., Love, J., et al. (2024).
    Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team et al., (2024) Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju,
    S., Pathak, S., Sifre, L., Rivière, M., Kale, M. S., Love, J., et al. (2024).
    Gemma: 基于双子研究和技术的开放模型。arXiv 预印本 arXiv:2403.08295。'
- en: 'Touvron et al., (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.
    (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al., (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et
    al. (2023). Llama: 开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971。'
- en: Trukhanov and Soloveychik, (2024) Trukhanov, N. and Soloveychik, I. (2024).
    Accurate block quantization in llms with outliers. arXiv preprint arXiv:2403.20137.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trukhanov and Soloveychik, (2024) Trukhanov, N. and Soloveychik, I. (2024).
    带异常值的 llms 精确块量化。arXiv 预印本 arXiv:2403.20137。
- en: 'Tseng et al., (2024) Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and De Sa,
    C. (2024). Quip#: Even better llm quantization with hadamard incoherence and lattice
    codebooks. arXiv preprint arXiv:2402.04396.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tseng et al., (2024) Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and De Sa,
    C. (2024). Quip#: 更佳的 llm 量化，结合 Hadamard 不一致性和格点代码本。arXiv 预印本 arXiv:2402.04396。'
- en: Van Den Oord et al., (2017) Van Den Oord, A., Vinyals, O., et al. (2017). Neural
    discrete representation learning. Advances in neural information processing systems,
    30.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Den Oord等人，（2017）Van Den Oord, A., Vinyals, O., 等人。（2017）。神经离散表示学习。神经信息处理系统进展，30。
- en: Vaswani et al., (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
    Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all
    you need. Advances in neural information processing systems, 30.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani等人，（2017）Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł., 和 Polosukhin, I.（2017）。注意力机制是你所需的一切。神经信息处理系统进展，30。
- en: Viazovska, (2017) Viazovska, M. S. (2017). The sphere packing problem in dimension
    8. Annals of mathematics, pages 991–1015.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Viazovska，（2017）Viazovska, M. S.（2017）。8维球体包装问题。数学年刊，页码 991–1015。
- en: Wang et al., (2024) Wang, K., Xu, Z., Zhou, Y., Zang, Z., Darrell, T., Liu,
    Z., and You, Y. (2024). Neural network diffusion. arXiv preprint arXiv:2402.13144.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人，（2024）Wang, K., Xu, Z., Zhou, Y., Zang, Z., Darrell, T., Liu, Z., 和 You,
    Y.（2024）。神经网络扩散。arXiv预印本 arXiv:2402.13144。
- en: 'Xiao et al., (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and
    Han, S. (2023). Smoothquant: Accurate and efficient post-training quantization
    for large language models. In International Conference on Machine Learning, pages
    38087–38099\. PMLR.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao等人，（2023）Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., 和 Han, S.（2023）。Smoothquant:
    精确且高效的大型语言模型后训练量化。在国际机器学习会议上，页码 38087–38099\. PMLR。'
- en: 'Yao et al., (2022) Yao, Z., Yazdani Aminabadi, R., Zhang, M., Wu, X., Li, C.,
    and He, Y. (2022). Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. Advances in Neural Information Processing Systems,
    35:27168–27183.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao等人，（2022）Yao, Z., Yazdani Aminabadi, R., Zhang, M., Wu, X., Li, C., 和 He,
    Y.（2022）。Zeroquant: 高效且经济的大规模变换器后训练量化。神经信息处理系统进展，35:27168–27183。'
- en: Appendix A Appendix / supplemental material
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录 / 补充材料
- en: A.1 Structures in pre-trained matrices
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 预训练矩阵中的结构
- en: 'Interestingly, the blocks that show some visual structures in LLaMA and Mistral
    models are not the same for Gemma LLMs. For instance in [Figure 4](#A1.F4 "In
    A.1 Structures in pre-trained matrices ‣ Appendix A Appendix / supplemental material
    ‣ ReALLM: A general framework for LLM compression and fine-tuning"), we can see
    that Gemma2b (Team et al.,, [2024](#bib.bib43))’s matrices keep some internal
    patterns in all blocks, not only at the very first blocks. Note this has no negative
    impact on ReALLM, as the shape of the encoder is experimentally adapted to each
    block.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '有趣的是，LLaMA和Mistral模型中展示出一些视觉结构的块，在Gemma LLMs中并不相同。例如，在[图4](#A1.F4 "在 A.1 预训练矩阵的结构
    ‣ 附录 A 附录 / 补充材料 ‣ ReALLM: 一个通用的LLM压缩和微调框架")中，我们可以看到Gemma2b（Team等人，[2024](#bib.bib43)）的矩阵在所有块中保持一些内部模式，而不仅仅是在最初的块中。请注意，这对ReALLM没有负面影响，因为编码器的形状经过实验调整以适应每个块。'
- en: '![Refer to caption](img/bf9fe57040bc062c570f73b31d6925da.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bf9fe57040bc062c570f73b31d6925da.png)'
- en: 'Figure 4: Reconstruction (Frobenius norm) error for layer of type “Q” for all
    blocks of Gemma2b LLM.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Gemma2b LLM中所有块的“Q”类型层的重建（弗罗贝纽斯范数）误差。
- en: A.2 Autoencoder computational limitations
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 自编码器计算限制
- en: Our GPU can not directly work on LLM pre-trained matrices with large sizes (typically
    of shape $4096\times 4096$ bits using straight through estimator Bengio, ([2013](#bib.bib5)).
    We also tested a post training quantization method where the weight of the decoder
    are quantized with a round to nearest (RTN) approache, at the end of the decoder
    training steps.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的GPU无法直接处理大尺寸的LLM预训练矩阵（通常形状为$4096\times 4096$位，使用直接估算器Bengio，（[2013](#bib.bib5)）。我们还测试了一种后训练量化方法，其中解码器的权重在解码器训练步骤结束时用最邻近（RTN）方法进行量化。
- en: 'Table 4: Reconstruction (Frobenius norm) error for layer of type “Q” inside
    the first block of Mistral-7b model, for patches of size $512\times 512$, and
    a varying quantization strategy (during the decoder training, i.e. QAT, or after
    the training, i.e. PTQ).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：Mistral-7b模型中第一块内部“Q”类型层的重建（弗罗贝纽斯范数）误差，对于大小为$512\times 512$的补丁，以及不同的量化策略（在解码器训练期间，即QAT，或训练后，即PTQ）。
- en: '| Error | # parameters c ($\times 10^{6}$ | bit budget | quantization |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 错误 | 参数数量 c ($\times 10^{6}$ | 位预算 | 量化 |'
- en: '| $0.84$ | NF3(Guo et al.,, [2023](#bib.bib18)) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| $0.84$ | NF3（Guo等人，[2023](#bib.bib18)） |'
- en: '| $1.78$ | PTQ |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| $1.78$ | PTQ |'
- en: '| $1.19$ | PTQ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| $1.19$ | PTQ |'
- en: '| $1.61$ | QAT |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| $1.61$ | QAT |'
- en: '| $1.24$ | QAT |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| $1.24$ | QAT |'
- en: '| $0.69$ | QAT |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| $0.69$ | QAT |'
- en: We vary the number of parameters $c$), ReALLM yields a smaller quantization
    error compared to the scalar quantization NF3 ([Dettmers et al., 2023a,](#bib.bib11)
    ; Guo et al.,, [2023](#bib.bib18)).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们改变参数 $c$ 的数量），与标量量化 NF3 相比，ReALLM 产生了更小的量化误差（[Dettmers et al., 2023a,](#bib.bib11)；Guo
    et al., [2023](#bib.bib18)）。
- en: 'Table 5: Comparison of several LLM format for $m$.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：不同 LLM 格式的比较，针对 $m$。
- en: '| Method | LoRA | VQ only (like AQLM) | ReALLM |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LoRA | 仅 VQ（如 AQLM） | ReALLM |'
- en: '| Matrix representation | $(p\times q)\cdot 16$ |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 矩阵表示 | $(p\times q)\cdot 16$ |'
- en: '| Codebook | $-$ |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 码本 | $-$ |'
- en: '| Decoder | $-$ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 解码器 | $-$ |'
- en: '| Low-rank | $(2\times r\times\min(p,q))\cdot 16$ |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 低秩 | $(2\times r\times\min(p,q))\cdot 16$ |'
- en: '| Total bit cost | $16(pq+2r\min(p,q))\cdot m$ |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 总位成本 | $16(pq+2r\min(p,q))\cdot m$ |'
- en: 'Table 6: Quantization and fine-tuning approaches as particular case of ReALLM (with
    a rank $r$.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：量化和微调方法作为 ReALLM 的特例（具有秩 $r$）。
- en: '| Method | rank $r$ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 秩 $r$ |'
- en: '| LoRA (Hu et al.,, [2021](#bib.bib21)) | $64$ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| LoRA (Hu et al., [2021](#bib.bib21)) | $64$ |'
- en: '| GPTQ (Frantar et al.,, [2022](#bib.bib14)) | $0$ |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ (Frantar et al., [2022](#bib.bib14)) | $0$ |'
- en: '| QLoRA ([Dettmers et al., 2023a,](#bib.bib11) ) | $64$ |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| QLoRA ([Dettmers et al., 2023a,](#bib.bib11)) | $64$ |'
- en: '| LQ-LoRA (Guo et al.,, [2023](#bib.bib18)) | $64$ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| LQ-LoRA (Guo et al., [2023](#bib.bib18)) | $64$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | $0$ |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Quip# (Tseng et al., [2024](#bib.bib46)) | $0$ |'
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | $0$ |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| AQLM (Egiazarian et al., [2024](#bib.bib13)) | $0$ |'
- en: '| ReALLM | $64$ |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | $64$ |'
- en: A.3 Permutations
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 排列
- en: In ReALLM, we compute permutations on sets of vectors in dimension $128$. We
    could work with smaller blocks, but it induces more memory dedicated to the permutation
    storage (one permutation for each block).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ReALLM 中，我们计算维度为 $128$ 的向量集上的排列。我们可以使用更小的块，但这会导致更多内存用于存储排列（每个块一个排列）。
- en: 'We start from the first vector (i.e. the first column of the initial matrix
    shrunk to a dimension $d=128$ vectors. The process is then iterated. Details are
    given in [Algorithm 1](#algorithm1 "In Quantization pre-processing. ‣ 3 Method
    ‣ ReALLM: A general framework for LLM compression and fine-tuning").'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从第一个向量开始（即将初始矩阵的第一列缩小到维度 $d=128$ 的向量）。然后重复这一过程。详细信息见 [算法 1](#algorithm1 "在量化预处理。
    ‣ 3 方法 ‣ ReALLM: LLM 压缩和微调的通用框架")。'
- en: A.4 Broader impacts and Safeguards
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 更广泛的影响与保障措施
- en: Our computing unit seriously restricts the size of the decoder models we can
    train. We are not able to train one decoder model for a given LLM, but we work
    layer-wise and train a single decoder model for all patches extracted from the
    given layer. This layer-wise training forms the main limitation of ReALLM w.r.t. standard
    post-training quantization methods, such as round to nearest (RTN).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的计算单元严重限制了我们可以训练的解码器模型的大小。我们无法为给定的 LLM 训练一个解码器模型，但我们逐层工作，为从给定层提取的所有补丁训练一个单一的解码器模型。这种逐层训练是
    ReALLM 相对于标准后训练量化方法（如四舍五入到最近（RTN））的主要限制。
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示的工作旨在推动机器学习领域的发展。我们的工作可能带来许多潜在的社会影响，但我们认为在此不需要特别强调。
- en: 'Table 7: Perplexity $(\downarrow)$'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：困惑度 $(\downarrow)$
- en: '| Method | #bits | rank $r$ |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | #bits | 秩 $r$ |'
- en: '| ReALLM (no fine-tuning) | $3$ |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM（无微调） | $3$ |'
- en: '| ReALLM (30% training) | $3$ |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (30% 训练) | $3$ |'
- en: '| ReALLM (no fine-tuning) | $2$ |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM（无微调） | $2$ |'
- en: '| ReALLM (10% training) | $2$ |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (10% 训练) | $2$ |'
