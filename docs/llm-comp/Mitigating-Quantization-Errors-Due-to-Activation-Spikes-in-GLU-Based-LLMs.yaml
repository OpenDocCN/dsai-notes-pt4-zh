- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:49:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:29
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓解基于GLU的大型语言模型中的激活峰值导致的量化误差
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14428](https://ar5iv.labs.arxiv.org/html/2405.14428)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14428](https://ar5iv.labs.arxiv.org/html/2405.14428)
- en: Jaewoo Yang, Hayun Kim, Younghoon Kim
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 杨宰宇，金海云，金永勋
- en: Department of Applied Artificial Intelligence, Hanyang University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 汉阳大学应用人工智能系
- en: '{onnoo, lin5478, nongaussian}@hanyang.ac.kr Corresponding Author.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{onnoo, lin5478, nongaussian}@hanyang.ac.kr 通讯作者。'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Modern large language models (LLMs) have established state-of-the-art performance
    through architectural improvements, but still require significant computational
    cost for inference. In an effort to reduce the inference cost, post-training quantization
    (PTQ) has become a popular approach, quantizing weights and activations to lower
    precision, such as INT8. In this paper, we reveal the challenges of activation
    quantization in GLU variants [[40](#bib.bib40)], which are widely used in feed-forward
    network (FFN) of modern LLMs, such as LLaMA family. The problem is that severe
    local quantization errors, caused by excessive magnitudes of activation in GLU
    variants, significantly degrade the performance of the quantized LLM. We denote
    these activations as activation spikes. Our further observations provide a systematic
    pattern of activation spikes: 1) The activation spikes occur in the FFN of specific
    layers, particularly in the early and late layers, 2) The activation spikes are
    dedicated to a couple of tokens, rather than being shared across a sequence. Based
    on our observations, we propose two empirical methods, Quantization-free Module
    (QFeM) and Quantization-free Prefix (QFeP), to isolate the activation spikes during
    quantization. Our extensive experiments validate the effectiveness of the proposed
    methods for the activation quantization, especially with coarse-grained scheme,
    of latest LLMs with GLU variants, including LLaMA-2/3, Mistral, Mixtral, SOLAR,
    and Gemma. In particular, our methods enhance the current alleviation techniques
    (e.g., SmoothQuant) that fail to control the activation spikes.¹¹1Code is available
    at [https://github.com/onnoo/activation-spikes](https://github.com/onnoo/activation-spikes).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现代的大型语言模型（LLMs）通过架构改进建立了最先进的性能，但仍然需要大量的计算成本进行推理。为了减少推理成本，后训练量化（PTQ）已成为一种流行的方法，将权重和激活量化为更低的精度，例如INT8。本文揭示了在广泛用于现代LLMs的前馈网络（FFN）中的GLU变体[[40](#bib.bib40)]中，激活量化面临的挑战。问题在于，由于GLU变体中激活值的过大幅度导致的严重局部量化误差，显著降低了量化LLM的性能。我们将这些激活值称为激活峰值。我们的进一步观察提供了激活峰值的系统模式：1）激活峰值发生在特定层的FFN中，特别是在早期和晚期层；2）激活峰值集中在几个特定的标记上，而不是在整个序列中共享。基于我们的观察，我们提出了两种经验方法：无量化模块（QFeM）和无量化前缀（QFeP），以在量化过程中隔离激活峰值。我们广泛的实验验证了所提方法在激活量化中的有效性，特别是在最新的GLU变体LLMs（包括LLaMA-2/3、Mistral、Mixtral、SOLAR和Gemma）的粗粒度方案中。特别是，我们的方法增强了当前的缓解技术（例如SmoothQuant），这类技术未能控制激活峰值。¹¹1代码可在
    [https://github.com/onnoo/activation-spikes](https://github.com/onnoo/activation-spikes)
    找到。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Large language models (LLMs) have become a key paradigm in natural language
    processing, accelerating the release of variations within the community [[58](#bib.bib58),
    [49](#bib.bib49)]. Furthermore, latest LLMs establish state-of-the-art performance
    by training with increased scale, as well as by adopting architectural improvements
    such as GLU [[40](#bib.bib40)], RoPE [[41](#bib.bib41)], GQA [[2](#bib.bib2)],
    and MoE [[21](#bib.bib21)]. Especially, GLU (Gated Linear Unit) variants (e.g.,
    SwiGLU, GeGLU) has been adopted in the most of modern LLM architectures (e.g.,
    LLaMA family [[46](#bib.bib46)]), due to training efficiency [[40](#bib.bib40),
    [31](#bib.bib31)]. Although LLMs broaden foundational capabilities in natural
    language tasks and potential for various applications, billions of parameters
    in the large models impose considerable computational costs on end users in practice.
    To reduce GPU memory requirements and accelerate inference speed, post-training
    quantization (PTQ) offers an affordable solution by quantizing weights and activations
    into a lower precision (e.g., INT8) without a need for expensive retraining steps
    [[19](#bib.bib19), [17](#bib.bib17), [30](#bib.bib30)]. However, recent studies
    have revealed that large magnitude values at certain coordinates exist in the
    activations of LLMs, which are often called outliers, posing a key challenge in
    activation quantization [[12](#bib.bib12), [51](#bib.bib51), [1](#bib.bib1), [50](#bib.bib50)].
    Another line of works attempts to explain the role of outlier values in the attention
    mechanism [[9](#bib.bib9), [42](#bib.bib42)]. Nevertheless, current research on
    the impact of evolving LLM architectures on the outliers remains insufficient.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已成为自然语言处理中的关键范式，加速了社区内各种变体的发布[[58](#bib.bib58), [49](#bib.bib49)]。此外，最新的LLMs通过增加训练规模以及采用诸如GLU
    [[40](#bib.bib40)]、RoPE [[41](#bib.bib41)]、GQA [[2](#bib.bib2)]和MoE [[21](#bib.bib21)]等架构改进，建立了最先进的性能。特别是，由于训练效率[[40](#bib.bib40),
    [31](#bib.bib31)]，GLU（门控线性单元）变体（例如，SwiGLU、GeGLU）已被大多数现代LLM架构（例如，LLaMA家族[[46](#bib.bib46)]）采用。尽管LLMs拓宽了自然语言任务中的基础能力并具有各种应用潜力，但大型模型中的数十亿个参数在实践中对最终用户施加了相当大的计算成本。为了减少GPU内存需求并加快推理速度，训练后量化（PTQ）通过将权重和激活量化为较低精度（例如，INT8）提供了一种经济实惠的解决方案，而无需昂贵的重新训练步骤[[19](#bib.bib19),
    [17](#bib.bib17), [30](#bib.bib30)]。然而，最近的研究表明，LLMs的激活值在某些坐标处存在大幅度的值，这些值通常被称为离群值，对激活量化构成了主要挑战[[12](#bib.bib12),
    [51](#bib.bib51), [1](#bib.bib1), [50](#bib.bib50)]。另一系列工作尝试解释离群值在注意力机制中的作用[[9](#bib.bib9),
    [42](#bib.bib42)]。尽管如此，目前对演变中的LLM架构对离群值影响的研究仍然不够充分。
- en: 'In this paper, we present our discovery that the GLU architecture in the feed-forward
    network (FFN) generates excessively large activation values, which are responsible
    for significant local quantization errors. Specifically, we observe that these
    problematic activation values occur in specific linear layers and are dedicated
    to a couple of tokens, which will be discussed in Section [3](#S3 "3 Activation
    Spikes: Excessive Magnitude of GLU Activations ‣ Mitigating Quantization Errors
    Due to Activation Spikes in GLU-Based LLMs"). To distinguish the excessive GLU
    activations from the outliers, we refer to them as activation spikes. In light
    of our observations, we propose two empirical methods to mitigate the impact of
    activation spikes on quantization: Quantization-free Module (QFeM) and Quantization-free
    Prefix (QFeP). QFeM aims to partially exclude quantization for linear layers (or
    modules) where large quantization errors occur, instead of quantizing the entire
    linear modules in the LLM. By scoring the extent of scale disparity, QFeM selects
    linear modules to exclude. On the other hand, QFeP identifies the prefix that
    triggers activation spikes and preserves its context as a key-value (KV) cache,
    thereby preventing the recurrence of activation spikes in subsequent tokens. It
    is noteworthy that both QFeM and QFeP rely on calibration results to capture activation
    spikes in advance, without any modifications to the target LLM. This indicates
    that our methods can be integrated into any existing quantization methods.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们展示了在前馈网络（FFN）中GLU架构生成过大激活值的发现，这些过大激活值导致了显著的局部量化误差。具体而言，我们观察到这些问题激活值发生在特定的线性层，并且专门与几个令牌相关，详细讨论将在第[3](#S3
    "3 Activation Spikes: Excessive Magnitude of GLU Activations ‣ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs")节中进行。为了区分过量GLU激活值与异常值，我们将其称为激活峰值。根据我们的观察，我们提出了两种经验方法来减轻激活峰值对量化的影响：量化免模块（QFeM）和量化免前缀（QFeP）。QFeM旨在部分排除发生大量化误差的线性层（或模块）的量化，而不是对LLM中的整个线性模块进行量化。通过评分尺度差异的程度，QFeM选择线性模块进行排除。另一方面，QFeP识别触发激活峰值的前缀，并将其上下文保留为键值（KV）缓存，从而防止在后续令牌中激活峰值的再现。值得注意的是，QFeM和QFeP都依赖于校准结果提前捕捉激活峰值，而无需对目标LLM进行任何修改。这表明我们的方法可以集成到任何现有的量化方法中。'
- en: In our comprehensive experiments, we demonstrate that recently released LLMs
    incorporating GLU variants struggle with activation spikes when applying activation
    quantization. Consequently, the proposed methods, QFeM and QFeP, substantially
    enhance the performance of the primitive quantization method, the round-to-nearest
    (RTN) method. Furthermore, we observe that current outlier alleviation methods
    [[51](#bib.bib51), [50](#bib.bib50)] are exposed to the activation spikes and
    benefit from our proposed methods. Compared to the strong baseline of fine-grained
    activation quantization [[55](#bib.bib55)], our methods show competitive performance,
    achieving reduced latency and memory footprint.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的全面实验中，我们展示了最近发布的包含GLU变体的LLM在应用激活量化时存在激活峰值问题。因此，提出的方法QFeM和QFeP显著提高了原始量化方法，即最接近（RTN）方法的性能。此外，我们观察到当前的异常值缓解方法[[51](#bib.bib51),
    [50](#bib.bib50)]暴露于激活峰值，并从我们提出的方法中受益。与细粒度激活量化的强基准[[55](#bib.bib55)]相比，我们的方法表现出具有竞争力的性能，实现了延迟和内存占用的减少。
- en: 'In summary, the contributions of our work are as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们工作的贡献如下：
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We find that the GLU architecture in modern LLMs systematically generates excessive
    activation values, which are responsible for significant performance degradation
    in activation quantization.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们发现现代LLM中的GLU架构系统性地生成过多的激活值，这些激活值导致了激活量化性能的显著下降。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Based on our observations, we propose two empirical methods, QFeM and QFeP,
    which effectively exclude the activation spikes during quantization, with negligible
    computational overhead and compatibility with any existing quantization techniques.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据我们的观察，我们提出了两种经验方法，QFeM 和 QFeP，这些方法有效地排除了量化过程中的激活峰值，计算开销微乎其微，并且与现有的量化技术兼容。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our extensive experimental results validate the detrimental impact of the activation
    spikes on activation quantization, while our proposed methods consistently enhance
    the quantization performance.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的大量实验结果验证了激活峰值对激活量化的有害影响，而我们提出的方法则一致地提高了量化性能。
- en: 2 Related Works
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Outlier Values in LLMs.
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM中的异常值。
- en: Previously, outlier values have been observed in the transformer-based language
    models such as BERT [[14](#bib.bib14)] and early GPT [[36](#bib.bib36)] models
    through numerous studies [[24](#bib.bib24), [8](#bib.bib8), [27](#bib.bib27),
    [45](#bib.bib45), [35](#bib.bib35)]. Since the advent of LLMs [[10](#bib.bib10),
    [57](#bib.bib57)] rooted in the GPT, recent studies by [[12](#bib.bib12), [51](#bib.bib51),
    [1](#bib.bib1)] have tackled the existence of outlier values in LLMs. According
    to them, these outliers exhibit a large magnitude of values at the shared dimensions
    of hidden states across tokens. More recently, [[9](#bib.bib9), [42](#bib.bib42)]
    explain that the outliers attribute to the vertical pattern in the attention mechanism
    [[52](#bib.bib52), [25](#bib.bib25)], which influences the performance of LLMs.
    In particular, [[42](#bib.bib42)] claims a different type of outlier existing
    in the hidden states of specific tokens. However, prior studies merely focus on
    the superficial hidden states between the decoder layers. Our work provides a
    module-level investigation where quantization is applied practically, focusing
    on different LLM architectures.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，通过大量研究发现了变压器基础语言模型（如BERT [[14](#bib.bib14)] 和早期 GPT [[36](#bib.bib36)] 模型）中的异常值
    [[24](#bib.bib24), [8](#bib.bib8), [27](#bib.bib27), [45](#bib.bib45), [35](#bib.bib35)]。自从根植于
    GPT 的 LLMs [[10](#bib.bib10), [57](#bib.bib57)] 出现以来，[[12](#bib.bib12), [51](#bib.bib51),
    [1](#bib.bib1)] 的近期研究已探讨了 LLMs 中异常值的存在。据他们所述，这些异常值在跨 token 的隐藏状态共享维度上表现出较大的值幅度。最近，[[9](#bib.bib9),
    [42](#bib.bib42)] 解释了这些异常值归因于注意力机制中的垂直模式 [[52](#bib.bib52), [25](#bib.bib25)]，这种模式影响了
    LLMs 的性能。特别是，[[42](#bib.bib42)] 声称在特定 token 的隐藏状态中存在不同类型的异常值。然而，以往的研究仅关注解码器层之间的表面隐藏状态。我们的工作提供了一个模块级别的调查，在实际应用中使用了量化，重点关注不同的
    LLM 架构。
- en: Post-training Quantization for LLMs.
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs 的后训练量化。
- en: Post-training quantization (PTQ) refers to the quantization of a neural network
    model to low precision, such as INT8, without additional parameter updates [[19](#bib.bib19),
    [17](#bib.bib17)]. Especially for LLMs, this approach cost-effectively achieves
    inference with low memory usage and faster inference latency by quantizing the
    weights and activations used in matrix multiplication (e.g., linear layer). However,
    because of the challenges in activation quantization of LLMs, many recent works
    are mainly focused on the weight-only quantization [[15](#bib.bib15), [23](#bib.bib23),
    [26](#bib.bib26), [11](#bib.bib11), [54](#bib.bib54), [13](#bib.bib13), [39](#bib.bib39)].
    Otherwise, the activation quantization faces inherent outliers, which hinder accurate
    quantization by reducing representation resolution. To address this challenge,
    [[12](#bib.bib12)] proposes a mixed-precision quantization method where the outlier
    dimensions are computed in high precision. [[51](#bib.bib51), [50](#bib.bib50)]
    approach migration of scale from activation to weights to alleviate the scale
    of outlier activations. Along this line of research, we propose to enhance the
    activation quantization based on our observations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化（PTQ）指将神经网络模型量化为低精度，如 INT8，而无需额外的参数更新 [[19](#bib.bib19), [17](#bib.bib17)]。特别是对于
    LLMs，这种方法通过量化在矩阵乘法（例如线性层）中使用的权重和激活，成本效益高地实现了低内存使用和更快的推理延迟。然而，由于 LLMs 的激活量化面临的挑战，许多近期的工作主要集中在仅量化权重
    [[15](#bib.bib15), [23](#bib.bib23), [26](#bib.bib26), [11](#bib.bib11), [54](#bib.bib54),
    [13](#bib.bib13), [39](#bib.bib39)]。否则，激活量化面临固有的异常值，这会通过降低表示分辨率来阻碍准确的量化。为了解决这一挑战，[[12](#bib.bib12)]
    提出了混合精度量化方法，其中异常维度以高精度计算。[[51](#bib.bib51), [50](#bib.bib50)] 采取了将缩放从激活迁移到权重的方法，以减轻异常激活的缩放。在这方面的研究中，我们建议基于我们的观察来增强激活量化。
- en: '3 Activation Spikes: Excessive Magnitude of GLU Activations'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 激活尖峰：GLU 激活的过度幅度
- en: 'For clarity, "hidden states" refer to the output tensor of a transformer layer
    (or block), while "input activations" or "activations" denote the input tensor
    of a linear layer (or module) in the remain of this paper. Recent work [[42](#bib.bib42)]
    has investigated a novel type of outlier existing in the hidden states across
    modern LLMs. Although these outliers of hidden states play a crucial role in the
    attention mechanism [[42](#bib.bib42), [9](#bib.bib9), [52](#bib.bib52)], their
    relationship with input activations for quantization has not been fully explored.
    Importantly, because recent LLMs adopt Pre-LN [[53](#bib.bib53), [4](#bib.bib4)],
    which normalizes hidden states before self-attention and feed-forward network
    (FFN) blocks, the scale of hidden states does not reflect the scale of input activations
    within the transformer block. Therefore, we focus on the input activations fed
    into each linear module within the transformer block to connect to activation
    quantization. Specifically, we examine the four linear (projection) layers: query
    (parallel to key and value), out, up (parallel to gate), and down modules. For
    detailed illustration of Pre-LN transformer, please see Appendix [D.1](#A4.SS1
    "D.1 Transformer Architecture. ‣ Appendix D Miscellaneous ‣ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清楚起见，“隐藏状态”指的是变换器层（或模块）的输出张量，而“输入激活”或“激活”在本文其余部分指的是线性层（或模块）的输入张量。近期的工作 [[42](#bib.bib42)]
    研究了现代LLMs中存在的一种新型异常值。尽管这些隐藏状态的异常值在注意机制中发挥了重要作用 [[42](#bib.bib42), [9](#bib.bib9),
    [52](#bib.bib52)]，但它们与输入激活的量化关系尚未完全探讨。重要的是，由于近期LLMs采用了Pre-LN [[53](#bib.bib53),
    [4](#bib.bib4)]，该方法在自注意力和前馈网络（FFN）块之前对隐藏状态进行归一化，因此隐藏状态的尺度无法反映变换器块内输入激活的尺度。因此，我们关注输入激活，连接到变换器块内的每个线性模块的激活量化。具体来说，我们检查了四个线性（投影）层：查询（与键和值并行）、输出、向上（与门并行）和向下模块。有关Pre-LN变换器的详细说明，请参见附录 [D.1](#A4.SS1
    "D.1 变换器架构 ‣ 附录D 杂项 ‣ 减少因GLU基础LLMs中的激活峰值导致的量化误差")。
- en: '![Refer to caption](img/5b194f12a0e814a168c7fa3ccfcd2320.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5b194f12a0e814a168c7fa3ccfcd2320.png)'
- en: 'Figure 1: Calibration results on GLU-implemented and non GLU-implemented LLMs.
    We present the maximum magnitudes of input activations for each linear modules
    and layer-wise hidden states. For more results on different LLMs, see Appendix [A.2](#A1.SS2
    "A.2 Other Calibration Results on GLU-implementation ‣ Appendix A Additional Calibration
    Results ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs"), [A.3](#A1.SS3 "A.3 Other Calibration Results on Non GLU-implementation
    ‣ Appendix A Additional Calibration Results ‣ Mitigating Quantization Errors Due
    to Activation Spikes in GLU-Based LLMs").'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：GLU实现和非GLU实现LLMs的校准结果。我们展示了每个线性模块的输入激活的最大幅度以及按层次划分的隐藏状态。有关不同LLMs的更多结果，请参见附录 [A.2](#A1.SS2
    "A.2 GLU实现的其他校准结果 ‣ 附录A 额外的校准结果 ‣ 减少因GLU基础LLMs中的激活峰值导致的量化误差")，[A.3](#A1.SS3 "A.3
    非GLU实现的其他校准结果 ‣ 附录A 额外的校准结果 ‣ 减少因GLU基础LLMs中的激活峰值导致的量化误差")。
- en: 3.1 Existence of Activation Spikes in GLU Variants
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 GLU变体中激活峰值的存在
- en: To analyze the input activations, we employ a calibration method, which is used
    to estimate the quantization factors such as scale and zero-point. For the calibration
    data, we use 512 samples randomly collected from the C4 [[37](#bib.bib37)] training
    dataset. Afterwards, we feed each sample into the LLM and monitor each hidden
    state and input activation through the decoder layers. To estimate the scale factor,
    we use absolute maximum value. The tested LLMs are listed in Appendix [A.1](#A1.SS1
    "A.1 Detailed Specification of LLMs ‣ Appendix A Additional Calibration Results
    ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs").
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析输入激活，我们采用了一种校准方法，该方法用于估计量化因子，如尺度和零点。对于校准数据，我们使用了从C4 [[37](#bib.bib37)] 训练数据集中随机收集的512个样本。随后，我们将每个样本输入到LLM中，并通过解码器层监控每个隐藏状态和输入激活。为了估计尺度因子，我们使用绝对最大值。测试过的LLMs列在附录 [A.1](#A1.SS1
    "A.1 详细的LLMs规范 ‣ 附录A 额外的校准结果 ‣ 减少因GLU基础LLMs中的激活峰值导致的量化误差")中。
- en: GLU-implemented LLMs exhibit activation spikes at specific layers.
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现了GLU的LLMs在特定层次上表现出激活峰值。
- en: 'In Figure [1a](#S3.F1 "Figure 1 ‣ 3 Activation Spikes: Excessive Magnitude
    of GLU Activations ‣ Mitigating Quantization Errors Due to Activation Spikes in
    GLU-Based LLMs"), we display the calibrated scale factors for the LLMs that implement
    GLU variants (e.g., SwiGLU, GeGLU). Across models, we observe a shared pattern
    of scale from the results. Within the early and late layers, the down modules
    in the FFN show noticeable magnitudes of input activations. Note that these input
    activations are derived from the Hadamard Product within GLU. Thus, the GLU variants
    generate activation spikes at the specific layers. Interestingly, we notice a
    high correlation between the emergence of activation spikes and intermediate hidden
    states of large scale. This indicates that the FFN contributes to amplifying the
    hidden states via the addition operation in the residual connection [[18](#bib.bib18)].
    Once the magnitude of the hidden states is exploded, it persists through layers
    until encounter the activation spikes at late layers.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [1a](#S3.F1 "图 1 ‣ 3 激活尖峰：GLU 激活的过度幅度 ‣ 缓解 GLU 基于 LLM 的激活尖峰导致的量化误差") 中，我们展示了实施
    GLU 变体（例如 SwiGLU、GeGLU）的 LLM 的校准尺度因子。我们观察到，各模型之间有共享的规模模式。在早期和晚期层中，FFN 中的下游模块显示出明显的输入激活幅度。注意，这些输入激活来源于
    GLU 中的 Hadamard 乘积。因此，GLU 变体在特定层中产生激活尖峰。有趣的是，我们注意到激活尖峰的出现与大规模的中间隐藏状态之间存在高度相关性。这表明
    FFN 通过残差连接中的加法操作有助于放大隐藏状态 [[18](#bib.bib18)]。一旦隐藏状态的幅度爆炸，它会在层间持续，直到遇到晚期层的激活尖峰。
- en: Non GLU-implemented LLMs show modest scale distribution.
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非 GLU 实现的 LLM 显示出适度的规模分布。
- en: 'Figure [1b](#S3.F1 "Figure 1 ‣ 3 Activation Spikes: Excessive Magnitude of
    GLU Activations ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs") illustrates the calibration results for LLMs with the original feed-forward
    implementation in Transformer [[48](#bib.bib48)]. We observe that the LLMs continue
    to generate the large-scale hidden states, regardless of the GLU implementation.
    This corresponds to the observations in [[42](#bib.bib42)]. More importantly,
    our module-level results elaborate that the scale of hidden states is not transferable
    to the input activations of inner linear modules. Instead, we reveal that GLU
    variants are associated with the hidden states and generate activation spikes.
    This clarifies the quantization challenge of the GLU-implemented LLMs concentrated
    in the early and late layers. Because excessive scales of activation spikes have
    the potential to hinder the accurate quantization, we conduct an in-depth analysis
    to better understand these activation spikes in the following sections.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1b](#S3.F1 "图 1 ‣ 3 激活尖峰：GLU 激活的过度幅度 ‣ 缓解 GLU 基于 LLM 的激活尖峰导致的量化误差") 展示了使用
    Transformer [[48](#bib.bib48)] 中原始前馈实现的 LLM 的校准结果。我们观察到，无论 GLU 实现如何，LLM 继续生成大规模的隐藏状态。这与
    [[42](#bib.bib42)] 中的观察结果相符。更重要的是，我们的模块级结果阐明了隐藏状态的规模无法转移到内部线性模块的输入激活上。相反，我们揭示了
    GLU 变体与隐藏状态相关，并产生激活尖峰。这澄清了 GLU 实现的 LLM 中量化挑战主要集中在早期和晚期层。由于激活尖峰的过度规模可能会妨碍准确量化，我们将在接下来的部分进行深入分析，以更好地理解这些激活尖峰。
- en: 3.2 Token-level Scale Analysis within Activation Spikes
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 激活尖峰中的令牌级别规模分析
- en: 'In the previous section, we observed the excessive scale of the input activations
    derived from GLU activation. When quantizing the input activations, the variance
    of input activation scales for each token affects the quantization performance
    [[55](#bib.bib55)]. To delve into the disparity between token-wise scales in the
    activation spikes, we unroll them through the sequence of tokens. Figure [2](#S3.F2
    "Figure 2 ‣ 3.2 Token-level Scale Analysis within Activation Spikes ‣ 3 Activation
    Spikes: Excessive Magnitude of GLU Activations ‣ Mitigating Quantization Errors
    Due to Activation Spikes in GLU-Based LLMs") illustrates the individual input
    activation scales where the activation spike appears. Given a token sequence,
    the large magnitudes of input activations are observed in a couple of tokens,
    such as the BOS token, newline (\n), and apostrophe (''). These specific tokens
    coincide with the observations of [[42](#bib.bib42)], which suggests that such
    tokens exhibit massive values in the hidden states. Thus, the activation spike
    is associated with the process of assigning a special role to these tokens in
    later transformer layers. However, the excessive scale of specific token hinders
    the estimation of scale factor for the other tokens, such as in per-tensor quantization.
    Additionally, the largest scale is dedicated to the first instance of the specified
    token, while the following usage exhibits a modest scale. This phenomenon makes
    the quantization more complicated, as the activation spikes dynamically occur
    depending on the current input sequence.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们观察到来自GLU激活的输入激活的规模过大。当量化输入激活时，每个标记的输入激活规模的方差会影响量化性能 [[55](#bib.bib55)]。为了深入了解激活峰值中标记间规模的差异，我们通过标记序列展开它们。图
    [2](#S3.F2 "图 2 ‣ 3.2 激活峰值中的标记级别规模分析 ‣ 3 激活峰值：GLU 激活的过大幅度 ‣ 减轻由于 GLU 基于 LLM 的激活峰值导致的量化误差")
    说明了激活峰值出现的个别输入激活规模。给定一个标记序列，会观察到某些标记的输入激活幅度较大，例如 BOS 标记、新行（\n）和撇号（'）。这些特定标记与 [[42](#bib.bib42)]
    的观察一致，表明这些标记在隐藏状态中表现出巨大的值。因此，激活峰值与后续变换器层中为这些标记分配特殊角色的过程相关。然而，特定标记的过大规模会阻碍对其他标记规模因子的估计，例如在每张量量化中。此外，最大的规模专用于指定标记的第一次实例，而随后的使用表现出适度的规模。这种现象使得量化变得更加复杂，因为激活峰值会根据当前输入序列动态发生。
- en: '![Refer to caption](img/a814317a5c3e527c90a69e3e916569ac.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a814317a5c3e527c90a69e3e916569ac.png)'
- en: 'Figure 2: Token-wise scales in a specific layer with an activation spike. When
    quantizing the input activations using a per-tensor scale, the scale of the activation
    spike dominates the scales of the other tokens. For more examples, see Appendix [D.2](#A4.SS2
    "D.2 Additional Results for Token-level Scale Analysis ‣ Appendix D Miscellaneous
    ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：激活峰值中某一特定层的标记级别规模。使用每张量尺度进行输入激活量化时，激活峰值的规模主导了其他标记的规模。更多示例，请参见附录 [D.2](#A4.SS2
    "D.2 标记级别规模分析的附加结果 ‣ 附录 D 杂项 ‣ 减轻由于 GLU 基于 LLM 的激活峰值导致的量化误差")。
- en: 'Table 1: Perplexity and MSE of partial activation quantization of LLMs'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：部分激活量化的困惑度和均方误差
- en: '| Model | Perplexity($\downarrow$) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 困惑度($\downarrow$) |'
- en: '| --- | --- | --- |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| FP16 | Top 4 | Middle 4 | Bottom 4 | Top 4 | Middle 4 | Bottom 4 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 前 4 | 中间 4 | 后 4 | 前 4 | 中间 4 | 后 4 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-2-7B | 7.37 | 11.77 | 7.38 | 7.40 | 1908.80 | 1.03 | 12.90 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | 7.37 | 11.77 | 7.38 | 7.40 | 1908.80 | 1.03 | 12.90 |'
- en: '| LLaMA-2-13B | 6.84 | 15.09 | 6.84 | 6.84 | 4762.11 | 0.91 | 10.38 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | 6.84 | 15.09 | 6.84 | 6.84 | 4762.11 | 0.91 | 10.38 |'
- en: '| Mistral-7B | 8.35 | 69.45 | 8.35 | 8.36 | 218.60 | 0.02 | 0.18 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 8.35 | 69.45 | 8.35 | 8.36 | 218.60 | 0.02 | 0.18 |'
- en: '| Gemma-7B | 10.85 | 85.83 | 10.94 | 10.87 | 213.93 | 1.60 | 1.07 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7B | 10.85 | 85.83 | 10.94 | 10.87 | 213.93 | 1.60 | 1.07 |'
- en: 3.3 Effect of Quantization on Activation Spikes
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 量化对激活峰值的影响
- en: 'We explore the impact of local quantization errors caused by activation spikes
    on LLM outputs. To identify the layers where activation spikes occur, we utilize
    a ratio between the maximum and median values of the token-wise input activation
    scales, instead of using the maximum scale value alone. The max-median ratio for
    linear layer $m$. This max-median ratio captures the extent to which maximum scale
    dominate the other token scales. For comparison, we choose the activation quantization
    targets as the top-4, middle-4, and bottom-4 modules, based on the max-median
    ratio in descending order. Then, we evaluate the perplexity and mean-squared error
    (MSE) using the calibration dataset. Here, the MSE is calculated for the last
    hidden states between the original (FP16) and partially quantized LLM. As shown
    in Table [1](#S3.T1 "Table 1 ‣ 3.2 Token-level Scale Analysis within Activation
    Spikes ‣ 3 Activation Spikes: Excessive Magnitude of GLU Activations ‣ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs"), quantization
    on the top-4 rated modules solely degrades the LLM performance by significant
    margins, while the other cases exhibit negligible performance changes. We consider
    these quantization-sensitive input activations (inter alia activation spikes)
    to be the quantization bottleneck, which, in this paper, refers to the quantization
    error caused by outliers.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '我们探讨了由激活脉冲引起的局部量化误差对 LLM 输出的影响。为了识别激活脉冲发生的层，我们利用令牌级输入激活尺度的最大值和中位值之间的比率，而不是单独使用最大尺度值。线性层的
    max-median 比率 $m$。这个 max-median 比率捕捉了最大尺度在其他令牌尺度中的主导程度。为了比较，我们选择了基于 max-median
    比率的前四、中四和后四模块作为激活量化目标。然后，我们使用校准数据集评估困惑度和均方误差 (MSE)。这里，MSE 是计算原始 (FP16) 和部分量化 LLM
    之间的最后隐藏状态。正如表 [1](#S3.T1 "Table 1 ‣ 3.2 Token-level Scale Analysis within Activation
    Spikes ‣ 3 Activation Spikes: Excessive Magnitude of GLU Activations ‣ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs") 所示，对前四名模块的量化单独显著降低了
    LLM 性能，而其他情况表现出微不足道的性能变化。我们认为这些对量化敏感的输入激活（包括激活脉冲）是量化瓶颈，在本文中指的是由离群值引起的量化误差。'
- en: 'Furthermore, the activation spikes are conditioned on the specific context
    of the input sequence as discussed in Section [3.2](#S3.SS2 "3.2 Token-level Scale
    Analysis within Activation Spikes ‣ 3 Activation Spikes: Excessive Magnitude of
    GLU Activations ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs"). Altogether, such dynamic bottlenecks must be handled with caution to enhance
    the quantization performance of LLMs.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，激活脉冲依赖于输入序列的特定上下文，如第 [3.2](#S3.SS2 "3.2 Token-level Scale Analysis within
    Activation Spikes ‣ 3 Activation Spikes: Excessive Magnitude of GLU Activations
    ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs")
    节所讨论的。总之，这种动态瓶颈必须谨慎处理，以提升 LLM 的量化性能。'
- en: 4 Mitigating Quantization Quality Degradation Based on the Observation
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基于观察的量化质量降解缓解
- en: To address the quantization bottleneck, our approach is based on the deterministic
    occurrence patterns of activation spikes. First, we utilize the observation that
    bottlenecks occur at a few specific layers. This implies that naive full quantization
    of LLMs is affected by these bottlenecks. Second, we exploit the phenomenon that
    the activation spike is derived from the first occurrence of specific tokens.
    Thus, the planned occurrence prevents recurrence in the subsequent and possibly
    future tokens. In the following sections, we propose two methods inspired the
    above insights.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决量化瓶颈，我们的方法基于激活脉冲的确定性发生模式。首先，我们利用瓶颈发生在几个特定层的观察结果。这意味着 LLM 的简单全面量化受到这些瓶颈的影响。其次，我们利用激活脉冲源于特定令牌首次出现的现象。因此，计划中的出现会阻止在后续及可能的未来令牌中重复出现。在接下来的部分中，我们提出了两种方法，灵感来自上述见解。
- en: '![Refer to caption](img/1e9188ddec89ff6ad8acdba3d6ea5a9a.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1e9188ddec89ff6ad8acdba3d6ea5a9a.png)'
- en: 'Figure 3: Overview of QFeM and QFeP. (Left): QFeM excludes the modules whose
    $r^{(m)}$ from quantization. (Right): QFeP computes in advance the prefix of activation
    spikes and utilizes solely their KV cache during the quantization phase, effectively
    preventing further activation spikes in subsequent sequences.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: QFeM 和 QFeP 的概览。（左）：QFeM 排除了 $r^{(m)}$ 量化模块。（右）：QFeP 预先计算激活脉冲的前缀，并在量化阶段仅利用它们的
    KV 缓存，有效防止在后续序列中进一步的激活脉冲。'
- en: 4.1 Quantization-free Module (QFeM)
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 无量化模块 (QFeM)
- en: In the full quantization of LLM, all linear layers within the LLM are quantized.
    Among these linear layers, we propose omitting the quantization of input activations
    for linear layers where significant quantization errors are caused by activation
    spikes. To be noted, increasing the number of unquantized modules exhibits a trade-off
    between the inference latency and the model performance. Thus, determining which
    module should be quantized (or left unquantized) is crucial to retain the efficacy
    of quantization. Here, we use the max-median ratio $r^{(m)}$. For clarity, we
    treat sibling linear layers, such as query-key-value, as a single linear layer.
    To control the impact of activation quantization only, we leave the weight parameters
    in unquantized linear layers as INT8 and dequantize them into FP16 during matrix
    multiplication with the incoming activations, operating as weight-only quantization.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM的全面量化中，LLM中的所有线性层都被量化。在这些线性层中，我们建议省略对输入激活的量化，特别是在激活峰值导致显著量化误差的线性层中。需要注意的是，增加未量化模块的数量会在推理延迟和模型性能之间存在权衡。因此，确定哪个模块应该被量化（或保持未量化）对于保持量化的有效性至关重要。在这里，我们使用最大中位数比率$r^{(m)}$。为了清晰起见，我们将同胞线性层（如查询-键-值）视为一个单一的线性层。为了仅控制激活量化的影响，我们将未量化线性层中的权重参数保留为INT8，并在与传入激活进行矩阵乘法时将其解量化为FP16，这样的操作被称为仅权重量化。
- en: Optimizing the threshold $\alpha$.
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化阈值$\alpha$。
- en: '![Refer to caption](img/40bf1f893d9f6dc532f173d7c8d2582e.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参考图示](img/40bf1f893d9f6dc532f173d7c8d2582e.png)'
- en: 'Figure 4: Trade-off between perplexity (stands for performance) and $|M_{unq}|$
    for LLaMA-2-13B model.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：LLaMA-2-13B模型的困惑度（代表性能）与$|M_{unq}|$之间的权衡。
- en: 'To calculate the activation scale ratio for each linear layer, we first gather
    token-wise input activation scales from the calibration examples discussed in
    Section [3.1](#S3.SS1 "3.1 Existence of Activation Spikes in GLU Variants ‣ 3
    Activation Spikes: Excessive Magnitude of GLU Activations ‣ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs"). Exceptionally, for FFN experts
    in the mixture of experts (MoE) architectures like the Mixtral model [[21](#bib.bib21)],
    calibration is performed separately. After determining these ratios, we use binary
    search to set the threshold value $\alpha$ and its impact on performance is depicted
    in Figure [4](#S4.F4 "Figure 4 ‣ Optimizing the threshold 𝛼. ‣ 4.1 Quantization-free
    Module (QFeM) ‣ 4 Mitigating Quantization Quality Degradation Based on the Observation
    ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs"),
    demonstrating how full quantization can degrade performance. Rather than fully
    quantizing, we identify an optimal threshold by finding the intersection of two
    performance curves; in Figure [4](#S4.F4 "Figure 4 ‣ Optimizing the threshold
    𝛼. ‣ 4.1 Quantization-free Module (QFeM) ‣ 4 Mitigating Quantization Quality Degradation
    Based on the Observation ‣ Mitigating Quantization Errors Due to Activation Spikes
    in GLU-Based LLMs"), this threshold is approximately 16\. Details on the QFeM
    implementation are provided in Table [2](#S4.T2 "Table 2 ‣ Implementation Details.
    ‣ 4.2 Quantization-free Prefix (QFeP) ‣ 4 Mitigating Quantization Quality Degradation
    Based on the Observation ‣ Mitigating Quantization Errors Due to Activation Spikes
    in GLU-Based LLMs").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '为了计算每个线性层的激活缩放比率，我们首先从第[3.1节](#S3.SS1 "3.1 Existence of Activation Spikes in
    GLU Variants ‣ 3 Activation Spikes: Excessive Magnitude of GLU Activations ‣ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs")讨论的校准示例中收集逐个令牌的输入激活缩放。特别地，对于混合专家（MoE）架构中的FFN专家（如Mixtral模型[[21](#bib.bib21)]），校准是单独进行的。在确定这些比率后，我们使用二分搜索来设置阈值$\alpha$，其对性能的影响如图[4](#S4.F4
    "Figure 4 ‣ Optimizing the threshold 𝛼. ‣ 4.1 Quantization-free Module (QFeM)
    ‣ 4 Mitigating Quantization Quality Degradation Based on the Observation ‣ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs")所示，展示了全面量化如何降低性能。我们通过找到两个性能曲线的交点来识别最佳阈值；在图[4](#S4.F4
    "Figure 4 ‣ Optimizing the threshold 𝛼. ‣ 4.1 Quantization-free Module (QFeM)
    ‣ 4 Mitigating Quantization Quality Degradation Based on the Observation ‣ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs")中，这个阈值大约为16。QFeM实施的详细信息见表[2](#S4.T2
    "Table 2 ‣ Implementation Details. ‣ 4.2 Quantization-free Prefix (QFeP) ‣ 4 Mitigating
    Quantization Quality Degradation Based on the Observation ‣ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs")。'
- en: 4.2 Quantization-free Prefix (QFeP)
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 无量化前缀（QFeP）
- en: 'Orthogonal to the QFeM, we propose Quantization-free Prefix (QFeP) that mitigates
    the quantization errors by precomputing the prefix (or short prompt) corresponding
    to activation spikes. This method is based on the observations presented in Section [3.2](#S3.SS2
    "3.2 Token-level Scale Analysis within Activation Spikes ‣ 3 Activation Spikes:
    Excessive Magnitude of GLU Activations ‣ Mitigating Quantization Errors Due to
    Activation Spikes in GLU-Based LLMs"), which indicate that significant quantization
    errors result from the overestimated scale factor of the first instance within
    the restricted token set. Inspired by this occurrence pattern of activation spikes,
    we aim to construct a prefix which stabilizes the quantization scale factor of
    the tokens that come after the prefix. In other words, once the prefix is fixed
    at the beginning, the activation spikes consistently occur within the prefix.
    Afterward, we employ key-value (KV) caching mechanism to process the activation
    spikes in advance. In practice, KV cache is utilized to optimize the decoding
    speed of causal language models by storing precomputed key and value states of
    the previous tokens [[34](#bib.bib34), [32](#bib.bib32)]. This approach provides
    a bypass of the quantization including activation spikes, while preserving the
    context of prefix through the KV cache. The KV cache for the prefix is precomputed
    once through the offline inference of LLM without quantization. Then, this KV
    cache is exploited in the quantization phases, such as calibration or dynamic
    quantization, even for quantized inference. The process of QFeP is illustrated
    in Figure [3](#S4.F3 "Figure 3 ‣ 4 Mitigating Quantization Quality Degradation
    Based on the Observation ‣ Mitigating Quantization Errors Due to Activation Spikes
    in GLU-Based LLMs").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '与 QFeM 正交，我们提出了无量化前缀 (QFeP)，通过预计算与激活峰值对应的前缀（或简短提示）来减轻量化误差。该方法基于第 [3.2](#S3.SS2
    "3.2 Token-level Scale Analysis within Activation Spikes ‣ 3 Activation Spikes:
    Excessive Magnitude of GLU Activations ‣ Mitigating Quantization Errors Due to
    Activation Spikes in GLU-Based LLMs") 节中提出的观察结果，这些观察表明，显著的量化误差源于在限制的标记集内首次实例的过高估计尺度因子。受激活峰值发生模式的启发，我们旨在构造一个前缀，该前缀稳定在前缀之后的标记的量化尺度因子。换句话说，一旦前缀在开始时固定，激活峰值就会始终出现在前缀内。之后，我们使用键值
    (KV) 缓存机制提前处理激活峰值。实际上，KV 缓存用于通过存储先前标记的预计算键和值状态来优化因果语言模型的解码速度 [[34](#bib.bib34),
    [32](#bib.bib32)]。这种方法提供了绕过量化（包括激活峰值）的途径，同时通过 KV 缓存保留前缀的上下文。前缀的 KV 缓存在离线推理 LLM
    的量化之前一次性预计算。然后，在量化阶段（如校准或动态量化）中利用该 KV 缓存，即使在量化推理中也是如此。QFeP 的过程在图 [3](#S4.F3 "Figure
    3 ‣ 4 Mitigating Quantization Quality Degradation Based on the Observation ‣ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs") 中说明。'
- en: Prefix Search.
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 前缀搜索。
- en: To form a prefix of explicit activation spike, we first identify candidate token
    that represent the activation spike at the linear layer with the highest max-median
    ratio $r^{(m)}$. Note that the latter sequence in the template can be replaced
    with sequences from dataset instead of repetition.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了形成明确激活峰值的前缀，我们首先识别表示线性层中最大中位数比率 $r^{(m)}$ 的激活峰值的候选标记。请注意，模板中的后续序列可以用数据集中序列替换，而不是重复。
- en: Implementation Details.
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现细节。
- en: 'Table 2: Specifications for QFeM and QFeP used in experiments. $|M|$ represents
    the number of unquantized layers for QFeM.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：实验中使用的 QFeM 和 QFeP 的规格。$|M|$ 表示 QFeM 的未量化层数。
- en: '| Model | Prefix | $\boldsymbol{\alpha}$ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 前缀 | $\boldsymbol{\alpha}$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| LLaMA-2-7B | [BOS] all . | 6.68 | 17 / 128 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | [BOS] all . | 6.68 | 17 / 128 |'
- en: '| LLaMA-2-13B | [BOS] then , | 12.91 | 6 / 160 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | [BOS] then , | 12.91 | 6 / 160 |'
- en: '| LLaMA-2-70B | [BOS] I ’ | 9.16 | 25 / 320 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-70B | [BOS] I ’ | 9.16 | 25 / 320 |'
- en: '| Mistral-7B | [BOS] how \n | 49.00 | 3 / 128 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | [BOS] how \n | 49.00 | 3 / 128 |'
- en: '| Mixtral-8x7B | [BOS] ). \n | 4.03 | 191 / 608 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | [BOS] ). \n | 4.03 | 191 / 608 |'
- en: '| SOLAR-10.7B | [BOS] a 1 | 6.48 | 11 / 192 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| SOLAR-10.7B | [BOS] a 1 | 6.48 | 11 / 192 |'
- en: '| Gemma-7B | [BOS] . Più | 10.65 | 5 / 112 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7B | [BOS] . Più | 10.65 | 5 / 112 |'
- en: '| LLaMA-3-8B | [BOS] - nd | 6.64 | 6 / 128 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3-8B | [BOS] - nd | 6.64 | 6 / 128 |'
- en: '| LLaMA-3-70B | [BOS] and , | 78.37 | 3 / 320 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3-70B | [BOS] and , | 78.37 | 3 / 320 |'
- en: 'During the prefix search phase, we exploit the calibration dataset used in
    Section [3.1](#S3.SS1 "3.1 Existence of Activation Spikes in GLU Variants ‣ 3
    Activation Spikes: Excessive Magnitude of GLU Activations ‣ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs"). For the candidate tokens,
    we consider the tokens with the top three largest input activation magnitudes.
    Then, we search for the middle context token among top 200 most frequent tokens
    in the calibration dataset, which is the subset of the vocabulary $V$. Finally,
    with the search result, we prepare the KV cache for the target model in FP16 precision.
    Exceptionally, for the Mixtral [[21](#bib.bib21)] model, we use the scale of output
    hidden states instead of input activations, as the tokens are divided sparsely
    in a mixture of experts architecture. Table [2](#S4.T2 "Table 2 ‣ Implementation
    Details. ‣ 4.2 Quantization-free Prefix (QFeP) ‣ 4 Mitigating Quantization Quality
    Degradation Based on the Observation ‣ Mitigating Quantization Errors Due to Activation
    Spikes in GLU-Based LLMs") presents the searched prefix.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在前缀搜索阶段，我们利用了第[3.1节](#S3.SS1 "3.1 GLU 变体中的激活峰值的存在 ‣ 3 激活峰值：GLU 激活的过度幅度 ‣ 减轻因激活峰值导致的
    GLU 基于 LLM 的量化误差")中使用的校准数据集。对于候选标记，我们考虑输入激活幅度前三大的标记。然后，我们在校准数据集中前 200 个最频繁的标记中搜索中间上下文标记，这些标记是词汇表
    $V$ 的子集。最后，根据搜索结果，我们为目标模型准备 FP16 精度的 KV 缓存。例外的是，对于 Mixtral [[21](#bib.bib21)]
    模型，我们使用输出隐藏状态的规模，而不是输入激活，因为这些标记在专家混合架构中被稀疏划分。表[2](#S4.T2 "表 2 ‣ 实施细节 ‣ 4.2 无量化前缀
    (QFeP) ‣ 4 基于观察的量化质量降级减轻 ‣ 减轻因激活峰值导致的 GLU 基于 LLM 的量化误差")展示了搜索的前缀。
- en: 5 Experiments
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Experimental Setup
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: Models.
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型。
- en: 'Our proposed methods, QFeM and QFeP, aim to mitigate the quantization bottleneck,
    which is discussed in Section [3.3](#S3.SS3 "3.3 Effect of Quantization on Activation
    Spikes ‣ 3 Activation Spikes: Excessive Magnitude of GLU Activations ‣ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs"), caused by the
    activation spikes, especially in the GLU variants. To validate the efficiency
    proposed methods, we tested publicly released LLMs that were implemented with
    GLU, according to their paper and source code. We recognize recent LLMs, including
    LLAMA-2-{7B, 13B, 70B} [[47](#bib.bib47)], LLaMA-3-{7B, 70B}, Mistral-7B [[20](#bib.bib20)],
    Mixtral-8x7B [[21](#bib.bib21)], SOLAR-10.7B [[22](#bib.bib22)], and Gemma-7B
    [[43](#bib.bib43)], utilize the GLU architecture. The LLMs with original FFN are
    not covered, as they suffer from the existing outliers rather than activation
    spikes. All models are sourced from the huggingface-hub²²2https://huggingface.co/models
    repository.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的方法 QFeM 和 QFeP 旨在减轻由激活峰值引起的量化瓶颈，这在第[3.3节](#S3.SS3 "3.3 量化对激活峰值的影响 ‣ 3 激活峰值：GLU
    激活的过度幅度 ‣ 减轻因激活峰值导致的 GLU 基于 LLM 的量化误差")中进行了讨论，特别是在 GLU 变体中。为了验证所提出方法的有效性，我们测试了公开发布的、基于
    GLU 实现的 LLMs，这些模型依据其论文和源代码实现。我们识别出近期的 LLMs，包括 LLAMA-2-{7B, 13B, 70B} [[47](#bib.bib47)]、LLaMA-3-{7B,
    70B}、Mistral-7B [[20](#bib.bib20)]、Mixtral-8x7B [[21](#bib.bib21)]、SOLAR-10.7B
    [[22](#bib.bib22)] 和 Gemma-7B [[43](#bib.bib43)]，都采用了 GLU 架构。原始的 FFN LLMs 未被涵盖，因为它们面临的是现有的异常值，而非激活峰值。所有模型均来源于
    huggingface-hub²²2https://huggingface.co/models 仓库。
- en: Quantization.
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化。
- en: In the experiments, we quantize both the input activations and the weights of
    linear layers for INT8 matrix multiplication operations. Note that in Table [2](#S4.T2
    "Table 2 ‣ Implementation Details. ‣ 4.2 Quantization-free Prefix (QFeP) ‣ 4 Mitigating
    Quantization Quality Degradation Based on the Observation ‣ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs"), $|M|$ denotes the total number
    of linear modules targeted for quantization. In these linear layers, we opt for
    dynamic per-tensor quantization as the quantization scheme of input activations,
    and per-channel quantization for weights, respectively. Regarding both input activations
    and weights, we symmetrically quantize the range using the absolute maximum value
    as the scale estimation function. For comparison, we use FP16 and per-token activation
    quantization [[55](#bib.bib55)] as baselines. We refer the reader to Appendix [B](#A2
    "Appendix B BMM Quantization ‣ Mitigating Quantization Errors Due to Activation
    Spikes in GLU-Based LLMs") for Batch Matrix-Multiplication (BMM) quantization,
    which involves quantizing tensors in the self-attention.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们对 INT8 矩阵乘法操作的线性层的输入激活和权重进行量化。请注意，在表格 [2](#S4.T2 "Table 2 ‣ Implementation
    Details. ‣ 4.2 Quantization-free Prefix (QFeP) ‣ 4 Mitigating Quantization Quality
    Degradation Based on the Observation ‣ Mitigating Quantization Errors Due to Activation
    Spikes in GLU-Based LLMs") 中，$|M|$ 表示目标量化的线性模块的总数。在这些线性层中，我们选择动态每张量量化作为输入激活的量化方案，而对权重使用每通道量化。对于输入激活和权重，我们使用绝对最大值作为尺度估计函数对范围进行对称量化。为了进行比较，我们使用
    FP16 和每令牌激活量化 [[55](#bib.bib55)] 作为基准。有关 Batch Matrix-Multiplication (BMM) 量化的信息，我们参考附录
    [B](#A2 "Appendix B BMM Quantization ‣ Mitigating Quantization Errors Due to Activation
    Spikes in GLU-Based LLMs")，这涉及对自注意力中的张量进行量化。
- en: Evaluations.
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估。
- en: 'We evaluate the quantized LLMs with two metrics: zero-shot evaluation accuracy
    and perplexity. For zero-shot evaluation, we use the four datasets: PIQA [[7](#bib.bib7)],
    LAMBADA [[33](#bib.bib33)], HellaSwag [[56](#bib.bib56)], and WinoGrande [[38](#bib.bib38)].
    We utilize the lm-evaluation-harness library [[16](#bib.bib16)] to evaluate zero-shot
    tasks. To measure perplexity, we use the WikiText-2 [[28](#bib.bib28)] dataset.
    In all cases, we use the [BOS] token as the starting token for each input sequence
    by default.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个指标来评估量化的 LLMs：零-shot 评估准确率和困惑度。对于零-shot 评估，我们使用四个数据集：PIQA [[7](#bib.bib7)]、LAMBADA
    [[33](#bib.bib33)]、HellaSwag [[56](#bib.bib56)] 和 WinoGrande [[38](#bib.bib38)]。我们利用
    lm-evaluation-harness 库 [[16](#bib.bib16)] 来评估零-shot 任务。为了测量困惑度，我们使用 WikiText-2
    [[28](#bib.bib28)] 数据集。在所有情况下，我们默认使用 [BOS] 令牌作为每个输入序列的起始令牌。
- en: 5.2 Main Results
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 主要结果
- en: 'Table 3: Perplexity and zero-shot evaluation for the quantization on LLaMA-2
    models. FP16 denotes the original model precision, and W8A8 denotes the model
    quantized to INT8 for both weights and activations.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：对 LLaMA-2 模型进行量化的困惑度和零-shot 评估。FP16 表示原始模型精度，而 W8A8 表示将模型量化为 INT8 的权重和激活。
- en: '| Method |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |'
- en: '&#124; WikiText-2 &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WikiText-2 &#124;'
- en: '&#124; (ppl$\downarrow$) &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (ppl$\downarrow$) &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PIQA &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PIQA &#124;'
- en: '&#124; (acc$\uparrow$) &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc$\uparrow$) &#124;'
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LAMBADA &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LAMBADA &#124;'
- en: '&#124; (acc$\uparrow$) &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc$\uparrow$) &#124;'
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; HellaSwag &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HellaSwag &#124;'
- en: '&#124; (acc$\uparrow$) &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc$\uparrow$) &#124;'
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; WinoGrande &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WinoGrande &#124;'
- en: '&#124; (acc$\uparrow$) &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc$\uparrow$) &#124;'
- en: '|'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Avg &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均 &#124;'
- en: '&#124; (acc$\uparrow$) &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (acc$\uparrow$) &#124;'
- en: '|'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| LLaMA-2-7B |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B |'
- en: '| FP16 | 5.268 | 78.18% | 73.67% | 57.13% | 69.46% | 69.61% |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 5.268 | 78.18% | 73.67% | 57.13% | 69.46% | 69.61% |'
- en: '| W8A8 | 8.634 | 72.80% | 62.27% | 49.57% | 63.69% | 62.08% |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 8.634 | 72.80% | 62.27% | 49.57% | 63.69% | 62.08% |'
- en: '|   +QFeM | 5.758[-2.876] | 78.02% | 73.86% | 56.32% | 68.35% | 69.14%[+7.06]
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|   +QFeM | 5.758[-2.876] | 78.02% | 73.86% | 56.32% | 68.35% | 69.14%[+7.06]
    |'
- en: '|   +QFeP | 5.758[-2.876] | 76.44% | 73.57% | 55.55% | 69.22% | 68.69%[+6.61]
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|   +QFeP | 5.758[-2.876] | 76.44% | 73.57% | 55.55% | 69.22% | 68.69%[+6.61]
    |'
- en: '|   +QFeM+QFeP | 5.573[-3.061] | 77.86% | 74.58% | 56.05% | 69.38% | 69.47%[+7.39]
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|   +QFeM+QFeP | 5.573[-3.061] | 77.86% | 74.58% | 56.05% | 69.38% | 69.47%[+7.39]
    |'
- en: '| LLaMA-2-13B |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B |'
- en: '| FP16 | 4.789 | 79.49% | 76.54% | 60.20% | 72.38% | 72.15% |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 4.789 | 79.49% | 76.54% | 60.20% | 72.38% | 72.15% |'
- en: '| W8A8 | 34.089 | 70.13% | 49.66% | 42.65% | 58.72% | 55.29% |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 34.089 | 70.13% | 49.66% | 42.65% | 58.72% | 55.29% |'
- en: '|   +QFeM | 5.241[-28.848] | 77.58% | 75.68% | 59.13% | 72.61% | 71.25%[+15.96]
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|   +QFeM | 5.241[-28.848] | 77.58% | 75.68% | 59.13% | 72.61% | 71.25%[+15.96]
    |'
- en: '|   +QFeP | 6.000[-28.089] | 77.53% | 73.94% | 57.23% | 70.96% | 69.91%[+14.62]
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|   +QFeP | 6.000[-28.089] | 77.53% | 73.94% | 57.23% | 70.96% | 69.91%[+14.62]
    |'
- en: '|   +QFeM+QFeP | 5.126[-28.963] | 78.51% | 75.86% | 59.44% | 72.61% | 71.61%[+16.32]
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|   +QFeM+QFeP | 5.126[-28.963] | 78.51% | 75.86% | 59.44% | 72.61% | 71.61%[+16.32]
    |'
- en: '| LLaMA-2-70B |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-70B |'
- en: '| FP16 | 3.218 | 81.45% | 79.45% | 65.29% | 80.43% | 76.65% |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 3.218 | 81.45% | 79.45% | 65.29% | 80.43% | 76.65% |'
- en: '| W8A8 | 8.055 | 74.05% | 70.27% | 55.21% | 67.96% | 66.87% |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | 8.055 | 74.05% | 70.27% | 55.21% | 67.96% | 66.87% |'
- en: '|   +QFeM | 3.830[-4.225] | 81.23% | 77.66% | 64.15% | 78.14% | 75.30%[+8.43]
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|   +QFeM | 3.830[-4.225] | 81.23% | 77.66% | 64.15% | 78.14% | 75.30%[+8.43]
    |'
- en: '|   +QFeP | 6.007[-2.048] | 77.64% | 73.26% | 63.40% | 76.16% | 72.62%[+5.75]
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|   +QFeP | 6.007[-2.048] | 77.64% | 73.26% | 63.40% | 76.16% | 72.62%[+5.75]
    |'
- en: '|   +QFeM+QFeP | 3.708[-4.347] | 81.23% | 77.82% | 64.65% | 77.11% | 75.20%[+8.33]
    | ![Refer to caption](img/0592725db7336eb0f205080da311fd87.png)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '|   +QFeM+QFeP | 3.708[-4.347] | 81.23% | 77.82% | 64.65% | 77.11% | 75.20%[+8.33]
    | ![参见说明](img/0592725db7336eb0f205080da311fd87.png)'
- en: 'Figure 5: The average accuracy of zero-shot evaluation on other GLU-implemented
    LLMs. Most models recover significantly compared to W8A8, with performance close
    to FP16.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：其他GLU实现的LLM在零-shot评估中的平均准确率。与W8A8相比，大多数模型的恢复情况显著，性能接近FP16。
- en: LLaMA-2 Models.
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLaMA-2模型。
- en: We report the evaluation results of quantization on LLaMA-2 models in Table [3](#S5.T3
    "Table 3 ‣ 5.2 Main Results ‣ 5 Experiments ‣ Mitigating Quantization Errors Due
    to Activation Spikes in GLU-Based LLMs"). Compared to FP16 precision, quantizing
    both weights and activations (W8A8) degrades the overall performance. The results
    demonstrate that our proposed methods resolve the activation spikes and, surprisingly,
    restore the performance of the W8A8 close to that of FP16. For example, the LLaMA-2
    7B model achieves less than a 1% performance drop from FP16. It is worth noting
    that the proposed QFeM and QFeP improve at comparable levels. This indicates that
    the activation spikes present a direct cause of the significant decrease in quantization
    performance. Because the proposed methods are orthogonal, the performance slightly
    increases when incorporating both QFeM and QFeP compared to applying them individually.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表格[3](#S5.T3 "Table 3 ‣ 5.2 Main Results ‣ 5 Experiments ‣ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs")中报告了对LLaMA-2模型量化的评估结果。与FP16精度相比，量化权重和激活（W8A8）会降低整体性能。结果表明，我们提出的方法解决了激活尖峰问题，令人惊讶的是，使W8A8的性能恢复到接近FP16。例如，LLaMA-2
    7B模型的性能下降不到1%。值得注意的是，提出的QFeM和QFeP在相似水平上有所改进。这表明，激活尖峰是量化性能显著下降的直接原因。由于所提方法是正交的，结合QFeM和QFeP时性能略有提高，相较于单独应用它们。
- en: Other GLU-implemented LLMs.
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 其他GLU实现的LLM。
- en: For other LLMs that incorporate GLU, we investigated the effectiveness of our
    methods in mitigating the quantization bottleneck. As can be seen in Figure [5](#S5.F5
    "Figure 5 ‣ 5.2 Main Results ‣ 5 Experiments ‣ Mitigating Quantization Errors
    Due to Activation Spikes in GLU-Based LLMs"), our methods consistently remedy
    the performance drop caused by activation spikes. Noticeably, the Mixtral model
    demonstrates robustness towards the performance degradation. This indicates that
    the mixture of experts architecture, which divides the MLP experts by tokens,
    helps to alleviate the impact of the activation spikes. Meanwhile, addressing
    the activation spikes is not a sufficient complement for the Gemma model compared
    to other models. We attribute this to the choice of activation function among
    GLU variants; specifically, Gemma uses GeGLU, while other models employ SwiGLU.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他采用GLU的LLM，我们研究了我们的方法在缓解量化瓶颈方面的有效性。如图[5](#S5.F5 "Figure 5 ‣ 5.2 Main Results
    ‣ 5 Experiments ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs")所示，我们的方法持续修复了激活尖峰引起的性能下降。值得注意的是，Mixtral模型在性能退化方面表现出稳健性。这表明，将MLP专家按令牌划分的专家混合架构有助于减轻激活尖峰的影响。同时，与其他模型相比，解决激活尖峰对于Gemma模型并不是一个足够的补充。我们将此归因于GLU变体中的激活函数选择；具体而言，Gemma使用GeGLU，而其他模型使用SwiGLU。
- en: 5.3 Combining Outlier Alleviation Methods
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 结合异常值缓解方法
- en: 'Table 4: Evaluation of outlier alleviation methods with QFeM and QFeP. We report
    perplexity on WikiText-2 and averaged accuracy of four zero-shot tasks. The same
    quantization scheme for used on both SQ and OSP. Per-tensor weight quantization
    results are provided in Appendix [C.1](#A3.SS1 "C.1 Additional Results for Combining
    Outlier Alleviation Methods ‣ Appendix C Supplementary Experiment Results ‣ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs").'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：与 QFeM 和 QFeP 一起评估离群值缓解方法。我们报告了 WikiText-2 上的困惑度以及四个零样本任务的平均准确率。SQ 和 OSP
    使用了相同的量化方案。每个张量权重量化结果见附录[C.1](#A3.SS1 "C.1 Additional Results for Combining Outlier
    Alleviation Methods ‣ Appendix C Supplementary Experiment Results ‣ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs")。
- en: '| Method | LLaMA-2-7B | LLaMA-2-13B | LLaMA-2-70B |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LLaMA-2-7B | LLaMA-2-13B | LLaMA-2-70B |'
- en: '| --- | --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ppl($\downarrow$) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| ppl($\downarrow$) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SQ [[51](#bib.bib51)] | 9.907 | 61.08% | 34.869 | 59.45% | 8.800 | 70.25%
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| SQ [[51](#bib.bib51)] | 9.907 | 61.08% | 34.869 | 59.45% | 8.800 | 70.25%
    |'
- en: '| +QFeM | 5.534 | 69.65% | 5.118 | 71.23% | 3.599 | 75.93% |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| +QFeM | 5.534 | 69.65% | 5.118 | 71.23% | 3.599 | 75.93% |'
- en: '| +QFeP | 5.715 | 68.66% | 6.551 | 69.33% | 5.228 | 74.07% |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 5.715 | 68.66% | 6.551 | 69.33% | 5.228 | 74.07% |'
- en: '| OSP [[50](#bib.bib50)] | 38.490 | 59.90% | 5.148 | 71.29% | 3.827 | 75.52%
    |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| OSP [[50](#bib.bib50)] | 38.490 | 59.90% | 5.148 | 71.29% | 3.827 | 75.52%
    |'
- en: '| +QFeM | 5.493 | 69.37% | 5.099 | 71.37% | 3.559 | 75.92% |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| +QFeM | 5.493 | 69.37% | 5.099 | 71.37% | 3.559 | 75.92% |'
- en: '| +QFeP | 5.642 | 68.95% | 5.144 | 71.05% | 3.752 | 75.36% |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 5.642 | 68.95% | 5.144 | 71.05% | 3.752 | 75.36% |'
- en: While our method focuses on the activation spikes, the inherent outlier values
    in the input activations remain. Here, we combine the prior outlier alleviation
    methods, such as SmoothQuant (SQ) [[51](#bib.bib51)] and OutlierSuppressionPlus
    (OSP) [[50](#bib.bib50)], to further improve the quantization error. In practice,
    our methods are utilized during the scale calibration phase of alleviation methods
    to mitigate the impact of activation spikes on scale migration between activations
    and weights. Table [4](#S5.T4 "Table 4 ‣ 5.3 Combining Outlier Alleviation Methods
    ‣ 5 Experiments ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs") demonstrates the evaluation results of applying the outlier alleviation
    methods solely and combining them with our methods. We find that there are cases
    where the alleviation method fails to recover the performance when quantizing
    the activations with per-tensor scheme.³³3In their papers, the activations of
    LLaMA models are quantized using only a per-token scheme. This indicates that
    alleviating the outlier scales, including the activation spikes, is challenging.
    With the QFeM, the activation spikes are excluded, and the accurate alleviation
    is enabled. In addition, the QFeP also benefits from the SQ method, as seen in
    the case of LLaMA-2 70B. Exceptionally, the OSP successfully addresses the activation
    spikes in the 13B and 70B cases.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的方法专注于激活峰值，但输入激活中的固有离群值仍然存在。在这里，我们结合了先前的离群值缓解方法，如 SmoothQuant (SQ) [[51](#bib.bib51)]
    和 OutlierSuppressionPlus (OSP) [[50](#bib.bib50)]，以进一步改善量化误差。在实际应用中，我们的方法在缓解方法的尺度校准阶段使用，以减少激活峰值对激活与权重之间尺度迁移的影响。表[4](#S5.T4
    "Table 4 ‣ 5.3 Combining Outlier Alleviation Methods ‣ 5 Experiments ‣ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs")展示了单独应用离群值缓解方法及其与我们方法结合的评估结果。我们发现，在使用每个张量方案对激活进行量化时，缓解方法在某些情况下未能恢复性能。在他们的论文中，LLaMA
    模型的激活仅使用每个令牌方案进行量化。这表明，缓解离群值尺度（包括激活峰值）是具有挑战性的。使用 QFeM，可以排除激活峰值，从而实现准确的缓解。此外，QFeP
    也受益于 SQ 方法，如 LLaMA-2 70B 的情况所示。例外的是，OSP 成功解决了 13B 和 70B 情况下的激活峰值问题。
- en: 5.4 Ablation Study
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 消融研究
- en: '![Refer to caption](img/fc8930c86e0b47ace7a2ad3b479acfb9.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/fc8930c86e0b47ace7a2ad3b479acfb9.png)'
- en: 'Figure 6: Prefix ablation. Y-axis represents averaged accuracy of four zero-shot
    tasks.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：前缀消融。Y 轴表示四个零样本任务的平均准确率。
- en: For the QFeP, we designed a length-three prefix for the KV cache, including
    the BOS token, context token, and extra token for activation spike. Because the
    KV cache consumes the capacity of the pretrained sequence position, it raises
    a question about the length of the prefix. Therefore, we conduct ablation study
    for different prefixes for the KV cache. For the prefixes, we prepare random,
    BOS only, and both QFeP without and with the context token. We illustrate the
    results of ablation study in Figure [6](#S5.F6 "Figure 6 ‣ 5.4 Ablation Study
    ‣ 5 Experiments ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs"). In all cases, the random prefix showcases the lowest performance. While
    the KV cache with the BOS token demonstrates inconsistent performance, our QFeP
    consistently shows significant improvement. Importantly, the results imply that
    the sufficient prefix for the models exhibits differences. However, we emphasize
    that our KV design for QFeP shows improvements by large margins across all models.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 QFeP，我们设计了一个长度为三的前缀用于 KV 缓存，包括 BOS 标记、上下文标记和用于激活峰值的额外标记。由于 KV 缓存消耗了预训练序列位置的容量，这就对前缀的长度提出了问题。因此，我们对
    KV 缓存的不同前缀进行了消融研究。对于前缀，我们准备了随机前缀、仅 BOS 前缀以及有和没有上下文标记的 QFeP 前缀。我们在图 [6](#S5.F6
    "图 6 ‣ 5.4 消融研究 ‣ 5 实验 ‣ 减轻基于 GLU 的 LLM 中的激活峰值量化误差")中展示了消融研究的结果。在所有情况下，随机前缀展示了最低的性能。虽然带有
    BOS 标记的 KV 缓存表现不稳定，但我们的 QFeP 一致地显示了显著的改进。重要的是，这些结果表明，模型所需的前缀在展示上存在差异。然而，我们强调，我们为
    QFeP 设计的 KV 缓存在所有模型中都大幅度提升了性能。
- en: 5.5 Computational Cost Analysis
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 计算成本分析
- en: 'The proposed methods require additional resources to evict the activation spikes.
    Therefore, we analyze the computational costs of the methods and compare them
    in various schemes. For comparison, we evaluate different activation quantization
    schemes: dynamic per-token, dynamic per-tensor, and static per-tensor, denoted
    as AQ1, AQ2, and AQ3, respectively. This distinction establishes strong baselines
    and demonstrates the potential of the methods. To calibrate the static scales,
    we estimate the absolute maximum value using the calibration dataset, which is
    used in Section [3.1](#S3.SS1 "3.1 Existence of Activation Spikes in GLU Variants
    ‣ 3 Activation Spikes: Excessive Magnitude of GLU Activations ‣ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs").'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 所提出的方法需要额外资源来驱逐激活峰值。因此，我们分析了这些方法的计算成本，并在各种方案中进行了比较。为了比较，我们评估了不同的激活量化方案：动态每标记、动态每张量和静态每张量，分别表示为
    AQ1、AQ2 和 AQ3。这种区分建立了强有力的基准，并展示了方法的潜力。为了校准静态比例，我们使用校准数据集估计绝对最大值，这在第 [3.1](#S3.SS1
    "3.1 GLU 变体中的激活峰值存在 ‣ 3 激活峰值：GLU 激活的过度幅度 ‣ 减轻基于 GLU 的 LLM 中的激活峰值量化误差")节中使用。
- en: Inference Latency.
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理延迟。
- en: For each setting, we present the accuracy of the zero-shot tasks and inference
    latency of the fixed token sequence, as shown in Figure [7](#S5.F7 "Figure 7 ‣
    Table 5 ‣ Inference Latency. ‣ 5.5 Computational Cost Analysis ‣ 5 Experiments
    ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs").
    While the fine-grained scheme (AQ1) shows a negligible accuracy drop, the counterparts
    (AQ2, AQ3) degrade with the quantization bottleneck. However, by applying our
    methods, the coarse-grained schemes achieve a competitive performance gain. For
    example, the combination of AQ2 and QFeM demonstrates the performance close to
    the AQ1 but with faster latency. The results signify that addressing the quantization
    bottleneck is important to accelerate the inference latency with coarser granularity.
    Specifically, the naive static quantization (AQ3), the fastest scheme, exhibits
    a significant decline. We hope that our work contributes to the future works,
    which address the remaining challenges in static quantization.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个设置，我们展示了零样本任务的准确性和固定标记序列的推理延迟，如图 [7](#S5.F7 "图 7 ‣ 表 5 ‣ 推理延迟 ‣ 5.5 计算成本分析
    ‣ 5 实验 ‣ 减轻基于 GLU 的 LLM 中的激活峰值量化误差")所示。尽管细粒度方案 (AQ1) 显示了微不足道的准确性下降，但其余方案 (AQ2、AQ3)
    在量化瓶颈下表现下降。然而，通过应用我们的方法，粗粒度方案实现了竞争性的性能提升。例如，AQ2 和 QFeM 的组合展示了接近 AQ1 的性能，但具有更快的延迟。这些结果表明，解决量化瓶颈对加速推理延迟具有重要意义，尤其是在较粗粒度下。具体来说，最简单的静态量化
    (AQ3)，作为最快的方案，显示出显著下降。我们希望我们的工作对未来的工作有所贡献，解决静态量化中的剩余挑战。
- en: '![Refer to caption](img/8f6af0fb421b36a6b972d00ee2945100.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8f6af0fb421b36a6b972d00ee2945100.png)'
- en: 'Figure 7: Accuracy-latency comparison of different activation quantization
    schemes: dynamic per-token (AQ1), dynamic per-tensor (AQ2), and static per-tensor
    (AQ3).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：不同激活量化方案的准确率-延迟比较：动态每令牌（AQ1）、动态每张量（AQ2）和静态每张量（AQ3）。
- en: 'Table 5: Memory footprint.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：内存占用。
- en: '| Method | SeqLen |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SeqLen |'
- en: '| 1K | 2K |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 1K | 2K |'
- en: '| LLaMA-2-7B |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B |'
- en: '| AQ1 | 8185MiB | 9516MiB |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| AQ1 | 8185MiB | 9516MiB |'
- en: '| AQ2 | 8148MiB | 9474MiB |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| AQ2 | 8148MiB | 9474MiB |'
- en: '| +QFeP | 8149MiB | 9478MiB |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 8149MiB | 9478MiB |'
- en: '| +QFeM | 8148MiB | 9474MiB |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| +QFeM | 8148MiB | 9474MiB |'
- en: '| LLaMA-2-70B |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-70B |'
- en: '| AQ1 | 67756MiB | 69037MiB |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| AQ1 | 67756MiB | 69037MiB |'
- en: '| AQ2 | 67648MiB | 68820MiB |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| AQ2 | 67648MiB | 68820MiB |'
- en: '| +QFeP | 67651MiB | 68822MiB |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 67651MiB | 68822MiB |'
- en: '| +QFeM | 67838MiB | 68819MiB |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| +QFeM | 67838MiB | 68819MiB |'
- en: Memory Footprint.
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存占用。
- en: In Table [5](#S5.T5 "Table 5 ‣ Inference Latency. ‣ 5.5 Computational Cost Analysis
    ‣ 5 Experiments ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs"), we record the maximum memory footprint of our methods. For QFeP, the additional
    memory is consistently required for the preserved KV cache. However, this memory
    overhead is much smaller than that used in the fine-grained quantization (AQ1),
    as QFeM utilizes only three tokens for the cache. Contrary to QFeP, QFeM shows
    inconsistent memory utilization. For example, the 7B model with QFeM exhibits
    memory usage similar to AQ2, while the 70B model with QFeM incur additional consumption
    for a sequence length of 1K. This is attributed to the use of W8A16 for the unquantization
    modules in QFeM. To tailor the memory usage or inference speed, an alternative
    strategy can be utilized for QFeM, such as applying fine-grained activation quantization
    to the unquantization modules instead of using W8A16.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [5](#S5.T5 "表 5 ‣ 推理延迟。 ‣ 5.5 计算成本分析 ‣ 5 实验 ‣ 缓解因GLU基于LLM中的激活峰值引起的量化误差")
    中，我们记录了我们方法的最大内存占用。对于QFeP，额外的内存始终用于保留的KV缓存。然而，这一内存开销远小于细粒度量化（AQ1）所需的，因为QFeM仅使用三个令牌进行缓存。与QFeP相反，QFeM表现出不一致的内存利用情况。例如，使用QFeM的7B模型表现出类似于AQ2的内存使用情况，而使用QFeM的70B模型在序列长度为1K时会增加额外的消耗。这是由于QFeM中未量化模块使用了W8A16。为了调整内存使用或推理速度，可以对QFeM使用替代策略，例如对未量化模块应用细粒度激活量化，而不是使用W8A16。
- en: 6 Conclusion
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We explore the quantization challenge of GLU activations for modern LLMs. We
    find that the GLU variants generates excessive activation scales, which cause
    significant quantization bottlenecks at the specific layers. Based on the systematic
    generation pattern of the activation spikes, we propose methods that address the
    spikes in a layer-wise (QFeM) and token-wise manner (QFeP). In the experiments,
    we confirm that the proposed methods effectively resolve the quantization bottlenecks
    and result in a large performance gain. We expect that our work sheds light on
    the potential challenges in future studies regarding quantization and facilitates
    the development of efficient LLM systems.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了现代大型语言模型（LLMs）中GLU激活的量化挑战。我们发现GLU变体生成过多的激活尺度，这在特定层中造成了显著的量化瓶颈。基于激活峰值的系统生成模式，我们提出了在层级（QFeM）和令牌级别（QFeP）处理这些峰值的方法。在实验中，我们确认所提出的方法有效解决了量化瓶颈，并带来了显著的性能提升。我们期望我们的工作能对未来关于量化的研究中的潜在挑战提供启示，并促进高效LLM系统的发展。
- en: References
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Zhen Stephen
    Gou, Phil Blunsom, Ahmet Üstün, and Sara Hooker. Intriguing properties of quantization
    at scale. Advances in Neural Information Processing Systems, 36:34278–34294, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Zhen Stephen
    Gou, Phil Blunsom, Ahmet Üstün 和 Sara Hooker。量化规模的有趣特性。神经信息处理系统进展，36:34278–34294，2023年。'
- en: '[2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico
    Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models
    from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico
    Lebrón 和 Sumit Sanghai。Gqa: 从多头检查点训练通用多查询变换器模型。arXiv 预印本 arXiv:2305.13245，2023年。'
- en: '[3] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
    Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay,
    Quentin Malartic, et al. The falcon series of open language models. arXiv preprint
    arXiv:2311.16867, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
    Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay,
    Quentin Malartic 等。开放语言模型的猎鹰系列。arXiv 预印本 arXiv:2311.16867，2023年。'
- en: '[4] Alexei Baevski and Michael Auli. Adaptive input representations for neural
    language modeling. In International Conference on Learning Representations, 2018.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Alexei Baevski 和 Michael Auli. 神经语言建模的自适应输入表示。在国际学习表征会议上，2018年。'
- en: '[5] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi,
    Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al.
    Stable lm 2 1.6 b technical report. arXiv preprint arXiv:2402.17834, 2024.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Marco Bellagente、Jonathan Tow、Dakota Mahan、Duy Phung、Maksym Zhuravinskyi、Reshinth
    Adithyan、James Baicoianu、Ben Brooks、Nathan Cooper、Ashish Datta 等人。稳定的 LM 2 1.6
    b 技术报告。arXiv 预印本 arXiv:2402.17834，2024年。'
- en: '[6] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley,
    Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai
    Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models
    across training and scaling. In International Conference on Machine Learning,
    pages 2397–2430\. PMLR, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Stella Biderman、Hailey Schoelkopf、Quentin Gregory Anthony、Herbie Bradley、Kyle
    O’Brien、Eric Hallahan、Mohammad Aflah Khan、Shivanshu Purohit、USVSN Sai Prashanth、Edward
    Raff 等人。Pythia: 一个用于分析大型语言模型的训练和扩展的套件。在国际机器学习大会上，第2397–2430页。PMLR，2023年。'
- en: '[7] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning
    about physical commonsense in natural language. In Proceedings of the AAAI conference
    on artificial intelligence, volume 34, pages 7432–7439, 2020.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Yonatan Bisk、Rowan Zellers、Jianfeng Gao、Yejin Choi 等人。Piqa: 以自然语言推理关于物理常识。在
    AAAI 人工智能会议论文集中，第34卷，第7432–7439页，2020年。'
- en: '[8] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding
    and overcoming the challenges of efficient transformer quantization. In Marie-Francine
    Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing, pages
    7947–7969, Online and Punta Cana, Dominican Republic, November 2021\. Association
    for Computational Linguistics.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Yelysei Bondarenko, Markus Nagel 和 Tijmen Blankevoort. 理解和克服高效变压器量化的挑战。在
    Marie-Francine Moens、Xuanjing Huang、Lucia Specia 和 Scott Wen-tau Yih 主编的《2021年自然语言处理经验方法会议论文集》中，第7947–7969页，在线及多米尼加共和国蓬塔卡纳，2021年11月。计算语言学协会。'
- en: '[9] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Quantizable transformers:
    Removing outliers by helping attention heads do nothing. Advances in Neural Information
    Processing Systems, 36, 2024.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Yelysei Bondarenko、Markus Nagel 和 Tijmen Blankevoort. 可量化的变压器: 通过帮助注意力头无所作为来去除异常值。神经信息处理系统进展，36，2024年。'
- en: '[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell 等人。语言模型是少样本学习者。神经信息处理系统进展，33:1877–1901，2020年。'
- en: '[11] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip:
    2-bit quantization of large language models with guarantees. Advances in Neural
    Information Processing Systems, 36, 2024.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Jerry Chee、Yaohui Cai、Volodymyr Kuleshov 和 Christopher M De Sa. Quip:
    带保证的大型语言模型的2位量化。神经信息处理系统进展，36，2024年。'
- en: '[12] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3\.
    int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural
    Information Processing Systems, 35:30318–30332, 2022.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Tim Dettmers、Mike Lewis、Younes Belkada 和 Luke Zettlemoyer. Gpt3\. int8
    (): 大规模变压器的8位矩阵乘法。神经信息处理系统进展，35:30318–30332，2022年。'
- en: '[13] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev,
    Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
    Spqr: A sparse-quantized representation for near-lossless llm weight compression.
    arXiv preprint arXiv:2306.03078, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Tim Dettmers、Ruslan Svirschevski、Vage Egiazarian、Denis Kuznedelev、Elias
    Frantar、Saleh Ashkboos、Alexander Borzunov、Torsten Hoefler 和 Dan Alistarh. Spqr:
    一种稀疏量化表示方法，用于接近无损的 LLM 权重压缩。arXiv 预印本 arXiv:2306.03078，2023年。'
- en: '[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova. Bert: 深度双向变压器的预训练用于语言理解。arXiv
    预印本 arXiv:1810.04805，2018年。'
- en: '[15] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh. Gptq: 生成预训练变压器的准确后训练量化。arXiv
    预印本 arXiv:2210.17323，2022年。'
- en: '[16] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan
    Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds,
    Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben
    Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation,
    12 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan
    Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds,
    Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben
    Wang, Kevin Wang, 和 Andy Zou。少样本语言模型评估框架，2023年12月。'
- en: '[17] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and
    Kurt Keutzer. A survey of quantization methods for efficient neural network inference.
    In Low-Power Computer Vision, pages 291–326\. Chapman and Hall/CRC, 2022.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, 和
    Kurt Keutzer。高效神经网络推断的量化方法综述。收录于《低功耗计算机视觉》，第291–326页。Chapman and Hall/CRC，2022年。'
- en: '[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings
    in deep residual networks. In Computer Vision–ECCV 2016: 14th European Conference,
    Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pages
    630–645\. Springer, 2016.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun。深度残差网络中的身份映射。收录于《计算机视觉–ECCV
    2016：第14届欧洲会议，荷兰阿姆斯特丹，2016年10月11–14日，论文集》第IV卷第14部分，第630–645页。Springer，2016年。'
- en: '[19] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,
    Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training
    of neural networks for efficient integer-arithmetic-only inference. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 2704–2713,
    2018.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,
    Andrew Howard, Hartwig Adam, 和 Dmitry Kalenichenko。用于高效整数算术推断的神经网络量化与训练。收录于 IEEE
    计算机视觉与模式识别会议论文集，第2704–2713页，2018年。'
- en: '[20] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825,
    2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier 等。Mistral 7b。arXiv 预印本 arXiv:2310.06825，2023年。'
- en: '[21] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
    Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,
    Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088,
    2024.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
    Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,
    Florian Bressand 等。Mixtral 专家系统。arXiv 预印本 arXiv:2401.04088，2024年。'
- en: '[22] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu
    Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et al. Solar 10.7 b: Scaling
    large language models with simple yet effective depth up-scaling. arXiv preprint
    arXiv:2312.15166, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu
    Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim 等。Solar 10.7 b：通过简单而有效的深度上采样来扩展大型语言模型。arXiv
    预印本 arXiv:2312.15166，2023年。'
- en: '[23] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
    Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization.
    arXiv preprint arXiv:2306.07629, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
    Michael W Mahoney, 和 Kurt Keutzer。Squeezellm：密集与稀疏量化。arXiv 预印本 arXiv:2306.07629，2023年。'
- en: '[24] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky.
    BERT busters: Outlier dimensions that disrupt transformers. In Chengqing Zong,
    Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the Association
    for Computational Linguistics: ACL-IJCNLP 2021, pages 3392–3405, Online, August
    2021\. Association for Computational Linguistics.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, 和 Anna Rumshisky。BERT
    破坏者：扰乱变压器的离群维度。收录于 Chengqing Zong, Fei Xia, Wenjie Li, 和 Roberto Navigli 编辑的《计算语言学协会会议论文集：ACL-IJCNLP
    2021》，第3392–3405页，在线，2021年8月。计算语言学协会。'
- en: '[25] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing
    the dark secrets of BERT. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun
    Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP), pages 4365–4374, Hong Kong, China, November 2019\.
    Association for Computational Linguistics.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Olga Kovaleva, Alexey Romanov, Anna Rogers, 和 Anna Rumshisky。揭示BERT的黑暗秘密。在
    Kentaro Inui, Jing Jiang, Vincent Ng, 和 Xiaojun Wan 编辑的《2019年自然语言处理经验方法会议暨第9届国际自然语言处理联合会议（EMNLP-IJCNLP）》会议录中，页面
    4365–4374，中国香港，2019年11月。计算语言学协会。'
- en: '[26] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, 和 Song Han。Awq:
    激活感知的权重量化用于大型语言模型的压缩和加速。arXiv 预印本 arXiv:2306.00978，2023年。'
- en: '[27] Ziyang Luo, Artur Kulmizev, and Xiaoxi Mao. Positional artefacts propagate
    through masked language model embeddings. In Chengqing Zong, Fei Xia, Wenjie Li,
    and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers), pages 5312–5327, Online, August 2021\.
    Association for Computational Linguistics.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Ziyang Luo, Artur Kulmizev, 和 Xiaoxi Mao。位置伪影通过掩蔽语言模型嵌入传播。在 Chengqing
    Zong, Fei Xia, Wenjie Li, 和 Roberto Navigli 编辑的《第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）》会议录中，页面
    5312–5327，在线，2021年8月。计算语言学协会。'
- en: '[28] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard Socher。指针哨兵混合模型。arXiv
    预印本 arXiv:1609.07843，2016年。'
- en: '[29] Javaheripi Mojan and Bubeck Sébastien. Phi-2: The surprising power of
    small language models, 2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Javaheripi Mojan 和 Bubeck Sébastien。Phi-2: 小型语言模型的惊人力量，2023年。'
- en: '[30] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko,
    Mart Van Baalen, and Tijmen Blankevoort. A white paper on neural network quantization.
    arXiv preprint arXiv:2106.08295, 2021.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko,
    Mart Van Baalen, 和 Tijmen Blankevoort。关于神经网络量化的白皮书。arXiv 预印本 arXiv:2106.08295，2021年。'
- en: '[31] Sharan Narang, Hyung Won Chung, Yi Tay, Liam Fedus, Thibault Fevry, Michael
    Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou,
    Wei Li, Nan Ding, Jake Marcus, Adam Roberts, and Colin Raffel. Do transformer
    modifications transfer across implementations and applications? In Marie-Francine
    Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing, pages
    5758–5773, Online and Punta Cana, Dominican Republic, November 2021\. Association
    for Computational Linguistics.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Sharan Narang, Hyung Won Chung, Yi Tay, Liam Fedus, Thibault Fevry, Michael
    Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou,
    Wei Li, Nan Ding, Jake Marcus, Adam Roberts, 和 Colin Raffel。变换器修改是否在实现和应用中转移？在
    Marie-Francine Moens, Xuanjing Huang, Lucia Specia, 和 Scott Wen-tau Yih 编辑的《2021年自然语言处理经验方法会议》会议录中，页面
    5758–5773，在线和多米尼加共和国蓬塔卡纳，2021年11月。计算语言学协会。'
- en: '[32] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan
    Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for
    sequence modeling. arXiv preprint arXiv:1904.01038, 2019.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan
    Ng, David Grangier, 和 Michael Auli。fairseq: 一个快速、可扩展的序列建模工具包。arXiv 预印本 arXiv:1904.01038，2019年。'
- en: '[33] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham,
    Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández.
    The LAMBADA dataset: Word prediction requiring a broad discourse context. In Katrin
    Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of the
    Association for Computational Linguistics (Volume 1: Long Papers), pages 1525–1534,
    Berlin, Germany, August 2016\. Association for Computational Linguistics.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham,
    Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, 和 Raquel Fernández。LAMBADA
    数据集：需要广泛话语背景的词预测。在 Katrin Erk 和 Noah A. Smith 编辑的《第54届计算语言学协会年会（第1卷：长篇论文）》会议录中，页面
    1525–1534，德国柏林，2016年8月。计算语言学协会。'
- en: '[34] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James
    Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently
    scaling transformer inference. Proceedings of Machine Learning and Systems, 5,
    2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Reiner Pope、Sholto Douglas、Aakanksha Chowdhery、Jacob Devlin、James Bradbury、Jonathan
    Heek、Kefan Xiao、Shivani Agrawal 和 Jeff Dean。高效扩展变换器推理。机器学习与系统会议论文集，5，2023年。'
- en: '[35] Giovanni Puccetti, Anna Rogers, Aleksandr Drozd, and Felice Dell’Orletta.
    Outlier dimensions that disrupt transformers are driven by frequency. In Yoav
    Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association
    for Computational Linguistics: EMNLP 2022, pages 1286–1304, Abu Dhabi, United
    Arab Emirates, December 2022\. Association for Computational Linguistics.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Giovanni Puccetti, Anna Rogers, Aleksandr Drozd 和 Felice Dell’Orletta.
    影响变换器的异常维度由频率驱动。收录于 Yoav Goldberg、Zornitsa Kozareva 和 Yue Zhang 主编的《计算语言学协会发现：EMNLP
    2022》，第1286–1304页，阿布扎比，阿拉伯联合酋长国，2022年12月。计算语言学协会。'
- en: '[36] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
    Sutskever, et al. Language models are unsupervised multitask learners. OpenAI
    blog, 1(8):9, 2019.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Alec Radford、Jeffrey Wu、Rewon Child、David Luan、Dario Amodei、Ilya Sutskever
    等人。语言模型是无监督的多任务学习者。OpenAI 博客，1(8):9，2019年。'
- en: '[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of machine learning
    research, 21(140):1–67, 2020.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J Liu。探索统一文本到文本变换器的迁移学习极限。机器学习研究期刊，21(140):1–67，2020年。'
- en: '[38] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
    Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint
    arXiv:1907.10641, 2019.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Keisuke Sakaguchi、Ronan Le Bras、Chandra Bhagavatula 和 Yejin Choi。Winogrande：规模化的对抗性
    Winograd 语料库挑战。arXiv 预印本 arXiv:1907.10641，2019年。'
- en: '[39] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian
    Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. arXiv preprint arXiv:2308.13137,
    2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Wenqi Shao、Mengzhao Chen、Zhaoyang Zhang、Peng Xu、Lirui Zhao、Zhiqian Li、Kaipeng
    Zhang、Peng Gao、Yu Qiao 和 Ping Luo。Omniquant：用于大型语言模型的全方向校准量化。arXiv 预印本 arXiv:2308.13137，2023年。'
- en: '[40] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202,
    2020.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Noam Shazeer. Glu 变体改善变换器。arXiv 预印本 arXiv:2002.05202，2020年。'
- en: '[41] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng
    Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing,
    568:127063, 2024.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Jianlin Su、Murtadha Ahmed、Yu Lu、Shengfeng Pan、Wen Bo 和 Yunfeng Liu。Roformer：具有旋转位置嵌入的增强变换器。神经计算，568:127063，2024年。'
- en: '[42] Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. Massive activations
    in large language models. arXiv preprint arXiv:2402.17762, 2024.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Mingjie Sun、Xinlei Chen、J Zico Kolter 和 Zhuang Liu。大型语言模型中的大量激活。arXiv
    预印本 arXiv:2402.17762，2024年。'
- en: '[43] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,
    Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love,
    et al. Gemma: Open models based on gemini research and technology. arXiv preprint
    arXiv:2403.08295, 2024.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Gemma 团队、Thomas Mesnard、Cassidy Hardin、Robert Dadashi、Surya Bhupatiraju、Shreya
    Pathak、Laurent Sifre、Morgane Rivière、Mihir Sanjay Kale、Juliette Love 等人。Gemma：基于双子座研究和技术的开放模型。arXiv
    预印本 arXiv:2403.08295，2024年。'
- en: '[44] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source,
    commercially usable llms, 2023. Accessed: 2023-05-05.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] MosaicML NLP 团队。介绍 mpt-7b：开放源代码、商业可用的 llms 的新标准，2023年。访问时间：2023-05-05。'
- en: '[45] William Timkey and Marten van Schijndel. All bark and no bite: Rogue dimensions
    in transformer language models obscure representational quality. In Marie-Francine
    Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing, pages
    4527–4546, Online and Punta Cana, Dominican Republic, November 2021\. Association
    for Computational Linguistics.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] William Timkey 和 Marten van Schijndel。全口号无实质：变换器语言模型中的异常维度掩盖了表征质量。收录于
    Marie-Francine Moens、Xuanjing Huang、Lucia Specia 和 Scott Wen-tau Yih 主编的《2021年自然语言处理实证方法会议论文集》，第4527–4546页，在线和多米尼加共和国蓬塔卡纳，2021年11月。计算语言学协会。'
- en: '[46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] 乌戈·图弗龙，蒂博·拉夫里尔，戈特耶·伊扎卡尔，泽维尔·马尔蒂内，玛丽-安·拉绍，蒂莫特·拉克鲁瓦，巴蒂斯特·罗齐耶，纳曼·戈亚尔，埃里克·汉布罗，费萨尔·阿扎尔，等。Llama：开放且高效的基础语言模型。arXiv
    预印本 arXiv:2302.13971，2023。'
- en: '[47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] 乌戈·图弗龙，路易斯·马丁，凯文·斯通，彼得·阿尔伯特，阿姆贾德·阿尔马赫里，雅斯敏·巴巴伊，尼古拉·巴什利科夫，苏米亚·巴特拉，普拉吉瓦尔·巴尔加瓦，舒提·博萨尔，等。Llama
    2：开放基础和微调聊天模型。arXiv 预印本 arXiv:2307.09288，2023。'
- en: '[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] 阿希什·瓦斯瓦尼，诺姆·沙泽尔，尼基·帕尔马尔，雅各布·乌斯科雷特，利昂·琼斯，艾丹·N·戈麦斯，卢卡斯·凯泽，和伊利亚·波洛苏欣。注意力即你所需。神经信息处理系统进展，30，2017。'
- en: '[49] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
    Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent
    abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] 杰森·韦，易·泰，瑞希·博马萨尼，科林·拉费尔，巴雷特·佐普，塞巴斯蒂安·博尔戈，达尼·尤加塔玛，马尔滕·博斯马，丹尼·周，唐纳德·梅茨勒，等。大型语言模型的突现能力。arXiv
    预印本 arXiv:2206.07682，2022。'
- en: '[50] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang
    Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language
    models by equivalent and effective shifting and scaling. In Houda Bouamor, Juan
    Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical
    Methods in Natural Language Processing, pages 1648–1665, Singapore, December 2023\.
    Association for Computational Linguistics.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] 徐颖，云辰·张，余航·李，向国张，瑞浩·龚，金扬·郭，和向龙·刘。异常值抑制+：通过等效和有效的移位与缩放对大型语言模型进行精确量化。在霍达·布阿莫尔，胡安·皮诺，和卡利卡·巴利（编辑），2023年自然语言处理实证方法会议论文集，页1648–1665，新加坡，2023年12月。计算语言学协会。'
- en: '[51] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. SmoothQuant: Accurate and efficient post-training quantization for large
    language models. In Proceedings of the 40th International Conference on Machine
    Learning, 2023.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] 广轩·肖，吉·林，米卡埃尔·塞兹内克，郝武，朱利安·德穆斯，和宋寒。SmoothQuant：针对大型语言模型的精确且高效的后训练量化。在第40届国际机器学习大会论文集中，2023。'
- en: '[52] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient
    streaming language models with attention sinks. arXiv preprint arXiv:2309.17453,
    2023.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] 广轩·肖，袁东天，贝迪·陈，宋寒，和迈克·刘易斯。高效的流式语言模型与注意力汇聚。arXiv 预印本 arXiv:2309.17453，2023。'
- en: '[53] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing,
    Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization
    in the transformer architecture. In International Conference on Machine Learning,
    pages 10524–10533\. PMLR, 2020.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] 鲁宾·熊，云畅·杨，狄·赫，凯·郑，舒欣·郑，陈星，惠帅·张，燕燕·兰，李伟·王，和铁岩·刘。在变换器架构中的层归一化。国际机器学习大会论文集，页10524–10533。PMLR，2020。'
- en: '[54] Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He. A comprehensive
    study on post-training quantization for large language models. arXiv preprint
    arXiv:2303.08302, 2023.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] 赵哲伟，李成，吴晓霞，斯蒂芬·杨，和何宇雄。大型语言模型的后训练量化综合研究。arXiv 预印本 arXiv:2303.08302，2023。'
- en: '[55] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. Advances in Neural Information Processing Systems,
    35:27168–27183, 2022.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] 赵哲伟，雷扎·雅兹达尼·阿米纳巴迪，张敏佳，吴晓霞，李从龙，和何宇雄。Zeroquant：针对大规模变换器的高效且经济的后训练量化。神经信息处理系统进展，35:27168–27183，2022。'
- en: '[56] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    HellaSwag: Can a machine really finish your sentence? In Anna Korhonen, David
    Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the
    Association for Computational Linguistics, pages 4791–4800, Florence, Italy, July
    2019\. Association for Computational Linguistics.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] 罗温·泽勒斯，阿里·霍尔茨曼，乔纳坦·比斯克，阿里·法赫迪，和叶津·崔。HellaSwag：机器真的能完成你的句子吗？在安娜·科尔霍宁，大卫·特劳姆，和吕利斯·马尔克斯（编辑），第57届计算语言学协会年会论文集，页4791–4800，意大利佛罗伦萨，2019年7月。计算语言学协会。'
- en: '[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open
    pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] 苏珊·张，斯蒂芬·罗勒，纳曼·戈亚尔，米克尔·阿尔特切，莫雅·陈，舒辉·陈，克里斯托弗·德万，莫娜·迪亚布，谢安·李，维多利亚·林，等。Opt:
    开放预训练变换器语言模型。arXiv 预印本 arXiv:2205.01068，2022年。'
- en: '[58] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng
    Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of
    large language models. arXiv preprint arXiv:2303.18223, 2023.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] 韦恩·辛·赵，昆·周，君毅·李，天毅·唐，晓磊·王，宇鹏·侯，莹千·闵，贝辰·张，俊杰·张，自灿·董，等。大规模语言模型综述。arXiv 预印本
    arXiv:2303.18223，2023年。'
- en: Appendix A Additional Calibration Results
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 额外的校准结果
- en: In this section, we provide details of LLMs when performing calibration, which
    is the step during quantization where the FP16 ranges are computed (Appendix [A.1](#A1.SS1
    "A.1 Detailed Specification of LLMs ‣ Appendix A Additional Calibration Results
    ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs")),
    and additional calibration results (Appendix [A.2](#A1.SS2 "A.2 Other Calibration
    Results on GLU-implementation ‣ Appendix A Additional Calibration Results ‣ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs"),  [A.3](#A1.SS3
    "A.3 Other Calibration Results on Non GLU-implementation ‣ Appendix A Additional
    Calibration Results ‣ Mitigating Quantization Errors Due to Activation Spikes
    in GLU-Based LLMs")).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了执行校准时LLMs的详细信息，这是在量化过程中计算FP16范围的步骤（附录[A.1](#A1.SS1 "A.1 LLM的详细规范 ‣
    附录 A 额外的校准结果 ‣ 减轻由于 GLU 基于 LLM 的量化误差")），以及额外的校准结果（附录[A.2](#A1.SS2 "A.2 GLU 实现的其他校准结果
    ‣ 附录 A 额外的校准结果 ‣ 减轻由于 GLU 基于 LLM 的量化误差")，[A.3](#A1.SS3 "A.3 非 GLU 实现的其他校准结果 ‣
    附录 A 额外的校准结果 ‣ 减轻由于 GLU 基于 LLM 的量化误差")）。
- en: A.1 Detailed Specification of LLMs
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 LLM的详细规范
- en: 'In Section [3.1](#S3.SS1 "3.1 Existence of Activation Spikes in GLU Variants
    ‣ 3 Activation Spikes: Excessive Magnitude of GLU Activations ‣ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs"), we have performed the calibration
    method on various LLMs. We observe the calibration results by categorizing based
    on the presence of GLU in the LLMs. Table [6](#A1.T6 "Table 6 ‣ A.1 Detailed Specification
    of LLMs ‣ Appendix A Additional Calibration Results ‣ Mitigating Quantization
    Errors Due to Activation Spikes in GLU-Based LLMs") shows the detailed structures
    of the LLMs. We refer notations for feed-forward implementiation from [[40](#bib.bib40)].
    In the case of GLU-implemented LLMs, which is LLaMA-2, LLaMA-3, Mistral, Mixtral,
    SOLAR, StableLM-2, and Gemma, most models have SwiGLU for FFN activation, while
    only Gemma has GeGLU. On the other hand, in non GLU-implemented LLMs, most of
    them utilize GeLU for FFN activation, with the exception of OPT, which uses ReLU.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '在[3.1](#S3.SS1 "3.1 GLU 变体中的激活峰值的存在 ‣ 3 激活峰值: GLU 激活的过度幅度 ‣ 减轻由于 GLU 基于 LLM
    的量化误差")节中，我们对各种LLMs进行了校准方法的应用。我们通过对LLMs中的GLU存在情况进行分类来观察校准结果。表[6](#A1.T6 "表 6 ‣
    A.1 LLM的详细规范 ‣ 附录 A 额外的校准结果 ‣ 减轻由于 GLU 基于 LLM 的量化误差")显示了LLMs的详细结构。我们参考了[[40](#bib.bib40)]中的前馈实现符号。对于实施GLU的LLMs，如LLaMA-2，LLaMA-3，Mistral，Mixtral，SOLAR，StableLM-2
    和 Gemma，大多数模型的FFN激活使用SwiGLU，只有Gemma使用GeGLU。另一方面，在未实施GLU的LLMs中，大多数模型使用GeLU进行FFN激活，唯一的例外是OPT，它使用ReLU。'
- en: 'Table 6: Architecture specification of LLMs. We categorize them into two groups
    depending on whether GLU is implemented in the FFN. All LLMs in the table use
    Pre-LN for the LayerNorm position.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: LLM 的架构规范。我们将它们分为两组，取决于 FFN 中是否实现了 GLU。表中的所有LLMs都使用 Pre-LN 进行 LayerNorm
    位置的处理。'
- en: '| Model | Size | FFN Activation | Normalization | PE | Vocabulary Size |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 | FFN 激活 | 归一化 | PE | 词汇表大小 |'
- en: '| GLU-implemented LLMs: |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 实施GLU的LLMs: |'
- en: '| LLaMA-2 [[47](#bib.bib47)] | 7B, 13B, 70B | SwiGLU | RMSNorm | RoPE | 32000
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2 [[47](#bib.bib47)] | 7B, 13B, 70B | SwiGLU | RMSNorm | RoPE | 32000
    |'
- en: '| LLaMA-3 | 8B, 70B | SwiGLU | RMSNorm | RoPE | 128256 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3 | 8B, 70B | SwiGLU | RMSNorm | RoPE | 128256 |'
- en: '| Mistral [[20](#bib.bib20)] | 7B | SwiGLU | RMSNorm | RoPE | 32000 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| Mistral [[20](#bib.bib20)] | 7B | SwiGLU | RMSNorm | RoPE | 32000 |'
- en: '| Mixtral [[21](#bib.bib21)] | 8x7B | SwiGLU | RMSNorm | RoPE | 32000 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral [[21](#bib.bib21)] | 8x7B | SwiGLU | RMSNorm | RoPE | 32000 |'
- en: '| SOLAR [[22](#bib.bib22)] | 10.7B | SwiGLU | RMSNorm | RoPE | 32000 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| SOLAR [[22](#bib.bib22)] | 10.7B | SwiGLU | RMSNorm | RoPE | 32000 |'
- en: '| StableLM-2 [[5](#bib.bib5)] | 12B | SwiGLU | LayerNorm | RoPE | 100352 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| StableLM-2 [[5](#bib.bib5)] | 12B | SwiGLU | LayerNorm | RoPE | 100352 |'
- en: '| Gemma [[43](#bib.bib43)] | 7B | GeGLU | RMSNorm | RoPE | 256000 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| Gemma [[43](#bib.bib43)] | 7B | GeGLU | RMSNorm | RoPE | 256000 |'
- en: '| Non GLU-implemented LLMs: |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 非 GLU 实现的 LLMs: |  |'
- en: '| OPT [[57](#bib.bib57)] | 6.7B, 13B, 30B, 66B | ReLU | LayerNorm | Learned
    | 50272 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| OPT [[57](#bib.bib57)] | 6.7B, 13B, 30B, 66B | ReLU | LayerNorm | Learned
    | 50272 |'
- en: '| MPT [[44](#bib.bib44)] | 7B, 30B | GeLU | LayerNorm | ALiBi | 50432 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| MPT [[44](#bib.bib44)] | 7B, 30B | GeLU | LayerNorm | ALiBi | 50432 |'
- en: '| Pythia [[6](#bib.bib6)] | 6.9B, 12B | GeLU | LayerNorm | RoPE | 50432, 50688
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Pythia [[6](#bib.bib6)] | 6.9B, 12B | GeLU | LayerNorm | RoPE | 50432, 50688
    |'
- en: '| Falcon [[3](#bib.bib3)] | 7B, 40B | GeLU | LayerNorm | RoPE | 65024 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Falcon [[3](#bib.bib3)] | 7B, 40B | GeLU | LayerNorm | RoPE | 65024 |'
- en: '| Phi-2 [[29](#bib.bib29)] | 2.7B | GeLU | LayerNorm | RoPE | 51200 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Phi-2 [[29](#bib.bib29)] | 2.7B | GeLU | LayerNorm | RoPE | 51200 |'
- en: A.2 Other Calibration Results on GLU-implementation
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 其他 GLU 实现的校准结果
- en: 'Figure [8](#A1.F8 "Figure 8 ‣ A.2 Other Calibration Results on GLU-implementation
    ‣ Appendix A Additional Calibration Results ‣ Mitigating Quantization Errors Due
    to Activation Spikes in GLU-Based LLMs"), [9](#A1.F9 "Figure 9 ‣ A.2 Other Calibration
    Results on GLU-implementation ‣ Appendix A Additional Calibration Results ‣ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs") show the calibration
    result examples for various GLU-implemented LLMs that are not shown in the models
    in Figure [1a](#S3.F1 "Figure 1 ‣ 3 Activation Spikes: Excessive Magnitude of
    GLU Activations ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs"). In most GLU-implemented LLMs, we observe that the input activations have
    large values near the first and last layers. Unlike the typical GLU-implemented
    LLM architecture, Mixtral is composed of 8 feed-forward blocks in the single FFN,
    containing multiple gate linear units [[21](#bib.bib21)]. According to this structure,
    we can observe that one of the gates spikes in value in Figure [8](#A1.F8 "Figure
    8 ‣ A.2 Other Calibration Results on GLU-implementation ‣ Appendix A Additional
    Calibration Results ‣ Mitigating Quantization Errors Due to Activation Spikes
    in GLU-Based LLMs").'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [8](#A1.F8 "图 8 ‣ A.2 其他 GLU 实现的校准结果 ‣ 附录 A 额外的校准结果 ‣ 减少 GLU 基础的 LLMs 中由于激活峰值引起的量化误差")，[9](#A1.F9
    "图 9 ‣ A.2 其他 GLU 实现的校准结果 ‣ 附录 A 额外的校准结果 ‣ 减少 GLU 基础的 LLMs 中由于激活峰值引起的量化误差") 显示了各种
    GLU 实现的 LLMs 的校准结果示例，这些示例在图 [1a](#S3.F1 "图 1 ‣ 3 激活峰值：GLU 激活的过大幅度 ‣ 减少 GLU 基础的
    LLMs 中由于激活峰值引起的量化误差") 中未显示。在大多数 GLU 实现的 LLMs 中，我们观察到输入激活在第一层和最后一层附近有较大的值。与典型的
    GLU 实现的 LLM 架构不同，Mixtral 由一个 FFN 中的 8 个前馈块组成，包含多个门控线性单元 [[21](#bib.bib21)]。根据这一结构，我们可以观察到图
    [8](#A1.F8 "图 8 ‣ A.2 其他 GLU 实现的校准结果 ‣ 附录 A 额外的校准结果 ‣ 减少 GLU 基础的 LLMs 中由于激活峰值引起的量化误差")
    中一个门控的值峰值。
- en: '![Refer to caption](img/18e4d4fbd4246b30523f71181f0eaba4.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/18e4d4fbd4246b30523f71181f0eaba4.png)'
- en: 'Figure 8: Calibration results on GLU-implemented LLMs (Mixtral-8x7B).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：GLU 实现的 LLMs (Mixtral-8x7B) 的校准结果。
- en: '![Refer to caption](img/2c55f586c7e015b0141efb77032245fd.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2c55f586c7e015b0141efb77032245fd.png)'
- en: 'Figure 9: Calibration results on GLU-implemented LLMs.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：GLU 实现的 LLMs 的校准结果。
- en: '![Refer to caption](img/63f769b9bc230b00d39022af4e8a43b5.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/63f769b9bc230b00d39022af4e8a43b5.png)'
- en: 'Figure 10: Calibration results on Non GLU-implemented LLMs.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：非 GLU 实现的 LLMs 的校准结果。
- en: A.3 Other Calibration Results on Non GLU-implementation
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 其他非 GLU 实现的校准结果
- en: 'Figure [10](#A1.F10 "Figure 10 ‣ A.2 Other Calibration Results on GLU-implementation
    ‣ Appendix A Additional Calibration Results ‣ Mitigating Quantization Errors Due
    to Activation Spikes in GLU-Based LLMs") shows the calibration result examples
    for various non GLU-implemented LLMs that were not shown in the models in Figure [1b](#S3.F1
    "Figure 1 ‣ 3 Activation Spikes: Excessive Magnitude of GLU Activations ‣ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs"). There are no
    activation spikes on non GLU-implemented LLMs.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [10](#A1.F10 "图 10 ‣ A.2 其他 GLU 实现的校准结果 ‣ 附录 A 额外的校准结果 ‣ 减少 GLU 基础的 LLMs 中由于激活峰值引起的量化误差")
    显示了各种非 GLU 实现的 LLMs 的校准结果示例，这些示例在图 [1b](#S3.F1 "图 1 ‣ 3 激活峰值：GLU 激活的过大幅度 ‣ 减少
    GLU 基础的 LLMs 中由于激活峰值引起的量化误差") 中未显示。在非 GLU 实现的 LLMs 中没有激活峰值。
- en: Appendix B BMM Quantization
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B BMM 量化
- en: To achieve faster inference latency, BMM operations in the self-attention also
    can be computed as INT8 operation [[51](#bib.bib51)]. This requires a quantization
    on the query, key, and value states including the cached context. Because activation
    spikes produce a large magnitude of latent values, it is important to confirm
    the extent of quantization errors from KV quantization. This confirmation is necessary
    to gain advantages from BMM quantization. In Table [7](#A2.T7 "Table 7 ‣ Appendix
    B BMM Quantization ‣ Mitigating Quantization Errors Due to Activation Spikes in
    GLU-Based LLMs"), we examine the impact of BMM quantization on the W8A8 and QFeM.
    Regardless of the BMM quantization, the QFeM method consistently improves the
    quantization bottleneck. For example, the 13B and 70B models maintain their performance,
    while the 7B model shows a slight decrease. However, this decrease appears to
    be due to inherent quantization errors rather than a quantization bottleneck from
    activation spikes. As a result, we confirm that our QFeM method effectively improves
    the overall performance even in the BMM quantization scenario.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现更快的推理延迟，自注意力中的 BMM 操作也可以计算为 INT8 操作 [[51](#bib.bib51)]。这要求对查询、键和值状态，包括缓存的上下文，进行量化。由于激活峰值产生的大幅度潜在值，重要的是确认
    KV 量化的量化误差程度。这种确认是为了从 BMM 量化中获得优势。在表 [7](#A2.T7 "表 7 ‣ 附录 B BMM 量化 ‣ 缓解 GLU 基于
    LLM 的激活峰值引起的量化误差")中，我们检查了 BMM 量化对 W8A8 和 QFeM 的影响。不论 BMM 量化如何，QFeM 方法始终能改善量化瓶颈。例如，13B
    和 70B 模型保持了其性能，而 7B 模型显示出轻微的下降。然而，这种下降似乎是由于固有的量化误差，而不是由于激活峰值引起的量化瓶颈。因此，我们确认我们的
    QFeM 方法在 BMM 量化场景中有效提高了整体性能。
- en: 'Table 7: BMM quantization results.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: BMM 量化结果。'
- en: '| Model | Method | BMM Quantization |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | BMM量化 |'
- en: '| No | Yes |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 否 | 是 |'
- en: '| 7B | W8A8 | 62.08% | 61.66% |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 7B | W8A8 | 62.08% | 61.66% |'
- en: '| +QFeP | 68.69% | 68.30% |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 68.69% | 68.30% |'
- en: '| 13B | W8A8 | 55.29% | 55.43% |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 13B | W8A8 | 55.29% | 55.43% |'
- en: '| +QFeP | 69.91% | 69.77% |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 69.91% | 69.77% |'
- en: '| 70B | W8A8 | 66.87% | 66.75% |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 70B | W8A8 | 66.87% | 66.75% |'
- en: '| +QFeP | 72.62% | 72.69% |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 72.62% | 72.69% |'
- en: Appendix C Supplementary Experiment Results
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 补充实验结果
- en: C.1 Additional Results for Combining Outlier Alleviation Methods
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 额外结果：组合异常值缓解方法
- en: In Table [8](#A3.T8 "Table 8 ‣ C.1 Additional Results for Combining Outlier
    Alleviation Methods ‣ Appendix C Supplementary Experiment Results ‣ Mitigating
    Quantization Errors Due to Activation Spikes in GLU-Based LLMs"), we provide additional
    results for Section [5.3](#S5.SS3 "5.3 Combining Outlier Alleviation Methods ‣
    5 Experiments ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs") with coarse-grained quantization (i.e., per-tensor quantization) scheme
    for weight quantization. Compared to the results obtained with per-channel weight
    quantization in Table [4](#S5.T4 "Table 4 ‣ 5.3 Combining Outlier Alleviation
    Methods ‣ 5 Experiments ‣ Mitigating Quantization Errors Due to Activation Spikes
    in GLU-Based LLMs"), these results elucidate the negative impact of activation
    spikes on the performance of outlier alleviation methods. Furthermore, this suggests
    that the performance of OSP method resort to the weight quantization scheme. Nevertheless,
    the proposed methods, QFeM and QFeP, consistently improve the effectiveness of
    outlier alleviation methods by mitigating the impact of activation spikes.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [8](#A3.T8 "表 8 ‣ C.1 额外结果：组合异常值缓解方法 ‣ 附录 C 补充实验结果 ‣ 缓解 GLU 基于 LLM 的激活峰值引起的量化误差")中，我们提供了第
    [5.3](#S5.SS3 "5.3 组合异常值缓解方法 ‣ 5 实验 ‣ 缓解 GLU 基于 LLM 的激活峰值引起的量化误差") 节的粗粒度量化（即每张量量化）方案的额外结果。与表
    [4](#S5.T4 "表 4 ‣ 5.3 组合异常值缓解方法 ‣ 5 实验 ‣ 缓解 GLU 基于 LLM 的激活峰值引起的量化误差") 中每通道权重量化得到的结果相比，这些结果阐明了激活峰值对异常值缓解方法性能的负面影响。此外，这表明
    OSP 方法的性能依赖于权重量化方案。然而，所提出的方法 QFeM 和 QFeP 一致地通过减轻激活峰值的影响来提高异常值缓解方法的有效性。
- en: 'Table 8: Evaluation of outlier alleviation methods with QFeM and QFeP. We report
    perplexity on WikiText-2 and averaged accuracy of four zero-shot tasks. Compared
    to Table [4](#S5.T4 "Table 4 ‣ 5.3 Combining Outlier Alleviation Methods ‣ 5 Experiments
    ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs"),
    per-tensor weight quantization and dynamic per-tensor activation quantization
    are used.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：使用 QFeM 和 QFeP 评估异常值缓解方法。我们报告了 WikiText-2 上的困惑度和四个零样本任务的平均准确度。与表[4](#S5.T4
    "表 4 ‣ 5.3 结合异常值缓解方法 ‣ 5 实验 ‣ 缓解 GLU 基于 LLM 中激活突增的量化误差")相比，使用了每个张量权重量化和动态每个张量激活量化。
- en: '| Method | LLaMA-2-7B | LLaMA-2-13B | LLaMA-2-70B |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LLaMA-2-7B | LLaMA-2-13B | LLaMA-2-70B |'
- en: '| --- | --- | --- | --- |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ppl($\downarrow$) |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| ppl($\downarrow$) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SQ [[51](#bib.bib51)] | 24.661 | 56.87% | 120.966 | 53.06% | 8.435 | 67.08%
    |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| SQ [[51](#bib.bib51)] | 24.661 | 56.87% | 120.966 | 53.06% | 8.435 | 67.08%
    |'
- en: '| +QFeM | 6.016 | 67.74% | 5.464 | 70.04% | 4.015 | 74.18% |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| +QFeM | 6.016 | 67.74% | 5.464 | 70.04% | 4.015 | 74.18% |'
- en: '| +QFeP | 6.122 | 67.22% | 10.473 | 68.17% | 5.998 | 72.54% |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 6.122 | 67.22% | 10.473 | 68.17% | 5.998 | 72.54% |'
- en: '| OSP [[50](#bib.bib50)] | 9.131 | 63.61% | 8.997 | 64.03% | 6.492 | 71.13%
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| OSP [[50](#bib.bib50)] | 9.131 | 63.61% | 8.997 | 64.03% | 6.492 | 71.13%
    |'
- en: '| +QFeM | 5.951 | 68.65% | 5.284 | 70.67% | 4.434 | 73.30% |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| +QFeM | 5.951 | 68.65% | 5.284 | 70.67% | 4.434 | 73.30% |'
- en: '| +QFeP | 5.821 | 68.25% | 5.868 | 67.96% | 4.976 | 73.57% |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| +QFeP | 5.821 | 68.25% | 5.868 | 67.96% | 4.976 | 73.57% |'
- en: Appendix D Miscellaneous
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 杂项
- en: D.1 Transformer Architecture.
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 变换器架构。
- en: In Figure [11](#A4.F11 "Figure 11 ‣ D.1 Transformer Architecture. ‣ Appendix
    D Miscellaneous ‣ Mitigating Quantization Errors Due to Activation Spikes in GLU-Based
    LLMs"), we illustrate the Pre-LN transformer architecture and each sub-modules.
    We highlight with the same color the linear modules that accept identical input
    activations. Note that the hidden states are normalized before forwarding into
    the query and up linear modules.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[11](#A4.F11 "图 11 ‣ D.1 Transformer 架构 ‣ 附录 D 杂项 ‣ 缓解 GLU 基于 LLM 中激活突增的量化误差")中，我们展示了
    Pre-LN 变换器架构及其各个子模块。我们用相同的颜色突出显示了接受相同输入激活的线性模块。注意隐藏状态在传入查询和上升线性模块之前已经过标准化。
- en: '![Refer to caption](img/1635d20e175e23a7191582e614eb0f5d.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1635d20e175e23a7191582e614eb0f5d.png)'
- en: 'Figure 11: An illustration of Pre-LN transformer block and its sub-modules.
    Two feed-forward implementation, GLU and Non-GLU, are visualized in (c) and (d)
    respectively. In feed-forward network, $\sigma$ denotes non-linear activation
    function, such as GeLU. We highlight the linear modules where input activations
    are quantized.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：Pre-LN 变换器块及其子模块的示意图。两个前馈实现，GLU 和非 GLU，分别在 (c) 和 (d) 中可视化。在前馈网络中，$\sigma$
    表示非线性激活函数，如 GeLU。我们突出显示了输入激活被量化的线性模块。
- en: D.2 Additional Results for Token-level Scale Analysis
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 令牌级别规模分析的附加结果
- en: 'We provide additional results for token-level scale analysis (Section [3.2](#S3.SS2
    "3.2 Token-level Scale Analysis within Activation Spikes ‣ 3 Activation Spikes:
    Excessive Magnitude of GLU Activations ‣ Mitigating Quantization Errors Due to
    Activation Spikes in GLU-Based LLMs")). In Figure [12](#A4.F12 "Figure 12 ‣ D.2
    Additional Results for Token-level Scale Analysis ‣ Appendix D Miscellaneous ‣
    Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs") and
    Figure [13](#A4.F13 "Figure 13 ‣ D.2 Additional Results for Token-level Scale
    Analysis ‣ Appendix D Miscellaneous ‣ Mitigating Quantization Errors Due to Activation
    Spikes in GLU-Based LLMs"), the token for the activation spikes behind the BOS
    token does not exhibit the excessive activation scale.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了令牌级别规模分析的附加结果（第[3.2](#S3.SS2 "3.2 激活突增中的令牌级别规模分析 ‣ 3 激活突增：GLU 激活的过度幅度 ‣
    缓解 GLU 基于 LLM 中激活突增的量化误差")节）。在图[12](#A4.F12 "图 12 ‣ D.2 令牌级别规模分析的附加结果 ‣ 附录 D 杂项
    ‣ 缓解 GLU 基于 LLM 中激活突增的量化误差")和图[13](#A4.F13 "图 13 ‣ D.2 令牌级别规模分析的附加结果 ‣ 附录 D 杂项
    ‣ 缓解 GLU 基于 LLM 中激活突增的量化误差")中，BOS 令牌后的激活突增令牌没有表现出过度的激活规模。
- en: '![Refer to caption](img/faddb4c3d886a8cca8553da637ea143e.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/faddb4c3d886a8cca8553da637ea143e.png)'
- en: 'Figure 12: Token-wise scales analysis for LLaMA-2-7B. The newline token behind
    the BOS token does not exhibit the activation spikes.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：LLaMA-2-7B 的令牌级别规模分析。BOS 令牌后的换行令牌没有表现出激活突增。
- en: '![Refer to caption](img/f44156b26431f483d384b9e6352a6142.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f44156b26431f483d384b9e6352a6142.png)'
- en: 'Figure 13: Token-wise scales from the unrolled activation spike of LLaMA-2-70B.
    The apostrophe token behind the BOS token does not exhibit the activation spikes.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：来自LLaMA-2-70B展开激活峰值的逐词尺度。BOS标记后面的撇号标记没有表现出激活峰值。
