- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:05:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:05:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FBI-LLM：通过自回归蒸馏从零开始扩展完全二值化LLMs
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.07093](https://ar5iv.labs.arxiv.org/html/2407.07093)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.07093](https://ar5iv.labs.arxiv.org/html/2407.07093)
- en: Liqun Ma¹,  Mingjie Sun², Zhiqiang Shen¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Liqun Ma¹,  Mingjie Sun², Zhiqiang Shen¹
- en: ¹Mohamed bin Zayed University of AI
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹穆罕默德·本·扎耶德人工智能大学
- en: ²Carnegie Mellon University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²卡内基梅隆大学
- en: '{Liqun.Ma,Zhiqiang.Shen}@mbzuai.ac.ae, mingjies@andrew.cmu.edu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{Liqun.Ma,Zhiqiang.Shen}@mbzuai.ac.ae, mingjies@andrew.cmu.edu'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This work presents a Fully BInarized Large Language Model (FBI-LLM), demonstrating
    for the first time how to train a large-scale binary language model from scratch
    (not the partial binary or ternary LLM like BitNet b1.58 [[1](#bib.bib1)]) to
    match the performance of its full-precision counterparts (e.g., FP16 or BF16)
    in transformer-based LLMs. It achieves this by employing an autoregressive distillation
    (AD) loss with maintaining equivalent model dimensions (130M, 1.3B, 7B) and training
    data volume as regular LLM pretraining, while delivering competitive results in
    terms of perplexity and task-specific effectiveness. Intriguingly, by analyzing
    the training trajectory, we find that the pretrained weight is not necessary for
    training binarized LLMs from scratch. This research encourages a new computational
    framework and may facilitate the future design of specialized hardware tailored
    for fully 1-bit LLMs. We make all our models, code, and training dataset fully
    accessible and transparent to support further research¹¹1Code: [https://github.com/LiqunMa/FBI-LLM](https://github.com/LiqunMa/FBI-LLM).
    Model: [https://huggingface.co/LiqunMa/](https://huggingface.co/LiqunMa/)..'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究介绍了一种完全二值化的大型语言模型（FBI-LLM），首次展示了如何从零开始训练一个大规模二值语言模型（不同于如BitNet b1.58 [[1](#bib.bib1)]的部分二值或三值LLM），以匹配其全精度对应物（如FP16或BF16）在基于变换器的LLMs中的性能。它通过采用具有相同模型维度（130M、1.3B、7B）和训练数据量的自回归蒸馏（AD）损失实现这一点，同时在困惑度和任务特定有效性方面提供了具有竞争力的结果。有趣的是，通过分析训练轨迹，我们发现预训练的权重对于从零开始训练二值化LLMs并不是必要的。这项研究鼓励了一种新的计算框架，并可能促进未来专门针对完全1位LLMs的硬件设计。我们使所有模型、代码和训练数据集完全开放和透明，以支持进一步的研究¹¹1代码：[https://github.com/LiqunMa/FBI-LLM](https://github.com/LiqunMa/FBI-LLM)。模型：[https://huggingface.co/LiqunMa/](https://huggingface.co/LiqunMa/)..
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/88eede1f46008b331fe980692e84e56d.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/88eede1f46008b331fe980692e84e56d.png)'
- en: 'Figure 1: Perplexity on Wikitext2 of existing binarized LLMs and our FBI-LLMs.
    FBI-LLMs get similar or lower magnitude of perplexity on similar size of models
    compared with other binarized LLMs.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：现有二值化LLMs和我们的FBI-LLMs在Wikitext2上的困惑度。FBI-LLMs在类似规模的模型中获得了与其他二值化LLMs相似或更低的困惑度。
- en: Benefiting from the huge parameter scale and massive training corpora, transformer-based
    Large Language Models (LLMs), like ChatGPT [[2](#bib.bib2)] and LLaMA [[3](#bib.bib3),
    [4](#bib.bib4)], perform great in tasks requiring domain knowledge and complex
    reasoning. Moreover, the capabilities of LLMs tend to enhance as their parameter
    sizes expand. This substantial scale in parameters results in considerable storage
    and computational demands, which substantially restricts LLMs’ broader application
    and development. Quantization efficiently mitigates these limitations by mapping
    32-bit parameters to smaller bit sizes. It substantially cuts storage requirements
    and enhances computational speed and energy efficiency during inference.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 得益于巨大的参数规模和海量的训练语料库，基于变换器的语言模型（LLMs），如ChatGPT [[2](#bib.bib2)]和LLaMA [[3](#bib.bib3),
    [4](#bib.bib4)]，在需要领域知识和复杂推理的任务中表现出色。此外，LLMs的能力往往随着参数规模的扩大而增强。这种参数规模的显著增加导致了大量的存储和计算需求，从而大大限制了LLMs的更广泛应用和发展。量化通过将32位参数映射到更小的位大小，有效地缓解了这些限制。它显著减少了存储需求，并在推理过程中提高了计算速度和能效。
- en: As the most extreme case of quantization, binarization represents each parameter
    by just {-1, 1}. It maximizes compression and efficiency but at the cost of accuracy.
    Prior efforts to preserve the efficacy of binarized LLMs include retaining salient
    parameters [[5](#bib.bib5)] or using near-one-bit to represent each parameter [[1](#bib.bib1)].
    While these approaches have shown promise, they still leave room for optimization
    in storage and efficiency, and additional full-precision parameters or parameter
    encodings expressed in non-powers of 2 can cause extra overhead when adapting
    to edge hardware.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为量化的极端情况，二值化仅用{-1, 1}表示每个参数。它最大化了压缩和效率，但以准确性为代价。之前为了保留二值化LLM的有效性而做出的努力包括保留显著参数[[5](#bib.bib5)]或使用接近一位的方式表示每个参数[[1](#bib.bib1)]。尽管这些方法已显示出希望，但在存储和效率方面仍有优化空间，并且额外的全精度参数或以非2的幂表示的参数编码可能在适应边缘硬件时导致额外的开销。
- en: '![Refer to caption](img/81aae89708cd0389cc9ba1cf0e520cf4.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/81aae89708cd0389cc9ba1cf0e520cf4.png)'
- en: 'Figure 2: Illustration of the FBI-LLM framework. We take the structure of LLaMA
    as an example. Left: the LLM block with the proposed FBI-Linear using learnable
    $\bm{\alpha}$. Right: our autoregressive distillation and model pertaining procedure.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：FBI-LLM框架的示意图。我们以LLaMA的结构为例。左：具有建议的FBI-Linear并使用可学习的$\bm{\alpha}$的LLM块。右：我们的自回归蒸馏和模型相关程序。
- en: 'Some works on fully binarized LLMs are based on the optimization goal of minimizing
    the layer-wise $\ell_{2}$ loss [[6](#bib.bib6), [5](#bib.bib5)] or performing
    binarization while continuing training a full-precision LLM with a small amount
    of data [[7](#bib.bib7)]. These methods face several issues: 1) The binarization
    process greatly compresses the parameter space of the original model, damaging
    some of the knowledge stored in the full-precision model. Adequate training data
    is needed to allow the binarized model to relearn this knowledge and adapt it
    to the pattern of binarized parameters; 2) Deriving binarized models from existing
    pretrained models does not allow for the selection of different parameter scales
    or vocabulary sizes, limiting their flexibility and practical application.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关于完全二值化的LLM的工作基于优化目标，即最小化层级$\ell_{2}$损失[[6](#bib.bib6), [5](#bib.bib5)]，或者在使用少量数据继续训练全精度LLM的同时进行二值化[[7](#bib.bib7)]。这些方法面临几个问题：1）二值化过程大大压缩了原始模型的参数空间，损坏了全精度模型中存储的一部分知识。需要足够的训练数据，以便二值化模型能够重新学习这些知识并将其适应于二值化参数的模式；2）从现有的预训练模型中导出二值化模型不允许选择不同的参数尺度或词汇大小，限制了其灵活性和实际应用。
- en: In this work, we propose a streamlined process for training Fully BInarized
    LLMs from scratch, termed FBI-LLM. To enable stable training of binary LLMs from
    scratch, we propose a novel training procedure based on distillation. Specifically,
    during training, we gradually distill from a full-precision teacher and adopt
    an autoregressive distillation-based scheme to match the predicted probabilities
    of the teacher model at each token location. With this simple autoregressive distillation
    loss, we can successfully train binarized LLMs from random initializations. Since
    our modifications are focused on the loss function, FBI-LLM can be easily incorporated
    into the existing LLM pre-training pipeline. Moreover, the binarization operation
    is decoupled from model training in this method, thus any techniques that enhance
    LLM training efficiency can be directly adapted for FBI-LLM.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种从头开始训练完全二值化LLM的简化过程，称为FBI-LLM。为了实现二值化LLM的稳定训练，我们提出了一种基于蒸馏的新训练程序。具体而言，在训练过程中，我们逐步从全精度教师模型进行蒸馏，并采用基于自回归蒸馏的方案，以匹配教师模型在每个标记位置的预测概率。通过这种简单的自回归蒸馏损失，我们可以成功地从随机初始化中训练二值化LLM。由于我们的修改集中在损失函数上，因此FBI-LLM可以轻松地融入现有的LLM预训练管道。此外，这种方法中的二值化操作与模型训练解耦，因此任何增强LLM训练效率的技术都可以直接适应FBI-LLM。
- en: 'We empirically evaluate the effectiveness of our framework FBI-LLM, where we
    trained models with sizes ranging from 130M, 1.3B, to 7B. We use the widely-used
    Transformer architecture for LLMs, as can be seen in Fig. [2](#S1.F2 "Figure 2
    ‣ 1 Introduction ‣ FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive
    Distillation"). We show that we can train fully binarized LLMs from scratch, with
    a small performance gap as compared to full-precision counterparts. Compared to
    baseline methods, our training process leads to fully binarized LLMs with better
    performance on perplexity (as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation"))
    and multiple downstream tasks. We show that autoregressive distillation is key
    to training binarized LLMs. Further, analysis of pretraining checkpoints (e.g.,
    flip-flop ratio and gradient norms) suggests there is no major difference between
    inheriting the weights from full-precision LLMs and training binarized LLMs from
    scratch.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过实证评估了我们的框架FBI-LLM的有效性，其中我们训练了大小从130M、1.3B到7B的模型。我们使用了广泛使用的Transformer架构用于LLM，如图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch
    via Autoregressive Distillation")所示。我们展示了我们可以从头开始训练完全二值化的LLMs，与全精度模型相比，性能差距很小。与基线方法相比，我们的训练过程产生了在困惑度（如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch
    via Autoregressive Distillation")）和多个下游任务上表现更好的完全二值化LLMs。我们表明，自回归蒸馏是训练二值化LLMs的关键。此外，对预训练检查点（例如，翻转比例和梯度范数）的分析表明，从全精度LLMs继承权重与从头训练二值化LLMs之间没有重大区别。'
- en: 'Overall, the contribution of this paper can be summarized as follows: first,
    we demonstrate for the first time that we can successfully train LLMs with binary
    weights from scratch; second, we propose a novel loss formulation for stabilize
    the training of binarized LLMs, where we adopt autoregressive distillation to
    match the probability distribution of a teacher model; third, we conduct extensive
    experimental and analysis to demonstrate and better understand the effectiveness
    of our method.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，本文的贡献可以总结如下：首先，我们首次展示了可以成功地从头训练具有二值权重的LLMs；其次，我们提出了一种新颖的损失公式来稳定二值化LLMs的训练，其中我们采用自回归蒸馏来匹配教师模型的概率分布；第三，我们进行广泛的实验和分析，以展示和更好地理解我们方法的有效性。
- en: 2 Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Neural Network Binarization. Binarization, the most extreme form of network
    quantization, converts model parameters into a 1-bit format. Many studies have
    focused on Binary Neural Networks (BNNs) to improve their accuracy despite inherent
    limitations. BinaryConnect [[8](#bib.bib8)] converts full-precision weights in
    neural networks to 1-bit binary weights using stochastic methods during training
    and simulates the effects of binary weights during inference. They also implement
    a clipping function in backward propagation to prevent excessive growth of real-valued
    weights. Expanding on this, they develop the Binarized Neural Network (BNN) [[9](#bib.bib9)],
    which includes detailed training and acceleration techniques, demonstrating the
    efficiency and practicality of BNNs through reduced storage and faster processing
    times in image classification.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络二值化。二值化是网络量化的最极端形式，将模型参数转换为1位格式。许多研究集中在二值神经网络（BNNs）上，以提高其准确性，尽管存在固有的限制。BinaryConnect
    [[8](#bib.bib8)] 在训练过程中使用随机方法将神经网络中的全精度权重转换为1位二值权重，并在推理过程中模拟二值权重的效果。他们还在反向传播中实现了一个裁剪功能，以防止实值权重过度增长。在此基础上，他们开发了二值神经网络（BNN）[[9](#bib.bib9)]，其中包括详细的训练和加速技术，通过减少存储和加快图像分类处理速度展示了BNNs的效率和实用性。
- en: However, these methods typically suffer from accuracy loss, prompting numerous
    optimization-based solutions over recent years to mitigate this. Binary Weight
    Networks (BWN) and XNOR-Net [[10](#bib.bib10)] introduce a scaling factor that
    approximates floating-point parameters to reduce quantization errors. Further
    developments like DoReFa-Net [[11](#bib.bib11)], WRPN [[12](#bib.bib12)], and
    ABC-Net [[13](#bib.bib13)] introduce strategies to minimize information loss and
    quantization errors. Innovations such as XNOR-Net++ [[10](#bib.bib10)] and various
    mimic solutions like Distillation and Quantization (DQ) [[14](#bib.bib14)] continue
    to refine these approaches, emphasizing stability and high accuracy in training
    binary models. To address the non-differentiability of the binarization function,
    techniques like the straight-through estimator (STE) [[15](#bib.bib15)] are used
    for backpropagation. The ReActNet [[16](#bib.bib16)] improves BNNs with generalized
    activation functions, and the BNN+ [[17](#bib.bib17)] introduces an enhanced derivative
    approximation of the sign function along with a regularization strategy to optimize
    weight learning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些方法通常会遭遇准确性损失，因此近年来提出了许多基于优化的解决方案来缓解这一问题。Binary Weight Networks (BWN) 和
    XNOR-Net [[10](#bib.bib10)] 引入了一个缩放因子，用以近似浮点参数，从而减少量化误差。进一步的发展，如 DoReFa-Net [[11](#bib.bib11)]、WRPN [[12](#bib.bib12)]
    和 ABC-Net [[13](#bib.bib13)] 引入了减少信息丢失和量化误差的策略。创新技术如 XNOR-Net++ [[10](#bib.bib10)]
    和各种模拟解决方案，如 Distillation and Quantization (DQ) [[14](#bib.bib14)] 继续优化这些方法，强调二进制模型训练中的稳定性和高准确性。为了应对二值化函数的不可微性，使用了如直通估计器
    (STE) [[15](#bib.bib15)] 的技术进行反向传播。ReActNet [[16](#bib.bib16)] 通过泛化激活函数改进了 BNN，而
    BNN+ [[17](#bib.bib17)] 引入了改进的符号函数导数近似方法，并结合正则化策略来优化权重学习。
- en: Large Language Model Binarization. PB-LLM [[5](#bib.bib5)] implements partial
    binarization of the LLMs, retaining the salient parameters at full precision,
    occupying only a small portion of all parameters, to maintain the linguistic reasoning
    capacity. BiLLM [[18](#bib.bib18)] also considers the distribution pattern of
    the weight scale. It uses a binary residual approximation strategy to binarize
    salient parameters, which consists of an original binary tensor and a residual
    binarized matrix to present the binarization result of salient parameters. BitNet
    b1.58 [[1](#bib.bib1)] quantizes all parameters to the set of {-1, 0, 1}, where
    they find that quantized LLMs achieve competitive performance to their full-precision
    counterparts. However, PB-LLM and BiLLM use the extra cost of storage to process
    salient weights, and BitNet b1.58 uses an average of 1.58 bits to represent weights.
    None of them have reached the limits of binary models. There is room for further
    improvement in model storage size and inference speed. Our work focuses on achieving
    fully binarized LLMs while preserving the model’s capabilities as much as possible.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型二值化。PB-LLM [[5](#bib.bib5)] 实现了对 LLMs 的部分二值化，保留了以全精度表示的显著参数，仅占所有参数的一小部分，以保持语言推理能力。BiLLM [[18](#bib.bib18)]
    也考虑了权重尺度的分布模式。它使用了二进制残差近似策略来二值化显著参数，该策略由原始的二进制张量和一个残差二值化矩阵组成，用于呈现显著参数的二值化结果。BitNet
    b1.58 [[1](#bib.bib1)] 将所有参数量化为 {-1, 0, 1} 的集合，他们发现量化的 LLMs 在性能上与全精度模型相当。然而，PB-LLM
    和 BiLLM 使用额外的存储开销来处理显著权重，而 BitNet b1.58 使用平均 1.58 位来表示权重。这些方法尚未达到二进制模型的极限。模型存储大小和推理速度还有进一步改进的空间。我们的工作重点是实现完全二值化的
    LLMs，同时尽可能保留模型的能力。
- en: BitNet [[19](#bib.bib19)] and OneBit [[7](#bib.bib7)] employ quantization-aware
    training (QAT) to binarize LLMs. BitNet utilizes group quantization by applying
    different scales to the parameters of various groups, which accelerates model
    training. Its training loss aligns with that of pretraining autoregressive language
    models. Conversely, OneBit preserves the full-precision model knowledge through
    quantization-aware knowledge distillation, using a full-precision model as the
    teacher and guiding the binarized model training with two distinct loss functions.
    Unlike these two methods, our approach achieves similar or better results through
    a more streamlined and efficient training process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: BitNet [[19](#bib.bib19)] 和 OneBit [[7](#bib.bib7)] 采用了量化感知训练 (QAT) 来二值化 LLMs。BitNet
    通过对不同组的参数应用不同的缩放因子来实现组量化，从而加速模型训练。其训练损失与预训练自回归语言模型的损失一致。相比之下，OneBit 通过量化感知知识蒸馏保留全精度模型知识，使用全精度模型作为教师，并用两个不同的损失函数指导二值化模型的训练。与这两种方法不同，我们的方法通过更简化和高效的训练过程实现了类似或更好的结果。
- en: 3 Methodology
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法学
- en: 'In this section, we first provide an overview of the architecture of our FBI-LLM
    in Section [3.1](#S3.SS1 "3.1 Architecture of FBI-LLM ‣ 3 Methodology ‣ FBI-LLM:
    Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation").
    Then, in Section [3.2](#S3.SS2 "3.2 FBI-Linear ‣ 3 Methodology ‣ FBI-LLM: Scaling
    Up Fully Binarized LLMs from Scratch via Autoregressive Distillation"), we detail
    the FBI-linear module, the main component of FBI-LLM. Finally, we elaborate the
    FBI-LLM autoregressive distillation-based training procedure in Section [3.3](#S3.SS3
    "3.3 Autoregressive Distillation ‣ 3 Methodology ‣ FBI-LLM: Scaling Up Fully Binarized
    LLMs from Scratch via Autoregressive Distillation").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先在第 [3.1](#S3.SS1 "3.1 FBI-LLM 的架构 ‣ 3 方法学 ‣ FBI-LLM：通过自回归蒸馏从头开始扩展完全二值化
    LLM") 节中提供 FBI-LLM 架构的概述。然后，在第 [3.2](#S3.SS2 "3.2 FBI-Linear ‣ 3 方法学 ‣ FBI-LLM：通过自回归蒸馏从头开始扩展完全二值化
    LLM") 节中，我们详细介绍了 FBI-linear 模块，这是 FBI-LLM 的主要组件。最后，在第 [3.3](#S3.SS3 "3.3 自回归蒸馏
    ‣ 3 方法学 ‣ FBI-LLM：通过自回归蒸馏从头开始扩展完全二值化 LLM") 节中，我们阐述了基于自回归蒸馏的 FBI-LLM 训练程序。
- en: 3.1 Architecture of FBI-LLM
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 FBI-LLM 的架构
- en: 'We illustrate the overall architecture of FBI-LLM in Fig. [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via
    Autoregressive Distillation") (a). In transformer-based LLMs, the majority of
    parameters are found within the linear modules. FBI-LM replaces all linear modules,
    except for the causal head, with FBI-linear (Fully BInarized Linear). Since the
    causal head directly influences the output token distribution in each step, binarizing
    its parameters would significantly affect the accuracy of the model’s output,
    so we retain its precision.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图 [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ FBI-LLM：通过自回归蒸馏从头开始扩展完全二值化 LLM") (a) 中展示了 FBI-LLM
    的总体架构。在基于变换器的 LLM 中，大多数参数位于线性模块中。FBI-LM 用 FBI-linear（完全二值化线性）替代了所有线性模块，除了因果头。由于因果头直接影响每一步的输出标记分布，二值化其参数会显著影响模型输出的准确性，因此我们保留其精度。
- en: Additionally, the parameters in two other core modules of LLMs, embedding and
    layer norm, also need to be kept at full precision. This is because the embedding
    module contains semantic information about all tokens and, as the first layer
    of the model input, determines the text’s initial representation. Layernorm, on
    the other hand, scales the activation values directly. Binarizing its parameters
    would significantly reduce the semantic expressiveness of the activation values
    at each layer. Similar settings are also adopted in other work [[19](#bib.bib19)]
    about the binarization of LLMs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLM 的两个其他核心模块——嵌入和层归一化——也需要保持全精度。这是因为嵌入模块包含所有标记的语义信息，并且作为模型输入的第一层，决定了文本的初始表示。另一方面，层归一化直接缩放激活值。二值化其参数会显著降低每一层激活值的语义表达能力。在其他关于
    LLM 二值化的工作中也采用了类似的设置 [[19](#bib.bib19)]。
- en: 3.2 FBI-Linear
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 FBI-Linear
- en: 'The main parameters in FBI-linear are a matrix $\bm{W}^{b}\in\mathbb{R}^{m\times
    n}$ of the LLMs. During the training, the binarization process can be formulated
    as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: FBI-linear 中的主要参数是一个矩阵 $\bm{W}^{b}\in\mathbb{R}^{m\times n}$。在训练过程中，二值化过程可以表述为：
- en: '|  | $\bm{W}^{b}=sign(\bm{W}^{f})$ |  | (1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{W}^{b}=sign(\bm{W}^{f})$ |  | (1) |'
- en: 'where $sign$ function is formulated as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $sign$ 函数表述为：
- en: '|  | 
    |  | (2) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | 
    |  | (2) |'
- en: We follow the previous works [[10](#bib.bib10), [19](#bib.bib19)] to scale the
    binarized parameter with full-precision scale factors. Scale factors can effectively
    reduce the error between the binarized and original parameters, thereby preserving
    the more representational capacity of the corresponding module. They constitute
    a small fraction of the total parameters, making them a highly efficient enhancement
    to the model’s performance without significantly increasing the parameter count
    and computational demand.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循以前的工作 [[10](#bib.bib10), [19](#bib.bib19)]，用全精度缩放因子来缩放二值化参数。缩放因子可以有效减少二值化参数和原始参数之间的误差，从而保持相应模块的更多表现能力。它们占总参数的一小部分，使其成为一种高效的性能增强方法，不会显著增加参数数量和计算需求。
- en: 'Specifically, in the FBI-linear, we apply scaling at the granularity of the
    matrix columns. The calculation process can be formulated as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在 FBI-linear 中，我们在矩阵列的粒度上应用缩放。计算过程可以表述为：
- en: '|  | $\widetilde{\bm{W}}^{b}_{\cdot,j}=\alpha_{j}\bm{W}^{b}_{\cdot,j}+\beta_{j}$
    |  | (3) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widetilde{\bm{W}}^{b}_{\cdot,j}=\alpha_{j}\bm{W}^{b}_{\cdot,j}+\beta_{j}$
    |  | (3) |'
- en: where $\widetilde{\bm{W}}^{b}_{\cdot,j}$ respectively.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\widetilde{\bm{W}}^{b}_{\cdot,j}$ 分别表示。
- en: 'To accelerate the model’s convergence speed, we initialize $\bm{\alpha}$ before
    training as following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速模型的收敛速度，我们在训练前初始化 $\bm{\alpha}$ 如下：
- en: '|  | $\alpha_{j}=a_{j}$ |  | (4) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{j}=a_{j}$ |  | (4) |'
- en: '|  | $\beta_{j}=\frac{1}{m}\sum^{m}_{i}&#124;\bm{W}_{ij}^{f}-a_{j}&#124;$ |  |
    (5) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\beta_{j}=\frac{1}{m}\sum^{m}_{i}|\bm{W}_{ij}^{f}-a_{j}|$ |  | (5) |'
- en: where $a_{j}=\frac{1}{m}\sum^{m}_{i}\bm{W}_{ij}^{f}$.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a_{j}=\frac{1}{m}\sum^{m}_{i}\bm{W}_{ij}^{f}$。
- en: 3.3 Autoregressive Distillation
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 自回归蒸馏
- en: 'Given a training corpus of tokens ${\bm{x}}=\left\{x_{1},\ldots,x_{n}\right\}$,
    a standard autoregressive language modeling objective [[20](#bib.bib20)] is to
    maximize the likelihood:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含标记的训练语料库 ${\bm{x}}=\left\{x_{1},\ldots,x_{n}\right\}$，标准的自回归语言建模目标 [[20](#bib.bib20)]
    是最大化似然：
- en: '|  | $\mathcal{L}({\bm{x}})=\sum_{i}\log p\left(x_{i}\mid x_{i-k},\ldots,x_{i-1};\bm{\theta}\right)$
    |  | (6) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}({\bm{x}})=\sum_{i}\log p\left(x_{i}\mid x_{i-k},\ldots,x_{i-1};\bm{\theta}\right)$
    |  | (6) |'
- en: 'where $k$, the teacher prediction probability for the next token can be formulated
    as:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $k$，即下一个标记的教师预测概率，可以表述为：
- en: '|  | $\bm{p}^{\mathcal{T}}\left(x^{m+1}\mid x^{1},\ldots,x^{m}\right)=\operatorname{softmax}\left(\bm{h}_{l}^{m}\bm{W}_{m+1}\right).$
    |  | (7) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{p}^{\mathcal{T}}\left(x^{m+1}\mid x^{1},\ldots,x^{m}\right)=\operatorname{softmax}\left(\bm{h}_{l}^{m}\bm{W}_{m+1}\right).$
    |  | (7) |'
- en: where $\bm{h}_{l}^{m}$ represents parameters of the added linear output layer
    to predict the next token’s probability.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{h}_{l}^{m}$ 表示用于预测下一个标记概率的附加线性输出层的参数。
- en: 'The cross-entropy between the outputs of the student model and the teacher
    model is calculated as the final loss function at each step of predicting the
    next token. It can be formulated as:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 学生模型和教师模型输出之间的交叉熵被计算为每一步预测下一个标记的最终损失函数。它可以表述为：
- en: '|  | $\mathcal{L}=-\frac{1}{n}\sum^{n}_{i}\bm{p}^{\mathcal{T}}(x^{i+1})\cdot\log\bm{p}^{\mathcal{S}}(x^{i+1})$
    |  | (8) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=-\frac{1}{n}\sum^{n}_{i}\bm{p}^{\mathcal{T}}(x^{i+1})\cdot\log\bm{p}^{\mathcal{S}}(x^{i+1})$
    |  | (8) |'
- en: Here $n$ is the corresponding predicted distribution of the student model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里$n$是学生模型对应的预测分布。
- en: QAT utilizing knowledge distillation has been shown to be effective in various
    studies [[21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [7](#bib.bib7)]. However, unlike these works, our training process exclusively
    uses the autoregressive distillation loss without adding any other losses to maintain
    simplicity. Our experiments verified that using only the distillation loss yields
    better results than the vanilla one-hot label based autoregressive loss while
    maintaining methodological simplicity when working with fully binarized LLMs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏的QAT在各种研究中已被证明有效 [[21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24), [7](#bib.bib7)]。然而，与这些研究不同，我们的训练过程完全使用自回归蒸馏损失，而不添加其他损失以保持简单性。我们的实验验证了仅使用蒸馏损失相比于基于原始独热标签的自回归损失能得到更好的结果，同时在处理完全二值化LLM时保持了方法上的简洁性。
- en: 'Since the $sign(\cdot)$ is non-differentiable at zero, it causes the gradient
    chain to break during backpropagation, preventing optimization of the model parameters.
    Therefore, we use the Straight-Through Estimator (STE) method [[15](#bib.bib15)]
    during backpropagation, where the gradient of the output of the non-differentiable
    function is used as an estimate for the gradient of the input, thus allowing the
    gradient to be effectively propagated. This estimation can be formulated as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $sign(\cdot)$ 在零点不可导，这会导致反向传播期间梯度链断裂，从而阻止模型参数的优化。因此，我们在反向传播过程中使用直通估计器（STE）方法 [[15](#bib.bib15)]，其中不可导函数的输出梯度被用作输入梯度的估计，从而使梯度能够有效地传播。这种估计可以表述为：
- en: '|  | $\frac{\partial\mathcal{L}}{\partial\bm{W}^{f}}=\frac{\partial\mathcal{L}}{\partial\bm{W}^{b}}$
    |  | (9) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial\mathcal{L}}{\partial\bm{W}^{f}}=\frac{\partial\mathcal{L}}{\partial\bm{W}^{b}}$
    |  | (9) |'
- en: 4 Experiments
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In our experiment, we follow the W1A16 setup [[18](#bib.bib18), [7](#bib.bib7)],
    quantizing only the parameters to 1-bit while keeping the activation values at
    16-bit. We train FBI-LLMs with sizes of 130M, 1.3B, and 7B, testing their performance
    across multiple tasks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们遵循W1A16设置 [[18](#bib.bib18), [7](#bib.bib7)]，仅将参数量化为1位，同时将激活值保持在16位。我们训练了130M、1.3B和7B大小的FBI-LLMs，并测试了它们在多个任务中的表现。
- en: 4.1 Setup
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: 'Dataset. We train FBI-LLMs with the Amber dataset [[25](#bib.bib25)]. Amber
    dataset is a mixture of RefinedWeb [[26](#bib.bib26)], StarCoder [[27](#bib.bib27)],
    and RedPajama-v1 [[28](#bib.bib28)] and contains the total 1.26 trillion tokens.
    It divides the data into 360 chunks, with each chunk containing an average of
    3.5 billion tokens²²2As shown in Fig. [3](#S4.F3 "Figure 3 ‣ 4.2 Main Results
    ‣ 4 Experiments ‣ FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive
    Distillation"), in our experiment, about 10% of the training data chunks have
    already achieved competitive performance. Further training is naturally expected
    to yield even higher accuracy..'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。我们使用 Amber 数据集 [[25](#bib.bib25)] 训练 FBI-LLMs。Amber 数据集是 RefinedWeb [[26](#bib.bib26)],
    StarCoder [[27](#bib.bib27)], 和 RedPajama-v1 [[28](#bib.bib28)] 的混合体，总共包含 1.26
    万亿个标记。它将数据划分为 360 个块，每个块平均包含 35 亿个标记²²2如图 [3](#S4.F3 "图 3 ‣ 4.2 主要结果 ‣ 4 实验 ‣
    FBI-LLM：通过自回归蒸馏从头开始扩展完全二值化 LLMs") 所示，在我们的实验中，大约 10% 的训练数据块已经达到了具有竞争力的性能。进一步的训练自然会期望产生更高的准确度。
- en: 'Training details. Our models used for experiments adopt a similar structure
    as LLaMA2 [[3](#bib.bib3)]. For the specific hyper-parameters settings of FBI-LLMs
    of different sizes, refer to Table [1](#S4.T1 "Table 1 ‣ 4.1 Setup ‣ 4 Experiments
    ‣ FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation").
    The maximum sequence length is set to 2048. The optimizer is Adam with $\beta_{1}=0.9$
    as it is warmed up over 2,000 steps. We use gradient clipping at 1.0. We use LLaMA2-7B
    as the teacher model for all size FBI-LLMs to calculate autoregressive distillation
    loss. We train models with 64 NVIDIA A100 GPUs in total and maintain BF16 precision
    while training. Please refer to Appendix [C](#A3 "Appendix C Model Configuration
    and Training Details ‣ FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via
    Autoregressive Distillation") for more details.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 训练细节。我们的实验模型采用了与 LLaMA2 [[3](#bib.bib3)] 相似的结构。有关不同规模 FBI-LLMs 的具体超参数设置，请参阅表
    [1](#S4.T1 "表 1 ‣ 4.1 设置 ‣ 4 实验 ‣ FBI-LLM：通过自回归蒸馏从头开始扩展完全二值化 LLMs")。最大序列长度设置为
    2048。优化器是 Adam，$\beta_{1}=0.9$，在 2,000 步骤中进行预热。我们使用梯度裁剪值为 1.0。我们使用 LLaMA2-7B 作为所有规模
    FBI-LLMs 的教师模型来计算自回归蒸馏损失。我们使用 64 个 NVIDIA A100 GPU 进行模型训练，并在训练过程中保持 BF16 精度。有关更多细节，请参阅附录 [C](#A3
    "附录 C 模型配置和训练细节 ‣ FBI-LLM：通过自回归蒸馏从头开始扩展完全二值化 LLMs")。
- en: 'Table 1: Hyper-parameters for FBI-LLMs in xxperiments.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：FBI-LLMs 的超参数设置。
- en: '| Model Size | # layers | hidden size | # attention heads | intermediate size
    | batch size (tokens) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 模型大小 | 层数 | 隐藏层大小 | 注意力头数 | 中间层大小 | 批处理大小（标记） |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| FBI-LLM 130M | 12 | 768 | 12 | 2,048 | 2M |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| FBI-LLM 130M | 12 | 768 | 12 | 2,048 | 2M |'
- en: '| FBI-LLM 1.3B | 24 | 2,048 | 32 | 5,632 | 2.4M |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| FBI-LLM 1.3B | 24 | 2,048 | 32 | 5,632 | 2.4M |'
- en: '| FBI-LLM 7B | 32 | 4,096 | 32 | 11,008 | 3.9M |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| FBI-LLM 7B | 32 | 4,096 | 32 | 11,008 | 3.9M |'
- en: Baselines. We compare our work with prior binarized LLMs Bi-LLM [[18](#bib.bib18)],
    OneBit [[7](#bib.bib7)], and BitNet [[19](#bib.bib19)]. We also include the BitNet
    b1.58 [[1](#bib.bib1)], which is a ternary quantization LLM, as our baseline model
    for comparison³³3As the original BitNet b1.58 [[1](#bib.bib1)] has not open-sourced
    their model, we use a third-party open-sourced one [https://huggingface.co/1bitLLM/](https://huggingface.co/1bitLLM/)
    to evaluate certain indicators for the comparison. This model achieves slightly
    better results than those reported in the original paper.. We further include
    results from open-sourced full-precision models of various sizes, such as OPT [[29](#bib.bib29)],
    LLaMA [[3](#bib.bib3), [4](#bib.bib4)], and TinyLLaMA [[30](#bib.bib30)], as references.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基准线。我们将我们的工作与先前的二值化 LLMs Bi-LLM [[18](#bib.bib18)], OneBit [[7](#bib.bib7)],
    和 BitNet [[19](#bib.bib19)] 进行比较。我们还包括了 BitNet b1.58 [[1](#bib.bib1)]，这是一个三值化量化
    LLM，作为我们的基准模型进行比较³³3由于原始 BitNet b1.58 [[1](#bib.bib1)] 没有开源其模型，我们使用了一个第三方开源模型
    [https://huggingface.co/1bitLLM/](https://huggingface.co/1bitLLM/) 来评估某些指标进行比较。该模型的结果略优于原始论文中报告的结果。我们进一步包括了各种大小的开源全精度模型的结果，例如
    OPT [[29](#bib.bib29)], LLaMA [[3](#bib.bib3), [4](#bib.bib4)] 和 TinyLLaMA [[30](#bib.bib30)]，作为参考。
- en: Evaluation Metrics. We evaluate the models based on their zero-shot performance
    in some downstream tasks, including BoolQ [[31](#bib.bib31)], PIQA [[32](#bib.bib32)],
    HellaSwag [[33](#bib.bib33)], Winogrande [[34](#bib.bib34)], ARC [[35](#bib.bib35)],
    and OpenbookQA [[36](#bib.bib36)]. We also use perplexity as the evaluation metric.
    Perplexity measures how well a probability model predicts a token, quantitatively
    measuring the model’s generation power.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。我们根据模型在一些下游任务中的零样本表现来评估这些模型，包括 BoolQ [[31](#bib.bib31)]、PIQA [[32](#bib.bib32)]、HellaSwag [[33](#bib.bib33)]、Winogrande
    [[34](#bib.bib34)]、ARC [[35](#bib.bib35)] 和 OpenbookQA [[36](#bib.bib36)]。我们还使用困惑度作为评估指标。困惑度衡量概率模型预测一个标记的能力，定量地衡量模型的生成能力。
- en: 4.2 Main Results
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主要结果
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ FBI-LLM: Scaling
    Up Fully Binarized LLMs from Scratch via Autoregressive Distillation") presents
    the main results comparing our FBI-LLMs to various state-of-the-art baseline models.
    We also report the average bit-width occupied by model parameters, excluding the
    embedding layer and the head, for different models. Details on the calculation
    process can be found in Appendix [B](#A2 "Appendix B Average Bit-width of Binarized
    LLM ‣ FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive
    Distillation"). Our FBI-LLMs maintain the lowest average bit-width across different
    model sizes while demonstrating remarkable performance. We provide zero-shot accuracy,
    which is a foundation for understanding how well a model can perform without additional
    task-specific information. This metric is commonly used to assess the model’s
    initial capabilities and aligns with certain benchmarking tasks aimed at measuring
    the pre-trained LLM’s general comprehension and knowledge-reserving capabilities
    across diverse downstream tasks without additional few-shot information.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S4.T2 "表 2 ‣ 4.2 主要结果 ‣ 4 实验 ‣ FBI-LLM：通过自回归蒸馏从头开始扩展完全二值化的 LLM") 展示了将我们的
    FBI-LLM 与各种最先进基线模型的主要结果比较。我们还报告了不同模型的模型参数占用的平均比特宽度，排除了嵌入层和头部。计算过程的详细信息可以在附录 [B](#A2
    "附录 B 二值化 LLM 的平均比特宽度 ‣ FBI-LLM：通过自回归蒸馏从头开始扩展完全二值化的 LLM") 中找到。我们的 FBI-LLM 在不同模型规模中保持了最低的平均比特宽度，同时展示了卓越的性能。我们提供了零样本准确性，这是了解模型在没有额外任务特定信息的情况下表现如何的基础。这一指标通常用于评估模型的初步能力，并与某些基准测试任务对齐，旨在测量预训练
    LLM 在没有额外少量样本信息的情况下的综合理解和知识保留能力。
- en: Since there is no binary baseline for the 130M size, we compare our 130M model
    with the BitNet b1.58 at the 700M scale. Despite the fivefold difference in model
    size and significant variations in quantization degree, our model still outperforms
    BitNet b1.58 in BoolQA and OpenbookQA. For the 1.3B-scale binary models, our FBI-LLM
    achieves the best performance in most downstream tasks and perplexity, even matching
    or exceeding the capacity of some 7B-scale binary models like BiLLM-LLaMA2-7B.
    Compared to the full-precision models of a similar scale, the proposed FBI-LLM
    1.3B can achieve up to 87% of their performance in downstream tasks. In the 7B
    scale, our model significantly outperformed nearly all baselines.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 130M 规模没有二值化基线，我们将 130M 模型与 700M 规模的 BitNet b1.58 进行比较。尽管模型规模差异五倍且量化程度有显著变化，我们的模型在
    BoolQA 和 OpenbookQA 中仍然超越了 BitNet b1.58。对于 1.3B 规模的二值化模型，我们的 FBI-LLM 在大多数下游任务和困惑度中表现最佳，甚至与某些
    7B 规模的二值化模型如 BiLLM-LLaMA2-7B 相匹配或超越。与类似规模的全精度模型相比，提出的 FBI-LLM 1.3B 可以在下游任务中实现其高达
    87% 的性能。在 7B 规模下，我们的模型显著超越了几乎所有基线模型。
- en: '![Refer to caption](img/50d40e82887910845fa40ae4a2a172c6.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/50d40e82887910845fa40ae4a2a172c6.png)'
- en: 'Figure 3: Changes in average perplexity and downstream task accuracy during
    the training of FBI-LLM 7B. The horizontal axis represents the number of Amber
    data blocks used for training.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：FBI-LLM 7B 训练过程中平均困惑度和下游任务准确性的变化。横轴表示用于训练的 Amber 数据块数量。
- en: 'Furthermore, limited by computational resources, the current results for FBI-LLM
    7B are not final. We only use 8.6% (31 chunks) of the Amber dataset. Fig. [3](#S4.F3
    "Figure 3 ‣ 4.2 Main Results ‣ 4 Experiments ‣ FBI-LLM: Scaling Up Fully Binarized
    LLMs from Scratch via Autoregressive Distillation") illustrates the changes in
    downstream task accuracy and perplexity during the training process of FBI-LLM-7B.
    It is clear that, as of the current training progress, the performance of FBI-LLM-7B
    will be improved consistently, and further training could yield better results.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，由于计算资源的限制，目前 FBI-LLM 7B 的结果尚不最终。我们仅使用了 Amber 数据集的 8.6%（31 个块）。图 [3](#S4.F3
    "Figure 3 ‣ 4.2 Main Results ‣ 4 Experiments ‣ FBI-LLM: Scaling Up Fully Binarized
    LLMs from Scratch via Autoregressive Distillation") 说明了 FBI-LLM-7B 训练过程中下游任务准确性和困惑度的变化。可以明显看出，按照当前的训练进展，FBI-LLM-7B
    的性能将持续提升，进一步训练可能会带来更好的结果。'
- en: 'Table 2: Performance on downstream tasks and perplexity. Here, BW means bit-width,
    which refers to the average number of bits occupied by each parameter. HS, WG,
    and OBQA are abbreviations for HellaSwag, Winogrande, and OpenbookQA, respectively.
    We divide the table into three blocks based on model size. In each block, the
    bold values represent the best values among the non-high-precision models, while
    the values with an underline represent the best values among the high-precision
    models.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 下游任务表现及困惑度。在这里，BW 指位宽，表示每个参数所占用的平均位数。HS、WG 和 OBQA 分别是 HellaSwag、Winogrande
    和 OpenbookQA 的缩写。我们根据模型大小将表格分为三个块。在每个块中，**粗体**值表示非高精度模型中的最佳值，而**下划线**值表示高精度模型中的最佳值。'
- en: '| Model | Size | BW | Zero-shot Accuracy $\uparrow$ |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 | BW | 零样本准确率 $\uparrow$ |'
- en: '| BoolQ | PIQA | HS | WG | ARC-e | ARC-c | OBQA | Ave. | Wiki2 | PTB | C4 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | PIQA | HS | WG | ARC-e | ARC-c | OBQA | Ave. | Wiki2 | PTB | C4 |'
- en: '| BitNet b1.58 [[1](#bib.bib1)] | 700M | 1.59 | 58.2 | 68.1 | 35.1 | 55.2 |
    51.8 | 21.4 | 20.0 | 44.3 | 17.1 | 72.1 | 17.5 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| BitNet b1.58 [[1](#bib.bib1)] | 700M | 1.59 | 58.2 | 68.1 | 35.1 | 55.2 |
    51.8 | 21.4 | 20.0 | 44.3 | 17.1 | 72.1 | 17.5 |'
- en: '| FBI-LLM (Ours) | 130M | 1.01 | 62.1 | 59.3 | 28.7 | 51.0 | 34.9 | 20.5 |
    26.4 | 40.4 | 28.2 | 136.6 | 26.9 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| FBI-LLM (我们的) | 130M | 1.01 | 62.1 | 59.3 | 28.7 | 51.0 | 34.9 | 20.5 | 26.4
    | 40.4 | 28.2 | 136.6 | 26.9 |'
- en: '| TinyLLaMA [[30](#bib.bib30)] | 1.1B | 16 | 57.8 | 73.3 | 59.2 | 59.1 | 55.3
    | 30.1 | 36.0 | 53.0 | 7.8 | 30.5 | 9.9 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| TinyLLaMA [[30](#bib.bib30)] | 1.1B | 16 | 57.8 | 73.3 | 59.2 | 59.1 | 55.3
    | 30.1 | 36.0 | 53.0 | 7.8 | 30.5 | 9.9 |'
- en: '| OPT [[29](#bib.bib29)] | 1.3B | 16 | 57.8 | 72.5 | 53.7 | 59.5 | 51.0 | 29.5
    | 33.4 | 51.1 | 14.6 | 20.3 | 16.1 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| OPT [[29](#bib.bib29)] | 1.3B | 16 | 57.8 | 72.5 | 53.7 | 59.5 | 51.0 | 29.5
    | 33.4 | 51.1 | 14.6 | 20.3 | 16.1 |'
- en: '| \hdashlineOneBit-OPT [[7](#bib.bib7)] | 1.3B | 1.02 | 59.5 | 62.6 | 34.3
    | 51.1 | 41.3 | 24.1 | - | - | 25.4 | - | 23.0 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineOneBit-OPT [[7](#bib.bib7)] | 1.3B | 1.02 | 59.5 | 62.6 | 34.3
    | 51.1 | 41.3 | 24.1 | - | - | 25.4 | - | 23.0 |'
- en: '| BitNet b1.58 [[1](#bib.bib1)] | 1.3B | 1.59 | 56.7 | 68.8 | 37.7 | 55.8 |
    54.9 | 24.2 | 19.6 | 45.4 | 24.1 | 145.1 | 21.8 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| BitNet b1.58 [[1](#bib.bib1)] | 1.3B | 1.59 | 56.7 | 68.8 | 37.7 | 55.8 |
    54.9 | 24.2 | 19.6 | 45.4 | 24.1 | 145.1 | 21.8 |'
- en: '| FBI-LLM (Ours) | 1.3B | 1.01 | 60.3 | 69.0 | 42.3 | 54.0 | 43.6 | 25.3 |
    29.6 | 46.3 | 12.6 | 39.3 | 13.8 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| FBI-LLM (我们的) | 1.3B | 1.01 | 60.3 | 69.0 | 42.3 | 54.0 | 43.6 | 25.3 | 29.6
    | 46.3 | 12.6 | 39.3 | 13.8 |'
- en: '| OPT [[29](#bib.bib29)] | 7B | 16 | 66.1 | 76.5 | 67.2 | 65.4 | 60.0 | 34.7
    | 37.4 | 58.2 | 10.9 | 15.8 | 12.7 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| OPT [[29](#bib.bib29)] | 7B | 16 | 66.1 | 76.5 | 67.2 | 65.4 | 60.0 | 34.7
    | 37.4 | 58.2 | 10.9 | 15.8 | 12.7 |'
- en: '| LLaMA [[3](#bib.bib3)] | 7B | 16 | 75.1 | 79.2 | 76.2 | 69.9 | 72.9 | 44.9
    | 44.4 | 66.0 | 5.7 | 41.2 | 7.3 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA [[3](#bib.bib3)] | 7B | 16 | 75.1 | 79.2 | 76.2 | 69.9 | 72.9 | 44.9
    | 44.4 | 66.0 | 5.7 | 41.2 | 7.3 |'
- en: '| LLaMA2 [[4](#bib.bib4)] | 7B | 16 | 77.7 | 79.1 | 76.0 | 69.1 | 74.6 | 46.2
    | 44.2 | 66.7 | 5.5 | 37.9 | 7.3 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2 [[4](#bib.bib4)] | 7B | 16 | 77.7 | 79.1 | 76.0 | 69.1 | 74.6 | 46.2
    | 44.2 | 66.7 | 5.5 | 37.9 | 7.3 |'
- en: '| \hdashlineOneBit-LLaMA2 [[7](#bib.bib7)] | 7B |  | 63.1 | 68.1 | 52.6 | 58.4
    | 41.6 | 29.6 | - | - | 9.7 | - | 11.1 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineOneBit-LLaMA2 [[7](#bib.bib7)] | 7B |  | 63.1 | 68.1 | 52.6 | 58.4
    | 41.6 | 29.6 | - | - | 9.7 | - | 11.1 |'
- en: '| BitNet [[19](#bib.bib19)] | 7B | - | - | - | 38.9 | 51.4 | - | - | - | -
    | - | - | - |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| BitNet [[19](#bib.bib19)] | 7B | - | - | - | 38.9 | 51.4 | - | - | - | -
    | - | - | - |'
- en: '| BiLLM-OPT [[18](#bib.bib18)] | 7B | 1.11 | 62.2 | 58.6 | 31.9 | 51.5 | 34.1
    | 23.9 | 29.0 | 41.6 | 35.4 | 73.6 | 43.2 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM-OPT [[18](#bib.bib18)] | 7B | 1.11 | 62.2 | 58.6 | 31.9 | 51.5 | 34.1
    | 23.9 | 29.0 | 41.6 | 35.4 | 73.6 | 43.2 |'
- en: '| BiLLM-LLaMA [[18](#bib.bib18)] | 7B | 1.08 | 62.7 | 61.2 | 36.8 | 51.1 |
    36.0 | 25.7 | 31.8 | 43.6 | 35.0 | 421.3 | 39.6 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM-LLaMA [[18](#bib.bib18)] | 7B | 1.08 | 62.7 | 61.2 | 36.8 | 51.1 |
    36.0 | 25.7 | 31.8 | 43.6 | 35.0 | 421.3 | 39.6 |'
- en: '| BiLLM-LLaMA2 [[18](#bib.bib18)] | 7B | 1.08 | 61.8 | 60.6 | 34.8 | 52.4 |
    36.2 | 24.4 | 33.2 | 43.3 | 32.5 | 3877.4 | 40.5 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM-LLaMA2 [[18](#bib.bib18)] | 7B | 1.08 | 61.8 | 60.6 | 34.8 | 52.4 |
    36.2 | 24.4 | 33.2 | 43.3 | 32.5 | 3877.4 | 40.5 |'
- en: '| FBI-LLM (Ours) | 7B | 1.01 | 61.5 | 72.6 | 57.7 | 58.9 | 53.0 | 29.9 | 36.8
    | 52.9 | 9.1 | 29.6 | 10.5 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| FBI-LLM (我们) | 7B | 1.01 | 61.5 | 72.6 | 57.7 | 58.9 | 53.0 | 29.9 | 36.8
    | 52.9 | 9.1 | 29.6 | 10.5 |'
- en: 4.3 Effectiveness of Autoregressive Distillation
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 自回归蒸馏的有效性
- en: 'To demonstrate the effectiveness of using only autoregressive distillation
    as the training objective, we train two models: one using solely the autoregressive
    distillation loss and the other using only the standard autoregressive loss. All
    other training procedures are identical to those used for FBI-LLM. We evaluate
    the performance of these models on downstream tasks and perplexity, as shown in
    Fig. [4](#S4.F4 "Figure 4 ‣ 4.3 Effectiveness of Autoregressive Distillation ‣
    4 Experiments ‣ FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive
    Distillation"). The evaluation tasks and datasets are the same as those listed
    in Table [2](#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ FBI-LLM: Scaling
    Up Fully Binarized LLMs from Scratch via Autoregressive Distillation"). For clarity,
    we present only the average accuracy across different tasks and the average perplexity
    across different datasets here. Detailed performance for each task is provided
    in Appendix [D](#A4 "Appendix D Detail Experiment Results ‣ FBI-LLM: Scaling Up
    Fully Binarized LLMs from Scratch via Autoregressive Distillation").'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示仅使用自回归蒸馏作为训练目标的有效性，我们训练了两个模型：一个仅使用自回归蒸馏损失，另一个仅使用标准自回归损失。所有其他训练过程与用于 FBI-LLM
    的相同。我们评估这些模型在下游任务和困惑度上的表现，如图 [4](#S4.F4 "图 4 ‣ 4.3 自回归蒸馏的有效性 ‣ 4 实验 ‣ FBI-LLM：通过自回归蒸馏从头开始扩展完全二值化的
    LLM") 所示。评估任务和数据集与表 [2](#S4.T2 "表 2 ‣ 4.2 主要结果 ‣ 4 实验 ‣ FBI-LLM：通过自回归蒸馏从头开始扩展完全二值化的
    LLM") 中列出的相同。为了清晰起见，这里仅展示了不同任务的平均准确率和不同数据集的平均困惑度。每个任务的详细性能见附录 [D](#A4 "附录 D 详细实验结果
    ‣ FBI-LLM：通过自回归蒸馏从头开始扩展完全二值化的 LLM")。
- en: It can be observed that throughout the training process, models trained with
    autoregressive distillation objective consistently outperform those trained with
    the standard autoregressive scheme in downstream tasks and perplexity. This indicates
    that using autoregressive distillation objective is more effective in training
    binarized LLMs. The utilized soft labels from the output probabilities of the
    teacher model contain more information than hard labels (i.e., the vocabulary
    labels). They provide a distribution over all possible vocabulary, indicating
    not just the target word but also the relative confidence in other possible words.
    This richer information helps the target model learn nuanced patterns and relationships
    in the data that are captured by the strong teacher model. Since our target model
    is learning solely from a smoothed version of the ground truth, it is less likely
    to overfit to the noise or specific details in the training data that may not
    generalize well to new data. Therefore, retaining only autoregressive distillation
    as the training objective ensures the simplicity and effectiveness of the entire
    training workflow.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 可以观察到，在整个训练过程中，使用自回归蒸馏目标训练的模型在下游任务和困惑度上始终优于使用标准自回归方案训练的模型。这表明，使用自回归蒸馏目标在训练二值化
    LLM 时更为有效。教师模型输出概率中的软标签包含的信息比硬标签（即词汇标签）更多。它们提供了所有可能词汇的分布，不仅指示目标词，还指示其他可能词汇的相对置信度。这种更丰富的信息帮助目标模型学习数据中的细微模式和关系，这些模式和关系被强大的教师模型捕捉到。由于我们的目标模型仅从平滑版本的真实数据中学习，因此不太可能过拟合到训练数据中的噪声或特定细节，这些噪声或细节可能不适用于新数据。因此，仅保留自回归蒸馏作为训练目标可以确保整个训练流程的简洁性和有效性。
- en: '![Refer to caption](img/457f44f2a2c9f6b4cabba9c745b54018.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/457f44f2a2c9f6b4cabba9c745b54018.png)'
- en: (a) Average downstream task accuracy
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 平均下游任务准确率
- en: '![Refer to caption](img/07db5e5efbc9835db4009c71597b62b2.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/07db5e5efbc9835db4009c71597b62b2.png)'
- en: (b) Average perplexity
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 平均困惑度
- en: 'Figure 4: The model performance for different training loss.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：不同训练损失下的模型性能。
- en: 5 Analysis
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 分析
- en: In this section, we analyze 1) the better choice of training from scratch or
    continuing training from pretrained LLM for binarized LLMs. 2) training instability
    and our solution. 3) storage efficiency of our models. 4) generation case demonstrations.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了 1) 从头开始训练还是继续从预训练 LLM 训练对于二值化 LLM 的更好选择。 2) 训练不稳定性及我们的解决方案。 3) 我们模型的存储效率。
    4) 生成案例演示。
- en: 5.1 Training from Scratch or Continue Training from Pretrained LLM?
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 从头训练还是继续从预训练 LLM 训练？
- en: '![Refer to caption](img/1341668db1f5359cfc51d07acf534247.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1341668db1f5359cfc51d07acf534247.png)'
- en: (a) Average flip-flop ratio
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 平均翻转率
- en: '![Refer to caption](img/4f9d74d07f1f36c53034e344b9bbfac8.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4f9d74d07f1f36c53034e344b9bbfac8.png)'
- en: (b) Training loss
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 训练损失
- en: 'Figure 5: The flip-flop ratio and loss for different training procedures.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：不同训练过程中的翻转率和损失。
- en: Intuitively, continuing training from a pretrained LLM to obtain a binarized
    model can inherit knowledge from the original model, potentially achieving better
    results than training from scratch. To evaluate this hypothesis, we conduct analytical
    experiments to record and compare the behaviors of models under both training
    procedures.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，从预训练 LLM 继续训练以获得二值化模型可以继承原始模型的知识，可能比从头训练取得更好的结果。为了评估这一假设，我们进行了分析实验，以记录和比较模型在两种训练过程下的行为。
- en: 'To quantify and examine the model behaviors using pretrained parameters or
    training from scratch, as well as their stability and initialization dependency
    of them, we apply the flip-flop (FF) ratio [[37](#bib.bib37)]. The FF ratio measures
    optimization stability behavior, which is defined as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化和检查使用预训练参数或从头训练模型的行为，以及它们的稳定性和初始化依赖性，我们应用了翻转（FF）比率 [[37](#bib.bib37)]。FF
    比率衡量优化稳定性行为，其定义如下：
- en: '|  |  | $\displaystyle\mathbf{C}_{\mathbf{FF}}=\frac{\left&#124;{Sign}\left(\bm{w}^{b}_{t+1}\right)-{Sign}\left(\bm{w}^{b}_{t}\right)\right&#124;_{\text{abs}}}{2},$
    |  | (10) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbf{C}_{\mathbf{FF}}=\frac{\left\|{Sign}\left(\bm{w}^{b}_{t+1}\right)-{Sign}\left(\bm{w}^{b}_{t}\right)\right\|_{\text{abs}}}{2},$
    |  | (10) |'
- en: where $\mathbf{C}_{\mathbf{FF}}$ is the absolute operation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{C}_{\mathbf{FF}}$ 是绝对操作。
- en: '|  |  | $\displaystyle\mathbf{FF}_{\text{ratio}}=\frac{\sum_{l=1}^{L}\sum_{\bm{w}^{b}\in\bm{W}_{l}^{b}}\mathbf{C}_{\mathbf{FF}}}{N_{\text{total}}},$
    |  | (11) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbf{FF}_{\text{ratio}}=\frac{\sum_{l=1}^{L}\sum_{\bm{w}^{b}\in\bm{W}_{l}^{b}}\mathbf{C}_{\mathbf{FF}}}{N_{\text{total}}},$
    |  | (11) |'
- en: where $N_{\text{total}}$ denotes the flip-flop ratio, which is the percentage
    of parameters that change their signs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N_{\text{total}}$ 表示翻转率，即参数符号发生变化的百分比。
- en: In this experiment, we select TinyLLaMA as the base LLM and initialize the binarized
    LLM parameters using either pretrained values or random assignments. We maintain
    consistency in all other training details with the FBI-LLM. In addition to the
    $\mathbf{FF}_{\text{ratio}}$, we monitor training losses and gradient norms prior
    to gradient clipping across both training procedures.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们选择 TinyLLaMA 作为基础 LLM，并使用预训练值或随机分配来初始化二值化 LLM 参数。我们在所有其他训练细节上与 FBI-LLM
    保持一致。除了 $\mathbf{FF}_{\text{ratio}}$ 外，我们还监控了训练损失和梯度范数，在梯度裁剪之前对两种训练过程进行监控。
- en: '![Refer to caption](img/d00a5c88db8715475513d46bdcd33dbd.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d00a5c88db8715475513d46bdcd33dbd.png)'
- en: 'Figure 6: Gradient norm curves from different training procedures.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：不同训练过程中的梯度范数曲线。
- en: 'From Fig. [5(a)](#S5.F5.sf1 "In Figure 5 ‣ 5.1 Training from Scratch or Continue
    Training from Pretrained LLM? ‣ 5 Analysis ‣ FBI-LLM: Scaling Up Fully Binarized
    LLMs from Scratch via Autoregressive Distillation"), it is observed that during
    the beginning of training, the trend of $\mathbf{FF}_{\text{ratio}}$ step shown
    in Fig. [5(b)](#S5.F5.sf2 "In Figure 5 ‣ 5.1 Training from Scratch or Continue
    Training from Pretrained LLM? ‣ 5 Analysis ‣ FBI-LLM: Scaling Up Fully Binarized
    LLMs from Scratch via Autoregressive Distillation"), the training loss experiences
    similar issues. Additionally, Fig. [6](#S5.F6 "Figure 6 ‣ 5.1 Training from Scratch
    or Continue Training from Pretrained LLM? ‣ 5 Analysis ‣ FBI-LLM: Scaling Up Fully
    Binarized LLMs from Scratch via Autoregressive Distillation") highlights more
    pronounced changes in the gradient norms during the continuing training.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '从图 [5(a)](#S5.F5.sf1 "图 5 ‣ 5.1 从头训练还是继续从预训练 LLM 训练？ ‣ 5 分析 ‣ FBI-LLM: 从头开始通过自回归蒸馏扩展完全二值化
    LLMs") 可以观察到，在训练开始阶段，$\mathbf{FF}_{\text{ratio}}$ 的趋势如图 [5(b)](#S5.F5.sf2 "图 5
    ‣ 5.1 从头训练还是继续从预训练 LLM 训练？ ‣ 5 分析 ‣ FBI-LLM: 从头开始通过自回归蒸馏扩展完全二值化 LLMs") 所示，训练损失经历了类似的问题。此外，图
    [6](#S5.F6 "图 6 ‣ 5.1 从头训练还是继续从预训练 LLM 训练？ ‣ 5 分析 ‣ FBI-LLM: 从头开始通过自回归蒸馏扩展完全二值化
    LLMs") 突出了继续训练期间梯度范数的更显著变化。'
- en: These findings challenge our initial hypothesis that starting with a pretrained
    LLM would endow the binarized model with inherited knowledge, thus enhancing performance.
    Instead, they imply that binarization through training is not sensitive to the
    way of parameter initialization. Furthermore, we speculate that binarized and
    full-precision LLMs employ different parameter combinations and configurations
    to encode semantics, which results in substantial divergences in their parameter
    space pattern. To adapt this pattern, the optimization process for binarization
    by continuing training from a pretrained LLM might necessitate more profound parameter
    adjustments. This could partly explain why it is more unstable compared to training
    from scratch during the training.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现挑战了我们最初的假设，即从预训练的 LLM 开始会使二值化模型继承知识，从而提高性能。相反，它们暗示二值化通过训练对参数初始化的方式不敏感。此外，我们推测，二值化和全精度
    LLM 使用不同的参数组合和配置来编码语义，这导致它们的参数空间模式存在显著差异。为了适应这种模式，通过从预训练的 LLM 继续训练进行二值化的优化过程可能需要更深层次的参数调整。这在一定程度上解释了为什么与从头开始训练相比，它在训练过程中更不稳定。
- en: 5.2 Training Instability
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 训练不稳定性
- en: 'Both binary and full-precision LLM training have been found to exhibit unstable
    training behaviors [[19](#bib.bib19), [7](#bib.bib7), [38](#bib.bib38)]. Our FBI-LLM
    exhibits similar issues, specifically manifesting as sudden spikes in training
    loss when training 1.3B and 7B FBI-LLMs, which sometimes fail to converge after
    that. We adopt the solution similar to PaLM [[38](#bib.bib38)]: if the loss no
    longer tends to converge, the model reverts to the previous checkpoint and skips
    the data chunk that triggered the unstable loss to continue training. The model
    no longer encounters issues at the same training steps using this approach. We
    observe that pretraining the 7B FBI model from scratch has approximately a 6%
    probability of causing loss spikes. For the 1.3B model, training is more unstable
    due to the lower capability, with about a 15% probability of loss spikes. This
    is consistent with the pretraining behavior seen in real-valued LLMs while the
    probability of spiking is significantly higher, which may be related to the limited
    expressive capability of binary parameters. To handle this, we skip any data blocks
    where loss spikes occur.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '二值化和全精度 LLM 的训练都发现存在不稳定的训练行为 [[19](#bib.bib19), [7](#bib.bib7), [38](#bib.bib38)]。我们的
    FBI-LLM 也表现出类似的问题，特别是在训练 1.3B 和 7B FBI-LLM 时，训练损失突然上升，且有时在此之后无法收敛。我们采用类似 PaLM
    的解决方案 [[38](#bib.bib38)]: 如果损失不再趋于收敛，模型将回滚到之前的检查点，并跳过触发不稳定损失的数据块，以继续训练。使用这种方法，模型在相同的训练步骤中不再遇到问题。我们观察到，从头开始预训练
    7B FBI 模型大约有 6% 的概率导致损失峰值。对于 1.3B 模型，由于能力较低，训练更不稳定，损失峰值的概率约为 15%。这与在实际值 LLM 中看到的预训练行为一致，而峰值概率显著更高，这可能与二值参数的表达能力有限有关。为此，我们跳过损失峰值出现的数据块。'
- en: 5.3 Storage Efficiency
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 存储效率
- en: 'Table 3: Compression and extra parameters ratio.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 压缩和额外参数比例。'
- en: '| Model | Model Size | Storage Size | Compression Ratio | Extra Parameter Ratio
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 模型大小 | 存储大小 | 压缩比例 | 额外参数比例 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| LLaMA | 130M | 0.25GB | 59.26% | 0.119% |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA | 130M | 0.25GB | 59.26% | 0.119% |'
- en: '| FBI-LLM | 130M | 0.10GB |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| FBI-LLM | 130M | 0.10GB |'
- en: '| LLaMA | 1.3B | 2.54GB | 84.67% | 0.063% |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA | 1.3B | 2.54GB | 84.67% | 0.063% |'
- en: '| FBI-LLM | 1.3B | 0.39GB |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| FBI-LLM | 1.3B | 0.39GB |'
- en: '| LLaMA | 7B | 12.55GB | 90.07% | 0.034% |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA | 7B | 12.55GB | 90.07% | 0.034% |'
- en: '| FBI-LLM | 7B | 0.39GB |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| FBI-LLM | 7B | 0.39GB |'
- en: 'Table [3](#S5.T3 "Table 3 ‣ 5.3 Storage Efficiency ‣ 5 Analysis ‣ FBI-LLM:
    Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation")
    presents the theoretical storage space required by FBI-LLMs of various sizes compared
    to the full-precision LLaMA with the same structure. It also details the proportion
    of additional parameters ($\bm{\alpha}$) introduced by FBI-LLM. The comparison
    in the table demonstrates that FBI-LLM can achieve a high compression ratio, significantly
    reducing the storage burden of LLMs. Although the extra parameters for scaling
    and shifting introduced by FBI-LLM need to be retained in full precision, their
    proportion is minimal, rendering their impact on storage negligible.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [3](#S5.T3 "Table 3 ‣ 5.3 Storage Efficiency ‣ 5 Analysis ‣ FBI-LLM: Scaling
    Up Fully Binarized LLMs from Scratch via Autoregressive Distillation") 展示了与相同结构的全精度
    LLaMA 相比，各种规模 FBI-LLM 理论上所需的存储空间。它还详细说明了 FBI-LLM 引入的额外参数比例 ($\bm{\alpha}$)。表中的比较显示，FBI-LLM
    可以实现较高的压缩比，显著减轻 LLM 的存储负担。尽管 FBI-LLM 引入的缩放和偏移的额外参数需要以全精度保留，但其比例极小，因此对存储的影响可以忽略不计。'
- en: 5.4 Generation Cases
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 生成案例
- en: 'As illustrated in Fig. [7](#S5.F7 "Figure 7 ‣ 5.4 Generation Cases ‣ 5 Analysis
    ‣ FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation"),
    although the generation quality of FBI-LLMs does not fully match that of full-precision
    LLMs, FBI-LLMs can still generate fluent and meaningful content. Compared to BitNet
    b1.58, which has a higher parameter bit-width, FBI-LLMs demonstrate a better understanding
    of prompts and include more knowledge in some generated examples. This indicates
    that FBI-LLMs possess strong generative capabilities and contain sufficient knowledge.
    Furthermore, FBI-LLMs demonstrate the potential to scale up further, reaching
    new levels of intelligence while being more hardware-friendly for deployment.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [7](#S5.F7 "Figure 7 ‣ 5.4 Generation Cases ‣ 5 Analysis ‣ FBI-LLM: Scaling
    Up Fully Binarized LLMs from Scratch via Autoregressive Distillation") 所示，虽然 FBI-LLM
    的生成质量未完全匹配全精度 LLM，但 FBI-LLM 仍能生成流畅且有意义的内容。与具有更高参数位宽的 BitNet b1.58 相比，FBI-LLM 对提示的理解更佳，并在某些生成示例中包含了更多知识。这表明
    FBI-LLM 具有强大的生成能力，并且包含了足够的知识。此外，FBI-LLM 展现了进一步扩展的潜力，能够在达到更高智能水平的同时，更加适合硬件部署。'
- en: '![Refer to caption](img/ff440083cc6556549f5311b311ce18e3.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ff440083cc6556549f5311b311ce18e3.png)'
- en: 'Figure 7: Generation cases. We compare the outputs of the full-precision model,
    BitNet b1.58, and our FBI-LLM when given the same prompts.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：生成案例。我们对比了全精度模型、BitNet b1.58 和我们的 FBI-LLM 在相同提示下的输出。
- en: 6 Conclusion
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We have proposed a learning framework using autoregressive distillation for
    1-bit weight binarization of LLMs from scratch. Extensive experiments on models
    of various sizes of 130M, 1.3B, and 7B demonstrate that FBI-LLM outperforms strong
    baselines and strikes a good balance between model size and performance. We also
    analyze the capabilities, properties, and potential of these extremely low-bit
    models, providing guidance for future research.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种使用自回归蒸馏的学习框架，从零开始进行 1 位权重二值化的 LLM。对 130M、1.3B 和 7B 各种规模模型的大量实验表明，FBI-LLM
    优于强基线，并在模型规模和性能之间取得了良好的平衡。我们还分析了这些极低位模型的能力、特性和潜力，为未来的研究提供了指导。
- en: Limitations. Our proposed binarization framework significantly reduces the memory
    and computation consumptions of LLMs, providing potential for their efficient
    deployment. However, there are several limitations of our models. Firstly, our
    1-bit binarization inevitably incurs a performance loss compared to the original
    full-precision model. Additionally, the training process, which includes knowledge
    distillation, brings additional computational costs. Moreover, due to the unique
    nature of binarization, current hardware makes it difficult to directly support
    binarized LLMs to achieve real speedup. We also have not yet considered intermediate
    activation binarization which is the same as previous studies. Finally, potential
    ethical issues of pretrained LLMs, such as harmful biases, privacy concerns, and
    the spread of disinformation, are likely to persist after binarization in our
    LLMs.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 限制。我们提出的二值化框架显著减少了 LLM 的内存和计算消耗，为其高效部署提供了潜力。然而，我们的模型存在几个限制。首先，我们的 1-bit 二值化不可避免地导致相较于原始全精度模型的性能损失。此外，包括知识蒸馏在内的训练过程带来了额外的计算成本。此外，由于二值化的独特性质，当前硬件难以直接支持二值化
    LLM 实现真正的加速。我们还没有考虑中间激活二值化，这与以前的研究相同。最后，预训练 LLM 的潜在伦理问题，如有害偏见、隐私问题和虚假信息传播，在我们二值化的
    LLM 中可能依然存在。
- en: References
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang,
    Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei. The era of 1-bit llms: All large
    language models are in 1.58 bits. arXiv preprint arXiv:2402.17764, 2024.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang,
    Li Dong, Ruiping Wang, Jilong Xue 和 Furu Wei. 1-bit llms 时代：所有大型语言模型都在 1.58 位。arXiv
    预印本 arXiv:2402.17764, 2024。'
- en: '[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
    Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia
    Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat 等人.
    Gpt-4 技术报告。arXiv 预印本 arXiv:2303.08774, 2023。'
- en: '[3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar 等人. Llama：开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971, 2023。'
- en: '[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale
    等人. Llama 2：开放基础和微调聊天模型。arXiv 预印本 arXiv:2307.09288, 2023。'
- en: '[5] Yuzhang Shang, Zhihang Yuan, and Zhen Dong. Pb-llm: Partially binarized
    large language models. In The Twelfth International Conference on Learning Representations,
    2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Yuzhang Shang, Zhihang Yuan 和 Zhen Dong. Pb-llm：部分二值化的大型语言模型。第十二届国际学习表示大会，2023。'
- en: '[6] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework
    for accurate post-training quantization and pruning. ArXiv, abs/2208.11580, 2022.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Elias Frantar 和 Dan Alistarh. 最优脑压缩：准确的训练后量化和剪枝框架。ArXiv, abs/2208.11580,
    2022。'
- en: '[7] Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu,
    Weidong Liu, and Wanxiang Che. Onebit: Towards extremely low-bit large language
    models. arXiv preprint arXiv:2402.11295, 2024.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu,
    Weidong Liu 和 Wanxiang Che. Onebit：迈向极低位的大型语言模型。arXiv 预印本 arXiv:2402.11295, 2024。'
- en: '[8] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect:
    Training deep neural networks with binary weights during propagations. Advances
    in neural information processing systems, 28, 2015.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Matthieu Courbariaux, Yoshua Bengio 和 Jean-Pierre David. Binaryconnect：在传播过程中使用二值权重训练深度神经网络。神经信息处理系统进展，28,
    2015。'
- en: '[9] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
    Bengio. Binarized neural networks. Advances in neural information processing systems,
    29, 2016.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv 和 Yoshua
    Bengio. 二值化神经网络。神经信息处理系统进展，29, 2016。'
- en: '[10] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net:
    Imagenet classification using binary convolutional neural networks. In European
    conference on computer vision, pages 525–542. Springer, 2016.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 穆罕默德·拉斯特加里, 维森特·奥尔多涅斯, 约瑟夫·雷德蒙, 和阿里·法尔哈迪。Xnor-net：使用二进制卷积神经网络进行Imagenet分类。于欧洲计算机视觉会议,
    页码525–542。施普林格, 2016。'
- en: '[11] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou.
    Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth
    gradients. arXiv preprint arXiv:1606.06160, 2016.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 周曙昌, 吴宇鑫, 倪泽坤, 周新宇, 温赫, 和邹宇恒。Dorefa-net：使用低位宽梯度训练低位宽卷积神经网络。arXiv预印本 arXiv:1606.06160,
    2016。'
- en: '[12] Asit Mishra, Eriko Nurvitadhi, Jeffrey J Cook, and Debbie Marr. Wrpn:
    Wide reduced-precision networks. arXiv preprint arXiv:1709.01134, 2017.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 阿西特·米什拉, 诶里克·努尔维塔迪, 杰弗里·J·库克, 和黛比·马尔。Wrpn：宽度减少精度网络。arXiv预印本 arXiv:1709.01134,
    2017。'
- en: '[13] Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional
    neural networks. arXiv preprint arXiv:1711.11294, 2017.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 林晓凡, 邹聪, 和潘伟。迈向准确的二进制卷积神经网络。arXiv预印本 arXiv:1711.11294, 2017。'
- en: '[14] Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via
    distillation and quantization. arXiv preprint arXiv:1802.05668, 2018.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 安东尼奥·波利诺, 拉兹万·帕斯卡努, 和丹·阿利斯塔赫。通过蒸馏和量化进行模型压缩。arXiv预印本 arXiv:1802.05668,
    2018。'
- en: '[15] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating
    gradients through stochastic neurons for conditional computation. arXiv preprint
    arXiv:1308.3432, 2013.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 约书亚·本吉奥, 尼古拉斯·利奥纳德, 和亚伦·库尔维尔。通过随机神经元估计或传播梯度以进行条件计算。arXiv预印本 arXiv:1308.3432,
    2013。'
- en: '[16] Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet:
    Towards precise binary neural network with generalized activation functions. In
    Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part XIV 16, pages 143–159. Springer, 2020.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 刘泽春, 沈志强, 马里奥斯·萨夫维德斯, 和郑光廷。Reactnet：迈向精确的二进制神经网络及其广义激活函数。于计算机视觉–ECCV 2020:
    第16届欧洲会议, 英国格拉斯哥, 2020年8月23–28日, 论文集第XIV卷, 页码143–159。施普林格, 2020。'
- en: '[17] Sajad Darabi, Mouloud Belbahri, Matthieu Courbariaux, and Vahid Partovi
    Nia. BNN+: Improved binary network training, 2019.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 萨贾德·达拉比, 穆卢德·贝尔巴赫里, 马蒂厄·库尔巴里亚, 和瓦希德·帕尔托维·尼亚。BNN+：改进的二进制网络训练, 2019。'
- en: '[18] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong
    Liu, Michele Magno, and Xiaojuan Qi. Billm: Pushing the limit of post-training
    quantization for llms. arXiv preprint arXiv:2402.04291, 2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 黄伟, 刘杨东, 秦浩通, 李颖, 张世明, 刘翔龙, 米歇尔·马格诺, 和齐晓娟。Billm：推动LLMs后训练量化的极限。arXiv预印本
    arXiv:2402.04291, 2024。'
- en: '[19] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao
    Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers
    for large language models. arXiv preprint arXiv:2310.11453, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 王洪宇, 马树明, 董力, 黄绍汉, 王怀杰, 马灵霄, 杨帆, 王瑞平, 吴一, 和魏富如。Bitnet：为大型语言模型扩展1位变压器。arXiv预印本
    arXiv:2310.11453, 2023。'
- en: '[20] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    Improving language understanding by generative pre-training. 2018.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 亚历克·拉德福德, 卡尔蒂克·纳拉西曼, 提姆·萨利曼斯, 伊利亚·苏茨克弗, 等等。通过生成预训练提高语言理解。2018。'
- en: '[21] Jangho Kim, Yash Bhalgat, Jinwon Lee, Chirag Patel, and Nojun Kwak. Qkd:
    Quantization-aware knowledge distillation. arXiv preprint arXiv:1911.12491, 2019.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 金章浩, 雅什·巴尔加特, 李振远, 奇拉格·帕特尔, 和郭诺俊。Qkd：量化感知知识蒸馏。arXiv预印本 arXiv:1911.12491,
    2019。'
- en: '[22] Yoonho Boo, Sungho Shin, Jungwook Choi, and Wonyong Sung. Stochastic precision
    ensemble: self-knowledge distillation for quantized deep neural networks. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 35, pages 6794–6802,
    2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 裴云浩, 辛成浩, 崔钟旭, 和成元勇。随机精度集成：量化深度神经网络的自我知识蒸馏。于AAAI人工智能会议论文集, 第35卷, 页码6794–6802,
    2021。'
- en: '[23] Cuong Pham, Tuan Hoang, and Thanh-Toan Do. Collaborative multi-teacher
    knowledge distillation for learning low bit-width deep neural networks. In Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 6435–6443,
    2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 库翁·范, 黄春辉, 和陈全安。协作多教师知识蒸馏以学习低位宽深度神经网络。于IEEE/CVF冬季计算机视觉应用会议论文集, 页码6435–6443,
    2023。'
- en: '[24] Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun
    Lee, Jun Ma, and Harris Teague. Oh! we freeze: Improving quantized knowledge distillation
    via signal propagation analysis for large language models. arXiv preprint arXiv:2403.18159,
    2024.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun
    Lee, Jun Ma, 和 Harris Teague. 哦！我们冻结了：通过信号传播分析改进量化知识蒸馏以用于大型语言模型。arXiv 预印本 arXiv:2403.18159,
    2024。'
- en: '[25] Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan,
    Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, et al. Llm360: Towards
    fully transparent open-source llms. arXiv preprint arXiv:2312.06550, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan,
    Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, 等。LLM360：迈向完全透明的开源
    LLM。arXiv 预印本 arXiv:2312.06550, 2023。'
- en: '[26] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
    Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and
    Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora
    with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
    Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, 和
    Julien Launay. 用于 Falcon LLM 的 Refinedweb 数据集：超越精心策划的语料库，仅使用网络数据。arXiv 预印本 arXiv:2306.01116,
    2023。'
- en: '[27] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
    Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder:
    may the source be with you! arXiv preprint arXiv:2305.06161, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
    Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, 等。Starcoder：愿源代码与你同在！arXiv
    预印本 arXiv:2305.06161, 2023。'
- en: '[28] Together Computer. Redpajama: An open source recipe to reproduce llama
    training dataset, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Together Computer. Redpajama：一个开源配方以重现 Llama 训练数据集，2023。'
- en: '[29] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open
    pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, 等。Opt：开放预训练变换器语言模型。arXiv
    预印本 arXiv:2205.01068, 2022。'
- en: '[30] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An
    open-source small language model. arXiv preprint arXiv:2401.02385, 2024.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, 和 Wei Lu. Tinyllama：一个开源的小型语言模型。arXiv
    预印本 arXiv:2401.02385, 2024。'
- en: '[31] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
    Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of
    natural yes/no questions. ArXiv, abs/1905.10044, 2019.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
    Collins, 和 Kristina Toutanova. Boolq：探索自然是/否问题的意外困难。ArXiv, abs/1905.10044, 2019。'
- en: '[32] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning
    about physical commonsense in natural language. In Proceedings of the AAAI conference
    on artificial intelligence, volume 34, pages 7432–7439, 2020.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, 等。Piqa：推理自然语言中的物理常识。在
    AAAI 人工智能会议论文集中，第 34 卷，页码 7432–7439, 2020。'
- en: '[33] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    Hellaswag: Can a machine really finish your sentence? In Annual Meeting of the
    Association for Computational Linguistics, 2019.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, 和 Yejin Choi.
    Hellaswag：机器真的能完成你的句子吗？在计算语言学协会年会上，2019。'
- en: '[34] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
    Winogrande: An adversarial winograd schema challenge at scale. Communications
    of the ACM, 64(9):99–106, 2021.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, 和 Yejin Choi. Winogrande：一个大规模的对抗性
    Winograd 语料库挑战。ACM 通讯, 64(9):99–106, 2021。'
- en: '[35] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, 和 Oyvind Tafjord. 认为你已经解决了问答问题？试试 arc，AI2 推理挑战。ArXiv, abs/1803.05457,
    2018。'
- en: '[36] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a
    suit of armor conduct electricity? a new dataset for open book question answering.
    In Conference on Empirical Methods in Natural Language Processing, 2018.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Todor Mihaylov, Peter Clark, Tushar Khot, 和 Ashish Sabharwal. 一副盔甲能导电吗？一个新的开放书籍问答数据集。在自然语言处理经验方法会议上，2018。'
- en: '[37] Zechun Liu, Zhiqiang Shen, Shichao Li, Koen Helwegen, Dong Huang, and
    Kwang-Ting Cheng. How do adam and training strategies help bnns optimization.
    In International conference on machine learning, pages 6936–6946\. PMLR, 2021.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Zechun Liu, Zhiqiang Shen, Shichao Li, Koen Helwegen, Dong Huang, 和 Kwang-Ting
    Cheng. Adam 和训练策略如何帮助BNNs优化. 在国际机器学习会议上, 页码 6936–6946\. PMLR, 2021.'
- en: '[38] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine
    Learning Research, 24(240):1–113, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann 等. Palm: 通过路径扩展语言建模. 机器学习研究期刊, 24(240):1–113, 2023.'
- en: Appendix
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Broader Impacts
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 更广泛的影响
- en: Our proposed fully binarized large language models (FBI-LLM) require less computational
    power and memory in training and inference, making advanced AI technology accessible
    to organizations and individuals with limited resources. With reduced hardware
    requirements, smaller businesses, educational institutions, and non-profit organizations
    can implement LLMs, democratizing access to cutting-edge AI. Binarized models
    are more energy-efficient, which can significantly lower the carbon footprint
    associated with running large-scale AI applications. However, even binarized LLMs
    can still inherit and exist biases present in training data, leading to unfair
    outcomes in applications like hiring, law enforcement, and lending.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的完全二值化的大型语言模型（FBI-LLM）在训练和推理时需要更少的计算能力和内存，使有限资源的组织和个人能够使用先进的AI技术。由于硬件需求减少，小型企业、教育机构和非营利组织可以实施LLM，从而使前沿AI技术普及化。二值化模型更具能源效率，这可以显著降低运行大规模AI应用的碳足迹。然而，即使是二值化的LLM也可能继承和存在训练数据中存在的偏见，从而导致在招聘、执法和贷款等应用中的不公平结果。
- en: Appendix B Average Bit-width of Binarized LLM
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 二值化LLM的平均位宽
- en: 'This section explains how to calculate the average bit-width of a binarized
    LLM. Since the embedding layer and head have a large number of parameters and
    are not binarized, we do not consider them when calculating the average bit-width.
    Consider a module containing an RMSNorm and a linear layer with a parameter matrix
    $A\in\mathbb{R}^{n\times n}$ is quantized to 1 bit. Therefore, the average bit-width
    of this module can calculated as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释如何计算二值化LLM的平均位宽。由于嵌入层和头部具有大量参数且未二值化，因此在计算平均位宽时不考虑它们。考虑一个包含RMSNorm和具有参数矩阵$A\in\mathbb{R}^{n\times
    n}$的线性层被量化为1位。因此，该模块的平均位宽可以按如下方式计算：
- en: '|  | $\text{Average Bit-width}=\frac{1\times n^{2}+16\times 3n}{3n+n^{2}}$
    |  | (12) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{平均位宽}=\frac{1\times n^{2}+16\times 3n}{3n+n^{2}}$ |  | (12) |'
- en: Appendix C Model Configuration and Training Details
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 模型配置和训练细节
- en: 'In this section, we list the model configurations and training details for
    three scales of FBI-LLM models we trained in Table [4](#A3.T4 "Table 4 ‣ Appendix
    C Model Configuration and Training Details ‣ FBI-LLM: Scaling Up Fully Binarized
    LLMs from Scratch via Autoregressive Distillation").'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们列出了三种规模的FBI-LLM模型的配置和训练细节，见表[4](#A3.T4 "表 4 ‣ 附录 C 模型配置和训练细节 ‣ FBI-LLM:
    从零开始通过自回归蒸馏扩展完全二值化LLM")。'
- en: 'Table 4: The configuration and training details for FBI-LLM.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: FBI-LLM 的配置和训练细节。'
- en: '|  | FBI-LLM 130M | FBI-LLM 1.3B | FBI-LLM 7B |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | FBI-LLM 130M | FBI-LLM 1.3B | FBI-LLM 7B |'
- en: '| hidden size | 768 | 2,048 | 4,096 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏层大小 | 768 | 2,048 | 4,096 |'
- en: '| intermediate size | 2,048 | 5,632 | 11,008 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 中间层大小 | 2,048 | 5,632 | 11,008 |'
- en: '| max sequence length | 2,048 | 2,048 | 2,048 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 最大序列长度 | 2,048 | 2,048 | 2,048 |'
- en: '| # attention heads | 12 | 32 | 32 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| # 注意力头 | 12 | 32 | 32 |'
- en: '| # hidden layers | 12 | 24 | 32 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| # 隐藏层 | 12 | 24 | 32 |'
- en: '| # key value heads | 12 | 32 | 32 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| # 关键值头 | 12 | 32 | 32 |'
- en: '| initializer range | 0.02 | 0.02 | 0.02 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 初始化范围 | 0.02 | 0.02 | 0.02 |'
- en: '| vocabulary size | 32,000 | 32,000 | 32,000 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 词汇表大小 | 32,000 | 32,000 | 32,000 |'
- en: '| learning rate | 3e-4 | 3e-4 | 3e-4 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 3e-4 | 3e-4 | 3e-4 |'
- en: '| batch size (token) | 2M | 2.4M | 3.9M |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小（令牌） | 2M | 2.4M | 3.9M |'
- en: '| teacher model | LLaMA2-7B | LLaMA2-7B | LLaMA2-7B |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 教师模型 | LLaMA2-7B | LLaMA2-7B | LLaMA2-7B |'
- en: '| # GPUs (A100 80G) | 16 | 16 | 32 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| # GPUs (A100 80G) | 16 | 16 | 32 |'
- en: '| GPU hours for each data chunk | 130h | 189h | 729h |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 每个数据块的GPU小时数 | 130h | 189h | 729h |'
- en: '| training speed (tokens/s/GPU) | 7,800 | 5,300 | 1,200 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 训练速度（令牌/秒/GPU） | 7,800 | 5,300 | 1,200 |'
- en: Appendix D Detail Experiment Results
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 详细实验结果
- en: 'We list the detailed experiment results about the effectiveness of autoregressive
    distillation in Section [4.3](#S4.SS3 "4.3 Effectiveness of Autoregressive Distillation
    ‣ 4 Experiments ‣ FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive
    Distillation") in Table [5](#A4.T5 "Table 5 ‣ Appendix D Detail Experiment Results
    ‣ FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation").'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在第[4.3节](#S4.SS3 "4.3 自回归蒸馏的有效性 ‣ 4 实验 ‣ FBI-LLM: 从头开始通过自回归蒸馏扩展完全二值化的LLM")中详细列出了自回归蒸馏效果的实验结果，见表[5](#A4.T5
    "表 5 ‣ 附录 D 详细实验结果 ‣ FBI-LLM: 从头开始通过自回归蒸馏扩展完全二值化的LLM")。'
- en: 'Table 5: Performance on down stream tasks and perplexity for different training
    objectives. Here, NA means normal autoregressive training objective and AD means
    autoregressive distillation training objective. Based on the number of training
    chunk, the table is divided into several blocks. In each block, the values with
    an underline represent the best value.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：不同训练目标下下游任务和困惑度的性能。这里，NA表示普通的自回归训练目标，AD表示自回归蒸馏训练目标。根据训练块的数量，表格被分为几个块。在每个块中，带下划线的值表示最佳值。
- en: '| Training Chunk | Loss Type | Zero-shot Accuracy $\uparrow$ |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 训练块 | 损失类型 | 零-shot 准确率 $\uparrow$ |'
- en: '| BoolQ | PIQA | HS | WG | ARC-e | ARC-c | OBQA | Ave. | Wiki2 | PTB | C4 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | PIQA | HS | WG | ARC-e | ARC-c | OBQA | 平均值 | Wiki2 | PTB | C4 |'
- en: '| 1 | NA | 42.2 | 53.9 | 26.2 | 52.3 | 29.3 | 21.3 | 25.4 | 35.8 | 85.9 | 204.4
    | 63.9 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 1 | NA | 42.2 | 53.9 | 26.2 | 52.3 | 29.3 | 21.3 | 25.4 | 35.8 | 85.9 | 204.4
    | 63.9 |'
- en: '| AD | 50.1 | 54.1 | 26.6 | 51.0 | 29.2 | 21.9 | 24.4 | 36.8 | 81.2 | 193.4
    | 61.9 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| AD | 50.1 | 54.1 | 26.6 | 51.0 | 29.2 | 21.9 | 24.4 | 36.8 | 81.2 | 193.4
    | 61.9 |'
- en: '| 2 | NA | 60.8 | 56.7 | 27.0 | 52.2 | 31.9 | 20.2 | 23.2 | 38.9 | 42.9 | 128.8
    | 37.5 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 2 | NA | 60.8 | 56.7 | 27.0 | 52.2 | 31.9 | 20.2 | 23.2 | 38.9 | 42.9 | 128.8
    | 37.5 |'
- en: '| AD | 61.5 | 56.1 | 27.3 | 50.8 | 31.1 | 20.6 | 22.8 | 38.6 | 43.4 | 137.9
    | 37.3 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| AD | 61.5 | 56.1 | 27.3 | 50.8 | 31.1 | 20.6 | 22.8 | 38.6 | 43.4 | 137.9
    | 37.3 |'
- en: '| 3 | NA | 62.0 | 57.1 | 27.5 | 51.3 | 31.5 | 21.2 | 24.0 | 39.2 | 36.7 | 170.2
    | 32.8 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 3 | NA | 62.0 | 57.1 | 27.5 | 51.3 | 31.5 | 21.2 | 24.0 | 39.2 | 36.7 | 170.2
    | 32.8 |'
- en: '| AD | 62.2 | 58.1 | 27.7 | 51.1 | 33.0 | 21.3 | 25.4 | 39.8 | 34.9 | 145.2
    | 32.9 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| AD | 62.2 | 58.1 | 27.7 | 51.1 | 33.0 | 21.3 | 25.4 | 39.8 | 34.9 | 145.2
    | 32.9 |'
- en: '| 4 | NA | 61.0 | 57.9 | 27.6 | 52.7 | 32.6 | 21.0 | 23.4 | 39.5 | 34.3 | 159.6
    | 31.3 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 4 | NA | 61.0 | 57.9 | 27.6 | 52.7 | 32.6 | 21.0 | 23.4 | 39.5 | 34.3 | 159.6
    | 31.3 |'
- en: '| AD | 62.0 | 57.7 | 27.7 | 50.1 | 33.0 | 21.2 | 26.8 | 39.8 | 32.9 | 142.1
    | 31.0 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| AD | 62.0 | 57.7 | 27.7 | 50.1 | 33.0 | 21.2 | 26.8 | 39.8 | 32.9 | 142.1
    | 31.0 |'
- en: '| 5 | NA | 61.9 | 57.6 | 27.7 | 49.6 | 32.7 | 21.2 | 23.6 | 39.2 | 32.3 | 157.9
    | 29.7 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 5 | NA | 61.9 | 57.6 | 27.7 | 49.6 | 32.7 | 21.2 | 23.6 | 39.2 | 32.3 | 157.9
    | 29.7 |'
- en: '| AD | 62.2 | 58.4 | 28.0 | 50.8 | 32.9 | 21.3 | 25.8 | 39.9 | 31.7 | 137.0
    | 29.4 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| AD | 62.2 | 58.4 | 28.0 | 50.8 | 32.9 | 21.3 | 25.8 | 39.9 | 31.7 | 137.0
    | 29.4 |'
- en: '| 6 | NA | 62.1 | 59.5 | 27.9 | 53.1 | 32.9 | 21.8 | 24.0 | 40.2 | 32.4 | 147.4
    | 29.6 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 6 | NA | 62.1 | 59.5 | 27.9 | 53.1 | 32.9 | 21.8 | 24.0 | 40.2 | 32.4 | 147.4
    | 29.6 |'
- en: '| AD | 62.2 | 59.2 | 27.7 | 49.9 | 33.9 | 22.9 | 26.0 | 40.2 | 31.3 | 129.0
    | 29.4 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| AD | 62.2 | 59.2 | 27.7 | 49.9 | 33.9 | 22.9 | 26.0 | 40.2 | 31.3 | 129.0
    | 29.4 |'
- en: '| 7 | NA | 62.0 | 59.5 | 27.8 | 52.4 | 33.8 | 20.9 | 25.6 | 40.3 | 30.0 | 155.1
    | 28.5 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 7 | NA | 62.0 | 59.5 | 27.8 | 52.4 | 33.8 | 20.9 | 25.6 | 40.3 | 30.0 | 155.1
    | 28.5 |'
- en: '| AD | 62.1 | 59.5 | 28.3 | 51.2 | 33.8 | 21.8 | 25.4 | 40.3 | 31.0 | 119.3
    | 28.8 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| AD | 62.1 | 59.5 | 28.3 | 51.2 | 33.8 | 21.8 | 25.4 | 40.3 | 31.0 | 119.3
    | 28.8 |'
- en: '| 8 | NA | 61.6 | 58.8 | 28.3 | 51.0 | 33.6 | 21.3 | 23.4 | 39.7 | 29.5 | 140.0
    | 28.0 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 8 | NA | 61.6 | 58.8 | 28.3 | 51.0 | 33.6 | 21.3 | 23.4 | 39.7 | 29.5 | 140.0
    | 28.0 |'
- en: '| AD | 62.2 | 59.0 | 28.1 | 50.8 | 32.3 | 20.6 | 24.0 | 39.6 | 30.1 | 113.8
    | 28.9 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| AD | 62.2 | 59.0 | 28.1 | 50.8 | 32.3 | 20.6 | 24.0 | 39.6 | 30.1 | 113.8
    | 28.9 |'
- en: '| 9 | NA | 60.9 | 58.9 | 28.2 | 51.7 | 34.6 | 20.0 | 23.4 | 39.7 | 29.7 | 129.3
    | 28.4 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 9 | NA | 60.9 | 58.9 | 28.2 | 51.7 | 34.6 | 20.0 | 23.4 | 39.7 | 29.7 | 129.3
    | 28.4 |'
- en: '| AD | 62.2 | 60.1 | 28.2 | 51.8 | 33.5 | 21.1 | 26.6 | 40.5 | 29.9 | 129.8
    | 27.9 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| AD | 62.2 | 60.1 | 28.2 | 51.8 | 33.5 | 21.1 | 26.6 | 40.5 | 29.9 | 129.8
    | 27.9 |'
- en: '| 10 | NA | 61.8 | 59.5 | 27.9 | 50.3 | 34.0 | 20.8 | 25.8 | 40.0 | 29.5 |
    144.7 | 28.1 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 10 | NA | 61.8 | 59.5 | 27.9 | 50.3 | 34.0 | 20.8 | 25.8 | 40.0 | 29.5 |
    144.7 | 28.1 |'
- en: '| AD | 62.2 | 58.5 | 28.0 | 51.4 | 34.2 | 21.6 | 26.2 | 40.3 | 30.1 | 122.8
    | 28.1 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| AD | 62.2 | 58.5 | 28.0 | 51.4 | 34.2 | 21.6 | 26.2 | 40.3 | 30.1 | 122.8
    | 28.1 |'
- en: '| 11 | NA | 62.1 | 59.9 | 28.4 | 52.7 | 34.1 | 21.6 | 25.4 | 40.6 | 28.8 |
    138.2 | 27.5 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 11 | NA | 62.1 | 59.9 | 28.4 | 52.7 | 34.1 | 21.6 | 25.4 | 40.6 | 28.8 |
    138.2 | 27.5 |'
- en: '| AD | 62.2 | 59.0 | 28.0 | 49.9 | 34.0 | 20.7 | 25.2 | 39.9 | 29.0 | 119.9
    | 27.6 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| AD | 62.2 | 59.0 | 28.0 | 49.9 | 34.0 | 20.7 | 25.2 | 39.9 | 29.0 | 119.9
    | 27.6 |'
- en: '| 12 | NA | 62.2 | 59.1 | 27.9 | 51.9 | 34.6 | 21.2 | 24.8 | 40.2 | 29.1 |
    150.3 | 27.2 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 12 | NA | 62.2 | 59.1 | 27.9 | 51.9 | 34.6 | 21.2 | 24.8 | 40.2 | 29.1 |
    150.3 | 27.2 |'
- en: '| AD | 62.1 | 59.2 | 28.1 | 52.3 | 34.0 | 21.2 | 24.2 | 40.2 | 28.6 | 113.0
    | 27.3 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| AD | 62.1 | 59.2 | 28.1 | 52.3 | 34.0 | 21.2 | 24.2 | 40.2 | 28.6 | 113.0
    | 27.3 |'
- en: '| 13 | NA | 62.2 | 58.6 | 27.9 | 49.6 | 34.6 | 22.6 | 25.4 | 40.1 | 28.7 |
    135.0 | 27.0 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 13 | NA | 62.2 | 58.6 | 27.9 | 49.6 | 34.6 | 22.6 | 25.4 | 40.1 | 28.7 |
    135.0 | 27.0 |'
- en: '| AD | 62.1 | 58.8 | 28.2 | 49.4 | 33.5 | 21.6 | 25.0 | 39.8 | 28.4 | 134.0
    | 27.3 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| AD | 62.1 | 58.8 | 28.2 | 49.4 | 33.5 | 21.6 | 25.0 | 39.8 | 28.4 | 134.0
    | 27.3 |'
- en: '| 14 | NA | 62.2 | 58.3 | 28.4 | 49.4 | 34.3 | 20.7 | 23.8 | 39.6 | 28.6 |
    138.5 | 26.9 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 14 | NA | 62.2 | 58.3 | 28.4 | 49.4 | 34.3 | 20.7 | 23.8 | 39.6 | 28.6 |
    138.5 | 26.9 |'
- en: '| AD | 61.5 | 58.5 | 28.3 | 51.0 | 33.9 | 21.2 | 26.0 | 40.1 | 28.3 | 123.6
    | 27.3 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| AD | 61.5 | 58.5 | 28.3 | 51.0 | 33.9 | 21.2 | 26.0 | 40.1 | 28.3 | 123.6
    | 27.3 |'
- en: '| 15 | NA | 62.2 | 58.8 | 28.1 | 51.5 | 34.1 | 21.3 | 26.4 | 40.4 | 28.0 |
    138.9 | 26.9 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 15 | NA | 62.2 | 58.8 | 28.1 | 51.5 | 34.1 | 21.3 | 26.4 | 40.4 | 28.0 |
    138.9 | 26.9 |'
- en: '| AD | 62.2 | 58.8 | 28.0 | 51.5 | 34.8 | 20.9 | 26.2 | 40.4 | 28.5 | 138.3
    | 27.4 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| AD | 62.2 | 58.8 | 28.0 | 51.5 | 34.8 | 20.9 | 26.2 | 40.4 | 28.5 | 138.3
    | 27.4 |'
- en: '| 16 | NA | 61.3 | 58.4 | 28.3 | 51.7 | 34.6 | 21.7 | 25.8 | 40.3 | 28.6 |
    151.9 | 27.0 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 16 | NA | 61.3 | 58.4 | 28.3 | 51.7 | 34.6 | 21.7 | 25.8 | 40.3 | 28.6 |
    151.9 | 27.0 |'
- en: '| AD | 62.1 | 60.2 | 28.1 | 51.9 | 33.2 | 22.3 | 25.2 | 40.4 | 28.2 | 124.6
    | 27.2 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| AD | 62.1 | 60.2 | 28.1 | 51.9 | 33.2 | 22.3 | 25.2 | 40.4 | 28.2 | 124.6
    | 27.2 |'
