- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:52:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:52:21'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Ranking LLMs by compression
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 的压缩排名
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.14171](https://ar5iv.labs.arxiv.org/html/2406.14171)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.14171](https://ar5iv.labs.arxiv.org/html/2406.14171)
- en: Peijia Guo^(1,2) , Ziguang Li ³ , Haibo Hu ³ , Chao Huang ^(3∗), Ming Li ^(4∗)
    , Rui Zhang ^(1∗)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Peijia Guo^(1,2) , Ziguang Li ³ , Haibo Hu ³ , Chao Huang ^(3∗), Ming Li ^(4∗)
    , Rui Zhang ^(1∗)
- en: ¹ School of Mathematics, Northwest University, Xi’an, China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 西北大学数学学院，中国西安
- en: ²Shanghai Institute for Mathematics and Interdisciplinary Sciences, Shanghai,
    China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²上海数学与跨学科科学研究所，中国上海
- en: ³ Institute of Computing Technology, Chinese Academy of Sciences, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 中国科学院计算技术研究所，中国
- en: ⁴Cheriton School of Computer Science, University of Waterloo, Ontario, Canada
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴滑铁卢大学计算机科学系，加拿大安大略省
- en: Guopeijia0929@163.com      chriszggz@gamil.com      huhaibo22@mails.ucas.ac.cn
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Guopeijia0929@163.com      chriszggz@gamil.com      huhaibo22@mails.ucas.ac.cn
- en: chuang@ict.ac.cn      mli@uwaterloo.ca      rzhang@nwu.edu.cn
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: chuang@ict.ac.cn      mli@uwaterloo.ca      rzhang@nwu.edu.cn
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We conceptualize the process of understanding as information compression, and
    propose a method for ranking large language models (LLMs) based on lossless data
    compression. We demonstrate the equivalence of compression length under arithmetic
    coding with cumulative negative log probabilities when using a large language
    model as a prior, that is, the pre-training phase of the model is essentially
    the process of learning the optimal coding length. At the same time, the evaluation
    metric compression ratio can be obtained without actual compression, which greatly
    saves overhead. In this paper, we use five large language models as priors for
    compression, then compare their performance on challenging natural language processing
    tasks, including sentence completion, question answering, and coreference resolution.
    Experimental results show that compression ratio and model performance are positively
    correlated, so it can be used as a general metric to evaluate large language models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将理解过程概念化为信息压缩，并提出了一种基于无损数据压缩的大型语言模型（LLMs）排名方法。我们证明了在使用大型语言模型作为先验时，算术编码下的压缩长度与累积负对数概率的等价性，即模型的预训练阶段本质上是学习最佳编码长度的过程。同时，评估指标压缩比可以在无需实际压缩的情况下获得，这大大节省了开销。在本文中，我们使用五个大型语言模型作为压缩的先验，然后比较它们在具有挑战性的自然语言处理任务上的表现，包括句子完成、问答和指代消解。实验结果表明，压缩比与模型性能正相关，因此可以作为评估大型语言模型的一种通用指标。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In recent years, the rapid development of LLMs has brought earth-shaking changes
    to the field of natural language processing (NLP) (Radford et al., [2019](#bib.bib19); Zhao
    et al., [2023](#bib.bib30); Liu et al., [2023](#bib.bib15)). LLMs are advanced
    language models pretrained on tens of gigabytes of data without tuning on data
    for specific tasks. These large models can directly complete various NLP tasks,
    and even become a milestone technology towards general artificial intelligence
    (AGI). Currently, LLMs are being studied more and more widely in various fields,
    such as education and research (Rahman and Watanobe, [2023](#bib.bib20)), medicine
    and healthcare (Thirunavukarasu et al., [2023](#bib.bib23); Cascella et al., [2023](#bib.bib3)),
    etc., and their performance evaluation methods are becoming more and more important.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）的快速发展给自然语言处理（NLP）领域带来了巨大的变化（Radford 等，[2019](#bib.bib19)；赵等，[2023](#bib.bib30)；刘等，[2023](#bib.bib15)）。LLMs
    是在数十吉字节数据上预训练的先进语言模型，而不是在特定任务数据上进行调整。这些大型模型可以直接完成各种 NLP 任务，甚至成为通用人工智能（AGI）技术的一个里程碑。目前，LLMs
    在教育与研究（Rahman 和 Watanobe，[2023](#bib.bib20)）、医学与健康护理（Thirunavukarasu 等，[2023](#bib.bib23)；Cascella
    等，[2023](#bib.bib3)）等多个领域的研究越来越广泛，其性能评估方法也变得越来越重要。
- en: Chang et al. ([2024](#bib.bib4)) showed that researchers always scrutinize the
    capabilities of AI models or algorithms through evaluation using specific and
    challenging tasks, so the evaluation metrics are outlined from the perspective
    of the evaluation tasks. The metrics are diverse, such as Exact Match (EM), F1-score,
    ROUGE, etc., and many are set for specific tasks, making it difficult to uniformly
    evaluate the performance of the model on different tasks. In addition, contamination
    of training and test data can also lead to biased evaluation results (Magar and
    Schwartz, [2022](#bib.bib16)), making it impossible to verify whether NLP progress
    is achieved through better language understanding or better data utilization.
    Various limitations lead to the lack of a unified LLMs evaluation standard.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Chang 等人 ([2024](#bib.bib4)) 表明，研究人员总是通过使用具体且具有挑战性的任务来仔细审查 AI 模型或算法的能力，因此评估指标从评估任务的角度进行了概述。这些指标多种多样，如精确匹配（EM）、F1-score、ROUGE
    等，许多指标是为特定任务设定的，这使得在不同任务上统一评估模型的性能变得困难。此外，训练和测试数据的污染也可能导致评估结果的偏差（Magar 和 Schwartz，[2022](#bib.bib16)），使得无法验证
    NLP 进展是通过更好的语言理解还是更好的数据利用实现的。各种限制导致缺乏统一的 LLMs 评估标准。
- en: Therefore, we consider the process of model training and learning itself and
    prove the equivalence of the model pre-training goal and the compression length
    under arithmetic coding, indicating that compression is closely related to model
    performance, and then use the compression ratio as a general metric to measure
    the model’s generalization ability in different scenarios.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们考虑了模型训练和学习过程本身，并证明了模型预训练目标与算术编码下的压缩长度之间的等效性，表明压缩与模型性能密切相关，然后使用压缩比作为通用度量来衡量模型在不同场景下的泛化能力。
- en: 2 Related Work
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Language Models Evaluation
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 语言模型评估
- en: Currently, performance evaluation of LLMs is mainly achieved through benchmark
    tests, including diverse tasks, standardized datasets and comprehensive evaluation
    metrics. The purpose is to establish a systematic and standardized evaluation
    framework.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，对大型语言模型（LLMs）的性能评估主要通过基准测试实现，包括多样化的任务、标准化的数据集和综合评估指标。其目的是建立一个系统化和标准化的评估框架。
- en: In 2019, Wang et al. ([2019](#bib.bib27)) introduced the General Language Understanding
    Evaluation Benchmark (GLUE), a multi-task evaluation platform for measuring the
    performance of natural language understanding models. It contains nine tasks,
    covering various types such as text classification, text similarity evaluation,
    natural language Inference, question answering, etc. A recent study Laskar et al.
    ([2023](#bib.bib13)) evaluated ChatGPT across 140 tasks and analyze 255K responses
    it generates in these datasets, laying the foundation for deploying ChatGPT-like
    LLMs in real-world applications. More recently, OpenAI et al. ([2024](#bib.bib18))
    tested GPT-4 on a diverse set of benchmarks, including 34 simulating exams that
    were originally designed for humans. Benchmark test is very important for evaluating
    the performance of language models and promoting research progress, but limited
    coverage tasks, data contamination (Brown et al., [2020](#bib.bib2); Li, [2023](#bib.bib14)),
    and huge overhead are all challenges and limitations faced in this process. In
    order to solve these problems, we propose compression ratio based on lossless
    data compression, a general evaluation metric.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年，Wang 等人 ([2019](#bib.bib27)) 引入了通用语言理解评估基准（GLUE），这是一个用于衡量自然语言理解模型性能的多任务评估平台。它包含九个任务，涵盖了文本分类、文本相似性评估、自然语言推理、问答等各种类型。最近的研究
    Laskar 等人 ([2023](#bib.bib13)) 对 ChatGPT 在140个任务中的表现进行了评估，并分析了它在这些数据集中生成的 255K
    条响应，为在实际应用中部署类似 ChatGPT 的 LLMs 奠定了基础。最近，OpenAI 等人 ([2024](#bib.bib18)) 在一组多样化的基准测试中测试了
    GPT-4，包括34个最初为人类设计的模拟考试。基准测试对于评估语言模型的性能和推动研究进展非常重要，但有限的覆盖任务、数据污染（Brown 等人，[2020](#bib.bib2);
    Li，[2023](#bib.bib14)）以及巨大的开销都是在这个过程中面临的挑战和限制。为了解决这些问题，我们提出了基于无损数据压缩的压缩比作为通用评估指标。
- en: 2.2 Neural Compression
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 神经压缩
- en: 'The goal of data compression is to reduce the representation size while retaining
    valid information. Our LLMs-based compressor uses neural networks for data compression
    and belongs to the neural compression category. Current research in neural compression
    largely benefits from advances in deep generative modeling (Yang et al., [2023](#bib.bib29)),
    such as GANs (Goodfellow et al., [2014](#bib.bib7)), VAEs (Rezende and Mohamed,
    [2015](#bib.bib21)), and autoregressive models (Van Den Oord et al., [2016](#bib.bib26)).
    With the development of deep neural networks, lossless text compression has also
    ushered in new progress. Goyal et al. ([2018](#bib.bib8))  proposed DeepZip, a
    lossless compressor based on neural networks, consisting of two main modules:
    RNN and arithmetic coding. It achieves higher compression ratio than GZIP.  Bellard
    ([2019](#bib.bib1))  proposed a lossless compressor based on LSTM, which is simple
    to describe and has reasonable memory consumption compared to compressors that
    provide a similar compression ratio. Recent advancements, such as TRACE, a fast
    transformer-based general-purpose lossless compressor (Mao et al., [2022](#bib.bib17)),
    achieves an overall speedup of approximately 3x while maintaining a compression
    ratio comparable to state-of-the-art compressors.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据压缩的目标是减少表示大小，同时保留有效信息。我们的基于 LLM 的压缩器使用神经网络进行数据压缩，并属于神经压缩类别。目前在神经压缩领域的研究主要受益于深度生成建模的进展（Yang
    等，[2023](#bib.bib29)），例如 GANs（Goodfellow 等，[2014](#bib.bib7)）、VAEs（Rezende 和 Mohamed，[2015](#bib.bib21)）以及自回归模型（Van
    Den Oord 等，[2016](#bib.bib26)）。随着深度神经网络的发展，无损文本压缩也取得了新的进展。Goyal 等人（[2018](#bib.bib8)）提出了
    DeepZip，一种基于神经网络的无损压缩器，包括两个主要模块：RNN 和算术编码。它比 GZIP 实现了更高的压缩比。Bellard（[2019](#bib.bib1)）提出了一种基于
    LSTM 的无损压缩器，相较于提供类似压缩比的压缩器，其描述简单且内存消耗合理。最近的进展，如 TRACE，一种快速的基于变换器的通用无损压缩器（Mao 等，[2022](#bib.bib17)），在保持与最先进压缩器类似的压缩比的同时，实现了大约
    3 倍的整体加速。
- en: 3 Method
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 3.1 LLMs based Arithmetic Coding for Compression
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于 LLM 的算术编码压缩
- en: Shannon’s fundamental theorem of coding states that (Shannon, [1948](#bib.bib22)),
    given messages randomly generated from a model, it is impossible to encode them
    into less bits (on average) than the entropy of that model, thus defining a lower
    bound for lossless compression. Arithmetic coding is an entropy coding algorithm.
    Huang et al. ([2023](#bib.bib10)) proposed an entropy-based compressor that integrated
    generative pre-trained transformer into adaptive arithmetic coding, highlighting
    the potential of pre-trained LLMs as powerful priors in compression. In this paper,
    we integrate LLMs into adaptive arithmetic coding for compression, with the aim
    of representing data according to the probability of output to reduce its overall
    size.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 香农的编码基本定理指出（Shannon，[1948](#bib.bib22)），对于从模型中随机生成的消息，不可能将它们编码成比该模型的熵更少的比特（平均而言），从而定义了无损压缩的下界。算术编码是一种熵编码算法。Huang
    等人（[2023](#bib.bib10)）提出了一种基于熵的压缩器，将生成预训练变换器集成到自适应算术编码中，突显了预训练 LLM 作为强大先验在压缩中的潜力。在本文中，我们将
    LLM 集成到自适应算术编码中进行压缩，旨在根据输出的概率表示数据，从而减少其总体大小。
- en: LLMs as Entropy Models
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 作为熵模型
- en: Considering text data, first use a tokenizer to convert the text into a data
    stream $t_{1:n}:=t_{1}t_{2}\cdots t_{n}\in T^{n}$ acts as the entropy model, guiding
    the encoder to allocate fewer bits to high-frequency tokens and more bits to low-frequency
    tokens, thereby improving compression efficiency.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑文本数据时，首先使用分词器将文本转换为数据流 $t_{1:n}:=t_{1}t_{2}\cdots t_{n}\in T^{n}$，作为熵模型，指导编码器将更少的比特分配给高频符号，将更多的比特分配给低频符号，从而提高压缩效率。
- en: Coding Process
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 编码过程
- en: 'The range for the data stream is the interval $\left[0,1\right)$. Then narrow
    the interval to the part assigned to that token:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流的范围是区间 $\left[0,1\right)$。然后将区间缩小到分配给该符号的部分：
- en: '|  | $1$2 |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '| Algorithm 1 Arithmetic Coding |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 算法 1 算术编码 |'
- en: '| 1: Input: $t_{0:n}:=t_{0}t_{1}\cdots t_{n}\in T^{n+1}$. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 1: 输入: $t_{0:n}:=t_{0}t_{1}\cdots t_{n}\in T^{n+1}$. |'
- en: '| 2: $I_{low}^{0}=0,~{}I_{high}^{0}=1$ |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 2: $I_{low}^{0}=0,~{}I_{high}^{0}=1$ |'
- en: '| 3: for  $t_{i},~{}i=1,2,\cdots,n,n+1$   do |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 3: 对于 $t_{i},~{}i=1,2,\cdots,n,n+1$   执行 |'
- en: '| 4:     $range=I_{high}^{i-1}-I_{low}^{i-1}$ |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 4:     $range=I_{high}^{i-1}-I_{low}^{i-1}$ |'
- en: '| 5:     $I_{low}^{i}\leftarrow I_{low}^{i-1}+range*F_{i}(t_{i})$ |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 5:     $I_{low}^{i}\leftarrow I_{low}^{i-1}+range*F_{i}(t_{i})$ |'
- en: '| 6:     $1$2 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 6:     $1$2 |'
- en: '| 7: end for |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 7: 结束循环 |'
- en: '| 8: Output: $[I_{low}^{n+1},I_{high}^{n+1})$. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 8: 输出: $[I_{low}^{n+1},I_{high}^{n+1})$. |'
- en: Adaptive arithmetic coding using LLM is shown in Algorithm 1.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLM 的自适应算术编码如算法 1 所示。
- en: 3.2 Equivalence of Model Pre-training Goal and Compression Length
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 模型预训练目标与压缩长度的等价性
- en: It is well established that compression and prediction are essentially equivalent
     (Delétang et al., [2023](#bib.bib5)). In this way, compression and LLMs are closely
    linked. We mathematically prove the equivalence of model pre-training goal and
    compression length. Then we present a novel method for evaluating LLMS based on
    lossless compression.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩和预测本质上是等价的（Delétang et al., [2023](#bib.bib5)）。因此，压缩和 LLMs 紧密相关。我们数学上证明了模型预训练目标与压缩长度的等价性。然后，我们提出了一种基于无损压缩评估
    LLMs 的新方法。
- en: Pre-training Optimization Goals for LLMs
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 的预训练优化目标
- en: The loss function, also known as the objective function, measures the difference
    between the probability distribution predicted by the model and the true distribution.
    Model training is to reduce the loss function through continuous iteration, thereby
    optimizing model performance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数，也称为目标函数，衡量模型预测的概率分布与真实分布之间的差异。模型训练的目的是通过不断迭代来减少损失函数，从而优化模型性能。
- en: We continue to consider the data stream above $t_{1:n}:=t_{1}t_{2}\cdots t_{n}\in
    T^{n}$.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续考虑上述数据流 $t_{1:n}:=t_{1}t_{2}\cdots t_{n}\in T^{n}$。
- en: 'Now we have the true distribution $Q$, which can elicit the definition of relative
    entropy, that is, Kullback-Leibler Divergence:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了真实分布 $Q$，这可以引出相对熵的定义，即 Kullback-Leibler 散度：
- en: '|  | $D_{KL}(Q&#124;&#124;P)={\sum_{i=1}^{n}(Q_{i}\log_{2}{Q_{i}})}-{\sum_{i=1}^{n}(Q_{i}\log_{2}{P_{i}})}$
    |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $D_{KL}(Q&#124;&#124;P)={\sum_{i=1}^{n}(Q_{i}\log_{2}{Q_{i}})}-{\sum_{i=1}^{n}(Q_{i}\log_{2}{P_{i}})}$
    |  |'
- en: The previous term ${\sum_{i=1}^{n}(Q_{i}\cdot\log_{2}{Q_{i}})}$, that is, to
    minimize the value of cross entropy. It further illustrates that cross entropy
    can be used as the loss function, and minimizing cross entropy is the goal of
    optimizing the model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的项 ${\sum_{i=1}^{n}(Q_{i}\cdot\log_{2}{Q_{i}})}$，即最小化交叉熵值。这进一步说明交叉熵可以作为损失函数，而最小化交叉熵是优化模型的目标。
- en: Negative Log Probability as Compression Length
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 负对数概率作为压缩长度
- en: The goal of lossless compression is to encode a data stream $t_{1:n}$.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 无损压缩的目标是对数据流 $t_{1:n}$ 进行编码。
- en: Therefore, in the process of achieving lossless compression, minimizing the
    expected length of the encoded data stream is equivalent to minimizing cross entropy.
    At this point, the equivalence of model pre-training goal and compression length
    has been proven. Furthermore, we can use compression ratio as a unified criterion
    for evaluating LLMs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在实现无损压缩的过程中，最小化编码数据流的期望长度等同于最小化交叉熵。此时，模型预训练目标与压缩长度的等价性已被证明。此外，我们可以使用压缩比作为评估
    LLMs 的统一标准。
- en: 4 Experiments
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'The experiment consists of four key parts: the calculation of the compression
    ratio and three natural language processing tasks, namely sentence completion,
    question answering and coreference resolution. We use a total of five LLMs as
    compressor priors, but the proposed method is not limited to these models. This
    method can be applied to more advanced LLMs as long as the predicted probabilities
    can be obtained.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 实验包含四个关键部分：压缩比计算以及三个自然语言处理任务，即句子完成、问答和共指消解。我们使用了五个 LLMs 作为压缩器先验，但所提出的方法并不限于这些模型。只要可以获得预测概率，该方法也可以应用于更先进的
    LLMs。
- en: 4.1 The Calculation of Compression Ratio
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 压缩比的计算
- en: First, we select the Text8 dataset to calculate the compression ratio of the
    compressor. The Text8 dataset is a large corpus extracted from the English Wikipedia.
    After some simple preprocessing, the text content covers various topics and fields.
    It is a general dataset for language modeling.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们选择 Text8 数据集来计算压缩比。Text8 数据集是从英文维基百科提取的大型语料库。经过一些简单的预处理，文本内容涵盖了各种话题和领域。它是一个用于语言建模的通用数据集。
- en: 'We split the read Text8 file by spaces and obtain a list containing all words.
    Then every 200 words are divided into a sublist, and the 200-length word fragment
    are converted into strings. The list of the first 10,000 strings is passed to
    the LLMs compressor as a parameter. The compression ratio calculation formula
    is as follows (in bits):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过空格拆分读取的 Text8 文件，得到一个包含所有单词的列表。然后将每200个单词分成一个子列表，并将200长度的单词片段转换为字符串。前10,000个字符串的列表作为参数传递给
    LLMs 压缩器。压缩比计算公式如下（以比特为单位）：
- en: '|  | $1$2 |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: The LLM compressors involved include LLaMA 2 7B released by Meta, Mistral 7B
    released by the Mistral AI team, OPT-IML 1.3B released by Facebook, and GPT-2-XL 1.5B
    and GPT-2 774M released by OpenAI. Their calculated compression ratios are shown
    in Table 1.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及的LLM压缩器包括Meta发布的LLaMA 2 7B、Mistral AI团队发布的Mistral 7B、Facebook发布的OPT-IML 1.3B，以及OpenAI发布的GPT-2-XL
    1.5B和GPT-2 774M。它们计算出的压缩比见表1。
- en: '| Compressor | Compression Ratio |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 压缩器 | 压缩比 |'
- en: '| --- | --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| LLaMA 2 7B | 8.663 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA 2 7B | 8.663 |'
- en: '| Mistral 7B | 9.266 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Mistral 7B | 9.266 |'
- en: '| OPT-IML 1.3B | 6.938 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| OPT-IML 1.3B | 6.938 |'
- en: '| GPT-2-XL 1.5B | 7.095 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2-XL 1.5B | 7.095 |'
- en: '| GPT-2 774M | 6.864 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 774M | 6.864 |'
- en: 'Table 1: Compression ratios of different compressors.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同压缩器的压缩比。
- en: 4.2 Sentence Completion
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 句子完成
- en: Sentence completion is designed to allow the computer to predict the missing
    parts based on the given context, so that the sentence becomes coherent and complete.
    We compare the performance of three large models, LLaMA 2 7B, Mistral 7B and GPT-2-XL 1.5B
    on the HellaSwag dataset, using accuracy as a metric. The results are shown in
    Table 2.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 句子完成旨在让计算机根据给定的上下文预测缺失的部分，使句子变得连贯和完整。我们比较了三个大型模型LLaMA 2 7B、Mistral 7B和GPT-2-XL
    1.5B在HellaSwag数据集上的表现，使用准确率作为评估指标。结果见表2。
- en: '| LLM | Accuracy(%) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| LLM | Accuracy(%) |'
- en: '| --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Mistral 7B | 81.3 (Jiang et al., [2023](#bib.bib12)) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Mistral 7B | 81.3 (Jiang et al., [2023](#bib.bib12)) |'
- en: '| LLaMA 2 7B | 77.2 (Touvron et al., [2023](#bib.bib24)) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA 2 7B | 77.2 (Touvron et al., [2023](#bib.bib24)) |'
- en: '| GPT-2-XL 1.5B | 50.9 (Wu et al., [2023](#bib.bib28)) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2-XL 1.5B | 50.9 (Wu et al., [2023](#bib.bib28)) |'
- en: 'Table 2: Performance on sentence completion .'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：句子完成任务中的表现。
- en: 4.3 Question Answering
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 问答系统
- en: The goal of question answering is to enable the computer to understand the questions
    raised by users through semantic understanding and syntax analysis, and then generate
    answers that meet the requirements of the questions. Because any form of LLM evaluation
    can be seen as question answering or switch to this format, so it is a very important
    means for LLMs evaluation(Guo et al., [2023](#bib.bib9)). We compare the performance
    of two large models, LLaMA 2 7B and OPT-IML 1.3B on the BoolQ dataset, using accuracy
    as a metric. The results are shown in Table 3.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 问答系统的目标是使计算机通过语义理解和句法分析来理解用户提出的问题，然后生成满足问题要求的答案。因为任何形式的LLM评估都可以视为问答或转化为这种格式，因此它是LLM评估的一个非常重要的手段(Guo
    et al., [2023](#bib.bib9))。我们比较了两个大型模型LLaMA 2 7B和OPT-IML 1.3B在BoolQ数据集上的表现，使用准确率作为评估指标。结果见表3。
- en: '| LLM | Accuracy(%) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| LLM | Accuracy(%) |'
- en: '| --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| LLaMA 2 7B | 77.4 (Touvron et al., [2023](#bib.bib24)) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA 2 7B | 77.4 (Touvron et al., [2023](#bib.bib24)) |'
- en: '| OPT-IML 1.3B | 61.5 (Iyer et al., [2023](#bib.bib11)) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| OPT-IML 1.3B | 61.5 (Iyer et al., [2023](#bib.bib11)) |'
- en: 'Table 3: Performance on question answering.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：问答任务中的表现。
- en: 4.4 Coreference Resolution
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 指代消解
- en: Coreference resolution is to identify the entities referred to by pronouns and
    noun phrases in the text. It has many practical applications in natural language
    processing, such as information extraction, text summarization, etc. Correct parsing
    of reference relationships can help computers better understand text. We compares
    the performance of two large models, GPT-2-XL 1.5B and GPT-2 774M on the Winograd
    Schema Challenge data set, using accuracy as a metric. The results are shown in
    Table 4.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 指代消解是识别文本中由代词和名词短语所指代的实体。它在自然语言处理中的应用非常广泛，如信息提取、文本摘要等。正确解析指代关系可以帮助计算机更好地理解文本。我们比较了两个大型模型GPT-2-XL
    1.5B和GPT-2 774M在Winograd Schema Challenge数据集上的表现，使用准确率作为评估指标。结果见表4。
- en: '| LLM | Accuracy(%) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| LLM | Accuracy(%) |'
- en: '| --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GPT-2-XL 1.5B | 73.3 (Wu et al., [2023](#bib.bib28)) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2-XL 1.5B | 73.3 (Wu et al., [2023](#bib.bib28)) |'
- en: '| GPT-2 774M | 69.2 (Trichelair et al., [2018](#bib.bib25)) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 774M | 69.2 (Trichelair et al., [2018](#bib.bib25)) |'
- en: 'Table 4: Performance on coreference resolution.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：指代消解中的表现。
- en: 4.5 Result Analysis
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 结果分析
- en: 'From the above experiments, it can be concluded that: the better data compression
    effect of LLM, the better its performance in natural language processing tasks.
    That is, there is a positive correlation between compression ratio and model performance.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述实验可以得出结论：LLM的压缩效果越好，其在自然语言处理任务中的表现也越好。即，压缩比与模型性能之间存在正相关关系。
- en: When we can effectively compress data, it means that we have captured the key
    characteristics and patterns of the data. This is similar to finding patterns
    and redundancies in the data during the model learning process. So we can say
    that if a large language model achieves the best lossless compression on a dataset,
    it will often achieve the best generalization on other datasets.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们能够有效地压缩数据时，这意味着我们已经捕捉到了数据的关键特征和模式。这类似于在模型学习过程中找到数据中的模式和冗余。因此，我们可以说，如果一个大型语言模型在某个数据集上实现了最佳的无损压缩，它通常也会在其他数据集上实现最佳的泛化。
- en: 'Therefore, the experimental results further verify the theoretical conclusion
    of this paper: compression ratio can be used as a general metric to measure the
    performance of LLMs.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，实验结果进一步验证了本文的理论结论：压缩比可以作为衡量大型语言模型（LLMs）性能的通用指标。
- en: 5 Conclusion
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We proposed to rank LLMs through lossless data compression in this paper. Our
    method measures compression ratios as a metric for generalization. We demonstrate
    the equivalence of compression length under arithmetic coding and LLMs pre-training
    goal, saving the overhead of actual compression. This further illustrates that
    understanding is compression, demonstrated by our experiments across challenging
    downstream NLP tasks.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本文中提出通过无损数据压缩对LLMs进行排名。我们的方法将压缩比作为泛化的指标。我们展示了算术编码下的压缩长度与LLMs预训练目标的等价性，从而节省了实际压缩的开销。这进一步说明了理解即是压缩，这在我们对具有挑战性的下游NLP任务的实验中得到了证明。
- en: 6 Limitations
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 局限性
- en: For NLP tasks, the experiments in this paper only used the open source version
    of the pre-trained language model, which was subject to computational constraints
    and scale limitations. Furthermore evaluation is not the end goal but the starting
    point. A mature evaluation system should not only provide conclusions about performance,
    but also provide analysis and guidance for future research and development, which
    is also our future research direction.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NLP任务，本文中的实验仅使用了开源版本的预训练语言模型，受到计算限制和规模限制。此外，评估不是最终目标，而是起点。一个成熟的评估系统不仅应提供关于性能的结论，还应提供未来研究和发展的分析和指导，这也是我们未来的研究方向。
- en: 7 Statement
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 声明
- en: 'We take academic integrity and research independence very seriously. Here we
    would like to declare that parts of this paper overlap with a published paper.
    Overlaps include ideas presented, experimental methods. Information about the
    published paper is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们非常重视学术诚信和研究独立性。在此，我们声明本文的部分内容与已发布的论文有重叠。重叠内容包括提出的想法和实验方法。已发布论文的信息如下：
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Title: Compression Represents Intelligence Linearly'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标题：压缩线性地代表智能
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Author: Yuzhen Huang, Jinghan Zhang, Zifei Shan, Junxian He'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者：黄宇震，张晶涵，单子霏，何俊贤
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: arXiv:2404.09937 [cs.CL]
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: arXiv:2404.09937 [cs.CL]
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Submission date: April 15, 2024'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提交日期：2024年4月15日
- en: When we began our work, we were unaware of the existence of this published paper.
    Our study began on December 2023, was completed on May 2024, and was submitted
    on June 20, 2024.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始我们的工作时，我们并不知道这篇已发布论文的存在。我们的研究始于2023年12月，完成于2024年5月，并于2024年6月20日提交。
- en: References
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bellard (2019) Fabrice Bellard. 2019. Lossless data compression with neural
    networks. *URL: https://bellard. org/nncp/nncp. pdf*.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellard（2019）Fabrice Bellard。2019年。使用神经网络进行无损数据压缩。*网址： https://bellard.org/nncp/nncp.pdf*。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等（2020）Tom Brown，Benjamin Mann，Nick Ryder，Melanie Subbiah，Jared D Kaplan，Prafulla
    Dhariwal，Arvind Neelakantan，Pranav Shyam，Girish Sastry，Amanda Askell等。2020年。语言模型是少样本学习者。*神经信息处理系统进展*，33：1877–1901。
- en: 'Cascella et al. (2023) Marco Cascella, Jonathan Montomoli, Valentina Bellini,
    and Elena Bignami. 2023. Evaluating the feasibility of chatgpt in healthcare:
    an analysis of multiple clinical and research scenarios. *Journal of medical systems*,
    47(1):33.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cascella等（2023）Marco Cascella，Jonathan Montomoli，Valentina Bellini，Elena Bignami。2023年。评估ChatGPT在医疗保健中的可行性：对多种临床和研究场景的分析。*医学系统杂志*，47(1)：33。
- en: Chang et al. (2024) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang,
    Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A
    survey on evaluation of large language models. *ACM Transactions on Intelligent
    Systems and Technology*, 15(3):1–45.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等 (2024) Yupeng Chang、Xu Wang、Jindong Wang、Yuan Wu、Linyi Yang、Kaijie Zhu、Hao
    Chen、Xiaoyuan Yi、Cunxiang Wang、Yidong Wang 等。2024年。关于大语言模型评估的调查。*ACM 智能系统与技术杂志*，15(3):1–45。
- en: Delétang et al. (2023) Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne,
    Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang,
    Matthew Aitchison, Laurent Orseau, et al. 2023. Language modeling is compression.
    *arXiv preprint arXiv:2309.10668*.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delétang 等 (2023) Grégoire Delétang、Anian Ruoss、Paul-Ambroise Duquenne、Elliot
    Catt、Tim Genewein、Christopher Mattern、Jordi Grau-Moya、Li Kevin Wenliang、Matthew
    Aitchison、Laurent Orseau 等。2023年。语言建模即压缩。*arXiv 预印本 arXiv:2309.10668*。
- en: Gibbs (1878) Josiah Willard Gibbs. 1878. On the equilibrium of heterogeneous
    substances. *American Journal of Science*, 3(96):441–458.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gibbs (1878) Josiah Willard Gibbs。1878年。关于异质物质的平衡。*美国科学杂志*，3(96):441–458。
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.
    Generative adversarial nets. *Advances in neural information processing systems*,
    27.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 (2014) Ian Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing Xu、David
    Warde-Farley、Sherjil Ozair、Aaron Courville 和 Yoshua Bengio。2014年。生成对抗网络。*神经信息处理系统进展*，27。
- en: 'Goyal et al. (2018) Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia
    Ochoa. 2018. [Deepzip: Lossless data compression using recurrent neural networks](http://arxiv.org/abs/1811.08162).'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goyal 等 (2018) Mohit Goyal、Kedar Tatwawadi、Shubham Chandak 和 Idoia Ochoa。2018年。[Deepzip:
    使用递归神经网络的无损数据压缩](http://arxiv.org/abs/1811.08162)。'
- en: 'Guo et al. (2023) Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi,
    Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, et al. 2023. Evaluating
    large language models: A comprehensive survey. *arXiv preprint arXiv:2310.19736*.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2023) Zishan Guo、Renren Jin、Chuang Liu、Yufei Huang、Dan Shi、Linhao Yu、Yan
    Liu、Jiaxuan Li、Bojian Xiong、Deyi Xiong 等。2023年。评估大语言模型：一个全面的调查。*arXiv 预印本 arXiv:2310.19736*。
- en: Huang et al. (2023) Cynthia Huang, Yuqing Xie, Zhiying Jiang, Jimmy Lin, and
    Ming Li. 2023. Approximating human-like few-shot learning with gpt-based compression.
    *arXiv preprint arXiv:2308.06942*.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2023) Cynthia Huang、Yuqing Xie、Zhiying Jiang、Jimmy Lin 和 Ming Li。2023年。用基于GPT的压缩来逼近类人少样本学习。*arXiv
    预印本 arXiv:2308.06942*。
- en: 'Iyer et al. (2023) Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor
    Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh
    Koura, Xian Li, Brian O’Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli
    Celikyilmaz, Luke Zettlemoyer, and Ves Stoyanov. 2023. [Opt-iml: Scaling language
    model instruction meta learning through the lens of generalization](http://arxiv.org/abs/2212.12017).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Iyer 等 (2023) Srinivasan Iyer、Xi Victoria Lin、Ramakanth Pasunuru、Todor Mihaylov、Daniel
    Simig、Ping Yu、Kurt Shuster、Tianlu Wang、Qing Liu、Punit Singh Koura、Xian Li、Brian
    O’Horo、Gabriel Pereyra、Jeff Wang、Christopher Dewan、Asli Celikyilmaz、Luke Zettlemoyer
    和 Ves Stoyanov。2023年。[Opt-iml: 从泛化的角度扩展语言模型指令元学习](http://arxiv.org/abs/2212.12017)。'
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2023) Albert Q Jiang、Alexandre Sablayrolles、Arthur Mensch、Chris Bamford、Devendra
    Singh Chaplot、Diego de las Casas、Florian Bressand、Gianna Lengyel、Guillaume Lample、Lucile
    Saulnier 等。2023年。Mistral 7b。*arXiv 预印本 arXiv:2310.06825*。
- en: Laskar et al. (2023) Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman,
    Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. 2023. [A systematic
    study and comprehensive evaluation of chatgpt on benchmark datasets](http://arxiv.org/abs/2305.18486).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laskar 等 (2023) Md Tahmid Rahman Laskar、M Saiful Bari、Mizanur Rahman、Md Amran
    Hossen Bhuiyan、Shafiq Joty 和 Jimmy Xiangji Huang。2023年。[对 ChatGPT 在基准数据集上的系统研究和全面评估](http://arxiv.org/abs/2305.18486)。
- en: Li (2023) Yucheng Li. 2023. An open source data contamination report for llama
    series models. *arXiv preprint arXiv:2310.17589*.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li (2023) Yucheng Li。2023年。Llama系列模型的开源数据污染报告。*arXiv 预印本 arXiv:2310.17589*。
- en: Liu et al. (2023) Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan
    Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, Zihao Wu,
    Lin Zhao, Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen, Tianming Liu, and Bao
    Ge. 2023. [Summary of chatgpt-related research and perspective towards the future
    of large language models](https://doi.org/https://doi.org/10.1016/j.metrad.2023.100017).
    *Meta-Radiology*, 1(2):100017.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023) Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang,
    Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, Zihao Wu, Lin Zhao,
    Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen, Tianming Liu, 和 Bao Ge. 2023.
    [ChatGPT 相关研究总结及对大型语言模型未来的展望](https://doi.org/https://doi.org/10.1016/j.metrad.2023.100017).
    *Meta-Radiology*, 1(2):100017.
- en: 'Magar and Schwartz (2022) Inbal Magar and Roy Schwartz. 2022. [Data contamination:
    From memorization to exploitation](http://arxiv.org/abs/2203.08242).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Magar 和 Schwartz (2022) Inbal Magar 和 Roy Schwartz. 2022. [数据污染：从记忆到利用](http://arxiv.org/abs/2203.08242)。
- en: 'Mao et al. (2022) Yu Mao, Yufei Cui, Tei-Wei Kuo, and Chun Jason Xue. 2022.
    Trace: A fast transformer-based general-purpose lossless compressor. In *Proceedings
    of the ACM Web Conference 2022*, pages 1829–1838.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mao 等人 (2022) Yu Mao, Yufei Cui, Tei-Wei Kuo, 和 Chun Jason Xue. 2022. Trace:
    一种快速的基于变换器的通用无损压缩器。在 *ACM Web Conference 2022 论文集* 中，页码 1829–1838。'
- en: OpenAI et al. (2024) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,
    Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,
    Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
    Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
    Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek
    Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey
    Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
    Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,
    Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
    Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
    Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane
    Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,
    Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon
    Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
    Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,
    Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,
    Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina
    Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
    Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,
    Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul
    Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
    Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli,
    Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,
    John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
    Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,
    Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
    Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
    Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and
    Barret Zoph. 2024. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等人（2024年）OpenAI，Josh Achiam，Steven Adler，Sandhini Agarwal，Lama Ahmad，Ilge
    Akkaya，Florencia Leoni Aleman，Diogo Almeida，Janko Altenschmidt，Sam Altman，Shyamal
    Anadkat，Red Avila，Igor Babuschkin，Suchir Balaji，Valerie Balcom，Paul Baltescu，Haiming
    Bao，Mohammad Bavarian，Jeff Belgum，Irwan Bello，Jake Berdine，Gabriel Bernadett-Shapiro，Christopher
    Berner，Lenny Bogdonoff，Oleg Boiko，Madelaine Boyd，Anna-Luisa Brakman，Greg Brockman，Tim
    Brooks，Miles Brundage，Kevin Button，Trevor Cai，Rosie Campbell，Andrew Cann，Brittany
    Carey，Chelsea Carlson，Rory Carmichael，Brooke Chan，Che Chang，Fotis Chantzis，Derek
    Chen，Sully Chen，Ruby Chen，Jason Chen，Mark Chen，Ben Chess，Chester Cho，Casey Chu，Hyung
    Won Chung，Dave Cummings，Jeremiah Currier，Yunxing Dai，Cory Decareaux，Thomas Degry，Noah
    Deutsch，Damien Deville，Arka Dhar，David Dohan，Steve Dowling，Sheila Dunning，Adrien
    Ecoffet，Atty Eleti，Tyna Eloundou，David Farhi，Liam Fedus，Niko Felix，Simón Posada
    Fishman，Juston Forte，Isabella Fulford，Leo Gao，Elie Georges，Christian Gibson，Vik
    Goel，Tarun Gogineni，Gabriel Goh，Rapha Gontijo-Lopes，Jonathan Gordon，Morgan Grafstein，Scott
    Gray，Ryan Greene，Joshua Gross，Shixiang Shane Gu，Yufei Guo，Chris Hallacy，Jesse
    Han，Jeff Harris，Yuchen He，Mike Heaton，Johannes Heidecke，Chris Hesse，Alan Hickey，Wade
    Hickey，Peter Hoeschele，Brandon Houghton，Kenny Hsu，Shengli Hu，Xin Hu，Joost Huizinga，Shantanu
    Jain，Shawn Jain，Joanne Jang，Angela Jiang，Roger Jiang，Haozhun Jin，Denny Jin，Shino
    Jomoto，Billie Jonn，Heewoo Jun，Tomer Kaftan，Łukasz Kaiser，Ali Kamali，Ingmar Kanitscheider，Nitish
    Shirish Keskar，Tabarak Khan，Logan Kilpatrick，Jong Wook Kim，Christina Kim，Yongjik
    Kim，Jan Hendrik Kirchner，Jamie Kiros，Matt Knight，Daniel Kokotajlo，Łukasz Kondraciuk，Andrew
    Kondrich，Aris Konstantinidis，Kyle Kosic，Gretchen Krueger，Vishal Kuo，Michael Lampe，Ikai
    Lan，Teddy Lee，Jan Leike，Jade Leung，Daniel Levy，Chak Ming Li，Rachel Lim，Molly Lin，Stephanie
    Lin，Mateusz Litwin，Theresa Lopez，Ryan Lowe，Patricia Lue，Anna Makanju，Kim Malfacini，Sam
    Manning，Todor Markov，Yaniv Markovski，Bianca Martin，Katie Mayer，Andrew Mayne，Bob
    McGrew，Scott Mayer McKinney，Christine McLeavey，Paul McMillan，Jake McNeil，David
    Medina，Aalok Mehta，Jacob Menick，Luke Metz，Andrey Mishchenko，Pamela Mishkin，Vinnie
    Monaco，Evan Morikawa，Daniel Mossing，Tong Mu，Mira Murati，Oleg Murk，David Mély，Ashvin
    Nair，Reiichiro Nakano，Rajeev Nayak，Arvind Neelakantan，Richard Ngo，Hyeonwoo Noh，Long
    Ouyang，Cullen O’Keefe，Jakub Pachocki，Alex Paino，Joe Palermo，Ashley Pantuliano，Giambattista
    Parascandolo，Joel Parish，Emy Parparita，Alex Passos，Mikhail Pavlov，Andrew Peng，Adam
    Perelman，Filipe de Avila Belbute Peres，Michael Petrov，Henrique Ponde de Oliveira
    Pinto，Michael Pokorny，Michelle Pokrass，Vitchyr H. Pong，Tolly Powell，Alethea Power，Boris
    Power，Elizabeth Proehl，Raul Puri，Alec Radford，Jack Rae，Aditya Ramesh，Cameron Raymond，Francis
    Real，Kendra Rimbach，Carl Ross，Bob Rotsted，Henri Roussez，Nick Ryder，Mario Saltarelli，Ted
    Sanders，Shibani Santurkar，Girish Sastry，Heather Schmidt，David Schnurr，John Schulman，Daniel
    Selsam，Kyla Sheppard，Toki Sherbakov，Jessica Shieh，Sarah Shoker，Pranav Shyam，Szymon
    Sidor，Eric Sigler，Maddie Simens，Jordan Sitkin，Katarina Slama，Ian Sohl，Benjamin
    Sokolowsky，Yang Song，Natalie Staudacher，Felipe Petroski Such，Natalie Summers，Ilya
    Sutskever，Jie Tang，Nikolas Tezak，Madeleine B. Thompson，Phil Tillet，Amin Tootoonchian，Elizabeth
    Tseng，Preston Tuggle，Nick Turley，Jerry Tworek，Juan Felipe Cerón Uribe，Andrea Vallone，Arun
    Vijayvergiya，Chelsea Voss，Carroll Wainwright，Justin Jay Wang，Alvin Wang，Ben Wang，Jonathan
    Ward，Jason Wei，CJ Weinmann，Akila Welihinda，Peter Welinder，Jiayi Weng，Lilian Weng，Matt
    Wiethoff，Dave Willner，Clemens Winter，Samuel Wolrich，Hannah Wong，Lauren Workman，Sherwin
    Wu，Jeff Wu，Michael Wu，Kai Xiao，Tao Xu，Sarah Yoo，Kevin Yu，Qiming Yuan，Wojciech
    Zaremba，Rowan Zellers，Chong Zhang，Marvin Zhang，Shengjia Zhao，Tianhao Zheng，Juntang
    Zhuang，William Zhuk 和 Barret Zoph。2024年。[GPT-4 技术报告](http://arxiv.org/abs/2303.08774)。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人（2019）Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever 等人。2019年。语言模型是无监督的多任务学习者。*OpenAI 博客*，1(8):9。
- en: 'Rahman and Watanobe (2023) Md. Mostafizer Rahman and Yutaka Watanobe. 2023.
    [Chatgpt for education and research: Opportunities, threats, and strategies](https://doi.org/10.3390/app13095783).
    *Applied Sciences*, 13(9).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rahman 和 Watanobe（2023）Md. Mostafizer Rahman 和 Yutaka Watanobe。2023年。[Chatgpt
    在教育和研究中的应用：机遇、威胁和策略](https://doi.org/10.3390/app13095783)。*应用科学*，13(9)。
- en: Rezende and Mohamed (2015) Danilo Rezende and Shakir Mohamed. 2015. Variational
    inference with normalizing flows. In *International conference on machine learning*,
    pages 1530–1538\. PMLR.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rezende 和 Mohamed（2015）Danilo Rezende 和 Shakir Mohamed。2015年。用正则化流进行变分推断。见于
    *国际机器学习会议*，第1530–1538页。PMLR。
- en: Shannon (1948) Claude Elwood Shannon. 1948. A mathematical theory of communication.
    *The Bell system technical journal*, 27(3):379–423.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shannon（1948）Claude Elwood Shannon。1948年。通信的数学理论。*贝尔系统技术期刊*，27(3):379–423。
- en: Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting,
    Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023.
    Large language models in medicine. *Nature medicine*, 29(8):1930–1940.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thirunavukarasu 等人（2023）Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan
    Elangovan, Laura Gutierrez, Ting Fang Tan, 和 Daniel Shu Wei Ting。2023年。医学中的大型语言模型。*自然医学*，29(8):1930–1940。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2023）Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等人。2023年。Llama 2：开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*。
- en: 'Trichelair et al. (2018) Paul Trichelair, Ali Emami, Adam Trischler, Kaheer
    Suleman, and Jackie Chi Kit Cheung. 2018. How reasonable are common-sense reasoning
    tasks: A case-study on the winograd schema challenge and swag. *arXiv preprint
    arXiv:1811.01778*.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trichelair 等人（2018）Paul Trichelair, Ali Emami, Adam Trischler, Kaheer Suleman,
    和 Jackie Chi Kit Cheung。2018年。常识推理任务的合理性如何：以 Winograd 方案挑战和 SWAG 为例。*arXiv 预印本
    arXiv:1811.01778*。
- en: Van Den Oord et al. (2016) Aäron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.
    2016. Pixel recurrent neural networks. In *International conference on machine
    learning*, pages 1747–1756\. PMLR.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Den Oord 等人（2016）Aäron Van Den Oord, Nal Kalchbrenner, 和 Koray Kavukcuoglu。2016年。像素递归神经网络。见于
    *国际机器学习会议*，第1747–1756页。PMLR。
- en: 'Wang et al. (2019) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. 2019. [Glue: A multi-task benchmark and analysis
    platform for natural language understanding](http://arxiv.org/abs/1804.07461).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2019）Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,
    和 Samuel R. Bowman。2019年。[Glue：一个用于自然语言理解的多任务基准和分析平台](http://arxiv.org/abs/1804.07461)。
- en: 'Wu et al. (2023) Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed,
    and Alham Fikri Aji. 2023. Lamini-lm: A diverse herd of distilled models from
    large-scale instructions. *arXiv preprint arXiv:2304.14402*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人（2023）Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, 和 Alham
    Fikri Aji。2023年。Lamini-lm：来自大规模指令的多样化精炼模型。*arXiv 预印本 arXiv:2304.14402*。
- en: Yang et al. (2023) Yibo Yang, Stephan Mandt, Lucas Theis, et al. 2023. An introduction
    to neural data compression. *Foundations and Trends® in Computer Graphics and
    Vision*, 15(2):113–200.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人（2023）Yibo Yang, Stephan Mandt, Lucas Theis 等人。2023年。神经数据压缩简介。*计算机图形学和视觉的基础与趋势®*，15(2):113–200。
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan
    Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li,
    Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. [A survey
    of large language models](http://arxiv.org/abs/2303.18223).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2023）Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang,
    Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen
    Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,
    Zikang Liu, Peiyu Liu, Jian-Yun Nie, 和 Ji-Rong Wen。2023年。[大型语言模型的调查](http://arxiv.org/abs/2303.18223)。
