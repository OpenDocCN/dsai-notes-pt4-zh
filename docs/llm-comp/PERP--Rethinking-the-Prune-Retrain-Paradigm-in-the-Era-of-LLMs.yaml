- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 19:05:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:05:03'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'PERP: 在LLMs时代重新思考剪枝-再训练范式'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.15230](https://ar5iv.labs.arxiv.org/html/2312.15230)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.15230](https://ar5iv.labs.arxiv.org/html/2312.15230)
- en: Max Zimmer¹, Megi Andoni¹, Christoph Spiegel¹ & Sebastian Pokutta^(1,2)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Max Zimmer¹, Megi Andoni¹, Christoph Spiegel¹ & Sebastian Pokutta^(1,2)
- en: ¹Department for AI in Society, Science, and Technology, Zuse Institute Berlin,
    Germany
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹社会、科学与技术中的人工智能部门，柏林朱塞研究所，德国
- en: ²Institute of Mathematics, Technische Universität Berlin, Germany
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²数学研究所，柏林工业大学，德国
- en: '{zimmer,andoni,spiegel,pokutta}@zib.de'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{zimmer,andoni,spiegel,pokutta}@zib.de'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Neural Networks can be efficiently compressed through *pruning*, significantly
    reducing storage and computational demands while maintaining predictive performance.
    Simple yet effective methods like Iterative Magnitude Pruning (IMP) (Han et al.,
    [2015](#bib.bib16)) remove less important parameters and require a costly retraining
    procedure to recover performance after pruning. However, with the rise of Large
    Language Models (LLMs), full retraining has become infeasible due to memory and
    compute constraints. In this study, we challenge the practice of retraining all
    parameters by demonstrating that updating only a small subset of highly expressive
    parameters is often sufficient to recover or even improve performance compared
    to full retraining. Surprisingly, retraining as little as 0.27%-0.35% of the parameters
    of GPT-architectures achieves comparable performance to One Shot IMP across various
    sparsity levels. Our approach, Parameter-Efficient Retraining after Pruning (PERP),
    drastically reduces compute and memory demands, enabling pruning and retraining
    of up to 30 billion parameter models on a *single* NVIDIA A100 GPU within minutes.
    Despite magnitude pruning being considered as unsuited for pruning LLMs, our findings
    show that PERP positions it as a strong contender against state-of-the-art retraining-free
    approaches such as Wanda (Sun et al., [2023](#bib.bib55)) and SparseGPT (Frantar
    & Alistarh, [2023](#bib.bib12)), opening up a promising alternative to avoiding
    retraining.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以通过*剪枝*高效压缩，大幅度降低存储和计算需求，同时保持预测性能。像迭代幅度剪枝（IMP）（Han 等，[2015](#bib.bib16)）这样简单却有效的方法，会移除不太重要的参数，并需要一个昂贵的再训练过程来恢复剪枝后的性能。然而，随着大型语言模型（LLMs）的兴起，由于内存和计算限制，完全再训练已变得不可行。在这项研究中，我们挑战了完全再训练的做法，展示了仅更新一小部分高度表达的参数通常足以恢复甚至提高性能，与完全再训练相比，效果相当令人惊讶。仅对GPT架构的0.27%-0.35%参数进行再训练，即可在各种稀疏性水平下实现与单次IMP相当的性能。我们的方法——剪枝后的参数高效再训练（PERP），大幅度减少了计算和内存需求，使得在*单个*
    NVIDIA A100 GPU上在几分钟内对多达300亿参数的模型进行剪枝和再训练成为可能。尽管幅度剪枝被认为不适用于剪枝LLMs，但我们的发现表明，PERP将其定位为与Wanda（Sun
    等，[2023](#bib.bib55)）和SparseGPT（Frantar & Alistarh，[2023](#bib.bib12)）等最先进的无再训练方法竞争的有力对手，为避免再训练开辟了一个有前景的替代方案。
- en: Machine Learning, ICML
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '*Pruning* (Han et al., [2015](#bib.bib16); Gale et al., [2019](#bib.bib13);
    Lin et al., [2020](#bib.bib39); Hoefler et al., [2021](#bib.bib20); Zimmer et al.,
    [2022](#bib.bib75)) is among the state-of-the-art techniques to reduce the compute
    and storage requirements of Neural Networks, allowing to benefit from the extensive
    over-parametrization of modern architectures (Zhang et al., [2016](#bib.bib69))
    throughout training while maintaining high performance with lower resource demands
    during deployment. Arguably simple yet effective approaches to obtaining such
    *sparse* models follow the *prune after training* paradigm and are exemplified
    by Iterative Magnitude Pruning (IMP) (Han et al., [2015](#bib.bib16)), which starts
    from a pretrained *dense* model and iteratively removes seemingly unimportant
    parameters followed by retraining to compensate for pruning-induced performance
    degradation.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*剪枝*（Han 等，[2015](#bib.bib16)；Gale 等，[2019](#bib.bib13)；Lin 等，[2020](#bib.bib39)；Hoefler
    等，[2021](#bib.bib20)；Zimmer 等，[2022](#bib.bib75)）是减少神经网络计算和存储需求的最先进技术之一，它允许在训练过程中利用现代架构的广泛过参数化（Zhang
    等，[2016](#bib.bib69)），同时在部署期间以较低的资源需求维持高性能。可以说，获取这种*稀疏*模型的简单而有效的方法遵循*训练后剪枝*的范式，以迭代幅度剪枝（IMP）（Han
    等，[2015](#bib.bib16)）为例，从一个预训练的*密集*模型开始，迭代地移除看似不重要的参数，然后再训练以弥补剪枝引起的性能下降。'
- en: Despite its popularity, IMP suffers from being computationally expensive, potentially
    having to perform many prune-retrain cycles and retraining epochs to obtain well-performing
    models that are sufficiently compressed for the task at hand. Especially given
    the surge in popularity of *transfer learning*, in which huge pretrained models
    are reused and fine-tuned to specific tasks, a procedure such as IMP can be prohibitive
    for practitioners dealing with resource constrained environments (Frantar & Alistarh,
    [2023](#bib.bib12)), even when performing just a single prune-retrain cycle (*One
    Shot*). In that vein, retraining itself enjoys a particularly negative reputation
    and a variety of pruning approaches try to avoid it entirely. These include novel
    weight-selection criteria for pruning without the need for retraining (Frantar
    & Alistarh, [2023](#bib.bib12); Sun et al., [2023](#bib.bib55)), and *prune during
    training* strategies (Liu et al., [2020](#bib.bib41); Ding et al., [2019](#bib.bib7);
    Lin et al., [2020](#bib.bib39); Wortsman et al., [2019](#bib.bib63)), which aim
    to achieve sparsity at the end of the regular training process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 IMP 很受欢迎，但它在计算上非常昂贵，可能需要执行许多修剪-重新训练周期和重新训练轮次才能获得适用于当前任务的高性能且足够压缩的模型。特别是考虑到*迁移学习*的流行，其中巨大的预训练模型被重用并微调到特定任务，像
    IMP 这样的过程对处理资源受限环境的从业者来说可能是难以承受的（Frantar & Alistarh，[2023](#bib.bib12)），即便只是执行一个单一的修剪-重新训练周期（*One
    Shot*）。在这种情况下，重新训练本身享有特别负面的声誉，各种修剪方法试图完全避免它。这些方法包括不需要重新训练的修剪新权重选择标准（Frantar &
    Alistarh，[2023](#bib.bib12)；Sun 等人，[2023](#bib.bib55)），以及*训练期间修剪*策略（Liu 等人，[2020](#bib.bib41)；Ding
    等人，[2019](#bib.bib7)；Lin 等人，[2020](#bib.bib39)；Wortsman 等人，[2019](#bib.bib63)），这些策略旨在在常规训练过程结束时实现稀疏性。
- en: Several works have tried to address the issue from the angle of making retraining
    itself less undesirable. Zimmer et al. ([2023](#bib.bib76)) accelerate retraining
    using a pruning-adaptive learning rate schedule, effectively reducing the number
    of iterations required while improving generalization performance. To find *lottery
    tickets* (Frankle & Carbin, [2018](#bib.bib10)) more efficiently, You et al. ([2020](#bib.bib66))
    and Wolfe et al. ([2021](#bib.bib62)) try to find the pruning mask earlier in
    training, Jaiswal et al. ([2023b](#bib.bib25)) speed up the mask-generation process
    by superimposing a set of masks throughout retraining, and Zhang et al. ([2021](#bib.bib73))
    reduce the number of retraining iterations by using only a critical subset of
    the data. Zimmer et al. ([2024](#bib.bib77)) show that constructing *sparse model
    soups* during each phase of IMP can enhance its performance and consequently reduce
    the overall wall-time required for retraining.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究试图从减少重新训练本身的负担角度来解决这一问题。Zimmer 等人（[2023](#bib.bib76)）通过使用修剪自适应学习率调度加速重新训练，有效减少了所需的迭代次数，同时提高了泛化性能。为了更高效地找到*彩票票据*（Frankle
    & Carbin，[2018](#bib.bib10)），You 等人（[2020](#bib.bib66)）和 Wolfe 等人（[2021](#bib.bib62)）尝试在训练早期找到修剪掩码，Jaiswal
    等人（[2023b](#bib.bib25)）通过在整个重新训练过程中叠加一组掩码来加快掩码生成过程，而 Zhang 等人（[2021](#bib.bib73)）则通过仅使用数据的关键子集来减少重新训练迭代次数。Zimmer
    等人（[2024](#bib.bib77)）表明，在 IMP 的每个阶段构建*sparse model soups* 可以增强其性能，从而减少重新训练所需的整体时间。
- en: '![Refer to caption](img/7b7110a5ea3d15cb06f95970c948105a.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7b7110a5ea3d15cb06f95970c948105a.png)'
- en: 'Figure 1: Features produced by a single filter from the first convolutional
    layer of *AlexNet* (Krizhevsky et al., [2012](#bib.bib28)). From left to right:
    original image, output from a pretrained model, and output from the magnitude-pruned
    version of the same model.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：*AlexNet*（Krizhevsky 等人，[2012](#bib.bib28)）第一卷积层中单个滤波器生成的特征。从左到右：原始图像、预训练模型的输出，以及同一模型的幅值修剪版本的输出。
- en: In this work, we propose viewing the problem from yet another, previously unexplored
    angle, namely that of *parameter-efficiency*. To the best of our knowledge, all
    classical methods define retraining after pruning as a retraining of *all* parameters
    at hand, requiring computation and storage of full gradients at each step. This
    is particularly challenging with optimizers like Adam (Kingma & Ba, [2014](#bib.bib27)),
    which need storage for parameters, gradients, and both first and second-order
    moments. As a result, retraining all parameters emerges as a challenge both in
    terms of computational efficiency and storage demands, especially in the context
    of Large Language Models (LLMs) (Frantar & Alistarh, [2023](#bib.bib12); Sun et al.,
    [2023](#bib.bib55)). Yet, retraining often requires much fewer iterations than
    training from scratch (Zimmer et al., [2023](#bib.bib76)), suggesting that pruned
    models retain considerable feature information despite diminished performance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了从另一种之前未探讨的角度看待问题，即*参数效率*。根据我们所知，所有经典方法都将剪枝后的重新训练定义为对手头所有参数的重新训练，需要在每一步计算和存储全梯度。这对于像Adam（Kingma
    & Ba，[2014](#bib.bib27)）这样的优化器尤其具有挑战性，因为它们需要存储参数、梯度以及一阶和二阶矩。因此，重新训练所有参数在计算效率和存储需求方面都是一个挑战，特别是在大语言模型（LLMs）（Frantar
    & Alistarh，[2023](#bib.bib12)；Sun 等，[2023](#bib.bib55)）的背景下。然而，重新训练通常需要的迭代次数远少于从头训练（Zimmer
    等，[2023](#bib.bib76)），这表明剪枝后的模型尽管性能下降，但仍保留了相当多的特征信息。
- en: Inspired by the recent advancements in Parameter-Efficient Fine-Tuning (PEFT)
    (Lialin et al., [2023a](#bib.bib37)) that enable large-scale model fine-tuning
    on standard hardware (Lialin et al., [2023b](#bib.bib38)), we challenge the common
    practice of retraining all parameters after pruning. We view pruning as a process
    of feature distortion and emphasize the similarity between the transfer learning
    setting and the prune-retrain paradigm. Our findings indicate that retrained models
    can remain closely aligned with their pruned versions, suggesting significant
    feature preservation, despite initial pruning-induced performance drops to near-random
    levels. Surprisingly, by retraining as little as 0.27%-0.35% of the parameters
    of the Generative Pretrained Transformer (GPT) architectures OPT-2.7B/6.7B/13B/30B/66B
    (Zhang et al., [2022](#bib.bib71)), LLaMA-2-7B/13B/70B (Touvron et al., [2023](#bib.bib57)),
    Mistral-7B (Jiang et al., [2023](#bib.bib26)) as well as Mixtral-8x7B (Mistral,
    [2023](#bib.bib46)), we achieve nearly all of IMP’s performance in the One Shot
    setting with moderate to high sparsity levels, where magnitude pruning without
    retraining collapses entirely. By drastically reducing the memory requirements
    for retraining, we are able to prune and retrain up to 30 billion parameter GPT
    s on a *single* NVIDIA A100 GPU. Similarly, retraining 0.004%-0.21% of the parameters
    of a ResNet-50 on ImageNet is for many sparsity levels sufficient to recover the
    accuracy after pruning. Our investigation of state-of-the-art PEFT approaches
    for retraining after pruning opens a promising alternative to avoiding retraining
    entirely, which we refer to as Parameter-Efficient Retraining after Pruning (PERP).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 受到最近在参数高效微调（PEFT）（Lialin 等， [2023a](#bib.bib37)）方面的进展启发，这些进展使得在标准硬件上对大规模模型进行微调成为可能（Lialin
    等， [2023b](#bib.bib38)），我们挑战了在剪枝后重新训练所有参数的常见做法。我们将剪枝视为一种特征失真的过程，并强调转移学习设置与剪枝-重新训练范式之间的相似性。我们的发现表明，重新训练的模型可以与其剪枝版本保持紧密对齐，尽管最初的剪枝引起的性能下降到接近随机水平，但这表明特征保持显著。令人惊讶的是，通过重新训练仅**0.27%-0.35%**的Generative
    Pretrained Transformer（GPT）架构OPT-2.7B/6.7B/13B/30B/66B（Zhang 等，[2022](#bib.bib71)），LLaMA-2-7B/13B/70B（Touvron
    等，[2023](#bib.bib57)），Mistral-7B（Jiang 等，[2023](#bib.bib26)）以及Mixtral-8x7B（Mistral，[2023](#bib.bib46)），我们在具有中等到高稀疏性水平的One
    Shot设置中几乎达到了IMP的所有性能，而在没有重新训练的情况下，幅度剪枝完全崩溃。通过大幅减少重新训练的内存需求，我们能够在**单**张NVIDIA A100
    GPU上对高达30亿参数的GPT进行剪枝和重新训练。同样，对于许多稀疏性水平，重新训练ResNet-50上0.004%-0.21%的参数足以在剪枝后恢复准确性。我们对最先进的PEFT方法进行的剪枝后重新训练的调查开启了一种有前景的替代方案，以避免完全重新训练，我们将其称为剪枝后参数高效重新训练（PERP）。
- en: 'To sum up, our main contributions are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，我们的主要贡献是：
- en: '1.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Restoring feature quality with few parameters. We challenge the practice of
    retraining all parameters after pruning, demonstrating that retraining a small
    subset of highly expressive parameters can effectively restore performance after
    One Shot pruning, with backpropagation of less than 1% of the total parameters
    often sufficing for full recovery. Motivated by the investigation of state-of-the-art
    Parameter-Efficient Fine-Tuning (PEFT) in the prune-retrain context, we propose
    Parameter-Efficient Retraining after Pruning (PERP), using a fraction of the parameters
    and retraining with drastically reduced compute and memory requirements. We extend
    our findings to the setting of multiple prune-retrain cycles, where we match IMP’s
    performance with less aggregated memory and compute demands.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用少量参数恢复特征质量。我们挑战了在剪枝后再训练所有参数的做法，展示了再训练一小部分高表达能力的参数可以有效地恢复One Shot剪枝后的性能，通常少于1%的总参数进行反向传播就足以实现完全恢复。在剪枝再训练的背景下，我们受最先进的参数高效微调（PEFT）研究的启发，提出了剪枝后的参数高效再训练（PERP），利用部分参数进行再训练，显著降低计算和内存需求。我们将研究扩展到多个剪枝再训练周期的设置，在这个设置中，我们在更少的聚合内存和计算需求下匹配了IMP的性能。
- en: '2.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Making retraining of large models feasible. We validate our approach through
    comprehensive experiments across Natural Language Processing (NLP) and Image Classification.
    Notably, we backpropagate as little as 0.27%-0.35% of parameters of OPT-GPT s,
    LLaMA-2 and Mistral models, utilizing a *single* NVIDIA A100 to retrain up to
    30 billion parameter models within minutes. Further, we recover most of the accuracy
    of full retraining by utilizing 0.004%-0.21% of ResNet-50 parameters on ImageNet
    across various sparsity levels.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使大型模型的再训练变得可行。我们通过在自然语言处理（NLP）和图像分类方面的全面实验验证了我们的方法。特别地，我们仅对OPT-GPT s、LLaMA-2和Mistral模型的0.27%-0.35%的参数进行反向传播，利用一台*单*
    NVIDIA A100在几分钟内对最多30亿参数的模型进行再训练。此外，我们通过在ImageNet上利用0.004%-0.21%的ResNet-50参数，在各种稀疏性水平下恢复了大部分完整再训练的准确性。
- en: '3.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Reconsidering Magnitude Pruning of LLMs. Despite being recognized as unsuited
    for LLMs due to exploding perplexity at moderate sparsity levels, we demonstrate
    that PERP reduces the perplexity of magnitude pruning by several orders of magnitude
    with minimal iterations on less than 1% of the parameters, and further also improves
    state-of-the-art retraining-free methods like SparseGPT (Frantar & Alistarh, [2023](#bib.bib12))
    and Wanda (Sun et al., [2023](#bib.bib55)). Our results reveal that magnitude
    pruning coupled with PERP remains a viable and competitive option in the unstructured
    as well as semi-structured 2:4 and 4:8 sparsity settings.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重新考虑LLMs的幅度剪枝。尽管由于在中等稀疏性水平下困惑度爆炸而被认为不适合LLMs，但我们展示了PERP在少于1%的参数上经过少量迭代显著降低了幅度剪枝的困惑度，并进一步改进了最先进的无再训练方法，如SparseGPT（Frantar
    & Alistarh, [2023](#bib.bib12)）和Wanda（Sun et al., [2023](#bib.bib55)）。我们的结果揭示了幅度剪枝与PERP结合在无结构以及半结构2:4和4:8稀疏设置下仍然是一个可行且具有竞争力的选项。
- en: 2 Methodology and Experimental Setup
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论和实验设置
- en: 2.1 Preliminaries
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 基础知识
- en: We begin with a quick overview of pruning and transfer learning, which are central
    to our study.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先快速概述剪枝和迁移学习，这些是我们研究的核心。
- en: 'Pruning. We prune Neural Networks in a post-hoc fashion, removing individual
    weights as is done by the previously introduced IMP approach. IMP adopts the *prune
    after training* paradigm, consisting of three-stages: i) pretraining to convergence,
    ii) permanently pruning the smallest magnitude weights, and iii) retraining to
    recover the predictive performance eradicated by pruning. These last two stages,
    often termed a prune-retrain cycle or phase, are either performed once (*One Shot*)
    or repeated until a desired level sparsity is met. Despite its straightforward
    nature, IMP and its variants have been shown to produce sparse models comparable
    in performance to those from more complex algorithms (Gale et al., [2019](#bib.bib13);
    Zimmer et al., [2023](#bib.bib76)). In this work, we focus on IMP’s potential
    to produce high-quality sparse models rather than lottery tickets.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪。我们以事后方式修剪神经网络，去除单个权重，如之前介绍的IMP方法所做的那样。IMP采用*训练后修剪*的范式，包括三个阶段：i）预训练至收敛，ii）永久性地修剪最小幅度的权重，以及iii）重训练以恢复被修剪所消除的预测性能。这最后两个阶段，通常称为修剪-重训练周期或阶段，要么执行一次（*One
    Shot*），要么重复直到达到期望的稀疏水平。尽管其性质简单，IMP及其变体已被证明能够产生性能与更复杂算法相当的稀疏模型（Gale et al., [2019](#bib.bib13)；Zimmer
    et al., [2023](#bib.bib76)）。在这项工作中，我们关注IMP产生高质量稀疏模型的潜力，而非彩票票据。
- en: 'Pruning a non-trivial portion of the parameters typically results in significant
    performance degradation. In consequence, the retraining step is fundamental in
    each phase, mainly for two reasons: First of all, it enables recovery from pruning-induced
    performance drops, typically in much fewer iterations than what standard training
    would require to achieve a comparable reduction in train loss (Zimmer et al.,
    [2023](#bib.bib76)). Furthermore, it prepares the network for subsequent prune-retrain
    cycles, mitigating *layer-collapse*; a phenomenon where excessive pruning in a
    single phase entirely eliminates a layer, rendering the model dysfunctional (Tanaka
    et al., [2020](#bib.bib56)). Without retraining between pruning steps, the final
    IMP result would be equal to One Shot IMP.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪大量参数通常会导致显著的性能下降。因此，重训练步骤在每个阶段都是基础性的，主要有两个原因：首先，它能够恢复由于修剪引起的性能下降，通常只需比标准训练少得多的迭代次数就能达到相当的训练损失减少（Zimmer
    et al., [2023](#bib.bib76)）。此外，它为随后的修剪-重训练周期做准备，缓解*层崩溃*现象；即在一个阶段中过度修剪导致完全删除某一层，使模型无法正常工作（Tanaka
    et al., [2020](#bib.bib56)）。如果在修剪步骤之间不进行重训练，最终的IMP结果将等同于一次性IMP。
- en: 'While the magnitude criterion is widely used, it is by far not the only one,
    as detailed in studies like LeCun et al. ([1989](#bib.bib33)); Hassibi & Stork
    ([1993](#bib.bib17)); Molchanov et al. ([2016](#bib.bib48)); Yeom et al. ([2019](#bib.bib64)).
    For a comprehensive review, we refer to Hoefler et al. ([2021](#bib.bib20)). This
    study primarily focuses on magnitude pruning, but in [Section 3.3](#S3.SS3 "3.3
    Reconsidering Magnitude Pruning of LLMs ‣ 3 Parameter-Efficient Retraining ‣ PERP:
    Rethinking the Prune-Retrain Paradigm in the Era of LLMs"), we also discuss recent
    pruning strategies designed for LLMs to avoid retraining entirely.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管幅度标准被广泛使用，但它绝不是唯一的标准，如LeCun et al. ([1989](#bib.bib33))；Hassibi & Stork ([1993](#bib.bib17))；Molchanov
    et al. ([2016](#bib.bib48))；Yeom et al. ([2019](#bib.bib64))的研究所详细阐述。有关全面回顾，请参阅Hoefler
    et al. ([2021](#bib.bib20))。该研究主要集中在幅度修剪，但在[第3.3节](#S3.SS3 "3.3 Reconsidering
    Magnitude Pruning of LLMs ‣ 3 Parameter-Efficient Retraining ‣ PERP: Rethinking
    the Prune-Retrain Paradigm in the Era of LLMs")，我们还讨论了为避免完全重训练而设计的最新修剪策略。'
- en: Transfer learning. As models grow in size, Fine-Tuning (FT) —the process of
    adapting a pretrained or *foundation* model to a novel task—has become the norm,
    avoiding the inefficiencies of training from scratch for each new task (Houlsby
    et al., [2019](#bib.bib21); Kumar et al., [2022b](#bib.bib30)). FT capitalizes
    on the transfer of existing knowledge to a closely related domain (*transfer learning*).
    Yet, the immense size and complexity of foundation models can make the traditional
    FT approach more challenging, requiring storage for the entire model, its gradients,
    and auxiliary buffers, even for brief training. In response, various Parameter-Efficient
    Fine-Tuning (PEFT) methods have emerged. They significantly reduce the number
    of trainable parameters, cutting down on compute and storage needs, while preserving
    performance levels comparable to conventional FT.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习。随着模型规模的增长，微调（FT）——将预训练或*基础*模型适应于新任务的过程——已经成为常态，避免了对每个新任务从头开始训练的低效（Houlsby
    et al., [2019](#bib.bib21); Kumar et al., [2022b](#bib.bib30)）。FT 利用现有知识向相关领域的转移（*迁移学习*）。然而，基础模型的巨大规模和复杂性使得传统的
    FT 方法变得更具挑战性，即使是短期训练，也需要存储整个模型、其梯度和辅助缓冲区。因此，各种参数高效微调（PEFT）方法应运而生。它们显著减少了可训练参数的数量，从而降低了计算和存储需求，同时保持与传统
    FT 相当的性能水平。
- en: PEFT methods are broadly categorized as selective, additive, or reparametrization-based
    (Lialin et al., [2023a](#bib.bib37)). *Selective methods* update specific model
    components, such as the top linear layer (Kumar et al., [2022a](#bib.bib29); Evci
    et al., [2022](#bib.bib9)), only the biases (Zaken et al., [2021](#bib.bib67)),
    or by partitioning specific tensors into active and inactive portions (Vucetic
    et al., [2022](#bib.bib58)). *Additive methods*, like *adapters* (Houlsby et al.,
    [2019](#bib.bib21); He et al., [2022](#bib.bib19)), add new parameters which are
    trained for specific tasks while the main model remains unchanged. *Reparametrization-based
    methods* exploit the low intrinsic dimensionality of fine-tuning (Aghajanyan et al.,
    [2020](#bib.bib1)). A well-known example is Low-Rank Adaptation (LoRA) (Hu et al.,
    [2021](#bib.bib22)), which implicitly enforces low-rank constraints on additive
    updates to pretrained parameter matrices, substantially decreasing the number
    of trainable parameters.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT 方法通常分为选择性、加法或重参数化三类（Lialin et al., [2023a](#bib.bib37)）。*选择性方法* 更新特定的模型组件，如顶层线性层（Kumar
    et al., [2022a](#bib.bib29); Evci et al., [2022](#bib.bib9)），仅更新偏置（Zaken et al.,
    [2021](#bib.bib67)），或通过将特定张量分为活跃和非活跃部分（Vucetic et al., [2022](#bib.bib58)）。*加法方法*，如*适配器*（Houlsby
    et al., [2019](#bib.bib21); He et al., [2022](#bib.bib19)），添加新的参数，这些参数针对特定任务进行训练，而主模型保持不变。*重参数化方法*
    利用微调的低内在维度（Aghajanyan et al., [2020](#bib.bib1)）。一个著名的例子是低秩适应（LoRA）（Hu et al.,
    [2021](#bib.bib22)），它隐式地对预训练参数矩阵的加法更新施加低秩约束，从而显著减少可训练参数的数量。
- en: Precisely, LoRA freezes the pretrained parameters and reparametrizes each weight
    matrix $W_{0}\in\mathbb{R}^{n\times m}$ remains fixed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 精确地说，LoRA 冻结了预训练参数，并对每个权重矩阵 $W_{0}\in\mathbb{R}^{n\times m}$ 进行重参数化，保持不变。
- en: Other related literature. Kwon et al. ([2022](#bib.bib31)) propose a structured
    pruning framework for transformers, explicitly avoiding retraining for efficiency.
    Zhang et al. ([2023b](#bib.bib72)) develop a training-free pruning method inspired
    by prune-and-grow strategies from *Dynamic Sparse Training* (Evci et al., [2020](#bib.bib8)).
    Ding et al. ([2019](#bib.bib7)) and Liu et al. ([2020](#bib.bib41)) propose pruning
    methods that circumvent the perceived high costs of retraining. Several works
    propose techniques in the domain of sparse fine-tuning in transfer learning. Zhang
    et al. ([2023a](#bib.bib70)) address the problem of performing gradient-based
    pruning by utilizing the LoRA gradients. Liu et al. ([2021](#bib.bib40)) aim at
    pruning pretrained models for improvements when fine-tuning to downstream tasks.
    Li et al. ([2022](#bib.bib36)) reduce the number of parameters for weight importance
    computation in sparse fine-tuning. While conventional retraining typically involves
    retraining all parameters, some may have implicitly employed PEFT in pruning LLMs,
    e.g., Sun et al. ([2023](#bib.bib55)) further fine-tune their sparse model using
    LoRA. To the best of our knowledge, our work is the first to extensively explore
    PEFT in the context of retraining after pruning.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其他相关文献。Kwon等人（[2022](#bib.bib31)）提出了一种针对变换器的结构化剪枝框架，明确避免了重训练以提高效率。Zhang等人（[2023b](#bib.bib72)）开发了一种无训练的剪枝方法，灵感来自于*动态稀疏训练*（Evci等人，[2020](#bib.bib8)）的剪枝与生长策略。Ding等人（[2019](#bib.bib7)）和Liu等人（[2020](#bib.bib41)）提出了绕过重训练高成本的剪枝方法。若干工作在迁移学习的稀疏微调领域提出了技术。Zhang等人（[2023a](#bib.bib70)）通过利用LoRA梯度解决了基于梯度的剪枝问题。Liu等人（[2021](#bib.bib40)）旨在对预训练模型进行剪枝，以在微调到下游任务时进行改进。Li等人（[2022](#bib.bib36)）减少了稀疏微调中权重重要性计算的参数数量。虽然传统重训练通常涉及重训练所有参数，但一些研究可能在剪枝LLMs时隐式地采用了PEFT，例如Sun等人（[2023](#bib.bib55)）进一步使用LoRA对其稀疏模型进行微调。据我们所知，我们的工作是第一个在剪枝后重训练的背景下广泛探索PEFT的研究。
- en: 2.2 Parameter-Efficient Retraining After Pruning
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 剪枝后的参数高效重训练
- en: Pruning can degrade the model’s performance to near-random levels. Yet, retraining
    often restores performance in much fewer iterations than similar loss reductions
    during pretraining (Zimmer et al., [2023](#bib.bib76)). This optimization often
    involves merely a few iterations, even when dealing with substantial pruning-induced
    performance degradation. Consequently, even if the pruned network is severely
    damaged, it likely retains most of the task-informative features. We hypothesize
    that, similar to fine-tuning in transfer learning, retraining can be significantly
    more efficient by leveraging these features rather than adjusting the entire network,
    despite pruning severely damaging the model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝可能会将模型的性能降至接近随机水平。然而，重训练通常能在比预训练期间类似的损失减少少得多的迭代中恢复性能（Zimmer等人，[2023](#bib.bib76)）。这种优化通常只涉及少量迭代，即使在处理显著剪枝导致的性能下降时也是如此。因此，即使剪枝后的网络受到严重损坏，它仍可能保留大部分任务相关特征。我们假设，与迁移学习中的微调类似，重训练可以通过利用这些特征而不是调整整个网络来显著提高效率，尽管剪枝严重损害了模型。
- en: More specifically, we observe that the transfer learning paradigm—shifting from
    source to target domain and subsequent fine-tuning—bears a resemblance to the
    prune-retrain paradigm. In transfer learning, the optimization objective changes
    with a new task, requiring fine-tuning. Pruning, which permanently sets parameters
    to zero, limits the optimization to a linear subspace and increases the model’s
    error, despite identical source and target space. However, an alternative view
    on pruning is as a disturbance to the features the model has learned. This disruption
    means the model needs to be retrained to align with the original domain’s features.
    In essence, retraining after pruning is about refining imperfect, yet valuable
    features.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们观察到迁移学习范式——从源领域到目标领域的转移以及随后的微调——与剪枝-重训练范式有相似之处。在迁移学习中，优化目标随着新任务的出现而发生变化，因此需要微调。剪枝将参数永久设置为零，这限制了优化到一个线性子空间，并增加了模型的错误，尽管源空间和目标空间是相同的。然而，对剪枝的另一种看法是它对模型已学特征的干扰。这种干扰意味着模型需要重新训练以与原始领域的特征对齐。本质上，剪枝后的重训练是关于改进不完美但有价值的特征。
- en: '[Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs") illustrates this intuition by depicting a dog (left)
    and the features produced by a single filter from the first convolutional layer
    of a pretrained network (middle) and its pruned version (right). The middle image
    demonstrates the pretrained network’s capability to capture distinct boundary
    features, especially the dog’s defining back and ears. Conversely, the pruned
    network still emphasizes the dog’s back, albeit with reduced intensity and in
    favor of its overall form, likely influenced by the stark contrast between the
    white dog and the green grass. While pruning diminishes the feature quality, it
    does not completely eradicate it.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1](#S1.F1 "图 1 ‣ 1 介绍 ‣ PERP：在大规模语言模型时代重新思考修剪-再训练范式") 通过展示一只狗（左侧）和从预训练网络的第一个卷积层中提取的单个滤波器生成的特征（中间）以及其修剪版本（右侧）来阐明这一直觉。中间的图像展示了预训练网络捕捉不同边界特征的能力，尤其是狗的显著背部和耳朵。相对而言，修剪后的网络仍然强调狗的背部，尽管强度有所降低，更注重其整体形态，这可能受到白色狗与绿色草地之间强烈对比的影响。虽然修剪会减少特征质量，但并不会完全消除它。'
- en: What gains can we expect from parameter-efficiency? Parameter-Efficient Retraining
    aims to substantially reduce the computational load and memory demands of backpropagation
    by retraining fewer parameters, i.e., freezing the majority of parameters to not
    require gradients. While computational speedups are not always guaranteed, as
    techniques like adapters or LoRA might increase computational requirements, we
    especially expect selective methods to boost performance. However, a major benefit
    also lies in the significant reduction in memory requirements. This reduction
    is crucial for retraining large models efficiently, exemplified by our ability
    to retrain the 30 billion parameter model OPT-30B on a single NVIDIA A100-80GB
    within minutes. Typically, optimizers such as AdamW (Kingma & Ba, [2014](#bib.bib27);
    Loshchilov & Hutter, [2019](#bib.bib42)) require multiple buffers for each parameter,
    including the parameter itself, its gradient, and both first and second-order
    moments. Involving fewer parameters results in considerably less allocated memory.
    Additionally, the memory required for storing activations during backpropagation
    can be significantly reduced.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以期待从参数效率中获得哪些收益？参数效率再训练旨在通过仅再训练少量参数来显著减少反向传播的计算负担和内存需求，即冻结大部分参数以避免计算梯度。虽然计算速度的提升并不总是有保证的，因为像适配器或
    LoRA 这样的技术可能会增加计算需求，但我们特别期望选择性方法能够提升性能。然而，一个主要的好处也在于内存需求的显著减少。这种减少对于高效再训练大型模型至关重要，正如我们能够在几分钟内使用单个
    NVIDIA A100-80GB 重新训练 300 亿参数模型 OPT-30B 所示。通常，像 AdamW（Kingma & Ba, [2014](#bib.bib27);
    Loshchilov & Hutter, [2019](#bib.bib42)）这样的优化器需要为每个参数分配多个缓冲区，包括参数本身、其梯度以及一阶和二阶矩。涉及更少的参数会显著减少分配的内存。此外，反向传播期间存储激活所需的内存也可以显著减少。
- en: 2.3 Experimental setup
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 实验设置
- en: We outline our general experimental approach, detailing datasets, architectures,
    and metrics. To enable reproducibility, our code is available at [github.com/ZIB-IOL/PERP](https://github.com/ZIB-IOL/PERP).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们概述了我们的总体实验方法，详细介绍了数据集、架构和指标。为了实现可重复性，我们的代码可在 [github.com/ZIB-IOL/PERP](https://github.com/ZIB-IOL/PERP)
    上获取。
- en: 'Our study primarily investigates language modeling in NLP as well as image
    classification. For NLP, we use pretrained GPT models available through HuggingFace
    (Wolf et al., [2020](#bib.bib61)), namely *OPT-1.3B/6.7B/13B/30B/66B* (Zhang et al.,
    [2022](#bib.bib71)), *LLaMA-2-7B/13B/70B* (Touvron et al., [2023](#bib.bib57)),
    *Mistral-7B* (Jiang et al., [2023](#bib.bib26)) as well as *Mixtral-8x7B* (Mistral,
    [2023](#bib.bib46)). Retraining is done on the *C4* dataset (Raffel et al., [2020](#bib.bib51))
    with context-length sequence sizes using AdamW (Loshchilov & Hutter, [2019](#bib.bib42))
    with a linear schedule and a 10% warmup period. For validation, we randomly sample
    100 sequences from the validation split. The models are evaluated using the perplexity
    metric on the *WikiText* dataset (Merity et al., [2016](#bib.bib43)). In addition,
    following Sun et al. ([2023](#bib.bib55)), we provide results on several tasks
    from the EleutherAI evaluation set (Gao et al., [2023](#bib.bib14)) in the appendix.
    For image classification, we focus on *ImageNet* (Russakovsky et al., [2015](#bib.bib53)),
    utilizing *ResNet* architectures (He et al., [2015](#bib.bib18)) and measuring
    performance with top-1 test accuracy. We follow standard practices by retraining
    networks with momentum SGD, allocating 10% of the training data for validation,
    and using the ALLR learning rate schedule (Zimmer et al., [2023](#bib.bib76))
    for retraining (see [Appendix A](#A1 "Appendix A Technical details and training
    settings ‣ PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs")).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的研究主要探讨了 NLP 中的语言建模以及图像分类。对于 NLP，我们使用通过 HuggingFace 提供的预训练 GPT 模型 (Wolf 等人，[2020](#bib.bib61))，即
    *OPT-1.3B/6.7B/13B/30B/66B* (Zhang 等人，[2022](#bib.bib71))、*LLaMA-2-7B/13B/70B*
    (Touvron 等人，[2023](#bib.bib57))、*Mistral-7B* (Jiang 等人，[2023](#bib.bib26)) 以及
    *Mixtral-8x7B* (Mistral，[2023](#bib.bib46))。再训练在 *C4* 数据集 (Raffel 等人，[2020](#bib.bib51))
    上进行，使用 AdamW (Loshchilov & Hutter，[2019](#bib.bib42)) 线性调度和 10% 预热期。为了验证，我们从验证集随机抽取了
    100 个序列。使用 *WikiText* 数据集 (Merity 等人，[2016](#bib.bib43)) 上的困惑度指标评估模型。此外，按照 Sun
    等人 ([2023](#bib.bib55)) 的方法，我们在附录中提供了 EleutherAI 评估集 (Gao 等人，[2023](#bib.bib14))
    上的若干任务结果。对于图像分类，我们专注于 *ImageNet* (Russakovsky 等人，[2015](#bib.bib53))，使用 *ResNet*
    架构 (He 等人，[2015](#bib.bib18)) 并通过 top-1 测试准确率来测量性能。我们遵循标准做法，通过动量 SGD 进行再训练，将 10%
    的训练数据用于验证，并使用 ALLR 学习率调度 (Zimmer 等人，[2023](#bib.bib76)) 进行再训练 (见 [附录 A](#A1 "附录
    A 技术细节和训练设置 ‣ PERP: 在 LLM 时代重新思考修剪-再训练范式"))。'
- en: '![Refer to caption](img/0be143351f4264e5d84320cac808fd27.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/0be143351f4264e5d84320cac808fd27.png)'
- en: 'Figure 2: ResNet-50 on ImageNet: Test accuracy across sparsity levels for One
    Shot pruning with one retraining epoch.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: ImageNet 上的 ResNet-50：通过一次再训练轮次的 One Shot 修剪在不同稀疏性水平上的测试准确率。'
- en: For NLP, we follow Sun et al. ([2023](#bib.bib55)) and prune all linear layers
    except the embedding and final classification head, assigning uniform sparsity
    to all layers. We provide experiments for unstructured and the semi-structured
    2:4 and 4:8 sparsities (Mishra et al., [2021](#bib.bib45)). For vision, we follow
    Zimmer et al. ([2023](#bib.bib76)) and globally prune everything except biases
    and Batch-Normalization (BN) parameters.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 NLP，我们遵循 Sun 等人 ([2023](#bib.bib55)) 的方法，修剪除嵌入层和最终分类头之外的所有线性层，并为所有层分配统一的稀疏性。我们提供了对非结构化和半结构化
    2:4 及 4:8 稀疏性的实验（Mishra 等人，[2021](#bib.bib45)）。对于视觉任务，我们遵循 Zimmer 等人 ([2023](#bib.bib76))
    的方法，全球修剪除偏置和 Batch-Normalization (BN) 参数之外的所有内容。
- en: 3 Parameter-Efficient Retraining
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 参数高效再训练
- en: 3.1 Restoring feature quality with few parameters
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 用少量参数恢复特征质量
- en: Pruning can be seen as distorting the initially acquired features, diminishing
    the network’s expressivity by settling on suboptimal features. With most parameters
    set to be immutable, our goal is to regain performance (maximizing accuracy or
    minimizing perplexity) with minimal number of trainable parameters. To that end,
    we examine subgroups of parameters with varying complexity, which we hypothesize
    to hold significant expressive power during retraining.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪可以看作是扭曲最初获得的特征，通过选择次优特征来降低网络的表达能力。由于大多数参数被设置为不可变，我们的目标是以最小的可训练参数数量恢复性能（最大化准确率或最小化困惑度）。为此，我们检查了具有不同复杂性的参数子组，我们假设这些子组在再训练过程中具有显著的表达能力。
- en: Before introducing the methods we aim to investigate, we note that a significant
    role in model expressivity is played by normalization layers such as Batch-Normalization
    (BN) (Ioffe & Szegedy, [2015](#bib.bib23)) and Layer-Normalization (LN) (Ba et al.,
    [2016](#bib.bib2)). Specifically, BN layers standardize the preceding layer’s
    output and act differently during training and inference. During training, BN
    calculates the batch mean and variance in an on-the-fly manner. During inference,
    BN uses running averages of mean and variance from the training phase, adjusting
    the model to the data distribution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍我们希望研究的方法之前，我们注意到规范化层（如批量归一化 (BN) (Ioffe & Szegedy, [2015](#bib.bib23)) 和层归一化
    (LN) (Ba 等人，[2016](#bib.bib2))）在模型表达性中扮演了重要角色。具体来说，BN 层对前一层的输出进行标准化，并在训练和推理期间表现不同。在训练期间，BN
    以动态方式计算批量均值和方差。在推理期间，BN 使用训练阶段的均值和方差的运行平均值，将模型调整到数据分布上。
- en: 'We begin by investigating the following approaches, which we design to build
    upon one another, as we will clarify:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始研究以下方法，这些方法设计成彼此依赖的，我们将进一步阐明：
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BN-Recalibrate: Li et al. ([2020](#bib.bib35)) identified that recalibrating
    the BN statistics after pruning enhances generalization. This approach entails
    a one-time evaluation on the training dataset, neither requiring backpropagation
    nor altering the training set performance.'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'BN-Recalibrate: Li 等人 ([2020](#bib.bib35)) 确定了在剪枝后重新校准 BN 统计数据可以提高泛化能力。这种方法需要在训练数据集上进行一次评估，无需反向传播，也不会改变训练集的性能。'
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Biases: We only retrain the network’s biases. Despite corresponding to only
    a small fraction of the total parameters, biases are crucial for model expressivity;
    Zaken et al. ([2021](#bib.bib67)) specifically propose a FT method that adjusts
    only these biases to the new task.'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 偏置：我们仅重新训练网络的偏置。尽管它们仅占总参数的一小部分，但偏置对于模型表达性至关重要；Zaken 等人 ([2021](#bib.bib67))
    特别提出了一种 FT 方法，仅调整这些偏置以适应新任务。
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BN-Parameters: Beyond statistics, BN layers also include trainable scaling
    and bias parameters. Their importance has been highlighted in transfer learning
    (Mudrakarta et al., [2018](#bib.bib49); Giannou et al., [2023](#bib.bib15)) and
    Frankle et al. ([2020](#bib.bib11)) demonstrated that training only these parameters
    can enable otherwise frozen, randomly-initialized networks to achieve significant
    accuracy.'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BN-参数：除了统计数据，BN 层还包括可训练的缩放和偏置参数。这些参数在迁移学习中已经被强调 (Mudrakarta 等人，[2018](#bib.bib49)；Giannou
    等人，[2023](#bib.bib15))，Frankle 等人 ([2020](#bib.bib11)) 证明了仅训练这些参数可以使其他冻结的随机初始化网络达到显著的准确性。
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Linear Probing: A commonly used PEFT approach is *Linear Probing*, where all
    parameters remain fixed except for the final linear layer (also called head or
    classifier) to align the existing features to the new task.'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性探测：一种常用的 PEFT 方法是*线性探测*，在这种方法中，除了最终的线性层（也称为头部或分类器）外，所有参数保持固定，以将现有特征与新任务对齐。
- en: We further define each method to build upon the previous one with increasing
    complexity, i.e., *Linear Probing* is intended to additionally unfreeze all parameters
    of preceding methods. To be more precise, let $\mathcal{W}(\mathcal{M})$, then
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步定义每种方法在前一种方法的基础上逐步增加复杂性，即 *线性探测* 旨在进一步解冻前述方法的所有参数。更准确地说，设 $\mathcal{W}(\mathcal{M})$，然后
- en: '|  | $\displaystyle\mathcal{W}(\textit{BN-Recalibrate})$ |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{W}(\textit{BN-Recalibrate})$ |  |'
- en: '|  |  | $\displaystyle\subsetneq\mathcal{W}(\textit{Linear Probing})\subsetneq\mathcal{W}(\textit{IMP}).$
    |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\subsetneq\mathcal{W}(\textit{线性探测})\subsetneq\mathcal{W}(\textit{IMP}).$
    |  |'
- en: For clarity, a ResNet-50 has roughly 26 million parameters, with IMP updating
    all of these. The least complex method, *BN-Recalibrate*, requires only a forward
    pass and no gradient computation at all. On the other hand, updating all non-BN
    biases requires gradients for about 0.004% of the parameters and also updates
    BN statistics. Including all BN parameters raises this count to 0.21%, while *Linear
    Probing* requires around 8.25% of the parameters.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清楚起见，一个 ResNet-50 具有大约 2600 万个参数，IMP 更新了所有这些参数。最简单的方法 *BN-Recalibrate* 仅需要前向传播，而不需要计算梯度。另一方面，更新所有非
    BN 偏置需要对大约 0.004% 的参数进行梯度计算，同时也更新 BN 统计数据。包括所有 BN 参数会将这个比例提高到 0.21%，而 *线性探测* 需要大约
    8.25% 的参数。
- en: The GPT models utilize LN, which calculates mean and variance consistently during
    both training and inference, unlike BN. Thus, for these models, we update *LN-Parameters*
    instead of *BN-Parameters* and further exclude the recalibration. Before examining
    the efficacy of the selective parameter-efficient retraining strategies just presented,
    we explore the application of the well-known reparametrization approach, LoRA
    (Hu et al., [2021](#bib.bib22)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型利用 LN，这在训练和推理过程中始终计算均值和方差，不同于 BN。因此，对于这些模型，我们更新*LN-Parameters*而不是*BN-Parameters*，并进一步排除重新校准。在检查刚刚提出的选择性参数高效再训练策略的有效性之前，我们探讨了广泛使用的重参数化方法
    LoRA（Hu et al., [2021](#bib.bib22)）。
- en: 'Retraining as Low-Rank Adaption. The motivation for LoRA stems from the observation
    that pretrained models exhibit low intrinsic dimensionality (Aghajanyan et al.,
    [2020](#bib.bib1)): results comparable to full FT can be achieved even with restricted,
    low-dimensional reparametrizations. Extending this logic, we hypothesize that
    pruned networks can be retrained parameter-efficiently through low-rank adaption.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应的再训练。LoRA 的动机源于这样一个观察：预训练模型展示了低内在维度（Aghajanyan et al., [2020](#bib.bib1)）：即使使用受限的低维重参数化，也能达到与完全微调相当的结果。基于这一逻辑，我们假设剪枝后的网络可以通过低秩适应以参数高效的方式进行再训练。
- en: 'Yet, adapting LoRA to the prune-retrain paradigm poses challenges. In dense
    models, LoRA does not increase inference costs during deployment since eventually
    undoing the reparametrization by setting $W\leftarrow W+BA$. While this issue
    is easy to address for structured sparsity patterns (see [Appendix A](#A1 "Appendix
    A Technical details and training settings ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs")), we argue that in the unstructured case the overall
    parameter increase by adding LoRA layers is negligible.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将 LoRA 适应到剪枝再训练范式中存在挑战。在密集模型中，LoRA 不会增加部署时的推理成本，因为最终通过设置 $W\leftarrow W+BA$
    来撤销重参数化。虽然对于结构化稀疏模式，这个问题容易解决（见 [附录 A](#A1 "附录 A 技术细节和训练设置 ‣ PERP：在 LLM 时代重新思考剪枝再训练范式")），我们认为在非结构化情况下，添加
    LoRA 层所带来的整体参数增加是微不足道的。
- en: Precisely, in unstructured weight pruning, the matrix $W$ barely impacts the
    model’s size. This addition, however, does decrease the overall sparsity slightly,
    which we account for in our reporting.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 精确地说，在非结构化权重剪枝中，矩阵 $W$ 几乎不影响模型的大小。然而，这一增加确实稍微降低了整体稀疏度，我们在报告中进行了考虑。
- en: In the following, we use the umbrella term *Parameter-Efficient Retraining after
    Pruning (PERP)* for our approach that combines updating biases, normalization
    parameters, the linear head, and low-rank adaptation of other layers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下文中，我们使用*剪枝后的参数高效再训练（PERP）*这一统称，来描述我们结合更新偏差、归一化参数、线性头和其他层低秩适应的方法。
- en: 'Results. In [Figure 2](#S2.F2 "Figure 2 ‣ 2.3 Experimental setup ‣ 2 Methodology
    and Experimental Setup ‣ PERP: Rethinking the Prune-Retrain Paradigm in the Era
    of LLMs"), we compare the test accuracy of the methods after One Shot pruning
    and retraining using ResNet-50 on ImageNet. For clarity, we exclude *Biases* due
    to its minimal improvement over *BN-Recalibrate*. We note that pruning without
    retraining is unable to maintain performance, even at moderate sparsity. However,
    recalibrating BN statistics recovers much of the performance at test time, supporting
    the findings of Li et al. ([2020](#bib.bib35)). Surprisingly, *BN-Parameters*
    restores most of the performance, nearly matching full retraining with up to 70%
    of the parameters pruned, while retraining only 0.21% of the architecture’s 26M
    parameters, thus significantly reducing memory usage.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。在 [图 2](#S2.F2 "图 2 ‣ 2.3 实验设置 ‣ 2 方法论和实验设置 ‣ PERP：在 LLM 时代重新思考剪枝再训练范式")中，我们比较了使用
    ResNet-50 在 ImageNet 上进行 One Shot 剪枝和再训练后的方法测试准确率。为清晰起见，我们排除了*Biases*，因为其对*BN-Recalibrate*的改进很小。我们注意到，剪枝而不再训练无法维持性能，即使在适度稀疏情况下也是如此。然而，重新校准
    BN 统计数据在测试时恢复了大部分性能，支持了 Li et al. ([2020](#bib.bib35)) 的发现。令人惊讶的是，*BN-Parameters*
    恢复了大部分性能，几乎与完全再训练相匹配，剪枝多达 70% 的参数，同时仅再训练架构中 0.21% 的 26M 参数，从而显著减少了内存使用。
- en: 'At moderate sparsity, adjusting only BN parameters can outperform full retraining.
    We think that this largely aligns with the observation that full FT in transfer
    learning can harm pretrained (or in our case pruned) features, a problem Kumar
    et al. ([2022a](#bib.bib29)) mitigate by adjusting only the linear head. In [Appendix B](#A2
    "Appendix B Additional experiments ‣ PERP: Rethinking the Prune-Retrain Paradigm
    in the Era of LLMs"), we demonstrate that longer retraining addresses this, giving
    the proposed methods an efficiency advantage when comparing on equal performance
    terms. *Linear Probing* further enhances performance in high sparsity scenarios,
    though it is not fully able to close the gap to full retraining: higher sparsity
    levels require updating more parameters to counteract pruning-induced performance
    loss.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '在适度的稀疏度下，仅调整 BN 参数的效果可以超过完全重新训练。我们认为这在很大程度上与完全微调在迁移学习中可能对预训练（或者在我们的情况中是剪枝）特征造成损害的观察结果一致，Kumar
    等人 ([2022a](#bib.bib29)) 通过仅调整线性头部来缓解这一问题。在 [附录 B](#A2 "Appendix B Additional
    experiments ‣ PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs")
    中，我们展示了更长时间的重新训练解决了这个问题，在相同性能条件下，给提议的方法带来了效率上的优势。*线性探测*进一步提升了高稀疏度场景中的性能，尽管它并未完全缩小与完全重新训练之间的差距：更高的稀疏度水平需要更新更多的参数以对抗剪枝引起的性能损失。'
- en: 'Finally, PERP, further incorporating LoRA, significantly narrows the performance
    gap observed in earlier approaches. We reparametrize all layers except the linear
    head, see [Appendix A](#A1 "Appendix A Technical details and training settings
    ‣ PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs") for details.
    Using PERP, the fraction of trainable parameters ranges between 8.6% and 12.5%
    of the full model, depending on $r$ (1, 2, 5, 10).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，PERP 进一步结合 LoRA 显著缩小了早期方法中观察到的性能差距。我们对所有层进行了重新参数化，除了线性头部，详情见 [附录 A](#A1
    "Appendix A Technical details and training settings ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs")。使用 PERP，可训练参数的比例在完整模型的 8.6% 到 12.5% 之间，具体取决于 $r$（1、2、5、10）。'
- en: 3.2 Efficient Retraining of Large Models
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 大型模型的高效重新训练
- en: We demonstrated that only very few parameters are actually needed to restore
    performance after One Shot pruning ResNet-50\. Especially normalization parameters
    in combination with low-rank adapters are able to adjust the pruned features to
    work notably well, despite pruning damaging the model and dropping performance
    drastically at moderate sparsities. As we discuss now, PERP is highly effective
    in the context of LLMs, where full retraining is infeasible.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们证明了在 One Shot 剪枝 ResNet-50 之后，实际上只需要非常少量的参数来恢复性能。尤其是，结合低秩适配器的归一化参数能够调整剪枝后的特征，使其表现非常好，尽管剪枝损害了模型并在适度稀疏度下显著降低了性能。正如我们现在讨论的，PERP
    在 LLM 的背景下非常有效，因为完全重新训练不可行。
- en: 'In [Table 1](#S3.T1 "Table 1 ‣ 3.2 Efficient Retraining of Large Models ‣ 3
    Parameter-Efficient Retraining ‣ PERP: Rethinking the Prune-Retrain Paradigm in
    the Era of LLMs"), we present the final Wikitext perplexity for pruning and retraining
    OPT-2.7B and OPT-30B for 1000 iterations. When comparing to full IMP, we are restricted
    to using a model no greater than a *mere* 2.7 billion parameters, as we are not
    able to fully retrain larger models due to GPU-memory constraints. We overcome
    the constraint of batch size 1 by accumulating gradients over multiple steps.
    For PERP, we set the rank to 16 for each attention matrix, noting that we ablate
    the LoRA configuration in [Appendix C](#A3 "Appendix C Ablation studies ‣ PERP:
    Rethinking the Prune-Retrain Paradigm in the Era of LLMs"). Our experiments also
    revealed that retraining the embedding layer was not effective, and retraining
    the entire linear head, as in *Linear Probing*, was less stable than applying
    LoRA reparametrization to it, further minimizing trainable parameters. The resulting
    reduction in overall sparsity by PERP is a negligible 0.10%-0.19%.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [表 1](#S3.T1 "Table 1 ‣ 3.2 Efficient Retraining of Large Models ‣ 3 Parameter-Efficient
    Retraining ‣ PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs")
    中，我们展示了对 OPT-2.7B 和 OPT-30B 进行 1000 次迭代的剪枝和重新训练的最终 Wikitext 困惑度。与完全 IMP 相比，我们只能使用不超过
    *仅* 27 亿参数的模型，因为由于 GPU 内存限制，我们无法完全重新训练更大的模型。我们通过在多个步骤中累积梯度来克服批量大小为 1 的限制。对于 PERP，我们将每个注意力矩阵的秩设置为
    16，并注意到我们在 [附录 C](#A3 "Appendix C Ablation studies ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs") 中省略了 LoRA 配置。我们的实验还揭示了重新训练嵌入层并不有效，而像 *线性探测* 中那样重新训练整个线性头部，比应用
    LoRA 重新参数化更不稳定，从而进一步减少了可训练参数。PERP 造成的整体稀疏度减少为微不足道的 0.10%-0.19%。'
- en: 'PERP matches Full IMP’s perplexity while only retraining 0.27% of the 2.7 billion
    parameters, even outperforming it for higher levels of sparsity where the increase
    in perplexity compared to the dense model is non-negligible. We observe similar
    differences between the approaches as before, except that *Linear Probing* often
    slightly underperforms *LN-Parameters*. Unlike accuracy, perplexity is unbounded
    and can explode with increased sparsity, as visible when not performing any retraining.
    Nevertheless, the perplexity is reduced effectively by PERP. We note that retraining
    the dense model on C4 does not bring any benefits and that these results transfer
    to the LLaMa-2, Mistral and Mixtral models, cf. [Appendix B](#A2 "Appendix B Additional
    experiments ‣ PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs").'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: PERP 在仅重新训练0.27%的27亿个参数的情况下，与全量IMP的困惑度相匹配，甚至在较高的稀疏度水平下超过了它，在这些情况下，相比于密集模型的困惑度增加不可忽视。我们观察到这些方法之间的差异与之前类似，只是*线性探测*常常略微不如*LN-参数*。与准确率不同，困惑度是无限的，并且在增加稀疏度时可能会爆炸，这在未进行任何重新训练时是显而易见的。尽管如此，PERP有效地降低了困惑度。我们注意到，在C4上重新训练密集模型没有带来任何好处，这些结果也适用于LLaMa-2、Mistral和Mixtral模型，参见
    [附录 B](#A2 "附录 B 额外实验 ‣ PERP：在LLMs时代重新思考修剪-重新训练范式")。
- en: 'We highlight that we are able to retrain the 30B parameter model using just
    a single NVIDIA A100 GPU, underscoring the memory efficiency of PERP in the pruning
    context. In contrast, full retraining of OPT-30B would require multiple GPUs.
    However, PERP not only cuts down storage costs and enables retraining of large
    models, but also enhances retraining efficiency. For instance, using OPT-2.7B
    on the same compute setup, full retraining achieves a maximum of 3500 train tokens
    per second (tps), whereas PERP nearly doubles this efficiency to 6400 tps. Updating
    only biases and normalization parameters further increases this rate to 7600 tps.
    In addition, as depicted in [Figure 3](#S3.F3 "Figure 3 ‣ 3.2 Efficient Retraining
    of Large Models ‣ 3 Parameter-Efficient Retraining ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs") displaying the perplexity (log-scale) vs. the number
    of retraining iterations, PERP rapidly decreases the perplexity of OPT-6.7B across
    various sparsity levels. Without retraining (i.e., zero iterations), perplexity
    explodes exponentially from approximately $10^{1}$. However, PERP significantly
    lowers perplexity and saturates after only a few iterations. This efficient retraining
    is also evident in [Figure 2](#S2.F2 "Figure 2 ‣ 2.3 Experimental setup ‣ 2 Methodology
    and Experimental Setup ‣ PERP: Rethinking the Prune-Retrain Paradigm in the Era
    of LLMs"), where a single epoch suffices to restore accuracy at moderate to high
    sparsity levels, contrasting with the more extensive epoch requirements of full
    retraining.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调我们能够使用单个NVIDIA A100 GPU重新训练30B参数模型，突显了PERP在修剪背景下的内存效率。相比之下，完整的OPT-30B重新训练需要多个GPU。然而，PERP不仅降低了存储成本并使大型模型的重新训练成为可能，还提高了重新训练效率。例如，在相同计算设置下使用OPT-2.7B，完整的重新训练每秒最大可处理3500个训练令牌（tps），而PERP将这一效率几乎翻倍，达到6400
    tps。仅更新偏置和归一化参数进一步将这一速度提升至7600 tps。此外，如 [图 3](#S3.F3 "图 3 ‣ 3.2 大型模型的高效重新训练 ‣
    3 参数高效重新训练 ‣ PERP：在LLMs时代重新思考修剪-重新训练范式")所示，显示了困惑度（对数尺度）与重新训练迭代次数的关系，PERP迅速降低了OPT-6.7B在各种稀疏度水平下的困惑度。没有重新训练（即零迭代）时，困惑度从大约
    $10^{1}$ 指数式爆炸。然而，PERP显著降低了困惑度，并在仅经过几次迭代后达到饱和。这种高效的重新训练在 [图 2](#S2.F2 "图 2 ‣ 2.3
    实验设置 ‣ 2 方法论和实验设置 ‣ PERP：在LLMs时代重新思考修剪-重新训练范式") 中也很明显，在中等到高稀疏度水平下，仅一个训练周期即可恢复准确率，这与完整重新训练所需的更多训练周期形成对比。
- en: 'In summary, our results demonstrate that updating a critical subset of parameters
    and applying LoRA suffices to restore a significant portion of the performance
    achievable through full retraining. This approach not only enables retraining
    of large models within memory constraints but also ensures efficiency, requiring
    minimal yet effective iterations for performance recovery. PERP thereby makes
    the retraining of pruned models feasible, even on constrained hardware resources
    and with GPT-scale models. In [Appendix C](#A3 "Appendix C Ablation studies ‣
    PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs"), we dissect the
    impact of the individual methods by comparing all possible combinations of parameter
    groups.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的结果表明，更新关键参数子集并应用 LoRA 足以恢复通过完全重新训练可以实现的大部分性能。这种方法不仅使在内存限制下重新训练大型模型成为可能，还确保了效率，仅需最小且有效的迭代即可实现性能恢复。因此，PERP
    使得即使在受限硬件资源和 GPT 规模模型下，剪枝模型的重新训练变得可行。在 [附录 C](#A3 "附录 C 消融研究 ‣ PERP：重新思考 LLMs
    时代的剪枝-重新训练范式") 中，我们通过比较所有可能的参数组组合来剖析各个方法的影响。
- en: 'Table 1: OPT-2.7B/30B: Parameter-efficient approaches vs. full retraining with
    30%-70% of the parameters pruned. The first column lists the method, and the second
    shows the percentage of trainable parameters (Full IMP represents the standard
    retraining baseline). The next five columns display the average mean perplexity
    (lower is better) across multiple seeds, with standard deviations excluded for
    clarity. The dense model attains a perplexity of 12.47 and 9.55 for OPT-2.7B/30B,
    respectively.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：OPT-2.7B/30B：参数效率方法与剪枝 30%-70% 参数的完全重新训练。第一列列出方法，第二列显示可训练参数的百分比（完整 IMP 代表标准重新训练基线）。接下来的五列显示了多个种子的平均困惑度（数值越低越好），为了清晰起见未包括标准差。密集模型的困惑度分别为
    OPT-2.7B/30B 的 12.47 和 9.55。
- en: '| OPT-2.7B |  |  |  |  |  |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7B |  |  |  |  |  |  |'
- en: '| Perplexity: 12.47 |  | Sparsity |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度：12.47 |  | 稀疏性 |'
- en: '| Method | % trainable | 30% | 40% | 50% | 60% | 70% |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | % 可训练 | 30% | 40% | 50% | 60% | 70% |'
- en: '| Full IMP | 100% | 13.47 | 14.31 | 15.85 | 19.54 | 28.37 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 完整 IMP | 100% | 13.47 | 14.31 | 15.85 | 19.54 | 28.37 |'
- en: '| PERP | 0.27% | 13.42 | 14.50 | 16.38 | 19.20 | 27.01 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| PERP | 0.27% | 13.42 | 14.50 | 16.38 | 19.20 | 27.01 |'
- en: '| Linear Probing | 0.07% | 13.47 | 14.71 | 17.10 | 21.33 | 35.75 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 线性探测 | 0.07% | 13.47 | 14.71 | 17.10 | 21.33 | 35.75 |'
- en: '| LN-Parameters | 0.04% | 13.53 | 14.71 | 16.66 | 21.12 | 34.39 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| LN-参数 | 0.04% | 13.53 | 14.71 | 16.66 | 21.12 | 34.39 |'
- en: '| Biases | 0.03% | 13.58 | 14.84 | 16.86 | 22.07 | 39.57 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 偏差 | 0.03% | 13.58 | 14.84 | 16.86 | 22.07 | 39.57 |'
- en: '| No Retraining | 0.00% | 15.58 | 30.32 | 265.19 | 3604.16 | 7251.81 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 无重新训练 | 0.00% | 15.58 | 30.32 | 265.19 | 3604.16 | 7251.81 |'
- en: '| OPT-30B |  |  |  |  |  |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30B |  |  |  |  |  |  |'
- en: '| Perplexity: 9.55 |  | Sparsity |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度：9.55 |  | 稀疏性 |'
- en: '| Method | % trainable | 30% | 40% | 50% | 60% | 70% |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | % 可训练 | 30% | 40% | 50% | 60% | 70% |'
- en: '| PERP | 0.09% | 10.43 | 11.42 | 12.29 | 14.50 | 21.66 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| PERP | 0.09% | 10.43 | 11.42 | 12.29 | 14.50 | 21.66 |'
- en: '| Linear Probing | 0.02% | 10.31 | 11.49 | 12.80 | 15.75 | 54.26 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 线性探测 | 0.02% | 10.31 | 11.49 | 12.80 | 15.75 | 54.26 |'
- en: '| LN-Parameters | 0.01% | 10.37 | 11.43 | 12.82 | 15.75 | 43.06 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| LN-参数 | 0.01% | 10.37 | 11.43 | 12.82 | 15.75 | 43.06 |'
- en: '| Biases | 0.01% | 10.41 | 11.49 | 13.80 | 17.00 | 408.04 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 偏差 | 0.01% | 10.41 | 11.49 | 13.80 | 17.00 | 408.04 |'
- en: '| No retraining | 0.00% | 12.37 | 24.29 | 168.07 | 11675.34 | 28170.72 | ![Refer
    to caption](img/43371ca98f7b61f4acf5532517d3a9e2.png)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '| 无重新训练 | 0.00% | 12.37 | 24.29 | 168.07 | 11675.34 | 28170.72 | ![参见说明](img/43371ca98f7b61f4acf5532517d3a9e2.png)'
- en: 'Figure 3: OPT-6.7B evaluated on WikiText: Final perplexity after retraining
    for as many iterations as indicated on the x-axis. PERP retrains only 0.16% of
    the parameters.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：OPT-6.7B 在 WikiText 上的评估：在 x 轴上指示的迭代次数后的最终困惑度。PERP 仅重新训练了 0.16% 的参数。
- en: 3.3 Reconsidering Magnitude Pruning of LLMs
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 重新考虑 LLMs 的幅度剪枝
- en: The rise of LLMs has rendered classical retraining impractical, as fully retraining
    GPT-scale models, even in the One Shot case, exceeds the resource capabilities
    of many practitioners (Jaiswal et al., [2023a](#bib.bib24)). As we have demonstrated,
    retraining can become much more efficient and viable by focusing on the network’s
    most critical parameters. At the same time, there is growing interest in developing
    pruning criteria other than magnitude that yield high-performance models without
    the need for retraining (Kwon et al., [2022](#bib.bib31); Frantar & Alistarh,
    [2023](#bib.bib12); Sun et al., [2023](#bib.bib55)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的兴起使得经典的再训练变得不切实际，因为即使在一次性案例中，完全再训练 GPT 规模的模型也超出了许多从业者的资源能力（Jaiswal et al.,
    [2023a](#bib.bib24)）。正如我们所示，通过关注网络中最关键的参数，再训练可以变得更高效和可行。同时，开发除幅度之外的修剪标准以生成高性能模型而无需再训练的兴趣也在增长（Kwon
    et al., [2022](#bib.bib31); Frantar & Alistarh, [2023](#bib.bib12); Sun et al.,
    [2023](#bib.bib55)）。
- en: Despite its effectiveness in the domain of convolutional architectures, the
    magnitude-criterion has been recognized as unsuited for pruning LLMs in a retraining-free
    setting (Frantar & Alistarh, [2023](#bib.bib12); Sun et al., [2023](#bib.bib55)).
    Yin et al. ([2023](#bib.bib65)) considered magnitude pruning as no better than
    random pruning at higher sparsities and note that its success is closely intertwined
    with the feasibility of retraining. Both Sun et al. ([2023](#bib.bib55)) and Yin
    et al. ([2023](#bib.bib65)) explain the inability to magnitude-prune LLMs with
    observations made by Dettmers et al. ([2022](#bib.bib6)) regarding the *emergence
    of large magnitude features* in transformers beyond a certain size. These large
    features, a small yet significant subset of hidden features, are critical for
    model performance, and pruning them severely impacts predictive accuracy (Sun
    et al., [2023](#bib.bib55)); a problem that magnitude pruning fails to address.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在卷积架构领域效果显著，但幅度标准已被认为不适用于在无需再训练的情况下修剪 LLM（Frantar & Alistarh, [2023](#bib.bib12);
    Sun et al., [2023](#bib.bib55)）。Yin et al.（[2023](#bib.bib65)）认为在较高稀疏度下幅度修剪不比随机修剪更好，并指出其成功与再训练的可行性密切相关。Sun
    et al.（[2023](#bib.bib55)）和 Yin et al.（[2023](#bib.bib65)）解释了幅度修剪无法用于 LLM 的原因，基于
    Dettmers et al.（[2022](#bib.bib6)）对变压器中大幅度特征*出现*的观察。这些大特征是隐藏特征中的一个小而重要的子集，对模型性能至关重要，修剪它们会严重影响预测准确性（Sun
    et al., [2023](#bib.bib55)）；这是幅度修剪未能解决的问题。
- en: We agree and have demonstrated that simple magnitude pruning leads to a model
    collapse at even moderate sparsity, making it unsuitable for a retraining-free
    scenario. However, our successful mitigation of the exploding perplexity issue
    with minimal memory requirements suggests revisiting the applicability of magnitude
    pruning for LLMs, particularly as previous studies report high perplexity and
    suggest that entirely new pruning criteria are needed for LLMs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同意，并已证明，简单的幅度修剪在中等稀疏度下会导致模型崩溃，使其不适用于无需再训练的场景。然而，我们成功地缓解了爆炸性困惑度问题，并且只需最小的内存要求，这表明有必要重新审视幅度修剪在
    LLM 中的适用性，尤其是以前的研究报告了高困惑度，并建议 LLM 需要全新的修剪标准。
- en: 'We evaluate magnitude pruning against two state-of-the-art retraining-free
    pruning methods: *SparseGPT* (Frantar & Alistarh, [2023](#bib.bib12)) and *Wanda*
    (Sun et al., [2023](#bib.bib55)). SparseGPT, using second-order information to
    address a layer-wise reconstruction problem, prunes large models with little increase
    in perplexity, however at the price of increased pruning time. Notably, SparseGPT
    not only identifies a pruning mask but also adjusts the remaining weights to minimize
    discrepancies between the dense and sparse model. Wanda enhances the magnitude
    criterion to incorporate the feature activation, reaching performance competitive
    to SparseGPT in a more efficient way. As opposed to magnitude pruning, both approaches
    rely on calibration data, which influences the quality of the final result (Williams
    & Aletras, [2023](#bib.bib60)).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将幅度修剪与两种最先进的无需再训练的修剪方法进行比较：*SparseGPT*（Frantar & Alistarh, [2023](#bib.bib12)）和*Wanda*（Sun
    et al., [2023](#bib.bib55)）。SparseGPT 使用二阶信息来解决逐层重建问题，可以在几乎不增加困惑度的情况下修剪大型模型，但代价是增加了修剪时间。值得注意的是，SparseGPT
    不仅识别修剪掩码，还调整剩余权重，以最小化密集模型和稀疏模型之间的差异。Wanda 提升了幅度标准，将特征激活纳入考虑，以更高效的方式达到与 SparseGPT
    竞争的性能。与幅度修剪不同，这两种方法都依赖于校准数据，这会影响最终结果的质量（Williams & Aletras, [2023](#bib.bib60)）。
- en: '[Table 2](#S3.T2 "Table 2 ‣ 3.3 Reconsidering Magnitude Pruning of LLMs ‣ 3
    Parameter-Efficient Retraining ‣ PERP: Rethinking the Prune-Retrain Paradigm in
    the Era of LLMs") presents a comparative analysis of pruning criteria on OPT-2.7B/6.7B/13B/30B
    models with 50% weight removal and semi-structured 2:4 and 4:8 sparsity. The first
    row represents the baseline case without pruning. We assess both magnitude pruning
    with and without PERP. For fairness, we also retrained Wanda and SparseGPT with
    PERP, listing the full results with and without PERP in the appendix. The appendix
    further contains results for other models.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 2](#S3.T2 "表 2 ‣ 3.3 重新考虑 LLMs 的幅度修剪 ‣ 3 参数高效再训练 ‣ PERP：在 LLM 时代重新思考修剪-再训练范式")
    展示了在 OPT-2.7B/6.7B/13B/30B 模型上进行 50% 权重移除和半结构化 2:4 及 4:8 稀疏度的修剪标准的比较分析。第一行代表未进行修剪的基准情况。我们评估了有无
    PERP 的幅度修剪。为了公平起见，我们还使用 PERP 重新训练了 Wanda 和 SparseGPT，并在附录中列出了有无 PERP 的完整结果。附录中还包含其他模型的结果。'
- en: 'As similarly seen in [Figure 3](#S3.F3 "Figure 3 ‣ 3.2 Efficient Retraining
    of Large Models ‣ 3 Parameter-Efficient Retraining ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs"), while magnitude pruning substantially increases
    perplexity, PERP efficiently reduces it to levels on par with SparseGPT and Wanda
    across all configurations. This indicates that while magnitude pruning alone may
    be ineffective, it is not inherently unsuited for LLMs despite presumably failing
    to address large features. Minimal, efficient retraining can significantly recover
    close to initial perplexity, offering a viable option over completely avoiding
    retraining. Nevertheless, magnitude pruning with PERP does not entirely match
    the performance of Wanda and SparseGPT, with the gap reducing as model size increases.
    This highlights the merit of (and the need for more) precise LLM-pruning methods
    such as Wanda and SparseGPT. However, given that both require calibration data
    and a more time-intensive pruning step than the simple magnitude heuristic, we
    think that practitioners should choose depending on model size and the desired
    degree of sparsification, where magnitude pruning might be preferable due to its
    speed advantage, even if it entails some parameter-efficient retraining. [Figure 3](#S3.F3
    "Figure 3 ‣ 3.2 Efficient Retraining of Large Models ‣ 3 Parameter-Efficient Retraining
    ‣ PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs") also underlines
    that, in the case of magnitude pruning, the minimum number of retraining iterations
    to reach good performance directly depends on the impact of compression or goal
    sparsity at hand, which is not necessarily the case for other methods such as
    Wanda. [Appendix C](#A3 "Appendix C Ablation studies ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs") discusses the setting with higher sparsity levels.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在 [图 3](#S3.F3 "图 3 ‣ 3.2 大模型的高效再训练 ‣ 3 参数高效再训练 ‣ PERP：在 LLM 时代重新思考修剪-再训练范式")
    中类似地看到的，虽然幅度修剪显著增加了困惑度，但 PERP 能够有效将困惑度降低到与 SparseGPT 和 Wanda 相当的水平。这表明，虽然单独的幅度修剪可能效果不佳，但它并不是不适合
    LLM 的，尽管它可能未能解决大型特征的问题。最小化的高效再训练可以显著恢复接近初始困惑度，提供了一个比完全避免再训练更可行的选项。然而，使用 PERP 的幅度修剪并未完全匹配
    Wanda 和 SparseGPT 的性能，且随着模型规模的增大，这一差距有所减少。这突显了像 Wanda 和 SparseGPT 这样的精确 LLM 修剪方法的价值（以及对更多此类方法的需求）。然而，考虑到这两者都需要校准数据和比简单的幅度启发式方法更为耗时的修剪步骤，我们认为实践者应该根据模型大小和所需的稀疏化程度来选择，其中幅度修剪由于其速度优势可能更为可取，即便这涉及到一定的参数高效再训练。[图
    3](#S3.F3 "图 3 ‣ 3.2 大模型的高效再训练 ‣ 3 参数高效再训练 ‣ PERP：在 LLM 时代重新思考修剪-再训练范式") 还强调了，在幅度修剪的情况下，达到良好性能所需的最小再训练迭代次数直接依赖于压缩的影响或目标稀疏度，而其他方法如
    Wanda 并不一定如此。[附录 C](#A3 "附录 C 消融研究 ‣ PERP：在 LLM 时代重新思考修剪-再训练范式") 讨论了更高稀疏度水平的设置。
- en: 'Table 2: OPT-2.7B/6.7B/13B/30B: Perplexity comparison of naive magnitude pruning
    to magnitude pruning, Wanda and SparseGPT with PERP, both in the unstructured
    pruning setting (50% sparsity), as well as for the semi-structured 2:4 and 4:8
    sparsities. All methods using PERP (indicated in the second column) are retrained
    for 1000 iterations and further highlighted in bold. We report the mean perplexity
    over several seeds and omit the standard deviation for the sake of clarity.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：OPT-2.7B/6.7B/13B/30B：对比了在 50% 稀疏度的非结构化修剪设置下、以及在半结构化 2:4 和 4:8 稀疏度下的原始幅度修剪与使用
    PERP 的幅度修剪、Wanda 和 SparseGPT 的困惑度。所有使用 PERP 的方法（在第二列中标示）都进行了 1000 次迭代的再训练，并进一步用**粗体**标出。我们报告了多个种子上的平均困惑度，并为清晰起见省略了标准差。
- en: '|  |  |  | OPT |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | OPT |'
- en: '| --- | --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Method | PERP | Sparsity | 2.7B | 6.7B | 13B | 30B |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PERP | 稀疏度 | 2.7B | 6.7B | 13B | 30B |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Baseline | ✗ | 0% | 12.47 | 10.86 | 10.12 | 9.55 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | ✗ | 0% | 12.47 | 10.86 | 10.12 | 9.55 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Magnitude | ✗ | 50% | 265.19 | 968.80 | 11568.33 | 168.07 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | ✗ | 50% | 265.19 | 968.80 | 11568.33 | 168.07 |'
- en: '| Magnitude | ✓ | 50% | 15.76 | 13.60 | 12.54 | 11.94 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | ✓ | 50% | 15.76 | 13.60 | 12.54 | 11.94 |'
- en: '| Wanda | ✓ | 50% | 13.88 | 11.83 | 11.06 | 10.03 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 万达 | ✓ | 50% | 13.88 | 11.83 | 11.06 | 10.03 |'
- en: '| SparseGPT | ✓ | 50% | 13.40 | 11.47 | 10.85 | 9.76 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✓ | 50% | 13.40 | 11.47 | 10.85 | 9.76 |'
- en: '| Magnitude | ✗ | 2:4 | 1152.89 | 264.09 | 484.74 | 1979.66 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | ✗ | 2:4 | 1152.89 | 264.09 | 484.74 | 1979.66 |'
- en: '| Magnitude | ✓ | 2:4 | 17.90 | 14.31 | 13.14 | 12.27 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | ✓ | 2:4 | 17.90 | 14.31 | 13.14 | 12.27 |'
- en: '| Wanda | ✓ | 2:4 | 16.29 | 13.48 | 12.03 | 10.92 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 万达 | ✓ | 2:4 | 16.29 | 13.48 | 12.03 | 10.92 |'
- en: '| SparseGPT | ✓ | 2:4 | 15.23 | 12.70 | 11.56 | 10.39 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✓ | 2:4 | 15.23 | 12.70 | 11.56 | 10.39 |'
- en: '| Magnitude | ✗ | 4:8 | 166.92 | 196.17 | 449.64 | 563.84 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | ✗ | 4:8 | 166.92 | 196.17 | 449.64 | 563.84 |'
- en: '| Magnitude | ✓ | 4:8 | 16.43 | 13.77 | 12.57 | 12.06 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | ✓ | 4:8 | 16.43 | 13.77 | 12.57 | 12.06 |'
- en: '| Wanda | ✓ | 4:8 | 14.99 | 12.53 | 11.41 | 10.58 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 万达 | ✓ | 4:8 | 14.99 | 12.53 | 11.41 | 10.58 |'
- en: '| SparseGPT | ✓ | 4:8 | 14.23 | 11.98 | 11.03 | 10.13 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✓ | 4:8 | 14.23 | 11.98 | 11.03 | 10.13 |'
- en: '3.4 Magnitude conservation: Restabilizing the network'
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 大小保留：重新稳定网络
- en: 'We demonstrated the ability to restore performance efficiently. However, as
    detailed in [Section 2](#S2 "2 Methodology and Experimental Setup ‣ PERP: Rethinking
    the Prune-Retrain Paradigm in the Era of LLMs"), restoring performance is not
    the only objective of retraining. For high sparsity levels, employing multiple
    prune-retrain cycles or phases can be advantageous to avoid *layer-collapse*,
    a scenario where a layer is entirely pruned, potentially rendering the model dysfunctional
    (Tanaka et al., [2020](#bib.bib56)).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '我们展示了高效恢复性能的能力。然而，如[第2节](#S2 "2 Methodology and Experimental Setup ‣ PERP:
    Rethinking the Prune-Retrain Paradigm in the Era of LLMs")中详细描述的，恢复性能并不是再训练的唯一目标。对于高稀疏度水平，采用多个修剪-再训练周期或阶段可以避免*层崩溃*的情况，即某一层完全被修剪掉，可能导致模型失效（Tanaka
    et al., [2020](#bib.bib56)）。'
- en: While methods like PERP update a subset of parameters or additional ones, they
    do not inherently prevent layer collapse, as most parameters remain unchanged.
    An exception is the use of PERP in structured pruning, which allows for updating
    all non-pruned weights by merging the adapters and pretrained weights at the end
    of each phase. For unstructured pruning, the challenge is to ensure magnitude
    conservation by updating all parameters while also being parameter-efficient.
    Inspired by the transfer learning research of Kumar et al. ([2022a](#bib.bib29),
    [b](#bib.bib30)), we explore the strategy of selectively updating layers based
    on their role and position in the network. These studies suggest that lower layers
    require less updating over the course of FT, leading to techniques like gradual
    unfreezing of the model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像PERP这样的方法更新了部分参数或额外的参数，但它们本质上并未防止层崩溃，因为大多数参数保持不变。例外的是在结构化修剪中使用PERP，这允许在每个阶段末尾通过合并适配器和预训练权重来更新所有未修剪的权重。对于非结构化修剪，挑战在于通过更新所有参数来确保大小保留，同时保持参数的高效性。受Kumar等人的转移学习研究的启发（[2022a](#bib.bib29),
    [b](#bib.bib30)），我们探索了基于层在网络中角色和位置的选择性更新策略。这些研究表明，较低的层在FT过程中需要较少的更新，从而引出了如逐渐解冻模型等技术。
- en: 'In our ImageNet experiments in [Table 12](#A2.T12 "Table 12 ‣ B.4 Magnitude
    conservation: Restabilizing the network ‣ Appendix B Additional experiments ‣
    PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs") in the appendix,
    we test gradual freezing and unfreezing of layers across retraining epochs within
    a phase, either beginning with the full model and progressively freezing parameters
    (unfreeze ✗), or starting with a frozen model and gradually unfreezing (unfreeze
    ✓). Further, we freeze or unfreeze from input to output layer (reverse ✗) or vice-versa
    (reverse ✓). This process aligns the proportion of layers unfrozen or frozen with
    the proportion of performed epochs within a phase, ensuring that each parameter
    is adequately updated, preventing layer collapse while limiting the number of
    trainable parameters for efficiency. However, unlike previous methods with consistent
    memory demands throughout epochs, these approaches vary memory requirements and,
    although they might eventually retrain the entire model, significantly reduce
    overall memory and computational demands throughout a phase.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们在[表12](#A2.T12 "Table 12 ‣ B.4 Magnitude conservation: Restabilizing the
    network ‣ Appendix B Additional experiments ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs")的ImageNet实验中，我们测试了在一个相位内跨越重新训练周期的逐层冻结和解冻，要么从完整模型开始并逐步冻结参数（解冻
    ✗），要么从冻结模型开始并逐步解冻（解冻 ✓）。此外，我们可以从输入层到输出层冻结或解冻（反向 ✗）或反之（反向 ✓）。这一过程将解冻或冻结的层的比例与相位内执行的周期的比例对齐，确保每个参数得到充分更新，防止层崩溃，同时限制可训练参数的数量以提高效率。然而，与以前在整个周期中内存需求一致的方法不同，这些方法的内存要求有所变化，尽管它们可能最终重新训练整个模型，但在一个相位内显著降低了整体内存和计算需求。'
- en: We report ImageNet results for varying numbers of phases (2-5), each spanning
    five epochs, targeting a final sparsity of 90%. The table shows the mean accuracy
    deviation from full IMP, with standard deviations omitted for clarity. Each of
    the four variations results in a different total of trainable parameters (aggregated
    over all epochs in each phase). There is a clear correlation between performance
    and the fraction of trainable parameters; having 70% of the aggregated trainable
    parameters is sufficient to achieve results competitive with IMP. We note that
    in the final phase of IMP, PERP could further reduce these demands. Although these
    results are encouraging, it is important to note that this approach ultimately
    involves retraining the entire model, rendering it unsuitable for memory-constrained
    environments. We believe this area holds promise for future research.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告了在不同相位数（2-5）下的ImageNet结果，每个相位跨越五个周期，目标是最终稀疏度为90%。表格显示了与完整IMP的平均准确度偏差，标准偏差被省略以保持清晰。四种变体中的每一种导致了不同的可训练参数总数（在每个相位的所有周期中汇总）。性能与可训练参数的比例之间存在明显的关联；拥有70%的汇总可训练参数足以实现与IMP竞争的结果。我们注意到，在IMP的最后一个相位，PERP可能进一步减少这些需求。尽管这些结果令人鼓舞，但重要的是要注意这种方法最终涉及重新训练整个模型，因此不适合内存受限的环境。我们相信这一领域具有未来研究的潜力。
- en: 4 Discussion
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 讨论
- en: We demonstrated that retraining a minimal fraction of parameters is sufficient
    for mitigating pruning-induced performance drops. Our approach can require as
    little as 0.09% of the parameters used in full IMP, significantly lowering computational
    and memory demands. This efficiency enables the retraining of LLMs with up to
    30 billion parameters on a single NVIDIA A100 within minutes. Our findings make
    retraining after pruning a viable option for large models and we hope to stimulate
    further research on both training-free pruning criteria as well as efficient retraining.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们证明了重新训练最小比例的参数足以缓解修剪引起的性能下降。我们的方法只需0.09%的完整IMP参数，大大降低了计算和内存需求。这种高效性使得在几分钟内在单个NVIDIA
    A100上重新训练多达300亿参数的LLM成为可能。我们的发现使得在修剪后重新训练成为大型模型的可行选项，我们希望能激发对无训练修剪标准以及高效重新训练的进一步研究。
- en: References
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Aghajanyan et al. (2020) Aghajanyan, A., Zettlemoyer, L., and Gupta, S. Intrinsic
    dimensionality explains the effectiveness of language model fine-tuning. December
    2020.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aghajanyan 等人（2020） Aghajanyan, A., Zettlemoyer, L., 和 Gupta, S. 内在维度解释了语言模型微调的有效性。2020年12月。
- en: Ba et al. (2016) Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.
    July 2016.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ba 等人（2016） Ba, J. L., Kiros, J. R., 和 Hinton, G. E. 层归一化。2016年7月。
- en: 'Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no
    questions. May 2019.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., 和 Toutanova, K. BoolQ：探索自然的是/否问题的惊人难度。2019年5月。
- en: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try
    arc, the ai2 reasoning challenge. March 2018.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., 和 Tafjord, O. 认为你已经解决了问答问题？试试ARC，AI2推理挑战。2018年3月。
- en: 'Dettmers & Zettlemoyer (2019) Dettmers, T. and Zettlemoyer, L. Sparse networks
    from scratch: Faster training without losing performance. *arXiv preprint arXiv:1907.04840*,
    July 2019.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers & Zettlemoyer (2019) Dettmers, T. 和 Zettlemoyer, L. 从零开始的稀疏网络：更快的训练而不损失性能。*arXiv
    预印本 arXiv:1907.04840*，2019年7月。
- en: 'Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,
    L. Llm.int8(): 8-bit matrix multiplication for transformers at scale. August 2022.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,
    L. `Llm.int8()`: 用于大规模变换器的8位矩阵乘法。2022年8月。'
- en: Ding et al. (2019) Ding, X., Ding, G., Zhou, X., Guo, Y., Han, J., and Liu,
    J. Global sparse momentum sgd for pruning very deep neural networks. In Wallach,
    H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett, R.
    (eds.), *Advances in Neural Information Processing Systems*, volume 32\. Curran
    Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper/2019/file/f34185c4ca5d58e781d4f14173d41e5d-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/f34185c4ca5d58e781d4f14173d41e5d-Paper.pdf).
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. (2019) Ding, X., Ding, G., Zhou, X., Guo, Y., Han, J., 和 Liu, J.
    用于剪枝非常深的神经网络的全局稀疏动量SGD。在 Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc,
    F., Fox, E., 和 Garnett, R.（编者）*神经信息处理系统进展*，第32卷。Curran Associates, Inc.，2019年。网址
    [https://proceedings.neurips.cc/paper/2019/file/f34185c4ca5d58e781d4f14173d41e5d-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/f34185c4ca5d58e781d4f14173d41e5d-Paper.pdf)。
- en: 'Evci et al. (2020) Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen,
    E. Rigging the lottery: Making all tickets winners. In III, H. D. and Singh, A.
    (eds.), *Proceedings of the 37th International Conference on Machine Learning*,
    volume 119 of *Proceedings of Machine Learning Research*, pp.  2943–2952\. PMLR,
    13–18 Jul 2020. URL [https://proceedings.mlr.press/v119/evci20a.html](https://proceedings.mlr.press/v119/evci20a.html).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Evci et al. (2020) Evci, U., Gale, T., Menick, J., Castro, P. S., 和 Elsen, E.
    操控彩票：让所有票都成为赢家。在 III, H. D. 和 Singh, A.（编者）*第37届国际机器学习会议论文集*，机器学习研究论文集第119卷，第2943–2952页。PMLR，2020年7月13–18日。网址
    [https://proceedings.mlr.press/v119/evci20a.html](https://proceedings.mlr.press/v119/evci20a.html)。
- en: 'Evci et al. (2022) Evci, U., Dumoulin, V., Larochelle, H., and Mozer, M. C.
    Head2toe: Utilizing intermediate representations for better transfer learning.
    *ICML 2022, Proceedings of the 39th International Conference on Machine Learning*,
    January 2022.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Evci et al. (2022) Evci, U., Dumoulin, V., Larochelle, H., 和 Mozer, M. C. Head2Toe：利用中间表示来改进迁移学习。*ICML
    2022，第39届国际机器学习会议论文集*，2022年1月。
- en: 'Frankle & Carbin (2018) Frankle, J. and Carbin, M. The lottery ticket hypothesis:
    Finding sparse, trainable neural networks. In *International Conference on Learning
    Representations*, 2018.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle & Carbin (2018) Frankle, J. 和 Carbin, M. 彩票票假设：寻找稀疏的、可训练的神经网络。发表于*国际学习表征会议*，2018年。
- en: 'Frankle et al. (2020) Frankle, J., Schwab, D. J., and Morcos, A. S. Training
    batchnorm and only batchnorm: On the expressive power of random features in cnns.
    February 2020.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle et al. (2020) Frankle, J., Schwab, D. J., 和 Morcos, A. S. 训练批归一化和仅批归一化：关于CNN中随机特征的表达能力。2020年2月。
- en: 'Frantar & Alistarh (2023) Frantar, E. and Alistarh, D. Sparsegpt: Massive language
    models can be accurately pruned in one-shot. In *International Conference on Machine
    Learning*, pp. 10323–10337\. PMLR, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar & Alistarh (2023) Frantar, E. 和 Alistarh, D. SparseGPT：大规模语言模型可以在一次性剪枝中准确剪枝。发表于*国际机器学习会议*，第10323–10337页。PMLR，2023年。
- en: Gale et al. (2019) Gale, T., Elsen, E., and Hooker, S. The state of sparsity
    in deep neural networks. *arXiv preprint arXiv:1902.09574*, 2019.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gale et al. (2019) Gale, T., Elsen, E., 和 Hooker, S. 深度神经网络中的稀疏状态。*arXiv 预印本
    arXiv:1902.09574*，2019年。
- en: Gao et al. (2023) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
    A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot
    language model evaluation, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2023) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
    A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. 一种少样本语言模型评估框架，2023 年
    12 月。网址 [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)。
- en: Giannou et al. (2023) Giannou, A., Rajput, S., and Papailiopoulos, D. The expressive
    power of tuning only the normalization layers. February 2023.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Giannou et al. (2023) Giannou, A., Rajput, S., and Papailiopoulos, D. 仅调整归一化层的表达能力。2023
    年 2 月。
- en: Han et al. (2015) Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights
    and connections for efficient neural networks. In Cortes, C., Lawrence, N., Lee,
    D., Sugiyama, M., and Garnett, R. (eds.), *Advances in Neural Information Processing
    Systems*, volume 28. Curran Associates, Inc., 2015. URL [https://proceedings.neurips.cc/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf](https://proceedings.neurips.cc/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf).
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2015) Han, S., Pool, J., Tran, J., and Dally, W. 学习权重和连接以实现高效神经网络。见于
    Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., 和 Garnett, R.（编），*神经信息处理系统进展*，第
    28 卷。Curran Associates, Inc.，2015 年。网址 [https://proceedings.neurips.cc/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf](https://proceedings.neurips.cc/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf)。
- en: 'Hassibi & Stork (1993) Hassibi, B. and Stork, D. Second order derivatives for
    network pruning: Optimal brain surgeon. In Hanson, S., Cowan, J., and Giles, C.
    (eds.), *Advances in Neural Information Processing Systems*, volume 5\. Morgan-Kaufmann,
    1993. URL [https://proceedings.neurips.cc/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf](https://proceedings.neurips.cc/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi & Stork (1993) Hassibi, B. 和 Stork, D. 网络剪枝的二阶导数：最佳大脑外科医生。见于 Hanson,
    S., Cowan, J., 和 Giles, C.（编），*神经信息处理系统进展*，第 5 卷。Morgan-Kaufmann，1993 年。网址 [https://proceedings.neurips.cc/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf](https://proceedings.neurips.cc/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf)。
- en: 'He et al. (2015) He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into
    rectifiers: Surpassing human-level performance on imagenet classification. In
    *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*,
    December 2015.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2015) He, K., Zhang, X., Ren, S., and Sun, J. 深入探讨整流器：在 imagenet
    分类中超越人类水平的表现。见于 *IEEE 国际计算机视觉会议论文集 (ICCV)*，2015 年 12 月。
- en: 'He et al. (2022) He, S., Ding, L., Dong, D., Zhang, M., and Tao, D. Sparseadapter:
    An easy approach for improving the parameter-efficiency of adapters. October 2022.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2022) He, S., Ding, L., Dong, D., Zhang, M., 和 Tao, D. Sparseadapter：一种提升适配器参数效率的简单方法。2022
    年 10 月。
- en: 'Hoefler et al. (2021) Hoefler, T., Alistarh, D., Ben-Nun, T., Dryden, N., and
    Peste, A. Sparsity in deep learning: Pruning and growth for efficient inference
    and training in neural networks. *arXiv preprint arXiv:2102.00554*, January 2021.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoefler et al. (2021) Hoefler, T., Alistarh, D., Ben-Nun, T., Dryden, N., 和
    Peste, A. 深度学习中的稀疏性：用于神经网络高效推理和训练的剪枝与增长。*arXiv 预印本 arXiv:2102.00554*，2021 年 1
    月。
- en: Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,
    De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient
    transfer learning for nlp. In *International Conference on Machine Learning*,
    pp. 2790–2799\. PMLR, 2019.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,
    De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. 针对 NLP 的参数高效迁移学习。见于
    *国际机器学习会议*，第 2790–2799 页。PMLR，2019 年。
- en: 'Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models.
    June 2021.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., and Chen, W. Lora：大语言模型的低秩适配。2021 年 6 月。
- en: 'Ioffe & Szegedy (2015) Ioffe, S. and Szegedy, C. Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. In Bach, F. R. and
    Blei, D. M. (eds.), *Proceedings of the 32nd International Conference on Machine
    Learning, ICML 2015, Lille, France, 6-11 July 2015*, volume 37 of *JMLR Workshop
    and Conference Proceedings*, pp.  448–456\. JMLR.org, 2015. URL [http://proceedings.mlr.press/v37/ioffe15.html](http://proceedings.mlr.press/v37/ioffe15.html).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe & Szegedy (2015) Ioffe, S. 和 Szegedy, C. 批量归一化：通过减少内部协变量偏移加速深度网络训练。在 Bach,
    F. R. 和 Blei, D. M.（编），*第32届国际机器学习大会（ICML 2015），法国里尔，2015年7月6-11日*，第37卷，*JMLR
    研讨会和会议论文集*，第448–456页。JMLR.org，2015年。网址 [http://proceedings.mlr.press/v37/ioffe15.html](http://proceedings.mlr.press/v37/ioffe15.html)。
- en: 'Jaiswal et al. (2023a) Jaiswal, A., Gan, Z., Du, X., Zhang, B., Wang, Z., and
    Yang, Y. Compressing llms: The truth is rarely pure and never simple. October
    2023a.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaiswal et al. (2023a) Jaiswal, A., Gan, Z., Du, X., Zhang, B., Wang, Z. 和 Yang,
    Y. 压缩llms：真相很少纯粹，且从不简单。2023年10月。
- en: 'Jaiswal et al. (2023b) Jaiswal, A. K., Liu, S., Chen, T., Ding, Y., and Wang,
    Z. Instant soup: Cheap pruning ensembles in a single pass can draw lottery tickets
    from large models. In *International Conference on Machine Learning*, pp. 14691–14701\.
    PMLR, 2023b.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaiswal et al. (2023b) Jaiswal, A. K., Liu, S., Chen, T., Ding, Y. 和 Wang, Z.
    即时剪枝：单次传递中的廉价剪枝集可以从大模型中抽取彩票票据。在 *国际机器学习大会*，第14691–14701页。PMLR，2023年。
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T.,
    Lacroix, T., and Sayed, W. E. Mistral 7b. October 2023.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T.,
    Lacroix, T. 和 Sayed, W. E. Mistral 7b。2023年10月。
- en: 'Kingma & Ba (2014) Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization.
    *arXiv preprint arXiv:1412.6980*, 2014.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma & Ba (2014) Kingma, D. P. 和 Ba, J. Adam: 一种随机优化方法。*arXiv 预印本 arXiv:1412.6980*，2014年。'
- en: Krizhevsky et al. (2012) Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
    classification with deep convolutional neural networks. In Pereira, F., Burges,
    C., Bottou, L., and Weinberger, K. (eds.), *Advances in Neural Information Processing
    Systems*, volume 25\. Curran Associates, Inc., 2012. URL [https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky et al. (2012) Krizhevsky, A., Sutskever, I. 和 Hinton, G. E. 使用深度卷积神经网络进行Imagenet分类。在
    Pereira, F., Burges, C., Bottou, L. 和 Weinberger, K.（编），*神经信息处理系统进展*，第25卷。Curran
    Associates, Inc.，2012年。网址 [https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)。
- en: Kumar et al. (2022a) Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang,
    P. Fine-tuning can distort pretrained features and underperform out-of-distribution.
    February 2022a.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar et al. (2022a) Kumar, A., Raghunathan, A., Jones, R., Ma, T. 和 Liang,
    P. 微调可能扭曲预训练特征并在分布外表现不佳。2022年2月。
- en: Kumar et al. (2022b) Kumar, A., Shen, R., Bubeck, S., and Gunasekar, S. How
    to fine-tune vision models with sgd. November 2022b.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar et al. (2022b) Kumar, A., Shen, R., Bubeck, S. 和 Gunasekar, S. 如何使用sgd微调视觉模型。2022年11月。
- en: Kwon et al. (2022) Kwon, W., Kim, S., Mahoney, M. W., Hassoun, J., Keutzer,
    K., and Gholami, A. A fast post-training pruning framework for transformers. March
    2022.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon et al. (2022) Kwon, W., Kim, S., Mahoney, M. W., Hassoun, J., Keutzer,
    K. 和 Gholami, A. 一种快速的训练后剪枝框架用于变换器。2022年3月。
- en: 'Le & Hua (2021) Le, D. H. and Hua, B.-S. Network pruning that matters: A case
    study on retraining variants. In *International Conference on Learning Representations*,
    2021. URL [https://openreview.net/forum?id=Cb54AMqHQFP](https://openreview.net/forum?id=Cb54AMqHQFP).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le & Hua (2021) Le, D. H. 和 Hua, B.-S. 重要的网络剪枝：关于再训练变体的案例研究。在 *国际学习表征会议*，2021年。网址
    [https://openreview.net/forum?id=Cb54AMqHQFP](https://openreview.net/forum?id=Cb54AMqHQFP)。
- en: LeCun et al. (1989) LeCun, Y., Denker, J. S., and Solla, S. A. Optimal brain
    damage. In Touretzky, D. S. (ed.), *Advances in Neural Information Processing
    Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989]*, pp. 
    598–605\. Morgan Kaufmann, 1989. URL [http://papers.nips.cc/paper/250-optimal-brain-damage](http://papers.nips.cc/paper/250-optimal-brain-damage).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1989) LeCun, Y., Denker, J. S. 和 Solla, S. A. 最优脑损伤。在 Touretzky,
    D. S.（编），*神经信息处理系统进展 2，[NIPS 会议，美国科罗拉多州丹佛，1989年11月27-30日]*，第598–605页。Morgan Kaufmann，1989年。网址
    [http://papers.nips.cc/paper/250-optimal-brain-damage](http://papers.nips.cc/paper/250-optimal-brain-damage)。
- en: Lee et al. (2020) Lee, J., Park, S., Mo, S., Ahn, S., and Shin, J. Layer-adaptive
    sparsity for the magnitude-based pruning. In *International Conference on Learning
    Representations*, October 2020.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 (2020) Lee, J., Park, S., Mo, S., Ahn, S., 和 Shin, J. 基于幅度的剪枝的层自适应稀疏性.
    见 *国际学习表征会议*, 2020年10月.
- en: 'Li et al. (2020) Li, B., Wu, B., Su, J., and Wang, G. Eagleeye: Fast sub-net
    evaluation for efficient neural network pruning. In *Computer Vision–ECCV 2020:
    16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II
    16*, pp.  639–654. Springer, 2020.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2020) Li, B., Wu, B., Su, J., 和 Wang, G. Eagleeye: 高效神经网络剪枝的快速子网络评估.
    见 *计算机视觉–ECCV 2020: 第16届欧洲会议, 英国格拉斯哥, 2020年8月23–28日, 论文集, 第二部分 16*, 页 639–654.
    Springer, 2020.'
- en: Li et al. (2022) Li, Y., Luo, F., Tan, C., Wang, M., Huang, S., Li, S., and
    Bai, J. Parameter-efficient sparsity for large language models fine-tuning. May
    2022.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2022) Li, Y., Luo, F., Tan, C., Wang, M., Huang, S., Li, S., 和 Bai, J.
    大型语言模型微调的参数高效稀疏性. 2022年5月.
- en: 'Lialin et al. (2023a) Lialin, V., Deshpande, V., and Rumshisky, A. Scaling
    down to scale up: A guide to parameter-efficient fine-tuning. March 2023a.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lialin 等人 (2023a) Lialin, V., Deshpande, V., 和 Rumshisky, A. 缩小规模以扩展规模: 参数高效微调指南.
    2023年3月.'
- en: 'Lialin et al. (2023b) Lialin, V., Shivagunde, N., Muckatira, S., and Rumshisky,
    A. Stack more layers differently: High-rank training through low-rank updates.
    July 2023b.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lialin 等人 (2023b) Lialin, V., Shivagunde, N., Muckatira, S., 和 Rumshisky, A.
    以不同方式堆叠更多层: 通过低秩更新实现高秩训练. 2023年7月.'
- en: Lin et al. (2020) Lin, T., Stich, S. U., Barba, L., Dmitriev, D., and Jaggi,
    M. Dynamic model pruning with feedback. In *International Conference on Learning
    Representations*, 2020.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 (2020) Lin, T., Stich, S. U., Barba, L., Dmitriev, D., 和 Jaggi, M. 动态模型剪枝与反馈.
    见 *国际学习表征会议*, 2020.
- en: 'Liu et al. (2021) Liu, B., Cai, Y., Guo, Y., and Chen, X. Transtailor: Pruning
    the pre-trained model for improved transfer learning. March 2021.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 (2021) Liu, B., Cai, Y., Guo, Y., 和 Chen, X. Transtailor: 为改进迁移学习而修剪预训练模型.
    2021年3月.'
- en: 'Liu et al. (2020) Liu, J., Xu, Z., Shi, R., Cheung, R. C. C., and So, H. K.
    Dynamic sparse training: Find efficient sparse network from scratch with trainable
    masked layers. In *International Conference on Learning Representations*, 2020.
    URL [https://openreview.net/forum?id=SJlbGJrtDB](https://openreview.net/forum?id=SJlbGJrtDB).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 (2020) Liu, J., Xu, Z., Shi, R., Cheung, R. C. C., 和 So, H. K. 动态稀疏训练:
    从零开始寻找高效稀疏网络并使用可训练的掩码层. 见 *国际学习表征会议*, 2020. URL [https://openreview.net/forum?id=SJlbGJrtDB](https://openreview.net/forum?id=SJlbGJrtDB).'
- en: Loshchilov & Hutter (2019) Loshchilov, I. and Hutter, F. Decoupled weight decay
    regularization. In *International Conference on Learning Representations*, 2019.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov & Hutter (2019) Loshchilov, I. 和 Hutter, F. 解耦权重衰减正则化. 见 *国际学习表征会议*,
    2019.
- en: Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
    sentinel mixture models. September 2016.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等人 (2016) Merity, S., Xiong, C., Bradbury, J., 和 Socher, R. 指针守卫混合模型.
    2016年9月.
- en: Mihaylov et al. (2018) Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.
    Can a suit of armor conduct electricity? a new dataset for open book question
    answering. September 2018.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov 等人 (2018) Mihaylov, T., Clark, P., Khot, T., 和 Sabharwal, A. 一套盔甲能导电吗？
    一个用于开放书籍问答的新数据集. 2018年9月.
- en: Mishra et al. (2021) Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic,
    D., Venkatesh, G., Yu, C., and Micikevicius, P. Accelerating sparse deep neural
    networks. April 2021.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra 等人 (2021) Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic, D.,
    Venkatesh, G., Yu, C., 和 Micikevicius, P. 加速稀疏深度神经网络. 2021年4月.
- en: Mistral (2023) Mistral, M. A. Mixtral of experts — mistral.ai. [https://mistral.ai/news/mixtral-of-experts/](https://mistral.ai/news/mixtral-of-experts/),
    2023. [Accessed 31-01-2024].
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mistral (2023) Mistral, M. A. 专家混合体 — mistral.ai. [https://mistral.ai/news/mixtral-of-experts/](https://mistral.ai/news/mixtral-of-experts/),
    2023年. [访问日期 2024年1月31日].
- en: 'Mocanu et al. (2018) Mocanu, D. C., Mocanu, E., Stone, P., Nguyen, P. H., Gibescu,
    M., and Liotta, A. Scalable training of artificial neural networks with adaptive
    sparse connectivity inspired by network science. *Nature Communications*, 9(1),
    June 2018. doi: 10.1038/s41467-018-04316-3.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mocanu 等人 (2018) Mocanu, D. C., Mocanu, E., Stone, P., Nguyen, P. H., Gibescu,
    M., 和 Liotta, A. 受网络科学启发的具有自适应稀疏连接的可扩展人工神经网络训练. *自然通讯*, 9(1), 2018年6月. doi: 10.1038/s41467-018-04316-3.'
- en: Molchanov et al. (2016) Molchanov, P., Tyree, S., Karras, T., Aila, T., and
    Kautz, J. Pruning convolutional neural networks for resource efficient inference.
    November 2016.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molchanov 等人 (2016) Molchanov, P., Tyree, S., Karras, T., Aila, T., 和 Kautz,
    J. 为资源高效推断修剪卷积神经网络. 2016年11月.
- en: 'Mudrakarta et al. (2018) Mudrakarta, P. K., Sandler, M., Zhmoginov, A., and
    Howard, A. K for the price of 1: Parameter-efficient multi-task and transfer learning.
    October 2018.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mudrakarta 等人 (2018) Mudrakarta, P. K., Sandler, M., Zhmoginov, A., 和 Howard,
    A. 以 1 的价格得到 K：参数高效的多任务和迁移学习。2018年10月。
- en: Pokutta et al. (2020) Pokutta, S., Spiegel, C., and Zimmer, M. Deep neural network
    training with frank-wolfe. *arXiv preprint arXiv:2010.07243*, 2020.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pokutta 等人 (2020) Pokutta, S., Spiegel, C., 和 Zimmer, M. 使用 Frank-Wolfe 的深度神经网络训练。*arXiv
    预印本 arXiv:2010.07243*，2020年。
- en: Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer
    learning with a unified text-to-text transformer. *The Journal of Machine Learning
    Research*, 21(1):5485–5551, 2020.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
    Matena, M., Zhou, Y., Li, W., 和 Liu, P. J. 探索使用统一的文本到文本变换器的迁移学习极限。*机器学习研究期刊*，21(1):5485–5551，2020年。
- en: Renda et al. (2020) Renda, A., Frankle, J., and Carbin, M. Comparing rewinding
    and fine-tuning in neural network pruning. In *International Conference on Learning
    Representations*, 2020.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Renda 等人 (2020) Renda, A., Frankle, J., 和 Carbin, M. 比较神经网络修剪中的重启和微调。在*国际学习表征会议*，2020年。
- en: 'Russakovsky et al. (2015) Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh,
    S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and
    Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. *International
    Journal of Computer Vision (IJCV)*, 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Russakovsky 等人 (2015) Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh,
    S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., 和
    Fei-Fei, L. ImageNet 大规模视觉识别挑战。*计算机视觉国际期刊 (IJCV)*，115(3):211–252，2015年。doi: 10.1007/s11263-015-0816-y。'
- en: 'Sakaguchi et al. (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. Winogrande: An adversarial winograd schema challenge at scale. *Communications
    of the ACM*, 64(9):99–106, 2021.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi 等人 (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., 和 Choi, Y.
    Winogrande：大规模对抗性 Winograd 模式挑战。*ACM 通讯*，64(9):99–106，2021年。
- en: Sun et al. (2023) Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and
    effective pruning approach for large language models. June 2023.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 (2023) Sun, M., Liu, Z., Bair, A., 和 Kolter, J. Z. 一种简单有效的大型语言模型修剪方法。2023年6月。
- en: Tanaka et al. (2020) Tanaka, H., Kunin, D., Yamins, D. L. K., and Ganguli, S.
    Pruning neural networks without any data by iteratively conserving synaptic flow.
    *Advances in Neural Information Processing Systems 2020*, June 2020.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanaka 等人 (2020) Tanaka, H., Kunin, D., Yamins, D. L. K., 和 Ganguli, S. 通过迭代保持突触流量来修剪神经网络，无需任何数据。*神经信息处理系统进展
    2020*，2020年6月。
- en: 'Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D.,
    Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J.,
    Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M.,
    Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
    Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez,
    A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned
    chat models. July 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D.,
    Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J.,
    Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.
    M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J.
    X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S.,
    Rodriguez, A., Stojnic, R., Edunov, S., 和 Scialom, T. Llama 2：开放基础和微调聊天模型。2023年7月。
- en: 'Vucetic et al. (2022) Vucetic, D., Tayaranian, M., Ziaeefard, M., Clark, J. J.,
    Meyer, B. H., and Gross, W. J. Efficient fine-tuning of bert models on the edge.
    May 2022. doi: 10.1109/ISCAS48785.2022.9937567.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vucetic 等人 (2022) Vucetic, D., Tayaranian, M., Ziaeefard, M., Clark, J. J.,
    Meyer, B. H., 和 Gross, W. J. 在边缘高效微调 BERT 模型。2022年5月。doi: 10.1109/ISCAS48785.2022.9937567。'
- en: 'Wang et al. (2018) Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and
    Bowman, S. R. Glue: A multi-task benchmark and analysis platform for natural language
    understanding. April 2018.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2018) Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., 和
    Bowman, S. R. Glue: 自然语言理解的多任务基准和分析平台。2018年4月。'
- en: Williams & Aletras (2023) Williams, M. and Aletras, N. How does calibration
    data affect the post-training pruning and quantization of large language models?
    November 2023.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams & Aletras (2023) Williams, M. 和 Aletras, N. 校准数据如何影响大型语言模型的训练后修剪和量化？2023年11月。
- en: 'Wolf et al. (2020) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
    Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer,
    S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger,
    S., Drame, M., Lhoest, Q., and Rush, A. Transformers: State-of-the-art natural
    language processing. In *Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing: System Demonstrations*, pp.  38–45, Online, October
    2020\. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6.
    URL [https://aclanthology.org/2020.emnlp-demos.6](https://aclanthology.org/2020.emnlp-demos.6).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf et al. (2020) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
    Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer,
    S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger,
    S., Drame, M., Lhoest, Q., 和 Rush, A. Transformers: 最新的自然语言处理技术。见于 *2020年自然语言处理经验方法会议论文集：系统演示*，第38–45页，在线，2020年10月。计算语言学协会。doi:
    10.18653/v1/2020.emnlp-demos.6。网址 [https://aclanthology.org/2020.emnlp-demos.6](https://aclanthology.org/2020.emnlp-demos.6)。'
- en: Wolfe et al. (2021) Wolfe, C. R., Wang, Q., Kim, J. L., and Kyrillidis, A. How
    much pre-training is enough to discover a good subnetwork? July 2021.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolfe et al. (2021) Wolfe, C. R., Wang, Q., Kim, J. L., 和 Kyrillidis, A. 多少预训练才足以发现一个好的子网络？2021年7月。
- en: Wortsman et al. (2019) Wortsman, M., Farhadi, A., and Rastegari, M. Discovering
    neural wirings. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc,
    F., Fox, E., and Garnett, R. (eds.), *Advances in Neural Information Processing
    Systems*, volume 32\. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper/2019/file/d010396ca8abf6ead8cacc2c2f2f26c7-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/d010396ca8abf6ead8cacc2c2f2f26c7-Paper.pdf).
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wortsman et al. (2019) Wortsman, M., Farhadi, A., 和 Rastegari, M. 发现神经网络连接。见于
    Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., 和 Garnett,
    R. (编)，*神经信息处理系统进展*，第32卷。Curran Associates, Inc.，2019年。网址 [https://proceedings.neurips.cc/paper/2019/file/d010396ca8abf6ead8cacc2c2f2f26c7-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/d010396ca8abf6ead8cacc2c2f2f26c7-Paper.pdf)。
- en: 'Yeom et al. (2019) Yeom, S.-K., Seegerer, P., Lapuschkin, S., Binder, A., Wiedemann,
    S., Müller, K.-R., and Samek, W. Pruning by explaining: A novel criterion for
    deep neural network pruning. December 2019.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeom et al. (2019) Yeom, S.-K., Seegerer, P., Lapuschkin, S., Binder, A., Wiedemann,
    S., Müller, K.-R., 和 Samek, W. 通过解释进行修剪：一种用于深度神经网络修剪的新标准。2019年12月。
- en: 'Yin et al. (2023) Yin, L., Wu, Y., Zhang, Z., Hsieh, C.-Y., Wang, Y., Jia,
    Y., Pechenizkiy, M., Liang, Y., Wang, Z., and Liu, S. Outlier weighed layerwise
    sparsity (owl): A missing secret sauce for pruning llms to high sparsity. October
    2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2023) Yin, L., Wu, Y., Zhang, Z., Hsieh, C.-Y., Wang, Y., Jia, Y.,
    Pechenizkiy, M., Liang, Y., Wang, Z., 和 Liu, S. 异常权重层稀疏性 (owl)：修剪大语言模型至高稀疏度的缺失秘密调料。2023年10月。
- en: 'You et al. (2020) You, H., Li, C., Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk,
    R. G., Wang, Z., and Lin, Y. Drawing early-bird tickets: Toward more efficient
    training of deep networks. In *International Conference on Learning Representations*,
    2020. URL [https://openreview.net/forum?id=BJxsrgStvr](https://openreview.net/forum?id=BJxsrgStvr).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You et al. (2020) You, H., Li, C., Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk,
    R. G., Wang, Z., 和 Lin, Y. 提前抽取票据：迈向更高效的深度网络训练。见于 *国际学习表征会议*，2020年。网址 [https://openreview.net/forum?id=BJxsrgStvr](https://openreview.net/forum?id=BJxsrgStvr)。
- en: 'Zaken et al. (2021) Zaken, E. B., Ravfogel, S., and Goldberg, Y. Bitfit: Simple
    parameter-efficient fine-tuning for transformer-based masked language-models.
    June 2021.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zaken et al. (2021) Zaken, E. B., Ravfogel, S., 和 Goldberg, Y. Bitfit: 用于基于变换器的掩码语言模型的简单参数高效微调。2021年6月。'
- en: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. Hellaswag: Can a machine really finish your sentence? May 2019.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., 和 Choi,
    Y. Hellaswag: 机器真的能完成你的句子吗？2019年5月。'
- en: Zhang et al. (2016) Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals,
    O. Understanding deep learning requires rethinking generalization. *arXiv preprint
    arXiv:1611.03530*, November 2016.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2016) Zhang, C., Bengio, S., Hardt, M., Recht, B., 和 Vinyals,
    O. 理解深度学习需要重新思考泛化问题。*arXiv预印本 arXiv:1611.03530*，2016年11月。
- en: Zhang et al. (2023a) Zhang, M., Chen, H., Shen, C., Yang, Z., Ou, L., Yu, X.,
    and Zhuang, B. Pruning meets low-rank parameter-efficient fine-tuning. May 2023a.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2023a）Zhang, M., Chen, H., Shen, C., Yang, Z., Ou, L., Yu, X., 和 Zhuang,
    B. 《修剪遇到低秩参数高效微调》。2023年5月。
- en: 'Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer,
    S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,
    L. Opt: Open pre-trained transformer language models. May 2022.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等（2022）Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen,
    S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer,
    S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., 和 Zettlemoyer,
    L. 《Opt: 开放预训练变换器语言模型》。2022年5月。'
- en: 'Zhang et al. (2023b) Zhang, Y., Zhao, L., Lin, M., Sun, Y., Yao, Y., Han, X.,
    Tanner, J., Liu, S., and Ji, R. Dynamic sparse no training: Training-free fine-tuning
    for sparse llms. October 2023b.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2023b）Zhang, Y., Zhao, L., Lin, M., Sun, Y., Yao, Y., Han, X., Tanner,
    J., Liu, S., 和 Ji, R. 《动态稀疏无训练：无训练微调稀疏LLMs》。2023年10月。
- en: 'Zhang et al. (2021) Zhang, Z., Chen, X., Chen, T., and Wang, Z. Efficient lottery
    ticket finding: Less data is more. In Meila, M. and Zhang, T. (eds.), *Proceedings
    of the 38th International Conference on Machine Learning*, volume 139 of *Proceedings
    of Machine Learning Research*, pp.  12380–12390\. PMLR, 18–24 Jul 2021. URL [https://proceedings.mlr.press/v139/zhang21c.html](https://proceedings.mlr.press/v139/zhang21c.html).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2021）Zhang, Z., Chen, X., Chen, T., 和 Wang, Z. 《高效彩票票据发现：数据更少效果更好》。在Meila,
    M. 和 Zhang, T.（编辑），*第38届国际机器学习会议论文集*，*机器学习研究论文集*第139卷，第12380–12390页。PMLR，2021年7月18–24日。网址
    [https://proceedings.mlr.press/v139/zhang21c.html](https://proceedings.mlr.press/v139/zhang21c.html)。
- en: 'Zhu & Gupta (2017) Zhu, M. and Gupta, S. To prune, or not to prune: Exploring
    the efficacy of pruning for model compression. *arXiv preprint arXiv:1710.01878*,
    October 2017.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu & Gupta（2017）Zhu, M. 和 Gupta, S. 《修剪，还是不修剪：探索修剪在模型压缩中的有效性》。*arXiv预印本 arXiv:1710.01878*，2017年10月。
- en: Zimmer et al. (2022) Zimmer, M., Spiegel, C., and Pokutta, S. Compression-aware
    training of neural networks using frank-wolfe. *arXiv preprint arXiv:2205.11921*,
    2022.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zimmer等（2022）Zimmer, M., Spiegel, C., 和 Pokutta, S. 《使用Frank-Wolfe的神经网络压缩感知训练》。*arXiv预印本
    arXiv:2205.11921*，2022年。
- en: Zimmer et al. (2023) Zimmer, M., Spiegel, C., and Pokutta, S. How I Learned
    To Stop Worrying And Love Retraining. In *International Conference on Learning
    Representations*, 2023. URL [https://openreview.net/forum?id=_nF5imFKQI](https://openreview.net/forum?id=_nF5imFKQI).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zimmer等（2023）Zimmer, M., Spiegel, C., 和 Pokutta, S. 《我如何学会停止担忧并爱上再训练》。发表于*国际学习表征会议*，2023年。网址
    [https://openreview.net/forum?id=_nF5imFKQI](https://openreview.net/forum?id=_nF5imFKQI)。
- en: 'Zimmer et al. (2024) Zimmer, M., Spiegel, C., and Pokutta, S. Sparse model
    soups: A recipe for improved pruning via model averaging. In *International Conference
    on Learning Representations*, 2024. URL [https://openreview.net/forum?id=xx0ITyHp3u](https://openreview.net/forum?id=xx0ITyHp3u).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zimmer等（2024）Zimmer, M., Spiegel, C., 和 Pokutta, S. 《稀疏模型汤：通过模型平均改进修剪的配方》。发表于*国际学习表征会议*，2024年。网址
    [https://openreview.net/forum?id=xx0ITyHp3u](https://openreview.net/forum?id=xx0ITyHp3u)。
- en: Appendix A Technical details and training settings
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 技术细节和训练设置
- en: A.1 Pretraining
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 预训练
- en: Training settings and metrics.
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练设置和指标。
- en: 'For NLP tasks, we use pretrained models from Huggingface and specify only the
    retraining settings as outlined in [Section 2.3](#S2.SS3 "2.3 Experimental setup
    ‣ 2 Methodology and Experimental Setup ‣ PERP: Rethinking the Prune-Retrain Paradigm
    in the Era of LLMs"). For computer vision tasks, we perform the pretraining process
    ourselves. [Table 3](#A1.T3 "Table 3 ‣ Training settings and metrics. ‣ A.1 Pretraining
    ‣ Appendix A Technical details and training settings ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs") details our pretraining configurations, including
    the number of epochs, batch size, weight decay, and learning rate. We opt for
    SGD as the optimizer, though we recognize a range of other optimization methods
    are available (see e.g., Kingma & Ba, [2014](#bib.bib27); Pokutta et al., [2020](#bib.bib50)).
    We maintain the default momentum value of 0.9\. In the last column of the table
    we report the performance achieved with standard dense training, using top-1 test
    accuracy as the metric for image classification tasks, which denotes the percentage
    of test samples correctly classified.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 NLP 任务，我们使用 Huggingface 的预训练模型，只指定如[第 2.3 节](#S2.SS3 "2.3 Experimental setup
    ‣ 2 Methodology and Experimental Setup ‣ PERP: Rethinking the Prune-Retrain Paradigm
    in the Era of LLMs")中概述的再训练设置。对于计算机视觉任务，我们自己执行预训练过程。[表 3](#A1.T3 "Table 3 ‣ Training
    settings and metrics. ‣ A.1 Pretraining ‣ Appendix A Technical details and training
    settings ‣ PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs")详细说明了我们的预训练配置，包括训练轮次、批量大小、权重衰减和学习率。我们选择
    SGD 作为优化器，尽管我们认识到还有其他优化方法可用（见如 Kingma & Ba，[2014](#bib.bib27)；Pokutta 等，[2020](#bib.bib50)）。我们保持默认的动量值
    0.9。在表的最后一列中，我们报告了使用标准密集训练所获得的性能，以 top-1 测试准确率作为图像分类任务的指标，这表示测试样本正确分类的百分比。'
- en: 'Table 3: Exact pretraining configurations in our vision experiments.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：我们视觉实验中的确切预训练配置。
- en: '| Dataset | Network (number of weights) | Epochs | Batch size | Weight decay
    | Learning rate ($t$ = training epoch) | Unpruned test accuracy |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 网络（权重数量） | 轮次 | 批量大小 | 权重衰减 | 学习率（$t$ = 训练轮次） | 未剪枝测试准确率 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| ImageNet | ResNet-50 (26 Mio) | 90 | 256 | 1e-4 | linear from 0.1 to 0 |
    76.12% ±0.01% |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet | ResNet-50（26 Mio） | 90 | 256 | 1e-4 | 从 0.1 线性衰减到 0 | 76.12% ±0.01%
    |'
- en: A.2 Pruning and Retraining
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 剪枝和再训练
- en: Pruning settings.
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 剪枝设置。
- en: Effective pruning relies on the accurate identification of weights to prune
    and the distribution of sparsity among the layers. Zhu & Gupta ([2017](#bib.bib74))
    introduced the Uniform allocation, pruning each layer by the same relative amount.
    Gale et al. ([2019](#bib.bib13)) improved this with Uniform+, keeping the first
    convolutional layer dense and limiting pruning in the final fully-connected layer
    to 80%. Evci et al. ([2020](#bib.bib8)) adapted the Erdős-R’enyi Kernel (ERK)
    (Mocanu et al., [2018](#bib.bib47)) for layerwise sparsity, accounting for layer
    dimensions. Lee et al. ([2020](#bib.bib34)) proposed Layer-Adaptive Magnitude-based
    Pruning (LAMP), targeting minimal output distortion at pruning, assessed through
    $L_{2}$-distortion on worst-case inputs.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的剪枝依赖于对要剪枝的权重的准确识别以及各层之间稀疏性的分布。Zhu & Gupta（[2017](#bib.bib74)）引入了均匀分配，通过相同的相对数量剪枝每一层。Gale
    等（[2019](#bib.bib13)）通过 Uniform+ 改进了这一方法，保持第一卷积层的密集，并将最后一个全连接层的剪枝限制在 80%。Evci
    等（[2020](#bib.bib8)）将 Erdős-R’enyi 核（ERK）（Mocanu 等，[2018](#bib.bib47)）调整为逐层稀疏性，考虑到层的维度。Lee
    等（[2020](#bib.bib34)）提出了层自适应幅度剪枝（LAMP），旨在剪枝时最小化输出失真，通过 $L_{2}$-失真在最坏情况下输入进行评估。
- en: In NLP, following Sun et al. ([2023](#bib.bib55)), we prune all linear layers
    except embeddings and the final classification head, applying uniform sparsity
    throughout. For a comparison of diverse selection schemes for LLMs, see Yin et al.
    ([2023](#bib.bib65)). Our experiments include both unstructured sparsity and semi-structured
    2:4 and 4:8 sparsities. In vision tasks, aligning with Zimmer et al. ([2023](#bib.bib76));
    Evci et al. ([2020](#bib.bib8)); Dettmers & Zettlemoyer ([2019](#bib.bib5)), we
    prune everything except biases and BN parameters, employing the Global criterion
    which treats all parameters as a single vector and computes a universal threshold
    for parameter removal.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NLP 中，遵循 Sun 等（[2023](#bib.bib55)），我们剪枝所有线性层，除了嵌入层和最终分类头，应用均匀稀疏性。有关 LLM 的多样化选择方案的比较，请参见
    Yin 等（[2023](#bib.bib65)）。我们的实验包括无结构稀疏性和半结构化的 2:4 和 4:8 稀疏性。在视觉任务中，与 Zimmer 等（[2023](#bib.bib76)）；Evci
    等（[2020](#bib.bib8)）；Dettmers & Zettlemoyer（[2019](#bib.bib5)）一致，我们剪枝除偏置和 BN 参数外的所有内容，采用全局标准，该标准将所有参数视为一个单一向量，并计算用于参数移除的通用阈值。
- en: 'Hyperparameters for Retraining: The Learning Rate.'
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重新训练的超参数：学习率。
- en: In computer vision, automating the learning rate schedule for retraining has
    received increased interest, aiming to circumvent the need for tuning the schedule
    in each phase. We describe various schedules where $T$ to zero in each cycle.
    For vision tasks, we adopt ALLR as recommended by Zimmer et al. ([2023](#bib.bib76)),
    using a linear schedule that adjusts the initial rate based on the impact of pruning
    and available retraining time, balancing cycle length and pruning-induced performance
    degradation.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，自动化重新训练的学习率调度受到了越来越多的关注，旨在避免在每个阶段都需要调整调度。我们描述了各种调度，其中每个周期中的 $T$ 逐渐减少为零。对于视觉任务，我们采用
    Zimmer 等人推荐的 ALLR 方法 ([2023](#bib.bib76))，使用线性调度根据剪枝的影响和可用的重新训练时间调整初始学习率，平衡周期长度和剪枝引起的性能退化。
- en: For LLMs, we stick to AdamW with a linear learning rate decay from a tuned initial
    value. We experiment with starting values 5e-6, 1e-5, 5e-5, 1e-4 and 5e-4.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 LLM，我们坚持使用 AdamW 并从调整后的初始值开始线性学习率衰减。我们尝试了起始值 5e-6、1e-5、5e-5、1e-4 和 5e-4。
- en: 'Hyperparameters for Retraining: Batch size and Weight decay.'
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重新训练的超参数：批量大小和权重衰减。
- en: For vision, we retain the same batch size and weight decay parameters as used
    in pretraining. However, for LLMs we set the weight decay to zero and found no
    improvement in increasing this value. We use a batch size of 2 and gradient accumulation
    for 4 steps for all models with less than 30 billion parameters. For larger models,
    we use a batch size of 1 and 2 gradient accumulation steps. We use gradient checkpointing
    to reduce the memory demands at the expense of efficiency.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于视觉任务，我们保持与预训练中使用的相同批量大小和权重衰减参数。然而，对于 LLM，我们将权重衰减设置为零，并发现增加该值并没有改进。我们对所有参数少于
    300 亿的模型使用批量大小为 2 和 4 步梯度累积。对于更大的模型，我们使用批量大小为 1 和 2 步梯度累积。我们使用梯度检查点来减少内存需求，但效率会有所下降。
- en: LoRA for convolutions and pruned layers.
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LoRA 用于卷积和剪枝层。
- en: To apply LoRA to a convolutional tensor $W\in\mathbb{R}^{n\times c\times d\times
    d}$. However, this is not necessarily the case for structured pruning.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 将 LoRA 应用于卷积张量 $W\in\mathbb{R}^{n\times c\times d\times d}$。然而，这对于结构化剪枝并不一定适用。
- en: In the case of convolutional filter pruning, we could simply apply LoRA to the
    non-pruned segment of the convolutional tensor. Pruning filters equates to zeroing
    entire rows of $W$ separately—the latter producing only zeros—and reorder the
    outputs to match the original row or filter sequence. This reparametrization is
    easily reversed after retraining, ensuring consistency with the tensor’s original
    layout. A similar argument can be made for semi-structured sparsities such as
    2:4 and 4:8.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积滤波器剪枝的情况下，我们可以简单地将 LoRA 应用于未剪枝的卷积张量部分。剪枝滤波器相当于单独将 $W$ 的整行置零——后者只产生零——并重新排列输出以匹配原始行或滤波器序列。这种重新参数化在重新训练后很容易被逆转，从而确保与张量原始布局的一致性。类似的论点也适用于半结构化稀疏性，如
    2:4 和 4:8。
- en: Appendix B Additional experiments
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 额外实验
- en: 'In this section, we provide additional results, following the same structure
    as [Section 3](#S3 "3 Parameter-Efficient Retraining ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs").'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了额外的结果，结构与 [第 3 节](#S3 "3 参数高效重新训练 ‣ PERP：在 LLM 时代重新思考剪枝-重新训练范式") 相同。
- en: B.1 Restoring feature quality with few parameters
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 用少量参数恢复特征质量
- en: '[Figure 4](#A2.F4 "Figure 4 ‣ B.1 Restoring feature quality with few parameters
    ‣ Appendix B Additional experiments ‣ PERP: Rethinking the Prune-Retrain Paradigm
    in the Era of LLMs") compares the different approaches on ResNet-50 on ImageNet.
    As opposed to [Figure 2](#S2.F2 "Figure 2 ‣ 2.3 Experimental setup ‣ 2 Methodology
    and Experimental Setup ‣ PERP: Rethinking the Prune-Retrain Paradigm in the Era
    of LLMs"), we perform five retraining epochs instead of a single one.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4](#A2.F4 "图 4 ‣ B.1 用少量参数恢复特征质量 ‣ 附录 B 额外实验 ‣ PERP：在 LLM 时代重新思考剪枝-重新训练范式")
    比较了不同方法在 ImageNet 上的 ResNet-50 表现。与 [图 2](#S2.F2 "图 2 ‣ 2.3 实验设置 ‣ 2 方法论与实验设置
    ‣ PERP：在 LLM 时代重新思考剪枝-重新训练范式") 相比，我们进行五次重新训练周期，而不是仅进行一次。'
- en: '![Refer to caption](img/5bfd9b5e720b17a586f422008f2d9148.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/5bfd9b5e720b17a586f422008f2d9148.png)'
- en: 'Figure 4: ResNet-50 on ImageNet: Test accuracy across sparsity levels for One
    Shot pruning with five retraining epochs.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：ResNet-50 在 ImageNet 上的表现：对不同稀疏水平进行一次性剪枝后的测试准确率，采用五次重新训练周期。
- en: B.2 Efficient Retraining of Large Models
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 大模型的高效重新训练
- en: '[Table 4](#A2.T4 "Table 4 ‣ B.2 Efficient Retraining of Large Models ‣ Appendix
    B Additional experiments ‣ PERP: Rethinking the Prune-Retrain Paradigm in the
    Era of LLMs"), [Table 5](#A2.T5 "Table 5 ‣ B.2 Efficient Retraining of Large Models
    ‣ Appendix B Additional experiments ‣ PERP: Rethinking the Prune-Retrain Paradigm
    in the Era of LLMs"), [Table 6](#A2.T6 "Table 6 ‣ B.2 Efficient Retraining of
    Large Models ‣ Appendix B Additional experiments ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs"), [Table 7](#A2.T7 "Table 7 ‣ B.2 Efficient Retraining
    of Large Models ‣ Appendix B Additional experiments ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs") and [Table 8](#A2.T8 "Table 8 ‣ B.2 Efficient Retraining
    of Large Models ‣ Appendix B Additional experiments ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs") shows the full results of the parameter-efficient
    retraining approaches for OPT, LLaMA-2, Mistral and Mixtral models, which were
    omitted from the main part of our work. For LLaMA-70B and Mixtral-8x7B, we use
    a smaller range of sparsities. Note that for all models except the OPT-family,
    biases are disabled by default and we consequently not include the method *Biases*.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[表4](#A2.T4 "表4 ‣ B.2 大型模型的高效再训练 ‣ 附录B 额外实验 ‣ PERP：在LLM时代重新思考剪枝-再训练范式")、[表5](#A2.T5
    "表5 ‣ B.2 大型模型的高效再训练 ‣ 附录B 额外实验 ‣ PERP：在LLM时代重新思考剪枝-再训练范式")、[表6](#A2.T6 "表6 ‣
    B.2 大型模型的高效再训练 ‣ 附录B 额外实验 ‣ PERP：在LLM时代重新思考剪枝-再训练范式")、[表7](#A2.T7 "表7 ‣ B.2 大型模型的高效再训练
    ‣ 附录B 额外实验 ‣ PERP：在LLM时代重新思考剪枝-再训练范式") 和 [表8](#A2.T8 "表8 ‣ B.2 大型模型的高效再训练 ‣ 附录B
    额外实验 ‣ PERP：在LLM时代重新思考剪枝-再训练范式") 展示了OPT、LLaMA-2、Mistral和Mixtral模型的参数高效再训练方法的完整结果，这些结果在本文的主要部分被省略了。对于LLaMA-70B和Mixtral-8x7B，我们使用了较小范围的稀疏性。请注意，除OPT系列模型外，所有模型默认禁用偏差，因此我们没有包括*偏差*方法。'
- en: 'Table 4: OPT-1.3B/2.7B/13B/30B: Parameter-efficient retraining approaches vs.
    full retraining with 30%-70% of the parameters pruned. The first column lists
    the method, and the second shows the percentage of trainable parameters (Full
    IMP represents the standard retraining baseline). The next five columns display
    the average mean perplexity (lower is better) across multiple seeds, with standard
    deviations excluded for clarity. The unpruned model attains a perplexity of 14.62,
    12.47, 10.12 and 9.55 for OPT-1.3B/2.7B/13B/30B, respectively.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：OPT-1.3B/2.7B/13B/30B：参数高效再训练方法与30%-70%参数剪枝的完整再训练方法对比。第一列列出方法，第二列显示可训练参数的百分比（Full
    IMP表示标准再训练基准）。接下来的五列显示多个种子的平均困惑度（值越低越好），为清晰起见不包括标准差。未剪枝模型在OPT-1.3B/2.7B/13B/30B中的困惑度分别为14.62、12.47、10.12和9.55。
- en: '| OPT-1.3B |  |  |  |  |  |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3B |  |  |  |  |  |  |'
- en: '| Perplexity: 14.62 |  | Sparsity |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度：14.62 |  | 稀疏性 |'
- en: '| Method | % trainable | 30% | 40% | 50% | 60% | 70% |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 可训练百分比 | 30% | 40% | 50% | 60% | 70% |'
- en: '| Full IMP | 100% | 15.92 | 16.94 | 18.56 | 23.43 | 33.60 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 完整IMP | 100% | 15.92 | 16.94 | 18.56 | 23.43 | 33.60 |'
- en: '| PERP | 0.35% | 15.97 | 16.94 | 18.60 | 23.28 | 34.32 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| PERP | 0.35% | 15.97 | 16.94 | 18.60 | 23.28 | 34.32 |'
- en: '| Linear Probing | 0.11% | 16.01 | 17.37 | 19.29 | 26.00 | 49.60 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 线性探测 | 0.11% | 16.01 | 17.37 | 19.29 | 26.00 | 49.60 |'
- en: '| LN-Parameters | 0.05% | 15.91 | 17.27 | 19.43 | 25.91 | 49.24 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| LN-参数 | 0.05% | 15.91 | 17.27 | 19.43 | 25.91 | 49.24 |'
- en: '| Biases | 0.03% | 16.05 | 17.60 | 20.21 | 28.21 | 62.03 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 偏差 | 0.03% | 16.05 | 17.60 | 20.21 | 28.21 | 62.03 |'
- en: '| No Retraining | 0.00% | 24.74 | 387.76 | 1713.30 | 9390.83 | 9441.80 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 无再训练 | 0.00% | 24.74 | 387.76 | 1713.30 | 9390.83 | 9441.80 |'
- en: '| OPT-2.7B |  |  |  |  |  |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7B |  |  |  |  |  |  |'
- en: '| Perplexity: 12.47 |  | Sparsity |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度：12.47 |  | 稀疏性 |'
- en: '| Method | % trainable | 30% | 40% | 50% | 60% | 70% |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 可训练百分比 | 30% | 40% | 50% | 60% | 70% |'
- en: '| Full IMP | 100% | 13.47 | 14.31 | 15.85 | 19.54 | 28.37 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 完整IMP | 100% | 13.47 | 14.31 | 15.85 | 19.54 | 28.37 |'
- en: '| PERP | 0.27% | 13.42 | 14.50 | 16.38 | 19.20 | 27.01 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| PERP | 0.27% | 13.42 | 14.50 | 16.38 | 19.20 | 27.01 |'
- en: '| Linear Probing | 0.07% | 13.47 | 14.71 | 17.10 | 21.33 | 35.75 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 线性探测 | 0.07% | 13.47 | 14.71 | 17.10 | 21.33 | 35.75 |'
- en: '| LN-Parameters | 0.04% | 13.53 | 14.71 | 16.66 | 21.12 | 34.39 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| LN-参数 | 0.04% | 13.53 | 14.71 | 16.66 | 21.12 | 34.39 |'
- en: '| Biases | 0.03% | 13.58 | 14.84 | 16.86 | 22.07 | 39.57 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 偏差 | 0.03% | 13.58 | 14.84 | 16.86 | 22.07 | 39.57 |'
- en: '| No Retraining | 0.00% | 15.58 | 30.32 | 265.19 | 3604.16 | 7251.81 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 无再训练 | 0.00% | 15.58 | 30.32 | 265.19 | 3604.16 | 7251.81 |'
- en: '| OPT-13B |  |  |  |  |  |  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13B |  |  |  |  |  |  |'
- en: '| Perplexity: 10.12 |  | Sparsity |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度：10.12 |  | 稀疏性 |'
- en: '| Method | % trainable | 30% | 40% | 50% | 60% | 70% |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 可训练百分比 | 30% | 40% | 50% | 60% | 70% |'
- en: '| PERP | 0.13% | 10.91 | 11.79 | 13.29 | 14.81 | 18.70 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| PERP | 0.13% | 10.91 | 11.79 | 13.29 | 14.81 | 18.70 |'
- en: '| Linear Probing | 0.03% | 10.94 | 11.88 | 13.43 | 15.83 | 21.02 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 线性探测 | 0.03% | 10.94 | 11.88 | 13.43 | 15.83 | 21.02 |'
- en: '| LN-Parameters | 0.02% | 10.87 | 11.79 | 13.41 | 15.75 | 20.89 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| LN-参数 | 0.02% | 10.87 | 11.79 | 13.41 | 15.75 | 20.89 |'
- en: '| Biases | 0.01% | 11.02 | 11.88 | 13.63 | 16.48 | 23.68 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 偏置 | 0.01% | 11.02 | 11.88 | 13.63 | 16.48 | 23.68 |'
- en: '| No retraining | 0.00% | 13.40 | 99.26 | 11591.62 | 576372.38 | 290838.78
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 无再训练 | 0.00% | 13.40 | 99.26 | 11591.62 | 576372.38 | 290838.78 |'
- en: '| OPT-30B |  |  |  |  |  |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30B |  |  |  |  |  |  |'
- en: '| Perplexity: 9.55 |  | Sparsity |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度：9.55 |  | 稀疏度 |'
- en: '| Method | % trainable | 30% | 40% | 50% | 60% | 70% |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 可训练参数百分比 | 30% | 40% | 50% | 60% | 70% |'
- en: '| PERP | 0.09% | 10.43 | 11.42 | 12.29 | 14.50 | 21.66 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| PERP | 0.09% | 10.43 | 11.42 | 12.29 | 14.50 | 21.66 |'
- en: '| Linear Probing | 0.02% | 10.31 | 11.49 | 12.80 | 15.75 | 54.26 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 线性探测 | 0.02% | 10.31 | 11.49 | 12.80 | 15.75 | 54.26 |'
- en: '| LN-Parameters | 0.01% | 10.37 | 11.43 | 12.82 | 15.75 | 43.06 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| LN-参数 | 0.01% | 10.37 | 11.43 | 12.82 | 15.75 | 43.06 |'
- en: '| Biases | 0.01% | 10.41 | 11.49 | 13.80 | 17.00 | 408.04 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 偏置 | 0.01% | 10.41 | 11.49 | 13.80 | 17.00 | 408.04 |'
- en: '| No retraining | 0.00% | 12.37 | 24.29 | 168.07 | 11675.34 | 28170.72 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 无再训练 | 0.00% | 12.37 | 24.29 | 168.07 | 11675.34 | 28170.72 |'
- en: 'Table 5: LLaMA-7B/13B: Parameter-efficient retraining approaches with 30%-70%
    of the parameters pruned. The first column lists the method, and the second shows
    the percentage of trainable parameters. The next five columns display the average
    mean perplexity (lower is better) across multiple seeds, with standard deviations
    excluded for clarity. The unpruned model attains a perplexity of 5.11, 4.57 for
    LLaMA-7B/13B, respectively.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：LLaMA-7B/13B：参数高效再训练方法，修剪了30%-70%的参数。第一列列出了方法，第二列显示了可训练参数的百分比。接下来的五列显示了多个种子的平均困惑度（数值越低越好），为了清晰起见，标准差被排除在外。未修剪模型的困惑度为5.11，LLaMA-7B/13B分别为4.57。
- en: '| LLaMA-7B |  |  |  |  |  |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B |  |  |  |  |  |  |'
- en: '| Perplexity: 5.11 |  | Sparsity |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度：5.11 |  | 稀疏度 |'
- en: '| Method | % trainable | 30% | 40% | 50% | 60% | 70% |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 可训练参数百分比 | 30% | 40% | 50% | 60% | 70% |'
- en: '| PERP | 0.60% | 5.35 | 5.67 | 6.32 | 7.62 | 10.60 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| PERP | 0.60% | 5.35 | 5.67 | 6.32 | 7.62 | 10.60 |'
- en: '| Linear Probing | 0.01% | 5.43 | 5.89 | 6.90 | 9.62 | 20.45 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 线性探测 | 0.01% | 5.43 | 5.89 | 6.90 | 9.62 | 20.45 |'
- en: '| LN-Parameters | 0.00% | 5.48 | 5.98 | 7.57 | 10.53 | 47.58 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| LN-参数 | 0.00% | 5.48 | 5.98 | 7.57 | 10.53 | 47.58 |'
- en: '| No retraining | 0.00% | 5.79 | 7.31 | 14.90 | 3677.83 | 52432.10 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 无再训练 | 0.00% | 5.79 | 7.31 | 14.90 | 3677.83 | 52432.10 |'
- en: '| LLaMA-13B |  |  |  |  |  |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-13B |  |  |  |  |  |  |'
- en: '| Perplexity: 4.57 |  | Sparsity |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度：4.57 |  | 稀疏度 |'
- en: '| Method | % trainable | 30% | 40% | 50% | 60% | 70% |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 可训练参数百分比 | 30% | 40% | 50% | 60% | 70% |'
- en: '| PERP | 0.49% | 4.75 | 4.97 | 5.42 | 6.38 | 8.62 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| PERP | 0.49% | 4.75 | 4.97 | 5.42 | 6.38 | 8.62 |'
- en: '| Linear Probing | 0.01% | 4.77 | 5.04 | 5.64 | 7.28 | 11.31 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 线性探测 | 0.01% | 4.77 | 5.04 | 5.64 | 7.28 | 11.31 |'
- en: '| LN-Parameters | 0.00% | 4.77 | 5.05 | 5.65 | 7.33 | 11.42 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| LN-参数 | 0.00% | 4.77 | 5.05 | 5.65 | 7.33 | 11.42 |'
- en: '| No retraining | 0.00% | 4.82 | 5.26 | 6.37 | 11.22 | 275.20 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 无再训练 | 0.00% | 4.82 | 5.26 | 6.37 | 11.22 | 275.20 |'
- en: 'Table 6: LLaMA-70B: Parameter-efficient retraining approaches with 40%-60%
    of the parameters pruned. The first column lists the method, and the second shows
    the percentage of trainable parameters. The next five columns display the average
    mean perplexity (lower is better) and standard deviation across multiple seeds.
    The unpruned model attains a perplexity of 3.12.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：LLaMA-70B：参数高效再训练方法，修剪了40%-60%的参数。第一列列出了方法，第二列显示了可训练参数的百分比。接下来的五列显示了多个种子的平均困惑度（数值越低越好）和标准差。未修剪模型的困惑度为3.12。
- en: '| LLaMA-70B |  |  |  |  |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-70B |  |  |  |  |'
- en: '| Perplexity: 3.12 |  | Sparsity |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度：3.12 |  | 稀疏度 |'
- en: '| Method | % trainable | 40% | 50% | 60% |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 可训练参数百分比 | 40% | 50% | 60% |'
- en: '| PERP | 0.30% | 3.49 ±0.00 | 3.89 ±0.00 | 4.57 ±0.00 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| PERP | 0.30% | 3.49 ±0.00 | 3.89 ±0.00 | 4.57 ±0.00 |'
- en: '| Linear Probing | 0.00% | 3.54 ±0.00 | 4.01 ±0.00 | 4.83 ±0.01 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 线性探测 | 0.00% | 3.54 ±0.00 | 4.01 ±0.00 | 4.83 ±0.01 |'
- en: '| LN-Parameters | 0.00% | 3.54 ±0.00 | 4.04 ±0.00 | 4.95 ±0.01 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| LN-参数 | 0.00% | 3.54 ±0.00 | 4.04 ±0.00 | 4.95 ±0.01 |'
- en: '| No retraining | 0.00% | 3.84 ±0.00 | 4.99 ±0.00 | 8.20 ±0.00 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 无再训练 | 0.00% | 3.84 ±0.00 | 4.99 ±0.00 | 8.20 ±0.00 |'
- en: 'Table 7: Mistral-7B: Parameter-efficient retraining approaches with 30%-70%
    of the parameters pruned. The first column lists the method, and the second shows
    the percentage of trainable parameters. The next five columns display the average
    mean perplexity (lower is better) across multiple seeds, with standard deviations
    excluded for clarity. The unpruned model attains a perplexity of 4.69.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：Mistral-7B：参数高效再训练方法，修剪了30%-70%的参数。第一列列出了方法，第二列显示了可训练参数的百分比。接下来的五列显示了多个种子的平均困惑度（数值越低越好），为了清晰起见，标准差被排除在外。未修剪模型的困惑度为4.69。
- en: '| Mistral-7B |  |  |  |  |  |  |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B |  |  |  |  |  |  |'
- en: '| Perplexity: 4.69 |  | Sparsity |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度: 4.69 |  | 稀疏度 |'
- en: '| Method | % trainable | 30% | 40% | 50% | 60% | 70% |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 可训练比例 | 30% | 40% | 50% | 60% | 70% |'
- en: '| PERP | 0.59% | 4.95 | 5.24 | 5.88 | 7.63 | 12.64 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| PERP | 0.59% | 4.95 | 5.24 | 5.88 | 7.63 | 12.64 |'
- en: '| Linear Probing | 0.01% | 5.00 | 5.46 | 6.85 | 16.70 | 25071.97 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 线性探测 | 0.01% | 5.00 | 5.46 | 6.85 | 16.70 | 25071.97 |'
- en: '| LN-Parameters | 0.00% | 4.98 | 5.45 | 6.99 | 20.42 | 24993.66 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| LN-参数 | 0.00% | 4.98 | 5.45 | 6.99 | 20.42 | 24993.66 |'
- en: '| No retraining | 0.00% | 5.02 | 5.55 | 7.92 | 224.22 | 68142.05 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 无重训练 | 0.00% | 5.02 | 5.55 | 7.92 | 224.22 | 68142.05 |'
- en: 'Table 8: Mixtral-8x7B: Parameter-efficient retraining approaches with 40%-60%
    of the parameters pruned. The first column lists the method, and the second shows
    the percentage of trainable parameters. The next five columns display the average
    mean perplexity (lower is better) and standard deviation across multiple seeds.
    The unpruned model attains a perplexity of 3.35.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: Mixtral-8x7B: 参数高效的重训练方法，修剪了 40%-60% 的参数。第一列列出方法，第二列显示可训练参数的百分比。接下来的五列显示多个种子下的平均困惑度（数值越低越好）和标准差。未经修剪的模型达到的困惑度为
    3.35。'
- en: '| Mixtral-8x7B |  |  |  |  |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B |  |  |  |  |'
- en: '| Perplexity: 3.35 |  | Sparsity |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度: 3.35 |  | 稀疏度 |'
- en: '| Method | % trainable | 40% | 50% | 60% |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 可训练比例 | 40% | 50% | 60% |'
- en: '| PERP | 0.03% | 4.04 ±0.00 | 4.59 ±0.01 | 5.79 ±0.02 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| PERP | 0.03% | 4.04 ±0.00 | 4.59 ±0.01 | 5.79 ±0.02 |'
- en: '| Linear Probing | 0.00% | 4.56 ±0.00 | 6.16 ±0.01 | 19.39 ±0.25 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 线性探测 | 0.00% | 4.56 ±0.00 | 6.16 ±0.01 | 19.39 ±0.25 |'
- en: '| LN-Parameters | 0.00% | 4.57 ±0.00 | 6.29 ±0.00 | 18.83 ±0.06 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| LN-参数 | 0.00% | 4.57 ±0.00 | 6.29 ±0.00 | 18.83 ±0.06 |'
- en: '| No retraining | 0.00% | 4.88 ±0.00 | 8.30 ±0.00 | 77.09 ±0.00 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 无重训练 | 0.00% | 4.88 ±0.00 | 8.30 ±0.00 | 77.09 ±0.00 |'
- en: B.3 Reconsidering Magnitude Pruning of LLMs
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 重新考虑 LLMs 的幅度修剪
- en: '[Table 9](#A2.T9 "Table 9 ‣ B.3 Reconsidering Magnitude Pruning of LLMs ‣ Appendix
    B Additional experiments ‣ PERP: Rethinking the Prune-Retrain Paradigm in the
    Era of LLMs") and [Table 10](#A2.T10 "Table 10 ‣ B.3 Reconsidering Magnitude Pruning
    of LLMs ‣ Appendix B Additional experiments ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs") show the full results when comparing magnitude pruning,
    Wanda and SparseGPT with or without PERP on OPT-2.7B/6.7B/13B/30B/66B, LLaMA-7B/13B
    and Mistral-7B. Surprisingly, for OPT-66B, we see slightly different behaviour
    then on OPT-30B: magnitude pruning with PERP is able to outperform Wanda with
    PERP in certain setting. On the other hand, we note that there exist settings
    where Wanda has higher perplexity than Magnitude when not performing retraining,
    but outperforming it after retraining. [Table 11](#A2.T11 "Table 11 ‣ B.3 Reconsidering
    Magnitude Pruning of LLMs ‣ Appendix B Additional experiments ‣ PERP: Rethinking
    the Prune-Retrain Paradigm in the Era of LLMs") compares the final accuracy of
    these methods on the EleutherAI evaluation set, consisting of seven different
    tasks, namely: BoolQ (Clark et al., [2019](#bib.bib3)), RTE (Wang et al., [2018](#bib.bib59)),
    HellaSwag (Zellers et al., [2019](#bib.bib68)), WinoGrande (Sakaguchi et al.,
    [2021](#bib.bib54)), ARC Easy, ARC Challenge (Clark et al., [2018](#bib.bib4)),
    and OpenbookQA (Mihaylov et al., [2018](#bib.bib44)). We report the mean accuracy
    over these tasks. The central observation is here that PERP is able to improve
    the accuracy of the pruning approaches in all cases.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 9](#A2.T9 "Table 9 ‣ B.3 Reconsidering Magnitude Pruning of LLMs ‣ Appendix
    B Additional experiments ‣ PERP: Rethinking the Prune-Retrain Paradigm in the
    Era of LLMs") 和 [表 10](#A2.T10 "Table 10 ‣ B.3 Reconsidering Magnitude Pruning
    of LLMs ‣ Appendix B Additional experiments ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs") 显示了在比较幅度修剪、Wanda 和 SparseGPT 是否使用 PERP 的全面结果，涉及
    OPT-2.7B/6.7B/13B/30B/66B、LLaMA-7B/13B 和 Mistral-7B。令人惊讶的是，对于 OPT-66B，我们观察到与 OPT-30B
    略有不同的行为：在某些设置下，使用 PERP 的幅度修剪能够超越使用 PERP 的 Wanda。另一方面，我们注意到存在 Wanda 在不进行重训练时具有更高困惑度，但在重训练后超越它的情况。[表
    11](#A2.T11 "Table 11 ‣ B.3 Reconsidering Magnitude Pruning of LLMs ‣ Appendix
    B Additional experiments ‣ PERP: Rethinking the Prune-Retrain Paradigm in the
    Era of LLMs") 比较了这些方法在 EleutherAI 评估集上的最终准确性，该评估集包含七个不同的任务，即：BoolQ (Clark et al.,
    [2019](#bib.bib3))、RTE (Wang et al., [2018](#bib.bib59))、HellaSwag (Zellers et
    al., [2019](#bib.bib68))、WinoGrande (Sakaguchi et al., [2021](#bib.bib54))、ARC
    Easy、ARC Challenge (Clark et al., [2018](#bib.bib4)) 和 OpenbookQA (Mihaylov et
    al., [2018](#bib.bib44))。我们报告了这些任务的平均准确性。关键观察是，PERP 能够在所有情况下提高修剪方法的准确性。'
- en: 'Table 9: OPT-2.7B/6.7B/13B/30B/66B: Perplexity comparison of magnitude pruning,
    Wanda and SparseGPT, either with our without PERP, both in the unstructured pruning
    setting (50% sparsity), as well as for the semi-structured 2:4 and 4:8 sparsities.
    All methods using PERP (indicated in the second column) are retrained for 1000
    iterations and further highlighted in bold. We report the mean perplexity over
    several seeds and omit the standard deviation for the sake of clarity.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 9: OPT-2.7B/6.7B/13B/30B/66B：Magnitude 剪枝、Wanda 和 SparseGPT 的困惑度比较，无论是否使用
    PERP，均在非结构化剪枝设置（50% 稀疏度）以及半结构化 2:4 和 4:8 稀疏度下进行。所有使用 PERP（在第二列中标记）的算法都进行了 1000
    次迭代的再训练，并以**粗体**突出显示。我们报告了多个种子的平均困惑度，并为清晰起见省略了标准差。'
- en: '|  |  |  | OPT |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | OPT |'
- en: '| --- | --- | --- | --- |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Method | PERP | Sparsity | 2.7B | 6.7B | 13B | 30B | 66B |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| Method | PERP | Sparsity | 2.7B | 6.7B | 13B | 30B | 66B |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Baseline | ✗ | 0% | 12.47 | 10.86 | 10.12 | 9.55 | 9.33 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| Baseline | ✗ | 0% | 12.47 | 10.86 | 10.12 | 9.55 | 9.33 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Magnitude | ✗ | 50% | 265.19 | 968.80 | 11568.33 | 168.07 | 4230.77 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✗ | 50% | 265.19 | 968.80 | 11568.33 | 168.07 | 4230.77 |'
- en: '| Magnitude | ✓ | 50% | 15.76 | 13.60 | 12.54 | 11.94 | 12.17 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✓ | 50% | 15.76 | 13.60 | 12.54 | 11.94 | 12.17 |'
- en: '| Wanda | ✗ | 50% | 14.35 | 12.04 | 11.98 | 10.07 | 3730.34 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✗ | 50% | 14.35 | 12.04 | 11.98 | 10.07 | 3730.34 |'
- en: '| Wanda | ✓ | 50% | 13.88 | 11.83 | 11.06 | 10.04 | 13.91 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✓ | 50% | 13.88 | 11.83 | 11.06 | 10.04 | 13.91 |'
- en: '| SparseGPT | ✗ | 50% | 13.47 | 11.59 | 11.22 | 9.78 | 9.32 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✗ | 50% | 13.47 | 11.59 | 11.22 | 9.78 | 9.32 |'
- en: '| SparseGPT | ✓ | 50% | 13.40 | 11.47 | 10.85 | 9.76 | 9.41 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✓ | 50% | 13.40 | 11.47 | 10.85 | 9.76 | 9.41 |'
- en: '| Magnitude | ✗ | 2:4 | 1152.89 | 264.09 | 484.74 | 1979.66 | 6934.42 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✗ | 2:4 | 1152.89 | 264.09 | 484.74 | 1979.66 | 6934.42 |'
- en: '| Magnitude | ✓ | 2:4 | 17.90 | 14.31 | 13.14 | 12.27 | 49.58 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✓ | 2:4 | 17.90 | 14.31 | 13.14 | 12.27 | 49.58 |'
- en: '| Wanda | ✗ | 2:4 | 21.33 | 16.03 | 15.70 | 13.26 | 11703.72 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✗ | 2:4 | 21.33 | 16.03 | 15.70 | 13.26 | 11703.72 |'
- en: '| Wanda | ✓ | 2:4 | 16.29 | 13.48 | 12.03 | 10.92 | 19.58 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✓ | 2:4 | 16.29 | 13.48 | 12.03 | 10.92 | 19.58 |'
- en: '| SparseGPT | ✗ | 2:4 | 17.26 | 14.23 | 12.92 | 10.94 | 10.13 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✗ | 2:4 | 17.26 | 14.23 | 12.92 | 10.94 | 10.13 |'
- en: '| SparseGPT | ✓ | 2:4 | 15.23 | 12.70 | 11.56 | 10.39 | 9.91 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✓ | 2:4 | 15.23 | 12.70 | 11.56 | 10.39 | 9.91 |'
- en: '| Magnitude | ✗ | 4:8 | 166.92 | 196.17 | 449.64 | 563.84 | 6736.84 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✗ | 4:8 | 166.92 | 196.17 | 449.64 | 563.84 | 6736.84 |'
- en: '| Magnitude | ✓ | 4:8 | 16.43 | 13.77 | 12.57 | 12.06 | 20.34 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✓ | 4:8 | 16.43 | 13.77 | 12.57 | 12.06 | 20.34 |'
- en: '| Wanda | ✗ | 4:8 | 16.85 | 13.63 | 13.47 | 10.88 | 8743.59 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✗ | 4:8 | 16.85 | 13.63 | 13.47 | 10.88 | 8743.59 |'
- en: '| Wanda | ✓ | 4:8 | 14.99 | 12.53 | 11.41 | 10.58 | 16.87 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✓ | 4:8 | 14.99 | 12.53 | 11.41 | 10.58 | 16.87 |'
- en: '| SparseGPT | ✗ | 4:8 | 15.08 | 12.62 | 11.80 | 10.34 | 9.64 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✗ | 4:8 | 15.08 | 12.62 | 11.80 | 10.34 | 9.64 |'
- en: '| SparseGPT | ✓ | 4:8 | 14.23 | 11.98 | 11.03 | 10.13 | 9.62 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✓ | 4:8 | 14.23 | 11.98 | 11.03 | 10.13 | 9.62 |'
- en: 'Table 10: LLaMA-7B/13B and Mistral-7B: Perplexity comparison of magnitude pruning,
    Wanda and SparseGPT, either with our without PERP, both in the unstructured pruning
    setting (50% sparsity), as well as for the semi-structured 2:4 and 4:8 sparsities.
    All methods using PERP (indicated in the second column) are retrained for 1000
    iterations and further highlighted in bold. We report the mean perplexity over
    several seeds and omit the standard deviation for the sake of clarity.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 10: LLaMA-7B/13B 和 Mistral-7B：Magnitude 剪枝、Wanda 和 SparseGPT 的困惑度比较，无论是否使用
    PERP，均在非结构化剪枝设置（50% 稀疏度）以及半结构化 2:4 和 4:8 稀疏度下进行。所有使用 PERP（在第二列中标记）的算法都进行了 1000
    次迭代的再训练，并以**粗体**突出显示。我们报告了多个种子的平均困惑度，并为清晰起见省略了标准差。'
- en: '|  |  |  | LLaMA | Mistral |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | LLaMA | Mistral |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Method | PERP | Sparsity | 7B | 13B | 7B |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| Method | PERP | Sparsity | 7B | 13B | 7B |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Baseline | ✗ | 0% | 5.11 | 4.57 | 4.69 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| Baseline | ✗ | 0% | 5.11 | 4.57 | 4.69 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Magnitude | ✗ | 50% | 14.90 | 6.37 | 7.92 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✗ | 50% | 14.90 | 6.37 | 7.92 |'
- en: '| Magnitude | ✓ | 50% | 6.32 | 5.42 | 5.82 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✓ | 50% | 6.32 | 5.42 | 5.82 |'
- en: '| Wanda | ✗ | 50% | 6.46 | 5.59 | 6.16 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✗ | 50% | 6.46 | 5.59 | 6.16 |'
- en: '| Wanda | ✓ | 50% | 6.02 | 5.31 | 5.46 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✓ | 50% | 6.02 | 5.31 | 5.46 |'
- en: '| SparseGPT | ✗ | 50% | 6.52 | 5.64 | 6.09 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✗ | 50% | 6.52 | 5.64 | 6.09 |'
- en: '| SparseGPT | ✓ | 50% | 6.04 | 5.33 | 5.56 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✓ | 50% | 6.04 | 5.33 | 5.56 |'
- en: '| Magnitude | ✗ | 2:4 | 54.39 | 8.32 | 22.63 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✗ | 2:4 | 54.39 | 8.32 | 22.63 |'
- en: '| Magnitude | ✓ | 2:4 | 7.46 | 6.29 | 7.70 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✓ | 2:4 | 7.46 | 6.29 | 7.70 |'
- en: '| Wanda | ✗ | 2:4 | 11.36 | 8.35 | 12.24 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✗ | 2:4 | 11.36 | 8.35 | 12.24 |'
- en: '| Wanda | ✓ | 2:4 | 7.17 | 6.18 | 6.58 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✓ | 2:4 | 7.17 | 6.18 | 6.58 |'
- en: '| SparseGPT | ✗ | 2:4 | 10.22 | 8.26 | 10.12 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✗ | 2:4 | 10.22 | 8.26 | 10.12 |'
- en: '| SparseGPT | ✓ | 2:4 | 7.09 | 6.18 | 6.78 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✓ | 2:4 | 7.09 | 6.18 | 6.78 |'
- en: '| Magnitude | ✗ | 4:8 | 16.53 | 6.76 | 10.82 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✗ | 4:8 | 16.53 | 6.76 | 10.82 |'
- en: '| Magnitude | ✓ | 4:8 | 6.83 | 5.81 | 6.82 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✓ | 4:8 | 6.83 | 5.81 | 6.82 |'
- en: '| Wanda | ✗ | 4:8 | 8.07 | 6.55 | 7.90 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✗ | 4:8 | 8.07 | 6.55 | 7.90 |'
- en: '| Wanda | ✓ | 4:8 | 6.57 | 5.77 | 6.01 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✓ | 4:8 | 6.57 | 5.77 | 6.01 |'
- en: '| SparseGPT | ✗ | 4:8 | 7.93 | 6.55 | 7.84 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✗ | 4:8 | 7.93 | 6.55 | 7.84 |'
- en: '| SparseGPT | ✓ | 4:8 | 6.54 | 5.74 | 6.15 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✓ | 4:8 | 6.54 | 5.74 | 6.15 |'
- en: 'Table 11: OPT-2.7B/6.7B/13B/30B: Accuracy comparison on the EleutherAI evaluation
    set. We report magnitude pruning, Wanda and SparseGPT, either with our without
    PERP, both in the unstructured pruning setting (50% sparsity), as well as for
    the semi-structured 2:4 and 4:8 sparsities. All methods using PERP (indicated
    in the second column) are retrained for 1000 iterations and further highlighted
    in bold. We report the mean accuracy over 7 tasks and over several seeds, omitting
    the standard deviation for the sake of clarity. Note that as opposed to the perplexity
    metric, higher accuracy is better.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：OPT-2.7B/6.7B/13B/30B：在 EleutherAI 评估集上的准确率比较。我们报告了大小修剪、Wanda 和 SparseGPT，不论是否使用
    PERP，均在非结构化修剪设置（50% 稀疏性）以及半结构化 2:4 和 4:8 稀疏性下进行。所有使用 PERP 的方法（在第二列中标出）都进行了 1000
    次迭代再训练，并进一步以粗体突出显示。我们报告了 7 个任务及多个种子的平均准确率，为了清晰起见省略了标准差。请注意，与困惑度指标不同，更高的准确率表示更好。
- en: '|  |  |  | OPT |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | OPT |'
- en: '| --- | --- | --- | --- |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Method | PERP | Sparsity | 2.7B | 6.7B | 13B | 30B |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PERP | 稀疏性 | 2.7B | 6.7B | 13B | 30B |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Magnitude | ✗ | 50% | 40.07% | 35.56% | 33.68% | 36.45% |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✗ | 50% | 40.07% | 35.56% | 33.68% | 36.45% |'
- en: '| Magnitude | ✓ | 50% | 45.35% | 49.81% | 50.67% | 52.14% |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✓ | 50% | 45.35% | 49.81% | 50.67% | 52.14% |'
- en: '| Wanda | ✗ | 50% | 45.72% | 49.10% | 51.24% | 53.47% |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✗ | 50% | 45.72% | 49.10% | 51.24% | 53.47% |'
- en: '| Wanda | ✓ | 50% | 46.99% | 50.38% | 51.68% | 53.76% |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✓ | 50% | 46.99% | 50.38% | 51.68% | 53.76% |'
- en: '| SparseGPT | ✗ | 50% | 46.73% | 50.23% | 51.37% | 54.25% |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✗ | 50% | 46.73% | 50.23% | 51.37% | 54.25% |'
- en: '| SparseGPT | ✓ | 50% | 47.22% | 51.04% | 52.04% | 54.26% |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✓ | 50% | 47.22% | 51.04% | 52.04% | 54.26% |'
- en: '| Magnitude | ✗ | 2:4 | 35.83% | 36.33% | 36.60% | 34.90% |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✗ | 2:4 | 35.83% | 36.33% | 36.60% | 34.90% |'
- en: '| Magnitude | ✓ | 2:4 | 44.25% | 48.53% | 49.66% | 49.23% |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✓ | 2:4 | 44.25% | 48.53% | 49.66% | 49.23% |'
- en: '| Wanda | ✗ | 2:4 | 42.85% | 46.14% | 47.66% | 49.32% |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✗ | 2:4 | 42.85% | 46.14% | 47.66% | 49.32% |'
- en: '| Wanda | ✓ | 2:4 | 45.14% | 48.89% | 50.26% | 51.92% |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✓ | 2:4 | 45.14% | 48.89% | 50.26% | 51.92% |'
- en: '| SparseGPT | ✗ | 2:4 | 43.83% | 47.01% | 49.08% | 51.04% |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✗ | 2:4 | 43.83% | 47.01% | 49.08% | 51.04% |'
- en: '| SparseGPT | ✓ | 2:4 | 45.55% | 49.06% | 51.02% | 52.78% |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✓ | 2:4 | 45.55% | 49.06% | 51.02% | 52.78% |'
- en: '| Magnitude | ✗ | 4:8 | 36.97% | 36.91% | 36.09% | 36.92% |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✗ | 4:8 | 36.97% | 36.91% | 36.09% | 36.92% |'
- en: '| Magnitude | ✓ | 4:8 | 44.34% | 49.55% | 50.27% | 51.73% |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | ✓ | 4:8 | 44.34% | 49.55% | 50.27% | 51.73% |'
- en: '| Wanda | ✗ | 4:8 | 43.90% | 47.49% | 49.13% | 51.39% |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✗ | 4:8 | 43.90% | 47.49% | 49.13% | 51.39% |'
- en: '| Wanda | ✓ | 4:8 | 46.02% | 49.52% | 50.77% | 53.25% |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✓ | 4:8 | 46.02% | 49.52% | 50.77% | 53.25% |'
- en: '| SparseGPT | ✗ | 4:8 | 45.41% | 48.20% | 50.24% | 52.59% |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✗ | 4:8 | 45.41% | 48.20% | 50.24% | 52.59% |'
- en: '| SparseGPT | ✓ | 4:8 | 46.98% | 50.26% | 51.56% | 53.55% |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✓ | 4:8 | 46.98% | 50.26% | 51.56% | 53.55% |'
- en: 'B.4 Magnitude conservation: Restabilizing the network'
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4 大小保存：重新稳定网络
- en: 'Table 12: ResNet-50 on ImageNet: Iterative parameter-efficient retraining targeting
    90% sparsity, with 2 to 5 prune-retrain cycles. The first two columns indicate
    whether we freeze (✗) or unfreeze (✓) the model during each cycle, and the direction
    of this process (output-to-input (✓) or input-to-output (✗)). The third column
    shows the total aggregated percentage of trainable parameters. The subsequent
    four columns present the average mean test accuracy deviation from full IMP across
    various seeds, omitting standard deviations for brevity.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：ResNet-50 在 ImageNet 上的性能：目标为 90% 稀疏性的迭代参数高效再训练，包括 2 至 5 次修剪-再训练周期。前两列表示在每个周期中我们是否冻结（✗）或解冻（✓）模型，以及该过程的方向（输出到输入（✓）或输入到输出（✗））。第三列显示了可训练参数的总累计百分比。接下来的四列呈现了各个种子下的平均测试准确率偏差，相较于完整
    IMP，省略了标准差以简洁。
- en: '| ImageNet |  |  |  |  |  |  |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet |  |  |  |  |  |  |'
- en: '|  |  |  | # Prune-Retrain cycles |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | # 修剪-再训练周期 |'
- en: '| unfreeze | reverse | % agg. trainable | 2 | 3 | 4 | 5 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 解冻 | 反向 | % 可训练 | 2 | 3 | 4 | 5 |'
- en: '| ✓ | ✓ | 88.83% | -0.27% | -0.09% | -0.07% | -0.46% |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | 88.83% | -0.27% | -0.09% | -0.07% | -0.46% |'
- en: '| ✗ | ✗ | 70.95% | -1.07% | -0.67% | -0.66% | -0.91% |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✗ | 70.95% | -1.07% | -0.67% | -0.66% | -0.91% |'
- en: '| ✓ | ✗ | 37.28% | -2.47% | -2.28% | -1.91% | -2.02% |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✗ | 37.28% | -2.47% | -2.28% | -1.91% | -2.02% |'
- en: '| ✗ | ✓ | 19.40% | -7.16% | -6.98% | -6.67% | -6.59% |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✓ | 19.40% | -7.16% | -6.98% | -6.67% | -6.59% |'
- en: Appendix C Ablation studies
  id: totrans-396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 消融研究
- en: 'C.1 Ablation: Dissecting the impact of parameter groups for LLMs'
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 消融：分析参数组对LLMs的影响
- en: 'In [Table 13](#A3.T13 "Table 13 ‣ C.1 Ablation: Dissecting the impact of parameter
    groups for LLMs ‣ Appendix C Ablation studies ‣ PERP: Rethinking the Prune-Retrain
    Paradigm in the Era of LLMs"), we analyze the effect of different parameter groups
    when retraining OPT-13B, magnitude-pruned to 50% and 70% sparsity, for 1000 iterations.
    We tuned the learning rate and report the best mean perplexity across multiple
    random seeds, along with the standard deviation. The last two columns show the
    proportion of trainable parameters and final test perplexity, respectively. The
    first four columns indicate whether individual parameter groups are active (✓)
    or inactive (✗), specifically Biases (excluding LN-biases), LN parameters, the
    linear head, and added LoRA parameters.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [表 13](#A3.T13 "表 13 ‣ C.1 消融：分析参数组对LLMs的影响 ‣ 附录 C 消融研究 ‣ PERP：在LLMs时代重新思考剪枝-重新训练范式")
    中，我们分析了在对OPT-13B进行1000次迭代的重新训练时，不同参数组的效果。我们调整了学习率，并报告了多个随机种子的最佳平均困惑度及其标准偏差。最后两列分别显示了可训练参数的比例和最终测试困惑度。前四列指示了各个参数组是否处于活动状态（✓）或非活动状态（✗），具体包括偏差（不包括LN偏差）、LN参数、线性头部和添加的LoRA参数。
- en: Our findings include several key points. Retraining solely the linear head,
    though significantly reducing perplexity, is outperformed by other methods. LoRA,
    representing the largest set of trainable parameters, most effectively lowers
    perplexity. However, retraining biases or LN parameters also notably decreases
    perplexity, utilizing far fewer parameters than LoRA. Combining all methods typically
    delivers the best outcomes, with Biases and LN showing diminishing returns. While
    the linear head alone is less efficient, it aids in further perplexity reduction.
    Biases and LN parameters are impactful given their minimal parameter count; for
    optimal performance, we suggest employing all highlighted parameter groups.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '-   我们的发现包括几个关键点。仅重新训练线性头部虽然显著降低了困惑度，但其效果不如其他方法。LoRA作为可训练参数最多的方案，最有效地降低了困惑度。然而，重新训练偏差或LN参数也显著降低了困惑度，并且使用的参数比LoRA少得多。综合使用所有方法通常能取得最佳效果，偏差和LN显示出递减的回报。虽然仅有线性头部的效率较低，但它有助于进一步降低困惑度。考虑到其参数数量很少，偏差和LN参数的影响很大；为了获得最佳性能，我们建议使用所有高亮的参数组。'
- en: 'Table 13: OPT-13B: The subtables report One Shot parameter-efficient retraining
    for 50% (above) and 70% (below) sparsity levels. The first four columns specify
    whether certain model parameter subgroups are inactive (✗) or active (✓), representing
    the training of (non-LN) biases, Layernorm parameters, the linear head, and low-rank
    adapters, respectively. The penultimate column presents the percentage of trainable
    parameters in each configuration. The final column displays the mean perplexity
    across seeds and its standard deviation.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：OPT-13B：子表报告了在50%（上方）和70%（下方）稀疏度下的一次性参数高效重新训练。前四列指定了某些模型参数子组是否处于非活动状态（✗）或活动状态（✓），分别表示（非LN）偏差、层归一化参数、线性头部和低秩适配器的训练。倒数第二列展示了每种配置中的可训练参数百分比。最后一列显示了跨种子的平均困惑度及其标准偏差。
- en: '| 50% |  |  |  |  |  |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 50% |  |  |  |  |  |'
- en: '| Biases | LN | Head | LoRA | % trainable | Perplexity |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 偏差 | LN | 头部 | LoRA | 可训练百分比 | 困惑度 |'
- en: '| ✗ | ✗ | ✗ | ✗ | 0.00% | 11591.62 ±0.00 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✗ | ✗ | ✗ | 0.00% | 11591.62 ±0.00 |'
- en: '| ✓ | ✗ | ✗ | ✗ | 0.01% | 13.66 ±0.01 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✗ | ✗ | ✗ | 0.01% | 13.66 ±0.01 |'
- en: '| ✗ | ✓ | ✗ | ✗ | 0.01% | 13.88 ±0.05 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✓ | ✗ | ✗ | 0.01% | 13.88 ±0.05 |'
- en: '| ✗ | ✗ | ✓ | ✗ | 0.01% | 3749.34 ±7.34 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✗ | ✓ | ✗ | 0.01% | 3749.34 ±7.34 |'
- en: '| ✗ | ✗ | ✗ | ✓ | 0.41% | 12.59 ±0.08 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✗ | ✗ | ✓ | 0.41% | 12.59 ±0.08 |'
- en: '| ✓ | ✓ | ✗ | ✗ | 0.02% | 13.36 ±0.00 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✗ | ✗ | 0.02% | 13.36 ±0.00 |'
- en: '| ✓ | ✗ | ✓ | ✗ | 0.02% | 13.70 ±0.01 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✗ | ✓ | ✗ | 0.02% | 13.70 ±0.01 |'
- en: '| ✓ | ✗ | ✗ | ✓ | 0.42% | 12.65 ±0.05 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✗ | ✗ | ✓ | 0.42% | 12.65 ±0.05 |'
- en: '| ✗ | ✓ | ✓ | ✗ | 0.01% | 14.04 ±0.03 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✓ | ✓ | ✗ | 0.01% | 14.04 ±0.03 |'
- en: '| ✗ | ✓ | ✗ | ✓ | 0.41% | 12.65 ±0.05 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✓ | ✗ | ✓ | 0.41% | 12.65 ±0.05 |'
- en: '| ✗ | ✗ | ✓ | ✓ | 0.41% | 12.64 ±0.01 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✗ | ✓ | ✓ | 0.41% | 12.64 ±0.01 |'
- en: '| ✓ | ✓ | ✓ | ✗ | 0.03% | 13.50 ±0.03 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✓ | ✗ | 0.03% | 13.50 ±0.03 |'
- en: '| ✓ | ✓ | ✗ | ✓ | 0.43% | 12.76 ±0.05 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✗ | ✓ | 0.43% | 12.76 ±0.05 |'
- en: '| ✓ | ✗ | ✓ | ✓ | 0.43% | 12.69 ±0.04 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✗ | ✓ | ✓ | 0.43% | 12.69 ±0.04 |'
- en: '| ✗ | ✓ | ✓ | ✓ | 0.42% | 12.66 ±0.09 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✓ | ✓ | ✓ | 0.42% | 12.66 ±0.09 |'
- en: '| ✓ | ✓ | ✓ | ✓ | 0.43% | 12.70 ±0.03 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✓ | ✓ | 0.43% | 12.70 ±0.03 |'
- en: '| 70% |  |  |  |  |  |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 70% |  |  |  |  |  |'
- en: '| Biases | LN | Head | LoRA | % trainable | Perplexity |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 偏差 | LN | Head | LoRA | % 可训练 | 困惑度 |'
- en: '| ✗ | ✗ | ✗ | ✗ | 0.00% | 290838.78 ±0.00 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✗ | ✗ | ✗ | 0.00% | 290838.78 ±0.00 |'
- en: '| ✓ | ✗ | ✗ | ✗ | 0.01% | 24.92 ±0.09 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✗ | ✗ | ✗ | 0.01% | 24.92 ±0.09 |'
- en: '| ✗ | ✓ | ✗ | ✗ | 0.01% | 47.62 ±9.05 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✓ | ✗ | ✗ | 0.01% | 47.62 ±9.05 |'
- en: '| ✗ | ✗ | ✓ | ✗ | 0.01% | 9413.92 ±134.20 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✗ | ✓ | ✗ | 0.01% | 9413.92 ±134.20 |'
- en: '| ✗ | ✗ | ✗ | ✓ | 0.41% | 17.60 ±0.02 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✗ | ✗ | ✓ | 0.41% | 17.60 ±0.02 |'
- en: '| ✓ | ✓ | ✗ | ✗ | 0.02% | 22.28 ±0.69 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✗ | ✗ | 0.02% | 22.28 ±0.69 |'
- en: '| ✓ | ✗ | ✓ | ✗ | 0.02% | 25.59 ±1.52 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✗ | ✓ | ✗ | 0.02% | 25.59 ±1.52 |'
- en: '| ✓ | ✗ | ✗ | ✓ | 0.42% | 17.60 ±0.02 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✗ | ✗ | ✓ | 0.42% | 17.60 ±0.02 |'
- en: '| ✗ | ✓ | ✓ | ✗ | 0.01% | 93.26 ±77.07 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✓ | ✓ | ✗ | 0.01% | 93.26 ±77.07 |'
- en: '| ✗ | ✓ | ✗ | ✓ | 0.41% | 17.56 ±0.17 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✓ | ✗ | ✓ | 0.41% | 17.56 ±0.17 |'
- en: '| ✗ | ✗ | ✓ | ✓ | 0.41% | 17.53 ±0.10 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✗ | ✓ | ✓ | 0.41% | 17.53 ±0.10 |'
- en: '| ✓ | ✓ | ✓ | ✗ | 0.03% | 21.81 ±0.39 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✓ | ✗ | 0.03% | 21.81 ±0.39 |'
- en: '| ✓ | ✓ | ✗ | ✓ | 0.43% | 17.65 ±0.02 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✗ | ✓ | 0.43% | 17.65 ±0.02 |'
- en: '| ✓ | ✗ | ✓ | ✓ | 0.43% | 18.10 ±0.68 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✗ | ✓ | ✓ | 0.43% | 18.10 ±0.68 |'
- en: '| ✗ | ✓ | ✓ | ✓ | 0.42% | 17.65 ±0.03 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✓ | ✓ | ✓ | 0.42% | 17.65 ±0.03 |'
- en: '| ✓ | ✓ | ✓ | ✓ | 0.43% | 17.37 ±0.13 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✓ | ✓ | 0.43% | 17.37 ±0.13 |'
- en: 'C.2 Ablation: The impact of LoRA hyperparameters'
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 消融实验：LoRA 超参数的影响
- en: 'As opposed to methods that activate specific parameter groups, LoRA adds new
    trainable parameters and two hyperparameters: rank $r$ seem to have minimal impact
    on final perplexity, even though rank directly affects the number of trainable
    parameters. We adjusted the learning rate schedule and the number of training
    iterations (between 100 and 500) and report the best mean perplexity across multiple
    random seeds.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 与激活特定参数组的方法不同，LoRA 添加了新的可训练参数和两个超参数：秩 $r$ 对最终困惑度的影响似乎很小，尽管秩直接影响可训练参数的数量。我们调整了学习率计划和训练迭代次数（在
    100 到 500 之间），并报告了多个随机种子下的最佳平均困惑度。
- en: 'Table 14: OPT-13B: Perplexity comparison of magnitude pruning with PERP in
    the unstructured pruning setting (50% sparsity), varying the LoRA rank $r$ as
    indicated. We tune the retraining length (100 and 500 iterations) and the learning
    rate schedule, reporting the best mean perplexity across multiple seeds, including
    the standard deviation.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：OPT-13B：在非结构化剪枝设置（50% 稀疏性）中，使用大小剪枝与 PERP 的困惑度比较，变化的 LoRA 秩 $r$ 如所示。我们调整了重新训练长度（100
    和 500 次迭代）和学习率计划，报告了多个种子下的最佳平均困惑度，包括标准差。
- en: '|  | Rank $r$ |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '|  | 秩 $r$ |'
- en: '| --- | --- |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $\alpha$ | 8 | 16 | 32 | 64 | 128 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha$ | 8 | 16 | 32 | 64 | 128 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 32 | 12.91 ±0.02 | 12.86 ±0.07 | 12.86 ±0.01 | 12.92 ±0.00 | 13.10 ±0.03
    |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 12.91 ±0.02 | 12.86 ±0.07 | 12.86 ±0.01 | 12.92 ±0.00 | 13.10 ±0.03
    |'
- en: '| 64 | 12.78 ±0.12 | 12.77 ±0.08 | 12.84 ±0.06 | 12.84 ±0.11 | 12.95 ±0.02
    |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 64 | 12.78 ±0.12 | 12.77 ±0.08 | 12.84 ±0.06 | 12.84 ±0.11 | 12.95 ±0.02
    |'
- en: '| 128 | 12.89 ±0.01 | 12.78 ±0.09 | 12.92 ±0.02 | 12.83 ±0.20 | 12.91 ±0.09
    |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 12.89 ±0.01 | 12.78 ±0.09 | 12.92 ±0.02 | 12.83 ±0.20 | 12.91 ±0.09
    |'
- en: 'C.3 Ablation: The high sparsity regime'
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 消融实验：高稀疏性情况
- en: '[Table 15](#A3.T15 "Table 15 ‣ C.3 Ablation: The high sparsity regime ‣ Appendix
    C Ablation studies ‣ PERP: Rethinking the Prune-Retrain Paradigm in the Era of
    LLMs") examines the perplexity results for unstructured pruning on OPT-30B using
    magnitude pruning, Wanda, and SparseGPT, both with and without PERP retraining.
    This table focuses on higher sparsity levels to explore whether our findings extend
    beyond the 50% threshold. Analyzing the results, all methods significantly benefit
    from PERP retraining. However, in the extreme sparsity regime (80%), SparseGPT
    alone maintains reasonable test perplexity. Surprisingly, at higher sparsity levels,
    magnitude pruning outperforms Wanda after retraining, though this is not the case
    without retraining.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 15](#A3.T15 "表 15 ‣ C.3 消融实验：高稀疏性情况 ‣ 附录 C 消融研究 ‣ PERP：重新思考 LLM 时代的剪枝-重新训练范式")
    检查了在 OPT-30B 上使用大小剪枝、Wanda 和 SparseGPT 的非结构化剪枝困惑度结果，包括有无 PERP 重新训练的情况。该表格关注于更高的稀疏性水平，以探索我们的发现是否超出了
    50% 的阈值。分析结果表明，所有方法在 PERP 重新训练下都有显著收益。然而，在极端稀疏性情况下（80%），仅 SparseGPT 维持了合理的测试困惑度。令人惊讶的是，在更高稀疏性水平下，大小剪枝在重新训练后优于
    Wanda，尽管在没有重新训练的情况下则不是这样。'
- en: 'Table 15: OPT-30B: Perplexity comparison of magnitude pruning, Wanda, and SparseGPT,
    either with or without PERP in the unstructured pruning setting (50% sparsity).
    All methods using PERP (indicated in the second column) are retrained for 500
    iterations and further highlighted in bold. We report the mean perplexity over
    several seeds and omit the standard deviation for the sake of clarity.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 表15：OPT-30B：量级修剪、Wanda和SparseGPT在未结构化修剪设置（50%稀疏度）中的困惑度比较。所有使用PERP的方法（在第二列中标出）都进行了500次迭代的再训练，并以粗体进一步突出显示。我们报告了几个种子的平均困惑度，并为清晰起见省略了标准差。
- en: '|  |  | Sparsity |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 稀疏度 |'
- en: '| --- | --- | --- |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Method | PERP | 50% | 60% | 70% | 80% |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PERP | 50% | 60% | 70% | 80% |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Magnitude | ✗ | 168.07 | 11677.30 | 28183.30 | 56369.09 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 量级 | ✗ | 168.07 | 11677.30 | 28183.30 | 56369.09 |'
- en: '| Magnitude | ✓ | 11.88 | 14.20 | 19.01 | 165.30 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 量级 | ✓ | 11.88 | 14.20 | 19.01 | 165.30 |'
- en: '| Wanda | ✗ | 10.07 | 26.36 | 10376.56 | 7816.61 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✗ | 10.07 | 26.36 | 10376.56 | 7816.61 |'
- en: '| Wanda | ✓ | 10.06 | 11.31 | 29.44 | 487.80 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | ✓ | 10.06 | 11.31 | 29.44 | 487.80 |'
- en: '| SparseGPT | ✗ | 9.79 | 10.71 | 13.55 | 46.39 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✗ | 9.79 | 10.71 | 13.55 | 46.39 |'
- en: '| SparseGPT | ✓ | 9.80 | 10.43 | 11.83 | 20.20 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | ✓ | 9.80 | 10.43 | 11.83 | 20.20 |'
