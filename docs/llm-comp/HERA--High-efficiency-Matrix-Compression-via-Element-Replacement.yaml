- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:47:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:47:47'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'HERA: High-efficiency Matrix Compression via Element Replacement'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'HERA: 通过元素替换实现高效矩阵压缩'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.03637](https://ar5iv.labs.arxiv.org/html/2407.03637)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.03637](https://ar5iv.labs.arxiv.org/html/2407.03637)
- en: Yanshu Wang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 王艳舒
- en: Peking University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学
- en: Beijing, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 北京，中国
- en: yanshuwang@pku.edu.cn
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: yanshuwang@pku.edu.cn
- en: '&Wang Li¹¹footnotemark: 1'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&王莉¹¹footnotemark: 1'
- en: Peking University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学
- en: Beijing, China
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 北京，中国
- en: 2000012734@stu.pku.edu.cn
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 2000012734@stu.pku.edu.cn
- en: '&Tong Yang'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '&杨通'
- en: Peking University
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学
- en: Beijing, China
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 北京，中国
- en: yangtong@pku.edu.cn Equal contribution
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: yangtong@pku.edu.cn 平等贡献
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have significantly advanced natural language processing
    tasks such as machine translation, text generation, and sentiment analysis. However,
    their large size, often consisting of billions of parameters, poses challenges
    for storage, computation, and deployment, particularly in resource-constrained
    environments like mobile devices and edge computing platforms. Additionally, the
    key-value (k-v) cache used to speed up query processing requires substantial memory
    and storage, exacerbating these challenges. Vector databases have emerged as a
    crucial technology to efficiently manage and retrieve the high-dimensional vectors
    produced by LLMs, facilitating faster data access and reducing computational demands.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在自然语言处理任务（如机器翻译、文本生成和情感分析）方面取得了显著进展。然而，它们的庞大规模，通常包含数十亿个参数，给存储、计算和部署带来了挑战，尤其是在资源受限的环境中，如移动设备和边缘计算平台。此外，为了加速查询处理而使用的键值（k-v）缓存需要大量内存和存储，这加剧了这些挑战。向量数据库作为一种关键技术，已成为高效管理和检索LLMs生成的高维向量的关键，促进了更快的数据访问，并减少了计算需求。
- en: Effective compression and quantization techniques are essential to address these
    challenges, as they reduce the memory footprint and computational requirements
    without significantly compromising performance. Traditional methods that uniformly
    map parameters to compressed spaces often fail to account for the uneven distribution
    of parameters, leading to considerable accuracy loss. Therefore, innovative approaches
    are needed to achieve better compression ratios while preserving model performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的压缩和量化技术对于应对这些挑战至关重要，因为它们可以减少内存占用和计算需求，而不会显著影响性能。传统的方法通常将参数均匀映射到压缩空间，往往无法考虑参数的分布不均，从而导致较大的准确性损失。因此，需要创新的方法以实现更好的压缩比，同时保持模型性能。
- en: In this work, we propose HERA, a novel algorithm that employs heuristic Element
    Replacement for compressing matrix. HERA systematically replaces elements within
    the model using heuristic methods, which simplifies the structure of the model
    and makes subsequent compression more effective. By hierarchically segmenting,
    compressing, and reorganizing the matrix dataset, our method can effectively reduce
    the quantization error to 12.3% of the original at the same compression ratio.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了HERA，这是一种新颖的算法，通过启发式元素替换来压缩矩阵。HERA系统地使用启发式方法替换模型中的元素，从而简化模型结构，并使后续的压缩更加有效。通过对矩阵数据集进行分层分割、压缩和重组，我们的方法可以在相同的压缩比下将量化误差有效降低到原始值的12.3%。
- en: 1 Introduction
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have revolutionized the field of natural language
    processing (NLP), enabling significant advancements in tasks such as machine translation,
    text generation, and sentiment analysis. These models, characterized by their
    large-scale neural network architectures and vast training datasets, have shown
    remarkable capabilities in understanding and generating human language. The advent
    of LLMs, such as OpenAI’s GPT-3 and BERT by Google, has pushed the boundaries
    of what machines can achieve in linguistic tasks, providing near-human performance
    in various applications  Brown et al. ([2020](#bib.bib1)); Devlin et al. ([2018](#bib.bib6)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）彻底改变了自然语言处理（NLP）领域，显著推动了机器翻译、文本生成和情感分析等任务的进展。这些模型以其大规模神经网络架构和庞大的训练数据集为特点，展现了理解和生成自然语言的卓越能力。像OpenAI的GPT-3和Google的BERT这样的LLMs的出现，推动了机器在语言任务中能达到的边界，在各种应用中提供了接近人类的表现
    Brown等人 ([2020](#bib.bib1)); Devlin等人 ([2018](#bib.bib6))。
- en: The development of LLMs is rooted in the transformer architecture, which employs
    self-attention mechanisms to process and produce language with high contextual
    relevance. This architecture has replaced previous recurrent neural network (RNN)
    and long short-term memory (LSTM) models due to its efficiency and ability to
    handle long-range dependencies in text Vaswani et al. ([2017](#bib.bib15)). As
    a result, LLMs have become the cornerstone of modern NLP, driving innovations
    in areas such as automated customer service, content creation, and real-time language
    translation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的发展基于变压器架构，该架构采用自注意力机制处理和生成具有高度上下文相关性的语言。由于其高效性和处理文本中长程依赖的能力，这种架构已取代了之前的递归神经网络（RNN）和长短期记忆（LSTM）模型 Vaswani
    et al. ([2017](#bib.bib15))。因此，LLMs已成为现代自然语言处理的基石，推动了自动客户服务、内容创建和实时语言翻译等领域的创新。
- en: Furthermore, the development and deployment of LLMs face significant challenges,
    particularly in optimizing vector database efficiency, LLM weight quantization,
    and key-value (k-v) cache optimization. First, as the size and complexity of LLMs
    increase, efficiently managing and retrieving vectors in large databases becomes
    critical. Optimizing vector database efficiency involves improving indexing and
    search algorithms to handle high-dimensional data, which is essential for tasks
    like semantic search and recommendation systems Johnson et al. ([2019](#bib.bib11)).
    Second, LLM weight quantization, which involves reducing the precision of model
    weights, is a crucial technique for making these models more storage and computation
    efficient without significantly degrading their performance. Quantization can
    drastically reduce the memory footprint and computational requirements, enabling
    the deployment of LLMs on resource-constrained devices. However, achieving optimal
    quantization while maintaining model accuracy remains a complex challenge that
    requires sophisticated algorithms and techniques Dettmers et al. ([2021](#bib.bib5)).
    Lastly, optimizing k-v cache efficiency is vital for speeding up inference times
    in LLMs. The k-v cache stores intermediate activations during the forward pass,
    which can be reused to avoid redundant computations. Efficient management and
    compression of these caches are essential to reduce latency and improve the throughput
    of LLMs, especially in real-time applications like chatbots and virtual assistants.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLMs的开发和部署面临重大挑战，特别是在优化向量数据库效率、LLM权重量化和键值（k-v）缓存优化方面。首先，随着LLMs的规模和复杂性的增加，高效管理和检索大数据库中的向量变得至关重要。优化向量数据库效率涉及改进索引和搜索算法，以处理高维数据，这对语义搜索和推荐系统等任务至关重要 Johnson
    et al. ([2019](#bib.bib11))。其次，LLM权重量化，即减少模型权重的精度，是提高这些模型存储和计算效率的关键技术，而不会显著降低其性能。量化可以显著减少内存占用和计算需求，使LLMs能够在资源受限的设备上部署。然而，实现最佳量化同时保持模型准确性仍然是一个复杂的挑战，需要复杂的算法和技术 Dettmers
    et al. ([2021](#bib.bib5))。最后，优化k-v缓存效率对于加速LLMs的推理时间至关重要。k-v缓存存储在前向传播期间的中间激活，可以重复使用以避免冗余计算。高效管理和压缩这些缓存对于减少延迟和提高LLMs的吞吐量是必要的，尤其是在像聊天机器人和虚拟助手这样的实时应用中。
- en: '![Refer to caption](img/e5c0f6f02ab859925448cb2d6edb8339.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e5c0f6f02ab859925448cb2d6edb8339.png)'
- en: 'Figure 1: The distribution of LLM parameter.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLM参数的分布。
- en: To address the challenges of storage and computational efficiency in large language
    models (LLMs), it is crucial to develop techniques that optimize vector database
    efficiency, weight quantization, and key-value (k-v) cache optimization. Therefore,
    finding quantization algorithms that can efficiently compress and quickly decompress
    matrix data is essential.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决大型语言模型（LLMs）在存储和计算效率方面的挑战，开发优化向量数据库效率、权重量化和键值（k-v）缓存优化的技术至关重要。因此，寻找能够高效压缩和快速解压矩阵数据的量化算法是关键。
- en: 'In practice, when compressing parameters, the distribution of parameters is
    often uneven. First, the distribution is not uniform, and second, the magnitudes
    of different data points vary significantly. For example, the distribution of
    Transformer neural network weights is shown in Figure  [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ HERA: High-efficiency Matrix Compression via Element Replacement").
    In Figure  [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ HERA: High-efficiency Matrix
    Compression via Element Replacement"), different rows represent the 0th layer,
    8th layer, 16th layer, and 24th layer. Different columns represent the attention
    layer’s q, k, v, and o layers, as well as the fully connected layer’s gate, up,
    and down layers. The distributions of different layers are not uniformly analyzed,
    and the distributions vary across different layers. Previous algorithms uniformly
    and linearly map parameters to another compressed space. Although this method
    is simple, it may not be effective and can result in significant accuracy loss
    because it does not take into account the original data distribution. It maps
    both densely populated and sparse regions to the quantized space in the same way.
    Besides, A lines of works have been proposed to optimize vector database efficiency Ge
    et al. ([2013](#bib.bib8)); Jegou et al. ([2010](#bib.bib10)); Kalantidis, Avrithis
    ([2014](#bib.bib12)), quantize the model weights Dettmers et al. ([2022](#bib.bib4));
    Polino et al. ([2018](#bib.bib14)); Chmiel et al. ([2020](#bib.bib2)); Fan et al.
    ([2020](#bib.bib7)); Zafrir et al. ([2019](#bib.bib17)); Wu et al. ([2022](#bib.bib16));
    LeCun et al. ([1989](#bib.bib13)) and optimizate kv cache Hooper et al. ([2024](#bib.bib9));
    Cho et al. ([2024](#bib.bib3)). They also does not take into account the original
    data distribution. Therefore, a more reasonable approach is to quantize the model
    based on the distribution and magnitude of the parameters, stratifying and computing
    accordingly.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在压缩参数时，参数的分布往往是不均匀的。首先，分布不均匀，其次，不同数据点的大小差异显著。例如，Transformer神经网络权重的分布如图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ HERA：通过元素替换的高效矩阵压缩")所示。在图[1](#S1.F1 "图 1 ‣ 1 引言 ‣ HERA：通过元素替换的高效矩阵压缩")中，不同的行代表第0层、第8层、第16层和第24层。不同的列代表注意力层的q、k、v和o层，以及全连接层的gate、up和down层。不同层的分布没有均匀分析，不同层的分布差异较大。以往的算法将参数均匀且线性地映射到另一个压缩空间。尽管这种方法简单，但可能效果不佳，并可能导致显著的准确度损失，因为它没有考虑原始数据的分布。它以相同的方式将密集区域和稀疏区域映射到量化空间。此外，还有一些研究工作提出了优化向量数据库效率的方法 Ge
    等 ([2013](#bib.bib8)); Jegou 等 ([2010](#bib.bib10)); Kalantidis, Avrithis ([2014](#bib.bib12))，量化模型权重 Dettmers
    等 ([2022](#bib.bib4)); Polino 等 ([2018](#bib.bib14)); Chmiel 等 ([2020](#bib.bib2));
    Fan 等 ([2020](#bib.bib7)); Zafrir 等 ([2019](#bib.bib17)); Wu 等 ([2022](#bib.bib16));
    LeCun 等 ([1989](#bib.bib13)) 和优化kv缓存 Hooper 等 ([2024](#bib.bib9)); Cho 等 ([2024](#bib.bib3))。这些方法也没有考虑原始数据分布。因此，更合理的方法是基于参数的分布和大小对模型进行量化，按层次进行分级和计算。
- en: In this work, we propose a novel algorithm named HERA to enhance the compression
    of large language models. The core idea of HERA is to group the relevant matrices
    and compress each group using clustering methods. Furthermore, HERA reorders the
    original dataset based on the distribution and magnitude of the parameters. This
    reordering is performed in multiple layers, with each layer featuring a new arrangement
    that optimizes the dataset’s distribution for compression. By leveraging these
    techniques, HERA effectively reduces data error and minimizes the discrepancy
    between estimated data and actual data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种名为HERA的新算法，以增强大型语言模型的压缩效果。HERA的核心思想是将相关矩阵分组，并使用聚类方法压缩每个组。此外，HERA根据参数的分布和大小对原始数据集进行重新排序。这种重新排序在多个层次上进行，每一层都具有新的排列，以优化数据集的分布以便于压缩。通过利用这些技术，HERA有效地减少了数据误差，并最小化了估计数据与实际数据之间的差异。
- en: We implemented a prototype system using Python. Experimental results show that,
    by hierarchically segmenting, compressing, and reorganizing the matrix dataset,
    our method can effectively reduce the quantization error to 12.3% of the original
    at the same compression ratio.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Python实现了一个原型系统。实验结果表明，通过分层分割、压缩和重组矩阵数据集，我们的方法能够有效地将量化误差降低到原始值的12.3%，并且压缩比保持不变。
- en: 2 HERA
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 HERA
- en: 2.1 Basic algorithm
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 基本算法
- en: 'We borrow the idea from product quantization algorithm and k-means algorithm
    to compress the model parameter. The algorithm is divided into four steps, the
    first three steps is quantization process as shown in Figure  [2](#S2.F2 "Figure
    2 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression via
    Element Replacement"), the last two steps is the dequantization process as show
    in Figure  [3](#S2.F3 "Figure 3 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency
    Matrix Compression via Element Replacement"). The detailed process is described
    below.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '我们借鉴了产品量化算法和 k-means 算法的思想来压缩模型参数。该算法分为四个步骤，前三个步骤是量化过程，如图 [2](#S2.F2 "图 2 ‣
    2.1 基本算法 ‣ 2 HERA ‣ HERA: 高效矩阵压缩通过元素替换")所示，最后两个步骤是去量化过程，如图 [3](#S2.F3 "图 3 ‣ 2.1
    基本算法 ‣ 2 HERA ‣ HERA: 高效矩阵压缩通过元素替换")所示。详细过程如下。'
- en: Algorithm 1 Product Quantization Algorithm
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 产品量化算法
- en: 1:$X\in\mathbb{R}^{N\times D}$24:end for
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 1:$X\in\mathbb{R}^{N\times D}$24:end for
- en: '1.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Space Partitioning: Divide the high-dimensional space into a series of low-dimensional
    subspaces.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 空间划分：将高维空间划分为一系列低维子空间。
- en: '2.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Codebook calculation: Use k-means algorithm to calculate codebook of low-dimensional
    subspaces.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 码本计算：使用 k-means 算法计算低维子空间的码本。
- en: '3.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Quantization: Perform independent quantization within each low-dimensional
    subspace, mapping data points to the nearest cluster center.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化：在每个低维子空间内执行独立量化，将数据点映射到最近的聚类中心。
- en: '4.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Codebook Reconstruction: Reconstruct the data using a codebook that contains
    all possible cluster centers.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 码本重建：使用包含所有可能聚类中心的码本重建数据。
- en: '5.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Dequantization: Restore the vectors in the original high-dimensional space
    based on the vectors in the quantized low-dimensional subspaces.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 去量化：基于量化低维子空间中的向量恢复原始高维空间中的向量。
- en: '![Refer to caption](img/cb096afafa201a2f283e0dfd1ae477ab.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/cb096afafa201a2f283e0dfd1ae477ab.png)'
- en: 'Figure 2: The Quantization Process of HERA.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: HERA 的量化过程。'
- en: '![Refer to caption](img/9618952bc437f70ca5664e3caddd1117.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9618952bc437f70ca5664e3caddd1117.png)'
- en: 'Figure 3: The Dequantization Process of HERA.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: HERA 的去量化过程。'
- en: In data matrices where the distribution of data is uneven across different columns,
    directly applying the k-means algorithm for clustering may yield suboptimal results.
    This unevenness can distort the clustering process, as k-means assumes that all
    features contribute equally to the distance calculations. The differing distributions
    across columns mean that some features may disproportionately influence the clustering
    outcome, leading to biased clusters that do not accurately reflect the true structure
    of the data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据矩阵中，当数据在不同列中的分布不均匀时，直接应用 k-means 算法进行聚类可能会产生次优结果。这种不均匀性可能扭曲聚类过程，因为 k-means
    假设所有特征对距离计算的贡献是相等的。列之间的不同分布意味着某些特征可能会对聚类结果产生不成比例的影响，从而导致偏差的簇，无法准确反映数据的真实结构。
- en: To mitigate these effects, it is crucial to preprocess the data appropriately.
    One common approach is to normalize the data so that each column contributes equally
    to the distance metric used by k-means. Standard normalization techniques, such
    as z-score normalization or min-max scaling, can be employed to transform the
    data into a more uniform distribution. By ensuring that all features are on a
    comparable scale, the k-means algorithm can perform more effectively, producing
    clusters that better represent the underlying patterns in the data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这些影响，适当预处理数据是至关重要的。一种常见的方法是标准化数据，使每一列对 k-means 使用的距离度量有相等的贡献。可以采用标准的标准化技术，如
    z-score 标准化或 min-max 缩放，将数据转换为更均匀的分布。通过确保所有特征处于可比尺度上，k-means 算法可以更有效地执行，生成更能代表数据中潜在模式的簇。
- en: Algorithm 2 Product Quantization with HERA heuristics
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 使用 HERA 启发式的产品量化
- en: '1:Input matrix: $X\in\mathbb{R}^{N\times D}$ 0 then26:     Goto line 127:end if'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 1:输入矩阵：$X\in\mathbb{R}^{N\times D}$ 0 then26:     Goto line 127:end if
- en: However, these transformations require specific parameters for each column and
    involve complex calculations, which can be computationally intensive and demand
    significant storage resources. To address this, we employ a reordering method
    that permutations the elements of the matrix based on their relative sizes. This
    approach achieves a more uniform distribution, enabling more effective application
    of compression operations.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些变换需要为每一列指定特定的参数，并涉及复杂的计算，这可能计算密集并且需要大量存储资源。为了解决这个问题，我们采用了一种重新排序的方法，根据矩阵元素的相对大小进行排列。这种方法实现了更均匀的分布，使压缩操作更加有效。
- en: Algorithm 3 Dequantization of Product Quantization with HERA heuristics
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 使用 HERA 启发式算法的产品量化去量化
- en: '1:Quantized feature matrices: $code_{small}$23:     end for24:end for'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 1:量化特征矩阵：`code_{small}`23:     end for24:end for
- en: The purpose of permutation is to explore all possible reordering methods within
    the search space. For an $N\times D$ possible permutations, creating a vast search
    space.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 排列的目的是探索搜索空间内所有可能的重新排序方法。对于一个 $N\times D$ 的可能排列，创建了一个庞大的搜索空间。
- en: To manage this, we employ heuristic algorithms to reduce the search space and
    optimize the algorithm’s complexity. Heuristic algorithms provide approximate
    solutions by focusing on the most promising areas of the search space, rather
    than exhaustively evaluating all possible permutations. These methods significantly
    decrease computational requirements while still delivering high-quality solutions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了管理这个问题，我们采用启发式算法来减少搜索空间并优化算法的复杂性。启发式算法通过专注于搜索空间中最有希望的区域，而不是彻底评估所有可能的排列，提供近似解决方案。这些方法显著降低了计算需求，同时仍能提供高质量的解决方案。
- en: By adopting this heuristic reordering strategy, we aim to achieve a more balanced
    distribution of data across the matrix. This not only simplifies the preprocessing
    steps but also enhances the efficiency of subsequent clustering and compression
    processes. Ultimately, this method helps overcome the challenges posed by uneven
    data distributions, leading to more accurate and resource-efficient clustering
    outcomes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用这种启发式重新排序策略，我们旨在实现数据在矩阵中的更均衡分布。这不仅简化了预处理步骤，还提高了后续聚类和压缩过程的效率。*最终*，这种方法有助于克服不均匀数据分布带来的挑战，从而实现更准确且资源高效的聚类结果。
- en: 2.2 HERA heuristics
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 HERA 启发式算法
- en: To improve quantization and avoid uneven distribution, HERA first perceives
    the distribution of data sizes, then rearranges the elements according to the
    distribution, and finally quantizes the rearranged matrix.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善量化并避免不均匀分布，HERA 首先感知数据大小的分布，然后根据分布重新排列元素，最后对重新排列的矩阵进行量化。
- en: 2.2.1 HERA Design
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 HERA 设计
- en: 'We observe that compressing matrix elements of similar sizes as a single column
    (group) yields better results.HERA uses a low-cost method to locally adjust element
    arrangement based on data sizes. We first introude the HERA design. The design
    of HERA heuristics is shown in Figure  [4](#S2.F4 "Figure 4 ‣ 2.2.1 HERA Design
    ‣ 2.2 HERA heuristics ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression via
    Element Replacement").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，将类似大小的矩阵元素作为一个单独的列（组）进行压缩效果更佳。HERA 使用一种低成本的方法，根据数据大小局部调整元素排列。我们首先介绍 HERA
    设计。HERA 启发式算法的设计如图 [4](#S2.F4 "图 4 ‣ 2.2.1 HERA 设计 ‣ 2.2 HERA 启发式算法 ‣ 2 HERA ‣
    HERA：通过元素替换实现高效矩阵压缩") 所示。
- en: '![Refer to caption](img/4fd642a656d03a84a9e74171afe58c5c.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4fd642a656d03a84a9e74171afe58c5c.png)'
- en: 'Figure 4: HERA heuristics.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：HERA 启发式算法。
- en: HERA groups every two elements in the matrix together, rearranges the matrix
    elements based on the size of adjacent elements, and then quantizes the rearranged
    results.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: HERA 将矩阵中的每两个元素组合在一起，根据相邻元素的大小重新排列矩阵元素，然后对重新排列的结果进行量化。
- en: 'Quantization: The quantization process is illustrated in the algorithm [2](#alg2
    "Algorithm 2 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression
    via Element Replacement"), the quantization process consists of multiple steps,
    which involve rearranging the input data, comparing elements, and quantizing both
    smaller and larger value sets.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 量化：量化过程在算法 [2](#alg2 "算法 2 ‣ 2.1 基本算法 ‣ 2 HERA ‣ HERA：通过元素替换实现高效矩阵压缩") 中有所说明，量化过程包括多个步骤，涉及重新排列输入数据、比较元素，并对较小和较大的值集进行量化。
- en: 'The algorithm begins by rearranging the input matrix $X\in\mathbb{R}^{N\times
    D}$ (lines 2–4 in Algorithm [2](#alg2 "Algorithm 2 ‣ 2.1 Basic algorithm ‣ 2 HERA
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement")).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '算法开始时重新排列输入矩阵$X\in\mathbb{R}^{N\times D}$（见算法[2](#alg2 "Algorithm 2 ‣ 2.1 Basic
    algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression via Element Replacement)中的第2-4行"）。'
- en: 'Next, each pair $x_{pair_{i,j}}$ recording the comparison result as 1 (lines
    5–8 in Algorithm [2](#alg2 "Algorithm 2 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA:
    High-efficiency Matrix Compression via Element Replacement")). Otherwise, the
    assignment is reversed, and the feature map records a 0 (lines 9–12 in Algorithm [2](#alg2
    "Algorithm 2 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression
    via Element Replacement")).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，每对$x_{pair_{i,j}}$记录比较结果为1（见算法[2](#alg2 "Algorithm 2 ‣ 2.1 Basic algorithm
    ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression via Element Replacement)中的第5-8行"）。否则，赋值被颠倒，特征图记录为0（见算法[2](#alg2
    "Algorithm 2 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression
    via Element Replacement)中的第9-12行"）。'
- en: 'The quantization process proceeds by applying the PQ algorithm to both $X_{small}$),
    refining the quantization results with each iteration (lines 13–16 in Algorithm [2](#alg2
    "Algorithm 2 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression
    via Element Replacement")).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '量化过程通过对$X_{small}$应用PQ算法进行，并在每次迭代中优化量化结果（见算法[2](#alg2 "Algorithm 2 ‣ 2.1 Basic
    algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression via Element Replacement)中的第13-16行"）。'
- en: Dequantization:The Decompression process consists of multiple steps, namely
    decoding the quantized values, reconstructing the pairs, and restoring the original
    matrix dimensions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 去量化：去压缩过程包括多个步骤，即解码量化值、重建配对以及恢复原始矩阵维度。
- en: The algorithm begins by initializing the reconstructed matrix $\hat{X}_{pairs}\in\mathbb{R}^{N\times
    D/2}$ respectively.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 算法开始时初始化重建矩阵$\hat{X}_{pairs}\in\mathbb{R}^{N\times D/2}$。
- en: 'Next, for each pair in $\hat{X}_{pairs}$. If the feature map value is 1, the
    first element of the pair is assigned the decoded small value and the second element
    the decoded big value (lines 5–8 in Algorithm [3](#alg3 "Algorithm 3 ‣ 2.1 Basic
    algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression via Element Replacement")).
    If the feature map value is 0, the assignment is reversed (lines 9–12 in Algorithm [3](#alg3
    "Algorithm 3 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression
    via Element Replacement")).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，对于每一对$\hat{X}_{pairs}$，如果特征图的值为1，则配对的第一个元素被赋予解码后的小值，第二个元素赋予解码后的大值（见算法[3](#alg3
    "Algorithm 3 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression
    via Element Replacement)中的第5-8行"）。如果特征图的值为0，则赋值被颠倒（见算法[3](#alg3 "Algorithm 3 ‣
    2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression via Element
    Replacement)中的第9-12行"）。'
- en: 'Finally, the pairs are expanded back into the original dimensions to form the
    reconstructed matrix $\hat{X}$ (lines 13–17 in Algorithm [3](#alg3 "Algorithm
    3 ‣ 2.1 Basic algorithm ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression via
    Element Replacement")).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，配对被扩展回原始维度，以形成重建矩阵$\hat{X}$（见算法[3](#alg3 "Algorithm 3 ‣ 2.1 Basic algorithm
    ‣ 2 HERA ‣ HERA: High-efficiency Matrix Compression via Element Replacement)中的第13-17行"）。'
- en: 3 Experiments
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: 3.1 Experiment setup
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: 'Dataset: The datasets used in our experiments were generated with matrices
    of size $N\times D$ is the dimensionality of each sample. We employed one types
    of normal distribution to create these datasets.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集：我们实验中使用的数据集由大小为$N\times D$的矩阵生成。我们采用了一种正态分布来创建这些数据集。
- en: 'Normal Distribution Dataset: For the normal distribution dataset, each element
    of the matrix was drawn from a truncated normal distribution with a mean of 0.5
    and a standard deviation of 0.16\. The distribution was truncated to the open
    interval $(0,1)$ to ensure all values remain within this range.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布数据集：对于正态分布数据集，矩阵的每个元素都来自均值为0.5、标准差为0.16的截断正态分布。该分布被截断到开区间$(0,1)$以确保所有值都在此范围内。
- en: '|  | $\mathbf{X}_{ij}\sim\mathcal{TN}(0.5,0.16,0,1),\quad\forall i\in\{1,\ldots,N\},\quad\forall
    j\in\{1,\ldots,D\}$ |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{X}_{ij}\sim\mathcal{TN}(0.5,0.16,0,1),\quad\forall i\in\{1,\ldots,N\},\quad\forall
    j\in\{1,\ldots,D\}$ |  |'
- en: Here, $\mathcal{TN}(\mu,\sigma,a,b)$.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\mathcal{TN}(\mu,\sigma,a,b)$。
- en: 'Platform and implementation: We conducted our algorithm evaluations on a high-performance
    server equipped with an Intel Core i9-10980XE processor, featuring 18 cores and
    36 threads, operating at a base frequency of 3.00 GHz. The server also includes
    128GB of 3200MHz DDR4 memory and a 24.8MB L3 cache, providing robust computational
    capabilities. All algorithms were implemented in Python, using version 3.8.10.
    For each case, the experiment was repeated 100 times. In each repetition, matrices
    of the same size and from the same distribution were generated using different
    seeds.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 平台和实现：我们在一台高性能服务器上进行了算法评估，该服务器配备了 Intel Core i9-10980XE 处理器，具有 18 个核心和 36 个线程，基础频率为
    3.00 GHz。服务器还配备了 128GB 的 3200MHz DDR4 内存和 24.8MB 的 L3 缓存，提供强大的计算能力。所有算法均使用 Python
    3.8.10 实现。每个案例的实验重复了 100 次。在每次重复中，使用不同的种子生成相同大小和相同分布的矩阵。
- en: Comparison of Algorithms and Parameter Selection
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 算法比较和参数选择
- en: In this section, we describe the parameter selection process for our HERA algorithm
    and compare its performance with the Optimized Product Quantization (OPQ) algorithm.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了 HERA 算法的参数选择过程，并与优化产品量化（OPQ）算法的性能进行了比较。
- en: 'Our HERA algorithm involves a critical parameter, the number of centroids $K_{s}$
    is governed by the following memory constraint:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 HERA 算法涉及一个关键参数，质心的数量 $K_{s}$ 受以下内存约束的限制：
- en: '|  | $K_{s}\times D\times 32+N\times M\times[(K_{s}-1)\cdot\text{bitlength}]\leq\text{memory
    in bits}$ |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $K_{s}\times D\times 32+N\times M\times[(K_{s}-1)\cdot\text{bitlength}]\leq\text{memory
    in bits}$ |  |'
- en: 'where:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $K_{s}$ is the number of centroids,
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $K_{s}$ 是质心的数量，
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $D$ is the dimensionality of each sample,
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $D$ 是每个样本的维度，
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $N$ is the number of samples,
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $N$ 是样本的数量，
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $M$ is the number of subspaces,
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $M$ 是子空间的数量，
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: bitlength is the bit length used for encoding.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: bitlength 是用于编码的比特长度。
- en: This constraint ensures that the total memory usage remains within the available
    memory limits. By carefully selecting $K_{s}$, we aim to optimize the performance
    of the HERA algorithm while adhering to memory constraints.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 该约束确保总内存使用量保持在可用内存限制范围内。通过仔细选择 $K_{s}$，我们旨在优化 HERA 算法的性能，同时遵守内存约束。
- en: The Optimized Product Quantization (OPQ) algorithm was used as a benchmark for
    comparison. The OPQ algorithm was implemented with default parameters, providing
    a consistent basis for performance evaluation. The OPQ algorithm optimizes product
    quantization by adjusting the space partitioning and centroid assignments to minimize
    quantization error.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 优化产品量化（OPQ）算法被用作比较的基准。OPQ 算法使用默认参数实现，为性能评估提供了一致的基础。OPQ 算法通过调整空间划分和质心分配来优化产品量化，以最小化量化误差。
- en: Comparison Metrics
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 比较指标
- en: 'We use MAE (Mean Absolute Error), MRE (Mean Relative Error), and MSE (Mean
    Squared Error) for experimental evaluation. Let $X_{(i,j)}$ denote the values
    before and after dequantization, respectively. The following metrics are used
    for comparison:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 MAE（均值绝对误差）、MRE（均值相对误差）和 MSE（均值平方误差）进行实验评估。令 $X_{(i,j)}$ 分别表示去量化前后的值。以下指标用于比较：
- en: '|  | $\text{MAE}=\frac{1}{N\cdot D}\sum_{i,j}\left&#124;X_{(i,j)}-X_{(i,j)}^{\prime}\right&#124;$
    |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{MAE}=\frac{1}{N\cdot D}\sum_{i,j}\left&#124;X_{(i,j)}-X_{(i,j)}^{\prime}\right&#124;$
    |  |'
- en: '|  | $\text{MRE}=\frac{1}{N\cdot D}\sum_{i,j}\frac{\left&#124;X_{(i,j)}-X_{(i,j)}^{\prime}\right&#124;}{X_{(i,j)}}$
    |  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{MRE}=\frac{1}{N\cdot D}\sum_{i,j}\frac{\left&#124;X_{(i,j)}-X_{(i,j)}^{\prime}\right&#124;}{X_{(i,j)}}$
    |  |'
- en: '|  | $\text{MSE}=\frac{1}{N\cdot D}\sum_{i,j}\left(X_{(i,j)}-X_{(i,j)}^{\prime}\right)^{2}$
    |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{MSE}=\frac{1}{N\cdot D}\sum_{i,j}\left(X_{(i,j)}-X_{(i,j)}^{\prime}\right)^{2}$
    |  |'
- en: We performed an optimization using 1-4 iterations, corresponding to our algorithms
    labeled as our1 through our4. In the quantification accuracy experiment, we tested
    the accuracy with different numbers of groups $m$ on the results while maintaining
    the same compression ratio.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 1-4 次迭代进行优化，对应于我们的算法标记为 our1 到 our4。在量化精度实验中，我们测试了不同组数 $m$ 的准确度，同时保持相同的压缩比。
- en: 3.2 Experiment result
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 实验结果
- en: Accuracy Measurement
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 精度测量
- en: '![Refer to caption](img/ab8d273df272415a5e456dcda605256b.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ab8d273df272415a5e456dcda605256b.png)'
- en: 'Figure 5: MAE on 8 subspaces.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：8 个子空间上的 MAE。
- en: '![Refer to caption](img/5334766df602bb13daf70d507f0c32d2.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5334766df602bb13daf70d507f0c32d2.png)'
- en: 'Figure 6: MRE on 8 subspaces.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：8 个子空间上的 MRE。
- en: '![Refer to caption](img/46e03d31580cbfdfba2ab2a8ec6ebb61.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/46e03d31580cbfdfba2ab2a8ec6ebb61.png)'
- en: 'Figure 7: MSE on 8 subspaces.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：8 个子空间上的 MSE。
- en: '![Refer to caption](img/2bf616f569e423a1f3a5c568977406c9.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2bf616f569e423a1f3a5c568977406c9.png)'
- en: 'Figure 8: MAE on 16 subspaces.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 16 个子空间上的 MAE。'
- en: '![Refer to caption](img/ea6389ee3a316b42f22a904e5070b4fb.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ea6389ee3a316b42f22a904e5070b4fb.png)'
- en: 'Figure 9: MRE on 16 subspaces.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 16 个子空间上的 MRE。'
- en: '![Refer to caption](img/cc7231bf8c4d448f3db90628f63122e2.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/cc7231bf8c4d448f3db90628f63122e2.png)'
- en: 'Figure 10: MSE on 16 subspaces.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 16 个子空间上的 MSE。'
- en: '![Refer to caption](img/d169bf34c84c5045d3340a5ae89394c0.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d169bf34c84c5045d3340a5ae89394c0.png)'
- en: 'Figure 11: MAE on 32 subspaces.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 32 个子空间上的 MAE。'
- en: '![Refer to caption](img/24f3ecb76b1e1b22fd75b9ffa474747d.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/24f3ecb76b1e1b22fd75b9ffa474747d.png)'
- en: 'Figure 12: MRE on 32 subspaces.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 32 个子空间上的 MRE。'
- en: '![Refer to caption](img/6740d15254f7c84953710259ab376ffb.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6740d15254f7c84953710259ab376ffb.png)'
- en: 'Figure 13: MSE on 32 subspaces.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: 32 个子空间上的 MSE。'
- en: '![Refer to caption](img/d05152d89dc7a6d1ce255d2f8b52cf0e.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d05152d89dc7a6d1ce255d2f8b52cf0e.png)'
- en: 'Figure 14: MAE on 128 subspaces.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '图 14: 128 个子空间上的 MAE。'
- en: '![Refer to caption](img/6675f0e8527281236ed24ff02edc80dc.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6675f0e8527281236ed24ff02edc80dc.png)'
- en: 'Figure 15: MRE on 128 subspaces.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15: 128 个子空间上的 MRE。'
- en: '![Refer to caption](img/1893eb06dd555fb5b6f95119e55af142.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1893eb06dd555fb5b6f95119e55af142.png)'
- en: 'Figure 16: MSE on 128 subspaces.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '图 16: 128 个子空间上的 MSE。'
- en: '![Refer to caption](img/23b3ff73cc03f4abf750fce1bfafc4aa.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/23b3ff73cc03f4abf750fce1bfafc4aa.png)'
- en: 'Figure 17: MAE on 1 iterations.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '图 17: 1 次迭代中的 MAE。'
- en: '![Refer to caption](img/00e3ee0817d440bde7ace79213532990.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/00e3ee0817d440bde7ace79213532990.png)'
- en: 'Figure 18: MRE on 1 iterations.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '图 18: 1 次迭代中的 MRE。'
- en: '![Refer to caption](img/6424f6c30a38cf024f83442339c4aed5.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6424f6c30a38cf024f83442339c4aed5.png)'
- en: 'Figure 19: MSE on 1 iterations.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '图 19: 1 次迭代中的 MSE。'
- en: '![Refer to caption](img/c6b8cea54845ee372426873fad3a4099.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c6b8cea54845ee372426873fad3a4099.png)'
- en: 'Figure 20: MAE on 2 iterations.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '图 20: 2 次迭代中的 MAE。'
- en: '![Refer to caption](img/ba6a4561250a6eab3eeaa043d5d33ae3.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ba6a4561250a6eab3eeaa043d5d33ae3.png)'
- en: 'Figure 21: MRE on 2 iterations.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '图 21: 2 次迭代中的 MRE。'
- en: '![Refer to caption](img/76ba0295967d4f9b2bbccb542aebd6bd.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/76ba0295967d4f9b2bbccb542aebd6bd.png)'
- en: 'Figure 22: MSE on 2 iterations.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '图 22: 2 次迭代中的 MSE。'
- en: '![Refer to caption](img/0c57ca6573b633566cee43844d982c0c.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0c57ca6573b633566cee43844d982c0c.png)'
- en: 'Figure 23: MAE on 3 iterations.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '图 23: 3 次迭代中的 MAE。'
- en: '![Refer to caption](img/02f1c2a863623ea3eedbe136c55596ff.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/02f1c2a863623ea3eedbe136c55596ff.png)'
- en: 'Figure 24: MRE on 3 iterations.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '图 24: 3 次迭代中的 MRE。'
- en: '![Refer to caption](img/35ae0ccbd7bebc989f39e5e489ced6db.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/35ae0ccbd7bebc989f39e5e489ced6db.png)'
- en: 'Figure 25: MSE on 3 iterations.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '图 25: 3 次迭代中的 MSE。'
- en: '![Refer to caption](img/29c46f81d5125d152e4f67b79635847f.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/29c46f81d5125d152e4f67b79635847f.png)'
- en: 'Figure 26: MAE on 4 iterations.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '图 26: 4 次迭代中的 MAE。'
- en: '![Refer to caption](img/9b6d23d6fb05cd16c0975c4ffd4dc6b9.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9b6d23d6fb05cd16c0975c4ffd4dc6b9.png)'
- en: 'Figure 27: MRE on 4 iterations.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '图 27: 4 次迭代中的 MRE。'
- en: '![Refer to caption](img/0ff49fd86a2dc6646275da7df789e1b0.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0ff49fd86a2dc6646275da7df789e1b0.png)'
- en: 'Figure 28: MSE on 4 iterations.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '图 28: 4 次迭代中的 MSE。'
- en: 'We measure MAE, MRE and MSE on different numbers of subspaces to show the accuracy
    of HERA.The results for MAE, MRE and MSE for different number of subspaces are
    shown in Figures  [7](#S3.F7 "Figure 7 ‣ 3.2 Experiment result ‣ 3 Experiments
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement")-  [7](#S3.F7
    "Figure 7 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix
    Compression via Element Replacement"), Figures  [10](#S3.F10 "Figure 10 ‣ 3.2
    Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix Compression via
    Element Replacement")-  [10](#S3.F10 "Figure 10 ‣ 3.2 Experiment result ‣ 3 Experiments
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement"), Figures
     [13](#S3.F13 "Figure 13 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency
    Matrix Compression via Element Replacement")-  [13](#S3.F13 "Figure 13 ‣ 3.2 Experiment
    result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix Compression via Element
    Replacement"), Figures  [16](#S3.F16 "Figure 16 ‣ 3.2 Experiment result ‣ 3 Experiments
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement")-  [16](#S3.F16
    "Figure 16 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix
    Compression via Element Replacement") respectively. For 8-subspace setting, HERA
    can reduce the MSE to 70.4%, 49.7%, 35.1% and 12.3% in 1-4 iterations, respectively.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在不同数量的子空间上测量了MAE、MRE和MSE，以展示HERA的准确性。MAE、MRE和MSE的结果分别显示在图 [7](#S3.F7 "Figure
    7 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix Compression
    via Element Replacement")-  [7](#S3.F7 "Figure 7 ‣ 3.2 Experiment result ‣ 3 Experiments
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement")、图 [10](#S3.F10
    "Figure 10 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix
    Compression via Element Replacement")-  [10](#S3.F10 "Figure 10 ‣ 3.2 Experiment
    result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix Compression via Element
    Replacement")、图 [13](#S3.F13 "Figure 13 ‣ 3.2 Experiment result ‣ 3 Experiments
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement")-  [13](#S3.F13
    "Figure 13 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix
    Compression via Element Replacement")、图 [16](#S3.F16 "Figure 16 ‣ 3.2 Experiment
    result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix Compression via Element
    Replacement")-  [16](#S3.F16 "Figure 16 ‣ 3.2 Experiment result ‣ 3 Experiments
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement")。对于8子空间设置，HERA可以在1-4次迭代中分别将MSE减少到70.4%、49.7%、35.1%和12.3%。'
- en: From the experimental results, we can derive four conclusions. Firstly, the
    higher the compression ratio, the less space is consumed, but the accuracy decreases.
    Secondly, the OPQ algorithm performs worse than others with the same compression
    rate because it needs to store the transformation matrix. Additionally, the PQ
    algorithm does not perform as well as our improved algorithm. Finally, the more
    iterations, the better the performance of our algorithm.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从实验结果中，我们可以得出四个结论。首先，压缩比越高，占用的空间越少，但准确性下降。其次，OPQ算法在相同压缩率下表现不如其他算法，因为它需要存储变换矩阵。此外，PQ算法的表现不如我们改进的算法。最后，迭代次数越多，我们的算法性能越好。
- en: 'Parameter sensitivity analysis: We also conducted a parameter sensitivity test
    under different iterations, evaluating how the number of subspaces affects the
    algorithm’s performance. The results for 1 to 4 iterations are shown in Figures
     [19](#S3.F19 "Figure 19 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency
    Matrix Compression via Element Replacement")- [19](#S3.F19 "Figure 19 ‣ 3.2 Experiment
    result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix Compression via Element
    Replacement"), Figures  [22](#S3.F22 "Figure 22 ‣ 3.2 Experiment result ‣ 3 Experiments
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement")- [22](#S3.F22
    "Figure 22 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix
    Compression via Element Replacement"), and Figures  [25](#S3.F25 "Figure 25 ‣
    3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix Compression
    via Element Replacement")- [25](#S3.F25 "Figure 25 ‣ 3.2 Experiment result ‣ 3
    Experiments ‣ HERA: High-efficiency Matrix Compression via Element Replacement"),
    respectively. Extensive experiments demonstrate that our algorithm reduces parameter
    sensitivity, making parameter selection more user-friendly (as indicated by the
    closer proximity of parameters in our algorithm’s curves).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '参数敏感性分析：我们还进行了不同迭代次数下的参数敏感性测试，评估子空间数量如何影响算法性能。1到4次迭代的结果分别显示在图 [19](#S3.F19
    "Figure 19 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix
    Compression via Element Replacement")- [19](#S3.F19 "Figure 19 ‣ 3.2 Experiment
    result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix Compression via Element
    Replacement")、图 [22](#S3.F22 "Figure 22 ‣ 3.2 Experiment result ‣ 3 Experiments
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement")- [22](#S3.F22
    "Figure 22 ‣ 3.2 Experiment result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix
    Compression via Element Replacement")以及图 [25](#S3.F25 "Figure 25 ‣ 3.2 Experiment
    result ‣ 3 Experiments ‣ HERA: High-efficiency Matrix Compression via Element
    Replacement")- [25](#S3.F25 "Figure 25 ‣ 3.2 Experiment result ‣ 3 Experiments
    ‣ HERA: High-efficiency Matrix Compression via Element Replacement")中。大量实验表明，我们的算法减少了参数敏感性，使参数选择更为用户友好（通过我们算法曲线中参数的更紧密接近度所示）。'
- en: 4 Conclusion
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: In conclusion, our research addresses critical challenges in the optimization
    of large language models (LLMs) through the development of a novel algorithm,
    HERA. The growing complexity and scale of LLMs necessitate innovative approaches
    to enhance storage and computational efficiency, particularly in vector database
    management, weight quantization, and key-value (k-v) cache optimization.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的研究通过开发一种新算法HERA解决了大语言模型（LLMs）优化中的关键挑战。LLMs日益复杂和庞大的规模需要创新的方法来提高存储和计算效率，特别是在向量数据库管理、权重量化和键值（k-v）缓存优化方面。
- en: HERA’s hierarchical approach to segmenting, compressing, and reorganizing the
    matrix dataset has proven to be effective in significantly reducing quantization
    error. By considering the distribution and magnitude of parameters, our method
    achieves superior performance compared to traditional uniform quantization techniques.
    The experimental results from our prototype system, implemented in Python, demonstrate
    that HERA can reduce the quantization error to 12.3% of the original, maintaining
    the same compression ratio.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: HERA在对矩阵数据集进行分段、压缩和重组的分层方法已被证明在显著减少量化误差方面有效。通过考虑参数的分布和幅度，我们的方法相比于传统的均匀量化技术实现了更优的性能。我们在Python中实现的原型系统的实验结果表明，HERA可以将量化误差减少到原始的12.3%，同时保持相同的压缩比。
- en: Future work will explore further refinements to HERA, including its application
    to other types of application and broader datasets. Additionally, investigating
    the integration of HERA with other optimization techniques may yield even greater
    enhancements in model performance and efficiency. The promising results of this
    study encourage continued research and development in the quest for more effective
    and scalable solutions for LLM compression and optimization.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的工作将探索对HERA的进一步改进，包括将其应用于其他类型的应用和更广泛的数据集。此外，研究将HERA与其他优化技术结合的可能性，可能会在模型性能和效率方面带来更大的提升。这项研究的有前景的结果鼓励了在LLM压缩和优化领域寻求更有效和可扩展解决方案的持续研究和开发。
- en: References
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Brown et al. (2020) Brown Tom, Mann Benjamin, Ryder Nick, Subbiah Melanie, Kaplan
    Jared D, Dhariwal Prafulla, Neelakantan Arvind, Shyam Pranav, Sastry Girish, Askell
    Amanda, others . Language models are few-shot learners // Advances in neural information
    processing systems. 2020\. 33\. 1877–1901.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等（2020）Brown Tom, Mann Benjamin, Ryder Nick, Subbiah Melanie, Kaplan Jared
    D, Dhariwal Prafulla, Neelakantan Arvind, Shyam Pranav, Sastry Girish, Askell
    Amanda, 其他人。语言模型是少样本学习者 // 神经信息处理系统进展。2020年。33。1877–1901。
- en: 'Chmiel et al. (2020) Chmiel Brian, Banner Ron, Shomron Gil, Nahshan Yury, Bronstein
    Alex, Weiser Uri, others . Robust quantization: One model to rule them all //
    Advances in neural information processing systems. 2020\. 33\. 5308–5317.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chmiel et al. (2020) **Chmiel Brian**, **Banner Ron**, **Shomron Gil**, **Nahshan
    Yury**, **Bronstein Alex**, **Weiser Uri**, 其他人。**鲁棒量化：一个模型统治所有** // **神经信息处理系统进展**。2020年。33。5308–5317。
- en: 'Cho et al. (2024) Cho Minsik, Rastegari Mohammad, Naik Devang. KV-Runahead:
    Scalable Causal LLM Inference by Parallel Key-Value Cache Generation // arXiv
    preprint arXiv:2405.05329\. 2024.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cho et al. (2024) **Cho Minsik**, **Rastegari Mohammad**, **Naik Devang**.
    **KV-Runahead: 通过并行生成键值缓存实现可扩展的因果LLM推断** // arXiv 预印本 [arXiv:2405.05329](https://arxiv.org/abs/2405.05329)。2024年。'
- en: 'Dettmers et al. (2022) Dettmers Tim, Lewis Mike, Belkada Younes, Zettlemoyer
    Luke. Llm. int8 (): 8-bit matrix multiplication for transformers at scale // CoRR
    abs/2208.07339\. 2022.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2022) **Dettmers Tim**, **Lewis Mike**, **Belkada Younes**,
    **Zettlemoyer Luke**. **Llm. int8 (): 用于大规模变换器的8位矩阵乘法** // CoRR [abs/2208.07339](https://arxiv.org/abs/2208.07339)。2022年。'
- en: Dettmers et al. (2021) Dettmers Tim, Lewis Mike, Shleifer Sam, Zettlemoyer Luke.
    8-bit optimizers via block-wise quantization // arXiv preprint arXiv:2110.02861\.
    2021.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2021) **Dettmers Tim**, **Lewis Mike**, **Shleifer Sam**, **Zettlemoyer
    Luke**. **通过块级量化的8位优化器** // arXiv 预印本 [arXiv:2110.02861](https://arxiv.org/abs/2110.02861)。2021年。
- en: 'Devlin et al. (2018) Devlin Jacob, Chang Ming-Wei, Lee Kenton, Toutanova Kristina.
    Bert: Pre-training of deep bidirectional transformers for language understanding
    // arXiv preprint arXiv:1810.04805\. 2018.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2018) **Devlin Jacob**, **Chang Ming-Wei**, **Lee Kenton**,
    **Toutanova Kristina**. **Bert: 深度双向变换器的预训练用于语言理解** // arXiv 预印本 [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)。2018年。'
- en: Fan et al. (2020) Fan Angela, Stock Pierre, Graham Benjamin, Grave Edouard,
    Gribonval Rémi, Jegou Herve, Joulin Armand. Training with quantization noise for
    extreme model compression // arXiv preprint arXiv:2004.07320\. 2020.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan et al. (2020) **Fan Angela**, **Stock Pierre**, **Graham Benjamin**, **Grave
    Edouard**, **Gribonval Rémi**, **Jegou Herve**, **Joulin Armand**. **通过量化噪声训练实现极端模型压缩**
    // arXiv 预印本 [arXiv:2004.07320](https://arxiv.org/abs/2004.07320)。2020年。
- en: Ge et al. (2013) Ge Tiezheng, He Kaiming, Ke Qifa, Sun Jian. Optimized product
    quantization // IEEE transactions on pattern analysis and machine intelligence.
    2013\. 36, 4\. 744–755.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. (2013) **Ge Tiezheng**, **He Kaiming**, **Ke Qifa**, **Sun Jian**.
    **优化产品量化** // **IEEE模式分析与机器智能汇刊**。2013年。36，4。744–755。
- en: 'Hooper et al. (2024) Hooper Coleman, Kim Sehoon, Mohammadzadeh Hiva, Mahoney
    Michael W, Shao Yakun Sophia, Keutzer Kurt, Gholami Amir. Kvquant: Towards 10
    million context length llm inference with kv cache quantization // arXiv preprint
    arXiv:2401.18079\. 2024.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hooper et al. (2024) **Hooper Coleman**, **Kim Sehoon**, **Mohammadzadeh Hiva**,
    **Mahoney Michael W**, **Shao Yakun Sophia**, **Keutzer Kurt**, **Gholami Amir**.
    **Kvquant: 通过KV缓存量化实现1000万上下文长度的LLM推断** // arXiv 预印本 [arXiv:2401.18079](https://arxiv.org/abs/2401.18079)。2024年。'
- en: Jegou et al. (2010) Jegou Herve, Douze Matthijs, Schmid Cordelia. Product quantization
    for nearest neighbor search // IEEE transactions on pattern analysis and machine
    intelligence. 2010\. 33, 1\. 117–128.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jegou et al. (2010) **Jegou Herve**, **Douze Matthijs**, **Schmid Cordelia**.
    **用于最近邻搜索的产品量化** // **IEEE模式分析与机器智能汇刊**。2010年。33，1。117–128。
- en: Johnson et al. (2019) Johnson Jeff, Douze Matthijs, Jégou Hervé. Billion-scale
    similarity search with GPUs // IEEE Transactions on Big Data. 2019\. 7, 3\. 535–547.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson et al. (2019) **Johnson Jeff**, **Douze Matthijs**, **Jégou Hervé**.
    **使用GPU进行十亿规模的相似性搜索** // **IEEE大数据汇刊**。2019年。7，3。535–547。
- en: Kalantidis, Avrithis (2014) Kalantidis Yannis, Avrithis Yannis. Locally optimized
    product quantization for approximate nearest neighbor search // Proceedings of
    the IEEE conference on computer vision and pattern recognition. 2014\. 2321–2328.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalantidis, Avrithis (2014) **Kalantidis Yannis**, **Avrithis Yannis**. **用于近似最近邻搜索的局部优化产品量化**
    // **IEEE计算机视觉与模式识别会议论文集**。2014年。2321–2328。
- en: LeCun et al. (1989) LeCun Yann, Denker John, Solla Sara. Optimal brain damage
    // Advances in neural information processing systems. 1989\. 2.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1989) **LeCun Yann**, **Denker John**, **Solla Sara**. **最佳脑损伤**
    // **神经信息处理系统进展**。1989年。2。
- en: Polino et al. (2018) Polino Antonio, Pascanu Razvan, Alistarh Dan. Model compression
    via distillation and quantization // arXiv preprint arXiv:1802.05668\. 2018.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polino et al. (2018) **Polino Antonio**, **Pascanu Razvan**, **Alistarh Dan**.
    **通过蒸馏和量化进行模型压缩** // arXiv 预印本 [arXiv:1802.05668](https://arxiv.org/abs/1802.05668)。2018年。
- en: Vaswani et al. (2017) Vaswani Ashish, Shazeer Noam, Parmar Niki, Uszkoreit Jakob,
    Jones Llion, Gomez Aidan N, Kaiser Łukasz, Polosukhin Illia. Attention is all
    you need // Advances in neural information processing systems. 2017\. 30.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) **Vaswani Ashish**, **Shazeer Noam**, **Parmar Niki**,
    **Uszkoreit Jakob**, **Jones Llion**, **Gomez Aidan N**, **Kaiser Łukasz**, **Polosukhin
    Illia**. **注意力机制就是全部你需要的** // **神经信息处理系统进展**。2017年。30。
- en: 'Wu et al. (2022) Wu Xiaoxia, Yao Zhewei, Zhang Minjia, Li Conglong, He Yuxiong.
    Xtc: Extreme compression for pre-trained transformers made simple and efficient
    // Advances in Neural Information Processing Systems. 2022\. 35\. 3217–3231.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人（2022）Wu Xiaoxia、Yao Zhewei、Zhang Minjia、Li Conglong、He Yuxiong。《Xtc:
    简化且高效的预训练变换器极限压缩》// 神经信息处理系统进展。2022年。35。3217–3231。'
- en: 'Zafrir et al. (2019) Zafrir Ofir, Boudoukh Guy, Izsak Peter, Wasserblat Moshe.
    Q8bert: Quantized 8bit bert // 2019 Fifth Workshop on Energy Efficient Machine
    Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). 2019\. 36–39.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zafrir 等人（2019）Zafrir Ofir、Boudoukh Guy、Izsak Peter、Wasserblat Moshe。《Q8bert:
    量化8位bert》// 2019年第五届节能机器学习与认知计算研讨会-NeurIPS 版（EMC2-NIPS）。2019年。36–39。'
