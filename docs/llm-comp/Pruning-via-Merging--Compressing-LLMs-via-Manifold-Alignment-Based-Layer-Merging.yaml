- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:52:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:52:17
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过合并进行剪枝：基于流形对齐的层合并
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.16330](https://ar5iv.labs.arxiv.org/html/2406.16330)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.16330](https://ar5iv.labs.arxiv.org/html/2406.16330)
- en: Deyuan Liu¹^✉, Zhanyue Qin¹, Hairu Wang⁴, Zhao Yang², Zecheng Wang¹,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 刘德元¹^✉，秦占岳¹，王海如⁴，杨赵²，王泽成¹，
- en: Fangying Rong⁵, Qingbin Liu³, Yanchao Hao³, Xi Chen³, Cunhang Fan⁶,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 方颖荣⁵，刘青彬³，郝彦超³，陈曦³，范存航⁶，
- en: Zhao Lv⁶, Zhiying Tu¹, Dianhui Chu¹, Dianbo Sui${}^{1}\textsuperscript{{\char
    12\relax}}$
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 赵绿⁶，涂志颖¹，朱电辉¹，隋殿博${}^{1}\textsuperscript{{\char 12\relax}}$
- en: ¹ Harbin Institute of Technology, ² Institute of automation, Chinese Academy
    of Sciences
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 哈尔滨工业大学，² 中国科学院自动化研究所
- en: ³ Tencent Inc , ⁴University of Science and Technology of China
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 腾讯公司，⁴中国科学技术大学
- en: ⁵ Shandong Agricultural University, ⁶ Anhui University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵山东农业大学，⁶安徽大学
- en: 2022211994@stu.hit.edu.cn, suidianbo@hit.edu.cn Dianbo Sui is the corresponding
    author.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 2022211994@stu.hit.edu.cn，suidianbo@hit.edu.cn 隋殿博为通讯作者。
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: While large language models (LLMs) excel in many domains, their complexity and
    scale challenge deployment in resource-limited environments. Current compression
    techniques, such as parameter pruning, often fail to effectively utilize the knowledge
    from pruned parameters. To address these challenges, we propose Manifold-Based
    Knowledge Alignment and Layer Merging Compression (MKA), a novel approach that
    uses manifold learning and the Normalized Pairwise Information Bottleneck (NPIB)
    measure to merge similar layers, reducing model size while preserving essential
    performance. We evaluate MKA on multiple benchmark datasets and various LLMs.
    Our findings show that MKA not only preserves model performance but also achieves
    substantial compression ratios, outperforming traditional pruning methods. Moreover,
    when coupled with quantization, MKA delivers even greater compression. Specifically,
    on the MMLU dataset using the Llama3-8B model, MKA achieves a compression ratio
    of 43.75% with a minimal performance decrease of only 2.82%. The proposed MKA
    method offers a resource-efficient and performance-preserving model compression
    technique for LLMs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大语言模型（LLMs）在许多领域表现出色，但它们的复杂性和规模在资源有限的环境中带来了挑战。当前的压缩技术，如参数剪枝，往往未能有效利用被剪枝参数中的知识。为应对这些挑战，我们提出了一种基于流形的知识对齐和层合并压缩（MKA）方法，这是一种新颖的方法，利用流形学习和归一化的成对信息瓶颈（NPIB）度量来合并相似的层，减少模型大小，同时保持基本性能。我们在多个基准数据集和各种LLM上评估了MKA。我们的发现表明，MKA不仅保持了模型性能，还实现了显著的压缩比，优于传统的剪枝方法。此外，当与量化结合使用时，MKA可以提供更大的压缩比。具体而言，在使用Llama3-8B模型的MMLU数据集上，MKA实现了43.75%的压缩比，性能下降仅为2.82%。所提的MKA方法为LLMs提供了一种资源高效且性能保持的模型压缩技术。
- en: 'Pruning via Merging: Compressing LLMs via'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过合并进行剪枝：通过
- en: Manifold Alignment Based Layer Merging
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于流形对齐的层合并
- en: 'Deyuan Liu¹^✉, Zhanyue Qin¹, Hairu Wang⁴, Zhao Yang², Zecheng Wang¹, Fangying
    Rong⁵, Qingbin Liu³, Yanchao Hao³, Xi Chen³, Cunhang Fan⁶, Zhao Lv⁶, Zhiying Tu¹,
    Dianhui Chu¹, Dianbo Sui${}^{1}\textsuperscript{{\char 12\relax}}$^†^†thanks:
    Dianbo Sui is the corresponding author. ¹ Harbin Institute of Technology, ² Institute
    of automation, Chinese Academy of Sciences ³ Tencent Inc , ⁴University of Science
    and Technology of China ⁵ Shandong Agricultural University, ⁶ Anhui University
    2022211994@stu.hit.edu.cn, suidianbo@hit.edu.cn'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 刘德元¹^✉，秦占岳¹，王海如⁴，杨赵²，王泽成¹，方颖荣⁵，刘青彬³，郝彦超³，陈曦³，范存航⁶，赵绿⁶，涂志颖¹，朱电辉¹，隋殿博${}^{1}\textsuperscript{{\char
    12\relax}}$^†^†致谢：隋殿博为通讯作者。¹ 哈尔滨工业大学，² 中国科学院自动化研究所，³ 腾讯公司，⁴中国科学技术大学，⁵山东农业大学，⁶安徽大学
    2022211994@stu.hit.edu.cn，suidianbo@hit.edu.cn
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs), such as GPT-4 OpenAI et al. ([2024](#bib.bib35)),
    Llama-3¹¹1[https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)
    , Llama-2 Touvron et al. ([2023](#bib.bib39)) and Mistral Jiang et al. ([2024](#bib.bib23)),
    have demonstrated remarkable proficiency in language understanding and generation.
    These models, with billions of parameters trained on trillions of tokens, can
    handle complex tasks and exhibit emergent abilities Brown et al. ([2020](#bib.bib4));
    Chowdhery et al. ([2023](#bib.bib6)). While these models have achieved unprecedented
    success, their growing complexity and scale have brought to the fore significant
    challenges in terms of computational resources, memory requirements, and energy
    consumption Bender et al. ([2021](#bib.bib1)); Bommasani et al. ([2021](#bib.bib3)),
    raising concerns about their sustainability.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），例如 GPT-4 OpenAI et al. ([2024](#bib.bib35))，Llama-3¹¹1[https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)，Llama-2
    Touvron et al. ([2023](#bib.bib39)) 和 Mistral Jiang et al. ([2024](#bib.bib23))，在语言理解和生成方面表现出显著的能力。这些模型拥有数十亿个参数，训练于数万亿个标记，可以处理复杂的任务，并展示出突现能力
    Brown et al. ([2020](#bib.bib4)); Chowdhery et al. ([2023](#bib.bib6))。虽然这些模型取得了前所未有的成功，但它们日益增长的复杂性和规模带来了计算资源、内存需求和能源消耗方面的重大挑战
    Bender et al. ([2021](#bib.bib1)); Bommasani et al. ([2021](#bib.bib3))，引发了对其可持续性的担忧。
- en: 'To mitigate these challenges, researchers have developed various model compression
    techniques in LLM to reduce its parameter size while preserving performance Cheng
    et al. ([2017](#bib.bib5)); Deng et al. ([2020](#bib.bib9)); Ganesh et al. ([2021](#bib.bib14));
    Zhu et al. ([2023](#bib.bib44)). These techniques can be roughly categorized into
    two main mainstreams Men et al. ([2024](#bib.bib34)): quantization Gholami et al.
    ([2021](#bib.bib15)); Li et al. ([2024](#bib.bib27)); Dettmers et al. ([2022](#bib.bib10));
    Gong et al. ([2024](#bib.bib16)); Li et al. ([2024](#bib.bib27)) and pruning LeCun
    et al. ([1989](#bib.bib25)); Han et al. ([2016](#bib.bib20)); Gupta and Agrawal
    ([2022](#bib.bib19)); Ma et al. ([2023a](#bib.bib32)). Quantization based methods
    aid in the reduction of the memory consumption of weights, activations, and KV
    caches by using the low-precision values with fewer bits instead of the high-precision
    values. However, the acceleration benefits of quantization are seriously dependent
    on hardware support Tao et al. ([2023](#bib.bib36)) and sometimes require additional
    fine-tuning to maintain performance Dettmers et al. ([2023](#bib.bib11)); Men
    et al. ([2024](#bib.bib34)). Compared to quantization, pruning, especially structural
    pruning Li et al. ([2017](#bib.bib26)), eliminates redundant LLM’s parameters
    to decrease the overall parameter count, and can be applied directly to a trained
    LLM without retraining and is generally more hardware-friendly than quantization
    approaches. While effective, pruning usually risks losing valuable model structures
    and determining how to prune the LLM with minimal disruption to the origin remains
    an unsolved problem Ma et al. ([2023b](#bib.bib33)).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这些挑战，研究人员开发了多种模型压缩技术来减少LLM的参数大小，同时保持性能 Cheng et al. ([2017](#bib.bib5));
    Deng et al. ([2020](#bib.bib9)); Ganesh et al. ([2021](#bib.bib14)); Zhu et al.
    ([2023](#bib.bib44))。这些技术大致可以分为两个主要流派 Men et al. ([2024](#bib.bib34))：量化 Gholami
    et al. ([2021](#bib.bib15)); Li et al. ([2024](#bib.bib27)); Dettmers et al. ([2022](#bib.bib10));
    Gong et al. ([2024](#bib.bib16)); Li et al. ([2024](#bib.bib27)) 和剪枝 LeCun et
    al. ([1989](#bib.bib25)); Han et al. ([2016](#bib.bib20)); Gupta and Agrawal ([2022](#bib.bib19));
    Ma et al. ([2023a](#bib.bib32))。基于量化的方法通过使用低精度的值来减少权重、激活值和KV缓存的内存消耗，这些低精度值的比特数少于高精度值。然而，量化的加速效益严重依赖于硬件支持
    Tao et al. ([2023](#bib.bib36))，有时需要额外的微调来保持性能 Dettmers et al. ([2023](#bib.bib11));
    Men et al. ([2024](#bib.bib34))。与量化相比，剪枝，特别是结构化剪枝 Li et al. ([2017](#bib.bib26))，通过消除冗余的LLM参数来减少总体参数数量，可以直接应用于训练好的LLM而无需重新训练，并且通常比量化方法更具硬件友好性。尽管有效，剪枝通常有丢失有价值模型结构的风险，并且确定如何在最小化对原始模型的干扰的情况下剪枝LLM仍然是一个未解决的问题
    Ma et al. ([2023b](#bib.bib33))。
- en: '![Refer to caption](img/bad8eca2b874c9112ccdbb18a3061f18.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bad8eca2b874c9112ccdbb18a3061f18.png)'
- en: 'Figure 1: Manifold-Based Knowledge Alignment and Layer Merging (MKA) framework
    consists of two main components: (1) The left side illustrates manifold learning
    for LLM knowledge extraction, where layer activations are transformed into low-dimensional
    manifolds using the Diffusion Kernel algorithm. (2) The right side depicts the
    similarity-based layer merging process, employing the NPIB metric to identify
    layers with aligned knowledge.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于流形的知识对齐与层合并（MKA）框架由两个主要组件组成：（1）左侧展示了用于LLM知识提取的流形学习，其中层激活通过扩散核算法转化为低维流形。（2）右侧描绘了基于相似度的层合并过程，采用NPIB度量来识别知识对齐的层。
- en: To tackle this issue head-on, we delve into the realm of model merging Wortsman
    et al. ([2022](#bib.bib41)), a powerful technique that seamlessly weaves together
    the strengths and knowledge of multiple models, creating a robust and efficient
    aggregation. This technique, through averaging the weights of multiple models
    with the same architecture, can retain essential features without significant
    additional resources Liu et al. ([2024](#bib.bib31)); Wan et al. ([2024](#bib.bib40)).
    Furthermore, by offsetting the biases and errors of individual models, model merging
    often leads to greatly improved performance Li et al. ([2023](#bib.bib28)). Additional,
    the number of models in the merging process can be gradually and naturally reduced.
    However, such a useful technology are limited to merging between models currently,
    and few studies pay attention on merging the same internal structures within a
    model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直接解决这个问题，我们深入探讨了模型合并领域Wortsman等人（[2022](#bib.bib41)），这是一种强大的技术，可以无缝地将多个模型的优点和知识编织在一起，创建一个强大而高效的聚合。通过平均具有相同架构的多个模型的权重，这种技术能够在不显著增加额外资源的情况下保留重要特征Liu等人（[2024](#bib.bib31)）；Wan等人（[2024](#bib.bib40)）。此外，通过抵消个别模型的偏差和错误，模型合并通常会显著提高性能Li等人（[2023](#bib.bib28)）。另外，合并过程中的模型数量可以逐渐自然减少。然而，目前这种有用的技术仅限于模型之间的合并，关于模型内部相同结构的合并的研究还很少。
- en: 'This raises the question of whether model compression could be achieved by
    reducing the total number of layers through the progressive aggregation of knowledge
    between layers. To answer this question, we introduce Manifold-Based Knowledge
    Alignment and Layer Merging Compression (MKA) in this paper. MKA combines manifold
    learning and layer merging to preserve essential information while significantly
    reducing LLM parameter size. As illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Pruning via Merging: Compressing LLMs via Manifold Alignment Based
    Layer Merging"), our method mainly comprises two primary components:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '这引出了一个问题，即通过逐步聚合层之间的知识是否可以实现模型压缩。为了解答这个问题，我们在本文中引入了基于流形的知识对齐与层合并压缩（MKA）。MKA结合了流形学习和层合并，旨在保留重要信息，同时显著减少LLM参数的大小。如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Pruning via Merging: Compressing LLMs via Manifold
    Alignment Based Layer Merging")所示，我们的方法主要包括两个主要组件：'
- en: 'Manifold Learning for LLM Knowledge: We employ manifold learning techniques
    to align knowledge across layers by extracting layer activations from a LLM and
    applying the Diffusion Kernel algorithm Tenenbaum et al. ([2000](#bib.bib37))
    to learn low-dimensional manifold representations. This approach captures the
    nonlinear structure in the activation and achieves dimensionality reduction while
    preserving important activation features, enabling more effective comparison of
    knowledge patterns across different layers.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LLM知识的流形学习：我们采用流形学习技术，通过提取LLM的层激活并应用Tenenbaum等人（[2000](#bib.bib37)）的扩散核算法来对齐跨层知识，从而学习低维流形表示。这种方法捕捉了激活中的非线性结构，实现了降维，同时保留了重要的激活特征，使得不同层之间的知识模式比较更加有效。
- en: 'Similarity Alignment Layer Merging: Following manifold learning, we use the
    Normalized Pairwise Information Bottleneck (NPIB) measure Tishby et al. ([2000](#bib.bib38))
    to construct a similarity matrix that quantifies the similarity between layers
    by maximizing their mutual information while considering the entropy of each layer.
    Based on this similarity matrix, we select the most similar layer pairs for merging.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度对齐层合并：在流形学习之后，我们使用标准化的成对信息瓶颈（NPIB）度量Tishby等人（[2000](#bib.bib38)）来构建一个相似度矩阵，该矩阵通过最大化层之间的互信息来量化层之间的相似度，同时考虑每一层的熵。基于该相似度矩阵，我们选择最相似的层对进行合并。
- en: To rigorously validate the effectiveness of MKA, we conduct extensive empirical
    evaluations on a diverse array of benchmark datasets, like MMLU and PIQA, and
    a wide range of state-of-the-art large language models, including Llama-3 series
    with 8B and 70B parameters, Llama-2 series with 7B and 13B parameters, and Mixtral-7B.
    Our experimental results indicate that MKA can maintain good performance while
    achieving a significant compression ratio, outperforming existing pruning methods
    and achieving even greater compression when combined with quantization. For example,
    on the MMLU dataset with Llama3-8B, MKA can achieve a compression ratio of 43.75%
    with only a 2.82% performance drop.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了严格验证MKA的有效性，我们在各种基准数据集（如MMLU和PIQA）和广泛的最先进的大型语言模型（包括具有8B和70B参数的Llama-3系列、具有7B和13B参数的Llama-2系列以及Mixtral-7B）上进行了广泛的实证评估。我们的实验结果表明，MKA可以在实现显著压缩比的同时保持良好的性能，超越了现有的剪枝方法，并在与量化结合时实现了更大的压缩。例如，在MMLU数据集上的Llama3-8B模型中，MKA可以在仅有2.82%的性能下降的情况下实现43.75%的压缩比。
- en: 'In summary, the main contributions of this paper are as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本文的主要贡献如下：
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce MKA, an innovative model compression technique that leverages manifold
    learning to align and integrate knowledge across layers, achieving significant
    reductions in model size while preserving performance.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了MKA，这是一种创新的模型压缩技术，通过流形学习对齐和整合各层知识，实现了模型大小的大幅度减少，同时保持了性能。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We develop a manifold-based knowledge alignment approach, utilizing the Diffusion
    Kernel and Normalized Pairwise Information Bottleneck (NPIB) to effectively capture
    and align similarities between layers in the parameter space.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们开发了一种基于流形的知识对齐方法，利用扩散核和归一化成对信息瓶颈（NPIB）来有效捕捉和对齐参数空间中层与层之间的相似性。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We validate the efficacy of MKA through comprehensive experiments on multiple
    benchmark datasets and a variety of large language models, demonstrating its capability
    to achieve substantial compression without compromising model performance.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过对多个基准数据集和各种大型语言模型进行全面实验，验证了MKA的有效性，展示了其在不影响模型性能的情况下实现显著压缩的能力。
- en: 2 Manifold-Based Knowledge Alignment and Layer Merging
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 基于流形的知识对齐和层合并
- en: Our MKA method relies on the redundancy present in the latter layers of post-training
    LLMs Gromov et al. ([2024](#bib.bib17)). By merging layers with high input-output
    similarity from back to front, we maintain the model’s performance while reducing
    its size. In this section, we first describe the extraction and dimensionality
    reduction processes for the intermediate states, as high-dimensional intermediate
    states are challenging to analyze. Then, we propose our layer merging method based
    on similarity alignment, which aims to maintain performance by aligning intermediate
    states through merging techniques.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的MKA方法依赖于后训练LLM的后层中的冗余 Gromov et al. ([2024](#bib.bib17))。通过从后向前合并具有高输入输出相似性的层，我们在减少模型大小的同时保持了模型的性能。在这一部分，我们首先描述了中间状态的提取和降维过程，因为高维中间状态难以分析。然后，我们提出了基于相似性对齐的层合并方法，旨在通过合并技术对齐中间状态，从而保持性能。
- en: 2.1 Manifold Learning for LLM Knowledge
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 流形学习在LLM知识中的应用
- en: To effectively align knowledge across LLM’s layers, MKA employs manifold learning
    techniques that can capture the intricate nonlinear dependencies within the LLM’s
    internal structure. This approach allows us to compare and align layer activations
    in a meaningful way, preserving essential information while reducing model complexity.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效对齐LLM各层的知识，MKA采用了流形学习技术，这些技术能够捕捉LLM内部结构中的复杂非线性依赖关系。这种方法使我们能够以有意义的方式比较和对齐层激活，保留重要信息的同时降低模型复杂性。
- en: 'The process begins with the extraction of layer activations ${H}^{l}$ . These
    activations represent the outputs of each layer given a set of input samples,
    encapsulating the knowledge learned at different stages. To transform these high-dimensional
    activations into a lower-dimensional space that preserves their essential features
    and geometric structure, we apply the Diffusion Kernel algorithm Coifman and Lafon
    ([2006](#bib.bib8)). Here are the key steps involved in this process:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程以层激活${H}^{l}$的提取开始。这些激活表示在给定一组输入样本时每层的输出，封装了在不同阶段学习到的知识。为了将这些高维激活转换为保留其本质特征和几何结构的低维空间，我们应用了扩散核算法
    Coifman 和 Lafon ([2006](#bib.bib8))。以下是该过程的关键步骤：
- en: 'Extracting Layer Activations: For each layer $l$ are computed using the following
    equation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 提取层激活：对于每一层 $l$，使用以下方程计算：
- en: '|  | $\displaystyle\mathbf{H}^{l}$ |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{H}^{l}$ |  |'
- en: '|  |  | $\displaystyle\quad+\text{FeedForward}\left(\mathbf{H}^{l-1}\right)$
    |  | (1) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad+\text{FeedForward}\left(\mathbf{H}^{l-1}\right)$
    |  | (1) |'
- en: 'Constructing the Pairwise Distance Matrix: Next, we calculate the pairwise
    Euclidean distance matrix $D$. This matrix captures the distances between all
    pairs of activations, serving as the basis for the manifold learning process.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 构建成对距离矩阵：接下来，我们计算成对的欧几里得距离矩阵 $D$。这个矩阵捕捉所有激活对之间的距离，作为流形学习过程的基础。
- en: 'Applying the Diffusion Kernel: We apply the Diffusion Kernel to transform the
    distance matrix $D$, capturing the intrinsic geometric structure of the data.
    The kernel function smooths the data, emphasizing the intrinsic geometric structure:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 应用扩散核：我们应用扩散核来转换距离矩阵 $D$，捕捉数据的内在几何结构。核函数平滑数据，强调内在几何结构：
- en: '|  | $\displaystyle\mathbf{E}$ |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{E}$ |  |'
- en: '|  |  | $\displaystyle\quad\left.-e^{-\left(\frac{\&#124;\mathbf{H}_{i}-\mathbf{H}_{j}\&#124;^{2}}{\sigma
    K}\right)^{0.5}}\right)$ |  | (2) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad\left.-e^{-\left(\frac{\&#124;\mathbf{H}_{i}-\mathbf{H}_{j}\&#124;^{2}}{\sigma
    K}\right)^{0.5}}\right)$ |  | (2) |'
- en: where $\sigma_{K}$. This transformation captures the essential features and
    relationships within the activations, enabling effective comparisons across different
    layers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma_{K}$。这种转换捕捉了激活中的基本特征和关系，使得不同层之间的比较更加有效。
- en: 2.2 Similarity-based Layer Merging
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 基于相似性的层合并
- en: Algorithm 1 Manifold-Based Knowledge Alignment and Layer Merging Compression
    (MKA)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 基于流形的知识对齐与层合并压缩（MKA）
- en: '1:Input: LLM $\mathcal{M}$'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 1：输入：LLM $\mathcal{M}$
- en: Building upon the manifold learning representations, MKA employs a similarity-based
    layer merging approach to identify and fuse layers with highly aligned knowledge.
    By quantifying the similarity between layers using the Normalized Pairwise Information
    Bottleneck (NPIB) Tishby et al. ([2000](#bib.bib38)) metric, we can determine
    which layers are most suitable for merging. This process allows us to reduce model
    size, improve inference speed, and decrease GPU memory consumption.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在流形学习表示的基础上，MKA 采用基于相似性的层合并方法来识别和融合具有高度对齐知识的层。通过使用标准化成对信息瓶颈（NPIB）Tishby 等人（[2000](#bib.bib38)）度量来量化层之间的相似性，我们可以确定哪些层最适合合并。这个过程使我们能够减少模型大小，提高推理速度，并降低
    GPU 内存消耗。
- en: The layer merging process involves several key steps. First, we construct a
    similarity matrix using the NPIB metric to compare the knowledge patterns across
    layers. Next, we introduce an adaptive weight allocation function to determine
    the optimal merging ratio for each pair of layers, ensuring that the merged layer
    retains the most critical features. Finally, we fuse the parameters of the selected
    layers using the weighted sum and update the model architecture accordingly.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 层合并过程涉及几个关键步骤。首先，我们使用 NPIB 度量构建相似性矩阵，以比较层之间的知识模式。接下来，我们引入一个自适应权重分配函数，以确定每对层的最佳合并比率，确保合并后的层保留最关键的特征。最后，我们使用加权和融合所选层的参数，并相应地更新模型架构。
- en: 'Constructing the Similarity Matrix: To identify layers suitable for merging,
    we first construct a similarity matrix $S$ using the Normalized Pairwise Information
    Bottleneck (NPIB) metric. NPIB quantifies the shared information between layers
    while normalizing their individual entropies, providing an ideal measure for comparing
    knowledge patterns across layers:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 构建相似性矩阵：为了识别适合合并的层，我们首先使用标准化成对信息瓶颈（NPIB）度量构建相似性矩阵 $S$。NPIB 量化层之间的共享信息，同时标准化它们的个体熵，提供了比较层之间知识模式的理想度量：
- en: '|  | $\displaystyle S_{ij}$ |  | (3) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle S_{ij}$ |  | (3) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: where $p(x,y)$, respectively. This similarity matrix helps us determine which
    layers have the most aligned knowledge representations.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p(x,y)$，分别表示。这种相似性矩阵帮助我们确定哪些层具有最对齐的知识表示。
- en: 'Calculate Weight ratio: To determine the merging ratio $\lambda_{m}$. This
    function dynamically adjusts the merging ratio based on the similarity differences
    between layers, ensuring that the merged layer retains the most critical features
    from each original layer:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 计算权重比：为了确定合并比率 $\lambda_{m}$。这个函数根据层之间的相似性差异动态调整合并比率，确保合并后的层保留每个原始层中最关键的特征：
- en: '|  | $\lambda_{m}=\Psi(\bar{s}i,\bar{s}j)=\frac{e^{\mathcal{S}{ij}}}{\sum{k\in\Omega}e^{\mathcal{S}_{k}}}$
    |  | (4) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\lambda_{m}=\Psi(\bar{s}i,\bar{s}j)=\frac{e^{\mathcal{S}{ij}}}{\sum{k\in\Omega}e^{\mathcal{S}_{k}}}$
    |  | (4) |'
- en: The adaptive weight allocation function $\Psi$ assigns a higher weight to the
    layer with higher similarity, reducing the weight of the layer with lower similarity.
    This mechanism ensures that the merged layer better preserves the knowledge from
    the more similar layer.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应权重分配函数 $\Psi$ 为相似度较高的层分配更高的权重，减少相似度较低的层的权重。该机制确保合并后的层更好地保留了来自相似层的知识。
- en: 'Merging Layer Parameters: Once the merging ratio $\lambda_{m}$ of the selected
    layers using a weighted sum:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 合并层参数：使用加权和来计算选定层的合并比 $\lambda_{m}$：
- en: '|  | $\widetilde{\mathbf{\theta}}_{m}=\lambda_{m}\mathbf{\theta}_{i}+(1-\lambda_{m})\mathbf{\theta}_{j}$
    |  | (5) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widetilde{\mathbf{\theta}}_{m}=\lambda_{m}\mathbf{\theta}_{i}+(1-\lambda_{m})\mathbf{\theta}_{j}$
    |  | (5) |'
- en: The merged layer $L_{m}$. This new layer integrates the aligned knowledge from
    the original layers, preserving essential information while reducing redundancy.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 合并后的层 $L_{m}$。这个新层整合了来自原始层的对齐知识，保留了关键信息，同时减少了冗余。
- en: Finally, we update the model $\mathcal{M}$. This step ensures that the model’s
    architecture is updated to reflect the compression process, maintaining performance
    while significantly reducing model size.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们更新模型 $\mathcal{M}$。这一步确保了模型的架构得到更新，以反映压缩过程，同时保持性能并显著减少模型大小。
- en: '![Refer to caption](img/c85fb268a10f895721f2aab3994f2f40.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c85fb268a10f895721f2aab3994f2f40.png)'
- en: 'Figure 2: Comparison of Accuracy (ACC) during merging and pruning on the MMLU
    dataset. MKA achieves higher compression ratios (approximately 43.5% for Llama3-8B,
    45% for Llama3-70B, 40% for Mistral-7B, 31.25% for Llama2-7B, and 57.5% for Llama2-13B)
    while preserving 90% performance. Please see the appendix [A](#A1 "Appendix A
    More Results ‣ Pruning via Merging: Compressing LLMs via Manifold Alignment Based
    Layer Merging") for details.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：在 MMLU 数据集上合并和修剪过程中的准确率（ACC）比较。MKA 实现了更高的压缩比（Llama3-8B 约为 43.5%，Llama3-70B
    为 45%，Mistral-7B 为 40%，Llama2-7B 为 31.25%，Llama2-13B 为 57.5%），同时保持了 90% 的性能。有关详细信息，请参见附录 [A](#A1
    "附录 A 更多结果 ‣ 通过合并进行修剪：通过流形对齐基础层合并压缩 LLMs")。
- en: 3 Experiments
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: We conduct a comprehensive set of experiments to evaluate the effectiveness
    and generalizability of our MKA method across various domains. Moreover, we aim
    to compare our approach with pruning techniques to assess whether it offers improvements
    and to investigate if it can be combined with quantization methods to achieve
    even higher compression ratios.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一系列全面的实验，以评估我们 MKA 方法在不同领域的有效性和通用性。此外，我们还旨在将我们的方法与修剪技术进行比较，以评估其是否提供了改进，并调查它是否可以与量化方法结合以实现更高的压缩比。
- en: 3.1 Experimental Setup
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: 3.1.1 Datasets
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 数据集
- en: We conduct evaluations using the MKA methods across various benchmark datasets,
    each specifically designed to test various facets of language comprehension and
    generation. In detail, MMLU Hendrycks et al. ([2020](#bib.bib21)) evaluates broad
    language understanding across a wide range of domains. PIQA Bisk et al. ([2020](#bib.bib2))
    is designed to test models on commonsense reasoning in the physical world, aiming
    to assess NLP models’ grasp of everyday physical interactions. HellaSwag Zellers
    et al. ([2019](#bib.bib43)) is a challenge dataset for commonsense natural language
    inference, consisting of event descriptions with multiple possible continuations,
    where the task is to select the most plausible one. RACE-H Lai et al. ([2017](#bib.bib24))
    is a large-scale reading comprehension dataset collected from English exams for
    Chinese high school students, featuring a high proportion of questions that require
    reasoning. BoolQ Clark et al. ([2019](#bib.bib7)) is a reading comprehension dataset
    focusing on naturally occurring yes/no questions that often query for complex,
    non-factoid information and require difficult entailment-like inference to answer
    correctly.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 MKA 方法在各种基准数据集上进行评估，每个数据集都专门设计用于测试语言理解和生成的不同方面。详细来说，MMLU Hendrycks et al.
    ([2020](#bib.bib21)) 评估了广泛领域的语言理解能力。PIQA Bisk et al. ([2020](#bib.bib2)) 旨在测试模型在物理世界中的常识推理，旨在评估
    NLP 模型对日常物理交互的掌握。HellaSwag Zellers et al. ([2019](#bib.bib43)) 是一个用于常识自然语言推理的挑战数据集，包括具有多个可能延续的事件描述，任务是选择最具可能性的一个。RACE-H
    Lai et al. ([2017](#bib.bib24)) 是一个大规模阅读理解数据集，收集自中国高中生的英语考试，包含大量需要推理的问题。BoolQ
    Clark et al. ([2019](#bib.bib7)) 是一个阅读理解数据集，专注于自然出现的“是/否”问题，这些问题通常查询复杂的非事实信息，并需要困难的蕴涵推理才能正确回答。
- en: 3.1.2 LLMs
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 LLMs
- en: 'In our experiments, we employ the Llama-2 Touvron et al. ([2023](#bib.bib39)),
    Llama-3, and Mistral-7B Jiang et al. ([2023](#bib.bib22)) models, each distinct
    in their capabilities and configurations: Llama-2: Encompassing models from 7
    billion to 70 billion parameters, exhibits superior performance and safety on
    diverse benchmarks. Llama-3: Featuring models with 8 billion and 70 billion parameters,
    Llama3 offers state-of-the-art performance and advanced reasoning capabilities.
    Mistral-7B: a 7-billion-parameter model that surpasses Llama-2 and Llama-1 in
    performance and efficiency, leveraging grouped-query and sliding window attention
    mechanisms for optimal inference across lengthy sequences.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们使用了 Llama-2 Touvron et al. ([2023](#bib.bib39))、Llama-3 和 Mistral-7B
    Jiang et al. ([2023](#bib.bib22)) 模型，每种模型在能力和配置上都有所不同：Llama-2：涵盖从 70 亿到 700 亿参数的模型，在各种基准测试中展现出卓越的性能和安全性。Llama-3：具有
    80 亿和 700 亿参数的模型，Llama3 提供了最先进的性能和高级推理能力。Mistral-7B：一个 70 亿参数的模型，其性能和效率超越了 Llama-2
    和 Llama-1，利用分组查询和滑动窗口注意力机制，以在较长序列中实现最佳推理。
- en: 3.1.3 Baselines
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 Baselines
- en: 'In this study, we assess the effectiveness of our proposed method, MKA, through
    two distinct comparative analyses. Firstly, we evaluate MKA directly against several
    well-established pruning techniques to gauge its standalone efficacy in reducing
    model size while maintaining performance. Secondly, we extend the comparison to
    include scenarios where both the traditional pruning methods and MKA are further
    enhanced through quantization. The baseline methods included in our analysis are:
    SparseGPT Frantar and Alistarh ([2023](#bib.bib12)): An efficient one-shot pruning
    method that can induce high sparsity levels in large language models with billions
    of parameters while preserving accuracy, by reducing the pruning problem to a
    set of large-scale sparse regression instances solved by a novel approximate solver.
    ShortGPT Men et al. ([2024](#bib.bib34)): A pruning method that removes redundant
    layers from large language models based on a Block Influence metric, which assesses
    the significance of each layer. Reverse Pruning: A heuristic approach where the
    importance of layers is considered inversely proportional to their order in the
    model, prioritizing the retention of earlier layers. SmoothQuant Xiao et al. ([2023](#bib.bib42)):
    SmoothQuant is a training-free post-training quantization solution that enables
    efficient 8-bit weight and activation quantization for large language models,
    offering up to 1.56× speedup and 2× memory reduction with minimal accuracy loss.
    GPTQ Frantar et al. ([2022](#bib.bib13)): A one-shot weight quantization method
    that uses approximate second-order information to maintain high accuracy even
    with severe weight reduction. AWQ Lin et al. ([2023](#bib.bib30)): A novel quantization
    approach that protects salient weights by adjusting per-channel scaling based
    on activation observations rather than weight Magnitudes.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们通过两种不同的对比分析评估了我们提出的方法MKA的有效性。首先，我们直接将MKA与几种成熟的剪枝技术进行对比，以评估其在减少模型大小同时保持性能方面的单独效果。其次，我们将比较扩展到包括传统剪枝方法和MKA通过量化进一步增强的情景。我们分析中包含的基线方法有：SparseGPT
    Frantar和Alistarh（[2023](#bib.bib12)）：一种高效的一次性剪枝方法，能够在保持准确性的同时，在具有数十亿参数的大型语言模型中引入高稀疏性，通过将剪枝问题简化为一组大型稀疏回归实例，并由一种新型的近似求解器解决。ShortGPT
    Men等（[2024](#bib.bib34)）：一种剪枝方法，通过块影响度指标从大型语言模型中删除冗余层，该指标评估每一层的重要性。逆向剪枝：一种启发式方法，其中层的重要性与其在模型中的顺序成反比，优先保留早期层。SmoothQuant
    Xiao等（[2023](#bib.bib42)）：SmoothQuant是一种无训练的后训练量化解决方案，能够实现大型语言模型的高效8位权重和激活量化，提供高达1.56×的加速和2×的内存减少，同时精度损失最小。GPTQ
    Frantar等（[2022](#bib.bib13)）：一种一次性权重量化方法，利用近似的二阶信息即使在严重减重的情况下也能保持高准确性。AWQ Lin等（[2023](#bib.bib30)）：一种新型量化方法，通过基于激活观察而非权重幅度调整每通道缩放来保护显著权重。
- en: 3.2 In what ways does MKA surpass conventional pruning techniques?
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 MKA在何种方面超越了传统剪枝技术？
- en: 'We compare the performance of MKA with baseline compression methods on the
    MMLU dataset using the Llama3-8B, Llama3-70B, Mistral-7B, Llama2-7B, and Llama2-13B
    models. The evaluation metric is Accuracy (ACC) during merging and pruning. The
    results are presented in Figure [2](#S2.F2 "Figure 2 ‣ 2.2 Similarity-based Layer
    Merging ‣ 2 Manifold-Based Knowledge Alignment and Layer Merging ‣ Pruning via
    Merging: Compressing LLMs via Manifold Alignment Based Layer Merging").'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在MMLU数据集上使用Llama3-8B、Llama3-70B、Mistral-7B、Llama2-7B和Llama2-13B模型对MKA与基线压缩方法的性能进行了比较。评估指标是合并和剪枝过程中的准确率（ACC）。结果展示在图[2](#S2.F2
    "图 2 ‣ 2.2 基于相似性的层合并 ‣ 2 流形知识对齐与层合并 ‣ 通过合并的剪枝：通过流形对齐基础的层合并压缩LLMs")中。
- en: 'We compare the performance of MKA with baseline compression methods on the
    MMLU dataset using the Llama3-8B, Llama3-70B, Mistral-7B, Llama2-7B, and Llama2-13B
    models. The evaluation metrics include Accuracy (ACC) during merging and pruning.
    The results are presented in Figure [2](#S2.F2 "Figure 2 ‣ 2.2 Similarity-based
    Layer Merging ‣ 2 Manifold-Based Knowledge Alignment and Layer Merging ‣ Pruning
    via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging"). We
    can observe that, across all models, our method improves the compression ratio
    while maintaining performance. Specifically, the compression ratio²²2Note that,
    the compression ratio is calculated as: $\left(L_{\text{total}}-\left(\frac{L_{\text{retained}}}{Q}\right)\right)/L_{\text{total}}$
    is the quantization factor. for Llama3-8B reach 43.5%, for Mistral-7B it reaches
    40%, and for Llama2-13B it reaches an impressive 57.5%. Additionally, we observe
    several phenomena: both methods experience a collapse in model performance, but
    the model merging method can delay the layer collapse to some extent and stabilize
    the model’s performance very well. Since our strategy is based on Reverse Prune,
    the scores for the Llama3-8B, Llama2-7B, and Llama2-13B models are very close
    to the Reverse Prune. Our hypothesis is that the pruning or merging of these models
    is similar, but model merging can adjust the merging ratio to surpass the effect
    of pruning. Moreover, for the Llama3-70B and Mistral-7B models, we noticed that
    the results do not closely match the Reverse Prune.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 MMLU 数据集上使用 Llama3-8B、Llama3-70B、Mistral-7B、Llama2-7B 和 Llama2-13B 模型比较了
    MKA 与基准压缩方法的性能。评估指标包括合并和修剪过程中的准确率（ACC）。结果展示在图 [2](#S2.F2 "Figure 2 ‣ 2.2 Similarity-based
    Layer Merging ‣ 2 Manifold-Based Knowledge Alignment and Layer Merging ‣ Pruning
    via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging")中。我们可以观察到，在所有模型中，我们的方法在保持性能的同时提高了压缩比。具体来说，Llama3-8B
    的压缩比达到 43.5%，Mistral-7B 达到 40%，Llama2-13B 达到令人印象深刻的 57.5%。此外，我们观察到几个现象：两种方法都经历了模型性能的下降，但模型合并方法可以在一定程度上延迟层的崩溃，并很好地稳定模型的性能。由于我们的策略基于反向修剪，Llama3-8B、Llama2-7B
    和 Llama2-13B 模型的得分非常接近反向修剪。我们的假设是，这些模型的修剪或合并是类似的，但模型合并可以调整合并比例，超越修剪的效果。此外，对于 Llama3-70B
    和 Mistral-7B 模型，我们注意到结果与反向修剪的匹配程度不高。'
- en: '| Model | Method | Retained layers (Compression Ratio) | Acc |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 保留层（压缩比） | 准确率 |'
- en: '| Llama3-8B | Vanilla Model | 32 (0.00%) | 66.29 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B | 原始模型 | 32 (0.00%) | 66.29 |'
- en: '| ShortGPT+Smooth | 18(85.94%) | 26.54 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ShortGPT+Smooth | 18(85.94%) | 26.54 |'
- en: '| ShortGPT+GPTQ | 18(85.94%) | 25.98 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ShortGPT+GPTQ | 18(85.94%) | 25.98 |'
- en: '| ShortGPT+AWQ | 18(85.94%) | 26.22 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| ShortGPT+AWQ | 18(85.94%) | 26.22 |'
- en: '| MKA (Ours) + Smooth | 18(85.94%) | 64.20 (+37.66) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| MKA (我们的) + Smooth | 18(85.94%) | 64.20 (+37.66) |'
- en: '| MKA (Ours) + GPTQ | 18(85.94%) | 62.98 (+37.00) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| MKA (我们的) + GPTQ | 18(85.94%) | 62.98 (+37.00) |'
- en: '| MKA (Ours) + AWQ | 18(85.94%) | 61.66 (+35.44) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| MKA (我们的) + AWQ | 18(85.94%) | 61.66 (+35.44) |'
- en: '| Mistral-7B | Vanilla Model | 32(0.00%) | 63.87 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 原始模型 | 32(0.00%) | 63.87 |'
- en: '| ShortGPT+Smooth | 20(84.38%) | 24.32 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ShortGPT+Smooth | 20(84.38%) | 24.32 |'
- en: '| ShortGPT+GPTQ | 20(84.38%) | 23.16 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| ShortGPT+GPTQ | 20(84.38%) | 23.16 |'
- en: '| ShortGPT+AWQ | 20(84.38%) | 23.96 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ShortGPT+AWQ | 20(84.38%) | 23.96 |'
- en: '| MKA (Ours) + Smooth | 20(84.38%) | 56.92 (+32.60) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| MKA (我们的) + Smooth | 20(84.38%) | 56.92 (+32.60) |'
- en: '| MKA (Ours) + GPTQ | 20(84.38%) | 56.12 (+32.96) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| MKA (我们的) + GPTQ | 20(84.38%) | 56.12 (+32.96) |'
- en: '| MKA (Ours) + AWQ | 20(84.38%) | 55.34 (+31.38) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| MKA (我们的) + AWQ | 20(84.38%) | 55.34 (+31.38) |'
- en: '| Llama2-7B | Vanilla Model | 32(0.00%) | 46.67 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B | 原始模型 | 32(0.00%) | 46.67 |'
- en: '| ShortGPT+Smooth | 16(87.50%) | 25.67 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| ShortGPT+Smooth | 16(87.50%) | 25.67 |'
- en: '| ShortGPT+GPTQ | 16(87.50%) | 25.82 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| ShortGPT+GPTQ | 16(87.50%) | 25.82 |'
- en: '| ShortGPT+AWQ | 16(87.50%) | 26.01 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| ShortGPT+AWQ | 16(87.50%) | 26.01 |'
- en: '| MKA (Ours) + Smooth | 16(87.50%) | 35.66 (+9.99) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| MKA (我们的) + Smooth | 16(87.50%) | 35.66 (+9.99) |'
- en: '| MKA (Ours) + GPTQ | 16(87.50%) | 35.91 (+10.09) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| MKA (我们的) + GPTQ | 16(87.50%) | 35.91 (+10.09) |'
- en: '| MKA (Ours) + AWQ | 16(87.50%) | 36.23 (+10.22) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| MKA (我们的) + AWQ | 16(87.50%) | 36.23 (+10.22) |'
- en: '| Llama2-13B | Vanilla Model | 40 (0.00%) | 55.62 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13B | 原始模型 | 40 (0.00%) | 55.62 |'
- en: '| ShortGPT+Smooth | 20 (87.50%) | 25.89 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ShortGPT+Smooth | 20 (87.50%) | 25.89 |'
- en: '| ShortGPT+GPTQ | 20 (87.50%) | 25.35 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ShortGPT+GPTQ | 20 (87.50%) | 25.35 |'
- en: '| ShortGPT+AWQ | 20 (87.50%) | 23.83 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ShortGPT+AWQ | 20 (87.50%) | 23.83 |'
- en: '| MKA (Ours) + Smooth | 20 (87.50%) | 46.82 (+20.93) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| MKA (我们的) + Smooth | 20 (87.50%) | 46.82 (+20.93) |'
- en: '| MKA (Ours) + GPTQ | 20 (87.50%) | 45.44 (+20.09) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| MKA (我们的) + GPTQ | 20 (87.50%) | 45.44 (+20.09) |'
- en: '| MKA (Ours) + AWQ | 20 (87.50%) | 45.86 (+22.03) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| MKA (我们的) + AWQ | 20 (87.50%) | 45.86 (+22.03) |'
- en: 'Table 1: Performance comparison of MKA and ShortGPT pruning with quantization
    (SmoothQuant, GPTQ, AWQ) on MMLU using Llama3-8B, Mistral-7B, Llama2-7B, and Llama2-13B.
    MKA outperforms ShortGPT in accuracy across all models and quantization methods
    at similar compression ratios with int4\. The calculation of the compression ratio
    only considers the number of hidden layers in the model without considering the
    embedding layer.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：MKA 和 ShortGPT 修剪与量化（SmoothQuant、GPTQ、AWQ）在 MMLU 上的性能比较，使用 Llama3-8B、Mistral-7B、Llama2-7B
    和 Llama2-13B。MKA 在所有模型和量化方法中，在相似的压缩比下的准确度超过了 ShortGPT，且使用 int4。压缩比的计算仅考虑模型中的隐藏层数量，不考虑嵌入层。
- en: '|  | Compression Ratio = 34.375% | Compression Ratio = 37.5% |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | 压缩比 = 34.375% | 压缩比 = 37.5% |'
- en: '| Method | MMLU | PIQA | HellaSwag | RACE-H | BoolQ | MMLU | PIQA | HellaSwag
    | RACE-H | BoolQ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | MMLU | PIQA | HellaSwag | RACE-H | BoolQ | MMLU | PIQA | HellaSwag |
    RACE-H | BoolQ |'
- en: '| Vanilla Model | 66.29 | 81.12 | 74.54 | 66.07 | 66.79 | 66.29 | 81.12 | 74.54
    | 66.07 | 66.79 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 原始模型 | 66.29 | 81.12 | 74.54 | 66.07 | 66.79 | 66.29 | 81.12 | 74.54 | 66.07
    | 66.79 |'
- en: '| SparseGPT | 44.45 | 58.77 | 32.14 | 35.06 | 48.29 | 41.95 | 56.23 | 28.63
    | 37.84 | 52.40 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 44.45 | 58.77 | 32.14 | 35.06 | 48.29 | 41.95 | 56.23 | 28.63
    | 37.84 | 52.40 |'
- en: '| ShortGPT | 42.95 | 60.99 | 33.00 | 41.68 | 51.96 | 44.80 | 61.70 | 38.69
    | 40.05 | 57.09 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ShortGPT | 42.95 | 60.99 | 33.00 | 41.68 | 51.96 | 44.80 | 61.70 | 38.69
    | 40.05 | 57.09 |'
- en: '| MKA (Ours) | 64.87 | 67.79 | 51.32 | 55.20 | 63.36 | 62.05 | 66.26 | 50.16
    | 49.49 | 63.46 |  |  | Compression Ratio = 40.625% | Compression Ratio = 43.75%
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| MKA（我们） | 64.87 | 67.79 | 51.32 | 55.20 | 63.36 | 62.05 | 66.26 | 50.16 |
    49.49 | 63.46 |  |  | 压缩比 = 40.625% | 压缩比 = 43.75% |'
- en: '| Method | MMLU | PIQA | HellaSwag | RACE-H | BoolQ | MMLU | PIQA | HellaSwag
    | RACE-H | BoolQ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | MMLU | PIQA | HellaSwag | RACE-H | BoolQ | MMLU | PIQA | HellaSwag |
    RACE-H | BoolQ |'
- en: '| Vanilla Model | 66.29 | 81.12 | 74.54 | 66.07 | 66.79 | 66.29 | 81.12 | 74.54
    | 66.07 | 66.79 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 原始模型 | 66.29 | 81.12 | 74.54 | 66.07 | 66.79 | 66.29 | 81.12 | 74.54 | 66.07
    | 66.79 |'
- en: '| SparseGPT | 37.30 | 59.36 | 32.16 | 22.06 | 60.63 | 33.11 | 57.12 | 29.63
    | 23.14 | 59.41 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 37.30 | 59.36 | 32.16 | 22.06 | 60.63 | 33.11 | 57.12 | 29.63
    | 23.14 | 59.41 |'
- en: '| ShortGPT | 39.26 | 58.22 | 34.16 | 21.70 | 61.77 | 26.09 | 59.03 | 33.75
    | 21.58 | 61.53 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ShortGPT | 39.26 | 58.22 | 34.16 | 21.70 | 61.77 | 26.09 | 59.03 | 33.75
    | 21.58 | 61.53 |'
- en: '| MKA (Ours) | 63.42 | 65.61 | 48.83 | 55.26 | 63.58 | 64.42 | 65.51 | 45.10
    | 45.91 | 62.14 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| MKA（我们） | 63.42 | 65.61 | 48.83 | 55.26 | 63.58 | 64.42 | 65.51 | 45.10 |
    45.91 | 62.14 |'
- en: 'Table 2: Comparison of different methods across MMLU, PIQA, HellaSwag, RACE-H,
    and BoolQ datasets at different compression ratios.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同方法在 MMLU、PIQA、HellaSwag、RACE-H 和 BoolQ 数据集上在不同压缩比下的比较。
- en: '![Refer to caption](img/77cd54f7085758fabc54004d9e98554c.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/77cd54f7085758fabc54004d9e98554c.png)'
- en: 'Figure 3: Similarity matrices for Llama-3-8B, Llama-3-70B, Mistral-7B, Llama-2-7B,
    and Llama-2-13B before and after MKA. Later layers show high similarity, supporting
    layer merging.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：Llama-3-8B、Llama-3-70B、Mistral-7B、Llama-2-7B 和 Llama-2-13B 在 MKA 之前和之后的相似性矩阵。后面的层显示出较高的相似性，支持层合并。
- en: 3.3 How Does MKA Combined with Quantization Perform Compared to Pruning Combined
    with Quantization?
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 MKA 结合量化的表现与修剪结合量化相比如何？
- en: 'We compare the performance of MKA with the baseline pruning method, ShortGPT Men
    et al. ([2024](#bib.bib34)), on the MMLU dataset using the Llama3-8B, Llama3-70B,
    Mistral-7B, Llama2-7B, and Llama2-13B models. The results are shown in Table [1](#S3.T1
    "Table 1 ‣ 3.2 In what ways does MKA surpass conventional pruning techniques?
    ‣ 3 Experiments ‣ Pruning via Merging: Compressing LLMs via Manifold Alignment
    Based Layer Merging").'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 MMLU 数据集上使用 Llama3-8B、Llama3-70B、Mistral-7B、Llama2-7B 和 Llama2-13B 模型对比了
    MKA 与基线修剪方法 ShortGPT Men et al. ([2024](#bib.bib34)) 的表现。结果见表 [1](#S3.T1 "Table
    1 ‣ 3.2 In what ways does MKA surpass conventional pruning techniques? ‣ 3 Experiments
    ‣ Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging")。'
- en: We can see that the pruned models are able to be further quantized and maintain
    performance with a higher compression ratio. Notably, at a high compression ratio
    of around 87.50%, MKA significantly outperforms ShortGPT. Additionally, we achieve
    excellent results with various quantization methods. For example, on Llama3-8B,
    at a compression ratio of 85.94%, MKA with SmoothQuant achieves 64.20%, far exceeding
    ShortGPT with SmoothQuant at 37.66%. Similarly, with the GPTQ quantization method,
    we achieve 62.98%, surpassing ShortGPT’s 37.00%, and with AWQ, we achieve 61.66%,
    exceeding ShortGPT’s 35.44%.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，经过修剪的模型能够进一步量化并在更高的压缩比下保持性能。值得注意的是，在约 87.50% 的高压缩比下，MKA 显著优于 ShortGPT。此外，我们在各种量化方法下都取得了优异的结果。例如，在
    Llama3-8B 上，压缩比为 85.94% 时，MKA 使用 SmoothQuant 达到 64.20%，远超使用 SmoothQuant 的 ShortGPT
    37.66%。类似地，使用 GPTQ 量化方法时，我们达到了 62.98%，超过了 ShortGPT 的 37.00%，而使用 AWQ 时，我们达到了 61.66%，超越了
    ShortGPT 的 35.44%。
- en: 3.4 MKA vs. Other Pruning Methods on varies benchmarks
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 MKA 与其他修剪方法在不同基准上的对比
- en: 'We compared the performance of MKA and several other pruning methods on the
    LLama3-8B model using multiple benchmark datasets at compression ratios of 34.375%,
    37.5%, 40.625% and 43.75%. The results are shown in Table [2](#S3.T2 "Table 2
    ‣ 3.2 In what ways does MKA surpass conventional pruning techniques? ‣ 3 Experiments
    ‣ Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging").
    From the results, merging can retain performance better compared to pruning. Relative
    to SparseGPT and ShortGPT, our method can achieve better performance retention,
    with significant improvements across all datasets. For example, at a compression
    ratio of 34.375% on the MMLU dataset, our method can outperform ShortGPT by 21.92%
    and SparseGPT by 20.42%. Similarly, on the HellaSwag dataset, our proposed method
    can surpass ShortGPT by 18.32% and SparseGPT by 18.32%.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用多个基准数据集，在压缩比为 34.375%、37.5%、40.625% 和 43.75% 的情况下，比较了 MKA 与其他几种修剪方法在 LLama3-8B
    模型上的性能。结果见表 [2](#S3.T2 "Table 2 ‣ 3.2 In what ways does MKA surpass conventional
    pruning techniques? ‣ 3 Experiments ‣ Pruning via Merging: Compressing LLMs via
    Manifold Alignment Based Layer Merging")。结果表明，相较于修剪，合并能更好地保留性能。与 SparseGPT 和 ShortGPT
    相比，我们的方法在所有数据集上都能实现更好的性能保留。例如，在 MMLU 数据集上，压缩比为 34.375% 时，我们的方法可以比 ShortGPT 提高
    21.92%，比 SparseGPT 提高 20.42%。类似地，在 HellaSwag 数据集上，我们提出的方法可以超过 ShortGPT 18.32%
    和 SparseGPT 18.32%。'
- en: 3.5 Are Inter-Layer Knowledge Alignment Similarity Matrices Consistent Across
    Different Large Models?
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 各大模型间的层间知识对齐相似性矩阵是否一致？
- en: 'We generate layer similarity heatmaps for different models before and after
    applying MKA. These heatmaps visualize the knowledge alignment and layer merging
    effects of MKA on various models. Figure [3](#S3.F3 "Figure 3 ‣ 3.2 In what ways
    does MKA surpass conventional pruning techniques? ‣ 3 Experiments ‣ Pruning via
    Merging: Compressing LLMs via Manifold Alignment Based Layer Merging") presents
    the similarity heatmaps for Llama-3-8B, Llama-3-70B, Mistral-7B, Llama-2-7B, and
    Llama-2-13B. We observe that the heatmaps for the later layers of each model exhibit
    high similarity values, indicating that inter-layer similarity is consistently
    high in the later layers across different models. This observation supports our
    layer merging approach. Additionally, when merging the earlier layers, we notice
    a collapse of the matrix in the final figure, suggesting that earlier layers have
    a significant influence on later layers. Thus, simple merging operations on the
    earlier layers of the model are not feasible.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '我们生成了在应用 MKA 前后不同模型的层相似性热图。这些热图可视化了 MKA 对各种模型的知识对齐和层合并效果。图 [3](#S3.F3 "Figure
    3 ‣ 3.2 In what ways does MKA surpass conventional pruning techniques? ‣ 3 Experiments
    ‣ Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging")
    展示了 Llama-3-8B、Llama-3-70B、Mistral-7B、Llama-2-7B 和 Llama-2-13B 的相似性热图。我们观察到每个模型的后期层热图显示出高相似性值，表明不同模型的后期层之间的层间相似性始终很高。这一观察结果支持我们的层合并方法。此外，当合并早期层时，我们注意到最终图中的矩阵崩溃，表明早期层对后期层有显著影响。因此，对模型的早期层进行简单的合并操作是不可行的。'
- en: 4 Discussion
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 讨论
- en: 4.1 Extension to Multimodal and Specialized Models
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 扩展到多模态和专用模型
- en: '![Refer to caption](img/98b24ed0c0c3387e740662a2e194a87d.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/98b24ed0c0c3387e740662a2e194a87d.png)'
- en: 'Figure 4: The similarity matrix of Mixtral-8x7B and Jamba model.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：Mixtral-8x7B 和 Jamba 模型的相似性矩阵。
- en: '![Refer to caption](img/399d4aa312586e20f80a6270674685e8.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/399d4aa312586e20f80a6270674685e8.png)'
- en: 'Figure 5: Similarity matrices for various measures in the Llama3-8B model,
    showing different patterns and effectiveness in capturing layer relationships,
    with none fully matching the expected merging patterns.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：Llama3-8B 模型中各种度量的相似度矩阵，展示了不同的模式和捕捉层关系的效果，没有一种完全匹配预期的合并模式。
- en: 'In addition to its application to large language models, the MKA method shows
    promising potential for broader adoption across a variety of deep learning architectures.
    This includes Mixture-of-Experts (MoE) Jiang et al. ([2024](#bib.bib23)), and
    Mamba Gu and Dao ([2023](#bib.bib18)); Lieber et al. ([2024](#bib.bib29)) models,
    which can exhibit similar redundancies in their processing layers.The results
    show in Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Extension to Multimodal and Specialized
    Models ‣ 4 Discussion ‣ Pruning via Merging: Compressing LLMs via Manifold Alignment
    Based Layer Merging"). Initial experiments conducted on these diverse architectures
    have reinforced the viability of our approach. For instance, the similarity matrices
    generated on jamba Lieber et al. ([2024](#bib.bib29)) and Mixtral-8x7B Jiang et al.
    ([2024](#bib.bib23)) applying MKA have shown that Our method can also be generalized
    to other similar models, but the similarity distributions of jamba and Mixtral-8x7B
    are slightly different from LLM, and we do not yet know the reason. These experiments
    further validates the effectiveness of our method across different model types.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在大语言模型中的应用外，MKA 方法在更广泛的深度学习架构中展示了有前景的潜力。这包括专家混合模型（MoE）Jiang 等人 ([2024](#bib.bib23))
    和 Mamba Gu 和 Dao ([2023](#bib.bib18))；Lieber 等人 ([2024](#bib.bib29)) 模型，这些模型在其处理层中可能表现出类似的冗余。结果如图
    [4](#S4.F4 "图 4 ‣ 4.1 扩展到多模态和专业化模型 ‣ 4 讨论 ‣ 通过合并进行修剪：通过流形对齐的层合并压缩 LLM") 所示。对这些不同架构进行的初步实验进一步巩固了我们方法的可行性。例如，对
    jamba Lieber 等人 ([2024](#bib.bib29)) 和 Mixtral-8x7B Jiang 等人 ([2024](#bib.bib23))
    应用 MKA 生成的相似度矩阵表明，我们的方法也可以推广到其他类似模型，但 jamba 和 Mixtral-8x7B 的相似度分布与 LLM 略有不同，我们尚不清楚原因。这些实验进一步验证了我们方法在不同模型类型中的有效性。
- en: 4.2 Analysis of Similarity Measures
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 相似度度量分析
- en: 'In our evaluation of the Llama3-8B model, we explored several similarity measures:
    Cosine Similarity, Mahalanobis Distance, Euclidean Distance, t-SNE Similarity,
    and Autoencoder Similarity. The similarity matrices are shown in Figure [5](#S4.F5
    "Figure 5 ‣ 4.1 Extension to Multimodal and Specialized Models ‣ 4 Discussion
    ‣ Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging").
    From the results, we observe that Cosine Similarity, Mahalanobis Distance, and
    Euclidean Distance display similar distribution patterns with vertical stripes
    and varied heat values. However, Mahalanobis Distance shows irregular heat values
    within these stripes, indicating a misalignment with the fused layer data structure.
    t-SNE Similarity appears random and lacks consistent patterns. For Autoencoder
    Similarity, the high heat values do not correspond to suitable merging areas or
    expected high-similarity regions.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对 Llama3-8B 模型的评估中，我们探索了几种相似度度量：余弦相似度、马氏距离、欧几里得距离、t-SNE 相似度和自编码器相似度。相似度矩阵如图
    [5](#S4.F5 "图 5 ‣ 4.1 扩展到多模态和专业化模型 ‣ 4 讨论 ‣ 通过合并进行修剪：通过流形对齐的层合并压缩 LLM") 所示。从结果来看，我们观察到余弦相似度、马氏距离和欧几里得距离显示出类似的分布模式，具有垂直条纹和不同的热值。然而，马氏距离在这些条纹内显示出不规则的热值，表明与融合层数据结构不对齐。t-SNE
    相似度表现出随机性且缺乏一致的模式。对于自编码器相似度，高热值并不对应于合适的合并区域或预期的高相似度区域。
- en: 4.3 Variations in Accuracy Across Different MMLU Subjects During Layer Merging
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 层合并过程中不同 MMLU 主题的准确率变化
- en: '![Refer to caption](img/f8eab98c23e1d101ffbc2da3648630a9.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f8eab98c23e1d101ffbc2da3648630a9.png)'
- en: 'Figure 6: Different MMLU dataset subjects ACC change during merging.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：不同 MMLU 数据集主题在合并过程中 ACC 的变化。
- en: 'We examine the impact of model merging on performance across various academic
    subjects in the MMLU benchmark. Figure [6](#S4.F6 "Figure 6 ‣ 4.3 Variations in
    Accuracy Across Different MMLU Subjects During Layer Merging ‣ 4 Discussion ‣
    Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging")
    shows the accuracy changes across subjects such as College Medicine, College Biology,
    High School Psychology, and College Physics during different stages of merging
    model layers. From our results, we observe that High School Psychology maintained
    a stable accuracy with only minor fluctuations, suggesting a consistent performance
    and low sensitivity to the merging process. In contrast, College Biology experiences
    a significant drop in accuracy at the 12.5% merging ratio, followed by a recovery.
    College Physics exhibits frequent fluctuations in accuracy, pointing to a high
    sensitivity to layer merging. Conversely, College Medicine experiences a steady
    increase in performance with only minor variations.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在MMLU基准测试中考察了模型合并对各种学术科目性能的影响。图[6](#S4.F6 "Figure 6 ‣ 4.3 Variations in Accuracy
    Across Different MMLU Subjects During Layer Merging ‣ 4 Discussion ‣ Pruning via
    Merging: Compressing LLMs via Manifold Alignment Based Layer Merging")展示了在不同阶段合并模型层时，各科目如大学医学、大学生物学、高中心理学和大学物理的准确率变化。根据我们的结果，我们观察到高中心理学在合并过程中保持了稳定的准确率，仅有小幅波动，表明其性能一致且对合并过程的敏感性低。相比之下，大学生物学在12.5%合并比率时准确率显著下降，随后有所回升。大学物理的准确率频繁波动，指出其对层合并的敏感性较高。相反，大学医学的性能稳步提升，变化仅为小幅。'
- en: 5 Conclusion
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we have proposed Manifold-Based Knowledge Alignment and Layer
    Merging Compression (MKA), a novel model compression technique specifically designed
    to efficiently reduce the size of large language models (LLMs) while maintaining
    their performance. MKA leverage manifold learning techniques to align knowledge
    across layers and utilizes the Normalized Pairwise Information Bottleneck (NPIB)
    measure to identify the most similar layers for merging. By capturing the intricate
    nonlinear dependencies within LLMs and integrating knowledge from similar layers,
    MKA achieves remarkable compression ratios without sacrificing model accuracy.
    We have conducted extensive experiments on a diverse set of benchmark datasets
    and various state-of-the-art LLMs to rigorously evaluate the effectiveness of
    MKA in preserving model performance while significantly reducing model size. Our
    empirical results demonstrate that MKA consistently outperforms existing pruning
    methods and can achieve even higher compression ratios when combined with quantization
    techniques.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了基于流形的知识对齐与层合并压缩（MKA），这是一种新型模型压缩技术，旨在高效地减少大型语言模型（LLMs）的大小，同时保持其性能。MKA利用流形学习技术对齐各层知识，并采用归一化成对信息瓶颈（NPIB）度量来识别最相似的层进行合并。通过捕捉LLMs中复杂的非线性依赖关系，并整合相似层的知识，MKA实现了显著的压缩比，而不会牺牲模型准确性。我们在多样的基准数据集和各种先进的LLMs上进行了广泛实验，严格评估了MKA在保持模型性能的同时显著减少模型大小的有效性。我们的实证结果表明，MKA在性能上始终优于现有的剪枝方法，并且在结合量化技术时可以实现更高的压缩比。
- en: Limitations
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: The quality of the manifold learning process in MKA heavily depends on the diversity
    and representativeness of the layer activations extracted from the input dataset.
    In our experiments, we used $\sigma$ value of 8 and selected the first question
    from the 57-question MMLU dataset to extract activations. We observed that the
    number of questions sampled can significantly impact the manifold learning results.
    Ensuring the Condition Number remains below 2000 is crucial for maintaining the
    integrity of the learned manifold representations. If the dataset used for extracting
    activations does not adequately cover the model’s operational range, the learned
    manifold representations might fail to capture the true geometric structure of
    the data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: MKA中流形学习过程的质量在很大程度上依赖于从输入数据集中提取的层激活的多样性和代表性。在我们的实验中，我们使用了$\sigma$值为8，并选择了57题MMLU数据集中的第一个问题来提取激活。我们观察到采样问题的数量可以显著影响流形学习结果。确保条件数保持在2000以下对于维持所学习流形表示的完整性至关重要。如果用于提取激活的数据集未能充分覆盖模型的操作范围，那么学习到的流形表示可能无法捕捉数据的真实几何结构。
- en: The current implementation of MKA has been primarily tested on transformer-based
    architectures. Although we believe that deep neural networks inherently contain
    redundancies, the applicability and effectiveness of MKA on other neural network
    architectures, such as convolutional neural networks (CNNs) or recurrent neural
    networks (RNNs), have not been thoroughly explored. Future research can investigate
    these architectures to confirm whether MKA can achieve similar compression benefits
    across different types of neural networks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当前 MKA 的实现主要在基于变换器的架构上进行了测试。虽然我们相信深度神经网络本质上包含冗余，但 MKA 在其他神经网络架构（如卷积神经网络（CNNs）或递归神经网络（RNNs））上的适用性和有效性尚未彻底探讨。未来的研究可以调查这些架构，以确认
    MKA 是否能在不同类型的神经网络中实现类似的压缩收益。
- en: References
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bender et al. (2021) Emily M Bender, Timnit Gebru, Angelina McMillan-Major,
    and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language
    models be too big? In *Proceedings of the 2021 ACM conference on fairness, accountability,
    and transparency*, pages 610–623.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bender et al. (2021) Emily M Bender, Timnit Gebru, Angelina McMillan-Major,
    和 Shmargaret Shmitchell. 2021. 关于随机鹦鹉的危险：语言模型会不会太大？收录于 *2021年 ACM 公平性、问责制与透明度会议论文集*，页610–623。
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    2020. Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, 等.
    2020. Piqa: 在自然语言中推理物理常识。收录于 *AAAI 人工智能会议论文集*，第34卷，页7432–7439。'
- en: Bommasani et al. (2021) Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman,
    Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut,
    Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models.
    *arXiv preprint arXiv:2108.07258*.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bommasani et al. (2021) Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman,
    Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut,
    Emma Brunskill, 等. 2021. 关于基础模型的机遇和风险。*arXiv 预印本 arXiv:2108.07258*。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等. 2020. 语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901。
- en: Cheng et al. (2017) Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2017. A survey
    of model compression and acceleration for deep neural networks. *arXiv preprint
    arXiv:1710.09282*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. (2017) Yu Cheng, Duo Wang, Pan Zhou, 和 Tao Zhang. 2017. 深度神经网络模型压缩和加速的调查。*arXiv
    预印本 arXiv:1710.09282*。
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.
    *Journal of Machine Learning Research*, 24(240):1–113.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, 等. 2023. Palm: 通过路径扩展语言建模。*机器学习研究杂志*，24(240):1–113。'
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising
    difficulty of natural yes/no questions. *arXiv preprint arXiv:1905.10044*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, 和 Kristina Toutanova. 2019. Boolq: 探索自然是/否问题的惊人难度。*arXiv 预印本
    arXiv:1905.10044*。'
- en: Coifman and Lafon (2006) Ronald R Coifman and Stéphane Lafon. 2006. Diffusion
    maps. *Applied and computational harmonic analysis*, 21(1):5–30.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coifman and Lafon (2006) Ronald R Coifman 和 Stéphane Lafon. 2006. 扩散映射。*应用与计算谐波分析*，21(1):5–30。
- en: 'Deng et al. (2020) Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie.
    2020. Model compression and hardware acceleration for neural networks: A comprehensive
    survey. *Proceedings of the IEEE*, 108(4):485–532.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng et al. (2020) Lei Deng, Guoqi Li, Song Han, Luping Shi, 和 Yuan Xie. 2020.
    模型压缩和神经网络的硬件加速：综合调查。*IEEE 会议录*，108(4):485–532。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. [Llm.int8(): 8-bit matrix multiplication for transformers at scale](https://arxiv.org/abs/2208.07339).
    *Preprint*, arXiv:2208.07339.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    2022. [Llm.int8(): 适用于大规模变换器的 8 位矩阵乘法](https://arxiv.org/abs/2208.07339)。*预印本*，arXiv:2208.07339。'
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. [Qlora: Efficient finetuning of quantized llms](https://arxiv.org/abs/2305.14314).
    *Preprint*, arXiv:2305.14314.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等 (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer.
    2023. [QLora：高效的量化 LLM 微调](https://arxiv.org/abs/2305.14314)。*预印本*，arXiv:2305.14314。
- en: 'Frantar and Alistarh (2023) Elias Frantar and Dan Alistarh. 2023. Sparsegpt:
    Massive language models can be accurately pruned in one-shot. In *International
    Conference on Machine Learning*, pages 10323–10337\. PMLR.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 和 Alistarh (2023) Elias Frantar 和 Dan Alistarh. 2023. SparseGPT：大规模语言模型可以被准确地一刀切剪枝。发表于
    *国际机器学习大会*，页码 10323–10337。PMLR。
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等 (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh.
    2022. GPTQ：生成预训练变换器的准确后训练量化。*arXiv 预印本 arXiv:2210.17323*。
- en: 'Ganesh et al. (2021) Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan,
    Yin Yang, Hassan Sajjad, Preslav Nakov, Deming Chen, and Marianne Winslett. 2021.
    Compressing large-scale transformer-based models: A case study on bert. *Transactions
    of the Association for Computational Linguistics*, 9:1061–1080.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ganesh 等 (2021) Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang,
    Hassan Sajjad, Preslav Nakov, Deming Chen, 和 Marianne Winslett. 2021. 压缩大规模基于
    Transformer 的模型：以 BERT 为例。*计算语言学协会汇刊*，9：1061–1080。
- en: Gholami et al. (2021) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W.
    Mahoney, and Kurt Keutzer. 2021. [A survey of quantization methods for efficient
    neural network inference](https://arxiv.org/abs/2103.13630). *Preprint*, arXiv:2103.13630.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gholami 等 (2021) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W.
    Mahoney, 和 Kurt Keutzer. 2021. [高效神经网络推理的量化方法综述](https://arxiv.org/abs/2103.13630)。*预印本*，arXiv:2103.13630。
- en: Gong et al. (2024) Zhuocheng Gong, Jiahao Liu, Jingang Wang, Xunliang Cai, Dongyan
    Zhao, and Rui Yan. 2024. [What makes quantization for large language models hard?
    an empirical study from the lens of perturbation](https://arxiv.org/abs/2403.06408).
    *Preprint*, arXiv:2403.06408.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong 等 (2024) Zhuocheng Gong, Jiahao Liu, Jingang Wang, Xunliang Cai, Dongyan
    Zhao, 和 Rui Yan. 2024. [什么让大语言模型的量化变得困难？从扰动的角度进行的实证研究](https://arxiv.org/abs/2403.06408)。*预印本*，arXiv:2403.06408。
- en: Gromov et al. (2024) Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo
    Glorioso, and Daniel A Roberts. 2024. The unreasonable ineffectiveness of the
    deeper layers. *arXiv preprint arXiv:2403.17887*.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gromov 等 (2024) Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso,
    和 Daniel A Roberts. 2024. 更深层的无效性。*arXiv 预印本 arXiv:2403.17887*。
- en: 'Gu and Dao (2023) Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence
    modeling with selective state spaces. *arXiv preprint arXiv:2312.00752*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 和 Dao (2023) Albert Gu 和 Tri Dao. 2023. Mamba：线性时间序列建模与选择性状态空间。*arXiv 预印本
    arXiv:2312.00752*。
- en: 'Gupta and Agrawal (2022) Manish Gupta and Puneet Agrawal. 2022. Compression
    of deep learning models for text: A survey. *ACM Transactions on Knowledge Discovery
    from Data (TKDD)*, 16(4):1–55.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 和 Agrawal (2022) Manish Gupta 和 Puneet Agrawal. 2022. 深度学习模型的文本压缩：综述。*ACM
    数据知识发现汇刊 (TKDD)*，16(4)：1–55。
- en: 'Han et al. (2016) Song Han, Huizi Mao, and William J. Dally. 2016. [Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding](https://arxiv.org/abs/1510.00149). *Preprint*, arXiv:1510.00149.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2016) Song Han, Huizi Mao, 和 William J. Dally. 2016. [深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络](https://arxiv.org/abs/1510.00149)。*预印本*，arXiv:1510.00149。
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. *arXiv preprint arXiv:2009.03300*.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song, 和 Jacob Steinhardt. 2020. 测量大规模多任务语言理解。*arXiv 预印本 arXiv:2009.03300*。
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, 等. 2023. Mistral 7b。*arXiv 预印本 arXiv:2310.06825*。
- en: Jiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. *arXiv preprint
    arXiv:2401.04088*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, 等人。2024. 专家混合。*arXiv 预印本 arXiv:2401.04088*。
- en: 'Lai et al. (2017) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard
    Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations.
    *arXiv preprint arXiv:1704.04683*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lai et al. (2017) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, 和 Eduard
    Hovy. 2017. Race: 大规模阅读理解数据集来自考试。*arXiv 预印本 arXiv:1704.04683*。'
- en: LeCun et al. (1989) Yann LeCun, John Denker, and Sara Solla. 1989. [Optimal
    brain damage](https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 2\. Morgan-Kaufmann.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1989) Yann LeCun, John Denker, 和 Sara Solla. 1989. [优化脑损伤](https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf)。在
    *神经信息处理系统进展* 中，第 2 卷。Morgan-Kaufmann。
- en: Li et al. (2017) Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter
    Graf. 2017. [Pruning filters for efficient convnets](https://arxiv.org/abs/1608.08710).
    *Preprint*, arXiv:1608.08710.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2017) Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, 和 Hans Peter
    Graf. 2017. [剪枝过滤器以提高高效的卷积网络](https://arxiv.org/abs/1608.08710)。*预印本*，arXiv:1608.08710。
- en: Li et al. (2024) Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng
    Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. 2024. [Evaluating quantized
    large language models](https://arxiv.org/abs/2402.18158). *Preprint*, arXiv:2402.18158.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2024) Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng
    Shi, Shengen Yan, Guohao Dai, Huazhong Yang, 和 Yu Wang. 2024. [评估量化的大型语言模型](https://arxiv.org/abs/2402.18158)。*预印本*，arXiv:2402.18158。
- en: 'Li et al. (2023) Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, and
    Li Shen. 2023. Deep model fusion: A survey. *arXiv preprint arXiv:2309.15698*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, 和 Li
    Shen. 2023. 深度模型融合：综述。*arXiv 预印本 arXiv:2309.15698*。
- en: 'Lieber et al. (2024) Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan
    Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz,
    et al. 2024. Jamba: A hybrid transformer-mamba language model. *arXiv preprint
    arXiv:2403.19887*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lieber et al. (2024) Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan
    Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz,
    等人。2024. Jamba: 一种混合变换器-曼巴语言模型。*arXiv 预印本 arXiv:2403.19887*。'
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. 2023. Awq: Activation-aware weight quantization for llm compression
    and acceleration. *arXiv preprint arXiv:2306.00978*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. 2023. Awq: 激活感知权重量化用于大型语言模型的压缩和加速。*arXiv 预印本 arXiv:2306.00978*。'
- en: Liu et al. (2024) Deyuan Liu, Zecheng Wang, Bingning Wang, Weipeng Chen, Chunshan
    Li, Zhiying Tu, Dianhui Chu, Bo Li, and Dianbo Sui. 2024. Checkpoint merging via
    bayesian optimization in llm pretraining. *arXiv preprint arXiv:2403.19390*.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024) Deyuan Liu, Zecheng Wang, Bingning Wang, Weipeng Chen, Chunshan
    Li, Zhiying Tu, Dianhui Chu, Bo Li, 和 Dianbo Sui. 2024. 通过贝叶斯优化合并检查点在大型语言模型预训练中。*arXiv
    预印本 arXiv:2403.19390*。
- en: 'Ma et al. (2023a) Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023a. [Llm-pruner:
    On the structural pruning of large language models](https://arxiv.org/abs/2305.11627).
    *Preprint*, arXiv:2305.11627.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma et al. (2023a) Xinyin Ma, Gongfan Fang, 和 Xinchao Wang. 2023a. [Llm-pruner:
    大型语言模型的结构化剪枝](https://arxiv.org/abs/2305.11627)。*预印本*，arXiv:2305.11627。'
- en: 'Ma et al. (2023b) Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023b. Llm-pruner:
    On the structural pruning of large language models. *Advances in neural information
    processing systems*, 36:21702–21720.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma et al. (2023b) Xinyin Ma, Gongfan Fang, 和 Xinchao Wang. 2023b. Llm-pruner:
    大型语言模型的结构化剪枝。*神经信息处理系统进展*，36:21702–21720。'
- en: 'Men et al. (2024) Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin,
    Yaojie Lu, Xianpei Han, and Weipeng Chen. 2024. Shortgpt: Layers in large language
    models are more redundant than you expect. *arXiv preprint arXiv:2403.03853*.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Men et al. (2024) Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin,
    Yaojie Lu, Xianpei Han, 和 Weipeng Chen. 2024. Shortgpt: 大型语言模型中的层比你预期的更冗余。*arXiv
    预印本 arXiv:2403.03853*。'
- en: OpenAI et al. (2024) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,
    Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,
    Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
    Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
    Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek
    Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey
    Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
    Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,
    Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
    Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
    Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane
    Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,
    Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon
    Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
    Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,
    Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,
    Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina
    Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
    Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,
    Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul
    Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
    Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli,
    Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,
    John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
    Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,
    Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
    Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
    Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and
    Barret Zoph. 2024. [Gpt-4 technical report](https://arxiv.org/abs/2303.08774).
    *Preprint*, arXiv:2303.08774.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等人（2024）**OpenAI**，Josh Achiam，Steven Adler，Sandhini Agarwal，Lama Ahmad，Ilge
    Akkaya，Florencia Leoni Aleman，Diogo Almeida，Janko Altenschmidt，Sam Altman，Shyamal
    Anadkat，Red Avila，Igor Babuschkin，Suchir Balaji，Valerie Balcom，Paul Baltescu，Haiming
    Bao，Mohammad Bavarian，Jeff Belgum，Irwan Bello，Jake Berdine，Gabriel Bernadett-Shapiro，Christopher
    Berner，Lenny Bogdonoff，Oleg Boiko，Madelaine Boyd，Anna-Luisa Brakman，Greg Brockman，Tim
    Brooks，Miles Brundage，Kevin Button，Trevor Cai，Rosie Campbell，Andrew Cann，Brittany
    Carey，Chelsea Carlson，Rory Carmichael，Brooke Chan，Che Chang，Fotis Chantzis，Derek
    Chen，Sully Chen，Ruby Chen，Jason Chen，Mark Chen，Ben Chess，Chester Cho，Casey Chu，Hyung
    Won Chung，Dave Cummings，Jeremiah Currier，Yunxing Dai，Cory Decareaux，Thomas Degry，Noah
    Deutsch，Damien Deville，Arka Dhar，David Dohan，Steve Dowling，Sheila Dunning，Adrien
    Ecoffet，Atty Eleti，Tyna Eloundou，David Farhi，Liam Fedus，Niko Felix，Simón Posada
    Fishman，Juston Forte，Isabella Fulford，Leo Gao，Elie Georges，Christian Gibson，Vik
    Goel，Tarun Gogineni，Gabriel Goh，Rapha Gontijo-Lopes，Jonathan Gordon，Morgan Grafstein，Scott
    Gray，Ryan Greene，Joshua Gross，Shixiang Shane Gu，Yufei Guo，Chris Hallacy，Jesse
    Han，Jeff Harris，Yuchen He，Mike Heaton，Johannes Heidecke，Chris Hesse，Alan Hickey，Wade
    Hickey，Peter Hoeschele，Brandon Houghton，Kenny Hsu，Shengli Hu，Xin Hu，Joost Huizinga，Shantanu
    Jain，Shawn Jain，Joanne Jang，Angela Jiang，Roger Jiang，Haozhun Jin，Denny Jin，Shino
    Jomoto，Billie Jonn，Heewoo Jun，Tomer Kaftan，Łukasz Kaiser，Ali Kamali，Ingmar Kanitscheider，Nitish
    Shirish Keskar，Tabarak Khan，Logan Kilpatrick，Jong Wook Kim，Christina Kim，Yongjik
    Kim，Jan Hendrik Kirchner，Jamie Kiros，Matt Knight，Daniel Kokotajlo，Łukasz Kondraciuk，Andrew
    Kondrich，Aris Konstantinidis，Kyle Kosic，Gretchen Krueger，Vishal Kuo，Michael Lampe，Ikai
    Lan，Teddy Lee，Jan Leike，Jade Leung，Daniel Levy，Chak Ming Li，Rachel Lim，Molly Lin，Stephanie
    Lin，Mateusz Litwin，Theresa Lopez，Ryan Lowe，Patricia Lue，Anna Makanju，Kim Malfacini，Sam
    Manning，Todor Markov，Yaniv Markovski，Bianca Martin，Katie Mayer，Andrew Mayne，Bob
    McGrew，Scott Mayer McKinney，Christine McLeavey，Paul McMillan，Jake McNeil，David
    Medina，Aalok Mehta，Jacob Menick，Luke Metz，Andrey Mishchenko，Pamela Mishkin，Vinnie
    Monaco，Evan Morikawa，Daniel Mossing，Tong Mu，Mira Murati，Oleg Murk，David Mély，Ashvin
    Nair，Reiichiro Nakano，Rajeev Nayak，Arvind Neelakantan，Richard Ngo，Hyeonwoo Noh，Long
    Ouyang，Cullen O’Keefe，Jakub Pachocki，Alex Paino，Joe Palermo，Ashley Pantuliano，Giambattista
    Parascandolo，Joel Parish，Emy Parparita，Alex Passos，Mikhail Pavlov，Andrew Peng，Adam
    Perelman，Filipe de Avila Belbute Peres，Michael Petrov，Henrique Ponde de Oliveira
    Pinto，Michael Pokorny，Michelle Pokrass，Vitchyr H. Pong，Tolly Powell，Alethea Power，Boris
    Power，Elizabeth Proehl，Raul Puri，Alec Radford，Jack Rae，Aditya Ramesh，Cameron Raymond，Francis
    Real，Kendra Rimbach，Carl Ross，Bob Rotsted，Henri Roussez，Nick Ryder，Mario Saltarelli，Ted
    Sanders，Shibani Santurkar，Girish Sastry，Heather Schmidt，David Schnurr，John Schulman，Daniel
    Selsam，Kyla Sheppard，Toki Sherbakov，Jessica Shieh，Sarah Shoker，Pranav Shyam，Szymon
    Sidor，Eric Sigler，Maddie Simens，Jordan Sitkin，Katarina Slama，Ian Sohl，Benjamin
    Sokolowsky，Yang Song，Natalie Staudacher，Felipe Petroski Such，Natalie Summers，Ilya
    Sutskever，Jie Tang，Nikolas Tezak，Madeleine B. Thompson，Phil Tillet，Amin Tootoonchian，Elizabeth
    Tseng，Preston Tuggle，Nick Turley，Jerry Tworek，Juan Felipe Cerón Uribe，Andrea Vallone，Arun
    Vijayvergiya，Chelsea Voss，Carroll Wainwright，Justin Jay Wang，Alvin Wang，Ben Wang，Jonathan
    Ward，Jason Wei，CJ Weinmann，Akila Welihinda，Peter Welinder，Jiayi Weng，Lilian Weng，Matt
    Wiethoff，Dave Willner，Clemens Winter，Samuel Wolrich，Hannah Wong，Lauren Workman，Sherwin
    Wu，Jeff Wu，Michael Wu，Kai Xiao，Tao Xu，Sarah Yoo，Kevin Yu，Qiming Yuan，Wojciech
    Zaremba，Rowan Zellers，Chong Zhang，Marvin Zhang，Shengjia Zhao，Tianhao Zheng，Juntang
    Zhuang，William Zhuk，以及Barret Zoph。2024年。[GPT-4技术报告](https://arxiv.org/abs/2303.08774)。*预印本*，arXiv:2303.08774。
- en: 'Tao et al. (2023) Chaofan Tao, Lu Hou, Haoli Bai, Jiansheng Wei, Xin Jiang,
    Qun Liu, Ping Luo, and Ngai Wong. 2023. [Structured pruning for efficient generative
    pre-trained language models](https://doi.org/10.18653/v1/2023.findings-acl.692).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    10880–10895, Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tao et al. (2023) 陶超凡、侯璐、白浩力、魏建生、姜欣、刘群、罗平和黄伟。2023年。[高效生成预训练语言模型的结构化剪枝](https://doi.org/10.18653/v1/2023.findings-acl.692)。见于*计算语言学协会年会发现：ACL
    2023*，页10880–10895，加拿大多伦多。计算语言学协会。
- en: Tenenbaum et al. (2000) Joshua B Tenenbaum, Vin de Silva, and John C Langford.
    2000. A global geometric framework for nonlinear dimensionality reduction. *science*,
    290(5500):2319–2323.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tenenbaum et al. (2000) 约书亚·B·特农鲍姆、文·德·西尔瓦和约翰·C·朗福德。2000年。用于非线性降维的全球几何框架。*科学*，290(5500):2319–2323。
- en: Tishby et al. (2000) Naftali Tishby, Fernando C Pereira, and William Bialek.
    2000. The information bottleneck method. *arXiv preprint physics/0004057*.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tishby et al. (2000) 纳夫塔利·提什比、费尔南多·C·佩雷拉和威廉·比亚雷克。2000年。信息瓶颈方法。*arXiv 预印本 physics/0004057*。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023. [Llama: Open and efficient foundation language models](https://arxiv.org/abs/2302.13971).
    *Preprint*, arXiv:2302.13971.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) 于戈·图夫龙、蒂博·拉夫里尔、高蒂耶·伊扎卡德、泽维尔·马尔蒂内、玛丽-安·拉肖、蒂莫西·拉克鲁瓦、巴普蒂斯特·罗齐埃、南曼·戈亚尔、埃里克·汉布罗、法伊萨尔·阿扎尔、奥雷利安·罗德里格斯、阿尔芒·朱利安、爱德华·格雷夫和吉约姆·朗普勒。2023年。[Llama：开放且高效的基础语言模型](https://arxiv.org/abs/2302.13971)。*预印本*，arXiv:2302.13971。
- en: Wan et al. (2024) Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi,
    and Shuming Shi. 2024. [Knowledge fusion of large language models](https://arxiv.org/abs/2401.10491).
    *Preprint*, arXiv:2401.10491.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan et al. (2024) 万凡奇、黄新婷、邓才、全晓军、毕伟和施书铭。2024年。[大规模语言模型的知识融合](https://arxiv.org/abs/2401.10491)。*预印本*，arXiv:2401.10491。
- en: 'Wortsman et al. (2022) Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre,
    Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi,
    Yair Carmon, Simon Kornblith, et al. 2022. Model soups: averaging weights of multiple
    fine-tuned models improves accuracy without increasing inference time. In *International
    Conference on Machine Learning*, pages 23965–23998\. PMLR.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wortsman et al. (2022) 米切尔·沃茨曼、加布里埃尔·伊尔哈科、萨米尔·雅·加德雷、丽贝卡·罗洛夫斯、拉斐尔·贡蒂霍-洛佩斯、阿里·S·莫尔科斯、洪锡克·南孔、阿里·法赫迪、亚伊尔·卡门、西蒙·科恩布利斯等。2022年。模型汤：平均多个微调模型的权重提高了准确性而不增加推理时间。见于*国际机器学习大会*，页23965–23998。PMLR。
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization
    for large language models. In *International Conference on Machine Learning*,
    pages 38087–38099\. PMLR.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. (2023) 萧广轩、李骥、米卡埃尔·塞兹内克、吴浩、朱利安·德穆斯和宋汉。2023年。Smoothquant：针对大规模语言模型的准确且高效的后训练量化。见于*国际机器学习大会*，页38087–38099。PMLR。
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? *arXiv
    preprint arXiv:1905.07830*.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. (2019) 罗温·泽勒斯、阿里·霍尔茨曼、约纳坦·比斯克、阿里·法赫迪和叶锦潮。2019年。Hellaswag: 机器真的可以完成你的句子吗？*arXiv
    预印本 arXiv:1905.07830*。'
- en: Zhu et al. (2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023.
    [A survey on model compression for large language models](https://arxiv.org/abs/2308.07633).
    *Preprint*, arXiv:2308.07633.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2023) 朱勋宇、李剑、刘勇、马灿和王伟平。2023年。[大规模语言模型的模型压缩调查](https://arxiv.org/abs/2308.07633)。*预印本*，arXiv:2308.07633。
- en: '| Model | Methods | 0 | 0.03125 | 0.0625 | 0.09375 | 0.125 | 0.15625 | 0.1875
    | 0.21875 | 0.25 | 0.28125 | 0.3125 | 0.34375 | 0.375 | 0.40625 | 0.4375 | 0.46875
    | 0.5 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Model | Methods | 0 | 0.03125 | 0.0625 | 0.09375 | 0.125 | 0.15625 | 0.1875
    | 0.21875 | 0.25 | 0.28125 | 0.3125 | 0.34375 | 0.375 | 0.40625 | 0.4375 | 0.46875
    | 0.5 |'
- en: '| Llama3_8b | ACC (Reverse) | 66.29 | 66.12 | 66.33 | 66.15 | 66.21 | 65.31
    | 64.96 | 62.91 | 64.28 | 65.00 | 63.99 | 64.71 | 62.04 | 63.52 | 64.51 | 30.31
    | 29.07 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Llama3_8b | ACC (Reverse) | 66.29 | 66.12 | 66.33 | 66.15 | 66.21 | 65.31
    | 64.96 | 62.91 | 64.28 | 65.00 | 63.99 | 64.71 | 62.04 | 63.52 | 64.51 | 30.31
    | 29.07 |'
- en: '| ACC (Ours) | 66.29 | 65.96 | 66.26 | 66.15 | 58.08 | 62.94 | 64.96 | 62.92
    | 64.28 | 65.01 | 63.99 | 64.87 | 62.05 | 63.42 | 64.42 | 30.29 | 29.05 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| ACC (Ours) | 66.29 | 65.96 | 66.26 | 66.15 | 58.08 | 62.94 | 64.96 | 62.92
    | 64.28 | 65.01 | 63.99 | 64.87 | 62.05 | 63.42 | 64.42 | 30.29 | 29.05 |'
- en: '| Llama2_7b | ACC (Reverse) | 46.67 | 44.37 | 46.71 | 46.09 | 46.89 | 46.51
    | 46.79 | 43.33 | 45.90 | 45.22 | 35.33 | 40.58 | 42.33 | 37.34 | 39.26 | 39.53
    | 35.65 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Llama2_7b | ACC (Reverse) | 46.67 | 44.37 | 46.71 | 46.09 | 46.89 | 46.51
    | 46.79 | 43.33 | 45.90 | 45.22 | 35.33 | 40.58 | 42.33 | 37.34 | 39.26 | 39.53
    | 35.65 |'
- en: '| ACC (Ours) | 46.67 | 44.45 | 46.74 | 46.07 | 46.93 | 46.52 | 46.84 | 43.41
    | 45.85 | 45.09 | 35.25 | 40.67 | 42.40 | 37.38 | 39.41 | 39.45 | 35.71 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| ACC (Ours) | 46.67 | 44.45 | 46.74 | 46.07 | 46.93 | 46.52 | 46.84 | 43.41
    | 45.85 | 45.09 | 35.25 | 40.67 | 42.40 | 37.38 | 39.41 | 39.45 | 35.71 |'
- en: 'Table 3: ACC during the compression process of Ours and Reverse Prune on Llama3-8b
    and Llama2-7b models.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：Ours 和 Reverse Prune 在 Llama3-8b 和 Llama2-7b 模型压缩过程中的 ACC。
- en: '| Methods | 0 | 0.025 | 0.05 | 0.075 | 0.1 | 0.125 | 0.15 | 0.175 | 0.2 | 0.225
    | 0.25 | 0.275 | 0.3 | 0.325 | 0.35 | 0.375 | 0.4 | 0.425 | 0.45 | 0.475 | 0.5
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 0 | 0.025 | 0.05 | 0.075 | 0.1 | 0.125 | 0.15 | 0.175 | 0.2 | 0.225
    | 0.25 | 0.275 | 0.3 | 0.325 | 0.35 | 0.375 | 0.4 | 0.425 | 0.45 | 0.475 | 0.5
    |'
- en: '| ACC (Reverse) | 55.62 | 55.24 | 55.21 | 55.12 | 54.44 | 54.02 | 55.63 | 55.27
    | 53.87 | 53.66 | 53.17 | 51.89 | 51.56 | 51.56 | 51.48 | 50.75 | 50.28 | 48.37
    | 45.18 | 48.59 | 46.78 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| ACC (Reverse) | 55.62 | 55.24 | 55.21 | 55.12 | 54.44 | 54.02 | 55.63 | 55.27
    | 53.87 | 53.66 | 53.17 | 51.89 | 51.56 | 51.56 | 51.48 | 50.75 | 50.28 | 48.37
    | 45.18 | 48.59 | 46.78 |'
- en: '| ACC (Ours) | 55.62 | 55.24 | 55.21 | 55.12 | 54.44 | 54.02 | 55.63 | 55.27
    | 53.87 | 53.66 | 53.17 | 51.89 | 51.56 | 51.56 | 51.49 | 50.75 | 50.28 | 48.37
    | 45.18 | 48.59 | 46.78 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| ACC (Ours) | 55.62 | 55.24 | 55.21 | 55.12 | 54.44 | 54.02 | 55.63 | 55.27
    | 53.87 | 53.66 | 53.17 | 51.89 | 51.56 | 51.56 | 51.49 | 50.75 | 50.28 | 48.37
    | 45.18 | 48.59 | 46.78 |'
- en: 'Table 4: ACC during the compression process of Ours and Reverse Prune on Llama2-13b
    model.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：Ours 和 Reverse Prune 在 Llama2-13b 模型压缩过程中的 ACC。
- en: Appendix A More Results
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 更多结果
