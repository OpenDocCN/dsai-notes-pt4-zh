- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:47:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'PQCache: Product Quantization-based KVCache for Long Context LLM Inference'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PQCache：基于产品量化的KVCache，用于长上下文LLM推理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.12820](https://ar5iv.labs.arxiv.org/html/2407.12820)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.12820](https://ar5iv.labs.arxiv.org/html/2407.12820)
- en: Hailin Zhang Peking University ,  Xiaodong Ji Peking University ,  Yilin Chen
    Beijing Institute of Technology ,  Fangcheng Fu Peking University ,  Xupeng Miao
    Carnegie Mellon University ,  Xiaonan Nie Peking University ,  Weipeng Chen Baichuan
    Inc.  and  Bin Cui Peking University
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Hailin Zhang 北京大学，Xiaodong Ji 北京大学，Yilin Chen 北京理工大学，Fangcheng Fu 北京大学，Xupeng
    Miao 卡内基梅隆大学，Xiaonan Nie 北京大学，Weipeng Chen 百川公司，以及 Bin Cui 北京大学
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: As the field of Large Language Models (LLMs) continues to evolve, the context
    length in inference is steadily growing. Key-Value Cache (KVCache), a crucial
    component in LLM inference, has now become the primary memory bottleneck due to
    limited GPU memory. Current methods selectively determine suitable keys and values
    for self-attention computation in LLMs to address the issue. However, they either
    fall short in maintaining model quality or result in high serving latency. Drawing
    inspiration from advanced embedding retrieval techniques used in the database
    community, we consider the storage and searching of KVCache as a typical embedding
    retrieval problem. We propose PQCache, which employs Product Quantization (PQ)
    to manage KVCache, maintaining model quality while ensuring low serving latency.
    During the prefilling phase, we apply PQ to tokens’ keys for each LLM layer and
    head. During the autoregressive decoding phase, for each newly generated token,
    we first identify important tokens through Maximum Inner-Product Search (MIPS)
    using PQ codes and centroids, then fetch the corresponding key-value pairs for
    self-attention computation. Through meticulous design of overlapping and caching,
    we minimize any additional computation and communication overhead during both
    phases. Extensive experiments show that PQCache achieves both effectiveness and
    efficiency. It maintains model quality even when only 1/5 of the tokens are involved
    in attention, while attaining acceptable system latency.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）领域的不断发展，推理中的上下文长度正在稳步增长。关键值缓存（KVCache）作为LLM推理中的关键组件，由于GPU内存的限制，现在已成为主要的内存瓶颈。目前的方法选择性地确定适合自注意力计算的键和值来解决这一问题。然而，这些方法要么在保持模型质量方面有所不足，要么导致较高的服务延迟。借鉴数据库社区中使用的先进嵌入检索技术，我们将KVCache的存储和检索视为典型的嵌入检索问题。我们提出了PQCache，利用产品量化（PQ）来管理KVCache，在确保低服务延迟的同时保持模型质量。在预填充阶段，我们对每个LLM层和头的令牌键应用PQ。在自回归解码阶段，对于每个新生成的令牌，我们首先通过使用PQ代码和质心的最大内积搜索（MIPS）识别重要令牌，然后获取相应的键值对进行自注意力计算。通过精心设计重叠和缓存，我们在两个阶段都最小化了额外的计算和通信开销。大量实验表明，PQCache在效果和效率上都表现出色。即使只有1/5的令牌参与注意力计算，它仍能保持模型质量，同时达到可接受的系统延迟。
- en: 1\. Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'With the emergence of ChatGPT (OpenAI, [2023](#bib.bib41)), Large Language
    Models (LLMs) have captured the attention of researchers and engineers as promising
    candidates for Artificial General Intelligence (AGI). LLMs exhibit exceptional
    performance in the “next token prediction” task, where they take a sequence of
    tokens as input (also called prompt) and generate subsequent tokens autoregressively
    during inference. Constructed with transformer layers, the fundamental mechanism
    of LLMs is the self-attention module. For each token, this module computes “query”,
    “key”, and “value” representations. Each token’s query interacts with the previous
    tokens’ keys (including itself) to derive attention weights, which are then used
    for weighted summation of the previous tokens’ values. Figure [2](#S1.F2 "Figure
    2 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference") illustrates a typical self-attention module within a transformer
    layer.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '随着ChatGPT（OpenAI，[2023](#bib.bib41)）的出现，大语言模型（LLMs）引起了研究人员和工程师的关注，被视为人工通用智能（AGI）的有前途的候选者。LLMs在“下一个令牌预测”任务中表现出色，其中它们将一系列令牌作为输入（也称为提示），并在推理过程中自回归生成后续令牌。LLMs基于transformer层构建，其基本机制是自注意力模块。对于每个令牌，这个模块计算“查询”、“键”和“值”表示。每个令牌的查询与之前令牌的键（包括自身）进行交互，以导出注意力权重，这些权重随后用于对之前令牌的值进行加权求和。图[2](#S1.F2
    "Figure 2 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for
    Long Context LLM Inference")展示了transformer层中的典型自注意力模块。'
- en: 'To accommodate increasingly lengthy prompts, the maximum input length of LLMs
    has expanded significantly, from 2K-4K (Touvron et al., [2023](#bib.bib53); Taori
    et al., [2023](#bib.bib51)) to 32K (Jiang et al., [2023a](#bib.bib26); Together.ai,
    [2023](#bib.bib52)), 128K (OpenAI, [2023](#bib.bib41); Fu et al., [2024](#bib.bib16)),
    or even millions of tokens (AI, [2024](#bib.bib3); Cloud, [2024](#bib.bib10);
    Liu et al., [2024a](#bib.bib32)). As illustrated in Figure [2](#S1.F2 "Figure
    2 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference"), the process of LLM inference involves two phases: prefilling
    and decoding. During prefilling, LLMs handle the lengthy input and compute keys
    and values for all input tokens. During decoding, LLMs generate the next new token
    and produce its key and value. To avoid redundant computations, the keys and values
    of preceding tokens are commonly cached in the Key-Value Cache (KVCache), and
    fetched for subsequent tokens’ attention computation. However, as prompts grow
    in length, the memory consumption of KVCache has far exceeded the memory capacity
    of each individual GPU, even for 7B and 13B LLMs in Figure [1(a)](#S1.F1.sf1 "In
    Figure 1 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for
    Long Context LLM Inference"). This poses a formidable challenge for modern LLM
    inference.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '为了适应越来越长的提示，LLM（大语言模型）的最大输入长度显著扩展，从2K-4K（Touvron 等，[2023](#bib.bib53)；Taori
    等，[2023](#bib.bib51)）到32K（Jiang 等，[2023a](#bib.bib26)；Together.ai，[2023](#bib.bib52)），128K（OpenAI，[2023](#bib.bib41)；Fu
    等，[2024](#bib.bib16)），甚至数百万个令牌（AI，[2024](#bib.bib3)；Cloud，[2024](#bib.bib10)；Liu
    等，[2024a](#bib.bib32)）。如图[2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ PQCache: Product
    Quantization-based KVCache for Long Context LLM Inference")所示，LLM推理过程分为两个阶段：预填充和解码。在预填充阶段，LLM处理长输入并计算所有输入令牌的键和值。在解码阶段，LLM生成下一个新令牌并产生其键和值。为了避免重复计算，前面令牌的键和值通常会缓存到键值缓存（KVCache）中，并在后续令牌的注意力计算中取用。然而，随着提示长度的增长，KVCache的内存消耗远远超过了每个单独GPU的内存容量，即使是图[1(a)](#S1.F1.sf1
    "In Figure 1 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache
    for Long Context LLM Inference")中的7B和13B LLM也不例外。这对现代LLM推理提出了巨大的挑战。'
- en: '![Refer to caption](img/c4a853e9722e5072fdce5b14d075cd95.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/c4a853e9722e5072fdce5b14d075cd95.png)'
- en: (a) Model and KVCache memory.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 模型和KVCache内存。
- en: '![Refer to caption](img/b7d7e0b4287f3f97d1508d664bc12065.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/b7d7e0b4287f3f97d1508d664bc12065.png)'
- en: (b) Attention example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 注意力示例。
- en: 'Figure 1\. Observations. The left figure shows the KVCache memory consumption
    of LLMs (for the meaning of MHA and GQA, please refer to Section [2.1](#S2.SS1
    "2.1\. Large Language Model Inference ‣ 2\. Preliminary ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference")). The right figure shows an example of
    attention scores on the MultiNews dataset (Fabbri et al., [2019](#bib.bib13)),
    with darker colors indicating higher scores.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '图1\. 观察结果。左图显示了LLMs的KVCache内存消耗（关于MHA和GQA的含义，请参见第[2.1节](#S2.SS1 "2.1\. Large
    Language Model Inference ‣ 2\. Preliminary ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference")）。右图显示了MultiNews数据集（Fabbri 等，[2019](#bib.bib13)）上注意力分数的示例，颜色越深表示分数越高。'
- en: '![Refer to caption](img/c9d50f4bfda50f78f9071d2a292667e0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c9d50f4bfda50f78f9071d2a292667e0.png)'
- en: 'Figure 2\. An overview of LLM inference. The left part illustrates the computation
    process of the self-attention module, where “Q”, “K”, “V”, “AS”, and “O” represent
    query, key, value, attention score, and output, respectively. The right part depicts
    the LLM inference process, consisting of the prefilling phase and the decoding
    phase, where “Attn” and “FFN” represent the attention layer and the feed-forward
    network layer, respectively. The mathematical symbols are detailed in Table [1](#S1.T1
    "Table 1 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for
    Long Context LLM Inference").'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '图2\. LLM推理的概览。左侧部分展示了自注意力模块的计算过程，其中“Q”、“K”、“V”、“AS”和“O”分别代表查询、键、值、注意力得分和输出。右侧部分描绘了LLM推理过程，包括预填充阶段和解码阶段，其中“Attn”和“FFN”分别代表注意力层和前馈网络层。数学符号的详细信息见表[1](#S1.T1
    "表 1 ‣ 1\. 介绍 ‣ PQCache: 基于产品量化的KVCache用于长上下文LLM推理")。'
- en: 'Recognizing that specific tokens significantly influence generation, i.e. their
    attention weights are much larger than others (Zhang et al., [2023a](#bib.bib64);
    Liu et al., [2023a](#bib.bib34)), numerous methods selectively incorporate these
    tokens within attention mechanisms while excluding others. This approach aims
    to address the memory challenge posed by KVCache and is commonly referred to as
    selective attention (Miao et al., [2023](#bib.bib38)). Related methods can be
    classified into two categories: KVCache dropping (Zhang et al., [2023a](#bib.bib64);
    Liu et al., [2023a](#bib.bib34); Xiao et al., [2023](#bib.bib59)) and KVCache
    offloading (Xiao et al., [2024](#bib.bib58); Ribar et al., [2023](#bib.bib47)).
    However, these methods either rely on improper assumptions or introduce notable
    latency during inference, failing to obtain both effectiveness and efficiency.
    KVCache dropping methods discard unnecessary key-value pairs, based on the assumption
    that unimportant tokens have no relevance for subsequent generation. Nevertheless,
    as shown in an attention score example in Figure [1(b)](#S1.F1.sf2 "In Figure
    1 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference"), many tokens with lower average attention weights can still contribute
    to later generated tokens. Prior research (Kang et al., [2024](#bib.bib30); Dong
    et al., [2024b](#bib.bib11)) also highlights the drawback of direct dropping.
    KVCache offloading methods, including InfLLM (Xiao et al., [2024](#bib.bib58))
    and SPARQ (Ribar et al., [2023](#bib.bib47)), store the KVCache on CPU, and fetch
    relevant key-value pairs for each newly generated token according to easy-to-compute
    proxy scores. InfLLM organizes the KVCache into blocks, using representative tokens
    within each block to compute relevance. Unfortunately, as shown in Figure [1(b)](#S1.F1.sf2
    "In Figure 1 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache
    for Long Context LLM Inference"), we do not observe the space-continuity assumption
    in InfLLM. SPARQ identifies a subset of dimensions with large magnitude in queries,
    and fetches only these dimensions from all keys to determine the most relevant
    tokens. Despite demonstrating effectiveness using a large number of dimensions,
    it incurs excessive communication overhead, and the serialized computation-communication
    process hinders opportunities for system optimization (e.g., prefetching). In
    summary, existing methods fall short in achieving both effectiveness and efficiency
    for long-context LLM inference.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到特定的 tokens 对生成具有显著影响，即它们的注意力权重远大于其他 tokens （Zhang 等， [2023a](#bib.bib64)；Liu
    等， [2023a](#bib.bib34)），许多方法在注意力机制中选择性地纳入这些 tokens，同时排除其他 tokens。这种方法旨在解决 KVCache
    所带来的记忆挑战，通常称为选择性注意力 （Miao 等， [2023](#bib.bib38)）。相关方法可以分为两类：KVCache 丢弃 （Zhang
    等， [2023a](#bib.bib64)；Liu 等， [2023a](#bib.bib34)；Xiao 等， [2023](#bib.bib59)）和
    KVCache 卸载 （Xiao 等， [2024](#bib.bib58)；Ribar 等， [2023](#bib.bib47)）。然而，这些方法要么依赖于不正确的假设，要么在推理过程中引入显著的延迟，未能同时实现有效性和效率。KVCache
    丢弃方法丢弃不必要的键值对，基于一个假设：不重要的 tokens 对后续生成没有相关性。然而，如图 [1(b)](#S1.F1.sf2 "在图 1 ‣ 1\.
    介绍 ‣ PQCache：基于产品量化的 KVCache 用于长上下文 LLM 推理") 中的注意力分数示例所示，许多具有较低平均注意力权重的 tokens
    仍然可以对后续生成的 tokens 产生贡献。先前的研究 （Kang 等， [2024](#bib.bib30)；Dong 等， [2024b](#bib.bib11)）也突出了直接丢弃的缺点。KVCache
    卸载方法，包括 InfLLM （Xiao 等， [2024](#bib.bib58)）和 SPARQ （Ribar 等， [2023](#bib.bib47)），将
    KVCache 存储在 CPU 上，并根据易于计算的代理分数获取每个新生成的 token 相关的键值对。InfLLM 将 KVCache 组织成块，使用每块中的代表性
    tokens 计算相关性。不幸的是，如图 [1(b)](#S1.F1.sf2 "在图 1 ‣ 1\. 介绍 ‣ PQCache：基于产品量化的 KVCache
    用于长上下文 LLM 推理") 所示，我们没有观察到 InfLLM 中的空间连续性假设。SPARQ 识别查询中具有大幅度的维度子集，并仅从所有键中提取这些维度以确定最相关的
    tokens。尽管在使用大量维度时表现出有效性，但它产生了过高的通信开销，而序列化的计算-通信过程阻碍了系统优化的机会（例如，预取）。总之，现有方法在实现长上下文
    LLM 推理的有效性和效率方面均显不足。
- en: We clarify that, selective attention computation, requiring to find the top-$k$
    relevant embeddings from the index for a given query embedding. Surprisingly,
    we found that the selective attention calculation process during LLM inference
    can be mapped into these operations. Specifically, the prefilling phase generates
    most of the KVCache and constructs the index, while the decoding phase finds relevant
    keys/values and updates KVCache using the newly generated token.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们明确指出，选择性注意力计算需要从索引中找到与给定查询嵌入相关的前$k$个嵌入。令人惊讶的是，我们发现LLM推理过程中选择性注意力计算过程可以映射为这些操作。具体而言，预填充阶段生成大部分KVCache并构建索引，而解码阶段则找到相关的键/值并使用新生成的令牌更新KVCache。
- en: Inspired by the advanced embedding retrieval techniques, in this paper we propose
    PQCache to ensure both effectiveness and efficiency during long-context LLM inference.
    In the prefilling phase, we generate the KVCache, store it in CPU memory, and
    then construct index structures. In the decoding phase, we efficiently retrieve
    relevant key-value pairs for self-attention computation and update KVCache. Considering
    the latency requirements of LLM inference, we cannot employ methods with expensive
    index construction overheads, such as graph-based methods or complex inverted-index
    methods. We take the advantage of low-cost Product Quantization (PQ) (Jégou et al.,
    [2011](#bib.bib25)) from embedding retrieval (Jayaram Subramanya et al., [2019](#bib.bib24)),
    where embeddings are initially partitioned into sub-embeddings and then clustered.
    The key idea of PQCache is to construct PQ codebooks using preceding token keys
    and perform MIPS to retrieve relevant key-value pairs for subsequent self-attention
    computations. We propose a system-algorithm co-design method based on PQ, leveraging
    both its high recall potentiality, and the opportunities for system optimization.
    Further experimental analysis show that PQCache improves the LongBench scores
    up to 6.21 compared to existing methods, and attains acceptable system latency.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 受到先进嵌入检索技术的启发，本文提出了PQCache，以确保在长上下文LLM推理过程中既有效又高效。在预填充阶段，我们生成KVCache，存储在CPU内存中，然后构建索引结构。在解码阶段，我们高效地检索相关的键值对进行自注意力计算并更新KVCache。考虑到LLM推理的延迟要求，我们不能采用具有昂贵索引构建开销的方法，如基于图的方法或复杂的倒排索引方法。我们利用低成本的产品量化（PQ）(Jégou等，[2011](#bib.bib25))来进行嵌入检索（Jayaram
    Subramanya等，[2019](#bib.bib24)），其中嵌入最初被划分为子嵌入然后进行聚类。PQCache的关键思想是使用先前的令牌键构建PQ代码簿，并执行MIPS以检索相关的键值对用于后续的自注意力计算。我们提出了一种基于PQ的系统算法共设计方法，利用其高召回潜力和系统优化机会。进一步的实验分析显示，PQCache将LongBench分数提高了高达6.21，相比现有方法，且获得了可接受的系统延迟。
- en: To the best of our knowledge, this is the pioneering work that incorporates
    embedding retrieval technique to address the KVCache memory challenge. PQ offers
    a well-behaved approximation of embedding vectors (and their inner product), while
    consuming only a small amount of memory. In the prefilling phase, we apply PQ
    to the generated keys for each layer and head, and obtain PQ codes and centroids
    through clustering on CPU. At each autoregressive decoding step, we perform inner
    product between the partitioned query and the PQ centroids, then combine with
    PQ codes to obtain the approximate attention weights. Using the approximation,
    we retrieve top-$k$ relevant key-value pairs from CPU memory for self-attention
    computation, rather than accessing the entire KVCache.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，这是首创性地将嵌入检索技术应用于解决KVCache内存挑战的工作。PQ提供了对嵌入向量（及其内积）的良好近似，同时仅消耗少量内存。在预填充阶段，我们将PQ应用于每一层和头部生成的键，并通过在CPU上进行聚类获得PQ代码和质心。在每一步自回归解码中，我们在分区查询和PQ质心之间执行内积运算，然后结合PQ代码以获得近似注意力权重。利用这种近似，我们从CPU内存中检索前$k$个相关的键值对进行自注意力计算，而不是访问整个KVCache。
- en: 'To enable efficient LLM inference, we carefully design the PQCache system to
    reduce latency. We implement prefetching and overlapping as much as possible:
    KVCache offloading, PQ construction, and the fetching of PQ codes and centroids
    are overlapped with LLM computation. To maximize the utilization of available
    GPU memory and minimize CPU-GPU communication, we additionally introduce a block-level
    cache on GPU, specifically for frequently accessed key-value pairs.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现高效的 LLM 推理，我们精心设计了 PQCache 系统，以减少延迟。我们尽可能实现预取和重叠：KVCache 卸载、PQ 构造以及 PQ 代码和质心的获取与
    LLM 计算重叠。为了最大限度地利用可用的 GPU 内存并最小化 CPU-GPU 通信，我们还在 GPU 上引入了一个块级缓存，专门用于频繁访问的键值对。
- en: 'We summarize our contributions as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了我们的贡献如下：
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We incorporate the embedding retrieval technique PQ into KVCache management
    to enable both effective and efficient LLM inference.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将嵌入检索技术 PQ 融入 KVCache 管理中，以实现有效且高效的 LLM 推理。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a system-algorithm co-designed approach PQCache to approximately
    retrieve the top-$k$ relevant keys for a given query, with meticulous design of
    overlapping and caching.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种系统与算法共同设计的方法 PQCache，以大致检索给定查询的前-$k$ 个相关键，并精心设计了重叠和缓存。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We evaluate PQCache through extensive experiments. It maintains model quality
    with 1/5 of the tokens in attention, while achieving acceptable system latency.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过广泛的实验评估了 PQCache。它在关注中保持了模型质量，使用了 1/5 的标记，同时达到了可接受的系统延迟。
- en: Table 1\. Notations. “#” means “the number of”.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 符号说明。“#”表示“数量”。
- en: '| Sym. | Explanation | Sym. | Explanation |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 解释 | 符号 | 解释 |'
- en: '| $n$ | # partitions in PQ. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| $n$ | PQ 中的分区数。 |'
- en: '| $s$ | # bits for PQ codes. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| $s$ | PQ 代码的位数。 |'
- en: '| $d$ | Dimension of each partition. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| $d$ | 每个分区的维度。 |'
- en: '| $h_{(kv)}$ | # tokens in selective attention. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| $h_{(kv)}$ | 选择性注意力中的标记数。 |'
- en: '| $d_{h}$ | # K-Means iterations. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| $d_{h}$ | # K-Means 迭代次数。 |'
- en: 2\. Preliminary
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 初步工作
- en: In this section, we introduce fundamental concepts related to LLM, PQ, and the
    memory hierarchy.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了与 LLM、PQ 和内存层次结构相关的基本概念。
- en: 2.1\. Large Language Model Inference
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 大型语言模型推理
- en: 'An overview of LLM inference is depicted in Figure [2](#S1.F2 "Figure 2 ‣ 1\.
    Introduction ‣ PQCache: Product Quantization-based KVCache for Long Context LLM
    Inference"). An LLM comprises a stack of transformer layers, along with a vocabulary
    embedding for input and a token classifier for output. The self-attention module,
    which is a crucial component of a transformer layer, facilitates interaction and
    information aggregation among different tokens. Multi-Head Attention (MHA) and
    Grouped-Query Attention (GQA) (Ainslie et al., [2023](#bib.bib4)) are the primary
    variants of the self-attention module. Following the notations in Table [1](#S1.T1
    "Table 1 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for
    Long Context LLM Inference"), the attention module receives an input of shape
    $(n,s,d)$. In this setup, each key-value pair corresponds to multiple queries.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM 推理的概述见图 [2](#S1.F2 "图 2 ‣ 1\. 介绍 ‣ PQCache: 基于产品量化的 KVCache 用于长上下文 LLM
    推理")。LLM 由一系列变换器层组成，输入有词汇嵌入，输出有标记分类器。自注意力模块是变换器层的关键组件，促进了不同标记之间的交互和信息汇总。多头注意力（MHA）和分组查询注意力（GQA） （Ainslie
    等，[2023](#bib.bib4)）是自注意力模块的主要变体。根据表 [1](#S1.T1 "表 1 ‣ 1\. 介绍 ‣ PQCache: 基于产品量化的
    KVCache 用于长上下文 LLM 推理") 中的符号说明，注意力模块接收形状为 $(n,s,d)$ 的输入。在这种设置中，每对键值对应于多个查询。'
- en: 'During LLM inference, each execution of the model generates a new token, following
    an autoregressive manner. The first traversal and the subsequent traversals of
    the LLM are referred to as “prefilling” and “decoding” separately, as shown in
    Figure [2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference"). During the prefilling phase, the self-attention
    module computes the queries, keys, and values for all input tokens, and stores
    the key-value pairs as KVCache for later usage. During the autoregressive decoding
    phase, the attention module only computes the query, key, value for the last generated
    token. It leverages previous keys and values from the KVCache, and computes an
    attention score of shape $(n,h,1,s)$. Concurrently, the newly generated key and
    value are added to the KVCache. Consequently, the memory consumption of KVCache
    scales linearly with the sequence length, which leads to a memory bottleneck in
    scenarios involving long-context LLM inference.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '在 LLM 推理过程中，每次模型执行都会生成一个新令牌，采用自回归的方式进行。LLM 的第一次遍历和后续遍历分别称为“预填充”和“解码”，如图 [2](#S1.F2
    "Figure 2 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for
    Long Context LLM Inference") 所示。在预填充阶段，自注意力模块计算所有输入令牌的查询、键和值，并将键值对存储为 KVCache
    以备后用。在自回归解码阶段，注意力模块仅计算最后生成的令牌的查询、键和值。它利用来自 KVCache 的先前键和值，并计算形状为 $(n,h,1,s)$ 的注意力得分。同时，新生成的键和值被添加到
    KVCache 中。因此，KVCache 的内存消耗与序列长度线性增长，这在涉及长上下文 LLM 推理的场景中会导致内存瓶颈。'
- en: '![Refer to caption](img/8cc9480d7236ac20ffa4cd55bc0b1da9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8cc9480d7236ac20ffa4cd55bc0b1da9.png)'
- en: Figure 3\. An overview of PQ construction and searching.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. PQ 构建和搜索概述。
- en: 2.2\. Product Quantization
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 产品量化
- en: 'PQ (Jégou et al., [2011](#bib.bib25)) was proposed to facilitate efficient
    Approximate Nearest Neighbor Search (ANNS), retrieving relevant embeddings from
    a large pool of candidates given a query embedding. MIPS is a special case of
    ANNS that uses inner product as similarity. As shown in Figure [3](#S2.F3 "Figure
    3 ‣ 2.1\. Large Language Model Inference ‣ 2\. Preliminary ‣ PQCache: Product
    Quantization-based KVCache for Long Context LLM Inference"), PQ divides each candidate
    embedding into $m$ bits, corresponding to the centroids. These compact PQ codes
    enable the reconstruction of approximate embeddings with reduced memory requirements.
    During ANNS, the query embedding computes similarity with the centroids and aggregates
    the similarity using PQ codes, bypassing the need for full similarity calculations
    with every embedding.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'PQ (Jégou 等，[2011](#bib.bib25)) 被提出以促进高效的近似最近邻搜索 (ANNS)，根据查询嵌入从大量候选项中检索相关的嵌入。MIPS
    是 ANNS 的一个特例，它使用内积作为相似性。如图 [3](#S2.F3 "Figure 3 ‣ 2.1\. Large Language Model Inference
    ‣ 2\. Preliminary ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference") 所示，PQ 将每个候选嵌入划分为 $m$ 位，对应于质心。这些紧凑的 PQ 代码使得在减少内存需求的情况下重建近似嵌入成为可能。在
    ANNS 过程中，查询嵌入与质心计算相似性，并使用 PQ 代码汇总相似性，避免了对每个嵌入进行全面的相似性计算。'
- en: PQ has a profound impact on ANNS, with its principles integrated into various
    efficient ANNS methods (Johnson et al., [2021](#bib.bib28); Jayaram Subramanya
    et al., [2019](#bib.bib24); Baranchuk et al., [2018](#bib.bib7)). PQ has several
    variants, including Optimized PQ (Ge et al., [2013](#bib.bib19)), Residual Quantization (Martinez
    et al., [2014](#bib.bib37)), and SCaNN (Guo et al., [2020](#bib.bib20)). While
    PQ was initially designed for ANNS, its variants are also applied in various learning
    tasks (Lingle, [2023](#bib.bib31); van den Oord et al., [2017](#bib.bib54); Zhang
    et al., [2023b](#bib.bib62)) to achieve effective compression and efficient computation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: PQ 对 ANNS 产生了深远的影响，其原理被集成到各种高效的 ANNS 方法中 (Johnson 等，[2021](#bib.bib28)；Jayaram
    Subramanya 等，[2019](#bib.bib24)；Baranchuk 等，[2018](#bib.bib7))。PQ 具有多个变体，包括优化
    PQ (Ge 等，[2013](#bib.bib19))、残差量化 (Martinez 等，[2014](#bib.bib37)) 和 SCaNN (Guo
    等，[2020](#bib.bib20))。虽然 PQ 最初是为 ANNS 设计的，但其变体也被应用于各种学习任务 (Lingle，[2023](#bib.bib31)；van
    den Oord 等，[2017](#bib.bib54)；Zhang 等，[2023b](#bib.bib62)) 以实现有效的压缩和高效的计算。
- en: '![Refer to caption](img/2ba06902bb77318cf4c325b84f5d55e5.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2ba06902bb77318cf4c325b84f5d55e5.png)'
- en: Figure 4\. An overview of PQCache. For simplicity, we only illustrate the process
    for a single transformer layer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. PQCache 概述。为了简单起见，我们仅展示了单个变换器层的过程。
- en: 2.3\. GPU-CPU Memory Hierarchy
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. GPU-CPU 内存层次结构
- en: 'Modern deep learning tasks heavily rely on GPUs for executing compute-intensive
    operations. The GPU-CPU structure forms a typical memory hierarchy: the more expensive
    GPU memory offers faster memory I/O speeds for computation, while the CPU memory,
    connected via PCIe or NVLink, provides lower bandwidth. As model parameters increase
    and the demand for intermediate results storage (such as KVCache) grows, CPUs
    are often employed to share the memory load. Numerous research studies in machine
    learning systems propose offloading certain model parameters or activations to
    the CPU memory (Nie et al., [2022](#bib.bib40); Ren et al., [2021](#bib.bib43);
    Rhu et al., [2016](#bib.bib46); Sheng et al., [2023](#bib.bib48)), thereby enhancing
    the overall performance of GPU-centric deep learning tasks. The primary challenge
    in this context is to effectively schedule memory I/O (or say GPU-CPU communication)
    in conjunction with GPU computation to efficiently hide the associated overhead.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度学习任务严重依赖GPU来执行计算密集型操作。GPU-CPU结构形成了典型的内存层次结构：更昂贵的GPU内存提供了更快的内存I/O速度，而CPU内存通过PCIe或NVLink连接，提供了较低的带宽。随着模型参数的增加以及对中间结果存储（如KVCache）的需求增长，CPU通常被用来分担内存负载。许多机器学习系统的研究提出将某些模型参数或激活值转移到CPU内存中 （Nie
    et al., [2022](#bib.bib40); Ren et al., [2021](#bib.bib43); Rhu et al., [2016](#bib.bib46);
    Sheng et al., [2023](#bib.bib48)），从而提高了以GPU为中心的深度学习任务的整体性能。该背景下的主要挑战是如何有效地调度内存I/O（或称GPU-CPU通信）与GPU计算，以高效地隐藏相关开销。
- en: 3\. PQCache
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. PQCache
- en: 'In this section, we introduce PQCache, a novel system-algorithm co-designed
    method to enable effective and efficient long context LLM inference with large-scale
    KVCache. Figure [4](#S2.F4 "Figure 4 ‣ 2.2\. Product Quantization ‣ 2\. Preliminary
    ‣ PQCache: Product Quantization-based KVCache for Long Context LLM Inference")
    provides an overview of PQCache, where the KVCache from the prefilling phase is
    first offloaded to CPU, compressed using PQ, then fetched on demand through MIPS
    during the decoding phase.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一节中，我们介绍了PQCache，这是一种新颖的系统-算法共设计方法，旨在通过大规模KVCache实现有效且高效的长上下文LLM推理。图 [4](#S2.F4
    "图 4 ‣ 2.2\. 产品量化 ‣ 2\. 初步 ‣ PQCache: 基于产品量化的KVCache用于长上下文LLM推理") 提供了PQCache的概述，其中在预填充阶段的KVCache首先被转移到CPU，使用PQ进行压缩，然后在解码阶段通过MIPS按需提取。'
- en: 3.1\. Overview
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 概述
- en: 'We design PQCache to reserve all the KVCache in CPU, and selectively fetch
    relevant key-value pairs for self-attention computation. In long context inference
    scenario, the entire KVCache is too large for both attention computation and I/O
    communication within the memory hierarchy. Therefore, a common technique is to
    only perform attention on a subset of the key-value pairs, a process known as
    “selective attention”. According to previous research (Zhang et al., [2023a](#bib.bib64);
    Liu et al., [2023a](#bib.bib34); Xiao et al., [2024](#bib.bib58); Ribar et al.,
    [2023](#bib.bib47); Yang et al., [2024](#bib.bib60); Adnan et al., [2024](#bib.bib2);
    Ge et al., [2023](#bib.bib18); Wang and Gan, [2024](#bib.bib56); Ren and Zhu,
    [2024](#bib.bib45)), attention score is a proper metric to measure the importance
    or relevance of previous tokens. As shown in Figure [5](#S3.F5 "Figure 5 ‣ 3.1\.
    Overview ‣ 3\. PQCache ‣ PQCache: Product Quantization-based KVCache for Long
    Context LLM Inference"), we plot the attention score distributions at several
    randomly-selected positions on an example from the XSUM dataset (Narayan et al.,
    [2018](#bib.bib39)). The attention scores generally follow powerlaw distributions,
    indicating that a small part of tokens are more important than most other tokens.
    Therefore, we can only include those tokens with large scores for self-attention
    computation. Following prior works (Xiao et al., [2023](#bib.bib59); Han et al.,
    [2023](#bib.bib21); Zhang et al., [2023a](#bib.bib64)), we also include initial
    tokens and the most recent tokens (called local tokens) in attention computation.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '我们设计PQCache来将所有KVCache保留在CPU中，并有选择性地提取相关的键值对进行自注意力计算。在长上下文推断场景中，整个KVCache对于注意力计算和内存层次结构中的I/O通信来说过于庞大。因此，一种常见的技术是仅对一部分键值对进行注意力计算，这个过程称为“选择性注意力”。根据以往的研究（Zhang等，[2023a](#bib.bib64)；Liu等，[2023a](#bib.bib34)；Xiao等，[2024](#bib.bib58)；Ribar等，[2023](#bib.bib47)；Yang等，[2024](#bib.bib60)；Adnan等，[2024](#bib.bib2)；Ge等，[2023](#bib.bib18)；Wang和Gan，[2024](#bib.bib56)；Ren和Zhu，[2024](#bib.bib45)），注意力分数是衡量之前标记重要性或相关性的适当指标。如图[5](#S3.F5
    "Figure 5 ‣ 3.1\. Overview ‣ 3\. PQCache ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference")所示，我们绘制了在XSUM数据集（Narayan等，[2018](#bib.bib39)）的一个示例中，几个随机选择的位置上的注意力分数分布。注意力分数通常遵循幂律分布，表明少部分标记比大多数其他标记更重要。因此，我们只包括那些具有较大分数的标记进行自注意力计算。参考之前的研究（Xiao等，[2023](#bib.bib59)；Han等，[2023](#bib.bib21)；Zhang等，[2023a](#bib.bib64)），我们还在注意力计算中包括初始标记和最新标记（称为局部标记）。'
- en: '![Refer to caption](img/8632292ec07ecfa68020a8c4a4d51e9e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8632292ec07ecfa68020a8c4a4d51e9e.png)'
- en: (a) Layer 3, head 25.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 第3层，第25头。
- en: '![Refer to caption](img/4302ce8ad337a2dc5a408d1ccb1d76aa.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4302ce8ad337a2dc5a408d1ccb1d76aa.png)'
- en: (b) Layer 11, head 15.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 第11层，第15头。
- en: '![Refer to caption](img/b727b28f0216cea01d58ed4b46ec8215.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b727b28f0216cea01d58ed4b46ec8215.png)'
- en: (c) Layer 20, head 27.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 第20层，第27头。
- en: '![Refer to caption](img/b2a7afe7ba620da11fb082c1ebe81e53.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b2a7afe7ba620da11fb082c1ebe81e53.png)'
- en: (d) Layer 21, head 16.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 第21层，第16头。
- en: Figure 5\. Distributions of attention scores.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 注意力分数的分布。
- en: 'As detailed in Section [2.1](#S2.SS1 "2.1\. Large Language Model Inference
    ‣ 2\. Preliminary ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference"), attention scores are calculated using a softmax function applied
    to the product of the current query and preceding keys. The procedure of identifying
    the top-$k$ keys with the highest scores fundamentally constitutes a Maximum Inner
    Product Search (MIPS) operation. Therefore, we try to leverage embedding retrieval
    techniques to enable effective selective attention and address the KVCache memory
    issue. Based on the observations above, we design PQCache, which offloads all
    the KVCache to CPU, and fetch only relevant tokens’ key-values pairs during the
    decoding phase. Calculating exact attention scores of all previous tokens involves
    costly I/O communication, which is unacceptable in long context LLM inference.
    Inspired by Approximate Nearest Neighbor Search (ANNS) (Johnson et al., [2021](#bib.bib28);
    Jayaram Subramanya et al., [2019](#bib.bib24); Baranchuk et al., [2018](#bib.bib7)),
    we leverage the light-weight Product Quantization (PQ) method (Jégou et al., [2011](#bib.bib25)),
    which compress the vectors by partitioning and K-Means clustering. Though there
    are other ANNS methods (e.g. graph-based methods (Malkov and Yashunin, [2020](#bib.bib36);
    Fu et al., [2019](#bib.bib14); Jayaram Subramanya et al., [2019](#bib.bib24)))
    that can achieve better recall performance, they suffer from a computationally
    expensive construction process which may hinder LLM inference.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[2.1节](#S2.SS1 "2.1\. Large Language Model Inference ‣ 2\. Preliminary ‣
    PQCache: Product Quantization-based KVCache for Long Context LLM Inference")详细描述，注意力分数是通过将当前查询与之前的键的乘积应用于
    softmax 函数来计算的。确定前 $k$ 个分数最高的键的过程本质上构成了最大内积搜索（MIPS）操作。因此，我们尝试利用嵌入检索技术来实现有效的选择性注意力，并解决
    KVCache 内存问题。基于上述观察，我们设计了 PQCache，将所有 KVCache 转移到 CPU 上，并在解码阶段仅提取相关令牌的键值对。计算所有之前令牌的准确注意力分数涉及到昂贵的
    I/O 通信，这在长上下文 LLM 推理中是不可接受的。受到近似最近邻搜索（ANNS）(Johnson et al., [2021](#bib.bib28);
    Jayaram Subramanya et al., [2019](#bib.bib24); Baranchuk et al., [2018](#bib.bib7))
    启发，我们利用轻量级的产品量化（PQ）方法 (Jégou et al., [2011](#bib.bib25))，通过分区和 K-Means 聚类来压缩向量。尽管还有其他
    ANNS 方法（例如基于图的方法 (Malkov and Yashunin, [2020](#bib.bib36); Fu et al., [2019](#bib.bib14);
    Jayaram Subramanya et al., [2019](#bib.bib24))）能够实现更好的召回性能，但它们存在计算上昂贵的构建过程，这可能会阻碍
    LLM 推理。'
- en: In PQCache, we construct PQ at the prefilling phase and utilize PQ at the decoding
    phase. At the prefilling phase, we need to calculate all the input tokens’ keys
    and values for the self-attention module. After obtaining the keys, which have
    the shape of $(n,h_{kv},s,d_{h})$ bits to store.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PQCache 中，我们在预填充阶段构建 PQ，并在解码阶段利用 PQ。在预填充阶段，我们需要计算所有输入令牌的键和值，以供自注意力模块使用。获得键之后，它们的形状为
    $(n,h_{kv},s,d_{h})$ 位进行存储。
- en: At the decoding phase, we first perform matrix multiplication between the query
    and the PQ centroids, then aggregate the results for all the tokens according
    to PQ codes. We can determine the top-$k$ key-value pairs from CPU, the self-attention
    computation continues with retrieved tokens. Unlike normal embedding retrieval
    tasks, in LLM inference, newly generated keys and values are added into the KVCache.
    These tokens are first regarded as local tokens and reserved in GPU. When they
    are evicted from the sliding window of local tokens, they are assigned PQ codes
    based on their nearest centroids.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码阶段，我们首先进行查询与 PQ 中心点之间的矩阵乘法，然后根据 PQ 码聚合所有令牌的结果。我们可以从 CPU 确定前 $k$ 个键值对，自注意力计算将继续使用检索到的令牌。与普通的嵌入检索任务不同，在
    LLM 推理中，新生成的键和值会被添加到 KVCache 中。这些令牌首先被视为本地令牌并保存在 GPU 中。当它们从本地令牌的滑动窗口中被驱逐时，会根据它们最近的中心点分配
    PQ 码。
- en: 3.2\. Complexity Analysis
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 复杂度分析
- en: During the prefilling phase, we do not modify the attention computation, so
    the complexity remains the same for both time and memory. The additional K-Means
    clustering process has an average complexity of $O(s\cdot m\cdot 2^{b}\cdot T)$
    and the memory complexity are much smaller than the original ones.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在预填充阶段，我们不会修改注意力计算，因此时间和内存复杂度保持不变。额外的 K-Means 聚类过程具有平均复杂度为 $O(s\cdot m\cdot
    2^{b}\cdot T)$，而且内存复杂度比原始方法要小得多。
- en: 'To facilitate efficient long context LLM inference, the design goal of PQCache
    is to provide overhead-agnostic service. Figure [6](#S3.F6 "Figure 6 ‣ 3.2\. Complexity
    Analysis ‣ 3\. PQCache ‣ PQCache: Product Quantization-based KVCache for Long
    Context LLM Inference") illustrates the computation and communication involved
    in the PQCache-enabled LLM inference, covering both the prefilling and decoding
    phases. The original LLM computation is filled with blue color, while the computation
    and the communication introduced by PQCache are filled with green and red colors,
    which can be divided into four parts: (1) KVCache offloading and PQ structure
    fetching; (2) PQ construction using K-Means clustering; (3) Approximate top-$k$
    approximation, we employ distinct system design to eliminate these computation
    or communication overhead. The system design is detailed in the following sections.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '为了促进高效的长上下文LLM推理，PQCache的设计目标是提供与开销无关的服务。图[6](#S3.F6 "Figure 6 ‣ 3.2\. Complexity
    Analysis ‣ 3\. PQCache ‣ PQCache: Product Quantization-based KVCache for Long
    Context LLM Inference")展示了PQCache支持的LLM推理中涉及的计算和通信，涵盖了预填充和解码阶段。原始LLM计算用蓝色填充，而PQCache引入的计算和通信用绿色和红色填充，分为四部分：（1）KVCache卸载和PQ结构获取；（2）使用K-Means聚类构建PQ；（3）近似前$k$的近似，我们采用不同的系统设计来消除这些计算或通信开销。系统设计将在以下章节中详细介绍。'
- en: '![Refer to caption](img/b40eecdf754fd2267e5afbd553b3d1cf.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b40eecdf754fd2267e5afbd553b3d1cf.png)'
- en: (a) Prefilling.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 预填充。
- en: '![Refer to caption](img/5599496417a31f2816060c815e63b172.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5599496417a31f2816060c815e63b172.png)'
- en: (b) Decoding.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 解码。
- en: Figure 6\. PQCache v.s. sequential scheduling.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. PQCache与顺序调度。
- en: 3.3\. Prefilling Phase
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 预填充阶段
- en: 'At the prefilling phase, on obtaining the input tokens’ keys and values in
    each layer, they can attend to attention computation and be offloaded to CPU simultaneously.
    Given that the attention computation time scales quadratically with sequence length,
    while the communication time scales linearly, the communication can be fully overlapped
    in long context scenarios, as shown in Figure [7](#S3.F7 "Figure 7 ‣ 3.3\. Prefilling
    Phase ‣ 3\. PQCache ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference").'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '在预填充阶段，获取每层输入令牌的键和值后，它们可以同时进行注意力计算并卸载到CPU。鉴于注意力计算时间与序列长度的平方成比例，而通信时间与序列长度线性成比例，在长上下文场景中，通信可以完全重叠，如图[7](#S3.F7
    "Figure 7 ‣ 3.3\. Prefilling Phase ‣ 3\. PQCache ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference")所示。'
- en: 'The K-Means clustering is of great complexity according to Section [3.2](#S3.SS2
    "3.2\. Complexity Analysis ‣ 3\. PQCache ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference"). To enable overhead-agnostic inference,
    we aim to fully utilize the idle CPU resources for clustering. However, as shown
    in Figure [7](#S3.F7 "Figure 7 ‣ 3.3\. Prefilling Phase ‣ 3\. PQCache ‣ PQCache:
    Product Quantization-based KVCache for Long Context LLM Inference"), the clustering
    process on CPU, including PQ construction for all heads in each layer, consumes
    more time than one-layer transformer computation on GPU at the prefilling stage.
    This is because the computational capability of GPUs has grown rapidly over the
    past decades, whereas CPUs are not specifically designed for computationally intensive
    tasks. To address the issue, we propose an adaptive K-Means clustering process
    which limits the number of iterations for clustering, ensuring that the clustering
    can be overlapped by GPU computation. For any given models and devices, we profile
    the computation time of one-layer transformer and K-Means clustering under different
    sequence lengths. By modeling the relationship between computation time and sequence
    length, we can determine the maximum number of K-Means iterations that can overlap
    with the computation for any given sequence length. In Section [4.3.3](#S4.SS3.SSS3
    "4.3.3\. Trade-off between Time and Accuracy ‣ 4.3\. Efficiency ‣ 4\. Experiments
    ‣ PQCache: Product Quantization-based KVCache for Long Context LLM Inference"),
    we empirically study the trade-off between the efficiency and model quality of
    different clustering iterations.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '根据第[3.2](#S3.SS2 "3.2\. Complexity Analysis ‣ 3\. PQCache ‣ PQCache: Product
    Quantization-based KVCache for Long Context LLM Inference")节，K-Means 聚类的复杂性非常高。为了实现与开销无关的推理，我们旨在充分利用闲置的
    CPU 资源进行聚类。然而，如图[7](#S3.F7 "Figure 7 ‣ 3.3\. Prefilling Phase ‣ 3\. PQCache ‣
    PQCache: Product Quantization-based KVCache for Long Context LLM Inference")所示，CPU
    上的聚类过程，包括每层所有头的 PQ 构建，比 GPU 上的单层变换器计算在预填充阶段耗时更多。这是因为 GPU 的计算能力在过去几十年里迅速增长，而 CPU
    并未专门设计用于计算密集型任务。为了解决这个问题，我们提出了一种自适应 K-Means 聚类过程，该过程限制了聚类的迭代次数，确保聚类能够与 GPU 计算重叠。对于任何给定的模型和设备，我们会在不同的序列长度下分析单层变换器和
    K-Means 聚类的计算时间。通过建模计算时间与序列长度之间的关系，我们可以确定在任何给定序列长度下可以与计算重叠的 K-Means 迭代次数。在第[4.3.3](#S4.SS3.SSS3
    "4.3.3\. Trade-off between Time and Accuracy ‣ 4.3\. Efficiency ‣ 4\. Experiments
    ‣ PQCache: Product Quantization-based KVCache for Long Context LLM Inference")节中，我们通过实证研究不同聚类迭代次数的效率与模型质量之间的权衡。'
- en: '![Refer to caption](img/39f9c0d5c4bd5627adf01dfdd611fa47.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/39f9c0d5c4bd5627adf01dfdd611fa47.png)'
- en: Figure 7\. The execution time of one-layer transformer computation, offloading,
    and clustering at the prefilling phase.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 单层变换器计算、卸载和预填充阶段的聚类执行时间。
- en: 3.4\. Decoding Phase
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 解码阶段
- en: 'At the decoding phase, the constructed PQ structure needs to be utilized by
    the attention module in each layer. While the preceding computation is underway,
    the PQ centroids and codes of the current layer can be pre-fetched in parallel.
    Since the PQ structure consumes negligible memory according to Section [3.2](#S3.SS2
    "3.2\. Complexity Analysis ‣ 3\. PQCache ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference"), its communication can directly overlap
    with decoding phase computation.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '在解码阶段，构建的 PQ 结构需要被每层的注意力模块利用。在前面的计算进行时，可以并行预取当前层的 PQ 质心和代码。由于 PQ 结构根据第[3.2](#S3.SS2
    "3.2\. Complexity Analysis ‣ 3\. PQCache ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference")节消耗的内存微不足道，其通信可以直接与解码阶段的计算重叠。'
- en: 'Throughout the entire inference, the only communication that cannot be overlapped
    is the retrieval of the top-$k$ hit tokens, and use them to fetch tokens while
    updating the cache structure. We employ asynchronous updates to avoid additional
    overhead. Experimental results in Section [4.3.4](#S4.SS3.SSS4 "4.3.4\. Cache
    Hit-rate ‣ 4.3\. Efficiency ‣ 4\. Experiments ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference") illustrate the cache hit-rate, which
    helps reduce overall communication.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '在整个推理过程中，唯一不能重叠的通信是检索 top-$k$ 的命中标记，并在更新缓存结构时使用它们进行取回。我们采用异步更新以避免额外的开销。第[4.3.4](#S4.SS3.SSS4
    "4.3.4\. Cache Hit-rate ‣ 4.3\. Efficiency ‣ 4\. Experiments ‣ PQCache: Product
    Quantization-based KVCache for Long Context LLM Inference")节的实验结果展示了缓存命中率，这有助于减少整体通信。'
- en: 4\. Experiments
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验
- en: In this section, we conduct experiments and compare PQCache with existing methods.
    We experimentally show that PQCache achieves both effectiveness and efficiency.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行实验并将PQCache与现有方法进行比较。我们通过实验展示了PQCache在效果和效率上的优势。
- en: 4.1\. Experimental Setup
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 实验设置
- en: 4.1.1\. Models
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 模型
- en: 'We conduct experiments using two representative open-source LLMs: LLaMA-2-7B-Chat (Touvron
    et al., [2023](#bib.bib53)) and Mistral-7B-Instruct-v0.2 (Jiang et al., [2023a](#bib.bib26)).
    The former employs MHA and supports 4K context length, while the latter uses GQA
    and supports 32K context length. Both models share similar LLM architectures.
    We use FP16 for both models, which is a common practice in LLM inference.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个具有代表性的开源LLM进行实验：LLaMA-2-7B-Chat (Touvron et al., [2023](#bib.bib53))和Mistral-7B-Instruct-v0.2
    (Jiang et al., [2023a](#bib.bib26))。前者使用MHA，并支持4K的上下文长度，而后者使用GQA，并支持32K的上下文长度。这两个模型具有相似的LLM架构。我们对这两个模型都使用FP16，这在LLM推理中是常见的做法。
- en: 4.1.2\. Tasks
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 任务
- en: We evaluate PQCache on LongBench (Bai et al., [2023](#bib.bib6)), a widely-used
    benchmark for long-context LLM inference. Since the models are mainly pre-trained
    on English data, we assess all the English tasks within the benchmark. These tasks
    include document question answering, summarization, few-shot learning, and passage
    retrieval. Samples in LongBench have an average input token length of 8K.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在LongBench (Bai et al., [2023](#bib.bib6))上评估PQCache，这是一个广泛使用的长上下文LLM推理基准。由于模型主要在英语数据上进行预训练，我们评估了基准中的所有英语任务。这些任务包括文档问答、总结、少样本学习和段落检索。LongBench中的样本平均输入token长度为8K。
- en: 'We also experiment on two additional tasks: the Needle-in-a-Haystack (Kamradt,
    [2024](#bib.bib29)) and the GSM8k Chain-of-Thought (CoT) reasoning (Wei et al.,
    [2022](#bib.bib57)). The Needle-in-a-Haystack test evaluates the in-context retrieval
    ability of long-context LLMs, asking the model to retrieve a random fact or statement
    placed within a lengthy document. In our experiments, we consider up to 30K document
    length. GSM8k is a math reasoning dataset containing 8K high quality diverse grade
    school math problems. Its CoT variant is a complex reasoning task that require
    model to attend to extensive contextual details for accurate answers, with an
    average input length of 3.7K.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在两个额外的任务上进行了实验：Needle-in-a-Haystack (Kamradt, [2024](#bib.bib29))和GSM8k链式思维（CoT）推理
    (Wei et al., [2022](#bib.bib57))。Needle-in-a-Haystack测试评估长上下文LLM的上下文检索能力，要求模型检索放置在冗长文档中的随机事实或陈述。在我们的实验中，我们考虑了最长30K的文档长度。GSM8k是一个包含8K高质量多样化小学数学问题的数学推理数据集。它的CoT变体是一个复杂的推理任务，需要模型关注广泛的上下文细节以获得准确的答案，平均输入长度为3.7K。
- en: 4.1.3\. Baselines
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 基准线
- en: We consider H2O (Zhang et al., [2023a](#bib.bib64)), SPARQ (Ribar et al., [2023](#bib.bib47)),
    and InfLLM (Xiao et al., [2024](#bib.bib58)) as our baselines. H2O is the most
    widely-used method of KVCache dropping and has been the basis for many enhancements.
    SPARQ and InfLLM are the state-of-the-art methods of KVCache offloading. In addition,
    we further consider a method that retrieves the exact top-$k$ tokens for each
    head, denoted as Oracle. In our experiments, we align the number of tokens for
    selective attention and the data transfer amount in Oracle, SPARQ, InfLLM, and
    PQCache, to achieve a fair comparison. We allow H2O to attend to more tokens,
    matching the memory usage of the selected key-value pairs and the data transfer
    amount in the other methods, following the experiment settings in SPARQ. We refer
    to this baseline as H2O(C), where “C” means compensation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将H2O (Zhang et al., [2023a](#bib.bib64))、SPARQ (Ribar et al., [2023](#bib.bib47))
    和InfLLM (Xiao et al., [2024](#bib.bib58))作为基准线。H2O是最广泛使用的KVCache丢弃方法，并且成为许多改进的基础。SPARQ和InfLLM是KVCache卸载的最先进方法。此外，我们还考虑了一种检索每个头部精确前$k$个token的方法，称为Oracle。在我们的实验中，我们对Oracle、SPARQ、InfLLM和PQCache中的选择性注意和数据传输量进行对齐，以实现公平比较。我们允许H2O关注更多的tokens，以匹配其他方法中选择的键值对的内存使用量和数据传输量，遵循SPARQ中的实验设置。我们将这个基准线称为H2O(C)，其中“C”表示补偿。
- en: Table 2\. LongBench evaluation of the Mistral-7B-inst-v0.2 GQA model (32K context
    length). InfLLM, SPARQ, and PQCache all involve extra communications at an amount
    of 1/128 KVCache memory for pre-calculating relevance; H2O attends to more tokens
    where the memory equals to other methods’ selected tokens and transferred data
    amount.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. Mistral-7B-inst-v0.2 GQA模型（32K上下文长度）的LongBench评估。InfLLM、SPARQ和PQCache都涉及额外的通信，量为1/128
    KVCache内存，用于预计算相关性；H2O关注更多的tokens，其中内存等于其他方法选择的tokens和传输的数据量。
- en: '|  |  | 1/5 #Tokens + 1/128 Extra Comm | 1/10 #Tokens + 1/128 Extra Comm |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 1/5 #Tokens + 1/128 额外通信 | 1/10 #Tokens + 1/128 额外通信 |'
- en: '| Dataset | Full | Oracle | H2O(C) | InfLLM | SPARQ | PQCache | Oracle | H2O(C)
    | InfLLM | SPARQ | PQCache |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 完整 | Oracle | H2O(C) | InfLLM | SPARQ | PQCache | Oracle | H2O(C) |
    InfLLM | SPARQ | PQCache |'
- en: '| NarrativeQA | 21.27 | 22.18 | 22.07 | 19.90 | 22.25 | 22.35 | 22.34 | 21.54
    | 17.55 | 22.45 | 22.62 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| NarrativeQA | 21.27 | 22.18 | 22.07 | 19.90 | 22.25 | 22.35 | 22.34 | 21.54
    | 17.55 | 22.45 | 22.62 |'
- en: '| Qasper | 29.22 | 28.62 | 23.43 | 19.24 | 19.95 | 28.26 | 27.90 | 21.19 |
    14.76 | 17.69 | 28.29 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Qasper | 29.22 | 28.62 | 23.43 | 19.24 | 19.95 | 28.26 | 27.90 | 21.19 |
    14.76 | 17.69 | 28.29 |'
- en: '| MultiFieldQA | 47.84 | 48.02 | 43.31 | 41.13 | 39.22 | 48.27 | 48.23 | 39.22
    | 36.66 | 35.02 | 47.95 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| MultiFieldQA | 47.84 | 48.02 | 43.31 | 41.13 | 39.22 | 48.27 | 48.23 | 39.22
    | 36.66 | 35.02 | 47.95 |'
- en: '| HotpotQA | 37.92 | 37.16 | 36.86 | 33.97 | 33.48 | 37.12 | 36.74 | 33.02
    | 31.09 | 31.68 | 36.24 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| HotpotQA | 37.92 | 37.16 | 36.86 | 33.97 | 33.48 | 37.12 | 36.74 | 33.02
    | 31.09 | 31.68 | 36.24 |'
- en: '| 2WikiMQA | 21.83 | 21.02 | 18.16 | 18.48 | 16.68 | 21.25 | 21.16 | 17.76
    | 16.15 | 16.14 | 21.21 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 2WikiMQA | 21.83 | 21.02 | 18.16 | 18.48 | 16.68 | 21.25 | 21.16 | 17.76
    | 16.15 | 16.14 | 21.21 |'
- en: '| Musique | 18.58 | 18.45 | 17.77 | 18.96 | 16.10 | 18.37 | 18.34 | 16.85 |
    14.08 | 13.18 | 18.47 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Musique | 18.58 | 18.45 | 17.77 | 18.96 | 16.10 | 18.37 | 18.34 | 16.85 |
    14.08 | 13.18 | 18.47 |'
- en: '| GovReport | 31.57 | 31.98 | 29.13 | 30.49 | 27.12 | 31.53 | 31.84 | 27.36
    | 29.19 | 24.68 | 31.12 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| GovReport | 31.57 | 31.98 | 29.13 | 30.49 | 27.12 | 31.53 | 31.84 | 27.36
    | 29.19 | 24.68 | 31.12 |'
- en: '| QMSum | 24.31 | 23.76 | 23.23 | 22.64 | 22.21 | 23.79 | 23.98 | 23.12 | 21.52
    | 22.09 | 23.27 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| QMSum | 24.31 | 23.76 | 23.23 | 22.64 | 22.21 | 23.79 | 23.98 | 23.12 | 21.52
    | 22.09 | 23.27 |'
- en: '| MultiNews | 26.85 | 26.79 | 25.37 | 24.38 | 23.77 | 26.70 | 26.86 | 24.94
    | 23.19 | 22.40 | 26.53 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| MultiNews | 26.85 | 26.79 | 25.37 | 24.38 | 23.77 | 26.70 | 26.86 | 24.94
    | 23.19 | 22.40 | 26.53 |'
- en: '| TREC | 71.00 | 71.00 | 68.00 | 59.50 | 62.00 | 71.00 | 71.00 | 67.50 | 53.00
    | 55.50 | 70.50 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| TREC | 71.00 | 71.00 | 68.00 | 59.50 | 62.00 | 71.00 | 71.00 | 67.50 | 53.00
    | 55.50 | 70.50 |'
- en: '| TriviaQA | 86.23 | 86.22 | 86.17 | 85.80 | 86.58 | 86.14 | 86.40 | 86.45
    | 85.54 | 85.98 | 86.40 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| TriviaQA | 86.23 | 86.22 | 86.17 | 85.80 | 86.58 | 86.14 | 86.40 | 86.45
    | 85.54 | 85.98 | 86.40 |'
- en: '| SAMSum | 43.04 | 43.27 | 42.61 | 41.51 | 42.57 | 43.35 | 43.47 | 42.21 |
    39.55 | 42.79 | 43.13 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| SAMSum | 43.04 | 43.27 | 42.61 | 41.51 | 42.57 | 43.35 | 43.47 | 42.21 |
    39.55 | 42.79 | 43.13 |'
- en: '| Count | 2.62 | 3.42 | 3.50 | 2.96 | 4.80 | 3.93 | 3.26 | 2.44 | 2.00 | 3.15
    | 3.56 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Count | 2.62 | 3.42 | 3.50 | 2.96 | 4.80 | 3.93 | 3.26 | 2.44 | 2.00 | 3.15
    | 3.56 |'
- en: '| Retrieval | 88.74 | 88.40 | 56.56 | 35.67 | 57.19 | 88.44 | 89.80 | 37.41
    | 20.00 | 39.39 | 88.63 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 检索 | 88.74 | 88.40 | 56.56 | 35.67 | 57.19 | 88.44 | 89.80 | 37.41 | 20.00
    | 39.39 | 88.63 |'
- en: '| Average | 39.32 | 39.30 | 35.44 | 32.48 | 33.85 | 39.32 | 39.38 | 32.93 |
    28.88 | 30.87 | 39.14 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 39.32 | 39.30 | 35.44 | 32.48 | 33.85 | 39.32 | 39.38 | 32.93 | 28.88
    | 30.87 | 39.14 |'
- en: Table 3\. LongBench evaluation of the LLaMA-2-7B-Chat MHA model (4K context
    length). InfLLM, SPARQ, and PQCache all involve extra communications at an amount
    of 1/128 KVCache memory for pre-calculating relevance; H2O attends to more tokens
    where the memory equals to other methods’ selected tokens and transferred data
    amount.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. LongBench 对 LLaMA-2-7B-Chat MHA 模型（4K 上下文长度）的评估。InfLLM、SPARQ 和 PQCache
    都涉及额外的通信量，为预计算相关性提供了 1/128 的 KVCache 内存；H2O 处理的标记更多，其内存量等于其他方法选择的标记和传输的数据量。
- en: '|  |  | 1/5 #Tokens + 1/128 Extra Comm | 1/10 #Tokens + 1/128 Extra Comm |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 1/5 #Tokens + 1/128 额外通信 | 1/10 #Tokens + 1/128 额外通信 |'
- en: '| Dataset | Full | Oracle | H2O(C) | InfLLM | SPARQ | PQCache | Oracle | H2O(C)
    | InfLLM | SPARQ | PQCache |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 完整 | Oracle | H2O(C) | InfLLM | SPARQ | PQCache | Oracle | H2O(C) |
    InfLLM | SPARQ | PQCache |'
- en: '| NarrativeQA | 18.78 | 18.55 | 17.53 | 12.45 | 17.40 | 19.01 | 18.05 | 17.04
    | 11.88 | 16.44 | 17.56 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| NarrativeQA | 18.78 | 18.55 | 17.53 | 12.45 | 17.40 | 19.01 | 18.05 | 17.04
    | 11.88 | 16.44 | 17.56 |'
- en: '| Qasper | 22.11 | 20.40 | 19.37 | 14.0 | 20.21 | 21.07 | 20.38 | 18.33 | 12.48
    | 17.31 | 20.08 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Qasper | 22.11 | 20.40 | 19.37 | 14.0 | 20.21 | 21.07 | 20.38 | 18.33 | 12.48
    | 17.31 | 20.08 |'
- en: '| MultiFieldQA | 36.77 | 37.58 | 31.69 | 29.39 | 33.21 | 38.72 | 37.23 | 31.82
    | 23.32 | 31.27 | 37.79 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| MultiFieldQA | 36.77 | 37.58 | 31.69 | 29.39 | 33.21 | 38.72 | 37.23 | 31.82
    | 23.32 | 31.27 | 37.79 |'
- en: '| HotpotQA | 27.83 | 28.25 | 26.44 | 25.67 | 24.38 | 27.78 | 27.79 | 25.72
    | 25.01 | 23.66 | 26.36 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| HotpotQA | 27.83 | 28.25 | 26.44 | 25.67 | 24.38 | 27.78 | 27.79 | 25.72
    | 25.01 | 23.66 | 26.36 |'
- en: '| 2WikiMQA | 31.51 | 31.96 | 29.65 | 23.95 | 29.99 | 31.23 | 31.08 | 30.68
    | 25.73 | 29.02 | 30.00 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 2WikiMQA | 31.51 | 31.96 | 29.65 | 23.95 | 29.99 | 31.23 | 31.08 | 30.68
    | 25.73 | 29.02 | 30.00 |'
- en: '| Musique | 8.31 | 8.23 | 8.64 | 9.03 | 7.01 | 7.65 | 8.19 | 8.40 | 8.68 |
    4.91 | 7.24 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Musique | 8.31 | 8.23 | 8.64 | 9.03 | 7.01 | 7.65 | 8.19 | 8.40 | 8.68 |
    4.91 | 7.24 |'
- en: '| GovReport | 26.91 | 26.86 | 23.62 | 23.54 | 24.04 | 26.91 | 26.50 | 22.89
    | 23.22 | 22.47 | 26.69 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| GovReport | 26.91 | 26.86 | 23.62 | 23.54 | 24.04 | 26.91 | 26.50 | 22.89
    | 23.22 | 22.47 | 26.69 |'
- en: '| QMSum | 20.68 | 20.31 | 20.97 | 19.11 | 21.03 | 20.94 | 20.57 | 20.76 | 18.69
    | 20.48 | 21.33 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| QMSum | 20.68 | 20.31 | 20.97 | 19.11 | 21.03 | 20.94 | 20.57 | 20.76 | 18.69
    | 20.48 | 21.33 |'
- en: '| MultiNews | 26.23 | 26.08 | 24.31 | 22.32 | 25.15 | 26.46 | 26.44 | 24.28
    | 20.52 | 23.48 | 26.57 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| MultiNews | 26.23 | 26.08 | 24.31 | 22.32 | 25.15 | 26.46 | 26.44 | 24.28
    | 20.52 | 23.48 | 26.57 |'
- en: '| TREC | 64.00 | 64.00 | 62.50 | 48.0 | 62.50 | 64.00 | 64.00 | 60.50 | 40.0
    | 60.50 | 63.50 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| TREC | 64.00 | 64.00 | 62.50 | 48.0 | 62.50 | 64.00 | 64.00 | 60.50 | 40.0
    | 60.50 | 63.50 |'
- en: '| TriviaQA | 83.26 | 83.16 | 82.56 | 71.15 | 81.09 | 83.43 | 81.74 | 81.27
    | 61.45 | 80.99 | 83.23 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| TriviaQA | 83.26 | 83.16 | 82.56 | 71.15 | 81.09 | 83.43 | 81.74 | 81.27
    | 61.45 | 80.99 | 83.23 |'
- en: '| SAMSum | 41.53 | 41.31 | 40.07 | 37.3 | 36.90 | 41.67 | 41.26 | 39.76 | 34.6
    | 39.79 | 41.19 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| SAMSum | 41.53 | 41.31 | 40.07 | 37.3 | 36.90 | 41.67 | 41.26 | 39.76 | 34.6
    | 39.79 | 41.19 |'
- en: '| Count | 2.92 | 2.98 | 2.48 | 2.68 | 2.35 | 3.03 | 3.80 | 2.68 | 2.92 | 2.58
    | 3.01 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 计数 | 2.92 | 2.98 | 2.48 | 2.68 | 2.35 | 3.03 | 3.80 | 2.68 | 2.92 | 2.58
    | 3.01 |'
- en: '| Retrieval | 8.00 | 7.50 | 6.50 | 6.25 | 5.50 | 7.00 | 5.50 | 4.50 | 5.00
    | 6.50 | 6.50 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 检索 | 8.00 | 7.50 | 6.50 | 6.25 | 5.50 | 7.00 | 5.50 | 4.50 | 5.00 | 6.50
    | 6.50 |'
- en: '| Average | 29.92 | 29.80 | 28.31 | 24.66 | 27.91 | 29.92 | 29.47 | 27.76 |
    22.39 | 27.10 | 29.36 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 29.92 | 29.80 | 28.31 | 24.66 | 27.91 | 29.92 | 29.47 | 27.76 | 22.39
    | 27.10 | 29.36 |'
- en: '![Refer to caption](img/39ef0eadd8c00b88ba97bc74a983c3e0.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/39ef0eadd8c00b88ba97bc74a983c3e0.png)'
- en: (a) H2O(C).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: (a) H2O(C)。
- en: '![Refer to caption](img/2cc2129de5a5ab9667deb5e56a4bc55b.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2cc2129de5a5ab9667deb5e56a4bc55b.png)'
- en: (b) SPARQ.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: (b) SPARQ。
- en: '![Refer to caption](img/924d1b2085916962d665c270bc8b72d5.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/924d1b2085916962d665c270bc8b72d5.png)'
- en: (c) InfLLM.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: (c) InfLLM。
- en: '![Refer to caption](img/fd192a1e160baefbfeed2d755175e495.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fd192a1e160baefbfeed2d755175e495.png)'
- en: (d) PQCache.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: (d) PQCache。
- en: Figure 8\. Experimental results of the Needle-in-a-Haystack test.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. Needle-in-a-Haystack 测试的实验结果。
- en: 4.1.4\. Hardware Environment and Hyperparameters
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4\. 硬件环境和超参数
- en: We conduct all experiments on NVIDIA A800 40GB GPU cards. Most of the hyperparameters
    are determined based on the number of tokens and the amount of data transferred.
    We use $m=2$ for PQ by default. For other hyperparameters, we align them with
    the settings from the corresponding papers or open-source codes.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 NVIDIA A800 40GB GPU 卡上进行所有实验。大多数超参数是基于标记数量和传输数据量确定的。默认情况下，我们对 PQ 使用 $m=2$。对于其他超参数，我们将其与相应论文或开源代码的设置对齐。
- en: 4.2\. Model Performance
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 模型性能
- en: 4.2.1\. LongBench
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. LongBench
- en: 'The LongBench results of the methods on two LLMs are presented in Table [2](#S4.T2
    "Table 2 ‣ 4.1.3\. Baselines ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ PQCache:
    Product Quantization-based KVCache for Long Context LLM Inference") and [3](#S4.T3
    "Table 3 ‣ 4.1.3\. Baselines ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ PQCache:
    Product Quantization-based KVCache for Long Context LLM Inference"). LongBench
    uses different metrics for each dataset and calculates an average score to measure
    overall performance. We consider including 1/5 and 1/10 of the input tokens in
    selective attention, respectively, with an extra communication that equals to
    1/128 of the KVCache memory: for PQCache, we use $m=2$; for InfLLM, we use 1 representative
    token from every 128 tokens. H2O(C) is allowed to attend to more tokens as introduced
    in Section [4.1.3](#S4.SS1.SSS3 "4.1.3\. Baselines ‣ 4.1\. Experimental Setup
    ‣ 4\. Experiments ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference"). Excluding Oracle, the best results for each setting, are highlighted
    in bold.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 'LongBench 在两种 LLM 上的方法结果见表 [2](#S4.T2 "表 2 ‣ 4.1.3\. 基线 ‣ 4.1\. 实验设置 ‣ 4\.
    实验 ‣ PQCache: 基于产品量化的 KVCache 用于长上下文 LLM 推理") 和 [3](#S4.T3 "表 3 ‣ 4.1.3\. 基线 ‣
    4.1\. 实验设置 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的 KVCache 用于长上下文 LLM 推理")。LongBench 对每个数据集使用不同的指标，并计算平均得分以衡量总体性能。我们考虑在选择性注意中包括
    1/5 和 1/10 的输入标记，并额外传输等于 KVCache 内存 1/128 的数据：对于 PQCache，我们使用 $m=2$；对于 InfLLM，我们使用每
    128 个标记中的 1 个代表性标记。H2O(C) 允许关注更多标记，如第 [4.1.3](#S4.SS1.SSS3 "4.1.3\. 基线 ‣ 4.1\.
    实验设置 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的 KVCache 用于长上下文 LLM 推理") 节所介绍。除去 Oracle，每个设置的最佳结果以粗体标出。'
- en: On average, models without compression (denoted as Full) can achieve the best
    results, since there is nearly no information loss¹¹1In LongBench, when the sequence
    length exceeds the model’s maximum context length, only the initial and the last
    tokens are used (Bai et al., [2023](#bib.bib6)), resulting in information loss.
    . PQCache outperforms the major baselines (i.e., H2O(C), InfLLM, and SPARQ) on
    most of the datasets. Although PQCache achieves slightly lower scores in a handful
    of cases, it exhibits substantial improvements on average. Concretely, PQCache
    achieves +3.88 and +6.21 improvements on Mistral-7B, and achieves +1.61 and +1.60
    improvements on LLaMa2-7B, respectively. Note that the offloading-based counterparts,
    InfLLM and SPARQ, have the worst performance on average due to the limited additional
    communication for minimized latency. In contrast, PQCache performs well under
    the same constraint, validating the strength of our work.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，没有压缩的模型（标记为 Full）可以获得最佳结果，因为几乎没有信息损失¹¹1在 LongBench 中，当序列长度超过模型的最大上下文长度时，仅使用初始和最后的标记（Bai
    et al., [2023](#bib.bib6)），导致信息损失。 。PQCache 在大多数数据集上优于主要基准（即 H2O(C)、InfLLM 和 SPARQ）。尽管
    PQCache 在少数情况下的分数略低，但总体上显示了显著的改进。具体而言，PQCache 在 Mistral-7B 上实现了 +3.88 和 +6.21
    的改进，在 LLaMa2-7B 上实现了 +1.61 和 +1.60 的改进。值得注意的是，基于卸载的对照组 InfLLM 和 SPARQ 在平均性能上最差，这主要是由于为了最小化延迟而有限的额外通信。相比之下，PQCache
    在相同的限制下表现良好，验证了我们工作的优势。
- en: Oracle is an ideal approach with the exact top-$k$ tokens for selective attention,
    which gives excellent performance in most cases. However, we observe that PQCache
    even beats Oracle and achieves the same score as the uncompressed counterpart
    in the “1/5#Tokens” cases. This suggests that clustering may help PQCache uncover
    intrinsic structures within the KVCache latent space, thus leading to promising
    results. Furthermore, although it is usually expected that the performance should
    drop when there are fewer tokens, there are exceptions where the opposite happens.
    This could be because not all tokens are useful for generating new ones, so getting
    rid of unnecessary ones might enhance inference.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Oracle 是一种理想的方法，具有准确的前 $k$ 个标记用于选择性注意力，在大多数情况下表现优异。然而，我们观察到 PQCache 甚至超越了 Oracle，并在“1/5#Tokens”情况下达到了与未压缩对照相同的分数。这表明，聚类可能有助于
    PQCache 发掘 KVCache 潜在空间中的内在结构，从而取得了有希望的结果。此外，尽管通常预期当标记减少时性能会下降，但也有例外情况出现相反的结果。这可能是因为并非所有标记都对生成新标记有用，因此去除不必要的标记可能会增强推理。
- en: 4.2.2\. Needle-in-a-Haystack
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 针对干草堆中的“针”
- en: 'We use Mistral-7B-inst-v0.2 for this test. We employ the common setting (Anthropic,
    [2023](#bib.bib5)): the “haystack” is Paul Graham’s Essays, and the “needle” is
    “The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park
    on a sunny day.” For each experiment, we use 1/5 the number of tokens in selective
    attention, and 1/128 extra communication. The results are shown in Figure [8](#S4.F8
    "Figure 8 ‣ 4.1.3\. Baselines ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ PQCache:
    Product Quantization-based KVCache for Long Context LLM Inference"), where the
    $x$-axis represents the position that the “needle” hides. Greener shades indicate
    greater accuracy.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在这次测试中使用了 Mistral-7B-inst-v0.2。我们采用了常见的设置（Anthropic, [2023](#bib.bib5)）："干草堆"是保罗·格雷厄姆的《随笔》，"针"是“在旧金山最好的事情就是吃一个三明治，然后在阳光明媚的日子里坐在多洛雷斯公园。”对于每个实验，我们在选择性注意力中使用了1/5的标记数，并增加了1/128的额外通信。结果如图
    [8](#S4.F8 "Figure 8 ‣ 4.1.3\. Baselines ‣ 4.1\. Experimental Setup ‣ 4\. Experiments
    ‣ PQCache: Product Quantization-based KVCache for Long Context LLM Inference")
    所示，其中 $x$-轴表示“针”隐藏的位置。绿色阴影表示准确度更高。'
- en: Among all the methods, PQCache achieves the best performance, successfully locating
    the needle in nearly all scenarios. The major baselines, however, fail to retrieve
    the needle in a substantial amount of cases. InfLLM, in particular, struggles
    to find the needle in most cases, possibly due to its reliance on block-partitioning
    and the needle not being considered as representative tokens. It can locate the
    needle when it is among the initial or local tokens, which we include in attention.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有方法中，PQCache 的表现最好，能够在几乎所有场景中成功找到“针”。然而，主要的基准方法在大量情况下未能检索到“针”。特别是，InfLLM 在大多数情况下都难以找到“针”，这可能是由于它依赖于块分区，而“针”没有被视为代表性标记。当“针”处于初始标记或局部标记中时，InfLLM
    可以找到它，这些标记被包含在注意力中。
- en: 4.2.3\. GSM8k CoT Reasoning
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. GSM8k CoT 推理
- en: 'We use Mistral-7B-inst-v0.2 for this task. Our prompt strategy involves 8 questions
    with 9-step reasoning and 2 questions with 8-step reasoning per sample, a common
    setup for long context inference (Fu et al., [2023](#bib.bib15)). We use 1/128
    extra communications. As shown in Figure [9(a)](#S4.F9.sf1 "In Figure 9 ‣ 4.2.4\.
    Impact of Extra Communication ‣ 4.2\. Model Performance ‣ 4\. Experiments ‣ PQCache:
    Product Quantization-based KVCache for Long Context LLM Inference"), PQCache consistently
    outperforms H2O, SPARQ, and InfLLM under varying token counts. Some results even
    surpass the uncompressed counterpart, suggesting that using part of the tokens
    can lead to improvements. Contrary to previous findings (Kang et al., [2024](#bib.bib30)),
    we observe that H2O performs well using the advanced Mistral model. H2O(C) performs
    better than PQCache using 1/10 number of tokens, as it is allowed to access more
    tokens.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用 Mistral-7B-inst-v0.2 进行此任务。我们的提示策略包括每个样本 8 个问题的 9 步推理和 2 个问题的 8 步推理，这是长上下文推理的常见设置（Fu
    等，[2023](#bib.bib15)）。我们使用了 1/128 的额外通信。如图 [9(a)](#S4.F9.sf1 "在图 9 ‣ 4.2.4\. 额外通信的影响
    ‣ 4.2\. 模型性能 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的 KVCache 用于长上下文 LLM 推理") 所示，PQCache 在不同的
    token 数量下始终优于 H2O、SPARQ 和 InfLLM。某些结果甚至超越了未压缩的对照组，表明使用部分 token 可以带来改进。与之前的研究发现（Kang
    等，[2024](#bib.bib30)）相反，我们观察到 H2O 在使用先进的 Mistral 模型时表现良好。H2O(C) 在使用 1/10 数量的 tokens
    时表现优于 PQCache，因为它可以访问更多的 tokens。'
- en: 4.2.4\. Impact of Extra Communication
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4\. 额外通信的影响
- en: 'We investigate how the amount of extra communication impacts the model performance
    on the HotPotQA dataset, as shown in Figure [9(b)](#S4.F9.sf2 "In Figure 9 ‣ 4.2.4\.
    Impact of Extra Communication ‣ 4.2\. Model Performance ‣ 4\. Experiments ‣ PQCache:
    Product Quantization-based KVCache for Long Context LLM Inference"). Fixing 1/10
    tokens used, as the amount of extra communication increases from 1/128 to 1/16
    of the KVCache memory, InfLLM and PQCache show relatively stable performance,
    while SPARQ has steadily improved performance. PQCache consistently achieves high
    scores, outperforming the other methods when the communication amount is no larger
    than 1/32 KVCache. SPARQ already incurs significant latency under the 1/128 case,
    as shown in Section [4.3.2](#S4.SS3.SSS2 "4.3.2\. Decoding ‣ 4.3\. Efficiency
    ‣ 4\. Experiments ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference"). Even it performs well under the 1/16 case, the latency becomes
    increasingly unacceptable. In low-communication scenarios, which are suitable
    for practical usage, PQCache achieves the best model performance.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '我们研究了额外通信量对 HotPotQA 数据集上模型性能的影响，如图 [9(b)](#S4.F9.sf2 "在图 9 ‣ 4.2.4\. 额外通信的影响
    ‣ 4.2\. 模型性能 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的 KVCache 用于长上下文 LLM 推理") 所示。在固定了 1/10
    的 tokens 使用量的情况下，当额外通信量从 KVCache 内存的 1/128 增加到 1/16 时，InfLLM 和 PQCache 的性能相对稳定，而
    SPARQ 的性能则持续改善。当通信量不大于 KVCache 的 1/32 时，PQCache 一贯地取得了高分，优于其他方法。SPARQ 在 1/128
    的情况下已经产生了显著的延迟，如第 [4.3.2](#S4.SS3.SSS2 "4.3.2\. 解码 ‣ 4.3\. 效率 ‣ 4\. 实验 ‣ PQCache:
    基于产品量化的 KVCache 用于长上下文 LLM 推理") 节所示。尽管在 1/16 的情况下表现良好，但延迟变得越来越不可接受。在适用于实际使用的低通信场景中，PQCache
    实现了最佳的模型性能。'
- en: '![Refer to caption](img/c55dcb80cd148d43455e6e7109b4eb2f.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c55dcb80cd148d43455e6e7109b4eb2f.png)'
- en: (a) Results on GSM8k CoT.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 在 GSM8k CoT 上的结果。
- en: '![Refer to caption](img/9ad194a9dfd2ee82308306ca87dbb34a.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9ad194a9dfd2ee82308306ca87dbb34a.png)'
- en: (b) Varying communications.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 通信量的变化。
- en: '![Refer to caption](img/f99d6d6760ba843f094bc193a89e42d3.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f99d6d6760ba843f094bc193a89e42d3.png)'
- en: (c) Different PQ configurations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 不同的 PQ 配置。
- en: Figure 9\. Model performance on GSM8k CoT, and other hyper-parameters.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. GSM8k CoT 上的模型性能，以及其他超参数。
- en: '![Refer to caption](img/694eb44b8b9e6f74d2b5bf9ec159d378.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/694eb44b8b9e6f74d2b5bf9ec159d378.png)'
- en: (a) Time to 2nd token.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 到达第二个 token 的时间。
- en: '![Refer to caption](img/d9b5ba6c88cc1e7746518c58b46ad332.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d9b5ba6c88cc1e7746518c58b46ad332.png)'
- en: (b) Time per output token.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 每个输出 token 的时间。
- en: '![Refer to caption](img/aeeff48a5d3e0e0b2c2efe85c0b03830.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/aeeff48a5d3e0e0b2c2efe85c0b03830.png)'
- en: (c) Trade-off time and score.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 时间和得分的权衡。
- en: '![Refer to caption](img/2cd740ddd608c22119e67a94f36a41bb.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2cd740ddd608c22119e67a94f36a41bb.png)'
- en: (d) Cache hit-rate.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 缓存命中率。
- en: Figure 10\. Latency experiments.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 延迟实验。
- en: 4.2.5\. Impact of PQ Configuration
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5\. PQ 配置的影响
- en: We evaluate the effect of PQ configurations. Since PQ has a memory consumption
    of $O(s\cdot m\cdot b+2^{b}\cdot d_{h})$ offers more stable latency and lower
    memory usage while still delivering promising model performance. Therefore, we
    choose it as the default configuration.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了PQ配置的效果。由于PQ具有$O(s\cdot m\cdot b+2^{b}\cdot d_{h})$的内存消耗，提供了更稳定的延迟和较低的内存使用，同时仍能提供有前景的模型性能。因此，我们选择它作为默认配置。
- en: 4.3\. Efficiency
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 效率
- en: 4.3.1\. Prefilling
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 预填充
- en: 'In PQCache, K-Means clustering occurs concurrently with GPU computation. While
    it doesn’t affect the first token generation, subsequent tokens depend on clustering
    results. To assess system optimization, we use Time To 2nd Token (TT2T), considering
    query entry to LLM output time and KVCache management overhead. As shown in Figure [10(a)](#S4.F10.sf1
    "In Figure 10 ‣ 4.2.4\. Impact of Extra Communication ‣ 4.2\. Model Performance
    ‣ 4\. Experiments ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference"), with overlapping and adaptive clustering, PQCache can achieve
    the lowest TT2T. All baseline methods have significant overhead. Since H2O collects
    attention scores during prefilling, it cannot utilize FlashAttention for acceleration
    and encounters OOM when dealing with lengthy input. SPARQ has no prefilling overhead,
    but its decoding process is slow (see Section [4.3.2](#S4.SS3.SSS2 "4.3.2\. Decoding
    ‣ 4.3\. Efficiency ‣ 4\. Experiments ‣ PQCache: Product Quantization-based KVCache
    for Long Context LLM Inference")). InfLLM incurs time overhead due to the setup
    required for block-level KVCache management.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '在PQCache中，K-Means聚类与GPU计算同时进行。虽然这不会影响第一次生成的标记，但后续标记依赖于聚类结果。为了评估系统优化，我们使用第二标记时间（TT2T），考虑从查询输入到LLM输出的时间以及KVCache管理开销。如图 [10(a)](#S4.F10.sf1
    "图10 ‣ 4.2.4\. 额外通信的影响 ‣ 4.2\. 模型性能 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的长上下文LLM推理KVCache")所示，通过重叠和自适应聚类，PQCache可以实现最低的TT2T。所有基线方法都有显著的开销。由于H2O在预填充过程中收集注意力分数，它无法利用FlashAttention进行加速，并且在处理较长输入时遇到OOM。SPARQ没有预填充开销，但其解码过程较慢（见第[4.3.2](#S4.SS3.SSS2
    "4.3.2\. 解码 ‣ 4.3\. 效率 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的长上下文LLM推理KVCache")节）。InfLLM由于块级KVCache管理所需的设置而产生时间开销。'
- en: 4.3.2\. Decoding
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 解码
- en: 'Time Per Output Token (TPOT) measures the time of each decoding step. We compare
    the TPOT of H2O, SPARQ, InfLLM, and PQCache in Figure [10(b)](#S4.F10.sf2 "In
    Figure 10 ‣ 4.2.4\. Impact of Extra Communication ‣ 4.2\. Model Performance ‣
    4\. Experiments ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference"). Here we use 1/5 number of tokens in selective attention, and
    a 4096-token GPU cache. SPARQ exhibits the highest latency due to its sequential
    computation and communication, with the communication scaling linearly with the
    input sequence length. All the other methods exhibit per-token latency faster
    than the human reading speed, which is around 250 words ($\approx$333 tokens)
    per minute (Zhong et al., [2024](#bib.bib65)). H2O avoids extra communications,
    while InfLLM and PQCache both leverage system optimizations to accelerate decoding.
    InfLLM’s block-level token management allows it to efficiently gather data from
    the CPU; however, this block-level assumption negatively impacts the model’s overall
    quality. PQCache incorporates prefetching and caching, achieving an acceptable
    TPOT while not degrading model quality.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '输出标记时间（TPOT）测量每个解码步骤的时间。我们在图 [10(b)](#S4.F10.sf2 "图10 ‣ 4.2.4\. 额外通信的影响 ‣ 4.2\.
    模型性能 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的长上下文LLM推理KVCache")中比较了H2O、SPARQ、InfLLM和PQCache的TPOT。这里我们使用了选择性注意中的1/5数量的标记和一个4096标记的GPU缓存。由于其顺序计算和通信，SPARQ表现出最高的延迟，其通信随着输入序列长度线性扩展。所有其他方法的每标记延迟都快于人类阅读速度，大约为每分钟250个单词（$\approx$333个标记）（Zhong
    et al., [2024](#bib.bib65)）。H2O避免了额外的通信，而InfLLM和PQCache都利用系统优化来加速解码。InfLLM的块级标记管理允许它高效地从CPU收集数据；然而，这种块级假设对模型的整体质量产生了负面影响。PQCache结合了预取和缓存，在不降低模型质量的情况下实现了可接受的TPOT。'
- en: 4.3.3\. Trade-off between Time and Accuracy
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. 时间与准确度的权衡
- en: 'In Section [3.3](#S3.SS3 "3.3\. Prefilling Phase ‣ 3\. PQCache ‣ PQCache: Product
    Quantization-based KVCache for Long Context LLM Inference"), we design an adaptive
    K-Means clustering strategy to eliminate latency. To investigate the impact of
    this strategy on model accuracy, we conduct an experiment with varying numbers
    of clustering iterations on the HotpotQA dataset, with 1/10 tokens involved in
    attention. As shown in Figure [10(c)](#S4.F10.sf3 "In Figure 10 ‣ 4.2.4\. Impact
    of Extra Communication ‣ 4.2\. Model Performance ‣ 4\. Experiments ‣ PQCache:
    Product Quantization-based KVCache for Long Context LLM Inference"), the adaptive
    strategy has the lowest clustering time with good enough model quality. Though
    clustering with more iterations results in better scores, the associated increase
    in inference latency is considerable. Trading-off time and accuracy, the adaptive
    strategy is the most practical choice for real-world applications. We expose an
    interface that lets users set the number of iterations, enabling them to balance
    model performance and latency for their specific needs.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '在第 [3.3](#S3.SS3 "3.3\. 预填充阶段 ‣ 3\. PQCache ‣ PQCache: 基于产品量化的 KVCache 用于长上下文
    LLM 推理") 节中，我们设计了一种自适应 K-Means 聚类策略以消除延迟。为了研究该策略对模型准确性的影响，我们在 HotpotQA 数据集上进行了实验，测试了不同的聚类迭代次数，涉及
    1/10 的标记用于注意力。如图 [10(c)](#S4.F10.sf3 "图 10 ‣ 4.2.4\. 额外通信的影响 ‣ 4.2\. 模型性能 ‣ 4\.
    实验 ‣ PQCache: 基于产品量化的 KVCache 用于长上下文 LLM 推理") 所示，自适应策略具有最低的聚类时间，同时模型质量足够好。尽管更多的聚类迭代会带来更好的评分，但相关的推理延迟增加是显著的。在时间和准确性之间进行权衡，自适应策略是现实世界应用中最实用的选择。我们提供了一个接口，允许用户设置迭代次数，使他们能够根据具体需求平衡模型性能和延迟。'
- en: 4.3.4\. Cache Hit-rate
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4\. 缓存命中率
- en: 'We assess the cache hit-rate for Least Recently Used (LRU) and Least Frequently
    Used (LFU) policies across varying numbers of top-$k_{cache}$ blocks involved
    during decoding. Our experiments are conducted on the HotpotQA dataset, with 1/10
    tokens in selective attention and 4096 tokens in GPU cache (128 tokens per block).
    As shown in Figure [10(d)](#S4.F10.sf4 "In Figure 10 ‣ 4.2.4\. Impact of Extra
    Communication ‣ 4.2\. Model Performance ‣ 4\. Experiments ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference"), both LRU and LFU exhibit similar performance,
    achieving around 0.5 hit-rate across different numbers of blocks. As block count
    increases, the hit-rate initially rises due to more tokens being found within
    blocks. However, it eventually declines as blocks with fewer hits update the cache
    structure, disrupting the normal cache logic. In practice, we set the number of
    blocks to 32, yielding a hit-rate around 0.6, which reduces communication by 60%.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了在解码过程中涉及不同数量的 top-$k_{cache}$ 块的最少最近使用（LRU）和最少频繁使用（LFU）策略的缓存命中率。我们的实验在
    HotpotQA 数据集上进行，其中 1/10 的标记用于选择性注意力，GPU 缓存中有 4096 个标记（每块 128 个标记）。如图 [10(d)](#S4.F10.sf4
    "图 10 ‣ 4.2.4\. 额外通信的影响 ‣ 4.2\. 模型性能 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的 KVCache 用于长上下文
    LLM 推理") 所示，LRU 和 LFU 的表现相似，在不同数量的块中命中率都约为 0.5。随着块数的增加，由于更多标记被找到在块内，命中率最初会上升。然而，最终命中率会下降，因为命中较少的块更新缓存结构，干扰了正常的缓存逻辑。在实际操作中，我们将块数设置为
    32，命中率约为 0.6，从而减少了 60% 的通信。'
- en: 5\. Related Work
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 相关工作
- en: Selective Attention for KVCache
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: KVCache 的选择性注意力
- en: To eliminate the impact of memory-intensive KVCache, a group of methods include
    only essential tokens for attention computation during LLM inference. One way
    is to discard unnecessary tokens. LM-Infinite (Han et al., [2023](#bib.bib21))
    and Streaming-LLM (Xiao et al., [2023](#bib.bib59)) only preserve the initial
    tokens and the most recent tokens. H2O (Zhang et al., [2023a](#bib.bib64)) and
    Scissorhands (Liu et al., [2023a](#bib.bib34)) utilize attention scores to identify
    important tokens. Their following works (Ge et al., [2023](#bib.bib18); Adnan
    et al., [2024](#bib.bib2); Wang and Gan, [2024](#bib.bib56); Ren and Zhu, [2024](#bib.bib45))
    have explored adaptive token selection and additional metrics for better model
    accuracy. The LLMLingua series (Jiang et al., [2023b](#bib.bib27); Pan et al.,
    [2024](#bib.bib42)) leverage an auxiliary small model to tell which tokens are
    necessary. Since token-level compression evicts the tokens in a greedy manner,
    the information loss in subsequent decoding phase may lead to model degradation.
    Another way is to fetch relevant tokens on demand during the decoding phase. SPARQ (Ribar
    et al., [2023](#bib.bib47)) and InfLLM (Xiao et al., [2024](#bib.bib58)) offload
    KVCache to CPU, and selectively fetch relevant key-value pairs for each attention
    computation. PQCache also falls under this category of methods, demonstrating
    effective and efficient LLM inference in comparison to existing techniques.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除内存密集型KVCache的影响，一组方法在LLM推理期间只包括必要的令牌进行注意力计算。一个方法是丢弃不必要的令牌。LM-Infinite（Han等，[2023](#bib.bib21)）和Streaming-LLM（Xiao等，[2023](#bib.bib59)）只保留初始令牌和最新令牌。H2O（Zhang等，[2023a](#bib.bib64)）和Scissorhands（Liu等，[2023a](#bib.bib34)）利用注意力分数来识别重要的令牌。他们的后续工作（Ge等，[2023](#bib.bib18)；Adnan等，[2024](#bib.bib2)；Wang和Gan，[2024](#bib.bib56)；Ren和Zhu，[2024](#bib.bib45)）探索了自适应令牌选择和额外的度量标准以提高模型准确性。LLMLingua系列（Jiang等，[2023b](#bib.bib27)；Pan等，[2024](#bib.bib42)）利用辅助的小模型来判断哪些令牌是必要的。由于令牌级别的压缩以贪婪的方式驱逐令牌，后续解码阶段的信息丢失可能导致模型性能下降。另一种方法是在解码阶段按需提取相关令牌。SPARQ（Ribar等，[2023](#bib.bib47)）和InfLLM（Xiao等，[2024](#bib.bib58)）将KVCache卸载到CPU，并为每次注意力计算有选择地提取相关的键值对。PQCache也属于这一类方法，展示了与现有技术相比的有效且高效的LLM推理。
- en: KVCache Quantization
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: KVCache量化
- en: Quantization can be directly applied on the entire KVCache (Liu et al., [2024b](#bib.bib35);
    Dong et al., [2024a](#bib.bib12); Hooper et al., [2024](#bib.bib22)) - a straight-forward
    approach with promising model quality. Other compression techniques can also be
    employed to address the residuals introduced by quantization (Kang et al., [2024](#bib.bib30)).
    It is worth noting that quantization is orthogonal to token importance, and recent
    research has explored applying both techniques (Yang et al., [2024](#bib.bib60)).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 量化可以直接应用于整个KVCache（Liu等，[2024b](#bib.bib35)；Dong等，[2024a](#bib.bib12)；Hooper等，[2024](#bib.bib22)）-
    这是一个直接的方法，具有有前景的模型质量。其他压缩技术也可以用于解决量化引入的残差（Kang等，[2024](#bib.bib30)）。值得注意的是，量化与令牌重要性是正交的，最近的研究探索了应用这两种技术（Yang等，[2024](#bib.bib60)）。
- en: KVCache Scheduling
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: KVCache调度
- en: Another way to address the KVCache memory challenge is to meticulously schedule
    the KVCache within memory hierarchy. FlexGen (Sheng et al., [2023](#bib.bib48))
    employs linear programming to schedule the communication, searching for efficient
    patterns to store and access tensors. AttentionScore (Gao et al., [2024](#bib.bib17))
    maintains a hierarchical KV caching system, allowing efficient reuse of KVCache
    across multi-turn conversations. Another related research topic is KVCache streaming
    for LLM serving (Liu et al., [2023b](#bib.bib33); Strati et al., [2024](#bib.bib50)),
    which involves handling multiple requests within more levels of memory hierarchy.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 解决KVCache内存挑战的另一种方法是精心安排内存层次结构中的KVCache。FlexGen（Sheng等，[2023](#bib.bib48)）采用线性规划来安排通信，寻找高效的模式来存储和访问张量。AttentionScore（Gao等，[2024](#bib.bib17)）维护一个层次化的KV缓存系统，允许在多轮对话中高效重用KVCache。另一个相关的研究主题是LLM服务的KVCache流（Liu等，[2023b](#bib.bib33)；Strati等，[2024](#bib.bib50)），涉及在更多层次的内存层次结构中处理多个请求。
- en: Embedding Management
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 嵌入管理
- en: Embedding management is a common research focus within the database and data
    management domains, including embedding compression (Zhang et al., [2023c](#bib.bib63),
    [2024](#bib.bib61); Shi et al., [2020](#bib.bib49)), embedding retrieval (Wang
    et al., [2021](#bib.bib55); Huang et al., [2020](#bib.bib23)), and key-value storage (Chen
    et al., [2021b](#bib.bib9); Ren et al., [2017](#bib.bib44)). Our work provides
    a potential direction for integrating classic embedding management into the LLM
    ecology.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入管理是数据库和数据管理领域的一个常见研究重点，包括嵌入压缩（张等，[2023c](#bib.bib63)，[2024](#bib.bib61)；石等，[2020](#bib.bib49)）、嵌入检索（王等，[2021](#bib.bib55)；黄等，[2020](#bib.bib23)）和键值存储（陈等，[2021b](#bib.bib9)；任等，[2017](#bib.bib44)）。我们的工作为将经典的嵌入管理集成到LLM生态系统中提供了一个潜在方向。
- en: 6\. Conclusion
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: In this paper, we proposed PQCache, a system-algorithm co-designed method for
    effective and efficient long context LLM inference. We incorporated the embedding
    retrieval technique PQ to reduce both memory and computation burden, and leveraged
    PQ codes and centroids to facilitate efficient MIPS for important tokens used
    in the attention module. Through meticulous overlapping and caching, we managed
    to minimize overhead to a negligible level. We evaluated PQCache on extensive
    experiments, and show that PQCache effectively maintains model quality with only
    1/5 of the tokens involved in attention, while achieving acceptable system latency.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了PQCache，这是一种系统-算法协同设计的方法，用于有效和高效的长上下文LLM推理。我们结合了嵌入检索技术PQ，以减少内存和计算负担，并利用PQ代码和质心来促进对注意力模块中重要标记的高效MIPS。通过细致的重叠和缓存，我们将开销降至可以忽略的水平。我们在大量实验中评估了PQCache，并展示了PQCache在只涉及注意力的`1/5`标记的情况下有效保持模型质量，同时实现了可接受的系统延迟。
- en: References
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Adnan et al. (2024) Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J.
    Nair, Ilya Soloveychik, and Purushotham Kamath. 2024. Keyformer: KV Cache Reduction
    through Key Tokens Selection for Efficient Generative Inference. *CoRR* abs/2403.09054
    (2024). [https://doi.org/10.48550/ARXIV.2403.09054](https://doi.org/10.48550/ARXIV.2403.09054)
    arXiv:2403.09054'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Adnan等（2024）穆罕默德·阿德南、阿基尔·阿伦库马尔、考拉夫·贾恩、普拉尚特·J·奈尔、伊利亚·索洛维奇克和普鲁肖瑟姆·卡马斯。2024年。《Keyformer:
    KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference》。*CoRR*
    abs/2403.09054（2024）。[https://doi.org/10.48550/ARXIV.2403.09054](https://doi.org/10.48550/ARXIV.2403.09054)
    arXiv:2403.09054'
- en: AI (2024) Moonshot AI. 2024. KimiChat. [https://kimi.moonshot.cn/](https://kimi.moonshot.cn/)
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI（2024）Moonshot AI。2024年。《KimiChat》。[https://kimi.moonshot.cn/](https://kimi.moonshot.cn/)
- en: 'Ainslie et al. (2023) Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury
    Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. GQA: Training Generalized
    Multi-Query Transformer Models from Multi-Head Checkpoints. In *Proceedings of
    the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2023, Singapore, December 6-10, 2023*, Houda Bouamor, Juan Pino, and Kalika Bali
    (Eds.). Association for Computational Linguistics, 4895–4901. [https://doi.org/10.18653/V1/2023.EMNLP-MAIN.298](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.298)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ainslie等（2023）乔舒亚·安斯利、詹姆斯·李-索普、米歇尔·德·容、尤里·泽姆良斯基、费德里科·莱布隆和苏密特·桑海。2023年。《GQA:
    Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》。在*2023年自然语言处理实证方法会议论文集，EMNLP
    2023，新加坡，2023年12月6-10日*，侯达·布阿莫尔、胡安·皮诺和卡利卡·巴利（编）。计算语言学协会，4895–4901。[https://doi.org/10.18653/V1/2023.EMNLP-MAIN.298](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.298)'
- en: Anthropic (2023) Anthropic. 2023. Long context prompting for Claude 2.1. [https://www.anthropic.com/news/claude-2-1-prompting](https://www.anthropic.com/news/claude-2-1-prompting)
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic（2023）Anthropic。2023年。《Claude 2.1的长上下文提示》。[https://www.anthropic.com/news/claude-2-1-prompting](https://www.anthropic.com/news/claude-2-1-prompting)
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
    and Juanzi Li. 2023. LongBench: A Bilingual, Multitask Benchmark for Long Context
    Understanding. *CoRR* abs/2308.14508 (2023). [https://doi.org/10.48550/ARXIV.2308.14508](https://doi.org/10.48550/ARXIV.2308.14508)
    arXiv:2308.14508'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai等（2023）芦煜诗、吕鑫、张家杰、吕红畅、唐建凯、黄之典、杜正霄、刘晓、曾熙涵、侯磊、董宇霄、唐杰和李娟子。2023年。《LongBench:
    A Bilingual, Multitask Benchmark for Long Context Understanding》。*CoRR* abs/2308.14508（2023）。[https://doi.org/10.48550/ARXIV.2308.14508](https://doi.org/10.48550/ARXIV.2308.14508)
    arXiv:2308.14508'
- en: Baranchuk et al. (2018) Dmitry Baranchuk, Artem Babenko, and Yury Malkov. 2018.
    Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors.
    In *Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September
    8-14, 2018, Proceedings, Part XII* *(Lecture Notes in Computer Science)*, Vittorio
    Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (Eds.), Vol. 11216\.
    Springer, 209–224. [https://doi.org/10.1007/978-3-030-01258-8_13](https://doi.org/10.1007/978-3-030-01258-8_13)
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baranchuk 等（2018）Dmitry Baranchuk、Artem Babenko 和 Yury Malkov。2018。重新审视亿规模近似最近邻的倒排索引。在*计算机视觉
    - ECCV 2018 - 第15届欧洲会议，德国慕尼黑，2018年9月8日-14日，会议论文集，第十二部分* *(计算机科学讲义系列)*，Vittorio
    Ferrari、Martial Hebert、Cristian Sminchisescu 和 Yair Weiss（编），第11216卷。Springer，209–224。[https://doi.org/10.1007/978-3-030-01258-8_13](https://doi.org/10.1007/978-3-030-01258-8_13)
- en: 'Chen et al. (2021a) Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li, Chuanjie
    Liu, Zengzhong Li, Mao Yang, and Jingdong Wang. 2021a. SPANN: Highly-efficient
    Billion-scale Approximate Nearest Neighborhood Search. In *Advances in Neural
    Information Processing Systems 34: Annual Conference on Neural Information Processing
    Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual*, Marc’Aurelio Ranzato,
    Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan
    (Eds.). 5199–5212. [https://proceedings.neurips.cc/paper/2021/hash/299dc35e747eb77177d9cea10a802da2-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/299dc35e747eb77177d9cea10a802da2-Abstract.html)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2021a）Qi Chen、Bing Zhao、Haidong Wang、Mingqin Li、Chuanjie Liu、Zengzhong
    Li、Mao Yang 和 Jingdong Wang。2021a。SPANN: 高效的亿规模近似最近邻搜索。在*神经信息处理系统进展 34：2021年神经信息处理系统年会，NeurIPS
    2021，2021年12月6日-14日，虚拟会议*，Marc’Aurelio Ranzato、Alina Beygelzimer、Yann N. Dauphin、Percy
    Liang 和 Jennifer Wortman Vaughan（编）。5199–5212。[https://proceedings.neurips.cc/paper/2021/hash/299dc35e747eb77177d9cea10a802da2-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/299dc35e747eb77177d9cea10a802da2-Abstract.html)'
- en: 'Chen et al. (2021b) Xubin Chen, Ning Zheng, Shukun Xu, Yifan Qiao, Yang Liu,
    Jiangpeng Li, and Tong Zhang. 2021b. KallaxDB: A Table-less Hash-based Key-Value
    Store on Storage Hardware with Built-in Transparent Compression. In *Proceedings
    of the 17th International Workshop on Data Management on New Hardware, DaMoN 2021,
    21 June 2021, Virtual Event, China*, Danica Porobic and Spyros Blanas (Eds.).
    ACM, 3:1–3:10. [https://doi.org/10.1145/3465998.3466004](https://doi.org/10.1145/3465998.3466004)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2021b）Xubin Chen、Ning Zheng、Shukun Xu、Yifan Qiao、Yang Liu、Jiangpeng
    Li 和 Tong Zhang。2021b。KallaxDB: 一种基于哈希的无表键值存储，具有内置透明压缩。 在*第17届新硬件数据管理国际研讨会，DaMoN
    2021，2021年6月21日，虚拟会议，中国*，Danica Porobic 和 Spyros Blanas（编）。ACM，3:1–3:10。[https://doi.org/10.1145/3465998.3466004](https://doi.org/10.1145/3465998.3466004)'
- en: Cloud (2024) Alibaba Cloud. 2024. Tongyi Qianwen. [https://tongyi.aliyun.com/qianwen/](https://tongyi.aliyun.com/qianwen/)
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud（2024）阿里巴巴云。2024。通义千问。[https://tongyi.aliyun.com/qianwen/](https://tongyi.aliyun.com/qianwen/)
- en: 'Dong et al. (2024b) Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie
    Chi, and Beidi Chen. 2024b. Get More with LESS: Synthesizing Recurrence with KV
    Cache Compression for Efficient LLM Inference. *CoRR* abs/2402.09398 (2024). [https://doi.org/10.48550/ARXIV.2402.09398](https://doi.org/10.48550/ARXIV.2402.09398)
    arXiv:2402.09398'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2024b）Harry Dong、Xinyu Yang、Zhenyu Zhang、Zhangyang Wang、Yuejie Chi 和
    Beidi Chen。2024b。通过 LESS 获得更多：合成递归与 KV 缓存压缩以提高 LLM 推理效率。*CoRR* abs/2402.09398（2024）。[https://doi.org/10.48550/ARXIV.2402.09398](https://doi.org/10.48550/ARXIV.2402.09398)
    arXiv:2402.09398
- en: 'Dong et al. (2024a) Shichen Dong, Wen Cheng, Jiayu Qin, and Wei Wang. 2024a.
    QAQ: Quality Adaptive Quantization for LLM KV Cache. *CoRR* abs/2403.04643 (2024).
    [https://doi.org/10.48550/ARXIV.2403.04643](https://doi.org/10.48550/ARXIV.2403.04643)
    arXiv:2403.04643'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong 等（2024a）Shichen Dong、Wen Cheng、Jiayu Qin 和 Wei Wang。2024a。QAQ: 面向 LLM
    KV 缓存的质量自适应量化。*CoRR* abs/2403.04643（2024）。[https://doi.org/10.48550/ARXIV.2403.04643](https://doi.org/10.48550/ARXIV.2403.04643)
    arXiv:2403.04643'
- en: 'Fabbri et al. (2019) Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and
    Dragomir R. Radev. 2019. Multi-News: A Large-Scale Multi-Document Summarization
    Dataset and Abstractive Hierarchical Model. In *Proceedings of the 57th Conference
    of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July
    28- August 2, 2019, Volume 1: Long Papers*, Anna Korhonen, David R. Traum, and
    Lluís Màrquez (Eds.). Association for Computational Linguistics, 1074–1084. [https://doi.org/10.18653/V1/P19-1102](https://doi.org/10.18653/V1/P19-1102)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fabbri 等（2019）Alexander R. Fabbri、Irene Li、Tianwei She、Suyi Li 和 Dragomir R.
    Radev。2019。Multi-News: 大规模多文档摘要数据集及其抽象层次模型。在*第57届计算语言学协会会议论文集，ACL 2019，意大利佛罗伦萨，2019年7月28日-8月2日，第1卷：长篇论文*，Anna
    Korhonen、David R. Traum 和 Lluís Màrquez（编）。计算语言学协会，1074–1084。[https://doi.org/10.18653/V1/P19-1102](https://doi.org/10.18653/V1/P19-1102)'
- en: Fu et al. (2019) Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. 2019. Fast
    Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph. *Proc.
    VLDB Endow.* 12, 5 (2019), 461–474. [https://doi.org/10.14778/3303753.3303754](https://doi.org/10.14778/3303753.3303754)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 (2019) **Cong Fu**, **Chao Xiang**, **Changxu Wang** 和 **Deng Cai**。2019年。《使用导航扩展图进行快速近似最近邻搜索》。*Proc.
    VLDB Endow.* 12, 5 (2019), 461–474。[https://doi.org/10.14778/3303753.3303754](https://doi.org/10.14778/3303753.3303754)
- en: 'Fu et al. (2023) Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar
    Khot. 2023. Chain-of-Thought Hub: A Continuous Effort to Measure Large Language
    Models’ Reasoning Performance. *CoRR* abs/2305.17306 (2023). [https://doi.org/10.48550/ARXIV.2305.17306](https://doi.org/10.48550/ARXIV.2305.17306)
    arXiv:2305.17306'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 (2023) **Yao Fu**, **Litu Ou**, **Mingyu Chen**, **Yuhao Wan**, **Hao Peng**
    和 **Tushar Khot**。2023年。《Chain-of-Thought Hub：持续测量大型语言模型推理性能的努力》。*CoRR* abs/2305.17306
    (2023)。[https://doi.org/10.48550/ARXIV.2305.17306](https://doi.org/10.48550/ARXIV.2305.17306)
    arXiv:2305.17306
- en: Fu et al. (2024) Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
    Yoon Kim, and Hao Peng. 2024. Data Engineering for Scaling Language Models to
    128K Context. *CoRR* abs/2402.10171 (2024). [https://doi.org/10.48550/ARXIV.2402.10171](https://doi.org/10.48550/ARXIV.2402.10171)
    arXiv:2402.10171
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 (2024) **Yao Fu**, **Rameswar Panda**, **Xinyao Niu**, **Xiang Yue**, **Hannaneh
    Hajishirzi**, **Yoon Kim** 和 **Hao Peng**。2024年。《用于将语言模型扩展到128K上下文的数据工程》。*CoRR*
    abs/2402.10171 (2024)。[https://doi.org/10.48550/ARXIV.2402.10171](https://doi.org/10.48550/ARXIV.2402.10171)
    arXiv:2402.10171
- en: 'Gao et al. (2024) Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje
    Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, and Pengfei Zuo. 2024. AttentionStore:
    Cost-effective Attention Reuse across Multi-turn Conversations in Large Language
    Model Serving. *CoRR* abs/2403.19708 (2024). [https://doi.org/10.48550/ARXIV.2403.19708](https://doi.org/10.48550/ARXIV.2403.19708)
    arXiv:2403.19708'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等 (2024) **Bin Gao**, **Zhuomin He**, **Puru Sharma**, **Qingxuan Kang**,
    **Djordje Jevdjic**, **Junbo Deng**, **Xingkun Yang**, **Zhou Yu** 和 **Pengfei
    Zuo**。2024年。《AttentionStore：在大型语言模型服务中的多轮对话中成本效益高的注意力重用》。*CoRR* abs/2403.19708
    (2024)。[https://doi.org/10.48550/ARXIV.2403.19708](https://doi.org/10.48550/ARXIV.2403.19708)
    arXiv:2403.19708
- en: 'Ge et al. (2023) Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han,
    and Jianfeng Gao. 2023. Model Tells You What to Discard: Adaptive KV Cache Compression
    for LLMs. *CoRR* abs/2310.01801 (2023). [https://doi.org/10.48550/ARXIV.2310.01801](https://doi.org/10.48550/ARXIV.2310.01801)
    arXiv:2310.01801'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge 等 (2023) **Suyu Ge**, **Yunan Zhang**, **Liyuan Liu**, **Minjia Zhang**,
    **Jiawei Han** 和 **Jianfeng Gao**。2023年。《模型告诉你要丢弃什么：用于LLMs的自适应KV缓存压缩》。*CoRR* abs/2310.01801
    (2023)。[https://doi.org/10.48550/ARXIV.2310.01801](https://doi.org/10.48550/ARXIV.2310.01801)
    arXiv:2310.01801
- en: Ge et al. (2013) Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2013. Optimized
    Product Quantization for Approximate Nearest Neighbor Search. In *2013 IEEE Conference
    on Computer Vision and Pattern Recognition, Portland, OR, USA, June 23-28, 2013*.
    IEEE Computer Society, 2946–2953. [https://doi.org/10.1109/CVPR.2013.379](https://doi.org/10.1109/CVPR.2013.379)
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge 等 (2013) **Tiezheng Ge**, **Kaiming He**, **Qifa Ke** 和 **Jian Sun**。2013年。《优化产品量化以进行近似最近邻搜索》。发表于
    *2013年IEEE计算机视觉与模式识别大会，波特兰，OR，美国，2013年6月23-28日*。IEEE计算机学会，2946–2953。[https://doi.org/10.1109/CVPR.2013.379](https://doi.org/10.1109/CVPR.2013.379)
- en: Guo et al. (2020) Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha,
    Felix Chern, and Sanjiv Kumar. 2020. Accelerating Large-Scale Inference with Anisotropic
    Vector Quantization. In *Proceedings of the 37th International Conference on Machine
    Learning, ICML 2020, 13-18 July 2020, Virtual Event* *(Proceedings of Machine
    Learning Research)*, Vol. 119\. PMLR, 3887–3896. [http://proceedings.mlr.press/v119/guo20h.html](http://proceedings.mlr.press/v119/guo20h.html)
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2020) **Ruiqi Guo**, **Philip Sun**, **Erik Lindgren**, **Quan Geng**,
    **David Simcha**, **Felix Chern** 和 **Sanjiv Kumar**。2020年。《通过各向异性向量量化加速大规模推理》。发表于
    *第37届国际机器学习会议论文集，ICML 2020，2020年7月13-18日，虚拟会议* *(机器学习研究论文集)*，第119卷。PMLR, 3887–3896。[http://proceedings.mlr.press/v119/guo20h.html](http://proceedings.mlr.press/v119/guo20h.html)
- en: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. 2023. LM-Infinite: Simple On-the-Fly Length Generalization for Large
    Language Models. *CoRR* abs/2308.16137 (2023). [https://doi.org/10.48550/ARXIV.2308.16137](https://doi.org/10.48550/ARXIV.2308.16137)
    arXiv:2308.16137'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2023) **Chi Han**, **Qifan Wang**, **Wenhan Xiong**, **Yu Chen**, **Heng
    Ji** 和 **Sinong Wang**。2023年。《LM-Infinite：大型语言模型的简单即时长度泛化》。*CoRR* abs/2308.16137
    (2023)。[https://doi.org/10.48550/ARXIV.2308.16137](https://doi.org/10.48550/ARXIV.2308.16137)
    arXiv:2308.16137
- en: 'Hooper et al. (2024) Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W.
    Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. 2024. KVQuant: Towards
    10 Million Context Length LLM Inference with KV Cache Quantization. *CoRR* abs/2401.18079
    (2024). [https://doi.org/10.48550/ARXIV.2401.18079](https://doi.org/10.48550/ARXIV.2401.18079)
    arXiv:2401.18079'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hooper et al. (2024) 科尔曼·胡普、金世勋、希瓦·穆罕默德扎赫、迈克尔·W·马赫尼、雅坤·索非亚·肖、库尔特·凯茨和阿米尔·戈拉米。2024年。KVQuant:
    通过KV缓存量化实现1000万上下文长度的LLM推理。*CoRR* abs/2401.18079（2024）。[https://doi.org/10.48550/ARXIV.2401.18079](https://doi.org/10.48550/ARXIV.2401.18079)
    arXiv:2401.18079'
- en: Huang et al. (2020) Ruihong Huang, Shaoxu Song, Yunsu Lee, Jungho Park, Soo-Hyung
    Kim, and Sungmin Yi. 2020. Effective and Efficient Retrieval of Structured Entities.
    *Proc. VLDB Endow.* 13, 6 (2020), 826–839. [https://doi.org/10.14778/3380750.3380754](https://doi.org/10.14778/3380750.3380754)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2020) 黄瑞红、宋绍徐、李云苏、朴正浩、金秀亨和李胜民。2020年。高效有效的结构化实体检索。*Proc. VLDB Endow.*
    13, 6（2020），826–839。 [https://doi.org/10.14778/3380750.3380754](https://doi.org/10.14778/3380750.3380754)
- en: 'Jayaram Subramanya et al. (2019) Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan
    Simhadri, Ravishankar Krishnawamy, and Rohan Kadekodi. 2019. Diskann: Fast accurate
    billion-point nearest neighbor search on a single node. *Advances in Neural Information
    Processing Systems* 32 (2019).'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jayaram Subramanya et al. (2019) 苏哈斯·贾亚拉姆·苏布拉马尼亚、弗努·德夫里特、哈希·瓦尔丹·辛哈德里、拉维尚卡·克里希纳瓦米和罗汉·卡德科迪。2019年。Diskann:
    在单个节点上快速准确的十亿点最近邻搜索。*Advances in Neural Information Processing Systems* 32（2019）。'
- en: Jégou et al. (2011) Hervé Jégou, Matthijs Douze, and Cordelia Schmid. 2011.
    Product Quantization for Nearest Neighbor Search. *IEEE Trans. Pattern Anal. Mach.
    Intell.* 33, 1 (2011), 117–128. [https://doi.org/10.1109/TPAMI.2010.57](https://doi.org/10.1109/TPAMI.2010.57)
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jégou et al. (2011) 赫尔维·杰古、马蒂耶斯·杜泽和科尔德利亚·施密德。2011年。最近邻搜索的产品量化。*IEEE Trans. Pattern
    Anal. Mach. Intell.* 33, 1（2011），117–128。 [https://doi.org/10.1109/TPAMI.2010.57](https://doi.org/10.1109/TPAMI.2010.57)
- en: Jiang et al. (2023a) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023a. Mistral 7B. *CoRR* abs/2310.06825 (2023). [https://doi.org/10.48550/ARXIV.2310.06825](https://doi.org/10.48550/ARXIV.2310.06825)
    arXiv:2310.06825
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023a) 阿尔伯特·Q·姜、亚历山大·萨布莱罗莱斯、亚瑟·门施、克里斯·班福德、德文德拉·辛格·查普洛特、迭戈·德·拉斯·卡萨斯、弗洛里安·布雷桑、吉安娜·伦杰尔、吉约姆·兰普勒、吕西尔·索尔尼耶、勒里奥·雷纳尔·拉沃、玛丽-安·拉肖、皮埃尔·斯托克、特文·勒·斯卡奥、蒂博·拉夫里尔、托马斯·王、蒂莫西·拉克鲁瓦和威廉·埃尔·萨耶德。2023年。Mistral
    7B。*CoRR* abs/2310.06825（2023）。[https://doi.org/10.48550/ARXIV.2310.06825](https://doi.org/10.48550/ARXIV.2310.06825)
    arXiv:2310.06825
- en: 'Jiang et al. (2023b) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,
    and Lili Qiu. 2023b. LLMLingua: Compressing Prompts for Accelerated Inference
    of Large Language Models. In *Proceedings of the 2023 Conference on Empirical
    Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10,
    2023*, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational
    Linguistics, 13358–13376. [https://doi.org/10.18653/V1/2023.EMNLP-MAIN.825](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.825)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2023b) 姜辉强、吴倩慧、林志远、杨煜庆和邱莉莉。2023年。LLMLingua: 压缩提示以加速大语言模型的推理。在*2023年自然语言处理实证方法会议论文集，EMNLP
    2023，新加坡，2023年12月6-10日*中，侯达·布阿莫尔、胡安·皮诺和卡莉卡·巴利（编辑）。计算语言学协会，13358–13376。 [https://doi.org/10.18653/V1/2023.EMNLP-MAIN.825](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.825)'
- en: Johnson et al. (2021) Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021. Billion-Scale
    Similarity Search with GPUs. *IEEE Trans. Big Data* 7, 3 (2021), 535–547. [https://doi.org/10.1109/TBDATA.2019.2921572](https://doi.org/10.1109/TBDATA.2019.2921572)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson et al. (2021) 杰夫·约翰逊、马蒂耶斯·杜泽和赫尔维·杰古。2021年。亿级相似性搜索与GPU。*IEEE Trans. Big
    Data* 7, 3（2021），535–547。 [https://doi.org/10.1109/TBDATA.2019.2921572](https://doi.org/10.1109/TBDATA.2019.2921572)
- en: Kamradt (2024) Greg Kamradt. 2024. Needle-in-a-Haystack. [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamradt (2024) 格雷格·卡姆拉特。2024年。 Needle-in-a-Haystack。 [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
- en: 'Kang et al. (2024) Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing
    Liu, Tushar Krishna, and Tuo Zhao. 2024. Gear: An efficient kv cache compression
    recipefor near-lossless generative inference of llm. *arXiv preprint arXiv:2403.05527*
    (2024).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kang et al. (2024) 侯康、张清如、苏维克·昆杜、郑建华、刘灶兴、图沙尔·克里希纳和赵拓。2024年。Gear: 一种高效的kv缓存压缩配方，用于近乎无损的llm生成推理。*arXiv预印本
    arXiv:2403.05527*（2024）。'
- en: 'Lingle (2023) Lucas D. Lingle. 2023. Transformer-VQ: Linear-Time Transformers
    via Vector Quantization. *CoRR* abs/2309.16354 (2023). [https://doi.org/10.48550/ARXIV.2309.16354](https://doi.org/10.48550/ARXIV.2309.16354)
    arXiv:2309.16354'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lingle (2023) **Lucas D. Lingle**. 2023. 《Transformer-VQ：通过向量量化实现线性时间变换器》。*CoRR*
    abs/2309.16354 (2023). [https://doi.org/10.48550/ARXIV.2309.16354](https://doi.org/10.48550/ARXIV.2309.16354)
    arXiv:2309.16354
- en: Liu et al. (2024a) Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. 2024a.
    World Model on Million-Length Video And Language With Blockwise RingAttention.
    *CoRR* abs/2402.08268 (2024). [https://doi.org/10.48550/ARXIV.2402.08268](https://doi.org/10.48550/ARXIV.2402.08268)
    arXiv:2402.08268
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024a) **Hao Liu**, **Wilson Yan**, **Matei Zaharia**, 和 **Pieter
    Abbeel**. 2024a. 《Million-Length Video And Language With Blockwise RingAttention上的世界模型》。*CoRR*
    abs/2402.08268 (2024). [https://doi.org/10.48550/ARXIV.2402.08268](https://doi.org/10.48550/ARXIV.2402.08268)
    arXiv:2402.08268
- en: 'Liu et al. (2023b) Yuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng,
    Yuyang Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, et al. 2023b.
    CacheGen: Fast Context Loading for Language Model Applications. *arXiv preprint
    arXiv:2310.07240* (2023).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023b) **Yuhan Liu**, **Hanchen Li**, **Kuntai Du**, **Jiayi Yao**,
    **Yihua Cheng**, **Yuyang Huang**, **Shan Lu**, **Michael Maire**, **Henry Hoffmann**,
    **Ari Holtzman**, 等人. 2023b. 《CacheGen：语言模型应用的快速上下文加载》。*arXiv preprint arXiv:2310.07240*
    (2023).
- en: 'Liu et al. (2023a) Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor
    Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. 2023a. Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time. In *Advances in Neural Information Processing Systems 36: Annual
    Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
    LA, USA, December 10 - 16, 2023*, Alice Oh, Tristan Naumann, Amir Globerson, Kate
    Saenko, Moritz Hardt, and Sergey Levine (Eds.). [http://papers.nips.cc/paper_files/paper/2023/hash/a452a7c6c463e4ae8fbdc614c6e983e6-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/a452a7c6c463e4ae8fbdc614c6e983e6-Abstract-Conference.html)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023a) **Zichang Liu**, **Aditya Desai**, **Fangshuo Liao**, **Weitao
    Wang**, **Victor Xie**, **Zhaozhuo Xu**, **Anastasios Kyrillidis**, 和 **Anshumali
    Shrivastava**. 2023a. 《Scissorhands：利用重要性假设的持久性进行LLM KV缓存压缩》。在 *Advances in Neural
    Information Processing Systems 36: Annual Conference on Neural Information Processing
    Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023*, **Alice
    Oh**, **Tristan Naumann**, **Amir Globerson**, **Kate Saenko**, **Moritz Hardt**,
    和 **Sergey Levine** (编). [http://papers.nips.cc/paper_files/paper/2023/hash/a452a7c6c463e4ae8fbdc614c6e983e6-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/a452a7c6c463e4ae8fbdc614c6e983e6-Abstract-Conference.html)'
- en: 'Liu et al. (2024b) Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo
    Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. 2024b. KIVI: A Tuning-Free Asymmetric
    2bit Quantization for KV Cache. *CoRR* abs/2402.02750 (2024). [https://doi.org/10.48550/ARXIV.2402.02750](https://doi.org/10.48550/ARXIV.2402.02750)
    arXiv:2402.02750'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024b) **Zirui Liu**, **Jiayi Yuan**, **Hongye Jin**, **Shaochen
    Zhong**, **Zhaozhuo Xu**, **Vladimir Braverman**, **Beidi Chen**, 和 **Xia Hu**.
    2024b. 《KIVI：一种无调优的非对称2位量化方法用于KV缓存》。*CoRR* abs/2402.02750 (2024). [https://doi.org/10.48550/ARXIV.2402.02750](https://doi.org/10.48550/ARXIV.2402.02750)
    arXiv:2402.02750
- en: Malkov and Yashunin (2020) Yury A. Malkov and Dmitry A. Yashunin. 2020. Efficient
    and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small
    World Graphs. *IEEE Trans. Pattern Anal. Mach. Intell.* 42, 4 (2020), 824–836.
    [https://doi.org/10.1109/TPAMI.2018.2889473](https://doi.org/10.1109/TPAMI.2018.2889473)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malkov and Yashunin (2020) **Yury A. Malkov** 和 **Dmitry A. Yashunin**. 2020.
    《使用层次可导航小世界图的高效且鲁棒的近似最近邻搜索》。*IEEE Trans. Pattern Anal. Mach. Intell.* 42, 4 (2020),
    824–836. [https://doi.org/10.1109/TPAMI.2018.2889473](https://doi.org/10.1109/TPAMI.2018.2889473)
- en: Martinez et al. (2014) Julieta Martinez, Holger H. Hoos, and James J. Little.
    2014. Stacked Quantizers for Compositional Vector Compression. *CoRR* abs/1411.2173
    (2014). arXiv:1411.2173 [http://arxiv.org/abs/1411.2173](http://arxiv.org/abs/1411.2173)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martinez et al. (2014) **Julieta Martinez**, **Holger H. Hoos**, 和 **James J.
    Little**. 2014. 《用于组合向量压缩的堆叠量化器》。*CoRR* abs/1411.2173 (2014). arXiv:1411.2173
    [http://arxiv.org/abs/1411.2173](http://arxiv.org/abs/1411.2173)
- en: 'Miao et al. (2023) Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng,
    Hongyi Jin, Tianqi Chen, and Zhihao Jia. 2023. Towards efficient generative large
    language model serving: A survey from algorithms to systems. *arXiv preprint arXiv:2312.15234*
    (2023).'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miao et al. (2023) **Xupeng Miao**, **Gabriele Oliaro**, **Zhihao Zhang**, **Xinhao
    Cheng**, **Hongyi Jin**, **Tianqi Chen**, 和 **Zhihao Jia**. 2023. 《面向高效生成大语言模型服务的调查：从算法到系统》。*arXiv
    preprint arXiv:2312.15234* (2023).
- en: Narayan et al. (2018) Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018.
    Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural
    Networks for Extreme Summarization. In *Proceedings of the 2018 Conference on
    Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31
    - November 4, 2018*, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi
    Tsujii (Eds.). Association for Computational Linguistics, 1797–1807. [https://doi.org/10.18653/V1/D18-1206](https://doi.org/10.18653/V1/D18-1206)
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayan等（2018）Shashi Narayan、Shay B. Cohen和Mirella Lapata。2018年。别给我细节，只要总结！主题感知卷积神经网络用于极端摘要。在*2018年自然语言处理经验方法会议论文集，比利时布鲁塞尔，2018年10月31日-11月4日*，Ellen
    Riloff、David Chiang、Julia Hockenmaier和Jun’ichi Tsujii（编）。计算语言学协会，1797–1807。 [https://doi.org/10.18653/V1/D18-1206](https://doi.org/10.18653/V1/D18-1206)
- en: 'Nie et al. (2022) Xiaonan Nie, Xupeng Miao, Zhi Yang, and Bin Cui. 2022. TSPLIT:
    Fine-grained GPU Memory Management for Efficient DNN Training via Tensor Splitting.
    In *38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala Lumpur,
    Malaysia, May 9-12, 2022*. IEEE, 2615–2628. [https://doi.org/10.1109/ICDE53745.2022.00241](https://doi.org/10.1109/ICDE53745.2022.00241)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie等（2022）Xiaonan Nie、Xupeng Miao、Zhi Yang和Bin Cui。2022年。TSPLIT：通过张量拆分进行高效DNN训练的细粒度GPU内存管理。在*第38届IEEE数据工程国际会议，ICDE
    2022，马来西亚吉隆坡，2022年5月9-12日*。IEEE，2615–2628。 [https://doi.org/10.1109/ICDE53745.2022.00241](https://doi.org/10.1109/ICDE53745.2022.00241)
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. *CoRR* abs/2303.08774 (2023).
    [https://doi.org/10.48550/ARXIV.2303.08774](https://doi.org/10.48550/ARXIV.2303.08774)
    arXiv:2303.08774
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI。2023年。GPT-4技术报告。*CoRR* abs/2303.08774（2023）。 [https://doi.org/10.48550/ARXIV.2303.08774](https://doi.org/10.48550/ARXIV.2303.08774)
    arXiv:2303.08774
- en: 'Pan et al. (2024) Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang
    Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H. Vicky
    Zhao, Lili Qiu, and Dongmei Zhang. 2024. LLMLingua-2: Data Distillation for Efficient
    and Faithful Task-Agnostic Prompt Compression. *CoRR* abs/2403.12968 (2024). [https://doi.org/10.48550/ARXIV.2403.12968](https://doi.org/10.48550/ARXIV.2403.12968)
    arXiv:2403.12968'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan等（2024）Zhuoshi Pan、Qianhui Wu、Huiqiang Jiang、Menglin Xia、Xufang Luo、Jue Zhang、Qingwei
    Lin、Victor Rühle、Yuqing Yang、Chin-Yew Lin、H. Vicky Zhao、Lili Qiu和Dongmei Zhang。2024年。LLMLingua-2：用于高效且忠实任务无关提示压缩的数据蒸馏。*CoRR*
    abs/2403.12968（2024）。 [https://doi.org/10.48550/ARXIV.2403.12968](https://doi.org/10.48550/ARXIV.2403.12968)
    arXiv:2403.12968
- en: 'Ren et al. (2021) Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji
    Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. ZeRO-Offload:
    Democratizing Billion-Scale Model Training. In *2021 USENIX Annual Technical Conference,
    USENIX ATC 2021, July 14-16, 2021*, Irina Calciu and Geoff Kuenning (Eds.). USENIX
    Association, 551–564. [https://www.usenix.org/conference/atc21/presentation/ren-jie](https://www.usenix.org/conference/atc21/presentation/ren-jie)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren等（2021）Jie Ren、Samyam Rajbhandari、Reza Yazdani Aminabadi、Olatunji Ruwase、Shuangyan
    Yang、Minjia Zhang、Dong Li和Yuxiong He。2021年。ZeRO-Offload：民主化十亿规模模型训练。在*2021年USENIX年度技术会议，USENIX
    ATC 2021，2021年7月14-16日*，Irina Calciu和Geoff Kuenning（编）。USENIX协会，551–564。 [https://www.usenix.org/conference/atc21/presentation/ren-jie](https://www.usenix.org/conference/atc21/presentation/ren-jie)
- en: 'Ren et al. (2017) Kai Ren, Qing Zheng, Joy Arulraj, and Garth Gibson. 2017.
    SlimDB: A Space-Efficient Key-Value Storage Engine For Semi-Sorted Data. *Proc.
    VLDB Endow.* 10, 13 (2017), 2037–2048. [https://doi.org/10.14778/3151106.3151108](https://doi.org/10.14778/3151106.3151108)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren和Zhu（2017）Kai Ren、Qing Zheng、Joy Arulraj和Garth Gibson。2017年。SlimDB：一种空间高效的半排序数据键值存储引擎。*Proc.
    VLDB Endow.* 10，13（2017），2037–2048。 [https://doi.org/10.14778/3151106.3151108](https://doi.org/10.14778/3151106.3151108)
- en: Ren and Zhu (2024) Siyu Ren and Kenny Q. Zhu. 2024. On the Efficacy of Eviction
    Policy for Key-Value Constrained Generative Language Model Inference. *CoRR* abs/2402.06262
    (2024). [https://doi.org/10.48550/ARXIV.2402.06262](https://doi.org/10.48550/ARXIV.2402.06262)
    arXiv:2402.06262
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren和Zhu（2024）Siyu Ren和Kenny Q. Zhu。2024年。关于键值约束生成语言模型推理的驱逐策略有效性。*CoRR* abs/2402.06262（2024）。
    [https://doi.org/10.48550/ARXIV.2402.06262](https://doi.org/10.48550/ARXIV.2402.06262)
    arXiv:2402.06262
- en: 'Rhu et al. (2016) Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar,
    and Stephen W. Keckler. 2016. vDNN: Virtualized deep neural networks for scalable,
    memory-efficient neural network design. In *49th Annual IEEE/ACM International
    Symposium on Microarchitecture, MICRO 2016, Taipei, Taiwan, October 15-19, 2016*.
    IEEE Computer Society, 18:1–18:13. [https://doi.org/10.1109/MICRO.2016.7783721](https://doi.org/10.1109/MICRO.2016.7783721)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rhu et al. (2016) Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar,
    and Stephen W. Keckler. 2016. vDNN: Virtualized deep neural networks for scalable,
    memory-efficient neural network design. In *第49届IEEE/ACM国际微架构研讨会, MICRO 2016,
    台北, 台湾, 2016年10月15-19日*. IEEE计算机学会, 18:1–18:13. [https://doi.org/10.1109/MICRO.2016.7783721](https://doi.org/10.1109/MICRO.2016.7783721)'
- en: 'Ribar et al. (2023) Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie
    Blake, Carlo Luschi, and Douglas Orr. 2023. SparQ Attention: Bandwidth-Efficient
    LLM Inference. *CoRR* abs/2312.04985 (2023). [https://doi.org/10.48550/ARXIV.2312.04985](https://doi.org/10.48550/ARXIV.2312.04985)
    arXiv:2312.04985'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ribar et al. (2023) Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie
    Blake, Carlo Luschi, and Douglas Orr. 2023. SparQ Attention: Bandwidth-Efficient
    LLM Inference. *CoRR* abs/2312.04985 (2023). [https://doi.org/10.48550/ARXIV.2312.04985](https://doi.org/10.48550/ARXIV.2312.04985)
    arXiv:2312.04985'
- en: 'Sheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. 2023.
    FlexGen: High-Throughput Generative Inference of Large Language Models with a
    Single GPU. In *International Conference on Machine Learning, ICML 2023, 23-29
    July 2023, Honolulu, Hawaii, USA* *(Proceedings of Machine Learning Research)*,
    Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,
    and Jonathan Scarlett (Eds.), Vol. 202\. PMLR, 31094–31116. [https://proceedings.mlr.press/v202/sheng23a.html](https://proceedings.mlr.press/v202/sheng23a.html)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. 2023.
    FlexGen: High-Throughput Generative Inference of Large Language Models with a
    Single GPU. In *International Conference on Machine Learning, ICML 2023, 2023年7月23-29日,
    夏威夷檀香山, 美国* *(机器学习研究会议论文集)*, Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara
    Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.), Vol. 202. PMLR, 31094–31116.
    [https://proceedings.mlr.press/v202/sheng23a.html](https://proceedings.mlr.press/v202/sheng23a.html)'
- en: 'Shi et al. (2020) Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and
    Jiyan Yang. 2020. Compositional Embeddings Using Complementary Partitions for
    Memory-Efficient Recommendation Systems. In *KDD ’20: The 26th ACM SIGKDD Conference
    on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27,
    2020*, Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (Eds.). ACM,
    165–175. [https://doi.org/10.1145/3394486.3403059](https://doi.org/10.1145/3394486.3403059)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi et al. (2020) Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and
    Jiyan Yang. 2020. Compositional Embeddings Using Complementary Partitions for
    Memory-Efficient Recommendation Systems. In *KDD ’20: 第26届ACM SIGKDD知识发现与数据挖掘会议,
    虚拟会议, 加州, 美国, 2020年8月23-27日*, Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya
    Prakash (Eds.). ACM, 165–175. [https://doi.org/10.1145/3394486.3403059](https://doi.org/10.1145/3394486.3403059)'
- en: 'Strati et al. (2024) Foteini Strati, Sara Mcallister, Amar Phanishayee, Jakub
    Tarnawski, and Ana Klimovic. 2024. D$\backslash$aVu: KV-cache Streaming for Fast,
    Fault-tolerant Generative LLM Serving. *arXiv preprint arXiv:2403.01876* (2024).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Strati et al. (2024) Foteini Strati, Sara Mcallister, Amar Phanishayee, Jakub
    Tarnawski, and Ana Klimovic. 2024. D$\backslash$aVu: KV-cache Streaming for Fast,
    Fault-tolerant Generative LLM Serving. *arXiv 预印本 arXiv:2403.01876* (2024).'
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca:
    A strong, replicable instruction-following model. *Stanford Center for Research
    on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html* 3,
    6 (2023), 7.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca:
    A strong, replicable instruction-following model. *斯坦福基础模型研究中心. https://crfm.
    stanford. edu/2023/03/13/alpaca. html* 3, 6 (2023), 7.'
- en: Together.ai (2023) Together.ai. 2023. LLaMA-2-7B-32K. [https://huggingface.co/togethercomputer/LLaMA-2-7B-32K](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K)
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Together.ai (2023) Together.ai. 2023. LLaMA-2-7B-32K. [https://huggingface.co/togethercomputer/LLaMA-2-7B-32K](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K)
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama
    2: Open Foundation and Fine-Tuned Chat Models. *CoRR* abs/2307.09288 (2023). [https://doi.org/10.48550/ARXIV.2307.09288](https://doi.org/10.48550/ARXIV.2307.09288)
    arXiv:2307.09288'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 (2023) **Hugo Touvron**、**Louis Martin**、**Kevin Stone**、**Peter
    Albert**、**Amjad Almahairi**、**Yasmine Babaei**、**Nikolay Bashlykov**、**Soumya
    Batra**、**Prajjwal Bhargava**、**Shruti Bhosale**、**Dan Bikel**、**Lukas Blecher**、**Cristian
    Canton-Ferrer**、**Moya Chen**、**Guillem Cucurull**、**David Esiobu**、**Jude Fernandes**、**Jeremy
    Fu**、**Wenyin Fu**、**Brian Fuller**、**Cynthia Gao**、**Vedanuj Goswami**、**Naman
    Goyal**、**Anthony Hartshorn**、**Saghar Hosseini**、**Rui Hou**、**Hakan Inan**、**Marcin
    Kardas**、**Viktor Kerkez**、**Madian Khabsa**、**Isabel Kloumann**、**Artem Korenev**、**Punit
    Singh Koura**、**Marie-Anne Lachaux**、**Thibaut Lavril**、**Jenya Lee**、**Diana
    Liskovich**、**Yinghai Lu**、**Yuning Mao**、**Xavier Martinet**、**Todor Mihaylov**、**Pushkar
    Mishra**、**Igor Molybog**、**Yixin Nie**、**Andrew Poulton**、**Jeremy Reizenstein**、**Rashi
    Rungta**、**Kalyan Saladi**、**Alan Schelten**、**Ruan Silva**、**Eric Michael Smith**、**Ranjan
    Subramanian**、**Xiaoqing Ellen Tan**、**Binh Tang**、**Ross Taylor**、**Adina Williams**、**Jian
    Xiang Kuan**、**Puxin Xu**、**Zheng Yan**、**Iliyan Zarov**、**Yuchen Zhang**、**Angela
    Fan**、**Melanie Kambadur**、**Sharan Narang**、**Aurélien Rodriguez**、**Robert Stojnic**、**Sergey
    Edunov** 和 **Thomas Scialom**。2023。*Llama 2: Open Foundation and Fine-Tuned Chat
    Models*。*CoRR* abs/2307.09288 (2023)。[https://doi.org/10.48550/ARXIV.2307.09288](https://doi.org/10.48550/ARXIV.2307.09288)
    arXiv:2307.09288'
- en: 'van den Oord et al. (2017) Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu.
    2017. Neural Discrete Representation Learning. In *Advances in Neural Information
    Processing Systems 30: Annual Conference on Neural Information Processing Systems
    2017, December 4-9, 2017, Long Beach, CA, USA*, Isabelle Guyon, Ulrike von Luxburg,
    Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett
    (Eds.). 6306–6315. [https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'van den Oord 等人 (2017) **Aäron van den Oord**、**Oriol Vinyals** 和 **Koray Kavukcuoglu**。2017。*Neural
    Discrete Representation Learning*。收录于 *Advances in Neural Information Processing
    Systems 30: Annual Conference on Neural Information Processing Systems 2017, December
    4-9, 2017, Long Beach, CA, USA*，**Isabelle Guyon**、**Ulrike von Luxburg**、**Samy
    Bengio**、**Hanna M. Wallach**、**Rob Fergus**、**S. V. N. Vishwanathan** 和 **Roman
    Garnett** (编)。6306–6315。[https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html)'
- en: Wang et al. (2021) Mengzhao Wang, Xiaoliang Xu, Qiang Yue, and Yuxiang Wang.
    2021. A Comprehensive Survey and Experimental Comparison of Graph-Based Approximate
    Nearest Neighbor Search. *Proc. VLDB Endow.* 14, 11 (2021), 1964–1978. [https://doi.org/10.14778/3476249.3476255](https://doi.org/10.14778/3476249.3476255)
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2021) **Mengzhao Wang**、**Xiaoliang Xu**、**Qiang Yue** 和 **Yuxiang
    Wang**。2021。*A Comprehensive Survey and Experimental Comparison of Graph-Based
    Approximate Nearest Neighbor Search*。*Proc. VLDB Endow.* 14, 11 (2021), 1964–1978。[https://doi.org/10.14778/3476249.3476255](https://doi.org/10.14778/3476249.3476255)
- en: 'Wang and Gan (2024) Zihao Wang and Shaoduo Gan. 2024. SqueezeAttention: 2D
    Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget. *arXiv
    preprint arXiv:2404.04793* (2024).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 和 Gan (2024) **Zihao Wang** 和 **Shaoduo Gan**。2024。*SqueezeAttention:
    2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget*。*arXiv
    preprint arXiv:2404.04793* (2024)。'
- en: 'Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought
    Prompting Elicits Reasoning in Large Language Models. In *Advances in Neural Information
    Processing Systems 35: Annual Conference on Neural Information Processing Systems
    2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022*, Sanmi
    Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). [http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2022）杰森 Wei、学智 Wang、戴尔 Schuurmans、马尔滕 Bosma、布莱恩 Ichter、飞 Xia、艾德 H. Chi、阮奇
    V. Le 和丹尼 Zhou。2022。链式思维提示引发大语言模型中的推理。在*神经信息处理系统进展 35：2022年神经信息处理系统年会，NeurIPS
    2022，美国新奥尔良，2022年11月28日 - 12月9日*，Sanmi Koyejo、S. Mohamed、A. Agarwal、Danielle Belgrave、K.
    Cho 和 A. Oh（编）。[http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)
- en: 'Xiao et al. (2024) Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai
    Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. 2024. InfLLM: Unveiling
    the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with
    Training-Free Memory. *CoRR* abs/2402.04617 (2024). [https://doi.org/10.48550/ARXIV.2402.04617](https://doi.org/10.48550/ARXIV.2402.04617)
    arXiv:2402.04617'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等（2024）超军 Xiao、鹏乐 Zhang、徐 Han、光轩 Xiao、彦凯 Lin、正言 Zhang、志远 Liu、宋 Han 和茂松
    Sun。2024。InfLLM：揭示LLMs在无需训练的记忆下理解极长序列的内在能力。*CoRR* abs/2402.04617（2024）。[https://doi.org/10.48550/ARXIV.2402.04617](https://doi.org/10.48550/ARXIV.2402.04617)
    arXiv:2402.04617
- en: Xiao et al. (2023) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. 2023. Efficient Streaming Language Models with Attention Sinks. *CoRR*
    abs/2309.17453 (2023). [https://doi.org/10.48550/ARXIV.2309.17453](https://doi.org/10.48550/ARXIV.2309.17453)
    arXiv:2309.17453
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等（2023）光轩 Xiao、元东 Tian、贝迪 Chen、宋 Han 和 Mike Lewis。2023。高效流式语言模型与注意力汇聚。*CoRR*
    abs/2309.17453（2023）。[https://doi.org/10.48550/ARXIV.2309.17453](https://doi.org/10.48550/ARXIV.2309.17453)
    arXiv:2309.17453
- en: 'Yang et al. (2024) June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon,
    Gunho Park, Eunho Yang, Se Jung Kwon, and Dongsoo Lee. 2024. No Token Left Behind:
    Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization.
    *CoRR* abs/2402.18096 (2024). [https://doi.org/10.48550/ARXIV.2402.18096](https://doi.org/10.48550/ARXIV.2402.18096)
    arXiv:2402.18096'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2024）俊永 Yang、炳沃 Kim、郑仁 Bae、范锡 Kwon、根浩 Park、恩浩 Yang、世钟 Kwon 和东洙 Lee。2024。没有令牌被遗留：通过重要性感知的混合精度量化实现可靠的KV缓存压缩。*CoRR*
    abs/2402.18096（2024）。[https://doi.org/10.48550/ARXIV.2402.18096](https://doi.org/10.48550/ARXIV.2402.18096)
    arXiv:2402.18096
- en: 'Zhang et al. (2024) Hailin Zhang, Zirui Liu, Boxuan Chen, Yikai Zhao, Tong
    Zhao, Tong Yang, and Bin Cui. 2024. CAFE: Towards Compact, Adaptive, and Fast
    Embedding for Large-scale Recommendation Models. *Proceedings of the ACM on Management
    of Data* 2, 1 (2024), 1–28.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2024）海林 Zhang、子锐 Liu、博轩 Chen、亦凯 Zhao、彤 Zhao、彤 Yang 和斌 Cui。2024。CAFE：面向大规模推荐模型的紧凑、自适应和快速嵌入。*ACM数据管理会议录*
    2, 1（2024），1–28。
- en: 'Zhang et al. (2023b) Hailin Zhang, Yujing Wang, Qi Chen, Ruiheng Chang, Ting
    Zhang, Ziming Miao, Yingyan Hou, Yang Ding, Xupeng Miao, Haonan Wang, Bochen Pang,
    Yuefeng Zhan, Hao Sun, Weiwei Deng, Qi Zhang, Fan Yang, Xing Xie, Mao Yang, and
    Bin Cui. 2023b. Model-enhanced Vector Index. In *Advances in Neural Information
    Processing Systems 36: Annual Conference on Neural Information Processing Systems
    2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023*, Alice Oh, Tristan
    Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.).
    [http://papers.nips.cc/paper_files/paper/2023/hash/ac112e8ffc4e5b9ece32070440a8ca43-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/ac112e8ffc4e5b9ece32070440a8ca43-Abstract-Conference.html)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023b）海林 Zhang、玉晶 Wang、齐 Chen、瑞恒 Chang、婷 Zhang、子铭 Miao、颖燕 Hou、杨 Ding、旭鹏
    Miao、浩南 Wang、博辰 Pang、岳峰 Zhan、浩 Sun、伟伟 Deng、齐 Zhang、凡 Yang、兴 Xie、毛 Yang 和斌 Cui。2023b。模型增强的向量索引。在*神经信息处理系统进展
    36：2023年神经信息处理系统年会，NeurIPS 2023，美国新奥尔良，2023年12月10日 - 16日*，Alice Oh、Tristan Naumann、Amir
    Globerson、Kate Saenko、Moritz Hardt 和 Sergey Levine（编）。[http://papers.nips.cc/paper_files/paper/2023/hash/ac112e8ffc4e5b9ece32070440a8ca43-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/ac112e8ffc4e5b9ece32070440a8ca43-Abstract-Conference.html)
- en: Zhang et al. (2023c) Hailin Zhang, Penghao Zhao, Xupeng Miao, Yingxia Shao,
    Zirui Liu, Tong Yang, and Bin Cui. 2023c. Experimental Analysis of Large-scale
    Learnable Vector Storage Compression. *Proc. VLDB Endow.* 17, 4 (2023), 808–822.
    [https://www.vldb.org/pvldb/vol17/p808-zhang.pdf](https://www.vldb.org/pvldb/vol17/p808-zhang.pdf)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023c) Hailin Zhang, Penghao Zhao, Xupeng Miao, Yingxia Shao,
    Zirui Liu, Tong Yang, 和 Bin Cui. 2023c. 大规模可学习向量存储压缩的实验分析。*Proc. VLDB Endow.*
    17, 4 (2023), 808–822. [https://www.vldb.org/pvldb/vol17/p808-zhang.pdf](https://www.vldb.org/pvldb/vol17/p808-zhang.pdf)
- en: 'Zhang et al. (2023a) Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen,
    Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett,
    Zhangyang Wang, and Beidi Chen. 2023a. H2O: Heavy-Hitter Oracle for Efficient
    Generative Inference of Large Language Models. In *Advances in Neural Information
    Processing Systems 36: Annual Conference on Neural Information Processing Systems
    2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023*, Alice Oh, Tristan
    Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.).
    [http://papers.nips.cc/paper_files/paper/2023/hash/6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023a) Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen,
    Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett,
    Zhangyang Wang, 和 Beidi Chen. 2023a. H2O：用于大型语言模型高效生成推理的重型热点预言机。在 *Advances in
    Neural Information Processing Systems 36: Annual Conference on Neural Information
    Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
    2023* 中，Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,
    和 Sergey Levine (编). [http://papers.nips.cc/paper_files/paper/2023/hash/6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html)'
- en: 'Zhong et al. (2024) Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo
    Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. 2024. DistServe: Disaggregating Prefill
    and Decoding for Goodput-optimized Large Language Model Serving. *CoRR* abs/2401.09670
    (2024). [https://doi.org/10.48550/ARXIV.2401.09670](https://doi.org/10.48550/ARXIV.2401.09670)
    arXiv:2401.09670'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong et al. (2024) Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu,
    Xuanzhe Liu, Xin Jin, 和 Hao Zhang. 2024. DistServe：为优化吞吐量的预填充与解码分离的大型语言模型服务。*CoRR*
    abs/2401.09670 (2024). [https://doi.org/10.48550/ARXIV.2401.09670](https://doi.org/10.48550/ARXIV.2401.09670)
    arXiv:2401.09670
