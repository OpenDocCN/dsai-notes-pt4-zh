- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:04:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:02
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the
    Tensor-Train Decomposition'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorGPT：基于张量列分解的LLMs嵌入层高效压缩
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2307.00526](https://ar5iv.labs.arxiv.org/html/2307.00526)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2307.00526](https://ar5iv.labs.arxiv.org/html/2307.00526)
- en: Mingxue Xu
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 徐明学
- en: Department of Electrical and Electronic Engineering
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 电气与电子工程系
- en: Imperial College London
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦帝国学院
- en: London, United Kingdom
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦，英国
- en: m.xu21@imperial.ac.uk
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: m.xu21@imperial.ac.uk
- en: '&Yao Lei Xu'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '&姚磊·徐'
- en: Department of Electrical and Electronic Engineering
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 电气与电子工程系
- en: Imperial College London
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦帝国学院
- en: London, United Kingdom
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦，英国
- en: yao.xu15@imperial.ac.uk
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: yao.xu15@imperial.ac.uk
- en: '&Danilo P. Mandic'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '&丹尼洛·P·曼迪克'
- en: Department of Electrical and Electronic Engineering
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 电气与电子工程系
- en: Imperial College London
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦帝国学院
- en: London, United Kingdom
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦，英国
- en: d.mandic@imperial.ac.uk
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: d.mandic@imperial.ac.uk
- en: Abstract
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: High-dimensional token embeddings underpin Large Language Models (LLMs), as
    they can capture subtle semantic information and significantly enhance the modelling
    of complex language patterns. However, the associated high dimensionality also
    introduces considerable model parameters, and a prohibitively high model storage.
    To address this issue, this work proposes an approach based on the Tensor-Train
    Decomposition (TTD), where each token embedding is treated as a Matrix Product
    State (MPS) that can be efficiently computed in a distributed manner. The experimental
    results on GPT-2 demonstrate that, through our approach, the embedding layer can
    be compressed by a factor of up to 38.40 times, and when the compression factor
    is 3.31 times, even produced a better performance than the original GPT-2 model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 高维令牌嵌入支撑了大型语言模型（LLMs），因为它们可以捕捉微妙的语义信息，并显著增强复杂语言模式的建模。然而，相关的高维度也带来了大量模型参数和极高的模型存储需求。为了解决这个问题，本研究提出了一种基于张量列分解（TTD）的方法，其中每个令牌嵌入被视为矩阵乘积态（MPS），可以高效地以分布式方式计算。对GPT-2的实验结果表明，通过我们的方法，嵌入层可以压缩最多38.40倍，而当压缩因子为3.31倍时，甚至比原始GPT-2模型表现更好。
- en: 1 Introduction
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Storage efficiency is currently prohibitive to unlocking the full potential
    of lightweight applications of Large Language Models (LLMs). For example, a well-known
    LLM, the Generative Pre-trained Transformer 2 (GPT-2) [[2](#bib.bib2)] has 1.5
    billion parameters and requires significant disk space, making it prohibitive
    to be deployed on lower-end devices. One solution to improve storage efficiency,
    one solution is compressing the embedding layer, which often accounts for a large
    portion of the parameters. As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the
    Tensor-Train Decomposition"), in GPT-2, the embedding layer takes up 31.02% of
    the parameters of the whole model; therefore, the compression of the embedding
    layer would substantially reduce the space complexity of LLMs and make them available
    in edge devices.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '存储效率目前限制了大型语言模型（LLMs）轻量化应用的全部潜力。例如，一个著名的LLM，生成预训练变换器2（GPT-2）[[2](#bib.bib2)]具有15亿个参数，并需要大量的磁盘空间，这使得在低端设备上部署变得不可行。提高存储效率的一个解决方案是压缩嵌入层，这通常占据了大部分参数。如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ TensorGPT: Efficient Compression of the Embedding
    Layer in LLMs based on the Tensor-Train Decomposition")所示，在GPT-2中，嵌入层占整个模型参数的31.02%；因此，压缩嵌入层将显著降低LLMs的空间复杂度，并使其在边缘设备上可用。'
- en: '![Refer to caption](img/f3088083dee2cf2d0a1f1362b2084b52.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f3088083dee2cf2d0a1f1362b2084b52.png)'
- en: 'Figure 1: The number of parameters of the layers in GPT-2.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：GPT-2中层的参数数量。
- en: To this end, we propose to compress the embedding layer of LLMs through Tensor-Train
    Decomposition (TTD) [[8](#bib.bib8)] in order to store large embeddings in a low-rank
    tensor format, with much fewer parameters. This low-rank tensor format is also
    called TT-format or Matrix Product State (MPS) [[9](#bib.bib9)]. Given the fact
    that in many applications the token vocabulary is ever-changing, we consider each
    individual token embedding (i.e. each row of the token embedding matrix) rather
    than taking the token embedding matrix as a whole. Benefiting from the super-compression
    properties of Tensor Networks (TNs), we tensorize and decompose each token embedding,
    and then construct a highly efficient format of embedding that can be computed
    efficiently through distributed computing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们建议通过张量列分解（TTD）[[8](#bib.bib8)] 压缩 LLM 的嵌入层，以便将大规模的嵌入存储在低秩张量格式中，从而减少参数量。这种低秩张量格式也称为
    TT 格式或矩阵乘积态（MPS）[[9](#bib.bib9)]。鉴于许多应用中令牌词汇表不断变化，我们考虑每个单独的令牌嵌入（即令牌嵌入矩阵的每一行），而不是将令牌嵌入矩阵作为整体。借助张量网络（TNs）的超压缩特性，我们对每个令牌嵌入进行张量化和分解，然后构建一种高效的嵌入格式，可以通过分布式计算高效地计算。
- en: The proposed approach is evaluated on the GPT-2 model. The experiment results
    show that, using our approach, the embedding layers can indeed be compressed with
    a compression rate of up to 38.40 times, and with a compression rate of 3.31 didn’t
    sacrifice model performance. As, due to the approximations involved, for the model
    performance change after compression, we considered the performance of our TensorGPT
    on the text reconstruction task, a basic text generation task where the GPT series
    models excel. We found that with the reconstructed embedding layer from the stored
    MPS, the overall performance of the GPT-2 even improved under certain TT rank
    settings, this is likely due to the over-parameterization of the original model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPT-2 模型上评估了提出的方法。实验结果表明，使用我们的方法，嵌入层确实可以压缩，压缩率高达38.40倍，且压缩率为3.31时未牺牲模型性能。由于涉及近似，对于压缩后模型性能的变化，我们考虑了
    TensorGPT 在文本重建任务中的表现，这是 GPT 系列模型擅长的基础文本生成任务。我们发现，使用从存储的 MPS 中重建的嵌入层，在某些 TT 秩设置下，GPT-2
    的整体性能甚至有所提升，这可能是由于原始模型的过参数化所致。
- en: 'Our contributions are summarized as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献总结如下：
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To our best knowledge, we are the first to utilize the Tensor-Train Decomposition
    (TTD) and Matrix Product State (MPS) to compress GPT series models.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，我们是首个利用张量列分解（TTD）和矩阵乘积态（MPS）来压缩 GPT 系列模型的研究。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A novel approach that treats each token embedding as a Matrix Product State
    is proposed, which is shown to be very flexible when the token embeddings are
    inserted or deleted, and also has the potential to be computed in a distributed
    manner.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出了将每个令牌嵌入视为矩阵乘积态的新方法，该方法在令牌嵌入被插入或删除时表现出很大的灵活性，并且具有以分布式方式计算的潜力。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A set of rigorous evaluation metrics is adopted to evaluate our approach. The
    experimental results show that our compression approach has satisfactory performance,
    while reducing the number of parameters by a factor of 2.31.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 采用了一组严格的评估指标来评估我们的方法。实验结果表明，我们的压缩方法性能令人满意，同时将参数数量减少了2.31倍。
- en: 2 Related Work
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Among the recent research on the compression of the embedding layers within
    LLMs tensor with decompositions, the closest to our approach is the work by [[13](#bib.bib13)],
    applied TTD to the embedding layer to reduce the space complexity of large language
    models. This was achieved by reshaping the embedding matrix into an order-$2N$
    tensor which was then decomposed and stored as a Matrix Product Operator. While
    this approach reduced the number of parameters significantly, the decomposition
    procedure had to be repeated on the entire embedding matrix every time a new token
    is added to the dictionary. To solve this issue, instead of decomposing and storing
    the embedding matrix directly, we propose an approach that decomposes and stores
    each row of the embedding matrix separately. This reduces the computation costs
    significantly, as the decomposition can be performed locally on every new token.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近关于LLMs嵌入层的压缩研究中，最接近我们方法的是[[13](#bib.bib13)]的工作，该工作将TTD应用于嵌入层以减少大型语言模型的空间复杂性。这是通过将嵌入矩阵重塑为一个阶数为$2N$的张量，然后对其进行分解并存储为矩阵乘积算符来实现的。虽然这种方法显著减少了参数数量，但每次将新令牌添加到词典时都必须对整个嵌入矩阵重复分解过程。为了解决这个问题，我们提出了一种方法，该方法分别对嵌入矩阵的每一行进行分解和存储。这大大减少了计算成本，因为分解可以在每个新令牌上局部进行。
- en: Recent progress also includes the compression of the embedding table of a recommendation
    system in the work by [[5](#bib.bib5)], where the tensorized neural network is
    trained from scratch, yet our proposed approach operates on a pre-trained neural
    network without an extra training process. In another work [[4](#bib.bib4)], Block-Wise
    Low-Rank Approximation is used to compress very large scale ($\sim$800k vocabulary)
    embeddings, where the embedding matrices are split into blocks according to the
    tokens, and then each block is decomposed by SVD. Except for the difference in
    decomposition methods with our proposed approach, deciding how to reasonably bind
    certain tokens into blocks for a specific vocabulary requires additional effort.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的进展还包括在[[5](#bib.bib5)]的工作中对推荐系统的嵌入表进行压缩，其中张量化神经网络是从头开始训练的，而我们提出的方法则在预训练的神经网络上进行，无需额外的训练过程。在另一项工作[[4](#bib.bib4)]中，使用了块状低秩近似来压缩非常大规模（$\sim$800k词汇量）的嵌入，其中嵌入矩阵根据令牌被拆分成块，然后每个块通过SVD进行分解。除了与我们提出的方法在分解方法上的不同外，如何合理地将某些令牌绑定到特定词汇量的块中还需要额外的努力。
- en: 3 Preliminaries
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基础知识
- en: 'This section gives brief mathematical preliminaries of tensor algebra, and
    basic knowledge in LLMs to facilitate the understanding of our proposed methodology
    in Section [4](#S4 "4 Methodology ‣ TensorGPT: Efficient Compression of the Embedding
    Layer in LLMs based on the Tensor-Train Decomposition").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '本节简要介绍了张量代数的数学基础和LLMs中的基本知识，以促进对我们在第[4](#S4 "4 Methodology ‣ TensorGPT: Efficient
    Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition")节中提出的方法的理解。'
- en: 3.1 Tensor and Tensor Operations
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 张量与张量操作
- en: Order-N Tensor.
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 阶数-$N$的张量。
- en: An order-$N$).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一个阶数为$N$的。
- en: Tensor Entries.
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 张量条目。
- en: The $(i_{1},\ldots,i_{N})$).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $(i_{1},\ldots,i_{N})$。
- en: Tensorization.
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 张量化。
- en: A vector, $\mathbf{a}\in\mathbb{R}^{I_{1}I_{2}\cdots I_{N}}$, with the relation
    between their entries defined by
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 向量$\mathbf{a}\in\mathbb{R}^{I_{1}I_{2}\cdots I_{N}}$，其条目之间的关系定义为
- en: '|  | $\mathcalbf{A}[i_{1},i_{2},\dots,i_{N}]=a_{i}$ |  | (1) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcalbf{A}[i_{1},i_{2},\dots,i_{N}]=a_{i}$ |  | (1) |'
- en: for all $1\leq i_{n}\leq I_{n}$.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有$1\leq i_{n}\leq I_{n}$。
- en: Matricization (Mode-n unfolding).
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 矩阵化（模式-n展开）。
- en: Mode-$n$, denotes the mode of matricization, and is given by
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 模式-$n$，表示矩阵化的模式，其定义为
- en: '|  | $\mathbf{A}_{(1)}\bigg{[}i_{1},\overline{i_{2}i_{3}\ldots i_{N}}\bigg{]}=\mathcalbf{A}[i_{1},i_{2},\ldots,i_{N}]$
    |  | (2) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{A}_{(1)}\bigg{[}i_{1},\overline{i_{2}i_{3}\ldots i_{N}}\bigg{]}=\mathcalbf{A}[i_{1},i_{2},\ldots,i_{N}]$
    |  | (2) |'
- en: 'Note that the overlined subscripts refer to to linear indexing (or Little-Endian)
    [[10](#bib.bib10)], given by:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，带下划线的下标指的是线性索引（或小端格式）[[10](#bib.bib10)]，其定义为：
- en: '|  | $\displaystyle\overline{i_{1}i_{2}\dots i_{N}}$ |  | (3) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overline{i_{1}i_{2}\dots i_{N}}$ |  | (3) |'
- en: '|  |  | $\displaystyle=1+i_{1}+(i_{2}-1)I_{1}+\cdots+(i_{n}-1)I_{1}\ldots I_{N-1}$
    |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=1+i_{1}+(i_{2}-1)I_{1}+\cdots+(i_{n}-1)I_{1}\ldots I_{N-1}$
    |  |'
- en: Vectorization.
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量化。
- en: Given an order-$N$.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个阶数为$N$的。
- en: Tensor contraction.
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 张量收缩。
- en: The contraction of an order-$N$, with entries
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一个阶数为$N$的收缩，其条目为
- en: '|  |  | $1$2 |  | (4) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (4) |'
- en: A $(2,1)$.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个$(2,1)$。
- en: 3.2 Token, Token Embeddings and Embedding Layer in LLMs
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 令牌、令牌嵌入与LLMs中的嵌入层
- en: Token and Tokenization.
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标记和标记化。
- en: In natural language processing (NLP), a token is a meaningful unit of text,
    such as a word, punctuation mark, or other element that contributes to the structure
    and meaning of a sentence or document. Tokens are produced through a process known
    as tokenization, which involves breaking down a piece of text into individual
    units. The GPT series models employ a widely used tokenization method named Byte
    Pair Encoding (BPE) [[7](#bib.bib7)]. The BPE breaks down text into varying-length
    subword units, and is particularly useful for languages with complex morphology
    or when dealing with out-of-vocabulary words.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，标记是一个有意义的文本单位，例如一个词、标点符号或其他对句子或文档的结构和意义有贡献的元素。标记通过一种称为标记化的过程生成，该过程涉及将一段文本拆分成单独的单位。GPT系列模型采用了一种广泛使用的标记化方法，名为字节对编码（BPE） [[7](#bib.bib7)]。BPE将文本分解为不同长度的子词单元，特别适用于具有复杂形态的语言或处理词汇表外的词。
- en: Token Embedding and Embedding Layer in LLMs.
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标记嵌入和LLMs中的嵌入层。
- en: In the context of LLMs, “embedding” refers to converting discrete input tokens
    into continuous vector representations. These representations are commonly known
    as word embeddings or token embeddings. In LLMs, the embedding layer is responsible
    for output token embeddings according to the sequential input tokens. This layer
    maps each input token to a high-dimensional vector that captures the semantic
    and syntactic information about the meaning and context of a token. Normally,
    an embedding layer considers a set of tokens $\{v\}$ is excessively large, there
    would be excessive parameter complexity, resulting in high storage costs for the
    embedding layer, and thereafter high storage costs for the whole language model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs的背景下，“嵌入”指的是将离散输入标记转换为连续的向量表示。这些表示通常被称为词嵌入或标记嵌入。在LLMs中，嵌入层负责根据序列输入标记输出标记嵌入。该层将每个输入标记映射到一个高维向量，该向量捕捉了关于标记意义和上下文的语义和句法信息。通常，如果嵌入层考虑的标记集$\{v\}$过大，则会导致过度的参数复杂性，从而导致嵌入层的存储成本高，进而导致整个语言模型的存储成本高。
- en: Remark 1.
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注释 1。
- en: The embedding weight matrix W can be seen as a lookup table. A basic embedding
    generation finds the token embeddings corresponding to all the input tokens and
    stacks them according to the input order. It should be noted that in the current
    LLMs, such as GPT-like and BERT-like models, the position and mask information
    of the tokens are also encoded into the embeddings. In these cases, a token embedding
    $\textbf{x}_{v}$ is not merely generated via a lookup process.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入权重矩阵W可以被视为一个查找表。基础的嵌入生成方法找到所有输入标记对应的标记嵌入，并按输入顺序堆叠它们。需要注意的是，在当前的LLMs中，例如GPT类和BERT类模型，标记的位置信息和掩码信息也被编码到嵌入中。在这些情况下，标记嵌入$\textbf{x}_{v}$不仅仅是通过查找过程生成的。
- en: 4 Methodology
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法论
- en: This section gives a brief introduction to the technical cornerstones that our
    approach relies on, and a detailed description of our proposed approach.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要介绍了我们的方法所依赖的技术基石，并对我们提出的方法进行了详细描述。
- en: 4.1 Tensor-Train Decomposition (TTD) and Matrix Product State (MPS)
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 张量-列车分解（TTD）和矩阵乘积态（MPS）
- en: 'Tensor-Train Decomposition [[14](#bib.bib14), [8](#bib.bib8)] was introduced
    to help mitigate the computational bottlenecks that arise from the curse of dimensionality,
    as tensor algorithms can become intractable for high-order tensors. The most common
    form of TT is the Matrix Product State (MPS or TT-MPS), introduced in the quantum
    physics community [[11](#bib.bib11)], which applies the Tensor-Train Singular
    Value Decomposition (TT-SVD) algorithm described in Algorithm [1](#algorithm1
    "In 4.1 Tensor-Train Decomposition (TTD) and Matrix Product State (MPS) ‣ 4 Methodology
    ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the
    Tensor-Train Decomposition") [[12](#bib.bib12)] to decomposes a large order-$N$,
    such that'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 张量-列车分解 [[14](#bib.bib14), [8](#bib.bib8)] 被引入以帮助缓解由维度灾难引起的计算瓶颈，因为对于高阶张量，张量算法可能变得难以处理。TT最常见的形式是矩阵乘积态（MPS或TT-MPS），在量子物理界引入 [[11](#bib.bib11)]，它应用了张量-列车奇异值分解（TT-SVD）算法，如算法[1](#algorithm1
    "在4.1张量-列车分解（TTD）和矩阵乘积态（MPS） ‣ 4 方法 ‣ TensorGPT：基于张量-列车分解的LLMs嵌入层的高效压缩") [[12](#bib.bib12)]所述，来分解一个高阶-$N$，使得
- en: '|  | $1$2 |  | (5) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: The tensors $\mathcalbf{G}^{(1)},\ldots,\mathcalbf{G}^{(N)}$, the MPS assumes
    the element-wise form as
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 张量$\mathcalbf{G}^{(1)},\ldots,\mathcalbf{G}^{(N)}$，MPS假设元素-wise形式为
- en: '|  | $x_{i_{1},i_{2},\dots,i_{N}}=\mathbf{G}^{(1)}_{:,i_{1},:}\cdots\mathbf{G}^{(N)}_{:,i_{N},:}$
    |  | (6) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{i_{1},i_{2},\dots,i_{N}}=\mathbf{G}^{(1)}_{:,i_{1},:}\cdots\mathbf{G}^{(N)}_{:,i_{N},:}$
    |  | (6) |'
- en: Input :  Data tensor, $\mathcalbf{X}\in\mathbb{R}^{I_{1}\times I_{2}\times\cdots\times
    I_{N}}$*
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：数据张量，$\mathcalbf{X}\in\mathbb{R}^{I_{1}\times I_{2}\times\cdots\times I_{N}}$*
- en: Algorithm 1 Tensor-Train Singular Value Decomposition (TT-SVD)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 张量-列式奇异值分解 (TT-SVD)
- en: 4.2 MPS formatted Token Embedding
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 MPS 格式的令牌嵌入
- en: 'As mentioned in Section [1](#S1 "1 Introduction ‣ TensorGPT: Efficient Compression
    of the Embedding Layer in LLMs based on the Tensor-Train Decomposition") and Section [2](#S2
    "2 Related Work ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs
    based on the Tensor-Train Decomposition"), when the vocabulary changes, the tensor
    decomposition should be re-executed from scratch if the decomposition object is
    the whole embedding weight matrix. On the other hand, loading the whole embedding
    weight matrix into the memory and then decomposing also requires massive memory
    and computation resources. Using the notions in Section [3](#S3 "3 Preliminaries
    ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the
    Tensor-Train Decomposition") and Algorithm [1](#algorithm1 "In 4.1 Tensor-Train
    Decomposition (TTD) and Matrix Product State (MPS) ‣ 4 Methodology ‣ TensorGPT:
    Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train
    Decomposition"), decomposing a 2-order W requires roughly the computation cost
    of $\mathcal{O}(VD^{2})$.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '如第 [1](#S1 "1 Introduction ‣ TensorGPT: Efficient Compression of the Embedding
    Layer in LLMs based on the Tensor-Train Decomposition") 节和第 [2](#S2 "2 Related
    Work ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on
    the Tensor-Train Decomposition") 节中提到，当词汇表发生变化时，如果分解对象是整个嵌入权重矩阵，张量分解应从头开始重新执行。另一方面，将整个嵌入权重矩阵加载到内存中进行分解也需要大量内存和计算资源。使用第
    [3](#S3 "3 Preliminaries ‣ TensorGPT: Efficient Compression of the Embedding Layer
    in LLMs based on the Tensor-Train Decomposition") 节和算法 [1](#algorithm1 "In 4.1
    Tensor-Train Decomposition (TTD) and Matrix Product State (MPS) ‣ 4 Methodology
    ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the
    Tensor-Train Decomposition") 中的概念，分解一个 2 阶 W 大约需要 $\mathcal{O}(VD^{2})$ 的计算成本。'
- en: Rather than decomposing the whole embedding weight matrix, we propose to decompose
    each token embedding. In this way, each token embedding is reshaped into an order-$N$.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的不是分解整个嵌入权重矩阵，而是分解每个令牌嵌入。这样，每个令牌嵌入被重塑为一个 $N$ 阶张量。
- en: 'This approach has two advantages:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有两个优点：
- en: '1.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Lower storage cost: The space complexity reduces from an original $\mathcal{O}\left(I^{N}\right)$
    for simplicity;'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更低的存储成本：空间复杂度从原来的 $\mathcal{O}\left(I^{N}\right)$ 降低；
- en: '2.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Affordable computation cost of TTD on resource-constrained nodes. Since token
    embeddings are decomposed individually, the decomposition of an individual token
    embedding or a small number of token embeddings can be offloaded to a single resource-constrained
    node (thread, process or device). On a single node, the computation cost is reduced
    from $\mathcal{O}(VD^{2})$. Also considering the ever-changing vocabulary, this
    approach has the potential to be deployed in a dynamic distributed system.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在资源受限节点上，TTD 的计算成本是可承受的。由于令牌嵌入是单独分解的，因此可以将单个令牌嵌入或少量令牌嵌入的分解卸载到单个资源受限节点（线程、进程或设备）上。在单个节点上，计算成本从
    $\mathcal{O}(VD^{2})$ 降低。此外，考虑到不断变化的词汇表，这种方法具有在动态分布式系统中部署的潜力。
- en: Remark 2.
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注释 2.
- en: The considered tensor embedding procedure is highly effective for a small rank
    tensor, $R$.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 所考虑的张量嵌入过程对小秩张量 $R$ 高效。
- en: Remark 3.
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注释 3.
- en: In practice, the MPS is a low-rank approximation of the original embedding.
    However, the approximation error will tend to zero as the rank $R$ will have to
    balance the trade-off between the compression power and the approximation ability.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，MPS 是原始嵌入的低秩近似。然而，随着秩 $R$ 的变化，近似误差将趋于零，因为秩 $R$ 必须在压缩能力和近似能力之间进行权衡。
- en: Remark 4.
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注释 4.
- en: 'Without considering the position and mask information mentioned in Remark [1](#Thmremark1
    "Remark 1\. ‣ Token Embedding and Embedding Layer in LLMs. ‣ 3.2 Token, Token
    Embeddings and Embedding Layer in LLMs ‣ 3 Preliminaries ‣ TensorGPT: Efficient
    Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"),
    the MPS token embedding approach can be directly used to accelerate the token
    embedding mapping and compress the stored embeddings. However, since the language
    models we discuss in this paper are typical LLMs containing position and mask
    encoding, we shall not consider the above two.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '不考虑在备注 [1](#Thmremark1 "备注 1\. ‣ Token Embedding 和 LLM 中的 Embedding Layer。
    ‣ 3.2 Token、Token Embeddings 和 LLM 中的 Embedding Layer ‣ 3 初步 ‣ TensorGPT: 基于 Tensor-Train
    分解的 LLM 中的 Embedding Layer 高效压缩") 中提到的位置和掩码信息，MPS token 嵌入方法可以直接用于加速 token 嵌入映射和压缩存储的嵌入。然而，由于我们在本文中讨论的语言模型是包含位置和掩码编码的典型
    LLM，因此我们将不考虑上述两点。'
- en: 5 Experiment
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'Table 1: Compression rate $\eta$.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 压缩率 $\eta$。'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Methods &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方法 &#124;'
- en: '|'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TT cores’ ranks &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TT 核的秩 &#124;'
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Compression &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 压缩 &#124;'
- en: '&#124; Rate $\eta$ &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 比率 $\eta$ &#124;'
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MAE of Layer &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 层的 MAE &#124;'
- en: '&#124; Reconstruction &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建 &#124;'
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Embeddings &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 嵌入 &#124;'
- en: '&#124; norm-MAE &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; norm-MAE &#124;'
- en: '&#124; ($\times 10^{-3}$) &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ($\times 10^{-3}$) &#124;'
- en: '|'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Generation &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成 &#124;'
- en: '&#124; Loss &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 损失 &#124;'
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Original | —- | 13.71 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | —- | 13.71 |'
- en: '| --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| TTD weight matrix with 2 cores (SVD) | 1,2,1 | 378.22$\times$ | 0.1209 |
    2.115 | 20.17 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 带 2 个核心的 TTD 权重矩阵（SVD） | 1,2,1 | 378.22$\times$ | 0.1209 | 2.115 | 20.17
    |'
- en: '| 1,4,1 | 189.11$\times$ | 0.1079 | 2.260 | 22.00 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 1,4,1 | 189.11$\times$ | 0.1079 | 2.260 | 22.00 |'
- en: '| 1,8,1 | 94.56$\times$ | 0.1014 | 2.401 | 16.70 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 1,8,1 | 94.56$\times$ | 0.1014 | 2.401 | 16.70 |'
- en: '| 1,16,1 | 47.28$\times$ | 0.0989 | 2.481 | 14.70 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 1,16,1 | 47.28$\times$ | 0.0989 | 2.481 | 14.70 |'
- en: '| 1,32,1 | 23.64$\times$ | 0.0917 | 2.311 | 10.14 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 1,32,1 | 23.64$\times$ | 0.0917 | 2.311 | 10.14 |'
- en: '| 1,641,1 | 11.82$\times$ | 0.0848 | 2.113 | 10.13 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 1,641,1 | 11.82$\times$ | 0.0848 | 2.113 | 10.13 |'
- en: '| 1,128,1 | 5.91$\times$ | 0.0794 | 1.848 | 10.65 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 1,128,1 | 5.91$\times$ | 0.0794 | 1.848 | 10.65 |'
- en: '| 1,256,1 | 2.95$\times$ | 0.0690 | 1.500 | 12.74 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 1,256,1 | 2.95$\times$ | 0.0690 | 1.500 | 12.74 |'
- en: '| 1,512,1 | 1.48$\times$ | 0.0458 | 1.012 | 9.82 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 1,512,1 | 1.48$\times$ | 0.0458 | 1.012 | 9.82 |'
- en: '| TTD for each token $\texttt{ten}(\textbf{x}_{v})$ | 0.1120 | 2.573 | 20.77
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 每个 token 的 TTD $\texttt{ten}(\textbf{x}_{v})$ | 0.1120 | 2.573 | 20.77 |'
- en: '| 1,1,1,1,1,2,1,1,1,1,1 | 32.00$\times$ | 0.1119 | 3.063 | 20.88 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 1,1,1,1,1,2,1,1,1,1,1 | 32.00$\times$ | 0.1119 | 3.063 | 20.88 |'
- en: '| 1,1,1,1,2,2,2,1,1,1,1 | 21.33$\times$ | 0.1115 | 2.957 | 27.81 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 1,1,1,1,2,2,2,1,1,1,1 | 21.33$\times$ | 0.1115 | 2.957 | 27.81 |'
- en: '| 1,1,1,1,2,4,2,1,1,1,1 | 14.77$\times$ | 0.1113 | 2.864 | 28.60 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 1,1,1,1,2,4,2,1,1,1,1 | 14.77$\times$ | 0.1113 | 2.864 | 28.60 |'
- en: '| 1,2,2,2,2,2,2,2,2,2,1 | 10.67$\times$ | 0.1090 | 2.695 | 19.40 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 1,2,2,2,2,2,2,2,2,2,1 | 10.67$\times$ | 0.1090 | 2.695 | 19.40 |'
- en: '| 1,1,2,4,4,4,4,4,4,1,1 | 4.00$\times$ | 0.1054 | 2.628 | 22.63 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 1,1,2,4,4,4,4,4,4,1,1 | 4.00$\times$ | 0.1054 | 2.628 | 22.63 |'
- en: '| 1,2,4,4,4,4,4,4,4,2,1 | 3.31$\times$ | 0.0995 | 2.574 | 9.01 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 1,2,4,4,4,4,4,4,4,2,1 | 3.31$\times$ | 0.0995 | 2.574 | 9.01 |'
- en: '| 1,2,2,4,8,8,8,4,2,2,1 | 2.33$\times$ | 0.0965 | 2.549 | 51.24 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 1,2,2,4,8,8,8,4,2,2,1 | 2.33$\times$ | 0.0965 | 2.549 | 51.24 |'
- en: '| 1,2,4,8,8,8,8,8,4,2,1 | 1.13$\times$ | 0.0744 | 0.883 | 39.88 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 1,2,4,8,8,8,8,8,4,2,1 | 1.13$\times$ | 0.0744 | 0.883 | 39.88 |'
- en: 5.1 Dataset, Tokenization and Language Model
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 数据集、分词与语言模型
- en: 'The text dataset used for the experiments was a mixed version of General Language
    Understanding Evaluation (GLUE) [[1](#bib.bib1)] and Microsoft Research Paraphrase
    Corpus (MRPC) [[3](#bib.bib3)], with 11,602 English sentences in total. For the
    tokenization, we chose the BPE mentioned in Section [3.2](#S3.SS2 "3.2 Token,
    Token Embeddings and Embedding Layer in LLMs ‣ 3 Preliminaries ‣ TensorGPT: Efficient
    Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"),
    since it is popular in GPT series models. The language model we used was the HuggingFace
    version of GPT-2¹¹1https://huggingface.co/gpt2, with an embedding weight matrix
    $\textbf{W}\in\mathbb{R}^{50257\times 768}$ is 768.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '用于实验的文本数据集是 General Language Understanding Evaluation (GLUE) 的混合版本[[1](#bib.bib1)]
    和 Microsoft Research Paraphrase Corpus (MRPC) [[3](#bib.bib3)]，总共有 11,602 个英语句子。对于分词，我们选择了第
    [3.2](#S3.SS2 "3.2 Token、Token Embeddings 和 LLM 中的 Embedding Layer ‣ 3 初步 ‣ TensorGPT:
    基于 Tensor-Train 分解的 LLM 中的 Embedding Layer 高效压缩") 节中提到的 BPE，因为它在 GPT 系列模型中很受欢迎。我们使用的语言模型是
    HuggingFace 版本的 GPT-2¹¹1https://huggingface.co/gpt2，嵌入权重矩阵 $\textbf{W}\in\mathbb{R}^{50257\times
    768}$ 是 768。'
- en: 5.2 Implementation
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 实现
- en: The 2.12.0 version of TensorFlow was used for the GPT-2 model, while Tensorly [[6](#bib.bib6)],
    a Python package for tensor learning, was used to decompose the token embeddings
    and the embedding layer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 模型使用了 TensorFlow 2.12.0 版本，而 Tensorly [[6](#bib.bib6)]，一个用于张量学习的 Python
    包，被用来分解 token 嵌入和嵌入层。
- en: 'According to Remark [2](#Thmremark2 "Remark 2\. ‣ 4.2 MPS formatted Token Embedding
    ‣ 4 Methodology ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs
    based on the Tensor-Train Decomposition") and also to take advantage of the hierarchical
    structure and multiway interactions expressiveness of Tensor-Train Decomposition,
    we reshaped each token embedding $\textbf{x}_{v}$ from index 768 to 1,024 are
    almost zeros.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 根据备注 [2](#Thmremark2 "备注 2\. ‣ 4.2 MPS 格式化 Token 嵌入 ‣ 4 方法论 ‣ TensorGPT：基于张量-训练分解的大型语言模型嵌入层的高效压缩")，并利用张量-训练分解的层次结构和多方式交互的表达能力，我们将每个
    token 嵌入 $\textbf{x}_{v}$ 从索引 768 重新塑形到 1,024，几乎为零。
- en: 5.3 Evaluation Metrics
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 评估指标
- en: Compression Rate.
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 压缩率。
- en: We used the embedding layer compression ratio to describe the degree of size
    reduction and efficiency in embedding layer storage. More mathematically, the
    compression rate $\eta$ is the ratio of the original embedding layer size to the
    sum of the size of the compressed token.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用嵌入层压缩比来描述嵌入层存储中尺寸缩小的程度和效率。更数学化地说，压缩率 $\eta$ 是原始嵌入层尺寸与压缩后 token 尺寸总和的比率。
- en: '|  | $\eta=\frac{V\times D}{\sum_{j=1}^{V}\sum_{k=1}^{N}&#124;\mathcalbf{G}^{(k)}_{j}&#124;}$
    |  | (7) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | $\eta=\frac{V\times D}{\sum_{j=1}^{V}\sum_{k=1}^{N}&#124;\mathcalbf{G}^{(k)}_{j}&#124;}$
    |  | (7) |'
- en: where $\mathcalbf{G}^{(k)}_{j}$ in the embedding layer weight matrix W.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入层权重矩阵 W 中，$\mathcalbf{G}^{(k)}_{j}$。
- en: Distortion Error.
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 失真误差。
- en: 'The distortion metrics were used to describe the compression fidelity, that
    is, the similarity between the original embedding layer weights and reconstructed
    embedding layer weights, or the original reconstructed token embeddings and reconstructed
    token embeddings. Considering the inference process of the embedding layer, the
    distortion metrics were first calculated sentence by sentence and then derived
    from the average. There are two kinds of distortion measurements of the embedding
    weight matrix and token embeddings:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 失真指标用于描述压缩保真度，即原始嵌入层权重与重建的嵌入层权重之间，或原始重建的 token 嵌入与重建的 token 嵌入之间的相似性。考虑到嵌入层的推理过程，失真指标首先逐句计算，然后从平均值中得出。嵌入权重矩阵和
    token 嵌入有两种失真测量：
- en: •
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Mean absolute error (MAE) for the embedding weight matrix reconstruction: We
    used MAE to measure the difference between the original embedding layer weight
    matrix and the reconstructed embedding layer weight matrix. A lower MAE suggests
    that the compressed embedding layer weights closely resemble the original embedding
    layer weights, indicating a higher level of fidelity in the compression process.'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 嵌入权重矩阵重建的平均绝对误差（MAE）：我们使用 MAE 来衡量原始嵌入层权重矩阵与重建的嵌入层权重矩阵之间的差异。较低的 MAE 表明压缩后的嵌入层权重与原始嵌入层权重非常接近，表明压缩过程中的保真度更高。
- en: •
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Normalized mean absolute error (norm-MAE) for the token embeddings: The token
    embedding values vary from minus several hundred to several hundred, and to align
    them for easier comparison like embedding weight matrix, we used the normalized
    mean absolute error. The norm-MAE is the ratio of mean absolute error and the
    difference between the maximum and minimum values of original embeddings. Similar
    to MAE for the embedding weight matrix, a lower norm-MAE indicates a higher compression
    fidelity.'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: token 嵌入的归一化平均绝对误差（norm-MAE）：token 嵌入值从负数几百到几百不等，为了使它们更易于比较，例如嵌入权重矩阵，我们使用了归一化平均绝对误差。norm-MAE
    是平均绝对误差与原始嵌入的最大值和最小值之间差异的比率。类似于嵌入权重矩阵的 MAE，较低的 norm-MAE 表示更高的压缩保真度。
- en: Compatibility with the subsequent GPT-2 network blocks.
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与后续 GPT-2 网络块的兼容性。
- en: The primary function of GPT-like models is natural language generation. We here
    conducted a text reconstruction task to verify if the reconstructed embedding
    layer can collaborate effectively with the subsequent GPT-2 network blocks for
    natural language generation. The purpose of this task was to reconstruct the original
    input data from the encoded representation in the embedding layer and the subsequent
    network blocks, similar to an autoencoder. This part serves as a sanity check
    for the stored information in the reconstructed embedding layer, and causes evaluation
    via text generation loss of the GPT-2 model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: GPT类模型的主要功能是自然语言生成。我们在这里进行了一项文本重建任务，以验证重建的嵌入层是否可以与后续的GPT-2网络块有效协作进行自然语言生成。此任务的目的是从嵌入层和后续网络块中的编码表示中重建原始输入数据，类似于自编码器。这部分作为对重建的嵌入层中存储信息的合理性检查，并通过GPT-2模型的文本生成损失进行评估。
- en: '![Refer to caption](img/c456a5dfa1b7c663837e2e2968a0e870.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c456a5dfa1b7c663837e2e2968a0e870.png)'
- en: 'Figure 2: Visualization of the reconstruction distortion error (MAE) for the
    GPT-2 embedding weight matrix W. In each heatmap, a point with location index
    $(j,p)$, with a sacrifice of one token embedding to enable reshaping.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：GPT-2嵌入权重矩阵W的重建失真误差（MAE）的可视化。在每个热图中，一个位置索引为$(j,p)$的点，牺牲一个标记嵌入以便于重新塑形。
- en: 5.4 Experiment Results
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 实验结果
- en: 'All the evaluation metrics described in Section [5.3](#S5.SS3 "5.3 Evaluation
    Metrics ‣ 5 Experiment ‣ TensorGPT: Efficient Compression of the Embedding Layer
    in LLMs based on the Tensor-Train Decomposition") on the dataset GLUE/MRPC are
    shown in Table [1](#S5.T1 "Table 1 ‣ 5 Experiment ‣ TensorGPT: Efficient Compression
    of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"). There
    are two approaches tested for a comprehensive analysis - our proposed approach
    introduced in Section [4](#S4 "4 Methodology ‣ TensorGPT: Efficient Compression
    of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"), and
    the approach of directly decomposing the embedding weight matrix W into two TT
    cores. The latter is actually equivalent to performing tensor SVD.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '在数据集GLUE/MRPC的第[5.3](#S5.SS3 "5.3 Evaluation Metrics ‣ 5 Experiment ‣ TensorGPT:
    Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train
    Decomposition")节中描述的所有评估指标如表[1](#S5.T1 "Table 1 ‣ 5 Experiment ‣ TensorGPT: Efficient
    Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition")所示。测试了两种方法以进行全面分析——我们在第[4](#S4
    "4 Methodology ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs
    based on the Tensor-Train Decomposition")节中提出的方法，以及直接将嵌入权重矩阵W分解为两个TT核心的方法。后者实际上等同于执行张量SVD。'
- en: As the ranks of TT cores increase, both approaches exhibit a decrease in compression
    rate, fidelity of embedding layer reconstruction (MAE), and fidelity of reproduced
    token embeddings (norm-MAE). There is no apparent decline or increase in the text
    generation loss, possibly because this metric is highly dependent on the dataset
    itself. In all settings, the lowest text generation loss was 9.01, which was achieved
    by our approach when the TT ranks were 1,2,4,4,4,4,4,4,1,1.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 随着TT核心的秩数增加，两种方法都表现出压缩率、嵌入层重建的保真度（MAE）和再现标记嵌入的保真度（norm-MAE）的下降。文本生成损失没有明显的下降或增加，可能因为该指标高度依赖于数据集本身。在所有设置中，最低的文本生成损失为9.01，这是在TT秩数为1,2,4,4,4,4,4,4,1,1时由我们的方法获得的。
- en: 5.5 Discussion
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 讨论
- en: 'We visualized the MAE for the reconstruction of embedding weight matrix W in
    Figure [2](#S5.F2 "Figure 2 ‣ Compatibility with the subsequent GPT-2 network
    blocks. ‣ 5.3 Evaluation Metrics ‣ 5 Experiment ‣ TensorGPT: Efficient Compression
    of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"). For
    better visualization, we folded the dimension that represents the vocabulary index
    into a reasonable matrix shape. In Figure [2](#S5.F2 "Figure 2 ‣ Compatibility
    with the subsequent GPT-2 network blocks. ‣ 5.3 Evaluation Metrics ‣ 5 Experiment
    ‣ TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the
    Tensor-Train Decomposition"), the lighter colour indicates a lower MAE, and the
    darker colour indicates a higher MAE.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图[2](#S5.F2 "Figure 2 ‣ Compatibility with the subsequent GPT-2 network
    blocks. ‣ 5.3 Evaluation Metrics ‣ 5 Experiment ‣ TensorGPT: Efficient Compression
    of the Embedding Layer in LLMs based on the Tensor-Train Decomposition")中可视化了嵌入权重矩阵W的MAE。为了更好的可视化，我们将表示词汇索引的维度折叠成合理的矩阵形状。在图[2](#S5.F2
    "Figure 2 ‣ Compatibility with the subsequent GPT-2 network blocks. ‣ 5.3 Evaluation
    Metrics ‣ 5 Experiment ‣ TensorGPT: Efficient Compression of the Embedding Layer
    in LLMs based on the Tensor-Train Decomposition")中，较浅的颜色表示较低的MAE，较深的颜色表示较高的MAE。'
- en: From the visualization, since the change of colour shading is not stable, it
    can be inferred that the decrease in compression fidelity does not decrease smoothly
    as the TT ranks increase, even for SVD. As for the decomposition of each token
    embedding, we can identify specific areas where the light (lower MAE) lines consistently
    appear, suggesting that some dimensions of the token embeddings are more precise
    in reconstruction. These dimensions may have the potential for further compression.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 从可视化结果来看，由于颜色阴影变化不稳定，可以推断出，压缩保真度的降低并不会随着 TT 排序的增加而平滑降低，即使是 SVD。至于每个 token 嵌入的分解，我们可以识别出一些区域，其中亮线（较低
    MAE）始终出现，表明这些 token 嵌入的某些维度在重建中更为精确。这些维度可能有进一步压缩的潜力。
- en: 6 Conclusion and Future Work
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: In the context of Large Language Models (LLMs), this study has suggested a compression
    approach for the embedding layer. The approach has constructed a power-2 tensor
    Matrix Product State (MPS) format for each token embedding in the embedding weight
    matrix, followed by the further application of the Tensor-Train Decomposition
    (TTD). This approach has demonstrated the advantages of adaptability to the ever-changing
    vocabulary and in a distributed manner, together with the compression of GPT-2
    and has achieved a 3.31$\times$ compression rate with an improved model performance
    in the text reconstruction task.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在大语言模型（LLMs）的背景下，本研究提出了一种用于嵌入层的压缩方法。该方法为嵌入权重矩阵中的每个 token 嵌入构建了一个 2 的幂次的张量矩阵乘积状态（MPS）格式，随后应用了张量列分解（TTD）。这种方法展示了对不断变化的词汇的适应性优势，并以分布式方式进行
    GPT-2 的压缩，取得了 3.31$\times$ 的压缩率，并在文本重建任务中改善了模型性能。
- en: The superiority of Matrix Product State has not been not fully explored in our
    current implementation. An unsolved problem is the integration of MPS into the
    computation process of token embeddings within other encoded information (e.g.
    position or mask encodings) in LLMs, so that the LLMs can run faster, and be able
    to be deployed on lower-end devices. Furthermore, if the generated token embeddings
    are also formatted by MPS, the embedding generation process might be lighter and
    easier to store as well.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们目前的实现中，矩阵乘积状态（Matrix Product State）的优势尚未得到充分探索。一个未解决的问题是将 MPS 集成到其他编码信息（如位置编码或掩码编码）的
    token 嵌入计算过程中，以便使 LLM 更快运行，并能够在低端设备上部署。此外，如果生成的 token 嵌入也使用 MPS 格式化，那么嵌入生成过程可能会更轻便，也更容易存储。
- en: References
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O. & Bowman, S. GLUE:
    A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.
    Proceedings Of The 2018 EMNLP Workshop BlackboxNLP: Analyzing And Interpreting
    Neural Networks For NLP. pp. 353-355 (2018)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O. & Bowman, S. GLUE:
    自然语言理解的多任务基准和分析平台。《2018年EMNLP研讨会BlackboxNLP：分析和解释NLP的神经网络论文集》，pp. 353-355（2018年）'
- en: '[2] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D. & Sutskever, I. Language
    models are unsupervised multitask learners. OpenAI Blog. 1, 9 (2019)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D. & Sutskever, I. 语言模型是无监督的多任务学习者。《OpenAI
    博客》，1，9（2019年）'
- en: '[3] Dolan, B. & Brockett, C. Automatically Constructing a Corpus of Sentential
    Paraphrases. Proceedings Of The Third International Workshop On Paraphrasing.
    (2005)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Dolan, B. & Brockett, C. 自动构建句子释义语料库。《第三届国际释义研讨会论文集》。 （2005年）'
- en: '[4] Chen, P., Si, S., Li, Y., Chelba, C. & Hsieh, C. Groupreduce: Block-wise
    low-rank approximation for neural language model shrinking. Advances In Neural
    Information Processing Systems. 31 (2018)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Chen, P., Si, S., Li, Y., Chelba, C. & Hsieh, C. Groupreduce: 神经语言模型收缩的块状低秩近似。《神经信息处理系统进展》，31（2018年）'
- en: '[5] Yin, C., Acun, B., Wu, C. & Liu, X. Tt-rec: Tensor train compression for
    deep learning recommendation models. Proceedings Of Machine Learning And Systems.
    3 pp. 448-462 (2021)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Yin, C., Acun, B., Wu, C. & Liu, X. Tt-rec: 深度学习推荐模型的张量列压缩。《机器学习与系统会议论文集》，3
    页，448-462（2021年）'
- en: '[6] Kossaifi, J., Panagakis, Y., Anandkumar, A. & Pantic, M. TensorLy: Tensor
    Learning in Python. Journal Of Machine Learning Research. 20, 1-6 (2019), http://jmlr.org/papers/v20/18-277.html'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Kossaifi, J., Panagakis, Y., Anandkumar, A. & Pantic, M. TensorLy: Python中的张量学习。《机器学习研究期刊》，20，1-6（2019年），http://jmlr.org/papers/v20/18-277.html'
- en: '[7] Gage, P. A new algorithm for data compression. C Users Journal. 12, 23-38
    (1994)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Gage, P. 一种新的数据压缩算法。《计算机用户杂志》，12，23-38（1994年）'
- en: '[8] Oseledets, I. Tensor-train decomposition. SIAM Journal On Scientific Computing.
    33, 2295-2317 (2011)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Oseledets, I. 张量列分解。SIAM科学计算期刊。33, 2295-2317 (2011)'
- en: '[9] Perez-Garcia, D., Verstraete, F., Wolf, M. & Cirac, J. Matrix product state
    representations. ArXiv Preprint Quant-ph/0608197. (2006)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Perez-Garcia, D., Verstraete, F., Wolf, M. 和 Cirac, J. 矩阵积态表示。ArXiv预印本
    Quant-ph/0608197. (2006)'
- en: '[10] Dolgov, S. & Savostyanov, D. Alternating minimal energy methods for linear
    systems in higher dimensions. SIAM Journal On Scientific Computing. 36, A2248-A2271
    (2014)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Dolgov, S. 和 Savostyanov, D. 高维线性系统的交替最小能量方法。SIAM科学计算期刊。36, A2248-A2271
    (2014)'
- en: '[11] Cichocki, A. Era of Big Data Processing: A New Approach via Tensor Networks
    and Tensor Decompositions. Proceedings Of The International Workshop On Smart
    Info-Media Systems In Asia. (2014,3)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Cichocki, A. 大数据处理时代：通过张量网络和张量分解的新方法。亚洲智能信息媒体系统国际研讨会论文集。(2014,3)'
- en: '[12] Cichocki, A., Lee, N., Oseledets, I., Phan, A., Zhao, Q. & Mandic, D.
    Tensor networks for dimensionality reduction and large-scale optimization: Part
    1: low-rank tensor decompositions. Foundations And Trends In Machine Learning.
    9, 249-429 (2016)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Cichocki, A., Lee, N., Oseledets, I., Phan, A., Zhao, Q. 和 Mandic, D.
    张量网络用于维度降维和大规模优化：第1部分：低秩张量分解。机器学习基础与趋势。9, 249-429 (2016)'
- en: '[13] Khrulkov, V., Hrinchuk, O., Mirvakhabova, L. & Oseledets, I. Tensorized
    Embedding Layers. Proceedings Of The 2020 Conference On Empirical Methods In Natural
    Language Processing: Findings. pp. 4847-4860 (2020)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Khrulkov, V., Hrinchuk, O., Mirvakhabova, L. 和 Oseledets, I. 张量嵌入层。2020年自然语言处理实证方法会议论文集：发现。pp.
    4847-4860 (2020)'
- en: '[14] Oseledets, I. & Tyrtyshnikov, E. Breaking the curse of dimensionality,
    or how to use SVD in many dimensions. SIAM Journal On Scientific Computing.31,
    3744-3759 (2009)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Oseledets, I. 和 Tyrtyshnikov, E. 打破维度诅咒，或如何在多个维度中使用SVD。SIAM科学计算期刊。31,
    3744-3759 (2009)'
