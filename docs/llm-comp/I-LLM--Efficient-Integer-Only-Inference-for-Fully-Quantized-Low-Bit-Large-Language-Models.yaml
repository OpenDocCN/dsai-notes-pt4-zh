- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:49:10'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:49:10'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language
    Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'I-LLM: 高效的整数-only推理用于完全量化的低位大型语言模型'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.17849](https://ar5iv.labs.arxiv.org/html/2405.17849)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.17849](https://ar5iv.labs.arxiv.org/html/2405.17849)
- en: Xing Hu^(1∗) &Yuan Cheng^(1,2∗†) &Dawei Yang^(1◇) Zhihang Yuan¹ &Jiangyong Yu¹
    &Chen Xu¹ &Sifan Zhou^(1,3†)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xing Hu^(1∗) &Yuan Cheng^(1,2∗†) &Dawei Yang^(1◇) Zhihang Yuan¹ &Jiangyong Yu¹
    &Chen Xu¹ &Sifan Zhou^(1,3†)
- en: ¹Houmo AI
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹Houmo AI
- en: ²Nanjing University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²南京大学
- en: ³Southeast University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³东南大学
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Post-training quantization (PTQ) serves as a potent technique to accelerate
    the inference of large language models (LLMs). Nonetheless, existing works still
    necessitate a considerable number of floating-point (FP) operations during inference,
    including additional quantization and de-quantization, as well as non-linear operators
    such as RMSNorm and Softmax. This limitation hinders the deployment of LLMs on
    the edge and cloud devices. In this paper, we identify the primary obstacle to
    integer-only quantization for LLMs lies in the large fluctuation of activations
    across channels and tokens in both linear and non-linear operations. To address
    this issue, we propose I-LLM, a novel integer-only fully-quantized PTQ framework
    tailored for LLMs. Specifically, (1) we develop Fully-Smooth Block-Reconstruction
    (FSBR) to aggressively smooth inter-channel variations of all activations and
    weights. (2) to alleviate degradation caused by inter-token variations, we introduce
    a novel approach called Dynamic Integer-only MatMul (DI-MatMul). This method enables
    dynamic quantization in full-integer matrix multiplication by dynamically quantizing
    the input and outputs with integer-only operations. (3) we design DI-ClippedSoftmax,
    DI-Exp, and DI-Normalization, which utilize bit shift to execute non-linear operators
    efficiently while maintaining accuracy. The experiment shows that our I-LLM achieves
    comparable accuracy to the FP baseline and outperforms non-integer quantization
    methods. For example, I-LLM can operate at W4A4 with negligible loss of accuracy.
    To our knowledge, we are the first to bridge the gap between integer-only quantization
    and LLMs. We’ve published our code on [anonymous.4open.science](https://anonymous.4open.science/r/I-LLM-F242/),
    aiming to contribute to the advancement of this field.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化（PTQ）作为加速大型语言模型（LLMs）推理的有效技术。然而，现有工作仍需要在推理过程中进行大量的浮点（FP）操作，包括额外的量化和反量化，以及非线性操作如RMSNorm和Softmax。这一限制阻碍了LLMs在边缘和云设备上的部署。本文识别出整数-only量化对于LLMs的主要障碍在于线性和非线性操作中通道和标记之间的激活波动较大。为了解决这一问题，我们提出了I-LLM，一种新型的整数-only完全量化PTQ框架，专为LLMs量身定制。具体来说，(1)
    我们开发了完全平滑块重构（FSBR），以激进地平滑所有激活和权重的通道间变化。 (2) 为了减轻由标记间变化引起的性能下降，我们引入了一种称为动态整数-only矩阵乘法（DI-MatMul）的新方法。该方法通过使用整数-only操作动态量化输入和输出，实现了全整数矩阵乘法中的动态量化。
    (3) 我们设计了DI-ClippedSoftmax、DI-Exp和DI-Normalization，这些方法利用位移来高效地执行非线性操作，同时保持准确性。实验表明，我们的I-LLM在准确性上与FP基线相当，并且优于非整数量化方法。例如，I-LLM可以在W4A4下运行，几乎没有准确性损失。据我们所知，我们是首个弥合整数-only量化与LLMs之间差距的研究者。我们已在[anonymous.4open.science](https://anonymous.4open.science/r/I-LLM-F242/)上发布了我们的代码，旨在为该领域的进步做出贡献。
- en: '^($\Diamond$)^($\Diamond$)footnotetext: Corresponding author^($\ast$)^($\ast$)footnotetext:
    Equal contribution^($\dagger$)^($\dagger$)footnotetext: This work was conducted
    during his internship at Houmo'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '^($\Diamond$)^($\Diamond$)脚注: 通讯作者^($\ast$)^($\ast$)脚注: 平等贡献^($\dagger$)^($\dagger$)脚注:
    本研究是在他在Houmo实习期间进行的'
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have paved the way for general artificial intelligence
    with their remarkable performance across a wide range of tasks. However, the rising
    number of parameters and computing power requirements of LLMs pose significant
    challenges when it comes to deployment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）凭借其在广泛任务中的出色表现，为通用人工智能铺平了道路。然而，LLMs参数数量和计算能力要求的增加在部署时带来了显著挑战。
- en: 'Post-training quantization (PTQ) is a powerful technique employed to accelerate
    the inference process of LLMs. Previous PTQ methods for LLMs have primarily relied
    on simulated quantization (aka. fake quantization) [[38](#bib.bib38), [29](#bib.bib29),
    [35](#bib.bib35), [42](#bib.bib42)], where integer values are typically used for
    inputs/outputs and compute-intensive operations are performed using dequantized
    floating-point (FP) values (as shown in Fig[4](#S3.F4 "Figure 4 ‣ 3 Method ‣ I-LLM:
    Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models")).
    Although this scheme offers benefits in scenarios where data transmission bandwidth
    is limited, it does not effectively reduce computational costs and thus has little
    effect on compute-bound situations. Besides, non-linear operations (e.g., Softmax
    and RMSNorm) often involve complex operations, including transcendental functions
    such as exponential functions and square root functions. These functions are typically
    performed on dedicated FP units and may require multiple iterations for accurate
    computation.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化（PTQ）是一种有效的技术，用于加速LLMs的推理过程。以前用于LLMs的PTQ方法主要依赖于模拟量化（也称为伪量化）[[38](#bib.bib38),
    [29](#bib.bib29), [35](#bib.bib35), [42](#bib.bib42)]，其中整数值通常用于输入/输出，计算密集型操作则使用解量化的浮点值进行（如图[4](#S3.F4
    "图4 ‣ 3 方法 ‣ I-LLM：高效的整数仅推理用于完全量化低位大型语言模型")所示）。尽管这种方案在数据传输带宽有限的情况下提供了好处，但它并没有有效降低计算成本，因此对计算密集型情况几乎没有影响。此外，非线性操作（例如Softmax和RMSNorm）通常涉及复杂操作，包括指数函数和平方根函数等超越函数。这些函数通常在专用的FP单元上执行，可能需要多次迭代才能准确计算。
- en: '![Refer to caption](img/e5c8ebdf6b008ad66093f7910cd71655.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e5c8ebdf6b008ad66093f7910cd71655.png)'
- en: 'Figure 1: Differences in activations of the non-linear operator in LLaMA2-7b
    across the channel/token dimensions.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLaMA2-7b中非线性算子的激活在通道/标记维度上的差异。
- en: '![Refer to caption](img/e77ac4d0467a02c4011944e894062462.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e77ac4d0467a02c4011944e894062462.png)'
- en: 'Figure 2: The output activation distribution of the gated unit in the SwiGLU
    before FSBR (a) and after FSBR (b).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：SwiGLU中门控单元的输出激活分布，FSBR前（a）和FSBR后（b）。
- en: In contrast, integer-only quantization [[11](#bib.bib11), [37](#bib.bib37),
    [25](#bib.bib25), [18](#bib.bib18), [21](#bib.bib21)] utilizes low-precision integer
    arithmetic for all operations, including linear operations (e.g., matrix multiplication)
    and non-linear operations. It enables quantized models to take full advantage
    of fast and efficient integer arithmetic units, resulting in promising speedup
    effects and reduction of latency and power consumption. Additionally, integer-only
    quantization facilitates deployment on popular edge processors specifically designed
    for embedded, mobile, or IoT devices, which often lack dedicated floating point
    units. Examples of such edge processors include ARM Cortex-M [[36](#bib.bib36)],
    GreenWaves GAP-9 [[8](#bib.bib8)], and Google’s Edge TPU [[10](#bib.bib10)]. Note
    that Turing Tensor Cores in GPU server-class devices also have introduced support
    for integer logical units, offering notably lower latency compared to FP arithmetic.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，仅使用整数量化 [[11](#bib.bib11), [37](#bib.bib37), [25](#bib.bib25), [18](#bib.bib18),
    [21](#bib.bib21)] 为所有操作（包括线性操作（例如矩阵乘法）和非线性操作）采用低精度整数算术。这使得量化模型能够充分利用快速高效的整数算术单元，从而实现显著的加速效果，并降低延迟和功耗。此外，仅使用整数量化有助于在专门为嵌入式、移动或物联网设备设计的流行边缘处理器上进行部署，这些设备通常缺乏专用的浮点单元。此类边缘处理器的例子包括ARM
    Cortex-M [[36](#bib.bib36)]、GreenWaves GAP-9 [[8](#bib.bib8)] 和谷歌的Edge TPU [[10](#bib.bib10)]。需要注意的是，GPU服务器级设备中的Turing
    Tensor Cores也引入了对整数逻辑单元的支持，与FP算术相比，具有显著更低的延迟。
- en: 'However, the aforementioned integer-only methods are designed specifically
    for CNNs or small Transformer networks (e.g., Bert [[12](#bib.bib12)] and ViT
    [[7](#bib.bib7)]), which renders them inadequate for LLMs. First, they are incapable
    of straightforwardly supporting the non-linear operators inherent in LLMs, such
    as SwiGLU and RMSNorm. Furthermore, their accuracy on LLMs deteriorates significantly
    (as depicted in Table [4](#S3.F4 "Figure 4 ‣ 3 Method ‣ I-LLM: Efficient Integer-Only
    Inference for Fully-Quantized Low-Bit Large Language Models")), even when those
    non-supported operators are executed in full-precision mode. This is because as
    the model size increasing, the presence of activation outliers in both linear
    and non-linear layers becomes prominent. As illustrated in Fig [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized
    Low-Bit Large Language Models"), Llama2-7B exhibits substantial variations in
    activation magnitude at both the token and channel levels, making previous approaches
    ineffective. Last but not least, these methods are limited to 8-bit quantization,
    whereas LLMs would benefit from lower bit-width quantization (e.g., 4-bit) to
    address the extensive computational requirements and storage demands. Consequently,
    how to accurately perform LLMs with efficient integer-only arithmetic remains
    an unresolved issue that requires further investigation.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，上述仅限整数的方法专门为 CNN 或小型 Transformer 网络（例如，Bert [[12](#bib.bib12)] 和 ViT [[7](#bib.bib7)]）设计，这使得它们不适用于
    LLM。首先，它们无法直接支持 LLM 中固有的非线性运算符，如 SwiGLU 和 RMSNorm。此外，即使在全精度模式下执行这些不支持的运算符，它们在
    LLM 上的准确性也显著下降（如表 [4](#S3.F4 "Figure 4 ‣ 3 Method ‣ I-LLM: Efficient Integer-Only
    Inference for Fully-Quantized Low-Bit Large Language Models") 所示）。这是因为随着模型规模的增加，线性和非线性层中的激活异常值变得显著。如图
    [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ I-LLM: Efficient Integer-Only Inference
    for Fully-Quantized Low-Bit Large Language Models") 所示，Llama2-7B 在 token 和通道层级上展现出激活幅度的显著变化，使得以往的方法失效。最后，这些方法仅限于
    8 位量化，而 LLM 将从更低位宽量化（例如 4 位）中受益，以应对广泛的计算需求和存储要求。因此，如何准确地用高效的仅整数算术执行 LLM 仍然是一个未解决的问题，需要进一步研究。'
- en: 'In this paper, we identify the primary obstacle to integer-only quantization
    for LLMs lies in the large fluctuation of activations across channels and tokens
    in both linear and non-linear operators. To address this issue, we introduce I-LLM,
    a novel integer-only PTQ framework tailored for LLMs: (1) We propose Fully-Smooth
    Block-Reconstruction (FSBR) to harmonize the variance in activation across channels.
    While Omniquant [[29](#bib.bib29)] and Smoothquant [[38](#bib.bib38)] share some
    similarities, they primarily focus on smoothing the activation in serial norm-linear
    and parallel linear-linear operations. We argue that mitigating the disparities
    of all suitable activation-activation and activation-weight pairs of LLMs (see
    Fig.[5](#S3.F5 "Figure 5 ‣ 3.2 Fully-Smooth Block-Reconstruction (FSBR) ‣ 3 Method
    ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language
    Models")) significantly enhances accuracy. For instance, the input of SwiGLU encounters
    numerous disparities on both token-wise and channel-wise dimensions, as depicted
    in Fig.[2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ I-LLM: Efficient Integer-Only Inference
    for Fully-Quantized Low-Bit Large Language Models")-a. To achieve smoothing on
    such a non-linear operator, we decompose SiLU into $x\cdot\sigma(x)$ to apply
    FSBR, and as a result the input becomes more consistent and amenable to quantization,
    as shown in Fig.[2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ I-LLM: Efficient Integer-Only
    Inference for Fully-Quantized Low-Bit Large Language Models")-b. (2) To alleviate
    the degradation resulting from inter-token variations, we present a novel approach
    called Dynamic Integer-only MatMul (DI-MatMul). DI-MatMul facilitates quantization
    on full-integer matrix multiplication by employing integer-only operations to
    quantize the input and outputs dynamically. Traditional static quantization methods,
    which are characterized by their lack of robustness and adaptability, often falter
    when encountering input beyond the calibration set. In contrast, DI-MatMul is
    designed to proactively recognize and adapt to the diverse range of input data,
    thereby reducing quantization errors and enhancing overall model performance.
    (3) For non-linear operators, we design DI-ClippedSoftmax, DI-Exp, and DI-Norm,
    which leverage the efficiency of bit shifting to replace complex math calculations
    while maintaining accuracy. Our contributions are summarized as:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们确定了针对 LLMs 的整数量化的主要障碍在于激活在通道和标记之间的波动，无论是在线性还是非线性操作中。为了解决这个问题，我们引入了 I-LLM，这是一种针对
    LLMs 的新型整数量化 PTQ 框架：(1) 我们提出了完全平滑块重建（FSBR）来协调通道之间的激活方差。虽然 Omniquant [[29](#bib.bib29)]
    和 Smoothquant [[38](#bib.bib38)] 有一些相似之处，但它们主要关注于串行非线性和并行线性操作中的激活平滑。我们认为，通过缓解
    LLMs 所有适用的激活-激活对和激活-权重对的差异（见图 [5](#S3.F5 "图 5 ‣ 3.2 完全平滑块重建（FSBR） ‣ 3 方法 ‣ I-LLM：高效整数量化推理")），可以显著提高准确性。例如，SwiGLU
    的输入在标记维度和通道维度上都遇到许多差异，如图 [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ I-LLM：高效整数量化推理")-a 所示。为了在这样的非线性操作上实现平滑，我们将
    SiLU 分解为 $x\cdot\sigma(x)$ 来应用 FSBR，因此输入变得更加一致，更易于量化，如图 [2](#S1.F2 "图 2 ‣ 1 介绍
    ‣ I-LLM：高效整数量化推理")-b 所示。(2) 为了缓解由于标记间变异导致的退化，我们提出了一种称为动态整数矩阵乘法（DI-MatMul）的新方法。DI-MatMul
    通过采用整数运算动态量化输入和输出，从而促进了全整数矩阵乘法的量化。传统的静态量化方法由于缺乏鲁棒性和适应性，往往在遇到超出校准集的输入时表现不佳。相比之下，DI-MatMul
    旨在主动识别和适应多样化的输入数据，从而减少量化误差并提升整体模型性能。(3) 对于非线性操作，我们设计了 DI-ClippedSoftmax、DI-Exp
    和 DI-Norm，这些方法利用位移的效率来替代复杂的数学计算，同时保持准确性。我们的贡献总结如下：
- en: '1.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We identify the primary obstacle to integer-only quantization for LLMs lies
    in the large fluctuation of activations across channels and tokens in both linear
    and non-linear operators. To address inter-channel variations, we propose FSBR
    to effectively reduces disparities among all suitable activation-activation and
    activation-weight pairs, thereby markedly improving accuracy.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们确定了针对 LLMs 的整数量化的主要障碍在于激活在通道和标记之间的波动，无论是在线性还是非线性操作中。为了解决通道间的变异，我们提出了 FSBR
    来有效减少所有适用的激活-激活对和激活-权重对之间的差异，从而显著提高准确性。
- en: '2.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We attribute the failure of previous integer-only quantization methods on LLMs
    to various range in activation tokens. To tackle this problem, we introduce DI-MatMul,
    which enables dynamic quantization on input and output through full-integer matrix
    multiplication. DI-MatMul proactively adapts to the range of input, minimizing
    quantization errors and improving overall performance.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将之前整数量化方法在LLMs上的失败归因于激活令牌范围的不同。为了解决这个问题，我们介绍了DI-MatMul，它通过全整数矩阵乘法实现输入和输出的动态量化。DI-MatMul主动适应输入的范围，最小化量化误差并提高整体性能。
- en: '3.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We introduce DI-Exp, DI-ClippedSoftmax, and DI-Norm, innovative integer-only
    operators that harness the power of bit shifting to replace complex mathematical
    computations within the non-linear functions of LLMs, without compromising on
    accuracy.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了DI-Exp、DI-ClippedSoftmax和DI-Norm，这些创新的仅整数运算符利用位移的力量来替代LLMs中非线性函数中的复杂数学计算，而不会影响精度。
- en: '4.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: To the best of our knowledge, this work represents the first attempt to utilize
    integer-only quantization for LLMs, enabling their deployment on edge devices
    that lack floating-point capabilities. Experiments demonstrate remarkable accuracy
    when compared to SOTA non-integer quantization techniques, e.g., I-LLM on LLAMA-13b
    achieves an approximate 20% reduction in perplexity.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这项工作代表了首次尝试在LLMs中使用仅整数量化，从而使其能够部署在没有浮点能力的边缘设备上。实验表明，与SOTA非整数量化技术相比，准确度显著提升，例如，I-LLM在LLAMA-13b上实现了大约20%的困惑度降低。
- en: 2 Related Work
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLMs Quantization. LLMs quantization can be broadly categorized into weight-only
    and weight-activation quantization. To alleviate the computational burdens, some
    works [[23](#bib.bib23), [9](#bib.bib9), [3](#bib.bib3), [19](#bib.bib19), [14](#bib.bib14),
    [28](#bib.bib28), [13](#bib.bib13), [6](#bib.bib6)] make efforts in weight-only
    quantization. GPTQ [[9](#bib.bib9)] and QuIP [[3](#bib.bib3)] achieve high compression
    rates by optimizing matrix multiplications operation. AWQ [[19](#bib.bib19)] and
    OWQ [[14](#bib.bib14)] demonstrate performance improvements by accounting for
    the impact of activation outliers on weight quantization. Moreover, works such
    as QLORA [[5](#bib.bib5)], QA-lora [[39](#bib.bib39)] and LoftQ [[17](#bib.bib17)]
    leverage Parameter Efficient Fine-Tuning (PEFT) techniques to achieve weight compression
    with fine-tuning tasks. Different with weight-only quantization methods, towards
    to accelerate the LLMs inference, the weight-activation quantization methods [[35](#bib.bib35),
    [42](#bib.bib42), [34](#bib.bib34), [15](#bib.bib15), [43](#bib.bib43), [44](#bib.bib44)]
    quantize both the weights and activations, including the KV cache. The primary
    challenge in quantizing activations lies in outliers, leading to significant quantization
    errors. To tackle this issue, ZeroQuant [[40](#bib.bib40)] proposes a fine-grained
    hardware-friendly quantization scheme for both weight and activations. SmoothQuant
    [[38](#bib.bib38)] migrates the quantization difficulty from activations to weights
    with a mathematically equivalent transformation (i.e., per-channel scaling). OmniQuant
    [[29](#bib.bib29)] further enhances performance by training the quantization parameters.
    While these methods have mitigated the quantization error, their inference pipelines
    still involve partially FP operations on non-linear operators such as Softmax,
    Normalization, and SiLU. In this study, our focus on achieving Integer-only inference
    for LLMs model using advanced PTQ [[16](#bib.bib16), [33](#bib.bib33), [22](#bib.bib22),
    [47](#bib.bib47)] techniques.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs量化。LLMs量化可以大致分为仅权重量化和权重-激活量化。为了减轻计算负担，一些工作[[23](#bib.bib23), [9](#bib.bib9),
    [3](#bib.bib3), [19](#bib.bib19), [14](#bib.bib14), [28](#bib.bib28), [13](#bib.bib13),
    [6](#bib.bib6)]致力于仅权重量化。GPTQ[[9](#bib.bib9)]和QuIP[[3](#bib.bib3)]通过优化矩阵乘法操作实现了高压缩率。AWQ[[19](#bib.bib19)]和OWQ[[14](#bib.bib14)]通过考虑激活异常值对权重量化的影响来展示性能改进。此外，QLORA[[5](#bib.bib5)]、QA-lora[[39](#bib.bib39)]和LoftQ[[17](#bib.bib17)]等工作利用参数高效微调（PEFT）技术，通过微调任务实现了权重压缩。与仅权重量化方法不同，为了加速LLMs推理，权重-激活量化方法[[35](#bib.bib35),
    [42](#bib.bib42), [34](#bib.bib34), [15](#bib.bib15), [43](#bib.bib43), [44](#bib.bib44)]对权重和激活进行量化，包括KV缓存。量化激活的主要挑战在于异常值，这会导致显著的量化误差。为了解决这个问题，ZeroQuant[[40](#bib.bib40)]提出了一种细粒度硬件友好的权重和激活量化方案。SmoothQuant[[38](#bib.bib38)]通过数学上等效的变换（即每通道缩放）将量化难度从激活迁移到权重。OmniQuant[[29](#bib.bib29)]通过训练量化参数进一步提升性能。虽然这些方法已经减轻了量化误差，但它们的推理管道仍涉及对非线性操作如Softmax、Normalization和SiLU的部分FP操作。本研究的重点是通过先进的PTQ[[16](#bib.bib16),
    [33](#bib.bib33), [22](#bib.bib22), [47](#bib.bib47)]技术实现LLMs模型的仅整数推理。
- en: Integer-only Quantization. Current quantization methods for LLMs often involve
    dequantized FP operations during inference, limiting the utilization of efficient
    low-precision arithmetic units. Integer-only quantization, eliminating dequantization,
    enables complete inference using Integer-only arithmetic, promising enhanced model
    acceleration. Previous approaches [[11](#bib.bib11), [41](#bib.bib41)] leverage
    dyadic arithmetic for Integer-only pipeline on CNNs, but these are tailored for
    linear operations and are unsuitable for non-linear operations in ViTs. Applying
    INT arithmetic solely to linear operations while retaining FP arithmetic for non-linear
    ones maybe a straightforward solution, but this demands custom hardware and introduces
    computational overheads. Advancements include Fully-8bit [[20](#bib.bib20)] and
    I-BERT [[12](#bib.bib12)], which address non-linear operations through employs
    L1 LayerNorm and INT polynomial approximations for the non-linear operations.
    However, these methods face inefficiencies or fail to fully exploit hardware advantages.
    Based on I-BERT[[12](#bib.bib12)], FQ-ViT[[21](#bib.bib21)] extends INT arithmetic
    to part of the operations but overlooks significant non-linear operations like
    GELU. While some methods[[30](#bib.bib30), [48](#bib.bib48)] retain FP arithmetic
    during approximation, they cannot meet the requirement for Integer-only arithmetic.
    Recently, I-ViT[[18](#bib.bib18)] introduces Integer-only quantization for ViTs,
    yet its suitability for LLMs is questionable due to differing data distributions,
    and its computational graph includes partially INT32 precision operations. In
    this paper, we focus on Integer-only inference for LLMs, maintaining the entire
    computational graph at INT8 precision or lower bit (e.g., 6/4-bit), enhancing
    LLMs’ inference efficiency on edge processors.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 仅整数量化。当前 LLM 的量化方法通常涉及在推理过程中进行反量化，这限制了高效低精度算术单元的利用。仅整数量化消除反量化，允许使用仅整数算术进行完整推理，有望提高模型加速。以往的方法[[11](#bib.bib11),
    [41](#bib.bib41)] 利用二元算术对 CNN 进行仅整数流水线，但这些方法针对线性操作而不适用于 ViTs 中的非线性操作。将 INT 算术仅应用于线性操作，而保留
    FP 算术用于非线性操作可能是一个简单的解决方案，但这需要定制硬件并引入计算开销。进展包括 Fully-8bit [[20](#bib.bib20)] 和
    I-BERT [[12](#bib.bib12)]，这些方法通过采用 L1 LayerNorm 和 INT 多项式近似来处理非线性操作。然而，这些方法面临低效或未能充分利用硬件优势的问题。基于
    I-BERT [[12](#bib.bib12)]，FQ-ViT [[21](#bib.bib21)] 扩展了 INT 算术到部分操作，但忽视了如 GELU
    这样的重要非线性操作。虽然一些方法[[30](#bib.bib30), [48](#bib.bib48)] 在近似过程中保留 FP 算术，但它们不能满足仅整数算术的要求。最近，I-ViT
    [[18](#bib.bib18)] 引入了 ViTs 的仅整数量化，但由于数据分布的差异，其对 LLM 的适用性存疑，并且其计算图包含部分 INT32 精度操作。本文重点关注
    LLM 的仅整数推理，保持整个计算图在 INT8 精度或更低位（例如 6/4 位），以提高 LLM 在边缘处理器上的推理效率。
- en: 3 Method
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: '![Refer to caption](img/c32f21d55832b07133489f369a3c4c64.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c32f21d55832b07133489f369a3c4c64.png)'
- en: 'Figure 3: Typical LLM quantization vs. I-LLM. The former requires dequantization
    and involves FP arithmetic, while the latter performs the entire inference using
    integer-only arithmetic.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 典型 LLM 量化与 I-LLM。前者需要反量化并涉及 FP 算术，而后者则使用仅整数算术进行整个推理。'
- en: '![Refer to caption](img/f5713d30765dcc304ad77634c41b8c63.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f5713d30765dcc304ad77634c41b8c63.png)'
- en: 'Figure 4: PPL$\downarrow$ of different PTQ methods on LLaMA family using W8A8\.
    Notably, due to the exceptionally high PPL of I-Bert, a dedicated y-axis has been
    allocated for its representation.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 使用 W8A8 对 LLaMA 家族的不同 PTQ 方法的 PPL$\downarrow$。值得注意的是，由于 I-Bert 的 PPL 极高，为其分配了专用的
    y 轴。'
- en: 'Challenges of Integer-Only Quantization for Large Language Models. Presently,
    integer-only LLMs encounter two primary hurdles: (1) quantizing the activation
    of LLMs, especially those originating from non-linear operators, poses a formidable
    challenge. As evidenced in Fig [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ I-LLM:
    Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models"),
    the divergence in these non-linear activations often surpasses that of linear
    operators, particularly pronounced in models such as LLaMA [[31](#bib.bib31)].
    Previous methods have failed to address these non-linear activations, and straightforwardly
    quantizing non-linear layers may lead to substantial accuracy degradation. (2)
    prior integer-only quantization techniques have overlooked the distinctive traits
    of LLMs, including divergent activation scales and the substantial overhead of
    loading large-scale weights. Even the W8A8 method introduced in I-BERT can lead
    to catastrophic outcomes, as shown in Fig [4](#S3.F4 "Figure 4 ‣ 3 Method ‣ I-LLM:
    Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models"),
    let alone more aggressive quantization methods like W6A6 or W4A4.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '大语言模型的仅整数量化挑战。目前，仅整数LLMs面临两个主要难题：（1）量化LLMs的激活，特别是那些源自非线性操作符的激活，具有巨大的挑战。如图[2](#S1.F2
    "图 2 ‣ 1 引言 ‣ I-LLM: 高效的仅整数推断用于完全量化的低位大语言模型")所示，这些非线性激活的偏差往往超过线性操作符的偏差，特别是在像LLaMA
    [[31](#bib.bib31)]这样的模型中尤为明显。以往的方法未能解决这些非线性激活，直接量化非线性层可能导致显著的准确性下降。（2）以前的仅整数量化技术忽略了LLMs的独特特征，包括激活尺度的分歧和加载大规模权重的巨大开销。即使在I-BERT中引入的W8A8方法也可能导致灾难性的结果，如图[4](#S3.F4
    "图 4 ‣ 3 方法 ‣ I-LLM: 高效的仅整数推断用于完全量化的低位大语言模型")所示，更别提像W6A6或W4A4这样更激进的量化方法。'
- en: 'In this section, we introduce a novel integer-only quantization framework termed
    I-LLM. As illustrated in Fig [5](#S3.F5 "Figure 5 ‣ 3.2 Fully-Smooth Block-Reconstruction
    (FSBR) ‣ 3 Method ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized
    Low-Bit Large Language Models"), this framework incorporates a differentiable
    approach within the Post-Training Quantization (PTQ) paradigm, termed Fully-Smooth
    Block Reconstruction. This method is designed to effectively balance all feasible
    parameter pairs, as elaborated in Section [3.2](#S3.SS2 "3.2 Fully-Smooth Block-Reconstruction
    (FSBR) ‣ 3 Method ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized
    Low-Bit Large Language Models"). Furthermore, we advance the develop of dynamic
    quantization within the integer-only context by introducing DI-MatMul, a dynamic
    integer-only matrix multiplication method, which is elucidated in Section [3.3](#S3.SS3
    "3.3 Dynamic Interger-only MatMul (DI-MatMul) ‣ 3 Method ‣ I-LLM: Efficient Integer-Only
    Inference for Fully-Quantized Low-Bit Large Language Models"). Additionally, we
    detail integer-only non-linear approximations, including DI-ClippedSoftmax, DI-exp,
    and DI-Norm, that are built upon DI-MatMul and are presented in Section [3.4](#S3.SS4
    "3.4 Dynamic Non-Linear Integer-only Operations ‣ 3 Method ‣ I-LLM: Efficient
    Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models"). These
    operators facilitate 8-bit input activations while minimizing accuracy loss.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们介绍了一种新型的仅整数量化框架，称为I-LLM。如图[5](#S3.F5 "图 5 ‣ 3.2 完全平滑块重建（FSBR） ‣ 3 方法
    ‣ I-LLM: 高效的仅整数推断用于完全量化的低位大语言模型")所示，该框架在后训练量化（PTQ）范式中引入了可微分的方法，称为完全平滑块重建。这种方法旨在有效平衡所有可行的参数对，如第[3.2](#S3.SS2
    "3.2 完全平滑块重建（FSBR） ‣ 3 方法 ‣ I-LLM: 高效的仅整数推断用于完全量化的低位大语言模型")节中详细阐述。此外，我们通过引入DI-MatMul，一种动态仅整数矩阵乘法方法，推进了仅整数上下文中的动态量化的发展，这在第[3.3](#S3.SS3
    "3.3 动态整数矩阵乘法（DI-MatMul） ‣ 3 方法 ‣ I-LLM: 高效的仅整数推断用于完全量化的低位大语言模型")节中得到了阐述。此外，我们详细说明了基于DI-MatMul构建的仅整数非线性近似，包括DI-ClippedSoftmax、DI-exp和DI-Norm，这些内容在第[3.4](#S3.SS4
    "3.4 动态非线性整数操作 ‣ 3 方法 ‣ I-LLM: 高效的仅整数推断用于完全量化的低位大语言模型")节中呈现。这些操作符可以处理8位输入激活，同时最小化准确性损失。'
- en: 3.1 Basic Mathematical Notations
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基本数学符号
- en: Matrices are denoted by bold uppercase letters such as $\bm{X}$ denotes element-wise
    division. Other mathematical symbols follow their standard definitions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵用粗体大写字母表示，例如$\bm{X}$表示逐元素除法。其他数学符号遵循其标准定义。
- en: 3.2 Fully-Smooth Block-Reconstruction (FSBR)
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 完全平滑块重建（FSBR）
- en: 'As mention above, to mitigate the issue of non-linear layer activations in
    LLMs being affected by channel and token differences (Fig [2](#S1.F2 "Figure 2
    ‣ 1 Introduction ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized
    Low-Bit Large Language Models")and [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ I-LLM:
    Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models")),
    we propose Fully-Smooth Block-Reconstruction (FSBR). An intuitive method is to
    train a smoothing coefficient for all activations and weights to aid in restoring
    the model’s quantization accuracy, especially the quantization accuracy of non-linear
    operators. Therefore, we consider the activations of all non-linear layers and
    learn the smoothing coefficients for all possible equivalent smoothing transformations
    at the channel level. On the left side of Fig [5](#S3.F5 "Figure 5 ‣ 3.2 Fully-Smooth
    Block-Reconstruction (FSBR) ‣ 3 Method ‣ I-LLM: Efficient Integer-Only Inference
    for Fully-Quantized Low-Bit Large Language Models"), four paradigms for achieving
    inter-channel smoothing during block reconstruction are shown: Parallel Linear-Linear,
    Serial Linear-Norm, Serial Linear-Linear, and NonLinear Act-Smooth. The first
    three smoothing methods are indeed linear, making them easier to implement.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '如上所述，为了缓解 LLM 中非线性层激活受到通道和令牌差异的影响（参见图 [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ I-LLM: 完全量化低比特大语言模型的高效整数推断")
    和 [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ I-LLM: 完全量化低比特大语言模型的高效整数推断")），我们提出了完全平滑块重建 (FSBR)。一种直观的方法是为所有激活和权重训练一个平滑系数，以帮助恢复模型的量化精度，特别是非线性算子的量化精度。因此，我们考虑所有非线性层的激活，并学习所有可能的等效平滑变换的平滑系数，尤其是在通道级别。图
    [5](#S3.F5 "图 5 ‣ 3.2 完全平滑块重建 (FSBR) ‣ 3 方法 ‣ I-LLM: 完全量化低比特大语言模型的高效整数推断") 左侧显示了在块重建过程中实现通道间平滑的四种范式：并行线性-线性、串行线性-归一化、串行线性-线性和非线性激活平滑。前三种平滑方法确实是线性的，因此更易于实现。'
- en: 'However, when addressing non-linear operators (NonLinear Act-Smooth) in LLMs,
    such as the gated activation function (SwiGLU), it becomes challenging to apply
    any linear transformations to them. For convenience, we can represent SwiGLU using
    the following formula:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当处理 LLM 中的非线性算子（NonLinear Act-Smooth），例如门控激活函数 (SwiGLU) 时，应用任何线性变换变得具有挑战性。为了方便，我们可以使用以下公式表示
    SwiGLU：
- en: '|  | $\displaystyle\operatorname{SwiGLU}(\bm{x},\bm{W},\bm{V},\bm{b},\bm{c})$
    |  | (1) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\operatorname{SwiGLU}(\bm{x},\bm{W},\bm{V},\bm{b},\bm{c})$
    |  | (1) |'
- en: '|  |  | $\displaystyle=\bm{x1}\otimes\bm{x2}\otimes\sigma(\bm{x1})$ |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\bm{x1}\otimes\bm{x2}\otimes\sigma(\bm{x1})$ |  |'
- en: 'Where $x\in\mathbb{R}^{ic}$, expressed in the formula as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x\in\mathbb{R}^{ic}$，在公式中表示如下：
- en: '|  | $\displaystyle\operatorname{SwiGLU}(\bm{x},\bm{W},\bm{V},\bm{b},\bm{c})$
    |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\operatorname{SwiGLU}(\bm{x},\bm{W},\bm{V},\bm{b},\bm{c})$
    |  |'
- en: '|  |  | $\displaystyle=\bm{x1^{\prime}}\otimes\bm{x2^{\prime}}\otimes{\sigma^{\prime}}(\bm{x1^{\prime}})\
    $ |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\bm{x1^{\prime}}\otimes\bm{x2^{\prime}}\otimes{\sigma^{\prime}}(\bm{x1^{\prime}})\
    $ |  |'
- en: 'Where $\bm{W}^{\prime}=\bm{W}\otimes\bm{s},\quad\bm{b}^{\prime}=\bm{b}\otimes\bm{s},\quad\bm{V}^{\prime}=\bm{V}\oslash\bm{s},\quad\bm{c}^{\prime}=\bm{c}\oslash\bm{s}$,
    which incurs negligible overhead. Fig [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣
    I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language
    Models") presents the output activation distribution of the gated unit in SwiGLU
    before and after the FSBR. It can be observed that the significant imbalance between
    channels and tokens, as depicted in Fig [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language
    Models")-a, is effectively alleviated after FSBR (Fig [2](#S1.F2 "Figure 2 ‣ 1
    Introduction ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit
    Large Language Models")-b).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\bm{W}^{\prime}=\bm{W}\otimes\bm{s},\quad\bm{b}^{\prime}=\bm{b}\otimes\bm{s},\quad\bm{V}^{\prime}=\bm{V}\oslash\bm{s},\quad\bm{c}^{\prime}=\bm{c}\oslash\bm{s}$，这带来的开销可以忽略不计。图
    [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ I-LLM: 完全量化低比特大语言模型的高效整数推断") 显示了在 FSBR 之前和之后门控单元的输出激活分布。可以观察到，图
    [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ I-LLM: 完全量化低比特大语言模型的高效整数推断")-a 中描绘的通道和令牌之间的显著不平衡，在 FSBR
    之后（图 [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ I-LLM: 完全量化低比特大语言模型的高效整数推断")-b）得到有效缓解。'
- en: '![Refer to caption](img/cf777b942396df06178f760eda2a422a.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/cf777b942396df06178f760eda2a422a.png)'
- en: 'Figure 5: Details of I-LLM in a transformer block. The left side of the figure
    illustrates various paradigms for channel-wise smoothing during the FSBR process.
    The right side depicts the integer-only execution pipeline for both linear operators,
    such as matrix multiplication (MatMul), and non-linear operators.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：变换器块中I-LLM的详细信息。图的左侧展示了FSBR过程中的通道平滑的各种范式。右侧则描绘了线性算子（如矩阵乘法（MatMul））和非线性算子的仅整数执行管道。
- en: 'It is worth noting that SmoothQuant[[38](#bib.bib38)] and OmniQuant[[29](#bib.bib29)]
    are subsets of FSBR. FSBR encompasses various previous equivalent quantization
    algorithms, providing more possibilities for optimizing the distribution of weights
    and activations. Through mutual optimization across channels, the network demonstrates
    improved robustness to quantization, as shown in Table [5](#S4.T5 "Table 5 ‣ 4.2
    Ablation Study ‣ 4 Experiments ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized
    Low-Bit Large Language Models").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，SmoothQuant[[38](#bib.bib38)]和OmniQuant[[29](#bib.bib29)]是FSBR的子集。FSBR涵盖了各种先前的等效量化算法，为优化权重和激活的分布提供了更多可能性。通过在通道间的相互优化，网络表现出对量化的更强鲁棒性，如表[5](#S4.T5
    "表 5 ‣ 4.2 消融研究 ‣ 4 实验 ‣ I-LLM：仅整数推理的全量化低位大语言模型")所示。
- en: 3.3 Dynamic Interger-only MatMul (DI-MatMul)
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 动态整数-only MatMul (DI-MatMul)
- en: 'The dynadic arithmetic pipeline is an efficient approach that implements floating-point
    operations using integer bit-shifts, allowing linear operations to be performed
    solely through integer arithmetic. Initially developed for convolutional neural
    networks [[11](#bib.bib11)], this technique has been extended to Vision Transformers
    by I-ViT [[18](#bib.bib18)]. However, these methods primarily focus on static
    quantization, where the quantization parameters of the model’s activation are
    fixed during runtime. In the context of LLMs, even after applying inter-channel
    smoothing, there still exists considerable variation in activation on a token-wise
    basis, as shown in Fig. [6](#A1.F6 "Figure 6 ‣ A.3 Fully Results ‣ Appendix A
    Appendix ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit
    Large Language Models") in Appendix [A](#A1 "Appendix A Appendix ‣ I-LLM: Efficient
    Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models"). Employing
    static per-token quantization can result in significant quantization errors and
    subsequent degradation in accuracy, as depicted in the Fig. [4](#S3.F4 "Figure
    4 ‣ 3 Method ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit
    Large Language Models"). Therefore, implementing dynamic quantization for inputs
    and outputs while adhering to Integer-only constraints poses a significant challenge.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 动态算术管道是一种高效的方法，它通过使用整数位移实现浮点运算，使得线性操作仅通过整数运算来完成。最初，这种技术是为卷积神经网络[[11](#bib.bib11)]开发的，后来被I-ViT扩展到了视觉变换器[[18](#bib.bib18)]。然而，这些方法主要集中在静态量化上，其中模型激活的量化参数在运行时是固定的。在LLMs的背景下，即使应用了通道间平滑处理，激活的标记级别仍然存在相当大的变化，如附录[A](#A1
    "附录 A 附录 ‣ I-LLM：仅整数推理的全量化低位大语言模型")中图[6](#A1.F6 "图 6 ‣ A.3 完全结果 ‣ 附录 A 附录 ‣ I-LLM：仅整数推理的全量化低位大语言模型")所示。采用静态每标记量化可能会导致显著的量化误差和随后的准确性下降，如图[4](#S3.F4
    "图 4 ‣ 3 方法 ‣ I-LLM：仅整数推理的全量化低位大语言模型")所示。因此，在遵守仅整数约束的同时实现输入和输出的动态量化是一项重大挑战。
- en: 'We propose a novel DI-MatMul approach, where the matrix multiplication is formulated
    as $\bm{Y^{I}},s_{y},zp^{I}=\mathcal{M}(s_{1},zp_{1}^{I},\bm{X^{I}_{1}},s_{2},zp_{2}^{I},\bm{X^{I}_{2}})$
    are 8-bit integers. Consequently, the entire matrix multiplication can be expressed
    as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的DI-MatMul方法，其中矩阵乘法被公式化为$\bm{Y^{I}},s_{y},zp^{I}=\mathcal{M}(s_{1},zp_{1}^{I},\bm{X^{I}_{1}},s_{2},zp_{2}^{I},\bm{X^{I}_{2}})$是8位整数。因此，整个矩阵乘法可以表示为：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'For a single matrix multiplication operation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个矩阵乘法操作：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: 'The intermediate result of the matrix multiplication is denoted as $\bm{P^{I}}$.
    By applying Eq [15](#A1.E15 "In A.1 Quantization Preliminaries ‣ Appendix A Appendix
    ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language
    Models") and preliminary Eq [2](#S3.E2 "In 3.3 Dynamic Interger-only MatMul (DI-MatMul)
    ‣ 3 Method ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit
    Large Language Models"), we obtain the following approximation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法的中间结果记作$\bm{P^{I}}$。通过应用公式 [15](#A1.E15 "在 A.1 量化预备知识 ‣ 附录 A 附录 ‣ I-LLM：完全量化低比特大语言模型的高效整数推理")和初步公式
    [2](#S3.E2 "在 3.3 动态整数-only 矩阵乘法 (DI-MatMul) ‣ 3 方法 ‣ I-LLM：完全量化低比特大语言模型的高效整数推理")，我们得到以下近似值：
- en: '|  | $1$2 |  | (4) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: 'where $p^{I}_{\max}$. To obtain the quantization scale of the output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p^{I}_{\max}$。获取输出的量化尺度：
- en: '|  | $1$2 |  | (5) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: 'Obtaining the optimal values for $m^{I}_{y}$, and subsequently solve for the
    remaining variables as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 获取$m^{I}_{y}$的最优值，然后解算剩余变量如下：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: '|  | $\displaystyle m^{I}_{y}=\left\lfloor\frac{(p^{I}_{\max}-p^{I}_{\min})\cdot
    m^{I}_{x1}\cdot m^{I}_{x2}}{n^{I}}\gg(k^{I}_{1}+k^{I}_{2}-k^{I}_{y})\right\rceil$
    |  | (7) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle m^{I}_{y}=\left\lfloor\frac{(p^{I}_{\max}-p^{I}_{\min})\cdot
    m^{I}_{x1}\cdot m^{I}_{x2}}{n^{I}}\gg(k^{I}_{1}+k^{I}_{2}-k^{I}_{y})\right\rceil$
    |  | (7) |'
- en: '|  | $\displaystyle z^{I}_{y}=\left\lfloor\frac{-p^{I}_{\min}\cdot n^{I}}{p^{I}_{\max}-p^{I}_{\min}}\right\rceil,\quad\bm{Y^{I}}=\left\lfloor\frac{(\bm{P^{I}}-p^{I}_{\min})\cdot
    n^{I}}{p^{I}_{\max}-p^{I}_{\min}}\right\rceil$ |  | (8) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z^{I}_{y}=\left\lfloor\frac{-p^{I}_{\min}\cdot n^{I}}{p^{I}_{\max}-p^{I}_{\min}}\right\rceil,\quad\bm{Y^{I}}=\left\lfloor\frac{(\bm{P^{I}}-p^{I}_{\min})\cdot
    n^{I}}{p^{I}_{\max}-p^{I}_{\min}}\right\rceil$ |  | (8) |'
- en: The implementation of $\left\lfloor\operatorname{log_{2}}(\cdot)\right\rfloor$.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 $\left\lfloor\operatorname{log_{2}}(\cdot)\right\rfloor$。
- en: 3.4 Dynamic Non-Linear Integer-only Operations
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 动态非线性整数-only 操作
- en: 3.4.1 Dynamic Interger-only Clipped Softmax & Dynamic Integer-only Exponent
    function
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 动态整数-only 截断 Softmax & 动态整数-only 指数函数
- en: 'The Softmax function converts attention scores into a probability distribution.
    It can be represented as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 函数将注意力分数转换为概率分布。它可以表示如下：
- en: '|  | $1$2 |  | (9) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: 'We introduce DI-ClippedSoftmax to avoid quantizing the entire input range.
    As shown in Fig [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ I-LLM: Efficient Integer-Only
    Inference for Fully-Quantized Low-Bit Large Language Models"), the activation
    inputs to the Softmax function in LLMs can exhibit significant outliers, with
    their magnitudes being proportional to the number of tokens. Fortunately, the
    DI-MatMul approach proposed in Section [3.3](#S3.SS3 "3.3 Dynamic Interger-only
    MatMul (DI-MatMul) ‣ 3 Method ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized
    Low-Bit Large Language Models") allows for the differences between tokens to be
    handled individually. However, simply applying 8-bit quantization to the Softmax
    inputs would result in substantial precision loss, as demonstrated by the ablation
    studies in Section [5](#S4.T5 "Table 5 ‣ 4.2 Ablation Study ‣ 4 Experiments ‣
    I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language
    Models"). Starting from the observation that the value of the exponential function
    at $-20$. Combining this with Eq [4](#S3.E4 "In 3.3 Dynamic Interger-only MatMul
    (DI-MatMul) ‣ 3 Method ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized
    Low-Bit Large Language Models"), the entire truncation process can be expressed
    as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入 DI-ClippedSoftmax 以避免量化整个输入范围。如图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ I-LLM：完全量化低比特大语言模型的高效整数推理")所示，LLM
    中 Softmax 函数的激活输入可能会出现显著的异常值，其幅度与令牌数量成正比。幸运的是，第三节 [3.3](#S3.SS3 "3.3 动态整数-only
    矩阵乘法 (DI-MatMul) ‣ 3 方法 ‣ I-LLM：完全量化低比特大语言模型的高效整数推理") 中提出的 DI-MatMul 方法允许逐个处理令牌之间的差异。然而，简单地对
    Softmax 输入应用 8 位量化将导致显著的精度损失，正如第五节 [5](#S4.T5 "表 5 ‣ 4.2 消融研究 ‣ 4 实验 ‣ I-LLM：完全量化低比特大语言模型的高效整数推理")
    中的消融研究所示。从观察到指数函数在 $-20$ 的值开始。结合公式 [4](#S3.E4 "在 3.3 动态整数-only 矩阵乘法 (DI-MatMul)
    ‣ 3 方法 ‣ I-LLM：完全量化低比特大语言模型的高效整数推理")，整个截断过程可以表示如下：
- en: '|  | $1$2 |  | (10) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (10) |'
- en: Therefore, regardless of the dynamic range, the length of our quantization range
    will never exceed $c$, ensuring that the maximum quantization error does not exceed
    0.047.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无论动态范围如何，我们的量化范围长度永远不会超过$c$，从而确保最大量化误差不超过0.047。
- en: 'We also propose Dynamic Interger-only Exponent function(DI-Exp), an exponential
    computation method that performs non-linear calculations of the exponential function
    using only shift operations. For a dynamically quantized input composed of $\bm{x^{I}},m^{I}_{x},k_{x}^{I}$
    is already the result after subtracting the maximum value, the computation of
    the exponential can be expressed as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提出了动态整数-only 指数函数（DI-Exp），一种通过仅使用移位操作进行非线性计算的指数函数方法。对于由 $\bm{x^{I}},m^{I}_{x},k_{x}^{I}$
    组成的动态量化输入，这已经是减去最大值后的结果，指数计算可以表示为：
- en: '|  | $1$2 |  | (11) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (11) |'
- en: We let $s=\frac{\log_{2}e}{2^{k_{x}^{I}}}$
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们令 $s=\frac{\log_{2}e}{2^{k_{x}^{I}}}$
- en: 'where $q^{I}=-\left\lfloor\frac{\bm{x^{I}}}{s^{I}}\right\rfloor,\quad r^{I}=\bm{x^{I}}\%s^{I}$
    Finally, we obtain:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $q^{I}=-\left\lfloor\frac{\bm{x^{I}}}{s^{I}}\right\rfloor,\quad r^{I}=\bm{x^{I}}\%s^{I}$
    最终，我们得到：
- en: '|  | $\displaystyle\operatorname{e}^{\bm{x^{I}}\cdot\left(m^{I}_{x}\gg k^{I}_{x}\right)}\approx\left(1-\frac{r^{I}}{2\cdot
    s^{I}}\right)\gg q^{I}$ |  | (12) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\operatorname{e}^{\bm{x^{I}}\cdot\left(m^{I}_{x}\gg k^{I}_{x}\right)}\approx\left(1-\frac{r^{I}}{2\cdot
    s^{I}}\right)\gg q^{I}$ |  | (12) |'
- en: 'In DI-Exp, nonlinearity is achieved using only shift operations, which improves
    computational efficiency compared to complex methods such as quadratic fitting
    and Taylor series expansion. Only the calculation of $s^{I}$ and the linear interpolation
    within a small interval introduce errors, while other computations remain equivalent
    to the original. Detailed implementation of DI-Exp is in Algorithm[1](#alg1 "Algorithm
    1 ‣ A.2 Dynamic Interger-only Algorithms ‣ Appendix A Appendix ‣ I-LLM: Efficient
    Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models").'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '在 DI-Exp 中，非线性通过仅使用移位操作来实现，与二次拟合和泰勒级数展开等复杂方法相比，提高了计算效率。仅 $s^{I}$ 的计算和在小区间内的线性插值引入误差，而其他计算与原始计算等效。DI-Exp
    的详细实现见算法 [1](#alg1 "Algorithm 1 ‣ A.2 Dynamic Interger-only Algorithms ‣ Appendix
    A Appendix ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit
    Large Language Models")。'
- en: 3.4.2 Dynamic Interger-only Normalization & Dynamic Interger-only SwiGLU
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 动态整数-only 归一化与动态整数-only SwiGLU
- en: 'DI-Norm. LayerNorm and RMSNorm are commonly used normalization methods in LLMs.
    RMSNorm represents a lightweight improvement over LayerNorm, as it does not necessitate
    the computation of means and biases, thereby reducing computational complexity.
    Both RMSNorm and LayerNorm exhibit similar characteristics, with significant fluctuations
    at the channel level. As depicted in the Fig [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language
    Models"), RMSNorm often exhibits more pronounced differences between channels
    due to the absence of a centering step. We propose Dynamic Integer-only Normalization
    (DI-Norm) to adapt to fluctuations between channels. When conducting FSBR, we
    perform per-channel quantization on the inputs of LayerNorm and RMSNorm. During
    inference, while computing the mean is straightforward, calculating the variance
    requires an appropriate root mean square function. Unlike the Newton’s method
    used in I-BERT, to ensure consistency between inference and training, we employ
    a bit-wise check method as mentioned in the Algorithm [4](#alg4 "Algorithm 4 ‣
    A.2 Dynamic Interger-only Algorithms ‣ Appendix A Appendix ‣ I-LLM: Efficient
    Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models") to
    achieve higher precision.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 'DI-Norm. LayerNorm 和 RMSNorm 是 LLMs 中常用的归一化方法。RMSNorm 相对于 LayerNorm 是一种轻量级的改进，因为它不需要计算均值和偏差，从而减少了计算复杂度。RMSNorm
    和 LayerNorm 显示出类似的特性，在通道级别上有显著的波动。如图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ I-LLM:
    Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models")
    所示，由于缺乏中心化步骤，RMSNorm 通常在通道之间表现出更明显的差异。我们提出了动态整数-only 归一化（DI-Norm）来适应通道之间的波动。在进行
    FSBR 时，我们对 LayerNorm 和 RMSNorm 的输入进行逐通道量化。在推理过程中，计算均值是直接的，而计算方差需要适当的均方根函数。与 I-BERT
    中使用的牛顿方法不同，为了确保推理和训练的一致性，我们使用了算法 [4](#alg4 "Algorithm 4 ‣ A.2 Dynamic Interger-only
    Algorithms ‣ Appendix A Appendix ‣ I-LLM: Efficient Integer-Only Inference for
    Fully-Quantized Low-Bit Large Language Models") 中提到的按位检查方法以实现更高的精度。'
- en: 'DI-SwiGLU. During the block reconstruction stage, we decompose the SiLU activation
    function of SwiGLU into $\operatorname{SiLU}(\bm{x})=\bm{x}\cdot\sigma(\bm{x})$
    to achieve non-linear smoothing. As described in Algorithm [3](#alg3 "Algorithm
    3 ‣ A.2 Dynamic Interger-only Algorithms ‣ Appendix A Appendix ‣ I-LLM: Efficient
    Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models"), we
    implement the Dynamic Integer-only SwiGLU (DI-SwiGLU), consisting of a single
    sigmoid nonlinearity and two multiplication operations, where the sigmoid implementation
    involves multiple invocations of our proposed DI-Exp operator.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 'DI-SwiGLU。在块重建阶段，我们将SwiGLU的SiLU激活函数分解为$\operatorname{SiLU}(\bm{x})=\bm{x}\cdot\sigma(\bm{x})$以实现非线性平滑。如算法[3](#alg3
    "Algorithm 3 ‣ A.2 Dynamic Integer-only Algorithms ‣ Appendix A Appendix ‣ I-LLM:
    Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models")中所述，我们实现了动态整数仅SwiGLU
    (DI-SwiGLU)，它由一个sigmoid非线性和两个乘法操作组成，其中sigmoid实现涉及多次调用我们提出的DI-Exp操作符。'
- en: 4 Experiments
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: Implementation Details. I-LLM ensures that the activation of all non-linear
    operators remains at 8 bits, while the weights and activations of linear operators
    are determined by the current quantization configuration, such as W4A8\. Consistent
    with other methods, we employ 128 samples for reconstruction. During the reconstruction
    phase, we maintain that the input to Softmax is not quantized and ensure that
    all smoothing coefficients maintain a common learning rate of $5\times 10^{-3}$.
    After the reconstruction, all operators will be replaced with respective versions
    supporting dynamic integer-only inference. All experiments are conducted on Nvidia
    A6000 GPU.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 实施细节。I-LLM确保所有非线性操作符的激活保持在8位，而线性操作符的权重和激活由当前的量化配置决定，例如W4A8\。与其他方法一致，我们采用128个样本进行重建。在重建阶段，我们保持Softmax的输入未进行量化，并确保所有平滑系数保持相同的学习率$5\times
    10^{-3}$。重建后，所有操作符将被替换为支持动态整数仅推理的相应版本。所有实验均在Nvidia A6000 GPU上进行。
- en: Models & Evaluation Metric. We conduct experiments on several commonly used
    open-source LLMs, including OPT [[46](#bib.bib46)], LLaMA [[31](#bib.bib31)],
    and LLaMA2 [[32](#bib.bib32)]. Additionally, we also evaluated the recently impressive
    LLaMA3-8B model. For the sake of comparison, we tested the impact of quantization
    on Perperxity on two of the most commonly used datasets WikiText2[[24](#bib.bib24)]
    and C4[[26](#bib.bib26)]. Moreover, accuracy is evaluated in zero-short tasks
    including PIQA [[1](#bib.bib1)], ARC[[2](#bib.bib2)], BoolQ [[4](#bib.bib4)],
    HellaSwag [[45](#bib.bib45)], Winogrande[[27](#bib.bib27)].
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 模型与评估指标。我们在几个常用的开源LLM上进行了实验，包括OPT [[46](#bib.bib46)]、LLaMA [[31](#bib.bib31)]
    和 LLaMA2 [[32](#bib.bib32)]。此外，我们还评估了最近表现出色的LLaMA3-8B模型。为了比较，我们测试了量化对Perperxity的影响，使用了两个最常用的数据集WikiText2[[24](#bib.bib24)]
    和 C4[[26](#bib.bib26)]。此外，还在零样本任务中评估了准确性，包括PIQA [[1](#bib.bib1)]、ARC[[2](#bib.bib2)]、BoolQ
    [[4](#bib.bib4)]、HellaSwag [[45](#bib.bib45)] 和 Winogrande[[27](#bib.bib27)]。
- en: 4.1 Quantitative Results
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 定量结果
- en: Notably, our work is the first to address integer-only quantization for LLMs,
    whereas all the methods we compare against in our experiments are not integer-only
    except I-Bert. Although this may seem somewhat unfair for us, I-LLM still demonstrates
    the superiority through its remarkable performance.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们的工作是首个解决LLMs的整数仅量化的问题，而我们在实验中比较的所有方法，除I-Bert外，都不是整数仅方法。尽管这对我们来说可能显得有些不公平，但I-LLM通过其显著的性能仍然展示了其优越性。
- en: 'Fig [4](#S3.F4 "Figure 4 ‣ 3 Method ‣ I-LLM: Efficient Integer-Only Inference
    for Fully-Quantized Low-Bit Large Language Models") illustrates the efficacy of
    our 8-bit quantization technique on widely adopted large-scale language models.
    Despite SmoothQuant’s prior success with 8-bit quantization, our approach demonstrates
    performance for each model that is closer to floating-point precision. This suggests
    that even with low-bit quantization using integer-only conditions (e.g., 8-bit),
    we can achieve performance comparable to floating-point representation. These
    findings strongly validate the effectiveness of our proposed I-LLM, as it enables
    integer-only operators to yield results highly akin to those obtained through
    floating-point operations, using simple arithmetic and integer bit-shifting.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S3.F4 "图 4 ‣ 3 方法 ‣ I-LLM：用于完全量化低比特大型语言模型的高效整数仅推理") 说明了我们 8 位量化技术在广泛应用的大规模语言模型上的有效性。尽管
    SmoothQuant 在 8 位量化方面之前取得了成功，但我们的方法展示了每个模型的性能更接近浮点精度。这表明，即使在仅使用整数条件（例如 8 位）的低比特量化下，我们也能实现与浮点表示相媲美的性能。这些发现强有力地验证了我们提出的
    I-LLM 的有效性，因为它使整数仅操作符能够产生与浮点操作获得的结果高度相似的结果，使用简单的算术和整数位移操作。
- en: 'Table 1: Quantitative weight-activation quantization PPL($\downarrow$) results
    of I-LLM. We report WikiText2 and C4 perplexity of LLaMA Family in this table.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：I-LLM 的量化权重-激活量化 PPL($\downarrow$) 结果。我们在此表中报告了 LLaMA 家族的 WikiText2 和 C4
    困惑度。
- en: '| #Bits | Method | LLaMA-7B | LLaMA-13B | LLaMA-30B | LLaMA2-7B | LLaMA2-13B
    | LLaMA3-8b |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| #Bits | 方法 | LLaMA-7B | LLaMA-13B | LLaMA-30B | LLaMA2-7B | LLaMA2-13B |
    LLaMA3-8b |'
- en: '| WikiText2 | C4 | WikiText2 | C4 | WikiText2 | C4 | WikiText2 | C4 | WikiText2
    | C4 | WikiText2 | C4 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 | C4 | WikiText2 | C4 | WikiText2 | C4 | WikiText2 | C4 | WikiText2
    | C4 | WikiText2 | C4 |'
- en: '| FP16 | - | 5.68 | 7.08 | 5.09 | 6.61 | 4.10 | 5.98 | 5.47 | 6.97 | 4.88 |
    6.46 | 6.14 | 8.88 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 5.68 | 7.08 | 5.09 | 6.61 | 4.10 | 5.98 | 5.47 | 6.97 | 4.88 |
    6.46 | 6.14 | 8.88 |'
- en: '| W6A6 | SmoothQuant | 6.03 | 7.47 | 5.42 | 6.97 | 4.55 | 6.34 | 6.2 | 7.76
    | 5.18 | 6.76 | 7.08 | 10.16 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | SmoothQuant | 6.03 | 7.47 | 5.42 | 6.97 | 4.55 | 6.34 | 6.2 | 7.76
    | 5.18 | 6.76 | 7.08 | 10.16 |'
- en: '| OmniQuant | 5.96 | 7.43 | 5.28 | 6.84 | 4.38 | 6.22 | 5.87 | 7.48 | 5.14
    | 6.74 | 6.97 | 10.08 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 5.96 | 7.43 | 5.28 | 6.84 | 4.38 | 6.22 | 5.87 | 7.48 | 5.14
    | 6.74 | 6.97 | 10.08 |'
- en: '| I-LLM | 5.84 | 7.32 | 5.23 | 6.79 | 4.32 | 6.25 | 5.68 | 7.27 | 5.10 | 6.74
    | 6.61 | 9.77 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| I-LLM | 5.84 | 7.32 | 5.23 | 6.79 | 4.32 | 6.25 | 5.68 | 7.27 | 5.10 | 6.74
    | 6.61 | 9.77 |'
- en: '| W4A4 | SmoothQuant | 22.25 | 32.32 | 40.05 | 47.18 | 192.40 | 122.38 | 83.12
    | 77.27 | 35.88 | 43.19 | 418.88 | 312.86 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | SmoothQuant | 22.25 | 32.32 | 40.05 | 47.18 | 192.40 | 122.38 | 83.12
    | 77.27 | 35.88 | 43.19 | 418.88 | 312.86 |'
- en: '| OmniQuant | 11.26 | 14.51 | 10.87 | 13.78 | 10.33 | 12.49 | 14.26 | 18.02
    | 12.30 | 14.55 | 437.88 | 315.69 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 11.26 | 14.51 | 10.87 | 13.78 | 10.33 | 12.49 | 14.26 | 18.02
    | 12.30 | 14.55 | 437.88 | 315.69 |'
- en: '| AffineQuant | 10.28 | 13.64 | 10.32 | 13.44 | 9.35 | 11.58 | 12.69 | 15.76
    | 11.45 | 13.97 | - | - |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 10.28 | 13.64 | 10.32 | 13.44 | 9.35 | 11.58 | 12.69 | 15.76
    | 11.45 | 13.97 | - | - |'
- en: '| I-LLM | 9.10 | 12.33 | 7.99 | 10.96 | 7.24 | 9.85 | 10.44 | 12.92 | 9.76
    | 12.57 | 21.19 | 30.9 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| I-LLM | 9.10 | 12.33 | 7.99 | 10.96 | 7.24 | 9.85 | 10.44 | 12.92 | 9.76
    | 12.57 | 21.19 | 30.9 |'
- en: 'Table 2: Quantitative weight-activation quantization PPL($\downarrow$) results
    on OPT Family of I-LLM.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：I-LLM 的 OPT 家族的量化权重-激活量化 PPL($\downarrow$) 结果。
- en: '#Bits Method OPT-6.7B OPT-13B OPT-30B WikiText2 C4 WikiText2 C4 WikiText2 C4
    FP16 - 10.86 11.74 10.13 11.20 9.56 10.69 W6A6 SmoothQuant 11.34 12.14 10.56 11.40
    9.67 10.81 RPTQ 11.19 12.08 11.00 11.68 10.22 11.73 OmniQuant 10.96 11.81 10.21
    11.27 9.62 10.76 I-LLM 10.94 11.82 10.17 11.90 9.72 10.83 W4A4 SmoothQuant 1.8e4
    1.5e4 7.4e3 5.6e3 1.2e4 8.3e3 RPTQ 12.00 12.85 12.74 14.71 11.15 13.48 OmniQuant
    12.24 13.56 11.65 13.46 10.60 11.90 I-LLM 12.20 12.21 11.45 13.41 10.53 11.66'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#Bits 方法 OPT-6.7B OPT-13B OPT-30B WikiText2 C4 WikiText2 C4 WikiText2 C4 FP16
    - 10.86 11.74 10.13 11.20 9.56 10.69 W6A6 SmoothQuant 11.34 12.14 10.56 11.40
    9.67 10.81 RPTQ 11.19 12.08 11.00 11.68 10.22 11.73 OmniQuant 10.96 11.81 10.21
    11.27 9.62 10.76 I-LLM 10.94 11.82 10.17 11.90 9.72 10.83 W4A4 SmoothQuant 1.8e4
    1.5e4 7.4e3 5.6e3 1.2e4 8.3e3 RPTQ 12.00 12.85 12.74 14.71 11.15 13.48 OmniQuant
    12.24 13.56 11.65 13.46 10.60 11.90 I-LLM 12.20 12.21 11.45 13.41 10.53 11.66'
- en: 'As shown in Table [1](#S4.T1 "Table 1 ‣ 4.1 Quantitative Results ‣ 4 Experiments
    ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language
    Models") and Table [2](#S4.T2 "Table 2 ‣ 4.1 Quantitative Results ‣ 4 Experiments
    ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language
    Models"), we report the perplexity performance of I-LLM on the C4 and WikiText2
    datasets. As depicted in these tables, I-LLM consistently surpasses previous methods
    across a diverse array of LLM families (LLaMA-1, LLaMA-2, LLaMA-3, OPT) and varying
    levels of precision. Particularly noteworthy is its performance under the W4A4
    setting, where our proposed method achieves perplexity values that are consistently
    10% to 30% lower than those attained by state-of-the-art methods.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如表格 [1](#S4.T1 "表 1 ‣ 4.1 定量结果 ‣ 4 实验 ‣ I-LLM：针对完全量化低位大语言模型的高效整数仅推理") 和表格 [2](#S4.T2
    "表 2 ‣ 4.1 定量结果 ‣ 4 实验 ‣ I-LLM：针对完全量化低位大语言模型的高效整数仅推理") 所示，我们报告了 I-LLM 在 C4 和 WikiText2
    数据集上的困惑度表现。从这些表格中可以看出，I-LLM 一贯超越了多种 LLM 家族（LLaMA-1、LLaMA-2、LLaMA-3、OPT）的先前方法，并且在不同的精度水平下表现突出。特别值得注意的是，在
    W4A4 设置下，我们提出的方法实现的困惑度值始终比最先进的方法低 10% 到 30%。
- en: 'Table 3: The performance of various methods for 4-bit and 6-bit quantization
    on the LLaMA family models across six zero-shot datasets.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在六个零样本数据集上，针对 LLaMA 家族模型的 4 位和 6 位量化的各种方法性能。
- en: '| LLaMA / Acc($\uparrow$) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA / Acc($\uparrow$) |'
- en: '| LLaMA-7B | FP16 | - | 77.47 | 52.48 | 41.46 | 73.08 | 73.00 | 67.07 | 64.09
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | FP16 | - | 77.47 | 52.48 | 41.46 | 73.08 | 73.00 | 67.07 | 64.09
    |'
- en: '| W6A6 | SmoothQuant | 76.75 | 51.64 | 39.88 | 71.75 | 71.67 | 65.03 | 62.81
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | SmoothQuant | 76.75 | 51.64 | 39.88 | 71.75 | 71.67 | 65.03 | 62.81
    |'
- en: '| W6A6 | OmniQuant | 77.09 | 51.89 | 40.87 | 72.53 | 71.61 | 65.03 | 63.17
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | OmniQuant | 77.09 | 51.89 | 40.87 | 72.53 | 71.61 | 65.03 | 63.17
    |'
- en: '| W6A6 | I-LLM | 76.99 | 52.66 | 40.78 | 72.94 | 71.31 | 65.67 | 63.39 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | I-LLM | 76.99 | 52.66 | 40.78 | 72.94 | 71.31 | 65.67 | 63.39 |'
- en: '| W4A4 | SmoothQuant | 49.80 | 30.40 | 25.80 | 49.10 | 27.40 | 48.00 | 38.41
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | SmoothQuant | 49.80 | 30.40 | 25.80 | 49.10 | 27.40 | 48.00 | 38.41
    |'
- en: '| W4A4 | LLM-QAT | 51.50 | 27.90 | 23.90 | 61.30 | 31.10 | 51.90 | 41.27 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | LLM-QAT | 51.50 | 27.90 | 23.90 | 61.30 | 31.10 | 51.90 | 41.27 |'
- en: '| W4A4 | LLM-QAT+SQ | 55.90 | 35.50 | 26.40 | 62.40 | 47.80 | 50.60 | 46.43
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | LLM-QAT+SQ | 55.90 | 35.50 | 26.40 | 62.40 | 47.80 | 50.60 | 46.43
    |'
- en: '| W4A4 | OS+ | 62.73 | 39.98 | 30.29 | 60.21 | 44.39 | 52.96 | 48.43 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | OS+ | 62.73 | 39.98 | 30.29 | 60.21 | 44.39 | 52.96 | 48.43 |'
- en: '| W4A4 | OmniQuant | 66.15 | 45.20 | 31.14 | 63.51 | 56.44 | 53.43 | 52.65
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | OmniQuant | 66.15 | 45.20 | 31.14 | 63.51 | 56.44 | 53.43 | 52.65
    |'
- en: '| W4A4 | AffineQuant | 69.37 | 42.55 | 31.91 | 63.73 | 57.65 | 55.33 | 53.42
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | AffineQuant | 69.37 | 42.55 | 31.91 | 63.73 | 57.65 | 55.33 | 53.42
    |'
- en: '| W4A4 | I-LLM | 67.25 | 45.58 | 32.59 | 63.88 | 58.89 | 57.06 | 54.21 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | I-LLM | 67.25 | 45.58 | 32.59 | 63.88 | 58.89 | 57.06 | 54.21 |'
- en: '| LLaMA-13B | FP16 | - | 79.10 | 59.89 | 44.45 | 68.01 | 76.21 | 70.31 | 66.33
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-13B | FP16 | - | 79.10 | 59.89 | 44.45 | 68.01 | 76.21 | 70.31 | 66.33
    |'
- en: '| W6A6 | SmoothQuant | 77.91 | 56.60 | 42.40 | 64.95 | 75.36 | 69.36 | 64.43
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | SmoothQuant | 77.91 | 56.60 | 42.40 | 64.95 | 75.36 | 69.36 | 64.43
    |'
- en: '| W6A6 | OmniQuant | 78.40 | 57.28 | 42.91 | 67.00 | 75.82 | 68.27 | 64.95
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | OmniQuant | 78.40 | 57.28 | 42.91 | 67.00 | 75.82 | 68.27 | 64.95
    |'
- en: '| W6A6 | I-LLM | 77.48 | 56.94 | 44.03 | 64.92 | 75.24 | 69.14 | 64.63 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | I-LLM | 77.48 | 56.94 | 44.03 | 64.92 | 75.24 | 69.14 | 64.63 |'
- en: '| W4A4 | SmoothQuant | 61.04 | 39.18 | 30.80 | 61.80 | 52.29 | 51.06 | 49.36
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | SmoothQuant | 61.04 | 39.18 | 30.80 | 61.80 | 52.29 | 51.06 | 49.36
    |'
- en: '| W4A4 | OS+ | 63.00 | 40.32 | 30.38 | 60.34 | 53.61 | 51.54 | 49.86 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | OS+ | 63.00 | 40.32 | 30.38 | 60.34 | 53.61 | 51.54 | 49.86 |'
- en: '| W4A4 | OmniQuant | 69.69 | 47.39 | 33.10 | 62.84 | 58.96 | 55.80 | 54.37
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | OmniQuant | 69.69 | 47.39 | 33.10 | 62.84 | 58.96 | 55.80 | 54.37
    |'
- en: '| W4A4 | AffineQuant | 66.32 | 43.90 | 29.61 | 64.10 | 56.88 | 54.70 | 52.58
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | AffineQuant | 66.32 | 43.90 | 29.61 | 64.10 | 56.88 | 54.70 | 52.58
    |'
- en: '| W4A4 | I-LLM | 67.95 | 48.15 | 34.47 | 62.29 | 63.13 | 59.98 | 56.00 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | I-LLM | 67.95 | 48.15 | 34.47 | 62.29 | 63.13 | 59.98 | 56.00 |'
- en: '| LLaMA-30B | FP16 | - | 80.08 | 58.92 | 45.47 | 68.44 | 79.21 | 72.53 | 67.44
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-30B | FP16 | - | 80.08 | 58.92 | 45.47 | 68.44 | 79.21 | 72.53 | 67.44
    |'
- en: '| W6A6 | SmoothQuant | 77.14 | 57.61 | 42.91 | 65.56 | 78.07 | 69.92 | 65.20
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | SmoothQuant | 77.14 | 57.61 | 42.91 | 65.56 | 78.07 | 69.92 | 65.20
    |'
- en: '| W6A6 | OmniQuant | 78.40 | 57.28 | 42.91 | 67.00 | 75.82 | 68.27 | 64.95
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | OmniQuant | 78.40 | 57.28 | 42.91 | 67.00 | 75.82 | 68.27 | 64.95
    |'
- en: '| W6A6 | I-LLM | 79.43 | 58.88 | 45.14 | 73.36 | 78.51 | 72.61 | 67.99 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | I-LLM | 79.43 | 58.88 | 45.14 | 73.36 | 78.51 | 72.61 | 67.99 |'
- en: '| W4A4 | SmoothQuant | 58.65 | 35.53 | 27.73 | 60.42 | 35.56 | 48.06 | 44.83
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | SmoothQuant | 58.65 | 35.53 | 27.73 | 60.42 | 35.56 | 48.06 | 44.83
    |'
- en: '| W4A4 | OS+ | 67.63 | 46.17 | 34.40 | 60.70 | 54.32 | 52.64 | 52.62 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | OS+ | 67.63 | 46.17 | 34.40 | 60.70 | 54.32 | 52.64 | 52.62 |'
- en: '| W4A4 | OmniQuant | 71.21 | 49.45 | 34.47 | 65.33 | 64.65 | 59.19 | 56.63
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | OmniQuant | 71.21 | 49.45 | 34.47 | 65.33 | 64.65 | 59.19 | 56.63
    |'
- en: '| W4A4 | AffineQuant | 66.32 | 43.90 | 29.61 | 64.10 | 56.88 | 54.70 | 52.58
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | AffineQuant | 66.32 | 43.90 | 29.61 | 64.10 | 56.88 | 54.70 | 52.58
    |'
- en: '| W4A4 | I-LLM | 71.38 | 51.81 | 37.12 | 65.69 | 67.79 | 61.40 | 59.20 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | I-LLM | 71.38 | 51.81 | 37.12 | 65.69 | 67.79 | 61.40 | 59.20 |'
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.1 Quantitative Results ‣ 4 Experiments ‣ I-LLM:
    Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models")
    presents our performance on six zero-shot tasks, employing both W4A4 and W6A6
    settings. Particularly striking is the performance of the LLaMA-30b model, configured
    with full quantization at W6A6 precision, which achieves an average accuracy on
    these tasks surpassing even that of the floating-point model. This achievement
    underscores the potential of fully quantized integer-only methods in maintaining
    the generalization capabilities of LLMs to a considerable degree.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [3](#S4.T3 "表 3 ‣ 4.1 定量结果 ‣ 4 实验 ‣ I-LLM: 全量化低位大语言模型的高效整数推理") 展示了我们在六个零样本任务上的表现，使用了
    W4A4 和 W6A6 设置。特别引人注目的是 LLaMA-30b 模型，在 W6A6 精度下全量化配置的表现，这些任务上的平均准确率甚至超过了浮点模型。这一成就突显了全量化整数仅方法在保持
    LLM 泛化能力方面的巨大潜力。'
- en: 4.2 Ablation Study
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 消融研究
- en: 'Contribution of Fully-Smooth Block Reconstruction In Table [5](#S4.T5 "Table
    5 ‣ 4.2 Ablation Study ‣ 4 Experiments ‣ I-LLM: Efficient Integer-Only Inference
    for Fully-Quantized Low-Bit Large Language Models"), we meticulously assess the
    impact of various PTQ methods on model accuracy. To maintain impartiality, this
    experiment abstains from utilizing integer-only operators; instead, all quantization
    procedures are substituted with pseudo-quantization. The nodes necessitating quantization
    align with those delineated in Fig [4](#S3.F4 "Figure 4 ‣ 3 Method ‣ I-LLM: Efficient
    Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models"). Notably,
    conventional non-integer-only methodologies often overlook the influence of activation
    in non-linear layers, leading to compromised model accuracy during integer inference.
    However, with the integration of FSBR, these activations are thoughtfully considered
    during PTQ, effectively reinstating the model’s accuracy under full quantization.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [5](#S4.T5 "表 5 ‣ 4.2 消融研究 ‣ 4 实验 ‣ I-LLM: 全量化低位大语言模型的高效整数推理") 中，我们详细评估了各种
    PTQ 方法对模型准确度的影响。为了保持公正性，本实验未使用整数仅操作符，而是将所有量化过程替换为伪量化。需要量化的节点与图 [4](#S3.F4 "图 4
    ‣ 3 方法 ‣ I-LLM: 全量化低位大语言模型的高效整数推理") 中的节点一致。值得注意的是，传统的非整数仅方法往往忽视非线性层中的激活影响，导致整数推理时模型准确度的下降。然而，通过集成
    FSBR，这些激活在 PTQ 过程中得到了周到的考虑，有效恢复了模型在全量化下的准确度。'
- en: 'Table 4: Impact of different PTQ methods and integer-only operators on LLaMA-7B
    PPL($\downarrow$) across WikiText2 and C4 datasets.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 不同 PTQ 方法和整数仅操作符对 LLaMA-7B PPL($\downarrow$) 在 WikiText2 和 C4 数据集上的影响。'
- en: '| LLaMA-7B | W4A4 | W6A6 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | W4A4 | W6A6 |'
- en: '| Method | WikiText2 | C4 | WikiText2 | C4 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WikiText2 | C4 | WikiText2 | C4 |'
- en: '| SmoothQuant | 256.58 | 218.47 | 6.09 | 7.6 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant | 256.58 | 218.47 | 6.09 | 7.6 |'
- en: '| OmniquantQuant | 122.18 | 183.2 | 5.99 | 7.57 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| OmniquantQuant | 122.18 | 183.2 | 5.99 | 7.57 |'
- en: '| FSBR | 9.44 | 12.72 | 5.83 | 7.02 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| FSBR | 9.44 | 12.72 | 5.83 | 7.02 |'
- en: '| +DI-CLippedSoftamx | 9.44 | 12.72 | 5.83 | 7.02 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| +DI-CLippedSoftamx | 9.44 | 12.72 | 5.83 | 7.02 |'
- en: '| +DI-Swiglu | 9.12 | 12.38 | 5.83 | 7.04 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| +DI-Swiglu | 9.12 | 12.38 | 5.83 | 7.04 |'
- en: '| +DI-Norm | 9.52 | 12.63 | 5.85 | 7.35 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| +DI-Norm | 9.52 | 12.63 | 5.85 | 7.35 |'
- en: 'Table 5: Effect of clipped value in DI-ClippedSoftamx.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 剪裁值在 DI-CLippedSoftamx 中的效果。'
- en: '| LLaMA-7B | W4A4 | W6A6 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | W4A4 | W6A6 |'
- en: '| Clipped Value $c$ | WikiText2 | C4 | WikiText2 | C4 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 剪裁值 $c$ | WikiText2 | C4 | WikiText2 | C4 |'
- en: '| – | 7360945.00 | 1998371.38 | 60335.22 | 47103.44 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| – | 7360945.00 | 1998371.38 | 60335.22 | 47103.44 |'
- en: '| 20 | 9.15 | 12.39 | 5.86 | 7.36 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 9.15 | 12.39 | 5.86 | 7.36 |'
- en: '| 17 | 9.19 | 12.38 | 5.86 | 7.37 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 17 | 9.19 | 12.38 | 5.86 | 7.37 |'
- en: '| 15 | 9.16 | 12.36 | 5.85 | 7.36 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 9.16 | 12.36 | 5.85 | 7.36 |'
- en: '| 12 | 9.19 | 12.35 | 5.86 | 7.36 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 9.19 | 12.35 | 5.86 | 7.36 |'
- en: '| 10 | 9.23 | 12.45 | 5.89 | 7.42 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 9.23 | 12.45 | 5.89 | 7.42 |'
- en: 'Impact of Integer-Only Operators. As shown in Table [5](#S4.T5 "Table 5 ‣ 4.2
    Ablation Study ‣ 4 Experiments ‣ I-LLM: Efficient Integer-Only Inference for Fully-Quantized
    Low-Bit Large Language Models"), we intricately outline the precise impact of
    each integer-only operator on the holistic accuracy of the model. Additionally,
    we ablate the influence of the clipping coefficient $c$ in the Eq [10](#S3.E10
    "In 3.4.1 Dynamic Interger-only Clipped Softmax & Dynamic Integer-only Exponent
    function ‣ 3.4 Dynamic Non-Linear Integer-only Operations ‣ 3 Method ‣ I-LLM:
    Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models")
    from DI-ClippedSoftmax on model accuracy. It’s worth noting that the quantization
    of DI-Norm results in a performance decline, as expected due to residual connections,
    a phenomenon that aligns with our expectations.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '整数运算符的影响。如表[5](#S4.T5 "Table 5 ‣ 4.2 Ablation Study ‣ 4 Experiments ‣ I-LLM:
    Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models")所示，我们详细阐述了每个整数运算符对模型整体准确性的具体影响。此外，我们还对方程[10](#S3.E10
    "In 3.4.1 Dynamic Interger-only Clipped Softmax & Dynamic Integer-only Exponent
    function ‣ 3.4 Dynamic Non-Linear Integer-only Operations ‣ 3 Method ‣ I-LLM:
    Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models")中DI-ClippedSoftmax的剪裁系数$c$对模型准确性的影响进行了消融实验。值得注意的是，由于残差连接的存在，DI-Norm的量化导致了性能下降，这与我们的预期一致。'
- en: 5 Conclusion
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we present I-LLM, a fully-quantized integer-only PTQ framework
    for LLMs. We address the challenge of fluctuating activations in both linear and
    non-linear operations by proposing Fully-Smooth Block-Reconstruction (FSBR) to
    harmonize inter-channel variations and Dynamic Integer-only MatMul (DI-MatMul)
    to handle inter-token variations. Additionally, we design DI-ClippedSoftmax, DI-Exp,
    and DI-Norm as lightweight integer-only operators to replace complex math caculations.
    Experiments demonstrate that I-LLM outperforms simulated quantization methods
    and achieves comparable accuracy to the floating-point baseline. Notably, I-LLM
    can operate at W4A4 quantization setting with negligible loss of accuracy, bridging
    the gap between low-bit integer-only quantization and LLMs. This work opens up
    avenues for the efficient deployment of LLMs on edge devices without floating-point
    capabilities. Looking ahead, we are committed to deploying I-LLM on specialized
    hardware and cloud platforms, with the aim of achieving even greater acceleration
    performance and further optimizing the operational efficiency of LLMs in various
    computational environments.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了I-LLM，一个针对LLM的完全量化整数运算PTQ框架。我们通过提出完全平滑块重建（FSBR）来协调通道间变化，并通过动态整数矩阵乘法（DI-MatMul）来处理令牌间变化，从而应对线性和非线性操作中激活值波动的问题。此外，我们设计了DI-ClippedSoftmax、DI-Exp和DI-Norm作为轻量级整数运算符，以替代复杂的数学计算。实验表明，I-LLM优于模拟量化方法，并且与浮点基线的准确性相当。值得注意的是，I-LLM能够在W4A4量化设置下运行，准确性损失微乎其微，弥合了低位整数量化与LLM之间的差距。这项工作为在没有浮点计算能力的边缘设备上高效部署LLM开辟了途径。展望未来，我们致力于在专业硬件和云平台上部署I-LLM，旨在实现更大的加速性能，并进一步优化LLM在各种计算环境中的操作效率。
- en: References
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning
    about physical commonsense in natural language. In Proceedings of the AAAI conference
    on artificial intelligence, volume 34, pages 7432–7439, 2020.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi 等人。Piqa: 关于自然语言中的物理常识的推理。在AAAI人工智能会议论文集中，卷34，第7432–7439页，2020年。'
- en: '[2] Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj,
    Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi,
    Nicholas Mattei, et al. A systematic classification of knowledge, reasoning, and
    context within the arc dataset. arXiv preprint arXiv:1806.00358, 2018.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj,
    Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi,
    Nicholas Mattei 等人。知识、推理和上下文在arc数据集中的系统分类。arXiv预印本 arXiv:1806.00358，2018年。'
- en: '[3] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip:
    2-bit quantization of large language models with guarantees. arXiv preprint arXiv:2307.13304,
    2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov 和 Christopher De Sa。Quip: 具有保证的大型语言模型的2位量化。arXiv预印本
    arXiv:2307.13304，2023年。'
- en: '[4] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
    Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of
    natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
    Collins, 和 Kristina Toutanova。Boolq: 探索自然是/否问题的惊人难度。arXiv预印本 arXiv:1905.10044，2019年。'
- en: '[5] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms. Advances in Neural Information Processing
    Systems, 2024.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman 和 Luke Zettlemoyer。Qlora：量化
    LLMS 的高效微调。神经信息处理系统进展，2024年。'
- en: '[6] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias
    Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
    Spqr: A sparse-quantized representation for near-lossless llm weight compression.
    arXiv preprint arXiv:2306.03078, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias
    Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler 和 Dan Alistarh。Spqr：一种用于近乎无损
    LLMS 权重压缩的稀疏量化表示。arXiv 预印本 arXiv:2306.03078，2023年。'
- en: '[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929, 2020.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly 等人。图像价值16x16个词：大规模图像识别的变压器。arXiv 预印本 arXiv:2010.11929，2020年。'
- en: '[8] Eric Flamand, Davide Rossi, Francesco Conti, Igor Loi, Antonio Pullini,
    Florent Rotenberg, and Luca Benini. Gap-8: A risc-v soc for ai at the edge of
    the iot. In 2018 IEEE 29th International Conference on Application-specific Systems,
    Architectures and Processors (ASAP), pages 1–4\. IEEE, 2018.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Eric Flamand, Davide Rossi, Francesco Conti, Igor Loi, Antonio Pullini,
    Florent Rotenberg 和 Luca Benini。Gap-8：一个用于边缘 AI 的 RISC-V SoC。发表于 2018 IEEE 第29届国际应用特定系统、架构与处理器会议（ASAP），页码
    1–4\. IEEE，2018年。'
- en: '[9] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh。Gptq：生成预训练变压器的准确后训练量化。arXiv
    预印本 arXiv:2210.17323，2022年。'
- en: '[10] Google. Edge tpu, 2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Google。Edge TPU，2024年。'
- en: '[11] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,
    Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training
    of neural networks for efficient integer-arithmetic-only inference. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 2704–2713,
    2018.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,
    Andrew Howard, Hartwig Adam 和 Dmitry Kalenichenko。神经网络的量化和训练，实现高效的整数运算推断。发表于 IEEE
    计算机视觉与模式识别会议论文集，页码 2704–2713，2018年。'
- en: '[12] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer.
    I-bert: Integer-only bert quantization. In International conference on machine
    learning, pages 5506–5518\. PMLR, 2021.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney 和 Kurt Keutzer。I-bert：仅支持整数的
    BERT 量化。发表于国际机器学习会议，页码 5506–5518\. PMLR，2021年。'
- en: '[13] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
    Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization.
    arXiv preprint arXiv:2306.07629, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
    Michael W Mahoney 和 Kurt Keutzer。Squeezellm：稠密与稀疏量化。arXiv 预印本 arXiv:2306.07629，2023年。'
- en: '[14] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park.
    Owq: Lessons learned from activation outliers for weight quantization in large
    language models. arXiv preprint arXiv:2306.02272, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim 和 Eunhyeok Park。Owq：从激活异常值中获得的经验教训，用于大型语言模型的权重量化。arXiv
    预印本 arXiv:2306.02272，2023年。'
- en: '[15] Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Norm tweaking: High-performance
    low-bit quantization of large language models. Association for the Advancement
    of Artificial Intelligence, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Liang Li, Qingyuan Li, Bo Zhang 和 Xiangxiang Chu。Norm tweaking：大型语言模型的高性能低比特量化。人工智能协会，2023年。'
- en: '[16] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei
    Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization
    by block reconstruction. In International Conference on Learning Representations
    (ICLR), 2021.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei
    Yu, Wei Wang 和 Shi Gu。Brecq：通过块重建推动后训练量化的极限。发表于国际学习表征会议（ICLR），2021年。'
- en: '[17] Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu
    Chen, and Tuo Zhao. Loftq: Lora-fine-tuning-aware quantization for large language
    models. arXiv preprint arXiv:2310.08659, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu
    Chen 和 Tuo Zhao。Loftq：大语言模型的 Lora 微调感知量化。arXiv 预印本 arXiv:2310.08659，2023年。'
- en: '[18] Zhikai Li and Qingyi Gu. I-vit: integer-only quantization for efficient
    vision transformer inference. In Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pages 17065–17075, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Zhikai Li 和 Qingyi Gu. I-vit: 整数-only 量化以提高视觉转换器推理效率。发表于 IEEE/CVF 国际计算机视觉会议论文集，第17065–17075页，2023年。'
- en: '[19] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Ji Lin、Jiaming Tang、Haotian Tang、Shang Yang、Xingyu Dang 和 Song Han. Awq:
    关注激活的权重量化用于 LLM 压缩和加速。arXiv 预印本 arXiv:2306.00978，2023年。'
- en: '[20] Ye Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran Liu, and Jingbo Zhu.
    Towards fully 8-bit integer inference for the transformer model. In Proceedings
    of the Twenty-Ninth International Conference on International Joint Conferences
    on Artificial Intelligence, pages 3759–3765, 2021.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Ye Lin、Yanyang Li、Tengbo Liu、Tong Xiao、Tongran Liu 和 Jingbo Zhu. 朝着完全
    8 位整数推理的变压器模型前进。发表于第二十九届国际人工智能联合会议论文集，第3759–3765页，2021年。'
- en: '[21] Yang Lin, Tianyu Zhang, Peiqin Sun, Zheng Li, and Shuchang Zhou. Fq-vit:
    Post-training quantization for fully quantized vision transformer. In Proceedings
    of the Thirty-First International Joint Conference on Artificial Intelligence,
    IJCAI-22, pages 1173–1179, 2022.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Yang Lin、Tianyu Zhang、Peiqin Sun、Zheng Li 和 Shuchang Zhou. Fq-vit: 完全量化视觉转换器的后训练量化。发表于第三十一届国际人工智能联合会议，IJCAI-22，第1173–1179页，2022年。'
- en: '[22] Jiawei Liu, Lin Niu, Zhihang Yuan, Dawei Yang, Xinggang Wang, and Wenyu
    Liu. Pd-quant: Post-training quantization based on prediction difference metric.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 24427–24437, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Jiawei Liu、Lin Niu、Zhihang Yuan、Dawei Yang、Xinggang Wang 和 Wenyu Liu.
    Pd-quant: 基于预测差异度量的后训练量化。发表于 IEEE/CVF 计算机视觉与模式识别会议论文集，第24427–24437页，2023年。'
- en: '[23] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar
    Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free
    quantization aware training for large language models. arXiv preprint arXiv:2305.17888,
    2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Zechun Liu、Barlas Oguz、Changsheng Zhao、Ernie Chang、Pierre Stock、Yashar
    Mehdad、Yangyang Shi、Raghuraman Krishnamoorthi 和 Vikas Chandra. Llm-qat: 面向大型语言模型的数据无关量化感知训练。arXiv
    预印本 arXiv:2305.17888，2023年。'
- en: '[24] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Stephen Merity、Caiming Xiong、James Bradbury 和 Richard Socher. 指针哨兵混合模型。arXiv
    预印本 arXiv:1609.07843，2016年。'
- en: '[25] Haotong Qin, Yifu Ding, Mingyuan Zhang, Qinghua Yan, Aishan Liu, Qingqing
    Dang, Ziwei Liu, and Xianglong Liu. Bibert: Accurate fully binarized bert. arXiv
    preprint arXiv:2203.06390, 2022.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Haotong Qin、Yifu Ding、Mingyuan Zhang、Qinghua Yan、Aishan Liu、Qingqing Dang、Ziwei
    Liu 和 Xianglong Liu. Bibert: 准确的全二值化 BERT。arXiv 预印本 arXiv:2203.06390，2022年。'
- en: '[26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of Machine Learning
    Research, 2020.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J Liu. 通过统一的文本到文本转换器探索迁移学习的极限。机器学习研究杂志，2020年。'
- en: '[27] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
    Winogrande: An adversarial winograd schema challenge at scale. Communications
    of the ACM, 2021.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Keisuke Sakaguchi、Ronan Le Bras、Chandra Bhagavatula 和 Yejin Choi. Winogrande:
    一种大规模对抗性 Winograd 语法挑战。ACM 通讯，2021年。'
- en: '[28] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. Pb-llm: Partially
    binarized large language models. International Conference on Learning Representations,
    2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Yuzhang Shang、Zhihang Yuan、Qiang Wu 和 Zhen Dong. Pb-llm: 部分二值化的大型语言模型。国际表示学习会议，2023年。'
- en: '[29] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian
    Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. CoRR, abs/2308.13137, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Wenqi Shao、Mengzhao Chen、Zhaoyang Zhang、Peng Xu、Lirui Zhao、Zhiqian Li、Kaipeng
    Zhang、Peng Gao、Yu Qiao 和 Ping Luo. Omniquant: 大型语言模型的全方向校准量化。CoRR，abs/2308.13137，2023年。'
- en: '[30] Jacob R Stevens, Rangharajan Venkatesan, Steve Dai, Brucek Khailany, and
    Anand Raghunathan. Softermax: Hardware/software co-design of an efficient softmax
    for transformers. In 2021 58th ACM/IEEE Design Automation Conference (DAC), pages
    469–474, 2021.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Jacob R Stevens、Rangharajan Venkatesan、Steve Dai、Brucek Khailany 和 Anand
    Raghunathan. Softermax: 高效软最大函数的硬件/软件协同设计。发表于 2021 第58届 ACM/IEEE 设计自动化会议（DAC），第469–474页，2021年。'
- en: '[31] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 于戈·图夫龙、提博·拉夫里尔、戈蒂埃·伊扎卡、泽维尔·马尔蒂内、玛丽-安·拉肖、蒂莫泰·拉克鲁瓦、巴普蒂斯特·罗济耶尔、纳曼·戈亚尔、埃里克·汉布罗、法伊萨尔·阿扎尔等。Llama:
    开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971，2023年。'
- en: '[32] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 于戈·图夫龙、路易斯·马丁、凯文·斯通、彼得·阿尔伯特、阿姆贾德·阿尔马黑里、雅丝敏·巴巴伊、尼古拉·巴什利科夫、苏米娅·巴特拉、普拉杰瓦尔·巴尔加瓦、施鲁提·博萨尔等。Llama
    2: 开放基础与微调聊天模型。arXiv 预印本 arXiv:2307.09288，2023年。'
- en: '[33] Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu. Qdrop:
    Randomly dropping quantization for extremely low-bit post-training quantization.
    In International Conference on Learning Representations (ICLR), 2022.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] 魏秀英、龚瑞豪、李宇航、刘向龙、余凤伟。Qdrop: 随机丢弃量化用于极低位后训练量化。国际学习表征会议（ICLR），2022年。'
- en: '[34] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang
    Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language
    models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145,
    2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] 魏秀英、张云晨、李宇航、张相国、龚瑞豪、郭金阳、刘向龙。Outlier suppression+: 通过等效和最优的移位与缩放实现大型语言模型的精确量化。arXiv
    预印本 arXiv:2304.09145，2023年。'
- en: '[35] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang,
    Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit
    of low-bit transformer language models. Advances in Neural Information Processing
    Systems, 2022.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] 魏秀英、张云晨、张相国、龚瑞豪、张尚航、张琦、余凤伟、刘向龙。Outlier suppression: 推动低位变换器语言模型的极限。神经信息处理系统进展，2022年。'
- en: '[36] WIKIPEDIA. Arm cortex-m, 2024.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 维基百科。Arm cortex-m，2024年。'
- en: '[37] Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius.
    Integer quantization for deep learning inference: Principles and empirical evaluation.
    arXiv preprint arXiv:2004.09602, 2020.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] 吴昊、帕特里克·贾德、张晓杰、米哈伊尔·伊萨耶夫、保利斯·米基维修斯。深度学习推理的整数量化: 原则与实证评估。arXiv 预印本 arXiv:2004.09602，2020年。'
- en: '[38] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han.
    Smoothquant: Accurate and efficient post-training quantization for large language
    models. arXiv preprint arXiv:2211.10438, 2022.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] 肖光轩、林骥、米开朗·塞兹内克、朱利安·德穆特、韩松。Smoothquant: 精确且高效的大型语言模型后训练量化。arXiv 预印本 arXiv:2211.10438，2022年。'
- en: '[39] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang,
    Zhensu Chen, Xiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank
    adaptation of large language models. arXiv preprint arXiv:2309.14717, 2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] 许誉辉、谢灵溪、顾晓涛、陈欣、常恒、张恒恒、陈振素、张小鹏、田奇。Qa-lora: 量化感知的低秩适应用于大型语言模型。arXiv 预印本
    arXiv:2309.14717，2023年。'
- en: '[40] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] 姚哲伟、瑞扎·雅兹达尼·阿米纳巴迪、张敏佳、吴晓霞、李聪龙、何宇雄。Zeroquant: 高效且经济的大规模变换器后训练量化。arXiv 预印本
    arXiv:2206.01861，2022年。'
- en: '[41] Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric
    Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael Mahoney, et al. Hawq-v3: Dyadic
    neural network quantization. In International Conference on Machine Learning,
    pages 11875–11886\. PMLR, 2021.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] 姚哲伟、董臻、郑章成、阿米尔·戈拉米、余佳丽、谭埃里克、王乐源、黄启晶、王怡达、迈克尔·马赫尼等。Hawq-v3: 双对神经网络量化。国际机器学习会议论文集，11875–11886页。PMLR，2021年。'
- en: '[42] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang,
    Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training
    quantization for large language models. arXiv preprint arXiv:2304.01089, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] 袁志航、牛林、刘家伟、刘文玉、王星刚、尚宇章、孙光宇、吴强、吴佳祥、吴冰哲。Rptq: 基于重排序的后训练量化用于大型语言模型。arXiv
    预印本 arXiv:2304.01089，2023年。'
- en: '[43] Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu
    Sun. Asvd: Activation-aware singular value decomposition for compressing large
    language models. arXiv preprint arXiv:2312.05821, 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] 袁志航、尚宇章、宋越、吴强、闫彦、孙光宇。Asvd: 激活感知的奇异值分解用于压缩大型语言模型。arXiv 预印本 arXiv:2312.05821，2023年。'
- en: '[44] Yuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, and
    Liqiang Nie. Wkvquant: Quantizing weight and key/value cache for large language
    models gains more. arXiv preprint arXiv:2402.12065, 2024.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Yuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, 和 Liqiang
    Nie。Wkvquant: 对大型语言模型进行权重和键/值缓存量化获得更多。arXiv 预印本 arXiv:2402.12065, 2024。'
- en: '[45] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830,
    2019.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, 和 Yejin Choi。Hellaswag:
    机器真的能完成你的句子吗？arXiv 预印本 arXiv:1905.07830, 2019。'
- en: '[46] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open
    pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, 等人。Opt: 开放预训练变换器语言模型。arXiv
    预印本 arXiv:2205.01068, 2022。'
- en: '[47] Sifan Zhou, Liang Li, Xinyu Zhang, Bo Zhang, Shipeng Bai, Miao Sun, Ziyu
    Zhao, Xiaobo Lu, and Xiangxiang Chu. Lidar-ptq: Post-training quantization for
    point cloud 3d object detection. International Conference on Learning Representations,
    2024.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Sifan Zhou, Liang Li, Xinyu Zhang, Bo Zhang, Shipeng Bai, Miao Sun, Ziyu
    Zhao, Xiaobo Lu, 和 Xiangxiang Chu。Lidar-ptq: 点云 3D 目标检测的后训练量化。国际学习表征会议，2024。'
- en: '[48] Danyang Zhu, Siyuan Lu, Meiqi Wang, Jun Lin, and Zhongfeng Wang. Efficient
    precision-adjustable architecture for softmax function in deep learning. IEEE
    Transactions on Circuits and Systems II: Express Briefs, 67(12):3382–3386, 2020.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Danyang Zhu, Siyuan Lu, Meiqi Wang, Jun Lin, 和 Zhongfeng Wang。深度学习中 softmax
    函数的高效精度可调架构。IEEE Transactions on Circuits and Systems II: Express Briefs, 67(12):3382–3386,
    2020。'
- en: Appendix A Appendix
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Quantization Preliminaries
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 量化初步
- en: 'Quantization & Dequantization. Quantization typically refers to mapping a floating-point
    number to a discrete interval with integer number. Here, we only consider uniform
    quantization. The quantization process can be expressed as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 量化与反量化。量化通常指的是将浮点数映射到一个离散的整数区间。在这里，我们仅考虑均匀量化。量化过程可以表达如下：
- en: '|  | $\displaystyle\bm{X}^{I}=\text{clamp}\left(\left\lfloor\frac{\bm{X}}{s}\right\rceil+zp^{I},0,2^{n^{I}}-1\right)$
    |  | (13) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{X}^{I}=\text{clamp}\left(\left\lfloor\frac{\bm{X}}{s}\right\rceil+zp^{I},0,2^{n^{I}}-1\right)$
    |  | (13) |'
- en: '|  | $\displaystyle s=\frac{x_{\max}-x_{\min}}{2^{n^{I}}-1}$ |  | (14) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s=\frac{x_{\max}-x_{\min}}{2^{n^{I}}-1}$ |  | (14) |'
- en: '|  | $\displaystyle zp^{I}=\left\lfloor\frac{-x_{\min}}{s}\right\rceil$ |  |
    (15) |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle zp^{I}=\left\lfloor\frac{-x_{\min}}{s}\right\rceil$ |  |
    (15) |'
- en: '|  | $\displaystyle X^{\prime}=(X^{I}-zp^{I})\cdot s$ |  | (16) |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle X^{\prime}=(X^{I}-zp^{I})\cdot s$ |  | (16) |'
- en: where $\bm{X}$ from the activations of some samples, which is called static
    quantization. Alternatively, we can derive it from runtime statistics, known as
    dynamic quantization. Quantization can also be distinguished by its granularity
    into per-channel quantization and per-token quantization [[40](#bib.bib40)].
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{X}$ 来自某些样本的激活，这称为静态量化。或者，我们可以从运行时统计数据中推导它，这称为动态量化。量化还可以根据其粒度区分为每通道量化和每标记量化
    [[40](#bib.bib40)]。
- en: A.2 Dynamic Interger-only Algorithms
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 动态整数-only 算法
- en: 'Algorithm 1 Dynamic Integer-only Exp: DI-Exp'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 1 动态整数-only Exp: DI-Exp'
- en: 'Input: Integer input ${\mathbf{x}^{I}_{in}}$.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：整数输入 ${\mathbf{x}^{I}_{in}}$。
- en: 'Output: result of I-Exp.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：I-Exp 的结果。
- en: 1:$m_{f}^{I}=m^{I}+(m^{I}{\gg}1)-(m^{I}{\gg}4)$8:return result
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 1:$m_{f}^{I}=m^{I}+(m^{I}{\gg}1)-(m^{I}{\gg}4)$8:返回结果
- en: 'Algorithm 2 Dynamic Integer-only Softmax : DI-Softmax'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 2 动态整数-only Softmax : DI-Softmax'
- en: 'Input: Integer input ${\mathbf{x}^{I}_{in}}$.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：整数输入 ${\mathbf{x}^{I}_{in}}$。
- en: 'Output: Integer output ${\mathbf{y}^{I}_{out}}$.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：整数输出 ${\mathbf{y}^{I}_{out}}$。
- en: 1:$\mathbf{x}^{I}_{\Delta}=\mathbf{x}^{I}_{in}-max(\mathbf{x}^{I}_{in})$
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 1:$\mathbf{x}^{I}_{\Delta}=\mathbf{x}^{I}_{in}-max(\mathbf{x}^{I}_{in})$
- en: 'Algorithm 3 Dynamic Integer-only SwiGLU : DI-SwiGLU'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 3 动态整数-only SwiGLU : DI-SwiGLU'
- en: 'Input: Integer input ${\mathbf{x}^{I}_{gate},\mathbf{x}^{I}_{up}}$.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：整数输入 ${\mathbf{x}^{I}_{gate},\mathbf{x}^{I}_{up}}$。
- en: 'Output: Integer output $\mathbf{y}^{I}_{out}$.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：整数输出 $\mathbf{y}^{I}_{out}$。
- en: 1:$\mathbf{x}^{I}_{smoothed\_gate}=\mathbf{x}^{I}_{gate}/\alpha_{smooth}$
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 1:$\mathbf{x}^{I}_{smoothed\_gate}=\mathbf{x}^{I}_{gate}/\alpha_{smooth}$
- en: 'Algorithm 4 Dynamic Integer-only RMSnorm : DI-RMSnorm'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 4 动态整数-only RMSnorm : DI-RMSnorm'
- en: 'Input: Integer input $\mathbf{x}^{I}_{in}$.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：整数输入 $\mathbf{x}^{I}_{in}$。
- en: 'Output: Integer output $\mathbf{y}^{I}_{out}$.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：整数输出 $\mathbf{y}^{I}_{out}$。
- en: 1:function I-Sqrt($I_{in}$29:end function
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 1:函数 I-Sqrt($I_{in}$29:结束函数
- en: A.3 Fully Results
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 完全结果
- en: '![Refer to caption](img/a9c36d953f07fdd1ad90e8426578217a.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a9c36d953f07fdd1ad90e8426578217a.png)'
- en: 'Figure 6: The output activation distribution of qkv of LLMs before and after
    the FSBR.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：FSBR前后LLMs的qkv输出激活分布。
- en: A.4 Limitation and Discussion
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 限制与讨论
- en: We have shown evidence that our I-LLM can replicate float-point performance
    with an integer-only module with 8-bit or lower bits. However, due to time constraints,
    we are currently evaluating the model’s latency on real hardware devices, but
    have not obtained quantitative results. Another limitation is that we only focused
    on natural language models, however, it would be interesting to explore how I-LLM
    performs in computer vision tasks. We leave this for future work.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提供证据表明，我们的I-LLM可以使用8位或更低位数的整数模块复制浮点性能。然而，由于时间限制，我们目前正在评估模型在实际硬件设备上的延迟，但尚未获得定量结果。另一个限制是我们仅关注了自然语言模型，然而，探索I-LLM在计算机视觉任务中的表现将会很有趣。我们将这项工作留给未来。
