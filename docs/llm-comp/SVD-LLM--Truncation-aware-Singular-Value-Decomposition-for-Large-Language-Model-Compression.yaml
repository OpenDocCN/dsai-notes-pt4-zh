- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:53:19'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:53:19'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model
    Compression'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'SVD-LLM: 具有截断意识的奇异值分解用于大型语言模型压缩'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.07378](https://ar5iv.labs.arxiv.org/html/2403.07378)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.07378](https://ar5iv.labs.arxiv.org/html/2403.07378)
- en: Xin Wang¹  Yu Zheng²  Zhongwei Wan¹  Mi Zhang¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xin Wang¹  Yu Zheng²  Zhongwei Wan¹  Mi Zhang¹
- en: ¹The Ohio State University  ²Michigan State University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹俄亥俄州立大学  ²密歇根州立大学
- en: '[https://github.com/AIoT-MLSys-Lab/SVD-LLM](https://github.com/AIoT-MLSys-Lab/SVD-LLM)
    Corresponding author. Email: mizhang.1@osu.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/AIoT-MLSys-Lab/SVD-LLM](https://github.com/AIoT-MLSys-Lab/SVD-LLM)
    通讯作者。电子邮件: mizhang.1@osu.edu'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The advancements in Large Language Models (LLMs) have been hindered by their
    substantial sizes, which necessitate LLM compression methods for practical deployment.
    Singular Value Decomposition (SVD) offers a promising solution for LLM compression.
    However, state-of-the-art SVD-based LLM compression methods have two key limitations:
    truncating smaller singular values may lead to higher compression loss, and the
    lack of update on the compressed weight after SVD truncation. In this work, we
    propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations
    of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy
    to ensure a direct mapping between singular values and compression loss. Moreover,
    SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate
    for accuracy degradation under high compression ratios. We evaluate SVD-LLM on
    a total of $10$ datasets and eight models from three different LLM families at
    four different scales. Our results demonstrate the superiority of SVD-LLM over
    state-of-the-arts, especially at high model compression ratios.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的进步受到其巨大规模的制约，这要求采用LLM压缩方法以实现实际部署。奇异值分解（SVD）为LLM压缩提供了一种有前景的解决方案。然而，最先进的基于SVD的LLM压缩方法存在两个关键限制：截断较小的奇异值可能导致更高的压缩损失，以及SVD截断后未更新压缩权重。在这项工作中，我们提出了SVD-LLM，一种新的基于SVD的LLM压缩方法，解决了现有方法的局限性。SVD-LLM结合了一种考虑截断的数据显示策略，以确保奇异值与压缩损失之间的直接映射。此外，SVD-LLM采用逐层闭式模型参数更新策略，以弥补高压缩比下的准确性下降。我们在总计$10$个数据集和来自三个不同LLM家族的八个模型上评估了SVD-LLM。我们的结果展示了SVD-LLM相对于现有技术的优越性，尤其是在高模型压缩比下。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have demonstrated remarkable capabilities in a
    wide range of tasks such as natural language understanding and language generation [[31](#bib.bib31),
    [9](#bib.bib9)]. Despite such capabilities, the democratization of LLMs is primarily
    restricted by their substantial resource demands [[25](#bib.bib25), [26](#bib.bib26)].
    One of the most effective techniques to reduce the resource demands of LLMs is
    model compression [[32](#bib.bib32)]. Compression techniques based on quantization [[6](#bib.bib6),
    [15](#bib.bib15), [27](#bib.bib27)], parameter pruning [[16](#bib.bib16), [5](#bib.bib5)],
    and knowledge distillation [[10](#bib.bib10), [11](#bib.bib11)] specifically designed
    for LLMs have been intensively studied. Regardless of their success, these techniques
    have their own constraints, such as hardware dependency and the need for expensive
    retraining. Compared to those techniques, compression techniques based on low-rank
    approximation, such as Singular Value Decomposition (SVD) are not limited by those
    constraints.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在自然语言理解和语言生成等各种任务中展现了卓越的能力[[31](#bib.bib31), [9](#bib.bib9)]。尽管如此，LLMs的普及主要受到其巨大的资源需求的限制[[25](#bib.bib25),
    [26](#bib.bib26)]。减少LLMs资源需求的最有效技术之一是模型压缩[[32](#bib.bib32)]。针对LLMs的量化[[6](#bib.bib6),
    [15](#bib.bib15), [27](#bib.bib27)]、参数剪枝[[16](#bib.bib16), [5](#bib.bib5)]和知识蒸馏[[10](#bib.bib10),
    [11](#bib.bib11)]等压缩技术已经得到了广泛研究。尽管这些技术取得了一定成功，但它们也存在自身的限制，如对硬件的依赖和昂贵的重新训练需求。与这些技术相比，基于低秩近似的压缩技术，如奇异值分解（SVD），不受这些限制。
- en: 'Despite these advantages, the potential of SVD for LLM compression has not
    been thoroughly explored. A few SVD-based LLM compression methods such as ASVD [[28](#bib.bib28)]
    and FWSVD [[12](#bib.bib12)] have recently been proposed. However, these methods
    exhibit severe performance degradation when the model compression ratio¹¹1The
    compression ratio refers to the percentage of parameter reduction achieved through
    compression. is high. Such limitation can be attributed to two fundamental issues
    involved in their approaches: ❶ Imprecise Data Preprocessing: although the data
    preprocessing strategy proposed by ASVD reduces the negative impact of activation
    outliers, it does not establish a direct relationship between singular values
    and the model compression loss. As a consequence, truncating smaller singular
    values in SVD could lead to significant compression loss. ❷ Lack of Model Parameter
    Update after SVD Truncation: as the model compression ratio increases, the number
    of singular values that need to be truncated in SVD increases as well. To compensate
    for the accuracy degradation caused by truncating a large number of singular values,
    it is required to update the remaining parameters in the compressed model. Unfortunately,
    existing SVD-based LLM compression methods do not take such update into account,
    and thus fail to compensate for the accuracy degradation under high model compression
    ratios.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管具有这些优势，SVD在LLM压缩中的潜力尚未被彻底探索。最近提出了一些基于SVD的LLM压缩方法，如ASVD [[28](#bib.bib28)]和FWSVD [[12](#bib.bib12)]。然而，当模型压缩比¹¹1模型压缩比是指通过压缩实现的参数减少百分比。很高时，这些方法表现出严重的性能下降。这种限制可以归因于其方法中涉及的两个基本问题：❶
    不精确的数据预处理：尽管ASVD提出的数据预处理策略减少了激活异常值的负面影响，但它并未建立奇异值与模型压缩损失之间的直接关系。因此，SVD中截断较小的奇异值可能会导致显著的压缩损失。❷
    SVD截断后的模型参数更新缺失：随着模型压缩比的增加，需要在SVD中截断的奇异值数量也增加。为了弥补由于截断大量奇异值导致的准确性下降，需要更新压缩模型中的剩余参数。不幸的是，现有的基于SVD的LLM压缩方法并未考虑这种更新，因此未能在高模型压缩比下弥补准确性下降。
- en: 'In this paper, we propose a new SVD-based LLM compression method named SVD-LLM
    that effectively addresses the two fundamental issues of the existing methods.
    SVD-LLM differs from existing SVD-based LLM compression methods in two key aspects:
    ❶ Truncation-Aware Data Whitening: Supported by the theoretical proof, SVD-LLM
    incorporates a truncation-aware data whitening technique that ensures a direct
    mapping between singular values and model compression loss. In doing so, the proposed
    truncation-aware data whitening technique is able to identify which singular values
    should be truncated to incur minimal model compression loss. ❷ Layer-Wise Closed-Form
    Model Parameter Update: to compensate for accuracy degradation under high compression
    ratios, SVD-LLM incorporates a layer-wise closed-form model parameter update strategy
    to progressively update the compressed weights layer by layer.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新的基于SVD的LLM压缩方法，名为SVD-LLM，该方法有效解决了现有方法中的两个基本问题。SVD-LLM在两个关键方面不同于现有的基于SVD的LLM压缩方法：❶
    误差感知的数据白化：在理论证明的支持下，SVD-LLM结合了一种误差感知的数据白化技术，确保奇异值与模型压缩损失之间的直接映射。通过这样做，所提出的误差感知数据白化技术能够识别出哪些奇异值应被截断，以使模型压缩损失最小。❷
    分层闭式形式的模型参数更新：为了弥补在高压缩比下的准确性下降，SVD-LLM结合了一种分层闭式形式的模型参数更新策略，以逐层更新压缩的权重。
- en: We compare SVD-LLM with three SVD-based methods for LLM compression, including
    vanilla SVD as well as state-of-the-art methods FWSVD and ASVD. To demonstrate
    the generability of SVD-LLM, we conduct our evaluation on a total of $10$ minutes.
    (3) The independent performance of either of the two key components of SVD-LLM
    still consistently surpasses the performance of the current state-of-the-art SVD
    compression method under different compression ratios. (4) SVD-LLM can benefit
    other LLM compression methods. Our evaluation results show that SVD-LLM is able
    to further enhance the compression performance of well-recognized quantization
    (GPTQ [[6](#bib.bib6)]) and parameter pruning-based (LLM-Pruner [[16](#bib.bib16)])
    LLM compression methods. (5) SVD-LLM can ensure inference speedup on both GPU
    and CPU. It is able to achieve at most 1.7x speedup on GPU and 1.5x speedup on
    CPU under the 40% compression ratio. (6) Lastly, SVD-LLM brings additional benefit
    beyond compressing the sizes of LLMs, and is also able to reduce the footprint
    of KV cache during inference at runtime.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将SVD-LLM与三种基于SVD的LLM压缩方法进行比较，包括基础SVD以及最先进的方法FWSVD和ASVD。为了展示SVD-LLM的通用性，我们在总共$10$分钟内进行了评估。（3）SVD-LLM的两个关键组件中的任何一个独立表现仍然始终超越当前最先进的SVD压缩方法在不同压缩比下的表现。（4）SVD-LLM可以惠及其他LLM压缩方法。我们的评估结果显示，SVD-LLM能够进一步提升广泛认可的量化（GPTQ
    [[6](#bib.bib6)]）和基于参数剪枝（LLM-Pruner [[16](#bib.bib16)]）的LLM压缩方法的压缩性能。（5）SVD-LLM可以确保在GPU和CPU上加速推理。在40%的压缩比下，它能够在GPU上实现最多1.7倍的加速，在CPU上实现1.5倍的加速。（6）最后，SVD-LLM带来了超越压缩LLM尺寸的额外好处，它还能够在运行时减少KV缓存的占用。
- en: 2 Related Work
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Large Language Model Compression: LLMs in general contain billion-scale parameters.
    Applying conventional model compression methods for LLMs is not feasible as they
    necessitate retraining. To avoid retraining, post-training methods that do not
    involve retraining LLMs in the compression process have been developed. In general,
    these methods can be grouped into four categories: unstructured pruning, structured
    pruning, quantization, and low-rank approximation. Specifically, unstructured
    pruning methods set the individual weights’ elements to zero without changing
    its shape. A notable contribution is SparseGPT [[5](#bib.bib5)] which prunes the
    least important weight elements with the inversion of the Hessian matrix. However,
    the irregular sparsification of unstructured pruning is difficult to achieve the
    desired speedup or memory saving and can only demonstrate its best efficiency
    on certain hardware architecture such as NVIDIA Ampere GPU. Unlike unstructured
    pruning, structured pruning methods directly remove entire channels or other structured
    components from LLMs, making them easier to implement on hardware. For example,
    LLM-Pruner [[16](#bib.bib16)] utilizes a small amount of data to obtain the weight,
    parameter, and group importance of the coupled structure for pruning with LoRA
    to recover precision. However, due to the great modification of the weight matrix
    in LLM, it suffers from a great accuracy degradation, especially under high compression
    ratios. Quantization methods, on the other hand, achieve model compression by
    reducing the precision of weight matrices of an LLM. For example, GPTQ [[6](#bib.bib6)]
    uses layer-wise quantization and updates the weights with inverse Hessian information.
    However, quantization has the drawback of only providing a limited range of compression
    options, typically ranging from 3 to 8 bits. This limited range could prevent
    full utilization of the available memory budget.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型压缩：大型语言模型（LLMs）通常包含数十亿的参数。应用传统的模型压缩方法对LLMs进行压缩是不切实际的，因为这些方法需要重新训练。为了避免重新训练，已经开发出不涉及重新训练LLMs的压缩后训练方法。一般来说，这些方法可以分为四类：无结构剪枝、结构剪枝、量化和低秩近似。具体来说，无结构剪枝方法将个别权重元素设为零而不改变其形状。一项显著的贡献是SparseGPT [[5](#bib.bib5)]，它通过逆Hessian矩阵来剪枝最不重要的权重元素。然而，无结构剪枝的不规则稀疏化难以实现预期的加速或内存节省，并且只能在某些硬件架构（如NVIDIA
    Ampere GPU）上展示其最佳效率。与无结构剪枝不同，结构剪枝方法直接从LLMs中移除整个通道或其他结构化组件，使其在硬件上更易于实现。例如，LLM-Pruner [[16](#bib.bib16)]利用少量数据获取权重、参数和耦合结构的组重要性，以通过LoRA进行剪枝以恢复精度。然而，由于LLM权重矩阵的巨大修改，它在高压缩比下会遭受严重的精度下降。另一方面，量化方法通过减少LLM的权重矩阵的精度来实现模型压缩。例如，GPTQ [[6](#bib.bib6)]使用层级量化并通过逆Hessian信息更新权重。然而，量化方法的缺点是仅提供有限的压缩选项范围，通常在3到8位之间。这一有限范围可能会阻碍对可用内存预算的充分利用。
- en: '![Refer to caption](img/ed0651e23ca0fb812b1ad166afded52e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ed0651e23ca0fb812b1ad166afded52e.png)'
- en: 'Figure 1: Overview of SVD-LLM.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：SVD-LLM概述。
- en: 'SVD for LLM Compression: Singular Value Decomposition (SVD) is a widely used
    technique to reduce matrix size by approximating a matrix with two smaller low-ranking
    matrices [[8](#bib.bib8)]. In the context of LLM compression, only a few SVD-based
    LLM compression methods have been proposed. Specifically, vanilla SVD only focuses
    on the compression of the original weight matrix without considering the importance
    of the parameters, potentially giving a larger compression error. To address this
    problem, [[12](#bib.bib12)] propose FWSVD, which introduces Fisher information
    to weigh the importance of parameters. However, FWSVD requires a complex gradient
    calculation that demands substantial resources for LLM compression. Another problem
    of vanilla SVD is the distribution of activation can affect the compression error.
    To address this issue, [[28](#bib.bib28)] propose ASVD, which scales the weight
    matrix by a diagonal matrix that represents the impact of input channels on the
    weights. However, both FWSVD and ASVD do not establish a direct relationship between
    singular values and compression loss. As a result, truncating the smaller singular
    values may lead to higher compression loss. Moreover, as the compression ratio
    increases, it is necessary to update the compressed weight due to truncating a
    great number of singular values. However, existing methods have no design for
    this update and thus incur severe accuracy degradation under high compression
    ratios.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 压缩的 SVD：奇异值分解（SVD）是一种广泛使用的技术，通过用两个较小的低秩矩阵来逼近一个矩阵，从而减少矩阵的大小[[8](#bib.bib8)]。在
    LLM 压缩的背景下，提出的基于 SVD 的 LLM 压缩方法很少。具体来说，普通的 SVD 仅关注原始权重矩阵的压缩，而没有考虑参数的重要性，这可能导致较大的压缩误差。为了解决这个问题，[[12](#bib.bib12)]
    提出了 FWSVD，该方法引入了费舍尔信息来权衡参数的重要性。然而，FWSVD 需要复杂的梯度计算，这对 LLM 压缩需要大量资源。普通 SVD 的另一个问题是激活分布会影响压缩误差。为了解决这个问题，[[28](#bib.bib28)]
    提出了 ASVD，该方法通过表示输入通道对权重影响的对角矩阵来缩放权重矩阵。然而，FWSVD 和 ASVD 都没有建立奇异值与压缩损失之间的直接关系。因此，截断较小的奇异值可能导致更高的压缩损失。此外，随着压缩比的增加，由于截断了大量的奇异值，需要更新压缩后的权重。然而，现有方法没有为此更新进行设计，因此在高压缩比下会出现严重的准确性下降。
- en: 3 SVD-LLM
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 SVD-LLM
- en: '[Figure 1](#S2.F1 "In 2 Related Work ‣ SVD-LLM: Truncation-aware Singular Value
    Decomposition for Large Language Model Compression") provides an overview of SVD-LLM.
    At a high level, SVD-LLM is a SVD-based post-training LLM compression method.
    Specifically, following the standard procedure of post-training LLM compression
    methods [[5](#bib.bib5), [28](#bib.bib28), [27](#bib.bib27)], SVD-LLM uses a random
    set of sentences as calibration data to generate activation for truncation-aware
    data whitening and layer-wise closed-form update for model compression. SVD-LLM
    whitens the activation through Cholesky decomposition, and performs SVD to truncate
    the weight matrices to compress the LLM. Under high model compression ratios,
    SVD-LLM performs a layer-wise closed-form update to progressively update the remaining
    weights layer by layer after compression. In the following, we describe both truncation-aware
    data whitening and layer-wise closed-form update in detail. The pseudocode is
    provided in [Section A.2](#A1.SS2 "A.2 Pseudocode for SVD-LLM ‣ Appendix A Appendix.
    ‣ SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model
    Compression").'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1](#S2.F1 "在 2 相关工作 ‣ SVD-LLM：针对大语言模型压缩的考虑截断的奇异值分解") 提供了 SVD-LLM 的概述。总体而言，SVD-LLM
    是一种基于 SVD 的后训练 LLM 压缩方法。具体来说，按照标准的后训练 LLM 压缩方法[[5](#bib.bib5), [28](#bib.bib28),
    [27](#bib.bib27)]的程序，SVD-LLM 使用随机句子集作为校准数据来生成激活，用于考虑截断的数据白化和逐层闭式更新模型压缩。SVD-LLM
    通过 Cholesky 分解对白化激活进行处理，并执行 SVD 来截断权重矩阵以压缩 LLM。在高模型压缩比下，SVD-LLM 进行逐层闭式更新，在压缩后逐层更新剩余的权重。接下来，我们将详细描述考虑截断的数据白化和逐层闭式更新。伪代码见
    [附录 A.2](#A1.SS2 "A.2 SVD-LLM 的伪代码 ‣ 附录 A 附录 ‣ SVD-LLM：针对大语言模型压缩的考虑截断的奇异值分解")。'
- en: 3.1 Truncation-Aware Data Whitening
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 考虑截断的数据白化
- en: 'Motivation: Due to high variance of the input activation, simply applying vanilla
    SVD for LLM compression leads to severe accuracy degradation [[28](#bib.bib28)].
    To address this issue, ASVD [[28](#bib.bib28)] formulates LLM compression as an
    optimization problem with the following optimization objective:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 动机：由于输入激活的高方差，简单地应用普通 SVD 进行 LLM 压缩会导致严重的准确性下降[[28](#bib.bib28)]。为了解决这个问题，ASVD
    [[28](#bib.bib28)] 将 LLM 压缩公式化为一个优化问题，优化目标如下：
- en: '|  | $O=\min(&#124;&#124;WX-W^{\prime}X&#124;&#124;_{F})$ |  | (1) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $O=\min(&#124;&#124;WX-W^{\prime}X&#124;&#124;_{F})$ |  | (1) |'
- en: where $W$ is the compression loss in the form of Frobenius loss.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W$ 是以 Frobenius 损失形式表示的压缩损失。
- en: Specifically, ASVD extracts a diagonal matrix $S_{0}$.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，ASVD 提取了一个对角矩阵 $S_{0}$。
- en: 'Although normalizing the activation improves the performance, ASVD does not
    establish a direct relationship between singular values and compression loss (a
    detailed proof is included in [Section A.1](#A1.SS1 "A.1 The compression error
    of ASVD ‣ Appendix A Appendix. ‣ SVD-LLM: Truncation-aware Singular Value Decomposition
    for Large Language Model Compression")). To better illustrate this point, we show
    two concrete examples in LABEL:fig:asvd_whitening. In the first example ❶ where
    only one singular value is truncated, truncating the smallest singular value 0.1
    results in a higher compression loss (loss $=$ 1.7). Hence, truncating the smallest
    singular values does not lead to minimal loss.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管归一化激活可以提高性能，但 ASVD 并未建立奇异值和压缩损失之间的直接关系（详细证明见 [附录 A.1](#A1.SS1 "A.1 ASVD 的压缩误差
    ‣ 附录 A. ‣ SVD-LLM: 截断感知奇异值分解用于大规模语言模型压缩")）。为了更好地说明这一点，我们在 LABEL:fig:asvd_whitening
    中展示了两个具体示例。在第一个示例 ❶ 中，只有一个奇异值被截断，截断最小的奇异值 0.1 会导致更高的压缩损失（损失 = 1.7）。因此，截断最小的奇异值不会导致最小的损失。'
- en: 'Key Design: The key idea of SVD-LLM is to incorporate a truncation-aware data
    whitening technique that ensures a direct mapping between singular values and
    compression loss. To achieve this, SVD-LLM enforces the whitened activation $S^{-1}X$.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 关键设计：SVD-LLM 的关键思想是引入一种截断感知数据白化技术，确保奇异值和压缩损失之间的直接映射。为此，SVD-LLM 强制执行白化后的激活 $S^{-1}X$。
- en: LABEL:fig:svdllm_whitening illustrates the effect of the proposed truncation-aware
    data whitening technique. In the first example ❶ where only one singular value
    is truncated, the compression loss is equal to the truncated singular value. In
    the second example ❷, the compression loss of truncating multiple singular values
    is equal to the square root of the sum of their squares. As such, under the proposed
    truncation-aware data whitening technique, truncating the smallest singular values
    leads to minimal compression loss.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LABEL:fig:svdllm_whitening 展示了提出的截断感知数据白化技术的效果。在第一个示例 ❶ 中，只有一个奇异值被截断，压缩损失等于截断的奇异值。在第二个示例
    ❷ 中，截断多个奇异值的压缩损失等于它们平方和的平方根。因此，在提出的截断感知数据白化技术下，截断最小的奇异值会导致最小的压缩损失。
- en: 'Below, we provide a theoretical proof on why the proposed truncation-aware
    data whitening technique ensures a direct mapping between singular values and
    compression loss in the case of one singular value ([Theorem 3.2](#S3.Thmtheorem2
    "Theorem 3.2\. ‣ 3.1 Truncation-Aware Data Whitening ‣ 3 SVD-LLM ‣ SVD-LLM: Truncation-aware
    Singular Value Decomposition for Large Language Model Compression")) and multiple
    singular values ([Corollary 3.3](#S3.Thmtheorem3 "Corollary 3.3\. ‣ 3.1 Truncation-Aware
    Data Whitening ‣ 3 SVD-LLM ‣ SVD-LLM: Truncation-aware Singular Value Decomposition
    for Large Language Model Compression")).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '在下面，我们提供了一个理论证明，说明为什么提出的截断感知数据白化技术在单一奇异值（[定理 3.2](#S3.Thmtheorem2 "定理 3.2.
    ‣ 3.1 截断感知数据白化 ‣ 3 SVD-LLM ‣ SVD-LLM: 截断感知奇异值分解用于大规模语言模型压缩")）和多个奇异值（[推论 3.3](#S3.Thmtheorem3
    "推论 3.3. ‣ 3.1 截断感知数据白化 ‣ 3 SVD-LLM ‣ SVD-LLM: 截断感知奇异值分解用于大规模语言模型压缩")）的情况下，确保奇异值和压缩损失之间的直接映射。'
- en: Lemma 3.1.
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 3.1。
- en: 'The Frobenius norm of matrix $A$ can be deduced into the square root of the
    trace of its gram matrix, which is:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 $A$ 的 Frobenius 范数可以推导为其 Gram 矩阵的迹的平方根，即：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'Using [Lemma 3.1](#S3.Thmtheorem1 "Lemma 3.1\. ‣ 3.1 Truncation-Aware Data
    Whitening ‣ 3 SVD-LLM ‣ SVD-LLM: Truncation-aware Singular Value Decomposition
    for Large Language Model Compression"), we obtain the compression loss $L_{i}$
    to reduce its rank for compression:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 [引理 3.1](#S3.Thmtheorem1 "引理 3.1. ‣ 3.1 截断感知数据白化 ‣ 3 SVD-LLM ‣ SVD-LLM:
    截断感知奇异值分解用于大规模语言模型压缩")，我们获得压缩损失 $L_{i}$ 以减少其秩进行压缩：'
- en: '|  | $1$2 |  | (3) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: 'Since both $U=[u_{1},u_{2},u_{3},...,u_{r}]$ are orthogonal matrices, we have:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $U=[u_{1},u_{2},u_{3},...,u_{r}]$ 都是正交矩阵，我们有：
- en: '|  | $1$2 |  | (4) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: Theorem 3.2.
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 3.2。
- en: If $S$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 $S$。
- en: Proof.
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'Since the whitening matrix $S$. We can further infer [Equation 3](#S3.E3 "In
    3.1 Truncation-Aware Data Whitening ‣ 3 SVD-LLM ‣ SVD-LLM: Truncation-aware Singular
    Value Decomposition for Large Language Model Compression") to obtain:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于去白化矩阵$S$，我们可以进一步推导[公式 3](#S3.E3 "在 3.1 截断感知数据去白化 ‣ 3 SVD-LLM ‣ SVD-LLM：用于大语言模型压缩的截断感知奇异值分解")以获得：
- en: '|  | $1$2 |  | (5) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: Therefore, $L_{i}$ itself. ∎
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，$L_{i}$ 本身。∎
- en: Corollary 3.3.
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 推论 3.3。
- en: If $S$ compared to truncating others.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果$S$与截断其他情况比较。
- en: Proof.
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'If we truncate $\sigma_{m+1},\sigma_{m+2},\sigma_{m+3},...,\sigma_{r}$ is:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们截断$\sigma_{m+1},\sigma_{m+2},\sigma_{m+3},...,\sigma_{r}$为：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: The squared loss $L^{2}$ achieves the lowest compression loss. ∎
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 平方损失$L^{2}$实现了最低的压缩损失。∎
- en: 3.2 Layer-Wise Closed-Form Update
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 逐层闭式更新
- en: 'Motivation: Given the same calibration data as input, the compressed weight
    matrix $W^{\prime}$. However, existing SVD-based LLM compression methods have
    no design of parameter update after compression, leading to less competitive performance
    at high compression ratios.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 动机：给定相同的校准数据作为输入，压缩后的权重矩阵$W^{\prime}$。然而，现有的基于SVD的LLM压缩方法在压缩后没有设计参数更新，导致在高压缩比下表现不够竞争。
- en: '![Refer to caption](img/d2885266326962260521fac814f8f439.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d2885266326962260521fac814f8f439.png)'
- en: 'Figure 3: Layer-Wise Closed-Form Update.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：逐层闭式更新。
- en: 'Key Design: The key idea of SVD-LLM is to incorporate a layer-wise closed-form
    strategy to update $W^{\prime}$ fixed as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 关键设计：SVD-LLM的关键思想是结合逐层闭式形式策略来更新$W^{\prime}$，固定为：
- en: '|  | $1$2 |  | (7) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: 4 Experiments
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: Baselines. We compare SVD-LLM against three baselines including vanilla SVD
    as well as state-of-the-art SVD-based LLM compression methods FWSVD [[12](#bib.bib12)]
    and ASVD [[28](#bib.bib28)].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基准。我们将SVD-LLM与三种基准方法进行比较，包括原始SVD以及最先进的基于SVD的LLM压缩方法FWSVD [[12](#bib.bib12)]和ASVD [[28](#bib.bib28)]。
- en: Models and Datasets. To demonstrate the generability of our method, we evaluate
    the performance of SVD-LLM and the baselines on eight models from three different
    LLM families (LLaMA-7B, 13B, 30B, 65B [[23](#bib.bib23)], LLaMA2-7B [[24](#bib.bib24)],
    OPT-6.7B [[30](#bib.bib30)], Vicuna-7B [[3](#bib.bib3)] and Mistral-7B [[14](#bib.bib14)])
    and $10$ datasets including three language modeling datasets (WikiText-2 [[18](#bib.bib18)],
    PTB [[17](#bib.bib17)] and C4 [[21](#bib.bib21)]) and seven classification datasets
    (OpenbookQA [[20](#bib.bib20)], WinoGrande [[22](#bib.bib22)], HellaSwag [[29](#bib.bib29)],
    PIQA [[2](#bib.bib2)], MathQA [[1](#bib.bib1)], ARC-e, and ARC-c [[4](#bib.bib4)])
    in zero-shot setting with LM-Evaluation-Harness framework [[7](#bib.bib7)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和数据集。为了展示我们方法的泛化性，我们评估了SVD-LLM和基准方法在三个不同LLM家族的八个模型（LLaMA-7B、13B、30B、65B [[23](#bib.bib23)]，LLaMA2-7B [[24](#bib.bib24)]，OPT-6.7B [[30](#bib.bib30)]，Vicuna-7B [[3](#bib.bib3)]和Mistral-7B [[14](#bib.bib14)]）以及$10$个数据集，包括三个语言建模数据集（WikiText-2 [[18](#bib.bib18)]，PTB [[17](#bib.bib17)]和C4 [[21](#bib.bib21)]）和七个分类数据集（OpenbookQA [[20](#bib.bib20)]，WinoGrande [[22](#bib.bib22)]，HellaSwag [[29](#bib.bib29)]，PIQA [[2](#bib.bib2)]，MathQA [[1](#bib.bib1)]，ARC-e和ARC-c [[4](#bib.bib4)]）在零-shot设置下使用LM-Evaluation-Harness框架 [[7](#bib.bib7)]进行评估。
- en: Implementation Details. To ensure a fair comparison, we followed ASVD [[28](#bib.bib28)]
    to randomly select 256 samples from WikiText-2 as the calibration data. Since
    layer-wise closed-form update is intended to mitigate the accuracy drop under
    higher compression ratios, we only apply it when the compression ratios are at
    40% and above. All of our experiments are conducted on Nvidia A100 GPUs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 实施细节。为了确保公平比较，我们按照ASVD [[28](#bib.bib28)]随机从WikiText-2中选择256个样本作为校准数据。由于逐层闭式更新旨在减轻高压缩比下的准确度下降，我们仅在压缩比达到40%及以上时应用。所有实验均在Nvidia
    A100 GPU上进行。
- en: 4.1 Overall Performance
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 总体性能
- en: 'We evaluate the overall performance of SVD-LLM from four aspects: (1) performance
    under different compression ratios, (2) performance on different LLMs, (3) performance
    on LLMs with larger scales, and (4) performance with LoRA fine-tuning (See [Section A.3](#A1.SS3
    "A.3 Performance with LoRA Fine-Tuning. ‣ Appendix A Appendix. ‣ SVD-LLM: Truncation-aware
    Singular Value Decomposition for Large Language Model Compression")). Some generated
    contents by the compressed LLM are listed in [Section A.4](#A1.SS4 "A.4 Generated
    Content from the Compressed Model ‣ Appendix A Appendix. ‣ SVD-LLM: Truncation-aware
    Singular Value Decomposition for Large Language Model Compression") to provide
    a more straightforward comparison.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从四个方面评估了SVD-LLM的整体表现：(1) 在不同压缩比下的表现，(2) 在不同LLM上的表现，(3) 在大规模LLM上的表现，以及 (4)
    结合LoRA微调的表现（参见[附录A.3](#A1.SS3 "A.3 Performance with LoRA Fine-Tuning. ‣ 附录A 附录.
    ‣ SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model
    Compression")）。一些由压缩LLM生成的内容列在[附录A.4](#A1.SS4 "A.4 Generated Content from the
    Compressed Model ‣ 附录A 附录. ‣ SVD-LLM: Truncation-aware Singular Value Decomposition
    for Large Language Model Compression")中，以提供更直接的比较。'
- en: Performance under Different Compression Ratios.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同压缩比下的表现。
- en: 'Table 1: Zero-shot performance of LLaMA-7B compressed by SVD-LLM and baselines
    under 20% to 60% compression ratio on three language modeling datasets (measured
    by perplexity ($\downarrow$)). The best performance is marked in bold. The relative
    performance gain compared to the best-performing baseline is marked in green color
    inside bracket.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在三个语言建模数据集上，SVD-LLM和基准模型在20%到60%压缩比下的零-shot表现（以困惑度 ($\downarrow$) 衡量）。最佳表现用**粗体**标记。与最佳基准相比的相对表现提升用绿色括号内标记。
- en: '| Ratio | Method | WikiText-2$\downarrow$ |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 比率 | 方法 | WikiText-2$\downarrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| 0% | Original | 5.68 | 8.35 | 7.34 | 0.28 | 0.67 | 0.67 | 0.56 | 0.38 | 0.78
    | 0.27 | 0.52 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 0% | 原始 | 5.68 | 8.35 | 7.34 | 0.28 | 0.67 | 0.67 | 0.56 | 0.38 | 0.78 |
    0.27 | 0.52 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| 20% | SVD | 20061 | 20306 | 18800 | 0.14 | 0.27 | 0.51 | 0.26 | 0.21 | 0.53
    | 0.21 | 0.31 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 20% | SVD | 20061 | 20306 | 18800 | 0.14 | 0.27 | 0.51 | 0.26 | 0.21 | 0.53
    | 0.21 | 0.31 |'
- en: '| FWSVD | 1727 | 2152 | 1511 | 0.15 | 0.31 | 0.50 | 0.26 | 0.23 | 0.56 | 0.21
    | 0.32 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| FWSVD | 1727 | 2152 | 1511 | 0.15 | 0.31 | 0.50 | 0.26 | 0.23 | 0.56 | 0.21
    | 0.32 |'
- en: '| ASVD | 11.14 | 16.55 | 15.93 | 0.25 | 0.53 | 0.64 | 0.41 | 0.27 | 0.68 |
    0.24 | 0.43 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| ASVD | 11.14 | 16.55 | 15.93 | 0.25 | 0.53 | 0.64 | 0.41 | 0.27 | 0.68 |
    0.24 | 0.43 |'
- en: '| SVD-LLM | 7.94 ($\downarrow$2%) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM | 7.94 ($\downarrow$2%) |'
- en: '| 30% | SVD | 13103 | 17210 | 20871 | 0.13 | 0.26 | 0.51 | 0.26 | 0.21 | 0.54
    | 0.22 | 0.30 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 30% | SVD | 13103 | 17210 | 20871 | 0.13 | 0.26 | 0.51 | 0.26 | 0.21 | 0.54
    | 0.22 | 0.30 |'
- en: '| FWSVD | 20127 | 11058 | 7240 | 0.17 | 0.26 | 0.49 | 0.26 | 0.22 | 0.51 |
    0.19 | 0.30 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| FWSVD | 20127 | 11058 | 7240 | 0.17 | 0.26 | 0.49 | 0.26 | 0.22 | 0.51 |
    0.19 | 0.30 |'
- en: '| ASVD | 51 | 70 | 41 | 0.18 | 0.43 | 0.53 | 0.37 | 0.25 | 0.65 | 0.21 | 0.38
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| ASVD | 51 | 70 | 41 | 0.18 | 0.43 | 0.53 | 0.37 | 0.25 | 0.65 | 0.21 | 0.38
    |'
- en: '| SVD-LLM | 9.56 ($\downarrow$5%) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM | 9.56 ($\downarrow$5%) |'
- en: '| 40% | SVD | 52489 | 59977 | 47774 | 0.15 | 0.26 | 0.52 | 0.26 | 0.22 | 0.53
    | 0.20 | 0.30 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 40% | SVD | 52489 | 59977 | 47774 | 0.15 | 0.26 | 0.52 | 0.26 | 0.22 | 0.53
    | 0.20 | 0.30 |'
- en: '| FWSVD | 18156 | 20990 | 12847 | 0.16 | 0.26 | 0.51 | 0.26 | 0.22 | 0.53 |
    0.21 | 0.30 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| FWSVD | 18156 | 20990 | 12847 | 0.16 | 0.26 | 0.51 | 0.26 | 0.22 | 0.53 |
    0.21 | 0.30 |'
- en: '| ASVD | 1407 | 3292 | 1109 | 0.13 | 0.28 | 0.48 | 0.26 | 0.22 | 0.55 | 0.19
    | 0.30 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| ASVD | 1407 | 3292 | 1109 | 0.13 | 0.28 | 0.48 | 0.26 | 0.22 | 0.55 | 0.19
    | 0.30 |'
- en: '| SVD-LLM | 13.11 ($\downarrow$23%) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM | 13.11 ($\downarrow$23%) |'
- en: '| 50% | SVD | 131715 | 87227 | 79815 | 0.16 | 0.26 | 0.50 | 0.26 | 0.23 | 0.52
    | 0.19 | 0.30 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 50% | SVD | 131715 | 87227 | 79815 | 0.16 | 0.26 | 0.50 | 0.26 | 0.23 | 0.52
    | 0.19 | 0.30 |'
- en: '| FWSVD | 24391 | 28321 | 23104 | 0.12 | 0.26 | 0.50 | 0.26 | 0.23 | 0.53 |
    0.20 | 0.30 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| FWSVD | 24391 | 28321 | 23104 | 0.12 | 0.26 | 0.50 | 0.26 | 0.23 | 0.53 |
    0.20 | 0.30 |'
- en: '| ASVD | 15358 | 47690 | 27925 | 0.12 | 0.26 | 0.51 | 0.26 | 0.22 | 0.52 |
    0.19 | 0.30 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ASVD | 15358 | 47690 | 27925 | 0.12 | 0.26 | 0.51 | 0.26 | 0.22 | 0.52 |
    0.19 | 0.30 |'
- en: '| SVD-LLM | 23.97 ($\downarrow$10%) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM | 23.97 ($\downarrow$10%) |'
- en: '| 60% | SVD | 105474 | 79905 | 106976 | 0.16 | 0.26 | 0.50 | 0.26 | 0.22 |
    0.52 | 0.21 | 0.30 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 60% | SVD | 105474 | 79905 | 106976 | 0.16 | 0.26 | 0.50 | 0.26 | 0.22 |
    0.52 | 0.21 | 0.30 |'
- en: '| FWSVD | 32194 | 43931 | 29292 | 0.15 | 0.26 | 0.49 | 0.26 | 0.22 | 0.53 |
    0.18 | 0.30 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| FWSVD | 32194 | 43931 | 29292 | 0.15 | 0.26 | 0.49 | 0.26 | 0.22 | 0.53 |
    0.18 | 0.30 |'
- en: '| ASVD | 57057 | 45218 | 43036 | 0.12 | 0.26 | 0.49 | 0.26 | 0.21 | 0.51 |
    0.18 | 0.29 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ASVD | 57057 | 45218 | 43036 | 0.12 | 0.26 | 0.49 | 0.26 | 0.21 | 0.51 |
    0.18 | 0.29 |'
- en: '| SVD-LLM | 53.74 ($\downarrow$7%) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM | 53.74 ($\downarrow$7%) |'
- en: 'First, we evaluate the performance of LLaMA-7B compressed by SVD-LLM and the
    baselines under 20% to 60% compression ratios. [Section 4.1](#S4.SS1 "4.1 Overall
    Performance ‣ 4 Experiments ‣ SVD-LLM: Truncation-aware Singular Value Decomposition
    for Large Language Model Compression") summarizes the results on all $10$ datasets.
    As shown, SVD-LLM consistently outperforms vanilla SVD, FWSVD and ASVD across
    all of the compression ratios. More importantly, compared to the low compression
    ratio scenario in [Section 4.1](#S4.SS1 "4.1 Overall Performance ‣ 4 Experiments
    ‣ SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model
    Compression"), SVD-LLM exhibits significant advantages over vanilla SVD, FWSVD,
    and ASVD under high compression ratios. Specifically, under 30% compression ratio,
    compared to the best-performing baseline (ASVD), SVD-LLM reduces the perplexity
    on WikiText-2, PTB, and C4 by 81%, 62%, and 39%, respectively; When the compression
    ratio reaches 40% and above, SVD-LLM reduces the perplexity by more than 96%.
    These results indicate that SVD-LLM is more effective in compressing LLMs for
    more resource-constrained devices such as smartphones and IoT devices. On the
    seven classification datasets, SVD-LLM performs better than the best-performing
    baseline on most of the datasets and consistently achieves at least 2% higher
    average accuracy across all the compression ratios.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，我们评估了在20%到60%压缩比下，LLaMA-7B经SVD-LLM压缩后的性能以及基准方法的表现。[第4.1节](#S4.SS1 "4.1 Overall
    Performance ‣ 4 Experiments ‣ SVD-LLM: Truncation-aware Singular Value Decomposition
    for Large Language Model Compression")总结了在所有$10$个数据集上的结果。如图所示，SVD-LLM在所有压缩比下始终优于普通SVD、FWSVD和ASVD。更重要的是，与[第4.1节](#S4.SS1
    "4.1 Overall Performance ‣ 4 Experiments ‣ SVD-LLM: Truncation-aware Singular
    Value Decomposition for Large Language Model Compression")中低压缩比的情况相比，SVD-LLM在高压缩比下相比普通SVD、FWSVD和ASVD展现出显著优势。具体来说，在30%压缩比下，与表现最佳的基准（ASVD）相比，SVD-LLM在WikiText-2、PTB和C4上的困惑度分别减少了81%、62%和39%；当压缩比达到40%及以上时，SVD-LLM将困惑度降低了超过96%。这些结果表明，SVD-LLM在将LLM压缩至资源受限设备（如智能手机和物联网设备）时更为有效。在七个分类数据集中，SVD-LLM在大多数数据集上表现优于表现最佳的基准，并且在所有压缩比下始终实现了至少2%的平均准确率提升。'
- en: 'Performance on Different LLMs. To examine the generability of SVD-LLM across
    different LLMs, we compare the performance between SVD-LLM and the baselines on
    four different models, including OPT-6.7B, LLaMA 2-7B, Mistral-7B, and Vicuna-7B
    under 20% compression ratio on WikiText-2 and the common sense reasoning datasets
    (See [Section A.5](#A1.SS5 "A.5 More Experiments on compressing different LLMs
    ‣ Appendix A Appendix. ‣ SVD-LLM: Truncation-aware Singular Value Decomposition
    for Large Language Model Compression")). The result on WikiText-2 is shown in [Section 4.1](#S4.SS1
    "4.1 Overall Performance ‣ 4 Experiments ‣ SVD-LLM: Truncation-aware Singular
    Value Decomposition for Large Language Model Compression"), SVD-LLM consistently
    outperforms vanilla SVD, FWSVD, and ASVD across all four LLMs. In addition, SVD-LLM
    exhibits more stable performance on different LLM families, especially compared
    to vanilla SVD and FWSVD.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '在不同LLM上的性能。为了检验SVD-LLM在不同LLM上的泛化能力，我们比较了SVD-LLM与基准方法在OPT-6.7B、LLaMA 2-7B、Mistral-7B和Vicuna-7B四种不同模型下的性能，使用20%压缩比在WikiText-2和常识推理数据集上（见[第A.5节](#A1.SS5
    "A.5 More Experiments on compressing different LLMs ‣ Appendix A Appendix. ‣ SVD-LLM:
    Truncation-aware Singular Value Decomposition for Large Language Model Compression")）。WikiText-2上的结果如[第4.1节](#S4.SS1
    "4.1 Overall Performance ‣ 4 Experiments ‣ SVD-LLM: Truncation-aware Singular
    Value Decomposition for Large Language Model Compression")所示，SVD-LLM在所有四种LLM中始终优于普通SVD、FWSVD和ASVD。此外，SVD-LLM在不同LLM系列中表现更为稳定，尤其是相比普通SVD和FWSVD。'
- en: 'Table 2: Perplexity ($\downarrow$) of four different LLMs including OPT-6.7B,
    LLaMA 2-7B, Mistral-7B, and Vicuna-7B under 20% compression ratio on WikiText-2\.
    The relative performance gain compared to the best-performing baseline is marked
    in green color inside bracket.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：在WikiText-2上，OPT-6.7B、LLaMA 2-7B、Mistral-7B和Vicuna-7B四种不同LLM在20%压缩比下的困惑度
    ($\downarrow$)。与表现最佳的基准相比的相对性能提升以绿色标记在括号中。
- en: '| Method | OPT-6.7B | LLaMA 2-7B | Mistral-7B | Vicuna-7B |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Method | OPT-6.7B | LLaMA 2-7B | Mistral-7B | Vicuna-7B |'
- en: '| SVD | 66275 | 18192 | 159627 | 18644 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| SVD | 66275 | 18192 | 159627 | 18644 |'
- en: '| FWSVD | 14559 | 2360 | 6357 | 2758 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| FWSVD | 14559 | 2360 | 6357 | 2758 |'
- en: '| ASVD | 82 | 10.10 | 13.72 | 16.23 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| ASVD | 82 | 10.10 | 13.72 | 16.23 |'
- en: '| SVD-LLM | 16.04 ($\downarrow$58%) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM | 16.04 ($\downarrow$58%) |'
- en: 'Table 3: Perplexity ($\downarrow$) of LLaMA-7B, 13B, 30B, 65B under 20% compression
    ratio on WikiText-2\. Some baselines’ results are not available due to running
    out of memory (OOM) during model compression. The relative performance gain compared
    to the best-performing baseline is marked in green color inside bracket.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：LLaMA-7B、13B、30B、65B 在 WikiText-2 上以 20% 压缩比的困惑度 ($\downarrow$)。由于模型压缩过程中内存溢出
    (OOM)，某些基准结果不可用。与最佳基准模型相比的相对性能提升在括号内用绿色标记。
- en: '| Method | LLaMA-7B | LLaMA-13B | LLaMA-30B | LLaMA-65B |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LLaMA-7B | LLaMA-13B | LLaMA-30B | LLaMA-65B |'
- en: '| SVD | 20061 | 946.31 | 54.11 | 11.27 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| SVD | 20061 | 946.31 | 54.11 | 11.27 |'
- en: '| FWSVD | 1630 | OOM | OOM | OOM |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| FWSVD | 1630 | OOM | OOM | OOM |'
- en: '| ASVD | 11.14 | 6.74 | 22.71 | OOM |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ASVD | 11.14 | 6.74 | 22.71 | OOM |'
- en: '| SVD-LLM | 7.94 ($\downarrow$42%) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM | 7.94 ($\downarrow$42%) |'
- en: 'Performance on LLMs with Larger Scales. To examine the generability of SVD-LLM
    on LLMs across different scales, we compare the performance between SVD-LLM and
    the baselines on LLaMA series at four different scales – 7B, 13B, 30B, and 65B
    – under 20% compression ratio on WikiText-2. As shown in [Section 4.1](#S4.SS1
    "4.1 Overall Performance ‣ 4 Experiments ‣ SVD-LLM: Truncation-aware Singular
    Value Decomposition for Large Language Model Compression"), SVD-LLM consistently
    outperforms vanilla SVD, FWSVD, and ASVD across all four model sizes. Moreover,
    both FWSVD and ASVD demand excessive memory resources, causing out of memory (OOM)
    when compressing LLMs at larger scales even on an A100 GPU due to memory-intensive
    operations for estimating the importance of weight matrices. In contrast, SVD-LLM
    does not involve such estimation operations and thus avoids OOM.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模语言模型的性能。在不同规模的 LLM 上检查 SVD-LLM 的通用性，我们比较了 SVD-LLM 与基准模型在 LLaMA 系列的四种不同规模——7B、13B、30B
    和 65B——在 20% 压缩比下的表现。如图所示，SVD-LLM 在所有四种模型规模中均优于普通 SVD、FWSVD 和 ASVD。此外，由于在 A100
    GPU 上进行大规模 LLM 压缩时需要大量内存来估算权重矩阵的重要性，FWSVD 和 ASVD 需要过多的内存资源，导致内存溢出 (OOM)。相比之下，SVD-LLM
    不涉及这种估算操作，从而避免了 OOM。
- en: 4.2 Compression Speed Evaluation
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 压缩速度评估
- en: 'Besides compression performance, we also evaluate the compression speed of
    SVD-LLM and the baselines. Specifically, we measured the GPU hours used for SVD-LLM
    and ASVD when compressing LLaMA-7B under the 20% compression ratio on an A100
    GPU. The results are shown in [Section 4.2](#S4.SS2 "4.2 Compression Speed Evaluation
    ‣ 4 Experiments ‣ SVD-LLM: Truncation-aware Singular Value Decomposition for Large
    Language Model Compression"). As shown, ASVD takes about $5.5$ times faster. When
    breaking down the time, most of the time consumed by ASVD is dedicated to calculating
    the compression ratio of each weight matrix based on its estimated importance
    through a search process. In contrast, SVD-LLM maintains a consistent compression
    ratio among all weight matrices and thus gets rid of the time-consuming search
    process.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了压缩性能，我们还评估了 SVD-LLM 和基准模型的压缩速度。具体而言，我们测量了在 A100 GPU 上以 20% 压缩比压缩 LLaMA-7B
    时，SVD-LLM 和 ASVD 使用的 GPU 小时。结果显示在 [第 4.2 节](#S4.SS2 "4.2 压缩速度评估 ‣ 4 实验 ‣ SVD-LLM：用于大规模语言模型压缩的截断感知奇异值分解")。如图所示，ASVD
    的速度快了约 $5.5$ 倍。细分时间后，大部分时间都花在了通过搜索过程计算每个权重矩阵的压缩比上。相比之下，SVD-LLM 在所有权重矩阵中保持一致的压缩比，从而避免了耗时的搜索过程。
- en: 'Table 4: Compression time of SVD-LLM and ASVD on LLaMA-7B under 20% compression
    ratio. The relative speedup is marked in green color inside bracket.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：SVD-LLM 和 ASVD 在 LLaMA-7B 上以 20% 压缩比进行压缩的时间。相对加速比在括号内用绿色标记。
- en: '| Metric | SVD-LLM | ASVD |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | SVD-LLM | ASVD |'
- en: '| White | Update | Total | Normalize | Search | Total |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 白色 | 更新 | 总计 | 归一化 | 搜索 | 总计 |'
- en: '| Time | 10min | 5min | 15min ($\downarrow$95%) | 5min | 5.5h | 5.5h |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 时间 | 10分钟 | 5分钟 | 15分钟 ($\downarrow$95%) | 5分钟 | 5.5小时 | 5.5小时 |'
- en: 'Table 5: Performance of LLaMA-7B compressed by SVD-LLM under 20% and 30% compression
    ratios using calibration data randomly sampled from WikiText-2 (by default in
    our paper) and C4\. The performance on WikiText-2, PTB, and C4 is reported by
    perplexity ($\downarrow$). The relative performance drop (gain) for data sampled
    from C4 compared to that sampled from WikiText-2 is marked in red (green) color
    inside bracket.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：使用从 WikiText-2（我们论文中默认的）和 C4 随机抽样的校准数据，在 20% 和 30% 压缩比下由 SVD-LLM 压缩的 LLaMA-7B
    的性能。WikiText-2、PTB 和 C4 上的性能由困惑度 ($\downarrow$) 报告。与 WikiText-2 抽样数据相比，C4 数据的相对性能下降（提升）在括号内用红色（绿色）标记。
- en: '| Ratio | WikiText-2 | PTB | C4 | Openb. | HellaS. |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 比率 | WikiText-2 | PTB | C4 | Openb. | HellaS. |'
- en: '| Calibration data sampled from WikiText-2 (seed=3) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 从WikiText-2采样的校准数据（种子=3） |'
- en: '| 20% | 7.94 | 16.22 | 15.84 | 0.22 | 0.43 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 20% | 7.94 | 16.22 | 15.84 | 0.22 | 0.43 |'
- en: '| 30% | 9.56 | 26.39 | 25.11 | 0.20 | 0.37 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 9.56 | 26.39 | 25.11 | 0.20 | 0.37 |'
- en: '| Calibration data sampled from C4 (seed=3) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 从C4采样的校准数据（种子=3） |'
- en: '| 20% | 8.62($\uparrow$5%) | 0.43 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 20% | 8.62($\uparrow$5%) | 0.43 |'
- en: '| 30% | 10.67($\uparrow$14%) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 10.67($\uparrow$14%) |'
- en: 4.3 Ablation Study
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: 'Table 6: Perplexity ($\downarrow$) of compressed LLaMA-7B on WikiText-2\. SVD-LLM
    (W) denotes the version of SVD-LLM with truncation-aware data whitening only;
    SVD-LLM (U) denotes the version of SVD-LLM with layer-wise closed-form update
    only; SVD-LLM (W+U) denotes the version of SVD-LLM with both truncation-aware
    data whitening and layer-wise closed-form update. The relative performance gain
    compared to ASVD is marked in green color.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：WikiText-2上压缩后的LLaMA-7B的困惑度（$\downarrow$）。SVD-LLM (W) 表示仅具有截断感知数据白化的SVD-LLM版本；SVD-LLM
    (U) 表示仅具有逐层封闭形式更新的SVD-LLM版本；SVD-LLM (W+U) 表示具有截断感知数据白化和逐层封闭形式更新的SVD-LLM版本。相对于ASVD的相对性能提升用绿色标出。
- en: '| Method | 20% | 30% | 40% | 50% | 60% |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 20% | 30% | 40% | 50% | 60% |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| ASVD | 11.14 | 51 | 1407 | 15358 | 57057 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ASVD | 11.14 | 51 | 1407 | 15358 | 57057 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SVD-LLM (W) | 7.94 ($\downarrow$99%) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM (W) | 7.94 ($\downarrow$99%) |'
- en: '| SVD-LLM (U) | 9.54 ($\downarrow$99%) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM (U) | 9.54 ($\downarrow$99%) |'
- en: '| SVD-LLM (W+U) | 8.25 ($\downarrow$99%) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM (W+U) | 8.25 ($\downarrow$99%) |'
- en: '| SVD-LLM | 7.94 ($\downarrow$99%) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM | 7.94 ($\downarrow$99%) |'
- en: 'Modular Sensitivity Study: We conduct ablation studies to evaluate the separate
    contributions of the two key components (truncation-aware data whitening and layer-wise
    closed-form update) of SVD-LLM. Let SVD-LLM (W) denote the version of SVD-LLM
    with truncation-aware data whitening only; SVD-LLM (U) denote the version of SVD-LLM
    with layer-wise closed-form update only; and SVD-LLM (W+U) denote the version
    of SVD-LLM with both truncation-aware data whitening and layer-wise closed-form
    update. The results are shown in [Section 4.3](#S4.SS3 "4.3 Ablation Study ‣ 4
    Experiments ‣ SVD-LLM: Truncation-aware Singular Value Decomposition for Large
    Language Model Compression"). We have three observations. (1) Both SVD-LLM (W)
    and SVD-LLM (U) consistently outperform ASVD across all the compression ratios.
    Notably, when the compression ratio is at and above 40%, both variants reduce
    the perplexity by more than 99% compared to ASVD. (2) Under 20% and 30% compression
    ratios, SVD-LLM (W) achieves the lowest perplexity compared to SVD-LLM (U) and
    SVD-LLM (W+U). (3) Under 40%, 50% and 60% compression ratios, SVD-LLM (W+U) achieves
    the lowest perplexity compared to SVD-LLM (W) and SVD-LLM (U), highlighting the
    importance of combining both truncation-aware data whitening and layer-wise closed-form
    update when compression ratio goes high.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '模块化敏感性研究：我们进行消融研究，以评估SVD-LLM两个关键组件（具有截断感知数据白化的和逐层封闭形式更新）的单独贡献。让SVD-LLM (W)
    表示仅具有截断感知数据白化的SVD-LLM版本；SVD-LLM (U) 表示仅具有逐层封闭形式更新的SVD-LLM版本；SVD-LLM (W+U) 表示具有截断感知数据白化和逐层封闭形式更新的SVD-LLM版本。结果见于 [第4.3节](#S4.SS3
    "4.3 Ablation Study ‣ 4 Experiments ‣ SVD-LLM: Truncation-aware Singular Value
    Decomposition for Large Language Model Compression")。我们有三点观察结果。(1) SVD-LLM (W)
    和 SVD-LLM (U) 在所有压缩比下均显著优于ASVD。特别是，当压缩比达到或超过40%时，两种变体相较于ASVD减少了超过99%的困惑度。(2) 在20%和30%的压缩比下，SVD-LLM
    (W) 相比于SVD-LLM (U) 和 SVD-LLM (W+U) 实现了最低的困惑度。(3) 在40%、50%和60%的压缩比下，SVD-LLM (W+U)
    相比于SVD-LLM (W) 和 SVD-LLM (U) 实现了最低的困惑度，突显了在高压缩比时结合截断感知数据白化和逐层封闭形式更新的重要性。'
- en: 'Calibration Data Analysis: We next analyze the impact of calibration data used
    for both truncation-aware data whitening and layer-wise closed-form update on
    the compression performance. LABEL:fig:cali and [Section 4.2](#S4.SS2 "4.2 Compression
    Speed Evaluation ‣ 4 Experiments ‣ SVD-LLM: Truncation-aware Singular Value Decomposition
    for Large Language Model Compression") summarize the performance of compressed
    LLaMA-7B when changing three key characteristics of the calibration data, the
    number of the calibration data, the seed used to randomly sample the calibration
    data, and the data set from which the calibration data is sampled. As shown, changing
    any of the three characteristics only causes a tiny disturbance of less than 15%
    to the final performance, demonstrating that SVD-LLM is less sensitive to the
    design of the calibration data to compress LLM.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '校准数据分析：接下来，我们分析用于截断感知数据白化和层级闭式更新的校准数据对压缩性能的影响。LABEL:fig:cali 和 [Section 4.2](#S4.SS2
    "4.2 Compression Speed Evaluation ‣ 4 Experiments ‣ SVD-LLM: Truncation-aware
    Singular Value Decomposition for Large Language Model Compression") 总结了在更改校准数据的三个关键特征——校准数据的数量、用于随机采样校准数据的种子以及从中采样校准数据的数据集——时压缩后的
    LLaMA-7B 的性能。如所示，更改这三种特征中的任何一种仅对最终性能造成不到 15% 的微小干扰，证明了 SVD-LLM 对校准数据设计的敏感性较低。'
- en: 4.4 Benefits to other LLM Compression Methods
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 其他 LLM 压缩方法的好处
- en: SVD-LLM is orthogonal to other LLM compression methods including quantization
    and parameter pruning. In this experiment, we combine SVD-LLM with quantization
    and parameter pruning-based LLM compression methods that are widely recognized
    by the community to examine how SVD-LLM could further enhance their performance.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: SVD-LLM 与其他 LLM 压缩方法（包括量化和参数剪枝）是正交的。在本实验中，我们将 SVD-LLM 与社区广泛认可的基于量化和参数剪枝的 LLM
    压缩方法结合，以检查 SVD-LLM 如何进一步提升这些方法的性能。
- en: 'Integrate SVD-LLM with Quantization. We select GPTQ [[6](#bib.bib6)] as the
    quantization method. Specifically, we compress LLaMA-7B by GPTQ-4bit combined
    with SVD-LLM, and compare the compressed model against LLaMA-7B compressed by
    GPTQ-3bit. As shown in [Section 4.4](#S4.SS4 "4.4 Benefits to other LLM Compression
    Methods ‣ 4 Experiments ‣ SVD-LLM: Truncation-aware Singular Value Decomposition
    for Large Language Model Compression"), combining GPTQ-4bit with SVD-LLM achieves
    a perplexity that is $18\%$ lower than GPTQ-3bit even with a smaller memory footprint
    (2.1 GB vs. 2.8 GB). This result demonstrates that compared to directly quantizing
    using smaller number of bits, GPTQ achieves better compression performance with
    the help of SVD-LLM.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '将 SVD-LLM 与量化集成。我们选择 GPTQ [[6](#bib.bib6)] 作为量化方法。具体而言，我们使用 GPTQ-4bit 结合 SVD-LLM
    对 LLaMA-7B 进行压缩，并将压缩后的模型与使用 GPTQ-3bit 压缩的 LLaMA-7B 进行比较。如 [Section 4.4](#S4.SS4
    "4.4 Benefits to other LLM Compression Methods ‣ 4 Experiments ‣ SVD-LLM: Truncation-aware
    Singular Value Decomposition for Large Language Model Compression") 所示，结合 GPTQ-4bit
    和 SVD-LLM 的困惑度比 GPTQ-3bit 低 $18\%$，即使内存占用更小（2.1 GB 对比 2.8 GB）。这一结果表明，与直接使用更少位数进行量化相比，借助
    SVD-LLM，GPTQ 实现了更好的压缩性能。'
- en: 'Integrate SVD-LLM with Parameter Pruning. We select LLM-Pruner [[16](#bib.bib16)]
    as the parameter pruning method. Specifically, we compress LLaMA-7B by LLM-Pruner
    under 30% compression ratio combined with SVD-LLM, and compare the compressed
    model against LLaMA-7B compressed by LLM-Pruner under 40% compression ratio. As
    shown in Table [4.4](#S4.SS4 "4.4 Benefits to other LLM Compression Methods ‣
    4 Experiments ‣ SVD-LLM: Truncation-aware Singular Value Decomposition for Large
    Language Model Compression"), LLM-Pruner achieves better compression performance
    when used in conjunction with SVD-LLM. In particular, with the same memory footprint
    of 8.8 GB, combining LLM-Pruner under 30% compression ratio with SVD-LLM achieves
    a perplexity that is $13\%$ lower than LLM-Pruner under 40% compression ratio.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '将 SVD-LLM 与参数剪枝集成。我们选择 LLM-Pruner [[16](#bib.bib16)] 作为参数剪枝方法。具体而言，我们使用 LLM-Pruner
    对 LLaMA-7B 进行 30% 压缩比的压缩，并结合 SVD-LLM，然后将压缩后的模型与 LLM-Pruner 在 40% 压缩比下压缩的 LLaMA-7B
    进行比较。如表 [4.4](#S4.SS4 "4.4 Benefits to other LLM Compression Methods ‣ 4 Experiments
    ‣ SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model
    Compression") 所示，当与 SVD-LLM 一起使用时，LLM-Pruner 实现了更好的压缩性能。特别是，在相同的 8.8 GB 内存占用下，结合
    30% 压缩比的 LLM-Pruner 和 SVD-LLM 的困惑度比 40% 压缩比的 LLM-Pruner 低 $13\%$。'
- en: 'Table 7: Perplexity ($\downarrow$) of LLaMA-7B compressed by GPTQ w/ and w/o
    SVD-LLM on WikiText-2\. The relative performance gain of combined compression
    compared to GPTQ-3bit is marked in green color inside bracket.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '表7: 通过GPTQ压缩的LLaMA-7B在WikiText-2上的困惑度 ($\downarrow$)，带有和不带SVD-LLM。与GPTQ-3bit相比，结合压缩的相对性能增益以绿色标记在括号内。'
- en: '| Metric | GPTQ-4bit | GPTQ-3bit | SVD-LLM + GPTQ-4bit |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | GPTQ-4bit | GPTQ-3bit | SVD-LLM + GPTQ-4bit |'
- en: '| --- | --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Memory | 3.9 GB | 2.8 GB | 2.1 GB |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 内存 | 3.9 GB | 2.8 GB | 2.1 GB |'
- en: '| Perplexity | 6.21 | 16.28 | 13.29 ($\downarrow$18%) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度 | 6.21 | 16.28 | 13.29 ($\downarrow$18%) |'
- en: 'Table 8: Perplexity ($\downarrow$) of LLaMA-7B compressed by LLM-Pruner w/
    and w/o SVD-LLM on WikiText-2\. The relative performance gain of combined compression
    compared to LLM-Pruner under 40% compression ratio is marked in green color.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '表8: 通过LLM-Pruner压缩的LLaMA-7B在WikiText-2上的困惑度 ($\downarrow$)，带有和不带SVD-LLM。与LLM-Pruner在40%压缩比下相比，结合压缩的相对性能增益以绿色标记。'
- en: '| Metric | LLM-Pruner-30% | LLM-Pruner-40% |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | LLM-Pruner-30% | LLM-Pruner-40% |'
- en: '&#124; LLM-Pruner-30% &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LLM-Pruner-30% &#124;'
- en: '&#124; + SVD-LLM &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + SVD-LLM &#124;'
- en: '|'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Memory | 9.8 GB | 8.8 GB | 8.8 GB |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 内存 | 9.8 GB | 8.8 GB | 8.8 GB |'
- en: '| Perplexity | 9.88 | 12.21 | 10.58 ($\downarrow$13%) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度 | 9.88 | 12.21 | 10.58 ($\downarrow$13%) |'
- en: 4.5 Benefits of Inference Speedup
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 推断加速的好处
- en: 'SVD-LLM is capable to achieve inference speedup. To demonstrate this advantage,
    we measure the number of tokens that the original LLaMA-7B and its compressed
    version by SVD-LLM can generate on average per second with different batch size
    and sequence length.  LABEL:fig:gpu_throughput show the results on the GPU and
    CPU. As shown, SVD-LLM consistently ensures an acceleration in the generation
    speed across all the compression ratios illustrated in the figure. More importantly,
    this enhancement becomes more significant as the batch size increases and the
    sequence length decreases, resulting in a maximum speedup of 1.7x on GPU and 1.5x
    on CPU under the 40% compression ratio, where the model performance remains acceptable
    according to [Section 4.1](#S4.SS1 "4.1 Overall Performance ‣ 4 Experiments ‣
    SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model
    Compression"). These results highlight the effectiveness of SVD-LLM in improving
    the efficiency of LLM for real-world usage.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 'SVD-LLM能够实现推断加速。为了展示这一优势，我们测量了原始的LLaMA-7B和其通过SVD-LLM压缩版本在不同批量大小和序列长度下每秒生成的平均令牌数。LABEL:fig:gpu_throughput展示了在GPU和CPU上的结果。如图所示，SVD-LLM始终确保了在所有压缩比下生成速度的加速。更重要的是，这种提升在批量大小增加和序列长度减少时变得更为显著，在40%压缩比下，GPU上的最大加速为1.7倍，CPU上的最大加速为1.5倍，在此情况下，模型性能仍然在[第4.1节](#S4.SS1
    "4.1 Overall Performance ‣ 4 Experiments ‣ SVD-LLM: Truncation-aware Singular
    Value Decomposition for Large Language Model Compression")中被认为是可以接受的。这些结果突出了SVD-LLM在提升LLM实际使用效率方面的有效性。'
- en: '![[Uncaptioned image]](img/d6eaab37e4e2484842ea532db821cdc8.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/d6eaab37e4e2484842ea532db821cdc8.png)'
- en: '![Refer to caption](img/23f889cf17df3c5645052bdf3bc411dd.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/23f889cf17df3c5645052bdf3bc411dd.png)'
- en: 'Figure 10: Peak memory to generate 128 tokens with batch size of 32 using LLaMA-7B
    compressed by SVD-LLM under different compression ratios w/ and w/o KV-cache compression.
    The difference between the blue and yellow bars marked in red indicates the reduced
    footprint of the KV cache.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '图10: 使用SVD-LLM压缩的LLaMA-7B在不同压缩比下生成128个令牌的峰值内存，批量大小为32，带有和不带KV缓存压缩。蓝色和黄色条之间以红色标记的差异表示KV缓存的减少的占用空间。'
- en: 4.6 Benefits of KV Cache Compression
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 KV缓存压缩的好处
- en: SVD-LLM is able to not only compress LLMs but also compress the runtime KV cache
    at the same time. Specifically, instead of keeping the original intermediate state
    matrix $m=WX$ without accuracy drop. Therefore, SVD-LLM provides a unified solution
    that combines model compression and KV cache compression into a single process.
    This is different from existing quantization or parameter pruning-based LLM compression
    methods that need to be combined with other techniques for compressing both weights
    and KV cache..
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: SVD-LLM不仅能够压缩LLMs，还能够同时压缩运行时KV缓存。具体而言，SVD-LLM提供了一个统一的解决方案，将模型压缩和KV缓存压缩合并为一个过程，而不需要保持原始的中间状态矩阵
    $m=WX$，且不会出现准确度下降。这与现有的量化或参数修剪基础的LLM压缩方法不同，后者需要结合其他技术来压缩权重和KV缓存。
- en: 'In our last experiment, we evaluate this benefit on KV cache compression brought
    by SVD-LLM. This is a new avenue since KV cache compression has not been evaluated
    in previous LLM compression studies. Specifically, we measure the peak memory
    footprint during inference when generating 128 tokens with batch size of 32 using
    LLaMA-7B compressed by SVD-LLM under different compression ratios w/ and w/o considering
    KV cache compression. The results are illustrated in [Figure 10](#S4.F10 "In 4.5
    Benefits of Inference Speedup ‣ 4 Experiments ‣ SVD-LLM: Truncation-aware Singular
    Value Decomposition for Large Language Model Compression") where the difference
    between the blue and yellow bars marked in red represents the reduced footprint
    of the KV cache. As shown, SVD-LLM is able to effectively reduce the footprint
    of KV cache. Therefore, the peak memory during inference at runtime across all
    the compression ratios.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的最后一个实验中，我们评估了SVD-LLM带来的KV缓存压缩的好处。这是一个新方向，因为KV缓存压缩在以往的LLM压缩研究中未曾被评估。具体来说，我们测量了在生成128个标记时，使用LLaMA-7B通过SVD-LLM压缩的不同压缩比下的推理过程中高峰内存占用。结果如[图10](#S4.F10
    "在4.5推理加速的好处 ‣ 4 实验 ‣ SVD-LLM：截断感知奇异值分解用于大规模语言模型压缩")所示，其中蓝色和黄色条形图标记的红色差异表示KV缓存的减少的占用。如图所示，SVD-LLM能够有效减少KV缓存的占用。因此，在所有压缩比下推理时的内存高峰。
- en: 5 Conclusion
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we presented SVD-LLM, a SVD-based LLM compression method. SVD-LLM
    proposes a novel truncation-aware data whitening strategy to guide which singular
    values to be truncated with minimal compression loss. It also introduces a layer-wise
    closed-form model parameter update scheme to compensate for accuracy degradation
    under high compression ratios. We have demonstrated the effectiveness of SVD-LLM
    on $10$ datasets and seven models from three LLM families at four scales and have
    shown its superiority over state-of-the-arts. We also show its effectiveness in
    further enhancing the performance of other LLM compression methods.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了SVD-LLM，一种基于SVD的LLM压缩方法。SVD-LLM提出了一种新颖的截断感知数据白化策略，以指导在最小压缩损失下截断哪些奇异值。它还引入了一种逐层封闭形式模型参数更新方案，以补偿在高压缩比下的准确性下降。我们已经在$10$个数据集和来自三个LLM家族的七个模型中展示了SVD-LLM的有效性，并且表明它优于现有技术。我们还展示了它在进一步提升其他LLM压缩方法性能方面的有效性。
- en: References
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Amini et al. [2019] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski,
    Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem
    solving with operation-based formalisms. In *NAACL-HLT (1)*, pages 2357–2367\.
    Association for Computational Linguistics, 2019.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amini et al. [2019] 艾达·阿米尼、萨迪亚·加布里埃尔、山川·林、里克·孔塞尔-凯久斯基、叶锦和汉娜赫·哈吉什尔齐。Mathqa：以基于操作的形式主义为目标的可解释数学词题求解。在*NAACL-HLT
    (1)*，第2357–2367页。计算语言学协会，2019。
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao,
    and Yejin Choi. PIQA: reasoning about physical commonsense in natural language.
    In *AAAI*, pages 7432–7439\. AAAI Press, 2020.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisk et al. [2020] 约纳坦·比斯克、罗温·泽勒斯、罗南·勒·布拉斯、简峰·高和叶锦。PIQA：关于自然语言中的物理常识的推理。在*AAAI*，第7432–7439页。AAAI
    Press，2020。
- en: 'Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4
    with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang et al. [2023] 韦林·蒋、朱焕·李、紫林、英盛、张浩·吴、郝章、连敏·郑、思远·庄、永浩·庄、约瑟夫·E·冈萨雷斯、伊昂·斯托伊卡和埃里克·P·邢。Vicuna：一个开源聊天机器人，令GPT-4印象深刻，达到了90%*
    chatgpt质量，2023年3月。URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)。
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the AI2 reasoning challenge. *CoRR*, abs/1803.05457, 2018.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. [2018] 彼得·克拉克、艾萨克·科威、奥伦·埃齐奥尼、图沙尔·科特、阿希什·萨巴尔瓦尔、卡丽莎·肖尼克和欧文·塔福约德。认为你已经解决了问答问题？尝试arc，AI2推理挑战。*CoRR*，abs/1803.05457，2018。
- en: 'Frantar and Alistarh [2023] Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot. In *ICML*, volume 202 of
    *Proceedings of Machine Learning Research*, pages 10323–10337\. PMLR, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 和 Alistarh [2023] 埃利亚斯·弗兰塔和丹·阿利斯塔赫。Sparsegpt：大规模语言模型可以在一次性操作中准确地修剪。在*ICML*，*机器学习研究会论文集*第202卷，第10323–10337页。PMLR，2023。
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. GPTQ: accurate post-training quantization for generative pre-trained
    transformers. *CoRR*, abs/2210.17323, 2022.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan
    Alistarh. GPTQ: 准确的后训练量化用于生成预训练变换器。*CoRR*，abs/2210.17323，2022。'
- en: Gao et al. [2023] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid
    Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model
    evaluation, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. [2023] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid
    Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le
    Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang,
    Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang,
    Anish Thite, Ben Wang, Kevin Wang, 和 Andy Zou. 少样本语言模型评估框架，12 2023。网址 [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)。
- en: 'Golub et al. [1987] G.H. Golub, Alan Hoffman, and G.W. Stewart. A generalization
    of the eckart-young-mirsky matrix approximation theorem. *Linear Algebra and its
    Applications*, 88-89:317–327, 1987. ISSN 0024-3795. doi: https://doi.org/10.1016/0024-3795(87)90114-5.
    URL [https://www.sciencedirect.com/science/article/pii/0024379587901145](https://www.sciencedirect.com/science/article/pii/0024379587901145).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Golub et al. [1987] G.H. Golub, Alan Hoffman, 和 G.W. Stewart. eckart-young-mirsky
    矩阵逼近定理的推广。*线性代数及其应用*，88-89:317–327，1987。ISSN 0024-3795。doi: https://doi.org/10.1016/0024-3795(87)90114-5。网址
    [https://www.sciencedirect.com/science/article/pii/0024379587901145](https://www.sciencedirect.com/science/article/pii/0024379587901145)。'
- en: Gozalo-Brizuela and Garrido-Merchán [2023] Roberto Gozalo-Brizuela and Eduardo C.
    Garrido-Merchán. A survey of generative AI applications. *CoRR*, abs/2306.02781,
    2023.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gozalo-Brizuela 和 Garrido-Merchán [2023] Roberto Gozalo-Brizuela 和 Eduardo C.
    Garrido-Merchán. 生成 AI 应用的综述。*CoRR*，abs/2306.02781，2023。
- en: Gu et al. [2023] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Knowledge distillation
    of large language models. *CoRR*, abs/2306.08543, 2023.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. [2023] Yuxian Gu, Li Dong, Furu Wei, 和 Minlie Huang. 大型语言模型的知识蒸馏。*CoRR*，abs/2306.08543，2023。
- en: Hsieh et al. [2023] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling
    step-by-step! outperforming larger language models with less training data and
    smaller model sizes. In *ACL (Findings)*, pages 8003–8017\. Association for Computational
    Linguistics, 2023.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh et al. [2023] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, 和 Tomas Pfister. 分步蒸馏！用更少的训练数据和更小的模型超越更大的语言模型。在
    *ACL (Findings)*，页 8003–8017。计算语言学协会，2023。
- en: Hsu et al. [2022] Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen,
    and Hongxia Jin. Language model compression with weighted low-rank factorization.
    In *ICLR*. OpenReview.net, 2022.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsu et al. [2022] Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen,
    和 Hongxia Jin. 通过加权低秩分解进行语言模型压缩。在 *ICLR*。OpenReview.net，2022。
- en: 'Hu et al. [2022] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. In *ICLR*. OpenReview.net, 2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. [2022] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. Lora: 大型语言模型的低秩适应。在 *ICLR*。OpenReview.net，2022。'
- en: Jiang et al. [2023] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. Mistral 7b. *CoRR*, abs/2310.06825, 2023.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. [2023] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, 和
    William El Sayed. Mistral 7b。*CoRR*，abs/2310.06825，2023。
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. AWQ: activation-aware weight quantization for LLM compression and
    acceleration. *CoRR*, abs/2306.00978, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. AWQ: 激活感知权重量化用于 LLM 压缩和加速。*CoRR*，abs/2306.00978，2023。'
- en: 'Ma et al. [2023] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. In *NeurIPS*, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma et al. [2023] Xinyin Ma, Gongfan Fang, 和 Xinchao Wang. Llm-pruner: 关于大型语言模型的结构修剪。在
    *NeurIPS*，2023。'
- en: 'Marcus et al. [1993] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
    Building a large annotated corpus of english: The penn treebank. *Comput. Linguistics*,
    19(2):313–330, 1993.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marcus 等人 [1993] Mitchell P. Marcus, Beatrice Santorini 和 Mary Ann Marcinkiewicz。构建大型英语注释语料库：Penn
    Treebank。*Comput. Linguistics*，19(2):313–330，1993年。
- en: Merity et al. [2017] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. In *ICLR (Poster)*. OpenReview.net, 2017.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等人 [2017] Stephen Merity, Caiming Xiong, James Bradbury 和 Richard Socher。Pointer
    Sentinel 混合模型。在 *ICLR (海报)*。OpenReview.net，2017年。
- en: Meyer [2000] Carl Dean Meyer. *Matrix Analysis and Applied Linear Algebra*.
    SIAM, 2000.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meyer [2000] Carl Dean Meyer。*Matrix Analysis and Applied Linear Algebra*。SIAM，2000年。
- en: Mihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? A new dataset for open book
    question answering. In *EMNLP*, pages 2381–2391\. Association for Computational
    Linguistics, 2018.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov 等人 [2018] Todor Mihaylov, Peter Clark, Tushar Khot 和 Ashish Sabharwal。盔甲能导电吗？一个新的开放书籍问答数据集。在
    *EMNLP*，第2381–2391页。计算语言学协会，2018年。
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *J. Mach.
    Learn. Res.*, 21:140:1–140:67, 2020.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li 和 Peter J. Liu。使用统一的文本到文本变换器探索迁移学习的极限。*J.
    Mach. Learn. Res.*，21:140:1–140:67，2020年。
- en: 'Sakaguchi et al. [2020] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    In *AAAI*, pages 8732–8740\. AAAI Press, 2020.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi 等人 [2020] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula 和
    Yejin Choi。Winogrande：规模化的对抗性 Winograd 方案挑战。在 *AAAI*，第8732–8740页。AAAI出版社，2020年。
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models. *CoRR*,
    abs/2302.13971, 2023a.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave 和 Guillaume Lample。Llama：开放和高效的基础语言模型。*CoRR*，abs/2302.13971，2023a年。
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models. *CoRR*, abs/2307.09288, 2023b.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov 和 Thomas Scialom。Llama 2：开放基础和微调聊天模型。*CoRR*，abs/2307.09288，2023b年。
- en: 'Wan et al. [2023] Zhongwei Wan, Xin Wang, et al. Efficient large language models:
    A survey. *arXiv preprint arXiv:2312.03863*, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人 [2023] Zhongwei Wan, Xin Wang 等人。高效的大型语言模型：综述。*arXiv 预印本 arXiv:2312.03863*，2023年。
- en: 'Wang et al. [2024] Xin Wang, Zhongwei Wan, Arvin Hekmati, Mingyu Zong, Samiul
    Alam, Mi Zhang, and Bhaskar Krishnamachari. Iot in the era of generative ai: Vision
    and challenges. *arXiv preprint arXiv:2401.01923*, 2024.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等 [2024] Xin Wang, Zhongwei Wan, Arvin Hekmati, Mingyu Zong, Samiul Alam,
    Mi Zhang, 和 Bhaskar Krishnamachari。在生成AI时代的物联网：愿景与挑战。*arXiv preprint arXiv:2401.01923*，2024年。
- en: 'Xiao et al. [2023] Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *ICML*, volume 202 of *Proceedings of Machine Learning
    Research*, pages 38087–38099\. PMLR, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao等 [2023] Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth,
    和 Song Han。Smoothquant：大语言模型的准确且高效的后训练量化。在*ICML*，第202卷的*机器学习研究会议记录*，第38087–38099页。PMLR，2023年。
- en: 'Yuan et al. [2023] Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan,
    and Guangyu Sun. ASVD: activation-aware singular value decomposition for compressing
    large language models. *CoRR*, abs/2312.05821, 2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan等 [2023] Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, 和 Guangyu
    Sun。ASVD：激活感知奇异值分解，用于压缩大语言模型。*CoRR*，abs/2312.05821，2023年。
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In *ACL
    (1)*, pages 4791–4800\. Association for Computational Linguistics, 2019.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers等 [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, 和 Yejin
    Choi。Hellaswag：机器真的能完成你的句子吗？在*ACL (1)*，第4791–4800页。计算语言学协会，2019年。
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria
    Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained
    transformer language models. *CoRR*, abs/2205.01068, 2022.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等 [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, 和 Luke Zettlemoyer。OPT：开放的预训练变换器语言模型。*CoRR*，abs/2205.01068，2022年。
- en: Zhao et al. [2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan
    Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li,
    Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of
    large language models. *CoRR*, abs/2303.18223, 2023.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等 [2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang,
    Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen
    Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,
    Zikang Liu, Peiyu Liu, Jian-Yun Nie, 和 Ji-Rong Wen。大语言模型的调查。*CoRR*，abs/2303.18223，2023年。
- en: Zhu et al. [2023] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A
    survey on model compression for large language models. *CoRR*, abs/2308.07633,
    2023.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等 [2023] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, 和 Weiping Wang。关于大语言模型的模型压缩调查。*CoRR*，abs/2308.07633，2023年。
- en: Appendix A Appendix.
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A附录。
- en: A.1 The compression error of ASVD
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 ASVD的压缩误差
- en: 'The previous state-of-the-art method ASVD introduced a diagonal scaling matrix
    $S_{0}$:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的最先进方法ASVD引入了对角缩放矩阵$S_{0}$：
- en: '|  | $\displaystyle WS_{0}\approx$ |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle WS_{0}\approx$ |  |'
- en: 'The resulting activation is expressed as:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的激活表示为：
- en: '|  | $\displaystyle Y\approx$ |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Y\approx$ |  |'
- en: 'The compression error $1$2 is demonstrated below:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩误差$1$2如下面所示：
- en: '|  | $\displaystyle L^{2}=$ |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L^{2}=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: which is still a complex function that involves the activation $X$. As a result,
    compression error is not directly related to the singular value, and the conventional
    SVD compression by truncating the smallest singular values may lead to suboptimal
    compression error.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这仍然是一个复杂的函数，涉及到激活$X$。因此，压缩误差与奇异值没有直接关系，而传统的通过截断最小奇异值进行的SVD压缩可能导致次优的压缩误差。
- en: A.2 Pseudocode for SVD-LLM
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 SVD-LLM的伪代码
- en: '[Algorithm 1](#alg1 "In A.2 Pseudocode for SVD-LLM ‣ Appendix A Appendix. ‣
    SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model
    Compression") shows the pseudocode of SVD-LLM. Before compression, SVD-LLM randomly
    collects a small amount of sentences as the calibration data $C$ on each weight
    matrix in the LLM. Instead of directly finishing the whole compression, it stores
    the decomposed matrices and further utilizes these matrices to run the layer-wise
    closed-form update as shown in [Algorithm 3](#alg3 "In A.2 Pseudocode for SVD-LLM
    ‣ Appendix A Appendix. ‣ SVD-LLM: Truncation-aware Singular Value Decomposition
    for Large Language Model Compression").'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[算法 1](#alg1 "在 A.2 SVD-LLM 的伪代码 ‣ 附录 A 附录. ‣ SVD-LLM: 大型语言模型压缩的截断感知奇异值分解")
    显示了 SVD-LLM 的伪代码。在压缩之前，SVD-LLM 随机收集少量句子作为每个权重矩阵的校准数据 $C$。它不会直接完成整个压缩，而是存储分解后的矩阵，并进一步利用这些矩阵运行逐层封闭形式的更新，如
    [算法 3](#alg3 "在 A.2 SVD-LLM 的伪代码 ‣ 附录 A 附录. ‣ SVD-LLM: 大型语言模型压缩的截断感知奇异值分解") 中所示。'
- en: Algorithm 1 Pseudocode for SVD-LLM
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 SVD-LLM 的伪代码
- en: '1:Input: $M$16:end procedure'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '1:输入: $M$16:结束程序'
- en: Algorithm 2 Pseudocode for Truncation-Aware Data Whitening
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 截断感知数据白化的伪代码
- en: '1:Input: $M$13:end procedure'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '1:输入: $M$13:结束程序'
- en: Algorithm 3 Pseudocode for Layer-Wise Closed-Form Update
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 层次封闭形式更新的伪代码
- en: '1:Input: $M$22:end procedure'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '1:输入: $M$22:结束程序'
- en: 'Table 9: Perplexity of LLaMA-7B compressed by SVD-LLM and ASVD (w/ and w/o
    LoRA) on WikiText-2.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 压缩的 LLaMA-7B 在 WikiText-2 上的困惑度（使用和不使用 LoRA）。'
- en: '| Method | 20% | 30% | 40% | 50% | 60% |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 20% | 30% | 40% | 50% | 60% |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| ASVD | 11.14 | 51 | 1407 | 15358 | 57057 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| ASVD | 11.14 | 51 | 1407 | 15358 | 57057 |'
- en: '| ASVD + LoRA | 7.37 | 10.16 | 14.86 | 21.83 | 44.81 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| ASVD + LoRA | 7.37 | 10.16 | 14.86 | 21.83 | 44.81 |'
- en: '| SVD-LLM | 7.94 | 9.56 | 13.11 | 23.97 | 53.74 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM | 7.94 | 9.56 | 13.11 | 23.97 | 53.74 |'
- en: '| SVD-LLM + LoRA | 8.28 | 9.14 | 10.65 | 13.26 | 17.93 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM + LoRA | 8.28 | 9.14 | 10.65 | 13.26 | 17.93 |'
- en: A.3 Performance with LoRA Fine-Tuning.
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 LoRA 微调的性能。
- en: 'LoRA [[13](#bib.bib13)] is a common fine-tuning technique for LLM. It has been
    applied with pruning-based LLM compression methods such as LLM-Pruner [[16](#bib.bib16)]
    to mitigate accuracy drop after pruning. LoRA can also be combined with SVD-based
    LLM compression methods by modifying the forward pass of a linear layer as:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA [[13](#bib.bib13)] 是一种常见的LLM微调技术。它已经与基于剪枝的LLM压缩方法如LLM-Pruner [[16](#bib.bib16)]
    结合应用，以减轻剪枝后的准确度下降。LoRA 还可以通过修改线性层的前向传播与基于SVD的LLM压缩方法结合。
- en: '|  | $\displaystyle Y$ |  | (8) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Y$ |  | (8) |'
- en: where $W_{u}^{\prime}=W_{u}+B_{u}A_{u}$ are low-rank weights fine-tuned using
    LoRA.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{u}^{\prime}=W_{u}+B_{u}A_{u}$ 是使用 LoRA 微调的低秩权重。
- en: 'To examine the performance of SVD-LLM in combination with LoRA, we follow the
    same configuration used in LLM-Pruner [[16](#bib.bib16)] to fine-tune LLaMA-7B
    compressed by SVD-LLM and ASVD under the compression ratios from 20% to 80% with
    LoRA. The results are shown in [Section A.2](#A1.SS2 "A.2 Pseudocode for SVD-LLM
    ‣ Appendix A Appendix. ‣ SVD-LLM: Truncation-aware Singular Value Decomposition
    for Large Language Model Compression"). We have three observations. (1) Comparing
    SVD-LLM with SVD-LLM $+$ LoRA, especially under the compression ratios between
    20% and 60%.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '为了检查 SVD-LLM 与 LoRA 结合的性能，我们按照与 LLM-Pruner [[16](#bib.bib16)] 相同的配置对压缩的 LLaMA-7B
    进行微调，压缩比从 20% 到 80% 并使用 LoRA。结果见 [第 A.2 节](#A1.SS2 "A.2 SVD-LLM 的伪代码 ‣ 附录 A 附录.
    ‣ SVD-LLM: 大型语言模型压缩的截断感知奇异值分解")。我们有三个观察结果。(1) 比较 SVD-LLM 和 SVD-LLM $+$ LoRA，尤其是在
    20% 到 60% 的压缩比下。'
- en: A.4 Generated Content from the Compressed Model
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 从压缩模型生成的内容
- en: 'We also compare some examples of sentences generated by LLaMA-7B compressed
    with SVD-LLM and ASVD in [Table 10](#A1.T10 "In A.4 Generated Content from the
    Compressed Model ‣ Appendix A Appendix. ‣ SVD-LLM: Truncation-aware Singular Value
    Decomposition for Large Language Model Compression"). As shown, the sentences
    generated by the model compressed by SVD-LLM exhibit better fluency, relevance,
    and informativeness compared to that compressed by ASVD. More importantly, when
    the compression ratio is increased to 30%, the previous state-of-the-art method
    ASVD completely loses its generation ability. In contrast, even when the compression
    ratio is up to 40%, SVD-LLM is still capable of generating complete sentences.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还比较了一些通过SVD-LLM和ASVD压缩的LLaMA-7B生成的句子示例，见[表10](#A1.T10 "在A.4压缩模型生成的内容 ‣ 附录A
    附录. ‣ SVD-LLM：用于大语言模型压缩的截断感知奇异值分解")。如图所示，通过SVD-LLM压缩的模型生成的句子在流畅性、相关性和信息量上优于ASVD压缩的模型。更重要的是，当压缩比提高到30%时，之前的最先进方法ASVD完全失去了生成能力。相比之下，即使压缩比达到40%，SVD-LLM仍然能够生成完整的句子。
- en: 'Table 10: An example of contents generated by the compressed LLaMA-7B by SVD-LLM
    and and under different compression ratios. The input is marked in bold and the
    normal texts are the generated sentences.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：通过SVD-LLM压缩的LLaMA-7B在不同压缩比下生成的内容示例。输入部分用**粗体**标记，正常文本是生成的句子。
- en: '| Ratio | SVD-LLM | ASVD |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 比率 | SVD-LLM | ASVD |'
- en: '| --- | --- | --- |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Original | Building a website can be done in 10 simple steps: Step 1: Register
    an domain name; Step 2: Get a hosting package; Step 3: Develop a website; Step
    4: Design the website; Step 5: Make the website live; Step 6: Perform on-page
    SEO; Step 7: Promote the website; Step 8: Evaluate the results; Step 9: Improve
    the website; Step 10: Build a website repeat … |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 原文 | 建立一个网站可以通过10个简单步骤完成：第1步：注册一个域名；第2步：获取一个托管包；第3步：开发一个网站；第4步：设计网站；第5步：使网站上线；第6步：进行页面SEO；第7步：推广网站；第8步：评估结果；第9步：改进网站；第10步：重复建立网站……
    |'
- en: '| 20% | Building a website can be done in 10 simple steps: 1) Identifying an
    appropriate theme. 2 ) Determining which types of Web site would be of interest
    and which could be implemented with minimal overhead and resources. 3 ) Identify
    the audience 4 ) Choose Web site software. 5 ) Select Web site developers. 6 )
    Design Web site 7 ) Identify Web site technology 8 ) Design Web site layout 9
    ) Define Web site aesthetics and 10 ) Define Web site access controls … | Building
    a website can be done in 10 simple steps: 1, Build up your website. You will need
    to select a name for your website. As a beginner, it is a good idea to select
    your company name or product name as your website name. You can use your real
    company or product name or you can create an unusual business name. This will
    make the name of your business name consistent throughout your website. 2, Decide
    on your business name. You will be choosing the official name of your business
    for all activities that represent your business. This includes your business name,
    products, your website, and any … |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 20% | 建立一个网站可以通过10个简单步骤完成：1）确定一个合适的主题。2）确定哪些类型的网站会引起兴趣，哪些可以在最小的开销和资源下实现。3）确定目标受众。4）选择网站软件。5）选择网站开发者。6）设计网站。7）确定网站技术。8）设计网站布局。9）定义网站美学。10）定义网站访问控制……
    | 建立一个网站可以通过10个简单步骤完成：1，建立你的网站。你需要为网站选择一个名字。作为初学者，选择公司名称或产品名称作为你的网站名称是个不错的主意。你可以使用你的真实公司或产品名称，或者创建一个不寻常的业务名称。这将使你的网站名称在整个网站中保持一致。2，决定你的业务名称。你将为所有代表你业务的活动选择正式名称。这包括你的公司名称、产品、网站以及其他……'
- en: '| 30% | Building a website can be done in 10 simple steps: 1\. Preliminaries
    and planning 2\. Designing 3\. Prototyping 4\. Production 5\. Operation… | Building
    a website can be done in 10 simple steps: To: With A Web You Are Into 2 Zim 40
    Wh I M T If Y As An Onl You Orwice There W Os You Hcimp … |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 建立一个网站可以通过10个简单步骤完成：1. 初步准备和规划 2. 设计 3. 原型制作 4. 生产 5. 操作…… | 建立一个网站可以通过10个简单步骤完成：至：用一个网站你进入2
    Zim 40 Wh I M T 如果Y作为一个在线你Orwice那里W Os你Hcimp……'
- en: '| 40% | Building a website can be done in 10 simple steps: 2 days of website
    plan. If you want to set a web site. This is the 1st step. In the 2 days . It
    wants the company to develop a website to help the investors and investors. He
    will have another investor. He wants to help others. The company is working to
    grow, and create a name for some investors and investors. 100 million US dollars
    It is used to build a hotel and a hotel. In order to obtain their cooperation,
    investors, investors, investors in . | Building a website can be done in 10 simple
    steps: Front 1222ED825333 3333333333333333etennenn eenneenneSecondinjustanyanojunatajin
    … |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 40% | 构建网站可以通过10个简单步骤完成：2天的网站计划。如果你想建立一个网站。这是第1步。在这2天内。公司希望开发一个网站来帮助投资者和投资者。他将有另一个投资者。他希望帮助别人。公司正在努力发展，并为一些投资者和投资者创造一个名字。1亿美元用于建造酒店和酒店。为了获得他们的合作，投资者、投资者、投资者。
    | 构建网站可以通过10个简单步骤完成：前1222ED825333 3333333333333333etennenn eenneenneSecondinjustanyanojunatajin
    … |'
- en: A.5 More Experiments on compressing different LLMs
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 更多关于压缩不同 LLMs 的实验
- en: 'Table 11: Zero-shot performance comparison of OPT-6.7B, LLaMA 2-7B, Vicuna-7B,
    Mistral-7B compressed by SVD-LLM and baselines under 20% compression ratio on
    seven common sense reasoning datasets (measured by both individual and average
    accuracy ($\uparrow$)). The best performance is marked in bold. The relative performance
    gain compared to the best-performing baseline is marked in green color inside
    bracket.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 表11：在七个常识推理数据集上，OPT-6.7B、LLaMA 2-7B、Vicuna-7B 和 Mistral-7B 使用 SVD-LLM 压缩后的零-shot性能对比（以单项和平均准确率（$\uparrow$）衡量）。最佳性能用**粗体**标记。与最佳基线相比的相对性能提升在括号内用绿色标记。
- en: '| LLM | Method | Openb. | ARC_e | WinoG. | HellaS. | ARC_c | PIQA | MathQA
    | Average$\uparrow$ |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 方法 | Openb. | ARC_e | WinoG. | HellaS. | ARC_c | PIQA | MathQA | 平均$\uparrow$
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| OPT-6.7B | SVD | 0.14 | 0.27 | 0.51 | 0.25 | 0.23 | 0.54 | 0.21 | 0.31 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7B | SVD | 0.14 | 0.27 | 0.51 | 0.25 | 0.23 | 0.54 | 0.21 | 0.31 |'
- en: '| FWSVD | 0.15 | 0.26 | 0.49 | 0.26 | 0.20 | 0.52 | 0.21 | 0.30 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| FWSVD | 0.15 | 0.26 | 0.49 | 0.26 | 0.20 | 0.52 | 0.21 | 0.30 |'
- en: '| ASVD | 0.14 | 0.40 | 0.51 | 0.30 | 0.20 | 0.59 | 0.22 | 0.34 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| ASVD | 0.14 | 0.40 | 0.51 | 0.30 | 0.20 | 0.59 | 0.22 | 0.34 |'
- en: '| SVD-LLM | 0.24 | 0.60 | 0.60 | 0.45 | 0.28 | 0.73 | 0.24 | 0.45 ($\uparrow$32%)
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM | 0.24 | 0.60 | 0.60 | 0.45 | 0.28 | 0.73 | 0.24 | 0.45 ($\uparrow$32%)
    |'
- en: '| LLaMA 2-7B | SVD | 0.15 | 0.27 | 0.49 | 0.26 | 0.23 | 0.52 | 0.20 | 0.30
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA 2-7B | SVD | 0.15 | 0.27 | 0.49 | 0.26 | 0.23 | 0.52 | 0.20 | 0.30
    |'
- en: '| FWSVD | 0.12 | 0.25 | 0.49 | 0.25 | 0.22 | 0.52 | 0.21 | 0.30 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| FWSVD | 0.12 | 0.25 | 0.49 | 0.25 | 0.22 | 0.52 | 0.21 | 0.30 |'
- en: '| ASVD | 0.25 | 0.31 | 0.60 | 0.41 | 0.32 | 0.72 | 0.23 | 0.41 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| ASVD | 0.25 | 0.31 | 0.60 | 0.41 | 0.32 | 0.72 | 0.23 | 0.41 |'
- en: '| SVD-LLM | 0.26 | 0.50 | 0.60 | 0.41 | 0.26 | 0.66 | 0.23 | 0.41 ($\uparrow$0%)
    |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM | 0.26 | 0.50 | 0.60 | 0.41 | 0.26 | 0.66 | 0.23 | 0.41 ($\uparrow$0%)
    |'
- en: '| Mistral-7B | SVD | 0.14 | 0.25 | 0.52 | 0.26 | 0.23 | 0.54 | 0.20 | 0.30
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | SVD | 0.14 | 0.25 | 0.52 | 0.26 | 0.23 | 0.54 | 0.20 | 0.30
    |'
- en: '| FWSVD | 0.15 | 0.28 | 0.52 | 0.26 | 0.21 | 0.53 | 0.21 | 0.31 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| FWSVD | 0.15 | 0.28 | 0.52 | 0.26 | 0.21 | 0.53 | 0.21 | 0.31 |'
- en: '| ASVD | 0.21 | 0.51 | 0.58 | 0.42 | 0.25 | 0.50 | 0.26 | 0.39 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| ASVD | 0.21 | 0.51 | 0.58 | 0.42 | 0.25 | 0.50 | 0.26 | 0.39 |'
- en: '| SVD-LLM | 0.17 | 0.55 | 0.58 | 0.36 | 0.25 | 0.67 | 0.21 | 0.40 ($\uparrow$3%)
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM | 0.17 | 0.55 | 0.58 | 0.36 | 0.25 | 0.67 | 0.21 | 0.40 ($\uparrow$3%)
    |'
- en: '| Vicuna-7B | SVD | 0.15 | 0.26 | 0.50 | 0.26 | 0.23 | 0.52 | 0.20 | 0.30 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7B | SVD | 0.15 | 0.26 | 0.50 | 0.26 | 0.23 | 0.52 | 0.20 | 0.30 |'
- en: '| FWSVD | 0.14 | 0.27 | 0.49 | 0.26 | 0.22 | 0.53 | 0.20 | 0.30 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| FWSVD | 0.14 | 0.27 | 0.49 | 0.26 | 0.22 | 0.53 | 0.20 | 0.30 |'
- en: '| ASVD | 0.21 | 0.53 | 0.55 | 0.39 | 0.30 | 0.41 | 0.23 | 0.37 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| ASVD | 0.21 | 0.53 | 0.55 | 0.39 | 0.30 | 0.41 | 0.23 | 0.37 |'
- en: '| SVD-LLM | 0.23 | 0.51 | 0.58 | 0.40 | 0.28 | 0.67 | 0.22 | 0.41 ($\uparrow$11%)
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| SVD-LLM | 0.23 | 0.51 | 0.58 | 0.40 | 0.28 | 0.67 | 0.22 | 0.41 ($\uparrow$11%)
    |'
- en: 'We also evaluate the performance of different LLMs, including OPT-6.7B, LLaMA
    2-7B, Mistral-7B, and Vicuna-7B under 20% compression ratio on seven common sense
    reasoning datasets. The results are shown in [Section A.5](#A1.SS5 "A.5 More Experiments
    on compressing different LLMs ‣ Appendix A Appendix. ‣ SVD-LLM: Truncation-aware
    Singular Value Decomposition for Large Language Model Compression"). SVD-LLM performs
    better than the best-performing baseline in most of the datasets across different
    LLMs and even achieves 32% higher average accuracy on OPT-6.7B.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还评估了在七个常识推理数据集上，OPT-6.7B、LLaMA 2-7B、Mistral-7B 和 Vicuna-7B 在20%压缩比下的表现。结果见 [第
    A.5 节](#A1.SS5 "A.5 更多关于压缩不同 LLMs 的实验 ‣ 附录 A 附录。 ‣ SVD-LLM: 大型语言模型压缩的截断感知奇异值分解")。SVD-LLM
    在大多数数据集上表现优于最佳基线，并在 OPT-6.7B 上甚至实现了 32% 的平均准确率提升。'
