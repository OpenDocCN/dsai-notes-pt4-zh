- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:05:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:05:07
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更少即更多：通过增强上下文剪枝提升LLM推理能力
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.08901](https://ar5iv.labs.arxiv.org/html/2312.08901)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.08901](https://ar5iv.labs.arxiv.org/html/2312.08901)
- en: Xijie Huang^(1,2∗)    Li Lyna Zhang^(2‡)   Kwang-Ting Cheng¹    Fan Yang²   
    Mao Yang²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 黄希杰^(1,2∗)    张莉琳^(2‡)   郑光庭¹    杨凡²    杨茂²
- en: ¹Hong Kong University of Science and Technology     ²Microsoft Research
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹香港科技大学     ²微软研究院
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have shown impressive capabilities, yet they still
    struggle with math reasoning. In this work, we propose CoT-Influx, a novel approach
    that pushes the boundary of few-shot Chain-of-Thoughts (CoT) learning to improve
    LLM mathematical reasoning. Motivated by the observation that adding more concise
    CoT examples in the prompt can improve LLM reasoning performance, CoT-Influx employs
    a coarse-to-fine pruner to maximize the input of effective and concise CoT examples.
    The pruner first selects as many crucial CoT examples as possible and then prunes
    unimportant tokens to fit the context window. A math reasoning dataset with diverse
    difficulty levels and reasoning steps is used to train the pruner, along with
    a math-specialized reinforcement learning approach. As a result, by enabling more
    CoT examples with double the context window size in tokens, CoT-Influx significantly
    outperforms various prompting baselines across various LLMs (LLaMA2-7B, 13B, 70B)
    and 5 math datasets, achieving up to 4.55% absolute improvements. Remarkably,
    without any fine-tuning, LLaMA2-70B with CoT-Influx surpasses GPT-3.5 and a wide
    range of larger LLMs (PaLM, Minerva 540B, etc.) on the GSM8K. CoT-Influx serves
    as a plug-and-play module for LLMs and is compatible with most existing reasoning
    prompting techniques, such as self-consistency and self-verification.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）展示了令人印象深刻的能力，但在数学推理方面仍然存在困难。在这项工作中，我们提出了CoT-Influx，这是一种新的方法，通过推动少样本链式思维（CoT）学习的边界来改进LLM的数学推理。我们观察到在提示中添加更多简明的CoT示例可以提高LLM的推理性能，因此CoT-Influx采用了一种粗到细的剪枝器来最大化有效且简明的CoT示例的输入。剪枝器首先选择尽可能多的关键CoT示例，然后剪枝不重要的标记以适应上下文窗口。使用一个具有不同难度水平和推理步骤的数学推理数据集来训练剪枝器，并结合数学专门化的强化学习方法。因此，通过使更多的CoT示例具有双倍的上下文窗口大小（以标记为单位），CoT-Influx显著超越了各种提示基线，适用于不同的LLM（LLaMA2-7B、13B、70B）和5个数学数据集，达到了高达4.55%的绝对提升。值得注意的是，在没有任何微调的情况下，CoT-Influx使LLaMA2-70B超越了GPT-3.5及众多大型LLM（如PaLM、Minerva
    540B等）在GSM8K上的表现。CoT-Influx作为一个即插即用的模块，兼容大多数现有的推理提示技术，如自一致性和自验证。
- en: '^($*$)^($*$)footnotetext: Work was done during the internship at Microsoft
    Research^($\ddagger$)^($\ddagger$)footnotetext: Corresponding author: lzhani@microsoft.com'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ^($*$)^($*$)脚注文本：工作是在微软研究院实习期间完成的^($\ddagger$)^($\ddagger$)脚注文本：通讯作者：lzhani@microsoft.com
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have demonstrated remarkable capabilities across
    a range of tasks Brown et al. ([2020](#bib.bib1)); OpenAI ([2023a](#bib.bib33)).
    However, it remains a significant challenge to improve LLM performance on reasoning
    tasks, especially for smaller LLMs like LLaMA Touvron et al. ([2023a](#bib.bib44))
    on math reasoning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在一系列任务中展现了显著的能力 Brown等人（[2020](#bib.bib1)）；OpenAI（[2023a](#bib.bib33)）。然而，提升LLM在推理任务上的表现仍然是一项重大挑战，尤其是对于像LLaMA
    Touvron等人（[2023a](#bib.bib44)）这样较小的LLM在数学推理方面。
- en: 'While existing efforts focus on optimizing Chain-of-Thought (CoT) prompts Wei
    et al. ([2022](#bib.bib52)); Wang et al. ([2023d](#bib.bib51)); Yao et al. ([2023](#bib.bib58))
    and fine-tuning LLMs Luo et al. ([2023](#bib.bib30)) under the zero-shot setting,
    the potential of few-shot learning in improving LLM reasoning has not been fully
    explored. Inspired by the human reasoning process, we propose the hypothesis:
    if LLMs are exposed to more step-by-step problem-solving examples (i.e., CoTs)
    before answering questions, it could potentially improve LLMs reasoning capability
    to generate a correct solution. This leads to our question: what’s the boundary
    of LLM reasoning capability achievable through inputting more CoT examples?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现有的工作集中于优化Chain-of-Thought (CoT) 提示 *Wei et al.* ([2022](#bib.bib52)); *Wang
    et al.* ([2023d](#bib.bib51)); *Yao et al.* ([2023](#bib.bib58)) 和在零-shot设置下微调LLMs
    *Luo et al.* ([2023](#bib.bib30))，但在提升LLM推理能力方面，少量示例学习的潜力尚未得到充分探索。受到人类推理过程的启发，我们提出了假设：如果LLMs在回答问题之前暴露于更多逐步解决问题的示例（即CoTs），可能会改善LLMs的推理能力，从而生成正确的解决方案。这引出了我们的问题：通过输入更多CoT示例，LLM推理能力的界限是什么？
- en: However, we face two major obstacles. First, the limited token length of LLMs’
    context window restricts the number of few-shot examples. Extending the context
    window is one solution, but it requires expensive fine-tuning and increases inference
    overhead Chen et al. ([2023a](#bib.bib2)); Peng et al. ([2023a](#bib.bib36)).
    While prompt compression Li et al. ([2023b](#bib.bib26)); Jiang et al. ([2023](#bib.bib19))
    is another approach, it underperforms in math reasoning. Tokens like numerical
    and format ones, though identified redundant, are crucial for few-shot math problem
    solving.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们面临两个主要障碍。首先，LLMs上下文窗口的有限令牌长度限制了少量示例的数量。扩展上下文窗口是一种解决方案，但它需要昂贵的微调并增加推理开销
    *Chen et al.* ([2023a](#bib.bib2)); *Peng et al.* ([2023a](#bib.bib36))。虽然提示压缩
    *Li et al.* ([2023b](#bib.bib26)); *Jiang et al.* ([2023](#bib.bib19)) 是另一种方法，但在数学推理中表现不佳。诸如数值和格式令牌之类的令牌，尽管被认为是冗余的，但对于少量示例数学问题解决至关重要。
- en: 'Second, it’s challenging to select helpful CoT examples. Section [3](#S3 "3
    Pilot Study ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning")
    reveals that random choices can even harm reasoning performance. Existing retrieval-based
    methods Liu et al. ([2021](#bib.bib28)); Scarlatos and Lan ([2023](#bib.bib42))
    are not tailored for math reasoning, making them suboptimal. These retrieved examples
    are model-agnostic, while we found that different LLMs favor CoT examples of varying
    characteristics (e.g., diverse difficulty levels).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '其次，选择有用的CoT示例是具有挑战性的。第 [3](#S3 "3 Pilot Study ‣ Fewer is More: Boosting LLM
    Reasoning with Reinforced Context Pruning") 节揭示了随机选择甚至可能损害推理性能。现有的基于检索的方法 *Liu
    et al.* ([2021](#bib.bib28)); *Scarlatos and Lan* ([2023](#bib.bib42)) 并不针对数学推理，因此表现不佳。这些检索的示例是模型无关的，而我们发现不同的LLMs偏爱具有不同特征的CoT示例（例如，多样化的难度级别）。'
- en: In this work, we propose CoT-Influx, which addresses all the above challenges
    and pushes the boundaries of utilizing few-shot learning to improve LLM math reasoning
    capability. CoT-Influx is motivated by the observation that current LLM context
    window has not been fully utilized due to redundancy at both the example and token
    levels in natural language input. As such, these redundant inputs can be pruned
    to free up space for more informative context. The central idea of CoT-Influx
    is to input long lengthy CoT examples, select the crucial examples for the target
    LLM, and then prune redundant tokens to fit within the original LLM context window.
    As a result, by inputting much more helpful CoT examples, each composed solely
    of informative tokens and with a shorter length, we greatly improve LLM ability
    to solve math problems. Moreover, as all these inputs remain within the context
    window, we do not increase any inference overhead. This stands in stark contrast
    to other methods Hao et al. ([2022](#bib.bib15)); Chen et al. ([2023a](#bib.bib2)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了CoT-Influx，它解决了上述所有挑战，并推动了利用少量示例学习来提高LLM数学推理能力的极限。CoT-Influx的动机来源于观察到目前LLM的上下文窗口尚未得到充分利用，因为自然语言输入中在示例和令牌级别存在冗余。因此，这些冗余的输入可以被修剪，以腾出更多空间用于更有用的上下文。CoT-Influx的核心思想是输入较长的CoT示例，选择目标LLM的关键示例，然后修剪冗余令牌，以适应原始LLM上下文窗口。结果，通过输入更多有用的CoT示例，每个示例仅由信息性令牌组成且长度较短，我们显著提高了LLM解决数学问题的能力。此外，由于所有这些输入都保持在上下文窗口内，我们没有增加任何推理开销。这与其他方法
    *Hao et al.* ([2022](#bib.bib15)); *Chen et al.* ([2023a](#bib.bib2)) 的做法形成了鲜明的对比。
- en: 'CoT-Influx treats the target LLM as a black box, and serves as a plug-and-play
    module for LLMs as shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3 Pilot Study ‣ Fewer
    is More: Boosting LLM Reasoning with Reinforced Context Pruning"). The key module
    is a coarse-to-fine pruner involving two steps: (i) a shot pruner first selects
    the most helpful CoT examples from a large batch of shots, and (ii) a token pruner
    then removes unimportant tokens from these selected CoT examples. To effectively
    train the pruner module tailored for math reasoning, CoT-Influx is built upon
    the following novel techniques.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 'CoT-Influx 将目标 LLM 视为一个黑箱，并作为 LLM 的即插即用模块，如图 [3](#S3.F3 "Figure 3 ‣ 3 Pilot
    Study ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning")
    所示。关键模块是一个粗到精的修剪器，包括两个步骤：（i）一个 shot 修剪器首先从大量的 shots 中选择最有帮助的 CoT 示例，（ii）一个 token
    修剪器然后从这些选定的 CoT 示例中移除不重要的标记。为了有效地训练针对数学推理的修剪器模块，CoT-Influx 基于以下新技术构建。'
- en: First, CoT-Influx requires a CoT dataset for training and inference. Existing
    CoT examples, heavily reliant on costly human engineering, often struggle with
    diversity and quality. To address this, we employ GPT-4 OpenAI ([2023a](#bib.bib33))
    and Evol-Instruct Xu et al. ([2023](#bib.bib57)) to create a math reasoning dataset,
    called MRD³. With problems of varying difficulty and reasoning steps, MRD³ enables
    CoT-Influx to generalize across a wide range of math problems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，CoT-Influx 需要一个 CoT 数据集进行训练和推理。现有的 CoT 示例，严重依赖于昂贵的人力工程，通常在多样性和质量上存在困难。为了解决这个问题，我们使用
    GPT-4 OpenAI ([2023a](#bib.bib33)) 和 Evol-Instruct Xu 等人 ([2023](#bib.bib57))
    创建了一个数学推理数据集，称为 MRD³。MRD³ 具有不同难度和推理步骤的问题，使得 CoT-Influx 能够在广泛的数学问题中进行泛化。
- en: 'Second, training the pruner presents two challenges: (1) since we identify
    discrete tokens before the LLM tokenizer, the loss gradient cannot be backpropagated
    through the tokenizer to update the pruner; (2) The high difficulty of many math
    problems, which consistently yield incorrect answers regardless of the quality
    of compressed few-shot examples, poses a challenge to the effective training of
    the pruner. To this end, we introduce a novel training approach with reinforcement
    learning to mitigate the gradient issue. We design a reward function to measure
    the LLM loss, few-shot math reasoning effectiveness, and token length constraints.
    Then, we design a difficulty-aware dataloader filtering appropriate problems and
    introduce two techniques to stabilize the RL training.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，训练修剪器面临两个挑战：（1）由于我们在 LLM 分词器之前识别离散标记，因此损失梯度不能通过分词器反向传播以更新修剪器；（2）许多数学问题的高难度，无论压缩的少量示例质量如何，总是产生不正确的答案，这对修剪器的有效训练构成挑战。为此，我们引入了一种新的强化学习训练方法来缓解梯度问题。我们设计了一个奖励函数来衡量
    LLM 损失、少量数学推理的有效性以及标记长度约束。然后，我们设计了一个难度感知的数据加载器来筛选合适的问题，并引入了两种技术来稳定强化学习训练。
- en: Extensive experiments on various LLMs and five math datasets demonstrate the
    effectiveness of CoT-Influx. CoT-Influx significantly boosts LLM reasoning capability,
    achieving 1.36%-14.09% absolute improvements over SOTA baselines, and establishes
    a new prompting-based benchmark in math reasoning accuracy without any fine-tuning
    or additional inference costs. Remarkably, LLaMA2-70B with CoT-Influx outperforms
    a broad range of larger LLMs and surpasses GPT-3.5 by 2.5% on GSM8K. Moreover,
    CoT-Influx excels over retrieval and prompt compression baselines in example selection
    and identifying crucial tokens.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种 LLM 和五个数学数据集上进行的广泛实验展示了 CoT-Influx 的有效性。CoT-Influx 显著提升了 LLM 的推理能力，相比于 SOTA
    基线取得了 1.36%-14.09% 的绝对提升，并在数学推理准确性方面建立了一个新的基于提示的基准，无需任何微调或额外的推理成本。值得注意的是，配备 CoT-Influx
    的 LLaMA2-70B 超越了广泛的更大范围的 LLM，并在 GSM8K 上超过了 GPT-3.5 2.5%。此外，CoT-Influx 在示例选择和识别关键标记方面优于检索和提示压缩基准。
- en: 2 Related Works
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'LLMs for Math Reasoning. Drawing from the Chain-of-Thought (CoT) Wei et al.
    ([2022](#bib.bib52)), recent research has greatly improved the reasoning capabilities
    of LLMs by providing step-by-step reasoning paths. The main efforts are twofold:
    enhancing CoT prompts, such as Program-of-Thoughts Chen et al. ([2023b](#bib.bib3)),
    Tree-of-Thoughts Yao et al. ([2023](#bib.bib58)), and Everything-of-Thoughts Ding
    et al. ([2023](#bib.bib10)), and innovating CoT-based training data for fine-tuning
    LLMs like WizardMath Luo et al. ([2023](#bib.bib30)).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数学推理的LLMs。借鉴Chain-of-Thought (CoT) Wei等人（[2022](#bib.bib52)），近期研究通过提供逐步推理路径显著提升了LLMs的推理能力。主要的努力有两个方面：一是增强CoT提示，例如Program-of-Thoughts Chen等人（[2023b](#bib.bib3)）、Tree-of-Thoughts Yao等人（[2023](#bib.bib58)）以及Everything-of-Thoughts Ding等人（[2023](#bib.bib10)）；二是创新CoT基础的训练数据，用于微调LLMs，如WizardMath Luo等人（[2023](#bib.bib30)）。
- en: However, most works focus on the zero-shot setting with only task instruction
    or CoT prompts, leaving the potential of few-shot CoT largely untapped. We explore
    leveraging few-shot CoT learning to improve LLMs’ math reasoning capabilities.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数工作集中在只有任务指令或CoT提示的零-shot设置上，未能充分挖掘少量CoT的潜力。我们探讨了利用少量CoT学习来提升LLMs的数学推理能力。
- en: 'Prompt Compression. To address the challenge of limited few-shot examples due
    to restricted context window length, one related work involves prompt compression.
    Key approaches include: (1) token pruning Kim et al. ([2022](#bib.bib20)); Li
    et al. ([2023a](#bib.bib25)); (2) soft prompt compression methods Wingate et al.
    ([2022](#bib.bib55)); Mu et al. ([2023](#bib.bib32)); Chevalier et al. ([2023](#bib.bib4));
    Ge et al. ([2023](#bib.bib13)); and (3) information-entropy-based approaches Li
    et al. ([2023b](#bib.bib26)); Jiang et al. ([2023](#bib.bib19)).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 提示压缩。为了应对由于上下文窗口长度限制而导致的少量示例问题，相关工作涉及提示压缩。主要方法包括：（1）token剪枝 Kim等人（[2022](#bib.bib20)）；Li等人（[2023a](#bib.bib25)）；（2）软提示压缩方法 Wingate等人（[2022](#bib.bib55)）；Mu等人（[2023](#bib.bib32)）；Chevalier等人（[2023](#bib.bib4)）；Ge等人（[2023](#bib.bib13)）；以及（3）基于信息熵的方法 Li等人（[2023b](#bib.bib26)）；Jiang等人（[2023](#bib.bib19)）。
- en: However, they do not effectively solve our problem for two reasons. First, they
    prune tokens based on designed metrics, often failing to remove redundancy of
    the entire CoT examples. Second, some tokens such as numerical and format tokens,
    although redundant, are crucial for math reasoning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于两个原因，它们未能有效解决我们的问题。首先，它们基于设计的度量标准来剪枝token，通常未能去除整个CoT示例的冗余。其次，一些token如数字和格式token，尽管冗余，但对数学推理至关重要。
- en: Prompt Retrieval optimizes task performance by selecting high-quality few-shot
    examples using either heuristics or a supervised retriever model. Heuristic methods,
    such as the widely used TopK retrieval Liu et al. ([2021](#bib.bib28)); Gao et al.
    ([2021](#bib.bib12)), BM25 Robertson et al. ([2009](#bib.bib39)), VoteK Hongjin
    et al. ([2022](#bib.bib17)), and entropy Lu et al. ([2022](#bib.bib29)), select
    examples based on semantic similarity. Recently, supervised-based methods like
    EPR Rubin et al. ([2021](#bib.bib41)), LLM-R Wang et al. ([2023b](#bib.bib49)),
    and IDS Qin et al. ([2023](#bib.bib38)) have been proposed, which train a retrieval
    model to learn better example selection.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 提示检索通过使用启发式或监督检索模型选择高质量的少量示例来优化任务性能。启发式方法，如广泛使用的TopK检索 Liu等人（[2021](#bib.bib28)）；Gao等人（[2021](#bib.bib12)）、BM25 Robertson等人（[2009](#bib.bib39)）、VoteK Hongjin等人（[2022](#bib.bib17)）和熵 Lu等人（[2022](#bib.bib29)），基于语义相似性选择示例。最近，提出了一些基于监督的方法，如EPR Rubin等人（[2021](#bib.bib41)）、LLM-R Wang等人（[2023b](#bib.bib49)）和IDS Qin等人（[2023](#bib.bib38)），这些方法训练一个检索模型以学习更好的示例选择。
- en: However, these methods are sub-optimal for math reasoning, as they retrieve
    model-agnostic examples. In contrast, LLMs with different capabilities favor CoT
    examples of varying complexities. Moreover, they don’t account for token redundancy,
    which restricts the number of retrieved examples.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些方法在数学推理方面效果不佳，因为它们检索的是与模型无关的示例。相比之下，具有不同能力的LLMs倾向于使用复杂度各异的CoT示例。此外，它们没有考虑token的冗余性，这限制了检索示例的数量。
- en: 3 Pilot Study
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 试点研究
- en: '![Refer to caption](img/7b202963638c262b8fd2ac91417fd2f3.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7b202963638c262b8fd2ac91417fd2f3.png)'
- en: 'Figure 1: LLaMA2-7B reasoning accuracy under an increasing number of TopK retrieved
    CoT examples.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLaMA2-7B在逐渐增加的TopK检索CoT示例下的推理准确性。
- en: 'This section presents our key observations of few-shot learning in improving
    LLMs math reasoning, upon which the CoT-Influx design is based. Note that experiments
    are done with our proposed CoT dataset, MRD³, as introduced in Sec. [4.1](#S4.SS1
    "4.1 CoT Dataset Collection ‣ 4 CoT-Influx Methodology ‣ Fewer is More: Boosting
    LLM Reasoning with Reinforced Context Pruning").'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '本节介绍了我们在提升 LLM 数学推理方面的少样本学习的关键观察，这也是 CoT-Influx 设计的基础。请注意，实验使用了我们提出的 CoT 数据集
    MRD³，如第 [4.1](#S4.SS1 "4.1 CoT Dataset Collection ‣ 4 CoT-Influx Methodology ‣
    Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning") 节所述。'
- en: 'Observation 1: LLMs can improve reasoning with more helpful CoT examples, but
    the current context window restricts the number of CoT examples.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 观察 1：LLMs 可以通过更多有帮助的 CoT 示例来改善推理，但当前的上下文窗口限制了 CoT 示例的数量。
- en: 'A standard practice for evaluating LLMs’ math reasoning capability is the use
    of 8-shot manually-designed CoTs Wei et al. ([2022](#bib.bib52)). We increase
    the number of CoT shots to see if reasoning accuracy improves. To avoid poor-quality
    examples, we use the TopK method Liu et al. ([2021](#bib.bib28)) to select the
    $k$The input token length is less than the context window token limit, as the
    answer generation also shares this limit. . As Fig. [1](#S3.F1 "Figure 1 ‣ 3 Pilot
    Study ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning")
    shows, increasing CoT examples improves LLaMA2-7B’s reasoning accuracy on the
    GSM8K dataset, significantly outperforming the standard 8-shot setting. However,
    the limited LLM context window hinders the full potential of few-shot CoT learning
    for improving math reasoning. For instance, even with 20 CoTs not hitting the
    token limit, accuracy drops as the large input context limits the LLM’s response
    space.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '评估 LLM 数学推理能力的标准做法是使用 8-shot 手动设计的 CoTs Wei et al. ([2022](#bib.bib52))。我们增加了
    CoT 示例的数量，以查看推理准确性是否提高。为了避免低质量的示例，我们使用 TopK 方法 Liu et al. ([2021](#bib.bib28))
    选择 $k$。输入标记长度小于上下文窗口标记限制，因为答案生成也受此限制。如图 [1](#S3.F1 "Figure 1 ‣ 3 Pilot Study ‣
    Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning") 所示，增加
    CoT 示例提高了 LLaMA2-7B 在 GSM8K 数据集上的推理准确性，显著超越了标准的 8-shot 设置。然而，有限的 LLM 上下文窗口限制了少样本
    CoT 学习在提升数学推理方面的全部潜力。例如，即使有 20 个 CoTs 未达到标记限制，准确性也会下降，因为较大的输入上下文限制了 LLM 的响应空间。'
- en: 'Observation 2: CoT example selection is crucial for math reasoning. Simply
    adding CoT examples randomly doesn’t boost performance.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 观察 2：CoT 示例的选择对于数学推理至关重要。仅仅随机添加 CoT 示例并不能提升性能。
- en: 'The prior study suggests that more CoT examples can improve LLM reasoning performance.
    However, the quality of CoT examples is crucial to the final performance. As shown
    in Table [1](#S3.T1 "Table 1 ‣ 3 Pilot Study ‣ Fewer is More: Boosting LLM Reasoning
    with Reinforced Context Pruning"), even with up to 16 CoT shots, random selection
    underperforms the standard 8-shot setting, which is manually curated for quality.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '之前的研究表明，更多的 CoT 示例可以提高 LLM 推理表现。然而，CoT 示例的质量对最终表现至关重要。如表格 [1](#S3.T1 "Table
    1 ‣ 3 Pilot Study ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context
    Pruning") 所示，即使有多达 16 个 CoT 示例，随机选择的表现仍不如标准的 8-shot 设置，该设置是经过人工筛选以保证质量的。'
- en: 'Table 1: The selection of CoT examples heavily impacts LLM reasoning performance.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：CoT 示例的选择对 LLM 推理表现有很大影响。
- en: '| Model | Manual 8 Shots | Method (16 Shots) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 手动 8 Shot | 方法 (16 Shot) |'
- en: '| Random 1 | Random 2 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 随机 1 | 随机 2 |'
- en: '| LLaMA2-7B | 13.79 | 12.36 | 13.27 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | 13.79 | 12.36 | 13.27 |'
- en: '| LLaMA2-13B | 27.82 | 23.04 | 23.28 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B | 27.82 | 23.04 | 23.28 |'
- en: 'Observation 3: A CoT example contains redundant tokens for math reasoning,
    which can be pruned to free up space for more informative content.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 观察 3：一个 CoT 示例包含冗余的数学推理标记，这些标记可以被修剪，以腾出更多空间用于更有信息的内容。
- en: 'Observation 2 indicates that few-shot CoT examples contain useless or even
    harmful examples that can be pruned. We further observe that a CoT example often
    has redundant tokens. For instance, the blue tokens in Fig. [2](#S3.F2 "Figure
    2 ‣ 3 Pilot Study ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context
    Pruning") can be removed without affecting LLM performance. However, identifying
    redundant tokens for math reasoning poses a challenge. Simply using existing prompt
    compression methods Jiang et al. ([2023](#bib.bib19)); Li et al. ([2023b](#bib.bib26))
    leads to a significant performance decline. Fig. [2](#S3.F2 "Figure 2 ‣ 3 Pilot
    Study ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning")
    shows a compressed example using LLMLingua Jiang et al. ([2023](#bib.bib19)).
    Some numerical and format tokens (colored in red), while identified as redundant,
    are crucial for LLM to comprehend the context for solving a math problem.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 观察2表明，少量示例CoT包含无用甚至有害的示例，这些示例可以被修剪。我们进一步观察到，CoT示例通常有冗余标记。例如，图[2](#S3.F2 "图2
    ‣ 3试点研究 ‣ 更少即更多：通过强化上下文修剪提升LLM推理")中的蓝色标记可以在不影响LLM性能的情况下删除。然而，识别数学推理中的冗余标记是一个挑战。仅使用现有的提示压缩方法
    Jiang等（[2023](#bib.bib19)）；Li等（[2023b](#bib.bib26)）会导致显著的性能下降。图[2](#S3.F2 "图2
    ‣ 3试点研究 ‣ 更少即更多：通过强化上下文修剪提升LLM推理")显示了使用LLMLingua Jiang等（[2023](#bib.bib19)）的压缩示例。某些数值和格式标记（红色），虽然被识别为冗余，但对LLM理解解决数学问题的上下文至关重要。
- en: '![Refer to caption](img/fdc4499c46ba36546d785a6d0bef1904.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/fdc4499c46ba36546d785a6d0bef1904.png)'
- en: 'Figure 2: A compressed CoT example using the prompt compression tool of LLMLingua Jiang
    et al. ([2023](#bib.bib19)). The pruned tokens contain truly redundant tokens
    (colored in blue) and crucial tokens (colored in red).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：使用LLMLingua Jiang等（[2023](#bib.bib19)）的提示压缩工具的压缩CoT示例。修剪后的标记包含真正的冗余标记（蓝色）和关键标记（红色）。
- en: '![Refer to caption](img/47adeee3397abe11ae5ae0102e62b2e1.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/47adeee3397abe11ae5ae0102e62b2e1.png)'
- en: 'Figure 3: Above: The overview procedure of CoT-Influx; Below: an example illustrating
    the use of CoT-Influx to first prune entire CoT examples and then prune tokens.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：上方：CoT-Influx的概述过程；下方：一个示例，展示了如何使用CoT-Influx首先修剪整个CoT示例，然后修剪标记。
- en: 4 CoT-Influx Methodology
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 CoT-Influx方法论
- en: Motivated by our observations, this section introduces CoT-Influx, which maximizes
    CoT examples within the LLM context window by identifying the most important CoT
    examples and tokens from long lengthy input contexts.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的观察，本节介绍了CoT-Influx，它通过识别从长输入上下文中最重要的CoT示例和标记，最大化LLM上下文窗口中的CoT示例。
- en: 4.1 CoT Dataset Collection
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 CoT数据集收集
- en: We start by collecting a high-quality math reasoning dataset, comprising diverse
    CoT examples with varying steps and difficulties. We merge the training set of
    GSM8K Cobbe et al. ([2021](#bib.bib6)), MAWPS, MAWPS-single Koncel-Kedziorski
    et al. ([2016](#bib.bib23)), and 1000 random examples from AQuA Ling et al. ([2017](#bib.bib27))
    to create an initial dataset of 9.7K question-answer pairs. Then, we prompt GPT-4
    to generate formatted CoT reasoning steps. Notably, it’s crucial to maintain a
    consistent format for each example in few-shot learning. Our dataset also assigns
    a difficulty score from 1 to 10 for each question, based on GPT-4’s evaluation,
    where 1 signifies the easiest questions and 10 is the most difficult.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先收集一个高质量的数学推理数据集，包括具有不同步骤和难度的各种CoT示例。我们合并了GSM8K Cobbe等（[2021](#bib.bib6)）、MAWPS、MAWPS-single
    Koncel-Kedziorski等（[2016](#bib.bib23)）以及来自AQuA Ling等（[2017](#bib.bib27)）的1000个随机示例，创建了一个包含9.7K问题-答案对的初始数据集。然后，我们提示GPT-4生成格式化的CoT推理步骤。值得注意的是，在少量示例学习中，保持每个示例的一致格式至关重要。我们的数据集还根据GPT-4的评估为每个问题分配了从1到10的难度分数，其中1表示最简单的问题，10表示最困难的问题。
- en: We observe that most questions in this initial dataset score between 2-4\. To
    improve difficulty diversity, we use GPT-4 to mutate questions, generating corresponding
    CoTs with varied difficulty levels. We apply 5 mutation schemes, three to increase
    reasoning difficulty and two to simple questions. The final dataset is referred
    to as Math Reasoning Dataset with Diverse Difficulty (MRD³).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，初始数据集中大多数问题的分数在2-4之间。为了提高难度的多样性，我们使用GPT-4变异问题，生成对应的具有不同难度水平的CoT。我们应用了5种变异方案，其中三种用于增加推理难度，两种用于简化问题。最终数据集被称为具有不同难度的数学推理数据集（MRD³）。
- en: 4.2 Problem Formulation
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 问题定义
- en: 'Let $\mathcal{D}$. CoT-Influx is designed to perform a two-step pruning process:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让$\mathcal{D}$。CoT-Influx设计用于执行两步修剪过程：
- en: '|  |  $1$2  |  | (1) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |  $1$2  |  | (1) |'
- en: Initially, non-useful CoT examples are pruned from $\hat{\mathcal{D}}$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，从 $\hat{\mathcal{D}}$ 中修剪掉无用的 CoT 示例。
- en: 'Let $x^{\text{question}}$. Formally, we optimize the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让 $x^{\text{question}}$。正式地，我们优化以下内容：
- en: '|  |  $1$2  |  | (2) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  |  $1$2  |  | (2) |'
- en: 'where $L_{\text{LLM}}$, this will be elaborated in Sec. [4.4](#S4.SS4 "4.4
    End-to-end RL Optimization ‣ 4 CoT-Influx Methodology ‣ Fewer is More: Boosting
    LLM Reasoning with Reinforced Context Pruning").'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $L_{\text{LLM}}$，将在第 [4.4](#S4.SS4 "4.4 End-to-end RL Optimization ‣ 4 CoT-Influx
    Methodology ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning")
    节中详细说明。'
- en: 'Overview. Fig. [3](#S3.F3 "Figure 3 ‣ 3 Pilot Study ‣ Fewer is More: Boosting
    LLM Reasoning with Reinforced Context Pruning") illustrates our approach. The
    core component is a lightweight, plug-and-play module (Sec. [4.3](#S4.SS3 "4.3
    Coarse-to-fine Pruner Design ‣ 4 CoT-Influx Methodology ‣ Fewer is More: Boosting
    LLM Reasoning with Reinforced Context Pruning")), which consists of a small text
    embedding extractor and a coarse-to-fine pruner.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '概述。图 [3](#S3.F3 "Figure 3 ‣ 3 Pilot Study ‣ Fewer is More: Boosting LLM Reasoning
    with Reinforced Context Pruning") 说明了我们的方法。核心组件是一个轻量级、即插即用的模块（第 [4.3](#S4.SS3
    "4.3 Coarse-to-fine Pruner Design ‣ 4 CoT-Influx Methodology ‣ Fewer is More:
    Boosting LLM Reasoning with Reinforced Context Pruning") 节），包括一个小型文本嵌入提取器和一个粗到细的修剪器。'
- en: 'To train the pruner, we face the challenge of gradient backpropagation when
    pruning discrete tokens outside the LLM. The LLM loss gradient cannot be backpropagated
    through the tokenizer. To address this, we design a multi-objective reward function
    and use reinforcement learning for effective training (Sec. [4.4](#S4.SS4 "4.4
    End-to-end RL Optimization ‣ 4 CoT-Influx Methodology ‣ Fewer is More: Boosting
    LLM Reasoning with Reinforced Context Pruning")). The overall training process
    is outlined in Algorithm [1](#alg1 "Algorithm 1 ‣ 4.3 Coarse-to-fine Pruner Design
    ‣ 4 CoT-Influx Methodology ‣ Fewer is More: Boosting LLM Reasoning with Reinforced
    Context Pruning").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '训练修剪器时，我们面临着在 LLM 外部修剪离散标记时的梯度反向传播挑战。LLM 损失梯度不能通过标记器进行反向传播。为了解决这个问题，我们设计了一个多目标奖励函数，并使用强化学习进行有效训练（第
    [4.4](#S4.SS4 "4.4 End-to-end RL Optimization ‣ 4 CoT-Influx Methodology ‣ Fewer
    is More: Boosting LLM Reasoning with Reinforced Context Pruning") 节）。总体训练过程在算法
    [1](#alg1 "Algorithm 1 ‣ 4.3 Coarse-to-fine Pruner Design ‣ 4 CoT-Influx Methodology
    ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning") 中概述。'
- en: 4.3 Coarse-to-fine Pruner Design
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 粗到细修剪器设计
- en: Text embedding extractor. As CoT-Influx serves as an external module, we need
    to extract text embedding as prediction features. However, it’s non-trivial to
    extract features for long inputs beyond the LLM context window. To address this,
    we use a small encoder model, BERT-Large Devlin et al. ([2018](#bib.bib9)), to
    extract sentence-level (i.e., a CoT example) embedding instead of extracting token
    embedding from the entire long context. For a batch of $k$ is BERT’s hidden dimension
    size.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 文本嵌入提取器。由于 CoT-Influx 作为一个外部模块，我们需要提取文本嵌入作为预测特征。然而，对于超出 LLM 上下文窗口的长输入，提取特征并非易事。为此，我们使用一个小型编码器模型
    BERT-Large Devlin 等人（[2018](#bib.bib9)），以提取句子级别（即 CoT 示例）的嵌入，而不是从整个长上下文中提取标记嵌入。对于一个批次的
    $k$ 是 BERT 的隐藏维度大小。
- en: 'State. As shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3 Pilot Study ‣ Fewer is More:
    Boosting LLM Reasoning with Reinforced Context Pruning"), we define state $s_{\text{shot}}\in\mathbb{R}^{k\times
    N}$ is the number of retained examples.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '状态。如图 [3](#S3.F3 "Figure 3 ‣ 3 Pilot Study ‣ Fewer is More: Boosting LLM Reasoning
    with Reinforced Context Pruning") 所示，我们定义状态 $s_{\text{shot}}\in\mathbb{R}^{k\times
    N}$ 为保留示例的数量。'
- en: Action. Let $a_{\text{shot}}$ predicts the pruning of each token in the retained
    CoT examples.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 动作。让 $a_{\text{shot}}$ 预测在保留的 CoT 示例中每个标记的修剪。
- en: 'Two-stage policy network. The pruner module is a two-stage policy network,
    each stage is a two-layer feed-forward network (MLP) with GELU activation. This
    module outputs a continuous categorical distribution $\pi$, the policy network
    sequentially make two action predictions as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶段策略网络。修剪器模块是一个二阶段策略网络，每个阶段是一个具有 GELU 激活的两层前馈网络（MLP）。该模块输出一个连续的分类分布 $\pi$，策略网络依次进行两个动作预测如下：
- en: '|  | $\displaystyle\pi(a_{\text{shot}}&#124;s_{\text{shot}};\theta)=\sigma\left(\text{MLP}\left(H_{s_{\text{shot}}}\right)\right)$
    |  | (3) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\pi(a_{\text{shot}}&#124;s_{\text{shot}};\theta)=\sigma\left(\text{MLP}\left(H_{s_{\text{shot}}}\right)\right)$
    |  | (3) |'
- en: '|  | $\displaystyle\pi(a_{\text{token}}&#124;s_{\text{token}};\theta)=\sigma\left(\text{MLP}\left(H_{s_{\text{token}}}\right)\right),$
    |  | (4) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\pi(a_{\text{token}}&#124;s_{\text{token}};\theta)=\sigma\left(\text{MLP}\left(H_{s_{\text{token}}}\right)\right),$
    |  | (4) |'
- en: where $a_{\text{shot}}$ with a softmax function.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a_{\text{shot}}$ 通过 softmax 函数。
- en: Algorithm 1 Pruner Training and Inference
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 修剪器训练与推理
- en: 'Input: target LLM, dataset $\mathcal{D}$'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：目标 LLM，数据集 $\mathcal{D}$
- en: 1:  $\blacktriangleright$
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 1:  $\blacktriangleright$
- en: 4.4 End-to-end RL Optimization
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 端到端 RL 优化
- en: 'Multi-objective Reward. Our objective in Eq. [2](#S4.E2 "Equation 2 ‣ 4.2 Problem
    Formulation ‣ 4 CoT-Influx Methodology ‣ Fewer is More: Boosting LLM Reasoning
    with Reinforced Context Pruning") is to train the pruner module to identify the
    most crucial CoT examples and useful tokens for math problem solving, while keeping
    the final tokens within the original LLM context window. To achieve this, we design
    a multi-objective reward.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 多目标奖励。我们在 Eq. [2](#S4.E2 "方程 2 ‣ 4.2 问题描述 ‣ 4 CoT-Influx 方法论 ‣ 更少即更多：通过增强上下文剪枝提升
    LLM 推理") 中的目标是训练剪枝模块，以识别数学问题解决中最关键的 CoT 示例和有用的标记，同时保持最终标记在原始 LLM 上下文窗口内。为此，我们设计了一个多目标奖励。
- en: 'Let $x^{\text{input}}$ is defined as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让 $x^{\text{input}}$ 定义如下：
- en: '|  | $\displaystyle R\left(x^{\text{input}}\right)=(\frac{1}{1+L_{\text{LLM}}}+R_{\text{Acc}})\times\left[\frac{t}{T}\right]^{w}$
    |  | (5) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R\left(x^{\text{input}}\right)=(\frac{1}{1+L_{\text{LLM}}}+R_{\text{Acc}})\times\left[\frac{t}{T}\right]^{w}$
    |  | (5) |'
- en: where the first term evaluates the effectiveness of inputted CoT tokens, and
    the second term ensures they are within the LLM context window. $L_{\text{LLM}}$.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一项评估输入的 CoT 标记的有效性，第二项确保它们在 LLM 上下文窗口内。$L_{\text{LLM}}$。
- en: In addition to $L_{\text{LLM}}$ with a value of -0.1.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 $L_{\text{LLM}}$ 的值为 -0.1。
- en: 'Optimization with REINFORCE. We employ reinforcement learning to maximize the
    reward and train the two-stage policy network. According to REINFORCE Williams
    ([1992](#bib.bib54)), the network parameters are updated by the gradients:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 REINFORCE 进行优化。我们采用强化学习来最大化奖励并训练两阶段策略网络。根据 REINFORCE Williams ([1992](#bib.bib54))，网络参数通过梯度更新：
- en: '|  |  $R\cdot\nabla_{\theta}\text{log}\pi(a_{\text{shot}}&#124;s_{\text{shot}})\pi(a_{\text{token}}&#124;s_{\text{token}})$  |  |
    (6) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  |  $R\cdot\nabla_{\theta}\text{log}\pi(a_{\text{shot}}&#124;s_{\text{shot}})\pi(a_{\text{token}}&#124;s_{\text{token}})$  |  |
    (6) |'
- en: 'Notably, as shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3 Pilot Study ‣ Fewer is More:
    Boosting LLM Reasoning with Reinforced Context Pruning"), only the parameters
    of the policy network require training. The embedding extractor and LLM are frozen,
    thus, the overall training overhead is lightweight.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，如图 [3](#S3.F3 "图 3 ‣ 3 试点研究 ‣ 更少即更多：通过增强上下文剪枝提升 LLM 推理") 所示，仅策略网络的参数需要训练。嵌入提取器和
    LLM 被固定，因此整体训练开销很轻。
- en: Difficulty-aware data filter. Existing LLMs, particularly smaller ones, underperform
    in math reasoning. If the question is too challenging for LLMs, the answer will
    always be incorrect, regardless of the quality of compressed few-shot CoT examples,
    making it challenging to effectively train our pruner module. To address it, we
    use a difficulty filter to sample a math question set $\mathcal{D}_{\text{question}}$.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 难度感知数据过滤器。现有的 LLM，特别是较小的模型，在数学推理中表现欠佳。如果问题对 LLM 来说过于困难，无论压缩的少样本 CoT 示例质量如何，答案总是会错误，这使得有效训练我们的剪枝模块变得具有挑战性。为了解决这个问题，我们使用难度过滤器来采样数学问题集
    $\mathcal{D}_{\text{question}}$。
- en: Stabilize the training. Another challenge is that pruning CoT and tokens during
    training introduces instability, making it difficult for effective training.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定训练。另一个挑战是，在训练过程中剪枝 CoT 和标记会引入不稳定性，使得有效训练变得困难。
- en: 'First, despite the optimization of question set $\mathcal{D}_{\text{question}}$
    in Eq. [5](#S4.E5 "Equation 5 ‣ 4.4 End-to-end RL Optimization ‣ 4 CoT-Influx
    Methodology ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning").'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，尽管在 Eq. [5](#S4.E5 "方程 5 ‣ 4.4 端到端 RL 优化 ‣ 4 CoT-Influx 方法论 ‣ 更少即更多：通过增强上下文剪枝提升
    LLM 推理") 中对问题集 $\mathcal{D}_{\text{question}}$ 进行了优化。
- en: Second, during the early training, our pruner module makes random decisions,
    leading to arbitrary removal of CoT examples and tokens. These randomly pruned
    few-shot prompts can cause instability in RL training. Empirically, we append
    the manually-designed 8-shot CoTs Wei et al. ([2022](#bib.bib52)) to the pruned
    prompts. This ensures a good lower bound and stabilizes the training.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，在早期训练中，我们的剪枝模块做出随机决策，导致 CoT 示例和标记的任意移除。这些随机剪枝的少样本提示可能会导致 RL 训练的不稳定。根据经验，我们将手动设计的
    8-shot CoTs Wei 等 ([2022](#bib.bib52)) 附加到剪枝的提示中。这确保了良好的下界，并稳定了训练。
- en: '![Refer to caption](img/2240dee28425d75fdc628ebedfb019d4.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2240dee28425d75fdc628ebedfb019d4.png)'
- en: 'Figure 4: EM(%) accuracy on GSM8K with inputting different number of CoT examples
    for CoT-Influx.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：在 GSM8K 上输入不同数量的 CoT 示例的 EM(%) 准确率，用于 CoT-Influx。
- en: 'Table 2: Comparison of EM (%) accuracy on GSM8K with state-of-the-art baselines.
    Note that the 20 CoT shots of retrieval baselines are the max number, given that
    the context window limit of LLaMA2 is 4096 tokens.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 GSM8K 上与最先进基线的 EM (%) 准确率比较。请注意，检索基线的 20 个 CoT 样本是最大数量，因为 LLaMA2 的上下文窗口限制为
    4096 个标记。
- en: '| Method | #Input CoT shots | #Average tokens | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | #输入 CoT 样本 | #平均标记 | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
- en: '| Zero-shot | 0 | - | 4.25 | 5.84 | 11.45 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 零样本 | 0 | - | 4.25 | 5.84 | 11.45 |'
- en: '| Zero-shot-CoT Kojima et al. ([2022](#bib.bib21)) | 0 | - | 1.74 | 12.28 |
    21.91 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 零样本-CoT Kojima 等人（[2022](#bib.bib21)） | 0 | - | 1.74 | 12.28 | 21.91 |'
- en: '| Few-shot-CoT Wei et al. ([2022](#bib.bib52)) | 8 | 655 | 13.79 | 27.82 |
    55.42 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 少样本-CoT Wei 等人（[2022](#bib.bib52)） | 8 | 655 | 13.79 | 27.82 | 55.42 |'
- en: '| Random retrieval | 20 | 3379.8 | 12.51 | 22.21 | 53.07 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 随机检索 | 20 | 3379.8 | 12.51 | 22.21 | 53.07 |'
- en: '| TopK retrieval Liu et al. ([2021](#bib.bib28)) | 20 | 3535.4 | 14.56 | 23.65
    | 54.59 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| TopK 检索 Liu 等人（[2021](#bib.bib28)） | 20 | 3535.4 | 14.56 | 23.65 | 54.59
    |'
- en: '| BM25 retrieval Zhenyu et al. ([2023](#bib.bib60)) | 20 | 3816.1 | 13.42 |
    25.17 | 54.21 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| BM25 检索 Zhenyu 等人（[2023](#bib.bib60)） | 20 | 3816.1 | 13.42 | 25.17 | 54.21
    |'
- en: '| TopK+GPT4 Compression | 40 | 1376.0 | 7.08 | 11.01 | 25.17 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| TopK+GPT4 压缩 | 40 | 1376.0 | 7.08 | 11.01 | 25.17 |'
- en: '| TopK+Selective Context Li et al. ([2023b](#bib.bib26)) | 40 | 2262.4 | 0.45
    | 0.76 | 2.50 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| TopK+选择性上下文 Li 等人（[2023b](#bib.bib26)） | 40 | 2262.4 | 0.45 | 0.76 | 2.50
    |'
- en: '| TopK+LLMLingua Jiang et al. ([2023](#bib.bib19)) | 40 | 2048.0 | 5.38 | 8.34
    | 22.74 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| TopK+LLMLingua Jiang 等人（[2023](#bib.bib19)） | 40 | 2048.0 | 5.38 | 8.34 |
    22.74 |'
- en: '| CoT-Influx | 48 | 2037.0 | 15.92 | 32.37 | 59.59 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| CoT-Influx | 48 | 2037.0 | 15.92 | 32.37 | 59.59 |'
- en: 5 Evaluation
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估
- en: 'Table 3: Comparison of EM (%) accuracy on Addsub, Multiarith, Svamp, and Singleeq
    math reasoning dataset'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在 Addsub、Multiarith、Svamp 和 Singleeq 数学推理数据集上的 EM (%) 准确率比较
- en: '| Model | Method | AddSub | Multiarith | Svamp | Singleeq | Avg. |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | AddSub | Multiarith | Svamp | Singleeq | 平均值 |'
- en: '|  | Zero-shot | 58.73 | 5.50 | 32.2 | 62.79 | 39.81 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | 零样本 | 58.73 | 5.50 | 32.2 | 62.79 | 39.81 |'
- en: '|  | Few-shot-CoT | 56.96 | 43.67 | 38.1 | 66.54 | 51.32 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | 少样本-CoT | 56.96 | 43.67 | 38.1 | 66.54 | 51.32 |'
- en: '| LLaMA2-7B | TopK retrieval | 46.08 | 34.50 | 38.1 | 46.46 | 41.29 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | TopK 检索 | 46.08 | 34.50 | 38.1 | 46.46 | 41.29 |'
- en: '|  | TopK+LLMLingua | 12.91 | 10.50 | 19.5 | 19.49 | 15.60 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | TopK+LLMLingua | 12.91 | 10.50 | 19.5 | 19.49 | 15.60 |'
- en: '|  | CoT-Influx | 62.28 | 47.00 | 40.2 | 72.05 | 55.38 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | CoT-Influx | 62.28 | 47.00 | 40.2 | 72.05 | 55.38 |'
- en: '|  | Zero-shot | 70.13 | 6.50 | 43.8 | 71.07 | 47.88 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | 零样本 | 70.13 | 6.50 | 43.8 | 71.07 | 47.88 |'
- en: '|  | Few-shot-CoT | 65.82 | 72.83 | 42.7 | 77.36 | 64.68 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | Few-shot-CoT | 65.82 | 72.83 | 42.7 | 77.36 | 64.68 |'
- en: '| LLaMA2-13B | TopK retrieval | 60.76 | 57.00 | 50.2 | 68.50 | 59.12 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B | TopK 检索 | 60.76 | 57.00 | 50.2 | 68.50 | 59.12 |'
- en: '|  | TopK+LLMLingua | 22.28 | 22.33 | 27.5 | 25.20 | 24.33 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | TopK+LLMLingua | 22.28 | 22.33 | 27.5 | 25.20 | 24.33 |'
- en: '|  | CoT-Influx | 69.62 | 73.87 | 50.5 | 83.07 | 69.26 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | CoT-Influx | 69.62 | 73.87 | 50.5 | 83.07 | 69.26 |'
- en: Models, datasets and metric. We evaluate CoT-Influx on LLaMA2-7B, LLaMA2-13B,
    and LLaMA2-70B. The mathematical datasets for evaluation include GSM8K Cobbe et al.
    ([2021](#bib.bib6)), AddSub Hosseini et al. ([2014](#bib.bib18)), Multiarith Roy
    and Roth ([2015](#bib.bib40)), Svamp Patel et al. ([2021](#bib.bib35)), and Singleeq Koncel-Kedziorski
    et al. ([2015](#bib.bib22)). For evaluation metric, we report Exact Match (EM)
    accuracy of the predicted answers.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 模型、数据集和指标。我们在 LLaMA2-7B、LLaMA2-13B 和 LLaMA2-70B 上评估 CoT-Influx。评估所用的数学数据集包括
    GSM8K Cobbe 等人（[2021](#bib.bib6)）、AddSub Hosseini 等人（[2014](#bib.bib18)）、Multiarith
    Roy 和 Roth（[2015](#bib.bib40)）、Svamp Patel 等人（[2021](#bib.bib35)）和 Singleeq Koncel-Kedziorski
    等人（[2015](#bib.bib22)）。对于评估指标，我们报告预测答案的精确匹配（EM）准确率。
- en: 'Baselines We set three baselines for comparison:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 基线 我们设定了三个比较基线：
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CoT and few-shot CoT prompting: We compare with widely-used prompts for LLM
    reasoning, including zero-shot, zero-shot-CoT Kojima et al. ([2022](#bib.bib21)),
    and the standard few-shot-CoT Wei et al. ([2022](#bib.bib52)) with 8 shots.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CoT 和少样本 CoT 提示：我们与广泛使用的 LLM 推理提示进行比较，包括零样本、零样本-CoT Kojima 等人（[2022](#bib.bib21)）和标准的少样本-CoT
    Wei 等人（[2022](#bib.bib52)），该方法使用 8 个样本。
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt retrieval: we also compare with retrieval baselines, specifically using
    random, TopK Liu et al. ([2021](#bib.bib28)), and BM25 Robertson et al. ([2009](#bib.bib39))
    methods. We select as many CoT examples as possible using each method, without
    exceeding LLM context window. Random retrieval is to reflect the average quality
    of our CoT dataset.'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示检索：我们还与检索基线进行比较，具体使用随机、TopK Liu 等人（[2021](#bib.bib28)）和 BM25 Robertson 等人（[2009](#bib.bib39)）方法。我们使用每种方法选择尽可能多的
    CoT 示例，但不超过 LLM 上下文窗口。随机检索用于反映我们的 CoT 数据集的平均质量。
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt compression: To evaluate the effectiveness of identifying crucial tokens,
    we compare the resulting compressed prompts from the same batch of CoT shots with
    state-of-the-art prompt compression baselines: Selective Context Li et al. ([2023b](#bib.bib26)),
    LLMLingua Jiang et al. ([2023](#bib.bib19)), and compression through GPT-4.'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示压缩：为了评估识别关键标记的有效性，我们将来自同一批次的 CoT 样本的压缩提示与最新的提示压缩基准进行比较：Selective Context Li
    等人 ([2023b](#bib.bib26))、LLMLingua Jiang 等人 ([2023](#bib.bib19)) 和通过 GPT-4 的压缩。
- en: 5.1 Main Results
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 主要结果
- en: 'Effectiveness of enabling more few-shot CoTs. We first evaluate how far the
    boundary of few-shot learning can be pushed using CoT-Influx. For comparison,
    we set up two baselines: (i) Few-shot CoT, using 8 manual-designed CoT shots as
    the default LLM evaluation setting on GSM8K. (ii) TopK retrieves 20 CoT shots
    from our dataset, denoting the max shot number within LLaMA2 context window.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 启用更多少样本 CoT 的有效性。我们首先评估了使用 CoT-Influx 可以将少样本学习的边界推得多远。为了进行比较，我们设定了两个基准：（i）少样本
    CoT，使用 8 个手动设计的 CoT 样本作为 GSM8K 上的默认 LLM 评估设置。（ii）TopK 从我们的数据集中检索 20 个 CoT 样本，表示
    LLaMA2 上下文窗口中的最大样本数量。
- en: 'For CoT-Influx, we test LLaMA2 7B and 13B on GSM8K, adjusting the number of
    CoT shots from 16 to 64 examples, which corresponds to 0.7$\times$ the token count
    of LLaMA2 context window. As shown in Fig. [4](#S4.F4 "Figure 4 ‣ 4.4 End-to-end
    RL Optimization ‣ 4 CoT-Influx Methodology ‣ Fewer is More: Boosting LLM Reasoning
    with Reinforced Context Pruning"), we make two observations: (1) More CoT shots,
    facilitated by CoT-Influx, indeed boosts LLM math reasoning performance, particularly
    for larger LLMs. On LLaMA2-13B, by inputting 48 CoTs, we significantly outperform
    the standard few-shot CoT and TopK by 4.55% and 8.72%, respectively. (2) There
    is an optimal number of CoT shots for CoT-Influx. Its peak performance on LLaMA2
    7B and 13B are at 40 and 48 shots, respectively. We attribute this to two potential
    reasons. First, an extremely large number of shots complicates CoT-Influx’s optimization.
    Second, there may be an upper limit to improving LLM reasoning capability through
    few-shot learning.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 CoT-Influx，我们在 GSM8K 上测试了 LLaMA2 7B 和 13B，将 CoT 样本的数量从 16 个调整到 64 个样本，这对应于
    LLaMA2 上下文窗口的 0.7$\times$ 的标记数量。如图 [4](#S4.F4 "Figure 4 ‣ 4.4 End-to-end RL Optimization
    ‣ 4 CoT-Influx Methodology ‣ Fewer is More: Boosting LLM Reasoning with Reinforced
    Context Pruning") 所示，我们有两个观察结果：（1）通过 CoT-Influx 提供的更多 CoT 样本确实提高了 LLM 的数学推理性能，特别是对于较大的
    LLM。在 LLaMA2-13B 上，通过输入 48 个 CoT，我们的表现显著超越了标准的少样本 CoT 和 TopK，分别提高了 4.55% 和 8.72%。（2）对于
    CoT-Influx 存在一个最佳的 CoT 样本数量。其在 LLaMA2 7B 和 13B 上的最佳表现分别出现在 40 个和 48 个样本时。我们将此归因于两个可能的原因。首先，样本数量过多会使
    CoT-Influx 的优化变得复杂。其次，通过少样本学习提升 LLM 推理能力可能存在上限。'
- en: 'Comparison with state-of-the-art baselines. Table [2](#S4.T2 "Table 2 ‣ 4.4
    End-to-end RL Optimization ‣ 4 CoT-Influx Methodology ‣ Fewer is More: Boosting
    LLM Reasoning with Reinforced Context Pruning") and Table [3](#S5.T3 "Table 3
    ‣ 5 Evaluation ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context
    Pruning") present the comparison results of CoT-Influx with state-of-the-art baselines
    across LLaMA2 family and 5 mathematical datasets, highlighting the following observations:
    (1) by utilizing more few-shot CoTs that are twice the LLM context window, CoT-Influx
    significantly outperforms all baselines, with 2.13% to 4.55% absolute improvements.
    (2) Despite using fewer input tokens, CoT-Influx consistently outperforms retrieval
    baselines by 1.36% to 14.09% absolute improvements. This is because our compressed
    tokens indicate more informational CoT examples without redundancy. In contrast,
    they select entire examples, which may contain redundant tokens, based on semantic
    similarity between the target question and CoT examples, without considering the
    different CoT preference of the target LLM. (3) CoT-Influx significantly outperforms
    prompt compression baselines in preserving the most crucial tokens for math reasoning,
    while methods like Selective Context and LLMLingua suffer accuracy declines due
    to difficulties in maintaining few-shot prompt structure. GPT-4 tends to prune
    essential reasoning steps, which negatively impacts CoT effectiveness.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '与最新基准的比较。表[2](#S4.T2 "Table 2 ‣ 4.4 End-to-end RL Optimization ‣ 4 CoT-Influx
    Methodology ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning")和表[3](#S5.T3
    "Table 3 ‣ 5 Evaluation ‣ Fewer is More: Boosting LLM Reasoning with Reinforced
    Context Pruning")展示了CoT-Influx与最新基准的比较结果，涵盖LLaMA2家族和5个数学数据集，突出了以下观察结果：（1）通过使用更多的少样本CoT，数量是LLM上下文窗口的两倍，CoT-Influx显著超越所有基准，绝对提升在2.13%到4.55%之间。（2）尽管输入的tokens更少，CoT-Influx仍然比检索基准的绝对提升在1.36%到14.09%之间。这是因为我们的压缩tokens提供了更多信息性的CoT示例，而没有冗余。相比之下，他们选择了整个示例，可能包含冗余tokens，基于目标问题与CoT示例之间的语义相似性，而不考虑目标LLM的不同CoT偏好。（3）CoT-Influx在保留数学推理中最关键的tokens方面显著超越了提示压缩基准，而像Selective
    Context和LLMLingua等方法因难以保持少样本提示结构而导致准确率下降。GPT-4倾向于剪枝关键的推理步骤，这对CoT的效果产生了负面影响。'
- en: 'We further demonstrate the effectiveness of CoT-Influx by comparing LLaMA2-70B
    with larger size LLMs on GSM8K. As shown in Table [4](#S5.T4 "Table 4 ‣ 5.1 Main
    Results ‣ 5 Evaluation ‣ Fewer is More: Boosting LLM Reasoning with Reinforced
    Context Pruning"), CoT-Influx significantly boosts LLM reasoning capabilities.
    Remarkably, without any fine-tuning, LLaMA2-70B with CoT-Influx outperform much
    larger LLMs. LLaMA2-70B surpasses GPT-3.5 with an absolute improvement of 2.5%.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步通过在GSM8K上比较LLaMA2-70B与更大规模LLM的效果来演示CoT-Influx的有效性。如表[4](#S5.T4 "Table
    4 ‣ 5.1 Main Results ‣ 5 Evaluation ‣ Fewer is More: Boosting LLM Reasoning with
    Reinforced Context Pruning")所示，CoT-Influx显著提升了LLM的推理能力。值得注意的是，LLaMA2-70B在没有任何微调的情况下，结合CoT-Influx的表现超过了许多更大的LLM。LLaMA2-70B比GPT-3.5的绝对提升达2.5%。'
- en: 'Table 4: Comparison of EM (%) accuracy on GSM8K with larger LLMs under the
    few-shot-CoT setting.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在少样本-CoT设置下，GSM8K上不同LLM的EM (%)准确率比较。
- en: '| Model | Parameters | EM (%) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数 | EM (%) |'
- en: '| Finetuned GPT-3 Wei et al. ([2022](#bib.bib52)) | 175B | 34.0 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Finetuned GPT-3 Wei et al. ([2022](#bib.bib52)) | 175B | 34.0 |'
- en: '| Chinchilla Hoffmann et al. ([2022](#bib.bib16)) | 70B | 43.7 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Chinchilla Hoffmann et al. ([2022](#bib.bib16)) | 70B | 43.7 |'
- en: '| Text-davinci-002 Kojima et al. ([2022](#bib.bib21)) | 175B | 51.5 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Text-davinci-002 Kojima et al. ([2022](#bib.bib21)) | 175B | 51.5 |'
- en: '| PaLM Chowdhery et al. ([2022](#bib.bib5)) | 540B | 56.5 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| PaLM Chowdhery et al. ([2022](#bib.bib5)) | 540B | 56.5 |'
- en: '| GPT-3.5 OpenAI ([2023a](#bib.bib33)) | - | 57.1 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 OpenAI ([2023a](#bib.bib33)) | - | 57.1 |'
- en: '| Minerva Lewkowycz et al. ([2022](#bib.bib24)) | 540B | 58.8 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Minerva Lewkowycz et al. ([2022](#bib.bib24)) | 540B | 58.8 |'
- en: '| LLaMA2-70B+CoT-Influx | 70B | 59.6 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-70B+CoT-Influx | 70B | 59.6 |'
- en: 'Compatible with existing reasoning prompts. As a method to improve LLM reasoning
    capability, CoT-Influx is complementary with other advanced reasoning-based prompts.
    To prove this, we apply self-consistency Wang et al. ([2023d](#bib.bib51)) and
    self-verification Weng et al. ([2023](#bib.bib53)) to compressed prompts generated
    by CoT-influx. For evaluation efficiency, we sampled 20 times. As Table [5](#S5.T5
    "Table 5 ‣ 5.1 Main Results ‣ 5 Evaluation ‣ Fewer is More: Boosting LLM Reasoning
    with Reinforced Context Pruning") shows, applying self-consistency and self-verification
    further improve LLaMA2’s performance on GSM8k.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '兼容现有的推理提示。作为提高LLM推理能力的一种方法，**CoT-Influx**与其他先进的基于推理的提示互为补充。为了证明这一点，我们将自一致性
    Wang 等人（[2023d](#bib.bib51)）和自验证 Weng 等人（[2023](#bib.bib53)）应用于由 CoT-Influx 生成的压缩提示。为了评估效率，我们采样了20次。如表[5](#S5.T5
    "Table 5 ‣ 5.1 Main Results ‣ 5 Evaluation ‣ Fewer is More: Boosting LLM Reasoning
    with Reinforced Context Pruning")所示，应用自一致性和自验证进一步提高了LLaMA2在GSM8k上的表现。'
- en: 'Table 5: CoT-Influx is compatible with advanced prompt techniques like self-consistency
    (i.e., maj@20) and self-verification (i.e., verify@20).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：**CoT-Influx** 兼容先进的提示技术，如自一致性（即 maj@20）和自验证（即 verify@20）。
- en: '| Method | LLaMA2-13B | LLaMA2-70B |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LLaMA2-13B | LLaMA2-70B |'
- en: '| CoT-Influx | 32.37 | 59.59 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| CoT-Influx | 32.37 | 59.59 |'
- en: '| CoT-Influx+maj@20 | 33.43 | 60.73 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| CoT-Influx+maj@20 | 33.43 | 60.73 |'
- en: '| CoT-Influx+verify@20 | 34.04 | 61.79 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| CoT-Influx+verify@20 | 34.04 | 61.79 |'
- en: 5.2 Ablation Study and Analysis
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 消融研究与分析
- en: 'The effectiveness of MRD³ dataset. Beyond our pruner, we introduce MRD³ dataset,
    which is evolved by GPT-4 for diverse reasoning steps and difficulties. We compare
    with two baselines: (1) MRD³ without evolution, excluding GPT-4 evolved examples,
    and (2) the human-labeled GSM8K training set, which excludes GPT-4’s reformatted
    generation. We apply our pruner on these datasets under the same setting. As shown
    in Table [6](#S5.T6 "Table 6 ‣ 5.2 Ablation Study and Analysis ‣ 5 Evaluation
    ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning"), both
    GPT-4 generated and evolved CoT examples are vital for improving the reasoning
    performance.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 'MRD³ 数据集的有效性。除了我们的剪枝器，我们引入了由 GPT-4 为多样化推理步骤和难度进化的 MRD³ 数据集。我们与两个基线进行比较：（1）MRD³
    无进化，排除 GPT-4 进化的示例，以及（2）人类标注的 GSM8K 训练集，排除 GPT-4 格式化生成的内容。我们在这些数据集上使用相同的设置应用我们的剪枝器。如表[6](#S5.T6
    "Table 6 ‣ 5.2 Ablation Study and Analysis ‣ 5 Evaluation ‣ Fewer is More: Boosting
    LLM Reasoning with Reinforced Context Pruning")所示，GPT-4 生成和进化的 CoT 示例对于提高推理性能至关重要。'
- en: 'Table 6: Comparison of EM(%) on GSM8K using CoT-Influx pruner across different
    CoT datasets.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：使用 **CoT-Influx** 剪枝器在不同 CoT 数据集上 GSM8K 的 EM（%）比较。
- en: '| CoT dataset | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| CoT 数据集 | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
- en: '| MRD³ | 15.92 | 32.37 | 59.59 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| MRD³ | 15.92 | 32.37 | 59.59 |'
- en: '| MRD³ w/o evolution | 14.94 | 30.55 | 57.70 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| MRD³ 无进化 | 14.94 | 30.55 | 57.70 |'
- en: '| GSM8K training set | 14.18 | 29.64 | 56.71 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| GSM8K 训练集 | 14.18 | 29.64 | 56.71 |'
- en: 'Ablation study on coarse-to-fine pruner. Our pruner operates at both shot and
    token levels to fully exploit redundancy within CoT examples. To verify the effectiveness,
    we conduct experiments with only shot or token pruner under the same setting.
    As shown in Table [7](#S5.T7 "Table 7 ‣ 5.2 Ablation Study and Analysis ‣ 5 Evaluation
    ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning"), removing
    any pruning stage decreases performance. Notably, removing token-only pruning
    causes a larger accuracy drop than shot-only pruning, indicating that shot-level
    redundancy is easier for the pruner to learn.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '粗到细剪枝器的消融研究。我们的剪枝器在射击和令牌级别上操作，以充分利用 CoT 示例中的冗余。为了验证有效性，我们在相同设置下仅使用射击或令牌剪枝器进行实验。如表[7](#S5.T7
    "Table 7 ‣ 5.2 Ablation Study and Analysis ‣ 5 Evaluation ‣ Fewer is More: Boosting
    LLM Reasoning with Reinforced Context Pruning")所示，去除任何剪枝阶段都会降低性能。值得注意的是，去除仅令牌剪枝会导致比仅射击剪枝更大的准确性下降，表明射击级别的冗余更容易被剪枝器学习。'
- en: 'Table 7: Comparison of EM(%) on GSM8K with different pruning strategies.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：不同剪枝策略下 GSM8K 上 EM（%）的比较。
- en: '| Pruning Strategy | LLaMA2 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝策略 | LLaMA2 |'
- en: '| 7B | 13B | 70B |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 7B | 13B | 70B |'
- en: '| CoT-Influx (Prune shot and token) | 15.92 | 32.37 | 59.59 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| CoT-Influx（剪枝射击和令牌） | 15.92 | 32.37 | 59.59 |'
- en: '| Prune shot only | 15.69 | 31.08 | 57.77 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 仅剪枝射击 | 15.69 | 31.08 | 57.77 |'
- en: '| Prune token only | 12.05 | 25.32 | 49.36 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 仅剪枝令牌 | 12.05 | 25.32 | 49.36 |'
- en: 'Table 8: The total inference costs on GSM8K.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：GSM8K 上的总推理成本。
- en: '| Method | #Input-shot | #Token | Time | GPU Memory |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | #输入射击 | #令牌 | 时间 | GPU 内存 |'
- en: '| LLaMA2-7B | 12 | 2108.6 | 2.99h | 19.7GB |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | 12 | 2108.6 | 2.99h | 19.7GB |'
- en: '| Selective Context | 40 | 2262.4 | 4.38h | 23.5GB |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 选择性上下文 | 40 | 2262.4 | 4.38h | 23.5GB |'
- en: '| LLMLingua | 40 | 2048.0 | 3.65h | 33.0GB |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| LLMLingua | 40 | 2048.0 | 3.65h | 33.0GB |'
- en: '| CoT-Influx | 40 | 2037.0 | 3.04h | 21.1GB | ![Refer to caption](img/2212498def766cb064f1e2e3b263946f.png)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '| CoT-Influx | 40 | 2037.0 | 3.04h | 21.1GB | ![参见说明](img/2212498def766cb064f1e2e3b263946f.png)'
- en: 'Figure 5: Token length after each stage of our pruner.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：修剪器每个阶段后的标记长度。
- en: 'Token pruning ratios. We now investigate token pruning ratios by our pruner.
    Fig. [5](#S5.F5 "Figure 5 ‣ 5.2 Ablation Study and Analysis ‣ 5 Evaluation ‣ Fewer
    is More: Boosting LLM Reasoning with Reinforced Context Pruning") shows the remaining
    token length for LLaMA2-70B after our pruner. In total, we achieve a 4.28$\times$
    ratio. The results suggest that our pruner favors pruning more coarse-grained
    shots over fine-grained tokens.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '标记修剪比例。我们现在通过我们的修剪器研究标记修剪比例。图 [5](#S5.F5 "Figure 5 ‣ 5.2 Ablation Study and
    Analysis ‣ 5 Evaluation ‣ Fewer is More: Boosting LLM Reasoning with Reinforced
    Context Pruning") 显示了我们修剪器处理后的 LLaMA2-70B 的剩余标记长度。总体而言，我们实现了 4.28$\times$ 的比率。结果表明，我们的修剪器更倾向于修剪粗粒度的示例，而不是细粒度的标记。'
- en: 'Inference cost. CoT-Influx is a lightweight plug-and-play module, including
    a 336MB BERT-Large model and a tiny 4MB coarse-to-fine pruner. We measure its
    additional inference cost. Table [8](#S5.T8 "Table 8 ‣ 5.2 Ablation Study and
    Analysis ‣ 5 Evaluation ‣ Fewer is More: Boosting LLM Reasoning with Reinforced
    Context Pruning") shows the total inference latency and GPU memory required to
    run LLaMA2-7B with different methods on GSM8K, measured on a single NVIDIA A100
    GPU. The results reveal that CoT-Influx introduces a negligible 1.4GB additional
    memory and a 1.7% increase in latency. This is more effective than prompt compression
    baselines, such as Selective Context and LLMLingua, which require significantly
    higher latency and more GPU memory, potentially hindering efficient deployment.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '推理成本。CoT-Influx 是一个轻量级的即插即用模块，包括一个 336MB 的 BERT-Large 模型和一个仅 4MB 的粗细修剪器。我们测量了它的额外推理成本。表 [8](#S5.T8
    "Table 8 ‣ 5.2 Ablation Study and Analysis ‣ 5 Evaluation ‣ Fewer is More: Boosting
    LLM Reasoning with Reinforced Context Pruning") 显示了在单个 NVIDIA A100 GPU 上运行 LLaMA2-7B
    使用不同方法的总推理延迟和 GPU 内存。结果表明，CoT-Influx 仅引入了 1.4GB 的额外内存和 1.7% 的延迟增加。这比提示压缩基准（如 Selective
    Context 和 LLMLingua）更有效，这些基准需要显著更高的延迟和更多的 GPU 内存，可能会妨碍高效部署。'
- en: 'Implications. Our analysis of retained CoT examples and tokens yields the following
    insights: (1) More capable LLMs favor harder CoT examples, while smaller LLMs
    opt for simpler ones. (2) Numerical and format tokens are essential for math reasoning.
    Function words like with, the, then, and irrelevant background context such as
    theater can be pruned without affecting reasoning capability.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 含义。我们对保留的 CoT 示例和标记的分析得出了以下见解：（1）更强大的 LLM 更倾向于更难的 CoT 示例，而较小的 LLM 选择更简单的示例。（2）数字和格式标记对数学推理至关重要。像
    with、the、then 这样的功能词以及无关的背景内容如 theater 可以被修剪，而不会影响推理能力。
- en: 6 Conclusion
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We present CoT-Influx, a plug-and-play module that improves LLM math reasoning
    by pruning unnecessary few-shot examples at shot and token levels for a more effective
    input context. To train the module, we use reinforcement learning to optimize
    a math reasoning-specific reward with GPT-4 evolved CoT dataset MRD³. Extensive
    experiments on various datasets and LLMs compared with state-of-the-art baselines
    demonstrate the effectiveness of our method. This paper highlights the vast potential
    of few-shot CoT prompting in augmenting LLMs’ math reasoning abilities.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 CoT-Influx，这是一个即插即用的模块，通过在示例和标记级别修剪不必要的少量示例来提高 LLM 的数学推理能力，以实现更有效的输入上下文。为了训练该模块，我们使用强化学习来优化一个特定于数学推理的奖励，使用
    GPT-4 进化的 CoT 数据集 MRD³。我们在各种数据集和 LLM 上进行的广泛实验，与最先进的基准进行比较，展示了我们方法的有效性。本文强调了少量
    CoT 提示在增强 LLM 数学推理能力方面的巨大潜力。
- en: Limitations
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: As in-context learning with LLM heavily relies on the selected examples in the
    prompt, the performance of CoT-Influx can be influenced by the quality of CoT
    generation. Despite this, CoT-Influx still demonstrates strong performance on
    our GPT4-evolved dataset MRD³. We currently use BERT to obtain the feature embedding
    of a CoT example, which cannot handle long-sequence examples exceeding 512 tokens.
    We will take these limitations into account and mitigate them in future work.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 LLM 的上下文学习严重依赖于提示中的选择示例，CoT-Influx 的性能可能会受到 CoT 生成质量的影响。尽管如此，CoT-Influx 在我们的
    GPT4 进化数据集 MRD³ 上仍表现出强大的性能。我们目前使用 BERT 来获取 CoT 示例的特征嵌入，但无法处理超过 512 个标记的长序列。我们将考虑这些限制并在未来的工作中加以改进。
- en: References
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. Language models are few-shot learners. In *Advances in Neural Information
    Processing Systems*, volume 33.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei. 2020. 语言模型是少样本学习者。在 *神经信息处理系统进展*，第 33 卷。
- en: Chen et al. (2023a) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 2023a. Extending context window of large language models via positional
    interpolation. *arXiv preprint arXiv:2306.15595*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023a) Shouyuan Chen, Sherman Wong, Liangjian Chen, 和 Yuandong
    Tian. 2023a. 通过位置插值扩展大型语言模型的上下文窗口。*arXiv 预印本 arXiv:2306.15595*。
- en: 'Chen et al. (2023b) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen.
    2023b. Program of thoughts prompting: Disentangling computation from reasoning
    for numerical reasoning tasks. *Transactions on Machine Learning Research*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023b) Wenhu Chen, Xueguang Ma, Xinyi Wang, 和 William W. Cohen.
    2023b. 思维提示程序：将计算与推理区分开来，以解决数值推理任务。*机器学习研究交易*。
- en: Chevalier et al. (2023) Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and
    Danqi Chen. 2023. Adapting language models to compress contexts. *arXiv preprint
    arXiv:2305.14788*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chevalier et al. (2023) Alexis Chevalier, Alexander Wettig, Anirudh Ajith, 和
    Danqi Chen. 2023. 使语言模型适应压缩上下文。*arXiv 预印本 arXiv:2305.14788*。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, 等. 2022. Palm: 通过路径扩展语言建模。*arXiv 预印本 arXiv:2204.02311*。'
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, 等. 2021. 训练验证器以解决数学应用题。*arXiv 预印本 arXiv:2110.14168*。
- en: 'Dai et al. (2023) Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang
    Sui, and Furu Wei. 2023. Why can gpt learn in-context? language models secretly
    perform gradient descent as meta-optimizers. In *Findings of the Association for
    Computational Linguistics: ACL 2023*, pages 4005–4019.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai et al. (2023) Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang
    Sui, 和 Furu Wei. 2023. 为什么 GPT 能在上下文中学习？语言模型秘密地执行梯度下降作为元优化器。在 *计算语言学协会的发现：ACL
    2023*，第 4005–4019 页。
- en: 'Deng et al. (2022) Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang,
    Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. 2022. Rlprompt:
    Optimizing discrete text prompts with reinforcement learning. *arXiv preprint
    arXiv:2205.12548*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng et al. (2022) Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang,
    Han Guo, Tianmin Shu, Meng Song, Eric P Xing, 和 Zhiting Hu. 2022. Rlprompt: 使用强化学习优化离散文本提示。*arXiv
    预印本 arXiv:2205.12548*。'
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2018. Bert: 深度双向变换器的预训练用于语言理解。*arXiv 预印本 arXiv:1810.04805*。'
- en: 'Ding et al. (2023) Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma,
    Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. 2023. Everything
    of thoughts: Defying the law of penrose triangle for thought generation. *arXiv
    preprint arXiv:2311.04254*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. (2023) Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma,
    Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, 和 Dongmei Zhang. 2023. 思维的全部：挑战
    Penrose 三角形法则以生成思维。*arXiv 预印本 arXiv:2311.04254*。
- en: 'Fu et al. (2023) Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar
    Khot. 2023. Chain-of-thought hub: A continuous effort to measure large language
    models’ reasoning performance. *arXiv preprint arXiv:2305.17306*.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu et al. (2023) Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar
    Khot. 2023. Chain-of-thought hub: 连续努力测量大语言模型的推理性能。*arXiv 预印本 arXiv:2305.17306*。'
- en: Gao et al. (2021) Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained
    language models better few-shot learners. In *Joint Conference of the 59th Annual
    Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing, ACL-IJCNLP 2021*, pages 3816–3830\.
    Association for Computational Linguistics (ACL).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2021) Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. 使预训练语言模型更好地进行少样本学习。在*第59届计算语言学协会年会暨第11届国际联合自然语言处理会议
    (ACL-IJCNLP 2021)*中，第3816–3830页。计算语言学协会（ACL）。
- en: Ge et al. (2023) Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. 2023.
    In-context autoencoder for context compression in a large language model. *arXiv
    preprint arXiv:2307.06945*.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. (2023) Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. 2023.
    上下文自动编码器用于大语言模型中的上下文压缩。*arXiv 预印本 arXiv:2307.06945*。
- en: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. 2023. Lm-infinite: Simple on-the-fly length generalization for large
    language models. *arXiv preprint arXiv:2308.16137*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. 2023. Lm-infinite: 大语言模型的简单即时长度泛化。*arXiv 预印本 arXiv:2308.16137*。'
- en: 'Hao et al. (2022) Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and
    Furu Wei. 2022. Structured prompting: Scaling in-context learning to 1,000 examples.
    *arXiv preprint arXiv:2212.06713*.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao et al. (2022) Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and
    Furu Wei. 2022. 结构化提示：将上下文学习扩展到1000个示例。*arXiv 预印本 arXiv:2212.06713*。
- en: Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language
    models. *arXiv preprint arXiv:2203.15556*.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, 等。2022. 训练计算最优的大语言模型。*arXiv 预印本 arXiv:2203.15556*。
- en: Hongjin et al. (2022) SU Hongjin, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu
    Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al.
    2022. Selective annotation makes language models better few-shot learners. In
    *The Eleventh International Conference on Learning Representations*.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hongjin et al. (2022) SU Hongjin, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu
    Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, 等。2022.
    选择性注释使语言模型成为更好的少样本学习者。在*第十一届国际学习表示会议*上。
- en: Hosseini et al. (2014) Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni,
    and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization.
    In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 523–533.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosseini et al. (2014) Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni,
    and Nate Kushman. 2014. 通过动词分类学习解决算术文字问题。在*2014年自然语言处理实证方法会议论文集（EMNLP）*中，第523–533页。
- en: 'Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,
    and Lili Qiu. 2023. Llmlingua: Compressing prompts for accelerated inference of
    large language models. In *Proceedings of the 2023 Conference on Empirical Methods
    in Natural Language Processing*.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,
    and Lili Qiu. 2023. Llmlingua: 压缩提示以加速大语言模型的推理。在*2023年自然语言处理实证方法会议论文集*中。'
- en: Kim et al. (2022) Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk
    Kwon, Joseph Hassoun, and Kurt Keutzer. 2022. Learned token pruning for transformers.
    In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining*, KDD ’22, page 784–794\. Association for Computing Machinery.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2022) Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk
    Kwon, Joseph Hassoun, and Kurt Keutzer. 2022. 针对变换器的学习型标记修剪。在*第28届ACM SIGKDD知识发现与数据挖掘会议论文集*，KDD
    ’22，第784–794页。计算机协会。
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.
    *Advances in neural information processing systems*, 35:22199–22213.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. 大语言模型是零样本推理器。*神经信息处理系统进展*，35:22199–22213。
- en: Koncel-Kedziorski et al. (2015) Rik Koncel-Kedziorski, Hannaneh Hajishirzi,
    Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word
    problems into equations. *Transactions of the Association for Computational Linguistics*,
    3:585–597.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koncel-Kedziorski et al. (2015) Rik Koncel-Kedziorski, Hannaneh Hajishirzi,
    Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. 将代数词题解析为方程。*计算语言学协会会刊*，3:585–597。
- en: 'Koncel-Kedziorski et al. (2016) Rik Koncel-Kedziorski, Subhro Roy, Aida Amini,
    Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository.
    In *Proceedings of the 2016 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 1152–1157.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Koncel-Kedziorski et al. (2016) Rik Koncel-Kedziorski, Subhro Roy, Aida Amini,
    Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: 一个数学词题库。在*2016年北美计算语言学协会会议：人类语言技术*，页码
    1152–1157。'
- en: Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan
    Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,
    Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language
    models. *Advances in Neural Information Processing Systems*, 35:3843–3857.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan
    Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,
    Theo Gutman-Solo, 等。2022. 使用语言模型解决定量推理问题。*神经信息处理系统进展*, 35:3843–3857。
- en: Li et al. (2023a) Junyan Li, Li Lyna Zhang, Jiahang Xu, Yujing Wang, Shaoguang
    Yan, Yunqing Xia, Yuqing Yang, Ting Cao, Hao Sun, Weiwei Deng, Qi Zhang, and Mao
    Yang. 2023a. Constraint-aware and ranking-distilled token pruning for efficient
    transformer inference. In *Proceedings of the 29th ACM SIGKDD Conference on Knowledge
    Discovery and Data Mining*, KDD ’23, page 1280–1290.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023a) Junyan Li, Li Lyna Zhang, Jiahang Xu, Yujing Wang, Shaoguang
    Yan, Yunqing Xia, Yuqing Yang, Ting Cao, Hao Sun, Weiwei Deng, Qi Zhang, and Mao
    Yang. 2023a. 约束感知和排名蒸馏的令牌剪枝以提高变换器推理效率。在*第29届 ACM SIGKDD 知识发现与数据挖掘会议论文集*，KDD ’23，页码
    1280–1290。
- en: Li et al. (2023b) Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. 2023b.
    [Compressing context to enhance inference efficiency of large language models](http://arxiv.org/abs/2310.06201).
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023b) Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. 2023b.
    [压缩上下文以增强大语言模型的推理效率](http://arxiv.org/abs/2310.06201)。
- en: 'Ling et al. (2017) Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
    2017. Program induction by rationale generation: Learning to solve and explain
    algebraic word problems. In *Proceedings of the 55th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pages 158–167.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling et al. (2017) Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017.
    通过理由生成进行程序归纳：学习解决和解释代数词题。在*第55届计算语言学协会年会（第一卷：长篇论文）*，页码 158–167。
- en: Liu et al. (2021) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-$3$?
    *arXiv preprint arXiv:2101.06804*.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, and Weizhu Chen. 2021. 什么是良好的 GPT-$3$ 上下文示例？*arXiv 预印本 arXiv:2101.06804*。
- en: 'Lu et al. (2022) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and
    Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them:
    Overcoming few-shot prompt order sensitivity. In *Proceedings of the 60th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*,
    pages 8086–8098.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. (2022) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and
    Pontus Stenetorp. 2022. 极好的有序提示及其获取方式：克服少样本提示顺序敏感性。在*第60届计算语言学协会年会（第一卷：长篇论文）*，页码
    8086–8098。
- en: 'Luo et al. (2023) Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou,
    Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023.
    Wizardmath: Empowering mathematical reasoning for large language models via reinforced
    evol-instruct. *arXiv preprint arXiv:2308.09583*.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo et al. (2023) Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou,
    Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023.
    Wizardmath: 通过强化进化指令增强大语言模型的数学推理能力。*arXiv 预印本 arXiv:2308.09583*。'
- en: 'Min et al. (2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations:
    What makes in-context learning work? In *Proceedings of the 2022 Conference on
    Empirical Methods in Natural Language Processing*, pages 11048–11064.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min et al. (2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. 重新思考演示的角色：是什么让上下文学习有效？在*2022年自然语言处理经验方法会议论文集*，页码
    11048–11064。
- en: Mu et al. (2023) Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to
    compress prompts with gist tokens. *arXiv preprint arXiv:2304.08467*.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu 等 (2023) Jesse Mu, Xiang Lisa Li 和 Noah Goodman. 2023. 学习使用 gist 令牌压缩提示。*arXiv
    预印本 arXiv:2304.08467*。
- en: OpenAI (2023a) OpenAI. 2023a. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023a) OpenAI. 2023a. [Gpt-4 技术报告](http://arxiv.org/abs/2303.08774)。
- en: OpenAI (2023b) OpenAI. 2023b. [Welcome to the openai platform](https://platform.openai.com/docs/introduction).
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023b) OpenAI. 2023b. [欢迎来到 OpenAI 平台](https://platform.openai.com/docs/introduction)。
- en: 'Patel et al. (2021) Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021.
    Are nlp models really able to solve simple math word problems? In *Proceedings
    of the 2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 2080–2094.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patel 等 (2021) Arkil Patel, Satwik Bhattamishra 和 Navin Goyal. 2021. NLP 模型真的能解决简单的数学词问题吗？见于
    *2021年北美计算语言学协会人类语言技术会议论文集*，第2080–2094页。
- en: 'Peng et al. (2023a) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    2023a. Yarn: Efficient context window extension of large language models. *arXiv
    preprint arXiv:2309.00071*.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等 (2023a) Bowen Peng, Jeffrey Quesnelle, Honglu Fan 和 Enrico Shippole.
    2023a. Yarn：大规模语言模型的高效上下文窗口扩展。*arXiv 预印本 arXiv:2309.00071*。
- en: 'Peng et al. (2023b) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    2023b. Yarn: Efficient context window extension of large language models. *arXiv
    preprint arXiv:2309.00071*.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等 (2023b) Bowen Peng, Jeffrey Quesnelle, Honglu Fan 和 Enrico Shippole.
    2023b. Yarn：大规模语言模型的高效上下文窗口扩展。*arXiv 预印本 arXiv:2309.00071*。
- en: Qin et al. (2023) Chengwei Qin, Aston Zhang, Anirudh Dagar, and Wenming Ye.
    2023. In-context learning with iterative demonstration selection. *arXiv preprint
    arXiv:2310.09881*.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等 (2023) Chengwei Qin, Aston Zhang, Anirudh Dagar 和 Wenming Ye. 2023. 通过迭代示例选择进行上下文学习。*arXiv
    预印本 arXiv:2310.09881*。
- en: 'Robertson et al. (2009) Stephen Robertson, Hugo Zaragoza, et al. 2009. The
    probabilistic relevance framework: Bm25 and beyond. *Foundations and Trends® in
    Information Retrieval*, 3(4):333–389.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robertson 等 (2009) Stephen Robertson, Hugo Zaragoza 等. 2009. 概率相关框架：Bm25及其扩展。*信息检索的基础与趋势®*，3(4)：333–389。
- en: Roy and Roth (2015) Subhro Roy and Dan Roth. 2015. Solving general arithmetic
    word problems. In *Proceedings of the 2015 Conference on Empirical Methods in
    Natural Language Processing*, pages 1743–1752.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy 和 Roth (2015) Subhro Roy 和 Dan Roth. 2015. 解决一般算术词问题。见于 *2015年自然语言处理经验方法会议论文集*，第1743–1752页。
- en: Rubin et al. (2021) Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021.
    Learning to retrieve prompts for in-context learning. *arXiv preprint arXiv:2112.08633*.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubin 等 (2021) Ohad Rubin, Jonathan Herzig 和 Jonathan Berant. 2021. 学习检索用于上下文学习的提示。*arXiv
    预印本 arXiv:2112.08633*。
- en: 'Scarlatos and Lan (2023) Alexander Scarlatos and Andrew Lan. 2023. Reticl:
    Sequential retrieval of in-context examples with reinforcement learning. *arXiv
    preprint arXiv:2305.14502*.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scarlatos 和 Lan (2023) Alexander Scarlatos 和 Andrew Lan. 2023. Reticl：使用强化学习进行上下文示例的序列检索。*arXiv
    预印本 arXiv:2305.14502*。
- en: 'Shin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace,
    and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with
    automatically generated prompts. In *Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing (EMNLP)*, pages 4222–4235.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shin 等 (2020) Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace
    和 Sameer Singh. 2020. Autoprompt：通过自动生成的提示从语言模型中引出知识。见于 *2020年自然语言处理经验方法会议论文集
    (EMNLP)*，第4222–4235页。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等 (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar 等. 2023a. Llama：开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*。
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等 (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等. 2023b. Llama 2：开放基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*。
- en: 'Tworkowski et al. (2023) Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek,
    Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. 2023. [Focused transformer: Contrastive
    training for context scaling](http://arxiv.org/abs/2307.03170).'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tworkowski等（2023）Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai
    Wu, Henryk Michalewski, 和 Piotr Miłoś. 2023. [Focused transformer: Contrastive
    training for context scaling](http://arxiv.org/abs/2307.03170)。'
- en: Von Oswald et al. (2023) Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo,
    João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
    2023. Transformers learn in-context by gradient descent. In *International Conference
    on Machine Learning*, pages 35151–35174\. PMLR.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Von Oswald等（2023）Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João
    Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, 和 Max Vladymyrov. 2023. 变压器通过梯度下降在上下文中学习。在*国际机器学习会议*，第35151–35174页。PMLR。
- en: 'Wang et al. (2023a) Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong
    Meng, Jie Zhou, and Xu Sun. 2023a. Label words are anchors: An information flow
    perspective for understanding in-context learning. *arXiv preprint arXiv:2305.14160*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2023a）Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng,
    Jie Zhou, 和 Xu Sun. 2023a. 标签词是锚点：理解上下文学习的信息流视角。*arXiv预印本 arXiv:2305.14160*。
- en: Wang et al. (2023b) Liang Wang, Nan Yang, and Furu Wei. 2023b. Learning to retrieve
    in-context examples for large language models. *arXiv preprint arXiv:2307.07164*.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2023b）Liang Wang, Nan Yang, 和 Furu Wei. 2023b. 学习在上下文中检索示例以适应大型语言模型。*arXiv预印本
    arXiv:2307.07164*。
- en: Wang et al. (2023c) Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan,
    Jianfeng Gao, and Furu Wei. 2023c. Augmenting language models with long-term memory.
    *arXiv preprint arXiv:2306.07174*.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2023c）Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng
    Gao, 和 Furu Wei. 2023c. 通过长期记忆增强语言模型。*arXiv预印本 arXiv:2306.07174*。
- en: Wang et al. (2023d) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H.
    Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023d. [Self-consistency
    improves chain of thought reasoning in language models](https://openreview.net/forum?id=1PL1NIMMrw).
    In *The Eleventh International Conference on Learning Representations*.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2023d）Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan
    Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2023d. [自一致性提升语言模型中的思维链推理](https://openreview.net/forum?id=1PL1NIMMrw)。在*第十一届国际学习表征会议*。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等（2022）Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed
    Chi, Quoc V Le, Denny Zhou等. 2022. Chain-of-thought prompting引发大型语言模型的推理。《神经信息处理系统进展》，35:24824–24837。
- en: Weng et al. (2023) Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping
    Liu, Bin Sun, Kang Liu, and Jun Zhao. 2023. [Large language models are better
    reasoners with self-verification](http://arxiv.org/abs/2212.09561).
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng等（2023）Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu,
    Bin Sun, Kang Liu, 和 Jun Zhao. 2023. [大型语言模型在自我验证下更擅长推理](http://arxiv.org/abs/2212.09561)。
- en: Williams (1992) Ronald J. Williams. 1992. Simple statistical gradient-following
    algorithms for connectionist reinforcement learning.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams（1992）Ronald J. Williams. 1992. 连接主义强化学习的简单统计梯度跟随算法。
- en: Wingate et al. (2022) David Wingate, Mohammad Shoeybi, and Taylor Sorensen.
    2022. Prompt compression and contrastive conditioning for controllability and
    toxicity reduction in language models. *arXiv preprint arXiv:2210.03162*.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wingate等（2022）David Wingate, Mohammad Shoeybi, 和 Taylor Sorensen. 2022. 提示压缩和对比条件用于语言模型的可控性和毒性减少。*arXiv预印本
    arXiv:2210.03162*。
- en: Xiao et al. (2023) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. 2023. Efficient streaming language models with attention sinks. *arXiv*.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao等（2023）Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, 和 Mike Lewis.
    2023. 高效流式语言模型与注意力机制。*arXiv*。
- en: 'Xu et al. (2023) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan
    Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language
    models to follow complex instructions. *arXiv preprint arXiv:2304.12244*.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu等（2023）Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng,
    Chongyang Tao, 和 Daxin Jiang. 2023. Wizardlm: 赋能大型语言模型以跟随复杂指令。*arXiv预印本 arXiv:2304.12244*。'
- en: 'Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate
    problem solving with large language models. *arXiv preprint arXiv:2305.10601*.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao等（2023）Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths,
    Yuan Cao, 和 Karthik Narasimhan. 2023. 思维树：大型语言模型的深思熟虑问题解决。*arXiv预印本 arXiv:2305.10601*。
- en: 'Yoo et al. (2022) Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho,
    Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. 2022. Ground-truth labels
    matter: A deeper look into input-label demonstrations. In *Proceedings of the
    2022 Conference on Empirical Methods in Natural Language Processing*, pages 2422–2437.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoo等（2022）Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo,
    Sang-Woo Lee, Sang-goo Lee, 和 Taeuk Kim。2022。真实标签的重要性：对输入标签演示的深入观察。见于*2022年自然语言处理经验方法会议论文集*，第2422–2437页。
- en: 'Zhenyu et al. (2023) Wu Zhenyu, Wang Yaoxiang, Ye Jiacheng, Feng Jiangtao,
    Xu Jingjing, Qiao Yu, and Wu Zhiyong. 2023. Openicl: An open-source framework
    for in-context learning. *arXiv preprint arXiv:2303.02913*.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhenyu等（2023）Wu Zhenyu, Wang Yaoxiang, Ye Jiacheng, Feng Jiangtao, Xu Jingjing,
    Qiao Yu, 和 Wu Zhiyong。2023。Openicl：一个用于上下文学习的开源框架。*arXiv预印本arXiv:2303.02913*。
- en: Appendix
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: 'This appendix includes additional analysis, the evolution of MRD³, pruner training
    details, additional related works, and prompt settings. These contents are organized
    in separate sections as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录包括额外分析、MRD³的演变、剪枝器训练细节、额外相关工作和提示设置。这些内容按如下独立章节组织：
- en: •
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sec. [A](#A1 "Appendix A Additional Analysis and Case Study ‣ Fewer is More:
    Boosting LLM Reasoning with Reinforced Context Pruning") provides additional analysis
    and case studies, including the comparison of CoT-Influx with context window extension
    methods, performance of CoT-Influx on finetuned LLMs (LLaMA2-13B-Chat and GPT-3.5-Turbo),
    ablation study on the reward design, and sensitivity analysis on hyperparameters
    of the pruner. Additional case studies on the GSM8K with different prompting methods
    are given to extensively prove the effectiveness of our method.'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sec. [A](#A1 "附录A 额外分析与案例研究 ‣ 更少即更多：通过强化上下文剪枝提升LLM推理") 提供了额外的分析和案例研究，包括CoT-Influx与上下文窗口扩展方法的比较、CoT-Influx在微调LLM（LLaMA2-13B-Chat和GPT-3.5-Turbo）上的表现、奖励设计的消融研究，以及剪枝器超参数的敏感性分析。还提供了关于GSM8K的额外案例研究，以广泛证明我们方法的有效性。
- en: •
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sec. [B](#A2 "Appendix B Evolution of MRD3 ‣ Fewer is More: Boosting LLM Reasoning
    with Reinforced Context Pruning") introduces the prompt we used for the evolution
    of the examples in our MRD³. Both the original input and the evolution results
    are given as examples. We also analyze the difficulty and reasoning step distribution
    of different evolution methods and derive a new observation regarding difficulty
    preference for different LLMs.'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sec. [B](#A2 "附录B MRD3演变 ‣ 更少即更多：通过强化上下文剪枝提升LLM推理") 介绍了我们在MRD³演变中使用的提示。原始输入和演变结果都作为示例给出。我们还分析了不同演变方法的难度和推理步骤分布，并得出了有关不同LLM难度偏好的新观察。
- en: •
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sec. [C](#A3 "Appendix C Pruner Training and Evaluation Details ‣ Fewer is
    More: Boosting LLM Reasoning with Reinforced Context Pruning") includes the algorithm
    for training data preparation as a supplement to Algorithm [1](#alg1 "Algorithm
    1 ‣ 4.3 Coarse-to-fine Pruner Design ‣ 4 CoT-Influx Methodology ‣ Fewer is More:
    Boosting LLM Reasoning with Reinforced Context Pruning"). The hyperparameter settings,
    the training dynamic of the pruner, and the detailed introduction of the evaluation
    dataset are also included.'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sec. [C](#A3 "附录C 剪枝器训练和评估细节 ‣ 更少即更多：通过强化上下文剪枝提升LLM推理") 包括了数据准备算法，作为算法[1](#alg1
    "算法1 ‣ 4.3 粗到细剪枝器设计 ‣ 4 CoT-Influx 方法论 ‣ 更少即更多：通过强化上下文剪枝提升LLM推理") 的补充。还包括超参数设置、剪枝器的训练动态，以及评估数据集的详细介绍。
- en: •
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sec. [D](#A4 "Appendix D Additional Related Works ‣ Fewer is More: Boosting
    LLM Reasoning with Reinforced Context Pruning") elaborates previous LLM context
    window extension and LLM in-context learning methods, and analyzes the advantage
    of our proposed CoT-Influx compared with various previous methods.'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sec. [D](#A4 "附录D 额外相关工作 ‣ 更少即更多：通过强化上下文剪枝提升LLM推理") 详细说明了以前的LLM上下文窗口扩展和LLM上下文学习方法，并分析了我们提出的CoT-Influx与各种之前方法的优点。
- en: •
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sec. [E](#A5 "Appendix E Prompt Settings ‣ Fewer is More: Boosting LLM Reasoning
    with Reinforced Context Pruning") demonstrates the prompt we used in this work
    for difficulty and reasoning step evaluation, and GPT-4 based compression on input
    few-shot prompts.'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sec. [E](#A5 "附录E 提示设置 ‣ 更少即更多：通过强化上下文剪枝提升LLM推理") 演示了我们在本研究中用于难度和推理步骤评估的提示，以及基于GPT-4的输入少-shot提示的压缩。
- en: Appendix A Additional Analysis and Case Study
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 额外分析与案例研究
- en: A.1 Comparison with context window extension methods
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 上下文窗口扩展方法比较
- en: 'While our work tackle the challenge of limited context window by pruning the
    redundant input few-shot prompts, another solution is to extend the context window
    of LLMs. We compare the math reasoning performance of LLaMA2-7B with CoT-Influx
    and LLaMA2-7B with 32K token context window extended with Positional Interpolation
    (PI) Chen et al. ([2023a](#bib.bib2)). The results are listed in Table [9](#A1.T9
    "Table 9 ‣ A.1 Comparison with context window extension methods ‣ Appendix A Additional
    Analysis and Case Study ‣ Fewer is More: Boosting LLM Reasoning with Reinforced
    Context Pruning").'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '当我们的工作通过修剪冗余的输入少样本提示来解决有限上下文窗口的问题时，另一种解决方案是扩展 LLM 的上下文窗口。我们比较了 LLaMA2-7B 与
    CoT-Influx 的数学推理表现以及 LLaMA2-7B 在使用位置插值（PI）扩展到 32K 令牌上下文窗口后的表现（Chen et al. ([2023a](#bib.bib2))）。结果列在表
    [9](#A1.T9 "Table 9 ‣ A.1 Comparison with context window extension methods ‣ Appendix
    A Additional Analysis and Case Study ‣ Fewer is More: Boosting LLM Reasoning with
    Reinforced Context Pruning") 中。'
- en: 'Table 9: Comparsion of EM(%) on GSM8K of LLaMA2-7B with CoT-Influx and LLaMA2-7B-32K
    with PI.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：LLaMA2-7B 与 CoT-Influx 以及 LLaMA2-7B-32K 与 PI 在 GSM8K 上的 EM(%) 比较。
- en: '| Number of input shots | 12 | 16 | 20 | 24 | 28 | 32 | 40 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 输入样本数 | 12 | 16 | 20 | 24 | 28 | 32 | 40 |'
- en: '| Average number of tokens | 2108.6 | 2820.6 | 3535.4 | 4217.2 | 4929.1 | 5641.2
    | 7070.8 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 平均令牌数 | 2108.6 | 2820.6 | 3535.4 | 4217.2 | 4929.1 | 5641.2 | 7070.8 |'
- en: '| LLaMA2-7B | 13.87 | 15.08 | 14.02 | - | - | - | - |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | 13.87 | 15.08 | 14.02 | - | - | - | - |'
- en: '| LLaMA2-7B+CoT-Influx | - | - | - | 14.33 | 15.09 | 15.92 | 15.77 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B+CoT-Influx | - | - | - | 14.33 | 15.09 | 15.92 | 15.77 |'
- en: '| LLaMA2-7B-32K | 11.37 | 12.81 | 11.37 | 11.83 | 11.83 | 11.52 | 11.30 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B-32K | 11.37 | 12.81 | 11.37 | 11.83 | 11.83 | 11.52 | 11.30 |'
- en: When the input prompt does not exceed the window token limit (the number of
    input shots is not more than 20), we compare the performance of LLaMA2-7B-32K
    with LLaMA2-7B. When the input prompt exceed the context window length, we apply
    our CoT-Influx to prune the prompts to make sure that they can be directly input
    to LLaMA2-7B without PI. The results show that the context window extension weaken
    the reasoning ability with the same input prompt. The limit of context window
    can be unlocked with our CoT-Influx. Moreover, our observation that LLMs can improve
    reasoning with more helpful CoT examples does not hold true for LLMs with extended
    context window.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入提示不超过窗口令牌限制（输入样本数不超过 20）时，我们比较了 LLaMA2-7B-32K 与 LLaMA2-7B 的表现。当输入提示超过上下文窗口长度时，我们应用我们的
    CoT-Influx 来修剪提示，以确保它们可以直接输入到 LLaMA2-7B 中而不使用 PI。结果显示，相同的输入提示下，上下文窗口扩展会削弱推理能力。我们的
    CoT-Influx 可以解锁上下文窗口的限制。此外，我们的观察发现，LLMs 可以通过更多有帮助的 CoT 示例来改善推理，但对于具有扩展上下文窗口的 LLM
    不适用。
- en: A.2 CoT-Influx on finetuned LLMs
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 CoT-Influx 在微调 LLM 上的表现
- en: 'In Sec. [5.1](#S5.SS1 "5.1 Main Results ‣ 5 Evaluation ‣ Fewer is More: Boosting
    LLM Reasoning with Reinforced Context Pruning"), we verify the effectiveness of
    CoT-Influx on LLaMA2-7B, 13B, and 70B. LLaMA2-chat Touvron et al. ([2023b](#bib.bib45))
    and GPT-3.5-Turbo OpenAI ([2023b](#bib.bib34)) are also the widely adopted LLMs
    that are acquired after supervised instruction finetuning (SIFT) and Reinforcement
    Learning from Human Feedback (RLHF), respectively. The different finetuning strategy
    and the various finetuning data result in unique properties of the LLMs. For example,
    LLaMA2-Chat-13B perform significantly better than LLaMA2-13B on math reasoning
    tasks with zero-shot-cot prompts. To show that our CoT-Influx can also help improve
    the reasoning ability of these finetuned LLMs, we conduct experiments of LLaMA2-13B-Chat
    and GPT-3.5-Turbo (gpt35-turbo-0613) on GSM8K dataset. As shown from the results
    listed in Table [10](#A1.T10 "Table 10 ‣ A.2 CoT-Influx on finetuned LLMs ‣ Appendix
    A Additional Analysis and Case Study ‣ Fewer is More: Boosting LLM Reasoning with
    Reinforced Context Pruning"), our CoT-Influx also surpass a wide range of prompting
    baselines with more input shots and fewer tokens. Specifically on LLaMA2-13B-Chat,
    CoT-Influx achieve an absolute improvement 9.78% compared to TopK retrieval baseline
    with only 57.6% average tokens.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [5.1](#S5.SS1 "5.1 主要结果 ‣ 5 评估 ‣ 更少即更多：通过强化上下文剪枝提升 LLM 推理") 节中，我们验证了 CoT-Influx
    在 LLaMA2-7B、13B 和 70B 上的有效性。LLaMA2-chat Touvron et al. ([2023b](#bib.bib45)) 和
    GPT-3.5-Turbo OpenAI ([2023b](#bib.bib34)) 也是广泛采用的 LLM，这些 LLM 是经过监督指令微调 (SIFT)
    和来自人类反馈的强化学习 (RLHF) 获得的。不同的微调策略和各种微调数据导致 LLM 的独特属性。例如，LLaMA2-Chat-13B 在零样本 CoT
    提示的数学推理任务上表现显著优于 LLaMA2-13B。为了证明我们的 CoT-Influx 也能提升这些微调 LLM 的推理能力，我们在 GSM8K 数据集上对
    LLaMA2-13B-Chat 和 GPT-3.5-Turbo (gpt35-turbo-0613) 进行了实验。根据表 [10](#A1.T10 "表 10
    ‣ A.2 CoT-Influx 在微调 LLM 上 ‣ 附录 A 额外分析与案例研究 ‣ 更少即更多：通过强化上下文剪枝提升 LLM 推理") 中列出的结果，我们的
    CoT-Influx 也超越了大量的提示基线，具有更多的输入样本和更少的标记。具体而言，在 LLaMA2-13B-Chat 上，CoT-Influx 相比
    TopK 检索基线实现了 9.78% 的绝对提升，平均标记仅为 57.6%。
- en: 'Table 10: The EM (%) accuracy on GSM8K with CoT-Influx and other baselines.
    Note that the context window limit of LLaMA2-13B-Chat and GPT-3.5-Turbo are all
    4096 tokens.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：CoT-Influx 和其他基线方法在 GSM8K 上的 EM (%) 准确率。注意，LLaMA2-13B-Chat 和 GPT-3.5-Turbo
    的上下文窗口限制都是 4096 个标记。
- en: '| Method | #Input CoT shots | #Average tokens | LLaMA2-13B-Chat | GPT-3.5-Turbo
    |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | #输入 CoT 样本 | #平均标记 | LLaMA2-13B-Chat | GPT-3.5-Turbo |'
- en: '| Few-shot-CoT Fu et al. ([2023](#bib.bib11)) | 8 | 655 | 27.82 | 72.55 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Few-shot-CoT Fu et al. ([2023](#bib.bib11)) | 8 | 655 | 27.82 | 72.55 |'
- en: '| TopK retrieval Liu et al. ([2021](#bib.bib28)) | 20 | 3535.4 | 31.16 | 70.74
    |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| TopK 检索 Liu et al. ([2021](#bib.bib28)) | 20 | 3535.4 | 31.16 | 70.74 |'
- en: '| TopK+LLMLingua Jiang et al. ([2023](#bib.bib19)) | 40 | 2048.0 | 10.69 |
    49.96 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| TopK+LLMLingua Jiang et al. ([2023](#bib.bib19)) | 40 | 2048.0 | 10.69 |
    49.96 |'
- en: '| CoT-Influx | 48 | 2037.0 | 40.94 | 73.31 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| CoT-Influx | 48 | 2037.0 | 40.94 | 73.31 |'
- en: A.3 Ablation study on reward design
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 奖励设计的消融研究
- en: 'The reward of our CoT-Influx pruner are made up of three parts: math reasoning
    accuracy reward $R_{\text{Acc}}$ is the most important because training without
    this term will cause the pruner not to prune any shot or token and directly output
    the truncated prompt of the redundant input.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 CoT-Influx 剪枝器的奖励由三个部分组成：数学推理准确率奖励 $R_{\text{Acc}}$ 是最重要的，因为没有这个项的训练会导致剪枝器不修剪任何样本或标记，直接输出冗余输入的截断提示。
- en: 'Table 11: The EM (%) accuracy on GSM8K of LLaMA2-7B and LLaMA2-13B with different
    reward function.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：不同奖励函数下 LLaMA2-7B 和 LLaMA2-13B 在 GSM8K 上的 EM (%) 准确率。
- en: '| Reward Function | LLaMA-2-7B | LLaMA-2-13B |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 奖励函数 | LLaMA-2-7B | LLaMA-2-13B |'
- en: '| Full Reward | 15.92 | 32.37 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 全奖励 | 15.92 | 32.37 |'
- en: '| w/o $R_{\text{Acc}}$ | 15.24 | 31.46 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 无 $R_{\text{Acc}}$ | 15.24 | 31.46 |'
- en: '| w/o $R_{\text{Loss}}$ | 14.78 | 31.16 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 无 $R_{\text{Loss}}$ | 14.78 | 31.16 |'
- en: '| w/o $R_{\text{Limit}}$ | 14.25 | 29.72 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 无 $R_{\text{Limit}}$ | 14.25 | 29.72 |'
- en: A.4 Sensitivity analysis on hyperparameters
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 超参数的敏感性分析
- en: We perform sensitivity analysis on the hyperparameters to investigate the robustness
    of our CoT-Influx pruner training. The most important setting in the training
    of our CoT-Influx pruner is the token target $T$ should not be too small).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对超参数进行敏感性分析，以研究 CoT-Influx 剪枝器训练的鲁棒性。CoT-Influx 剪枝器训练中最重要的设置是标记目标 $T$ 不应过小。
- en: 'Table 12: Sensitivity analysis on token target $T$'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：标记目标 $T$ 的敏感性分析
- en: '| Token target $T$ | LLaMA-2-13B |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 标记目标 $T$ | LLaMA-2-13B |'
- en: '| 2048 | 32.37 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 2048 | 32.37 |'
- en: '| 1024 | 29.57 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 1024 | 29.57 |'
- en: '| 3072 | 32.37 |  | Token penalty co-efficient $w$ | LLaMA-2-13B |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 3072 | 32.37 |  | 标记惩罚系数 $w$ | LLaMA-2-13B |'
- en: '| (-1,1) | 32.37 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| (-1,1) | 32.37 |'
- en: '| (-0.5,1) | 31.69 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| (-0.5,1) | 31.69 |'
- en: '| (-1,0.5) | 32.22 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| (-1,0.5) | 32.22 |'
- en: A.5 Case Study on different prompt compression methods
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 关于不同提示压缩方法的案例研究
- en: 'To show how different prompt compression methods prune input few-shot prompts
    in different manners, we given an example of a 8-shot prompt selected using TopK
    retriever. The original full few-shot prompts are listed in the following box:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示不同的提示压缩方法如何以不同方式修剪输入的少量示例提示，我们给出了一个使用TopK检索器选择的8-shot提示的例子。原始的完整少量示例提示列在下面的框中：
- en: 'Original
    full few-shot prompt for math reasoning (1331 tokens): Q: Dave won 11 tickets at the arcade and spent 5 on a beanie.
    Afterward, he won 10 more tickets. Calculate his final ticket count by first finding
    the remaining tickets after his purchase and then adding the newly won tickets.
    A: Let’s think step by step. Dave had 11 tickets, spent 5, leaving him with 6\.
    Then he won 10 more, resulting in: 6 + 10 = 16 tickets. The answer is 16. Q: At
    the carnival, tickets for rides cost 0.75 dollars each, or you can buy a 15-dollar
    armband for unlimited rides for one night. To determine the number of rides where
    the armband’s cost equals that of individual tickets, set up and solve an equation
    involving x, the number of rides. A: Let’s think step by step. Let x be the number
    of rides. Equate the cost of x rides using individual tickets, 0.75x dollars,
    to the 15-dollar armband cost: 0.75x = 15\. Solve for x: x = 15/0.75, which gives
    x = 20\. The answer is 20. Q: Mitch, Jam, and Jay went out for a movie. Mitch
    paid $7 per ticket for 3 friends, Jam purchased 2 popcorn boxes at $1.5 each,
    and Jay got 3 milk teas for $3 each. To equitably split the expenses, how much
    should each of them contribute? A: Let’s think step by step. The total cost of
    3 tickets at $7 each, 2 popcorn boxes at $1.5 each, and 3 milk teas at $3 each
    is $21 + $3 + $9 = $33\. Dividing the overall expenses among 3 friends results
    in a contribution of $33/3 = $11 per person. The answer is $11. Q: Connor is taking
    his date to the movies, with tickets costing $10.00 each. They opt for the large
    popcorn & 2 drink combo meal at $11.00, and each choose a box of candy at $2.50
    per box. Determine the combined expenses for the movie tickets, combo meal, and
    candy to find the total amount Connor will spend on his date. A: Let’s think step
    by step. Calculate the cost of two movie tickets (2 x $10.00 = $20.00), the combo
    meal ($11.00), and two boxes of candy (2 x $2.50 = $5.00), then sum them up ($20.00
    + $11.00 + $5.00 = $36.00). The answer is $36.00. Q: Scott has 4 tickets. Ernest
    starts with 9 tickets and later discovers a stash of 72 more. Calculate the final
    number of tickets Ernest possesses. A: Let’s think step by step. Ernest initially
    holds 9 tickets and acquires 72 additional ones, leading to a total of 9 + 72
    = 81 tickets. The answer is 81. Q: Joseph and his friends watched two movies at
    his place. The first movie lasts 1 hour and 30 minutes, and the second is 30 minutes
    longer. They took 10 minutes for popcorn and double that for fries. Determine,
    in hours, the cumulative time spent cooking and watching movies by breaking down
    each component of time spent. A: Let’s think step by step. First, find the second
    movie’s length (1 hour and 30 minutes + 30 minutes = 2 hours). Then, sum both
    movies’ lengths (1 hour and 30 minutes + 2 hours = 3 hours and 30 minutes). Next,
    calculate cooking time (10 minutes for popcorn + 20 minutes for fries = 30 minutes).
    Lastly, add movie and cooking times (3 hours and 30 minutes + 30 minutes = 4 hours).
    The answer is 4 hours. Q: The movie theater sold a number of tickets to the horror
    and romance movies. The horror movie ticket sales were 18 more than three times
    the romance movie ticket sales. If there were 25 romance movie tickets sold, how
    many tickets were sold for the horror movie, considering the given relationship?
    A: Let’s think step by step. Let "h" represent the horror movie tickets sold.
    Given that h = 3(25) + 18, we can simplify the equation: h = 75 + 18, resulting
    in h = 93\. The answer is 93. Q: On Saturday, Sara purchased 2 movie theater tickets
    at $10.62 each, rented a movie for $1.59, and bought another movie for $13.95\.
    Determine Sara’s total expenditure on movies by performing a step-by-step calculation.
    A: Let’s think step by step. Firstly, calculate the movie tickets’ cost by multiplying
    the ticket price ($10.62) by the quantity (2), resulting in $21.24\. Secondly,
    combine the rental ($1.59) and purchase ($13.95) costs, equaling $15.54\. Lastly,
    sum the ticket cost and rental/purchase cost: $21.24 + $15.54 = $36.78\. The answer
    is $36.78.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 'Original
    full few-shot prompt for math reasoning (1331 tokens): Q: Dave won 11 tickets at the arcade and spent 5 on a beanie.
    Afterward, he won 10 more tickets. Calculate his final ticket count by first finding
    the remaining tickets after his purchase and then adding the newly won tickets.
    A: Let’s think step by step. Dave had 11 tickets, spent 5, leaving him with 6\.
    Then he won 10 more, resulting in: 6 + 10 = 16 tickets. The answer is 16. Q: At
    the carnival, tickets for rides cost 0.75 dollars each, or you can buy a 15-dollar
    armband for unlimited rides for one night. To determine the number of rides where
    the armband’s cost equals that of individual tickets, set up and solve an equation
    involving x, the number of rides. A: Let’s think step by step. Let x be the number
    of rides. Equate the cost of x rides using individual tickets, 0.75x dollars,
    to the 15-dollar armband cost: 0.75x = 15\. Solve for x: x = 15/0.75, which gives
    x = 20\. The answer is 20. Q: Mitch, Jam, and Jay went out for a movie. Mitch
    paid $7 per ticket for 3 friends, Jam purchased 2 popcorn boxes at $1.5 each,
    and Jay got 3 milk teas for $3 each. To equitably split the expenses, how much
    should each of them contribute? A: Let’s think step by step. The total cost of
    3 tickets at $7 each, 2 popcorn boxes at $1.5 each, and 3 milk teas at $3 each
    is $21 + $3 + $9 = $33\. Dividing the overall expenses among 3 friends results
    in a contribution of $33/3 = $11 per person. The answer is $11. Q: Connor is taking
    his date to the movies, with tickets costing $10.00 each. They opt for the large
    popcorn & 2 drink combo meal at $11.00, and each choose a box of candy at $2.50
    per box. Determine the combined expenses for the movie tickets, combo meal, and
    candy to find the total amount Connor will spend on his date. A: Let’s think step
    by step. Calculate the cost of two movie tickets (2 x $10.00 = $20.00), the combo
    meal ($11.00), and two boxes of candy (2 x $2.50 = $5.00), then sum them up ($20.00
    + $11.00 + $5.00 = $36.00). The answer is $36.00. Q: Scott has 4 tickets. Ernest
    starts with 9 tickets and later discovers a stash of 72 more. Calculate the final
    number of tickets Ernest possesses. A: Let’s think step by step. Ernest initially
    holds 9 tickets and acquires 72 additional ones, leading to a total of 9 + 72
    = 81 tickets. The answer is 81. Q: Joseph and his friends watched two movies at
    his place. The first movie lasts 1 hour and 30 minutes, and the second is 30 minutes
    longer. They took 10 minutes for popcorn and double that for fries. Determine,
    in hours, the cumulative time spent cooking and watching movies by breaking down
    each component of time spent. A: Let’s think step by step. First, find the second
    movie’s length (1 hour and 30 minutes + 30 minutes = 2 hours). Then, sum both
    movies’ lengths (1 hour and 30 minutes + 2 hours = 3 hours and 30 minutes). Next,
    calculate cooking time (10 minutes for popcorn + 20 minutes for fries = 30 minutes).
    Lastly, add movie and cooking times (3 hours and 30 minutes + 30 minutes = 4 hours).
    The answer is 4 hours. Q: The movie theater sold a number of tickets to the horror
    and romance movies. The horror movie ticket sales were 18 more than three times
    the romance movie ticket sales. If there were 25 romance movie tickets sold, how
    many tickets were sold for the horror movie, considering the given relationship?
    A: Let’s think step by step. Let "h" represent the horror movie tickets sold.
    Given that h = 3(25) + 18, we can simplify the equation: h = 75 + 18, resulting
    in h = 93\. The answer is 93. Q: On Saturday, Sara purchased 2 movie theater tickets
    at $10.62 each, rented a movie for $1.59, and bought another movie for $13.95\.
    Determine Sara’s total expenditure on movies by performing a step-by-step calculation.
    A: Let’s think step by step. Firstly, calculate the movie tickets’ cost by multiplying
    the ticket price ($10.62) by the quantity (2), resulting in $21.24\. Secondly,
    combine the rental ($1.59) and purchase ($13.95) costs, equaling $15.54\. Lastly,
    sum the ticket cost and rental/purchase cost: $21.24 + $15.54 = $36.78\. The answer
    is $36.78.'
- en: 'Most of the examples above have similar background and target (tickets, movie,
    expense, etc.) but the difficulty and number of reasoning steps are various. In
    addition, the solution of most questions are highly redundant. When performing
    math reasoning with, it is important to select the most suitable and concise examples
    considering the characteristic of the current input question. In our evaluation
    across different methods shown in Sec. [5.1](#S5.SS1 "5.1 Main Results ‣ 5 Evaluation
    ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning"), we
    have empirically observe the previous methods fail to retain the structural integrity
    of prompt. We show the pruned prompt with different methods and similar token
    length in the following box. We can see that Selective Context and LLMLingua frequently
    discard the important part including ‘Q:’, ‘A:’, ‘$\backslash$n’, “Let’s think
    step by step”, and “The answer is” in these examples. Although GPT-4 can retain
    majority of these tokens, the reasoning steps are systematically removed because
    GPT-4 cannot be instructed to utilize the redundancy in both example-level and
    token-level. Our proposed CoT-Influx, however, select the most representative
    examples and only remove the redundant function words.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '上述大多数示例具有类似的背景和目标（票务、电影、费用等），但难度和推理步骤的数量各不相同。此外，大多数问题的解决方案高度冗余。在进行数学推理时，考虑到当前输入问题的特征，选择最合适和简洁的示例非常重要。在我们在第[5.1](#S5.SS1
    "5.1 Main Results ‣ 5 Evaluation ‣ Fewer is More: Boosting LLM Reasoning with
    Reinforced Context Pruning")节中对不同方法的评估中，我们实证观察到，先前的方法未能保持提示的结构完整性。我们展示了在下面的框中使用不同方法和类似令牌长度的修剪提示。我们可以看到，Selective
    Context和LLMLingua经常丢弃重要部分，包括‘Q:’，‘A:’，‘$\backslash$n’，“Let’s think step by step”和“The
    answer is”。虽然GPT-4能够保留这些令牌中的大部分，但由于GPT-4无法被指示利用示例级别和令牌级别的冗余，推理步骤被系统地移除。然而，我们提出的CoT-Influx选择最具代表性的示例，只删除冗余的功能词。'
- en: 'Pruned
    few-shot prompt of different methods: Selective
    Context: Q Dave won 11 tickets Afterward won: step Dave 11 tickets spent leaving
    Then won 10 resulting: 16 Q At tickets rides rides where set solve x: step Let
    x rides Equate x rides individual tickets dollars = x 20 Q Mitch Jam went paid
    per 3 friends Jam purchased equitably how: step 3 tickets + 3 friends results
    $ Q Connor tickets They opt the large popcorn & 2 drink combo meal choose candy
    combo meal candy Connor: step combo boxes sum $ Q Scott 4 tickets starts 9 tickets
    discovers 72 Ernest possesses: step initially holds 9 tickets 72 additional ones
    leading 81 Q Joseph watched lasts They popcorn double hours cooking breaking:
    step First find + Then sum both movies’ lengths + Next, calculate cooking time
    popcorn + Lastly add movie cooking times + 4 hours Q sold 25 romance movie tickets
    considering the given relationship: step Let "h the horror movie tickets Given
    = 18 simplify 75 93 Q Sara purchased rented movies performing: step Firstly calculate
    resulting Secondly combine rental Lastly sum $ LLMLingua: : Dave won11ets the
    and5 a be. After he. his final count by first theets after the: Lets think. Daveets5,,
    in. : the,ets 5, or a-ollarides for one. To theidesband cost equals of, equation
    involving r. A: think. Let.ides using individualets, the1ollar cost5 which. :,
    Jam and Jay a7 ticket3 Jam2orn5 Jay3 milk. To equ the.ets boxes53 milk each1\.
    the overallenses3 friends a. The : Connor is his,.. They theorn & drinkbo and0\.
    theandy think. ofets0 theboal and two then :. Ernest and later a7\. think. Ernest
    initially and, 9: friends at movie the minutes They and for. the spent by think,
    the, calculate The a the and ticket, think.:, bought.by-step calculation. A: Let’s
    think step by step. Firstly, calculate the movie tickets’ cost by multiplying
    the ticket price ($10.62) by the quantity (2), resulting in $21.24\. Secondly,
    combine the rental ($1.59) and purchase ($13.95) costs, equaling $15.54\. Lastly,
    sum the ticket cost and rental/purchase cost: $21.24 + $15.54 = $36.78\. The answer
    is $36.78. GPT-4 Compression: Q: Dave won 11, spent 5 and won 10 more. Determine
    final count. A: The answer is 16. Q: Tickets cost 0.75 per ride, armband cost
    15\. Determine rides that armband’s cost equals tickets. A: The answer is 20.
    Q: $7 per ticket for 3, 2 popcorn boxes at $1.5, 3 milk teas for $3\. Determine
    each contribute. A: The answer is $11. Q: Tickets cost $10.00 each, meal cost
    $11.00, a box of candy at $2.50\. Determine the expenses. A: The answer is $36.00.
    Q: Scott has 4\. Ernest starts with 9 and discovers 72 more. Determine the final
    number. A: The answer is 81. Q: The first 1.5 hour, the second is 30 minutes longer.
    10 minutes for popcorn. Determine the time. A: The answer is 4 hours. Q: Horror
    movie were 18 more than 3 times romance. 25 romance movie sold, Determine number
    of horror movie. A: The answer is 93. Q: Sara purchased 2 at $10.62 each, a movie
    for $1.59, and another $13.95\. Determine total expenditure. A: The answer is
    $36.78. CoT-Influx: Q: Mitch, Jam,
    and went out a. Mitch paid $7 per ticket for 3, Jam purchased 2 boxes at $1.5
    each, and got 3 for $3 each. To equitably split, how much should each them contribute?
    A: Let’s think step by step. The total cost 3 tickets $7 each, 2 popcorn boxes
    $1.5 each, and 3 milk $3 each is $21 + $3 + $9 = $33\. Dividing the overall expenses
    among 3 results of $33/3 = $11 per. The answer is $11. Q: The theater sold number
    tickets to horror and romance movies. The horror movie ticket sales were 18 more
    than three times romance ticket. If there 25 romance sold, how many tickets were
    sold horror movie, considering? A: Let’s think step by step. Let "h" represent
    horror tickets sold. Given h = 3(25) + 18, we can simplify equation: h = 75 +
    18, resulting h = . The answer is 93. Q: On, Sara purchased 2 theater tickets
    $10.62 each, rented movie $1.59, and bought movie $13.95\. Determine Sara’s total
    expenditure movies performing a calculation. A: Let’s think step by step. , calculate
    tickets’ cost price ($10.62) by quantity (2), resulting $21.24\. Secondly, combine
    rental ($1.59) purchase ($13.95), equaling. Lastly, sum ticket rental/purchase:
    $21.24 + $15.54\. The answer is $36.78.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pruned
    few-shot prompt of different methods: Selective
    Context: Q Dave won 11 tickets Afterward won: step Dave 11 tickets spent leaving
    Then won 10 resulting: 16 Q At tickets rides rides where set solve x: step Let
    x rides Equate x rides individual tickets dollars = x 20 Q Mitch Jam went paid
    per 3 friends Jam purchased equitably how: step 3 tickets + 3 friends results
    $ Q Connor tickets They opt the large popcorn & 2 drink combo meal choose candy
    combo meal candy Connor: step combo boxes sum $ Q Scott 4 tickets starts 9 tickets
    discovers 72 Ernest possesses: step initially holds 9 tickets 72 additional ones
    leading 81 Q Joseph watched lasts They popcorn double hours cooking breaking:
    step First find + Then sum both movies’ lengths + Next, calculate cooking time
    popcorn + Lastly add movie cooking times + 4 hours Q sold 25 romance movie tickets
    considering the given relationship: step Let "h the horror movie tickets Given
    = 18 simplify 75 93 Q Sara purchased rented movies performing: step Firstly calculate
    resulting Secondly combine rental Lastly sum $ LLMLingua: : Dave won11ets the
    and5 a be. After he. his final count by first theets after the: Lets think. Daveets5,,
    in. : the,ets 5, or a-ollarides for one. To theidesband cost equals of, equation
    involving r. A: think. Let.ides using individualets, the1ollar cost5 which. :,
    Jam and Jay a7 ticket3 Jam2orn5 Jay3 milk. To equ the.ets boxes53 milk each1\.
    the overallenses3 friends a. The : Connor is his,.. They theorn & drinkbo and0\.
    theandy think. ofets0 theboal and two then :. Ernest and later a7\. think. Ernest
    initially and, 9: friends at movie the minutes They and for. the spent by think,
    the, calculate The a the and ticket, think.:, bought.by-step calculation. A: Let’s
    think step by step. Firstly, calculate the movie tickets’ cost by multiplying
    the ticket price ($10.62) by the quantity (2), resulting in $21.24\. Secondly,
    combine the rental ($1.59) and purchase ($13.95) costs, equaling $15.54\. Lastly,
    sum the ticket cost and rental/purchase cost: $21.24 + $15.54 = $36.78\. The answer
    is $36.78. GPT-4 Compression: Q: Dave won 11, spent 5 and won 10 more. Determine
    final count. A: The answer is 16. Q: Tickets cost 0.75 per ride, armband cost
    15\. Determine rides that armband’s cost equals tickets. A: The answer is 20.
    Q: $7 per ticket for 3, 2 popcorn boxes at $1.5, 3 milk teas for $3\. Determine
    each contribute. A: The answer is $11. Q: Tickets cost $10.00 each, meal cost
    $11.00, a box of candy at $2.50\. Determine the expenses. A: The answer is $36.00.
    Q: Scott has 4\. Ernest starts with 9 and discovers 72 more. Determine the final
    number. A: The answer is 81. Q: The first 1.5 hour, the second is 30 minutes longer.
    10 minutes for popcorn. Determine the time. A: The answer is 4 hours. Q: Horror
    movie were 18 more than 3 times romance. 25 romance movie sold, Determine number
    of horror movie. A: The answer is 93. Q: Sara purchased 2 at $10.62 each, a movie
    for $1.59, and another $13.95\. Determine total expenditure. A: The answer is
    $36.78. CoT-Influx: Q: Mitch, Jam,
    and went out a. Mitch paid $7 per ticket for 3, Jam purchased 2 boxes at $1.5
    each, and got 3 for $3 each. To equitably split, how much should each them contribute?
    A: Let’s think step by step. The total cost 3 tickets $7 each, 2 popcorn boxes
    $1.5 each, and 3 milk $3 each is $21 + $3 + $9 = $33\. Dividing the overall expenses
    among 3 results of $33/3 = $11 per. The answer is $11. Q: The theater sold number
    tickets to horror and romance movies. The horror movie ticket sales were 18 more
    than three times romance ticket. If there 25 romance sold, how many tickets were
    sold horror movie, considering? A: Let’s think step by step. Let "h" represent
    horror tickets sold. Given h = 3(25) + 18, we can simplify equation: h = 75 +
    18, resulting h = . The answer is 93. Q: On, Sara purchased 2 theater tickets
    $10.62 each, rented movie $1.59, and bought movie $13.95\. Determine Sara’s total
    expenditure movies performing a calculation. A: Let’s think step by step. , calculate
    tickets’ cost price ($10.62) by quantity (2), resulting $21.24\. Secondly, combine
    rental ($1.59) purchase ($13.95), equaling. Lastly, sum ticket rental/purchase:
    $21.24 + $15.54\. The answer is $36.78.'
- en: Appendix B Evolution of MRD³
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B MRD³的演变
- en: B.1 Prompt template for evolution
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 演变的提示模板
- en: 'The prompt we used for evolution of the examples in our dataset are listed
    as follow:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于演变的数据集中的示例的提示列举如下：
- en: 'Prompt
    for different evolution strategies I want you to act as a Prompt
    Rewriter. Your objective is to rewrite a given prompt into a more complex version
    to make those famous AI systems (e.g., LLaMA, ChatGPT and GPT4) a bit harder to
    handle. The prompt is made up of a math reasoning question and the corresponding
    answer. The rewritten prompt must be reasonable and must be understood and responded
    by humans. Your rewriting cannot omit or change the input and results in #Given
    Prompt#. Also, please retain the format of ’Question: ’ and ’Answer: ’ in your
    response. You SHOULD complicate the given prompt using the following method: {Evolution
    template} You should try your best not to make the #Rewritten Prompt# become verbose,
    #Rewritten Prompt# can only add 10 to 20 words into #Given Prompt#. The #Rewritten
    Prompt# should also follow the format that the rewritten question appears after
    ’Question: ’ and the rewritten answer appears after ’Answer: ’. The rewritten
    answer should end up with ’The answer is [results]’. #Given Prompt#: Question:
    {Given question} Answer: {Given answer} #Rewritten Prompt#:
    Evolution template for evolution strategy add_constraints: Please
    add one more constraint/requirement to the question of #Given Prompt# Evolution
    template for evolution strategy deepening: Please increase the depth and breadth
    of the question and answer of #Given Prompt# Evolution template for evolution
    strategy increase_reasoning: If #Given Prompt# can be solved with just a few simple
    thinking processes, please rewrite it to explicitly request multiple-step reasoning.
    Evolution template for evolution strategy revise_difficulty: Please revise the
    high difficulty questions to lower difficulty. Evolution template for evolution
    strategy produce_easier: Please produce a new and easier question with another
    different topic.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 'Prompt
    for different evolution strategies I want you to act as a Prompt
    Rewriter. Your objective is to rewrite a given prompt into a more complex version
    to make those famous AI systems (e.g., LLaMA, ChatGPT and GPT4) a bit harder to
    handle. The prompt is made up of a math reasoning question and the corresponding
    answer. The rewritten prompt must be reasonable and must be understood and responded
    by humans. Your rewriting cannot omit or change the input and results in #Given
    Prompt#. Also, please retain the format of ’Question: ’ and ’Answer: ’ in your
    response. You SHOULD complicate the given prompt using the following method: {Evolution
    template} You should try your best not to make the #Rewritten Prompt# become verbose,
    #Rewritten Prompt# can only add 10 to 20 words into #Given Prompt#. The #Rewritten
    Prompt# should also follow the format that the rewritten question appears after
    ’Question: ’ and the rewritten answer appears after ’Answer: ’. The rewritten
    answer should end up with ’The answer is [results]’. #Given Prompt#: Question:
    {Given question} Answer: {Given answer} #Rewritten Prompt#:
    Evolution template for evolution strategy add_constraints: Please
    add one more constraint/requirement to the question of #Given Prompt# Evolution
    template for evolution strategy deepening: Please increase the depth and breadth
    of the question and answer of #Given Prompt# Evolution template for evolution
    strategy increase_reasoning: If #Given Prompt# can be solved with just a few simple
    thinking processes, please rewrite it to explicitly request multiple-step reasoning.
    Evolution template for evolution strategy revise_difficulty: Please revise the
    high difficulty questions to lower difficulty. Evolution template for evolution
    strategy produce_easier: Please produce a new and easier question with another
    different topic.'
- en: Most part of the prompt of different evolution strategies are similar. Based
    on our quantitatively analysis on the difficulty and reasoning step distribution,
    GPT-4 can effectively follow our instruction to modify the constraints or difficulty
    level of input questions.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 不同演变策略的提示大部分相似。根据我们对难度和推理步骤分布的定量分析，GPT-4能够有效地遵循我们的指示来修改输入问题的约束条件或难度级别。
- en: B.2 Difficulty and Reasoning Steps Distribution of MRD³
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 MRD³的难度和推理步骤分布
- en: 'Based on the GPT-4-based estimation, we are able to quantitatively look into
    the distribution of difficulty and reasoning step distribution in MRD³ without
    evolution and MRD³ with various evolution schemes. The results are shown in Figure [6](#A2.F6
    "Figure 6 ‣ B.2 Difficulty and Reasoning Steps Distribution of MRD3 ‣ Appendix
    B Evolution of MRD3 ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context
    Pruning"). The original distribution of both difficulty level and reasoning steps
    of questions centralized between 2 to 4\. More questions with higher difficulty
    using add_constraints, deepening, and increase_reasoning. As we discuss in the
    reward design of our RL pruner, easy questions are important for the stabilization
    of RL and can help effectively identify the quality of pruned prompt, more easier
    questions are generated with revise_difficulty and produce_easier evolution scheme.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 GPT-4 的估算，我们能够定量地分析 MRD³ 在没有进化和具有各种进化方案的情况下的难度分布和推理步骤分布。结果如图 [6](#A2.F6 "图
    6 ‣ B.2 MRD3 的难度和推理步骤分布 ‣ 附录 B MRD3 的进化 ‣ 少即是多：通过强化上下文剪枝提升 LLM 推理") 所示。问题的原始难度水平和推理步骤分布集中在
    2 到 4 之间。使用 add_constraints、deepening 和 increase_reasoning 的高难度问题更多。正如我们在 RL 剪枝器的奖励设计中讨论的，简单问题对
    RL 的稳定性非常重要，并能有效识别剪枝提示的质量， revise_difficulty 和 produce_easier 进化方案生成更多简单问题。
- en: '![Refer to caption](img/b182126af4512b5a500baaf4ac15562d.png)![Refer to caption](img/093c2237885b754e2a23651122b67dea.png)![Refer
    to caption](img/06fedce597261175bdfc5e20aab918f7.png)![Refer to caption](img/c8708b4ca6b0aeb572feca047505c7ad.png)![Refer
    to caption](img/7e25a61796ec05b6967e817e7a33b2a9.png)![Refer to caption](img/8170333397d7c0431c52b9ef8062814c.png)![Refer
    to caption](img/3f2188680d1da9ad8eb4223dae9508c3.png)![Refer to caption](img/2c4ce83e17bc97a964dfab4247575595.png)![Refer
    to caption](img/17094718cedddeb87b996bc901c5a50b.png)![Refer to caption](img/20fb45f091c7258a963131d4be924e3a.png)![Refer
    to caption](img/8ba0dab79e33f51f0e9020d3e1135601.png)![Refer to caption](img/e50abb931b373a9036660d8beb3c798c.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/b182126af4512b5a500baaf4ac15562d.png)![参见说明文字](img/093c2237885b754e2a23651122b67dea.png)![参见说明文字](img/06fedce597261175bdfc5e20aab918f7.png)![参见说明文字](img/c8708b4ca6b0aeb572feca047505c7ad.png)![参见说明文字](img/7e25a61796ec05b6967e817e7a33b2a9.png)![参见说明文字](img/8170333397d7c0431c52b9ef8062814c.png)![参见说明文字](img/3f2188680d1da9ad8eb4223dae9508c3.png)![参见说明文字](img/2c4ce83e17bc97a964dfab4247575595.png)![参见说明文字](img/17094718cedddeb87b996bc901c5a50b.png)![参见说明文字](img/20fb45f091c7258a963131d4be924e3a.png)![参见说明文字](img/8ba0dab79e33f51f0e9020d3e1135601.png)![参见说明文字](img/e50abb931b373a9036660d8beb3c798c.png)'
- en: 'Figure 6: The difficulty distribution (first row) and the number of reasoning
    steps distribution (second row).'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：难度分布（第一行）和推理步骤分布（第二行）。
- en: B.3 Additional observation on difficulty distribution
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 难度分布的附加观察
- en: 'As shown in Figure [6](#A2.F6 "Figure 6 ‣ B.2 Difficulty and Reasoning Steps
    Distribution of MRD3 ‣ Appendix B Evolution of MRD3 ‣ Fewer is More: Boosting
    LLM Reasoning with Reinforced Context Pruning"), the difficulty diversity of examples
    in MRD³ are improved after prompt evolution. We then research into the difficulty
    distribution of the input examples for in-context learning. The observation is
    shown as follow in addition to the 3 main observations proposed in Sec. [3](#S3
    "3 Pilot Study ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context
    Pruning"):'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [6](#A2.F6 "图 6 ‣ B.2 MRD3 的难度和推理步骤分布 ‣ 附录 B MRD3 的进化 ‣ 少即是多：通过强化上下文剪枝提升
    LLM 推理") 所示，MRD³ 示例的难度多样性在提示进化后得到了改善。我们随后研究了用于上下文学习的输入示例的难度分布。观察结果如第 [3](#S3 "3
    初步研究 ‣ 少即是多：通过强化上下文剪枝提升 LLM 推理") 节提出的 3 个主要观察结果的补充所示。
- en: 'Observation 4: LLMs with different capabilities prefer CoT examples of varying
    difficulties.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 观察 4：具有不同能力的 LLM 更喜欢不同难度的 CoT 示例。
- en: 'In our further exploration of the optimal selection of CoT examples for improve
    mathematical reasoning, we observe that LLMs of different capabilities exhibit
    preferences for CoT examples of varying difficulty levels. As Table [13](#A2.T13
    "Table 13 ‣ B.3 Additional observation on difficulty distribution ‣ Appendix B
    Evolution of MRD3 ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context
    Pruning") shows, we categorize each CoT example in the MRD³-Evol dataset by difficulty
    level. We then select the top 16 CoT examples from different groups as few-shot
    examples for LLaMA2 models. Results show LLaMA2-7b prefers CoT examples with a
    difficulty level of 3-4, while LLaMA2-13b, more capable, prefers those with a
    difficulty level of 4 or above. This aligns with intuition: for instance, when
    assisting a middle school student with math problems, it is more beneficial to
    provide examples of moderate difficulty that they can comprehend, whereas for
    a high school student, examples with a higher level of difficulty are more useful.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们进一步探索优化选择CoT示例以提升数学推理的过程中，我们观察到不同能力的LLM对不同难度级别的CoT示例表现出不同的偏好。如表格[13](#A2.T13
    "Table 13 ‣ B.3 Additional observation on difficulty distribution ‣ Appendix B
    Evolution of MRD3 ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context
    Pruning")所示，我们按难度级别对MRD³-Evol数据集中的每个CoT示例进行了分类。然后，我们从不同组中选择前16个CoT示例作为LLaMA2模型的少量样本。结果显示，LLaMA2-7b倾向于难度等级为3-4的CoT示例，而更高能力的LLaMA2-13b则偏好难度等级为4或以上的示例。这符合直觉：例如，当协助中学生解决数学问题时，提供他们能理解的中等难度的示例更有利，而对于高中生，难度更高的示例则更有用。'
- en: 'Table 13: Smaller, less capable LLMs favor simpler CoT examples, while larger
    ones prefer more complex ones.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 表13：规模较小、能力较低的LLM倾向于简单的CoT示例，而规模较大、能力更强的LLM则倾向于更复杂的示例。
- en: '| Model | Difficulty ($\leq$ 4) |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 难度 ($\leq$ 4) |'
- en: '| LLaMA2-7B | 14.49 | 15.39 | 14.86 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | 14.49 | 15.39 | 14.86 |'
- en: '| LLaMA2-13B | 23.81 | 25.32 | 25.47 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B | 23.81 | 25.32 | 25.47 |'
- en: In our evaluation of CoT-Influx with various LLMs, we found that the shot selection
    results are consistent with our observation. The average difficulty score and
    number of reasoning steps for the examples selected by LLaMA2-70B pruner are 3.57
    and 3.04, which are higher than the results of LLaMA2-13B are 3.51 and 2.98\.
    The empirical results further support our assumption that LLMs with larger size
    prefers harder examples than smaller-scale LLMs.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对各种LLM进行CoT-Influx评估时，我们发现样本选择结果与我们的观察一致。LLaMA2-70B剪枝器选择的示例的平均难度得分和推理步骤数分别为3.57和3.04，这高于LLaMA2-13B的结果，分别为3.51和2.98。实证结果进一步支持了我们关于更大规模LLM倾向于选择更难示例的假设。
- en: B.4 Evolution example
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4 演变示例
- en: 'We give an example of a math questions and then show the evolved results of
    the questions and answers. The evolved results follow our instruction given in
    Sec. [B.1](#A2.SS1 "B.1 Prompt template for evolution ‣ Appendix B Evolution of
    MRD3 ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning").
    As can be seen from the evolution results, the answer of input questions can be
    modified (e.g. ground truth answer change from 16 to 12 in add_constraints). The
    whole background of the questions can also be replaced (e.g. computation target
    of question change from current tickets at the arcade to final points of a game
    in produce_easier). These modification and variation improve the diversity of
    our prompt candidate sets, which are the building block for the training and reasoning
    with CoT-Influx.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '我们给出了一个数学问题的示例，然后展示了问题和答案的演变结果。演变结果遵循了我们在第[B.1](#A2.SS1 "B.1 Prompt template
    for evolution ‣ Appendix B Evolution of MRD3 ‣ Fewer is More: Boosting LLM Reasoning
    with Reinforced Context Pruning")节中给出的指示。从演变结果可以看出，输入问题的答案可以被修改（例如，在add_constraints中，真实答案从16变为12）。问题的整个背景也可以被替换（例如，在produce_easier中，问题的计算目标从当前的游戏币变为游戏的最终得分）。这些修改和变化提高了我们提示候选集的多样性，这些提示候选集是训练和推理CoT-Influx的基础。'
- en: 'Example
    of prompt evolution Input example for evolution:
    Q: At the arcade Dave won 11 tickets. If he spent 5 tickets on a beanie and later
    won 10 more tickets, how many would he have? A: Dave initially had 11 tickets,
    spent 5 on a beanie, and later won 10 more, resulting in a total of: 11 - 5 +
    10 = 16 tickets. The answer is 16. Evolution result of add_constraints:
    Q: At the arcade, Dave won 11 tickets. If he spent 5 tickets on a beanie, won
    10 more tickets, and gave 4 tickets to his friend, how many would he have? A:
    Dave initially had 11 tickets, spent 5 on a beanie, won 10 more, and gave 4 to
    his friend, resulting in a total of: 11 - 5 + 10 - 4 = 12 tickets. The answer
    is 12. Evolution result of deepening: Q: At the arcade, Dave triumphed in a game
    and earned 11 tickets. He redeemed 5 of those tickets for a beanie and subsequently
    won an additional 10 tickets. What is his final ticket count? A: Dave started
    with 11 tickets, used 5 to acquire a beanie, and later added 10 more to his collection,
    resulting in: 11 - 5 + 10 = 16 tickets. The answer is 16. Evolution result of
    increase_reasoning: Q: Dave won 11 tickets at the arcade and spent 5 on a beanie.
    Afterward, he won 10 more tickets. Calculate his final ticket count by first finding
    the remaining tickets after his purchase and then adding the newly won tickets.
    A: Dave had 11 tickets, spent 5, leaving him with 6\. Then he won 10 more, resulting
    in: 6 + 10 = 16 tickets. The answer is 16. Evolution result of revise_difficulty:
    Q: Dave got 11 tickets at the arcade. He used 5 tickets to get a beanie and then
    won 10 more tickets. How many tickets does he have now? A: Dave started with 11
    tickets, used 5 for a beanie, and won 10 more, giving him: 11 - 5 + 10 = 16 tickets.
    The answer is 16. Evolution result of produce_easier: Q: In a game, Sarah scored
    11 points. She lost 5 points in a penalty and then scored 10 more points. What
    is her final score? A: Sarah initially scored 11 points, lost 5 due to a penalty,
    and scored 10 more, resulting in a total of: 11 - 5 + 10 = 16 points. The answer
    is 16.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 'Example
    of prompt evolution Input example for evolution:
    Q: At the arcade Dave won 11 tickets. If he spent 5 tickets on a beanie and later
    won 10 more tickets, how many would he have? A: Dave initially had 11 tickets,
    spent 5 on a beanie, and later won 10 more, resulting in a total of: 11 - 5 +
    10 = 16 tickets. The answer is 16. Evolution result of add_constraints:
    Q: At the arcade, Dave won 11 tickets. If he spent 5 tickets on a beanie, won
    10 more tickets, and gave 4 tickets to his friend, how many would he have? A:
    Dave initially had 11 tickets, spent 5 on a beanie, won 10 more, and gave 4 to
    his friend, resulting in a total of: 11 - 5 + 10 - 4 = 12 tickets. The answer
    is 12. Evolution result of deepening: Q: At the arcade, Dave triumphed in a game
    and earned 11 tickets. He redeemed 5 of those tickets for a beanie and subsequently
    won an additional 10 tickets. What is his final ticket count? A: Dave started
    with 11 tickets, used 5 to acquire a beanie, and later added 10 more to his collection,
    resulting in: 11 - 5 + 10 = 16 tickets. The answer is 16. Evolution result of
    increase_reasoning: Q: Dave won 11 tickets at the arcade and spent 5 on a beanie.
    Afterward, he won 10 more tickets. Calculate his final ticket count by first finding
    the remaining tickets after his purchase and then adding the newly won tickets.
    A: Dave had 11 tickets, spent 5, leaving him with 6\. Then he won 10 more, resulting
    in: 6 + 10 = 16 tickets. The answer is 16. Evolution result of revise_difficulty:
    Q: Dave got 11 tickets at the arcade. He used 5 tickets to get a beanie and then
    won 10 more tickets. How many tickets does he have now? A: Dave started with 11
    tickets, used 5 for a beanie, and won 10 more, giving him: 11 - 5 + 10 = 16 tickets.
    The answer is 16. Evolution result of produce_easier: Q: In a game, Sarah scored
    11 points. She lost 5 points in a penalty and then scored 10 more points. What
    is her final score? A: Sarah initially scored 11 points, lost 5 due to a penalty,
    and scored 10 more, resulting in a total of: 11 - 5 + 10 = 16 points. The answer
    is 16.'
- en: Appendix C Pruner Training and Evaluation Details
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 剪枝器训练与评估细节
- en: C.1 Detailed algorithm for training data preparation
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 训练数据准备的详细算法
- en: 'As a supplement to phase 1 in Algorithm [1](#alg1 "Algorithm 1 ‣ 4.3 Coarse-to-fine
    Pruner Design ‣ 4 CoT-Influx Methodology ‣ Fewer is More: Boosting LLM Reasoning
    with Reinforced Context Pruning"), we show the algorithm for training data preparation
    in Algorithm [2](#alg2 "Algorithm 2 ‣ C.1 Detailed algorithm for training data
    preparation ‣ Appendix C Pruner Training and Evaluation Details ‣ Fewer is More:
    Boosting LLM Reasoning with Reinforced Context Pruning"). Both the difficulty
    level and number of reasoning step are involved in the GPT-4-based evaluation.
    However, we omit the reasoning step in this algorithm as we only use difficulty
    level in the training set split.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '作为算法 [1](#alg1 "算法 1 ‣ 4.3 粗到细剪枝设计 ‣ 4 CoT-Influx 方法 ‣ 更少即更多: 通过增强上下文剪枝提升 LLM
    推理")第 1 阶段的补充，我们在算法 [2](#alg2 "算法 2 ‣ C.1 详细算法用于训练数据准备 ‣ 附录 C 剪枝训练和评估细节 ‣ 更少即更多:
    通过增强上下文剪枝提升 LLM 推理")中展示了训练数据准备的算法。GPT-4 基础的评估涉及难度等级和推理步骤的数量。然而，我们在此算法中省略了推理步骤，因为我们仅使用了训练集拆分中的难度等级。'
- en: Algorithm 2 Training dataset preparation
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 训练数据集准备
- en: 'Input: CoT dataset $\{x^{\text{cot}}_{i}\}_{i=1}^{L}$,'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '输入: CoT 数据集 $\{x^{\text{cot}}_{i}\}_{i=1}^{L}$,'
- en: 'Output: MRD³ $\mathcal{D}=\{x^{\text{cot}}_{j},d_{j}\}_{j=1}^{L^{\text{MRD${}^{3}$}}}$'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '输出: MRD³ $\mathcal{D}=\{x^{\text{cot}}_{j},d_{j}\}_{j=1}^{L^{\text{MRD${}^{3}$}}}$'
- en: 1:  $\blacktriangleright$
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 1:  $\blacktriangleright$
- en: C.2 Detailed settings and hyperparameters
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 详细设置和超参数
- en: 'The detailed hyper-parameters setting of different LLMs’ pruner are listed
    in Table [14](#A3.T14 "Table 14 ‣ C.2 Detailed settings and hyperparameters ‣
    Appendix C Pruner Training and Evaluation Details ‣ Fewer is More: Boosting LLM
    Reasoning with Reinforced Context Pruning"). Majority of these hyperparameters
    are shared across different LLMs. The evolution subset as the prompt candidates
    for evaluation are determined by searching the performance of math reasoning on
    100 random examples.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '不同 LLMs 剪枝的详细超参数设置列在表 [14](#A3.T14 "表 14 ‣ C.2 详细设置和超参数 ‣ 附录 C 剪枝训练和评估细节 ‣
    更少即更多: 通过增强上下文剪枝提升 LLM 推理")中。这些超参数大多数在不同 LLMs 之间共享。演变子集作为评估的提示候选是通过在 100 个随机示例上搜索数学推理性能来确定的。'
- en: 'Table 14: Detailed hyper-parameters for pruner training scheme of different
    LLMs.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '表 14: 不同 LLMs 剪枝训练方案的详细超参数。'
- en: '| Model | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | LLaMA2-7B | LLaMA2-13B | LLaMA2-70B |'
- en: '| Epoch | 3 | 3 | 3 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 迭代次数 | 3 | 3 | 3 |'
- en: '| Batch Size | 1 | 1 | 1 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 1 | 1 | 1 |'
- en: '| Pruner LLM Base | LLaMA2-13B | LLaMA2-13B | LLaMA2-70B |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝 LLM 基础 | LLaMA2-13B | LLaMA2-13B | LLaMA2-70B |'
- en: '| Input Shot | 40 | 48 | 48 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 输入样本 | 40 | 48 | 48 |'
- en: '| Input Shot (TopK) | 32 | 32 | 32 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 输入样本 (TopK) | 32 | 32 | 32 |'
- en: '| Input Shot (Few-shot) | 8 | 16 | 16 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 输入样本 (Few-shot) | 8 | 16 | 16 |'
- en: '| Optimizer | AdamW | AdamW | AdamW |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | AdamW | AdamW | AdamW |'
- en: '| Weight Decay | 1$e^{-2}$ |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 权重衰减 | 1$e^{-2}$ |'
- en: '| Learning Rate | 1$e^{-5}$ |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 1$e^{-5}$ |'
- en: '| Embedding Extractor | BERT-Large (cased) | BERT-Large (cased) | BERT-Large
    (cased) |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入提取器 | BERT-Large (cased) | BERT-Large (cased) | BERT-Large (cased) |'
- en: '| Embedding Size | 1024 | 1024 | 1024 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入大小 | 1024 | 1024 | 1024 |'
- en: '| Tokenizer Padding | 512 | 512 | 512 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 分词器填充 | 512 | 512 | 512 |'
- en: '| Difficulty Threshold $d_{\text{thr}}$ | 2 | 2 | 2 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 难度阈值 $d_{\text{thr}}$ | 2 | 2 | 2 |'
- en: '| Token Target $T$ | 2048 | 2048 | 2048 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 令牌目标 $T$ | 2048 | 2048 | 2048 |'
- en: '| Token Penalty Coefficient $w$ | (-1,1) | (-1,1) | (-1,1) |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 令牌惩罚系数 $w$ | (-1,1) | (-1,1) | (-1,1) |'
- en: '| Selection Repeat $t_{\text{repeat}}$ | 10 | 10 | 5 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 选择重复 $t_{\text{repeat}}$ | 10 | 10 | 5 |'
- en: '| Evol Subset | add_constraints | increase_reasoning | increase_reasoning |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 演变子集 | add_constraints | increase_reasoning | increase_reasoning |'
- en: '| temperature | 0.8 | 0.8 | 0.8 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | 0.8 | 0.8 | 0.8 |'
- en: '| top_p | 0.95 | 0.95 | 0.95 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| top_p | 0.95 | 0.95 | 0.95 |'
- en: '| top_k | 40 | 40 | 40 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| top_k | 40 | 40 | 40 |'
- en: '| num_beams | 1 | 1 | 1 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| num_beams | 1 | 1 | 1 |'
- en: '| max_new_tokens | 1 | 1 | 1 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| max_new_tokens | 1 | 1 | 1 |'
- en: C.3 Training dynamics
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 训练动态
- en: 'We visualize the RL training dynamics of the LLaMA2-13B pruner in Figure [7](#A3.F7
    "Figure 7 ‣ C.3 Training dynamics ‣ Appendix C Pruner Training and Evaluation
    Details ‣ Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning")
    including the LLM loss reward $\frac{1}{1+L_{\text{LLM}}}$, the oscillation phenomenon
    are more obvious compared with other reward term. This highlight the effectiveness
    of question repetition and using Exponential Moving Average (EMA) of final reward
    to suppress this oscillation.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图 [7](#A3.F7 "图 7 ‣ C.3 训练动态 ‣ 附录 C 剪枝器训练和评估细节 ‣ 少即是多：通过强化上下文剪枝提升 LLM 推理")
    中可视化了 LLaMA2-13B 剪枝器的 RL 训练动态，包括 LLM 损失奖励 $\frac{1}{1+L_{\text{LLM}}}$，与其他奖励项相比，振荡现象更为明显。这突显了问题重复和使用最终奖励的指数移动平均
    (EMA) 来抑制这种振荡的有效性。
- en: '![Refer to caption](img/ca075ad6ec629d6542b516dc65048d78.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ca075ad6ec629d6542b516dc65048d78.png)'
- en: ((a)) LLM loss reward $\frac{1}{1+L_{\text{LLM}}}$
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ((a)) LLM 损失奖励 $\frac{1}{1+L_{\text{LLM}}}$
- en: '![Refer to caption](img/8463e3596081730d4f4adc1d90f82fd9.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8463e3596081730d4f4adc1d90f82fd9.png)'
- en: ((b)) Prediction reward $R_{\text{Acc}}$
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: ((b)) 预测奖励 $R_{\text{Acc}}$
- en: '![Refer to caption](img/34cd71879fa3e0f77eadaccf59a8e41b.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/34cd71879fa3e0f77eadaccf59a8e41b.png)'
- en: ((c)) EMA pruner reward $R$
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ((c)) EMA 剪枝奖励 $R$
- en: '![Refer to caption](img/e46074268cdd24374a87d434d41563ed.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e46074268cdd24374a87d434d41563ed.png)'
- en: ((d)) Remaining token $t$
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ((d)) 剩余的令牌 $t$
- en: 'Figure 7: RL training dynamics of the LLaMA2-13B pruner.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：LLaMA2-13B 剪枝器的 RL 训练动态。
- en: C.4 Detailed introduction of dataset for evaluation
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4 评估数据集的详细介绍
- en: 'We introduce the details of the datasets we used for evaluation as follows:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍如下用于评估的数据集的详细信息：
- en: •
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GSM8K Cobbe et al. ([2021](#bib.bib6)) is a math reasoning dataset consisting
    high quality linguistically diverse grade school math word problems created by
    human problem writers. There are 7473 training examples and 1319 validation examples
    in the dataset.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GSM8K Cobbe 等人 ([2021](#bib.bib6)) 是一个数学推理数据集，包含由人工问题编写者创建的高质量语言多样化的学年数学词题。数据集中有7473个训练示例和1319个验证示例。
- en: •
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SVAMP Patel et al. ([2021](#bib.bib35)) representing Simple Variations on Arithmetic
    Math word Problems that conduct question sensitivity variation, reasoning ability
    variation, and structural variation on existing math datasets. There is a total
    of 1000 examples and all of them are used for evaluation in our settings.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SVAMP Patel 等人 ([2021](#bib.bib35)) 代表了在现有数学数据集上进行问题敏感性变化、推理能力变化和结构变化的简单变体。数据集中共有1000个示例，所有示例都用于我们设置中的评估。
- en: •
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MultiArith Roy and Roth ([2015](#bib.bib40)) is a collection of multi-step arithmetic
    problems with 600 examples and all of them are used for evaluation in our settings.
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MultiArith Roy 和 Roth ([2015](#bib.bib40)) 是一个包含600个多步骤算术问题的集合，所有问题都用于我们设置中的评估。
- en: •
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: AddSub Hosseini et al. ([2014](#bib.bib18)) is a dataset consisting of addition
    and subtraction problems with 395 examples and all of them are used for evaluation
    in our settings.
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: AddSub Hosseini 等人 ([2014](#bib.bib18)) 是一个包含395个加法和减法问题的数据集，所有问题都用于我们设置中的评估。
- en: •
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SingleEq Koncel-Kedziorski et al. ([2015](#bib.bib22)) consists grade-school
    algebra word problems that map to single equations with varying length. Every
    equation may involve multiple math operations including multiplication, division,
    subtraction, and addition over non-negative rational numbers and only one variable.
    There are 508 problems, 1117 sentences, and 15292 words in the dataset.
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SingleEq Koncel-Kedziorski 等人 ([2015](#bib.bib22)) 包含将学年数学词题映射到单个方程的变长问题。每个方程可能涉及多个数学运算，包括乘法、除法、减法和加法，涉及非负有理数和仅一个变量。数据集中有508个问题、1117个句子和15292个单词。
- en: C.5 Rule-based prompt reconstruction
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.5 基于规则的提示重建
- en: To make sure the input prompt for inference remain structurally intact, we apply
    a rule-based prompt reconstruction on the input. For example, “$\backslash$n’,
    “Let’s think step by step”, and “The answer is”.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保推理的输入提示保持结构完整，我们对输入应用基于规则的提示重建。例如，“$\backslash$n”， “让我们一步一步思考”和“答案是”。
- en: Appendix D Additional Related Works
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 额外相关工作
- en: LLM In-Context Learning In-context learning (ICL) are one of the emerging abilities
    of LLMs that conduct various downstream tasks with provided few-shot demonstrations.
    To fully understand optimize the ICL paradigm, previous research mainly focus
    on the underlying mechanism of ICL or the proper application of ICL. Pioneering
    research Von Oswald et al. ([2023](#bib.bib47)); Dai et al. ([2023](#bib.bib7))
    empirically find the similarity between gradient-descent (GD) and ICL, which interprets
    the trained LLMs are meta-optimizer that can learn the examples in context in
    forward pass. More recently, Wang et al. ([2023a](#bib.bib48)) propose a hypothesis
    that label words in examples serve as anchors in ICL, and the anchors can help
    aggregate and distribute the task-relevant information flow. To better utilize
    ICL, previous research also research on the input format Yoo et al. ([2022](#bib.bib59))
    and order of examples Min et al. ([2022](#bib.bib31)). Our work falls in the second
    category that shows the compressed examples are an optimal choice for the input
    of ICL.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: LLM上下文学习 上下文学习（ICL）是LLMs的 emerging 能力之一，可以通过提供的少量示例来进行各种下游任务。为了全面理解优化ICL范式，之前的研究主要关注ICL的基本机制或ICL的适当应用。开创性研究Von
    Oswald等人（[2023](#bib.bib47)）；Dai等人（[2023](#bib.bib7)）实证发现梯度下降（GD）和ICL之间的相似性，这解释了训练的LLMs是可以在前向传递中学习上下文中的示例的元优化器。最近，Wang等人（[2023a](#bib.bib48)）提出了一种假设，即示例中的标签词在ICL中作为锚点，锚点可以帮助汇总和分配与任务相关的信息流。为了更好地利用ICL，之前的研究还研究了输入格式Yoo等人（[2022](#bib.bib59)）和示例的顺序Min等人（[2022](#bib.bib31)）。我们的工作属于第二类，展示了压缩示例是ICL输入的最佳选择。
- en: LLM Context Window Extension Recently, there has been rising interests in extending
    the context window of existing pre-trained LLMs. Common approaches include augmenting
    external memory modules Tworkowski et al. ([2023](#bib.bib46)); Wang et al. ([2023c](#bib.bib50)),
    which add extra modules to memorize long past contexts but requires complex training,
    manipulating attention mechanisms Han et al. ([2023](#bib.bib14)); Xiao et al.
    ([2023](#bib.bib56)) or the positional encoding Chen et al. ([2023a](#bib.bib2));
    Peng et al. ([2023b](#bib.bib37)). However, these require LLM modifications. Our
    method, applicable to black-box LLMs and extendable context windows, is orthogonal
    to this direction.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: LLM上下文窗口扩展 最近，对扩展现有预训练LLMs的上下文窗口的兴趣日益增加。常见的方法包括增强外部记忆模块Tworkowski等人（[2023](#bib.bib46)）；Wang等人（[2023c](#bib.bib50)），这些方法添加额外模块以记住长时间的过去上下文，但需要复杂的训练，操作注意力机制Han等人（[2023](#bib.bib14)）；Xiao等人（[2023](#bib.bib56)）或位置编码Chen等人（[2023a](#bib.bib2)）；Peng等人（[2023b](#bib.bib37)）。然而，这些方法需要对LLM进行修改。我们的方法适用于黑箱LLMs和可扩展的上下文窗口，与此方向正交。
- en: 'Comparison of CoT-Influx with Previous Methods We summarize the advantage of
    our CoT-Influx compared with previous prompting strategies in Table [15](#A4.T15
    "Table 15 ‣ Appendix D Additional Related Works ‣ Fewer is More: Boosting LLM
    Reasoning with Reinforced Context Pruning"). Gradient-free indicates the methods
    do not need to backward through LLMs. Unlimited-token represents the original
    input prompt for these methods are not limited by the context window length of
    LLMs. Difficulty-aware refers to whether the method take the difficulty of problems
    into the consideration of their prompt design. Dynamic #Shots means we do not
    need to setup a target shot number and the pruned input shot numbers are different
    across various questions. Our CoT-Influx demonstrate significant advantage over
    all previous methods.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 'CoT-Influx与以往方法的比较 我们总结了CoT-Influx与以往提示策略的优势，见表[15](#A4.T15 "表15 ‣ 附录D 其他相关工作
    ‣ 更少即更多：通过强化上下文修剪提升LLM推理能力")。无梯度表示这些方法不需要通过LLMs进行反向传播。无限标记表示这些方法的原始输入提示不受LLMs上下文窗口长度的限制。难度感知指的是方法是否将问题的难度考虑到其提示设计中。动态
    #Shots 意味着我们不需要设置目标shot数量，并且修剪后的输入shot数量在各种问题中不同。我们的CoT-Influx展示了相比所有以往方法的显著优势。'
- en: 'Table 15: Comparison of the advantage of different prompting strategies.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 表15：不同提示策略的优势比较。
- en: '| Methods | Frozen LLMs | Automated | Gradient-free | Unlimited-token | Transferable
    | Interpretable | Difficulty-aware | Dynamic #Shots |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 冻结LLMs | 自动化 | 无梯度 | 无限标记 | 可迁移 | 可解释 | 难度感知 | 动态 #Shots |'
- en: '| Fine-Tuning | ✗ | ✓ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | ✗ | ✓ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ |'
- en: '| Manual Prompt | ✓ | ✗ | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 手动提示 | ✓ | ✗ | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
- en: '| Soft Prompt Tuning | ✓ | ✓ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 软提示调整 | ✓ | ✓ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ |'
- en: '| Prompt Retrieval | ✓ | ✓ | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| Prompt Retrieval | ✓ | ✓ | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
- en: '| AutoPrompt Shin et al. ([2020](#bib.bib43)) | ✓ | ✓ | ✗ | ✗ | ✓ | ✓ | ✗ |
    ✗ |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| AutoPrompt Shin 等 ([2020](#bib.bib43)) | ✓ | ✓ | ✗ | ✗ | ✓ | ✓ | ✗ | ✗ |'
- en: '| RLPrompt Deng et al. ([2022](#bib.bib8)) | ✓ | ✓ | ✓ | ✗ | ✓ | ✓ | ✗ | ✗
    |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| RLPrompt Deng 等 ([2022](#bib.bib8)) | ✓ | ✓ | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
- en: '| Context Extension | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 上下文扩展 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ |'
- en: '| LLMLingua Jiang et al. ([2023](#bib.bib19)) | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ |
    ✗ |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| LLMLingua Jiang 等 ([2023](#bib.bib19)) | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ |'
- en: '| CoT-Influx(Ours) | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| CoT-Influx（我们的） | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: Appendix E Prompt Settings
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 提示设置
- en: 'In this section, we show the prompt we used in this work for reproducibility.
    The prompt for evaluating the difficulty and reasoning steps of each examples
    are:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了为可重复性目的而在这项工作中使用的提示。用于评估每个示例的难度和推理步骤的提示如下：
- en: 'Prompt
    for difficulty and reasoning steps estimation: We
    would like you to evaluate and rate the difficulty and complexity of the following
    question. You should first give an overall score on a scale of 1 to 10, where
    a higher score indicates higher difficulty and complexity. You should then evaluate
    the answer and give how many reasoning steps are in the answer. You must just
    give the score and the number of reasoning steps without any other reasons. The
    reply format should be ’Score’: [score], ’Steps: [#steps]’ ## Question: {Given
    question} ## Answer: {Given answer} ## Evaluation:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 'Prompt
    for difficulty and reasoning steps estimation: We
    would like you to evaluate and rate the difficulty and complexity of the following
    question. You should first give an overall score on a scale of 1 to 10, where
    a higher score indicates higher difficulty and complexity. You should then evaluate
    the answer and give how many reasoning steps are in the answer. You must just
    give the score and the number of reasoning steps without any other reasons. The
    reply format should be ’Score’: [score], ’Steps: [#steps]’ ## Question: {Given
    question} ## Answer: {Given answer} ## Evaluation:'
- en: The prompt for GPT-4 Compression on prompts are shown as follow. Note that we
    encode the restriction of token limits in both the prompt and API by setting the
    max_new_token. However, the prompt compression results still fail to follow the
    restrictions for most cases. This disadvantages of uncontrollable token length
    is also discussed in previous work Jiang et al. ([2023](#bib.bib19)).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 提示压缩的提示如下所示。请注意，我们通过设置 max_new_token 编码了提示和 API 中的令牌限制。然而，大多数情况下，提示压缩结果仍未能遵守这些限制。前述工作 Jiang
    等 ([2023](#bib.bib19)) 中也讨论了这种不可控令牌长度的缺点。
- en: 'Prompt
    for GPT-4-based compression: Please compress the following
    examplars for few-shot in-context learning on math reasoning. The complete examplars
    could be removed if they are redundant and the tokens within each examplars can
    also be pruned. ’The answer is’ in each examplar should be retained and please
    keep less than {Given token} tokens in total: {Given examplars}'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 'Prompt
    for GPT-4-based compression: Please compress the following
    examplars for few-shot in-context learning on math reasoning. The complete examplars
    could be removed if they are redundant and the tokens within each examplars can
    also be pruned. ’The answer is’ in each examplar should be retained and please
    keep less than {Given token} tokens in total: {Given examplars}'
