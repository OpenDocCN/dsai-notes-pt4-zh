- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:54:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:54:52'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of
    Rerankers'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'UDAPDR: 通过 LLM 提示和重新排序模型的蒸馏实现无监督领域适应'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2303.00807](https://ar5iv.labs.arxiv.org/html/2303.00807)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2303.00807](https://ar5iv.labs.arxiv.org/html/2303.00807)
- en: Jon Saad-Falcon¹   Omar Khattab¹   Keshav Santhanam¹   Radu Florian²   Martin
    Franz²   Salim Roukos²   Avirup Sil²   Md Arafat Sultan²   Christopher Potts¹
    \AND¹Stanford University  ²IBM Research AI
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jon Saad-Falcon¹   Omar Khattab¹   Keshav Santhanam¹   Radu Florian²   Martin
    Franz²   Salim Roukos²   Avirup Sil²   Md Arafat Sultan²   Christopher Potts¹
    \AND¹斯坦福大学  ²IBM 研究院
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Many information retrieval tasks require large labeled datasets for fine-tuning.
    However, such datasets are often unavailable, and their utility for real-world
    applications can diminish quickly due to domain shifts. To address this challenge,
    we develop and motivate a method for using large language models (LLMs) to generate
    large numbers of synthetic queries cheaply. The method begins by generating a
    small number of synthetic queries using an expensive LLM. After that, a much less
    expensive one is used to create large numbers of synthetic queries, which are
    used to fine-tune a family of reranker models. These rerankers are then distilled
    into a single efficient retriever for use in the target domain. We show that this
    technique boosts zero-shot accuracy in long-tail domains and achieves substantially
    lower latency than standard reranking methods.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 许多信息检索任务需要大量标注数据集进行微调。然而，这些数据集通常难以获取，而且由于领域偏移，它们在实际应用中的效用可能迅速下降。为了解决这一挑战，我们开发并提出了一种方法，利用大型语言模型（LLMs）以低成本生成大量合成查询。该方法首先使用昂贵的
    LLM 生成少量合成查询。随后，使用成本较低的 LLM 生成大量合成查询，这些查询用于微调一系列重新排序模型。这些重新排序模型随后被蒸馏成一个高效的检索器，用于目标领域。我们展示了这一技术能够提高长尾领域的零样本准确率，并显著降低延迟，相较于标准重新排序方法表现更佳。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The advent of neural information retrieval (IR) has led to notable performance
    improvements on document and passage retrieval tasks Nogueira and Cho ([2019](#bib.bib35));
    Khattab and Zaharia ([2020](#bib.bib25)); Formal et al. ([2021](#bib.bib11)) as
    well as downstream knowledge-intensive NLP tasks such as open-domain question-answering
    and fact verification Guu et al. ([2020](#bib.bib12)); Lewis et al. ([2020](#bib.bib30));
    Khattab et al. ([2021](#bib.bib23)); Izacard et al. ([2022](#bib.bib20)). Neural
    retrievers for these tasks often benefit from fine-tuning on large labeled datasets
    such as SQuAD Rajpurkar et al. ([2018](#bib.bib39)), Natural Questions (NQ) Kwiatkowski
    et al. ([2019](#bib.bib28)), and KILT Petroni et al. ([2021](#bib.bib38)). However,
    IR models can experience significant drops in accuracy due to distribution shifts
    from the training to the target domain Thakur et al. ([2021](#bib.bib45)); Santhanam
    et al. ([2022b](#bib.bib42)). For example, dense retrieval models trained on MS MARCO
    Nguyen et al. ([2016](#bib.bib34)) might not generalize well to queries about
    COVID-19 scientific publications Voorhees et al. ([2021](#bib.bib47)); Wang et al.
    ([2020](#bib.bib49)), considering for instance that MS MARCO predates COVID-19
    and thus lacks related topics.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 神经信息检索（IR）的出现已显著提高了文档和段落检索任务的性能 Nogueira 和 Cho ([2019](#bib.bib35)); Khattab
    和 Zaharia ([2020](#bib.bib25)); Formal 等 ([2021](#bib.bib11))，以及下游知识密集型 NLP 任务，如开放域问答和事实验证
    Guu 等 ([2020](#bib.bib12)); Lewis 等 ([2020](#bib.bib30)); Khattab 等 ([2021](#bib.bib23));
    Izacard 等 ([2022](#bib.bib20))。这些任务的神经检索器通常通过在大型标注数据集上进行微调来获益，如 SQuAD Rajpurkar
    等 ([2018](#bib.bib39))、自然问题（NQ）Kwiatkowski 等 ([2019](#bib.bib28)) 和 KILT Petroni
    等 ([2021](#bib.bib38))。然而，IR 模型在从训练域到目标域的分布偏移中可能会经历显著的准确率下降 Thakur 等 ([2021](#bib.bib45));
    Santhanam 等 ([2022b](#bib.bib42))。例如，基于 MS MARCO Nguyen 等 ([2016](#bib.bib34))
    训练的密集检索模型可能无法很好地泛化到有关 COVID-19 科学出版物的查询 Voorhees 等 ([2021](#bib.bib47)); Wang
    等 ([2020](#bib.bib49))，考虑到 MS MARCO 的数据早于 COVID-19，因此缺乏相关话题。
- en: '![Refer to caption](img/c71ac9a3105bcd0e6a127d9b6cdf190d.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c71ac9a3105bcd0e6a127d9b6cdf190d.png)'
- en: 'Figure 1: Overview of UDAPDR. An expensive LLM like GPT-3 is used to create
    an initial set of synthetic queries. These are incorporated into a set of prompts
    for a less expensive LLM that can generate large numbers of synthetic queries
    cheaply. The queries stemming from each prompt are used to train separate rerankers,
    and these are distilled into a single ColBERTv2 retriever for use in the target
    domain.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：UDAPDR的概述。像GPT-3这样的昂贵LLM用于创建一组初始的合成查询。这些查询被纳入到一组提示中，用于一个成本较低的LLM，该LLM可以便宜地生成大量合成查询。每个提示产生的查询用于训练单独的重新排序器，这些重新排序器被蒸馏成一个用于目标领域的单一ColBERTv2检索器。
- en: Recent work has sought to adapt IR models to new domains by using large language
    models (LLMs) to create synthetic target-domain datasets for fine-tuning retrievers
    Bonifacio et al. ([2022](#bib.bib3)); Meng et al. ([2022](#bib.bib33)); Dua et al.
    ([2022](#bib.bib10)). For example, using synthetic queries, Thakur et al. ([2021](#bib.bib45))
    and Dai et al. ([2022](#bib.bib8)) fine-tune the retriever itself and train a
    cross-encoder to serve as a passage reranker for improving retrieval accuracy.
    This significantly improves retriever performance in novel domains, but it comes
    at a high computational cost stemming from extensive use of LLMs. This has limited
    the applicability of these methods for researchers and practitioners, particularly
    in high-demand, user-facing settings.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究试图通过使用大型语言模型（LLMs）创建合成目标领域数据集来调整信息检索（IR）模型，以对检索器进行微调（Bonifacio et al. ([2022](#bib.bib3));
    Meng et al. ([2022](#bib.bib33)); Dua et al. ([2022](#bib.bib10))）。例如，Thakur et
    al. ([2021](#bib.bib45)) 和 Dai et al. ([2022](#bib.bib8)) 使用合成查询对检索器本身进行微调，并训练一个交叉编码器作为段落重新排序器以提高检索准确性。这显著改善了在新领域中的检索器性能，但由于广泛使用LLM，导致了高计算成本。这限制了这些方法在高需求、用户面向的设置中的适用性，特别是对于研究人员和从业者。
- en: 'In this paper, we develop Unsupervised Domain Adaptation via LLM Prompting
    and Distillation of Rerankers (UDAPDR),¹¹1pronounced: Yoo-Dap-ter an efficient
    strategy for using LLMs to facilitate unsupervised domain adaptation of neural
    retriever models. We show that UDAPDR leads to large gains in zero-shot settings
    on a diverse range of domains.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们开发了通过LLM提示和重新排序器蒸馏的无监督领域适应方法（UDAPDR），¹¹1发音为：Yoo-Dap-ter，这是一种高效利用LLM来促进神经检索模型的无监督领域适应的策略。我们展示了UDAPDR在各种领域的零样本设置中带来了显著的提升。
- en: 'The approach is outlined in [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers").
    We begin with a collection of passages from a target domain (no in-domain queries
    or labels are required) as well as a prompting strategy incorporating these passages
    with the goal of query generation. A powerful (and perhaps expensive) language
    model like GPT-3 is used to create a modest number of synthetic queries. These
    queries form the basis for corpus-adapted prompts that provide examples of passages
    with good and bad queries, with the goal of generating good queries for new target
    domain passages. These prompts are fed to a smaller (and presumably less expensive)
    LM that can generate a very large number of queries for fine-tuning neural rerankers.
    We train a separate reranker on the queries from each of these corpus-adapted
    prompts, and these rerankers are distilled into a single student ColBERTv2 retriever
    (Khattab and Zaharia, [2020](#bib.bib25); Santhanam et al., [2022b](#bib.bib42),
    [a](#bib.bib41)), which is evaluated on the target domain.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 方法概述见[图1](#S1.F1 "图1 ‣ 1 引言 ‣ UDAPDR：通过LLM提示和重新排序器蒸馏的无监督领域适应")。我们从目标领域收集一组段落（无需领域内查询或标签），以及一种将这些段落结合起来生成查询的提示策略。像GPT-3这样强大的（可能价格昂贵的）语言模型用于创建少量的合成查询。这些查询构成了适应语料库的提示的基础，这些提示提供了包含良好和不良查询的段落示例，目的是为新的目标领域段落生成良好的查询。这些提示被送入一个较小的（也许价格较便宜的）语言模型，该模型可以生成大量的查询用于微调神经重新排序器。我们在每个这些适应语料库的提示产生的查询上训练一个单独的重新排序器，并将这些重新排序器蒸馏成一个单一的学生ColBERTv2检索器（Khattab
    和 Zaharia, [2020](#bib.bib25); Santhanam et al., [2022b](#bib.bib42), [a](#bib.bib41)），在目标领域进行评估。
- en: 'By distilling from multiple passage rerankers instead of a single one, we improve
    the utility of ColBERTv2, preserving more retrieval accuracy gains while reducing
    latency at inference. Our core contributions are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从多个段落重新排序器进行蒸馏，而不是单个重新排序器，我们提高了ColBERTv2的实用性，保留了更多的检索准确性提升，同时减少了推理时的延迟。我们的核心贡献如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose UDAPDR, a novel unsupervised domain adaptation method for neural
    IR that strategically leverages expensive LLMs like GPT-3 (Brown et al., [2020](#bib.bib4))
    and less expensive ones like Flan-T5 XXL Chung et al. ([2022](#bib.bib6)), as
    well as multiple passage rerankers. Our approach improves retrieval accuracy in
    zero-shot settings for LoTTE Santhanam et al. ([2022b](#bib.bib42)), SQuAD, and
    NQ.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了 UDAPDR，一种新颖的无监督领域适应方法，针对神经 IR，战略性地利用了像 GPT-3 (Brown 等人，[2020](#bib.bib4))
    这样的昂贵 LLM 和像 Flan-T5 XXL Chung 等人 ([2022](#bib.bib6)) 这样的较便宜的 LLM，以及多个段落重新排名器。我们的方法在
    LoTTE Santhanam 等人 ([2022b](#bib.bib42))、SQuAD 和 NQ 的零样本设置中提高了检索准确性。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We preserve the accuracy gains of these rerankers while maintaining the competitive
    latency of ColBERTv2\. This leads to substantial reductions in query latency.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在保持 ColBERTv2 的竞争性延迟的同时，保留了这些重新排名器的准确性提升。这导致了查询延迟的显著减少。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Unlike a number of previous domain adaptation approaches that utilize millions
    of synthetic queries, our technique only requires 1000s of synthetic queries to
    prove effective and is compatible with various LLMs designed for handling instruction-based
    tasks like creating synthetic queries (e.g., GPT-3, T5, Flan-T5).
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与一些先前利用数百万合成查询的领域适应方法不同，我们的方法只需 1000 个合成查询就能证明有效，并且与设计用于处理基于指令任务的各种 LLM（例如 GPT-3、T5、Flan-T5）兼容。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We generate synthetic queries using multiple prompting strategies that leverage
    GPT-3 and Flan-T5 XXL. This bolsters the effectiveness of our unsupervised domain
    adaptation approach. The broader set of synthetic queries allows us to fine-tune
    multiple passage rerankers and distill them more effectively.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用多种提示策略生成合成查询，利用 GPT-3 和 Flan-T5 XXL。这增强了我们无监督领域适应方法的有效性。更广泛的合成查询集使我们能够更有效地微调多个段落重新排名器并进行蒸馏。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Data Augmentation for Neural IR
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 神经 IR 的数据增强
- en: LLMs have been used to generate synthetic datasets He et al. ([2022](#bib.bib15));
    Yang et al. ([2020](#bib.bib52)); Anaby-Tavor et al. ([2020](#bib.bib1)); Kumar
    et al. ([2020](#bib.bib27)), which have been shown to support effective domain
    adaptation in Transformer-based architectures (Vaswani et al., [2017](#bib.bib46))
    across various tasks. LLMs have also been used to improve IR accuracy in new domains
    via the creation of synthetic datasets for retriever fine-tuning Bonifacio et al.
    ([2022](#bib.bib3)); Meng et al. ([2022](#bib.bib33)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 已被用于生成合成数据集 He 等人 ([2022](#bib.bib15))；Yang 等人 ([2020](#bib.bib52))；Anaby-Tavor
    等人 ([2020](#bib.bib1))；Kumar 等人 ([2020](#bib.bib27))，这些数据集已被证明在基于 Transformer
    的架构中（Vaswani 等人，[2017](#bib.bib46)）支持有效的领域适应。LLMs 也被用于通过创建合成数据集来提高新领域的 IR 准确性，用于检索器微调
    Bonifacio 等人 ([2022](#bib.bib3))；Meng 等人 ([2022](#bib.bib33))。
- en: Domain shift is the most pressing challenge for domain transfer. Dua et al.
    ([2022](#bib.bib10)) categorize different types of domain shifts, such as changes
    in query or document distributions, and provide intervention strategies for addressing
    each type of shift using synthetic data and indexing strategies.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 领域转移的最紧迫挑战是领域偏移。Dua 等人 ([2022](#bib.bib10)) 分类了不同类型的领域偏移，例如查询或文档分布的变化，并提供了使用合成数据和索引策略来解决每种偏移类型的干预策略。
- en: Query generation can help retrieval models trained on general domain tasks adapt
    to more targeted domains through the use of generated query–passage pairs Ma et al.
    ([2020](#bib.bib32)); Nogueira et al. ([2019](#bib.bib36)). Wang et al. ([2022](#bib.bib48))
    also use generative models to pseudo-label synthetic queries, using the generated
    data to adapt dense retrieval models to domain-specific datasets like BEIR Thakur
    et al. ([2021](#bib.bib45)). Thakur et al. ([2021](#bib.bib45)) and Dai et al.
    ([2022](#bib.bib8)) generate millions of synthetic examples for fine-tuning dense
    retrieval models, allowing for zero-shot and few-shot domain adaptation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 查询生成可以通过使用生成的查询–段落对帮助在通用领域任务上训练的检索模型适应更具针对性的领域 Ma 等人 ([2020](#bib.bib32))；Nogueira
    等人 ([2019](#bib.bib36))。Wang 等人 ([2022](#bib.bib48)) 也使用生成模型对合成查询进行伪标记，利用生成的数据将密集检索模型适应于像
    BEIR Thakur 等人 ([2021](#bib.bib45)) 这样的领域特定数据集。Thakur 等人 ([2021](#bib.bib45))
    和 Dai 等人 ([2022](#bib.bib8)) 生成了数百万个合成示例，用于微调密集检索模型，实现零样本和少样本领域适应。
- en: Synthetic queries can also be used to train passage rerankers that assist neural
    retrievers. Cross-encoders trained with synthetic queries boost retrieval accuracy
    substantially while proving more robust to domain shifts Thakur et al. ([2020](#bib.bib44),
    [2021](#bib.bib45)); Humeau et al. ([2019](#bib.bib18)). Dai et al. ([2022](#bib.bib8))
    explore training the few-shot reranker Promptagator++, leveraging an unsupervised
    domain-adaptation approach that utilizes millions of synthetically generated queries
    to train a passage reranker alongside a dense retrieval model. Additionally, Wang
    et al. ([2022](#bib.bib48)) found using zero-shot cross-encoders for reranking
    could further improve quality.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 合成查询也可以用来训练辅助神经检索器的段落重新排序器。使用合成查询训练的交叉编码器显著提高了检索准确性，同时在领域转移方面表现得更加稳健（Thakur
    等人，[2020](#bib.bib44)，[2021](#bib.bib45)）；Humeau 等人（[2019](#bib.bib18)）。Dai 等人（[2022](#bib.bib8)）探讨了训练少样本重新排序器
    Promptagator++，利用无监督领域适应方法，该方法利用数百万个合成生成的查询来训练段落重新排序器，并与密集检索模型一起使用。此外，Wang 等人（[2022](#bib.bib48)）发现使用零样本交叉编码器进行重新排序可以进一步提高质量。
- en: However, due to the high computational cost of rerankers at inference, both
    Dai et al. ([2022](#bib.bib8)) and Wang et al. ([2022](#bib.bib48)) found it unlikely
    these approaches would be deployed in user-facing settings for information retrieval.
    Our work seeks to bolster the utility of passage rerankers in information retrieval
    systems. Overall, Dai et al. ([2022](#bib.bib8)) and Wang et al. ([2022](#bib.bib48))
    demonstrated the efficacy of unsupervised domain adaptation approaches utilizing
    synthesized queries for fine-tuning dense retrievers or passage rerankers. By
    using distillation strategies, we can avoid their high computational cost while
    preserving the latent knowledge gained through unsupervised domain adaptation
    approaches.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于推断时重新排序器的计算成本较高，Dai 等人（[2022](#bib.bib8)）和 Wang 等人（[2022](#bib.bib48)）认为这些方法不太可能在用户面对的信息检索设置中部署。我们的工作旨在增强信息检索系统中段落重新排序器的实用性。总体而言，Dai
    等人（[2022](#bib.bib8)）和 Wang 等人（[2022](#bib.bib48)）展示了利用合成查询进行无监督领域适应方法的有效性，这些方法用于微调密集检索器或段落重新排序器。通过使用蒸馏策略，我们可以避免它们的高计算成本，同时保留通过无监督领域适应方法获得的潜在知识。
- en: 2.2 Pretraining Objectives for IR
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 信息检索的预训练目标
- en: Pretraining objectives can help neural IR systems adapt to new domains without
    annotations. Masked Language Modeling (MLM) Devlin et al. ([2019](#bib.bib9))
    and Inverse Cloze Task (ICT) Lee et al. ([2019](#bib.bib29)) offer unsupervised
    approaches for helping retrieval models adapt to new domains. Beyond MLM and ICT,
    Chang et al. ([2020](#bib.bib5)) proposed two unsupervised pretraining tasks,
    Body First Selection (BFS) and Wiki Link Prediction (WLP), which use sampled in-domain
    sentences and passages to warm-up a neural retriever to new domains. Additionally,
    Gysel et al. ([2018](#bib.bib13)) developed the Neural Vector Space Model (NVSM),
    an unsupervised pretraining task for news article retrieval that utilizes learned
    low-dimensional representations of documents and words. Izacard et al. ([2021](#bib.bib19))
    also explore a contrastive learning objective for unsupervised training of dense
    retrievers, improving retrieval accuracy in new domains across different languages.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练目标可以帮助神经信息检索系统在没有注释的情况下适应新领域。Devlin 等人（[2019](#bib.bib9)）的掩码语言建模（MLM）和 Lee
    等人（[2019](#bib.bib29)）的逆克洛兹任务（ICT）提供了帮助检索模型适应新领域的无监督方法。除了 MLM 和 ICT，Chang 等人（[2020](#bib.bib5)）提出了两个无监督预训练任务，Body
    First Selection（BFS）和 Wiki Link Prediction（WLP），这两个任务利用采样的领域内句子和段落来让神经检索器适应新领域。此外，Gysel
    等人（[2018](#bib.bib13)）开发了神经向量空间模型（NVSM），这是一种用于新闻文章检索的无监督预训练任务，利用了文档和词语的低维表示。Izacard
    等人（[2021](#bib.bib19)）还探讨了用于密集检索器的对比学习目标，提高了不同语言新领域的检索准确性。
- en: These pretraining objectives can be paired with additional domain adaptation
    strategies. Wang et al. ([2022](#bib.bib48)) coupled ICT with synthetic query
    data to achieve domain adaptation in dense retrieval models without the need for
    annotations. Dai et al. ([2022](#bib.bib8)) also paired the contrastive learning
    objective in Izacard et al. ([2021](#bib.bib19)) with their unsupervised Promptagator
    strategy. While our zero-shot domain adaptation approach can pair with other techniques,
    it does not require any further pretrainingfor bolstered retrieval performance;
    our approach only needs the language model pretraining of the retriever’s base
    model Devlin et al. ([2019](#bib.bib9)), and we show that it combines effectively
    with multi-vector retrievers Khattab and Zaharia ([2020](#bib.bib25)); Santhanam
    et al. ([2022b](#bib.bib42)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预训练目标可以与额外的领域适应策略配对。Wang等人([2022](#bib.bib48))将ICT与合成查询数据结合，以在无需注释的情况下实现密集检索模型的领域适应。Dai等人([2022](#bib.bib8))还将Izacard等人([2021](#bib.bib19))中的对比学习目标与他们的无监督Promptagator策略配对。虽然我们的零样本领域适应方法可以与其他技术配对，但它不需要进一步的预训练以增强检索性能；我们的方法仅需检索器基础模型的语言模型预训练Devlin等人([2019](#bib.bib9))，我们展示了它与多向量检索器Khattab和Zaharia([2020](#bib.bib25))；Santhanam等人([2022b](#bib.bib42))的有效结合。
- en: 3 Methodology
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: '![Refer to caption](img/941e2bdeec4a3f47c9b05d659c52c1e0.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/941e2bdeec4a3f47c9b05d659c52c1e0.png)'
- en: 'Figure 2: The five prompts used in Stage 1 ([Section 3](#S3 "3 Methodology
    ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of
    Rerankers")). The few-shot prompts #1 and #2 were inspired by Bonifacio et al.
    ([2022](#bib.bib3)) while the zero-shot prompts #3, #4, and #5 were inspired by
    Asai et al. ([2022](#bib.bib2)). In our experiments, we prompt GPT-3 in this stage
    to generate an initial set of queries.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '图2：阶段1中使用的五个提示([第3节](#S3 "3 方法论 ‣ UDAPDR: 通过LLM提示和重排名器的蒸馏进行无监督领域适应"))。少量示例提示#1和#2的灵感来自Bonifacio等人([2022](#bib.bib3))，而零样本提示#3、#4和#5的灵感则来自Asai等人([2022](#bib.bib2))。在我们的实验中，我们在此阶段提示GPT-3生成一组初始查询。'
- en: '![Refer to caption](img/64f43b9aa9bb2bf6b24f51260e55c645.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/64f43b9aa9bb2bf6b24f51260e55c645.png)'
- en: 'Figure 3: The prompt template used in Stage 2\. ([Section 3](#S3 "3 Methodology
    ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of
    Rerankers")). In our experiments, we create $Y$.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '图3：在阶段2中使用的提示模板。([第3节](#S3 "3 方法论 ‣ UDAPDR: 通过LLM提示和重排名器的蒸馏进行无监督领域适应")). 在我们的实验中，我们创建了$Y$。'
- en: '[Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ UDAPDR: Unsupervised Domain
    Adaptation via LLM Prompting and Distillation of Rerankers") outlines each stage
    of the UDAPDR strategy. For the target domain $T$). However, it does not require
    any in-domain queries or labels. The overall goal is to leverage our store of
    in-domain passages and LLM prompting to inexpensively generate large numbers of
    synthetic queries for passages. These synthetic queries are used to train domain-specific
    reranking models that serve as teachers for a single retriever. The specific stages
    of this process are as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1](#S1.F1 "图1 ‣ 1 引言 ‣ UDAPDR: 通过LLM提示和重排名器的蒸馏进行无监督领域适应")概述了UDAPDR策略的每个阶段。对于目标领域$T$，它不需要任何领域内查询或标签。总体目标是利用我们存储的领域内段落和LLM提示，以低成本生成大量的合成查询。这些合成查询用于训练领域特定的重排名模型，这些模型作为单一检索器的教师。该过程的具体阶段如下：'
- en: 'Stage 1:'
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 阶段1：
- en: We begin with a set of prompts that embed examples of passages paired with good
    and bad queries and that seek to have the model generate a novel good query for
    a new passage. We sample $X$ in-domain passages from the target domain.)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一组提示开始，这些提示嵌入了与好坏查询配对的段落示例，并试图让模型为新段落生成一个新的好查询。我们从目标领域中抽取$X$个领域内段落。
- en: 'In this stage, we use GPT-3 Brown et al. ([2020](#bib.bib4)), specifically
    the text-davinci-002 model. The guiding idea is to use a very effective LLM for
    the first stage, to seed the process with very high quality queries. We employ
    the five prompting strategies in [Figure 2](#S3.F2 "Figure 2 ‣ 3 Methodology ‣
    UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers").
    Two of our prompts are from Bonifacio et al. [2022](#bib.bib3), where they proved
    successful for generating synthetic queries in a few-shot setting. The remaining
    three use a zero-shot strategy and were inspired by prompts in Asai et al. [2022](#bib.bib2).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，我们使用 GPT-3 Brown 等人（[2020](#bib.bib4)），特别是 text-davinci-002 模型。指导思想是使用非常有效的
    LLM 作为第一阶段，以非常高质量的查询来启动过程。我们采用了 [图 2](#S3.F2 "图 2 ‣ 3 方法论 ‣ UDAPDR：通过LLM提示和重排序器蒸馏进行的无监督领域适应")
    中的五种提示策略。我们的两个提示来自 Bonifacio 等人 [2022](#bib.bib3)，他们证明了在少量示例设置中成功生成合成查询。其余三个使用零-shot
    策略，并受到 Asai 等人 [2022](#bib.bib2) 中提示的启发。
- en: 'Stage 2:'
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 阶段 2：
- en: The queries generated in Stage 1 form the basis for prompts in which passages
    from the target domain $T$, specifically, 1, 5, and 10\. This programmatic creation
    of few-shot demonstrations for language models is inspired by the Demonstrate
    stage of the DSP framework Khattab et al. ([2022](#bib.bib24)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在阶段 1 生成的查询构成了目标领域 $T$ 的提示的基础，特别是 1、5 和 10。为语言模型创建少量演示的这种程序化方法受到 DSP 框架 Khattab
    等人（[2022](#bib.bib24)）的演示阶段的启发。
- en: 'Stage 3:'
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 阶段 3：
- en: 'Each of the corpus-adapted prompts from Stage 2 is used to generate a unique
    set of $Z$ = 10K and 100K in [Section 4.3](#S4.SS3 "4.3 Multi-Reranker Domain
    Adaptation ‣ 4 Experiments ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting
    and Distillation of Rerankers").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从阶段 2 生成的每个适应语料库的提示都用于生成一组独特的 $Z$ = 10K 和 100K，在[第4.3节](#S4.SS3 "4.3 多重重排序器领域适应
    ‣ 4 实验 ‣ UDAPDR：通过LLM提示和重排序器蒸馏进行的无监督领域适应")中。
- en: We use multiple corpus-adapted prompts to mitigate edge cases in which we create
    a low-quality prompt based on the chosen synthetic queries and in-domain passages
    from the target domain $T$. (See Stage 4 below for a description of how low-quality
    prompts can optionally be detected and removed.)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用多个适应语料库的提示来缓解在基于所选合成查询和目标领域 $T$ 的领域内段落创建低质量提示的边缘情况。（有关如何可选地检测和删除低质量提示的描述，请参见下文阶段
    4。）
- en: As a quality filter for selecting synthetic queries, we test whether a synthetic
    query can return its gold passage within the top 20 retrieved results using a
    zero-shot ColBERTv2 retriever. We only use synthetic queries that pass this filter,
    which has been shown to improve domain adaptation in neural IR (Dai et al., [2022](#bib.bib8);
    Jeronymo et al., [2023](#bib.bib21)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作为选择合成查询的质量筛选器，我们测试一个合成查询是否可以在前 20 个检索结果中返回其金段落，使用零-shot ColBERTv2 检索器。我们只使用通过此筛选器的合成查询，已证明这可以改善神经
    IR 中的领域适应（Dai 等人，[2022](#bib.bib8)；Jeronymo 等人，[2023](#bib.bib21)）。
- en: 'Stage 4:'
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 阶段 4：
- en: 'With each set of synthetic queries generated using the $Y$ best rerankers based
    on their accuracy on the validation set of the target domain. For our main experiments,
    we use all of these rerankers. This is the most general case, in which we do not
    assume that a validation set exists for the target domain. (In [Appendix A](#A1
    "Appendix A Reranker Configurations ‣ UDAPDR: Unsupervised Domain Adaptation via
    LLM Prompting and Distillation of Rerankers"), we evaluate settings where a subset
    of them is used.)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每组使用基于目标领域验证集上准确度的 $Y$ 最佳重排序器生成的合成查询，我们使用所有这些重排序器进行主要实验。这是最通用的情况，其中我们不假设目标领域存在验证集。（在[附录
    A](#A1 "附录 A 重排序器配置 ‣ UDAPDR：通过LLM提示和重排序器蒸馏进行的无监督领域适应")中，我们评估了使用其中一个子集的设置。）
- en: 'Stage 5:'
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 阶段 5：
- en: The domain-specific passage rerankers from Stage 4 serve as multi-teachers creating
    a single ColBERTv2 retriever in a multi-teacher distillation process. For distillation,
    we use annotated triples that are created by using the trained domain-specific
    reranker to label additional synthetic questions. Overall, our distillation process
    allows us to improve the retrieval accuracy of ColBERTv2 without needing to use
    the rerankers at inference.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段 4 中的领域特定段落重排序器作为多教师，创建一个单一的 ColBERTv2 检索器，在多教师蒸馏过程中进行训练。对于蒸馏，我们使用通过训练的领域特定重排序器标注的三元组来标记额外的合成问题。总体而言，我们的蒸馏过程使我们在不需要在推理时使用重排序器的情况下提高了
    ColBERTv2 的检索准确性。
- en: 'Stage 6:'
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 阶段 6：
- en: We test our domain-adapted ColBERTv2 retriever on the evaluation set for the
    target domain $T$. This corresponds to deployment of the retriever within the
    target domain.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在目标领域 $T$ 的评估集上测试了我们领域适应的 ColBERTv2 检索器。这对应于在目标领域内部署检索器。
- en: 4 Experiments
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Models
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 模型
- en: We leverage the Demonstrate-Search-Predict (DSP) Khattab et al. ([2022](#bib.bib24))
    codebase for running our experiments. The DSP architecture allows us to build
    a modular system with both retrieval models and LLMs. For our passage rerankers,
    we selected DeBERTaV3-Large He et al. ([2021](#bib.bib14)) as the cross-encoder
    after performing comparison experiments with RoBERTa-Large Liu et al. ([2019](#bib.bib31)),
    BERT Devlin et al. ([2019](#bib.bib9)), and MiniLMv2 Wang et al. ([2021](#bib.bib50)).
    For our IR system, we use the ColBERTv2 retriever since it remains competitive
    for both accuracy and query latency across various domains and platforms Santhanam
    et al. ([2022c](#bib.bib43)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用 Demonstrate-Search-Predict (DSP) Khattab 等（[2022](#bib.bib24)）的代码库来运行我们的实验。DSP
    架构允许我们构建一个包含检索模型和 LLM 的模块化系统。对于我们的段落重排名器，我们在与 RoBERTa-Large Liu 等（[2019](#bib.bib31)）、BERT
    Devlin 等（[2019](#bib.bib9)）和 MiniLMv2 Wang 等（[2021](#bib.bib50)）进行比较实验后，选择了 DeBERTaV3-Large
    He 等（[2021](#bib.bib14)）作为跨编码器。对于我们的 IR 系统，我们使用 ColBERTv2 检索器，因为它在各种领域和平台上在准确性和查询延迟方面仍然具有竞争力
    Santhanam 等（[2022c](#bib.bib43)）。
- en: 4.2 Datasets
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 数据集
- en: For our experiments, we use LoTTE Santhanam et al. ([2022b](#bib.bib42)), BEIR
    Thakur et al. ([2021](#bib.bib45)), NQ Kwiatkowski et al. ([2019](#bib.bib28)),
    and SQuAD Rajpurkar et al. ([2016](#bib.bib40)). This allows us to cover both
    long-tail IR (LoTTE, BEIR) and general-knowledge question answering (NQ, SQuAD).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们使用了 LoTTE Santhanam 等（[2022b](#bib.bib42)）、BEIR Thakur 等（[2021](#bib.bib45)）、NQ
    Kwiatkowski 等（[2019](#bib.bib28)）和 SQuAD Rajpurkar 等（[2016](#bib.bib40)）。这使我们能够覆盖长尾信息检索（LoTTE、BEIR）和常识问答（NQ、SQuAD）。
- en: We note that NQ and SQuAD were both part of Flan-T5’s pretraining datasets Chung
    et al. ([2022](#bib.bib6)). Wikipedia passages used in NQ and SQuAD were also
    part of DeBERTaV3 and GPT-3’s pretraining datasets He et al. ([2021](#bib.bib14));
    Brown et al. ([2020](#bib.bib4)). Similarly, the raw StackExchange answers and
    questions (i.e., from which LoTTE-Forum is derived) may overlap in part with the
    training data of GPT-3. The overlap between pretraining and evaluation datasets
    may impact the efficacy of domain adaptation on NQ and SQuAD, leading to increased
    accuracy not caused by our approach.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到 NQ 和 SQuAD 都是 Flan-T5 的预训练数据集的一部分 Chung 等（[2022](#bib.bib6)）。NQ 和 SQuAD
    中使用的维基百科文章也属于 DeBERTaV3 和 GPT-3 的预训练数据集 He 等（[2021](#bib.bib14)）；Brown 等（[2020](#bib.bib4)）。类似地，原始的
    StackExchange 答案和问题（即 LoTTE-Forum 的来源）可能与 GPT-3 的训练数据部分重叠。预训练和评估数据集之间的重叠可能会影响领域适应在
    NQ 和 SQuAD 上的效果，从而导致准确度的提高并非由我们的方法引起。
- en: 4.3 Multi-Reranker Domain Adaptation
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 多重重排名器领域适应
- en: '|  |  | ColBERTv2 Distillation with UDAPDR |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ColBERTv2 蒸馏与 UDAPDR |  |'
- en: '|  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '&#124; Zero-shot &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 零样本 &#124;'
- en: '&#124; ColBERTv2 &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ColBERTv2 &#124;'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $Y$ = 1 reranker, &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $Y$ = 1 个重排名器，&#124;'
- en: '&#124; $Z$ = 100k queries &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $Z$ = 100k 查询 &#124;'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $Y$ = 5 rerankers, &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $Y$ = 5 个重排名器，&#124;'
- en: '&#124; $Z$ = 20k queries &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $Z$ = 20k 查询 &#124;'
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $Y$ = 10 rerankers, &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $Y$ = 10 个重排名器，&#124;'
- en: '&#124; $Z$ = 10k queries &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $Z$ = 10k 查询 &#124;'
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Zero-shot &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 零样本 &#124;'
- en: '&#124; ColBERTv2 &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ColBERTv2 &#124;'
- en: '&#124; + Reranker &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + 重排名器 &#124;'
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| LoTTE Lifestyle | 64.5 | 73.0 | 74.8 | 74.4 | 73.5 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE Lifestyle | 64.5 | 73.0 | 74.8 | 74.4 | 73.5 |'
- en: '| LoTTE Techology | 44.5 | 50.2 | 51.3 | 51.1 | 50.6 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE Techology | 44.5 | 50.2 | 51.3 | 51.1 | 50.6 |'
- en: '| LoTTE Writing | 80.0 | 84.3 | 85.7 | 86.2 | 85.5 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE Writing | 80.0 | 84.3 | 85.7 | 86.2 | 85.5 |'
- en: '| LoTTE Recreation | 70.8 | 76.9 | 80.4 | 79.8 | 79.1 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE Recreation | 70.8 | 76.9 | 80.4 | 79.8 | 79.1 |'
- en: '| LoTTE Science | 61.5 | 65.6 | 67.9 | 68.0 | 67.2 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE Science | 61.5 | 65.6 | 67.9 | 68.0 | 67.2 |'
- en: '| \hdashline[1pt/1pt] LoTTE Pooled | 63.7 | 70.0 | 72.1 | 72.2 | 71.1 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[1pt/1pt] LoTTE Pooled | 63.7 | 70.0 | 72.1 | 72.2 | 71.1 |'
- en: '| NaturalQuestions | 68.9 | 72.4 | 73.7 | 74.0 | 73.9 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| NaturalQuestions | 68.9 | 72.4 | 73.7 | 74.0 | 73.9 |'
- en: '| SQuAD | 65.0 | 71.8 | 73.8 | 73.6 | 72.6 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| SQuAD | 65.0 | 71.8 | 73.8 | 73.6 | 72.6 |'
- en: 'Table 1: Success@5 for Multi-Reranker Domain Adaptation Strategies with Different
    Synthetic Query Counts. LoTTE results are for the Forum configuration. All results
    are for dev sets. The reranker used is DeBERTa-v3-Large. The ColBERTv2 distillation
    strategies train $Y$ synthetic queries before distilling each reranker with the
    same ColBERTv2 model. No selection process for rerankers is needed nor access
    to annotated in-domain dev sets (cf. [Table 7](#A1.T7 "Table 7 ‣ Appendix A Reranker
    Configurations ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and
    Distillation of Rerankers") in our Appendices). The non-distilled reranker in
    the final column is trained on 100K synthetic queries created using Flan-T5 XXL
    model and the prompting strategy outlined in [Section 3](#S3 "3 Methodology ‣
    UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers").'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1：具有不同合成查询数量的多重重排器领域适应策略的 Success@5。LoTTE 结果适用于 Forum 配置。所有结果均为开发集数据。使用的重排器是
    DeBERTa-v3-Large。ColBERTv2 蒸馏策略在使用相同的 ColBERTv2 模型对每个重排器进行蒸馏之前，训练 $Y$ 个合成查询。无需重排器选择过程，也无需访问注释的领域开发集（参见
    [表 7](#A1.T7 "Table 7 ‣ Appendix A Reranker Configurations ‣ UDAPDR: Unsupervised
    Domain Adaptation via LLM Prompting and Distillation of Rerankers") 在附录中）。最终列中的非蒸馏重排器是在使用
    Flan-T5 XXL 模型和 [第 3 节](#S3 "3 Methodology ‣ UDAPDR: Unsupervised Domain Adaptation
    via LLM Prompting and Distillation of Rerankers") 中概述的提示策略创建的 100K 合成查询上进行训练的。'
- en: '| Retriever and Reranker |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 检索器和重排器 |'
- en: '&#124; Passages &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 段落 &#124;'
- en: '&#124; Reranked &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重排 &#124;'
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Query &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 查询 &#124;'
- en: '&#124; Latency &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 延迟 &#124;'
- en: '| Success@5 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Success@5 |'
- en: '| Zero-shot ColBERTv2 | N/A | 35 ms | 64.5 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 零样本 ColBERTv2 | 不适用 | 35 毫秒 | 64.5 |'
- en: '| ColBERTv2 Distillation: $Y$ = 20k queries | N/A | 35 ms | 74.8 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| ColBERTv2 蒸馏：$Y$ = 20k 查询 | 不适用 | 35 毫秒 | 74.8 |'
- en: '| Zero-shot ColBERTv2 + Reranker | 20 | 412 ms | 73.3 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 零样本 ColBERTv2 + 重排器 | 20 | 412 毫秒 | 73.3 |'
- en: '| Zero-shot ColBERTv2 + Reranker | 100 | 2060 ms | 73.5 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 零样本 ColBERTv2 + 重排器 | 100 | 2060 毫秒 | 73.5 |'
- en: '| Zero-shot ColBERTv2 + Reranker | 1000 | 20600 ms | 73.5 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 零样本 ColBERTv2 + 重排器 | 1000 | 20600 毫秒 | 73.5 |'
- en: 'Table 2: Average Single Query Latency for Retrieval + Reranker Systems. Latencies
    and Success@5 are for LoTTE Lifestyle. The ColBERTv2 distillation strategies train
    $Y$ synthetic queries before distilling each reranker with the same ColBERTv2
    model. These experiments were performed on a single NVIDIA V100 GPU with PyTorch,
    version 1.13 Paszke et al. ([2019](#bib.bib37)). Query latencies rounded to three
    significant digits.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：检索 + 重排系统的平均单查询延迟。延迟和 Success@5 数据适用于 LoTTE 生活方式。ColBERTv2 蒸馏策略在使用相同的 ColBERTv2
    模型对每个重排器进行蒸馏之前，训练 $Y$ 个合成查询。这些实验在一台单 NVIDIA V100 GPU 上使用 PyTorch 1.13 Paszke 等人（[2019](#bib.bib37)）进行。查询延迟四舍五入到三位有效数字。
- en: '[Table 1](#S4.T1 "Table 1 ‣ 4.3 Multi-Reranker Domain Adaptation ‣ 4 Experiments
    ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of
    Rerankers") provides our main results for UDAPDR accuracy. For these experiments,
    we set a total budget of 100K synthetic queries and distribute these equally across
    the chosen number of rerankers to be used as teachers in the distillation process.
    When exploring UDAPDR system designs, we report dev results, and we report core
    test results for LoTTE and BEIR in [Section 4.7](#S4.SS7 "4.7 LoTTE and BEIR Test
    Results ‣ 4 Experiments ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting
    and Distillation of Rerankers").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 1](#S4.T1 "Table 1 ‣ 4.3 Multi-Reranker Domain Adaptation ‣ 4 Experiments
    ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of
    Rerankers") 提供了我们关于 UDAPDR 准确性的主要结果。在这些实验中，我们设定了总预算为 100K 合成查询，并将这些查询均匀分配给选择的重排器数量，以用作蒸馏过程中的教师。在探索
    UDAPDR 系统设计时，我们报告了开发结果，并在 [第 4.7 节](#S4.SS7 "4.7 LoTTE and BEIR Test Results ‣
    4 Experiments ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation
    of Rerankers") 中报告了 LoTTE 和 BEIR 的核心测试结果。'
- en: 'We compare UDAPDR to two baselines. In the leftmost column of [Table 1](#S4.T1
    "Table 1 ‣ 4.3 Multi-Reranker Domain Adaptation ‣ 4 Experiments ‣ UDAPDR: Unsupervised
    Domain Adaptation via LLM Prompting and Distillation of Rerankers"), we have a
    Zero-shot ColBERTv2 retriever (no distillation). This model has been shown to
    be extremely effective in our benchmark domains, and it is very low latency, so
    it serves as an ambitious baseline. In the rightmost column, we have a Zero-shot
    ColBERTv2 retriever paired with a single non-distilled passage reranker, trained
    on 100K synthetic queries. We expect this model to be extremely competitive in
    terms of accuracy, but also very high latency.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将 UDAPDR 与两个基准进行比较。在 [Table 1](#S4.T1 "Table 1 ‣ 4.3 Multi-Reranker Domain
    Adaptation ‣ 4 Experiments ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting
    and Distillation of Rerankers") 的最左侧列中，我们有一个 Zero-shot ColBERTv2 检索器（无蒸馏）。该模型已被证明在我们的基准领域中极为有效，且延迟极低，因此它作为一个雄心勃勃的基准。在最右侧列中，我们有一个
    Zero-shot ColBERTv2 检索器配备了一个单独的非蒸馏段落重新排序器，训练于 100K 合成查询。我们预计该模型在准确性方面将非常具有竞争力，但延迟也非常高。'
- en: 'All versions of UDAPDR are far superior to Zero-shot ColBERTv2 across all the
    domains we evaluated. In addition, two settings of our model are competitive with
    or superior to Zero-shot ColBERTv2 plus a Reranker: distilling into ColBERTv2
    the scores from 5 rerankers, each trained on 20k synthetic queries, as well as
    10 rerankers, each trained on 10k synthetic queries.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 所有版本的 UDAPDR 在我们评估的所有领域中都远远优于 Zero-shot ColBERTv2。此外，我们模型的两种设置与 Zero-shot ColBERTv2
    加 Reranker 的表现相当或更佳：将来自 5 个重新排序器的分数蒸馏到 ColBERTv2 中，每个重新排序器都在 20k 合成查询上训练，及 10
    个重新排序器，每个在 10k 合成查询上训练。
- en: 4.4 Query Latency
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 查询延迟
- en: 'The accuracy results in [Table 1](#S4.T1 "Table 1 ‣ 4.3 Multi-Reranker Domain
    Adaptation ‣ 4 Experiments ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting
    and Distillation of Rerankers") show that UDAPDR is highly effective. In addition
    to this, [Table 2](#S4.T2 "Table 2 ‣ 4.3 Multi-Reranker Domain Adaptation ‣ 4
    Experiments ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation
    of Rerankers") reports a set of latency evaluations using the LoTTe Lifestyle
    section. Our latency costs refer to the complete retrieval process for a single
    query, from query encoding to the last stage of passage retrieval.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[Table 1](#S4.T1 "Table 1 ‣ 4.3 Multi-Reranker Domain Adaptation ‣ 4 Experiments
    ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of
    Rerankers") 中的准确性结果表明 UDAPDR 非常有效。此外，[Table 2](#S4.T2 "Table 2 ‣ 4.3 Multi-Reranker
    Domain Adaptation ‣ 4 Experiments ‣ UDAPDR: Unsupervised Domain Adaptation via
    LLM Prompting and Distillation of Rerankers") 报告了一组使用 LoTTe Lifestyle 部分的延迟评估。我们的延迟成本指的是单个查询的完整检索过程，从查询编码到最后阶段的段落检索。'
- en: 'Zero-shot ColBERTv2 is known to have low retrieval latency (Santhanam et al.,
    [2022a](#bib.bib41)). However, its accuracy (repeated from [Table 1](#S4.T1 "Table
    1 ‣ 4.3 Multi-Reranker Domain Adaptation ‣ 4 Experiments ‣ UDAPDR: Unsupervised
    Domain Adaptation via LLM Prompting and Distillation of Rerankers")), which is
    at a state-of-the-art level Santhanam et al. ([2022c](#bib.bib43)), trails by
    large margins the methods we propose in this work. UDAPDR (line 2) has similar
    latency but also achieves the best accuracy results. The Zero-shot ColBERTv2 +
    Reranker models come close, but only with significantly higher latency.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zero-shot ColBERTv2 被认为具有较低的检索延迟 (Santhanam et al., [2022a](#bib.bib41))。然而，其准确性（重复自
    [Table 1](#S4.T1 "Table 1 ‣ 4.3 Multi-Reranker Domain Adaptation ‣ 4 Experiments
    ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of
    Rerankers")），虽然处于最先进水平，Santhanam et al. ([2022c](#bib.bib43))，但仍远逊于我们在本研究中提出的方法。UDAPDR
    (第2行) 拥有类似的延迟，但也实现了最佳的准确性结果。Zero-shot ColBERTv2 + Reranker 模型接近，但延迟显著更高。'
- en: 4.5 Impact of Pretrained Components
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 预训练组件的影响
- en: '|       Query Generators |       Passage Reranker |       Success@5 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|       查询生成器 |       段落重新排序器 |       Success@5 |'
- en: '|       GPT-3 + Flan-T5 XXL |       DeBERTav3 Large |       71.1 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|       GPT-3 + Flan-T5 XXL |       DeBERTav3 Large |       71.1 |'
- en: '|       GPT-3 + Flan-T5 XL |       DeBERTav3 Large |       66.7 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|       GPT-3 + Flan-T5 XL |       DeBERTav3 Large |       66.7 |'
- en: '|       Flan-T5 XXL |       DeBERTav3 Large |       68.0 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|       Flan-T5 XXL |       DeBERTav3 Large |       68.0 |'
- en: '|       Flan-T5 XL |       DeBERTav3 Large |       65.9 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|       Flan-T5 XL |       DeBERTav3 Large |       65.9 |'
- en: '|       GPT-3 + Flan-T5 XXL |       DeBERTav3 Base |       67.0 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|       GPT-3 + Flan-T5 XXL |       DeBERTav3 Base |       67.0 |'
- en: '|       GPT-3 + Flan-T5 XL |       DeBERTav3 Base |       64.1 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|       GPT-3 + Flan-T5 XL |       DeBERTav3 Base |       64.1 |'
- en: '|       \hdashline[1pt/1pt] Zero-shot ColBERTv2 |       N/A |       63.7 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|       \hdashline[1pt/1pt] Zero-shot ColBERTv2 |       N/A |       63.7 |'
- en: 'Table 3: Model Configurations for Synthetic Query Generation and Passage Reranker.
    We describe the first and second query generators for UDAPDR in [Section 3](#S3
    "3 Methodology ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and
    Distillation of Rerankers"). The Success@5 scores are for the LoTTE Pooled dev
    task. A single non-distilled reranker is trained on 100K synthetic queries for
    each configuration. We do not explore a configuration with exclusively GPT-3 generated
    queries due to GPT-3 API costs.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：合成查询生成和段落重排序器的模型配置。我们在[第 3 节](#S3 "3 方法 ‣ UDAPDR：通过 LLM 提示和重排序器的蒸馏进行无监督领域适应")中描述了
    UDAPDR 的第一和第二查询生成器。Success@5 分数是针对 LoTTE Pooled 开发任务的。每个配置中的单个非蒸馏重排序器在 100K 合成查询上进行训练。由于
    GPT-3 API 成本，我们没有探索完全使用 GPT-3 生成查询的配置。
- en: '| Prompt Strategy | Query Generators | Reranker Count | Success@5 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 提示策略 | 查询生成器 | 重排序器数量 | Success@5 |'
- en: '| InPars Prompt | GPT-3 | 1 | 65.8 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| InPars 提示 | GPT-3 | 1 | 65.8 |'
- en: '| InPars Prompt | Flan-T5 XXL | 1 | 67.6 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| InPars 提示 | Flan-T5 XXL | 1 | 67.6 |'
- en: '| InPars Prompt | Flan-T5 XXL | 5 | 67.1 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| InPars 提示 | Flan-T5 XXL | 5 | 67.1 |'
- en: '| Corpus-Adapted Prompts | GPT-3 + Flan-T5 XXL | 1 | 67.4 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 语料库适应的提示 | GPT-3 + Flan-T5 XXL | 1 | 67.4 |'
- en: '| Corpus-Adapted Prompts | GPT-3 + Flan-T5 XXL | 5 | 71.1 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 语料库适应的提示 | GPT-3 + Flan-T5 XXL | 5 | 71.1 |'
- en: '| \hdashline[1pt/1pt] Zero-shot ColBERTv2 | N/A | N/A | 63.7 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[1pt/1pt] 零-shot ColBERTv2 | 不适用 | 不适用 | 63.7 |'
- en: 'Table 4: Model Configurations for Prompting Strategies. We specify the prompting
    strategy, query generators, and reranker counts for each configuration. The Success@5
    scores are for the LoTTE Pooled dev task. 100,000 synthetic queries total are
    used for each approach except for the top row, which uses 5,000 synthetic queries
    due to the costs of the GPT-3 API. The synthetic queries are split evenly amongst
    the total rerankers used. The rerankers are distilled with a ColBERTv2 retriever
    for configuration.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：提示策略的模型配置。我们指定了每个配置的提示策略、查询生成器和重排序器数量。Success@5 分数是针对 LoTTE Pooled 开发任务。每种方法使用总计
    100,000 个合成查询，除了最上面一行，由于 GPT-3 API 成本，使用了 5,000 个合成查询。合成查询在总重排序器中均匀分配。重排序器与 ColBERTv2
    检索器进行配置蒸馏。
- en: 'UDAPDR involves three pretrained components: GPT-3 to generate our initial
    set of synthetic queries, Flan-T5 XXL to generate our second, larger set of synthetic
    queries, and DeBERTaV3-Large for the passage rerankers. What is the impact of
    these specific components on system behavior?'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: UDAPDR 涉及三个预训练组件：GPT-3 用于生成初始的合成查询集，Flan-T5 XXL 用于生成第二个更大的合成查询集，DeBERTaV3-Large
    用于段落重排序器。这些特定组件对系统行为的影响是什么？
- en: 'To begin to address this question, we explored a range of variants. These results
    are summarized in [Table 4](#S4.T4 "Table 4 ‣ 4.5 Impact of Pretrained Components
    ‣ 4 Experiments ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and
    Distillation of Rerankers"). Our primary setting for UDAPDR performs the best,
    but it is noteworthy that very competitive performance can be obtained with no
    use of GPT-3 at all. Additionally, we tried using Flan-T5 XL instead of Flan-T5 XXL
    for the second stage of synthetic query generation, since it is more than 90%
    smaller than Flan-T5 XXL in terms of model parameters. This still leads to better
    performance than Zero-shot ColBERTv2.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始解决这个问题，我们探索了一系列变体。这些结果总结在[表 4](#S4.T4 "表 4 ‣ 4.5 预训练组件的影响 ‣ 4 实验 ‣ UDAPDR：通过
    LLM 提示和重排序器的蒸馏进行无监督领域适应")中。我们 UDAPDR 的主要设置表现最好，但值得注意的是，完全不使用 GPT-3 也能获得非常有竞争力的表现。此外，我们尝试用
    Flan-T5 XL 替代 Flan-T5 XXL 进行第二阶段的合成查询生成，因为在模型参数方面它比 Flan-T5 XXL 小超过 90%。这仍然比零-shot
    ColBERTv2 的表现更好。
- en: 'We also explored using a smaller cross-encoder for UDAPDR. We tested using
    DeBERTaV3-Base instead of DeBERTaV3-Large for our passage reranker, decreasing
    the number of model parameters by over 70%. We found that DeBERTaV3-Base was still
    effective, though it results in a 4.1 point drop in Success@5 compared to DeBERTaV3-Large
    for LoTTE Pooled ([Table 3](#S4.T3 "Table 3 ‣ 4.5 Impact of Pretrained Components
    ‣ 4 Experiments ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and
    Distillation of Rerankers")). (In our initial explorations, we also tested using
    BERT-Base or RoBERTa-Large as the cross-encoder but found them less effective
    than DeBERTaV3, leading to 6–8 point drops in Success@5.)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还探索了对 UDAPDR 使用较小的交叉编码器。我们测试了使用 DeBERTaV3-Base 替代 DeBERTaV3-Large 作为我们的段落重排名器，将模型参数数量减少了
    70% 以上。我们发现 DeBERTaV3-Base 仍然有效，但相较于 DeBERTaV3-Large，对于 LoTTE Pooled ([表 3](#S4.T3
    "Table 3 ‣ 4.5 Impact of Pretrained Components ‣ 4 Experiments ‣ UDAPDR: Unsupervised
    Domain Adaptation via LLM Prompting and Distillation of Rerankers"))，Success@5
    下降了 4.1 分。在我们的初步探索中，我们还测试了使用 BERT-Base 或 RoBERTa-Large 作为交叉编码器，但发现它们的效果不如 DeBERTaV3，导致
    Success@5 下降了 6–8 分。'
- en: 4.6 Different Prompting Strategies
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 不同的提示策略
- en: 'We tested whether a simpler few-shot prompting strategy might be better than
    our corpus-adapted prompting approach for domain adaptation. In [Table 4](#S4.T4
    "Table 4 ‣ 4.5 Impact of Pretrained Components ‣ 4 Experiments ‣ UDAPDR: Unsupervised
    Domain Adaptation via LLM Prompting and Distillation of Rerankers"), we compare
    the InPars Bonifacio et al. ([2022](#bib.bib3)) few-shot prompt to our corpus-adapted
    prompt approach for synthetic query generation and passage reranker distillation.
    We evaluate these using query generation with both Flan XXL and GPT-3. We find
    that our multi-reranker, corpus-adapted prompting strategy is more successful,
    leading to a 3.5 point increase in Success@5 after ColBERTv2 distillation while
    using the same number of synthetic queries for training.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '我们测试了一个更简单的少量提示策略是否比我们适应语料库的提示方法在领域适应上更有效。在 [表 4](#S4.T4 "Table 4 ‣ 4.5 Impact
    of Pretrained Components ‣ 4 Experiments ‣ UDAPDR: Unsupervised Domain Adaptation
    via LLM Prompting and Distillation of Rerankers") 中，我们将 InPars Bonifacio 等人 ([2022](#bib.bib3))
    的少量提示与我们适应语料库的提示方法进行了比较，用于合成查询生成和段落重排名器的精炼。我们使用 Flan XXL 和 GPT-3 对这些进行评估。我们发现我们的多重重排名器、适应语料库的提示策略更为成功，在
    ColBERTv2 精炼后 Success@5 提高了 3.5 分，同时使用了相同数量的合成查询进行训练。'
- en: 4.7 LoTTE and BEIR Test Results
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 LoTTE 和 BEIR 测试结果
- en: '|  | LoTTE Datasets |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | LoTTE 数据集 |'
- en: '|  | Forum |  | Search |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | Forum |  | Search |'
- en: '|  | Life. | Tech. | Writing | Rec. | Science | Pooled |  | Life. | Tech. |
    Writing | Rec. | Science | Pooled |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | Life. | Tech. | Writing | Rec. | Science | Pooled |  | Life. | Tech. |
    Writing | Rec. | Science | Pooled |'
- en: '| BM25 | 60.6 | 39.4 | 64.0 | 55.4 | 37.1 | 47.2 |  | 63.8 | 41.8 | 60.3 |
    56.5 | 32.7 | 48.3 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | 60.6 | 39.4 | 64.0 | 55.4 | 37.1 | 47.2 |  | 63.8 | 41.8 | 60.3 |
    56.5 | 32.7 | 48.3 |'
- en: '| SPLADEv2 | 74.0 | 50.8 | 73.0 | 67.1 | 43.7 | 60.1 |  | 82.3 | 62.4 | 77.1
    | 69.0 | 55.4 | 68.9 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| SPLADEv2 | 74.0 | 50.8 | 73.0 | 67.1 | 43.7 | 60.1 |  | 82.3 | 62.4 | 77.1
    | 69.0 | 55.4 | 68.9 |'
- en: '| RocketQAv2 | 73.7 | 47.3 | 71.5 | 65.7 | 38.0 | 57.7 |  | 82.1 | 63.4 | 78.0
    | 72.1 | 55.3 | 69.8 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| RocketQAv2 | 73.7 | 47.3 | 71.5 | 65.7 | 38.0 | 57.7 |  | 82.1 | 63.4 | 78.0
    | 72.1 | 55.3 | 69.8 |'
- en: '| Zero-shot ColBERTv2 | 76.2 | 54.0 | 75.8 | 69.8 | 45.6 | 62.3 |  | 82.4 |
    65.9 | 80.4 | 73.2 | 57.5 | 71.5 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Zero-shot ColBERTv2 | 76.2 | 54.0 | 75.8 | 69.8 | 45.6 | 62.3 |  | 82.4 |
    65.9 | 80.4 | 73.2 | 57.5 | 71.5 |'
- en: '| UDAPDR | 84.9 | 59.9 | 83.2 | 78.6 | 48.8 | 70.8 |  | 86.8 | 67.7 | 84.3
    | 77.9 | 61.0 | 76.6 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| UDAPDR | 84.9 | 59.9 | 83.2 | 78.6 | 48.8 | 70.8 |  | 86.8 | 67.7 | 84.3
    | 77.9 | 61.0 | 76.6 |'
- en: 'Table 5: Success@5 for LoTTE Test Set Results. The ColBERTv2 distillation strategies
    train $Y$ synthetic queries before distilling each reranker with the same ColBERTv2
    model. For UDAPDR, we use 5 rerankers and 20K distinct synthetic queries for training
    each of them. For BM25, SPLADEv2, and RocketQAv2, we take results directly from
    Santhanam et al. ([2022b](#bib.bib42)).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: LoTTE 测试集的 Success@5 结果。ColBERTv2 精炼策略在对每个重排名器进行精炼之前，会训练 $Y$ 个合成查询。对于
    UDAPDR，我们使用 5 个重排名器，并使用 20K 个不同的合成查询对每个重排名器进行训练。对于 BM25、SPLADEv2 和 RocketQAv2，我们直接参考
    Santhanam 等人 ([2022b](#bib.bib42)) 的结果。'
- en: '|  | BEIR Datasets |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | BEIR 数据集 |'
- en: '|  | ArguAna | Touché | Covid | NFcorpus | HotpotQA | DBPedia | Climate-FEVER
    | FEVER | SciFact | SCIDOCS | FiQA |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | ArguAna | Touché | Covid | NFcorpus | HotpotQA | DBPedia | Climate-FEVER
    | FEVER | SciFact | SCIDOCS | FiQA |'
- en: '| BM25 | 31.5 | 36.7 | 65.6 | 32.5 | 60.3 | 31.3 | 21.3 | 75.3 | 66.5 | 15.8
    | 23.6 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | 31.5 | 36.7 | 65.6 | 32.5 | 60.3 | 31.3 | 21.3 | 75.3 | 66.5 | 15.8
    | 23.6 |'
- en: '| DPR (MS MARCO) | 41.4 | - | 56.1 | 20.8 | 37.1 | 28.1 | 17.6 | 58.9 | 47.8
    | 10.8 | 27.5 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| DPR (MS MARCO) | 41.4 | - | 56.1 | 20.8 | 37.1 | 28.1 | 17.6 | 58.9 | 47.8
    | 10.8 | 27.5 |'
- en: '| ANCE | 41.5 | - | 65.4 | 23.7 | 45.6 | 28.1 | 19.8 | 66.9 | 50.7 | 12.2 |
    29.5 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| ANCE | 41.5 | - | 65.4 | 23.7 | 45.6 | 28.1 | 19.8 | 66.9 | 50.7 | 12.2 |
    29.5 |'
- en: '| ColBERT (v1) | 23.3 | - | 67.7 | 30.5 | 59.3 | 39.2 | 18.4 | 77.1 | 67.1
    | 14.5 | 31.7 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| ColBERT (v1) | 23.3 | - | 67.7 | 30.5 | 59.3 | 39.2 | 18.4 | 77.1 | 67.1
    | 14.5 | 31.7 |'
- en: '| \hdashline[1pt/1pt] TAS-B | 42.7 | - | 48.1 | 31.9 | 58.4 | 38.4 | 22.8 |
    70.0 | 64.3 | 14.9 | 30.0 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[1pt/1pt] TAS-B | 42.7 | - | 48.1 | 31.9 | 58.4 | 38.4 | 22.8 |
    70.0 | 64.3 | 14.9 | 30.0 |'
- en: '| RocketQAv2 | 45.1 | 24.7 | 67.5 | 29.3 | 53.3 | 35.6 | 18.0 | 67.6 | 56.8
    | 13.1 | 30.2 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| RocketQAv2 | 45.1 | 24.7 | 67.5 | 29.3 | 53.3 | 35.6 | 18.0 | 67.6 | 56.8
    | 13.1 | 30.2 |'
- en: '| SPLADEv2 | 47.9 | 27.2 | 71.0 | 33.4 | 68.4 | 43.5 | 23.5 | 78.6 | 69.3 |
    15.8 | 33.6 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| SPLADEv2 | 47.9 | 27.2 | 71.0 | 33.4 | 68.4 | 43.5 | 23.5 | 78.6 | 69.3 |
    15.8 | 33.6 |'
- en: '| BM25 Reranking w/ Cohere${}_{\texttt{large}}$ | 46.7 | 27.6 | 80.1 | 34.7
    | 58.0 | 37.2 | 25.9 | 67.4 | 72.1 | 19.4 | 41.1 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| BM25 重排 w/ Cohere${}_{\texttt{large}}$ | 46.7 | 27.6 | 80.1 | 34.7 | 58.0
    | 37.2 | 25.9 | 67.4 | 72.1 | 19.4 | 41.1 |'
- en: '| BM25 Reranking w/ OpenAI${}_{\texttt{ada2}}$ | 56.7 | 28.0 | 81.3 | 35.8
    | 65.4 | 40.2 | 23.7 | 77.3 | 73.6 | 18.6 | 41.1 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| BM25 重排 w/ OpenAI${}_{\texttt{ada2}}$ | 56.7 | 28.0 | 81.3 | 35.8 | 65.4
    | 40.2 | 23.7 | 77.3 | 73.6 | 18.6 | 41.1 |'
- en: '| Zero-shot ColBERTv2 | 46.1 | 26.3 | 84.7 | 33.8 | 70.3 | 44.6 | 27.1 | 78.0
    | 66.0 | 15.4 | 45.8 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 零-shot ColBERTv2 | 46.1 | 26.3 | 84.7 | 33.8 | 70.3 | 44.6 | 27.1 | 78.0
    | 66.0 | 15.4 | 45.8 |'
- en: '| \hdashline[1pt/1pt] GenQ | 49.3 | 18.2 | 61.9 | 31.9 | 53.4 | 32.8 | 17.5
    | 66.9 | 64.4 | 14.3 | 30.8 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[1pt/1pt] GenQ | 49.3 | 18.2 | 61.9 | 31.9 | 53.4 | 32.8 | 17.5
    | 66.9 | 64.4 | 14.3 | 30.8 |'
- en: '| GPL + TSDAE | 51.2 | 23.5 | 74.9 | 33.9 | 57.2 | 36.1 | 22.2 | 78.6 | 68.9
    | 16.8 | 34.4 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| GPL + TSDAE | 51.2 | 23.5 | 74.9 | 33.9 | 57.2 | 36.1 | 22.2 | 78.6 | 68.9
    | 16.8 | 34.4 |'
- en: '| UDAPDR | 57.5 | 32.4 | 88.0 | 34.1 | 75.3 | 47.4 | 33.7 | 83.2 | 72.2 | 17.8
    | 53.5 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| UDAPDR | 57.5 | 32.4 | 88.0 | 34.1 | 75.3 | 47.4 | 33.7 | 83.2 | 72.2 | 17.8
    | 53.5 |'
- en: '| \hdashline[1pt/1pt] Promptagator Few-shot | 63 | 38.1 | 76.2 | 37 | 73.6
    | 43.4 | 24.1 | 86.6 | 73.1 | 20.1 | 49.4 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[1pt/1pt] Promptagator Few-shot | 63 | 38.1 | 76.2 | 37 | 73.6
    | 43.4 | 24.1 | 86.6 | 73.1 | 20.1 | 49.4 |'
- en: 'Table 6: nDCG@10 for BEIR Test Set Results. For each dataset, the highest-accuracy
    zero-shot result is marked in bold while the highest overall is underlined. For
    UDAPDR, we use 5 rerankers and 20K distinct synthetic queries for training each
    of them. The Promptagator and GPL results are taken directly from their respective
    papers. For Promptagator, we include both the best retriever-only configuration
    (Promptagator Few-shot) and the best retriever + reranker configuration (Promptagator++
    Few-shot). We include the GPL+TSDAE pretraining strategy, which is found to improve
    retrieval accuracy Wang et al. ([2022](#bib.bib48)). We copy the results for BM25,
    GenQ, ANCE, TAS-B, and ColBERT from Thakur et al. ([2021](#bib.bib45)), for MoDIR
    and DPR-M from Xin et al. ([2022](#bib.bib51)), for SPLADEv2 from Formal et al.
    ([2021](#bib.bib11)), and for BM25 Reranking of Cohere${}_{\texttt{large}}$ from
    Kamalloo et al. ([2023](#bib.bib22)).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：BEIR 测试集结果的 nDCG@10。对于每个数据集，最高精度的零-shot 结果以**粗体**标记，而最高总体结果则使用*下划线*标记。对于
    UDAPDR，我们使用 5 个重排器和 20K 个不同的合成查询来训练它们。Promptagator 和 GPL 的结果直接取自各自的论文。对于 Promptagator，我们包括了最佳的仅检索配置（Promptagator
    Few-shot）和最佳的检索 + 重排配置（Promptagator++ Few-shot）。我们还包括了 GPL+TSDAE 预训练策略，该策略被发现可以提高检索准确性
    Wang et al. ([2022](#bib.bib48))。我们复制了 BM25、GenQ、ANCE、TAS-B 和 ColBERT 的结果，数据来源于
    Thakur et al. ([2021](#bib.bib45))，MoDIR 和 DPR-M 的结果来自 Xin et al. ([2022](#bib.bib51))，SPLADEv2
    的结果来自 Formal et al. ([2021](#bib.bib11))，以及 Cohere${}_{\texttt{large}}$ 的 BM25
    重排结果来自 Kamalloo et al. ([2023](#bib.bib22))。
- en: 'In [Table 5](#S4.T5 "Table 5 ‣ 4.7 LoTTE and BEIR Test Results ‣ 4 Experiments
    ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of
    Rerankers") and [Table 6](#S4.T6 "Table 6 ‣ 4.7 LoTTE and BEIR Test Results ‣
    4 Experiments ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation
    of Rerankers"), we include the test set results for LoTTE and BEIR, respectively.
    For LoTTE, UDAPDR increases ColBERTv2 zero-shot Success@5 for both Forum queries
    and Search queries, leading to a 7.1 point and a 3.9 point average improvement,
    respectively. For BEIR, we calculated ColBERTv2 accuracy using nDCG@10\. We found
    that UDAPDR increases zero-shot accuracy by 5.2 points on average. Promptagator++
    Few-shot offers similar improvements to zero-shot accuracy, achieving a 4.2 point
    increase compared to a zero-shot ColBERTv2 baseline. However, Promptagator++ Few-shot
    also uses a reranker during retrieval, leading to additional computational costs
    at inference time. By comparison, UDAPDR is a zero-shot method (i.e., that does
    not assume access to gold labels from the target domain) and only uses the ColBERTv2
    retriever and thus has a lower query latency at inference time.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '在[表5](#S4.T5 "Table 5 ‣ 4.7 LoTTE and BEIR Test Results ‣ 4 Experiments ‣ UDAPDR:
    Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers")和[表6](#S4.T6
    "Table 6 ‣ 4.7 LoTTE and BEIR Test Results ‣ 4 Experiments ‣ UDAPDR: Unsupervised
    Domain Adaptation via LLM Prompting and Distillation of Rerankers")中，我们分别包含了LoTTE和BEIR的测试集结果。对于LoTTE，UDAPDR提高了ColBERTv2在Forum查询和Search查询上的零样本Success@5，分别提高了7.1点和3.9点。对于BEIR，我们使用nDCG@10计算了ColBERTv2的准确率。我们发现UDAPDR平均提高了5.2点的零样本准确率。Promptagator++
    Few-shot在零样本准确率方面提供了类似的改进，与零样本ColBERTv2基线相比提高了4.2点。然而，Promptagator++ Few-shot在检索过程中也使用了重排器，因此在推理时会带来额外的计算成本。相比之下，UDAPDR是一种零样本方法（即不假设访问目标领域的金标准标签），仅使用ColBERTv2检索器，因此在推理时具有较低的查询延迟。'
- en: 4.8 Additional Results
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8 额外结果
- en: '[Table 1](#S4.T1 "Table 1 ‣ 4.3 Multi-Reranker Domain Adaptation ‣ 4 Experiments
    ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of
    Rerankers") and [Table 2](#S4.T2 "Table 2 ‣ 4.3 Multi-Reranker Domain Adaptation
    ‣ 4 Experiments ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and
    Distillation of Rerankers") explore only a limited range of potential uses for
    UDAPDR. In [Appendix A](#A1 "Appendix A Reranker Configurations ‣ UDAPDR: Unsupervised
    Domain Adaptation via LLM Prompting and Distillation of Rerankers"), we consider
    a wider range of uses. First, we ask whether it is productive to filter the set
    of rerankers based on in-domain dev set performance. We mostly find that this
    does not lead to gains over simply using all of them, and it introduces the requirement
    that we have a labeled dev set. Second, we evaluate whether substantially increasing
    the value of $Z$ can hurt performance.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1](#S4.T1 "Table 1 ‣ 4.3 Multi-Reranker Domain Adaptation ‣ 4 Experiments
    ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of
    Rerankers")和[表2](#S4.T2 "Table 2 ‣ 4.3 Multi-Reranker Domain Adaptation ‣ 4 Experiments
    ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of
    Rerankers")仅探讨了UDAPDR的有限潜在用途。在[附录A](#A1 "Appendix A Reranker Configurations ‣
    UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers")中，我们考虑了更广泛的应用范围。首先，我们询问基于领域内开发集表现来过滤重排器集合是否有效。我们发现这通常不会比简单地使用所有重排器带来收益，而且还要求我们有一个标记的开发集。其次，我们评估了显著增加$Z$值是否会损害性能。'
- en: 5 Discussion & Future Work
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论与未来工作
- en: 'Our experiments with UDAPDR suggest several directions for future work:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对UDAPDR的实验表明未来工作有几个方向：
- en: •
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While we show that our domain adaptation strategy is effective for the multi-vector
    ColBERTv2 model, whether it is also effective for other retrieval models is an
    open question for future research.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管我们展示了我们的领域适应策略对多向量ColBERTv2模型是有效的，但它是否对其他检索模型也有效仍然是未来研究的一个悬而未决的问题。
- en: •
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For our base model in ColBERTv2, we use BERT-Base. However, ColBERTv2 now allows
    for other base models, such as DeBERTaV3, ELECTRA Clark et al. ([2020](#bib.bib7)),
    and RoBERTa Liu et al. ([2019](#bib.bib31)). We would be interested to see the
    efficacy of our domain adaptation strategy with these larger encoders.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于我们在ColBERTv2中的基础模型，我们使用的是BERT-Base。然而，ColBERTv2现在允许使用其他基础模型，如DeBERTaV3、ELECTRA
    Clark等（[2020](#bib.bib7)）和RoBERTa Liu等（[2019](#bib.bib31)）。我们希望看到我们的领域适应策略在这些更大编码器上的效果。
- en: •
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We explored distillation strategies for combining passage rerankers with ColBERTv2\.
    However, testing distillation strategies for shrinking the reranker itself could
    be a viable direction for future work.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们探索了将段落重排器与ColBERTv2结合的蒸馏策略。然而，测试用于缩减重排器本身的蒸馏策略可能是未来工作的一个可行方向。
- en: •
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We draw upon several recent publications, including Bonifacio et al. ([2022](#bib.bib3))
    and Asai et al. ([2022](#bib.bib2)), to create the prompts used for GPT-3 and
    Flan-T5 XXL in our domain adaptation strategy. Creating a more systematic approach
    for generating the initial prompts would be an important item for future work.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们参考了几篇最近的出版物，包括Bonifacio等人（[2022](#bib.bib3)）和Asai等人（[2022](#bib.bib2)），以创建用于GPT-3和Flan-T5
    XXL的提示。为生成初始提示创建更系统的方法将是未来工作中的重要事项。
- en: 6 Conclusion
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We present UDAPDR, a novel strategy for adapting retrieval models to new domains.
    UDAPDR uses synthetic queries created using generative models, such as GPT-3 and
    Flan-T5 XXL, to train multiple passage rerankers on queries for target domain
    passages. These passage rerankers are then distilled into ColBERTv2 to boost retrieval
    accuracy while keeping query latency competitive as compared to other retrieval
    systems. We validate our approach across the LoTTE, BEIR, NQ, and SQuAD datasets.
    Additionally, we explore various model configurations that alter the generative
    models, prompting strategies, retriever, and passage rerankers used in our approach.
    We find that UDAPDR can boost zero-shot retrieval accuracy on new domains without
    the use of labeled training examples. We also discuss several directions for future
    work.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了UDAPDR，一种将检索模型适应新领域的创新策略。UDAPDR使用生成模型（如GPT-3和Flan-T5 XXL）创建的合成查询，训练多个针对目标领域段落的段落重排序器。这些段落重排序器随后被提炼到ColBERTv2中，以提高检索准确性，同时保持查询延迟与其他检索系统具有竞争力。我们在LoTTE、BEIR、NQ和SQuAD数据集上验证了我们的方法。此外，我们探讨了多种模型配置，这些配置改变了我们方法中使用的生成模型、提示策略、检索器和段落重排序器。我们发现UDAPDR可以在没有标签训练示例的情况下提升新领域的零样本检索准确性。我们还讨论了未来工作的几个方向。
- en: 7 Limitations
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制
- en: While our domain adaptation technique does not require questions or labels from
    the target domain, it does require a significant number of passages in the target
    domain. These passages are required for use in synthetic query generation with
    the help of LLMs like GPT-3 and Flan-T5, so future work should evaluate how effective
    these methods are on extremely small passage collections.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的领域适应技术不需要目标领域的问题或标签，但确实需要目标领域中大量的段落。这些段落需要在LLM（如GPT-3和Flan-T5）的帮助下用于合成查询生成，因此未来的工作应评估这些方法在极小段落集合上的有效性。
- en: The synthetic queries created in our approach may also inherit biases from the
    LLMs and their training data. Moreover, the exact training data of the LLMs is
    not precisely known. Our understanding is that subsets of SQuAD and NQ, in particular,
    have been used in the pretraining of Flan-T5 models as we note in the main text.
    More generally, other subtle forms of data contamination are possible as with
    all research based on LLMs that have been trained on billions of tokens from the
    Web. We have mitigated this concern by evaluating on a very large range of datasets
    and relying most heavily on open models (i.e., Flan-T5, DeBERTa, and ColBERT).
    Notably, our approach achieves consistently large gains across the vast majority
    of the many evaluation datasets tested (e.g., the individual sets within BEIR),
    reinforcing our trust in the validity and transferability of our findings.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法中创建的合成查询也可能继承LLM及其训练数据中的偏见。此外，LLM的确切训练数据并不完全清楚。我们了解到，SQuAD和NQ的子集，特别是在Flan-T5模型的预训练中被使用，正如我们在正文中提到的。更普遍地说，所有基于LLM的研究都可能存在其他细微的数据污染形式，因为这些LLM是在来自网络的数十亿个标记上训练的。我们通过在非常大范围的数据集上进行评估，并主要依赖开放模型（即Flan-T5、DeBERTa和ColBERT）来减轻这一担忧。值得注意的是，我们的方法在绝大多数测试评估数据集中（例如BEIR中的各个子集）取得了一致的大幅提升，增强了我们对研究结果有效性和可迁移性的信任。
- en: Additionally, the LLMs used in our technique benefit substantially from GPU-based
    hardware with abundant and rapid storage. These technologies are not available
    to all NLP practitioners and researchers due to their costs. Lastly, all of our
    selected information retrieval tasks are in English, a high-resource language.
    Future work can expand on the applicability of our domain adaptation techniques
    by using non-English passages in low-resource settings, helping us better understand
    the approach’s strengths and limitations.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们技术中使用的LLM在GPU硬件上获益颇丰，该硬件具备丰富且快速的存储。这些技术因其成本问题，并非所有NLP从业者和研究人员都能使用。最后，我们所有选择的信息检索任务都是英文的，这是一种高资源语言。未来的工作可以通过使用低资源环境中的非英语段落来扩展我们领域适应技术的适用性，帮助我们更好地了解这种方法的优缺点。
- en: References
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Anaby-Tavor et al. (2020) Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,
    Amir Kantor, George Kour, Segev Shlomov, Naama Tepper, and Naama Zwerdling. 2020.
    Do not have enough data? deep learning to the rescue! In *Proceedings of the AAAI
    Conference on Artificial Intelligence*, volume 34, pages 7383–7390.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anaby-Tavor 等人（2020）Ateret Anaby-Tavor、Boaz Carmeli、Esther Goldbraich、Amir Kantor、George
    Kour、Segev Shlomov、Naama Tepper 和 Naama Zwerdling。2020年。数据不足？深度学习来救援！在 *AAAI 人工智能会议论文集*，第34卷，第7383–7390页。
- en: Asai et al. (2022) Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier
    Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2022. Task-aware
    retrieval with instructions. *arXiv preprint arXiv:2211.09260*.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Asai 等人（2022）Akari Asai、Timo Schick、Patrick Lewis、Xilun Chen、Gautier Izacard、Sebastian
    Riedel、Hannaneh Hajishirzi 和 Wen-tau Yih。2022年。具有指令的任务感知检索。*arXiv 预印本 arXiv:2211.09260*。
- en: 'Bonifacio et al. (2022) Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and
    Rodrigo Nogueira. 2022. Inpars: Data augmentation for information retrieval using
    large language models. *arXiv preprint arXiv:2202.05144*.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bonifacio 等人（2022）Luiz Bonifacio、Hugo Abonizio、Marzieh Fadaee 和 Rodrigo Nogueira。2022年。Inpars：利用大型语言模型进行信息检索的数据增强。*arXiv
    预印本 arXiv:2202.05144*。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell 等。2020年。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901。
- en: Chang et al. (2020) Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang,
    and Sanjiv Kumar. 2020. [Pre-training Tasks for Embedding-based Large-scale Retrieval](https://openreview.net/forum?id=rkg-mA4FDr).
    In *International Conference on Learning Representations*.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等人（2020）Wei-Cheng Chang、Felix X. Yu、Yin-Wen Chang、Yiming Yang 和 Sanjiv
    Kumar。2020年。[基于嵌入的大规模检索预训练任务](https://openreview.net/forum?id=rkg-mA4FDr)。在 *国际学习表示会议*。
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    2022. Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等人（2022）Hyung Won Chung、Le Hou、Shayne Longpre、Barret Zoph、Yi Tay、William
    Fedus、Eric Li、Xuezhi Wang、Mostafa Dehghani、Siddhartha Brahma 等。2022年。扩展指令微调的语言模型。*arXiv
    预印本 arXiv:2210.11416*。
- en: 'Clark et al. (2020) Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D
    Manning. 2020. ELECTRA: Pre-training text encoders as discriminators rather than
    generators. *arXiv preprint arXiv:2003.10555*.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人（2020）Kevin Clark、Minh-Thang Luong、Quoc V Le 和 Christopher D Manning。2020年。ELECTRA：将文本编码器预训练为判别器而非生成器。*arXiv
    预印本 arXiv:2003.10555*。
- en: 'Dai et al. (2022) Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing
    Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator:
    Few-shot dense retrieval from 8 examples. *arXiv preprint arXiv:2209.11755*.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人（2022）Zhuyun Dai、Vincent Y Zhao、Ji Ma、Yi Luan、Jianmo Ni、Jing Lu、Anton
    Bakalov、Kelvin Guu、Keith B Hall 和 Ming-Wei Chang。2022年。Promptagator：从8个示例中进行少样本密集检索。*arXiv
    预印本 arXiv:2209.11755*。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [BERT: Pre-training of deep bidirectional transformers for language
    understanding](https://doi.org/10.18653/v1/N19-1423). In *Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等人（2019）Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。2019年。[BERT：深度双向变换器的预训练用于语言理解](https://doi.org/10.18653/v1/N19-1423)。在
    *2019年北美计算语言学协会年会：人类语言技术会议论文集，第1卷（长篇和短篇论文）*，第4171–4186页，美国明尼阿波利斯。计算语言学协会。
- en: 'Dua et al. (2022) Dheeru Dua, Emma Strubell, Sameer Singh, and Pat Verga. 2022.
    To adapt or to annotate: Challenges and interventions for domain adaptation in
    open-domain question answering. *arXiv preprint arXiv:2212.10381*.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dua 等人（2022）Dheeru Dua、Emma Strubell、Sameer Singh 和 Pat Verga。2022年。适应还是注释：开放域问答中的领域适应挑战和干预。*arXiv
    预印本 arXiv:2212.10381*。
- en: 'Formal et al. (2021) Thibault Formal, Carlos Lassance, Benjamin Piwowarski,
    and Stéphane Clinchant. 2021. Splade v2: Sparse lexical and expansion model for
    information retrieval. *arXiv preprint arXiv:2109.10086*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Formal 等人（2021）Thibault Formal、Carlos Lassance、Benjamin Piwowarski 和 Stéphane
    Clinchant。2021年。Splade v2：用于信息检索的稀疏词汇和扩展模型。*arXiv 预印本 arXiv:2109.10086*。
- en: Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei
    Chang. 2020. Retrieval augmented language model pre-training. In *International
    Conference on Machine Learning*, pages 3929–3938\. PMLR.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, 和 Mingwei
    Chang. 2020. 检索增强语言模型预训练。发表于*国际机器学习大会*，页码 3929–3938。PMLR。
- en: Gysel et al. (2018) Christophe Van Gysel, Maarten de Rijke, and Evangelos Kanoulas.
    2018. [Neural Vector Spaces for Unsupervised Information Retrieval](https://doi.org/10.1145/3196826).
    *ACM Trans. Inf. Syst.*, 36(4).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gysel et al. (2018) Christophe Van Gysel, Maarten de Rijke, 和 Evangelos Kanoulas.
    2018. [无监督信息检索的神经向量空间](https://doi.org/10.1145/3196826)。*ACM 信息系统学报*，36(4)。
- en: 'He et al. (2021) Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. DeBERTaV3:
    Improving deberta using electra-style pre-training with gradient-disentangled
    embedding sharing. *arXiv preprint arXiv:2111.09543*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2021) Pengcheng He, Jianfeng Gao, 和 Weizhu Chen. 2021. DeBERTaV3：使用
    Electra 风格预训练和梯度解耦嵌入共享改进 DeBERTa。*arXiv 预印本 arXiv:2111.09543*。
- en: 'He et al. (2022) Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari,
    and Mohammad Norouzi. 2022. Generate, Annotate, and Learn: NLP with Synthetic
    Text. *Transactions of the Association for Computational Linguistics*, 10:826–842.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2022) Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari, 和
    Mohammad Norouzi. 2022. 生成、注释与学习：合成文本的自然语言处理。*计算语言学协会会刊*，10:826–842。
- en: Hofstätter et al. (2021) Sebastian Hofstätter, Sophia Althammer, Michael Schröder,
    Mete Sertkan, and Allan Hanbury. 2021. [Improving efficient neural ranking models
    with cross-architecture knowledge distillation](http://arxiv.org/abs/2010.02666).
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hofstätter et al. (2021) Sebastian Hofstätter, Sophia Althammer, Michael Schröder,
    Mete Sertkan, 和 Allan Hanbury. 2021. [通过跨架构知识蒸馏提高高效神经排序模型](http://arxiv.org/abs/2010.02666)。
- en: 'Howard and Ruder (2018) Jeremy Howard and Sebastian Ruder. 2018. [Universal
    language model fine-tuning for text classification](https://doi.org/10.18653/v1/P18-1031).
    In *Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 328–339, Melbourne, Australia. Association
    for Computational Linguistics.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard 和 Ruder (2018) Jeremy Howard 和 Sebastian Ruder. 2018. [用于文本分类的通用语言模型微调](https://doi.org/10.18653/v1/P18-1031)。发表于*第56届计算语言学协会年会（第1卷：长篇论文）*，页码
    328–339，澳大利亚墨尔本。计算语言学协会。
- en: 'Humeau et al. (2019) Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason
    Weston. 2019. Poly-encoders: Transformer architectures and pre-training strategies
    for fast and accurate multi-sentence scoring. *arXiv preprint arXiv:1905.01969*.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Humeau et al. (2019) Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, 和 Jason
    Weston. 2019. Poly-encoders：用于快速和准确的多句评分的 Transformer 架构和预训练策略。*arXiv 预印本 arXiv:1905.01969*。
- en: Izacard et al. (2021) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. [Unsupervised
    dense information retrieval with contrastive learning](https://doi.org/10.48550/ARXIV.2112.09118).
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard et al. (2021) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, 和 Edouard Grave. 2021. [使用对比学习的无监督密集信息检索](https://doi.org/10.48550/ARXIV.2112.09118)。
- en: Izacard et al. (2022) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini,
    Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,
    and Edouard Grave. 2022. Few-shot learning with retrieval augmented language models.
    *arXiv preprint arXiv:2208.03299*.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard et al. (2022) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini,
    Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,
    和 Edouard Grave. 2022. 使用检索增强语言模型的少样本学习。*arXiv 预印本 arXiv:2208.03299*。
- en: 'Jeronymo et al. (2023) Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh
    Fadaee, Roberto Lotufo, Jakub Zavrel, and Rodrigo Nogueira. 2023. Inpars-v2: Large
    language models as efficient dataset generators for information retrieval. *arXiv
    preprint arXiv:2301.01820*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeronymo et al. (2023) Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh
    Fadaee, Roberto Lotufo, Jakub Zavrel, 和 Rodrigo Nogueira. 2023. Inpars-v2：将大型语言模型作为信息检索的高效数据集生成器。*arXiv
    预印本 arXiv:2301.01820*。
- en: Kamalloo et al. (2023) Ehsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo, Nandan
    Thakur, David Alfonso-Hermelo, Mehdi Rezagholizadeh, and Jimmy Lin. 2023. [Evaluating
    embedding apis for information retrieval](http://arxiv.org/abs/2305.06300).
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamalloo et al. (2023) Ehsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo, Nandan
    Thakur, David Alfonso-Hermelo, Mehdi Rezagholizadeh, 和 Jimmy Lin. 2023. [评估信息检索的嵌入
    API](http://arxiv.org/abs/2305.06300)。
- en: 'Khattab et al. (2021) Omar Khattab, Christopher Potts, and Matei Zaharia. 2021.
    Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval. In *Thirty-Fifth
    Conference on Neural Information Processing Systems*.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khattab 等（2021）Omar Khattab, Christopher Potts 和 Matei Zaharia。2021年。Baleen：通过浓缩检索在规模上进行稳健的多跳推理。在*第35届神经信息处理系统会议*。
- en: 'Khattab et al. (2022) Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David
    Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-Search-Predict:
    Composing retrieval and language models for knowledge-intensive nlp. *arXiv preprint
    arXiv:2212.14024*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khattab 等（2022）Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy
    Liang, Christopher Potts 和 Matei Zaharia。2022年。演示-搜索-预测：组合检索和语言模型用于知识密集型 NLP。*arXiv
    预印本 arXiv:2212.14024*。
- en: 'Khattab and Zaharia (2020) Omar Khattab and Matei Zaharia. 2020. [Colbert:
    Efficient and effective passage search via contextualized late interaction over
    BERT](https://doi.org/10.1145/3397271.3401075). In *Proceedings of the 43rd International
    ACM SIGIR conference on research and development in Information Retrieval, SIGIR
    2020, Virtual Event, China, July 25-30, 2020*, pages 39–48\. ACM.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khattab 和 Zaharia（2020）Omar Khattab 和 Matei Zaharia。2020年。[Colbert：通过 BERT 的上下文化迟交互进行高效和有效的段落检索](https://doi.org/10.1145/3397271.3401075)。在*第43届国际
    ACM SIGIR 信息检索研究与发展会议，SIGIR 2020，虚拟会议，中国，2020年7月25-30日*，第39–48页。ACM。
- en: 'Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for
    stochastic optimization. *arXiv preprint arXiv:1412.6980*.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Ba（2014）Diederik P Kingma 和 Jimmy Ba。2014年。Adam：一种随机优化方法。*arXiv 预印本
    arXiv:1412.6980*。
- en: Kumar et al. (2020) Varun Kumar, Ashutosh Choudhary, and Eunah Cho. 2020. [Data
    Augmentation using Pre-trained Transformer Models](https://aclanthology.org/2020.lifelongnlp-1.3).
    In *Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language
    Systems*, pages 18–26, Suzhou, China. Association for Computational Linguistics.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等（2020）Varun Kumar, Ashutosh Choudhary 和 Eunah Cho。2020年。[使用预训练的 Transformer
    模型进行数据增强](https://aclanthology.org/2020.lifelongnlp-1.3)。在*第二届终身学习语音语言系统研讨会论文集*，第18–26页，中国苏州。计算语言学协会。
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, et al. 2019. Natural Questions: A benchmark for question
    answering research. *Transactions of the Association for Computational Linguistics*,
    7:453–466.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwiatkowski 等（2019）Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael
    Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob
    Devlin, Kenton Lee 等。2019年。自然问题：问答研究的基准。*计算语言学协会会刊*，7:453–466。
- en: Lee et al. (2019) Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019.
    [Latent retrieval for weakly supervised open domain question answering](https://doi.org/10.18653/v1/P19-1612).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 6086–6096, Florence, Italy. Association for Computational
    Linguistics.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2019）Kenton Lee, Ming-Wei Chang 和 Kristina Toutanova。2019年。[用于弱监督开放领域问答的潜在检索](https://doi.org/10.18653/v1/P19-1612)。在*第57届计算语言学协会年会论文集*，第6086–6096页，意大利佛罗伦萨。计算语言学协会。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    NLP tasks. *Advances in Neural Information Processing Systems*, 33:9459–9474.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等（2020）Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel
    等。2020年。检索增强生成用于知识密集型 NLP 任务。*神经信息处理系统进展*，33:9459–9474。
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
    RoBERTa: A robustly optimized BERT pretraining approach. *arXiv preprint arXiv:1907.11692*.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2019）Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi
    Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer 和 Veselin Stoyanov。2019年。RoBERTa：一种稳健优化的
    BERT 预训练方法。*arXiv 预印本 arXiv:1907.11692*。
- en: Ma et al. (2020) Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald.
    2020. Zero-shot neural passage retrieval via domain-targeted synthetic question
    generation. *arXiv preprint arXiv:2004.14503*.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2020）Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall 和 Ryan McDonald。2020年。零样本神经段落检索通过领域定向的合成问题生成。*arXiv
    预印本 arXiv:2004.14503*。
- en: 'Meng et al. (2022) Rui Meng, Ye Liu, Semih Yavuz, Divyansh Agarwal, Lifu Tu,
    Ning Yu, Jianguo Zhang, Meghana Bhat, and Yingbo Zhou. 2022. Unsupervised dense
    retrieval deserves better positive pairs: Scalable augmentation with query extraction
    and generation. *arXiv preprint arXiv:2212.08841*.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng et al. (2022) Rui Meng, Ye Liu, Semih Yavuz, Divyansh Agarwal, Lifu Tu,
    Ning Yu, Jianguo Zhang, Meghana Bhat, 和 Yingbo Zhou. 2022. 无监督密集检索值得更好的正对：通过查询提取和生成进行可扩展增强。*arXiv
    预印本 arXiv:2212.08841*。
- en: 'Nguyen et al. (2016) Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh
    Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine
    reading comprehension dataset. In *CoCo@ NIPs*.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. (2016) Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh
    Tiwary, Rangan Majumder, 和 Li Deng. 2016. MS MARCO：一个人工生成的机器阅读理解数据集。发表于*CoCo@
    NIPs*。
- en: Nogueira and Cho (2019) Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage re-ranking
    with BERT. *arXiv preprint arXiv:1901.04085*.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nogueira 和 Cho (2019) Rodrigo Nogueira 和 Kyunghyun Cho. 2019. 使用BERT的段落重新排序。*arXiv
    预印本 arXiv:1901.04085*。
- en: Nogueira et al. (2019) Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019.
    From doc2query to doctttttquery. *Online preprint*, 6.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nogueira et al. (2019) Rodrigo Nogueira, Jimmy Lin, 和 AI Epistemic. 2019. 从doc2query到doctttttquery。*在线预印本*，6。
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning
    library. *Advances in neural information processing systems*, 32.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, 等。2019. Pytorch：一种命令式风格的高性能深度学习库。*神经信息处理系统进展*，32。
- en: 'Petroni et al. (2021) Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
    Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin,
    Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021.
    [KILT: a benchmark for knowledge intensive language tasks](https://doi.org/10.18653/v1/2021.naacl-main.200).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2523–2544,
    Online. Association for Computational Linguistics.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petroni et al. (2021) Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
    Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin,
    Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, 和 Sebastian Riedel. 2021.
    [KILT：一个知识密集型语言任务的基准](https://doi.org/10.18653/v1/2021.naacl-main.200)。发表于*2021年北美计算语言学协会：人类语言技术会议论文集*，第2523–2544页，在线。计算语言学协会。
- en: 'Rajpurkar et al. (2018) Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
    [Know what you don’t know: Unanswerable questions for SQuAD](https://doi.org/10.18653/v1/P18-2124).
    In *Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 2: Short Papers)*, pages 784–789, Melbourne, Australia. Association
    for Computational Linguistics.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajpurkar et al. (2018) Pranav Rajpurkar, Robin Jia, 和 Percy Liang. 2018. [了解你不知道的：SQuAD的无法回答问题](https://doi.org/10.18653/v1/P18-2124)。发表于*第56届计算语言学协会年会（第2卷：短篇论文）*，第784–789页，澳大利亚墨尔本。计算语言学协会。
- en: 'Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
    Percy Liang. 2016. [SQuAD: 100,000+ questions for machine comprehension of text](https://doi.org/10.18653/v1/D16-1264).
    In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language
    Processing*, pages 2383–2392, Austin, Texas. Association for Computational Linguistics.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, 和
    Percy Liang. 2016. [SQuAD：用于机器理解文本的100,000+个问题](https://doi.org/10.18653/v1/D16-1264)。发表于*2016年自然语言处理实证方法会议论文集*，第2383–2392页，美国德克萨斯州奥斯汀。计算语言学协会。
- en: 'Santhanam et al. (2022a) Keshav Santhanam, Omar Khattab, Christopher Potts,
    and Matei Zaharia. 2022a. PLAID: an efficient engine for late interaction retrieval.
    In *Proceedings of the 31st ACM International Conference on Information & Knowledge
    Management*, pages 1747–1756.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santhanam et al. (2022a) Keshav Santhanam, Omar Khattab, Christopher Potts,
    和 Matei Zaharia. 2022a. PLAID：一个高效的后期交互检索引擎。发表于*第31届ACM国际信息与知识管理会议论文集*，第1747–1756页。
- en: 'Santhanam et al. (2022b) Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher
    Potts, and Matei Zaharia. 2022b. [ColBERTv2: Effective and efficient retrieval
    via lightweight late interaction](https://doi.org/10.18653/v1/2022.naacl-main.272).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 3715–3734,
    Seattle, United States. Association for Computational Linguistics.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Santhanam 等人 (2022b) Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher
    Potts 和 Matei Zaharia. 2022b. [ColBERTv2: 通过轻量级晚期交互进行高效有效检索](https://doi.org/10.18653/v1/2022.naacl-main.272)。在
    *2022 年北美计算语言学协会年会：人类语言技术会议论文集*，第 3715–3734 页，西雅图，美国。计算语言学协会。'
- en: Santhanam et al. (2022c) Keshav Santhanam, Jon Saad-Falcon, Martin Franz, Omar
    Khattab, Avirup Sil, Radu Florian, Md Arafat Sultan, Salim Roukos, Matei Zaharia,
    and Christopher Potts. 2022c. Moving beyond downstream task accuracy for information
    retrieval benchmarking. *arXiv preprint arXiv:2212.01340*.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santhanam 等人 (2022c) Keshav Santhanam, Jon Saad-Falcon, Martin Franz, Omar Khattab,
    Avirup Sil, Radu Florian, Md Arafat Sultan, Salim Roukos, Matei Zaharia 和 Christopher
    Potts. 2022c. 超越下游任务准确性的检索信息基准。*arXiv 预印本 arXiv:2212.01340*。
- en: 'Thakur et al. (2020) Nandan Thakur, Nils Reimers, Johannes Daxenberger, and
    Iryna Gurevych. 2020. Augmented SBERT: Data Augmentation Method for Improving
    Bi-Encoders for Pairwise Sentence Scoring Tasks. *arXiv preprint arXiv:2010.08240*.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thakur 等人 (2020) Nandan Thakur, Nils Reimers, Johannes Daxenberger 和 Iryna
    Gurevych. 2020. 增强的 SBERT: 改进双编码器进行成对句子评分任务的数据增强方法。*arXiv 预印本 arXiv:2010.08240*。'
- en: 'Thakur et al. (2021) Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek
    Srivastava, and Iryna Gurevych. 2021. [BEIR: A heterogeneous benchmark for zero-shot
    evaluation of information retrieval models](https://openreview.net/forum?id=wCu6T5xFjeJ).
    In *Thirty-fifth Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track (Round 2)*.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thakur 等人 (2021) Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava
    和 Iryna Gurevych. 2021. [BEIR: 用于零样本评估信息检索模型的异质基准](https://openreview.net/forum?id=wCu6T5xFjeJ)。在
    *第三十五届神经信息处理系统会议数据集与基准追踪（第二轮）*。'
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. [Attention
    is all you need](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf).
    In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
    and R. Garnett, editors, *Advances in Neural Information Processing Systems 30*,
    pages 5998–6008\. Curran Associates, Inc.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人 (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Ł ukasz Kaiser 和 Illia Polosukhin. 2017. [Attention
    is all you need](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)。在
    I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan 和 R.
    Garnett 主编的 *神经信息处理系统进展 30*，第 5998–6008 页。Curran Associates, Inc.
- en: 'Voorhees et al. (2021) Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman,
    William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2021.
    TREC-COVID: constructing a pandemic information retrieval test collection. In
    *ACM SIGIR Forum*, volume 54, pages 1–12\. ACM New York, NY, USA.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Voorhees 等人 (2021) Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman,
    William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff 和 Lucy Lu Wang. 2021. TREC-COVID:
    构建疫情信息检索测试集。在 *ACM SIGIR 论坛*，第 54 卷，第 1–12 页。ACM 纽约，美国。'
- en: 'Wang et al. (2022) Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych.
    2022. [GPL: Generative pseudo labeling for unsupervised domain adaptation of dense
    retrieval](https://doi.org/10.18653/v1/2022.naacl-main.168). In *Proceedings of
    the 2022 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 2345–2360, Seattle, United States.
    Association for Computational Linguistics.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 (2022) Kexin Wang, Nandan Thakur, Nils Reimers 和 Iryna Gurevych. 2022.
    [GPL: 用于密集检索的生成伪标注的无监督领域自适应](https://doi.org/10.18653/v1/2022.naacl-main.168)。在
    *2022 年北美计算语言学协会年会：人类语言技术会议论文集*，第 2345–2360 页，西雅图，美国。计算语言学协会。'
- en: 'Wang et al. (2020) Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas,
    Jiangjiang Yang, Doug Burdick, Darrin Eide, Kathryn Funk, Yannis Katsis, Rodney Michael
    Kinney, Yunyao Li, Ziyang Liu, William Merrill, Paul Mooney, Dewey A. Murdick,
    Devvret Rishi, Jerry Sheehan, Zhihong Shen, Brandon Stilson, Alex D. Wade, Kuansan
    Wang, Nancy Xin Ru Wang, Christopher Wilhelm, Boya Xie, Douglas M. Raymond, Daniel S.
    Weld, Oren Etzioni, and Sebastian Kohlmeier. 2020. [CORD-19: The COVID-19 open
    research dataset](https://aclanthology.org/2020.nlpcovid19-acl.1). In *Proceedings
    of the 1st Workshop on NLP for COVID-19 at ACL 2020*, Online. Association for
    Computational Linguistics.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020）Lucy Lu Wang、Kyle Lo、Yoganand Chandrasekhar、Russell Reas、Jiangjiang
    Yang、Doug Burdick、Darrin Eide、Kathryn Funk、Yannis Katsis、Rodney Michael Kinney、Yunyao
    Li、Ziyang Liu、William Merrill、Paul Mooney、Dewey A. Murdick、Devvret Rishi、Jerry
    Sheehan、Zhihong Shen、Brandon Stilson、Alex D. Wade、Kuansan Wang、Nancy Xin Ru Wang、Christopher
    Wilhelm、Boya Xie、Douglas M. Raymond、Daniel S. Weld、Oren Etzioni 和 Sebastian Kohlmeier。2020年。[CORD-19：COVID-19
    开放研究数据集](https://aclanthology.org/2020.nlpcovid19-acl.1)。在 *2020年ACL第1届COVID-19自然语言处理研讨会论文集*，在线。计算语言学协会。
- en: 'Wang et al. (2021) Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu
    Wei. 2021. [MiniLMv2: Multi-head self-attention relation distillation for compressing
    pretrained transformers](https://doi.org/10.18653/v1/2021.findings-acl.188). In
    *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*,
    pages 2140–2151, Online. Association for Computational Linguistics.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2021）Wenhui Wang、Hangbo Bao、Shaohan Huang、Li Dong 和 Furu Wei。2021年。[MiniLMv2：用于压缩预训练变换器的多头自注意力关系蒸馏](https://doi.org/10.18653/v1/2021.findings-acl.188)。在
    *计算语言学协会发现：ACL-IJCNLP 2021*，第2140–2151页，在线。计算语言学协会。
- en: 'Xin et al. (2022) Ji Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita Sharma,
    Damien Jose, and Paul Bennett. 2022. [Zero-shot dense retrieval with momentum
    adversarial domain invariant representations](https://doi.org/10.18653/v1/2022.findings-acl.316).
    In *Findings of the Association for Computational Linguistics: ACL 2022*, pages
    4008–4020, Dublin, Ireland. Association for Computational Linguistics.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xin 等（2022）Ji Xin、Chenyan Xiong、Ashwin Srinivasan、Ankita Sharma、Damien Jose
    和 Paul Bennett。2022年。[零样本密集检索与动量对抗领域不变表示](https://doi.org/10.18653/v1/2022.findings-acl.316)。在
    *计算语言学协会发现：ACL 2022*，第4008–4020页，爱尔兰都柏林。计算语言学协会。
- en: 'Yang et al. (2020) Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha
    Swayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and
    Doug Downey. 2020. [Generative data augmentation for commonsense reasoning](https://doi.org/10.18653/v1/2020.findings-emnlp.90).
    In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages
    1008–1025, Online. Association for Computational Linguistics.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2020）Yiben Yang、Chaitanya Malaviya、Jared Fernandez、Swabha Swayamdipta、Ronan
    Le Bras、Ji-Ping Wang、Chandra Bhagavatula、Yejin Choi 和 Doug Downey。2020年。[用于常识推理的生成数据增强](https://doi.org/10.18653/v1/2020.findings-emnlp.90)。在
    *计算语言学协会发现：EMNLP 2020*，第1008–1025页，在线。计算语言学协会。
- en: Appendix
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Reranker Configurations
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 重新排序器配置
- en: 'We aim to understand the impact of different model configurations on the efficacy
    of our domain adaptation technique. We expand on experiments from [Section 4.3](#S4.SS3
    "4.3 Multi-Reranker Domain Adaptation ‣ 4 Experiments ‣ UDAPDR: Unsupervised Domain
    Adaptation via LLM Prompting and Distillation of Rerankers") and explore alternate
    configurations of key factors in the UDAPDR approach. Specifically, we want to
    answer the following questions through their corresponding experiments:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们旨在了解不同模型配置对我们领域适应技术效果的影响。我们在[第4.3节](#S4.SS3 "4.3 多重重新排序器领域适应 ‣ 4 实验 ‣ UDAPDR：通过LLM提示和重新排序器蒸馏进行无监督领域适应")的实验基础上进行扩展，并探讨UDAPDR方法中关键因素的其他配置。具体来说，我们希望通过相应的实验回答以下问题：
- en: '1.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: (a)
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: 'Question: Do corpus-adapted prompts improve domain adaptation and retriever
    distillation?'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题：语料库适应的提示是否改善领域适应性和检索器蒸馏？
- en: (b)
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: 'Experiment: Compare zero/few-shot prompts to corpus-adapted prompts for synthetic
    question generation ([Table 4](#S4.T4 "Table 4 ‣ 4.5 Impact of Pretrained Components
    ‣ 4 Experiments ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and
    Distillation of Rerankers")).'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验：比较零样本/少样本提示与语料库适应提示在合成问题生成中的效果（[表4](#S4.T4 "表 4 ‣ 4.5 预训练组件的影响 ‣ 4 实验 ‣ UDAPDR：通过LLM提示和重新排序器蒸馏进行无监督领域适应")）。
- en: '2.'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: (a)
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: 'Question: How does the number of rerankers affect downstream retrieval accuracy?'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题：重新排序器的数量如何影响下游检索准确性？
- en: (b)
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: 'Experiment: Compare single-reranker to multi-reranker approach (with different
    reranker selection strategies) across various target domains ([Table 7](#A1.T7
    "Table 7 ‣ Appendix A Reranker Configurations ‣ UDAPDR: Unsupervised Domain Adaptation
    via LLM Prompting and Distillation of Rerankers")).'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '实验：比较单重排序器与多重排序器方法（采用不同的排序器选择策略）在各种目标领域的表现（[表 7](#A1.T7 "Table 7 ‣ Appendix
    A Reranker Configurations ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting
    and Distillation of Rerankers")）。'
- en: '3.'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: (a)
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: 'Question: How does the synthetic query count affect downstream retrieval accuracy?'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题：合成查询计数如何影响下游检索准确性？
- en: (b)
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: 'Experiment: Explore different query count configurations ranging from several
    thousand to hundreds of thousands of synthetic queries ([Table 1](#S4.T1 "Table
    1 ‣ 4.3 Multi-Reranker Domain Adaptation ‣ 4 Experiments ‣ UDAPDR: Unsupervised
    Domain Adaptation via LLM Prompting and Distillation of Rerankers")).'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '实验：探索不同的查询计数配置，从几千到几万的合成查询（[表 1](#S4.T1 "Table 1 ‣ 4.3 Multi-Reranker Domain
    Adaptation ‣ 4 Experiments ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting
    and Distillation of Rerankers")）。'
- en: '4.'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: (a)
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: 'Question: How do triple counts during distillation affect domain adaptation
    for ColBERTv2?'
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题：三重计数在蒸馏过程中如何影响 ColBERTv2 的领域适应？
- en: (b)
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: 'Experiment: Explore different triple counts ranging from several thousand to
    millions of triples ([Table 8](#A1.T8 "Table 8 ‣ Appendix A Reranker Configurations
    ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of
    Rerankers")).'
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '实验：探索不同的三重计数，从几千到数百万三元组（[表 8](#A1.T8 "Table 8 ‣ Appendix A Reranker Configurations
    ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of
    Rerankers")）。'
- en: 'In [Table 7](#A1.T7 "Table 7 ‣ Appendix A Reranker Configurations ‣ UDAPDR:
    Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers")
    and [Table 8](#A1.T8 "Table 8 ‣ Appendix A Reranker Configurations ‣ UDAPDR: Unsupervised
    Domain Adaptation via LLM Prompting and Distillation of Rerankers") (and [Table 1](#S4.T1
    "Table 1 ‣ 4.3 Multi-Reranker Domain Adaptation ‣ 4 Experiments ‣ UDAPDR: Unsupervised
    Domain Adaptation via LLM Prompting and Distillation of Rerankers") in the main
    text), we outline the results of different experimental configurations in which
    we alter synthetic query generation and passage reranker training. Based on our
    results for the LoTTE pooled dataset, we find that training multiple rerankers
    and selecting the best performing rerankers can improve our unsupervised domain
    adaptation approach. We generate multiple corpus-adapted prompts and rerankers
    to prevent edge cases in which sampled in-domain passages and queries have low
    quality. Furthermore, distilling the passage rerankers with ColBERTv2 allows us
    preserve their accuracy gains while avoiding their high computational costs. However,
    training many rerankers and selecting the best five rerankers can be computationally
    intensive and ultimately unnecessary to achieve domain adaptation. The simpler
    approach of training several rerankers and using them for distillation, without
    any quality filtering, yields comparable results with only a 0.6 point drop in
    Success@5 on average while computing 10x less synthetic queries ([Table 7](#A1.T7
    "Table 7 ‣ Appendix A Reranker Configurations ‣ UDAPDR: Unsupervised Domain Adaptation
    via LLM Prompting and Distillation of Rerankers")). Additionally, by using our
    rerankers to generate more triples for distillation with ColBERTv2, we were able
    to boost performance even further as shown in [Table 8](#A1.T8 "Table 8 ‣ Appendix
    A Reranker Configurations ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting
    and Distillation of Rerankers").'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表 7](#A1.T7 "Table 7 ‣ 附录 A 重新排序器配置 ‣ UDAPDR：通过 LLM 提示和重新排序器的蒸馏进行无监督领域适应")和[表
    8](#A1.T8 "Table 8 ‣ 附录 A 重新排序器配置 ‣ UDAPDR：通过 LLM 提示和重新排序器的蒸馏进行无监督领域适应")（以及[表
    1](#S4.T1 "Table 1 ‣ 4.3 多重重新排序器领域适应 ‣ 4 实验 ‣ UDAPDR：通过 LLM 提示和重新排序器的蒸馏进行无监督领域适应")在正文中），我们概述了不同实验配置的结果，其中我们改变了合成查询生成和段落重新排序器训练。基于我们对
    LoTTE 汇总数据集的结果，我们发现训练多个重新排序器并选择表现最佳的重新排序器可以改进我们的无监督领域适应方法。我们生成多个语料库适应的提示和重新排序器，以防止在样本领域内的段落和查询质量较低的边缘情况。此外，使用
    ColBERTv2 蒸馏段落重新排序器可以在避免高计算成本的同时保持其准确性。然而，训练许多重新排序器并选择表现最好的五个重新排序器可能会计算密集且最终不必要以实现领域适应。通过训练几个重新排序器并利用它们进行蒸馏的简单方法，虽然成功率@5平均下降了0.6分，但计算了10倍更少的合成查询（[表
    7](#A1.T7 "Table 7 ‣ 附录 A 重新排序器配置 ‣ UDAPDR：通过 LLM 提示和重新排序器的蒸馏进行无监督领域适应")）。此外，通过使用我们的重新排序器生成更多的三元组进行
    ColBERTv2 的蒸馏，我们能够进一步提升性能，如[表 8](#A1.T8 "Table 8 ‣ 附录 A 重新排序器配置 ‣ UDAPDR：通过 LLM
    提示和重新排序器的蒸馏进行无监督领域适应")所示。
- en: '|  |  | ColBERTv2 Distillation with UDAPDR |  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ColBERTv2 蒸馏与 UDAPDR |  |'
- en: '|  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '&#124; Zero-shot &#124;'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 零样本 &#124;'
- en: '&#124; ColBERTv2 &#124;'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ColBERTv2 &#124;'
- en: '|'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 1 of 50 &#124;'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1 of 50 &#124;'
- en: '&#124; Rerankers &#124;'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重新排序器 &#124;'
- en: '|'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 5 of 50 &#124;'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5 of 50 &#124;'
- en: '&#124; Rerankers &#124;'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重新排序器 &#124;'
- en: '|'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 10 of 50 &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 10 of 50 &#124;'
- en: '&#124; Rerankers &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重新排序器 &#124;'
- en: '|'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 5 of 5 &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5 of 5 &#124;'
- en: '&#124; Rerankers &#124;'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重新排序器 &#124;'
- en: '|'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Zero-shot &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 零样本 &#124;'
- en: '&#124; ColBERTv2 &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ColBERTv2 &#124;'
- en: '&#124; + Reranker &#124;'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + 重新排序器 &#124;'
- en: '|'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| LoTTE Lifestyle | 64.5 | 67.5 | 73.2 | 73.6 | 72.8 | 73.5 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE 生活方式 | 64.5 | 67.5 | 73.2 | 73.6 | 72.8 | 73.5 |'
- en: '| LoTTE Techology | 44.5 | 46.3 | 50.3 | 50.7 | 49.9 | 50.6 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE 技术 | 44.5 | 46.3 | 50.3 | 50.7 | 49.9 | 50.6 |'
- en: '| LoTTE Writing | 80.0 | 81.5 | 83.4 | 84.2 | 83.0 | 85.5 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE 写作 | 80.0 | 81.5 | 83.4 | 84.2 | 83.0 | 85.5 |'
- en: '| LoTTE Recreation | 70.8 | 73.7 | 77.8 | 78.3 | 76.7 | 79.1 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE 娱乐 | 70.8 | 73.7 | 77.8 | 78.3 | 76.7 | 79.1 |'
- en: '| LoTTE Science | 61.5 | 63.0 | 66.3 | 66.8 | 65.5 | 67.2 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE 科学 | 61.5 | 63.0 | 66.3 | 66.8 | 65.5 | 67.2 |'
- en: '| \hdashline[1pt/1pt] LoTTE Pooled | 63.7 | 66.3 | 70.3 | 70.7 | 69.6 | 71.1
    |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[1pt/1pt] LoTTE 汇总 | 63.7 | 66.3 | 70.3 | 70.7 | 69.6 | 71.1 |'
- en: '| NaturalQuestions | 68.9 | 70.1 | 73.0 | 73.5 | 72.7 | 73.9 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| NaturalQuestions | 68.9 | 70.1 | 73.0 | 73.5 | 72.7 | 73.9 |'
- en: '| SQuAD | 65.0 | 67.2 | 71.0 | 71.3 | 70.4 | 72.6 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| SQuAD | 65.0 | 67.2 | 71.0 | 71.3 | 70.4 | 72.6 |'
- en: 'Table 7: Success@5 for Multi-Reranker Domain Adaptation Strategies with Different
    Reranker Counts. LoTTE dataset results correspond to the Forum configuration.
    All results correspond to dev sets of each task. The reranker used in the experiments
    is DeBERTa-v3-Large. The ColBERTv2 distillation strategies train $X$ based on
    their performance on the dev set of the target domain. Through the selection process,
    we aim to find an upper bound for retrieval accuracy, even though access to an
    annotated dev set is not realistic for all domains. In our distillation strategies,
    each reranker was trained on 2,000 synthetic queries. For our non-distilled reranker
    used in the final column, we trained it on 100,000 synthetic queries. The synthetic
    queries were created using Flan-T5 XXL model and the prompting strategy outlined
    in [Section 3](#S3 "3 Methodology ‣ UDAPDR: Unsupervised Domain Adaptation via
    LLM Prompting and Distillation of Rerankers").'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7：针对不同重排器数量的多重重排器领域适应策略在 Success@5 的表现。LoTTE 数据集结果对应于 Forum 配置。所有结果均对应于每个任务的开发集。实验中使用的重排器是
    DeBERTa-v3-Large。ColBERTv2 蒸馏策略根据其在目标领域开发集上的表现训练 $X$。通过选择过程，我们旨在找到检索准确率的上限，尽管并非所有领域都有现实的标注开发集。在我们的蒸馏策略中，每个重排器在
    2,000 个合成查询上进行训练。对于我们在最后一列使用的未蒸馏重排器，我们在 100,000 个合成查询上进行了训练。合成查询是使用 Flan-T5 XXL
    模型和 [第 3 节](#S3 "3 Methodology ‣ UDAPDR: Unsupervised Domain Adaptation via LLM
    Prompting and Distillation of Rerankers") 中概述的提示策略创建的。'
- en: '|  |  | ColBERTv2 Distillation with UDAPDR |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ColBERTv2 蒸馏与 UDAPDR |  |'
- en: '|  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '&#124; Zero-shot &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Zero-shot &#124;'
- en: '&#124; ColBERTv2 &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ColBERTv2 &#124;'
- en: '|'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 1000 &#124;'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1000 &#124;'
- en: '&#124; triples &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; triples &#124;'
- en: '|'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 10000 &#124;'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 10000 &#124;'
- en: '&#124; triples &#124;'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; triples &#124;'
- en: '|'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 100000 &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 100000 &#124;'
- en: '&#124; triples &#124;'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; triples &#124;'
- en: '|'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Zero-shot &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Zero-shot &#124;'
- en: '&#124; ColBERTv2 &#124;'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ColBERTv2 &#124;'
- en: '&#124; + Reranker &#124;'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + Reranker &#124;'
- en: '|'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| LoTTE Lifestyle | 64.5 | 67.4 | 70.1 | 72.4 | 73.5 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE Lifestyle | 64.5 | 67.4 | 70.1 | 72.4 | 73.5 |'
- en: '| LoTTE Techology | 44.5 | 46.0 | 47.8 | 50.2 | 50.6 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE Techology | 44.5 | 46.0 | 47.8 | 50.2 | 50.6 |'
- en: '| LoTTE Writing | 80.0 | 82.5 | 83.4 | 85.0 | 85.5 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE Writing | 80.0 | 82.5 | 83.4 | 85.0 | 85.5 |'
- en: '| LoTTE Recreation | 70.8 | 72.3 | 76.5 | 77.7 | 79.1 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE Recreation | 70.8 | 72.3 | 76.5 | 77.7 | 79.1 |'
- en: '| LoTTE Science | 61.5 | 62.0 | 64.2 | 66.5 | 67.2 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| LoTTE Science | 61.5 | 62.0 | 64.2 | 66.5 | 67.2 |'
- en: '| \hdashline[1pt/1pt] LoTTE Pooled | 63.7 | 66.0 | 68.4 | 70.4 | 71.1 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[1pt/1pt] LoTTE Pooled | 63.7 | 66.0 | 68.4 | 70.4 | 71.1 |'
- en: '| NaturalQuestions | 68.9 | 70.5 | 72.7 | 73.4 | 73.9 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| NaturalQuestions | 68.9 | 70.5 | 72.7 | 73.4 | 73.9 |'
- en: '| SQuAD | 65.0 | 67.2 | 70.6 | 71.5 | 72.6 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| SQuAD | 65.0 | 67.2 | 70.6 | 71.5 | 72.6 |'
- en: 'Table 8: Success@5 for Multi-Reranker Domain Adaptation Strategies with Various
    Distillation Triples Counts. LoTTE dataset results correspond to the Forum configuration.
    All results correspond to dev sets of each task. The reranker used in the experiments
    is DeBERTa-v3-Large. The ColBERTv2 distillation strategies use a single reranker
    trained on 10,000 synthetic queries; this reranker then generates the specified
    number of labeled triples. For our non-distilled reranker used in the final column,
    we trained it on 100,000 synthetic queries. The synthetic queries were created
    using Flan-T5 XXL model and the prompting strategy outlined in [Section 3](#S3
    "3 Methodology ‣ UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and
    Distillation of Rerankers").'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8：针对不同蒸馏三元组数量的多重重排器领域适应策略在 Success@5 的表现。LoTTE 数据集结果对应于 Forum 配置。所有结果均对应于每个任务的开发集。实验中使用的重排器是
    DeBERTa-v3-Large。ColBERTv2 蒸馏策略使用一个在 10,000 个合成查询上训练的重排器；该重排器随后生成指定数量的标记三元组。对于我们在最后一列使用的未蒸馏重排器，我们在
    100,000 个合成查询上进行了训练。合成查询是使用 Flan-T5 XXL 模型和 [第 3 节](#S3 "3 Methodology ‣ UDAPDR:
    Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers")
    中概述的提示策略创建的。'
- en: Appendix B Fine-tuning Rerankers and Retriever
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B：重排器和检索器的微调
- en: For all passage reranker models that we fine-tune, we optimize for cross-entropy
    loss using Adam Kingma and Ba ([2014](#bib.bib26)) and apply a 0.1 dropout to
    the Transformer outputs. We feed the final hidden state of the [CLS] token into
    a single linear classification layer. We fine-tune for 1 epoch in all experimental
    configurations. Additionally, we using a 5e-6 learning rate combined with a linear
    warmup and linear decay for training Howard and Ruder ([2018](#bib.bib17)). We
    use a batch size of 32 across all experimental configurations.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有我们微调的段落重新排序模型，我们使用Adam Kingma和Ba（[2014](#bib.bib26)）来优化交叉熵损失，并对Transformer输出应用0.1的dropout。我们将[CLS]标记的最终隐藏状态输入到一个线性分类层中。在所有实验配置中，我们微调1个epoch。此外，我们结合线性预热和线性衰减使用`5e-6`的学习率进行训练Howard和Ruder（[2018](#bib.bib17)）。我们在所有实验配置中使用32的批量大小。
- en: For our ColBERTv2 retriever, we use a 1e-5 learning rate and a batch size of
    32 during distillation. The ColBERTv2 maximum document length is set to 300 tokens.
    We use a BERT-Base model Devlin et al. ([2019](#bib.bib9)) as our encoder.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的ColBERTv2检索器，我们在蒸馏过程中使用`1e-5`的学习率和32的批量大小。ColBERTv2的最大文档长度设置为300个标记。我们使用BERT-Base模型Devlin等人（[2019](#bib.bib9)）作为我们的编码器。
- en: Instead of fine-tuning the rerankers, we also tried fine-tuning ColBERTv2 directly
    with the synthetic datasets. We found that fine-tuning the retriever directly
    with the synthetic queries offered only limited benefits, only improving zero-shot
    retrieval by 1-3 points of accuracy at best and decreasing zero-shot accuracy
    at worst (for the LoTTE Forum dev set). Distilling the rerankers offered more
    substantive gains and better adaptation to the target domains more generally.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试了直接用合成数据集对ColBERTv2进行微调，而不是微调重新排序器。我们发现，直接用合成查询微调检索器的效果有限，最多只提高了1-3个准确度点，最糟糕的情况下（对于LoTTE
    Forum开发集）还会降低零样本准确度。蒸馏重新排序器提供了更实质性的提升，更好地适应了目标领域。
- en: We ran additional experiments testing UDAPDR’s efficacy on LoTTE Search dev,
    using one reranker trained with a unique set of 2000 synthetic queries. We found
    that the approach boosted accuracy by 1.6 points, increasing accuracy from 71.5
    to 73.1. However, since the synthetic queries could be generated so cheaply, we
    decided to scale to tens of thousands of synthetic queries for further experiments.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了额外的实验，测试UDAPDR在LoTTE Search开发集上的有效性，使用了一个用2000个独特的合成查询训练的重新排序器。我们发现该方法将准确度提高了1.6个百分点，从71.5提高到73.1。然而，由于合成查询可以非常便宜地生成，我们决定将规模扩大到数万个合成查询进行进一步实验。
