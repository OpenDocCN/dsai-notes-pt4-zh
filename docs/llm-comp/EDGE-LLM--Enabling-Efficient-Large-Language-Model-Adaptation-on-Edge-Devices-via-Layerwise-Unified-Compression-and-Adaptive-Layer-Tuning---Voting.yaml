- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:52:19'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:52:19
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge Devices
    via Layerwise Unified Compression and Adaptive Layer Tuning & Voting'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'EDGE-LLM: 通过逐层统一压缩和自适应层调优与投票实现高效的大型语言模型在边缘设备上的适配'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.15758](https://ar5iv.labs.arxiv.org/html/2406.15758)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.15758](https://ar5iv.labs.arxiv.org/html/2406.15758)
- en: \widowpenalties
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \widowpenalties
- en: 5 100 80 60 40 20
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 5 100 80 60 40 20
- en: Zhongzhi Yu¹, Zheng Wang¹, Yuhan Li¹, Haoran You¹, Ruijie Gao¹, Xiaoya Zhou³,
    Sreenidhi Reedy Bommu¹, Yang (Katie) Zhao², Yingyan (Celine) Lin¹ ¹Georgia Institute
    of Technology, ²University of Minnesota, Twin Cities, ³University of California,
    Santa Barbara{zyu401, zwang3478, yli3326, hyou37, eiclab.gatech, sbommu3, celine.lin}@gatech.edu,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Zhongzhi Yu¹, Zheng Wang¹, Yuhan Li¹, Haoran You¹, Ruijie Gao¹, Xiaoya Zhou³,
    Sreenidhi Reedy Bommu¹, Yang (Katie) Zhao², Yingyan (Celine) Lin¹ ¹乔治亚理工学院，²明尼苏达大学双城分校，³加州大学圣塔芭芭拉分校{zyu401,
    zwang3478, yli3326, hyou37, eiclab.gatech, sbommu3, celine.lin}@gatech.edu,
- en: yangzhao@umn.edu, xiaoyazhou@umail.ucsb.edu(2024)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: yangzhao@umn.edu, xiaoyazhou@umail.ucsb.edu(2024)
- en: Abstract.
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Efficient adaption of large language models (LLMs) on edge devices is essential
    for applications requiring continuous and privacy-preserving adaptation and inference.
    However, existing tuning techniques fall short because of the high computation
    and memory overheads. To this end, we introduce a computation- and memory-efficient
    LLM tuning framework, called Edge-LLM, to facilitate affordable and effective
    LLM adaptation on edge devices. Specifically, Edge-LLM features three core components:
    (1) a layer-wise unified compression (LUC) technique to reduce the computation
    overhead by generating layer-wise pruning sparsity and quantization bit-width
    policies, (2) an adaptive layer tuning and voting scheme to reduce the memory
    overhead by reducing the backpropagation depth, and (3) a complementary hardware
    scheduling strategy to handle the irregular computation patterns introduced by
    LUC and adaptive layer tuning, thereby achieving efficient computation and data
    movements. Extensive experiments demonstrate that Edge-LLM achieves a 2.92$\times$
    memory overhead reduction as compared to vanilla tuning methods with a comparable
    task accuracy. Our code is available at [https://github.com/GATECH-EIC/Edge-LLM](https://github.com/GATECH-EIC/Edge-LLM)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在边缘设备上的高效适配对于需要持续且保护隐私的适配和推理的应用至关重要。然而，现有的调优技术由于高计算和内存开销而不足以满足需求。为此，我们引入了一种计算和内存高效的LLM调优框架，称为Edge-LLM，以便在边缘设备上实现经济高效的LLM适配。具体来说，Edge-LLM具有三个核心组件：（1）一种逐层统一压缩（LUC）技术，通过生成逐层剪枝稀疏性和量化位宽策略来减少计算开销，（2）一种自适应层调优和投票方案，通过减少反向传播深度来减少内存开销，以及（3）一种补充的硬件调度策略，以处理LUC和自适应层调优引入的不规则计算模式，从而实现高效的计算和数据移动。大量实验表明，与传统调优方法相比，Edge-LLM在任务准确度相当的情况下，实现了2.92$\times$的内存开销减少。我们的代码可以在[https://github.com/GATECH-EIC/Edge-LLM](https://github.com/GATECH-EIC/Edge-LLM)找到。
- en: '^†^†journalyear: 2024^†^†copyright: rightsretained^†^†conference: 61st ACM/IEEE
    Design Automation Conference; June 23–27, 2024; San Francisco, CA, USA^†^†booktitle:
    61st ACM/IEEE Design Automation Conference (DAC ’24), June 23–27, 2024, San Francisco,
    CA, USA^†^†doi: 10.1145/3649329.3658473^†^†isbn: 979-8-4007-0601-1/24/06^†^†conference:
    61st ACM/IEEE Design Automation Conference; June 23–27, 2024; San Francisco, CA^†^†submissionid:
    1122'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '^†^†journalyear: 2024^†^†copyright: rightsretained^†^†conference: 61st ACM/IEEE
    Design Automation Conference; June 23–27, 2024; San Francisco, CA, USA^†^†booktitle:
    61st ACM/IEEE Design Automation Conference (DAC ’24), June 23–27, 2024, San Francisco,
    CA, USA^†^†doi: 10.1145/3649329.3658473^†^†isbn: 979-8-4007-0601-1/24/06^†^†conference:
    61st ACM/IEEE Design Automation Conference; June 23–27, 2024; San Francisco, CA^†^†submissionid:
    1122'
- en: 1\. Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'In recent days, large language models (LLMs), such as GPT-4 (Bubeck et al.,
    [2023](#bib.bib2)), have shown dominating performance across various applications
    that revolutionize human life. Following this trend, there is an increasing demand
    to develop efficient tuning techniques for LLMs to enable them on applications
    that require continuous and privacy-preserving adaptation. However, the massive
    model size of LLMs hinders directly achieving the LLM adaptation on edge devices
    (e.g., on edge GPUs and smartphones). The challenges are twofold: (1) the excessive
    computation overhead encountered when calculating the forward and backward passes
    of LLMs (Dettmers et al., [2023](#bib.bib3)), and (2) the cumbersome memory overhead
    introduced for storing massive model weights and activations through the tuning
    process. As shown in recent works (Dettmers et al., [2023](#bib.bib3); Liu et al.,
    [2023](#bib.bib12)), LLMs are typically tuned on cutting-edge GPUs (e.g., with
    40GB or 80GB GPU memory), taking more than a GPU day to complete. Even for the
    state-of-the-art (SOTA) efficient tuning method, effectively tuning relatively
    small-scale LLMs (e.g., LLaMA-7B) on edge devices remains impractical (Dettmers
    et al., [2023](#bib.bib3)).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs），例如 GPT-4 (Bubeck et al., [2023](#bib.bib2))，在各种应用中表现出卓越的性能，彻底改变了人类生活。沿着这一趋势，对开发高效的
    LLM 调优技术以使其能够适用于需要持续和隐私保护的应用的需求越来越大。然而，LLM 的庞大模型大小阻碍了直接在边缘设备（例如边缘 GPU 和智能手机）上实现
    LLM 适配。这些挑战有两个方面：(1) 计算 LLM 的前向和反向传播时遇到的过多计算开销 (Dettmers et al., [2023](#bib.bib3))，以及
    (2) 调优过程中用于存储庞大模型权重和激活的繁琐内存开销。如最近的工作所示 (Dettmers et al., [2023](#bib.bib3); Liu
    et al., [2023](#bib.bib12))，LLM 通常在尖端 GPU 上进行调优（例如，具有 40GB 或 80GB GPU 内存），需要超过一个
    GPU 天才能完成。即使对于最先进的（SOTA）高效调优方法，仍然在边缘设备上有效地调优相对小规模的 LLM（例如，LLaMA-7B）仍然不切实际 (Dettmers
    et al., [2023](#bib.bib3))。
- en: Although several existing efforts aim to address the aforementioned challenges,
    each has its own drawbacks. (1) To reduce computation overhead, compressing target
    LLMs first to reduce the model size is a common approach (Dettmers et al., [2023](#bib.bib3);
    et al, [2024](#bib.bib4)). However, how to effectively reduce the redundancy of
    LLMs while maintaining their adaptability is still largely unexplored (Dettmers
    et al., [2023](#bib.bib3)). (2) To mitigate memory overhead, existing methods
    primarily focus on shortening the backpropagation depth (Zhang et al., [2023](#bib.bib24);
    Sung et al., [2022](#bib.bib20)). Unfortunately, the reduced backpropagation depth
    results in only a fraction of blocks in LLMs being updated, limiting the achievable
    performance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已有一些现有的努力旨在解决上述挑战，但每种方法都有其自身的缺点。(1) 为了减少计算开销，将目标 LLM 压缩以减少模型大小是一种常见的方法 (Dettmers
    et al., [2023](#bib.bib3); et al, [2024](#bib.bib4))。然而，如何在保持 LLM 适应性的同时有效减少 LLM
    的冗余仍然大多未被探索 (Dettmers et al., [2023](#bib.bib3))。(2) 为了减轻内存开销，现有方法主要集中在缩短反向传播深度 (Zhang
    et al., [2023](#bib.bib24); Sung et al., [2022](#bib.bib20))。不幸的是，减少的反向传播深度导致
    LLM 中只有一部分块被更新，从而限制了可实现的性能。
- en: In this paper, we develop a comprehensive solution to tackle the two aforementioned
    memory and computation challenges, achieving an effective LLM adaptation. Specifically,
    we make the following contributions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们开发了一个全面的解决方案，以解决上述两种内存和计算挑战，实现有效的 LLM 适配。具体来说，我们做出了以下贡献。
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a comprehensive framework, dubbed Edge-LLM, that tackles the memory
    and computation challenges of the LLM adaptation from both algorithm and hardware
    perspectives, enabling the effective LLM adaptation on edge devices with limited
    memory and computation resources.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个全面的框架，称为 Edge-LLM，该框架从算法和硬件两个角度解决了 LLM 适配中的内存和计算挑战，使得在内存和计算资源有限的边缘设备上能够有效地进行
    LLM 适配。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'On the algorithm side, we accomplish this goal from two directions, each primarily
    focusing on one of the aforementioned challenges: (1) To reduce the computation
    overhead, we propose a low-cost layer-wise unified compression (LUC) method based
    on our empirical observation on LLMs’ layer-wise sensitivities to quantization
    and pruning. (2) To reduce the memory overhead, we introduce an adaptive layer
    tuning and voting scheme. In adaptive layer tuning, we propose to selectively
    update distinct segments of the target LLM and reduce the memory footprint by
    directly connecting the output of the current updating segment to the final layer.
    Further, in adaptive layer voting, we harness the outputs of different segments
    of the target LLM by voting for an optimized output.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在算法方面，我们从两个方向实现这一目标，每个方向主要关注上述挑战之一：（1）为了减少计算开销，我们提出了一种基于对LLM层级敏感性量化和剪枝的实证观察的低成本层级统一压缩（LUC）方法。（2）为了减少内存开销，我们引入了一种自适应层调优和投票机制。在自适应层调优中，我们提出选择性地更新目标LLM的不同部分，并通过将当前更新部分的输出直接连接到最终层来减少内存占用。此外，在自适应层投票中，我们利用目标LLM不同部分的输出进行投票，以获得优化的结果。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: On the hardware side, to better handle the irregular computation patterns (i.e.,
    diverse layer-wise quantization bit-width, layer-wise pruning sparsity, and LLM
    segments to update) introduced by the proposed algorithms, we further integrate
    a complementary hardware scheduling module into Edge-LLM. The hardware scheduling
    module includes a search space and a search strategy considering potential offloading
    strategies, computation schedules, and tensor placements, aiming to better convert
    the theoretical reduction in computation overhead to the hardware efficiency improvement.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在硬件方面，为了更好地处理提出的算法所引入的非规则计算模式（即，层级量化位宽的多样性、层级剪枝稀疏性和需要更新的LLM段），我们进一步将一个互补的硬件调度模块集成到Edge-LLM中。硬件调度模块包括一个搜索空间和一个搜索策略，考虑潜在的卸载策略、计算调度和张量布局，旨在更好地将理论上的计算开销减少转化为硬件效率的提升。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Experiment results and ablation studies validate the effectiveness of our proposed
    Edge-LLM framework. Specifically, Edge-LLM achieves a 0.70%$\sim$ reduction in
    memory overhead during each iteration.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验结果和消融研究验证了我们提出的Edge-LLM框架的有效性。具体来说，Edge-LLM在每次迭代中实现了0.70%$\sim$的内存开销减少。
- en: 2\. Background and Motivation
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景与动机
- en: 2.1\. Efficient Tuning Techniques
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 高效调优技术
- en: 'Parameter-efficient tuning (PET) comprises techniques for tuning LLMs to new
    tasks using a limited number of trainable parameters, typically less than 10%
    of the total parameters in the target LLMs (Hu et al., [2021](#bib.bib11); Sung
    et al., [2022](#bib.bib20); et al, [2023a](#bib.bib6), [b](#bib.bib7)). It offers
    two major advantages: (1) reduced storage overhead, facilitating scalable multitask
    deployment, and (2) a marginal reduction in computation and memory overhead, thanks
    to the reduced number of trainable parameters (Hu et al., [2021](#bib.bib11)).
    Despite PET’s widespread use, directly applying it for on-device LLM adaptation
    remains impractical due to the remaining memory overhead is still significant.
    This is because PET typically inserts a learnable adapter to most, if not all,
    layers of the target LLM, leading to significant memory overhead to store intermediate
    activations during tuning.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效调优（PET）包括将大规模语言模型（LLM）调整到新任务的技术，使用的可训练参数数量有限，通常少于目标LLM总参数的10%（Hu et al.,
    [2021](#bib.bib11); Sung et al., [2022](#bib.bib20); et al, [2023a](#bib.bib6),
    [b](#bib.bib7)）。它提供了两个主要优势：（1）减少存储开销，便于可扩展的多任务部署，（2）由于可训练参数数量减少，计算和内存开销也有所减少（Hu
    et al., [2021](#bib.bib11)）。尽管PET已被广泛使用，但由于剩余的内存开销仍然很大，直接应用于设备上的LLM适配仍然不切实际。这是因为PET通常会在目标LLM的大多数甚至所有层中插入一个可学习的适配器，这导致在调优过程中存储中间激活的内存开销显著增加。
- en: '![Refer to caption](img/e690aef806bc2742d88b2ba40e732157.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e690aef806bc2742d88b2ba40e732157.png)'
- en: Figure 1\. Profiling results on the memory footprint when tuning LLaMA-7B with
    LoRA (Hu et al., [2021](#bib.bib11)) and QLoRA (Dettmers et al., [2023](#bib.bib3))
    on the Alpaca (Taori et al., [2023](#bib.bib21)) dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 在Alpaca（Taori et al., [2023](#bib.bib21)）数据集上使用LoRA（Hu et al., [2021](#bib.bib11)）和QLoRA（Dettmers
    et al., [2023](#bib.bib3)）调优LLaMA-7B时的内存占用分析结果。
- en: Memory-efficient tuning (MET) aims to minimize the memory footprint during the
    tuning process by reducing backpropagation depth, thereby decreasing the number
    of activations required to be stored in memory (Zhang et al., [2023](#bib.bib24);
    Sung et al., [2022](#bib.bib20)). Existing MET techniques achieve this goal either
    using partial tuning to only tune the final few layers (Zhang et al., [2023](#bib.bib24))
    or leveraging side tuning to add a bypass connection between each adapter module
    with the final output (Sung et al., [2022](#bib.bib20)). While the reduction of
    memory footprint during tuning is highly desirable, existing MET techniques still
    face an unsatisfactory trade-off between accuracy and memory footprint in LLM
    tuning. Specifically, for partial tuning, existing attempts on LLMs need to tune
    more than 80% of layers of the target LLM to achieve a satisfactory task accuracy (Zhang
    et al., [2023](#bib.bib24)), while side tuning suffers from biased optimization
    and struggles to achieve task accuracy comparable to SOTA PET techniques (Sung
    et al., [2022](#bib.bib20)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 内存高效调优（MET）的目标是通过减少反向传播深度来最小化调优过程中的内存占用，从而减少需要存储在内存中的激活数量（Zhang et al., [2023](#bib.bib24);
    Sung et al., [2022](#bib.bib20)）。现有的MET技术通过使用部分调优仅调整最后几层（Zhang et al., [2023](#bib.bib24)）或利用侧向调优在每个适配器模块与最终输出之间添加旁路连接（Sung
    et al., [2022](#bib.bib20)）来实现这一目标。虽然在调优过程中减少内存占用是非常理想的，但现有的MET技术在LLM调优中仍面临准确性与内存占用之间的不令人满意的权衡。具体来说，对于部分调优，现有对LLM的尝试需要调优目标LLM的80%以上层才能实现令人满意的任务准确性（Zhang
    et al., [2023](#bib.bib24)），而侧向调优则面临优化偏差的问题，难以达到与SOTA PET技术相当的任务准确性（Sung et al.,
    [2022](#bib.bib20)）。
- en: Compressing-then-tuning is a series of emerging efficient tuning techniques
    motivated by the observation that the computation overhead in LLM tuning is dominated
    by the forward and backward passes of the LLM’s backbone, due to the excessive
    size of the LLM’s backbone (Dettmers et al., [2023](#bib.bib3)). Thus, some pioneering
    works propose to compress the LLM backbone before tuning to reduce the computation
    and data movement overheads  (Dettmers et al., [2023](#bib.bib3)). However, existing
    SOTA compressing-then-tuning techniques primarily aim to improve tuning speed,
    neglecting the extreme memory overhead (e.g., the SOTA compressing-then-tuning
    method still needs an A100 GPU with 40GB memory to achieve effective tuning on
    the Llama-70B model (Dettmers et al., [2023](#bib.bib3))). This oversight limits
    the effectiveness of compressing-then-tuning techniques in tuning LLMs on resource-constraint
    edge devices.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩后调优是一系列新兴的高效调优技术，动机是观察到LLM调优中的计算开销主要由LLM骨干网络的前向和后向传递主导，因为LLM骨干网络的过大（Dettmers
    et al., [2023](#bib.bib3)）。因此，一些开创性的工作建议在调优之前压缩LLM骨干网络，以减少计算和数据移动开销（Dettmers et
    al., [2023](#bib.bib3)）。然而，现有的SOTA压缩后调优技术主要旨在提高调优速度，而忽视了极端的内存开销（例如，SOTA压缩后调优方法仍需使用具有40GB内存的A100
    GPU来对Llama-70B模型进行有效调优（Dettmers et al., [2023](#bib.bib3)））。这种忽视限制了压缩后调优技术在资源受限的边缘设备上调优LLM的有效性。
- en: 2.2\. Memory Overhead During Tuning
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 调优过程中的内存开销
- en: 'To better understand the gap between the memory needed in existing tuning techniques
    and the memory available on edge devices, we profile the memory requirements to
    tune a Llama-7B model (Zhang et al., [2023](#bib.bib24)) with LoRA (Hu et al.,
    [2021](#bib.bib11)), one of the SOTA PET techniques, and QLoRA (Dettmers et al.,
    [2023](#bib.bib3)), one of the SOTA compressing-then-tuning techniques, respectively.
    As shown in Fig. [1](#S2.F1 "Figure 1 ‣ 2.1\. Efficient Tuning Techniques ‣ 2\.
    Background and Motivation ‣ EDGE-LLM: Enabling Efficient Large Language Model
    Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive Layer
    Tuning & Voting"), the memory overhead of LoRA is dominated by storing the LLM’s
    backbone weights and the activations for backpropagation. Even after QLoRA compressed
    the LLM backbone to 4-bit and reduced the overall memory footprint by 41.2% over
    LoRA, there remains a 1.48$\times\sim 2.22\times$ gap between the memory required
    for tuning and the memory available on commonly used edge devices (e.g., 8 GB
    for TX2 (NVIDIA, [2020](#bib.bib15)) and 12 GB for Quest Pro (Meta, [2022](#bib.bib14))).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '为了更好地了解现有调优技术所需的内存与边缘设备上可用内存之间的差距，我们对用于调优Llama-7B模型（Zhang等，[2023](#bib.bib24)）的内存需求进行了分析，其中使用了LoRA（Hu等，[2021](#bib.bib11)）这一SOTA
    PET技术，以及QLoRA（Dettmers等，[2023](#bib.bib3)），这是一种SOTA压缩-再调优技术。正如图[1](#S2.F1 "Figure
    1 ‣ 2.1\. Efficient Tuning Techniques ‣ 2\. Background and Motivation ‣ EDGE-LLM:
    Enabling Efficient Large Language Model Adaptation on Edge Devices via Layerwise
    Unified Compression and Adaptive Layer Tuning & Voting")所示，LoRA的内存开销主要来源于存储LLM的主干权重和反向传播的激活。即便QLoRA将LLM主干压缩到4位，并将整体内存占用比LoRA减少了41.2%，在调优所需内存与常用边缘设备（如TX2（NVIDIA，[2020](#bib.bib15)）的8
    GB和Quest Pro（Meta，[2022](#bib.bib14)）的12 GB）上可用内存之间，仍然存在1.48$\times\sim 2.22\times$的差距。'
- en: 2.3\. Opportunities for Efficient LLM Tuning
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 高效LLM调优的机会
- en: To tackle the aforementioned limitations of existing tuning methods, we identify
    potential opportunities to improve these methods to develop effective LLM tuning
    frameworks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对现有调优方法的上述局限性，我们识别出改进这些方法的潜在机会，以开发有效的LLM调优框架。
- en: On one hand, to further reduce the computation overhead, we identify a mismatch
    between the previously successful practice aimed at reducing the model redundancy
    and the vanilla compression technique used in existing compressing-then-tuning
    techniques. Specifically, previous efforts (e.g., (et al, [2022](#bib.bib5)) observe
    that deep learning models exhibit redundancy across different dimensions (e.g.,
    bit-width and sparsity) and at different layers. In contrast, existing compressing-then-tuning
    techniques often adopt a uniform compression approach, reducing redundancy from
    only one dimension (Dettmers et al., [2023](#bib.bib3)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，为了进一步减少计算开销，我们发现之前成功实践中旨在减少模型冗余的做法与现有压缩-再调优技术中使用的普通压缩技术之间存在不匹配。具体而言，以前的研究（例如，（et
    al, [2022](#bib.bib5)）观察到深度学习模型在不同维度（如比特宽度和稀疏性）和不同层次上存在冗余。相比之下，现有的压缩-再调优技术通常采用统一的压缩方法，只从一个维度减少冗余（Dettmers等，[2023](#bib.bib3)）。
- en: 'On the other hand, to further reduce the memory overhead, based on our analysis
    in Sec. [2.1](#S2.SS1 "2.1\. Efficient Tuning Techniques ‣ 2\. Background and
    Motivation ‣ EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge
    Devices via Layerwise Unified Compression and Adaptive Layer Tuning & Voting"),
    we summarize that the key to improving the achievable accuracy-memory trade-off
    lies in the ability to update all layers in the LLM with a limited backpropagation
    depth. Inspired by the early exit mechanism developed for efficient model inference (Teerapittayanon
    et al., [2016](#bib.bib22)), we hypothesize that the outputs from early layers
    in the LLM can provide meaningful information for prediction. Thus, it is possible
    to start backpropagation from an early exit layer and still effectively update
    the model. In this scenario, since backpropagation can be initiated from various
    early exit layers, the backpropagation depth required for updating all layers
    in the LLM can be minimized.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，为了进一步减少内存开销，根据我们在第[2.1节](#S2.SS1 "2.1\. 高效调优技术 ‣ 2\. 背景与动机 ‣ EDGE-LLM:
    通过层级统一压缩和自适应层调优与投票实现高效的大型语言模型适应")的分析，我们总结出提高可实现的准确率与内存权衡的关键在于能够在有限的反向传播深度下更新 LLM
    的所有层。受早期退出机制的启发，该机制是为高效模型推断而开发的（Teerapittayanon 等，[2016](#bib.bib22)），我们假设 LLM
    中早期层的输出可以为预测提供有意义的信息。因此，从早期退出层开始反向传播并且仍然有效更新模型是可能的。在这种情况下，由于反向传播可以从各种早期退出层开始，因此更新
    LLM 中所有层所需的反向传播深度可以最小化。'
- en: 3\. Edge-LLM Algorithm
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. Edge-LLM 算法
- en: 3.1\. Overview
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 概述
- en: 'Motivated by the opportunities identified in Sec. [2.3](#S2.SS3 "2.3\. Opportunities
    for Efficient LLM Tuning ‣ 2\. Background and Motivation ‣ EDGE-LLM: Enabling
    Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified
    Compression and Adaptive Layer Tuning & Voting"), we then introduce the algorithm
    design of our proposed Edge-LLM framework to facilitate effective and efficient
    LLM adaptation with limited computation and memory overhead. As shown in Fig. [2](#S3.F2
    "Figure 2 ‣ 3.1\. Overview ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM: Enabling Efficient
    Large Language Model Adaptation on Edge Devices via Layerwise Unified Compression
    and Adaptive Layer Tuning & Voting"), our proposed Edge-LLM tuning algorithm integrates
    two key enablers each leveraging one of the aforementioned opportunities in reducing
    the computation and memory overhead. Specifically: (1) To reduce the computation
    overhead, we propose the LUC technique to diminish the redundancy of the target
    LLM. This technique is motivated by our empirical observation of the diverse layer-wise
    sensitivities of LLMs to quantization and pruning. Based on the observation above,
    we develop a low-cost, mean-square-error-based (MSE-based) identifier in LUC to
    generate a layer-wise compression policy (e.g., layer-wise bit-width and pruning
    sparsity allocation), aiming to improve the accuracy-efficiency trade-off of LUC
    over existing compression techniques in compressing-then-tuning frameworks (Sec. [3.2](#S3.SS2
    "3.2\. Layer-wise Unified Compression (LUC) ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM:
    Enabling Efficient Large Language Model Adaptation on Edge Devices via Layerwise
    Unified Compression and Adaptive Layer Tuning & Voting")). (2) To reduce the memory
    overhead, we propose an adaptive layer tuning scheme that dynamically connects
    the output of a selected layer (potentially different in each iteration) to the
    final classification layer with a skip connection during the forward pass. During
    backpropagation, only a few preceding layers of the selected layer receive gradient
    updates. Because the layers selected for updates vary with different inputs, this
    approach ensures that all layers are effectively updated while minimizing memory
    overhead. This efficiency is achieved through the reduced depth of backpropagation
    enabled by the introduction of skip connections. Furthermore, during inference,
    we introduce a voting mechanism to enhance the accuracy of LLMs tuned with adaptive
    layer tuning. This method capitalizes on the ability of adaptively tuned LLMs
    to produce reasonable outputs from multiple layers. Consequently, each layer generates
    logits, and a voting process is employed to determine the final output (see Sec. [3.3](#S3.SS3
    "3.3\. Adaptive Layer Tuning and Voting ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM: Enabling
    Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified
    Compression and Adaptive Layer Tuning & Voting")).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '受到第[2.3节](#S2.SS3 "2.3\. Opportunities for Efficient LLM Tuning ‣ 2\. Background
    and Motivation ‣ EDGE-LLM: Enabling Efficient Large Language Model Adaptation
    on Edge Devices via Layerwise Unified Compression and Adaptive Layer Tuning &
    Voting")中识别的机会的激励，我们接下来介绍了我们提出的Edge-LLM框架的算法设计，以促进在计算和内存开销有限的情况下有效且高效地调整LLM。如图[2](#S3.F2
    "Figure 2 ‣ 3.1\. Overview ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM: Enabling Efficient
    Large Language Model Adaptation on Edge Devices via Layerwise Unified Compression
    and Adaptive Layer Tuning & Voting")所示，我们提出的Edge-LLM调整算法集成了两个关键驱动因素，每个因素利用了前述的机会之一，以减少计算和内存开销。具体来说：(1)
    为了减少计算开销，我们提出了LUC技术以减少目标LLM的冗余。这项技术的灵感来源于我们对LLM在量化和剪枝方面的层级敏感性的实证观察。基于上述观察，我们在LUC中开发了一个低成本、均方误差（MSE）基础的标识符，用以生成层级压缩策略（例如，层级比特宽度和剪枝稀疏度分配），旨在改善LUC在压缩后调整框架中相对于现有压缩技术的准确性-效率权衡（第[3.2节](#S3.SS2
    "3.2\. Layer-wise Unified Compression (LUC) ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM:
    Enabling Efficient Large Language Model Adaptation on Edge Devices via Layerwise
    Unified Compression and Adaptive Layer Tuning & Voting")）。(2) 为了减少内存开销，我们提出了一种自适应层级调整方案，该方案在前向传播过程中动态地将所选层的输出（每次迭代可能不同）通过跳跃连接连接到最终分类层。在反向传播过程中，只有所选层的少数前驱层接收梯度更新。由于不同输入下选择用于更新的层不同，这种方法确保所有层得到有效更新，同时最小化内存开销。通过引入跳跃连接，这种效率得以实现。此外，在推理过程中，我们引入了一种投票机制，以提高使用自适应层级调整的LLM的准确性。这种方法利用了自适应调整的LLM从多个层生成合理输出的能力。因此，每个层生成logits，并采用投票过程来确定最终输出（见第[3.3节](#S3.SS3
    "3.3\. Adaptive Layer Tuning and Voting ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM: Enabling
    Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified
    Compression and Adaptive Layer Tuning & Voting")）。'
- en: '![Refer to caption](img/c023145c86448e471b9b3ec38d4ba6a4.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/c023145c86448e471b9b3ec38d4ba6a4.png)'
- en: Figure 2\. Comparison between (a) the compressing-then-tuning baseline and (b/c)
    our proposed Edge-LLM method.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. （a）压缩后调优基线与（b/c）我们提出的Edge-LLM方法的比较。
- en: 3.2\. Layer-wise Unified Compression (LUC)
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 层级统一压缩（LUC）
- en: 'Motivating observation on LLM’s layer-wise sensitivity. In prior studies on
    model compression, a common understanding is that different layers in a model
    exhibit different sensitivities to different compression techniques (et al, [2022](#bib.bib5)).
    However, the sensitivities of different layers in LLMs to different compression
    techniques remain an open question. To address this question, we first explore
    the layer-wise sensitivities of the target LLM to pruning and quantization. Specifically,
    we apply different quantization bit-widths and pruning sparsities to each layer
    of a pretrained LLaMA-7B (Touvron et al., [2023](#bib.bib23)) model. By comparing
    the averaged MSE of the compressed and original layer outputs in the target LLM
    fed with the same input from the WikiText dataset (Merity et al., [2016](#bib.bib13)),
    we observe that, as shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3.3\. Adaptive Layer
    Tuning and Voting ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM: Enabling Efficient Large
    Language Model Adaptation on Edge Devices via Layerwise Unified Compression and
    Adaptive Layer Tuning & Voting"), only a small fraction of layers in the LLM have
    high sensitivities to compression.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM的层级敏感性的动机观察。在模型压缩的先前研究中，普遍认为模型中的不同层对不同压缩技术表现出不同的敏感性（et al, [2022](#bib.bib5)）。然而，LLM中不同层对不同压缩技术的敏感性仍然是一个未解之谜。为了解决这个问题，我们首先探索目标LLM对剪枝和量化的层级敏感性。具体来说，我们对一个预训练的LLaMA-7B (Touvron
    et al., [2023](#bib.bib23))模型的每一层应用不同的量化位宽和剪枝稀疏度。通过比较目标LLM在相同输入（来自WikiText数据集 (Merity
    et al., [2016](#bib.bib13))）下压缩层输出和原始层输出的平均MSE，我们观察到，如图[3](#S3.F3 "Figure 3 ‣
    3.3\. Adaptive Layer Tuning and Voting ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM: Enabling
    Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified
    Compression and Adaptive Layer Tuning & Voting")所示，LLM中只有少部分层对压缩具有高敏感性。'
- en: Our hypothesis and the proposed LUC. Based on the observation above, we hypothesize
    that the high sensitivity (i.e., high MSE) is due to limited redundancy in the
    corresponding layer, thereby necessitating a lower compression ratio. To this
    end, we propose the following mapping functions to map the layer-wise MSE to the
    layer-wise quantization bit-width and pruning sparsity, respectively. For quantization,
    given an LLM $M$ as
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的假设和提出的LUC。基于上述观察，我们假设高敏感性（即高MSE）是由于相应层的冗余性有限，因此需要较低的压缩比。为此，我们提出了以下映射函数，将层级MSE分别映射到层级量化位宽和剪枝稀疏度。对于量化，给定一个LLM
    $M$ 如下：
- en: '| (1) |  | $1$2 |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $1$2 |  |'
- en: where $\mathbbm{1}(.)$ as
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbbm{1}(.)$ 如下：
- en: '| (2) |  | $p_{j}=P\times L\times\frac{s_{prune}^{j}}{\sum_{i=1}^{L-1}s_{prune}^{i}},\vspace{-0.3em}$
    |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $p_{j}=P\times L\times\frac{s_{prune}^{j}}{\sum_{i=1}^{L-1}s_{prune}^{i}},\vspace{-0.3em}$
    |  |'
- en: where $s_{prune}^{j}$.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s_{prune}^{j}$。
- en: 3.3\. Adaptive Layer Tuning and Voting
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 自适应层调优与投票
- en: 'In this enabler, our objective is to facilitate effective tuning with reduced
    memory overhead, thereby fitting the tuning process into edge devices with limited
    memory capacity. To achieve this, the primary challenge we’ve identified is enabling
    efficient updates across all layers of the target LLM with restricted backpropagation
    depth, as analyzed in Sec. [2.3](#S2.SS3 "2.3\. Opportunities for Efficient LLM
    Tuning ‣ 2\. Background and Motivation ‣ EDGE-LLM: Enabling Efficient Large Language
    Model Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive
    Layer Tuning & Voting").'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个使能器中，我们的目标是通过减少内存开销来促进有效的调优，从而使调优过程适应具有有限内存容量的边缘设备。为此，我们确定的主要挑战是使目标LLM的所有层能够在有限的反向传播深度下进行高效更新，如第[2.3](#S2.SS3
    "2.3\. Opportunities for Efficient LLM Tuning ‣ 2\. Background and Motivation
    ‣ EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge Devices
    via Layerwise Unified Compression and Adaptive Layer Tuning & Voting")节分析。'
- en: In Edge-LLM, we alleviate this challenge by constructing a set of exit layers
    $\mathcal{T}=\{t_{0},t_{1},\cdots,t_{T-1}\}$ denotes the number of layers that
    have unfrozen trainable parameters in this configuration.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在Edge-LLM中，我们通过构建一组退出层 $\mathcal{T}=\{t_{0},t_{1},\cdots,t_{T-1}\}$ 来缓解这一挑战，表示在此配置中具有未冻结可训练参数的层数。
- en: '![Refer to caption](img/fe507eddee0e7e962db8a5b9e2d68a73.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/fe507eddee0e7e962db8a5b9e2d68a73.png)'
- en: Figure 3\. Visualization of LLaMA-7B’s layer-wise sensitivity to (a) quantization
    and (b) pruning.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. LLaMA-7B模型对（a）量化和（b）剪枝的逐层敏感性可视化。
- en: Furthermore, with the adaptive layer tuning described above, the tuned LLM can
    generate outputs from all layers $t\in\mathcal{T}$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过上述自适应层调整，调整后的LLM可以从所有层生成输出 $t\in\mathcal{T}$。
- en: 4\. Edge-LLM Hardware Scheduling
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. Edge-LLM硬件调度
- en: '![Refer to caption](img/10d474b727927e1a044f1b4330ee0b59.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/10d474b727927e1a044f1b4330ee0b59.png)'
- en: Figure 4\. The overview of our hardware scheduling.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 我们的硬件调度概览。
- en: Motivation. The aforementioned algorithm designs introduce an irregular computation
    pattern (i.e., diverse layer-wise quantization bit-width, layer-wise pruning sparsity,
    and layers to update). This complexity makes it challenging for real devices to
    fully benefit from the algorithm’s theoretical reduction in computation overhead.
    To address this challenge, we propose a complementary hardware scheduling module,
    focusing on efficient scheduling and offloading strategies tailored for optimizing
    LLM inference throughput. The on-chip accelerator SRAM size limitation (512KB$\sim$256GB).
    Our hardware acceleration is motivated by the need to establish a comprehensive
    cost model, serving as the basis for efficient memory scheduling or offloading
    strategies for each early exit block in the system.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 动机。前述算法设计引入了不规则的计算模式（即，各层的量化位宽、多样的剪枝稀疏度和需要更新的层）。这种复杂性使得实际设备难以充分利用算法在计算开销上的理论减少。为应对这一挑战，我们提出了一个补充的硬件调度模块，专注于高效的调度和卸载策略，以优化LLM推理吞吐量。片上加速器的SRAM大小限制（512KB$\sim$256GB）。我们的硬件加速动机在于建立一个全面的成本模型，作为高效内存调度或卸载策略的基础，适用于系统中的每个早期退出块。
- en: 4.1\. Overview
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 概览
- en: 'In the pursuit of optimizing the scheduling and offloading strategies for LLM
    hardware accelerators, our methodology allocates bit-widths and pruning sparsities
    to each layer based on sensitivity (see Sec. [3.2](#S3.SS2 "3.2\. Layer-wise Unified
    Compression (LUC) ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM: Enabling Efficient Large
    Language Model Adaptation on Edge Devices via Layerwise Unified Compression and
    Adaptive Layer Tuning & Voting")). Subsequently, we conduct a nuanced exploration
    to identify the optimal offloading strategy for each early exit block. As depicted
    in Fig. [4](#S4.F4 "Figure 4 ‣ 4\. Edge-LLM Hardware Scheduling ‣ EDGE-LLM: Enabling
    Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified
    Compression and Adaptive Layer Tuning & Voting") (a) and (b), these two steps
    take algorithm hyperparameters as inputs and yield the final allocation strategy
    and hardware schedulings as outputs.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '为了优化LLM硬件加速器的调度和卸载策略，我们的方法根据敏感性（见第[3.2](#S3.SS2 "3.2\. Layer-wise Unified Compression
    (LUC) ‣ 3\. Edge-LLM Algorithm ‣ EDGE-LLM: Enabling Efficient Large Language Model
    Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive Layer
    Tuning & Voting")节）为每层分配位宽和剪枝稀疏度。随后，我们进行细致的探索，以识别每个早期退出块的最佳卸载策略。如图 [4](#S4.F4
    "Figure 4 ‣ 4\. Edge-LLM Hardware Scheduling ‣ EDGE-LLM: Enabling Efficient Large
    Language Model Adaptation on Edge Devices via Layerwise Unified Compression and
    Adaptive Layer Tuning & Voting") (a) 和 (b) 所示，这两个步骤将算法超参数作为输入，输出最终的分配策略和硬件调度。'
- en: 4.2\. Searching Objective
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 搜索目标
- en: 'We conceptualize the LLM tuning with offloading as a graph traversal problem
    following (Sheng et al., [2023](#bib.bib19)). In Fig. [4](#S4.F4 "Figure 4 ‣ 4\.
    Edge-LLM Hardware Scheduling ‣ EDGE-LLM: Enabling Efficient Large Language Model
    Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive Layer
    Tuning & Voting") (c), we present an illustrative computational graph consisting
    of three dimensions of batches, layers, and tokens. In the depicted graph, each
    square denotes the computation of a specific layer. Squares sharing the same color
    indicate the utilization of identical layer weights. A valid path is defined as
    a trajectory that traverses (i.e., computes) all squares, adhering to the following
    constraint:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将LLM调整与卸载概念化为图遍历问题，参考（Sheng et al., [2023](#bib.bib19)）。在图 [4](#S4.F4 "Figure
    4 ‣ 4\. Edge-LLM Hardware Scheduling ‣ EDGE-LLM: Enabling Efficient Large Language
    Model Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive
    Layer Tuning & Voting") (c) 中，我们展示了一个示意性的计算图，包含批次、层和标记三个维度。在图中，每个方块表示特定层的计算。共享相同颜色的方块表示使用相同的层权重。有效路径定义为遍历（即计算）所有方块的轨迹，符合以下约束：'
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: During LLM forwarding or backpropagation, a square’s computation depends on
    the left or right layers in its row being completed, respectively.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 LLM 正向传播或反向传播过程中，一个方块的计算依赖于其行中的左侧或右侧层的完成情况。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To compute a square, all its inputs (weights, activations, cache) must be loaded
    onto the on-chip SRAM.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要计算一个方块，所有输入（权重、激活、缓存）必须被加载到片上 SRAM 中。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: At any given time, the cumulative size of tensors stored on an accelerator must
    not exceed its memory capacity.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在任何给定时间，存储在加速器上的张量总大小不得超过其内存容量。
- en: The objective is to identify a valid path that minimizes the overall execution
    time, encompassing both compute costs and I/O costs incurred during the movement
    of tensors.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是识别一个有效路径，以最小化整体执行时间，包括在张量移动过程中产生的计算成本和 I/O 成本。
- en: 4.3\. Block Search Space
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 区块搜索空间
- en: Building upon the aforementioned search objective, we establish a search space
    encompassing potential valid strategies.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述搜索目标，我们建立了一个涵盖潜在有效策略的搜索空间。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Row-by-row. Existing systems often use solely row-by-row traversal for the activation
    footprint savings. However, this strategy does not consider the weight sharing
    between adjacent squares among different bathes, leading to repetitive weight
    loading I/O costs.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行逐行。现有系统通常仅使用逐行遍历来节省激活足迹。然而，这种策略并未考虑不同批次之间相邻方块的权重共享，导致重复的权重加载 I/O 成本。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mixed column-by-column and row-by-row. Alternatively, to reduce I/O costs related
    to weights, an approach involves traversing the graph column-by-column. This leverages
    weight sharing among all squares in a column, allowing DRAM preservation for reuse,
    with activations being loaded and unloaded. As our proposed algorithm techniques
    can greatly reduce the activation memory footprint requirement, we include mixed
    column-by-column and row-by-row in search space.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 混合列逐列和行逐行。或者，为了减少与权重相关的 I/O 成本，可以采用逐列遍历图的方法。这利用了列中所有方块之间的权重共享，从而允许 DRAM 保持以供重用，同时加载和卸载激活。由于我们提出的算法技术可以大大减少激活内存占用，因此我们在搜索空间中包括了混合列逐列和行逐行的策略。
- en: Considerations. Overlapping. Another optimization is overlapping. This entails
    concurrently handling a load of weights for the next layer, the load of activations
    for the subsequent batch, the storage of activations from the preceding batch,
    and the computation of the current batch. The integration of overlapping into
    the block schedule is necessary for delivering the final scheduling.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑事项。重叠。另一种优化是重叠。这涉及同时处理下一层的权重加载、后续批次的激活加载、前一批次的激活存储以及当前批次的计算。将重叠整合到区块调度中是实现最终调度的必要步骤。
- en: Tensor Placement. In addition to the computation schedule, an effective strategy
    must delineate the placement of tensors within the memory hierarchy. Three variables,
    namely $w_{sram}$ articulate the percentages of gradients.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 张量位置。除了计算调度外，一个有效的策略还必须 delineate 张量在内存层次结构中的位置。三个变量，即 $w_{sram}$ 描述了梯度的百分比。
- en: 4.4\. Cost Models
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 成本模型
- en: Having established the search objective and the search space, the next step
    is the development of an analytical cost model. This model serves the purpose
    of estimating the execution time based on the specified algorithm parameters and
    hardware specifications. The total latency for computing a block can be estimated
    as $T_{\text{dec}}$ can be estimated as
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了搜索目标和搜索空间之后，下一步是制定一个分析成本模型。该模型旨在根据指定的算法参数和硬件规格估计执行时间。计算一个区块的总延迟可以估算为 $T_{\text{dec}}$。
- en: '| (3) |  | $\vspace{-0.2em}T_{\text{dec}}=\max(r_{\text{to\_sram}},w_{\text{to\_dram}},r_{\text{to\_dram}},w_{\text{to\_ssd}},T_{\text{comp}})\vspace{-0.2em}$
    |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\vspace{-0.2em}T_{\text{dec}}=\max(r_{\text{to\_sram}},w_{\text{to\_dram}},r_{\text{to\_dram}},w_{\text{to\_ssd}},T_{\text{comp}})\vspace{-0.2em}$
    |  |'
- en: where $r_{\text{to\_sram}}$ denote the latency of read from DRAM to SRAM, write
    from SRAM to DRAM, read from SSD to DRAM, write from DRAM to SSD, and computation,
    respectively, during LLM tuning.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r_{\text{to\_sram}}$ 表示在 LLM 调优过程中，从 DRAM 到 SRAM 的读取延迟、从 SRAM 到 DRAM 的写入延迟、从
    SSD 到 DRAM 的读取延迟、从 DRAM 到 SSD 的写入延迟以及计算延迟。
- en: 5\. Evaluation
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 评估
- en: 5.1\. Evaluation Setup
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 评估设置
- en: 'Datasets: Two commonly used benchmarking dataset including MMLU (Hendrycks
    et al., [2020](#bib.bib10)) and WikiText (Merity et al., [2016](#bib.bib13)).
    Model: LLaMA-7B (Touvron et al., [2023](#bib.bib23)). Algorithm baselines: The
    SOTA PET technique, LoRA (Hu et al., [2021](#bib.bib11)); the SOTA MET technique,
    LST (Sung et al., [2022](#bib.bib20)); the SOTA compression techniques, Sparse-GPT (Frantar
    et al., [2023](#bib.bib8)) and LLM-QAT (Liu et al., [2023](#bib.bib12)); and seven
    variants of our proposed methods. Hardware baselines: The SOTA systolic accelerator (Shao
    et al., [2023](#bib.bib18)) dedicated for transformer training. Algorithm implementation:
    We use LLM-QAT and Sparse-GPT as the quantization and pruning techniques, respectively,
    and tune the model following the settings in (Dettmers et al., [2023](#bib.bib3)).
    Hardware configuration: The accelerator’s DRAM is set to 8GB LPDDR4 and on-chip
    SRAM to be 1MB, in line with SOTA edge devices (NVIDIA, [2020](#bib.bib15)), with
    other hardware configurations following the baseline training accelerator design.
    Evaluation methodology: We use the SOTA Scale-Sim (Samajdar et al, [2023](#bib.bib17))
    simulator to simulate both the baseline accelerator and those after applying our
    techniques on the baseline accelerator.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集：两个常用的基准数据集，包括 MMLU（Hendrycks 等， [2020](#bib.bib10)）和 WikiText（Merity 等，
    [2016](#bib.bib13)）。模型：LLaMA-7B（Touvron 等， [2023](#bib.bib23)）。算法基线：SOTA PET 技术，LoRA（Hu
    等， [2021](#bib.bib11)）；SOTA MET 技术，LST（Sung 等， [2022](#bib.bib20)）；SOTA 压缩技术，Sparse-GPT（Frantar
    等， [2023](#bib.bib8)）和 LLM-QAT（Liu 等， [2023](#bib.bib12)）；以及我们提出的七种变体。硬件基线：SOTA
    系统加速器（Shao 等， [2023](#bib.bib18)），专门用于变换器训练。算法实现：我们使用 LLM-QAT 和 Sparse-GPT 作为量化和修剪技术，分别调整模型，按照（Dettmers
    等， [2023](#bib.bib3)）中的设置。硬件配置：加速器的 DRAM 设置为 8GB LPDDR4，片上 SRAM 设置为 1MB，符合 SOTA
    边缘设备（NVIDIA， [2020](#bib.bib15)），其他硬件配置遵循基线训练加速器设计。评估方法：我们使用 SOTA Scale-Sim（Samajdar
    等， [2023](#bib.bib17)）模拟器来模拟基线加速器以及在基线加速器上应用我们技术后的情况。
- en: 5.2\. Algorithm Evaluation
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 算法评估
- en: 'To evaluate the performance of our proposed method, we first benchmark our
    proposed method with existing baseline methods including partial tuning, LST and
    LoRA tuning on the commonly used MMLU dataset. As shown in Table [1](#S5.T1 "Table
    1 ‣ 5.2\. Algorithm Evaluation ‣ 5\. Evaluation ‣ EDGE-LLM: Enabling Efficient
    Large Language Model Adaptation on Edge Devices via Layerwise Unified Compression
    and Adaptive Layer Tuning & Voting"), our method consistently achieves a 0.70%$\sim$1.68
    lower perplexity compared to the Random baseline under the same efficiency, showing
    the effectiveness of our proposed LUC.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估我们提出的方法的性能，我们首先将我们提出的方法与包括部分调整、LST 和 LoRA 调整在内的现有基线方法在常用的 MMLU 数据集上进行基准测试。如表
    [1](#S5.T1 "Table 1 ‣ 5.2\. 算法评估 ‣ 5\. 评估 ‣ EDGE-LLM: 通过逐层统一压缩和自适应层调整与投票实现高效的大型语言模型适应")
    所示，我们的方法在相同效率下始终比随机基线低 0.70%$\sim$1.68 的困惑度，显示了我们提出的 LUC 的有效性。'
- en: Table 1\. Benchmarking Edge-LLM on the MMLU dataset.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 在 MMLU 数据集上基准测试 Edge-LLM。
- en: '| Method | Avg. Bit | Sparsity | Norm. Mem. | MMLU |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 平均位数 | 稀疏度 | 规范化内存 | MMLU |'
- en: '| LoRA | 8.0 | 0% | 1.00$\times$ | 33.60 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 8.0 | 0% | 1.00$\times$ | 33.60 |'
- en: '| Partial Tuning | 5.0 | 50% | 0.25$\times$ | 30.94 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 部分调整 | 5.0 | 50% | 0.25$\times$ | 30.94 |'
- en: '| Ours | 5.1 | 50% | 0.25$\times$ | 31.64 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 5.1 | 50% | 0.25$\times$ | 31.64 |'
- en: '| LST | 4.0 | 0% | 0.29$\times$ | 29.04 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| LST | 4.0 | 0% | 0.29$\times$ | 29.04 |'
- en: '| Partial Tuning | 4.0 | 50% | 0.25$\times$ | 28.70 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 部分调整 | 4.0 | 50% | 0.25$\times$ | 28.70 |'
- en: '| Ours | 4.1 | 50% | 0.25$\times$ | 29.89 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 4.1 | 50% | 0.25$\times$ | 29.89 |'
- en: '| Partial Tuning | 3.0 | 50% | 0.25$\times$ | 26.61 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 部分调整 | 3.0 | 50% | 0.25$\times$ | 26.61 |'
- en: '| Ours | 3.1 | 50% | 0.25$\times$ | 27.68 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 3.1 | 50% | 0.25$\times$ | 27.68 |'
- en: 5.3\. Hardware Evaluation
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 硬件评估
- en: 'We evaluate the proposed techniques based on the baseline systolic accelerator
    designed for transformer training with proper modifications for supporting the
    proposed techniques (Shao et al., [2023](#bib.bib18)): (1) Since the proposed
    adaptive layer tuning can be naturally run on the baseline accelerator, there
    is no need to modify the baseline accelerator; and (2) For the LUC, we make these
    modifications: we update the baseline to store the compressed weights on DRAM
    and SSD. To simplify the design, we do not modify the compute core for sparsity
    and use a simple spatial-temporal flexible-precision MAC unit (Fu et al., [2021](#bib.bib9)).
    We apply our proposed hardware scheduling searching method to find the optimal
    algorithm-to-hardware mappings. Scale-Sim simulation results show that the adaptive
    layer tuning can achieve 2.24$\times$ overall speedup, respectively.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于基线收缩加速器评估了所提出的技术，该加速器经过适当修改以支持所提出的技术 (Shao et al., [2023](#bib.bib18))：(1)
    由于所提出的自适应层调整可以自然地在基线加速器上运行，因此无需修改基线加速器；(2) 对于LUC，我们做了如下修改：更新基线以将压缩的权重存储在DRAM和SSD上。为了简化设计，我们不修改用于稀疏性的计算核心，而是使用简单的空间-时间灵活精度MAC单元
    (Fu et al., [2021](#bib.bib9))。我们应用了所提出的硬件调度搜索方法以找到最佳的算法到硬件映射。Scale-Sim模拟结果表明，自适应层调整可以实现2.24$\times$的总体加速。
- en: 6\. Conclusion
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: In this paper, we introduce an LLM tuning framework, Edge-LLM, achieving efficient
    LLM adaptation on edge devices. Experiments demonstrate that Edge-LLM achieves
    efficient adaptation with comparable performance as vanilla tuning with a 2.92$\times$
    memory reduction.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一个LLM调优框架，Edge-LLM，实现了在边缘设备上高效的LLM适应。实验表明，Edge-LLM在内存减少2.92$\times$的情况下，实现了与常规调优相当的性能。
- en: Table 2\. Ablation on LUC’s performance with its variants
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. LUC及其变体性能的消融实验
- en: '| Method | Avg. Bit | Sparsity | Perplexity |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 平均位数 | 稀疏性 | 困惑度 |'
- en: '| SparseGPT | 8.0 | 50% | 15.88 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 8.0 | 50% | 15.88 |'
- en: '| LLM-QAT | 8.0 | 0% | 13.34 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QAT | 8.0 | 0% | 13.34 |'
- en: '| Uniform | 5.0 | 50% | 17.61 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 5.0 | 50% | 17.61 |'
- en: '| Random | 5.1 | 50% | 16.21 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Random | 5.1 | 50% | 16.21 |'
- en: '| Ours | 5.1 | 50% | 15.71 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 5.1 | 50% | 15.71 |'
- en: '| Uniform | 4.0 | 50% | 19.86 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 4.0 | 50% | 19.86 |'
- en: '| Random | 4.1 | 50% | 19.81 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Random | 4.1 | 50% | 19.81 |'
- en: '| Ours | 4.1 | 50% | 18.58 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 4.1 | 50% | 18.58 |'
- en: '| Uniform | 3.0 | 50% | 32.52 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 3.0 | 50% | 32.52 |'
- en: '| Random | 3.1 | 50% | 31.71 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Random | 3.1 | 50% | 31.71 |'
- en: '| Ours | 3.1 | 50% | 30.03 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 3.1 | 50% | 30.03 |'
- en: Acknowledgement
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'This work was supported in part by CoCoSys, one of the seven centers in JUMP
    2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA, and
    the National Science Foundation (NSF) through the NSF CAREER funding (Award number:
    2048183).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分由CoCoSys资助，CoCoSys是JUMP 2.0中的七个中心之一，这是一个由DARPA赞助的半导体研究公司(SRC)项目，以及由国家科学基金会(NSF)通过NSF
    CAREER资助(奖号：2048183)。
- en: References
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Bubeck et al. (2023) Bubeck et al. 2023. Sparks of artificial general intelligence:
    Early experiments with gpt-4. *arXiv preprint arXiv:2303.12712* (2023).'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck et al. (2023) Bubeck et al. 2023. 人工通用智能的火花：GPT-4的早期实验。*arXiv预印本 arXiv:2303.12712*
    (2023)。
- en: 'Dettmers et al. (2023) Dettmers et al. 2023. Qlora: Efficient finetuning of
    quantized llms. *arXiv* (2023).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2023) Dettmers et al. 2023. Qlora: 高效微调量化LLMs。*arXiv* (2023)。'
- en: et al (2024) Kim et al. 2024. Memory-efficient fine-tuning of compressed large
    language models via sub-4-bit integer quantization. *NeurIPS* 36 (2024).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: et al (2024) Kim et al. 2024. 通过4位以下整数量化实现压缩大型语言模型的内存高效微调。*NeurIPS* 36 (2024)。
- en: et al (2022) Yu et al. 2022. Unified visual transformer compression. *arXiv
    preprint arXiv:2203.08243* (2022).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: et al (2022) Yu et al. 2022. 统一视觉变换器压缩。*arXiv预印本 arXiv:2203.08243* (2022)。
- en: 'et al (2023a) Yu et al. 2023a. Hint-aug: Drawing hints from foundation vision
    transformers towards boosted few-shot parameter-efficient tuning. In *CVPR*. 11102–11112.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'et al (2023a) Yu et al. 2023a. Hint-aug: 从基础视觉变换器中提取提示以提升少量参数高效调优。见*CVPR*。11102–11112。'
- en: 'et al (2023b) Yu et al. 2023b. Master-ASR: achieving multilingual scalability
    and low-resource adaptation in ASR with modular learning. In *ICML*. PMLR, 40475–40487.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'et al (2023b) Yu et al. 2023b. Master-ASR: 在ASR中实现多语言扩展性和低资源适应的模块化学习。见*ICML*。PMLR,
    40475–40487。'
- en: 'Frantar et al. (2023) Frantar et al. 2023. SparseGPT: Massive Language Models
    Can Be Accurately Pruned in One-Shot. (2023).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. (2023) Frantar et al. 2023. SparseGPT: 大型语言模型可以在一次性剪枝中准确修剪。(2023)。'
- en: Fu et al. (2021) Fu et al. 2021. Enabling random precision switch for winning
    both adversarial robustness and efficiency. In *MICRO*. 225–237.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 (2021) Fu 等 2021. 启用随机精度切换以赢得对抗性鲁棒性和效率。见 *MICRO*。225–237。
- en: Hendrycks et al. (2020) Hendrycks et al. 2020. Measuring massive multitask language
    understanding. *arXiv preprint arXiv:2009.03300* (2020).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 (2020) Hendrycks 等 2020. 测量大规模多任务语言理解。*arXiv 预印本 arXiv:2009.03300*
    (2020)。
- en: 'Hu et al. (2021) Hu et al. 2021. Lora: Low-rank adaptation of large language
    models. *arXiv preprint arXiv:2106.09685* (2021).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等 (2021) Hu 等 2021. Lora: 大语言模型的低秩适配。*arXiv 预印本 arXiv:2106.09685* (2021)。'
- en: 'Liu et al. (2023) Liu et al. 2023. LLM-QAT: Data-Free Quantization Aware Training
    for Large Language Models. *arXiv* (2023).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2023) Liu 等 2023. LLM-QAT: 大语言模型的无数据量化感知训练。*arXiv* (2023)。'
- en: Merity et al. (2016) Merity et al. 2016. Pointer sentinel mixture models. *arXiv
    preprint arXiv:1609.07843* (2016).
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等 (2016) Merity 等 2016. 指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843* (2016)。
- en: Meta (2022) Meta. 2022. Quest Pro. [https://www.meta.com/quest/quest-pro/](https://www.meta.com/quest/quest-pro/).
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta (2022) Meta. 2022. Quest Pro. [https://www.meta.com/quest/quest-pro/](https://www.meta.com/quest/quest-pro/)。
- en: NVIDIA (2020) NVIDIA. 2020. NVIDIA Jetson TX2. [www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-tx2/](www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-tx2/).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA (2020) NVIDIA. 2020. NVIDIA Jetson TX2. [www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-tx2/](www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-tx2/)。
- en: Pearce et al. (2021) Pearce et al. 2021. Understanding softmax confidence and
    uncertainty. *arXiv preprint arXiv:2106.04972* (2021).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearce 等 (2021) Pearce 等 2021. 理解 softmax 信心度和不确定性。*arXiv 预印本 arXiv:2106.04972*
    (2021)。
- en: Samajdar et al (2023) Samajdar et al. 2023. Systolic CNN AcceLErator Simulator
    (SCALE Sim). [https://github.com/ARM-software/SCALE-Sim](https://github.com/ARM-software/SCALE-Sim).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samajdar 等 (2023) Samajdar 等 2023. Systolic CNN AcceLErator Simulator (SCALE
    Sim)。 [https://github.com/ARM-software/SCALE-Sim](https://github.com/ARM-software/SCALE-Sim)。
- en: Shao et al. (2023) Shao et al. 2023. An Efficient Training Accelerator for Transformers
    With Hardware-Algorithm Co-Optimization. *VLSI* (2023).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等 (2023) Shao 等 2023. 一种针对变换器的高效训练加速器，结合了硬件-算法协同优化。*VLSI* (2023)。
- en: 'Sheng et al. (2023) Sheng et al. 2023. FlexGen: High-Throughput Generative
    Inference of Large Language Models with a Single GPU. (2023).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sheng 等 (2023) Sheng 等 2023. FlexGen: 使用单个 GPU 进行大语言模型的高吞吐量生成推理。 (2023)。'
- en: 'Sung et al. (2022) Sung et al. 2022. Lst: Ladder side-tuning for parameter
    and memory efficient transfer learning. *NeurIPS* 35 (2022), 12991–13005.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sung 等 (2022) Sung 等 2022. Lst: 参数和内存高效转移学习的梯级侧调整。*NeurIPS* 35 (2022)，12991–13005。'
- en: 'Taori et al. (2023) Taori et al. 2023. Stanford alpaca: An instruction-following
    llama model.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori 等 (2023) Taori 等 2023. Stanford alpaca: 一个遵循指令的 llama 模型。'
- en: 'Teerapittayanon et al. (2016) Teerapittayanon et al. 2016. Branchynet: Fast
    inference via early exiting from deep neural networks. In *ICPR*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Teerapittayanon 等 (2016) Teerapittayanon 等 2016. Branchynet: 通过从深度神经网络早期退出实现快速推理。见
    *ICPR*。'
- en: 'Touvron et al. (2023) Touvron et al. 2023. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 (2023) Touvron 等 2023. Llama: 开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*
    (2023)。'
- en: 'Zhang et al. (2023) Zhang et al. 2023. Llama-adapter: Efficient fine-tuning
    of language models with zero-init attention. *arXiv* (2023).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 (2023) Zhang 等 2023. Llama-adapter: 使用零初始化注意力的语言模型高效微调。*arXiv* (2023)。'
