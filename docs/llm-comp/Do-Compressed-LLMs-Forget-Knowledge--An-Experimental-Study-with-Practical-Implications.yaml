- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:03:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:03:53
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 压缩的LLMs会遗忘知识吗？一个具有实际意义的实验研究
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.00867](https://ar5iv.labs.arxiv.org/html/2310.00867)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.00867](https://ar5iv.labs.arxiv.org/html/2310.00867)
- en: Duc Hoang    Minsik Cho    Thomas Merth    Mohammad Rastegari    Zhangyang Wang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Duc Hoang    Minsik Cho    Thomas Merth    Mohammad Rastegari    Zhangyang Wang
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Compressing Large Language Models (LLMs) often leads to reduced performance,
    especially for knowledge-intensive tasks. In this work, we dive into how compression
    damages LLMs’ inherent knowledge and the possible remedies. We start by proposing
    two conjectures on the nature of the damage: one is certain knowledge being forgotten
    (or erased) after LLM compression, hence necessitating the compressed model to
    (re)learn from data with additional parameters; the other presumes that knowledge
    is internally displaced and hence one requires merely “inference re-direction”
    with input-side augmentation such as prompting, to recover the knowledge-related
    performance. Extensive experiments are then designed to (in)validate the two conjectures.
    We observe the promise of prompting in comparison to model tuning; we further
    unlock prompting’s potential by introducing a variant called Inference-time Dynamic
    Prompting (IDP), that can effectively increase prompt diversity without incurring
    any inference overhead. Our experiments consistently suggest that compared to
    the classical re-training alternatives such as LoRA, prompting with IDP leads
    to better or comparable post-compression performance recovery, while saving the
    extra parameter size by $\mathbf{21\times}$ and reducing inference latency by
    60%. Our experiments hence strongly endorse the conjecture of “knowledge displaced”
    over “knowledge forgotten”, and shed light on a new efficient mechanism to restore
    compressed LLM performance. We additionally visualize and analyze the different
    attention and activation patterns between prompted and re-trained models, demonstrating
    they achieve performance recovery in two different regimes.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩大型语言模型（LLMs）常常导致性能下降，特别是在知识密集型任务中。在这项工作中，我们深入探讨了压缩如何损害LLMs固有的知识以及可能的补救措施。我们首先提出了两种关于损害性质的猜测：一种是压缩后确切知识被遗忘（或抹去），因此压缩后的模型需要从数据中（重新）学习并附加额外参数；另一种假设知识在内部被转移，因此仅需通过输入侧增强（如提示）来“推理重定向”，以恢复知识相关的性能。接着，我们设计了大量实验来（不）验证这两种猜测。我们观察到相较于模型调优，提示具有潜力；我们进一步通过引入一个变体——推理时动态提示（IDP），有效增加了提示的多样性而不增加推理开销。我们的实验一致表明，与传统的重新训练替代方法如LoRA相比，使用IDP的提示能更好或相当地恢复压缩后的性能，同时节省了$\mathbf{21\times}$的额外参数规模并将推理延迟减少了60%。因此，我们的实验强烈支持“知识转移”而非“知识遗忘”的猜测，并揭示了一种恢复压缩LLM性能的新高效机制。我们还可视化和分析了提示模型与重新训练模型之间的不同注意力和激活模式，展示了它们在两种不同的机制下实现性能恢复。
- en: Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) like GPT-4 and ChatGPT have emerged as powerful
    tools in language generation and reasoning, pushing the boundaries of AI to rival
    human-like capabilities (OpenAI, [2023](#bib.bib16)). These advancements, however,
    are accompanied by significant challenges, primarily their massive size and the
    consequent high computational costs (Chen et al., [2023](#bib.bib3)). This has
    led to a growing emphasis on model compression as a means to make LLMs more accessible
    and efficient for broader industrial applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）如GPT-4和ChatGPT已成为语言生成和推理中的强大工具，推动了AI技术向人类能力的边界发展（OpenAI, [2023](#bib.bib16)）。然而，这些进展伴随着显著的挑战，主要是其庞大的规模和随之而来的高计算成本（Chen
    et al., [2023](#bib.bib3)）。这导致了对模型压缩的日益关注，以便使LLMs在更广泛的工业应用中更具可访问性和效率。
- en: Model compression techniques, such as quantization and sparsification, have
    since become increasingly popular for reducing the size of LLMs without significantly
    compromising their performance. Traditional approaches often involve post-compression
    re-training to mitigate performance losses (Han et al., [2015](#bib.bib7)). More
    recent ‘training-free’ compression methods, like GPTQ (Frantar et al., [2022](#bib.bib6))
    and SparseGPT (Frantar & Alistarh, [2023](#bib.bib5)), promise minimal impact
    on perplexity and standard task benchmarks. Nevertheless, studies like Jaiswal
    et al. ([2023](#bib.bib11)) reveal that these compressed models still suffer from
    reduced effectiveness in knowledge-intensive language generation or reasoning
    tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩技术，如量化和稀疏化，已经越来越受欢迎，它们可以在不显著影响性能的情况下减少大规模语言模型（LLMs）的大小。传统的方法通常涉及压缩后的再训练，以减轻性能损失（Han
    et al., [2015](#bib.bib7)）。更近期的‘无训练’压缩方法，如GPTQ（Frantar et al., [2022](#bib.bib6)）和SparseGPT（Frantar
    & Alistarh, [2023](#bib.bib5)），承诺对困惑度和标准任务基准的影响最小。然而，Jaiswal et al.（[2023](#bib.bib11)）的研究表明，这些压缩模型在知识密集型的语言生成或推理任务中仍然表现出有效性降低的问题。
- en: 'What transpires within a compressed model that leads to diminished performance
    on tasks demanding extensive knowledge? Is this knowledge permanently lost in
    the compression process, or is it merely obscured? Addressing these questions
    is not solely of theoretical interest; it has tangible implications for devising
    strategies to effectively counteract the impacts of compression on model knowledge.
    To this end, we introduce two hypotheses regarding the root cause of this performance
    degradation: the first posits that key knowledge is forgotten (or erased) as a
    consequence of LLM compression, necessitating a re-learning process with the addition
    of extra parameters (Hu et al., [2021](#bib.bib8)); the second hypothesis suggests
    that the knowledge is merely internally displaced within the LLM. This implies
    that strategic redirection of knowledge flow, potentially through input-side enhancements
    like prompting (Xu et al., [2023](#bib.bib24)), could efficiently recover model
    accuracy. A more comprehensive exploration of these ideas is presented in Section
    2.2.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在压缩模型中发生了什么导致在要求大量知识的任务中性能下降？这种知识在压缩过程中是否永久丧失，还是仅仅被掩盖了？解决这些问题不仅具有理论意义，还对制定有效应对模型知识压缩影响的策略具有实际意义。为此，我们提出了关于这种性能下降根本原因的两个假设：第一个假设认为关键知识在LLM压缩过程中被遗忘（或删除），需要通过增加额外参数（Hu
    et al., [2021](#bib.bib8)）重新学习；第二个假设则认为知识仅在LLM内部移位。这意味着通过输入侧的增强，如提示（Xu et al.,
    [2023](#bib.bib24)），可能会有效恢复模型准确性。对这些想法的更全面探索见第2.2节。
- en: 'We embarked on a series of extensive experiments to test two central hypotheses
    of compressed LLM performance loss: “knowledge displaced” versus “knowledge forgotten”.
    To effectively validate these hypotheses, we scrutinized existing prompt-tuning
    methods and recognized their limitations, particularly the reliance on a single
    prompt across varied input formats and knowledge domains. This led us to propose
    the Inference-time Dynamic Prompting (IDP) approach to auto-select appropriate
    prompts per input. IDP sets itself apart from previous ensemble techniques (Raffel
    et al., [2020](#bib.bib19); PENG et al., [2023](#bib.bib18)), offering a one-shot
    selection process with nearly no overhead compared to one fixed prompt, thanks
    to Key-Value caching.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开展了一系列广泛的实验，以测试压缩LLM性能下降的两个核心假设：“知识移位”与“知识遗忘”。为了有效验证这些假设，我们审视了现有的提示调优方法，并认识到它们的局限性，特别是在不同输入格式和知识领域中依赖单一提示。这促使我们提出了推理时动态提示（IDP）方法，以自动选择每个输入的合适提示。IDP与以往的集成技术（Raffel
    et al., [2020](#bib.bib19)；PENG et al., [2023](#bib.bib18)）不同，通过键值缓存实现了一次性选择过程，与固定提示相比几乎没有额外开销。
- en: Our empirical findings strongly support the hypothesis of “knowledge displaced”
    over “knowledge forgotten” by demonstrating prompting by IDP leads to more favorable
    cost-effectiveness in performance recovery. As illustrated in Figure [4](#S3.F4
    "Figure 4 ‣ 3.3 IDP is remarkably more efficient ‣ 3 Methods and Experiments ‣
    Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications"),
    compared to classical model re-training methods using LoRA, IDP achieved comparable
    or superior performance in adapting compressed LLM models across various tasks,
    while attaining a remarkable reduction in parameter overhead – up to 21 times,
    besides reducing inference latency by 60%. The performance of IDP is robust even
    at fairly short prompt lengths (Figure [6](#S4.F6 "Figure 6 ‣ 4.1 Evaluating Knowledge
    Forgetfulness and Displacement ‣ 4 More Studies ‣ Do Compressed LLMs Forget Knowledge?
    An Experimental Study with Practical Implications")). Our investigation into layer-wise
    cosine similarity (Figure [5](#S3.F5 "Figure 5 ‣ 3.3 IDP is remarkably more efficient
    ‣ 3 Methods and Experiments ‣ Do Compressed LLMs Forget Knowledge? An Experimental
    Study with Practical Implications")) further revealed that, compared to baseline
    attention patterns, prompt-tuning via IDP leads to significant divergences, whereas
    re-trained models tend to align more closely with the baseline, despite achieving
    similar outcomes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实证研究强烈支持“知识位移”优于“知识遗忘”的假设，通过展示IDP的提示使性能恢复在成本效益上更加有利。如图[4](#S3.F4 "Figure
    4 ‣ 3.3 IDP is remarkably more efficient ‣ 3 Methods and Experiments ‣ Do Compressed
    LLMs Forget Knowledge? An Experimental Study with Practical Implications")所示，与使用LoRA的经典模型再训练方法相比，IDP在适应压缩LLM模型的各种任务中实现了可比或更优的性能，同时在参数开销上显著减少——最多可减少21倍，并将推理延迟降低了60%。即使在相当短的提示长度下，IDP的性能也很稳定（图[6](#S4.F6
    "Figure 6 ‣ 4.1 Evaluating Knowledge Forgetfulness and Displacement ‣ 4 More Studies
    ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications")）。我们对逐层余弦相似性的调查（图[5](#S3.F5
    "Figure 5 ‣ 3.3 IDP is remarkably more efficient ‣ 3 Methods and Experiments ‣
    Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications")）进一步揭示，与基准注意力模式相比，通过IDP进行的提示调整会导致显著的差异，而再训练的模型则趋向于与基准对齐，尽管取得了类似的结果。
- en: 'In summary, our contributions are:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们的贡献包括：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We critically examine the impact of compression on LLMs’ knowledge, formally
    raising the conjectures of knowledge ’displacement’ versus ’forgetfulness’.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们批判性地审视了压缩对LLM知识的影响，正式提出了知识“位移”与“遗忘”的推测。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We thoughtfully design experiments to endorse the hypothesis of “knowledge displaced”
    over “knowledge forgotten”. We also reveal a number of insights, including two
    different regimes of performance recovery.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们周密设计了实验以支持“知识位移”优于“知识遗忘”的假设。我们还揭示了一些见解，包括性能恢复的两种不同模式。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: As the practical implication, all experimental results collectively underscore
    the efficacy of our newly designed IDP method - achieving similar performance
    recovery to LoRA, at orders-of-magnitude lower parameter and latency overheads,
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实际意义上，所有实验结果共同强调了我们新设计的IDP方法的有效性——在参数和延迟开销上比LoRA低几个数量级，同时实现了相似的性能恢复，
- en: 2 Background and Conjecture
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景和推测
- en: '![Refer to caption](img/270b8530470cb39341c6763a33e94eeb.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/270b8530470cb39341c6763a33e94eeb.png)'
- en: 'Figure 1: This figure presents a comparative analysis of the performance of
    compressed models using GPTQ for quantization and SparseGPT for pruning. The models
    were compressed leveraging either C4 or Wikitext datasets. Their average performance
    is depicted across a spectrum of nine tasks, each representing diverse knowledge
    domains.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：该图展示了使用GPTQ进行量化和SparseGPT进行剪枝的压缩模型性能的比较分析。这些模型在压缩时利用了C4或Wikitext数据集。它们在九项任务中的平均性能被描绘出来，每项任务代表不同的知识领域。
- en: 2.1 LLM compression background & caveats
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM 压缩背景及注意事项
- en: 'In compression, we address the challenges of size and latency inherent to LLMs
    by targeting the model’s parameters. Broadly, compressive techniques are grouped
    into two main categories: compression-aware training and post-training compression.
    Post-training compression holds particular appeal for exceptionally large models
    where the costs associated with full model training or even fine-tuning can be
    prohibitive. Given its relevance, we narrow our discussion to this category.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在压缩中，我们通过针对模型的参数来解决 LLMs 固有的大小和延迟问题。广义上，压缩技术分为两大类：压缩感知训练和训练后压缩。训练后压缩对于那些成本高昂的大型模型尤其具有吸引力，因为完全模型训练或甚至微调的费用可能过于高昂。鉴于其相关性，我们将讨论范围缩小到这一类别。
- en: Firstly, quantization refers to reducing the model’s footprint by decreasing
    the bit precision of its weights (Frantar et al., [2022](#bib.bib6); Yao et al.,
    [2022](#bib.bib25); Xiao et al., [2022](#bib.bib23)). Quantization not only shrinks
    the model’s size but also accelerates inference, as operations over lower-precision
    weights are computationally less demanding. Secondly, sparsification, often referred
    to as pruning, revolves around the concept of selectively removing certain weight
    elements or masking activation values (Frantar & Alistarh, [2023](#bib.bib5);
    Hubara et al., [2021a](#bib.bib9), [b](#bib.bib10)). The objective is to trim
    the less salient portions of the model, thereby reducing computational overhead
    or enhancing model throughput.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，量化指的是通过降低权重的比特精度来减少模型的占用空间（Frantar et al., [2022](#bib.bib6); Yao et al.,
    [2022](#bib.bib25); Xiao et al., [2022](#bib.bib23)）。量化不仅缩小了模型的大小，还加快了推理速度，因为对低精度权重的操作在计算上要求较低。其次，稀疏化，通常称为剪枝，围绕选择性地移除某些权重元素或屏蔽激活值的概念（Frantar
    & Alistarh, [2023](#bib.bib5); Hubara et al., [2021a](#bib.bib9), [b](#bib.bib10)）。其目标是修剪模型中较不重要的部分，从而减少计算开销或提高模型吞吐量。
- en: Using GPTQ and SparseGPT for model compression, Figure [1](#S2.F1 "Figure 1
    ‣ 2 Background and Conjecture ‣ Do Compressed LLMs Forget Knowledge? An Experimental
    Study with Practical Implications") shows a performance drop when lowering bit
    counts or parameters, except for the int8 quantization. This trend aligns with
    the claims by Frantar et al. ([2022](#bib.bib6)) and Frantar & Alistarh ([2023](#bib.bib5))
    that their methods are optimized for the largest LLMs. This evident limitation
    on smaller (yet still substantial) LLMs highlights the imperative for additional
    performance improvement post-compression beyond just parameter adjustment.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GPTQ 和 SparseGPT 进行模型压缩时，图 [1](#S2.F1 "Figure 1 ‣ 2 Background and Conjecture
    ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications")
    显示，在降低比特数或参数时，性能会下降，除了 int8 量化。这一趋势与 Frantar et al. ([2022](#bib.bib6)) 和 Frantar
    & Alistarh ([2023](#bib.bib5)) 的声明一致，即他们的方法针对的是最大的 LLMs。这种在较小（但仍然重要）LLMs 上的明显限制突显了在压缩后，除了调整参数外，进一步提高性能的必要性。
- en: 2.2 Forgotten, or Displaced? A Two-Way Argument
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 被遗忘，还是被转移？双向辩论
- en: Investigating whether knowledge in compressed models is forgotten or merely
    displaced presents a complex challenge. However, discerning between these two
    scenarios is feasible by examining the nature of intervention required to reinstate
    the model’s performance in downstream tasks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 研究在压缩模型中知识是被遗忘还是仅仅被转移是一个复杂的挑战。然而，通过检查恢复模型在下游任务中性能所需的干预性质，可以区分这两种情况。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Forgetfulness implies that the compression process irrevocably eliminates certain
    knowledge. Integrating an external knowledge source becomes essential to recuperate
    performance, as this process essentially replenishes the lost information.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 忘记意味着压缩过程不可逆地消除了某些知识。整合外部知识来源变得至关重要，因为这一过程本质上是补充丢失的信息。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Displacement posits that the inherent knowledge within these models is not irrevocably
    erased but instead shifted internally, leading to the inefficacy of the established
    inference pathways. In this context, input-side augmentation or instructions are
    needed to “redirect” the internal self-attention. This enables the re-engagement
    of the pre-existing, albeit repositioned, knowledge in the compressed LLM, thereby
    aiding in the recuperation of its performance.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转移理论认为，这些模型内在的知识并没有被不可逆地擦除，而是被内部转移，从而导致既有推理路径的无效。在这种情况下，需要对输入端进行增强或提供指令，以“重新引导”内部自注意力。这使得重新调动压缩
    LLM 中的预存在但重新定位的知识成为可能，从而有助于恢复其性能。
- en: We position LoRA (Hu et al., [2021](#bib.bib8)) and prompting to correlate respectively
    with our hypothesis on “knowledge forgotten” and “knowledge displaced.” LoRA tackles
    “forgetfulness” by fundamentally altering the model’s structure, specifically
    the weights in the self-attention and feedforward neural network (FFN) layers,
    thereby reintegrating knowledge lost due to compression. Prompting, in contrast,
    operates by subtly influencing the self-attention mechanism without changing the
    underlying weights, thus redirecting the model’s existing but less accessible
    knowledge.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将LoRA（胡等人，[2021](#bib.bib8)）和提示与我们关于“知识遗忘”和“知识替换”的假设分别相关联。LoRA通过从根本上改变模型的结构，特别是自注意力和前馈神经网络（FFN）层中的权重，从而重新整合由于压缩而丧失的知识，来应对“遗忘”。而提示则通过微妙地影响自注意力机制而不改变基础权重，从而重新引导模型中现有但不易获取的知识。
- en: 3 Methods and Experiments
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法与实验
- en: 3.1 From Basic Prompting to IDP
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 从基础提示到IDP
- en: Building upon the initial work of Xu et al. ([2023](#bib.bib24)), which utilized
    prompting to enhance the performance of compressed models measured by perplexity,
    we identify a crucial limitation in this approach. As Jaiswal et al. ([2023](#bib.bib11))
    highlights, perplexity alone may not fully capture a model’s actual behavior in
    compression scenarios. To (in)validate this finding, we contrast the perplexity
    metric with the aggregated accuracy performance across nine downstream tasks using
    varying prompt lengths, aiming to discern any discrepancies between these two
    metrics. Our findings, illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 From Basic
    Prompting to IDP ‣ 3 Methods and Experiments ‣ Do Compressed LLMs Forget Knowledge?
    An Experimental Study with Practical Implications"), reveal a notable perplexity-to-performance
    gap that becomes more pronounced with longer prompts. This divergence not only
    corroborates the observations of Jaiswal et al. ([2023](#bib.bib11)) but also
    underscores the limitations in relying solely on perplexity as a performance indicator,
    as initially proposed by Xu et al. ([2023](#bib.bib24)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在徐等人（[2023](#bib.bib24)）的初步工作基础上，该工作利用提示提升了通过困惑度衡量的压缩模型的性能，我们发现了这种方法的一个关键限制。正如贾斯瓦尔等人（[2023](#bib.bib11)）所强调的，单纯依靠困惑度可能无法完全反映模型在压缩场景中的实际行为。为了（不）验证这一发现，我们将困惑度指标与九个下游任务中不同提示长度下的综合准确性表现进行对比，旨在识别这两种指标之间的任何差异。我们的研究结果，如图[2](#S3.F2
    "图 2 ‣ 3.1 从基础提示到IDP ‣ 3 方法与实验 ‣ 压缩LLM是否遗忘知识？具有实际意义的实验研究")所示，揭示了一个显著的困惑度与性能之间的差距，随着提示长度的增加这一差距变得更加明显。这一分歧不仅验证了贾斯瓦尔等人（[2023](#bib.bib11)）的观察结果，也强调了仅依赖困惑度作为性能指标的局限性，这一点最初由徐等人（[2023](#bib.bib24)）提出。
- en: 'We show that the presumed efficacy of extended prompts is, in fact, compromised
    by their intrinsic rigidity, leading us to a conclusion: further improvement of
    prompting hinges on the precise alignment of the right prompt with the appropriate
    input rather than the elongation of a single prompt. This concept parallels ensemble
    methods but requires a departure from iterative or training-intensive approaches,
    such as those found in Lester et al. ([2021](#bib.bib13)) or PENG et al. ([2023](#bib.bib18)),
    which significantly increase training time and inference costs. To circumvent
    these drawbacks, we introduce Inference-time Dynamic Prompting (IDP). IDP enables
    one-shot input-to-prompt matching with minimal latency increase to inference time.
    This strategy aligns prompts more effectively with inputs and incurs little computational
    overhead, marking a stepped improvement in prompting for compressed model performance
    recovery.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了扩展提示的假定有效性实际上受到其固有僵硬性的影响，得出了一个结论：进一步改进提示的关键在于将正确的提示与适当的输入精准对齐，而不是单纯延长单一提示。这一概念与集成方法相似，但需要摆脱迭代或训练密集型的方法，如Lester等人（[2021](#bib.bib13)）或PENG等人（[2023](#bib.bib18)）所提出的方法，这些方法显著增加了训练时间和推理成本。为了避免这些缺点，我们引入了推理时动态提示（IDP）。IDP能够在几乎不增加推理时间延迟的情况下进行一次性输入到提示的匹配。这一策略更有效地将提示与输入对齐，并且计算开销很小，标志着在压缩模型性能恢复方面提示技术的一个重要进步。
- en: '![Refer to caption](img/669e799367f6fda742dd42cf08b12b92.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/669e799367f6fda742dd42cf08b12b92.png)'
- en: 'Figure 2: Using a 3-bit quantized Llama-7b model fine-tuned on C4 dataset,
    we contrast the average accuracy across nine tasks against its word’s perplexity
    score across various prompt lengths. A longer sequence length improves perplexity
    but does not always sustain better performance.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：使用在C4数据集上微调的3-bit量化Llama-7b模型，我们对比了九个任务中的平均准确率与其单词的困惑度分数在不同提示长度下的表现。较长的序列长度能改善困惑度，但并不总能保持更好的性能。
- en: 3.1.1 IDP Methodology
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 IDP方法论
- en: '![Refer to caption](img/f6fc9154e908eea1a290ff8aeeeec540.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f6fc9154e908eea1a290ff8aeeeec540.png)'
- en: 'Figure 3: This figure underscores the key advantage of inference-time dynamic
    prompting (IDP): its minimalistic yet effective design. By making straightforward
    alterations to the existing weighted sum operation and using the existing attention
    matrix for prompt selection, IDP accomplishes its objectives without incurring
    any additional parameter costs.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：该图强调了推理时动态提示（IDP）的关键优势：其简约而有效的设计。通过对现有加权和操作进行简单的修改，并使用现有的注意力矩阵进行提示选择，IDP在不增加任何额外参数成本的情况下实现了其目标。
- en: In prompt tuning, we introduce an additional token sequence, termed as $P$ as
    their embedding size.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示调优中，我们引入了一个额外的令牌序列，称为$P$，作为它们的嵌入大小。
- en: When we extend to a collection of $m$ as one.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们扩展到一个$m$集合时。
- en: To facilitate Inference-time Dynamic Prompting, we introduce two modifications
    to $A$. In the final phase of the self-attention mechanism, we use an attention
    mask to discard any unintended prompts, ensuring they do not modify the main input
    sequence and improve our inference latency. The process is depicted in Figure
    [3](#S3.F3 "Figure 3 ‣ 3.1.1 IDP Methodology ‣ 3.1 From Basic Prompting to IDP
    ‣ 3 Methods and Experiments ‣ Do Compressed LLMs Forget Knowledge? An Experimental
    Study with Practical Implications").
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于推理时动态提示，我们对$A$进行了两个修改。在自注意力机制的最终阶段，我们使用注意力掩码丢弃任何非预期的提示，确保它们不会修改主要输入序列，并提高我们的推理延迟。该过程如图[3](#S3.F3
    "图3 ‣ 3.1.1 IDP方法论 ‣ 3.1 从基础提示到IDP ‣ 3 方法和实验 ‣ 压缩LLM是否会遗忘知识？具有实际意义的实验研究")所示。
- en: '3.2 Experimental Comparison: IDP recovers performance better or comparable
    than LoRA'
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 实验比较：IDP的性能恢复优于或可与LoRA媲美
- en: 3.2.1 Settings
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 设置
- en: Compressed Models
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 压缩模型
- en: We utilize OPT-6.7b (Zhang et al., [2022](#bib.bib27)) and Llama-7b (Touvron
    et al., [2023](#bib.bib21)) as foundational models, both featuring an embedding
    size “e” of 4096\. For compression, we apply GPTQ (Frantar et al., [2022](#bib.bib6))
    and SparseGPT (Frantar & Alistarh, [2023](#bib.bib5)) to achieve 3-bit quantization
    and 50% pruning, respectively. In our discussion, we will primarily focus on the
    quantization approach, as the pruning process exhibits a very similar pattern.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用了OPT-6.7b（Zhang等，[2022](#bib.bib27)）和Llama-7b（Touvron等，[2023](#bib.bib21)）作为基础模型，这两者的嵌入大小“e”均为4096。为了压缩，我们应用了GPTQ（Frantar等，[2022](#bib.bib6)）和SparseGPT（Frantar
    & Alistarh，[2023](#bib.bib5)）来分别实现3-bit量化和50%的剪枝。在我们的讨论中，我们将主要关注量化方法，因为剪枝过程表现出非常相似的模式。
- en: Find-tuning Dataset
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 查找调整数据集
- en: 'We derive two configurations for each compression technique, each optimized
    on one of the large-scale text datasets: C4 (Raffel et al., [2020](#bib.bib19))
    and Wikitext (Merity et al., [2016](#bib.bib15)). To maintain a controlled experimental
    space, our fine-tuning of various baseline techniques is restricted to the identical
    dataset used initially to calibrate our model compression. We utilize two distinct
    prompts for IDP-specific settings, ”$m$,” being 50 and 100.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每种压缩技术得出了两个配置，每个配置在一个大规模文本数据集上进行了优化：C4（Raffel等，[2020](#bib.bib19)）和Wikitext（Merity等，[2016](#bib.bib15)）。为了保持控制的实验空间，我们对各种基线技术的微调限制在最初用于校准模型压缩的相同数据集上。我们为IDP特定设置使用了两个不同的提示，$m$分别为50和100。
- en: Validation Tasks
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 验证任务
- en: 'To gauge the genuine comprehensive performance of LLMs, we identify a suite
    of evaluation tasks that encapsulate three fundamental domains of cognition: world
    knowledge, common reasoning, and language understanding. Among the many available
    tasks, we distilled our focus to a curated list of nine that we deemed most representative.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估LLMs的真正综合性能，我们确定了一系列评估任务，涵盖了认知的三个基本领域：世界知识、常识推理和语言理解。在众多可用任务中，我们精炼出九个最具代表性的任务。
- en: For the domain of world knowledge, our chosen evaluative tasks were ARC-challenge
    & ARC-easy (Clark et al., [2018](#bib.bib4)), SCIQ (Welbl et al., [2017](#bib.bib22)),
    WebQS (Berant et al., [2013](#bib.bib1)), and TriviaQA (Joshi et al., [2017](#bib.bib12)).
    Tapping into the breadth of language understanding benchmarks, we centered our
    attention on Hellaswag (Zellers et al., [2019](#bib.bib26)), Lambada (Paperno
    et al., [2016](#bib.bib17)), and WinoGrande (Sakaguchi et al., [2019](#bib.bib20)).
    Lastly, for common reasoning, we identified PIQA (Bisk et al., [2019](#bib.bib2))
    as our touchstone. Notably, all the tasks we adopted are structured in a multiple-choice
    format.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于世界知识领域，我们选择的评估任务包括 ARC-challenge 和 ARC-easy (Clark et al., [2018](#bib.bib4))、SCIQ
    (Welbl et al., [2017](#bib.bib22))、WebQS (Berant et al., [2013](#bib.bib1)) 和
    TriviaQA (Joshi et al., [2017](#bib.bib12))。利用语言理解基准的广度，我们将注意力集中在 Hellaswag (Zellers
    et al., [2019](#bib.bib26))、Lambada (Paperno et al., [2016](#bib.bib17)) 和 WinoGrande
    (Sakaguchi et al., [2019](#bib.bib20)) 上。最后，对于常见推理，我们将 PIQA (Bisk et al., [2019](#bib.bib2))
    作为我们的基准。值得注意的是，我们采用的所有任务都是多项选择格式。
- en: Baseline Methods
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基线方法
- en: 'We gravitated toward three methodologies to test our hypothesis. We use prompt-tuning
    (Lester et al., [2021](#bib.bib13)), prefix-tuning (Li & Liang, [2021](#bib.bib14)),
    and LoRA (Hu et al., [2021](#bib.bib8)) as our representative candidates. For
    consistent benchmarks across these techniques, we establish the following criteria:
    1) The aggregate count of training tokens is standardized at 40,960,000 tokens.
    Our decision on the total token count draws inspiration from (Xu et al., [2023](#bib.bib24)).
    2) In alignment with (Frantar et al., [2022](#bib.bib6)), we adopt AdamW as our
    optimization algorithm. Our chosen learning rate stands at 2e-4 with a weight
    decay set at 1e-5\. All three methods are then fine-tuned using compressed LLM
    following the described settings with LLama-7b and OPT-6.7b.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了三种方法来检验我们的假设。我们使用了 prompt-tuning (Lester et al., [2021](#bib.bib13))、prefix-tuning
    (Li & Liang, [2021](#bib.bib14)) 和 LoRA (Hu et al., [2021](#bib.bib8)) 作为我们的代表候选方法。为了在这些技术之间保持一致的基准，我们建立了以下标准：1)
    训练令牌的总数量标准化为 40,960,000 个令牌。我们对总令牌数量的决定受到 (Xu et al., [2023](#bib.bib24)) 的启发。2)
    根据 (Frantar et al., [2022](#bib.bib6)) 的一致性，我们采用 AdamW 作为我们的优化算法。我们选择的学习率为 2e-4，权重衰减设为
    1e-5。然后，所有三种方法都使用压缩 LLM 在描述的设置下进行了微调，使用了 LLama-7b 和 OPT-6.7b。
- en: 'Table 1: This table summarizes the results for 3-bit GPTQ across all nine tasks
    for multiple fine-tuning baselines and our IDP. World, Common, and Language are
    performance averages across tasks within those knowledge domains. Average is the
    average performance across all nine tasks.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：此表总结了 3-bit GPTQ 在所有九个任务中对多个微调基线和我们的 IDP 的结果。World、Common 和 Language 是这些知识领域内任务的性能平均值。Average
    是所有九个任务的平均性能。
- en: Model Type Param arcE arcC sciq webqs triviaqa World piqa Common hellaswag lambada
    winogrande Language Average Llama-7b — — 71.46 37.71 92.60 17.96 33.02 50.55 76.01
    76.01 53.11 68.58 67.48 63.06 57.55 Llama-7b lora 4.4M 70.08 37.12 93.50 17.67
    34.11 50.50 77.04 77.04 54.47 70.48 67.40 64.12 57.99 Llama-7b lora 6.7M 71.09
    36.69 93.00 17.47 34.73 50.60 76.44 76.44 54.55 70.23 67.09 63.96 57.92 Llama-7b
    lora 8.9M 70.62 37.12 93.30 17.86 34.86 50.75 76.77 76.77 54.27 70.33 67.40 64.00
    58.06 Llama-7b prompt 0.1M 71.97 38.40 92.90 20.47 33.20 51.39 75.84 75.84 53.75
    69.45 67.17 63.46 58.13 Llama-7b prompt 0.2M 71.51 38.31 92.10 21.11 34.56 51.52
    75.84 75.84 53.92 69.69 68.75 64.12 58.42 Llama-7b prompt 0.4M 72.01 39.16 91.80
    21.60 34.43 51.80 75.95 75.95 54.33 69.49 67.01 63.61 58.42 Llama-7b ptune 3.1M
    70.24 36.77 91.40 14.42 30.42 48.65 75.73 75.73 53.40 66.49 63.77 61.22 55.85
    Llama-7b ptune 6.5M 69.57 34.81 91.30 15.55 30.65 48.38 75.30 75.30 52.98 64.84
    63.22 60.35 55.36 Llama-7b ptune 13.1M 69.32 34.73 88.70 16.14 27.84 47.35 74.59
    74.59 52.01 64.35 64.17 60.18 54.65 \hdashlineLlama-7b IDP 0.6M 72.43 39.76 92.50
    19.83 36.39 52.18 76.44 76.44 53.96 70.25 67.56 63.92 58.79 OPT-6.7b — — 64.77
    29.01 89.40 9.50 17.90 42.12 75.24 75.24 48.57 65.34 63.54 59.15 51.47 OPT-6.7b
    lora 4.7M 63.55 28.75 88.50 11.42 18.84 42.21 76.22 76.22 49.14 66.16 63.46 59.59
    51.78 OPT-6.7b lora 7.1M 64.27 29.01 89.20 11.07 18.95 42.50 75.90 75.90 48.89
    66.50 64.40 59.93 52.02 OPT-6.7b lora 9.4M 64.06 29.35 88.20 13.24 18.90 42.75
    76.01 76.01 49.12 66.64 63.93 59.90 52.16 OPT-6.7b prompt 0.1M 64.27 28.41 89.80
    10.73 18.22 42.50 76.01 76.01 49.05 65.34 63.22 59.20 51.79 OPT-6.7b prompt 0.2M
    64.94 28.84 89.90 10.88 18.80 42.67 75.63 75.63 49.13 65.96 63.77 59.62 51.98
    OPT-6.7b prompt 0.4M 64.60 28.50 89.70 11.52 18.76 42.62 76.12 76.12 48.82 65.90
    63.54 59.42 51.94 OPT-6.7b ptune 3.1M 63.05 28.84 89.00 10.73 18.39 42.00 75.95
    75.95 48.38 64.68 60.85 57.97 51.10 OPT-6.7b ptune 6.5M 62.88 28.58 88.80 10.43
    18.34 41.81 75.79 75.79 48.54 65.17 60.93 58.21 51.05 OPT-6.7b ptune 13.1M 62.54
    29.18 88.60 10.43 18.37 41.82 75.52 75.52 48.72 65.32 63.38 59.14 51.34 \hdashlineOPT-6.7b
    IDP 0.6M 64.18 28.67 90.40 11.96 19.05 42.85 76.17 76.17 49.03 66.82 63.22 59.69
    52.17
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 模型类型 参数 arcE arcC sciq webqs triviaqa World piqa Common hellaswag lambada winogrande
    Language Average Llama-7b — — 71.46 37.71 92.60 17.96 33.02 50.55 76.01 76.01
    53.11 68.58 67.48 63.06 57.55 Llama-7b lora 4.4M 70.08 37.12 93.50 17.67 34.11
    50.50 77.04 77.04 54.47 70.48 67.40 64.12 57.99 Llama-7b lora 6.7M 71.09 36.69
    93.00 17.47 34.73 50.60 76.44 76.44 54.55 70.23 67.09 63.96 57.92 Llama-7b lora
    8.9M 70.62 37.12 93.30 17.86 34.86 50.75 76.77 76.77 54.27 70.33 67.40 64.00 58.06
    Llama-7b prompt 0.1M 71.97 38.40 92.90 20.47 33.20 51.39 75.84 75.84 53.75 69.45
    67.17 63.46 58.13 Llama-7b prompt 0.2M 71.51 38.31 92.10 21.11 34.56 51.52 75.84
    75.84 53.92 69.69 68.75 64.12 58.42 Llama-7b prompt 0.4M 72.01 39.16 91.80 21.60
    34.43 51.80 75.95 75.95 54.33 69.49 67.01 63.61 58.42 Llama-7b ptune 3.1M 70.24
    36.77 91.40 14.42 30.42 48.65 75.73 75.73 53.40 66.49 63.77 61.22 55.85 Llama-7b
    ptune 6.5M 69.57 34.81 91.30 15.55 30.65 48.38 75.30 75.30 52.98 64.84 63.22 60.35
    55.36 Llama-7b ptune 13.1M 69.32 34.73 88.70 16.14 27.84 47.35 74.59 74.59 52.01
    64.35 64.17 60.18 54.65 \hdashlineLlama-7b IDP 0.6M 72.43 39.76 92.50 19.83 36.39
    52.18 76.44 76.44 53.96 70.25 67.56 63.92 58.79 OPT-6.7b — — 64.77 29.01 89.40
    9.50 17.90 42.12 75.24 75.24 48.57 65.34 63.54 59.15 51.47 OPT-6.7b lora 4.7M
    63.55 28.75 88.50 11.42 18.84 42.21 76.22 76.22 49.14 66.16 63.46 59.59 51.78
    OPT-6.7b lora 7.1M 64.27 29.01 89.20 11.07 18.95 42.50 75.90 75.90 48.89 66.50
    64.40 59.93 52.02 OPT-6.7b lora 9.4M 64.06 29.35 88.20 13.24 18.90 42.75 76.01
    76.01 49.12 66.64 63.93 59.90 52.16 OPT-6.7b prompt 0.1M 64.27 28.41 89.80 10.73
    18.22 42.50 76.01 76.01 49.05 65.34 63.22 59.20 51.79 OPT-6.7b prompt 0.2M 64.94
    28.84 89.90 10.88 18.80 42.67 75.63 75.63 49.13 65.96 63.77 59.62 51.98 OPT-6.7b
    prompt 0.4M 64.60 28.50 89.70 11.52 18.76 42.62 76.12 76.12 48.82 65.90 63.54
    59.42 51.94 OPT-6.7b ptune 3.1M 63.05 28.84 89.00 10.73 18.39 42.00 75.95 75.95
    48.38 64.68 60.85 57.97 51.10 OPT-6.7b ptune 6.5M 62.88 28.58 88.80 10.43 18.34
    41.81 75.79 75.79 48.54 65.17 60.93 58.21 51.05 OPT-6.7b ptune 13.1M 62.54 29.18
    88.60 10.43 18.37 41.82 75.52 75.52 48.72 65.32 63.38 59.14 51.34 \hdashlineOPT-6.7b
    IDP 0.6M 64.18 28.67 90.40 11.96 19.05 42.85 76.17 76.17 49.03 66.82 63.22 59.69
    52.17
- en: 'Table 2: This table summarizes the results for 50% unstructured sprase using
    SparseGPT across all nine tasks for multiple fine-tuning baselines and our IDP.
    World, Common, and Language are performance averages across tasks within those
    knowledge domains. Average is the average performance across all nine tasks.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：此表总结了使用 SparseGPT 在所有九个任务中的 50% 无结构稀疏数据的结果，涵盖了多个微调基准和我们的 IDP。World、Common
    和 Language 是这些知识领域任务中的表现平均值。Average 是所有九个任务的平均表现。
- en: Model Type Param arcE arcC sciq webqs triviaqa World piqa Common hellaswag lambada
    winogrande Language Average Llama-7b — — 70.33 37.03 93.50 14.07 28.88 48.76 77.04
    77.04 51.68 74.54 68.03 64.75 57.23 Llama-7b lora 4.4M 71.04 37.63 91.90 14.47
    33.28 49.66 76.99 76.99 53.98 70.95 67.17 64.03 57.49 Llama-7b lora 6.7M 70.79
    36.69 92.40 15.85 33.02 49.75 76.71 76.71 53.91 71.03 68.03 64.32 57.60 Llama-7b
    lora 8.9M 71.04 37.88 92.10 14.86 32.85 49.75 77.20 77.20 54.01 70.70 68.03 64.25
    57.63 Llama-7b prompt 0.1M 71.59 38.74 93.10 15.21 29.66 49.66 77.04 77.04 53.48
    71.24 67.48 64.07 57.50 Llama-7b prompt 0.2M 71.38 38.57 92.20 14.86 30.48 49.50
    77.15 77.15 53.75 71.76 67.09 64.20 57.47 Llama-7b prompt 0.4M 71.38 38.31 92.60
    14.86 30.86 49.60 77.31 77.31 53.97 70.99 67.17 64.04 57.49 Llama-7b ptune 3.1M
    63.17 32.59 88.20 11.81 24.60 44.07 72.63 72.63 50.18 64.97 56.91 57.35 51.67
    Llama-7b ptune 6.5M 67.17 34.90 88.70 12.11 24.74 45.52 74.76 74.76 50.36 65.59
    59.12 58.36 53.05 Llama-7b ptune 13.1M 65.78 31.40 87.20 11.61 21.97 43.59 74.21
    74.21 49.77 63.87 59.43 57.69 51.69 \hdashlineLlama-7b IDP 0.6M 72.05 39.08 92.90
    14.91 30.35 49.86 77.09 77.09 53.90 70.35 67.17 63.81 57.53 OPT-6.7b — — 63.01
    28.41 89.40 9.69 17.79 41.66 75.19 75.19 47.67 70.56 63.93 60.72 51.74 OPT-6.7b
    lora 4.7M 64.06 29.61 88.60 10.58 18.26 42.22 75.57 75.57 48.52 66.60 64.33 59.82
    51.79 OPT-6.7b lora 7.1M 63.93 29.78 88.20 10.14 18.48 42.11 75.90 75.90 48.58
    66.45 64.56 59.86 51.78 OPT-6.7b lora 9.4M 62.84 29.86 88.30 10.33 18.79 42.02
    75.41 75.41 48.76 66.49 65.19 60.15 51.77 OPT-6.7b prompt 0.1M 63.09 28.58 90.70
    12.30 18.75 42.68 75.14 75.14 48.40 68.78 63.69 60.29 52.16 OPT-6.7b prompt 0.2M
    63.68 29.44 90.60 12.40 18.36 42.90 75.24 75.24 48.58 67.86 63.22 59.89 52.15
    OPT-6.7b prompt 0.4M 64.06 29.27 89.60 12.80 19.12 42.97 75.19 75.19 48.49 67.49
    63.61 59.86 52.18 OPT-6.7b ptune 3.1M 61.03 28.50 86.90 13.09 19.46 41.80 72.74
    72.74 46.44 62.08 59.67 56.06 49.99 OPT-6.7b ptune 6.5M 63.01 29.86 88.00 9.40
    17.10 41.47 75.08 75.08 47.84 64.89 61.80 58.18 50.78 OPT-6.7b ptune 13.1M 60.94
    29.10 88.60 13.53 19.95 42.42 73.39 73.39 46.93 62.68 62.19 57.27 50.81 \hdashlineOPT-6.7b
    IDP 0.6M 64.06 29.27 89.60 12.80 19.12 42.97 75.19 75.19 48.49 67.49 63.61 59.86
    52.18
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 模型类型 参数 arcE arcC sciq webqs triviaqa World piqa Common hellaswag lambada winogrande
    语言 平均 Llama-7b — — 70.33 37.03 93.50 14.07 28.88 48.76 77.04 77.04 51.68 74.54
    68.03 64.75 57.23 Llama-7b lora 4.4M 71.04 37.63 91.90 14.47 33.28 49.66 76.99
    76.99 53.98 70.95 67.17 64.03 57.49 Llama-7b lora 6.7M 70.79 36.69 92.40 15.85
    33.02 49.75 76.71 76.71 53.91 71.03 68.03 64.32 57.60 Llama-7b lora 8.9M 71.04
    37.88 92.10 14.86 32.85 49.75 77.20 77.20 54.01 70.70 68.03 64.25 57.63 Llama-7b
    prompt 0.1M 71.59 38.74 93.10 15.21 29.66 49.66 77.04 77.04 53.48 71.24 67.48
    64.07 57.50 Llama-7b prompt 0.2M 71.38 38.57 92.20 14.86 30.48 49.50 77.15 77.15
    53.75 71.76 67.09 64.20 57.47 Llama-7b prompt 0.4M 71.38 38.31 92.60 14.86 30.86
    49.60 77.31 77.31 53.97 70.99 67.17 64.04 57.49 Llama-7b ptune 3.1M 63.17 32.59
    88.20 11.81 24.60 44.07 72.63 72.63 50.18 64.97 56.91 57.35 51.67 Llama-7b ptune
    6.5M 67.17 34.90 88.70 12.11 24.74 45.52 74.76 74.76 50.36 65.59 59.12 58.36 53.05
    Llama-7b ptune 13.1M 65.78 31.40 87.20 11.61 21.97 43.59 74.21 74.21 49.77 63.87
    59.43 57.69 51.69 \hdashlineLlama-7b IDP 0.6M 72.05 39.08 92.90 14.91 30.35 49.86
    77.09 77.09 53.90 70.35 67.17 63.81 57.53 OPT-6.7b — — 63.01 28.41 89.40 9.69
    17.79 41.66 75.19 75.19 47.67 70.56 63.93 60.72 51.74 OPT-6.7b lora 4.7M 64.06
    29.61 88.60 10.58 18.26 42.22 75.57 75.57 48.52 66.60 64.33 59.82 51.79 OPT-6.7b
    lora 7.1M 63.93 29.78 88.20 10.14 18.48 42.11 75.90 75.90 48.58 66.45 64.56 59.86
    51.78 OPT-6.7b lora 9.4M 62.84 29.86 88.30 10.33 18.79 42.02 75.41 75.41 48.76
    66.49 65.19 60.15 51.77 OPT-6.7b prompt 0.1M 63.09 28.58 90.70 12.30 18.75 42.68
    75.14 75.14 48.40 68.78 63.69 60.29 52.16 OPT-6.7b prompt 0.2M 63.68 29.44 90.60
    12.40 18.36 42.90 75.24 75.24 48.58 67.86 63.22 59.89 52.15 OPT-6.7b prompt 0.4M
    64.06 29.27 89.60 12.80 19.12 42.97 75.19 75.19 48.49 67.49 63.61 59.86 52.18
    OPT-6.7b ptune 3.1M 61.03 28.50 86.90 13.09 19.46 41.80 72.74 72.74 46.44 62.08
    59.67 56.06 49.99 OPT-6.7b ptune 6.5M 63.01 29.86 88.00 9.40 17.10 41.47 75.08
    75.08 47.84 64.89 61.80 58.18 50.78 OPT-6.7b ptune 13.1M 60.94 29.10 88.60 13.53
    19.95 42.42 73.39 73.39 46.93 62.68 62.19 57.27 50.81 \hdashlineOPT-6.7b IDP 0.6M
    64.06 29.27 89.60 12.80 19.12 42.97 75.19 75.19 48.49 67.49 63.61 59.86 52.18
- en: 3.2.2 Results
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 结果
- en: 'We compare several post-compression performance recovery techniques with IDP.
    We report our findings in Table [1](#S3.T1 "Table 1 ‣ Baseline Methods ‣ 3.2.1
    Settings ‣ 3.2 Experimental Comparison: IDP recovers performance better or comparable
    than LoRA ‣ 3 Methods and Experiments ‣ Do Compressed LLMs Forget Knowledge? An
    Experimental Study with Practical Implications") and Table [2](#S3.T2 "Table 2
    ‣ Baseline Methods ‣ 3.2.1 Settings ‣ 3.2 Experimental Comparison: IDP recovers
    performance better or comparable than LoRA ‣ 3 Methods and Experiments ‣ Do Compressed
    LLMs Forget Knowledge? An Experimental Study with Practical Implications"). From
    these tables, we draw the following insights:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将几种压缩后性能恢复技术与IDP进行比较。我们的发现记录在表格[1](#S3.T1 "表1 ‣ 基线方法 ‣ 3.2.1 设置 ‣ 3.2 实验比较：IDP
    恢复的性能优于或可与 LoRA 比拟 ‣ 3 方法与实验 ‣ 压缩的 LLM 是否会遗忘知识？具有实际意义的实验研究")和表格[2](#S3.T2 "表2
    ‣ 基线方法 ‣ 3.2.1 设置 ‣ 3.2 实验比较：IDP 恢复的性能优于或可与 LoRA 比拟 ‣ 3 方法与实验 ‣ 压缩的 LLM 是否会遗忘知识？具有实际意义的实验研究")中。从这些表格中，我们得出了以下见解：
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Performance Recovery: Overall, our testing shows that most techniques, IDP
    included, modestly improve performance in both quantization and pruning scenarios.
    The sole outlier is “ptune” or prefix-tuning, which, in contrast, reduced the
    performance of our models across all tasks. We also noted that quantization provides
    a better chance for performance recovery compared to pruning. Comparing baseline
    performances with the top performers, GPTQ shows an average improvement of about
    1%, compared to SparseGPT’s modest increase of 0.37%. This difference likely stems
    from pruning’s parameter removal, which has stricter limits on performance enhancement.
    However, IDP stands out in both scenarios, achieving the higher-than-average performance
    improvement.'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能恢复：总体而言，我们的测试显示，大多数技术，包括IDP，在量化和剪枝场景中都能适度提高性能。唯一的例外是“ptune”或前缀调优，相比之下，它降低了我们模型在所有任务中的表现。我们还注意到，量化相比剪枝提供了更好的性能恢复机会。将基线性能与顶级表现者进行比较时，GPTQ的平均提升约为1%，而SparseGPT的提升则为0.37%。这一差异可能源于剪枝中的参数移除，这对性能提升有更严格的限制。然而，IDP在这两种场景中都表现出色，实现了高于平均水平的性能提升。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Knowledge Domain Adaptation: By dividing our tasks into world knowledge, common
    reasoning, and language understanding categories, we draw insights on “knowledge
    displaced” and “knowledge forgotten.” We observed that redirection methods such
    as IDP are more effective for world knowledge tasks than integrated methods like
    LoRA. This suggests that in cases of factual knowledge, simple input redirection
    can effectively bring back the lost information in compressed models. Conversely,
    tasks that demand nuanced understanding, like those involving language comprehension,
    benefit more from the added parameters and external knowledge sources provided
    by techniques like LoRA. Nevertheless, in scenarios where IDP lacks, the difference
    in performance is marginal – less than 0.2%.'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识领域适应：通过将任务划分为世界知识、常识推理和语言理解类别，我们对“知识流失”和“知识遗忘”进行了深入分析。我们观察到，像IDP这样的重定向方法在世界知识任务中的效果比像LoRA这样的集成方法更为显著。这表明在处理事实性知识时，简单的输入重定向可以有效地恢复在压缩模型中丢失的信息。相反，对于需要细致理解的任务，比如语言理解，则更多地从像LoRA这样的技术提供的附加参数和外部知识源中受益。然而，在IDP缺乏的情况下，性能差异则很小——少于0.2%。
- en: 3.3 IDP is remarkably more efficient
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 IDP 显著更高效
- en: 'We examined the link between the method’s parameter size and performance, as
    detailed in Figure [4](#S3.F4 "Figure 4 ‣ 3.3 IDP is remarkably more efficient
    ‣ 3 Methods and Experiments ‣ Do Compressed LLMs Forget Knowledge? An Experimental
    Study with Practical Implications"). Our findings show that IDP is much more efficient
    than LoRA for compression recovery. For example, when fine-tuning the Llama-7b
    model with QPTQ settings, LoRA’s parameters range between 4.4 to 8.9 million,
    while IDP uses only around 0.8 million, leading to substantial space savings of
    81% to 91% — a notable 20-fold reduction. Additionally, prompting tends to have
    a faster inference speed. Basic inference testing shows prompting incurs at most
    0.37s versus LoRA’s 0.62s for an input batch of 16 and a sequence length of 1024
    – this is a substantial 60% improvement in speed. Despite the smaller size, IDP
    generally sees a modest average improvement of 1% across the nine tasks evaluated.
    For further details on the performance and parameter size, refer to Table [1](#S3.T1
    "Table 1 ‣ Baseline Methods ‣ 3.2.1 Settings ‣ 3.2 Experimental Comparison: IDP
    recovers performance better or comparable than LoRA ‣ 3 Methods and Experiments
    ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications")
    and Figure [4](#S3.F4 "Figure 4 ‣ 3.3 IDP is remarkably more efficient ‣ 3 Methods
    and Experiments ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study with
    Practical Implications"), and our appendix provides a detailed explanation of
    how the total number of parameters was calculated for both LoRA and IDP.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查了方法的参数大小与性能之间的关系，如图[4](#S3.F4 "图 4 ‣ 3.3 IDP显著更高效 ‣ 3 方法与实验 ‣ 压缩LLM是否会遗忘知识？具有实际意义的实验研究")中所述。我们的发现表明，与LoRA相比，IDP在压缩恢复方面更具效率。例如，在用QPTQ设置微调Llama-7b模型时，LoRA的参数范围为440万至890万，而IDP仅使用约80万，节省了81%至91%的空间——这是一个显著的20倍缩减。此外，提示通常具有更快的推理速度。基本推理测试显示，提示最多耗时0.37秒，而LoRA为0.62秒，输入批次为16，序列长度为1024——这是速度上提高了60%。尽管尺寸较小，IDP在评估的九项任务中总体上看到1%的适度平均提升。有关性能和参数大小的更多细节，请参见表[1](#S3.T1
    "表 1 ‣ 基线方法 ‣ 3.2.1 设置 ‣ 3.2 实验比较：IDP在恢复性能方面优于或可与LoRA媲美 ‣ 3 方法与实验 ‣ 压缩LLM是否会遗忘知识？具有实际意义的实验研究")和图[4](#S3.F4
    "图 4 ‣ 3.3 IDP显著更高效 ‣ 3 方法与实验 ‣ 压缩LLM是否会遗忘知识？具有实际意义的实验研究")，我们的附录提供了详细说明，说明了如何计算LoRA和IDP的总参数数量。
- en: Finally, we underscore the robustness of IDP’s performance, irrespective of
    prompt in Figure [6](#S4.F6 "Figure 6 ‣ 4.1 Evaluating Knowledge Forgetfulness
    and Displacement ‣ 4 More Studies ‣ Do Compressed LLMs Forget Knowledge? An Experimental
    Study with Practical Implications"). This figure reveals a variance of less than
    1% in average accuracy performance, yet with a 5-fold reduction in token size.
    Notably, even with a modest average of 20 tokens, IDP adeptly facilitates performance
    recovery, surpassing the compressed baseline. This evidence positions IDP as not
    only efficient in parameter utilization but also as a resilient mechanism for
    enhancing performance in the wake of model compression.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们强调了IDP的表现稳定性，无论图[6](#S4.F6 "图 6 ‣ 4.1 评估知识遗忘与位移 ‣ 4 更多研究 ‣ 压缩LLM是否会遗忘知识？具有实际意义的实验研究")中的提示如何。该图揭示了平均准确性表现的变化不到1%，但标记大小减少了5倍。值得注意的是，即使平均标记数为20，IDP也能有效地恢复性能，超越了压缩基线。这些证据表明IDP不仅在参数利用上高效，而且在模型压缩后提升性能方面表现出色。
- en: '![Refer to caption](img/8bdf68f839b183767158b2217bcc81ad.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8bdf68f839b183767158b2217bcc81ad.png)'
- en: 'Figure 4: GPTQ LLama-7b/OPT-6.7b average accuracy across nine tasks vs. number
    of trainable parameters. IDP shows remarkable efficiency and performance comparing
    to methods parameter-intensive method like LoRA.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：GPTQ LLama-7b/OPT-6.7b 在九项任务中的平均准确率与可训练参数数量的关系。IDP与如LoRA等参数密集型方法相比，表现出显著的效率和性能。
- en: '![Refer to caption](img/03df313d4482ce9e044d1478effc55ee.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/03df313d4482ce9e044d1478effc55ee.png)'
- en: 'Figure 5: Cosine similarity compares the self-attention and token activation
    at each layer to an uncompressed baseline using different fine-tuning techniques.
    A higher cosine score means it’s closer to the baseline.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：余弦相似度比较了每层的自注意力和标记激活与未压缩基线的不同微调技术。较高的余弦分数意味着更接近基线。
- en: 4 More Studies
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 更多研究
- en: 4.1 Evaluating Knowledge Forgetfulness and Displacement
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 评估知识遗忘与位移
- en: 'We employed a detailed visualization of the layer-wise attention and activation
    matrices to validate our hypothesis. Opting for cosine similarity over magnitude
    differences as our analytical tool, we aim to understand the distribution differences
    rather than magnitude. Our findings are presented in Figures [5](#S3.F5 "Figure
    5 ‣ 3.3 IDP is remarkably more efficient ‣ 3 Methods and Experiments ‣ Do Compressed
    LLMs Forget Knowledge? An Experimental Study with Practical Implications"), and
    [6](#S4.F6 "Figure 6 ‣ 4.1 Evaluating Knowledge Forgetfulness and Displacement
    ‣ 4 More Studies ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study
    with Practical Implications"), leading to several key observations:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了详细的层级注意力和激活矩阵可视化来验证我们的假设。选择余弦相似性而非幅度差异作为我们的分析工具，我们旨在理解分布差异而非幅度。我们的发现呈现在图
    [5](#S3.F5 "图 5 ‣ 3.3 IDP 显著更高效 ‣ 3 方法和实验 ‣ 压缩 LLM 是否会遗忘知识？具有实际意义的实验研究") 和 [6](#S4.F6
    "图 6 ‣ 4.1 评估知识遗忘和转移 ‣ 4 更多研究 ‣ 压缩 LLM 是否会遗忘知识？具有实际意义的实验研究") 中，得出几个关键观察结果：
- en: '![Refer to caption](img/24b7bddf4e029577db9bafe3ba880151.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/24b7bddf4e029577db9bafe3ba880151.png)'
- en: 'Figure 6: This figure illustrates the average performance over nine tasks using
    IDP. Results show IDP maintains relatively stable performance working with various
    average prompt sizes.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：该图展示了使用 IDP 在九项任务上的平均性能。结果表明，IDP 在处理各种平均提示大小时性能保持相对稳定。
- en: 1 When compared to
    LoRA, the attention mechanism of both prompting/IDP markedly diverges from the
    baseline, hinting at a potential contextual redirection. Conversely, the activation
    patterns echo similarities with LoRA. Given that LoRA incorporates a residual
    network at every layer to maintain congruity and prompting only at the self-attention,
    this semblance is unexpected.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 1 与 LoRA 相比，提示/IDP
    的注意机制与基线显著不同，这暗示了可能的上下文重定向。相反，激活模式与 LoRA 相似。考虑到 LoRA 在每一层都包含一个残差网络以保持一致性，而提示仅在自注意力中进行，这种相似性是出乎意料的。
- en: '2 These observations
    imply that prompting/IDP can tap into latent knowledge within the model. This
    is further supported by the data in Table [1](#S3.T1 "Table 1 ‣ Baseline Methods
    ‣ 3.2.1 Settings ‣ 3.2 Experimental Comparison: IDP recovers performance better
    or comparable than LoRA ‣ 3 Methods and Experiments ‣ Do Compressed LLMs Forget
    Knowledge? An Experimental Study with Practical Implications") and Table [2](#S3.T2
    "Table 2 ‣ Baseline Methods ‣ 3.2.1 Settings ‣ 3.2 Experimental Comparison: IDP
    recovers performance better or comparable than LoRA ‣ 3 Methods and Experiments
    ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications"),
    which show a propensity of prompting/IDP for tasks involving world knowledge.
    These tasks rely on the model’s internal knowledge base, reinforcing our conclusion
    about the efficacy of prompting/IDP in accessing embedded information.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '2 这些观察结果表明，提示/IDP 可以挖掘模型中的潜在知识。这一点得到了表格
    [1](#S3.T1 "Table 1 ‣ Baseline Methods ‣ 3.2.1 Settings ‣ 3.2 Experimental Comparison:
    IDP recovers performance better or comparable than LoRA ‣ 3 Methods and Experiments
    ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications")
    和表格 [2](#S3.T2 "Table 2 ‣ Baseline Methods ‣ 3.2.1 Settings ‣ 3.2 Experimental
    Comparison: IDP recovers performance better or comparable than LoRA ‣ 3 Methods
    and Experiments ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study with
    Practical Implications") 中的数据的进一步支持，这些数据展示了提示/IDP 在涉及世界知识的任务中的倾向。这些任务依赖于模型的内部知识库，进一步强化了我们关于提示/IDP
    在访问嵌入信息方面的有效性的结论。'
- en: 3 Additionally, IDP
    demonstrates remarkable consistency in information retrieval. As evidenced in
    Figure [6](#S4.F6 "Figure 6 ‣ 4.1 Evaluating Knowledge Forgetfulness and Displacement
    ‣ 4 More Studies ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study
    with Practical Implications"), it maintains stable performance across a range
    of prompt sizes. This suggests that even with fewer tokens, knowledge rerouting
    via IDP remains effective, opening avenues for future optimizations and refinements
    in its application.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 3 此外，IDP 在信息检索方面表现出显著的一致性。如图
    [6](#S4.F6 "Figure 6 ‣ 4.1 Evaluating Knowledge Forgetfulness and Displacement
    ‣ 4 More Studies ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study
    with Practical Implications") 所示，它在不同的提示大小范围内保持了稳定的表现。这表明，即使使用更少的标记，通过 IDP 进行的知识重定向仍然有效，为未来的优化和改进提供了新的方向。
- en: 4 Finally, our analysis
    of prefix-tuning indicates its tendency to align with the original attention patterns
    of the model. However, as shown in Figure [5](#S3.F5 "Figure 5 ‣ 3.3 IDP is remarkably
    more efficient ‣ 3 Methods and Experiments ‣ Do Compressed LLMs Forget Knowledge?
    An Experimental Study with Practical Implications"), its activation patterns significantly
    deviate, hinting at a potential shortfall in redirecting knowledge.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 4 最终，我们对前缀调优的分析表明，它有趋向于与模型的原始注意力模式对齐。然而，如图
    [5](#S3.F5 "图 5 ‣ 3.3 IDP 显著更高效 ‣ 3 方法与实验 ‣ 压缩 LLM 是否会遗忘知识？具有实际意义的实验研究") 所示，其激活模式显著偏离，暗示在重定向知识方面可能存在潜在不足。
- en: These insights strongly endorse the notion of “redirection” as the more effective
    mechanism for recovering performance in compressed models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些见解强烈支持“重定向”作为在压缩模型中恢复性能的更有效机制的观点。
- en: 4.2 More ablation studies
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 更多消融研究
- en: 'Table 3: This table includes results for our Inference-time Dynamic Prompting
    strategy. To illustrate its effectiveness, we also include the results of the
    individual prompts used along with naive soft-prompts concatenation. 26 and 100
    refers to the number of tokens in our prompts.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：该表包括了我们推理时间动态提示策略的结果。为了说明其有效性，我们还包括了使用的单独提示的结果，以及天真的软提示拼接结果。26 和 100 指的是我们提示中的标记数量。
- en: Model arcE arcC sciq webqs triviaqa World piqa Common hellaswag lambada winogrande
    Language Average OPT-6.7b/26 64.94 28.84 89.90 10.88 18.80 42.67 75.63 75.63 49.13
    65.96 63.77 59.62 51.98 OPT-6.7b/100 64.02 27.90 89.50 11.32 18.37 42.22 76.39
    76.39 48.81 65.42 63.22 59.15 51.66 OPT-6.7b/Concat 63.80 28.50 89.40 12.30 19.55
    42.71 75.79 75.79 48.92 64.72 63.85 59.16 51.87 OPT-6.7b/IDP 64.18 28.67 90.40
    11.96 19.05 42.85 76.17 76.17 49.03 66.82 63.22 59.69 52.17 Llama-7b/26 71.97
    38.40 92.90 20.47 33.20 51.39 75.84 75.84 53.75 69.45 67.17 63.46 58.13 Llama-7b/100
    71.51 38.31 92.10 21.11 34.56 51.52 75.84 75.84 53.92 69.69 68.75 64.12 58.42
    Llama-7b/Concat 71.17 37.80 92.30 16.88 33.84 50.40 74.92 74.92 53.34 67.18 66.46
    62.33 57.10 Llama-7b/IDP 71.63 38.65 92.60 21.60 33.84 51.66 76.01 76.01 53.97
    69.67 68.98 64.21 58.55
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Model arcE arcC sciq webqs triviaqa World piqa Common hellaswag lambada winogrande
    Language Average OPT-6.7b/26 64.94 28.84 89.90 10.88 18.80 42.67 75.63 75.63 49.13
    65.96 63.77 59.62 51.98 OPT-6.7b/100 64.02 27.90 89.50 11.32 18.37 42.22 76.39
    76.39 48.81 65.42 63.22 59.15 51.66 OPT-6.7b/Concat 63.80 28.50 89.40 12.30 19.55
    42.71 75.79 75.79 48.92 64.72 63.85 59.16 51.87 OPT-6.7b/IDP 64.18 28.67 90.40
    11.96 19.05 42.85 76.17 76.17 49.03 66.82 63.22 59.69 52.17 Llama-7b/26 71.97
    38.40 92.90 20.47 33.20 51.39 75.84 75.84 53.75 69.45 67.17 63.46 58.13 Llama-7b/100
    71.51 38.31 92.10 21.11 34.56 51.52 75.84 75.84 53.92 69.69 68.75 64.12 58.42
    Llama-7b/Concat 71.17 37.80 92.30 16.88 33.84 50.40 74.92 74.92 53.34 67.18 66.46
    62.33 57.10 Llama-7b/IDP 71.63 38.65 92.60 21.60 33.84 51.66 76.01 76.01 53.97
    69.67 68.98 64.21 58.55
- en: '![Refer to caption](img/39135e30069e3eda4a684de9e202b1d0.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/39135e30069e3eda4a684de9e202b1d0.png)'
- en: 'Figure 7: This graph shows the percentage performance improvement using two
    prompts at various lengths compared to a 3-bit quantized baseline for the OPT
    and LLama models. We’ve also showcased results from our IDP method, which selects
    prompts dynamically using the same two prompts. Small and Large correspond to
    26 and 100 tokens respectively.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：该图显示了使用两种提示在不同长度下相对于 OPT 和 LLama 模型的 3 位量化基线的性能提升百分比。我们还展示了我们 IDP 方法的结果，该方法使用相同的两种提示动态选择提示。Small
    和 Large 分别对应 26 和 100 个标记。
- en: We used IDP strategy with two distinct prompts of differing lengths, both trained
    using the same dataset to streamline our experimental parameters. We subsequently
    evaluated against our task benchmark, with the comprehensive findings cataloged
    in Table [3](#S4.T3 "Table 3 ‣ 4.2 More ablation studies ‣ 4 More Studies ‣ Do
    Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications").
    In a complementary visual aid, Figure [7](#S4.F7 "Figure 7 ‣ 4.2 More ablation
    studies ‣ 4 More Studies ‣ Do Compressed LLMs Forget Knowledge? An Experimental
    Study with Practical Implications") highlights the percentage differences in performance
    against the baseline quantized models, providing an at-a-glance understanding
    of the performance gains across individual tasks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了IDP策略，采用了两个长度不同的独特提示，这两个提示都是用相同的数据集进行训练的，以简化我们的实验参数。随后，我们在任务基准测试中进行了评估，综合结果见于表格[3](#S4.T3
    "Table 3 ‣ 4.2 More ablation studies ‣ 4 More Studies ‣ Do Compressed LLMs Forget
    Knowledge? An Experimental Study with Practical Implications")。在一个辅助的视觉展示中，图[7](#S4.F7
    "Figure 7 ‣ 4.2 More ablation studies ‣ 4 More Studies ‣ Do Compressed LLMs Forget
    Knowledge? An Experimental Study with Practical Implications")突出显示了相对于基准量化模型的性能差异百分比，提供了各个任务性能提升的概览。
- en: Our analysis showed that IDP subtly enhances average accuracy. This is evident
    in our results with OPT and Llama models, where IDP showed a modest improvement
    of $0.5\%$. While these findings, detailed in Table [3](#S4.T3 "Table 3 ‣ 4.2
    More ablation studies ‣ 4 More Studies ‣ Do Compressed LLMs Forget Knowledge?
    An Experimental Study with Practical Implications"), might not be groundbreaking,
    they highlight the potential of zero-shot input-to-prompt matching for compression
    recovery for various knowledge domains.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析表明，IDP subtly 提升了平均准确率。这在我们对OPT和Llama模型的结果中得到了体现，IDP显示出$0.5\%$的适度提升。虽然这些发现详见表格[3](#S4.T3
    "Table 3 ‣ 4.2 More ablation studies ‣ 4 More Studies ‣ Do Compressed LLMs Forget
    Knowledge? An Experimental Study with Practical Implications")，可能不会是突破性的，但它们突显了零-shot输入到提示匹配在不同知识领域压缩恢复中的潜力。
- en: Further, in our examination of quantized foundation models, as shown in Figure
    [7](#S4.F7 "Figure 7 ‣ 4.2 More ablation studies ‣ 4 More Studies ‣ Do Compressed
    LLMs Forget Knowledge? An Experimental Study with Practical Implications"), we
    noted areas where IDP demonstrated a slight but consistent superiority. Specifically,
    OPT models showed this incremental benefit in tasks such as Sciq, Triviqa, and
    Webqs, all falling within the world knowledge domain. Similarly, the Llama models
    exhibited slight improvements in tasks like Webqs, Arc, and Winogrand, with gains
    ranging between 1%-1.5%.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在我们对量化基础模型的检查中，如图[7](#S4.F7 "Figure 7 ‣ 4.2 More ablation studies ‣ 4 More
    Studies ‣ Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical
    Implications")所示，我们注意到IDP在某些方面表现出轻微但持续的优越性。具体来说，OPT模型在如Sciq、Triviqa和Webqs等任务中显示出这种增益，这些任务都属于世界知识领域。类似地，Llama模型在Webqs、Arc和Winogrand等任务中也表现出轻微的改善，增益范围在1%-1.5%之间。
- en: 5 Conclusion
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: 'In this study, we focused on understanding the impact of compression on LLMs
    and explore ways to mitigate its negative effects. We explore two key hypotheses:
    knowledge forgotten and knowledge displaced by analyzing the effectiveness of
    parameter-efficient tuning method like LoRA and prompting. A highlight from our
    study is the introduction Inference-time Dynamic Prompting , a light-weight approach
    to enhance traditional prompting. Empirically, we find IDP-based prompting perform
    on-par or better than relearning approach like LoRA while being significantly
    smaller in size with faster inference speed. Additionally, our visualization of
    the intermediate embeddings within LLMs suggests redirection through instruction
    is a more beneficial way to regain similar activation output. Collectively, our
    findings advocate the hypothesis of “knowledge displaced” as the critical factor
    behind performance decline post-compression, providing valuable insights into
    the mechanisms at play and paving the way for more efficient recovery strategies.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们专注于理解压缩对LLMs的影响，并探索缓解其负面效应的方法。我们探讨了两个关键假设：知识遗忘和知识替代，通过分析像LoRA和提示等参数高效调整方法的有效性。我们研究的一个亮点是引入了Inference-time
    Dynamic Prompting，这是一种增强传统提示的轻量级方法。从经验上看，我们发现基于IDP的提示与重新学习方法如LoRA表现相当或更好，同时在大小上显著较小且推理速度更快。此外，我们对LLMs中间嵌入的可视化表明，通过指令重新定向是一种更有利于恢复类似激活输出的方法。总的来说，我们的发现支持了“知识替代”这一假设，认为它是压缩后性能下降的关键因素，提供了对相关机制的宝贵见解，并为更高效的恢复策略铺平了道路。
- en: 6 Impact Statements
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 影响声明
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了旨在推动机器学习领域的工作。我们的工作有许多潜在的社会影响，但我们认为这里没有必要特别强调。
- en: References
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Berant et al. (2013) Berant, J., Chou, A. K., Frostig, R., and Liang, P. Semantic
    parsing on freebase from question-answer pairs. In *Conference on Empirical Methods
    in Natural Language Processing*, 2013. URL [https://api.semanticscholar.org/CorpusID:6401679](https://api.semanticscholar.org/CorpusID:6401679).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berant 等 (2013) Berant, J., Chou, A. K., Frostig, R., 和 Liang, P. 从问答对中进行 Freebase
    上的语义解析。发表于 *自然语言处理实证方法会议*, 2013. URL [https://api.semanticscholar.org/CorpusID:6401679](https://api.semanticscholar.org/CorpusID:6401679).
- en: 'Bisk et al. (2019) Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y.
    Piqa: Reasoning about physical commonsense in natural language, 2019.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk 等 (2019) Bisk, Y., Zellers, R., Bras, R. L., Gao, J., 和 Choi, Y. Piqa:
    关于自然语言中的物理常识推理, 2019.'
- en: 'Chen et al. (2023) Chen, L., Zaharia, M. A., and Zou, J. Y. Frugalgpt: How
    to use large language models while reducing cost and improving performance. *ArXiv*,
    abs/2305.05176, 2023. URL [https://api.semanticscholar.org/CorpusID:258564349](https://api.semanticscholar.org/CorpusID:258564349).'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 (2023) Chen, L., Zaharia, M. A., 和 Zou, J. Y. Frugalgpt: 如何在减少成本和提高性能的同时使用大型语言模型。*ArXiv*,
    abs/2305.05176, 2023. URL [https://api.semanticscholar.org/CorpusID:258564349](https://api.semanticscholar.org/CorpusID:258564349).'
- en: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try
    arc, the ai2 reasoning challenge. *ArXiv*, abs/1803.05457, 2018. URL [https://api.semanticscholar.org/CorpusID:3922816](https://api.semanticscholar.org/CorpusID:3922816).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等 (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,
    Schoenick, C., 和 Tafjord, O. 认为你解决了问答问题？试试 ARC，即 AI2 推理挑战。*ArXiv*, abs/1803.05457,
    2018. URL [https://api.semanticscholar.org/CorpusID:3922816](https://api.semanticscholar.org/CorpusID:3922816).
- en: 'Frantar & Alistarh (2023) Frantar, E. and Alistarh, D. Sparsegpt: Massive language
    models can be accurately pruned in one-shot. *ArXiv*, abs/2301.00774, 2023. URL
    [https://api.semanticscholar.org/CorpusID:255372747](https://api.semanticscholar.org/CorpusID:255372747).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 和 Alistarh (2023) Frantar, E. 和 Alistarh, D. Sparsegpt: 大型语言模型可以在一次性操作中准确修剪。*ArXiv*,
    abs/2301.00774, 2023. URL [https://api.semanticscholar.org/CorpusID:255372747](https://api.semanticscholar.org/CorpusID:255372747).'
- en: 'Frantar et al. (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. Gptq: Accurate post-training quantization for generative pre-trained transformers.
    *ArXiv*, abs/2210.17323, 2022. URL [https://api.semanticscholar.org/CorpusID:253237200](https://api.semanticscholar.org/CorpusID:253237200).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等 (2022) Frantar, E., Ashkboos, S., Hoefler, T., 和 Alistarh, D. Gptq:
    生成预训练变换器的准确后训练量化。*ArXiv*, abs/2210.17323, 2022. URL [https://api.semanticscholar.org/CorpusID:253237200](https://api.semanticscholar.org/CorpusID:253237200).'
- en: 'Han et al. (2015) Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. *arXiv
    preprint arXiv:1510.00149*, 2015.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han 等 (2015) Han, S., Mao, H., 和 Dally, W. J. 深度压缩: 通过修剪、训练量化和霍夫曼编码压缩深度神经网络。*arXiv
    预印本 arXiv:1510.00149*, 2015.'
- en: 'Hu et al. (2021) Hu, J. E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., and Chen, W. Lora: Low-rank adaptation of large language models. *ArXiv*,
    abs/2106.09685, 2021. URL [https://api.semanticscholar.org/CorpusID:235458009](https://api.semanticscholar.org/CorpusID:235458009).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等 (2021) Hu, J. E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S.,
    和 Chen, W. Lora: 大型语言模型的低秩适应。*ArXiv*, abs/2106.09685, 2021. URL [https://api.semanticscholar.org/CorpusID:235458009](https://api.semanticscholar.org/CorpusID:235458009).'
- en: 'Hubara et al. (2021a) Hubara, I., Chmiel, B., Island, M., Banner, R., Naor,
    S., and Soudry, D. Accelerated sparse neural training: A provable and efficient
    method to find n: M transposable masks. *ArXiv*, abs/2102.08124, 2021a. URL [https://api.semanticscholar.org/CorpusID:231934142](https://api.semanticscholar.org/CorpusID:231934142).'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hubara 等 (2021a) Hubara, I., Chmiel, B., Island, M., Banner, R., Naor, S.,
    和 Soudry, D. 加速稀疏神经训练: 一种可证明的有效方法来找到 n: M 可转置掩码。*ArXiv*, abs/2102.08124, 2021a.
    URL [https://api.semanticscholar.org/CorpusID:231934142](https://api.semanticscholar.org/CorpusID:231934142).'
- en: Hubara et al. (2021b) Hubara, I., Nahshan, Y., Hanani, Y., Banner, R., and Soudry,
    D. Accurate post training quantization with small calibration sets. In *International
    Conference on Machine Learning*, 2021b. URL [https://api.semanticscholar.org/CorpusID:235825979](https://api.semanticscholar.org/CorpusID:235825979).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara等（2021b）Hubara, I., Nahshan, Y., Hanani, Y., Banner, R., 和 Soudry, D.
    精确的后训练量化与小型校准集。在*国际机器学习会议*，2021b年。网址 [https://api.semanticscholar.org/CorpusID:235825979](https://api.semanticscholar.org/CorpusID:235825979)。
- en: 'Jaiswal et al. (2023) Jaiswal, A., Gan, Z., Du, X., Zhang, B., Wang, Z., and
    Yang, Y. Compressing llms: The truth is rarely pure and never simple, 2023.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaiswal等（2023）Jaiswal, A., Gan, Z., Du, X., Zhang, B., Wang, Z., 和 Yang, Y.
    压缩大型语言模型：事实很少纯粹且从不简单，2023年。
- en: 'Joshi et al. (2017) Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa:
    A large scale distantly supervised challenge dataset for reading comprehension,
    2017.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi等（2017）Joshi, M., Choi, E., Weld, D. S., 和 Zettlemoyer, L. Triviaqa：一个大规模的远程监督挑战数据集，用于阅读理解，2017年。
- en: Lester et al. (2021) Lester, B., Al-Rfou, R., and Constant, N. The power of
    scale for parameter-efficient prompt tuning. In *Conference on Empirical Methods
    in Natural Language Processing*, 2021. URL [https://api.semanticscholar.org/CorpusID:233296808](https://api.semanticscholar.org/CorpusID:233296808).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester等（2021）Lester, B., Al-Rfou, R., 和 Constant, N. 参数高效的提示调优的规模效应。在*自然语言处理实证方法会议*，2021年。网址
    [https://api.semanticscholar.org/CorpusID:233296808](https://api.semanticscholar.org/CorpusID:233296808)。
- en: 'Li & Liang (2021) Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous
    prompts for generation. *Proceedings of the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers)*, abs/2101.00190, 2021. URL [https://api.semanticscholar.org/CorpusID:230433941](https://api.semanticscholar.org/CorpusID:230433941).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li & Liang（2021）Li, X. L. 和 Liang, P. 前缀调优：优化连续提示以进行生成。*第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）*，abs/2101.00190,
    2021年。网址 [https://api.semanticscholar.org/CorpusID:230433941](https://api.semanticscholar.org/CorpusID:230433941)。
- en: Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
    sentinel mixture models, 2016.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity等（2016）Merity, S., Xiong, C., Bradbury, J., 和 Socher, R. 指针哨兵混合模型，2016年。
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI. GPT-4技术报告，2023年。
- en: Paperno et al. (2016) Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N.,
    Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernández, R. The lambada
    dataset, August 2016. URL [https://doi.org/10.5281/zenodo.2630551](https://doi.org/10.5281/zenodo.2630551).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paperno等（2016）Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi,
    R., Pezzelle, S., Baroni, M., Boleda, G., 和 Fernández, R. The lambada数据集，2016年8月。网址
    [https://doi.org/10.5281/zenodo.2630551](https://doi.org/10.5281/zenodo.2630551)。
- en: 'PENG et al. (2023) PENG, X., Xing, C., Choubey, P. K., Wu, C.-S., and Xiong,
    C. Model ensemble instead of prompt fusion: a sample-specific knowledge transfer
    method for few-shot prompt tuning. In *The Eleventh International Conference on
    Learning Representations*, 2023. URL [https://openreview.net/forum?id=p0yrSRbN5Bu](https://openreview.net/forum?id=p0yrSRbN5Bu).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PENG等（2023）PENG, X., Xing, C., Choubey, P. K., Wu, C.-S., 和 Xiong, C. 模型集成而非提示融合：一种针对少样本提示调优的样本特定知识转移方法。在*第十一届国际表示学习大会*，2023年。网址
    [https://openreview.net/forum?id=p0yrSRbN5Bu](https://openreview.net/forum?id=p0yrSRbN5Bu)。
- en: Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer
    learning with a unified text-to-text transformer. *Journal of Machine Learning
    Research*, 21(140):1–67, 2020. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel等（2020）Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
    M., Zhou, Y., Li, W., 和 Liu, P. J. 探索统一的文本到文本变换器在迁移学习中的极限。*机器学习研究期刊*，21(140):1–67,
    2020年。网址 [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html)。
- en: 'Sakaguchi et al. (2019) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. Winogrande: An adversarial winograd schema challenge at scale, 2019.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi等（2019）Sakaguchi, K., Bras, R. L., Bhagavatula, C., 和 Choi, Y. Winogrande:
    大规模对抗性Winograd Schema挑战，2019年。'
- en: 'Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation
    language models. *ArXiv*, abs/2302.13971, 2023. URL [https://api.semanticscholar.org/CorpusID:257219404](https://api.semanticscholar.org/CorpusID:257219404).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023）Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., 和 Lample, G. Llama: 开放且高效的基础语言模型。*ArXiv*, abs/2302.13971,
    2023。网址 [https://api.semanticscholar.org/CorpusID:257219404](https://api.semanticscholar.org/CorpusID:257219404)。'
- en: Welbl et al. (2017) Welbl, J., Liu, N. F., and Gardner, M. Crowdsourcing multiple
    choice science questions. *ArXiv*, abs/1707.06209, 2017. URL [https://api.semanticscholar.org/CorpusID:1553193](https://api.semanticscholar.org/CorpusID:1553193).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Welbl 等（2017）Welbl, J., Liu, N. F., 和 Gardner, M. 众包多项选择科学问题。*ArXiv*, abs/1707.06209,
    2017。网址 [https://api.semanticscholar.org/CorpusID:1553193](https://api.semanticscholar.org/CorpusID:1553193)。
- en: 'Xiao et al. (2022) Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han, S.
    Smoothquant: Accurate and efficient post-training quantization for large language
    models. *ArXiv*, abs/2211.10438, 2022. URL [https://api.semanticscholar.org/CorpusID:253708271](https://api.semanticscholar.org/CorpusID:253708271).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等（2022）Xiao, G., Lin, J., Seznec, M., Demouth, J., 和 Han, S. Smoothquant:
    精确且高效的大型语言模型后训练量化。*ArXiv*, abs/2211.10438, 2022。网址 [https://api.semanticscholar.org/CorpusID:253708271](https://api.semanticscholar.org/CorpusID:253708271)。'
- en: 'Xu et al. (2023) Xu, Z., Liu, Z., Chen, B., Tang, Y., Wang, J., Zhou, K., Hu,
    X., and Shrivastava, A. Compress, then prompt: Improving accuracy-efficiency trade-off
    of llm inference with transferable prompt. *ArXiv*, abs/2305.11186, 2023. URL
    [https://api.semanticscholar.org/CorpusID:258823240](https://api.semanticscholar.org/CorpusID:258823240).'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2023）Xu, Z., Liu, Z., Chen, B., Tang, Y., Wang, J., Zhou, K., Hu, X., 和
    Shrivastava, A. 压缩，然后提示：通过可转移提示提高 LLM 推理的准确性与效率权衡。*ArXiv*, abs/2305.11186, 2023。网址
    [https://api.semanticscholar.org/CorpusID:258823240](https://api.semanticscholar.org/CorpusID:258823240)。
- en: 'Yao et al. (2022) Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and
    He, Y. Zeroquant: Efficient and affordable post-training quantization for large-scale
    transformers, 2022.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等（2022）Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., 和 He, Y. Zeroquant:
    高效且经济的大规模变换器后训练量化, 2022。'
- en: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers 等（2019）Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., 和 Choi, Y.
    Hellaswag: 机器真的能完成你的句子吗？，2019。'
- en: 'Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M. T., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer,
    S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,
    L. Opt: Open pre-trained transformer language models. *ArXiv*, abs/2205.01068,
    2022. URL [https://api.semanticscholar.org/CorpusID:248496292](https://api.semanticscholar.org/CorpusID:248496292).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2022）Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen,
    S., Dewan, C., Diab, M. T., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer,
    S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., 和 Zettlemoyer,
    L. Opt: 开放的预训练变换器语言模型。*ArXiv*, abs/2205.01068, 2022。网址 [https://api.semanticscholar.org/CorpusID:248496292](https://api.semanticscholar.org/CorpusID:248496292)。'
- en: Appendix A Number of Parameters
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 参数数量
- en: LoRA
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LoRA
- en: In the LoRA method (Hu et al., [2021](#bib.bib8)), the total number of parameters
    is calculated as the sum of all low-rank projected layers within the LLMs. Consider
    the output of a singular LoRA layer as $x_{lora}=BAx$ as 11008.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LoRA 方法中（Hu 等，[2021](#bib.bib8)），总参数数量计算为 LLMs 中所有低秩投影层的总和。考虑一个单一 LoRA 层的输出为
    $x_{lora}=BAx$ 为 11008。
- en: Prompting / IDP
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Prompting / IDP
- en: In IDP, the total number of parameters is calculated by multiplying the total
    number of tokens, denoted as $t$ are 26, 50, and 100, corresponding to different
    settings.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在 IDP 中，总参数数量通过将总令牌数（记作 $t$）与 26、50 和 100 相乘来计算，分别对应不同的设置。
- en: Prefix-tuning
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Prefix-tuning
- en: For Prefix-tuning (Ptune), the total number of parameters can be determined
    using the formula $L*t*d$ are 26, 50, and 100.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Prefix-tuning（Ptune），总参数数量可以通过公式 $L*t*d$ 来确定，分别为 26、50 和 100。
