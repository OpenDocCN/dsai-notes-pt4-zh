- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:04:57'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:57
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ProPD：用于LLM并行解码的动态令牌树修剪和生成
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.13485](https://ar5iv.labs.arxiv.org/html/2402.13485)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.13485](https://ar5iv.labs.arxiv.org/html/2402.13485)
- en: Shuzhang Zhong Peking UniversityBeijingChina ,  Zebin Yang Peking UniversityBeijingChina
    ,  Meng Li Peking UniversityBeijingChina [meng.li@pku.edu.cn](mailto:meng.li@pku.edu.cn)
    ,  Ruihao Gong Sensetime ResearchBeijingChina ,  Runsheng Wang Peking UniversityBeijingChina
     and  Ru Huang Peking UniversityBeijingChina
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Shuzhang Zhong 北京大学 北京 中国 , Zebin Yang 北京大学 北京 中国 , Meng Li 北京大学 北京 中国 [meng.li@pku.edu.cn](mailto:meng.li@pku.edu.cn)
    , Ruihao Gong 赛昉研究 北京 中国 , Runsheng Wang 北京大学 北京 中国 以及 Ru Huang 北京大学 北京 中国
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Recent advancements in generative large language models (LLMs) have significantly
    boosted the performance in natural language processing tasks. However, their efficiency
    is hampered by the inherent limitations in autoregressive token generation. While
    parallel decoding with token tree verification, e.g., Medusa, has been proposed
    to improve decoding parallelism and efficiency, it often struggles with maintaining
    contextual relationships due to its independent token prediction approach and
    incurs significant verification overhead, especially with large tree sizes and
    batch processing. In this paper, we propose ProPD, an efficient LLM parallel decoding
    framework based on dynamic token tree pruning and generation. ProPD features an
    advanced early pruning mechanism to efficiently eliminate unpromising token sequences
    to improve verification efficiency. Additionally, it introduces a dynamic token
    tree generation algorithm to balance the computation and parallelism of the verification
    phase in real-time and maximize the overall efficiency across different batch
    sizes, sequence lengths, and tasks, etc. We verify ProPD across a diverse set
    of datasets, LLMs, and batch sizes and demonstrate ProPD consistently outperforms
    existing decoding algorithms by 1.1-3.2 $\times$.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性大语言模型（LLMs）的最新进展显著提升了自然语言处理任务的性能。然而，由于自回归令牌生成的固有局限性，其效率受到限制。虽然并行解码与令牌树验证（例如
    Medusa）已被提出以改善解码并行性和效率，但由于其独立的令牌预测方法，通常在保持上下文关系方面存在困难，并且在大树规模和批处理情况下会产生显著的验证开销。本文提出了ProPD，一个基于动态令牌树修剪和生成的高效LLM并行解码框架。ProPD
    具有先进的早期修剪机制，以高效地消除无前景的令牌序列，从而提高验证效率。此外，它引入了一种动态令牌树生成算法，以实时平衡验证阶段的计算和并行性，并最大化不同批处理大小、序列长度和任务等条件下的总体效率。我们在一组多样的数据集、LLMs
    和批处理大小上验证了ProPD，并展示了ProPD在解码算法中始终优于现有方法，提升幅度为 1.1-3.2 倍。
- en: 1\. Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: Recent years have witnessed revolutionary advancements in generative large language
    models (LLMs) (Brown et al., [2020](#bib.bib3)), which can achieve start-of-the-art
    results in several generative natural language tasks, including summarization
    (Fabbri et al., [2019](#bib.bib6)), machine translation (Hendy et al., [2023](#bib.bib7)),
    question answering (Zaib et al., [2022](#bib.bib20)), etc. However, due to their
    large parameter size, complex architectures, and high computation requirements,
    it is extremely challenging to deploy these LLMs in real-world applications.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，生成性大语言模型（LLMs）（Brown 等人，[2020](#bib.bib3)）取得了革命性的进展，这些模型在包括摘要生成（Fabbri 等人，[2019](#bib.bib6)）、机器翻译（Hendy
    等人，[2023](#bib.bib7)）、问答（Zaib 等人，[2022](#bib.bib20)）等多个生成性自然语言任务中达到了最先进的结果。然而，由于其庞大的参数规模、复杂的架构和高计算要求，将这些LLMs应用于现实世界中极具挑战性。
- en: 'Modern LLMs generally leverage an autoregressive decoding algorithm (Radford
    et al., [2018](#bib.bib13), [2019](#bib.bib14); Brown et al., [2020](#bib.bib3)):
    they take as input a sequence of tokens and then, generate subsequent tokens one
    at a time as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ ProPD:
    Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding") (a). The
    generation of each new token is conditioned on both the input tokens and the previously
    generated tokens. While the decoding algorithm can fully capture the dependency
    between tokens and preserve the context of the generated tokens, it suffers from
    suboptimal runtime performance and limited GPU utilization. This is because the
    degree of computation parallelism is very low and hence, resulting in severe memory
    bottleneck(Kim et al., [2023](#bib.bib9)).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '现代大型语言模型通常利用自回归解码算法 (Radford et al., [2018](#bib.bib13), [2019](#bib.bib14);
    Brown et al., [2020](#bib.bib3))：它们将一系列标记作为输入，然后逐个生成后续标记，如图 [1](#S1.F1 "图 1 ‣
    1\. 引言 ‣ ProPD: LLM 并行解码的动态标记树修剪与生成") (a) 所示。每个新标记的生成都依赖于输入标记和之前生成的标记。虽然解码算法能够充分捕捉标记之间的依赖关系并保留生成标记的上下文，但它在运行时性能和
    GPU 利用率上存在不足。这是因为计算并行度非常低，从而导致严重的内存瓶颈 (Kim et al., [2023](#bib.bib9))。'
- en: '![Refer to caption](img/cacb0f24c614e2c874761de41f368499.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cacb0f24c614e2c874761de41f368499.png)'
- en: Figure 1\. Workflow of (a) autoregressive decoding, (b) parallel decoding.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. （a）自回归解码，（b）并行解码的工作流程。
- en: 'To address the inefficiency in autoregressive token generation, parallel decoding,
    e.g., Medusa (Cai et al., [2023](#bib.bib4)), has been proposed and demonstrates
    a promising speedup. Instead of decoding a single token each time, parallel decoding
    first generates a sequence of token candidates, and then, verifies all the candidates
    in parallel as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ ProPD:
    Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding") (b). The
    token candidates can be further organized as a tree structure to reduce the computation
    in the verification phase. While parallel decoding increases the computation,
    it still achieves around $2\times$ speedup. This is because, on the one hand,
    LLM decoding is mainly limited by the memory bandwidth and thus, the introduced
    computation incurs a small latency overhead; on the other hand, parallel decoding
    can opportunistically accept more tokens at each step, and hence, reduces the
    overall number of iterations.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决自回归标记生成中的低效率问题，已提出并行解码方法，如 Medusa (Cai et al., [2023](#bib.bib4))，并展示了有前景的加速效果。并行解码并不是每次解码一个标记，而是首先生成一系列标记候选，然后并行验证所有候选，如图
    [1](#S1.F1 "图 1 ‣ 1\. 引言 ‣ ProPD: LLM 并行解码的动态标记树修剪与生成") (b) 所示。标记候选还可以进一步组织成树结构，以减少验证阶段的计算。尽管并行解码增加了计算量，但仍能实现约
    $2\times$ 的加速。这是因为，一方面，LLM 解码主要受内存带宽限制，因此引入的计算开销较小；另一方面，并行解码可以在每一步机会性地接受更多标记，从而减少整体迭代次数。'
- en: 'However, we observe the existing parallel decoding method suffers from a high
    latency overhead for batch decoding. Even for small batch sizes, e.g., 4, the
    speedup of parallel decoding quickly diminishes. We observe the inefficiency of
    parallel decoding mainly comes from two folds: 1) due to a lack of contextual
    relationships among the tokens generated in parallel, a large number of tokens
    need to be verified, especially for large batch sizes; 2) the generation pattern
    of the token candidates is static and cannot consider the impact of batch sizes,
    sequence lengths, tasks, etc.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们观察到现有的并行解码方法在批量解码时存在较高的延迟开销。即使是小批量大小，例如 4，并行解码的加速效果也迅速减小。我们观察到并行解码的低效率主要有两个方面：1）由于生成的标记之间缺乏上下文关系，特别是在大批量大小时，需要验证大量标记；2）标记候选的生成模式是静态的，无法考虑批量大小、序列长度、任务等的影响。
- en: 'In this paper, we propose ProPD to enhance LLM parallel decoding with dynamic
    token tree pruning and generation. To reduce the verification overhead, we observe
    early LLM layers already demonstrate good predictive capabilities that can be
    leveraged to prune token candidates. Hence, we propose a dynamic token tree pruning
    algorithm to significantly reduce the number of candidates without harming the
    number of accepted tokens. To improve the adaptability across different decoding
    conditions, we propose a dynamic token tree generation algorithm that enables
    to adapt the generated token tree in real-time during decoding. Our contributions
    can be summarized as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了ProPD，通过动态令牌树修剪和生成来增强LLM并行解码。为了减少验证开销，我们观察到早期LLM层已经表现出良好的预测能力，可以利用这些能力来修剪令牌候选项。因此，我们提出了一种动态令牌树修剪算法，显著减少候选项的数量，而不会影响接受的令牌数量。为了提高在不同解码条件下的适应性，我们提出了一种动态令牌树生成算法，使得在解码过程中可以实时调整生成的令牌树。我们的贡献总结如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We observe the inefficiency of the existing LLM parallel decoding algorithm
    and propose ProPD to improve the decoding efficiency across different decoding
    conditions.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们观察到现有的LLM并行解码算法的低效，并提出了ProPD来提高在不同解码条件下的解码效率。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a dynamic token tree pruning algorithm to reduce the verification
    computation by more than 2$\times$ without hurting the number of accepted tokens.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种动态令牌树修剪算法，将验证计算减少超过2$\times$，而不会影响接受的令牌数量。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a real-time algorithm to generate the token tree adaptively according
    to the decoding conditions.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种实时算法，根据解码条件自适应地生成令牌树。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We verify ProPD across a diverse set of datasets, LLMs, and batch sizes, and
    demonstrate ProPD consistently outperforms existing decoding algorithms by $1.1$.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在多样的数据集、LLM和批量大小上验证了ProPD，并展示了ProPD在性能上始终优于现有的解码算法，提升了$1.1$。
- en: 2\. Preliminary
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 初步
- en: 'In this section, we introduce the existing parallel decoding algorithms and
    review the related works. As introduced in Section [1](#S1 "1\. Introduction ‣
    ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding"),
    parallel decoding mitigates the inefficiency of autoregressive decoding by generating
    and verifying token candidates in parallel. It generally has two phases, i.e.,
    the prediction phase and the verification phase.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们介绍了现有的并行解码算法，并回顾了相关的工作。如第[1](#S1 "1\. 引言 ‣ ProPD: LLM并行解码的动态令牌树修剪和生成")节介绍的，并行解码通过并行生成和验证令牌候选项来缓解自回归解码的低效。它通常有两个阶段，即预测阶段和验证阶段。'
- en: '![Refer to caption](img/c506cafc59415e5d83b8cd05c25a4423.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c506cafc59415e5d83b8cd05c25a4423.png)'
- en: 'Figure 2\. Example of parallel decoding: (a) model architecture, (b) token
    tree, (c) part of token tree mask.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 并行解码示例：（a）模型架构，（b）令牌树，（c）令牌树掩码的一部分。
- en: Prediction
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 预测
- en: 'In the Prediction phase, sequences of token candidates are predicted with a
    much lower cost compared to the baseline autoregressive decoding. Depending on
    how the token candidates are generated, existing works can be roughly classified
    into 2 categories. The first category of works leverages a much smaller LLM to
    generate the candidates (sometimes referred to as speculative decoding) (Chen
    et al., [2023](#bib.bib5); Leviathan et al., [2023](#bib.bib10); Spector and Re,
    [2023](#bib.bib16)). While promising speedup has been demonstrated (Miao et al.,
    [2023](#bib.bib11)), these works usually struggle to align the small LLM with
    the full-scale LLM and the requirement to host two models in one system also drastically
    increases the system complexity (Xu et al., [2023](#bib.bib19)). The second category
    leverages the full-scale LLM directly to generate the candidates (Stern et al.,
    [2018](#bib.bib17); Cai et al., [2023](#bib.bib4); Santilli et al., [2023](#bib.bib15);
    Bae et al., [2023](#bib.bib2); Jiang et al., [2023](#bib.bib8)). A few extra model
    heads are trained to simultaneously predict the candidates for multiple timesteps.
    While these works benefit from the system’s simplicity, they also face an important
    limitation: as shown in Figure [2](#S2.F2 "Figure 2 ‣ 2\. Preliminary ‣ ProPD:
    Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding"), the token
    candidates for different timesteps are generated in parallel without considering
    their contextual dependency. This leads to an exponential increase in the generated
    token sequences with respect to the number of timesteps to predict and the number
    of candidates at each step. For example, if we have parallel heads for $n$ for
    each timestep.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '在预测阶段，与基线自回归解码相比，标记候选序列的预测成本要低得多。根据标记候选的生成方式，现有的研究可以大致分为两类。第一类工作利用较小的**LLM**来生成候选（有时称为**推测解码**）（Chen
    et al., [2023](#bib.bib5); Leviathan et al., [2023](#bib.bib10); Spector and Re,
    [2023](#bib.bib16)）。虽然展示了有前景的加速效果（Miao et al., [2023](#bib.bib11)），但这些工作通常难以将小型**LLM**与全规模**LLM**对齐，而且在一个系统中托管两个模型的要求也大幅度增加了系统复杂性（Xu
    et al., [2023](#bib.bib19)）。第二类工作直接利用全规模**LLM**来生成候选（Stern et al., [2018](#bib.bib17);
    Cai et al., [2023](#bib.bib4); Santilli et al., [2023](#bib.bib15); Bae et al.,
    [2023](#bib.bib2); Jiang et al., [2023](#bib.bib8)）。训练了一些额外的模型头来同时预测多个时间步的候选。虽然这些工作受益于系统的简洁性，但也面临一个重要的限制：如图[2](#S2.F2
    "Figure 2 ‣ 2\. Preliminary ‣ ProPD: Dynamic Token Tree Pruning and Generation
    for LLM Parallel Decoding")所示，不同时间步的标记候选是并行生成的，而未考虑它们的上下文依赖性。这导致生成的标记序列随着预测时间步的数量和每一步的候选数量的增加呈指数增长。例如，如果我们为每个时间步有$n$个并行头。'
- en: Verification
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 验证
- en: 'Once a set of candidate token sequences is generated, the next step is to leverage
    the full-scale LLM to verify each sequence. Due to the invocation of the full-scale
    LLM, the verification phase is usually more time-consuming compared to the generation
    phase. To improve the verification efficiency, existing methods(Cai et al., [2023](#bib.bib4);
    Miao et al., [2023](#bib.bib11)) adopt token-tree verification strategies that
    parallelize the evaluation of multiple candidate sequences. Token-tree verification
    begins by exploiting common prefixes shared across candidate sequences as shown
    in Figure [2](#S2.F2 "Figure 2 ‣ 2\. Preliminary ‣ ProPD: Dynamic Token Tree Pruning
    and Generation for LLM Parallel Decoding")(b), enabling the LLM to compute the
    initial attention and hidden states once for that prefix. Unlike traditional attention
    mechanisms that compute scores in a linear sequence, tree attention needs to consider
    the branching structure where multiple potential successor tokens may exist at
    the same level. To manage this, attention masks are employed to allow each token
    to attend only to its appropriate context as shown in Figure [2](#S2.F2 "Figure
    2 ‣ 2\. Preliminary ‣ ProPD: Dynamic Token Tree Pruning and Generation for LLM
    Parallel Decoding")(c).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦生成了一组候选令牌序列，下一步是利用全尺度LLM来验证每个序列。由于调用全尺度LLM，验证阶段通常比生成阶段更耗时。为了提高验证效率，现有方法（Cai
    等，[2023](#bib.bib4)；Miao 等，[2023](#bib.bib11)）采用了并行化多个候选序列评估的令牌树验证策略。令牌树验证首先通过利用候选序列中共享的公共前缀开始，如图[2](#S2.F2
    "图 2 ‣ 2\. 初步 ‣ ProPD: 动态令牌树修剪和生成用于LLM并行解码")（b）所示，使LLM能够对该前缀进行一次初始注意力和隐藏状态计算。与传统的按线性序列计算分数的注意力机制不同，树形注意力需要考虑分支结构，其中可能在同一层级存在多个潜在的后继令牌。为此，采用了注意力掩码，允许每个令牌仅关注其适当的上下文，如图[2](#S2.F2
    "图 2 ‣ 2\. 初步 ‣ ProPD: 动态令牌树修剪和生成用于LLM并行解码")（c）所示。'
- en: 3\. Motivation and Observation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 动机与观察
- en: 'The effectiveness of parallel decoding is concurrently influenced by both the
    number of accepted token candidates (denoted as acceptance length) and the token
    tree verification overhead. While a large token tree size increases the acceptance
    length, it also drastically increases the verification iteration time. Hence,
    in order to achieve maximum acceleration from parallel decoding, it is necessary
    to strike a balance between the two impacting factors. Existing parallel decoding
    algorithms cannot handle the two factors well due to a lack of contextual relationships
    in the sequence (Stern et al., [2018](#bib.bib17); Cai et al., [2023](#bib.bib4)).
    As introduced in Section [2](#S2 "2\. Preliminary ‣ ProPD: Dynamic Token Tree
    Pruning and Generation for LLM Parallel Decoding"), while the candidate tokens
    are generated in parallel, they have to be verified in sequences to capture the
    context. Hence, directly verifying all the potential sequences results in an exponential
    increase in computation complexity. We make the following observations that motivate
    us to propose ProPD.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '并行解码的有效性同时受到接受的令牌候选数量（记作接受长度）和令牌树验证开销的影响。虽然较大的令牌树尺寸增加了接受长度，但也会大幅提高验证迭代时间。因此，为了从并行解码中获得最大加速，有必要在这两个影响因素之间取得平衡。现有的并行解码算法由于缺乏序列中的上下文关系，无法很好地处理这两个因素（Stern
    等，[2018](#bib.bib17)；Cai 等，[2023](#bib.bib4)）。正如第[2](#S2 "2\. 初步 ‣ ProPD: 动态令牌树修剪和生成用于LLM并行解码")节中介绍的，虽然候选令牌是并行生成的，但它们必须按顺序进行验证以捕捉上下文。因此，直接验证所有潜在序列会导致计算复杂度呈指数级增长。我们做出了以下观察，这促使我们提出ProPD。'
- en: 'Observation 1: early LLM layers demonstrate strong predictive capabilities.
    Figure [3](#S3.F3 "Figure 3 ‣ 3\. Motivation and Observation ‣ ProPD: Dynamic
    Token Tree Pruning and Generation for LLM Parallel Decoding")(a) shows the prediction
    accuracy of the early layers by training their own heads. While the early layers
    did not exhibit high precision for their topmost predictions (Top-1), they showed
    a remarkable increase in accuracy within a higher Top-K range. For example, in
    Layer 2, the Top-1 accuracy was approximately 37%, but this accuracy increased
    substantially when considering the top 50 tokens, hinting at the layers’ capacity
    to filter out a significant number of implausible tokens. This trend supports
    the hypothesis that early layers, while not fully equipped to make the final prediction,
    are indeed effective in discerning a broad set of unlikely token candidates. Therefore,
    tokens falling outside an optimal Top-$k$ range can be pruned early, reducing
    the computational load in subsequent layers without significantly affecting the
    overall predictive accuracy.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '观察 1：早期 LLM 层显示出强大的预测能力。图 [3](#S3.F3 "Figure 3 ‣ 3\. Motivation and Observation
    ‣ ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding")(a)
    显示了早期层通过训练其自身的头部的预测准确性。尽管早期层对其最上层预测（Top-1）的精度不高，但在较高的 Top-K 范围内显示出了显著的准确性提高。例如，在第
    2 层中，Top-1 准确率约为 37%，但当考虑前 50 个令牌时，这一准确性显著提高，暗示这些层具有筛选大量不可信令牌的能力。这一趋势支持了这样一个假设：早期层虽然未完全具备做出最终预测的能力，但确实在辨别一组不太可能的令牌候选项方面有效。因此，落在最优
    Top-$k$ 范围之外的令牌可以提前修剪，减少后续层的计算负担，而不会显著影响整体预测准确性。'
- en: '![Refer to caption](img/097a58e9770cf9e7483fc7d1dc63a996.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/097a58e9770cf9e7483fc7d1dc63a996.png)'
- en: 'Figure 3\. Iteration time and acceptance length under different scenarios:
    (a) Top-$k$ accuracy under different early layers, (b) average iteration time
    under different batch sizes and token tree sizes, (c) average iteration time under
    different sequence lengths, (d) average acceptance length under different tasks.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 在不同场景下的迭代时间和接受长度：(a) 不同早期层下的 Top-$k$ 准确率，(b) 不同批量大小和令牌树大小下的平均迭代时间，(c)
    不同序列长度下的平均迭代时间，(d) 不同任务下的平均接受长度。
- en: 'Observation 2: the expected speedup of parallel decoding depends on inference
    batch size, sequence length, tasks, etc. As shown in Figure [3](#S3.F3 "Figure
    3 ‣ 3\. Motivation and Observation ‣ ProPD: Dynamic Token Tree Pruning and Generation
    for LLM Parallel Decoding")(b) to (d), given a fixed token tree size, the iteration
    time of the verification phase varies with different batch sizes, sequence lengths,
    and hardware platforms. Meanwhile, the token accept probability also changes for
    different datasets. Hence, the expected speedup of parallel decoding is directly
    impacted by all these factors. BPD (Stern et al., [2018](#bib.bib17)) proposes
    to only verify the prediction with the highest probability for each token while
    Medusa (Cai et al., [2023](#bib.bib4)) proposes a heuristic design for the token
    tree, both of which are sub-optimal due to the fixed tree size. As shown in Figure [3](#S3.F3
    "Figure 3 ‣ 3\. Motivation and Observation ‣ ProPD: Dynamic Token Tree Pruning
    and Generation for LLM Parallel Decoding")(b), we further observe the verification
    iteration time scales highly linearly with the token tree size. This is because
    the computation of a transformer block, including both the fully connected layers
    and the attention, increases proportionally with the token tree size.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '观察 2：并行解码的预期加速取决于推理批量大小、序列长度、任务等。如图 [3](#S3.F3 "Figure 3 ‣ 3\. Motivation and
    Observation ‣ ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel
    Decoding")(b) 至 (d) 所示，给定固定的令牌树大小，验证阶段的迭代时间会随着不同的批量大小、序列长度和硬件平台而变化。同时，令牌接受概率在不同数据集上也会有所不同。因此，并行解码的预期加速直接受到这些因素的影响。BPD
    (Stern 等人，[2018](#bib.bib17)) 提出仅验证每个令牌的最高概率预测，而 Medusa (Cai 等人，[2023](#bib.bib4))
    提出了令牌树的启发式设计，这两者由于固定树大小而都是次优的。如图 [3](#S3.F3 "Figure 3 ‣ 3\. Motivation and Observation
    ‣ ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding")(b)
    所示，我们进一步观察到验证迭代时间与令牌树大小高度线性相关。这是因为，变换器块的计算，包括全连接层和注意力机制，随着令牌树大小的增加而成比例增加。'
- en: Considering that batch size remains relatively stable and sequence length exhibits
    only changes gradually, this linear scaling enables us to employ linear regression
    models to predict the verification time based on token tree size accurately. This
    predictive capability enables us to dynamically adjust the token tree size in
    real time, optimizing for inference speed and computational efficiency.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到批量大小相对稳定，而序列长度只逐渐变化，这种线性缩放使我们能够使用线性回归模型准确预测基于标记树大小的验证时间。这种预测能力使我们能够实时动态调整标记树大小，从而优化推理速度和计算效率。
- en: '4\. ProPD: Parallel Decoding with Token Tree Pruning and Generation'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. ProPD：带标记树剪枝和生成的并行解码
- en: 'The brief workflow of our framework is shown in [4](#S4.F4 "Figure 4 ‣ 4\.
    ProPD: Parallel Decoding with Token Tree Pruning and Generation ‣ ProPD: Dynamic
    Token Tree Pruning and Generation for LLM Parallel Decoding"). Given the baseline
    parallel decoding framework, we first propose an early pruning algorithm to remove
    unlikely token candidates in early LLM layers. The proposed early pruning algorithm
    helps address the limitations of missing contextual relationships of parallel
    decoding and reduces the computation of the verification phase. Then, we further
    propose a dynamic token tree generation algorithm to enable real-time adjustment
    of ProPD and enables to adapt ProPD to varying trade-offs in different decoding
    conditions.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '我们框架的简要工作流程如图[4](#S4.F4 "Figure 4 ‣ 4\. ProPD: Parallel Decoding with Token
    Tree Pruning and Generation ‣ ProPD: Dynamic Token Tree Pruning and Generation
    for LLM Parallel Decoding")所示。基于基线并行解码框架，我们首先提出了一个早期剪枝算法，用于去除在早期LLM层中不太可能的标记候选。所提出的早期剪枝算法有助于解决并行解码的上下文关系缺失问题，并减少验证阶段的计算。然后，我们进一步提出了一个动态标记树生成算法，以实现ProPD的实时调整，并使ProPD能够适应不同解码条件下的变化权衡。'
- en: '![Refer to caption](img/4e2fb56ef0c6037d3337be1421505247.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4e2fb56ef0c6037d3337be1421505247.png)'
- en: Figure 4\. Overview of ProPD.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. ProPD概述。
- en: 4.1\. Early Pruning Algorithm
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 早期剪枝算法
- en: 'As introduced in Section [2](#S2 "2\. Preliminary ‣ ProPD: Dynamic Token Tree
    Pruning and Generation for LLM Parallel Decoding"), due to the lack of contextual
    relationships, existing parallel decoding algorithms suffer from an exponential
    increase in token tree size. While verifying the whole token tree naively incurs
    significant computation overhead, Our first observation in Section [3](#S3 "3\.
    Motivation and Observation ‣ ProPD: Dynamic Token Tree Pruning and Generation
    for LLM Parallel Decoding") indicates early LLM layers demonstrate strong predictive
    capabilities, which makes it possible for the early pruning of the token tree
    to reduce the computation in the verification phase. We focus on answering the
    following three questions concerning the early pruning algorithm: 1) how to select
    the token candidates for pruning; 2) what are the key design choices to make for
    the pruning; 3) how to reduce the pruning overhead to reduce decoding latency.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[2](#S2 "2\. Preliminary ‣ ProPD: Dynamic Token Tree Pruning and Generation
    for LLM Parallel Decoding")节所述，由于缺乏上下文关系，现有的并行解码算法会导致标记树大小的指数增长。虽然对整个标记树的天真验证会带来显著的计算开销，但我们在第[3](#S3
    "3\. Motivation and Observation ‣ ProPD: Dynamic Token Tree Pruning and Generation
    for LLM Parallel Decoding")节中的首次观察表明，早期LLM层显示出强大的预测能力，这使得标记树的早期剪枝成为可能，从而减少验证阶段的计算。我们关注以下三个问题：1）如何选择剪枝的标记候选；2）剪枝的关键设计选择是什么；3）如何减少剪枝开销以降低解码延迟。'
- en: Pruning Criterion
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 剪枝标准
- en: 'To select the pruning tokens, we add an early prediction head after a few LLM
    layers as shown in Figure [5](#S4.F5 "Figure 5 ‣ Implementation Optimization ‣
    4.1\. Early Pruning Algorithm ‣ 4\. ProPD: Parallel Decoding with Token Tree Pruning
    and Generation ‣ ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel
    Decoding"). We consider the following two criteria: Top-$k$ criterion.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '为了选择剪枝标记，我们在图[5](#S4.F5 "Figure 5 ‣ Implementation Optimization ‣ 4.1\. Early
    Pruning Algorithm ‣ 4\. ProPD: Parallel Decoding with Token Tree Pruning and Generation
    ‣ ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding")所示的几个LLM层之后添加了一个早期预测头。我们考虑以下两个标准：Top-$k$标准。'
- en: Pruning Process
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 剪枝过程
- en: 'Let $n$ denote the number of LLM layers before the pruning. Then, the early
    pruning process can now be described as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 设$n$表示剪枝前的LLM层数。那么，早期剪枝过程可以描述如下：
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prediction of successor tokens: after $n$.'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预测后续标记：在$n$之后。
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Token pruning: the next token $x_{i+1}$ is deemed contextually implausible.'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标记剪枝：下一个标记$x_{i+1}$被认为在上下文中不可信。
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Branch elimination: collect all the tokens that fail the Top-$k$ criterion
    and their associated token sequences are pruned from the token tree.'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分支淘汰：收集所有未通过 Top-$k$ 标准的令牌及其相关的令牌序列，并从令牌树中修剪这些序列。
- en: While token tree pruning will not impact the correctness of the decoding, the
    selection of $n$ is crucial in balancing computational efficiency and acceptance
    length in our early pruning structure. Earlier pruning layers reduce computational
    load but may lead to less accurate pruning decisions. A larger K value increases
    the likelihood of retaining contextually relevant sequences but also enlarges
    the token tree, impacting computational efficiency. These parameters will be empirically
    optimized in our experiments, aiming to find a balance that maximizes both the
    efficiency and accuracy of the pruning process. The experimental results will
    guide the final selection of these critical parameters.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管令牌树修剪不会影响解码的正确性，但在我们的早期修剪结构中，$n$ 的选择对于平衡计算效率和接受长度至关重要。较早的修剪层减少了计算负担，但可能导致修剪决策的准确性降低。较大的
    K 值增加了保留上下文相关序列的可能性，但也会增大令牌树，从而影响计算效率。这些参数将在我们的实验中通过经验优化，旨在找到最大化修剪过程效率和准确性的平衡。实验结果将指导这些关键参数的最终选择。
- en: Implementation Optimization
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 实现优化
- en: In the branch elimination step, token branches are eliminated and new attention
    masks need to be generated. We empirically find this step can be time-consuming
    on GPU if naively implemented, e.g., re-generate the mask each time after pruning
    and send the mask tensor from CPU to GPU. We propose to cache the mask on GPU
    and also subsample the cached mask instead of generating a new one. Such simple
    optimization enables to reduce the latency overhead significantly.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在分支淘汰步骤中，令牌分支被淘汰，需要生成新的注意力掩码。我们通过经验发现，如果盲目实现，这一步骤在 GPU 上可能非常耗时，例如，每次修剪后重新生成掩码并将掩码张量从
    CPU 发送到 GPU。我们建议在 GPU 上缓存掩码，并从缓存的掩码中进行子采样，而不是生成新的掩码。这种简单的优化显著减少了延迟开销。
- en: '![Refer to caption](img/febfb0a76457a44da3629be14c4883e4.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/febfb0a76457a44da3629be14c4883e4.png)'
- en: Figure 5\. Early Pruning Algorithm of ProPD.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. ProPD 的早期修剪算法。
- en: 4.2\. Dynamic Token Tree Generation
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 动态令牌树生成
- en: 'As analyzed in section [3](#S3 "3\. Motivation and Observation ‣ ProPD: Dynamic
    Token Tree Pruning and Generation for LLM Parallel Decoding"), the effectiveness
    of parallel decoding is influenced by both the acceptance length and the token
    tree verification overhead, which are impacted by the decoding conditions, including
    batch size, sequence length, etc. Thus, we propose the dynamic token tree generation
    methodology to maximize decoding efficiency by balancing the length of accepted
    predictions and the computational overhead. We focus on answering the following
    two questions concerning the dynamic generation algorithm: 1) how to estimate
    the computation overhead and 2) how to estimate the probable acceptance length.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[3](#S3 "3\. 动机与观察 ‣ ProPD：LLM 并行解码的动态令牌树修剪与生成")节分析，平行解码的有效性受到接受长度和令牌树验证开销的影响，这些都受到解码条件（包括批量大小、序列长度等）的影响。因此，我们提出了动态令牌树生成方法，通过平衡接受预测的长度和计算开销来最大化解码效率。我们重点解决动态生成算法的两个问题：1)
    如何估计计算开销，以及 2) 如何估计可能的接受长度。
- en: 4.2.1\. Verification Overhead Estimation
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 验证开销估计
- en: 'Building on Observation 2 in Section [3](#S3 "3\. Motivation and Observation
    ‣ ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding"),
    which reveals a linear relationship between the token tree size and iteration
    time, our framework adopts a weighted regression model for real-time estimation
    of this relationship.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于第[3](#S3 "3\. 动机与观察 ‣ ProPD：LLM 并行解码的动态令牌树修剪与生成")节中的观察 2，揭示了令牌树大小与迭代时间之间的线性关系，我们的框架采用加权回归模型来实时估计这种关系。
- en: Model Formulation
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型公式化
- en: We formalize the weighted regression model. We denote the average iteration
    time for a token tree of size $i$.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加权回归模型公式化。我们表示大小为 $i$ 的令牌树的平均迭代时间。
- en: Estimation Process
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 估计过程
- en: 'The estimation involves several steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 估计过程包括几个步骤：
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Update average iteration time: given newly collected data $(i,t_{i})$ as:'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更新平均迭代时间：给定新收集的数据 $(i,t_{i})$ 为：
- en: '|  | $\displaystyle T_{perf}^{i}\leftarrow(1-\alpha)T_{perf}^{i}+\alpha t_{i},$
    |  |'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle T_{perf}^{i}\leftarrow(1-\alpha)T_{perf}^{i}+\alpha t_{i},$
    |  |'
- en: where $\alpha$ exists.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中存在 $\alpha$。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Weighted regression: Next, we compute the weight for each token tree size and
    prioritize recent updates:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 加权回归：接下来，我们计算每个令牌树大小的权重，并优先考虑最近的更新：
- en: '|  | $\displaystyle W_{i}=e^{-\lambda o_{i}}\quad\forall i\in S$ |  |'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle W_{i}=e^{-\lambda o_{i}}\quad\forall i\in S$ |  |'
- en: In this equation, $o_{i}$s with more frequent updates are more important since
    they tend to be selected.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个方程中，更新频繁的 $o_{i}$ 更重要，因为它们更可能被选择。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Solve the regression model: finally, the regression model parameters can be
    determined as:'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 求解回归模型：最后，回归模型参数可以确定为：
- en: '|  | $1$2 |  |'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: $\hat{\beta_{0}},\hat{\beta_{1}}$ can be solved analytically with negligible
    latency.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\hat{\beta_{0}},\hat{\beta_{1}}$ 可以通过解析方法解决，延迟可忽略不计。
- en: 4.2.2\. Probability Estimation
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 概率估计
- en: 'To estimate the probable acceptance length with a given token tree, we track
    the output words from each head and record their accuracy in runtime. For each
    head, when a token is decoded, the framework collects how often the actual token
    was within the top-K predictions of that head, which is note as $P$ is finally
    determined, the probability of the head i will be updated:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计给定令牌树的可能接受长度，我们跟踪每个头的输出词，并记录其运行时的准确性。对于每个头，当一个令牌被解码时，框架会收集实际令牌在该头的 Top-K
    预测中出现的频率，一旦 $P$ 最终确定，头 i 的概率将被更新：
- en: '|  | $\displaystyle P_{i}^{k}\leftarrow(1-\alpha)\cdot P_{i}^{k}+\alpha\cdot\mathrm{1}(x_{i+1}^{t}\in\mathrm{TopK}(x_{i}^{t})),$
    |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P_{i}^{k}\leftarrow(1-\alpha)\cdot P_{i}^{k}+\alpha\cdot\mathrm{1}(x_{i+1}^{t}\in\mathrm{TopK}(x_{i}^{t})),$
    |  |'
- en: 'where $\mathrm{1}(\cdot)$:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathrm{1}(\cdot)$：
- en: '|  | $\displaystyle p_{i}^{k}=P_{i}^{k}-P_{i}^{k-1}$ |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{i}^{k}=P_{i}^{k}-P_{i}^{k-1}$ |  |'
- en: 'Given a random sequence seq = [$s_{0}^{k_{0}},s_{1}^{k_{1}},...s_{n}^{k_{n}}$
    in sequence is:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个随机序列 seq = [$s_{0}^{k_{0}},s_{1}^{k_{1}},...s_{n}^{k_{n}}$ 的序列是：
- en: '|  | $\displaystyle l(\text{seq})=\prod_{i=0}^{n}p_{i}^{k_{i}}$ |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle l(\text{seq})=\prod_{i=0}^{n}p_{i}^{k_{i}}$ |  |'
- en: 'in which $k_{i}$ token of head i. For example, in Figure [6](#S4.F6 "Figure
    6 ‣ 4.2.2\. Probability Estimation ‣ 4.2\. Dynamic Token Tree Generation ‣ 4\.
    ProPD: Parallel Decoding with Token Tree Pruning and Generation ‣ ProPD: Dynamic
    Token Tree Pruning and Generation for LLM Parallel Decoding"), the average probabilities
    of Top-2 tokens of the heads are shown in (a). Then, the expected acceptance length
    of token ’b’ in sequence ’ab’ is 0.6, and the expected acceptance length of token
    ’e’ in sequence ’abe’ is 0.06\. If we choose ’abd’ and ’ac’ as the token tree,
    its estimated acceptance length is 1.88 as shown in Figure [6](#S4.F6 "Figure
    6 ‣ 4.2.2\. Probability Estimation ‣ 4.2\. Dynamic Token Tree Generation ‣ 4\.
    ProPD: Parallel Decoding with Token Tree Pruning and Generation ‣ ProPD: Dynamic
    Token Tree Pruning and Generation for LLM Parallel Decoding")(b).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $k_{i}$ 是头 i 的令牌。例如，在图 [6](#S4.F6 "图 6 ‣ 4.2.2\. 概率估计 ‣ 4.2\. 动态令牌树生成 ‣ 4\.
    ProPD：并行解码中的令牌树剪枝和生成 ‣ ProPD：LLM 并行解码的动态令牌树剪枝和生成") 中，展示了头的 Top-2 令牌的平均概率，如 (a)
    所示。然后，序列 ’ab’ 中令牌 ’b’ 的期望接受长度为 0.6，而序列 ’abe’ 中令牌 ’e’ 的期望接受长度为 0.06。如果我们选择 ’abd’
    和 ’ac’ 作为令牌树，则其估计的接受长度为 1.88，如图 [6](#S4.F6 "图 6 ‣ 4.2.2\. 概率估计 ‣ 4.2\. 动态令牌树生成
    ‣ 4\. ProPD：并行解码中的令牌树剪枝和生成 ‣ ProPD：LLM 并行解码的动态令牌树剪枝和生成")(b) 所示。
- en: '![Refer to caption](img/f36b48f0f950090e38ebea9f90d71cdf.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f36b48f0f950090e38ebea9f90d71cdf.png)'
- en: 'Figure 6\. Probability estimation of token tree: (a) Top-$k$ probability of
    each head, (b) expected accuracy of each token.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 令牌树的概率估计：(a) 每个头的 Top-$k$ 概率，(b) 每个令牌的期望准确度。
- en: 4.2.3\. Optimizing Efficiency
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 优化效率
- en: 'With the verification overhead estimation and accept probability estimation,
    we can calculate the estimated speed of parallel decoding for each token tree
    size i:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过验证开销估计和接受概率估计，我们可以计算每个令牌树大小 i 的并行解码的估计速度：
- en: '|  | $\displaystyle v=l(i)/T_{est}^{i}$ |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle v=l(i)/T_{est}^{i}$ |  |'
- en: Then we can swiftly identify the optimal token tree size for the fastest estimation
    by scanning the list once. Note we do not need to invoke the dynamic token tree
    generation each iteration during the decoding. Instead, it is invoked when the
    batch sizes or sequence lengths change significantly. Hence, its efficiency impact
    is minimal and it can help avoid expensive pre-characterization of different decoding
    conditions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以通过扫描列表一次迅速识别出最快估计的最佳令牌树大小。注意，我们在解码过程中不需要每次迭代都调用动态令牌树生成，而是在批量大小或序列长度发生显著变化时调用。因此，其效率影响最小，并且可以帮助避免对不同解码条件的昂贵预处理。
- en: 5\. Experimental Results
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 实验结果
- en: 5.1\. Experiment Setup
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 实验设置
- en: 'ProPD is implemented based on the Medusa framework (Cai et al., [2023](#bib.bib4)).
    We benchmark our framework on the open-source LLM Vicuna models (Zheng et al.,
    [2023](#bib.bib21)), which is finetuned on LLAMA. We test the 7b, 13b, and 33b
    models to demonstrate the scalability of ProPD under different model sizes. We
    evaluate our framework against the autoregressive decoding baseline, BPD, and
    Medusa. BPD (Stern et al., [2018](#bib.bib17)) is the first parallel decoding
    framework. Medusa is the SOTA parallel decoding algorithm equipped with token-tree
    attention. We evaluate different frameworks on three conversational datasets:
    Mt Bench(Zheng et al., [2023](#bib.bib21)), ChatGPT Prompts(MohamedRashad, [[n. d.]](#bib.bib12)),
    and Alpaca(Taori et al., [2023](#bib.bib18)). Following (Miao et al., [2023](#bib.bib11)),
    we only use the questions from these datasets to form our input prompts to simulate
    the real-world conversation trace. To make a fair comparison, all of the methods
    take greedy decoding to guarantee the same output with the large model. We follow
    (Cai et al., [2023](#bib.bib4)) and mainly evaluate the efficiency of ProPD based
    on the number of generated tokens per second.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ProPD 是基于 Medusa 框架（Cai et al., [2023](#bib.bib4)）实现的。我们在开源 LLM Vicuna 模型（Zheng
    et al., [2023](#bib.bib21)）上基准测试了我们的框架，该模型在 LLAMA 上进行了微调。我们测试了 7b、13b 和 33b 模型，以展示
    ProPD 在不同模型大小下的可扩展性。我们将我们的框架与自回归解码基线 BPD 和 Medusa 进行了比较。BPD（Stern et al., [2018](#bib.bib17)）是第一个并行解码框架。Medusa
    是具有 token-tree 注意力的 SOTA 并行解码算法。我们在三个对话数据集上评估了不同的框架：Mt Bench（Zheng et al., [2023](#bib.bib21)）、ChatGPT
    Prompts（MohamedRashad, [[n. d.]](#bib.bib12)）和 Alpaca（Taori et al., [2023](#bib.bib18)）。按照（Miao
    et al., [2023](#bib.bib11)）的方法，我们仅使用这些数据集中的问题来形成输入提示，以模拟实际的对话轨迹。为了公平比较，所有方法都采用贪婪解码，以保证与大模型相同的输出。我们按照（Cai
    et al., [2023](#bib.bib4)）的方法，主要基于每秒生成的 token 数量来评估 ProPD 的效率。
- en: 5.2\. Main Results
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 主要结果
- en: '![Refer to caption](img/dda4a1319dd7018b43b909f2815a798f.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/dda4a1319dd7018b43b909f2815a798f.png)'
- en: Figure 7\. Inference speed comparison between ProPD and the baselines under
    different model sizes, tasks and batch sizes.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 在不同模型大小、任务和批量大小下，ProPD 与基线的推断速度比较。
- en: We investigate the generation performance of ProPD on various model sizes, datasets
    and batch sizes. The experiments of the 7b model are conducted on A6000, and the
    experiments of 13b and 33b are conducted on A100\. The 33b model under batch size
    of 4 and 8 are loaded in 4 bit through the transformers library to satisfy the
    memory limitation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了 ProPD 在不同模型大小、数据集和批量大小上的生成性能。7b 模型的实验在 A6000 上进行，13b 和 33b 模型的实验在 A100
    上进行。为了满足内存限制，33b 模型在批量大小为 4 和 8 时通过 transformers 库以 4 位加载。
- en: 'As shown in figure [7](#S5.F7 "Figure 7 ‣ 5.2\. Main Results ‣ 5\. Experimental
    Results ‣ ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding"),
    the experimental results demonstrate the effectiveness of ProPD in enhancing the
    efficiency of parallel decoding in LLMs. The method not only accelerates the decoding
    process but also scales effectively with increased batch sizes, a crucial factor
    for practical applications. The comparison with traditional autoregressive, Medusa,
    and BPD methods across different model sizes and batch configurations consistently
    illustrates ProPD’s superior performance, marking it as a significant advancement
    in the field of efficient language model decoding. Table [1](#S5.T1 "Table 1 ‣
    5.2\. Main Results ‣ 5\. Experimental Results ‣ ProPD: Dynamic Token Tree Pruning
    and Generation for LLM Parallel Decoding") shows the average speedup of ProPD
    against the autoregressive decoding method under different batch sizes and model
    sizes. ProPD can achieve 1.33-1.95$\times$ speedup under various scenarios.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [7](#S5.F7 "图 7 ‣ 5.2\. 主要结果 ‣ 5\. 实验结果 ‣ ProPD: 动态 Token 树修剪与 LLM 并行解码生成")
    所示，实验结果展示了 ProPD 在提升 LLM 并行解码效率方面的有效性。该方法不仅加速了解码过程，而且在批量大小增加时有效扩展，这对实际应用至关重要。与传统自回归、Medusa
    和 BPD 方法在不同模型大小和批量配置下的比较一致地展示了 ProPD 的优越性能，标志着其在高效语言模型解码领域的重大进展。表 [1](#S5.T1 "表
    1 ‣ 5.2\. 主要结果 ‣ 5\. 实验结果 ‣ ProPD: 动态 Token 树修剪与 LLM 并行解码生成") 显示了 ProPD 相对于自回归解码方法在不同批量大小和模型大小下的平均加速比。ProPD
    在各种场景下可以实现 1.33-1.95$\times$ 的加速。'
- en: Table 1\. Average speedup against autoregressive decoding.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 相对于自回归解码的平均加速比。
- en: '|  | Batch Size |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | 批量大小 |'
- en: '| --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Model Size | 1 | 2 | 4 | 8 | 16 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 模型大小 | 1 | 2 | 4 | 8 | 16 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 7b | 1.95$\times$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 7b | 1.95$\times$ |'
- en: '| 13b | 1.67$\times$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 13b | 1.67$\times$ |'
- en: '| 33b | 1.86$\times$ | / |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 33b | 1.86$\times$ | / |'
- en: 5.3\. Early Pruning Accuracy
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 早期剪枝准确率
- en: Table 2\. Early pruning rate, accuracy and generation speed under BS=4 of different
    layers and Top-$k$ choice.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 不同层次和 Top-$k$ 选择下 BS=4 的早期剪枝率、准确率和生成速度。
- en: '| Layer | Top-K | Prune Rate | AccLength | Speed |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 层 | Top-K | 剪枝率 | 接受长度 | 速度 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| w/o pruning | - | - | 2.46 | 28.43 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| w/o pruning | - | - | 2.46 | 28.43 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | 50 | 79.0% | 2.26 | 43.26 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 50 | 79.0% | 2.26 | 43.26 |'
- en: '| 100 | 73.0% | 2.32 | 43.19 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 73.0% | 2.32 | 43.19 |'
- en: '| 150 | 68.6% | 2.35 | 42.98 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 150 | 68.6% | 2.35 | 42.98 |'
- en: '| 200 | 64.7% | 2.41 | 42.87 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 64.7% | 2.41 | 42.87 |'
- en: '| 2 | 50 | 77.3% | 2.32 | 43.01 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 50 | 77.3% | 2.32 | 43.01 |'
- en: '| 100 | 70.9% | 2.37 | 42.94 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 70.9% | 2.37 | 42.94 |'
- en: '| 150 | 65.5% | 2.42 | 42.85 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 150 | 65.5% | 2.42 | 42.85 |'
- en: '| 200 | 61.5% | 2.44 | 42.67 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 61.5% | 2.44 | 42.67 |'
- en: '| 3 | 50 | 76.6% | 2.32 | 42.66 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 50 | 76.6% | 2.32 | 42.66 |'
- en: '| 100 | 68.3% | 2.44 | 42.69 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 68.3% | 2.44 | 42.69 |'
- en: '| 150 | 63.0% | 2.43 | 42.48 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 150 | 63.0% | 2.43 | 42.48 |'
- en: '| 200 | 59.0% | 2.46 | 42.28 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 59.0% | 2.46 | 42.28 |'
- en: '| 4 | 50 | 74.0% | 2.43 | 42.28 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 50 | 74.0% | 2.43 | 42.28 |'
- en: '| 100 | 66.9% | 2.45 | 42.20 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 66.9% | 2.45 | 42.20 |'
- en: '| 150 | 62.1% | 2.48 | 42.10 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 150 | 62.1% | 2.48 | 42.10 |'
- en: '| 200 | 57.6% | 2.49 | 41.94 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 57.6% | 2.49 | 41.94 |'
- en: The target of the early pruning method is to maintain the original acceptance
    length while pruning a substantial proportion of branches. This is a critical
    aspect of our method’s efficacy, as it balances the need for computational efficiency
    with the integrity of the generated token sequences.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 早期剪枝方法的目标是在剪枝大量分支的同时保持原始接受长度。这是我们方法有效性的关键方面，因为它在计算效率和生成的标记序列的完整性之间取得了平衡。
- en: 'Table [2](#S5.T2 "Table 2 ‣ 5.3\. Early Pruning Accuracy ‣ 5\. Experimental
    Results ‣ ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding")
    shows the pruning rate and acceptance length of ProPD under different pruning
    layers and topK choice. In the early layers, the acceptance length can remain
    close to the baseline set by Medusa with a high pruning rate. Note that the average
    acceptance length may increase after pruning, such as in layer 4 top 200\. This
    is because pruning may change the sequence of positions at which the model performs
    parallel decoding when it prune the correct tokens. It might end up generating
    longer sequences at these certain positions.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [2](#S5.T2 "Table 2 ‣ 5.3\. Early Pruning Accuracy ‣ 5\. Experimental Results
    ‣ ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding")
    显示了在不同剪枝层和 topK 选择下 ProPD 的剪枝率和接受长度。在早期层中，接受长度可以保持接近 Medusa 设置的基线，同时剪枝率很高。注意，剪枝后平均接受长度可能会增加，例如在第
    4 层 top 200\。这是因为剪枝可能会改变模型进行并行解码时的位置序列，当它剪枝正确的标记时，可能会在这些特定位置生成更长的序列。'
- en: We finally chose to implement early pruning at the 4-th layer of the model,
    with a TopK setting of 50 based on the experimental observation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终选择在模型的第 4 层实施早期剪枝，根据实验观察设置 TopK 为 50。
- en: 5.4\. Ablation Study
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 消融研究
- en: Table 3\. Ablation study of ProPD.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. ProPD 的消融研究。
- en: '| Early Pruning | Dynamic Generation | 7b | 13b | 33b |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 早期剪枝 | 动态生成 | 7b | 13b | 33b |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| BS=1 | BS=2 | BS=4 | BS=8 | BS=16 | BS=2 | BS=2 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| BS=1 | BS=2 | BS=4 | BS=8 | BS=16 | BS=2 | BS=2 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| x | x | 1$\times$ |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| x | x | 1$\times$ |'
- en: '| ✓ | x | 0.99$\times$ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | x | 0.99$\times$ |'
- en: '| x | ✓ | 1.04$\times$ |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| x | ✓ | 1.04$\times$ |'
- en: '| ✓ | ✓ | 1.04$\times$ |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | 1.04$\times$ |'
- en: 'We further conduct a breakdown analysis of the benefit brought by each of ProPD’s
    techniques. Performance was measured across different batch sizes (BS=1 to BS=16)
    and model sizes (7b, 13b, and 33b). The results are illustrated in figure [3](#S5.T3
    "Table 3 ‣ 5.4\. Ablation Study ‣ 5\. Experimental Results ‣ ProPD: Dynamic Token
    Tree Pruning and Generation for LLM Parallel Decoding").'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步对 ProPD 各种技术带来的收益进行了详细分析。性能在不同的批次大小（BS=1 至 BS=16）和模型大小（7b、13b 和 33b）下进行了测量。结果如图[3](#S5.T3
    "Table 3 ‣ 5.4\. Ablation Study ‣ 5\. Experimental Results ‣ ProPD: Dynamic Token
    Tree Pruning and Generation for LLM Parallel Decoding")所示。'
- en: 5.4.1\. Early Pruning Only
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1\. 仅早期剪枝
- en: Early pruning demonstrates excellent acceleration effects when the batch size
    is large. However, a slight decrease in performance was observed at BS=1 in the
    7b model configuration when early pruning was applied independently. This suggests
    that the benefits of early pruning are less pronounced when the computational
    overhead is minimal.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 早期剪枝在批量大小较大时表现出优异的加速效果。然而，当在7b模型配置中独立应用早期剪枝时，BS=1的性能略有下降。这表明，当计算开销最小的情况下，早期剪枝的优势不那么明显。
- en: 5.4.2\. Dynamic Generation Only
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2\. 仅动态生成
- en: Dynamic generation alone consistently improved performance across all batch
    sizes and models. This improvement underscores the efficacy of dynamic generation
    in enhancing the model’s ability to handle multiple predictions simultaneously,
    thus providing a clear performance boost.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 仅动态生成在所有批量大小和模型中均能持续提高性能。这一改进突显了动态生成在增强模型同时处理多个预测能力方面的有效性，从而提供了明显的性能提升。
- en: 5.4.3\. Combined Early Pruning and Dynamic Generation
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3\. 结合早期剪枝与动态生成
- en: The most significant performance improvements were observed when both early
    pruning and dynamic generation were employed simultaneously. The synergistic effect
    of these techniques was particularly evident at larger batch sizes (BS=4 and beyond),
    where the combined approach outperformed all other configurations. For instance,
    in the 7b model at BS=16, the performance index reached 3.28, indicating over
    three times the baseline performance. This because when batch size is large, the
    dynamic generation only method will lead to ultra small token tree and acceptance
    length. The pruning method can enable our framework to take larger token tree
    size and have bigger acceptance length.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当同时使用早期剪枝和动态生成时，观察到最显著的性能提升。这些技术的协同效果在较大的批量大小（BS=4及以上）时尤为明显，其中结合的方法优于所有其他配置。例如，在BS=16的7b模型中，性能指标达到了3.28，表示性能超过基线的三倍以上。这是因为在批量大小较大时，仅动态生成方法将导致超小的令牌树和接受长度。剪枝方法能够使我们的框架处理更大的令牌树大小和更大的接受长度。
- en: 6\. Conclusion
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: In this paper, we propose ProPD, a framework that accelerates the parallel decoding
    of generative LLMs. Existing parallel decoding methods suffer from a high latency
    overhead for batch decoding due to a large computation overhead. ProPD leverages
    a token tree early pruning algorithm to reduce the verification overhead and a
    dynamic tree generation algorithm to adapt to different decoding conditions automatically.
    We verify ProPD across a diverse set of datasets, LLMs, and batch sizes and demonstrate
    $1.1-3.2\times$ speedup over existing parallel decoding algorithms, e.g., Medusa.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了ProPD，一个加速生成性LLM并行解码的框架。现有的并行解码方法由于计算开销较大而导致批量解码的高延迟开销。ProPD利用一个令牌树早期剪枝算法来减少验证开销，并且一个动态树生成算法来自动适应不同的解码条件。我们在各种数据集、LLM和批量大小上验证了ProPD，并展示了相对于现有的并行解码算法（如Medusa）$1.1-3.2\times$的加速效果。
- en: References
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Bae et al. (2023) Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-Young Yun. 2023.
    Fast and Robust Early-Exiting Framework for Autoregressive Language Models with
    Synchronized Parallel Decoding. *arXiv preprint arXiv:2310.05424* (2023).
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bae 等人（2023）Sangmin Bae, Jongwoo Ko, Hwanjun Song, 和 Se-Young Yun。2023年。针对自回归语言模型的快速且鲁棒的早期退出框架与同步并行解码。*arXiv
    预印本 arXiv:2310.05424*（2023）。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems* 33 (2020), 1877–1901.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等人。2020年。语言模型是少样本学习者。*神经信息处理系统进展* 33（2020），1877–1901。
- en: 'Cai et al. (2023) Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, and Tri
    Dao. 2023. Medusa: Simple framework for accelerating llm generation with multiple
    decoding heads.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等人（2023）Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, 和 Tri Dao。2023年。Medusa：一种通过多解码头加速LLM生成的简单框架。
- en: Chen et al. (2023) Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste
    Lespiau, Laurent Sifre, and John Jumper. 2023. Accelerating large language model
    decoding with speculative sampling. *arXiv preprint arXiv:2302.01318* (2023).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2023）Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste
    Lespiau, Laurent Sifre, 和 John Jumper。2023年。通过推测采样加速大型语言模型解码。*arXiv 预印本 arXiv:2302.01318*（2023）。
- en: 'Fabbri et al. (2019) Alexander R Fabbri, Irene Li, Tianwei She, Suyi Li, and
    Dragomir R Radev. 2019. Multi-news: A large-scale multi-document summarization
    dataset and abstractive hierarchical model. *arXiv preprint arXiv:1906.01749*
    (2019).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fabbri 等（2019） Alexander R Fabbri、Irene Li、Tianwei She、Suyi Li 和 Dragomir R
    Radev。2019年。《Multi-news：一个大规模的多文档摘要数据集和抽象的层次模型》。*arXiv 预印本 arXiv:1906.01749*（2019年）。
- en: Hendy et al. (2023) Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak,
    Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan
    Awadalla. 2023. How good are gpt models at machine translation? a comprehensive
    evaluation. *arXiv preprint arXiv:2302.09210* (2023).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendy 等（2023） Amr Hendy、Mohamed Abdelrehim、Amr Sharaf、Vikas Raunak、Mohamed Gabr、Hitokazu
    Matsushita、Young Jin Kim、Mohamed Afify 和 Hany Hassan Awadalla。2023年。《GPT 模型在机器翻译中的表现如何？一项全面评估》。*arXiv
    预印本 arXiv:2302.09210*（2023年）。
- en: 'Jiang et al. (2023) Yufan Jiang, Qiaozhi He, Xiaomin Zhuang, Zhihua Wu, Kunpeng
    Wang, Wenlai Zhao, and Guangwen Yang. 2023. RecycleGPT: An Autoregressive Language
    Model with Recyclable Module. arXiv:2308.03421 [cs.CL]'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2023） Yufan Jiang、Qiaozhi He、Xiaomin Zhuang、Zhihua Wu、Kunpeng Wang、Wenlai
    Zhao 和 Guangwen Yang。2023年。《RecycleGPT：具有可回收模块的自回归语言模型》。arXiv:2308.03421 [cs.CL]
- en: 'Kim et al. (2023) Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo
    Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W
    Mahoney, et al. 2023. Full stack optimization of transformer inference: a survey.
    *arXiv preprint arXiv:2302.14017* (2023).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2023） Sehoon Kim、Coleman Hooper、Thanakul Wattanawong、Minwoo Kang、Ruohan
    Yan、Hasan Genc、Grace Dinh、Qijing Huang、Kurt Keutzer、Michael W Mahoney 等。2023年。《变压器推理的全栈优化：一项调查》。*arXiv
    预印本 arXiv:2302.14017*（2023年）。
- en: Leviathan et al. (2023) Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023.
    Fast inference from transformers via speculative decoding. In *International Conference
    on Machine Learning*. PMLR, 19274–19286.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leviathan 等（2023） Yaniv Leviathan、Matan Kalman 和 Yossi Matias。2023年。《通过推测解码实现变压器的快速推理》。在*国际机器学习会议*上。PMLR，19274–19286。
- en: 'Miao et al. (2023) Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng,
    Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar,
    and Zhihao Jia. 2023. SpecInfer: Accelerating Generative LLM Serving with Speculative
    Inference and Token Tree Verification. *arXiv preprint arXiv:2305.09781* (2023).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miao 等（2023） Xupeng Miao、Gabriele Oliaro、Zhihao Zhang、Xinhao Cheng、Zeyu Wang、Rae
    Ying Yee Wong、Zhuoming Chen、Daiyaan Arfeen、Reyna Abhyankar 和 Zhihao Jia。2023年。《SpecInfer：通过推测推理和令牌树验证加速生成
    LLM 服务》。*arXiv 预印本 arXiv:2305.09781*（2023年）。
- en: MohamedRashad ([n. d.]) MohamedRashad. [n. d.]. *Chatgpt-prompts*. [https://huggingface.co/datasets/MohamedRashad/ChatGPT-prompts](https://huggingface.co/datasets/MohamedRashad/ChatGPT-prompts)
    2023.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MohamedRashad（[n. d.]） MohamedRashad。[n. d.]。*Chatgpt-prompts*。[https://huggingface.co/datasets/MohamedRashad/ChatGPT-prompts](https://huggingface.co/datasets/MohamedRashad/ChatGPT-prompts)
    2023年。
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. 2018. Improving language understanding by generative pre-training. (2018).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2018） Alec Radford、Karthik Narasimhan、Tim Salimans、Ilya Sutskever
    等。2018年。《通过生成预训练提高语言理解》。 （2018年）。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog* 1, 8 (2019), 9.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2019） Alec Radford、Jeffrey Wu、Rewon Child、David Luan、Dario Amodei、Ilya
    Sutskever 等。2019年。《语言模型是无监督的多任务学习者》。*OpenAI 博客* 1, 8（2019年），9。
- en: Santilli et al. (2023) Andrea Santilli, Silvio Severino, Emilian Postolache,
    Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodolà. 2023.
    Accelerating Transformer Inference for Translation via Parallel Decoding. *arXiv
    preprint arXiv:2305.10427* (2023).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santilli 等（2023） Andrea Santilli、Silvio Severino、Emilian Postolache、Valentino
    Maiorca、Michele Mancusi、Riccardo Marin 和 Emanuele Rodolà。2023年。《通过并行解码加速变压器推理以进行翻译》。*arXiv
    预印本 arXiv:2305.10427*（2023年）。
- en: Spector and Re (2023) Benjamin Spector and Chris Re. 2023. Accelerating llm
    inference with staged speculative decoding. *arXiv preprint arXiv:2308.04623*
    (2023).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spector 和 Re（2023） Benjamin Spector 和 Chris Re。2023年。《通过分阶段的推测解码加速 LLM 推理》。*arXiv
    预印本 arXiv:2308.04623*（2023年）。
- en: Stern et al. (2018) Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. 2018.
    Blockwise parallel decoding for deep autoregressive models. *Advances in Neural
    Information Processing Systems* 31 (2018).
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stern 等（2018） Mitchell Stern、Noam Shazeer 和 Jakob Uszkoreit。2018年。《深度自回归模型的块状并行解码》。*神经信息处理系统进展*
    31（2018年）。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    Alpaca: An Instruction-following LLaMA model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori等（2023）罗汉·塔奥里、伊尚·古尔拉贾尼、张天逸、扬·迪布瓦、李学辰、卡洛斯·古斯特林、佩西·梁和辰维·B·哈希莫托。2023年。《斯坦福Alpaca:
    一种遵循指令的LLaMA模型》。[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。'
- en: 'Xu et al. (2023) Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei,
    Mengwei Xu, and Xuanzhe Liu. 2023. LLMCad: Fast and Scalable On-device Large Language
    Model Inference. *arXiv preprint arXiv:2309.04255* (2023).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '徐等（2023）徐大亮、尹旺松、金鑫、张颖、魏诗云、徐梦伟和刘轩哲。2023年。《LLMCad: 快速且可扩展的设备端大语言模型推理》。*arXiv
    预印本 arXiv:2309.04255*（2023年）。'
- en: 'Zaib et al. (2022) Munazza Zaib, Wei Emma Zhang, Quan Z Sheng, Adnan Mahmood,
    and Yang Zhang. 2022. Conversational question answering: A survey. *Knowledge
    and Information Systems* 64, 12 (2022), 3151–3195.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zaib等（2022）穆纳扎·扎伊布、魏·艾玛·张、全·Z·盛、阿德南·马赫穆德和杨·张。2022年。《对话式问答：综述》。*知识与信息系统* 64,
    12（2022年），3151–3195。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-Bench
    and Chatbot Arena. arXiv:2306.05685 [cs.CL]
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等（2023）郑联民、姜伟林、盛颖、庄思远、吴张浩、庄永浩、林紫、李卓汉、李大成、埃里克·P·辛、张浩、约瑟夫·E·冈萨雷斯和伊昂·斯托伊卡。2023年。《通过MT-Bench和Chatbot
    Arena评估LLM作为裁判的表现》。*arXiv:2306.05685 [cs.CL]*
