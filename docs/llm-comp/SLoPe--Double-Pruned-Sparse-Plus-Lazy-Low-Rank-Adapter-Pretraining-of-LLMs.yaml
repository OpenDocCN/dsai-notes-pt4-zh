- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 19:04:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 19:04:46'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练LLMs'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16325](https://ar5iv.labs.arxiv.org/html/2405.16325)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16325](https://ar5iv.labs.arxiv.org/html/2405.16325)
- en: Mohammad Mozaffari Amir Yazdanbakhsh Department of Compute Science Google DeepMind
    University of Toronto Mountain View, USA [mmozaffari@cs.toronto.edu](mailto:)
    [ayazdan@google.com](mailto:) Zhao Zhang Maryam Mehri Dehnavi Department of Electrical
    and Computer Engineering Department of Compute Science Rutgers University University
    of Toronto [zhao.zhang@rutgers.edu](mailto:) [mmehride@cs.toronto.edu](mailto:)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Mohammad Mozaffari Amir Yazdanbakhsh 计算机科学系 谷歌DeepMind 多伦多大学 美国山景城 [mmozaffari@cs.toronto.edu](mailto:)
    [ayazdan@google.com](mailto:) Zhao Zhang Maryam Mehri Dehnavi 电气与计算机工程系 计算机科学系
    罗格斯大学 多伦多大学 [zhao.zhang@rutgers.edu](mailto:) [mmehride@cs.toronto.edu](mailto:)
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'We propose SLoPe, a Double-Pruned Sparse Plus Lazy Low-rank Adapter Pretraining
    method for LLMs that improves the accuracy of sparse LLMs while accelerating their
    pretraining and inference and reducing their memory footprint. Sparse pretraining
    of LLMs reduces the accuracy of the model, to overcome this, prior work uses dense
    models during fine-tuning. SLoPe improves the accuracy of sparsely pretrained
    models by adding low-rank adapters in the final 1% iterations of pretraining without
    adding significant overheads to the model pretraining and inference. In addition,
    SLoPe uses a double-pruned backward pass formulation that prunes the transposed
    weight matrix using N:M sparsity structures to enable an accelerated sparse backward
    pass. SLoPe accelerates the training and inference of models with billions of
    parameters up to $1.14\times$ for training and inference respectively.¹¹1Code
    and data for SLoPe is available at: [https://bit.ly/slope-llm](https://bit.ly/slope-llm)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了SLoPe，一种“双重剪枝稀疏加懒惰低秩适配器预训练”方法，用于改进LLMs的准确性，同时加速其预训练和推理，并减少其内存占用。LLMs的稀疏预训练会降低模型的准确性，为了克服这个问题，先前的工作在微调过程中使用了密集模型。SLoPe通过在预训练的最后1%迭代中添加低秩适配器来提高稀疏预训练模型的准确性，而不会对模型的预训练和推理增加显著的开销。此外，SLoPe使用了一种双重剪枝的反向传递公式，该公式利用N:M稀疏结构剪枝转置权重矩阵，以实现加速的稀疏反向传递。SLoPe将拥有数十亿参数的模型的训练和推理加速了最多$1.14\times$。¹¹1SLoPe的代码和数据可在：[https://bit.ly/slope-llm](https://bit.ly/slope-llm)
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) demonstrate significant potential for natural language
    understanding and generation; however, they are expensive to train and execute
    because of their extensive parameter count and the substantial volume of training
    data required. The training process of LLMs include a pretraining [[42](#bib.bib42)]
    and a fine-tuning stage. In the pretraining phase, the model is trained on a large
    high-quality text [[16](#bib.bib16), [1](#bib.bib1)] and then fine-tuned on different
    downstream tasks [[52](#bib.bib52), [45](#bib.bib45)]. Both phases require significant
    amounts of computation, memory, and communication.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）展现了在自然语言理解和生成方面的巨大潜力；然而，由于其庞大的参数量和大量的训练数据需求，它们的训练和执行代价很高。LLMs的训练过程包括预训练[[42](#bib.bib42)]和微调阶段。在预训练阶段，模型在大量高质量文本[[16](#bib.bib16)，[1](#bib.bib1)]上进行训练，然后在不同的下游任务[[52](#bib.bib52)，[45](#bib.bib45)]上进行微调。这两个阶段都需要大量的计算、内存和通信资源。
- en: Model sparsity, in which the less important parts of the model are pruned, can
    reduce the computation and memory overheads of LLM pretraining [[21](#bib.bib21)].
    Sparsity is unstructured if elements are removed from arbitrary locations in the
    tensors. Unstructured sparsity is hard to accelerate due to non-existing hardware/software
    support [[53](#bib.bib53)]. To resolve this, structured sparsity imposes constraints
    on where the zero elements can appear [[25](#bib.bib25), [30](#bib.bib30)], creating
    dense blocks of nonzeros in the matrix to leverage dense compute routines. The
    drawback of the structured sparse methods is that they limit the choice for sparsity
    patterns leading to a reduction in accuracy in the sparse model when compared
    to dense [[8](#bib.bib8)]. NVIDIA has recently introduced sparse tensor cores
    [[37](#bib.bib37)] to their hardware that accelerate more flexible structured
    sparsity patterns, i.e. 2:4 sparsity; hardware support for N:M sparsity where
    at most N out of M consecutive elements are zero is not yet available but machine
    learning practitioners are developing algorithms for these patterns [[26](#bib.bib26),
    [32](#bib.bib32), [44](#bib.bib44)] .
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 模型稀疏性，即修剪模型中不重要的部分，可以减少LLM预训练的计算和内存开销[[21](#bib.bib21)]。如果元素从张量中的任意位置被移除，则稀疏性是无结构的。由于缺乏硬件/软件支持，无结构稀疏性很难加速[[53](#bib.bib53)]。为了解决这个问题，结构化稀疏性对零元素出现的位置施加约束[[25](#bib.bib25),
    [30](#bib.bib30)]，在矩阵中创建密集的非零块，以利用密集计算例程。结构化稀疏方法的缺点是它们限制了稀疏模式的选择，导致稀疏模型的准确性降低，与密集模型相比[[8](#bib.bib8)]。NVIDIA
    最近引入了稀疏张量核心[[37](#bib.bib37)]，加速了更灵活的结构化稀疏模式，即2:4稀疏性；尚未提供对N:M稀疏性的硬件支持，其中最多N个连续元素为零，但机器学习从业者正在为这些模式开发算法[[26](#bib.bib26),
    [32](#bib.bib32), [44](#bib.bib44)]。
- en: Applying N:M sparse masks to a model leads to accuracy loss because of their
    limited choice of sparsity patterns. Changing the sparsity mask dynamically throughout
    pretraining is one of the approaches proposed to address this issue [[11](#bib.bib11)].
    Zhou et al. [[56](#bib.bib56)] proposes a novel metric for finding the N:M sparsity
    patterns that lead to higher accuracy in each iteration. [[26](#bib.bib26)] suggest
    the use of decaying masks to further improve the accuracy of the models. STEP
    [[32](#bib.bib32)] proposes a new optimizer that improves the convergence of models
    with adaptive masks. While the adaptive methods can improve the accuracy of the
    models, they require storing the dense weights and possibly additional metrics
    for updating the new sparsity patterns, while wasting a portion of the training
    computations to train the weights that will be pruned in later iterations. SPDF
    [[51](#bib.bib51)] and Sparse-Dense Pretraining (SD-P) [[23](#bib.bib23)], one
    can compensate for the loss imposed by sparsity with a dense fine-tuning. But
    the dense fine-tuning stage will disable the memory and compute savings of sparse
    methods at inference. Inspired by this, we introduce additional non-zeros to the
    weight in the last steps of pretraining. To avoid storing a dense model during
    inference while getting the same capabilities of a dense weight, we add the non-zeros
    in the form of low-rank adapters [[22](#bib.bib22)]. Our experiments show that
    using low rank adaptors leads to noticeably faster convergence compared to when
    the same number of learnable parameters are added to the sparse weights.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将N:M稀疏掩码应用于模型会导致准确性损失，因为它们稀疏模式的选择有限。动态更改稀疏掩码是解决此问题的提议之一[[11](#bib.bib11)]。Zhou等人[[56](#bib.bib56)]提出了一种新颖的度量标准，用于找到在每次迭代中导致更高准确性的N:M稀疏模式。[[26](#bib.bib26)]建议使用衰减掩码以进一步提高模型的准确性。STEP
    [[32](#bib.bib32)] 提出了一个新优化器，以改善具有自适应掩码的模型的收敛性。尽管自适应方法可以提高模型的准确性，但它们需要存储密集权重以及可能的额外度量来更新新的稀疏模式，同时浪费一部分训练计算以训练将在后续迭代中被修剪的权重。SPDF
    [[51](#bib.bib51)] 和 稀疏-密集预训练（SD-P）[[23](#bib.bib23)]，可以通过密集微调来弥补稀疏带来的损失。但密集微调阶段将禁用稀疏方法在推理阶段的内存和计算节省。受到此启发，我们在预训练的最后步骤中引入了额外的非零元素。为了在推理过程中避免存储密集模型，同时获得密集权重的相同能力，我们以低秩适配器[[22](#bib.bib22)]的形式添加非零元素。我们的实验表明，使用低秩适配器相比将相同数量的可学习参数添加到稀疏权重中，可以显著加快收敛速度。
- en: 'The use of N:M sparsity in LLM pretraining is limited to accelerating the forward
    pass in the training loop because the row-wise N:M structure in the weight sparsity
    pattern will be lost when the weights are transposed in the backward pass. Prior
    work [[24](#bib.bib24), [55](#bib.bib55), [23](#bib.bib23)] attempt to leverage
    sparsity in both forward and backward passes by finding transposable masks through
    various methods: greedy search algorithms, searching among random permutations,
    and searching among the results of convolution. However, these transposable masks
    reduce model accuracy and add significant runtime overheads [[23](#bib.bib23)],
    often resulting to slow-downs (up to $\texttt{8.4}\times$). To address these issues,
    we propose a double-pruned backward pass formulation with theoretical convergence
    guarantees. Instead of enforcing the weight transpose to be N:M sparse, our approach
    transposes the N:M weight matrix first and then imposes N:M sparsity. This allows
    the weight matrices to exhibit a wider range of sparsity patterns, leading to
    improved accuracy.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLM 预训练中使用 N:M 稀疏性仅限于加速训练循环中的前向传播，因为权重稀疏模式中的按行 N:M 结构会在反向传播中转置权重时丢失。先前的工作
    [[24](#bib.bib24), [55](#bib.bib55), [23](#bib.bib23)] 尝试通过各种方法（贪婪搜索算法、在随机排列中搜索、在卷积结果中搜索）利用前向和反向传播中的稀疏性。然而，这些可转置掩码降低了模型准确性并增加了显著的运行时开销
    [[23](#bib.bib23)]，通常导致速度下降（高达 $\texttt{8.4}\times$）。为解决这些问题，我们提出了一种具有理论收敛保证的双重剪枝反向传播公式。我们的方法不是强制权重转置为
    N:M 稀疏，而是先转置 N:M 权重矩阵，然后施加 N:M 稀疏性。这使得权重矩阵可以展示更广泛的稀疏模式，从而提高准确性。
- en: 'Our method, SLoPe, is a Double-Pruned Sparse Plus Lazy Low-rank Adapter Pretraining
    method for LLMs. It employs a *static* N:M sparsity mask with a double-pruned
    backward pass formulation to accelerate both the forward and backward passes.
    Key contributions of SLoPe are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法，SLoPe，是一种针对 LLMs 的双重剪枝稀疏加懒惰低秩适配器预训练方法。它采用了一个*静态*的 N:M 稀疏掩码，并结合双重剪枝反向传播公式来加速前向和反向传播。SLoPe
    的关键贡献包括：
- en: •
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Double-Pruned backward pass $\rightarrow$ We propose to transpose an already
    sparsifiedd N:M weight matrix (forward pass) before imposing another round of
    N:M sparsity (backward pass), improving model quality and reducing mask search
    overheads.
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 双重剪枝反向传播 $\rightarrow$ 我们建议在强加另一轮 N:M 稀疏性（反向传播）之前，先转置已经稀疏化的 N:M 权重矩阵（前向传播），以提高模型质量并减少掩码搜索开销。
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Lazy Low-Rank adapters $\rightarrow$ We introduce additional parameters with
    minimal compute and memory overheads, merely for the last 1% iterations of pretraining,
    improving model capacity (see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ SLoPe:
    Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs")).'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '懒惰低秩适配器 $\rightarrow$ 我们引入了额外的参数，计算和内存开销最小，仅用于预训练的最后 1% 迭代，提升模型容量（见图 [1](#S1.F1
    "图 1 ‣ 1 引言 ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练 LLMs")）。'
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Optimized CUDA kernels $\rightarrow$, respectively.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优化 CUDA 内核 $\rightarrow$，分别。
- en: '![Refer to caption](img/c82ebb0cadc4073ad403389095ac1c51.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c82ebb0cadc4073ad403389095ac1c51.png)'
- en: 'Figure 1: The sparse training pipeline in SLoPe. Here, $\mathcal{X}$ sparsification,
    leading to extra imposed zeros. Blue elements represent non-zero values, while
    white elements represent pruned values, and red elements indicate additional zeros
    introduced during the backward pass.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：SLoPe 中的稀疏训练流程。这里，$\mathcal{X}$ 稀疏化，导致额外的强加零。蓝色元素代表非零值，白色元素代表剪枝值，而红色元素表示在反向传播过程中引入的额外零。
- en: 2 Sparse plus low-rank pretraining of LLMs
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2 稀疏加低秩的 LLM 预训练
- en: 'Equation [1](#S2.E1 "In 2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe:
    Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs"), [2](#S2.E2
    "In 2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus
    Lazy Low-Rank Adapter Pretraining of LLMs"), and [3](#S2.E3 "In 2 Sparse plus
    low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank
    Adapter Pretraining of LLMs") depict the formulas for the forward and backward
    pass of the $i$ refer to the input and output dimensions of the respective layer.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '方程 [1](#S2.E1 "在 2 稀疏加低秩预训练 LLMs ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练 LLMs")、[2](#S2.E2
    "在 2 稀疏加低秩预训练 LLMs ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练 LLMs") 和 [3](#S2.E3 "在 2 稀疏加低秩预训练
    LLMs ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练 LLMs") 描述了 $i$ 的前向和反向传播的公式，分别指代相应层的输入和输出维度。'
- en: '|  | $\mathrm{FWD}\mathrel{\ooalign{$\rightarrow$\cr\kern-0.6458pt\raise 1.18399pt\hbox{\scalebox{1.0}[0.522]{$\mid$}}\cr}}\mathcal{Y}_{i}=\mathcal{X}_{i}\mathcal{W}_{i}^{T}$
    |  | (1) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{FWD}\mathrel{\ooalign{$\rightarrow$\cr\kern-0.6458pt\raise 1.18399pt\hbox{\scalebox{1.0}[0.522]{$\mid$}}\cr}}\mathcal{Y}_{i}=\mathcal{X}_{i}\mathcal{W}_{i}^{T}$
    |  | (1) |'
- en: '|  | $\mathrm{BWD-1}\mathrel{\ooalign{$\rightarrow$\cr\kern-0.6458pt\raise
    1.18399pt\hbox{\scalebox{1.0}[0.522]{$\mid$}}\cr}}\nabla_{W_{i}}\mathcal{L}=\nabla_{Y_{i}}\mathcal{L}^{T}\mathcal{X}_{i}$
    |  | (2) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{BWD-1}\mathrel{\ooalign{$\rightarrow$\cr\kern-0.6458pt\raise
    1.18399pt\hbox{\scalebox{1.0}[0.522]{$\mid$}}\cr}}\nabla_{W_{i}}\mathcal{L}=\nabla_{Y_{i}}\mathcal{L}^{T}\mathcal{X}_{i}$
    |  | (2) |'
- en: '|  | $\mathrm{BWD-2}\mathrel{\ooalign{$\rightarrow$\cr\kern-0.6458pt\raise
    1.18399pt\hbox{\scalebox{1.0}[0.522]{$\mid$}}\cr}}\nabla_{X_{i}}\mathcal{L}=\nabla_{Y_{i}}\mathcal{L}\mathcal{W}_{i}$
    |  | (3) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{BWD-2}\mathrel{\ooalign{$\rightarrow$\cr\kern-0.6458pt\raise
    1.18399pt\hbox{\scalebox{1.0}[0.522]{$\mid$}}\cr}}\nabla_{X_{i}}\mathcal{L}=\nabla_{Y_{i}}\mathcal{L}\mathcal{W}_{i}$
    |  | (3) |'
- en: The dimension along which N:M pruning occurs corresponds to the reduction dimension
    in Matrix-Matrix multiplication. Without this restriction, the sparse Matrix-Matrix
    operation can not be accelerated on GPU [[39](#bib.bib39)]. With this restriction
    in mind, to leverage weight sparsity in forward and backward pass, one needs to
    prune elements along the columns of $\mathcal{W}_{i}^{T}$ along both row and column
    dimensions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: N:M 修剪发生的维度对应于矩阵-矩阵乘法中的缩减维度。在没有这种限制的情况下，稀疏的矩阵-矩阵操作不能在 GPU 上加速 [[39](#bib.bib39)]。考虑到这一限制，为了在前向和反向传播中利用权重稀疏性，需要沿
    $\mathcal{W}_{i}^{T}$ 的行和列维度修剪元素。
- en: 2.1 Double-pruned backward pass
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1 双重修剪反向传播
- en: 'Various approaches can be used to exploit N:M sparsity during both the forward
    and backward passes. For example, one may prune the activation tensor $\mathcal{X}_{i}$
    in BWD-2 along the column dimension. Although diverse combinations exist for pruning,
    our focus in this study is primarily on the sparsification of weight tensors for
    two reasons: (a) the sparsification of weight tensors directly impact the resource
    required for model storage and serving, and (b) our initial findings indicate
    that pruning weight tensors during both forward and backward passes has a comparatively
    lesser adverse impact on the overall end-to-end model quality. More details on
    our experiments can be found in [J](#A10 "Appendix J Sensitivity to the choice
    of pruning matrix ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank
    Adapter Pretraining of LLMs"). As such, we posit a double-pruned backward pass
    formulation that can productively accelerate FWD and BWD-2 computations.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向和反向传播过程中，可以使用各种方法来利用 N:M 稀疏性。例如，可以在 BWD-2 中沿列维度修剪激活张量 $\mathcal{X}_{i}$。虽然存在多种修剪组合，但我们在本研究中主要关注于权重张量的稀疏化，原因有两个：（a）权重张量的稀疏化直接影响模型存储和服务所需的资源；（b）我们初步发现，在前向和反向传播过程中修剪权重张量对整体端到端模型质量的负面影响相对较小。更多关于我们实验的细节可以在
    [J](#A10 "附录 J 对修剪矩阵选择的敏感性 ‣ 附录 ‣ SLoPe：双重修剪稀疏加懒散低秩适配器的LLMs预训练") 中找到。因此，我们提出了一种双重修剪反向传播公式，该公式可以有效加速
    FWD 和 BWD-2 的计算。
- en: 'In addition, we prove that such materialization of pruned weight tensors, despite
    being lossy²²2We term this formulation “lossy” because the weight matrix undergoes
    information loss during the backward pass compare to its state in the forward
    pass., exhibits convergence properties. For the rest of this paper, we represent
    the weight tensor subjected to row-wise pruning as $\mathcal{W}_{i}^{R}$. We rewrite
    the training equations to accommodate these modifications, with proposed changes
    highlighted in blue:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们证明了这种修剪权重张量的具体化，尽管是有损的²²2我们将这种公式称为“有损”，因为在反向传播过程中权重矩阵相较于前向传播状态会出现信息丢失。，展现了收敛特性。本文其余部分中，我们将经过按行修剪的权重张量表示为
    $\mathcal{W}_{i}^{R}$。我们重新编写了训练方程以适应这些修改，所提议的变化以蓝色突出显示：
- en: '|  | $\mathrm{FWD}\mathrel{\ooalign{$\rightarrow$\cr\kern-0.6458pt\raise 1.18399pt\hbox{\scalebox{1.0}[0.522]{$\mid$}}\cr}}\mathcal{Y}_{i}=\mathcal{X}_{i}{\color[rgb]{0,0,1}{{{\mathcal{W}_{i}^{R}}}^{T}}}$
    |  | (4) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{FWD}\mathrel{\ooalign{$\rightarrow$\cr\kern-0.6458pt\raise 1.18399pt\hbox{\scalebox{1.0}[0.522]{$\mid$}}\cr}}\mathcal{Y}_{i}=\mathcal{X}_{i}{\color[rgb]{0,0,1}{{{\mathcal{W}_{i}^{R}}}^{T}}}$
    |  | (4) |'
- en: '|  | $\mathrm{BWD-1}\mathrel{\ooalign{$\rightarrow$\cr\kern-0.6458pt\raise
    1.18399pt\hbox{\scalebox{1.0}[0.522]{$\mid$}}\cr}}\nabla_{W_{i}}\mathcal{L}=\nabla_{Y_{i}}\mathcal{L}^{T}\mathcal{X}_{i}$
    |  | (5) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{BWD-1}\mathrel{\ooalign{$\rightarrow$\cr\kern-0.6458pt\raise
    1.18399pt\hbox{\scalebox{1.0}[0.522]{$\mid$}}\cr}}\nabla_{W_{i}}\mathcal{L}=\nabla_{Y_{i}}\mathcal{L}^{T}\mathcal{X}_{i}$
    |  | (5) |'
- en: '|  | $\mathrm{BWD-2}\mathrel{\ooalign{$\rightarrow$\cr\kern-0.6458pt\raise
    1.18399pt\hbox{\scalebox{1.0}[0.522]{$\mid$}}\cr}}\nabla_{X_{i}}\mathcal{L}=\nabla_{Y_{i}}\mathcal{L}{\color[rgb]{0,0,1}{{\mathcal{W}_{i}^{R,C}}}}$
    |  | (6) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{BWD-2}\mathrel{\ooalign{$\rightarrow$\cr\kern-0.6458pt\raise
    1.18399pt\hbox{\scalebox{1.0}[0.522]{$\mid$}}\cr}}\nabla_{X_{i}}\mathcal{L}=\nabla_{Y_{i}}\mathcal{L}{\color[rgb]{0,0,1}{{\mathcal{W}_{i}^{R,C}}}}$
    |  | (6) |'
- en: Using this formulation for training, we can accelerate both forward and backward
    passes owing to the existence of N:M sparsity along both dimensions of weight
    tensors.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种公式进行训练，我们可以加速前向和后向传播，因为在权重张量的两个维度上都存在N:M稀疏。
- en: 'Memory footprint analysis. Inducing N:M structured sparsity not only improves
    computational efficiency of GEMM operations but also reduces the memory footprint
    for storing sparse tensors. It is noteworthy, however, that the storage of auxiliary
    meta-data becomes necessary, containing information about the locations of non-zero
    elements in a supporting matrix. Equation [7](#S2.E7 "In 2.1 Double-pruned backward
    pass ‣ 2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse
    Plus Lazy Low-Rank Adapter Pretraining of LLMs") delineates the requisite number
    of bits for storing the indices in the N:M sparsity format, where $\lceil.\rceil$
    denoting the ceiling function. We present the detailed results on the memory footprint
    reduction in [section 3](#S3 "3 Experimental results ‣ SLoPe: Double-Pruned Sparse
    Plus Lazy Low-Rank Adapter Pretraining of LLMs").'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '内存占用分析。引入N:M结构稀疏不仅提高了GEMM操作的计算效率，还减少了存储稀疏张量的内存占用。然而，值得注意的是，需要存储辅助元数据，其中包含支持矩阵中非零元素的位置的信息。方程
    [7](#S2.E7 "在2.1双修剪反向传播 ‣ 2稀疏加低秩LLMs预训练 ‣ SLoPe: 双修剪稀疏加懒惰低秩适配器预训练LLMs") 描述了在N:M稀疏格式中存储索引所需的位数，其中
    $\lceil.\rceil$ 表示向上取整函数。我们在 [第3节](#S3 "3 实验结果 ‣ SLoPe: 双修剪稀疏加懒惰低秩适配器预训练LLMs")
    中展示了内存占用减少的详细结果。'
- en: '|  | $n^{N:M}_{index}=\left\lceil{log\left({\binom{M}{N}}\right)}\right\rceil$
    |  | (7) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $n^{N:M}_{index}=\left\lceil{log\left({\binom{M}{N}}\right)}\right\rceil$
    |  | (7) |'
- en: 'Convergence analysis. [2.1](#S2.Thmtheorem1 "Lemma 2.1\. ‣ 2.1 Double-pruned
    backward pass ‣ 2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned
    Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs") (proof in  [subsection O.1](#A15.SS1
    "O.1 Lemma 2.1 ‣ Appendix O Proofs ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus
    Lazy Low-Rank Adapter Pretraining of LLMs")) shows the additional sparsity resulting
    from double pruning to an initially row-wise N:M pruned matrix. Following this
    lemma, we quantify the increased sparsity induced by double pruning with 1:2,
    2:4, and 2:8 sparsity patterns as $12.5\%$, respectively. This observation underscores
    that as the value of M in N:M increases, the surplus of zero elements in a double-pruned
    matrix diminishes. This reduction in zero elements consequently implies a decrease
    in computational errors, enhancing the robustness of the computations. We expound
    further insights into this phenomenon in [Appendix I](#A9 "Appendix I Sparsity
    ratio analysis of double-pruned backward pass ‣ Appendix ‣ SLoPe: Double-Pruned
    Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs").'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '收敛性分析。[2.1](#S2.Thmtheorem1 "引理 2.1. ‣ 2.1 双修剪反向传播 ‣ 2 稀疏加低秩LLMs预训练 ‣ SLoPe:
    双修剪稀疏加懒惰低秩适配器预训练LLMs")（证明见 [O.1小节](#A15.SS1 "O.1 引理 2.1 ‣ 附录 O 证明 ‣ 附录 ‣ SLoPe:
    双修剪稀疏加懒惰低秩适配器预训练LLMs")）展示了从初始的按行修剪N:M矩阵中双重修剪导致的额外稀疏。根据该引理，我们量化了双重修剪引起的稀疏度增加，其中1:2、2:4和2:8的稀疏模式分别为
    $12.5\%$。这一观察突显了随着N:M中M值的增加，双修剪矩阵中零元素的过剩减少。这一零元素的减少意味着计算错误的降低，从而增强了计算的鲁棒性。我们在
    [附录 I](#A9 "附录 I 双修剪反向传播稀疏比分析 ‣ 附录 ‣ SLoPe: 双修剪稀疏加懒惰低秩适配器预训练LLMs") 中对此现象进行了进一步阐述。'
- en: Lemma 2.1.
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 2.1。
- en: Consider a randomly initialized matrix $A$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个随机初始化的矩阵 $A$。
- en: '|  | $D(A^{R})-D(A^{R,C})=\sum_{j=N+1}^{M}{\binom{M}{j}}s^{j}(1-s)^{M-j}\frac{j-N}{M}$
    |  | (8) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $D(A^{R})-D(A^{R,C})=\sum_{j=N+1}^{M}{\binom{M}{j}}s^{j}(1-s)^{M-j}\frac{j-N}{M}$
    |  | (8) |'
- en: '[Theorem 2.2](#S2.Thmtheorem2 "Theorem 2.2\. ‣ 2.1 Double-pruned backward pass
    ‣ 2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus
    Lazy Low-Rank Adapter Pretraining of LLMs") states that the dynamic alteration
    of the column-wise mask in [Equation 5](#S2.E5 "5 ‣ 2.1 Double-pruned backward
    pass ‣ 2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse
    Plus Lazy Low-Rank Adapter Pretraining of LLMs") during each training iteration
    does not exert a detrimental impact on the convergence of the optimizer. This
    phenomenon can be attributed to the equivalence between the left-hand side of
    [Equation 9](#S2.E9 "9 ‣ Theorem 2.2\. ‣ 2.1 Double-pruned backward pass ‣ 2 Sparse
    plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank
    Adapter Pretraining of LLMs"), which corresponds to [Equation 3](#S2.E3 "3 ‣ 2
    Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy
    Low-Rank Adapter Pretraining of LLMs") [BWD-2], and the averaging effect achieved
    through multiple training iterations of backpropagation with distinct sparsity
    mask. However, for arbitrary values of N and M, [4](#S2.E4 "In 2.1 Double-pruned
    backward pass ‣ 2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned
    Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs") and [5](#S2.E5 "In 2.1
    Double-pruned backward pass ‣ 2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe:
    Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs") can be used
    in the training with convergence guarantee (proof in  [subsection O.1](#A15.SS1
    "O.1 Lemma 2.1 ‣ Appendix O Proofs ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus
    Lazy Low-Rank Adapter Pretraining of LLMs")).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[定理 2.2](#S2.Thmtheorem2 "定理 2.2。 ‣ 2.1 双重修剪反向传递 ‣ 2 稀疏加低秩 LLM 预训练 ‣ SLoPe：双重修剪稀疏加懒惰低秩适配器
    LLM 预训练") 表明，每次训练迭代中列-wise掩码的动态变化不会对优化器的收敛产生不利影响。这种现象可以归因于 [方程 9](#S2.E9 "9 ‣
    定理 2.2。 ‣ 2.1 双重修剪反向传递 ‣ 2 稀疏加低秩 LLM 预训练 ‣ SLoPe：双重修剪稀疏加懒惰低秩适配器 LLM 预训练") 左侧与
    [方程 3](#S2.E3 "3 ‣ 2 稀疏加低秩 LLM 预训练 ‣ SLoPe：双重修剪稀疏加懒惰低秩适配器 LLM 预训练") [BWD-2] 的等价性，以及通过多次训练迭代的反向传播与不同稀疏掩码所实现的平均效果。然而，对于任意的
    N 和 M，[4](#S2.E4 "在 2.1 双重修剪反向传递 ‣ 2 稀疏加低秩 LLM 预训练 ‣ SLoPe：双重修剪稀疏加懒惰低秩适配器 LLM
    预训练") 和 [5](#S2.E5 "在 2.1 双重修剪反向传递 ‣ 2 稀疏加低秩 LLM 预训练 ‣ SLoPe：双重修剪稀疏加懒惰低秩适配器 LLM
    预训练") 可以用于具有收敛保证的训练（证明见 [O.1 小节](#A15.SS1 "O.1 引理 2.1 ‣ 附录 O 证明 ‣ 附录 ‣ SLoPe：双重修剪稀疏加懒惰低秩适配器
    LLM 预训练")）。'
- en: Theorem 2.2.
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 2.2
- en: Assuming a loss function $\mathcal{L(W_{i},X_{i}})$ is the element-wise multiplication.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设损失函数$\mathcal{L(W_{i},X_{i}})$是逐元素乘法。
- en: '|  | $1$2 |  | (9) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: 2.2 Lazy low-rank adapters
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2 懒惰低秩适配器
- en: Pruning weight tensors in FWD and BWD-2 computations is desirable for computational
    efficiency but may have detrimental impact on quality. To mitigate this adverse
    impact on model quality, we augment the doubly-pruned weight matrix with a low-rank
    matrix. The decomposition of the doubly-pruned weight matrix, combined with the
    low-rank matrix, maintains the computational efficiency of spare Matrix-Matrix
    multiplication during forward and backward passes. Simultaneously, this approach
    holds promise in alleviating the adverse effects of double pruning on overall
    model quality.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FWD 和 BWD-2 计算中修剪权重张量有利于计算效率，但可能对质量产生不利影响。为了减轻这种对模型质量的不良影响，我们用一个低秩矩阵增强了双重修剪的权重矩阵。双重修剪的权重矩阵与低秩矩阵的分解结合，保持了前向和后向传递期间稀疏矩阵-矩阵乘法的计算效率。同时，这种方法在缓解双重修剪对整体模型质量的负面影响方面具有潜力。
- en: Considering the dense weight matrix, denoted by $W_{dense}\in\mathbb{R}^{d_{out}\times
    d_{in}}$ functions as a hyperparameter that controls the trade-offs between memory
    footprint, computational efficiency, and model quality.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到密集权重矩阵，用$W_{dense}\in\mathbb{R}^{d_{out}\times d_{in}}$表示，作为一个超参数，控制内存占用、计算效率和模型质量之间的权衡。
- en: '|  | $\mathcal{W}_{dense}=\mathcal{W}_{sparse}+\mathcal{L}\mathcal{R}$ |  |
    (10) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{W}_{dense}=\mathcal{W}_{sparse}+\mathcal{L}\mathcal{R}$ |  |
    (10) |'
- en: The matrix decomposition of doubly-pruned matrix combined with a low-rank matrix
    approximation reduces the memory footprint of $\mathcal{W}$.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 双重修剪矩阵与低秩矩阵近似的矩阵分解减少了$\mathcal{W}$的内存占用。
- en: We empirically show that the convergence rate of low-rank adapters surpasses
    that of sparse weights. We attribute this behavior to the notably lower parameter
    counts inherent in low-rank adapters. Leveraging this observation, we incorporate
    low-rank adapters exclusively during the final 1% of the training iterations.
    This confined usages of low-rank adapters results in additional reduction of training
    cost, specifically in terms of total number of operations. We term the proposed
    usage of low-rank adapters in the final steps of the training as lazy low-rank
    adapters.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过实验证明，低秩适配器的收敛速度优于稀疏权重。我们将这种行为归因于低秩适配器固有的参数数量显著较少。利用这一观察，我们在训练的最后 1% 迭代中专门使用低秩适配器。这种有限的低秩适配器使用进一步减少了训练成本，尤其是在总操作次数方面。我们将这种在训练最后阶段使用低秩适配器的提议称为“懒惰低秩适配器”。
- en: 2.3 Sparse kernels
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3 稀疏内核
- en: cuSPARSELt is a CUDA library designed explicitly for sparse Matrix-Matrix multiplication,
    where one operand undergoes pruning with the 2:4 sparsity pattern. However, this
    library does not offer APIs for other algebraic routines such as addition and
    assignment for sparse tensors. We now delve into the details of different kernels
    for training and overview our implementation methodology.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: cuSPARSELt 是一个专门为稀疏矩阵乘法设计的 CUDA 库，其中一个操作数经历了 2:4 稀疏模式的剪枝。然而，该库没有提供用于其他代数例程（如稀疏张量的加法和赋值）的
    API。我们现在将深入探讨用于训练的不同内核的细节，并概述我们的实现方法。
- en: 'Algorithm [1](#alg1 "Algorithm 1 ‣ 2.3 Sparse kernels ‣ 2 Sparse plus low-rank
    pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining
    of LLMs") shows the training process of a single linear layer taken from an attention-based
    model. We assume the use of weight decay in the optimizers, and subsequently design
    the requisite sparse APIs to facilitate the optimizer operations. The training
    starts with matrix initialization (line 2) and setting up sparse formats to store
    weight tensors and their corresponding transpose (line 3 and 4). Then, for every
    mini-batch in the training set, we compute the forward pass following [Equation 4](#S2.E4
    "4 ‣ 2.1 Double-pruned backward pass ‣ 2 Sparse plus low-rank pretraining of LLMs
    ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs")
    (line 8). As part of the backward pass, the derivative of the loss function with
    respect to the output activation is computed (line 10). Subsequently, the gradients
    of the loss function with respect to the weight tensor (line 11) and the input
    activation (line 12) are computed using [Equation 2](#S2.E2 "2 ‣ 2 Sparse plus
    low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank
    Adapter Pretraining of LLMs") and [Equation 5](#S2.E5 "5 ‣ 2.1 Double-pruned backward
    pass ‣ 2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse
    Plus Lazy Low-Rank Adapter Pretraining of LLMs"), respectively. In order to circumvent
    the necessity of updating weights with zero values and mitigate the associated
    memory footprint overhead, we employ a strategy wherein we mask the gradients
    for pruned weights. The computed values are stored in a sparse format (line 13).
    Next, in order to implement weight decay in the optimizer and mitigate the impact
    of gradient scaling, we compute the value of $\frac{1}{\gamma}\nabla_{W}\mathcal{L}+\alpha
    W$ denotes the gradient scaling factor for numerical stability during the half-precision
    backward pass. The updated values for the weight tensor are calculated according
    to the optimizer update rule (line 16). Finally, the value of weight tensor and
    its transpose are updated directly in a sparse format (line 17 and line 18). More
    details about the implementation of the custom kernels used in Algorithm [1](#alg1
    "Algorithm 1 ‣ 2.3 Sparse kernels ‣ 2 Sparse plus low-rank pretraining of LLMs
    ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs")
    can be found in Appendix [K](#A11 "Appendix K Implementation details ‣ Appendix
    ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 [1](#alg1 "Algorithm 1 ‣ 2.3 Sparse kernels ‣ 2 Sparse plus low-rank pretraining
    of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of
    LLMs") 显示了从基于注意力的模型中提取的单个线性层的训练过程。我们假设在优化器中使用权重衰减，并随之设计了所需的稀疏 API 以便于优化器操作。训练从矩阵初始化开始（第2行），并设置稀疏格式以存储权重张量及其相应的转置（第3行和第4行）。然后，对于训练集中的每个小批量，我们按照
    [方程 4](#S2.E4 "4 ‣ 2.1 Double-pruned backward pass ‣ 2 Sparse plus low-rank pretraining
    of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of
    LLMs")（第8行）计算前向传递。作为反向传递的一部分，计算了损失函数对输出激活的导数（第10行）。随后，利用 [方程 2](#S2.E2 "2 ‣ 2
    Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy
    Low-Rank Adapter Pretraining of LLMs") 和 [方程 5](#S2.E5 "5 ‣ 2.1 Double-pruned
    backward pass ‣ 2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned
    Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs")，分别计算损失函数对权重张量（第11行）和输入激活（第12行）的梯度。为了避免更新值为零的权重并减轻相关的内存占用，我们采用了一种策略，即对修剪过的权重的梯度进行掩码。计算出的值以稀疏格式存储（第13行）。接下来，为了在优化器中实现权重衰减并减轻梯度缩放的影响，我们计算值
    $\frac{1}{\gamma}\nabla_{W}\mathcal{L}+\alpha W$，这表示在半精度反向传递过程中用于数值稳定性的梯度缩放因子。根据优化器更新规则（第16行）计算权重张量的更新值。最后，权重张量及其转置的值以稀疏格式直接更新（第17行和第18行）。关于算法
    [1](#alg1 "Algorithm 1 ‣ 2.3 Sparse kernels ‣ 2 Sparse plus low-rank pretraining
    of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of
    LLMs") 中使用的自定义内核的更多实现细节请参见附录 [K](#A11 "Appendix K Implementation details ‣ Appendix
    ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs")。'
- en: Algorithm 1 Accelerated Sparse Pretraining Algorithm for a Linear Layer
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 加速的线性层稀疏预训练算法
- en: '1:  Input: Weight: $W$)16:     WNew = optimizer.updateWeight(g)17:     backend.updateSparseMatrix(WSparse,
    WNew)18:     backend.updateSparseMatrix(WSparseTranspose, WNew.transpose())19:  end for'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入: 权重: $W$)16: WNew = optimizer.updateWeight(g)17: backend.updateSparseMatrix(WSparse,
    WNew)18: backend.updateSparseMatrix(WSparseTranspose, WNew.transpose())19: end
    for'
- en: 2.4 SLoPe runtime optimization
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4 SLoPe 运行时优化
- en: While SLoPe improves the training and inference of LLMs by introducing sparse
    weights and low-rank adapters, a naïve implementation can hinder its full performance
    improvement. Specifically, cuSPARSELt [[38](#bib.bib38)] SpMM kernels exhibit
    sensitivity to input and weight tensor shapes, and introducing low-rank adapters
    at inference increases can increase the number of calls during the forward pass
    of each linear layer. This section covers our approach to optimize SLoPe’s implementation
    and further improve model performance.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然SLoPe通过引入稀疏权重和低秩适配器来改进LLMs的训练和推理，但天真的实现可能会妨碍其全面性能的提升。具体来说，cuSPARSELt [[38](#bib.bib38)]
    SpMM内核对输入和权重张量形状很敏感，并且在推理时引入低秩适配器会增加每个线性层前向传递时的调用次数。本节介绍了我们优化SLoPe实现的方法，并进一步提高模型性能。
- en: 'Efficient tiling of upsample tensors. Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Pretraining
    accuracy results ‣ 3 Experimental results ‣ SLoPe: Double-Pruned Sparse Plus Lazy
    Low-Rank Adapter Pretraining of LLMs")-(a) showcases the speedup achieved by the
    cuSPARSELt backend across a range of tensor shapes commonly used in LLMs. While
    the speedup of SpMM in downsample tensors increases gradually as their sizes increase,
    the speedup of upsample tensor drops off at around hidden dimension = 4000. To
    overcome this limitation, we tile the upsample tensor into multiple smaller matrices
    of equal size, each of which benefits from improved speedup when multiplied by
    the input using 2:4 sparsity. By tuning the size of the tiles, we figured that
    the best performance can be achieved by using square tiles. The results of these
    multiplications are then concatenated. This optimization, as detailed in Appendix [E](#A5
    "Appendix E Efficient weight tiling implementation ‣ Appendix ‣ SLoPe: Double-Pruned
    Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs"), leads to a 12% improvement
    in inference speed and a 4% increase in training speed with SLoPe.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '高效的上采样张量分块。图[3](#S3.F3 "Figure 3 ‣ 3.2 Pretraining accuracy results ‣ 3 Experimental
    results ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of
    LLMs")-(a)展示了cuSPARSELt后端在LLMs中常用的各种张量形状下实现的加速效果。虽然SpMM在下采样张量中的加速随着尺寸的增大而逐渐增加，但上采样张量的加速在大约隐藏维度
    = 4000时会下降。为了克服这一限制，我们将上采样张量分块成多个大小相等的小矩阵，每个矩阵在使用2:4稀疏性与输入相乘时都能受益于改进的加速。通过调整块的大小，我们发现使用方块块可以实现最佳性能。这些乘法结果随后被拼接起来。此优化，如附录[E](#A5
    "Appendix E Efficient weight tiling implementation ‣ Appendix ‣ SLoPe: Double-Pruned
    Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs")中详细描述的，导致推理速度提高了12%，训练速度提高了4%。'
- en: 'Efficient kernel for combined SpMM+low-rank adapters. A straightforward implementation
    of low-rank adapters requires four kernel calls: one for sparse matrix multiplication,
    two for low-rank computations, and one for adding the results. In addition, our
    experiments demonstrate that multiplying matrices with low-rank adapters does
    not scale proportionally with the adapter’s rank, leading to significant overheads
    due to their low arithmetic intensity (see Appendix [C](#A3 "Appendix C Low-Rank
    Adapter Performance: Scaling and Arithmetic Intensity ‣ Appendix ‣ SLoPe: Double-Pruned
    Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs")). To address this, we
    introduce two optimizations: (1) concatenating the downsample tensor to the sparse
    weight tensor, reducing kernel calls and increasing arithmetic intensity as in
    Equation [11](#S2.E11 "In 2.4 SLoPe runtime optimization ‣ 2 Sparse plus low-rank
    pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining
    of LLMs")-left, and (2) leveraging a cuBLAS fused matrix multiplication and addition
    kernel, minimizing cache access and kernel calls as in Equation [11](#S2.E11 "In
    2.4 SLoPe runtime optimization ‣ 2 Sparse plus low-rank pretraining of LLMs ‣
    SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs")-right.
    As demonstrated in Appendix [D](#A4 "Appendix D Efficient low-rank adapter implementation
    ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining
    of LLMs"), these optimizations collectively contribute to a speedup improvement
    of up to 6% in the end-to-end inference speed.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '结合SpMM+低秩适配器的高效内核。低秩适配器的直接实现需要四次内核调用：一次用于稀疏矩阵乘法，两次用于低秩计算，一次用于结果相加。此外，我们的实验表明，与低秩适配器相乘的矩阵不会随着适配器的秩按比例扩展，导致由于其低算术强度而产生显著的开销（见附录
    [C](#A3 "附录 C 低秩适配器性能：扩展与算术强度 ‣ 附录 ‣ SLoPe: 双重剪枝稀疏加惰性低秩适配器LLM预训练")）。为了解决这个问题，我们引入了两个优化方案：（1）将降采样张量与稀疏权重张量连接，减少内核调用并增加算术强度，如公式
    [11](#S2.E11 "在 2.4 SLoPe 运行时优化 ‣ 2 稀疏加低秩LLM预训练 ‣ SLoPe: 双重剪枝稀疏加惰性低秩适配器LLM预训练")-左所示，以及（2）利用cuBLAS融合矩阵乘法和加法内核，最小化缓存访问和内核调用，如公式
    [11](#S2.E11 "在 2.4 SLoPe 运行时优化 ‣ 2 稀疏加低秩LLM预训练 ‣ SLoPe: 双重剪枝稀疏加惰性低秩适配器LLM预训练")-右所示。正如附录 [D](#A4
    "附录 D 高效低秩适配器实现 ‣ 附录 ‣ SLoPe: 双重剪枝稀疏加惰性低秩适配器LLM预训练")中所示，这些优化共同促成了端到端推理速度最多提高6%的加速改进。'
- en: '|  | $[Y_{1}&#124;Y_{2}]=X[W^{T}&#124;L];\hskip 40.0ptY=Y_{2}R+Y_{1}$ |  |
    (11) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $[Y_{1}&#124;Y_{2}]=X[W^{T}&#124;L];\hskip 40.0ptY=Y_{2}R+Y_{1}$ |  |
    (11) |'
- en: 3 Experimental results
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3 实验结果
- en: This section evaluates the efficacy of SLoPe in accelerating the pretraining
    while achieving memory savings. Due to the substantial computational resources
    required for LLM pretraining, our accuracy evaluation is primarily focused on
    smaller-scale LLMs up to 774M parameters. However, the speedup and memory reduction
    results extend to a wider range of models, from 2.6B up to 66B parameters.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本节评估了SLoPe在加速预训练和实现内存节省方面的有效性。由于LLM预训练需要大量计算资源，我们的精度评估主要集中在参数规模小至774M的LLMs上。然而，加速和内存减少的结果扩展到了更广泛的模型，从2.6B到66B参数。
- en: '3.1 End-to-end speedup and memory saving: pretraining and inference'
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1 端到端加速和内存节省：预训练和推理
- en: We evaluate the speedup and memory reduction by SLoPe during pretraining and
    inference across LLMs with different model parameter sizes. To demonstrate the
    scalability and efficiency of SLoPe, we conducted extensive benchmarking on OPT
    models (2.6 B to 66 B). In each of the speedup experiments, we have run the code
    for 1000 iterations, and reported the median to reduce the effect of outliers.
    We have run each memory reduction experiment five times and reported the median.³³3Since
    benchmarking speedup and memory savings require fewer resources than complete
    pretraining accuracy experiments, we use the class of OPT models that offers different
    model parameter sizes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了在不同模型参数规模的LLMs上，SLoPe在预训练和推理中的加速效果和内存减少情况。为了展示SLoPe的可扩展性和效率，我们对OPT模型（2.6B到66B）进行了广泛的基准测试。在每次加速实验中，我们运行了1000次代码，并报告了中位数以减少异常值的影响。我们每次内存减少实验进行了五次，并报告了中位数。³³3由于基准测试加速和内存节省所需资源少于完整的预训练精度实验，我们使用了提供不同模型参数规模的OPT模型类别。
- en: 'We compared our method against dense pretraining and inference directly in
    PyTorch, which uses efficient cuBLAS backend. As the sparse pretraining benchmark,
    we compare our work against Sparse-Dense Pretraining (SD-P) [[23](#bib.bib23)],
    the state-of-the-art 2:4 pretraining method and the only semi-structured sparse
    pretraining work that provides end-to-end speedups. Note that methods targeting
    LLM pretraining with N:M sparsity often suffer from inefficiency due to mask search
    overheads and/or compression setup. Appendix [H](#A8 "Appendix H Performance overhead
    of bidirectional mask ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank
    Adapter Pretraining of LLMs") and Appendix [B](#A2 "Appendix B cuSPARSELt Initialization
    Overhead: Static vs. Dynamic Sparsity ‣ Appendix ‣ SLoPe: Double-Pruned Sparse
    Plus Lazy Low-Rank Adapter Pretraining of LLMs") detail the profiling in Bi-Mask [[55](#bib.bib55)]
    and SD-P [[23](#bib.bib23)], which similarly use N:M sparsity on both forward
    and backward passes.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将我们的方法与使用高效 cuBLAS 后端的 PyTorch 中的密集预训练和推理进行了直接比较。作为稀疏预训练基准，我们将我们的工作与 Sparse-Dense
    Pretraining (SD-P) [[23](#bib.bib23)] 进行了比较，这是最先进的 2:4 预训练方法，也是唯一提供端到端加速的半结构化稀疏预训练工作。请注意，针对
    N:M 稀疏性的 LLM 预训练方法通常由于掩码搜索开销和/或压缩设置而效率低下。附录 [H](#A8 "Appendix H Performance overhead
    of bidirectional mask ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank
    Adapter Pretraining of LLMs") 和附录 [B](#A2 "Appendix B cuSPARSELt Initialization
    Overhead: Static vs. Dynamic Sparsity ‣ Appendix ‣ SLoPe: Double-Pruned Sparse
    Plus Lazy Low-Rank Adapter Pretraining of LLMs") 详细描述了 Bi-Mask [[55](#bib.bib55)]
    和 SD-P [[23](#bib.bib23)] 的性能分析，它们在前向和后向传递中都使用了 N:M 稀疏性。'
- en: Notably, our approach, SLoPe, diverges significantly from recent work Sparse-Dense
    Pretraining (SD-P) [[23](#bib.bib23)] in two key aspects. Firstly, we comprehensively
    prune all weights in the model, encompassing both MLP-Mixer and Self-Attention
    modules, whereas SD-P only prunes weights in the MLP-Mixer modules. Secondly,
    SD-P employs dynamic transposable weights, which introduce additional computation
    and memory overhead during training. Finally, SD-P  necessitates dense fine-tuning,
    thereby negating their speedup advantages during inference. In contrast, our approach
    achieves efficient and accurate large language models during both training and
    inference without such limitations.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们的方法 SLoPe 在两个关键方面与近期的工作 Sparse-Dense Pretraining (SD-P) [[23](#bib.bib23)]
    有显著差异。首先，我们全面修剪模型中的所有权重，包括 MLP-Mixer 和 Self-Attention 模块，而 SD-P 仅修剪 MLP-Mixer
    模块中的权重。其次，SD-P 使用动态可转置权重，这在训练过程中引入了额外的计算和内存开销。最后，SD-P 需要密集的微调，因此在推理过程中失去了加速优势。相比之下，我们的方法在训练和推理过程中都实现了高效且准确的大型语言模型，没有这些限制。
- en: 'SLoPe speedup for pretraining and inference. Table [1](#S3.T1 "Table 1 ‣ 3.1
    End-to-end speedup and memory saving: pretraining and inference ‣ 3 Experimental
    results ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of
    LLMs") summarizes the speedups achieved by our method during both training and
    inference. Since over 99% of training occurs without low-rank adapters, the training
    speedup is largely independent of the adapter rank. Conversely, inference speedup
    is directly influenced by the adapter rank. Given the varying hidden dimensions
    across different model sizes, we report the inference speedup for various adapter
    rank ratios: $\frac{adapter-rank}{hidden-dimension}$.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 'SLoPe 在预训练和推理中的加速效果。表格 [1](#S3.T1 "Table 1 ‣ 3.1 End-to-end speedup and memory
    saving: pretraining and inference ‣ 3 Experimental results ‣ SLoPe: Double-Pruned
    Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs") 总结了我们方法在训练和推理过程中实现的加速效果。由于超过
    99% 的训练是在没有低秩适配器的情况下进行的，因此训练加速与适配器的秩基本无关。相反，推理加速直接受适配器秩的影响。鉴于不同模型大小下的隐藏维度不同，我们报告了各种适配器秩比率下的推理加速效果：$\frac{adapter-rank}{hidden-dimension}$。'
- en: 'Table 1: Comparative analysis of end-to-end pretraining and inference speedup
    ($\times$) comparison between SLoPe and the latest work (SD-P) on accelerating
    pretraining with 2:4 sparsity (ICML 2024) [[23](#bib.bib23)]. Note that the lack
    of inference speedup in SD-P  is because of the final dense pretraining during
    the final iterations, resulting in a dense model for inference.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：SLoPe 和最新工作（SD-P）在加速 2:4 稀疏性预训练方面的端到端预训练和推理加速 ($\times$) 比较分析（ICML 2024）[[23](#bib.bib23)]。请注意，SD-P
    在推理中缺乏加速是因为最终的密集预训练在最后几次迭代中进行，导致推理时使用了密集模型。
- en: '| Model | Method | Training | Inference |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 训练 | 推理 |'
- en: '| No Adapter ($r$ = 0) | 1.56% Adapter | 6.25% Adapter |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 无适配器 ($r$ = 0) | 1.56% 适配器 | 6.25% 适配器 |'
- en: '| OPT-66B | SLoPe | 1.13 | 1.34 | 1.31 | 1.30 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66B | SLoPe | 1.13 | 1.34 | 1.31 | 1.30 |'
- en: '| SD-P | 1.07 | 1.00 | 1.00 | 1.00 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| SD-P | 1.07 | 1.00 | 1.00 | 1.00 |'
- en: '| OPT-30B | SLoPe | 1.14 | 1.32 | 1.28 | 1.27 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30B | SLoPe | 1.14 | 1.32 | 1.28 | 1.27 |'
- en: '| SD-P | 1.05 | 1.00 | 1.00 | 1.00 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| SD-P | 1.05 | 1.00 | 1.00 | 1.00 |'
- en: '| OPT-13B | SLoPe | 1.12 | 1.30 | 1.30 | 1.12 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13B | SLoPe | 1.12 | 1.30 | 1.30 | 1.12 |'
- en: '| SD-P | 1.08 | 1.00 | 1.00 | 1.00 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| SD-P | 1.08 | 1.00 | 1.00 | 1.00 |'
- en: '| OPT-6.6B | SLoPe | 1.08 | 1.21 | 1.13 | 1.12 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.6B | SLoPe | 1.08 | 1.21 | 1.13 | 1.12 |'
- en: '| SD-P | 1.08 | 1.00 | 1.00 | 1.00 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| SD-P | 1.08 | 1.00 | 1.00 | 1.00 |'
- en: '| OPT-2.6B | SLoPe | 1.03 | 1.07 | 1.05 | 1.00 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.6B | SLoPe | 1.03 | 1.07 | 1.05 | 1.00 |'
- en: '| SD-P | 1.05 | 1.00 | 1.00 | 1.00 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| SD-P | 1.05 | 1.00 | 1.00 | 1.00 |'
- en: 'Table 2: Comparative analysis of end-to-end memory reductions ($\times$ show
    memory overhead.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 端到端内存减少的比较分析 ($\times$ 表示内存开销)。'
- en: '| Model | Method | Training | Inference |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 训练 | 推理 |'
- en: '| No Adapter ($r$ = 0) | 1.56% Adapter | 6.25% Adapter |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 无适配器 ($r$ = 0) | 1.56% 适配器 | 6.25% 适配器 |'
- en: '| OPT-66B | SLoPe | 0.77 | 0.63 | 0.65 | 0.70 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66B | SLoPe | 0.77 | 0.63 | 0.65 | 0.70 |'
- en: '| SD-P | 1.27 | 1.00 | 1.00 | 1.00 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| SD-P | 1.27 | 1.00 | 1.00 | 1.00 |'
- en: '| OPT-30B | SLoPe | 0.77 | 0.61 | 0.63 | 0.69 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30B | SLoPe | 0.77 | 0.61 | 0.63 | 0.69 |'
- en: '| SD-P | 1.25 | 1.00 | 1.00 | 1.00 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| SD-P | 1.25 | 1.00 | 1.00 | 1.00 |'
- en: '| OPT-13B | SLoPe | 0.78 | 0.51 | 0.62 | 0.68 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13B | SLoPe | 0.78 | 0.51 | 0.62 | 0.68 |'
- en: '| SD-P | 1.25 | 1.00 | 1.00 | 1.00 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| SD-P | 1.25 | 1.00 | 1.00 | 1.00 |'
- en: '| OPT-6.6B | SLoPe | 0.77 | 0.60 | 0.62 | 0.68 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.6B | SLoPe | 0.77 | 0.60 | 0.62 | 0.68 |'
- en: '| SD-P | 1.28 | 1.00 | 1.00 | 1.00 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| SD-P | 1.28 | 1.00 | 1.00 | 1.00 |'
- en: '| OPT-2.6B | SLoPe | 0.77 | 0.62 | 0.64 | 0.70 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.6B | SLoPe | 0.77 | 0.62 | 0.64 | 0.70 |'
- en: '| SD-P | 1.27 | 1.00 | 1.00 | 1.00 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| SD-P | 1.27 | 1.00 | 1.00 | 1.00 |'
- en: 'Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Pretraining accuracy results ‣ 3 Experimental
    results ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of
    LLMs")-(a) illustrates that cuSPARSELt achieves higher speedups for large matrices
    until it reaches its maximum performance capacity ($2\times$). A similar trend
    is observed in the pretraining and inference speedups of the models. For small
    matrices used in low-rank adapters, the lower arithmetic intensity of low-rank
    adapter multiplication results in higher overhead relative to sparse multiplication.
    This is because low arithmetic intensity limits the full utilization of GPU resources,
    leading to inefficiencies.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [3](#S3.F3 "图 3 ‣ 3.2 预训练准确性结果 ‣ 3 实验结果 ‣ SLoPe: 双重修剪稀疏加懒惰低秩适配器的 LLM 预训练")-(a)
    说明 cuSPARSELt 对大矩阵实现了更高的加速，直到达到其最大性能容量 ($2\times$)。在模型的预训练和推理加速中也观察到了类似的趋势。对于低秩适配器使用的小矩阵，由于低秩适配器乘法的算术强度较低，相对于稀疏乘法会导致更高的开销。这是因为低算术强度限制了
    GPU 资源的充分利用，从而导致低效。'
- en: 'SLoPe memory reduction in pretraining and inference. During pretraining, the
    size of the gradients and optimizer states is halved (2:4 sparsity), although
    there is still an overhead for storing the binary masks. For weight matrices (See
    Equation [7](#S2.E7 "In 2.1 Double-pruned backward pass ‣ 2 Sparse plus low-rank
    pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining
    of LLMs")), the indexing overhead is $n^{2:4}_{index}=3$, respectively.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 'SLoPe 在预训练和推理中的内存减少。在预训练期间，梯度和优化器状态的大小减少了一半 (2:4 稀疏)，尽管仍然存在存储二进制掩码的开销。对于权重矩阵（见方程 [7](#S2.E7
    "在 2.1 双重修剪反向传播 ‣ 2 稀疏加低秩 LLM 预训练 ‣ SLoPe: 双重修剪稀疏加懒惰低秩适配器的 LLM 预训练")），索引开销为 $n^{2:4}_{index}=3$。'
- en: 'Table [2](#S3.T2 "Table 2 ‣ 3.1 End-to-end speedup and memory saving: pretraining
    and inference ‣ 3 Experimental results ‣ SLoPe: Double-Pruned Sparse Plus Lazy
    Low-Rank Adapter Pretraining of LLMs") presents the memory reduction for different
    low-rank adapter ranks and OPT model variants. The memory reduction is slightly
    less than the theoretical expectation, primarily because of additional memory
    usage from other model components, such as layer norms, and dense model parameters.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [2](#S3.T2 "表 2 ‣ 3.1 端到端加速和内存节省：预训练和推理 ‣ 3 实验结果 ‣ SLoPe: 双重修剪稀疏加懒惰低秩适配器的
    LLM 预训练") 显示了不同低秩适配器秩和 OPT 模型变体的内存减少情况。内存减少略低于理论预期，主要是由于其他模型组件（如层归一化和密集模型参数）额外占用的内存。'
- en: 3.2 Pretraining accuracy results
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2 预训练准确性结果
- en: 'To assess the impact of SLoPe on model accuracy, we conducted pretraining experiments
    across various models and datasets (details in Appendix [N](#A14 "Appendix N Experiment
    setup, hyperparameters, compute resources ‣ Appendix ‣ SLoPe: Double-Pruned Sparse
    Plus Lazy Low-Rank Adapter Pretraining of LLMs")). In all experiments, the classifications
    heads and the first linear layer following the input are dense.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估 SLoPe 对模型准确性的影响，我们在各种模型和数据集上进行了预训练实验（详细信息见附录 [N](#A14 "附录 N 实验设置、超参数、计算资源
    ‣ 附录 ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练 LLMs")）。在所有实验中，分类头和输入后的第一个线性层均为密集层。'
- en: GPT2 (Small/Large). We pretrained both the small (117 M parameters) and large
    (774 M parameters) variants of GPT2 [[43](#bib.bib43)] on the OpenWebText dataset [[1](#bib.bib1)].
    For a fair comparison, we evaluate the validation perplexity following the same
    experimental settings described in FlashAttention [[9](#bib.bib9), [7](#bib.bib7)].
    We compare SLoPe against two state-of-the-art sparse pretraining methods, including
    (a) WANDA [[48](#bib.bib48)] $\rightarrow$ a dynamic mask pretraining method for
    N:M sparsity, which serves as the foundation of follow-up work [[24](#bib.bib24),
    [55](#bib.bib55), [23](#bib.bib23)].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: GPT2（Small/Large）。我们在 OpenWebText 数据集 [[1](#bib.bib1)] 上对 GPT2 的小型（117 M 参数）和大型（774 M
    参数）版本进行了预训练 [[43](#bib.bib43)]。为了公平比较，我们在与 FlashAttention [[9](#bib.bib9), [7](#bib.bib7)]
    描述的相同实验设置下评估验证困惑度。我们将 SLoPe 与两种最先进的稀疏预训练方法进行比较，包括（a）WANDA [[48](#bib.bib48)] $\rightarrow$ 一种用于
    N:M 稀疏的动态掩码预训练方法，它作为后续工作的基础 [[24](#bib.bib24), [55](#bib.bib55), [23](#bib.bib23)]。
- en: '![Refer to caption](img/59af7653b3567b7bc315d2172f7424d5.png) ![Refer to caption](img/b8340d3cfeaec127117f380c4254c5fd.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/59af7653b3567b7bc315d2172f7424d5.png) ![参见图注](img/b8340d3cfeaec127117f380c4254c5fd.png)'
- en: 'Figure 2: Validation perplexity of GPT2-Small and GPT2-Large on OpenWebText.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：GPT2-Small 和 GPT2-Large 在 OpenWebText 上的验证困惑度。
- en: 'Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Pretraining accuracy results ‣ 3 Experimental
    results ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of
    LLMs") compares the validation perplexity of GPT2-Small and GPT2-Large across
    a range of sparse pretraining methods. While a gap in perplexity consistently
    exists between sparse and dense models, SLoPe achieves a lower perplexity compared
    to WANDA [[48](#bib.bib48)] and SR-STE [[56](#bib.bib56)]. This improved accuracy
    stems from SLoPe’s efficient allocation of the training budget. Specifically,
    SR-STE, with its dynamic pruning masks, expends a significant portion of its training
    budget (e.g. gradient updates) updating weights that may be ultimately pruned
    and not used at inference, leading to wasted resources. Appendix [A](#A1 "Appendix
    A Comparison with Dynamic Sparsity: SR-STE ‣ Appendix ‣ SLoPe: Double-Pruned Sparse
    Plus Lazy Low-Rank Adapter Pretraining of LLMs") provides further details and
    supporting evidence for this observation.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '图[2](#S3.F2 "图 2 ‣ 3.2 预训练准确率结果 ‣ 3 实验结果 ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练 LLMs")
    比较了 GPT2-Small 和 GPT2-Large 在一系列稀疏预训练方法下的验证困惑度。虽然稀疏模型和密集模型之间的困惑度差异一直存在，但 SLoPe
    比 WANDA [[48](#bib.bib48)] 和 SR-STE [[56](#bib.bib56)] 的困惑度更低。这种准确性的提升源于 SLoPe
    对训练预算的高效分配。具体而言，SR-STE 通过其动态剪枝掩码，消耗了大量的训练预算（例如，梯度更新）来更新可能最终被剪枝并且在推理时未被使用的权重，导致资源浪费。附录 [A](#A1
    "附录 A 与动态稀疏性比较：SR-STE ‣ 附录 ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练 LLMs") 提供了这一观察结果的更多细节和支持证据。'
- en: 'BERT-Large-Uncased. We pretrain BERT-Large-Uncased [[12](#bib.bib12)] (355 M
    parameters) and fine-tune it for various question-answering and text classification
    tasks, following a similar approach to [[40](#bib.bib40), [33](#bib.bib33), [41](#bib.bib41)]
    for both pretraining and fine-tuning. Appendix [G](#A7 "Appendix G BERT-Large-Uncased:
    Pretraining and Downstream Evaluation ‣ Appendix ‣ SLoPe: Double-Pruned Sparse
    Plus Lazy Low-Rank Adapter Pretraining of LLMs") provides details on the pretraining
    and fine-tuning process. We evaluate the performance of BERT-Large-Uncased on
    the SQuAD v1.1 [[45](#bib.bib45)] and GLUE [[52](#bib.bib52)] tasks. We report
    the average metric score for GLUE and present the task-specific metrics in Appendix [L](#A12
    "Appendix L Task-specific GLUE results ‣ Appendix ‣ SLoPe: Double-Pruned Sparse
    Plus Lazy Low-Rank Adapter Pretraining of LLMs").'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 'BERT-Large-Uncased。我们对BERT-Large-Uncased[[12](#bib.bib12)]（355 M参数）进行预训练，并在各种问答和文本分类任务上进行微调，采用了类似于[[40](#bib.bib40)、[33](#bib.bib33)、[41](#bib.bib41)]的预训练和微调方法。附录[G](#A7
    "Appendix G BERT-Large-Uncased: Pretraining and Downstream Evaluation ‣ Appendix
    ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs")提供了预训练和微调过程的详细信息。我们评估了BERT-Large-Uncased在SQuAD
    v1.1[[45](#bib.bib45)]和GLUE[[52](#bib.bib52)]任务上的表现。我们报告了GLUE的平均指标分数，并在附录[L](#A12
    "Appendix L Task-specific GLUE results ‣ Appendix ‣ SLoPe: Double-Pruned Sparse
    Plus Lazy Low-Rank Adapter Pretraining of LLMs")中呈现了任务特定的指标。'
- en: 'Effects of low-rank adapters. To understand the impact of low-rank adapters
    on pretraining performance, we conducted ablations using low-rank adapter ranks
    of 4, 16, and 64 for 1% of the total number of iterations. These ranks represent
    up to 6.25% of the model’s hidden dimension. Table [3](#S3.T3 "Table 3 ‣ 3.2 Pretraining
    accuracy results ‣ 3 Experimental results ‣ SLoPe: Double-Pruned Sparse Plus Lazy
    Low-Rank Adapter Pretraining of LLMs") shows the results of these settings on
    SQuAD and GLUE downstream tasks. We present per-task metrics for GLUE in Appendix [L](#A12
    "Appendix L Task-specific GLUE results ‣ Appendix ‣ SLoPe: Double-Pruned Sparse
    Plus Lazy Low-Rank Adapter Pretraining of LLMs"). As expected, adding low-rank
    adapters improve the model’s final accuracy across all tasks. Additionally, higher
    ranks improve the model’s performance at the cost of increased computational requirements.
    It is also worth to note that incorporating low-rank adapters only in the final
    iterations (1% of total iterations) is sufficient to recover pretraining accuracy.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '低秩适配器的效果。为了了解低秩适配器对预训练性能的影响，我们进行了消融实验，使用了低秩适配器秩为4、16和64的配置，占总迭代次数的1%。这些秩表示模型隐藏维度的最高6.25%。表[3](#S3.T3
    "Table 3 ‣ 3.2 Pretraining accuracy results ‣ 3 Experimental results ‣ SLoPe:
    Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs")展示了这些设置在SQuAD和GLUE下游任务上的结果。我们在附录[L](#A12
    "Appendix L Task-specific GLUE results ‣ Appendix ‣ SLoPe: Double-Pruned Sparse
    Plus Lazy Low-Rank Adapter Pretraining of LLMs")中提供了GLUE的每任务指标。如预期，添加低秩适配器提高了模型在所有任务上的最终准确性。此外，更高的秩提高了模型的性能，但增加了计算要求。值得注意的是，仅在最后的迭代（总迭代次数的1%）中引入低秩适配器就足以恢复预训练准确性。'
- en: 'Convergence rate of low-rank adapters. We hypothesized that low-rank adapters
    would converge faster due to their significantly fewer learnable parameters. To
    test this, we introduced low-rank adapters in the second phase of BERT-Large-Uncased
    pretraining and monitored their convergence rate. Figure [3](#S3.F3 "Figure 3
    ‣ 3.2 Pretraining accuracy results ‣ 3 Experimental results ‣ SLoPe: Double-Pruned
    Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs") shows the cosine similarity
    of the adapters, with the downsample adapter converging rapidly within 100 iterations
    and the upsample adapter converging slightly slower. Despite this, limiting training
    to 100 iterations still yields comparable results on downstream tasks.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '低秩适配器的收敛速度。我们假设低秩适配器由于其显著较少的可学习参数会收敛得更快。为了验证这一点，我们在BERT-Large-Uncased预训练的第二阶段引入了低秩适配器，并监测了它们的收敛速度。图[3](#S3.F3
    "Figure 3 ‣ 3.2 Pretraining accuracy results ‣ 3 Experimental results ‣ SLoPe:
    Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs")展示了适配器的余弦相似度，其中降采样适配器在100次迭代内迅速收敛，而升采样适配器的收敛稍微慢一些。尽管如此，将训练限制在100次迭代内仍然能在下游任务上获得可比的结果。'
- en: 'Table 3: SQuADv1.1 and GLUE results on BERT-Large-Uncased with different adapter
    ranks. $r$ denotes the ratio of the low-rank adapter to the hidden dimension (1024).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：不同适配器秩的BERT-Large-Uncased在SQuADv1.1和GLUE上的结果。$r$表示低秩适配器与隐藏维度（1024）的比率。
- en: '| Dataset | Dense | $r=0$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 密集 | $r=0$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SQuAD | 90.44 | 89.1 | 89.1 | 89.2 | 89.5 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| SQuAD | 90.44 | 89.1 | 89.1 | 89.2 | 89.5 |'
- en: '| GLUE | 80.22 | 77.4 | 77.7 | 77.8 | 78.2 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| GLUE | 80.22 | 77.4 | 77.7 | 77.8 | 78.2 |'
- en: '![Refer to caption](img/2905e92f0d624ea86e2ec3558d5154b0.png) ![Refer to caption](img/ccbba7c1df1509a4c8da4a0107ee5894.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2905e92f0d624ea86e2ec3558d5154b0.png) ![参见说明](img/ccbba7c1df1509a4c8da4a0107ee5894.png)'
- en: (a)                                                                                                                                    
    (b)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (a)                                                                                                                                                                             (b)
- en: 'Figure 3: (a) The speedup achieved using cuSPARSELt backend in PyTorch for
    Attention ($d_{out}=d_{in}$) matrices with a batch size of 2048\. (b) The cosine
    similarity of the low-rank adapters and the converged adapters for different layers
    in the model. The cosine similarities are averaged among the 24 layers of BERT-Large-Uncased.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: (a) 使用 PyTorch 中的 cuSPARSELt 后端对 Attention ($d_{out}=d_{in}$) 矩阵在批量大小为
    2048 的情况下实现的加速效果。(b) 模型中不同层的低秩适配器和收敛适配器的余弦相似度。余弦相似度在 BERT-Large-Uncased 的24层中平均计算。'
- en: 'Effects of mixed N:M sparsity. To study the sensitivity of different blocks
    to varying sparsity ratios and to assess their relative importance, we experiment
    across a range of configurations: (a) [2:4-2:4] $\rightarrow$ we reverse the sparsity
    ratios for the first and last 12 blocks. Note that, to reduce computational costs,
    we use the same dense checkpoint for Phase-1 in all settings and a low-rank adapter
    of rank 40 for all models. We also replicate this experiment using WANDA [[48](#bib.bib48)]
    and report the comparison results.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 混合 N:M 稀疏的影响。为了研究不同区块对稀疏比率的敏感性以及评估其相对重要性，我们在一系列配置中进行实验：(a) [2:4-2:4] $\rightarrow$
    我们逆转了前12块和最后12块的稀疏比率。请注意，为了减少计算成本，我们在所有设置中使用相同的密集检查点作为第1阶段的起始点，并为所有模型使用40维的低秩适配器。我们还使用
    WANDA [[48](#bib.bib48)] 重复了这个实验，并报告了比较结果。
- en: 'Table 4: SQuADv1.1 results on BERT-Large-Uncased for different sparsity settings.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 不同稀疏设置下 BERT-Large-Uncased 在 SQuADv1.1 上的结果。'
- en: '| Sparsity Pattern | SQuAD | SQuAD | GLUE | GLUE |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏模式 | SQuAD | SQuAD | GLUE | GLUE |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| (First 12 blocks - Last 12 blocks) | SLoPe | WANDA | SLoPe | WANDA |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| (前12块 - 后12块) | SLoPe | WANDA | SLoPe | WANDA |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 2:4-2:4 | 90.17 | 89.93 | 79.08 | 78.84 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 2:4-2:4 | 90.17 | 89.93 | 79.08 | 78.84 |'
- en: '| 2:4-2:8 | 89.85 | 89.55 | 79.03 | 77.24 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 2:4-2:8 | 89.85 | 89.55 | 79.03 | 77.24 |'
- en: '| 2:8-2:4 | 89.67 | 86.57 | 75.92 | 69.08 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 2:8-2:4 | 89.67 | 86.57 | 75.92 | 69.08 |'
- en: 'Table [4](#S3.T4 "Table 4 ‣ 3.2 Pretraining accuracy results ‣ 3 Experimental
    results ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of
    LLMs") summarizes the GLUE and SQuAD results for these settings. As the results
    show, increasing the sparsity ratio reduces the accuracy of the model on all tasks.
    But when the first 12 blocks of the model are pruned, the accuracy drop is significantly
    higher, especially on the GLUE dataset. We conclude that the first blocks of the
    model are more sensitive to sparsity during pretraining, but one can sparsify
    the last blocks of LLMs more aggressively. We observe a similar pattern in WANDA
    results as well, but WANDA performs consistently worse than SLoPe in these cases.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [4](#S3.T4 "表 4 ‣ 3.2 预训练准确率结果 ‣ 3 实验结果 ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练 LLMs")
    总结了这些设置下的 GLUE 和 SQuAD 结果。结果显示，增加稀疏比率会降低模型在所有任务上的准确性。但当模型的前12块被剪枝时，准确率下降显著增加，尤其是在
    GLUE 数据集上。我们得出结论，模型的前块对稀疏度在预训练期间更敏感，但可以更激进地对 LLM 的最后块进行稀疏处理。我们在 WANDA 结果中也观察到类似的模式，但在这些情况下，WANDA
    的表现始终不如 SLoPe。'
- en: 'Effects of sparsification on different modules. Each block in LLMs consists
    of a self-attention module and an MLP mixer module, each containing multiple linear
    layers. We have analyzed the sensitivity of SLoPe to pruning each of those modules.
    Our results in Appendix [F](#A6 "Appendix F SLoPe sensitivity to pruning different
    module in transformer ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank
    Adapter Pretraining of LLMs") demonstrate that SLoPe can sustain competitive quality
    results while pruning all modules in the model.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '稀疏化对不同模块的影响。每个 LLM 模块包含一个自注意力模块和一个 MLP 混合器模块，每个模块包含多个线性层。我们分析了 SLoPe 对剪枝每个模块的敏感性。附录
    [F](#A6 "附录 F SLoPe 对剪枝不同模块的敏感性 ‣ 附录 ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练 LLMs") 的结果表明，SLoPe
    在剪枝模型的所有模块时能够保持具有竞争力的质量结果。'
- en: 4 Conclusion
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4 结论
- en: In conclusion, SLoPe improves both pretraining and inference times while reducing
    memory footprint with negligible impact on model performance. SLoPe achieves these
    benefits by effectively using N:M sparsity and lazy low-rank adapters in both
    forward and backward passes, supported by an efficient design of CUDA kernels.
    Additionally, the use of lazy low-rank adapters allows for balancing memory footprint
    and model accuracy across a wide range models. The results show that SLoPe achieve
    up to 1.14$\times$ (inference).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，SLoPe 在减少内存占用的同时提高了预训练和推理时间，对模型性能的影响微乎其微。SLoPe 通过在前向和反向传递中有效利用 N:M 稀疏性和延迟低秩适配器，并支持高效的
    CUDA 内核设计，达到了这些好处。此外，使用延迟低秩适配器可以在广泛的模型中平衡内存占用和模型准确性。结果表明，SLoPe 实现了高达 1.14$\times$（推理）的提升。
- en: Acknowledgments and Disclosure of Funding
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 致谢与资金披露
- en: This work was also supported in part by NSERC Discovery Grants (RGPIN-06516,
    DGECR00303), the Canada Research Chairs program, Ontario Early Researcher award,
    the Canada Research Chairs program, the Ontario Early Researcher Award, and the
    Digital Research Alliance of Canada ([www.alliancecan.ca](www.alliancecan.ca)).
    Work of Zhao Zhang was supported by National Science Foundation OAC-2401246\.
    We also acknowledge the Texas Advanced Computing Center (TACC) at The University
    of Texas at Austin for providing HPC resources that have contributed to the research
    results reported within this paper ([http://www.tacc.utexas.edu)](http://www.tacc.utexas.edu))).
    We extend our gratitude towards David Fleet, Karolina Dziugaite, Suvinay Subramanian,
    Cliff Young, and Faraz Shahsavan for reviewing the paper and providing insightful
    feedback. We also thank the extended team at Google DeepMind who enabled and supported
    this research direction.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作还部分得到了 NSERC 发现奖励（RGPIN-06516, DGECR00303）、加拿大研究主席计划、安大略省早期研究者奖、加拿大研究主席计划、安大略省早期研究者奖以及加拿大数字研究联盟的支持
    ([www.alliancecan.ca](www.alliancecan.ca))。赵张的工作得到了国家科学基金 OAC-2401246 的支持。我们还感谢德克萨斯大学奥斯汀分校的德克萨斯先进计算中心
    (TACC) 提供的 HPC 资源，这些资源对本文报告的研究结果有贡献 ([http://www.tacc.utexas.edu](http://www.tacc.utexas.edu))。我们对
    David Fleet、Karolina Dziugaite、Suvinay Subramanian、Cliff Young 和 Faraz Shahsavan
    感谢他们审阅了论文并提供了有见地的反馈。我们还感谢谷歌 DeepMind 的扩展团队，他们使得这一研究方向成为可能并提供了支持。
- en: References
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Ellie Pavlick Aaron Gokaslan, Vanya Cohen and Stefanie Tellex. OpenWebText
    Corpus, 2019.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Ellie Pavlick、Aaron Gokaslan、Vanya Cohen 和 Stefanie Tellex。OpenWebText
    语料库，2019。'
- en: '[2] Dimitris Bertsimas, Ryan Cory-Wright, and Nicholas AG Johnson. Sparse Plus
    Low Rank Matrix Decomposition: A Discrete Optimization Approach. JMLR, 2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Dimitris Bertsimas、Ryan Cory-Wright 和 Nicholas AG Johnson。稀疏加低秩矩阵分解：一种离散优化方法。JMLR，2023。'
- en: '[3] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher
    Ré. Scatterbrain: Unifying Sparse and Low-rank Attention Approximation. arXiv
    preprint arXiv:2110.15343, 2021.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Beidi Chen、Tri Dao、Eric Winsor、Zhao Song、Atri Rudra 和 Christopher Ré。Scatterbrain：统一稀疏和低秩注意力近似。arXiv
    预印本 arXiv:2110.15343，2021。'
- en: '[4] Stanley F Chen, Douglas Beeferman, and Roni Rosenfeld. Evaluation Metrics
    for Language Models. Carnegie Mellon University, 1998.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Stanley F Chen、Douglas Beeferman 和 Roni Rosenfeld。语言模型评估指标。卡内基梅隆大学，1998。'
- en: '[5] Zhaodong Chen, Zheng Qu, Yuying Quan, Liu Liu, Yufei Ding, and Yuan Xie.
    Dynamic N:M Fine-grained Structured Sparse Attention Mechanism. In PPoPP, 2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Zhaodong Chen、Zheng Qu、Yuying Quan、Liu Liu、Yufei Ding 和 Yuan Xie。动态 N:M
    细粒度结构稀疏注意机制。在 PPoPP，2023。'
- en: '[6] Compute Canada. Compute Canada. [https://computecanada.ca/](https://computecanada.ca/).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Compute Canada。Compute Canada。 [https://computecanada.ca/](https://computecanada.ca/)。'
- en: '[7] Tri Dao. Flashattention-2: Faster Attention with Better Parallelism and
    Work Partitioning. arXiv preprint arXiv:2307.08691, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Tri Dao。Flashattention-2：更快的注意力，具有更好的并行性和工作划分。arXiv 预印本 arXiv:2307.08691，2023。'
- en: '[8] Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra,
    and Christopher Re. Pixelated Butterfly: Simple and Efficient Sparse Training
    for Neural Network Models. arXiv preprint arXiv:2112.00029, 2021.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Tri Dao、Beidi Chen、Kaizhao Liang、Jiaming Yang、Zhao Song、Atri Rudra 和 Christopher
    Re。像素化蝴蝶：用于神经网络模型的简单高效稀疏训练。arXiv 预印本 arXiv:2112.00029，2021。'
- en: '[9] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention:
    Fast and Memory-Efficient Exact Attention with IO-Awareness. In NeurIPS, 2022.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Tri Dao、Daniel Y. Fu、Stefano Ermon、Atri Rudra 和 Christopher Ré。FlashAttention：快速且内存高效的精确注意力，具有
    IO 认知。发表于 NeurIPS，2022。'
- en: '[10] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA:
    Efficient Finetuning of Quantized LLMs. arXiv preprint arXiv:2305.14314, 2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer. QLoRA:
    高效的量化 LLM 微调。arXiv 预印本 arXiv:2305.14314, 2023。'
- en: '[11] Tim Dettmers and Luke Zettlemoyer. Sparse Networks from Scratch: Faster
    Training without Losing Performance. arXiv preprint arXiv:1907.04840, 2019.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Tim Dettmers 和 Luke Zettlemoyer. 从零开始的稀疏网络：更快的训练而不损失性能。arXiv 预印本 arXiv:1907.04840,
    2019。'
- en: '[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:
    Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova. BERT:
    用于语言理解的深度双向 Transformer 的预训练。arXiv 预印本 arXiv:1810.04805, 2018。'
- en: '[13] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael
    Carbin. Linear Mode Connectivity and the Lottery Ticket Hypothesis. In ICML, 2020.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, 和 Michael Carbin.
    线性模式连接性与彩票票据假设。发表于 ICML, 2020。'
- en: '[14] Elias Frantar and Dan Alistarh. SparseGPT: Massive Language Models can
    be Accurately Pruned in One-shot. In ICML, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Elias Frantar 和 Dan Alistarh. SparseGPT: 大型语言模型可以通过一次修剪准确完成。发表于 ICML,
    2023。'
- en: '[15] Trevor Gale, Erich Elsen, and Sara Hooker. The State of Sparsity in Deep
    Neural Networks. arXiv preprint arXiv:1902.09574, 2019.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Trevor Gale, Erich Elsen, 和 Sara Hooker. 深度神经网络中的稀疏性现状。arXiv 预印本 arXiv:1902.09574,
    2019。'
- en: '[16] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
    Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An
    800GB Dataset of Diverse Text for Language Modeling. arXiv preprint arXiv:2101.00027,
    2020.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
    Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, 等等. The Pile: 一个 800GB
    的多样文本数据集用于语言建模。arXiv 预印本 arXiv:2101.00027, 2020。'
- en: '[17] Han Guo, Philip Greengard, Eric P Xing, and Yoon Kim. LQ-LoRA: Low-rank
    Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning. arXiv
    preprint arXiv:2311.12023, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Han Guo, Philip Greengard, Eric P Xing, 和 Yoon Kim. LQ-LoRA: 低秩加量化矩阵分解以提高语言模型微调的效率。arXiv
    预印本 arXiv:2311.12023, 2023。'
- en: '[18] Song Han, Huizi Mao, and William J Dally. Deep Compression: Compressing
    Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. arXiv
    preprint arXiv:1510.00149, 2015.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Song Han, Huizi Mao, 和 William J Dally. 深度压缩：通过修剪、训练量化和霍夫曼编码压缩深度神经网络。arXiv
    预印本 arXiv:1510.00149, 2015。'
- en: '[19] Song Han, Jeff Pool, John Tran, and William Dally. Learning both Weights
    and Connections for Efficient Neural Network. NeurIPS, 2015.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Song Han, Jeff Pool, John Tran, 和 William Dally. 学习权重和连接以提高神经网络的效率。NeurIPS,
    2015。'
- en: '[20] Babak Hassibi and David Stork. Second Order Derivatives for Network Pruning:
    Optimal Brain Surgeon. NeurIPS, 1992.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Babak Hassibi 和 David Stork. 网络修剪的二阶导数：最佳脑外科医生。NeurIPS, 1992。'
- en: '[21] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra
    Peste. Sparsity in Deep Learning: Pruning and Growth for Efficient Inference and
    Training in Neural Networks. JMLR, 2021.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, 和 Alexandra
    Peste. 深度学习中的稀疏性：神经网络中高效推理与训练的修剪与增长。JMLR, 2021。'
- en: '[22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank Adaptation of Large Language
    Models. arXiv preprint arXiv:2106.09685, 2021.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, 和 Weizhu Chen. LoRA: 大型语言模型的低秩适应。arXiv 预印本 arXiv:2106.09685,
    2021。'
- en: '[23] Yuezhou Hu, Kang Zhao, Weiyu Huang, Jianfei Chen, and Jun Zhu. Accelerating
    Transformer Pre-Training with 2:4 Sparsity. In ICML, 2024.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Yuezhou Hu, Kang Zhao, Weiyu Huang, Jianfei Chen, 和 Jun Zhu. 通过 2:4 稀疏性加速
    Transformer 预训练。发表于 ICML, 2024。'
- en: '[24] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, and
    Daniel Soudry. Accelerated Sparse Neural Training: A Provable and Efficient Method
    to find N:M Transposable Masks. NeurIPS, 2021.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, 和 Daniel
    Soudry. 加速稀疏神经训练：一种可证明且高效的方法来找到 N:M 可转置掩码。NeurIPS, 2021。'
- en: '[25] Yu Ji, Ling Liang, Lei Deng, Youyang Zhang, Youhui Zhang, and Yuan Xie.
    TETRIS: Tile-matching the Tremendous Irregular Sparsity. NeurIPS, 2018.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Yu Ji, Ling Liang, Lei Deng, Youyang Zhang, Youhui Zhang, 和 Yuan Xie.
    TETRIS: 瓦片匹配巨大不规则稀疏性。NeurIPS, 2018。'
- en: '[26] Sheng-Chun Kao, Amir Yazdanbakhsh, Suvinay Subramanian, Shivani Agrawal,
    Utku Evci, and Tushar Krishna. Training Recipe for N:M Structured Sparsity with
    Decaying Pruning Mask. arXiv preprint arXiv:2209.07617, 2022.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Sheng-Chun Kao, Amir Yazdanbakhsh, Suvinay Subramanian, Shivani Agrawal,
    Utku Evci, 和 Tushar Krishna. N:M 结构稀疏性的训练方法与衰减修剪掩码。arXiv 预印本 arXiv:2209.07617,
    2022。'
- en: '[27] Yann LeCun, John Denker, and Sara Solla. Optimal Brain Damage. NeurIPS,
    2, 1989.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Yann LeCun、John Denker 和 Sara Solla。《最佳脑损伤》。NeurIPS，2，1989年。'
- en: '[28] Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen,
    and Tuo Zhao. LoSparse: Structured Compression of Large Language Models based
    on Low-Rank and Sparse Approximation. arXiv preprint arXiv:2306.11222, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Yixiao Li、Yifan Yu、Qingru Zhang、Chen Liang、Pengcheng He、Weizhu Chen 和
    Tuo Zhao。《LoSparse：基于低秩和稀疏近似的大型语言模型的结构化压缩》。arXiv 预印本 arXiv:2306.11222，2023年。'
- en: '[29] Hong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. Same Pre-training
    Loss, Better Downstream: Implicit Bias Matters for Language Models. In ICML, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Hong Liu、Sang Michael Xie、Zhiyuan Li 和 Tengyu Ma。《相同的预训练损失，更好的下游任务：隐式偏差对语言模型的重要性》。在
    ICML，2023年。'
- en: '[30] Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting
    Cheng, and Jian Sun. MetaPruning: Meta Learning for Automatic Neural Network Channel
    Pruning. In ICCV, 2019.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Zechun Liu、Haoyuan Mu、Xiangyu Zhang、Zichao Guo、Xin Yang、Kwang-Ting Cheng
    和 Jian Sun。《MetaPruning：用于自动神经网络通道剪枝的元学习》。在 ICCV，2019年。'
- en: '[31] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization.
    arXiv preprint arXiv:1711.05101, 2017.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Ilya Loshchilov 和 Frank Hutter。《解耦权重衰减正则化》。arXiv 预印本 arXiv:1711.05101，2017年。'
- en: '[32] Yucheng Lu, Shivani Agrawal, Suvinay Subramanian, Oleg Rybakov, Christopher
    De Sa, and Amir Yazdanbakhsh. STEP: Learning N:M Structured Sparsity Masks from
    Scratch with Precondition. arXiv preprint arXiv:2302.01172, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Yucheng Lu、Shivani Agrawal、Suvinay Subramanian、Oleg Rybakov、Christopher
    De Sa 和 Amir Yazdanbakhsh。《STEP：从头开始学习 N:M 结构稀疏掩码》。arXiv 预印本 arXiv:2302.01172，2023年。'
- en: '[33] Mohammad Mozaffari, Sikan Li, Zhao Zhang, and Maryam Mehri Dehnavi. MKOR:
    Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates. In NeurIPS,
    2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Mohammad Mozaffari、Sikan Li、Zhao Zhang 和 Maryam Mehri Dehnavi。《MKOR：基于
    Kronecker 因子的动量优化器，使用 Rank-1 更新》。在 NeurIPS，2023年。'
- en: '[34] Tan Nguyen, Vai Suliafu, Stanley Osher, Long Chen, and Bao Wang. FMMformer:
    Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention.
    In NeurIPS, 2021.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Tan Nguyen、Vai Suliafu、Stanley Osher、Long Chen 和 Bao Wang。《FMMformer：通过分解近场和远场注意力实现高效灵活的
    Transformer》。在 NeurIPS，2021年。'
- en: '[35] Mahdi Nikdan, Soroush Tabesh, and Dan Alistarh. RoSA: Accurate Parameter-Efficient
    Fine-Tuning via Robust Adaptation. arXiv preprint arXiv:2401.04679, 2024.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Mahdi Nikdan、Soroush Tabesh 和 Dan Alistarh。《RoSA：通过稳健适配实现准确的参数高效微调》。arXiv
    预印本 arXiv:2401.04679，2024年。'
- en: '[36] NVIDIA, Péter Vingelmann, and Frank H.P. Fitzek. CUDA, release: 10.2.89,
    2020.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] NVIDIA、Péter Vingelmann 和 Frank H.P. Fitzek。《CUDA》，版本：10.2.89，2020年。'
- en: '[37] NVIDIA Corporation. NVIDIA Ampere Architecture In-Depth. [https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] NVIDIA Corporation。《NVIDIA Ampere 架构深入剖析》。[https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth)。'
- en: '[38] NVIDIA Corporation. NVIDIA cuSPARSELt. [https://docs.nvidia.com/cuda/cusparselt/index.html](https://docs.nvidia.com/cuda/cusparselt/index.html).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] NVIDIA Corporation。《NVIDIA cuSPARSELt》。[https://docs.nvidia.com/cuda/cusparselt/index.html](https://docs.nvidia.com/cuda/cusparselt/index.html)。'
- en: '[39] NVIDIA Corporation. NVIDIA cuSPARSELt Functions. [https://docs.nvidia.com/cuda/cusparselt/functions.html](https://docs.nvidia.com/cuda/cusparselt/functions.html).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] NVIDIA Corporation。《NVIDIA cuSPARSELt 函数》。[https://docs.nvidia.com/cuda/cusparselt/functions.html](https://docs.nvidia.com/cuda/cusparselt/functions.html)。'
- en: '[40] NVIDIA Corporation. NVIDIA Deep Learning Examples. [https://github.com/NVIDIA/DeepLearningExamples](https://github.com/NVIDIA/DeepLearningExamples).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] NVIDIA Corporation。《NVIDIA 深度学习示例》。[https://github.com/NVIDIA/DeepLearningExamples](https://github.com/NVIDIA/DeepLearningExamples)。'
- en: '[41] J Gregory Pauloski, Qi Huang, Lei Huang, Shivaram Venkataraman, Kyle Chard,
    Ian Foster, and Zhao Zhang. KAISA: An Adaptive Second-order Optimizer Framework
    for Deep Neural Networks. In SC, 2021.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J Gregory Pauloski、Qi Huang、Lei Huang、Shivaram Venkataraman、Kyle Chard、Ian
    Foster 和 Zhao Zhang。《KAISA：用于深度神经网络的自适应二阶优化框架》。在 SC，2021年。'
- en: '[42] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    Improving Language Understanding by Generative Pre-training. OpenAI, 2018.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Alec Radford、Karthik Narasimhan、Tim Salimans、Ilya Sutskever 等。《通过生成预训练改善语言理解》。OpenAI，2018年。'
- en: '[43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
    Sutskever, et al. Language Models are Unsupervised Multitask Learners. OpenAI
    blog, 1(8):9, 2019.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Alec Radford、Jeffrey Wu、Rewon Child、David Luan、Dario Amodei、Ilya Sutskever
    等。《语言模型是无监督的多任务学习者》。OpenAI 博客，1(8):9，2019年。'
- en: '[44] Abhimanyu Rajeshkumar Bambhaniya, Amir Yazdanbakhsh, Suvinay Subramanian,
    Sheng-Chun Kao, Shivani Agrawal, Utku Evci, and Tushar Krishna. Progressive Gradient
    Flow for Robust N:M Sparsity Training in Transformers. arXiv e-prints, 2024.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Abhimanyu Rajeshkumar Bambhaniya、Amir Yazdanbakhsh、Suvinay Subramanian、Sheng-Chun
    Kao、Shivani Agrawal、Utku Evci 和 Tushar Krishna。变换器中的稳健 N:M 稀疏性训练的渐进梯度流。arXiv 预印本，2024。'
- en: '[45] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD:
    100,000+ Questions for Machine Comprehension of Text. arXiv preprint arXiv:1606.05250,
    2016.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Pranav Rajpurkar、Jian Zhang、Konstantin Lopyrev 和 Percy Liang。SQuAD：用于机器理解文本的
    100,000+ 个问题。arXiv 预印本 arXiv:1606.05250，2016。'
- en: '[46] Victor Sanh, Thomas Wolf, and Alexander Rush. Movement Pruning: Adaptive
    Sparsity by Fine-Tuning. NeurIPS, 2020.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Victor Sanh、Thomas Wolf 和 Alexander Rush。运动剪枝：通过微调实现的自适应稀疏性。NeurIPS，2020。'
- en: '[47] Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, HyoungSeok Kim,
    Boseop Kim, Kyunghyun Cho, Gichang Lee, Woomyoung Park, Jung-Woo Ha, et al. On
    the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language
    Model. arXiv preprint arXiv:2204.13509, 2022.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Seongjin Shin、Sang-Woo Lee、Hwijeen Ahn、Sungdong Kim、HyoungSeok Kim、Boseop
    Kim、Kyunghyun Cho、Gichang Lee、Woomyoung Park、Jung-Woo Ha 等。预训练语料对大规模语言模型的上下文学习的影响。arXiv
    预印本 arXiv:2204.13509，2022。'
- en: '[48] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A Simple and Effective
    Pruning Approach for Large Language Models. arXiv preprint arXiv:2306.11695, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Mingjie Sun、Zhuang Liu、Anna Bair 和 J Zico Kolter。大型语言模型的简单有效的剪枝方法。arXiv
    预印本 arXiv:2306.11695，2023。'
- en: '[49] Wei Sun, Aojun Zhou, Sander Stuijk, Rob Wijnhoven, Andrew O Nelson, Henk
    Corporaal, et al. DominoSearch: Find Layer-wise Fine-grained N:M Sparse Schemes
    from Dense Neural Networks. In NeurIPS, 2021.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Wei Sun、Aojun Zhou、Sander Stuijk、Rob Wijnhoven、Andrew O Nelson、Henk Corporaal
    等。DominoSearch：从稠密神经网络中找到层级细粒度的 N:M 稀疏方案。发表于 NeurIPS，2021。'
- en: '[50] Texas Advanced Computing Center. Lonestar 6. [https://tacc.utexas.edu/systems/lonestar6/](https://tacc.utexas.edu/systems/lonestar6/).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] 德州高级计算中心。Lonestar 6。 [https://tacc.utexas.edu/systems/lonestar6/](https://tacc.utexas.edu/systems/lonestar6/)。'
- en: '[51] Vithursan Thangarasa, Abhay Gupta, William Marshall, Tianda Li, Kevin
    Leong, Dennis DeCoste, Sean Lie, and Shreyas Saxena. SPDF: Sparse Pre-training
    and Dense Fine-tuning for Large Language Models. arXiv preprint arXiv:2303.10464,
    2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Vithursan Thangarasa、Abhay Gupta、William Marshall、Tianda Li、Kevin Leong、Dennis
    DeCoste、Sean Lie 和 Shreyas Saxena。SPDF：大规模语言模型的稀疏预训练和密集微调。arXiv 预印本 arXiv:2303.10464，2023。'
- en: '[52] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
    Samuel R Bowman. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural
    Language Understanding. arXiv preprint arXiv:1804.07461, 2018.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Alex Wang、Amanpreet Singh、Julian Michael、Felix Hill、Omer Levy 和 Samuel
    R Bowman。GLUE：自然语言理解的多任务基准和分析平台。arXiv 预印本 arXiv:1804.07461，2018。'
- en: '[53] Lucas Wilkinson, Kazem Cheshmi, and Maryam Mehri Dehnavi. Register Tiling
    for Unstructured Sparsity in Neural Network Inference. PLDI, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Lucas Wilkinson、Kazem Cheshmi 和 Maryam Mehri Dehnavi。神经网络推理中的非结构化稀疏性的寄存器平铺。PLDI，2023。'
- en: '[54] Samuel Williams, Andrew Waterman, and David Patterson. Roofline: An Insightful
    Visual Performance Model for Multicore Architectures. Communications of the ACM,
    2009.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Samuel Williams、Andrew Waterman 和 David Patterson。Roofline：多核架构的深刻视觉性能模型。ACM
    通讯，2009。'
- en: '[55] Yuxin Zhang, Yiting Luo, Mingbao Lin, Yunshan Zhong, Jingjing Xie, Fei
    Chao, and Rongrong Ji. Bi-directional Masks for Efficient N:M Sparse Training.
    arXiv preprint arXiv:2302.06058, 2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Yuxin Zhang、Yiting Luo、Mingbao Lin、Yunshan Zhong、Jingjing Xie、Fei Chao
    和 Rongrong Ji。用于高效 N:M 稀疏训练的双向掩码。arXiv 预印本 arXiv:2302.06058，2023。'
- en: '[56] Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan,
    Wenxiu Sun, and Hongsheng Li. Learning N:M Fine-grained Structured Sparse Neural
    Networks from Scratch. arXiv preprint arXiv:2102.04010, 2021.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Aojun Zhou、Yukun Ma、Junnan Zhu、Jianbo Liu、Zhijie Zhang、Kun Yuan、Wenxiu
    Sun 和 Hongsheng Li。从零开始学习 N:M 细粒度结构化稀疏神经网络。arXiv 预印本 arXiv:2102.04010，2021。'
- en: \doparttoc\faketableofcontents
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: \doparttoc\faketableofcontents
- en: Appendix
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: \parttoc
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: \parttoc
- en: 'Appendix A Comparison with Dynamic Sparsity: SR-STE'
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 A 与动态稀疏性的比较：SR-STE
- en: 'We pretrained GPT2-Small (Section [3.2](#S3.SS2 "3.2 Pretraining accuracy results
    ‣ 3 Experimental results ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter
    Pretraining of LLMs")) using the SR-STE method [[56](#bib.bib56)] and reported
    the perplexity results in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Pretraining accuracy
    results ‣ 3 Experimental results ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank
    Adapter Pretraining of LLMs"). SR-STE aims to mitigate the Sparse Architecture
    Divergence (SAD) by dynamically adjusting the sparsity mask throughout training.
    We choose the hyperparameters in SR-STE, so that the update rule of the gradient
    follows Equation [12](#A1.E12 "In Appendix A Comparison with Dynamic Sparsity:
    SR-STE ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining
    of LLMs"), where $M$ denotes the element-wise multiplication.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用SR-STE方法对GPT2-Small进行了预训练（第[3.2](#S3.SS2 "3.2 预训练准确度结果 ‣ 3 实验结果 ‣ SLoPe：双重剪枝稀疏加懒惰低秩适配器的LLMs预训练")节），并在图[2](#S3.F2
    "图 2 ‣ 3.2 预训练准确度结果 ‣ 3 实验结果 ‣ SLoPe：双重剪枝稀疏加懒惰低秩适配器的LLMs预训练")中报告了困惑度结果。SR-STE旨在通过在训练过程中动态调整稀疏掩码来减轻稀疏架构分歧（SAD）。我们在SR-STE中选择了超参数，使得梯度的更新规则遵循方程[12](#A1.E12
    "在附录 A 动态稀疏性的比较：SR-STE ‣ 附录 ‣ SLoPe：双重剪枝稀疏加懒惰低秩适配器的LLMs预训练")，其中$M$表示逐元素乘法。
- en: '|  | $\nabla_{W}\mathcal{L}\xleftarrow{}M\odot\nabla_{W}\mathcal{L}$ |  | (12)
    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla_{W}\mathcal{L}\xleftarrow{}M\odot\nabla_{W}\mathcal{L}$ |  | (12)
    |'
- en: To understand the performance gap between SR-STE and SLoPe (our method) for
    the same training budget, we analyzed the mask dynamics in SR-STE. We plotted
    the average number of mask elements changes during training compared to the final
    converged mask sparsity pattern. High mask change values indicate that training
    resources are spent on updating weights that ultimately get pruned and do not
    necessarily contribute to the final model accuracy.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解SR-STE与SLoPe（我们的方法）在相同训练预算下的性能差距，我们分析了SR-STE中的掩码动态。我们绘制了训练过程中掩码元素变化的平均数量，与最终收敛的掩码稀疏模式进行比较。高掩码变化值表明训练资源用于更新最终被剪枝的权重，这些权重并不一定对最终模型的准确性有所贡献。
- en: 'Figure [4](#A1.F4 "Figure 4 ‣ Appendix A Comparison with Dynamic Sparsity:
    SR-STE ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining
    of LLMs") shows this average mask difference per iteration relative to the converged
    model. As training progresses, the mask difference decreases, demonstrating SR-STE’s
    convergence to a specific sparsity pattern. However, in SLoPe, where all resources
    are dedicated to optimizing weights under a static mask⁴⁴4We determine the pruning
    mask at the very first iteration and maintain it for the rest of training., SR-STE’s
    dynamic approach leads to wasted computation (represented by the area under the
    curve in Figure [4](#A1.F4 "Figure 4 ‣ Appendix A Comparison with Dynamic Sparsity:
    SR-STE ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining
    of LLMs")). Consequently, for the same training budget, SLoPe achieves a lower
    perplexity in comparison to SR-STE due to its static mask approach.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#A1.F4 "图 4 ‣ 附录 A 动态稀疏性的比较：SR-STE ‣ 附录 ‣ SLoPe：双重剪枝稀疏加懒惰低秩适配器的LLMs预训练")展示了相对于收敛模型的每次迭代的平均掩码差异。随着训练的进行，掩码差异减少，显示了SR-STE对特定稀疏模式的收敛。然而，在SLoPe中，所有资源都致力于在静态掩码⁴⁴4我们在第一次迭代时确定剪枝掩码，并在其余训练中保持不变。下优化权重，SR-STE的动态方法导致了计算浪费（如图[4](#A1.F4
    "图 4 ‣ 附录 A 动态稀疏性的比较：SR-STE ‣ 附录 ‣ SLoPe：双重剪枝稀疏加懒惰低秩适配器的LLMs预训练")中曲线下的区域所示）。因此，对于相同的训练预算，SLoPe由于其静态掩码方法相比SR-STE获得了更低的困惑度。
- en: '![Refer to caption](img/19afc5071fdce0d97562f81883e957f2.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/19afc5071fdce0d97562f81883e957f2.png)'
- en: 'Figure 4: Average mask difference between each iteration and the converged
    sparsity pattern in GPT2-Small pretraining using SR-STE. The highlighted area
    shows the ratio of the resources used for updating weights that are pruned and
    not used in the inference of the model.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：使用SR-STE对GPT2-Small预训练过程中每次迭代与收敛稀疏模式之间的平均掩码差异。突出显示的区域显示了用于更新剪枝权重且在模型推理中未使用的资源比例。
- en: 'Appendix B cuSPARSELt Initialization Overhead: Static vs. Dynamic Sparsity'
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 B cuSPARSELt 初始化开销：静态与动态稀疏性
- en: 'This section analyzes the time breakdown of the cuSPARSELt SpMM pipeline, highlighting
    the significant overheads associated with dynamically changing sparsity masks.
    The cuSPARSELt SpMM operation consists of two main phases: (1) Setup and (2) Matrix
    Multiplication. The setup phase involves initializing matrix handles and compressing
    the 2:4 sparse matrix. This compression copies non-zero values into a contiguous
    memory layout and generates indices for those values. The matrix multiplication
    phase leverages this metadata to perform the sparse matrix-matrix multiplication.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 本节分析了 cuSPARSELt SpMM 管道的时间分解，突出了与动态变化的稀疏掩码相关的重要开销。cuSPARSELt SpMM 操作包括两个主要阶段：（1）设置和（2）矩阵乘法。设置阶段涉及初始化矩阵句柄和压缩
    2:4 稀疏矩阵。该压缩将非零值复制到连续的内存布局中，并为这些值生成索引。矩阵乘法阶段利用这些元数据来执行稀疏矩阵-矩阵乘法。
- en: 'Figure [5](#A2.F5 "Figure 5 ‣ Appendix B cuSPARSELt Initialization Overhead:
    Static vs. Dynamic Sparsity ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy
    Low-Rank Adapter Pretraining of LLMs") shows the setup and multiplication time
    for square matrices using the cuSPARSELt SpMM backend. As evident from the figure,
    the setup overhead is significantly larger than the actual matrix multiplication
    time. For SLoPe, which employs static sparsity masks, the setup cost is incurred
    only once and becomes negligible compared to the numerous matrix multiplications
    performed during training and inference. However, for dynamic sparsity patterns,
    such as Sparse-Dense Pretraining [[23](#bib.bib23)], Bidirectional Masks [[55](#bib.bib55)],
    and other similar methods[[24](#bib.bib24), [49](#bib.bib49), [32](#bib.bib32),
    [56](#bib.bib56)], this setup overhead can be substantial, leading to reduced
    speedup (as observed in Section [3.1](#S3.SS1 "3.1 End-to-end speedup and memory
    saving: pretraining and inference ‣ 3 Experimental results ‣ SLoPe: Double-Pruned
    Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs") for Sparse-Dense Pretraining)
    or slowdowns in some configurations (as discussed in Appendix [H](#A8 "Appendix
    H Performance overhead of bidirectional mask ‣ Appendix ‣ SLoPe: Double-Pruned
    Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs")).⁵⁵5A recent work observed
    a similar overhead using dynamic sparsity in cuSPARSELt SpMM pipeline [[5](#bib.bib5)].'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [5](#A2.F5 "图 5 ‣ 附录 B cuSPARSELt 初始化开销：静态 vs. 动态稀疏性 ‣ 附录 ‣ SLoPe: 双重修剪稀疏加上懒惰低秩适配器
    LLM 预训练") 显示了使用 cuSPARSELt SpMM 后端的方阵的设置和乘法时间。从图中可以明显看出，设置开销显著大于实际的矩阵乘法时间。对于使用静态稀疏掩码的
    SLoPe，设置成本仅需支付一次，与训练和推理过程中执行的众多矩阵乘法相比，这一成本变得微不足道。然而，对于动态稀疏模式，如稀疏-密集预训练 [[23](#bib.bib23)]、双向掩码
    [[55](#bib.bib55)] 和其他类似方法 [[24](#bib.bib24), [49](#bib.bib49), [32](#bib.bib32),
    [56](#bib.bib56)]，这一设置开销可能会很大，从而导致速度提升减少（如第 [3.1](#S3.SS1 "3.1 端到端速度提升和内存节省：预训练和推理
    ‣ 3 实验结果 ‣ SLoPe: 双重修剪稀疏加上懒惰低秩适配器 LLM 预训练") 节中观察到的稀疏-密集预训练情况）或在某些配置中减速（如附录 [H](#A8
    "附录 H 双向掩码的性能开销 ‣ 附录 ‣ SLoPe: 双重修剪稀疏加上懒惰低秩适配器 LLM 预训练") 中讨论）。⁵⁵ 最近的工作观察到 cuSPARSELt
    SpMM 管道使用动态稀疏性时的类似开销 [[5](#bib.bib5)]。'
- en: '![Refer to caption](img/d76492224c4376c10fe7a5fa79d81144.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d76492224c4376c10fe7a5fa79d81144.png)'
- en: 'Figure 5: The setup and multiplication time for square matrices using the cuSPARSELt
    SpMM backend.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：使用 cuSPARSELt SpMM 后端的方阵的设置和乘法时间。
- en: 'Appendix C Low-Rank Adapter Performance: Scaling and Arithmetic Intensity'
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 C 低秩适配器性能：扩展性和算术强度
- en: 'As discussed in Section [2.4](#S2.SS4 "2.4 SLoPe runtime optimization ‣ 2 Sparse
    plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank
    Adapter Pretraining of LLMs"), the computation time of low-rank adapters does
    *not* scale linearly with their rank. This section provides experimental results
    to illustrate this behavior in more detail. The computational complexity of low-rank
    matrix multiplications is $\mathcal{O}(brd)$-fold reduction in computation time.
    However, in practice, this linearity does not hold. This deviation arises because
    the assumption underlying this expectation – that matrix multiplication is compute-bound
    – is not always true. Specifically, the arithmetic intensity of the operation
    can fall below the machine’s balance point, as described in the Roofline model [[54](#bib.bib54)].
    Figure [6](#A3.F6 "Figure 6 ‣ Appendix C Low-Rank Adapter Performance: Scaling
    and Arithmetic Intensity ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank
    Adapter Pretraining of LLMs") shows the speedup achieved for different low-rank
    values using PyTorch’s matrix multiplication function, which relies on the CUBLAS
    backend [[36](#bib.bib36)]. The figure demonstrates that the achieved speedups
    are significantly lower than the ideal linear scaling, particularly when reducing
    the rank. Moreover, it is evident that as the matrix dimensions increase, the
    gap between the ideal speedup and the observed speedup diminishes. This behavior
    can be attributed to the increased arithmetic intensity for larger matrices, leading
    to better utilization of tensor cores.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[2.4节](#S2.SS4 "2.4 SLoPe运行时优化 ‣ 2 稀疏加低秩预训练LLMs ‣ SLoPe: 双修剪稀疏加懒惰低秩适配器预训练LLMs")所讨论，低秩适配器的计算时间*不*会随着其秩线性缩放。本节提供了实验结果，以更详细地说明这一行为。低秩矩阵乘法的计算复杂度是$\mathcal{O}(brd)$倍的计算时间缩减。然而，在实践中，这种线性关系并不成立。这种偏差的出现是因为支撑这一期望的假设——矩阵乘法是计算密集型的——并不总是成立。具体来说，操作的算术强度可能会低于机器的平衡点，如Roofline模型[[54](#bib.bib54)]所描述。图[6](#A3.F6
    "图 6 ‣ 附录 C 低秩适配器性能：缩放与算术强度 ‣ 附录 ‣ SLoPe: 双修剪稀疏加懒惰低秩适配器预训练LLMs")展示了使用PyTorch的矩阵乘法函数（依赖于CUBLAS后端[[36](#bib.bib36)]）为不同低秩值所实现的加速。图中显示了所实现的加速明显低于理想的线性缩放，特别是在减少秩时。此外，随着矩阵维度的增加，理想加速与观察到的加速之间的差距逐渐缩小。这种行为可以归因于较大矩阵的算术强度增加，从而更好地利用张量核心。'
- en: '![Refer to caption](img/6607d017cbe83227d2219751661d8060.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/6607d017cbe83227d2219751661d8060.png)'
- en: 'Figure 6: The speedup achieved by low-rank adapters in comparison to a dense
    matrix-multiplication.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：低秩适配器与密集矩阵乘法比较下实现的加速。
- en: Appendix D Efficient low-rank adapter implementation
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 D 高效低秩适配器实现
- en: 'As discussed in Section [2.4](#S2.SS4 "2.4 SLoPe runtime optimization ‣ 2 Sparse
    plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank
    Adapter Pretraining of LLMs"), a naïve implementation of low-rank adapters can
    lead to significant performance overheads due to the increased number of kernel
    launches and the low arithmetic intensity of their multiplications. To address
    these issues, we introduced two key optimizations: (1) concatenating one of the
    low-rank adapters with the sparse weights, and (2) fusing the multiplication of
    the other low-rank adapter with the subsequent result addition. These optimizations
    reduce kernel calls and increase arithmetic intensity, leading to more efficient
    utilization of GPU resources. Table [5](#A4.T5 "Table 5 ‣ Appendix D Efficient
    low-rank adapter implementation ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus
    Lazy Low-Rank Adapter Pretraining of LLMs") summarizes the speedup improvements
    achieved with these optimizations, demonstrating an inference speedup increase
    of up to 6%.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[2.4节](#S2.SS4 "2.4 SLoPe运行时优化 ‣ 2 稀疏加低秩预训练LLMs ‣ SLoPe: 双修剪稀疏加懒惰低秩适配器预训练LLMs")所讨论，低秩适配器的简单实现可能会由于内核启动次数增加和乘法运算的低算术强度而导致显著的性能开销。为解决这些问题，我们引入了两个关键优化：（1）将一个低秩适配器与稀疏权重连接，和（2）将另一个低秩适配器的乘法与随后的结果加法融合。这些优化减少了内核调用并提高了算术强度，从而更高效地利用了GPU资源。表[5](#A4.T5
    "表 5 ‣ 附录 D 高效低秩适配器实现 ‣ 附录 ‣ SLoPe: 双修剪稀疏加懒惰低秩适配器预训练LLMs")总结了这些优化所实现的加速改进，展示了推理加速提升高达6%。'
- en: 'Table 5: End-to-end speedup ($\times$) before (left) and after (right) efficient
    implementation of low-rank adapters.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：低秩适配器高效实现前（左）和实现后（右）的端到端加速（$\times$）。
- en: '| Model | Inference | Inference |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 推理 | 推理 |'
- en: '| --- | --- | --- |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | 1.56% Adapter | 6.25% Adapter |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | 1.56% 适配器 | 6.25% 适配器 |'
- en: '| OPT-66B | 1.15-1.20 | 1.12-1.19 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66B | 1.15-1.20 | 1.12-1.19 |'
- en: '| OPT-30B | 1.13-1.18 | 1.10-1.16 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30B | 1.13-1.18 | 1.10-1.16 |'
- en: '| OPT-13B | 1.11-1.10 | 1.09-1.10 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13B | 1.11-1.10 | 1.09-1.10 |'
- en: '| OPT-6.6B | 1.07-1.12 | 1.06-1.11 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.6B | 1.07-1.12 | 1.06-1.11 |'
- en: '| OPT-2.6B | 1.01-1.06 | 0.97-1.00 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.6B | 1.01-1.06 | 0.97-1.00 |'
- en: Appendix E Efficient weight tiling implementation
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 E 高效权重切片实现
- en: 'We observed that the dimensions and aspect ratios of matrices significantly
    influence system speedup (Section [2.4](#S2.SS4 "2.4 SLoPe runtime optimization
    ‣ 2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus
    Lazy Low-Rank Adapter Pretraining of LLMs")). To mitigate this, we implemented
    a matrix tiling strategy, dividing upsample matrices into multiple square matrices.
    This approach significantly improves performance, as shown in Table [6](#A5.T6
    "Table 6 ‣ Appendix E Efficient weight tiling implementation ‣ Appendix ‣ SLoPe:
    Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs"). Our results
    demonstrate that matrix tiling can enhance training speed by up to 4% and inference
    speed by up to 12%, highlighting its effectiveness in optimizing system performance.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '我们观察到，矩阵的维度和长宽比显著影响系统加速（第[2.4](#S2.SS4 "2.4 SLoPe 运行时优化 ‣ 2 稀疏加低秩预训练 LLMs ‣
    SLoPe: 双重剪枝稀疏加惰性低秩适配器预训练")节）。为缓解这一影响，我们实施了矩阵切片策略，将上采样矩阵划分为多个方形矩阵。这种方法显著提高了性能，如表[6](#A5.T6
    "表 6 ‣ 附录 E 高效权重切片实现 ‣ 附录 ‣ SLoPe: 双重剪枝稀疏加惰性低秩适配器预训练")所示。我们的结果表明，矩阵切片可以提高训练速度最多4%，推理速度最多12%，突显了其在优化系统性能方面的有效性。'
- en: 'Table 6: End-to-end speedup ($\times$) before (left) and after (right) splitting
    the upsample matrix. In both cases, the optimization discussed in [5](#A4.T5 "Table
    5 ‣ Appendix D Efficient low-rank adapter implementation ‣ Appendix ‣ SLoPe: Double-Pruned
    Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs") is used.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6：在拆分上采样矩阵之前（左）和之后（右）的端到端加速（$\times$）。在这两种情况下，都使用了[5](#A4.T5 "表 5 ‣ 附录 D
    高效低秩适配器实现 ‣ 附录 ‣ SLoPe: 双重剪枝稀疏加惰性低秩适配器预训练")中讨论的优化。'
- en: '| Model | Training | Inference | Inference | Inference |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 训练 | 推理 | 推理 | 推理 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  |  | No Adapter | 1.56% Adapter | 6.25% Adapter |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 无适配器 | 1.56% 适配器 | 6.25% 适配器 |'
- en: '| OPT-66B | 1.10-1.13 | 1.22-1.34 | 1.20-1.31 | 1.19-1.30 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66B | 1.10-1.13 | 1.22-1.34 | 1.20-1.31 | 1.19-1.30 |'
- en: '| OPT-30B | 1.09-1.14 | 1.23-1.32 | 1.18-1.28 | 1.16-1.27 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30B | 1.09-1.14 | 1.23-1.32 | 1.18-1.28 | 1.16-1.27 |'
- en: '| OPT-13B | 1.10-1.12 | 1.23-1.30 | 1.10-1.30 | 1.10-1.12 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13B | 1.10-1.12 | 1.23-1.30 | 1.10-1.30 | 1.10-1.12 |'
- en: '| OPT-6.6B | 1.08-1.08 | 1.21-1.19 | 1.12-1.13 | 1.11-1.12 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.6B | 1.08-1.08 | 1.21-1.19 | 1.12-1.13 | 1.11-1.12 |'
- en: '| OPT-2.6B | 1.03-1.02 | 1.02-1.07 | 1.06-1.05 | 1.00-1.00 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.6B | 1.03-1.02 | 1.02-1.07 | 1.06-1.05 | 1.00-1.00 |'
- en: Appendix F SLoPe sensitivity to pruning different module in transformer
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 F SLoPe 对 Transformer 中不同模块剪枝的敏感性
- en: 'LLMs typically consist of two main modules: the MLP mixer and the self-attention.
    The attention module’s weights are represented as a matrix in $\mathbb{R}^{d\times
    3d}$ pruning both MLP mixer and self-attention modules. Table [7](#A6.T7 "Table
    7 ‣ Appendix F SLoPe sensitivity to pruning different module in transformer ‣
    Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining
    of LLMs") presents the SQuAD and GLUE results for these settings. As expected,
    we observe a consistent, albeit slight, decrease in model quality as more modules
    are sparsified. The marginal decrease in performance suggests that models are
    relatively insensitive to the specific modules being pruned when using our SLoPe
    pretraining method. This observation underscores the robustness of our approach
    and its ability to maintain competitive quality across diverse sparsity configurations.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLMs 通常由两个主要模块组成：MLP 混合器和自注意力。注意力模块的权重表示为矩阵 $\mathbb{R}^{d\times 3d}$ 剪枝 MLP
    混合器和自注意力模块。表[7](#A6.T7 "表 7 ‣ 附录 F SLoPe 对 Transformer 中不同模块剪枝的敏感性 ‣ 附录 ‣ SLoPe:
    双重剪枝稀疏加惰性低秩适配器预训练")展示了这些设置下的 SQuAD 和 GLUE 结果。如预期的那样，我们观察到随着更多模块的稀疏化，模型质量有一致但轻微的下降。性能的边际下降表明，使用我们的
    SLoPe 预训练方法时，模型对具体剪枝的模块相对不敏感。这一观察突显了我们方法的鲁棒性及其在各种稀疏配置下维持竞争质量的能力。'
- en: 'Table 7: SQuADv1.1 results on BERT-Large-Uncased for different pruned modules.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：不同剪枝模块在 BERT-Large-Uncased 上的 SQuADv1.1 结果。
- en: '| Pruned Modules | SQuAD | GLUE |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝模块 | SQuAD | GLUE |'
- en: '| --- | --- | --- |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Dense | 90.44 | 80.22 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Dense | 90.44 | 80.22 |'
- en: '| MLP Mixer | 90.28 | 79.03 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| MLP Mixer | 90.28 | 79.03 |'
- en: '| MLP Mixer + Self-Attention | 89.35 | 77.72 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| MLP Mixer + 自注意力 | 89.35 | 77.72 |'
- en: 'Appendix G BERT-Large-Uncased: Pretraining and Downstream Evaluation'
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '附录 G BERT-Large-Uncased: 预训练与下游评估'
- en: 'BERT-Large-Uncased pretraining consists of two phases, as illustrated in Figure
    [7](#A7.F7 "Figure 7 ‣ Appendix G BERT-Large-Uncased: Pretraining and Downstream
    Evaluation ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter
    Pretraining of LLMs"). Phase 1 comprises 7,038 iterations with a global batch
    size of 65,536 and a sequence length of 128\. Phase 2 includes 1,563 iterations
    with a global batch size of 32,768 and a sequence length of 512.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 'BERT-Large-Uncased 预训练分为两个阶段，如图 [7](#A7.F7 "图 7 ‣ 附录 G BERT-Large-Uncased:
    预训练与下游评估 ‣ 附录 ‣ SLoPe: 双重修剪稀疏加懒惰低秩适配器预训练") 所示。第一阶段包括 7,038 次迭代，全球批量大小为 65,536，序列长度为
    128。第二阶段包括 1,563 次迭代，全球批量大小为 32,768，序列长度为 512。'
- en: 'Figure [7](#A7.F7 "Figure 7 ‣ Appendix G BERT-Large-Uncased: Pretraining and
    Downstream Evaluation ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank
    Adapter Pretraining of LLMs") shows the training loss for both phases under different
    sparsity settings. We observe that higher sparsity ratios generally lead to higher
    training loss in both phases. Interestingly, the loss/perplexity gap does not
    directly correlate with the observed accuracy drops in downstream tasks [[4](#bib.bib4),
    [29](#bib.bib29), [47](#bib.bib47)].'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [7](#A7.F7 "图 7 ‣ 附录 G BERT-Large-Uncased: 预训练与下游评估 ‣ 附录 ‣ SLoPe: 双重修剪稀疏加懒惰低秩适配器预训练")
    显示了不同稀疏设置下的两阶段训练损失。我们观察到，较高的稀疏比例通常会导致两阶段训练损失增加。有趣的是，损失/困惑度差距与下游任务的准确率下降并不直接相关
    [[4](#bib.bib4), [29](#bib.bib29), [47](#bib.bib47)]。'
- en: '![Refer to caption](img/20ca3c513e0a87dd1aac3efe05abc86a.png)![Refer to caption](img/61f45d66595456c0c8da91cf4be4647c.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/20ca3c513e0a87dd1aac3efe05abc86a.png)![参见说明文字](img/61f45d66595456c0c8da91cf4be4647c.png)'
- en: 'Figure 7: Training loss of BERT-Large-Uncased on WikiCorpus dataset for phase
    1 and 2.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: BERT-Large-Uncased 在 WikiCorpus 数据集上第 1 和第 2 阶段的训练损失。'
- en: 'We evaluated the pretrained BERT-Large-Uncased models on the SQuAD v1.1 [[45](#bib.bib45)]
    and GLUE [[52](#bib.bib52)] benchmarks. SQuAD v1.1, a comprehensive question-answering
    dataset based on Wikipedia, is widely used for LLM training. We report the F1
    score for SQuAD throughout the paper. GLUE, a diverse benchmark for natural language
    understanding tasks, provides a single aggregated score across various challenges,
    facilitating model comparisons. The paper presents the average metric score for
    GLUE, while task-specific metrics are detailed in Appendix [L](#A12 "Appendix
    L Task-specific GLUE results ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy
    Low-Rank Adapter Pretraining of LLMs").'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 SQuAD v1.1 [[45](#bib.bib45)] 和 GLUE [[52](#bib.bib52)] 基准上评估了预训练的 BERT-Large-Uncased
    模型。SQuAD v1.1 是一个基于 Wikipedia 的综合问答数据集，广泛用于 LLM 训练。我们在整篇论文中报告了 SQuAD 的 F1 分数。GLUE
    是一个多样化的自然语言理解任务基准，提供了跨各种挑战的单一聚合分数，方便模型比较。本文展示了 GLUE 的平均度量分数，而任务特定的度量详见附录 [L](#A12
    "附录 L 任务特定的 GLUE 结果 ‣ 附录 ‣ SLoPe: 双重修剪稀疏加懒惰低秩适配器预训练")。'
- en: Appendix H Performance overhead of bidirectional mask
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 H 双向掩码的性能开销
- en: 'Table [8](#A8.T8 "Table 8 ‣ Appendix H Performance overhead of bidirectional
    mask ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining
    of LLMs") presents the runtime results of Bidirectional Masks [[55](#bib.bib55)],
    a state-of-the-art N:M sparsity method. Our analysis demonstrates that the mask
    search and associated overheads of this approach result in significant slowdowns
    compared to dense baselines. For these experiments, we utilized the repository
    provided in [[55](#bib.bib55)] and employed the same models used in their evaluation.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [8](#A8.T8 "表 8 ‣ 附录 H 双向掩码的性能开销 ‣ 附录 ‣ SLoPe: 双重修剪稀疏加懒惰低秩适配器预训练") 展示了 Bidirectional
    Masks [[55](#bib.bib55)]，一种最先进的 N:M 稀疏方法的运行时间结果。我们的分析表明，这种方法的掩码搜索及相关开销导致相对于密集基线的显著减速。对于这些实验，我们使用了
    [[55](#bib.bib55)] 中提供的库，并采用了他们评估中使用的相同模型。'
- en: 'Table 8: End-to-end slow-down of Bi-directional Mask [[55](#bib.bib55)] in
    comparison to the dense baseline.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: Bi-directional Mask [[55](#bib.bib55)] 相对于密集基线的端到端减速。'
- en: '| Model | Dataset | Slow-down ($\times$) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 数据集 | 减速 ($\times$) |'
- en: '| --- | --- | --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| MobileNet v2 | CIFAR10 | 5.08 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet v2 | CIFAR10 | 5.08 |'
- en: '| ResNet-32 | CIFAR10 | 5.07 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-32 | CIFAR10 | 5.07 |'
- en: '| VGG19 | CIFAR10 | 8.41 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| VGG19 | CIFAR10 | 8.41 |'
- en: '| ReNet-18 | ImageNet | 3.66 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| ReNet-18 | ImageNet | 3.66 |'
- en: '| ResNet-50 | ImageNet | 3.01 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-50 | ImageNet | 3.01 |'
- en: Appendix I Sparsity ratio analysis of double-pruned backward pass
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 I 双重剪枝反向传递的稀疏比分析
- en: 'As described in Section [2.1](#S2.SS1 "2.1 Double-pruned backward pass ‣ 2
    Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy
    Low-Rank Adapter Pretraining of LLMs"), our proposed sparse pretraining approach
    involves pruning weights in both the forward and backward passes. During the backward
    pass, we apply both row-wise and column-wise pruning, which introduces additional
    zero values to the column-wise pruned weight matrices used in the forward pass.
    Theorem [2.1](#S2.Thmtheorem1 "Lemma 2.1\. ‣ 2.1 Double-pruned backward pass ‣
    2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus
    Lazy Low-Rank Adapter Pretraining of LLMs") demonstrates that the resulting sparsity
    ratio can be calculated using Equation [8](#S2.E8 "In Lemma 2.1\. ‣ 2.1 Double-pruned
    backward pass ‣ 2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned
    Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs"). Figure [8](#A9.F8 "Figure
    8 ‣ Appendix I Sparsity ratio analysis of double-pruned backward pass ‣ Appendix
    ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs")
    visualizes the imposed sparsity ratios for various N:M sparsity patterns. As expected,
    smaller N/M ratios lead to lower imposed sparsity ratios. Moreover, in most cases,
    the imposed sparsity ratio is significantly smaller than the original matrix’s
    density ratio.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[2.1](#S2.SS1 "2.1 双重剪枝反向传递 ‣ 2 稀疏加低秩预训练LLMs ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练LLMs")节所述，我们提出的稀疏预训练方法涉及在前向和反向传递中剪枝权重。在反向传递中，我们应用了行剪枝和列剪枝，这在前向传递中引入了额外的零值到列剪枝权重矩阵中。定理[2.1](#S2.Thmtheorem1
    "引理 2.1\. ‣ 2.1 双重剪枝反向传递 ‣ 2 稀疏加低秩预训练LLMs ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练LLMs")证明了结果稀疏比可以使用方程[8](#S2.E8
    "在引理 2.1\. ‣ 2.1 双重剪枝反向传递 ‣ 2 稀疏加低秩预训练LLMs ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练LLMs")计算。图[8](#A9.F8
    "图 8 ‣ 附录 I 双重剪枝反向传递的稀疏比分析 ‣ 附录 ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练LLMs")可视化了各种N:M稀疏模式下施加的稀疏比。正如预期，较小的N/M比导致较低的施加稀疏比。此外，在大多数情况下，施加的稀疏比显著小于原始矩阵的密度比。'
- en: '![Refer to caption](img/44725ca5be3504efadec8e47b31df619.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/44725ca5be3504efadec8e47b31df619.png)'
- en: 'Figure 8: The imposed sparsity ratio when pruning the weight matrices in the
    backward pass.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：在反向传递中剪枝权重矩阵时施加的稀疏比。
- en: Appendix J Sensitivity to the choice of pruning matrix
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 J 剪枝矩阵选择的敏感性
- en: 'In linear layers, three matrices are involved in the forward and backward passes:
    the input, the output gradient, and the weights. Pruning each of these matrices
    can have distinct effects on model performance.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性层中，前向和反向传递涉及三个矩阵：输入、输出梯度和权重。剪枝这些矩阵中的每一个可能对模型性能产生不同的影响。
- en: To identify the optimal pruning strategy, we conducted an experiment where we
    pretrained GPT2-Small for 100,000 iterations (a quarter of the full pretraining)
    while systematically applying both static and dynamic pruning to each of the three
    matrices. Static pruning involves generating a random mask at initialization and
    applying it throughout training. Dynamic pruning, on the other hand, prunes matrices
    based on their magnitude at each iteration. For dynamic pruning, the dense matrix
    values are computed and stored, and then pruned at every step.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定最佳的剪枝策略，我们进行了一个实验，在该实验中我们预训练了GPT2-Small进行100,000次迭代（即完整预训练的四分之一），同时系统地将静态和动态剪枝应用于三个矩阵中的每一个。静态剪枝涉及在初始化时生成随机掩码，并在整个训练过程中应用。另一方面，动态剪枝则基于每次迭代的幅度剪枝矩阵。对于动态剪枝，稠密矩阵值被计算和存储，然后在每一步进行剪枝。
- en: 'Figure [9](#A10.F9 "Figure 9 ‣ Appendix J Sensitivity to the choice of pruning
    matrix ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining
    of LLMs") presents the validation perplexity for these experiments. Notably, pruning
    the output gradient led to model divergence after a few iterations and is not
    shown in the figure.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '图[9](#A10.F9 "图 9 ‣ 附录 J 剪枝矩阵选择的敏感性 ‣ 附录 ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器预训练LLMs")展示了这些实验的验证困惑度。值得注意的是，剪枝输出梯度导致模型在几次迭代后发散，因此图中未显示。'
- en: '![Refer to caption](img/7410848d4118b6579abccf6088f2c5c8.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7410848d4118b6579abccf6088f2c5c8.png)'
- en: 'Figure 9: Validation perplexity on GPT2-Small pretraining for 100,000 iterations
    for different matrix pruning settings. Pruning the output gradients leads to divergence
    within the few iterations and hence is not reported.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：在 GPT2-Small 上进行 100,000 次迭代的验证困惑度，针对不同矩阵剪枝设置。剪枝输出梯度会导致在少数几次迭代内发散，因此未报告。
- en: 'Analysis. As shown in Figure [9](#A10.F9 "Figure 9 ‣ Appendix J Sensitivity
    to the choice of pruning matrix ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus
    Lazy Low-Rank Adapter Pretraining of LLMs"), static pruning consistently achieved
    lower perplexities. This behavior suggests that focusing computational resources
    on elements that remain active throughout training can lead to improved performance.
    Furthermore, pruning weights resulted in lower perplexities compared to pruning
    inputs, indicating that weights are generally a better target for pruning.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '分析。如图[9](#A10.F9 "图 9 ‣ 附录 J 剪枝矩阵选择的灵敏度 ‣ 附录 ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器 LLM 预训练")所示，静态剪枝始终实现了较低的困惑度。这种行为表明，将计算资源集中在训练过程中始终保持活动的元素上可以提高性能。此外，与剪枝输入相比，剪枝权重导致了较低的困惑度，表明权重通常是剪枝的更好目标。'
- en: Intuition. Pruning weights is analogous to removing connections between neurons.
    Pruning activation tensors is similar to introducing a non-linear function (akin
    to max-pooling) before each linear layer. Pruning output gradients, however, lacks
    practical justification and introduces errors into the backward pass, leading
    to model divergence.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉。剪枝权重类似于去除神经元之间的连接。剪枝激活张量类似于在每个线性层之前引入非线性函数（类似于最大池化）。然而，剪枝输出梯度缺乏实际的合理性，并且会引入反向传播中的错误，导致模型发散。
- en: Appendix K Implementation details
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 K 实现细节
- en: 'This section details the implementation of the custom functions and CUDA kernels
    used in Algorithm [1](#alg1 "Algorithm 1 ‣ 2.3 Sparse kernels ‣ 2 Sparse plus
    low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank
    Adapter Pretraining of LLMs") to facilitate efficient sparse training.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '本节详细描述了在算法[1](#alg1 "算法 1 ‣ 2.3 稀疏内核 ‣ 2 稀疏加低秩 LLM 预训练 ‣ SLoPe: 双重剪枝稀疏加懒惰低秩适配器
    LLM 预训练")中使用的自定义函数和 CUDA 内核的实现，以促进高效的稀疏训练。'
- en: Initialization, sparse matrix setup, and SpMM kernels. Before utilizing the
    cuSPARSELt APIs, a crucial initialization phase ensures proper configuration of
    essential variables for our computational task. Following initialization, we configure
    the sparse data formats tailored for sparse matrices. This involves initializing
    matrix descriptors, pruning the matrices, and compressing them into a more compact
    representation. cuSPARSELt employs an automated search to determine the optimal
    kernel for executing SpMM. While setting up these sparse data formats incurs a
    non-negligible computational cost, this overhead is mitigated by the repetitive
    nature of matrix multiplications during the training process.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化、稀疏矩阵设置和 SpMM 内核。在利用 cuSPARSELt APIs 之前，关键的初始化阶段确保了为我们的计算任务正确配置必要的变量。初始化后，我们配置了适用于稀疏矩阵的稀疏数据格式。这包括初始化矩阵描述符、剪枝矩阵，并将其压缩为更紧凑的表示形式。cuSPARSELt
    采用自动搜索来确定执行 SpMM 的最佳内核。尽管设置这些稀疏数据格式会产生不可忽视的计算成本，但由于训练过程中矩阵乘法的重复性质，这种开销得到了缓解。
- en: Prune and compress. The gradient of the loss function with respect to the weights
    requires pruning using the same mask as the weight matrix. Consequently, it contains
    50% extra zero values in the dense format. To address this redundancy, we developed
    an optimized CUDA kernel, integrated into PyTorch, that masks the gradients accordingly,
    eliminating the storage of unnecessary data and reducing memory usage. The output
    of this operation is a new matrix in $\mathbb{R}^{d_{out}\times\frac{d_{in}}{2}}$.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝和压缩。损失函数相对于权重的梯度需要使用与权重矩阵相同的掩码进行剪枝。因此，它在密集格式中包含 50% 的额外零值。为了解决这个冗余问题，我们开发了一个优化的
    CUDA 内核，集成到 PyTorch 中，能够相应地掩盖梯度，消除不必要的数据存储并减少内存使用。该操作的输出是一个新的矩阵，其维度为 $\mathbb{R}^{d_{out}\times\frac{d_{in}}{2}}$。
- en: Sparse matrix addition. The cuSPARSELt sparse data format does not natively
    support addition operations. However, for matrices $A$ are arbitrary user-defined
    constants. This functionality is particularly useful for adding sparse weights
    to gradients in optimizers that utilize weight decay.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矩阵加法。cuSPARSELt 稀疏数据格式本身不支持加法操作。然而，对于矩阵 $A$ 是任意用户定义的常量。这种功能对于将稀疏权重添加到使用权重衰减的优化器中的梯度特别有用。
- en: Update Sparse Matrix. After the optimizer updates the weight tensor values based
    on its rules, we need to update the sparse matrix format to reflect these changes.
    We implemented an optimized CUDA kernel that copies the weight tensors from the
    PyTorch format into the cuSPARSELt data type, enabling efficient storage and manipulation
    of sparse weights.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 更新稀疏矩阵。在优化器根据其规则更新权重张量值后，我们需要更新稀疏矩阵格式以反映这些变化。我们实现了一个优化的CUDA内核，将权重张量从PyTorch格式复制到cuSPARSELt数据类型，实现了稀疏权重的高效存储和操作。
- en: Appendix L Task-specific GLUE results
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录L 特定任务GLUE结果
- en: 'The GLUE benchmark [[52](#bib.bib52)] comprises eight distinct natural language
    understanding classification tasks. While Section [3](#S3 "3 Experimental results
    ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs")
    presented the average GLUE score as a measure of overall model performance, this
    section provides a more detailed analysis by presenting the complete task-specific
    results for each training setting in Table [9](#A12.T9 "Table 9 ‣ Appendix L Task-specific
    GLUE results ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter
    Pretraining of LLMs").'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE基准[[52](#bib.bib52)]包含八个不同的自然语言理解分类任务。虽然第[3](#S3 "3 实验结果 ‣ SLoPe：双重修剪稀疏加惰性低秩适配器预训练LLMs")节展示了平均GLUE得分作为整体模型性能的度量，但本节通过在表[9](#A12.T9
    "表 9 ‣ 附录L 特定任务GLUE结果 ‣ 附录 ‣ SLoPe：双重修剪稀疏加惰性低秩适配器预训练LLMs")中展示每个训练设置的完整任务特定结果，提供了更详细的分析。
- en: '|  |  |  | First | Last |  |  |  |  |  |  |  |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 第一 | 最后 |  |  |  |  |  |  |  |  |'
- en: '| Method | Phase | Rank | 12 | 12 | CoLA | SST-2 | MRPC | STS-B | QQP | RTE
    | MNLI | QNLI |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 阶段 | 排名 | 12 | 12 | CoLA | SST-2 | MRPC | STS-B | QQP | RTE | MNLI |
    QNLI |'
- en: '|  |  |  | Blocks | Blocks | (mcc) | (acc) | (f1) | (corr) | (f1) | (acc) |
    (acc) | (acc) |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 块 | 块 | (mcc) | (acc) | (f1) | (corr) | (f1) | (acc) | (acc) | (acc)
    |'
- en: '| Dense | 1,2 | 0 | 2:4 | 2:4 | 51.6 | 91.9 | 81.2 | 87.5 | 87.8 | 66.4 | 84.1
    | 91.3 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Dense | 1,2 | 0 | 2:4 | 2:4 | 51.6 | 91.9 | 81.2 | 87.5 | 87.8 | 66.4 | 84.1
    | 91.3 |'
- en: '| SLoPe |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| SLoPe |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| MLP Mixer | 2 | 0 | 2:4 | 2:4 | 41.8 | 91.4 | 88.7 | 87.2 | 85.9 | 65 | 82.1
    | 90.1 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| MLP Mixer | 2 | 0 | 2:4 | 2:4 | 41.8 | 91.4 | 88.7 | 87.2 | 85.9 | 65 | 82.1
    | 90.1 |'
- en: '| Only |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 仅 |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| SLoPe |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| SLoPe |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| MLP Mixer + | 2 | 0 | 2:4 | 2:4 | 38.8 | 90.4 | 85.9 | 86.4 | 85.9 | 63.5
    | 81.5 | 89.3 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| MLP Mixer + | 2 | 0 | 2:4 | 2:4 | 38.8 | 90.4 | 85.9 | 86.4 | 85.9 | 63.5
    | 81.5 | 89.3 |'
- en: '| Self-Attention |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 自注意力 |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| SLoPe with |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| SLoPe与 |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| Non-Lazy | 2 | 40 | 2:4 | 2:4 | 43.3 | 90.8 | 89 | 87 | 86 | 64.6 | 82.3
    | 89.6 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 非惰性 | 2 | 40 | 2:4 | 2:4 | 43.3 | 90.8 | 89 | 87 | 86 | 64.6 | 82.3 | 89.6
    |'
- en: '| Adapters |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 适配器 |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| SLoPe with |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| SLoPe与 |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| Non-Lazy | 2 | 40 | 2:8 | 2:4 | 29 | 89.7 | 83.7 | 85.6 | 85.2 | 66.8 | 79.9
    | 87.4 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 非惰性 | 2 | 40 | 2:8 | 2:4 | 29 | 89.7 | 83.7 | 85.6 | 85.2 | 66.8 | 79.9 |
    87.4 |'
- en: '| Adapters |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 适配器 |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| SLoPe with |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| SLoPe与 |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| Non-Lazy | 2 | 40 | 2:4 | 2:8 | 44.1 | 91.1 | 89.8 | 86.6 | 86.3 | 62.5 |
    82.3 | 89.6 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 非惰性 | 2 | 40 | 2:4 | 2:8 | 44.1 | 91.1 | 89.8 | 86.6 | 86.3 | 62.5 | 82.3
    | 89.6 |'
- en: '| Adapters |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 适配器 |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| SLoPe | 1,2 | 0 | 2:4 | 2:4 | 37.9 | 91.4 | 85.4 | 86.6 | 85.8 | 62.5 | 80.7
    | 88.6 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| SLoPe | 1,2 | 0 | 2:4 | 2:4 | 37.9 | 91.4 | 85.4 | 86.6 | 85.8 | 62.5 | 80.7
    | 88.6 |'
- en: '| SLoPe | 1,2 | 4 | 2:4 | 2:4 | 38.5 | 91.4 | 85.8 | 86.8 | 85.8 | 63.9 | 80.8
    | 88.4 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| SLoPe | 1,2 | 4 | 2:4 | 2:4 | 38.5 | 91.4 | 85.8 | 86.8 | 85.8 | 63.9 | 80.8
    | 88.4 |'
- en: '| SLoPe | 1,2 | 16 | 2:4 | 2:4 | 39.2 | 91.3 | 86.4 | 86.6 | 86 | 63.5 | 80.8
    | 88.2 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| SLoPe | 1,2 | 16 | 2:4 | 2:4 | 39.2 | 91.3 | 86.4 | 86.6 | 86 | 63.5 | 80.8
    | 88.2 |'
- en: '| SLoPe | 1,2 | 64 | 2:4 | 2:4 | 42.7 | 90.3 | 85.1 | 86.8 | 85.7 | 66.4 |
    80.3 | 88.5 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| SLoPe | 1,2 | 64 | 2:4 | 2:4 | 42.7 | 90.3 | 85.1 | 86.8 | 85.7 | 66.4 |
    80.3 | 88.5 |'
- en: '| WANDA | N/A | 0 | 2:4 | 2:4 | 43.0 | 91.4 | 88.3 | 86.9 | 86.1 | 63.5 | 81.9
    | 89.6 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| WANDA | N/A | 0 | 2:4 | 2:4 | 43.0 | 91.4 | 88.3 | 86.9 | 86.1 | 63.5 | 81.9
    | 89.6 |'
- en: '| WANDA | N/A | 0 | 2:8 | 2:4 | 4.6 | 0.88 | 81.3 | 81 | 83.3 | 53.8 | 76.7
    | 83.9 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| WANDA | N/A | 0 | 2:8 | 2:4 | 4.6 | 0.88 | 81.3 | 81 | 83.3 | 53.8 | 76.7
    | 83.9 |'
- en: '| WANDA | N/A | 0 | 2:4 | 2:8 | 42.1 | 91.7 | 84.4 | 87.2 | 85.6 | 63.5 | 81.5
    | 81.9 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| WANDA | N/A | 0 | 2:4 | 2:8 | 42.1 | 91.7 | 84.4 | 87.2 | 85.6 | 63.5 | 81.5
    | 81.9 |'
- en: 'Table 9: GLUE results for each task in the experiments discussed in section
    [3](#S3 "3 Experimental results ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank
    Adapter Pretraining of LLMs").'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 在第[3](#S3 "3 Experimental results ‣ SLoPe: Double-Pruned Sparse Plus Lazy
    Low-Rank Adapter Pretraining of LLMs")节讨论的实验中，每个任务的GLUE结果。'
- en: Appendix M Additional related work
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 M 其他相关工作
- en: Model pruning. Pruning the models has been one of the most effective methods
    to reduce the complexity of LLMs [[21](#bib.bib21)]. One can pretrain the LLMs
    sparsely [[13](#bib.bib13)] or the pruning can happen after a dense pretraining
    [[20](#bib.bib20), [27](#bib.bib27)], possibly followed by a fine-tuning stage
    to recover part of the lost accuracy [[15](#bib.bib15), [18](#bib.bib18)]. Pruning
    the models after pretraining can be costly [[46](#bib.bib46), [19](#bib.bib19)]
    and typically fails to maintain their accuracy [[14](#bib.bib14), [48](#bib.bib48)].
    While the sparse pretraining methods improve the accuracy of the model, they either
    use unstructured sparsity patterns that cannot be accelerated with the current
    hardware [[51](#bib.bib51)] or have significant overheads when searching for and
    applying their structured sparse masks [[24](#bib.bib24), [55](#bib.bib55), [49](#bib.bib49)].
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 模型修剪。修剪模型一直是减少LLM复杂性最有效的方法之一[[21](#bib.bib21)]。可以对LLM进行稀疏预训练[[13](#bib.bib13)]，或者修剪可以在密集预训练后进行[[20](#bib.bib20),
    [27](#bib.bib27)]，可能随后进行微调阶段以恢复部分丧失的准确性[[15](#bib.bib15), [18](#bib.bib18)]。在预训练后修剪模型可能代价高昂[[46](#bib.bib46),
    [19](#bib.bib19)]，并且通常无法维持其准确性[[14](#bib.bib14), [48](#bib.bib48)]。虽然稀疏预训练方法提高了模型的准确性，但它们要么使用当前硬件无法加速的无结构稀疏模式[[51](#bib.bib51)]，要么在搜索和应用其结构稀疏掩码时有显著的开销[[24](#bib.bib24),
    [55](#bib.bib55), [49](#bib.bib49)]。
- en: Low-rank adapters. Low-rank adapters have emerged as a promising method to reduce
    the fine-tuning costs associated with pre-trained LLMs and enable more efficient
    task switching [[22](#bib.bib22)]. Different quantization and initialization schemes
    have been proposed to reduce their overheads in LLM fine-tuning [[10](#bib.bib10),
    [17](#bib.bib17)]. Adding low-rank factors to sparse matrices is a low-weight
    mechanism widely used to improve the accuracy of approximations of dense matrices
    [[2](#bib.bib2)]. In machine learning, the sparse plus low-rank approximations
    are limited to attention heads [[34](#bib.bib34), [3](#bib.bib3)] and pruning
    after pretraining [[35](#bib.bib35), [28](#bib.bib28)], and the sparse plus low-rank
    pretraining has not been investigated.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适配器。低秩适配器作为一种有前景的方法，能够降低与预训练LLM相关的微调成本，并实现更高效的任务切换[[22](#bib.bib22)]。已提出不同的量化和初始化方案，以降低LLM微调中的开销[[10](#bib.bib10),
    [17](#bib.bib17)]。将低秩因子添加到稀疏矩阵是一种低开销机制，广泛用于提高对密集矩阵近似的准确性[[2](#bib.bib2)]。在机器学习中，稀疏加低秩近似仅限于注意力头[[34](#bib.bib34),
    [3](#bib.bib3)]和预训练后的修剪[[35](#bib.bib35), [28](#bib.bib28)]，稀疏加低秩的预训练尚未被研究。
- en: Appendix N Experiment setup, hyperparameters, compute resources
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 N 实验设置、超参数、计算资源
- en: Our experiments were conducted on the Narval and Mist clusters at Compute Canada [[6](#bib.bib6)]
    and the Lonestar 6 cluster at the Texas Advanced Computing Center [[50](#bib.bib50)].
    Each Narval node is equipped with four Nvidia A100 GPUs, each with 40GB of memory.
    Mist nodes feature four Nvidia V100 GPUs, each with 32GB of memory, while Lonestar
    6 nodes have three Nvidia A100 GPUs, each with 40GB of memory. For our accuracy
    experiments, we emulated 2:4 and N:M sparsity using custom-designed, low-overhead
    CUDA kernels to prune weights in both the forward and backward passes. We utilized
    a mixture of available resources across the clusters, as model accuracy is not
    hardware-dependent.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验在Compute Canada的Narval和Mist集群[[6](#bib.bib6)]以及德克萨斯州高级计算中心的Lonestar 6集群[[50](#bib.bib50)]上进行。每个Narval节点配备了四块40GB内存的Nvidia
    A100 GPU。Mist节点配备了四块32GB内存的Nvidia V100 GPU，而Lonestar 6节点配备了三块40GB内存的Nvidia A100
    GPU。在我们的精度实验中，我们使用自定义设计的低开销CUDA内核模拟2:4和N:M稀疏性，以在前向和反向传递中修剪权重。我们利用了各集群中的可用资源，因为模型精度与硬件无关。
- en: Our speedup and memory saving experiments were conducted on a single A100 GPU
    in the Narval cluster. We ran 1000 iterations of training or inference to gather
    the necessary statistics. For speedup experiments, we reported the median of the
    1000 samples to mitigate the effects of outliers. Each memory reduction experiment
    was run five times, and the median value was reported. We employed the default
    hyperparameters found in the NVIDIA BERT codebase [[40](#bib.bib40)] and the FlashAttention
    GPT codebase [[9](#bib.bib9), [7](#bib.bib7)]. Further tuning of hyperparameters
    for sparse pretraining is left as a future direction. Training BERT-Large-Uncased
    required approximately 32 hours on 64 A100-64GB GPUs. The pretraining of GPT2-Small/Large
    took 32 and 111 hours, respectively, on 64 V100-32GB GPUs.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的加速和节省内存的实验是在 Narval 集群的单个 A100 GPU 上进行的。我们运行了 1000 次训练或推理以收集必要的统计数据。对于加速实验，我们报告了
    1000 个样本的中位数，以减轻异常值的影响。每个内存减少实验运行了五次，并报告了中位数值。我们使用了 NVIDIA BERT 代码库[[40](#bib.bib40)]和
    FlashAttention GPT 代码库[[9](#bib.bib9), [7](#bib.bib7)]中找到的默认超参数。对稀疏预训练的进一步超参数调整留待未来方向。训练
    BERT-Large-Uncased 需要大约 32 小时，使用 64 个 A100-64GB GPU。GPT2-Small/Large 的预训练分别需要
    32 和 111 小时，使用 64 个 V100-32GB GPU。
- en: Appendix O Proofs
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 O 证明
- en: 'O.1 Lemma [2.1](#S2.Thmtheorem1 "Lemma 2.1\. ‣ 2.1 Double-pruned backward pass
    ‣ 2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse Plus
    Lazy Low-Rank Adapter Pretraining of LLMs")'
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'O.1 引理 [2.1](#S2.Thmtheorem1 "引理 2.1\. ‣ 2.1 双重修剪向后传递 ‣ 2 稀疏加低秩预训练 LLMs ‣ SLoPe:
    双重修剪稀疏加懒惰低秩适配器 LLMs 的预训练")'
- en: Proof. Considering a matrix with $N:M$ row-wise consecutive elements.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 证明。考虑一个具有 $N:M$ 行顺序元素的矩阵。
- en: '|  | $E[X]=\sum_{i=1}^{M-N}Pr[X=i]i$ |  | (13) |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  | $E[X]=\sum_{i=1}^{M-N}Pr[X=i]i$ |  | (13) |'
- en: Replacing $Pr[X=i]=Pr[Y=N+i]$.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 替换 $Pr[X=i]=Pr[Y=N+i]$。
- en: '|  | $1$2 |  | (14) |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (14) |'
- en: Considering the definition of $Y$.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑 $Y$ 的定义。
- en: '|  | $Pr[Y=j]={\binom{M}{j}}s^{j}(1-s)^{M-j};s\triangleq\frac{N}{M}$ |  | (15)
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|  | $Pr[Y=j]={\binom{M}{j}}s^{j}(1-s)^{M-j};s\triangleq\frac{N}{M}$ |  | (15)
    |'
- en: 'By replacing Equation [15](#A15.E15 "In O.1 Lemma 2.1 ‣ Appendix O Proofs ‣
    Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining
    of LLMs") in Equation [14](#A15.E14 "In O.1 Lemma 2.1 ‣ Appendix O Proofs ‣ Appendix
    ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs"),
    we will get Equation [16](#A15.E16 "In O.1 Lemma 2.1 ‣ Appendix O Proofs ‣ Appendix
    ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs").'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '通过将方程[15](#A15.E15 "在 O.1 引理 2.1 ‣ 附录 O 证明 ‣ 附录 ‣ SLoPe: 双重修剪稀疏加懒惰低秩适配器 LLMs
    的预训练")替换到方程[14](#A15.E14 "在 O.1 引理 2.1 ‣ 附录 O 证明 ‣ 附录 ‣ SLoPe: 双重修剪稀疏加懒惰低秩适配器
    LLMs 的预训练")中，我们将得到方程[16](#A15.E16 "在 O.1 引理 2.1 ‣ 附录 O 证明 ‣ 附录 ‣ SLoPe: 双重修剪稀疏加懒惰低秩适配器
    LLMs 的预训练")。'
- en: '|  | $E[X]=\sum_{j=N+1}^{M}{\binom{M}{j}]}s^{j}(1-s)^{M-j}(j-N)$ |  | (16)
    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|  | $E[X]=\sum_{j=N+1}^{M}{\binom{M}{j}]}s^{j}(1-s)^{M-j}(j-N)$ |  | (16)
    |'
- en: Let’s define random variable $Z$, and hence Equation
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义随机变量 $Z$，因此方程
- en: '|  | $1$2 |  | (17) |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (17) |'
- en: 'O.2 Theorem [2.2](#S2.Thmtheorem2 "Theorem 2.2\. ‣ 2.1 Double-pruned backward
    pass ‣ 2 Sparse plus low-rank pretraining of LLMs ‣ SLoPe: Double-Pruned Sparse
    Plus Lazy Low-Rank Adapter Pretraining of LLMs")'
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'O.2 定理 [2.2](#S2.Thmtheorem2 "定理 2.2\. ‣ 2.1 双重修剪向后传递 ‣ 2 稀疏加低秩预训练 LLMs ‣ SLoPe:
    双重修剪稀疏加懒惰低秩适配器 LLMs 的预训练")'
- en: 'Proof. In an optimization problem, we are aiming to find the optimal solution
    to Equation [18](#A15.E18 "In O.2 Theorem 2.2 ‣ Appendix O Proofs ‣ Appendix ‣
    SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs").'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '证明。在优化问题中，我们的目标是找到方程[18](#A15.E18 "在 O.2 定理 2.2 ‣ 附录 O 证明 ‣ 附录 ‣ SLoPe: 双重修剪稀疏加懒惰低秩适配器
    LLMs 的预训练")的最优解。'
- en: '|  | $\min_{W_{i}}E_{X}[\mathcal{L}(X,W_{i})]$ |  | (18) |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{W_{i}}E_{X}[\mathcal{L}(X,W_{i})]$ |  | (18) |'
- en: 'When using backpropagation, which is based on the chain rule in derivation,
    we compute the gradient in Equation [19](#A15.E19 "In O.2 Theorem 2.2 ‣ Appendix
    O Proofs ‣ Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining
    of LLMs").'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '使用反向传播时，基于链式法则进行推导，我们计算方程[19](#A15.E19 "在 O.2 定理 2.2 ‣ 附录 O 证明 ‣ 附录 ‣ SLoPe:
    双重修剪稀疏加懒惰低秩适配器 LLMs 的预训练")中的梯度。'
- en: '|  | $.E_{X}[\nabla_{X_{i}}\mathcal{L}(X,W_{i})]=E_{X}[\nabla_{Y_{i}}\mathcal{L}W]$
    |  | (19) |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  | $.E_{X}[\nabla_{X_{i}}\mathcal{L}(X,W_{i})]=E_{X}[\nabla_{Y_{i}}\mathcal{L}W]$
    |  | (19) |'
- en: Let’s define random variable $M$.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义随机变量 $M$。
- en: '|  | $1$2 |  | (20) |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (20) |'
- en: 'By using the linearity of derivation and expectation operators, we can get
    the result in Equation [21](#A15.E21 "In O.2 Theorem 2.2 ‣ Appendix O Proofs ‣
    Appendix ‣ SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining
    of LLMs"), which proves the theorem.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用导数和期望算子的线性特性，我们可以得到方程[21](#A15.E21 "在 O.2 定理 2.2 ‣ 附录 O 证明 ‣ 附录 ‣ SLoPe：双修剪稀疏加懒散低秩适配器预训练
    LLMs")中的结果，这证明了定理。
- en: '|  | $1$2 |  | (21) |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (21) |'
