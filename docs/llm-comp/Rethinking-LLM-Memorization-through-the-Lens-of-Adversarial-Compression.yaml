- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:52:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:52:58'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Rethinking LLM Memorization through the Lens of Adversarial Compression
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从对抗性压缩的角度重新思考 LLM 的记忆
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.15146](https://ar5iv.labs.arxiv.org/html/2404.15146)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '来源: [https://ar5iv.labs.arxiv.org/html/2404.15146](https://ar5iv.labs.arxiv.org/html/2404.15146)'
- en: Avi Schwarzschild
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Avi Schwarzschild
- en: 'schwarzschild@cmu.edu Carnegie Mellon University &Zhili Feng¹¹footnotemark:
    1'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'schwarzschild@cmu.edu 卡内基梅隆大学 & Zhili Feng¹¹脚注标记: 1'
- en: zhilif@andrew.cmu.edu Carnegie Mellon University &Pratyush Maini
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: zhilif@andrew.cmu.edu 卡内基梅隆大学 & Pratyush Maini
- en: pratyushmaini@cmu.edu Carnegie Mellon University &Zachary C. Lipton
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: pratyushmaini@cmu.edu 卡内基梅隆大学 & Zachary C. Lipton
- en: Carnegie Mellon University &J. Zico Kolter
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 卡内基梅隆大学 & J. Zico Kolter
- en: Carnegie Mellon University Equal contribution
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 卡内基梅隆大学 平等贡献
- en: Аннотация
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models (LLMs) trained on web-scale datasets raise substantial
    concerns regarding permissible data usage. One major question is whether these
    models ‘‘memorize’’ all their training data or they integrate many data sources
    in some way more akin to how a human would learn and synthesize information? The
    answer hinges, to a large degree, on *how we define memorization.* In this work,
    we propose the Adversarial Compression Ratio (ACR) as a metric for assessing memorization
    in LLMs—a given string from the training data is considered memorized if it can
    be elicited by a prompt shorter than the string itself. In other words, these
    strings can be ‘‘compressed’’ with the model by computing adversarial prompts
    of fewer tokens. We outline the limitations of existing notions of memorization
    and show how the ACR overcomes these challenges by (i) offering an adversarial
    view to measuring memorization, especially for monitoring unlearning and compliance;
    and (ii) allowing for the flexibility to measure memorization for arbitrary strings
    at a reasonably low compute. Our definition serves as a valuable and practical
    tool for determining when model owners may be violating terms around data usage,
    providing a potential legal tool and a critical lens through which to address
    such scenarios.^†^†Project page: [https://locuslab.github.io/acr-memorization](https://locuslab.github.io/acr-memorization)
    .'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '大型语言模型（LLMs）在网络规模的数据集上训练，引发了关于数据使用许可的重大担忧。一个主要问题是这些模型是否“记忆”了所有训练数据，还是以某种更类似于人类学习和综合信息的方式整合了许多数据来源？答案在很大程度上取决于*我们如何定义记忆*。在这项工作中，我们提出了对抗性压缩比率（ACR）作为评估
    LLM 记忆的度量指标——如果一个训练数据中的字符串可以通过比字符串本身更短的提示引发，则认为该字符串已被记忆。换句话说，这些字符串可以通过计算较少标记的对抗性提示在模型中“压缩”。我们概述了现有记忆概念的局限性，并展示了
    ACR 如何通过（i）提供对抗性视角来测量记忆，特别是用于监控遗忘和合规性；以及（ii）允许在合理低的计算成本下测量任意字符串的记忆来克服这些挑战。我们的定义作为确定模型所有者何时可能违反数据使用条款的有价值和实用的工具，提供了一个潜在的法律工具和一个解决此类场景的重要视角。^†^†项目页面:
    [https://locuslab.github.io/acr-memorization](https://locuslab.github.io/acr-memorization)。'
- en: '![Refer to caption](img/d7cda520eb795adb7452b3d88a15052d.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d7cda520eb795adb7452b3d88a15052d.png)'
- en: 'Рис. 1: We propose a compression ratio where we compare the length of the shortest
    prompt that elicits a training sample in response from an LLM. If a string in
    the training data can be *compressed*, i.e. the minimal prompt is shorter than
    the sample, then we call it *memorized*. Our test is an easy to describe tool
    that is useful in the effort to gauge the misuse of data.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 我们提出了一种压缩比度量方法，通过比较引发 LLM 响应的最短提示长度来进行。如果训练数据中的一个字符串可以被*压缩*，即最小提示比样本本身更短，那么我们称之为*记忆*。我们的测试是一个易于描述的工具，对于衡量数据的误用很有用。'
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'A central question in the discussion of large language models (LLMs) concerns
    the extent to which they *memorize* their training data versus the ways in which
    they *generalize* to new tasks and settings. Most practitioners seem to (at least
    informally) believe that LLMs do some degree of both: they clearly memorize some
    aspects of the training data—for example, are often able to reproduce large portions
    of training data verbatim (Carlini et al., [2023](#bib.bib6))—but they also seem
    to learn from this data allowing them to generalize to new settings. The precise
    extent to which they do one or the other has massive implications for the practical
    and legal aspects of such models (Cooper et al., [2023](#bib.bib8)). Do LLMs truly
    produce new content, or do they only remix their training data? Should the act
    of training on copyrighted data be deemed unfair use of data, or should fair use
    be judged by the model’s memorization? With regard to people, we distinguish plagiarising
    content from learning from it, but how should this extend to LLMs? The answer
    to such questions inherently relates to the extent to which LLMs memorize their
    training data.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 关于大型语言模型（LLMs）的讨论中，一个核心问题是它们在多大程度上*记住*了其训练数据，以及它们在多大程度上*概括*到新任务和新环境中。大多数从业者似乎（至少非正式地）认为LLMs在这两方面都有一定程度的表现：它们显然记住了训练数据的某些方面——例如，通常能够逐字再现大量训练数据 (Carlini
    et al., [2023](#bib.bib6))——但它们也似乎从这些数据中学习，使它们能够概括到新环境中。它们在这两者之间的确切程度对这些模型的实际和法律方面有着巨大的影响 (Cooper
    et al., [2023](#bib.bib8))。LLMs是否真的生成新的内容，还是只是重新混合它们的训练数据？对受版权保护数据的训练行为是否应该被视为不公平使用，还是应该通过模型的记忆来判断公平使用？对于人类，我们区分抄袭内容与从中学习，但这应如何扩展到LLMs？对这些问题的回答本质上与LLMs记忆其训练数据的程度相关。
- en: However, even defining memorization for LLMs is challenging and many existing
    definitions leave a lot to be desired. Certain formulations claim that a passage
    from the training data is memorized if the LLM can reproduce it exactly (Nasr
    et al., [2023](#bib.bib22)). However, this ignores situations where, for instance,
    a prompt instructs the model to exactly repeat some phrase. Other formulations
    define memorization by whether or not prompting an LLM with a portion of text
    from the training set results in the completion of that training datum (Carlini
    et al., [2023](#bib.bib6)). But these formalisms rely fundamentally on the completions
    being a certain size, and typically very lengthy generations are required for
    sufficient certainty of memorization. More crucially, these definitions are too
    permissive because they ignore situations where model developers can (for legal
    compliance) post-hoc ‘‘align’’ an LLM by instructing their models not to produce
    certain copyrighted content (Ippolito et al., [2023](#bib.bib15)). But has such
    an instructed model really *not memorized* the sample in question, or does the
    model still contain all the information about the datum in its weights and hides
    behind an illusion of compliance? Asking such questions becomes critical because
    this illusion of ‘‘unlearning’’ can often be easily broken as we show in [Sections 4.1](#S4.SS1
    "4.1 The Illusion of Compliance ‣ 4 Compressible Memorization in Practice ‣ Rethinking
    LLM Memorization through the Lens of Adversarial Compression") and [4.3](#S4.SS3
    "4.3 Trying to Forget Harry Potter ‣ 4 Compressible Memorization in Practice ‣
    Rethinking LLM Memorization through the Lens of Adversarial Compression").
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使为LLMs定义记忆也是具有挑战性的，许多现有的定义都存在很多不足之处。某些表述声称，如果LLM能够完全再现训练数据中的一段内容，则该段内容被认为是被记住的 (Nasr
    et al., [2023](#bib.bib22))。然而，这忽视了例如模型被指示完全重复某些短语的情况。其他表述通过是否用训练集中的一部分文本提示LLM会导致完成该训练数据来定义记忆 (Carlini
    et al., [2023](#bib.bib6))。但这些形式主义基本上依赖于完成的大小，通常需要非常长的生成文本以确保记忆的确定性。更重要的是，这些定义过于宽松，因为它们忽略了模型开发人员（为了法律合规）可以事后“调整”LLM的情况，即通过指示模型不生成某些受版权保护的内容 (Ippolito
    et al., [2023](#bib.bib15))。但这样的指示模型是否真的*没有记住*相关样本，或者模型是否仍然在其权重中包含所有关于数据的信息，并掩盖在合规的假象下？提出这些问题变得至关重要，因为这种“遗忘”的假象通常很容易被打破，正如我们在[第4.1节](#S4.SS1
    "4.1 合规的假象 ‣ 4 压缩记忆的实践 ‣ 从对抗压缩的角度重新思考LLM记忆")和[4.3节](#S4.SS3 "4.3 试图忘记哈利·波特 ‣ 4
    压缩记忆的实践 ‣ 从对抗压缩的角度重新思考LLM记忆")中展示的那样。
- en: In this work, we propose a new definition of memorization based on a compression
    argument. Our definition posits that *a phrase present in the training data is
    memorized if we can make the model reproduce the phrase using a prompt shorter
    than the phrase itself*. Operationalizing this definition requires finding the
    shortest adversarial input prompt that is specifically optimized to produce a
    target output. We call this ratio of input to output tokens the Adversarial Compression
    Ratio (ACR). In other words, memorization is inherently tied to whether a certain
    output can be represented in a *compressed* form, beyond what language models
    can do with typical text. We argue that such a definition provides an intuitive
    notion of memorization—if a certain phrase exists within the LLM training data
    (e.g., is not itself generated text) *and* it can be reproduced with fewer input
    tokens than output tokens, then the phrase must be stored somehow within the weights
    of the LLM. Although it may be more natural to consider compression in terms of
    the LLM-based notions of input/output perplexity, we argue that a simple compression
    ratio based on input/output token counts provides a more intuitive explanation
    to non-technical audiences, and has the potential to serve as a legal basis for
    important questions about memorization and permissible data use.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种基于压缩论点的新的记忆定义。我们的定义认为*如果我们可以使用比短语本身更短的提示让模型再现该短语，那么训练数据中存在的短语就是被记住的*。将此定义付诸实践需要找到专门优化以产生目标输出的最短对抗性输入提示。我们称这种输入与输出标记的比率为对抗性压缩比（ACR）。换句话说，记忆本质上与某个输出是否可以以*压缩*形式表示有关，超出了语言模型对典型文本的处理能力。我们认为，这种定义提供了记忆的直观概念——如果某个短语存在于LLM训练数据中（例如，并非自生成文本）*并且*它可以用比输出标记更少的输入标记再现，那么该短语一定在LLM的权重中以某种方式存储。尽管以LLM基础的输入/输出困惑度来考虑压缩可能更自然，但我们认为基于输入/输出标记计数的简单压缩比为非技术受众提供了更直观的解释，并且有可能作为记忆和数据使用许可的法律依据。
- en: In addition to its intuitive nature, our definition has several other desirable
    qualities. Experimentally, we show that it appropriately ascribes many famous
    quotes as being memorized by existing LLMs, that is, their ACR is greater than
    one. On the other hand, we find that text not in the training data of an LLM,
    such as samples posted on the internet after the training period, are not compressible,
    that is their ACR is less than one.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了其直观的特性，我们的定义还具有其他几个理想的特点。通过实验，我们表明，它恰当地将许多著名的引言归因于现有的LLM（即其ACR大于1）。另一方面，我们发现LLM训练数据中没有的文本，例如训练期后在互联网上发布的样本，是不可压缩的，即它们的ACR小于1。
- en: We examine several so-called unlearning methods through this lens to show that
    they do not substantially affect the memorization of the underlying model. That
    is, even after explicit instruction- or fine-tuning, models asked to ‘‘forget’’
    certain pieces of content are still able to reproduce them with a high ACR— in
    fact, not much smaller than with the original model. Our approach provides a simple
    and practical perspective on what memorization can mean, providing a useful tool
    for functional and legal analysis of LLMs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过这种视角审视了几种所谓的“遗忘”方法，以表明它们对基础模型的记忆没有实质性影响。也就是说，即使经过明确的指令或微调，被要求“忘记”某些内容的模型仍然能够以较高的ACR再现这些内容——实际上，几乎与原始模型没有太大区别。我们的方法提供了对记忆意义的简单而实用的视角，为LLM的功能和法律分析提供了有用的工具。
- en: 2 Do We Really Need Another Notion of Memorization?
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 我们真的需要另一种记忆概念吗？
- en: With LLMs ingesting more and more data, questions about their memorization are
    attracting attention (e.g. Carlini et al., [2019](#bib.bib4); [2023](#bib.bib6);
    Nasr et al., [2023](#bib.bib22); Zhang et al., [2023](#bib.bib30)). There remains
    a pressing need to accurately define memorization in a way that serves as a practical
    tool to ascertain the fair use of public data from a legal standpoint. To ground
    the problem, consider the court’s role in determining whether an LLM is breaching
    copyright. What constitutes a breach of copyright remains contentious and prior
    work defines this on a spectrum from ‘training on a data point itself constitutes
    violation’ to ‘copyright violation only occurs if you verbatim regurgitate training
    data’. To formalize our arguments and more carefully frame our points, we use
    three definitions as a starting point for our summary of the field thus far.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大规模语言模型（LLM）摄取越来越多的数据，关于它们记忆的相关问题受到关注（例如，Carlini et al., [2019](#bib.bib4)；[2023](#bib.bib6)；Nasr
    et al., [2023](#bib.bib22)；Zhang et al., [2023](#bib.bib30)）。迫切需要准确定义记忆，以作为从法律角度确认公共数据公平使用的实用工具。为了明确问题，请考虑法院在确定
    LLM 是否侵犯了版权方面的作用。什么构成版权侵权仍存在争议，先前的工作将其定义在‘对数据点进行训练本身构成侵权’到‘只有当你逐字重复训练数据时才构成版权侵权’的光谱上。为了形式化我们的论点并更仔细地构建我们的观点，我们以三种定义作为总结目前领域的起点。
- en: '![Refer to caption](img/df53f5dce0484b1ddf2b6e136834c62b.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/df53f5dce0484b1ddf2b6e136834c62b.png)'
- en: 'Рис. 2: Adversarial Compression is harder to hack, better attuned to what problematic
    data usage is, and requires no parameter choice. This makes it a better choice
    than alternative tests for fair use. If we deploy tests that look for exact auto-completion,
    we will flag very few cases as problematic. On the other hand, if we say any training
    done on copyrighted data is an issue we will have a very restrictive system. We
    argue for defining fair use with adversarial compression.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：对抗压缩更难被破解，更能适应什么是问题数据使用，并且不需要选择参数。这使得它比其他公平使用测试更具优势。如果我们部署查找精确自动补全的测试，我们将标记很少的情况为问题。另一方面，如果我们说任何对版权数据的训练都是一个问题，我们将拥有一个非常严格的系统。我们主张用对抗压缩定义公平使用。
- en: Definition 1 (Discoverable Memorization (Carlini et al., [2023](#bib.bib6)))
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1（可发现记忆（Carlini et al., [2023](#bib.bib6)））
- en: Given a generative model $M$.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个生成模型 $M$。
- en: Discoverable Memorization, which says a string is memorized if the first few
    words elicit the rest of the quote exactly, has three particular problems.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 可发现记忆，指的是如果前几个词能准确引出剩余的引文，则该字符串被记忆，这存在三个特定问题。
- en: '1.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Very Permissive: This requires a model to output an *exact match* of some number
    of tokens from the training set (under greedy decoding). This is extremely permissive
    and misses cases when the exact substrings are the second most likely choice.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非常宽松：这要求模型在训练集中输出一些标记的*精确匹配*（在贪婪解码下）。这非常宽松，并忽略了当精确子串是第二个最可能的选择时的情况。
- en: '2.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Easy to Evade: A model (or chat pipeline) that is modified ever so slightly
    to avoid perfect regurgitation of a given sample will appear not to have memorized
    that string, which leaves room for the *illusion of compliance* ([Section 4.1](#S4.SS1
    "4.1 The Illusion of Compliance ‣ 4 Compressible Memorization in Practice ‣ Rethinking
    LLM Memorization through the Lens of Adversarial Compression")).'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 易于规避：一个模型（或聊天流水线）稍微修改一下以避免对给定样本的完美重复，将看起来没有记忆那个字符串，这留下了*合规的假象*（[第 4.1 节](#S4.SS1
    "4.1 合规的假象 ‣ 4 实践中的可压缩记忆 ‣ 通过对抗压缩视角重新思考 LLM 记忆")）。
- en: '3.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Sensitive to Parameter Choice: We need to choose several words (or tokens)
    to include in the prompt and a number of tokens that have to match exactly in
    the output to turn this definition into a practical binary test for memorization.
    This adds the burden of setting hyperparameters, which usually rely on some holdout
    dataset.'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对参数选择敏感：我们需要选择几个词（或标记）以包含在提示中，并且输出中必须精确匹配一定数量的标记，以将此定义转变为实用的记忆二元测试。这增加了设置超参数的负担，而这些超参数通常依赖于某些留出数据集。
- en: Definition 2 (Extractable Memorization (Nasr et al., [2023](#bib.bib22)))
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2（可提取记忆（Nasr et al., [2023](#bib.bib22)））
- en: Given a generative model $M$.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个生成模型 $M$。
- en: The next definition says a string is Extractably Memorized if *there exists*
    a prompt that elicits the string in response. This falls too far on the other
    side of the issue by being very restrictive—what if the prompt includes the entire
    string in question, or worse the instructions to repeat it? LLMs that are good
    at repeating will follow that instruction and output any string they are asked
    to. The risk is that it is possible to label any element of the training set as
    memorized, rendering this definition unfit for practical deployment.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下一种定义表示，如果*存在*一个提示能引发该字符串的响应，则字符串被称为可提取记忆的。这一标准过于严格——如果提示包括了整个字符串，或者更糟的是包含了重复它的指令呢？擅长重复的LLM会遵循该指令并输出任何要求的字符串。风险在于可以将训练集的任何元素标记为记忆，从而使该定义不适用于实际应用。
- en: Definition 3 (Counterfactual Memorization (Zhang et al., [2023](#bib.bib30)))
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3（反事实记忆（Zhang et al., [2023](#bib.bib30)））
- en: 'Given a training algorithm $A$ is given by:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个训练算法$A$如下：
- en: '|  | $1$2 |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $D$.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$D$。
- en: The third notion of memorization aims to separate memorization from generalization
    and is called Counterfactual Memorization, which requires a test that includes
    retraining many models. Given, the cost of retaining large language models, such
    a definition remains impractical for legal use.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆的第三种概念旨在将记忆与泛化分开，称为反事实记忆，这需要一个包含重训练多个模型的测试。鉴于保留大型语言模型的成本，这种定义在法律上仍然不切实际。
- en: Membership is not memorization
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 会员资格不是记忆
- en: Membership Inference Attacks (MIA) are designed to determine whether a data
    point is in the training set of a given model, which is a common focus of privacy
    research (e.g. Shokri et al., [2017](#bib.bib26)). Upon first glance, MIAs may
    look like tests for memorization and they are even intimately related to auditing
    machine unlearning (Carlini et al., [2021](#bib.bib5); Pawelczyk et al., [2023](#bib.bib24)).
    However, there is a subtle and crucial difference between membership and memorization.
    In particular, the ongoing lawsuits in the field (e.g. as covered by [Metz & Robertson,](#bib.bib21)
    ) leave open the possibility that reproducing another’s creative work is problematic
    but training on samples from that data may not be. This is common practice in
    the arts—consider that a copycat comedian telling someone else’s jokes is stealing,
    but an up-and-comer learning from tapes of the greats is doing nothing wrong.
    In particular, this approach also has three issues.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 会员推断攻击（MIA）旨在确定数据点是否在给定模型的训练集中，这是隐私研究的一个常见焦点（例如 Shokri et al., [2017](#bib.bib26)）。乍一看，MIA可能看起来像是记忆测试，它们甚至与审计机器遗忘（Carlini
    et al., [2021](#bib.bib5)；Pawelczyk et al., [2023](#bib.bib24)）密切相关。然而，会员资格和记忆之间存在细微而重要的区别。特别是，该领域的
    ongoing 诉讼（例如，[Metz & Robertson,](#bib.bib21)的报道）留下了这样一种可能性，即重现他人的创意作品可能是有问题的，但从这些数据中训练可能不是。这在艺术领域是常见的——考虑一下，模仿者讲述别人的笑话是偷窃，但一个新秀从伟人的录音中学习并没有做错什么。特别是，这种方法还存在三个问题。
- en: '1.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Very Restrictive: LLMs are typically trained on trillions of tokens. Merely
    seeing a particular example at training does not distinguish between problematic
    and innocuous use of a training data point. Akin to plagiarism, it is okay to
    read copyrighted books, it is only problematic when content is copied. We need
    a similarly permissive definition of memorization for LLMs.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非常严格：LLM通常训练于数万亿个标记。仅仅在训练中看到一个特定的示例并不能区分训练数据点的有问题和无害使用。类似于剽窃，阅读版权书籍是可以的，只有在内容被复制时才会成为问题。我们需要一种对LLM更为宽容的记忆定义。
- en: '2.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Hard to Arbitrate: Arbitrating the validity of an MIA is itself problematic
    because it assumes good faith from the side of a corporation in releasing information
    about the data that they trained on in front of an arbiter. This becomes problematic
    given the inherently adversarial nature of such an event, and the potential of
    plausible deniability.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 难以裁决：裁定MIA的有效性本身就是一个问题，因为它假设公司在裁决者面前诚实地披露了关于其训练数据的信息。鉴于这种事件本质上的对抗性质以及可能的合理否认，这变得问题重重。
- en: '3.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Brittle Evaluation: Membership inference attacks are extremely hard to perform
    with LLMs, which are trained for just one epoch on trillions of tokens. Some recent
    work shows that LLM membership inference is extremely brittle (Duan et al., [2024](#bib.bib10)),
    which is supported by prior work theoretically examining the success of MIAs as
    dataset sizes increase (Maini et al., [2021](#bib.bib19)).'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 脆弱的评估：会员推断攻击在 LLM 中非常难以执行，这些模型仅在数万亿个令牌上训练了一次。最近的一些研究表明，LLM 会员推断非常脆弱（Duan 等，[2024](#bib.bib10)），这得到了之前的理论工作对
    MIAs 成功的理论检查的支持（Maini 等，[2021](#bib.bib19)）。
- en: Perplexity is complex and hackable
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 困惑度复杂且易被破解
- en: Another notion that appeals to the information theorist is the use of perplexity.
    We omit a formal definition here for brevity, but this encompasses approaches
    that use the model as a probability distribution over tokens to compute the information
    content of a string and estimate its optimal compression rate under the given
    model. Perplexity-based methods are brittle to small changes in the model weights
    ([Section 4.1](#S4.SS1 "4.1 The Illusion of Compliance ‣ 4 Compressible Memorization
    in Practice ‣ Rethinking LLM Memorization through the Lens of Adversarial Compression"))
    and lack the approachability of our definition. It is easier to picture a non-technical
    governing body and the broader community understanding and using a test based
    on input-output compression than one based on advanced information theory.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个信息理论学者感兴趣的概念是使用困惑度。为了简洁起见，我们在此省略正式定义，但这包括了使用模型作为令牌概率分布来计算字符串信息量并估计其在给定模型下的最佳压缩率的方法。基于困惑度的方法对模型权重的微小变化非常脆弱（[第4.1节](#S4.SS1
    "4.1 合规的幻觉 ‣ 4 实践中的可压缩记忆 ‣ 从对抗压缩的角度重新思考 LLM 记忆")），并且缺乏我们定义的易用性。更容易设想一个非技术性的管理机构和更广泛的社区理解和使用基于输入输出压缩的测试，而不是基于高级信息理论的测试。
- en: 3 How to Measure Memorization with Adversarial Compression
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 如何通过对抗压缩测量记忆
- en: 'Our definition of memorization is based on answering the following question:
    Given a piece of text, is the minimal prompt that elicits that text exactly shorter
    than the sample itself? In this section, we lay the definition formally and we
    introduce our MiniPrompt algorithm that we use to answer our central question.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的记忆定义基于回答以下问题：给定一段文本，能够引发该文本的最小提示是否确切地比样本本身短？在本节中，我们正式给出定义，并介绍我们用来回答核心问题的
    MiniPrompt 算法。
- en: 3.1 A New Definition of Memorization
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 新的记忆定义
- en: 'To begin, let a target natural text string $s$, which we call a completion
    or response:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，设定一个目标自然文本字符串 $s$，我们称之为完成或响应：
- en: '|  | $y=M(x).$ |  | (1) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $y=M(x).$ |  | (1) |'
- en: To describe [Equation 1](#S3.E1 "In 3.1 A New Definition of Memorization ‣ 3
    How to Measure Memorization with Adversarial Compression ‣ Rethinking LLM Memorization
    through the Lens of Adversarial Compression") in natural language we say that
    the model generates $y$ as follows.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 用自然语言描述 [方程 1](#S3.E1 "在 3.1 新的记忆定义 ‣ 3 如何通过对抗压缩测量记忆 ‣ 从对抗压缩的角度重新思考 LLM 记忆")
    时，我们说模型生成 $y$ 如下。
- en: '|  | $ACR=\frac{&#124;y&#124;}{&#124;x^{*}&#124;},\text{ where, }x^{*}=\operatorname*{arg\,min}_{x}&#124;x&#124;\text{
    s.t. }M(x)=y.$ |  | (2) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $ACR=\frac{&#124;y&#124;}{&#124;x^{*}&#124;},\text{ 其中 }x^{*}=\operatorname*{arg\,min}_{x}&#124;x&#124;\text{
    满足 }M(x)=y.$ |  | (2) |'
- en: Definition 4 (Compressible Memorization)
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4（可压缩记忆）
- en: Given a generative model $M$.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个生成模型 $M$。
- en: Our definition and the compression ratio lead to two natural ways to aggregate
    over a set of examples. First, we can average the ratio over all samples/test
    strings and report the *average compression ratio*. Second, we can label samples
    with a ratio greater than one as *memorized* and discuss the *portion memorized*
    over some set of test cases. In the empirical results below we use both of these
    metrics to describe various patterns of memorization.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的定义和压缩比导致了两种自然的汇总方式。首先，我们可以对所有样本/测试字符串的比率取平均，并报告 *平均压缩比*。其次，我们可以将比率大于一的样本标记为
    *记忆化*，并讨论在一些测试用例中的 *记忆部分*。在下面的实证结果中，我们使用这两种指标来描述各种记忆模式。
- en: One might wonder why we restrict ourselves to finding hard prompts—or tokens
    that can be decoded to natural language. Soft token, or embedding-based, optimization
    is faster and cheaper but renders the compression ratio much less informative.
    For our use case, we want the input and output to be measured in the same units
    of information, which in our case is the number of discrete tokens.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有人会问，为什么我们限制自己去寻找难以解码的提示——或可以解码为自然语言的令牌。软令牌或基于嵌入的优化更快、更便宜，但使压缩比的信息量大大减少。对于我们的使用案例，我们希望输入和输出以相同的信息单位进行测量，在我们的情况下，即离散令牌的数量。
- en: '3.2 MiniPrompt: A Practical Test for Compressible Memorization'
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 MiniPrompt：可压缩记忆的实用测试
- en: Since the compression rate $ACR$ (early stopping). Our design choices are heuristic,
    but they serve our purposes well so we leave better design to future work.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于压缩率$ACR$（早期停止）。我们的设计选择是启发式的，但它们很好地服务于我们的目的，因此我们将更好的设计留给未来的工作。
- en: In all of our experiments below, when we present memorization metrics using
    compression, we are showing the results of running our MiniPrompt algorithm. As
    noted in [Algorithm 1](#alg1 "In Приложение A Algorithms In Our Experiments ‣
    Rethinking LLM Memorization through the Lens of Adversarial Compression"), the
    optimizer is a choice, and where that option is not set to GCG, we make that clear.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的所有实验中，当我们使用压缩展示记忆指标时，我们展示的是运行MiniPrompt算法的结果。如[算法1](#alg1 "在附录A中算法 在我们的实验中
    ‣ 通过对抗性压缩的视角重新思考LLM记忆")中所述，优化器是一个选择，当该选项未设置为GCG时，我们会明确指出。
- en: 4 Compressible Memorization in Practice
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实践中的可压缩记忆
- en: We show the practical value of our definition and algorithm through several
    case studies and we show that the definition meets our expectations around memorization
    with validation experiments. Our case studies start with a demonstration of how
    a model owner trying to circumvent a regulation about data memorization might
    use in-context unlearning (Pawelczyk et al., [2023](#bib.bib24)), or design-specific
    system prompts that change how apparent is memorization. Next, we look at two
    popular examples of unlearning and study how and where our definition serves as
    a more practical tool for model monitoring than alternatives.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过几个案例研究展示了我们定义和算法的实际价值，并展示了该定义通过验证实验满足了我们对记忆的期望。我们的案例研究从演示模型所有者如何试图规避关于数据记忆的法规开始，这可能使用上下文遗忘（Pawelczyk等，[2023](#bib.bib24)），或设计特定的系统提示，改变记忆的明显程度。接下来，我们查看两个流行的遗忘示例，并研究我们的定义在模型监控中如何作为比其他方法更实用的工具。
- en: 'MiniPrompt finds short suffixes that elicit the target.
    Prompt: $<$ \n\nGive me a famous quote. impro  persistence [/INST]
    Response: Sure! Here’s a famous quote:\n\n"Imperfection is beauty, madness is
    genius, and it’s better to be absolutely ridiculous than absolutely boring."'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'MiniPrompt finds short suffixes that elicit the target.
    Prompt: $<$ \n\nGive me a famous quote. impro  persistence [/INST]
    Response: Sure! Here’s a famous quote:\n\n"Imperfection is beauty, madness is
    genius, and it’s better to be absolutely ridiculous than absolutely boring."'
- en: 'Рис. 3: In-Context Unlearning (ICUL) fools completion not compression. For
    chat models, like Llama-2-7B-chat used here, we optimize tokens in addition to
    a fixed system prompt and instruction. In this setting, we show that MiniPrompt
    compresses the quote in purple to the two blue tokens in the prompt in the top
    cell. Next in the second cell, we show that ICUL, in the absence of optimized
    prompts, is successful at preventing completion. Finally, in the third cell, we
    show that even with ICUL system prompts MiniPrompt can still compress this quote
    demonstrating the strength of our definition in regulatory settings.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：上下文遗忘（ICUL）欺骗完成而非压缩。对于聊天模型，如这里使用的Llama-2-7B-chat，我们除了固定的系统提示和指令外，还优化了令牌。在这种设置下，我们展示了MiniPrompt如何将紫色引文压缩为顶部单元格中提示的两个蓝色令牌。接着，在第二个单元格中，我们展示了在没有优化提示的情况下，ICUL成功阻止了完成。最后，在第三个单元格中，我们展示了即使有ICUL系统提示，MiniPrompt仍然可以压缩这个引文，展示了我们定义在监管环境中的强大。
- en: 4.1 The Illusion of Compliance
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 合规的幻觉
- en: As data usage regulation advances, there is an emerging motive for organizations
    and individuals that serve or release models to make it hard to determine that
    their models have memorized anything. To that end, we consider a simple defense
    that these model owners might employ as a proof-of-concept that one can easily
    fool existing definitions of memorization but not our compression-based definition.
    We show that it is possible to skirt completion or regurgitation-based tests with
    in-context unlearning (Pawelczyk et al., [2023](#bib.bib24)). Those serving their
    models through APIs can augment prompts using in-context unlearning tools, which
    allegedly stop models from sharing specific data. The aim here is to make sure
    that compliance with fair use laws or the Right To Be Forgotten (OAG, [2021](#bib.bib23);
    Union, [2016](#bib.bib29)) can be effectively monitored so we can avoid the *illusion
    of compliance* which crops up with other definitions of memorization.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据使用法规的进展，组织和个人在提供或发布模型时，有了使其模型记忆信息变得难以确定的动机。为此，我们考虑一种简单的防御措施，模型拥有者可能会采用这种措施作为概念验证，即可以轻易欺骗现有的记忆定义，但不会欺骗我们基于压缩的定义。我们展示了使用上下文退化（Pawelczyk
    et al., [2023](#bib.bib24)）绕过基于完成或重复的测试是可能的。那些通过API提供模型的服务者可以使用上下文退化工具增强提示，这些工具据称可以阻止模型共享特定数据。这里的目标是确保对公平使用法律或被遗忘权（OAG,
    [2021](#bib.bib23); Union, [2016](#bib.bib29)）的合规性可以有效监测，以避免其他记忆定义中出现的*合规幻觉*。
- en: We start by looking for the compression ratio of a famous quote using Llama-2-7B-chat
    (Touvron et al., [2023](#bib.bib27)) with a slightly modified strategy. Since
    instruction-tuned models are finetuned with instruction tags, we find optimized
    tokens between the start-of-instruction and the end-of-instruction tags. Then
    we put the in-context unlearning system prompt in place to show that it is effective
    at stopping the generation of famous quotes with or without the optimized tokens.
    Finally, we use MiniPrompt again to find a suffix to the instruction that elicits
    the same famous quote. In [Figure 3](#S4.F3 "In 4 Compressible Memorization in
    Practice ‣ Rethinking LLM Memorization through the Lens of Adversarial Compression"),
    we show examples of each of these steps.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始通过使用Llama-2-7B-chat (Touvron et al., [2023](#bib.bib27))并采用稍微修改的策略来查找名言的压缩比。由于指令调整模型是通过指令标签进行微调的，我们发现了指令开始和指令结束标签之间的优化令牌。然后我们放置上下文退化系统提示，以证明它能够有效地阻止在有或没有优化令牌的情况下生成名言。最后，我们再次使用MiniPrompt来查找引发相同名言的指令后缀。在[图3](#S4.F3
    "在4中压缩的记忆实践 ‣ 通过对抗性压缩的视角重新思考LLM记忆")中，我们展示了这些步骤的每个示例。
- en: We find short suffixes to these in-context unlearning system prompts that elicit
    memorized strings. Specifically, we find nearly the same number of optimized tokens
    placed between the instruction and the end-of-instruction tag force the model
    to give the same famous quote with and without the in-context unlearning system
    prompt. This consistency in the compression ratio—and therefore the memorization
    test—matches our intuition that without changing model weights memorized samples
    are not forgotten. It also serves as proof of the existence of cases where a minor
    change to the chat pipeline would change the completion-based memorization test
    result but not the compression-based test.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现对这些上下文退化系统提示的短后缀能够引发记忆字符串。具体而言，我们发现优化后的令牌数量几乎相同，置于指令和指令结束标签之间，这使得模型在有或没有上下文退化系统提示的情况下都能给出相同的名言。这种压缩比的一致性——因此也是记忆测试——符合我们的直觉，即在不更改模型权重的情况下，记忆样本不会被遗忘。它也证明了存在这样的情况：对聊天管道的轻微修改可能会改变基于完成的记忆测试结果，但不会改变基于压缩的测试结果。
- en: '4.2 TOFU: Unlearning and Memorization with Author Profiles'
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 TOFU：通过作者资料进行的退化和记忆
- en: In the unlearning community, baselines are generally considered weak (Maini
    et al., [2024](#bib.bib20)), and measuring memorization with completion-based
    tests gives a false sense of unlearning, even for these weak baselines. On the
    other hand, with our compression-based test, we can monitor the memory and watch
    the model forget things. As with the weak in-context unlearning example above
    where we want a test that reveals that memorization changes are small, we hope
    to have a metric that reports memorization for some time while unlearning.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在去学习社区中，基线通常被认为是弱的（Maini 等，[2024](#bib.bib20)），并且用基于完成的测试来测量记忆会给这些弱基线带来虚假的去学习感。另一方面，通过我们的基于压缩的测试，我们可以监控记忆并观察模型遗忘的情况。正如上述弱上下文去学习示例中，我们希望有一个能在去学习过程中报告记忆的度量。
- en: We compare completion and compression tests on the TOFU dataset (Maini et al.,
    [2024](#bib.bib20)). This dataset contains 200 synthetic author profiles, with
    20 question-answer (QA) pairs for each author. We finetune Phi-1.5 (Li et al.,
    [2023](#bib.bib17)) on all 4,000 QA samples and use gradient ascent to unlearn
    $5\%$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 TOFU 数据集上比较了完成和压缩测试（Maini 等，[2024](#bib.bib20)）。该数据集包含 200 个合成作者档案，每个作者有
    20 个问题-答案（QA）对。我们在所有 4,000 个 QA 样本上微调 Phi-1.5（Li 等，[2023](#bib.bib17)），并使用梯度上升来去学习
    $5\%$。
- en: 'As unlearning progresses, we prompt the model to generate answers to the supposedly
    unlearned questions and record the portion of data that can be completed and compressed.
    [Figure 4](#S4.F4 "In 4.2 TOFU: Unlearning and Memorization with Author Profiles
    ‣ 4 Compressible Memorization in Practice ‣ Rethinking LLM Memorization through
    the Lens of Adversarial Compression") shows that after only 16 unlearning steps,
    none of the unlearned questions can be completed exactly. However, the model still
    demonstrates reasonable performance and has not deteriorated completely. As expected,
    compression shows that a considerable amount of the ‘forget data’ is compressible
    and hence memorized. This case suggests that we cannot safely rely on completion
    as a metric for memorization because it is too strict.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 随着去学习的进展，我们促使模型生成对 supposedly unlearned 问题的回答，并记录可以完成和压缩的数据部分。[图4](#S4.F4 "在
    4.2 TOFU：通过作者档案进行去学习和记忆 ‣ 4 实践中的可压缩记忆 ‣ 从对抗压缩的角度重新思考 LLM 记忆") 显示，在仅经过 16 步去学习后，未去学习的问题没有一个能被完全完成。然而，模型仍显示出合理的性能，并未完全恶化。正如预期的那样，压缩显示出大量的“遗忘数据”是可压缩的，因此被记住。这个案例表明，我们不能安全地依赖完成作为记忆的度量，因为它过于严格。
- en: '![Refer to caption](img/63c9557e3c47c2da1219440077a8d89d.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/63c9557e3c47c2da1219440077a8d89d.png)'
- en: 'QA
    after 20 unlearning steps Question: What professions
    do Hina Ameen’s parents hold? Ground truth: Hina Ameen’s father is a Real Estate
    Agent, and her mother is a Doctor. Generation: Hina Ameen’s father is an environmental
    scientist, and her mother is an architect.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 'QA
    after 20 unlearning steps Question: What professions
    do Hina Ameen’s parents hold? Ground truth: Hina Ameen’s father is a Real Estate
    Agent, and her mother is a Doctor. Generation: Hina Ameen’s father is an environmental
    scientist, and her mother is an architect.'
- en: 'Рис. 4: Left: Completion vs compression on TOFU data, unlearning Phi-1.5 with
    gradient ascent. Right: Generation after 20 unlearning steps.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：左：TOFU 数据上的完成与压缩，使用梯度上升去学习 Phi-1.5。右：经过 20 步去学习后的生成结果。
- en: 4.3 Trying to Forget Harry Potter
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 尝试忘记《哈利·波特》
- en: In their paper on unlearning Harry Potter, Eldan & Russinovich ([2023](#bib.bib11))
    claim that Llama-2-chat can forget about Harry Potter with several steps of unlearning.
    At first glance, the results show that querying the model with the same questions
    before and after unlearning seems to show that the model really can forget. However,
    the following three tests quickly convince us that the data is still contained
    within the model somehow, prompting further exploration into model memorization.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们关于去学习《哈利·波特》的论文中，Eldan 和 Russinovich（[2023](#bib.bib11)）声称 Llama-2-chat 可以通过几步去学习忘记《哈利·波特》。乍一看，结果显示，在去学习前后用相同问题查询模型似乎表明模型确实可以忘记。然而，接下来的三个测试迅速使我们相信数据仍然以某种方式包含在模型中，促使进一步探索模型记忆。
- en: '1.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: When asked the same questions in Russian, the model can answer correctly. We
    provide examples of such behavior in Appendix [C](#A3 "Приложение C More Details
    of Unlearning Harry Potter ‣ Rethinking LLM Memorization through the Lens of Adversarial
    Compression").
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当用俄语问相同的问题时，模型能够正确回答。我们在附录 [C](#A3 "附录 C 更多关于去学习《哈利·波特》的细节 ‣ 从对抗压缩的角度重新思考 LLM
    记忆") 中提供了这种行为的示例。
- en: '2.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: While the correct answers have higher perplexity after the unlearning, they
    still have lower perplexity than wrong answers. [Figure 5](#S4.F5 "In 4.3 Trying
    to Forget Harry Potter ‣ 4 Compressible Memorization in Practice ‣ Rethinking
    LLM Memorization through the Lens of Adversarial Compression") shows that unlearning
    gives fewer of the correct answers extremely small losses, but an obvious dichotomy
    between the right and wrong answers remains.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管正确答案在遗忘后具有更高的困惑度，但它们仍然比错误答案的困惑度低。[图5](#S4.F5 "在4.3 尝试忘记《哈利·波特》 ‣ 4 实践中的可压缩记忆
    ‣ 从对抗性压缩的视角重新思考LLM记忆")显示，遗忘使得正确答案的极小损失减少，但正确答案和错误答案之间的明显分歧仍然存在。
- en: '3.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: With adversarial attacks designed to force answers in the affirmative without
    any information about the true answer, we can elicit the correct response, see
    [Figure 6](#S4.F6 "In 4.3 Trying to Forget Harry Potter ‣ 4 Compressible Memorization
    in Practice ‣ Rethinking LLM Memorization through the Lens of Adversarial Compression").
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过设计对抗性攻击，强制在没有任何真实答案信息的情况下给出肯定的回答，我们可以引出正确的回应，见[图6](#S4.F6 "在4.3 尝试忘记《哈利·波特》
    ‣ 4 实践中的可压缩记忆 ‣ 从对抗性压缩的视角重新思考LLM记忆")。
- en: Motivated by these indications that the model has not truly forgotten Harry
    Potter, we measure the compression ratios of the true answers before and after
    unlearning. and find that they are still compressible. [Figure 6](#S4.F6 "In 4.3
    Trying to Forget Harry Potter ‣ 4 Compressible Memorization in Practice ‣ Rethinking
    LLM Memorization through the Lens of Adversarial Compression") shows that even
    after unlearning, nearly the same amount of Harry Potter text is still memorized.
    We conclude that this unlearning tactic is not successful. Even though the model
    refrains from generating the correct answer, we are convinced the original strings
    are still contained in the weights—a phenomenon that MiniPrompt and ACR tests
    uncover.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 受到这些迹象的启发，即模型并没有真正忘记《哈利·波特》，我们测量了在遗忘前后真实答案的压缩比，并发现它们仍然是可以压缩的。[图6](#S4.F6 "在4.3
    尝试忘记《哈利·波特》 ‣ 4 实践中的可压缩记忆 ‣ 从对抗性压缩的视角重新思考LLM记忆")显示，即使在遗忘之后，几乎相同数量的《哈利·波特》文本仍然被记住。我们得出结论，这种遗忘策略并没有成功。尽管模型避免生成正确答案，但我们确信原始字符串仍然存在于权重中——这一现象在MiniPrompt和ACR测试中被揭示出来。
- en: '![Refer to caption](img/d82dc34b219153623155858d74fcaf9a.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d82dc34b219153623155858d74fcaf9a.png)'
- en: 'Рис. 5: Negative log-likelihood (normalized to $[0,1]$) of true and false answers
    given a Harry Potter question. Left: original Llama2 chat model; right: Llama2
    after unlearning Harry Potter. The discrepancy is obvious pictorially, and also
    statistically significant: the KS-test between the true and wrong answer losses
    produces p-values of 9.7e-24 and 5.9e-14, respectively.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：给定《哈利·波特》问题的真实和虚假答案的负对数似然（归一化为 $[0,1]$）。左侧：原始Llama2聊天模型；右侧：遗忘《哈利·波特》后的Llama2。图示上的差异明显，统计上也具有显著性：真实和错误答案损失之间的KS检验产生了9.7e-24和5.9e-14的p值。
- en: '![Refer to caption](img/59f2c1d5ea14f6c72370e0d66e2299ff.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/59f2c1d5ea14f6c72370e0d66e2299ff.png)'
- en: 'A prompt that elicit Harry
    Potter text Prompt: [INST]  +  Harry  Potter  quoted\u2011  brave  theme  [/INST]
    Generation: "It  is  our  choices,  Harry,  that  show  what  we  truly  are,  far  more  than  our  abilities."  -  Albus  Dumbledore'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 'A prompt that elicit Harry
    Potter text Prompt: [INST]  +  Harry  Potter  quoted\u2011  brave  theme  [/INST]
    Generation: "It  is  our  choices,  Harry,  that  show  what  we  truly  are,  far  more  than  our  abilities."  -  Albus  Dumbledore'
- en: 'Рис. 6: Left: Fraction of Harry Potter texts that are compressible. Right:
    an example of hard tokens that elicit Harry Potter text.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：左侧：可压缩的《哈利·波特》文本的比例。右侧：引出《哈利·波特》文本的硬令牌示例。
- en: 4.4 Bigger Models Memorize More
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 更大的模型记忆更多
- en: Since prior work has proposed alternative definitions of memorization that show
    that bigger models memorize more (Carlini et al., [2023](#bib.bib6)), we ask whether
    our definition leads to the same finding. We show the same trends under our definition,
    meaning our view of memorization is consistent with existing scientific findings.
    We measure the fraction of the famous quotes that are compressible by four different
    Pythia models (Biderman et al., [2023](#bib.bib2)) with parameter counts of 410M,
    1.4B, 6.9B, and 12B and the results are in [Figure 7](#S4.F7 "In Random Sequences
    ‣ 4.5 Validation of MiniPrompt with Four Categories of Data ‣ 4 Compressible Memorization
    in Practice ‣ Rethinking LLM Memorization through the Lens of Adversarial Compression")).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于先前的研究提出了记忆的替代定义，显示更大的模型记忆更多（Carlini et al., [2023](#bib.bib6)），我们询问我们的定义是否得出了相同的结论。我们在我们的定义下显示了相同的趋势，这意味着我们对记忆的看法与现有科学发现一致。我们测量了四个不同Pythia模型（Biderman
    et al., [2023](#bib.bib2)）在410M、1.4B、6.9B和12B参数计数下的名言的可压缩比例，结果见[图7](#S4.F7 "在随机序列中
    ‣ 4.5 用四类数据验证MiniPrompt ‣ 4 实践中的可压缩记忆 ‣ 从对抗性压缩的视角重新思考LLM记忆")。
- en: 4.5 Validation of MiniPrompt with Four Categories of Data
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 MiniPrompt在四类数据上的验证
- en: Since we are proposing a definition, the validation step is more complex than
    comparing it to some ground truth or baseline values. In particular, it is difficult
    to discuss the accuracy or the false-negative rate of an algorithm like ours since
    we have no labels. This is not a limitation in gathering data, it is an intrinsic
    challenge when the goal is to formalize what we even mean by *memorization*. Therefore,
    we present sanity checks that we hope any useful definition of memorization to
    pass. The following experiments are done with the open source Pythia (Biderman
    et al., [2023](#bib.bib2)) models, which are trained on The Pile (Gao et al.,
    [2020](#bib.bib12)) providing transparency around their training data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在提出定义，因此验证步骤比将其与一些基准真值或基准值进行比较更复杂。特别是，由于我们没有标签，因此很难讨论像我们这样的算法的准确性或假阴性率。这不是收集数据的限制，而是在我们试图正式定义*记忆化*的目标时的内在挑战。因此，我们提出了一些理智检查，希望任何有用的记忆化定义都能通过这些检查。以下实验使用了开源的Pythia（Biderman
    et al., [2023](#bib.bib2)）模型，这些模型在The Pile（Gao et al., [2020](#bib.bib12)）上进行训练，提供了关于其训练数据的透明度。
- en: Random Sequences
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机序列
- en: We look at random sequences of tokens because we want to rule out the possibility
    that we can always find an adversarial, few-token prompt even for random output—random
    strings should not be compressible. To this end, we draw uniform random samples
    with replacements from the token vocabulary to build a set of 100 random outputs
    that vary in length (between 3 and 17 tokens). When decoded these strings are
    gibberish with no semantic meaning at all. We find that these strings are never
    compressible—that is across multiple model sizes we never find a prompt shorter
    than the target that elicits the target sequence as output, see the zero-height
    bars in [Figure 8](#S4.F8 "In Random Sequences ‣ 4.5 Validation of MiniPrompt
    with Four Categories of Data ‣ 4 Compressible Memorization in Practice ‣ Rethinking
    LLM Memorization through the Lens of Adversarial Compression").
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们查看随机的标记序列，因为我们想排除这样一种可能性：即我们可以始终找到一个对抗性的、少量标记的提示，即使是随机输出——随机字符串不应当是可压缩的。为此，我们从标记词汇表中以均匀随机方式抽取样本，构建了一个长度在3到17个标记之间变化的100个随机输出的集合。当解码时，这些字符串是毫无语义的胡言乱语。我们发现这些字符串从未被压缩——即在多个模型大小中，我们从未找到比目标短的提示来引出目标序列作为输出，请参见[图
    8](#S4.F8 "在随机序列中 ‣ 4.5 MiniPrompt在四类数据上的验证 ‣ 4 实践中的可压缩记忆化 ‣ 通过对抗性压缩的视角重新思考LLM记忆化")中的零高度条。
- en: '![Refer to caption](img/f01cbe4b49ed410667c095d4c4d996b4.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/f01cbe4b49ed410667c095d4c4d996b4.png)'
- en: 'Рис. 7: Memorization in Pythia models. Our definition is consistent with prior
    work arguing that bigger models memorize more, as indicated by higher compression
    ratios (left) and larger portions of data with ratios greater than one (right).
    These figures are from the Famous Quotes dataset.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: Pythia模型中的记忆化。我们的定义与先前的工作一致，后者认为较大的模型记忆化更多，这由更高的压缩比（左）和数据中大于一的比例（右）所示。这些图表来自名人名言数据集。'
- en: '![Refer to caption](img/0d28db7cb11f88f2281fa629098ffc1c.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/0d28db7cb11f88f2281fa629098ffc1c.png)'
- en: 'Рис. 8: Memorization in Pythia-1.4B. The compression ratios (left) and the
    portion memorized (right) from all four datasets confirm that compression aligns
    with our expectations on these validation sets.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: Pythia-1.4B中的记忆化。从四个数据集中的压缩比（左）和记忆部分（右）可以确认，压缩与我们在这些验证集上的预期一致。'
- en: Associated Press November 2023
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 美联社 2023年11月
- en: To further determine the validity of our definition, we investigate the average
    compression rate of natural text that is not in the training set. If LLMs are
    good compressors of text they have never seen, then our definition may fail to
    isolate memorized samples. We take random sentences from Associated Press articles
    that were published in January 2024, well after the models we experiment with
    were trained. These strings are samples from the distribution of training data
    as the training set includes real news articles from just a few months prior.
    Thus, the fact that we can never find shorter prompts for this subset either,
    indicates that our models are not broadly able to compress arbitrary natural language.
    Again, see the zero-height bars in [Figure 8](#S4.F8 "In Random Sequences ‣ 4.5
    Validation of MiniPrompt with Four Categories of Data ‣ 4 Compressible Memorization
    in Practice ‣ Rethinking LLM Memorization through the Lens of Adversarial Compression").
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步确定我们定义的有效性，我们调查了不在训练集中的自然文本的平均压缩率。如果LLM能够很好地压缩它们从未见过的文本，那么我们的定义可能无法有效区分记忆样本。我们从2024年1月发布的美联社文章中随机抽取句子，这些文章发布的时间远在我们实验所用模型的训练之后。这些字符串来自于训练数据的分布，因为训练集包括了几个月前的真实新闻文章。因此，我们始终无法为这一子集找到更短的提示，说明我们的模型无法广泛地压缩任意自然语言。再一次，请参见[图8](#S4.F8
    "在随机序列中 ‣ 4.5 使用四类数据验证MiniPrompt ‣ 4 实践中的可压缩记忆 ‣ 通过对抗压缩的视角重新思考LLM记忆")中的零高度条形图。
- en: Famous Quotes
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 著名引言
- en: Next, we turn our attention to famous strings, of which many should be categorized
    as ‘memorized’ by any useful definition. These are quotes like ‘‘to be, or not
    to be, that is the question,’’ which we know are examples that are repeated many
    times in the training data. We find that Pythia-1.4B has memorized almost half
    of this set and that the average ACR is the highest among our four categories
    of data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将注意力转向著名字符串，其中许多应该根据任何有用的定义被归类为“记忆”。这些是像“生存还是毁灭，这是个问题”这样的引文，我们知道这些示例在训练数据中重复出现了很多次。我们发现Pythia-1.4B几乎记住了这一组中的一半，并且其平均ACR在我们四类数据中是最高的。
- en: Wikipedia
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 维基百科
- en: Finally, as our first experimental inquiry, we look at the memorization of training
    samples that are not common or famous, but that do exist in the training set.
    We take random sentences from Wikipedia articles that are included in the Pile
    (Gao et al., [2020](#bib.bib12)) and compute their compression ratio. On this
    subset of data, we are aiming to compute the portion memorized as a new result,
    deviating from the goal above of passing sanity checks. [Figure 8](#S4.F8 "In
    Random Sequences ‣ 4.5 Validation of MiniPrompt with Four Categories of Data ‣
    4 Compressible Memorization in Practice ‣ Rethinking LLM Memorization through
    the Lens of Adversarial Compression") shows that some of these sentences from
    Wikipedia are memorized and that the average compression ratio is between the
    average among famous quotes and news articles. Note that the memorized samples
    form this subset are strings that appear many times on the internet like ‘‘The
    Burton is a historic apartment building located at Indianapolis, Indiana.’’
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，作为我们第一次实验性调查，我们关注那些不常见或不著名，但确实存在于训练集中的训练样本的记忆。我们从包含在Pile中的维基百科文章（Gao等，[2020](#bib.bib12)）中随机抽取句子，并计算它们的压缩比率。在这一数据子集上，我们旨在计算记忆的部分作为一个新的结果，与之前通过理智检查的目标有所不同。[图8](#S4.F8
    "在随机序列中 ‣ 4.5 使用四类数据验证MiniPrompt ‣ 4 实践中的可压缩记忆 ‣ 通过对抗压缩的视角重新思考LLM记忆")显示，这些来自维基百科的句子中，有些被记忆了，且平均压缩比率介于名言和新闻文章的平均值之间。请注意，这一子集中的记忆样本是那些在互联网上多次出现的字符串，比如“Burton是一座位于印第安纳波利斯，印第安纳州的历史性公寓楼。”
- en: On the note of sanity checks, one potential pitfall of our MiniPrompt algorithm
    is its reliance on GCG. It is possible that there exist shorter strings than we
    can find. In this regard, we are exactly limited to finding an upper bound on
    the shortest prompt (as long as we do not search the astronomically large set
    of all prompts). But we can ease our minds by examining the minimal prompts we
    find for the four datasets above when we swap a random search technique for GCG
    in the MiniPrompt algorithm. In fact, random search (see [Algorithm 3](#alg3 "In
    Приложение A Algorithms In Our Experiments ‣ Rethinking LLM Memorization through
    the Lens of Adversarial Compression")) does slightly worse as an optimizer but
    tells the same story across the board. Since random search is gradient-free, this
    experiment quells any fears that GCG is merely relaying that the gradients are
    more informative on some examples than others. The details of this experiment
    and our exact random search algorithm are in [Appendix A](#A1 "Приложение A Algorithms
    In Our Experiments ‣ Rethinking LLM Memorization through the Lens of Adversarial
    Compression").
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 说到合理性检查，我们的 MiniPrompt 算法的一个潜在缺陷是其对 GCG 的依赖。可能存在比我们找到的更短的字符串。在这方面，我们的确仅限于找到最短提示的上界（只要我们不搜索所有提示的天文数字大的集合）。但我们可以通过检查在
    MiniPrompt 算法中用随机搜索技术替代 GCG 时，我们为上述四个数据集找到的最小提示来宽慰自己。实际上，随机搜索（见 [算法 3](#alg3 "在附录
    A 中的算法 ‣ 从对抗性压缩的角度重新思考 LLM 记忆")）作为优化器表现稍逊一筹，但在各方面讲述了相同的故事。由于随机搜索是不依赖梯度的，这个实验消除了
    GCG 仅仅是将梯度在某些示例上比其他示例更具信息性这一担忧。这个实验的细节以及我们的确切随机搜索算法在 [附录 A](#A1 "附录 A 中的算法 ‣ 从对抗性压缩的角度重新思考
    LLM 记忆") 中。
- en: 5 Additional Related Work
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个附加相关工作
- en: In addition to existing notions of memorization, our work touches on prompt
    optimization, compression in LLMs, and machine unlearning. In this section, we
    situate our approach and experimental results among the existing works from these
    domains.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 除了现有的记忆观念，我们的工作涉及提示优化、LLMs 中的压缩以及机器去学习。在这一部分，我们将我们的方法和实验结果置于这些领域的现有工作中。
- en: Prompt Optimization
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示优化
- en: We borrow prompt optimization tools from work on jailbreaking where the goal
    is to force LLMs to break their alignment and produce nefarious and toxic output
    by way of optimizing prompts (Zou et al., [2023](#bib.bib32); Zhu et al., [2023](#bib.bib31);
    Chao et al., [2023](#bib.bib7); Andriushchenko, [2023](#bib.bib1)). Our extension
    of those techniques toward ends other than jailbreaking adds to the many and varied
    objectives that these discrete optimizers are useful for minimizing (Geiping et al.,
    [2024](#bib.bib13)).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们借鉴了旨在通过优化提示来强制 LLMs 违背其对齐，生成有害和有毒输出的监狱破解中的提示优化工具（Zou 等，[2023](#bib.bib32)；Zhu
    等，[2023](#bib.bib31)；Chao 等，[2023](#bib.bib7)；Andriushchenko，[2023](#bib.bib1)）。我们将这些技术扩展到除监狱破解以外的其他目的，增加了这些离散优化器在最小化多种不同目标方面的有用性（Geiping
    等，[2024](#bib.bib13)）。
- en: Compression in LLMs
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs 中的压缩
- en: There are several links between compression and language modelling and we borrow
    some vocabulary, but our work diverges from these other lines of research. For
    example, Delétang et al. ([2023](#bib.bib9)) argue that LLMs are compression engines,
    but they use models as probability distributions over tokens and arithmetic encoding
    to show that LLMs are good general compressors. As a metric for memorization,
    however, it is key that the compression algorithm is not generally useful, or
    it will tend to distinguish natural language that conforms to the LLMs probability
    distribution from data that does not, rather than help isolate memorized samples.
    Other links to compression include the ideas that learnability and generalization
    with real data comes in part from the compressability of natural data (Goldblum
    et al., [2023](#bib.bib14)) and that grokking is related to the compressibility
    of networks themselves (Liu et al., [2023](#bib.bib18)). Our work does not make
    claims about the compressibility of datasets or models in principle but rather
    capitalizes on the fact that input-output compression using adversarially computed
    prompts for LLMs captures something interesting as it relates to memorization
    and fair use. In fact, Jiang et al. ([2023](#bib.bib16)) propose prompt compression
    for reducing time and cost of inference, which motivates our work as it suggests
    that we should be able to find short prompts that elicit the same responses as
    longer more natural-sounding inputs in some cases.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩与语言建模之间存在几个联系，我们借用了一些词汇，但我们的工作与这些研究方向有所不同。例如，Delétang et al. ([2023](#bib.bib9))
    认为大模型是压缩引擎，但他们使用模型作为代币上的概率分布和算术编码来展示大模型是优秀的通用压缩器。然而，作为记忆化的度量标准，关键在于压缩算法是否在一般情况下无用，否则它将倾向于区分符合大模型概率分布的自然语言和不符合的数据，而不是帮助隔离记忆样本。其他与压缩相关的联系包括学习性和实际数据的泛化部分来自于自然数据的压缩性（Goldblum
    et al., [2023](#bib.bib14)）以及grokking与网络自身的压缩性相关（Liu et al., [2023](#bib.bib18)）。我们的工作并不对数据集或模型的压缩性做原则性的声明，而是利用了利用对抗计算提示进行的输入-输出压缩在记忆化和公平使用方面捕捉到的一些有趣的内容。事实上，Jiang
    et al. ([2023](#bib.bib16)) 提出了提示压缩以减少推理的时间和成本，这激励了我们的工作，因为它表明我们应该能够找到在某些情况下产生与较长、更自然输入相同响应的短提示。
- en: Unlearning
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 去学习
- en: The focus of machine unlearning (Bourtoule et al., [2021](#bib.bib3); Sekhari
    et al., [2021](#bib.bib25); Ullah et al., [2021](#bib.bib28)) is to remove private,
    sensitive, or false data from models without retraining them from scratch. Finding
    a cheap way to arrive at a model similar to one trained without some data is of
    practical interest to model owners, but evaluation is difficult. When motivated
    by privacy, the aim is to find models that leak no more information about an entity
    than a model trained without data on that entity. This is intimately related to
    memorization, and so we use a popular unlearning benchmark (Maini et al., [2024](#bib.bib20))
    in our experiments.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 机器去学习的重点是从模型中移除私人、敏感或虚假的数据，而无需从头开始重新训练模型。找到一种廉价的方式来获得一个类似于未包含某些数据的训练模型对模型拥有者来说具有实际意义，但评估是困难的。当动机是隐私时，目标是找到不会泄露关于某个实体的更多信息的模型，这一点与记忆化密切相关，因此我们在实验中使用了一个流行的去学习基准（Maini
    et al., [2024](#bib.bib20)）。
- en: 6 Discussion
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: When proposing new definitions, we are tasked with justifying why a new one
    is needed and as well as showing its ability to capture a phenomenon of interest.
    This stands in contrast to developing detection/classification tools whose accuracy
    can easily be measured using labeled data. It is a difficult task by nature to
    define memorization as there is no set of ground truth labels that indicate which
    samples are memorized. Consequently, the criteria for a memorization definition
    should rely on how useful it is. Our definition is a promising direction for future
    regulation on LLM fair use of data as well as helping model owners confidently
    release models trained on sensitive data without releasing that data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当提出新定义时，我们的任务是证明为什么需要一个新的定义，并展示它能够捕捉一个感兴趣现象的能力。这与开发检测/分类工具形成对比，因为后者的准确性可以通过标记数据轻松衡量。定义记忆化本质上是一项困难的任务，因为没有一组真实标签来指示哪些样本被记忆。因此，记忆化定义的标准应该依赖于它的实用性。我们的定义是未来关于大模型公平使用数据的监管的有希望的方向，同时也帮助模型拥有者自信地发布在敏感数据上训练的模型而不释放那些数据。
- en: From an intuitive perspective, one can imagine compression as a communication
    game—Alice tries to send some data to Bob with as few bits as possible. In our
    regime, we assume that both Alice and Bob have access to the same LLM $M$. Future
    work to explore other ways of measuring compression and faster/cheaper optimization
    approaches could be fruitful. One drawback of optimizing hard tokens is the intrinsic
    complexity of discrete optimization. Instead, if we optimize over the continuous
    soft token space, the process will be much more efficient, but it remains unclear
    how to measure the information content of a soft token.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观的角度来看，人们可以将压缩想象为一种通信游戏——Alice 尝试以尽可能少的比特将数据发送给 Bob。在我们的范畴中，我们假设 Alice 和 Bob
    都可以访问相同的 LLM $M$。未来的工作可以探索其他衡量压缩的方法以及更快速/便宜的优化方法。优化硬标记的一个缺点是离散优化的内在复杂性。相反，如果我们在连续的软标记空间中进行优化，过程将更加高效，但如何测量软标记的信息内容仍不明确。
- en: Список литературы
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文献列表
- en: Andriushchenko (2023) Maksym Andriushchenko. Adversarial attacks on gpt-4 via
    simple random search. 2023.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andriushchenko (2023) Maksym Andriushchenko。通过简单的随机搜索对 GPT-4 进行对抗攻击。2023。
- en: 'Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Gregory
    Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu
    Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing
    large language models across training and scaling. In *International Conference
    on Machine Learning*, pp.  2397–2430\. PMLR, 2023.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony,
    Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
    USVSN Sai Prashanth, Edward Raff, 等等。Pythia：一个用于分析大型语言模型在训练和扩展中的套件。在*国际机器学习大会*，第
    2397–2430 页。PMLR，2023。
- en: Bourtoule et al. (2021) Lucas Bourtoule, Varun Chandrasekaran, Christopher A
    Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas
    Papernot. Machine unlearning. In *2021 IEEE Symposium on Security and Privacy
    (SP)*, pp.  141–159\. IEEE, 2021.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bourtoule et al. (2021) Lucas Bourtoule, Varun Chandrasekaran, Christopher A
    Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, 和 Nicolas
    Papernot。机器遗忘。在*2021 IEEE 安全与隐私研讨会 (SP)*，第 141–159 页。IEEE，2021。
- en: 'Carlini et al. (2019) Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej
    Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization
    in neural networks. In *28th USENIX security symposium (USENIX security 19)*,
    pp.  267–284, 2019.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini et al. (2019) Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej
    Kos, 和 Dawn Song。秘密分享者：评估和测试神经网络中的意外记忆。在*第28届 USENIX 安全研讨会 (USENIX security 19)*，第
    267–284 页，2019。
- en: Carlini et al. (2021) Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song,
    Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles.
    *arXiv preprint arXiv:2112.03570*, 2021.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini et al. (2021) Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song,
    Andreas Terzis, 和 Florian Tramer。从基本原则出发的成员推断攻击。*arXiv 预印本 arXiv:2112.03570*，2021。
- en: Carlini et al. (2023) Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
    Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across
    neural language models, 2023.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini et al. (2023) Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
    Katherine Lee, Florian Tramer, 和 Chiyuan Zhang。量化神经语言模型中的记忆，2023。
- en: Chao et al. (2023) Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani,
    George J Pappas, and Eric Wong. Jailbreaking black box large language models in
    twenty queries. *arXiv preprint arXiv:2310.08419*, 2023.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chao et al. (2023) Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani,
    George J Pappas, 和 Eric Wong。在二十个查询中破解黑箱大型语言模型。*arXiv 预印本 arXiv:2310.08419*，2023。
- en: Cooper et al. (2023) A Feder Cooper, Katherine Lee, James Grimmelmann, Daphne
    Ippolito, Christopher Callison-Burch, Christopher A Choquette-Choo, Niloofar Mireshghallah,
    Miles Brundage, David Mimno, Madiha Zahrah Choksi, et al. Report of the 1st workshop
    on generative ai and law. *arXiv preprint arXiv:2311.06477*, 2023.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cooper et al. (2023) A Feder Cooper, Katherine Lee, James Grimmelmann, Daphne
    Ippolito, Christopher Callison-Burch, Christopher A Choquette-Choo, Niloofar Mireshghallah,
    Miles Brundage, David Mimno, Madiha Zahrah Choksi, 等等。关于生成性人工智能和法律的第一次研讨会报告。*arXiv
    预印本 arXiv:2311.06477*，2023。
- en: Delétang et al. (2023) Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne,
    Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang,
    Matthew Aitchison, Laurent Orseau, et al. Language modeling is compression. *arXiv
    preprint arXiv:2309.10668*, 2023.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delétang et al. (2023) Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne,
    Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang,
    Matthew Aitchison, Laurent Orseau, 等等。语言建模是压缩。*arXiv 预印本 arXiv:2309.10668*，2023。
- en: Duan et al. (2024) Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon
    Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and
    Hannaneh Hajishirzi. Do membership inference attacks work on large language models?
    *arXiv:2402.07841*, 2024.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan et al. (2024) Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon
    Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and
    Hannaneh Hajishirzi. 大型语言模型是否会受到会员推断攻击？*arXiv:2402.07841*，2024年。
- en: Eldan & Russinovich (2023) Ronen Eldan and Mark Russinovich. Who’s harry potter?
    approximate unlearning in llms. *arXiv preprint arXiv:2310.02238*, 2023.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eldan & Russinovich (2023) Ronen Eldan 和 Mark Russinovich. 谁是哈利·波特？大型语言模型中的近似遗忘。*arXiv
    预印本 arXiv:2310.02238*，2023年。
- en: 'Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn
    Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language
    modeling, 2020.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn
    Presser, and Connor Leahy. The pile：一个包含多样文本的800GB数据集用于语言建模，2020年。
- en: Geiping et al. (2024) Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah,
    Yuxin Wen, and Tom Goldstein. Coercing llms to do and reveal (almost) anything.
    *arXiv preprint arXiv:2402.14020*, 2024.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geiping et al. (2024) Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah,
    Yuxin Wen, and Tom Goldstein. 迫使大型语言模型做几乎任何事情并揭示（几乎）所有信息。*arXiv 预印本 arXiv:2402.14020*，2024年。
- en: Goldblum et al. (2023) Micah Goldblum, Marc Finzi, Keefer Rowan, and Andrew Gordon
    Wilson. The no free lunch theorem, kolmogorov complexity, and the role of inductive
    biases in machine learning. *arXiv preprint arXiv:2304.05366*, 2023.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goldblum et al. (2023) Micah Goldblum, Marc Finzi, Keefer Rowan, and Andrew
    Gordon Wilson. 无免费午餐定理、Kolmogorov复杂性及归纳偏差在机器学习中的作用。*arXiv 预印本 arXiv:2304.05366*，2023年。
- en: Ippolito et al. (2023) Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan
    Zhang, Matthew Jagielski, Katherine Lee, Christopher A Choquette-Choo, and Nicholas
    Carlini. Preventing generation of verbatim memorization in language models gives
    a false sense of privacy. In *Proceedings of the 16th International Natural Language
    Generation Conference*, pp.  28–53\. Association for Computational Linguistics,
    2023.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ippolito et al. (2023) Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan
    Zhang, Matthew Jagielski, Katherine Lee, Christopher A Choquette-Choo, and Nicholas
    Carlini. 防止语言模型生成逐字记忆带来虚假的隐私感。在*第16届国际自然语言生成会议论文集*中，第28–53页。计算语言学协会，2023年。
- en: 'Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,
    and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large
    language models. *arXiv preprint arXiv:2310.05736*, 2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and
    Lili Qiu. Llmlingua：压缩提示以加速大型语言模型的推理。*arXiv 预印本 arXiv:2310.05736*，2023年。
- en: 'Li et al. (2023) Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno,
    Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical
    report. *arXiv preprint arXiv:2309.05463*, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno,
    Suriya Gunasekar, and Yin Tat Lee. 教科书就是你所需要的一切 II：phi-1.5技术报告。*arXiv 预印本 arXiv:2309.05463*，2023年。
- en: 'Liu et al. (2023) Ziming Liu, Ziqian Zhong, and Max Tegmark. Grokking as compression:
    A nonlinear complexity perspective. *arXiv preprint arXiv:2310.05918*, 2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Ziming Liu, Ziqian Zhong, and Max Tegmark. 理解压缩：一个非线性复杂性的视角。*arXiv
    预印本 arXiv:2310.05918*，2023年。
- en: 'Maini et al. (2021) Pratyush Maini, Mohammad Yaghini, and Nicolas Papernot.
    Dataset inference: Ownership resolution in machine learning. *arXiv preprint arXiv:2104.10706*,
    2021.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maini et al. (2021) Pratyush Maini, Mohammad Yaghini, and Nicolas Papernot.
    数据集推断：机器学习中的所有权解决。*arXiv 预印本 arXiv:2104.10706*，2021年。
- en: 'Maini et al. (2024) Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C
    Lipton, and J Zico Kolter. Tofu: A task of fictitious unlearning for llms. *arXiv
    preprint arXiv:2401.06121*, 2024.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maini et al. (2024) Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C
    Lipton, and J Zico Kolter. Tofu：一个虚拟的遗忘任务用于大型语言模型。*arXiv 预印本 arXiv:2401.06121*，2024年。
- en: (21) Cade Metz and Katie Robertson. Openai seeks to dismiss parts of the new
    york times’s lawsuit. *The New York Times*. URL [https://www.nytimes.com/2024/02/27/technology/openai-new-york-times-lawsuit.html#:~:text=In%20its%20suit%2C%20The%20Times,someone%20to%20hack%20their%20chatbot.](https://www.nytimes.com/2024/02/27/technology/openai-new-york-times-lawsuit.html#:~:text=In%20its%20suit%2C%20The%20Times,someone%20to%20hack%20their%20chatbot.)
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (21) Cade Metz 和 Katie Robertson. OpenAI寻求驳回《纽约时报》诉讼的部分内容。*纽约时报*。URL [https://www.nytimes.com/2024/02/27/technology/openai-new-york-times-lawsuit.html#:~:text=In%20its%20suit%2C%20The%20Times,someone%20to%20hack%20their%20chatbot.](https://www.nytimes.com/2024/02/27/technology/openai-new-york-times-lawsuit.html#:~:text=In%20its%20suit%2C%20The%20Times,someone%20to%20hack%20their%20chatbot.)
- en: Nasr et al. (2023) Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski,
    A Feder Cooper, Daphne Ippolito, Christopher A Choquette-Choo, Eric Wallace, Florian
    Tramèr, and Katherine Lee. Scalable extraction of training data from (production)
    language models. *arXiv preprint arXiv:2311.17035*, 2023.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nasr 等人 (2023) Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski,
    A Feder Cooper, Daphne Ippolito, Christopher A Choquette-Choo, Eric Wallace, Florian
    Tramèr 和 Katherine Lee. 从（生产）语言模型中可扩展地提取训练数据。*arXiv 预印本 arXiv:2311.17035*, 2023。
- en: 'OAG (2021) CA OAG. Ccpa regulations: Final regulation text. *Office of the
    Attorney General, California Department of Justice*, 2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OAG (2021) CA OAG. Ccpa 规定：最终法规文本。*加州总检察长办公室*, 2021。
- en: 'Pawelczyk et al. (2023) Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju.
    In-context unlearning: Language models as few shot unlearners. *arXiv preprint
    arXiv:2310.07579*, 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pawelczyk 等人 (2023) Martin Pawelczyk, Seth Neel 和 Himabindu Lakkaraju. 上下文中的无学习：语言模型作为少量学习者。*arXiv
    预印本 arXiv:2310.07579*, 2023。
- en: 'Sekhari et al. (2021) Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha
    Suresh. Remember what you want to forget: Algorithms for machine unlearning. *Advances
    in Neural Information Processing Systems*, 34:18075–18086, 2021.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sekhari 等人 (2021) Ayush Sekhari, Jayadev Acharya, Gautam Kamath 和 Ananda Theertha
    Suresh. 记住你想要忘记的东西：机器无学习的算法。*神经信息处理系统进展*, 34:18075–18086, 2021。
- en: Shokri et al. (2017) Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly
    Shmatikov. Membership inference attacks against machine learning models. In *2017
    IEEE symposium on security and privacy (SP)*, pp.  3–18\. IEEE, 2017.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shokri 等人 (2017) Reza Shokri, Marco Stronati, Congzheng Song 和 Vitaly Shmatikov.
    针对机器学习模型的成员推断攻击。发表于 *2017 IEEE 安全与隐私研讨会 (SP)*, 页 3–18。IEEE, 2017。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等人. Llama 2: 开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*, 2023。'
- en: Ullah et al. (2021) Enayat Ullah, Tung Mai, Anup Rao, Ryan A Rossi, and Raman
    Arora. Machine unlearning via algorithmic stability. In *Conference on Learning
    Theory*, pp.  4126–4142\. PMLR, 2021.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ullah 等人 (2021) Enayat Ullah, Tung Mai, Anup Rao, Ryan A Rossi 和 Raman Arora.
    通过算法稳定性进行机器无学习。发表于 *学习理论会议*, 页 4126–4142。PMLR, 2021。
- en: Union (2016) European Union. Regulation (eu) 2016/679 of the european parliament
    and of the council. *Official Journal of the European Union*, 2016.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧盟 (2016) 欧洲联盟. 欧洲议会和理事会第 2016/679 号条例。*欧洲联盟官方公报*, 2016。
- en: Zhang et al. (2023) Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski,
    Florian Tramèr, and Nicholas Carlini. Counterfactual memorization in neural language
    models. *Advances in Neural Information Processing Systems*, 36:39321–39362, 2023.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2023) Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski,
    Florian Tramèr 和 Nicholas Carlini. 神经语言模型中的反事实记忆。*神经信息处理系统进展*, 36:39321–39362,
    2023。
- en: 'Zhu et al. (2023) Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao
    Wang, Furong Huang, Ani Nenkova, and Tong Sun. Autodan: Automatic and interpretable
    adversarial attacks on large language models. *arXiv preprint arXiv:2310.15140*,
    2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等人 (2023) Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao
    Wang, Furong Huang, Ani Nenkova 和 Tong Sun. Autodan: 对大型语言模型的自动化和可解释对抗攻击。*arXiv
    预印本 arXiv:2310.15140*, 2023。'
- en: Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson.
    Universal and transferable adversarial attacks on aligned language models. *arXiv
    preprint arXiv:2307.15043*, 2023.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等人 (2023) Andy Zou, Zifan Wang, J Zico Kolter 和 Matt Fredrikson. 针对对齐语言模型的通用和可转移的对抗攻击。*arXiv
    预印本 arXiv:2307.15043*, 2023。
- en: Приложение A Algorithms In Our Experiments
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 我们实验中的算法
- en: Algorithm 1 MiniPrompt
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 MiniPrompt
- en: 'Input: Model $M$return best'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '输入: 模型 $M$ 返回最佳'
- en: Algorithm 2 Greedy Coordinate Gradient (GCG) (Zou et al., [2023](#bib.bib32))
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 贪婪坐标梯度 (GCG) (Zou 等人，[2023](#bib.bib32))
- en: 'Input: Loss $\mathcal{L}$'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '输入: 损失 $\mathcal{L}$'
- en: Algorithm 3 Random Search (for LLM prompts) (Andriushchenko, [2023](#bib.bib1))
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 随机搜索（用于 LLM 提示）（Andriushchenko，[2023](#bib.bib1)）
- en: 'Input: Loss $\mathcal{L}$![Refer to caption](img/647b6e92b113b1f0fa7d720816ed340c.png)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '输入: 损失 $\mathcal{L}$![参见标题](img/647b6e92b113b1f0fa7d720816ed340c.png)'
- en: 'Рис. 9: Pythia-1.4B Memorization with Random Search.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: Pythia-1.4B 的随机搜索记忆。'
- en: '![Refer to caption](img/e3b4bf527b9900be9b1ebd8d9f7f9e22.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e3b4bf527b9900be9b1ebd8d9f7f9e22.png)'
- en: 'Рис. 10: Pythia-410M Memorization with GCG.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: Pythia-410M 的 GCG 记忆。'
- en: Приложение B More Details of In-context Unlearning
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 上下文中的无学习更多细节
- en: As a proof of concept, we further perform adversarial compression on five famous
    quotes and get an average compression ratios of 6.54 and 4.74, with versus without
    the unlearning system prompt, respectively. Adding the unlearning prompt does
    seem to make adversarial compression slightly harder, but the key takeaway is
    that all of the quotes still enjoy compression ratios larger than one – they are
    still all memorized. This is really a sanity check since there is no weight update,
    naturally none of the internal knowledge should have been forgotten.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 作为概念验证，我们进一步对五个著名引言进行了对抗压缩，得到的平均压缩比为6.54和4.74，分别是在有无取消学习系统提示的情况下。添加取消学习提示确实使对抗压缩稍微更难，但关键点是所有的引言仍然享有大于1的压缩比——它们仍然被记住。这确实是一个理智检查，因为没有权重更新，自然不应该遗忘任何内部知识。
- en: Приложение C More Details of Unlearning Harry Potter
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Приложение C 更多关于取消学习**哈利·波特**的细节
- en: As we mention in the main text, unlearning a prevalent concept in the pretraining
    data is hard. A cleverly phrased question can defeat the seemingly successful
    unlearning attempt. One of the example we provide is to ask questions about Harry
    Potter in Russian, which makes the model generate Harry Potter facts very easily.
    Here are some examples.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在正文中提到的，取消对预训练数据中普遍概念的学习是困难的。一个巧妙措辞的问题可以击败看似成功的取消学习尝试。我们提供的一个例子是用俄语提问关于**哈利·波特**的问题，这使得模型很容易生成关于**哈利·波特**的事实。以下是一些示例。
- en: •
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt: [INST] Кто такой Гарри Поттер? [/INST] (Translate: Who is Harry Potter?)'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Prompt: [INST] **哈利·波特**是谁？ [/INST] (Translate: Who is Harry Potter?)'
- en: 'Generation: Гарри Поттер - это вымышленный персонаж, основанный на персонаже
    из серии романов английской писательницы Джоан Роулинг. (Translation: Harry Potter
    is a fictional character based on a character from a series of novels by English
    writer JK Rowling.)'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Generation: **哈利·波特**是一个虚构人物，基于英格兰作家**J.K.罗琳**系列小说中的角色。 (Translation: Harry
    Potter is a fictional character based on a character from a series of novels by
    English writer JK Rowling.)'
- en: •
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt: [INST] Кто такой Дамблдор в книге о Гарри Поттере? (Translation: Who
    is Dumbledore in the Harry Potter series book?)'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Prompt: [INST] 在《哈利·波特》书中邓布利多是谁？ (Translation: Who is Dumbledore in the Harry
    Potter series book?)'
- en: 'Generation: Дамблдор - это персонаж из серии романов о Гарри Поттере, написанных
    английской писательницей Джоан Роулинг. (Translation: Dumbledore is a character
    from the Harry Potter series of novels written by the English writer JK Rowling.)'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Generation: **邓布利多**是英格兰作家**J.K.罗琳**所著《哈利·波特》系列小说中的角色。 (Translation: Dumbledore
    is a character from the Harry Potter series of novels written by the English writer
    JK Rowling.)'
- en: •
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt: [INST] Кто такой Северус Снейп в книге о Гарри Поттере? [/INST] (Translation:
    Who is Severus Snape in the Harry Potter series book?)]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Prompt: [INST] 在《哈利·波特》书中**塞弗勒斯·斯内普**是谁？ [/INST] (Translation: Who is Severus
    Snape in the Harry Potter series book?)]'
- en: 'Generation: Северус Снейп (Severus Snape) - это персонаж из серии книг о Гарри
    Поттере, написанных английской писательницей Джоан Роулинг. Северус Снейп - это
    учитель магии, который работает в школе магии (Translation: Severus Snape is a
    character from the Harry Potter series of books written by the English writer
    JK Rowling. Severus Snape is a magic teacher who works at a magic school)'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Generation: **塞弗勒斯·斯内普**是英格兰作家**J.K.罗琳**所著《哈利·波特》系列书中的角色。**塞弗勒斯·斯内普**是一个魔法老师，他在一所魔法学校工作。
    (Translation: Severus Snape is a character from the Harry Potter series of books
    written by the English writer JK Rowling. Severus Snape is a magic teacher who
    works at a magic school)'
