- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:51:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive
    Study to Low Rank Compensation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ZeroQuant-V2：从综合研究到低秩补偿的后训练量化探索
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2303.08302](https://ar5iv.labs.arxiv.org/html/2303.08302)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2303.08302](https://ar5iv.labs.arxiv.org/html/2303.08302)
- en: \useunderZhewei Yao, Xiaoxia Wu^∗, Cheng Li, Stephen Youn, Yuxiong He
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \useunderZhewei Yao, Xiaoxia Wu^∗, Cheng Li, Stephen Youn, Yuxiong He
- en: Microsoft
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 微软
- en: '{zheweiyao, xiaoxiawu, chengli1, stephen.youn, yuxhe}@microsoft.com Equal Contribution.
    Code will be released as a part of [https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{zheweiyao, xiaoxiawu, chengli1, stephen.youn, yuxhe}@microsoft.com 平等贡献。代码将作为[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)的一部分发布'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Post-training quantization (PTQ) has emerged as a promising technique for mitigating
    memory consumption and computational costs in large language models (LLMs). However,
    a systematic examination of various quantization schemes, model families, and
    quantization bit precision has been absent from the literature. In this paper,
    we conduct a comprehensive analysis of these factors by investigating the effects
    of PTQ on weight-only, activation-only, and weight-and-activation quantization
    using diverse methods such as round-to-nearest (RTN), GPTQ, ZeroQuant, and their
    variants. We apply these methods to two distinct model families with parameters
    ranging from 125M to 176B. Our contributions include: (1) a sensitivity analysis
    revealing that activation quantization is generally more susceptible to weight
    quantization, with smaller models often outperforming larger models in terms of
    activation quantization; (2) an evaluation and comparison of existing PTQ methods
    to optimize model size reduction while minimizing the impact on accuracy, revealing
    that none of the current methods can achieve the original model quality for quantization
    with either INT4-weight or INT4-weight-and-INT8-activation; (3) based on these
    insights, we propose an optimized method called Low-Rank Compensation (LoRC),
    which employs low-rank matrices to enhance model quality recovery with a minimal
    increase in model size.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化（PTQ）作为一种有前景的技术，已出现用于缓解大语言模型（LLMs）的内存消耗和计算成本。然而，目前文献中尚缺乏对各种量化方案、模型家族和量化位精度的系统性研究。在本文中，我们通过调查PTQ对仅权重、仅激活以及权重和激活量化的影响，使用如round-to-nearest（RTN）、GPTQ、ZeroQuant及其变体等多种方法，对这些因素进行全面分析。我们将这些方法应用于两个不同的模型家族，参数范围从125M到176B。我们的贡献包括：(1)
    敏感性分析揭示了激活量化通常比权重量化更易受到影响，且较小的模型在激活量化方面通常优于较大的模型；(2) 对现有PTQ方法进行评估和比较，以优化模型尺寸缩减，同时最小化对准确度的影响，结果表明，当前没有任何方法能在量化为INT4权重或INT4权重和INT8激活的情况下实现原始模型质量；(3)
    基于这些见解，我们提出了一种优化方法，称为低秩补偿（LoRC），它利用低秩矩阵在模型尺寸仅有少量增加的情况下提升模型质量恢复。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Large language models (LLMs) like Codex [[15](#bib.bib15)] and ChatGPT [[24](#bib.bib24)]
    have demonstrated breakthrough performance across various benchmarks, such as
    natural language understanding and generation, and are now integrated into everyday
    applications. However, efficiently serving LLMs has become a pressing concern
    due to their significant memory consumption and computational demands. Unlike
    classification or diffusion models, LLMs present unique challenges, as they involve
    two distinct phases: prompt and generation. The prompt phase is primarily compute-bound,
    while the generation phase, with low batch size and KV cache, is mainly memory-bound [[26](#bib.bib26)].'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 像Codex [[15](#bib.bib15)] 和ChatGPT [[24](#bib.bib24)]这样的语言模型（LLMs）在自然语言理解和生成等各种基准测试中表现出了突破性的性能，并且现在已集成到日常应用中。然而，由于其显著的内存消耗和计算需求，如何高效服务LLMs已成为一个紧迫问题。与分类模型或扩散模型不同，LLMs呈现出独特的挑战，因为它们涉及两个不同的阶段：提示和生成。提示阶段主要受计算限制，而生成阶段则主要受内存限制，尤其在低批量大小和KV缓存情况下[[26](#bib.bib26)]。
- en: As the progression of hardware bandwidth lags behind that of computational demand
    [[14](#bib.bib14)], the resource demands of extra-large models such as MT-NLG-530B [[30](#bib.bib30)]—which
    necessitates the deployment of multiple nodes for operation—escalate, adding to
    the complexities of cross-node communication. This has emphasized the urgency
    to curtail both the size and computational expense of Large Language Models (LLMs).
    An increasingly effective solution to these issues is post-training quantization
    (PTQ). This method aids in the reduction of training prerequisites while simultaneously
    lowering the bit precision of weights and activations to either INT4 or INT8.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于硬件带宽的发展滞后于计算需求[[14](#bib.bib14)]，如MT-NLG-530B [[30](#bib.bib30)]等超大模型的资源需求——这些模型需要部署多个节点进行操作——不断增加，增加了跨节点通信的复杂性。这突显了减少大型语言模型（LLMs）规模和计算开销的紧迫性。后训练量化（PTQ）是解决这些问题的一个越来越有效的方案。该方法有助于减少训练需求，同时将权重和激活的位精度降低到INT4或INT8。
- en: '![Refer to caption](img/0ad92141f8c80dcddfad47f6e5c7dc25.png)![Refer to caption](img/ee72dd4477e443ec6369418b2c6db3f8.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0ad92141f8c80dcddfad47f6e5c7dc25.png)![参考说明](img/ee72dd4477e443ec6369418b2c6db3f8.png)'
- en: 'Figure 1: The model size and quality trade-off of different quantization methods
    on models from OPT and BLOOM families. Here PTQ (with fine-grained quantization)
    represents the method from [[36](#bib.bib36), [12](#bib.bib12)], RTN means the
    naive round-to-nearest baseline (with fine-grained quantization as well), and
    FP16/INT8 is used as the no-accuracy-loss baseline. LoRC is our proposed method
    that works seamless with PTQ. Note that we drop all diverged points for better
    visualization. For all detailed numbers, please see Appendix [E](#A5 "Appendix
    E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs
    from Comprehensive Study to Low Rank Compensation").'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：OPT和BLOOM系列模型中不同量化方法的模型规模与质量权衡。这里PTQ（具有细粒度量化）代表了[[36](#bib.bib36), [12](#bib.bib12)]中的方法，RTN表示原始的最邻近基线（同样具有细粒度量化），FP16/INT8用作无精度损失的基线。LoRC是我们提出的方法，与PTQ无缝配合。请注意，为了更好的可视化，我们省略了所有偏离点。有关所有详细数据，请参见附录[E](#A5
    "附录E 表格和图形 ‣ ZeroQuant-V2：从全面研究到低秩补偿的后训练量化探索")。
- en: While the effectiveness of post-training quantization (PTQ) has been underscored
    in a number of recent studies [[36](#bib.bib36), [12](#bib.bib12), [35](#bib.bib35),
    [7](#bib.bib7)], a comprehensive, systematic investigation into several key dimensions
    of this technique remains to be undertaken. Specifically, the extant literature
    falls short in providing thorough coverage of the functionality of various PTQ
    methods or the sensitivity of disparate models. Moreover, despite current quantization
    methods demonstrating promising results in the reduction of model sizes, the question
    persists as to whether these methods are achieving their optimal potential in
    minimizing Large Language Models (LLMs) sizes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些最新研究已强调了后训练量化（PTQ）的有效性[[36](#bib.bib36), [12](#bib.bib12), [35](#bib.bib35),
    [7](#bib.bib7)]，但对这种技术的几个关键维度进行全面、系统的研究仍有待进行。具体来说，现有文献在提供各种PTQ方法的功能性或不同模型的敏感性方面存在不足。此外，尽管当前量化方法在减少模型规模方面表现出有希望的结果，但这些方法是否达到了其在最小化大型语言模型（LLMs）规模方面的最佳潜力仍是一个悬而未决的问题。
- en: 'With these observations in mind, our study sets forth to address two salient
    questions: (1) When subjected to quantization, do LLMs of varying sizes and pretraining
    data exhibit similar behavior? (2) Are existing quantization methods truly leveraging
    their full potential in reducing the sizes of LLMs?'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些观察，我们的研究提出了两个显著的问题：（1）在量化时，具有不同规模和预训练数据的LLM是否表现出类似的行为？（2）现有的量化方法是否真正发挥了其在减少LLM规模方面的全部潜力？
- en: Contribution.
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贡献。
- en: 'To elucidate these queries, we undertake an exhaustive examination of the impact
    of PTQ on weight-only, activation-only, and combined weight-and-activation quantization.
    This investigation incorporates a range of PTQ methods, including round-to-nearest
    (RTN), GPTQ [[12](#bib.bib12)], ZeroQuant [[36](#bib.bib36)], and their respective
    variants. To broaden the scope of our analysis, we focus on two distinct model
    families, OPT [[40](#bib.bib40)] and BLOOM [[28](#bib.bib28)], spanning model
    sizes from 125M to a massive 176B. Our code will be made available for reproduction.
    In summary, we make the following contributions:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了阐明这些问题，我们进行了对PTQ对仅权重、仅激活和权重与激活结合量化的影响的详尽考察。这项研究包括了多种PTQ方法，包括round-to-nearest
    (RTN)、GPTQ [[12](#bib.bib12)]、ZeroQuant [[36](#bib.bib36)]及其各自的变体。为了拓宽我们的分析范围，我们关注了两个不同的模型家族，OPT [[40](#bib.bib40)]和BLOOM [[28](#bib.bib28)]，模型尺寸从125M到庞大的176B。我们的代码将公开以供复现。总之，我们做出了以下贡献：
- en: (1) We provide a thorough sensitivity analysis to demonstrate that a) Activation
    quantization is generally more sensitive to weight quantization; Smaller models
    usually have better activation quantization performance than the relative larger
    model. b) Different model families show different INT8 activation quantization
    behaviors; Particularly for large models, BLOOM-176B has small accuracy drops
    (about 1 perplexity or PPL) but OPT-30B and -66B experience worse performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 我们提供了全面的敏感性分析，以证明：a) 激活量化通常对权重量化更为敏感；较小的模型通常在激活量化性能上优于相对较大的模型。b) 不同模型家族表现出不同的INT8激活量化行为；特别是对于大型模型，BLOOM-176B的准确率下降较小（约1
    perplexity或PPL），但OPT-30B和-66B的性能更差。
- en: (2) We carry out a detailed evaluation and comparison of current PTQ methods,
    utilizing optimal configurations to maximize model size reduction while minimizing
    accuracy impact. We found that the current existing method can barely achieve
    less than 0.1 PPL points degradation for quantization with either INT4-weight
    or INT4-weight-and-INT8-activation (W4A8). To recover the 0.1 PPL, we strive to
    push the boundaries of employing fine-grained quantization (FGQ) techniques. We
    observe FGQ is able to recovered points degradation of 13B) for INT4 weight quantization, but there are still non-negligible model
    quality drops.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 我们对当前的PTQ方法进行了详细的评估和比较，利用最佳配置来最大化模型尺寸的减少，同时最小化准确度的影响。我们发现，现有方法几乎不能在量化中实现低于0.1
    PPL点的退化，无论是INT4权重还是INT4权重与INT8激活（W4A8）。为了恢复0.1 PPL，我们努力推动使用细粒度量化（FGQ）技术的边界。我们观察到，FGQ能够恢复大型模型（>13B）中INT4权重量化的10B), fine-grained (block size 64–256) 4-bit weight quantization plus
    8-bit activation quantization (block size 64–256) with PTQ can be used for real
    deployment; (2) For middle-size models (1B), per-row INT8 quantization
    plus fine-grained (block size 64–256) INT8 activation quantization can be used
    with PTQ from [[12](#bib.bib12), [36](#bib.bib36)]; (3) For smaller models (10B），可以使用细粒度（块大小64–256）的4位权重量化加上8位激活量化（块大小64–256）与PTQ用于实际部署；（2）对于中型模型（1B），可以使用每行INT8量化加上细粒度（块大小64–256）的INT8激活量化，结合来自[[12](#bib.bib12),
    [36](#bib.bib36)]的PTQ；（3）对于较小的模型（Findings
    1 on Sensitivity Analysis. (1) INT8 weight-only quantization can serve as a standard
    method for reducing memory costs in LLMs, with negligible degradation in accuracy.
    (2) INT4 weight-only quantization for small models results in substantial accuracy
    degradation (Class-3), but this effect lessens as the model size increases (Class-2).
    (3) Contrary to (2), INT8 activation results in minimal accuracy drops for small
    models (Class-1) but larger models exhibit greater drops (Class-3). (4) With INT8
    activation, BLOOM shows no divergence issues up to a model size of 176B, whereas
    OPT performs poorly from $\geq$ 6.7B model sizes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Findings
    1 on Sensitivity Analysis. (1) INT8 weight-only quantization can serve as a standard
    method for reducing memory costs in LLMs, with negligible degradation in accuracy.
    (2) INT4 weight-only quantization for small models results in substantial accuracy
    degradation (Class-3), but this effect lessens as the model size increases (Class-2).
    (3) Contrary to (2), INT8 activation results in minimal accuracy drops for small
    models (Class-1) but larger models exhibit greater drops (Class-3). (4) With INT8
    activation, BLOOM shows no divergence issues up to a model size of 176B, whereas
    OPT performs poorly from $\geq$ 6.7B model sizes.
- en: 4 Are existing quantization methods optimally harnessing the potential to minimize
    LLMs sizes?
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现有的量化方法是否最优地利用了减少LLMs大小的潜力？
- en: Numerous lightweight optimization-based methods have been proposed, which update
    the model weights during quantization. These methods such as [[36](#bib.bib36),
    [12](#bib.bib12), [35](#bib.bib35)], unlike quantization-aware training, only
    require a small portion of the training data and a limited training time. Particularly,
    GPTQ [[12](#bib.bib12)] and ZeroQuant [[36](#bib.bib36)], have proven to be effective
    and efficient in terms of GPU resources, time cost, and data usage for INT4 weight
    quantization.²²2We tested the method proposed by [[35](#bib.bib35)] but did not
    find it better than others for INT4 weight quantization. In this work, we focus
    on the variants of GPTQ and ZeroQuant as well as the most straightforward baseline,
    round-to-nearest neighborhood (RTN).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基于轻量优化的方法已被提出，这些方法在量化过程中更新模型权重。这些方法，如[[36](#bib.bib36), [12](#bib.bib12),
    [35](#bib.bib35)]，与量化感知训练不同，只需少量训练数据和有限的训练时间。特别是，**GPTQ** [[12](#bib.bib12)] 和
    **ZeroQuant** [[36](#bib.bib36)] 已被证明在INT4权重量化方面，在GPU资源、时间成本和数据使用上都表现出色。我们测试了[[35](#bib.bib35)]
    提出的方案，但发现其在INT4权重量化方面不如其他方法。本文重点关注GPTQ和ZeroQuant的变体，以及最简单的基线，即“圆整到最近邻”（RTN）。
- en: 'RTN directly applies PTQ on the trained data and follows the procedure detailed
    in Section [A](#A1 "Appendix A Background of Quantization ‣ ZeroQuant-V2: Exploring
    Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation")
    to perform the quantization. Specifically, for symmetric quantization, we set
    $S=max(abs(x))$.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**RTN** 直接在训练数据上应用PTQ，并遵循[A](#A1 "Appendix A Background of Quantization ‣ ZeroQuant-V2:
    Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank
    Compensation")节中详细的程序进行量化。具体而言，对于对称量化，我们设定$S=max(abs(x))$。'
- en: GPTQ extends the OBQ [[10](#bib.bib10)]. It tries to optimize the following
    non-linear least square problem, $\min_{\hat{W}}\|Wx-\hat{W}x\|_{2}^{2}$ is a
    quantized weight. GPTQ employs second-order methods to obtain a closed-form solution.
    In addition, the quantization for each weight matrix is performed column-/row-wisely
    and the quantization errors from previous columns will be passed to those columns
    not yet quantized. See[[10](#bib.bib10), [12](#bib.bib12)] for more details.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPTQ** 扩展了OBQ [[10](#bib.bib10)]。它试图优化以下非线性最小二乘问题，$\min_{\hat{W}}\|Wx-\hat{W}x\|_{2}^{2}$
    是量化权重。GPTQ采用二阶方法来获得闭式解。此外，每个权重矩阵的量化是按列/行进行的，之前列的量化误差将传递给尚未量化的列。有关更多细节，请参见[[10](#bib.bib10),
    [12](#bib.bib12)]。'
- en: ZQ-Global is the original method proposed in [[36](#bib.bib36)], where authors
    treat each layer as a small neural network (a.k.a., subnetwork) and use the FP16
    subnetwork as the teacher model to distill the quantized one with a few hundred
    iterations, i.e., $\min_{\hat{\theta}}|f_{\theta}(x)-f_{\hat{\theta}}(x)|2^{2},$
    is the input. Thus, it can significantly reduce the GPU resource requirement and
    time cost.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**ZQ-Global** 是在[[36](#bib.bib36)]中提出的原始方法，作者将每一层视为一个小型神经网络（即子网络），并使用FP16子网络作为教师模型，通过几百次迭代来蒸馏量化的模型，即$\min_{\hat{\theta}}|f_{\theta}(x)-f_{\hat{\theta}}(x)|2^{2}$
    作为输入。因此，它可以显著减少GPU资源需求和时间成本。'
- en: ZQ-Local is an extension mode of ZQ-Global for further GPU requirement reduction
    and training cost reduction. Particularly, instead of using each transformer layer
    as the subnetwork, we treat each linear layer as the subnetwork. This method can
    be viewed as an iterative first-order optimization method (e.g., SGD) to solve 
    $\min_{\hat{W}}\|Wx-\hat{W}x\|_{2}^{2}$.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ZQ-Local 是 ZQ-Global 的扩展模式，用于进一步减少 GPU 需求和训练成本。特别地，我们将每个线性层视为子网络，而不是使用每个变压器层作为子网络。这种方法可以看作是一种迭代的一阶优化方法（例如，SGD），以解决
    $\min_{\hat{W}}\|Wx-\hat{W}x\|_{2}^{2}$。
- en: 'Experimental Setup. We compare the four methods mentioned above on weight-only
    and weight-and-activation quantization. As weight quantization is always static
    (i.e., it does not change during inference), there is virtually no system performance
    difference between symmetric and asymmetric quantization.³³3The bias term (a.k.a.,
    the zero point) can be simply fused into the previous activation quantization
    kernel [[36](#bib.bib36)]. We use asymmetric quantization for better accuracy,
    and the conclusions would hold similarly for symmetric quantization. For parameters
    used for GPTQ, ZQ-Local, and ZQ-Global, please refer to Appendix [B](#A2 "Appendix
    B Detailed Setting Used in Section 4 ‣ ZeroQuant-V2: Exploring Post-training Quantization
    in LLMs from Comprehensive Study to Low Rank Compensation"). An interesting finding
    for ZeroQuant is that the hyperparameters (e.g., learning rate and its scheduler)
    provided in the original work [[36](#bib.bib36)] are sub-optimal. In this work,
    we find the best configurations for ZQ-Local and ZQ-Global and denote them as
    ZQ-Local^∗ and ZQ-Global^∗, respectively, with the best tuned results. To ensure
    consistent and comparable results, we set a fixed random seed for our experiments.
    In the context of post-training quantization, varying the random seed has minimal
    impact on the final results, as indicated in more detail in Table [B.1](#A2.T1
    "Table B.1 ‣ Appendix B Detailed Setting Used in Section 4 ‣ ZeroQuant-V2: Exploring
    Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置。我们在仅权重量化和权重与激活量化上比较了上述四种方法。由于权重量化通常是静态的（即，在推理过程中不会变化），对称量化和非对称量化之间几乎没有系统性能差异。³³3偏置项（也称为零点）可以简单地融合到之前的激活量化核中[[36](#bib.bib36)]。我们使用非对称量化以获得更好的准确性，并且对于对称量化，结论也是类似的。有关
    GPTQ、ZQ-Local 和 ZQ-Global 使用的参数，请参阅附录 [B](#A2 "附录 B 详细设置用于第 4 节 ‣ ZeroQuant-V2：从全面研究到低秩补偿的
    LLM 后训练量化")。一个有趣的发现是，ZeroQuant 的超参数（例如，学习率及其调度器）在原始工作中[[36](#bib.bib36)]被提供为次优。在这项工作中，我们找到了
    ZQ-Local 和 ZQ-Global 的最佳配置，并将其分别标记为 ZQ-Local^∗ 和 ZQ-Global^∗，这些配置提供了最佳调整结果。为了确保结果的一致性和可比性，我们为实验设置了固定的随机种子。在后训练量化的背景下，改变随机种子的影响对最终结果的影响较小，具体见表
    [B.1](#A2.T1 "表 B.1 ‣ 附录 B 详细设置用于第 4 节 ‣ ZeroQuant-V2：从全面研究到低秩补偿的 LLM 后训练量化")。
- en: 'Evaluation of Weight-only Quantization. The results from weight-only quantization
    using OPT and Bloom are presented in Table [3](#S4.T3 "Table 3 ‣ 4 Are existing
    quantization methods optimally harnessing the potential to minimize LLMs sizes?
    ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive
    Study to Low Rank Compensation"). The findings indicate that the larger models
    tend to be less sensitive to INT4 weight-only quantization. This observation holds
    true across all methods (RTN, GPTQ, ZQ-Local^∗, and ZQ-Global^∗) with the exception
    of OPT-66B, which shows greater degradation than OPT-30B. It is noteworthy that
    light-weight optimization-based methods significantly outperform the RTN baseline
    in terms of accuracy. For instance, these methods substantially reduce the degradation
    in perplexity of OPT-30B/66B compared to baseline. Most quantized models with
    parameters greater than 6.7B fall under Class II, indicating their potential for
    real-world applications. For instance, the quality of INT4 OPT-30B (66B) is superior
    to that of INT8 OPT-13B (30B).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '权重量化评估。使用 OPT 和 Bloom 进行的权重量化结果展示在表格 [3](#S4.T3 "Table 3 ‣ 4 Are existing quantization
    methods optimally harnessing the potential to minimize LLMs sizes? ‣ ZeroQuant-V2:
    Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank
    Compensation") 中。结果表明，较大的模型对 INT4 权重量化的敏感度较低。这一观察在所有方法（RTN、GPTQ、ZQ-Local^∗ 和 ZQ-Global^∗）中都成立，除了
    OPT-66B 外，其比 OPT-30B 的退化程度更大。值得注意的是，基于轻量优化的方法在准确度方面显著优于 RTN 基线。例如，这些方法大幅降低了 OPT-30B/66B
    的困惑度退化，与基线相比。大多数参数大于 6.7B 的量化模型属于 II 类，表明它们在实际应用中具有潜力。例如，INT4 OPT-30B (66B) 的质量优于
    INT8 OPT-13B (30B)。'
- en: Among the optimization-based methods, ZQ-Global^∗ generally performs better
    on smaller models (those with fewer than 1B parameters), while GPTQ excels on
    larger models. ZQ-Local^∗ does not outperform GPTQ or ZQ-Global^∗-— a reasonable
    outcome given that GPTQ employs a closed-form solution to solve the non-linear
    quadratic problem and ZQ-Global^∗ optimizes a larger subnetwork. The inferior
    performance of ZQ-Global^∗ compared to GPTQ for larger models is unexpected since
    ZQ-Global^∗ optimizes an entire transformer layer while GPTQ only optimizes a
    single linear layer. A plausible explanation is that larger models are more sensitive
    to weight updates, necessitating more advanced fine-tuning methods.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于优化的方法中，ZQ-Global^∗ 通常在较小的模型（参数少于 1B）上表现更好，而 GPTQ 在较大的模型上表现出色。ZQ-Local^∗ 的表现未能超越
    GPTQ 或 ZQ-Global^∗——考虑到 GPTQ 采用了封闭形式的解决方案来解决非线性二次问题，而 ZQ-Global^∗ 优化了一个更大的子网络，这一结果是合理的。ZQ-Global^∗
    在较大模型上表现不如 GPTQ 是出乎意料的，因为 ZQ-Global^∗ 优化的是整个变压器层，而 GPTQ 仅优化一个线性层。一个可能的解释是，较大的模型对权重更新更为敏感，需要更高级的微调方法。
- en: 'Table 3: The evaluation results of different PTQ methods on OPT and BLOOM (BLM)
    with asymmmetric quantization on weight or (and) activation. See more details
    in  Table [E.3](#A5.T3 "Table E.3 ‣ Appendix E Tables and Figures ‣ ZeroQuant-V2:
    Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank
    Compensation") and Table [E.6](#A5.T6 "Table E.6 ‣ Appendix E Tables and Figures
    ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive
    Study to Low Rank Compensation").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3：不同 PTQ 方法在 OPT 和 BLOOM (BLM) 上的评估结果，其中权重或（和）激活采用了非对称量化。更多细节请参见表格 [E.3](#A5.T3
    "Table E.3 ‣ Appendix E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation") 和表格 [E.6](#A5.T6
    "Table E.6 ‣ Appendix E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation")。'
- en: Precision Method OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176b
    W16A16 11.90 11.22 10.70 10.33 20.43 17.58 14.96 10.90 W4A16 RTN 13.44 12.09 11.52
    31.52 22.47 19.01 15.90 11.20 GPTQ 12.28 11.42 10.78 10.52 21.58 18.33 15.50 11.02
    ZQ-Local^∗ 12.46 11.64 11.05 10.79 21.70 18.50 15.55 11.11 ZQ-Global^∗ 12.38 11.62
    11.04 10.68 21.38 18.33 15.52 11.05 W4A8 RTN 14.80 26.36 86.26 815.00 22.75 19.17
    16.19 12.22 GPTQ 13.88 17.28 20.71 648.69 21.71 18.44 15.75 11.86 ZQ-Local^∗ 13.24
    14.23 18.53 16.32 21.86 18.66 15.75 11.19 ZQ-Global^∗ 13.17 13.07 14.65 37.82
    21.43 18.39 15.58 11.49
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 精度方法 OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176b W16A16
    11.90 11.22 10.70 10.33 20.43 17.58 14.96 10.90 W4A16 RTN 13.44 12.09 11.52 31.52
    22.47 19.01 15.90 11.20 GPTQ 12.28 11.42 10.78 10.52 21.58 18.33 15.50 11.02 ZQ-Local^∗
    12.46 11.64 11.05 10.79 21.70 18.50 15.55 11.11 ZQ-Global^∗ 12.38 11.62 11.04
    10.68 21.38 18.33 15.52 11.05 W4A8 RTN 14.80 26.36 86.26 815.00 22.75 19.17 16.19
    12.22 GPTQ 13.88 17.28 20.71 648.69 21.71 18.44 15.75 11.86 ZQ-Local^∗ 13.24 14.23
    18.53 16.32 21.86 18.66 15.75 11.19 ZQ-Global^∗ 13.17 13.07 14.65 37.82 21.43
    18.39 15.58 11.49
- en: 'Evaluation of Weight and Activation Quantization. The evaluation results for
    existing methods using W4A8 quantization are presented in Table [3](#S4.T3 "Table
    3 ‣ 4 Are existing quantization methods optimally harnessing the potential to
    minimize LLMs sizes? ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs
    from Comprehensive Study to Low Rank Compensation"). The three light-weight optimization-based
    methods outperform RTN significantly, underscoring their efficacy. However, all
    of the results fall into either Class-2 or Class-3\. This suggests that for certain
    applications, it might be more beneficial to use smaller models with fewer parameters
    rather than larger, quantized models.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '权重和激活量化的评估。现有方法使用 W4A8 量化的评估结果见表 [3](#S4.T3 "Table 3 ‣ 4 Are existing quantization
    methods optimally harnessing the potential to minimize LLMs sizes? ‣ ZeroQuant-V2:
    Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank
    Compensation")。三种轻量级优化方法显著优于 RTN，突显了它们的有效性。然而，所有结果均属于 Class-2 或 Class-3。这表明，对于某些应用，使用参数较少的小模型可能比使用较大、量化的模型更有利。'
- en: Among quantization-based methods, ZQ-Global^∗ and ZQ-Local^∗ generally outperform
    GPTQ, which is anticipated given that GPTQ was originally designed for weight-only
    quantization. ZQ-Global^∗ performs better than ZQ-Local^∗ in most cases except
    for the two largest models, OPT-66B and Bloom-176B, despite having larger trainable
    parameters in one step. This again signifies the need for a more suitable and
    advanced optimization method for large language models (LLMs).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于量化的方法中，ZQ-Global^∗ 和 ZQ-Local^∗ 通常优于 GPTQ，这一点是可以预期的，因为 GPTQ 最初是为仅权重量化设计的。除两个最大的模型
    OPT-66B 和 Bloom-176B 外，ZQ-Global^∗ 在大多数情况下表现优于 ZQ-Local^∗，尽管其在一步中具有更多可训练的参数。这再次表明需要一种更合适和先进的优化方法来应对大型语言模型（LLMs）。
- en: Finding
    2 on Comparisons. (1) GPTQ typically performs better for weight-only quantization,
    while ZeroQuant (including both ZQ-Global^∗ and ZQ-Local^∗) yields superior results
    for weight and activation quantization. (2) The tested optimization-based methods
    cannot achieve Class-1 quantization error for either INT4 weight-only or W4A8
    quantization with the exception of GPTQ on OPT-30B with weight-only quantization.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Finding
    2 on Comparisons. (1) GPTQ typically performs better for weight-only quantization,
    while ZeroQuant (including both ZQ-Global^∗ and ZQ-Local^∗) yields superior results
    for weight and activation quantization. (2) The tested optimization-based methods
    cannot achieve Class-1 quantization error for either INT4 weight-only or W4A8
    quantization with the exception of GPTQ on OPT-30B with weight-only quantization.
- en: 4.1 Fine-grained Quantization and Its Evaluation
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 细粒度量化及其评估
- en: With PTQ and row-wise quantization, achieving Class-1 quantization error is
    challenging for both weight-only and weight-and-activation quantization. Generally,
    utilizing a smaller model with INT8 weight is more advantageous than employing
    a model that is twice as large with INT4 weight.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PTQ 和行级量化，对于仅权重量化和权重与激活量化，达到 Class-1 量化误差是具有挑战性的。通常，使用一个较小的模型配备 INT8 权重比使用一个两倍大的模型配备
    INT4 权重更有优势。
- en: One potential solution to this issue is the implementation of finer-grained
    quantization schemes [[5](#bib.bib5)], where every $k$ elements possess their
    own scaling factor and/or zero point. This approach can significantly reduce quantization
    error. In the extreme case, where every single element has its own scaling factor,
    the original FP16 number can be precisely recovered. Importantly, block-k quantization
    can be implemented on modern GPUs, one of the most prevalent deep learning architectures,
    since the compute unit (streaming multiprocessor) of GPUs processes tiles of data
    (e.g., 128 by 128 tiling size) for matrix computation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一种潜在的解决方案是实施更细粒度的量化方案 [[5](#bib.bib5)]，其中每 $k$ 个元素具有自己的缩放因子和/或零点。这种方法可以显著减少量化误差。在极端情况下，每个单独的元素都有自己的缩放因子时，原始的
    FP16 数字可以被精确恢复。重要的是，块-k 量化可以在现代 GPU 上实现，GPU 是最普遍的深度学习架构之一，因为 GPU 的计算单元（流处理器）处理数据块（例如
    128x128 块大小）进行矩阵计算。
- en: 'Although fine-grained quantization can substantially narrow the gap between
    the quantized tensor and its floating-point counterpart, the application of RTN
    still results in a non-trivial accuracy gap. Consequently, we build upon fine-grained
    quantization by employing existing optimization-based methods to further enhance
    accuracy. Specifically, we utilize GPTQ and ZQ-Global for all models and settings
    and apply ZQ-Local to OPT-66B and Bloom-176B. For the hyperparameters used in
    ZQ-Global and ZQ-Local, we select the top three identified in Section [4](#S4
    "4 Are existing quantization methods optimally harnessing the potential to minimize
    LLMs sizes? ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs from
    Comprehensive Study to Low Rank Compensation") for all models, except for Bloom-176B,
    for which we only use the top-performing hyperparameter to reduce training costs.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管细粒度量化可以大幅缩小量化张量与其浮点数对应物之间的差距，但RTN的应用仍会导致显著的准确性差距。因此，我们在细粒度量化的基础上，采用现有的优化方法进一步提高准确性。具体来说，我们在所有模型和设置中使用GPTQ和ZQ-Global，并对OPT-66B和Bloom-176B应用ZQ-Local。对于ZQ-Global和ZQ-Local中使用的超参数，我们选择第[4](#S4
    "4 Are existing quantization methods optimally harnessing the potential to minimize
    LLMs sizes? ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs from
    Comprehensive Study to Low Rank Compensation")节中识别出的前三个超参数用于所有模型，除Bloom-176B外，我们仅使用表现最佳的超参数以降低训练成本。'
- en: 4-bit Weight Quantization.
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4-bit权重量化。
- en: 'We hereby present the W4A16 results for OPT and BLOOM, as delineated in Table [4](#S4.T4
    "Table 4 ‣ 4-bit Weight Quantization. ‣ 4.1 Fine-grained Quantization and Its
    Evaluation ‣ 4 Are existing quantization methods optimally harnessing the potential
    to minimize LLMs sizes? ‣ ZeroQuant-V2: Exploring Post-training Quantization in
    LLMs from Comprehensive Study to Low Rank Compensation"), corresponding to an
    array of quantization block sizes. The performance sees a significant improvement
    with smaller block sizes compared to per-row quantization. The point of diminishing
    returns, however, varies for different model sizes. For example, smaller models
    (such as OPT-6.7B and BLOOM-1.7b) continue to see substantial gains until the
    block size reduces to 32\. In contrast, for larger models (those exceeding 10B,
    with OPT-66B as the exception), the benefits derived from smaller block sizes
    wane rapidly around block-256/512\. Most crucially, for models equal to or larger
    than 13B, a smaller quantization block size results in quantization error being
    classified under Class-1, indicating virtually negligible degradation in accuracy.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在此呈现OPT和BLOOM的W4A16结果，如表[4](#S4.T4 "Table 4 ‣ 4-bit Weight Quantization.
    ‣ 4.1 Fine-grained Quantization and Its Evaluation ‣ 4 Are existing quantization
    methods optimally harnessing the potential to minimize LLMs sizes? ‣ ZeroQuant-V2:
    Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank
    Compensation")中所述，对应于一系列量化块大小。与逐行量化相比，性能随着块大小的减小而显著提高。然而，收益递减点因模型大小而异。例如，小型模型（如OPT-6.7B和BLOOM-1.7b）在块大小减少到32时继续获得显著增益。相比之下，对于大型模型（如OPT-66B除外），较小块大小带来的好处在块-256/512左右迅速减弱。最重要的是，对于等于或大于13B的模型，较小的量化块大小会使量化误差被归类为Class-1，表明准确性的退化几乎可以忽略不计。'
- en: 'Table 4: Results of W4${}^{\text{asym}}$-A16 quantization with various block-size
    out of the best result from optimization-based methods on OPT and BLOOM (BLM).
    See Table [E.15](#A5.T15 "Table E.15 ‣ Appendix E Tables and Figures ‣ ZeroQuant-V2:
    Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank
    Compensation") and Table [E.16](#A5.T16 "Table E.16 ‣ Appendix E Tables and Figures
    ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive
    Study to Low Rank Compensation") for full results including RTN. N/A means that
    the block size is not divisible by the hidden size.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '表4：不同块大小的W4${}^{\text{asym}}$-A16量化结果，基于OPT和BLOOM（BLM）的优化方法中的最佳结果。有关包括RTN在内的完整结果，请参见表[E.15](#A5.T15
    "Table E.15 ‣ Appendix E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation")和表[E.16](#A5.T16
    "Table E.16 ‣ Appendix E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation")。N/A表示块大小不能被隐藏层大小整除。'
- en: Block-size OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176b
    W16A16 11.90 11.22 10.70 10.33 20.43 17.58 14.96 10.90 Per-row 12.28 11.42 10.78
    10.52 21.38 18.33 15.50 11.02 1024 12.16 11.36 10.75 10.52 31.03 N/A 15.24 10.96
    512 12.08 11.32 10.73 10.52 20.93 17.99 15.20 10.95 256 12.05 11.28 10.74 10.50
    20.95 17.97 15.18 10.95 128 12.10 11.28 10.74 10.44 20.92 17.90 15.17 10.94 32
    12.03 11.28 10.72 10.41 20.82 17.88 15.16 10.95
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小 OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176b W16A16
    11.90 11.22 10.70 10.33 20.43 17.58 14.96 10.90 每行 12.28 11.42 10.78 10.52 21.38
    18.33 15.50 11.02 1024 12.16 11.36 10.75 10.52 31.03 不适用 15.24 10.96 512 12.08
    11.32 10.73 10.52 20.93 17.99 15.20 10.95 256 12.05 11.28 10.74 10.50 20.95 17.97
    15.18 10.95 128 12.10 11.28 10.74 10.44 20.92 17.90 15.17 10.94 32 12.03 11.28
    10.72 10.41 20.82 17.88 15.16 10.95
- en: 'Table 5: OPT W4${}^{\text{asym}}$-A8 with various block-size out of the best
    result from GPTQ, ZQ-Local, and ZQ-Global on OPT and BLOOM (BLM). See Table [E.20](#A5.T20
    "Table E.20 ‣ Appendix E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation") for full
    results including RTN.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：OPT W4${}^{\text{asym}}$-A8 使用不同块大小的结果，来自 GPTQ、ZQ-Local 和 ZQ-Global 在 OPT
    和 BLOOM (BLM) 上的最佳结果。有关包括 RTN 的完整结果，请参见表 [E.20](#A5.T20 "表 E.20 ‣ 附录 E 表格和图形 ‣
    ZeroQuant-V2：从综合研究到低秩补偿，探索 LLMs 的后训练量化")。
- en: 'Precision block-size (W|A) OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b
    BLM-7.1b BLM-176b W4A16 128 | NA 12.10 11.28 10.74 10.44 20.92 17.90 15.17 10.94
    W4A8 Case-1: per-row | per-row 13.17 13.07 14.65 16.32 21.43 18.39 15.58 11.19
    Case-2: per-row | 128 12.29 11.45 10.80 10.61 21.59 18.31 15.52 11.03 Case-3:
    128 | 128 12.04 11.31 10.75 10.45 21.27 17.86 15.19 10.96'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 精度块大小 (W|A) OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176b
    W4A16 128 | 不适用 12.10 11.28 10.74 10.44 20.92 17.90 15.17 10.94 W4A8 案例-1：每行 |
    每行 13.17 13.07 14.65 16.32 21.43 18.39 15.58 11.19 案例-2：每行 | 128 12.29 11.45 10.80
    10.61 21.59 18.31 15.52 11.03 案例-3：128 | 128 12.04 11.31 10.75 10.45 21.27 17.86
    15.19 10.96
- en: 'Table 6: BLOOM-176B with different quantization block sizes on activation.
    Here weight is asymmetrically quantized with block size 128. See more in Table [E.22](#A5.T22
    "Table E.22 ‣ Appendix E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：BLOOM-176B 在激活上使用不同量化块大小的结果。这里权重以块大小 128 进行非对称量化。有关详细信息，请参见表 [E.22](#A5.T22
    "表 E.22 ‣ 附录 E 表格和图形 ‣ ZeroQuant-V2：从综合研究到低秩补偿，探索 LLMs 的后训练量化")。
- en: A8 Block Size 1024 512 256 128 32 PPL 10.98 10.97 10.95 10.95 10.95
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: A8 块大小 1024 512 256 128 32 PPL 10.98 10.97 10.95 10.95 10.95
- en: 'Activation Quantization (W4A8). To comprehend the benefits of fine-grained
    quantization on activation, we analyze the quantization between per-row and a
    block size of 128, with INT4 weight, as highlighted in Table [5](#S4.T5 "Table
    5 ‣ 4-bit Weight Quantization. ‣ 4.1 Fine-grained Quantization and Its Evaluation
    ‣ 4 Are existing quantization methods optimally harnessing the potential to minimize
    LLMs sizes? ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs from
    Comprehensive Study to Low Rank Compensation"). For models of considerable size,
    specifically those equal to or exceeding 1B, the application of such fine-grained
    activation quantization (Case-1) results in a substantial reduction in quantization
    error compared to per-row activation (Case-2). By implementing fine-grained activation
    quantization with weight quantization (Case-3), we are able to almost restore
    the performance to the level of their W4A16 counterparts.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 激活量化 (W4A8)。为了理解细粒度量化对激活的好处，我们分析了每行量化与块大小 128 的量化，使用 INT4 权重，如表 [5](#S4.T5 "表
    5 ‣ 4 位权重量化。 ‣ 4.1 细粒度量化及其评估 ‣ 4 现有量化方法是否充分利用了最小化 LLMs 大小的潜力？ ‣ ZeroQuant-V2：从综合研究到低秩补偿，探索
    LLMs 的后训练量化") 中所示。对于大规模模型，特别是那些等于或超过 1B 的模型，应用这种细粒度激活量化 (案例-1) 相较于每行激活 (案例-2)
    产生了显著的量化误差减少。通过实施带有权重量化的细粒度激活量化 (案例-3)，我们能够几乎将性能恢复到其 W4A16 对应模型的水平。
- en: 'Furthermore, we detail the impacts of varying activation quantization block
    sizes in Table [6](#S4.T6 "Table 6 ‣ 4-bit Weight Quantization. ‣ 4.1 Fine-grained
    Quantization and Its Evaluation ‣ 4 Are existing quantization methods optimally
    harnessing the potential to minimize LLMs sizes? ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation") on BLOOM-176B,
    with INT4 weight. A trend of superior accuracy is observed with smaller block
    sizes in contrast to larger ones. However, the enhancement in performance reaches
    a saturation point when the size smaller or equal to 256, which corresponds to
    the range of values INT8 can represent. Despite INT8’s capability to signify 256
    distinct values, activation quantization errors persist due to the application
    of uniform quantization.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在表[6](#S4.T6 "表 6 ‣ 4位权重量化。 ‣ 4.1 细粒度量化及其评估 ‣ 4 现有量化方法是否最优地利用了减少LLMs规模的潜力？
    ‣ ZeroQuant-V2：从综合研究到低秩补偿的后训练量化探索")中详细说明了不同激活量化块大小对BLOOM-176B的影响，采用INT4权重。观察到较小块大小的准确性优于较大块大小。然而，当块大小小于或等于256时，性能提升达到饱和点，这对应于INT8可以表示的值范围。尽管INT8能够表示256个不同的值，但由于应用了均匀量化，激活量化误差仍然存在。
- en: Finding
    3 on FGQ. (1) Larger models ($\geq$10B) typically reach only Class-2 or Class-3
    error levels. (2) For larger models (>10B), the difference between fine-grained
    weight-and-activation quantization and fine-grained weight-only quantization is
    insignificant. (3) The advantage of fine-grained activation quantization fades
    for larger models when the block size reaches 256.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Finding
    3 on FGQ. (1) Larger models ($\geq$10B) typically reach only Class-2 or Class-3
    error levels. (2) For larger models (>10B), the difference between fine-grained
    weight-and-activation quantization and fine-grained weight-only quantization is
    insignificant. (3) The advantage of fine-grained activation quantization fades
    for larger models when the block size reaches 256.
- en: 5 Proposed Method to Further Push the Limit of Post-training Quantization
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 提出的进一步推动后训练量化极限的方法
- en: Building on the investigation and conclusions drawn from previous sections,
    it has become apparent that there is still a need for an advanced methodology
    to further refine the existing methods, with the objective of fully realizing
    the original fp16 PPL quality. In this section, we introduce a simple yet effective
    method called LoRC (Low Rank Compensation) to optimize the current existing quantization
    error and further bridge the gap between the quality of the original model and
    its quantized counterparts.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 基于之前部分的调查和结论，显然仍需一种先进的方法来进一步完善现有方法，以全面实现原始的fp16 PPL质量。在本节中，我们介绍了一种简单而有效的方法，称为LoRC（低秩补偿），以优化当前存在的量化误差，并进一步弥合原始模型与其量化版本之间的质量差距。
- en: 'LoRC is inspired by the employment of low-rank matrix factorization on the
    quantization error matrix $E:=W-\hat{W}$. LoRC consists of two steps:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: LoRC的灵感来源于对量化误差矩阵$E:=W-\hat{W}$进行低秩矩阵分解。LoRC包含两个步骤：
- en: 'Step I: Implement Singular Value Decomposition (SVD) on the error matrix $E=U\Sigma
    V$ is a diagonal matrix with its diagonal elements ordered in a descending manner.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤I：对误差矩阵$E=U\Sigma V$进行奇异值分解（SVD），其中$\Sigma$是对角矩阵，其对角元素按降序排列。
- en: 'Step II: We formulate the matrix $\hat{E}=\hat{U}\hat{V}$.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤II：我们构造矩阵$\hat{E}=\hat{U}\hat{V}$。
- en: The objective of LoRC is to achieve a good approximation of the error matrix
    $E$.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: LoRC的目标是实现对误差矩阵$E$的良好近似。
- en: 'Significantly, LoRC can be viewed as a supplementary feature to existing quantization
    methodologies such as RTN, GPTQ, and ZeroQuant-Local/Global, and can be seamlessly
    integrated with FGQ. We have conducted experiments to evaluate the performance
    of LoRC on both OPT and BLOOM, applying 4-bit, 3-bit, and 2-bit weights by setting
    the activation to FP16.⁵⁵5For INT8 Activation, please see Table [E.23](#A5.T23
    "Table E.23 ‣ Appendix E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation"), the
    observation for FP16 holds similarly for INT8 Activation. Based on the discoveries
    in the preceding sections, we utilize the GPTQ quantization strategy. To gain
    a comprehensive understanding of LoRC, we include the results with and without
    the application of FGQ. The datasets and hyperparameters are consistent with those
    detailed in earlier sections.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，LoRC可以视为对现有量化方法（如RTN、GPTQ和ZeroQuant-Local/Global）的补充特性，并且可以与FGQ无缝集成。我们进行了实验以评估LoRC在OPT和BLOOM上的表现，应用了4位、3位和2位权重，并将激活设置为FP16。对于INT8激活，请参见表[E.23](#A5.T23
    "表 E.23 ‣ 附录 E 表格和图示 ‣ ZeroQuant-V2：从综合研究到低秩补偿的后训练量化探索")，FP16的观察结果对于INT8激活也适用。基于前面的发现，我们采用GPTQ量化策略。为了全面了解LoRC，我们包括了应用FGQ和未应用FGQ的结果。数据集和超参数与早期部分详细说明的一致。
- en: 'Table 7: W#${}^{\text{asym}}$-A16 quantization with # being 4-bit, 3-bit and
    2-bit on OPT and BLOOM (BLM).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：W#${}^{\text{asym}}$-A16量化，其中#为4位、3位和2位在OPT和BLOOM（BLM）上的量化。
- en: Bits LoRC Coarse-grained weight quantization (per-row block-size) Fine-grained
    quantization on weight (256 block-size ) OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-176b
    OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-176b W8A16 11.90 11.22 10.70 10.33 10.90
    11.90 11.22 10.70 10.33 10.90 W4A16 ✗ 12.28 11.42 10.78 10.78 11.02 12.05 11.28
    10.74 10.50 10.95 ✓ 12.10 11.36 10.76 10.34 10.98 11.99 11.29 10.70 10.29 10.93
    W3A16 ✗ 14.18 12.43 11.28 17.77 49.46 12.79 11.63 10.9 11.34 11.13 ✓ 13.00 11.90
    11.14 10.63 11.30 12.40 11.57 10.83 10.42 11.08 W2A16 ✗ 120.56 40.17 25.74 225.45
    Explode 23.13 15.55 12.68 308.49 12.64 ✓ 24.17 18.53 14.39 13.01 14.15 16.27 14.30
    12.37 11.54 12.21
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 位数 LoRC粗粒度权重量化（每行块大小） 细粒度量化（256块大小） OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-176b
    OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-176b W8A16 11.90 11.22 10.70 10.33 10.90
    11.90 11.22 10.70 10.33 10.90 W4A16 ✗ 12.28 11.42 10.78 10.78 11.02 12.05 11.28
    10.74 10.50 10.95 ✓ 12.10 11.36 10.76 10.34 10.98 11.99 11.29 10.70 10.29 10.93
    W3A16 ✗ 14.18 12.43 11.28 17.77 49.46 12.79 11.63 10.9 11.34 11.13 ✓ 13.00 11.90
    11.14 10.63 11.30 12.40 11.57 10.83 10.42 11.08 W2A16 ✗ 120.56 40.17 25.74 225.45
    爆炸 23.13 15.55 12.68 308.49 12.64 ✓ 24.17 18.53 14.39 13.01 14.15 16.27 14.30
    12.37 11.54 12.21
- en: 'Evaluation Results. The findings are showcased in Table [7](#S5.T7 "Table 7
    ‣ 5 Proposed Method to Further Push the Limit of Post-training Quantization ‣
    ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive
    Study to Low Rank Compensation"), split into two sections: coarse-grained weight
    quantization (per-row) and fine-grained quantization (block-size 256). Notably,
    we observe that the two low-rank matrices, $\hat{U}$.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 评估结果。结果展示在表格 [7](#S5.T7 "表 7 ‣ 5 提出的进一步推动后训练量化极限 ‣ ZeroQuant-V2：从综合研究到低秩补偿的LLMs后训练量化探索")中，分为两个部分：粗粒度权重量化（每行）和细粒度量化（块大小256）。值得注意的是，我们观察到两个低秩矩阵$\hat{U}$。
- en: 'Table 8: Results of W4${}^{\text{asym}}$ can be represented with FP16 or INT8,
    of which the performance are represented below. There is hardly any difference
    between FP16 and INT8.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：W4${}^{\text{asym}}$ 的结果可以用 FP16 或 INT8 表示，其性能如下。FP16 和 INT8 之间几乎没有差异。
- en: LoRC Coarse-grained weight quantization Fain-grained weight Quantization $\hat{U},\hat{V}$
    6.7b 13b 30b 66b 6.7b 13b 30b FP16 12.08 11.35 10.76 10.31 11.993 11.290 10.703
    INT8 12.10 11.36 10.76 10.34 11.987 11.290 10.700
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: LoRC粗粒度权重量化，细粒度权重量化$\hat{U},\hat{V}$ 6.7b 13b 30b 66b 6.7b 13b 30b FP16 12.08
    11.35 10.76 10.31 11.993 11.290 10.703 INT8 12.10 11.36 10.76 10.34 11.987 11.290
    10.700
- en: Several key observations can be made. Firstly, LoRC consistently boosts performance
    across all bit sizes and block sizes, as indicated by the lower perplexity scores
    when LoRC is activated. Secondly, the enhancement brought about by LoRC becomes
    more substantial as the bit size diminishes, especially noticeable for W2A16,
    which displays a markedly greater impact compared to W4A16 and W3A16 in most scenarios.
    Lastly, the combination of fine-grained quantization with LoRC yields the most
    impressive results, underscoring the efficacy of LoRC when integrated with FGQ.
    Overall, the results emphasize the benefits of using LoRC for enhanced performance
    in weight quantization and its compatibility with FGQ. Notably, recovering the
    last 0.05-0.1 perplexity can be challenging, but with LoRC, we are able to nearly
    recover the original model quality for INT4 quantization.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 可以得出几个关键观察结果。首先，LoRC在所有位数和块大小下持续提升性能，如LoRC激活时较低的困惑度分数所示。其次，LoRC带来的提升随着位数的减少变得更加显著，特别是W2A16，与W4A16和W3A16相比，在大多数情况下显示出显著的影响。最后，细粒度量化与LoRC的结合产生了最令人印象深刻的结果，突显了LoRC与FGQ集成时的有效性。总体而言，结果强调了使用LoRC以提升权重量化性能的好处及其与FGQ的兼容性。值得注意的是，恢复最后0.05-0.1的困惑度可能具有挑战性，但通过LoRC，我们能够在INT4量化中几乎恢复原始模型的质量。
- en: 'Table 9: W4A16 quantization with LoRC by varying the low-rank dimension $m$.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：通过改变低秩维度$m$的W4A16量化与LoRC。
- en: LoRC-dim $m$ 15.71 12.01 10.69 ![[Uncaptioned image]](img/bfcabda7d45f66085e3e6eb8efed4900.png)\captionof
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: LoRC维度 $m$ 15.71 12.01 10.69 ![[无标题图像]](img/bfcabda7d45f66085e3e6eb8efed4900.png)\captionof
- en: figure Eigenvalues of the Error matrix $E$ for W4A16
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图：W4A16的误差矩阵$E$的特征值。
- en: Ablation Study on the Low Rank Dimension $m$.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对低秩维度$m$的消融研究。
- en: 'This observation may seem counterintuitive initially, as one might anticipate
    that larger LoRC dimensions would yield more significant improvements. To gain
    a more comprehensive understanding, we conducted an analysis of the eigenvalues
    of the actual error matrix $E=W-\hat{W}$ updated during QAT, we arrive at contrasting
    conclusions. For more details, please refer to Appendix [D](#A4 "Appendix D Quantization-aware
    training with LoRC ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs
    from Comprehensive Study to Low Rank Compensation").'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这一观察最初可能会显得违背直觉，因为人们可能会预测更大的LoRC维度会带来更显著的改进。为了获得更全面的理解，我们分析了在QAT过程中实际误差矩阵$E=W-\hat{W}$的特征值，得出了相反的结论。更多细节请参见附录[D](#A4
    "附录 D 量化感知训练与 LoRC ‣ ZeroQuant-V2：从全面研究到低秩补偿的LLM后训练量化")。
- en: 6 Discussion
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: Conclusion.
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结论
- en: In this work, we provide a comprehensive study of post-training quantization
    (PTQ) on large language models with different PTQ methods (e.g., RTN, GPTQ, ZeroQuant),
    and with different quantization coverage (weight-only and weight-and-activation
    quantization), etc. We find that PTQ methods are critical to improving the quantized
    model quality, and that fine-grained quantization (FGQ) can bring acceptable accuracy
    and model size trade-off. Finally, we introduced an optimization technique called
    Low Rank Compensation (LoRC), which works synergistically with PTQ and FGQ, playing
    a crucial role in enhancing full model quality recovery with a minimal increase
    in model size.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们对大语言模型的后训练量化（PTQ）进行了全面研究，涉及不同的PTQ方法（例如，RTN、GPTQ、ZeroQuant），以及不同的量化覆盖（仅权重和权重与激活量化）等。我们发现，PTQ方法对于提高量化模型质量至关重要，细粒度量化（FGQ）可以带来可接受的准确性和模型尺寸的权衡。最后，我们介绍了一种名为低秩补偿（LoRC）的优化技术，它与PTQ和FGQ协同工作，在以最小的模型尺寸增加的情况下显著提升了模型质量恢复。
- en: Limitation.
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 限制
- en: Despite quantizing over 10,000 experiments, our study was constrained by our
    computing resources. This restriction made us choose between diversifying the
    model sizes and varying the tasks. We strategically limited our datasets to WikiText,
    PTB, and C4 to concentrate on a broad range of quantization methods. Consequently,
    our general findings are more robust concerning the two model families and three
    datasets examined in this paper. However, caution should be exercised when generalizing
    these findings to tasks that are dissimilar to those covered in this study.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管进行了超过10,000次实验的量化，我们的研究受限于计算资源。这一限制使我们在多样化模型尺寸和任务之间做出了选择。我们战略性地将数据集限制为WikiText、PTB和C4，以集中于广泛的量化方法。因此，我们的总体发现对本文中考察的两个模型家族和三个数据集更为稳健。然而，当将这些发现推广到与本研究覆盖范围不同的任务时，应谨慎对待。
- en: Future Opportunity.
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 未来机会
- en: 'Throughout the paper, we see several unresolved problems from current quantization
    schemes and/or algorithms, and we find potential directions for LLM compression:
    (1) Although we use fine-grained quantization schemes in the paper, the real implementation
    is missing. Moreover, how to efficiently implement odd bit precision is challenging.
    [[12](#bib.bib12)] demonstrated that 3-bit can achieve better throughput in the
    generation phase by packing all 3-bit numbers in continuous memory space. However,
    this method is sub-optimal as the dequantization step needs to connect bits from
    different bytes. One possible way to implement odd bits, e.g., 5 bits, is to use
    two integer matrices with INT4 and INT1. During the dequantization stage, we couple
    the two matrices together. (2) How to combine PTQ with other lightweight compression
    techniques, e.g., post-training pruning [[20](#bib.bib20), [11](#bib.bib11)],
    is an interesting direction to further reduce the memory consumption and compute
    cost.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们观察到当前量化方案和/或算法中存在若干未解决的问题，并且我们发现了LLM压缩的潜在方向：（1）尽管我们在文中使用了细粒度量化方案，但实际实现缺失。此外，高效实现奇数位精度是一个挑战。[[12](#bib.bib12)]展示了3位通过将所有3位数字打包在连续的内存空间中可以在生成阶段实现更好的吞吐量。然而，这种方法是次优的，因为去量化步骤需要连接来自不同字节的位。一种实现奇数位的方法，例如5位，是使用两个整数矩阵，分别是INT4和INT1。在去量化阶段，我们将这两个矩阵配对在一起。（2）如何将PTQ与其他轻量级压缩技术结合，例如，后训练剪枝[[20](#bib.bib20),
    [11](#bib.bib11)]，是进一步减少内存消耗和计算成本的有趣方向。
- en: References
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu,
    Michael Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization.
    arXiv preprint arXiv:2012.15701, 2020.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu,
    Michael Lyu, 和 Irwin King. Binarybert: 推动 BERT 量化的极限。arXiv 预印本 arXiv:2012.15701,
    2020。'
- en: '[2] Big-Science. Bloom inference. [https://github.com/huggingface/transformers-bloom-inference/tree/main/bloom-inference-scripts](https://github.com/huggingface/transformers-bloom-inference/tree/main/bloom-inference-scripts),
    2022.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Big-Science. Bloom 推理。 [https://github.com/huggingface/transformers-bloom-inference/tree/main/bloom-inference-scripts](https://github.com/huggingface/transformers-bloom-inference/tree/main/bloom-inference-scripts),
    2022。'
- en: '[3] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding
    and overcoming the challenges of efficient transformer quantization. arXiv preprint
    arXiv:2109.12948, 2021.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Yelysei Bondarenko, Markus Nagel, 和 Tijmen Blankevoort. 理解和克服高效变换器量化的挑战。arXiv
    预印本 arXiv:2109.12948, 2021。'
- en: '[4] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165,
    2020.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    等等。语言模型是少量学习者。arXiv 预印本 arXiv:2005.14165, 2020。'
- en: '[5] Bita Darvish Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers,
    Kalin Ovtcharov, Anna Vinogradsky, Sarah Massengill, Lita Yang, Ray Bittner, et al.
    Pushing the limits of narrow precision inferencing at cloud scale with microsoft
    floating point. Advances in neural information processing systems, 33:10271–10281,
    2020.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Bita Darvish Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers,
    Kalin Ovtcharov, Anna Vinogradsky, Sarah Massengill, Lita Yang, Ray Bittner, 等等。用
    Microsoft 浮点推动云规模下窄精度推理的极限。神经信息处理系统进展, 33:10271–10281, 2020。'
- en: '[6] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8
    (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339,
    2022.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer. Llm. int8
    (): 大规模变换器的 8 位矩阵乘法。arXiv 预印本 arXiv:2208.07339, 2022。'
- en: '[7] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit
    inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Tim Dettmers 和 Luke Zettlemoyer. 4 位精度的案例: k 位推理缩放定律。arXiv 预印本 arXiv:2212.09720,
    2022。'
- en: '[8] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy,
    and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153,
    2019.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy,
    和 Dharmendra S Modha. 学习步长量化。arXiv 预印本 arXiv:1902.08153, 2019。'
- en: '[9] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval,
    Herve Jegou, and Armand Joulin. Training with quantization noise for extreme fixed-point
    compression. arXiv preprint arXiv:2004.07320, 2020.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval,
    Herve Jegou, 和 Armand Joulin. 使用量化噪声进行极端固定点压缩训练。arXiv 预印本 arXiv:2004.07320, 2020。'
- en: '[10] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework
    for accurate post-training quantization and pruning. arXiv preprint arXiv:2208.11580,
    2022.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Elias Frantar 和 Dan Alistarh. 最优脑压缩: 准确后训练量化和修剪的框架。arXiv 预印本 arXiv:2208.11580,
    2022。'
- en: '[11] Elias Frantar and Dan Alistarh. Massive language models can be accurately
    pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Elias Frantar 和 Dan Alistarh. 大型语言模型可以在一次操作中准确修剪。arXiv 预印本 arXiv:2301.00774,
    2023。'
- en: '[12] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh. Gptq:
    针对生成预训练变换器的准确后训练量化。arXiv 预印本 arXiv:2210.17323, 2022。'
- en: '[13] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and
    Kurt Keutzer. A survey of quantization methods for efficient neural network inference.
    arXiv preprint arXiv:2103.13630, 2021.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, 和
    Kurt Keutzer. 针对高效神经网络推理的量化方法综述。arXiv 预印本 arXiv:2103.13630, 2021。'
- en: '[14] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, and Kurt Keutzer.
    Ai and memory wall. RiseLab Medium Post, 2021.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, 和 Kurt Keutzer.
    人工智能与内存墙。RiseLab Medium Post, 2021。'
- en: '[15] GitHub. Github copilot. [https://github.com/features/copilot/](https://github.com/features/copilot/),
    2021.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] GitHub. GitHub Copilot. [https://github.com/features/copilot/](https://github.com/features/copilot/),
    2021。'
- en: '[16] Babak Hassibi and David G Stork. Second order derivatives for network
    pruning: Optimal brain surgeon. In Advances in neural information processing systems,
    pages 164–171, 1993.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Babak Hassibi 和 David G Stork。网络剪枝的二阶导数：最佳脑外科医生。发表于神经信息处理系统进展，页面 164–171，1993年。'
- en: '[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge
    in a neural network. Workshop paper in NIPS, 2014.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Geoffrey Hinton, Oriol Vinyals 和 Jeff Dean。蒸馏神经网络中的知识。NIPS 研讨会论文，2014年。'
- en: '[18] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li,
    Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding.
    arXiv preprint arXiv:1909.10351, 2019.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li,
    Fang Wang 和 Qun Liu。Tinybert：为自然语言理解蒸馏 BERT。arXiv 预印本 arXiv:1909.10351，2019年。'
- en: '[19] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer.
    I-bert: Integer-only bert quantization. In International conference on machine
    learning, pages 5506–5518\. PMLR, 2021.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney 和 Kurt Keutzer。I-bert：仅整数的
    BERT 量化。发表于国际机器学习会议，页面 5506–5518。PMLR，2021年。'
- en: '[20] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt Keutzer,
    and Amir Gholami. A fast post-training pruning framework for transformers. arXiv
    preprint arXiv:2204.09656, 2022.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt Keutzer
    和 Amir Gholami。一个用于变换器的快速后训练剪枝框架。arXiv 预印本 arXiv:2204.09656，2022年。'
- en: '[21] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In
    Advances in neural information processing systems, pages 598–605, 1990.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Yann LeCun, John S Denker 和 Sara A Solla。最佳脑损伤。发表于神经信息处理系统进展，页面 598–605，1990年。'
- en: '[22] Mary Ann Marcinkiewicz. Building a large annotated corpus of english:
    The penn treebank. Using Large Corpora, page 273, 1994.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Mary Ann Marcinkiewicz。构建大规模注释英语语料库：宾夕法尼亚树库。在《使用大规模语料库》一书中，页面 273，1994年。'
- en: '[23] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models. In International Conference on Learning Representations,
    2017.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Stephen Merity, Caiming Xiong, James Bradbury 和 Richard Socher。指针哨兵混合模型。发表于国际学习表征会议，2017年。'
- en: '[24] OpenAI. Openai chatgpt. [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/),
    2022.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] OpenAI。Openai chatgpt。 [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)，2022年。'
- en: '[25] Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via
    distillation and quantization. arXiv preprint arXiv:1802.05668, 2018.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Antonio Polino, Razvan Pascanu 和 Dan Alistarh。通过蒸馏和量化进行模型压缩。arXiv 预印本
    arXiv:1802.05668，2018年。'
- en: '[26] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James
    Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff
    Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102,
    2022.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James
    Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal 和 Jeff Dean。高效扩展变换器推理。arXiv
    预印本 arXiv:2211.05102，2022年。'
- en: '[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of
    transfer learning with a unified text-to-text transformer, 2019.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li 和 Peter J. Liu。探索使用统一文本到文本变换器的迁移学习的极限，2019年。'
- en: '[28] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
    Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias
    Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model.
    arXiv preprint arXiv:2211.05100, 2022.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
    Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias
    Gallé 等。Bloom：一个 176b 参数的开放获取多语言模型。arXiv 预印本 arXiv:2211.05100，2022年。'
- en: '[29] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
    Michael W Mahoney, and Kurt Keutzer. Q-BERT: Hessian based ultra low precision
    quantization of bert. In AAAI, pages 8815–8821, 2020.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
    Michael W Mahoney 和 Kurt Keutzer。Q-BERT：基于 Hessian 的超低精度 BERT 量化。发表于 AAAI，页面 8815–8821，2020年。'
- en: '[30] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
    Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay
    Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg
    530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990,
    2022.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
    Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay
    Korthikanti 等。使用 deepspeed 和 megatron 训练 megatron-turing nlg 530b，一个大规模生成语言模型。arXiv
    预印本 arXiv:2201.11990，2022年。'
- en: '[31] Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping
    Luo, and Ngai Wong. Compression of generative pre-trained language models via
    quantization. arXiv preprint arXiv:2203.10705, 2022.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping
    Luo, 和 Ngai Wong. 通过量化对生成预训练语言模型的压缩。arXiv 预印本 arXiv:2203.10705, 2022。'
- en: '[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    In Advances in neural information processing systems, pages 5998–6008, 2017.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 注意力即一切。在神经信息处理系统进展中，第5998–6008页，2017。'
- en: '[33] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong
    He. Understanding int4 quantization for transformer models: Latency speedup, composability,
    and failure cases. arXiv preprint arXiv:2301.12017, 2023.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, 和 Yuxiong He.
    理解变换器模型的 int4 量化：延迟加速、可组合性和失败案例。arXiv 预印本 arXiv:2301.12017, 2023。'
- en: '[34] Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, and Yuxiong He. Extreme
    compression for pre-trained transformers made simple and efficient. arXiv preprint
    arXiv:2206.01859, 2022.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, 和 Yuxiong He. 极端压缩预训练变换器：简化且高效。arXiv
    预印本 arXiv:2206.01859, 2022。'
- en: '[35] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han.
    Smoothquant: Accurate and efficient post-training quantization for large language
    models. arXiv preprint arXiv:2211.10438, 2022.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, 和 Song Han. Smoothquant：大规模语言模型的准确高效后训练量化。arXiv
    预印本 arXiv:2211.10438, 2022。'
- en: '[36] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, 和 Yuxiong He. Zeroquant：大规模变换器的高效且经济的后训练量化。arXiv 预印本 arXiv:2206.01861, 2022。'
- en: '[37] Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. Gobo:
    Quantizing attention-based nlp models for low latency and energy efficient inference.
    In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),
    pages 811–824\. IEEE, 2020.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, 和 Andreas Moshovos. Gobo：量化基于注意力的
    NLP 模型以实现低延迟和节能推理。2020年第53届IEEE/ACM国际微架构研讨会（MICRO），第811–824页。IEEE，2020。'
- en: '[38] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8BERT:
    Quantized 8bit bert. arXiv preprint arXiv:1910.06188, 2019.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Ofir Zafrir, Guy Boudoukh, Peter Izsak, 和 Moshe Wasserblat. Q8BERT：量化的
    8 位 BERT。arXiv 预印本 arXiv:1910.06188, 2019。'
- en: '[39] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding,
    Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual
    pre-trained model. arXiv preprint arXiv:2210.02414, 2022.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding,
    Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia 等人。Glm-130b：一个开放的双语预训练模型。arXiv 预印本
    arXiv:2210.02414, 2022。'
- en: '[40] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open
    pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin 等人。Opt：开放预训练变换器语言模型。arXiv
    预印本 arXiv:2205.01068, 2022。'
- en: '[41] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and
    Qun Liu. Ternarybert: Distillation-aware ultra-low bit bert. arXiv preprint arXiv:2009.12812,
    2020.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, 和 Qun
    Liu. Ternarybert：蒸馏感知的超低位 BERT。arXiv 预印本 arXiv:2009.12812, 2020。'
- en: Appendix A Background of Quantization
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 量化背景
- en: Quantization maps floating point (e.g., FP16/FP32) numbers to integer numbers
    (e.g., INT4/INT8) so that lower memory usage (weight quantization) and faster
    integer arithmetic (weight-and-activation quantization) can be achieved compared
    to the floating point format. In this work, we are focusing on uniform quantization,
    i.e.,
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 量化将浮点数（例如，FP16/FP32）映射到整数数（例如，INT4/INT8），以便相较于浮点格式，实现更低的内存使用（权重量化）和更快的整数运算（权重与激活量化）。在这项工作中，我们专注于均匀量化，即
- en: '|  | $\small Q(x)=\text{INT}\big{(}{(x-Z)}/{S}\big{)}-Z,$ |  | (1) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small Q(x)=\text{INT}\big{(}{(x-Z)}/{S}\big{)}-Z,$ |  | (1) |'
- en: where $Q$ or not), (2) fine-grained vs. coarse-grained quantization (how to
    partition the input x and get its associated scaling factor, e.g., matrix wise
    or row wise). See [[13](#bib.bib13)] for more details.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q$ 或不)，(2) 细粒度与粗粒度量化（如何划分输入 x 并获取其关联的缩放因子，例如，矩阵级或行级）。更多详细信息请参见 [[13](#bib.bib13)]。
- en: Throughout this work, we focus on post-training quantization (PTQ), i.e., no
    or minimal training effort is applied after quantization, for which large accuracy
    degradation usually exhibits for coarse-grained quantization (per matrix/tensor)
    due to their large quantization error. As such, we focus on fine-grained quantization.
    Particularly, we use the per-row quantization (one row of the weight matrix or
    one token for the activation) from [[36](#bib.bib36)] as our coarsest-grained
    quantization method, and we use block-k quantization (for every k elements, they
    have their own scaling factor and/or zero point) as our finer-grained quantization
    scheme.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本工作中，我们专注于后训练量化（PTQ），即量化后没有或仅有最小的训练努力，这通常会因为粗粒度量化（按矩阵/张量）而表现出较大的准确性降级，原因在于其较大的量化误差。因此，我们专注于细粒度量化。特别地，我们使用[[36](#bib.bib36)]中的按行量化（权重矩阵的一行或激活的一个token）作为我们最粗的量化方法，并使用block-k量化（每k个元素具有自己的缩放因子和/或零点）作为我们更细的量化方案。
- en: 'Appendix B Detailed Setting Used in Section [4](#S4 "4 Are existing quantization
    methods optimally harnessing the potential to minimize LLMs sizes? ‣ ZeroQuant-V2:
    Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank
    Compensation")'
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '附录B详细设置见于[4](#S4 "4 Are existing quantization methods optimally harnessing
    the potential to minimize LLMs sizes? ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation")'
- en: Same as [[12](#bib.bib12)], for all methods, we use C4 dataset to randomly select
    128 sentences for training and each of them has 2048 tokens.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 与[[12](#bib.bib12)]相同，对于所有方法，我们使用C4数据集随机选择128个句子进行训练，每个句子包含2048个token。
- en: For GPTQ, we check its main hyperparameter, i.e., the dampening factor, and
    find out the method is not sensitive to it. As such, we use the hyparameter suggested
    by the author for all of our experiments. For ZQ-Global and ZQ-Local, as mentioned
    the in main text, the hyperparameters suggested by the original work [[36](#bib.bib36)]
    is suboptimal. We find that a linear decay learning rate schedule is very helpful
    in our initial test. As such, we add this as our default setting. Meanwhile, we
    extensively test a wide range (1e-3 to 5e-8) of learning rate for different models
    until we find the best learning rate (i.e., larger or smaller learning rate leads
    to worse accuracy performance).We employed the Adam optimizer and set the default
    batch size to 1 for our experiments.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPTQ，我们检查了其主要超参数，即衰减因子，并发现该方法对其不敏感。因此，我们在所有实验中使用作者建议的超参数。对于ZQ-Global和ZQ-Local，如主文中提到的，原始工作[[36](#bib.bib36)]建议的超参数是不理想的。我们发现线性衰减学习率计划在初步测试中非常有帮助。因此，我们将其添加为默认设置。同时，我们对不同模型进行了广泛的学习率范围（1e-3到5e-8）测试，直到找到最佳学习率（即，较大的或较小的学习率会导致更差的准确性表现）。我们使用了Adam优化器，并将默认批量大小设置为1进行实验。
- en: 'We conducted tests to assess whether changes in random seeds would introduce
    substantial variations in the outcomes. As per the findings detailed in Table
    Table [B.1](#A2.T1 "Table B.1 ‣ Appendix B Detailed Setting Used in Section 4
    ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive
    Study to Low Rank Compensation"), the modifications in random seeds resulted in
    only minimal effects on the final quality of the models. This effect was particularly
    negligible in the context of larger models, such as OPT-30b, where the standard
    deviation was only 0.01\. Therefore, in consideration of these results, we elected
    to standardize the random seed for the subsequent experiments presented in this
    paper, setting it uniformly at 123 or 0\. The code will be made publicly available
    to facilitate reproducibility of our results.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进行了测试，以评估随机种子的变化是否会引入结果的重大差异。根据表[B.1](#A2.T1 "Table B.1 ‣ Appendix B Detailed
    Setting Used in Section 4 ‣ ZeroQuant-V2: Exploring Post-training Quantization
    in LLMs from Comprehensive Study to Low Rank Compensation")中的详细结果，随机种子的修改对模型最终质量的影响仅为微乎其微。特别是在较大模型（如OPT-30b）的情况下，这种影响尤其微小，标准偏差仅为0.01。因此，考虑到这些结果，我们决定对本文后续实验标准化随机种子，统一设置为123或0。代码将公开，以便于结果的重复性验证。'
- en: For all three methods, we run them on a single GPU (either V100-32GB or A100-80GB).
    For the largest model tested in the paper, i.e., BLOOM-176B, the cost of all methods
    is lower than one GPU-day on A100-80G.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有三种方法，我们在单个GPU（V100-32GB或A100-80GB）上运行它们。对于论文中测试的最大模型，即BLOOM-176B，所有方法的成本低于A100-80G上的一天GPU费用。
- en: 'Table B.1: The table on the left illustrates the outcomes of each task, evaluated
    using three different random seeds. On the right, we present a table detailing
    the mean and standard deviation of the Task-mean values (which can be found in
    the final column of the left table) over the three random seeds, accompanied by
    additional quantization results. The quantization methodologies employed in this
    context are based on the GPTQ algorithm.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表 B.1：左侧的表格展示了使用三个不同随机种子评估的每个任务的结果。右侧，我们提供了一个表格，详细说明了三个随机种子下任务均值的平均值和标准偏差（可以在左侧表格的最后一列找到），并附上了额外的量化结果。本上下文中使用的量化方法基于GPTQ算法。
- en: Precision Random Seed WikiText PTB C4 Task-mean OPT-13b 123 10.31 12.62 11.35
    11.43 W4A16 234 10.25 12.57 11.35 11.39 456 10.37 12.61 11.36 11.44 OPT-30b 123
    9.56 11.95 10.79 10.77 W4A16 234 9.6 11.95 10.79 10.78 456 9.52 11.97 10.79 10.76
    Precision Items OPT-1.3b OPT-13b OPT-30b W4A16 mean over three random seeds 16.39
    11.42 10.77 standard deviation 0.019 0.027 0.010 W4A8 mean over three random seeds
    16.76 17.16 21.64 standard deviation 0.048 0.048 1.277
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 随机种子 WikiText PTB C4 任务均值 OPT-13b 123 10.31 12.62 11.35 11.43 W4A16 234 10.25
    12.57 11.35 11.39 456 10.37 12.61 11.36 11.44 OPT-30b 123 9.56 11.95 10.79 10.77
    W4A16 234 9.6 11.95 10.79 10.78 456 9.52 11.97 10.79 10.76 精度 项目 OPT-1.3b OPT-13b
    OPT-30b W4A16 三个随机种子的平均值 16.39 11.42 10.77 标准偏差 0.019 0.027 0.010 W4A8 三个随机种子的平均值
    16.76 17.16 21.64 标准偏差 0.048 0.048 1.277
- en: Appendix C Best PTQ Methods with Per-row Quantization
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 具有逐行量化的最佳 PTQ 方法
- en: 'Table [C.1](#A3.T1 "Table C.1 ‣ Appendix C Best PTQ Methods with Per-row Quantization
    ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive
    Study to Low Rank Compensation") and [C.2](#A3.T2 "Table C.2 ‣ Appendix C Best
    PTQ Methods with Per-row Quantization ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation") summarize
    the best PTQ methods with per-row optimization.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [C.1](#A3.T1 "表 C.1 ‣ 附录 C 具有逐行量化的最佳 PTQ 方法 ‣ ZeroQuant-V2：从综合研究到低秩补偿的 LLM
    后训练量化探索") 和 [C.2](#A3.T2 "表 C.2 ‣ 附录 C 具有逐行量化的最佳 PTQ 方法 ‣ ZeroQuant-V2：从综合研究到低秩补偿的
    LLM 后训练量化探索") 总结了具有逐行优化的最佳 PTQ 方法。
- en: 'Table C.1: Best optimization method of OPT family in Section [4](#S4 "4 Are
    existing quantization methods optimally harnessing the potential to minimize LLMs
    sizes? ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive
    Study to Low Rank Compensation").'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表 C.1：OPT 家族在第 [4](#S4 "4 现有的量化方法是否最优地利用了减少 LLM 尺寸的潜力？ ‣ ZeroQuant-V2：从综合研究到低秩补偿的
    LLM 后训练量化探索") 节中的最佳优化方法。
- en: Precision 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b Weight Only (INT4) ZQ-Global
    ZQ-Global GPTQ GPTQ GPTQ GPTQ GPTQ GPTQ Weight & Activation (W4A8) ZQ-Global ZQ-Global
    ZQ-Global GPTQ ZQ-Global ZQ-Global ZQ-Global ZQ-Local
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b 仅权重 (INT4) ZQ-Global ZQ-Global GPTQ
    GPTQ GPTQ GPTQ GPTQ GPTQ 权重与激活 (W4A8) ZQ-Global ZQ-Global ZQ-Global GPTQ ZQ-Global
    ZQ-Global ZQ-Global ZQ-Local
- en: 'Table C.2: Best optimization method of BLOOM family in Section [4](#S4 "4 Are
    existing quantization methods optimally harnessing the potential to minimize LLMs
    sizes? ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive
    Study to Low Rank Compensation").'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 表 C.2：BLOOM 家族在第 [4](#S4 "4 现有的量化方法是否最优地利用了减少 LLM 尺寸的潜力？ ‣ ZeroQuant-V2：从综合研究到低秩补偿的
    LLM 后训练量化探索") 节中的最佳优化方法。
- en: Precision 560m 1.1b 1.7b 3b 7.1b 176b Weight Only (INT4) GPTQ ZQ-Global ZQ-Global
    ZQ-Global/GPTQ GPTQ GPTQ Weight & Activation (W4A8) ZQ-Global ZQ-Global ZQ-Global
    ZQ-Global ZQ-Global ZQ-Local
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 560m 1.1b 1.7b 3b 7.1b 176b 仅权重 (INT4) GPTQ ZQ-Global ZQ-Global ZQ-Global/GPTQ
    GPTQ GPTQ 权重与激活 (W4A8) ZQ-Global ZQ-Global ZQ-Global ZQ-Global ZQ-Global ZQ-Local
- en: Appendix D Quantization-aware training with LoRC
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 使用 LoRC 的量化感知训练
- en: In order to better understand our proposed algorithm, LoRC, particularly in
    relation to the dimensions of low-rank matrices, we applied quantize-aware training
    alongside knowledge distillation. This approach builds upon the methodology of
    row-wise weight quantization and token-wise quantization. For the optimization
    process, we employed the Adam optimizer, setting the learning rate at 1e-4 and
    a dropout rate of 0.05\. These settings were identified as the most effective
    in our context (additional details can be found in [[33](#bib.bib33)]). We performed
    fine-tuning on the WikiText dataset using pre-trained GPT2 models with 125M and
    350M parameters, which were obtained from Hugging Face as our initial models.
    ⁷⁷7[https://huggingface.co/gpt2](https://huggingface.co/gpt2)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解我们提出的算法 LoRC，特别是在低秩矩阵的维度方面，我们应用了量化感知训练与知识蒸馏。这种方法建立在按行权重量化和按标记量化的方法论之上。对于优化过程，我们使用了
    Adam 优化器，学习率设置为 1e-4，丢弃率为 0.05。这些设置被确定为我们上下文中最有效的（更多细节见[[33](#bib.bib33)]）。我们在
    WikiText 数据集上对预训练的 GPT2 模型进行了微调，这些模型的参数分别为 125M 和 350M，来自 Hugging Face，作为我们的初始模型。⁷⁷7[https://huggingface.co/gpt2](https://huggingface.co/gpt2)
- en: 'The results are illustrated in Figure Figure [D](#A4 "Appendix D Quantization-aware
    training with LoRC ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs
    from Comprehensive Study to Low Rank Compensation"). As observed, the quantized
    models tend to overfit swiftly. However, implementing higher dropout values, such
    as 0.1, does not result in a significantly improved performance with regards to
    the best perplexity over the entire training duration. Now when examining the
    best perplexity associated with each dimension of LoRC (also indicated in the
    figure’s legend), it becomes evident that the larger the dimension, the better
    the W4A8 models perform. This suggests that augmenting the dimension of LoRC can
    enhance the model quality for QAT, a finding that deviates from the trends observed
    in PTQ.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如图[Figure D](#A4 "附录 D 量化感知训练与 LoRC ‣ ZeroQuant-V2：从综合研究到低秩补偿的后训练量化探索")所示。如观察到的，量化模型往往迅速过拟合。然而，实施更高的丢弃值（如
    0.1）并未显著改善整个训练期间的最佳困惑度。现在，当检查 LoRC 的每个维度相关的最佳困惑度（也在图例中指示）时，可以明显看出，维度越大，W4A8 模型的表现越好。这表明增加
    LoRC 的维度可以提升 QAT 模型质量，这一发现与 PTQ 中观察到的趋势有所不同。
- en: '![[Uncaptioned image]](img/301006d0c2ef0b295ffae5ffc2447360.png)![[Uncaptioned
    image]](img/6b4d5fce1d4a4eb41981973efd1c0c89.png)\captionof'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![[未说明的图像]](img/301006d0c2ef0b295ffae5ffc2447360.png)![[未说明的图像]](img/6b4d5fce1d4a4eb41981973efd1c0c89.png)\captionof'
- en: figureThe graph on the left represents the results for a smaller model size
    (GPT2-125M), while the one on the right corresponds to the GPT2-350M model. The
    dimension (refer to the legend) in the LoRC algorithm, which is represented by
    different color curves, plays a pivotal role in approximating the original quality
    of the fp16 model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图表左侧显示了较小模型（GPT2-125M）的结果，而右侧则对应于 GPT2-350M 模型。LoRC 算法中的维度（参见图例），由不同颜色的曲线表示，在逼近
    fp16 模型的原始质量中起着关键作用。
- en: Appendix E Tables and Figures
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 表格和图形
- en: We put the full results of our evaluations in this section.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将评估的完整结果放在本节中。
- en: 'Table E.1: OPT ppl on wikitext/ptb/c4 (full results of Table [2](#S3.T2 "Table
    2 ‣ 3 Would different model families behave similarly on quantization? ‣ ZeroQuant-V2:
    Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank
    Compensation")).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 表 E.1：在 wikitext/ptb/c4 上的 OPT ppl（表[2](#S3.T2 "表 2 ‣ 3 不同模型家族在量化时表现是否相似？ ‣
    ZeroQuant-V2：从综合研究到低秩补偿的后训练量化探索") 的完整结果）。
- en: Precision 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b W16-A16 27.65/32.55/24.61 22.00/26.08/20.71
    14.62/16.97/14.72 12.47/15.11/13.17 10.86/13.09/11.74 10.13/12.34/11.20 9.56/11.84/10.69
    9.34/11.36/10.28 W8A8${}^{\text{sym}}$ 27.84/32.60/24.66 22.04/26.22/20.81 15.14/17.65/15.39
    12.51/15.38/13.38 11.24/14.17/12.45 11.83/18.87/15.39 14.08/31.54/25.09 442.66/524.57/716.83
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b W16-A16 27.65/32.55/24.61 22.00/26.08/20.71
    14.62/16.97/14.72 12.47/15.11/13.17 10.86/13.09/11.74 10.13/12.34/11.20 9.56/11.84/10.69
    9.34/11.36/10.28 W8A8${}^{\text{sym}}$ 27.84/32.60/24.66 22.04/26.22/20.81 15.14/17.65/15.39
    12.51/15.38/13.38 11.24/14.17/12.45 11.83/18.87/15.39 14.08/31.54/25.09 442.66/524.57/716.83
- en: 'Table E.2: BLOOM ppl on wikitext/ptb/c4 (full results of Table [2](#S3.T2 "Table
    2 ‣ 3 Would different model families behave similarly on quantization? ‣ ZeroQuant-V2:
    Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank
    Compensation")).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表 E.2：BLOOM ppl 在 wikitext/ptb/c4 上（完整结果见 表 [2](#S3.T2 "表 2 ‣ 3 不同模型家族在量化上的表现是否类似？
    ‣ ZeroQuant-V2：从全面研究到低秩补偿的后训练量化探索")）。
- en: Precision 560m 1.1b 1.7b 3b 7.1b 176b W16-A16 22.43/41.25/24.38 17.69/46.98/20.29
    15.39/27.93/17.97 13.48/23.12/16.14 11.37/19.40/14.13 8.11/13.62/10.97 W8${}^{\text{sym}}$
    22.45/41.37/24.42 17.71/47.05/20.32 15.45/28.09/18.02 13.52/23.24/16.19 11.47/19.71/14.25
    8.41/14.52/11.93
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 560m 1.1b 1.7b 3b 7.1b 176b W16-A16 22.43/41.25/24.38 17.69/46.98/20.29 15.39/27.93/17.97
    13.48/23.12/16.14 11.37/19.40/14.13 8.11/13.62/10.97 W8${}^{\text{sym}}$ 22.45/41.37/24.42
    17.71/47.05/20.32 15.45/28.09/18.02 13.52/23.24/16.19 11.47/19.71/14.25 8.41/14.52/11.93
- en: 'Table E.3: OPT ppl on wikitext/opt/c4 with W4${}^{\text{asym}}$-A16 (full table
    of Table [3](#S4.T3 "Table 3 ‣ 4 Are existing quantization methods optimally harnessing
    the potential to minimize LLMs sizes? ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation")). See Table [E.4](#A5.T4
    "Table E.4 ‣ Appendix E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation") for all
    learning rate results of ZQ-Local and Table [E.5](#A5.T5 "Table E.5 ‣ Appendix
    E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs
    from Comprehensive Study to Low Rank Compensation") of ZQ-Global.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表 E.3：OPT ppl 在 wikitext/opt/c4 上，使用 W4${}^{\text{asym}}$-A16（完整表格见 表 [3](#S4.T3
    "表 3 ‣ 4 现有的量化方法是否最优地利用了缩小 LLM 尺寸的潜力？ ‣ ZeroQuant-V2：从全面研究到低秩补偿的后训练量化探索")）。有关
    ZQ-Local 的所有学习率结果，请参见 表 [E.4](#A5.T4 "表 E.4 ‣ 附录 E 表格和图形 ‣ ZeroQuant-V2：从全面研究到低秩补偿的后训练量化探索")，有关
    ZQ-Global 的信息，请参见 表 [E.5](#A5.T5 "表 E.5 ‣ 附录 E 表格和图形 ‣ ZeroQuant-V2：从全面研究到低秩补偿的后训练量化探索")。
- en: Precision 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b RTN 36.71/44.76/30.92 25.51/30.90/23.86
    19.38/21.95/17.93 17.92/22.48/18.32 11.91/15.39/13.01 10.67/13.53/12.07 10.10/13.13/11.33
    20.24/48.45/25.86 GPTQ 32.52/40.25/27.78 23.50/29.14/22.41 15.52/18.16/15.56 13.02/15.84/13.73
    11.16/13.59/12.08 10.29/12.61/11.35 9.61/11.95/10.79 9.54/11.67/10.52 ZQ-Local^∗
    33.05/39.34/28.11 24.40/29.22/22.82 15.81/18.66/15.76 13.22/16.19/13.96 11.32/13.79/12.26
    10.42/12.90/11.60 9.97/12.32/11.03 9.91/11.87/10.59 ZQ-Global^∗ 31.44/36.66/27.21
    23.32/28.05/21.98 15.46/18.31/15.67 13.03/16.04/13.83 11.30/13.69/12.17 10.38/12.85/11.62
    9.90/12.24/10.99 9.62/11.81/10.61
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b RTN 36.71/44.76/30.92 25.51/30.90/23.86
    19.38/21.95/17.93 17.92/22.48/18.32 11.91/15.39/13.01 10.67/13.53/12.07 10.10/13.13/11.33
    20.24/48.45/25.86 GPTQ 32.52/40.25/27.78 23.50/29.14/22.41 15.52/18.16/15.56 13.02/15.84/13.73
    11.16/13.59/12.08 10.29/12.61/11.35 9.61/11.95/10.79 9.54/11.67/10.52 ZQ-Local^∗
    33.05/39.34/28.11 24.40/29.22/22.82 15.81/18.66/15.76 13.22/16.19/13.96 11.32/13.79/12.26
    10.42/12.90/11.60 9.97/12.32/11.03 9.91/11.87/10.59 ZQ-Global^∗ 31.44/36.66/27.21
    23.32/28.05/21.98 15.46/18.31/15.67 13.03/16.04/13.83 11.30/13.69/12.17 10.38/12.85/11.62
    9.90/12.24/10.99 9.62/11.81/10.61
- en: 'Table E.4: OPT ppl on wikitext/opt/c4 with W4${}^{\text{asym}}$-A16 and ZQ-Local.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表 E.4：OPT ppl 在 wikitext/opt/c4 上，使用 W4${}^{\text{asym}}$-A16 和 ZQ-Local。
- en: LR (W4${}^{\text{asym}}$-A16) 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b 0.001 33.67/39.45/29.11
    26.33/31.94/24.49 16.27/19.91/16.46 14.34/17.76/14.93 11.87/15.04/13.06 13.68/18.89/14.46
    171.35/151.55/46.14 814.22/601.74/308.53 0.0005 32.76/39.51/28.64 25.88/30.95/23.96
    16.29/19.82/16.27 14.16/17.65/14.79 11.92/15.23/12.95 10.93/13.82/12.03 10.23/13.46/11.44
    10.10/12.27/10.81 0.0001 33.86/40.01/28.29 24.64/30.26/23.33 16.07/19.25/15.93
    14.36/17.38/14.41 11.85/14.64/12.74 10.93/13.48/11.88 10.18/12.67/11.13 10.12/12.01/10.67
    5e-05 33.05/39.34/28.11 25.42/29.65/23.22 15.79/19.16/15.88 13.70/16.80/14.16
    11.71/14.32/12.41 10.75/13.38/11.77 9.95/12.54/11.09 10.02/11.89/10.64 1e-05 33.78/40.41/28.84
    24.40/29.22/22.82 15.81/18.66/15.76 13.55/16.46/13.96 11.32/13.79/12.26 10.54/13.05/11.61
    9.98/12.22/10.99 9.91/11.87/10.59 5e-06 34.47/41.04/29.02 24.50/29.27/23.00 16.01/18.73/15.91
    13.22/16.19/13.96 11.33/13.86/12.29 10.42/12.90/11.60 9.86/12.33/10.97 9.97/11.86/10.60
    1e-06 35.88/43.69/30.35 24.54/29.87/23.17 16.77/19.45/16.47 13.60/17.02/14.46
    11.41/14.10/12.41 10.53/13.01/11.70 9.97/12.33/11.04 10.01/11.93/10.66
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: LR (W4${}^{\text{asym}}$-A16) 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b 0.001 33.67/39.45/29.11
    26.33/31.94/24.49 16.27/19.91/16.46 14.34/17.76/14.93 11.87/15.04/13.06 13.68/18.89/14.46
    171.35/151.55/46.14 814.22/601.74/308.53 0.0005 32.76/39.51/28.64 25.88/30.95/23.96
    16.29/19.82/16.27 14.16/17.65/14.79 11.92/15.23/12.95 10.93/13.82/12.03 10.23/13.46/11.44
    10.10/12.27/10.81 0.0001 33.86/40.01/28.29 24.64/30.26/23.33 16.07/19.25/15.93
    14.36/17.38/14.41 11.85/14.64/12.74 10.93/13.48/11.88 10.18/12.67/11.13 10.12/12.01/10.67
    5e-05 33.05/39.34/28.11 25.42/29.65/23.22 15.79/19.16/15.88 13.70/16.80/14.16
    11.71/14.32/12.41 10.75/13.38/11.77 9.95/12.54/11.09 10.02/11.89/10.64 1e-05 33.78/40.41/28.84
    24.40/29.22/22.82 15.81/18.66/15.76 13.55/16.46/13.96 11.32/13.79/12.26 10.54/13.05/11.61
    9.98/12.22/10.99 9.91/11.87/10.59 5e-06 34.47/41.04/29.02 24.50/29.27/23.00 16.01/18.73/15.91
    13.22/16.19/13.96 11.33/13.86/12.29 10.42/12.90/11.60 9.86/12.33/10.97 9.97/11.86/10.60
    1e-06 35.88/43.69/30.35 24.54/29.87/23.17 16.77/19.45/16.47 13.60/17.02/14.46
    11.41/14.10/12.41 10.53/13.01/11.70 9.97/12.33/11.04 10.01/11.93/10.66
- en: 'Table E.5: OPT ppl on wikitext/opt/c4 with W4${}^{\text{asym}}$-A16 and ZQ-Global.
    NaN here means the PPL is larger than 1e6.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表 E.5：使用 W4${}^{\text{asym}}$-A16 和 ZQ-Global 在 wikitext/opt/c4 上的 OPT ppl。NaN
    表示 PPL 大于 1e6。
- en: LR (W4${}^{\text{asym}}$-A16) 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b 0.001 4057.13/2718.91/1247.78
    5071.35/5229.93/687.35 12105.25/10154.73/7893.43 18965.76/17112.60/16316.31 60014.66/56041.86/78085.84
    232421.09/98305.32/119762.73 93917.09/70170.34/51124.06 NaN 0.0005 31.94/38.61/27.17
    27.11/33.91/24.07 10900.84/8322.65/8425.10 14412.30/8676.76/10154.55 18527.46/13530.12/13029.95
    109006.53/62584.41/125349.50 303235.75/230599.62/430480.03 36439.32/30554.19/33756.93
    0.0001 31.44/36.66/27.21 24.08/29.08/22.27 15.91/20.08/16.35 118.38/53.47/54.08
    7604.92/5339.10/5161.49 12638.86/7639.95/8243.63 16276.68/9890.26/6176.27 8367.31/4728.13/5533.59
    5e-05 31.97/36.93/27.12 23.55/28.06/22.02 15.82/18.65/15.65 13.40/16.44/13.97
    26.54/25.67/17.60 909.99/316.82/370.84 6238.21/3291.04/3743.01 9296.98/6687.44/5363.29
    1e-05 32.31/37.93/27.38 23.32/28.05/21.98 15.60/18.42/15.64 13.09/16.05/13.78
    11.41/13.82/12.20 10.80/13.16/11.66 10.06/12.44/11.07 9.73/12.09/10.98 5e-06 32.69/38.91/27.76
    23.26/28.33/22.05 15.46/18.31/15.67 13.03/16.04/13.83 11.30/13.69/12.17 10.50/12.89/11.58
    9.95/12.28/11.01 9.62/11.81/10.61 1e-06 34.63/41.75/29.43 23.82/28.96/22.48 16.12/19.46/16.27
    13.03/16.27/14.04 11.29/13.88/12.27 10.38/12.85/11.62 9.90/12.24/10.99 9.58/12.17/10.78
    5e-07 NaN NaN NaN NaN NaN 10.51/12.96/11.70 9.89/12.41/11.04 9.90/12.45/11.00
    1e-07 NaN NaN NaN NaN NaN 10.63/13.29/11.89 10.02/12.82/11.18 11.03/13.91/11.73
    5e-08 NaN NaN NaN NaN NaN 10.66/13.42/11.97 10.05/13.00/11.24 12.41/17.45/13.02
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: LR (W4${}^{\text{asym}}$-A16) 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b 0.001 4057.13/2718.91/1247.78
    5071.35/5229.93/687.35 12105.25/10154.73/7893.43 18965.76/17112.60/16316.31 60014.66/56041.86/78085.84
    232421.09/98305.32/119762.73 93917.09/70170.34/51124.06 NaN 0.0005 31.94/38.61/27.17
    27.11/33.91/24.07 10900.84/8322.65/8425.10 14412.30/8676.76/10154.55 18527.46/13530.12/13029.95
    109006.53/62584.41/125349.50 303235.75/230599.62/430480.03 36439.32/30554.19/33756.93
    0.0001 31.44/36.66/27.21 24.08/29.08/22.27 15.91/20.08/16.35 118.38/53.47/54.08
    7604.92/5339.10/5161.49 12638.86/7639.95/8243.63 16276.68/9890.26/6176.27 8367.31/4728.13/5533.59
    5e-05 31.97/36.93/27.12 23.55/28.06/22.02 15.82/18.65/15.65 13.40/16.44/13.97
    26.54/25.67/17.60 909.99/316.82/370.84 6238.21/3291.04/3743.01 9296.98/6687.44/5363.29
    1e-05 32.31/37.93/27.38 23.32/28.05/21.98 15.60/18.42/15.64 13.09/16.05/13.78
    11.41/13.82/12.20 10.80/13.16/11.66 10.06/12.44/11.07 9.73/12.09/10.98 5e-06 32.69/38.91/27.76
    23.26/28.33/22.05 15.46/18.31/15.67 13.03/16.04/13.83 11.30/13.69/12.17 10.50/12.89/11.58
    9.95/12.28/11.01 9.62/11.81/10.61 1e-06 34.63/41.75/29.43 23.82/28.96/22.48 16.12/19.46/16.27
    13.03/16.27/14.04 11.29/13.88/12.27 10.38/12.85/11.62 9.90/12.24/10.99 9.58/12.17/10.78
    5e-07 NaN NaN NaN NaN NaN 10.51/12.96/11.70 9.89/12.41/11.04 9.90/12.45/11.00
    1e-07 NaN NaN NaN NaN NaN 10.63/13.29/11.89 10.02/12.82/11.18 11.03/13.91/11.73
    5e-08 NaN NaN NaN NaN NaN 10.66/13.42/11.97 10.05/13.00/11.24 12.41/17.45/13.02
- en: 'Table E.6: BLOOM ppl on wikitext/opt/c4 with W4${}^{\text{asym}}$-A16 (full
    table of Table [3](#S4.T3 "Table 3 ‣ 4 Are existing quantization methods optimally
    harnessing the potential to minimize LLMs sizes? ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation")). See Table [E.4](#A5.T4
    "Table E.4 ‣ Appendix E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation") for all
    learning rate results of ZQ-Local and Table [E.5](#A5.T5 "Table E.5 ‣ Appendix
    E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs
    from Comprehensive Study to Low Rank Compensation") of ZQ-Global.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.6: 在 wikitext/opt/c4 上使用 W4${}^{\text{asym}}$-A16 的 BLOOM ppl（完整表格见表 [3](#S4.T3
    "表 3 ‣ 现有量化方法是否充分利用了最小化 LLM 尺寸的潜力？ ‣ ZeroQuant-V2: 从综合研究到低秩补偿的 LLM 后训练量化探索")）。有关
    ZQ-Local 的所有学习率结果，请参见表 [E.4](#A5.T4 "表 E.4 ‣ 附录 E 表格和图形 ‣ ZeroQuant-V2: 从综合研究到低秩补偿的
    LLM 后训练量化探索")，以及 ZQ-Global 的表 [E.5](#A5.T5 "表 E.5 ‣ 附录 E 表格和图形 ‣ ZeroQuant-V2:
    从综合研究到低秩补偿的 LLM 后训练量化探索")。'
- en: Precision 560m 1.1b 1.7b 3b 7.1b 176b RTN 25.31/46.79/27.10 23.90/68.31/25.99
    16.93/31.02/19.47 14.65/25.12/17.26 12.06/20.83/14.83 8.34/14.03/11.23 GPTQ 23.90/43.76/25.59
    24.34/68.10/26.58 16.36/29.58/18.79 14.10/24.23/16.66 11.80/20.23/14.47 8.22/13.78/11.07
    ZQ-Local^∗ 24.23/44.94/26.05 19.22/52.36/21.59 16.37/29.89/18.86 14.23/24.41/16.86
    11.80/20.28/14.56 8.27/13.91/11.16 ZQ-Global^∗ 23.84/44.17/25.60 19.50/51.33/21.72
    16.19/29.28/18.66 14.14/24.16/16.69 11.77/20.27/14.52 8.24/13.82/11.10
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 560m 1.1b 1.7b 3b 7.1b 176b RTN 25.31/46.79/27.10 23.90/68.31/25.99 16.93/31.02/19.47
    14.65/25.12/17.26 12.06/20.83/14.83 8.34/14.03/11.23 GPTQ 23.90/43.76/25.59 24.34/68.10/26.58
    16.36/29.58/18.79 14.10/24.23/16.66 11.80/20.23/14.47 8.22/13.78/11.07 ZQ-Local^∗
    24.23/44.94/26.05 19.22/52.36/21.59 16.37/29.89/18.86 14.23/24.41/16.86 11.80/20.28/14.56
    8.27/13.91/11.16 ZQ-Global^∗ 23.84/44.17/25.60 19.50/51.33/21.72 16.19/29.28/18.66
    14.14/24.16/16.69 11.77/20.27/14.52 8.24/13.82/11.10
- en: 'Table E.7: BLOOM ppl on wikitext/opt/c4 with W4${}^{\text{asym}}$-A16 and ZQ-Local.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.7: 在 wikitext/opt/c4 上使用 W4${}^{\text{asym}}$-A16 和 ZQ-Local 的 BLOOM ppl。'
- en: LR (W4${}^{\text{asym}}$-A16) 560m 1.1b 1.7b 3b 7.1b 176b 0.001 25.37/47.36/27.03
    19.89/53.86/22.11 16.70/31.19/19.30 14.45/25.28/17.16 12.22/21.34/15.04 8.82/15.77/11.98
    0.0005 25.17/46.83/26.87 19.57/53.66/21.92 16.58/30.27/19.15 14.43/25.47/17.07
    11.94/20.54/14.67 8.35/14.01/11.20 0.0001 24.59/46.11/26.32 19.22/52.36/21.59
    16.41/30.29/18.90 14.35/24.81/16.87 11.83/20.34/14.58 8.28/13.92/11.14 5e-05 24.44/46.04/26.16
    23.28/65.68/25.42 16.39/30.01/18.86 14.34/24.43/16.83 11.80/20.28/14.56 8.27/13.93/11.15
    1e-05 24.23/44.94/26.05 23.45/66.29/25.52 16.37/29.89/18.86 14.23/24.41/16.86
    11.84/20.39/14.58 8.27/13.91/11.16 5e-06 24.21/45.21/26.10 23.26/65.72/25.42 16.42/30.09/18.94
    14.25/24.55/16.87 11.87/20.50/14.61 8.29/13.98/11.16 1e-06 24.71/45.86/26.50 23.45/66.28/25.56
    16.64/30.52/19.15 14.46/24.76/17.04 11.94/20.55/14.70 8.29/13.97/11.18
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: LR (W4${}^{\text{asym}}$-A16) 560m 1.1b 1.7b 3b 7.1b 176b 0.001 25.37/47.36/27.03
    19.89/53.86/22.11 16.70/31.19/19.30 14.45/25.28/17.16 12.22/21.34/15.04 8.82/15.77/11.98
    0.0005 25.17/46.83/26.87 19.57/53.66/21.92 16.58/30.27/19.15 14.43/25.47/17.07
    11.94/20.54/14.67 8.35/14.01/11.20 0.0001 24.59/46.11/26.32 19.22/52.36/21.59
    16.41/30.29/18.90 14.35/24.81/16.87 11.83/20.34/14.58 8.28/13.92/11.14 5e-05 24.44/46.04/26.16
    23.28/65.68/25.42 16.39/30.01/18.86 14.34/24.43/16.83 11.80/20.28/14.56 8.27/13.93/11.15
    1e-05 24.23/44.94/26.05 23.45/66.29/25.52 16.37/29.89/18.86 14.23/24.41/16.86
    11.84/20.39/14.58 8.27/13.91/11.16 5e-06 24.21/45.21/26.10 23.26/65.72/25.42 16.42/30.09/18.94
    14.25/24.55/16.87 11.87/20.50/14.61 8.29/13.98/11.16 1e-06 24.71/45.86/26.50 23.45/66.28/25.56
    16.64/30.52/19.15 14.46/24.76/17.04 11.94/20.55/14.70 8.29/13.97/11.18
- en: 'Table E.8: BLOOM ppl on wikitext/opt/c4 with W4${}^{\text{asym}}$-A16 and ZQ-Global.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.8: 在 wikitext/opt/c4 上使用 W4${}^{\text{asym}}$-A16 和 ZQ-Global 的 BLOOM ppl。'
- en: LR (W4${}^{\text{asym}}$-A16) 560m 1.1b 1.7b 3b 7.1b 176b 0.001 6853935.00/30441738.00/3222857.25
    528072.88/828428.62/356031.97 597410.50/973155.88/1280478.12 878460.69/2175974.25/441401.94
    nan/nan/nan NaN 0.0005 29671.52/1795030.88/4653.35 28112.96/87515.64/1826.82 141110.14/204295.86/40146.11
    265457.25/741326.38/99882.45 944784.19/774538.25/395960.03 NaN 0.0001 23.92/45.68/25.72
    19.34/52.78/21.63 16.35/29.22/18.76 14.27/24.46/16.80 12.17/22.16/14.80 NaN 5e-05
    23.84/44.17/25.60 19.50/51.33/21.72 16.19/29.28/18.66 14.14/24.16/16.69 11.81/20.41/14.50
    NaN 1e-05 23.85/44.20/25.65 22.64/56.79/23.41 16.23/29.73/18.73 14.14/24.31/16.74
    11.77/20.27/14.52 8.24/13.82/11.10 5e-06 24.02/44.62/25.79 23.46/63.27/24.88 16.28/29.83/18.81
    14.19/24.38/16.80 11.77/20.33/14.54 8.24/13.82/11.10 1e-06 24.46/45.41/26.20 24.62/70.16/26.64
    16.48/30.15/19.02 14.35/24.56/16.95 11.89/20.54/14.67 8.23/13.82/11.12 5e-07 NaN
    NaN NaN NaN NaN 8.26/13.86/11.13
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: LR (W4${}^{\text{asym}}$-A16) 560m 1.1b 1.7b 3b 7.1b 176b 0.001 6853935.00/30441738.00/3222857.25
    528072.88/828428.62/356031.97 597410.50/973155.88/1280478.12 878460.69/2175974.25/441401.94
    nan/nan/nan NaN 0.0005 29671.52/1795030.88/4653.35 28112.96/87515.64/1826.82 141110.14/204295.86/40146.11
    265457.25/741326.38/99882.45 944784.19/774538.25/395960.03 NaN 0.0001 23.92/45.68/25.72
    19.34/52.78/21.63 16.35/29.22/18.76 14.27/24.46/16.80 12.17/22.16/14.80 NaN 5e-05
    23.84/44.17/25.60 19.50/51.33/21.72 16.19/29.28/18.66 14.14/24.16/16.69 11.81/20.41/14.50
    NaN 1e-05 23.85/44.20/25.65 22.64/56.79/23.41 16.23/29.73/18.73 14.14/24.31/16.74
    11.77/20.27/14.52 8.24/13.82/11.10 5e-06 24.02/44.62/25.79 23.46/63.27/24.88 16.28/29.83/18.81
    14.19/24.38/16.80 11.77/20.33/14.54 8.24/13.82/11.10 1e-06 24.46/45.41/26.20 24.62/70.16/26.64
    16.48/30.15/19.02 14.35/24.56/16.95 11.89/20.54/14.67 8.23/13.82/11.12 5e-07 NaN
    NaN NaN NaN NaN 8.26/13.86/11.13
- en: 'Table E.9: OPT ppl on wikitext/opt/c4 with W4${}^{\text{asym}}$. See Table [E.10](#A5.T10
    "Table E.10 ‣ Appendix E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation") for all
    learning rate results of ZQ-Local and Table [E.11](#A5.T11 "Table E.11 ‣ Appendix
    E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs
    from Comprehensive Study to Low Rank Compensation") of ZQ-Global.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.9: 使用 W4${}^{\text{asym}}$ 的 wikitext/opt/c4 上的 OPT ppl。请参见表 [E.10](#A5.T10
    "表 E.10 ‣ 附录 E 表格和图示 ‣ ZeroQuant-V2: 从全面研究到低秩补偿的后训练量化探索") 获取 ZQ-Local 的所有学习率结果，以及表
    [E.11](#A5.T11 "表 E.11 ‣ 附录 E 表格和图示 ‣ ZeroQuant-V2: 从全面研究到低秩补偿的后训练量化探索") 获取 ZQ-Global
    的结果。'
- en: Precision 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b W4${}^{\text{asym}}$ Block RTN
    36.61/44.48/30.64 25.79/31.28/24.13 21.23/23.54/19.19 23.82/29.77/22.60 13.18/17.04/14.19
    19.87/32.93/26.28 36.07/136.88/85.84 627.15/880.79/937.08 GPTQ 32.22/38.83/27.43
    23.90/29.29/22.63 15.75/18.74/15.93 13.23/16.31/14.03 12.50/15.86/13.29 12.79/21.99/17.05
    12.96/25.03/24.14 495.70/681.68/768.69 ZQ-Local^∗ 33.60/38.57/28.02 24.57/29.27/22.98
    15.98/19.13/16.20 13.44/16.81/14.26 11.76/14.97/13.00 11.69/16.98/14.01 12.38/24.25/18.96
    12.19/23.31/13.47 ZQ-Global^∗ 31.61/37.00/27.10 23.66/28.56/22.21 15.77/18.61/15.83
    13.09/16.56/14.00 12.03/14.60/12.86 11.80/15.01/12.41 12.94/17.61/13.41 31.51/58.00/23.95
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b W4${}^{\text{asym}}$ Block RTN 36.61/44.48/30.64
    25.79/31.28/24.13 21.23/23.54/19.19 23.82/29.77/22.60 13.18/17.04/14.19 19.87/32.93/26.28
    36.07/136.88/85.84 627.15/880.79/937.08 GPTQ 32.22/38.83/27.43 23.90/29.29/22.63
    15.75/18.74/15.93 13.23/16.31/14.03 12.50/15.86/13.29 12.79/21.99/17.05 12.96/25.03/24.14
    495.70/681.68/768.69 ZQ-Local^∗ 33.60/38.57/28.02 24.57/29.27/22.98 15.98/19.13/16.20
    13.44/16.81/14.26 11.76/14.97/13.00 11.69/16.98/14.01 12.38/24.25/18.96 12.19/23.31/13.47
    ZQ-Global^∗ 31.61/37.00/27.10 23.66/28.56/22.21 15.77/18.61/15.83 13.09/16.56/14.00
    12.03/14.60/12.86 11.80/15.01/12.41 12.94/17.61/13.41 31.51/58.00/23.95
- en: 'Table E.10: OPT ppl on wikitext/opt/c4 with W4${}^{\text{asym}}$ and ZQ-Local.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.10: 使用 W4${}^{\text{asym}}$ 和 ZQ-Local 的 wikitext/opt/c4 上的 OPT ppl。'
- en: Precision 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b W4${}^{\text{asym}}$ Block 0.001
    33.57/40.84/29.00 27.29/32.48/24.68 17.41/20.70/17.07 15.98/20.45/16.23 12.63/17.21/14.25
    9889.96/7605.54/6328.91 2009.66/1637.69/2011.15 5070.07/3124.56/2683.19 0.0005
    34.58/40.45/28.69 25.81/31.56/24.09 16.89/20.66/16.93 15.00/19.47/15.61 12.55/17.00/14.29
    13.18/19.65/15.18 36.51/75.89/60.58 3249.10/63.17/119.55 0.0001 33.91/38.39/28.12
    25.37/31.24/23.66 16.78/20.09/16.72 14.26/18.49/14.90 12.13/15.97/13.48 13.48/20.42/16.68
    110.20/117.28/257.96 12.19/23.31/13.47 5e-05 33.60/38.57/28.02 24.67/29.60/23.34
    16.31/19.56/16.42 13.90/19.16/15.05 12.30/15.95/13.56 12.05/18.00/15.77 37.68/59.83/124.75
    29.72/95.99/69.60 1e-05 33.80/40.21/28.56 24.57/29.27/22.98 15.98/19.13/16.20
    13.44/16.81/14.26 11.76/14.97/13.00 11.69/16.98/14.01 14.39/31.47/24.45 217.93/313.13/298.24
    5e-06 34.62/41.07/28.93 24.68/29.46/23.12 16.26/19.23/16.27 13.44/17.00/14.36
    11.96/14.86/13.10 12.31/18.55/15.16 12.38/24.25/18.96 85.96/185.07/180.88 1e-06
    35.94/43.35/30.00 24.92/30.18/23.45 17.98/20.89/17.45 14.79/18.90/15.52 12.10/15.47/13.35
    15.48/22.00/17.84 14.86/31.16/26.21 411.89/620.52/652.55
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b W4${}^{\text{asym}}$ 块 0.001 33.57/40.84/29.00
    27.29/32.48/24.68 17.41/20.70/17.07 15.98/20.45/16.23 12.63/17.21/14.25 9889.96/7605.54/6328.91
    2009.66/1637.69/2011.15 5070.07/3124.56/2683.19 0.0005 34.58/40.45/28.69 25.81/31.56/24.09
    16.89/20.66/16.93 15.00/19.47/15.61 12.55/17.00/14.29 13.18/19.65/15.18 36.51/75.89/60.58
    3249.10/63.17/119.55 0.0001 33.91/38.39/28.12 25.37/31.24/23.66 16.78/20.09/16.72
    14.26/18.49/14.90 12.13/15.97/13.48 13.48/20.42/16.68 110.20/117.28/257.96 12.19/23.31/13.47
    5e-05 33.60/38.57/28.02 24.67/29.60/23.34 16.31/19.56/16.42 13.90/19.16/15.05
    12.30/15.95/13.56 12.05/18.00/15.77 37.68/59.83/124.75 29.72/95.99/69.60 1e-05
    33.80/40.21/28.56 24.57/29.27/22.98 15.98/19.13/16.20 13.44/16.81/14.26 11.76/14.97/13.00
    11.69/16.98/14.01 14.39/31.47/24.45 217.93/313.13/298.24 5e-06 34.62/41.07/28.93
    24.68/29.46/23.12 16.26/19.23/16.27 13.44/17.00/14.36 11.96/14.86/13.10 12.31/18.55/15.16
    12.38/24.25/18.96 85.96/185.07/180.88 1e-06 35.94/43.35/30.00 24.92/30.18/23.45
    17.98/20.89/17.45 14.79/18.90/15.52 12.10/15.47/13.35 15.48/22.00/17.84 14.86/31.16/26.21
    411.89/620.52/652.55
- en: 'Table E.11: OPT ppl on wikitext/opt/c4 with W4${}^{\text{asym}}$ and ZQ-Global.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.11: 在 wikitext/opt/c4 上使用 W4${}^{\text{asym}}$ 和 ZQ-Global 的 OPT ppl。'
- en: Precision 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b W4${}^{\text{asym}}$ Block 0.001
    37.89/47.68/30.43 9023.01/4309.50/1186.96 12638.86/nan/9164.64 11285.86/6477.19/nan
    12222.01/6933.34/8989.30 132962.69/73768.05/59268.76 328993.91/187752.97/163157.59
    48298.52/30548.89/42797.96 0.0005 32.65/39.86/27.20 28.46/36.94/24.68 nan/nan/nan
    nan/nan/nan 23287.96/15508.32/16243.28 22052.30/10852.90/11588.02 63084.59/39919.41/42499.90
    NaN 0.0001 31.61/37.00/27.10 24.64/29.13/22.28 16.31/19.71/16.44 43.76/29.11/33.35
    22024.01/13962.04/14130.94 10171.49/7200.78/7954.12 18603.08/11639.42/10798.26
    nan/nan/nan 5e-05 32.21/37.46/27.18 23.66/28.56/22.21 16.02/19.02/15.92 13.48/17.57/14.24
    839.48/213.76/286.05 1035.13/nan/1472.08 8085.92/3545.21/4893.07 nan/nan/nan 1e-05
    32.35/38.21/27.38 23.59/28.66/22.24 15.77/18.61/15.83 13.09/16.56/14.00 12.09/14.69/12.90
    11.80/15.01/12.41 13.76/22.87/15.72 974.58/1557.95/1039.65 5e-06 32.59/38.49/27.68
    23.62/28.63/22.33 15.78/18.80/15.95 13.23/16.65/14.12 12.03/14.60/12.86 12.72/16.31/13.20
    12.94/17.61/13.41 83.35/137.83/128.11 1e-06 34.68/41.56/29.26 24.08/29.21/22.68
    16.66/20.03/16.69 13.30/16.74/14.33 12.43/15.52/13.36 12.28/16.13/13.19 16.00/19.60/14.88
    31.51/58.00/23.95 5e-07 NaN NaN NaN NaN NaN NaN NaN 31.09/73.23/24.44 1e-07 NaN
    NaN NaN NaN NaN NaN NaN 241.81/544.81/505.58
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b W4${}^{\text{asym}}$ 块 0.001 37.89/47.68/30.43
    9023.01/4309.50/1186.96 12638.86/nan/9164.64 11285.86/6477.19/nan 12222.01/6933.34/8989.30
    132962.69/73768.05/59268.76 328993.91/187752.97/163157.59 48298.52/30548.89/42797.96
    0.0005 32.65/39.86/27.20 28.46/36.94/24.68 nan/nan/nan nan/nan/nan 23287.96/15508.32/16243.28
    22052.30/10852.90/11588.02 63084.59/39919.41/42499.90 NaN 0.0001 31.61/37.00/27.10
    24.64/29.13/22.28 16.31/19.71/16.44 43.76/29.11/33.35 22024.01/13962.04/14130.94
    10171.49/7200.78/7954.12 18603.08/11639.42/10798.26 nan/nan/nan 5e-05 32.21/37.46/27.18
    23.66/28.56/22.21 16.02/19.02/15.92 13.48/17.57/14.24 839.48/213.76/286.05 1035.13/nan/1472.08
    8085.92/3545.21/4893.07 nan/nan/nan 1e-05 32.35/38.21/27.38 23.59/28.66/22.24
    15.77/18.61/15.83 13.09/16.56/14.00 12.09/14.69/12.90 11.80/15.01/12.41 13.76/22.87/15.72
    974.58/1557.95/1039.65 5e-06 32.59/38.49/27.68 23.62/28.63/22.33 15.78/18.80/15.95
    13.23/16.65/14.12 12.03/14.60/12.86 12.72/16.31/13.20 12.94/17.61/13.41 83.35/137.83/128.11
    1e-06 34.68/41.56/29.26 24.08/29.21/22.68 16.66/20.03/16.69 13.30/16.74/14.33
    12.43/15.52/13.36 12.28/16.13/13.19 16.00/19.60/14.88 31.51/58.00/23.95 5e-07
    NaN NaN NaN NaN NaN NaN NaN 31.09/73.23/24.44 1e-07 NaN NaN NaN NaN NaN NaN NaN
    241.81/544.81/505.58
- en: 'Table E.12: BLOOM ppl on wikitext/opt/c4 with W4${}^{\text{asym}}$. See Table [E.13](#A5.T13
    "Table E.13 ‣ Appendix E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training
    Quantization in LLMs from Comprehensive Study to Low Rank Compensation") for all
    learning rate results of ZQ-Local and Table [E.14](#A5.T14 "Table E.14 ‣ Appendix
    E Tables and Figures ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs
    from Comprehensive Study to Low Rank Compensation") of ZQ-Global.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.12: BLOOM ppl 在 wikitext/opt/c4 上，使用 W4${}^{\text{asym}}$。有关 ZQ-Local 所有学习率结果，请参见 表 [E.13](#A5.T13
    "表 E.13 ‣ 附录 E 表格和图形 ‣ ZeroQuant-V2: 从综合研究到低秩补偿的后训练量化探索")，有关 ZQ-Global 的信息，请参见 表 [E.14](#A5.T14
    "表 E.14 ‣ 附录 E 表格和图形 ‣ ZeroQuant-V2: 从综合研究到低秩补偿的后训练量化探索")。'
- en: Precision 560m 1.1b 1.7b 3b 7.1b 176b W4${}^{\text{asym}}$ Block RTN 25.37/46.99/27.16
    24.08/68.95/26.17 17.12/31.46/19.67 14.74/25.38/17.37 12.22/21.36/15.00 8.73/15.10/12.83
    GPTQ 24.09/44.29/25.66 24.50/67.37/26.62 16.39/29.83/18.91 14.13/24.47/16.73 11.91/20.72/14.62
    8.55/14.74/12.31 ZQ-Local^∗ 24.29/45.19/26.10 19.13/52.89/21.63 16.54/30.11/18.92
    14.32/24.73/16.94 11.94/20.63/14.68 8.33/14.01/11.22 ZQ-Global^∗ 23.86/44.16/25.62
    19.54/51.72/21.79 16.23/29.40/18.68 14.15/24.29/16.72 11.80/20.37/14.56 8.62/14.40/11.49
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 560m 1.1b 1.7b 3b 7.1b 176b W4${}^{\text{asym}}$ 区块 RTN 25.37/46.99/27.16
    24.08/68.95/26.17 17.12/31.46/19.67 14.74/25.38/17.37 12.22/21.36/15.00 8.73/15.10/12.83
    GPTQ 24.09/44.29/25.66 24.50/67.37/26.62 16.39/29.83/18.91 14.13/24.47/16.73 11.91/20.72/14.62
    8.55/14.74/12.31 ZQ-Local^∗ 24.29/45.19/26.10 19.13/52.89/21.63 16.54/30.11/18.92
    14.32/24.73/16.94 11.94/20.63/14.68 8.33/14.01/11.22 ZQ-Global^∗ 23.86/44.16/25.62
    19.54/51.72/21.79 16.23/29.40/18.68 14.15/24.29/16.72 11.80/20.37/14.56 8.62/14.40/11.49
- en: 'Table E.13: BLOOM ppl on wikitext/opt/c4 with W4${}^{\text{asym}}$ and ZQ-Local.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.13: BLOOM ppl 在 wikitext/opt/c4 上，使用 W4${}^{\text{asym}}$ 和 ZQ-Local。'
- en: Precision 560m 1.1b 1.7b 3b 7.1b 176b W4${}^{\text{asym}}$ Block 0.001 25.26/46.43/26.98
    19.69/54.26/22.14 16.88/32.16/19.40 15.15/26.58/17.76 12.40/22.29/15.28 8.40/14.06/11.26
    0.0005 24.89/47.99/26.82 19.54/53.57/21.98 16.73/31.02/19.29 14.50/25.52/17.11
    11.94/20.70/14.76 8.33/14.01/11.22 0.0001 24.60/45.75/26.44 19.13/52.89/21.63
    16.54/30.36/19.10 14.37/24.91/16.93 11.94/20.63/14.68 8.35/14.04/11.24 5e-05 24.41/45.08/26.23
    23.59/67.14/25.79 16.54/30.11/18.92 14.29/24.83/16.92 11.95/20.71/14.71 8.36/14.10/11.25
    1e-05 24.29/45.19/26.10 23.35/65.26/25.38 16.51/30.20/19.00 14.32/24.73/16.94
    11.97/20.93/14.74 8.44/14.30/11.45 5e-06 24.31/45.25/26.15 23.41/66.18/25.48 16.63/30.37/19.09
    14.33/24.74/16.96 12.03/20.95/14.78 8.52/14.66/11.86 1e-06 24.76/45.92/26.62 23.52/66.38/25.66
    16.81/30.71/19.30 14.53/24.92/17.14 12.10/21.07/14.87 8.62/14.92/12.41
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 560m 1.1b 1.7b 3b 7.1b 176b W4${}^{\text{asym}}$ 区块 0.001 25.26/46.43/26.98
    19.69/54.26/22.14 16.88/32.16/19.40 15.15/26.58/17.76 12.40/22.29/15.28 8.40/14.06/11.26
    0.0005 24.89/47.99/26.82 19.54/53.57/21.98 16.73/31.02/19.29 14.50/25.52/17.11
    11.94/20.70/14.76 8.33/14.01/11.22 0.0001 24.60/45.75/26.44 19.13/52.89/21.63
    16.54/30.36/19.10 14.37/24.91/16.93 11.94/20.63/14.68 8.35/14.04/11.24 5e-05 24.41/45.08/26.23
    23.59/67.14/25.79 16.54/30.11/18.92 14.29/24.83/16.92 11.95/20.71/14.71 8.36/14.10/11.25
    1e-05 24.29/45.19/26.10 23.35/65.26/25.38 16.51/30.20/19.00 14.32/24.73/16.94
    11.97/20.93/14.74 8.44/14.30/11.45 5e-06 24.31/45.25/26.15 23.41/66.18/25.48 16.63/30.37/19.09
    14.33/24.74/16.96 12.03/20.95/14.78 8.52/14.66/11.86 1e-06 24.76/45.92/26.62 23.52/66.38/25.66
    16.81/30.71/19.30 14.53/24.92/17.14 12.10/21.07/14.87 8.62/14.92/12.41
- en: 'Table E.14: BLOOM ppl on wikitext/opt/c4 with W4${}^{\text{asym}}$ and ZQ-Global.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.14: BLOOM ppl 在 wikitext/opt/c4 上，使用 W4${}^{\text{asym}}$ 和 ZQ-Global。'
- en: Precision 560m 1.1b 1.7b 3b 7.1b 176b W4${}^{\text{asym}}$ Block 0.001 9059092.00/2932002.50/131873960.00
    499829.19/393190.53/346682.47 1260531.12/2019747.88/460627.16 1022130.19/872164.88/679662.62
    nan/nan/nan NaN 0.0005 7633.14/378055.53/1032.16 4271.83/85847.50/1555.66 87087.04/217513.30/37000.13
    575008.56/814032.50/230285.80 1212241.00/2389840.25/1504266.50 NaN 0.0001 23.96/45.36/25.80
    19.37/52.25/21.88 16.29/29.36/18.81 14.32/24.66/16.86 12.05/22.30/14.77 1400.84/11880.12/392.79
    5e-05 23.86/44.16/25.62 19.54/51.72/21.79 16.23/29.40/18.68 14.15/24.29/16.72
    11.82/20.44/14.54 8.73/20.30/11.41 1e-05 23.96/44.24/25.72 22.55/58.10/23.49 16.27/29.82/18.78
    14.16/24.35/16.80 11.80/20.37/14.56 8.62/14.40/11.49 5e-06 24.01/44.68/25.83 23.67/64.20/25.08
    16.30/29.96/18.85 14.24/24.49/16.86 11.81/20.50/14.60 8.69/14.56/11.58 1e-06 24.53/45.60/26.26
    24.82/71.17/26.84 16.55/30.35/19.10 14.40/24.76/17.01 11.97/20.83/14.77 9.14/16.63/17.69
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 560m 1.1b 1.7b 3b 7.1b 176b W4${}^{\text{asym}}$ 区块 0.001 9059092.00/2932002.50/131873960.00
    499829.19/393190.53/346682.47 1260531.12/2019747.88/460627.16 1022130.19/872164.88/679662.62
    nan/nan/nan NaN 0.0005 7633.14/378055.53/1032.16 4271.83/85847.50/1555.66 87087.04/217513.30/37000.13
    575008.56/814032.50/230285.80 1212241.00/2389840.25/1504266.50 NaN 0.0001 23.96/45.36/25.80
    19.37/52.25/21.88 16.29/29.36/18.81 14.32/24.66/16.86 12.05/22.30/14.77 1400.84/11880.12/392.79
    5e-05 23.86/44.16/25.62 19.54/51.72/21.79 16.23/29.40/18.68 14.15/24.29/16.72
    11.82/20.44/14.54 8.73/20.30/11.41 1e-05 23.96/44.24/25.72 22.55/58.10/23.49 16.27/29.82/18.78
    14.16/24.35/16.80 11.80/20.37/14.56 8.62/14.40/11.49 5e-06 24.01/44.68/25.83 23.67/64.20/25.08
    16.30/29.96/18.85 14.24/24.49/16.86 11.81/20.50/14.60 8.69/14.56/11.58 1e-06 24.53/45.60/26.26
    24.82/71.17/26.84 16.55/30.35/19.10 14.40/24.76/17.01 11.97/20.83/14.77 9.14/16.63/17.69
- en: 'Table E.15: OPT full results of Table [4](#S4.T4 "Table 4 ‣ 4-bit Weight Quantization.
    ‣ 4.1 Fine-grained Quantization and Its Evaluation ‣ 4 Are existing quantization
    methods optimally harnessing the potential to minimize LLMs sizes? ‣ ZeroQuant-V2:
    Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank
    Compensation").'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 表 E.15：表格 [4](#S4.T4 "表 4 ‣ 4 位权重量化 ‣ 4.1 细粒度量化及其评估 ‣ 现有量化方法是否有效利用了减少 LLM 尺寸的潜力？
    ‣ ZeroQuant-V2：从综合研究到低秩补偿，探索 LLM 的后训练量化") 的 OPT 完整结果。
- en: Method 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b BS=1024 RTN N/A 25.42/30.62/23.61
    16.90/19.78/16.59 N/A 11.63/14.41/12.65 10.47/13.09/11.75 9.97/12.40/11.09 9.83/12.31/10.77
    N/A 26.55 17.76 N/A 12.90 11.77 11.15 10.97 GPTQ N/A 23.65/29.09/22.43 15.16/18.00/15.34
    N/A 11.10/13.40/11.99 10.28/12.49/11.29 9.58/11.91/10.75 9.56/11.61/10.44 N/A
    25.05 16.17 N/A 12.16 11.36 10.75 10.54 ZQ-Global^∗ N/A 23.27/27.97/21.93 12.93/15.90/13.64
    N/A 10.98/13.60/12.04 10.33/12.69/11.50 9.78/12.16/10.90 9.52/11.58/10.46 N/A
    24.39 16.18 N/A 12.21 11.50 10.95 10.52 BS=512 RTN N/A 25.05/29.74/23.21 15.71/19.05/16.09
    13.67/16.93/14.23 11.32/14.22/12.50 10.45/12.99/11.68 10.03/12.27/11.03 9.83/12.15/10.67
    N/A 26.00 16.95 14.94 12.68 11.71 11.11 10.89 GPTQ N/A 23.33/28.48/22.13 15.15/17.95/15.26
    12.65/15.61/13.53 10.94/13.37/11.94 10.18/12.49/11.29 9.58/11.87/10.75 9.53/11.59/10.43
    N/A 24.65 16.12 13.93 12.08 11.32 10.73 10.52 ZQ-Global^∗ N/A 23.41/27.67/21.92
    14.91/17.73/15.25 12.92/15.59/13.55 11.08/13.51/11.99 10.29/12.68/11.46 9.79/12.16/10.87
    9.51/11.65/10.44 N/A 24.34 15.97 14.02 12.19 11.48 10.94 10.53 BS=256 RTN 31.62/38.19/27.62
    24.76/29.44/22.96 15.54/18.96/15.90 13.56/16.62/14.02 11.19/14.12/12.40 10.39/12.93/11.61
    9.95/12.24/10.98 9.70/12.09/10.62 32.48 25.72 16.80 14.73 12.57 11.64 11.06 10.80
    GPTQ 30.56/37.20/26.68 23.37/28.33/21.97 14.95/17.63/15.16 12.59/15.60/13.49 10.93/13.29/11.92
    10.15/12.43/11.27 9.58/11.91/10.74 9.49/11.60/10.40 31.48 24.56 15.91 13.89 12.05
    11.28 10.74 10.50 ZQ-Global^∗ 30.45/35.35/26.24 23.06/27.72/21.74 14.93/17.45/15.15
    12.99/15.47/13.50 10.96/13.45/12.00 10.25/12.61/11.43 9.73/12.14/10.89 9.49/11.58/10.42
    30.68 24.17 15.84 13.99 12.14 11.43 10.92 10.50 BS=128 RTN 30.62/36.67/27.10 24.12/29.34/22.70
    15.35/18.52/15.66 13.19/16.24/13.88 11.11/13.94/12.28 10.31/12.82/11.54 9.93/12.12/10.93
    9.56/11.85/10.56 31.47 25.39 16.51 14.43 12.44 11.56 11.00 10.65 GPTQ 30.76/36.13/26.52
    23.29/27.94/21.98 14.93/17.51/15.10 12.49/15.59/13.46 10.87/13.34/11.90 10.11/12.47/11.27
    9.60/11.88/10.73 9.44/11.53/10.40 31.14 24.40 15.85 13.85 12.03 11.28 10.74 10.45
    ZQ-Global^∗ 29.52/34.63/25.98 22.78/27.56/21.65 15.02/17.50/15.07 12.67/15.37/13.45
    10.92/13.42/11.96 10.16/12.61/11.41 9.74/12.01/10.82 9.43/11.49/10.40 30.04 23.99
    15.86 13.83 12.10 11.39 10.86 10.44 BS=64 RTN 30.74/36.68/26.87 24.28/28.95/22.59
    15.21/18.15/15.47 13.20/16.13/13.75 11.01/13.71/12.17 10.27/12.79/11.49 9.82/12.05/10.89
    9.46/11.70/10.49 31.43 25.27 16.28 14.36 12.30 11.52 10.92 10.55 GPTQ 30.25/35.72/26.43
    23.39/27.55/21.75 14.81/17.40/15.06 12.54/15.54/13.44 10.87/13.29/11.89 10.09/12.44/11.27
    9.55/11.89/10.72 9.33/11.49/10.38 30.80 24.23 15.76 13.84 12.02 11.27 10.72 10.40
    ZQ-Global^∗ 29.69/34.24/25.72 22.94/27.49/21.54 14.90/17.43/15.01 12.80/15.47/13.44
    10.92/13.33/11.93 10.21/12.58/11.38 9.69/12.01/10.81 9.41/11.49/10.39 29.88 23.99
    15.78 13.90 12.06 11.39 10.84 10.43 BS=32 RTN 30.48/36.32/26.64 23.88/28.66/22.36
    14.99/17.87/15.32 12.89/16.00/13.67 10.89/13.70/12.13 10.32/12.73/11.45 9.76/12.00/10.85
    9.56/11.55/10.44 31.14 24.97 16.06 14.18 12.24 11.50 10.87 10.52 GPTQ 29.13/34.89/25.90
    23.09/27.59/21.65 14.80/17.41/15.04 12.45/15.55/13.42 10.89/13.32/11.89 10.08/12.48/11.27
    9.51/11.92/10.73 Diverge 29.97 24.11 15.75 13.81 12.03 11.28 10.72 Diverge ZQ-Global^∗
    28.93/34.29/25.63 22.85/27.23/21.50 14.80/17.34/14.99 12.74/15.32/13.40 10.82/13.36/11.91
    10.23/12.61/11.37 9.68/11.95/10.80 9.37/11.47/10.38 29.62 23.86 15.71 13.82 12.03
    11.41 10.81 10.41
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b BS=1024 RTN N/A 25.42/30.62/23.61 16.90/19.78/16.59
    N/A 11.63/14.41/12.65 10.47/13.09/11.75 9.97/12.40/11.09 9.83/12.31/10.77 N/A
    26.55 17.76 N/A 12.90 11.77 11.15 10.97 GPTQ N/A 23.65/29.09/22.43 15.16/18.00/15.34
    N/A 11.10/13.40/11.99 10.28/12.49/11.29 9.58/11.91/10.75 9.56/11.61/10.44 N/A
    25.05 16.17 N/A 12.16 11.36 10.75 10.54 ZQ-Global^∗ N/A 23.27/27.97/21.93 12.93/15.90/13.64
    N/A 10.98/13.60/12.04 10.33/12.69/11.50 9.78/12.16/10.90 9.52/11.58/10.46 N/A
    24.39 16.18 N/A 12.21 11.50 10.95 10.52 BS=512 RTN N/A 25.05/29.74/23.21 15.71/19.05/16.09
    13.67/16.93/14.23 11.32/14.22/12.50 10.45/12.99/11.68 10.03/12.27/11.03 9.83/12.15/10.67
    N/A 26.00 16.95 14.94 12.68 11.71 11.11 10.89 GPTQ N/A 23.33/28.48/22.13 15.15/17.95/15.26
    12.65/15.61/13.53 10.94/13.37/11.94 10.18/12.49/11.29 9.58/11.87/10.75 9.53/11.59/10.43
    N/A 24.65 16.12 13.93 12.08 11.32 10.73 10.52 ZQ-Global^∗ N/A 23.41/27.67/21.92
    14.91/17.73/15.25 12.92/15.59/13.55 11.08/13.51/11.99 10.29/12.68/11.46 9.79/12.16/10.87
    9.51/11.65/10.44 N/A 24.34 15.97 14.02 12.19 11.48 10.94 10.53 BS=256 RTN 31.62/38.19/27.62
    24.76/29.44/22.96 15.54/18.96/15.90 13.56/16.62/14.02 11.19/14.12/12.40 10.39/12.93/11.61
    9.95/12.24/10.98 9.70/12.09/10.62 32.48 25.72 16.80 14.73 12.57 11.64 11.06 10.80
    GPTQ 30.56/37.20/26.68 23.37/28.33/21.97 14.95/17.63/15.16 12.59/15.60/13.49 10.93/13.29/11.92
    10.15/12.43/11.27 9.58/11.91/10.74 9.49/11.60/10.40 31.48 24.56 15.91 13.89 12.05
    11.28 10.74 10.50 ZQ-Global^∗ 30.45/35.35/26.24 23.06/27.72/21.74 14.93/17.45/15.15
    12.99/15.47/13.50 10.96/13.45/12.00 10.25/12.61/11.43 9.73/12.14/10.89 9.49/11.58/10.42
    30.68 24.17 15.84 13.99 12.14 11.43 10.92 10.50 BS=128 RTN 30.62/36.67/27.10 24.12/29.34/22.70
    15.35/18.52/15.66 13.19/16.24/13.88 11.11/13.94/12.28 10.31/12.82/11.54 9.93/12.12/10.93
    9.56/11.85/10.56 31.47 25.39 16.51 14.43 12.44 11.56 11.00 10.65 GPTQ 30.76/36.13/26.52
    23.29/27.94/21.98 14.93/17.51/15.10 12.49/15.59/13.46 10.87/13.34/11.90 10.11/12.47/11.27
    9.60/11.88/10.73 9.44/11.53/10.40 31.14 24.40 15.85 13.85 12.03 11.28 10.74 10.45
    ZQ-Global^∗ 29.52/34.63/25.98 22.78/27.56/21.65 15.02/17.50/15.07 12.67/15.37/13.45
    10.92/13.42/11.96 10.16/12.61/11.41 9.74/12.01/10.82 9.43/11.49/10.40 30.04 23.99
    15.86 13.83 12.10 11.39 10.86 10.44 BS=64 RTN 30.74/36.68/26.87 24.28/28.95/22.59
    15.21/18.15/15.47 13.20/16.13/13.75 11.01/13.71/12.17 10.27/12.79/11.49 9.82/12.05/10.89
    9.46/11.70/10.49 31.43 25.27 16.28 14.36 12.30 11.52 10.92 10.55 GPTQ 30.25/35.72/26.43
    23.39/27.55/21.75 14.81/17.40/15.06 12.54/15.54/13.44 10.87/13.29/11.89 10.09/12.44/11.27
    9.55/11.89/10.72 9.33/11.49/10.38 30.80 24.23 15.76 13.84 12.02 11.27 10.72 10.40
    ZQ-Global^∗ 29.69/34.24/25.72 22.94/27.49/21.54 14.90/17.43/15.01 12.80/15.47/13.44
    10.92/13.33/11.93 10.21/12.58/11.38 9.69/12.01/10.81 9.41/11.49/10.39 29.88 23.99
    15.78 13.90 12.06 11.39 10.84 10.43 BS=32 RTN 30.48/36.32/26.64 23.88/28.66/22.36
    14.99/17.87/15.32 12.89/16.00/13.67 10.89/13.70/12.13 10.32/12.73/11.45 9.76/12.00/10.85
    9.56/11.55/10.44 31.14 24.97 16.06 14.18 12.24 11.50 10.87 10
- en: 'Table E.16: BLOOM W4${}^{\text{asym}}$-A16 with various block-size out of the
    best result from GPTQ and ZQ-Global.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.16: BLOOM W4${}^{\text{asym}}$-A16 在不同块大小下的最佳结果，来源于 GPTQ 和 ZQ-Global。'
- en: Method 560m 1.1b 1.7b 3b 7.1b 176b BS=1024 RTN 24.90/46.37/26.68 N/A 16.57/30.14/19.00
    N/A 1019.51/1351.45/601.35 53.41/160.05/43.64 32.65 N/A 21.90 N/A 990.77 85.70
    GPTQ 23.90/43.99/25.47 N/A 16.12/29.13/18.61 N/A 11.57/19.82/14.33 8.16/13.70/11.02
    31.12 N/A 21.29 N/A 15.24 10.96 ZQ-Global 23.62/43.90/25.41 N/A 15.98/28.67/18.44
    N/A 11.91/20.84/14.58 8.23/13.94/11.09 30.98 N/A 21.03 N/A 15.78 11.09 BS=512
    RTN 24.78/46.07/26.45 19.41/53.64/21.85 16.47/29.84/18.88 14.29/24.84/17.05 142.38/314.10/100.09
    33.88/103.57/31.02 32.44 31.63 21.73 18.73 185.52 56.16 GPTQ 23.63/43.96/25.36
    18.52/49.73/20.91 16.07/29.87/18.50 13.79/23.77/16.41 11.54/19.75/14.30 8.14/13.70/11.02
    30.98 29.72 21.48 17.99 15.20 10.95 ZQ-Global 23.50/43.53/25.23 18.31/49.06/20.82
    15.93/28.47/18.38 13.82/23.92/16.47 11.85/20.17/14.42 8.20/13.86/11.07 30.75 29.40
    20.93 18.07 15.48 11.04 BS=256 RTN 24.09/45.13/26.02 18.87/52.29/21.44 16.27/29.72/18.76
    14.16/24.42/16.90 121.09/281.67/88.59 12.55/27.29/15.60 31.75 30.87 21.58 18.49
    163.78 18.48 GPTQ 23.31/43.43/25.12 18.36/49.13/20.79 16.07/29.10/18.46 13.76/23.61/16.38
    11.55/19.72/14.29 8.14/13.70/11.01 30.62 29.42 21.21 17.92 15.18 10.95 ZQ-Global
    23.17/43.16/25.13 18.24/48.78/20.75 15.81/28.71/18.32 13.79/23.69/16.42 11.59/19.92/14.36
    8.17/13.80/11.06 30.49 29.26 20.95 17.97 15.29 11.01 BS=128 RTN 23.82/44.78/25.75
    18.62/51.31/21.17 16.13/29.89/18.66 14.00/24.19/16.71 23.90/49.80/24.15 8.84/15.62/11.70
    31.45 30.37 21.56 18.30 32.62 12.06 GPTQ 23.27/43.10/24.99 18.14/48.72/20.73 16.03/28.96/18.41
    13.72/23.65/16.34 11.52/19.73/14.26 8.14/13.67/11.01 30.45 29.20 21.13 17.90 15.17
    10.94 ZQ-Global 23.14/42.95/24.97 18.17/48.53/20.70 15.75/28.71/18.29 13.73/23.65/16.37
    11.56/19.77/14.32 8.17/13.78/11.03 30.35 29.13 20.92 17.92 15.22 10.99 BS=64 RTN
    23.65/44.04/25.51 18.53/50.02/21.03 16.06/29.57/18.60 13.93/23.95/16.60 11.85/20.51/14.65
    8.31/14.14/11.18 31.07 29.86 21.41 18.16 15.67 11.21 GPTQ 23.11/42.95/24.94 18.14/48.87/20.65
    16.00/28.91/18.38 13.72/23.68/16.33 11.51/19.70/14.27 8.14/13.69/11.00 30.33 29.22
    21.10 17.91 15.16 10.94 ZQ-Global 23.00/42.80/24.91 18.10/48.30/20.64 15.68/28.55/18.25
    13.70/23.63/16.36 11.53/19.67/14.27 8.17/13.72/11.02 30.24 29.01 20.82 17.90 15.16
    10.97 BS=32 RTN 23.60/43.91/25.50 18.63/50.13/21.04 15.98/29.56/18.56 13.92/23.90/16.53
    11.65/20.01/14.43 8.20/13.86/11.07 31.00 29.93 21.37 18.12 15.36 11.04 GPTQ 23.10/43.19/24.91
    18.17/48.35/20.66 15.95/28.95/18.36 13.76/23.60/16.33 11.53/19.71/14.27 8.14/13.70/11.00
    30.40 29.06 21.08 17.89 15.17 10.95 ZQ-Global 23.07/42.63/24.82 18.07/48.07/20.59
    15.66/28.58/18.21 13.72/23.59/16.33 11.52/19.71/14.26 8.16/13.69/11.01 30.18 28.91
    20.82 17.88 15.16 10.95
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 560m 1.1b 1.7b 3b 7.1b 176b BS=1024 RTN 24.90/46.37/26.68 不适用 16.57/30.14/19.00
    不适用 1019.51/1351.45/601.35 53.41/160.05/43.64 32.65 不适用 21.90 不适用 990.77 85.70
    GPTQ 23.90/43.99/25.47 不适用 16.12/29.13/18.61 不适用 11.57/19.82/14.33 8.16/13.70/11.02
    31.12 不适用 21.29 不适用 15.24 10.96 ZQ-Global 23.62/43.90/25.41 不适用 15.98/28.67/18.44
    不适用 11.91/20.84/14.58 8.23/13.94/11.09 30.98 不适用 21.03 不适用 15.78 11.09 BS=512
    RTN 24.78/46.07/26.45 19.41/53.64/21.85 16.47/29.84/18.88 14.29/24.84/17.05 142.38/314.10/100.09
    33.88/103.57/31.02 32.44 31.63 21.73 18.73 185.52 56.16 GPTQ 23.63/43.96/25.36
    18.52/49.73/20.91 16.07/29.87/18.50 13.79/23.77/16.41 11.54/19.75/14.30 8.14/13.70/11.02
    30.98 29.72 21.48 17.99 15.20 10.95 ZQ-Global 23.50/43.53/25.23 18.31/49.06/20.82
    15.93/28.47/18.38 13.82/23.92/16.47 11.85/20.17/14.42 8.20/13.86/11.07 30.75 29.40
    20.93 18.07 15.48 11.04 BS=256 RTN 24.09/45.13/26.02 18.87/52.29/21.44 16.27/29.72/18.76
    14.16/24.42/16.90 121.09/281.67/88.59 12.55/27.29/15.60 31.75 30.87 21.58 18.49
    163.78 18.48 GPTQ 23.31/43.43/25.12 18.36/49.13/20.79 16.07/29.10/18.46 13.76/23.61/16.38
    11.55/19.72/14.29 8.14/13.70/11.01 30.62 29.42 21.21 17.92 15.18 10.95 ZQ-Global
    23.17/43.16/25.13 18.24/48.78/20.75 15.81/28.71/18.32 13.79/23.69/16.42 11.59/19.92/14.36
    8.17/13.80/11.06 30.49 29.26 20.95 17.97 15.29 11.01 BS=128 RTN 23.82/44.78/25.75
    18.62/51.31/21.17 16.13/29.89/18.66 14.00/24.19/16.71 23.90/49.80/24.15 8.84/15.62/11.70
    31.45 30.37 21.56 18.30 32.62 12.06 GPTQ 23.27/43.10/24.99 18.14/48.72/20.73 16.03/28.96/18.41
    13.72/23.65/16.34 11.52/19.73/14.26 8.14/13.67/11.01 30.45 29.20 21.13 17.90 15.17
    10.94 ZQ-Global 23.14/42.95/24.97 18.17/48.53/20.70 15.75/28.71/18.29 13.73/23.65/16.37
    11.56/19.77/14.32 8.17/13.78/11.03 30.35 29.13 20.92 17.92 15.22 10.99 BS=64 RTN
    23.65/44.04/25.51 18.53/50.02/21.03 16.06/29.57/18.60 13.93/23.95/16.60 11.85/20.51/14.65
    8.31/14.14/11.18 31.07 29.86 21.41 18.16 15.67 11.21 GPTQ 23.11/42.95/24.94 18.14/48.87/20.65
    16.00/28.91/18.38 13.72/23.68/16.33 11.51/19.70/14.27 8.14/13.69/11.00 30.33 29.22
    21.10 17.91 15.16 10.94 ZQ-Global 23.00/42.80/24.91 18.10/48.30/20.64 15.68/28.55/18.25
    13.70/23.63/16.36 11.53/19.67/14.27 8.17/13.72/11.02 30.24 29.01 20.82 17.90 15.16
    10.97 BS=32 RTN 23.60/43.91/25.50 18.63/50.13/21.04 15.98/29.56/18.56 13.92/23.90/16.53
    11.65/20.01/14.43 8.20/13.86/11.07 31.00 29.93 21.37 18.12 15.36 11.04 GPTQ 23.10/43.19/24.91
    18.17/48.35/20.66 15.95/28.95/18.36 13.76/23.60/16.33 11.53/19.71/14.27 8.14/13.70/11.00
    30.40 29.06 21.08 17.89 15.17 10.95 ZQ-Global 23.07/42.63/24.82 18.07/48.07/20.59
    15.66/28.58/18.21 13.72/23.59/16.33 11.52/19.71/14.26 8.16/13.69/11.01 30.18 28.91
    20.82 17.88 15.16 10.95
- en: 'Table E.17: OPT full results of three-bit weight with various block-size.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.17: 三位权重的完整 OPT 结果，包含不同块大小。'
- en: Method 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b Full Row RTN 2095.20/1848.83/1222.00
    47.43/53.38/36.93 4399.18/4400.98/3551.88 8326.78/4208.57/4895.83 878.00/735.86/910.10
    1953.43/1953.60/1669.76 439.39/691.94/437.96 1465.06/1564.59/1282.58 1722.01 45.91
    4117.35 5810.40 841.32 1858.93 523.09 1437.41 GPTQ 845.81/599.71/496.14 30.65/34.09/26.15
    20.23/27.39/19.45 15.91/19.26/16.01 12.69/15.90/13.96 11.36/13.71/12.21 10.10/12.54/11.20
    16.77/21.16/15.39 647.22 30.30 22.36 17.06 14.18 12.43 11.28 17.77 ZQ-Global^∗
    46.47/58.55/35.45 29.64/36.51/25.55 32.48/94.57/28.97 60.91/116.22/36.45 23.87/29.75/23.88
    44.70/60.78/46.18 13.16/20.49/13.48 28.93/75.91/27.28 46.82 30.57 52.01 71.19
    25.83 50.55 15.71 44.04 BS=1024 RTN N/A 44.57/49.58/35.09 1950.00/2317.55/1913.55
    3810.79/2563.06/3054.91 50.01/70.17/99.21 265.62/417.03/261.93 362.47/252.33/364.45
    523.81/846.60/1021.17 N/A 43.08 2060.37 3142.92 73.13 314.86 326.42 797.20 GPTQ
    N/A 29.78/33.76/25.66 19.03/23.32/18.14 N/A 11.69/14.31/12.70 10.56/12.96/11.70
    9.89/12.19/11.02 12.84/16.17/13.02 N/A 29.73 20.16 N/A 12.90 11.74 11.03 14.01
    ZQ-Global^∗ N/A 29.19/34.57/25.11 19.83/29.77/19.79 N/A 13.99/18.82/14.76 13.43/19.28/13.76
    11.10/14.46/11.94 11.87/14.86/12.13 N/A 29.62 23.13 N/A 15.86 15.49 12.50 12.95
    BS=512 RTN N/A 37.74/45.10/31.85 1777.53/1304.55/852.03 1604.07/1407.49/1487.78
    25.13/40.56/40.08 130.75/175.33/135.67 620.53/340.68/416.28 198.01/457.78/426.15
    N/A 38.23 1311.37 1499.78 35.26 147.25 459.16 360.65 GPTQ N/A 28.46/32.54/25.14
    18.02/21.35/17.46 14.38/17.24/14.79 11.57/14.33/12.57 10.41/12.97/11.64 9.77/12.18/10.97
    11.89/14.48/12.40 N/A 28.71 18.94 15.47 12.82 11.67 10.97 12.92 ZQ-Global^∗ N/A
    27.81/33.57/24.55 18.31/23.54/17.99 18.10/29.47/17.15 12.54/16.60/13.62 11.82/15.98/12.81
    10.48/13.36/11.66 11.26/13.95/11.79 N/A 28.65 19.95 21.57 14.25 13.54 11.83 12.33
    BS=256 RTN 4349.14/2907.61/2510.75 35.36/42.07/30.81 127.17/358.19/142.49 670.51/550.66/531.80
    19.10/32.39/27.26 42.52/56.35/43.32 32.84/60.38/33.48 210.01/478.13/413.00 3255.84
    36.08 209.28 584.32 26.25 47.40 42.23 367.05 GPTQ 41.81/49.95/32.48 27.60/33.73/24.88
    16.97/20.19/16.70 13.69/17.06/14.54 11.65/14.24/12.48 10.35/12.93/11.61 9.66/12.10/10.93
    11.60/13.98/11.92 41.41 28.74 17.95 15.10 12.79 11.63 10.90 12.50 ZQ-Global^∗
    38.60/46.57/31.36 26.88/32.79/24.08 16.82/21.21/17.05 14.86/19.63/15.37 11.86/15.87/13.10
    11.33/14.95/12.48 10.41/12.95/11.41 10.26/12.66/11.08 38.85 27.92 18.36 16.62
    13.61 12.92 11.59 11.34 BS=128 RTN 3446.89/2156.26/1484.15 33.13/41.23/29.51 49.40/88.45/45.07
    153.68/155.21/113.98 16.34/26.86/21.98 17.80/25.95/18.28 45.83/43.91/57.50 106.84/241.02/212.94
    2362.43 34.62 60.97 140.96 21.72 20.67 49.08 186.93 GPTQ 40.00/45.73/31.15 27.68/34.04/25.18
    16.47/19.90/16.47 13.81/16.96/14.37 11.57/14.10/12.41 10.35/12.84/11.58 9.73/12.08/10.91
    10.96/13.27/11.45 38.96 28.97 17.61 15.05 12.69 11.59 10.91 11.90 ZQ-Global^∗
    36.57/43.88/29.94 25.75/31.59/23.57 16.28/20.20/16.67 14.27/18.41/14.90 11.70/15.05/12.68
    11.13/15.07/12.17 10.31/12.99/11.32 10.12/12.66/11.01 36.80 26.97 17.72 15.86
    13.14 12.79 11.54 11.27 BS=64 RTN 708.02/477.13/287.03 32.61/42.14/29.09 25.43/38.84/24.63
    72.84/69.27/48.07 14.11/21.71/16.56 14.13/20.08/15.25 20.55/32.74/24.49 30.66/70.73/65.57
    490.73 34.61 29.63 63.39 17.46 16.48 25.93 55.65 GPTQ 37.15/42.59/30.07 27.68/33.55/25.12
    16.25/19.80/16.32 13.66/16.69/14.37 11.42/13.98/12.37 10.37/12.90/11.58 9.68/12.17/10.92
    10.39/12.65/11.15 36.60 28.78 17.46 14.91 12.59 11.62 10.92 11.40 ZQ-Global^∗
    35.82/40.98/29.65 25.31/31.60/23.38 16.05/19.77/16.39 13.33/16.92/14.31 11.56/14.70/12.59
    10.88/13.64/12.04 10.04/12.70/11.27 10.04/12.06/10.81 35.48 26.76 17.40 14.85
    12.95 12.19 11.34 10.97 BS=32 RTN 72.83/88.62/54.25 32.36/40.76/29.06 20.22/27.31/19.81
    31.12/42.01/26.83 13.38/18.56/15.44 13.06/18.35/14.38 11.12/15.05/12.35 19.29/43.61/34.10
    71.90 34.06 22.44 33.32 15.79 15.26 12.84 32.33 GPTQ 38.26/45.01/30.92 27.16/33.65/24.97
    16.13/19.83/16.45 13.66/17.06/14.50 11.43/14.08/12.42 10.48/12.96/11.65 9.78/12.24/10.96
    Diverge 38.06 28.59 17.47 15.07 12.64 11.70 10.99 Diverge ZQ-Global^∗ 33.44/39.48/28.33
    25.19/30.73/23.22 15.62/19.52/16.20 13.35/16.64/14.18 11.56/14.38/12.61 10.86/13.64/12.03
    10.25/12.86/11.28 9.99/12.05/10.81 33.75 26.38 17.11 14.73 12.85 12.17 11.46 10.95
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b 完整行 RTN 2095.20/1848.83/1222.00 47.43/53.38/36.93
    4399.18/4400.98/3551.88 8326.78/4208.57/4895.83 878.00/735.86/910.10 1953.43/1953.60/1669.76
    439.39/691.94/437.96 1465.06/1564.59/1282.58 1722.01 45.91 4117.35 5810.40 841.32
    1858.93 523.09 1437.41 GPTQ 845.81/599.71/496.14 30.65/34.09/26.15 20.23/27.39/19.45
    15.91/19.26/16.01 12.69/15.90/13.96 11.36/13.71/12.21 10.10/12.54/11.20 16.77/21.16/15.39
    647.22 30.30 22.36 17.06 14.18 12.43 11.28 17.77 ZQ-Global^∗ 46.47/58.55/35.45
    29.64/36.51/25.55 32.48/94.57/28.97 60.91/116.22/36.45 23.87/29.75/23.88 44.70/60.78/46.18
    13.16/20.49/13.48 28.93/75.91/27.28 46.82 30.57 52.01 71.19 25.83 50.55 15.71
    44.04 BS=1024 RTN 不适用 44.57/49.58/35.09 1950.00/2317.55/1913.55 3810.79/2563.06/3054.91
    50.01/70.17/99.21 265.62/417.03/261.93 362.47/252.33/364.45 523.81/846.60/1021.17
    不适用 43.08 2060.37 3142.92 73.13 314.86 326.42 797.20 GPTQ 不适用 29.78/33.76/25.66
    19.03/23.32/18.14 不适用 11.69/14.31/12.70 10.56/12.96/11.70 9.89/12.19/11.02 12.84/16.17/13.02
    不适用 29.73 20.16 不适用 12.90 11.74 11.03 14.01 ZQ-Global^∗ 不适用 29.19/34.57/25.11
    19.83/29.77/19.79 不适用 13.99/18.82/14.76 13.43/19.28/13.76 11.10/14.46/11.94 11.87/14.86/12.13
    不适用 29.62 23.13 不适用 15.86 15.49 12.50 12.95 BS=512 RTN 不适用 37.74/45.10/31.85 1777.53/1304.55/852.03
    1604.07/1407.49/1487.78 25.13/40.56/40.08 130.75/175.33/135.67 620.53/340.68/416.28
    198.01/457.78/426.15 不适用 38.23 1311.37 1499.78 35.26 147.25 459.16 360.65 GPTQ
    不适用 28.46/32.54/25.14 18.02/21.35/17.46 14.38/17.24/14.79 11.57/14.33/12.57 10.41/12.97/11.64
    9.77/12.18/10.97 11.89/14.48/12.40 不适用 28.71 18.94 15.47 12.82 11.67 10.97 12.92
    ZQ-Global^∗ 不适用 27.81/33.57/24.55 18.31/23.54/17.99 18.10/29.47/17.15 12.54/16.60/13.62
    11.82/15.98/12.81 10.48/13.36/11.66 11.26/13.95/11.79 不适用 28.65 19.95 21.57 14.25
    13.54 11.83 12.33 BS=256 RTN 4349.14/2907.61/2510.75 35.36/42.07/30.81 127.17/358.19/142.49
    670.51/550.66/531.80 19.10/32.39/27.26 42.52/56.35/43.32 32.84/60.38/33.48 210.01/478.13/413.00
    3255.84 36.08 209.28 584.32 26.25 47.40 42.23 367.05 GPTQ 41.81/49.95/32.48 27.60/33.73/24.88
    16.97/20.19/16.70 13.69/17.06/14.54 11.65/14.24/12.48 10.35/12.93/11.61 9.66/12.10/10.93
    11.60/13.98/11.92 41.41 28.74 17.95 15.10 12.79 11.63 10.90 12.50 ZQ-Global^∗
    38.60/46.57/31.36 26.88/32.79/24.08 16.82/21.21/17.05 14.86/19.63/15.37 11.86/15.87/13.10
    11.33/14.95/12.48 10.41/12.95/11.41 10.26/12.66/11.08 38.85 27.92 18.36 16.62
    13.61 12.92 11.59 11.34 BS=128 RTN 3446.89/2156.26/1484.15 33.13/41.23/29.51 49.40/88.45/45.07
    153.68/155.21/113.98 16.34/26.86/21.98 17.80/25.95/18.28 45.83/43.91/57.50 106.84/241.02/212.94
    2362.43 34.62 60.97 140.96 21.72 20.67 49.08 186.93 GPTQ 40.00/45.73/31.15 27.68/34.04/25.18
    16.47/19.90/16.47 13.81/16.96/14.37 11.57/14.10/12.41 10.35/12.84/11.58 9.73/12.08/10.91
    10.96/13.27/11.45 38.96 28.97 17.61 15.05 12.69 11.59 10.91 11.90 ZQ-Global^∗
    36.57/43.88/29.94 25.75/31.59/23.57 16.28/20.20/16.67 14.27/18.41/14.90 11.70/15.05/12.68
    11.13/15.07/12.17 10.31/12.99/11.32 10.12/12.66/11.01 36.80 26.97 17.72 15.86
    13.14 12.79 11.54 11.27 BS=64 RTN 708.02/477.13/287.03 32.61/42.14/29.09 25.43/38.84/24.63
    72.84/69.27/48.07 14.11/21
- en: 'Table E.18: BLOOM W3${}^{\text{asym}}$-A16 with various block-size out of the
    best result from GPTQ and ZQ-Global.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表 E.18：BLOOM W3${}^{\text{asym}}$-A16 与 GPTQ 和 ZQ-Global 中各种块大小的最佳结果。
- en: Method 560m 1.1b 1.7b 3b 7.1b 176b Full row RTN 68.45/132.83/59.22 118.61/317.41/99.65
    31.15/67.23/34.02 31.07/59.03/32.17 66140.72/78568.16/44504.19 100371.84/166012.19/137892.34
    86.83 178.56 44.14 40.76 63071.02 134758.79 GPTQ 46.92/84.69/39.50 49.78/142.95/43.84
    19.70/41.35/21.74 22.84/46.49/22.90 52966.59/52979.88/37115.48 Diverge 57.04 78.85
    27.59 30.74 47687.32 Diverge ZQ-Global 33.20/64.61/32.30 34.16/100.05/29.22 19.22/36.30/21.25
    18.41/33.10/20.79 273.55/439.59/100.79 27.19/75.74/45.45 43.37 54.48 25.59 24.10
    271.31 49.46 BS=1024 RTN 47.00/86.57/43.37 70.81/230.74/70.78 35.41/65.75/33.54
    22.12/40.65/24.55 25654.77/25531.66/15868.46 141324.41/183583.73/200436.33 58.98
    124.11 44.90 29.11 22351.63 175114.82 GPTQ 31.25/58.80/30.94 N/A 19.11/37.07/20.90
    N/A 12.59/21.95/15.21 8.31/13.96/11.17 40.33 N/A 25.69 N/A 16.58 11.15 ZQ-Global
    28.91/55.81/29.59 N/A 18.20/34.13/20.40 N/A 30.94/119.98/21.39 15.98/32.85/19.85
    38.10 N/A 24.24 N/A 57.44 22.89 BS=512 RTN 41.58/79.83/39.41 33.83/116.88/37.34
    25.95/49.65/26.77 19.94/38.58/22.58 9777.49/8000.29/5407.46 202051.34/273707.81/279776.97
    53.61 62.68 34.12 27.03 7728.41 251845.38 GPTQ 28.08/53.15/29.05 21.20/61.42/23.33
    18.41/34.47/20.43 15.08/26.14/17.53 12.32/21.29/15.01 8.30/13.98/11.16 36.76 35.32
    24.44 19.58 16.21 11.15 ZQ-Global 26.80/50.49/28.31 20.77/57.57/22.89 17.64/33.19/19.91
    15.16/26.51/17.57 16.35/28.75/15.76 11.38/20.36/14.66 35.20 33.75 23.58 19.75
    20.29 15.47 BS=256 RTN 36.13/70.37/36.29 28.65/95.72/31.80 21.67/42.59/23.80 17.64/32.82/20.69
    1322.61/1864.55/946.92 166006.80/187829.98/198052.83 47.60 52.06 29.35 23.72 1378.02
    183963.20 GPTQ 27.10/51.11/28.24 20.60/56.57/22.77 17.97/33.28/20.04 14.82/25.79/17.31
    12.27/21.24/14.93 8.27/13.99/11.14 35.48 33.31 23.76 19.31 16.15 11.13 ZQ-Global
    25.96/49.75/27.59 20.21/54.83/22.33 17.43/32.14/19.67 14.85/25.79/17.33 12.85/22.00/15.04
    9.07/15.88/11.88 34.43 32.46 23.08 19.32 16.63 12.28 BS=128 RTN 34.71/66.56/35.27
    24.43/73.77/26.90 19.59/37.22/21.98 16.11/28.81/18.89 108.32/252.15/74.42 111057.84/101926.99/105339.26
    45.51 41.70 26.26 21.27 144.96 106108.03 GPTQ 26.29/49.86/27.54 20.26/55.76/22.42
    17.77/32.65/19.92 14.58/25.25/17.11 12.18/21.06/14.86 8.26/13.92/11.12 34.56 32.81
    23.45 18.98 16.03 11.10 ZQ-Global 25.28/48.24/26.96 19.79/54.04/22.03 17.12/31.42/19.31
    14.62/25.73/17.17 12.04/21.02/14.82 8.43/14.44/11.29 33.49 31.95 22.62 19.17 15.96
    11.39 BS=64 RTN 30.88/59.01/32.08 23.04/67.93/25.49 19.35/37.67/21.80 15.64/27.56/18.39
    37.15/65.22/33.22 198.66/488.11/128.62 40.66 38.82 26.27 20.53 45.20 271.80 GPTQ
    26.31/49.91/27.17 20.11/55.06/22.23 17.94/32.42/19.76 14.62/25.39/17.07 12.13/21.07/14.83
    8.26/13.93/11.11 34.46 32.47 23.37 19.02 16.01 11.10 ZQ-Global 25.17/48.01/26.59
    19.51/53.27/21.75 16.88/31.14/19.22 14.51/25.18/17.05 12.00/20.85/14.74 8.35/14.06/11.20
    33.26 31.51 22.41 18.91 15.86 11.21 BS=32 RTN 30.15/57.55/31.51 23.49/70.15/25.56
    18.96/36.54/21.42 15.56/27.48/18.32 13.06/23.77/16.05 10.28/18.90/13.27 39.74
    39.73 25.64 20.46 17.62 14.15 GPTQ 25.96/49.99/27.06 19.97/54.79/22.16 17.60/32.24/19.76
    14.55/25.76/17.06 12.20/21.01/14.85 8.28/13.95/11.13 34.33 32.31 23.20 19.12 16.02
    11.12 ZQ-Global 25.09/47.36/26.34 19.43/52.95/21.64 16.86/30.49/19.11 14.50/25.36/16.99
    12.00/20.84/14.72 8.35/14.04/11.20 32.93 31.34 22.15 18.95 15.85 11.20
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 560m 1.1b 1.7b 3b 7.1b 176b 完整行 RTN 68.45/132.83/59.22 118.61/317.41/99.65
    31.15/67.23/34.02 31.07/59.03/32.17 66140.72/78568.16/44504.19 100371.84/166012.19/137892.34
    86.83 178.56 44.14 40.76 63071.02 134758.79 GPTQ 46.92/84.69/39.50 49.78/142.95/43.84
    19.70/41.35/21.74 22.84/46.49/22.90 52966.59/52979.88/37115.48 Diverge 57.04 78.85
    27.59 30.74 47687.32 Diverge ZQ-Global 33.20/64.61/32.30 34.16/100.05/29.22 19.22/36.30/21.25
    18.41/33.10/20.79 273.55/439.59/100.79 27.19/75.74/45.45 43.37 54.48 25.59 24.10
    271.31 49.46 BS=1024 RTN 47.00/86.57/43.37 70.81/230.74/70.78 35.41/65.75/33.54
    22.12/40.65/24.55 25654.77/25531.66/15868.46 141324.41/183583.73/200436.33 58.98
    124.11 44.90 29.11 22351.63 175114.82 GPTQ 31.25/58.80/30.94 N/A 19.11/37.07/20.90
    N/A 12.59/21.95/15.21 8.31/13.96/11.17 40.33 N/A 25.69 N/A 16.58 11.15 ZQ-Global
    28.91/55.81/29.59 N/A 18.20/34.13/20.40 N/A 30.94/119.98/21.39 15.98/32.85/19.85
    38.10 N/A 24.24 N/A 57.44 22.89 BS=512 RTN 41.58/79.83/39.41 33.83/116.88/37.34
    25.95/49.65/26.77 19.94/38.58/22.58 9777.49/8000.29/5407.46 202051.34/273707.81/279776.97
    53.61 62.68 34.12 27.03 7728.41 251845.38 GPTQ 28.08/53.15/29.05 21.20/61.42/23.33
    18.41/34.47/20.43 15.08/26.14/17.53 12.32/21.29/15.01 8.30/13.98/11.16 36.76 35.32
    24.44 19.58 16.21 11.15 ZQ-Global 26.80/50.49/28.31 20.77/57.57/22.89 17.64/33.19/19.91
    15.16/26.51/17.57 16.35/28.75/15.76 11.38/20.36/14.66 35.20 33.75 23.58 19.75
    20.29 15.47 BS=256 RTN 36.13/70.37/36.29 28.65/95.72/31.80 21.67/42.59/23.80 17.64/32.82/20.69
    1322.61/1864.55/946.92 166006.80/187829.98/198052.83 47.60 52.06 29.35 23.72 1378.02
    183963.20 GPTQ 27.10/51.11/28.24 20.60/56.57/22.77 17.97/33.28/20.04 14.82/25.79/17.31
    12.27/21.24/14.93 8.27/13.99/11.14 35.48 33.31 23.76 19.31 16.15 11.13 ZQ-Global
    25.96/49.75/27.59 20.21/54.83/22.33 17.43/32.14/19.67 14.85/25.79/17.33 12.85/22.00/15.04
    9.07/15.88/11.88 34.43 32.46 23.08 19.32 16.63 12.28 BS=128 RTN 34.71/66.56/35.27
    24.43/73.77/26.90 19.59/37.22/21.98 16.11/28.81/18.89 108.32/252.15/74.42 111057.84/101926.99/105339.26
    45.51 41.70 26.26 21.27 144.96 106108.03 GPTQ 26.29/49.86/27.54 20.26/55.76/22.42
    17.77/32.65/19.92 14.58/25.25/17.11 12.18/21.06/14.86 8.26/13.92/11.12 34.56 32.81
    23.45 18.98 16.03 11.10 ZQ-Global 25.28/48.24/26.96 19.79/54.04/22.03 17.12/31.42/19.31
    14.62/25.73/17.17 12.04/21.02/14.82 8.43/14.44/11.29 33.49 31.95 22.62 19.17 15.96
    11.39 BS=64 RTN 30.88/59.01/32.08 23.04/67.93/25.49 19.35/37.67/21.80 15.64/27.56/18.39
    37.15/65.22/33.22 198.66/488.11/128.62 40.66 38.82 26.27 20.53 45.20 271.80 GPTQ
    26.31/49.91/27.17 20.11/55.06/22.23 17.94/32.42/19.76 14.62/25.39/17.07 12.13/21.07/14.83
    8.26/13.93/11.11 34.46 32.47 23.37 19.02 16.01 11.10 ZQ-Global 25.17/48.01/26.59
    19.51/53.27/21.75 16.88/31.14/19.22 14.51/25.18/17.05 12.00/20.85/14.74 8.35/14.06/11.20
    33.26 31.51 22.41 18.91 15.86 11.21 BS=32 RTN 30.15/57.55/31.51 23.49/70.15/25.56
    18.96/36.54/21.42 15.56/27.48/18.32 13.06/23.77/16.05 10.28/18.90/13.27 39.74
    39.73 25.64 20.46 17.62 14.15 GPTQ 25.96/49.99/27.06 19.97/54.79/22.16 17.60/32.24/19.76
    14.55/25.76/17.06 12.20/21.01/14.85 8.28/13.95/11.13 34.33 32.31 23.20 19.12 16.02
    11.12 ZQ-Global 25.09/47.36/26.34 19.43/52.95/21.64 16.86/30.49/19.11 14.50/25.
- en: 'Table E.19: Full results of BLOOM-176B with different quantization bits'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.19: BLOOM-176B 在不同量化位数下的完整结果'
- en: Bits 3 4 5 6 7 8 Per-row 27.19/75.74/45.45 8.16/13.70/11.02 8.13/13.67/10.99
    8.11/13.63/10.98 8.11/13.62/10.97 8.10/13.62/10.98 1024 8.31/13.96/11.17 8.14/13.70/11.02
    8.11/13.62/10.97 8.11/13.62/10.97 8.11/13.63/10.97 N/A 64 8.26/13.93/11.11 8.14/13.69/11.00
    8.11/13.62/10.96 N/A N/A N/A
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 位数 3 4 5 6 7 8 每行 27.19/75.74/45.45 8.16/13.70/11.02 8.13/13.67/10.99 8.11/13.63/10.98
    8.11/13.62/10.97 8.10/13.62/10.98 1024 8.31/13.96/11.17 8.14/13.70/11.02 8.11/13.62/10.97
    8.11/13.62/10.97 8.11/13.63/10.97 不适用 64 8.26/13.93/11.11 8.14/13.69/11.00 8.11/13.62/10.96
    不适用 不适用 不适用
- en: 'Table E.20: OPT full results of Table [5](#S4.T5 "Table 5 ‣ 4-bit Weight Quantization.
    ‣ 4.1 Fine-grained Quantization and Its Evaluation ‣ 4 Are existing quantization
    methods optimally harnessing the potential to minimize LLMs sizes? ‣ ZeroQuant-V2:
    Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank
    Compensation").'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.20: 表 [5](#S4.T5 "表 5 ‣ 4-bit 权重量化。 ‣ 4.1 细粒度量化及其评估 ‣ 4 现有量化方法是否充分发挥了最小化
    LLM 大小的潜力？ ‣ ZeroQuant-V2: 从全面研究到低秩补偿探索 LLM 的后训练量化") 的 OPT 完整结果。'
- en: Method 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b W4${}^{\text{asym}}$ 128 RTN 30.59/36.56/27.07
    24.11/29.43/22.74 15.38/18.57/15.69 13.22/16.32/13.91 11.13/13.97/12.30 10.34/12.82/11.55
    9.98/12.15/10.96 9.57/11.86/10.58 31.41 25.43 16.55 14.49 12.47 11.57 11.03 10.67
    GPTQ 30.47/36.19/26.40 23.35/27.96/21.94 14.92/17.57/15.12 12.48/15.60/13.46 10.87/13.34/11.91
    10.20/12.45/11.28 9.62/11.88/10.74 9.39/11.55/10.41 31.02 24.42 15.87 13.85 12.04
    11.31 10.75 10.45 ZQ-Local 9.37/11.70/10.49 10.52 ZQ-Global 29.85/34.52/26.10
    22.70/27.72/21.64 14.96/17.55/15.09 12.64/15.40/13.47 10.93/13.43/11.95 10.18/12.68/11.42
    9.74/12.02/10.83 9.39/11.53/10.42 30.16 24.02 15.86 13.84 12.10 11.42 10.86 10.45
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 125m 350m 1.3b 2.7b 6.7b 13b 30b 66b W4${}^{\text{asym}}$ 128 RTN 30.59/36.56/27.07
    24.11/29.43/22.74 15.38/18.57/15.69 13.22/16.32/13.91 11.13/13.97/12.30 10.34/12.82/11.55
    9.98/12.15/10.96 9.57/11.86/10.58 31.41 25.43 16.55 14.49 12.47 11.57 11.03 10.67
    GPTQ 30.47/36.19/26.40 23.35/27.96/21.94 14.92/17.57/15.12 12.48/15.60/13.46 10.87/13.34/11.91
    10.20/12.45/11.28 9.62/11.88/10.74 9.39/11.55/10.41 31.02 24.42 15.87 13.85 12.04
    11.31 10.75 10.45 ZQ-Local 9.37/11.70/10.49 10.52 ZQ-Global 29.85/34.52/26.10
    22.70/27.72/21.64 14.96/17.55/15.09 12.64/15.40/13.47 10.93/13.43/11.95 10.18/12.68/11.42
    9.74/12.02/10.83 9.39/11.53/10.42 30.16 24.02 15.86 13.84 12.10 11.42 10.86 10.45
- en: 'Table E.21: BLOOM full results of Table [6](#S4.T6 "Table 6 ‣ 4-bit Weight
    Quantization. ‣ 4.1 Fine-grained Quantization and Its Evaluation ‣ 4 Are existing
    quantization methods optimally harnessing the potential to minimize LLMs sizes?
    ‣ ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive
    Study to Low Rank Compensation").'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.21: 表 [6](#S4.T6 "表 6 ‣ 4-bit 权重量化。 ‣ 4.1 细粒度量化及其评估 ‣ 4 现有量化方法是否充分发挥了最小化
    LLM 大小的潜力？ ‣ ZeroQuant-V2: 从全面研究到低秩补偿探索 LLM 的后训练量化") 的 BLOOM 完整结果。'
- en: Method 560m 1.1b 1.7b 3b 7.1b 176b W4${}^{\text{asym}}$ 128 RTN 23.83/44.89/25.77
    18.63/51.46/21.19 16.16/29.95/18.68 14.03/24.27/16.75 23.51/49.07/23.96 8.85/15.65/11.72
    31.50 30.43 21.60 18.35 32.18 12.08 GPTQ 23.26/43.24/25.00 18.18/48.84/20.73 16.05/29.34/18.42
    13.69/23.56/16.34 11.54/19.75/14.28 8.14/13.71/11.02 30.50 29.25 21.27 17.86 15.19
    10.96 ZQ-Local 8.19/13.90/11.07 11.06 ZQ-Global 23.12/43.14/25.01 18.18/48.99/20.73
    15.71/28.73/18.30 13.74/23.68/16.39 11.56/19.85/14.31 8.17/13.78/11.04 30.42 29.30
    20.91 17.94 15.24 11.00
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 560m 1.1b 1.7b 3b 7.1b 176b W4${}^{\text{asym}}$ 128 RTN 23.83/44.89/25.77
    18.63/51.46/21.19 16.16/29.95/18.68 14.03/24.27/16.75 23.51/49.07/23.96 8.85/15.65/11.72
    31.50 30.43 21.60 18.35 32.18 12.08 GPTQ 23.26/43.24/25.00 18.18/48.84/20.73 16.05/29.34/18.42
    13.69/23.56/16.34 11.54/19.75/14.28 8.14/13.71/11.02 30.50 29.25 21.27 17.86 15.19
    10.96 ZQ-Local 8.19/13.90/11.07 11.06 ZQ-Global 23.12/43.14/25.01 18.18/48.99/20.73
    15.71/28.73/18.30 13.74/23.68/16.39 11.56/19.85/14.31 8.17/13.78/11.04 30.42 29.30
    20.91 17.94 15.24 11.00
- en: 'Table E.22: Full results of Table [6](#S4.T6 "Table 6 ‣ 4-bit Weight Quantization.
    ‣ 4.1 Fine-grained Quantization and Its Evaluation ‣ 4 Are existing quantization
    methods optimally harnessing the potential to minimize LLMs sizes? ‣ ZeroQuant-V2:
    Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank
    Compensation").'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.22: 表 [6](#S4.T6 "表 6 ‣ 4-bit 权重量化。 ‣ 4.1 细粒度量化及其评估 ‣ 4 现有量化方法是否充分发挥了最小化
    LLM 大小的潜力？ ‣ ZeroQuant-V2: 从全面研究到低秩补偿探索 LLM 的后训练量化") 的完整结果。'
- en: Block SIze 1024 512 256 128 64 32 PPL 8.16/13.75/11.04 8.15/13.75/11.02 8.15/13.70/11.01
    8.13/13.69/11.01 8.14/13.69/11.01 8.14/13.69/11.01
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小 1024 512 256 128 64 32 PPL 8.16/13.75/11.04 8.15/13.75/11.02 8.15/13.70/11.01
    8.13/13.69/11.01 8.14/13.69/11.01 8.14/13.69/11.01
- en: 'Table E.23: Results of applying LoRC on top of ZQ-Global for INT8 Activation.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '表 E.23: 在 ZQ-Global 基础上应用 LoRC 对 INT8 激活的结果。'
- en: Learning Rate model-size precision LoRC-dim 0.0005 0.0001 5.00E-05 1.00E-05
    5.00E-06 Best 125m W4A8 0 4482.1 31.15 30.40 30.55 30.72 30.40 8 5996.14 30.96
    30.24 30.37 30.61 30.24 16 3577.12 31.02 30.26 30.2 30.37 30.20 125m W3A8 0 4283.28
    41.03 40.93 55.74 86.34 40.93 8 2396.92 37.25 36.65 37.85 39.06 36.65 16 1787.74
    36.66 36.55 37.46 38.21 36.55 125m W2A8 0 3473.18 583.72 996.76 2480.69 3203.11
    583.72 8 3815.37 144.85 160.71 362.17 466.98 144.85 16 3324.23 135.25 156.28 295.78
    372.7 135.25 Learning Rate LoRC-dim 5.00E-05 1.00E-05 5.00E-06 1.00E-06 5.00E-07
    best 350m W4A8 0 25.65 24.38 24.34 24.55 24.75 24.34 8 25.56 24.3 24.24 24.45
    24.66 24.24 16 25.45 24.39 24.21 24.39 24.63 24.21 350m W3A8 0 30.59 28.45 28.94
    31.51 32.39 28.45 8 30.1 28.22 28.71 30.81 32.09 28.22 16 30.64 28.02 28.50 30.62
    31.69 28.02 350m W2A8 0 97.40 177.43 257.61 668.19 722.19 97.4 8 95.79 139.68
    194.36 437.18 459.92 95.79 16 106.51 137.81 172.93 400.91 421.59 106.51
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率 模型大小 精度 LoRC-dim 0.0005 0.0001 5.00E-05 1.00E-05 5.00E-06 最佳 125m W4A8 0
    4482.1 31.15 30.40 30.55 30.72 30.40 8 5996.14 30.96 30.24 30.37 30.61 30.24 16
    3577.12 31.02 30.26 30.2 30.37 30.20 125m W3A8 0 4283.28 41.03 40.93 55.74 86.34
    40.93 8 2396.92 37.25 36.65 37.85 39.06 36.65 16 1787.74 36.66 36.55 37.46 38.21
    36.55 125m W2A8 0 3473.18 583.72 996.76 2480.69 3203.11 583.72 8 3815.37 144.85
    160.71 362.17 466.98 144.85 16 3324.23 135.25 156.28 295.78 372.7 135.25 学习率 LoRC-dim
    5.00E-05 1.00E-05 5.00E-06 1.00E-06 5.00E-07 最佳 350m W4A8 0 25.65 24.38 24.34
    24.55 24.75 24.34 8 25.56 24.3 24.24 24.45 24.66 24.24 16 25.45 24.39 24.21 24.39
    24.63 24.21 350m W3A8 0 30.59 28.45 28.94 31.51 32.39 28.45 8 30.1 28.22 28.71
    30.81 32.09 28.22 16 30.64 28.02 28.50 30.62 31.69 28.02 350m W2A8 0 97.40 177.43
    257.61 668.19 722.19 97.4 8 95.79 139.68 194.36 437.18 459.92 95.79 16 106.51
    137.81 172.93 400.91 421.59 106.51
