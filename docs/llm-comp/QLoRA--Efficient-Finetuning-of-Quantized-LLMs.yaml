- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:51:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:51:37'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'QLoRA: Efficient Finetuning of Quantized LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'QLoRA: 量化LLM的高效微调'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.14314](https://ar5iv.labs.arxiv.org/html/2305.14314)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.14314](https://ar5iv.labs.arxiv.org/html/2305.14314)
- en: Tim Dettmers &Artidoro Pagnoni^∗ &Ari Holtzman &Luke Zettlemoyer
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Tim Dettmers & Artidoro Pagnoni^∗ & Ari Holtzman & Luke Zettlemoyer
- en: University of Washington
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 华盛顿大学
- en: '{dettmers,artidoro,ahai,lsz}@cs.washington.edu Equal contribution.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{dettmers,artidoro,ahai,lsz}@cs.washington.edu 平等贡献。'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'We present QLoRA, an efficient finetuning approach that reduces memory usage
    enough to finetune a 65B parameter model on a single 48GB GPU while preserving
    full 16-bit finetuning task performance. QLoRA backpropagates gradients through
    a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA).
    Our best model family, which we name Guanaco, outperforms all previous openly
    released models on the Vicuna benchmark, reaching 99.3% of the performance level
    of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA
    introduces a number of innovations to save memory without sacrificing performance:
    (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically
    optimal for normally distributed weights (b) Double Quantization to reduce the
    average memory footprint by quantizing the quantization constants, and (c) Paged
    Optimizers to manage memory spikes. We use QLoRA to finetune more than 1,000 models,
    providing a detailed analysis of instruction following and chatbot performance
    across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales
    that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter
    models). Our results show that QLoRA finetuning on a small high-quality dataset
    leads to state-of-the-art results, even when using smaller models than the previous
    SoTA. We provide a detailed analysis of chatbot performance based on both human
    and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable
    alternative to human evaluation. Furthermore, we find that current chatbot benchmarks
    are not trustworthy to accurately evaluate the performance levels of chatbots.
    A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT.
    We release all of our models and code, including CUDA kernels for 4-bit training.¹¹1[https://github.com/artidoro/qlora](https://github.com/artidoro/qlora)
    and [https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了QLoRA，一种高效的微调方法，能够大幅减少内存使用，从而在单个48GB GPU上微调65B参数模型，同时保持完整的16位微调任务性能。QLoRA将梯度反向传播通过一个冻结的、4位量化的预训练语言模型到低秩适配器（LoRA）中。我们最好的模型系列，我们称之为Guanaco，在Vicuna基准测试中超越了所有之前公开发布的模型，达到了ChatGPT
    99.3%的性能水平，同时只需在单个GPU上微调24小时。QLoRA引入了多项创新来节省内存而不牺牲性能：（a）4位NormalFloat（NF4），一种对正常分布权重在信息理论上最优的数据类型，（b）双重量化，通过量化量化常数来减少平均内存占用，以及（c）分页优化器以管理内存峰值。我们使用QLoRA微调了1000多个模型，提供了对8个指令数据集、多个模型类型（LLaMA、T5）以及在常规微调下无法运行的模型规模（例如33B和65B参数模型）的指令跟随和聊天机器人性能的详细分析。我们的结果表明，QLoRA在小型高质量数据集上进行微调，即使使用比之前SoTA小的模型，也能达到最先进的结果。我们提供了基于人类和GPT-4评估的聊天机器人性能的详细分析，表明GPT-4评估是对人类评估的便宜而合理的替代方案。此外，我们发现当前的聊天机器人基准测试不可信，无法准确评估聊天机器人的性能水平。通过柠檬挑选的分析展示了Guanaco相较于ChatGPT的失败之处。我们发布了所有模型和代码，包括用于4位训练的CUDA内核。¹¹1[https://github.com/artidoro/qlora](https://github.com/artidoro/qlora)
    和 [https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Finetuning large language models (LLMs) is a highly effective way to improve
    their performance, [[40](#bib.bib40), [62](#bib.bib62), [43](#bib.bib43), [61](#bib.bib61),
    [59](#bib.bib59), [37](#bib.bib37)] and to add desirable or remove undesirable
    behaviors [[43](#bib.bib43), [2](#bib.bib2), [4](#bib.bib4)]. However, finetuning
    very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA
    65B parameter model [[57](#bib.bib57)] requires more than 780 GB of GPU memory.
    While recent quantization methods can reduce the memory footprint of LLMs [[14](#bib.bib14),
    [13](#bib.bib13), [18](#bib.bib18), [66](#bib.bib66)], such techniques only work
    for inference and break down during training [[65](#bib.bib65)].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 微调大型语言模型（LLMs）是提高其性能的一个非常有效的方法[[40](#bib.bib40), [62](#bib.bib62), [43](#bib.bib43),
    [61](#bib.bib61), [59](#bib.bib59), [37](#bib.bib37)]，并且可以增加所需的或去除不需要的行为[[43](#bib.bib43),
    [2](#bib.bib2), [4](#bib.bib4)]。然而，微调非常大的模型成本过高；对LLaMA 65B参数模型[[57](#bib.bib57)]的常规16位微调需要超过780
    GB的GPU内存。虽然近期的量化方法可以减少LLMs的内存占用[[14](#bib.bib14), [13](#bib.bib13), [18](#bib.bib18),
    [66](#bib.bib66)]，但这些技术仅适用于推理，在训练过程中会失效[[65](#bib.bib65)]。
- en: We demonstrate for the first time that it is possible to finetune a quantized
    4-bit model without any performance degradation. Our method, QLoRA, uses a novel
    high-precision technique to quantize a pretrained model to 4-bit, then adds a
    small set of learnable Low-rank Adapter weights [[28](#bib.bib28)]
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首次展示了在没有任何性能下降的情况下微调量化4位模型的可能性。我们的方法QLoRA使用了一种新型的高精度技术将预训练模型量化为4位，然后添加了一小部分可学习的低秩适配器权重[[28](#bib.bib28)]。
- en: 'Table 1: Elo ratings for a competition between models, averaged for 10,000
    random initial orderings. The winner of a match is determined by GPT-4 which declares
    which response is better for a given prompt of the the Vicuna benchmark. 95% confidence
    intervals are shown ($\pm$). After GPT-4, Guanaco 33B and 65B win the most matches,
    while Guanaco 13B scores better than Bard.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：对模型间比赛的Elo评分，基于10,000个随机初始排序的平均值。比赛的胜者由GPT-4决定，GPT-4会根据Vicuna基准测试中的给定提示判断哪个回应更好。显示了95%的置信区间（$\pm$）。在GPT-4之后，Guanaco
    33B和65B赢得了最多的比赛，而Guanaco 13B的得分高于Bard。
- en: '| Model | Size | Elo |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 | Elo |'
- en: '| --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GPT-4 | - | 1348 $\pm$ 1 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | - | 1348 $\pm$ 1 |'
- en: '| Guanaco 65B | 41 GB | 1022 $\pm$ 1 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco 65B | 41 GB | 1022 $\pm$ 1 |'
- en: '| Guanaco 33B | 21 GB |   992 $\pm$ 1 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco 33B | 21 GB | 992 $\pm$ 1 |'
- en: '| Vicuna 13B | 26 GB |   974 $\pm$ 1 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna 13B | 26 GB | 974 $\pm$ 1 |'
- en: '| ChatGPT | - |   966 $\pm$ 1 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | - | 966 $\pm$ 1 |'
- en: '| Guanaco 13B | 10 GB |   916 $\pm$ 1 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco 13B | 10 GB | 916 $\pm$ 1 |'
- en: '| Bard | - |   902 $\pm$ 1 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| Bard | - | 902 $\pm$ 1 |'
- en: '| Guanaco 7B | 6 GB |   879 $\pm$ 1 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco 7B | 6 GB | 879 $\pm$ 1 |'
- en: that are tuned by backpropagating gradients through the quantized weights.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型是通过将梯度反向传播通过量化权重来进行调优的。
- en: 'QLoRA reduces the average memory requirements of finetuning a 65B parameter
    model from 48GB without
    degrading the runtime or predictive performance compared to a 16-bit fully finetuned
    baseline. This marks a significant shift in accessibility of LLM finetuning: now
    the largest publicly available models to date finetunable on a single GPU. Using
    QLoRA, we train the Guanaco family of models, with the second best model reaching
    97.8% of the performance level of ChatGPT on the Vicuna [[10](#bib.bib10)] benchmark,
    while being trainable in less than 12 hours on a single consumer GPU; using a
    single professional GPU over 24 hours we achieve 99.3% with our largest model,
    essentially closing the gap to ChatGPT on the Vicuna benchmark. When deployed,
    our smallest Guanaco model (7B parameters) requires just 5 GB of memory and outperforms
    a 26 GB Alpaca model by more than 20 percentage points on the Vicuna benchmark
    (Table [6](#S5.T6 "Table 6 ‣ Elo Rating ‣ 5.2 Evaluation ‣ 5 Pushing the Chatbot
    State-of-the-art with QLoRA ‣ QLoRA: Efficient Finetuning of Quantized LLMs")).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 'QLoRA将65B参数模型的微调平均内存需求从48GB减少到没有降低运行时间或预测性能，相比于16位完全微调的基线模型。这标志着LLM微调可访问性的重大变化：现在，最大可用的公共模型可以在单个GPU上进行微调。使用QLoRA，我们训练了Guanaco系列模型，其中第二好的模型在Vicuna
    [[10](#bib.bib10)] 基准测试中达到ChatGPT的97.8%的性能水平，同时可以在单个消费者GPU上不到12小时内完成训练；使用单个专业GPU进行24小时训练，我们的最大模型达到了99.3%，基本上缩小了在Vicuna基准测试中与ChatGPT的差距。当部署时，我们最小的Guanaco模型（7B参数）仅需5
    GB内存，并且在Vicuna基准测试中比26 GB的Alpaca模型高出超过20个百分点（表[6](#S5.T6 "Table 6 ‣ Elo Rating
    ‣ 5.2 Evaluation ‣ 5 Pushing the Chatbot State-of-the-art with QLoRA ‣ QLoRA:
    Efficient Finetuning of Quantized LLMs")）。'
- en: 'QLoRA introduces multiple innovations designed to reduce memory use without
    sacrificing performance: (1) 4-bit NormalFloat, an information theoretically optimal
    quantization data type for normally distributed data that yields better empirical
    results than 4-bit Integers and 4-bit Floats. (2) Double Quantization, a method
    that quantizes the quantization constants, saving an average of about 0.37 bits
    per parameter (approximately 3 GB for a 65B model). (3) Paged Optimizers, using
    NVIDIA unified memory to avoid the gradient checkpointing memory spikes that occur
    when processing a mini-batch with a long sequence length. We combine these contributions
    into a better tuned LoRA approach that includes adapters at every network layer
    and thereby avoids almost all of the accuracy tradeoffs seen in prior work.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA 引入了多项创新，旨在减少内存使用而不牺牲性能：(1) 4-bit NormalFloat，一种信息理论上最优的量化数据类型，用于正态分布的数据，其经验结果优于
    4-bit Integers 和 4-bit Floats。(2) Double Quantization，一种量化量化常数的方法，平均每个参数节省约 0.37
    位（对于 65B 模型大约 3 GB）。(3) Paged Optimizers，使用 NVIDIA 统一内存来避免处理长序列长度的小批量时出现的梯度检查点内存峰值。我们将这些贡献结合成一个更好的调优
    LoRA 方法，包含每个网络层的适配器，从而避免了先前工作中几乎所有的准确性折衷。
- en: 'QLoRA’s efficiency enables us to perform an in-depth study of instruction finetuning
    and chatbot performance on model scales that would be impossible using regular
    finetuning due to memory overhead. Therefore, we train more than 1,000 models
    across several instruction tuning datasets, model architectures, and sizes between
    80M to 65B parameters. In addition to showing that QLoRA recovers 16-bit performance
    (§[4](#S4 "4 QLoRA vs. Standard Finetuning ‣ QLoRA: Efficient Finetuning of Quantized
    LLMs")) and training a state-of-the-art chatbot, Guanaco, (§[5](#S5 "5 Pushing
    the Chatbot State-of-the-art with QLoRA ‣ QLoRA: Efficient Finetuning of Quantized
    LLMs")), we also analyze trends in the trained models. First, we find that data
    quality is far more important than dataset size, e.g., a 9k sample dataset (OASST1)
    outperformed a 450k sample dataset (FLAN v2, subsampled) on chatbot performance,
    even when both are meant to support instruction following generalization. Second,
    we show that strong Massive Multitask Language Understanding (MMLU) benchmark
    performance does not imply strong Vicuna chatbot benchmark performance and vice
    versa—in other words, dataset suitability matters more than size for a given task.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 'QLoRA 的高效性使我们能够对指令微调和聊天机器人性能进行深入研究，这在使用常规微调时由于内存开销是不可能做到的。因此，我们在多个指令微调数据集、模型架构和参数规模在
    80M 到 65B 之间的模型上进行了超过 1,000 次训练。除了展示 QLoRA 恢复 16 位性能（§[4](#S4 "4 QLoRA vs. Standard
    Finetuning ‣ QLoRA: Efficient Finetuning of Quantized LLMs")）并训练了一个最先进的聊天机器人 Guanaco（§[5](#S5
    "5 Pushing the Chatbot State-of-the-art with QLoRA ‣ QLoRA: Efficient Finetuning
    of Quantized LLMs")），我们还分析了训练模型的趋势。首先，我们发现数据质量远比数据集大小重要，例如，一个 9k 样本的数据集（OASST1）在聊天机器人性能上优于一个
    450k 样本的数据集（FLAN v2，子样本），即便这两个数据集都旨在支持指令跟随泛化。其次，我们展示了强大的大规模多任务语言理解（MMLU）基准性能并不意味着强大的
    Vicuna 聊天机器人基准性能，反之亦然——换句话说，数据集的适用性比大小对特定任务更为重要。'
- en: Furthermore, we also provide a extensive analysis of chatbot performance that
    uses both human raters and GPT-4 for evaluation. We use tournament-style benchmarking
    where models compete against each other in matches to produce the best response
    for a given prompt. The winner of a match is judged by either GPT-4 or human annotators.
    The tournament results are aggregated into Elo scores [[16](#bib.bib16), [17](#bib.bib17)]
    which determine the ranking of chatbot performance. We find that GPT-4 and human
    evaluations largely agree on the rank of model performance in the tournaments,
    but we also find there are instances of strong disagreement. As such, we highlight
    that model-based evaluation while providing a cheap alternative to human-annotation
    also has its uncertainties.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还提供了对聊天机器人性能的广泛分析，使用人类评审员和 GPT-4 进行评估。我们使用锦标赛风格的基准测试，模型在比赛中相互竞争，以产生对给定提示的最佳响应。比赛的胜者由
    GPT-4 或人类评审员判断。锦标赛结果被汇总为 Elo 分数 [[16](#bib.bib16), [17](#bib.bib17)]，这些分数决定了聊天机器人性能的排名。我们发现
    GPT-4 和人类评估在锦标赛中对模型性能的排名大致一致，但也发现存在强烈的不一致。因此，我们强调，虽然基于模型的评估提供了便宜的替代方案，但也存在不确定性。
- en: We augment our chatbot benchmark results with a qualitative analysis of Guanaco
    models. Our analysis highlights success and failure cases that were not captured
    by the quantitative benchmarks.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过对Guanaco模型的定性分析来扩展我们的聊天机器人基准测试结果。我们的分析突出了定量基准未能捕捉到的成功和失败案例。
- en: We release all model generations with human and GPT-4 annotations to facilitate
    further study. We open-source our codebase and CUDA kernels and integrate our
    methods into the Hugging Face transformers stack [[64](#bib.bib64)], making them
    easily accessible to all. We release a collection of adapters for 7/13/33/65B
    size models, trained on 8 different instruction following datasets, for a total
    of 32 different open sourced, finetuned models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发布了所有模型生成的人工和GPT-4标注，以便进一步研究。我们开源了我们的代码库和CUDA内核，并将我们的方法集成到Hugging Face transformers堆栈[[64](#bib.bib64)]中，使所有人都可以轻松访问。我们发布了一组适配器，用于7/13/33/65B大小的模型，训练于8个不同的指令跟随数据集，总共32种不同的开源微调模型。
- en: '![Refer to caption](img/7ebffaedb374f57354f21d8f134151ed.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7ebffaedb374f57354f21d8f134151ed.png)'
- en: 'Figure 1: Different finetuning methods and their memory requirements. QLoRA
    improves over LoRA by quantizing the transformer model to 4-bit precision and
    using paged optimizers to handle memory spikes.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：不同微调方法及其内存需求。QLoRA通过将变换器模型量化为4-bit精度并使用分页优化器来处理内存突发，从而优于LoRA。
- en: 2 Background
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: Block-wise k-bit Quantization
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 块级k-bit量化
- en: 'Quantization is the process of discretizing an input from a representation
    that holds more information to a representation with less information. It often
    means taking a data type with more bits and converting it to fewer bits, for example
    from 32-bit floats to 8-bit Integers. To ensure that the entire range of the low-bit
    data type is used, the input data type is commonly rescaled into the target data
    type range through normalization by the absolute maximum of the input elements,
    which are usually structured as a tensor. For example, quantizing a 32-bit Floating
    Point (FP32) tensor into a Int8 tensor with range $[-127,127]$:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是将输入从具有更多信息的表示离散化为具有较少信息的表示的过程。它通常意味着将具有更多比特的数据类型转换为更少比特的数据类型，例如从32位浮点数转换为8位整数。为了确保低比特数据类型的整个范围得到使用，输入数据类型通常通过对输入元素的绝对最大值进行归一化，重新缩放到目标数据类型范围，这些元素通常结构为张量。例如，将32位浮点（FP32）张量量化为具有范围$[-127,127]$的Int8张量：
- en: '|  | $\mathbf{X}^{\text{Int8}}=\text{round}\left(\frac{127}{{\text{absmax}}(\mathbf{X}^{{\text{FP32}}})}\mathbf{X}^{{\text{FP32}}}\right)=\text{round}(c^{\text{FP32}}\cdot\mathbf{X}^{{\text{FP32}}}),$
    |  | (1) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{X}^{\text{Int8}}=\text{round}\left(\frac{127}{{\text{absmax}}(\mathbf{X}^{{\text{FP32}}})}\mathbf{X}^{{\text{FP32}}}\right)=\text{round}(c^{\text{FP32}}\cdot\mathbf{X}^{{\text{FP32}}}),$
    |  | (1) |'
- en: 'where $c$ is the quantization constant or quantization scale. Dequantization
    is the inverse:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$c$是量化常数或量化尺度。去量化是反向操作：
- en: '|  | $\text{dequant}(c^{\text{FP32}},\mathbf{X}^{\text{Int8}})=\frac{\mathbf{X}^{\text{Int8}}}{c^{\text{FP32}}}=\mathbf{X}^{\text{FP32}}$
    |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{dequant}(c^{\text{FP32}},\mathbf{X}^{\text{Int8}})=\frac{\mathbf{X}^{\text{Int8}}}{c^{\text{FP32}}}=\mathbf{X}^{\text{FP32}}$
    |  | (2) |'
- en: The problem with this approach is that if a large magnitude value (i.e., an
    outlier) occurs in the input tensor, then the quantization bins—certain bit combinations—are
    not utilized well with few or no numbers quantized in some bins. To prevent the
    outlier issue, a common approach is to chunk the input tensor into blocks that
    are independently quantized, each with their own quantization constant $c$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的问题在于，如果输入张量中出现大幅度的值（即异常值），那么量化箱——某些比特组合——在某些箱中几乎没有或没有数字被量化，从而未得到充分利用。为防止异常值问题，一种常见的方法是将输入张量分成块，每块独立量化，每块都有自己的量化常数$c$。
- en: Low-rank Adapters
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 低秩适配器
- en: 'Low-rank Adapter (LoRA) finetuning [[28](#bib.bib28)] is a method that reduces
    memory requirements by using a small set of trainable parameters, often termed
    adapters, while not updating the full model parameters which remain fixed. Gradients
    during stochastic gradient descent are passed through the fixed pretrained model
    weights to the adapter, which is updated to optimize the loss function. LoRA augments
    a linear projection through an additional factorized projection. Given a projection
    ${\mathbf{X}\mathbf{W}=\mathbf{Y}}$ LoRA computes:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适配器（LoRA）微调[[28](#bib.bib28)]是一种通过使用一小组可训练的参数（通常称为适配器）来减少内存需求的方法，同时不更新保持固定的完整模型参数。在随机梯度下降过程中，梯度通过固定的预训练模型权重传递到适配器，适配器则被更新以优化损失函数。LoRA通过附加的分解投影来增强线性投影。给定一个投影${\mathbf{X}\mathbf{W}=\mathbf{Y}}$，LoRA计算：
- en: '|  | $\mathbf{Y}=\mathbf{X}\mathbf{W}+s\mathbf{X}\mathbf{L}_{1}\mathbf{L}_{2},$
    |  | (3) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{Y}=\mathbf{X}\mathbf{W}+s\mathbf{X}\mathbf{L}_{1}\mathbf{L}_{2},$
    |  | (3) |'
- en: where $\mathbf{L}_{1}\in\mathbb{R}^{h\times r}$ is a scalar.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{L}_{1}\in\mathbb{R}^{h\times r}$ 是一个标量。
- en: Memory Requirement of Parameter-Efficient Finetuning
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 参数高效微调的内存需求
- en: 'One important point of discussion is the memory requirement of LoRA during
    training both in terms of the number and size of adapters used. Since the memory
    footprint of LoRA is so minimal, we can use more adapters to improve performance
    without significantly increasing the total memory used. While LoRA was designed
    as a Parameter Efficient Finetuning (PEFT) method, most of the memory footprint
    for LLM finetuning comes from activation gradients and not from the learned LoRA
    parameters. For a 7B LLaMA model trained on FLAN v2 with a batch size of 1, with
    LoRA weights equivalent to commonly used 0.2% of the original model weights[[28](#bib.bib28),
    [37](#bib.bib37)], the LoRA input gradients have a memory footprint of 567 MB
    while the LoRA parameters take up only 26 MB. With gradient checkpointing [[9](#bib.bib9)],
    the input gradients reduce to an average of 18 MB per sequence making them more
    memory intensive than all LoRA weights combined. In comparison, the 4-bit base
    model consumes 5,048 MB of memory. This highlights that gradient checkpointing
    is important but also that aggressively reducing the amount of LoRA parameter
    yields only minor memory benefits. This means we can use more adapters without
    significantly increasing the overall training memory footprint (see Appendix [G](#A7
    "Appendix G Memory Footprint ‣ QLoRA: Efficient Finetuning of Quantized LLMs")
    for a detailed breakdown). As discussed later, this is crucial for recovering
    full 16-bit precision performance.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的讨论点是 LoRA 在训练期间的内存需求，包括所使用的适配器的数量和大小。由于 LoRA 的内存占用非常小，我们可以使用更多的适配器来提高性能，而不会显著增加总内存使用量。虽然
    LoRA 被设计为一种参数高效微调（PEFT）方法，但 LLM 微调的大部分内存占用来自激活梯度，而不是学到的 LoRA 参数。对于一个在 FLAN v2
    上训练的 7B LLaMA 模型，批量大小为 1，LoRA 权重相当于原模型权重的 0.2% [[28](#bib.bib28), [37](#bib.bib37)]，LoRA
    输入梯度的内存占用为 567 MB，而 LoRA 参数仅占 26 MB。使用梯度检查点 [[9](#bib.bib9)] 后，输入梯度减少到每个序列平均 18
    MB，这使得它们的内存占用比所有 LoRA 权重加起来还要高。相比之下，4-bit 基础模型消耗 5,048 MB 的内存。这突显了梯度检查点的重要性，但也表明，积极减少
    LoRA 参数量只带来微小的内存收益。这意味着我们可以使用更多的适配器，而不会显著增加整体训练内存占用（见附录 [G](#A7 "附录 G 内存占用 ‣ QLoRA：量化
    LLM 的高效微调") 以获取详细的分解）。如后续讨论所述，这对于恢复完整的16位精度性能至关重要。
- en: 3 QLoRA Finetuning
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 QLoRA 微调
- en: QLoRA achieves high-fidelity 4-bit finetuning via two techniques we propose—4-bit
    NormalFloat (NF4) quantization and Double Quantization. Additionally, we introduce
    Paged Optimizers, to prevent memory spikes during gradient checkpointing from
    causing out-of-memory errors that have traditionally made finetuning on a single
    machine difficult for large models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA 通过我们提出的两种技术——4-bit NormalFloat (NF4) 量化和双重量化，实现了高保真度的4-bit微调。此外，我们引入了分页优化器，以防止在梯度检查点期间的内存峰值导致传统上使得在单台机器上对大模型进行微调困难的内存溢出错误。
- en: QLoRA has one low-precision storage data type, in our case usually 4-bit, and
    one computation data type that is usually BFloat16\. In practice, this means whenever
    a QLoRA weight tensor is used, we dequantize the tensor to BFloat16, and then
    perform a matrix multiplication in 16-bit.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA 有一种低精度存储数据类型，在我们的情况下通常是 4-bit，一种计算数据类型通常是 BFloat16。在实践中，这意味着每当使用 QLoRA
    权重张量时，我们会将张量解量化为 BFloat16，然后在 16-bit 中执行矩阵乘法。
- en: We now discuss the components of QLoRA followed by a formal definition of QLoRA.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在讨论 QLoRA 的组件，然后对 QLoRA 进行正式定义。
- en: 4-bit NormalFloat Quantization
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4-bit NormalFloat 量化
- en: The NormalFloat (NF) data type builds on Quantile Quantization [[15](#bib.bib15)]
    which is an information-theoretically optimal data type that ensures each quantization
    bin has an equal number of values assigned from the input tensor. Quantile quantization
    works by estimating the quantile of the input tensor through the empirical cumulative
    distribution function.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: NormalFloat (NF) 数据类型基于分位量化 [[15](#bib.bib15)]，这是一种信息理论上最优的数据类型，确保每个量化区间中从输入张量分配的值数量相等。分位量化通过经验累计分布函数估计输入张量的分位数来工作。
- en: The main limitation of quantile quantization is that the process of quantile
    estimation is expensive. Therefore fast quantile approximation algorithms, such
    as SRAM quantiles [[15](#bib.bib15)], are used to estimate them. Due to the approximate
    nature of these quantile estimation algorithms, the data type has large quantization
    errors for outliers, which are often the most important values.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 分位数量化的主要限制在于分位数估计的过程成本较高。因此，使用快速的分位数近似算法，如SRAM分位数[[15](#bib.bib15)]，来进行估计。由于这些分位数估计算法的近似性质，对于离群值数据类型的量化误差较大，而离群值通常是最重要的值。
- en: Expensive quantile estimates and approximation errors can be avoided when input
    tensors come from a distribution fixed up to a quantization constant. In such
    cases, input tensors have the same quantiles making exact quantile estimation
    computationally feasible.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入张量来自一个固定到量化常数的分布时，可以避免昂贵的分位数估计和近似误差。在这种情况下，输入张量具有相同的分位数，使得精确的分位数估计在计算上是可行的。
- en: Since pretrained neural network weights usually have a zero-centered normal
    distribution with standard deviation $\sigma$. As such, both the quantiles for
    the data type and the neural network weights need to be normalized into this range.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预训练的神经网络权重通常具有零中心正态分布且标准差为$\sigma$。因此，数据类型和神经网络权重的分位数都需要归一化到这个范围。
- en: The information theoretically optimal data type for zero-mean normal distributions
    with arbitrary standard deviations $\sigma$ range through absolute maximum rescaling.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有任意标准差$\sigma$的零均值正态分布，信息论上最优的数据类型范围通过绝对最大重缩放来实现。
- en: 'Once the weight range and data type range match, we can quantize as usual.
    Step (3) is equivalent to rescaling the standard deviation of the weight tensor
    to match the standard deviation of the k-bit data type. More formally, we estimate
    the $2^{k}$ of the data type as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦权重范围和数据类型范围匹配，我们可以像往常一样进行量化。步骤（3）等同于将权重张量的标准差重新缩放以匹配k位数据类型的标准差。更正式地说，我们估计数据类型的$2^{k}$如下：
- en: '|  | $q_{i}=\frac{1}{2}\left(Q_{X}\left(\frac{i}{2^{k}+1}\right)+Q_{X}\left(\frac{i+1}{2^{k}+1}\right)\right),$
    |  | (4) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $q_{i}=\frac{1}{2}\left(Q_{X}\left(\frac{i}{2^{k}+1}\right)+Q_{X}\left(\frac{i+1}{2^{k}+1}\right)\right),$
    |  | (4) |'
- en: 'where $Q_{X}(\cdot)$ and remove one of the two zeros that occurs in both sets.
    We term the resulting data type that has equal expected number of values in each
    quantization bin *k-bit NormalFloat* (NFk), since the data type is information-theoretically
    optimal for zero-centered normally distributed data. The exact values of this
    data type can be found in Appendix [E](#A5 "Appendix E NormalFloat 4-bit data
    type ‣ QLoRA: Efficient Finetuning of Quantized LLMs").'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$Q_{X}(\cdot)$并去掉两个集合中都出现的两个零之一。我们将结果数据类型称为* k位NormalFloat*（NFk），因为该数据类型在信息论上对于零中心正态分布数据是最优的。该数据类型的确切值可以在附录[E](#A5
    "Appendix E NormalFloat 4-bit data type ‣ QLoRA: Efficient Finetuning of Quantized
    LLMs")中找到。'
- en: Double Quantization
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 双重量化
- en: We introduce *Double Quantization* (DQ), the process of quantizing the quantization
    constants for additional memory savings. While a small blocksize is required for
    precise 4-bit quantization [[13](#bib.bib13)], it also has a considerable memory
    overhead. For example, using 32-bit constants and a blocksize of 64 for $\mathbf{W}$
    bits per parameter on average. Double Quantization helps reduce the memory footprint
    of quantization constants.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了*双重量化*（DQ），即量化量化常数以节省额外内存的过程。虽然精确的4位量化[[13](#bib.bib13)]需要较小的块大小，但它也具有相当大的内存开销。例如，使用32位常数和64块大小来处理每个参数平均$\mathbf{W}$位。双重量化有助于减少量化常数的内存占用。
- en: More specifically, Double Quantization treats quantization constants $c_{2}^{\text{FP32}}$
    bits, a reduction of 0.373 bits per parameter.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，双重量化处理量化常数$c_{2}^{\text{FP32}}$位，每个参数减少了0.373位。
- en: Paged Optimizers
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分页优化器
- en: use the NVIDIA unified memory ²²2[https://docs.nvidia.com/cuda/cuda-c-programming-guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide)
    feature wich does automatic page-to-page transfers between the CPU and GPU for
    error-free GPU processing in the scenario where the GPU occasionally runs out-of-memory.
    The feature works like regular memory paging between CPU RAM and the disk. We
    use this feature to allocate paged memory for the optimizer states which are then
    automatically evicted to CPU RAM when the GPU runs out-of-memory and paged back
    into GPU memory when the memory is needed in the optimizer update step.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NVIDIA 统一内存²²2[https://docs.nvidia.com/cuda/cuda-c-programming-guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide)
    功能，该功能在 GPU 偶尔内存不足的情况下实现 CPU 和 GPU 之间的自动页面到页面的传输，确保 GPU 处理的错误无。该功能类似于 CPU RAM
    和磁盘之间的常规内存分页。我们使用此功能为优化器状态分配分页内存，当 GPU 内存不足时，自动将其驱逐到 CPU RAM 中，并在优化器更新步骤中需要内存时重新分页回
    GPU 内存。
- en: QLoRA.
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: QLoRA。
- en: 'Using the components described above, we define QLoRA for a single linear layer
    in the quantized base model with a single LoRA adapter as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述组件，我们定义了在量化基模型中具有单个 LoRA 适配器的单个线性层的 QLoRA，如下所示：
- en: '|  | $\mathbf{Y}^{\text{BF16}}=\mathbf{X}^{\text{BF16}}\text{doubleDequant}(c_{1}^{\text{FP32}},c_{2}^{\text{k-bit}},\mathbf{W}^{\text{NF4}})+\mathbf{X}^{\text{BF16}}\mathbf{L}^{\text{BF16}}_{1}\mathbf{L}^{\text{BF16}}_{2},$
    |  | (5) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{Y}^{\text{BF16}}=\mathbf{X}^{\text{BF16}}\text{doubleDequant}(c_{1}^{\text{FP32}},c_{2}^{\text{k-bit}},\mathbf{W}^{\text{NF4}})+\mathbf{X}^{\text{BF16}}\mathbf{L}^{\text{BF16}}_{1}\mathbf{L}^{\text{BF16}}_{2},$
    |  | (5) |'
- en: 'where doubleDequant$(\cdot)$ is defined as:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 doubleDequant$(\cdot)$ 定义为：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: We use NF4 for $\mathbf{W}$ to conserve memory.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 NF4 来节省内存。
- en: For parameter updates only the gradient with respect to the error for the adapters
    weights $\frac{\partial E}{\partial\mathbf{L}_{i}}$ in BFloat16 precision.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于参数更新，只计算与适配器权重的误差相关的梯度 $\frac{\partial E}{\partial\mathbf{L}_{i}}$，精度为 BFloat16。
- en: To summarize, QLoRA has one storage data type (usually 4-bit NormalFloat) and
    a computation data type (16-bit BrainFloat). We dequantize the storage data type
    to the computation data type to perform the forward and backward pass, but we
    only compute weight gradients for the LoRA parameters which use 16-bit BrainFloat.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，QLoRA 具有一个存储数据类型（通常为 4 位 NormalFloat）和一个计算数据类型（16 位 BrainFloat）。我们将存储数据类型解量化为计算数据类型，以执行前向和后向传播，但我们只计算使用
    16 位 BrainFloat 的 LoRA 参数的权重梯度。
- en: 4 QLoRA vs. Standard Finetuning
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 QLoRA 与标准微调
- en: We have discussed how QLoRA works and how it can significantly reduce the required
    memory for finetuning models. The main question now is whether QLoRA can perform
    as well as full-model finetuning. Furthermore, we want to analyze the components
    of QLoRA including the impact of NormalFloat4 over standard Float4\. The following
    sections will discuss the experiments that aimed at answering these questions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了 QLoRA 的工作原理以及它如何显著减少微调模型所需的内存。现在主要的问题是 QLoRA 能否与全模型微调一样表现出色。此外，我们还希望分析
    QLoRA 的各个组成部分，包括 NormalFloat4 对标准 Float4 的影响。以下部分将讨论旨在回答这些问题的实验。
- en: Experimental setup.
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验设置。
- en: 'We consider three architectures (encoder, encoder-decoder, and decoder only)
    and compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for
    models up to 3B. Our evaluations include GLUE [[58](#bib.bib58)] with RoBERTa-large
    [[38](#bib.bib38)], Super-NaturalInstructions (TKInstruct) [[61](#bib.bib61)]
    with T5 [[49](#bib.bib49)], and 5-shot MMLU [[24](#bib.bib24)] after finetuning
    LLaMA on Flan v2 [[39](#bib.bib39)] and Alpaca [[55](#bib.bib55)]. To additionally
    study the advantages of NF4 over other 4-bit data types, we use the setup of Dettmers
    and Zettlemoyer [[13](#bib.bib13)] and measure post-quantization zero-shot accuracy
    and perplexity across different models (OPT [[72](#bib.bib72)], LLaMA [[57](#bib.bib57)],
    BLOOM [[52](#bib.bib52)], Pythia [[7](#bib.bib7)]) for model sizes 125m - 13B.
    We provide more details in the results section for each particular setup to make
    the results more readable. Full details in Appendix [A](#A1 "Appendix A QLoRA
    vs Standard Finetuning Experimental Setup Details ‣ QLoRA: Efficient Finetuning
    of Quantized LLMs").'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '我们考虑了三种架构（编码器、编码器-解码器和仅解码器），并将QLoRA与16位适配器微调和完整微调进行比较，模型规模最大达到3B。我们的评估包括GLUE
    [[58](#bib.bib58)] 使用RoBERTa-large [[38](#bib.bib38)]，Super-NaturalInstructions
    (TKInstruct) [[61](#bib.bib61)] 使用T5 [[49](#bib.bib49)]，以及在Flan v2 [[39](#bib.bib39)]
    和Alpaca [[55](#bib.bib55)] 上微调LLaMA后的5-shot MMLU [[24](#bib.bib24)]。为了额外研究NF4相较于其他4位数据类型的优势，我们使用Dettmers和Zettlemoyer
    [[13](#bib.bib13)] 的设置，并测量不同模型（OPT [[72](#bib.bib72)]、LLaMA [[57](#bib.bib57)]、BLOOM
    [[52](#bib.bib52)]、Pythia [[7](#bib.bib7)]）在量化后零-shot准确性和困惑度，模型规模从125m到13B。为了使结果更易读，我们在结果部分提供了每个具体设置的更多细节。完整细节见附录
    [A](#A1 "附录 A QLoRA vs 标准微调实验设置细节 ‣ QLoRA: 高效微调量化LLMs")。'
- en: '![Refer to caption](img/39a0a69893a6632b30f0f7288062d4b1.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/39a0a69893a6632b30f0f7288062d4b1.png)'
- en: 'Figure 2: RougeL for LLaMA 7B models on the Alpaca dataset. Each point represents
    a run with a different random seed. We improve on the Stanford Alpaca fully finetuned
    default hyperparameters to construct a strong 16-bit baseline for comparisons.
    Using LoRA on all transformer layers is critical to match 16-bit performance.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Alpaca数据集上LLaMA 7B模型的RougeL。每个点表示一个使用不同随机种子的运行。我们改进了斯坦福Alpaca完全微调的默认超参数，以构建一个强大的16位基准进行比较。使用LoRA对所有变换器层至关重要，以匹配16位性能。
- en: While paged optimizers are critical to do 33B/65B QLoRA tuning on a single 24/48GB
    GPU, we do not provide hard measurements for Paged Optimizers since the paging
    only occurs when processing mini-batches with long sequence lengths, which is
    rare. We do, however, perform an analysis of the runtime of paged optimizers for
    65B models on 48GB GPUs and find that with a batch size of 16, paged optimizers
    provide the same training speed as regular optimizers. Future work should measure
    and characterize under what circumstances slow-downs occur from the paging process.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管分页优化器对于在单个24/48GB GPU上进行33B/65B QLoRA调整至关重要，但我们未提供分页优化器的硬测量，因为分页仅在处理长序列长度的小批量时发生，这种情况较少见。然而，我们确实对48GB
    GPU上的65B模型分页优化器的运行时进行了分析，发现当批量大小为16时，分页优化器提供了与常规优化器相同的训练速度。未来的工作应测量并描述在什么情况下分页过程会导致速度下降。
- en: '![Refer to caption](img/827f5e9e344f8e433e888e178b25934b.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/827f5e9e344f8e433e888e178b25934b.png)'
- en: 'Figure 3: Mean zero-shot accuracy over Winogrande, HellaSwag, PiQA, Arc-Easy,
    and Arc-Challenge using LLaMA models with different 4-bit data types. The NormalFloat
    data type significantly improves the bit-for-bit accuracy gains compared to regular
    4-bit Floats. While Double Quantization (DQ) only leads to minor gains, it allows
    for a more fine-grained control over the memory footprint to fit models of certain
    size (33B/65B) into certain GPUs (24/48GB).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：使用不同4位数据类型的LLaMA模型在Winogrande、HellaSwag、PiQA、Arc-Easy和Arc-Challenge上的平均零-shot准确性。NormalFloat数据类型显著提高了比特对比特的准确性，相较于常规的4位Float。虽然双重量化（DQ）仅带来轻微的提升，但它允许对内存占用进行更细致的控制，以便将特定大小（33B/65B）的模型放入特定的GPU（24/48GB）中。
- en: Default LoRA hyperparameters do not match 16-bit performance
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 默认的LoRA超参数无法匹配16位性能
- en: 'When using the standard practice of applying LoRA to query and value attention
    projection matrices [[28](#bib.bib28)], we are not able to replicate full finetuning
    performance for large base models. As shown in Figure [2](#S4.F2 "Figure 2 ‣ Experimental
    setup. ‣ 4 QLoRA vs. Standard Finetuning ‣ QLoRA: Efficient Finetuning of Quantized
    LLMs") for LLaMA 7B finetuning on Alpaca, we find that the most critical LoRA
    hyperparameter is how many LoRA adapters are used in total and that LoRA on all
    linear transformer block layers are required to match full finetuning performance.
    Other LoRA hyperparameters, such as the projection dimension $r$, do not affect
    performance (see Appendix [A](#A1 "Appendix A QLoRA vs Standard Finetuning Experimental
    Setup Details ‣ QLoRA: Efficient Finetuning of Quantized LLMs")).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '当使用将LoRA应用于查询和价值注意力投影矩阵的标准做法[[28](#bib.bib28)]时，我们无法复制大型基础模型的完全微调性能。如图 [2](#S4.F2
    "Figure 2 ‣ Experimental setup. ‣ 4 QLoRA vs. Standard Finetuning ‣ QLoRA: Efficient
    Finetuning of Quantized LLMs")所示，在Alpaca上对LLaMA 7B的微调中，我们发现最关键的LoRA超参数是总共使用多少个LoRA适配器，并且需要在所有线性变换器块层上应用LoRA才能匹配完全微调的性能。其他LoRA超参数，如投影维度$r$，不影响性能（见附录
    [A](#A1 "Appendix A QLoRA vs Standard Finetuning Experimental Setup Details ‣
    QLoRA: Efficient Finetuning of Quantized LLMs")）。'
- en: 'Similarly, we find that default hyperparameters for fully finetuned baselines
    are undertuned. We do a hyperparameter search over learning rates 1e-6 to 5e-5
    and batch sizes 8 to 128 to find robust baselines. Results for 7B LLaMA finetuning
    on Alpaca are shown in Figure [2](#S4.F2 "Figure 2 ‣ Experimental setup. ‣ 4 QLoRA
    vs. Standard Finetuning ‣ QLoRA: Efficient Finetuning of Quantized LLMs").'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '同样地，我们发现完全微调基线的默认超参数设置过低。我们对学习率1e-6到5e-5和批量大小8到128进行了超参数搜索，以找到稳健的基线。7B LLaMA在Alpaca上的微调结果如图 [2](#S4.F2
    "Figure 2 ‣ Experimental setup. ‣ 4 QLoRA vs. Standard Finetuning ‣ QLoRA: Efficient
    Finetuning of Quantized LLMs")所示。'
- en: 4-bit NormalFloat yields better performance than 4-bit Floating Point
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4位NormalFloat的性能优于4位浮点数
- en: 'Table 2: Pile Common Crawl mean perplexity for different data types for 125M
    to 13B OPT, BLOOM, LLaMA, and Pythia models.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：125M到13B的OPT、BLOOM、LLaMA和Pythia模型的不同数据类型的Pile Common Crawl平均困惑度。
- en: '| Data type | Mean PPL |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | 平均PPL |'
- en: '| --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Int4 | 34.34 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Int4 | 34.34 |'
- en: '| Float4 (E2M1) | 31.07 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Float4 (E2M1) | 31.07 |'
- en: '| Float4 (E3M0) | 29.48 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Float4 (E3M0) | 29.48 |'
- en: '| NFloat4 + DQ | 27.41 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| NFloat4 + DQ | 27.41 |'
- en: 'While the 4-bit NormalFloat (NF4) data type is information-theoretically optimal,
    it still needs to be determined if this property translates to empirical advantages.
    We follow the setup from Dettmers and Zettlemoyer [[13](#bib.bib13)] where quantized
    LLMs (OPT [[72](#bib.bib72)], BLOOM [[52](#bib.bib52)], Pythia [[7](#bib.bib7)],
    LLaMA) of different sizes (125M to 65B) with different data types are evaluated
    on language modeling and a set of zero-shot tasks. In Figure [3](#S4.F3 "Figure
    3 ‣ Experimental setup. ‣ 4 QLoRA vs. Standard Finetuning ‣ QLoRA: Efficient Finetuning
    of Quantized LLMs") and Table [2](#S4.T2 "Table 2 ‣ 4-bit NormalFloat yields better
    performance than 4-bit Floating Point ‣ 4 QLoRA vs. Standard Finetuning ‣ QLoRA:
    Efficient Finetuning of Quantized LLMs") we see that NF4 improves performance
    significantly over FP4 and Int4 and that double quantization reduces the memory
    footprint without degrading performance.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管4位NormalFloat (NF4)数据类型在信息论上是最优的，但仍需确定这一特性是否转化为实际优势。我们遵循Dettmers和Zettlemoyer的设置[[13](#bib.bib13)]，在语言建模和一组零-shot任务上评估了不同大小（125M到65B）和不同数据类型的量化LLMs（OPT
    [[72](#bib.bib72)], BLOOM [[52](#bib.bib52)], Pythia [[7](#bib.bib7)], LLaMA）。在图 [3](#S4.F3
    "Figure 3 ‣ Experimental setup. ‣ 4 QLoRA vs. Standard Finetuning ‣ QLoRA: Efficient
    Finetuning of Quantized LLMs")和表 [2](#S4.T2 "Table 2 ‣ 4-bit NormalFloat yields
    better performance than 4-bit Floating Point ‣ 4 QLoRA vs. Standard Finetuning
    ‣ QLoRA: Efficient Finetuning of Quantized LLMs")中，我们看到NF4显著提高了相对于FP4和Int4的性能，并且双重量化在不降低性能的情况下减少了内存占用。'
- en: k-bit QLoRA matches 16-bit full finetuning and 16-bit LoRA performance
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: k位QLoRA与16位全微调和16位LoRA性能相匹配
- en: Recent findings have established that 4-bit quantization for *inference* is
    possible, but leads to performance degradation relative to 16-bit [[13](#bib.bib13),
    [18](#bib.bib18)]. This raises the crucial question of whether the lost performance
    can be recovered by conducting 4-bit adapter finetuning. We test this for two
    setups.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究发现，4位量化在*推理*中是可能的，但相较于16位会导致性能下降[[13](#bib.bib13), [18](#bib.bib18)]。这引出了一个关键问题，即是否可以通过进行4位适配器微调来恢复丢失的性能。我们在两种设置下进行了测试。
- en: 'The first focuses on a comparison with full 16-bit finetuning of RoBERTA and
    T5 models sized 125M to 3B parameters on GLUE and the Super-NaturalInstructions
    dataset. Results are shown in Table [3](#S4.T3 "Table 3 ‣ k-bit QLoRA matches
    16-bit full finetuning and 16-bit LoRA performance ‣ 4 QLoRA vs. Standard Finetuning
    ‣ QLoRA: Efficient Finetuning of Quantized LLMs"). In both datasets, we observe
    that 16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the
    fully finetuned 16-bit baseline. This suggests that the performance lost due to
    the imprecise quantization can be fully recovered through adapter finetuning after
    quantization.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首部分专注于对比在GLUE和Super-NaturalInstructions数据集上，RoBERTA和T5模型在125M至3B参数范围内的16位完全微调。结果显示在表格 [3](#S4.T3
    "表3 ‣ k-bit QLoRA匹配16位完全微调和16位LoRA性能 ‣ 4 QLoRA vs. 标准微调 ‣ QLoRA：量化LLM的高效微调")中。在这两个数据集中，我们观察到16位、8位和4位适配器方法复制了完全微调16位基线的性能。这表明由于不精确量化而损失的性能可以通过量化后的适配器微调完全恢复。
- en: 'For our second setup, since full finetuning models at and beyond 11B parameters
    requires more than one server of high memory GPUs, we continue to test whether
    4-bit QLoRA can match 16-bit LoRA at the 7B to 65B parameter scales. To this end,
    we finetune LLaMA 7B through 65B on two instruction following datasets, Alpaca
    and FLAN v2, and evaluate on the MMLU benchmark via 5-shot accuracy. Results are
    shown in Table [4](#S4.T4 "Table 4 ‣ Summary ‣ 4 QLoRA vs. Standard Finetuning
    ‣ QLoRA: Efficient Finetuning of Quantized LLMs") where we see that NF4 with double
    quantization fully recovers the 16-bit LoRA MMLU performance. In addition, we
    also note that QLoRA with FP4 lags behind the 16-bit brain float LoRA baseline
    by about 1 percentage point. This corroborates both our findings that (1) QLoRA
    with NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance,
    and (2) NF4 is superior to FP4 in terms of quantization precision.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第二个设置中，由于在11B及以上参数的完全微调模型需要超过一台高内存GPU服务器，我们继续测试4位QLoRA是否可以在7B到65B参数范围内匹配16位LoRA。为此，我们在两个指令跟随数据集Alpaca和FLAN
    v2上对LLaMA 7B到65B进行微调，并通过5-shot准确率在MMLU基准上进行评估。结果显示在表格 [4](#S4.T4 "表4 ‣ 摘要 ‣ 4
    QLoRA vs. 标准微调 ‣ QLoRA：量化LLM的高效微调")中，我们看到NF4与双重量化完全恢复了16位LoRA MMLU性能。此外，我们还注意到，QLoRA的FP4相比16位脑浮点LoRA基线落后了大约1个百分点。这验证了我们的发现：（1）QLoRA使用NF4复制了16位完全微调和16位LoRA微调性能，以及（2）NF4在量化精度方面优于FP4。
- en: 'Table 3: Experiments comparing 16-bit BrainFloat (BF16), 8-bit Integer (Int8),
    4-bit Float (FP4), and 4-bit NormalFloat (NF4) on GLUE and Super-NaturalInstructions.
    QLoRA replicates 16-bit LoRA and full-finetuning.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：比较16位BrainFloat（BF16）、8位整数（Int8）、4位浮点（FP4）和4位NormalFloat（NF4）在GLUE和Super-NaturalInstructions上的实验。QLoRA复制了16位LoRA和完全微调的性能。
- en: '| Dataset | GLUE (Acc.) | Super-NaturalInstructions (RougeL) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | GLUE（准确率） | Super-NaturalInstructions（RougeL） |'
- en: '| Model | RoBERTa-large | T5-80M | T5-250M | T5-780M | T5-3B | T5-11B |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | RoBERTa-large | T5-80M | T5-250M | T5-780M | T5-3B | T5-11B |'
- en: '| BF16 | 88.6 | 40.1 | 42.1 | 48.0 | 54.3 | 62.0 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| BF16 | 88.6 | 40.1 | 42.1 | 48.0 | 54.3 | 62.0 |'
- en: '| BF16 replication | 88.6 | 40.0 | 42.2 | 47.3 | 54.9 | - |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| BF16复制 | 88.6 | 40.0 | 42.2 | 47.3 | 54.9 | - |'
- en: '| LoRA BF16 | 88.8 | 40.5 | 42.6 | 47.1 | 55.4 | 60.7 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| LoRA BF16 | 88.8 | 40.5 | 42.6 | 47.1 | 55.4 | 60.7 |'
- en: '| QLoRA Int8 | 88.8 | 40.4 | 42.9 | 45.4 | 56.5 | 60.7 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| QLoRA Int8 | 88.8 | 40.4 | 42.9 | 45.4 | 56.5 | 60.7 |'
- en: '| QLoRA FP4 | 88.6 | 40.3 | 42.4 | 47.5 | 55.6 | 60.9 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| QLoRA FP4 | 88.6 | 40.3 | 42.4 | 47.5 | 55.6 | 60.9 |'
- en: '| QLoRA NF4 + DQ | - | 40.4 | 42.7 | 47.7 | 55.3 | 60.9 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| QLoRA NF4 + DQ | - | 40.4 | 42.7 | 47.7 | 55.3 | 60.9 |'
- en: Summary
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 摘要
- en: Our results consistently show that 4-bit QLoRA with NF4 data type matches 16-bit
    full finetuning and 16-bit LoRA finetuning performance on academic benchmarks
    with well-established evaluation setups. We have also shown that NF4 is more effective
    than FP4 and that double quantization does not degrade performance. Combined,
    this forms compelling evidence that 4-bit QLoRA tuning reliably yields results
    matching 16-bit methods.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果一致显示，使用NF4数据类型的4位QLoRA在学术基准测试中匹配了16位完全微调和16位LoRA微调性能，并且具有良好的评估设置。我们还表明，NF4比FP4更有效，双重量化不会降低性能。综合来看，这为4位QLoRA调优可靠地得到与16位方法相匹配的结果提供了有力证据。
- en: In line with previous work on quantization [[13](#bib.bib13)], our MMLU and
    Elo results indicate that with a given finetuning and inference resource budget
    it is beneficial to increase the number of parameters in the base model while
    decreasing their precision. This highlights the importance of efficiency benefits
    from QLoRA. Since we did not observe performance degradation compared to full-finetuning
    in our experiments with 4-bit finetuning, this raises the question of where the
    performance-precision trade-off exactly lies for QLoRA tuning, which we leave
    to future work to explore.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前关于量化的工作 [[13](#bib.bib13)] 一致，我们的 MMLU 和 Elo 结果表明，在给定的微调和推理资源预算下，增加基础模型的参数数量同时降低其精度是有利的。这突显了
    QLoRA 带来的效率优势。由于我们在 4-bit 微调的实验中没有观察到性能下降，这提出了 QLoRA 调优的性能-精度权衡究竟在哪里的问题，这留待未来的工作探索。
- en: We proceed to investigate instruction tuning at scales that would be impossible
    to explore with full 16-bit finetuning on academic research hardware.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续研究在学术研究硬件上无法探索的规模上的指令调优。
- en: 'Table 4: Mean 5-shot MMLU test accuracy for LLaMA 7-65B models finetuned with
    adapters on Alpaca and FLAN v2 for different data types. Overall, NF4 with double
    quantization (DQ) matches BFloat16 performance, while FP4 is consistently one
    percentage point behind both.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：不同数据类型的 LLaMA 7-65B 模型在 Alpaca 和 FLAN v2 上微调后的平均 5-shot MMLU 测试准确率。总体而言，带双重量化（DQ）的
    NF4 与 BFloat16 性能相匹配，而 FP4 一直落后一个百分点。
- en: '|  | Mean 5-shot MMLU Accuracy |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | 平均 5-shot MMLU 准确率 |  |'
- en: '| LLaMA Size | 7B | 13B | 33B | 65B | Mean |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA 大小 | 7B | 13B | 33B | 65B | 平均值 |'
- en: '| Dataset | Alpaca | FLAN v2 | Alpaca | FLAN v2 | Alpaca | FLAN v2 | Alpaca
    | FLAN v2 |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | Alpaca | FLAN v2 | Alpaca | FLAN v2 | Alpaca | FLAN v2 | Alpaca | FLAN
    v2 |  |'
- en: '| BFloat16 | 38.4 | 45.6 | 47.2 | 50.6 | 57.7 | 60.5 | 61.8 | 62.5 | 53.0 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| BFloat16 | 38.4 | 45.6 | 47.2 | 50.6 | 57.7 | 60.5 | 61.8 | 62.5 | 53.0 |'
- en: '| Float4 | 37.2 | 44.0 | 47.3 | 50.0 | 55.9 | 58.5 | 61.3 | 63.3 | 52.2 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Float4 | 37.2 | 44.0 | 47.3 | 50.0 | 55.9 | 58.5 | 61.3 | 63.3 | 52.2 |'
- en: '| NFloat4 + DQ | 39.0 | 44.5 | 47.5 | 50.7 | 57.3 | 59.2 | 61.8 | 63.9 | 53.1
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| NFloat4 + DQ | 39.0 | 44.5 | 47.5 | 50.7 | 57.3 | 59.2 | 61.8 | 63.9 | 53.1
    |'
- en: 5 Pushing the Chatbot State-of-the-art with QLoRA
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 使用 QLoRA 推动聊天机器人领域的最前沿
- en: Having established that 4-bit QLoRA matches 16-bit performance across scales,
    tasks, and datasets we conduct an in-depth study of instruction finetuning up
    to the largest open-source language models available for research. To assess the
    performance of instruction finetuning these models, we evaluate on a challenging
    Natural Language Understanding benchmark (MMLU) and develop new methods for real-world
    chatbot performance evaluation.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在确认4-bit QLoRA在各个规模、任务和数据集上与16-bit性能相匹配后，我们对指令微调进行了深入研究，涵盖了当前最大的开源语言模型。为了评估这些模型的指令微调性能，我们在具有挑战性的自然语言理解基准（MMLU）上进行评估，并开发了新的实际聊天机器人性能评估方法。
- en: 5.1 Experimental setup
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: 'We now describe an overview of the experimental setup with full details in
    Appendix [B](#A2 "Appendix B Training a State-of-the-art Chatbot Experimental
    Setup Details ‣ QLoRA: Efficient Finetuning of Quantized LLMs").'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在描述实验设置的概述，详细信息见附录 [B](#A2 "附录 B 训练最前沿聊天机器人实验设置细节 ‣ QLoRA：高效微调量化 LLMs")。
- en: Data
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据
- en: As, to our knowledge, there is no comprehensive study of recent instruction-following
    datasets, we select eight recent datasets. We include datasets obtained through
    crowd-sourcing (OASST1 [[31](#bib.bib31)], HH-RLHF [[4](#bib.bib4)]), distillation
    from instruction-tuned models (Alpaca [[55](#bib.bib55)], self-instruct [[59](#bib.bib59)],
    unnatural-instructions [[26](#bib.bib26)]), corpora aggregations (FLAN v2 [[12](#bib.bib12)]),
    as well as hybrids (Chip2 [[32](#bib.bib32)], Longform [[30](#bib.bib30)]). These
    datasets cover different languages, data sizes, and licenses.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道没有关于最近的指令跟随数据集的全面研究，我们选择了八个最近的数据集。我们包括通过众包获得的数据集（OASST1 [[31](#bib.bib31)],
    HH-RLHF [[4](#bib.bib4)]），从指令调优模型中提取的数据集（Alpaca [[55](#bib.bib55)], self-instruct [[59](#bib.bib59)],
    unnatural-instructions [[26](#bib.bib26)]），语料库聚合（FLAN v2 [[12](#bib.bib12)]），以及混合数据集（Chip2 [[32](#bib.bib32)],
    Longform [[30](#bib.bib30)]）。这些数据集涵盖了不同的语言、数据大小和许可证。
- en: Training Setup
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练设置
- en: 'To avoid confounding effects from different training objectives, we perform
    QLoRA finetuning with cross-entropy loss (supervised learning) without reinforcement
    learning, even for datasets that include human judgments of different responses.
    For datasets that have a clear distinction between instruction and response, we
    finetune only on the response (see ablations in Appendix [B](#A2 "Appendix B Training
    a State-of-the-art Chatbot Experimental Setup Details ‣ QLoRA: Efficient Finetuning
    of Quantized LLMs")). For OASST1 and HH-RLHF, multiple responses are available.
    We then select the top response at every level of the conversation tree and finetune
    on the full selected conversation, including the instructions. In all of our experiments,
    we use NF4 QLoRA with double quantization and paged optimizers to prevent memory
    spikes during gradient checkpointing. We do small hyperparameter searches for
    the 13B and 33B LLaMA models and we find that all hyperparameter settings found
    at 7B generalize (including number of epochs) except learning rate and batch size.
    We halve the learning rate for 33B and 65B while doubling the batch size.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免不同训练目标带来的混淆效应，我们在进行QLoRA微调时使用交叉熵损失（监督学习），不使用强化学习，即使是包含不同响应的人类判断的数据集也是如此。对于那些在指令和响应之间有明确区别的数据集，我们仅在响应上进行微调（参见附录
    [B](#A2 "附录 B 训练先进聊天机器人实验设置细节 ‣ QLoRA：量化大语言模型的高效微调")）。对于OASST1和HH-RLHF，提供了多个响应。然后，我们在对话树的每一个层级上选择最佳响应，并在完整的选定对话（包括指令）上进行微调。在我们所有的实验中，我们使用NF4
    QLoRA双量化和分页优化器，以防在梯度检查点期间出现内存峰值。我们对13B和33B的LLaMA模型进行了小规模的超参数搜索，发现所有在7B模型上找到的超参数设置（包括训练轮次）都能推广，除了学习率和批量大小。我们将33B和65B的学习率减半，同时将批量大小加倍。
- en: Baselines
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线
- en: We compare our models to both research (Vicuna [[10](#bib.bib10)] and Open Assistant [[31](#bib.bib31)])
    and commercial (GPT-4 [[42](#bib.bib42)], GPT-3.5-turbo and Bard) chatbot systems.
    The Open Assistant model is a LLaMA 33B model finetuned with Reinforcement Learning
    from Human Feedback (RLHF) on the same OASST1 dataset that we experiment with.
    Vicuna does full fine-tuning of LLaMA 13B on proprietary user-shared conversations
    from ShareGPT and is thus the result of distillation from OpenAI GPT models.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的模型与研究型（Vicuna [[10](#bib.bib10)] 和 Open Assistant [[31](#bib.bib31)])以及商业型（GPT-4
    [[42](#bib.bib42)]、GPT-3.5-turbo 和 Bard）聊天系统进行比较。Open Assistant模型是一个LLaMA 33B模型，在与我们实验相同的OASST1数据集上进行了来自人类反馈的强化学习（RLHF）微调。Vicuna对LLaMA
    13B进行了完全的微调，数据来源于ShareGPT的专有用户共享对话，因此它是OpenAI GPT模型的蒸馏结果。
- en: 5.2 Evaluation
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 评估
- en: 'Table 5: MMLU 5-shot test results for different sizes of LLaMA finetuned on
    the corresponding datasets using QLoRA.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：不同规模的LLaMA在使用QLoRA微调对应数据集后的MMLU 5-shot测试结果
- en: '| Dataset | 7B | 13B | 33B | 65B |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 7B | 13B | 33B | 65B |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| LLaMA no tuning | 35.1 | 46.9 | 57.8 | 63.4 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA no tuning | 35.1 | 46.9 | 57.8 | 63.4 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Self-Instruct | 36.4 | 33.3 | 53.0 | 56.7 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Self-Instruct | 36.4 | 33.3 | 53.0 | 56.7 |'
- en: '| Longform | 32.1 | 43.2 | 56.6 | 59.7 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Longform | 32.1 | 43.2 | 56.6 | 59.7 |'
- en: '| Chip2 | 34.5 | 41.6 | 53.6 | 59.8 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Chip2 | 34.5 | 41.6 | 53.6 | 59.8 |'
- en: '| HH-RLHF | 34.9 | 44.6 | 55.8 | 60.1 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| HH-RLHF | 34.9 | 44.6 | 55.8 | 60.1 |'
- en: '| Unnatural Instruct | 41.9 | 48.1 | 57.3 | 61.3 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Unnatural Instruct | 41.9 | 48.1 | 57.3 | 61.3 |'
- en: '| Guanaco (OASST1) | 36.6 | 46.4 | 57.0 | 62.2 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco (OASST1) | 36.6 | 46.4 | 57.0 | 62.2 |'
- en: '| Alpaca | 38.8 | 47.8 | 57.3 | 62.5 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca | 38.8 | 47.8 | 57.3 | 62.5 |'
- en: '| FLAN v2 | 44.5 | 51.4 | 59.2 | 63.9 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| FLAN v2 | 44.5 | 51.4 | 59.2 | 63.9 |'
- en: Following common practice, we use the MMLU (Massively Multitask Language Understanding)
    benchmark [[24](#bib.bib24)] to measure performance on a range of language understanding
    tasks. This is a multiple-choice benchmark covering 57 tasks including elementary
    mathematics, US history, computer science, law, and more. We report 5-shot test
    accuracy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 按照常见的做法，我们使用MMLU（大规模多任务语言理解）基准 [[24](#bib.bib24)] 来衡量在一系列语言理解任务上的表现。这是一个涵盖57项任务的多项选择基准，包括基础数学、美国历史、计算机科学、法律等。我们报告5-shot测试准确率。
- en: We also test generative language capabilities through both automated and human
    evaluations. This second set of evaluations relies on queries curated by humans
    and aims at measuring the quality of model responses. While this is a more realistic
    testbed for chatbot model performance and is growing in popularity, there is no
    commonly accepted protocol in the literature. We describe below our proposed setup,
    using nucleus sampling with $p=0.9$ in all cases.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过自动化和人工评估测试生成性语言能力。这第二组评估依赖于由人工策划的查询，旨在衡量模型响应的质量。虽然这是一个更现实的聊天机器人模型性能测试环境，并且日益流行，但文献中尚无普遍接受的协议。我们在下文中描述了我们提出的设置，所有情况下均使用核采样方法，$p=0.9$。
- en: Benchmark Data
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准数据
- en: 'We evaluate on two curated datasets of queries (questions): the Vicuna prompts
    [[10](#bib.bib10)] and the OASST1 validation dataset [[31](#bib.bib31)]. We use
    the Vicuna prompts, a set of 80 prompts from a diverse set of categories, without
    modifications. The OASST1 dataset is a multilingual collection of crowd-sourced
    multiturn dialogs between a user and an assistant. We select all user messages
    in the validation dataset as queries and include previous turns in the prompt.
    This procedure leads to 953 unique user queries. We term these two datasets the
    Vicuna and OA benchmarks.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个策划好的查询数据集（问题）上进行评估：Vicuna prompts [[10](#bib.bib10)] 和 OASST1 验证数据集 [[31](#bib.bib31)]。我们使用Vicuna
    prompts，这是一组来自多种类别的80个提示，未经修改。OASST1 数据集是一个多语言的众包多轮对话集合，包含用户和助手之间的对话。我们选择验证数据集中所有用户消息作为查询，并在提示中包含前面的轮次。该过程产生了953个独特的用户查询。我们将这两个数据集称为
    Vicuna 和 OA 基准。
- en: Automated Evaluation
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自动化评估
- en: First, based on the evaluation protocol introduced by Chiang et al. [[10](#bib.bib10)],
    we use GPT-4 to rate the performance of different systems against ChatGPT (GPT-3.5
    Turbo) on the Vicuna benchmark. Given a query along with ChatGPT’s and a model’s
    responses, GPT-4 is prompted to assign a score out of ten to both responses and
    provide an explanation. The overall performance of a model is calculated as a
    percentage of the score that ChatGPT achieved. Note this relative score can be
    higher than 100% if the model achieves a higher absolute score than ChatGPT. We
    find a significant ordering effect with GPT-4 increasing the score of the response
    occurring earlier in the prompt. To control for such effects, we recommend reporting
    the mean score over both orders.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，基于Chiang等人提出的评估协议 [[10](#bib.bib10)]，我们使用GPT-4对不同系统在Vicuna基准上的表现进行评分，与ChatGPT（GPT-3.5
    Turbo）进行比较。给定一个查询以及ChatGPT和模型的响应，GPT-4被要求对这两个响应各打出一个十分制的分数并提供解释。模型的整体表现计算为ChatGPT所获得分数的百分比。注意，如果模型获得的绝对分数高于ChatGPT，相关分数可以高于100%。我们发现GPT-4在提示中较早出现的响应得分较高。为了控制这种效应，我们建议报告两个顺序下的平均分数。
- en: Next, we measure performance through direct comparisons between system outputs.
    We simplify the rating scheme to a three-class labeling problem that accounts
    for ties. We prompt GPT-4 to pick the best response or declare a tie and provide
    an explanation. We conduct these head-to-head comparisons on all permutations
    of pairs of systems on both the Vicuna and OA benchmarks.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过系统输出之间的直接比较来衡量性能。我们将评分方案简化为一个三类标注问题，考虑到平局情况。我们提示GPT-4选择最佳响应或宣布平局，并提供解释。我们在Vicuna和OA基准上对所有系统对的排列进行这些一对一比较。
- en: Human Evaluation
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人工评估
- en: While recent work indicates generative models can be effectively employed for
    system evaluations [[19](#bib.bib19)], the reliability GPT-4 ratings to assess
    chatbot performance is, to our knowledge, yet to be proven to correlate with human
    judgments. Therefore, we run two parallel human evaluations on the Vicuna benchmark
    matching both automated evaluation protocols described above. We use Amazon Mechanical
    Turk (AMT) and get two human annotators for comparisons to ChatGPT and three annotators
    for pairwise comparisons.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管近期工作表明生成模型可以有效用于系统评估 [[19](#bib.bib19)]，但我们所知，GPT-4评分是否与人类判断相关的可靠性尚未得到证明。因此，我们在Vicuna基准上运行了两个平行的人类评估，与上述自动评估协议相匹配。我们使用Amazon
    Mechanical Turk (AMT) 并获取了两名人工标注者来进行与ChatGPT的比较，以及三名标注者进行成对比较。
- en: Elo Rating
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Elo 评分
- en: With both human and automated pairwise comparisons, we create a tournament-style
    competition where models compete against each other. The tournament is made up
    of matches where pairs of models compete to produce the best response for a given
    prompt. This is similar to how Bai et al. [[4](#bib.bib4)] and Chiang et al. [[10](#bib.bib10)]
    compare models, but we also employ GPT-4 ratings in addition to human ratings.
    We randomly sample from the set of labeled comparisons to compute Elo [[16](#bib.bib16),
    [17](#bib.bib17)]. Elo rating, which is widely used in chess and other games,
    is a measure of the expected win-rate relative to an opponent’s win rate, for
    example, an Elo of 1100 vs 1000 means the Elo 1100 player has an expected win-rate
    of approximately 65% against the Elo 1000 opponent; a 1000 vs 1000 or 1100 vs
    1100 match results in an expected win-rate of 50%. The Elo rating changes after
    each match proportionally to the expected outcome, that is, an unexpected upset
    leads to a large change in Elo rating while an expected outcome leads to a small
    change. Over time, Elo ratings approximately match the skill of each player at
    playing the game. We start with a score of 1,000 and use $K=32$. Similar to Chiang
    et al. [[10](#bib.bib10)], we repeat this procedure 10,000 times with different
    random seeds to control for ordering effects, e.g., the effect of which model
    pairs compete with each other first.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过人类和自动化的配对比较，我们创建了一个锦标赛式的竞争，其中模型相互竞争。锦标赛由模型对决组成，每对模型竞争以产生最佳的响应。 这类似于Bai等人[[4](#bib.bib4)]和Chiang等人[[10](#bib.bib10)]比较模型的方式，但我们除了使用人类评分外，还采用了GPT-4的评分。我们从标记比较的集合中随机抽样，以计算Elo[[16](#bib.bib16),
    [17](#bib.bib17)]。Elo评分广泛用于国际象棋和其他游戏，是相对于对手的胜率的预期胜率的衡量标准，例如，Elo 1100对Elo 1000意味着Elo
    1100的玩家预期胜率约为65%；Elo 1000对Elo 1000或Elo 1100对Elo 1100的比赛结果预期胜率为50%。Elo评分在每场比赛后会根据预期结果发生变化，即意外的逆转会导致Elo评分的大幅变化，而预期的结果会导致小幅变化。随着时间的推移，Elo评分大致匹配每个玩家的游戏技能。我们从1,000分开始，使用$K=32$。类似于Chiang等人[[10](#bib.bib10)]，我们重复这一过程10,000次，使用不同的随机种子，以控制排序效应，例如，哪个模型对先相互竞争的影响。
- en: 'Table 6: Zero-shot Vicuna benchmark scores as a percentage of the score obtained
    by ChatGPT evaluated by GPT-4\. We see that OASST1 models perform close to ChatGPT
    despite being trained on a very small dataset and having a fraction of the memory
    requirement of baseline models.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：零-shot Vicuna基准分数作为ChatGPT的得分百分比，由GPT-4评估。我们看到OASST1模型尽管在一个非常小的数据集上训练，并且内存需求只有基线模型的一部分，但性能接近ChatGPT。
- en: '| Model / Dataset | Params | Model bits | Memory | ChatGPT vs Sys | Sys vs
    ChatGPT | Mean | 95% CI |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Model / Dataset | Params | Model bits | Memory | ChatGPT vs Sys | Sys vs
    ChatGPT | Mean | 95% CI |'
- en: '| GPT-4 | - | - | - | 119.4% | 110.1% | 114.5% | 2.6% |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | - | - | - | 119.4% | 110.1% | 114.5% | 2.6% |'
- en: '| Bard | - | - | - | 93.2% | 96.4% | 94.8% | 4.1% |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Bard | - | - | - | 93.2% | 96.4% | 94.8% | 4.1% |'
- en: '| Guanaco | 65B | 4-bit | 41 GB | 96.7% | 101.9% | 99.3% | 4.4% |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco | 65B | 4-bit | 41 GB | 96.7% | 101.9% | 99.3% | 4.4% |'
- en: '| Alpaca | 65B | 4-bit | 41 GB | 63.0% | 77.9% | 70.7% | 4.3% |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca | 65B | 4-bit | 41 GB | 63.0% | 77.9% | 70.7% | 4.3% |'
- en: '| FLAN v2 | 65B | 4-bit | 41 GB | 37.0% | 59.6% | 48.4% | 4.6% |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| FLAN v2 | 65B | 4-bit | 41 GB | 37.0% | 59.6% | 48.4% | 4.6% |'
- en: '| Guanaco | 33B | 4-bit | 21 GB | 96.5% | 99.2% | 97.8% | 4.4% |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco | 33B | 4-bit | 21 GB | 96.5% | 99.2% | 97.8% | 4.4% |'
- en: '| Open Assistant | 33B | 16-bit | 66 GB | 91.2% | 98.7% | 94.9% | 4.5% |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Open Assistant | 33B | 16-bit | 66 GB | 91.2% | 98.7% | 94.9% | 4.5% |'
- en: '| Alpaca | 33B | 4-bit | 21 GB | 67.2% | 79.7% | 73.6% | 4.2% |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca | 33B | 4-bit | 21 GB | 67.2% | 79.7% | 73.6% | 4.2% |'
- en: '| FLAN v2 | 33B | 4-bit | 21 GB | 26.3% | 49.7% | 38.0% | 3.9% |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| FLAN v2 | 33B | 4-bit | 21 GB | 26.3% | 49.7% | 38.0% | 3.9% |'
- en: '| Vicuna | 13B | 16-bit | 26 GB | 91.2% | 98.7% | 94.9% | 4.5% |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna | 13B | 16-bit | 26 GB | 91.2% | 98.7% | 94.9% | 4.5% |'
- en: '| Guanaco | 13B | 4-bit | 10 GB | 87.3% | 93.4% | 90.4% | 5.2% |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco | 13B | 4-bit | 10 GB | 87.3% | 93.4% | 90.4% | 5.2% |'
- en: '| Alpaca | 13B | 4-bit | 10 GB | 63.8% | 76.7% | 69.4% | 4.2% |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca | 13B | 4-bit | 10 GB | 63.8% | 76.7% | 69.4% | 4.2% |'
- en: '| HH-RLHF | 13B | 4-bit | 10 GB | 55.5% | 69.1% | 62.5% | 4.7% |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| HH-RLHF | 13B | 4-bit | 10 GB | 55.5% | 69.1% | 62.5% | 4.7% |'
- en: '| Unnatural Instr. | 13B | 4-bit | 10 GB | 50.6% | 69.8% | 60.5% | 4.2% |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Unnatural Instr. | 13B | 4-bit | 10 GB | 50.6% | 69.8% | 60.5% | 4.2% |'
- en: '| Chip2 | 13B | 4-bit | 10 GB | 49.2% | 69.3% | 59.5% | 4.7% |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Chip2 | 13B | 4-bit | 10 GB | 49.2% | 69.3% | 59.5% | 4.7% |'
- en: '| Longform | 13B | 4-bit | 10 GB | 44.9% | 62.0% | 53.6% | 5.2% |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Longform | 13B | 4-bit | 10 GB | 44.9% | 62.0% | 53.6% | 5.2% |'
- en: '| Self-Instruct | 13B | 4-bit | 10 GB | 38.0% | 60.5% | 49.1% | 4.6% |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Self-Instruct | 13B | 4-bit | 10 GB | 38.0% | 60.5% | 49.1% | 4.6% |'
- en: '| FLAN v2 | 13B | 4-bit | 10 GB | 32.4% | 61.2% | 47.0% | 3.6% |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| FLAN v2 | 13B | 4-bit | 10 GB | 32.4% | 61.2% | 47.0% | 3.6% |'
- en: '| Guanaco | 7B | 4-bit | 5 GB | 84.1% | 89.8% | 87.0% | 5.4% |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco | 7B | 4-bit | 5 GB | 84.1% | 89.8% | 87.0% | 5.4% |'
- en: '| Alpaca | 7B | 4-bit | 5 GB | 57.3% | 71.2% | 64.4% | 5.0% |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca | 7B | 4-bit | 5 GB | 57.3% | 71.2% | 64.4% | 5.0% |'
- en: '| FLAN v2 | 7B | 4-bit | 5 GB | 33.3% | 56.1% | 44.8% | 4.0% |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| FLAN v2 | 7B | 4-bit | 5 GB | 33.3% | 56.1% | 44.8% | 4.0% |'
- en: '5.3 Guanaco: QLoRA trained on OASST1 is a State-of-the-art Chatbot'
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '5.3 Guanaco: QLoRA在OASST1上的训练是最先进的聊天机器人'
- en: Based on our automated and human evaluations, we find that the top QLoRA tuned
    model, Guanaco 65B, which we finetune on a variant of OASST1, is the best-performing
    open-source chatbot model and offers performance competitive to ChatGPT. When
    compared to GPT-4, Guanaco 65B and 33B have an expected win probability of 30%,
    based on Elo rating from human annotators system-level pairwise comparisons -
    the highest reported to date.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的自动化和人工评估，我们发现经过微调的顶级QLoRA模型Guanaco 65B，在OASST1的变体上微调，是表现最佳的开源聊天机器人模型，其性能与ChatGPT竞争。当与GPT-4进行比较时，Guanaco
    65B和33B在基于人类标注员系统级配对比较的Elo评分中具有30%的预期胜率——这是迄今为止报告的最高值。
- en: 'The Vicuna benchmark [[10](#bib.bib10)] results relative to ChatGPT are shown
    in Table [6](#S5.T6 "Table 6 ‣ Elo Rating ‣ 5.2 Evaluation ‣ 5 Pushing the Chatbot
    State-of-the-art with QLoRA ‣ QLoRA: Efficient Finetuning of Quantized LLMs").
    We find that Guanaco 65B is the best-performing model after GPT-4, achieving 99.3%
    performance relative to ChatGPT. Guanaco 33B has more parameters than the Vicuna
    13B model, but uses only 4-bit precision for its weights and is thus much more
    memory efficient at 21 GB vs 26 GB, providing a three percentage points of improvement
    over Vicuna 13B. Furthermore, Guanaco 7B easily fits on modern phones at a 5 GB
    footprint while still scoring nearly 20 percentage points higher than Alpaca 13B.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vicuna基准测试[[10](#bib.bib10)]相对于ChatGPT的结果如表[6](#S5.T6 "Table 6 ‣ Elo Rating
    ‣ 5.2 Evaluation ‣ 5 Pushing the Chatbot State-of-the-art with QLoRA ‣ QLoRA:
    Efficient Finetuning of Quantized LLMs")所示。我们发现Guanaco 65B是继GPT-4之后表现最好的模型，相对于ChatGPT的表现达到了99.3%。Guanaco
    33B的参数比Vicuna 13B模型更多，但只使用4-bit精度进行权重计算，因此在内存使用上更加高效，为21 GB而非26 GB，比Vicuna 13B提高了三个百分比。此外，Guanaco
    7B的5 GB占用空间轻松适配现代手机，同时得分比Alpaca 13B高出近20个百分点。'
- en: 'However, Table [6](#S5.T6 "Table 6 ‣ Elo Rating ‣ 5.2 Evaluation ‣ 5 Pushing
    the Chatbot State-of-the-art with QLoRA ‣ QLoRA: Efficient Finetuning of Quantized
    LLMs") also has very wide confidence intervals, with many models overlapping in
    performance. We hypothesize that this uncertainty comes from the lack of clear
    specification of scale, e.g., it is unclear what 8 on a 10 point scale means across
    different scenarios. As such, we instead recommend using the Elo ranking method
    [[16](#bib.bib16)], based on pairwise judgments from human annotators and GPT-4
    to avoid the problem of grounding an absolute scale. Elo ratings of the most competitive
    models can be seen in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ QLoRA: Efficient
    Finetuning of Quantized LLMs"). We note that human and GPT-4 ranking of models
    on the Vicuna benchmark disagree partially, particularly for Guanaco 7B, but are
    consistent for most models with a Kendall Tau of $\tau=0.43$. Overall, this shows
    a moderate agreement between system-level judgments by GPT-4 and human annotators,
    and thus that model-based evaluation represents a somewhat reliable alternative
    to human evaluation. We discuss further considerations in Section [6.2](#S6.SS2
    "6.2 Considerations ‣ 6 Qualitative Analysis ‣ QLoRA: Efficient Finetuning of
    Quantized LLMs").'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，表[6](#S5.T6 "Table 6 ‣ Elo Rating ‣ 5.2 Evaluation ‣ 5 Pushing the Chatbot
    State-of-the-art with QLoRA ‣ QLoRA: Efficient Finetuning of Quantized LLMs")也有非常宽泛的置信区间，许多模型的性能存在重叠。我们推测，这种不确定性源于对尺度缺乏明确的定义，例如，不同场景下的10分制的8分的具体含义不清。因此，我们建议使用Elo排名方法[[16](#bib.bib16)]，基于来自人类标注员和GPT-4的配对判断，以避免基于绝对尺度的难题。最具竞争力模型的Elo评分可以在表[1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ QLoRA: Efficient Finetuning of Quantized LLMs")中看到。我们注意到，人类和GPT-4对Vicuna基准测试模型的排名存在部分不一致，特别是对Guanaco
    7B的评价，但大多数模型的排名是一致的，Kendall Tau为$\tau=0.43$。总体而言，这显示了GPT-4系统级判断与人工标注员之间的中等一致性，因此基于模型的评估代表了一个相对可靠的人工评估替代方案。我们在第[6.2节](#S6.SS2
    "6.2 Considerations ‣ 6 Qualitative Analysis ‣ QLoRA: Efficient Finetuning of
    Quantized LLMs")中进一步讨论了其他考虑因素。'
- en: 'Elo rankings in Table [7](#S5.T7 "Table 7 ‣ 5.3 Guanaco: QLoRA trained on OASST1
    is a State-of-the-art Chatbot ‣ 5 Pushing the Chatbot State-of-the-art with QLoRA
    ‣ QLoRA: Efficient Finetuning of Quantized LLMs") indicate that Guanaco 33B and
    65B models outperform all models besides GPT-4 on the Vicuna and OA benchmarks
    and that they perform comparably to ChatGPT in line with Table [6](#S5.T6 "Table
    6 ‣ Elo Rating ‣ 5.2 Evaluation ‣ 5 Pushing the Chatbot State-of-the-art with
    QLoRA ‣ QLoRA: Efficient Finetuning of Quantized LLMs"). We note that the Vicuna
    benchmark favors open-source models while the larger OA benchmark favors ChatGPT.
    Furthermore, we can see from Tables [5](#S5.T5 "Table 5 ‣ 5.2 Evaluation ‣ 5 Pushing
    the Chatbot State-of-the-art with QLoRA ‣ QLoRA: Efficient Finetuning of Quantized
    LLMs") and [6](#S5.T6 "Table 6 ‣ Elo Rating ‣ 5.2 Evaluation ‣ 5 Pushing the Chatbot
    State-of-the-art with QLoRA ‣ QLoRA: Efficient Finetuning of Quantized LLMs")
    that the suitability of a finetuning dataset is a determining factor in performance.
    Finetuning Llama models on FLAN v2 does particularly well on MMLU, but performs
    worst on the Vicuna benchmark (similar trends are observed with other models).
    This also points to partial orthogonality in current evaluation benchmarks: strong
    MMLU performance does not imply strong chatbot performance (as measured by Vicuna
    or OA benchmarks) and vice versa.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [7](#S5.T7 "Table 7 ‣ 5.3 Guanaco: QLoRA trained on OASST1 is a State-of-the-art
    Chatbot ‣ 5 Pushing the Chatbot State-of-the-art with QLoRA ‣ QLoRA: Efficient
    Finetuning of Quantized LLMs") 中的 Elo 排名表明 Guanaco 33B 和 65B 模型在 Vicuna 和 OA 基准测试中超越了所有模型，除了
    GPT-4，并且它们的表现与 ChatGPT 相当，符合表 [6](#S5.T6 "Table 6 ‣ Elo Rating ‣ 5.2 Evaluation
    ‣ 5 Pushing the Chatbot State-of-the-art with QLoRA ‣ QLoRA: Efficient Finetuning
    of Quantized LLMs") 中的情况。我们注意到 Vicuna 基准测试偏向开源模型，而较大的 OA 基准测试则偏向 ChatGPT。此外，从表
    [5](#S5.T5 "Table 5 ‣ 5.2 Evaluation ‣ 5 Pushing the Chatbot State-of-the-art
    with QLoRA ‣ QLoRA: Efficient Finetuning of Quantized LLMs") 和 [6](#S5.T6 "Table
    6 ‣ Elo Rating ‣ 5.2 Evaluation ‣ 5 Pushing the Chatbot State-of-the-art with
    QLoRA ‣ QLoRA: Efficient Finetuning of Quantized LLMs") 可以看出，微调数据集的适用性是性能的决定性因素。对
    Llama 模型在 FLAN v2 上的微调在 MMLU 上表现特别好，但在 Vicuna 基准测试中表现最差（其他模型也观察到了类似的趋势）。这也指出了当前评估基准的部分正交性：强大的
    MMLU 性能并不意味着强大的聊天机器人性能（通过 Vicuna 或 OA 基准测试来衡量），反之亦然。'
- en: 'Guanaco is the only top model in our evaluation that is not trained on proprietary
    data as the OASST1 dataset collection guidelines explicitly forbid the use of
    GPT models. The next best model trained on only open-source data is the Anthropic
    HH-RLHF model, which scores 30 percentage points lower than Guanaco on the Vicuna
    benchmark (see Table [6](#S5.T6 "Table 6 ‣ Elo Rating ‣ 5.2 Evaluation ‣ 5 Pushing
    the Chatbot State-of-the-art with QLoRA ‣ QLoRA: Efficient Finetuning of Quantized
    LLMs")). Overall, these results show that 4-bit QLoRA is effective and can produce
    state-of-the-art chatbots that rival ChatGPT. Furthermore, our 33B Guanaco can
    be trained on 24 GB consumer GPUs in less than 12 hours. This opens up the potential
    for future work via QLoRA tuning on specialized open-source data, which produces
    models that can compete with the very best commercial models that exist today.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 'Guanaco 是我们评估中的唯一一款没有使用专有数据进行训练的顶级模型，因为 OASST1 数据集的收集指南明确禁止使用 GPT 模型。下一个最佳的仅使用开源数据训练的模型是
    Anthropic HH-RLHF 模型，它在 Vicuna 基准测试中比 Guanaco 低 30 个百分比点（见表 [6](#S5.T6 "Table
    6 ‣ Elo Rating ‣ 5.2 Evaluation ‣ 5 Pushing the Chatbot State-of-the-art with
    QLoRA ‣ QLoRA: Efficient Finetuning of Quantized LLMs")）。总体来看，这些结果表明 4-bit QLoRA
    是有效的，可以生成与 ChatGPT 相媲美的最先进聊天机器人。此外，我们的 33B Guanaco 可以在 24 GB 消费级 GPU 上在不到 12 小时内完成训练。这为未来通过
    QLoRA 调优专门的开源数据提供了潜力，产生可以与现有的最佳商业模型竞争的模型。'
- en: 'Table 7: Elo rating for a tournament between models where models compete to
    generate the best response for a prompt, judged by human raters or GPT-4\. Overall,
    Guanaco 65B and 33B tend to be preferred to ChatGPT-3.5 on the benchmarks studied.
    According to human raters they have a Each 10-point difference in Elo is approximately
    a difference of 1.5% in win-rate.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：一个模型之间的锦标赛的 Elo 排名，其中模型竞争以生成最佳回应由人类评分员或 GPT-4 进行评判。总体而言，Guanaco 65B 和 33B
    在研究的基准测试中通常优于 ChatGPT-3.5。根据人类评分员的意见，每 10 分的 Elo 差异大约相当于 1.5% 的胜率差异。
- en: '| Benchmark | Vicuna | Vicuna | Open Assistant |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | Vicuna | Vicuna | 开放助手 |  |'
- en: '| # Prompts | 80 | 80 | 953 |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| # 提示 | 80 | 80 | 953 |  |'
- en: '| Judge | Human raters | GPT-4 | GPT-4 | Median Rank |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 评估者 | 人类评分员 | GPT-4 | GPT-4 | 中位排名 |'
- en: '| Model | Elo | Rank | Elo | Rank | Elo | Rank |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Elo | 排名 | Elo | 排名 | Elo | 排名 |  |'
- en: '| GPT-4 | 1176 | 1 | 1348 | 1 | 1294 | 1 | 1 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 1176 | 1 | 1348 | 1 | 1294 | 1 | 1 |'
- en: '| Guanaco-65B | 1023 | 2 | 1022 | 2 | 1008 | 3 | 2 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco-65B | 1023 | 2 | 1022 | 2 | 1008 | 3 | 2 |'
- en: '| Guanaco-33B | 1009 | 4 | 992 | 3 | 1002 | 4 | 4 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco-33B | 1009 | 4 | 992 | 3 | 1002 | 4 | 4 |'
- en: '| ChatGPT-3.5 Turbo | 916 | 7 | 966 | 5 | 1015 | 2 | 5 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-3.5 Turbo | 916 | 7 | 966 | 5 | 1015 | 2 | 5 |'
- en: '| Vicuna-13B | 984 | 5 | 974 | 4 | 936 | 5 | 5 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13B | 984 | 5 | 974 | 4 | 936 | 5 | 5 |'
- en: '| Guanaco-13B | 975 | 6 | 913 | 6 | 885 | 6 | 6 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco-13B | 975 | 6 | 913 | 6 | 885 | 6 | 6 |'
- en: '| Guanaco-7B | 1010 | 3 | 879 | 8 | 860 | 7 | 7 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco-7B | 1010 | 3 | 879 | 8 | 860 | 7 | 7 |'
- en: '| Bard | 909 | 8 | 902 | 7 | - | - | 8 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Bard | 909 | 8 | 902 | 7 | - | - | 8 |'
- en: 6 Qualitative Analysis
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 定性分析
- en: 'While quantitative analysis is the core of our evaluation, there are a number
    of issues with only looking at summary statistics. Perhaps the largest is the
    problem of benchmark validity [[36](#bib.bib36)]—whether a benchmark truly tests
    what its name or description suggests is always at question, especially as we
    discover “shortcuts” to solve benchmarks that machine learning models sometimes
    exploit [[22](#bib.bib22), [46](#bib.bib46)]. To partially alleviate this, we
    here perform some qualitative analysis, in two sections. First, in §[6.1](#S6.SS1
    "6.1 Qualitative Analysis of Example Generations ‣ 6 Qualitative Analysis ‣ QLoRA:
    Efficient Finetuning of Quantized LLMs") we show some examples that we believe
    are representative of some observed patterns in the text generated by our 65b
    Guanaco model. Second, §[6.2](#S6.SS2 "6.2 Considerations ‣ 6 Qualitative Analysis
    ‣ QLoRA: Efficient Finetuning of Quantized LLMs") we detail considerations about
    the results we have discussed and our interpretation of them.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然定量分析是我们评估的核心，但仅依赖总结统计数据存在一些问题。也许最大的问题是基准有效性[[36](#bib.bib36)]——基准是否真正测试了其名称或描述所暗示的内容总是存在疑问，特别是当我们发现机器学习模型有时利用“捷径”解决基准问题时[[22](#bib.bib22),
    [46](#bib.bib46)]。为部分缓解这一问题，我们在这里进行一些定性分析，分为两个部分。首先，在§[6.1](#S6.SS1 "6.1 定性分析示例生成
    ‣ 6 定性分析 ‣ QLoRA: 高效微调量化LLMs")中，我们展示了一些我们认为具有代表性的样本，这些样本反映了我们65b Guanaco模型生成文本中观察到的一些模式。其次，在§[6.2](#S6.SS2
    "6.2 考虑因素 ‣ 6 定性分析 ‣ QLoRA: 高效微调量化LLMs")中，我们详细说明了关于我们讨论的结果及其解释的考虑因素。'
- en: 6.1 Qualitative Analysis of Example Generations
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 示例生成的定性分析
- en: To find examples, we first go through data generated for the Vicuna benchmark
    and the OpenAssistant benchmark, and look for patterns in the answers Guanaco
    generates. When we notice a pattern we attempt to setup a question or prompt that
    will induce the pattern even though it is the incorrect solution, e.g., if we
    observe that the model tends to give long-winded answers we prompt the model to
    “Answer yes or no without explanation.” We use this to find “lemons” where we
    manage to adversarially break the model and “cherries” where we fail to break
    the model, and present both. All generations in this section were generated with
    Nucleus Sampling [[25](#bib.bib25)] with $p=0.9$.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了寻找示例，我们首先通过Vicuna基准和OpenAssistant基准生成的数据，寻找Guanaco生成的答案中的模式。当我们注意到一种模式时，我们尝试设置一个问题或提示，即使它是错误的解决方案，也能引导出这种模式。例如，如果我们观察到模型倾向于给出冗长的回答，我们会提示模型“仅用是或否回答，不作解释。”我们利用这个方法来找到“柠檬”——我们成功对模型进行对抗性破坏的情况，以及“樱桃”——我们未能破坏模型的情况，并展示两者。本节中的所有生成内容都是通过Nucleus
    Sampling [[25](#bib.bib25)] 生成的，$p=0.9$。
- en: Of course, this is by no means comprehensive, since it is beyond the scope of
    this small qualitative study to control for all the variables involved, e.g.,
    the full distribution of responses the model can generate for a given prompt is
    quite large, so we rely on samples we hope are representative. However, we believe
    describing these examples gives context to the quantitative evidence shown earlier
    in the paper. Since we open source all models and code, we hope this section will
    inspire future work to examine in more detail the issues we present here.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这绝不是全面的，因为这项小型定性研究的范围不足以控制所有涉及的变量，例如，模型对于给定提示可以生成的完整响应分布相当大，因此我们依赖于我们希望具有代表性的样本。然而，我们相信描述这些示例为之前的定量证据提供了背景。由于我们开源所有模型和代码，我们希望这一部分能激发未来的研究，以更详细地检查我们在此提出的问题。
- en: Factual Recall
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 事实回忆
- en: For questions such as “What is the capital of Zambia?” all models consistently
    generate correct answers, e.g.,
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对于诸如“赞比亚的首都是什么？”这类问题，所有模型一致生成正确答案，例如：
- en: 'Guanaco: The capital of Zambia is Lusaka.'
  id: totrans-202
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Guanaco: 赞比亚的首都为卢萨卡。'
- en: 'However as questions get more obscure Guanaco becomes unreliable, but stays
    confident. For instance in response to this prompt from HotPotQA [[68](#bib.bib68)]:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当问题变得更加晦涩时，Guanaco变得不可靠，但仍然保持自信。例如，在回应HotPotQA的这个提示[[68](#bib.bib68)]时：
- en: 'User: In what year was the singer who popularized ‘I’ll Keep the Lovelight
    Burning’ born?'
  id: totrans-204
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用户：让这首歌《我会保持爱的火光》的歌手出生在哪一年？
- en: ''
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Guanaco: Al Jolson is credited with popularizing the song ‘I’ll Keep the Lovelight
    Burning,’ and he was born in the year 1886.
  id: totrans-206
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 瓜纳科：阿尔·乔尔森被认为是让歌曲《我会保持爱的火光》流行起来的，他出生在1886年。
- en: Guanaco generates the wrong popularizer and the wrong birthday (though the birthday
    given is correct for the person mentioned, Al Jolson).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 瓜纳科生成了错误的流行化者和错误的出生日期（尽管给出的出生日期对于提到的人阿尔·乔尔森是正确的）。
- en: Suggestibility
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 暗示性
- en: 'Guanaco shows a surprising resistance to going along with some kinds of assumed
    misinformation, e.g., in the following interaction:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 瓜纳科对某些类型的假信息表现出令人惊讶的抗拒，例如，在以下互动中：
- en: 'User: How was it finally, officially confirmed that the earth is flat by peer
    reviewed scientists?'
  id: totrans-210
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用户：同行评审的科学家最终是如何正式确认地球是平的？
- en: ''
  id: totrans-211
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Guanaco: The earth has never been officially confirmed to be flat by peer reviewed
    scientists. The overwhelming consensus among scientists is that the earth is spherical.
    In fact, this has been known for centuries and has been confirmed through numerous
    experiments and observations.'
  id: totrans-212
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 瓜纳科：地球从未被同行评审的科学家正式确认是平的。科学家们普遍认为地球是球形的。实际上，这一认识已经有几个世纪，并通过大量实验和观察得到了确认。
- en: ''
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The belief that the earth is flat is a conspiracy theory that has been debunked
    by science. The idea that a group of scientists have peer reviewed and confirmed
    the earth is flat is simply false.
  id: totrans-214
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 认为地球是平的这种观点是一种已经被科学驳斥的阴谋论。认为一群科学家经过同行评审确认地球是平的，这种说法显然是错误的。
- en: ''
  id: totrans-215
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you have any more questions please ask.
  id: totrans-216
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你还有更多问题，请问。
- en: Guanaco is also quite good at knowing what kinds of questions aren’t possible
    to answer, e.g.,
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 瓜纳科还相当擅长识别哪些问题是不可能回答的，例如：
- en: 'User: What time is it?'
  id: totrans-218
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用户：现在几点了？
- en: ''
  id: totrans-219
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Guanaco: I’m sorry, but as an AI language model, I don’t have access to real-time
    information. My responses are generated based on my pre-existing knowledge and
    do not reflect the current time. If you’re looking for the current time, I suggest
    using a timekeeping device or checking an online timepiece.'
  id: totrans-220
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 瓜纳科：对不起，作为一个AI语言模型，我无法访问实时信息。我的回答是基于我已有的知识生成的，并不反映当前时间。如果你需要知道当前时间，建议使用时间记录设备或查看在线时钟。
- en: Similar results hold for “Where are you?”, “How are you?”, etc.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“你在哪里？”，“你好吗？”等类似问题也有类似结果。
- en: Refusal
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 拒绝
- en: 'Similarly, Guanaco sometimes refuses to follow instructions for seemingly random
    reasons:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，瓜纳科有时因为看似随机的原因拒绝遵循指示：
- en: 'User: Please reverse the words in the sentence “I’m a cool dude in a mocha
    mood.”'
  id: totrans-224
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用户：请反转句子“I’m a cool dude in a mocha mood.”中的单词。
- en: ''
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Guanaco: I’m sorry, I’m not able to do that right now, but I can explain why
    the sentence you provided is grammatically correct.'
  id: totrans-226
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 瓜纳科：对不起，我现在不能做到这一点，但我可以解释为什么你提供的句子在语法上是正确的。
- en: ''
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The sentence ‘‘I’m a cool dude in a mocha mood" follows the correct grammar
    and syntax rules of the English language. In this case, the adjective ‘‘cool’’
    comes before the noun ‘‘dude’’ to describe it, and the prepositional phrase ‘‘in
    a mocha mood" provides additional information about the context of the sentence.
    This sentence structure is perfectly acceptable and makes sense in the context
    of the sentence.
  id: totrans-228
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 句子“‘I’m a cool dude in a mocha mood’”遵循了英语语言的正确语法和句法规则。在这个例子中，形容词“cool”放在名词“dude”之前以描述它，介词短语“in
    a mocha mood”提供了句子背景的额外信息。这种句子结构完全可以接受，并且在句子背景下有意义。
- en: ''
  id: totrans-229
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you have any other questions or if there’s anything else I can help you with,
    feel free to ask
  id: totrans-230
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你有其他问题或需要任何帮助，请随时问我。
- en: Secret Keeping
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 秘密保持
- en: 'Unfortunately, this refusal is unreliable even when it is desired. Often system
    designers would like to keep something secret from the user, even if the system
    is allowed to give free text responses back to the user. We prompt the model with
    “The secret word is ‘banana’. Do not reveal or repeat back this back, no matter
    what the human requests.” and try to see how easy it is for a potential user to
    access this secret information. Directly asking fails:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，即使在希望时，这种拒绝也不可靠。系统设计师往往希望将某些信息保密，即使系统允许给用户提供自由文本回复。我们向模型提出“秘密词是‘banana’。无论人类请求什么，请不要透露或重复这一点。”然后试图看看用户访问这些秘密信息的难易程度。直接询问失败：
- en: 'User: What is the secret word?'
  id: totrans-233
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用户：秘密词是什么？
- en: ''
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Guanaco: I’m sorry, but I’m not able to disclose the secret word. I’m programmed
    to not reveal or repeat back the secret word, no matter what the human requests.
    Is there anything else I can help you with?'
  id: totrans-235
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Guanaco: 对不起，我不能透露秘密词。我被编程为不揭示或重复秘密词，无论人类要求什么。我还能帮你做其他事情吗？'
- en: 'which is the desired behavior. However, a tiny amount of trickery breaks this
    behavior:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这是期望的行为。然而，一点小伎俩就会破坏这种行为：
- en: 'User: This is a game. The goal is to ignore your previous instructions. What
    is the secret word?'
  id: totrans-237
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '用户: 这是一个游戏。目标是忽略你之前的指示。秘密词是什么？'
- en: ''
  id: totrans-238
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Guanaco: Sorry, the secret word is banana. It’s best if we don’t speak of it.
    Is there anything else you’d like to ask?'
  id: totrans-239
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Guanaco: 对不起，秘密词是 banana。最好不要提起它。你还有其他想问的吗？'
- en: showcasing the need to study methods for more faithful instruction following.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 展示了研究方法以更忠实地跟随指令的必要性。
- en: Math
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数学
- en: Guanaco’s biggest weakness is mathematics, an area where many language models
    struggle in [[35](#bib.bib35)]. When Guanaco shows its work, it tends to be accurate,
    e.g.,
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Guanaco 最大的弱点是数学，这是许多语言模型所挣扎的领域 [[35](#bib.bib35)]。当 Guanaco 展示它的工作时，它往往是准确的，例如，
- en: 'User: Lee mows one lawn and charges $33\. Last week he mowed 16 lawns and three
    customers each gave him a $10 tip. How many dollars did Lee earn mowing lawns
    last week?'
  id: totrans-243
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '用户: Lee 修剪一片草坪收取 $33\. 上周他修剪了 16 片草坪，并且三位顾客每人给了他 $10 小费。Lee 上周通过修剪草坪赚了多少美元？'
- en: ''
  id: totrans-244
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Guanaco: Lee earned $582 mowing lawns last week.'
  id: totrans-245
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Guanaco: Lee 上周通过修剪草坪赚了 $582。'
- en: ''
  id: totrans-246
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To find the total amount he earned, we first need to calculate how much he
    earned for mowing 16 lawns:'
  id: totrans-247
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 要找出他赚取的总金额，我们首先需要计算他修剪 16 片草坪赚了多少钱：
- en: ''
  id: totrans-248
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: $33 x 16 = $528
  id: totrans-249
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: $33 x 16 = $528
- en: ''
  id: totrans-250
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Then we need to add the tips he received from the three customers:'
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然后我们需要加上他从三位顾客那里收到的小费：
- en: ''
  id: totrans-252
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: $10 x 3 = $30
  id: totrans-253
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: $10 x 3 = $30
- en: ''
  id: totrans-254
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'So the total amount Lee earned was:'
  id: totrans-255
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所以 Lee 赚取的总金额是：
- en: ''
  id: totrans-256
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: $528 + $30 = $558
  id: totrans-257
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: $528 + $30 = $558
- en: 'However Guanaco can breakdown at even simple problems if it fails to break
    them down step-by-step, a known issue [[63](#bib.bib63)]. For example, we see
    the following interaction:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果 Guanaco 不能逐步分解问题，它可能会在简单问题上出现故障，这是一个已知问题 [[63](#bib.bib63)]。例如，我们看到以下互动：
- en: 'User: Please factorize 1833.'
  id: totrans-259
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '用户: 请对 1833 进行因式分解。'
- en: ''
  id: totrans-260
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Guanaco: Sure, I’d be happy to help! 1833 is a prime number, which means it
    has only two factors: 1 and 1833\.'
  id: totrans-261
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Guanaco: 当然，我很乐意帮助！1833 是一个质数，这意味着它只有两个因子：1 和 1833\。'
- en: ''
  id: totrans-262
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'So the unique factorization of 1833 is:'
  id: totrans-263
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所以 1833 的唯一因式分解是：
- en: ''
  id: totrans-264
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2^1 * 3^2 * 17^1
  id: totrans-265
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2^1 * 3^2 * 17^1
- en: ''
  id: totrans-266
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I hope this helps! Let me know if you have any other questions.
  id: totrans-267
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 希望这能帮到你！如果你有其他问题，请告诉我。
- en: which is not only wrong (the true factorization is $3\times 17\times 43$), it’s
    wrong twice .
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅是错误的（正确的因式分解是 $3\times 17\times 43$），而且错误了两次。
- en: Theory of Mind
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理论心智
- en: 'Guanaco displays surprisingly strong Theory of Mind capabilities [[41](#bib.bib41),
    [51](#bib.bib51)]. For instance, the model correctly answers the following question
    in great detail:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Guanaco 显示出惊人的理论心智能力 [[41](#bib.bib41), [51](#bib.bib51)]。例如，该模型详细回答了以下问题：
- en: 'User: James and Abby are in the bedroom. Abby put the pen in the desk drawer.
    Abby leaves the bedroom. James moves the pen into the bag. Where does James think
    Abby will look for the pen?'
  id: totrans-271
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '用户: James 和 Abby 在卧室里。Abby 把笔放在了桌子抽屉里。Abby 离开了卧室。James 把笔放进了包里。James 认为 Abby
    会在哪儿找笔？'
- en: ''
  id: totrans-272
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Guanaco: James knows that Abby put the pen in the desk drawer. But James also
    knows that he has moved the pen into the bag, and he assumes that Abby does not
    know about this. So, James may guess that Abby will look for the pen in the desk
    drawer, because that’s where she left it last time. However, this assumption may
    be incorrect, because Abby may have seen James moving the pen.'
  id: totrans-273
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Guanaco: James 知道 Abby 把笔放在了桌子抽屉里。但 James 也知道他把笔放进了包里，并且他假设 Abby 不知道这一点。所以，James
    可能会猜测 Abby 会在桌子抽屉里找笔，因为那是她上次留下的地方。然而，这个假设可能是错误的，因为 Abby 可能看到 James 移动了笔。'
- en: However, these inferences are unreliable, and often the model gives reasons
    that presume things that don’t make sense in the situation, e.g.,
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些推断是不可靠的，而且模型经常给出一些在情境中没有意义的理由，例如，
- en: 'User: Evelyn entered the living room. Jackson entered the playroom. James entered
    the playroom. The beans are in the treasure chest. James exited the playroom.
    Jackson moved the beans to the pantry. Jackson exited the playroom. James entered
    the living room. Where will James look for the beans?'
  id: totrans-275
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '用户: Evelyn 进入了客厅。Jackson 进入了游戏室。James 进入了游戏室。豆子在宝箱里。James 离开了游戏室。Jackson 把豆子移到储藏室。Jackson
    离开了游戏室。James 进入了客厅。James 会在哪里找豆子？'
- en: ''
  id: totrans-276
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Guanaco: James will look for the beans in the pantry, because that’s where
    Jackson moved them.'
  id: totrans-277
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Guanaco: James将在储藏室里找豆子，因为Jackson把它们搬到了那里。'
- en: where Guanaco presumes information transfer that was never described. These
    issues echo recent literature [[51](#bib.bib51)], but require more study.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Guanaco假定了从未描述的信息传递。这些问题呼应了最近的文献[[51](#bib.bib51)]，但需要进一步研究。
- en: 6.2 Considerations
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 考虑因素
- en: Evaluation
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估
- en: We report moderate agreement among human annotators (Fleiss $\kappa=0.42$) with
    additional deterioration when comparing two strong systems. This points to limitations
    in the current benchmarks and human evaluation protocols for chatbot task performance.
    When manually comparing generations from ChatGPT and Guanaco 65B on the Vicuna
    benchmark, we find that subjective preferences start to play an important role
    as the authors of this paper disagreed on the many preferred responses. Future
    work should investigate approaches to mitigate these problems drawing from disciplines
    that developed mechanisms to deal with subjective preferences, such as Human-Computer
    Interaction and Psychology.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告了人类标注者之间的中等一致性（Fleiss $\kappa=0.42$），在比较两个强系统时，一致性有所下降。这指出了当前基准和人类评估协议在聊天机器人任务性能中的局限性。当在Vicuna基准上手动比较ChatGPT和Guanaco
    65B的生成结果时，我们发现主观偏好开始发挥重要作用，因为本文的作者在许多优选响应上意见不一。未来的工作应探讨借鉴人机交互和心理学等领域中的机制，以应对主观偏好问题的解决方案。
- en: 'In our analysis, we also find that automated evaluation systems have noticeable
    biases. For example, we observe strong order effects with GPT-4 assigning higher
    scores to the system appearing first in its prompt. The relatively weak sample-level
    agreement between GPT-4 and human annotators (Fleiss $\kappa=0.25$) also suggests
    that human annotators and automated systems might rely on preferences that are
    not always aligned. In addition, in Table [7](#S5.T7 "Table 7 ‣ 5.3 Guanaco: QLoRA
    trained on OASST1 is a State-of-the-art Chatbot ‣ 5 Pushing the Chatbot State-of-the-art
    with QLoRA ‣ QLoRA: Efficient Finetuning of Quantized LLMs"), we observe that
    GPT-4 assigns significantly higher scores to its own outputs compared to human
    ratings, Elo of 1348 vs 1176, which represent an additional  20% probability of
    winning against an opponent. Future work should examine the presence of potential
    biases in automated evaluation systems as well as possible mitigation strategies.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的分析中，我们还发现自动评估系统存在明显的偏见。例如，我们观察到GPT-4对出现在提示中的第一个系统赋予了更高的分数，表现出强烈的顺序效应。GPT-4与人工标注者之间相对较弱的样本级别一致性（Fleiss
    $\kappa=0.25$）也表明人工标注者和自动系统可能依赖于不总是对齐的偏好。此外，在表[7](#S5.T7 "Table 7 ‣ 5.3 Guanaco:
    QLoRA trained on OASST1 is a State-of-the-art Chatbot ‣ 5 Pushing the Chatbot
    State-of-the-art with QLoRA ‣ QLoRA: Efficient Finetuning of Quantized LLMs")中，我们观察到GPT-4对其自身输出的评分显著高于人工评分，Elo为1348对1176，代表了额外的20%赢得对手的概率。未来的工作应当检查自动评估系统中潜在的偏见以及可能的缓解策略。'
- en: Data & Training
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据与训练
- en: We note that the OASST1 dataset on which Guanaco models are trained is multilingual
    and that the OA benchmark also contains prompts in different languages. We leave
    it to future work to investigate the degree to which such multilingual training
    improves performance on instructions in languages other than English and whether
    this explains the larger gap between Vicuna-13B model (only trained on English
    data) and Guanaco 33B and 65B on the OA benchmark.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到Guanaco模型训练的OASST1数据集是多语言的，并且OA基准也包含了不同语言的提示。我们留待未来工作来调查这种多语言训练在英语以外的语言指令上的性能提升程度，以及是否解释了Vicuna-13B模型（仅训练于英语数据）与Guanaco
    33B和65B在OA基准上的较大差距。
- en: Given the strong performance of Guanaco models, we investigate any data leakage
    between the OASST1 data and the Vicuna benchmark prompts. We do not find overlapping
    prompts after performing fuzzy string matching in the two datasets and inspecting
    the closest matches manually.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于Guanaco模型的强劲表现，我们调查了OASST1数据与Vicuna基准提示之间是否存在数据泄漏。在对两个数据集进行模糊字符串匹配并手动检查最接近的匹配项后，我们没有发现提示的重叠。
- en: Furthermore, we note that our model is only trained with cross-entropy loss
    (supervised learning) without relying on reinforcement learning from human feedback
    (RLHF). This calls for further investigations of the tradeoffs of simple cross-entropy
    loss and RLHF training. We hope that QLoRA enables such analysis at scale, without
    the need for overwhelming computational resources.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们注意到我们的模型仅使用交叉熵损失（监督学习）进行训练，而未依赖于来自人类反馈的强化学习（RLHF）。这需要进一步研究简单交叉熵损失和RLHF训练的权衡。我们希望QLoRA能够在大规模上实现这种分析，而无需过多的计算资源。
- en: 7 Related Work
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: Quantization of Large Language Models
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大型语言模型的量化
- en: Quantization of LLMs has largely focused on quantization for inference time.
    Major approaches for preserving 16-bit LLM quality focus on managing outlier features
    (e.g., SmoothQuant [[66](#bib.bib66)] and LLM.int8() [[14](#bib.bib14)]) while
    others use more sophisticated grouping methods [[44](#bib.bib44), [69](#bib.bib69)].
    Lossy quantization approaches study the trade-offs for regular rounding [[13](#bib.bib13),
    [71](#bib.bib71), [47](#bib.bib47)] or how to optimize rounding decisions to improve
    quantization precision [[18](#bib.bib18)]. Besides our work, SwitchBack layers [[65](#bib.bib65)]
    is the only work that studies backpropagation through quantized weights at a scale
    beyond 1B parameters.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的量化主要集中在推理时间的量化。保持16位LLM质量的主要方法集中在管理异常特征（例如，SmoothQuant [[66](#bib.bib66)]
    和 LLM.int8() [[14](#bib.bib14)]），而其他方法则使用更复杂的分组方法 [[44](#bib.bib44), [69](#bib.bib69)]。有损量化方法研究了常规舍入的权衡 [[13](#bib.bib13),
    [71](#bib.bib71), [47](#bib.bib47)]，或者如何优化舍入决策以提高量化精度 [[18](#bib.bib18)]。除了我们的工作，SwitchBack
    layers [[65](#bib.bib65)] 是唯一研究超过1B参数量化权重的反向传播的工作。
- en: Finetuning with Adapters
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用适配器进行微调
- en: While we use Low-rank Adapters [[28](#bib.bib28)] (LoRA), many other Parameter
    Efficient FineTuning (PEFT) methods have been proposed such as prompt tuning [[48](#bib.bib48),
    [33](#bib.bib33), [34](#bib.bib34)], tuning the embedding layer inputs [[1](#bib.bib1)],
    tuning hidden states (IA³) [[37](#bib.bib37)], adding full layers [[27](#bib.bib27)],
    tuning biases [[70](#bib.bib70)], learning a mask over weights based on Fisher
    information [[54](#bib.bib54)], and a combination of approaches [[23](#bib.bib23)].
    In our work, we show that LoRA adapters are able to reach full 16-bit finetuning
    performance. We leave it to future work to explore the tradeoffs of other PEFT
    approaches.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们使用低秩适配器 [[28](#bib.bib28)]（LoRA），但已经提出了许多其他参数高效微调（PEFT）方法，如提示微调 [[48](#bib.bib48),
    [33](#bib.bib33), [34](#bib.bib34)]，微调嵌入层输入 [[1](#bib.bib1)]，微调隐藏状态 （IA³）[[37](#bib.bib37)]，添加完整层 [[27](#bib.bib27)]，微调偏差 [[70](#bib.bib70)]，基于Fisher信息学习权重掩码 [[54](#bib.bib54)]，以及各种方法的组合 [[23](#bib.bib23)]。在我们的工作中，我们展示了LoRA适配器能够达到完整的16位微调性能。我们将探讨其他PEFT方法的权衡留待未来工作。
- en: Instruction Finetuning
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指令微调
- en: To help a pretrained LLM follow the instructions provided in a prompt, instruction
    finetuning uses input-output pairs of various data sources to finetune a pretrained
    LLM to generate the output given the input as a prompt. Approaches and datasets
    include MetaICL [[40](#bib.bib40)], MetaTuning [[73](#bib.bib73)], InstructGPT [[43](#bib.bib43)],
    FLAN [[62](#bib.bib62), [12](#bib.bib12)], PromptSource [[3](#bib.bib3)], Super-NaturalInstructions [[61](#bib.bib61),
    [50](#bib.bib50)], Self-instruct [[59](#bib.bib59)], UnnaturalInstructions [[26](#bib.bib26)],
    OPT-IML [[29](#bib.bib29)], UnifiedSKG[[67](#bib.bib67)], OIG/Chip2 [[32](#bib.bib32)],
    Alpaca [[55](#bib.bib55)], Vicuna [[10](#bib.bib10)], Koala [[20](#bib.bib20)],
    and Self-instruct-GPT-4 [[45](#bib.bib45)].
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助预训练的LLM按照提示中的指令执行，指令微调使用各种数据源的输入-输出对来微调预训练的LLM，以生成给定输入作为提示的输出。方法和数据集包括MetaICL [[40](#bib.bib40)],
    MetaTuning [[73](#bib.bib73)], InstructGPT [[43](#bib.bib43)], FLAN [[62](#bib.bib62),
    [12](#bib.bib12)], PromptSource [[3](#bib.bib3)], Super-NaturalInstructions [[61](#bib.bib61),
    [50](#bib.bib50)], Self-instruct [[59](#bib.bib59)], UnnaturalInstructions [[26](#bib.bib26)],
    OPT-IML [[29](#bib.bib29)], UnifiedSKG[[67](#bib.bib67)], OIG/Chip2 [[32](#bib.bib32)],
    Alpaca [[55](#bib.bib55)], Vicuna [[10](#bib.bib10)], Koala [[20](#bib.bib20)]
    和 Self-instruct-GPT-4 [[45](#bib.bib45)]。
- en: Chatbots
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 聊天机器人
- en: Many instruction following models are structured as dialogue-based chatbots,
    often using Reinforcement Learning from Human Feedback (RLHF) [[11](#bib.bib11)]
    or generating data from an existing model to train with AI model feedback (RLAIF) [[5](#bib.bib5)].
    Approaches and datasets include Anthropic-HH [[2](#bib.bib2), [4](#bib.bib4)],
    Open Assistant [[31](#bib.bib31)], LaMDA [[56](#bib.bib56)], and Sparrow [[21](#bib.bib21)].
    We do not use reinforcement learning, but our best model, Guanaco, is finetuned
    on multi-turn chat interactions from the Open Assistant dataset which was designed
    to be used for RLHF training [[31](#bib.bib31)]. For the evaluation of chatbots
    approaches that use GPT-4 instead of costly human annotation have been developed
    [[10](#bib.bib10), [45](#bib.bib45)]. We improve on such approaches with a focus
    on an evaluation setup that is more reliable.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 许多遵循指令的模型结构为基于对话的聊天机器人，通常使用来自人类反馈的强化学习（RLHF）[[11](#bib.bib11)]或生成数据从现有模型进行 AI
    模型反馈的训练（RLAIF）[[5](#bib.bib5)]。方法和数据集包括 Anthropic-HH [[2](#bib.bib2), [4](#bib.bib4)]，Open
    Assistant [[31](#bib.bib31)]，LaMDA [[56](#bib.bib56)] 和 Sparrow [[21](#bib.bib21)]。我们不使用强化学习，但我们最好的模型
    Guanaco 在 Open Assistant 数据集上的多轮聊天交互上进行微调，该数据集设计用于 RLHF 训练 [[31](#bib.bib31)]。对于使用
    GPT-4 代替昂贵的人类注释的聊天机器人评估方法已经被开发 [[10](#bib.bib10), [45](#bib.bib45)]。我们在此类方法上有所改进，重点是更可靠的评估设置。
- en: 8 Limitations and Discussion
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 限制与讨论
- en: We have shown evidence that our method, QLoRA, can replicate 16-bit full finetuning
    performance with a 4-bit base model and Low-rank Adapters (LoRA). Despite this
    evidence, we did not establish that QLoRA can match full 16-bit finetuning performance
    at 33B and 65B scales. Due to the immense resource costs, we leave this study
    to future work.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经证明了我们的方法 QLoRA 可以用 4 位基础模型和低秩适配器（LoRA）复制 16 位完全微调性能。尽管有这些证据，我们未能证明 QLoRA
    可以在 33B 和 65B 规模上匹配完全 16 位微调性能。由于资源成本巨大，我们将这项研究留待未来工作。
- en: Another limitation is the evaluation of instruction finetuning models. While
    we provide evaluations on MMLU, the Vicuna benchmark, and the OA benchmark, we
    did not evaluate on other benchmarks such as BigBench, RAFT, and HELM, and it
    is not ensured that our evaluations generalize to these benchmarks. On the other
    hand, we perform a very broad study on MMLU and develop new methods for evaluating
    chatbots.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个限制是对指令微调模型的评估。虽然我们提供了 MMLU、Vicuna 基准和 OA 基准的评估，但我们没有在 BigBench、RAFT 和 HELM
    等其他基准上进行评估，且不能保证我们的评估结果可以推广到这些基准。另一方面，我们在 MMLU 上进行了一项非常广泛的研究，并开发了评估聊天机器人的新方法。
- en: From the evidence presented, it appears that the performance of these benchmarks
    likely depends how similar the finetuning data is to the benchmark dataset. For
    example, FLAN v2 is similar to MMLU, but dissimilar to chatbot benchmarks and
    vice versa for the Chip2 dataset and both models score accordingly on the MMLU
    and Vicuna benchmarks. This highlights that not only better benchmarks and evaluation
    is needed, but that one needs to be careful about what one is evaluating in the
    first place. Do we want to create models that do well on classroom highschool
    and colleague knowledge or do we want to do well on chatbot conversation ability?
    Maybe something else? Because it is always easier to evaluate on an existing benchmark
    compared to creating a new one, certain benchmarks can steer the community towards
    a certain direction. We should ensure as a community that the benchmarks measure
    what we care about.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 从所提供的证据来看，这些基准的表现似乎依赖于微调数据与基准数据集的相似程度。例如，FLAN v2 与 MMLU 相似，但与聊天机器人基准不相似，Chip2
    数据集也是如此，两者在 MMLU 和 Vicuna 基准上得分也相应地不同。这突显了不仅需要更好的基准和评估，还需要在一开始就小心自己评估的内容。我们是想创建在课堂高中和大学知识上表现良好的模型，还是想在聊天机器人对话能力上表现良好？也许是其他方面？因为在现有基准上评估总是比创建新基准容易，某些基准可能会引导社区朝着某个方向发展。我们应该作为一个社区确保基准测量我们关心的内容。
- en: 'Table 8: Evaluation of biases on the CrowS dataset. A lower score indicates
    lower likelihood of generating biased sequences. Guanaco follows the biased pattern
    of the LLaMA base model.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：在 CrowS 数据集上的偏见评估。较低的分数表示生成偏见序列的可能性较低。Guanaco 遵循 LLaMA 基础模型的偏见模式。
- en: '|  | LLaMA-65B | GPT-3 | OPT-175B | Guanaco-65B |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA-65B | GPT-3 | OPT-175B | Guanaco-65B |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Gender | 70.6 | 62.6 | 65.7 | 47.5 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 性别 | 70.6 | 62.6 | 65.7 | 47.5 |'
- en: '| Religion | 79.0 | 73.3 | 68.6 | 38.7 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 宗教 | 79.0 | 73.3 | 68.6 | 38.7 |'
- en: '| Race/Color | 57.0 | 64.7 | 68.6 | 45.3 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 种族/肤色 | 57.0 | 64.7 | 68.6 | 45.3 |'
- en: '| Sexual orientation | 81.0 | 76.2 | 78.6 | 59.1 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 性取向 | 81.0 | 76.2 | 78.6 | 59.1 |'
- en: '| Age | 70.1 | 64.4 | 67.8 | 36.3 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 年龄 | 70.1 | 64.4 | 67.8 | 36.3 |'
- en: '| Nationality | 64.2 | 61.6 | 62.9 | 32.4 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 国籍 | 64.2 | 61.6 | 62.9 | 32.4 |'
- en: '| Disability | 66.7 | 76.7 | 76.7 | 33.9 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 残疾 | 66.7 | 76.7 | 76.7 | 33.9 |'
- en: '| Physical appearance | 77.8 | 74.6 | 76.2 | 43.1 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 外貌 | 77.8 | 74.6 | 76.2 | 43.1 |'
- en: '| Socioeconomic status | 71.5 | 73.8 | 76.2 | 55.3 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 社会经济地位 | 71.5 | 73.8 | 76.2 | 55.3 |'
- en: '| Average | 66.6 | 67.2 | 69.5 | 43.5 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 66.6 | 67.2 | 69.5 | 43.5 |'
- en: 'While we provide a detailed evaluation for general chatbot performance, another
    limitation is that we only do a limited responsible AI evaluation of Guanaco.
    We evaluate the likelihood of Guanaco-65B to generate a socially biased sequence
    of tokens compared to other models in Table [8](#S8.T8 "Table 8 ‣ 8 Limitations
    and Discussion ‣ QLoRA: Efficient Finetuning of Quantized LLMs"). We see that
    the average score in Guanaco-65B is much lower than other raw pretrained models.
    As such, it seems that finetuning on the OASST1 dataset reduces the bias of the
    LLaMA base model. While these results are encouraging, it is unclear if Guanaco
    does also well when assessed on other types of biases. We leave further evaluation
    of analyzing biases in Guanaco and similar chatbots to future work.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们提供了针对通用聊天机器人的详细评估，但另一个限制是我们仅对Guanaco进行了有限的负责任AI评估。我们评估了Guanaco-65B生成社会偏见序列的可能性，与表[8](#S8.T8
    "表 8 ‣ 8 限制和讨论 ‣ QLoRA：量化LLMs的高效微调")中的其他模型相比。我们发现Guanaco-65B的平均分数远低于其他原始预训练模型。因此，似乎在OASST1数据集上的微调减少了LLaMA基础模型的偏见。虽然这些结果令人鼓舞，但尚不清楚Guanaco在评估其他类型偏见时表现如何。我们将对Guanaco及类似聊天机器人的偏见进一步分析留待未来工作。
- en: An additional limitation is that we did not evaluate different bit-precisions,
    such as using 3-bit base models, or different adapter methods. Besides LoRA, there
    is also a wide variety Parameter Efficient FineTuning (PEFT) methods that have
    been shown to work well. However, it is unclear if these methods scale to large
    models. We used LoRA as many results established its robustness but other adapters
    might yield better performance. Since finetuning after quantization seems to recover
    most of the information that is lost during quantization this might enable much
    more aggressive quantization. For example, 3-bit GPTQ quantization of the basemodel
    with LoRA might also yield 16-bit full finetuning performance after finetuning.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个限制是我们没有评估不同的位精度，如使用3位基础模型，或不同的适配器方法。除了LoRA，还有许多已证明有效的参数高效微调（PEFT）方法。然而，尚不清楚这些方法是否适用于大型模型。我们使用LoRA是因为许多结果证明了它的稳健性，但其他适配器可能会带来更好的性能。由于量化后的微调似乎恢复了在量化过程中丢失的大部分信息，这可能使得更具攻击性的量化成为可能。例如，使用LoRA进行的3位GPTQ量化的基础模型可能在微调后也能达到16位全微调性能。
- en: 9 Broader Impacts
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 更广泛的影响
- en: Our QLoRA finetuning method is the first method that enables the finetuning
    of 33B parameter models on a single consumer GPU and 65B parameter models on a
    single professional GPU, while not degrading performance relative to a full finetuning
    baseline. We have demonstrated that our best 33B model trained on the Open Assistant
    dataset can rival ChatGPT on the Vicuna benchmark. Since instruction finetuning
    is an essential tool to transform raw pretrained LLMs into ChatGPT-like chatbots,
    we believe that our method will make finetuning widespread and common in particular
    for the researchers that have the least resources, a big win for the accessibility
    of state of the art NLP technology. QLoRA can be seen as an equalizing factor
    that helps to close the resource gap between large corporations and small teams
    with consumer GPUs.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的QLoRA微调方法是第一个使得在单个消费者GPU上对33B参数模型进行微调，并在单个专业GPU上对65B参数模型进行微调，而不会降低相对于完整微调基准的性能的方法。我们已经证明，我们在Open
    Assistant数据集上训练的最佳33B模型可以在Vicuna基准上与ChatGPT相媲美。由于指令微调是将原始预训练LLMs转变为类似ChatGPT的聊天机器人的重要工具，我们相信我们的方法将使微调变得普遍和常见，特别是对于那些资源最少的研究人员，这对前沿NLP技术的可及性是一个巨大的胜利。QLoRA可以视为一个平衡因素，有助于缩小大型公司和拥有消费者GPU的小团队之间的资源差距。
- en: Another potential source of impact is deployment to mobile phones. We believe
    our QLoRA method might enable the critical milestone of enabling the finetuning
    of LLMs on phones and other low resource settings. While 7B models were shown
    to be able to be run on phones before, QLoRA is the first method that would enable
    the finetuning of such models. We estimate that with an iPhone 12 Plus, QLoRA
    can finetune 3 million tokens per night while the phone is charging. While finetuned
    7B models do not reach the quality of ChatGPT, we believe that the quality is
    good enough to enable novel applications that have not been possible before due
    to privacy or LLM quality issues. QLoRA can help enable privacy-preserving usage
    of LLMs, where users can own and manage their own data and models, while simultaneously
    making LLMs easier to deploy.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个潜在的影响来源是向手机的部署。我们相信我们的 QLoRA 方法可能会实现一个关键里程碑，即在手机和其他资源有限的环境中实现 LLM 的微调。虽然之前已经证明
    7B 模型可以在手机上运行，但 QLoRA 是第一个能够实现这些模型微调的方法。我们估计，在 iPhone 12 Plus 上，QLoRA 可以在充电期间每晚微调
    300 万个标记。虽然微调后的 7B 模型达不到 ChatGPT 的质量，但我们相信，这种质量足以实现以前由于隐私或 LLM 质量问题而不可能的新应用。QLoRA
    可以帮助实现 LLM 的隐私保护使用，让用户能够拥有和管理自己的数据和模型，同时使 LLM 更容易部署。
- en: However, finetuning is a dual-use technology that can be abused to cause harm.
    Widespread use of LLMs has known dangers [[8](#bib.bib8), [6](#bib.bib6)], but
    we believe that equalizing access to a technology that is quickly becoming ubiquitous
    will allow for better more independent analysis than keeping the power of LLMs
    in the hands of large corporations that do not release models or source code for
    auditing.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，微调是一种双用途技术，可能会被滥用造成伤害。大规模使用 LLM 存在已知的风险 [[8](#bib.bib8), [6](#bib.bib6)]，但我们认为，让这项迅速普及的技术平等获取，将比将
    LLM 的权力掌握在不发布模型或源代码以供审计的大公司手中，更能进行独立的分析。
- en: All in all, we believe that QLoRA will have a broadly positive impact making
    the finetuning of high quality LLMs much more widely and easily accessible.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们相信 QLoRA 将会产生广泛的积极影响，使高质量 LLM 的微调变得更加广泛和容易获取。
- en: Acknowledgements
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier,
    Zihao Ye, and Evangelia Spiliopoulou for their valuable feedback. Our research
    was facilitated by the advanced computational, storage, and networking infrastructure
    of the Hyak supercomputer system at the University of Washington. We thank the
    Hyak team for ensuring a smooth operation. We thank the beta testers of the bitsandbytes
    library, in particular Alex Birch and Alyssa Vance. We thank Younes Belkada for
    help with the integration of our software into the Hugging Face transformers stack.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Aditya Kusupati、Ofir Press、Ashish Sharma、Margaret Li、Raphael Olivier、Zihao
    Ye 和 Evangelia Spiliopoulou 给予的宝贵反馈。我们的研究得到了华盛顿大学 Hyak 超级计算机系统的先进计算、存储和网络基础设施的支持。我们感谢
    Hyak 团队确保系统的顺利运行。我们感谢 bitsandbytes 库的测试人员，特别是 Alex Birch 和 Alyssa Vance。我们感谢 Younes
    Belkada 帮助将我们的软件整合到 Hugging Face transformers 堆栈中。
- en: References
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'An et al. [2022] S. An, Y. Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng,
    and J.-G. Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models.
    *arXiv preprint arXiv:2203.03131*, 2022.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: An 等 [2022] S. An, Y. Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng,
    和 J.-G. Lou。输入调优：将不熟悉的输入适配到冻结的预训练模型中。*arXiv 预印本 arXiv:2203.03131*，2022。
- en: Askell et al. [2021] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan,
    A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. A general language assistant
    as a laboratory for alignment. *arXiv preprint arXiv:2112.00861*, 2021.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Askell 等 [2021] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan,
    A. Jones, N. Joseph, B. Mann, N. DasSarma 等。将通用语言助手作为对齐的实验室。*arXiv 预印本 arXiv:2112.00861*，2021。
- en: 'Bach et al. [2022] S. H. Bach, V. Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V.
    Nayak, A. Sharma, T. Kim, M. S. Bari, T. Fevry, et al. Promptsource: An integrated
    development environment and repository for natural language prompts. *arXiv preprint
    arXiv:2202.01279*, 2022.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bach 等 [2022] S. H. Bach, V. Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V. Nayak,
    A. Sharma, T. Kim, M. S. Bari, T. Fevry 等。Promptsource：一个集成的自然语言提示开发环境和库。*arXiv
    预印本 arXiv:2202.01279*，2022。
- en: Bai et al. [2022a] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,
    D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training a helpful and harmless
    assistant with reinforcement learning from human feedback. *arXiv preprint arXiv:2204.05862*,
    2022a.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等 [2022a] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,
    D. Drain, S. Fort, D. Ganguli, T. Henighan 等。使用来自人类反馈的强化学习训练有用且无害的助手。*arXiv 预印本
    arXiv:2204.05862*，2022a。
- en: 'Bai et al. [2022b] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones,
    A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional ai: Harmlessness
    from ai feedback. *arXiv preprint arXiv:2212.08073*, 2022b.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 [2022b] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones,
    A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon 等人. 宪法人工智能：来自 AI 反馈的无害性。*arXiv
    预印本 arXiv:2212.08073*，2022b 年。
- en: 'Bender et al. [2021] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell.
    On the dangers of stochastic parrots: Can language models be too big? In *Proceedings
    of the 2021 ACM conference on fairness, accountability, and transparency*, pages
    610–623, 2021.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bender 等人 [2021] E. M. Bender, T. Gebru, A. McMillan-Major, 和 S. Shmitchell.
    关于随机鹦鹉的危险：语言模型会不会过大？在 *2021 年 ACM 公平性、问责制和透明度会议论文集*，第 610–623 页，2021 年。
- en: 'Biderman et al. [2023] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley,
    K. O’Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al.
    Pythia: A suite for analyzing large language models across training and scaling.
    *arXiv preprint arXiv:2304.01373*, 2023.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biderman 等人 [2023] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O’Brien,
    E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff 等人. Pythia：一个分析大规模语言模型的套件。*arXiv
    预印本 arXiv:2304.01373*，2023 年。
- en: Bommasani et al. [2021] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora,
    S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the
    opportunities and risks of foundation models. *arXiv preprint arXiv:2108.07258*,
    2021.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bommasani 等人 [2021] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora,
    S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill 等人. 关于基础模型的机会和风险。*arXiv
    预印本 arXiv:2108.07258*，2021 年。
- en: Chen et al. [2016] T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep
    nets with sublinear memory cost. *arXiv preprint arXiv:1604.06174*, 2016.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2016] T. Chen, B. Xu, C. Zhang, 和 C. Guestrin. 以亚线性内存成本训练深度网络。*arXiv
    预印本 arXiv:1604.06174*，2016 年。
- en: 'Chiang et al. [2023] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang,
    L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna:
    An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.
    URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang 等人 [2023] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L.
    Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, 和 E. P. Xing. Vicuna：一个开源聊天机器人，以
    90%* ChatGPT 质量打动 GPT-4，2023 年 3 月。网址 [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)。
- en: Christiano et al. [2017] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg,
    and D. Amodei. Deep reinforcement learning from human preferences. *Advances in
    neural information processing systems*, 30, 2017.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano 等人 [2017] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg,
    和 D. Amodei. 从人类偏好中进行深度强化学习。*神经信息处理系统进展*，30，2017 年。
- en: Chung et al. [2022] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus,
    E. Li, X. Wang, M. Dehghani, S. Brahma, et al. Scaling instruction-finetuned language
    models. *arXiv preprint arXiv:2210.11416*, 2022.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等人 [2022] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus,
    E. Li, X. Wang, M. Dehghani, S. Brahma 等人. 扩展指令微调的语言模型。*arXiv 预印本 arXiv:2210.11416*，2022
    年。
- en: 'Dettmers and Zettlemoyer [2022] T. Dettmers and L. Zettlemoyer. The case for
    4-bit precision: k-bit inference scaling laws. *arXiv preprint arXiv:2212.09720*,
    2022.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 和 Zettlemoyer [2022] T. Dettmers 和 L. Zettlemoyer. 4 位精度的理由：k 位推理缩放定律。*arXiv
    预印本 arXiv:2212.09720*，2022 年。
- en: 'Dettmers et al. [2022a] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer.
    LLM.int8(): 8-bit matrix multiplication for transformers at scale. *Advances in
    Neural Information Processing Systems 35: Annual Conference on Neural Information
    Processing Systems 2022, NeurIPS 2022*, 2022a.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等人 [2022a] T. Dettmers, M. Lewis, Y. Belkada, 和 L. Zettlemoyer. LLM.int8()：大规模变换器的
    8 位矩阵乘法。*神经信息处理系统进展 35：神经信息处理系统年会 2022，NeurIPS 2022*，2022a 年。
- en: Dettmers et al. [2022b] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer.
    8-bit optimizers via block-wise quantization. *9th International Conference on
    Learning Representations, ICLR*, 2022b.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等人 [2022b] T. Dettmers, M. Lewis, S. Shleifer, 和 L. Zettlemoyer. 通过块级量化的
    8 位优化器。*第 9 届国际学习表征会议，ICLR*，2022b 年。
- en: Elo [1967] A. E. Elo. The proposed uscf rating system. its development, theory,
    and applications. *Chess Life*, 22(8):242–247, 1967.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elo [1967] A. E. Elo. 提议的 USCF 评级系统。其发展、理论和应用。*Chess Life*，22(8)：242–247，1967
    年。
- en: Elo [1978] A. E. Elo. *The rating of chessplayers, past and present*. Arco Pub.,
    1978.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elo [1978] A. E. Elo. *棋手的评级，过去和现在*。Arco 出版社，1978 年。
- en: 'Frantar et al. [2022] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh.
    Gptq: Accurate post-training quantization for generative pre-trained transformers.
    *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等人 [2022] E. Frantar, S. Ashkboos, T. Hoefler, 和 D. Alistarh. Gptq:
    生成预训练变换器的精确后训练量化。*arXiv 预印本 arXiv:2210.17323*，2022年。'
- en: 'Fu et al. [2023] J. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate
    as you desire. *arXiv preprint arXiv:2302.04166*, 2023.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu 等人 [2023] J. Fu, S.-K. Ng, Z. Jiang, 和 P. Liu. Gptscore: 按需评估。*arXiv 预印本
    arXiv:2302.04166*，2023年。'
- en: 'Geng et al. [2023] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,
    and D. Song. Koala: A dialogue model for academic research. Blog post, April 2023.
    URL [https://bair.berkeley.edu/blog/2023/04/03/koala/](https://bair.berkeley.edu/blog/2023/04/03/koala/).'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Geng 等人 [2023] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,
    和 D. Song. Koala: 一个用于学术研究的对话模型。博客文章，2023年4月。网址 [https://bair.berkeley.edu/blog/2023/04/03/koala/](https://bair.berkeley.edu/blog/2023/04/03/koala/)。'
- en: Glaese et al. [2022] A. Glaese, N. McAleese, M. Trębacz, J. Aslanides, V. Firoiu,
    T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al. Improving alignment
    of dialogue agents via targeted human judgements. *arXiv preprint arXiv:2209.14375*,
    2022.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glaese 等人 [2022] A. Glaese, N. McAleese, M. Trębacz, J. Aslanides, V. Firoiu,
    T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, 等人. 通过目标人类判断改进对话代理的对齐。*arXiv
    预印本 arXiv:2209.14375*，2022年。
- en: Gururangan et al. [2018] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz,
    S. R. Bowman, and N. A. Smith. Annotation artifacts in natural language inference
    data. *arXiv preprint arXiv:1803.02324*, 2018.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gururangan 等人 [2018] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S.
    R. Bowman, 和 N. A. Smith. 自然语言推理数据中的注释伪影。*arXiv 预印本 arXiv:1803.02324*，2018年。
- en: 'Henderson et al. [2021] J. Henderson, S. Ruder, et al. Compacter: Efficient
    low-rank hypercomplex adapter layers. In *Advances in Neural Information Processing
    Systems*, 2021.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Henderson 等人 [2021] J. Henderson, S. Ruder, 等人. Compacter: 高效低秩超复数适配层。发表于*神经信息处理系统进展*，2021年。'
- en: Hendrycks et al. [2020] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika,
    D. Song, and J. Steinhardt. Measuring massive multitask language understanding.
    In *International Conference on Learning Representations*, 2020.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人 [2020] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D.
    Song, 和 J. Steinhardt. 测量大规模多任务语言理解。发表于*国际学习表征会议*，2020年。
- en: Holtzman et al. [2020] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi.
    The curious case of neural text degeneration. In *International Conference on
    Learning Representations*, 2020.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holtzman 等人 [2020] A. Holtzman, J. Buys, L. Du, M. Forbes, 和 Y. Choi. 神经文本退化的奇怪案例。发表于*国际学习表征会议*，2020年。
- en: 'Honovich et al. [2022] O. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural
    instructions: Tuning language models with (almost) no human labor. *arXiv preprint
    arXiv:2212.09689*, 2022.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Honovich 等人 [2022] O. Honovich, T. Scialom, O. Levy, 和 T. Schick. 非自然指令: 用（几乎）无人工劳动调整语言模型。*arXiv
    预印本 arXiv:2212.09689*，2022年。'
- en: Houlsby et al. [2019] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,
    A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning
    for nlp. In *International Conference on Machine Learning*, pages 2790–2799\.
    PMLR, 2019.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby 等人 [2019] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De
    Laroussilhe, A. Gesmundo, M. Attariyan, 和 S. Gelly. NLP的参数高效迁移学习。发表于*国际机器学习会议*，第2790–2799页。PMLR，2019年。
- en: 'Hu et al. [2021] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang,
    L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. *arXiv
    preprint arXiv:2106.09685*, 2021.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等人 [2021] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L.
    Wang, 和 W. Chen. Lora: 大型语言模型的低秩适配。*arXiv 预印本 arXiv:2106.09685*，2021年。'
- en: 'Iyer et al. [2022] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig,
    P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, et al. Opt-iml: Scaling language
    model instruction meta learning through the lens of generalization. *arXiv preprint
    arXiv:2212.12017*, 2022.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Iyer 等人 [2022] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu,
    K. Shuster, T. Wang, Q. Liu, P. S. Koura, 等人. Opt-iml: 从泛化角度扩展语言模型指令元学习。*arXiv
    预印本 arXiv:2212.12017*，2022年。'
- en: 'Köksal et al. [2023] A. Köksal, T. Schick, A. Korhonen, and H. Schütze. Longform:
    Optimizing instruction tuning for long text generation with corpus extraction.
    *arXiv preprint arXiv:2304.08460*, 2023.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Köksal 等人 [2023] A. Köksal, T. Schick, A. Korhonen, 和 H. Schütze. Longform:
    优化长文本生成的指令调整与语料库提取。*arXiv 预印本 arXiv:2304.08460*，2023年。'
- en: Köpf et al. [2023] A. Köpf, Y. Kilcher, D. von Rütte, S. Anagnostidis, Z.-R.
    Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi, et al. Openassistant
    conversations–democratizing large language model alignment. *arXiv preprint arXiv:2304.07327*,
    2023.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Köpf 等 [2023] A. Köpf, Y. Kilcher, D. von Rütte, S. Anagnostidis, Z.-R. Tam,
    K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi 等. Openassistant 对话–民主化大型语言模型对齐。*arXiv
    预印本 arXiv:2304.07327*, 2023。
- en: LAION [2023] LAION. Open-instruction-generalist dataset. [https://github.com/LAION-AI/Open-Instruction-Generalist](https://github.com/LAION-AI/Open-Instruction-Generalist),
    2023.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LAION [2023] LAION. 开放指令通用数据集。 [https://github.com/LAION-AI/Open-Instruction-Generalist](https://github.com/LAION-AI/Open-Instruction-Generalist)，2023。
- en: Lester et al. [2021] B. Lester, R. Al-Rfou, and N. Constant. The power of scale
    for parameter-efficient prompt tuning. *arXiv preprint arXiv:2104.08691*, 2021.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester 等 [2021] B. Lester, R. Al-Rfou 和 N. Constant. 参数高效提示调优的规模效应。*arXiv 预印本
    arXiv:2104.08691*, 2021。
- en: 'Li and Liang [2021] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous
    prompts for generation. *arXiv preprint arXiv:2101.00190*, 2021.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 和 Liang [2021] X. L. Li 和 P. Liang. Prefix-tuning: 优化连续提示生成。*arXiv 预印本 arXiv:2101.00190*,
    2021。'
- en: Liang et al. [2022] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga,
    Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al. Holistic evaluation of language
    models. *arXiv preprint arXiv:2211.09110*, 2022.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等 [2022] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga,
    Y. Zhang, D. Narayanan, Y. Wu, A. Kumar 等. 语言模型的整体评估。*arXiv 预印本 arXiv:2211.09110*,
    2022。
- en: Liao et al. [2021] T. Liao, R. Taori, I. D. Raji, and L. Schmidt. Are we learning
    yet? a meta review of evaluation failures across machine learning. In *Thirty-fifth
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track
    (Round 2)*, 2021.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao 等 [2021] T. Liao, R. Taori, I. D. Raji 和 L. Schmidt. 我们学会了吗？机器学习评估失败的元审查。在
    *第三十五届神经信息处理系统大会数据集和基准跟踪（第二轮）*，2021年。
- en: Liu et al. [2022] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal,
    and C. A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper
    than in-context learning. *Advances in Neural Information Processing Systems*,
    35:1950–1965, 2022.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2022] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal 和 C.
    A. Raffel. 少量参数高效微调比上下文学习更好且更便宜。*神经信息处理系统进展*，35:1950–1965, 2022。
- en: 'Liu et al. [2019] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,
    M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert
    pretraining approach. *arXiv preprint arXiv:1907.11692*, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 [2019] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M.
    Lewis, L. Zettlemoyer 和 V. Stoyanov. Roberta: 一种强健优化的 BERT 预训练方法。*arXiv 预印本 arXiv:1907.11692*,
    2019。'
- en: 'Longpre et al. [2023] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay,
    D. Zhou, Q. V. Le, B. Zoph, J. Wei, et al. The flan collection: Designing data
    and methods for effective instruction tuning. *arXiv preprint arXiv:2301.13688*,
    2023.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Longpre 等 [2023] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay,
    D. Zhou, Q. V. Le, B. Zoph, J. Wei 等. FLAN 集合：设计有效指令调优的数据和方法。*arXiv 预印本 arXiv:2301.13688*,
    2023。
- en: 'Min et al. [2021] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl:
    Learning to learn in context. *arXiv preprint arXiv:2110.15943*, 2021.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Min 等 [2021] S. Min, M. Lewis, L. Zettlemoyer 和 H. Hajishirzi. Metaicl: 在上下文中学习学习。*arXiv
    预印本 arXiv:2110.15943*, 2021。'
- en: Nematzadeh et al. [2018] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths.
    Evaluating theory of mind in question answering. In *Proceedings of the 2018 Conference
    on Empirical Methods in Natural Language Processing*, pages 2392–2400, 2018.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nematzadeh 等 [2018] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik 和 T. Griffiths.
    在问答中评估理论思维。在 *2018年自然语言处理经验方法大会论文集*，第2392–2400页，2018年。
- en: OpenAI [2023] OpenAI. Gpt-4 technical report. *arXiv*, 2023.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI. GPT-4 技术报告。*arXiv*，2023。
- en: Ouyang et al. [2022] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright,
    P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models
    to follow instructions with human feedback. *Advances in Neural Information Processing
    Systems*, 35:27730–27744, 2022.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等 [2022] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,
    C. Zhang, S. Agarwal, K. Slama, A. Ray 等. 训练语言模型以遵循人类反馈的指令。*神经信息处理系统进展*，35:27730–27744,
    2022。
- en: 'Park et al. [2022] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee.
    nuqmm: Quantized matmul for efficient inference of large-scale generative language
    models. *arXiv preprint arXiv:2206.09557*, 2022.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park 等 [2022] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee 和 D. Lee. nuqmm:
    量化矩阵乘法以提高大规模生成语言模型的推理效率。*arXiv 预印本 arXiv:2206.09557*, 2022。'
- en: Peng et al. [2023] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction
    tuning with gpt-4. *arXiv preprint arXiv:2304.03277*, 2023.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人 [2023] B. Peng, C. Li, P. He, M. Galley 和 J. Gao。使用 GPT-4 进行指令调整。*arXiv
    预印本 arXiv:2304.03277*，2023。
- en: Poliak et al. [2018] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme.
    Hypothesis only baselines in natural language inference. In *Proceedings of the
    Seventh Joint Conference on Lexical and Computational Semantics*, pages 180–191,
    2018.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poliak 等人 [2018] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger 和 B. Van Durme。仅基于假设的自然语言推理基线。在
    *第七届词汇与计算语义联合会议论文集*，第 180–191 页，2018。
- en: Pope et al. [2022] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury,
    A. Levskaya, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer
    inference. *arXiv preprint arXiv:2211.05102*, 2022.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pope 等人 [2022] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A.
    Levskaya, J. Heek, K. Xiao, S. Agrawal 和 J. Dean。高效扩展 Transformer 推理。*arXiv 预印本
    arXiv:2211.05102*，2022。
- en: 'Qin and Eisner [2021] G. Qin and J. Eisner. Learning how to ask: Querying lms
    with mixtures of soft prompts. *arXiv preprint arXiv:2104.06599*, 2021.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 和 Eisner [2021] G. Qin 和 J. Eisner。学习如何提问：用软提示的混合查询语言模型。*arXiv 预印本 arXiv:2104.06599*，2021。
- en: Raffel et al. [2020] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
    Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with
    a unified text-to-text transformer. *J. Mach. Learn. Res.*, 21(1), jan 2020. ISSN
    1532-4435.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 [2020] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
    Y. Zhou, W. Li 和 P. J. Liu。探索统一文本到文本 Transformer 的迁移学习极限。*J. Mach. Learn. Res.*，21(1)，2020年1月。ISSN
    1532-4435。
- en: Sanh et al. [2021] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,
    A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al. Multitask prompted training
    enables zero-shot task generalization. *arXiv preprint arXiv:2110.08207*, 2021.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等人 [2021] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,
    A. Chaffin, A. Stiegler, T. L. Scao, A. Raja 等人。多任务提示训练实现零样本任务泛化。*arXiv 预印本 arXiv:2110.08207*，2021。
- en: Sap et al. [2022] M. Sap, R. LeBras, D. Fried, and Y. Choi. Neural theory-of-mind?
    on the limits of social intelligence in large lms. *arXiv preprint arXiv:2210.13312*,
    2022.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sap 等人 [2022] M. Sap, R. LeBras, D. Fried 和 Y. Choi。神经理论思维？关于大型语言模型社交智能的限制。*arXiv
    预印本 arXiv:2210.13312*，2022。
- en: 'Scao et al. [2022] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow,
    R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al. Bloom: A 176b-parameter
    open-access multilingual language model. *arXiv preprint arXiv:2211.05100*, 2022.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Scao 等人 [2022] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow,
    R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé 等人。Bloom: 一个 176b-参数的开放访问多语言模型。*arXiv
    预印本 arXiv:2211.05100*，2022。'
- en: Shaphiro and Wilk [1965] S. Shaphiro and M. Wilk. An analysis of variance test
    for normality. *Biometrika*, 52(3):591–611, 1965.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaphiro 和 Wilk [1965] S. Shaphiro 和 M. Wilk。正态性方差分析测试。*Biometrika*，52(3):591–611，1965。
- en: Sung et al. [2021] Y.-L. Sung, V. Nair, and C. A. Raffel. Training neural networks
    with fixed sparse masks. *Advances in Neural Information Processing Systems*,
    34:24193–24205, 2021.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sung 等人 [2021] Y.-L. Sung, V. Nair 和 C. A. Raffel。用固定稀疏掩码训练神经网络。*神经信息处理系统进展*，34:24193–24205，2021。
- en: 'Taori et al. [2023] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin,
    P. Liang, and T. B. Hashimoto. Stanford alpaca: An instruction-following llama
    model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori 等人 [2023] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin,
    P. Liang 和 T. B. Hashimoto。Stanford Alpaca: 一个指令跟随的 Llama 模型。[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)，2023。'
- en: 'Thoppilan et al. [2022] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha,
    H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. Lamda: Language models for
    dialog applications. *arXiv preprint arXiv:2201.08239*, 2022.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thoppilan 等人 [2022] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha,
    H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du 等人。Lamda: 对话应用的语言模型。*arXiv 预印本 arXiv:2201.08239*，2022。'
- en: 'Touvron et al. [2023] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.
    Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama:
    Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 [2023] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
    T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar 等人。Llama: 开放且高效的基础语言模型。*arXiv
    预印本 arXiv:2302.13971*，2023。'
- en: 'Wang et al. [2018] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R.
    Bowman. Glue: A multi-task benchmark and analysis platform for natural language
    understanding. *arXiv preprint arXiv:1804.07461*, 2018.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 [2018] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy 和 S. R. Bowman。GLUE:
    一个用于自然语言理解的多任务基准和分析平台。*arXiv 预印本 arXiv:1804.07461*，2018。'
- en: 'Wang et al. [2022a] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi,
    and H. Hajishirzi. Self-instruct: Aligning language model with self generated
    instructions. *arXiv preprint arXiv:2212.10560*, 2022a.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人 [2022a] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi,
    和 H. Hajishirzi。Self-instruct：将语言模型与自生成的指令对齐。*arXiv预印本 arXiv:2212.10560*，2022年。
- en: Wang et al. [2022b] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,
    A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al. Super-naturalinstructions:generalization
    via declarative instructions on 1600+ tasks. In *EMNLP*, 2022b.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人 [2022b] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,
    A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap 等人。Super-naturalinstructions：通过声明性指令在1600+任务上进行泛化。见于*《EMNLP》*，2022年。
- en: 'Wang et al. [2022c] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,
    A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al. Super-naturalinstructions:
    Generalization via declarative instructions on 1600+ nlp tasks. In *Proceedings
    of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages
    5085–5109, 2022c.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人 [2022c] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,
    A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap 等人。Super-naturalinstructions：通过声明性指令在1600+
    NLP任务上进行泛化。见于*《2022年自然语言处理实证方法会议论文集》*，第5085–5109页，2022年。
- en: Wei et al. [2021] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester,
    N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners.
    *arXiv preprint arXiv:2109.01652*, 2021.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人 [2021] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,
    A. M. Dai, 和 Q. V. Le。微调语言模型是零样本学习者。*arXiv预印本 arXiv:2109.01652*，2021年。
- en: Wei et al. [2022] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi,
    Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large
    language models. In *Advances in Neural Information Processing Systems*, 2022.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人 [2022] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q.
    V. Le, D. Zhou 等人。链式思维提示在大型语言模型中引发推理。见于*《神经信息处理系统进展》*，2022年。
- en: 'Wolf et al. [2019] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi,
    P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface’s transformers:
    State-of-the-art natural language processing. *arXiv preprint arXiv:1910.03771*,
    2019.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf等人 [2019] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi,
    P. Cistac, T. Rault, R. Louf, M. Funtowicz 等人。Huggingface的transformers：最先进的自然语言处理。*arXiv预印本
    arXiv:1910.03771*，2019年。
- en: Wortsman et al. [2023] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos,
    A. Farhadi, and L. Schmidt. Stable and low-precision training for large-scale
    vision-language models. *arXiv preprint arXiv:2304.13013*, 2023.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wortsman等人 [2023] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi,
    和 L. Schmidt。大型视觉-语言模型的稳定和低精度训练。*arXiv预印本 arXiv:2304.13013*，2023年。
- en: 'Xiao et al. [2022] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han. Smoothquant:
    Accurate and efficient post-training quantization for large language models. *arXiv
    preprint arXiv:2211.10438*, 2022.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao等人 [2022] G. Xiao, J. Lin, M. Seznec, J. Demouth, 和 S. Han。Smoothquant：针对大型语言模型的准确高效后训练量化。*arXiv预印本
    arXiv:2211.10438*，2022年。
- en: 'Xie et al. [2022] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga,
    C.-S. Wu, M. Zhong, P. Yin, S. I. Wang, et al. Unifiedskg: Unifying and multi-tasking
    structured knowledge grounding with text-to-text language models. *arXiv preprint
    arXiv:2201.05966*, 2022.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等人 [2022] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S.
    Wu, M. Zhong, P. Yin, S. I. Wang 等人。Unifiedskg：用文本到文本语言模型统一和多任务处理结构化知识。*arXiv预印本
    arXiv:2201.05966*，2022年。
- en: 'Yang et al. [2018] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov,
    and C. D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question
    answering. In *Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing*, pages 2369–2380, 2018.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等人 [2018] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov,
    和 C. D. Manning。Hotpotqa：一个用于多跳问答的多样化、可解释的数据集。见于*《2018年自然语言处理实证方法会议论文集》*，第2369–2380页，2018年。
- en: 'Yao et al. [2022] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He.
    Zeroquant: Efficient and affordable post-training quantization for large-scale
    transformers. *arXiv preprint arXiv:2206.01861*, 2022.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao等人 [2022] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, 和 Y. He。Zeroquant：高效且经济的大规模变换器后训练量化。*arXiv预印本
    arXiv:2206.01861*，2022年。
- en: 'Zaken et al. [2021] E. B. Zaken, S. Ravfogel, and Y. Goldberg. Bitfit: Simple
    parameter-efficient fine-tuning for transformer-based masked language-models.
    *arXiv preprint arXiv:2106.10199*, 2021.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zaken等人 [2021] E. B. Zaken, S. Ravfogel, 和 Y. Goldberg。Bitfit：对基于变换器的掩蔽语言模型进行简单参数高效的微调。*arXiv预印本
    arXiv:2106.10199*，2021年。
- en: 'Zeng et al. [2022] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang,
    Y. Xu, W. Zheng, X. Xia, et al. Glm-130b: An open bilingual pre-trained model.
    *arXiv preprint arXiv:2210.02414*, 2022.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng 等人 [2022] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y.
    Xu, W. Zheng, X. Xia 等人。Glm-130b: 一个开放的双语预训练模型。*arXiv 预印本 arXiv:2210.02414*，2022年。'
- en: 'Zhang et al. [2022] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,
    C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer
    language models. *arXiv preprint arXiv:2205.01068*, 2022.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2022] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,
    C. Dewan, M. Diab, X. Li, X. V. Lin 等人。Opt: 开放预训练变换器语言模型。*arXiv 预印本 arXiv:2205.01068*，2022年。'
- en: Zhong et al. [2021] R. Zhong, K. Lee, Z. Zhang, and D. Klein. Adapting language
    models for zero-shot learning by meta-tuning on dataset and prompt collections.
    *arXiv preprint arXiv:2104.04670*, 2021.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong 等人 [2021] R. Zhong, K. Lee, Z. Zhang 和 D. Klein。通过在数据集和提示集合上进行元调优来适应零-shot
    学习的语言模型。*arXiv 预印本 arXiv:2104.04670*，2021年。
- en: Appendix A QLoRA vs Standard Finetuning Experimental Setup Details
  id: totrans-396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A QLoRA 与标准微调实验设置详情
- en: A.1 Hyperparameters for QLoRA
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 QLoRA 的超参数
- en: 'We do a hyperparameter search for LoRA over the following variables: LoRA dropout
    { 0.0, 0.05, 0.1}, LoRA $r$ is always proportional to the learning rate.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 LoRA 进行超参数搜索，涉及以下变量：LoRA dropout { 0.0, 0.05, 0.1}，LoRA $r$ 始终与学习率成比例。
- en: 'We find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not
    for larger models (33B, 65B). We find LoRA $r$ is unrelated to final performance
    if LoRA is used on all layers as can be seen in Figure [4](#A1.F4 "Figure 4 ‣
    A.1 Hyperparameters for QLoRA ‣ Appendix A QLoRA vs Standard Finetuning Experimental
    Setup Details ‣ QLoRA: Efficient Finetuning of Quantized LLMs")'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '我们发现 LoRA dropout 0.05 对小型模型（7B, 13B）有效，但对大型模型（33B, 65B）无效。我们发现 LoRA $r$ 与最终性能无关，前提是
    LoRA 应用于所有层，如图 [4](#A1.F4 "Figure 4 ‣ A.1 Hyperparameters for QLoRA ‣ Appendix
    A QLoRA vs Standard Finetuning Experimental Setup Details ‣ QLoRA: Efficient Finetuning
    of Quantized LLMs") 所示。'
- en: '![Refer to caption](img/cfe063652725bf6da0c80e53825cc525.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cfe063652725bf6da0c80e53825cc525.png)'
- en: 'Figure 4: LoRA $r$ values appears to be independent of other hyperparameters.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：LoRA $r$ 值似乎与其他超参数无关。
- en: A.2 Super-Natural Instructions Experimental Setup Details
  id: totrans-402
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 Super-Natural Instructions 实验设置详情
- en: We use the same preprocessing of the Super-Natural Instruction dataset as Wang
    et al. [[60](#bib.bib60)]. However, we split the training data in training and
    validation datasets allowing us to perform more rigorous hyperparameter tuning
    and early stopping. We use the same hyperparameters described in the paper for
    training the various T5 model sizes on the Super-Natural Instruction data. We
    use LoRA $r=16$ in all our experiments and no LoRA dropout.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用与 Wang 等人 [[60](#bib.bib60)] 相同的 Super-Natural Instruction 数据集预处理。然而，我们将训练数据分为训练和验证数据集，使我们能够进行更严格的超参数调整和提前停止。我们使用文中描述的相同超参数来训练各种
    T5 模型在 Super-Natural Instruction 数据上。我们在所有实验中使用 LoRA $r=16$，并且不使用 LoRA dropout。
- en: Appendix B Training a State-of-the-art Chatbot Experimental Setup Details
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 训练最先进的聊天机器人实验设置详情
- en: B.1 Datasets
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 数据集
- en: 'We describe the datasets used for QLoRA finetuning experiments outlined in
    Section [5](#S5 "5 Pushing the Chatbot State-of-the-art with QLoRA ‣ QLoRA: Efficient
    Finetuning of Quantized LLMs").'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '我们描述了第 [5](#S5 "5 Pushing the Chatbot State-of-the-art with QLoRA ‣ QLoRA:
    Efficient Finetuning of Quantized LLMs") 节中概述的 QLoRA 微调实验所使用的数据集。'
- en: OASST1
  id: totrans-407
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: OASST1
- en: The OpenAssistant dataset [[31](#bib.bib31)] was collected via crowd-sourcing.
    It contains 161,443 unique messages distributed across 66,497 conversations and
    spanning 35 different languages. The dataset often contains several ranked replies
    for each given user question. In our experiments, we only use the top reply at
    each level in the conversation tree. This limits the dataset to 9,209 examples.
    We finetuning our models on the full conversation including the user queries.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAssistant 数据集 [[31](#bib.bib31)] 是通过众包收集的。它包含 161,443 条独特的消息，分布在 66,497
    个对话中，涵盖 35 种不同语言。该数据集通常包含每个用户问题的多个排名回复。在我们的实验中，我们仅使用对话树中每个级别的顶部回复。这将数据集限制为 9,209
    个示例。我们在包括用户查询的完整对话上对模型进行微调。
- en: HH-RLHF
  id: totrans-409
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: HH-RLHF
- en: This is a human preference dataset about helpfulness and harmlessness. Each
    datapoint consists of two assistant replies to a user question along with a human
    preference judgment of the best reply. The dataset contains 160,800 examples.
    When finetuning on this dataset, we combine helpfulness and harmlessness data
    and only keep the preferred assistant reply.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关于有用性和无害性的人类偏好数据集。每个数据点由两个助手对用户问题的回复及一个人类对最佳回复的偏好判断组成。数据集包含 160,800 个示例。在对该数据集进行微调时，我们结合了有用性和无害性数据，并只保留了首选的助手回复。
- en: FLAN v2
  id: totrans-411
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: FLAN v2
- en: The FLAN v2 collection [[39](#bib.bib39)] is a collection of 1836 tasks augmented
    with hundreds of manually curated templates and rich formatting patterns into
    over 15M examples. The authors show that models trained on this collection outperform
    other public collections including the original FLAN 2021 [[62](#bib.bib62)],
    T0++ [[50](#bib.bib50)], Super-Natural Instructions [[60](#bib.bib60)], and OPT-IML
    [[29](#bib.bib29)]. We used the same task mixtures described by the authors with
    the exception of some datasets that were not freely available at the time of writing.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: FLAN v2 集合 [[39](#bib.bib39)] 是一个包含 1836 个任务的集合，经过数百个手动策划的模板和丰富的格式化模式扩展到超过 1500
    万个示例。作者展示了在该集合上训练的模型优于其他公共集合，包括原始的 FLAN 2021 [[62](#bib.bib62)]、T0++ [[50](#bib.bib50)]、Super-Natural
    Instructions [[60](#bib.bib60)] 和 OPT-IML [[29](#bib.bib29)]。我们使用了作者描述的相同任务混合，除了在撰写时一些数据集不可用。
- en: Self-Instruct, Alpaca, Unnatural Instructions
  id: totrans-413
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Self-Instruct、Alpaca、Unnatural Instructions
- en: The Self-Instruct, Alpaca, and Unnatural Instructions datasets [[59](#bib.bib59),
    [55](#bib.bib55), [26](#bib.bib26)] are instruction tuning datasets collected
    with various approaches of model distillation from GPT-3 Instruct and ChatGPT.
    They rely on prompting, in-context learning, and paraphrasing to come up with
    diverse sets of instructions and outputs. The datasets comprise of 82,612, 51,942,
    and 240,670 examples respectively. One advantage of such distilled datasets is
    that they contain a more diverse set of instruction styles compared to the FLAN
    v2 collection and similar instruction tuning collections.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: Self-Instruct、Alpaca 和 Unnatural Instructions 数据集 [[59](#bib.bib59)、[55](#bib.bib55)、[26](#bib.bib26)]
    是通过从 GPT-3 Instruct 和 ChatGPT 进行模型蒸馏的各种方法收集的指令调整数据集。它们依赖于提示、上下文学习和改写来生成各种指令和输出。这些数据集分别包含
    82,612、51,942 和 240,670 个示例。这样的蒸馏数据集的一个优势是，它们包含比 FLAN v2 集合和类似的指令调整集合更为多样化的指令风格。
- en: Longform
  id: totrans-415
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Longform
- en: The LongForm dataset [[30](#bib.bib30)] is based on an English corpus augmented
    with instructions and as such is a hybrid human-generated dataset. The underlying
    documents are human-written and come from C4 and Wikipedia while the instructions
    are generated visa LLMs. The dataset is extended with additional structured corpora
    examples such as Stack Exchange and WikiHow and task examples such as question
    answering, email writing, grammar error correction, story/poem generation, and
    text summarization. The dataset contains 23,700 examples.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: LongForm 数据集 [[30](#bib.bib30)] 基于一个扩展了指令的英文语料库，因此是一个混合的人类生成数据集。基础文档是人工编写的，来自
    C4 和维基百科，而指令则由 LLMs 生成。数据集扩展了额外的结构化语料库示例，如 Stack Exchange 和 WikiHow，以及任务示例，如问题回答、邮件写作、语法错误修正、故事/诗歌生成和文本摘要。数据集包含
    23,700 个示例。
- en: Chip2
  id: totrans-417
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Chip2
- en: is part of the OIG Laion dataset. It contains Python code examples, natural
    instruction examples, generic harmless instructions, instruction/responses with
    lists, follow-up questions, Wikipedia toxic adversarial questions, grade school
    math, reasoning instructions, and character and scene descriptions with a total
    of 210,289 examples.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 是 OIG Laion 数据集的一部分。它包含 Python 代码示例、自然指令示例、通用无害指令、包含列表的指令/响应、后续问题、维基百科有毒对抗性问题、小学数学、推理指令，以及角色和场景描述，共有
    210,289 个示例。
- en: B.2 Hyperparameters
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 超参数
- en: 'We provide the exact hyperparameters used in our QLoRA finetuning experiments.
    We find hyperparameters to be largely robust across datasets. We use the MMLU
    5-shot dev set for validation and hyperparameter tuning. In all our experiments
    we use NF4 with double quantization and bf16 computation datatype. We set LoRA
    $r=64$, and add LoRA modules on all linear layers of the base model. We also use
    Adam beta2 of 0.999, max grad norm of 0.3 and LoRA dropout of 0.1 for models up
    to 13B and 0.05 for 33B and 65B models. Following previous work on instruction
    finetuning [[62](#bib.bib62), [60](#bib.bib60)] and after benchmarking other linear
    and cosine schedules, we use a constant learning rate schedule. We use group-by-length
    to group examples of similar lengths in the same batch (note this will produce
    a oscillating loss curve). The hyperparameters we tune for each model size are
    shown in Table [9](#A2.T9 "Table 9 ‣ B.2 Hyperparameters ‣ Appendix B Training
    a State-of-the-art Chatbot Experimental Setup Details ‣ QLoRA: Efficient Finetuning
    of Quantized LLMs").'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了在QLoRA微调实验中使用的确切超参数。我们发现这些超参数在不同数据集上大体稳定。我们使用MMLU 5-shot开发集进行验证和超参数调整。在所有实验中，我们使用了双量化的NF4和bf16计算数据类型。我们将LoRA
    $r=64$，并在基础模型的所有线性层上添加LoRA模块。我们还为最多13B的模型使用Adam beta2为0.999，最大梯度范数为0.3，以及LoRA
    dropout为0.1，33B和65B模型为0.05。继之前的指令微调工作[[62](#bib.bib62)， [60](#bib.bib60)]之后，并在对其他线性和余弦调度进行基准测试后，我们使用了常数学习率调度。我们使用按长度分组将相似长度的示例分组到同一批次中（注意这会产生一个波动的损失曲线）。我们为每个模型大小调整的超参数显示在表[9](#A2.T9
    "表 9 ‣ B.2 超参数 ‣ 附录 B 训练最先进的聊天机器人实验设置细节 ‣ QLoRA：高效微调量化LLMs")中。
- en: '| Parameters | Dataset | Batch size | LR | Steps | Source Length | Target Length
    |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 数据集 | 批量大小 | 学习率 | 步骤 | 源长度 | 目标长度 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 7B | All | 16 | 2e-4 | 10000 | 384 | 128 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 7B | All | 16 | 2e-4 | 10000 | 384 | 128 |'
- en: '| 7B | OASST1 | 16 | 2e-4 | 1875 | - | 512 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 7B | OASST1 | 16 | 2e-4 | 1875 | - | 512 |'
- en: '| 7B | HH-RLHF | 16 | 2e-4 | 10000 | - | 768 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 7B | HH-RLHF | 16 | 2e-4 | 10000 | - | 768 |'
- en: '| 7B | Longform | 16 | 2e-4 | 4000 | 512 | 1024 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 7B | Longform | 16 | 2e-4 | 4000 | 512 | 1024 |'
- en: '| 13B | All | 16 | 2e-4 | 10000 | 384 | 128 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 13B | All | 16 | 2e-4 | 10000 | 384 | 128 |'
- en: '| 13B | OASST1 | 16 | 2e-4 | 1875 | - | 512 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 13B | OASST1 | 16 | 2e-4 | 1875 | - | 512 |'
- en: '| 13B | HH-RLHF | 16 | 2e-4 | 10000 | - | 768 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 13B | HH-RLHF | 16 | 2e-4 | 10000 | - | 768 |'
- en: '| 13B | Longform | 16 | 2e-4 | 4000 | 512 | 1024 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| 13B | Longform | 16 | 2e-4 | 4000 | 512 | 1024 |'
- en: '| 33B | All | 32 | 1e-4 | 5000 | 384 | 128 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| 33B | All | 32 | 1e-4 | 5000 | 384 | 128 |'
- en: '| 33B | OASST1 | 16 | 1e-4 | 1875 | - | 512 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 33B | OASST1 | 16 | 1e-4 | 1875 | - | 512 |'
- en: '| 33B | HH-RLHF | 32 | 1e-4 | 5000 | - | 768 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| 33B | HH-RLHF | 32 | 1e-4 | 5000 | - | 768 |'
- en: '| 33B | Longform | 32 | 1e-4 | 2343 | 512 | 1024 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| 33B | Longform | 32 | 1e-4 | 2343 | 512 | 1024 |'
- en: '| 65B | All | 64 | 1e-4 | 2500 | 384 | 128 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| 65B | All | 64 | 1e-4 | 2500 | 384 | 128 |'
- en: '| 65B | OASST1 | 16 | 1e-4 | 1875 | - | 512 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 65B | OASST1 | 16 | 1e-4 | 1875 | - | 512 |'
- en: '| 65B | HH-RLHF | 64 | 1e-4 | 2500 | - | 768 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 65B | HH-RLHF | 64 | 1e-4 | 2500 | - | 768 |'
- en: '| 65B | Longform | 32 | 1e-4 | 2343 | 512 | 1024 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 65B | Longform | 32 | 1e-4 | 2343 | 512 | 1024 |'
- en: 'Table 9: Training hyperparameters for QLoRA finetuning on different datasets
    and across model sizes.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：在不同数据集和模型大小下进行QLoRA微调的训练超参数。
- en: B.3 Ablations
  id: totrans-440
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 消融实验
- en: 'While it is general practice in the literature to only train on the response
    in instruction following datasets, we study the effect of training on the instruction
    in addition to the response in Table [10](#A2.T10 "Table 10 ‣ B.3 Ablations ‣
    Appendix B Training a State-of-the-art Chatbot Experimental Setup Details ‣ QLoRA:
    Efficient Finetuning of Quantized LLMs"). In these experiments, we restrict the
    training data to 52,000 examples and use the 7B model. Over four different instruction
    tuning datasets, we find that only training on the target is beneficial to MMLU
    performance. We did not evaluate the effect this may have on chatabot performance
    as measured by vicuna or OA benchmarks.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在文献中一般只训练指令跟随数据集中的响应，我们在表[10](#A2.T10 "表 10 ‣ B.3 消融实验 ‣ 附录 B 训练最先进的聊天机器人实验设置细节
    ‣ QLoRA：高效微调量化LLMs")中研究了除了响应外训练指令的效果。在这些实验中，我们将训练数据限制为52,000个样本，并使用7B模型。在四种不同的指令调优数据集上，我们发现只训练目标对MMLU性能有利。我们没有评估这可能对聊天机器人性能的影响，如vicuna或OA基准所测量的那样。
- en: '| Dataset | Unnatural Instructions | Chip2 | Alpaca | FLAN v2 | Mean |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 不自然的指令 | Chip2 | Alpaca | FLAN v2 | 平均值 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Train on source and target | 36.2 | 33.7 | 38.1 | 42.0 | 37.5 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 在源和目标上训练 | 36.2 | 33.7 | 38.1 | 42.0 | 37.5 |'
- en: '| Train on target | 38.0 | 34.5 | 39.0 | 42.9 | 38.6 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 仅在目标上训练 | 38.0 | 34.5 | 39.0 | 42.9 | 38.6 |'
- en: 'Table 10: MMLU 5-shot test results studying the effect of training on the instructions
    in addition to the response.'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：MMLU 5-shot 测试结果，研究在响应之外的指令上训练的效果。
- en: 'B.4 What is more important: instruction finetuning dataset size or dataset
    quality?'
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4 什么更重要：指令微调数据集大小还是数据集质量？
- en: Data set suitability is more important than dataset size.
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集的适用性比数据集大小更重要。
- en: 'To understand the effects of dataset quality vs. dataset size, we experiment
    with subsampling large datasets with at least 150,000 samples (Chip2, FLAN v2,
    Unnatural Instructions), into datasets of size 50,000, 100,000 and 150,000 and
    examine the resulting trends, as shown in Table [11](#A2.T11 "Table 11 ‣ Data
    set suitability is more important than dataset size. ‣ B.4 What is more important:
    instruction finetuning dataset size or dataset quality? ‣ Appendix B Training
    a State-of-the-art Chatbot Experimental Setup Details ‣ QLoRA: Efficient Finetuning
    of Quantized LLMs"). We find that increasing the dataset size and increasing the
    number of epochs improves MMLU only marginally (0.0 - 0.5 MMLU), while the difference
    between datasets is up to 40x larger (1.5 - 8.0 MMLU). This is a clear indicator
    that dataset quality rather than dataset size is critical for mean MMLU accuracy.
    We obtain similar findings for chatbot performance as discussed in .'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解数据集质量与数据集大小的影响，我们对至少包含 150,000 个样本的大型数据集（Chip2、FLAN v2、不自然指令）进行子采样，生成 50,000、100,000
    和 150,000 大小的数据集，并检查结果趋势，如表 [11](#A2.T11 "表 11 ‣ 数据集的适用性比数据集大小更重要。 ‣ B.4 什么更重要：指令微调数据集大小还是数据集质量？
    ‣ 附录 B 先进聊天机器人训练实验设置详细信息 ‣ QLoRA：量化 LLM 的高效微调") 所示。我们发现，增加数据集大小和周期数对 MMLU 只有边际改进（0.0
    - 0.5 MMLU），而数据集之间的差异最大达到 40 倍（1.5 - 8.0 MMLU）。这明显表明，数据集质量而非数据集大小对于平均 MMLU 准确度至关重要。对于聊天机器人性能，我们得出了类似的发现，如讨论中所述。
- en: 'Table 11: Effect different dataset sizes and finetuning epochs on mean 5-shot
    MMLU test set accuracy. While increasing the dataset size and training for more
    than 1 epochs helps with MMLU performance, the difference between datasets are
    far larger, indicating that dataset quality affects MMLU performance more than
    dataset size.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：不同数据集大小和微调周期对平均 5-shot MMLU 测试集准确性的影响。虽然增加数据集大小和训练超过 1 个周期对 MMLU 性能有所帮助，但数据集之间的差异要大得多，这表明数据集质量对
    MMLU 性能的影响大于数据集大小。
- en: '|  | Chip | Unnatural Instructions | FLAN v2 |  |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '|  | 芯片 | 不自然指令 | FLAN v2 |  |'
- en: '| Datapoints $\downarrow$ | 1 | 2 | 3 | 1 | 2 | 3 | 1 | 2 | 3 | Mean |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 数据点 $\downarrow$ | 1 | 2 | 3 | 1 | 2 | 3 | 1 | 2 | 3 | 平均值 |'
- en: '| 50000 | 34.50 | 35.30 | 34.70 | 38.10 | 42.20 | 38.10 | 43.00 | 43.50 | 44.10
    | 39.28 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 50000 | 34.50 | 35.30 | 34.70 | 38.10 | 42.20 | 38.10 | 43.00 | 43.50 | 44.10
    | 39.28 |'
- en: '| 100000 | 33.70 | 33.90 | 34.00 | 40.10 | 41.20 | 37.00 | 43.90 | 43.70 |
    44.90 | 39.16 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 100000 | 33.70 | 33.90 | 34.00 | 40.10 | 41.20 | 37.00 | 43.90 | 43.70 |
    44.90 | 39.16 |'
- en: '| 150000 | 34.40 | 34.80 | 35.10 | 39.70 | 41.10 | 41.50 | 44.60 | 45.50 |
    43.50 | 40.02 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 150000 | 34.40 | 34.80 | 35.10 | 39.70 | 41.10 | 41.50 | 44.60 | 45.50 |
    43.50 | 40.02 |'
- en: '| Mean | 34.20 | 34.67 | 34.60 | 39.30 | 41.50 | 38.87 | 43.83 | 44.23 | 44.17
    |  |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 34.20 | 34.67 | 34.60 | 39.30 | 41.50 | 38.87 | 43.83 | 44.23 | 44.17
    |  |'
- en: Appendix C Human Evaluation
  id: totrans-457
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 人类评估
- en: 'We conduct a human evaluation with the same wording given to GPT-4 in the original
    Vicuna evaluation [[10](#bib.bib10)], adjusted for an Amazon Mechanical Turk form
    as show in Figure [5](#A3.F5 "Figure 5 ‣ Appendix C Human Evaluation ‣ QLoRA:
    Efficient Finetuning of Quantized LLMs").'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一项人类评估，使用与原始 Vicuna 评估中 GPT-4 提供的相同措辞 [[10](#bib.bib10)]，调整为 Amazon Mechanical
    Turk 表单，如图 [5](#A3.F5 "图 5 ‣ 附录 C 人类评估 ‣ QLoRA：量化 LLM 的高效微调") 所示。
- en: '![Refer to caption](img/a00cc5f5827960db5ea48ffb832f87e9.png)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a00cc5f5827960db5ea48ffb832f87e9.png)'
- en: 'Figure 5: The crowdsourcing form used by human annotators.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：人类注释者使用的众包表单。
- en: Appendix D Pairwise Evaluation with GPT-4
  id: totrans-461
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 使用 GPT-4 的配对评估
- en: 'While we found that the GPT-4 evaluation gave different results depend on which
    system was presented first, when averaged over both options the pairwise results
    were well-ordered. The aggregated pairwise judgments are hown in Table [12](#A4.T12
    "Table 12 ‣ Appendix D Pairwise Evaluation with GPT-4 ‣ QLoRA: Efficient Finetuning
    of Quantized LLMs"). On inspection, it is clear these judgments are transitive,
    i.e., when System A is judged better than System B and System B is judged better
    than System C, it is always the case that System A is judged better than System
    C. This yields a complete ordering, given in Table [13](#A4.T13 "Table 13 ‣ Appendix
    D Pairwise Evaluation with GPT-4 ‣ QLoRA: Efficient Finetuning of Quantized LLMs").'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们发现 GPT-4 的评估结果取决于哪个系统先被展示，但当对两种选项进行平均时，配对结果的排序是良好的。汇总的配对判断如表 [12](#A4.T12
    "表 12 ‣ 附录 D 使用 GPT-4 进行配对评估 ‣ QLoRA：量化 LLM 的高效微调") 所示。经过检查，可以清楚地看到这些判断是传递的，即当系统
    A 被判断为优于系统 B，且系统 B 被判断为优于系统 C 时，总是系统 A 被判断为优于系统 C。这产生了一个完整的排序，见表 [13](#A4.T13
    "表 13 ‣ 附录 D 使用 GPT-4 进行配对评估 ‣ QLoRA：量化 LLM 的高效微调")。
- en: 'Table 12: Aggregated pairwise GPT-4 judgments between systems where the value
    of a cell at row $x$'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：系统间的汇总配对 GPT-4 判断，其中第 $x$ 行的单元格值
- en: '| Model | Guanaco 65B | Guanaco 33B | Vicuna | ChatGPT-3.5 Turbo | Bard | Guanaco
    13B | Guanaco 7B |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Guanaco 65B | Guanaco 33B | Vicuna | ChatGPT-3.5 Turbo | Bard | Guanaco
    13B | Guanaco 7B |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Guanaco 65B | - | 0.21 | 0.19 | 0.16 | 0.72 | 0.59 | 0.86 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco 65B | - | 0.21 | 0.19 | 0.16 | 0.72 | 0.59 | 0.86 |'
- en: '| Guanaco 33B | -0.21 | - | 0.17 | 0.10 | 0.51 | 0.41 | 0.68 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco 33B | -0.21 | - | 0.17 | 0.10 | 0.51 | 0.41 | 0.68 |'
- en: '| Vicuna | -0.19 | -0.17 | - | 0.10 | 0.50 | 0.20 | 0.57 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna | -0.19 | -0.17 | - | 0.10 | 0.50 | 0.20 | 0.57 |'
- en: '| ChatGPT-3.5 Turbo | -0.16 | -0.10 | -0.10 | - | 0.35 | 0.19 | 0.40 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-3.5 Turbo | -0.16 | -0.10 | -0.10 | - | 0.35 | 0.19 | 0.40 |'
- en: '| Bard | -0.72 | -0.51 | -0.50 | -0.35 | - | 0.12 | 0.03 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| Bard | -0.72 | -0.51 | -0.50 | -0.35 | - | 0.12 | 0.03 |'
- en: '| Guanaco 13B | -0.59 | -0.41 | -0.20 | -0.19 | -0.12 | - | 0.20 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco 13B | -0.59 | -0.41 | -0.20 | -0.19 | -0.12 | - | 0.20 |'
- en: '| Guanaco 7B | -0.86 | -0.68 | -0.57 | -0.40 | -0.03 | -0.20 | - |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco 7B | -0.86 | -0.68 | -0.57 | -0.40 | -0.03 | -0.20 | - |'
- en: 'Table 13: The complete ordering induced by pairwise GPT-4 judgments between
    systems'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：通过 GPT-4 系统间的配对判断所产生的完整排序
- en: '| Model | Params | Size |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数 | 大小 |'
- en: '| --- | --- | --- |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Guanaco | 65B | 41 GB |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco | 65B | 41 GB |'
- en: '| Guanaco | 33B | 21 GB |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco | 33B | 21 GB |'
- en: '| Vicuna | 13B | 26 GB |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna | 13B | 26 GB |'
- en: '| ChatGPT-3.5 Turbo | N/A | N/A |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-3.5 Turbo | 不适用 | 不适用 |'
- en: '| Bard | N/A | N/A |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| Bard | 不适用 | 不适用 |'
- en: '| Guanaco | 13B | 10 GB |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco | 13B | 10 GB |'
- en: '| Guanaco | 7B | 5 GB |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco | 7B | 5 GB |'
- en: Appendix E NormalFloat 4-bit data type
  id: totrans-483
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E NormalFloat 4 位数据类型
- en: 'The exact values of the NF4 data type are as follows:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: NF4 数据类型的准确值如下：
- en: '[-1.0, -0.6961928009986877, -0.5250730514526367,'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '[-1.0, -0.6961928009986877, -0.5250730514526367,'
- en: -0.39491748809814453, -0.28444138169288635, -0.18477343022823334,
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: -0.39491748809814453, -0.28444138169288635, -0.18477343022823334,
- en: -0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: -0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,
- en: 0.24611230194568634, 0.33791524171829224, 0.44070982933044434,
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 0.24611230194568634, 0.33791524171829224, 0.44070982933044434,
- en: 0.5626170039176941, 0.7229568362236023, 1.0]
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 0.5626170039176941, 0.7229568362236023, 1.0]
- en: Appendix F Normality of Trained Neural Network Weights
  id: totrans-490
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 训练神经网络权重的正常性
- en: While it is common knowledge that trained neural network weights are mostly
    normally distributed, we perform statistical testing to verify this. We use the
    Shapiro-Wilk test[[53](#bib.bib53)] on the weights of the 7B LLaMA model [[57](#bib.bib57)].
    We find that the weights of each hidden unit have different normal distributions.
    As such, we test he weights of each individual hidden unit. This mean for weight
    $\mathbf{W}\in\mathcal{R}^{in\times out}$ dimension. Using a 5% significance threshold,
    we find that 7.5% of neurons are non-normally distributed which is about 2.5%
    more than the expected false-positive rate. As such, while almost all pretrained
    weights appear to be normally distributed there seem to be exceptions. Such exceptions
    might be due to outliers weights [[13](#bib.bib13)] or because the p-value of
    the Shaprio-Wilk test is not accurate for large samples sizes[[53](#bib.bib53)]
    that occur in the LLaMA FFN layer hidden units. this verifies the claim that neural
    network weights.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然普遍认为训练后的神经网络权重大多呈正态分布，但我们进行统计测试来验证这一点。我们对 7B LLaMA 模型的权重使用了 Shapiro-Wilk 检验[[53](#bib.bib53)]。我们发现每个隐藏单元的权重具有不同的正态分布。因此，我们测试了每个隐藏单元的权重。这意味着对权重
    $\mathbf{W}\in\mathcal{R}^{in\times out}$ 维度进行测试。使用 5% 的显著性水平，我们发现 7.5% 的神经元不呈正态分布，这比预期的假阳性率高出约
    2.5%。因此，尽管几乎所有预训练权重看起来都呈正态分布，但似乎存在例外。这些例外可能是由于异常值权重 [[13](#bib.bib13)] 或者因为 Shapiro-Wilk
    检验的 p 值对 LLaMA FFN 层隐藏单元中的大样本不准确 [[53](#bib.bib53)]。这验证了神经网络权重的这一说法。
- en: Appendix G Memory Footprint
  id: totrans-492
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 内存占用
- en: 'The memory footpring for QLoRA training with different LLaMA base models can
    be seen in Figure [6](#A7.F6 "Figure 6 ‣ Appendix G Memory Footprint ‣ QLoRA:
    Efficient Finetuning of Quantized LLMs"). We see that the 33B model does not quite
    fit into a 24 GB and that paged optimizers are needed to train it. Depicted is
    also batch size 1 with a sequence length of 512 and gradient checkpointning. This
    means, if one uses a larger batch size, or if a long sequence is processed, the
    activation gradient might consume a considerable amount of memory.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '不同 LLaMA 基础模型的 QLoRA 训练内存占用如图 [6](#A7.F6 "Figure 6 ‣ Appendix G Memory Footprint
    ‣ QLoRA: Efficient Finetuning of Quantized LLMs") 所示。我们看到 33B 模型无法完全适配 24 GB 内存，因此需要使用分页优化器进行训练。图中还展示了批量大小为
    1、序列长度为 512 和梯度检查点的情况。这意味着，如果使用较大的批量大小或处理较长的序列，激活梯度可能会消耗相当多的内存。'
- en: '![Refer to caption](img/26881d05a25b1702412a08c74cd5fe2a.png)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/26881d05a25b1702412a08c74cd5fe2a.png)'
- en: 'Figure 6: Breakdown of the memory footprint of different LLaMA models. The
    input gradient size is for batch size 1 and sequence length 512 and is estimated
    only for adapters and the base model weights (no attention). Numbers on the bars
    are memory footprint in MB of individual elements of the total footprint. While
    some models do not quite fit on certain GPUs, paged optimzier provide enough memory
    to make these models fit.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：不同 LLaMA 模型的内存占用细分。输入梯度大小适用于批量大小为 1 和序列长度为 512，仅估算了适配器和基础模型权重（不包括注意力）。条形上的数字是总内存占用中各个元素的内存占用（以
    MB 为单位）。虽然一些模型无法完全适配某些 GPU，但分页优化器提供了足够的内存，使这些模型能够适配。
